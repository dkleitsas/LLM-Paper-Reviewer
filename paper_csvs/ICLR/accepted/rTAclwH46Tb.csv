Section,Section Appearance Order,Paragraph
THE HONG KONG UNIVERSITY OF SCIENCE AND TECHNOLOGY,0.0,"1The Hong Kong University of Science and Technology
2Xi’an Jiaotong University
rpan@connect.ust.hk, yehaishan@xjtu.edu.cn"
ABSTRACT,0.000723589001447178,ABSTRACT
ABSTRACT,0.001447178002894356,"Learning rate schedulers have been widely adopted in training deep neural net-
works. Despite their practical importance, there is a discrepancy between its prac-
tice and its theoretical analysis. For instance, it is not known what schedules
of SGD achieve best convergence, even for simple problems such as optimizing
quadratic objectives. In this paper, we propose Eigencurve, the ﬁrst family of
learning rate schedules that can achieve minimax optimal convergence rates (up
to a constant) for SGD on quadratic objectives when the eigenvalue distribution
of the underlying Hessian matrix is skewed. The condition is quite common in
practice. Experimental results show that Eigencurve can signiﬁcantly outperform
step decay in image classiﬁcation tasks on CIFAR-10, especially when the num-
ber of epochs is small. Moreover, the theory inspires two simple learning rate
schedulers for practical applications that can approximate eigencurve. For some
problems, the optimal shape of the proposed schedulers resembles that of cosine
decay, which sheds light to the success of cosine decay for such situations. For
other situations, the proposed schedulers are superior to cosine decay."
INTRODUCTION,0.002170767004341534,"1
INTRODUCTION"
INTRODUCTION,0.002894356005788712,Many machine learning models can be represented as the following optimization problem:
INTRODUCTION,0.00361794500723589,"min
w f(w) ≜1 n n
X"
INTRODUCTION,0.004341534008683068,"i=1
fi(w),
(1.1)"
INTRODUCTION,0.005065123010130246,"such as logistic regression, deep neural networks. To solve the above problem, stochastic gradient
descent (SGD) (Robbins & Monro, 1951) has been widely adopted due to its computation efﬁciency
in large-scale learning problems (Bottou & Bousquet, 2008), especially for training deep neural
networks."
INTRODUCTION,0.005788712011577424,"Given the popularity of SGD in this ﬁeld, different learning rate schedules have been proposed to
further improve its convergence rates. Among them, the most famous and widely used ones are
inverse time decay, step decay (Gofﬁn, 1977), and cosine scheduler (Loshchilov & Hutter, 2017).
The learning rates generated by the inverse time decay scheduler depends on the current iteration
number inversely. Such a scheduling strategy comes from the theory of SGD on strongly convex
functions, and is extended to non-convex objectives like neural networks while still achieving good
performance. Step decay scheduler keeps the learning rate piecewise constant and decreases it by
a factor after a given amount of epochs. It is theoretically proved in Ge et al. (2019) that when
the objective is quadratic, step decay scheduler outperforms inverse time decay. Empirical results
are also provided in the same work to demonstrate the better convergence property of step decay
in training neural networks when compared with inverse time decay. However, even step decay"
INTRODUCTION,0.006512301013024602,"∗Equal contribution.
†Corresponding author is Haishan Ye.
‡Jointly with Google Research."
INTRODUCTION,0.00723589001447178,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007959479015918957,"is proved to be near optimal on quadratic objectives, it is not truly optimal. There still exists a
log T gap away from the minimax optimal convergence rate, which turns out to be non-trivial in a
wide range of settings and may greatly impact step decay’s empirical performance. Cosine decay
scheduler (Loshchilov & Hutter, 2017) generates cosine-like learning rates in the range [0, T], with
T being the maximum iteration. It is a heuristic scheduling strategy which relies on the observation
that good performance in practice can be achieved via slowly decreasing the learning rate in the
beginning and “reﬁning” the solution in the end with a very small learning rate. Its convergence
property on smooth non-convex functions has been shown in Li et al. (2021), but the provided
bound is still not tight enough to explain its success in practice."
INTRODUCTION,0.008683068017366137,"Except cosine decay scheduler, all aforementioned learning rate schedulers have (or will have) a
tight convergence bound on quadratic objectives. In fact, studying their convergence property on
quadratic objective functions is quite important for understanding their behaviors in general non-
convex problems. Recent studies in Neural Tangent Kernel (NTK) (Arora et al., 2019; Jacot et al.,
2020) suggest that when neural networks are sufﬁciently wide, the gradient descent dynamic of
neural networks can be approximated by NTK. In particular, when the loss function is least-square
loss, neural network’s inference is equivalent to kernel ridge regression with respect to the NTK
in expectation. In other words, for regression tasks, the non-convex objective in neural networks
resembles quadratic objectives when the network is wide enough."
INTRODUCTION,0.009406657018813314,"The existence of log T gap in step decay’s convergence upper bound, which will be proven to be tight
in a wide range of settings, implies that there is still room for improvement in theory. Meanwhile, the
existence of cosine decay scheduler, which has no strong theoretical convergence guarantees but pos-
sesses good empirical performance in certain tasks, suggests that its convergence rate may depend on
some speciﬁc properties of the objective determined by the network and dataset in practice. Hence
it is natural to ask what those key properties may be, and whether it is possible to ﬁnd theoretically-
optimal schedulers whose empirical performance are comparable to cosine decay if those properties
are available. In this paper, we offer an answer to these questions. We ﬁrst derive a novel eigenvalue-
distribution-based learning rate scheduler called eigencurve for quadratic functions. Combining
with eigenvalue distributions of different types of networks, new neural-network-based learning rate
schedulers can be generated based on our proposed paradigm, which achieve better convergence
properties than step decay in Ge et al. (2019). Speciﬁcally, eigencurve closes the log T gap in
step decay and reaches minimax optimal convergence rates if the Hessian spectrum is skewed. We
summarize the main contributions of this paper as follows."
INTRODUCTION,0.010130246020260492,"1. To the best of our knowledge, this is the ﬁrst work that incorporates the eigenvalue distribu-
tion of objective function’s Hessian matrix into designing learning rate schedulers. Accord-
ingly, based on the eigenvalue distribution of the Hessian, we propose a novel eigenvalue
distribution based learning rate scheduler named eigencurve.
2. Theoretically, eigencurve can achieve optimal convergence rate (up to a constant) for
SGD on quadratic objectives when the eigenvalue distribution of the Hessian is skewed.
Furthermore, even when the Hessian is not skewed, eigencurve can still achieve no
worse convergence rate than the step decay schedule in Ge et al. (2019), whose convergence
rate are proven to be sub-optimal in a wide range of settings.
3. Empirically, on image classiﬁcation tasks, eigencurve achieves optimal convergence
rate for several models on CIFAR-10 and ImageNet if the loss can be approximated by
quadratic objectives. Moreover, it obtains much better performance than step decay on
CIFAR-10, especially when the number of epochs is small.
4. Intuitively, our learning rate scheduler sheds light on the theoretical property of cosine
decay and provides a perspective of understanding the reason why it can achieve good
performance on image classiﬁcation tasks. The same idea has been used to inspire and
discover several simple families of schedules that works in practice."
INTRODUCTION,0.01085383502170767,"Problem Setup
For the theoretical analysis and the aim to derive our eigenvalue-dependent learn-
ing rate schedulers, we mainly focus on the quadratic function, that is,"
INTRODUCTION,0.011577424023154847,"min
w f(w) ≜Eξ [f(w, ξ)] , where f(w, ξ) = 1"
INTRODUCTION,0.012301013024602027,"2w⊤H(ξ)w −b(ξ)⊤w,
(1.2)"
INTRODUCTION,0.013024602026049204,"where ξ denotes the data sample. Hence, the Hessian of f(w) is
H = Eξ [H(ξ)] .
(1.3)"
INTRODUCTION,0.013748191027496382,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01447178002894356,"Letting us denote b = Eξ[b(ξ)], we can obtain the optima of problem (1.2)"
INTRODUCTION,0.015195369030390739,"w∗= H−1b.
(1.4)"
INTRODUCTION,0.015918958031837915,"Given an initial iterate w0 and the learning rate sequence {ηt}, the stochastic gradient update is"
INTRODUCTION,0.016642547033285094,"wt+1 = wt −ηt∇f(wt, ξ) = wt −ηt(H(ξ)wt −b(ξ)).
(1.5)"
INTRODUCTION,0.017366136034732273,We denote that
INTRODUCTION,0.01808972503617945,"nt = Hwt −b −(H(ξ)wt −b(ξ)) ,
µ ≜λmin(H),
L ≜λmax(H),
and,
κ ≜L/µ. (1.6)"
INTRODUCTION,0.01881331403762663,"In this paper, we assume that"
INTRODUCTION,0.019536903039073805,"Eξ

ntn⊤
t

⪯σ2H.
(1.7)"
INTRODUCTION,0.020260492040520984,The reason for this assumption is presented in Appendix G.5.
INTRODUCTION,0.020984081041968163,"Table 1: Convergence rate of SGD with common sched-
ulers on quadratic objectives."
INTRODUCTION,0.02170767004341534,"Scheduler
Convergence rate of SGD in
quadratic objectives"
INTRODUCTION,0.02243125904486252,"Constant
Not guaranteed to converge"
INTRODUCTION,0.023154848046309694,"Inverse Time Decay
Θ

dσ2 T
· κ "
INTRODUCTION,0.023878437047756874,"Step Decay
Θ

dσ2"
INTRODUCTION,0.024602026049204053,"T
· log T "
INTRODUCTION,0.02532561505065123,"(Ge et al. (2019); Wu et al. (2021);
This work - Theorem 4)"
INTRODUCTION,0.02604920405209841,"Eigencurve
O

dσ2"
INTRODUCTION,0.026772793053545588,"T

with skewed Hessian spectrums,"
INTRODUCTION,0.027496382054992764,"O

dσ2"
INTRODUCTION,0.028219971056439943,"T
· log κ"
INTRODUCTION,0.02894356005788712,"
in worst case
(This work - Theorem 1, Corollary 2, 3)"
INTRODUCTION,0.029667149059334298,"Related Work
In convergence anal-
ysis, one key property that separates
SGD from vanilla gradient descent is
that in SGD, noise in gradients domi-
nates. In gradient descent (GD), con-
stant learning rate can achieve linear
convergence O(cT ) with 0 < c < 1 for
strongly convex objectives, i.e. obtain-
ing f(w(t)) −f(w∗) ≤ϵ in O(log( 1"
INTRODUCTION,0.030390738060781478,"ϵ ))
iterations. However, in SGD, f(w(t))
cannot even be guaranteed to converge
to f(w∗) due to the existence of gra-
dient noise (Bottou et al., 2018).
In-
tuitively, this noise leads to a variance
proportional to the learning rate size, so
constant learning rate will always intro-
duce a Ω(ηt) = Ω(η0) gap when com-
pared with the convergence rate of GD.
Fortunately, inverse time decay scheduler solves the problem by decaying the learning rate inversely
proportional to the iteration number t, which achieves O( 1"
INTRODUCTION,0.031114327062228653,"T ) convergence rate for strongly convex
objectives, speciﬁcally, O( dσ2"
INTRODUCTION,0.03183791606367583,"T
· κ). However, this is sub-optimal since the minimax optimal rate
for SGD is O( dσ2"
INTRODUCTION,0.03256150506512301,"T ) (Ge et al., 2019; Jain et al., 2018). Moreover, in practice, κ can be very big
for large neural networks, which makes inverse time decay scheduler undesirable for those models.
This is when step decay (Gofﬁn, 1977) comes to play. Empirically, it is widely adopted in tasks
such as image classiﬁcation and serves as a baseline for a lot of models. Theoretically, it has been
proven that step decay can achieve nearly optimal convergence rate O( dσ2"
INTRODUCTION,0.03328509406657019,"T · log T) for strongly con-
vex least square regression (Ge et al., 2019). A tighter set of instance-dependent bounds in a recent
work (Wu et al., 2021), which is carried out independently from ours, also proves its near optimal-
ity. Nevertheless, step decay is not always the best choice for image classiﬁcation tasks. In practice,
cosine decay (Loshchilov & Hutter, 2017) can achieve comparable or even better performance, but
the reason behind this superior performance is still unknown in theory (Gotmare et al., 2018). All
the aforementioned results are summarized in Table 1, along with our results in this paper. It is
worth mentioning that the minimax optimal rate O( dσ2"
INTRODUCTION,0.03400868306801737,"T ) can be achieved by iterate averaging meth-
ods (Jain et al., 2018; Bach & Moulines, 2013; D´efossez & Bach, 2015; Frostig et al., 2015; Jain
et al., 2016; Neu & Rosasco, 2018), but it is not a common practice to use them in training deep
neural networks, so only the ﬁnal iterate (Shamir, 2012) behavior of SGD is analyzed in this paper,
i.e. the point right after the last iteration."
INTRODUCTION,0.03473227206946455,"Paper organization: Section 2 describes the motivation of our eigencurve scheduler. Section 3
presents the exact form and convergence rate of the proposed eigencurve scheduler, along with
the lower bound of step decay. Section 4 shows the experimental results. Section 5 discusses the
discovery and limitation of eigencurve and Section 6 gives our conclusion."
INTRODUCTION,0.03545586107091172,Published as a conference paper at ICLR 2022
MOTIVATION,0.0361794500723589,"2
MOTIVATION"
MOTIVATION,0.03690303907380608,"In this section, we will give the main motivation and intuition of our eigencurve learning rate
scheduler. We ﬁrst give the scheduling strategy to achieve the optimal convergence rate in the case
that the Hessian is diagonal. Then, we show that the inverse time learning rate is sub-optimal in
most cases. Comparing these two scheduling methods brings up the reason why we should design
eigenvalue distribution dependent learning rate scheduler."
MOTIVATION,0.03762662807525326,"Letting H be a diagonal matrix diag(λ1, λ2, . . . , λd) and reformulating Eqn. (1.5), we have
wt+1 −w∗=wt −w∗−ηt(H(ξ)wt −b(ξ))
=wt −w∗−ηt(Hwt −b) + ηt (Hwt −b −(H(ξ)wt −b(ξ)))
=wt −w∗−ηt(Hwt −b −(Hw∗−b)) + ηt (Hwt −b −(H(ξ)wt −b(ξ)))
= (I −ηtH) (wt −w∗) + ηtnt."
MOTIVATION,0.03835021707670044,"It follows,"
MOTIVATION,0.03907380607814761,"E
h
λj (wt+1,j −w∗,j)2i E[nt]=0
=
λj
n
(1 −ηtλj)2E

(wt,j −w∗,j)2
+ η2
t E ∥[nt]j∥2o"
MOTIVATION,0.03979739507959479,"(1.7)
≤(1 −ηtλj)2 · λjE

(wt,j −w∗,j)2
+ η2
t λ2
jσ2.
(2.1)"
MOTIVATION,0.04052098408104197,"Since H is diagonal, we can set step size scheduling for each dimension separately. Letting us
choose step size ηt coordinately with the step size ηt,j =
1
λj(t+1) being optimal for the j-th coordi-
nate, then we have the following proposition.
Proposition 1. Assume that H is diagonal matrix with eigenvalues λ1 ≥λ2 ≥. . . , λd ≥0 and
Eqn. (1.7) holds. If we set step size ηt,j =
1
λj(t+1), it holds that"
MOTIVATION,0.04124457308248915,"2E [f(wt+1) −f(w∗)] = E  
d
X"
MOTIVATION,0.041968162083936326,"j=1
λj (wt+1,j −w∗,j)2  ≤"
MOTIVATION,0.0426917510853835,"Pd
j=1 λj(w1,j −w∗,j)2"
MOTIVATION,0.04341534008683068,"(t + 1)2
+
t
(t + 1)2 ·dσ2. (2.2)"
MOTIVATION,0.04413892908827786,"The leading equality here is proved in Appendix G.1, with the followed inequality proved in Ap-
pendix E. From Eqn. (2.2), we can observe that choosing proper step sizes coordinately can achieve
the optimal convergence rate (Ge et al., 2019; Jain et al., 2018). Instead, if the widely used inverse
time scheduler ηt = 1/(L + µt) is chosen, we can show that only a sub-optimal convergence rate
can be obtained, especially when λj’s vary from each other.
Proposition 2. If we set the inverse time step size ηt =
1
(L+µt), then we have"
MOTIVATION,0.04486251808972504,"2E [f(wt+1) −f(w∗)] = E  
d
X"
MOTIVATION,0.045586107091172216,"j=1
λj (wt+1,j −w∗,j)2  "
MOTIVATION,0.04630969609261939,"≤
 L + µ"
MOTIVATION,0.04703328509406657,"L + µt 2
 
d
X"
MOTIVATION,0.04775687409551375,"j=1
λj(w1,j −w∗,j)2  + d
X j=1"
MOTIVATION,0.04848046309696093,"λ2
j
2λj −µ ·
1
L + µt · σ2 +
λ2
jσ2"
MOTIVATION,0.049204052098408106,(L + µt)2 ! . (2.3)
MOTIVATION,0.049927641099855286,"Remark 1. Eqn. (2.2) shows that if one can choose step size coordinate-wise with step size ηt,j =
1
λj(t+1), then SGD can achieve a convergence rate"
MOTIVATION,0.05065123010130246,"E [f(wT +1) −f(w∗)] ≤O
 d"
MOTIVATION,0.05137481910274964,"T · σ2

.
(2.4)"
MOTIVATION,0.05209840810419682,"which matches the lower bound (Ge et al., 2019; Jain et al., 2018). In contrast, replacing L = λ1
and µ = λd in Proposition 2, we can obtain that the convergence rate of SGD being"
MOTIVATION,0.052821997105643996,"E [f(wT +1) −f(w∗)] ≤O  1 T d
X j=1"
MOTIVATION,0.053545586107091175,"λj
λd
· σ2 "
MOTIVATION,0.05426917510853835,".
(2.5)"
MOTIVATION,0.05499276410998553,"Since it holds that λj ≥λd, the convergence rate in Eqn. (2.4) is better than the one in Eqn. (2.5),
especially when the eigenvalues of the Hessian (H matrix) decay rapidly. In fact, the upper bound
in Eqn. (2.5) is tight for the inverse time decay scheduling, as proven in Ge et al. (2019)."
MOTIVATION,0.055716353111432707,Published as a conference paper at ICLR 2022
MOTIVATION,0.056439942112879886,"Main Intuition
The diagonal case H = diag(λ1, λ2, . . . , λd) provides an important intuition for
designing eigenvalue dependent learning rate scheduling. In fact, for general non-diagonal H, letting
H = UΛU ⊤be the spectral decomposition of the Hessian and setting w′ = U ⊤w, then the Hessian
becomes a diagonal matrix from perspective of updating w′, with the variance of the stochastic
gradient being unchanged since U is a unitary matrix. This is also the core idea of Newton’s method
and many second-order methods (Huang et al., 2020). However, given our focus in this paper being
learning rate schedulers only, we move the relevant discussion of their relationship to Appendix H."
MOTIVATION,0.057163531114327065,"Proposition 1 and 2 imply that a good learning rate scheduler should decrease the error of each co-
ordinate. The inverse time decay scheduler is only optimal for the coordinate related to the smallest
eigenvalue. That’s the reason why it is sub-optimal overall. Thus, we should reduce the learning
rate gradually such that we can run a optimal learning rate associated to λj to sufﬁciently drop the
error of j-th coordinate. Furthermore, given the total iteration T and the eigenvalue distribution of
the Hessian, we should allocate the running time for each optimal learning rate associated to λj."
EIGENVALUE DEPENDENT STEP SCHEDULING,0.05788712011577424,"3
EIGENVALUE DEPENDENT STEP SCHEDULING"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.05861070911722142,"Figure 1: Eigencurve : piecewise in-
verse time decay scheduling."
EIGENVALUE DEPENDENT STEP SCHEDULING,0.059334298118668596,"Just as discussed in Section 2, to obtain better conver-
gence rate for SGD, we should consider Hessian’s eigen-
value distribution and schedule the learning rate based on
the distribution. In this section, we propose a novel learn-
ing rate scheduler for this task, which can be regarded as
piecewise inverse time decay (see Figure 1). The method
is very simple, we group eigenvalues according to their
value and denote si to be the number of eigenvalues lie in
the range Ri = [µ · 2i, µ · 2i+1), that is,"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.060057887120115776,"si = #λj ∈[µ · 2i, µ · 2i+1).
(3.1)"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.060781476121562955,"Then, there are at most Imax = log2 κ such ranges. By the inverse time decay theory, the optimal
learning rate associated to eigenvalues in the range Ri should be"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06150506512301013,"ηt = O

1
2i−1µ · (t −ti)"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06222865412445731,"
, with 0 = t0 < t1 < t2 < · · · < tImax = T.
(3.2)"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06295224312590449,"Our scheduling strategy is to run the optimal learning rate for eigenvalues in each Ri for a period to
sufﬁciently decrease the error associated to eigenvalues in Ri."
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06367583212735166,"To make the step size sequence {ηt}T
t=1 monotonely decreasing, we deﬁne the step sizes as"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06439942112879884,"ηt =
1"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06512301013024602,"L + µ Pi−1
j=1 ∆j2j−1 + 2i−1µ(t −ti−1)
if
t ∈[ti−1, ti)
(3.3) where"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.0658465991316932,"0 = t0 < t1 < t2 < · · · < tImax = T, ∆i = ti −ti−1, and Imax = log2 κ.
(3.4)"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06657018813314038,"To make the total error, that is the sum of error associated with Ri, to be small, we should allocate
∆i according to si−1’s. Intuitively, a large portion of eigenvalues lying in the range Ri should
allocate a large portion of iterations. Speciﬁcally, we propose to allocate ∆i as follows:"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06729377713458755,"∆i =
√si−1
PImax−1
i′=0
√si′
· T, with si = #λj ∈[µ · 2i, µ · 2i+1).
(3.5)"
EIGENVALUE DEPENDENT STEP SCHEDULING,0.06801736613603473,"In the rest of this section, we will show that the step size scheduling according to Eqn. (3.3) and
(3.5) can achieve better convergence rate than the one in Ge et al. (2019) when si is non-uniformly
distributed. In fact, a better ∆i allocation can be calculated using numerical optimization."
THEORETICAL ANALYSIS,0.06874095513748191,"3.1
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.0694645441389291,"Lemma 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T-steps starting from w0 and a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3), the ﬁnal iterate"
THEORETICAL ANALYSIS,0.07018813314037627,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.07091172214182344,wT +1 satisﬁes
THEORETICAL ANALYSIS,0.07163531114327062,E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) · κ2
THEORETICAL ANALYSIS,0.0723589001447178,"∆2
1
+ 15"
THEORETICAL ANALYSIS,0.07308248914616498,2 · σ2µ
THEORETICAL ANALYSIS,0.07380607814761216,"Imax−1
X ˜i=0"
THEORETICAL ANALYSIS,0.07452966714905933,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 . (3.6)"
THEORETICAL ANALYSIS,0.07525325615050651,"Since the bias term is a high order term, the variance term in Eqn. (3.6) dominates the error for
wT +1. For simplicity, instead of using numerical methods to ﬁnd the optimal {∆i}, we propose to
use ∆i deﬁned in Eqn. (3.5). The value of ∆i is linear to square root of the number of eigenvalues
lying in the range [µ · 2i−1, µ · 2i). Using such ∆i, eigencurve has the following convergence
property.
Theorem 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T-steps starting from w0, a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3) and ∆i deﬁned in
Eqn. (3.5), the ﬁnal iterate wT +1 satisﬁes"
THEORETICAL ANALYSIS,0.07597684515195369,"E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) ·
κ2 ·
PImax−1
i=0
√si
2"
THEORETICAL ANALYSIS,0.07670043415340087,"s0T 2
+
15
PImax−1
i=0
√si
2"
THEORETICAL ANALYSIS,0.07742402315484805,"T
· σ2."
THEORETICAL ANALYSIS,0.07814761215629522,"Please refer to Appendix D, F and G for the full proof of Lemma 1 and Theorem 1. The variance"
THEORETICAL ANALYSIS,0.0788712011577424,"term
15(
PImax−1
i=0
√si)
2"
THEORETICAL ANALYSIS,0.07959479015918958,"T
· σ2 in above theorem shows that when si’s vary largely from each other,
then the variance can be close to O
  d"
THEORETICAL ANALYSIS,0.08031837916063676,"T · σ2
which matches the lower bound (Ge et al., 2019). For
example, letting Imax = 100, s0 = 0.99d and si = 0.01"
THEORETICAL ANALYSIS,0.08104196816208394,"99 d, we can obtain that
P99
i=0
√si
2"
THEORETICAL ANALYSIS,0.08176555716353111,"T
· σ2 = (
√"
THEORETICAL ANALYSIS,0.0824891461649783,"0.99 + 99 ×
p"
THEORETICAL ANALYSIS,0.08321273516642547,0.01/99)2 · d
THEORETICAL ANALYSIS,0.08393632416787265,T · σ2 < 4d
THEORETICAL ANALYSIS,0.08465991316931983,T · σ2.
THEORETICAL ANALYSIS,0.085383502170767,"Figure 2:
Power law observed in
ResNet-18 on ImageNet, both eigen-
value (x-axis) and density (y-axis) are
plotted in log scale."
THEORETICAL ANALYSIS,0.08610709117221418,"We can observe that if the variance of si’s is large, the
variance term in Theorem 1 can be close to dσ2/T.
More generally, as rigorously stated in Corollary 2,
eigencurve achieves minimax optimal convergence
rate if the Hessian spectrum satisﬁes an extra assumption
of “power law”: the density of eigenvalue λ is exponen-
tially decaying with increasing value of λ in log scale, i.e.
ln(λ). This assumption comes from the observation of
estimated Hessian spectrums in practice (see Figure 2),
which will be further illustrated in Section 4.1.
Corollary 2. Given the same setting as in Theorem 1,
when Hessian H’s eigenvalue distribution p(λ) satisﬁes
“power law”, i.e."
THEORETICAL ANALYSIS,0.08683068017366136,p(λ) = 1
THEORETICAL ANALYSIS,0.08755426917510854,Z · exp(−α(ln(λ) −ln(µ))) = 1
THEORETICAL ANALYSIS,0.08827785817655572,"Z ·
µ λ α"
THEORETICAL ANALYSIS,0.08900144717800289,"(3.7)
for some α > 1, where Z =
R L
µ (µ/λ)αdλ, there exists a
constant C(α) which only depends on α, such that the ﬁnal iterate wT +1 satisﬁes"
THEORETICAL ANALYSIS,0.08972503617945007,"E [f(wT +1) −f(w∗)] ≤

(f(w0) −f(w∗)) · κ2"
THEORETICAL ANALYSIS,0.09044862518089725,T 2 + dσ2 T
THEORETICAL ANALYSIS,0.09117221418234443,"
· C(α)."
THEORETICAL ANALYSIS,0.0918958031837916,"Please refer to Appendix G.3 for the proof. As for the worst-case guarantee, it is easy to check that
only when si’s are equal to each other, that is, si = d/Imax = d/ log2(κ), the variance term reaches
its maximum.
Corollary 3. Given the same setting as in Theorem 1, when si = d/ log2(κ) for all 0 ≤i ≤
Imax −1, the variance term in Theorem 1 reaches its maximum and wT +1 satisﬁes"
THEORETICAL ANALYSIS,0.09261939218523878,E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) · κ2 log2 κ
THEORETICAL ANALYSIS,0.09334298118668596,"T 2
+ 15d · log κ T
σ2."
THEORETICAL ANALYSIS,0.09406657018813314,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.09479015918958032,"Remark 2. When si’s vary from each other, our eigenvalue dependent learning rate scheduler can
achieve faster convergence rate than eigenvalue independent scheduler such as step decay which
suffers from an extra log(T) term (Ge et al., 2019). Only when si’s are equal to each other, Corol-
lary 3 shows that the bound of variance matches to lower bound up to log κ which is same to the
one in Proposition 3 of Ge et al. (2019)."
THEORETICAL ANALYSIS,0.0955137481910275,"Furthermore, we show that this log T gap between step decay and eigencurve certainly exists
for problem instances of skewed Hessian spectrums. For simplicity, we only discuss the case where
H is diagonal.
Theorem 4. Let objective function f(x) be quadratic. We run SGD for T-steps starting from w0 and
a step decay learning rate sequence {ηt}T
t=1 deﬁned in Algorithm 1 of Ge et al. (2019) with η1 ≤
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ

ntn⊤
t

=
σ2H and (3) λj (w0,j −w∗,j)2 ̸= 0 for ∀j = 1, 2, . . . , d, the ﬁnal iterate wT +1 satisﬁes,"
THEORETICAL ANALYSIS,0.09623733719247468,"E [f(wT +1) −f(w∗)] = Ω
dσ2"
THEORETICAL ANALYSIS,0.09696092619392185,"T
· log T
"
THEORETICAL ANALYSIS,0.09768451519536903,"The proof is provided in Appendix G.4. Removing this extra log T term may not seem to be a big
deal in theory, but experimental results suggest the opposite."
EXPERIMENTS,0.09840810419681621,"4
EXPERIMENTS"
EXPERIMENTS,0.09913169319826338,"To demonstrate eigencurve ’s practical value, empirical experiments are conducted on the task
of image classiﬁcation 1. Two well-known dataset are used: CIFAR-10 (Krizhevsky et al., 2009)
and ImageNet (Deng et al., 2009). For full experimental results on more datasets, please refer to
Appendix A."
EXPERIMENTS,0.09985528219971057,"4.1
HESSIAN SPECTRUM’S SKEWNESS IN PRACTICE"
EXPERIMENTS,0.10057887120115774,"According to estimated2 eigenvalue distributions of Hessian on CIFAR-10 and ImageNet, as shown
in Figure 3, it can be observed that all of them are highly skewed and share a similar tendency: A
large portion of small eigenvalues and a tiny portion of large eigenvalues. This phenomenon has also
been observed and explained by other researchers in the past(Sagun et al., 2017; Arjevani & Field,
2020). On top of that, when we plot both eigenvalues and density in log scale, the “power law” arises.
Therefore, if the loss surface can be approximated by quadratic objectives, then eigencurve has
already achieved optimal convergence rate for those practical settings. The exact values of the extra
constant terms are presented in Appendix A.2."
EXPERIMENTS,0.10130246020260492,"4.2
IMAGE CLASSIFICATION ON CIFAR-10 WITH EI G E N C U R V E SCHEDULING"
EXPERIMENTS,0.1020260492040521,"This optimality in theory induces eigencurve ’s superior performance in practice, which is
demonstrated in Table 2 and Figure 4. The full set of ﬁgures are available in Appendix A.8. All mod-
els are trained with stochastic gradient descent (SGD), no momentum, batch size 128 and weight
decay wd = 0.0005. For full details of the experiment setup, please refer to Appendix B."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10274963820549927,"4.3
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10347322720694646,"By simplifying the form of eigencurve and capturing some of its key properties, two simple and
practical schedules are proposed: Elastic Step Decay and Cosine-power Decay, whose empirical
performance are better than or at least comparable to cosine decay. Due to page limit, we leave all
the experimental results in Appendix A.5, A.6, A.7."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10419681620839363,"Elastic Step Decay: ηt = η0/2k , if t ∈

(1 −rk)T, (1 −rk+1)T

(4.1)"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.1049204052098408,"Cosine-power Decay: ηt = ηmin + (η0 −ηmin)
1"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10564399421128799,"2(1 + cos(
t
tmax
π))
α
(4.2)"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10636758321273516,"1Code: https://github.com/opensource12345678/why_cosine_works/tree/main
2Please refer to Appendix B.2 for details of the estimation and preprocessing procedure."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10709117221418235,Published as a conference paper at ICLR 2022
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10781476121562952,"0
25
50
75
100
125
150
175"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.1085383502170767,"Eigenvlaue 10
6 10
4 10
2 100"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10926193921852388,Density (Log Scale)
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.10998552821997105,"0
50
100
150
200
250"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11070911722141824,"Eigenvlaue 10
6 10
4 10
2 100"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11143270622286541,Density (Log Scale)
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11215629522431259,"0
200
400
600
800
1000
1200"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11287988422575977,"Eigenvlaue 10
7 10
6 10
5 10
4 10
3 10
2 10
1 100"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11360347322720694,Density (Log Scale)
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11432706222865413,"Figure 3: The estimated eigenvalue distribution of Hessian for ResNet-18 on CIFAR-10, GoogLeNet
on CIFAR-10 and ResNet-18 on ImageNet respectively. Notice that the density here is all shown
in log scale. First row: original scale for eigenvalues. Second row: log scale for preprocessed
eigenvalues."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.1150506512301013,"Table 2: CIFAR-10: training losses and test accuracy of different schedules. Step Decay denotes the
scheduler proposed in Ge et al. (2019) and General Step Decay means the same type of scheduler
with searched interval numbers and decay rates. “*” before a number means at least one occurrence
of loss explosion among all 5 trial experiments."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11577424023154848,"#Epoch
Schedule
ResNet-18
GoogLeNet
VGG16
Loss
Acc(%)
Loss
Acc(%)
Loss
Acc(%) =10"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11649782923299566,"Inverse Time Decay
1.58±0.02
79.45±1.00
2.61±0.00
86.54±0.94
2.26±0.00
84.47±0.74
Step Decay
1.82±0.04
73.77±1.48
2.59±0.02
87.04±0.48
2.42±0.45
82.98±0.27
General Step Decay
1.52±0.02
81.99±0.35
1.93±0.03
88.32±1.32
2.14±0.42
86.79±0.36
Cosine Decay
1.42±0.01
84.23±0.07
1.94±0.00
90.56±0.31
2.03±0.00
87.99±0.13
Eigencurve
1.36±0.01
85.62±0.28
1.33±0.00
90.65±0.15
1.87±0.00
88.73±0.11 =100"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11722141823444283,"Inverse Time Decay
0.73±0.00
90.82±0.43
0.62±0.02
92.05±0.69
1.32±0.62
*76.24±13.77
Step Decay
0.26±0.01
91.39±1.03
0.28±0.00
92.83±0.15
0.59±0.00
91.37±0.20
General Step Decay
0.17±0.00
93.97±0.21
0.13±0.00
94.18±0.18
0.20±0.00
*92.36±0.46
Cosine Decay
0.17±0.00
94.04±0.21
0.12±0.00
94.62±0.11
0.20±0.00
93.17±0.05
Eigencurve
0.14±0.00
94.05±0.18
0.12±0.00
94.75±0.15
0.18±0.00
92.88±0.24"
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11794500723589002,"Figure 4: Example: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses.
Right: test accuracy. For full ﬁgures of this experiment, please refer to Appendix A.8."
INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS,0.11866859623733719,Published as a conference paper at ICLR 2022
DISCUSSION,0.11939218523878437,"5
DISCUSSION"
DISCUSSION,0.12011577424023155,"Cosine Decay and Eigencurve
For ResNet-18 on CIFAR-10 dataset, eigencurve scheduler
presents an extremely similar learning rate curve to cosine decay, especially when the number of
training epochs is set to 100, as shown in Figure 5. This directly links cosine decay to our theory:
the empirically superior performance of cosine decay is very likely to stem from the utilization of the
“skewness” among Hessian matrix’s eigenvalues. For other situations, especially when the number
of iterations is small, as shown in Table 2, eigencurve presents a better performance than cosine
decay ."
DISCUSSION,0.12083936324167872,"Figure 5: Eigencurve ’s learning rate curve generated by the estimated eigenvalue distribution for
ResNet-18 on CIFAR-18 after training 50/100/200 epochs. The cosine decay’s learning rate curve
(green) is also provided for comparison."
DISCUSSION,0.12156295224312591,"Sensitiveness to Hessian’s Eigenvalue Distributions
One limitation of eigencurve is that it
requires a precomputed eigenvalue distribution of objective functions’s Hessian matrix, which can
be time-consuming for large models. This issue can be overcome by reusing the estimated eigen-
value distribution from similar settings. Further experiments on CIFAR-10 suggest the effective-
ness of this approach. Please refer to Appendix A.3 for more details. This evidence suggests that
eigencurve’s performance is not very sensitive to estimated eigenvalue distributions."
DISCUSSION,0.12228654124457308,"Relationship with Numerically Near-optimal Schedulers
In Zhang et al. (2019), a dynamic
programming algorithm was proposed to ﬁnd almost optimal schedulers if the exact loss of the
quadratic objective is accessible. While it is certainly the case, eigencurve still possesses several
additional advantages over this type of approaches. First, eigencurve can be used to ﬁnd simple-
formed schedulers. Compared with schedulers numerically computed by dynamic programming,
eigencurve provides an analytic framework, so it is able to bypass the Hessian spectrum estima-
tion process if some useful assumptions of the Hessian spectrum can be obtained, such as ”power
law”. Second, eigencurve has a clear theoretical convergence guarantee. Dynamic programming
can ﬁnd almost optimal schedulers, but the convergence property of the computed scheduler is still
unclear. Our work ﬁlls this gap."
CONCLUSION,0.12301013024602026,"6
CONCLUSION"
CONCLUSION,0.12373371924746744,"In this paper, a novel learning rate schedule named eigencurve is proposed, which utilizes the
“skewness” of objective’s Hessian matrix’s eigenvalue distribution and reaches minimax optimal
convergence rates for SGD on quadratic objectives with skewed Hessian spectrums. This condition
of skewed Hessian spectrums is observed and indeed satisﬁed in practical settings of image classiﬁ-
cation. Theoretically, eigencurve achieves no worse convergence guarantee than step decay for
quadratic functions and reaches minimax optimal convergence rate (up to a constant) with skewed
Hessian spectrums, e.g. under “power law”. Empirically, experimental results on CIFAR-10 show
that eigencurve signiﬁcantly outperforms step decay, especially when the number of epochs is
small. The idea of eigencurve offers a possible explanation for cosine decay’s effectiveness in
practice and inspires two practical families of schedules with simple forms."
CONCLUSION,0.12445730824891461,Published as a conference paper at ICLR 2022
CONCLUSION,0.1251808972503618,ACKNOWLEDGEMENT
CONCLUSION,0.12590448625180897,"This work is supported by GRF 16201320. Rui Pan acknowledges support from the Hong Kong PhD
Fellowship Scheme (HKPFS). The work of Haishan Ye was supported in part by National Natural
Science Foundation of China under Grant No. 12101491."
REFERENCES,0.12662807525325614,REFERENCES
REFERENCES,0.12735166425470332,"Yossi Arjevani and Michael Field. Analytic characterization of the hessian in shallow relu models:
A tale of symmetry, 2020."
REFERENCES,0.12807525325615052,"Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an inﬁnitely wide neural net, 2019."
REFERENCES,0.1287988422575977,"Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con-
vergence rate o (1/n). arXiv preprint arXiv:1306.2119, 2013."
REFERENCES,0.12952243125904486,"Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 557–565. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/
botev17a.html."
REFERENCES,0.13024602026049203,"L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J. Platt, D. Koller,
Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, vol-
ume 20. Curran Associates, Inc., 2008.
URL https://proceedings.neurips.cc/
paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf."
REFERENCES,0.1309696092619392,"L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning, 2018."
REFERENCES,0.1316931982633864,"Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016."
REFERENCES,0.13241678726483358,"Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM trans-
actions on intelligent systems and technology (TIST), 2(3):1–27, 2011."
REFERENCES,0.13314037626628075,"Alexandre D´efossez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and
optimal sampling distributions.
In Artiﬁcial Intelligence and Statistics, pp. 205–213. PMLR,
2015."
REFERENCES,0.13386396526772792,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.1345875542691751,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.1353111432706223,"Murat
A
Erdogdu
and
Andrea
Montanari.
Convergence
rates
of
sub-sampled
new-
ton methods.
In C. Cortes,
N. Lawrence,
D. Lee,
M. Sugiyama,
and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf."
REFERENCES,0.13603473227206947,"Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the empirical risk
minimizer in a single pass. In Conference on learning theory, pp. 728–763. PMLR, 2015."
REFERENCES,0.13675832127351664,"Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure for least squares. In Advances in
Neural Information Processing Systems, pp. 14977–14988, 2019."
REFERENCES,0.13748191027496381,"Jean-Louis Gofﬁn. On convergence rates of subgradient optimization methods. Mathematical pro-
gramming, 13(1):329–347, 1977."
REFERENCES,0.138205499276411,Published as a conference paper at ICLR 2022
REFERENCES,0.1389290882778582,"Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at
deep learning heuristics: Learning rate restarts, warmup and distillation, 2018."
REFERENCES,0.13965267727930536,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour, 2018."
REFERENCES,0.14037626628075253,"Roger Grosse and James Martens.
A kronecker-factored approximate ﬁsher matrix for convo-
lution layers.
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The
33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 573–582, New York, New York, USA, 20–22 Jun 2016. PMLR. URL
https://proceedings.mlr.press/v48/grosse16.html."
REFERENCES,0.1410998552821997,"Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9
(8):1735–1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735."
REFERENCES,0.14182344428364688,"Xunpeng Huang, Xianfeng Liang, Zhengyang Liu, Lei Li, Yue Yu, and Yitan Li. Span: A stochastic
projected approximate newton method.
In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pp. 1520–1527, 2020."
REFERENCES,0.14254703328509408,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks, 2020."
REFERENCES,0.14327062228654125,"Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford.
Par-
allelizing stochastic approximation through mini-batching and tail-averaging.
arXiv preprint
arXiv:1610.03774, 2016."
REFERENCES,0.14399421128798842,"Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Acceler-
ating stochastic gradient descent for least squares regression, 2018."
REFERENCES,0.1447178002894356,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.14544138929088277,"Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. A second look at exponential and cosine step
sizes: Simplicity, adaptivity, and performance, 2021."
REFERENCES,0.14616497829232997,"Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017."
REFERENCES,0.14688856729377714,"Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. URL
https://aclanthology.org/J93-2004."
REFERENCES,0.1476121562952243,"Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic gradient de-
scent. In Conference On Learning Theory, pp. 3222–3242. PMLR, 2018."
REFERENCES,0.14833574529667148,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400–407, 1951."
REFERENCES,0.14905933429811866,"Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singu-
larity and beyond, 2017."
REFERENCES,0.14978292329956586,"Nicol N Schraudolph.
Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723–1738, 2002."
REFERENCES,0.15050651230101303,"Ohad Shamir. Is averaging needed for strongly convex stochastic gradient descent. Open problem
presented at COLT, 2012."
REFERENCES,0.1512301013024602,"Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes, 2012."
REFERENCES,0.15195369030390737,Published as a conference paper at ICLR 2022
REFERENCES,0.15267727930535455,"Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Last iterate
risk bounds of sgd with decaying stepsize for overparameterized linear regression. arXiv preprint
arXiv:2110.06198, 2021."
REFERENCES,0.15340086830680175,"Minghan Yang, Dong Xu, Hongyu Chen, Zaiwen Wen, and Mengyun Chen. Enhance curvature
information by structured stochastic quasi-newton methods. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10654–10663, June 2021."
REFERENCES,0.15412445730824892,"Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. Pyhessian: Neural networks
through the lens of the hessian, 2020."
REFERENCES,0.1548480463096961,"Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization,
2015."
REFERENCES,0.15557163531114326,"Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model, 2019."
REFERENCES,0.15629522431259044,Published as a conference paper at ICLR 2022
REFERENCES,0.15701881331403764,"A
MORE EXPERIMENTAL RESULTS"
REFERENCES,0.1577424023154848,"A.1
RIDGE REGRESSION"
REFERENCES,0.15846599131693198,We compare different types of schedulings on ridge regression
REFERENCES,0.15918958031837915,f(w) = 1
REFERENCES,0.15991316931982633,"n||Xw −Y ||2
2 + α||w||2
2."
REFERENCES,0.16063675832127353,"This experiment is only an empirical proof of our theory. In fact, the optima of ridge regression has
a closed form and can be directly computed with"
REFERENCES,0.1613603473227207,"w∗=
 
X⊤X + nαI
−1 XT Y."
REFERENCES,0.16208393632416787,"Thus the optimal training loss f(w∗) can be calculated accordingly. In all experiments, we use the
loss gap f(wT ) −f(w∗) as our performance metric."
REFERENCES,0.16280752532561504,"Experiments are conducted on a4a datasets (Chang & Lin, 2011; Dua & Graff, 2017) (https://
www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html#a4a/),
which contains 4, 781 samples and 123 features. This dataset is chosen majorly because it has a
moderate number of samples and features, which enables us to compute the exact Hessian matrix
H = 2(X⊤X/n + αI) and its corresponding eigenvalue distribution in acceptable time and space
consumption."
REFERENCES,0.16353111432706222,"In all of our experiments, we set α = 10−3. The model is optimized via SGD without momentum,
with batch size 1, initial learning rate η0 ∈{0.1, 0.06, 0.03, 0.02, 0.01, 0.006, 0.003, 0.002, 0.001,
0.0006, 0.0003, 0.0002, 0.0001} and learning rate of last iteration ηmin ∈{0.1, 0.01, 0.001, 0.0001,
0.00001, 0, “UNRESTRICTED”}. Here “UNRESTRICTED” denotes the case where ηmin is not
set, which is useful for eigencurve, who can decide the learning rate curve without setting ηmin.
Given η0 and ηmin, we adjust all schedulers as follows. For inverse time decay ηt = η0/(1 + γη0t)
and exponential decay ηt = γtη0, the hyperparameter γ is computed accordingly based on η0 and
ηmin. For cosine decay, η0 and ηmin is directly used, with no restart adopted. For eigencurve,
the learning rate curve is linearly scaled to match the given ηmin."
REFERENCES,0.16425470332850942,"In addition, for eigencurve, we use the eigenvalue distribution of the Hessian matrix, which is
directly computed via eigenvalue decomposition, as shown in Figure 6."
REFERENCES,0.1649782923299566,"Figure 6: The eigenvalue distribution of Hessian for ridge regression on a4a. Left: original scale for
eigenvalues. Right: log scale for eigenvalues. Notice that the density here is shown in log scale."
REFERENCES,0.16570188133140376,"All experimental results demonstrate that eigencurve can obtain similar or better training losses
when compared with other schedulers, as shown in Table 3."
REFERENCES,0.16642547033285093,Published as a conference paper at ICLR 2022
REFERENCES,0.1671490593342981,Table 3: Ridge regression: training loss gaps of different schedules over 5 trials.
REFERENCES,0.1678726483357453,Training loss - optimal training loss: f(wT ) −f(w∗)
REFERENCES,0.16859623733719248,"Schedule
#Epoch = 1
#Epoch = 5"
REFERENCES,0.16931982633863965,"Constant
0.014963±0.001369
0.004787±0.000175
Inverse Time Decay
0.007284±0.000190
0.002098±0.000160
Exponetial Decay
0.008351±0.000360
0.000931±0.000100
Cosine Decay
0.007767±0.000006
0.001167±0.000142
Eigencurve
0.006977±0.000197
0.000676±0.000069"
REFERENCES,0.17004341534008682,"Schedule
#Epoch = 25
#Epoch = 250"
REFERENCES,0.170767004341534,"Constant
0.001351±0.000179
0.000122±0.000009
Inverse Time Decay
0.000637±0.000143
0.000011±0.000001
Exponetial Decay
0.000048±0.000007
0.000000±0.000000
Cosine Decay
0.000054±0.000005
0.000000±0.000000
Eigencurve
0.000045±0.000008
0.000000±0.000000"
REFERENCES,0.1714905933429812,"A.2
EXACT VALUE OF THE EXTRA TERM ON CIFAR-10 EXPERIMENTS"
REFERENCES,0.17221418234442837,"In Section 4.1, we have already given the qualitative evidence that shows eigencurve ’s op-
timality for practical settings on CIFAR-10. Here we strengthen this argument by providing the
quantitative evidence as well. The exact value of the extra term is presented in Table 4, where we
assume CIFAR-10 has batch size 128, number of epochs 200 and weight decay 5 × 10−4, while
ImageNet has batch size 256, number of epochs 90 and weight decay 10−4."
REFERENCES,0.17293777134587554,"Table 4: Convergence rate of SGD with common schedulers given the estimated eigenvalue distri-
bution of Hessian, assuming the objective is quadratic."
REFERENCES,0.1736613603473227,Value of the extra term
REFERENCES,0.17438494934876989,Scheduler
REFERENCES,0.17510853835021709,"Convergence
rate of SGD in
quadratic
functions"
REFERENCES,0.17583212735166426,"CIFAR-10
+ ResNet18
CIFAR-10
+ GoogLeNet
CIFAR-10
+ VGG16
ImageNet
+ ResNet18"
REFERENCES,0.17655571635311143,"Inverse Time
Decay
Θ

dσ2 T
· κ"
REFERENCES,0.1772793053545586,"
3.39 × 105
4.92 × 105
6.50 × 105
6.80 × 106"
REFERENCES,0.17800289435600578,"Step Decay
Θ

dσ2"
REFERENCES,0.17872648335745298,"T
· log T"
REFERENCES,0.17945007235890015,"
16.25
16.25
16.25
18.78"
REFERENCES,0.18017366136034732,"Eigencurve O  dσ2 T
·"
REFERENCES,0.1808972503617945,"PImax−1
i=0
√si
2 d !"
REFERENCES,0.18162083936324167,"where Imax = log2 κ,
si = #λj ∈[µ · 2i, µ · 2i+1)"
REFERENCES,0.18234442836468887,"8.15
5.97
7.12
12.61"
REFERENCES,0.18306801736613604,"Minimax optimal rate
Ω

dσ2"
REFERENCES,0.1837916063675832,"T

1
1
1
1"
REFERENCES,0.18451519536903038,"It is worth noticing that the extra term’s value of eigencurve is independent from the number of
iterations T, since the value (PImax−1
i=0
√si)2/d only depends on the Hessian spectrum. So basically
eigencurve has already achieved the minimax optimal rate (up to a constant) for models and
datasets listed in Table 4, if the loss landscape around the optima can be approximated by quadratic
functions. For full details of the estimation process, please refer to Appendix B."
REFERENCES,0.18523878437047755,"A.3
REUSING EIGENCURVE FOR DIFFERENT MODELS ON CIFAR-10"
REFERENCES,0.18596237337192476,"For image classiﬁcation tasks on CIFAR-10, we check the performance of reusing ResNet-18’s
eigenvalue distribution for other models. As shown in Table 5, experimental results demonstrate that
Hessian’s eigenvalue distribution of Resnet-18 on CIFAR-10 can be applied to GoogLeNet/VGG16
and still achieves good peformance. Here the experiment settings are exactly the same as Section 4.2
in main paper."
REFERENCES,0.18668596237337193,Published as a conference paper at ICLR 2022
REFERENCES,0.1874095513748191,"Table 5: CIFAR-10: training losses and test accuracy of different schedules over 5 trials. Here
all eigencurve schedules are generated based on ResNet-18’s Hessian spectrums. “*” before a
number means at least one occurrence of loss explosion among all 5 trial experiments."
REFERENCES,0.18813314037626627,"#Epoch
Schedule
GoogLeNet
VGG16
Loss
Acc(%)
Loss
Acc(%) =10"
REFERENCES,0.18885672937771347,"Inverse Time Decay
2.61±0.00
86.54±0.94
2.26±0.00
84.47±0.74
Step Decay
2.59±0.02
87.04±0.48
2.42±0.45
82.98±0.27
General Step Decay
1.93±0.03
88.32±1.32
2.14±0.42
86.79±0.36
Cosine Decay
1.94±0.00
90.56±0.31
2.03±0.00
87.99±0.13
Eigencurve (transferred)
1.65±0.00
91.17±0.20
1.89±0.00
88.17±0.32 =100"
REFERENCES,0.18958031837916064,"Inverse Time Decay
0.62±0.02
92.05±0.69
1.32±0.62
*76.24±13.77
Step Decay
0.28±0.00
92.83±0.15
0.59±0.00
91.37±0.20
General Step Decay
0.13±0.00
94.18±0.18
0.20±0.00
*92.36±0.46
Cosine Decay
0.12±0.00
94.62±0.11
0.20±0.00
93.17±0.05
Eigencurve (transferred)
0.11±0.00
94.81±0.19
0.20±0.00
93.17±0.09"
REFERENCES,0.19030390738060782,"A.4
COMPARISON WITH EXPONENTIAL MOVING AVERAGE ON CIFAR-10"
REFERENCES,0.191027496382055,"Besides learning rate schedules, Exponential Moving Averaging (EMA) method"
REFERENCES,0.19175108538350216,"wt = α t
X"
REFERENCES,0.19247467438494936,"k=0
(1 −α)t−kwk
⇐⇒
wt = αwt + (1 −α)wt−1"
REFERENCES,0.19319826338639653,"is another competitive practical method that is commonly adopted in training neural networks with
SGD. Thus, it is natural to ask whether eigencurve can beat this method as well. The answer
is yes. In Table 6, we present additional experimental results on CIFAR-10 to compare the perfor-
mance of eigencurve and exponential moving averaging. It can be observed that there is a large
performance gap between those two methods."
REFERENCES,0.1939218523878437,"Table 6: CIFAR-10: training losses and test accuracy of Exponential Moving Average (EMA) and
eigencurve with #Epoch = 100 over 5 trials. For EMA, we search its constant learning rate
ηt = η0 ∈{1.0, 0.6, 0.3, 0.2, 0.1} and decay α ∈{0.9, 0.95, 0.99, 0.995, 0.999}. Other settings
remain the same as Section 4.2."
REFERENCES,0.19464544138929088,"Method/Schedule
ResNet-18
GoogLeNet
VGG16
Loss
Acc(%)
Loss
Acc(%)
Loss
Acc(%)"
REFERENCES,0.19536903039073805,"EMA
0.30±0.01
90.09±0.82
0.33±0.01
93.42±0.26
0.49±0.00
91.87±0.82
Eigencurve
0.14±0.00
94.05±0.18
0.12±0.00
94.75±0.15
0.18±0.00
92.88±0.24"
REFERENCES,0.19609261939218525,"A.5
IMAGENET CLASSIFICATION WITH ELASTIC STEP DECAY"
REFERENCES,0.19681620839363242,"One key observation in CIFAR-10 experiments is the existence of “power law” shown in Figure 3.
Also, notice that in the form of eigencurve , speciﬁcally Eqn. (3.5), iteration interval length ∆i
is proportional to the square root of eigenvalue density si in range [µ·2i, µ·2i+1). Combining those
two facts together, it suggests the length of “learning rate interval” should have lengths exponentially
decreasing."
REFERENCES,0.1975397973950796,"Based on this idea, Elastic Step Decay (ESD) is proposed, which has the following form,"
REFERENCES,0.19826338639652677,"ηt = η0/2k , if t ∈

(1 −rk)T, (1 −rk+1)T
"
REFERENCES,0.19898697539797394,"Compared to general step decay with adjustable interval lengths, elastic step decay does not require
manual adjustment for the length of each interval. Instead, they are all controlled by one hyper-
parameter r ∈(0, 1), which decides the “shrinking speed” of interval lengths. Experiments on"
REFERENCES,0.19971056439942114,Published as a conference paper at ICLR 2022
REFERENCES,0.20043415340086831,"CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority in practice, as shown in Table 7,
Table 8."
REFERENCES,0.2011577424023155,"For experiments on CIFAR-10/CIFAR-100, we adopt the same settings as eigencurve , except
we only use common step decay with three same-length intervals + decay factor 10."
REFERENCES,0.20188133140376266,"Table 7: Elastic Step Decay on CIFAR-10/CIFAR-100: test accuracy(%) of different schedules over
5 trials. “*” before a number means at least one occurrence of loss explosion among all 5 trial
experiments."
REFERENCES,0.20260492040520983,"#Epoch
Schedule
ResNet-18
GoogLeNet
VGG16
CIFAR-10
CIFAR-100
CIFAR-10
CIFAR-100
CIFAR-10
CIFAR-100 =10"
REFERENCES,0.20332850940665703,"Inverse Time Decay
79.45±1.00
48.73±1.66
86.54±0.94
57.90±1.27
84.47±0.74
50.04±0.83
Step Decay
79.67±0.74
54.54±0.26
88.37±0.13
63.05±0.35
85.18±0.06
45.86±0.31
Cosine Decay
84.23±0.07
61.26±1.11
90.56±0.31
69.09±0.27
87.99±0.13
55.42±0.28
ESD
85.38±0.38
64.17±0.57
91.23±0.33
70.46±0.41
88.67±0.21
57.23±0.39 =100"
REFERENCES,0.2040520984081042,"Inverse Time Decay
90.82±0.43
69.82±0.37
92.05±0.69
73.54±0.28
*76.24±13.77
67.70±0.49
Step Decay
93.68±0.07
73.13±0.12
94.13±0.32
76.80±0.16
92.62±0.15
70.02±0.41
Cosine Decay
94.04±0.21
74.65±0.41
94.62±0.11
78.13±0.54
93.17±0.05
72.47±0.28
ESD
94.06±0.11
74.76±0.33
94.65±0.11
78.23±0.20
93.25±0.12
72.50±0.26"
REFERENCES,0.20477568740955138,"For experiments on ImageNet, we use ResNet-50 trained via SGD without momentum, batch size
256 and weight decay wd = 10−4. Since no momentum is used, the initial learning rate is set to
η0 = 1.0 instead of η0 = 0.1. Two step decay baselines are adopted. “Step Decay [30-60]” is the
common choice that decays the learning rate 10 folds at the end of epoch 30 and epoch 60. “Step
Decay [30-60-80]” is another popular choice for the ImageNet setting (Goyal et al., 2018), which
further decays learning rate 10 folds at epoch 80. For cosine decay scheduler, the hyperparameter
ηmin is set to be 0. As for the dataset, we use the common ILSVRC 2012 dataset, which contains
1000 classes, around 1.2M images for training and 50,000 images for validation. For all experiments,
we search r ∈{1/2, 1/
√"
REFERENCES,0.20549927641099855,"2} for ESD, with other hyperparameter search and selection process being
the same as eigencurve ."
REFERENCES,0.20622286541244572,"Table 8: Elastic Step Decay on ImageNet-1k: Losses and validation accuracy of different schedul-
ings for ResNet-50 with #Epoch=90 over 3 trials."
REFERENCES,0.20694645441389292,"Schedule
Training loss
Top-1 validation
acc(%)
Top-5 validation
acc(%)"
REFERENCES,0.2076700434153401,#Epoch=90
REFERENCES,0.20839363241678727,"Step Decay [30-60]
1.4726±0.0057
75.55±0.13
92.63±0.08
Step Decay [30-60-80]
1.4738±0.0080
76.05±0.33
92.83±0.15
Cosine Decay
1.4697±0.0049
76.57±0.07
93.25±0.05
ESD (r = 1/
√"
REFERENCES,0.20911722141823444,"2)
1.4317±0.0027
76.79±0.10
93.31±0.05"
REFERENCES,0.2098408104196816,"A.6
LANGUAGE MODELING WITH ELASTIC STEP DECAY"
REFERENCES,0.2105643994211288,"More experiments on language modeling are conducted to further demonstrate Elastic Step Decay’s
superiority over other schedulers."
REFERENCES,0.21128798842257598,"For all experiments, we follow almost the same setting in Zaremba et al. (2015), where a large
regularized LSTM recurrent neural network (Hochreiter & Schmidhuber, 1997) is trained on Penn
Treebank (Marcus et al., 1993) for language modeling task. The Penn Treebank dataset has a training
set of 929k words, a validation set of 73k words and a test set of 82k words. SGD without momentum
is adopted for training, with batch size 20 and 35 unrolling steps in LSTM."
REFERENCES,0.21201157742402316,"Other details are exactly the same, except for the number of training epochs. In Zaremba et al.
(2015), it uses 55 epochs to train the large regularized LSTM, which is changed to 30 epochs in our
setting, since we found that the model starts overﬁtting after 30 epochs. We conducted hyperparam-
eter search for all schedules, as shown in Table 9."
REFERENCES,0.21273516642547033,Published as a conference paper at ICLR 2022
REFERENCES,0.2134587554269175,Table 9: Hyperparameter search for schedulers.
REFERENCES,0.2141823444283647,"Scheduler
Form
Hyperparameter choices"
REFERENCES,0.21490593342981187,"Inverse Time Decay
ηt =
η0
1+λ·η0·t"
REFERENCES,0.21562952243125905,"η0 ∈{100, 10−1, 10−2, 10−3},
and set λ, so that
ηmin ∈{10−2, 10−3, 10−4, 10−5, 10−6}"
REFERENCES,0.21635311143270622,"General Step Decay
ηt = η0 · γk,
if t ∈[k, k + 1) · T K"
REFERENCES,0.2170767004341534,"η0 = 1,
K ∈{3, 4, 5, ⌊log T ⌋}, ⌊log T ⌋+ 1},
γ ∈{ 1 2 , 1"
REFERENCES,0.2178002894356006,"5 ,
1
10 }"
REFERENCES,0.21852387843704776,"Cosine Decay
ηt = ηmin + 1"
REFERENCES,0.21924746743849494,"2 (η0 −ηmin)
 
1 + cos
  tπ"
REFERENCES,0.2199710564399421,"T

η0 ∈{100, 10−1, 10−2, 10−3},
ηmin ∈{10−2, 10−3, 10−4, 0}"
REFERENCES,0.22069464544138928,"Elastic Step Decay
ηt = η0/2k,
if t ∈
h
(1 −rk)T, (1 −rk+1)T

η0 = 1,
r ∈{2−1, 2−1/2, 2−1/3, 2−1/5, 2−1/20},"
REFERENCES,0.22141823444283648,"Baseline
ηt ="
REFERENCES,0.22214182344428365,"(
η0
for ﬁrst 14 epochs
η0
1.15k
for epoch k + 14
η0 = 1"
REFERENCES,0.22286541244573083,"Experimental results show that Elastic Step Decay signiﬁcantly outperforms other schedulers, as
shown in Table 10."
REFERENCES,0.223589001447178,Table 10: Scheduler performance on LSTM + Penn Treebank over 5 trials.
REFERENCES,0.22431259044862517,"Scheduler
Validation perplexity
Test perplexity"
REFERENCES,0.22503617945007237,"Inverse Time Decay
114.9±1.1
112.7±1.1
General Step Decay
82.4±0.1
79.1±0.2
Baseline (Zaremba et al., 2015)
82.2
78.4
Cosine Decay
82.4±0.4
78.5±0.4
Elastic Step Decay
81.1±0.2
77.4±0.3"
REFERENCES,0.22575976845151954,Published as a conference paper at ICLR 2022
REFERENCES,0.22648335745296672,"A.7
IMAGE CLASSIFICATION ON IMAGENET WITH COSINE-POWER SCHEDULING"
REFERENCES,0.2272069464544139,"Another key observation in CIFAR-10 experiments is that eigencurve ’s learning rate curve
shape changes in a ﬁxed tendency: more “concave” learning rate curves for less training epochs,
which inspire the cosine-power schedule in following form."
REFERENCES,0.22793053545586106,"Cosine-power : ηt = ηmin + (η0 −ηmin)
1"
REFERENCES,0.22865412445730826,"2(1 + cos(
t
tmax
π))
α"
REFERENCES,0.22937771345875543,"Results in Table 11 show the schedulings’ performance with α = 0.5/1/2, which are denoted as
√"
REFERENCES,0.2301013024602026,"Cosine/Cosine/Cosine2 respectively. Notice that the best scheduler gradually moves from small α
to larger α when the number of epochs increases. For #epoch=270, since the number of epochs is
large enough to make model converge, it is reasonable that the accuracy gap between all schedulers
is small."
REFERENCES,0.23082489146164978,"For experiments on ImageNet, we use ResNet-18 trained via SGD without momentum, batch size
256 and weight decay wd = 10−4. Since no momentum is used, the initial learning rate is set to
η0 = 1.0 instead of η0 = 0.1. The hyperparameters ηmin is set to be 0 for all cosine-power scheduler.
As for the dataset, we use the common ILSVRC 2012 dataset, which contains 1000 classes, around
1.2M images for training and 50,000 images for validation."
REFERENCES,0.23154848046309695,"Table 11: Cosine-power Decay on ImageNet: training losses and validation accuracy (%) of different
schedulings for ResNet-18 over 3 trials. Settings #Epoch≥90 only have 1 trial due to constraints of
resource and time."
REFERENCES,0.23227206946454415,"#Epoch
Schedule
Training
loss
Top-1
validation acc (%)
Top-5
validation acc (%) 1 √"
REFERENCES,0.23299565846599132,"Cosine
5.4085±0.0080
30.01±0.21
55.26±0.33
Cosine
5.4330±0.0106
26.43±0.31
50.85±0.43
Cosine2
5.4939±0.0157
21.81±0.21
44.53±0.09 5 √"
REFERENCES,0.2337192474674385,"Cosine
2.9515±0.0057
57.27±0.15
80.71±0.12
Cosine
2.8389±0.0061
55.67±0.08
79.46±0.16
Cosine2
2.9160±0.0099
52.75±0.20
77.11±0.08 30 √"
REFERENCES,0.23444283646888567,"Cosine
2.1739±0.0046
67.56±0.03
87.82±0.09
Cosine
2.0402±0.0031
67.97±0.10
88.12±0.03
Cosine2
2.0525±0.0032
67.41±0.05
87.70±0.10 90 √"
REFERENCES,0.23516642547033284,"Cosine
1.9056
69.85
89.46
Cosine
1.7676
70.46
89.75
Cosine2
1.7403
70.42
89.69 270 √"
REFERENCES,0.23589001447178004,"Cosine
1.7178
71.37
90.31
Cosine
1.5756
71.93
90.33
Cosine2
1.5250
71.69
90.37"
REFERENCES,0.2366136034732272,"Figure 7: Learning rate curve of three cosine-power schedulers. Top: original scale; Bottom: log
scale."
REFERENCES,0.23733719247467439,Published as a conference paper at ICLR 2022
REFERENCES,0.23806078147612156,"A.8
FULL FIGURES FOR EIGENCURVE EXPERIMENTS IN SECTION 4.2"
REFERENCES,0.23878437047756873,"Please refer to Figure 8, 9, 10, 11, 12 and 13."
REFERENCES,0.23950795947901593,"Figure 8: CIFAR-10 results for ResNet-18, with #Epoch = 10. Left: training losses. Right: test
accuracy."
REFERENCES,0.2402315484804631,"Figure 9: CIFAR-10 results for GoogLeNet, with #Epoch = 10. Left: training losses. Right: test
accuracy."
REFERENCES,0.24095513748191028,"Figure 10: CIFAR-10 results for VGG16, with #Epoch = 10. Left: training losses. Right: test
accuracy."
REFERENCES,0.24167872648335745,Published as a conference paper at ICLR 2022
REFERENCES,0.24240231548480462,"Figure 11: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses. Right: test
accuracy."
REFERENCES,0.24312590448625182,"Figure 12: CIFAR-10 results for GoogLeNet, with #Epoch = 100. Left: training losses. Right: test
accuracy."
REFERENCES,0.243849493487699,"Figure 13: CIFAR-10 results for VGG16, with #Epoch = 100. Left: training losses. Right: test
accuracy."
REFERENCES,0.24457308248914617,"B
DETAILED EXPERIMENTAL SETTINGS FOR IMAGE CLASSIFICATION ON
CIFAR-10/CIFAR-100"
REFERENCES,0.24529667149059334,"B.1
BASIC SETTINGS"
REFERENCES,0.2460202604920405,"As mentioned in the main paper, all models are trained with stochastic gradient descent (SGD),
no momentum, batch size 128 and weight decay wd = 0.0005. Furthermore, we perform a grid
search to choose the best hyperparameters of all schedulers, with a validation set created from 5, 000"
REFERENCES,0.2467438494934877,Published as a conference paper at ICLR 2022
REFERENCES,0.24746743849493488,"samples in the training set, i.e. one-tenth of the training set. The remaining 45, 000 samples are then
used for training the model. After obtaining hyperparameters with the best validation accuracy, we
train the model again with the full training set and test the trained model on test set, where 5 trials
of experiments are conducted. The mean and standard deviation of the test results are reported."
REFERENCES,0.24819102749638206,"Here the grid search explores hyperparameters η0
∈
{1.0, 0.6, 0.3, 0.2, 0.1} and ηmin
∈
{0.01, 0.001, 0.0001, 0, “UNRESTRICTED”}, where η0 denotes the initial learning rate and ηmin
stands for the learning rate of last iteration. “UNRESTRICTED” denotes the case where ηmin is
not set, which is useful for eigencurve, who can decide the learning rate curve without setting
ηmin. Given η0 and ηmin, we adjust all schedulers as follows. For inverse time decay, the hy-
perparameter γ is computed accordingly based on η0 and ηmin. For cosine decay, η0 and ηmin is
directly used, with no restart adopted. For general step decay, we search the interval number in
{3, 4, 5, ⌊log T⌋, ⌊log T⌋+ 1} and decay factor in {2, 5, 10}. For step decay proposed in Ge et al.
(2019), the interval number is ﬁxed to be ⌊log T⌋, along with a decay factor 2. For eigencurve,
two major modiﬁcations are made to make it more suitable for practical settings:"
REFERENCES,0.24891461649782923,"ηt =
1/L 1 + 1"
REFERENCES,0.2496382054992764,"κ
Pi−1
j=1 ∆j2j−1 + 2i−1"
REFERENCES,0.2503617945007236,"κ (t −ti−1)
=
η0
1 + 1"
REFERENCES,0.25108538350217074,"κ
Pi−1
j=1 ∆jβj−1 + βi−1"
REFERENCES,0.25180897250361794,"κ (t −ti−1)
."
REFERENCES,0.25253256150506515,"Here we change 1/L to η0 so that it is possible to adjust the initial learning rate of eigencurve.
We also change the ﬁxed constant 2 to a general constant β > 1, which is aimed at making the
learning rate curve smoother. The learning rate curve of eigencurve is then linearly scaled to
match the given ηmin."
REFERENCES,0.2532561505065123,"Notice that the learning rate η0 can be larger than 1/L, while the loss still does not explode. There
are several explanations for this phenomenon. First, in basic non-smooth analysis of GD and SGD
with inverse time decay scheduler, the learning rate can be larger than 1/L if the gradient norm is
bounded (Shamir & Zhang, 2012). Second, deep learning has a non-convex loss landscape, espe-
cially when the parameter is far away from the optima. Hence it is common to use larger learning
rate at ﬁrst. As long as the loss does not explode, it is okay. So we still include large learning rate
η0 in our grid search process."
REFERENCES,0.2539797395079595,"B.2
SETTINGS FOR EIGENCURVE"
REFERENCES,0.25470332850940663,"In addition, for our eigencurve scheduler, we use PyHessian (Yao et al., 2020) to generate Hes-
sian matrix’s eigenvalue distribution for all models. The whole process consists of three phases,
which are illustrated as follows."
REFERENCES,0.25542691751085383,"1) Training the model
Almost all CNN models on CIFAR-10 have non-convex objectives, thus
the Hessian’s eigenvalue distributions are different for different parameters. Normally, we want the
this distribution to reﬂect the overall tendency of most parts of the training process. According to
the phenomenon demonstrated in Appendix E, ﬁgure A.11-A.17 of Yao et al. (2020), the eigenvalue
distribution of ResNet’s Hessian presents similar tendency after training 30 epochs, which suggests
that the Hessian’s eigenvalue distribution can be used after sufﬁcient training."
REFERENCES,0.25615050651230103,"In all CIFAR-10 experiments, we use the Hessian’s eigenvalue distribution of models after train-
ing 180 epochs. Since the goal here is to sufﬁciently train the model, not to obtain good perfor-
mance, common baseline settings are adopted for training. For all models used for eigenvalue
distribution estimation, we adopt SGD with momentum = 0.9, batch size 128, weight decay
wd = 0.0005 and initial learning rate 0.1. On top of that, we use step decay, which decays the
learning rate by a factor of 10 at epoch 80 and 120. All of them are default settings of the PyHessian
code (https://github.com/amirgholami/PyHessian/blob/master/training.
py, commit: f4c3f77)."
REFERENCES,0.2568740955137482,"ImageNet adopts a similar setting, with training epochs being 90, SGD with momentum = 0.9,
batch size 256, weight decay wd = 0.0001, inital learning rate 0.1 and step decay schedule decays
learning rate by a factor of 10 at epoch 30 and 60."
REFERENCES,0.2575976845151954,"2) Estimating Hessian matrix’s eigenvalue distribution for the trained model
After obtaining
the checkpoint of a sufﬁciently trained model, we then run PyHessian to estimate the Hessian’s"
REFERENCES,0.2583212735166425,Published as a conference paper at ICLR 2022
REFERENCES,0.2590448625180897,"eigenvalue distribution for that checkpoint. The goal here is to obtain the Hessian’s eigenvalue
distribution with sufﬁcient precision. To be more speciﬁc, the length of intervals around each esti-
mated eigenvalue. PyHessian estimates the eigenvalue spectral density (ESD) of a model’s Hessian,
in other words, the output is a list of eigenvalue intervals, along with the density of each interval,
where the whole density adds up to 1. Precision means the interval length here."
REFERENCES,0.2597684515195369,"It is natural that the estimation precision is related to the complexity of the PyHessian algorithm, e.g.
the better precision it yields, the more time and space it consumes. More speciﬁcally, the algorithm
has a time complexity of O(Nn2
vd) and space complexity O(Bd + nvd), where d is the number of
model parameters, N is the number of samples used for estimating the ESD, B is the batch size and
nv is the iteration number of Stochastic Lanczos Quadrature used in PyHessian, which controls the
estimation precision (see Algorithm 1 of Yao et al. (2020))."
REFERENCES,0.26049204052098407,"In our experiments, we use nv = 5000 for ResNet-18 and nv = 3000 for GoogLeNet/VGG16,
which gives an eigenvalue distribution estimation with precision around 10−5 to 10−4. N and B are
both set to 200 due to GPU memory constraint, i.e. we use one mini-batch to estimate the eigenvalue
distribution. It turns out that this one-batch estimation is good enough and yields similar results to
full dataset settings shown in Yao et al. (2020)."
REFERENCES,0.26121562952243127,"However, space complexity is still a bottleneck here. Due to the large number of nv and space
complexity O(Bd + nvd) of PyHessian, the value of d cannot be very large. In practice, with a
NVIDIA GeForce 2080 Ti GPU, which has around 11GB memory, the maximum acceptable pa-
rameter number d is around 200K −400K. This implies that the model has to be compressed. In
our experiments, we reduce the number of channels by a factor of C for all models. For ResNet-18,
C = 16. For GoogLeNet, C = 4. For VGG16, C = 8. Notice that those compressed models are
only used for eigenvalue distribution estimation. In experiments of comparing different scheduling,
we still use the original model with no compression."
REFERENCES,0.2619392185238784,"One
may
refer
to
https://github.com/opensource12345678/why_cosine_
works/tree/main/eigenvalue_distribution for generated eigenvalue distributions."
REFERENCES,0.2626628075253256,"3) Generating eigencurve scheduler with the estimated eigenvalue distribution
After ob-
taining the eigenvalue distribution, we do a preprocessing before plug it into our eigencurve
scheduler."
REFERENCES,0.2633863965267728,"First, we notice that there are negative eigenvalues in the ﬁnal distribution. Theoretically, if the
parameter is right at the optimal point, no negative eigenvalues should exist for Hessian matrix.
Thus we conjecture that those negative eigenvalues are caused by the fact that the model is closed to
optima w∗, but not exactly at that point. Furthermore, the estimation precision loss can be another
cause. In fact, most of those negative eigenvalues are small, e.g. 98.6% of those negative eigenvalues
lie in [−0.1, 0), and can be generally ignored without much loss. In our case, we set them to their
absolute values."
REFERENCES,0.26410998552821996,"Second, for a given weight decay value wd, we need to take the implicit L2 regularization into
account, since it affects the Hessian matrix as well. Therefore, for all eigenvalues after the ﬁrst step,
we add wd to them."
REFERENCES,0.26483357452966716,"After preprocessing, we plug the eigenvalue distribution into our eigencurve scheduler and gen-
erates the exact form of eigencurve."
REFERENCES,0.2655571635311143,"ηt =
1/L 1 + 1"
REFERENCES,0.2662807525325615,"κ
Pi−1
j=1 ∆j2j−1 + 2i−1"
REFERENCES,0.2670043415340087,"κ (t −ti−1)
=
η0
1 + 1"
REFERENCES,0.26772793053545585,"κ
Pi−1
j=1 ∆jβj−1 + βi−1"
REFERENCES,0.26845151953690305,κ (t −ti−1)
REFERENCES,0.2691751085383502,"For experiments with 100 epochs, we set β = 1.000005, so that the learning rate curve is much
smoother. For experiments with 10 epochs, we set β = 2.0. In our experiments, β serves as a ﬁxed
constant, not hyperparameters. So no hyperparameter search is conducted on β. One can do that in
practice though, if computation resource allows."
REFERENCES,0.2698986975397974,Published as a conference paper at ICLR 2022
REFERENCES,0.2706222865412446,"B.3
COMPUTE RESOURCE AND IMPLEMENTATION DETAILS"
REFERENCES,0.27134587554269174,"All
the
code
for
results
in
main
paper
can
be
found
in
https://github.com/
opensource12345678/why_cosine_works/tree/main, which is released under the
MIT license."
REFERENCES,0.27206946454413894,"All experiments on CIFAR-10/CIFAR-100 are conducted on a single NVIDIA GeForce 2080
Ti GPU, where ResNet-18/GoogLeNet/VGG16 takes around 20mins/90mins/40mins to train 100
epochs, respectively. High-precision eigenvalue distribution estimation, e.g. nv ≥3000, requires
around 1-2 days to complete, but this is no longer necessary given the released results."
REFERENCES,0.2727930535455861,"The ResNet-18 model is implemented in Tensorﬂow 2.0. We use tensorﬂow-gpu 2.3.1 in our code.
The GoogLeNet and VGG16 model is implemented in Pytorch, speciﬁcally, 1.7.0+cu101."
REFERENCES,0.2735166425470333,"B.4
LICENSE OF PYHESSIAN"
REFERENCES,0.2742402315484805,"According
to
https://github.com/amirgholami/PyHessian/blob/master/
LICENSE, PyHessian (Yao et al., 2020) is released under the MIT License."
REFERENCES,0.27496382054992763,"C
DETAILED EXPERIMENTAL SETTINGS FOR IMAGE CLASSIFICATION ON
IMAGENET"
REFERENCES,0.27568740955137483,"One may refer to https://www.image-net.org/download for speciﬁc terms of access for
ImageNet. The dataset can be downloaded from https://image-net.org/challenges/
LSVRC/2012/2012-downloads.php, with training set being “Training images (Task 1 & 2)”
and validation set being “Validation images (all tasks)”. Notice that registration and veriﬁcation of
institute is required for successful download."
REFERENCES,0.276410998552822,"ResNet-18 experiments on ImageNet are conducted on two NVIDIA GeForce 2080 Ti GPUs with
data parallelism, while ResNet-50 experiments are conducted on 4 GPUs in a similar fashion. Both
models take around 2 days to train 90 epochs, about 20mins-30mins per epoch. Those ResNet
models on ImageNet are implemented in Pytorch, speciﬁcally, 1.7.0+cu101."
REFERENCES,0.2771345875542692,"D
IMPORTANT PROPOSITIONS AND LEMMAS"
REFERENCES,0.2778581765557164,"Proposition 3. Letting f(x) be a monotonically increasing function in the range [t0, ˜t], then it holds
that"
REFERENCES,0.2785817655571635,"˜t−1
X"
REFERENCES,0.2793053545586107,"k=t0
f(k) ≤
Z ˜t"
REFERENCES,0.28002894356005786,"t0
f(x) dx.
(D.1)"
REFERENCES,0.28075253256150506,"If f(x) is monotonically decreasing in the range [t0, ˜t], then it holds that Z ˜t"
REFERENCES,0.28147612156295226,"t0
f(x) dx ≤"
REFERENCES,0.2821997105643994,"˜t−1
X"
REFERENCES,0.2829232995658466,"k=t0
f(k) ≤ ˜t
X"
REFERENCES,0.28364688856729375,"k=t0
f(k) ≤f(t0) +
Z ˜t"
REFERENCES,0.28437047756874095,"t0
f(x) dx.
(D.2)"
REFERENCES,0.28509406657018815,"Lemma 2. Function f(x) = exp(−αx)x2 with 0 < α and x ∈(0, 1] is monotone decreasing in
the range x ∈( 2"
REFERENCES,0.2858176555716353,"α, +∞) and monotone increasing in the range x ∈[0, 2 α]."
REFERENCES,0.2865412445730825,Proof. We can obtain the derivative of f(x) as
REFERENCES,0.28726483357452964,∇f(x) = x exp(−αx)(2 −αx).
REFERENCES,0.28798842257597684,"Thus, it holds that ∇f(x) ≥0 when x ∈[0, 2"
REFERENCES,0.28871201157742404,"α]. This implies that f(x) is monotone increasing when
x ∈[0, 2"
REFERENCES,0.2894356005788712,"α]. Similarly, we can obtain that f(x) is monotone decreasing when x ∈( 2"
REFERENCES,0.2901591895803184,"α, +∞)."
REFERENCES,0.29088277858176553,"Lemma 3. It holds that
Z
exp(−αx)x dx = −α−1(x exp(−αx) + α−1 exp(−αx)).
(D.3)"
REFERENCES,0.29160636758321273,Published as a conference paper at ICLR 2022
REFERENCES,0.29232995658465993,Proof.
REFERENCES,0.2930535455861071,"Z
exp(−αx)x dx = −α−1
Z
x d exp(−αx) = −α−1(x exp(−αx) −
Z
exp(−αx)dx)"
REFERENCES,0.2937771345875543,= −α−1(x exp(−αx) + α−1 exp(−αx)).
REFERENCES,0.2945007235890014,"E
PROOF OF SECTION 2"
REFERENCES,0.2952243125904486,"Proof of Proposition 1. By iteratively applying Eqn. (2.1), we can obtain that"
REFERENCES,0.2959479015918958,"E
h
λj (wt+1,j −w∗,j)2i (2.1)
≤ tY"
REFERENCES,0.29667149059334297,"i=1
(1 −ηi,jλj)2 · λj(w1,j −w∗,j)2"
REFERENCES,0.29739507959479017,"+ λ2
jσ2 · t
X k=1 tY"
REFERENCES,0.2981186685962373,"i=k+1
(1 −ηi,jλj)2η2
k,j"
REFERENCES,0.2988422575976845,"=λj(w1,j −w∗,j)2"
REFERENCES,0.2995658465991317,"(t + 1)2
+ σ2 · t
X k=1"
REFERENCES,0.30028943560057886,(k + 1)2
REFERENCES,0.30101302460202606,"(t + 1)2 ·
1
(k + 1)2"
REFERENCES,0.3017366136034732,"=λj(w1,j −w∗,j)2"
REFERENCES,0.3024602026049204,"(t + 1)2
+
t
(t + 1)2 · σ2."
REFERENCES,0.3031837916063676,"Summing up each coordinate, we can obtain the result."
REFERENCES,0.30390738060781475,"Proof of Proposition 2. Let us denote σ2
j = λ2
jσ2. By Eqn. (2.1), we can obtain that"
REFERENCES,0.30463096960926195,"E
h
λj (wt+1,j −w∗,j)2i"
REFERENCES,0.3053545586107091,"≤Πt
i=1(1 −ηi,jλj)2 · λj(w1,j −w∗,j)2 + t
X"
REFERENCES,0.3060781476121563,"k=1
Πi=k(1 −ηi,jλj)η2
k,jσ2
j ≤exp  −2 t
X"
REFERENCES,0.3068017366136035,"i=1
ηi,jλj !"
REFERENCES,0.30752532561505064,"· λj(w1,j −w∗,j)2 + t
X"
REFERENCES,0.30824891461649784,"k=1
exp  −2 t
X"
REFERENCES,0.308972503617945,"i=k
ηi,jλj !"
REFERENCES,0.3096960926193922,"η2
k,jσ2
j = exp  −2λj t
X i=1"
REFERENCES,0.3104196816208394,"1
L + µi !"
REFERENCES,0.3111432706222865,"· λj(w1,j −w∗,j)2 + t
X"
REFERENCES,0.3118668596237337,"k=1
exp  −2λj t
X i=k"
REFERENCES,0.3125904486251809,"1
L + µi !"
REFERENCES,0.3133140376266281,"η2
k,jσ2
j"
REFERENCES,0.3140376266280753,"≤exp
2λj"
REFERENCES,0.3147612156295224,"µ ln
 L + µ"
REFERENCES,0.3154848046309696,L + µt
REFERENCES,0.31620839363241676,"
· λj(w1,j −w∗,j)2 + t
X"
REFERENCES,0.31693198263386396,"k=1
exp
2λj"
REFERENCES,0.31765557163531116,"µ ln
L + µk"
REFERENCES,0.3183791606367583,L + µt
REFERENCES,0.3191027496382055,"
·
σ2
j
(L + µk)2"
REFERENCES,0.31982633863965265,"=
 L + µ"
REFERENCES,0.32054992764109985,L + µt  2λj
REFERENCES,0.32127351664254705,"µ
· λj(w1,j −w∗,j)2 + t
X k=1"
REFERENCES,0.3219971056439942,(L + µk)  2λj
REFERENCES,0.3227206946454414,"µ −2
"
REFERENCES,0.32344428364688854,(L + µt) 2λj
REFERENCES,0.32416787264833574,"µ
· σ2
j"
REFERENCES,0.32489146164978294,"≤
 L + µ"
REFERENCES,0.3256150506512301,L + µt  2λj
REFERENCES,0.3263386396526773,"µ
· λj(w1,j −w∗,j)2 +
σ2
j
2λj −µ
(L + µt) 2λj µ −1"
REFERENCES,0.32706222865412443,(L + µt) 2λj
REFERENCES,0.32778581765557163,"µ
+
σ2
j
(L + µt)2"
REFERENCES,0.32850940665701883,"=
 L + µ"
REFERENCES,0.329232995658466,L + µt  2λj
REFERENCES,0.3299565846599132,"µ
· λj(w1,j −w∗,j)2 +
1
2λj −µ ·
1
L + µt · σ2
j +
σ2
j
(L + µt)2 ."
REFERENCES,0.3306801736613603,"The third inequality is because function F(x) = 1/(L + µx) is monotone decreasing in the range
[1, ∞), and it holds that t
X i=k"
REFERENCES,0.3314037626628075,"1
L + µi ≥
Z t i=k"
REFERENCES,0.3321273516642547,"1
L + µi di = 1"
REFERENCES,0.33285094066570187,"µ ln
 L + µt"
REFERENCES,0.33357452966714907,"L + µk 
."
REFERENCES,0.3342981186685962,Published as a conference paper at ICLR 2022
REFERENCES,0.3350217076700434,"The last inequality is because function F(x) = (L + µx)2λj/µ−2 is monotone increasing in the
range [0, ∞), and it holds that t
X"
REFERENCES,0.3357452966714906,"k=1
(L + µk) 2λj"
REFERENCES,0.33646888567293776,"µ −2 ≤
Z t"
REFERENCES,0.33719247467438496,"k=1
(L + µk) 2λj"
REFERENCES,0.3379160636758321,"µ −2dk + (L + µt) 2λj µ −2 =
1"
REFERENCES,0.3386396526772793,"µ

2λj"
REFERENCES,0.3393632416787265,"µ −1
 ·

(L + µt) 2λj"
REFERENCES,0.34008683068017365,µ −1 −(L + µ) 2λj
REFERENCES,0.34081041968162085,"µ −1

+ (L + µt) 2λj µ −2"
REFERENCES,0.341534008683068,"<
1
2λj −µ(L + µt) 2λj"
REFERENCES,0.3422575976845152,µ −1 + (L + µt) 2λj µ −2.
REFERENCES,0.3429811866859624,"By µ ≤λj, σ2
j = λ2
jσ2, and summing up from i = 1 to d, we can obtain the result."
REFERENCES,0.34370477568740954,"F
PRELIMINARIES"
REFERENCES,0.34442836468885674,"Lemma 4. Let objective function f(x) be quadratic. Running SGD for T-steps starting from w0
and a learning rate sequence {ηt}T
t=1, the ﬁnal iterate wT +1 satisﬁes"
REFERENCES,0.3451519536903039,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)
"
REFERENCES,0.3458755426917511,"=E

(w0 −w∗)⊤· PT . . . P0HP0 . . . PT · (w0 −w∗)
 + T
X"
REFERENCES,0.3465991316931983,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ

, (F.1)"
REFERENCES,0.3473227206946454,where Pt = I −ηtH.
REFERENCES,0.3480463096960926,"Proof. Reformulating Eqn. (1.5), we have"
REFERENCES,0.34876989869753977,"wt+1 −w∗=wt −w∗−ηt(H(ξ)wt −b(ξ))
=wt −w∗−ηt(Hwt −b) + ηt (Hwt −b −(H(ξ)wt −b(ξ)))
=wt −w∗−ηt(Hwt −b −(Hw∗−b)) + ηt (Hwt −b −(H(ξ)wt −b(ξ)))
= (I −ηtH) (wt −w∗) + ηtnt
=Pt(wt −w∗) + ηtnt."
REFERENCES,0.34949348769898697,"Thus, we can obtain that"
REFERENCES,0.35021707670043417,"wt+1 −w∗= Pt . . . P0(w0 −w∗) + t
X"
REFERENCES,0.3509406657018813,"τ=0
Pt . . . Pτ+1ητnτ.
(F.2)"
REFERENCES,0.3516642547033285,"We can decompose above stochastic process associated with SGD’s update into two simpler pro-
cesses as follows:"
REFERENCES,0.35238784370477566,"wb
t+1 −w∗= Pt(wb
t −w∗),
and
wv
t+1 −w∗= Pt(wv
t −w∗) + ηtnt, with wv
0 = w∗,
(F.3)"
REFERENCES,0.35311143270622286,which entails that
REFERENCES,0.35383502170767006,"wb
t+1 −w∗= Pt . . . P0(wb
0 −w∗) = P0 . . . Pt(wb
0 −w∗)
(F.4)"
REFERENCES,0.3545586107091172,"wv
t+1 −w∗= t
X"
REFERENCES,0.3552821997105644,"τ=0
Pt . . . Pτ+1ητnτ = t
X"
REFERENCES,0.35600578871201155,"τ=0
Pτ+1 . . . Ptητnτ
(F.5)"
REFERENCES,0.35672937771345875,"(F.2)
⇒wt+1 −w∗=
 
wb
t+1 −w∗

+
 
wv
t+1 −w∗

(F.6)"
REFERENCES,0.35745296671490595,"where the last equality in Eqn. (F.4) and Eqn. (F.5) is because the commutative property PtPt′ =
(I −ηtH)(I −ηt′H) = (I −ηt′H)(I −ηtH) = Pt′Pt holds for ∀t, t′."
REFERENCES,0.3581765557163531,Published as a conference paper at ICLR 2022
REFERENCES,0.3589001447178003,"Thus, we have"
REFERENCES,0.35962373371924744,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)
"
REFERENCES,0.36034732272069464,"(F.6)
= E

(wb
T +1 −w∗)⊤H(wb
T +1 −w∗) + 2(wv
T +1 −w∗)⊤H(wb
T +1 −w∗)"
REFERENCES,0.36107091172214184,"+(wv
T +1 −w∗)⊤H(wv
T +1 −w∗)
"
REFERENCES,0.361794500723589,"(F.4)
= E

(wb
0 −w∗)⊤PT . . . P0HP0 . . . PT (wb
0 −w∗) + 2(wv
T +1 −w∗)⊤HP0 . . . PT (wb
0 −w∗)"
REFERENCES,0.3625180897250362,"+(wv
T +1 −w∗)⊤H(wv
T +1 −w∗)
"
REFERENCES,0.36324167872648333,"(F.5)
= E

(wb
0 −w∗)⊤PT . . . P0HP0 . . . PT (wb
0 −w∗) + 2 T
X"
REFERENCES,0.36396526772793053,"τ=0
PT . . . Pτ+1ητnτ !⊤"
REFERENCES,0.36468885672937773,"HP0 . . . PT (wb
0 −w∗) + T
X"
REFERENCES,0.3654124457308249,"τ=0
PT . . . Pτ+1ητnτ !⊤ H T
X"
REFERENCES,0.3661360347322721,"τ=0
PT . . . Pτ+1ητnτ ! "
REFERENCES,0.3668596237337192,"E[nτ ]=0
=
E

(wb
0 −w∗)⊤PT . . . P0HP0 . . . PT (wb
0 −w∗)
 + E  
T
X"
REFERENCES,0.3675832127351664,"τ=0,τ ′=0
ητητ ′ · n⊤
τ PT . . . Pτ+1 · H · Pτ ′+1 . . . PT nτ ′  "
REFERENCES,0.3683068017366136,"=E

(wb
0 −w∗)⊤PT . . . P0HP0 . . . PT (wb
0 −w∗)
 + T
X"
REFERENCES,0.36903039073806077,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ

,"
REFERENCES,0.36975397973950797,"where the last equality is because when τ and τ ′ are different, it holds that"
REFERENCES,0.3704775687409551,"E[n⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ ′] = 0"
REFERENCES,0.3712011577424023,due to independence between nτ and nτ ′.
REFERENCES,0.3719247467438495,"Lemma 5. Given the assumption that Eξ

ntn⊤
t

⪯σ2H, then the variance term satisﬁes that T
X"
REFERENCES,0.37264833574529665,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ

≤σ2
d
X"
REFERENCES,0.37337192474674386,"j=1
λ2
j T
X"
REFERENCES,0.374095513748191,"k=0
η2
k T
Y"
REFERENCES,0.3748191027496382,"i=k+1
(1 −ηiλj)2 ,
(F.7)"
REFERENCES,0.3755426917510854,where Pt = I −ηtH.
REFERENCES,0.37626628075253254,"Proof. Denote Aτ ≜PT . . . Pτ+1H
1
2 , then"
REFERENCES,0.37698986975397974,"A⊤
τ =

PT . . . Pτ+1H
1
2
⊤
=

H
1
2
⊤
P ⊤
τ+1 . . . P ⊤
T = H
1
2 Pτ+1 . . . PT ,
(F.8)"
REFERENCES,0.37771345875542695,"where the second equality is entailed by the fact that H
1
2 , Pτ+1, . . . , PT are symmetric matrices."
REFERENCES,0.3784370477568741,Published as a conference paper at ICLR 2022
REFERENCES,0.3791606367583213,"Therefore, we have, T
X"
REFERENCES,0.37988422575976843,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ
"
REFERENCES,0.38060781476121563,"(F.8)
= T
X"
REFERENCES,0.38133140376266283,"τ=0
E

η2
τn⊤
τ AτA⊤
τ nτ

= T
X"
REFERENCES,0.38205499276411,"τ=0
η2
τE

tr
 
n⊤
τ AτA⊤
τ nτ

= T
X"
REFERENCES,0.3827785817655572,"τ=0
η2
τE

tr
 
A⊤
τ nτn⊤
τ Aτ
 = T
X"
REFERENCES,0.3835021707670043,"τ=0
η2
τtr
 
E

A⊤
τ nτn⊤
τ Aτ

= T
X"
REFERENCES,0.3842257597684515,"τ=0
η2
τtr
 
A⊤
τ E

nτn⊤
τ

Aτ
 ≤σ2 · T
X"
REFERENCES,0.3849493487698987,"τ=0
η2
τtr
 
A⊤
τ HAτ

= σ2 · T
X"
REFERENCES,0.38567293777134587,"τ=0
η2
τtr
 
AτA⊤
τ H
 =σ2 · T
X"
REFERENCES,0.38639652677279307,"τ=0
η2
τtr (PT . . . Pτ+1HPτ+1 . . . PT H)"
REFERENCES,0.3871201157742402,"=σ2
d
X"
REFERENCES,0.3878437047756874,"j=1
λ2
j T
X"
REFERENCES,0.3885672937771346,"k=0
η2
k T
Y"
REFERENCES,0.38929088277858176,"i=k+1
(1 −ηiλj)2 ,"
REFERENCES,0.39001447178002896,"where the third and sixth equality come from the cyclic property of trace, while the ﬁrst inequality
is because of the condition Eξ

ntn⊤
t

⪯σ2H, where"
REFERENCES,0.3907380607814761,"∀x,
x⊤E[nτn⊤
τ ]x ≤σ2x⊤Hx"
REFERENCES,0.3914616497829233,"⇒
∀z,
z⊤A⊤
τ E[nτn⊤
τ ]Aτz = (Aτz)⊤E[nτn⊤
τ ](Aτz) ≤σ2(Aτz)⊤H(Aτz) = σ2z⊤A⊤
τ HAτz"
REFERENCES,0.3921852387843705,"⇒
A⊤
τ E[nτn⊤
τ ]Aτ ⪯σ2A⊤
τ HAτ
⇒
tr
 
A⊤
τ E[nτn⊤
τ ]Aτ

≤σ2tr
 
A⊤
τ HAτ

."
REFERENCES,0.39290882778581765,"Lemma 6. Letting λ˜j be the smallest positive eigenvalue of H, then the bias term satisﬁes that"
REFERENCES,0.39363241678726485,"E

(w0 −w∗)⊤· PT . . . P0HP0 . . . PT · (w0 −w∗)
"
REFERENCES,0.394356005788712,"≤(w0 −w∗)⊤H(w0 −w∗) · exp  −2λ˜j T
X"
REFERENCES,0.3950795947901592,"k=0
ηk !"
REFERENCES,0.3958031837916064,".
(F.9)"
REFERENCES,0.39652677279305354,"Proof. Letting H = UΛU ⊤be the spectral decomposition of H and uj be j-th column of U, we
can obtain that"
REFERENCES,0.39725036179450074,"E

(w0 −w∗)⊤· PT . . . P0HP0 . . . PT · (w0 −w∗)
 = d
X"
REFERENCES,0.3979739507959479,"j=1
λj · (u⊤
j (w0 −w∗))2 · T
Y"
REFERENCES,0.3986975397973951,"k=0
(1 −ηkλj)2 ≤ d
X"
REFERENCES,0.3994211287988423,"j=1
λj · (u⊤
j (w0 −w∗))2 · exp  −2λj T
X"
REFERENCES,0.40014471780028943,"k=0
ηk ! ."
REFERENCES,0.40086830680173663,"Since λ˜j is the smallest positive eigenvalue of H, it holds that d
X"
REFERENCES,0.4015918958031838,"j=1
λj · (u⊤
j (w0 −w∗))2 · exp  −2λj T
X"
REFERENCES,0.402315484804631,"k=0
ηk ! ≤ d
X"
REFERENCES,0.4030390738060782,"j=1
λj · (u⊤
j (w0 −w∗))2 · exp  −2λ˜j T
X"
REFERENCES,0.4037626628075253,"k=0
ηk !"
REFERENCES,0.4044862518089725,"=(w0 −w∗)⊤H(w0 −w∗) · exp  −2λ˜j T
X"
REFERENCES,0.40520984081041966,"k=0
ηk ! ."
REFERENCES,0.40593342981186686,Published as a conference paper at ICLR 2022
REFERENCES,0.40665701881331406,"G
PROOF OF THEOREMS"
REFERENCES,0.4073806078147612,"Lemma 7. Let learning rate ηt is deﬁned in Eqn. (3.3). Assuming k ∈[ti′−1, ti′] with 1 ≤i′ ≤˜i ≤
T, the sequence {ηt}T
t=0 satisﬁes that"
REFERENCES,0.4081041968162084,"t˜i+1−1
X"
REFERENCES,0.40882778581765555,"t=k
ηt ≥"
REFERENCES,0.40955137481910275,"˜i+1
X"
REFERENCES,0.41027496382054995,i=i′+1
REFERENCES,0.4109985528219971,"1
2i−1µ ln αi"
REFERENCES,0.4117221418234443,"αi−1
+
1
2i′−1µ ln
αi′
αi′−1 + 2i′−1µ(k −ti′−1),
(G.1)"
REFERENCES,0.41244573082489144,where αi is deﬁned as
REFERENCES,0.41316931982633864,"αi ≜L + µ i
X"
REFERENCES,0.41389290882778584,"j=1
∆j2j−1 = 1"
REFERENCES,0.414616497829233,"ηti
.
(G.2)"
REFERENCES,0.4153400868306802,"Proof. First, we divide learning rates into two groups: those who are guaranteed to cover a full
interval and those who may not."
REFERENCES,0.41606367583212733,"t˜i+1−1
X"
REFERENCES,0.41678726483357453,"t=k
ηt ="
REFERENCES,0.41751085383502173,"t˜i+1−1
X"
REFERENCES,0.4182344428364689,"t=ti′
ηt +"
REFERENCES,0.4189580318379161,"ti′−1
X"
REFERENCES,0.4196816208393632,"t=k
ηt ="
REFERENCES,0.4204052098408104,"˜i+1
X"
REFERENCES,0.4211287988422576,i=i′+1
REFERENCES,0.42185238784370477,"ti−1
X"
REFERENCES,0.42257597684515197,"t=ti−1
ηt +"
REFERENCES,0.4232995658465991,"ti′−1
X"
REFERENCES,0.4240231548480463,"t=k
ηt"
REFERENCES,0.4247467438494935,"Furthermore, because ηt is monotonically decreasing with respect to t, by Proposition 3, we have"
REFERENCES,0.42547033285094066,"t˜i+1−1
X"
REFERENCES,0.42619392185238786,"t=k
ηt
(D.2)
≥"
REFERENCES,0.426917510853835,"˜i+1
X"
REFERENCES,0.4276410998552822,i=i′+1 Z ti
REFERENCES,0.4283646888567294,"ti−1
ηtdt +
Z ti′"
REFERENCES,0.42908827785817655,"k
ηtdt"
REFERENCES,0.42981186685962375,"(3.3)
="
REFERENCES,0.4305354558610709,"˜i+1
X"
REFERENCES,0.4312590448625181,i=i′+1 Z ti ti−1 1
REFERENCES,0.4319826338639653,"L + µ Pi−1
j=1 ∆j2j−1 + 2i−1µ(t −ti−1)
dt"
REFERENCES,0.43270622286541244,"+
Z ti′ k 1"
REFERENCES,0.43342981186685964,"L + µ Pi′−1
j=1 ∆j2j−1 + 2i′−1µ(t −ti′−1)
dt ="
REFERENCES,0.4341534008683068,"˜i+1
X"
REFERENCES,0.434876989869754,i=i′+1
REFERENCES,0.4356005788712012,"1
2i−1µ ln
L + µ Pi−1
j=1 ∆j2j−1 + 2i−1µ(ti −ti−1)"
REFERENCES,0.4363241678726483,"L + µ Pi−1
j=1 ∆j2j−1"
REFERENCES,0.4370477568740955,"+
1
2i′−1µ ln
L + µ Pi′−1
j=1 ∆j2j−1 + 2i′−1µ(ti′ −ti′−1)"
REFERENCES,0.4377713458755427,"L + µ Pi′−1
j=1 ∆j2j−1 + 2i′−1µ(k −ti′−1) ="
REFERENCES,0.4384949348769899,"˜i+1
X"
REFERENCES,0.4392185238784371,i=i′+1
REFERENCES,0.4399421128798842,"1
2i−1µ ln
L + µ Pi
j=1 ∆j2j−1"
REFERENCES,0.4406657018813314,"L + µ Pi−1
j=1 ∆j2j−1"
REFERENCES,0.44138929088277856,"+
1
2i′−1µ ln
L + µ Pi′"
REFERENCES,0.44211287988422576,j=1 ∆j2j−1
REFERENCES,0.44283646888567296,"L + µ Pi′−1
j=1 ∆j2j−1 + 2i′−1µ(k −ti′−1) ="
REFERENCES,0.4435600578871201,"˜i+1
X"
REFERENCES,0.4442836468885673,i=i′+1
REFERENCES,0.44500723589001445,"1
2i−1µ ln αi"
REFERENCES,0.44573082489146165,"αi−1
+
1
2i′−1µ ln
αi′
αi′−1 + 2i′−1µ(k −ti′−1)."
REFERENCES,0.44645441389290885,"Lemma 8. Letting sequence {αi} be deﬁned in Eqn. (G.2), given 1 ≤˜i, it holds that"
REFERENCES,0.447178002894356,"˜i+1
X i=1  "
REFERENCES,0.4479015918958032,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.44862518089725034,"2
˜i−j+2"
REFERENCES,0.44934876989869754,"α−2
˜i−i+2
i"
REFERENCES,0.45007235890014474,"ti−1
X"
REFERENCES,0.4507959479015919,"k=ti−1
(αi−1 + 2i−1µ(k −ti−1))2
˜i−i+2−2"
REFERENCES,0.4515195369030391,"≤2 ·
1
2˜i+1µ
·"
REFERENCES,0.45224312590448623,"˜i+1
X i=1  "
REFERENCES,0.45296671490593343,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.45369030390738063,"2
˜i−j+2"
REFERENCES,0.4544138929088278,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i
. (G.3)"
REFERENCES,0.455137481910275,Published as a conference paper at ICLR 2022
REFERENCES,0.4558610709117221,"Proof. Notice that g(k) := (αi−1 + 2i−1µ(k −ti−1))2
˜i−i+2−2 is a monotonically increasing func-
tion, we have,"
REFERENCES,0.4565846599131693,"˜i+1
X i=1  "
REFERENCES,0.4573082489146165,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.45803183791606367,"2
˜i−j+2"
REFERENCES,0.45875542691751087,"α−2
˜i−i+2
i"
REFERENCES,0.459479015918958,"ti−1
X"
REFERENCES,0.4602026049204052,"k=ti−1
(αi−1 + 2i−1µ(k −ti−1))2
˜i−i+2−2"
REFERENCES,0.4609261939218524,"(D.1)
≤"
REFERENCES,0.46164978292329956,"˜i+1
X i=1  "
REFERENCES,0.46237337192474676,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.4630969609261939,"2
˜i−j+2"
REFERENCES,0.4638205499276411,"α−2
˜i−i+2
i Z ti"
REFERENCES,0.4645441389290883,"ti−1
(αi−1 + 2i−1µ(t −ti−1))2
˜i−i+2−2dt ="
REFERENCES,0.46526772793053545,"˜i+1
X i=1  "
REFERENCES,0.46599131693198265,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.4667149059334298,"2
˜i−j+2"
REFERENCES,0.467438494934877,"α−2
˜i−i+2
i
((2
˜i−i+2 −1) · 2i−1µ)−1 
α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1
 ="
REFERENCES,0.4681620839363242,"˜i+1
X i=1  "
REFERENCES,0.46888567293777134,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.46960926193921854,"2
˜i−j+2"
REFERENCES,0.4703328509406657,"α−2
˜i−i+2
i
1
(2˜i+1 −2i−1)µ"
REFERENCES,0.4710564399421129,"
α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1
 ≤"
REFERENCES,0.4717800289435601,"˜i+1
X i=1  "
REFERENCES,0.4725036179450072,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.4732272069464544,"2
˜i−j+2"
REFERENCES,0.47395079594790157,"α−2
˜i−i+2
i
1
(2˜i+1 −2˜i)µ"
REFERENCES,0.47467438494934877,"
α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1
 ="
REFERENCES,0.47539797395079597,"˜i+1
X i=1  "
REFERENCES,0.4761215629522431,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.4768451519536903,"2
˜i−j+2"
REFERENCES,0.47756874095513746,"α−2
˜i−i+2
i
1
2˜i+1µ
1
1 −1 2"
REFERENCES,0.47829232995658466,"
α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1
"
REFERENCES,0.47901591895803186,"=2 ·
1
2˜i+1µ"
REFERENCES,0.479739507959479,"˜i+1
X i=1  "
REFERENCES,0.4804630969609262,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.48118668596237335,"2
˜i−j+2"
REFERENCES,0.48191027496382055,"α−2
˜i−i+2
i

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1
"
REFERENCES,0.48263386396526775,"=2 ·
1
2˜i+1µ"
REFERENCES,0.4833574529667149,"˜i+1
X i=1  "
REFERENCES,0.4840810419681621,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.48480463096960924,"2
˜i−j+2"
REFERENCES,0.48552821997105644,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i
."
REFERENCES,0.48625180897250364,"Lemma 9. Letting {αi} be a positive sequence, given 1 ≤˜i, it holds that"
REFERENCES,0.4869753979739508,"˜i+1
X i=1  "
REFERENCES,0.487698986975398,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.48842257597684513,"2
˜i−j+2"
REFERENCES,0.48914616497829233,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i
≤α−1
˜i+1.
(G.4)"
REFERENCES,0.48986975397973953,"Proof. First, we have"
REFERENCES,0.4905933429811867,"˜i+1
X i=1  "
REFERENCES,0.4913169319826339,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.492040520984081,"2
˜i−j+2"
REFERENCES,0.4927641099855282,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i ="
REFERENCES,0.4934876989869754,"˜i+1
X i=1  "
REFERENCES,0.49421128798842257,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.49493487698986977,"2
˜i−j+2  "
REFERENCES,0.4956584659913169,"α−1
i
−α2
˜i−i+2−1
i−1
α2˜i−i+2
i   ="
REFERENCES,0.4963820549927641,"˜i+1
X i=1  "
REFERENCES,0.4971056439942113,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.49782923299565845,"2
˜i−j+2"
REFERENCES,0.49855282199710566,"α−1
i
−"
REFERENCES,0.4992764109985528,"˜i+1
X i=1  "
REFERENCES,0.5,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5007235890014472,"2
˜i−j+2  "
REFERENCES,0.5014471780028944,"α2
˜i−i+2−1
i−1
α2˜i−i+2
i  "
REFERENCES,0.5021707670043415,"=α−1
˜i+1 + ˜i
X i=1  "
REFERENCES,0.5028943560057887,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5036179450072359,"2
˜i−j+2"
REFERENCES,0.5043415340086831,"α−1
i
−"
REFERENCES,0.5050651230101303,"˜i+1
X i=1  "
REFERENCES,0.5057887120115774,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5065123010130246,"2
˜i−j+2  "
REFERENCES,0.5072358900144718,"α2
˜i−i+2−1
i−1
α2˜i−i+2
i  ."
REFERENCES,0.507959479015919,Published as a conference paper at ICLR 2022
REFERENCES,0.5086830680173662,"Furthermore, we reformulate the term P˜i
i=1"
REFERENCES,0.5094066570188133,"""
Q˜i+1
j=i+1

αj−1 αj"
REFERENCES,0.5101302460202605,"2
˜i−j+2#"
REFERENCES,0.5108538350217077,"α−1
i
as follows ˜i
X i=1  "
REFERENCES,0.5115774240231549,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5123010130246021,"2
˜i−j+2"
REFERENCES,0.5130246020260492,"α−1
i
= ˜i
X i=1  "
REFERENCES,0.5137481910274964,"˜i+1
Y j=i+2 αj−1 αj"
REFERENCES,0.5144717800289436,"2
˜i−j+2"
REFERENCES,0.5151953690303908,"
 αi αi+1"
REFERENCES,0.515918958031838,"2
˜i−i+1"
REFERENCES,0.516642547033285,"α−1
i
(G.5) = ˜i
X i=1  "
REFERENCES,0.5173661360347322,"˜i+1
Y j=i+2 αj−1 αj"
REFERENCES,0.5180897250361794,"2
˜i−j+2 "
REFERENCES,0.5188133140376266,"α2
˜i−i+1−1
i
α2˜i−i+1
i+1 ! (G.6)"
REFERENCES,0.5195369030390738,"i′′=i+1
="
REFERENCES,0.5202604920405209,"˜i+1
X i′′=2  "
REFERENCES,0.5209840810419681,"˜i+1
Y"
REFERENCES,0.5217076700434153,j=i′′+1 αj−1 αj
REFERENCES,0.5224312590448625,"2
˜i−j+2  "
REFERENCES,0.5231548480463097,"α2
˜i−i′′+2−1
i′′−1
α2˜i−i′′+2
i′′ "
REFERENCES,0.5238784370477568,"
(G.7)"
REFERENCES,0.524602026049204,"i=i′′
="
REFERENCES,0.5253256150506512,"˜i+1
X i=2  "
REFERENCES,0.5260492040520984,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5267727930535456,"2
˜i−j+2  "
REFERENCES,0.5274963820549927,"α2
˜i−i+2−1
i−1
α2˜i−i+2
i "
REFERENCES,0.5282199710564399,".
(G.8)"
REFERENCES,0.5289435600578871,"Combining above results, we can obtain that"
REFERENCES,0.5296671490593343,"˜i+1
X i=1  "
REFERENCES,0.5303907380607815,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5311143270622286,"2
˜i−j+2"
REFERENCES,0.5318379160636758,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i"
REFERENCES,0.532561505065123,"=α−1
˜i+1 +"
REFERENCES,0.5332850940665702,"˜i+1
X i=2  "
REFERENCES,0.5340086830680174,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5347322720694645,"2
˜i−j+2  "
REFERENCES,0.5354558610709117,"α2
˜i−i+2−1
i−1
α2˜i−i+2
i   −"
REFERENCES,0.5361794500723589,"˜i+1
X i=1  "
REFERENCES,0.5369030390738061,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.5376266280752533,"2
˜i−j+2  "
REFERENCES,0.5383502170767004,"α2
˜i−i+2−1
i−1
α2˜i−i+2
i  "
REFERENCES,0.5390738060781476,"=α−1
˜i+1 −  "
REFERENCES,0.5397973950795948,"˜i+1
Y j=2 αj−1 αj"
REFERENCES,0.540520984081042,"2
˜i−j+2 "
REFERENCES,0.5412445730824892,"α2
˜i+1−1
0
α2˜i+1
1 !"
REFERENCES,0.5419681620839363,"≤α−1
˜i+1."
REFERENCES,0.5426917510853835,"Lemma 10. Letting us denote vt+1,j ≜Pt
k=0 η2
k
Qt
i=k+1 (1 −ηiλj)2 with ηi deﬁned in Eqn. (3.3),
for 1 ≤t ≤t′, it holds that"
REFERENCES,0.5434153400868307,"vt′,j ≤max(vt,j, ηt/λj).
(G.9)"
REFERENCES,0.5441389290882779,"Proof. If vt+1,j ≤max(vt,j, ηt/λj) holds for ∀t ≥1, then it naturally follows that"
REFERENCES,0.5448625180897251,"vt′,j ≤max

vt′−1,j, ηt′−1 λj"
REFERENCES,0.5455861070911722,"
≤max

vt′−2,j, ηt′−2"
REFERENCES,0.5463096960926194,"λj
, ηt′−1"
REFERENCES,0.5470332850940666,"λj
)
"
REFERENCES,0.5477568740955138,≤. . .
REFERENCES,0.548480463096961,"≤max

vt,j, ηt"
REFERENCES,0.5492040520984081,"λj
, . . . ηt′−2"
REFERENCES,0.5499276410998553,"λj
, ηt′−1 λj "
REFERENCES,0.5506512301013025,"= max

vt,j, ηt λj "
REFERENCES,0.5513748191027497,"where the last equality is entailed by the fact that t ≤t′ and ηt deﬁned in Eqn. (3.3) is monotonically
decreasing. We then prove vt+1,j ≤max(vt,j, ηt/λj) holds for ∀t ≥1."
REFERENCES,0.5520984081041969,Published as a conference paper at ICLR 2022
REFERENCES,0.552821997105644,"For ∀t ≥1, we have"
REFERENCES,0.5535455861070911,"vt+1,j = t
X"
REFERENCES,0.5542691751085383,"k=0
η2
k tY"
REFERENCES,0.5549927641099855,"i=k+1
(1 −ηiλj)2"
REFERENCES,0.5557163531114327,"=η2
t + t−1
X"
REFERENCES,0.5564399421128798,"k=0
η2
k tY"
REFERENCES,0.557163531114327,"i=k+1
(1 −ηiλj)2"
REFERENCES,0.5578871201157742,"=η2
t + (1 −ηtλj)2
t−1
X"
REFERENCES,0.5586107091172214,"k=0
η2
k t−1
Y"
REFERENCES,0.5593342981186686,"i=k+1
(1 −ηiλj)2"
REFERENCES,0.5600578871201157,"=η2
t + (1 −ηtλj)2vt,j"
REFERENCES,0.5607814761215629,(G.10)
REFERENCES,0.5615050651230101,"1) If vt+1,j ≤vt,j, then it naturally follows vt+1,j ≤max(vt,j, ηt/λj)."
REFERENCES,0.5622286541244573,"2) If vt+1,j > vt,j, denote a ≜(1 −ηtλj)2, b ≜η2
t , we have vt+1,j = avt,j + b, where a ∈[0, 1)
and b ≥0. It follows,"
REFERENCES,0.5629522431259045,"vt+1,j > vt,j
⇒
avt,j + b > vt,j"
REFERENCES,0.5636758321273516,"⇒
vt,j <
b
1 −a"
REFERENCES,0.5643994211287988,"⇒
vt+1,j = avt,j + b < a ·
b
1 −a + b =
b
1 −a"
REFERENCES,0.565123010130246,"Therefore,"
REFERENCES,0.5658465991316932,"vt+1,j <
b
1 −a =
η2
t
1 −(1 −ηtλj)2 <
η2
t
1 −(1 −ηtλj) = ηt"
REFERENCES,0.5665701881331404,"λj
≤max

vt,j, ηt λj 
,"
REFERENCES,0.5672937771345875,"where the second inequality is entailed by the fact that 1 −ηtλj ∈[0, 1)."
REFERENCES,0.5680173661360347,"Lemma 11. Letting vt,j be deﬁned as Lemma 10 and index ˜i satisfy λj ∈[µ · 2˜i, µ · 2˜i+1), then
v˜i+1,j has the following property"
REFERENCES,0.5687409551374819,"vt˜i+1,j ≤15 ·
ηt˜i+1"
REFERENCES,0.5694645441389291,"λj
.
(G.11)"
REFERENCES,0.5701881331403763,"Proof. By the fact that (1 −x) ≤exp(−x), we have"
REFERENCES,0.5709117221418234,"vt+1,j ≤ t
X"
REFERENCES,0.5716353111432706,"k=0
exp  −2 t
X"
REFERENCES,0.5723589001447178,"t′=k+1
ηt′λj ! η2
k."
REFERENCES,0.573082489146165,"Setting t = t˜i+1 −1 in above equation, we have"
REFERENCES,0.5738060781476122,"vt˜i+1,j ≤"
REFERENCES,0.5745296671490593,"t˜i+1−1
X"
REFERENCES,0.5752532561505065,"k=0
exp  −2"
REFERENCES,0.5759768451519537,"t˜i+1−1
X"
REFERENCES,0.5767004341534009,"t′=k+1
ηt′λj "
REFERENCES,0.5774240231548481,"η2
k.
(G.12)"
REFERENCES,0.5781476121562952,Published as a conference paper at ICLR 2022
REFERENCES,0.5788712011577424,"Now we bound the variance term. First, we have"
REFERENCES,0.5795947901591896,"t˜i+1−1
X"
REFERENCES,0.5803183791606368,"k=0
exp  −2"
REFERENCES,0.581041968162084,"t˜i+1−1
X"
REFERENCES,0.5817655571635311,"t=k+1
ηtλj "
REFERENCES,0.5824891461649783,"η2
k ="
REFERENCES,0.5832127351664255,"t˜i+1−1
X"
REFERENCES,0.5839363241678727,"k=0
exp  −2"
REFERENCES,0.5846599131693199,"t˜i+1−1
X"
REFERENCES,0.585383502170767,"t=k
ηtλj "
REFERENCES,0.5861070911722142,"exp(2ηkλj)η2
k ≤"
REFERENCES,0.5868306801736614,"t˜i+1−1
X"
REFERENCES,0.5875542691751086,"k=0
exp  −2"
REFERENCES,0.5882778581765558,"t˜i+1−1
X"
REFERENCES,0.5890014471780028,"t=k
ηtλj "
REFERENCES,0.58972503617945,"exp
2λj L"
REFERENCES,0.5904486251808972,"
η2
k"
REFERENCES,0.5911722141823444,≤exp(2) ·
REFERENCES,0.5918958031837916,"t˜i+1−1
X"
REFERENCES,0.5926193921852387,"k=0
exp  −2"
REFERENCES,0.5933429811866859,"t˜i+1−1
X"
REFERENCES,0.5940665701881331,"t=k
ηtλj "
REFERENCES,0.5947901591895803,"η2
k,"
REFERENCES,0.5955137481910275,"where the ﬁrst inequality is because ηk ≤1/L. Hence, we can obtain"
REFERENCES,0.5962373371924746,"t˜i+1−1
X"
REFERENCES,0.5969609261939218,"k=0
exp  −2"
REFERENCES,0.597684515195369,"t˜i+1−1
X"
REFERENCES,0.5984081041968162,"t=k+1
ηtλj  η2
k"
REFERENCES,0.5991316931982634,≤exp(2) ·
REFERENCES,0.5998552821997105,"t˜i+1−1
X"
REFERENCES,0.6005788712011577,"k=0
exp  −2"
REFERENCES,0.6013024602026049,"t˜i+1−1
X"
REFERENCES,0.6020260492040521,"t=k
ηtλj  η2
k"
REFERENCES,0.6027496382054993,= exp(2) ·
REFERENCES,0.6034732272069464,"˜i+1
X i=1"
REFERENCES,0.6041968162083936,"ti−1
X"
REFERENCES,0.6049204052098408,"k=ti−1
exp  −2"
REFERENCES,0.605643994211288,"t˜i+1−1
X"
REFERENCES,0.6063675832127352,"t=k
ηtλj "
REFERENCES,0.6070911722141823,"η2
k."
REFERENCES,0.6078147612156295,"Furthermore, combining with Eqn. (G.1) and the condition λj ∈[µ · 2˜i, µ · 2˜i+1), we can obtain"
REFERENCES,0.6085383502170767,"˜i+1
X i=1"
REFERENCES,0.6092619392185239,"ti−1
X"
REFERENCES,0.6099855282199711,"k=ti−1
exp  −2"
REFERENCES,0.6107091172214182,"t˜i+1−1
X"
REFERENCES,0.6114327062228654,"t=k
ηtλj "
REFERENCES,0.6121562952243126,"η2
k ≤"
REFERENCES,0.6128798842257598,"˜i+1
X i=1"
REFERENCES,0.613603473227207,"ti−1
X"
REFERENCES,0.6143270622286541,"k=ti−1
exp  −2"
REFERENCES,0.6150506512301013,"t˜i+1−1
X"
REFERENCES,0.6157742402315485,"t=k
ηtµ · 2
˜i  η2
k"
REFERENCES,0.6164978292329957,"(G.1)
≤"
REFERENCES,0.6172214182344429,"˜i+1
X i=1"
REFERENCES,0.61794500723589,"ti−1
X"
REFERENCES,0.6186685962373372,"k=ti−1
exp  −2"
REFERENCES,0.6193921852387844,"˜i+1
X"
REFERENCES,0.6201157742402316,"j=i+1
2
˜i−j+1 ln αj"
REFERENCES,0.6208393632416788,"αj−1
−2 · 2
˜i−i+1 ln
αi
αi−1 + 2i−1µ(k −ti−1)  η2
k ="
REFERENCES,0.6215629522431259,"˜i+1
X i=1"
REFERENCES,0.622286541244573,"ti−1
X"
REFERENCES,0.6230101302460203,"k=ti−1
exp  "
REFERENCES,0.6237337192474675,"˜i+1
X"
REFERENCES,0.6244573082489147,"j=i+1
2
˜i−j+2 ln αj−1"
REFERENCES,0.6251808972503617,"αj
+ 2
˜i−i+2 ln αi−1 + 2i−1µ(k −ti−1) αi  η2
k ="
REFERENCES,0.625904486251809,"˜i+1
X i=1"
REFERENCES,0.6266280752532561,"ti−1
X"
REFERENCES,0.6273516642547033,k=ti−1  
REFERENCES,0.6280752532561505,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6287988422575976,"2
˜i−j+2"
REFERENCES,0.6295224312590448,"·
αi−1 + 2i−1µ(k −ti−1) αi"
REFERENCES,0.630246020260492,"2
˜i−i+2 η2
k ="
REFERENCES,0.6309696092619392,"˜i+1
X i=1  "
REFERENCES,0.6316931982633864,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6324167872648335,"2
˜i−j+2 "
REFERENCES,0.6331403762662807,"ti−1
X"
REFERENCES,0.6338639652677279,k=ti−1
REFERENCES,0.6345875542691751,αi−1 + 2i−1µ(k −ti−1) αi
REFERENCES,0.6353111432706223,"2
˜i−i+2 η2
k"
REFERENCES,0.6360347322720694,"(3.3)
="
REFERENCES,0.6367583212735166,"˜i+1
X i=1  "
REFERENCES,0.6374819102749638,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.638205499276411,"2
˜i−j+2  ·"
REFERENCES,0.6389290882778582,"ti−1
X"
REFERENCES,0.6396526772793053,k=ti−1
REFERENCES,0.6403762662807525,αi−1 + 2i−1µ(k −ti−1) αi
REFERENCES,0.6410998552821997,"2
˜i−i+2 "
REFERENCES,0.6418234442836469,"L + µ i−1
X"
REFERENCES,0.6425470332850941,"j=1
∆j2j−1 + 2i−1µ(k −ti−1)   −2"
REFERENCES,0.6432706222865412,"(G.2)
="
REFERENCES,0.6439942112879884,"˜i+1
X i=1  "
REFERENCES,0.6447178002894356,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6454413892908828,"2
˜i−j+2  ·"
REFERENCES,0.64616497829233,"ti−1
X"
REFERENCES,0.6468885672937771,k=ti−1
REFERENCES,0.6476121562952243,αi−1 + 2i−1µ(k −ti−1) αi
REFERENCES,0.6483357452966715,"2
˜i−i+2  
αi−1 + 2i−1µ(k −ti−1)
−2"
REFERENCES,0.6490593342981187,Published as a conference paper at ICLR 2022 =
REFERENCES,0.6497829232995659,"˜i+1
X i=1  "
REFERENCES,0.650506512301013,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6512301013024602,"2
˜i−j+2"
REFERENCES,0.6519536903039074,"α−2
˜i−i+2
i ·"
REFERENCES,0.6526772793053546,"ti−1
X"
REFERENCES,0.6534008683068018,"k=ti−1
(αi−1 + 2i−1µ(k −ti−1))2
˜i−i+2  
αi−1 + 2i−1µ(k −ti−1)
−2 ="
REFERENCES,0.6541244573082489,"˜i+1
X i=1  "
REFERENCES,0.6548480463096961,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6555716353111433,"2
˜i−j+2"
REFERENCES,0.6562952243125905,"α−2
˜i−i+2
i"
REFERENCES,0.6570188133140377,"ti−1
X"
REFERENCES,0.6577424023154848,"k=ti−1
(αi−1 + 2i−1µ(k −ti−1))2
˜i−i+2−2"
REFERENCES,0.658465991316932,"(G.3)
≤2 ·
1
2˜i+1µ
·"
REFERENCES,0.6591895803183792,"˜i+1
X i=1  "
REFERENCES,0.6599131693198264,"˜i+1
Y j=i+1 αj−1 αj"
REFERENCES,0.6606367583212736,"2
˜i−j+2"
REFERENCES,0.6613603473227206,"

α2
˜i−i+2−1
i
−α2
˜i−i+2−1
i−1

α−2
˜i−i+2
i"
REFERENCES,0.6620839363241678,"(G.4)
≤2 ·
1
2˜i+1µ
· α−1
˜i+1 ≤2 ·
ηt˜i+1 λj
,"
REFERENCES,0.662807525325615,"where the last inequality is because of the condition λj ∈[µ · 2˜i, µ · 2˜i+1) and the deﬁnition of αi."
REFERENCES,0.6635311143270622,"Therefore, we have"
REFERENCES,0.6642547033285094,"vt˜i+1,j ≤2 exp(2) ·
ηt˜i+1"
REFERENCES,0.6649782923299565,"λj
≤15 ·
ηt˜i+1 λj
."
REFERENCES,0.6657018813314037,"Lemma 12. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD
for T-steps starting from w0 and a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3), the ﬁnal
iterate wT +1 satisﬁes"
REFERENCES,0.6664254703328509,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)

≤(w0 −w∗)⊤H(w0 −w∗) · exp  −2µ T
X"
REFERENCES,0.6671490593342981,"k=0
ηk !"
REFERENCES,0.6678726483357453,+ 15σ2µ
REFERENCES,0.6685962373371924,"Imax−1
X ˜i=0"
REFERENCES,0.6693198263386396,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 ."
REFERENCES,0.6700434153400868,"Proof. The target of this lemma is to obtain the explicit form to bound the variance term. By the
deﬁnition of vt+1,j in Lemma 10, we can obtain that T
X"
REFERENCES,0.670767004341534,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ
"
REFERENCES,0.6714905933429812,"(F.7)
≤σ2
d
X"
REFERENCES,0.6722141823444283,"j=1
λ2
j · vT +1,j"
REFERENCES,0.6729377713458755,"(G.9)
≤σ2
d
X"
REFERENCES,0.6736613603473227,"j=1
λ2
j · max

vt˜i+1+1,j,
ηt˜i+1+1 λj "
REFERENCES,0.6743849493487699,"(G.11)
≤σ2
d
X"
REFERENCES,0.6751085383502171,"j=1
λ2
j · max

15 ·
ηt˜i+1+1"
REFERENCES,0.6758321273516642,"λj
,
ηt˜i+1+1 λj "
REFERENCES,0.6765557163531114,"=15σ2
d
X"
REFERENCES,0.6772793053545586,"j=1
λj · ηt˜i+1+1 ≤15σ2
d
X"
REFERENCES,0.6780028943560058,"j=1
λj · ηt˜i+1 ≤15σ2µ"
REFERENCES,0.678726483357453,"Imax−1
X"
REFERENCES,0.6794500723589001,"˜i=0
2
˜i+1s˜i · ηt˜i+1,"
REFERENCES,0.6801736613603473,"where the last inequality is because λj ∈[2˜iµ, 2˜i+1µ) and there are s˜i such λj’s lie in this range.
By Eqn. (3.3), we have"
REFERENCES,0.6808972503617945,"ηt˜i+1 =
1"
REFERENCES,0.6816208393632417,"L + µ P˜i+1
j=1 ∆j2j−1 .
(G.13)"
REFERENCES,0.6823444283646889,Published as a conference paper at ICLR 2022
REFERENCES,0.683068017366136,"Therefore, we have T
X"
REFERENCES,0.6837916063675832,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ

≤15σ2µ"
REFERENCES,0.6845151953690304,"Imax−1
X ˜i=0"
REFERENCES,0.6852387843704776,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 .
(G.14)"
REFERENCES,0.6859623733719248,"Combining with Lemma 4 and Lemma 6, we can obtain that"
REFERENCES,0.6866859623733719,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)

≤(w0 −w∗)⊤H(w0 −w∗) · exp  −2µ T
X"
REFERENCES,0.6874095513748191,"k=0
ηk !"
REFERENCES,0.6881331403762663,+ 15σ2µ
REFERENCES,0.6888567293777135,"Imax−1
X ˜i=0"
REFERENCES,0.6895803183791607,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 ."
REFERENCES,0.6903039073806078,"Lemma 13. For ∀t ≥0, the learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3) satisﬁes"
REFERENCES,0.691027496382055,"ηt ≤
1
L + µt
(G.15)"
REFERENCES,0.6917510853835022,"Proof. For ∀t ≥0, there ∃i ≥1, where t ∈[ti−1, ti). Given the form deﬁned in Eqn. (3.3), we
have,"
REFERENCES,0.6924746743849494,"ηt =
1"
REFERENCES,0.6931982633863966,"L + µ Pi−1
j=1 ∆j2j−1 + 2i−1µ(t −ti−1) ≤
1"
REFERENCES,0.6939218523878437,"L + µ Pi−1
j=1 ∆j + µ(t −ti−1)"
REFERENCES,0.6946454413892909,"(3.4)
=
1"
REFERENCES,0.695369030390738,"L + µ Pi−1
j=1(tj −tj−1) + µ(t −ti−1)"
REFERENCES,0.6960926193921853,"=
1
L + µ(ti−1 −t0) + µ(t −ti−1)"
REFERENCES,0.6968162083936325,"=
1
L + µ(t −t0)"
REFERENCES,0.6975397973950795,"=
1
L + µt"
REFERENCES,0.6982633863965267,"Lemma 14. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD
for T-steps starting from w0 and a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3), the ﬁnal
iterate wT +1 satisﬁes"
REFERENCES,0.6989869753979739,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)

≤(w0 −w∗)⊤H(w0 −w∗) · κ2 ∆2
1"
REFERENCES,0.6997105643994211,+ 15σ2µ
REFERENCES,0.7004341534008683,"Imax−1
X ˜i=0"
REFERENCES,0.7011577424023154,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 ."
REFERENCES,0.7018813314037626,Proof. The target of this lemma is to obtain the explicit form to bound the bias term.
REFERENCES,0.7026049204052098,Published as a conference paper at ICLR 2022
REFERENCES,0.703328509406657,"First, by Eqn. (G.1) and the condition λj ∈[µ · 2˜i, µ · 2˜i+1), we have exp  −2λj T
X"
REFERENCES,0.7040520984081042,"k=0
ηk ! ≤exp  −2"
REFERENCES,0.7047756874095513,"t˜i+1−1
X"
REFERENCES,0.7054992764109985,"k=0
ηkλj  ≤exp  −2"
REFERENCES,0.7062228654124457,"˜i+1
X i=1"
REFERENCES,0.7069464544138929,"1
2i−1µ ln αi"
REFERENCES,0.7076700434153401,"αi−1
λj   ≤exp  −"
REFERENCES,0.7083936324167872,"˜i+1
X"
REFERENCES,0.7091172214182344,"i=1
2
˜i−i+2 ln αi αi−1  ="
REFERENCES,0.7098408104196816,"˜i+1
Y i=1 αi−1 αi"
REFERENCES,0.7105643994211288,"2
˜i−i+2 ≤"
REFERENCES,0.711287988422576,"˜i+1
Y i=1 αi−1 αi"
REFERENCES,0.7120115774240231,"2
=
 α1 α˜i+1"
REFERENCES,0.7127351664254703,"2
= L2 · η2
t˜i+1"
REFERENCES,0.7134587554269175,(G.16)
REFERENCES,0.7141823444283647,"For λj = µ, since µ ∈[µ · 2˜i, µ · 2˜i+1) for ˜i = 0, it follows, exp  −2µ T
X"
REFERENCES,0.7149059334298119,"k=0
ηk !"
REFERENCES,0.715629522431259,"≤L2 · η2
t1"
REFERENCES,0.7163531114327062,"(G.15)
≤

L
L + µt1"
REFERENCES,0.7170767004341534,"2
(3.4)
=

L
L + µ∆1"
REFERENCES,0.7178002894356006,"2
≤
 L µ∆1"
REFERENCES,0.7185238784370478,"2
= κ2"
REFERENCES,0.7192474674384949,"∆2
1
(G.17)"
REFERENCES,0.7199710564399421,"Combining with Lemma 12, we obtain that,"
REFERENCES,0.7206946454413893,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)

≤(w0 −w∗)⊤H(w0 −w∗) · κ2 ∆2
1"
REFERENCES,0.7214182344428365,+ 15σ2µ
REFERENCES,0.7221418234442837,"Imax−1
X ˜i=0"
REFERENCES,0.7228654124457308,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 ."
REFERENCES,0.723589001447178,"G.1
PROOF OF LEMMA 1"
REFERENCES,0.7243125904486252,"Lemma 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T-steps starting from w0 and a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3), the ﬁnal iterate
wT +1 satisﬁes"
REFERENCES,0.7250361794500724,E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) · κ2
REFERENCES,0.7257597684515196,"∆2
1
+ 15"
REFERENCES,0.7264833574529667,2 · σ2µ
REFERENCES,0.7272069464544139,"Imax−1
X ˜i=0"
REFERENCES,0.7279305354558611,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 . (3.6)"
REFERENCES,0.7286541244573083,"Proof. For ∀t ≥0, we have"
REFERENCES,0.7293777134587555,"f(wt) −f(w∗)
(1.2)
= E
1"
REFERENCES,0.7301013024602026,"2w⊤
t H(ξ)wt −b(ξ)⊤wt"
REFERENCES,0.7308248914616498,"
−E
1"
REFERENCES,0.731548480463097,"2w⊤
∗H(ξ)w∗−b(ξ)⊤w∗  =
1"
REFERENCES,0.7322720694645442,"2w⊤
t E[H(ξ)]wt −E[b(ξ)]⊤wt"
REFERENCES,0.7329956584659914,"
−
1"
REFERENCES,0.7337192474674384,"2w⊤
∗E[H(ξ)]w∗−E[b(ξ)]⊤w∗  =
1"
REFERENCES,0.7344428364688856,"2w⊤
t Hwt −b⊤wt"
REFERENCES,0.7351664254703328,"
−
1"
REFERENCES,0.73589001447178,"2w⊤
∗Hw∗−b⊤w∗ "
REFERENCES,0.7366136034732272,"(1.4)
=
1"
REFERENCES,0.7373371924746743,"2w⊤
t Hwt −b⊤wt"
REFERENCES,0.7380607814761215,"
−
1"
REFERENCES,0.7387843704775687,"2b⊤ 
H⊤−1 b −b⊤H−1b
 =
1"
REFERENCES,0.7395079594790159,"2w⊤
t Hwt −b⊤wt"
REFERENCES,0.7402315484804631,"
−
1"
REFERENCES,0.7409551374819102,"2b⊤H−1b −b⊤H−1b
 =1"
REFERENCES,0.7416787264833574,"2w⊤
t Hwt −b⊤wt + 1"
REFERENCES,0.7424023154848046,2b⊤H−1b =1
REFERENCES,0.7431259044862518,"2w⊤
t Hwt −1"
REFERENCES,0.743849493487699,2b⊤wt −1
REFERENCES,0.7445730824891461,2b⊤wt + 1
REFERENCES,0.7452966714905933,2b⊤H−1b
REFERENCES,0.7460202604920405,Published as a conference paper at ICLR 2022 =1
REFERENCES,0.7467438494934877,"2w⊤
t Hwt −1"
REFERENCES,0.7474674384949349,"2w⊤
t b −1"
REFERENCES,0.748191027496382,2b⊤wt + 1
REFERENCES,0.7489146164978292,2b⊤H−1b =1
REFERENCES,0.7496382054992764,"2w⊤
t Hwt −1"
REFERENCES,0.7503617945007236,"2w⊤
t Hw∗−1"
REFERENCES,0.7510853835021708,"2w⊤
∗Hwt + 1"
REFERENCES,0.751808972503618,"2w⊤
∗Hw∗ =1"
REFERENCES,0.7525325615050651,"2(wt −w∗)⊤H(wt −w∗),"
REFERENCES,0.7532561505065123,"where the 5th equality is entailed by the fact that H⊤= H is a symmetric matrix, and the 9th
equality uses both H⊤= H and Eqn 1.4."
REFERENCES,0.7539797395079595,"Combine the above result with Lemma 14, we obtain that"
REFERENCES,0.7547033285094067,E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) · κ2
REFERENCES,0.7554269175108539,"∆2
1
+ 15"
REFERENCES,0.756150506512301,2 · σ2µ
REFERENCES,0.7568740955137482,"Imax−1
X ˜i=0"
REFERENCES,0.7575976845151954,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 ."
REFERENCES,0.7583212735166426,"G.2
PROOF OF THEOREM 1"
REFERENCES,0.7590448625180898,"Theorem 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T-steps starting from w0, a learning rate sequence {ηt}T
t=1 deﬁned in Eqn. (3.3) and ∆i deﬁned in
Eqn. (3.5), the ﬁnal iterate wT +1 satisﬁes"
REFERENCES,0.7597684515195369,"E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) ·
κ2 ·
PImax−1
i=0
√si
2"
REFERENCES,0.7604920405209841,"s0T 2
+
15
PImax−1
i=0
√si
2"
REFERENCES,0.7612156295224313,"T
· σ2."
REFERENCES,0.7619392185238785,Proof. We have µ ·
REFERENCES,0.7626628075253257,"Imax−1
X ˜i=0"
REFERENCES,0.7633863965267728,"2˜i+1s˜i
L + µ P˜i+1
j=1 ∆j2j−1 <µ ·"
REFERENCES,0.76410998552822,"Imax−1
X ˜i=0"
REFERENCES,0.7648335745296672,"2˜i+1s˜i
µ2˜i∆˜i+1"
REFERENCES,0.7655571635311144,"(3.5)
= 2"
REFERENCES,0.7662807525325616,"Imax−1
X ˜i=0"
REFERENCES,0.7670043415340086,"s˜i
√s˜i
PImax−1
i=0
√si · T = 2 T ·"
REFERENCES,0.7677279305354558,"Imax−1
X i=0 √si ·"
REFERENCES,0.768451519536903,"Imax−1
X ˜i=0"
REFERENCES,0.7691751085383502,"√s˜i =
2
PImax−1
i=0
√si
2 T
."
REFERENCES,0.7698986975397974,"Combining with Lemma 1 and the deﬁnition of ∆1, we can obtain that"
REFERENCES,0.7706222865412445,"E [f(wT +1) −f(w∗)] ≤(f(w0) −f(w∗)) ·
κ2 ·
PImax−1
i=0
√si
2"
REFERENCES,0.7713458755426917,"s0T 2
+
15
PImax−1
i=0
√si
2"
REFERENCES,0.7720694645441389,"T
· σ2."
REFERENCES,0.7727930535455861,"G.3
PROOF OF COROLLARY 2"
REFERENCES,0.7735166425470333,"Corollary 2. Given the same setting as in Theorem 1, when Hessian H’s eigenvalue distribution
p(λ) satisﬁes “power law”, i.e."
REFERENCES,0.7742402315484804,p(λ) = 1
REFERENCES,0.7749638205499276,Z · exp(−α(ln(λ) −ln(µ))) = 1
REFERENCES,0.7756874095513748,"Z ·
µ λ"
REFERENCES,0.776410998552822,"α
(3.7)"
REFERENCES,0.7771345875542692,"for some α > 1, where Z =
R L
µ (µ/λ)αdλ, there exists a constant C(α) which only depends on α,
such that the ﬁnal iterate wT +1 satisﬁes"
REFERENCES,0.7778581765557163,"E [f(wT +1) −f(w∗)] ≤

(f(w0) −f(w∗)) · κ2"
REFERENCES,0.7785817655571635,T 2 + dσ2 T
REFERENCES,0.7793053545586107,"
· C(α)."
REFERENCES,0.7800289435600579,Published as a conference paper at ICLR 2022
REFERENCES,0.7807525325615051,"Proof. According to Theorem 1,"
REFERENCES,0.7814761215629522,E [f(wT +1) −f(w∗)]
REFERENCES,0.7821997105643994,"≤(f(w0) −f(w∗)) ·
κ2 ·
PImax−1
i=0
√si
2"
REFERENCES,0.7829232995658466,"s0T 2
+
15
PImax−1
i=0
√si
2"
REFERENCES,0.7836468885672938,"T
· σ2"
REFERENCES,0.784370477568741,=(f(w0) −f(w∗)) · κ2 T 2 ·
REFERENCES,0.7850940665701881,"PImax−1
i=0
√si
2"
REFERENCES,0.7858176555716353,"s0
+ dσ2"
REFERENCES,0.7865412445730825,"T
·
15
PImax−1
i=0
√si
2 d
."
REFERENCES,0.7872648335745297,"The key terms here are C1 ≜
PImax−1
i=0
√si
2
/s0 and C2 ≜15
PImax−1
i=0
√si
2
/d. As long as"
REFERENCES,0.7879884225759769,"we can bound both terms with a constant C(α), the corollary will be directly proved."
REFERENCES,0.788712011577424,"1) If κ < 2, then there is only one interval with s0 = d. By setting C(α) = max(C1, C2) = 15, this
completes the proof."
REFERENCES,0.7894356005788712,"2) If κ ≥2, then bounding C1 and C2 be done by computing the value of si under power law. For
all interval i except the last interval, we have, si d"
REFERENCES,0.7901591895803184,"(3.1)
= #λj ∈[µ · 2i, µ · 2i+1) =
Z µ·2i+1"
REFERENCES,0.7908827785817656,"µ·2i
p(λ)dλ"
REFERENCES,0.7916063675832128,"(3.7)
=
Z µ·2i+1"
REFERENCES,0.7923299565846599,"µ·2i
1
Z ·
µ λ"
REFERENCES,0.7930535455861071,"α
dλ = 1"
REFERENCES,0.7937771345875543,"Z · µα ·
Z µ·2i+1"
REFERENCES,0.7945007235890015,"µ·2i
λ−αdλ = Z L µ µ λ α
dλ !−1"
REFERENCES,0.7952243125904487,"· µα ·
Z µ·2i+1"
REFERENCES,0.7959479015918958,"µ·2i
λ−αdλ = Z L"
REFERENCES,0.796671490593343,"µ
λ−αdλ !−1"
REFERENCES,0.7973950795947902,"·
Z µ·2i+1"
REFERENCES,0.7981186685962374,"µ·2i
λ−αdλ"
REFERENCES,0.7988422575976846,"=
 λ1−α 1 −α L µ"
REFERENCES,0.7995658465991317,"−1
·
 λ1−α 1 −α"
REFERENCES,0.8002894356005789,µ·2i+1 µ·2i
REFERENCES,0.8010130246020261,"
=

λ1−α
L µ"
REFERENCES,0.8017366136034733,"−1
·

λ1−α
µ·2i+1 µ·2i "
REFERENCES,0.8024602026049205,"=
 
L1−α −µ1−α−1 ·

µ1−α ·
 
2i+11−α −µ1−α ·
 
2i1−α"
REFERENCES,0.8031837916063675,"=µ1−α ·
 
2i+11−α −µ1−α ·
 
2i1−α"
REFERENCES,0.8039073806078147,"L1−α −µ1−α
="
REFERENCES,0.804630969609262," 
2i+11−α −
 
2i1−α"
REFERENCES,0.8053545586107091,κ1−α −1
REFERENCES,0.8060781476121563,=2i(1−α) · 21−α −1
REFERENCES,0.8068017366136034,κ1−α −1
REFERENCES,0.8075253256150506,"Therefore, we have"
REFERENCES,0.8082489146164978,si = d · 2i(1−α) · 21−α −1
REFERENCES,0.808972503617945,κ1−α −1 = d · 21−α −1
REFERENCES,0.8096960926193922,"κ1−α −1 · 2i(1−α)
(G.18)"
REFERENCES,0.8104196816208393,"holds for all interval i except the last interval i′ = Imax −1 = log2 κ −1 > 0. This last interval
may not completely covers [µ · 2i′, µ · 2i′+1) due to the boundary truncated by L, but we still have"
REFERENCES,0.8111432706222865,si′ ≤d · 21−α −1
REFERENCES,0.8118668596237337,κ1−α −1 · 2i′(1−α)
REFERENCES,0.8125904486251809,Published as a conference paper at ICLR 2022
REFERENCES,0.8133140376266281,"It follows,
 Imax−1
X i=0 √si !2"
REFERENCES,0.8140376266280752,≤d · 21−α −1
REFERENCES,0.8147612156295224,κ1−α −1 ·
REFERENCES,0.8154848046309696,"Imax−1
X i=0 p"
REFERENCES,0.8162083936324168,"2i(1−α)
!2"
REFERENCES,0.816931982633864,= d · 21−α −1
REFERENCES,0.8176555716353111,κ1−α −1 ·
REFERENCES,0.8183791606367583,"Imax−1
X"
REFERENCES,0.8191027496382055,"i=0
2i(1−α)/2
!2"
REFERENCES,0.8198263386396527,=d · 21−α −1
REFERENCES,0.8205499276410999,κ1−α −1 ·
REFERENCES,0.821273516642547,"Imax−1
X i=0 1 2"
REFERENCES,0.8219971056439942,i(α−1)/2!2
REFERENCES,0.8227206946454414,≤d · 21−α −1
REFERENCES,0.8234442836468886,"κ1−α −1 · ∞
X i=0 1 2"
REFERENCES,0.8241678726483358,i(α−1)/2!2
REFERENCES,0.8248914616497829,= d · 21−α −1
REFERENCES,0.8256150506512301,"κ1−α −1 · ∞
X i=0"
REFERENCES,0.8263386396526773,"
1
2(α−1)/2 i!2"
REFERENCES,0.8270622286541245,=d · 21−α −1
REFERENCES,0.8277858176555717,κ1−α −1 ·
REFERENCES,0.8285094066570188,"1
1 −
1
2(α−1)/2 !2"
REFERENCES,0.829232995658466,= d · 21−α −1
REFERENCES,0.8299565846599132,"κ1−α −1 ·

1
1 −2(1−α)/2 2
. Thus, C1 ="
REFERENCES,0.8306801736613604,"PImax−1
i=0
√si
2"
REFERENCES,0.8314037626628076,"s0
≤
d · 21−α−1"
REFERENCES,0.8321273516642547,"κ1−α−1 ·

1
1−2(1−α)/2
2"
REFERENCES,0.8328509406657019,d · 21−α−1
REFERENCES,0.8335745296671491,"κ1−α−1 · 20(1−α)
=

1
1 −2(1−α)/2 2"
REFERENCES,0.8342981186685963,"C2 =
15
PImax−1
i=0
√si
2 d
≤15"
REFERENCES,0.8350217076700435,d · d · 21−α −1
REFERENCES,0.8357452966714906,"κ1−α −1 ·

1
1 −2(1−α)/2 2"
REFERENCES,0.8364688856729378,"=15 · 1 −
  1"
REFERENCES,0.837192474674385,"2
α−1"
REFERENCES,0.8379160636758322,"1 −
  1"
REFERENCES,0.8386396526772794,"κ
α−1 ·

1
1 −2(1−α)/2 2"
REFERENCES,0.8393632416787264,"≤15 ·

1
1 −2(1−α)/2 2
."
REFERENCES,0.8400868306801736,Here the last inequality for C2 is entailed by κ ≥2 and α > 1.
REFERENCES,0.8408104196816208,"By setting C(α) = max(C1, C2) = 15 ·

1
1−2(1−α)/2
2
, we obtain"
REFERENCES,0.841534008683068,E [f(wT +1) −f(w∗)]
REFERENCES,0.8422575976845152,≤(f(w0) −f(w∗)) · κ2 T 2 ·
REFERENCES,0.8429811866859623,"PImax−1
i=0
√si
2"
REFERENCES,0.8437047756874095,"s0
+ dσ2"
REFERENCES,0.8444283646888567,"T
·
15
PImax−1
i=0
√si
2 d
."
REFERENCES,0.8451519536903039,=(f(w0) −f(w∗)) · κ2
REFERENCES,0.8458755426917511,T 2 · C1 + dσ2
REFERENCES,0.8465991316931982,"T
· C2"
REFERENCES,0.8473227206946454,"≤

(f(w0) −f(w∗)) · κ2"
REFERENCES,0.8480463096960926,T 2 + dσ2 T
REFERENCES,0.8487698986975398,"
· C(α)."
REFERENCES,0.849493487698987,"G.4
PROOF OF THEOREM 4"
REFERENCES,0.8502170767004341,"Theorem 4. Let objective function f(x) be quadratic. We run SGD for T-steps starting from w0 and
a step decay learning rate sequence {ηt}T
t=1 deﬁned in Algorithm 1 of Ge et al. (2019) with η1 ≤
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ

ntn⊤
t

=
σ2H and (3) λj (w0,j −w∗,j)2 ̸= 0 for ∀j = 1, 2, . . . , d, the ﬁnal iterate wT +1 satisﬁes,"
REFERENCES,0.8509406657018813,"E [f(wT +1) −f(w∗)] = Ω
dσ2"
REFERENCES,0.8516642547033285,"T
· log T
"
REFERENCES,0.8523878437047757,Published as a conference paper at ICLR 2022
REFERENCES,0.8531114327062229,"Proof. The lower bound here is an asymptotic bound. Speciﬁcally, we require"
REFERENCES,0.85383502170767,"T
log T ≥max "
REFERENCES,0.8545586107091172,"216, 16, 1"
REFERENCES,0.8552821997105644,"256 ·
σ2"
REFERENCES,0.8560057887120116,"minj λj (w0,j −w∗,j)2 !"
REFERENCES,0.8567293777134588,".
(G.19)"
REFERENCES,0.8574529667149059,"In Ge et al. (2019), step decay has following learning rate sequence:"
REFERENCES,0.8581765557163531,ηt = η1
REFERENCES,0.8589001447178003,"2ℓ
if t ∈

1 +
T
log T · ℓ,
T
log T · (ℓ+ 1)

,
(G.20)"
REFERENCES,0.8596237337192475,"where ℓ= 0, 1, . . . , log T −1. Notice that the index start from t = 1 instead of t = 0. For
consistency with our framework, we set η0 = 0, which produces the exact same step decay scheduler
while only adding one extra iteration, thus does not affect the overall asymptotic bound."
REFERENCES,0.8603473227206947,We ﬁrst translate the general notations to diagonal cases so that the idea of the proof can be clearer.
REFERENCES,0.8610709117221418,"Since f(x) is quadratic, according to the proof of Lemma 1 in Appendix G.1,"
REFERENCES,0.861794500723589,f(wT +1) −f(w∗) =1
REFERENCES,0.8625180897250362,2(wT +1 −w∗)⊤H(wT +1 −w∗).
REFERENCES,0.8632416787264834,"Furthermore, according to Lemma 4, where Pt = I −ηtH,"
REFERENCES,0.8639652677279306,"E

(wT +1 −w∗)⊤H(wT +1 −w∗)
"
REFERENCES,0.8646888567293777,"=E

(w0 −w∗)⊤· PT . . . P0HP0 . . . PT · (w0 −w∗)
 + T
X"
REFERENCES,0.8654124457308249,"τ=0
E

η2
τn⊤
τ · PT . . . Pτ+1HPτ+1 . . . PT · nτ
 = d
X"
REFERENCES,0.8661360347322721,"j=1
λj (w0,j −w∗,j)2
T
Y"
REFERENCES,0.8668596237337193,"k=0
(1 −ηkλj)2 + T
X"
REFERENCES,0.8675832127351665,"τ=0
η2
τ d
X j=1 T
Y"
REFERENCES,0.8683068017366136,"k=τ+1
λj(1 −ηkλj)2E

n2
τ,j
 = d
X"
REFERENCES,0.8690303907380608,"j=1
λj (w0,j −w∗,j)2
T
Y"
REFERENCES,0.869753979739508,"k=0
(1 −ηkλj)2 + T
X"
REFERENCES,0.8704775687409552,"τ=0
η2
τ d
X j=1 T
Y"
REFERENCES,0.8712011577424024,"k=τ+1
λj(1 −ηkλj)2 · λjσ2 = d
X"
REFERENCES,0.8719247467438495,"j=1
λj (w0,j −w∗,j)2
T
Y"
REFERENCES,0.8726483357452967,"k=0
(1 −ηkλj)2 + σ2
d
X"
REFERENCES,0.8733719247467439,"j=1
λ2
j T
X"
REFERENCES,0.874095513748191,"τ=0
η2
τ T
Y"
REFERENCES,0.8748191027496383,"k=τ+1
(1 −ηkλj)2."
REFERENCES,0.8755426917510853,"Here the second equality is entailed by the fact that H and Pt are diagonal, and the third equality
comes from Eξ

ntn⊤
t

= σ2H. Thus, by denoting bj ≜λj (w0,j −w∗,j)2 QT
k=0(1 −ηkλj)2 and
vj ≜PT
τ=0 η2
τ
QT
k=τ+1(1 −ηkλj)2, we have,"
REFERENCES,0.8762662807525325,E [f(wT +1 −f(w∗)] = 1
E,0.8769898697539797,"2E

(wT +1 −w∗)⊤H(wT +1 −w∗)
 = 1 2    
d
X"
E,0.877713458755427,"j=1
bj  + "
E,0.8784370477568741,"σ2
d
X"
E,0.8791606367583212,"j=1
λ2
jvj   "
E,0.8798842257597684,".
(G.21)"
E,0.8806078147612156,"To proceed the analysis, we divide all eigenvalues {λj} into two groups:"
E,0.8813314037626628,"A =

j
λj > log T 8η1T"
E,0.88205499276411,"
,
B =

j
λj ≤log T 8η1T"
E,0.8827785817655571,"
,
(G.22)"
E,0.8835021707670043,"where group A are those large eigenvalues that the variance term vj will ﬁnally dominate, and group
B are those small eigenvalues that the bias term bj will ﬁnally dominate. Rigorously speaking,"
E,0.8842257597684515,a) For ∀j ∈A:
E,0.8849493487698987,Step decay’s bottleneck in variance term actually occurs at the ﬁrst interval ℓthat satisﬁes
E,0.8856729377713459,"2ℓ≥λjη1 ·
8T
log T
(G.23)"
E,0.886396526772793,Published as a conference paper at ICLR 2022
E,0.8871201157742402,"We ﬁrst show that interval ℓis well-deﬁned for any dimension j ∈A. Since j ∈A, it follows from
the deﬁnition of A in Eqn. (G.22),"
E,0.8878437047756874,λj > log T
E,0.8885672937771346,"8η1T
=⇒
λjη1 ·
8T
log T > 1 = 20"
E,0.8892908827785818,"On the other hand, since we assume T/ log T ≥216 in Eqn. (G.19), which implies T ≥216 ⇒
log T ≥16, it follows"
E,0.8900144717800289,"λjη1 ·
8T
log T ≤λjη1 · T 2 ≤λj L · T 2 ≤T"
E,0.8907380607814761,"2 = 2log T −1,"
E,0.8914616497829233,"where the second inequality comes from η1 ≤1/L in assumption (1), and the third inequality is
entailed by λj ≤L given the deﬁnition of L in Eqn. (1.6)."
E,0.8921852387843705,"As a result, we have"
E,0.8929088277858177,"λjη1 ·
8T
log T ∈
 
20, 2log T −1 thus"
E,0.8936324167872648,"2ℓ≥λjη1 ·
8T
log T"
E,0.894356005788712,"will guaranteed be satisiﬁed for some interval ℓ= 1, . . . , log T −1. Since interval ℓis the ﬁrst
interval satisiﬁes Eqn. (G.23), we also have"
E,0.8950795947901592,"2ℓ−1 < λjη1 ·
8T
log T
=⇒
2ℓ< λjη1 · 16T"
E,0.8958031837916064,"log T
(G.24)"
E,0.8965267727930536,"Back to our analysis for the lower bound, by focusing on the variance produced by interval ℓonly,
we have, vj = T
X"
E,0.8972503617945007,"τ=0
η2
τ T
Y"
E,0.8979739507959479,"k=τ+1
(1 −ηkλj)2 ≥"
E,0.8986975397973951,"(ℓ+1)·
T
log T
X"
E,0.8994211287988423,"τ=ℓ·
T
log T +1
η2
τ T
Y"
E,0.9001447178002895,"k=τ+1
(1 −ηkλj)2 ≥"
E,0.9008683068017366,"(ℓ+1)·
T
log T
X"
E,0.9015918958031838,"τ=ℓ·
T
log T +1
η2
τ T
Y"
E,0.902315484804631,"k=ℓ·
T
log T +1
(1 −ηkλj)2 ="
E,0.9030390738060782,"(ℓ+1)·
T
log T
X"
E,0.9037626628075254,"τ=ℓ·
T
log T +1 η1 2ℓ"
E,0.9044862518089725,"2
T
Y"
E,0.9052098408104197,"k=ℓ·
T
log T +1
(1 −ηkλj)2"
E,0.9059334298118669,"=
T
log T ·
η1 2ℓ"
E,0.9066570188133141,"2
T
Y"
E,0.9073806078147613,"k=ℓ·
T
log T +1
(1 −ηkλj)2"
E,0.9081041968162084,"(G.24)
>
T
log T ·"
E,0.9088277858176556,"η1
λjη1 · 16T log T"
E,0.9095513748191028,"!2
T
Y"
E,0.91027496382055,"k=ℓ·
T
log T +1
(1 −ηkλj)2 = 1"
E,0.9109985528219972,"256 · log T T
· 1"
E,0.9117221418234442,"λ2
j
· T
Y"
E,0.9124457308248914,"k=ℓ·
T
log T +1
(1 −ηkλj)2 ≥1"
E,0.9131693198263386,"256 · log T T
· 1"
E,0.9138929088277858,"λ2
j
· "
E,0.914616497829233,"
1 − T
X"
E,0.9153400868306801,"k=ℓ·
T
log T +1
2ηkλj "
E,0.9160636758321273,"
=
1
256 · log T T
· 1"
E,0.9167872648335745,"λ2
j
· "
E,0.9175108538350217,"
1 −2λj T
X"
E,0.9182344428364689,"k=ℓ·
T
log T +1
ηk  
 = 1"
E,0.918958031837916,"256 · log T T
· 1"
E,0.9196816208393632,"λ2
j
· "
E,0.9204052098408104,"
1 −2λj"
E,0.9211287988422576,"log T −1
X i=ℓ"
E,0.9218523878437048,"(i+1)·
T
log T
X"
E,0.9225759768451519,"k=i·
T
log T +1
ηk  
"
E,0.9232995658465991,Published as a conference paper at ICLR 2022 = 1
E,0.9240231548480463,"256 · log T T
· 1"
E,0.9247467438494935,"λ2
j
· "
E,0.9254703328509407,"
1 −2λj"
E,0.9261939218523878,"log T −1
X i=ℓ"
E,0.926917510853835,"(i+1)·
T
log T
X"
E,0.9276410998552822,"k=i·
T
log T +1 η1"
I,0.9283646888567294,"2i  
 = 1"
I,0.9290882778581766,"256 · log T T
· 1"
I,0.9298118668596237,"λ2
j
· "
I,0.9305354558610709,1 −2λj
I,0.9312590448625181,"log T −1
X i=ℓ"
I,0.9319826338639653,"T
log T · η1"
I,0.9327062228654125,2i ! ≥1
I,0.9334298118668596,"256 · log T T
· 1"
I,0.9341534008683068,"λ2
j
·

1 −2λj ·
T
log T ·
η1
2ℓ−1 "
I,0.934876989869754,"(G.23)
≥
1
256 · log T T
· 1"
I,0.9356005788712012,"λ2
j
· "
I,0.9363241678726484,"1 −4λj ·
T
log T ·
η1
λjη1 ·
8T
log T ! = 1"
I,0.9370477568740955,"256 · log T T
· 1"
I,0.9377713458755427,"λ2
j
· 1 2 = 1"
I,0.9384949348769899,"512 · log T T
· 1 λ2
j"
I,0.9392185238784371,"Here the ﬁrst inequality is obtained by focusing variance generated in interval ℓonly. The second
inequality utilizes τ ≥ℓ· T/ log T. The fourth inequality is entailed by (1 −a1)(1 −a2) =
1 −a1 −a2 + a1a2 ≥1 −a1 −a2 for ∀a1, a2 ∈[0, 1], where by mathematical induction, we can
extend this inequality for more terms Qn
i=1(1 −ai) ≥1 −Pn
i=1 ai as long as Pn
i=1 ai ≤1. The
ﬁfth inequality comes from Plog T −1
i=ℓ
1/2i ≤P∞
i=ℓ1/2i = 1/2ℓ−1."
I,0.9399421128798843,b) For ∀j ∈B:
I,0.9406657018813314,"Step decay’s bottleneck will occur in the bias term. Since j ∈B, it follows from the deﬁnition of B
in Eqn.(G.22),"
I,0.9413892908827786,λj ≤log T
I,0.9421128798842258,"8η1T
=⇒
η1λj ≤log T 8T ,"
I,0.942836468885673,we have
I,0.9435600578871202,"bj =λj (w0,j −w∗,j)2
T
Y"
I,0.9442836468885673,"k=0
(1 −ηkλj)2"
I,0.9450072358900145,"≥λj (w0,j −w∗,j)2 ·  1 − T
X"
I,0.9457308248914617,"k=0
2ηkλj !"
I,0.9464544138929089,"= λj (w0,j −w∗,j)2 ·  1 − T
X"
I,0.947178002894356,"k=1
2ηkλj !"
I,0.9479015918958031,"=λj (w0,j −w∗,j)2 · "
I,0.9486251808972503,"
1 −"
I,0.9493487698986975,"log T −1
X i=0"
I,0.9500723589001447,"(i+1)·
T
log T
X"
I,0.9507959479015919,"k=i·
T
log T +1
2ηkλj  
"
I,0.951519536903039,"=λj (w0,j −w∗,j)2 · "
I,0.9522431259044862,"
1 −"
I,0.9529667149059334,"log T −1
X i=0"
I,0.9536903039073806,"(i+1)·
T
log T
X"
I,0.9544138929088278,"k=i·
T
log T +1"
I,0.9551374819102749,"η1λj
2i−1  
"
I,0.9558610709117221,"=λj (w0,j −w∗,j)2 · "
I,0.9565846599131693,1 −η1λj
I,0.9573082489146165,"log T −1
X i=0"
I,0.9580318379160637,"T
log T ·
1
2i−1 !"
I,0.9587554269175108,"≥λj (w0,j −w∗,j)2 ·

1 −4η1λj ·
T
log T "
I,0.959479015918958,"≥λj (w0,j −w∗,j)2 ·

1 −4 · log T"
T,0.9602026049204052,"8T
·
T
log T "
T,0.9609261939218524,"=λj (w0,j −w∗,j)2 · 1 2,"
T,0.9616497829232996,"where the ﬁrst inequality is caused by (1 −a1)(1 −a2) = 1 −a1 −a2 + a1a2 ≥1 −a1 −a2
for ∀a1, a2 ∈[0, 1] and applying mathematical induction for {an} to obtain Qn
i=1(1 −ai) ≥"
T,0.9623733719247467,Published as a conference paper at ICLR 2022
T,0.9630969609261939,"1 −Pn
i=1 ai as long as Pn
i=1 ai ≤1.
The second equality is because η0 = 0.
The sec-
ond inequality comes from Plog T −1
i=0
1/2i−1 ≤P∞
i=0 1/2i−1 = 4. The last inequality follows
η1λj ≤log T/(8T)."
T,0.9638205499276411,"From assumption (3), we know λj (w0,j −w∗,j)2 > 0. Furthermore, as we require"
T,0.9645441389290883,"T
log T ≥
1
256 ·
σ2"
T,0.9652677279305355,"minj λj (w0,j −w∗,j)2"
T,0.9659913169319826,"in Eqn. (G.19),"
T,0.9667149059334298,"bj ≥λj (w0,j −w∗,j)2 · 1"
T,0.967438494934877,"2 ≥min
j
λj (w0,j −w∗,j)2 · 1"
T,0.9681620839363242,"2 ≥
1
512 · σ2"
T,0.9688856729377714,T · log T.
T,0.9696092619392185,"In sum, we have obtained"
T,0.9703328509406657,"∀j ∈A,
vj ≥
1
512 · log T T
· 1 λ2
j"
T,0.9710564399421129,"∀j ∈B,
bj ≥
1
512 · σ2"
T,0.9717800289435601,T · log T
T,0.9725036179450073,"By combining with Eqn. (G.21), we have"
T,0.9732272069464544,"E [f(wT +1 −f(w∗)] =1 2    
d
X"
T,0.9739507959479016,"j=1
bj  + "
T,0.9746743849493488,"σ2
d
X"
T,0.975397973950796,"j=1
λ2
jvj     ≥1 2    X"
T,0.9761215629522432,"j∈B
bj  +  σ2 X"
T,0.9768451519536903,"j∈A
λ2
jvj    "
T,0.9775687409551375,"≥|B| ·

1
1024 · σ2"
T,0.9782923299565847,"T · log T

+
X"
T,0.9790159189580319,"j∈A
σ2 · λ2
j ·
1
1024 · log T T
· 1 λ2
j"
T,0.9797395079594791,"=|B| ·

1
1024 · σ2"
T,0.9804630969609262,"T · log T

+ |A| ·

1
1024 · σ2"
T,0.9811866859623734,"T · log T
"
T,0.9819102749638206,"= (|A| + |B|) ·

1
1024 · σ2"
T,0.9826338639652678,"T · log T
"
T,0.983357452966715,"=d ·
1
1024 · σ2"
T,0.984081041968162,T · log T
T,0.9848046309696092,"=Ω
dσ2"
T,0.9855282199710564,"T
· log T

,"
T,0.9862518089725036,"where the ﬁrst inequality is because both the bias and variance terms are non-negative, given bj =
λj (w0,j −w∗,j)2 QT
k=0(1 −ηkλj)2 ≥0 and vj = PT
τ=0 η2
τ
QT
k=τ+1(1 −ηkλj)2 ≥0."
T,0.9869753979739508,"Remark 3. The requirement T/ log T ≥1/256 · σ2/

minj λj (w0,j −w∗,j)2
and assumption"
T,0.9876989869753979,"λj (w0,j −w∗,j)2 ̸= 0 for ∀j = 1, 2, . . . , d can be replaced with T/ log T > 1/(8η1µ), since in
that case j ∈A holds for ∀j = 1, 2, . . . , d and B = ∅. In particular, if η1 = 1/L, this requirement
on T becomes T/ log T ≥κ/8."
T,0.9884225759768451,"G.5
THE REASON OF USING ASSUMPTION (1.7)"
T,0.9891461649782923,"In all of our analysis, we employ assumption (1.7)"
T,0.9898697539797395,"Eξ

ntn⊤
t

⪯σ2H
where nt = Hwt −b −(H(ξ)wt −b(ξ))"
T,0.9905933429811867,Published as a conference paper at ICLR 2022
T,0.9913169319826338,"which is the same as the one in Appendix C, Theorem 13 of Ge et al. (2019). This key theorem is
the major difference between our work and Ge et al. (2019), which directly entails its main theorem
by instantiating σ with speciﬁc values in its assumptions."
T,0.992040520984081,"On the other hand, it is possible to use the assumptions in Ge et al. (2019); Bach & Moulines (2013);
Jain et al. (2016) instead of our assumption (1.7) for least square regression:"
T,0.9927641099855282,"min
w f(w)
where f(w) ≜1"
T,0.9934876989869754,"2E(x,y)∼D

(y −w⊤x)2
(G.25)"
T,0.9942112879884226,"y = w⊤
∗x + ϵ with ϵ satisfying E(x,y)∼D

ϵ2xx⊤
⪯σ2H for ∀(x, y) ∼D
(G.26)"
T,0.9949348769898697,"E

||x||2xx⊤
⪯R2H
(G.27)"
T,0.9956584659913169,"By combining our Lemma 1 and assumption (1.7) with Lemma 5, Lemma 8 and Lemma 9 in Ge
et al. (2019), one can obtain similar results in this paper with their assumptions. For simplicity, we
just use assumption (1.7) here."
T,0.9963820549927641,"H
RELATIONSHIP WITH (STOCHASTIC) NEWTON’S METHOD"
T,0.9971056439942113,"Our motivation in Proposition 1 shares a similar idea with (stochastic) Newton’s method on quadratic
objectives"
T,0.9978292329956585,"wt+1 =wt −ηtH−1∇f(wt, ξ),"
T,0.9985528219971056,"where the parameters are also updated coordinately in the “rotated space”, i.e. given H = UΛU ⊤
and w′ = U ⊤w. In particular, when the Hessian H is diagonal and ηt = 1/(t + 1), the update
formula is exactly the same as the one for Proposition 1."
T,0.9992764109985528,"Despite of this similarity, our method differ from Newton method’s and its practical variants in sev-
eral aspects. First of all, our method focuses on learning rate schedulers and is a ﬁrst-order method.
This property is especially salient when we consider eigencurve’s derivatives in Section 4.3:
only hyperparameter search is needed, just like other common learning rate schedulers. In addition,
most second-order methods, e.g.
Schraudolph (2002); Erdogdu & Montanari (2015); Grosse &
Martens (2016); Byrd et al. (2016); Botev et al. (2017); Huang et al. (2020); Yang et al. (2021), ap-
proximates the Hessian matrix or the Hessian inverse and exploits the curvature information, while
eigencurve only utilizes the rough estimation of the Hessian spectrum. On top of that, this es-
timation is only an one-time effect and can be even further removed for similar models. These key
differences highlight eigencurve’s advantages over most second-order methods in practice."
