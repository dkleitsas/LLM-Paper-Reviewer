Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010570824524312897,"In the quantized network, its gradient shows vanishing except for non-
differentiable points. The network thus cannot be learned by the standard back-
propagation, so that an alternative approach called Straight Through Estimator
(STE), which replaces the part of the gradient with a simple differentiable func-
tion, is used. While STE is known to work well for learning the quantized network
empirically, it has not been established theoretically. A recent study by Yin et al.
(2019) has provided theoretical support for STE. However, its justiï¬cation is still
limited to the model in the one-hidden layer network with the binary activation
where Gaussian generates the input data, and the true labels are output from the
teacher network with the same binary network architecture. In this paper, we dis-
cuss the effectiveness of STEs in more general situations without assuming the
shape of the input distribution and the labels. By considering the scale symmetry
of the network and speciï¬c properties of the STEs, we ï¬nd that STE with clipped
Relu is superior to STEs with identity function and vanilla Relu. The clipped
Relu STE, which breaks the scale symmetry, may pick up one of the local minima
degenerated in scales, while the identity STE and vanilla Relu STE, which keep
the scale symmetry, may not pick it up. To conï¬rm this observation, we further
present an analysis of a simple misspeciï¬ed model as an example. We ï¬nd that
all the stationary points are identical with the vanishing points of the cRelu STE
gradient, while some of them are not identical with the vanishing points of the
identity and Relu STE. Finally we have numerically conï¬rmed the observation
for the mixture Gaussian model with various teacher network."
INTRODUCTION,0.0021141649048625794,"1
INTRODUCTION"
INTRODUCTION,0.003171247357293869,"Quantization of the weights and the activations is a promising technique to save the memory and
accelerate the inference speed in deep neural networks (DNNs) which have been getting wider and
deeper in recent years. There are two main approaches of the quantization for DNNs (Krishnamoor-
thi (2018)): Post-training quantization (PTQ) and quantization-aware training (QAT). In the PTQ,
the pre-trained networks are simply quantized without re-training the model. This approach allows
us to achieve the nearly ï¬‚oating point accuracy at 8-bits, while below 8-bits, this results in signiï¬cant
accuracy degradation. Although there have been recent attempts to alleviate the accuracy degrada-
tion in PTQ (Banner et al. (2018); Choukroun et al. (2019); Zhao et al. (2019); Kryzhanovskiy et al.
(2021)), the QAT, where the quantized weights and activations are trained (e.g., Zhou et al. (2016);
Hubara et al. (2017); Rastegari et al. (2016)), usually leads to better accuracy."
INTRODUCTION,0.004228329809725159,"The difï¬culty in QAT is that the weights and the activations are discretized, and intrinsically non-
differentiable. If we take the derivatives forcibly, they either vanish or diverge. To avoid this prob-
lem, we replace them with the derivatives of some differentiable function in the backward pass only,
called the Straight-Through Estimator (STE) (Hinton et al. (2012); Bengio et al. (2013)). Since the
replacement leads to bias, it is not always possible to learn the network successfully. However this
approach can be applied to the low bits below 8-bits with tolerable accuracy degradation (Zhou et al.
(2016); Choi et al. (2018); Esser et al. (2019); Bhalgat et al. (2020))."
INTRODUCTION,0.005285412262156448,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006342494714587738,"Originally, STE was introduced as the replacement with the derivative of the identity function by
Hinton et al. (2012). Later, the term â€STEâ€ has been extensively used as the replacement with vari-
ous functions. In the binary network, Bengio et al. (2013) studied the replacement with the derivative
of sigmoid function as well as the original STE, and Hubara et al. (2017) used the derivative of the
identity function clipped in the region of |x| â‰¤1. In low-bits network, Zhou et al. (2016) used the
quantized gradient after the replacement with the identity function, and Choi et al. (2018), Esser
et al. (2019), and Bhalgat et al. (2020) used the the derivative of the clipped Relu (cRelu) for leaning
the step size."
MAIN CONTRIBUTION,0.007399577167019027,"1.1
MAIN CONTRIBUTION"
MAIN CONTRIBUTION,0.008456659619450317,"In this paper, we refer to the proxy of the gradient as the STE gradient 1. Although STE has been
much success in training quantized DNNs empirically, the theoretical justiï¬cation is very limited to
the speciï¬c situations. It also remains unclear what differentiable functions should be used for the
STE gradient. Our aim is to shed light on a new factor to determine the functions."
MAIN CONTRIBUTION,0.009513742071881607,"Without assuming the shape of the input distribution and the loss function, we ï¬rst discuss the
properties of the three STEs, identity STE, Relu STE, and cRelu STE in one-hidden-layer network
with binary activation from the perspective of the intrinsic symmetry. We ï¬nd that because the
identity STE and Relu STE keep the scale symmetry, the STE gradients should be zero for any scale
to converge to the true (local) minimum of the loss function, while in the case of the cRelu STE,
which breaks the scale symmetry, even if its gradient is not zero at the minimum at some scale and
it becomes zero at other scale, the cRelu STE converges to the minimum. Therefore, the back-
propagation using the cRelu STE is most likely to converge to the minimum of the loss function
among three STEs. This result reveals differences between Relu STE and cRelu STE, which could
not be found in the models by Yin et al. (2019) and Long et al. (2021)."
MAIN CONTRIBUTION,0.010570824524312896,"Recently, Kunin et al. (2020) discussed the symmetry embedded in non-quantized neural networks,
and the effect of its breaking such as the discretization, the weight decay, and the stochasticity during
training. We discuss the new effect of breaking symmetry by the proxy of the gradient in training
the quantized neural networks."
MAIN CONTRIBUTION,0.011627906976744186,"Next, to conï¬rm this observation, we have studied a similar model of the Gaussian input as discussed
in Yin et al. (2019) as an example. We have employed a misspeciï¬ed model suitable for most
practical situations. Unlike the previous study, the true labels are presumed to be generated by the
non-quantized network architecture. Consequently, we ï¬nd that ignoring the scale degeneracy for
the weight in front of the activation, all the stationary points are identical with the vanishing points
in the cRelu STE gradient, while some of them are not identical with the vanishing points in the
identity and the Relu STE gradients. In particular, at the global minimum point, both identity and
Relu STE gradients do not vanish. Finally, to conï¬rm it in more general case, we have numerically
studied the model of the mixture Gaussian input."
RELATED WORKS,0.012684989429175475,"1.2
RELATED WORKS"
RELATED WORKS,0.013742071881606765,"Recently, there have been a few theoretical studies on the justiï¬cation of the STEs. Shekhovtsov &
Yanush (2020) derived STE by the linearization of their proposed estimator in a stochastic binary
deep network where the noises are injected before the binary activation to be a smooth network.
Cheng et al. (2019) argued that STE can be interpreted as a projected Wasserstein gradient ï¬‚ow
under certain conditions. Yin et al. (2019) that inspired our study examined which of the three
STEâ€™s, identity STE, Relu STE, and cRelu STE, converges to the true minimum in one-hidden-
layer network with binarized activation and Gaussian input data. They clariï¬ed that if the labels
are obtained from a teacher network of the same architecture (Du et al. (2018)), all the three STEs
show the non-negative correlation with the population gradient, but only the identity STE gradient
does not become zero in a local minimum. This indicates that the back-propagation using either
Relu STE or cRelu STE may convergence to the local minimum, while it is impossible to show
the convergence to the local minimum using the identity STE. Furthermore, they showed that all
the three STEs gradients vanish at the global minimum, indicating that they may achieve the global
minimum independent of the choice of the STE, if we start with an appropriate initial value."
RELATED WORKS,0.014799154334038054,1It is called â€œcoarseâ€ gradient in Yin et al. (2019).
RELATED WORKS,0.015856236786469344,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.016913319238900635,"Very recently, Long et al. (2021) have discussed the justiï¬cation of a class of STEs with certain
monotonicity, which generalizes the Relu STE, in one-hidden-layer network for the hinge loss,
claiming that the STE gradients vanish at the global minimum."
NOTATIONS,0.017970401691331923,"1.3
NOTATIONS"
NOTATIONS,0.019027484143763214,"|Â·| denotes the Euclidean norm of a vector. A capital letter in bold denotes the matrix, e.g., Z, and
its component is given by the letter with lower indices, e.g., Zij. A small letter in bold denotes the
vector, e.g., g, and its component is given by the letter with lower indices, e.g., gj. ImÃ—m is the
m Ã— m identity matrix, and 1m is the m-dimensional vector of all ones."
"GENERAL DISCUSSION ON STES IN ONE-HIDDEN LAYER CNN WITH
BINARY ACTIVATION",0.0200845665961945,"2
GENERAL DISCUSSION ON STES IN ONE-HIDDEN LAYER CNN WITH
BINARY ACTIVATION"
PRELIMINARIES,0.021141649048625793,"2.1
PRELIMINARIES"
PRELIMINARIES,0.022198731501057084,The one-hidden layer convolutional neural network with a binary activation function is realized as
PRELIMINARIES,0.023255813953488372,"y(Z, v, w) = vT Ïƒ(Zw) =
X"
PRELIMINARIES,0.024312896405919663,"i
viÏƒ(
X"
PRELIMINARIES,0.02536997885835095,"j
Zijwj),
(1)"
PRELIMINARIES,0.026427061310782242,"where Ïƒ(Â·) is the step function,"
PRELIMINARIES,0.02748414376321353,"Ïƒ(x) =

1
x > 0,
0
x â‰¤0,
(2)"
PRELIMINARIES,0.02854122621564482,"Z âˆˆRmÃ—n is constructed by deividing the input data, e.g. an image, into m patches with size n,
and w âˆˆRn and v âˆˆRm are the trainable weights, i.e. kernels, in the ï¬rst and second layers,
respectively (Du et al. (2018)). We assume that Z is generated by a continuous distribution P(Z).
Due to the property of the step function, the network is invariant under the scale transformation for
w, w â†’kw (k > 0)."
PRELIMINARIES,0.02959830866807611,"Using the loss for each sample, â„“(y(v, w, Z); yâˆ—) with yâˆ—being the labels generated by the true
distribution q(yâˆ—|Z), we can express the population loss function as
L(v, w; yâˆ—) = EZ [â„“(y(v, w, Z); yâˆ—)] .
(3)"
BACK-PROPAGATION AND STE GRADIENTS,0.0306553911205074,"2.2
BACK-PROPAGATION AND STE GRADIENTS"
BACK-PROPAGATION AND STE GRADIENTS,0.03171247357293869,"The gradients of the loss for each sample w.r.t v and w can be formally expressed as
âˆ‚â„“
âˆ‚vi
= âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.03276955602536998,"âˆ‚y
âˆ‚y
âˆ‚vi
= âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.03382663847780127,"âˆ‚y Ïƒ(
X"
BACK-PROPAGATION AND STE GRADIENTS,0.03488372093023256,"j
Zijwj),
(4)"
BACK-PROPAGATION AND STE GRADIENTS,0.035940803382663845,"âˆ‚â„“
âˆ‚wj
=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.03699788583509514,"âˆ‚â„“
âˆ‚xi"
BACK-PROPAGATION AND STE GRADIENTS,0.03805496828752643,"âˆ‚xi
âˆ‚wj
=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.039112050739957716,"âˆ‚â„“
âˆ‚y viÎ´ ï£« ï£­X"
BACK-PROPAGATION AND STE GRADIENTS,0.040169133192389,"jâ€²
Zijâ€²wjâ€² ï£¶"
BACK-PROPAGATION AND STE GRADIENTS,0.0412262156448203,"ï£¸Zij,
(5)"
BACK-PROPAGATION AND STE GRADIENTS,0.042283298097251586,where xi = Ïƒ(P
BACK-PROPAGATION AND STE GRADIENTS,0.04334038054968287,"j Zijwj) and we used the property that the derivative of the step function is Diracâ€™s
delta function, dÏƒ(z)/dz = Î´(z). The gradient w.r.t v can be utilized for learning the network, while
it is impossible to use the gradient w.r.t w due to the presence of the delta function. Remarkably,
both population gradients w.r.t v and w are smooth functions due to its average over the continuous
distribution, EZ  âˆ‚â„“ âˆ‚vi"
BACK-PROPAGATION AND STE GRADIENTS,0.04439746300211417,"
=
Z ï£« ï£­Y"
BACK-PROPAGATION AND STE GRADIENTS,0.045454545454545456,"iâ€²,jâ€²
d ËœZiâ€²jâ€² ï£¶"
BACK-PROPAGATION AND STE GRADIENTS,0.046511627906976744,"ï£¸P(
n
ËœZiâ€²jâ€²
o
) âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.04756871035940803,"âˆ‚y Ïƒ

ËœZi1|w|

,
(6) EZ  âˆ‚â„“ âˆ‚wj"
BACK-PROPAGATION AND STE GRADIENTS,0.048625792811839326,"
=
1
|w| X"
BACK-PROPAGATION AND STE GRADIENTS,0.049682875264270614,"i
Fij(v, w, ËœZi1 = 0),"
BACK-PROPAGATION AND STE GRADIENTS,0.0507399577167019,"Fij(v, w, ËœZi1) â‰¡
Z ï£« ï£­
Y"
BACK-PROPAGATION AND STE GRADIENTS,0.05179704016913319,"(iâ€²,jâ€²)Ì¸=(i,1)
d ËœZiâ€²jâ€² ï£¶"
BACK-PROPAGATION AND STE GRADIENTS,0.052854122621564484,"ï£¸P(
n
ËœZiâ€²jâ€²
o
)"
BACK-PROPAGATION AND STE GRADIENTS,0.05391120507399577,"""
âˆ‚â„“
âˆ‚y vi
X"
BACK-PROPAGATION AND STE GRADIENTS,0.05496828752642706,"kâ€²
Oi
kâ€²j ËœZikâ€² # ,
(7)"
BACK-PROPAGATION AND STE GRADIENTS,0.056025369978858354,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.05708245243128964,"where we used the orthogonal transformation, ËœZikâ€² = P"
BACK-PROPAGATION AND STE GRADIENTS,0.05813953488372093,"jâ€² Oi
kâ€²jâ€²Zijâ€², Zijâ€² = P"
BACK-PROPAGATION AND STE GRADIENTS,0.05919661733615222,"kâ€² Oi
kâ€²jâ€² ËœZikâ€², with
Oi
1jâ€² = wjâ€²/|w| and P"
BACK-PROPAGATION AND STE GRADIENTS,0.06025369978858351,"jâ€² Oi
kâ€²jâ€²Oi
lâ€²jâ€² = Î´kâ€²,lâ€². We can easily see that the gradient w.r.t. w does not"
BACK-PROPAGATION AND STE GRADIENTS,0.0613107822410148,"have the component in w-direction, P"
BACK-PROPAGATION AND STE GRADIENTS,0.06236786469344609,"j wjEZ
h
âˆ‚â„“
âˆ‚wj"
BACK-PROPAGATION AND STE GRADIENTS,0.06342494714587738,"i
= 0. Its vanishing originally follows from
the scale symmetry of the network: Because the loss has the scale symmetry, its gradient in the
w-direction is obviously zero."
BACK-PROPAGATION AND STE GRADIENTS,0.06448202959830866,"Instead of the population gradient, we use the STE in practice, which replaces the delta function
with the derivative of some differentiable function ÂµSTE, to train the kernel w,"
BACK-PROPAGATION AND STE GRADIENTS,0.06553911205073996,"gSTE
j
(v, w, Z) â‰¡
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.06659619450317125,"âˆ‚â„“
âˆ‚y viÂµâ€²
STE ï£« ï£­X"
BACK-PROPAGATION AND STE GRADIENTS,0.06765327695560254,"jâ€²
Zijâ€²wjâ€² ï£¶"
BACK-PROPAGATION AND STE GRADIENTS,0.06871035940803383,"ï£¸Zij.
(8)"
BACK-PROPAGATION AND STE GRADIENTS,0.06976744186046512,"We consider three types of ÂµSTE(x): (i) the identity type ÂµSTE(x) = x , (ii) vanilla ReLU type
ÂµSTE(x) = xÏƒ(x) and (iii) the cReLU type ÂµSTE(x) = xÏƒ(x)Ïƒ(r âˆ’x) with the upper clipping
value r. The surrogate back-propagation using the STE is described in Algorithm 1 (Yin et al.
(2019))."
BACK-PROPAGATION AND STE GRADIENTS,0.0708245243128964,"Algorithm 1 Surrogate back-propagation using STE for learning one-hidden-layer CNN.
Input: initialization v0 âˆˆRm, w0 âˆˆRn, learning rate Î·."
BACK-PROPAGATION AND STE GRADIENTS,0.07188160676532769,"for t = 0, 1, , . . . do"
BACK-PROPAGATION AND STE GRADIENTS,0.07293868921775898,"vt+1 = vt âˆ’Î· EZ
 âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.07399577167019028,"âˆ‚v(vt, wt; Z)
"
BACK-PROPAGATION AND STE GRADIENTS,0.07505285412262157,"wt+1 = wt âˆ’Î·EZ

gSTE(vt, wt; Z)
"
BACK-PROPAGATION AND STE GRADIENTS,0.07610993657505286,end for
BACK-PROPAGATION AND STE GRADIENTS,0.07716701902748414,The average of the STE gradients over the input data is also expressed as
BACK-PROPAGATION AND STE GRADIENTS,0.07822410147991543,"EZ

gSTE
j
(v, w; Z)

=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.07928118393234672,"Z
d ËœZi1Âµâ€²
STE

ËœZi1|w|

Fij(v, w, ËœZi1).
(9)"
BACK-PROPAGATION AND STE GRADIENTS,0.080338266384778,"Note that in general, the STE gradients have the non-zero component in the w-direction due to their
partial collapse of the form of the gradient, P"
BACK-PROPAGATION AND STE GRADIENTS,0.08139534883720931,"j wjEZ

gSTE
j
(v, w; Z)

Ì¸= 0."
BACK-PROPAGATION AND STE GRADIENTS,0.0824524312896406,"It is obvious from Eq.(9) that the only difference between the STE gradients with the identity func-
tion, Relu, and cRelu is the region of the integration over ËœZi1,"
BACK-PROPAGATION AND STE GRADIENTS,0.08350951374207188,"EZ

gA
j (v, w; Z)

=
X i Z"
BACK-PROPAGATION AND STE GRADIENTS,0.08456659619450317,"RA
d ËœZi1Fij(v, w, ËœZi1),
(10)"
BACK-PROPAGATION AND STE GRADIENTS,0.08562367864693446,"where A = id, Relu, cRelu, and the integration reigion RA are given as Rid = (âˆ’âˆ, +âˆ),
RRelu = [0, +âˆ), and RcRelu = [0,
r
|w|]."
BACK-PROPAGATION AND STE GRADIENTS,0.08668076109936575,"Following the scale invariance of Fij(v, w, ËœZi1) for w, Fiâ€²j(v, kw, ËœZiâ€²1) = Fiâ€²j(v, w, ËœZiâ€²1) (k >
0), the STE gradients have the following interesting features:"
BACK-PROPAGATION AND STE GRADIENTS,0.08773784355179703,"1. The identity and Relu STE gradients have scale invariance for w,"
BACK-PROPAGATION AND STE GRADIENTS,0.08879492600422834,"EZ
h
gid/Relu
j
(v, kw; Z)
i
= EZ
h
gid/Relu
j
(v, w; Z)
i
.
(11)"
BACK-PROPAGATION AND STE GRADIENTS,0.08985200845665962,"2. The cRelu STE gradient does not have scale invariance for w due to the clipping effect,"
BACK-PROPAGATION AND STE GRADIENTS,0.09090909090909091,"EZ

gcRelu
j
(v, kw, r; Z)

Ì¸= EZ

gcRelu
j
(v, w, r; Z)

,
(12)"
BACK-PROPAGATION AND STE GRADIENTS,0.0919661733615222,"where we have explicitly shown the dependence of the upper clipping value r.
3. Instead, the scale transformation of w for the cRelu STE gradient can be compensated by
that of r:"
BACK-PROPAGATION AND STE GRADIENTS,0.09302325581395349,"EZ

gcRelu
j
(v, w, kr; Z)

= EZ

gcRelu
j
(v, w/k, r; Z)

.
(13)"
BACK-PROPAGATION AND STE GRADIENTS,0.09408033826638477,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.09513742071881606,"If we take kr with ï¬xed r large enough to cover most of the distribution P(Z), the left-hand
side is identical with the Relu STE gradient. Here we deï¬ne the spread of the distribution
as Ï. When we redeï¬ne w/k as w on the right hand, the cRelu STE gradient for |w| < r/Ï
is approximately identical with the Relu STE gradient,"
BACK-PROPAGATION AND STE GRADIENTS,0.09619450317124736,"EZ

gcRelu
j
(v, w, r; Z)

â‰ƒEZ

gRelu
j
(v, w; Z)

for |w| < r/Ï.
(14)"
BACK-PROPAGATION AND STE GRADIENTS,0.09725158562367865,"This result is intuitively obvious: when w is small enough to keep most of the pre-activation
values in the clipped range, the cRelu gradient and the Relu gradient are approximately
identical."
BACK-PROPAGATION AND STE GRADIENTS,0.09830866807610994,"The important issue is that there is no guarantee that the weights obtained by the STE back-
propagation are the ones at the (local) minimum of the loss function. In other words, at the stationary
points, (v, w) = (vs, ws), deï¬ned as the vanishing points of the population gradient, EZ  âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.09936575052854123,"âˆ‚vi
(v = vs, w = ws)

= 0,
EZ  âˆ‚â„“"
BACK-PROPAGATION AND STE GRADIENTS,0.10042283298097252,"âˆ‚wj
(v = vs, w = ws)

= 0,
(15)"
BACK-PROPAGATION AND STE GRADIENTS,0.1014799154334038,"the STE gradient is not zero in general. Here we consider the local minimum point that the STE
gradient does not vanish,"
BACK-PROPAGATION AND STE GRADIENTS,0.10253699788583509,"EZ

gSTE
j
(vs, ws; Z)

Ì¸= 0.
(16)"
BACK-PROPAGATION AND STE GRADIENTS,0.10359408033826638,"Note that (v, w) = (vs, kws) for any k > 0 is also the local minimum point due to the scale
invariance of the loss. In the case of the Relu/identity STEs, their gradient has the same ï¬nite value
for any scales due to the scale invariance shown in Eq.(11),"
BACK-PROPAGATION AND STE GRADIENTS,0.10465116279069768,"EZ
h
gid/Relu
j
(vs, kws; Z)
i
= EZ
h
gid/Relu
j
(vs, ws; Z)
i
Ì¸= 0.
(17)"
BACK-PROPAGATION AND STE GRADIENTS,0.10570824524312897,"In the case of the cRelu STE, however, the value of its gradient changes as the scale changes due to
the breaking of the scale symmetry shown in Eq.(12). If it crosses zero as a function of scale,"
BACK-PROPAGATION AND STE GRADIENTS,0.10676532769556026,"EZ

gcRelu
j
(vs, k0ws, r; Z)

= 0 for some k0,
(18)"
BACK-PROPAGATION AND STE GRADIENTS,0.10782241014799154,"the back propagation using cRelu STE can converge at the local minimum as was done by using
the population gradient. Therefore, we conjecture that due to breaking scale symmetry, the back-
propagation using cRelu STE is the most likely to achieve the (local) minimum of the loss function
in the three STEs. This also implies that the cRelu STE is less biased than the other. We conï¬rm
the conjecture analytically for the Gaussian input with Relu-type teacher network in Sec.3, and
numerically for the mixture Gaussian input with various teacher network in Sec. 4."
BACK-PROPAGATION AND STE GRADIENTS,0.10887949260042283,"Since the above discussion is focused only on symmetry, the conjecture can be generalized to net-
works with more symmetries in a straightforward way. In that case, if we employ an STE that breaks
all of those symmetries instead of cRelu STE, it is more likely to achieve the minimum."
BACK-PROPAGATION AND STE GRADIENTS,0.10993657505285412,"The similar mechanism can be also found in physical phenomena. For instance, this is the case of
the ferromagnet, which has rotational symmetry in energy (or Hamiltonian). In the system, if we
impose the magnetic ï¬eld, which breaks the rotational symmetry, the spin states, corresponding to
minimum points, become aligned with the same direction."
BACK-PROPAGATION AND STE GRADIENTS,0.1109936575052854,"3
ONE-HIDDEN LAYER CNN WITH BINARY ACTIVATION: GAUSSIAN INPUT
AND LABELS GENERATED BY NON-QUANTIZED RELU NETWORK"
BACK-PROPAGATION AND STE GRADIENTS,0.11205073995771671,We consider the simple model similar to Yin et al. (2019) given by the Gaussian input with
BACK-PROPAGATION AND STE GRADIENTS,0.113107822410148,"the variance Ë†Ïƒ2, Z âˆ¼P(Z) =

1
âˆš"
BACK-PROPAGATION AND STE GRADIENTS,0.11416490486257928,"2Ï€Ë†Ïƒ2
mn
eâˆ’P i,j"
BACK-PROPAGATION AND STE GRADIENTS,0.11522198731501057,"Z2
ij
2Ë†Ïƒ2 . We use the squared loss function,"
BACK-PROPAGATION AND STE GRADIENTS,0.11627906976744186,"â„“(y(v, w, Z); yâˆ—(vâˆ—, wâˆ—, Z)) =
1
2 (y âˆ’yâˆ—)2 . Unlike in Yin et al. (2019), the true labels are as-
sumed to be generated by the non-quantized Relu network,"
BACK-PROPAGATION AND STE GRADIENTS,0.11733615221987315,"yâˆ—(vâˆ—, wâˆ—, Z) =
X"
BACK-PROPAGATION AND STE GRADIENTS,0.11839323467230443,"i
vâˆ—
i fRelu ï£« ï£­X"
BACK-PROPAGATION AND STE GRADIENTS,0.11945031712473574,"j
Zijwâˆ—
j ï£¶"
BACK-PROPAGATION AND STE GRADIENTS,0.12050739957716702,"ï£¸,
(19)"
BACK-PROPAGATION AND STE GRADIENTS,0.12156448202959831,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.1226215644820296,with the Relu activation deï¬ned as fRelu(x) = xÏƒ(x).
BACK-PROPAGATION AND STE GRADIENTS,0.12367864693446089,"Note that the models are misspeciï¬ed, i.e., the population loss cannnot be zero even at the global
minimum. In fact, it is inferred from the accuracy degradation in most practical situations (Hubara
et al. (2017)) that the binary networks are considered as misspeciï¬ed models. On the other hand, if
the true labels are generated by the same binarized architecture as was used in Yin et al. (2019), the
network becomes a speciï¬ed model."
BACK-PROPAGATION AND STE GRADIENTS,0.12473572938689217,"As shown below, this misspeciï¬ed model leads to a more striking difference between three STEs
than the speciï¬ed model. Particularly, the behaviors of three STE gradients at the global minimum
are completely different from each other."
BACK-PROPAGATION AND STE GRADIENTS,0.12579281183932348,"3.1
POPULATION LOSS, ITS GRADIENT AND STATIONARY POINTS"
BACK-PROPAGATION AND STE GRADIENTS,0.12684989429175475,"The population loss L(v, w; vâˆ—, wâˆ—) is derived in Appendix A.1 as"
BACK-PROPAGATION AND STE GRADIENTS,0.12790697674418605,"L(v, w; vâˆ—, wâˆ—) = EZ [â„“(v, w, Z; vâˆ—, wâˆ—)] = 1"
BACK-PROPAGATION AND STE GRADIENTS,0.12896405919661733,"8vT  
ImÃ—m + 1m1T
m

v âˆ’Ë†Ïƒ|wâˆ—| 2
âˆš"
BACK-PROPAGATION AND STE GRADIENTS,0.13002114164904863,"2Ï€ vT  
cos Ï•ImÃ—m + 1m1T
m

vâˆ— + Ë†Ïƒ2"
BACK-PROPAGATION AND STE GRADIENTS,0.13107822410147993,"4Ï€ |wâˆ—|2vâˆ—T  
(Ï€ âˆ’1)ImÃ—m + 1m1T
m

vâˆ—
(20)"
BACK-PROPAGATION AND STE GRADIENTS,0.1321353065539112,"where Ï• is the angle between wâˆ—and w. Note that the loss is scale invariant for w, and thus its
derivative satisï¬es ew Â· âˆ‚L"
BACK-PROPAGATION AND STE GRADIENTS,0.1331923890063425,âˆ‚w = 0. The stationary points are given by
BACK-PROPAGATION AND STE GRADIENTS,0.13424947145877378,"âˆ‚L
âˆ‚v = 0 â‡”v = 2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
BACK-PROPAGATION AND STE GRADIENTS,0.13530655391120508,"
cos Ï•ImÃ—m + 1 âˆ’cos Ï•"
BACK-PROPAGATION AND STE GRADIENTS,0.13636363636363635,"m + 1 1m1T
m"
BACK-PROPAGATION AND STE GRADIENTS,0.13742071881606766,"
vâˆ—
(21)"
BACK-PROPAGATION AND STE GRADIENTS,0.13847780126849896,"âˆ‚L
âˆ‚w = 0 â‡”vT vâˆ—= 0 or Ï• = 0 or Ï• = Ï€
(22)"
BACK-PROPAGATION AND STE GRADIENTS,0.13953488372093023,"where we used
 
ImÃ—m + 1m1T
m
âˆ’1 = ImÃ—m âˆ’
1
m+11m1T
m."
BACK-PROPAGATION AND STE GRADIENTS,0.14059196617336153,There are three stationary points 2 as classiï¬ed in Appendix A.2:
BACK-PROPAGATION AND STE GRADIENTS,0.1416490486257928,"1. middle saddle point given by v = Â¯v â‰¡
2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
BACK-PROPAGATION AND STE GRADIENTS,0.1427061310782241,"
cos Â¯Ï•ImÃ—m + 1âˆ’cos Â¯Ï•"
BACK-PROPAGATION AND STE GRADIENTS,0.14376321353065538,"m+1 1m1T
m

vâˆ—, Ï• ="
BACK-PROPAGATION AND STE GRADIENTS,0.14482029598308668,"Â¯Ï• â‰¡arccos

(vâˆ—T 1m)
2"
BACK-PROPAGATION AND STE GRADIENTS,0.14587737843551796,âˆ’(m+1)(vâˆ—)2+(vâˆ—T 1m)2
BACK-PROPAGATION AND STE GRADIENTS,0.14693446088794926,"
, which satisï¬es Â¯vT vâˆ—= 0. This exists if and only"
BACK-PROPAGATION AND STE GRADIENTS,0.14799154334038056,"if (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2."
BACK-PROPAGATION AND STE GRADIENTS,0.14904862579281183,"2. local minimum if (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2 , otherwise saddle point given by v ="
BACK-PROPAGATION AND STE GRADIENTS,0.15010570824524314,"vÏ€ â‰¡2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
BACK-PROPAGATION AND STE GRADIENTS,0.1511627906976744,"
âˆ’ImÃ—m +
2
m+11m1T
m

vâˆ—, Ï• = Ï€."
BACK-PROPAGATION AND STE GRADIENTS,0.1522198731501057,"3. global minimum given by v = v0 â‰¡
2Ë†Ïƒ|wâˆ—|
âˆš"
BACK-PROPAGATION AND STE GRADIENTS,0.15327695560253699,"2Ï€ vâˆ—, Ï• = 0. At the global minimum, the"
BACK-PROPAGATION AND STE GRADIENTS,0.1543340380549683,"population loss does not become zero, L(v = v0, w = wâˆ—) = Ë†Ïƒ2|wâˆ—|2(Ï€âˆ’2)"
BACK-PROPAGATION AND STE GRADIENTS,0.1553911205073996,"4Ï€
vâˆ—T v."
STE GRADIENT,0.15644820295983086,"3.2
STE GRADIENT"
STE GRADIENT,0.15750528541226216,The STE gradient is given by
STE GRADIENT,0.15856236786469344,"gSTE
j
=
X"
STE GRADIENT,0.15961945031712474,"i
viÂµâ€²
STE ï£« ï£­X"
STE GRADIENT,0.160676532769556,"jâ€²
Zijâ€²wjâ€² ï£¶ ï£¸Zij Ã— ï£« ï£­X"
STE GRADIENT,0.16173361522198731,"iâ€²
viâ€²Ïƒ ï£« ï£­X"
STE GRADIENT,0.16279069767441862,"jâ€²
Ziâ€²jâ€²wjâ€² ï£¶ ï£¸âˆ’
X"
STE GRADIENT,0.1638477801268499,"iâ€²
vâˆ—
iâ€²fRelu ï£« ï£­X"
STE GRADIENT,0.1649048625792812,"jâ€²
Ziâ€²jâ€²wâˆ—
jâ€² ï£¶ ï£¸ ï£¶"
STE GRADIENT,0.16596194503171247,"ï£¸.
(23)"
STE GRADIENT,0.16701902748414377,"2The deï¬nition of the stationary points is not mathematically rigorous in our paper. Even if points are
non-differentiable, but the gradient gives zero at the points in the one-side limit, we call them the stationary
points."
STE GRADIENT,0.16807610993657504,Under review as a conference paper at ICLR 2022
STE GRADIENT,0.16913319238900634,"The converging points by the back-propagation in Algorithm 1 are characterized by vanishing the
gradients, EZ  âˆ‚â„“"
STE GRADIENT,0.17019027484143764,"âˆ‚vi
(v, w; Z)

= 0, EZ

gSTE
j
(v, w; Z)

= 0.
(24)"
STE GRADIENT,0.17124735729386892,"We discuss the explicit form of the three STE gradients and the associated vanishing points. The
detailed derivation of the STE gradients is shown in Appendix A.3-A.5."
IDENTITY STE,0.17230443974630022,"3.2.1
IDENTITY STE"
IDENTITY STE,0.1733615221987315,The identity STE gradient is given by
IDENTITY STE,0.1744186046511628,"EZ

gid
=
Ë†Ïƒ
âˆš"
IDENTITY STE,0.17547568710359407,"2Ï€
w
|w|
 
vT v

âˆ’Ë†Ïƒ2"
IDENTITY STE,0.17653276955602537,"2 wâˆ— 
vT vâˆ—
(25)"
IDENTITY STE,0.17758985200845667,"Combined with Eq.(21), we ï¬nd that the equation in Eq.(24) has no solution. Therefore, if we use
the back-propagation by identity STE, it can not converge to any points."
RELU STE,0.17864693446088795,"3.2.2
RELU STE"
RELU STE,0.17970401691331925,The Relu STE gradient is expressed as
RELU STE,0.18076109936575052,"EZ [grelu] =
Ë†Ïƒ
2
âˆš"
RELU STE,0.18181818181818182,"2Ï€
w
|w|

vT  
ImÃ—m + 1m1T
m

v

âˆ’Ë†Ïƒ2"
RELU STE,0.1828752642706131,"2Ï€
|wâˆ—|"
RELU STE,0.1839323467230444,"|w| w

vT  
âˆ’ImÃ—m + 1m1T
m

vâˆ—"
RELU STE,0.1849894291754757,"âˆ’Ë†Ïƒ2

(Ï€ âˆ’Ï•) wâˆ—+ |wâˆ—| sin Ï•"
RELU STE,0.18604651162790697,"|w|
w
  
vT vâˆ—"
RELU STE,0.18710359408033828,"2Ï€
.
(26)"
RELU STE,0.18816067653276955,"Combined with Eq.(21), the gradients vanish at the following points:"
RELU STE,0.18921775898520085,"1. If vâˆ—satisï¬es (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2 , Ï• = Â¯Ï• = arccos

(vâˆ—T 1m)
2"
RELU STE,0.19027484143763213,"âˆ’(m+1)(vâˆ—)2+(vâˆ—T 1m)2 
,"
RELU STE,0.19133192389006343,"v = Â¯v = 2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
RELU STE,0.19238900634249473,"
cos Ï•ImÃ—m + 1âˆ’cos Ï•"
RELU STE,0.193446088794926,"m+1 1m1T
m

vâˆ—."
RELU STE,0.1945031712473573,"2. Ï• = Ï€, v = Â¯v = 2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
RELU STE,0.19556025369978858,"
âˆ’ImÃ—m +
2
m+11m1T
m

."
RELU STE,0.19661733615221988,"We ï¬nd the solutions are identical with two stationary points shown in item 1 and 2 in Sec.3.1,
leading to the the saddle point or local minimum, while the global minimum point at Ï• = 0, v = v0
cannot be obtained. Therefore, by using the back-propagation with Relu STE, it always converges
to Ï• = Ï€ if (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2 , and it does not converge to any points, otherwise."
CLIPPED RELU STE,0.19767441860465115,"3.2.3
CLIPPED RELU STE"
CLIPPED RELU STE,0.19873150105708245,The cRelu STE is given by
CLIPPED RELU STE,0.19978858350951373,"EZ [gcrelu] =
Ë†Ïƒ
2
âˆš"
CLIPPED RELU STE,0.20084566596194503,"2Ï€
w
|w|"
CLIPPED RELU STE,0.20190274841437633,"
1 âˆ’eâˆ’1"
CLIPPED RELU STE,0.2029598308668076,"2(
r
Ë†Ïƒ|w|)
2
vT  
ImÃ—m + 1m1T
m

v âˆ’Ë†Ïƒ2"
CLIPPED RELU STE,0.2040169133192389,"2Ï€
|wâˆ—| |w|"
CLIPPED RELU STE,0.20507399577167018,"
1 âˆ’eâˆ’1"
CLIPPED RELU STE,0.20613107822410148,"2(
r
Ë†Ïƒ|w|)
2
w

vT  
âˆ’ImÃ—m + 1m1T
m

vâˆ—"
CLIPPED RELU STE,0.20718816067653276,"âˆ’Ë†Ïƒ2

C(w, Ï•) wâˆ—"
CLIPPED RELU STE,0.20824524312896406,"|wâˆ—| + S(w, Ï•)

1
sin Ï•
w
|w| âˆ’cot Ï• wâˆ— |wâˆ—|"
CLIPPED RELU STE,0.20930232558139536,"  
vT vâˆ—
,
(27)"
CLIPPED RELU STE,0.21035940803382663,"where C(|w|, Ï•) and S(|w|, Ï•) are given in Eq.(106).
At Ï• = 0, Ï€ they are simpliï¬ed as"
CLIPPED RELU STE,0.21141649048625794,"C(|w|, 0) =
|wâˆ—|"
CLIPPED RELU STE,0.2124735729386892,"2Ï€

Ï€ âˆ’
r
Ë†Ïƒ|w|eâˆ’1"
CLIPPED RELU STE,0.2135306553911205,"2(
r
Ë†Ïƒ|w|)
2âˆš"
CLIPPED RELU STE,0.21458773784355178,"2Ï€ âˆ’Ï€erfc

r
âˆš"
CLIPPED RELU STE,0.2156448202959831,2Ë†Ïƒ|w|
CLIPPED RELU STE,0.2167019027484144,"
, C(|w|, Ï€) = S(|w|, Ï€) ="
CLIPPED RELU STE,0.21775898520084566,"S(|w|, 0) = 0."
CLIPPED RELU STE,0.21881606765327696,"Remarkably, the cRelu STE gradient is proportional to the Relu STE gradient in the case of vT vâˆ—=
0 or Ï• = Ï€:"
CLIPPED RELU STE,0.21987315010570824,"EZ [gcrelu] |vT vâˆ—=0 or Ï•=Ï€ =

1 âˆ’eâˆ’1"
CLIPPED RELU STE,0.22093023255813954,"2(
r
Ë†Ïƒ|w|)
2
EZ [grelu] |vT vâˆ—=0 or Ï•=Ï€.
(28)"
CLIPPED RELU STE,0.2219873150105708,Under review as a conference paper at ICLR 2022
CLIPPED RELU STE,0.22304439746300211,"As shown in Sec.3.2.2, all the vanishing points in the Relu STE can be found at vT vâˆ—= 0 or Ï• = Ï€,
leading to the saddle point or the local minimum, so that they are also found in the vanishing points
of cRelu STE. On the other hand, the behavior of cRelu STE gradient at Ï• = 0 is not related to that
of Relu STE gradient. The cRelu STE gradient at Ï• = 0 is written as"
CLIPPED RELU STE,0.22410147991543342,"EZ [gcrelu] =
Ë†Ïƒ
2
âˆš"
CLIPPED RELU STE,0.2251585623678647,"2Ï€ wâˆ—
1 âˆ’eâˆ’1"
CLIPPED RELU STE,0.226215644820296,"2(
r
Ïƒ|w|)
2
vT

1
|wâˆ—|
 
ImÃ—m + 1m1T
m

v"
CLIPPED RELU STE,0.22727272727272727,"âˆ’2Ë†Ïƒ
âˆš 2Ï€"
CLIPPED RELU STE,0.22832980972515857," 
(âˆ’1 + Î») ImÃ—m + 1m1T
m

vâˆ—

,
(29) where"
CLIPPED RELU STE,0.22938689217758984,"Î» = Î»(|w|) =
Ï€ âˆ’
r
Ë†Ïƒ|w|eâˆ’1"
CLIPPED RELU STE,0.23044397463002114,"2(
r
Ë†Ïƒ|w|)
2âˆš"
CLIPPED RELU STE,0.23150105708245244,"2Ï€ âˆ’Ï€erfc

r
âˆš"
CLIPPED RELU STE,0.23255813953488372,2Ë†Ïƒ|w| 
CLIPPED RELU STE,0.23361522198731502,1 âˆ’eâˆ’1
CLIPPED RELU STE,0.2346723044397463,"2(
r
Ë†Ïƒ|w|)
2
.
(30)"
CLIPPED RELU STE,0.2357293868921776,This becomes zero if
CLIPPED RELU STE,0.23678646934460887,"v =
2Ë†Ïƒ
âˆš"
CLIPPED RELU STE,0.23784355179704017,"2Ï€ |wâˆ—|
 
ImÃ—m + 1m1T
m
âˆ’1  
(âˆ’1 + Î») ImÃ—m + 1m1T
m

vâˆ—"
CLIPPED RELU STE,0.23890063424947147,"=
2Ë†Ïƒ
âˆš"
CLIPPED RELU STE,0.23995771670190275,"2Ï€ |wâˆ—|

(âˆ’1 + Î») ImÃ—m + 2 âˆ’Î»"
CLIPPED RELU STE,0.24101479915433405,"m + 11m1T
m"
CLIPPED RELU STE,0.24207188160676532,"
vâˆ—.
(31)"
CLIPPED RELU STE,0.24312896405919662,"To be consistent with Eq.(21) at Ï• = 0, only Î» = 2 is allowed. The solution provides the global
minimum shown in item 3 in Sec.3.1."
CLIPPED RELU STE,0.2441860465116279,"In fact, Î» monotonically increases from 0 to Ï€ as
r
Ë†Ïƒ|w| increases, and thus the solution at Î» =
2, corresponding to the global minimum point, can be obtained for any clipping value r and the
variance Ë†Ïƒ2 by changing the scale |w|. Consequently, we get all the stationary points are found in
cRelu STE:"
CLIPPED RELU STE,0.2452431289640592,"1. If vâˆ—satisï¬es (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2 , Ï• = Â¯Ï• = arccos

(vâˆ—T 1m)
2"
CLIPPED RELU STE,0.2463002114164905,"âˆ’(m+1)(vâˆ—)2+(vâˆ—T 1m)2 
,"
CLIPPED RELU STE,0.24735729386892177,"v = Â¯v = 2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
CLIPPED RELU STE,0.24841437632135308,"
cos Ï•ImÃ—m + 1âˆ’cos Ï•"
CLIPPED RELU STE,0.24947145877378435,"m+1 1m1T
m

vâˆ—."
CLIPPED RELU STE,0.25052854122621565,"2. Ï• = Ï€, v = Â¯v = 2Ë†Ïƒ|wâˆ—|
âˆš 2Ï€"
CLIPPED RELU STE,0.25158562367864695,"
âˆ’ImÃ—m +
2
m+11m1T
m

."
CLIPPED RELU STE,0.2526427061310782,"3. Ï• = 0, |w| = w0 â‰¡r"
CLIPPED RELU STE,0.2536997885835095,"Ë†Ïƒc0, v = v0 â‰¡2Ë†Ïƒ|wâˆ—|
âˆš"
CLIPPED RELU STE,0.2547568710359408,"2Ï€ vâˆ—, where w0 (or c0) is given by Î»(w0) = 2."
CLIPPED RELU STE,0.2558139534883721,"Note that while the global minimum point is degenerated in scales for w, i.e. if (w, v) = (w0, v0)
gives the global minimum, (w, v) = (kw0, v0) for any k > 0 also gives the global minimum, the
cRelu STE picks up the point at the scale determined by Î» = 2."
EXPERIMENTS,0.2568710359408034,"4
EXPERIMENTS"
EXPERIMENTS,0.25792811839323465,"To conï¬rm our conjecture in more general case, we have numerically studied the Gaussian mixture
input with various mean values. As teacher networks, we have tested tanh-type and sin-type as
well as Relu-type. For all the examined setups, cRelu STE behaves like the population gradient,
while id/Relu STEs show qualitatively different behaviors as described below. To calculate the
population loss and STE gradients, we have generated the mixture Gaussian samples and have taken
their average. The population gradient has been obtained by calculating the ï¬nite difference of the
population loss. We have demonstrated the back propagation with learning rate Î· = 0.01 given in
Algorithm 1."
EXPERIMENTS,0.25898520084566595,"Shown in Fig. 1 are the results of ten mixture Gaussian input with random mean values for each
components of Z. We employed m = 20, n = 25, and the tanh-type teacher network. We ï¬nd
that the population gradient and cRelu STE show similar results, while id/Relu STEs are completely
different. This indicates cRelu STE is less biased than id/Relu STEs."
EXPERIMENTS,0.26004228329809725,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.26109936575052856,"In the case of id/Relu STEs, at early steps up to around 500 step, |w| decreases around the magnitude
of the update quantity, so that it begins to oscillate. Then it escapes the oscillation, and the loss
function shows the convergence to a point different from the local minimum achieved by population
gradient, while |w| becomes larger and larger due to their scale invariance. Interestingly, the values
of the loss function are small compared to the one obtained by population gradient, which implies
that id/Relu STEs avoid being trapped in the local solution due to their large bias. However, note
that even at that point, the magnitude of w continues to grow and eventually become numerically
unstable."
EXPERIMENTS,0.26215644820295986,"Figure 1: Numerical results of the back-propagation by population gradient and three STEs. We
generate 10000 samples which follow ten mixture Gaussian input with random mean values. We
employ the tanh-type teacher network."
SUMMARY,0.2632135306553911,"5
SUMMARY"
SUMMARY,0.2642706131078224,"We have found that breaking symmetry embedded in the network by STEs enhances the possibility
of convergence to the true (local) minimum of the loss function. We have demonstrated that if an
STE breaks the scale symmetry embedded in the one-hidden-layer network with a binary activation,
it is more likely to achieve the local minimum than the one which keeps the symmetry. The discus-
sion can be generalized to the network with more symmetries in a straightforward way. The more
symmetries embedded in the network an STE breaks, the more likely it is to converge. To conï¬rm
the mechanism, we have studied three STEs, identity STE, Relu STE, and cRelu STE, in a sim-
ple misspeciï¬ed model with Gaussian input. We have found that the back-propagation by the cRelu
STE, which breaks the scale symmetry, can converge to the global minimum, while identy/Relu STE
cannot. Finally we have numerically conï¬rmed the mechanism for the mixture Gaussian model with
various teacher network."
REFERENCES,0.2653276955602537,REFERENCES
REFERENCES,0.266384778012685,"Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of
convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018."
REFERENCES,0.26744186046511625,"Yoshua Bengio, Nicholas LÂ´eonard, and Aaron Courville.
Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.26849894291754756,Under review as a conference paper at ICLR 2022
REFERENCES,0.26955602536997886,"Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving
low-bit quantization through learnable offsets and better initialization.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696â€“697,
2020."
REFERENCES,0.27061310782241016,"Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao, and Lawrence
Carin.
Straight-through estimator as projected wasserstein gradient ï¬‚ow.
arXiv preprint
arXiv:1910.02176, 2019."
REFERENCES,0.27167019027484146,"Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018."
REFERENCES,0.2727272727272727,"Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net-
works for efï¬cient inference. In ICCV Workshops, pp. 3009â€“3018, 2019."
REFERENCES,0.273784355179704,"Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Donâ€™t be afraid of spurious local minima. In International Conference on
Machine Learning, pp. 1339â€“1348. PMLR, 2018."
REFERENCES,0.2748414376321353,"Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019."
REFERENCES,0.2758985200845666,"Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky.
Neural networks for machine learning.
Coursera, video lectures, 264(1):2146â€“2153, 2012."
REFERENCES,0.2769556025369979,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Quan-
tized neural networks: Training neural networks with low precision weights and activations. The
Journal of Machine Learning Research, 18(1):6869â€“6898, 2017."
REFERENCES,0.27801268498942916,"Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efï¬cient inference: A
whitepaper. arXiv preprint arXiv:1806.08342, 2018."
REFERENCES,0.27906976744186046,"Vladimir Kryzhanovskiy, Gleb Balitskiy, Nikolay Kozyrskiy, and Aleksandr Zuruev. Qpp: Real-
time quantization parameter prediction for deep neural networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 10684â€“10692, 2021."
REFERENCES,0.28012684989429176,"Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
preprint arXiv:2012.04728, 2020."
REFERENCES,0.28118393234672306,"Ziang Long, Penghang Yin, and Jack Xin. Learning quantized neural nets by coarse gradient method
for nonlinear classiï¬cation. Research in the Mathematical Sciences, 8(3):1â€“19, 2021."
REFERENCES,0.2822410147991543,"Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classiï¬cation using binary convolutional neural networks. In European conference on computer
vision, pp. 525â€“542. Springer, 2016."
REFERENCES,0.2832980972515856,"Alexander Shekhovtsov and Viktor Yanush. Reintroducing straight-through estimators as principled
methods for stochastic binary networks. arXiv preprint arXiv:2006.06880, 2020."
REFERENCES,0.2843551797040169,"Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. arXiv preprint
arXiv:1903.05662, 2019."
REFERENCES,0.2854122621564482,"Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network
quantization without retraining using outlier channel splitting. In International conference on
machine learning, pp. 7543â€“7552. PMLR, 2019."
REFERENCES,0.2864693446088795,"Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients.
arXiv preprint
arXiv:1606.06160, 2016."
REFERENCES,0.28752642706131076,Under review as a conference paper at ICLR 2022
REFERENCES,0.28858350951374206,"A
DERIVATION OF SEC.3"
REFERENCES,0.28964059196617337,"In the Appendix, we provide the results of the Gaussian with variance 1, N(0, 1). For some function
f(Z), the expectation value of the Gaussian with the generic variance Ë†Ïƒ, N(0, Ë†Ïƒ2), shown in Sec.3
is written as"
REFERENCES,0.29069767441860467,"EZâˆ¼N(0,Ïƒ2)[f(Z)] = r"
REFERENCES,0.2917547568710359,"1
2Ï€Ë†Ïƒ2 Z Y"
REFERENCES,0.2928118393234672,"i,j
dZijf(Z)eâˆ’"
REFERENCES,0.2938689217758985,"P
i,j Z2
ij
2Ë†Ïƒ2 = r 1
2Ï€ Z Y"
REFERENCES,0.2949260042283298,"i,j
dZâ€²
ijf(Ë†ÏƒZâ€²)eâˆ’"
REFERENCES,0.2959830866807611,"P
i,j Z
â€²2
ij
2"
REFERENCES,0.29704016913319237,"= EZâˆ¼N(0,1)[f(Ë†ÏƒZ)]
(32)"
REFERENCES,0.29809725158562367,"where we have changed the integral variables from Z to Zâ€² = Z/Ïƒ. Therefore we get the results
with the variance Ë†Ïƒ by only changing from f(Z) to f(Ë†ÏƒZ)."
REFERENCES,0.29915433403805497,"A.1
DERIVATION OF LOSS FUNCTION"
REFERENCES,0.30021141649048627,The expectaion of the loss is divided into the following three terms:
REFERENCES,0.3012684989429176,"L(v, w; vâˆ—, wâˆ—) =1"
EZ,0.3023255813953488,"2EZ

vT Ïƒ(Zw)vT Ïƒ(Zw)

âˆ’EZ

vT Ïƒ(Zw)vâˆ—T fRelu(Zwâˆ—)
 + 1"
EZ,0.3033826638477801,"2EZ

vâˆ—T fRelu(Zwâˆ—)vâˆ—T fRelu(Zwâˆ—)

(33)"
EZ,0.3044397463002114,"The calculation of the ï¬rst term and the third term are given in â€œProof of Lemma 1â€ in Yin et al.
(2019) and â€œProof of Section 3â€ in Ref. Du et al. (2018), respectively:"
EZ,0.3054968287526427,"1
2EZ

vT Ïƒ(Zw)vT Ïƒ(Zw)

+ 1"
EZ,0.30655391120507397,"2EZ

vâˆ—T fRelu(Zwâˆ—)vâˆ—T fRelu(Zwâˆ—)
 = 1"
EZ,0.30761099365750527,"8vT  
ImÃ—m + 1m1T
m

v + 1"
EZ,0.3086680761099366,"4Ï€ (wâˆ—)2vâˆ—T  
(Ï€ âˆ’1)ImÃ—m + 1m1T
m

vâˆ—
(34)"
EZ,0.3097251585623679,"To discuss the second term, we evaluate the following quantity:"
EZ,0.3107822410147992,Fij = EZ [Ïƒ(Zw)fRelu(Zwâˆ—)]ij
EZ,0.3118393234672304,"=

1
âˆš 2Ï€"
EZ,0.3128964059196617,mn Z ï£« ï£­Y
EZ,0.313953488372093,"k,l
dZkl ï£¶ ï£¸eâˆ’1"
P,0.3150105708245243,"2
P"
P,0.31606765327695563,"k,l Z2
klÏƒ X"
P,0.3171247357293869,"mâ€²
Zimâ€²wmâ€² !  X"
P,0.3181818181818182,"mâ€²
Zjmâ€²wâˆ—
mâ€² ! Ïƒ X"
P,0.3192389006342495,"mâ€²
Zjmâ€²wâˆ—
mâ€² ! (35)"
P,0.3202959830866808,(i) i Ì¸= j case:
P,0.321353065539112,"Fij =

1
âˆš 2Ï€"
P,0.3224101479915433,2n Z  Y
P,0.32346723044397463,"l
dZildZjl ! eâˆ’1"
P,0.32452431289640593,"2
P"
P,0.32558139534883723,"l(Z2
il+Z2
jl)Ïƒ X"
P,0.3266384778012685,"mâ€²
Zimâ€²wmâ€² !  X"
P,0.3276955602536998,"mâ€²
Zjmâ€²wâˆ—
mâ€² ! Ïƒ X"
P,0.3287526427061311,"mâ€²
Zjmâ€²wâˆ—
mâ€² !"
P,0.3298097251585624,"=

1
âˆš 2Ï€"
P,0.3308668076109937,2n Z  Y
P,0.33192389006342493,"l
d ËœZild ËœZjl ! eâˆ’1"
P,0.33298097251585623,"2
P"
P,0.33403805496828753,"l( Ëœ
Z2
il+ Ëœ
Z2
jl)Ïƒ

ËœZi1|w|
 
ËœZj1|wâˆ—|

Ïƒ

ËœZj1|wâˆ—|
"
P,0.33509513742071884,"=

1
âˆš 2Ï€"
P,0.3361522198731501,"2 Z
d ËœZi1d ËœZj1eâˆ’1"
P,0.3372093023255814,"2( Ëœ
Z2
i1+ Ëœ
Z2
j1)Ïƒ

ËœZi1
 
ËœZj1|wâˆ—|

Ïƒ

ËœZj1
"
P,0.3382663847780127,"=
1
2
âˆš"
P,0.339323467230444,"2Ï€ |wâˆ—|,
(36)"
P,0.3403805496828753,where we used the orthogonal transformation in the second line:
P,0.34143763213530653,"ËœZil =
X"
P,0.34249471458773784,"mâ€²
Oi
lmâ€²Zimâ€²,"
P,0.34355179704016914,"ËœZjl =
X"
P,0.34460887949260044,"mâ€²
Oj
lmâ€²Zjmâ€²
(37)"
P,0.3456659619450317,Under review as a conference paper at ICLR 2022
P,0.346723044397463,"with Oi
1mâ€² = wmâ€²/|w| and Oj
1mâ€² = wâˆ—
mâ€²/|wâˆ—|."
P,0.3477801268498943,(ii) i = j case:
P,0.3488372093023256,"Fii =

1
âˆš 2Ï€"
P,0.3498942917547569,n Z Y
P,0.35095137420718814,"l
dZileâˆ’1"
P,0.35200845665961944,"2
P"
P,0.35306553911205074,"l Z2
ilÏƒ X"
P,0.35412262156448204,"m
Zimwm !  X"
P,0.35517970401691334,"m
Zimwâˆ—
m ! Ïƒ X"
P,0.3562367864693446,"m
Zimwâˆ—
m !"
P,0.3572938689217759,. (38)
P,0.3583509513742072,"Without loss of generality, we can choose"
P,0.3594080338266385,"wâˆ—= (wâˆ—, 0nâˆ’1),
w = (w1, w2, 0nâˆ’2)
(39)"
P,0.36046511627906974,"with wâˆ—> 0, so that this can be written as"
P,0.36152219873150104,"Fii =

1
âˆš 2Ï€"
P,0.36257928118393234,"2 Z
dZi1dZi2eâˆ’1"
P,0.36363636363636365,"2(Z2
i1+Z2
i2)Ïƒ"
X,0.36469344608879495,"2
X"
X,0.3657505285412262,"m=1
Zimwm !"
X,0.3668076109936575,Zi1wâˆ—Ïƒ (Zi1)
X,0.3678646934460888,"=

1
âˆš 2Ï€"
X,0.3689217758985201,"2 Z
dZdÎ¸Zeâˆ’1"
X,0.3699788583509514,2 Z2Ïƒ (cos(Ï• âˆ’Î¸)) Z cos Î¸wâˆ—Ïƒ (cos Î¸) = 1 2Ï€ Z âˆ
DZ,0.37103594080338265,"0
dZ
Z Ï€/2"
DZ,0.37209302325581395,"Ï•âˆ’Ï€/2
dÎ¸Z2eâˆ’1"
DZ,0.37315010570824525,2 Z2 cos Î¸wâˆ—
DZ,0.37420718816067655,"=
1
2
âˆš"
DZ,0.3752642706131078,"2Ï€ wâˆ—(1 + cos Ï•)
(40)"
DZ,0.3763213530655391,where Ï• is the angle between wâˆ—and w. Eqs.(36) and (40) are combined as
DZ,0.3773784355179704,"Fij =
1
2
âˆš"
DZ,0.3784355179704017,"2Ï€ |wâˆ—|(1 + cos Ï•)Î´ij +
1
2
âˆš"
DZ,0.379492600422833,"2Ï€ |wâˆ—|(1 âˆ’Î´ij),
(41)"
DZ,0.38054968287526425,so that the second term in Eq. (33) is expressed as
DZ,0.38160676532769555,"EZ

vT Ïƒ(Zw)vâˆ—T fRelu(Zwâˆ—)

= |wâˆ—| 2
âˆš"
DZ,0.38266384778012685,"2Ï€ vT  
cos Ï•ImÃ—m + 1m1T
m

vâˆ—.
(42)"
DZ,0.38372093023255816,"Consequently, the loss function is summarized as"
DZ,0.38477801268498946,"L(v, w; vâˆ—, wâˆ—) = 1"
DZ,0.3858350951374207,"8vT  
ImÃ—m + 1m1T
m

v âˆ’|wâˆ—| 2
âˆš"
DZ,0.386892177589852,"2Ï€ vT  
cos Ï•ImÃ—m + 1m1T
m

vâˆ— + 1"
DZ,0.3879492600422833,"4Ï€ |wâˆ—|2vâˆ—T  
(Ï€ âˆ’1)ImÃ—m + 1m1T
m

vâˆ—.
(43)"
DZ,0.3890063424947146,The population gradient w.r.t v and w is thus given by
DZ,0.39006342494714585,"âˆ‚L
âˆ‚v = 1"
DZ,0.39112050739957716,"4
 
ImÃ—m + 1m1T
m

v âˆ’Ë†Ïƒ|wâˆ—| 2
âˆš 2Ï€"
DZ,0.39217758985200846," 
cos Ï•ImÃ—m + 1m1T
m

vâˆ—,
(44)"
DZ,0.39323467230443976,"âˆ‚L
âˆ‚w =
1
|w|
âˆ‚L
âˆ‚Ï•eÏ• =
Ë†Ïƒ|wâˆ—|
2
âˆš"
DZ,0.39429175475687106,"2Ï€|w| sin Ï•
 
vT vâˆ—
eÏ•,
(45)"
DZ,0.3953488372093023,"where without loss of generality,
we can take ew
=
(cos Ï•, sin Ï•, 0nâˆ’2),
eÏ•
=
(âˆ’sin Ï•, cos Ï•, 0nâˆ’2). Note that ew Â· âˆ‚L"
DZ,0.3964059196617336,âˆ‚w = 0 is satisï¬ed due to the scale symmetry for the loss.
DZ,0.3974630021141649,"A.2
CLASSIFICATION OF STATIONARY POINTS"
DZ,0.3985200845665962,"Using Eq.(21), the loss at the three stationary points is rewritten as"
DZ,0.39957716701902746,L = âˆ’|wâˆ—|2
DZ,0.40063424947145876,4Ï€ vâˆ—T
DZ,0.40169133192389006,cos2 Ï•ImÃ—m +
DZ,0.40274841437632136,âˆ’(cos Ï• âˆ’1)2
DZ,0.40380549682875266,"m + 1
+ 1 !"
DZ,0.4048625792811839,"1m1T
m ! vâˆ—"
DZ,0.4059196617336152,+ |wâˆ—|2
DZ,0.4069767441860465,"2Ï€ vâˆ—T  
(Ï€ âˆ’1) ImÃ—m + 1m1T
m

vâˆ—.
(46)"
DZ,0.4080338266384778,Under review as a conference paper at ICLR 2022
DZ,0.4090909090909091,"Obviously, this shows that Ï• = 0 is the global minimum point,"
DZ,0.41014799154334036,L â‰¥L|Ï•=0 = |wâˆ—|2
DZ,0.41120507399577166,"4Ï€ (Ï€ âˆ’2)vâˆ—T v.
(47)"
DZ,0.41226215644820297,"At Ï• = Ï€, this becomes"
DZ,0.41331923890063427,L|Ï•=Ï€ = âˆ’|wâˆ—|2
DZ,0.4143763213530655,"4Ï€ vâˆ—T

ImÃ—m +
 âˆ’4"
DZ,0.4154334038054968,"m + 1 + 1

1m1T
m 
vâˆ—"
DZ,0.4164904862579281,+ |wâˆ—|2
DZ,0.4175475687103594,"4Ï€ vâˆ—T  
(Ï€ âˆ’1) ImÃ—m + 1m1T
m

vâˆ—"
DZ,0.4186046511627907,= |wâˆ—|2
DZ,0.41966173361522197,"4Ï€ vâˆ—T

(Ï€ âˆ’2) ImÃ—m +
4
m + 11m1T
m"
DZ,0.42071881606765327,"
vâˆ—
(48)"
DZ,0.42177589852008457,"At Ï• = Â¯Ï• = arccos

(vâˆ—T 1m)
2"
DZ,0.42283298097251587,âˆ’(m+1)(vâˆ—)2+(vâˆ—T 1m)2
DZ,0.42389006342494717,"
, this becomes"
DZ,0.4249471458773784,L|Ï•= Â¯Ï• = âˆ’|wâˆ—|2
DZ,0.4260042283298097,4Ï€ vâˆ—T ï£® ï£°
DZ,0.427061310782241,"(
 
vâˆ—T 1m
2"
DZ,0.4281183932346723,âˆ’(m + 1) (vâˆ—)2 + (vâˆ—T 1m)2 )2 ImÃ—m + ï£«
DZ,0.42917547568710357,ï£­âˆ’(m + 1)
DZ,0.43023255813953487,"(
(vâˆ—)2"
DZ,0.4312896405919662,âˆ’(m + 1) (vâˆ—)2 + (vâˆ—T 1m)2 )2 + 1 ï£¶
DZ,0.4323467230443975,"ï£¸1m1T
m ï£¹ ï£»vâˆ—"
DZ,0.4334038054968288,+ |wâˆ—|2
DZ,0.43446088794926,"4Ï€ vâˆ—T  
(Ï€ âˆ’1) ImÃ—m + 1m1T
m

vâˆ—"
DZ,0.4355179704016913,= âˆ’|wâˆ—|2 4Ï€
DZ,0.4365750528541226,"(
 
vâˆ—T 1m
2 (vâˆ—)2"
DZ,0.4376321353065539,"âˆ’(m + 1) (vâˆ—)2 + (vâˆ—T 1m)2 +
 
vâˆ—T 1m
2
)"
DZ,0.43868921775898523,+ |wâˆ—|2
DZ,0.4397463002114165,"4Ï€ vâˆ—T  
(Ï€ âˆ’1) ImÃ—m + 1m1T
m

vâˆ—.
(49)"
DZ,0.4408033826638478,"Using these results, we obtain"
DZ,0.4418604651162791,L|Ï•= Â¯Ï• âˆ’L|Ï•=Ï€
DZ,0.4429175475687104,= âˆ’|wâˆ—|2 4Ï€
DZ,0.4439746300211416," 
vâˆ—T 1m
2 (vâˆ—)2"
DZ,0.4450317124735729,âˆ’(m + 1) (vâˆ—)2 + (vâˆ—T 1m)2 + |wâˆ—|2
DZ,0.44608879492600423,"4Ï€
(vâˆ—)2 âˆ’|wâˆ—|2"
DZ,0.44714587737843553,"4Ï€
 
vâˆ—T 1m
2
4
m + 1"
DZ,0.44820295983086683,= |wâˆ—|2 4Ï€
DZ,0.4492600422832981,"n
(m + 1) (vâˆ—)2 âˆ’2
 
vâˆ—T 1m
2o2"
DZ,0.4503171247357294,"n
(m + 1) (vâˆ—)2 âˆ’(vâˆ—T 1m)2o
(m + 1)
.
(50)"
DZ,0.4513742071881607,"Note that because (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2 is satisï¬ed to exist the stationary point at Ï• = Â¯Ï•,
the loss at Ï• = Ï€ is smaller than the one at at Ï• = Â¯Ï•,"
DZ,0.452431289640592,"L|v=Â¯v,Ï•= Â¯Ï• â‰¥L|v=vÏ€,Ï•=Ï€.
(51)"
DZ,0.45348837209302323,"is satisï¬ed. When (m + 1) (vâˆ—)2 = 2
 
vâˆ—T 1m
2, L|Ï•= Â¯Ï• = L|Ï•=Ï€, because the middle stationary
point at Ï• = Â¯Ï• is merged into the point at Ï• = Ï€."
DZ,0.45454545454545453,"This implies that Ï• = 0 is the global minimum, Ï• = Ï€ is the local minimum, and Ï• = Â¯Ï• is the
saddle point or the local maximum if (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2."
DZ,0.45560253699788583,"To fully clarify whether the stationary points are local minimum, maximum or saddle point, we have
calculated the hessian matrix, H ="
DZ,0.45665961945031713,"âˆ‚2L
âˆ‚v2
âˆ‚2L
âˆ‚vâˆ‚Ï•
âˆ‚2L
âˆ‚Ï•âˆ‚v
âˆ‚2L
âˆ‚Ï•2 ! = 1"
DZ,0.45771670190274844,"4
 
ImÃ—m + 1m1T
m

|wâˆ—|
2
âˆš"
DZ,0.4587737843551797,2Ï€ sin Ï•vâˆ—
DZ,0.459830866807611,"|wâˆ—|
2
âˆš"
DZ,0.4608879492600423,"2Ï€ sin Ï•vâˆ—T
|wâˆ—|
2
âˆš"
DZ,0.4619450317124736,2Ï€vT vâˆ—cos Ï• !
DZ,0.4630021141649049,".
(52)"
DZ,0.46405919661733613,Under review as a conference paper at ICLR 2022
DZ,0.46511627906976744,"(i) v = Â¯v, Ï• = Â¯Ï•:"
DZ,0.46617336152219874,"For z = (x, y)T (x âˆˆRm, y âˆˆR),"
DZ,0.46723044397463004,zT Hz = 1
DZ,0.4682875264270613,"4xT  
ImÃ—m + 1m1T
m

x + |wâˆ—|
âˆš"
DZ,0.4693446088794926,"2Ï€ y sin Â¯Ï•
 
xT vâˆ— = 1"
DZ,0.4704016913319239,"4
 
xT 1m
2 +
1"
DZ,0.4714587737843552,"2x + |wâˆ—|y sin Â¯Ï•
âˆš"
DZ,0.4725158562367865,"2Ï€
vâˆ—
2
âˆ’|wâˆ—|2y2 sin2 Â¯Ï• (vâˆ—)2"
DZ,0.47357293868921774,"2Ï€
,
(53)"
DZ,0.47463002114164904,"where we used Â¯vT vâˆ—= 0. Therefore, depending on x and y, the Hessian can be either positive or
negative, which means the stationary point is a saddle point."
DZ,0.47568710359408034,"(ii) v = vÏ€ â‰¡2|wâˆ—|
âˆš 2Ï€"
DZ,0.47674418604651164,"
âˆ’ImÃ—m +
2
m+11m1T
m

vâˆ—, Ï• = Ï€ H = 1"
DZ,0.47780126849894294,"4
 
ImÃ—m + 1m1T
m

0
0
âˆ’|wâˆ—| 2
âˆš"
DZ,0.4788583509513742,"2Ï€vT
Ï€ vâˆ— !"
DZ,0.4799154334038055,".
(54)"
DZ,0.4809725158562368,"Note that vT
Ï€ vâˆ—= 2|wâˆ—|
âˆš"
DZ,0.4820295983086681,"2Ï€ vâˆ—T 
âˆ’ImÃ—m +
2
m+11m1T
m

vâˆ—. If (m + 1) (vâˆ—)2 â‰¥2
 
vâˆ—T 1m
2, then"
DZ,0.48308668076109934,"vT
Ï€ vâˆ—â‰¤0, so that the matrix becomes a positive deï¬nite matrix: For any x âˆˆRm, y âˆˆR, this
satisï¬es"
DZ,0.48414376321353064,"1
4xT  
ImÃ—m + 1m1T
m

x âˆ’|wâˆ—| 2
âˆš"
DZ,0.48520084566596194,"2Ï€ vT
Ï€ vâˆ—y2 â‰¥0,
(55)"
DZ,0.48625792811839325,"which means the stationary point is the local minimum. On the other hand, if (m + 1) (vâˆ—)2 â‰¤
2
 
vâˆ—T 1m
2, the matrix becomes either positive or negative, depending on x and y, so that this
stationary point becomes a saddle point."
DZ,0.48731501057082455,"(iii) v = v0 â‰¡2|wâˆ—|
âˆš"
DZ,0.4883720930232558,"2Ï€ vâˆ—, Ï• = 0 H = 1"
DZ,0.4894291754756871,"4
 
ImÃ—m + 1m1T
m

0
0
|wâˆ—|
2
âˆš"
DZ,0.4904862579281184,"2Ï€vT
0 vâˆ— !"
DZ,0.4915433403805497,".
(56)"
DZ,0.492600422832981,"The Hessian becomes a positive deï¬nite matrix: For any x âˆˆRm, y âˆˆR, this satisï¬es"
DZ,0.49365750528541225,"1
4xT  
ImÃ—m + 1m1T
m

x + |wâˆ—| 2
âˆš"
DZ,0.49471458773784355,"2Ï€ vT
0 vâˆ—y2 â‰¥0.
(57)"
DZ,0.49577167019027485,"A.3
DERIVATION OF IDENTITY STE GRADIENT"
DZ,0.49682875264270615,"By substituting Âµâ€²(x) = 1 into Eq.(23), we obtain"
DZ,0.4978858350951374,"EZ [g]i =

1
âˆš 2Ï€"
DZ,0.4989429175475687,mn Z ï£« ï£­Y
DZ,0.5,"k,l
dZkl ï£¶ ï£¸eâˆ’1"
P,0.5010570824524313,"2
P"
P,0.5021141649048626,"k,l Z2
kl ï£« ï£­X"
P,0.5031712473572939,"j
Zjivj ï£¶ ï£¸ Ã— X k
vkÏƒ X"
P,0.5042283298097252,"l
Zklwl ! âˆ’
X"
P,0.5052854122621564,"k
vâˆ—
kfRelu X"
P,0.5063424947145877,"l
Zklwâˆ—
l !!"
P,0.507399577167019,".
(58)"
P,0.5084566596194503,We calculate each term as follows.
P,0.5095137420718816,"A.3.1
CALCULATION OF THE FIRST TERM"
P,0.5105708245243129,We calculate the ï¬rst term as was done in Lemma 9 (and also Lemma 11) in Yin et al. (2019).
P,0.5116279069767442,"To discuss the ï¬rst term, we evaluate"
P,0.5126849894291755,"F id
ijk â‰¡EZ [Zji [Ïƒ(Zw)]k] .
(59)"
P,0.5137420718816068,Under review as a conference paper at ICLR 2022
P,0.514799154334038,"Using F id
ijk, the ï¬rst term can be expressed as"
P,0.5158562367864693,"ï¬rst term =
X"
P,0.5169133192389006,"j,k
F id
ijkvjvk.
(60)"
P,0.5179704016913319,(i) j Ì¸= k case:
P,0.5190274841437632,"F id
ijk = 0,
(61)"
P,0.5200845665961945,because of the odd symmetry for i-th and j-th components in the integrand.
P,0.5211416490486258,(ii) j = k case:
P,0.5221987315010571,"F id
ijj =

1
âˆš 2Ï€"
P,0.5232558139534884,n Z  Y
P,0.5243128964059197,"l
dZjl ! eâˆ’1"
P,0.5253699788583509,"2
P"
P,0.5264270613107822,"l Z2
jlZjiÏƒ X"
P,0.5274841437632135,"l
Zjlwl !"
P,0.5285412262156448,"=

1
âˆš 2Ï€"
P,0.5295983086680761,n Z  Y
P,0.5306553911205074,"l
d ËœZjl ! eâˆ’1"
P,0.5317124735729387,"2
P"
P,0.53276955602537,"l Ëœ
Z2
jl
 X"
P,0.5338266384778013,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.5348837209302325,"Ïƒ

ËœZj1
"
P,0.5359408033826638,"=

1
âˆš 2Ï€"
P,0.5369978858350951,n Z  Y
P,0.5380549682875264,"l
d ËœZjl ! eâˆ’1"
P,0.5391120507399577,"2
P"
P,0.540169133192389,"l Ëœ
Z2
jlOj T
i1
ËœZj1Ïƒ

ËœZj1
"
P,0.5412262156448203,"=

1
âˆš 2Ï€"
P,0.5422832980972516," Z
d ËœZj1eâˆ’1"
P,0.5433403805496829,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1Ïƒ

ËœZj1
"
P,0.5443974630021141,"=

1
âˆš 2Ï€"
P,0.5454545454545454,"
Oj T
i1
=

1
âˆš 2Ï€"
P,0.5465116279069767,"
Oj
1i =
1
âˆš"
P,0.547568710359408,"2Ï€
wi
|w|
(62)"
P,0.5486257928118393,where we used the orthogonal transformation in the second line:
P,0.5496828752642706,"ËœZjl =
X"
P,0.5507399577167019,"mâ€²
Oj
lmâ€²Zjmâ€²
(63)"
P,0.5517970401691332,"with Oi
1mâ€² = wmâ€²/|w|."
P,0.5528541226215645,The ï¬rst term is reduced to
P,0.5539112050739958,"ï¬rst term =
1
âˆš"
P,0.554968287526427,"2Ï€
wi
|w|
 
vT v

.
(64)"
P,0.5560253699788583,"A.3.2
CALCULATION OF THE SECOND TERM"
P,0.5570824524312896,"To discuss the second term, we evaluate"
P,0.5581395348837209,"Gid
ijk = EZ [Zji [fRelu(Zwâˆ—)]k] .
(65)"
P,0.5591966173361522,"Using Gid
ijk, the second term can be expressed as"
P,0.5602536997885835,"second term =
X"
P,0.5613107822410148,"j,k
Gid
ijkvjvâˆ—
k.
(66)"
P,0.5623678646934461,(i) j Ì¸= k case:
P,0.5634249471458774,"Gid
ijk = 0,
(67)"
P,0.5644820295983086,because of the odd symmetry.
P,0.5655391120507399,Under review as a conference paper at ICLR 2022
P,0.5665961945031712,(ii) j = k case:
P,0.5676532769556025,"Gid
ijj =

1
âˆš 2Ï€"
P,0.5687103594080338,n Z  Y
P,0.5697674418604651,"l
dZjl ! eâˆ’1"
P,0.5708245243128964,"2
P"
P,0.5718816067653277,"l Z2
jlZjifRelu X"
P,0.572938689217759,"l
Zjlwâˆ—
l !"
P,0.5739957716701902,"=

1
âˆš 2Ï€"
P,0.5750528541226215,n Z  Y
P,0.5761099365750528,"l
d ËœZjl ! eâˆ’1"
P,0.5771670190274841,"2
P"
P,0.5782241014799154,"l Ëœ
Z2
jl
 X"
P,0.5792811839323467,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.580338266384778,"fRelu

|wâˆ—| ËœZj1
"
P,0.5813953488372093,"=

1
âˆš 2Ï€"
P,0.5824524312896406,n Z  Y
P,0.5835095137420718,"l
d ËœZjl ! eâˆ’1"
P,0.5845665961945031,"2
P"
P,0.5856236786469344,"l Ëœ
Z2
jlOj T
i1
ËœZj1fRelu

|wâˆ—| ËœZj1
"
P,0.5866807610993657,"=

1
âˆš 2Ï€"
P,0.587737843551797," Z
d ËœZj1eâˆ’1"
P,0.5887949260042283,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1fRelu

|wâˆ—| ËœZj1
"
P,0.5898520084566596,"=

1
âˆš 2Ï€"
P,0.5909090909090909," Z
d ËœZj1eâˆ’1"
P,0.5919661733615222,"2 Ëœ
Z2
j1Oj T
i1
ËœZ2
j1|wâˆ—|Ïƒ

|wâˆ—| ËœZj1
"
P,0.5930232558139535,= |wâˆ—|
OJ T,0.5940803382663847,"2
Oj T
i1
= |wâˆ—|"
OJ,0.595137420718816,"2
Oj
1i = 1"
OJ,0.5961945031712473,"2wâˆ—
i
(68)"
OJ,0.5972515856236786,where we have used the orthogonal transformation in the second line:
OJ,0.5983086680761099,"ËœZjl =
X"
OJ,0.5993657505285412,"mâ€²
Oj
lmâ€²Zjmâ€²
(69)"
OJ,0.6004228329809725,"with Oi
1mâ€² = wâˆ—
mâ€²/|wâˆ—|, and also have used
Z âˆ"
OJ,0.6014799154334038,"0
dxx2eâˆ’(1/2)x2 = âˆš"
OJ,0.6025369978858351,"2Ï€
2
.
(70)"
OJ,0.6035940803382663,The second term is written as
OJ,0.6046511627906976,second term = 1
WI,0.6057082452431289,"2wi
 
vT vâˆ—
.
(71)"
WI,0.6067653276955602,"A.4
DERIVATION OF RELU STE GRADIENT"
WI,0.6078224101479915,"By substituting Âµâ€²(x) = Ïƒ(x) into Eq.(23), we obtain"
WI,0.6088794926004228,"EZ [grelu]i =

1
âˆš 2Ï€"
WI,0.6099365750528541,mn Z ï£« ï£­Y
WI,0.6109936575052854,"k,l
dZkl ï£¶ ï£¸eâˆ’1"
P,0.6120507399577167,"2
P"
P,0.6131078224101479,"k,l Z2
kl ï£« ï£­X"
P,0.6141649048625792,"j
ZjivjÏƒ X"
P,0.6152219873150105,"l
Zjlwl !ï£¶ ï£¸ Ã— X k
vkÏƒ X"
P,0.6162790697674418,"l
Zklwl ! âˆ’
X"
P,0.6173361522198731,"k
vâˆ—
kfRelu X"
P,0.6183932346723044,"l
Zklwâˆ—
l !!"
P,0.6194503171247357,".
(72)"
P,0.620507399577167,We calculate each term as follows.
P,0.6215644820295984,"A.4.1
CALCULATION OF THE FIRST TERM"
P,0.6226215644820295,"We calculate the ï¬rst term as was done in Lemma9 (and also Lemma11) in Yin et al. (2019). We
evaluate"
P,0.6236786469344608,"F relu
ijk = EZ
h
Zji [Ïƒ(Zw)]j [Ïƒ(Zw)]k
i
.
(73)"
P,0.6247357293868921,"Using F relu
ijk , the ï¬rst term can be expressed as"
P,0.6257928118393234,"ï¬rst term =
X"
P,0.6268498942917548,"j,k
F relu
ijk vjvk.
(74)"
P,0.627906976744186,Under review as a conference paper at ICLR 2022
P,0.6289640591966174,(i) j Ì¸= k case:
P,0.6300211416490487,"F relu
ijk =

1
âˆš 2Ï€"
P,0.63107822410148,2n Z  Y
P,0.6321353065539113,"l
dZjldZkl ! eâˆ’1"
P,0.6331923890063424,"2
P"
P,0.6342494714587738,"l Z2
jl+Z2
klZjiÏƒ X"
P,0.635306553911205,"l
Zjlwl ! Ïƒ X"
P,0.6363636363636364,"l
Zklwl !"
P,0.6374207188160677,"=

1
âˆš 2Ï€"
P,0.638477801268499,2n Z  Y
P,0.6395348837209303,"l
d ËœZjld ËœZkl ! eâˆ’1"
P,0.6405919661733616,"2
P"
P,0.6416490486257929,"l Ëœ
Z2
jl+ Ëœ
Z2
kl
 X"
P,0.642706131078224,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.6437632135306554,"Ïƒ

ËœZj1

Ïƒ

ËœZk1
"
P,0.6448202959830867,"=

1
âˆš 2Ï€"
P,0.645877378435518,"2 Z
d ËœZj1d ËœZk1eâˆ’1"
P,0.6469344608879493,"2( Ëœ
Z2
j1+ Ëœ
Z2
k1)Oj T
i1
ËœZj1Ïƒ

ËœZj1

Ïƒ

ËœZk1
"
P,0.6479915433403806,"=
1
2
âˆš"
P,0.6490486257928119,"2Ï€ Oj T
i1"
P,0.6501057082452432,"=
1
2
âˆš"
P,0.6511627906976745,"2Ï€
wi
|w|,
(75)"
P,0.6522198731501057,where we used the orthogonal transformation in the second line:
P,0.653276955602537,"ËœZjl =
X"
P,0.6543340380549683,"mâ€²
Oj
lmâ€²Zjmâ€²
(76)"
P,0.6553911205073996,"with Oi
1mâ€² = wmâ€²/|w|."
P,0.6564482029598309,(ii) j = k case:
P,0.6575052854122622,"We get the same expression given in Eq.(62),"
P,0.6585623678646935,"F relu
ijj
=

1
âˆš 2Ï€"
P,0.6596194503171248,n Z  Y
P,0.6606765327695561,"l
dZjl ! eâˆ’1"
P,0.6617336152219874,"2
P"
P,0.6627906976744186,"l Z2
jlZjiÏƒ X"
P,0.6638477801268499,"l
Zjlwl ! =
1
âˆš"
P,0.6649048625792812,"2Ï€
wi
|w|.
(77)"
P,0.6659619450317125,The ï¬rst term is written as
P,0.6670190274841438,"ï¬rst term =
1
2
âˆš"
P,0.6680761099365751,"2Ï€
wi
|w|vT  
ImÃ—m + 1m1T
m

v.
(78)"
P,0.6691331923890064,"A.4.2
CALCULATION OF SECOND TERM"
P,0.6701902748414377,"To discuss the second term, we evaluate"
P,0.671247357293869,"Grelu
ijk = EZ
h
Zji
h
[Ïƒ(Zw)]j fRelu(Zwâˆ—)
i k"
P,0.6723044397463002,"i
.
(79)"
P,0.6733615221987315,"Using Grelu
ijk , the second term can be expressed as"
P,0.6744186046511628,"second term =
X"
P,0.6754756871035941,"j,k
Grelu
ijk vjvâˆ—
k.
(80)"
P,0.6765327695560254,(i) j Ì¸= k case:
P,0.6775898520084567,"Grelu
ijk =

1
âˆš 2Ï€"
P,0.678646934460888,2n Z  Y
P,0.6797040169133193,"l
dZjldZkl ! eâˆ’1"
P,0.6807610993657506,"2
P"
P,0.6818181818181818,"l Z2
jl+Z2
klZjiÏƒ X"
P,0.6828752642706131,"l
Zjlwl ! fRelu X"
P,0.6839323467230444,"l
Zklwâˆ—
l !"
P,0.6849894291754757,"=

1
âˆš 2Ï€"
P,0.686046511627907,2n Z  Y
P,0.6871035940803383,"l
d ËœZjld ËœZkl ! eâˆ’1"
P,0.6881606765327696,"2
P"
P,0.6892177589852009,"l Ëœ
Z2
jl+ Ëœ
Z2
kl
 X"
P,0.6902748414376322,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.6913319238900634,"Ïƒ

ËœZj1

fRelu

|wâˆ—| ËœZk1
"
P,0.6923890063424947,"=

1
âˆš 2Ï€"
P,0.693446088794926,"2 Z
d ËœZj1d ËœZk1eâˆ’1"
P,0.6945031712473573,"2( Ëœ
Z2
j1+ Ëœ
Z2
k1)Oj T
i1
ËœZj1Ïƒ

ËœZj1

|wâˆ—| ËœZk1Ïƒ

ËœZk1
"
P,0.6955602536997886,= |wâˆ—|
P,0.6966173361522199,"2Ï€ Oj T
i1 = 1"
P,0.6976744186046512,"2Ï€
|wâˆ—|"
P,0.6987315010570825,"|w| wi,
(81)"
P,0.6997885835095138,Under review as a conference paper at ICLR 2022
P,0.7008456659619451,"where we used the orthogonal transformation in the second line:
ËœZjl =
X"
P,0.7019027484143763,"mâ€²
Oj
lmâ€²Zjmâ€²,"
P,0.7029598308668076,"ËœZkl =
X"
P,0.7040169133192389,"mâ€²
Ok
lmâ€²Zkmâ€²
(82)"
P,0.7050739957716702,"with Oi
1mâ€² = wmâ€²/|w| and Oj
1mâ€² = wâˆ—
mâ€²/|wâˆ—|."
P,0.7061310782241015,(ii) j = k case:
P,0.7071881606765328,"Grelu
ijj =

1
âˆš 2Ï€"
P,0.7082452431289641,m Z  Y
P,0.7093023255813954,"l
dZjl ! eâˆ’1"
P,0.7103594080338267,"2
P"
P,0.7114164904862579,"l Z2
jlZjiÏƒ X"
P,0.7124735729386892,"l
Zjlwl ! fRelu X"
P,0.7135306553911205,"l
Zjlwâˆ—
l !"
P,0.7145877378435518,"=

1
âˆš 2Ï€"
P,0.7156448202959831,m Z  Y
P,0.7167019027484144,"l
dZjl ! eâˆ’1"
P,0.7177589852008457,"2
P"
P,0.718816067653277,"l Z2
jlZjiÏƒ"
X,0.7198731501057083,"2
X"
X,0.7209302325581395,"l=1
Zjlwl !"
X,0.7219873150105708,fRelu (wâˆ—Zj1) = 1 2Ï€
X,0.7230443974630021,"Z  2
Y"
X,0.7241014799154334,"l=1
dZjl ! eâˆ’1"
X,0.7251585623678647,"2
P2
l=1 Z2
jlZji (Î´i1 + Î´i2) Ïƒ"
X,0.726215644820296,"2
X"
X,0.7272727272727273,"l=1
Zjlwl !"
X,0.7283298097251586,fRelu (wâˆ—Zj1) = 1 2Ï€
X,0.7293868921775899,"Z
dZdÎ¸Zeâˆ’1"
X,0.7304439746300211,2 Z2 (Z cos Î¸Î´i1 + Z sin Î¸Î´i2) Ïƒ (Zw cos (Î¸ âˆ’Ï•)) fRelu (wâˆ—Z cos Î¸) = 1 2Ï€
X,0.7315010570824524,"Z
dZdÎ¸Zeâˆ’1"
X,0.7325581395348837,2 Z2 (Z cos Î¸Î´i1 + Z sin Î¸Î´i2) Ïƒ (Zw cos (Î¸ âˆ’Ï•)) wâˆ—Z cos Î¸Ïƒ (cos Î¸) (83) = wâˆ— 2Ï€ Z âˆ
X,0.733615221987315,"0
dZZ3eâˆ’1"
X,0.7346723044397463,2 Z2 Z Ï€/2
X,0.7357293868921776,"âˆ’Ï€/2+Ï•
dÎ¸
 
cos2 Î¸Î´i1 + cos Î¸ sin Î¸Î´i2
 = wâˆ— Ï€"
X,0.7367864693446089,"sin 2Ï• 4
+ 1"
X,0.7378435517970402,"2 (Ï€ âˆ’Ï•)

Î´i1 + 1 âˆ’cos 2Ï• 4
Î´i2  = wâˆ— Ï€ 1"
X,0.7389006342494715,2 (Ï€ âˆ’Ï•) Î´i1 + sin Ï•
X,0.7399577167019028,"2
(cos Ï•Î´i1 + sin Ï•Î´i2)

(84)"
X,0.741014799154334,"where we have chosen
wâˆ—= (wâˆ—, 0nâˆ’1),
w = (w1, w2, 0nâˆ’2)
(85)
with wâˆ—> 0 and Ï• is deï¬ned as the angle between wâˆ—and w. We also have used the formula,
Z âˆ"
X,0.7420718816067653,"0
dxx3eâˆ’(1/2)x2 = 2,
Z
dÎ¸ cos2 Î¸ = sin 2Î¸ 4
+ 1"
X,0.7431289640591966,"2Î¸,
Z
dÎ¸ cos Î¸ sin Î¸ = âˆ’1"
X,0.7441860465116279,"4 cos 2Î¸.
(86)"
X,0.7452431289640592,"Note that
wâˆ—
i
|wâˆ—| = Î´i1,"
X,0.7463002114164905,"wi
|w| = cos Ï•Î´i1 + sin Ï•Î´i2.
(87)"
X,0.7473572938689218,This reduces to
X,0.7484143763213531,"Grelu
ijj = 1 Ï€ 1"
X,0.7494714587737844,"2 (Ï€ âˆ’Ï•) wâˆ—
i + sin Ï•"
X,0.7505285412262156,"2
|wâˆ—|"
X,0.7515856236786469,|w| wi
X,0.7526427061310782,"
(88)"
X,0.7536997885835095,"In summary, the second term is written as"
X,0.7547568710359408,second term = 1
X,0.7558139534883721,"2Ï€
|wâˆ—|"
X,0.7568710359408034,"|w| wi

vT  
âˆ’ImÃ—m + 1m1T
m

vâˆ—"
X,0.7579281183932347,"+

(Ï€ âˆ’Ï•) wâˆ—
i + |wâˆ—| sin Ï•"
X,0.758985200845666,"|w|
wi"
X,0.7600422832980972,"  
vT vâˆ—"
X,0.7610993657505285,"2Ï€
.
(89)"
X,0.7621564482029598,Under review as a conference paper at ICLR 2022
X,0.7632135306553911,"A.5
DERIVATION OF CLIPPED RELU STE GRADIENT"
X,0.7642706131078224,"By substituting Âµâ€²(x) = Ïƒ(x)Ïƒ(r âˆ’x) into Eq.(23), we obtain"
X,0.7653276955602537,"EZ [gcrelu]i =

1
âˆš 2Ï€"
X,0.766384778012685,mn Z ï£« ï£­Y
X,0.7674418604651163,"k,l
dZkl ï£¶ ï£¸eâˆ’1"
P,0.7684989429175476,"2
P"
P,0.7695560253699789,"k,l Z2
kl Ã— ï£« ï£­X"
P,0.7706131078224101,"j
ZjivjÏƒ X"
P,0.7716701902748414,"l
Zjlwl ! Ïƒ  r âˆ’
X"
P,0.7727272727272727,"l
Zjlwl !ï£¶ ï£¸ Ã— X k
vkÏƒ X"
P,0.773784355179704,"l
Zklwl ! âˆ’
X"
P,0.7748414376321353,"k
vâˆ—
kfRelu X"
P,0.7758985200845666,"l
Zklwâˆ—
l !!"
P,0.7769556025369979,".
(90)"
P,0.7780126849894292,We calculate each term as follows.
P,0.7790697674418605,"A.5.1
CALCULATION OF THE FIRST TERM"
P,0.7801268498942917,"To discuss the ï¬rst term, we evaluate"
P,0.781183932346723,"F crelu
ijk
= EZ
h
Zji [Ïƒ(Zw) âŠ™Ïƒ(r âˆ’Zw)]j [Ïƒ(Zw)]k
i
.
(91)"
P,0.7822410147991543,"Using F crelu
ijk
, the ï¬rst term can be expressed as"
P,0.7832980972515856,"ï¬rst term =
X"
P,0.7843551797040169,"j,k
F crelu
ijk
vjvk.
(92)"
P,0.7854122621564482,(i) j Ì¸= k case:
P,0.7864693446088795,"F crelu
ijk
=

1
âˆš 2Ï€"
P,0.7875264270613108,2n Z  Y
P,0.7885835095137421,"l
dZjldZkl ! eâˆ’1"
P,0.7896405919661733,"2
P"
P,0.7906976744186046,"l Z2
jl+Z2
klZji Ã— Ïƒ X"
P,0.7917547568710359,"l
Zjlwl ! Ïƒ  r âˆ’
X"
P,0.7928118393234672,"l
Zjlwl ! Ïƒ X"
P,0.7938689217758985,"l
Zklwl !"
P,0.7949260042283298,"=

1
âˆš 2Ï€"
P,0.7959830866807611,2n Z  Y
P,0.7970401691331924,"l
d ËœZjld ËœZkl ! eâˆ’1"
P,0.7980972515856237,"2
P"
P,0.7991543340380549,"l Ëœ
Z2
jl+ Ëœ
Z2
kl
 X"
P,0.8002114164904862,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.8012684989429175,"Ã— Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1

Ïƒ

ËœZk1
"
P,0.8023255813953488,"=

1
âˆš 2Ï€"
P,0.8033826638477801,"2 Z
d ËœZj1d ËœZk1eâˆ’1"
P,0.8044397463002114,"2( Ëœ
Z2
j1+ Ëœ
Z2
k1)Oj T
i1
ËœZj1Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1

Ïƒ

ËœZk1
"
P,0.8054968287526427,"=
1
2
âˆš 2Ï€"
P,0.806553911205074,Z r/|w|
P,0.8076109936575053,"0
d ËœZj1eâˆ’1"
P,0.8086680761099366,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1"
P,0.8097251585623678,"=
1
2
âˆš"
P,0.8107822410147991,"2Ï€ Oj T
i1

1 âˆ’eâˆ’1"
P,0.8118393234672304,"2(
r
|w|)
2"
P,0.8128964059196617,"=
1
2
âˆš 2Ï€"
P,0.813953488372093,"
1 âˆ’eâˆ’1"
P,0.8150105708245243,"2(
r
|w|)
2 wi"
P,0.8160676532769556,"|w|,
(93)"
P,0.8171247357293869,where we used the orthogonal transformation in the second line:
P,0.8181818181818182,"ËœZjl =
X"
P,0.8192389006342494,"mâ€²
Oj
lmâ€²Zjmâ€²
(94)"
P,0.8202959830866807,"with Oi
1mâ€² = wmâ€²/|w|."
P,0.821353065539112,Under review as a conference paper at ICLR 2022
P,0.8224101479915433,(ii) j = k case:
P,0.8234672304439746,"F crelu
ijj
=

1
âˆš 2Ï€"
P,0.8245243128964059,n Z  Y
P,0.8255813953488372,"l
dZjl ! eâˆ’1"
P,0.8266384778012685,"2
P"
P,0.8276955602536998,"l Z2
jlZjiÏƒ X"
P,0.828752642706131,"l
Zjlwl ! Ïƒ  r âˆ’
X"
P,0.8298097251585623,"l
Zjlwl !"
P,0.8308668076109936,"=

1
âˆš 2Ï€"
P,0.8319238900634249,n Z  Y
P,0.8329809725158562,"l
d ËœZjl ! eâˆ’1"
P,0.8340380549682875,"2
P"
P,0.8350951374207188,"l Ëœ
Z2
jl
 X"
P,0.8361522198731501,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.8372093023255814,"Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1
 =
1
âˆš 2Ï€"
P,0.8382663847780126,"Z
d ËœZj1eâˆ’1"
P,0.8393234672304439,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1
 =
1
âˆš 2Ï€"
P,0.8403805496828752,Z r/|w|
P,0.8414376321353065,"0
d ËœZj1eâˆ’1"
P,0.8424947145877378,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1 =
1
âˆš"
P,0.8435517970401691,"2Ï€ Oj T
i1

1 âˆ’eâˆ’1"
P,0.8446088794926004,"2(
r
|w|)
2 =
1
âˆš 2Ï€"
P,0.8456659619450317,"
1 âˆ’eâˆ’1"
P,0.846723044397463,"2(
r
|w|)
2 wi"
P,0.8477801268498943,"|w|,
(95)"
P,0.8488372093023255,"In summary, the ï¬rst term is written as"
P,0.8498942917547568,"ï¬rst term =
1
2
âˆš"
P,0.8509513742071881,"2Ï€
wi
|w|"
P,0.8520084566596194,"
1 âˆ’eâˆ’1"
P,0.8530655391120507,"2(
r
|w|)
2
vT  
ImÃ—m + 1m1T
m

v.
(96)"
P,0.854122621564482,"A.5.2
CALCULATION OF THE SECOND TERM"
P,0.8551797040169133,"To discuss the second term, we evaluate"
P,0.8562367864693446,"Gcrelu
ijk
= EZ
h
Zji
h
[Ïƒ(Zw) âŠ™Ïƒ(r âˆ’Zw)]j fRelu(Zwâˆ—)
i k"
P,0.857293868921776,"i
.
(97)"
P,0.8583509513742071,"Using Gcrelu
ijk , the second term can be expressed as"
P,0.8594080338266384,"second term =
X"
P,0.8604651162790697,"j,k
Gcrelu
ijk vjvâˆ—
k.
(98)"
P,0.861522198731501,(i) j Ì¸= k case:
P,0.8625792811839323,"Gcrelu
ijk
=

1
âˆš 2Ï€"
P,0.8636363636363636,2n Z  Y
P,0.864693446088795,"l
dZjldZkl ! eâˆ’1"
P,0.8657505285412262,"2
P"
P,0.8668076109936576,"l Z2
jl+Z2
klZji Ã— Ïƒ X"
P,0.8678646934460887,"l
Zjlwl ! Ïƒ  r âˆ’
X"
P,0.86892177589852,"l
Zjlwl ! fRelu X"
P,0.8699788583509513,"l
Zklwâˆ—
l !"
P,0.8710359408033826,"=

1
âˆš 2Ï€"
P,0.872093023255814,2n Z  Y
P,0.8731501057082452,"l
d ËœZjld ËœZkl ! eâˆ’1"
P,0.8742071881606766,"2
P"
P,0.8752642706131079,"l Ëœ
Z2
jl+ Ëœ
Z2
kl
 X"
P,0.8763213530655392,"mâ€²
Oj T
imâ€² ËœZjmâ€² !"
P,0.8773784355179705,"Ã— Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1

fRelu

|wâˆ—| ËœZk1
"
P,0.8784355179704016,"=

1
âˆš 2Ï€"
P,0.879492600422833,"2 Z
d ËœZj1d ËœZk1eâˆ’1"
P,0.8805496828752643,"2( Ëœ
Z2
j1+ Ëœ
Z2
k1)Oj T
i1
ËœZj1"
P,0.8816067653276956,"Ã— Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1

|wâˆ—| ËœZk1Ïƒ

ËœZk1
"
P,0.8826638477801269,"=

1
âˆš 2Ï€"
P,0.8837209302325582,"2 Z
d ËœZj1eâˆ’1"
P,0.8847780126849895,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1Ïƒ

ËœZj1

Ïƒ

r âˆ’|w| ËœZj1

|wâˆ—|"
P,0.8858350951374208,"=

1
âˆš 2Ï€"
P,0.8868921775898521,2 Z r/|w|
P,0.8879492600422833,"0
d ËœZj1eâˆ’1"
P,0.8890063424947146,"2 Ëœ
Z2
j1Oj T
i1
ËœZj1|wâˆ—|"
P,0.8900634249471459,= |wâˆ—|
P,0.8911205073995772,"2Ï€ Oj T
i1

1 âˆ’eâˆ’1"
P,0.8921775898520085,"2(
r
|w|)
2"
P,0.8932346723044398,= |wâˆ—| 2Ï€
P,0.8942917547568711,"
1 âˆ’eâˆ’1"
P,0.8953488372093024,"2(
r
|w|)
2 wi"
P,0.8964059196617337,"|w|,
(99)"
P,0.8974630021141649,Under review as a conference paper at ICLR 2022
P,0.8985200845665962,"where we used the orthogonal transformation in the second line:
ËœZil =
X"
P,0.8995771670190275,"mâ€²
Oi
lmâ€²Zimâ€²,"
P,0.9006342494714588,"ËœZkl =
X"
P,0.9016913319238901,"mâ€²
Ok
lmâ€²Zkmâ€²
(100)"
P,0.9027484143763214,"with Oi
1mâ€² = wmâ€²/|w| and Oj
1mâ€² = wâˆ—
mâ€²/|wâˆ—|."
P,0.9038054968287527,(ii) j = k case:
P,0.904862579281184,"Gcrelu
ijj
=

1
âˆš 2Ï€"
P,0.9059196617336153,m Z  Y
P,0.9069767441860465,"l
dZjl ! eâˆ’1"
P,0.9080338266384778,"2
P"
P,0.9090909090909091,"l Z2
jlZji Ã— Ïƒ X"
P,0.9101479915433404,"l
Zjlwl ! Ïƒ  r âˆ’
X"
P,0.9112050739957717,"l
Zjlwl ! fRelu X"
P,0.912262156448203,"l
Zjlwâˆ—
l !"
P,0.9133192389006343,"=

1
âˆš 2Ï€"
P,0.9143763213530656,m Z  Y
P,0.9154334038054969,"l
dZjl ! eâˆ’1"
P,0.9164904862579282,"2
P"
P,0.9175475687103594,"l Z2
jlZji Ã— Ïƒ"
X,0.9186046511627907,"2
X"
X,0.919661733615222,"l=1
Zjlwl ! Ïƒ  r âˆ’"
X,0.9207188160676533,"2
X"
X,0.9217758985200846,"l=1
Zjlwl !"
X,0.9228329809725159,fRelu (wâˆ—Zj1) = 1 2Ï€
X,0.9238900634249472,"Z  2
Y"
X,0.9249471458773785,"l=1
dZjl ! eâˆ’1"
X,0.9260042283298098,"2
P2
l=1 Z2
jlZji (Î´i1 + Î´i2) Ã— Ïƒ"
X,0.927061310782241,"2
X"
X,0.9281183932346723,"l=1
Zjlwl ! Ïƒ  r âˆ’"
X,0.9291754756871036,"2
X"
X,0.9302325581395349,"l=1
Zjlwl !"
X,0.9312896405919662,fRelu (wâˆ—Zj1) = 1 2Ï€
X,0.9323467230443975,"Z
dZdÎ¸Zeâˆ’1"
X,0.9334038054968288,2 Z2 (Z cos Î¸Î´i1 + Z sin Î¸Î´i2)
X,0.9344608879492601,Ã— Ïƒ (Zw cos (Î¸ âˆ’Ï•)) Ïƒ  r âˆ’
X,0.9355179704016914,"2
X"
X,0.9365750528541226,"l=1
Zjlwl !"
X,0.9376321353065539,fRelu (wâˆ—Z cos Î¸) = 1 2Ï€
X,0.9386892177589852,"Z
dZdÎ¸Zeâˆ’1"
X,0.9397463002114165,2 Z2 (Z cos Î¸Î´i1 + Z sin Î¸Î´i2)
X,0.9408033826638478,"Ã— Ïƒ (Zw cos (Î¸ âˆ’Ï•)) Ïƒ (r âˆ’Zw cos (Î¸ âˆ’Ï•)) wâˆ—Z cos Î¸Ïƒ (cos Î¸)
(101) = wâˆ— 2Ï€ Z Ï€/2"
X,0.9418604651162791,"âˆ’Ï€/2+Ï•
dÎ¸
 
cos2 Î¸Î´i1 + cos Î¸ sin Î¸Î´i2
 Z
r
w cos(Î¸âˆ’Ï•)"
X,0.9429175475687104,"0
dZZ3eâˆ’1 2 Z2 = wâˆ— 2Ï€ Z Ï€/2"
X,0.9439746300211417,"âˆ’Ï€/2+Ï•
dÎ¸
 
cos2 Î¸Î´i1 + cos Î¸ sin Î¸Î´i2
 Ã— ( 2 âˆ’"
X,0.945031712473573,"
r
w cos (Î¸ âˆ’Ï•)"
X,0.9460887949260042,"2
+ 2 ! eâˆ’1"
X,0.9471458773784355,"2(
r
w cos(Î¸âˆ’Ï•))
2) (102)"
X,0.9482029598308668,"where we have chosen
wâˆ—= (wâˆ—, 0nâˆ’1),
w = (w1, w2, 0nâˆ’2)
(103)
with wâˆ—> 0 and Ï• is deï¬ned as the angle between wâˆ—and w. We also have used the formula,
Z t"
X,0.9492600422832981,"0
dxx3eâˆ’1"
X,0.9503171247357294,2 x2 = 2 âˆ’(t2 + 2)eâˆ’1
X,0.9513742071881607,"2 t2.
(104)"
X,0.952431289640592,"Note that
wâˆ—
i
|wâˆ—| = Î´i1,"
X,0.9534883720930233,"wi
|w| = cos Ï•Î´i1 + sin Ï•Î´i2.
(105)"
X,0.9545454545454546,Under review as a conference paper at ICLR 2022
X,0.9556025369978859,"Using C(w, Ï•), S(w, Ï•) deï¬ned as"
X,0.9566596194503171,"C(w, Ï•) â‰¡wâˆ— 2Ï€ Z Ï€/2"
X,0.9577167019027484,"âˆ’Ï€/2+Ï•
dÎ¸ cos2 Î¸ ( 2 âˆ’"
X,0.9587737843551797,"
r
w cos (Î¸ âˆ’Ï•)"
X,0.959830866807611,"2
+ 2 ! eâˆ’1"
X,0.9608879492600423,"2(
r
w cos(Î¸âˆ’Ï•))
2) ,"
X,0.9619450317124736,"S(w, Ï•) â‰¡wâˆ— 2Ï€ Z Ï€/2"
X,0.9630021141649049,"âˆ’Ï€/2+Ï•
dÎ¸ sin Î¸ cos Î¸ ( 2 âˆ’"
X,0.9640591966173362,"
r
w cos (Î¸ âˆ’Ï•)"
X,0.9651162790697675,"2
+ 2 ! eâˆ’1"
X,0.9661733615221987,"2(
r
w cos(Î¸âˆ’Ï•))
2) , (106)"
X,0.96723044397463,"If Ï• Ì¸= 0, Ï€, Gcrelu
ijj
is formally written as"
X,0.9682875264270613,"Gcrelu
ijj
= C(w, Ï•) wâˆ—
i
|wâˆ—| + S(w, Ï•)

1
sin Ï•
wi
|w| âˆ’cot Ï• wâˆ—
i
|wâˆ—|"
X,0.9693446088794926,"
(107)"
X,0.9704016913319239,"If Ï• = 0 or Ï€, Gcrelu
ijj
is formally written as"
X,0.9714587737843552,"Gcrelu
ijj
= C(w, 0) wâˆ—
i
|wâˆ—|
for Ï• = 0,
(108)"
X,0.9725158562367865,"Gcrelu
ijj
= 0 for Ï• = Ï€
(109)"
X,0.9735729386892178,"where we used S(w, 0) = S(w, Ï€) = C(w, Ï€) = 0 and"
X,0.9746300211416491,"C(w, 0) = wâˆ— 2Ï€ Z Ï€/2"
X,0.9756871035940803,"âˆ’Ï€/2
dÎ¸ cos2 Î¸

2 âˆ’

r
w cos Î¸"
X,0.9767441860465116,"2
+ 2

eâˆ’1"
X,0.9778012684989429,"2(
r
w cos Î¸)
2 = wâˆ— 2Ï€"
X,0.9788583509513742,"
Ï€ âˆ’r weâˆ’1 2( r w)
2âˆš"
X,0.9799154334038055,"2Ï€ âˆ’Ï€erfc

r
âˆš"
W,0.9809725158562368,2w
W,0.9820295983086681,"
(110)"
W,0.9830866807610994,"with the following formula
Z Ï€/2"
W,0.9841437632135307,"âˆ’Ï€/2
dÎ¸eâˆ’
a2"
W,0.985200845665962,"2 cos2 Î¸ = Ï€erfc
 a
âˆš 2"
W,0.9862579281183932,"
,
(111) Z Ï€/2"
W,0.9873150105708245,"âˆ’Ï€/2
dÎ¸ cos2 Î¸eâˆ’
a2"
W,0.9883720930232558,2 cos2 Î¸ = 1 2
W,0.9894291754756871,"
aeâˆ’a2 2 âˆš"
W,0.9904862579281184,"2Ï€ +
 
1 âˆ’a2
Ï€erfc
 a
âˆš 2"
W,0.9915433403805497,"
.
(112)"
W,0.992600422832981,"In summary, the second term is written as"
W,0.9936575052854123,second term = 1
W,0.9947145877378436,"2Ï€
|wâˆ—| |w|"
W,0.9957716701902748,"
1 âˆ’eâˆ’1"
W,0.9968287526427061,"2(
r
|w|)
2
wi

vT  
âˆ’ImÃ—m + 1m1T
m

vâˆ—"
W,0.9978858350951374,"+

C(w, Ï•) wâˆ—
i
|wâˆ—| + S(w, Ï•)

1
sin Ï•
wi
|w| âˆ’cot Ï• wâˆ—
i
|wâˆ—|"
W,0.9989429175475687,"  
vT vâˆ—
.
(113)"
