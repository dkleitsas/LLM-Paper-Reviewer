Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001594896331738437,"Prioritized Experience Replay (ER) has been empirically shown to improve sample
efﬁciency across many domains and attracted great attention; however, there is little
theoretical understanding of why such prioritized sampling helps and its limitations.
In this work, we take a deep look at the prioritized ER. In a supervised learning
setting, we show the equivalence between the error-based prioritized sampling
method for mean squared error and uniform sampling for cubic power loss. We then
provide theoretical insight into why it improves convergence rate upon uniform
sampling during early learning. Based on the insight, we further point out two
limitations of the prioritized ER method: 1) outdated priorities and 2) insufﬁcient
coverage of the sample space. To mitigate the limitations, we propose our model-
based stochastic gradient Langevin dynamics sampling method. We show that
our method does provide states distributed close to an ideal prioritized sampling
distribution estimated by the brute-force method, which does not suffer from the
two limitations. We conduct experiments on both discrete and continuous control
problems to show our approach’s efﬁcacy and examine the practical implication of
our method in an autonomous driving application."
INTRODUCTION,0.003189792663476874,"1
INTRODUCTION"
INTRODUCTION,0.004784688995215311,"Experience Replay (ER) (Lin, 1992) has been a popular method for training large-scale modern
Reinforcement Learning (RL) systems (Degris et al., 2012; Adam & Busoniu, 2012; Mnih et al.,
2015a; Hessel et al., 2018; François-Lavet et al., 2018). In ER, visited experiences are stored in a
buffer, and at each time step, a mini-batch of experiences is uniformly sampled to update the training
parameters in the value or policy function. Such a method is empirically shown to effectively stabilize
the training and improve the sample efﬁciency of deep RL algorithms. Several follow-up works
propose different variants to improve upon it (Schaul et al., 2016; Andrychowicz et al., 2017; Oh
et al., 2018; de Bruin et al., 2018; Horgan et al., 2018; Zha et al., 2019; Novati & Koumoutsakos,
2019; Sun et al., 2020). The most relevant one to our work is prioritized ER (Schaul et al., 2016),
which attempts to improve the vanilla ER method by sampling those visited experiences proportional
to their absolute Temporal Difference (TD) errors. Empirically, it can signiﬁcantly improve sample
efﬁciency upon vanilla ER on many tested domains."
INTRODUCTION,0.006379585326953748,"To gain an intuitive understanding of why the prioritized ER method works, one may recall Model-
Based RL (MBRL) methods (Kaelbling et al., 1996; Bertsekas, 2009; Sutton & Barto, 2018). ER can
be thought of as an instance of a classical model-based RL architecture—Dyna (Sutton, 1991), using
a (limited) non-parametric model given by the buffer (van Seijen & Sutton, 2015; van Hasselt et al.,
2019). A Dyna agent uses real experience to update its policy as well as its reward and dynamics
model. In-between taking actions, the agent can get hypothetical experiences from the model to
further improve the policy. Existing works show that smart ways of sampling those experiences can
further improve sample efﬁciency of a MBRL agent (Sutton et al., 2008; Gu et al., 2016; Goyal et al.,
2019; Holland et al., 2018; Pan et al., 2018; Corneil et al., 2018; Janner et al., 2019; Chelu et al.,
2020). Particularly, prioritized sweeping (Moore & Atkeson, 1993) improves upon vanilla Dyna.
The idea behind prioritized sweeping is quite intuitive: we should give high priority to states whose
absolute TD errors are large because they are likely to cause the most change in value estimates."
INTRODUCTION,0.007974481658692184,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.009569377990430622,"Hence, applying TD error-based prioritized sampling to ER is a natural idea in a model-free RL
setting."
INTRODUCTION,0.011164274322169059,"This work provides a theoretical insight into the prioritized ER’s advantage and points out its two
drawbacks: outdated priorities and insufﬁcient sample space coverage, which may signiﬁcantly
weaken its efﬁcacy. To mitigate the two issues, we propose to leverage the ﬂexibility of using an
environment model to acquire hypothetical experiences by simulating priorities. Speciﬁcally, we
bring in the Stochastic Gradient Langevin Dynamics (SGLD) sampling method to acquire states.
We demonstrate that the hypothetical experiences generated by our method are distributed closer to
the desired TD error-based sampling distribution, which does not suffer from the two drawbacks.
Finally, we demonstrate the utility of our method on various benchmark discrete and continuous
control domains and an autonomous driving application."
BACKGROUND,0.012759170653907496,"2
BACKGROUND"
BACKGROUND,0.014354066985645933,"In this section, we ﬁrstly review basic concepts in RL. Then we brieﬂy introduce the prioritized ER
method, which will be examined in-depth in the next section. We conclude this section by discussing
a classic MBRL architecture called Dyna (Sutton, 1991) and its recent variants, which are most
relevant to our work."
BACKGROUND,0.01594896331738437,"Basic notations. We consider a discounted Markov Decision Process (MDP) framework (Szepesvári,
2010). A MDP can be denoted as a tuple (S, A, P, R, γ) including state space S, action space A,
probability transition kernel P, reward function R, and discount rate γ ∈[0, 1]. At each environment
time step t, an RL agent observes a state st ∈S, takes an action at ∈A, and moves to the next state
st+1 ∼P(·|st, at), and receives a scalar reward signal rt+1 = R(st, at, st+1). A policy is a mapping
π : S × A →[0, 1] that determines the probability of choosing an action at a given state."
BACKGROUND,0.017543859649122806,"A popular algorithm to ﬁnd an optimal policy is Q-learning (Watkins & Dayan, 1992). With function
approximation, parameterized action-values Qθ are updated using θ = θ + αδt∇θQθ(st, at) for
stepsize α > 0 with TD-error δt
def
= rt+1 + γ maxa′∈A Qθ(st+1, a′) −Qθ(st, at). The policy is
deﬁned by acting greedily w.r.t. these action-values."
BACKGROUND,0.019138755980861243,"ER methods. ER is critical when using neural networks to estimate Qθ, as used in DQN (Mnih
et al., 2015b), both to stabilize and speed up learning. ER method uniformly samples a mini-batch
of experiences from those visited ones in the form of (st, at, st+1, rt+1) to update neural network
parameters. Prioritized ER (Schaul et al., 2016) improves upon it by prioritized sampling experiences,
where the probability of sampling a certain experience is proportional to its TD error magnitude, i.e.,
p(st, at, st+1, rt+1) ∝|δt|. However, the underlying theoretical mechanism behind this method is
still not well understood."
BACKGROUND,0.02073365231259968,"MBRL. With a model, an agent has more ﬂexibility to sample hypothetical experiences. We
consider a one-step model which maps a state-action pair to its possible next state and reward:
P : S × A 7→S × R. We build on the Dyna formalism (Sutton, 1991) for MBRL, and more
speciﬁcally, the recently proposed HC-Dyna (Pan et al., 2019) as shown in Algorithm 1. HC-
Dyna provides a special approach to Search-Control (SC)—the mechanism of generating states or
state-action pairs from which to query the model to get the next states and rewards. HC-Dyna’s
search-control mechanism generates states by Hill Climbing (HC) on some criterion function h(·).
The term HC is used for generality as the vanilla gradient ascent is modiﬁed to resolve certain
challenges (Pan et al., 2019)."
BACKGROUND,0.022328548644338118,"The algorithmic framework maintains two buffers: the conventional ER buffer storing experiences
(i.e., an experience/transition has the form of (st, at, st+1, rt+1)) and a search-control queue stor-
ing the states acquired by search-control mechanisms. At each time step t, a real experience
(st, at, st+1, rt+1) is collected and stored into ER buffer. Then the HC search-control process starts
to collect states and store them into the search-control queue. A hypothetical experience is obtained
by ﬁrst selecting a state s from the search-control queue, then selecting an action a according to
the current policy, and then querying the model to get the next state s′ and reward r to form an
experience (s, a, s′, r). These hypothetical transitions are combined with real experiences into a
single mini-batch to update the training parameters. The n updates, performed before taking the next
action, are called planning updates (Sutton & Barto, 2018), as they improve the value/policy by using"
BACKGROUND,0.023923444976076555,Under review as a conference paper at ICLR 2022
BACKGROUND,0.025518341307814992,Algorithm 1 HC-Dyna: Generic framework
BACKGROUND,0.02711323763955343,"Input: Hill Climbing (HC) criterion function h : S 7→R, batch-size b; Initialize empty search-
control queue Bsc; empty ER buffer Ber; initialize policy and model P; HC stepsize αh; mini-
batch size b; environment P; mixing rate ρ decides the proportion of hypothetical experiences in a
mini-batch.
for t = 1, 2, . . . do"
BACKGROUND,0.028708133971291867,"Add (st, at, st+1, rt+1) to Ber
while within some budget time steps do"
BACKGROUND,0.030303030303030304,"s ←s + αh∇sh(s) // HC for search-control
Add s into Bsc
// n planning updates/steps
for n times do"
BACKGROUND,0.03189792663476874,"B ←∅// initialize an empty mini-batch B
for bρ times do"
BACKGROUND,0.03349282296650718,"Sample s ∼Bsc, on-policy action a
Sample s′, r ∼P(s, a)
B ←(s, a, s′, r)
Sample b(1 −ρ) experiences from Ber, add to B
Update policy/value on mixed mini-batch B"
BACKGROUND,0.03508771929824561,"a model. The choice of pairing states with on-policy actions to form hypothetical experiences has
been reported to be beneﬁcial (Gu et al., 2016; Pan et al., 2018; Janner et al., 2019)."
BACKGROUND,0.03668261562998405,"Two instances have been proposed for h(·): the value function v(s) (Pan et al., 2019) and the gradient
magnitude ||∇sv(s)|| (Pan et al., 2020). The former is used as a measure of the utility of a state:
doing HC on the learned value function should ﬁnd high-value states without being constrained by
the physical environment dynamics. The latter is considered as a measure of the value approximation
difﬁculty, then doing HC provides additional states whose values are difﬁcult to learn. The two suffer
from several issues as we discuss in the Appendix A.1. This paper will introduce a HC search-control
method motivated by overcoming the limitations of the prioritized ER."
A DEEPER LOOK AT ERROR-BASED PRIORITIZED SAMPLING,0.03827751196172249,"3
A DEEPER LOOK AT ERROR-BASED PRIORITIZED SAMPLING"
A DEEPER LOOK AT ERROR-BASED PRIORITIZED SAMPLING,0.03987240829346093,"In this section, we provide theoretical motivation for error-based prioritized sampling by showing its
equivalence to optimizing a cubic power objective with uniform sampling in a supervised learning
setting. We prove that optimizing the cubic objective provides a faster convergence rate during early
learning. Based on the insight, we discuss two limitations of the prioritized ER: 1) outdated priorities
and 2) insufﬁcient coverage of the sample space. We then empirically study the such limitations."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.04146730462519936,"3.1
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.0430622009569378,"In the l2 regression, we minimize the mean squared error minθ
1
2n
Pn
i=1(fθ(xi) −yi)2, for training
set T = {(xi, yi)}n
i=1 and function approximator fθ, such as a neural network. In error-based
prioritized sampling, we deﬁne the priority of a sample (x, y) ∈T as |fθ(x) −y|; the probability of
drawing a sample (x, y) ∈T is typically q(x, y; θ) ∝|fθ(x) −y|. We employ the following form to
compute the probability of a point (x, y) ∈T :"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.044657097288676235,"q(x, y; θ)
def
=
|fθ(x) −y|
Pn
i=1 |fθ(xi) −yi|.
(1)"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.046251993620414676,"We can show an equivalence between the gradients of the squared objective with this prioritization
and the cubic power objective
1
3n
Pn
i=1 |fθ(xi) −yi|3 in the Theorem 1 below. See Appendix A.3
for the proof.
Theorem 1. For a constant c determined by θ, T , we have"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.04784688995215311,"cE(x,y)∼q(x,y;θ)[∇θ(1/2)(fθ(x) −y)2] = E(x,y)∼uniform(T )[∇θ(1/3)|fθ(x) −y|3]."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.049441786283891544,"We empirically verify this equivalence in the Appendix A.7. This simple theorem provides an intuitive
reason for why prioritized sampling can help improve sample efﬁciency: the gradient direction of"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.051036682615629984,Under review as a conference paper at ICLR 2022
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.05263157894736842,"the cubic function is sharper than that of the square function when the error is relatively large
(Figure 8). We refer readers to the work by Fujimoto et al. (2020) regarding more discussions about
the equivalence between prioritized sampling and of uniform sampling. Our Theorem 2 further proves
that optimizing the cubic power objective by gradient descent has faster convergence rate than the
squared objective, and this provides a solid motivation for using error-based prioritized sampling. See
Appendix A.4 for a more detailed version of the theorem (which includes an additional conclusion)
and its proof and empirical simulations.
Theorem 2 (Fast early learning, concise version). Let n be a positive integer (i.e., the number of
training samples). Let xt, ˜xt ∈Rn be the target estimates of all samples at time t, t ≥0. Let
xt(i)(i ∈[n], [n]
def
= {1, 2, ..., n}) denote the ith element in the vector. We deﬁne the objectives:"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.05422647527910686,"ℓ2(x, y)
def
= 1 2 · n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.05582137161084529,"i=1
(x(i) −y(i))2,
ℓ3(x, y)
def
= 1 3 · n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.05741626794258373,"i=1
|x(i) −y(i)|3."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.05901116427432217,The total absolute prediction errors:
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.06060606060606061,"δt
def
= n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.06220095693779904,"i=1
δt(i) = n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.06379585326953748,"i=1
|xt(i) −y(i)|,
˜δt
def
= n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.06539074960127592,"i=1
˜δt(i) = n
X"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.06698564593301436,"i=1
|˜xt(i) −y(i)|,"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.0685805422647528,"where y(i) ∈R is the training target for the ith training sample. Let {xt}t≥0 and {˜xt}t≥0 be
generated by using ℓ2, ℓ3 objectives respectively. That is, ∀i ∈[n],"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07017543859649122,dxt(i)
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07177033492822966,"dt
= −η · dℓ2(xt, y)"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.0733652312599681,"dxt(i)
,
d˜xt(i)"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07496012759170653,"dt
= −η · dℓ3(˜xt, y)"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07655502392344497,"d˜xt(i)
."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07814992025518341,"Given any 0 < ϵ ≤δ0 = Pn
i=1 δ0(i), deﬁne the following hitting time,"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.07974481658692185,"tϵ
def
= min
t {t ≥0 : δt ≤ϵ},
˜tϵ
def
= min
t {t ≥0 : ˜δt ≤ϵ}."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.08133971291866028,"Assume the same initialization x0 = ˜x0. We have the following conclusion.
If there exists δ0 ∈R and 0 < ϵ ≤δ0 such that 1
n · n
X i=1"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.08293460925039872,"1
δ0(i) ≤log (δ0/ϵ) δ0"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.08452950558213716,"ϵ −1 ,
(2)"
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.0861244019138756,"then we have tϵ ≥˜tϵ, which means gradient descent using the cubic loss function will achieve the
total absolute error threshold ϵ faster than using the squared objective function."
THEORETICAL INSIGHT INTO ERROR-BASED PRIORITIZED SAMPLING,0.08771929824561403,"This theorem illustrates that when the total loss of all training examples is greater than some threshold,
cubic power learns faster. For example, let the number of samples n = 1000, and each sample has
initial loss δ0(i) = 2. Then δ0 = 2000. Setting ϵ = 570(i.e., ϵ(i) ≈0.57) satisﬁes the inequality (2).
This implies using the cubic objective is faster when reducing the total loss from 2000 to 570. Though
it is not our focus here to investigate the practical utility of the high power objectives, we include
some empirical results and discuss the practical utilities of such objectives in Appendix A.6."
LIMITATIONS OF THE PRIORITIZED ER,0.08931419457735247,"3.2
LIMITATIONS OF THE PRIORITIZED ER"
LIMITATIONS OF THE PRIORITIZED ER,0.09090909090909091,"Inspired by the above theorems, we now discuss two drawbacks of prioritized sampling: outdated
priorities and insufﬁcient sample space coverage. Then we empirically examine their importance
and effects in the next section."
LIMITATIONS OF THE PRIORITIZED ER,0.09250398724082935,"The above two theorems show that the advantage of prioritized sampling comes from the faster
convergence rate of cubic power objective during early learning. By Theorem 1, such advantage
requires to update the priorities of all training samples by using the updated training parameters θ
at each time step. In RL, however, at the each time step t, the original prioritized ER method only
updates the priorities of those experiences from the sampled mini-batch, leaving the priorities of the
rest of experiences unchanged (Schaul et al., 2016). We call this limitation outdated priorities. It is
typically infeasible to update the priorities of all experiences at each time step."
LIMITATIONS OF THE PRIORITIZED ER,0.09409888357256778,"In fact, in RL, “all training samples” in RL are restricted to those visited experiences in the ER buffer,
which may only contain a small subset of the whole state space, making the estimate of the prioritized"
LIMITATIONS OF THE PRIORITIZED ER,0.09569377990430622,Under review as a conference paper at ICLR 2022
LIMITATIONS OF THE PRIORITIZED ER,0.09728867623604466,"0.00
0.25
0.50
0.75
1.00
1e5 0.15 0.20 0.30 0.40 0.50"
LIMITATIONS OF THE PRIORITIZED ER,0.09888357256778309,(a) |T | = 4000
LIMITATIONS OF THE PRIORITIZED ER,0.10047846889952153,"0.00
0.25
0.50
0.75
1.00
1e5 0.15 0.50"
LIMITATIONS OF THE PRIORITIZED ER,0.10207336523125997,"Root
mean
squared"
LIMITATIONS OF THE PRIORITIZED ER,0.10366826156299841,"error
averaged"
LIMITATIONS OF THE PRIORITIZED ER,0.10526315789473684,"over
50runs"
LIMITATIONS OF THE PRIORITIZED ER,0.10685805422647528,"L2
PrioritizedL2
Full-PrioritizedL2"
LIMITATIONS OF THE PRIORITIZED ER,0.10845295055821372,(b) |T | = 400
LIMITATIONS OF THE PRIORITIZED ER,0.11004784688995216,"2
4
6
8
time steps
1e4 2000 1750 500 250 0"
LIMITATIONS OF THE PRIORITIZED ER,0.11164274322169059,Average
LIMITATIONS OF THE PRIORITIZED ER,0.11323763955342903,Return
LIMITATIONS OF THE PRIORITIZED ER,0.11483253588516747,"per
Episode
(30runs)"
LIMITATIONS OF THE PRIORITIZED ER,0.11642743221690591,"ER
PrioritizedER
Full-PrioritizedER"
LIMITATIONS OF THE PRIORITIZED ER,0.11802232854864433,(c) Mountain Car.
LIMITATIONS OF THE PRIORITIZED ER,0.11961722488038277,"Figure 1:
Comparing L2 (black), PrioritizedL2 (red), and Full-PrioritizedL2 (blue) in terms of testing
RMSE v.s. number of mini-batch updates. (a)(b) show the results trained on a large and small training set,
respectively. (c) shows the result of a corresponding RL experiment on mountain car domain. We compare
episodic return v.s. environment time steps for ER (black), PrioritizedER (red), and Full-PrioritizedER
(blue). Results are averaged over 50 random seeds on (a), (b) and 30 on (c). The shade indicates standard error."
LIMITATIONS OF THE PRIORITIZED ER,0.12121212121212122,"sampling distribution inaccurate. There can be many reasons for the small coverage: the exploration
is difﬁcult, the state space is huge, or the memory resource of the buffer is quite limited, etc. We call
this issue insufﬁcient sample space coverage, which is also noted by Fedus et al. (2020)."
LIMITATIONS OF THE PRIORITIZED ER,0.12280701754385964,"Note that insufﬁcient sample space coverage should not be considered equivalent to off-policy
distribution issue. The latter refers to some old experiences in the ER buffer may be unlikely to
appear under the current policy (Novati & Koumoutsakos, 2019; Zha et al., 2019; Sun et al., 2020;
Oh et al., 2021). In contrast, the issue of insufﬁcient sample space coverage can raise naturally. For
example, the state space is large and an agent is only able to visit a small subset of the state space
during early learning stage. We visualize the state space coverage issue on a RL domain in Section 4."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.12440191387559808,"3.3
NEGATIVE EFFECTS OF THE LIMITATIONS"
NEGATIVE EFFECTS OF THE LIMITATIONS,0.12599681020733652,"In this section, we empirically show that the outdated priorities and insufﬁcient sample space coverage
signiﬁcantly blur the advantage of the prioritized sampling method."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.12759170653907495,"Experiment setup. We conduct experiments on a supervised learning task. We generate a training
set T by uniformly sampling x ∈[−2, 2] and adding zero-mean Gaussian noise with standard
deviation σ = 0.5 to the target fsin(x) values. Deﬁne fsin(x)
def
= sin(8πx) if x ∈[−2, 0) and
fsin(x) = sin(πx) if x ∈[0, 2]. The testing set contains 1k samples where the targets are not
noise-contaminated. Previous work (Pan et al., 2020) show that the high frequency region [−2, 0]
usually takes long time to learn. Hence we expect error-based prioritized sampling to make a clear
difference in terms of sample efﬁciency on this dataset. We use 32 × 32 tanh layers neural network
for all algorithms. We refer to Appendix A.8 for missing details and A.7 for additional experiments."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.1291866028708134,"Naming of algorithms. L2: the l2 regression with uniformly sampling from T . Full-PrioritizedL2:
the l2 regression with prioritized sampling according to the distribution deﬁned in (1), the priorities
of all samples in the training set are updated after each mini-batch update. PrioritizedL2: the only
difference with Full-PrioritizedL2 is that only the priorities of those training examples sampled in
the mini-batch are updated at each iteration, the rest of the training samples use the original priorities.
This resembles the approach taken by the prioritized ER in RL (Schaul et al., 2016). We show the
learning curves in Figure 1."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.13078149920255183,"Outdated priorities. Figure 1 (a) shows that PrioritizedL2 without updating all priorities can be
signiﬁcantly worse than Full-PrioritizedL2. Correspondingly, we further verify this phenomenon
on the classical Mountain Car domain (Brockman et al., 2016). Figure 1(c) shows the evaluation
learning curves of different DQN variants in an RL setting. We use a small 16 × 16 ReLu NN as the
Q-function, which should highlight the issue of priority updating: every mini-batch update potentially
perturbs the values of many other states. Hence many experiences in the ER buffer have the wrong
priorities. Full-PrioritizedER does perform signiﬁcantly better."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.13237639553429026,"Sample space coverage. To check the effect of insufﬁcient sample space coverage, we examine
how the relative performances of L2 and Full-PrioritizedL2 change when we train them on a smaller
training dataset with only 400 examples as shown in Figure 1(b). The small training set has a small
coverage of the sample space. Unsurprisingly, using a small training set makes all algorithms perform
worse; however, it signiﬁcantly narrows the gap between Full-PrioritizedL2 and L2. This indicates
that prioritized sampling needs sufﬁcient samples across the sample space to estimate the prioritized
sampling distribution reasonably accurate. We further verify the sample space coverage issue in
prioritized ER on a RL problem in the next section."
NEGATIVE EFFECTS OF THE LIMITATIONS,0.1339712918660287,Under review as a conference paper at ICLR 2022
ADDRESSING THE LIMITATIONS,0.13556618819776714,"4
ADDRESSING THE LIMITATIONS"
ADDRESSING THE LIMITATIONS,0.1371610845295056,"In this section, we propose a Stochastic Gradient Langevin Dynamics (SGLD) sampling method
to mitigate the limitations of the prioritized ER method mentioned in the above section. Then we
empirically examine our sampling distribution. We also describe how our sampling method is used
for the search-control component in Dyna."
SAMPLING METHOD,0.13875598086124402,"4.1
SAMPLING METHOD"
SAMPLING METHOD,0.14035087719298245,"SGLD sampling method. Let vπ(·; θ) : S 7→R be a differentiable value function under policy π
parameterized by θ. For s ∈S, deﬁne y(s)
def
= Er,s′∼Pπ(s′,r|s)[r + γvπ(s′; θ)], and denote the TD"
SAMPLING METHOD,0.1419457735247209,"error as δ(s, y; θt)
def
= y(s) −v(s; θt). Given some initial state s0 ∈S, let the state sequence {si}
be the one generated by updating rule si+1 ←si + αh∇s log |δ(si, y(si); θt)| + Xi, where αh is a
stepsize and Xi is a Gaussian random variable with some constant variance.1 Then {si} converges to
the distribution p(s) ∝|δ(s, y(s))| as i →∞. The proof is a direct consequence of the convergent
behavior of Langevin dynamics stochastic differential equation (SDE) (Roberts, 1996; Welling &
Teh, 2011; Zhang et al., 2017). We include a brief background knowledge in Appendix A.2."
SAMPLING METHOD,0.14354066985645933,"It should be noted that, this sampling method enables us to acquire states 1) whose absolute TD errors
are estimated by using current parameter θt and 2) that are not restricted to those visited ones. We
empirically verify the two points in Section 4.2."
SAMPLING METHOD,0.14513556618819776,"Implementation. In practice, we can compute the state value estimate by v(s) = maxa Q(s, a; θt)
as suggested by Pan et al. (2019). In the case that a true environment model is not available, we
compute an estimate ˆy(s) of y(s) by a learned model. Then at each time step t, states approximately
following the distribution p(s) ∝|δ(s, y(s))| can be generated by"
SAMPLING METHOD,0.1467304625199362,"s ←s + αh∇s log |ˆy(s) −max
a
Q(s, a; θt)| + X,
(3)"
SAMPLING METHOD,0.14832535885167464,"where X is a Gaussian random variable with zero-mean and some small variance. Observing that αh
is small, we consider ˆy(s) as a constant given a state s without backpropagating through it. Though
this updating rule introduces bias due to the usage of a learned model, fortunately, the difference
between the sampling distribution acquired by the true model and the learned model can be upper
bounded as we show in Theorem 3 in Appendix A.5."
SAMPLING METHOD,0.14992025518341306,"Algorithmic details. We present our algorithm called Dyna-TD in the Algorithm 3 in Appendix A.8.
Our algorithm follows Algorithm 1. Particularly, we choose the function h(s)
def
= log |ˆy(s) −
maxa Q(s, a; θt)| for HC search-control process, i.e., run the updating rule 3 to generate states."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.15151515151515152,"4.2
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.15311004784688995,"We visualize the distribution of the sampled states by our method and those from the buffer of the
prioritized ER, verifying that our sampled states have an obviously larger coverage of the state
space. We then empirically verify that our sampling distribution is closer to a brute-force calculated
prioritized sampling distribution—which does not suffer from the two limitations—than the prioritized
ER method. Finally, we discuss concerns regarding computational cost. Please see Appendix A.8 for
any missing details."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.1547049441786284,"Large sample space coverage. During early learning, we visualize 2k states sampled from 1) DQN’s
buffer trained by prioritized ER and 2) our algorithm Dyna-TD’s Search-Control (SC) queue on the
continuous state GridWorld (Figure 2(a)). Figure 2 (b-c) visualize state distributions with different
sampling methods via heatmap. Darker color indicates higher density. (b)(c) show that DQN’s ER
buffer, no matter with or without prioritized sampling, does not cover well the top-left part and the
right half part on the GridWorld. In contrast, Figure 2 (d) shows that states from our SC queue are
more diversely distributed on the square. These visualizations verify that our sampled states cover
better the sample space than the prioritized ER does."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.15629984051036683,"Sampling distribution is close to the ideal one. We denote our sampling distribution as p1(·), the
one acquired by conventional prioritized ER as p2(·), and the one computed by thorough priority"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.15789473684210525,1The stepsize and variance affect the temperature parameter. We treat the two as hyper-parameters.
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.1594896331738437,Under review as a conference paper at ICLR 2022 S G
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.16108452950558214,(a) GridWorld
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.16267942583732056,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.16427432216905902,(b) PER (uniform)
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.16586921850079744,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.1674641148325359,(c) PER (prioritized)
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.16905901116427433,"0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.17065390749601275,(d) Dyna-TD SC queue
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.1722488038277512,"Figure 2: (a) shows the GridWorld (Pan et al., 2019). It has S = [0, 1]2, A = {up, down, right, left}. The
agent starts from the left bottom and learn to reach the right top within as few steps as possible. (b) and (c)
respectively show the state distributions with uniform and prioritized sampling methods from the ER buffer of
prioritized ER. (d) shows the SC queue state distribution of our Dyna-TD."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.17384370015948963,"1
2
3
4
5
time steps
1e4 0.000 0.025"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.17543859649122806,"Distance
between"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.17703349282296652,"actual
sampling
distribution"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.17862838915470494,"to desired
distribution"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.18022328548644337,(20runs)
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.18181818181818182,"Dyna-TD
PrioritizedER
Dyna-TD-Long"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.18341307814992025,(a) on-policy weighting
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.1850079744816587,"1
2
3
4
5
time steps
1e4"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.18660287081339713,0.0004
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.18819776714513556,0.0010
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.189792663476874,"Distance
between"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19138755980861244,"actual
sampling
distribution"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19298245614035087,"to desired
distribution"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19457735247208932,(20runs)
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19617224880382775,"Dyna-TD
PrioritizedER
Dyna-TD-Long"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19776714513556617,(b) uniform weighting
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.19936204146730463,"0
1
2
3
4
5
6
computation time cost (second)1e3 2000 1500 1000 500 0"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.20095693779904306,"ER
PrioritizedER
Dyna-TD"
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.2025518341307815,(c) time cost v.s. performance
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.20414673046251994,"Figure 3:
(a)(b) show the distance change as a function of environment time steps for Dyna-TD (black),
PrioritizedER (forest green), and Dyna-TD-Long (orange), with different weighting schemes. The dashed
line corresponds to our algorithm with an online learned model. The corresponding evaluation learning curve is
in the Figure 4(c). (d) shows the policy evaluation performance as a function of running time (in seconds) with
ER(magenta). All results are averaged over 20 random seeds. The shade indicates standard error."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.20574162679425836,"updating of enumerating all states in the state space as p∗(·) (this one should be unrealistic in practice
and we call it the ideal distribution as it does not suffer from the two limitations we discussed). We
visualize how well p1(·) and p2(·) can approximate p∗(·) on the GridWorld domain, where the state
distributions can be conveniently estimated by discretizing the continuous state GridWorld to a 50×50
one. We compute the distances of p1, p2 to p∗by two sensible weighting schemes: 1) on-policy
weighting: P2500
j=1 dπ(sj)|pi(sj)−p∗(sj)|, i ∈{1, 2}, where dπ is approximated by uniformly sample
3k states from a recency buffer; 2) uniform weighting:
1
2500
P2500
j=1 |pi(sj) −p∗(sj)|, i ∈{1, 2}."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.20733652312599682,"We plot the distances change when we train our Algorithm 3 and the prioritized ER in Figure 3(a)(b).
They show that the HC procedure in our algorithm Dyna-TD, either with a true or an online learned
model, produces a state distribution with signiﬁcantly closer distance to the desired sampling dis-
tribution p∗than PrioritizedER under both weighting schemes. In contrast, the state distribution
acquired from PrioritizedER, which suffers from the two limitations, is far away from p∗. It should
also be noted that we include Dyna-TD-Long, which runs a large number of HC steps so that its
corresponding sampling distribution should be closer to stationary distribution. However, there is
only tiny difference between the regular Dyna-TD and Dyna-TD-Long, implying that one can save
computational cost by running fewer HC steps."
EMPIRICAL VERIFICATION OF TD ERROR-BASED SAMPLING METHOD,0.20893141945773525,"Computational cost. Let the mini-batch size be b, and the number of HC steps be kHC. If we
assume one mini-batch update takes O(c), then the time cost of our sampling is O(ckHC/b), which
is reasonable. On the GridWorld, Figure 3(c) shows that given the same time budget, our algorithm
achieves better performance.This makes the additional time spent on search-control worth it."
EXPERIMENTS,0.21052631578947367,"5
EXPERIMENTS"
EXPERIMENTS,0.21212121212121213,"In this section, we design experiments to answer the following questions. (1) By mitigating the
limitations of conventional prioritized ER method, can Dyna-TD outperform the prioritized ER under
various planning budgets in different environments? (2) Can Dyna-TD outperforms the existing Dyna
variants? (3) How effective is Dyna-TD under an online learned model, particularly for more realistic
applications where actions are continuous, or input dimensionality is higher?"
EXPERIMENTS,0.21371610845295055,"Baselines. ER is DQN with a regular ER buffer without prioritized sampling. PrioritizedER is the
one by Schaul et al. (2016), which has the drawbacks as discussed in our paper. Dyna-Value (Pan"
EXPERIMENTS,0.215311004784689,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.21690590111642744,"1
2
3
4
5
time steps
1e4 2000 1500 1000 500 0"
EXPERIMENTS,0.21850079744816586,"(a) MountainCar, n = 10"
EXPERIMENTS,0.22009569377990432,"1
2
3
4
5
time steps
1e4 2000 1500 1000 500 0"
EXPERIMENTS,0.22169059011164274,"(b) MountainCar, n = 30"
EXPERIMENTS,0.22328548644338117,"1
2
3
4
5
time steps
1e4 400 50"
EXPERIMENTS,0.22488038277511962,"(c) Acrobot, n = 10"
EXPERIMENTS,0.22647527910685805,"1
2
3
4
5
time steps
1e4 400 50"
EXPERIMENTS,0.22807017543859648,Average
EXPERIMENTS,0.22966507177033493,Return
EXPERIMENTS,0.23125996810207336,"per
Episode
(20runs)"
EXPERIMENTS,0.23285486443381181,"ER
PrioritizedER
Dyna-Frequency
Dyna-Value
Dyna-TD"
EXPERIMENTS,0.23444976076555024,"(d) Acrobot, n = 30"
EXPERIMENTS,0.23604465709728867,"1
2
3
4
5
time steps
1e4 2000 1500 1000 500 0"
EXPERIMENTS,0.23763955342902712,"(e) GridWorld, n = 10"
EXPERIMENTS,0.23923444976076555,"1
2
3
4
5
time steps
1e4 2000 1500 1000 500 0"
EXPERIMENTS,0.24082934609250398,"(f) GridWorld, n = 30"
EXPERIMENTS,0.24242424242424243,"1
2
3
4
5
time steps
1e4 0 100 200 300 400 500"
EXPERIMENTS,0.24401913875598086,"(g) CartPole, n = 10"
EXPERIMENTS,0.24561403508771928,"1
2
3
4
5
time steps
1e4 0 100 200 300 400 500"
EXPERIMENTS,0.24720893141945774,"(h) CartPole, n = 30"
EXPERIMENTS,0.24880382775119617,"Figure 4:
Episodic return v.s. environment time steps. We show evaluation learning curves of Dyna-TD
(black), Dyna-Frequency (red), Dyna-Value (blue), PrioritizedER (forest green), and ER(magenta) with
planning updates n = 10, 30. The dashed line denotes Dyna-TD with an online learned model. All results are
averaged over 20 random seeds after smoothing over a window of size 30. The shade indicates standard error."
EXPERIMENTS,0.2503987240829346,"et al., 2019) is the Dyna variant which performs HC on the learned value function to acquire states
to populate the SC queue. Dyna-Frequency (Pan et al., 2020) is the Dyna variant which performs
HC on the norm of the gradient of the value function to acquire states to populate the SC queue.
For fair comparison, at each environment time step, we stochastically sample the same number
of mini-batches to train those model-free baselines as the number of planning updates in Dyna
variants. We are able to ﬁx the same HC hyper-parameter setting across all environments. Please see
Appendix A.8 for any missing details."
EXPERIMENTS,0.25199362041467305,"Performances on benchmarks. Figure 4 shows the performances of different algorithms on Moun-
tainCar, Acrobot, GridWorld (Figure 2(a)), and CartPole. On these small domains, we focus on
studying our sampling distribution and hence we need to isolate the effect of model errors (by
using a true environment model), though we include our algorithm Dyna-TD with an online learned
model for curiosity. We have the following observations. First, our algorithm Dyna-TD consistently
outperforms PrioritizedER across domains and planning updates. In contrast, the PrioritizedER may
not even outperform regular ER, as occurred in the previous supervised learning experiment. Second,
Dyna-TD’s performance signiﬁcantly improves and even outperforms other Dyna variants when
increasing the planning budget (i.e., planning updates n) from 10 to 30. This validates the utility of
those additional hypothetical experiences acquired by our sampling method. In contrast, both ER and
PrioritizedER show limited gain when increasing the planning budget (i.e., number of mini-batch
updates), which implies the limited utility of those visited experiences. Third, Dyna-Value/Frequency
frequently converge to a sub-optimal policy when using a large number of planning updates, while
Dyna-TD always ﬁnds a better one. It may be that the two Dyna variants frequently generate
high-value/frequency states whose TD errors are low, which wastes samples and leads to serious
distribution bias. Dyna-Frequency additionally suffers from explosive or zero gradients, and hence is
sensitive to hyper-parameters (Pan et al., 2020), which may explain its inconsistent performances."
EXPERIMENTS,0.2535885167464115,"A demo for continuous control.
We demonstrate that our approach can be applied for Mu-
joco (Todorov et al., 2012) continuous control problems with an online learned model and still
achieve superior performance. We use DDPG (Deep Deterministic Policy Gradient) (Lillicrap et al.,
2016; Silver et al., 2014) as an example for use inside our Dyna-TD. Let πθ′ : S 7→A be the actor,
then we set the HC function as h(s)
def
= log |ˆy −Qθ(s, πθ′(s))| where ˆy is the TD target. Figure 5
(a)(b) shows the learning curves of DDPG trained with ER, PrioritizedER, and our Dyna-TD on
Hopper and Walker2d respectively. Since other Dyna variants never show an advantage and are
not relevant to the purpose of this experiment, we no longer include them. Dyna-TD shows quick
improvement as before. This indicates our sampled hypothetical experiences could be helpful for
actor-critic algorithms that are known to be prone to local optimums. Additionally, we note again
that ER outperforms PrioritizedER, as occurred in the discrete control and supervised learning
(PrioritizedL2 is worse than L2) experiments."
EXPERIMENTS,0.2551834130781499,"Autonomous driving application. We study the practical utility of our method in a relatively large
autonomous driving application (Leurent, 2018) with an online learned model. We use the roundabout-
v0 domain (Figure 6 (a)). The agent learns to go through a roundabout by lane change and longitude"
EXPERIMENTS,0.2567783094098884,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2583732057416268,"1
2
3
4
5
time steps
1e5 0 500 1000 1500 2000 2500"
EXPERIMENTS,0.25996810207336524,"Dyna-TD
PrioritizedER
ER
Dyna-Frequency
Dyna-Value"
EXPERIMENTS,0.26156299840510366,(a) Hopper-v2
EXPERIMENTS,0.2631578947368421,"1
2
3
4
5
time steps
1e5 0 500 1000 1500 2000 2500"
EXPERIMENTS,0.2647527910685805,"Dyna-TD
PrioritizedER
ER
Dyna-Frequency
Dyna-Value"
EXPERIMENTS,0.266347687400319,(b) Walker2d-v2
EXPERIMENTS,0.2679425837320574,"Figure 5: (a) (b) show episodic
returns v.s. environment time steps
of Dyna-TD (black) with an on-
line learned model, and other com-
petitors on Hopper and Walker2d
respectively. Results are averaged
over 5 random seeds after smooth-
ing over a window of size 30. The
shade indicates standard error."
EXPERIMENTS,0.26953748006379585,(a) roundabout
EXPERIMENTS,0.2711323763955343,"0.0
0.5
1.0
1.5
Driving time steps
1e3 0 10 13"
EXPERIMENTS,0.2727272727272727,Cumulative
EXPERIMENTS,0.2743221690590112,Number
EXPERIMENTS,0.2759170653907496,"of
Car
Crashes
(50runs)"
EXPERIMENTS,0.27751196172248804,"ER
PrioritizedER
Dyna-TD"
EXPERIMENTS,0.27910685805422647,(b) Num of car crashes
EXPERIMENTS,0.2807017543859649,"0.0
0.2
0.4
0.6
0.8
1.0
time steps
1e4 2 6 8"
EXPERIMENTS,0.2822966507177033,Average speed
EXPERIMENTS,0.2838915470494418,"per
Episode
(50runs)"
EXPERIMENTS,0.28548644338118023,(c) Avg. speed
EXPERIMENTS,0.28708133971291866,"0.0
0.2
0.4
0.6
0.8
1.0
time steps
1e4 8.6 8.8 9.0 9.2 9.4"
EXPERIMENTS,0.2886762360446571,"ER
PrioritizedER
Dyna-TD"
EXPERIMENTS,0.2902711323763955,(d) Episodic return
EXPERIMENTS,0.291866028708134,"Figure 6: (a) shows the roundabout domain with S ⊂R90. (b) shows crashes v.s. total driving time steps during
policy evaluation. (c) shows the average speed per evaluation episode v.s. environment time steps. (d) shows the
episodic return v.s. trained environment time steps. We show Dyna-TD (black) with an online learned model,
PrioritizedER (forest green), and ER (magenta). Results are averaged over 50 random seeds after smoothing
over a window of size 30. The shade indicates standard error."
EXPERIMENTS,0.2934609250398724,"control. The reward is designed such that the car should go through the roundabout as fast as possible
without collision. We observe that all algorithms perform similarly when evaluating algorithms by
episodic return (Figure 6 (d)). In contrast, there is a signiﬁcantly lower number of car crashes with
the policy learned by our algorithm, as shown in Figure 6(b). Figure 6 (c) suggests that ER and
PrioritizedER gain reward mainly due to fast speed which potentially incur more car crashes. The
conventional prioritized ER method still incurs many crashes, which may indicate its prioritized
sampling distribution does not provide enough crash experiences to learn."
DISCUSSION,0.29505582137161085,"6
DISCUSSION"
DISCUSSION,0.2966507177033493,"We provide theoretical insight into the error-based prioritized sampling by establishing its equivalence
to the uniform sampling for a cubic power objective in a supervised learning setting. Then we identify
two drawbacks of prioritized ER: outdated priorities and insufﬁcient sample space coverage. We
mitigate the two limitations by SGLD sampling method with empirical veriﬁcation. Our empirical
results on both discrete and continuous control domains show the efﬁcacy of our method."
DISCUSSION,0.2982456140350877,"There are several promising future directions. First, a natural follow-up question is how a model
should be learned to beneﬁt our sampling method. Existing results show that learning a model while
considering how to use it should make the policy robust to model errors (Farahmand et al., 2017;
Farahmand, 2018). Second, one may apply our approach with a model in some latent space (Hamilton
et al., 2014; Wahlström et al., 2015; Ha & Schmidhuber, 2018; Hafner et al., 2019; Schrittwieser et al.,
2020), which enables our method to scale to large domains. Third, since there are existing works
examining how ER is affected by boostrap return (Daley & Amato, 2019), by buffer or mini-batch
size (Zhang & Sutton, 2017; Liu & Zou, 2017), and by number of environment steps taken per
gradient step (Fu et al., 2019; van Hasselt et al., 2018; Fedus et al., 2020). It is worth studying the
theoretical implications of those design choices and their effects on prioritized ER’s efﬁcacy."
DISCUSSION,0.29984051036682613,"Last, as our cubic objective explains only one version of the error-based prioritization, efforts should
also be made to theoretically interpret other prioritized sampling distributions, such as distribution
location or reward-based prioritization (Lambert et al., 2020). It is interesting to explore whether these
alternatives can also be formulated as surrogate objectives. Furthermore, a recent work by Fujimoto
et al. (2020) establishes an equivalence between various prioritized sampling distributions and uniform
sampling for different loss functions, which bears similarities to our Theorem 1. It is interesting to
study if those general loss functions have faster convergence rate as shown in our Theorem 2."
DISCUSSION,0.3014354066985646,Under review as a conference paper at ICLR 2022
ETHIC STATEMENT,0.30303030303030304,"7
ETHIC STATEMENT"
ETHIC STATEMENT,0.30462519936204147,"This work is about the methodology of how to sample hypothetical experiences in model-based rein-
forcement learning efﬁciently. The potential impact of this work is likely to be further improvement
of sample efﬁciency of reinforcement learning methods, which should be generally beneﬁcial to the
reinforcement learning research community. We have not considered speciﬁc applications or practical
scenarios as the goal of this work. Hence, it does not have any direct ethical consequences."
REPRODUCIBLE STATEMENT,0.3062200956937799,"8
REPRODUCIBLE STATEMENT"
REPRODUCIBLE STATEMENT,0.3078149920255183,"We commit to ensuring that other researchers with reasonable background knowledge in our area
can reproduce our theoretical and empirical results. We provide detailed theoretical derivation for
our theorems 1 and 2 in the Appendix, where there is a clear table of contents pointing to different
concrete mathematical derivations for these theorems. We also provide sufﬁcient experimental details
to reproduce our empirical results in Appendix A.8. We will provide our repository upon acceptance
or reviewer’s request."
REFERENCES,0.3094098883572568,REFERENCES
REFERENCES,0.31100478468899523,"Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., and et al. TensorFlow: Large-scale
machine learning on heterogeneous systems. Software available from tensorﬂow.org, 2015."
REFERENCES,0.31259968102073366,"Adam, S. and Busoniu, L. Experience Replay for Real-Time Reinforcement Learning Control.
Systems, 2012."
REFERENCES,0.3141945773524721,"Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J.,
Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. Advances in Neural Information
Processing Systems, pp. 5048–5058, 2017."
REFERENCES,0.3157894736842105,"Bertsekas, D. P. Neuro-Dynamic Programming. Springer US, 2009."
REFERENCES,0.31738437001594894,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
OpenAI Gym. arXiv:1606.01540, 2016."
REFERENCES,0.3189792663476874,"Chelu, V., Precup, D., and van Hasselt, H. Forethought and hindsight in credit assignment. Advances
in Neural Information Processing Systems, 2020."
REFERENCES,0.32057416267942584,"Chiang, T.-S., Hwang, C.-R., and Sheu, S. J. Diffusion for global optimization in Rn. SIAM Journal
on Control and Optimization, pp. 737–753, 1987."
REFERENCES,0.32216905901116427,"Corneil, D. S., Gerstner, W., and Brea, J. Efﬁcient model-based deep reinforcement learning with
variational state tabulation. In International Conference on Machine Learning, pp. 1049–1058,
2018."
REFERENCES,0.3237639553429027,"Daley, B. and Amato, C. Reconciling lambda-returns with experience replay. Advances in Neural
Information Processing Systems, pp. 1133–1142, 2019."
REFERENCES,0.3253588516746411,"de Bruin, T., Kober, J., Tuyls, K., and Babuska, R. Experience selection in deep reinforcement
learning for control. Journal of Machine Learning Research, 2018."
REFERENCES,0.3269537480063796,"Degris, T., Pilarski, P. M., and Sutton, R. S. Model-free reinforcement learning with continuous
action in practice. In American Control Conference (ACC), 2012."
REFERENCES,0.32854864433811803,"Durmus, A. and Moulines, E. Nonasymptotic convergence analysis for the unadjusted Langevin
algorithm. The Annals of Applied Probability, pp. 1551–1587, 2017."
REFERENCES,0.33014354066985646,"Farahmand, A.-m. Iterative value-aware model learning. Advances in Neural Information Processing
Systems, pp. 9072–9083, 2018."
REFERENCES,0.3317384370015949,"Farahmand, A.-M., Barreto, A., and Nikovski, D. Value-Aware Loss Function for Model-based
Reinforcement Learning. International Conference on Artiﬁcial Intelligence and Statistics, pp.
1486–1494, 2017."
REFERENCES,0.3333333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.3349282296650718,"Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland, M., and Dabney, W.
Revisiting fundamentals of experience replay. International Conference on Machine Learning, pp.
3061–3071, 2020."
REFERENCES,0.3365231259968102,"François-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., and Pineau, J. An introduction to
deep reinforcement learning. Foundations and Trends R⃝in Machine Learning, pp. 219–354, 2018."
REFERENCES,0.33811802232854865,"Fu, J., Kumar, A., Soh, M., and Levine, S. Diagnosing bottlenecks in deep q-learning algorithms.
International Conference on Machine Learning, pp. 2021–2030, 2019."
REFERENCES,0.3397129186602871,"Fujimoto, S., Meger, D., and Precup, D. An equivalence between loss functions and non-uniform
sampling in experience replay. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.3413078149920255,"Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks.
In International Conference on Artiﬁcial Intelligence and Statistics, 2010."
REFERENCES,0.34290271132376393,"Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.
International Conference on Learning Representations, 2015."
REFERENCES,0.3444976076555024,"Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap, T., Levine, S., Larochelle, H., and Bengio, Y.
Recall traces: Backtracking models for efﬁcient reinforcement learning. International Conference
on Learning Representations, 2019."
REFERENCES,0.34609250398724084,"Gu, S., Lillicrap, T. P., Sutskever, I., and Levine, S. Continuous Deep Q-Learning with Model-based
Acceleration. In International Conference on Machine Learning, pp. 2829–2838, 2016."
REFERENCES,0.34768740031897927,"Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. Advances in Neural
Information Processing Systems, pp. 2450–2462, 2018."
REFERENCES,0.3492822966507177,"Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning
latent dynamics for planning from pixels. International Conference on Machine Learning, pp.
2555–2565, 2019."
REFERENCES,0.3508771929824561,"Hamilton, W. L., Fard, M. M., and Pineau, J. Efﬁcient learning and planning with compressed
predictive states. Journal of Machine Learning Research, 2014."
REFERENCES,0.3524720893141946,"Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot,
B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning.
AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.35406698564593303,"Holland, G. Z., Talvitie, E., and Bowling, M. The effect of planning shape on dyna-style planning in
high-dimensional state spaces. CoRR, abs/1806.01825, 2018."
REFERENCES,0.35566188197767146,"Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., and Silver, D.
Distributed prioritized experience replay. International Conference on Learning Representations,
2018."
REFERENCES,0.3572567783094099,"Huber, P. J. Robust estimation of a location parameter. Annals of Mathematical Statistics, pp. 73–101,
1964."
REFERENCES,0.3588516746411483,"Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy
optimization. Advances in Neural Information Processing Systems, pp. 12519–12530, 2019."
REFERENCES,0.36044657097288674,"Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey. Journal of
Artiﬁcial Intelligence Research, pp. 237–285, 1996."
REFERENCES,0.3620414673046252,"Kingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on
Learning Representations, 2014."
REFERENCES,0.36363636363636365,"Lambert, N., Amos, B., Yadan, O., and Calandra, R. Objective mismatch in model-based reinforce-
ment learning. arXiv preprint arXiv:2002.04523, 2020."
REFERENCES,0.3652312599681021,"Leurent, E. An environment for autonomous driving decision-making. GitHub repository https:
//github.com/eleurent/highway-env, 2018."
REFERENCES,0.3668261562998405,Under review as a conference paper at ICLR 2022
REFERENCES,0.3684210526315789,"Leurent, E., Blanco, Y., Eﬁmov, D., and Maillard, O. Approximate robust control of uncertain
dynamical systems. CoRR, abs/1903.00220, 2019."
REFERENCES,0.3700159489633174,"Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
Continuous control with deep reinforcement learning. International Conference on Learning
Representations, 2016."
REFERENCES,0.37161084529505584,"Lin, L.-J. Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching.
Machine Learning, 1992."
REFERENCES,0.37320574162679426,"Liu, R. and Zou, J. The effects of memory replay in reinforcement learning. Conference on
Communication, Control, and Computing, 2017."
REFERENCES,0.3748006379585327,"Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., and et al. Human-level control through deep
reinforcement learning. Nature, 2015a."
REFERENCES,0.3763955342902711,"Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou,
I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through
deep reinforcement learning. Nature, 2015b."
REFERENCES,0.37799043062200954,"Moore, A. W. and Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and
less time. Machine learning, pp. 103–130, 1993."
REFERENCES,0.379585326953748,"Novati, G. and Koumoutsakos, P. Remember and forget for experience replay. International
Conference on Machine Learning, pp. 4851–4860, 2019."
REFERENCES,0.38118022328548645,"Oh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. International Conference on Machine
Learning, pp. 3878–3887, 2018."
REFERENCES,0.3827751196172249,"Oh, Y., Lee, K., Shin, J., Yang, E., and Hwang, S. J. Learning to sample with local and global
contexts in experience replay buffer. International Conference on Learning Representations, 2021."
REFERENCES,0.3843700159489633,"Pan, Y., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at
replay mechanisms for sample-based planning in continuous state domains. In International Joint
Conference on Artiﬁcial Intelligence, pp. 4794–4800, 2018."
REFERENCES,0.38596491228070173,"Pan, Y., Yao, H., Farahmand, A.-m., and White, M. Hill climbing on value estimates for search-control
in dyna. International Joint Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.3875598086124402,"Pan, Y., Mei, J., and massoud Farahmand, A. Frequency-based search-control in dyna. In International
Conference on Learning Representations, 2020."
REFERENCES,0.38915470494417864,"Roberts, Gareth O.and Tweedie, R. L. Exponential convergence of langevin distributions and their
discrete approximations. Bernoulli, pp. 341–363, 1996."
REFERENCES,0.39074960127591707,"Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized Experience Replay. In International
Conference on Learning Representations, 2016."
REFERENCES,0.3923444976076555,"Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart,
E., Hassabis, D., Graepel, T., Lillicrap, T., and Silver, D. Mastering atari, go, chess and shogi by
planning with a learned model. Nature, pp. 604–609, 2020."
REFERENCES,0.3939393939393939,"Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy
gradient algorithms. In International Conference on Machine Learning, pp. I–387–I–395, 2014."
REFERENCES,0.39553429027113235,"Sun, P., Zhou, W., and Li, H. Attentive experience replay. AAAI Conference on Artiﬁcial Intelligence,
pp. 5900–5907, 2020."
REFERENCES,0.39712918660287083,"Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine Learning, 1990."
REFERENCES,0.39872408293460926,"Sutton, R. S. Integrated modeling and control based on reinforcement learning and dynamic program-
ming. In Advances in Neural Information Processing Systems, 1991."
REFERENCES,0.4003189792663477,Under review as a conference paper at ICLR 2022
REFERENCES,0.4019138755980861,"Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, second
edition, 2018."
REFERENCES,0.40350877192982454,"Sutton, R. S., Szepesvári, C., Geramifard, A., and Bowling, M. Dyna-style planning with linear func-
tion approximation and prioritized sweeping. Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 528–536, 2008."
REFERENCES,0.405103668261563,"Szepesvári, Cs. Algorithms for Reinforcement Learning. Morgan Claypool Publishers, 2010."
REFERENCES,0.40669856459330145,"Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012."
REFERENCES,0.4082934609250399,"van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement
learning and the deadly triad. Deep Reinforcement Learning Workshop at Advances in Neural
Information Processing Systems, 2018."
REFERENCES,0.4098883572567783,"van Hasselt, H. P., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement
learning? Advances in Neural Information Processing Systems, pp. 14322–14333, 2019."
REFERENCES,0.41148325358851673,"van Seijen, H. and Sutton, R. S. A deeper look at planning as learning from replay. In International
Conference on Machine Learning, pp. 2314–2322, 2015."
REFERENCES,0.4130781499202552,"Wahlström, N., Schön, T. B., and Deisenroth, M. P. From pixels to torques: Policy learning with deep
dynamical models. Deep Learning Workshop at International Conference on Machine Learning,
2015."
REFERENCES,0.41467304625199364,"Watkins, C. J. C. H. and Dayan, P. Q-learning. Machine Learning, pp. 279–292, 1992."
REFERENCES,0.41626794258373206,"Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient Langevin dynamics. In
International Conference on Machine Learning, pp. 681–688, 2011."
REFERENCES,0.4178628389154705,"Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience replay optimization. International Joint
Conference on Artiﬁcial Intelligence, pp. 4243–4249, 2019."
REFERENCES,0.4194577352472089,"Zhang, S. and Sutton, R. S. A Deeper Look at Experience Replay. Deep Reinforcement Learning
Symposium at Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.42105263157894735,"Zhang, Y., Liang, P., and Charikar, M. A hitting time analysis of stochastic gradient langevin
dynamics. Conference on Learning Theory, pp. 1980–2022, 2017."
REFERENCES,0.4226475279106858,Under review as a conference paper at ICLR 2022
REFERENCES,0.42424242424242425,"A
APPENDIX"
REFERENCES,0.4258373205741627,The appendix includes the following contents:
REFERENCES,0.4274322169059011,"1. Section A.1: background in Dyna architecture and two Dyna variants.
2. Section A.2: background of Langevin dynamics.
3. Section A.3: the full proof of Theorem 1.
4. Section A.4: the full proof of Theorem 2 and its simulations.
5. Section A.5: the theorem characterizing the error bound between the sampling distribution
estimated by using a true model and a learned model. It includes the the full proof.
6. Section A.6: a discussion and some empirical study of using high power objectives.
7. Section A.7: supplementary experimental results: training error results to check the negative
effects of the limitations of prioritized sampling; results to verify the equivalence between
prioritized sampling and cubic power; results/evaluation learning curve to supplement the
autonomous driving application; results on MazeGridWorld from (Pan et al., 2020).
8. Section A.8: details for reproducible research."
REFERENCES,0.42902711323763953,"A.1
BACKGROUND IN DYNA"
REFERENCES,0.430622009569378,"Dyna integrates model-free and model-based policy updates in an online RL setting (Sutton, 1990).
As shown in Algorithm 2, at each time step, a Dyna agent uses the real experience to learn a model
and performs a model-free policy update. During the planning stage, simulated experiences are
acquired from the model to further improve the policy. It should be noted that, in Dyna, the concept of
planning refers to any computational process which leverages a model to improve policy, according
to Sutton & Barto (2018), Chapter 8. The mechanism of generating states or state-action pairs from
which to query the model is called search-control, which is of critical importance to improving
sample efﬁciency. The below algorithm shows a naive search-control strategy: simply use visited
state-action pairs and store them into the search-control queue. During the planning stage, these pairs
are uniformly sampled according to the original paper."
REFERENCES,0.43221690590111644,"The recent works by Pan et al. (2019, 2020) propose two search-control strategies to generate states.
The ﬁrst one is to search high-value states actively, and the second one is to search states whose
values are difﬁcult to learn."
REFERENCES,0.43381180223285487,"However, there are several limitations of the two previous works. First, they do not provide any
theoretical justiﬁcation to use the stochastic gradient ascent trajectories for search-control. Second,
HC on gradient norm and Hessian norm of the learned value function (Pan et al., 2020) suffers
from great computation cost and zero or explosive gradient due to the high order differentiation
(i.e., ∇s||∇sv(s)||) as suggested by the authors. When using ReLu as activation functions, such
high order differentiation almost results in zero gradients. We empirically veriﬁed this phenomenon.
And this phenomenon can also be veriﬁed by intuition from the work by Goodfellow et al. (2015),
which suggests that ReLU neural networks are locally almost linear. Then it is not surprising to have
zero higher order derivatives. Third, the two methods are prone to result in sub-optimal policies:
consider that the values of states are relatively well-learned and ﬁxed, then value-based search-control
(Dyna-Value) would still ﬁnd those high-value states even though they might already have low TD
error."
REFERENCES,0.4354066985645933,"A.2
DISCUSSION ON THE LANGEVIN DYNAMICS MONTE CARLO METHOD"
REFERENCES,0.4370015948963317,"Theoretical mechanism. Deﬁne a SDE: dW(t) = ∇U(Wt)dt +
√"
REFERENCES,0.43859649122807015,"2dBt, where Bt ∈Rd is a
d-dimensional Brownian motion and U is a continuous differentiable function. It turns out that
the Langevin diffusion (Wt)t≥0 converges to a unique invariant distribution p(x) ∝exp (U(x))
(Chiang et al., 1987). By applying the Euler-Maruyama discretization scheme to the SDE, we
acquire the discretized version Yk+1 = Yk + αk+1∇U(Yk) + √2αk+1Zk+1 where (Zk)k≥1 is an
i.i.d. sequence of standard d-dimensional Gaussian random vectors and (αk)k≥1 is a sequence of
step sizes. It has been proved that the limiting distribution of the sequence (Yk)k≥1 converges to the
invariant distribution of the underlying SDE (Roberts, 1996; Durmus & Moulines, 2017). As a result,
considering U(·) as log |δ(·)|, Y as s justiﬁes our SGLD sampling method.."
REFERENCES,0.44019138755980863,Under review as a conference paper at ICLR 2022
REFERENCES,0.44178628389154706,Algorithm 2 Tabular Dyna
REFERENCES,0.4433811802232855,"Initialize Q(s, a); initialize model M(s, a), ∀(s, a) ∈S × A
while true do"
REFERENCES,0.4449760765550239,"observe s, take action a by ϵ-greedy w.r.t Q(s, ·)
execute a, observe reward R and next State s′
Q-learning update for Q(s, a)
update model M(s, a) (i.e. by counting)
store (s, a) into search-control queue // this is a naive search-control strategy
for i=1:d do"
REFERENCES,0.44657097288676234,"sample (˜s, ˜a) from search-control queue
(˜s′, ˜R) ←M(˜s, ˜a) // simulated transition
Q-learning update for Q(˜s, ˜a) // planning updates/steps"
REFERENCES,0.4481658692185008,"A.3
PROOF FOR THEOREM 1"
REFERENCES,0.44976076555023925,"Theorem 1. For a constant c determined by θ, T , we have"
REFERENCES,0.4513556618819777,"E
(x,y)∼uniform(T )"
REFERENCES,0.4529505582137161,"""
1
3 · ∂|fθ(x) −y|3 ∂θ #"
REFERENCES,0.45454545454545453,"= c ·
E
(x,y)∼q(x,y;θ)"
REFERENCES,0.45614035087719296,"""
1
2 · ∂(fθ(x) −y)2 ∂θ #"
REFERENCES,0.45773524720893144,"Proof. For the l.h.s., we have,"
REFERENCES,0.45933014354066987,"E
(x,y)∼uniform(T )"
REFERENCES,0.4609250398724083,"""
1
3 · ∂|fθ(x) −y|3 ∂θ # (4)"
REFERENCES,0.4625199362041467,"=
1
3 · n · n
X i=1"
REFERENCES,0.46411483253588515,∂|fθ(x(i)) −y(i)|3
REFERENCES,0.46570972886762363,"∂θ
(5)"
REFERENCES,0.46730462519936206,"=
1
3 · n · n
X i=1"
REFERENCES,0.4688995215311005,"∂

(fθ(x(i)) −y(i))2  3 2"
REFERENCES,0.4704944178628389,"∂θ
(6)"
REFERENCES,0.47208931419457734,"=
1
3 · n · n
X i=1"
REFERENCES,0.47368421052631576,"∂

(fθ(x(i)) −y(i))2 3 2"
REFERENCES,0.47527910685805425,"∂(fθ(x) −y)2
· ∂(fθ(x(i)) −y(i))2"
REFERENCES,0.4768740031897927,"∂θ
(7)"
REFERENCES,0.4784688995215311,"=
1
2 · n · n
X"
REFERENCES,0.4800637958532695,"i=1
|fθ(x(i)) −y(i)| · ∂(fθ(x(i)) −y(i))2"
REFERENCES,0.48165869218500795,"∂θ
.
(8)"
REFERENCES,0.48325358851674644,"On the other hand, for the r.h.s., we have,"
REFERENCES,0.48484848484848486,"E
(x,y)∼q(x,y;θ)"
REFERENCES,0.4864433811802233,"""
1
2 · ∂(fθ(x) −y)2 ∂θ # (9) = 1 2 · n
X"
REFERENCES,0.4880382775119617,"i=1
q(xi, yi; θ) · ∂(fθ(x(i)) −y(i))2"
REFERENCES,0.48963317384370014,"∂θ
(10)"
REFERENCES,0.49122807017543857,"=
n
Pn
j=1 |fθ(xj) −yj| ·"
REFERENCES,0.49282296650717705,"""
1
2 · n · n
X"
REFERENCES,0.4944178628389155,"i=1
|fθ(x(i)) −y(i)| · ∂(fθ(x(i)) −y(i))2 ∂θ # (11)"
REFERENCES,0.4960127591706539,"=
n
Pn
j=1 |fθ(xj) −yj| ·
E
(x,y)∼uniform(T )"
REFERENCES,0.49760765550239233,"""
1
3 · ∂|fθ(x) −y|3 ∂θ #"
REFERENCES,0.49920255183413076,".
(12)"
REFERENCES,0.5007974481658692,Setting c =
REFERENCES,0.5023923444976076,"Pn
i=1 |fθ(xi)−yi|"
REFERENCES,0.5039872408293461,"n
completes the proof."
REFERENCES,0.5055821371610846,Under review as a conference paper at ICLR 2022
REFERENCES,0.507177033492823,"A.4
PROOF FOR THEOREM 2"
REFERENCES,0.5087719298245614,"Theorem 2. Let n be a positive integer (i.e., the number of training samples). Let xt, ˜xt ∈Rn be the
target estimates of all samples at time t. Let xt(i)(i ∈[n], [n]
def
= {1, 2, ..., n}) denote the ith element
in the vector. We deﬁne the following notations."
REFERENCES,0.5103668261562998,"ℓ2(x, y)
def
= 1 2 · n
X"
REFERENCES,0.5119617224880383,"i=1
(x(i) −y(i))2,
ℓ3(x, y)
def
= 1 3 · n
X"
REFERENCES,0.5135566188197768,"i=1
|x(i) −y(i)|3,"
REFERENCES,0.5151515151515151,"δt
def
= n
X"
REFERENCES,0.5167464114832536,"i=1
δt(i) = n
X"
REFERENCES,0.518341307814992,"i=1
|xt(i) −y(i)|,
˜δt
def
= n
X"
REFERENCES,0.5199362041467305,"i=1
˜δt(i) = n
X"
REFERENCES,0.5215311004784688,"i=1
|˜xt(i) −y(i)|, ∀t ≥0"
REFERENCES,0.5231259968102073,"where y(i) ∈R is the training target for the ith training sample. Let {xt}t≥0 and {˜xt}t≥0 be
generated by using ℓ2, ℓ3 objectives respectively. That is, ∀i ∈[n],"
REFERENCES,0.5247208931419458,dxt(i)
REFERENCES,0.5263157894736842,"dt
= −η · dℓ2(xt, y)"
REFERENCES,0.5279106858054227,"dxt(i)
,
d˜xt(i)"
REFERENCES,0.529505582137161,"dt
= −η · dℓ3(˜xt, y)"
REFERENCES,0.5311004784688995,"d˜xt(i)
."
REFERENCES,0.532695374800638,"Assume the same initialization x0 = ˜x0. Then: (i) For all i ∈[n], deﬁne the following hitting time,
which is the minimum time that the absolute error takes to be ≤ϵ(i),"
REFERENCES,0.5342902711323764,"tϵ(i)
def
= min
t {t ≥0 : δt(i) ≤ϵ(i)},
˜tϵ(i)
def
= min
t {t ≥0 : ˜δt(i) ≤ϵ(i)}."
REFERENCES,0.5358851674641149,"Then, ∀i ∈[n] s.t. δ0(i) > 1, given an absolute error threshold ϵ(i) ≥0, there exists ϵ0(i) ∈(0, 1),
such that for all ϵ(i) > ϵ0(i), tϵ(i) ≥˜tϵ(i)."
REFERENCES,0.5374800637958532,"(ii) Deﬁne the following quantity, for all t ≥0,"
REFERENCES,0.5390749601275917,"H−1
t
def
= 1 n · n
X i=1"
REFERENCES,0.5406698564593302,"1
δt(i) = 1 n · n
X i=1"
REFERENCES,0.5422647527910686,"1
|xt(i) −y(i)|.
(13)"
REFERENCES,0.543859649122807,"Given any 0 < ϵ ≤δ0 = Pn
i=1 δ0(i), deﬁne the following hitting time, which is the minimum time
that the total absolute error takes to be ≤ϵ,"
REFERENCES,0.5454545454545454,"tϵ
def
= min
t {t ≥0 : δt ≤ϵ},
˜tϵ
def
= min
t {t ≥0 : ˜δt ≤ϵ}.
(14)"
REFERENCES,0.5470494417862839,"If there exists δ0 ∈R and 0 < ϵ ≤δ0 such that the following holds,"
REFERENCES,0.5486443381180224,"H−1
0
≤log (δ0/ϵ) δ0"
REFERENCES,0.5502392344497608,"ϵ −1 ,
(15)"
REFERENCES,0.5518341307814992,"then we have, tϵ ≥˜tϵ, which means gradient descent using the cubic loss function will achieve the
total absolute error threshold ϵ faster than using the square loss function."
REFERENCES,0.5534290271132376,"Proof. First part. (i). For the ℓ2 loss function, for all i ∈[n] and t ≥0, we have,"
REFERENCES,0.5550239234449761,"dδt(i) dt
= n
X j=1"
REFERENCES,0.5566188197767146,"dδt(i)
dxt(j) · dxt(j)"
REFERENCES,0.5582137161084529,"dt
(16)"
REFERENCES,0.5598086124401914,= dδt(i)
REFERENCES,0.5614035087719298,dxt(i) · dxt(i) dt
REFERENCES,0.5629984051036683, dδt(i)
REFERENCES,0.5645933014354066,"dxt(j) = 0 for all i ̸= j

(17)"
REFERENCES,0.5661881977671451,"= sgn{xt(i) −y(i)} · (−η) · dℓ2(xt, y)"
REFERENCES,0.5677830940988836,"dxt(i)
(18)"
REFERENCES,0.569377990430622,"= sgn{xt(i) −y(i)} · (−η) · (xt(i) −y(i))
(19)
= −η |xt(i) −y(i)|
(20)
= −η · δt(i),
(21)"
REFERENCES,0.5709728867623605,"which implies that,"
REFERENCES,0.5725677830940988,d{log δt(i)}
REFERENCES,0.5741626794258373,"dt
=
1
δt(i) · dδt(i)"
REFERENCES,0.5757575757575758,"dt
= −η.
(22)"
REFERENCES,0.5773524720893142,Under review as a conference paper at ICLR 2022
REFERENCES,0.5789473684210527,"Taking integral, we have,"
REFERENCES,0.580542264752791,"log δt(i) −log δ0(i) = −η · t.
(23)"
REFERENCES,0.5821371610845295,"Let δt(i) = ϵ(i). We have,"
REFERENCES,0.583732057416268,"tϵ(i)
def
= 1"
REFERENCES,0.5853269537480064,"η · log
δ0(i) δt(i) 
= 1"
REFERENCES,0.5869218500797448,"η · log
δ0(i) ϵ(i)"
REFERENCES,0.5885167464114832,"
.
(24)"
REFERENCES,0.5901116427432217,"On the other hand, for the ℓ3 loss function, we have,"
REFERENCES,0.5917065390749602,"d{˜δt(i)−1} dt
= n
X j=1"
REFERENCES,0.5933014354066986,d˜δt(i)−1
REFERENCES,0.594896331738437,d˜xt(j) · d˜xt(j)
REFERENCES,0.5964912280701754,"dt
(25)"
REFERENCES,0.5980861244019139,= d˜δt(i)−1
REFERENCES,0.5996810207336523,"d˜xt(i)
· d˜xt(i)"
REFERENCES,0.6012759170653907,"dt
(26)"
REFERENCES,0.6028708133971292,"= −
1
˜δt(i)2 · d˜δt(i)"
REFERENCES,0.6044657097288676,d˜xt(i) · d˜xt(i)
REFERENCES,0.6060606060606061,"dt
(27) = −
1"
REFERENCES,0.6076555023923444,"(˜xt(i) −y(i))2 · sgn{˜xt(i) −y(i)} · (−η) · dℓ3(˜xt, y)"
REFERENCES,0.6092503987240829,"d˜xt(i)
(28)"
REFERENCES,0.6108452950558214,= −sgn{˜xt(i) −y(i)}
REFERENCES,0.6124401913875598,"(˜xt(i) −y(i))2
· (−η) · (˜xt(i) −y(i))2 · sgn{˜xt(i) −y(i)}
(29)"
REFERENCES,0.6140350877192983,"= η.
(30)"
REFERENCES,0.6156299840510366,"Taking integral, we have,"
REFERENCES,0.6172248803827751,"1
˜δt(i)
−
1
˜δ0(i)
= η · t.
(31)"
REFERENCES,0.6188197767145136,"Let ˜δt(i) = ϵ(i). We have,"
REFERENCES,0.620414673046252,"˜tϵ(i)
def
= 1"
REFERENCES,0.6220095693779905,"η ·

1
˜δt(i)
−
1
˜δ0(i) 
= 1"
REFERENCES,0.6236044657097288,"η ·
 1"
REFERENCES,0.6251993620414673,"ϵ(i) −
1
˜δ0(i)"
REFERENCES,0.6267942583732058,"
.
(32)"
REFERENCES,0.6283891547049442,"Then we have,"
REFERENCES,0.6299840510366826,tϵ(i) −˜tϵ(i) = 1
REFERENCES,0.631578947368421,"η · log
δ0(i) ϵ(i) 
−1"
REFERENCES,0.6331738437001595,"η ·
 1"
REFERENCES,0.6347687400318979,"ϵ(i) −
1
˜δ0(i)"
REFERENCES,0.6363636363636364,"
(33) = 1"
REFERENCES,0.6379585326953748,"η ·

log
1
ϵ(i) −
1
ϵ(i)"
REFERENCES,0.6395534290271132,"
−

log
1
δ0(i) −
1
˜δ0(i)"
REFERENCES,0.6411483253588517,"
.
(34)"
REFERENCES,0.6427432216905901,"According to x0(i) = ˜x0(i), we have"
REFERENCES,0.6443381180223285,"δ0(i) = |xt(i) −y(i)|
(35)
= |˜xt(i) −y(i)|
(36)"
REFERENCES,0.645933014354067,"= ˜δ0(i).
(37)"
REFERENCES,0.6475279106858054,"Deﬁne the following function, for all x > 0,"
REFERENCES,0.6491228070175439,f(x) = log 1 x −1
REFERENCES,0.6507177033492823,"x.
(38)"
REFERENCES,0.6523125996810207,"We have, the continuous function f is monotonically increasing for x ∈(0, 1] and monotoni-
cally decreasing for x ∈(1, ∞). Also, note that, maxx>0 f(x) = f(1) = −1, limx→0 f(x) =
limx→∞f(x) = −∞."
REFERENCES,0.6539074960127592,"Given δ0(i) = ˜δ0(i) > 1, we have f(δ0(i)) < f(1) = −1. According to the intermediate value
theorem, there exists ϵ0(i) ∈(0, 1), such that f(ϵ0(i)) = f(δ0(i)). Since f(·) is monotonically"
REFERENCES,0.6555023923444976,Under review as a conference paper at ICLR 2022
REFERENCES,0.6570972886762361,"increasing on (0, 1] and monotonically decreasing on (1, ∞), for all ϵ(i) ∈[ϵ0(i), δ0(i)], we have
f(ϵ(i)) ≥f(δ0(i))2. Therefore, we have,"
REFERENCES,0.6586921850079744,tϵ(i) −˜tϵ(i) = 1
REFERENCES,0.6602870813397129,"η · (f(ϵ(i)) −f(δ0(i))) ≥0.
(39)"
REFERENCES,0.6618819776714514,"Second part. (ii). For the square loss function, we have, for all t ≥0, dδt dt = n
X i=1"
REFERENCES,0.6634768740031898,dδt(i)
REFERENCES,0.6650717703349283,"dt
(40)"
REFERENCES,0.6666666666666666,"= −η · n
X"
REFERENCES,0.6682615629984051,"i=1
δt(i)
(by eq. (16))
(41)"
REFERENCES,0.6698564593301436,"= −η · δt,
(42)"
REFERENCES,0.671451355661882,"which implies that,"
REFERENCES,0.6730462519936204,d{log δt}
REFERENCES,0.6746411483253588,"dt
= 1"
REFERENCES,0.6762360446570973,"δt
· dδt"
REFERENCES,0.6778309409888357,"dt = −η.
(43)"
REFERENCES,0.6794258373205742,"Taking integral, we have,"
REFERENCES,0.6810207336523126,"log δt −log δ0 = −η · t.
(44)"
REFERENCES,0.682615629984051,"Let δt = ϵ. We have,"
REFERENCES,0.6842105263157895,"tϵ
def
= 1"
REFERENCES,0.6858054226475279,"η · log
δ0 δt 
= 1"
REFERENCES,0.6874003189792663,"η · log
δ0 ϵ"
REFERENCES,0.6889952153110048,"
.
(45)"
REFERENCES,0.6905901116427432,"After tϵ time, for all i ∈[n], we have,"
REFERENCES,0.6921850079744817,"δtϵ(i) = δ0(i) · exp{−η · tϵ}.
(46)"
REFERENCES,0.69377990430622,"On the other hand, for the cubic loss function, we have, for all t ≥0,"
REFERENCES,0.6953748006379585,"dH−1
t
dt
= 1 n · n
X i=1"
REFERENCES,0.696969696969697,d{˜δt(i)−1}
REFERENCES,0.6985645933014354,"dt
(47)"
REFERENCES,0.7001594896331739,"= η.
(by eq. (25))
(48)"
REFERENCES,0.7017543859649122,"Taking integral, we have,"
REFERENCES,0.7033492822966507,"H−1
t
−H−1
0
= η · t,
(49)"
REFERENCES,0.7049441786283892,"which means given a H−1
t
value, we can calculate the hitting time as, t = 1"
REFERENCES,0.7065390749601276,"η ·
 
H−1
t
−H−1
0

.
(50)"
REFERENCES,0.7081339712918661,"Now consider after tϵ time, using gradient descent with the square loss function we have δtϵ(i) =
δ0(i) · exp{−η · tϵ} for all i ∈[n], which corresponds to,"
REFERENCES,0.7097288676236044,"H−1
tϵ
= 1 n · n
X i=1"
REFERENCES,0.7113237639553429,"1
δtϵ(i)
(51) = 1 n · n
X i=1"
REFERENCES,0.7129186602870813,"1
δ0(i) · exp{−η · tϵ}.
(by eq. (46))
(52)"
REFERENCES,0.7145135566188198,"2Note that ϵ(i) < δ0(i) by the design of using gradient descent updating rule. If the two are equal,
tϵ(i) = ˜tϵ(i) = 0 holds trivially."
REFERENCES,0.7161084529505582,Under review as a conference paper at ICLR 2022
REFERENCES,0.7177033492822966,"Therefore, the hitting time of using gradient descent with the cubic loss function to achieve the H−1
tϵ
value is,"
REFERENCES,0.7192982456140351,˜tϵ = 1
REFERENCES,0.7208931419457735,"η ·
 
H−1
tϵ −H−1
0

(53) = 1 η · 1
n · n
X i=1"
REFERENCES,0.722488038277512,"1
δ0(i) · exp{−η · tϵ} −1 n · n
X i=1"
REFERENCES,0.7240829346092504,"1
δ0(i) ! (54) = 1"
REFERENCES,0.7256778309409888,"η · (exp{η · tϵ} −1) · H−1
0
(55) ≤1"
REFERENCES,0.7272727272727273,η · (exp{η · tϵ} −1) · log (δ0/ϵ) δ0
REFERENCES,0.7288676236044657,"ϵ −1
(by eq. (15))
(56) = 1"
REFERENCES,0.7304625199362041,"η ·
δ0"
REFERENCES,0.7320574162679426,"ϵ −1

· log (δ0/ϵ) δ0"
REFERENCES,0.733652312599681,"ϵ −1
(57) = 1"
REFERENCES,0.7352472089314195,"η · log
δ0 ϵ"
REFERENCES,0.7368421052631579,"
(58)"
REFERENCES,0.7384370015948963,"= tϵ,
(by eq. (45))
(59)"
REFERENCES,0.7400318979266348,ﬁnishing the proof.
REFERENCES,0.7416267942583732,Remark. Figure 7 shows the function f(x) = ln 1 x −1
REFERENCES,0.7432216905901117,"x, x > 0. Fix arbitrary x′ > 1, there will be
another root ϵ0 < 1 s.t. f(ϵ0) = f(x′). However, there is no real-valued solution for ϵ0. The solution
in C is ϵ0 = −
1
W (log 1/δ0−1/δ0−πi), where W(·) is a Wright Omega function. Hence, ﬁnding the
exact value of ϵ0 would require a deﬁnition of ordering on complex plane. Our current theorem
statement is sufﬁcient for the purpose of characterizing convergence rate. The theorem states that
there always exists some desired low error level < 1, minimizing the square loss converges slower
than the cubic loss."
REFERENCES,0.74481658692185,"0
1
2
3
4
5
5 4 3 2 1 0"
REFERENCES,0.7464114832535885,"f(x) = ln1
x
1
x"
REFERENCES,0.748006379585327,Figure 7: The function f(x) = ln 1 x −1
REFERENCES,0.7496012759170654,"x, x > 0. The function reaches maximum at x = 1."
REFERENCES,0.7511961722488039,"Simulations. The theorem says that if we want to minimize our loss function to certain small nonzero
error level, the cubic loss function offers faster convergence rate. Intuitively, cubic loss provides
sharper gradient information when the loss is large as shown in Figure 8(a)(b). Here we provides
a simulation. Consider the following minimization problems: minx≥0 x2 and minx≥0 x3. We use"
REFERENCES,0.7527910685805422,the hitting time formulae tϵ = 1
REFERENCES,0.7543859649122807,"η · ln
 δ0"
REFERENCES,0.7559808612440191,"ϵ
	
, ˜tϵ = 1"
REFERENCES,0.7575757575757576,"η ·

1
ϵ −1 δ0"
REFERENCES,0.759170653907496,"
derived in the proof, to compute the"
REFERENCES,0.7607655502392344,hitting time ratio tϵ
REFERENCES,0.7623604465709729,"˜tϵ under different initial values x0 and ﬁnal error value ϵ. In Figure 8(c)(d), we
can see that it usually takes a signiﬁcantly shorter time for the cubic loss to reach a certain xt with
various initial x0 values."
REFERENCES,0.7639553429027113,"A.5
ERROR BOUND BETWEEN SAMPLING DISTRIBUTIONS"
REFERENCES,0.7655502392344498,"We now provide the error bound between the sampling distribution estimated by using a true
model and a learned model. We denote the transition probability distribution under policy π and
the true model as Pπ(r, s′|s), and the learned model as ˆPπ(r, s′|s). Let p(s) and ˆp(s) be the
convergent distributions described in the above sampling method by using the true and learned models
respectively. Let dtv(·, ·) be the total variation distance between the two probability distributions."
REFERENCES,0.7671451355661882,Under review as a conference paper at ICLR 2022
REFERENCES,0.7687400318979266,"Figure 8: (a) show cubic v.s. square function. (b) shows their absolute derivatives. (c) shows the hitting time
ratio v.s. initial value x0 under different target value xt. (d) shows the ratio v.s. the target xt to reach under
different x0. Note that a ratio larger than 1 indicates a longer time to reach the given xt for the square loss."
REFERENCES,0.7703349282296651,"Deﬁne u(s)
def
= |δ(s, y(s))|, ˆu(s)
def
= |δ(s, ˆy(s))|, Z
def
=
R"
REFERENCES,0.7719298245614035,"s∈S u(s)ds, ˆZ
def
=
R"
REFERENCES,0.773524720893142,"s∈S ˆu(s)ds. Then we have
the following bound."
REFERENCES,0.7751196172248804,"Theorem 3. Assume: 1) the reward magnitude is bounded |r| ≤Rmax and deﬁne Vmax
def
= Rmax 1−γ ;"
REFERENCES,0.7767145135566188,"2) the largest model error for a single state is ϵs
def
= maxs dtv(Pπ(·|s), ˆPπ(·|s)) and the to-
tal model error is bounded, i.e.
ϵ
def
=
R"
REFERENCES,0.7783094098883573,"s∈S ϵsds < ∞.
Then ∀s ∈S, |p(s) −ˆp(s)| ≤"
REFERENCES,0.7799043062200957,min( Vmax(p(s)ϵ+ϵs)
REFERENCES,0.7814992025518341,"ˆ
Z
, Vmax(ˆp(s)ϵ+ϵs) Z
)."
REFERENCES,0.7830940988835726,"Proof. First, we bound the estimated temporal difference error. Fix an arbitrary state s ∈S, it is
sufﬁcient the consider the case u(s) > ˆu(s), then"
REFERENCES,0.784688995215311,|u(s) −ˆu(s)| = u(s) −ˆu(s)
REFERENCES,0.7862838915470495,"=E(r,s′)∼Pπ[r + γvπ(s′)] −E(r,s′)∼ˆ
Pπ[r + γvπ(s′)] =
Z"
REFERENCES,0.7878787878787878,"s,r
(r + γvπ(s))(Pπ(s′, r|s) −ˆPπ(s′, r|s))ds′dr"
REFERENCES,0.7894736842105263,≤(Rmax + γ Rmax
REFERENCES,0.7910685805422647,"1 −γ )
Z"
REFERENCES,0.7926634768740032,"s,r
(Pπ(s′, r|s) −ˆPπ(s′, r|s))ds′dr"
REFERENCES,0.7942583732057417,"≤Vmaxdtv(Pπ(·|s), ˆPπ(·|s)) ≤Vmaxϵs"
REFERENCES,0.79585326953748,"Now, we show that |Z −ˆZ| ≤Vmaxϵ."
REFERENCES,0.7974481658692185,"|Z −ˆZ| = |
Z"
REFERENCES,0.7990430622009569,"s∈S
u(s)ds −
Z"
REFERENCES,0.8006379585326954,"s∈S
ˆu(s)ds| = |
Z"
REFERENCES,0.8022328548644339,"s∈S
(u(s) −ˆu(s))ds| ≤
Z"
REFERENCES,0.8038277511961722,"s∈S
|u(s) −ˆu(s)|ds ≤Vmax Z"
REFERENCES,0.8054226475279107,"s∈S
ϵsds = Vmaxϵ"
REFERENCES,0.8070175438596491,Consider the case p(s) > ˆp(s) ﬁrst.
REFERENCES,0.8086124401913876,p(s) −ˆp(s) = u(s)
REFERENCES,0.810207336523126,"Z
−ˆu(s) ˆZ ≤u(s)"
REFERENCES,0.8118022328548644,"Z
−u(s) −Vmaxϵs"
REFERENCES,0.8133971291866029,"ˆZ
= u(s) ˆZ −u(s)Z + ZVmaxϵs Z ˆZ"
REFERENCES,0.8149920255183413,≤u(s)Vmaxϵ + ZVmaxϵs
REFERENCES,0.8165869218500797,"Z ˆZ
= Vmax(p(s)ϵ + ϵs)"
REFERENCES,0.8181818181818182,"ˆZ
Meanwhile, below inequality should also hold:"
REFERENCES,0.8197767145135566,p(s) −ˆp(s) = u(s)
REFERENCES,0.8213716108452951,"Z
−ˆu(s)"
REFERENCES,0.8229665071770335,"ˆZ
≤ˆu(s) + Vmaxϵs"
REFERENCES,0.8245614035087719,"Z
−ˆu(s) ˆZ"
REFERENCES,0.8261562998405104,= ˆu(s) ˆZ −ˆu(s)Z + ˆZVmaxϵs
REFERENCES,0.8277511961722488,"Z ˆZ
≤Vmax(ˆp(s)ϵ + ϵs) Z"
REFERENCES,0.8293460925039873,Under review as a conference paper at ICLR 2022
REFERENCES,0.8309409888357256,"0.00
0.25
0.50
0.75
1.00
1e5 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8325358851674641,(a) σ = 0.1
REFERENCES,0.8341307814992025,"0.00
0.25
0.50
0.75
1.00
1e5 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.835725677830941,"L2
Cubic
Power4"
REFERENCES,0.8373205741626795,(b) σ = 0.5
REFERENCES,0.8389154704944178,"Figure 9: Figure(a)(b) show the testing RMSE as a function of number of mini-batch updates with
increasing noise standard deviation σ added to the training targets. We compare the performances
of Power4(magenta), L2 (black), Cubic (forest green). The results are averaged over 50 random
seeds. The shade indicates standard error. Note that the testing set is not noise-contaminated."
REFERENCES,0.8405103668261563,"Because both the two inequalities must hold, when p(s) −ˆp(s) > 0, we have:"
REFERENCES,0.8421052631578947,p(s) −ˆp(s) ≤min(Vmax(p(s)ϵ + ϵs)
REFERENCES,0.8437001594896332,"ˆZ
, Vmax(ˆp(s)ϵ + ϵs) Z
)"
REFERENCES,0.8452950558213717,It turns out that the bound is the same when p(s) ≤ˆp(s). This completes the proof.
REFERENCES,0.84688995215311,"A.6
HIGH POWER LOSS FUNCTIONS"
REFERENCES,0.8484848484848485,"We would like to point out that directly using a high power objective in general problems is unlikely
to have an advantage."
REFERENCES,0.8500797448165869,"First, notice that our convergence rate is characterized w.r.t. to the expected updating rule, not
stochastic gradient updating rule. When using a stochastic sample to estimate the gradient, high
power objectives are sensitive to the outliers as they augment the effect of noise. Robustness to
outliers is also the motivation behind the Huber loss (Huber, 1964) which, in fact, uses low power
error in most places so it can be less sensitive to outliers."
REFERENCES,0.8516746411483254,"We conduct experiments to examine the effect of noise on using high power objectives. We use the
same dataset as described in Section 3.3. We use a training set with 4k training examples. The naming
rules are as follows. Cubic is minimizing the cubic objective (i.e. minθ 1"
REFERENCES,0.8532695374800638,"n
Pn
i=1 |fθ(xi) −yi|3) by
uniformly sampling, and Power4 is minθ 1"
REFERENCES,0.8548644338118022,"n
Pn
i=1(fθ(xi) −yi)4 by uniformly sampling."
REFERENCES,0.8564593301435407,"Figure 9 (a)(b) shows the learning curves of uniformly sampling for Cubic and for Power4 trained
by adding noises with standard deviation σ = 0.1, 0.5 respectively to the training targets. It is not
surprising that all algorithms learn slower when we increase the noise variance added to the target
variables. However, one can see that high power objectives is more sensitive to noise variance added
to the targets than the regular L2: when σ = 0.1, the higher power objectives perform better than the
regular L2; after increasing σ to 0.5, Cubic becomes almost the same as L2, while Power4 becomes
worse than L2."
REFERENCES,0.8580542264752791,"Second, it should be noted that in our theorem, we do not characterize the convergence rate to the
minimum; instead, we show the convergence rate to a certain low error solution, corresponding to
early learning performance. In optimization literature, it is known that cubic power would converge
slower to the minimizer as it has a relatively ﬂat bottom. However, it may be an interesting future
direction to study how to combine objectives with different powers so that optimizing the hybrid
objective leads to a faster convergence rate to the optimum and is robust to outliers."
REFERENCES,0.8596491228070176,"A.7
ADDITIONAL EXPERIMENTS"
REFERENCES,0.861244019138756,"In this section, we include the following additional experimental results:"
REFERENCES,0.8628389154704944,"1. As a supplementary to Figure 1 from Section 3.3, we show the learning performance
measured by training errors to show the negative effects of the two limitations."
REFERENCES,0.8644338118022329,Under review as a conference paper at ICLR 2022
REFERENCES,0.8660287081339713,"0.00
0.25
0.50
0.75
1.00
1e5 0.45 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.8676236044657097,(a) |T | = 4000
REFERENCES,0.8692185007974481,"0.00
0.25
0.50
0.75
1.00
1e5 0.45 0.70"
REFERENCES,0.8708133971291866,"Root
mean
squared"
REFERENCES,0.8724082934609251,"error
averaged"
REFERENCES,0.8740031897926634,"over
50runs"
REFERENCES,0.8755980861244019,"L2
PrioritizedL2
Full-PrioritizedL2"
REFERENCES,0.8771929824561403,(b) |T | = 400
REFERENCES,0.8787878787878788,"Figure 10: Figure (a)(b) show the training RMSE as a function of number of mini-batch updates
with a training set containing 4k examples and another containing 400 examples respectively. We
compare the performances of Full-PrioritizedL2 (blue), L2 (black), and PrioritizedL2 (red). The
results are averaged over 50 random seeds. The shade indicates standard error."
REFERENCES,0.8803827751196173,"0.00
0.25
0.50
0.75
1.00
1e5 0.50 0.55 0.60 0.65 0.70"
REFERENCES,0.8819776714513556,"(a) b=128, σ = 0.5"
REFERENCES,0.8835725677830941,"0.00
0.25
0.50
0.75
1.00
1e5 0.5 0.7"
REFERENCES,0.8851674641148325,"Root
mean
squared"
REFERENCES,0.886762360446571,"error
averaged"
REFERENCES,0.8883572567783095,"over
(50runs)"
REFERENCES,0.8899521531100478,"Cubic
Full-PrioritizedL2"
REFERENCES,0.8915470494417863,"(b) b=512, σ = 0.5"
REFERENCES,0.8931419457735247,"0.00
0.25
0.50
0.75
1.00
1e5 0.15 0.20 0.30 0.40 0.50"
REFERENCES,0.8947368421052632,"(c) b=128, σ = 0.5"
REFERENCES,0.8963317384370016,"0.00
0.25
0.50
0.75
1.00
1e5 0.05 0.50"
REFERENCES,0.89792663476874,"Root
mean
squared"
REFERENCES,0.8995215311004785,"error
averaged"
REFERENCES,0.9011164274322169,"over
(50runs)"
REFERENCES,0.9027113237639554,"Cubic
Full-PrioritizedL2"
REFERENCES,0.9043062200956937,"(d) b=512, σ = 0.5"
REFERENCES,0.9059011164274322,"Figure 11: Figure(a)(b) show the training RMSE as a function of number of mini-batch updates with
increasing mini-batch size b. Figure (c)(d) show the testing RMSE. We compare the performances
of Full-PrioritizedL2 (blue), Cubic (forest green). As we increase the mini-batch size, the two
performs more similar to each other. The results are averaged over 50 random seeds. The shade
indicates standard error."
REFERENCES,0.9074960127591707,"2. Empirical veriﬁcation of Theorem 1 (prioritized sampling and uniform sampling on cubic
power equivalence)."
REFERENCES,0.9090909090909091,3. Results on MazeGridWorld from Pan et al. (2020).
REFERENCES,0.9106858054226475,"A.7.1
TRAINING ERROR CORRESPONDING TO FIGURE 1 FROM SECTION 3.3"
REFERENCES,0.9122807017543859,"Note that our Theorem 1 and 2 characterize the expected gradient calculated on the training set;
hence it is sufﬁcient to examine the learning performances measured by training errors. However,
the testing error is usually the primary concern, so we put the testing error in the main body. As a
sanity check, we also investigate the learning performances measured by training error and ﬁnd that
those algorithms behave similarly as shown in Figure 10 where the algorithms are trained by using
training sets with decreasing training examples from (a) to (b). As we reduce the training set size,
Full-PrioritizedL2 is closer to L2. Furthermore, PrioritizedL2 is always worse than Full-PrioritizedL2.
These observations show the negative effects resulting from the issues of outdated priorities and
insufﬁcient sample space coverage."
REFERENCES,0.9138755980861244,"A.7.2
EMPIRICAL VERIFICATION OF THEOREM 1"
REFERENCES,0.9154704944178629,"Theorem 1 states that the expected gradient of doing prioritized sampling on mean squared error is
equal to the gradient of doing uniformly sampling on cubic power loss. As a result, we expect that the
learning performance on the training set (note that we calculate gradient by using training examples)
should be similar when we use a large mini-batch update as the estimate of the expectation terms
become close."
REFERENCES,0.9170653907496013,"We use the same dataset as described in Section 3.3 and keep using training size 4k. Figure 11(a)(b)
shows that when we increase the mini-batch size, the two algorithms Full-PrioritizedL2 and Cubic
are becoming very close to each other, verifying our theorem."
REFERENCES,0.9186602870813397,Under review as a conference paper at ICLR 2022 S G
REFERENCES,0.9202551834130781,(a) MazeGridWorld
REFERENCES,0.9218500797448166,"1
2
3
4
5
time steps
1e4 2000 1750 500 100"
REFERENCES,0.9234449760765551,Average
REFERENCES,0.9250398724082934,Return
REFERENCES,0.9266347687400319,"per
Episode
(20runs)
Dyna-Frequency
Dyna-Value
Dyna-TD"
REFERENCES,0.9282296650717703,"(b) MazeGW, n = 30"
REFERENCES,0.9298245614035088,"Figure 12: Figure(a) shows MazeGridWorld(GW) taken from Pan et al. (2020) and the learning
curves are in (b). We show evaluation learning curves of Dyna-TD (black), Dyna-Frequency (red),
and Dyna-Value (blue). The dashed line indicates Dyna-TD trained with an online learned model.
All results are averaged over 20 random seeds after smoothing over a window of size 30. The shade
indicates standard error."
REFERENCES,0.9314194577352473,"Note that our theorem characterizes the expected gradient calculated on the training set; hence it
is sufﬁcient to examine the learning performances measured by training errors. However, usually,
the testing error is the primary concern. For completeness, we also investigate the learning perfor-
mances measured by testing error and ﬁnd that the tested algorithms behave similarly as shown in
Figure 11(c)(d)."
REFERENCES,0.9330143540669856,"A.7.3
RESULTS ON MAZEGRIDWORLD DOMAIN"
REFERENCES,0.9346092503987241,"In Figure 12, we demonstrate that our algorithm can work better than Dyna-Frequency on a Maze-
GridWorld domain (Pan et al., 2020), where Dyna-Frequency was shown to be superior to Dyna-Value
and model-free baselines. This result further conﬁrms the usefulness of our sampling approach."
REFERENCES,0.9362041467304625,"A.8
REPRODUCIBLE RESEARCH"
REFERENCES,0.937799043062201,"Our implementations are based on tensorﬂow with version 1.13.0 (Abadi et al., 2015). We use Adam
optimizer (Kingma & Ba, 2014) for all experiments."
REFERENCES,0.9393939393939394,"A.8.1
REPRODUCE EXPERIMENTS BEFORE SECTION 5"
REFERENCES,0.9409888357256778,"Supervised learning experiment.
For the supervised learning experiment shown in section 3, we
use 32 × 32 tanh units neural network, with learning rate swept from {0.01, 0.001, 0.0001, 0.00001}
for all algorithms. We compute the constant c as speciﬁed in the Theorem 1 at each time step for
Cubic loss. We compute the testing error every 500 iterations/mini-batch updates and our evaluation
learning curves are plotted by averaging 50 random seeds. For each random seed, we randomly split
the dataset to testing set and training set and the testing set has 1k data points. Note that the testing
set is not noise-contaminated."
REFERENCES,0.9425837320574163,"Reinforcement Learning experiments in Section 3.
We use a particularly small neural network
16 × 16 to highlight the issue of incomplete priority updating. Intuitively, a large neural network
may be able to memorize each state’s value and thus updating one state’s value is less likely to affect
others. We choose a small neural network, in which case a complete priority updating for all states
should be very important. We set the maximum ER buffer size as 10k and mini-batch size as 32. The
learning rate is chosen from {0.0001, 0.001} and the target network is updated every 1k steps."
REFERENCES,0.9441786283891547,"Distribution distance computation in Section 4.
We now introduce the implementation details
for Figure 3. The distance is estimated by the following steps. First, in order to compute the desired
sampling distribution, we discretize the domain into 50 × 50 grids and calculate the absolute TD
error of each grid (represented by the left bottom vertex coordinates) by using the true environment
model and the current learned Q function. We then normalize these priorities to get probability
distribution p∗. Note that this distribution is considered as the desired one since we have access to
all states across the state space with priorities computed by current Q-function at each time step."
REFERENCES,0.9457735247208932,Under review as a conference paper at ICLR 2022
REFERENCES,0.9473684210526315,"Second, we estimate our sampling distribution by randomly sampling 3k states from search-control
queue and count the number of states falling into each discretized grid and normalize these counts to
get p1. Third, for comparison, we estimate the sampling distribution of the conventional prioritized
ER (Schaul et al., 2016) by sampling 3k states from the prioritized ER buffer and count the states
falling into each grid and compute its corresponding distribution p2 by normalizing the counts.
Then we compute the distances of p1, p2 to p∗by two weighting schemes: 1) on-policy weighting:
P2500
j=1 dπ(sj)|pi(sj) −p∗(sj)|, i ∈{1, 2}, where dπ is approximated by uniformly sample 3k states
from a recency buffer and normalizing their visitation counts on the discretized GridWorld; 2) uniform
weighting:
1
2500
P2500
j=1 |pi(sj)−p∗(sj)|, i ∈{1, 2}. We examine the two weighting schemes because
of two considerations: for the on-policy weighting, we concern about the asymptotic convergent
behavior and want to down-weight those states with relatively high TD error but get rarely visited as
the policy gets close to optimal; uniform weighting makes more sense during early learning stage,
where we consider all states are equally important and want the agents to sufﬁciently explore the
whole state space."
REFERENCES,0.94896331738437,"Computational cost v.s. performance in Section 4.
The setting is the same as we used for
Section 5. We use plan step/updates=10 to generate that learning curve."
REFERENCES,0.9505582137161085,"A.8.2
REPRODUCE EXPERIMENTS IN SECTION 5"
REFERENCES,0.9521531100478469,"For our algorithm, the pseudo-code with concrete parameter settings is presented in Algorithm 4."
REFERENCES,0.9537480063795853,"Common settings. For all discrete control domains other than roundabout-v0, we use 32 × 32 neural
network with ReLu hidden units except the Dyna-Frequency which uses tanh units as suggested by
the author (Pan et al., 2020). This is one of its disadvantages: the search-control of Dyna-Frequency
requires the computation of Hessian-gradient product and it is empirically observed that the Hessian
is frequently zero when using ReLu as hidden units. Except the output layer parameters which were
initialized from a uniform distribution [−0.003, 0.003], all other parameters are initialized using
Xavier initialization (Glorot & Bengio, 2010). We use mini-batch size b = 32 and maximum ER
buffer size 50k. All algorithms use target network moving frequency 1000 and we sweep learning
rate from {0.001, 0.0001}. We use warm up steps = 5000 (i.e. random action is taken in the ﬁrst 5k
time steps) to populate the ER buffer before learning starts. We keep exploration noise as 0.1 without
decaying."
REFERENCES,0.9553429027113237,"Hyper-parameter settings.
Across RL experiments including both discrete and continuous
control tasks, we are able to ﬁx the same parameters for our hill climbing updating rule 3
s ←s + αh∇s log |ˆy(s) −maxa Q(s, a; θt)| + X, where we ﬁx αh = 0.1, X ∼N(0, 0.01)."
REFERENCES,0.9569377990430622,"For our algorithm Dyna-TD, we are able to keep the same parameter setting across all discrete
domains: c = 20 and learning rate 0.001. For all Dyna variants, we fetch the same number of states
(m = 20) from hill climbing (i.e. search-control process) as Dyna-TD does, and use ϵaccept = 0.1
and set the maximum number of gradient step as k = 100 unless otherwise speciﬁed."
REFERENCES,0.9585326953748007,"Our Prioritized ER is implemented as the proportional version with sum tree data structure. To ensure
fair comparison, since all model-based methods are using mixed mini-batch of samples, we use
prioritized ER without importance ratio but half of mini-batch samples are uniformly sampled from
the ER buffer as a strategy for bias correction. For Dyna-Value and Dyna-Frequency, we use the
setting as described by the original papers."
REFERENCES,0.960127591706539,"For the purpose of learning an environment model on those discrete control domains, we use a
64 × 64 ReLu units neural network to predict s′ −s and reward given a state-action pair s, a; and we
use mini-batch size 128 and learning rate 0.0001 to minimize the mean squared error objective for
training the environment model."
REFERENCES,0.9617224880382775,"Environment-speciﬁc settings. All of the environments are from OpenAI (Brockman et al., 2016)
except that: 1) the GridWorld envirnoment is taken from Pan et al. (2019) and the MazeGridWorld is
from Pan et al. (2020); 2) Roundabout-v0 is from (Leurent et al., 2019). For all OpenAI environments,
we use the default setting except on Mountain Car where we set the episodic length limit to 2k. The
GridWorld has state space S = [0, 1]2 and each episode starts from the left bottom and the goal area
is at the top right [0.95, 1.0]2. There is a wall in the middle with a hole to allow the agent to pass.
MazeGridWorld is a more complicated version where the state and action spaces are the same as"
REFERENCES,0.9633173843700159,Under review as a conference paper at ICLR 2022
REFERENCES,0.9649122807017544,Algorithm 3 Dyna-TD
REFERENCES,0.9665071770334929,"Input: m: number of states to fetch through search-control; Bsc: empty search-control queue;
Ber: ER buffer; ϵaccept: threshold for accepting a state; initialize Q-network Qθ
for t = 1, 2, . . . do"
REFERENCES,0.9681020733652312,"Observe (st, at, st+1, rt+1) and add it to Ber
// Hill climbing on absolute TD error
Sample s from Ber, c ←0, ˜s ←s
while c < m do"
REFERENCES,0.9696969696969697,"ˆy ←Es′,r∼ˆ
P(·|s,a)[r + γ maxa Qθ(s′, a)]
Update s by rule (3)
if s is out of the state space then"
REFERENCES,0.9712918660287081,"Sample s from Ber, ˜s ←s // restart
continue
if ||˜s −s||2/
√"
REFERENCES,0.9728867623604466,"d ≥ϵaccept then
// d is the number of state variables, i.e. S ⊂Rd
Add s into Bsc, ˜s ←s, c ←c + 1
//n planning updates
for n times do"
REFERENCES,0.9744816586921851,"Sample a mixed mini-batch with half samples from Bsc and half from Ber
Update Q-network parameters by using the mixed mini-batch"
REFERENCES,0.9760765550239234,"GridWorld, but there are two walls in the middle and it takes a long time for model-free methods to
be successful. On the this domain, we use the same setting as the original paper for all Dyna variants.
We use exactly the same setting as described above except that we change the Q−network size to
64 × 64 ReLu units, and number of search-control samples is m = 50 as used by the original paper.
We refer readers to the original paper (Pan et al., 2020) for more details."
REFERENCES,0.9776714513556619,"On roundabout-v0 domain, we use 64×64 ReLu units for all algorithms and set mini-batch size as 64.
The environment model is learned by using a 200×200 ReLu neural network trained by the same way
mentioned above. For Dyna-TD, we start using the model after 5k steps and set m = 100, k = 500
and we do search-control every 50 environment time steps to reduce computational cost. To alleviate
the effect of model error, we use only 16 out of 64 samples from the search-control queue in a
mini-batch."
REFERENCES,0.9792663476874003,"On Mujoco domains Hopper and Walker2d, we use 200 × 100 ReLu units for all algorithms and set
mini-batch size as 64. The environment model is learned by using a 200 × 200 ReLu neural network
trained by the same way mentioned above. For Dyna-TD, we start using the model after 10k steps
and set m = 100, k = 500 and we do search-control every 50 environment time steps to reduce
computational cost. To alleviate the effect of model error, we use only 16 out of 64 samples from the
search-control queue in a mini-batch."
REFERENCES,0.9808612440191388,Under review as a conference paper at ICLR 2022
REFERENCES,0.9824561403508771,Algorithm 4 Dyna-TD with implementation details
REFERENCES,0.9840510366826156,"Input or notations: k = 20: number search-control states to acquire by hill climbing, kb = 100:
the budget of maximum number of hill climbing steps; ρ = 0.5: percentage of samples from
search-control queue, d : S ⊂Rd; empty search-control queue Bsc and ER buffer Ber
empirical covariance matrix: ˆΣs ←I
µss ←0 ∈Rd×d, µs ←0 ∈Rd
(auxiliary variables for computing empirical covariance matrix,
sample average will be maintained for µss, µs)
nτ ←0: count for parameter updating times, τ ←1000 target network updating frequency
ϵaccept ←0: threshold for accepting a state
Initialize Q network Qθ and target Q network Qθ′
for t = 1, 2, . . . do"
REFERENCES,0.9856459330143541,"Observe (s, a, s′, r) and add it to Ber
µss ←µss(t−1)+ss⊤"
REFERENCES,0.9872408293460925,"t
, µs ←µs(t−1)+s"
REFERENCES,0.988835725677831,"t
ˆΣs ←µss −µsµ⊤
s
ϵaccept ←(1 −β)ϵaccept + β||s′ −s||2 for β = 0.001
// Hill climbing on absolute TD error
Sample s from Ber, c ←0, ˜s ←s, i ←0
while c < k and i < kb do"
REFERENCES,0.9904306220095693,"// since environment is deterministic, the environment model becomes a Dirac-delta distribu-
tion and we denote it as a deterministic function M : S × A 7→S × R
s′, r ←M(s, a)"
REFERENCES,0.9920255183413078,"ˆy ←r + γ maxa Qθ(s′, a)
// add a smooth constant 10−5 inside the logarithm
s ←s + αh∇s log(|ˆy −maxa Q(s, a; θt)| + 10−5) + X, X ∼N(0, 0.01ˆΣs)
if s is out of the state space then"
REFERENCES,0.9936204146730463,"// restart hill climbing
Sample s from Ber, ˜s ←s
continue
if ||˜s −s||2/
√"
REFERENCES,0.9952153110047847,"d ≥ϵaccept then
Add s into Bsc, ˜s ←s, c ←c + 1
i ←i + 1
for n times do"
REFERENCES,0.9968102073365231,"Sample a mixed mini-batch b, with proportion ρ from Bsc and 1 −ρ from Ber
Update parameters θ (i.e. DQN update) with b
nτ ←nτ + 1
if mod(nτ, τ) == 0 then"
REFERENCES,0.9984051036682615,Qθ′ ←Qθ
