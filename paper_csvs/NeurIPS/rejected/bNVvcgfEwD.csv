Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005263157894736842,"As model sizes in deep learning continue to expand, memory-efficient optimizers
1"
ABSTRACT,0.0010526315789473684,"are increasingly critical to manage the substantial memory demands of popular
2"
ABSTRACT,0.0015789473684210526,"algorithms like Adam and AdamW. Among these, Adafactor has emerged as one
3"
ABSTRACT,0.002105263157894737,"of the widely adopted choices for training deep learning tasks, particularly large
4"
ABSTRACT,0.002631578947368421,"language models. However, despite its practical success, there is limited theoretical
5"
ABSTRACT,0.003157894736842105,"analysis on Adafactor’s convergence. This paper presents a comprehensive analysis
6"
ABSTRACT,0.0036842105263157894,"on Adafactor in a non-convex smooth setting, demonstrating its convergence to find
7"
ABSTRACT,0.004210526315789474,"a stationary point at a rate of ˜O(1/
√"
ABSTRACT,0.004736842105263158,"T). We find that the default hyper-parameter
8"
ABSTRACT,0.005263157894736842,"setting results in a sub-optimal rate in our framework, and propose an alternative
9"
ABSTRACT,0.005789473684210527,"setting that could theoretically achieve optimal convergence rate. This finding
10"
ABSTRACT,0.00631578947368421,"is further supported by some experimental results. We also prove that Adafactor
11"
ABSTRACT,0.006842105263157895,"with a suitable time-varying clipping threshold could also converge, achieving
12"
ABSTRACT,0.007368421052631579,"performance in experiments comparable to that of the standard constant setting.
13"
INTRODUCTION,0.007894736842105263,"1
Introduction
14"
INTRODUCTION,0.008421052631578947,"The adaptive gradient-based methods, such as the well-known AdaGrad [9, 29], RMSProp [30],
15"
INTRODUCTION,0.008947368421052631,"Adadelta [35], Adam [15] and AdamW [22], have become the preferred approaches in solving the
16"
INTRODUCTION,0.009473684210526316,"following unconstrained stochastic optimization problem in deep learning fields:
17"
INTRODUCTION,0.01,"min
X∈Rn×m f(X) = EZ∈P[l(X; Z)],
(1)"
INTRODUCTION,0.010526315789473684,"where the object function f is non-convex and P denotes a probability distribution. During the
18"
INTRODUCTION,0.011052631578947368,"training process, these adaptive methods require to store the historical gradients’ information so as
19"
INTRODUCTION,0.011578947368421053,"to adaptively tune their step-sizes. For example, both Adam and AdamW maintain the exponential
20"
INTRODUCTION,0.012105263157894737,"average of gradients and squared gradients, and AdaGrad stores the cumulative of squared gradients.
21"
INTRODUCTION,0.01263157894736842,"Despite their effectiveness, these algorithms pose substantial memory challenges for GPUs to save
22"
INTRODUCTION,0.013157894736842105,"these additional gradients’ information, especially when training large language models (LLMs),
23"
INTRODUCTION,0.01368421052631579,"such as GPT-3 [5], which contains over 175 billion parameters.
24"
INTRODUCTION,0.014210526315789474,"To address memory constraints, several memory-efficient optimization algorithms have been devel-
25"
INTRODUCTION,0.014736842105263158,"oped, e.g., [26, 1, 23, 17]. One of the most popular optimizers is Adafactor [26] which employs
26"
INTRODUCTION,0.015263157894736841,"a rank-1 matrix factorization to approximate the second moment matrix in Adam. For an n × m
27"
INTRODUCTION,0.015789473684210527,"weight matrices, this technique reduces memory usage from O(mn) to O(m + n) by only tracking
28"
INTRODUCTION,0.01631578947368421,"the moving averages of the row and column sums of the squared gradients matrix. Additionally,
29"
INTRODUCTION,0.016842105263157894,"Adafactor eliminates the first-order momentum used in Adam and incorporates update clipping to
30"
INTRODUCTION,0.017368421052631578,"enhance training stability.
31"
INTRODUCTION,0.017894736842105262,"The empirical results reveal that Adafactor achieves comparable performance to Adam in training
32"
INTRODUCTION,0.018421052631578946,"Transformer models [26] . In real applications, several LLMs including PaLM [6] and T5 [24] have
33"
INTRODUCTION,0.018947368421052633,"applied Adafactor as their main optimizers [38]. In spite of Adafactor’s widely usage, there is still
34"
INTRODUCTION,0.019473684210526317,"limited understanding on its convergence in theory, especially the effect of the matrix approximation
35"
INTRODUCTION,0.02,"and update clipping, and the explanation for its hyper-parameter setting in experiments.
36"
INTRODUCTION,0.020526315789473684,"In this paper, we take a closer look on Adafactor’s convergence under non-convex smooth optimization
37"
INTRODUCTION,0.021052631578947368,"problems, considering the typical bounded gradient setting as those for AdaGrad [19, 32] and Adam
38"
INTRODUCTION,0.02157894736842105,"[34]. We aim to provide a convergence rate for Adafactor and explain the influence of the hyper-
39"
INTRODUCTION,0.022105263157894735,"parameters for the convergence speed. We also prove in theory why the default parameter setting is
40"
INTRODUCTION,0.022631578947368423,"effective in practical scenarios. The analysis to Adafactor is non-trivial compared to other adaptive
41"
INTRODUCTION,0.023157894736842106,"methods such as AdaGrad and Adam due to the unique matrix factorization and update clipping
42"
INTRODUCTION,0.02368421052631579,"mechanisms. Based on a new proxy step-size construction and some new compositions as well as
43"
INTRODUCTION,0.024210526315789474,"estimations, we analyze the additional error terms in the Descent Lemma introduced by the matrix
44"
INTRODUCTION,0.024736842105263158,"approximation and update clipping. Our main contributions are summarized as follows.
45"
INTRODUCTION,0.02526315789473684,"Contributions
46"
INTRODUCTION,0.025789473684210525,"• We provide a convergence analysis for the full-batch Adafactor considering bounded gradients
47"
INTRODUCTION,0.02631578947368421,"and a broader range of parameter setting which covers the default one in [26]. The result shows
48"
INTRODUCTION,0.026842105263157896,"that Adafactor could converge to find a stationary point with a rate of ˜O(1/
√"
INTRODUCTION,0.02736842105263158,"T) where T
49"
INTRODUCTION,0.027894736842105264,"denotes the total iteration number.
50"
INTRODUCTION,0.028421052631578948,"• We further investigate the more realistic stochastic Adafactor. It’s found that a simple variant of
51"
INTRODUCTION,0.02894736842105263,"Adafactor, which drops the update clipping, could attain the best convergence rate of ˜O(1/
√ T)
52"
INTRODUCTION,0.029473684210526315,"when the second moment decay rate is 1 −1/k. We also verify that the default decay rate
53"
INTRODUCTION,0.03,"1 −1/k0.8 could lead to a sub-optimal convergence rate in our framework. To illustrate this
54"
INTRODUCTION,0.030526315789473683,"finding, we provide some empirical results, showing that the potential best hyper-parameter
55"
INTRODUCTION,0.03105263157894737,"setting in theory could perform better than the default one used in experiments.
56"
INTRODUCTION,0.031578947368421054,"• We extend our study to include a time-varying clipping threshold. Our analysis implies that
57"
INTRODUCTION,0.032105263157894734,"with proper selections of clipping threshold and hyper-parameters, Adafactor could also achieve
58"
INTRODUCTION,0.03263157894736842,"the best convergence rate of ˜O(1/
√"
INTRODUCTION,0.03315789473684211,"T). We also do some experiments to show that the new
59"
INTRODUCTION,0.03368421052631579,"clipping threshold scheme achieves comparable performance and training stability to the original
60"
INTRODUCTION,0.034210526315789476,"constant threshold setting.
61"
INTRODUCTION,0.034736842105263156,"The rest of the paper is organized as follows. The next section provides some most relevant works.
62"
INTRODUCTION,0.035263157894736843,"Section 3 presents some necessary notations definitions and problem setup. Section 4 reviews
63"
INTRODUCTION,0.035789473684210524,"Adafactor and introduces its essential mechanism. In Section 5 and Section 6, we separately provide
64"
INTRODUCTION,0.03631578947368421,"convergence bounds for full-batch Adafactor and stochastic Adafactor without update clipping. We
65"
INTRODUCTION,0.03684210526315789,"further discuss the hyper-parameters’ dependency. In Section 7, we investigate Adafactor using a
66"
INTRODUCTION,0.03736842105263158,"time-increasing update clipping threshold. Section 8 provides experimental results to support our
67"
INTRODUCTION,0.037894736842105266,"theory. All the detailed proof could be found in the appendix.
68"
RELATED WORK,0.038421052631578946,"2
Related work
69"
RELATED WORK,0.03894736842105263,"In this paper, we mainly investigate the theoretical convergence of Adafactor. Although there is
70"
RELATED WORK,0.039473684210526314,"limited works on Adafactor in theory, it’s necessary to briefly discuss related works on the convergence
71"
RELATED WORK,0.04,"of other adaptive methods, particularly on non-convex smooth optimization. Here, we briefly list
72"
RELATED WORK,0.04052631578947368,"some of the most related works.
73"
RELATED WORK,0.04105263157894737,"Convergence of adaptive methods
Several studies address the convergence of AdaGrad in non-
74"
RELATED WORK,0.041578947368421056,"convex settings. For example, [19] considered a simple variant with delayed step-size, while [32]
75"
RELATED WORK,0.042105263157894736,"and [39] assumed bounded stochastic gradients. Other works [14, 10, 21, 3, 31, 27, 33] derived
76"
RELATED WORK,0.04263157894736842,"convergence bounds under more relaxed assumptions. Another line of research has investigated the
77"
RELATED WORK,0.0431578947368421,"convergence of Adam. For instance, [34, 7, 39, 11, 8] assumed bounded gradients. [28, 36, 31]
78"
RELATED WORK,0.04368421052631579,"considered more relaxed noise assumptions without relying on bounded gradients. Additionally, [18]
79"
RELATED WORK,0.04421052631578947,"derived convergence bounds for Adam under generalized smooth conditions.
80"
RELATED WORK,0.04473684210526316,"Overall, the convergence analysis of optimizers typically starts with standard assumptions, such as
81"
RELATED WORK,0.045263157894736845,"bounded gradients and smooth objective functions. In subsequent studies, these assumptions are
82"
RELATED WORK,0.045789473684210526,"gradually relaxed to investigate the convergence properties of the optimizers under less stringent
83"
RELATED WORK,0.04631578947368421,"conditions.
84"
RELATED WORK,0.04684210526315789,"Memory efficient algorithms
As large models are increasingly used in deep learning, memory
85"
RELATED WORK,0.04736842105263158,"constraints have become a central issue during training. Consequently, several memory-efficient
86"
RELATED WORK,0.04789473684210526,"optimizers have been developed to address this challenge.
87"
RELATED WORK,0.04842105263157895,"One approach to save memory involves applying matrix factorization to oeptimization algorithms.
88"
RELATED WORK,0.04894736842105263,"For instance, [25] used matrix factorization in the second moment estimator of gradients in Adam,
89"
RELATED WORK,0.049473684210526316,"similar to the concept behind Adafactor. [23] introduced CAME, a variant of Adafactor, which
90"
RELATED WORK,0.05,"incorporates a confidence-guided strategy to mitigate instability caused by erroneous updates. [37]
91"
RELATED WORK,0.05052631578947368,"proposed Adapprox, leveraging randomized low-rank matrix approximation for Adam’s second
92"
RELATED WORK,0.05105263157894737,"moment estimator, demonstrating superior performance and reduced memory usage compared to
93"
RELATED WORK,0.05157894736842105,"AdamW.
94"
RELATED WORK,0.05210526315789474,"There are some other techniques to save the memory. For example, [12] relied on a “Shampoo""
95"
RELATED WORK,0.05263157894736842,"technique to reduce the storage requirement of full-matrix preconditioning methods. Notably, their
96"
RELATED WORK,0.053157894736842105,"method could be further extended to the more realistic tensor case. [1] presented a memory-saved
97"
RELATED WORK,0.05368421052631579,"version of AdaGrad, called SM3, by maintaining k sets gradient accumulator. They proved the
98"
RELATED WORK,0.05421052631578947,"convergence guarantee of SM3 on online convex optimization and the effectiveness in experiments.
99"
RELATED WORK,0.05473684210526316,"Recently, [17] built a 4-bit Adam using quantization techniques to compress the first and second
100"
RELATED WORK,0.05526315789473684,"moment estimators in Adam, also reducing memory usage.
101"
RELATED WORK,0.05578947368421053,"In summary, many existing optimizers, particularly adaptive methods like AdaGrad and Adam, face
102"
RELATED WORK,0.05631578947368421,"memory overhead. In response, the discussed works have designed memory-efficient optimizers that
103"
RELATED WORK,0.056842105263157895,"aim to achieve comparable performance to these existing methods while achieving memory benefits.
104"
PROBLEM SETUP,0.057368421052631575,"3
Problem setup
105"
PROBLEM SETUP,0.05789473684210526,"To start with, we introduce some necessary notations.
106"
PROBLEM SETUP,0.05842105263157895,"Notations
For any two matrices X = (xij)ij, Y = (yij)ij ∈Rn×m, we define ⟨X, Y ⟩=
107
Pn
i=1
Pm
j=1 xijyij. X ⊙Y , X/Y and
√"
PROBLEM SETUP,0.05894736842105263,"X denote the coordinate-wise product, quotient and
108"
PROBLEM SETUP,0.05947368421052632,"squared root respectively. 0n and 1n denote the zero and one n-dimensional vector respectively,
109"
PROBLEM SETUP,0.06,"and 1n×m denotes the one n × m-dimensional matrix. The index set [n] denotes {1, 2, · · · , n}.
110"
PROBLEM SETUP,0.060526315789473685,"∥· ∥F denotes the Frobenius norm. For a positive sequence {αi}i≥1, we define Pb
i=a αi = 0 and
111
Qb
i=a αi = 1 if a > b. The operator RMS(·) denotes
112"
PROBLEM SETUP,0.061052631578947365,RMS(X) =
PROBLEM SETUP,0.06157894736842105,"v
u
u
t 1 mn n
X i=1 m
X"
PROBLEM SETUP,0.06210526315789474,"j=1
x2
ij."
PROBLEM SETUP,0.06263157894736843,"We consider unconstrained stochastic optimization (1) over Rn×m with the Frobenius norm. The
113"
PROBLEM SETUP,0.06315789473684211,"objective function f : Rn×m →R is differentiable. Given an n × m matrix X, we assume a gradient
114"
PROBLEM SETUP,0.06368421052631579,"oracle that returns a random matrix g(X, Z) ∈Rn×m dependent by the random sample Z. The
115"
PROBLEM SETUP,0.06421052631578947,"deterministic gradient of f at X is denoted by ∇f(X) ∈Rn×m.
116"
PROBLEM SETUP,0.06473684210526316,"Assumptions
We make the following standard assumptions throughout the paper.
117"
PROBLEM SETUP,0.06526315789473684,"• (A1) L-smoothness: For any X, Y ∈Rn×m, ∥∇f(Y ) −∇f(X)∥F ≤L∥Y −X∥F ;
118"
PROBLEM SETUP,0.06578947368421052,"• (A2) Bounded below: There exists f ∗> −∞such that f(X) ≥f ∗, ∀X ∈Rn×m;
119"
PROBLEM SETUP,0.06631578947368422,"• (A3) Unbiased estimator: The gradient oracle provides an unbiased estimator of ∇f(X), i.e.,
120"
PROBLEM SETUP,0.0668421052631579,"EZ [g(X, Z)] = ∇f(X), ∀X ∈Rn×m;
121"
PROBLEM SETUP,0.06736842105263158,"• (A4) Almost surely bounded stochastic gradient: for any X ∈Rn×m, ∥g(X, Z)∥F ≤G, a.s..
122"
PROBLEM SETUP,0.06789473684210526,"Combining (A3) and (A4), it’s easy to verify that ∥∇f(X)∥≤G, ∀X ∈Rn×m. Assumptions
123"
PROBLEM SETUP,0.06842105263157895,"(A1)-(A3) are standard in the non-convex smooth convergence analysis. Although Assumption (A4)
124"
PROBLEM SETUP,0.06894736842105263,"is a bit strong since it requires an almost surely bounded stochastic gradients instead of an expected
125"
PROBLEM SETUP,0.06947368421052631,"one, it’s still commonly used to derive the high probability convergence bound, see e.g., [32, 14],
126"
PROBLEM SETUP,0.07,"which is a stronger result than an expected convergence. In coordinate-wise algorithm, another
127"
PROBLEM SETUP,0.07052631578947369,"standard assumption is l∞-bounded gradient where ∥g(X, Z)∥∞≤G∞, see e.g., [8]. These two
128"
PROBLEM SETUP,0.07105263157894737,"types of assumption are equivalent up to dimension factors.
129"
REVIEW OF ADAFACTOR,0.07157894736842105,"4
Review of Adafactor
130"
REVIEW OF ADAFACTOR,0.07210526315789474,"In this section, we briefly discuss Adafactor based on the reference [26]. The pseudocode for
131"
REVIEW OF ADAFACTOR,0.07263157894736842,"Adafactor is presented in Algorithm 1.
132"
REVIEW OF ADAFACTOR,0.0731578947368421,Algorithm 1 Adafactor
REVIEW OF ADAFACTOR,0.07368421052631578,"Input: Initialization point X1 ∈Rn×m, R0 = 0m, C0 = 0⊤
n , relative step-sizes {ρk}k≥1, decay
rate {β2,k}k≥1 ∈[0, 1), regularization constants ϵ1, ϵ2 > 0, clipping threshold d.
for k = 1, · · · , T do"
REVIEW OF ADAFACTOR,0.07421052631578948,"Gk = g(Xk, Zk);
Rk = β2,kRk−1 + (1 −β2,k)(Gk ⊙Gk + ϵ11n1⊤
m)1m;
Ck = β2,kCk−1 + (1 −β2,k)1⊤
n (Gk ⊙Gk + ϵ11n1⊤
m);
Wk = (RkCk)/1⊤
n Rk;
Uk = Gk/√Wk;
ηk = max{ϵ2, RMS(Xk)}ρk/ max{1, RMS(Uk)/d};
Xk+1 = Xk −ηk · Gk/√Wk;
end for"
REVIEW OF ADAFACTOR,0.07473684210526316,"Matrix factorization
Adafactor could be severed as a saved-memory version of Adam. Throughout
133"
REVIEW OF ADAFACTOR,0.07526315789473684,"the training process, Adam maintain two n × m matrices Mk and Vk using exponential moving
134"
REVIEW OF ADAFACTOR,0.07578947368421053,"average update,
135"
REVIEW OF ADAFACTOR,0.07631578947368421,"Mk = β1,kMk−1 + (1 −β1,k)Gk,
Vk = β2,kVk−1 + (1 −β2,k)Gk ⊙Gk,
(2)
where β1,k, β2,k ∈(0, 1), thereby tripling the memory usage. The innovation in Adafactor lies
136"
REVIEW OF ADAFACTOR,0.07684210526315789,"in its method of approximating Vk by factoring it into two rank-1 matrices, specifically the row
137"
REVIEW OF ADAFACTOR,0.07736842105263157,"sums and column sums of Vk. This approximation is guided by maintaining a minimal general
138"
REVIEW OF ADAFACTOR,0.07789473684210527,"Kullback-Leibler (KL) divergence as follows,
139"
REVIEW OF ADAFACTOR,0.07842105263157895,"min
X∈Rn,Y ∈R1×m n
X i=1 m
X"
REVIEW OF ADAFACTOR,0.07894736842105263,"j=1
d

(Vk)ij, (XY )ij

,
s.t.
(X)i ≥0, (Y )j ≥0, ∀i ∈[n], j ∈[m],"
REVIEW OF ADAFACTOR,0.07947368421052632,"where d(p, q) = p log(p/q) −p + q. The choice of KL-divergence over the more typical Frobenius
140"
REVIEW OF ADAFACTOR,0.08,"norm allows for an analytical solution to be derived, specifically given by
141"
REVIEW OF ADAFACTOR,0.08052631578947368,"X = Vk1m,
Y = 1⊤
n Vk/
 
1⊤
n Vk1m

."
REVIEW OF ADAFACTOR,0.08105263157894736,"Therefore, Adafactor only requires to maintain two vectors Rk = Vk1m, Ck = 1⊤
n Vk, sufficiently
142"
REVIEW OF ADAFACTOR,0.08157894736842106,"reducing the memory from 2mn to m + n. Although this factorization sacrifices some information
143"
REVIEW OF ADAFACTOR,0.08210526315789474,"of the squared gradients, Adafactor still delivers performance comparable to Adam in many real
144"
REVIEW OF ADAFACTOR,0.08263157894736842,"application tasks, making it a practical choice where memory is a constraint.
145"
REVIEW OF ADAFACTOR,0.08315789473684211,"Increasing decay rate
In Adam, corrective terms are introduced into Mk and Vk, resulting in two
146"
REVIEW OF ADAFACTOR,0.08368421052631579,"increasing-to-one decay rates. Theoretically, it has been demonstrated that a value closed to one for
147"
REVIEW OF ADAFACTOR,0.08421052631578947,"β2,k would ensure the convergence, e.g., [8, 39, 36]. Inspired by this observation, Adafactor used an
148"
REVIEW OF ADAFACTOR,0.08473684210526315,"increasing second moment decay rate β2,k = 1 −1/kc, c ≥0, and the empirical default setting is
149"
REVIEW OF ADAFACTOR,0.08526315789473685,"c = 0.8. As pointed out by [26], this setting allows for enjoying the stability of a low β2,k at the early
150"
REVIEW OF ADAFACTOR,0.08578947368421053,"stage of training and the insurance of convergence from a high β2,k as the run progresses. Moreover,
151"
REVIEW OF ADAFACTOR,0.0863157894736842,"it also leverages the bias correction.
152"
REVIEW OF ADAFACTOR,0.0868421052631579,"Update clipping
Adafactor modifies the update process by discarding the first-order moment Mk
153"
REVIEW OF ADAFACTOR,0.08736842105263158,"and instead applies an update clipping technique inside the step-size ηk. This involves dividing
154"
REVIEW OF ADAFACTOR,0.08789473684210526,"the root-mean-square of the update Uk, denoted as RMS(Uk), when it exceeds a threshold d.
155"
REVIEW OF ADAFACTOR,0.08842105263157894,"This mechanism helps to calibrate the second moment estimator Wk when it’s larger-than-desired
156"
REVIEW OF ADAFACTOR,0.08894736842105264,"Gk ⊙Gk. Empirical findings in [26] indicated that implementing update clipping leads to significant
157"
REVIEW OF ADAFACTOR,0.08947368421052632,"performance improvements when the warm-up technique is not used.
158"
REVIEW OF ADAFACTOR,0.09,"Relative step-sizes
Adafactor incorporates a step-size proportional to scale of Xk, denoted by
159"
REVIEW OF ADAFACTOR,0.09052631578947369,"RMS(Xk), which is shown in experiments more resilient to the more naive parameter initialization
160"
REVIEW OF ADAFACTOR,0.09105263157894737,"and scaling schemes [26].
161"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.09157894736842105,"5
Convergence result for full-batch Adafactor
162"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.09210526315789473,"We first provide the convergence bound for full-batch Adafactor. At each iteration, full-batch
163"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.09263157894736843,"Adafactor obtains the deterministic gradient ∇f(Xk) and then updates Rk, Ck using ∇f(Xk)
164"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.0931578947368421,"instead of Gk in Algorithm 1.
165"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.09368421052631579,"Theorem 5.1. Let {Xk}k≥1 be generated by Algorithm 1 with g(Xk, Zk) = ∇f(Xk), ∀k ≥1. If
166"
CONVERGENCE RESULT FOR FULL-BATCH ADAFACTOR,0.09421052631578947,"Assumptions (A1) and (A2) hold, ∥∇f(Xk)∥F ≤G, ∀k ≥1, β2,1 = 1"
AND,0.09473684210526316,"2 and
167"
AND,0.09526315789473684,"ρk = ρ0/
√"
AND,0.09578947368421052,"k,
0 < β2,k < 1,
∀k ≥1,
(3)"
AND,0.09631578947368422,"for some positive constant ρ0, then for any T ≥1,
168"
AND,0.0968421052631579,"min
k∈[T ] ∥∇f(Xk)∥2
F ≤O
log T
√ T"
AND,0.09736842105263158,"
.
(4)"
AND,0.09789473684210526,"The result indicates that full-batch Adafactor could find a stationary point at a rate of O(log T/
√"
AND,0.09842105263157895,"T)
169"
AND,0.09894736842105263,"under the non-convex smooth case, which is similar to gradient descent but with a sub-optimal rate
170"
AND,0.09947368421052631,"compared to O(1/T) [4]. The hyper-parameter setting in (3) only requires β2,k ∈(0, 1), denoting
171"
AND,0.1,"a much wider range including the default one which requires β2,k to increase to one. The detailed
172"
AND,0.10052631578947369,"version for the above result can be found in Theorem A.1 from the appendix.
173"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10105263157894737,"6
Stochastic Adafactor without update clipping
174"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10157894736842105,"In the stochastic case, we start from the simple scenario of
175"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10210526315789474,"ηk = max{ϵ2, RMS(Xk)}ρk
(5)"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10263157894736842,"without considering the update clipping 1/ max{1, RMS(Uk)/d)} in Algorithm 1, where the main
176"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.1031578947368421,"reasons are as follows.
177"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.1036842105263158,"• As pointed out in the experiments from [26], Adafactor’s performance shows little difference
178"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10421052631578948,"with and without update clipping when implementing learning rate warm-up. Since the warm-up
179"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10473684210526316,"technique is a popular method in deep learning [38], it’s reasonable to drop the update clipping.
180"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10526315789473684,"• In stochastic Adafactor, the correlation between Gk and ηk would be more complex if the update
181"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10578947368421053,"clipping is involved. The proof would be simpler when dropping the update clipping, which
182"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10631578947368421,"could help to better understand the analysis for Adafactor.
183"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10684210526315789,"We now present the probabilistic convergence bound for Adafactor without update clipping as follows,
184"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10736842105263159,"where we summarize different convergence rate with respect to the factor c from β2,k = 1−1/kc, c ∈
185"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10789473684210527,"[1/2, 1].
186"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10842105263157895,"Theorem 6.1. Let {Xk}k≥1 be generated by Algorithm 1 without update clipping where ηk is given
187"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10894736842105263,"by (5) for each k ≥1. If Assumptions (A1)-(A4) hold, and
188"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.10947368421052632,"β2,1 = 1/2,
ρ1 = ρ0,"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11,"β2,k = 1 −1/kc,
ρk = ρ0/
√"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11052631578947368,"k,
∀k ≥2,
(6)"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11105263157894738,"for some constants 1/2 ≤c ≤1, ρ0 > 0, then for any T ≥1, δ ∈(0, 1), with probability at least
189"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11157894736842106,"1 −δ,
190 191"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11210526315789474,"min
k∈[T ] ∥∇f(Xk)∥2
F ≤O

1
T c−1/2 log
T δ 
."
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11263157894736842,"The above result indicates that with appropriate hyper-parameters, Adafactor without update clipping
192"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11315789473684211,"could approximately find a stationary point. When the decay rate β2,k is 1 −1/k, the convergence
193"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11368421052631579,"rate could attain to O(log T/
√"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11421052631578947,"T), matching the rate of stochastic gradient descent [4] and the lower
194"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11473684210526315,"rate in [2] up to only a logarithm factor. The hyper-parameter setting in (6) covers the experimental
195"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11526315789473685,"default setting where c = 0.8. The result shows a sub-optimal rate of O(log T/T 0.3) under the
196"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.11578947368421053,"default setting. This finding is further complemented by the coming numerical experiments in Section
197"
STOCHASTIC ADAFACTOR WITHOUT UPDATE CLIPPING,0.1163157894736842,"8. The detailed version of the above results can be found in Theorem B.1 from the appendix.
198"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.1168421052631579,"6.1
Discussion of the hyper-parameter dependency
199"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.11736842105263158,"In this section, we discuss the dependency of several important hyper-parameters in Theorem 6.1
200"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.11789473684210526,"and the detailed version in Theorem B.1 in the appendix. It’s worthy to mention that the dominated
201"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.11842105263157894,"order in our convergence bound is determined by the total iteration number T, whereas other hyper-
202"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.11894736842105263,"parameters could be regarded as constants. However, we hope to improve the dependency of these
203"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.11947368421052632,"hyper-parameters as much as possible to make the convergence bound tight.
204"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12,"Discussion of c and the optimal rate
The convergence bound in Theorem 6.1 reveals that when
205"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12052631578947369,"c = 1, β2,k = 1 −1/k and ρk = ρ0/
√"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12105263157894737,"k, the convergence rate attains the optimal rate matching
206"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12157894736842105,"the lower bound. In addition, when c is closed to 1/2, the convergence rate deteriorates. This
207"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12210526315789473,"phenomenon somehow explains that a small decay rate β2,k (c is low) may harm the convergence
208"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12263157894736842,"speed, as β2,k should be closed enough to 1 to ensure the convergence, which has been pointed out
209"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.1231578947368421,"similarly for Adam in [8, 39, 36].
210"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12368421052631579,"The theoretical best parameter setting remains a small gap to the default one of c = 0.8. To verify our
211"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12421052631578948,"theoretical finding, we provide some empirical evidence in Section 8, showing that β2,k = 1 −1/k
212"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12473684210526316,"performs even better than the default one and the performance would be better when c increases from
213"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12526315789473685,"1/2 to 1.
214"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12578947368421053,"Dependency to mn
It’s clear to see that the convergence bounds in Theorem A.1 and Theorem
215"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12631578947368421,"B.1 are free of the curse of the dimension factor mn as mn only appears on the denominator in each
216"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.1268421052631579,"coefficient. We think that solving the curse of dimension is vital since the applied range for Adafactor
217"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12736842105263158,"includes many deep learning tasks where mn are comparable large to T.
218"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12789473684210526,"Dependency to ϵ1, ϵ2
The convergence bounds in (37) and (39) from Theorem B.1 has a dependency
219"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12842105263157894,"of O(ϵ−1
1
log(1/ϵ1)) on ϵ1.1 Although the polynomial dependency to ϵ1 is a bit worse since ϵ1
220"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12894736842105264,"ususally takes a small value in experiments, e.g., the default setting is 10−30, it’s still common in
221"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.12947368421052632,"some theoretical convergence results, e.g., [34, 18]. We also perform some experiments to show
222"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13,"that a relatively large ϵ1, roughly 10−3, makes no observable effect on the performance. Thereby,
223"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13052631578947368,"ϵ1 could be regarded as a constant in comparison to T and the influence brought by 1/ϵ1 could be
224"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13105263157894737,"somehow acceptable.
225"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13157894736842105,"Since the default value of ϵ2 is 10−3 in experiments, it could also be regarded as a constant compared
226"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13210526315789473,"to T. Therefore, the dependency O(1/ϵ2) on ϵ2 shows little effect on convergence bounds given the
227"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13263157894736843,"sufficiently large T.
228"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13315789473684211,"Dependency to the scale of parameters.
The convergence bounds in Theorem B.1 contain a
229"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.1336842105263158,"O(Θmax) factor where Θmax denotes the maximum values of ∥Xk∥∞along the training process.
230"
DISCUSSION OF THE HYPER-PARAMETER DEPENDENCY,0.13421052631578947,"It’s reasonable to assume that Θmax ≤G0 for a comparable large constant G0 in practice.
231"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13473684210526315,"7
Convergence of Adafactor with update clipping
232"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13526315789473684,"In this section, we take a closer look on the comprehensive Adafactor with both matrix factorization
233"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13578947368421052,"and update clipping. We slightly change the update clipping threshold d in Algorithm 1 to a time-
234"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13631578947368422,"varying threshold dk. The step-size ηk then becomes
235"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1368421052631579,"ηk = max{ϵ2, RMS(Xk)}ρk"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13736842105263158,"max{1, RMS(Uk)/dk}.
(7)"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13789473684210526,"Then, we present the following convergence bound.
236"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13842105263157894,"Theorem 7.1. Let {Xk}k≥1 be generated by Algorithm 1 with ηk given by (7) for each k ≥1. If
237"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.13894736842105262,"Assumptions (A1)-(A4) hold, and
238"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1394736842105263,"d1 = 1,
β2,1 = 1/2,
ρ1 = ρ0,"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14,dk = k
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1405263157894737,"c
2(α−1) ,
β2,k = 1 −1/kc,
ρk = ρ0/
√"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14105263157894737,"k,
∀k ≥2,
(8)"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14157894736842105,1The detailed calculation could be found in (45) and (46) in the appendix.
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14210526315789473,"for some constants α > 1, 1/2 ≤c ≤1, ρ0 > 0, then for any T ≥1, δ ∈(0, 1), with probability at
239"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14263157894736841,"least 1 −δ,
240 241"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1431578947368421,"min
k∈[T ] ∥∇f(Xk)∥2
F ≤O

1
T c−1/2 log
T δ 
."
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1436842105263158,"Discussion of Theorem 7.1
The convergence result indicates that with a proper selection of the
242"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14421052631578948,"clipping threshold, along with the commonly used step-size ρk and decay rate β2,k, Adafactor can
243"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14473684210526316,"find a stationary point when T is sufficiently large. The dependency of convergence bound on c
244"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14526315789473684,"remains consistent with Theorem 6.1, achieving the optimal order when c = 1. In addition, the
245"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14578947368421052,"convergence bound can still avoid the curse of dimension, which is shown in the detailed version
246"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1463157894736842,"Theorem C.1 from the appendix.
247"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14684210526315788,"The additional hyper-parameter α primarily influences the dependency on ϵ1, specifically as
248"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14736842105263157,"O
 
ϵ−α
1
log(1/ϵ1)

. Thus, our convergence bound may deteriorate as α increases, possibly due
249"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14789473684210527,"to the limitation of our proof framework. This dependency could be potentially improved to
250"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14842105263157895,"O
 
ϵ−1
1
log(1/ϵ1)

when mn is comparable to 1/ϵ1, which is practical when implementing a large-
251"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14894736842105263,"size model.2 In our experiments, we tested different values of α and found that suitably small values,
252"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.14947368421052631,"such as α = 4, 6, 7, 8 can lead to performance and training stability comparable to the default setting,
253"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15,"even without implementing the warm-up technique. This finding suggests that our new threshold
254"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15052631578947367,"setting plays a similar role in enhancing training stability as the default one, which is also the main
255"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15105263157894736,"motivation of update clipping. Since ϵ1 can be set to a relatively large value, e.g., 10−3, a dependency
256"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15157894736842106,"like O(ϵ−4
1
log(1/ϵ1)) is somewhat acceptable for sufficiently large T.
257"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15210526315789474,"The time-increasing dk provides the following intuition: As shown in [26, Figure 1], during the
258"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15263157894736842,"early stages of training, a high decay rate β2,k can cause larger-than-desired updates and training
259"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.1531578947368421,"instability. Therefore, we set a low threshold dk to ensure that the update clipping mechanism
260"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15368421052631578,"effectively calibrates these larger-than-desired updates. As training progresses, the sequences and
261"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15421052631578946,"updates become more stable, and the second moment estimator Wk becomes more accurate in
262"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15473684210526314,"estimating the squared gradients, which is also shown in [26, Figure 1]. Consequently, there is
263"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15526315789473685,"less need for update clipping, corresponding to a relatively large dk. We have also verified through
264"
CONVERGENCE OF ADAFACTOR WITH UPDATE CLIPPING,0.15578947368421053,"experiments that our setting can achieve performance comparable to the default setting of d = 1.
265"
EXPERIMENTS,0.1563157894736842,"8
Experiments
266"
EXPERIMENTS,0.1568421052631579,"In this section, we will report our experimental results based on the insights obtained in our theory.
267"
EXPERIMENTS,0.15736842105263157,"We will mainly provide the following three experiments:
268"
EXPERIMENTS,0.15789473684210525,"• We test Adafactor without update clipping under different decay rate parameters c, aiming to
269"
EXPERIMENTS,0.15842105263157893,"demonstrate performance improvement as c increases from 0.5 to 1 with optimal performance at
270"
EXPERIMENTS,0.15894736842105264,"c = 1, as indicated in Theorem 6.1 and Theorem 7.1.
271"
EXPERIMENTS,0.15947368421052632,"• We evaluate the sensitivity of Adafactor to different values of ϵ1, particularly showing that a
272"
EXPERIMENTS,0.16,"relatively large ϵ1 does not significantly impact performance.
273"
EXPERIMENTS,0.16052631578947368,"• We assess the performance of Adafactor with a time-increasing dk setting, as described in
274"
EXPERIMENTS,0.16105263157894736,"Theorem 7.1, and compare it to the default constant setting.
275"
EXPERIMENT SETUP,0.16157894736842104,"8.1
Experiment setup
276"
EXPERIMENT SETUP,0.16210526315789472,"In all experiments, the initialization is R0 = 0m and C0 = 0⊤
n . We use a learning rate with the
277"
EXPERIMENT SETUP,0.16263157894736843,"warm-up technique as described in [26], specifically ρk = min{10−6 · k, 1/
√"
EXPERIMENT SETUP,0.1631578947368421,"k} for all experiments
278"
EXPERIMENT SETUP,0.1636842105263158,"unless otherwise specified. The batch size is set to 256, and the total number of epochs is 400 by
279"
EXPERIMENT SETUP,0.16421052631578947,"default. Our models are ResNet-20 and ResNet-110 [13], and we use the CIFAR-10 and CIFAR-100
280"
EXPERIMENT SETUP,0.16473684210526315,"datasets [16] without any data augmentation. The experiments are conducted using the PyTorch
281"
EXPERIMENT SETUP,0.16526315789473683,"implementation of Adafactor on a single NVIDIA GeForce RTX 4090 GPU.
282"
EXPERIMENT SETUP,0.16578947368421051,"0.5
0.6
0.7
0.8
0.9
1.0
decay rate parameter c 0.5 0.6 0.7"
EXPERIMENT SETUP,0.16631578947368422,Test Accuracy
EXPERIMENT SETUP,0.1668421052631579,(a) ResNet-20 on CIFAR-10
EXPERIMENT SETUP,0.16736842105263158,"0.5
0.6
0.7
0.8
0.9
1.0
decay rate parameter c 0.15 0.20 0.25 0.30 0.35 0.40"
EXPERIMENT SETUP,0.16789473684210526,Test Accuracy
EXPERIMENT SETUP,0.16842105263157894,(b) ResNet-20 on CIFAR-100
EXPERIMENT SETUP,0.16894736842105262,"0.5
0.6
0.7
0.8
0.9
1.0
decay rate parameter c 0.1 0.2 0.3 0.4 0.5"
EXPERIMENT SETUP,0.1694736842105263,Test Accuracy
EXPERIMENT SETUP,0.17,(c) ResNet-110 on CIFAR-100
EXPERIMENT SETUP,0.1705263157894737,"Figure 1: Average test accuracy and standard deviation (shallow blue region) under different decay
rate parameters c."
EXPERIMENT SETUP,0.17105263157894737,"0
20000
40000
60000
80000
Step t 0 1 2 3 4"
EXPERIMENT SETUP,0.17157894736842105,Training Loss
EXPERIMENT SETUP,0.17210526315789473,"1 = 10
30"
EXPERIMENT SETUP,0.1726315789473684,"1 = 10
15"
EXPERIMENT SETUP,0.1731578947368421,"1 = 10
8"
EXPERIMENT SETUP,0.1736842105263158,"1 = 10
5"
EXPERIMENT SETUP,0.17421052631578948,"1 = 10
3"
EXPERIMENT SETUP,0.17473684210526316,"1 = 10
1"
EXPERIMENT SETUP,0.17526315789473684,(a) ResNet-20 on CIFAR-10
EXPERIMENT SETUP,0.17578947368421052,"0
20000
40000
60000
80000
Step t 0 2 4 6"
EXPERIMENT SETUP,0.1763157894736842,Training Loss
EXPERIMENT SETUP,0.17684210526315788,"1 = 10
30"
EXPERIMENT SETUP,0.1773684210526316,"1 = 10
15"
EXPERIMENT SETUP,0.17789473684210527,"1 = 10
8"
EXPERIMENT SETUP,0.17842105263157895,"1 = 10
5"
EXPERIMENT SETUP,0.17894736842105263,"1 = 10
3"
EXPERIMENT SETUP,0.1794736842105263,"1 = 10
1"
EXPERIMENT SETUP,0.18,(b) ResNet-20 on CIFAR-100
EXPERIMENT SETUP,0.18052631578947367,"0
20000
40000
60000
80000
Step t 0 2 4 6 8"
EXPERIMENT SETUP,0.18105263157894738,Training Loss
EXPERIMENT SETUP,0.18157894736842106,"1 = 10
30"
EXPERIMENT SETUP,0.18210526315789474,"1 = 10
15"
EXPERIMENT SETUP,0.18263157894736842,"1 = 10
8"
EXPERIMENT SETUP,0.1831578947368421,"1 = 10
5"
EXPERIMENT SETUP,0.18368421052631578,"1 = 10
3"
EXPERIMENT SETUP,0.18421052631578946,"1 = 10
1"
EXPERIMENT SETUP,0.18473684210526317,(c) ResNet-110 on CIFAR-100
EXPERIMENT SETUP,0.18526315789473685,"Figure 2: Training loss vs. steps using Adafactor without update clipping under different ϵ1. The
step-size ηt, decay rate β2,k, and learning rate warm-up are set by default."
EXPERIMENT SETUP,0.18578947368421053,"8.2
Report on Experiment 1
283"
EXPERIMENT SETUP,0.1863157894736842,"We test Adafactor without update clipping using decay rate parameter c ranging from 0.5 to 1.0 in
284"
EXPERIMENT SETUP,0.1868421052631579,"increments of 0.05, while keeping other hyper-parameters at their default values. Each experiment is
285"
EXPERIMENT SETUP,0.18736842105263157,"run 10 times with 100 epochs, and we plot the average test accuracy and standard deviation (shallow
286"
EXPERIMENT SETUP,0.18789473684210525,"blue region) in Figure 1. The results indicate that c = 1.0 yields better performance and stability
287"
EXPERIMENT SETUP,0.18842105263157893,"compared to c < 1.0 on different models and datasets, corresponding to the highest test accuracy and
288"
EXPERIMENT SETUP,0.18894736842105264,"thinner shallow blue band. These performances show a noticeable improving trend as c increases
289"
EXPERIMENT SETUP,0.18947368421052632,"from 0.5 to 1.0, aligning roughly with the results in Theorem 6.1.
290"
EXPERIMENT SETUP,0.19,"8.3
Report on Experiment 2
291"
EXPERIMENT SETUP,0.19052631578947368,"In the second experiment, we test Adafactor without update clipping under different ϵ1 values. We
292"
EXPERIMENT SETUP,0.19105263157894736,"plot the training loss against the step t on different models and datasets in Figure 2. The performance
293"
EXPERIMENT SETUP,0.19157894736842104,"for ϵ1 = 10−8 and ϵ1 = 10−5 is nearly identical to that for ϵ1 = 10−30. Moreover, even a larger
294"
EXPERIMENT SETUP,0.19210526315789472,"value of 10−3 achieves comparable training performance, though with a slower decrease in loss.
295"
EXPERIMENT SETUP,0.19263157894736843,"Notably, ϵ1 = 10−3 requires approximately the same number of steps (t ≈20000) as ϵ1 = 10−30 to
296"
EXPERIMENT SETUP,0.1931578947368421,"achieve near-zero training loss. We conclude that Adafactor is not sensitive to the choice of ϵ1, and a
297"
EXPERIMENT SETUP,0.1936842105263158,"relatively large ϵ1 can still lead to convergence, making the polynomial dependency O(1/ϵ1) in our
298"
EXPERIMENT SETUP,0.19421052631578947,"convergence bounds acceptable.
299"
EXPERIMENT SETUP,0.19473684210526315,"8.4
Report on Experiment 3
300"
EXPERIMENT SETUP,0.19526315789473683,"In this experiment, we explore the appropriate values of α in Theorem 7.1 to achieve performance
301"
EXPERIMENT SETUP,0.1957894736842105,"comparable to the default setting of d = 1. As indicated by Theorem 7.1, a relatively small α is
302"
EXPERIMENT SETUP,0.19631578947368422,"desirable for better dependency on ϵ1. We train models with α set to 4, 6, 7, 8, and 9, keeping other
303"
EXPERIMENT SETUP,0.1968421052631579,"hyper-parameters at their default values. We also train models with the default d = 1 setting as the
304"
EXPERIMENT SETUP,0.19736842105263158,"baseline. We plot the training loss against the steps in Figures 3 without step-size warm-up and 4
305"
EXPERIMENT SETUP,0.19789473684210526,"with step-size warm-up.
306"
EXPERIMENT SETUP,0.19842105263157894,2The detailed calculation could be found in (95) from the appendix.
K,0.19894736842105262,"0K
2K
4K
6K
8K
10K
Step t 0.0 0.5 1.0 1.5 2.0"
K,0.1994736842105263,Training Loss
K,0.2,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.2005263157894737,(a) ResNet-20 on CIFAR-10
K,0.20105263157894737,"0K
5K
10K
15K
20K
Step t 0 1 2 3 4 5"
K,0.20157894736842105,Training Loss
K,0.20210526315789473,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.2026315789473684,(b) ResNet-20 on CIFAR-100
K,0.2031578947368421,"0K
2K
4K
6K
8K
10K
Step t 0 2 4 6"
K,0.2036842105263158,Training Loss
K,0.20421052631578948,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.20473684210526316,(c) ResNet-110 on CIFAR-100
K,0.20526315789473684,"Figure 3: Training loss vs. steps on different models and datasets. We use step-size without warm-up
technique and test under different α."
K,0.20578947368421052,"0K
5K
10K
15K
20K
25K
30K
Step t 0 1 2 3"
K,0.2063157894736842,Training Loss
K,0.20684210526315788,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.2073684210526316,(a) ResNet-20 on CIFAR-10
K,0.20789473684210527,"0K
10K
20K
30K
40K
50K
Step t 0 2 4 6"
K,0.20842105263157895,Training Loss
K,0.20894736842105263,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.2094736842105263,(b) ResNet-20 on CIFAR-100
K,0.21,"0K
5K
10K
15K
20K
25K
30K
Step t 0 2 4 6"
K,0.21052631578947367,Training Loss
K,0.21105263157894738,"= 9.0
= 8.0
= 7.0
= 6.0
= 4.0
Baseline"
K,0.21157894736842106,(c) ResNet-110 on CIFAR-100
K,0.21210526315789474,"Figure 4: Training loss vs. steps on different models and datasets. We use step-size with warm-up
technique by default and test under different α."
K,0.21263157894736842,"The results indicate that, for these values of α, Adafactor achieves comparable or even better
307"
K,0.2131578947368421,"convergence speed compared to the default threshold (represented by ""Baseline""). The comparable
308"
K,0.21368421052631578,"results to the ""Baseline"" in Figure 3 further suggest that the time-increasing dk in Theorem 7.1 plays a
309"
K,0.21421052631578946,"role similar to that of the default setting, enhancing training stability even when the step-size warm-up
310"
K,0.21473684210526317,"is turned off.
311"
CONCLUSIONS,0.21526315789473685,"9
Conclusions
312"
CONCLUSIONS,0.21578947368421053,"In this paper, we investigate the convergence behavior of Adafactor on non-convex smooth landscapes,
313"
CONCLUSIONS,0.2163157894736842,"considering bounded stochastic gradients. We introduce a new proxy step-size to decouple the
314"
CONCLUSIONS,0.2168421052631579,"stochastic gradients from the unique adaptive step-size. Additionally, we use new estimations to
315"
CONCLUSIONS,0.21736842105263157,"control the errors introduced by matrix factorization and update clipping in Adafactor.
316"
CONCLUSIONS,0.21789473684210525,"Our findings reveal that full-batch Adafactor is capable of finding a stationary point, requiring
317"
CONCLUSIONS,0.21842105263157896,"only a step-size ηk ∼O(1/
√"
CONCLUSIONS,0.21894736842105264,"k) and a second moment decay rate β2,k ∈(0, 1), denoting a wide
318"
CONCLUSIONS,0.21947368421052632,"range including the default setup. In the case of stochastic Adafactor without update clipping, the
319"
CONCLUSIONS,0.22,"convergence rate can achieve the optimal order ˜O(1/
√"
CONCLUSIONS,0.22052631578947368,"T) when β2,k = 1 −1/kc, c = 1. However,
320"
CONCLUSIONS,0.22105263157894736,"performance deteriorates as c decreases. This finding is supported by experimental results. We also
321"
CONCLUSIONS,0.22157894736842104,"explore Adafactor with a time-increasing clipping threshold and derive similar convergence results.
322"
CONCLUSIONS,0.22210526315789475,"The empirical results demonstrate that the new clipping threshold provides performance comparable
323"
CONCLUSIONS,0.22263157894736843,"to the default constant setting.
324"
CONCLUSIONS,0.2231578947368421,"Limitations
There are several limitations in our work that warrant further investigation. First,
325"
CONCLUSIONS,0.2236842105263158,"the polynomial dependency on ϵ1 in our convergence bounds may be further improved to a better
326"
CONCLUSIONS,0.22421052631578947,"dependency, such as log(1/ϵ1). Second, although we provide convergence results for several variants
327"
CONCLUSIONS,0.22473684210526315,"of Adafactor and demonstrate comparable performance to the original one in experiments, the
328"
CONCLUSIONS,0.22526315789473683,"convergence bound for stochastic vanilla Adafactor remains unknown. Finally, our experimental
329"
CONCLUSIONS,0.22578947368421054,"results primarily focus on traditional deep learning tasks due to our GPU limitations. It would be
330"
CONCLUSIONS,0.22631578947368422,"beneficial to test the scalability of our theoretical results, e.g., on large language models.
331"
REFERENCES,0.2268421052631579,"References
332"
REFERENCES,0.22736842105263158,"[1] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive
333"
REFERENCES,0.22789473684210526,"optimization. In Advances in Neural Information Processing Systems, 2019.
334"
REFERENCES,0.22842105263157894,"[2] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood-
335"
REFERENCES,0.22894736842105262,"worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming,
336"
REFERENCES,0.2294736842105263,"199(1-2):165–214, 2023.
337"
REFERENCES,0.23,"[3] Amit Attia and Tomer Koren. SGD with AdaGrad stepsizes: full adaptivity with high probability
338"
REFERENCES,0.2305263157894737,"to unknown parameters, unbounded gradients and affine variance. In International Conference
339"
REFERENCES,0.23105263157894737,"on Machine Learning, 2023.
340"
REFERENCES,0.23157894736842105,"[4] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
341"
REFERENCES,0.23210526315789473,"learning. SIAM Review, 60(2):223–311, 2018.
342"
REFERENCES,0.2326315789473684,"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
343"
REFERENCES,0.2331578947368421,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
344"
REFERENCES,0.2336842105263158,"few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.
345"
REFERENCES,0.23421052631578948,"[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
346"
REFERENCES,0.23473684210526316,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
347"
REFERENCES,0.23526315789473684,"Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–
348"
REFERENCES,0.23578947368421052,"113, 2023.
349"
REFERENCES,0.2363157894736842,"[7] Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and
350"
REFERENCES,0.23684210526315788,"Adam in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv
351"
REFERENCES,0.2373684210526316,"preprint arXiv:1807.06766, 2018.
352"
REFERENCES,0.23789473684210527,"[8] Alexandre Défossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence
353"
REFERENCES,0.23842105263157895,"proof of Adam and Adagrad. Transactions on Machine Learning Research, 2022.
354"
REFERENCES,0.23894736842105263,"[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
355"
REFERENCES,0.2394736842105263,"and stochastic optimization. Journal of Machine Learning Research, 12(7):2121–2159, 2011.
356"
REFERENCES,0.24,"[10] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai,
357"
REFERENCES,0.24052631578947367,"and Rachel Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded
358"
REFERENCES,0.24105263157894738,"gradients and affine variance. In Conference on Learning Theory, 2022.
359"
REFERENCES,0.24157894736842106,"[11] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis
360"
REFERENCES,0.24210526315789474,"for algorithms of the Adam family. In Annual Workshop on Optimization for Machine Learning,
361"
REFERENCES,0.24263157894736842,"2021.
362"
REFERENCES,0.2431578947368421,"[12] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor
363"
REFERENCES,0.24368421052631578,"optimization. In International Conference on Machine Learning, pages 1842–1850. PMLR,
364"
REFERENCES,0.24421052631578946,"2018.
365"
REFERENCES,0.24473684210526317,"[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
366"
REFERENCES,0.24526315789473685,"recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
367"
REFERENCES,0.24578947368421053,"Recognition, 2016.
368"
REFERENCES,0.2463157894736842,"[14] Ali Kavis, Kfir Yehuda Levy, and Volkan Cevher. High probability bounds for a class of
369"
REFERENCES,0.2468421052631579,"nonconvex algorithms with AdaGrad stepsize. In International Conference on Learning Repre-
370"
REFERENCES,0.24736842105263157,"sentations, 2022.
371"
REFERENCES,0.24789473684210525,"[15] Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International
372"
REFERENCES,0.24842105263157896,"Conference on Learning Representations, 2015.
373"
REFERENCES,0.24894736842105264,"[16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
374"
REFERENCES,0.24947368421052632,"2009.
375"
REFERENCES,0.25,"[17] Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. Advances
376"
REFERENCES,0.2505263157894737,"in Neural Information Processing Systems, 36, 2024.
377"
REFERENCES,0.25105263157894736,"[18] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of Adam under relaxed
378"
REFERENCES,0.25157894736842107,"assumptions. In Advances in Neural Information Processing Systems, 2023.
379"
REFERENCES,0.2521052631578947,"[19] Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with
380"
REFERENCES,0.25263157894736843,"adaptive stepsizes. In International Conference on Artificial Intelligence and Statistics, 2019.
381"
REFERENCES,0.2531578947368421,"[20] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum.
382"
REFERENCES,0.2536842105263158,"In Workshop on International Conference on Machine Learning, 2020.
383"
REFERENCES,0.2542105263157895,"[21] Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Nguyen. High probability
384"
REFERENCES,0.25473684210526315,"convergence of stochastic gradient methods. In International Conference on Machine Learning,
385"
REFERENCES,0.25526315789473686,"2023.
386"
REFERENCES,0.2557894736842105,"[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
387"
REFERENCES,0.2563157894736842,"Conference on Learning Representations, 2019.
388"
REFERENCES,0.25684210526315787,"[23] Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. CAME:
389"
REFERENCES,0.2573684210526316,"Confidence-guided adaptive memory efficient optimization. In Proceedings of the 61st Annual
390"
REFERENCES,0.2578947368421053,"Meeting of the Association for Computational Linguistics, 2023.
391"
REFERENCES,0.25842105263157894,"[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
392"
REFERENCES,0.25894736842105265,"Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
393"
REFERENCES,0.2594736842105263,"text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.
394"
REFERENCES,0.26,"[25] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
395"
REFERENCES,0.26052631578947366,"and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
396"
REFERENCES,0.26105263157894737,"layer. In International Conference on Learning Representations, 2017.
397"
REFERENCES,0.2615789473684211,"[26] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
398"
REFERENCES,0.26210526315789473,"cost. In International Conference on Machine Learning, 2018.
399"
REFERENCES,0.26263157894736844,"[27] Li Shen, Congliang Chen, Fangyu Zou, Zequn Jie, Ju Sun, and Wei Liu. A unified analysis
400"
REFERENCES,0.2631578947368421,"of AdaGrad with weighted aggregation and momentum acceleration. IEEE Transactions on
401"
REFERENCES,0.2636842105263158,"Neural Networks and Learning Systems, 2023.
402"
REFERENCES,0.26421052631578945,"[28] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper
403"
REFERENCES,0.26473684210526316,"hyper-parameter. In International Conference on Learning Representations, 2020.
404"
REFERENCES,0.26526315789473687,"[29] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint
405"
REFERENCES,0.2657894736842105,"arXiv:1002.4862, 2010.
406"
REFERENCES,0.26631578947368423,"[30] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running
407"
REFERENCES,0.2668421052631579,"average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
408"
REFERENCES,0.2673684210526316,"[31] Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap
409"
REFERENCES,0.26789473684210524,"between the upper bound and lower bound of Adam’s iteration complexity. In Advances in
410"
REFERENCES,0.26842105263157895,"Neural Information Processing Systems, 2023.
411"
REFERENCES,0.26894736842105266,"[32] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over
412"
REFERENCES,0.2694736842105263,"nonconvex landscapes. Journal of Machine Learning Research, 21(1):9047–9076, 2020.
413"
REFERENCES,0.27,"[33] Junchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic
414"
REFERENCES,0.27052631578947367,"nonconvex minimax optimization. In Advances in Neural Information Processing Systems,
415"
REFERENCES,0.2710526315789474,"2022.
416"
REFERENCES,0.27157894736842103,"[34] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive
417"
REFERENCES,0.27210526315789474,"methods for nonconvex optimization. In Advances in Neural Information Processing Systems,
418"
REFERENCES,0.27263157894736845,"2018.
419"
REFERENCES,0.2731578947368421,"[35] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
420"
REFERENCES,0.2736842105263158,"2012.
421"
REFERENCES,0.27421052631578946,"[36] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can con-
422"
REFERENCES,0.27473684210526317,"verge without any modification on update rules. In Advances in Neural Information Processing
423"
REFERENCES,0.2752631578947368,"Systems, 2022.
424"
REFERENCES,0.27578947368421053,"[37] Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kölker, Zhefeng Wang, and
425"
REFERENCES,0.27631578947368424,"Xiaoming Yuan. Adapprox: Adaptive approximation in adam optimization via randomized
426"
REFERENCES,0.2768421052631579,"low-rank matrices. arXiv preprint arXiv:2403.14958, 2024.
427"
REFERENCES,0.2773684210526316,"[38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
428"
REFERENCES,0.27789473684210525,"Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv
429"
REFERENCES,0.27842105263157896,"preprint arXiv:2303.18223, 2023.
430"
REFERENCES,0.2789473684210526,"[39] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for
431"
REFERENCES,0.2794736842105263,"convergences of Adam and RMSProp. In Proceedings of the IEEE Conference on Computer
432"
REFERENCES,0.28,"Vision and Pattern Recognition, 2019.
433"
REFERENCES,0.2805263157894737,"A
Proof detail for full-batch case
434"
REFERENCES,0.2810526315789474,"We first provide the full-batch Adafactor as follows. The only difference to Algorithm (1) is the
435"
REFERENCES,0.28157894736842104,"replacement of stochastic gradient by deterministic gradient ∇f(Xk) at each iteration.
436"
REFERENCES,0.28210526315789475,Algorithm 2 Full-batch Adafactor
REFERENCES,0.2826315789473684,"Input: Initialization point X1 ∈Rn×m, R0 = 0n, C0 = 0⊤
m, relative step-sizes {ρk}k≥1, decay
rate {β2,k}k≥1 ∈[0, 1), regularization constants ϵ1, ϵ2 > 0, clipping threshold d.
for k = 1, · · · , T do"
REFERENCES,0.2831578947368421,"¯Gk = ∇f(Xk);
¯Rk = β2,k ¯Rk−1 + (1 −β2,k)( ¯Gk ⊙¯Gk + ϵ11n1⊤
m)1m;
¯Ck = β2,k ¯Ck−1 + (1 −β2,k)1⊤
n ( ¯Gk ⊙¯Gk + ϵ11n1⊤
m);
¯
Wk = ( ¯Rk ¯Ck)/1⊤
n ¯Rk;
¯Uk = ¯Gk/
p ¯
Wk;
ˆηk = max{ϵ2, RMS(Xk)}ρk/ max{1, RMS( ¯Uk)/d};
Xk+1 = Xk −ˆηk · ¯Gk/
p ¯
Wk;
end for"
REFERENCES,0.2836842105263158,"Then, we provide the detailed version of Theorem 5.1 as follows.
437"
REFERENCES,0.28421052631578947,"Theorem A.1. Let {Xk}k≥1 be generated by Algorithm 2.
If Assumptions (A1), (A2) hold,
438"
REFERENCES,0.2847368421052632,"∥∇f(Xk)∥F ≤G, ∀k ≥1 and
439"
REFERENCES,0.28526315789473683,"ρk = ρ0/
√"
REFERENCES,0.28578947368421054,"k,
0 < β2,k < 1,
∀k ≥1,
for some positive constant ρ0, then for any T ≥1,
440"
REFERENCES,0.2863157894736842,"min
k∈[T ] ∥∇f(Xk)∥2
F ≤A0A1(f(X1) −f ∗+ ∆2
0 log T + ∆2
0)
√ T
,"
REFERENCES,0.2868421052631579,"min
k∈[T ] ∥∇f(Xk)∥2
F ≤A0A′
1(f(X1) −f ∗+ ˜∆2
0 log T + ∆2
0)
√"
REFERENCES,0.2873684210526316,"T
,
(9)"
REFERENCES,0.28789473684210526,"where we define
441"
REFERENCES,0.28842105263157897,"Θmin = min
k∈[T ] ∥Xk∥∞,
Θmax = max
k∈[T ] ∥Xk∥∞,
G = G2 + mnϵ1,
(10)"
REFERENCES,0.2889473684210526,"and the other constant parameters are given by
442"
REFERENCES,0.2894736842105263,"∆2
0 = Ld2mn(ϵ2 + Θmax)2ρ2
0
2
,
˜∆2
0 = LG2G(ϵ2 + Θmax)2ρ2
0
2mnϵ2
1(1 −β2,1)2
,"
REFERENCES,0.29,"A0 =
max
n
1,
G√G
dϵ1mn(1−β2,1)
o"
REFERENCES,0.2905263157894737,"ρ0 max{ϵ2, Θmin}
, A1 =
q"
REFERENCES,0.2910526315789474,"G4 + G2(m + n)ϵ1 + mnϵ2
1,"
REFERENCES,0.29157894736842105,"A′
1 = s"
REFERENCES,0.29210526315789476,"2
 G4"
REFERENCES,0.2926315789473684,"mnϵ1
+ G2 + ϵ1 
. (11)"
REFERENCES,0.2931578947368421,"A.1
Preliminary
443"
REFERENCES,0.29368421052631577,"We first denote the auxiliary matrix ¯G2
k,ϵ1 = ¯Gk ⊙¯Gk + ϵ11n1⊤
m. In addition, we define ¯Vk =
444

¯v(k)
ij
"
REFERENCES,0.2942105263157895,"ij as follows,
445"
REFERENCES,0.29473684210526313,"¯V0 = 0n×m,
¯Vk = β2,k ¯Vk−1 + (1 −β2,k) ¯G2
k,ϵ1,
k ≥1.
(12)"
REFERENCES,0.29526315789473684,"To simplify the notation, we let ¯Gk =

¯g(k)
ij
"
REFERENCES,0.29578947368421055,"ij, R(i)
¯
Vk, C(j)
¯
Vk and S ¯
Vk be the i-th row sum, j-th column
446"
REFERENCES,0.2963157894736842,"sum and the coordinate sum of ¯Vk respectively. The same definition principal is applied to the
447"
REFERENCES,0.2968421052631579,"notation R(i)
¯
G2
k,ϵ1 and C(j)
¯
G2
k,ϵ1. We also use ¯w(k)
ij , ¯v(k)
ij , ¯u(k)
ij to denote the coordinates of ¯
Wk, ¯Vk, ¯Uk
448"
REFERENCES,0.29736842105263156,"in Algorithm 2 respectively. We also define values G1, G2, G as follows:
449"
REFERENCES,0.29789473684210527,"G1 = G2 + mϵ1,
G2 = G2 + nϵ1,
G = G2 + mnϵ1.
(13)"
REFERENCES,0.2984210526315789,"A.2
Technical lemmas
450"
REFERENCES,0.29894736842105263,"Following the descent lemma for a L-smooth objective function f, we derive that
451"
REFERENCES,0.29947368421052634,"f(Y ) ≤f(X) + ⟨∇f(X), Y −X⟩+ L"
REFERENCES,0.3,"2 ∥Y −X∥2
F ,
∀X, Y ∈Rn×m.
(14)"
REFERENCES,0.3005263157894737,"In the following, we will provide some necessary technical lemmas.
452"
REFERENCES,0.30105263157894735,"Lemma A.1. Let β2,k ∈(0, 1) and Γk be defined by
453"
REFERENCES,0.30157894736842106,"Γ0 = 0,
Γk = β2,kΓk−1 + (1 −β2,k),
∀k ≥1."
REFERENCES,0.3021052631578947,"Then, (1 −β2,1) ≤Γk ≤1, ∀k ≥1.
454"
REFERENCES,0.3026315789473684,"Proof. We could prove the result by induction. Since Γ0 = 0, it’s easy to derive that (1 −β2,1) =
455"
REFERENCES,0.3031578947368421,"Γ1 ≤1. Suppose that for any j ∈[k −1], (1 −β2,1) ≤Γj ≤1. Then
456"
REFERENCES,0.3036842105263158,"Γk ≥β2,k(1 −β2,1) + (1 −β2,k) ≥1 −β2,1,
Γk ≤β2,k + (1 −β2,k) ≤1."
REFERENCES,0.3042105263157895,"The induction is then complete.
457"
REFERENCES,0.30473684210526314,"Lemma A.2. Let ¯Vk be defined in (12). For any k ≥0, it holds that
458"
REFERENCES,0.30526315789473685,"¯Rk = ¯Vk1m,
¯Ck = 1⊤
n ¯Vk,
S ¯
Vk = 1⊤
n ¯Rk = 1⊤
n ¯Vk1m."
REFERENCES,0.3057894736842105,"As a consequence,
459"
REFERENCES,0.3063157894736842,"R(i)
¯
Vk = β2,kR(i)
¯
Vk−1 + (1 −β2,k)R(i)
¯
G2
k,ϵ1,
C(j)
¯
Vk = β2,kC(j)
¯
Vk−1 + (1 −β2,k)C(j)
¯
G2
k,ϵ1.
(15)"
REFERENCES,0.3068421052631579,"Proof. Note that ¯R0 = ¯V01m = 0n and ¯C0 = 1⊤
n ¯V0 = 0⊤
m. Suppose that for any j ≤k −1,
460"
REFERENCES,0.30736842105263157,"¯Rj = ¯Vj1m, ¯Cj = 1⊤
n ¯Vj. Then using the updated rule in Algorithm 2 and (12),
461"
REFERENCES,0.3078947368421053,"¯Rk = β2,k ¯Rk−1 + (1 −β2,k) ¯G2
k,ϵ11m =
 
β2,k ¯Vk−1 + (1 −β2,k) ¯G2
k,ϵ1

1m = ¯Vk1m,
¯Ck = β2,k ¯Ck−1 + (1 −β2,k)1⊤
n ¯G2
k,ϵ1 = 1⊤
n
 
β2,k ¯Vk−1 + (1 −β2,k) ¯G2
k,ϵ1

= 1⊤
n ¯Vk.
(16)"
REFERENCES,0.30842105263157893,"Since S ¯
Vk represents the coordinate sum of ¯Vk, we could derive that
462"
REFERENCES,0.30894736842105264,"S ¯
Vk = n
X i=1 m
X"
REFERENCES,0.3094736842105263,"j=1
¯v(k)
ij
= 1⊤
n ¯Rk = 1⊤
n ¯Vk1m.
(17)"
REFERENCES,0.31,"Since R(i)
¯
Vk denotes the i-th row sum of ¯Vk, it’s the i-th coordinate of ¯Rk. Hence, for each coordinate
463"
REFERENCES,0.3105263157894737,"of ¯Rk, using (16),
464"
REFERENCES,0.31105263157894736,"R(i)
¯
Vk = β2,kR(i)
¯
Vk−1 + (1 −β2,k)R(i)
¯
G2
k,ϵ1."
REFERENCES,0.31157894736842107,"Similarly, we could derive the results related to C(j)
¯
Vk .
465"
REFERENCES,0.3121052631578947,"Lemma A.3. Following the parameter setting in (3), for any i ∈[n], j ∈[m], k ≥1, it holds that
466"
REFERENCES,0.3126315789473684,"R(i)
¯
Vk ∈[mϵ1(1 −β2,1), G1],
C(j)
¯
Vk ∈[nϵ1(1 −β2,1), G2],
S ¯
Vk ∈[mnϵ1(1 −β2,1), G]."
REFERENCES,0.3131578947368421,"Proof. Recalling the definition of ¯Vk in (12) and ∥∇f(Xk)∥F ≤G, ∀k ≥1, we derive that
467"
REFERENCES,0.3136842105263158,"S ¯
Vk = n
X i=1 m
X"
REFERENCES,0.3142105263157895,"j=1
¯v(k)
ij
= n
X i=1 m
X j=1 k
X"
REFERENCES,0.31473684210526315,"p=1
(1 −β2,p)

¯g(p)
ij
2
+ ϵ1   
k
Y"
REFERENCES,0.31526315789473686,"l=p+1
β2,l   ≤ k
X"
REFERENCES,0.3157894736842105,"p=1
(1 −β2,p)  
k
Y"
REFERENCES,0.3163157894736842,"l=p+1
β2,l "
REFERENCES,0.31684210526315787,"∥¯Gp∥2
F + Γkmnϵ1 ≤G2Γk + mnϵ1 ≤G,
(18)"
REFERENCES,0.3173684210526316,"where the last inequality comes from Lemma A.1. Following (18) and Lemma A.1, we also derive
468"
REFERENCES,0.3178947368421053,"that
469"
REFERENCES,0.31842105263157894,"S ¯
Vk ≥mnϵ1Γk ≥mnϵ1(1 −β2,1)."
REFERENCES,0.31894736842105265,"We also derive the upper bounds for R(i)
¯
Vk and C(j)
¯
Vk as follows,
470"
REFERENCES,0.3194736842105263,"R(i)
¯
Vk = m
X"
REFERENCES,0.32,"j=1
¯v(k)
ij
≤ k
X"
REFERENCES,0.32052631578947366,"p=1
(1 −β2,p)  
k
Y"
REFERENCES,0.32105263157894737,"l=p+1
β2,l "
REFERENCES,0.3215789473684211,"∥¯Gp∥2
F + Γkmϵ1 ≤G2Γk + mϵ1 ≤G1,"
REFERENCES,0.32210526315789473,"C(j)
¯
Vk = n
X"
REFERENCES,0.32263157894736844,"i=1
¯v(k)
ij
≤ k
X"
REFERENCES,0.3231578947368421,"p=1
(1 −β2,p)  
k
Y"
REFERENCES,0.3236842105263158,"l=p+1
β2,l "
REFERENCES,0.32421052631578945,"∥¯Gp∥2
F + Γknϵ1 ≤G2Γk + nϵ1 ≤G2.
(19)"
REFERENCES,0.32473684210526316,"Similarly, the lower bound could be derived by
471"
REFERENCES,0.32526315789473687,"R(i)
¯
Vk ≥mϵ1Γk ≥mϵ1(1 −β2,1),
C(j)
¯
Vk ≥nϵ1Γk ≥nϵ1(1 −β2,1). 472"
REFERENCES,0.3257894736842105,"A.3
Proof of Theorem A.1
473"
REFERENCES,0.3263157894736842,"Now we move to prove the main result. Using (14) and the updated rule in Algorithm 2,
474"
REFERENCES,0.3268421052631579,"f(Xk+1) ≤f(Xk) + ⟨¯Gk, Xk+1 −Xk⟩+ L"
REFERENCES,0.3273684210526316,"2 ∥Xk+1 −Xk∥2
F"
REFERENCES,0.32789473684210524,= f(Xk) −ˆηk
REFERENCES,0.32842105263157895,"*
¯Gk,
¯Gk
p ¯
Wk +"
REFERENCES,0.32894736842105265,"+ Lˆη2
k
2 "
REFERENCES,0.3294736842105263,"¯Gk
p ¯
Wk  2 F
."
REFERENCES,0.33,"We then re-arrange the order, sum up both sides over k ∈[t] and apply f(Xt+1) ≥f ∗from
475"
REFERENCES,0.33052631578947367,"Assumption (A2) to get,
476 t
X"
REFERENCES,0.3310526315789474,"k=1
ˆηk  ¯Gk"
REFERENCES,0.33157894736842103,"4p ¯
Wk  2"
REFERENCES,0.33210526315789474,"F
|
{z
}
(a)"
REFERENCES,0.33263157894736844,"≤f(X1) −f ∗+ L 2 t
X"
REFERENCES,0.3331578947368421,"k=1
ˆη2
k "
REFERENCES,0.3336842105263158,"¯Gk
p ¯
Wk  2"
REFERENCES,0.33421052631578946,"F
|
{z
}
(b)"
REFERENCES,0.33473684210526317,".
(20)"
REFERENCES,0.3352631578947368,"Since Θmin ≤∥Xk∥∞≤Θmax, we have Θmin ≤RMS(Xk) ≤Θmax for any k ≥1. Hence, using
477"
REFERENCES,0.3357894736842105,"ˆηk defined in Algorithm 2,
478"
REFERENCES,0.33631578947368423,"ˆηk =
max{ϵ2, RMS(Xk)}ρk
max

1, ∥¯Uk∥F /(d√mn)
	 ≤(ϵ2 + Θmax)ρk min

1, d√mn"
REFERENCES,0.3368421052631579,∥¯Uk∥F
REFERENCES,0.3373684210526316,"
.
(21)"
REFERENCES,0.33789473684210525,"Using (21), ¯Uk = ¯Gk/
p ¯
Wk, ∆0 in (11) and ρk = ρ0/
√"
REFERENCES,0.33842105263157896,"k, we thus derive that
479"
REFERENCES,0.3389473684210526,"(b) ≤Ld2mn(ϵ2 + Θmax)2 2 t
X"
REFERENCES,0.3394736842105263,"k=1
ρ2
k · ∥¯Uk∥2
F
∥¯Uk∥2
F
= ∆2
0 t
X k=1"
REFERENCES,0.34,"1
k .
(22)"
REFERENCES,0.3405263157894737,"To lower bound (a), we first discuss the maximum operator inside ˆηk. Let
480"
REFERENCES,0.3410526315789474,"E1 =

k ∈[t] | ∥¯Uk∥F ≥d√mn
	
,
E2 =

k ∈[t] | ∥¯Uk∥F ≤d√mn
	
."
REFERENCES,0.34157894736842104,"When k ∈E1, since ∥Xk∥∞≥Θmin, it derives that
481"
REFERENCES,0.34210526315789475,"ˆηk ≥d√mn max{ϵ2, Θmin}ρk"
REFERENCES,0.3426315789473684,"∥¯Uk∥F
.
(23)"
REFERENCES,0.3431578947368421,"Using Lemma A.2, we first derive that ¯w(k)
ij
= (R(i)
¯
VkC(j)
¯
Vk )/S ¯
Vk. Then, applying Lemma A.3 and
482"
REFERENCES,0.3436842105263158,"∥∇f(Xk)∥F ≤G, we could upper bound ∥¯Uk∥2
F as follows,
483"
REFERENCES,0.34421052631578947,"∥¯Uk∥2
F = n
X i=1 m
X j=1"
REFERENCES,0.3447368421052632,"
¯g(k)
ij
2
S ¯
Vk"
REFERENCES,0.3452631578947368,"R(i)
¯
VkC(j)
¯
Vk
≤
∥¯Gk∥2
F G
mnϵ2
1(1 −β2,1)2 ≤
G2G
mnϵ2
1(1 −β2,1)2 .
(24)"
REFERENCES,0.34578947368421054,"Hence, combining with (23) and (24), we have
484 X"
REFERENCES,0.3463157894736842,"k∈E1
ˆηk  ¯Gk"
REFERENCES,0.3468421052631579,"4p ¯
Wk  2"
REFERENCES,0.3473684210526316,"F
≥d√mn max{ϵ2, Θmin}
X k∈E1"
REFERENCES,0.34789473684210526,"ρk
∥¯Uk∥F  ¯Gk"
REFERENCES,0.34842105263157896,"4p ¯
Wk  2 F"
REFERENCES,0.3489473684210526,"≥dϵ1mn(1 −β2,1) max{ϵ2, Θmin} G√G X"
REFERENCES,0.3494736842105263,"k∈E1
ρk  ¯Gk"
REFERENCES,0.35,"4p ¯
Wk  2"
REFERENCES,0.3505263157894737,"F
.
(25)"
REFERENCES,0.3510526315789474,"When k ∈E2, we obtain that ˆηk = max{ϵ2, RMS(Xk)}ρk ≥max{ϵ2, Θmin}ρk and thus
485 X"
REFERENCES,0.35157894736842105,"k∈E2
ˆηk  ¯Gk"
REFERENCES,0.35210526315789475,"4p ¯
Wk  2"
REFERENCES,0.3526315789473684,"F
≥max{ϵ2, Θmin}
X"
REFERENCES,0.3531578947368421,"k∈E2
ρk  ¯Gk"
REFERENCES,0.35368421052631577,"4p ¯
Wk  2"
REFERENCES,0.3542105263157895,"F
.
(26)"
REFERENCES,0.3547368421052632,"Combining with (25) and (26), we derive that
486"
REFERENCES,0.35526315789473684,"(a) ≥max{ϵ2, Θmin} min

1, dϵ1mn(1 −β2,1) G√G 
t
X"
REFERENCES,0.35578947368421054,"k=1
ρk  ¯Gk"
REFERENCES,0.3563157894736842,"4p ¯
Wk  2"
REFERENCES,0.3568421052631579,"F
.
(27)"
REFERENCES,0.35736842105263156,"We also derive from Lemma A.2 and Lemma A.3 that for any i ∈[n], j ∈[m],
487"
REFERENCES,0.35789473684210527,"¯w(k)
ij
=
R(i)
¯
VkC(j)
¯
Vk
S ¯
Vk
≤
R(i)
¯
VkC(j)
¯
Vk
q"
REFERENCES,0.358421052631579,"R(i)
¯
VkC(j)
¯
Vk ≤
q"
REFERENCES,0.3589473684210526,"R(i)
¯
VkC(j)
¯
Vk ≤
p"
REFERENCES,0.35947368421052633,"G1G2.
(28)"
REFERENCES,0.36,"Using (28), we have
488  ¯Gk"
REFERENCES,0.3605263157894737,"4p ¯
Wk  2 F
= n
X i=1 m
X j=1"
REFERENCES,0.36105263157894735,"
¯g(k)
ij
2 q"
REFERENCES,0.36157894736842106,"¯w(k)
ij
≥∥¯Gk∥2
F
√G1G2
= ∥¯Gk∥2
F
A1
,
(29)"
REFERENCES,0.36210526315789476,"where A1 has been defined in (11). Plugging (29) into (27), we derive that
489"
REFERENCES,0.3626315789473684,"(a) ≥max{ϵ2, Θmin}"
REFERENCES,0.3631578947368421,"A1
min

1, dϵ1mn(1 −β2,1) G√G 
t
X"
REFERENCES,0.3636842105263158,"k=1
ρk∥¯Gk∥2
F .
(30)"
REFERENCES,0.3642105263157895,"Plugging (22) and (30) into (20), and using ρk = ρ0/
√"
REFERENCES,0.36473684210526314,"k, we thus derive that
490"
REFERENCES,0.36526315789473685,"min
k∈[t] ∥¯Gk∥2
F t
X k=1 1
√ k
≤ t
X k=1"
REFERENCES,0.36578947368421055,"ρk∥¯Gk∥2
F
ρ0
≤A0A1 "
REFERENCES,0.3663157894736842,"f(X1) −f ∗+ ∆2
0 t
X k=1"
K,0.3668421052631579,"1
k ! ,"
K,0.36736842105263157,"where A0 is given in (11). Moreover, we have the following results,
491 t
X k=1"
K,0.3678947368421053,"1
k ≤1 +
Z t 1"
K,0.3684210526315789,"1
xdx = 1 + log t, t
X k=1 1
√ k
≥
√"
K,0.36894736842105263,"t.
(31)"
K,0.36947368421052634,"We thus derive the first desired result in (9) as follows,
492"
K,0.37,"min
k∈[t] ∥¯Gk∥2
F ≤A0A1
√ t"
K,0.3705263157894737," 
f(X1) −f ∗+ ∆2
0 + ∆2
0 log t

.
(32)"
K,0.37105263157894736,"Avoiding the curse of dimension
To derive a free-dimension numerator bound, we first derive
493"
K,0.37157894736842106,"from (21) and (24) with ρk = ρ0/
√"
K,0.3721052631578947,"k that
494"
K,0.3726315789473684,"(b) ≤L(ϵ2 + Θmax)2 2 t
X"
K,0.37315789473684213,"k=1
ρ2
k∥¯Uk∥2
F ≤LG2G(ϵ2 + Θmax)2"
K,0.3736842105263158,"2mnϵ2
1(1 −β2,1)2 t
X"
K,0.3742105263157895,"k=1
ρ2
k = ˜∆2
0 t
X k=1"
K,0.37473684210526315,"1
k ,
(33)"
K,0.37526315789473685,"where ˜∆0 has been defined in (11). In addition, we derive from Lemma A.2, Lemma A.3 and (13)
495"
K,0.3757894736842105,"that
496"
K,0.3763157894736842,"¯w(k)
ij
=
R(i)
¯
VkC(j)
¯
Vk
S ¯
Vk
≤2G1G2"
K,0.37684210526315787,"mnϵ1
≤2
 G4"
K,0.3773684210526316,"mnϵ1
+ G2 + ϵ1"
K,0.3778947368421053,"
= (A′
1)2,
(34)"
K,0.37842105263157894,"where we use m + n ≤mn and A′
1 in (11). Thereby, we have
497  ¯Gk"
K,0.37894736842105264,"4p ¯
Wk  2 F
= n
X i=1 m
X j=1"
K,0.3794736842105263,"
¯g(k)
ij
2 q"
K,0.38,"¯w(k)
ij
≥∥¯Gk∥2
F
A′
1
."
K,0.38052631578947366,"Combining with (27), we thus derive that
498"
K,0.38105263157894737,"(a) ≥max{ϵ2, Θmin}"
K,0.3815789473684211,"A′
1
min

1, dϵ1mn(1 −β2,1) G√G 
t
X"
K,0.3821052631578947,"k=1
ρk∥¯Gk∥2
F
(35)"
K,0.38263157894736843,"Plugging (33) and (35) into (20), and using ρk = ρ0/
√"
K,0.3831578947368421,"k, we derive that
499"
K,0.3836842105263158,"min
k∈[t] ∥¯Gk∥2
F t
X k=1 1
√ k
≤ t
X k=1"
K,0.38421052631578945,"ρk∥¯Gk∥2
F
ρ0
≤A0A′
1 "
K,0.38473684210526315,"f(X1) −f ∗+ ˜∆2
0 t
X k=1"
K,0.38526315789473686,"1
k ! ,"
K,0.3857894736842105,"where A0 has been defined in (11). Using (31), we derive the second desired result in (9).
500"
K,0.3863157894736842,"min
k∈[t] ∥¯Gk∥2
F ≤A0A′
1
√ t"
K,0.3868421052631579,"
f(X1) −f ∗+ ˜∆2
0 + ˜∆2
0 log t

.
(36)"
K,0.3873684210526316,"B
Proof detail for stochastic Adafactor without update clipping
501"
K,0.38789473684210524,"We first provide the detailed version of Theorem 6.1.
502"
K,0.38842105263157894,"Theorem B.1 (Formal statement of Theorem 6.1). Let {Xk}k≥1 be generated by Algorithm 1 without
503"
K,0.38894736842105265,"update clipping where ηk is given by (5) for each k ≥1. If Assumptions (A1)-(A4) hold, and
504"
K,0.3894736842105263,"β2,1 = 1/2,
ρ1 = ρ0,"
K,0.39,"β2,k = 1 −1/kc,
ρk = ρ0/
√"
K,0.39052631578947367,"k,
∀k ≥2,"
K,0.3910526315789474,"for some constants 1/2 ≤c ≤1, ρ0 > 0, then for any T ≥1, δ ∈(0, 1), we have the following
505"
K,0.391578947368421,"results.
506"
K,0.39210526315789473,"When c = 1, with probability at least 1 −δ,
507"
K,0.39263157894736844,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.3931578947368421,"
C1 log
T δ"
K,0.3936842105263158,"
+ C2 log T + C2 + C3"
K,0.39421052631578946,"
,
(37)"
K,0.39473684210526316,"min
k∈[T ] ∥¯Gk∥2
F ≤C′
0
√ T"
K,0.3952631578947368,"
C1 log
T δ"
K,0.3957894736842105,"
+ (C′
2 + C′
3) log T + C′
2 + C′
3"
K,0.39631578947368423,"
.
(38)"
K,0.3968421052631579,"When 1/2 ≤c < 1, with probability at least 1 −δ,
508"
K,0.3973684210526316,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.39789473684210525,"
C1 log
T δ"
K,0.39842105263157895,"
+
C2
1 −c · T 1−c + C2 + C3"
K,0.3989473684210526,"
,
(39)"
K,0.3994736842105263,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.4,"
C1 log
T δ"
K,0.4005263157894737,"
+ 2C′
2
1 −c · T 1−c + C′
3 log T + C′
2 + C′
3"
K,0.4010526315789474,"
.
(40)"
K,0.40157894736842104,"Here, Θmin, Θmax and G are as in (10), and
509"
K,0.40210526315789474,"C1 = f(X1) −f ∗+ 24G2(ϵ2 + Θmax)ρ0
√ϵ1
.
(41)"
K,0.4026315789473684,"The C0, C2, C3 are constants defined as
510"
K,0.4031578947368421,"C0 =
2√2G
ρ0 max{ϵ2, Θmin},
C3 = C2"
LOG,0.4036842105263158,"4 log

2 + 2G2 ϵ1 
,"
LOG,0.40421052631578946,"C2 = 32mnG
3
2 (ϵ2 + Θmax)ρ0
max{m, n}ϵ1
+ 4LmnG(ϵ2 + Θmax)2ρ2
0
max{m, n}ϵ1
.
(42)"
LOG,0.4047368421052632,"The C′
0, C′
2, C′
3 are positive constants (that could be further upper bounded by constants independent
511"
LOG,0.4052631578947368,"from m, n), defined by
512"
LOG,0.40578947368421053,"C′
0 =
2
r"
LOG,0.4063157894736842,"2

G2
mnϵ1 + G + ϵ1
"
LOG,0.4068421052631579,"ρ0 max{ϵ2, Θmin}
, C′
2 = 4G3(G1 + G2)(ϵ2 + Θmax)ρ0, C′
3 = LG3(ϵ2 + Θmax)2ρ2
0
2
, (43)"
LOG,0.4073684210526316,"and G1, G2, G3 are given by
513 G1 = s"
LOG,0.40789473684210525,"6
 G4"
LOG,0.40842105263157896,"mnϵ1
+ G2 + ϵ1"
LOG,0.4089473684210526,"
,
G3 = 4(G4 + G2mnϵ1)"
LOG,0.4094736842105263,"mnϵ2
1
,"
LOG,0.41,"G2 = 2
 G3"
LOG,0.4105263157894737,"mnϵ1
+
2G2
√mnϵ1
+
G
√mn + G + √ϵ1"
LOG,0.4110526315789474,"
.
(44)"
LOG,0.41157894736842104,"Calculation of hyper-parameter dependency
To derive a free dimension bound, we shall use the
514"
LOG,0.41210526315789475,"convergence bounds in (38) and (40). From (43), it’s easy to show that m, n could only exist in the
515"
LOG,0.4126315789473684,"denominator of C′
0, C′
2, C′
3, which could avoid the curse of dimension.
516"
LOG,0.4131578947368421,"To calculate the dependency of ϵ1, we first show that its dependency in coefficients C0, C1, C2, C3 as
517"
LOG,0.41368421052631577,"follows, based on the assumption that 0 < ϵ1 < 1,
518"
LOG,0.4142105263157895,"C0 ∼O (1) ,
C1 ∼O (1/√ϵ1) ,
C2 ∼O (1/ϵ1) ,
C3 ∼O (C2 log(1/ϵ1)) .
(45)"
LOG,0.4147368421052632,"Thereby, with the convergence bounds in (37) and (39), it’s easy to show that
519"
LOG,0.41526315789473683,"min
k∈[T ] ∥¯Gk∥2
F ≤O
 
ϵ−1
1
log(1/ϵ1)

.
(46)"
LOG,0.41578947368421054,"Proposition B.1. Following the same assumptions and settings in Theorem 6.1, then with probability
520"
LOG,0.4163157894736842,"at least 1 −δ,
521"
LOG,0.4168421052631579,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T "
LOG,0.41736842105263156,"C1 log
T δ"
LOG,0.41789473684210526,"
+ C2 T
X k=1"
LOG,0.41842105263157897,"1
kc + C3 ! ,"
LOG,0.4189473684210526,"and with probability at least 1 −δ,
522"
LOG,0.41947368421052633,"min
k∈[T ] ∥¯Gk∥2
F ≤C′
0
√ T "
LOG,0.42,"C1 log
T δ"
LOG,0.4205263157894737,"
+ C′
2 T
X k=1"
LOG,0.42105263157894735,"1
kc/2+1/2 + C′
3 T
X k=1"
K,0.42157894736842105,"1
k ! ,"
K,0.42210526315789476,"where all constants are given as in Theorem B.1.
523"
K,0.4226315789473684,"B.1
Preliminary
524"
K,0.4231578947368421,"We first follow the notations of ¯Gk =

¯g(k)
ij
"
K,0.4236842105263158,"ij and G, G1, G2 in (13). Let Gk =

g(k)
ij
"
K,0.4242105263157895,"ij and
525"
K,0.42473684210526313,"ξk = Gk −¯Gk. We also define G2
k,ϵ1 = Gk ⊙Gk + ϵ11n1⊤
m and Vk =

v(k)
ij
"
K,0.42526315789473684,"ij as follows,
526"
K,0.42578947368421055,"V0 = 0n×m,
Vk = β2,kVk−1 + (1 −β2,k)G2
k,ϵ1,
k ≥1.
(47)"
K,0.4263157894736842,"We also define R(i)
Vk, C(j)
Vk and SVk as the i-th row sum, j-th column sum and coordinate sum of Vk
527"
K,0.4268421052631579,"respectively. R(i)
G2
k,ϵ1 and C(j)
G2
k,ϵ1 represent the same definitions with respect to G2
k,ϵ1. Then, using a
528"
K,0.42736842105263156,"similar deduction in Lemma A.2, we also obtain that for all k ≥1,
529"
K,0.42789473684210527,"R(i)
Vk = β2,kR(i)
Vk−1 + (1 −β2,k)G2
k,ϵ11m,
C(j)
Vk = β2,kC(j)
Vk−1 + (1 −β2,k)1⊤
n G2
k,ϵ1.
(48)"
K,0.4284210526315789,"As a consequence of (48), each coordinate of Wk satisfies that
530"
K,0.42894736842105263,"w(k)
ij
= R(i)
VkC(j)
Vk
SVk
="
K,0.42947368421052634,"
β2,kR(i)
Vk−1 + (1 −β2,k)R(i)
G2
k,ϵ1"
K,0.43," 
β2,kC(j)
Vk−1 + (1 −β2,k)C(j)
G2
k,ϵ1 "
K,0.4305263157894737,"β2,kSVk−1 + (1 −β2,k)SG2
k,ϵ1
. (49)"
K,0.43105263157894735,"Next, we introduce a proxy step-size matrix Ak =

a(k)
ij
"
K,0.43157894736842106,"ij such that
531"
K,0.4321052631578947,"a(k)
ij ="
K,0.4326315789473684,"
β2,kR(i)
Vk−1 + (1 −β2,k)G1
 
β2,kC(j)
Vk−1 + (1 −β2,k)G2
"
K,0.43315789473684213,"β2,kSVk−1 + (1 −β2,k)G
.
(50)"
K,0.4336842105263158,"The proxy step-size technique is a standard way in the convergence analysis of adaptive methods,
532"
K,0.4342105263157895,"e.g., [32, 8]. We provide a new proxy step-size in (50) to handle the matrix factorization in Adafactor.
533"
K,0.43473684210526314,"This construction satisfies two properties. First, it’s independent from Zk in order to disrupt the
534"
K,0.43526315789473685,"correlation of stochastic gradients and adaptive step-sizes. Second, it needs to remain sufficiently
535"
K,0.4357894736842105,"close to the original adaptive step-size w(k)
ij to avoid generating divergent terms.
536"
K,0.4363157894736842,"B.2
Technical lemmas
537"
K,0.4368421052631579,"In the following, we first provide some more necessary technical lemmas. We introduce a concentra-
538"
K,0.4373684210526316,"tion inequality for the martingale difference sequence, see [20] for a proof.
539"
K,0.4378947368421053,"Lemma B.1. Suppose that {Zs}s∈[T ] is a martingale difference sequence with respect to ζ1, · · · , ζT .
540"
K,0.43842105263157893,"Assume that for each s ∈[T], σs is a random variable dependent on ζ1, · · · , ζs−1 and satisfies that
541"
K,0.43894736842105264,"E

exp
Z2
s
σ2s"
K,0.4394736842105263,"
| ζ1, · · · , ζs−1 
≤e."
K,0.44,"Then for any λ > 0, and for any δ ∈(0, 1), it holds that
542 P T
X"
K,0.4405263157894737,"s=1
Zs > 1"
K,0.44105263157894736,"λ log
1 δ 
+ 3 4λ T
X"
K,0.44157894736842107,"s=1
σ2
s ! ≤δ."
K,0.4421052631578947,"Lemma B.2. Following the parameter setting in (6), for any i ∈[n], j ∈[m], k ≥1, it holds that
543"
K,0.44263157894736843,"R(i)
G2
k,ϵ1, R(i)
Vk ∈[mϵ1/2, G1],
C(j)
G2
k,ϵ1, C(j)
Vk ∈[nϵ1/2, G2],
SG2
k,ϵ1, SVk ∈[mnϵ1/2, G]."
K,0.4431578947368421,"Proof. First, using Assumption (A4), we derive that
544"
K,0.4436842105263158,"mnϵ1/2 ≤SG2
k,ϵ1 = n
X i=1 m
X j=1"
K,0.4442105263157895,"
g(k)
ij
2
+ ϵ1"
K,0.44473684210526315,"
= ∥Gk∥2
F + mnϵ1 ≤G,"
K,0.44526315789473686,"mϵ1/2 ≤R(i)
G2
k,ϵ1 = m
X j=1"
K,0.4457894736842105,"
g(k)
ij
2
+ ϵ1"
K,0.4463157894736842,"
≤∥Gk∥2
F + mϵ1 ≤G1,"
K,0.4468421052631579,"nϵ1/2 ≤C(j)
G2
k,ϵ1 = n
X i=1"
K,0.4473684210526316,"
g(k)
ij
2
+ ϵ1"
K,0.4478947368421053,"
≤∥Gk∥2
F + nϵ1 ≤G2."
K,0.44842105263157894,"Using the similar deduction for Lemma A.3, we could show that mϵ1(1 −β2,1) ≤R(i)
Vk ≤G1. Since
545"
K,0.44894736842105265,"β2,1 = 1/2 from (6), we then obtain the desired result. The bounds for C(j)
Vk , SVk could be also
546"
K,0.4494736842105263,"derived by using similar arguments.
547"
K,0.45,"We have the following lemma to upper bound each coordinate of the proxy step-size matrix Ak
548"
K,0.45052631578947366,"defined in (50) .
549"
K,0.45105263157894737,"Lemma B.3. For any k ≥1, it holds that
550"
K,0.4515789473684211,"β2,k(1 −β2,k)ϵ1 ≤a(k)
ij ≤2 min

G,
G2"
K,0.45210526315789473,"mnϵ1
+ G + ϵ1"
K,0.45263157894736844,"
,
∀i ∈[n], j ∈[m]."
K,0.4531578947368421,"Proof. We first have
551"
K,0.4536842105263158,"β2,kR(i)
Vk−1 + (1 −β2,k)G1
β2,kSVk−1 + (1 −β2,k)G ≤
β2,kR(i)
Vk−1
β2,kSVk−1
+ (1 −β2,k)G1"
K,0.45421052631578945,"(1 −β2,k)G ≤2.
(51)"
K,0.45473684210526316,"Then, recalling the definition of a(k)
ij in (50) and Lemma B.2, it derives that C(j)
Vk−1 ≤G2 and thereby
552"
K,0.45526315789473687,"β2,kC(j)
Vk−1 + (1 −β2,k)G2 ≤G2 ≤G. Then combining with (51), we derive a(k)
ij
≤2G. We also
553"
K,0.4557894736842105,"derive a free dimension bound from Lemma B.2 for a(k)
ij as follows,
554"
K,0.45631578947368423,"a(k)
ij ≤2G1G2"
K,0.4568421052631579,"mnϵ1
= 2(G2 + G(m + n)ϵ1 + mnϵ2
1)
mnϵ1
≤2
 G2"
K,0.4573684210526316,"mnϵ1
+ G + ϵ1 
,"
K,0.45789473684210524,"where we use m + n ≤mn when m, n ≥2 and β2,kSVk−1 + (1 −β2,k)G ≥mnϵ1/2. To lower
555"
K,0.45842105263157895,"bound a(k)
ij , we derive from Lemma B.2 that β2,kSVk−1 + (1 −β2,k)G ≤G. Thereby,
556"
K,0.4589473684210526,"a(k)
ij ≥
β2,k(1 −β2,k)

R(i)
Vk−1G2 + C(j)
Vk−1G1
"
K,0.4594736842105263,"G
≥β2,k(1 −β2,k) · (mG2 + nG1)ϵ1"
G,0.46,2G
G,0.4605263157894737,"= β2,k(1 −β2,k) · [(m + n)G2 + 2mnϵ1]ϵ1"
G,0.4610526315789474,"2(G2 + mnϵ1)
≥β2,k(1 −β2,k)ϵ1. 557"
G,0.46157894736842103,"Lemma B.4. Let Wk and Vk be defined in Algorithm 1 without update clipping where ηk is given by
558"
G,0.46210526315789474,"(5) and (47) respectively. For any k ≥1, it holds that
559"
G,0.4626315789473684,"Gk
√Wk  2"
G,0.4631578947368421,"F
≤
2G
max{m, n}ϵ1"
G,0.4636842105263158,"Gk
√Vk  2 F
."
G,0.46421052631578946,"Proof. Recalling (49), v(k)
ij
≤R(i)
Vk ,v(k)
ij
≤C(j)
Vk and Lemma B.2, one could verify that
560"
G,0.46473684210526317,"
g(k)
ij
2"
G,0.4652631578947368,"w(k)
ij
="
G,0.46578947368421053,"
g(k)
ij
2
SVk"
G,0.4663157894736842,"R(i)
VkC(j)
Vk
≤
2

g(k)
ij
2
G"
G,0.4668421052631579,"nϵ1v(k)
ij
,"
G,0.4673684210526316,"
g(k)
ij
2"
G,0.46789473684210525,"w(k)
ij
="
G,0.46842105263157896,"
g(k)
ij
2
SVk"
G,0.4689473684210526,"R(i)
VkC(j)
Vk
≤
2

g(k)
ij
2
G"
G,0.4694736842105263,"mϵ1v(k)
ij
,"
G,0.47,"which leads to the desired result that
561"
G,0.4705263157894737,"∥Uk∥2
F =

Gk
√Wk  2"
G,0.4710526315789474,"F
≤
2G
max{m, n}ϵ1"
G,0.47157894736842104,"Gk
√Vk  2 F
. 562"
G,0.47210526315789475,"The following lemma is inspired by [8, Lemma 5.2] where they considered a constant β2,k. Here, we
563"
G,0.4726315789473684,"generalize the result to the case of time-varying β2,k and provide the proof detail.
564"
G,0.4731578947368421,"Lemma B.5. For any t ≥1, if β2,k are as in (6), then it holds that
565 t
X"
G,0.47368421052631576,"k=1
(1 −β2,k)

Gk
√Vk  2"
G,0.47421052631578947,"F
≤mn log
2(G2 + ϵ1) ϵ1"
G,0.4747368421052632,"
+ 4mn t
X"
G,0.47526315789473683,"k=1
(1 −β2,k)."
G,0.47578947368421054,"Proof. Recalling the definition of Vk and since V0 = 0n×m, we have that for any k ≥1,
566"
G,0.4763157894736842,"v(k)
ij
= β2,kv(k−1)
ij
+ (1 −β2,k)

g(k)
ij
2
+ ϵ1  = k
X"
G,0.4768421052631579,"p=1
(1 −β2,p)

g(p)
ij
2
+ ϵ1   
k
Y"
G,0.47736842105263155,"l=p+1
β2,l  ."
G,0.47789473684210526,"Then, we have
567"
G,0.47842105263157897,"(1 −β2,k) ·"
G,0.4789473684210526,"
g(k)
ij
2"
G,0.47947368421052633,"v(k)
ij
=
xk
yk + θk
,
(52)"
G,0.48,"where we set y0 = 0, θ0 = 0 and
568"
G,0.4805263157894737,"xk = (1 −β2,k)

g(k)
ij
2
,
yk = k
X"
G,0.48105263157894734,"p=1
(1 −β2,p)

g(p)
ij
2
 
k
Y"
G,0.48157894736842105,"l=p+1
β2,l  ,"
G,0.48210526315789476,"θk = ϵ1 k
X"
G,0.4826315789473684,"p=1
(1 −β2,p)  
k
Y"
G,0.4831578947368421,"l=p+1
β2,l "
G,0.48368421052631577,",
∀k ≥1."
G,0.4842105263157895,"Then we have yk −xk = β2,kyk−1, ∀k ≥1. Moreover, since yk ≥xk, we could use log x ≥
569"
G,0.48473684210526313,"1 −1/x, ∀x ≥1 to derive that
570"
G,0.48526315789473684,"xk
yk + θk
≤log(yk + θk) −log(yk + θk −xk) = log(yk + θk) −log(β2,kyk−1 + θk)"
G,0.48578947368421055,"= log

yk + θk
yk−1 + θk−1"
G,0.4863157894736842,"
+ log
 yk−1 + θk−1"
G,0.4868421052631579,"β2,kyk−1 + θk 
."
G,0.48736842105263156,"Noting that θk = β2,kθk−1 + (1 −β2,k)ϵ1, which leads to β2,kθk−1 ≤θk. Hence, we further have
571"
G,0.48789473684210527,"xk
yk + θk
≤log

yk + θk
yk−1 + θk−1"
G,0.4884210526315789,"
+ log

yk−1 + θk−1
β2,k(yk−1 + θk−1)"
G,0.48894736842105263,"
= log

yk + θk
yk−1 + θk−1"
G,0.48947368421052634,"
−log β2,k. (53)"
G,0.49,"Hence, summing up on both sides of (52) and (53) over k ∈[t], and noting that x1 = y1, we obtain
572"
G,0.4905263157894737,"that
573 t
X"
G,0.49105263157894735,"k=1
(1 −β2,k) ·"
G,0.49157894736842106,"
g(k)
ij
2"
G,0.4921052631578947,"v(k)
ij
=
x1
y1 + θ1
+ t
X k=2"
G,0.4926315789473684,"xk
yk + ϵk"
G,0.49315789473684213,"≤1 + log
 yt + θt"
G,0.4936842105263158,"y1 + θ1 
− t
X"
G,0.4942105263157895,"k=2
log β2,k.
(54)"
G,0.49473684210526314,"Note that y1 + θ1 ≥(1 −β2,1)ϵ1 = ϵ1/2. Moreover, using Lemma A.1 and Assumption (A4), we
574"
G,0.49526315789473685,"have θt = Γtϵ1 ≤ϵ1 and yt ≤ΓtG2 ≤G2. We then derive that
575"
G,0.4957894736842105,"yt + θt
y1 + θ1
≤2(G2 + ϵ1)"
G,0.4963157894736842,"ϵ1
.
(55)"
G,0.4968421052631579,"Noting that for k ≥2, c ∈[1/2, 1], β2,k ≥β2,2 = 1 −1/2c ≥1 −1/
√"
G,0.49736842105263157,"2, we then derive that
576"
G,0.4978947368421053,"−log β2,k ≤1 −β2,k"
G,0.49842105263157893,"β2,k
≤ √"
G,0.49894736842105264,"2(1 −β2,k)
√"
G,0.4994736842105263,"2 −1
≤4(1 −β2,k).
(56)"
G,0.5,"Finally, plugging (55), (56) into (54), and then summing (54) up over i ∈[n], j ∈[m], we obtain the
577"
G,0.5005263157894737,"desired result.
578"
G,0.5010526315789474,"Next, we have the following probabilistic result relying on the property of the martingale difference
579"
G,0.501578947368421,"sequence which is commonly used in the analysis of adaptive methods.
580"
G,0.5021052631578947,"Lemma B.6. Following the parameter setting in (6), for any T ≥1 and λ > 0, with probability at
581"
G,0.5026315789473684,"least 1 −δ, ∀t ∈[T],
582 − t
X"
G,0.5031578947368421,"k=1
ηk"
G,0.5036842105263157,"¯Gk,
ξk
√Ak ≤1 4 t
X"
G,0.5042105263157894,"k=1
ηk ¯Gk 4√Ak  2"
G,0.5047368421052632,"F
+ 24G2(ϵ2 + Θmax)ρ0
√ϵ1
log
T δ 
."
G,0.5052631578947369,"Proof. Let ζk = −ηk
D
¯Gk,
ξk
√Ak"
G,0.5057894736842106,"E
and the filtration Fk = σ (Z1, · · · , Zk) where σ(·) denotes the
583"
G,0.5063157894736842,"σ-algebra. Note that ηk, ¯Gk and Ak are dependent by {X1, · · · , Xk−1} and thereby Fk−1. Since
584"
G,0.5068421052631579,"ξk is dependent by Fk, we could prove that {ζk}k≥1 is a martingale difference sequence since
585"
G,0.5073684210526316,E [ζk | Fk−1] = −ηk
G,0.5078947368421053,"¯Gk, E [ξk | Fk−1]
√Ak = 0,"
G,0.508421052631579,"where we apply that E [ξk | Fk−1] = EZk[ξk] = 0 from Assumption (A3). Then, using Assumption
586"
G,0.5089473684210526,"(A3) and Assumption (A4), we have
587"
G,0.5094736842105263,"∥¯Gk∥F = ∥EZk[Gk]∥F ≤EZk∥Gk∥F ≤G,
∥ξk∥F = ∥Gk −¯Gk∥F ≤2G."
G,0.51,"Let ωk = 2Gηk

¯
Gk
√Ak"
G,0.5105263157894737,"F . We thus derive from the Cauchy-Schwarz inequality that
588"
G,0.5110526315789473,"E

exp
 ζ2
k
ω2
k"
G,0.511578947368421,"
| Fk−1 
≤E  exp  
"
G,0.5121052631578947,"¯
Gk
√Ak 2"
G,0.5126315789473684,"F ∥ξk∥2
F"
G,0.5131578947368421,"4G2

¯
Gk
√Ak 2 F "
G,0.5136842105263157,"
| Fk−1 "
G,0.5142105263157895,≤exp(1).
G,0.5147368421052632,"Then, using Lemma B.1, it leads to that for any λ > 0, with probability at least 1 −δ,
589 − t
X"
G,0.5152631578947369,"k=1
ηk"
G,0.5157894736842106,"¯Gk,
ξk
√Ak"
G,0.5163157894736842,"≤3λG2
t
X"
G,0.5168421052631579,"k=1
η2
k"
G,0.5173684210526316,"¯Gk
√Ak  2 F
+ 1"
G,0.5178947368421053,"λ log
1 δ "
G,0.5184210526315789,"= 3λG2
t
X k=1 n
X i=1 m
X j=1 ηk
q"
G,0.5189473684210526,"a(k)
ij
· ηk"
G,0.5194736842105263,"
¯g(k)
ij
2 q"
G,0.52,"a(k)
ij
+ 1"
G,0.5205263157894737,"λ log
1 δ"
G,0.5210526315789473,"
.
(57)"
G,0.521578947368421,"Meanwhile, when Θmin ≤∥Xk∥∞≤Θmax, ρk = ρ0/
√"
G,0.5221052631578947,"k, we have
590"
G,0.5226315789473684,"Θmin ≤RMS(Xk) ≤Θmax,
max{ϵ2, Θmin}ρ0
√"
G,0.5231578947368422,"k
≤ηk ≤(ϵ2 + Θmax)ρ0
√"
G,0.5236842105263158,"k
.
(58)"
G,0.5242105263157895,"Combining with Lemma B.3, we derive that
591 ηk
q"
G,0.5247368421052632,"a(k)
ij
≤
ηk
p"
G,0.5252631578947369,"β2,k(1 −β2,k)ϵ1
≤(ϵ2 + Θmax)ρ0
p"
G,0.5257894736842105,"β2,kϵ1
· kc/2 √"
G,0.5263157894736842,"k
(59)"
G,0.5268421052631579,"≤
(ϵ2 + Θmax)ρ0
p"
G,0.5273684210526316,"min{β2,1, β2,2}ϵ1
≤2(ϵ2 + Θmax)ρ0
√ϵ1
,
(60)"
G,0.5278947368421053,"where we use β2,1 = 1/2, β2,2 = 1 −1/2c ≥1 −1/
√"
G,0.5284210526315789,"2, c ∈[1/2, 1] from (6) in the last inequality.
592"
G,0.5289473684210526,"Hence, plugging (60) into (57) and then re-scaling the δ, we found that with probability at least 1 −δ,
593"
G,0.5294736842105263,"for all t ∈[T],
594 − t
X"
G,0.53,"k=1
ηk"
G,0.5305263157894737,"¯Gk,
ξk
√Ak"
G,0.5310526315789473,"≤6λG2(ϵ2 + Θmax)ρ0
√ϵ1 t
X"
G,0.531578947368421,"k=1
ηk ¯Gk 4√Ak  2 F
+ 1"
G,0.5321052631578947,"λ log
T δ 
."
G,0.5326315789473685,"Setting λ = √ϵ1/(24G2(ϵ2 + Θmax)ρ0), we derive the desired result.
595"
G,0.533157894736842,"The following key lemma provides an upper bound for the error brought by the proxy step-size a(k)
ij ,
596"
G,0.5336842105263158,"illustrating the error is controllable.
597"
G,0.5342105263157895,"Lemma B.7. For any k ≥1, i ∈[n], j ∈[m], it holds that
598"
G,0.5347368421052632,"w(k)
ij −a(k)
ij

q"
G,0.5352631578947369,"a(k)
ij
≤
p"
G,0.5357894736842105,"1 −β2,k min{4
√"
G,0.5363157894736842,"G, G1 + G2},
(61)"
G,0.5368421052631579,"where G is as in (13) and G1, G2 are as in (44).
599"
G,0.5373684210526316,"Proof. To simplify the notation, we let
600"
G,0.5378947368421053,"X = β2,kR(i)
Vk−1 + (1 −β2,k)R(i)
G2
k,ϵ1,
∆X = (1 −β2,k)(G1 −R(i)
G2
k,ϵ1),"
G,0.5384210526315789,"Y = β2,kC(j)
Vk−1 + (1 −β2,k)C(j)
G2
k,ϵ1,
∆Y = (1 −β2,k)(G2 −C(j)
G2
k,ϵ1),"
G,0.5389473684210526,"Z = β2,kSVk−1 + (1 −β2,k)SG2
k,ϵ1,
∆Z = (1 −β2,k)(G −SG2
k,ϵ1).
(62)"
G,0.5394736842105263,"Then we have
601"
G,0.54,"w(k)
ij −a(k)
ij
 =

XY"
G,0.5405263157894736,"Z
−(X + ∆X)(Y + ∆Y )"
G,0.5410526315789473,Z + ∆Z
G,0.541578947368421,"=

XY ∆Z −XZ∆Y −Y Z∆X −Z(∆X∆Y )"
G,0.5421052631578948,Z(Z + ∆Z) .
G,0.5426315789473685,"Applying Lemma B.2, we could verify that X, Y, Z ≥0 and
602"
G,0.5431578947368421,"0 ≤∆X ≤(1 −β2,k)G1,
0 ≤∆Y ≤(1 −β2,k)G2,
0 ≤∆Z ≤(1 −β2,k)G.
(63)"
G,0.5436842105263158,"Hence, we derive that
603"
G,0.5442105263157895,"w(k)
ij −a(k)
ij

q"
G,0.5447368421052632,"a(k)
ij
= |XY ∆Z −XZ∆Y −Y Z∆X −Z(∆X∆Y )| Z
p"
G,0.5452631578947369,(X + ∆X)(Y + ∆Y )(Z + ∆Z)
G,0.5457894736842105,"≤
|X∆Y + Y ∆X + (∆X∆Y )|
p"
G,0.5463157894736842,"(X + ∆X)(Y + ∆Y )(Z + ∆Z)
|
{z
}
(I)"
G,0.5468421052631579,"+
XY ∆Z Z
p"
G,0.5473684210526316,"(X + ∆X)(Y + ∆Y )(Z + ∆Z)
|
{z
}
(II) . (64)"
G,0.5478947368421052,"Since XY ≥0 from (62), Term (I) could be bounded as
604"
G,0.5484210526315789,"(I) ≤
|X∆Y + Y ∆X + (∆X∆Y )|
p"
G,0.5489473684210526,"(X∆Y + Y ∆X + (∆X∆Y ))(Z + ∆Z)
≤ r"
G,0.5494736842105263,X∆Y + Y ∆X + (∆X∆Y )
G,0.55,"Z + ∆Z
.
(65)"
G,0.5505263157894736,"Recalling the definition, we have R(i)
Vk−1 ≤SVk−1, C(j)
Vk−1 ≤SVk−1 for any i ∈[n], j ∈[m]. Further,
605"
G,0.5510526315789473,"applying Lemma B.2 and (63), we derive that
606"
G,0.5515789473684211,"X∆Y
Z + ∆Z ≤ "
G,0.5521052631578948,"R(i)
Vk−1
SVk−1
+
R(i)
G2
k,ϵ1
G "
G,0.5526315789473685,"∆Y ≤2(1 −β2,k)G2."
G,0.5531578947368421,"Y ∆X
Z + ∆Z ≤ "
G,0.5536842105263158,"C(j)
Vk−1
SVk−1
+
C(j)
G2
k,ϵ1
G "
G,0.5542105263157895,"∆X ≤2(1 −β2,k)G1,"
G,0.5547368421052632,"∆X∆Y
Z + ∆Z ≤∆X(1 −β2,k)G"
G,0.5552631578947368,"(1 −β2,k)G
≤(1 −β2,k)G1."
G,0.5557894736842105,"We then derive from (65), G1 ≤G and G2 ≤G that
607"
G,0.5563157894736842,"(I) ≤
q"
G,0.5568421052631579,"5(1 −β2,k)G.
(66)"
G,0.5573684210526316,"To derive a free dimension bound, we could obtain from Lemma B.2, (63) and G ≥mnϵ1/2 that
608"
G,0.5578947368421052,"Z + ∆Z ≥mnϵ1/2. Hence,
609"
G,0.5584210526315789,"X∆Y
Z + ∆Z ≤2(1 −β2,k)G1G2"
G,0.5589473684210526,"mnϵ1
,
Y ∆X
Z + ∆Z ≤2(1 −β2,k)G1G2"
G,0.5594736842105263,"mnϵ1
,
∆X∆Y
Z + ∆Z ≤2(1 −β2,k)G1G2"
G,0.56,"mnϵ1
."
G,0.5605263157894737,"We then derive that
610 (I) ≤ s"
G,0.5610526315789474,"6(1 −β2,k)G1G2"
G,0.5615789473684211,"mnϵ1
= s"
G,0.5621052631578948,"6(1 −β2,k)(G4 + G2ϵ1(m + n) + mnϵ2
1)
mnϵ1
≤G1
p"
G,0.5626315789473684,"1 −β2,k, (67)"
G,0.5631578947368421,"where we used m + n ≤mn, and G1 is defined in (44). Then, combining with (66) and (67), we
611"
G,0.5636842105263158,"have
612"
G,0.5642105263157895,"(I) ≤
p"
G,0.5647368421052632,"1 −β2,k min{
√"
G,0.5652631578947368,"5G, G1},
(68)"
G,0.5657894736842105,"where we applied that m + n ≤mn when m, n ≥2. Then we move to bound (II). Recalling the
613"
G,0.5663157894736842,"definitions in (62), we have X ≤Z, Y ≤Z. Applying (63), we have
614"
G,0.5668421052631579,"(II) ≤
XY ∆Z
Z
√"
G,0.5673684210526316,"XY ∆Z
≤ √ XY ∆Z Z
≤
√"
G,0.5678947368421052,"∆Z ≤
q"
G,0.5684210526315789,"(1 −β2,k)G."
G,0.5689473684210526,"Similarly, we derive from Lemma B.2 that Z ≥mnϵ1/2, X ≤G1, Y ≤G2. Hence,
615"
G,0.5694736842105264,(II) ≤ √ XY ∆Z
G,0.57,"Z
≤2
p"
G,0.5705263157894737,"(1 −β2,k)G1G2G mnϵ1 ≤2
p"
G,0.5710526315789474,"1 −β2,k  G3"
G,0.5715789473684211,"mnϵ1
+
2G2
√mnϵ1
+ G +
G
√mn + √ϵ1"
G,0.5721052631578948,"
≤G2
p"
G,0.5726315789473684,"1 −β2,k,"
G,0.5731578947368421,"where G2 has been defined in (44). We thus derive that
616"
G,0.5736842105263158,"(II) ≤
p"
G,0.5742105263157895,"1 −β2,k min{
√"
G,0.5747368421052632,"G, G2}.
(69)
Combining (68) with (69), we then derive the desired result.
617"
G,0.5752631578947368,"B.3
Proof of Proposition B.1
618"
G,0.5757894736842105,"Using the inequality in (14), we have
619"
G,0.5763157894736842,"f(Xk+1) ≤f(Xk) + ⟨¯Gk, Xk+1 −Xk⟩+ L"
G,0.5768421052631579,"2 ∥Xk+1 −Xk∥2
F"
G,0.5773684210526315,≤f(Xk) −ηk
G,0.5778947368421052,"¯Gk,
Gk
√Wk"
G,0.578421052631579,"+ Lη2
k
2"
G,0.5789473684210527,"Gk
√Wk  2 F
."
G,0.5794736842105264,"Introducing the proxy step-size matrix Ak in (50) and then summing up both sides over k ∈[t], we
620"
G,0.58,"derive that
621"
G,0.5805263157894737,"f(Xt+1) ≤f(X1) − t
X"
G,0.5810526315789474,"k=1
ηk"
G,0.5815789473684211,"¯Gk, Gk
√Ak "
G,0.5821052631578948,"|
{z
}
A + t
X"
G,0.5826315789473684,"k=1
ηk"
G,0.5831578947368421,"¯Gk, Gk ⊙

1
√Ak
−
1
√Wk "
G,0.5836842105263158,"|
{z
}
B + t
X k=1"
G,0.5842105263157895,"Lη2
k
2"
G,0.5847368421052631,"Gk
√Wk  2"
G,0.5852631578947368,"F
|
{z
}
C"
G,0.5857894736842105,".
(70)"
G,0.5863157894736842,"Estimation for A
We first introduce ξk into A,
622 A = − t
X"
G,0.5868421052631579,"k=1
ηk ¯Gk 4√Ak  2 F
− t
X"
G,0.5873684210526315,"k=1
ηk"
G,0.5878947368421052,"¯Gk,
ξk
√Ak"
G,0.588421052631579,".
(71)"
G,0.5889473684210527,"Then, using Lemma B.6, with probability at least 1 −δ, for all t ∈[T],
623"
G,0.5894736842105263,"A = −3 4 t
X"
G,0.59,"k=1
ηk ¯Gk 4√Ak  2"
G,0.5905263157894737,"F
+ 24G2(ϵ2 + Θmax)ρ0
√ϵ1
log
T δ"
G,0.5910526315789474,"
.
(72)"
G,0.5915789473684211,"Estimation for B
Term B is essentially the error brought by the proxy step-size Ak. We will first
624"
G,0.5921052631578947,"calculate the gap of 1/
q"
G,0.5926315789473684,"w(k)
ij and 1/
q"
G,0.5931578947368421,"a(k)
ij as follows,
625 "
Q,0.5936842105263158,"1
q"
Q,0.5942105263157895,"w(k)
ij
−
1
q"
Q,0.5947368421052631,"a(k)
ij =
1
q"
Q,0.5952631578947368,"w(k)
ij
q"
Q,0.5957894736842105,"a(k)
ij  q"
Q,0.5963157894736842,"w(k)
ij −
q"
Q,0.5968421052631578,"a(k)
ij ≤
1
q"
Q,0.5973684210526315,"w(k)
ij
q"
Q,0.5978947368421053,"a(k)
ij"
Q,0.598421052631579,"rw(k)
ij −a(k)
ij
.
(73)"
Q,0.5989473684210527,"We then apply (73) and Young’s inequality,
626 B ≤ t
X k=1 n
X i=1 m
X"
Q,0.5994736842105263,"j=1
ηk
¯g(k)
ij g(k)
ij "
Q,0.6,"1
q"
Q,0.6005263157894737,"w(k)
ij
−
1
q"
Q,0.6010526315789474,"a(k)
ij  ≤ t
X k=1 n
X i=1 m
X"
Q,0.6015789473684211,"j=1
ηk"
Q,0.6021052631578947,"¯g(k)
ij g(k)
ij

q"
Q,0.6026315789473684,"w(k)
ij
q"
Q,0.6031578947368421,"a(k)
ij"
Q,0.6036842105263158,"rw(k)
ij −a(k)
ij ≤1 4 t
X k=1 n
X i=1 m
X"
Q,0.6042105263157894,"j=1
ηk ·"
Q,0.6047368421052631,"
¯g(k)
ij
2 q"
Q,0.6052631578947368,"a(k)
ij
+ 4 t
X k=1 n
X i=1 m
X"
Q,0.6057894736842105,"j=1
ηk ·"
Q,0.6063157894736843,"w(k)
ij −a(k)
ij

q"
Q,0.6068421052631578,"a(k)
ij
· "
Q,0.6073684210526316,"g(k)
ij
q"
Q,0.6078947368421053,"w(k)
ij   2"
Q,0.608421052631579,".
(74)"
Q,0.6089473684210527,"Thus, plugging (61) in Lemma B.7 into (74), we derive that
627 B ≤1 4 t
X"
Q,0.6094736842105263,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.61,"F
+ 4
√ G t
X"
Q,0.6105263157894737,"k=1
ηk
p"
Q,0.6110526315789474,"1 −β2,k"
Q,0.611578947368421,"Gk
√Wk  2 F ≤1 4 t
X"
Q,0.6121052631578947,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6126315789473684,"F
+ 4
√ G t
X k=1"
Q,0.6131578947368421,"(ϵ2 + Θmax)ρ0
√ k p"
Q,0.6136842105263158,"1 −β2,k"
Q,0.6142105263157894,"Gk
√Wk  2 F ≤1 4 t
X"
Q,0.6147368421052631,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6152631578947368,"F
+ 4
√ G t
X"
Q,0.6157894736842106,"k=1
(ϵ2 + Θmax)ρ0(1 −β2,k)

Gk
√Wk  2"
Q,0.6163157894736843,"F
,
(75)"
Q,0.6168421052631579,"where we used (58) in the second inequality and 1/
√"
Q,0.6173684210526316,"k ≤1/kc/2, c ∈[1/2, 1]. Furthermore, using
628"
Q,0.6178947368421053,"Lemma B.4 and Lemma B.5, we derive that
629 B ≤1 4 t
X"
Q,0.618421052631579,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6189473684210526,"F
+ 8mnG
3
2 (ϵ2 + Θmax)ρ0
max{m, n}ϵ1 """
Q,0.6194736842105263,"log

2 + 2G2 ϵ1 
+ 4 t
X"
Q,0.62,"k=1
(1 −β2,k) #"
Q,0.6205263157894737,. (76)
Q,0.6210526315789474,"Estimating C
Using the similar deduction in (75) and (76), we derive that
630"
Q,0.621578947368421,"C ≤LmnG(ϵ2 + Θmax)2ρ2
0
max{m, n}ϵ1 """
Q,0.6221052631578947,"log

2 + 2G2 ϵ1 
+ 4 t
X"
Q,0.6226315789473684,"k=1
(1 −β2,k) #"
Q,0.6231578947368421,".
(77)"
Q,0.6236842105263158,"Putting together
We first re-arrange the order in (70) and use f(Xt+1) ≥f ∗in Assumption (A2)
631"
Q,0.6242105263157894,"to derive that
632"
Q,0.6247368421052631,"0 ≤f(X1) −f ∗+ A + B + C.
(78)"
Q,0.6252631578947369,"We then plug (72), (76), (77) into (78) and set t = T, which leads to that with probability at least
633"
Q,0.6257894736842106,"1 −δ,
634 1
2 T
X"
Q,0.6263157894736842,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6268421052631579,"F
≤C1 log
T δ"
Q,0.6273684210526316,"
+ C2 T
X"
Q,0.6278947368421053,"k=1
(1 −β2,k) + C3,
(79)"
Q,0.628421052631579,"where C1, C2, C3 are as in Theorem B.1. Moreover, using Lemma B.3 and (58), we have
635 1
2 T
X"
Q,0.6289473684210526,"k=1
ηk ¯Gk 4√Ak  2 F
≥ T
X k=1"
Q,0.6294736842105263,"ηk
 ¯Gk
2
F"
Q,0.63,"2 maxi,j
q"
Q,0.6305263157894737,"a(k)
ij
≥ρ0 max{ϵ2, Θmin} 2√2G T
X k=1"
Q,0.6310526315789474,"¯Gk
2
F
√"
Q,0.631578947368421,"k
.
(80)"
Q,0.6321052631578947,"Combining with (80) and (79), and using PT
k=1 1/
√ k ≥
√"
Q,0.6326315789473684,"T, we derive that
636"
Q,0.6331578947368421,"min
k∈[T ] ∥¯Gk∥2 ≤C0
√ T "
Q,0.6336842105263157,"C1 log
T δ"
Q,0.6342105263157894,"
+ C2 T
X"
Q,0.6347368421052632,"k=1
(1 −β2,k) + C3 !"
Q,0.6352631578947369,",
(81)"
Q,0.6357894736842106,"where C0 has already been defined in (42). We then derive the first desired result that
637"
Q,0.6363157894736842,"min
k∈[T ] ∥¯Gk∥2 ≤C0
√ T "
Q,0.6368421052631579,"C1 log
T δ"
Q,0.6373684210526316,"
+ C2 T
X k=1"
Q,0.6378947368421053,"1
kc + C3 ! ."
Q,0.638421052631579,"Free dimension bound
We follow the similar deduction in (75) and use Lemma B.7 to derive that
638 B ≤1 4 t
X"
Q,0.6389473684210526,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6394736842105263,"F
+ 4(G1 + G2)(ϵ2 + Θmax)ρ0 t
X k=1"
Q,0.64,"1
kc/2+1/2"
Q,0.6405263157894737,"Gk
√Wk  2"
Q,0.6410526315789473,"F
.
(82)"
Q,0.641578947368421,"Recalling the definition of w(k)
ij in (49) and Lemma B.2, we derive that
639"
Q,0.6421052631578947,"w(k)
ij
= R(i)
VkC(j)
Vk
SVk
≥mnϵ2
1
4G ,

Gk
√ W k  2"
Q,0.6426315789473684,"F
≤
∥Gk∥2
F
mini,j w(k)
ij
≤4G2G"
Q,0.6431578947368422,"mnϵ2
1
≤G3,
(83)"
Q,0.6436842105263157,"where G3 is as in (44). We thus derive from (82) and (83) that
640 B ≤1 4 t
X"
Q,0.6442105263157895,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.6447368421052632,"F
+ 4G3(G1 + G2)(ϵ2 + Θmax)ρ0 t
X k=1"
Q,0.6452631578947369,"1
kc/2+1/2 .
(84)"
Q,0.6457894736842106,"Using (58) and (83), we derive that
641 C = t
X k=1"
Q,0.6463157894736842,"Lη2
k
2"
Q,0.6468421052631579,"Gk
√Wk  2"
Q,0.6473684210526316,"F
≤LG3(ϵ2 + Θmax)2ρ2
0
2 t
X k=1"
Q,0.6478947368421053,"1
k .
(85)"
Q,0.6484210526315789,"Plugging the unchanged estimation for A in (72), (84) and (85) into (70), we have that with probability
642"
Q,0.6489473684210526,"at least 1 −δ, for all t ∈[T],
643 1
2 t
X"
Q,0.6494736842105263,"k=1
ηk ¯Gk 4√Ak  2"
Q,0.65,"F
≤C1 log
T δ"
Q,0.6505263157894737,"
+ C′
2 t
X k=1"
Q,0.6510526315789473,"1
kc/2+1/2 + C′
3 t
X k=1"
Q,0.651578947368421,"1
k ,
(86)"
Q,0.6521052631578947,"where C′
2, C′
3 are given as in (43) and C1 is as in (41). Further, using Lemma B.3 and the similar
644"
Q,0.6526315789473685,"deduction for (80),
645 1
2 t
X"
Q,0.6531578947368422,"k=1
ηk ¯Gk 4√Ak  2 F
≥ t
X k=1"
Q,0.6536842105263158,"ηk
 ¯Gk
2
F"
Q,0.6542105263157895,"2 maxi,j
q"
Q,0.6547368421052632,"a(k)
ij
≥1 C′
0 t
X k=1"
Q,0.6552631578947369,"¯Gk
2
F
√"
Q,0.6557894736842105,"k
,
(87)"
Q,0.6563157894736842,"where C′
0 is as in (43). Combining with (86) and (87), and setting t = T, we derive the second
646"
Q,0.6568421052631579,"desired result in Proposition B.1 that
647"
Q,0.6573684210526316,"min
k∈[T ] ∥¯Gk∥2 ≤C′
0
√ T "
Q,0.6578947368421053,"C1 log
T δ"
Q,0.6584210526315789,"
+ C′
2 T
X k=1"
Q,0.6589473684210526,"1
kc/2+1/2 + C′
3 T
X k=1"
K,0.6594736842105263,"1
k ! ."
K,0.66,"B.4
Proof of Theorem B.1
648"
K,0.6605263157894737,"Now based on the result in Proposition B.1, we could further derive the final convergence rate. Noting
649"
K,0.6610526315789473,"that when c = 1, we could bound that
650 T
X k=1"
K,0.661578947368421,"1
k ≤1 +
Z T 1"
K,0.6621052631578948,"1
xdx ≤1 + log T.
(88)"
K,0.6626315789473685,"Then, we obtain that
651"
K,0.6631578947368421,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.6636842105263158,"
C1 log
T δ"
K,0.6642105263157895,"
+ C2 log T + C2 + C3 
,"
K,0.6647368421052632,"min
k∈[T ] ∥¯Gk∥2
F ≤C′
0
√ T"
K,0.6652631578947369,"
C1 log
T δ"
K,0.6657894736842105,"
+ (C′
2 + C′
3) log T + C′
2 + C′
3 
."
K,0.6663157894736842,"When 1/2 ≤c < 1, we have
652 T
X k=1"
K,0.6668421052631579,"1
kc ≤1 +
Z T 1"
K,0.6673684210526316,"1
xc dx ≤1 + T 1−c 1 −c, T
X k=1"
K,0.6678947368421052,"1
kc/2+1/2 ≤1 +
Z T 1"
K,0.6684210526315789,"1
xc/2+1/2 dx ≤1 + 2T (1−c)/2"
K,0.6689473684210526,"1 −c
.
(89)"
K,0.6694736842105263,"Then, we obtain that
653"
K,0.67,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.6705263157894736,"
C1 log
T δ"
K,0.6710526315789473,"
+
C2
1 −c · T 1−c + C2 + C3 
,"
K,0.671578947368421,"min
k∈[T ] ∥¯Gk∥2
F ≤C0
√ T"
K,0.6721052631578948,"
C1 log
T δ"
K,0.6726315789473685,"
+ 2C′
2
1 −c · T 1−c + C′
3 log T + C′
2 + C′
3 
."
K,0.6731578947368421,"C
Proof detail for stochastic Adafactor with update clipping
654"
K,0.6736842105263158,"We first provide the detailed version of Theorem 7.1 as follows.
655"
K,0.6742105263157895,"Theorem C.1. Let {Xk}k≥1 be the sequence generated by Algorithm 1 with (7). If Assumptions
656"
K,0.6747368421052632,"(A1) -(A4) hold, and
657"
K,0.6752631578947368,"ρk = ρ0/
√"
K,0.6757894736842105,"k,
dk = k"
K,0.6763157894736842,"c
2(α−1) ,
∀k ≥1,
β2,1 = 1/2,
β2,k = 1 −1/kc, ∀k ≥2."
K,0.6768421052631579,"When c = 1, with probability at least 1 −δ,
658"
K,0.6773684210526316,"min
k∈[T ] ∥¯Gk∥2
F ≤D0
√ T"
K,0.6778947368421052,"
C1 log
T δ"
K,0.6784210526315789,"
+ (C2 + D1(α)) log T + C2 + D1(α) + C3"
K,0.6789473684210526,"
,
(90)"
K,0.6794736842105263,"min
k∈[T ] ∥¯Gk∥2
F ≤D0
√ T"
K,0.68,"
C1 log
T δ"
K,0.6805263157894736,"
+ (C′
2 + C′
3 + D1(α)) log T + C′
2 + C′
3 + D1(α)

.
(91)"
K,0.6810526315789474,"When 1/2 ≤c < 1, with probability at least 1 −δ,
659"
K,0.6815789473684211,"min
k∈[T ] ∥¯Gk∥2
F ≤D0
√ T"
K,0.6821052631578948,"
C1 log
T δ"
K,0.6826315789473684,"
+ C2 + D1(α)"
K,0.6831578947368421,"1 −c
· T 1−c + C2 + D1(α) + C3"
K,0.6836842105263158,"
,
(92)"
K,0.6842105263157895,"min
k∈[T ] ∥¯Gk∥2
F ≤D0
√ T"
K,0.6847368421052632,"
C1 log
T δ"
K,0.6852631578947368,"
+ C′
3 log T + 2(C′
2 + D1(α))"
K,0.6857894736842105,"1 −c
· T
1−c"
K,0.6863157894736842,"2
+ C′
2 + C′
3 + D1(α)

, (93)"
K,0.6868421052631579,"where C1, C2, C3, C′
2, C′
3 are as in Theorem B.1 and
660"
K,0.6873684210526316,"D0 = min{C0, C′
0},
D1(α) = G1+αG1−α
4
√G(ϵ2 + Θmax)ρ0
√mnϵ1
,
G4 = mnϵ1"
K,0.6878947368421052,"2√G .
(94)"
K,0.6884210526315789,"Calculation of hyper-parameters’ dependency
We first calculate the dependency on m, n, ϵ1, α
661"
K,0.6889473684210526,"in the additional coefficient D1(α) as follows,
662"
K,0.6894736842105263,D1(α) ∼O
K,0.69,√1 + mnϵ1 mnϵ1
K,0.6905263157894737,α−1 s
K,0.6910526315789474,"1
mnϵ2
1
+ 1 ϵ1 !"
K,0.6915789473684211,",
(95)"
K,0.6921052631578948,"which is free of the curse of dimension since mn exists in the denominator. Recalling the definitions
663"
K,0.6926315789473684,"of C′
0, C1, C′
2, C′
3 in (41) and (43), it’s easy to verify that these coefficients are also free of the
664"
K,0.6931578947368421,"curse of dimension factor m, n since m, n exist in the denominator. Thereby, we also derive a free
665"
K,0.6936842105263158,"dimension bound selecting (91) and (93).
666"
K,0.6942105263157895,"To calculate the dependency on ϵ1, we could combine with (45) and (95) to derive that
667"
K,0.6947368421052632,"C0D1(α) ∼O
 
ϵ−α
1

,
C0C1 ∼O

1/ϵ−1/2
1

,
C0C3 ∼O
 
ϵ−1
1
log(1/ϵ1)

."
K,0.6952631578947368,"Thereby, selecting the bounds in (90) and (92) and noting that α > 1, we derive that the order on ϵ1 is
668 O
 1"
K,0.6957894736842105,"ϵα
1
log
 1 ϵ1"
K,0.6963157894736842,"
.
(96)"
K,0.6968421052631579,"Moreover, it’s clear to reveal that there exist mn in denominator, which could improve the dependency
669"
K,0.6973684210526315,"on ϵ1. If we suppose that mn is comparable to ϵ1, then we derive that C0D1(α) ∼O(ϵ−1/2
1
) and the
670"
K,0.6978947368421052,"order on ϵ1 is
671 O
 1"
K,0.6984210526315789,"ϵ1
log
 1 ϵ1"
K,0.6989473684210527,"
.
(97)"
K,0.6994736842105264,"C.1
Proof of Theorem C.1
672"
K,0.7,"We define
673"
K,0.7005263157894737,"˜Gk =
Gk
max{1, ∥Uk∥F /(dk
√mn)},
ˆρk = max{ϵ2, RMS(Xk)}ρk.
(98)"
K,0.7010526315789474,"Since RMS(Uk) = ∥Uk∥F /√mn, Θmin ≤RMS(Xk) ≤Θmax, we derive that
674"
K,0.7015789473684211,"Xk+1 = Xk −ˆρk
˜Gk
√Wk
,"
K,0.7021052631578948,"max{ϵ2, Θmin}ρ0
√"
K,0.7026315789473684,"k
≤ˆρk ≤(ϵ2 + Θmax)ρ0
√"
K,0.7031578947368421,"k
≤(ϵ2 + Θmax)ρ0
p"
K,0.7036842105263158,"1 −β2,k,
(99)"
K,0.7042105263157895,"where we applied that 1/
√"
K,0.7047368421052631,"k ≤1/kc/2, c ∈[1/2, 1] and β2,k = 1−1/kc in the last inequality. Using
675"
K,0.7052631578947368,"the inequalities in (14) and (99), we have
676"
K,0.7057894736842105,"f(Xk+1) ≤f(Xk) + ⟨¯Gk, Xk+1 −Xk⟩+ L"
K,0.7063157894736842,"2 ∥Xk+1 −Xk∥2
F"
K,0.7068421052631579,≤f(Xk) −ˆρk
K,0.7073684210526315,"*
¯Gk,
˜Gk
√Wk +"
K,0.7078947368421052,"+ Lˆρ2
k
2 "
K,0.708421052631579,"˜Gk
√Wk  2 F
."
K,0.7089473684210527,"Summing up both sides over k ∈[t] and using f(Xt+1) ≥f ∗from Assumption (A2), we derive that
677"
K,0.7094736842105264,"0 ≤f(X1) −f ∗+ t
X"
K,0.71,"k=1
−ˆρk"
K,0.7105263157894737,"*
¯Gk,
˜Gk
√Wk +"
K,0.7110526315789474,"|
{z
}
D + t
X k=1"
K,0.7115789473684211,"Lˆρ2
k
2 "
K,0.7121052631578947,"˜Gk
√Wk  2"
K,0.7126315789473684,"F
|
{z
}
E"
K,0.7131578947368421,".
(100)"
K,0.7136842105263158,"Introducing Ak in (50), we further have the following decomposition,
678 D = − t
X"
K,0.7142105263157895,"k=1
ˆρk"
K,0.7147368421052631,"*
¯Gk,
˜Gk
√Ak + + t
X"
K,0.7152631578947368,"k=1
ˆρk"
K,0.7157894736842105,"¯Gk,

1
√Ak
−
1
√Wk"
K,0.7163157894736842,"
⊙˜Gk "
K,0.716842105263158,"|
{z
}
D.1 = − t
X"
K,0.7173684210526315,"k=1
ˆρk ¯Gk 4√Ak  2"
K,0.7178947368421053,"F
+ D.1 − t
X"
K,0.718421052631579,"k=1
ˆρk"
K,0.7189473684210527,"*
¯Gk,
˜Gk
√Ak
−EZk"
K,0.7194736842105263,"""
˜Gk
√Ak #+"
K,0.72,"|
{z
}
D.2 + t
X"
K,0.7205263157894737,"k=1
ˆρk"
K,0.7210526315789474,"*
¯Gk,
¯Gk
√Ak
−EZk"
K,0.7215789473684211,"""
˜Gk
√Ak #+"
K,0.7221052631578947,"|
{z
}
D.3"
K,0.7226315789473684,".
(101)"
K,0.7231578947368421,"Estimating E
Hence, using (98), (99), Lemma B.4 and Lemma B.5, we derive that
679 E ≤L 2 t
X"
K,0.7236842105263158,"k=1
ˆρ2
k"
K,0.7242105263157895,"Gk
√Wk  2"
K,0.7247368421052631,"F
≤L(ϵ2 + Θmax)2ρ2
0
2 t
X"
K,0.7252631578947368,"k=1
(1 −β2,k)

Gk
√Wk  2 F"
K,0.7257894736842105,"≤LmnG(ϵ2 + Θmax)2ρ2
0
max{m, n}ϵ1 """
K,0.7263157894736842,"log

2 + 2G2 ϵ1 
+ 4 t
X"
K,0.7268421052631578,"k=1
(1 −β2,k) #"
K,0.7273684210526316,".
(102)"
K,0.7278947368421053,"To avoid the curse of dimension, we drive from (98) and (83) that
680 "
K,0.728421052631579,"˜Gk
√Wk  2 F
=
1"
K,0.7289473684210527,"(max{1, ∥Uk∥F /(dk
√mn)})2"
K,0.7294736842105263,"Gk
√Wk  2"
K,0.73,"F
≤

Gk
√Wk  2"
K,0.7305263157894737,"F
≤G3.
(103)"
K,0.7310526315789474,"Then, using (99) and (103), we derive that
681"
K,0.7315789473684211,"E ≤LG3(ϵ2 + Θmax)2ρ2
0
2 t
X k=1"
K,0.7321052631578947,"1
k .
(104)"
K,0.7326315789473684,"Estimating D.1
We could follow the similar deduction in (73) and (74) to derive that
682 D.1 ≤ t
X k=1 n
X i=1 m
X"
K,0.7331578947368421,"j=1
ˆρk|¯g(k)
ij ˜g(k)
ij | "
Q,0.7336842105263158,"1
q"
Q,0.7342105263157894,"w(k)
ij
−
1
q"
Q,0.7347368421052631,"a(k)
ij  ≤ t
X k=1 n
X i=1 m
X"
Q,0.7352631578947368,"j=1
ˆρk
|¯g(k)
ij ˜g(k)
ij |
q"
Q,0.7357894736842105,"w(k)
ij
q"
Q,0.7363157894736843,"a(k)
ij"
Q,0.7368421052631579,"rw(k)
ij −a(k)
ij ≤1 4 t
X k=1 n
X i=1 m
X"
Q,0.7373684210526316,"j=1
ˆρk ·"
Q,0.7378947368421053,"
¯g(k)
ij
2 q"
Q,0.738421052631579,"a(k)
ij
+ 4 t
X k=1 n
X i=1 m
X"
Q,0.7389473684210527,"j=1
ˆρk ·"
Q,0.7394736842105263,"w(k)
ij −a(k)
ij

q"
Q,0.74,"a(k)
ij
· "
Q,0.7405263157894737,"˜g(k)
ij
q"
Q,0.7410526315789474,"w(k)
ij   2"
Q,0.741578947368421,".
(105)"
Q,0.7421052631578947,"Using Lemma B.7 and (105), we further derive that
683"
Q,0.7426315789473684,"D.1 ≤1 4 t
X"
Q,0.7431578947368421,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7436842105263158,"F
+ 4
√ G t
X"
Q,0.7442105263157894,"k=1
ˆρk
p"
Q,0.7447368421052631,"1 −β2,k "
Q,0.7452631578947368,"˜Gk
√Wk  2 F ≤1 4 t
X"
Q,0.7457894736842106,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7463157894736843,"F
+ 4
√ G t
X"
Q,0.7468421052631579,"k=1
ˆρk
p"
Q,0.7473684210526316,"1 −β2,k"
Q,0.7478947368421053,"Gk
√Wk  2 F
."
Q,0.748421052631579,"Using (99), Lemma B.4 and Lemma B.5, we further have
684"
Q,0.7489473684210526,"D.1 ≤1 4 t
X"
Q,0.7494736842105263,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.75,"F
+ 4
√"
Q,0.7505263157894737,"G(ϵ2 + Θmax)ρ0 t
X"
Q,0.7510526315789474,"k=1
(1 −β2,k)

Gk
√Wk  2 F ≤1 4 t
X"
Q,0.751578947368421,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7521052631578947,"F
+ 8mnG
3
2 (ϵ2 + Θmax)ρ0
max{m, n}ϵ1 """
Q,0.7526315789473684,"log

2 + 2G2 ϵ1 
+ 4 t
X"
Q,0.7531578947368421,"k=1
(1 −β2,k) # . (106)"
Q,0.7536842105263157,"To avoid the curse of dimension, we apply Lemma B.7, (99) and (83) to derive that
685"
Q,0.7542105263157894,"D.1 ≤1 4 t
X"
Q,0.7547368421052632,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7552631578947369,"F
+ 4(G1 + G2) t
X"
Q,0.7557894736842106,"k=1
ˆρk
p"
Q,0.7563157894736842,"1 −β2,k"
Q,0.7568421052631579,"Gk
√Wk  2 F ≤1 4 t
X"
Q,0.7573684210526316,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7578947368421053,"F
+ 4(G1 + G2)(ϵ2 + Θmax)ρ0 t
X k=1"
Q,0.758421052631579,"1
kc/2+1/2"
Q,0.7589473684210526,"Gk
√Wk  2 F ≤1 4 t
X"
Q,0.7594736842105263,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.76,"F
+ 4G3(G1 + G2)(ϵ2 + Θmax)ρ0 t
X k=1"
Q,0.7605263157894737,"1
kc/2+1/2 .
(107)"
Q,0.7610526315789473,"Estimating D.2
Since Ak is independent from Zk, it further leads to
686"
Q,0.761578947368421,"D.2 = − t
X"
Q,0.7621052631578947,"k=1
ˆρk"
Q,0.7626315789473684,"¯Gk
√Ak
, ˜Gk −EZk
h
˜Gk
i
."
Q,0.7631578947368421,"Then, the deduction for estimating D.2 follows the similar idea as in Lemma B.6, relying on a
687"
Q,0.7636842105263157,"martingale difference sequence.
688"
Q,0.7642105263157895,"Let us set φk = −ˆρk
D
¯
Gk
√Ak , ˜Gk −EZk
h
˜Gk
iE
and the filtration Fk = σ (Z1, · · · , Zk). Noting that
689"
Q,0.7647368421052632,"ˆρk, ¯Gk and Ak are dependent by Fk−1. Since ξk is dependent by Fk, we could prove that {φk}k≥1
690"
Q,0.7652631578947369,"is a martingale difference sequence by showing that
691"
Q,0.7657894736842106,E [φk | Fk−1] = −ˆρk
Q,0.7663157894736842,"¯Gk
√Ak
, EZk
h
˜Gk −EZk[ ˜Gk]
i
= 0."
Q,0.7668421052631579,"In addition, using Assumptions (A3), (A4) and Jensen’s inequality, we have
692"
Q,0.7673684210526316,"∥˜Gk∥F =
∥Gk∥F
max{1, ∥Uk∥/(dk
√mn)} ≤∥Gk∥F ≤G,
∥EZk[ ˜Gk]∥F ≤EZk∥˜Gk∥F ≤G."
Q,0.7678947368421053,"Therefore, we derive that
693"
Q,0.7684210526315789,"∥˜Gk −EZk[ ˜Gk]∥F ≤∥˜Gk∥F + ∥EZk[ ˜Gk]∥F ≤2G.
(108)"
Q,0.7689473684210526,"Let ω′
k = 2Gˆρk

¯
Gk
√Ak"
Q,0.7694736842105263,"F . We thus derive from the Cauchy-Schwarz inequality and (108) that
694"
Q,0.77,"E

exp
 φ2
k
(ω′
k)2"
Q,0.7705263157894737,"
| Fk−1 
≤E  exp  
"
Q,0.7710526315789473,"¯
Gk
√Ak 2"
Q,0.771578947368421,"F ∥˜Gk −EZk[ ˜Gk]∥2
F"
Q,0.7721052631578947,"4G2

¯
Gk
√Ak 2 F "
Q,0.7726315789473684,"
| Fk−1 "
Q,0.7731578947368422,≤exp(1).
Q,0.7736842105263158,"Then, using Lemma B.1, it leads to that for any λ > 0, with probability at least 1 −δ,
695 D.2 = t
X"
Q,0.7742105263157895,"k=1
φk ≤3λG2
t
X"
Q,0.7747368421052632,"k=1
ˆρ2
k"
Q,0.7752631578947369,"¯Gk
√Ak  2 F
+ 1"
Q,0.7757894736842105,"λ log
1 δ "
Q,0.7763157894736842,"= 3λG2
t
X k=1 n
X i=1 m
X j=1 ˆρk
q"
Q,0.7768421052631579,"a(k)
ij
· ˆρk"
Q,0.7773684210526316,"
¯g(k)
ij
2 q"
Q,0.7778947368421053,"a(k)
ij
+ 1"
Q,0.7784210526315789,"λ log
1 δ 
."
Q,0.7789473684210526,"Since {β2,k}k≥2 is non-decreasing, we could apply Lemma B.3 to derive that
696"
Q,0.7794736842105263,"1
q"
Q,0.78,"a(k)
ij
≤ s"
Q,0.7805263157894737,"1
β2,k(1 −β2,k)ϵ1
≤ s"
Q,0.7810526315789473,"1
min{β2,1, β2,2}(1 −β2,k)ϵ1
≤
2
p"
Q,0.781578947368421,"(1 −β2,k)ϵ1
."
Q,0.7821052631578947,"Then, we apply (99), and re-scale δ to obtain that for any λ > 0, with probability at least 1 −δ, for
697"
Q,0.7826315789473685,"all t ∈[T],
698"
Q,0.783157894736842,"D.2 ≤6λG2ρ0(ϵ2 + Θmax)
√ϵ1 t
X"
Q,0.7836842105263158,"k=1
ˆρk ¯Gk 4√Ak  2 F
+ 1"
Q,0.7842105263157895,"λ log
T δ 
."
Q,0.7847368421052632,"Setting λ = √ϵ1/(24G2ρ0(ϵ2 + Θmax)), we derive that
699"
Q,0.7852631578947369,"D.2 ≤1 4 t
X"
Q,0.7857894736842105,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.7863157894736842,"F
+ 24G2ρ0(ϵ2 + Θmax)
√ϵ1
log
T δ"
Q,0.7868421052631579,"
.
(109)"
Q,0.7873684210526316,"Estimating D.3
First, since Ak is independent from Zk and EZk[Gk] = ¯Gk, we have
700 D.3 = t
X"
Q,0.7878947368421053,"k=1
ˆρk"
Q,0.7884210526315789,"*
¯Gk, EZk[Gk]
√Ak
−EZk[ ˜Gk]
√Ak + ≤ t
X"
Q,0.7889473684210526,"k=1
ˆρk ¯Gk
√ Ak F
·  EZk"
Q,0.7894736842105263,"
Gk −
Gk
max{1, ∥Uk∥F /(dk
√mn)} "
Q,0.79,"|
{z
}
Ωk F"
Q,0.7905263157894736,".
(110)"
Q,0.7910526315789473,"We define the random variable S(1)
k , S(2)
k
and ˜S(1)
k
using the indicator function χ and G4 in (94) as
701"
Q,0.791578947368421,"follows,
702"
Q,0.7921052631578948,"S(1)
k
= χ{∥Uk∥F >dk
√mn},
S(2)
k
= χ{∥Uk∥F ≤dk
√mn},
˜S(1)
k
= χ{∥Gk∥F ≥dkG4}."
Q,0.7926315789473685,"From (83), we derive that
703"
Q,0.7931578947368421,"∥Uk∥F ≤∥Gk∥F ·
2√G
√mnϵ1
."
Q,0.7936842105263158,"Hence, S(1)
k
≤˜S(1)
k , ∀k ≥1. Note that when S(2)
k
= 1, it’s equivalent to Ωk = 0. Then, we derive
704"
Q,0.7942105263157895,"that
705"
Q,0.7947368421052632,"∥EZk[Ωk]∥F =
EZk[ΩkS(1)
k ] + EZk[ΩkS(2)
k ]

F =
EZk[ΩkS(1)
k ]

F"
Q,0.7952631578947369,"≤EZk
ΩkS(1)
k

F ≤EZk
Ωk ˜S(1)
k

F ≤EZk
Gk ˜S(1)
k

F ≤Gα (dkG4)1−α ,
(111)"
Q,0.7957894736842105,"Furthermore, we use Assumption (A4) and Lemma B.2 to derive a lower bound for a(k)
ij where
706"
Q,0.7963157894736842,"a(k)
ij ≥mnϵ2
1
4G ,

¯Gk
√ Ak"
Q,0.7968421052631579,"F
≤
∥¯Gk∥F"
Q,0.7973684210526316,"mini,j
q"
Q,0.7978947368421052,"a(k)
ij
≤2G√G
√mnϵ1
.
(112)"
Q,0.7984210526315789,"Combining with (99), (110), (111) and (112), we thus derive that
707"
Q,0.7989473684210526,"D.3 ≤2G1+αG1−α
4
√G(ϵ2 + Θmax)ρ0
√mnϵ1 t
X k=1"
Q,0.7994736842105263,"1
dα−1
k
√"
Q,0.8,"k
.
(113)"
Q,0.8005263157894736,"Putting together
Both E and D.1 are bounded with two estimations, one of which owns a better
708"
Q,0.8010526315789473,"dependency to 1/ϵ1 and the other avoids the curse of the dimension. We thereby derive two results.
709"
Q,0.8015789473684211,"Plugging (106), (109) and (113) into (101) and then combining with (102) and (100), we then derive
710"
Q,0.8021052631578948,"that with probability at least 1 −δ, for all t ∈[T],
711 1
2 t
X"
Q,0.8026315789473685,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.8031578947368421,"F
≤C1 log
T δ"
Q,0.8036842105263158,"
+ C2 t
X"
Q,0.8042105263157895,"k=1
(1 −β2,k) + C3 + D1(α) t
X k=1"
Q,0.8047368421052632,"1
dα−1
k
√"
Q,0.8052631578947368,"k
,
(114)"
Q,0.8057894736842105,"where C1, C2, C3 are as in Theorem B.1 and D1(α) is as in (94). Plugging (107), (109) and (113)
712"
Q,0.8063157894736842,"into (101), then combining with (104) and (100), we then derive that with probability at least 1 −δ,
713"
Q,0.8068421052631579,"for all t ∈[T],
714 1
2 t
X"
Q,0.8073684210526316,"k=1
ˆρk ¯Gk 4√Ak  2"
Q,0.8078947368421052,"F
≤C1 log
T δ"
Q,0.8084210526315789,"
+ C′
2 t
X k=1"
Q,0.8089473684210526,"1
kc/2+1/2 + C′
3 t
X k=1"
Q,0.8094736842105263,"1
k + D1(α) t
X k=1"
Q,0.81,"1
dα−1
k
√ k
. (115)"
Q,0.8105263157894737,"where C′
2, C′
3 are as in Theorem B.1. Moreover, using (99), we reveal that the lower bound for ˆρk is
715"
Q,0.8110526315789474,"the same the one for ηk in (58). Thereby, following the same deduction in (80) and (86), we derive
716"
Q,0.8115789473684211,"that
717 1
2 T
X"
Q,0.8121052631578948,"k=1
ˆρk ¯Gk 4√Ak  2 F
≥ T
X k=1 ˆρk 2"
Q,0.8126315789473684,"¯Gk
2
F"
Q,0.8131578947368421,"maxi,j
q"
Q,0.8136842105263158,"a(k)
ij
≥1 D0 T
X k=1 1
√ k"
Q,0.8142105263157895,"¯Gk
2
F ,
(116)"
Q,0.8147368421052632,"where D0 = min{C0, C′
0} that has been defined in (94). Setting t = T on (114) and (115), and then
718"
Q,0.8152631578947368,"using (116), we then derive that
719"
Q,0.8157894736842105,"min
t∈[T ]"
Q,0.8163157894736842,"¯Gk
2
F ≤
D0
PT
t=1 1/
√ k "
Q,0.8168421052631579,"C1 log
T δ"
Q,0.8173684210526316,"
+ C2 t
X"
Q,0.8178947368421052,"k=1
(1 −β2,k) + C3 + D1(α) t
X k=1"
Q,0.8184210526315789,"1
dα−1
k
√ k ! ,"
Q,0.8189473684210526,"min
t∈[T ]"
Q,0.8194736842105264,"¯Gk
2
F ≤
D0
PT
t=1 1/
√ k "
Q,0.82,"C1 log
T δ"
Q,0.8205263157894737,"
+ C′
2 t
X k=1"
Q,0.8210526315789474,"1
k(c+1)/2 + C′
3 t
X k=1"
Q,0.8215789473684211,"1
k + D1(α) t
X k=1"
Q,0.8221052631578948,"1
dα−1
k
√ k ! ."
Q,0.8226315789473684,"Then, using the results in (88) and (89), we could derive the desired result in Theorem C.1.
720"
Q,0.8231578947368421,"NeurIPS Paper Checklist
721"
Q,0.8236842105263158,"The checklist is designed to encourage best practices for responsible machine learning research,
722"
Q,0.8242105263157895,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
723"
Q,0.8247368421052632,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
724"
Q,0.8252631578947368,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
725"
Q,0.8257894736842105,"towards the page limit.
726"
Q,0.8263157894736842,"Please read the checklist guidelines carefully for information on how to answer these questions. For
727"
Q,0.8268421052631579,"each question in the checklist:
728"
Q,0.8273684210526315,"• You should answer [Yes] , [No] , or [NA] .
729"
Q,0.8278947368421052,"• [NA] means either that the question is Not Applicable for that particular paper or the
730"
Q,0.828421052631579,"relevant information is Not Available.
731"
Q,0.8289473684210527,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
732"
Q,0.8294736842105264,"The checklist answers are an integral part of your paper submission. They are visible to the
733"
Q,0.83,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
734"
Q,0.8305263157894737,"(after eventual revisions) with the final version of your paper, and its final version will be published
735"
Q,0.8310526315789474,"with the paper.
736"
Q,0.8315789473684211,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
737"
Q,0.8321052631578948,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
738"
Q,0.8326315789473684,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
739"
Q,0.8331578947368421,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
740"
Q,0.8336842105263158,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
741"
Q,0.8342105263157895,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
742"
Q,0.8347368421052631,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
743"
Q,0.8352631578947368,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
744"
Q,0.8357894736842105,"please point to the section(s) where related material for the question can be found.
745"
Q,0.8363157894736842,"IMPORTANT, please:
746"
Q,0.8368421052631579,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
747"
Q,0.8373684210526315,"• Keep the checklist subsection headings, questions/answers and guidelines below.
748"
Q,0.8378947368421052,"• Do not modify the questions and only use the provided macros for your answers.
749"
CLAIMS,0.838421052631579,"1. Claims
750"
CLAIMS,0.8389473684210527,"Question: Do the main claims made in the abstract and introduction accurately reflect the
751"
CLAIMS,0.8394736842105263,"paper’s contributions and scope?
752"
CLAIMS,0.84,"Answer: [Yes]
753"
CLAIMS,0.8405263157894737,"Justification: [NA]
754"
CLAIMS,0.8410526315789474,"Guidelines:
755"
CLAIMS,0.8415789473684211,"• The answer NA means that the abstract and introduction do not include the claims
756"
CLAIMS,0.8421052631578947,"made in the paper.
757"
CLAIMS,0.8426315789473684,"• The abstract and/or introduction should clearly state the claims made, including the
758"
CLAIMS,0.8431578947368421,"contributions made in the paper and important assumptions and limitations. A No or
759"
CLAIMS,0.8436842105263158,"NA answer to this question will not be perceived well by the reviewers.
760"
CLAIMS,0.8442105263157895,"• The claims made should match theoretical and experimental results, and reflect how
761"
CLAIMS,0.8447368421052631,"much the results can be expected to generalize to other settings.
762"
CLAIMS,0.8452631578947368,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
763"
CLAIMS,0.8457894736842105,"are not attained by the paper.
764"
LIMITATIONS,0.8463157894736842,"2. Limitations
765"
LIMITATIONS,0.8468421052631578,"Question: Does the paper discuss the limitations of the work performed by the authors?
766"
LIMITATIONS,0.8473684210526315,"Answer: [Yes]
767"
LIMITATIONS,0.8478947368421053,"Justification: [NA]
768"
LIMITATIONS,0.848421052631579,"Guidelines:
769"
LIMITATIONS,0.8489473684210527,"• The answer NA means that the paper has no limitation while the answer No means that
770"
LIMITATIONS,0.8494736842105263,"the paper has limitations, but those are not discussed in the paper.
771"
LIMITATIONS,0.85,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
772"
LIMITATIONS,0.8505263157894737,"• The paper should point out any strong assumptions and how robust the results are to
773"
LIMITATIONS,0.8510526315789474,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
774"
LIMITATIONS,0.8515789473684211,"model well-specification, asymptotic approximations only holding locally). The authors
775"
LIMITATIONS,0.8521052631578947,"should reflect on how these assumptions might be violated in practice and what the
776"
LIMITATIONS,0.8526315789473684,"implications would be.
777"
LIMITATIONS,0.8531578947368421,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
778"
LIMITATIONS,0.8536842105263158,"only tested on a few datasets or with a few runs. In general, empirical results often
779"
LIMITATIONS,0.8542105263157894,"depend on implicit assumptions, which should be articulated.
780"
LIMITATIONS,0.8547368421052631,"• The authors should reflect on the factors that influence the performance of the approach.
781"
LIMITATIONS,0.8552631578947368,"For example, a facial recognition algorithm may perform poorly when image resolution
782"
LIMITATIONS,0.8557894736842105,"is low or images are taken in low lighting. Or a speech-to-text system might not be
783"
LIMITATIONS,0.8563157894736843,"used reliably to provide closed captions for online lectures because it fails to handle
784"
LIMITATIONS,0.8568421052631578,"technical jargon.
785"
LIMITATIONS,0.8573684210526316,"• The authors should discuss the computational efficiency of the proposed algorithms
786"
LIMITATIONS,0.8578947368421053,"and how they scale with dataset size.
787"
LIMITATIONS,0.858421052631579,"• If applicable, the authors should discuss possible limitations of their approach to
788"
LIMITATIONS,0.8589473684210527,"address problems of privacy and fairness.
789"
LIMITATIONS,0.8594736842105263,"• While the authors might fear that complete honesty about limitations might be used by
790"
LIMITATIONS,0.86,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
791"
LIMITATIONS,0.8605263157894737,"limitations that aren’t acknowledged in the paper. The authors should use their best
792"
LIMITATIONS,0.8610526315789474,"judgment and recognize that individual actions in favor of transparency play an impor-
793"
LIMITATIONS,0.861578947368421,"tant role in developing norms that preserve the integrity of the community. Reviewers
794"
LIMITATIONS,0.8621052631578947,"will be specifically instructed to not penalize honesty concerning limitations.
795"
THEORY ASSUMPTIONS AND PROOFS,0.8626315789473684,"3. Theory Assumptions and Proofs
796"
THEORY ASSUMPTIONS AND PROOFS,0.8631578947368421,"Question: For each theoretical result, does the paper provide the full set of assumptions and
797"
THEORY ASSUMPTIONS AND PROOFS,0.8636842105263158,"a complete (and correct) proof?
798"
THEORY ASSUMPTIONS AND PROOFS,0.8642105263157894,"Answer: [Yes]
799"
THEORY ASSUMPTIONS AND PROOFS,0.8647368421052631,"Justification: [NA]
800"
THEORY ASSUMPTIONS AND PROOFS,0.8652631578947368,"Guidelines:
801"
THEORY ASSUMPTIONS AND PROOFS,0.8657894736842106,"• The answer NA means that the paper does not include theoretical results.
802"
THEORY ASSUMPTIONS AND PROOFS,0.8663157894736843,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
803"
THEORY ASSUMPTIONS AND PROOFS,0.8668421052631579,"referenced.
804"
THEORY ASSUMPTIONS AND PROOFS,0.8673684210526316,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
805"
THEORY ASSUMPTIONS AND PROOFS,0.8678947368421053,"• The proofs can either appear in the main paper or the supplemental material, but if
806"
THEORY ASSUMPTIONS AND PROOFS,0.868421052631579,"they appear in the supplemental material, the authors are encouraged to provide a short
807"
THEORY ASSUMPTIONS AND PROOFS,0.8689473684210526,"proof sketch to provide intuition.
808"
THEORY ASSUMPTIONS AND PROOFS,0.8694736842105263,"• Inversely, any informal proof provided in the core of the paper should be complemented
809"
THEORY ASSUMPTIONS AND PROOFS,0.87,"by formal proofs provided in appendix or supplemental material.
810"
THEORY ASSUMPTIONS AND PROOFS,0.8705263157894737,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8710526315789474,"4. Experimental Result Reproducibility
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.871578947368421,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
813"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8721052631578947,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
814"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8726315789473684,"of the paper (regardless of whether the code and data are provided or not)?
815"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8731578947368421,"Answer: [Yes]
816"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8736842105263158,"Justification: [NA]
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8742105263157894,"Guidelines:
818"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8747368421052631,"• The answer NA means that the paper does not include experiments.
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8752631578947369,"• If the paper includes experiments, a No answer to this question will not be perceived
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8757894736842106,"well by the reviewers: Making the paper reproducible is important, regardless of
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8763157894736842,"whether the code and data are provided or not.
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8768421052631579,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8773684210526316,"to make their results reproducible or verifiable.
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8778947368421053,"• Depending on the contribution, reproducibility can be accomplished in various ways.
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.878421052631579,"For example, if the contribution is a novel architecture, describing the architecture fully
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8789473684210526,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8794736842105263,"be necessary to either make it possible for others to replicate the model with the same
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.88,"dataset, or provide access to the model. In general. releasing code and data is often
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8805263157894737,"one good way to accomplish this, but reproducibility can also be provided via detailed
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8810526315789474,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.881578947368421,"of a large language model), releasing of a model checkpoint, or other means that are
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8821052631578947,"appropriate to the research performed.
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8826315789473684,"• While NeurIPS does not require releasing code, the conference does require all submis-
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8831578947368421,"sions to provide some reasonable avenue for reproducibility, which may depend on the
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8836842105263157,"nature of the contribution. For example
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8842105263157894,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
837"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8847368421052632,"to reproduce that algorithm.
838"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8852631578947369,"(b) If the contribution is primarily a new model architecture, the paper should describe
839"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8857894736842106,"the architecture clearly and fully.
840"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8863157894736842,"(c) If the contribution is a new model (e.g., a large language model), then there should
841"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8868421052631579,"either be a way to access this model for reproducing the results or a way to reproduce
842"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8873684210526316,"the model (e.g., with an open-source dataset or instructions for how to construct
843"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8878947368421053,"the dataset).
844"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.888421052631579,"(d) We recognize that reproducibility may be tricky in some cases, in which case
845"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8889473684210526,"authors are welcome to describe the particular way they provide for reproducibility.
846"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8894736842105263,"In the case of closed-source models, it may be that access to the model is limited in
847"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.89,"some way (e.g., to registered users), but it should be possible for other researchers
848"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8905263157894737,"to have some path to reproducing or verifying the results.
849"
OPEN ACCESS TO DATA AND CODE,0.8910526315789473,"5. Open access to data and code
850"
OPEN ACCESS TO DATA AND CODE,0.891578947368421,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
851"
OPEN ACCESS TO DATA AND CODE,0.8921052631578947,"tions to faithfully reproduce the main experimental results, as described in supplemental
852"
OPEN ACCESS TO DATA AND CODE,0.8926315789473684,"material?
853"
OPEN ACCESS TO DATA AND CODE,0.8931578947368422,"Answer: [No]
854"
OPEN ACCESS TO DATA AND CODE,0.8936842105263157,"Justification: Our code is based on Pytorch package which is standard. In addition, we
855"
OPEN ACCESS TO DATA AND CODE,0.8942105263157895,"have clarified the detailed experimental setup in our paper and the experiments are easy to
856"
OPEN ACCESS TO DATA AND CODE,0.8947368421052632,"reproduce.
857"
OPEN ACCESS TO DATA AND CODE,0.8952631578947369,"Guidelines:
858"
OPEN ACCESS TO DATA AND CODE,0.8957894736842106,"• The answer NA means that paper does not include experiments requiring code.
859"
OPEN ACCESS TO DATA AND CODE,0.8963157894736842,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
860"
OPEN ACCESS TO DATA AND CODE,0.8968421052631579,"public/guides/CodeSubmissionPolicy) for more details.
861"
OPEN ACCESS TO DATA AND CODE,0.8973684210526316,"• While we encourage the release of code and data, we understand that this might not be
862"
OPEN ACCESS TO DATA AND CODE,0.8978947368421053,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
863"
OPEN ACCESS TO DATA AND CODE,0.8984210526315789,"including code, unless this is central to the contribution (e.g., for a new open-source
864"
OPEN ACCESS TO DATA AND CODE,0.8989473684210526,"benchmark).
865"
OPEN ACCESS TO DATA AND CODE,0.8994736842105263,"• The instructions should contain the exact command and environment needed to run to
866"
OPEN ACCESS TO DATA AND CODE,0.9,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
867"
OPEN ACCESS TO DATA AND CODE,0.9005263157894737,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
868"
OPEN ACCESS TO DATA AND CODE,0.9010526315789473,"• The authors should provide instructions on data access and preparation, including how
869"
OPEN ACCESS TO DATA AND CODE,0.901578947368421,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
870"
OPEN ACCESS TO DATA AND CODE,0.9021052631578947,"• The authors should provide scripts to reproduce all experimental results for the new
871"
OPEN ACCESS TO DATA AND CODE,0.9026315789473685,"proposed method and baselines. If only a subset of experiments are reproducible, they
872"
OPEN ACCESS TO DATA AND CODE,0.9031578947368422,"should state which ones are omitted from the script and why.
873"
OPEN ACCESS TO DATA AND CODE,0.9036842105263158,"• At submission time, to preserve anonymity, the authors should release anonymized
874"
OPEN ACCESS TO DATA AND CODE,0.9042105263157895,"versions (if applicable).
875"
OPEN ACCESS TO DATA AND CODE,0.9047368421052632,"• Providing as much information as possible in supplemental material (appended to the
876"
OPEN ACCESS TO DATA AND CODE,0.9052631578947369,"paper) is recommended, but including URLs to data and code is permitted.
877"
OPEN ACCESS TO DATA AND CODE,0.9057894736842105,"6. Experimental Setting/Details
878"
OPEN ACCESS TO DATA AND CODE,0.9063157894736842,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
879"
OPEN ACCESS TO DATA AND CODE,0.9068421052631579,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
880"
OPEN ACCESS TO DATA AND CODE,0.9073684210526316,"results?
881"
OPEN ACCESS TO DATA AND CODE,0.9078947368421053,"Answer: [Yes]
882"
OPEN ACCESS TO DATA AND CODE,0.9084210526315789,"Justification: [NA]
883"
OPEN ACCESS TO DATA AND CODE,0.9089473684210526,"Guidelines:
884"
OPEN ACCESS TO DATA AND CODE,0.9094736842105263,"• The answer NA means that the paper does not include experiments.
885"
OPEN ACCESS TO DATA AND CODE,0.91,"• The experimental setting should be presented in the core of the paper to a level of detail
886"
OPEN ACCESS TO DATA AND CODE,0.9105263157894737,"that is necessary to appreciate the results and make sense of them.
887"
OPEN ACCESS TO DATA AND CODE,0.9110526315789473,"• The full details can be provided either with the code, in appendix, or as supplemental
888"
OPEN ACCESS TO DATA AND CODE,0.911578947368421,"material.
889"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9121052631578948,"7. Experiment Statistical Significance
890"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9126315789473685,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
891"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9131578947368421,"information about the statistical significance of the experiments?
892"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9136842105263158,"Answer: [Yes]
893"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9142105263157895,"Justification: [NA]
894"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9147368421052632,"Guidelines:
895"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9152631578947369,"• The answer NA means that the paper does not include experiments.
896"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9157894736842105,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
897"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9163157894736842,"dence intervals, or statistical significance tests, at least for the experiments that support
898"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9168421052631579,"the main claims of the paper.
899"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9173684210526316,"• The factors of variability that the error bars are capturing should be clearly stated (for
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9178947368421052,"example, train/test split, initialization, random drawing of some parameter, or overall
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9184210526315789,"run with given experimental conditions).
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9189473684210526,"• The method for calculating the error bars should be explained (closed form formula,
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9194736842105263,"call to a library function, bootstrap, etc.)
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.92,"• The assumptions made should be given (e.g., Normally distributed errors).
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9205263157894736,"• It should be clear whether the error bar is the standard deviation or the standard error
906"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9210526315789473,"of the mean.
907"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.921578947368421,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
908"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9221052631578948,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
909"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9226315789473685,"of Normality of errors is not verified.
910"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9231578947368421,"• For asymmetric distributions, the authors should be careful not to show in tables or
911"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9236842105263158,"figures symmetric error bars that would yield results that are out of range (e.g. negative
912"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9242105263157895,"error rates).
913"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9247368421052632,"• If error bars are reported in tables or plots, The authors should explain in the text how
914"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9252631578947368,"they were calculated and reference the corresponding figures or tables in the text.
915"
EXPERIMENTS COMPUTE RESOURCES,0.9257894736842105,"8. Experiments Compute Resources
916"
EXPERIMENTS COMPUTE RESOURCES,0.9263157894736842,"Question: For each experiment, does the paper provide sufficient information on the com-
917"
EXPERIMENTS COMPUTE RESOURCES,0.9268421052631579,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
918"
EXPERIMENTS COMPUTE RESOURCES,0.9273684210526316,"the experiments?
919"
EXPERIMENTS COMPUTE RESOURCES,0.9278947368421052,"Answer: [Yes]
920"
EXPERIMENTS COMPUTE RESOURCES,0.9284210526315789,"Justification: [NA]
921"
EXPERIMENTS COMPUTE RESOURCES,0.9289473684210526,"Guidelines:
922"
EXPERIMENTS COMPUTE RESOURCES,0.9294736842105263,"• The answer NA means that the paper does not include experiments.
923"
EXPERIMENTS COMPUTE RESOURCES,0.93,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
924"
EXPERIMENTS COMPUTE RESOURCES,0.9305263157894736,"or cloud provider, including relevant memory and storage.
925"
EXPERIMENTS COMPUTE RESOURCES,0.9310526315789474,"• The paper should provide the amount of compute required for each of the individual
926"
EXPERIMENTS COMPUTE RESOURCES,0.9315789473684211,"experimental runs as well as estimate the total compute.
927"
EXPERIMENTS COMPUTE RESOURCES,0.9321052631578948,"• The paper should disclose whether the full research project required more compute
928"
EXPERIMENTS COMPUTE RESOURCES,0.9326315789473684,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
929"
EXPERIMENTS COMPUTE RESOURCES,0.9331578947368421,"didn’t make it into the paper).
930"
CODE OF ETHICS,0.9336842105263158,"9. Code Of Ethics
931"
CODE OF ETHICS,0.9342105263157895,"Question: Does the research conducted in the paper conform, in every respect, with the
932"
CODE OF ETHICS,0.9347368421052632,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
933"
CODE OF ETHICS,0.9352631578947368,"Answer: [Yes]
934"
CODE OF ETHICS,0.9357894736842105,"Justification: [NA]
935"
CODE OF ETHICS,0.9363157894736842,"Guidelines:
936"
CODE OF ETHICS,0.9368421052631579,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
937"
CODE OF ETHICS,0.9373684210526316,"• If the authors answer No, they should explain the special circumstances that require a
938"
CODE OF ETHICS,0.9378947368421052,"deviation from the Code of Ethics.
939"
CODE OF ETHICS,0.9384210526315789,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
940"
CODE OF ETHICS,0.9389473684210526,"eration due to laws or regulations in their jurisdiction).
941"
BROADER IMPACTS,0.9394736842105263,"10. Broader Impacts
942"
BROADER IMPACTS,0.94,"Question: Does the paper discuss both potential positive societal impacts and negative
943"
BROADER IMPACTS,0.9405263157894737,"societal impacts of the work performed?
944"
BROADER IMPACTS,0.9410526315789474,"Answer: [NA]
945"
BROADER IMPACTS,0.9415789473684211,"Justification: [NA]
946"
BROADER IMPACTS,0.9421052631578948,"Guidelines:
947"
BROADER IMPACTS,0.9426315789473684,"• The answer NA means that there is no societal impact of the work performed.
948"
BROADER IMPACTS,0.9431578947368421,"• If the authors answer NA or No, they should explain why their work has no societal
949"
BROADER IMPACTS,0.9436842105263158,"impact or why the paper does not address societal impact.
950"
BROADER IMPACTS,0.9442105263157895,"• Examples of negative societal impacts include potential malicious or unintended uses
951"
BROADER IMPACTS,0.9447368421052632,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
952"
BROADER IMPACTS,0.9452631578947368,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
953"
BROADER IMPACTS,0.9457894736842105,"groups), privacy considerations, and security considerations.
954"
BROADER IMPACTS,0.9463157894736842,"• The conference expects that many papers will be foundational research and not tied
955"
BROADER IMPACTS,0.9468421052631579,"to particular applications, let alone deployments. However, if there is a direct path to
956"
BROADER IMPACTS,0.9473684210526315,"any negative applications, the authors should point it out. For example, it is legitimate
957"
BROADER IMPACTS,0.9478947368421052,"to point out that an improvement in the quality of generative models could be used to
958"
BROADER IMPACTS,0.9484210526315789,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
959"
BROADER IMPACTS,0.9489473684210527,"that a generic algorithm for optimizing neural networks could enable people to train
960"
BROADER IMPACTS,0.9494736842105264,"models that generate Deepfakes faster.
961"
BROADER IMPACTS,0.95,"• The authors should consider possible harms that could arise when the technology is
962"
BROADER IMPACTS,0.9505263157894737,"being used as intended and functioning correctly, harms that could arise when the
963"
BROADER IMPACTS,0.9510526315789474,"technology is being used as intended but gives incorrect results, and harms following
964"
BROADER IMPACTS,0.9515789473684211,"from (intentional or unintentional) misuse of the technology.
965"
BROADER IMPACTS,0.9521052631578948,"• If there are negative societal impacts, the authors could also discuss possible mitigation
966"
BROADER IMPACTS,0.9526315789473684,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
967"
BROADER IMPACTS,0.9531578947368421,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
968"
BROADER IMPACTS,0.9536842105263158,"feedback over time, improving the efficiency and accessibility of ML).
969"
SAFEGUARDS,0.9542105263157895,"11. Safeguards
970"
SAFEGUARDS,0.9547368421052631,"Question: Does the paper describe safeguards that have been put in place for responsible
971"
SAFEGUARDS,0.9552631578947368,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
972"
SAFEGUARDS,0.9557894736842105,"image generators, or scraped datasets)?
973"
SAFEGUARDS,0.9563157894736842,"Answer: [NA]
974"
SAFEGUARDS,0.9568421052631579,"Justification: [NA]
975"
SAFEGUARDS,0.9573684210526315,"Guidelines:
976"
SAFEGUARDS,0.9578947368421052,"• The answer NA means that the paper poses no such risks.
977"
SAFEGUARDS,0.958421052631579,"• Released models that have a high risk for misuse or dual-use should be released with
978"
SAFEGUARDS,0.9589473684210527,"necessary safeguards to allow for controlled use of the model, for example by requiring
979"
SAFEGUARDS,0.9594736842105264,"that users adhere to usage guidelines or restrictions to access the model or implementing
980"
SAFEGUARDS,0.96,"safety filters.
981"
SAFEGUARDS,0.9605263157894737,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
982"
SAFEGUARDS,0.9610526315789474,"should describe how they avoided releasing unsafe images.
983"
SAFEGUARDS,0.9615789473684211,"• We recognize that providing effective safeguards is challenging, and many papers do
984"
SAFEGUARDS,0.9621052631578947,"not require this, but we encourage authors to take this into account and make a best
985"
SAFEGUARDS,0.9626315789473684,"faith effort.
986"
LICENSES FOR EXISTING ASSETS,0.9631578947368421,"12. Licenses for existing assets
987"
LICENSES FOR EXISTING ASSETS,0.9636842105263158,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
988"
LICENSES FOR EXISTING ASSETS,0.9642105263157895,"the paper, properly credited and are the license and terms of use explicitly mentioned and
989"
LICENSES FOR EXISTING ASSETS,0.9647368421052631,"properly respected?
990"
LICENSES FOR EXISTING ASSETS,0.9652631578947368,"Answer: [NA]
991"
LICENSES FOR EXISTING ASSETS,0.9657894736842105,"Justification: [NA]
992"
LICENSES FOR EXISTING ASSETS,0.9663157894736842,"Guidelines:
993"
LICENSES FOR EXISTING ASSETS,0.966842105263158,"• The answer NA means that the paper does not use existing assets.
994"
LICENSES FOR EXISTING ASSETS,0.9673684210526315,"• The authors should cite the original paper that produced the code package or dataset.
995"
LICENSES FOR EXISTING ASSETS,0.9678947368421053,"• The authors should state which version of the asset is used and, if possible, include a
996"
LICENSES FOR EXISTING ASSETS,0.968421052631579,"URL.
997"
LICENSES FOR EXISTING ASSETS,0.9689473684210527,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
998"
LICENSES FOR EXISTING ASSETS,0.9694736842105263,"• For scraped data from a particular source (e.g., website), the copyright and terms of
999"
LICENSES FOR EXISTING ASSETS,0.97,"service of that source should be provided.
1000"
LICENSES FOR EXISTING ASSETS,0.9705263157894737,"• If assets are released, the license, copyright information, and terms of use in the
1001"
LICENSES FOR EXISTING ASSETS,0.9710526315789474,"package should be provided. For popular datasets, paperswithcode.com/datasets
1002"
LICENSES FOR EXISTING ASSETS,0.9715789473684211,"has curated licenses for some datasets. Their licensing guide can help determine the
1003"
LICENSES FOR EXISTING ASSETS,0.9721052631578947,"license of a dataset.
1004"
LICENSES FOR EXISTING ASSETS,0.9726315789473684,"• For existing datasets that are re-packaged, both the original license and the license of
1005"
LICENSES FOR EXISTING ASSETS,0.9731578947368421,"the derived asset (if it has changed) should be provided.
1006"
LICENSES FOR EXISTING ASSETS,0.9736842105263158,"• If this information is not available online, the authors are encouraged to reach out to
1007"
LICENSES FOR EXISTING ASSETS,0.9742105263157895,"the asset’s creators.
1008"
NEW ASSETS,0.9747368421052631,"13. New Assets
1009"
NEW ASSETS,0.9752631578947368,"Question: Are new assets introduced in the paper well documented and is the documentation
1010"
NEW ASSETS,0.9757894736842105,"provided alongside the assets?
1011"
NEW ASSETS,0.9763157894736842,"Answer: [NA]
1012"
NEW ASSETS,0.9768421052631578,"Justification: [NA]
1013"
NEW ASSETS,0.9773684210526316,"Guidelines:
1014"
NEW ASSETS,0.9778947368421053,"• The answer NA means that the paper does not release new assets.
1015"
NEW ASSETS,0.978421052631579,"• Researchers should communicate the details of the dataset/code/model as part of their
1016"
NEW ASSETS,0.9789473684210527,"submissions via structured templates. This includes details about training, license,
1017"
NEW ASSETS,0.9794736842105263,"limitations, etc.
1018"
NEW ASSETS,0.98,"• The paper should discuss whether and how consent was obtained from people whose
1019"
NEW ASSETS,0.9805263157894737,"asset is used.
1020"
NEW ASSETS,0.9810526315789474,"• At submission time, remember to anonymize your assets (if applicable). You can either
1021"
NEW ASSETS,0.9815789473684211,"create an anonymized URL or include an anonymized zip file.
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821052631578947,"14. Crowdsourcing and Research with Human Subjects
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826315789473684,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831578947368421,"include the full text of instructions given to participants and screenshots, if applicable, as
1025"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836842105263158,"well as details about compensation (if any)?
1026"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842105263157894,"Answer: [NA]
1027"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847368421052631,"Justification: [NA]
1028"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852631578947368,"Guidelines:
1029"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857894736842105,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1030"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863157894736843,"human subjects.
1031"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868421052631579,"• Including this information in the supplemental material is fine, but if the main contribu-
1032"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873684210526316,"tion of the paper involves human subjects, then as much detail as possible should be
1033"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878947368421053,"included in the main paper.
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988421052631579,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889473684210527,"or other labor should be paid at least the minimum wage in the country of the data
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894736842105263,"collector.
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905263157894737,"Subjects
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910526315789474,"Question: Does the paper describe potential risks incurred by study participants, whether
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991578947368421,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921052631578947,"approvals (or an equivalent approval/review based on the requirements of your country or
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926315789473684,"institution) were obtained?
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931578947368421,"Answer: [NA]
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936842105263158,"Justification: [NA]
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942105263157894,"Guidelines:
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947368421052631,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952631578947368,"human subjects.
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957894736842106,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963157894736843,"may be required for any human subjects research. If you obtained IRB approval, you
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968421052631579,"should clearly state this in the paper.
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973684210526316,"• We recognize that the procedures for this may vary significantly between institutions
1052"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978947368421053,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1053"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998421052631579,"guidelines for their institution.
1054"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989473684210526,"• For initial submissions, do not include any information that would break anonymity (if
1055"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9994736842105263,"applicable), such as the institution conducting the review.
1056"
