Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014619883040935672,"Modern ML applications increasingly rely on complex deep learning models and
1"
ABSTRACT,0.0029239766081871343,"large datasets. There has been an exponential growth in the amount of computa-
2"
ABSTRACT,0.0043859649122807015,"tion needed to train the largest models. Therefore, to scale computation and data,
3"
ABSTRACT,0.005847953216374269,"these models are inevitably trained in a distributed manner in clusters of nodes,
4"
ABSTRACT,0.007309941520467836,"and their updates are aggregated before being applied to the model. However, a
5"
ABSTRACT,0.008771929824561403,"distributed setup is prone to Byzantine failures of individual nodes, components,
6"
ABSTRACT,0.01023391812865497,"and software. With data augmentation added to these settings, there is a critical
7"
ABSTRACT,0.011695906432748537,"need for robust and efficient aggregation systems. We define the quality of workers
8"
ABSTRACT,0.013157894736842105,"as reconstruction ratios ∈(0, 1], and formulate aggregation as a Maximum Like-
9"
ABSTRACT,0.014619883040935672,"lihood Estimation procedure using Beta densities. We show that the Regularized
10"
ABSTRACT,0.01608187134502924,"form of log-likelihood wrt subspace can be approximately solved using iterative
11"
ABSTRACT,0.017543859649122806,"least squares solver, and provide convergence guarantees using recent Convex
12"
ABSTRACT,0.019005847953216373,"Optimization landscape results. Our empirical findings demonstrate that our ap-
13"
ABSTRACT,0.02046783625730994,"proach significantly enhances the robustness of state-of-the-art Byzantine resilient
14"
ABSTRACT,0.021929824561403508,"aggregators. We evaluate our method in a distributed setup with a parameter server,
15"
ABSTRACT,0.023391812865497075,"and show simultaneous improvements in communication efficiency and accuracy
16"
ABSTRACT,0.024853801169590642,"across various tasks.
17"
INTRODUCTION,0.02631578947368421,"1
Introduction
18"
INTRODUCTION,0.027777777777777776,"How to Design Aggregators? We consider the problem of designing aggregation functions that can
19"
INTRODUCTION,0.029239766081871343,"be written as optimization problems of the form,
20"
INTRODUCTION,0.03070175438596491,"A(g1, . . . , gp) ∈arg min
Y ∈C Ag1,...,gp(Y ),
(1)"
INTRODUCTION,0.03216374269005848,"where {gi}p
i=1 ⊆Rn are given estimates of an unknown summary statistic used to compute the
21"
INTRODUCTION,0.033625730994152045,"Aggregator Y ∗. If we choose A to be a quadratic function that decomposes over gi’s, and C = Rn,
22"
INTRODUCTION,0.03508771929824561,"then we can see A is simply the standard mean operator. There is a mature literature of studying such
23"
INTRODUCTION,0.03654970760233918,"functions for various scientific computing applications [1]. More recently, from the machine learning
24"
INTRODUCTION,0.038011695906432746,"standpoint there has been a plethora of work [2, 3, 4, 5] on designing provably robust aggregators A
25"
INTRODUCTION,0.039473684210526314,"for mean estimation tasks under various technical assumptions on the distribution or moments of gi.
26"
INTRODUCTION,0.04093567251461988,"Distributed ML Use Cases. Consider training a model with a large dataset such as ImageNet-1K
27"
INTRODUCTION,0.04239766081871345,"[6] or its augmented version which would require data to be distributed over p workers and uses
28"
INTRODUCTION,0.043859649122807015,"back propagation. Indeed, in this case, gi’s are typically the gradients computed by individual
29"
INTRODUCTION,0.04532163742690058,"workers at each iteration. In settings where the training objective is convex, the convergence and
30"
INTRODUCTION,0.04678362573099415,"generalization properties of distributed optimization can be achieved by defining A as a weighted
31"
INTRODUCTION,0.04824561403508772,"combination of gradients facilitated by a simple consensus matrix, even if some gi’s are noisy [7, 8].
32"
INTRODUCTION,0.049707602339181284,"In a distributed setup, as long as the model is convex we can simultaneously minimize the total
33"
INTRODUCTION,0.05116959064327485,"iteration or communication complexity to a significant extent i.e., it is possible to achieve convergence
34 …"
INTRODUCTION,0.05263157894736842,"𝐺!:
𝑤!× …"
INTRODUCTION,0.054093567251461985,"𝑈∈𝑅""×""
Σ ∈𝑅""×$"
INTRODUCTION,0.05555555555555555,𝑉% ∈𝑅$×$
INTRODUCTION,0.05701754385964912,"𝑌= 𝑈[: , 1: 𝑚]"
INTRODUCTION,0.05847953216374269,"𝐺= 𝐺!|𝐺&| ⋯|𝐺"" ×
×"
INTRODUCTION,0.059941520467836254,"𝑌
1
𝑝𝑌𝑌%𝐺1
𝑑"
INTRODUCTION,0.06140350877192982,"𝐺'(,
1 ≤𝑖≤𝑝,
1 ≤𝑗≤𝑛 SVD"
INTRODUCTION,0.06286549707602339,"∗5
𝑔!! 𝑔!""
𝑔!#"
INTRODUCTION,0.06432748538011696,"𝑔""! 𝑔""""
𝑔""#"
INTRODUCTION,0.06578947368421052,"𝑔$! 𝑔$""
𝑔$# … … … 𝑔!(𝑡) 𝑔""(𝑡) 𝑔#(𝑡) … …"
INTRODUCTION,0.06725146198830409,𝑔!(𝑡+ 1)
INTRODUCTION,0.06871345029239766,"𝑔""(𝑡+ 1)"
INTRODUCTION,0.07017543859649122,𝑔#(𝑡+ 1) … …
INTRODUCTION,0.07163742690058479,"𝑑)*!
𝑑)"
INTRODUCTION,0.07309941520467836,"𝑔'
1 ≤𝑖≤𝑝"
INTRODUCTION,0.07456140350877193,Left singular
INTRODUCTION,0.07602339181286549,vectors
INTRODUCTION,0.07748538011695906,iteration 𝑡
INTRODUCTION,0.07894736842105263,Augmented Data
INTRODUCTION,0.0804093567251462,"Stable Diffusion
𝑔! + 𝑛, 𝑛~𝑁(0, 𝜎""𝐼)"
INTRODUCTION,0.08187134502923976,"𝑔! + 𝑛, 𝑛~𝑁(0, 𝑊)"
INTRODUCTION,0.08333333333333333,Augmented Data
INTRODUCTION,0.0847953216374269,"Stable Diffusion
𝑔! + 𝑛, 𝑛~𝑁(0, 𝜎""𝐼)"
INTRODUCTION,0.08625730994152046,"𝑔! + 𝑛, 𝑛~𝑁(0, 𝑊)
iteration 𝑡+ 1"
INTRODUCTION,0.08771929824561403,Right singular
INTRODUCTION,0.0891812865497076,"vectors
Singular"
INTRODUCTION,0.09064327485380116,"values
Concatenated 
gradient matrix
Gradients from"
INTRODUCTION,0.09210526315789473,"workers
Weights for"
INTRODUCTION,0.0935672514619883,"workers’ 
gradient 
subspaces"
INTRODUCTION,0.09502923976608187,"𝐺&:
𝑤&×"
INTRODUCTION,0.09649122807017543,"𝐺"":
𝑤""× …"
INTRODUCTION,0.097953216374269,Flag Aggregator
INTRODUCTION,0.09941520467836257,Estimate Subspace for Aggregation
INTRODUCTION,0.10087719298245613,"Figure 1: Robust gradient aggregation in our distributed training framework. In our applications, each of
the p workers provides gradients computed using a random sample obtained from given training data, derived
synthetic data from off-the-shelf Diffusion models, and random noise in each iteration. Our Flag Aggregator
(FA) removes high frequency noise components by using few rounds of Singular Value Decomposition of the
concatenated Gradient Matrix G, and provides new update Y ∗."
INTRODUCTION,0.1023391812865497,"and robustness under technical assumptions on the moments of (unknown) distribution from which
35"
INTRODUCTION,0.10380116959064327,"gi’s are drawn. However, it is still an open problem to determine the optimality of these procedures
36"
INTRODUCTION,0.10526315789473684,"in terms of either convergence or robustness [9, 10].
37"
INTRODUCTION,0.1067251461988304,"Potential Causes of Noise. When data is distributed among workers, hardware and software failures
38"
INTRODUCTION,0.10818713450292397,"in workers [11, 12, 13] can cause them to send incorrect gradients, which can significantly mislead
39"
INTRODUCTION,0.10964912280701754,"the model [14]. To see this, let’s consider a simple experiment with 15 workers, that f of them
40"
INTRODUCTION,0.1111111111111111,"produce uniformly random gradients. Figure 2 shows that the model accuracy is heavily impacted
41"
INTRODUCTION,0.11257309941520467,"when f > 0 when mean is used to aggregate the gradients.
42"
INTRODUCTION,0.11403508771929824,"0
10
20
30
40
50
Epoch 10 20 30 40 50 60"
INTRODUCTION,0.1154970760233918,Top-1 Accuracy (%)
INTRODUCTION,0.11695906432748537,"Mean, f = 0
Mean, f = 1
Mean, f = 2
Mean, f = 3"
INTRODUCTION,0.11842105263157894,"Figure 2:
Tolerance to f
Byzantine workers for a non-
robust aggregator (mean)."
INTRODUCTION,0.11988304093567251,"The failures can occur due to component or software failures and
43"
INTRODUCTION,0.12134502923976608,"their probability increases with the scale of the system [15, 16, 17].
44"
INTRODUCTION,0.12280701754385964,"Reliability theory is used to analyze such failures, see Chapter 9
45"
INTRODUCTION,0.12426900584795321,"in [18], but for large-scale training, the distribution of total system
46"
INTRODUCTION,0.12573099415204678,"failures is not independent over workers, making the total noise in
47"
INTRODUCTION,0.12719298245614036,"gradients dependent and a key challenge for large-scale training.
48"
INTRODUCTION,0.1286549707602339,"Moreover, even if there are no issues with the infrastructure, our
49"
INTRODUCTION,0.1301169590643275,"work is motivated by the prevalence of data augmentation, including
50"
INTRODUCTION,0.13157894736842105,"hand-chosen augmentations. Since number of parameters n is often
51"
INTRODUCTION,0.13304093567251463,"greater than number of samples, data augmentation improves the
52"
INTRODUCTION,0.13450292397660818,"generalization capabilities of large-scale models under technical con-
53"
INTRODUCTION,0.13596491228070176,"ditions [19, 20, 21]. In particular, Adversarial training is a common
54"
INTRODUCTION,0.13742690058479531,"technique that finds samples that are close to training samples but
55"
INTRODUCTION,0.1388888888888889,"classified as a different class at the current set of parameters, and
56"
INTRODUCTION,0.14035087719298245,"then use such samples for parameter update purposes [22]. Unfortunately, computing adversarial
57"
INTRODUCTION,0.14181286549707603,"samples is often difficult [23], done using randomized algorithms [24] and so may introduce depen-
58"
INTRODUCTION,0.14327485380116958,"dent (across samples) noise themselves. In other words, using adversarial training paradigm, or the
59"
INTRODUCTION,0.14473684210526316,"so-called inner optimization can lead to noise in gradients, which can cause or simulate dependent
60"
INTRODUCTION,0.14619883040935672,"“Byzantine” failures in the distributed context.
61"
INTRODUCTION,0.1476608187134503,"Available Computational Solutions. Most existing open source implementations of A rely just
62"
INTRODUCTION,0.14912280701754385,"on (functions of) pairwise distances to filter gradients from workers using suitable neighborhood
63"
INTRODUCTION,0.15058479532163743,"based thresholding schemes, based on moment conditions [25, 26, 27]. While these may be a good
64"
INTRODUCTION,0.15204678362573099,"strategy when the noise in samples/gradients is somewhat independent, these methods are suboptimal
65"
INTRODUCTION,0.15350877192982457,"when the noise is dependent or nonlinear, especially when n is large. Moreover, choosing discrete
66"
INTRODUCTION,0.15497076023391812,"hyperparameters such as number of neighbors is impractical in our use cases since they hamper
67"
INTRODUCTION,0.1564327485380117,"convergence of the overall training procedure. To mitigate the suboptimality of existing aggregation
68"
INTRODUCTION,0.15789473684210525,"schemes, we explicitly estimate a subspace Y spanned by “most” of the gradient workers, and then
69"
INTRODUCTION,0.15935672514619884,"use this subspace to estimate that a sparse linear combination of gi gradients, acheiving robustness.
70"
INTRODUCTION,0.1608187134502924,"We present a new optimization based formulation for generalized gradient aggregation purposes in
71"
INTRODUCTION,0.16228070175438597,"the context of distributed training of deep learning architectures, as shown in Figure 1.
72"
INTRODUCTION,0.16374269005847952,"Summary of our Contributions. From the theoretical perspective, we present a simple Maximum
73"
INTRODUCTION,0.1652046783625731,"Likelihood Based estimation procedure for aggregation purposes, with novel regularization functions.
74"
INTRODUCTION,0.16666666666666666,"Algorithmically, we argue that any procedure used to solve Flag Optimization can be directly used to
75"
INTRODUCTION,0.16812865497076024,"obtain the optimal summary statistic Y ∗for our aggregation purposes. Experimentally, our results
76"
INTRODUCTION,0.1695906432748538,"show resilience against Byzantine attacks, encompassing physical failures, while effectively managing
77"
INTRODUCTION,0.17105263157894737,"the stochasticity arising from data augmentation schemes. In practice, we achieve a significantly
78"
INTRODUCTION,0.17251461988304093,"(≈20%) better accuracy on standard datasets. Our implementation offers substantial advantages in
79"
INTRODUCTION,0.1739766081871345,"reducing communication complexity across diverse noise settings through the utilization of our novel
80"
INTRODUCTION,0.17543859649122806,"aggregation function, making it applicable in numerous scenarios.
81"
ROBUST AGGREGATORS AS ORTHOGONALITY CONSTRAINED OPTIMIZATION,0.17690058479532164,"2
Robust Aggregators as Orthogonality Constrained Optimization
82"
ROBUST AGGREGATORS AS ORTHOGONALITY CONSTRAINED OPTIMIZATION,0.1783625730994152,"In this section, we first provide the basic intuition of our proposed approach to using subspaces for
83"
ROBUST AGGREGATORS AS ORTHOGONALITY CONSTRAINED OPTIMIZATION,0.17982456140350878,"aggregation purposes using linear algebra, along with connections of our approach standard eigende-
84"
ROBUST AGGREGATORS AS ORTHOGONALITY CONSTRAINED OPTIMIZATION,0.18128654970760233,"composition based denoising approaches. We then present our overall optimization formulation in
85"
ROBUST AGGREGATORS AS ORTHOGONALITY CONSTRAINED OPTIMIZATION,0.1827485380116959,"two steps, and argue that it can be optimized using existing methods.
86"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.18421052631578946,"2.1
Optimal Subspace Hypothesis for Distributed Descent
87"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.18567251461988304,"We will use lowercase letters y, g to denote vectors, and uppercase letters Y, G to denote ma-
88"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.1871345029239766,"trices.
We will use boldfont 1 to denote the vector of all ones in appropriate dimensions.
89"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.18859649122807018,"0.0
0.2
0.4
0.6
0.8
1.0
Value of Workers (v) 0 250 500 750 1000 1250 1500 1750 2000"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.19005847953216373,Frequency
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.1915204678362573,"Optimal
Subspace
Suboptimal
Subspace"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.19298245614035087,"Figure 3: Distributions of Ex-
plained Variances on Minibatches"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.19444444444444445,"Let gi ∈Rn is the gradient vector from worker i, and Y ∈Rn×m
90"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.195906432748538,"is an orthogonal matrix representation of a subspace that gradients
91"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.19736842105263158,"could live in such that m ≤p. Now, we may interpret each column
92"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.19883040935672514,"of Y as a basis function that act on gi ∈Rn, i.e., j−th coordinate of
93"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20029239766081872,"(Y T g)j for 1 ≤j ≤m is the application of j−th basis or column
94"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20175438596491227,"of Y on g. Recall that by definition of dot product, we have that
95"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20321637426900585,"if Y:,j ⊥x, then (Y T g)j will be close to zero. Equivalently, if
96"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2046783625730994,"g ∈span(Y ), then (Y T g)T Y T g will be bounded away from zero,
97"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20614035087719298,"see Chapter 2 in [28]. Assuming that G ∈Rn×p is the gradient
98"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20760233918128654,"matrix of p workers, Y Y T G ∈Rn×p is the reconstruction of G
99"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.20906432748538012,"using Y as basis. That is, ith column of Y T G specifies the amount
100"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.21052631578947367,"of gradient from worker i as a function of Y , and high l2 norm of
101"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.21198830409356725,"Y T gi implies that there is a basis in Y such that Y ̸⊥gi. So it is
102"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2134502923976608,"easy to see that the average over columns of Y Y T G would give the final gradient for update.
103"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2149122807017544,"Explained Variance of worker i. If we denote zi = Y T gi ∈Rm representing the transformation
104"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.21637426900584794,"of gradient gi to zi using Y , then, 0 ≤∥zi∥2
2 = zT
i zi = (Y T g)T Y T g = gT
i Y Y T gi is a scalar,
105"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.21783625730994152,"and so is equal to its trace tr
 
gT
i Y Y T gi

. Moreover, when Y is orthogonal, we have 0 ≤∥zi∥2 =
106"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.21929824561403508,"∥Y T gi∥2 ≤∥Y ∥2∥gi∥2 ≤∥gi∥2 since the operator norm (or largest singular value) ∥Y ∥2 of Y is at
107"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.22076023391812866,"most 1. Our main idea is to use ∥zi∥2
2, ∥gi∥2
2 to define the quality of the subspace Y for aggregation,
108"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2222222222222222,"as is done in some previous works for Robust Principal Component Estimation [29] – the quantity
109"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2236842105263158,"∥zi∥2
2/∥gi∥2
2 is called as Explained/Expressed variance of subspace Y wrt i−th worker [30, 31] – we
110"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.22514619883040934,"refer to ∥zi∥2
2/∥gi∥2
2 as the “value” of i−th worker. In Figure 3, we can see from the spike near 1.0
111"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.22660818713450293,"that if we choose the subspace carefully (blue) as opposed to merely choosing the mean gradient
112"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.22807017543859648,"(with unit norm) of all workers, then we can increase the value of workers.
113"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.22953216374269006,"Advantages of Subspace based Aggregation. We can see that using subspace Y , we can easily: 1.
114"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2309941520467836,"handle different number of gradients from each worker, 2. compute gradient reconstruction Y Y T G
115"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2324561403508772,efficiently whenever Y is constrained to be orthogonal Y = P
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.23391812865497075,"i yiyT
i where yi is the i−th column
116"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.23538011695906433,"of Y , otherwise have to use eigendecomposition of Y to measure explained variance which can
117"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.23684210526315788,"be time consuming. In (practical) distributed settings, the quality (or noise level) of gradients in
118"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.23830409356725146,"each worker may be different, and/or each worker may use a different batch size. In such cases,
119"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.23976608187134502,"handcrafted aggregation schemes may be difficult to maintain, and fine-tune. For these purposes with
120"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2412280701754386,"an Orthogonal Subspace Y , we can simply reweigh gradients of worker i according to its noise level,
121"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.24269005847953215,"and/or use gi ∈Rn×bi where bi is the batch size of i−th worker with tr(zT
i zi) instead.
122"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.24415204678362573,"Why is optimizing over subspaces called “Flag” Optimization? Recent optimization results
123"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.24561403508771928,"suggest that we can exploit the finer structure available in Flag Manifold to specify Y more precisely
124"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.24707602339181287,"[32]. For example, Y ∈Rm×n can be parametrized directly as a subspace of dimension m or
125"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.24853801169590642,"as a nested sequence of Yk ∈Rmk×n, k = 1, ..., K where mk < mk+1 ≤p ≤n such that
126"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.25,"span(Yk) ⊆span(Yk+1) with YK ∈Rm×n. When mk+1 = mk = 1, we have the usual (real)
127"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.25146198830409355,"Grassmanian Manifold (quotient of orthogonal group) whose coordinates can be used for optimization,
128"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.25292397660818716,"please see Section 5 in [33] for details. In fact, [34] used this idea to extend median in one-dimensional
129"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2543859649122807,"vector spaces to different finite dimensional subspaces using the so-called chordal distance between
130"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.25584795321637427,"them. In our distributed training context, we use the explained variance of each worker instead. Here,
131"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.2573099415204678,"workers may specify dimensions along which gradient information is relevant for faster convergence
132"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.25877192982456143,"– an advantage currently not available in existing aggregation implementations – which may be used
133"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.260233918128655,"for smart initialization also. We use “Flag” to emphasize this additional nested structure available in
134"
OPTIMAL SUBSPACE HYPOTHESIS FOR DISTRIBUTED DESCENT,0.26169590643274854,"our formulation for distributed training purposes.
135"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2631578947368421,"2.2
Approximate Maximum Likelihood Estimation of Optimal Subspace
136"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2646198830409357,"Now that we can evaluate a subspace Y on individual gradients gi, we now show that finding subspace
137"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.26608187134502925,"Y can be formulated using standard maximum likelihood estimation principles [35]. Our formulation
138"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2675438596491228,"reveals that regularization is critical for aggregation especially in distributed training. In order to
139"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.26900584795321636,"write down the objective function for finding optimal Y , we proceed in the following two steps:
140"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.27046783625730997,"Step 1. Assume that each worker provides a single gradient for simplicity. Now, denoting the value of
141"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2719298245614035,"information v of worker i by vi = zT
i zi
gT
i gi , we have vi ∈[0, 1]. Now by assuming that vi’s are observed
142"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2733918128654971,from Beta distribution with α = 1 and β = 1
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.27485380116959063,"2 (for simplicity), we can see that the likelihood P(vi) is,
143"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.27631578947368424,P(vi) := (1 −vi)−1 2
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2777777777777778,"B(1, 1 2)
="
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.27923976608187134,"
1 −zT
i zi
gT
i gi −1 2"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2807017543859649,"B(1, 1"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2821637426900585,"2)
,
(2)"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.28362573099415206,"where B(a, b) is the normalization constant. Then, the total log-likelihood of observing gradients gi
144"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2850877192982456,"as a function of Y (or vi’s) is given by taking the log of product of P(vi)’s as (ignoring constants),
145 log p
Y"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.28654970760233917,"i=1
P(vi) ! = p
X"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2880116959064328,"i=1
log (P(vi)) = −1 2 p
X"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2894736842105263,"i=1
log(1 −vi).
(3)"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.2909356725146199,"Step 2. Now we use Taylor’s series with constant a > 0 to approximate individual worker log-
146"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.29239766081871343,"likelihoods log(1 −vi) ≈a(1 −vi)
1
a −a as follows: first, we know that exp

log(vi)"
APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION OF OPTIMAL SUBSPACE,0.29385964912280704,"a

= v"
A,0.2953216374269006,"1
a
i . On
147"
A,0.29678362573099415,"the other hand, using Taylor expansion of exp about the origin (so large a > 1 is better), we have that
148"
A,0.2982456140350877,"exp

log(vi)"
A,0.2997076023391813,"a

≈1 + log(vi)"
A,0.30116959064327486,"a
. Whence, we have that 1 + log(vi) a
≈v"
"A
I WHICH IMMEDIATELY IMPLIES",0.3026315789473684,"1
a
i which immediately implies
149"
"A
I WHICH IMMEDIATELY IMPLIES",0.30409356725146197,that log(vi) ≈av
A,0.3055555555555556,"1
a
i −a. So, by substituting the Taylor series approximation of log in Equation 3, we
150"
A,0.30701754385964913,"obtain the negative log-likelihood approximation to be minimized for robust aggregation purposes as,
151 −log p
Y"
A,0.3084795321637427,"i=1
P(vi) ! ≈1 2 p
X i=1"
A,0.30994152046783624,"
a (1 −vi)"
A,0.31140350877192985,"1
a −a

,
(4)"
A,0.3128654970760234,"where a > 1 is a sufficiently large constant. In the above mentioned steps, the first step is standard.
152"
A,0.31432748538011696,"Our key insight is using Taylor expansion in (4) with a sufficiently large a to eliminate log optimization
153"
A,0.3157894736842105,"which are known to be computationally expensive to solve, and instead solve smooth ℓa, a > 1 norm
154"
A,0.3172514619883041,"based optimization problems which can be done efficiently by modifying existing procedures [36].
155"
A,0.31871345029239767,"Extension to general beta distributions, and gradients α > 0, β > 0, gi ∈Rn×k. Note that our
156"
A,0.3201754385964912,"derivation in the above two steps can be extended to any beta shape parameters α > 0, β > 0 – there
157"
A,0.3216374269005848,"will be two terms in the final negative log-likelihood expression in our formulation (4), one for each
158"
A,0.3230994152046784,"α, β. Similarly, by simply using vi = tr
 
gT
i Y Y T gi

to define value of worker i in equation (2), and
159"
A,0.32456140350877194,"then in our estimator in (4), we can easily handle multiple k gradients from a single worker i for Y .
160"
A,0.3260233918128655,"Algorithm 1 Distributed SGD with proposed Flag Aggregator (FA) at the Parameter Server
Input: Number of workers p, loss functions l1, l2, ..., lp, per-worker minibatch size B, learning rate
schedule αt, initial parameters w0, number of iterations T
Output: Updated parameters wT from any worker"
A,0.32748538011695905,1 for t = 1 to T do
A,0.32894736842105265,"2
for p = 1 to p in parallel on machine p do"
A,0.3304093567251462,"3
Select a minibatch: ip,1,t, ip,2,t,...,ip,B,t
gp,t ←1"
A,0.33187134502923976,"B
PB
b=1 ∇lip,b,t(wt−1)"
A,0.3333333333333333,"4
Gt ←{g1,t, · · · , gp,t} // Parameter Server receives gradients from p workers"
A,0.3347953216374269,"5
ˆYt ←IRLS( ˆGt) with ˆGt = Gt + λ∇R(Y )1T // Do IRLS at the Parameter Server for ˆY"
A,0.3362573099415205,"6
Obtain gradient direction dt: dt = 1"
A,0.33771929824561403,"p ˆYt ˆY T
t Gt1 // Compute, Send dt to all p machines"
A,0.3391812865497076,"7
for p = 1 to p in parallel on machine p do"
A,0.3406432748538012,"8
update model: wt ←wt−1 −αt · dt"
RETURN WT,0.34210526315789475,9 Return wT
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3435672514619883,"2.3
Flag Aggregator for Distributed Optimization
161"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.34502923976608185,"It is now easy to see that by choosing a = 2, in equation (4), we obtain the negative loglikelihood
162"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.34649122807017546,"(ignoring constants) as (Pp
i=1
p"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.347953216374269,"1 −gT
i Y Y T gi) showing that Flag Median can indeed be seen as
163"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.34941520467836257,"an Maximum Likelihood Estimator (MLE). In particular, Flag Median can be seen as an MLE of
164"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3508771929824561,Beta Distribution with parameters α = 1 and β = 1
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.35233918128654973,"2. Recent results suggest that in many cases, MLE
165"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3538011695906433,"is ill-posed, and regularization is necessary, even when the likelihood distribution is Gaussian [37].
166"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.35526315789473684,"So, based on the Flag Median estimator for subspaces, we propose an optimization based subspace
167"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3567251461988304,"estimator Y ∗for aggregation purposes. We formulate our Flag Aggregator (FA) objective function
168"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.358187134502924,"with respect to Y as a regularized sum of likelihood based (or data) terms in (4) using trace operators
169"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.35964912280701755,"tr(·) as the solution to the following constrained optimization problem:
170"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3611111111111111,"min
Y :Y T Y =I A(Y ) := p
X i=1"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.36257309941520466,"v
u
u
t "
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.36403508771929827,"1 −tr
 
Y T gigT
i Y
"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3654970760233918,"∥gi∥2
2 !"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3669590643274854,"+ λR(Y )
(5)"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3684210526315789,"where λ > 0 is a regularization hyperparameter. In our analysis, and implementation, we provide
171"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.36988304093567254,"support for two possible choices for R(Y ):
172"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3713450292397661,"(1) Mathematical norms: R(Y ) can be a form of norm-based regularization other than ∥Y ∥2
Fro since
173"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.37280701754385964,"it is constant over the feasible set in (5). For example, it could be convex norm with efficient
174"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3742690058479532,"subgradient oracle such as, i.e. element-wise: Pn
i=1
Pm
j=1 ∥Yij∥1 or Pm
i=1 ∥Yi,i∥1,
175"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3757309941520468,"(2) Data-dependent norms: Following our subspace construction in Section 2.1, we may choose
176"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.37719298245614036,"R(Y ) =
1
p−1
Pp
i,j=1,i̸=j"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3786549707602339,"r
1 −tr(Y T (gi−gj)(gi−gj)T Y ) D2
ij"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.38011695906432746,"
where D2
ij = ∥gi −gj∥2
2 denotes the
177"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3815789473684211,"distance between gradient vectors gi, gj from workers i, j. Intuitively, the pairwise terms in our
178"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3830409356725146,"loss function (5) favors subspace Y that also reconstructs the pairwise vectors gi −gj that are close
179"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3845029239766082,"to each other. So, by setting λ = Θ(p), that is, the pairwise terms dominate the objective function
180"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.38596491228070173,"in (5). Hence, λ regularizes optimal solutions Y ∗of (5) to contain gi’s with low pairwise distance
181"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.38742690058479534,"in its span – similar in spirit to AggregaThor in [38].
182"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3888888888888889,"Convergence of Flag Aggregator (FA) Algorithm 1.
With these, we can state our main algorithmic
183"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.39035087719298245,"result showing that our FA (5) can be solved efficiently using standard convex optimization proof
184"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.391812865497076,"techniques. In particular, in supplement, we present a smooth Semi-Definite Programming (SDP)
185"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3932748538011696,"relaxation of FA in equation (5) using the Flag structure. This allows us to view the IRLS procedure
186"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.39473684210526316,"in 1 as solving the low rank parametrization of the smooth SDP relaxation, thus guaranteeing fast
187"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3961988304093567,"convergence to second order optimal (local) solutions. Importantly, our SDP based proof works for
188"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.39766081871345027,"any degree of approximation of the constant a in equation (4) and only relies on smoothness of the
189"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.3991228070175439,"loss function wrt Y , although speed of convergence is reduced for higher values of a ̸= 2, see [39].
190"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.40058479532163743,"We leave determining the exact dependence of a on rate of convergence for future work.
191"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.402046783625731,"How is FA aggregator different from (Bulyan and Multi-Krum)? Bulyan is a strong Byzantine
192"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.40350877192982454,"resilient gradient aggregation rule for p ≥4f + 3 where p is the total number of workers and f is
193"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.40497076023391815,"the number of Byzantine workers. Bulyan is a two-stage algorithm. In the first stage, a gradient
194"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4064327485380117,"aggregation rule R like coordinate-wise median [40] or Krum [9] is recursively used to select
195"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.40789473684210525,"θ = p −2f gradients. The process uses R to select gradient vector gi which is closest to R’s output
196"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4093567251461988,"(e.g. for Krum, this would be the gradient with the top score, and hence the exact output of R). The
197"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4108187134502924,"chosen gradient is removed from the received set and added to the selection set S repeatedly until
198"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.41228070175438597,"|S| = θ. The second stage produces the resulting gradient. If β = θ −2f, each coordinate would
199"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4137426900584795,"be the average of β-nearest to the median coordinate of the θ gradients in S. In matrix terms, if we
200"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4152046783625731,"consider S ∈Rp×m as a matrix with each column having one non-zero entry summing to 1, Bulyan
201"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4166666666666667,would return 1
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.41812865497076024,"mReLU(GS)1m, where 1m ∈Rm is the vector of all ones, while FA would return
202"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4195906432748538,"1
pY Y T G1p. Importantly, the gradient matrix is being right-multiplied in Bulyan, but left-multiplied
203"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.42105263157894735,"in FA, before getting averaged. While this may seem like a discrepancy, in supplement we show that
204"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.42251461988304095,"by observing the optimality conditions of (5) wrt Y , we show that 1"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4239766081871345,"mY Y T G can be seen as a right
205"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.42543859649122806,"multiplication by a matrix parametrized by lagrangian multipliers associated with the orthogonality
206"
FLAG AGGREGATOR FOR DISTRIBUTED OPTIMIZATION,0.4269005847953216,"constraints in (5). This means it should be possible to combine both approaches for faster aggregation.
207"
EXPERIMENTS,0.4283625730994152,"3
Experiments
208"
EXPERIMENTS,0.4298245614035088,"In this section, we conduct experiments to test our proposed FA in the context of distributed training
209"
EXPERIMENTS,0.43128654970760233,"in two testbeds. First, to test the performance of our FA scheme solved using IRLS (Flag Mean) on
210"
EXPERIMENTS,0.4327485380116959,"standard Byzantine benchmarks. Then, to evaluate the ability of existing state-of-the-art gradient
211"
EXPERIMENTS,0.4342105263157895,"aggregators we augment data via two techniques that can be implemented with Sci-kit package.
212"
EXPERIMENTS,0.43567251461988304,"Implementation Details.
We implement FA in Pytorch [41], which is popular but does not support
213"
EXPERIMENTS,0.4371345029239766,"Byzantine resilience natively. We adopt the parameter server architecture and employ Pytorch’s
214"
EXPERIMENTS,0.43859649122807015,"distributed RPC framework with TensorPipe backend for machine-to-machine communication. We
215"
EXPERIMENTS,0.44005847953216376,"extend Garfield’s Pytorch library [42] with FA and limit our IRLS convergence criteria to a small
216"
EXPERIMENTS,0.4415204678362573,"error, 10−10, or 5 iterations of flag mean for SVD calculation. We set m = ⌈p+1"
EXPERIMENTS,0.44298245614035087,"2 ⌉.
217"
SETUP,0.4444444444444444,"3.1
Setup
218"
SETUP,0.44590643274853803,"Baselines: We compare FA to several existing aggregation rules: (1) coordinate-wise Trimmed
219"
SETUP,0.4473684210526316,"Mean [40] (2) coordinate-wise Median [40] (3) mean-around-median (MeaMed) [43] (4) Phocas
220"
SETUP,0.44883040935672514,"[44] (5) Multi-Krum [9] (6) Bulyan [45].
221"
SETUP,0.4502923976608187,"Accuracy: The fraction of correct predictions among all predictions, using the test dataset (top-1
222"
SETUP,0.4517543859649123,"cross-accuracy).
223"
SETUP,0.45321637426900585,"Testbed: We used 4 servers as our experimental platform. Each server has 2 Intel(R) Xeon(R) Gold
224"
SETUP,0.4546783625730994,"6240 18-core CPU @ 2.60GHz with Hyper-Threading and 384GB of RAM. Servers have a Tesla
225"
SETUP,0.45614035087719296,"V100 PCIe 32GB GPU and employ a Mellanox ConnectX-5 100Gbps NIC to connect to a switch.
226"
SETUP,0.45760233918128657,"We use one of the servers as the parameter server and instantiate 15 workers on other servers, each
227"
SETUP,0.4590643274853801,"hosting 5 worker nodes, unless specified differently in specific experiments. For the experiments
228"
SETUP,0.4605263157894737,"designed to show scalability, we instantiate 60 workers.
229"
SETUP,0.4619883040935672,"Dataset and model: We focus on the image classification task since it is a widely used task for
230"
SETUP,0.46345029239766083,"benchmarking in distributed training [46]. We train ResNet-18 [47] on CIFAR-10 [48] which has
231"
SETUP,0.4649122807017544,"60,000 32 × 32 color images in 10 classes. For the scalability experiment, we train a CNN with two
232"
SETUP,0.46637426900584794,"convolutional layers followed by two fully connected layers on MNIST [49] which has 70,000 28 ×
233"
SETUP,0.4678362573099415,"28 grayscale images in 10 classes. We also run another set of experiments on Tiny ImageNet [50] in
234"
SETUP,0.4692982456140351,"the supplement. We use SGD as the optimizer, and cross-entropy to measure loss. The batch size
235"
SETUP,0.47076023391812866,"for each worker is 128 unless otherwise stated. Also, we use a learning decay strategy where we
236"
SETUP,0.4722222222222222,"decrease the learning rate by a factor of 0.2 every 10 epochs.
237"
SETUP,0.47368421052631576,"Threat models: We evaluate FA under two classes of Byzantine workers. They can send uniformly
238"
SETUP,0.47514619883040937,"random gradients that are representative of errors in the physical setting, or use non-linear augmented
239"
SETUP,0.4766081871345029,"data described as below.
240"
SETUP,0.4780701754385965,"Evaluating resilience against nonlinear data augmentation: In order to induce Byzantine behavior
241"
SETUP,0.47953216374269003,"in our workers we utilize ODE solvers to approximately solve 2 non-linear processes, Lotka Volterra
242"
SETUP,0.48099415204678364,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
SETUP,0.4824561403508772,"0
10
20
30
40
Epoch 10 20 30 40 50 60 70"
SETUP,0.48391812865497075,Top-1 Accuracy (%)
SETUP,0.4853801169590643,(a) f = 1
SETUP,0.4868421052631579,"0
10
20
30
40
Epoch 10 20 30 40 50 60"
SETUP,0.48830409356725146,Top-1 Accuracy (%)
SETUP,0.489766081871345,(b) f = 2
SETUP,0.49122807017543857,"0
10
20
30
40
Epoch 10 20 30 40 50"
SETUP,0.4926900584795322,Top-1 Accuracy (%)
SETUP,0.49415204678362573,(c) f = 3
SETUP,0.4956140350877193,Figure 4: Tolerance to the number of Byzantine workers for robust aggregators for batch size 128.
SETUP,0.49707602339181284,"[51] and Arnold’s Cat Map [52], as augmentation methods. Since the augmented samples are
243"
SETUP,0.49853801169590645,"deterministic, albeit nonlinear functions of training samples, the “noise” is dependent across samples.
244"
SETUP,0.5,"In Lotka Volterra, we use the following linear gradient transformation of 2D pixels:
245"
SETUP,0.5014619883040936,"(x, y) →(αx −βxy, δxy −γy),"
SETUP,0.5029239766081871,"where α, β, γ and δ are hyperparameters. We choose them to be 2 3, 4"
SETUP,0.5043859649122807,"3, −1 and −1 respectively.
246"
SETUP,0.5058479532163743,"Second, we use a nonsmooth transformation called Arnold’s Cat Map as a data augmentation scheme.
247"
SETUP,0.5073099415204678,"Once again, the map can be specified using a two-dimensional matrix as,
248"
SETUP,0.5087719298245614,"(x, y) →
2x + y"
SETUP,0.5102339181286549,"N
, x + y N"
SETUP,0.5116959064327485,"
mod 1,"
SETUP,0.5131578947368421,"where mod represents the modulus operation, x and y are the coordinates or pixels of images and N
249"
SETUP,0.5146198830409356,"is the height/width of images (assumed to be square). We also used a smooth approximation of the
250"
SETUP,0.5160818713450293,"Cat Map obtained by approximating the mod function as,
251"
SETUP,0.5175438596491229,"(x, y) →1 n"
SETUP,0.5190058479532164,"
2x + y
(1 + exp(−m log(α1),
x + y
(1 + exp(−m log(α2) 
,"
SETUP,0.52046783625731,where α1 = 2x+y
SETUP,0.5219298245614035,"n
, α2 = x+y"
SETUP,0.5233918128654971,"n , and m is the degree of approximation, which we choose to be 0.95 in
252"
SETUP,0.5248538011695907,"our data augmentation experiments.
253"
SETUP,0.5263157894736842,"How to perform nonlinear data augmentation? In all three cases, we used SciPy’s [53] solve_ivp
254"
SETUP,0.5277777777777778,"method to solve the differential equations, by using the LSODA solver. In addition to the setup
255"
SETUP,0.5292397660818714,"described above, we also added a varying level of Gaussian noise to each of the training images. All
256"
SETUP,0.5307017543859649,"the images in the training set are randomly chosen to be augmented with varying noise levels of the
257"
SETUP,0.5321637426900585,"above mentioned augmentation schemes. We have provided the code that implements all our data
258"
SETUP,0.533625730994152,"augmentation schemes in the supplement zipped folder.
259"
RESULTS,0.5350877192982456,"3.2
Results
260"
RESULTS,0.5365497076023392,"Tolerance to the number of Byzantine workers: In this experiment, we show the effect of Byzantine
261"
RESULTS,0.5380116959064327,"behavior on the convergence of different gradient aggregation rules in comparison to FA. Byzantine
262"
RESULTS,0.5394736842105263,"workers send random gradients and we vary the number of them from 1 to 3. Figure 4 shows that for
263"
RESULTS,0.5409356725146199,"some rules, i.e. Trimmed Mean, the presence of even a single Byzantine worker has a catastrophic
264"
RESULTS,0.5423976608187134,"impact. For other rules, as the number of Byzantine workers increases, filtering out the outliers
265"
RESULTS,0.543859649122807,"becomes more challenging because the amount of noise increases. Regardless, FA remains more
266"
RESULTS,0.5453216374269005,"robust compared to other approaches.
267"
RESULTS,0.5467836257309941,"Marginal utility of larger batch sizes under a fixed noise level:
268"
RESULTS,0.5482456140350878,"We empirically verified the batch size required to identify our optimal Y ∗- the FA matrix at each
269"
RESULTS,0.5497076023391813,"iteration. In particular, we fixed the noise level to f = 3 Byzantine workers and varied batch sizes.
270"
RESULTS,0.5511695906432749,"We show the results in Figure 5. Our results indicate that, in cases where a larger batch size is
271"
RESULTS,0.5526315789473685,"a training requirement, FA achieves a significantly better accuracy compared to the existing
272"
RESULTS,0.554093567251462,"state of the art aggregators. This may be useful in some large scale vision applications, see [54, 55]
273"
RESULTS,0.5555555555555556,"for more details. Empirically, we can already see that our spectral relaxation to identify gradient
274"
RESULTS,0.5570175438596491,"subspace is effective in practice in all our experiments.
275"
RESULTS,0.5584795321637427,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.5599415204678363,"0
10
20
30
Epoch 10 20 30 40 50 60"
RESULTS,0.5614035087719298,Top-1 Accuracy (%)
RESULTS,0.5628654970760234,(a) bs = 64
RESULTS,0.564327485380117,"0
10
20
30
Epoch 10 20 30 40 50"
RESULTS,0.5657894736842105,Top-1 Accuracy (%)
RESULTS,0.5672514619883041,(b) bs = 128
RESULTS,0.5687134502923976,"0
10
20
30
Epoch 10 20 30 40 50"
RESULTS,0.5701754385964912,Top-1 Accuracy (%)
RESULTS,0.5716374269005848,(c) bs = 192
RESULTS,0.5730994152046783,Figure 5: Marginal utility of larger batch sizes under a fixed noise level f = 3.
RESULTS,0.5745614035087719,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.5760233918128655,"0
10
20
Epoch 10 20 30 40 50 60"
RESULTS,0.577485380116959,Top-1 Accuracy (%)
RESULTS,0.5789473684210527,"(a) p = 15, f = 3"
RESULTS,0.5804093567251462,"0
10
20
30
Epoch 10 20 30 40 50 60"
RESULTS,0.5818713450292398,Top-1 Accuracy (%)
RESULTS,0.5833333333333334,"(b) p = 11, f = 2"
RESULTS,0.5847953216374269,"0
10
20
30
Epoch 10 20 30 40 50 60"
RESULTS,0.5862573099415205,Top-1 Accuracy (%)
RESULTS,0.5877192982456141,"(c) p = 13, f = 2"
RESULTS,0.5891812865497076,"0
10
20
30
Epoch 10 20 30 40 50 60"
RESULTS,0.5906432748538012,Top-1 Accuracy (%)
RESULTS,0.5921052631578947,"(d) p = 15, f = 2"
RESULTS,0.5935672514619883,"Figure 6: We present results under two different gradient attacks. The attack in (a) corresponds to
simply dropping 10% of gradients from f workers. The attacks in (b)-(d) correspond to generic f
workers sending random gradient vectors, i.e. we simply fix noise level while adding more workers."
RESULTS,0.5950292397660819,"Tolerance to communication loss: To analyze the effect of unreliable communication channels
276"
RESULTS,0.5964912280701754,"between the workers and the parameter server on convergence, we design an experiment where the
277"
RESULTS,0.597953216374269,"physical link between some of the workers and the parameter server randomly drops a percentage of
278"
RESULTS,0.5994152046783626,"packets. Here, we set the loss rate of three links to 10% i.e., there are 3 Byzantine workers in our
279"
RESULTS,0.6008771929824561,"setting. The loss is introduced using the netem queuing discipline in Linux designed to emulate the
280"
RESULTS,0.6023391812865497,"properties of wide area networks [56]. The two main takeaways in Figure 6a are:
281"
RESULTS,0.6038011695906432,"1. FA converges to a significantly higher accuracy than other aggregators, and thus is more
robust to unreliable underlying network transports.
2. Considering time-to-accuracy for comparison, FA reaches a similar accuracy in less total
number of training iterations, and thus is more robust to slow underlying network transports."
RESULTS,0.6052631578947368,"Analyzing the marginal utility of additional workers. To see the effect of adding more workers
282"
RESULTS,0.6067251461988304,"to a fixed number of Byzantine workers, we ran experiments where we fixed f, and increased p.
283"
RESULTS,0.6081871345029239,"Our experimental results shown in Figures 6b-6d indicate that our FA algorithm possesses strong
284"
RESULTS,0.6096491228070176,"resilience property for reasonable choices of p.
285"
RESULTS,0.6111111111111112,"The effect of having augmented data during training in Byzantine workers: Figure 7 shows FA
286"
RESULTS,0.6125730994152047,"can handle nonlinear data augmentation in a much more stable fashion. Please see supplement for
287"
RESULTS,0.6140350877192983,"details on the level of noise, and exact solver settings that were used to obtain augmented images.
288"
RESULTS,0.6154970760233918,"The effect of the regularization parameter in FA: The data-dependent regularization parameter λ
289"
RESULTS,0.6169590643274854,"in FA provides flexibility in the loss function to cover aggregators that benefit from pairwise distances
290"
RESULTS,0.618421052631579,"such as Bulyan and Multi-Krum. To verify whether varying λ can interpolate Bulyan and Multi-Krum,
291"
RESULTS,0.6198830409356725,"we change λ in Figure 8. We can see when FA improves or performs similarly for a range of λ. Here,
292"
RESULTS,0.6213450292397661,"we set p and f to satisfy the strong Byzantine resilience condition of Bulyan, i.e, p ≥4f + 3.
293"
RESULTS,0.6228070175438597,"Scaling out to real-world situations with more workers: In distributed ML, p and f are usually
294"
RESULTS,0.6242690058479532,"large. To test high-dimensional settings commonly dealt in Semantic Vision with our FA, we used
295"
RESULTS,0.6257309941520468,"ResNet-18. Now, to specifically test the scalability of FA, we fully utilized our available GPU servers
296"
RESULTS,0.6271929824561403,"and set up to p = 60 workers (up to f = 14 Byzantine) with the MNIST dataset and a simple CNN
297"
RESULTS,0.6286549707602339,"with two convolutional layers followed by two fully connected layers (useful for simple detection).
298"
RESULTS,0.6301169590643275,"Figure 9 shows evidence that FA is feasible for larger setups.
299"
RESULTS,0.631578947368421,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.6330409356725146,"0
10
20
30
Epoch 10 20 30 40 50 60"
RESULTS,0.6345029239766082,Top-1 Accuracy (%)
RESULTS,0.6359649122807017,"Figure 7: Accuracy of us-
ing augmented data in f =
3 workers"
RESULTS,0.6374269005847953,"0
5
10
15
20
Epoch 10 20 30 40 50 60 70"
RESULTS,0.6388888888888888,Top-1 Accuracy (%)
RESULTS,0.6403508771929824,"FA, = 0
FA, = 4
FA, = 8
FA, = 16
Bulyan
Multi-Krum"
RESULTS,0.6418128654970761,"Figure 8: CIFAR10 with
ResNet-18, p = 7, and
f = 1"
RESULTS,0.6432748538011696,"0
5
10
15
20
Epoch 20 40 60 80 100"
RESULTS,0.6447368421052632,Top-1 Accuracy (%)
RESULTS,0.6461988304093568,"FA, p = 15, f = 3
FA, p = 30, f = 6
FA, p = 60, f = 12"
RESULTS,0.6476608187134503,"Figure 9: Scaling FA to
larger setups"
RESULTS,0.6491228070175439,"200
400
600
800
1000
Time (s) 10 20 30 40 50"
RESULTS,0.6505847953216374,Top-1 Accuracy (%)
RESULTS,0.652046783625731,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.6535087719298246,(a) Cropped Time-to-accuracy
RESULTS,0.6549707602339181,"0
10
20
30
40
Epoch 0 500 1000 1500 2000 2500 3000"
RESULTS,0.6564327485380117,Time (s)
RESULTS,0.6578947368421053,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.6593567251461988,(b) Iteration time
RESULTS,0.6608187134502924,"500
1000
1500
2000
2500
3000
Time (s) 10 20 30 40 50"
RESULTS,0.6622807017543859,Top-1 Accuracy (%)
RESULTS,0.6637426900584795,"Flag Aggregator
Bulyan
Multi-Krum
MeaMed
Median
Trimmed Mean
Phocas"
RESULTS,0.6652046783625731,(c) Total Time-to-accuracy
RESULTS,0.6666666666666666,Figure 10: Wall clock time comparison
DISCUSSION AND LIMITATION,0.6681286549707602,"4
Discussion and Limitation
300"
DISCUSSION AND LIMITATION,0.6695906432748538,"Is it possible to fully “offload” FA computation to switches? Recent work propose that aggregation
301"
DISCUSSION AND LIMITATION,0.6710526315789473,"be performed entirely on network infrastructure to alleviate any communication bottleneck that may
302"
DISCUSSION AND LIMITATION,0.672514619883041,"arise [57, 58]. However, to the best of our knowledge, switches that are in use today only allow
303"
DISCUSSION AND LIMITATION,0.6739766081871345,"limited computation to be performed on gradient gi as packets whenever they are transmitted [59, 60].
304"
DISCUSSION AND LIMITATION,0.6754385964912281,"That is, programmability is restrictive at the moment— switches used in practice have no floating
305"
DISCUSSION AND LIMITATION,0.6769005847953217,"point, or loop support, and are severely memory/state constrained. Fortunately, solutions seem near.
306"
DISCUSSION AND LIMITATION,0.6783625730994152,"For instance, [61] have already introduced support for floating point arithmetic in programmable
307"
DISCUSSION AND LIMITATION,0.6798245614035088,"switches. We may use quantization approaches for SVD calculation with some accuracy loss [62] to
308"
DISCUSSION AND LIMITATION,0.6812865497076024,"approximate floating point arithmetic. Offloading FA to switches has great potential in improving
309"
DISCUSSION AND LIMITATION,0.6827485380116959,"its computational complexity because the switch would perform as a high-throughput streaming
310"
DISCUSSION AND LIMITATION,0.6842105263157895,"parameter server to synchronize gradients over the network. Considering that FA’s accuracy currently
311"
DISCUSSION AND LIMITATION,0.685672514619883,"outperforms its competition in several experiments, an offloaded FA can reach their accuracy even
312"
DISCUSSION AND LIMITATION,0.6871345029239766,"faster or it could reach a higher accuracy in the same amount of time.
313"
DISCUSSION AND LIMITATION,0.6885964912280702,"Potential Limitation. Because in every iteration of FA, we perform SVD, the complexity of the
314"
DISCUSSION AND LIMITATION,0.6900584795321637,"algorithm would be O(nNδ(Pp
i=1 ki)2) with Nδ being the number of iterations for the algorithm.
315"
DISCUSSION AND LIMITATION,0.6915204678362573,"Figure 10 show the wall clock time it takes for FA to reach a certain accuracy (10a) or epoch(10b)
316"
DISCUSSION AND LIMITATION,0.6929824561403509,"compared to other methods under a fixed amount of random noise f = 3 with p = 15 workers.
317"
DISCUSSION AND LIMITATION,0.6944444444444444,"Although the iteration complexity of FA is higher, here each iteration has a higher utility as reflected in
318"
DISCUSSION AND LIMITATION,0.695906432748538,"the time-to-accuracy measures. This makes FA comparable to others in a shorter time span, however,
319"
DISCUSSION AND LIMITATION,0.6973684210526315,"if there is more wall clock time to spare, FA converges to a better state as shown in Figure 10c where
320"
DISCUSSION AND LIMITATION,0.6988304093567251,"we let the same number of total iterations finish for all methods.
321"
CONCLUSION,0.7002923976608187,"5
Conclusion
322"
CONCLUSION,0.7017543859649122,"In this paper we proposed Flag Aggregator (FA) that can be used for robust aggregation of gradients
323"
CONCLUSION,0.7032163742690059,"in distributed training. FA is an optimization-based subspace estimator that formulates aggregation as
324"
CONCLUSION,0.7046783625730995,"a Maximum Likelihood Estimation procedure using Beta densities. We perform extensive evaluations
325"
CONCLUSION,0.706140350877193,"of FA and show it can be effectively used in providing Byzantine resilience for gradient aggregation.
326"
CONCLUSION,0.7076023391812866,"Using techniques from convex optimization, we theoretically analyze FA and with tractable relaxations
327"
CONCLUSION,0.7090643274853801,"show its amenability to be solved by off-the-shelf solvers or first-order reweighing methods.
328"
REFERENCES,0.7105263157894737,"References
329"
REFERENCES,0.7119883040935673,"[1] Michel Grabisch, Jean-Luc Marichal, Radko Mesiar, and Endre Pap. Aggregation functions,
330"
REFERENCES,0.7134502923976608,"volume 127. Cambridge University Press, 2009.
331"
REFERENCES,0.7149122807017544,"[2] Sivaraman Balakrishnan, Simon S. Du, Jerry Li, and Aarti Singh.
Computationally effi-
332"
REFERENCES,0.716374269005848,"cient robust sparse estimation in high dimensions. In Satyen Kale and Ohad Shamir, edi-
333"
REFERENCES,0.7178362573099415,"tors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings
334"
REFERENCES,0.7192982456140351,"of Machine Learning Research, pages 169–212. PMLR, 07–10 Jul 2017.
URL https:
335"
REFERENCES,0.7207602339181286,"//proceedings.mlr.press/v65/balakrishnan17a.html.
336"
REFERENCES,0.7222222222222222,"[3] Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Eric Price, and Alistair Stewart. Outlier-
337"
REFERENCES,0.7236842105263158,"robust high-dimensional sparse estimation via iterative filtering. Advances in Neural Information
338"
REFERENCES,0.7251461988304093,"Processing Systems, 32, 2019.
339"
REFERENCES,0.7266081871345029,"[4] Yu Cheng, Ilias Diakonikolas, Rong Ge, Shivam Gupta, Daniel M. Kane, and Mahdi
340"
REFERENCES,0.7280701754385965,"Soltanolkotabi. Outlier-robust sparse estimation via non-convex optimization. Advances
341"
REFERENCES,0.72953216374269,"in Neural Information Processing Systems, 2022.
342"
REFERENCES,0.7309941520467836,"[5] Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, and Thanasis Pittas.
343"
REFERENCES,0.7324561403508771,"Robust sparse mean estimation via sum of squares. In Po-Ling Loh and Maxim Raginsky,
344"
REFERENCES,0.7339181286549707,"editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings
345"
REFERENCES,0.7353801169590644,"of Machine Learning Research, pages 4703–4763. PMLR, 02–05 Jul 2022. URL https:
346"
REFERENCES,0.7368421052631579,"//proceedings.mlr.press/v178/diakonikolas22e.html.
347"
REFERENCES,0.7383040935672515,"[6] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
348"
REFERENCES,0.7397660818713451,"Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
349"
REFERENCES,0.7412280701754386,"ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
350"
REFERENCES,0.7426900584795322,"(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.
351"
REFERENCES,0.7441520467836257,"[7] Konstantinos I Tsianos and Michael G Rabbat. Distributed strongly convex optimization. In
352"
REFERENCES,0.7456140350877193,"2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton),
353"
REFERENCES,0.7470760233918129,"pages 593–600. IEEE, 2012.
354"
REFERENCES,0.7485380116959064,"[8] Tao Yang, Xinlei Yi, Junfeng Wu, Ye Yuan, Di Wu, Ziyang Meng, Yiguang Hong, Hong Wang,
355"
REFERENCES,0.75,"Zongli Lin, and Karl H Johansson. A survey of distributed optimization. Annual Reviews in
356"
REFERENCES,0.7514619883040936,"Control, 47:278–305, 2019.
357"
REFERENCES,0.7529239766081871,"[9] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
358"
REFERENCES,0.7543859649122807,"with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st Interna-
359"
REFERENCES,0.7558479532163743,"tional Conference on Neural Information Processing Systems, NIPS’17, page 118–128. Curran
360"
REFERENCES,0.7573099415204678,"Associates Inc., 2017. ISBN 9781510860964.
361"
REFERENCES,0.7587719298245614,"[10] Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan.
362"
REFERENCES,0.7602339181286549,"Byzantine machine learning made easy by resilient averaging of momentums. In Kamalika
363"
REFERENCES,0.7616959064327485,"Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, edi-
364"
REFERENCES,0.7631578947368421,"tors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of
365"
REFERENCES,0.7646198830409356,"Proceedings of Machine Learning Research, pages 6246–6283. PMLR, 17–23 Jul 2022. URL
366"
REFERENCES,0.7660818713450293,"https://proceedings.mlr.press/v162/farhadkhani22a.html.
367"
REFERENCES,0.7675438596491229,"[11] Leonardo Bautista-Gomez, Ferad Zyulkyarov, Osman Unsal, and Simon McIntosh-Smith.
368"
REFERENCES,0.7690058479532164,"Unprotected computing: A large-scale study of dram raw error rate on a supercomputer. In SC
369"
REFERENCES,0.77046783625731,"’16: Proceedings of the International Conference for High Performance Computing, Networking,
370"
REFERENCES,0.7719298245614035,"Storage and Analysis, pages 645–655, 2016. doi: 10.1109/SC.2016.54.
371"
REFERENCES,0.7733918128654971,"[12] Bianca Schroeder and Garth A. Gibson. Disk failures in the real world: What does an mttf
372"
REFERENCES,0.7748538011695907,"of 1,000,000 hours mean to you? In Proceedings of the 5th USENIX Conference on File and
373"
REFERENCES,0.7763157894736842,"Storage Technologies, FAST ’07, page 1–es, USA, 2007. USENIX Association.
374"
REFERENCES,0.7777777777777778,"[13] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. Understanding network failures
375"
REFERENCES,0.7792397660818714,"in data centers: Measurement, analysis, and implications. SIGCOMM Comput. Commun.
376"
REFERENCES,0.7807017543859649,"Rev., 41(4):350–361, aug 2011. ISSN 0146-4833. doi: 10.1145/2043164.2018477. URL
377"
REFERENCES,0.7821637426900585,"https://doi.org/10.1145/2043164.2018477.
378"
REFERENCES,0.783625730994152,"[14] Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses
379"
REFERENCES,0.7850877192982456,"for distributed learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
380"
REFERENCES,0.7865497076023392,"and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32.
381"
REFERENCES,0.7880116959064327,"Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
382"
REFERENCES,0.7894736842105263,"file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf.
383"
REFERENCES,0.7909356725146199,"[15] Guosai Wang, Lifei Zhang, and Wei Xu. What can we learn from four years of data center
384"
REFERENCES,0.7923976608187134,"hardware failures? In 2017 47th Annual IEEE/IFIP International Conference on Dependable
385"
REFERENCES,0.793859649122807,"Systems and Networks (DSN), pages 25–36, 2017. doi: 10.1109/DSN.2017.26.
386"
REFERENCES,0.7953216374269005,"[16] Devesh Tiwari, Saurabh Gupta, James Rogers, Don Maxwell, Paolo Rech, Sudharshan Vazhku-
387"
REFERENCES,0.7967836257309941,"dai, Daniel Oliveira, Dave Londo, Nathan DeBardeleben, Philippe Navaux, Luigi Carro, and
388"
REFERENCES,0.7982456140350878,"Arthur Bland. Understanding gpu errors on large-scale hpc systems and the implications for
389"
REFERENCES,0.7997076023391813,"system design and operation. In 2015 IEEE 21st International Symposium on High Performance
390"
REFERENCES,0.8011695906432749,"Computer Architecture (HPCA), pages 331–342, 2015. doi: 10.1109/HPCA.2015.7056044.
391"
REFERENCES,0.8026315789473685,"[17] Bin Nie, Devesh Tiwari, Saurabh Gupta, Evgenia Smirni, and James H. Rogers. A large-scale
392"
REFERENCES,0.804093567251462,"study of soft-errors on gpus in the field. In 2016 IEEE International Symposium on High
393"
REFERENCES,0.8055555555555556,"Performance Computer Architecture (HPCA), pages 519–530, 2016. doi: 10.1109/HPCA.2016.
394"
REFERENCES,0.8070175438596491,"7446091.
395"
REFERENCES,0.8084795321637427,"[18] Sheldon M Ross. Introduction to probability models. Academic press, 2014.
396"
REFERENCES,0.8099415204678363,"[19] Fanny Yang, Zuowen Wang, and Christina Heinze-Deml. Invariance-inducing regulariza-
397"
REFERENCES,0.8114035087719298,"tion using worst-case transformations suffices to boost accuracy and spatial robustness.
398"
REFERENCES,0.8128654970760234,"In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-
399"
REFERENCES,0.814327485380117,"nett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
400"
REFERENCES,0.8157894736842105,"Associates, Inc., 2019.
URL https://proceedings.neurips.cc/paper/2019/file/
401"
REFERENCES,0.8172514619883041,"1d01bd2e16f57892f0954902899f0692-Paper.pdf.
402"
REFERENCES,0.8187134502923976,"[20] Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain
403"
REFERENCES,0.8201754385964912,"shift robustness, 2017. URL https://arxiv.org/abs/1710.11469.
404"
REFERENCES,0.8216374269005848,"[21] Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep su-
405"
REFERENCES,0.8230994152046783,"pervised domain adaptation and generalization. In IEEE International Conference on Computer
406"
REFERENCES,0.8245614035087719,"Vision (ICCV), 2017.
407"
REFERENCES,0.8260233918128655,"[22] Sravanti Addepalli, Samyak Jain, et al. Efficient and effective augmentation strategy for
408"
REFERENCES,0.827485380116959,"adversarial training. Advances in Neural Information Processing Systems, 35:1488–1501, 2022.
409"
REFERENCES,0.8289473684210527,"[23] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial
410"
REFERENCES,0.8304093567251462,"training. arXiv preprint arXiv:2001.03994, 2020.
411"
REFERENCES,0.8318713450292398,"[24] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
412"
REFERENCES,0.8333333333333334,"smoothing. In international conference on machine learning, pages 1310–1320. PMLR, 2019.
413"
REFERENCES,0.8347953216374269,"[25] Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. Dis-
414"
REFERENCES,0.8362573099415205,"tributed learning with curious and adversarial machines. arXiv preprint arXiv:2302.04787,
415"
REFERENCES,0.8377192982456141,"2023.
416"
REFERENCES,0.8391812865497076,"[26] Youssef Allouah, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafaël Pinot, and
417"
REFERENCES,0.8406432748538012,"John Stephan. Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In
418"
REFERENCES,0.8421052631578947,"International Conference on Artificial Intelligence and Statistics, pages 1232–1300. PMLR,
419"
REFERENCES,0.8435672514619883,"2023.
420"
REFERENCES,0.8450292397660819,"[27] Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan.
421"
REFERENCES,0.8464912280701754,"Byzantine machine learning made easy by resilient averaging of momentums. In International
422"
REFERENCES,0.847953216374269,"Conference on Machine Learning, pages 6246–6283. PMLR, 2022.
423"
REFERENCES,0.8494152046783626,"[28] P-A Absil. Optimization algorithms on matrix manifolds. Princeton University Press, 2008.
424"
REFERENCES,0.8508771929824561,"[29] John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma.
Robust principal
425"
REFERENCES,0.8523391812865497,"component analysis: Exact recovery of corrupted low-rank matrices via convex optimization.
426"
REFERENCES,0.8538011695906432,"Advances in neural information processing systems, 22, 2009.
427"
REFERENCES,0.8552631578947368,"[30] Matthias Hein and Thomas Bühler. An inverse power method for nonlinear eigenproblems with
428"
REFERENCES,0.8567251461988304,"applications in 1-spectral clustering and sparse pca. Advances in neural information processing
429"
REFERENCES,0.8581871345029239,"systems, 23, 2010.
430"
REFERENCES,0.8596491228070176,"[31] Rudrasis Chakraborty, Soren Hauberg, and Baba C Vemuri. Intrinsic grassmann averages for
431"
REFERENCES,0.8611111111111112,"online linear and robust subspace learning. In Proceedings of the IEEE Conference on Computer
432"
REFERENCES,0.8625730994152047,"Vision and Pattern Recognition, pages 6196–6204, 2017.
433"
REFERENCES,0.8640350877192983,"[32] D. Monk. The geometry of flag manifolds. Proceedings of the London Mathematical So-
434"
REFERENCES,0.8654970760233918,"ciety, s3-9(2):253–286, 1959. doi: https://doi.org/10.1112/plms/s3-9.2.253. URL https:
435"
REFERENCES,0.8669590643274854,"//londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-9.2.253.
436"
REFERENCES,0.868421052631579,"[33] Ke Ye, Ken Sze-Wai Wong, and Lek-Heng Lim. Optimization on flag manifolds. Mathematical
437"
REFERENCES,0.8698830409356725,"Programming, 194(1-2):621–660, 2022.
438"
REFERENCES,0.8713450292397661,"[34] Nathan Mankovich, Emily J King, Chris Peterson, and Michael Kirby. The flag median
439"
REFERENCES,0.8728070175438597,"and flagirls. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
440"
REFERENCES,0.8742690058479532,"Recognition, pages 10339–10347, 2022.
441"
REFERENCES,0.8757309941520468,"[35] Kevin P Murphy. Probabilistic machine learning: an introduction. MIT press, 2022.
442"
REFERENCES,0.8771929824561403,"[36] Massimo Fornasier, Holger Rauhut, and Rachel Ward. Low-rank matrix recovery via iteratively
443"
REFERENCES,0.8786549707602339,"reweighted least squares minimization. SIAM Journal on Optimization, 21(4):1614–1640, 2011.
444"
REFERENCES,0.8801169590643275,"[37] Toni Karvonen and Chris J Oates. Maximum likelihood estimation in gaussian process regression
445"
REFERENCES,0.881578947368421,"is ill-posed. Journal of Machine Learning Research, 24(120):1–47, 2023.
446"
REFERENCES,0.8830409356725146,"[38] Georgios Damaskinos, El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, and Sébastien
447"
REFERENCES,0.8845029239766082,"Rouault. Aggregathor: Byzantine machine learning via robust gradient aggregation. Proceedings
448"
REFERENCES,0.8859649122807017,"of Machine Learning and Systems, 1:81–106, 2019.
449"
REFERENCES,0.8874269005847953,"[39] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion:
450"
REFERENCES,0.8888888888888888,"Understanding statistical guarantees for convex relaxation via nonconvex optimization. SIAM
451"
REFERENCES,0.8903508771929824,"journal on optimization, 30(4):3098–3121, 2020.
452"
REFERENCES,0.8918128654970761,"[40] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
453"
REFERENCES,0.8932748538011696,"learning: Towards optimal statistical rates. In Proceedings of the 35th International Conference
454"
REFERENCES,0.8947368421052632,"on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5650–
455"
REFERENCES,0.8961988304093568,"5659. PMLR, 10–15 Jul 2018.
456"
REFERENCES,0.8976608187134503,"[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
457"
REFERENCES,0.8991228070175439,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
458"
REFERENCES,0.9005847953216374,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
459"
REFERENCES,0.902046783625731,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
460"
REFERENCES,0.9035087719298246,"high-performance deep learning library. In Advances in Neural Information Processing Systems,
461"
REFERENCES,0.9049707602339181,"volume 32, 2019.
462"
REFERENCES,0.9064327485380117,"[42] Rachid Guerraoui, Arsany Guirguis, Jérémy Plassmann, Anton Ragot, and Sébastien Rouault.
463"
REFERENCES,0.9078947368421053,"Garfield: System support for byzantine machine learning (regular paper). In 2021 51st Annual
464"
REFERENCES,0.9093567251461988,"IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 39–51,
465"
REFERENCES,0.9108187134502924,"2021. doi: 10.1109/DSN48987.2021.00021.
466"
REFERENCES,0.9122807017543859,"[43] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd, 2018.
467"
REFERENCES,0.9137426900584795,"[44] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Phocas: dimensional byzantine-resilient
468"
REFERENCES,0.9152046783625731,"stochastic gradient descent, 2018.
469"
REFERENCES,0.9166666666666666,"[45] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault. The hidden vulnerability of
470"
REFERENCES,0.9181286549707602,"distributed learning in Byzantium. In Proceedings of the 35th International Conference on
471"
REFERENCES,0.9195906432748538,"Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3521–3530.
472"
REFERENCES,0.9210526315789473,"PMLR, 10–15 Jul 2018.
473"
REFERENCES,0.922514619883041,"[46] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
Project
474"
REFERENCES,0.9239766081871345,"adam: Building an efficient and scalable deep learning training system. In Proceedings of the
475"
REFERENCES,0.9254385964912281,"11th USENIX Conference on Operating Systems Design and Implementation, OSDI’14, page
476"
REFERENCES,0.9269005847953217,"571–582, USA, 2014.
477"
REFERENCES,0.9283625730994152,"[47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
478"
REFERENCES,0.9298245614035088,"recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
479"
REFERENCES,0.9312865497076024,"pages 770–778, 2016. doi: 10.1109/CVPR.2016.90.
480"
REFERENCES,0.9327485380116959,"[48] Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https:
481"
REFERENCES,0.9342105263157895,"//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
482"
REFERENCES,0.935672514619883,"[49] Yann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010. URL http:
483"
REFERENCES,0.9371345029239766,"//yann.lecun.com/exdb/mnist/.
484"
REFERENCES,0.9385964912280702,"[50] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge, 2015.
485"
REFERENCES,0.9400584795321637,"[51] David Kelly. Rough path recursions and diffusion approximations. The Annals of Applied
486"
REFERENCES,0.9415204678362573,"Probability, 26(1):425–461, 2016.
487"
REFERENCES,0.9429824561403509,"[52] Jianghong Bao and Qigui Yang. Period of the discrete arnold cat map and general cat map.
488"
REFERENCES,0.9444444444444444,"Nonlinear Dynamics, 70(2):1365–1375, 2012.
489"
REFERENCES,0.945906432748538,"[53] Fundamental algorithms for scientific computing in python. https://scipy.org/, 2023.
490"
REFERENCES,0.9473684210526315,"[54] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
491"
REFERENCES,0.9488304093567251,"Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
492"
REFERENCES,0.9502923976608187,"In International Conference on Learning Representations, 2017. URL https://openreview.
493"
REFERENCES,0.9517543859649122,"net/forum?id=H1oyRlYgg.
494"
REFERENCES,0.9532163742690059,"[55] Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
495"
REFERENCES,0.9546783625730995,"Large-batch training for lstm and beyond. In Proceedings of the International Conference for
496"
REFERENCES,0.956140350877193,"High Performance Computing, Networking, Storage and Analysis, SC ’19, 2019.
497"
REFERENCES,0.9576023391812866,"[56] Kevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R. Ganger,
498"
REFERENCES,0.9590643274853801,"Phillip B. Gibbons, and Onur Mutlu. Gaia: Geo-distributed machine learning approaching lan
499"
REFERENCES,0.9605263157894737,"speeds. In Proceedings of the 14th USENIX Conference on Networked Systems Design and
500"
REFERENCES,0.9619883040935673,"Implementation, page 629–647, 2017.
501"
REFERENCES,0.9634502923976608,"[57] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim,
502"
REFERENCES,0.9649122807017544,"Arvind Krishnamurthy, Masoud Moshref, Dan Ports, and Peter Richtárik. Scaling distributed
503"
REFERENCES,0.966374269005848,"machine learning with {In-Network} aggregation. In 18th USENIX Symposium on Networked
504"
REFERENCES,0.9678362573099415,"Systems Design and Implementation (NSDI 21), pages 785–808, 2021.
505"
REFERENCES,0.9692982456140351,"[58] ChonLam Lao, Yanfang Le, Kshiteej Mahajan, Yixi Chen, Wenfei Wu, Aditya Akella, and
506"
REFERENCES,0.9707602339181286,"Michael Swift. {ATP}: In-network aggregation for multi-tenant learning. In 18th USENIX
507"
REFERENCES,0.9722222222222222,"Symposium on Networked Systems Design and Implementation (NSDI 21), pages 741–761,
508"
REFERENCES,0.9736842105263158,"2021.
509"
REFERENCES,0.9751461988304093,"[59] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin Izzard,
510"
REFERENCES,0.9766081871345029,"Fernando Mujica, and Mark Horowitz. Forwarding metamorphosis: Fast programmable match-
511"
REFERENCES,0.9780701754385965,"action processing in hardware for sdn. In Proceedings of the ACM SIGCOMM 2013 Conference
512"
REFERENCES,0.97953216374269,"on SIGCOMM, SIGCOMM ’13, page 99–110, New York, NY, USA, 2013. Association for
513"
REFERENCES,0.9809941520467836,"Computing Machinery. ISBN 9781450320566. doi: 10.1145/2486001.2486011. URL https:
514"
REFERENCES,0.9824561403508771,"//doi.org/10.1145/2486001.2486011.
515"
REFERENCES,0.9839181286549707,"[60] N McKeown. Pisa: Protocol independent switch architecture. In P4 Workshop, 2015.
516"
REFERENCES,0.9853801169590644,"[61] Yifan Yuan, Omar Alama, Jiawei Fei, Jacob Nelson, Dan RK Ports, Amedeo Sapio, Marco
517"
REFERENCES,0.9868421052631579,"Canini, and Nam Sung Kim. Unlocking the power of inline {Floating-Point} operations on
518"
REFERENCES,0.9883040935672515,"programmable switches. In 19th USENIX Symposium on Networked Systems Design and
519"
REFERENCES,0.9897660818713451,"Implementation (NSDI 22), pages 683–700, 2022.
520"
REFERENCES,0.9912280701754386,"[62] Zhourui Song, Zhenyu Liu, and Dongsheng Wang. Computation error analysis of block floating
521"
REFERENCES,0.9926900584795322,"point arithmetic oriented convolution neural network accelerator design. In Proceedings of the
522"
REFERENCES,0.9941520467836257,"Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications
523"
REFERENCES,0.9956140350877193,"of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in
524"
REFERENCES,0.9970760233918129,"Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. ISBN 978-1-57735-
525"
REFERENCES,0.9985380116959064,"800-8.
526"
