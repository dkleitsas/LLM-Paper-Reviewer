Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010845986984815619,"Transformer-based trackers have established a dominant role in the field of visual
1"
ABSTRACT,0.0021691973969631237,"object tracking. While these trackers exhibit promising performance, their deploy-
2"
ABSTRACT,0.0032537960954446853,"ment on resource-constrained devices remains challenging due to inefficiencies. To
3"
ABSTRACT,0.004338394793926247,"improve the inference efficiency and reduce the computation cost, prior approaches
4"
ABSTRACT,0.005422993492407809,"have aimed to either design lightweight trackers or distill knowledge from larger
5"
ABSTRACT,0.006507592190889371,"teacher models into more compact student trackers. However, these solutions
6"
ABSTRACT,0.007592190889370932,"often sacrifice accuracy for speed. Thus, we propose a general model compression
7"
ABSTRACT,0.008676789587852495,"framework for efficient transformer object tracking, named CompressTracker, to
8"
ABSTRACT,0.009761388286334056,"reduce the size of a pre-trained tracking model into a lightweight tracker with
9"
ABSTRACT,0.010845986984815618,"minimal performance degradation. Our approach features a novel stage division
10"
ABSTRACT,0.01193058568329718,"strategy that segments the transformer layers of the teacher model into distinct
11"
ABSTRACT,0.013015184381778741,"stages, enabling the student model to emulate each corresponding teacher stage
12"
ABSTRACT,0.014099783080260303,"more effectively. Additionally, we also design a unique replacement training tech-
13"
ABSTRACT,0.015184381778741865,"nique that involves randomly substituting specific stages in the student model
14"
ABSTRACT,0.016268980477223426,"with those from the teacher model, as opposed to training the student model in
15"
ABSTRACT,0.01735357917570499,"isolation. Replacement training enhances the student model’s ability to replicate
16"
ABSTRACT,0.01843817787418655,"the teacher model’s behavior. To further forcing student model to emulate teacher
17"
ABSTRACT,0.019522776572668113,"model, we incorporate prediction guidance and stage-wise feature mimicking to
18"
ABSTRACT,0.020607375271149676,"provide additional supervision during the teacher model’s compression process.
19"
ABSTRACT,0.021691973969631236,"Our framework CompressTracker is structurally agnostic, making it compatible
20"
ABSTRACT,0.0227765726681128,"with any transformer architecture. We conduct a series of experiment to verify the
21"
ABSTRACT,0.02386117136659436,"effectiveness and generalizability of CompressTracker. Our CompressTracker-4
22"
ABSTRACT,0.024945770065075923,"with 4 transformer layers, which is compressed from OSTrack, retains about 96%
23"
ABSTRACT,0.026030368763557483,"performance on LaSOT (66.1% AUC) while achieves 2.17× speed up.
24"
INTRODUCTION,0.027114967462039046,"1
Introduction
25"
INTRODUCTION,0.028199566160520606,"Visual object tracking is tasked with continuously localizing a target object across video frames based
26"
INTRODUCTION,0.02928416485900217,"on the initial bounding box in the first frame. Transformer-based trackers have achieved promising
27"
INTRODUCTION,0.03036876355748373,"performance on well-established benchmarks, their deployment on resource-restricted device remains
28"
INTRODUCTION,0.031453362255965296,"a significant challenge. Developing a strong tracker with high efficiency is of great significance.
29"
INTRODUCTION,0.03253796095444685,"To reduce the inference cost of models, previous works attempt to design lightweight trackers or
30"
INTRODUCTION,0.033622559652928416,"transfer the knowledge from teacher models to student trackers. Despite achieving increased speed,
31"
INTRODUCTION,0.03470715835140998,"these existing methods still exhibit notable limitations. (1) Inferior Accuracy. Certain works propose
32"
INTRODUCTION,0.03579175704989154,"lightweight tracking models [6, 10, 4, 21, 26] or employ neural architecture search (NAS) to search
33"
INTRODUCTION,0.0368763557483731,"better architecture [42]. Due to the limited number of parameters, these models often suffer from
34"
INTRODUCTION,0.03796095444685466,"underfitting and inferior performance. (2) Complex Training. Some works [15] aim to enhance the
35"
INTRODUCTION,0.039045553145336226,"accuracy of fast trackers through transferring the knowledge from a teacher tracker to a student model.
36"
INTRODUCTION,0.04013015184381779,"(a) We compress OSTrack and achieve promising performance.
(b) Performance and speed comparison
(c) FLOPs and params comparison"
INTRODUCTION,0.04121475054229935,"FLOPs
Params
FPS
AUC"
INTRODUCTION,0.04229934924078091,"Figure 1: We apply our framework to OSTrack under several different layer configurations. (a) We
implement each enhancement into our CompressTracker step by step. The training time is calculated
by using 8 NVIDIA RTX 3090 GPUs. Notably, our CompressTracker-4 accelerates OSTrack by
2.17× while preserving approximately 96% of its original accuracy, thereby demonstrating the
effectiveness of our framework. (b) Performance and speed comparison of CompressTracker variants
with different numbers of layers. CT-x refers to a version of CompressTracker with ’x’ layers. (c)
FLOPs and parameters comparison of CompressTracker variants with different numbers of layers.
Despite the improved performance, [15] introduces a complex multi-stage training strategy, which
37"
INTRODUCTION,0.04338394793926247,"is time-consuming. Any suboptimal performance in these individual stages can cumulatively result
38"
INTRODUCTION,0.044468546637744036,"in suboptimal performance in the final model. (3) Stucture Limitation. Additionally, the model
39"
INTRODUCTION,0.0455531453362256,"reduction paradigm in [15] severely restricts the structure of student models to be consistent only
40"
INTRODUCTION,0.046637744034707156,"with the teacher’s model.
41"
INTRODUCTION,0.04772234273318872,"Thus, we introduce CompressTracker, a novel and general model compression framework to enhance
42"
INTRODUCTION,0.04880694143167028,"the efficiency of transformer tracking models. The current dominant trackers are one-stream mod-
43"
INTRODUCTION,0.049891540130151846,"els [44, 15, 4, 10] characterized by a series of sequential transformer encoder layers, each designed
44"
INTRODUCTION,0.0509761388286334,"to refine the temporal matching features across frames. The output of each layer is a critical temporal
45"
INTRODUCTION,0.052060737527114966,"matching result that is refined as the layers get deeper. Given this layer-wise refinement, it becomes a
46"
INTRODUCTION,0.05314533622559653,"natural progression to consider the model not as a single entity but as a series of interconnected stages
47"
INTRODUCTION,0.05422993492407809,"and encourage student tracker to align teacher model at each stage. We propose the stage division
48"
INTRODUCTION,0.055314533622559656,"strategy, which involves partitioning the teacher model, a complex pretrained transformer-based
49"
INTRODUCTION,0.05639913232104121,"tracking model, into distinct stages that correspond to the layers of a simpler student model. This is
50"
INTRODUCTION,0.057483731019522775,"achieved by dividing the teacher model into a number of stages equivalent to the student model’s
51"
INTRODUCTION,0.05856832971800434,"layers. Each stage in the student model is then tasked with learning and replicating the functional
52"
INTRODUCTION,0.0596529284164859,"behavior of its corresponding stage in the teacher model. This division is not merely a structural
53"
INTRODUCTION,0.06073752711496746,"alteration but a strategic educational approach. By focusing each stage of the student model on
54"
INTRODUCTION,0.06182212581344902,"mimicking a specific stage of the teacher, we enable a targeted and efficient transfer of knowledge.
55"
INTRODUCTION,0.06290672451193059,"The student model learns not just the ’what’ of tracking—i.e., the raw matching of features—but also
56"
INTRODUCTION,0.06399132321041215,"the ’how’—i.e., the strategies developed by the teacher model at each layer of processing.
57"
INTRODUCTION,0.0650759219088937,"Contrary to conventional practices that isolate the training of student models, we employ a replacement
58"
INTRODUCTION,0.06616052060737528,"training methodology that strategically intertwines the teacher and student models. The core of this
59"
INTRODUCTION,0.06724511930585683,"methodology is the dynamic substitution of stages during training. we randomly select stages from
60"
INTRODUCTION,0.06832971800433839,"the student model and replace them with the corresponding stages from the teacher model. By doing
61"
INTRODUCTION,0.06941431670281996,"so, we situate the teacher model and the student model within a collaborative environment. This
62"
INTRODUCTION,0.07049891540130152,"arrangement permits the unaltered stages of the teacher model to collaboratively inform and enhance
63"
INTRODUCTION,0.07158351409978309,"the learning of the substituted stages in the student model rather than supervising the entire student
64"
INTRODUCTION,0.07266811279826464,"model as a single entity. The student model is not merely learning in parallel but is directly engaging
65"
INTRODUCTION,0.0737527114967462,"with the teacher’s learned behaviors. After training, we can just combine each stage of student model
66"
INTRODUCTION,0.07483731019522777,"for inference. The replacement training leads to a more authentic replication of the teacher’s tracking
67"
INTRODUCTION,0.07592190889370933,"strategies and helps to prevent the student model from overfitting to specific stages of the teacher
68"
INTRODUCTION,0.0770065075921909,"model, promoting a more stable training.
69"
INTRODUCTION,0.07809110629067245,"To augment the learning process, we introduce prediction guidance, which serves as a supervisory
70"
INTRODUCTION,0.07917570498915401,"signal for the student model by leveraging the teacher model’s predictions. By using the predictions
71"
INTRODUCTION,0.08026030368763558,"of the teacher model as a reference, the student model can converge more quickly. Furthermore,
72"
INTRODUCTION,0.08134490238611713,"to enhance the similarity of the temporal matching features across corresponding stages, we have
73"
INTRODUCTION,0.0824295010845987,"developed a stage-wise feature mimicking strategy. This approach systematically aligns the feature
74"
INTRODUCTION,0.08351409978308026,"representations learned at each stage of the student model with those of the teacher model, thereby
75"
INTRODUCTION,0.08459869848156182,"promoting a more accurate and consistent learning. In Figure 1 (a), we show the procedure and the
76"
INTRODUCTION,0.08568329718004339,"results we are able to achieve with each step toward an efficient transformer tracker.
77"
INTRODUCTION,0.08676789587852494,"Compared to previous works, our CompressTracker holds many merits. (1) Enhanced Mimicking
78"
INTRODUCTION,0.0878524945770065,"and Performance. CompressTracker enables the student model to better mimic the teacher model,
79"
INTRODUCTION,0.08893709327548807,"resulting in better performance. As shown in Figure 1, our CompressTracker-4 achieves 2.17× speed
80"
INTRODUCTION,0.09002169197396963,"up while maintaining about 96% accuracy. (2) Simplified Training Process. Our CompressTracker
81"
INTRODUCTION,0.0911062906724512,"streamlines training into a single but efficient step. This simplification not only reduces the time
82"
INTRODUCTION,0.09219088937093275,"and resources required for training but also minimizes the potential for sub-optimal performance
83"
INTRODUCTION,0.09327548806941431,"associated with complex procedures. The training process for CompressTracker-4 requires merely 20
84"
INTRODUCTION,0.09436008676789588,"hours on 8 NVIDIA RTX 3090 GPUs. (3) Heterogeneous Model Compression. Our stage division
85"
INTRODUCTION,0.09544468546637744,"strategy gives a high degree of flexibility in the design of the student model. Our framework supports
86"
INTRODUCTION,0.09652928416485901,"any transformer architecture for student model, which is not restricted to the same structure of teacher
87"
INTRODUCTION,0.09761388286334056,"tracker. The number of layers and their structure are not predetermined but can be tailored to fit the
88"
INTRODUCTION,0.09869848156182212,"specific computational constraints and requirements of the deployment environment.
89"
INTRODUCTION,0.09978308026030369,"Our contribution can be summarized as follows: (1) We introduce a novel and general model
90"
INTRODUCTION,0.10086767895878525,"compression framework, CompressTracker, to facilitate the efficient transformer-based object tracking.
91"
INTRODUCTION,0.1019522776572668,"(2) We propose a stage division strategy that enables a fine-grained imitation of the teacher model at
92"
INTRODUCTION,0.10303687635574837,"the stage level, enhancing the precision and efficiency of knowledge transfer. (3) We propose the
93"
INTRODUCTION,0.10412147505422993,"replacement training to improve the student model’s capacity to replicate the teacher model’s behavior.
94"
INTRODUCTION,0.1052060737527115,"(4) We further incorporate the prediction guidance and feature mimicking to accelerate and refine
95"
INTRODUCTION,0.10629067245119306,"the learning process of the student model. (5) Our CompressTracker breaks structural limitations,
96"
INTRODUCTION,0.10737527114967461,"adapting to various transformer architectures for student model. It outperforms existing models,
97"
INTRODUCTION,0.10845986984815618,"notably accelerating OSTrack [44] by 2.17× while preserving approximately 96% accuracy.
98"
RELATED WORK,0.10954446854663774,"2
Related Work
99"
RELATED WORK,0.11062906724511931,"Visual Object Tracking. Visual object tracking aims to localize the target object of each frame
100"
RELATED WORK,0.11171366594360087,"based on its initial appearance. Previous tracking methods [2, 28, 46, 3, 16, 27, 5, 23, 12, 41]
101"
RELATED WORK,0.11279826464208242,"utilize a two-stream pipeline to decouple the feature extraction and relation modeling. Recently, the
102"
RELATED WORK,0.113882863340564,"one-stream pipeline hold the dominant role. [44, 14, 15, 1, 37, 8, 11, 19] combine feature extraction
103"
RELATED WORK,0.11496746203904555,"and relation modeling into a unified process. These models are built upon vision transformer, which
104"
RELATED WORK,0.11605206073752712,"consists of a series of transformer encoder layers. Thanks to a more adequate relationship modeling
105"
RELATED WORK,0.11713665943600868,"between template and search frame, one-stream models achieve impressive performance. However,
106"
RELATED WORK,0.11822125813449023,"these models suffer from low inference efficiency, which is the main obstacle to practical deployment.
107"
RELATED WORK,0.1193058568329718,"Efficient Tracking. Some works have attempted to speed up tracking models. [42] utilizes neural
108"
RELATED WORK,0.12039045553145336,"architecture search (NAS) to search a light Siamese network, and the searching process is complex.
109"
RELATED WORK,0.12147505422993492,"[6, 10, 4, 26] design a lightweight tracking model, but the small number of parameters restricts the
110"
RELATED WORK,0.12255965292841649,"accuracy to a large degree. MixFormerV2 [15] propose a complex multi-stage model reduction
111"
RELATED WORK,0.12364425162689804,"strategy. Although MixFormerV2-S achieves real-time speed on CPU, the multi-stage training
112"
RELATED WORK,0.12472885032537961,"strategy is time consuming, which requires about 120 hours (5 days) on 8 Nvidia RTX8000 GPUs,
113"
RELATED WORK,0.12581344902386118,"even several times the original training time of MixFormer [14]. Any suboptimal performance
114"
RELATED WORK,0.12689804772234273,"during these stages impact the final model’s performance negatively. Besides, the reduction paradigm
115"
RELATED WORK,0.1279826464208243,"imposes constraints on the design of student models. To address these shortcuts, we propose the
116"
RELATED WORK,0.12906724511930587,"general model compression framework, CompressTracker, to explore the roadmap toward an end-
117"
RELATED WORK,0.1301518438177874,"to-end and traininig-efficient model compression for lightweight transformer-based tracker. Our
118"
RELATED WORK,0.13123644251626898,"CompressTracker break the structure restriction and achieves balance between speed and accuracy.
119"
RELATED WORK,0.13232104121475055,"Transformer Compression. Model compression aims to reduce the size and computational cost of
120"
RELATED WORK,0.1334056399132321,"a large model while retaining as much performance as possible, and recently many attempts have
121"
RELATED WORK,0.13449023861171366,"been made to speed up a large pretrained tranformer model. [18] reduced the number of parameters
122"
RELATED WORK,0.13557483731019523,"through pruning technique, and [35] accomplished the quantization of BERT to 2-bits utilizing
123"
RELATED WORK,0.13665943600867678,"Hessian information. [34, 36, 25, 38] leverage the knowledge distillation to transfer the knowledge
124"
RELATED WORK,0.13774403470715835,"from teacher to student model and exploit pretrained model. Beyond language models, considerable
125"
RELATED WORK,0.13882863340563992,"focus has also been placed on compressing vision transformer models. [33, 40, 9, 20, 7, 43, 45] utilize
126"
RELATED WORK,0.1399132321041215,"multiple model compression techniques to compress vision transformer models. MixFormerV2 [15]
127"
RELATED WORK,0.14099783080260303,Prediction
RELATED WORK,0.1420824295010846,Guidance
RELATED WORK,0.14316702819956617,"Random 
Replacement"
RELATED WORK,0.1442516268980477,Template
RELATED WORK,0.14533622559652928,"Search Area …
…"
RELATED WORK,0.14642082429501085,"Student
Prediction"
RELATED WORK,0.1475054229934924,"Stage 1
Stage 2
Stage N"
RELATED WORK,0.14859002169197397,"Stage 1
Stage 2
Stage N"
RELATED WORK,0.14967462039045554,"Teacher 
Prediction"
RELATED WORK,0.15075921908893708,Template
RELATED WORK,0.15184381778741865,"Search Area …
…"
RELATED WORK,0.15292841648590022,"Prediction
Stage 1
Stage 2"
RELATED WORK,0.1540130151843818,(b) Inference Phase
RELATED WORK,0.15509761388286333,(a) Training Phase
RELATED WORK,0.1561822125813449,Stage N
RELATED WORK,0.15726681127982647,GroundTruth
RELATED WORK,0.15835140997830802,GroundTruth
RELATED WORK,0.1594360086767896,Supervision
RELATED WORK,0.16052060737527116,"Student Layer
Teacher Layer
Trainable
Frozen
Random Path"
RELATED WORK,0.1616052060737527,"Feature
Mimicking"
RELATED WORK,0.16268980477223427,"…
Random 
Replacement"
RELATED WORK,0.16377440347071584,"Random 
Replacement"
RELATED WORK,0.1648590021691974,"Feature
Mimicking"
RELATED WORK,0.16594360086767895,"Feature
Mimicking …"
RELATED WORK,0.16702819956616052,"Figure 2: CompressTracker Framework. (a) In the training phase, we divide both the teacher model
and student model into an identical number of stages. We implement a series of training strategies
including replacement training, prediction guidance, and stage-wise feature mimicking, to enhance
the student model’s ability to emulate the teacher model. The dotted lines represent the randomly
selected paths for replacement training, with black dotted lines indicating the chosen path, while grey
dotted lines denote paths not selected in a specific training iteration. (b) During inference process, we
simply combine each stage of the student model for testing purposes."
RELATED WORK,0.1681127982646421,"proposed a two-stage model reduction paradigm to distill a lightweight tracker, relying on the complex
128"
RELATED WORK,0.16919739696312364,"multi-stage distillation training. However, our CompressTracker propose an end-to-end and efficient
129"
RELATED WORK,0.1702819956616052,"compression training to achieve any transformer structure compression, which speed up OSTrack
130"
RELATED WORK,0.17136659436008678,"2.17× while maintaining about 96% accuracy.
131"
COMPRESSTRACKER,0.17245119305856832,"3
CompressTracker
132"
COMPRESSTRACKER,0.1735357917570499,"In this section, we will introduce our proposed general model compression framework, Com-
133"
COMPRESSTRACKER,0.17462039045553146,"pressTracker. The workflow of our CompressTracker in illustrated in Figure 2.
134"
STAGE DIVISION,0.175704989154013,"3.1
Stage Division
135"
STAGE DIVISION,0.17678958785249457,"Recently, transformer-based one-stream tracking models [8, 14, 44, 15] surpass conventional Siamese
136"
STAGE DIVISION,0.17787418655097614,"trackers [2, 13, 12], becoming the dominant manner in the field of visual object tracking. These
137"
STAGE DIVISION,0.1789587852494577,"models consist of several transformer encoder layers, each generating and progressively refining
138"
STAGE DIVISION,0.18004338394793926,"temporal matching features. Building upon this layer-wise refinement, we introduce the stage division
139"
STAGE DIVISION,0.18112798264642083,"strategy, which segments the model into a series of sequential stages. This approach encourages the
140"
STAGE DIVISION,0.1822125813449024,"student model to emulate the teacher model’s behavior at each individual stage. Specifically, we
141"
STAGE DIVISION,0.18329718004338394,"denote the pretrained tracker and the compressed model as teacher and student model, with Nt and
142"
STAGE DIVISION,0.1843817787418655,"Ns layers, respectively. Both teacher and student models are then divided into Ns stages, where each
143"
STAGE DIVISION,0.18546637744034708,"stage in the student model encompasses a single layer, and each corresponding stage in the teacher
144"
STAGE DIVISION,0.18655097613882862,"model may aggregate multiple layers. For a specific stage i, we establish a correspondence between
145"
STAGE DIVISION,0.1876355748373102,"the stages of the teacher and student models. The objective of stage division is to enforce each stage
146"
STAGE DIVISION,0.18872017353579176,"of the student model to replicate its counterpart in the teacher model. This stage division strategy
147"
STAGE DIVISION,0.1898047722342733,"breaks the traditional approach that treats the model as an indivisible whole [6, 10, 4, 15]. Instead, it
148"
STAGE DIVISION,0.19088937093275488,"enables a fine-grained learning process where the student model transfers knowledge from the teacher
149"
STAGE DIVISION,0.19197396963123645,"in a more detailed, stage-specific manner.
150"
STAGE DIVISION,0.19305856832971802,"Unlike the reduction paradigm adopted in [15], which confines itself to pruning within identical
151"
STAGE DIVISION,0.19414316702819956,"structures, our CompressTracker framework facilitates support for arbitrary transformer structures of
152"
STAGE DIVISION,0.19522776572668113,"the student tracker, thanks to our innovative stage-wise division design. To align the size and channel
153"
STAGE DIVISION,0.1963123644251627,"dimensions of the student model’s temporal matching features with those of the teacher model, we
154"
STAGE DIVISION,0.19739696312364424,"implement input and output projection layers before and after the student layers, respectively. These
155"
STAGE DIVISION,0.1984815618221258,"projection layers serve as an adjustment mechanism to ensure compatibility between the teacher
156"
STAGE DIVISION,0.19956616052060738,"and student models and allow for a broader range of architectural possibilities for the student model.
157"
STAGE DIVISION,0.20065075921908893,"During the inference process, these input and output injection layers are omitted.
158"
REPLACEMENT TRAINING,0.2017353579175705,"3.2
Replacement Training
159"
REPLACEMENT TRAINING,0.20281995661605207,"During the training process, we adopt the replacement training to integrates teacher model and student
160"
REPLACEMENT TRAINING,0.2039045553145336,"models, diverging from the conventional practice of training the student model in isolation. In a
161"
REPLACEMENT TRAINING,0.20498915401301518,"specific training iteration, we implement a stochastic process to determine which stages of the student
162"
REPLACEMENT TRAINING,0.20607375271149675,"model are to be replaced by the corresponding stages of the teacher model. For the specific stage
163"
REPLACEMENT TRAINING,0.20715835140997832,"i, we decide whether to replace or not by random Bernoulli sampling bi with probability p, where
164"
REPLACEMENT TRAINING,0.20824295010845986,"bi ∈{0, 1}. If bi equals 1, the output from the preceding stage i −1 is directed to the i student stage,
165"
REPLACEMENT TRAINING,0.20932754880694143,"otherwise, we channel the output into the i frozen teacher stage. This replacement training creates
166"
REPLACEMENT TRAINING,0.210412147505423,"a collaborative learning environment where the teacher model dynamically supervises the student
167"
REPLACEMENT TRAINING,0.21149674620390455,"model. The unreplaced stages of teacher provide valuable contextual supervision for a specific stage
168"
REPLACEMENT TRAINING,0.21258134490238612,"in the student model. Consequently, the student model is not operating in parallel but is actively
169"
REPLACEMENT TRAINING,0.21366594360086769,"engaged with and learning from the teacher’s established behaviors. For the optimization of student
170"
REPLACEMENT TRAINING,0.21475054229934923,"model, we only require the groundtruth box and denote the loss as Ltrack. Upon completion of the
171"
REPLACEMENT TRAINING,0.2158351409978308,"training process, the student model’s stages are harmoniously combined for inference. We show the
172"
REPLACEMENT TRAINING,0.21691973969631237,"pseudocode code in Appendix A.1.
173"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.21800433839479394,"3.3
Prediction Guidance & Stage-wise Feature Mimicking
174"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.21908893709327548,"Replacement training enables the student model to learn the behavior of each individual stage,
175"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22017353579175705,"resulting in enhanced performance. However, merely forcing student model to emulate teacher model
176"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22125813449023862,"may be overly challenging for a smaller-sized student. Thus, we employ the teacher’s predictions to
177"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22234273318872017,"further guide the learning of compressed tracker. We apply the same loss as Ltrack for prediction
178"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22342733188720174,"guidance, which is denoted as Lpred. With the aid of prediction guidance, student benefits from a
179"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.2245119305856833,"quicker and stable learning process, assimilating knowledge from teacher model more effectively.
180"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22559652928416485,"While prediction guidance accelerates the convergence, the student tracker might not entirely match
181"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22668112798264642,"the complex behavior of the teacher model. We introduce the stage-wise feature mimicking to further
182"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.227765726681128,"synchronize the temporal matching features between corresponding stages of the teacher and student
183"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.22885032537960953,"models. This alignment is quantified by calculating the L2 distance between the outputs of these
184"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.2299349240780911,"stages, which is referred as Lfeat. It is worth noting that any metric assessing the discrepancy in
185"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.23101952277657267,"feature distributions can serve as the loss function. However, we choose a simple L2 distance rather
186"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.23210412147505424,"than a complex loss to highlight the effectiveness of our stage division and replacement training
187"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.23318872017353579,"strategies. The stage-wise feature mimicking not only promotes a closer similarity in the feature
188"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.23427331887201736,"representations of corresponding stages but also enhances the overall coherence between the teacher
189"
PREDICTION GUIDANCE & STAGE-WISE FEATURE MIMICKING,0.23535791757049893,"and student models.
190"
PROGRESSIVE REPLACEMENT,0.23644251626898047,"3.4
Progressive Replacement
191"
PROGRESSIVE REPLACEMENT,0.23752711496746204,"In Section 3.2, we describe the replacement training strategy. Although setting the Bernoulli sampling
192"
PROGRESSIVE REPLACEMENT,0.2386117136659436,"probability p as a constant value can realize the compression, these stages have not been trained
193"
PROGRESSIVE REPLACEMENT,0.23969631236442515,"together at the same time and there may be some dissonance. A further finetuning step is necessary
194"
PROGRESSIVE REPLACEMENT,0.24078091106290672,"to achieve better harmony among the stages. Thus, we introduce a progressive replacement strategy
195"
PROGRESSIVE REPLACEMENT,0.2418655097613883,"to bridges the gap between the two initially separate training phases, fostering an end-to-end easy-to-
196"
PROGRESSIVE REPLACEMENT,0.24295010845986983,"hard learning process. By adjusting the value of p, we can control the number of stages to be replaced.
197"
PROGRESSIVE REPLACEMENT,0.2440347071583514,"The value of p gradually increases from pinit to 1.0, allowing for a more incremental and coherent
198"
PROGRESSIVE REPLACEMENT,0.24511930585683298,"training progression:
199 p = 
 "
PROGRESSIVE REPLACEMENT,0.24620390455531455,"pinit,
0 <= t < α1m,
pinit + pinit
t−α1m
(1−α1−α2)m,
α1m <= t <= (1 −α2)m,
1.0,
(1 −α2)m < t <= m,
(1)"
PROGRESSIVE REPLACEMENT,0.2472885032537961,"where m represents the total number of training epochs, and t is a specific training epoch, α1 and
200"
PROGRESSIVE REPLACEMENT,0.24837310195227766,"α2 are hyper parameters to modulate the training process. Specifically, α1 controls the duration of
201"
PROGRESSIVE REPLACEMENT,0.24945770065075923,"Method
LaSOT
LaSOText
TNL2K
TrackingNet
UAV123
FPS
AUC
PNorm
P
AUC
P
AUC
P
AUC
PNorm
P
AUC
P"
PROGRESSIVE REPLACEMENT,0.25054229934924077,"OSTrack-256 [44]
69.1
78.7
75.2
47.4
53.3
54.3
-
83.1
87.8
82.0
68.3
-
105
CompressTracker-2
60.4 87%
68.5
61.5
40.4 85%
43.8
48.5 89%
45.0
78.2 94%
83.3
74.8
62.5 92%
82.5
346 3.30×
CompressTracker-3
64.9 94%
74.0
68.4
44.6 94%
49.6
52.6 97%
50.9
81.6 98%
86.7
79.4
65.4 96%
88.3
267 2.54×
CompressTracker-4
66.1 96%
75.2
70.6
45.7 96%
50.8
53.6 99%
52.5
82.1 99%
87.6
80.1
67.4 99%
88.0
228 2.17×
CompressTracker-6
67.5 98%
77.5
72.4
46.7 99%
52.5
54.7 101%
54.3
82.9 99%
87.8
81.5
67.9 99%
88.7
162 1.54×
CompressTracker-8
68.4 99%
78.0
73.1
47.2 99%
53.1
55.2 102%
54.8
83.3 101%
88.0
81.9
68.2 99%
89.0
127 1.21×
Table 1: Compress OSTrack. We compress OSTrack multiple configurations with different layer
settings. CompressTracker-x denotes the compressed student model with ’x’ layers. We report the
performance on 5 benchmarks and calculate the performance gap in comparison to the original
OSTrack. Our CompressTracker effectively achieves the balance between performance and speed."
PROGRESSIVE REPLACEMENT,0.25162689804772237,"Method
LaSOT
LaSOText
TNL2K
TrackingNet
UAV123
FPS
AUC
PNorm
P
AUC
P
AUC
P
AUC
PNorm
P
AUC
P"
PROGRESSIVE REPLACEMENT,0.2527114967462039,"MixFormerV2-B [15]
70.6
80.8
76.2
50.6
56.9
57.4
58.4
83.4
88.1
81.6
69.9
92.1
165
MixFormerV2-S [15]
60.6
69.9
60.4
43.6
46.2
48.3
43.0
75.8
81.1
70.4
65.8
86.8
325
CompressTracker-M-S
62.0 88%
70.9
63.2
44.5 88%
47.1
50.2 87%
47.8
77.7 93%
82.5
73.0
66.9 96%
87.1
325 1.97×
Table 2: Compress MixFormerV2. We compress MixFormerV2 into CompressTracker-M-S with
4 layers, which is the same as MixFormerV2-S including the dimension of MLP layer. We report
the performance on 5 benchmarks and calculate the performance gap in comparison to the origin
MixFormerV2-B. Our CompressTracker-M-S outperforms MixFormerV2-S under the same setting.
warmup process, whereas α2 determines the length of final finetuning process. The mathematical
202"
PROGRESSIVE REPLACEMENT,0.25379609544468545,"expectation of p for each layer is:
203"
PROGRESSIVE REPLACEMENT,0.25488069414316705,"E(p) =
Z m"
PROGRESSIVE REPLACEMENT,0.2559652928416486,"0
pdt = [1 + pinit"
PROGRESSIVE REPLACEMENT,0.25704989154013014,"2
+ 1 −pinit"
PROGRESSIVE REPLACEMENT,0.25813449023861174,"2
(α2 −α1)]m.
(2)"
PROGRESSIVE REPLACEMENT,0.2592190889370933,"It is worth noting that each layer is optimized fewer times than the total iteration count, according to
204"
PROGRESSIVE REPLACEMENT,0.2603036876355748,"the mathematical expectation. Through dynamically adjusting the replacement rate p, we eliminate
205"
PROGRESSIVE REPLACEMENT,0.2613882863340564,"the requirement of finetuning and accomplish an end-to-end model compression.
206"
TRAINING AND INFERENCE,0.26247288503253796,"3.5
Training and Inference
207"
TRAINING AND INFERENCE,0.2635574837310195,"Our CompressTracker is a general framework applicable to a wide array of student model architectures.
208"
TRAINING AND INFERENCE,0.2646420824295011,"For the optimization of student model, our CompressTracker solely requires an end-to-end and easy-
209"
TRAINING AND INFERENCE,0.26572668112798264,"to-hand training process instead of multi-stage training methodologies. Furthermore, our approach
210"
TRAINING AND INFERENCE,0.2668112798264642,"simplifies the loss function design, eliminating the need for complex formulations. During training,
211"
TRAINING AND INFERENCE,0.2678958785249458,"teacher model is frozen and we only optimize student tracker. The total loss for CompressTracker is:
212"
TRAINING AND INFERENCE,0.26898047722342733,"L = λtrackLtrack + λpredLpred + λfeatLfeat.
(3)"
TRAINING AND INFERENCE,0.27006507592190887,"After training, the various stages of the student model are combined to create a unified model for the
213"
TRAINING AND INFERENCE,0.27114967462039047,"inference phase. Consistent with previous methods [44, 14], a Hanning window penalty is adopted.
214"
EXPERIMENTS,0.272234273318872,"4
Experiments
215"
IMPLEMENT DETAILS,0.27331887201735355,"4.1
Implement Details
216"
IMPLEMENT DETAILS,0.27440347071583515,"Our framework CompressTracker is general and not dependent on a specific transformer structure,
217"
IMPLEMENT DETAILS,0.2754880694143167,"hence we select OSTrack [44] as baseline, which is a simple and effective transformer-based tracker.
218"
IMPLEMENT DETAILS,0.2765726681127983,"The training datasets consist of LaSOT [17], TrackingNet [32], GOT-10K [24], and COCO [29],
219"
IMPLEMENT DETAILS,0.27765726681127983,"following OSTrack [44] and MixFormerV2 [15]. We set λtrack as 1, λpred as 1, and λfeat as 0.2.
220"
IMPLEMENT DETAILS,0.2787418655097614,"The pinit is set as 0.5. We train the CompressTracker with AdamW optimizer [31], with the weight
221"
IMPLEMENT DETAILS,0.279826464208243,"decay as 10−4 and the initial learning rate of 4 × 10−5. The batch size is 128. The total training
222"
IMPLEMENT DETAILS,0.2809110629067245,"epochs is 500 with 60K image pairs per epoch andthe learning rate is reduced by a factor of 10 after
223"
IMPLEMENT DETAILS,0.28199566160520606,"400 epochs. α1 and α2 are set as 0.1. The search and template images are resized to resolutions
224"
IMPLEMENT DETAILS,0.28308026030368766,"of 288 × 288 and 128 × 128. We initialize the CompressTracker with the pretrained parameters of
225"
IMPLEMENT DETAILS,0.2841648590021692,"OSTrack. We report the inference speed on a NVIDIA RTX 2080Ti GPU.
226"
IMPLEMENT DETAILS,0.28524945770065074,"Method
LaSOT
LaSOText
TNL2K
TrackingNet
UAV123
FPS
AUC
PNorm
P
AUC
P
AUC
P
AUC
PNorm
P
AUC
P"
IMPLEMENT DETAILS,0.28633405639913234,"OSTrack-256 [44]
69.1
78.7
75.2
47.4
53.3
54.3
-
83.1
87.8
82.0
68.3
-
105
SMAT [21]
61.7
71.1
64.6
-
-
-
-
78.6
84.2
75.6
64.3
83.9
158
CompressTracker-SMAT
62.8 91%
72.2
64.0
43.4 92%
46.0
49.6 91%
46.9
79.7 96%
85.0
75.4
65.9 96%
86.4
138 1.31×
Table 3: Compress OSTrack for SMAT. We compress OSTrack into CompressTracker-SAMT
with 4 SMAT layers, which is the same as SMAT. We report the performance on 5 benchmarks and
calculate the performance gap in comparison to the original OSTrack. Our CompressTracker-SAMT
outperforms SMAT under the same setting."
IMPLEMENT DETAILS,0.2874186550976139,"Method
LaSOT
LaSOText
TNL2K
TrackingNet
UAV123
FPS
AUC
PNorm
P
AUC
P
AUC
P
AUC
PNorm
P
AUC
P"
IMPLEMENT DETAILS,0.2885032537960954,"CompressTracker-2
60.4
68.5
61.5
40.4
43.8
48.5
45.0
78.2
83.3
74.8
62.5
82.5
346
CompressTracker-3
64.9
74.0
68.4
44.6
49.6
52.6
50.9
81.6
86.7
79.4
65.4
88.3
267
CompressTracker-4
66.1
75.2
70.6
45.7
50.8
53.6
52.5
82.1
87.6
80.1
67.4
88.0
228
CompressTracker-6
67.5
77.5
72.4
46.7
52.5
54.7
54.3
82.9
87.8
81.5
67.9
88.7
162
CompressTracker-8
68.4
78.0
73.1
47.2
53.1
55.2
54.8
83.3
88.0
81.9
68.2
89.0
127"
IMPLEMENT DETAILS,0.289587852494577,"HiT-Base [26]
64.6
73.3
68.1
44.1
-
-
-
80.0
84.4
77.3
65.6
-
175
HiT-Samll [26]
60.5
68.3
61.5
40.4
-
-
-
77.7
81.9
73.1
63.3
-
192
HiT-Tiny [26]
54.8
60.5
52.9
35.8
-
-
-
74.6
78.1
68.8
53.2
-
204
SMAT [21]
61.7
71.1
64.6
-
-
-
-
78.6
84.2
75.6
64.3
83.9
158
MixFormerV2-S [15]
60.6
69.9
60.4
43.6
46.2
48.3
43.0
75.8
81.1
70.4
65.8
86.8
325
FEAR-L [6]
57.9
68.6
60.9
-
-
-
-
-
-
-
-
-
-
FEAR-XS [6]
53.5
64.1
54.5
-
-
-
-
-
-
-
-
-
80
HCAT [10]
59.0
68.3
60.5
-
-
-
-
76.6
82.6
72.9
63.6
-
195
E.T.Track [4]
59.1
-
-
-
-
-
-
74.5
80.3
70.6
62.3
-
150
LightTrack-LargeA [42]
55.5
-
56.1
-
-
-
-
73.6
78.8
70.0
-
-
-
LightTrack-Mobile [42]
53.8
-
53.7
-
-
-
-
72.5
77.9
69.5
-
-
120
STARK-Lightning [41]
58.6
69.0
57.9
-
-
-
-
-
-
-
-
-
200
DiMP [3]
56.9
65.0
56.7
-
-
-
-
74.0
80.1
68.7
65.4
-
77
SiamFC++ [39]
54.4
62.3
54.7
-
-
-
-
75.4
80.0
70.5
-
-
90"
IMPLEMENT DETAILS,0.29067245119305857,"Table 4: State-of-the-art comparison. We compare our CompressTracker which is compressed from
OSTrack with previous light-weight tracking models. Our CompressTracker demonstrates superior
performance over previous models.
4.2
Compress Object Tracker
227"
IMPLEMENT DETAILS,0.2917570498915401,"In this section, we compress the pretrained OSTrack into different layer configurations. We report
228"
IMPLEMENT DETAILS,0.2928416485900217,"the performance of our CompressTracker across these configurations in Table 1. CompressTracker-
229"
IMPLEMENT DETAILS,0.29392624728850325,"4 compress OSTrack from 12 layers into 4 layers, and maintain 96% and 99% performance on
230"
IMPLEMENT DETAILS,0.2950108459869848,"LaSOT and TrackingNet while achieving 2.17× speed up. Furthermore, as shown in Figure 1,
231"
IMPLEMENT DETAILS,0.2960954446854664,"the training process of CompressTracker-4 is notably efficient, requiring only approximately 20
232"
IMPLEMENT DETAILS,0.29718004338394793,"hours using 8 NVIDIA RTX 3090 GPUs. For CompressTracker-6 and CompressTracker-8, as
233"
IMPLEMENT DETAILS,0.2982646420824295,"we increase the number of layers, the performance gap between our compresstracker and OSTrack
234"
IMPLEMENT DETAILS,0.2993492407809111,"diminishes. It is worth noting that our CompressTracker even outperforms the origin OSTrack on some
235"
IMPLEMENT DETAILS,0.3004338394793926,"benchmarks. Specifically, CompressTracker-6 reaches 54.7% AUC on TNL2K, and CompressTracker-
236"
IMPLEMENT DETAILS,0.30151843817787416,"8 achieves 55.2% AUC on TNL2K and 83.3% AUC on TrackingNet, while the origin OSTrack only
237"
IMPLEMENT DETAILS,0.30260303687635576,"achieves 54.3% AUC on TNL2K and 83.1% AUC on TrackingNet. Our framework CompressTracker
238"
IMPLEMENT DETAILS,0.3036876355748373,"demonstrates near lossless compression with the added benefit of increased processing speed.
239"
IMPLEMENT DETAILS,0.3047722342733189,"Moreover, to affirm the generalization ability of our approach, we conduct experiments on Mix-
240"
IMPLEMENT DETAILS,0.30585683297180044,"FormerV2 [15] and SMAT [21]. MixFormerV2-S is a fully transformer tracking model consisting
241"
IMPLEMENT DETAILS,0.306941431670282,"of 4 transformer layers, trained via a complex multi-stages model reduction paradigm. Following
242"
IMPLEMENT DETAILS,0.3080260303687636,"MixFormerV2-S, we adopt MixFormerV2-B as teacher and compress it to a student model with
243"
IMPLEMENT DETAILS,0.3091106290672451,"4 layers. The results are shown in Table 2. Our CompressTracker-M-S share the same structure
244"
IMPLEMENT DETAILS,0.31019522776572667,"and channel dimension of MLP layers with MixFormerV2-S and outperforms MixFormerV2-S by
245"
IMPLEMENT DETAILS,0.31127982646420826,"about 1.4% AUC on LaSOT. SMAT replace the vanilla attention in transformer layer with sepa-
246"
IMPLEMENT DETAILS,0.3123644251626898,"rated attention. We compress OSTrack into a student model CompressTracker-SMAT, aligning the
247"
IMPLEMENT DETAILS,0.31344902386117135,"number and structure of transformer layer with SAMT. We maintain the decoder of OSTrack for
248"
IMPLEMENT DETAILS,0.31453362255965295,"CompressTracker-SMAT. CompressTracker-SMAT surpasses SMAT by 1.1% AUC on LaSOT, which
249"
IMPLEMENT DETAILS,0.3156182212581345,"demonstrates that our framework is flexible and not limited by the structure of transformer layer.
250"
IMPLEMENT DETAILS,0.31670281995661603,"Results in Table 1, 2, 3 verify the generalization ability and effectiveness of our framework.
251"
COMPARISON WITH STATE-OF-THE-ARTS,0.31778741865509763,"4.3
Comparison with State-of-the-arts
252"
COMPARISON WITH STATE-OF-THE-ARTS,0.3188720173535792,"To demonstrate the effectiveness of our CompressTracker, we compare our CompressTracker with
253"
COMPARISON WITH STATE-OF-THE-ARTS,0.3199566160520607,"state-of-the-art efficient trackers in 5 benchmarks. As shown in Table 4, our CompressTracker
254"
COMPARISON WITH STATE-OF-THE-ARTS,0.3210412147505423,"#
Init. method
AUC
1
MAE-first4
59.9%
2
OSTrack-first4
62.0%
3
OSTrack-skip4
62.3%"
COMPARISON WITH STATE-OF-THE-ARTS,0.32212581344902386,"Table 5: Backbone Initialization. ‘MAE-
first4’ denotes initializing the student model
using the first 4 layers of MAE-B. ‘OSTrack-
skip4’ represents utilizing every fourth layer
of OSTrack for the student model."
COMPARISON WITH STATE-OF-THE-ARTS,0.3232104121475054,"#
Init. & Opt.
AUC
1
Random & Trainable
62.3%
2
Teacher & Frozen
62.6%
3
Teacher & Trainable
62.8%"
COMPARISON WITH STATE-OF-THE-ARTS,0.324295010845987,"Table 6: Decoder Initialization and Optimization.
‘Random’ denotes randomly initialized decoder, and
’Teacher’ means the decoder is initialized with teacher
parameters. ‘Frozen’ represents that the decoder is
frozen, and ’Trainable’ denotes decoder is trainable."
COMPARISON WITH STATE-OF-THE-ARTS,0.32537960954446854,"#
Layer Split
AUC
1
Even
62.8%
2
Uneven
62.7%"
COMPARISON WITH STATE-OF-THE-ARTS,0.3264642082429501,"Table 7: Stage Division. ‘Even’ denotes evenly divid-
ing stage strategy, and ‘Uneven’ means that the layer
number of each stage in teacher model is 2,2,6,2."
COMPARISON WITH STATE-OF-THE-ARTS,0.3275488069414317,"#
Epochs
AUC
1
300
65.2%
2
500
66.1%"
COMPARISON WITH STATE-OF-THE-ARTS,0.3286334056399132,"Table 8: Training Epochs. ’300’
and ’500’ denote the total training
epochs."
COMPARISON WITH STATE-OF-THE-ARTS,0.3297180043383948,Table 9: Ablation studies on LaSOT. The default choice for our model is colored in gray .
COMPARISON WITH STATE-OF-THE-ARTS,0.33080260303687636,"outperforms previous efficient trackers. Both HiT [26] and SMAT [21] are solely trained on the
255"
COMPARISON WITH STATE-OF-THE-ARTS,0.3318872017353579,"groundtruth and reduce computation through specialized network architectures. MixFormerV2-S [15]
256"
COMPARISON WITH STATE-OF-THE-ARTS,0.3329718004338395,"achieves model compression via a model reduction paradigm. Our CompressTracker-4 achieves
257"
COMPARISON WITH STATE-OF-THE-ARTS,0.33405639913232105,"66.1% AUC on LaSOT while maintaining 228 FPS. CompressTracker-4 outperforms HiT-Base
258"
COMPARISON WITH STATE-OF-THE-ARTS,0.3351409978308026,"by 1.5% AUC on LaSOT without any specialized model structure design. CompressTracker-4
259"
COMPARISON WITH STATE-OF-THE-ARTS,0.3362255965292842,"achieves the balance between speed and accuracy. Meanwhile, our CompressTracker-2, with just two
260"
COMPARISON WITH STATE-OF-THE-ARTS,0.33731019522776573,"transformer layers, maintains the highest speed at 346 FPS and also obtains competitive performance.
261"
COMPARISON WITH STATE-OF-THE-ARTS,0.3383947939262473,"CompressTracker-2 surpasses HiT-Tiny by 5.6% AUC on LaSOT, and achieves about the same
262"
COMPARISON WITH STATE-OF-THE-ARTS,0.33947939262472887,"performance as MixFormerV2-S with only two transformer layers. As we add more transformer
263"
COMPARISON WITH STATE-OF-THE-ARTS,0.3405639913232104,"layers with CompressTracker-6 and CompressTracker-8, we see further improvements in performance.
264"
COMPARISON WITH STATE-OF-THE-ARTS,0.34164859002169196,"These outcomes demonstrate the effectiveness of our CompressTracker framework.
265"
COMPARISON WITH STATE-OF-THE-ARTS,0.34273318872017355,"#
Prediction
Guidance
Feature
Mimicking
Replacement
Traininig
AUC"
COMPARISON WITH STATE-OF-THE-ARTS,0.3438177874186551,"1
62.8
2
✓
63.5
3
✓
63.3
4
✓
63.7
5
✓
✓
64.1
6
✓
✓
64.5
7
✓
✓
64.3
8
✓
✓
✓
65.2"
COMPARISON WITH STATE-OF-THE-ARTS,0.34490238611713664,"Table 10: Ablation studies on LaSOT to
analyze the supervision of student model.
The default choice for our model is col-
ored in gray ."
COMPARISON WITH STATE-OF-THE-ARTS,0.34598698481561824,"CompressTracker-2
CompressTracker-3
CompressTracker-4
CompressTracker-6
CompressTracker-8
56 58 60 62 64 66 68 AUC % 56.6 60.7 62.8 63.6 64.9 58.6 62.4 63.8 65.7 66.7 60.1 64.3 65.2 66.5 67.5"
COMPARISON WITH STATE-OF-THE-ARTS,0.3470715835140998,"Naive Training
Distill Training
CompressTracker"
COMPARISON WITH STATE-OF-THE-ARTS,0.3481561822125813,Figure 3: Ablation study on training strategy. 266
ABLATION STUDY,0.3492407809110629,"4.4
Ablation Study
267"
ABLATION STUDY,0.35032537960954446,"In this section, we conduct a series of ablation studies on LaSOT to explore the factors contributing to
268"
ABLATION STUDY,0.351409978308026,"the effectiveness of our CompressTracker. Unless otherwise specified, the teacher model is OSTrack,
269"
ABLATION STUDY,0.3524945770065076,"and the student model has 4 encoder layers. The student model is trained for 300 epochs. Please see
270"
ABLATION STUDY,0.35357917570498915,"Appendix A.2 for more analysis.
271"
ABLATION STUDY,0.3546637744034707,"Backbone Initialization. We initialize the backbone of student model with different parameters and
272"
ABLATION STUDY,0.3557483731019523,"only train the student model with groundtruth supervision. The results are shown in Table 5. It can
273"
ABLATION STUDY,0.35683297180043383,"be observed that utilizing the knowledge from teacher model is crucial. Moreover, initializing with
274"
ABLATION STUDY,0.3579175704989154,"skipped layers (#3) yields slightly better performance than continuous layers. This suggests that
275"
ABLATION STUDY,0.35900216919739697,"initialization with skipped layers leads to improved representation similarity.
276"
ABLATION STUDY,0.3600867678958785,"Decoder Initialization and Optimization. We investigate the influence of decoder’s initialization and
277"
ABLATION STUDY,0.3611713665943601,"optimization on the accuracy of student tracker in Table 6. Initializing the decoder with parameters
278"
ABLATION STUDY,0.36225596529284165,"from the teacher model (#2) results in an improvement of approximately 0.3% compared to a decoder
279"
ABLATION STUDY,0.3633405639913232,"initialized randomly (#1), which underscores the benefits of transferring knowledge from the teacher
280"
ABLATION STUDY,0.3644251626898048,"model to enhance the accuracy of the student model’s decoder. Furthermore, making the decoder
281"
ABLATION STUDY,0.36550976138828634,"trainable leads to an additional improvement of 0.2%.
282"
ABLATION STUDY,0.3665943600867679,"Stage Division. Our stage division strategy divides the teacher model into the several stages, and we
283"
ABLATION STUDY,0.3676789587852495,"explore the stage division strategy in Table 7. We design two kinds of division strategy: even and
284"
ABLATION STUDY,0.368763557483731,"uneven, For the even division, we evenly split the teacher model’s 12 layers into 4 stages, with each
285"
ABLATION STUDY,0.36984815618221256,"stage comprising 3 layers. For uneven division, we follow the design manner in [22, 30] and divide
286"
ABLATION STUDY,0.37093275488069416,"the 12 layers at a ratio of 1:1:3:1. Consequently, the number of layers in each stage of the teacher
287"
ABLATION STUDY,0.3720173535791757,"model is 2, 2, 6, and 2, respectively. The performance of the two approaches is comparable, leading
288"
ABLATION STUDY,0.37310195227765725,"us to select the equal division strategy for simplicity.
289"
ABLATION STUDY,0.37418655097613884,"Analysis on Supervision. We conduct a series of experiments to comprehensively analyze the
290"
ABLATION STUDY,0.3752711496746204,"supervision effects on the student model and to verify the effectiveness of our proposed training
291"
ABLATION STUDY,0.37635574837310193,"strategy. Results are presented in Table 10. Our proposed replacement training approach (#4)
292"
ABLATION STUDY,0.3774403470715835,"improves by 0.9 % AUC compared to singly training student model on groundtruth (#1), which
293"
ABLATION STUDY,0.37852494577006507,"demonstrates that the replacement training enhances the similarity between teacher and student
294"
ABLATION STUDY,0.3796095444685466,"models. Besides, prediction guidance (#5) and feature mimicking (#8) further boost the performance,
295"
ABLATION STUDY,0.3806941431670282,"indicating the effectiveness of the two strategies. Compared to only training on groundtruth (#1), our
296"
ABLATION STUDY,0.38177874186550975,"proposed replacement training, prediction guidance and feature mimicking collectively assist student
297"
ABLATION STUDY,0.38286334056399135,"model in more closely mimicking the teacher model, resulting in a total increase of 2.4% AUC.
298"
ABLATION STUDY,0.3839479392624729,"To further explore the generalization ability of our proposed training strategy, we compare the
299"
ABLATION STUDY,0.38503253796095444,"performance of models with different layer numbers and training settings, as illustrated in Figure 3.
300"
ABLATION STUDY,0.38611713665943603,"’Naive Training’ denotes that the student model is trained without teacher supervision and replacement
301"
ABLATION STUDY,0.3872017353579176,"training. ’Distill Training’ represents that the student model is trained only with teacher supervision.
302"
ABLATION STUDY,0.3882863340563991,"’CompressTracker’ refers to the same training setting in Table 10 #8. It can be observed that as the
303"
ABLATION STUDY,0.3893709327548807,"number of layers increases, there is a corresponding improvement in accuracy. Our CompressTracker
304"
ABLATION STUDY,0.39045553145336226,"shows a noticeable performance boost due to our proposed training strategy, which verifies the
305"
ABLATION STUDY,0.3915401301518438,"effectiveness and generalization ability of our framework.
306"
ABLATION STUDY,0.3926247288503254,"Training Epochs. Based on the analysis in Section 3.4, the optimization steps for each layer are
307"
ABLATION STUDY,0.39370932754880694,"lower than total training steps. Thus, to ensure adequate training of each stage, we increase the
308"
ABLATION STUDY,0.3947939262472885,"training epochs from 300 to 500, and show the result in Table 8. Extending the training epochs
309"
ABLATION STUDY,0.3958785249457701,"ensures that student models receive comprehensive training, leading to improved accuracy.
310"
LIMITATION,0.3969631236442516,"5
Limitation
311"
LIMITATION,0.39804772234273317,"While our CompressTracker demonstrates promising performance and generalization, its training
312"
LIMITATION,0.39913232104121477,"is somewhat inefficient, requiring about 2× time compared to training a student model on ground
313"
LIMITATION,0.4002169197396963,"truth data (20h vs. 8h on 8 NVIDIA 3090 GPUs, as shown in Figure 1 (a)). Moreover, a performance
314"
LIMITATION,0.40130151843817785,"gap still exists between the teacher and student models suggests room for improvement in lossless
315"
LIMITATION,0.40238611713665945,"compression. Future efforts will focus on developing more efficient training methods to boost student
316"
LIMITATION,0.403470715835141,"model accuracy and decrease training duration.
317"
BROADER IMPACTS,0.40455531453362253,"6
Broader Impacts
318"
BROADER IMPACTS,0.40563991323210413,"Our CompressTracker framework efficiently compresses object tracking models for edge device
319"
BROADER IMPACTS,0.4067245119305857,"deployment but poses potential misuse risks, such as unauthorized surveillance. We recommend users
320"
BROADER IMPACTS,0.4078091106290672,"to carefully consider the real-world implications and adopt risk mitigation strategies.
321"
CONCLUSION,0.4088937093275488,"7
Conclusion
322"
CONCLUSION,0.40997830802603036,"In this paper, we propose a general compression framework, CompressTracker, for visual object
323"
CONCLUSION,0.41106290672451196,"tracking. We propose a novel stage division strategy to separate the structural dependencies between
324"
CONCLUSION,0.4121475054229935,"the student and teacher models. We propose the replacement training to enhance student’s ability
325"
CONCLUSION,0.41323210412147504,"to emulate the teacher model. We further introduce the prediction guidance and stage-wise feature
326"
CONCLUSION,0.41431670281995664,"mimicking to improve performance. Extensive experiments verify the effectiveness and generalization
327"
CONCLUSION,0.4154013015184382,"ability of our CompressTracker. Our CompressTracker is capable of accelerating tracking models
328"
CONCLUSION,0.4164859002169197,"while preserving performance to the greatest extent possible.
329"
REFERENCES,0.4175704989154013,"References
330"
REFERENCES,0.41865509761388287,"[1] Yifan Bai, Zeyang Zhao, Yihong Gong, and Xing Wei. Artrackv2: Prompting autoregressive tracker where
331"
REFERENCES,0.4197396963123644,"to look and how to describe. arXiv preprint arXiv:2312.17133, 2023.
332"
REFERENCES,0.420824295010846,"[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional
333"
REFERENCES,0.42190889370932755,"siamese networks for object tracking. In Computer Vision–ECCV 2016 Workshops: Amsterdam, The
334"
REFERENCES,0.4229934924078091,"Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part II 14, pages 850–865. Springer, 2016.
335"
REFERENCES,0.4240780911062907,"[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model
336"
REFERENCES,0.42516268980477223,"prediction for tracking. In Proceedings of the IEEE/CVF international conference on computer vision,
337"
REFERENCES,0.4262472885032538,"pages 6182–6191, 2019.
338"
REFERENCES,0.42733188720173537,"[4] Philippe Blatter, Menelaos Kanakis, Martin Danelljan, and Luc Van Gool. Efficient visual tracking with
339"
REFERENCES,0.4284164859002169,"exemplar transformers. In Proceedings of the IEEE/CVF Winter conference on applications of computer
340"
REFERENCES,0.42950108459869846,"vision, pages 1571–1581, 2023.
341"
REFERENCES,0.43058568329718006,"[5] David S Bolme, J Ross Beveridge, Bruce A Draper, and Yui Man Lui. Visual object tracking using adaptive
342"
REFERENCES,0.4316702819956616,"correlation filters. In 2010 IEEE computer society conference on computer vision and pattern recognition,
343"
REFERENCES,0.43275488069414314,"pages 2544–2550. IEEE, 2010.
344"
REFERENCES,0.43383947939262474,"[6] Vasyl Borsuk, Roman Vei, Orest Kupyn, Tetiana Martyniuk, Igor Krashenyi, and Jiˇri Matas. Fear: Fast,
345"
REFERENCES,0.4349240780911063,"efficient, accurate and robust visual tracker. In European Conference on Computer Vision, pages 644–663.
346"
REFERENCES,0.4360086767895879,"Springer, 2022.
347"
REFERENCES,0.4370932754880694,"[7] Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, and Eric P Xing. Vision
348"
REFERENCES,0.43817787418655096,"transformer slimming: Multi-dimension searching in continuous optimization space. In Proceedings of the
349"
REFERENCES,0.43926247288503256,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4931–4941, 2022.
350"
REFERENCES,0.4403470715835141,"[8] Boyu Chen, Peixia Li, Lei Bai, Lei Qiao, Qiuhong Shen, Bo Li, Weihao Gan, Wei Wu, and Wanli Ouyang.
351"
REFERENCES,0.44143167028199565,"Backbone is all your need: A simplified architecture for visual object tracking. In European Conference on
352"
REFERENCES,0.44251626898047725,"Computer Vision, pages 375–392. Springer, 2022.
353"
REFERENCES,0.4436008676789588,"[9] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers for
354"
REFERENCES,0.44468546637744033,"visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages
355"
REFERENCES,0.44577006507592193,"12270–12280, 2021.
356"
REFERENCES,0.44685466377440347,"[10] Xin Chen, Ben Kang, Dong Wang, Dongdong Li, and Huchuan Lu. Efficient visual tracking via hierarchical
357"
REFERENCES,0.447939262472885,"cross-attention transformer. In European Conference on Computer Vision, pages 461–477. Springer, 2022.
358"
REFERENCES,0.4490238611713666,"[11] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu. Seqtrack: Sequence to sequence learning
359"
REFERENCES,0.45010845986984815,"for visual object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
360"
REFERENCES,0.4511930585683297,"Recognition, pages 14572–14581, 2023.
361"
REFERENCES,0.4522776572668113,"[12] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In
362"
REFERENCES,0.45336225596529284,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8126–8135,
363"
REFERENCES,0.4544468546637744,"2021.
364"
REFERENCES,0.455531453362256,"[13] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, and Rongrong Ji. Siamese box adaptive
365"
REFERENCES,0.4566160520607375,"network for visual tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern
366"
REFERENCES,0.45770065075921906,"recognition, pages 6668–6677, 2020.
367"
REFERENCES,0.45878524945770066,"[14] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. Mixformer: End-to-end tracking with iterative
368"
REFERENCES,0.4598698481561822,"mixed attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
369"
REFERENCES,0.4609544468546638,"pages 13608–13618, 2022.
370"
REFERENCES,0.46203904555314534,"[15] Yutao Cui, Tianhui Song, Gangshan Wu, and Limin Wang. Mixformerv2: Efficient fully transformer
371"
REFERENCES,0.4631236442516269,"tracking. Advances in Neural Information Processing Systems, 36, 2024.
372"
REFERENCES,0.4642082429501085,"[16] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking
373"
REFERENCES,0.46529284164859,"by overlap maximization. In Proceedings of the IEEE/CVF conference on computer vision and pattern
374"
REFERENCES,0.46637744034707157,"recognition, pages 4660–4669, 2019.
375"
REFERENCES,0.46746203904555317,"[17] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and
376"
REFERENCES,0.4685466377440347,"Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In Proceedings of the
377"
REFERENCES,0.46963123644251625,"IEEE/CVF conference on computer vision and pattern recognition, pages 5374–5383, 2019.
378"
REFERENCES,0.47071583514099785,"[18] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
379"
REFERENCES,0.4718004338394794,"networks. arXiv preprint arXiv:1803.03635, 2018.
380"
REFERENCES,0.47288503253796094,"[19] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized relation modeling for transformer tracking.
381"
REFERENCES,0.47396963123644253,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18686–
382"
REFERENCES,0.4750542299349241,"18695, 2023.
383"
REFERENCES,0.4761388286334056,"[20] Chengyue Gong and Dilin Wang. Nasvit: Neural architecture search for efficient vision transformers with
384"
REFERENCES,0.4772234273318872,"gradient conflict-aware supernet training. ICLR Proceedings 2022, 2022.
385"
REFERENCES,0.47830802603036876,"[21] Goutam Yelluru Gopal and Maria A Amer. Separable self and mixed attention transformers for efficient
386"
REFERENCES,0.4793926247288503,"object tracking. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
387"
REFERENCES,0.4804772234273319,"pages 6708–6717, 2024.
388"
REFERENCES,0.48156182212581344,"[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
389"
REFERENCES,0.482646420824295,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
390"
REFERENCES,0.4837310195227766,"[23] João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized
391"
REFERENCES,0.4848156182212581,"correlation filters. IEEE transactions on pattern analysis and machine intelligence, 37(3):583–596, 2014.
392"
REFERENCES,0.48590021691973967,"[24] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic
393"
REFERENCES,0.48698481561822127,"object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):1562–
394"
REFERENCES,0.4880694143167028,"1577, 2019.
395"
REFERENCES,0.4891540130151844,"[25] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
396"
REFERENCES,0.49023861171366595,"Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.
397"
REFERENCES,0.4913232104121475,"[26] Ben Kang, Xin Chen, Dong Wang, Houwen Peng, and Huchuan Lu. Exploring lightweight hierarchical
398"
REFERENCES,0.4924078091106291,"vision transformers for efficient visual tracking. In Proceedings of the IEEE/CVF International Conference
399"
REFERENCES,0.49349240780911063,"on Computer Vision, pages 9612–9621, 2023.
400"
REFERENCES,0.4945770065075922,"[27] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of
401"
REFERENCES,0.4956616052060738,"siamese visual tracking with very deep networks. In Proceedings of the IEEE/CVF conference on computer
402"
REFERENCES,0.4967462039045553,"vision and pattern recognition, pages 4282–4291, 2019.
403"
REFERENCES,0.49783080260303686,"[28] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese re-
404"
REFERENCES,0.49891540130151846,"gion proposal network. In Proceedings of the IEEE conference on computer vision and pattern recognition,
405"
REFERENCES,0.5,"pages 8971–8980, 2018.
406"
REFERENCES,0.5010845986984815,"[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
407"
REFERENCES,0.5021691973969631,"and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:
408"
REFERENCES,0.5032537960954447,"13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
409"
REFERENCES,0.5043383947939263,"740–755. Springer, 2014.
410"
REFERENCES,0.5054229934924078,"[30] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
411"
REFERENCES,0.5065075921908894,"convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern
412"
REFERENCES,0.5075921908893709,"recognition, pages 11976–11986, 2022.
413"
REFERENCES,0.5086767895878525,"[31] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
414"
REFERENCES,0.5097613882863341,"arXiv:1711.05101, 2017.
415"
REFERENCES,0.5108459869848156,"[32] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet:
416"
REFERENCES,0.5119305856832972,"A large-scale dataset and benchmark for object tracking in the wild. In Proceedings of the European
417"
REFERENCES,0.5130151843817787,"conference on computer vision (ECCV), pages 300–317, 2018.
418"
REFERENCES,0.5140997830802603,"[33] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient
419"
REFERENCES,0.5151843817787418,"vision transformers with dynamic token sparsification. Advances in neural information processing systems,
420"
REFERENCES,0.5162689804772235,"34:13937–13949, 2021.
421"
REFERENCES,0.517353579175705,"[34] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:
422"
REFERENCES,0.5184381778741866,"smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
423"
REFERENCES,0.5195227765726681,"[35] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and
424"
REFERENCES,0.5206073752711496,"Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI
425"
REFERENCES,0.5216919739696312,"Conference on Artificial Intelligence, volume 34, pages 8815–8821, 2020.
426"
REFERENCES,0.5227765726681128,"[36] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression.
427"
REFERENCES,0.5238611713665944,"arXiv preprint arXiv:1908.09355, 2019.
428"
REFERENCES,0.5249457700650759,"[37] Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yihong Gong. Autoregressive visual tracking. In
429"
REFERENCES,0.5260303687635575,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9697–9706,
430"
REFERENCES,0.527114967462039,"2023.
431"
REFERENCES,0.5281995661605207,"[38] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing bert
432"
REFERENCES,0.5292841648590022,"by progressive module replacing. arXiv preprint arXiv:2002.02925, 2020.
433"
REFERENCES,0.5303687635574837,"[39] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. Siamfc++: Towards robust and accurate visual
434"
REFERENCES,0.5314533622559653,"tracking with target estimation guidelines. In Proceedings of the AAAI conference on artificial intelligence,
435"
REFERENCES,0.5325379609544468,"volume 34, pages 12549–12556, 2020.
436"
REFERENCES,0.5336225596529284,"[40] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng
437"
REFERENCES,0.53470715835141,"Xu, and Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In Proceedings of
438"
REFERENCES,0.5357917570498916,"the AAAI Conference on Artificial Intelligence, volume 36, pages 2964–2972, 2022.
439"
REFERENCES,0.5368763557483731,"[41] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer
440"
REFERENCES,0.5379609544468547,"for visual tracking. In Proceedings of the IEEE/CVF international conference on computer vision, pages
441"
REFERENCES,0.5390455531453362,"10448–10457, 2021.
442"
REFERENCES,0.5401301518438177,"[42] Bin Yan, Houwen Peng, Kan Wu, Dong Wang, Jianlong Fu, and Huchuan Lu. Lighttrack: Finding
443"
REFERENCES,0.5412147505422994,"lightweight neural networks for object tracking via one-shot architecture search. In Proceedings of the
444"
REFERENCES,0.5422993492407809,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15180–15189, 2021.
445"
REFERENCES,0.5433839479392625,"[43] Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, and Yu Li. Vitkd: Practical guidelines for vit
446"
REFERENCES,0.544468546637744,"feature knowledge distillation. arXiv preprint arXiv:2209.02432, 2022.
447"
REFERENCES,0.5455531453362256,"[44] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation
448"
REFERENCES,0.5466377440347071,"modeling for tracking: A one-stream framework. In European conference on computer vision, pages
449"
REFERENCES,0.5477223427331888,"341–357. Springer, 2022.
450"
REFERENCES,0.5488069414316703,"[45] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Minivit:
451"
REFERENCES,0.5498915401301518,"Compressing vision transformers with weight multiplexing. In Proceedings of the IEEE/CVF Conference
452"
REFERENCES,0.5509761388286334,"on Computer Vision and Pattern Recognition, pages 12145–12154, 2022.
453"
REFERENCES,0.5520607375271149,"[46] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and Weiming Hu. Ocean: Object-aware anchor-free
454"
REFERENCES,0.5531453362255966,"tracking. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
455"
REFERENCES,0.5542299349240781,"Proceedings, Part XXI 16, pages 771–787. Springer, 2020.
456"
REFERENCES,0.5553145336225597,"A
Appendix / supplemental material
457"
REFERENCES,0.5563991323210412,Algorithm 1 Pseudocode of OSTrack in a PyTorch-like style
REFERENCES,0.5574837310195228,"# z/x: RGB image of template/search region
# patch_embed: patch embedding layer,
# pos_embed_z/pos_embed_z: position embedding for template/search region
# blocks: transformer block layers
# decoder: decoder network"
REFERENCES,0.5585683297180043,"def forward(x, z):"
REFERENCES,0.559652928416486,"# patch embedding layer
x, z = patch_embed(x), patch_embed(z)"
REFERENCES,0.5607375271149675,"# add position embedding
x, z = x + pos_embed_x, z + pos_embed_z"
REFERENCES,0.561822125813449,"# concat
x = torch.cat([z, x], dim=1)"
REFERENCES,0.5629067245119306,"# transformer layers
for i, blk in enumerate(blocks):"
REFERENCES,0.5639913232104121,x = blk(x)
REFERENCES,0.5650759219088937,"# decode the matching result
x = decoder(x)"
REFERENCES,0.5661605206073753,"A.1
Replacement Training
458"
REFERENCES,0.5672451193058569,"We present the pseudocode for the training and testing phases of CompressTracker in Algorithm 2
459"
REFERENCES,0.5683297180043384,"and Algorithm 3, respectively. Additionally, the pseudocode of OSTrack [44] is also shown in
460"
REFERENCES,0.56941431670282,"Algorithm 1. During training process, we employ Bernoulli sampling to implement a replacement
461"
REFERENCES,0.5704989154013015,"training strategy, while in the test phase, we integrate the student layers and discard the teacher layer.
462"
REFERENCES,0.571583514099783,"#
Replacement
AUC
Training
Time"
RANDOM,0.5726681127982647,"1
Random
65.2%
12 h"
RANDOM,0.5737527114967462,"2
Decouple-300
64.6%
16 h"
RANDOM,0.5748373101952278,"Table 11: Ablation study on re-
placement training."
RANDOM,0.5759219088937093,"#
Replacement
AUC"
RANDOM,0.5770065075921909,"1
w/ Progressive
65.2%"
RANDOM,0.5780911062906724,"2
w/o Progressive
64.8%"
RANDOM,0.579175704989154,"Table 12: Ablation study on
progressive replacement."
RANDOM,0.5802603036876356,"#
Model
Training
Time
1
CompressTracker-4
20 h
2
OSTrack
17 h
3
MixFormerV2-S
120 h"
RANDOM,0.5813449023861171,"Table 13: Training Time com-
parison."
RANDOM,0.5824295010845987,"0.1
0.3
0.5
0.7
0.9
61 62 63 64 65 66 67 AUC % 61.6 62.3"
RANDOM,0.5835140997830802,"63.7
63.6 63.2 62.8 64.0 64.8 65.0 64.2 63.5 65.3"
RANDOM,0.5845986984815619,"66.2
66.3 66.0"
RANDOM,0.5856832971800434,"CompressTracker-3
CompressTracker-4
CompressTracker-6"
RANDOM,0.586767895878525,"Figure 4: Ablation study on different replace-
ment probability."
RANDOM,0.5878524945770065,"CompressTracker-2
CompressTracker-3
CompressTracker-4
CompressTracker-6
CompressTracker-8
5 10 15 20 25 30"
RANDOM,0.588937093275488,"Training Time (h) 6 7 8 12 14
14 16 20 25"
"NAIVE TRAINING
COMPRESSTRACKER",0.5900216919739696,"29
Naive Training
CompressTracker"
"NAIVE TRAINING
COMPRESSTRACKER",0.5911062906724512,Figure 5: Training Time.
"NAIVE TRAINING
COMPRESSTRACKER",0.5921908893709328,Algorithm 2 Pseudocode of CompressTracker for Training in a PyTorch-like style
"NAIVE TRAINING
COMPRESSTRACKER",0.5932754880694143,"# z/x: RGB image of template/search region
# patch_embed: patch embedding layer,
# pos_embed_z/pos_embed_z: position embedding for template/search region
# bernoulli_sample: bernoulli sampling function with probability of p
# n_s/n_t: layer number of student/teacher model
# teacher_blocks: transformer block layers of a pretrained teacher
# student_blocks: transformer block layers of student model
# decoder: decoder network"
"NAIVE TRAINING
COMPRESSTRACKER",0.5943600867678959,"def forward(x, z):"
"NAIVE TRAINING
COMPRESSTRACKER",0.5954446854663774,"# patch embedding layer
x, z = patch_embed(x), patch_embed(z)"
"NAIVE TRAINING
COMPRESSTRACKER",0.596529284164859,"# add position embedding
x, z = x + pos_embed_x, z + pos_embed_z"
"NAIVE TRAINING
COMPRESSTRACKER",0.5976138828633406,"# concat
x = torch.cat([z, x], dim=1)"
"NAIVE TRAINING
COMPRESSTRACKER",0.5986984815618221,"# replacement sampling
inference_blocks = []
for i in range(n):"
"NAIVE TRAINING
COMPRESSTRACKER",0.5997830802603037,if bernoulli_sample() == 1:
"NAIVE TRAINING
COMPRESSTRACKER",0.6008676789587852,"inference_blocks.append(student_blocks[i])
else:"
"NAIVE TRAINING
COMPRESSTRACKER",0.6019522776572668,for j in range(n_t//n_s):
"NAIVE TRAINING
COMPRESSTRACKER",0.6030368763557483,inference_blocks.append(teacher_blocks[i*(n_t//n_s) + j])
"NAIVE TRAINING
COMPRESSTRACKER",0.60412147505423,"# randomly replaced transformer layers
for i, blk in enumerate(inference_blocks):"
"NAIVE TRAINING
COMPRESSTRACKER",0.6052060737527115,x = blk(x)
"NAIVE TRAINING
COMPRESSTRACKER",0.6062906724511931,"# decode the matching result
x = decoder(x)"
"NAIVE TRAINING
COMPRESSTRACKER",0.6073752711496746,Algorithm 3 Pseudocode of CompressTracker for Testing in a PyTorch-like style
"NAIVE TRAINING
COMPRESSTRACKER",0.6084598698481561,"# z/x: RGB image of template/search region
# patch_embed: patch embedding layer,
# pos_embed_z/pos_embed_z: position embedding for template/search region
# student_blocks: transformer block layers of student model
# decoder: decoder network"
"NAIVE TRAINING
COMPRESSTRACKER",0.6095444685466378,"def forward(x, z):"
"NAIVE TRAINING
COMPRESSTRACKER",0.6106290672451193,"# patch embedding layer
x, z = patch_embed(x), patch_embed(z)"
"NAIVE TRAINING
COMPRESSTRACKER",0.6117136659436009,"# add position embedding
x, z = x + pos_embed_x, z + pos_embed_z"
"NAIVE TRAINING
COMPRESSTRACKER",0.6127982646420824,"# concat
x = torch.cat([z, x], dim=1)"
"NAIVE TRAINING
COMPRESSTRACKER",0.613882863340564,"# transformer layers
for i, blk in enumerate(student_blocks):"
"NAIVE TRAINING
COMPRESSTRACKER",0.6149674620390455,x = blk(x)
"NAIVE TRAINING
COMPRESSTRACKER",0.6160520607375272,"# decode the matching result
x = decoder(x)"
"NAIVE TRAINING
COMPRESSTRACKER",0.6171366594360087,"A.2
More Ablation Study
463"
"NAIVE TRAINING
COMPRESSTRACKER",0.6182212581344902,"We represent more ablation studies on LaSOT to explore the factors contributing to effectiveness of
464"
"NAIVE TRAINING
COMPRESSTRACKER",0.6193058568329718,"our CompressTracker. Unless otherwise specified, teacher model is OSTrack,and student model has 4
465"
"NAIVE TRAINING
COMPRESSTRACKER",0.6203904555314533,"encoder layers. The student model is trained for 300 epochs, and the pinit is set as 0.5.
466"
"NAIVE TRAINING
COMPRESSTRACKER",0.6214750542299349,"Replacement Training. To evaluate the efficiency and effectiveness of our replacement training
467"
"NAIVE TRAINING
COMPRESSTRACKER",0.6225596529284165,"strategy, we conduct a series of experiments and results are presented in Table 11. ’Random’ denotes
468"
"NAIVE TRAINING
COMPRESSTRACKER",0.6236442516268981,"our replacement training, and ’Decouple-300’ represents decoupling the training of each stage. Result
469"
"NAIVE TRAINING
COMPRESSTRACKER",0.6247288503253796,"of # 1 aligns with our replacement training with 300 training epochs, while in # 2, we employ a
470"
"NAIVE TRAINING
COMPRESSTRACKER",0.6258134490238612,"decoupled training approach for each stage. Initially, we substitute the first stage of the teacher
471"
"NAIVE TRAINING
COMPRESSTRACKER",0.6268980477223427,"model with its counterpart in the student model, training the first stage for 75 epochs. Subsequently,
472"
"NAIVE TRAINING
COMPRESSTRACKER",0.6279826464208242,"the first trained stage of the student model is frozen, and the second stage undergoes training for
473"
"NAIVE TRAINING
COMPRESSTRACKER",0.6290672451193059,"an additional 75 epochs. Following this iterative process, we train the four stages cumulatively
474"
"NAIVE TRAINING
COMPRESSTRACKER",0.6301518438177874,"over 300 epochs, with an additional 30 epochs for fine-tuning. The ’Decouple-300’ (# 2) approach
475"
"NAIVE TRAINING
COMPRESSTRACKER",0.631236442516269,"achieves 64.6% AUC on LaSOT with the same training epochs, marginally lower by 0.6% AUC
476"
"NAIVE TRAINING
COMPRESSTRACKER",0.6323210412147505,"than our replacement training strategy (# 1). The ’Decouple-300’ approach (# 2) requires a complex,
477"
"NAIVE TRAINING
COMPRESSTRACKER",0.6334056399132321,"multi-stage trainingalong with supplementary fine-tuning, while our CompressTracker operates on an
478"
"NAIVE TRAINING
COMPRESSTRACKER",0.6344902386117137,"end-to-end, single-step basis. Besides, the ’Decouple-300’ approach may suffer from suboptimal
479"
"NAIVE TRAINING
COMPRESSTRACKER",0.6355748373101953,"outcomes at a specific training process, but our CompressTracker can avoid this problem through its
480"
"NAIVE TRAINING
COMPRESSTRACKER",0.6366594360086768,"unified training manner, which validates the superiority of our replacement training strategy.
481"
"NAIVE TRAINING
COMPRESSTRACKER",0.6377440347071583,"Replacement Probability. We investigate the impact of replacement probability on the accuracy of
482"
"NAIVE TRAINING
COMPRESSTRACKER",0.6388286334056399,"student model in Figure 4. We maintain a constant replacement probability instead of implementing
483"
"NAIVE TRAINING
COMPRESSTRACKER",0.6399132321041214,"the progressive replacement strategy and train the student model with 300 epochs and 30 extra
484"
"NAIVE TRAINING
COMPRESSTRACKER",0.6409978308026031,"finetuning epochs. It can be observed from Figure 4 that performance is adversely affected when
485"
"NAIVE TRAINING
COMPRESSTRACKER",0.6420824295010846,"the replacement probability is set either too high or too low. Optimal results are achieved when the
486"
"NAIVE TRAINING
COMPRESSTRACKER",0.6431670281995662,"replacement probability is within the range of 0.5 to 0.7. Specifically, a too low probability leads to
487"
"NAIVE TRAINING
COMPRESSTRACKER",0.6442516268980477,"inadequate training, whereas a too high probability may result in the insufficient interaction between
488"
"NAIVE TRAINING
COMPRESSTRACKER",0.6453362255965293,"teacher model and student tracker. Thus, we set the pinit as 0.5 based on the experiment result.
489"
"NAIVE TRAINING
COMPRESSTRACKER",0.6464208242950108,"Progressive Replacement. In Table 12, we illustrate the impact of progressive replacement strategy.
490"
"NAIVE TRAINING
COMPRESSTRACKER",0.6475054229934925,"The first row (# 1) corresponds to the same setting of CompressTracker, while in the second row (# 2)
491"
"NAIVE TRAINING
COMPRESSTRACKER",0.648590021691974,"we fix the sampling probability as 0.5 and the student model is trained with 300 epochs followed by
492"
"NAIVE TRAINING
COMPRESSTRACKER",0.6496746203904555,"30 finetuning epochs. The absence of progressive replacement leads to a performance degradation of
493"
"NAIVE TRAINING
COMPRESSTRACKER",0.6507592190889371,"0.4% AUC, thereby highlighting the efficacy of our progressive replacement approach.
494"
"NAIVE TRAINING
COMPRESSTRACKER",0.6518438177874186,"Training Time. We compare the training time of CompressTracker with 500 training epochs across
495"
"NAIVE TRAINING
COMPRESSTRACKER",0.6529284164859002,"different layers in Figure 5. ’Naive Training’ denotes solely training on groundtruth data with
496"
"NAIVE TRAINING
COMPRESSTRACKER",0.6540130151843818,"300 epochs, and ’CompressTracker’ represents our proposed training strategy with 500 epochs.
497"
"NAIVE TRAINING
COMPRESSTRACKER",0.6550976138828634,"The training time is recorded on 8 NVIDIA RTX 3090 GPUs. Besides, the training times of
498"
"NAIVE TRAINING
COMPRESSTRACKER",0.6561822125813449,"our CompressTracker-4, OSTrack, and MixFormerV2-S are presented in Table 13. Although our
499"
"NAIVE TRAINING
COMPRESSTRACKER",0.6572668112798264,"CompressTracker requires a longer training time compared to the ’Naive Training’, the increased
500"
"NAIVE TRAINING
COMPRESSTRACKER",0.658351409978308,"computational overhead remains within acceptable limits. Moreover, MixFormerV2-S is trained
501"
"NAIVE TRAINING
COMPRESSTRACKER",0.6594360086767896,"on 8 Nvidia RTX8000 GPUs, and we estimate this will take roughly 80 hours on 8 NVIDIA RTX
502"
"NAIVE TRAINING
COMPRESSTRACKER",0.6605206073752712,"3090 GPUs based on the relative computational capabilities of these GPUs. The training time of our
503"
"NAIVE TRAINING
COMPRESSTRACKER",0.6616052060737527,"CompressTracker-4 is significantly less than that of MixFormerV2-S, which validate the efficiency
504"
"NAIVE TRAINING
COMPRESSTRACKER",0.6626898047722343,"and effectiveness of our framework.
505"
"NAIVE TRAINING
COMPRESSTRACKER",0.6637744034707158,"NeurIPS Paper Checklist
506"
CLAIMS,0.6648590021691974,"1. Claims
507"
CLAIMS,0.665943600867679,"Question: Do the main claims made in the abstract and introduction accurately reflect the
508"
CLAIMS,0.6670281995661606,"paper’s contributions and scope?
509"
CLAIMS,0.6681127982646421,"Answer: [Yes]
510"
CLAIMS,0.6691973969631236,"Justification: We summarize our contribution in the abstract and introduction.
511"
CLAIMS,0.6702819956616052,"Guidelines:
512"
CLAIMS,0.6713665943600867,"• The answer NA means that the abstract and introduction do not include the claims
513"
CLAIMS,0.6724511930585684,"made in the paper.
514"
CLAIMS,0.6735357917570499,"• The abstract and/or introduction should clearly state the claims made, including the
515"
CLAIMS,0.6746203904555315,"contributions made in the paper and important assumptions and limitations. A No or
516"
CLAIMS,0.675704989154013,"NA answer to this question will not be perceived well by the reviewers.
517"
CLAIMS,0.6767895878524945,"• The claims made should match theoretical and experimental results, and reflect how
518"
CLAIMS,0.6778741865509761,"much the results can be expected to generalize to other settings.
519"
CLAIMS,0.6789587852494577,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
520"
CLAIMS,0.6800433839479393,"are not attained by the paper.
521"
LIMITATIONS,0.6811279826464208,"2. Limitations
522"
LIMITATIONS,0.6822125813449024,"Question: Does the paper discuss the limitations of the work performed by the authors?
523"
LIMITATIONS,0.6832971800433839,"Answer: [Yes]
524"
LIMITATIONS,0.6843817787418656,"Justification: We discuss the limitations of our work in our maniscript.
525"
LIMITATIONS,0.6854663774403471,"Guidelines:
526"
LIMITATIONS,0.6865509761388287,"• The answer NA means that the paper has no limitation while the answer No means that
527"
LIMITATIONS,0.6876355748373102,"the paper has limitations, but those are not discussed in the paper.
528"
LIMITATIONS,0.6887201735357917,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
529"
LIMITATIONS,0.6898047722342733,"• The paper should point out any strong assumptions and how robust the results are to
530"
LIMITATIONS,0.6908893709327549,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
531"
LIMITATIONS,0.6919739696312365,"model well-specification, asymptotic approximations only holding locally). The authors
532"
LIMITATIONS,0.693058568329718,"should reflect on how these assumptions might be violated in practice and what the
533"
LIMITATIONS,0.6941431670281996,"implications would be.
534"
LIMITATIONS,0.6952277657266811,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
535"
LIMITATIONS,0.6963123644251626,"only tested on a few datasets or with a few runs. In general, empirical results often
536"
LIMITATIONS,0.6973969631236443,"depend on implicit assumptions, which should be articulated.
537"
LIMITATIONS,0.6984815618221258,"• The authors should reflect on the factors that influence the performance of the approach.
538"
LIMITATIONS,0.6995661605206074,"For example, a facial recognition algorithm may perform poorly when image resolution
539"
LIMITATIONS,0.7006507592190889,"is low or images are taken in low lighting. Or a speech-to-text system might not be
540"
LIMITATIONS,0.7017353579175705,"used reliably to provide closed captions for online lectures because it fails to handle
541"
LIMITATIONS,0.702819956616052,"technical jargon.
542"
LIMITATIONS,0.7039045553145337,"• The authors should discuss the computational efficiency of the proposed algorithms
543"
LIMITATIONS,0.7049891540130152,"and how they scale with dataset size.
544"
LIMITATIONS,0.7060737527114967,"• If applicable, the authors should discuss possible limitations of their approach to
545"
LIMITATIONS,0.7071583514099783,"address problems of privacy and fairness.
546"
LIMITATIONS,0.7082429501084598,"• While the authors might fear that complete honesty about limitations might be used by
547"
LIMITATIONS,0.7093275488069414,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
548"
LIMITATIONS,0.710412147505423,"limitations that aren’t acknowledged in the paper. The authors should use their best
549"
LIMITATIONS,0.7114967462039046,"judgment and recognize that individual actions in favor of transparency play an impor-
550"
LIMITATIONS,0.7125813449023861,"tant role in developing norms that preserve the integrity of the community. Reviewers
551"
LIMITATIONS,0.7136659436008677,"will be specifically instructed to not penalize honesty concerning limitations.
552"
THEORY ASSUMPTIONS AND PROOFS,0.7147505422993492,"3. Theory Assumptions and Proofs
553"
THEORY ASSUMPTIONS AND PROOFS,0.7158351409978309,"Question: For each theoretical result, does the paper provide the full set of assumptions and
554"
THEORY ASSUMPTIONS AND PROOFS,0.7169197396963124,"a complete (and correct) proof?
555"
THEORY ASSUMPTIONS AND PROOFS,0.7180043383947939,"Answer: [NA]
556"
THEORY ASSUMPTIONS AND PROOFS,0.7190889370932755,"Justification: We do not include theoretical results.
557"
THEORY ASSUMPTIONS AND PROOFS,0.720173535791757,"Guidelines:
558"
THEORY ASSUMPTIONS AND PROOFS,0.7212581344902386,"• The answer NA means that the paper does not include theoretical results.
559"
THEORY ASSUMPTIONS AND PROOFS,0.7223427331887202,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
560"
THEORY ASSUMPTIONS AND PROOFS,0.7234273318872018,"referenced.
561"
THEORY ASSUMPTIONS AND PROOFS,0.7245119305856833,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
562"
THEORY ASSUMPTIONS AND PROOFS,0.7255965292841648,"• The proofs can either appear in the main paper or the supplemental material, but if
563"
THEORY ASSUMPTIONS AND PROOFS,0.7266811279826464,"they appear in the supplemental material, the authors are encouraged to provide a short
564"
THEORY ASSUMPTIONS AND PROOFS,0.7277657266811279,"proof sketch to provide intuition.
565"
THEORY ASSUMPTIONS AND PROOFS,0.7288503253796096,"• Inversely, any informal proof provided in the core of the paper should be complemented
566"
THEORY ASSUMPTIONS AND PROOFS,0.7299349240780911,"by formal proofs provided in appendix or supplemental material.
567"
THEORY ASSUMPTIONS AND PROOFS,0.7310195227765727,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7321041214750542,"4. Experimental Result Reproducibility
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7331887201735358,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7342733188720173,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.735357917570499,"of the paper (regardless of whether the code and data are provided or not)?
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7364425162689805,"Answer: [Yes]
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.737527114967462,"Justification: We include all the details in our paper, including datasets, model, and training
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7386117136659436,"details. Other researchers can reproduce our result easily, and we will release our code once
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7396963123644251,"our work is accepted.
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7407809110629068,"Guidelines:
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7418655097613883,"• The answer NA means that the paper does not include experiments.
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7429501084598699,"• If the paper includes experiments, a No answer to this question will not be perceived
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7440347071583514,"well by the reviewers: Making the paper reproducible is important, regardless of
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.745119305856833,"whether the code and data are provided or not.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7462039045553145,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7472885032537961,"to make their results reproducible or verifiable.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7483731019522777,"• Depending on the contribution, reproducibility can be accomplished in various ways.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494577006507592,"For example, if the contribution is a novel architecture, describing the architecture fully
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505422993492408,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7516268980477223,"be necessary to either make it possible for others to replicate the model with the same
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7527114967462039,"dataset, or provide access to the model. In general. releasing code and data is often
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537960954446855,"one good way to accomplish this, but reproducibility can also be provided via detailed
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.754880694143167,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7559652928416486,"of a large language model), releasing of a model checkpoint, or other means that are
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7570498915401301,"appropriate to the research performed.
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7581344902386117,"• While NeurIPS does not require releasing code, the conference does require all submis-
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7592190889370932,"sions to provide some reasonable avenue for reproducibility, which may depend on the
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7603036876355749,"nature of the contribution. For example
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613882863340564,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762472885032538,"to reproduce that algorithm.
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7635574837310195,"(b) If the contribution is primarily a new model architecture, the paper should describe
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764642082429501,"the architecture clearly and fully.
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657266811279827,"(c) If the contribution is a new model (e.g., a large language model), then there should
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7668112798264642,"either be a way to access this model for reproducing the results or a way to reproduce
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678958785249458,"the model (e.g., with an open-source dataset or instructions for how to construct
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7689804772234273,"the dataset).
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7700650759219089,"(d) We recognize that reproducibility may be tricky in some cases, in which case
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7711496746203904,"authors are welcome to describe the particular way they provide for reproducibility.
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7722342733188721,"In the case of closed-source models, it may be that access to the model is limited in
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733188720173536,"some way (e.g., to registered users), but it should be possible for other researchers
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744034707158352,"to have some path to reproducing or verifying the results.
608"
OPEN ACCESS TO DATA AND CODE,0.7754880694143167,"5. Open access to data and code
609"
OPEN ACCESS TO DATA AND CODE,0.7765726681127982,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
610"
OPEN ACCESS TO DATA AND CODE,0.7776572668112798,"tions to faithfully reproduce the main experimental results, as described in supplemental
611"
OPEN ACCESS TO DATA AND CODE,0.7787418655097614,"material?
612"
OPEN ACCESS TO DATA AND CODE,0.779826464208243,"Answer: [No]
613"
OPEN ACCESS TO DATA AND CODE,0.7809110629067245,"Justification: We provide the core pseudocode in appendix, and we will release our code
614"
OPEN ACCESS TO DATA AND CODE,0.7819956616052061,"after acceptance.
615"
OPEN ACCESS TO DATA AND CODE,0.7830802603036876,"Guidelines:
616"
OPEN ACCESS TO DATA AND CODE,0.7841648590021691,"• The answer NA means that paper does not include experiments requiring code.
617"
OPEN ACCESS TO DATA AND CODE,0.7852494577006508,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
618"
OPEN ACCESS TO DATA AND CODE,0.7863340563991323,"public/guides/CodeSubmissionPolicy) for more details.
619"
OPEN ACCESS TO DATA AND CODE,0.7874186550976139,"• While we encourage the release of code and data, we understand that this might not be
620"
OPEN ACCESS TO DATA AND CODE,0.7885032537960954,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
621"
OPEN ACCESS TO DATA AND CODE,0.789587852494577,"including code, unless this is central to the contribution (e.g., for a new open-source
622"
OPEN ACCESS TO DATA AND CODE,0.7906724511930586,"benchmark).
623"
OPEN ACCESS TO DATA AND CODE,0.7917570498915402,"• The instructions should contain the exact command and environment needed to run to
624"
OPEN ACCESS TO DATA AND CODE,0.7928416485900217,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
625"
OPEN ACCESS TO DATA AND CODE,0.7939262472885033,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
626"
OPEN ACCESS TO DATA AND CODE,0.7950108459869848,"• The authors should provide instructions on data access and preparation, including how
627"
OPEN ACCESS TO DATA AND CODE,0.7960954446854663,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
628"
OPEN ACCESS TO DATA AND CODE,0.797180043383948,"• The authors should provide scripts to reproduce all experimental results for the new
629"
OPEN ACCESS TO DATA AND CODE,0.7982646420824295,"proposed method and baselines. If only a subset of experiments are reproducible, they
630"
OPEN ACCESS TO DATA AND CODE,0.7993492407809111,"should state which ones are omitted from the script and why.
631"
OPEN ACCESS TO DATA AND CODE,0.8004338394793926,"• At submission time, to preserve anonymity, the authors should release anonymized
632"
OPEN ACCESS TO DATA AND CODE,0.8015184381778742,"versions (if applicable).
633"
OPEN ACCESS TO DATA AND CODE,0.8026030368763557,"• Providing as much information as possible in supplemental material (appended to the
634"
OPEN ACCESS TO DATA AND CODE,0.8036876355748374,"paper) is recommended, but including URLs to data and code is permitted.
635"
OPEN ACCESS TO DATA AND CODE,0.8047722342733189,"6. Experimental Setting/Details
636"
OPEN ACCESS TO DATA AND CODE,0.8058568329718004,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
637"
OPEN ACCESS TO DATA AND CODE,0.806941431670282,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
638"
OPEN ACCESS TO DATA AND CODE,0.8080260303687635,"results?
639"
OPEN ACCESS TO DATA AND CODE,0.8091106290672451,"Answer: [Yes]
640"
OPEN ACCESS TO DATA AND CODE,0.8101952277657267,"Justification: We include all the details in our paper, including datasets, model, and train-
641"
OPEN ACCESS TO DATA AND CODE,0.8112798264642083,"ing details. Other researchers can reproduce our result easily. We also provide the core
642"
OPEN ACCESS TO DATA AND CODE,0.8123644251626898,"pseudocode in appendix.
643"
OPEN ACCESS TO DATA AND CODE,0.8134490238611713,"Guidelines:
644"
OPEN ACCESS TO DATA AND CODE,0.8145336225596529,"• The answer NA means that the paper does not include experiments.
645"
OPEN ACCESS TO DATA AND CODE,0.8156182212581344,"• The experimental setting should be presented in the core of the paper to a level of detail
646"
OPEN ACCESS TO DATA AND CODE,0.8167028199566161,"that is necessary to appreciate the results and make sense of them.
647"
OPEN ACCESS TO DATA AND CODE,0.8177874186550976,"• The full details can be provided either with the code, in appendix, or as supplemental
648"
OPEN ACCESS TO DATA AND CODE,0.8188720173535792,"material.
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8199566160520607,"7. Experiment Statistical Significance
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8210412147505423,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8221258134490239,"information about the statistical significance of the experiments?
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232104121475055,"Answer: [No]
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.824295010845987,"Justification: Our paper does not report error bars.
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253796095444685,"Guidelines:
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264642082429501,"• The answer NA means that the paper does not include experiments.
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275488069414316,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8286334056399133,"dence intervals, or statistical significance tests, at least for the experiments that support
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8297180043383948,"the main claims of the paper.
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8308026030368764,"• The factors of variability that the error bars are capturing should be clearly stated (for
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318872017353579,"example, train/test split, initialization, random drawing of some parameter, or overall
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8329718004338394,"run with given experimental conditions).
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834056399132321,"• The method for calculating the error bars should be explained (closed form formula,
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351409978308026,"call to a library function, bootstrap, etc.)
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8362255965292842,"• The assumptions made should be given (e.g., Normally distributed errors).
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8373101952277657,"• It should be clear whether the error bar is the standard deviation or the standard error
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8383947939262473,"of the mean.
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8394793926247288,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8405639913232104,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841648590021692,"of Normality of errors is not verified.
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427331887201736,"• For asymmetric distributions, the authors should be careful not to show in tables or
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438177874186551,"figures symmetric error bars that would yield results that are out of range (e.g. negative
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8449023861171366,"error rates).
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459869848156182,"• If error bars are reported in tables or plots, The authors should explain in the text how
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8470715835140998,"they were calculated and reference the corresponding figures or tables in the text.
675"
EXPERIMENTS COMPUTE RESOURCES,0.8481561822125814,"8. Experiments Compute Resources
676"
EXPERIMENTS COMPUTE RESOURCES,0.8492407809110629,"Question: For each experiment, does the paper provide sufficient information on the com-
677"
EXPERIMENTS COMPUTE RESOURCES,0.8503253796095445,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
678"
EXPERIMENTS COMPUTE RESOURCES,0.851409978308026,"the experiments?
679"
EXPERIMENTS COMPUTE RESOURCES,0.8524945770065075,"Answer: [Yes]
680"
EXPERIMENTS COMPUTE RESOURCES,0.8535791757049892,"Justification: We provide the type of GPU we used and time of execution
681"
EXPERIMENTS COMPUTE RESOURCES,0.8546637744034707,"Guidelines:
682"
EXPERIMENTS COMPUTE RESOURCES,0.8557483731019523,"• The answer NA means that the paper does not include experiments.
683"
EXPERIMENTS COMPUTE RESOURCES,0.8568329718004338,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
684"
EXPERIMENTS COMPUTE RESOURCES,0.8579175704989154,"or cloud provider, including relevant memory and storage.
685"
EXPERIMENTS COMPUTE RESOURCES,0.8590021691973969,"• The paper should provide the amount of compute required for each of the individual
686"
EXPERIMENTS COMPUTE RESOURCES,0.8600867678958786,"experimental runs as well as estimate the total compute.
687"
EXPERIMENTS COMPUTE RESOURCES,0.8611713665943601,"• The paper should disclose whether the full research project required more compute
688"
EXPERIMENTS COMPUTE RESOURCES,0.8622559652928417,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
689"
EXPERIMENTS COMPUTE RESOURCES,0.8633405639913232,"didn’t make it into the paper).
690"
CODE OF ETHICS,0.8644251626898047,"9. Code Of Ethics
691"
CODE OF ETHICS,0.8655097613882863,"Question: Does the research conducted in the paper conform, in every respect, with the
692"
CODE OF ETHICS,0.8665943600867679,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
693"
CODE OF ETHICS,0.8676789587852495,"Answer: [Yes]
694"
CODE OF ETHICS,0.868763557483731,"Justification: Our research conforms with the NeurIPS Code of Ethics.
695"
CODE OF ETHICS,0.8698481561822126,"Guidelines:
696"
CODE OF ETHICS,0.8709327548806941,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
697"
CODE OF ETHICS,0.8720173535791758,"• If the authors answer No, they should explain the special circumstances that require a
698"
CODE OF ETHICS,0.8731019522776573,"deviation from the Code of Ethics.
699"
CODE OF ETHICS,0.8741865509761388,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
700"
CODE OF ETHICS,0.8752711496746204,"eration due to laws or regulations in their jurisdiction).
701"
BROADER IMPACTS,0.8763557483731019,"10. Broader Impacts
702"
BROADER IMPACTS,0.8774403470715835,"Question: Does the paper discuss both potential positive societal impacts and negative
703"
BROADER IMPACTS,0.8785249457700651,"societal impacts of the work performed?
704"
BROADER IMPACTS,0.8796095444685467,"Answer: [Yes]
705"
BROADER IMPACTS,0.8806941431670282,"Justification: We discuss the potential negative societal impacts.
706"
BROADER IMPACTS,0.8817787418655098,"Guidelines:
707"
BROADER IMPACTS,0.8828633405639913,"• The answer NA means that there is no societal impact of the work performed.
708"
BROADER IMPACTS,0.8839479392624728,"• If the authors answer NA or No, they should explain why their work has no societal
709"
BROADER IMPACTS,0.8850325379609545,"impact or why the paper does not address societal impact.
710"
BROADER IMPACTS,0.886117136659436,"• Examples of negative societal impacts include potential malicious or unintended uses
711"
BROADER IMPACTS,0.8872017353579176,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
712"
BROADER IMPACTS,0.8882863340563991,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
713"
BROADER IMPACTS,0.8893709327548807,"groups), privacy considerations, and security considerations.
714"
BROADER IMPACTS,0.8904555314533622,"• The conference expects that many papers will be foundational research and not tied
715"
BROADER IMPACTS,0.8915401301518439,"to particular applications, let alone deployments. However, if there is a direct path to
716"
BROADER IMPACTS,0.8926247288503254,"any negative applications, the authors should point it out. For example, it is legitimate
717"
BROADER IMPACTS,0.8937093275488069,"to point out that an improvement in the quality of generative models could be used to
718"
BROADER IMPACTS,0.8947939262472885,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
719"
BROADER IMPACTS,0.89587852494577,"that a generic algorithm for optimizing neural networks could enable people to train
720"
BROADER IMPACTS,0.8969631236442517,"models that generate Deepfakes faster.
721"
BROADER IMPACTS,0.8980477223427332,"• The authors should consider possible harms that could arise when the technology is
722"
BROADER IMPACTS,0.8991323210412148,"being used as intended and functioning correctly, harms that could arise when the
723"
BROADER IMPACTS,0.9002169197396963,"technology is being used as intended but gives incorrect results, and harms following
724"
BROADER IMPACTS,0.9013015184381779,"from (intentional or unintentional) misuse of the technology.
725"
BROADER IMPACTS,0.9023861171366594,"• If there are negative societal impacts, the authors could also discuss possible mitigation
726"
BROADER IMPACTS,0.903470715835141,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
727"
BROADER IMPACTS,0.9045553145336226,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
728"
BROADER IMPACTS,0.9056399132321041,"feedback over time, improving the efficiency and accessibility of ML).
729"
SAFEGUARDS,0.9067245119305857,"11. Safeguards
730"
SAFEGUARDS,0.9078091106290672,"Question: Does the paper describe safeguards that have been put in place for responsible
731"
SAFEGUARDS,0.9088937093275488,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
732"
SAFEGUARDS,0.9099783080260304,"image generators, or scraped datasets)?
733"
SAFEGUARDS,0.911062906724512,"Answer: [Yes]
734"
SAFEGUARDS,0.9121475054229935,"Justification: We describe the safeguards in our paper.
735"
SAFEGUARDS,0.913232104121475,"Guidelines:
736"
SAFEGUARDS,0.9143167028199566,"• The answer NA means that the paper poses no such risks.
737"
SAFEGUARDS,0.9154013015184381,"• Released models that have a high risk for misuse or dual-use should be released with
738"
SAFEGUARDS,0.9164859002169198,"necessary safeguards to allow for controlled use of the model, for example by requiring
739"
SAFEGUARDS,0.9175704989154013,"that users adhere to usage guidelines or restrictions to access the model or implementing
740"
SAFEGUARDS,0.9186550976138829,"safety filters.
741"
SAFEGUARDS,0.9197396963123644,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
742"
SAFEGUARDS,0.920824295010846,"should describe how they avoided releasing unsafe images.
743"
SAFEGUARDS,0.9219088937093276,"• We recognize that providing effective safeguards is challenging, and many papers do
744"
SAFEGUARDS,0.9229934924078091,"not require this, but we encourage authors to take this into account and make a best
745"
SAFEGUARDS,0.9240780911062907,"faith effort.
746"
LICENSES FOR EXISTING ASSETS,0.9251626898047722,"12. Licenses for existing assets
747"
LICENSES FOR EXISTING ASSETS,0.9262472885032538,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
748"
LICENSES FOR EXISTING ASSETS,0.9273318872017353,"the paper, properly credited and are the license and terms of use explicitly mentioned and
749"
LICENSES FOR EXISTING ASSETS,0.928416485900217,"properly respected?
750"
LICENSES FOR EXISTING ASSETS,0.9295010845986985,"Answer: [Yes]
751"
LICENSES FOR EXISTING ASSETS,0.93058568329718,"Justification: All the code, data and models we used are credited.
752"
LICENSES FOR EXISTING ASSETS,0.9316702819956616,"Guidelines:
753"
LICENSES FOR EXISTING ASSETS,0.9327548806941431,"• The answer NA means that the paper does not use existing assets.
754"
LICENSES FOR EXISTING ASSETS,0.9338394793926247,"• The authors should cite the original paper that produced the code package or dataset.
755"
LICENSES FOR EXISTING ASSETS,0.9349240780911063,"• The authors should state which version of the asset is used and, if possible, include a
756"
LICENSES FOR EXISTING ASSETS,0.9360086767895879,"URL.
757"
LICENSES FOR EXISTING ASSETS,0.9370932754880694,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
758"
LICENSES FOR EXISTING ASSETS,0.938177874186551,"• For scraped data from a particular source (e.g., website), the copyright and terms of
759"
LICENSES FOR EXISTING ASSETS,0.9392624728850325,"service of that source should be provided.
760"
LICENSES FOR EXISTING ASSETS,0.940347071583514,"• If assets are released, the license, copyright information, and terms of use in the
761"
LICENSES FOR EXISTING ASSETS,0.9414316702819957,"package should be provided. For popular datasets, paperswithcode.com/datasets
762"
LICENSES FOR EXISTING ASSETS,0.9425162689804772,"has curated licenses for some datasets. Their licensing guide can help determine the
763"
LICENSES FOR EXISTING ASSETS,0.9436008676789588,"license of a dataset.
764"
LICENSES FOR EXISTING ASSETS,0.9446854663774403,"• For existing datasets that are re-packaged, both the original license and the license of
765"
LICENSES FOR EXISTING ASSETS,0.9457700650759219,"the derived asset (if it has changed) should be provided.
766"
LICENSES FOR EXISTING ASSETS,0.9468546637744034,"• If this information is not available online, the authors are encouraged to reach out to
767"
LICENSES FOR EXISTING ASSETS,0.9479392624728851,"the asset’s creators.
768"
NEW ASSETS,0.9490238611713666,"13. New Assets
769"
NEW ASSETS,0.9501084598698482,"Question: Are new assets introduced in the paper well documented and is the documentation
770"
NEW ASSETS,0.9511930585683297,"provided alongside the assets?
771"
NEW ASSETS,0.9522776572668112,"Answer: [NA]
772"
NEW ASSETS,0.9533622559652929,"Justification: We do not release new assets.
773"
NEW ASSETS,0.9544468546637744,"Guidelines:
774"
NEW ASSETS,0.955531453362256,"• The answer NA means that the paper does not release new assets.
775"
NEW ASSETS,0.9566160520607375,"• Researchers should communicate the details of the dataset/code/model as part of their
776"
NEW ASSETS,0.9577006507592191,"submissions via structured templates. This includes details about training, license,
777"
NEW ASSETS,0.9587852494577006,"limitations, etc.
778"
NEW ASSETS,0.9598698481561823,"• The paper should discuss whether and how consent was obtained from people whose
779"
NEW ASSETS,0.9609544468546638,"asset is used.
780"
NEW ASSETS,0.9620390455531453,"• At submission time, remember to anonymize your assets (if applicable). You can either
781"
NEW ASSETS,0.9631236442516269,"create an anonymized URL or include an anonymized zip file.
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642082429501084,"14. Crowdsourcing and Research with Human Subjects
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96529284164859,"Question: For crowdsourcing experiments and research with human subjects, does the paper
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9663774403470716,"include the full text of instructions given to participants and screenshots, if applicable, as
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674620390455532,"well as details about compensation (if any)?
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685466377440347,"Answer: [NA]
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696312364425163,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707158351409978,"Guidelines:
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718004338394793,"• The answer NA means that the paper does not involve crowdsourcing nor research with
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972885032537961,"human subjects.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739696312364425,"• Including this information in the supplemental material is fine, but if the main contribu-
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750542299349241,"tion of the paper involves human subjects, then as much detail as possible should be
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761388286334056,"included in the main paper.
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772234273318872,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783080260303688,"or other labor should be paid at least the minimum wage in the country of the data
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793926247288504,"collector.
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804772234273319,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815618221258134,"Subjects
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982646420824295,"Question: Does the paper describe potential risks incurred by study participants, whether
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837310195227765,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848156182212582,"approvals (or an equivalent approval/review based on the requirements of your country or
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859002169197397,"institution) were obtained?
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869848156182213,"Answer: [NA]
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880694143167028,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891540130151844,"• The answer NA means that the paper does not involve crowdsourcing nor research with
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902386117136659,"human subjects.
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913232104121475,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924078091106291,"may be required for any human subjects research. If you obtained IRB approval, you
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934924078091106,"should clearly state this in the paper.
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945770065075922,"• We recognize that the procedures for this may vary significantly between institutions
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956616052060737,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967462039045553,"guidelines for their institution.
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978308026030369,"• For initial submissions, do not include any information that would break anonymity (if
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989154013015185,"applicable), such as the institution conducting the review.
815"
