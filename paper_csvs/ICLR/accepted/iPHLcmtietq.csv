Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033444816053511705,"Deep convolutional classiﬁers linearly separate image classes and improve accuracy
as depth increases. They progressively reduce the spatial dimension whereas the
number of channels grows with depth. Spatial variability is therefore transformed
into variability along channels. A fundamental challenge is to understand the role
of non-linearities together with convolutional ﬁlters in this transformation. ReLUs
with biases are often interpreted as thresholding operators that improve discrimi-
nation through sparsity. This paper demonstrates that it is a different mechanism
called phase collapse which eliminates spatial variability while linearly separating
classes. We show that collapsing the phases of complex wavelet coefﬁcients is
sufﬁcient to reach the classiﬁcation accuracy of ResNets of similar depths. How-
ever, replacing the phase collapses with thresholding operators that enforce sparsity
considerably degrades the performance. We explain these numerical results by
showing that the iteration of phase collapses progressively improves separation of
classes, as opposed to thresholding non-linearities."
INTRODUCTION,0.006688963210702341,"1
INTRODUCTION"
INTRODUCTION,0.010033444816053512,"CNN image classiﬁers progressively eliminate spatial variables through iterated ﬁlterings and subsam-
plings, while linear classiﬁcation accuracy improves as depth increases (Oyallon, 2017). It has also
been numerically observed that CNNs concentrate training samples of each class in small separated
regions of a progressively lower-dimensional space. It can ultimately produce a neural collapse
(Papyan et al., 2020), where all training samples of each class are mapped to a single point. In
this case, the elimination of spatial variables comes with a collapse of within-class variability and
perfect linear separability. This increase in linear classiﬁcation accuracy is obtained in standard CNN
architectures like ResNets from the iteration of linear convolutional operators and ReLUs with biases."
INTRODUCTION,0.013377926421404682,"A difﬁculty in understanding the underlying mathematics comes from the ﬂexibility of ReLUs. Indeed,
a linear combination of biased ReLUs can approximate any non-linearity. Many papers interpret
iterations on ReLUs and linear operators as sparse code computations (Sun et al., 2018; Sulam et al.,
2018; 2019; Mahdizadehaghdam et al., 2019; Zarka et al., 2020; 2021). We show that it is a different
mechanism, called phase collapse, which underlies the increase in classiﬁcation accuracy of these
architectures. A phase collapse is the elimination of phases of complex-valued wavelet coefﬁcients
with a modulus, which we show to concentrate spatial variability. This is demonstrated by introducing
a structured convolutional neural network with wavelet ﬁlters and no biases."
INTRODUCTION,0.016722408026755852,"Section 2 introduces and explains phase collapses. Complex-valued representations are used because
they reveal the mathematics of spatial variability. Indeed, translations are diagonalized in the Fourier
basis, where they become a complex phase shift. Invariants to translations are computed with a
modulus, which collapses the phases of this complex representation. Section 2 explains how this can
improve linear classiﬁcation. Phase collapses can also be calculated with ReLUs and real ﬁlters. A
CNN with complex-valued ﬁlters is indeed just a particular instance of a real-valued CNN, whose
channels are paired together to deﬁne complex numbers."
INTRODUCTION,0.020066889632107024,"Section 3 demonstrates the role of phase collapse in deep classiﬁcation architectures. It introduces a
Learned Scattering network with phase collapses. This network applies a learned 1 × 1 convolutional
complex operator Pj on each layer xj, followed by a phase collapse, which is obtained with a complex
wavelet ﬁltering operator W and a modulus:"
INTRODUCTION,0.023411371237458192,"xj+1 = |WPjxj|.
(1)"
INTRODUCTION,0.026755852842809364,Published as a conference paper at ICLR 2022
INTRODUCTION,0.030100334448160536,"It does not use any bias. This network architecture is illustrated in Figure 1. With the addition of
skip-connections, we show that this phase collapse network reaches ResNet accuracy on ImageNet
and CIFAR-10."
INTRODUCTION,0.033444816053511704,"Section 4 compares phase collapses with other non-linearities such as thresholdings or more general
amplitude reduction operators. Such non-linearities can enforce sparsity but do not modify the phase.
We show that the accuracy of a Learned Scattering network is considerably reduced when the phase
collapse modulus is replaced by soft-thresholdings with learned biases. This is also true of more
general phase-preserving non-linearities and architectures."
INTRODUCTION,0.03678929765886288,"Section 5 explains the performance of iterated phase collapses by showing that each phase collapse
progressively improves linear discriminability. On the opposite, the improvements in classiﬁcation
accuracy of successive sparse code computations are shown to quickly saturate."
INTRODUCTION,0.04013377926421405,"The main contribution of this paper is a demonstration that the classiﬁcation accuracy of deep neural
networks mostly relies on phase collapses, which are sufﬁcient to linearly separate the different classes
on natural image databases. This is captured by the Learned Scattering architecture which reaches
ResNet-18 accuracy on ImageNet and CIFAR-10. We also show that phase collapses are necessary to
reach this accuracy, by demonstrating numerically and theoretically that iterating phase-preserving
non-linearities leads to a signiﬁcantly worse performance."
INTRODUCTION,0.043478260869565216,"Figure 1: Architecture of a Learned Scattering network with phase collapses. It has J + 1 layers with
J = 11 for ImageNet and J = 8 for CIFAR-10. Each layer is computed with a 1 × 1 convolutional
operator Pj which linearly combines channels. It is followed by a phase collapse, computed with
a spatial convolutional ﬁltering with a complex wavelet W and a complex modulus |·|. A layer
of depth j corresponds to a scale 2j/2 and a subsampling by 2 is applied every two layers, after
W. A skip-connection concatenates the outputs of WPj and
WPj
. A ﬁnal 1 × 1 PJ reduces the
dimension before a linear classiﬁer."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.046822742474916385,"2
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.05016722408026756,"Deep convolutional classiﬁers achieve linear separation of image classes. We show that linear
classiﬁcation on raw images has a poor accuracy because image classes are invariant to local
translations. This geometric within-class variability takes the form of random phase ﬂuctuations, and
as a result all classes have a zero mean. To improve classiﬁcation accuracy, non-linear operators must
separate class means, which therefore requires to collapse these phase ﬂuctuations."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.05351170568561873,"Translations and phase shifts
Translations capture the spatial topology of the grid on which the
image is deﬁned. These translations are transformed into phase shifts by a Fourier transform. We
prove that this remains approximately valid for images convolved with appropriate complex ﬁlters."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.056856187290969896,"Let x be an image indexed by u ∈Z2. We write xτ(u) = x(u −τ) the translation of x by τ. It is
diagonalized by the Fourier transform bx(ω) = P"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.06020066889632107,"u x(u) e−iω·u, which creates a phase shift:"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.06354515050167224,"c
xτ(ω) = e−iω·τ bx(ω).
(2)"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.06688963210702341,"This diagonalization explains the need to introduce complex numbers to analyze the mathematical
properties of geometric within-class variabilities. Computations can however be carried with real
numbers, as we will show."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.07023411371237458,"A Fourier transform is computed by ﬁltering x with complex exponentials eiω·u. One may replace
these by complex wavelet ﬁlters ψ that are localized in space and in the Fourier domain. The following
theorem proves that small translations can still be approximated by a phase shift in this case. We
denote by ∗the convolution of images."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.07357859531772576,Published as a conference paper at ICLR 2022
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.07692307692307693,"Theorem 1. Let ψ: Z2 →C be a ﬁlter with ∥ψ∥2 = 1, whose center frequency ξ and bandwidth σ
are deﬁned by: ξ =
1 (2π)2 Z"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.0802675585284281,"[−π,π]2 ω | bψ(ω)|2 dω and σ2 =
1 (2π)2 Z"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.08361204013377926,"[−π,π]2 |ω −ξ|2| bψ(ω)|2 dω."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.08695652173913043,"Then, for any τ ∈Z2,
∥xτ ∗ψ −e−iξ·τ(x ∗ψ)∥∞≤σ |τ| ∥x∥2.
(3)"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.0903010033444816,"The proof is in Appendix C. This theorem proves that if |τ| ≪1/σ then xτ ∗ψ ≈e−iξ·τx ∗ψ. In
this case, a translation by τ produces a phase shift by ξ · τ."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.09364548494983277,"Phase collapse and stationarity
We deﬁne a phase collapse as the elimination of the phase created
by a spatial ﬁltering with a complex wavelet ψ. We now show that phase collapses improve linear
classiﬁcation of classes that are invariant to global or local translations."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.09698996655518395,"The training images corresponding to the class label y may be represented as the realizations of a
random vector Xy. To achieve linear separation, it is sufﬁcient that class means E

Xy

are separated
and within-class variances around these means are small enough (Hastie et al., 2009). The goal of
classiﬁcation is to ﬁnd a representation of the input images in which these properties hold."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.10033444816053512,"To simplify the analysis, we consider the particular case where each class y is invariant to translations.
More precisely, each random vector Xy is stationary, which means that its probability distribution is
invariant to translations. Equation (2) then implies that the phases of Fourier coefﬁcients of Xy are
uniformly distributed in [0, 2π], leading to E[ b
Xy(ω)] = 0 for ω ̸= 0. The class means E[Xy] are thus
constant images whose pixel values are all equal to E[ b
Xy(0)]. A linear classiﬁer can then only rely
on the average colors of the classes, which are often equal in practice. It thus cannot discriminate
such translation-invariant classes."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.10367892976588629,"Eliminating uniform phase ﬂuctuations of non-zero frequencies is thus necessary to create separated
class means, which can be achieved with the modulus of the Fourier transform. It is a translation-
invariant representation: |bxτ| = |bx|. This improves linear discriminability of stationary classes,
because E[| b
Xy|] may be different for different y. However, | b
Xy| has a high variance, because the
Fourier transform is unstable to small deformations (Bruna and Mallat, 2013)."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.10702341137123746,"Fourier modulus descriptors can be improved by using ﬁlters ψ that have a localized support in space.
Theorem 1 shows that the phase of Xy ∗ψ is also uniformly distributed in [0, 2π]. It results that
E[Xy ∗ψ] = 0, and x ∗ψ still provides no information for linear classiﬁcation. Applying a modulus
similarly computes approximate invariants to small translations: |xτ ∗ψ| ≈|x ∗ψ|, with an error
bounded by σ |τ| ∥x∥2. More generally, these phase collapses compute approximate invariants to
deformations which are well approximated by translations over the support of ψ. This representation
improves linear classiﬁcation by creating different non-zero class means E[|Xy ∗ψ|] while achieving
a lower variance than Fourier coefﬁcients, as it is stable to deformations (Bruna and Mallat, 2013)."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.11036789297658862,"Image classes are usually not invariant to global translations, because of e.g. centered subjects or the
sky located in the topmost part of the image. However, classes are often invariant to local translations,
up to an unknown maximum scale. This is captured by the notion of local stationarity, which means
that the probability distribution of Xy is nearly invariant to translations smaller than some maximum
scale (Priestley, 1965). The above discussion remains valid if Xy is only locally stationary over a
domain larger than the support of ψ. The use of so-called “windowed absolute spectra” E
Xy ∗ψ
"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.11371237458193979,for locally stationary processes has previously been studied in Tygert et al. (2016).
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.11705685618729098,"Real or complex networks
The use of complex numbers is a mathematical abstraction which
allows diagonalizing translations, which are then represented by complex phases. It provides a
mathematical interpretation of ﬁltering operations performed on real numbers. We show that a real
network can still implement complex phase collapses."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.12040133779264214,"In the ﬁrst layer of a CNN, one can observe that ﬁlters are often oscillatory patterns with small
supports, where some ﬁlters have nearly the same orientation and frequency but with a phase shifted
by some α (Krizhevsky et al., 2012). We reproduce in Appendix A a ﬁgure from Shang et al. (2016)
which evidences this phenomenon. It shows that real ﬁlters may be arranged in groups (ψα)α that"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.12374581939799331,Published as a conference paper at ICLR 2022
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.12709030100334448,"can be written ψα = Re(e−iαψ) for a single complex ﬁlter ψ and several phases α. A CNN with
complex ﬁlters is thus a structured real-valued CNN, where several real ﬁlters (ψα)α have been
regrouped into a single complex ﬁlter ψ. This structure simpliﬁes the mathematical interpretation of
non-linearities by explicitly deﬁning the phase, which is otherwise a hidden variable relating multiple
ﬁlter outputs within each layer."
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.13043478260869565,"A phase collapse is explicitly computed with a complex wavelet ﬁlter and a modulus. It can also be
implicitly calculated by real-valued CNNs. Indeed, for any real-valued signal x, we have:"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.13377926421404682,|x ∗ψ| = 1 2 Z π
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.13712374581939799,"−π
ReLU(x ∗ψα) dα.
(4)"
ELIMINATING SPATIAL VARIABILITY WITH PHASE COLLAPSES,0.14046822742474915,"Furthermore, this integral is well approximated by a sum over 4 phases, allowing to compute complex
moduli with real-valued ﬁlters and ReLUs without biases. See Appendix D for a proof of eq. (4) and
its approximation."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.14381270903010032,"3
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.14715719063545152,"This section introduces a learned scattering transform, which is a highly structured CNN architecture
relying on phase collapses and reaching ResNet accuracy on the ImageNet (Russakovsky et al., 2015)
and CIFAR-10 (Krizhevsky, 2009) datasets."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.1505016722408027,"Scattering transform
Theorem 1 proves that a modulus applied to the output of a complex wavelet
ﬁlter produces a locally invariant descriptor. This descriptor can then be subsampled, depending upon
the ﬁlter’s bandwidth. We brieﬂy review the scattering transform (Mallat, 2012; Bruna and Mallat,
2013), which iterates phase collapses."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.15384615384615385,"A scattering transform over J scales is implemented with a network of depth J, whose ﬁlters are
speciﬁed by the choice of wavelet. Let x0 = x. For 0 ≤j < J, the (j +1)-th layer xj+1 is computed
by applying a phase collapse on the j-th layer xj. It is implemented by a modulus which collapses
the phases created by a wavelet ﬁltering operator W:"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.15719063545150502,"xj+1 =
Wxj
.
(5)"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.1605351170568562,"The operator W is deﬁned with Morlet ﬁlters (Bruna and Mallat, 2013). It has one low-pass ﬁlter g0,
and L zero-mean complex band-pass ﬁlters (gℓ)ℓ, having an angular direction ℓπ/L for 0 < ℓ≤L.
It thus transforms an input image x(u) into L + 1 sub-band images which are subsampled by 2:"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.16387959866220736,"Wx(u, ℓ) = x ∗gℓ(2u).
(6)"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.16722408026755853,"The cascade of j low-pass ﬁlters g0 with a ﬁnal band-pass ﬁlter gℓ, each followed by a subsampling,
computes wavelet coefﬁcients at a scale 2j. One can also modify the wavelet ﬁltering W to compute
intermediate scales 2j/2, as explained in Appendix G. The spatial subsampling is then only computed
every other layer, and the depth of the network becomes twice larger. Applying a linear classiﬁer
on such a scattering transform gives good results on simple classiﬁcation problems such as MNIST
(LeCun et al., 2010). However, results are well below ResNet accuracy on CIFAR-10 and ImageNet,
as shown in Table 1."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.1705685618729097,"Learned Scattering
The prior work of Zarka et al. (2021) showed that a scattering transform can
reach ResNet accuracy by incorporating learned 1 × 1 convolutional operators and soft-thresholding
non-linearities in-between wavelet ﬁlters. In contrast, we introduce a Learned Scattering architecture
whose sole non-linearity is a phase collapse. It shows that neither biases nor thresholdings are
necessary to reach a high accuracy in image classiﬁcation. A similar result had previously been
obtained on image denoising (Mohan et al., 2019)."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.17391304347826086,"The Learned Scattering (LScat) network inserts in eq. (5) a learned complex 1 × 1 convolutional
operator Pj which reduces the channel dimensionality of each layer xj before each phase collapse:"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.17725752508361203,"xj+1 =
WPjxj
.
(7)"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.1806020066889632,"Similar architectures which separate space-mixing and channel-mixing operators had previously been
studied in the context of basis expansion (Qiu et al., 2018; Ulicny et al., 2019) or to ﬁlter scattering"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.18394648829431437,Published as a conference paper at ICLR 2022
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.18729096989966554,"Table 1: Error of linear classiﬁers applied to a scattering (Scat), learned scattering (LScat) and
learned scattering with skip connections (+ skip), on CIFAR-10 and ImageNet. The last column
gives the single-crop error of ResNet-20 for CIFAR-10 and ResNet-18 for ImageNet, taken from
https://pytorch.org/vision/stable/models.html."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.19063545150501673,"Scat
LScat
LScat + skip
ResNet"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.1939799331103679,"CIFAR-10
Top-1 error (%)
27.7
11.7
7.7
8.8"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.19732441471571907,"ImageNet
Top-5 error (%)
54.1
15.2
11.0
10.9
Top-1 error (%)
73.0
35.9
30.1
30.2"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.20066889632107024,"channels (Cotter and Kingsbury, 2019). This separation is also a major feature of recent architectures
such as Vision Transformers (Dosovitskiy et al., 2021) or MLP-Mixer (Tolstikhin et al., 2021)."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.2040133779264214,"Each Pj computes discriminative channels whose spatial variability is eliminated by the phase
collapse operator. Their role is further discussed in Section 5. Table 1 gives the accuracy of a linear
classiﬁer applied to the last layer of this Learned Scattering. It provides an important improvement
over a scattering transform, but it does not yet reach the accuracy of ResNet-18."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.20735785953177258,"Including the linear classiﬁer, the architecture uses a total number of layers J + 1 = 12 for ImageNet
and J + 1 = 9 for CIFAR, by introducing intermediate scales. The number of channels of Pjxj is
the same as in a standard ResNet architecture (He et al., 2016) and remains no larger than 512. More
details are provided in Appendix G."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.21070234113712374,"Skip-connections across moduli
Equation (7) imposes that all phases are collapsed at each layer,
after computing a wavelet transform. More ﬂexibility is provided by adding a skip-connection which
concatenates WPjxj with its modulus:"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.2140468227424749,"xj+1 =
hWPjxj
 , WPjxj
i
.
(8)"
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.21739130434782608,"The skip-connection produces a cascade of convolutional ﬁlters W without non-linearities in-between.
The resulting convolutional operator WW · · · W is a “wavelet packet” transform which generalizes
the wavelet transform (Coifman and Wickerhauser, 1992). Wavelet packets are obtained as the
cascade of low-pass and band-pass ﬁlters (gℓ)ℓ, each followed by a subsampling. Besides wavelets,
wavelet packets include ﬁlters having a larger spatial support and a narrower Fourier bandwidth. A
wavelet packet transform is then similar to a local Fourier transform. Applying a modulus on such
wavelet packet coefﬁcients deﬁnes local spatial invariants over larger domains."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.22073578595317725,"As discussed in Section 2, image classes are usually invariant to local rather than global translations.
Section 2 explains that a phase collapse improves discriminability for image classes that are locally
translation-invariant over the ﬁlter’s support. Indeed, phases of wavelet coefﬁcients are then uniformly
distributed over [0, 2π], yielding zero-mean coefﬁcients for all classes. At scales where there is no
local translation-invariance, these phases are no longer uniformly distributed, and they encode
information about the spatial localization of features. Introducing a skip-connection provides the
ﬂexibility to choose whether to eliminate phases at different scales or to propagate them up to the last
layer. Indeed, the next 1 × 1 operator Pj+1 linearly combines
WPjxj
 and WPjxj and may learn
to use only one of these. This adds some localization information, which appears to be important."
LEARNED SCATTERING NETWORK WITH PHASE COLLAPSES,0.22408026755852842,"Table 1 shows that the skip-connection indeed improves classiﬁcation accuracy. A linear classiﬁer on
this Learned Scattering reaches ResNet-18 accuracy on CIFAR-10 and ImageNet. It demonstrates
that collapsing appropriate phases is sufﬁcient to obtain a high accuracy on large-scale classiﬁcation
problems. Learning is reduced to 1 × 1 convolutions (Pj)j across channels."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.22742474916387959,"4
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.23076923076923078,"We now compare phase collapses with amplitude reductions, which are non-linearities which preserve
the phase and act on the amplitude. We show that the accuracy of a Learned Scattering network is
considerably reduced when the phase collapse modulus is replaced by soft-thresholdings with learned
biases. This result remains true for other amplitude reductions and architectures."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.23411371237458195,Published as a conference paper at ICLR 2022
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.23745819397993312,"Thresholding and sparsity
A complex soft-thresholding reduces the amplitude of its input z =
|z|eiϕ by b while preserving the phase: ρb(z) = ReLU(|z| −b) eiϕ. Similarly to its real counterpart,
it is obtained as the proximal operator of the complex modulus (Yang et al., 2012):"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2408026755852843,"ρb(z) = arg min
w∈C
b|w| + 1"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.24414715719063546,"2|w −z|2.
(9)"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.24749163879598662,"Soft-thresholdings and moduli have opposite properties, since soft-thresholdings preserve the phase
while attenuating the amplitude, whereas moduli preserve the amplitude while eliminating the phase.
In contrast, ReLUs with biases are more general non-linearities which can act both on phase and
amplitude. This is best illustrated over R where the phase is replaced by the sign, through the
even-odd decomposition. If z ∈R and b ≥0, then the even part of ReLU(z −b) is ReLU(|z| −b),
which is an absolute value with a dead-zone [−b, b]. When b = 0, it becomes an absolute value |z|.
The odd part is a soft-thresholding ρb(z) = sign(z) ReLU(|z| −b). Over C, a similar result can be
obtained through the decomposition into phase harmonics (Mallat et al., 2019)."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2508361204013378,"We have explained how phase collapses can improve the classiﬁcation accuracy of locally stationary
processes by separating class means E
Xy ∗ψ

. In contrast, since the phase of Xy ∗ψ is uniformly
distributed for such processes, then it is also true of ρb(Xy ∗ψ). This implies that E

ρb(Xy ∗ψ)

= 0
for all b. Class means of locally stationary processes are thus not separated by a thresholding."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.25418060200668896,"When class means E[Xy ∗ψ] are separated, a soft-thresholding of Xy ∗ψ may however improve
classiﬁcation accuracy. If Xy ∗ψ is sparse, then a soft-thresholding ρb(Xy ∗ψ) reduces the within-
class variance (Donoho and Johnstone, 1994; Zarka et al., 2021). Coefﬁcients below the threshold
may be assimilated to unnecessary “clutter” which is set to 0. To improve classiﬁcation, convolutional
ﬁlters must then produce high-amplitude coefﬁcients corresponding to discriminative “features”."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.25752508361204013,"Phase collapses versus amplitude reductions
A Learned Scattering with phase collapses pre-
serves the amplitudes of wavelet coefﬁcients and eliminates their phases. On the opposite, one may
use a non-linearity which preserves the phases of wavelet coefﬁcients but attenuates their amplitudes,
such as a soft-thresholding. We show that such non-linearities considerably degrade the classiﬁcation
accuracy compared to phase collapses."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2608695652173913,"Several previous works made the hypothesis that sparsifying neural responses with thresholdings
is a major mechanism for improving classiﬁcation accuracy (Sun et al., 2018; Sulam et al., 2018;
2019; Mahdizadehaghdam et al., 2019; Zarka et al., 2020; 2021). The dimensionality of sparse
representations can then be reduced with random ﬁlters which implement a form of compressed
sensing (Donoho, 2006; Candes et al., 2006). The interpretation of CNNs as compressed sensing
machines with random ﬁlters has been studied (Giryes et al., 2015), but it never led to classiﬁcation
results close to e.g. ResNet accuracy."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.26421404682274247,"To test this hypothesis, we replace the modulus non-linearity in the Learned Scattering architecture
with thresholdings, or more general phase-preserving non-linearities. A Learned Amplitude Reduction
Scattering applies a non-linearity ρ(z) which preserves the phases of wavelet coefﬁcients z = |z|eiϕ:
ρ(z) = eiϕ ρ(|z|). Without skip-connections, each layer xj+1 is computed from xj by:"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.26755852842809363,"xj+1 = ρ(WPjxj),
(10)"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2709030100334448,"and with skip-connections:
xj+1 =
h
ρ(WPjxj) , WPjxj
i
.
(11)"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.27424749163879597,"A soft-thresholding is deﬁned by ρ(|z|) = ReLU(|z| −b) for some threshold b. We also deﬁne
an amplitude hyperbolic tangent ρ(|z|) = (e|z| −e−|z|)/(e|z| + e−|z|), an amplitude sigmoid as
ρ(|z|) = (1 + e−a log |z|−b)−1 and an amplitude soft-sign as ρ(|z|) = |z|/(1 + |z|). The soft-
thresholding and sigmoid parameters a and b are learned for each layer and each channel."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.27759197324414714,"We evaluate the classiﬁcation performance of a Learned Amplitude Reduction Scattering on CIFAR-
10, by applying a linear classiﬁer on the last layer. Classiﬁcation results are given in Table 2 for
different amplitude reductions, with or without skip-connections. Learned Amplitude Reduction
Scatterings yield much larger errors than a Learned Scattering with phase collapses. Without skip-
connections, they are even above a scattering transform, which also uses phase collapses but does not"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2809364548494983,Published as a conference paper at ICLR 2022
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2842809364548495,"Table 2: Top-1 error (in %) on CIFAR-10 with a linear classiﬁer applied to a Scattering network
(Scat) and several Learned Scattering networks (LScat) with several non-linearities. They include a
modulus (Mod), an amplitude soft-thresholding (Thresh), an amplitude hyperbolic tangent (ATanh),
an amplitude sigmoid (ASigmoid), and an amplitude Soft-sign (ASign)."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.28762541806020064,"Scat
LScat"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2909698996655518,"Mod
AThresh
ATanh
ASigmoid
ASign"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.29431438127090304,"Without skip
27.7
11.7
36.7
40.7
38.5
39.9
With skip
-
7.7
22.5
19.2
17.0
19.5"
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.2976588628762542,"have learned 1 × 1 convolutional projections (Pj)j. It demonstrates that high accuracies result from
phase collapses without biases, as opposed to amplitude reduction operators including thresholdings,
which learn bias parameters. Similar experiments in the real domain with a standard ResNet-18
architecture on the ImageNet dataset can be found in Appendix B."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.3010033444816054,"ReLUs with biases
Most CNNs, including ResNets, use ReLUs with biases. A ReLU with bias
simultaneously affects the sign and the amplitude of its real input. Over complex numbers, it amounts
to transforming the phase and the amplitude. These numerical experiments show that accuracy
improvements result from acting on the sign or phase rather than the amplitude. Furthermore, this
can be constrained to collapsing the phase of wavelet coefﬁcients while preserving their amplitude."
PHASE COLLAPSES VERSUS AMPLITUDE REDUCTIONS,0.30434782608695654,"Several CNN architectures have demonstrated a good classiﬁcation accuracy with iterated thresh-
olding algorithms, which increase sparsity. However, all these architecture also modiﬁed the sign of
coefﬁcients by computing non-negative sparse codes (Sun et al., 2018; Sulam et al., 2018; Mahdizade-
haghdam et al., 2019) or with additional ReLU or modulus layers (Zarka et al., 2020; 2021). It seems
that it is the sign or phase collapse of these non-linearities which is responsible for good classiﬁcation
accuracies, as opposed to the calculation of sparse codes through iterated amplitude reductions."
ITERATING PHASE COLLAPSES AND AMPLITUDE REDUCTIONS,0.3076923076923077,"5
ITERATING PHASE COLLAPSES AND AMPLITUDE REDUCTIONS"
ITERATING PHASE COLLAPSES AND AMPLITUDE REDUCTIONS,0.3110367892976589,"We now provide a theoretical justiﬁcation to the above numerical results in simpliﬁed mathematical
frameworks. This section studies the behavior of phase collapses and amplitude reductions when they
are iterated over several layers. It shows that phase collapses beneﬁt from iterations over multiple
layers, whereas there is no signiﬁcant gain in performance when iterating amplitude reductions."
ITERATED PHASE COLLAPSES,0.31438127090301005,"5.1
ITERATED PHASE COLLAPSES"
ITERATED PHASE COLLAPSES,0.3177257525083612,"We explain the role of iterated phase collapses with multiple ﬁlters at each layer. Classiﬁcation
accuracy is improved through the creation of additional dimensions to separate class means. The
learned projectors (Pj)j are optimized for this separation."
ITERATED PHASE COLLAPSES,0.3210702341137124,"We consider the classiﬁcation of stationary processes Xy ∈Rd, corresponding to different image
classes indexed by y. Given a realization x of Xy, and because of stationarity, the optimal linear clas-
siﬁer is calculated from the empirical mean 1/d P
u x(u). It computes an optimal linear estimation
of E

Xy(u)

= µy. If all classes have the same mean µy = µ, then all linear classiﬁers fail."
ITERATED PHASE COLLAPSES,0.32441471571906355,"As explained in Section 2, linear classiﬁcation can be improved by computing (|x ∗ψk|)k for some
wavelet ﬁlters (ψk)k. These phase collapses create additional directions with non-zero means which
may separate the classes. If Xy is stationary, then |Xy ∗ψk| remains stationary for any ψk. An
optimal linear classiﬁer applied to (|x ∗ψk(u)|)k is thus obtained by a linear combination of all
empirical means (1/d P
u |x ∗ψk(u)|)k. They are proportional to the ℓ1 norm ∥x ∗ψk∥1, which is a
measure of sparsity of x ∗ψk."
ITERATED PHASE COLLAPSES,0.3277591973244147,"If linear classiﬁcation on (|x ∗ψk(u)|)k fails, it reveals that the means E

|Xy ∗ψk(u)|

= µy,k
are not sufﬁciently different. Separation can be improved by considering the spatial variations of
|Xy ∗ψk(u)| for different y. These variations can be revealed by a phase collapse on a new set of"
ITERATED PHASE COLLAPSES,0.3311036789297659,Published as a conference paper at ICLR 2022
ITERATED PHASE COLLAPSES,0.33444816053511706,"wavelet ﬁlters ψk′, which computes (||x∗ψk|∗ψk′|)k,k′. This phase collapse iteration is the principle
used by scattering transforms to discriminate textures (Bruna and Mallat, 2013; Sifre and Mallat,
2013): each successive phase collapse creates additional directions to separate class means."
ITERATED PHASE COLLAPSES,0.3377926421404682,"However, this may still not be sufﬁcient to separate class means. More discriminant statistical
properties may be obtained by linearly combining (|x ∗ψk|)k across k before applying a new ﬁlter
ψk′. In a Learned Scattering with phase collapse, this is done with a linear projector P1 across
the channel indices k, before computing a convolution with the next ﬁlter ψk′. The 1 × 1 operator
P1 is optimized to improve the linear classiﬁcation accuracy. It amounts to learning weights wk
such that E[
P"
ITERATED PHASE COLLAPSES,0.3411371237458194,"k wk
Xy ∗ψk
 ∗ψk′] is as different as possible for different y. Because these are
proportional to the ℓ1 norms
P"
ITERATED PHASE COLLAPSES,0.34448160535117056,"k wk|x ∗ψk| ∗ψk′
1, it means that the images P"
ITERATED PHASE COLLAPSES,0.34782608695652173,"k wk|x ∗ψk|∗ψk′
have different sparsity levels depending upon the class y of x. The weights (wk)k of P1 can thus
be interpreted as features along channels providing different sparsiﬁcations for different classes. A
Learned Scattering network learns such Pj at each scale j."
ITERATED AMPLITUDE REDUCTIONS,0.3511705685618729,"5.2
ITERATED AMPLITUDE REDUCTIONS"
ITERATED AMPLITUDE REDUCTIONS,0.35451505016722407,"Sparse representations and amplitude reduction algorithms may improve linear classiﬁcation by
reducing the variance of class mean estimations, which can be interpreted as clutter removal. Such
approaches are studied in Zarka et al. (2021) by modeling the clutter as an additive white noise.
Although a single thresholding step may improve linear classiﬁcation, we show that iterating more
than one thresholding does not improve the classiﬁcation accuracy, if no phase collapses are inserted."
ITERATED AMPLITUDE REDUCTIONS,0.35785953177257523,"To understand these properties, we consider the discrimination of classes Xy for which class means
E(Xy) = µy are all different. If there exists y′ such that ∥µy −µy′∥is small, then the class y can still
be discriminated from y′ if we can estimate E(Xy) sufﬁciently accurately from a single realization x
of Xy. This is a mean estimation problem. Suppose that Xy = µy + N(0, σ2) is contaminated with
Gaussian white noise, where the noise models some clutter. Suppose also that there exists a linear
orthogonal operator D such that Dµy is sparse for every y, and hence has its energy concentrated
in few non-zero coefﬁcients. Such a D may be computed by minimizing the expected ℓ1 norm
P"
ITERATED AMPLITUDE REDUCTIONS,0.3612040133779264,"y E

∥DXy∥1

. The estimation of µy can be improved with a soft-thresholding estimator (Donoho
and Johnstone, 1994), which sets to zero all coefﬁcients below a threshold b proportional to σ. It
amounts to computing ρb(Dx), where ρb is a soft-thresholding."
ITERATED AMPLITUDE REDUCTIONS,0.36454849498327757,"However, we explain below why this approach cannot be further iterated without inserting phase
collapses. The reason is that a sparse representation ρb(Dx) concentrates its entropy in the phases
of the coefﬁcients, rather than their amplitude. We then show that such processes cannot be further
sparsiﬁed, which means that a second thresholding ρb′(D′ρb(Dx)) will not reduce further the variance
of class mean estimators. This entails that a model of within-class variability relying on amplitude
reductions cannot be the sole mechanism behind the performance of deep networks."
ITERATED AMPLITUDE REDUCTIONS,0.36789297658862874,"Iterating amplitude reductions may however be useful if it is alternated with another non-linearity
which partly or fully collapses phases. Reducing the entropy of the phases of ρb(Dx) allows ρb′D′"
ITERATED AMPLITUDE REDUCTIONS,0.3712374581939799,"to further sparsify the process and hence further reduce the within-class variability. As mentioned
in Section 4, this is the case for previous work which used iterated sparsiﬁcation operators (Sun
et al., 2018; Sulam et al., 2018; Mahdizadehaghdam et al., 2019). Indeed, these networks compute
non-negative sparse codes where sparsity is enforced with a ReLU, which acts both on phases and
amplitudes. Our results shows that the beneﬁt of iterating non-negative sparse coding comes from the
sign collapse due to the non-negativity constraint."
ITERATED AMPLITUDE REDUCTIONS,0.3745819397993311,"We now qualitatively demonstrate these claims with two theorems. We ﬁrst show that ﬁnding the
sparsest representation of a random process (i.e., minimizing its ℓ1 norm) is the same as maximizing
a lower bound on the entropy of its phases."
ITERATED AMPLITUDE REDUCTIONS,0.3779264214046823,"Theorem 2. Let X denote a random vector in Cd with a probability density p. Let H(X) be the
entropy of X with respect to the Lebesgue measure:"
ITERATED AMPLITUDE REDUCTIONS,0.38127090301003347,"H(X) = −
Z
p(x) log p(x) dx."
ITERATED AMPLITUDE REDUCTIONS,0.38461538461538464,Published as a conference paper at ICLR 2022
ITERATED AMPLITUDE REDUCTIONS,0.3879598662207358,If D ∈U(d) is a unitary operator then:
ITERATED AMPLITUDE REDUCTIONS,0.391304347826087,"H

ϕ(DX)
 |DX|

≥H(X) −d −2d log
1"
ITERATED AMPLITUDE REDUCTIONS,0.39464882943143814,"dE[∥DX∥1]

,"
ITERATED AMPLITUDE REDUCTIONS,0.3979933110367893,"where ϕ(DX) ∈[0, 2π]d (resp. |DX| ∈Rd
+) is the random process of the entry-wise phases (resp.
moduli) of DX."
ITERATED AMPLITUDE REDUCTIONS,0.4013377926421405,"The proof is in Appendix E. This theorem gives a lower-bound on the conditional entropy of the phases
of DX with a decreasing function of the expected ℓ1 norm of DX. Minimizing over D this expected
ℓ1 norm amounts to maximizing the lower bound on H

ϕ(DX)
 |DX|

. An extreme situation"
ITERATED AMPLITUDE REDUCTIONS,0.40468227424749165,"arises when this entropy reaches its maximal value of d log(2π). In this case, the phase ϕ(DX) has a
maximum-entropy distribution and is therefore uniformly distributed in [0, 2π]d. Moreover, in this
extreme case ϕ(DX) is independent from |DX|, since its conditional distribution does not depend
on |DX|. Such statistical properties have previously been observed on wavelet coefﬁcients of natural
images (Rao et al., 2001), where the wavelet transform seems to be a nearly optimal sparsifying
unitary dictionary."
ITERATED AMPLITUDE REDUCTIONS,0.4080267558528428,"The second theorem considers the extreme case of a random process whose phases are conditionally
independent and uniform. It proves that such a process cannot be signiﬁcantly sparsiﬁed with a
change of basis."
ITERATED AMPLITUDE REDUCTIONS,0.411371237458194,"Theorem 3. Assume that ϕ(ρb(DX)) is uniformly distributed in [0, 2π]d and independent from
|ρb(DX)|. Then there exists a constant Cd > 0 which depends on the dimension d, such that for any
D′ ∈U(d),
E

∥D′ρb(DX)∥1

≥CdE[∥ρb(DX)∥1]."
ITERATED AMPLITUDE REDUCTIONS,0.41471571906354515,"The proof is in Appendix F. This theorem shows that random processes with conditionally in-
dependent and uniform phases have an ℓ1 norm which cannot be signiﬁcantly decreased by any
unitary transformation. Numerical evaluations suggest that the constant Cd may be chosen to be
√π/2 ≈0.886, independently of the dimension d. This constant arises as the value of E[|Z|] when
Z is a complex normal random variable with E[|Z|2] = 1."
ITERATED AMPLITUDE REDUCTIONS,0.4180602006688963,"These two theorems explain qualitatively that linear classiﬁcation on ρb(Dx) cannot be improved by
another thresholding that would take advantage of another sparsiﬁcation operator. Indeed, Theorem 2
shows that if ρb(Dx) is sparse, then its phases have random ﬂuctuations of high entropy. Theorem 3
indicates that such random phases prevent a further sparsiﬁcation of ρb(Dx) with some linear operator
D′. Applying a second thresholding ρb′(D′ρb(Dx)) thus cannot signiﬁcantly reduce the variance of
class mean estimators."
CONCLUSION,0.4214046822742475,"6
CONCLUSION"
CONCLUSION,0.42474916387959866,"This paper studies the improvement of linear separability for image classiﬁcation in deep convolutional
networks. We show that it mostly relies on a phase collapse phenomenon. Eliminating the phase of
wavelet coefﬁcients improves the separation of class means. We introduced a Learned Scattering
network with wavelet phase collapses and learned 1 × 1 convolutional ﬁlters (Pj)j, which reaches
ResNet accuracy. The learned 1 × 1 operators (Pj) enhance discriminability by computing channels
that have different levels of sparsity for different classes."
CONCLUSION,0.4280936454849498,"When class means are separated, thresholding non-linearities can improve classiﬁcation by reducing
the variance of class mean estimators. When used alone, the classiﬁcation performance is poor over
complex datasets such as ImageNet or CIFAR-10, because class means are not sufﬁciently separated.
Furthermore, the iteration of thresholdings on sparsiﬁcation operators requires intermediary phase
collapses."
CONCLUSION,0.431438127090301,"These results show that linear separation of classes result from acting on the sign or phase of network
coefﬁcients rather than their amplitude. Furthermore, this can be constrained to collapsing the phase
of wavelet coefﬁcients while preserving their amplitude. The elimination of spatial variability with
phase collapses is thus both necessary and sufﬁcient to linearly separate classes on complex image
datasets."
CONCLUSION,0.43478260869565216,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.43812709030100333,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4414715719063545,"The code to reproduce the experiments of the paper is available at https://github.com/
FlorentinGuth/PhaseCollapse. All experimental details and hyperparameters are also
provided in Appendix G."
REPRODUCIBILITY STATEMENT,0.44481605351170567,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.44816053511705684,"This work was supported by a grant from the PRAIRIE 3IA Institute of the French ANR-19-P3IA-
0001 program. We would like to thank the Scientiﬁc Computing Core at the Flatiron Institute for the
use of their computing resources. We also thank Antoine Brochard, Brice Ménard and Rudy Morel
for heplful comments."
REFERENCES,0.451505016722408,REFERENCES
REFERENCES,0.45484949832775917,"E. Oyallon. Analyzing and Introducing Structures in Deep Convolutional Neural Networks. Theses,
Paris Sciences et Lettres, October 2017."
REFERENCES,0.45819397993311034,"V. Papyan, X. Y. Han, and D. L. Donoho. Prevalence of neural collapse during the terminal phase of
deep learning training. Proceedings of the National Academy of Sciences, 2020."
REFERENCES,0.46153846153846156,"X. Sun, N. M. Nasrabadi, and T. D. Tran. Supervised deep sparse coding networks. In 2018 25th
IEEE International Conference on Image Processing (ICIP), pages 346–350, 2018."
REFERENCES,0.46488294314381273,"J. Sulam, V. Papyan, Y. Romano, and M. Elad. Multilayer convolutional sparse modeling: Pursuit
and dictionary learning. IEEE Transactions on Signal Processing, 66(15):4090–4104, 2018."
REFERENCES,0.4682274247491639,"J. Sulam, A. Aberdam, A. Beck, and M. Elad. On multi-layer basis pursuit, efﬁcient algorithms and
convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2019."
REFERENCES,0.47157190635451507,"S. Mahdizadehaghdam, A. Panahi, H. Krim, and L. Dai. Deep dictionary learning: A parametric
network approach. IEEE Transactions on Image Processing, 28(10):4790–4802, Oct 2019."
REFERENCES,0.47491638795986624,"J. Zarka, L. Thiry, T. Angles, and S. Mallat. Deep network classiﬁcation by scattering and homotopy
dictionary learning. In International Conference on Learning Representations, ICLR, 2020."
REFERENCES,0.4782608695652174,"J. Zarka, F. Guth, and S. Mallat. Separation and concentration in deep networks. In International
Conference on Learning Representations, ICLR, 2021."
REFERENCES,0.4816053511705686,"T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: data mining, inference
and prediction. Springer, 2 edition, 2009."
REFERENCES,0.48494983277591974,"J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach.
Intell., 35(8):1872–1886, 2013."
REFERENCES,0.4882943143812709,"M. B. Priestley. Evolutionary spectra and non-stationary processes. Journal of the Royal Statis-
tical Society: Series B (Methodological), 27(2):204–229, 1965. doi: https://doi.org/10.1111/
j.2517-6161.1965.tb01488.x. URL https://rss.onlinelibrary.wiley.com/doi/
abs/10.1111/j.2517-6161.1965.tb01488.x."
REFERENCES,0.4916387959866221,"M. Tygert, J. Bruna, S. Chintala, Y. LeCun, S. Piantino, and A. Szlam. A mathematical motivation
for complex-valued convolutional networks. Neural computation, 28(5):815–825, 2016."
REFERENCES,0.49498327759197325,"A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in Neural Information Processing Systems 25, NeurIPS, pages 1097–1105,
2012."
REFERENCES,0.4983277591973244,"W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural
networks via concatenated rectiﬁed linear units, 2016."
REFERENCES,0.5016722408026756,"O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV), 115(3):211–252, 2015."
REFERENCES,0.5050167224080268,Published as a conference paper at ICLR 2022
REFERENCES,0.5083612040133779,"A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.5117056856187291,"S. Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):
1331–1398, 2012."
REFERENCES,0.5150501672240803,"Y. LeCun, C. Cortes, and C.J. Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010."
REFERENCES,0.5183946488294314,"S. Mohan, Z. Kadkhodaie, E. P. Simoncelli, and C. Fernandez-Granda. Robust and interpretable
blind image denoising via bias-free convolutional neural networks. In International Conference on
Learning Representations, 2019."
REFERENCES,0.5217391304347826,"Q. Qiu, X. Cheng, R. Calderbank, and G. Sapiro. DCFNet: Deep neural network with decomposed
convolutional ﬁlters. International Conference on Machine Learning, 2018."
REFERENCES,0.5250836120401338,"M. Ulicny, V. Krylov, and R. Dahyot. Harmonic networks for image classiﬁcation. In Proceedings of
the British Machine Vision Conference, Sep. 2019."
REFERENCES,0.5284280936454849,"F. Cotter and N. G. Kingsbury. A learnable scatternet: Locally invariant convolutional layers. In
2019 IEEE International Conference on Image Processing, ICIP, pages 350–354. IEEE, 2019."
REFERENCES,0.5317725752508361,"A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words:
Transformers for image recognition at scale. ICLR, 2021."
REFERENCES,0.5351170568561873,"I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner,
D. Keysers, J. Uszkoreit, M. Lucic, and A. Dosovitskiy. Mlp-mixer: An all-mlp architecture for
vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.5384615384615384,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016."
REFERENCES,0.5418060200668896,"R.R. Coifman and M.V. Wickerhauser. Entropy-based algorithms for best basis selection. IEEE
Transactions on Information Theory, 38(2):713–718, 1992. doi: 10.1109/18.119732."
REFERENCES,0.5451505016722408,"Z. Yang, C. Zhang, and L. Xie. On phase transition of compressed sensing in the complex domain.
IEEE Signal Processing Letters, 19(1):47–50, Jan 2012. ISSN 1558-2361. doi: 10.1109/lsp.2011.
2177496. URL http://dx.doi.org/10.1109/LSP.2011.2177496."
REFERENCES,0.5484949832775919,"S. Mallat, S. Zhang, and G. Rochette. Phase harmonic correlations and convolutional neural networks.
Information and Inference: A Journal of the IMA, 9(3):721–747, 11 2019."
REFERENCES,0.5518394648829431,"D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81(3):
425–455, 09 1994."
REFERENCES,0.5551839464882943,"D. L. Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–1306,
2006."
REFERENCES,0.5585284280936454,"E. J. Candes, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate
measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences, 59(8):1207–1223, 2006."
REFERENCES,0.5618729096989966,"R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A
universal classiﬁcation strategy? CoRR, abs/1504.08291, 2015. URL http://arxiv.org/
abs/1504.08291."
REFERENCES,0.5652173913043478,"L. Sifre and S. Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1233–
1240, 2013."
REFERENCES,0.568561872909699,"R. Rao, B. Olshausen, M. Lewicki, M. Wainwright, O. Schwartz, and E. P. Simoncelli. Natural image
statistics and divisive normalization: Modeling nonlinearities and adaptation in cortical neurons.
Statistical Theories of the Brain, 01 2001."
REFERENCES,0.5719063545150501,Published as a conference paper at ICLR 2022
REFERENCES,0.5752508361204013,"S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning - Volume 37, page 448–456, 2015."
REFERENCES,0.5785953177257525,"M. Andreux, T. Angles, G. Exarchakis, R. Leonarduzzi, G. Rochette, L. Thiry, J. Zarka, S. Mallat,
J. Andén, E. Belilovsky, J. Bruna, V. Lostanlen, M. J. Hirn, E. Oyallon, S. Zhang, C. E. Cella, and
M. Eickenberg. Kymatio: Scattering transforms in python. Journal of Machine Learning Research,
21(60):1–6, 2020."
REFERENCES,0.5819397993311036,Published as a conference paper at ICLR 2022
REFERENCES,0.5852842809364549,"A
PAIRED ALEXNET FILTERS"
REFERENCES,0.5886287625418061,"Section 2 explains that real networks can still implement phase collapses. This is done with several
real ﬁlters ψα = Re(e−iαψ) which correspond to several phases α of the same complex ﬁlter ψ.
Shang et al. (2016) showed that the ﬁlters in e.g. the ﬁrst layer of AlexNet (Krizhevsky et al., 2012)
can indeed be grouped in such a way. For the sake of completeness, we reproduce in Figure 2 a
ﬁgure from Shang et al. (2016). This suggests that real-valued networks may indeed implement phase
collapses using eq. (4)."
REFERENCES,0.5919732441471572,"Figure 2: First-layer ﬁlters from AlexNet (Krizhevsky et al., 2012). They have been paired so that they
approximately correspond to two different phases of the same complex ﬁlter ψ. Figure reproduced
from Shang et al. (2016)."
REFERENCES,0.5953177257525084,"B
PHASE COLLAPSE VERSUS AMPLITUDE REDUCTION WITH RESNET"
REFERENCES,0.5986622073578596,"We now evaluate the classiﬁcation error of phase collapses and amplitude reduction non-linearities in
the real domain. We use a standard ResNet-18 architecture without biases. We replace the ReLU
non-linearity by an absolute value or sign collapse |x| and several sign-preserving (i.e., odd) non-
linearities. They include a soft-thresholding ρb(x) = sign(x) ReLU(|x| −b), an hyperbolic tangent
ρ(x) = (ex −e−x)/(ex + e−x), and a soft-sign ρ(x) = x/(1 + |x|). We do not report results for an
amplitude sigmoid ρ(x) = sign(x)(1 + e−a log |x|−b)−1 because of optimization instabilities when
learning the parameters a and b."
REFERENCES,0.6020066889632107,"Classiﬁcation results on the ImageNet dataset are given in Table 3. The error of bias-free ReLUs and
sign collapses are comparable to a standard ResNet-18, and conﬁrm that sign collapses are sufﬁcient
to reach such accuracies. In contrast, the performance of amplitude reduction non-linearities, which
preserve the sign of network coefﬁcients, is signiﬁcantly worse. The conclusions of Section 4 thus
still hold in the real domain and when the spatial ﬁlters are not constrained to be wavelets."
REFERENCES,0.6053511705685619,"Table 3: Classiﬁcation errors on ImageNet of bias-free ResNet-18 (BFResNet) architectures with
several non-linearities. They include a ReLU, an absolute value which performs sign collapses (Abs),
a soft-thresholding (Thresh), a hyperbolic tangent (Tanh), and a soft-sign (Sign). They are compared
to the original ResNet-18 architecture, which uses a ReLU and learns biases."
REFERENCES,0.6086956521739131,"ResNet
BFResNet"
REFERENCES,0.6120401337792643,"ReLU
Abs
Thresh
Tanh
Sign"
REFERENCES,0.6153846153846154,"Top-5 error (%)
10.9
12.3
13.9
25.7
22.4
24.2
Top-1 error (%)
30.2
32.6
35.3
50.0
44.6
49.3"
REFERENCES,0.6187290969899666,Published as a conference paper at ICLR 2022
REFERENCES,0.6220735785953178,"C
PROOF OF THEOREM 1"
REFERENCES,0.6254180602006689,We have:
REFERENCES,0.6287625418060201,"∥xτ ∗ψ −e−iξ·τ(x ∗ψ)∥∞= ∥x ∗(ψτ −e−iξ·τψ)∥∞
by covariance of convolution,"
REFERENCES,0.6321070234113713,"≤∥ψτ −e−iξ·τψ∥2∥x∥2
by Young’s inequality,"
REFERENCES,0.6354515050167224,and then:
REFERENCES,0.6387959866220736,"∥ψτ −e−iξ·τψ∥
2
2 =
1 (2π)2 Z"
REFERENCES,0.6421404682274248,"[−π,π]2 |c
ψτ(ω) −e−iξ·τ bψ(ω)|2dω
by Plancherel, =
1 (2π)2 Z"
REFERENCES,0.6454849498327759,"[−π,π]2 |e−iω·τ bψ(ω) −e−iξ·τ bψ(ω)|2dω
since ψτ(u) = ψ(u −τ), =
1 (2π)2 Z"
REFERENCES,0.6488294314381271,"[−π,π]2 |e−iω·τ −e−iξ·τ|2| bψ(ω)|2dω ≤
1 (2π)2 Z"
REFERENCES,0.6521739130434783,"[−π,π]2 |(ω −ξ) · τ|2| bψ(ω)|2dω
since x ∈R 7→eix is 1-Lipschitz, ≤
1 (2π)2 Z"
REFERENCES,0.6555183946488294,"[−π,π]2 |ω −ξ|2|τ|2| bψ(ω)|2dω
by Cauchy-Schwarz,"
REFERENCES,0.6588628762541806,"= σ2|τ|2,"
REFERENCES,0.6622073578595318,which leads to the desired result of eq. (3):
REFERENCES,0.6655518394648829,∥xτ ∗ψ −e−iξ·τ(x ∗ψ)∥∞≤σ |τ| ∥x∥2.
REFERENCES,0.6688963210702341,"D
PROOF OF EQUATION (4)"
REFERENCES,0.6722408026755853,"We have:
ReLU(x ∗ψα) = ReLU(x ∗Re(e−iαψ)) = ReLU(Re(e−iαx ∗ψ)),"
REFERENCES,0.6755852842809364,"since x is real. By writing: x ∗ψ = |x ∗ψ|eiϕ(x∗ψ) where ϕ(x ∗ψ) is the phase of x ∗ψ, this leads
to:"
REFERENCES,0.6789297658862876,"ReLU(Re(e−iαx ∗ψ)) = ReLU(Re(|x ∗ψ|ei(ϕ(x∗ψ)−α)))
= ReLU(|x ∗ψ| cos(ϕ(x ∗ψ) −α))
= |x ∗ψ| ReLU(cos(ϕ(x ∗ψ) −α)),"
REFERENCES,0.6822742474916388,"since ReLU activation is positive-homogeneous of degree 1. Thus: 1
2 Z π"
REFERENCES,0.68561872909699,"−π
ReLU(x ∗ψα)dα = 1 2 Z π"
REFERENCES,0.6889632107023411,"−π
|x ∗ψ| ReLU(cos(ϕ(x ∗ψ) −α))dα = 1"
REFERENCES,0.6923076923076923,"2|x ∗ψ|
Z π−ϕ(x∗ψ)"
REFERENCES,0.6956521739130435,"−π−ϕ(x∗ψ)
ReLU(cos(−α))dα
with a change of variable, = 1"
REFERENCES,0.6989966555183946,"2|x ∗ψ|
Z π"
REFERENCES,0.7023411371237458,"−π
ReLU(cos(α))dα
since cos is 2π periodic and even, = 1"
REFERENCES,0.705685618729097,"2|x ∗ψ|
Z π/2"
REFERENCES,0.7090301003344481,"−π/2
cos(α)dα"
REFERENCES,0.7123745819397993,= |x ∗ψ|.
REFERENCES,0.7157190635451505,"For z ∈C, we have |z| =
q"
REFERENCES,0.7190635451505016,"|Re(z)|2 + |Im(z)|2 ≈|Re(z)| + |Im(z)| in the following sense: 1
√"
REFERENCES,0.7224080267558528,2(|Re(z)| + |Im(z)|) ≤|z| ≤|Re(z)| + |Im(z)|.
REFERENCES,0.725752508361204,Published as a conference paper at ICLR 2022
REFERENCES,0.7290969899665551,We can write:
REFERENCES,0.7324414715719063,"|Re(z)| = ReLU(Re(z)) + ReLU(−Re(z)),
|Im(z)| = ReLU(Im(z)) + ReLU(−Im(z))."
REFERENCES,0.7357859531772575,"and then, using Im(z) = Re(eiπ/2z) and eiπ = −1:"
REFERENCES,0.7391304347826086,|z| ≈ReLU(Re(z)) + ReLU(Re(e−iπz)) + ReLU(Re(e−iπ/2z)) + ReLU(Re(eiπ/2z)).
REFERENCES,0.7424749163879598,"Finally,"
REFERENCES,0.745819397993311,|x ∗ψ| = 1 2 Z π
REFERENCES,0.7491638795986622,"−π
ReLU(x ∗ψα)dα ≈
X"
REFERENCES,0.7525083612040134,"α∈{−π/2,0,π/2,π}
ReLU(Re(x ∗ψα)),"
REFERENCES,0.7558528428093646,"which shows that the integral can be well approximated with a sum of 4 phases α of the complex
ﬁlter ψ."
REFERENCES,0.7591973244147158,"E
PROOF OF THEOREM 2"
REFERENCES,0.7625418060200669,We ﬁrst use the chain rule for the entropy:
REFERENCES,0.7658862876254181,"H

ϕ(DX)
 |DX|

= H(|DX|, ϕ(DX)) −H(|DX|)."
REFERENCES,0.7692307692307693,The ﬁrst term is rewritten with a change of variable:
REFERENCES,0.7725752508361204,"H(|DX|, ϕ(DX)) = H(DX) − d
X"
REFERENCES,0.7759197324414716,"k=1
E[log |(DX)k|]"
REFERENCES,0.7792642140468228,"= H(X) − d
X"
REFERENCES,0.782608695652174,"k=1
E[log |(DX)k|]
as D is unitary and hence |det(D)| = 1,"
REFERENCES,0.7859531772575251,"≥H(X) −dE

log
1"
REFERENCES,0.7892976588628763,d∥DX∥1
REFERENCES,0.7926421404682275,"
by concavity,"
REFERENCES,0.7959866220735786,"≥H(X) −d log
1"
REFERENCES,0.7993311036789298,"dE[∥DX∥1]

by concavity."
REFERENCES,0.802675585284281,"The second term is bounded using the fact that the exponential distribution E(λ) is the maximum-
entropy distribution on R+ with mean 1 λ:"
REFERENCES,0.8060200668896321,"H(|DX|) ≤ d
X"
REFERENCES,0.8093645484949833,"k=1
H(|(DX)k|) ≤ d
X"
REFERENCES,0.8127090301003345,"k=1
log(eE[|(DX)k|])"
REFERENCES,0.8160535117056856,"≤d log
e"
REFERENCES,0.8193979933110368,"dE[∥DX∥1]

by concavity."
REFERENCES,0.822742474916388,Combining both inequalities and rearranging terms yields the stated bound:
REFERENCES,0.8260869565217391,"H

ϕ(DX)
 |DX|

≥H(X) −d −2d log
1"
REFERENCES,0.8294314381270903,"dE[∥DX∥1]

."
REFERENCES,0.8327759197324415,"F
PROOF OF THEOREM 3"
REFERENCES,0.8361204013377926,"We begin with the following lemma:
Lemma 1. Let (θ1, . . . , θd) be i.i.d. uniform random variables in [0, 2π]. Then there exists a constant
Cd > 0 such that for all (ρ1, . . . , ρd) ∈Rd, then: E "" | d
X"
REFERENCES,0.8394648829431438,"k=1
ρkeiθk| # ≥Cd"
REFERENCES,0.842809364548495,"v
u
u
t d
X"
REFERENCES,0.8461538461538461,"k=1
ρ2
k."
REFERENCES,0.8494983277591973,Published as a conference paper at ICLR 2022
REFERENCES,0.8528428093645485,"This is proved by observing that the left-hand side is a norm on Rd. One can indeed verify that it
is positive deﬁnite, homogeneous and satisﬁes the triangle inequality. Since all norms on Rd are
equivalent, there exists a constant Cd > 0 such that: E "" | d
X"
REFERENCES,0.8561872909698997,"k=1
ρkeiθk| # ≥Cd"
REFERENCES,0.8595317725752508,"v
u
u
t d
X"
REFERENCES,0.862876254180602,"k=1
ρ2
k."
REFERENCES,0.8662207357859532,"for all (ρ1, . . . , ρd) ∈Rd."
REFERENCES,0.8695652173913043,"Going back to the proof of Theorem 3, and letting X′ = ρb(DX), we then have:"
REFERENCES,0.8729096989966555,"E
h
∥D′X′∥1
 |X′|
i
= d
X m=1
E "" | d
X"
REFERENCES,0.8762541806020067,"k=1
D′
m,kX′
k| |X′| # ≥Cd d
X m=1"
REFERENCES,0.8795986622073578,"v
u
u
t d
X"
REFERENCES,0.882943143812709,"k=1
|D′
m,k|
2|X′
k|
2
by the above lemma, ≥Cd d
X m=1 d
X"
REFERENCES,0.8862876254180602,"k=1
|D′
m,k|
2|X′
k|
by concavity, because d
X"
REFERENCES,0.8896321070234113,"k=1
|D′
m,k|
2 = 1,"
REFERENCES,0.8929765886287625,"= Cd
X′
1
because d
X"
REFERENCES,0.8963210702341137,"m=1
|D′
m,k|
2 = 1."
REFERENCES,0.8996655518394648,Taking the expectation ﬁnishes the proof:
REFERENCES,0.903010033444816,"E

∥D′X′∥1

≥CdE

∥X′∥1

.
(12)"
REFERENCES,0.9063545150501672,"G
EXPERIMENTAL DETAILS"
REFERENCES,0.9096989966555183,"Channel operators
In all experiments we set P0 = Id, and factorize the classiﬁer with an additional
complex 1 × 1 convolutional operator PJ, which reduces the dimension before all channels and
positions are linearly combined. The architectures implemented are thus also written as QJ
j=1 PjρW,
where ρ is the non-linearity. Each operator (Pj)1≤j≤J is preceded by a standardization. It sets the
complex mean µ = E[z] of every channel to zero, and the real variance σ2 = E[|z|2] of every channel
to one. This is similar to a complex 2D batch-normalization layer (Ioffe and Szegedy, 2015), but
without learned afﬁne parameters. Each operator (Pj)1≤j≤J is additionally followed by a spatial
divisive normalization (Rao et al., 2001), similarly to the local response normalization of Krizhevsky
et al. (2012). It sets the norm across channels of each spatial position to one. The sizes of the (Pj)j
are speciﬁed in Table 4."
REFERENCES,0.9130434782608695,"The total numbers of parameters for each architecture are speciﬁed in Table 5. Learned Scattering
with phase collapse have a large number of parameters compared to ResNet, despite the comparable
width. This is because the predeﬁned wavelet operator W expands the dimension by a factor of
L + 1, which means that the input dimension of the learned (Pj)j is higher than in ResNet. The
skip-connection further increases this input dimension by a factor of 2."
REFERENCES,0.9163879598662207,"Table 4: Number cj of complex output channels of Pj, 1 ≤j ≤J. The total number of projectors is
J = 8 for CIFAR and J = 11 for ImageNet."
REFERENCES,0.919732441471572,"j
1
2
3
4
5
6
7
8
9
10
11"
REFERENCES,0.9230769230769231,"CIFAR-10
cj
64
128
256
512
512
512
512
512
-
-
-"
REFERENCES,0.9264214046822743,"ImageNet
cj
32
64
64
128
256
512
512
512
512
512
256"
REFERENCES,0.9297658862876255,Published as a conference paper at ICLR 2022
REFERENCES,0.9331103678929766,"Table 5: Number of real parameters (in millions) of Learned Scattering network architectures. A
complex parameter is counted as two real parameters."
REFERENCES,0.9364548494983278,"PCScat
PCScat + skip
ResNet"
REFERENCES,0.939799331103679,"CIFAR-10
41.6
83.1
0.27"
REFERENCES,0.9431438127090301,"ImageNet
36.0
62.8
11.7"
REFERENCES,0.9464882943143813,"Spatial ﬁlters
We use elongated Morlet ﬁlters for the L complex band-pass ﬁlters (gℓ)ℓwhich are
rotated versions of a mother wavelet g: gℓ(u) = g(r−πℓ/Lu), with rθ the rotation by angle θ. The
mother wavelet g is deﬁned as:"
REFERENCES,0.9498327759197325,"g(u) =
σ2"
REFERENCES,0.9531772575250836,"2π/s2 (eiξ·u −K)e−u·Σu/2
with Σ =

σ2
0
0
σ2s2"
REFERENCES,0.9565217391304348,"
,
(13)"
REFERENCES,0.959866220735786,"Its parameters are its center frequency ξ = ((3π/4)/2γ, 0), its bandwidth σ = 1.25 × 2−γ, and its
slant s = 0.5, where 2γ designates the scale of the band-pass ﬁlter and is to be adjusted."
REFERENCES,0.9632107023411371,"g is rotated along L = 8 angles for Imagenet and L = 4 angles for CIFAR: θℓ= (πℓ/L)1≤ℓ≤L. The
(gℓ)ℓare then discretized for numerical computations, and K is adjusted so that they have a zero
mean."
REFERENCES,0.9665551839464883,"Finally, we use for the low frequency g0 a Gaussian window:"
REFERENCES,0.9698996655518395,g0(u) = σ2
REFERENCES,0.9732441471571907,"2π e−σ2∥u∥2
2/2."
REFERENCES,0.9765886287625418,"The ﬁlters are implemented with the Kymatio package (Andreux et al., 2020)."
REFERENCES,0.979933110367893,"Intermediate scales 2j/2 are obtained by applying a subsampling by 2 after each block of 2 layers.
This introduces intermediate scales and generates a wavelet ﬁlterbank with 2 scales per octave: the
ﬁlters are designed so that when j low-pass ﬁlters and one band-pass ﬁlter are cascaded, with a
subsampling every 2 layers, the scale of the resulting wavelet is 2j/2."
REFERENCES,0.9832775919732442,"Each block comprises in its ﬁrst layer a low-frequency ﬁlter g1
0 with γ = −1/2 and band-pass ﬁlters
with γ = 0. In the second layer, we use the same low-frequency ﬁlter g2
0 = g1
0 with γ = −1/2. The
band-pass ﬁlters g2
ℓare obtained with parameters ξ′ = (π/
√"
REFERENCES,0.9866220735785953,"2, 0), σ′ = 1.25
p"
REFERENCES,0.9899665551839465,"2/3, and s′ =
√ 0.2."
REFERENCES,0.9933110367892977,"For CIFAR experiments, the J = 8 layers are grouped in 4 successive blocks of 2 layers. For
ImageNet experiments, the ﬁrst layer consists of band-pass elongated Morlet ﬁlters gℓand a low-pass
Gaussian window g0 with γ = 0, followed by a subsampling of 2. The 10 following layers are
grouped in 5 blocks of 2 layers."
REFERENCES,0.9966555183946488,"Optimization
We use the optimizer SGD with an initial learning rate of 0.01, a momentum of
0.9, a weight decay of 0.0001, and a batch size of 128. The classiﬁer is preceded by a 2D batch-
normalization layer. We use traditional data augmentation: horizontal ﬂips and random crops for
CIFAR, random resized crops of size 224 and horizontal ﬂips for ImageNet. Classiﬁcation error on
ImageNet validation set is computed on a single center crop of size 224. On CIFAR, training lasts
for 300 epochs and the learning rate is divided by 10 every 70 epochs. On ImageNet, training lasts
for 150 epochs and the learning rate is divided by 10 every 45 epochs. All experiments ran during
the preparation of this paper, including preliminary ones, required around 10k 32GB NVIDIA V100
GPU-hours."
