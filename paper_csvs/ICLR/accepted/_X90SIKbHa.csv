Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007874015748031496,"Anderson mixing (AM) is a powerful acceleration method for ﬁxed-point itera-
tions, but its computation requires storing many historical iterations. The extra
memory footprint can be prohibitive when solving high-dimensional problems in
a resource-limited machine. To reduce the memory overhead, we propose a novel
class of short-term recurrence AM methods (ST-AM). The ST-AM methods only
store two previous iterations with cheap corrections. We prove that the basic ver-
sion of ST-AM is equivalent to the full-memory AM in strongly convex quadratic
optimization, and with minor changes it has local linear convergence for solving
general nonlinear ﬁxed-point problems. We further analyze the convergence prop-
erties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally,
we apply ST-AM to several applications including solving root-ﬁnding problems
and training neural networks. Experimental results show that ST-AM is competi-
tive with the long-memory AM and outperforms many existing optimizers."
INTRODUCTION,0.0015748031496062992,"1
INTRODUCTION"
INTRODUCTION,0.002362204724409449,"Anderson mixing (AM) (Anderson, 1965) is a powerful sequence acceleration method (Brezinski
et al., 2018) for ﬁxed-point iterations and has been widely used in scientiﬁc computing (Lin et al.,
2019; Fu et al., 2020; An et al., 2017), e.g., the self-consistent ﬁeld iterations in electronic structure
computations (Garza & Scuseria, 2012; Arora et al., 2017). Speciﬁcally, we consider a ﬁxed-point
iteration xk+1 = g(xk), k = 0, 1, . . . , where g : Rd 7→Rd is the ﬁxed-point map. By using
m historical iterations, AM(m) aims to extrapolate a new iterate that satisﬁes certain optimality
property. When the function evaluation is costly, the reduction of the number of iterations brought
by AM can save a large amount of computation (Fang & Saad, 2009)."
INTRODUCTION,0.0031496062992125984,"AM can be used as a method for solving nonlinear equations (Kelley, 2018) as the ﬁxed-point
problem x = g(x) is equivalent to h(x) := x −g(x) = 0. In practice, since computing the
Jacobian of h(x) is commonly difﬁcult or even unavailable (Nocedal & Wright, 2006), AM can be
seen as a practical alternate for Newton’s method (An et al., 2017). Also, compared with the classical
iterative methods such as the nonlinear conjugate gradient (CG) method (Hager & Zhang, 2006), no
line-search or trust-region technique is used in AM, which is preferable for large-scale unconstrained
optimization. Empirically, it is observable that AM can largely accelerate convergence, though its
theoretical analysis is still under-explored. It turns out that in the linear case (Walker & Ni, 2011;
Potra & Engler, 2013), the full-memory AM (m = k) is essentially equivalent to GMRES (Saad
& Schultz, 1986), a powerful Krylov subspace method that can exhibit superlinear convergence
behaviour in solving linear systems (Van der Vorst & Vuik, 1993). For general nonlinear problems,
AM is recognized as a multisecant quasi-Newton method (Fang & Saad, 2009; Brezinski et al.,
2018). As far as we know, only local linear convergence has been obtained for the limited-memory
AM (m < k) in general (Toth & Kelley, 2015; Evans et al., 2020; De Sterck & He, 2021)."
INTRODUCTION,0.003937007874015748,"For the application of AM, one of the major concerns is the historical length m, a critical factor
related to the efﬁciency of AM (Walker & Ni, 2011). A larger m can incorporate more historical"
INTRODUCTION,0.004724409448818898,∗Corresponding author.
INTRODUCTION,0.005511811023622047,Published as a conference paper at ICLR 2022
INTRODUCTION,0.006299212598425197,"information into one extrapolation, but it incurs heavier memory overhead since 2m vectors of
dimension d need to be stored in AM(m). The additional memory footprint can be prohibitive for
solving high-dimensional problems in a resource-limited machine (Deng, 2019). Using small m
can alleviate the memory overhead but may deteriorate the efﬁcacy of AM since much historical
information is omitted in the extrapolation (Walker & Ni, 2011; Evans et al., 2020)."
INTRODUCTION,0.007086614173228346,"To address the memory issue of AM, we deeply investigate the properties of the historical iterations
produced by AM and leverage them to develop the short-term recurrence variant, namely ST-AM.
The basic version of ST-AM imposes some orthogonality property on the historical sequence, which
is inspired by the CG method (Hestenes & Stiefel, 1952) that enjoys a three-term recurrence. Fur-
thermore, to better suit the more difﬁcult nonconvex optimization, a regularized short-term form is
introduced. We highlight the main contributions of our work as follows."
INTRODUCTION,0.007874015748031496,"1. We develop a novel class of short-term recurrence AM methods (ST-AM), including the
basic ST-AM, the modiﬁed ST-AM (MST-AM), and the regularized ST-AM (RST-AM).
The basic ST-AM is applicable for linear systems; MST-AM can solve general ﬁxed-point
problems; RST-AM aims for solving stochastic optimization. An important feature of ST-
AM is that all methods only need to store two previous iterations with cheap corrections,
which signiﬁcantly reduces the memory requirement compared with the classical AM.
2. A complete theoretical analysis of the ST-AM methods is given. When solving strongly
convex quadratic optimization, we prove that the basic ST-AM is equivalent to the full-
memory AM and the convergence rate is similar to that of the CG method. We also prove
that MST-AM has improved local linear convergence for solving ﬁxed-point problems.
Besides, we establish the global convergence property and complexity analysis for RST-
AM when solving stochastic optimization problems.
3. The numerical results on solving (non)linear equations and cubic-regularized quadratic op-
timization are consistent with the theoretical results for the basic ST-AM and MST-AM.
Furthermore, extensive experiments on training neural networks for image classiﬁcation
and language modeling show that RST-AM is competitive with the long-memory AM and
outperforms many existing optimizers such as SGD and Adam."
RELATED WORK,0.008661417322834646,"2
RELATED WORK"
RELATED WORK,0.009448818897637795,"AM is also known as an extrapolation algorithm in scientiﬁc computing (Anderson, 2019). A paral-
lel method is Shanks transformation (Shanks, 1955) which transforms an existing sequence to a new
sequence for faster convergence. Related classical algorithms include Minimal Polynomial Extrap-
olation (Cabay & Jackson, 1976) and Reduced Rank Extrapolation (Eddy, 1979), and a framework
of these extrapolation algorithms including AM is given in (Brezinski et al., 2018). Note that an
elegant recursive algorithm named ϵ-algorithm had been discovered for Shanks transformation for
scalar sequence (Wynn, 1956), and was later generalized as the vector ϵ-algorithm (Wynn, 1962) to
handle vector sequences, but this short-term recurrence form is not equivalent to the original Shanks
transformation in general (Brezinski & Redivo-Zaglia, 2017). Since AM is closely related to quasi-
Newton methods (Fang & Saad, 2009), there are also some works trying to derive equivalent forms
of the full-memory quasi-Newton methods using limited memory (Kolda et al., 1998; Berahas et al.,
2021), while no short-term recurrence is available. To the best of our knowledge, ST-AM is the ﬁrst
attempt to short-term recurrence quasi-Newton methods."
RELATED WORK,0.010236220472440945,"Recently, there have been growing demands for solving large-scale and high-dimensional ﬁxed-
point problems in scientiﬁc computing (Lin et al., 2019) and machine learning (Bottou et al., 2018).
For these applications, Newton-like methods (Byrd et al., 2016; Wang et al., 2017; Mokhtari et al.,
2018) are less appealing due to the heavy memory and computational cost, especially in nonconvex
stochastic optimization, where only sublinear convergence can be expected if only stochastic gradi-
ents can be accessed (Nemirovski & Yudin, 1983). On the other side, ﬁrst-order methods (Necoara
et al., 2019) stand out for their low per-iteration cost, though the convergence can be slow in prac-
tice. When training neural networks, SGD with momentum (SGDM) (Qian, 1999), and adaptive
learning rate methods, e.g. AdaGrad (Duchi et al., 2011), RMSprop (Tieleman & Hinton, 2012),
Adam (Kingma & Ba, 2014), are very popular optimizers. Our methods have the nature of quasi-
Newton methods while the memory footprint is largely reduced to be close to ﬁrst-order methods.
Thus, ST-AM can be a competitive optimizer from both theoretical and practical perspectives."
RELATED WORK,0.011023622047244094,Published as a conference paper at ICLR 2022
METHODOLOGY,0.011811023622047244,"3
METHODOLOGY"
METHODOLOGY,0.012598425196850394,"In this section, we give the details of the proposed ST-AM. We always assume the objective function
as f : Rd →R, the ﬁxed-point map g : Rd 7→Rd. Moreover, we do not distinguish rk = −∇f(xk)
and rk = g(xk) −xk in our discussion as ∇f(x) = 0 is equivalent to g(x) = x −∇f(x) = x."
ANDERSON MIXING,0.013385826771653543,"3.1
ANDERSON MIXING"
ANDERSON MIXING,0.014173228346456693,The AM ﬁnds the ﬁxed point of g via maintaining two sequences of length m (m ≤k):
ANDERSON MIXING,0.014960629921259842,"Xk = [∆xk−m, ∆xk−m+1, · · · , ∆xk−1], Rk = [∆rk−m, ∆rk−m+1, · · · , ∆rk−1] ∈Rd×m,
(1)"
ANDERSON MIXING,0.015748031496062992,"where the operator ∆denotes the forward difference, e.g. ∆xk = xk+1 −xk. Each update of AM
can be decoupled into two steps, namely the projection step and the mixing step:"
ANDERSON MIXING,0.01653543307086614,"¯xk = xk −XkΓk,
(Projection step),
xk+1 = ¯xk + βk¯rk,
(Mixing step),
(2)"
ANDERSON MIXING,0.01732283464566929,where ¯rk := rk −RkΓk and βk > 0 is the mixing parameter. The Γk is determined by
ANDERSON MIXING,0.01811023622047244,"Γk = arg min
Γ∈Rm ∥rk −RkΓ∥2.
(3)"
ANDERSON MIXING,0.01889763779527559,"Thus, the full form of AM (Fang & Saad, 2009; Walker & Ni, 2011) is"
ANDERSON MIXING,0.01968503937007874,"xk+1 = xk + βkrk −(Xk + βkRk) Γk.
(4)"
ANDERSON MIXING,0.02047244094488189,"Remark 1. To see the rationality of AM, assume g is continuously differentiable, then we have
h(xj)−h(xj−1) ≈h′(xk)(xj−xj−1) around xk, where h′(xk) is the Jacobian of h(x) := x−g(x).
So, it is reasonable to assume Rk ≈−h′(xk)Xk, and we see ∥rk −RkΓ∥2 ≈∥rk + h′(xk)XkΓ∥2.
Thus, we can recognize (3) as solving h′(xk)dk = h(xk) in a least-squares sense, where dk =
XkΓk. The mixing step incorporates rk into the new update xk+1 if βk > 0. Otherwise, if βk = 0,
then xk+1 = ¯xk is an interpolation of the previous iterates, leading to a stagnation."
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.02125984251968504,"3.2
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.02204724409448819,The basic ST-AM is to solve the strongly convex quadratic optimization:
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.02283464566929134,"min
x∈Rd f(x) := 1"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.023622047244094488,"2xTAx −bTx,
(5)"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.024409448818897637,"where A ≻0. Let p−1 = q−1 = p0 = q0 = 0 ∈Rd. At the k-th iteration, given the two matrices
Pk−1 = (pk−2, pk−1) ∈Rd×2, Qk−1 = (qk−2, qk−1) ∈Rd×2 and deﬁning p = xk −xk−1 and
q = rk −rk−1, the basic ST-AM constructs"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.025196850393700787,"˜p = p −Pk−1(QT
k−1q),
˜q = q −Qk−1(QT
k−1q),
(6a)"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.025984251968503937,"pk = ˜p/∥˜q∥2,
qk = ˜q/∥˜q∥2.
(6b)"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.026771653543307086,"Then, we update Pk = (pk−1, pk), Qk = (qk−1, qk) ∈Rd×2. Such construction ensures QT
k Qk =
I2 for k ≥2 and the storage of Pk and Qk is equal to AM(2). With the corrected Pk and Qk, the
ST-AM method modiﬁes the projection step and the mixing step accordingly, that is,"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.027559055118110236,"¯xk = xk −PkΓk,
(Projection step),
xk+1 = ¯xk + βk¯rk,
(Mixing step),
(7)"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.028346456692913385,"where Γk = arg min ∥rk −QkΓ∥2 = QT
k rk and ¯rk = rk −QkΓk. Thus, the ST-AM replaces Xk
and Rk in (1) by Pk and Qk respectively and imposes the orthogonality condition on Qk. The details
of basic ST-AM are given in Algorithm 2 in Appendix C.1. Deﬁne ¯Pk = (p1, p2, . . . , pk), ¯Qk =
(q1, q2, . . . , qk), the Krylov subspace Km(A, v) ≡span{v, Av, A2v, . . . , Am−1v}, the range of X
as range(X). We give the properties of the basic ST-AM in Theorem 1.
Theorem 1. Let {xk} be the sequence generated by the basic ST-AM. The following relations hold:
(i) ∥˜q∥2 > 0, range( ¯Pk) = range(Xk) = Kk(A, r0), range( ¯Qk) = range(Rk) = AKk(A, r0);
(ii) ¯Qk = −A ¯Pk, ¯QT
k ¯Qk = Ik;
(iii) ¯rk ⊥range( ¯Qk) and ¯xk = x0 + zk, where zk = arg minz∈Kk(A,r0) ∥r0 −Az∥2.
If ∥¯rk∥2 = 0, then xk+1 is the exact solution."
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.029133858267716535,Published as a conference paper at ICLR 2022
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.029921259842519685,"The proof is in Appendix C.1. Note that the property (iii) in Theorem 1 exactly describes the relation
¯xk = xG
k, where xG
k is the output of the k-th iteration of GMRES (Saad & Schultz, 1986). Moreover,
let ¯xAM
k
be the k-th intermediate iterate in the full-memory AM. It holds that ¯xAM
k
= xG
k (See
Proposition 1 in Appendix C.1.), which induces that ¯xk = ¯xAM
k
= xG
k. This equivalence indicates
that ST-AM is more efﬁcient than AM and GMRES since only two historical iterations need to
be stored. Moreover, by directly applying the convergence analysis of GMRES (Corollary 6.33 in
(Saad, 2003)), we obtain the convergence rate of the basic ST-AM for solving (5):
Corollary 1. Suppose the eigenvalues of A lie in [µ, L] with µ > 0, and let {xk} be the se-
quence generated by the basic ST-AM, then the k-th intermediate residual ¯rk satisﬁes ∥¯rk∥2 ≤ 2
√"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.030708661417322834,"L/µ−1
√ L/µ+1"
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.031496062992125984,"k
∥r0∥2. Moreover, the algorithm ﬁnds the exact solution in at most (d+1) iterations."
THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING,0.03228346456692913,"Remark 2. The GMRES can be simpliﬁed to an elegant three-term recurrence algorithm called the
conjugate residual (CR) method (Algorithm 6.20 in (Saad, 2003)) when solving (5). Thus, a similar
simpliﬁcation for AM is expected to exist. Like CG and Chebyshev acceleration (Algorithm 12.1 in
(Saad, 2003)), the convergence rate of ST-AM has the optimal dependence on the condition number,
while ST-AM does not form the Hessian-vector products explicitly."
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03307086614173228,"3.3
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03385826771653543,"For general nonlinear ﬁxed-point problems, global convergence may be unavailable for the basic ST-
AM, as a counter-example exists for AM (Mai & Johansson, 2020). Thus, we propose a modiﬁed
version of the basic ST-AM (MST-AM) and prove the local linear convergence rate under similar
conditions used in (Toth & Kelley, 2015; Evans et al., 2020). Concretely, the MST-AM makes three
main changes to the basic ST-AM."
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03464566929133858,"Change 1: Instead of applying the normalization (6b), the MST-AM constructs pk and qk via"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03543307086614173,"ζk = (QT
k−1Qk−1)†QT
k−1q,
pk = p −Pk−1ζk,
qk = q −Qk−1ζk,
(8)"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03622047244094488,"where “ † ” is the Moore-Penrose inverse. Accordingly, we choose Γk = arg min ∥rk −QkΓ∥2 =
(QT
k Qk)†QT
k rk. This change relaxes the orthonormality for Qk (k ≥2), but keeps the orthogonality
condition: QT
k−1qk = 0. In fact, ¯QT
k−1qk = 0 in the case of solving (5)."
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03700787401574803,"Change 2: MST-AM imposes the boundedness constraints on Pk−1ζk and Qk−1ζk: If ∥Pk−1ζk∥2 >
cp∥p∥2 or ∥Qk−1ζk∥2 > cq∥q∥2, then Pk = Pk−1, Qk = Qk−1, where cp > 0, cq ∈(0, 1) are
predeﬁned constants. It is worth mentioning that adding some boundedness condition is common in
the analysis of AM (Toth & Kelley, 2015; Evans et al., 2020)."
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03779527559055118,"Change 3: MST-AM restarts, i.e. setting Pk = Qk = 0 ∈Rd×2 every m iterations. This restart
operation is to limit the number of higher-order terms appeared in the residual expansion in our
analysis and we can set m to be a large number in practice."
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03858267716535433,"The detailed description of MST-AM is given in Appendix C.2. In the next theorem, we establish
the convergence rate analysis for the MST-AM.
Theorem 2. Let {xk} be the sequence generated by MST-AM, x∗∈Rd be a ﬁxed point of g and m
be the restarting period for MST-AM. Suppose that in the ball B(ρ) := {x ∈Rd|∥x−x∗∥2 < ρ} for
some ρ > 0, g is Lipschitz continuously differentiable and there are constants κ ∈(0, 1) and ˆκ > 0
with (i) ∥g(y) −g(x)∥2 ≤κ∥y −x∥2 for every x, y ∈B(ρ), and (ii) ∥g′(y) −g′(x)∥2 ≤ˆκ∥y −x∥2
for every x, y ∈B(ρ), where g′ is the Jacobian of g. Assume |1 −βk| + κβk ≤κ0 for a constant
κ0 ∈(0, 1). If x0 is sufﬁciently close to x∗, then for rk := g(xk) −xk, the following bound holds:"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.03937007874015748,"∥rk+1∥2 ≤θk(|1 −βk| + κβk)∥rk∥2 + ˆκ mk
X"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04015748031496063,"j=0
O
 
∥rk−j∥2
2

,
(9)"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04094488188976378,"where θk = ∥¯rk∥2/∥rk∥2 ≤1 and mk = k mod m. Thus, the residuals {rk} converge Q-linearly,
and the errors {∥xk −x∗∥2} converge R-linearly.
Remark 3. In a local region around x∗, the convergence rate is determined by the ﬁrst-order term
θk(|1 −βk| + κβk)∥rk∥2. We can choose βk = 1 such that |1 −βk| + κβk = κ < 1. Since
¯rk is the orthogonal projection of rk onto the subspace range(Qk)⊥, θk has the interpretation of"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04173228346456693,Published as a conference paper at ICLR 2022
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04251968503937008,Algorithm 1 RST-AM for stochastic programming
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04330708661417323,"Input: x0 ∈Rd, βk > 0, αk ∈[0, 1], δ(1)
k
> 0, δ(2)
k
> 0.
Output: x ∈Rd"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04409448818897638,"1: P0, Q0 = 0 ∈Rd×2, p0, q0 = 0 ∈Rd"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04488188976377953,"2: for k = 0, 1, . . . , until convergence, do
3:
rk = −∇fSk(xk)
4:
if k > 0 then
5:
p = xk −xk−1, q = rk −rk−1
6:
ζk = (QT
k−1Qk−1 + δ(1)
k P T
k−1Pk−1)†QT
k−1q
7:
qk = q −Qk−1ζk, pk = p −Pk−1ζk
8:
Pk = [pk−1, pk], Qk = [qk−1, qk]
9:
end if
10:
Check Condition (13) and use smaller αk if (13) is violated
11:
Γk = (QT
k Qk + δ(2)
k P T
k Pk)†QT
k rk
12:
¯xk = xk −αkPkΓk, ¯rk = rk −αkQkΓk
13:
xk+1 = ¯xk + βk¯rk
14:
Apply learning rate schedule of αk, βk
15: end for
16: return xk"
THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING,0.04566929133858268,"the direction-sine between rk and the subspace range(Qk). When θk is small, e.g., rk nearly lies in
range(Qk), the acceleration by MST-AM is signiﬁcant. Compared to AM(m), MST-AM incorporates
historical information with orthogonalization. In the SPD linear case and without restart, the global
orthogonality property holds, i.e. ¯rk ⊥range( ¯Qk), which means there is no loss of historical
information, while AM(m) (Evans et al., 2020) does not have such property in this ideal case."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.046456692913385826,"3.4
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.047244094488188976,"Inspired by the recent work on stochastic Anderson mixing (SAM) method (Wei et al., 2021), we
develop a regularized ST-AM (RST-AM) for solving nonconvex stochastic optimization problems."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.048031496062992125,Consider the nonconvex optimization problem minx∈Rd f(x) := 1
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.048818897637795275,"T
PT
i=1 fξi(x), where fξi : Rd →
R is the loss function corresponding to i-th data sample and T is the number of data samples. In
mini-batch training, the gradient is evaluated for fSk(xk) :=
1
nk
P"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.049606299212598425,"i∈Sk fξi(xk), where Sk ⊆[T] :=
{1, 2, . . . , T} is the sampled mini-batch, and nk := |Sk| is the batch size. In this case, we set
rk = −∇fSk(xk) (Line 3 in Algorithm 1), which is an unbiased estimate of the negative gradient."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.050393700787401574,"Recalling from (8), ζk = (QT
k−1Qk−1)†QT
k−1∆rk−1 = arg min ∥∆rk−1 −Qk−1ζ∥2 as q = ∆rk−1
by deﬁnition. Since ST-AM is based on a local quadratic approximation (5) in a small region around
xk, a large magnitude of ∥Pk−1ζk∥2 tends to make the change from ∆xk−1 to pk = ∆xk−1 −
Pk−1ζk too aggressively, which may lead to instability. Consequently, we add a penalty term in the
above least squares problem, i.e."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.051181102362204724,"ζk = arg min ∥∆rk−1 −Qk−1ζ∥2
2 + δ(1)
k ∥Pk−1ζ∥2
2,
(10)"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05196850393700787,"where δ(1)
k
> 0. The same as SAM (Wei et al., 2021), we also add a regularization term for comput-
ing Γk via
Γk = arg min ∥rk −QkΓ∥2
2 + δ(2)
k ∥PkΓ∥2
2,
(11)"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05275590551181102,"where δ(2)
k
> 0, and a damping term αk is used as shown in Line 12 in Algorithm 1. In practice, we
choose the two regularization parameters as"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05354330708661417,"δ(1)
k
=
c1∥rk∥2
2
∥∆xk−1∥2
2 + ϵ0
, δ(2)
k
= max
 c2∥rk∥2
2
∥pk∥2
2 + ϵ0
, Cβ−2
k"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05433070866141732,"
,
(12)"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05511811023622047,"where c1, c2, C > 0 are constants, and ϵ0 > 0 is a small constant to bound the denominators
away from zero.
Assuming ∥pk−1∥2 ≈∥pk∥2 = O(∥∆xk−1∥2), the choices of (12) make
∥δ(1)
k P T
k−1Pk−1∥2 ≈O(∥rk∥2
2) and ∥δ(2)
k P T
k Pk∥2 ≈O(∥rk∥2
2) aware of the change of the local
curvature: large (small) ∥rk∥2 tends to lead to a large (small) regularization."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05590551181102362,Published as a conference paper at ICLR 2022
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05669291338582677,"Remark 4.
One update of xk given by Line 11-13 in Algorithm 1 can be formulated as xk+1 =
xk + Hkrk, where Hk = βkI −αkYkZ†
kQT
k , Yk = Pk + βkQk, Zk = QT
k Qk + δ(2)
k P T
k Pk. To
guarantee the positive deﬁniteness of Hk, we follow the same procedure in SAM. Let λk be the
largest eigenvalue of YkZ†
kQT
k + QkZ†
kY T
k . If αk satisﬁes
αkλk ≤2βk(1 −µ),
(13)"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05748031496062992,"then sT
k Hksk ≥βkµ∥sk∥2
2, ∀sk ∈Rd, where µ ∈(0, 1) is a constant. Note that λk can be cheaply
obtained by computing the largest eigenvalue of a matrix of R4×4 (see Appendix C.3.1)."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05826771653543307,"We summarize the RST-AM in Algorithm 1 and establish its convergence properties here. First, we
impose the same assumptions on the objective function f as those in (Wei et al., 2021).
Assumption 1. f : Rd →R is continuously differentiable. f(x) ≥f low > −∞for any x ∈Rd.
∇f is globally L-Lipschitz continuous; namely ∥∇f(x)−∇f(y)∥2 ≤L∥x−y∥2 for any x, y ∈Rd.
Assumption 2. For any iteration k, the stochastic gradient ∇fξk(xk) satisﬁes Eξk[∇fξk(xk)] =
∇f(xk), Eξk[∥∇fξk(xk) −∇f(xk)∥2
2] ≤σ2, where σ > 0, and ξk, k = 0, 1, . . . are independent
samples that are independent of {xj}k
j=0."
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05905511811023622,"The diminishing condition about βk is +∞
X"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.05984251968503937,"k=0
βk = +∞, +∞
X"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.06062992125984252,"k=0
β2
k < +∞.
(14)"
THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING,0.06141732283464567,"We give the convergence properties of RST-AM in nonconvex (stochastic) optimization and proofs
are deferred to Appendix C.3.2.
Theorem 3. Suppose Assumption 1 hold and {xk} is the sequence generated by full-batch RST-AM,
i.e. nk = T. Let βk = β ∈(0,
µ
2L(1+C−1)] be a constant, αk ∈[0, 1] and satisﬁes (13), then"
N,0.06220472440944882,"1
N N−1
X"
N,0.06299212598425197,"k=0
∥∇f(xk)∥2
2 ≤2(f(x0) −f low)"
N,0.06377952755905512,"Nµβ
,
(15)"
N,0.06456692913385827,in the N iterations. To ensure 1
N,0.06535433070866142,"N
PN−1
k=0 ∥∇f(xk)∥2
2 < ϵ, the number of iterations is O(1/ϵ) .
Theorem 4. Suppose Assumptions 1 and 2 hold and {xk} is the sequence generated by RST-AM"
N,0.06614173228346457,"with batch size nk = n ≤T. If βk ∈(0,
µ
4L(1+C−1)] and satisﬁes (14), αk ∈[0, min{1, β"
N,0.06692913385826772,"1
2
k }] and
satisﬁes (13), then
lim inf
k→∞∥∇f(xk)∥2 = 0 with probability 1
and
∃Mf > 0 →E[f(xk)] ≤Mf, ∀k.
(16)"
N,0.06771653543307087,"If Eξk[∥∇fξk(xk)∥2
2] ≤Mg, ∀k, where Mg > 0 is a constant, we have
lim
k→∞∥∇f(xk)∥2 = 0 with probability 1.
(17)"
N,0.06850393700787402,"Theorem 5. Suppose Assumptions 1 and 2 hold and {xk}N−1
k=0 is the ﬁrst N iterations generated by
RST-AM with ﬁxed batch size nk = n. Let βk = min{
µ
4L(1+C−1),
˜
D
σ
√"
N,0.06929133858267716,"N }, where ˜D is a problem-"
N,0.07007874015748032,"independent constant; αk ∈[0, min{1, β"
N,0.07086614173228346,"1
2
k }] and satisﬁes (13). Let R be a random variable follow-
ing PR(k) := Prob{R = k} = 1/N, then"
N,0.07165354330708662,"E[∥∇f(xR)∥2
2] ≤16DfL(1 + C−1)"
N,0.07244094488188976,"Nµ2
+
σ
µ
√ N"
DF,0.07322834645669292,4Df
DF,0.07401574803149606,"˜D
+ 4(L + µ−1)(1 + C−1) ˜D n !"
DF,0.07480314960629922,",
(18)"
DF,0.07559055118110236,"where Df := f(x0) −f low and the expectation is taken with respect to R and {Sj}N−1
j=0 . To ensure
E[∥∇f(xR)∥2
2] ≤ϵ, the number of iterations is O(1/ϵ2).
Remark 5. The proofs of Theorem 4 and 5 are based on the analysis of SAM (Wei et al., 2021).
The theorems show that the convergence of RST-AM is no worse than SGD (Robbins & Monro,
1951). There are two key differences between RST-AM and SAM: RST-AM is based on short-term
recurrences while SAM usually maintains longer historical sequences to ensure effectiveness; RST-
AM uses additional correction and regularization terms (Line 5-8 in Algorithm 1) to incorporate
historical information while SAM simply discards the oldest iteration to make space for ∆xk−1
and ∆rk−1. The reduced memory requirement in RST-AM makes it applicable for solving more
challenging problems in machine learning."
DF,0.07637795275590552,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.07716535433070866,"4
EXPERIMENTS"
EXPERIMENTS,0.07795275590551182,"We validated the effectiveness of our proposed ST-AM methods in various applications in ﬁxed-point
iterations and nonconvex optimization, including linear and nonlinear problems, deterministic and
stochastic optimization. Speciﬁcally, we ﬁrst tested ST-AM in linear problems, cubic-regularized
quadratic minimization (Carmon & Duchi, 2020) and a multiscale deep equilibrium (MDEQ) model
(Bai et al., 2020). Then we applied RST-AM to train neural networks and compared them with
several ﬁrst-order and second-order optimizers. Experimental details are in Appendix D."
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.07874015748031496,"4.1
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.07952755905511812,"We veriﬁed the properties of ST-AM declared in Theorem 1 and 2 by solving four problems (details
are in Appendix D.1): (I) strongly convex quadratic optimization (corresponding to Theorem 1);
(II) solving a nonsymmetric linear system Ax = b (corresponding to ˆκ = 0 in Theorem 2); (III)
cubic-regularized quadratic minimization minx∈Rd f(x) := ∥Ax−b∥2
2 + M"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08031496062992126,"3 ∥x∥3
2 (corresponding to
ˆκ > 0 in Theorem 2); (IV) root-ﬁnding problems in MDEQ on CIFAR-10 (Krizhevsky et al., 2009)."
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08110236220472442,"The compared methods were gradient descent (GD), ﬁxed-point iteration (FP), conjugate residual
method (CR) (Saad, 2003), BFGS (Nocedal & Wright, 2006), Broyden’s method (Broyden, 1965),
and the full-memory AM (AM). We used the basic ST-AM to solve Problem I and II, and MST-AM
(cp = cq = 1) to solve Problem III and IV."
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08188976377952756,"0
10
20
30
40
50
iteration 10
14 10
11 10
8 10
5 10
2"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08267716535433071,"GD: ||rk||2/||r0||2
CR: ||rk||2/||r0||2
AM: ||rk||2/||r0||2
ST-AM: ||rk||2/||r0||2"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08346456692913386,(a) Problem I
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08425196850393701,"0
10
20
30
40
50
iteration 10
13 10
10 10
7 10
4 10
1"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08503937007874016,||rk||2/||r0||2
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08582677165354331,"FP
CR
AM
ST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08661417322834646,(b) Problem II
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08740157480314961,"0
10
20
30
40
50
iteration 10
14 10
11 10
8 10
5 10
2"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08818897637795275,||rk||2/||r0||2
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08897637795275591,"GD
BFGS
AM
MST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.08976377952755905,(c) Problem III (M = 0.1)
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09055118110236221,"2
4
6
8
10
12
14
16
step 0.0 0.2 0.4 0.6 0.8"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09133858267716535,Forward: ||f(z)||2/||z||2
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09212598425196851,"Broyden
AM
MST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09291338582677165,(d) Forward process
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09370078740157481,"2
4
6
8
10
12
14
16
18
step 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09448818897637795,Backward: ||f(z)||2/||z||2
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09527559055118111,"Broyden
AM
MST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09606299212598425,(e) Backward process
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09685039370078741,"0
10
20
30
40
50
epoch 55 60 65 70 75 80 85"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09763779527559055,Test Accuracy %
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.0984251968503937,"Broyden
AM
MST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.09921259842519685,"25
30
35
40
45
50
83 84 85 86"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1,"84.52
84.90
85.31 0.6 0.8 1.0 1.2"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10078740157480315,Test Loss
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1015748031496063,(f) Test accuracy and loss
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10236220472440945,"Figure 1: (a) ∥rk∥2/∥r0∥2 of GD and CR, and ∥¯rk∥2/∥r0∥2 of AM and ST-AM for solving Prob-
lem I; (b) ∥rk∥2/∥r0∥2 for solving Problem II; (c) ∥rk∥2/∥r0∥2 for solving Problem III (M = 0.1);
(d)(e) relative residuals of the forward and backward root-ﬁnding processes in MDEQ, and shaded
areas correspond to the standard deviations; (f) test accuracy and loss in MDEQ/CIFAR-10."
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1031496062992126,"The numerical results shown in Figure 1 demonstrate the power of ST-AM as a variant of Krylov
subspace methods. It signiﬁcantly accelerates the slow convergence of the GD or FP method, and
can outperform AM. Figure 1(a) clearly veriﬁes the correctness of Theorem 1: within the machine
precision, the intermediate residual ¯rk of ST-AM coincides with the residual rk of CR. Note that
AM fails to coincide with CR and ST-AM due to the intrinsic numerical weakness to solve (3), as
also pointed out in (Walker & Ni, 2011). Figure 1(b) shows that ST-AM can outperform CR, though
both methods enjoy short-term recurrences and are equivalent for solving SPD linear systems. Fig-
ure 1(c) also shows MST-AM surpasses BFGS in solving cubic-regularized problems. The tests in
MDEQ/CIFAR-10 indicate that MST-AM is comparable to the full-memory methods in the forward
root-ﬁnding process and converges faster in the backward process. The accuracy is also comparable."
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10393700787401575,Published as a conference paper at ICLR 2022
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1047244094488189,"0
50
100
150
200
epoch 10
8 10
6 10
4 10
2 100"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10551181102362205,Train Loss
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1062992125984252,"SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10708661417322834,(a) Train loss (w/o preconditioning)
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1078740157480315,"0
50
100
150
200
epoch 10
10 10
7 10
4 10
1 102"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.10866141732283464,Squared norm of gradient
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1094488188976378,"SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.11023622047244094,(b) SNG (w/o preconditioning)
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1110236220472441,"0
50
100
150
200
epoch 10
6 10
4 10
2 100"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.11181102362204724,Train Loss
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1125984251968504,"Adagrad
RMSprop
Adagrad_SAM(2)
RMSprop_SAM(2)
Adagrad_RST-AM
RMSprop_RST-AM
RST-AM"
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.11338582677165354,(c) Train loss (w/ preconditioning)
EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM,0.1141732283464567,"Figure 2: Experiments on MNIST. (a)(b) Training loss and the squared norm of gradient (SNG) (w/o
preconditioning for SAM, RST-AM); (c) Training loss (w/ preconditioning for SAM, RST-AM)."
EXPERIMENTS ABOUT RST-AM,0.11496062992125984,"4.2
EXPERIMENTS ABOUT RST-AM"
EXPERIMENTS ABOUT RST-AM,0.115748031496063,"We applied RST-AM to train neural networks, with full-batch training on MNIST (LeCun et al.,
1998), and mini-batch training on CIFAR-10/CIFAR-100 and Penn Treebank (Marcus et al., 1993)."
EXPERIMENTS ABOUT RST-AM,0.11653543307086614,"Experiments on MNIST. We trained a convolutional neural network (CNN) on MNIST to see the
convergence behaviour of RST-AM in nonconvex optimization (cf. Theorem 3), for which we were
only concerned about the training loss. Figure 2(a)(b) show that the short-memory SAMs (m = 2, 5)
hardly show any improvement over the ﬁrst-order optimizers SGD and Adam, while RST-AM can
close the gap of the long-memory (m = 10) and the short-memory methods. We also considered
the effect of preconditioning on RST-AM (see Appendix A.3). The notation “A B” means B method
preconditioned by A method. Figure 2(c) indicates that preconditioning also works much better for
RST-AM than SAM(2), and RMSprop RST-AM can outperform the non-preconditioned RST-AM."
EXPERIMENTS ABOUT RST-AM,0.1173228346456693,"Table 1: Experiments on CIFAR10/CIFAR100. “-” means failing to complete the test in our device
due to memory limit. “*” indicates numbers published in (Wei et al., 2021)."
EXPERIMENTS ABOUT RST-AM,0.11811023622047244,(a) Final TOP1 test accuracy (mean ± standard deviation) (%) for training 160 epochs.
EXPERIMENTS ABOUT RST-AM,0.1188976377952756,"Method
Test accuracy on CIFAR10
Test accuracy on CIFAR100"
EXPERIMENTS ABOUT RST-AM,0.11968503937007874,"ResNet18
ResNet20
ResNet32
ResNet44
ResNet56
WRN16-4
ResNet18
ResNeXt
DenseNet"
EXPERIMENTS ABOUT RST-AM,0.1204724409448819,"SGDM∗
94.82±.15
92.03±.16
92.86±.15
93.10±.23
93.47±.28
94.90±.09
77.27±.09
78.41±.54
78.49±.12
Adam∗
93.03±.07
91.17±.13
92.03±.28
92.28±.62
92.39±.23
92.45±.11
72.41±.17
73.57±.17
70.80±.23
AdaBound
94.25±.31
90.77±.08
91.73±.06
92.00±.18
92.44±.04
93.50±.12
75.07±.14
75.74±.20
76.06±.13
AdaBelief∗
94.65±.13
91.15±.21
92.15±.17
92.79±.24
93.30±.07
94.46±.13
76.25±.06
78.27±.16
78.83±.15
Lookahead∗
94.92±.33
92.07±.04
92.86±.15
93.26±.24
93.36±.13
94.90±.15
77.63±.35
78.93±.12
79.37±.16
AdaHessian∗
94.36±.09
91.92±.32
92.18±.18
92.74±.11
92.40±.06
94.04±.12
76.59±.42
-
-
SAM(2)
95.07±.04
92.14±.33
93.04±.23
93.46±.09
93.66±.06
95.07±.16
77.51±.24
79.02±.21
80.00±.23
SAM(10)∗
95.17±.10
92.43±.19
93.22±.32
93.57±.14
93.77±.12
95.23±.07
78.13±.14
79.31±.27
80.09±.52
RST-AM
95.27±.04
92.39±.11
93.24±.36
93.52±.02
93.69±.18
95.21±.09
77.91±.22
79.53±.34
80.36±.25"
EXPERIMENTS ABOUT RST-AM,0.12125984251968504,"(b) The memory and computation cost compared with SGDM. The notations “m”,“t/e” and “t” are abbrevia-
tions of memory, per-epoch time and total running time, respectively."
EXPERIMENTS ABOUT RST-AM,0.1220472440944882,"Cost
CIFAR10/ResNet18
CIFAR10/WRN16-4
CIFAR100/ResNeXt50
CIFAR100/DenseNet121
(× SGDM)
m
t/e
t
m
t/e
t
m
t/e
t
m
t/e
t"
EXPERIMENTS ABOUT RST-AM,0.12283464566929134,"SGDM∗
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
SAM(10)∗
1.73
1.78
1.00
1.26
1.28
0.80
1.30
1.16
0.58
1.16
1.19
0.60
RST-AM
1.05
1.46
0.82
1.03
1.14
0.71
1.04
1.07
0.54
1.01
1.11
0.55"
EXPERIMENTS ABOUT RST-AM,0.1236220472440945,"Experiments on CIFAR. We trained ResNet18/20/32/44/56 (He et al., 2016), WideResNet16-4
(Zagoruyko & Komodakis, 2016) (abbr. WRN16-4) on CIFAR-10, and ResNet18, ResNeXt50 (Xie
et al., 2017), DenseNet121 (Huang et al., 2017) on CIFAR-100. The baseline optimizers were
SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang et al., 2020), Lookahead (Zhang
et al., 2019), AdaHessian (Yao et al., 2021) and SAM. Here, some results of the baselines in (Wei
et al., 2021) were used for reference since the experimental settings were the same. Table 1(a) shows
RST-AM improves SAM(2) and has comparable test accuracy to SAM(10). RST-AM also outper-
forms other baseline optimizers. Table 1(b) reports the memory and computation cost, where we"
EXPERIMENTS ABOUT RST-AM,0.12440944881889764,Published as a conference paper at ICLR 2022
EXPERIMENTS ABOUT RST-AM,0.1251968503937008,"used SGDM as the baseline and other optimizers were terminated when achieving a comparable or
better test accuracy than SGDM. It indicates that RST-AM introduces ≤5% extra memory over-
head compared with SGDM, and signiﬁcantly reduces the memory footprint of AM. Since RST-AM
needs fewer training epochs, the total running time is less than SGDM."
EXPERIMENTS ABOUT RST-AM,0.12598425196850394,"0
100
200
300
400
500
epoch 80 85 90 95 100 105 110 115 120"
EXPERIMENTS ABOUT RST-AM,0.12677165354330708,Validation PPL
EXPERIMENTS ABOUT RST-AM,0.12755905511811025,"SGDM
Adam
AdaBelief
SAM(2)
SAM(10)
RST-AM"
EXPERIMENTS ABOUT RST-AM,0.1283464566929134,(a) 1-Layer LSTM
EXPERIMENTS ABOUT RST-AM,0.12913385826771653,"0
100
200
300
400
500
epoch 65 70 75 80 85 90 95 100"
EXPERIMENTS ABOUT RST-AM,0.12992125984251968,Validation PPL
EXPERIMENTS ABOUT RST-AM,0.13070866141732285,"SGDM
Adam
AdaBelief
SAM(2)
SAM(10)
RST-AM"
EXPERIMENTS ABOUT RST-AM,0.131496062992126,(b) 2-Layer LSTM
EXPERIMENTS ABOUT RST-AM,0.13228346456692913,"0
100
200
300
400
500
epoch 60 65 70 75 80 85 90 95 100"
EXPERIMENTS ABOUT RST-AM,0.13307086614173227,Validation PPL
EXPERIMENTS ABOUT RST-AM,0.13385826771653545,"SGDM
Adam
AdaBelief
SAM(2)
SAM(10)
RST-AM"
EXPERIMENTS ABOUT RST-AM,0.1346456692913386,(c) 3-Layer LSTM
EXPERIMENTS ABOUT RST-AM,0.13543307086614173,"Figure 3: Validation perplexity of training 1,2,3-layer LSTM on Penn Treebank."
EXPERIMENTS ABOUT RST-AM,0.13622047244094487,"Table 2: Test perplexity of training 1,2,3-layer
LSTM on Penn Treebank. Lower is better."
EXPERIMENTS ABOUT RST-AM,0.13700787401574804,"Method
1-Layer
2-Layer
3-Layer"
EXPERIMENTS ABOUT RST-AM,0.1377952755905512,"SGDM
83.48±.03
65.89±.18
61.88±.23
Adam
80.33±.15
64.32±.06
59.72±.13
AdaBelief
81.29±.35
64.68±.10
60.46±.07
SAM(2)
80.79±.19
65.52±.29
61.13±.12
SAM(10)
78.78±.14
62.46±.11
58.93±.09
RST-AM
78.41±.18
62.46±.08
58.31±.23"
EXPERIMENTS ABOUT RST-AM,0.13858267716535433,"Experiments on Penn Treebank. We trained
LSTMs with 1-3 layer(s) on Penn Treebank and
report the validation perplexity in Figure 3 and
test perplexity in Table 2 (lower is better). The
results suggest that RST-AM is comparable to
or even better than SAM(10). The improvement
of RST-AM over other optimizers is also sig-
niﬁcant. We report the computation and mem-
ory cost in Appendix D.2.4. RST-AM can still
surpass Adam while using much fewer epochs,
thus reducing the total running time."
EXPERIMENTS ABOUT RST-AM,0.13937007874015747,Table 3: Test accuracy (%) for adversarial training.
EXPERIMENTS ABOUT RST-AM,0.14015748031496064,"Optimizer
CIFAR10/ResNet18
CIFAR100/DenseNet121
Clean
FGSM
PGD-20
C&W∞
Clean
FGSM
PGD-20
C&W∞
SGD
82.16
63.23
51.91
50.22
59.45
39.76
30.92
29.00
RST-AM
82.53
63.78
52.43
50.52
60.48
40.41
31.20
29.52"
EXPERIMENTS ABOUT RST-AM,0.14094488188976378,Table 4: FID score for SN-GAN.
EXPERIMENTS ABOUT RST-AM,0.14173228346456693,"Method
Adam
AdaBelief
RST-AM"
EXPERIMENTS ABOUT RST-AM,0.14251968503937007,"Best FID
13.07±.18
12.80±.09
12.05±.15
Final FID
13.34±.14
13.59±.21
12.50±.29"
EXPERIMENTS ABOUT RST-AM,0.14330708661417324,"Adversarial training. We applied RST-AM to adversarial training (Madry et al., 2018) as the outer-
optimizer and compared it with SGD by the clean test accuracy and robust test accuracy. The results
on CIFAR10/ResNet18 and CIFAR100/DenseNet121 are reported in Table 3. It can be seen that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy. More results
can be found in Appendix D.2.5."
EXPERIMENTS ABOUT RST-AM,0.14409448818897638,"Generative adversarial network (GAN). We tested RST-AM by training a GAN equipped with
spectral normalization (SN-GAN) (Miyato et al., 2018), where the generator and discriminator net-
works were ResNets and the dataset was CIFAR-10. Table 4 shows that RST-AM can achieve lower
FID score (better accuracy) than Adam and AdaBelief."
CONCLUSION,0.14488188976377953,"5
CONCLUSION"
CONCLUSION,0.14566929133858267,"In this paper, to address the memory issue of Anderson mixing (AM), we develop a novel class
of short-term recurrence AM methods (ST-AM) and test it in various applications, including solv-
ing linear and nonlinear problems, deterministic and stochastic optimization. We give a complete
theoretical analysis of the proposed methods. We prove that the basic ST-AM is equivalent to the
full-memory AM in strongly convex quadratic optimization. With some minor changes, it has local
linear convergence for solving general ﬁxed-point problems under some common assumptions. We
also introduce the regularized form of ST-AM and analyze its convergence properties. The numerical
results show that the ST-AM methods are comparable to or even better than the long-memory AM
while consuming less memory. The regularized ST-AM also outperforms many existing optimizers
in training neural networks in various tasks."
CONCLUSION,0.14645669291338584,Published as a conference paper at ICLR 2022
CONCLUSION,0.14724409448818898,ACKNOWLEDGMENTS
CONCLUSION,0.14803149606299212,"This work was supported by the National Key R&D Program of China (No. 2021YFA1001300),
National Natural Science Foundation of China (No.61925601), Tsinghua University Initiative Scien-
tiﬁc Research Program, National Natural Science Foundation of China (No.11901338), and Huawei
Noah’s Ark Lab. We thank all anonymous reviewers for their valuable comments and suggestions
on this work."
REFERENCES,0.14881889763779527,REFERENCES
REFERENCES,0.14960629921259844,"Hengbin An, Xiaowei Jia, and Homer F Walker. Anderson acceleration and application to the three-
temperature energy equations. Journal of Computational Physics, 347:1–19, 2017."
REFERENCES,0.15039370078740158,"Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM
(JACM), 12(4):547–560, 1965."
REFERENCES,0.15118110236220472,"Donald G Anderson. Comments on “Anderson acceleration, mixing and extrapolation”. Numerical
Algorithms, 80(1):135–234, 2019."
REFERENCES,0.15196850393700786,"Marcin Andrychowicz, Misha Denil, Sergio G´omez Colmenarejo, Matthew W Hoffman, David
Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient
descent by gradient descent. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 3988–3996, 2016."
REFERENCES,0.15275590551181104,"Akash Arora, David C Morse, Frank S Bates, and Kevin D Dorfman. Accelerating self-consistent
ﬁeld theory of block polymers in a variable unit cell. The Journal of Chemical Physics, 146(24):
244902, 2017."
REFERENCES,0.15354330708661418,"Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural
Information Processing Systems, 32:690–701, 2019."
REFERENCES,0.15433070866141732,"Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In Advances
in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.15511811023622046,"Albert S Berahas, Frank E Curtis, and Baoyu Zhou. Limited-memory BFGS with displacement
aggregation. Mathematical Programming, pp. 1–37, 2021."
REFERENCES,0.15590551181102363,"Lon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018."
REFERENCES,0.15669291338582678,"Claude Brezinski and Michela Redivo-Zaglia. Shanks function transformations in a vector space.
Applied Numerical Mathematics, 116:57–63, 2017."
REFERENCES,0.15748031496062992,"Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. Shanks sequence transformations and
Anderson acceleration. SIAM Review, 60(3):646–669, 2018."
REFERENCES,0.15826771653543306,"Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics
of Computation, 19(92):577–593, 1965."
REFERENCES,0.15905511811023623,"Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016."
REFERENCES,0.15984251968503937,"Stan Cabay and LW Jackson. A polynomial extrapolation method for ﬁnding limits and antilimits
of vector sequences. SIAM Journal on Numerical Analysis, 13(5):734–752, 1976."
REFERENCES,0.16062992125984252,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39–57. IEEE, 2017."
REFERENCES,0.16141732283464566,"Yair Carmon and John C Duchi. First-order methods for nonconvex quadratic minimization. SIAM
Review, 62(2):395–436, 2020."
REFERENCES,0.16220472440944883,"Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. Report
on the 11th IWSLT evaluation campaign, IWSLT 2014.
In Proceedings of the International
Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57, 2014."
REFERENCES,0.16299212598425197,Published as a conference paper at ICLR 2022
REFERENCES,0.16377952755905512,"Hans De Sterck and Yunhui He. On the asymptotic linear convergence speed of Anderson accelera-
tion, Nesterov acceleration, and nonlinear GMRES. SIAM Journal on Scientiﬁc Computing, (0):
S21–S46, 2021."
REFERENCES,0.16456692913385826,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255. IEEE, 2009."
REFERENCES,0.16535433070866143,"Yunbin Deng. Deep learning on mobile devices: A review. In Mobile Multimedia/Image Processing,
Security, and Applications 2019, volume 10993, pp. 109930A. International Society for Optics
and Photonics, 2019."
REFERENCES,0.16614173228346457,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011."
REFERENCES,0.16692913385826771,"Iain S Duff, Albert Maurice Erisman, and John Ker Reid. Direct methods for sparse matrices.
Oxford University Press, 2017."
REFERENCES,0.16771653543307086,"Rick Durrett. Probability: Theory and examples, volume 49. Cambridge University Press, 2019."
REFERENCES,0.16850393700787403,"R P Eddy. Extrapolating to the limit of a vector sequence. In Peter C.C. Wang, Arthur L. Schoenstadt,
Bert I. Russak, and Craig Comstock (eds.), Information Linkage Between Applied Mathematics
and Industry, pp. 387–396. Academic Press, 1979."
REFERENCES,0.16929133858267717,"Claire Evans, Sara Pollock, Leo G Rebholz, and Mengying Xiao. A proof that Anderson acceler-
ation improves the convergence rate in linearly converging ﬁxed-point methods (but not in those
converging quadratically). SIAM Journal on Numerical Analysis, 58(1):788–810, 2020."
REFERENCES,0.1700787401574803,"Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
Numerical Linear Algebra with Applications, 16(3):197–221, 2009."
REFERENCES,0.17086614173228346,"Anqi Fu, Junzi Zhang, and Stephen Boyd. Anderson accelerated Douglas–Rachford splitting. SIAM
Journal on Scientiﬁc Computing, 42(6):A3560–A3583, 2020."
REFERENCES,0.17165354330708663,"Alejandro J Garza and Gustavo E Scuseria. Comparison of self-consistent ﬁeld convergence accel-
eration techniques. The Journal of Chemical Physics, 137(5):054110, 2012."
REFERENCES,0.17244094488188977,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.1732283464566929,"Gene H Golub and Charles F Van Loan. Matrix computations, 4th. Johns Hopkins, 2013."
REFERENCES,0.17401574803149605,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.17480314960629922,"William W Hager and Hongchao Zhang. A survey of nonlinear conjugate gradient methods. Paciﬁc
Journal of Optimization, 2(1):35–58, 2006."
REFERENCES,0.17559055118110237,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.1763779527559055,"Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems.
Journal of Research of the National Bureau of Standards, 49:409–435, 1952."
REFERENCES,0.17716535433070865,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems, 30, 2017."
REFERENCES,0.17795275590551182,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700–4708, 2017."
REFERENCES,0.17874015748031497,"Carl T Kelley. Numerical methods for nonlinear equations. Acta Numerica, 27:207–287, 2018."
REFERENCES,0.1795275590551181,Published as a conference paper at ICLR 2022
REFERENCES,0.18031496062992125,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.18110236220472442,"Tamara G Kolda, Dianne P O’leary, and Larry Nazareth. BFGS with update skipping and varying
memory. SIAM Journal on Optimization, 8(4):1060–1083, 1998."
REFERENCES,0.18188976377952756,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.1826771653543307,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.18346456692913385,"Lin Lin, Jianfeng Lu, and Lexing Ying. Numerical methods for Kohn–Sham density functional
theory. Acta Numerica, 28:405–539, 2019."
REFERENCES,0.18425196850393702,"Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503–528, 1989."
REFERENCES,0.18503937007874016,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2019."
REFERENCES,0.1858267716535433,"Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2018."
REFERENCES,0.18661417322834645,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.18740157480314962,"Vien Mai and Mikael Johansson. Anderson acceleration of proximal gradient methods. In Interna-
tional Conference on Machine Learning, pp. 6620–6629. PMLR, 2020."
REFERENCES,0.18818897637795276,"Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. 1993."
REFERENCES,0.1889763779527559,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.18976377952755905,"Aryan Mokhtari, Mark Eisen, and Alejandro Ribeiro. IQN: An incremental quasi-Newton method
with local superlinear convergence rate. SIAM Journal on Optimization, 28(2):1670–1698, 2018."
REFERENCES,0.19055118110236222,"Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of ﬁrst order methods for
non-strongly convex optimization. Mathematical Programming, 175(1):69–107, 2019."
REFERENCES,0.19133858267716536,"Arkadij Semenoviˇc Nemirovski and David Borisovich Yudin.
Problem complexity and method
efﬁciency in optimization. 1983."
REFERENCES,0.1921259842519685,"Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006."
REFERENCES,0.19291338582677164,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pp. 311–318, 2002."
REFERENCES,0.19370078740157481,"Florian A Potra and Hans Engler. A characterization of the behavior of the Anderson acceleration
on linear problems. Linear Algebra and Its Applications, 438(3):1002–1011, 2013."
REFERENCES,0.19448818897637796,"Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12
(1):145–151, 1999."
REFERENCES,0.1952755905511811,"Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In
International Conference on Machine Learning, pp. 8093–8104. PMLR, 2020."
REFERENCES,0.19606299212598424,Published as a conference paper at ICLR 2022
REFERENCES,0.1968503937007874,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400–407, 1951."
REFERENCES,0.19763779527559056,"Youcef Saad and Martin H Schultz. GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing, 7(3):856–
869, 1986."
REFERENCES,0.1984251968503937,"Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003."
REFERENCES,0.19921259842519684,"Damien Scieur, Alexandre dAspremont, and Francis Bach.
Regularized nonlinear acceleration.
Mathematical Programming, 179(1):47–83, 2020."
REFERENCES,0.2,"Daniel Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal
of Mathematics and Physics, 34(1-4):1–42, 1955."
REFERENCES,0.20078740157480315,"Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139–1147, 2013."
REFERENCES,0.2015748031496063,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–
31, 2012."
REFERENCES,0.20236220472440944,"Alex Toth and CT Kelley.
Convergence analysis for Anderson acceleration.
SIAM Journal on
Numerical Analysis, 53(2):805–819, 2015."
REFERENCES,0.2031496062992126,"Henk A Van der Vorst and C Vuik. The superlinear convergence behaviour of GMRES. Journal of
Computational and Applied Mathematics, 48(3):327–341, 1993."
REFERENCES,0.20393700787401575,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998–6008, 2017."
REFERENCES,0.2047244094488189,"Homer F Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. SIAM Journal on
Numerical Analysis, 49(4):1715–1735, 2011."
REFERENCES,0.20551181102362204,"Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017."
REFERENCES,0.2062992125984252,"Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic Anderson mixing for nonconvex stochastic
optimization. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.20708661417322835,"Peter Wynn. On a device for computing the em(Sn) transformation. Mathematical Tables and Other
Aids to Computation, pp. 91–96, 1956."
REFERENCES,0.2078740157480315,"Peter Wynn. Acceleration techniques for iterated vector and matrix problems. Mathematics of
Computation, 16(79):301–322, 1962."
REFERENCES,0.20866141732283464,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1492–1500, 2017."
REFERENCES,0.2094488188976378,"Yunan Yang. Anderson acceleration for seismic inversion. Geophysics, 86(1):R99–R108, 2021."
REFERENCES,0.21023622047244095,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
AdaHessian: An adaptive second order optimizer for machine learning. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 10665–10673, 2021."
REFERENCES,0.2110236220472441,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
In British Machine Vision
Conference 2016. British Machine Vision Association, 2016."
REFERENCES,0.21181102362204723,"Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. In Advances in Neural Information Processing Systems, pp. 9597–9608,
2019."
REFERENCES,0.2125984251968504,"Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.21338582677165355,Published as a conference paper at ICLR 2022
REFERENCES,0.2141732283464567,"A
ADDITIONAL PRELIMINARIES"
REFERENCES,0.21496062992125983,"We provide some additional preliminaries in this section for readers that are not familiar with An-
derson mixing, ﬁxed-point iterations and some techniques mentioned in the main paper."
REFERENCES,0.215748031496063,"A.1
FIXED-POINT ITERATION"
REFERENCES,0.21653543307086615,"The ﬁxed-point problem and the optimization problem are the main application scenarios of our
methods. It is worth pointing out that there are some minor differences between these two prob-
lems that make algorithm designs different. The key difference is that for a ﬁxed-point problem,
the Jacobian (if exists) is generally not symmetric while for optimization, the Hessian is naturally
symmetric. In principle, a ﬁxed-point solver can also be applicable for an optimization problem
since the ﬁrst-order necessary condition of minx∈Rd f(x), where f : Rd →R, is ∇f(x) = 0."
REFERENCES,0.2173228346456693,"Consider a contraction mapping g : Rd 7→Rd, i.e. for some κ < 1, ∥g(x) −g(y)∥2 ≤κ∥x −
y∥2, ∀x, y ∈Rd. According to the contraction mapping theorem, a unique ﬁxed point x∗exists
for g and the iterates generated by the iteration xk+1 = g(xk) converge to x∗, starting from any
x0 ∈Rd. In practice, the damped ﬁxed-point iteration is also commonly used: given the βk ∈(0, 2),
the update is xk+1 = (1 −βk)xk + βkg(xk) = xk + βkrk, where rk := g(xk) −xk is called the
residual."
REFERENCES,0.21811023622047243,"The ﬁxed-point iteration, also known as Picard iteration in some areas, can converge very slowly in
practice. Anderson mixing is a method to improve the convergence."
REFERENCES,0.2188976377952756,"A.2
ANOTHER FORM OF ANDERSON MIXING"
REFERENCES,0.21968503937007874,"The derivation of Anderson mixing (AM) in Section 3.1 explicitly interprets AM as a two-step
procedure. In the literature, there is another equivalent form of AM."
REFERENCES,0.2204724409448819,"Let the projection coefﬁcients Γk = (γ(1)
k , . . . , γ(m)
k
)T ∈Rm. Deﬁne the auxiliary coefﬁcients
{θ(j)
k }m
j=0 as θ(0)
k
= γ(1)
k , θ(j)
k
= ∆γ(j)
k (j = 1, . . . , m −1), θ(m)
k
= 1 −γ(m)
k
, then Pm
j=0 θ(j)
k
="
REFERENCES,0.22125984251968503,"1 and ¯rk = Pm
j=0 θ(j)
k rk−m+j. Hence the least squares problem (3) can be reformulated as a"
REFERENCES,0.2220472440944882,"constrained problem min{θ(j)
k }m
j=0 ∥Pm
j=0 θ(j)
k rk−m+j∥2 s.t. Pm
j=0 θ(j)
k
= 1, which indicates that"
REFERENCES,0.22283464566929134,"as a linear combination of the historical residuals {rj}m
j=k−m, ¯rk is minimal in terms of the L2-norm."
REFERENCES,0.22362204724409449,"Also, the projection step and the mixing step (2) can be reformulated as ¯xk = Pm
j=0 θ(j)
k xk−m+j
and xk+1 = (1−βk) Pm
j=0 θ(j)
k xk−m+j +βk
Pm
j=0 θ(j)
k g(xk−m+j), respectively. Such formulation
is also adopted in the literature (Toth & Kelley, 2015; Mai & Johansson, 2020; Scieur et al., 2020)."
REFERENCES,0.22440944881889763,"Let Hk be the solution to the constrained optimization problem (Fang & Saad, 2009)"
REFERENCES,0.2251968503937008,"min
Hk ∥Hk −βkI∥F subject to HkRk = −Xk,
(19)"
REFERENCES,0.22598425196850394,"then the update (4) is xk+1 = xk+Hkrk. It suggests that AM is a multisecant quasi-Newton method
(Fang & Saad, 2009)."
REFERENCES,0.22677165354330708,"A.3
PRECONDITIONED ANDERSON MIXING"
REFERENCES,0.22755905511811023,"Like preconditioning for Krylov subspace methods (Saad, 2003), preconditioning can also be incor-
porated into Anderson mixing to mitigate the ill-conditioning of the original problem (Wei et al.,
2021). The idea is to replace the mixing step in (2) via a preconditioned mixing."
REFERENCES,0.2283464566929134,"Suppose that there is a basic solver preconditioner(xk, sk), which works as a black-box procedure
that updates xk given the residual sk, i.e. xk+1 = preconditioner(xk, sk), then the preconditioned
mixing of RST-AM is
xk+1 = preconditioner(¯xk, ¯rk).
which substitutes for the Line 13 in Algorithm 1. The simple mixing (Line 13 in Algorithm 1
can be seen as a special case by deﬁning preconditioner(xk, sk) = xk + βksk, i.e. precondi-
tioned by a damped ﬁxed-point iteration. Moreover, if we write the preconditioning operation as"
REFERENCES,0.22913385826771654,Published as a conference paper at ICLR 2022
REFERENCES,0.22992125984251968,"preconditioner(xk, sk) := xk + Gksk, where Gk is the matrix to approximate the inverse Jaco-
bian, then the preconditioned AM is"
REFERENCES,0.23070866141732282,"xk+1 = xk + Gkrk −(Xk + GkRk)(RT
k Rk)†RT
k rk
(20)"
REFERENCES,0.231496062992126,"(cf. the deﬁnitions of Xk, Rk in Section 3.1). The matrix Hk = Gk −(Xk + GkRk)(RT
k Rk)†RT
k
forms a low-rank updated approximation to the inverse Jacobian: it solves"
REFERENCES,0.23228346456692914,"min
Hk ∥Hk −Gk∥F subject to HkRk = −Xk,"
REFERENCES,0.23307086614173228,which is a direct extension of (19).
REFERENCES,0.23385826771653542,"For the stochastic Anderson mixing, using damped projection can be helpful, i.e. ¯xk = (1−αk)xk+
αk(xk −XkΓk) = xk −αkXkΓk and ¯rk = rk −αkRkΓk correspondingly. Then the preconditioned
AM with damped projection is"
REFERENCES,0.2346456692913386,"xk+1 = xk + Gkrk −αk(Xk + GkRk)Γk.
(21)"
REFERENCES,0.23543307086614174,"B
ADDITIONAL DISCUSSION"
REFERENCES,0.23622047244094488,"B.1
THE MEMORY AND COMPUTATIONAL EFFICIENCY"
REFERENCES,0.23700787401574802,"Since the ST-AM methods only need to store two previous iterations, the memory and computational
cost can be reduced to be close to ﬁrst-order methods."
REFERENCES,0.2377952755905512,"Memory cost. Assume that the iteration number is k and the model parameter size is d. The full-
memory AM stores all previous iterations, thus the additional memory is 2kd. To reduce the memory
overhead, the limited-memory AM(m) only maintains the most recent m iterations while discarding
the older historical information (cf. (1)). Hence the additional memory of AM(m) is 2md. Choosing
a suitable m can be problem-dependent (Walker & Ni, 2011), and it is often suggested that m ≥5
(Mai & Johansson, 2020; Fu et al., 2020) to avoid too much historical information being discarded.
There is no equivalence between AM(m) and the full-memory AM. On the other side, our ST-AM
methods have the same memory footprint as that of AM(2), i.e. only introducing 4d additional
memory. For a large model that contains millions or even billions of parameters, such reduction in
memory requirement can be signiﬁcant."
REFERENCES,0.23858267716535433,"Computational cost. For ST-AM, besides the gradient evaluations, the main additional computa-
tional cost comes from vector corrections, the projection and mixing step. These operations are
very cheap: matrix multiplications of R2×d × Rd×2, R2×d × Rd×1, and Rd×2 × R2×1; the pseudo-
inverse can be exactly solved as the size of the matrix is R2×2. Also, the QT
k−1Qk−1 and P T
k−1Pk−1
in Line 6 in Algorithm 1 can reuse the results in the previous iteration (cf. Line 11). So the total
additional computational cost of RST-AM is O(d)."
REFERENCES,0.23937007874015748,"B.2
APPLICABILITY"
REFERENCES,0.24015748031496062,"In the main paper, we develop a class of short-term recurrence Anderson mixing. Among them, the
basic ST-AM can serve as a new linear solver for linear systems; MST-AM can be used as a new
ﬁxed-point solver for nonlinear equations; RST-AM is a new method for optimization. These meth-
ods are based on Anderson mixing and have close relationship between each other. The theoretical
analysis and numerical results show the great potential of ST-AM methods for various applications.
Here, we give some comparisons between the ST-AM methods and some other classical methods."
REFERENCES,0.2409448818897638,"ST-AM versus current solvers for linear systems.
ST-AM is an iterative solver. It is equivalent
to the full-memory AM and GMRES in strongly convex quadratic optimization (or SPD linear sys-
tems), and can also have linear convergence rate for general nonsymmetric linear systems. Since it is
based on short-term recurrences, like the CG method, the memory and per-iteration cost of ST-AM
is economical. On the other side, the LU factorization based methods (Golub & Van Loan, 2013)
are direct solvers that can incur overwhelming memory and computation cost for large sparse linear
systems. Although sparse direct solvers (Duff et al., 2017) can alleviate overhead, they are often
more complicated to implement and difﬁcult for parallel computing. Moreover, iterative solvers can
beneﬁt from the preconditioning technique that improves convergence (Saad, 2003). Also, ST-AM"
REFERENCES,0.24173228346456693,Published as a conference paper at ICLR 2022
REFERENCES,0.24251968503937008,"has advantages over other iterative methods since it does not need to directly access the matrix and
only the residual is required. In the case that explicitly accessing the matrix is difﬁcult, this property
is appealing. Hence, unlike nonlinear CG that relies on line search, it is direct to extend ST-AM
to unconstrained optimization where the gradient is commonly available while the Hessian can be
too costly to obtain. Moreover, ST-AM is very ﬂexible and any iterative solver can be viewed as a
black-box iterative process to be accelerated by ST-AM."
REFERENCES,0.24330708661417322,"MST-AM versus other quasi-Newton methods for nonlinear equations.
MST-AM has the na-
ture of quasi-Newton methods since it is built upon AM that is recognized as a multisecant quasi-
Newton method (Fang & Saad, 2009). In scientiﬁc computing, AM has been successfully applied to
solve many difﬁcult nonlinear problems arising from many areas (An et al., 2017; Lin et al., 2019;
Fu et al., 2020; Yang, 2021). Since AM only manipulates residuals and does not use line-search
or trust-region technique, it is efﬁcient to apply AM to accelerate a slowly convergent black-box
iterative process. A comprehensive discussion about the applicability of AM for nonlinear problems
and the relation between AM and Broyden’s methods can be found in (Fang & Saad, 2009)."
REFERENCES,0.2440944881889764,"One of the biggest issues of AM and other quasi-Newton methods is the additional memory over-
head, because they need to store historical iterations to form the secant equations. To make a com-
promise, limited-memory quasi-Newton methods such as L-BFGS (Liu & Nocedal, 1989) are pro-
posed in which only limited number of historical iterations are stored. In each update, the oldest
iteration is discarded to make space for the up-to-date secant pair. As a result, the limited-memory
quasi-Newton methods can lose the local superlinear convergence properties achieved by the full-
memory schemes (Berahas et al., 2021). Also, as stated by Berahas et al. (2021), the choice of the
number of historical iterations (i.e. historical length) is problem-dependent, and one does not know
a priori the best choice when solving a particular problem."
REFERENCES,0.24488188976377953,"Unlike the limited-memory quasi-Newton methods whose performance can be sensitive to the his-
torical length, our MST-AM method only needs to store two corrected historical iterations. MST-
AM carefully incorporates historical information through orthogonalization. In the ideal case, i.e.
strongly convex quadratic optimization (or SPD linear systems), it is equivalent to the full-memory
AM which means there is no loss of historical information. Our experiments verify the theoretical
properties of MST-AM and indicate MST-AM can be a competitive method for nonlinear equations."
REFERENCES,0.24566929133858267,"RST-AM versus ﬁrst-order methods for stochastic optimization.
For stochastic optimization
such as training deep neural networks in machine learning, the ﬁrst-order methods have come to
dominate the ﬁeld due to the low memory and per-iteration cost. The RST-AM is an extension of
ST-AM and MST-AM to tackle this challenging problem. RST-AM has theoretical guarantees in
deterministic/stochastic optimization and also inherits several advantages of AM and ST-AM:"
REFERENCES,0.24645669291338582,"• Fast convergence in quadratic optimization. It is important to possess such property for an
optimizer since a smooth function can be approximated by a quadratic function in the local
region around the optima and many techniques such as trust region (Nocedal & Wright,
2006) rely on this local approximation. Adaptive learning rate methods such as AdaGrad
(Duchi et al., 2011), Adam (Kingma & Ba, 2014) use a diagonal approximation of the
Fisher information matrix that is an approximation of the Hessian. However, in the sim-
ple quadratic case, these methods can only roughly match the performance of the Jacobi
method for solving linear systems (Saad, 2003), which is better than gradient descent but
far inferior to the powerful Krylov subspace methods. The momentum method mimics
CG method by incorporating a historical iteration into the search direction. However, the
choice of momentum and stepsize can be an art (Sutskever et al., 2013). For RST-AM,
there is no need to determine the stepsize and the fast convergence rate of ST-AM can be
recovered by simply setting αk = 1 and δ(1)
k
= δ(2)
k
= 0. For more difﬁcult functions, the
damping and regularization in RST-AM can be enabled to improve stability."
REFERENCES,0.247244094488189,"• Theoretical guarantee in stochastic optimization. If only ﬁrst-order information can be ac-
cessed, the SGD (Ghadimi & Lan, 2013) achieves an optimal convergence rate O(1/ϵ2) to
obtain an ϵ-accurate solution (Nemirovski & Yudin, 1983). In such case, it seems to be a big
mismatch for current second-order methods, because there is no theoretical improvement
albeit with more memory and computation resource. Such mismatch may also account for
the popularity of ﬁrst-order methods. Since RST-AM has very limited additional memory"
REFERENCES,0.24803149606299213,Published as a conference paper at ICLR 2022
REFERENCES,0.24881889763779527,"overhead, and also achieves the O(1/ϵ2) complexity, it can be applied to many applications
that are dominated by ﬁrst-order methods."
REFERENCES,0.24960629921259841,"• Flexibility in use. The application of RST-AM can be very ﬂexible. In principle, RST-AM
can be applied to improve any slowly convergent black-box iterative process by viewing
the latter as a ﬁxed-point iteration. For example, consider accelerating a solver of a com-
mercial software, where we have no access to the underlying codes to provide our custom
implementation, or some case rewriting the codes is too cumbersome. Moreover, RST-
AM can efﬁciently incorporate the preconditioning technique. Hence any optimizer, even
an optimizer built upon neural networks (Andrychowicz et al., 2016), can be used as a
preconditioner for RST-AM. Preconditioning largely enhances the applicability of RST-
AM for various applications. For example, RST-AM can be preconditioned by Adam for
the language task and SGDM for image classiﬁcation. So any ﬁne-tuned ﬁrst-order opti-
mizer can be combined with RST-AM to achieve an overall improvement. As RST-AM is
light-weight, this additional cost is marginal and can be largely counteracted by the actual
improvement. So in some sense, the purpose of RST-AM is not to totally replace current
off-the-shelf optimizers but to achieve collaborative effectiveness: RST-AM is aware of the
second-order information while the ﬁrst-order method can mitigate the ill-conditioning of
the problem."
REFERENCES,0.2503937007874016,"Overall, the ST-AM methods have wide applicability and can be competent methods from both
theoretical and practical perspectives."
REFERENCES,0.2511811023622047,"C
PROOFS"
REFERENCES,0.25196850393700787,We give more details about ST-AM and the proofs of the theorems in the main paper.
REFERENCES,0.25275590551181104,"C.1
THE BASIC ST-AM FOR STRONGLY CONVEX QUADRATIC OPTIMIZATION"
REFERENCES,0.25354330708661416,Recall that the strongly convex quadratic optimization is formulated as
REFERENCES,0.2543307086614173,"min
x∈Rd f(x) := 1"
REFERENCES,0.2551181102362205,"2xTAx −bTx,
(22)"
REFERENCES,0.2559055118110236,"where A ∈Rd×d is SPD, b ∈Rd. Solving (22) is equivalent to solving the SPD linear system"
REFERENCES,0.2566929133858268,"Ax = b.
(23)"
REFERENCES,0.2574803149606299,The detail of the basic ST-AM is given in Algorithm 2.
REFERENCES,0.25826771653543307,"We ﬁrst state the relationship of AM with GMRES in the following proposition. Similar results can
also be found in (Walker & Ni, 2011; Wei et al., 2021)."
REFERENCES,0.25905511811023624,"Let xG
k, rG
k := b −AxG
k denote the k-th GMRES iterate and residual, respectively, and Kk(A, v) :=
span{v, Av, . . . , Ak−1v} denotes the k-th Krylov subspace generated by A and v. Deﬁne ej :=
(1, 1, . . . , 1)T ∈Rj for j ≥1. Let range(X) denote the linear space spanned by the columns
of X. The main results of the full-memory AM are stated in Proposition 1 and Proposition 2.
The Proposition 1 is the same as the Proposition 2 in (Wei et al., 2021), and we restate it here for
completeness. The Proposition 2 is new as far as we know."
REFERENCES,0.25984251968503935,"Proposition 1 (General linear system). For solving a general linear system Ax = b with the full-
memory AM (m = k), suppose that βk > 0 and the ﬁxed-point map is g(x) = (I −A)x + b. If
the initial point of AM is x0 = xG
0 and rank(Rk) = m, then the intermediate iterate ¯xk satisﬁes
¯xk = xG
k."
REFERENCES,0.2606299212598425,Proof. The deﬁnition of the ﬁxed-point map suggests that the residual rk = g(xk)−xk = b−Axk.
REFERENCES,0.2614173228346457,"Since Rk = −AXk and A is nonsingular, we have rank(Xk) = m. We ﬁrst show"
REFERENCES,0.2622047244094488,"range(Xk) = Kk(A, rG
0 )
(24)"
REFERENCES,0.262992125984252,"by induction. We abbreviate Kk(A, rG
0 ) as Kk in this proof."
REFERENCES,0.2637795275590551,Published as a conference paper at ICLR 2022
REFERENCES,0.26456692913385826,Algorithm 2 ST-AM for strongly convex quadratic optimization
REFERENCES,0.26535433070866143,"Input: x0 ∈Rd, βk > 0, 0 < max iter ≤d.
Output: x ∈Rd"
REFERENCES,0.26614173228346455,"1: P0, Q0 = 0 ∈Rd×2, p0, q0 = 0 ∈Rd"
REFERENCES,0.2669291338582677,"2: for k = 0, 1, . . . , max iter do
3:
rk = −∇f(xk)
4:
if k > 0 then
5:
p = xk −xk−1, q = rk −rk−1
(Compute ∆xk−1, ∆rk−1)
6:
˜q = q −Qk−1(QT
k−1q), ˜p = p −Pk−1(QT
k−1q)
(˜q ⊥Qk−1)
7:
pk = ˜p/∥˜q∥2, qk = ˜q/∥˜q∥2
(∥qk∥2 = 1)
8:
Pk = [pk−1, pk], Qk = [qk−1, qk]
(QT
k Qk = I2)
9:
end if
10:
Γk = QT
k rk
11:
¯xk = xk −PkΓk, ¯rk = rk −QkΓk
(Projection step: ¯rk ⊥Qk)
12:
xk+1 = ¯xk + βk¯rk
(Mixing step)
13:
if ∥¯rk∥2 = 0 then
14:
break
15:
end if
16: end for
17: return xk"
REFERENCES,0.2677165354330709,"First, ∆x0 = β0r0 = β0rG
0 since x1 = x0 + β0r0. If k = 1, then the proof is complete. Then,
suppose that k > 1 and, as an inductive hypothesis, that range(Xk−1) = Kk−1. With (4) we have"
REFERENCES,0.268503937007874,"∆xk−1 = xk −xk−1
= βk−1rk−1 −(Xk−1 + βk−1Rk−1)Γk−1
= βk−1(b −Axk−1) −(Xk−1 −βk−1AXk−1)Γk−1
= βk−1b −βk−1A(x0 + ∆x0 + · · · + ∆xk−2) −(Xk−1 −βk−1AXk−1)Γk−1
= βk−1r0 −βk−1AXk−1ek−1 −(Xk−1 −βk−1AXk−1)Γk−1.
(25)"
REFERENCES,0.2692913385826772,"Since r0 ∈Kk−1, and by the inductive hypothesis range(Xk−1) ⊆Kk−1 which also implies
range(AXk−1) ⊆Kk, we know ∆xk−1 ∈Kk, which implies range(Xk) ⊆Kk. Since we assume
rank(Xk) = m = k which implies dim(range(Xk)) = dim(Kk), we have range(Xk) = Kk, thus
completing the induction. As a result, we also have"
REFERENCES,0.2700787401574803,"range(Rk) = range(AXk) = AKk(A, rG
0 ).
(26)"
REFERENCES,0.27086614173228346,"Recalling that to determine Γk, we solve the least squares problem (3) and Rk = −AXk. We have"
REFERENCES,0.27165354330708663,"Γk = arg min
Γ∈Rm ∥rk + AXkΓ∥2.
(27)"
REFERENCES,0.27244094488188975,"Since rank(AXk) = rank(Xk) = m, (27) has a unique solution. Also, since rk = b −Axk =
b −A(x0 + Xkek) = r0 −AXkek, we have rk + AXkΓ = r0 −AXkek + AXkΓ = r0 −AXk˜Γ,
where ˜Γ = ek −Γ. So Γk solves (27) if and only if ˜Γk = ek −Γk solves"
REFERENCES,0.2732283464566929,"min
˜Γ∈Rm ∥r0 −AXk˜Γ∥2,
(28)"
REFERENCES,0.2740157480314961,"According to (24), (28) is equal to minz∈Km(A,rG
0) ∥r0 −Az∥2 which is the GMRES minimization
problem. Since the solution of (28) is also unique, we have"
REFERENCES,0.2748031496062992,"¯xk = xk −XkΓk = xk −Xk(ek −˜Γk) = x0 + Xk˜Γk = xG
k."
REFERENCES,0.2755905511811024,"In Proposition 1, the assumption that Rk has full column rank is critical to ensure no stagnation
occurs in AM for solving a general linear system. In fact, for SPD linear systems (23) or strongly
convex quadratic optimization (22), when AM breaks down, i.e. Rk is rank deﬁcient, AM obtains
the exact solution, as shown in the next proposition."
REFERENCES,0.2763779527559055,Published as a conference paper at ICLR 2022
REFERENCES,0.27716535433070866,"Proposition 2 (SPD). For applying the full-memory AM to minimize a strongly convex quadratic
problem (22), or equivalently, solve a SPD linear system (23), suppose that βk > 0 and the ﬁxed-
point map is g(x) = (I −A)x + b. If rank(Rk) = k holds for 1 ≤k < s while failing to hold for
k = s, where s ≥1, then the residual of AM satisﬁes rs = ¯rs−1 = 0."
REFERENCES,0.27795275590551183,"Proof. The deﬁnition of g suggests that the residual rk = g(xk)−xk = b−Axk. The relation Rk =
−AXk holds during the iterations and the nonsingularity of A implies rank(Xk) = rank(Rk)."
REFERENCES,0.27874015748031494,"For s = 1, since the ﬁrst step of AM is x1 = x0 + β0r0, the assumption rank(R1) = 0 implies that
rank(r0) = rank(X1) = 0, i.e. r1 = ¯r0 := 0."
REFERENCES,0.2795275590551181,"For s > 1, because ∆xs−1 = xs −xs−1 = −Xs−1Γs−1 + βs−1¯rs−1, the rank deﬁciency of
Xs implies ∆xs−1 ∈range(Xs−1), which further implies ¯rs−1 ∈range(Xs−1). So there exists
ζ ∈Rs−1, such that ¯rs−1 = Xs−1ζ. Note that according to (3), ¯rs−1 ⊥Rs−1 = −AXs−1, so we
have"
REFERENCES,0.2803149606299213,"0 = ¯rT
s−1AXs−1 = (Xs−1ζ)TAXs−1 = ζTXT
s−1AXs−1.
(29)"
REFERENCES,0.2811023622047244,"Because rank(Xs−1) = s −1 and A is SPD, we know XT
s−1AXs−1 is also SPD. So ζ = 0, which
implies ¯rs−1 = 0. Hence xs = ¯xs−1 and rs = ¯rs−1 = 0."
REFERENCES,0.28188976377952757,Now we give the proof of Theorem 1.
REFERENCES,0.2826771653543307,"Proof of Theorem 1. Besides relations (i)-(iii), we add an auxiliary relation here:
(iv) rk = r0 + ¯Qk¯Γk ∈Kk+1(A, r0), where ¯Γk ∈Rk.
We prove the relations (i)-(iv) by induction."
REFERENCES,0.28346456692913385,"For k = 1, since ¯r0 ̸= 0, according to Proposition 2, rank(∆x0) = rank(X1) = 1, rank(∆r0) =
rank(R1) = 1, so ˜q ̸= 0, which implies Line 7 in Algorithm 2 is well-deﬁned. The relation (i) holds.
Since ˜q = q = ∆x0, ˜p = p = ∆r0, and ∆r0 = −A∆x0, the equality ¯Q1 = −A ¯P1 also holds. Due
to the normalization in Line 7, ¯QT
1 ¯Q1 = 1. Since r1 = r0 −β0Ar0 and range( ¯Q1) = range(Ar0),
it is clear that r1 = r0−¯Q1¯Γ1 ∈K2(A, r0), namely relation (iv). Due to the projection step Line 11,
¯r1 ⊥range(Q1) = range( ¯Q1). Also, ¯r1 = r1 −Q1Γ1 = r0 −β0Ar0 −Q1Γ1 = r0 −¯Q1η1, where
the last equality is due to span{Ar0} = range(Q1) = range( ¯Q1). For rG
1 = r0 −Az1, where
z1 = arg minz∈K1(A,r0) ∥r0 −Az∥2, it holds rG
1 ⊥AK1(A, r0) = range( ¯Q1). As a result, both ¯r1
and rG
1 are the orthogonal projections of r0 onto the subspace range( ¯Q1)⊥, which implies ¯r1 = rG
1 .
So ¯x1 = xG
1 = x0 + z1 because their residuals are equal and A is nonsingular. Hence relation (iii)
holds."
REFERENCES,0.284251968503937,"Suppose that k > 1, and as an inductive hypothesis, the relations (i)-(iv) hold for j = 1, . . . , k −1.
Consider the k-th iteration. From Line 6 in Algorithm 2, ˜q ∈range(∆rk−1, Qk−1), and ˜p ∈
range(∆xk−1, Pk−1). We ﬁrst prove that ˜q ̸= 0 by contradiction."
REFERENCES,0.28503937007874014,"If ˜q = 0, then from Line 6 in Algorithm 2, ∆rk−1 ∈range(Qk−1) ⊆range( ¯Qk−1), which implies
∆xk−1 ∈range(Pk−1) ⊆range( ¯Pk−1) as ∆rk−1 = −A∆xk−1 and ¯Qk−1 = −A ¯Pk−1 and A is
nonsingular. From Line 11 and Line 12, we have"
REFERENCES,0.2858267716535433,"∆xk−1 = xk −xk−1 = −Pk−1Γk−1 + βk−1¯rk−1.
(30)"
REFERENCES,0.2866141732283465,"So ¯rk−1 ∈range(Pk−1) ⊆range( ¯Pk−1) since ∆xk−1 ∈range(Pk−1).
Hence there exists
ζ ∈Rk−1, such that ¯rk−1 = ¯Pk−1ζ. From the inductive hypothesis, we know ¯rk−1 ⊥¯Qk−1 =
−A ¯Pk−1, so we have"
REFERENCES,0.2874015748031496,"0 = ¯rT
k−1A ¯Pk−1 = ( ¯Pk−1ζ)TA ¯Pk−1 = ζT ¯P T
k−1A ¯Pk−1."
REFERENCES,0.28818897637795277,"Since ¯QT
k−1 ¯Qk−1 = Ik−1, we know rank( ¯Qk−1) = k −1, which implies rank( ¯Pk−1) = k −1 due
to ¯Qk−1 = −A ¯Pk−1. Hence ¯P T
k−1A ¯Pk−1 is also SPD. Then ζ = 0 which implies ¯rk−1 = 0. It is
impossible otherwise Algorithm 2 has terminated in the (k −1)-th iteration. So ˜q ̸= 0 and Line 7 is
well-deﬁned."
REFERENCES,0.2889763779527559,"Since ¯rk−1 = rk−1 −Qk−1Γk−1, and rk−1 ∈Kk(A, r0), range(Qk−1) ⊆range( ¯Qk−1) =
AKk−1(A, r0) as the inductive hypothesis, we have ¯rk−1 ∈Kk(A, r0), which together with (30)"
REFERENCES,0.28976377952755905,Published as a conference paper at ICLR 2022
REFERENCES,0.2905511811023622,"and range(Pk−1) ⊆range( ¯Pk−1) = Kk−1(A, r0) infers ∆xk−1 ∈Kk(A, r0). Hence ∆rk−1 =
−A∆xk−1 ∈AKk(A, r0). As a result, qk = ˜q/∥˜q∥2 ∈range(∆rk−1, Qk−1) ⊆AKk(A, r0).
So range( ¯Qk) = range( ¯Qk−1, qk) ⊆AKk(A, r0). Moreover, qk /∈range( ¯Qk−1), otherwise
˜q ∈range( ¯Qk−1) that implies ∆rk−1 = q ∈range( ¯Qk−1), which is impossible following the
former proof of ˜q ̸= 0. So we have range( ¯Qk) = AKk(A, r0)."
REFERENCES,0.29133858267716534,"Because ∆rk−1 = −A∆xk−1 and Qk−1 = −APk−1 due to ¯Qk−1 = −A ¯Pk−1, Line 6 in Algo-
rithm 2 infers ˜q = −A˜p, which implies qk = −Apk. So ¯Qk = −A ¯Pk. Since A is nonsingular and
range( ¯Qk) = AKk(A, r0), we have range( ¯Pk) = Kk(A, r0)."
REFERENCES,0.2921259842519685,"As range(Xk) = Kk(A, r0) and range(Rk) = AKk(A, r0) has been proved in Proposition 1, the
relation (i) holds for the k-th iteration."
REFERENCES,0.2929133858267717,"To prove ¯QT
k ¯Qk = Ik, it sufﬁces to show qk ⊥¯Qk−1, as the equalities ¯QT
k−1 ¯Qk−1 = Ik−1 and
∥qk∥2 = 1 has already held. It is equivalent to prove ˜q ⊥¯Qk−1. From the construction of ˜q in
Line 6 in Algorithm 2, we know QT
k−1˜q = 0, so ˜q ⊥span(qk−2, qk−1) (for k = 2, ˜q ⊥q0 = 0
clearly holds). To further prove ˜q ⊥range( ¯Qk−3)(k ≥4), note that"
REFERENCES,0.2937007874015748,"∆rk−1 = −A∆xk−1 = APk−1Γk−1 −βk−1A¯rk−1 = −Qk−1Γk−1 −βk−1A¯rk−1,"
REFERENCES,0.29448818897637796,"where the second equality is a direct substitution with (30). Therefore,"
REFERENCES,0.2952755905511811,"¯QT
k−3∆rk−1 = −¯QT
k−3Qk−1Γk−1 −βk−1 ¯QT
k−3A¯rk−1 = 0 −βk−1(A ¯Qk−3)T¯rk−1 = 0,
(31)"
REFERENCES,0.29606299212598425,"where the second equality is due to range(Qk−1) = span(qk−2, qk−1) ⊥range( ¯Qk−3) and A is
SPD, the third equality is due to ¯rk−1 ⊥range( ¯Qk−1) = AKk−1(A, r0) and range(A ¯Qk−3) =
A2Kk−3(A, r0) ⊆AKk−1(A, r0). As a result, noting that q = ∆rk−1, we obtain"
REFERENCES,0.2968503937007874,"¯QT
k−3˜q = ¯QT
k−3q −¯QT
k−3Qk−1(QT
k−1q) = 0,"
REFERENCES,0.29763779527559053,"which is due to (31) and range(Qk−1) = span{qk−2, qk−1} ⊥range( ¯Qk−3). Therefore, we show
that ¯QT
k ¯Q = Ik, which along with ¯Qk = −A ¯Pk proves relation (ii) in the k-th iteration."
REFERENCES,0.2984251968503937,"Next, we prove the relation (iv). We have"
REFERENCES,0.2992125984251969,"rk = ¯rk−1 −βk−1A¯rk−1
= rk−1 −Qk−1Γk−1 −βk−1A(rk−1 −Qk−1Γk−1)
= rk−1 −Qk−1Γk−1 −βk−1Ark−1 + βk−1AQk−1Γk−1
= r0 + ¯Qk−1¯Γk−1 −Qk−1Γk−1 −βk−1(Ar0 + A ¯Qk−1¯Γk−1) + βk−1AQk−1Γk−1,"
REFERENCES,0.3,"where the last equality is due to rk−1 = r0 + ¯Qk−1¯Γk−1 by the inductive hypothesis. Since
range( ¯Qk−1) = AKk−1(A, r0) ⊆AKk(A, r0), range(Qk−1) ⊆range( ¯Qk), span{Ar0} ⊆
AKk(A, r0), range(AQk−1)
⊆
range(A ¯Qk−1)
⊆
A2Kk−1(A, r0)
⊆
AKk(A, r0), and
range( ¯Qk) = AKk(A, r0), it is clear that rk = r0 + ¯Qk¯Γk ∈Kk+1(A, r0) for some ¯Γk ∈Rk.
The relation (iv) is proved."
REFERENCES,0.30078740157480316,"Finally, we prove the relation (iii). For proving ¯rk ⊥range( ¯Qk), note that ¯rk ⊥span{qk−1, qk} =
range(Qk) already holds due to the projection step (Line 10 and Line 11 in Algorithm 2). It sufﬁces
to prove ¯rk ⊥range( ¯Qk−2). In fact, since we have ¯rk = rk −QkΓk, we can prove that rk ⊥
range( ¯Qk−2) and QkΓk ⊥range( ¯Qk−2):"
REFERENCES,0.3015748031496063,"Since range(Qk) = span{qk−1, qk} ⊥range( ¯Qk−2) as induced from ¯QT
k ¯Qk = Ik, it is clear
that QkΓk ⊥range( ¯Qk−2). For rk, according to Line 12 in Algorithm 2, we have rk = ¯rk−1 −
βk−1A¯rk−1. We have ¯rk−1 ⊥range( ¯Qk−1) ⊇range( ¯Qk−2) by the inductive hypothesis. Also,
¯QT
k−2A¯rk−1 = (A ¯Qk−2)T¯rk−1 = 0 due to range(A ¯Qk−2) = A2Kk−2(A, r0) ⊆AKk−1(A, r0) =
range( ¯Qk−1)."
REFERENCES,0.30236220472440944,"Therefore, we obtain ¯rk ⊥range( ¯Qk−2), which along with ¯rk ⊥span{qk−1, qk} implies ¯rk ⊥
range( ¯Qk)."
REFERENCES,0.3031496062992126,"To prove ¯xk = xG
k := x0 + zk, where zk = arg minz∈Kk(A,r0) ∥r0 −Az∥2, ﬁrst we have rk =
r0 + ¯Qk¯Γk, where ¯Γk ∈Rk. Hence ¯rk = rk −QkΓk = r0 + ¯Qk¯Γk −QkΓk = r0 −¯Qkηk, where
ηk ∈Rk. Since ¯rk ⊥¯Qk, ¯rk is the orthogonal projection of r0 onto the subspace range( ¯Qk)⊥.
On the other side, for GMRES, rG
k = r0 −Azk ⊥AKk(A, r0) = range( ¯Qk), so rG
k is also the"
REFERENCES,0.30393700787401573,Published as a conference paper at ICLR 2022
REFERENCES,0.3047244094488189,"orthogonal projection of r0 onto the subspace range( ¯Qk)⊥. So ¯rk = rG
k , which further indicates
¯xk = xG
k. Hence, the relation (iii) holds."
REFERENCES,0.30551181102362207,"With relations (i)-(iv) being proved in the k-th iteration, we complete the induction."
REFERENCES,0.3062992125984252,"C.2
MODIFIED ST-AM FOR GENERAL FIXED-POINT ITERATIONS"
REFERENCES,0.30708661417322836,"Algorithm 2 is suitable for analysis and implementation in the linear case. For general nonlinear
ﬁxed-point iterations, we adopt an alternative form as described in Algorithm 3 which discards the
normalization of ˜q in each iteration (Line 7 in Algorithm 2). In Line 7 in Algorithm 3, the orthogonal
projection of ∆rk−1 is checked to ensure ∆rk−1 is “less linearly dependent” on range(Qk), which
ensures ∥qk∥2 is bounded away from zero; the check of ∥Pk−1ζk∥2 ensures that ∥pk −∆xk−1∥2 ≤
cp∥∆xk−1∥2, which is also important since a large deviation from ∆xk−1 can make ∥pk∥2 > ρ
(ρ is the radius introduced in Theorem 2). When this condition cannot be satisﬁed, the algorithm
simply reuses the old Pk−1, Qk−1. The main procedure of MST-AM restarts every m iterations, i.e.
Pk, Qk = 0. Such restart mechanism is to restrict the higher-order terms in the residual expansion,
as shown in (9). Also, restart can ﬂush out the outdated historical information that may weaken the
quality of Pk and Qk that are used to pursue a local ﬁrst-order approximation of g in MST-AM."
REFERENCES,0.30787401574803147,Algorithm 3 MST-AM for nonlinear ﬁxed-point problems
REFERENCES,0.30866141732283464,"Input: x0 ∈Rd, βk ∈(0, 1], cp > 0, cq ∈(0, 1), m > 0.
Output: x ∈Rd"
REFERENCES,0.3094488188976378,"1: P0, Q0 = 0 ∈Rd×2, p0, q0 = 0 ∈Rd"
REFERENCES,0.3102362204724409,"2: for k = 0, 1, . . . , until convergence do
3:
rk = g(xk) −xk
4:
if k mod m ̸= 0 then
5:
p = xk −xk−1, q = rk −rk−1
6:
ζk = (QT
k−1Qk−1)†QT
k−1q
7:
if ∥Pk−1ζk∥2 ≤cp∥p∥2 and ∥Qk−1ζk∥2 ≤cq∥q∥2 then
8:
pk = p −Pk−1ζk, qk = q −Qk−1ζk
9:
Pk = [pk−1, pk], Qk = [qk−1, qk]
(qk ⊥Qk−1)
10:
else
11:
Pk = Pk−1, Qk = Qk−1
12:
end if
13:
else
14:
Pk, Qk = 0 ∈Rd×2, pk, qk = 0 ∈Rd"
REFERENCES,0.3110236220472441,"15:
end if
16:
Γk = (QT
k Qk)†QT
k rk
17:
¯xk = xk −PkΓk, ¯rk = rk −QkΓk
(Projection step: ¯rk ⊥Qk)
18:
xk+1 = ¯xk + βk¯rk
(Mixing step)
19: end for
20: return xk
(The notation “†” is the Moore-Penrose pseudoinverse.)"
REFERENCES,0.31181102362204727,"In the linear case, Algorithm 2 and Algorithm 3 (with m = ∞) are equivalent. Similar to Algo-
rithm 2, we have the following properties held for MST-AM:"
REFERENCES,0.3125984251968504,"Claim 1. In the k-th iteration (k > 0) of Algorithm 1 applied to minimize a strongly convex
quadratic problem (5), assuming cp = ∞, cq = 1, m = ∞, the following relations hold:
(i) ∥qk∥2 > 0, range( ¯Pk) = range(Xk) = Kk(A, r0), range( ¯Qk) = range(Rk) = AKk(A, r0);
(ii) ¯Qk = −A ¯Pk, qi ⊥qj(1 ≤i ̸= j ≤k);
(iii) ¯rk ⊥range( ¯Qk) and ¯xk = x0 + zk, where zk = arg minz∈Kk(A,r0) ∥r0 −Az∥2.
If ∥¯rk∥2 = 0, then xk+1 is the exact solution."
REFERENCES,0.31338582677165355,"The proof of Claim 1 is essentially the same as the proof of Theorem 1, with a special care that
¯QT
k ¯Qk = Ik is replaced by the relation that columns of ¯Qk are orthogonal to each other, in other
words, ¯QT
k ¯Qk = diag{∥q1∥2
2, . . . , ∥qk∥2
2}."
REFERENCES,0.31417322834645667,Published as a conference paper at ICLR 2022
REFERENCES,0.31496062992125984,"For minimizing nonlinear functions or accelerating nonlinear ﬁxed-point iterations, the long-term
relation that ¯QT
k ¯Qk = diag{∥q1∥2
2, . . . , ∥qk∥2
2} generally cannot hold, while the orthogonalization
procedure in Line 6 still leads to a short-term orthogonality relation: qi ⊥qj for |i −j| ≤2.
Hence, QT
k Qk = diag{∥qk−1∥2
2, ∥qk∥2
2} for k ≥1.
Thus the pseudoinverse (QT
k Qk)† =
diag{I(∥qk−1∥2 ̸= 0)1/∥qk−1∥2
2, I(∥qk∥2 ̸= 0)1/∥qk∥2
2}, where I(·) is the indicator function
that"
REFERENCES,0.315748031496063,"I(x) =

1
x is true,
0
x is false."
REFERENCES,0.3165354330708661,"Now, we give the proof of Theorem 2."
REFERENCES,0.3173228346456693,"Proof of Theorem 2. For convenience, we restate the main assumptions of g here:"
REFERENCES,0.31811023622047246,"(i) ∥g(y) −g(x)∥2 ≤κ∥y −x∥2, κ ∈(0, 1), for ∀x, y ∈B(ρ),
(32a)"
REFERENCES,0.3188976377952756,"(ii) ∥g′(y) −g′(x)∥2 ≤ˆκ∥y −x∥2, ˆκ > 0, for ∀x, y ∈B(ρ).
(32b)"
REFERENCES,0.31968503937007875,"Also, since |1−βk|+κβk < 1, we know βk > 0 is bounded, i.e. βk ≤β where β > 0 is a constant."
REFERENCES,0.32047244094488186,"The proof is based on the two lemmas given in Lemma 1 and Lemma 2. Besides (9), we also prove
that ∥rk∥2 ≤∥r0∥2 by induction."
REFERENCES,0.32125984251968503,"For k = 0, ¯x0 = x0 ∈B(ρ), and due to (48b), ∥x1 −x∗∥2 ≤∥x0 −x∗∥2 + β0∥r0∥2 ≤∥x0 −
x∗∥2 + β(1 + κ)∥x0 −x∗∥2 ≤ρ provided ∥x0 −x∗∥2 ≤ρ/(1 + β(1 + κ)). Since"
REFERENCES,0.3220472440944882,"r1 = g(x1) −x1 = g(x1) −(x0 + β0r0) = g(x1) −(x0 + r0) + (1 −β0)r0
= g(x1) −g(x0) + (1 −β0)r0,"
REFERENCES,0.3228346456692913,it follows that
REFERENCES,0.3236220472440945,"∥r1∥2 ≤∥g(x1) −g(x0)∥2 + |1 −β0|∥r0∥2
≤κβ0∥r0∥2 + |1 −β0|∥r0∥2 = (κβ0 + |1 −β0|)∥r0∥2."
REFERENCES,0.32440944881889766,"Also note that θ0 = ∥¯r0∥2/∥r0∥2 = 1. Thus (9) holds. Because κβ0 + |1 −β0| ≤κ0 < 1, we have
∥r1∥2 < ∥r0∥2."
REFERENCES,0.3251968503937008,"Now, suppose that (9) and ∥rk∥2 ≤∥r0∥2 hold for k ≥0. We establish the results for k + 1."
REFERENCES,0.32598425196850395,"Let Γk = (γ(1)
k , γ(2)
k )T ∈R2. Since Γk = (QT
k Qk)†QT
k rk, QT
k Qk = diag{∥qk−1∥2
2, ∥qk∥2
2}, it
follows that"
REFERENCES,0.32677165354330706,"γ(1)
k
= I(qk−1 ̸= 0) rT
k qk−1
qT
k−1qk−1
, γ(2)
k
= I(qk ̸= 0)rT
k qk
qT
k qk
.
(33)"
REFERENCES,0.32755905511811023,"Therefore,"
REFERENCES,0.3283464566929134,"|γ(1)
k | ≤I(qk−1 ̸= 0) ∥rk∥2"
REFERENCES,0.3291338582677165,"∥qk−1∥2
, |1 −γ(1)
k | ≤max

I(qk−1 = 0), I(qk−1 ̸= 0)∥rk −qk−1∥2"
REFERENCES,0.3299212598425197,"∥qk−1∥2 
,"
REFERENCES,0.33070866141732286,"|γ(2)
k | ≤I(qk ̸= 0)∥rk∥2"
REFERENCES,0.331496062992126,"∥qk∥2
, |1 −γ(2)
k | ≤max

I(qk = 0), I(qk ̸= 0)∥rk −qk∥2 ∥qk∥2"
REFERENCES,0.33228346456692914,"
.
(34)"
REFERENCES,0.33307086614173226,"Deﬁne c =
1+cp
(1−κ)(1−cq). We have"
REFERENCES,0.33385826771653543,"∥PkΓk∥2 = ∥pkγ(2)
k
+ pk−1γ(1)
k−1∥2 ≤∥pkγ(2)
k ∥2 + ∥pk−1γ(1)
k−1∥2"
REFERENCES,0.3346456692913386,"≤I(qk ̸= 0)∥pk∥2
∥rk∥2
∥qk∥2
+ I(qk−1 ̸= 0)∥pk−1∥2
∥rk∥2
∥qk−1∥2
≤2c∥rk∥2,
(35)"
REFERENCES,0.3354330708661417,where the second inequality is due to (34) and the third inequality is due to (49). Then
REFERENCES,0.3362204724409449,∥¯xk −x∗∥2 = ∥xk −PkΓk −x∗∥2 ≤∥xk −x∗∥2 + ∥PkΓk∥2
REFERENCES,0.33700787401574805,"≤
1
1 −κ∥rk∥2 + 2c∥rk∥2 = (
1
1 −κ + 2c)∥rk∥2,"
REFERENCES,0.33779527559055117,Published as a conference paper at ICLR 2022
REFERENCES,0.33858267716535434,"and due to ∥¯rk∥2 ≤∥rk∥2, it holds that"
REFERENCES,0.33937007874015745,∥xk+1 −x∗∥2 = ∥¯xk + βk¯rk −x∗∥2 ≤∥¯xk −x∗∥2 + βk∥¯rk∥2
REFERENCES,0.3401574803149606,"≤∥¯xk −x∗∥2 + β∥rk∥2 = (
1
1 −κ + 2c + β)∥rk∥2."
REFERENCES,0.3409448818897638,"By the inductive hypothesis that ∥rk∥2 ≤∥r0∥2, and (48b), it has"
REFERENCES,0.3417322834645669,"∥¯xk −x∗∥2 ≤(
1
1 −κ + 2c)∥r0∥2 ≤

1
1 −κ + 2c

(1 + κ)∥x0 −x∗∥2,
(36) and"
REFERENCES,0.3425196850393701,"∥xk+1 −x∗∥2 ≤

1
1 −κ + 2c + β

∥r0∥2 ≤

1
1 −κ + 2c + β

(1 + κ) ∥x0 −x∗∥2.
(37)"
REFERENCES,0.34330708661417325,"As a result, we can choose ∥x0 −x∗∥2 sufﬁciently small to ensure ¯xk ∈B(ρ) and xk+1 ∈B(ρ),
which ensure the g(¯xk) and g(xk+1) are well deﬁned."
REFERENCES,0.34409448818897637,"At the end of the k-th iteration of Algorithm 3, we have"
REFERENCES,0.34488188976377954,"rk+1 = g(xk+1) −xk+1
= g(xk+1) −g(¯xk) + g(¯xk) −(¯xk + βk¯rk)
= (g(xk+1) −g(¯xk)) + (g(¯xk) −¯xk −¯rk) + (1 −βk)¯rk.
(38)"
REFERENCES,0.34566929133858265,"Let Lk := g(xk+1) −g(¯xk) + (1 −βk)¯rk, Hk := g(¯xk) −¯xk −¯rk, then"
REFERENCES,0.3464566929133858,"∥Lk∥2 ≤κ∥xk+1 −¯xk∥2 + |1 −βk|∥¯rk∥2
= κβk∥¯rk∥2 + |1 −βk|∥¯rk∥2
= θk(κβk + |1 −βk|)∥rk∥2,
(39)"
REFERENCES,0.347244094488189,which bounds the linear part of the residual rk+1.
REFERENCES,0.3480314960629921,"For the higher-order terms Hk, we have"
REFERENCES,0.3488188976377953,"Hk = g(¯xk) −(xk −PkΓk + rk −QkΓk)
= g(¯xk) −g(xk) + (Pk + Qk)Γk."
REFERENCES,0.34960629921259845,"= g(¯xk) −g(xk) + (pk + qk)γ(2)
k
+ (pk−1 + qk−1)γ(1)
k .
(40)"
REFERENCES,0.35039370078740156,"According to the formula
R 1
0 g′(x + t(y −x))(y −x)dt = g(y) −g(x), we have"
REFERENCES,0.35118110236220473,"g(¯xk) −g(xk) = g(¯xk) −g(xk −pkγ(2)
k ) + g(xk −pkγ(2)
k ) −g(xk) =
Z 1"
REFERENCES,0.35196850393700785,"0
−g′(xk −pkγ(2)
k
−tpk−1γ(1)
k )pk−1γ(1)
k dt +
Z 1"
REFERENCES,0.352755905511811,"0
−g′(xk −tpkγ(2)
k )pkγ(2)
k dt.
(41)"
REFERENCES,0.3535433070866142,"Also, by Lemma 2, we have"
REFERENCES,0.3543307086614173,"pk + qk =
Z 1"
REFERENCES,0.3551181102362205,"0
g′(xk −tpk)pkdt + ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.35590551181102364,"j=k−mk
O(∥∆rj∥2),"
REFERENCES,0.35669291338582676,"pk−1 + qk−1 =
Z 1"
REFERENCES,0.35748031496062993,"0
g′(xk−1 −tpk−1)pk−1dt + ˆκ∥∆rυ(k)−1∥2 k−2
X"
REFERENCES,0.35826771653543305,"j=k−mk
O(∥∆rj∥2),"
REFERENCES,0.3590551181102362,"where π(k) denotes that the latest update of qk by Line 8 occurred in the π(k)-th iteration and
υ(k) = π(π(k)−1) marks that qk−1 records qυ(k) that is the penultimate update by Line 8 occurring
in the υ(k)-th iteration."
REFERENCES,0.3598425196850394,Published as a conference paper at ICLR 2022
REFERENCES,0.3606299212598425,"By substituting these relations to (40), it follows that Hk =  
Z 1"
REFERENCES,0.36141732283464567,"0
g′(xk −tpk)pkdt + ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.36220472440944884,"j=k−mk
O(∥∆rj∥2) "
REFERENCES,0.36299212598425196,"γ(2)
k +  
Z 1"
REFERENCES,0.3637795275590551,"0
g′(xk−1 −tpk−1)pk−1dt + ˆκ∥∆rυ(k)−1∥2 k−2
X"
REFERENCES,0.36456692913385824,"j=k−mk
O(∥∆rj∥2) "
REFERENCES,0.3653543307086614,"γ(1)
k −
Z 1"
REFERENCES,0.3661417322834646,"0
g′(xk −pkγ(2)
k
−tpk−1γ(1)
k )pk−1γ(1)
k dt −
Z 1"
REFERENCES,0.3669291338582677,"0
g′(xk −tpkγ(2)
k )pkγ(2)
k dt =
Z 1"
REFERENCES,0.36771653543307087,"0
(g′(xk −tpk) −g′(xk −tpkγ(2)
k ))pkγ(2)
k dt +
Z 1"
REFERENCES,0.36850393700787404,"0
(g′(xk−1 −tpk−1) −g′(xk −pkγ(2)
k
−tpk−1γ(1)
k ))pk−1γ(1)
k dt"
REFERENCES,0.36929133858267715,"+ ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.3700787401574803,"j=k−mk
O(∥∆rj∥2)γ(2)
k"
REFERENCES,0.37086614173228344,"+ ˆκ∥∆rυ(k)−1∥2 k−2
X"
REFERENCES,0.3716535433070866,"j=k−mk
O(∥∆rj∥2)γ(1)
k"
REFERENCES,0.3724409448818898,"= Ak + Bk + Ck + Dk.
(42)
Then we can bound each terms of Hk as follows (Here we assume qk ̸= 0 and qk−1 ̸= 0 as qk = 0
leads to γ(2)
k
= 0 and qk−1 = 0 leads to γ(1)
k
= 0 where the result is trivial.):"
REFERENCES,0.3732283464566929,∥Ak∥2 = Z 1
REFERENCES,0.37401574803149606,"0
(g′(xk −tpk) −g′(xk −tpkγ(2)
k ))pkγ(2)
k dt

2 ≤
Z 1"
REFERENCES,0.37480314960629924,"0
∥g′(xk −tpk) −g′(xk −tpkγ(2)
k )∥2∥pkγ(2)
k ∥2dt"
REFERENCES,0.37559055118110235,"≤ˆκ
Z 1"
REFERENCES,0.3763779527559055,"0
∥tpk −tpkγ(2)
k ∥2∥pk∥2|γ(2)
k |dt = ˆκ"
REFERENCES,0.37716535433070864,"2 ∥pk∥2
2|1 −γ(2)
k ||γ(2)
k | = ˆκ"
REFERENCES,0.3779527559055118,"2 ∥pk∥2
2
O(∥rk∥2∥rk −qk∥2)"
REFERENCES,0.378740157480315,"∥qk∥2
2
= ˆκ k
X"
REFERENCES,0.3795275590551181,"j=k−mk
O(∥rj∥2
2),
(43)"
REFERENCES,0.38031496062992126,"where the third equality is due to (34), and the last equality is due to (49) and (50b);"
REFERENCES,0.38110236220472443,∥Bk∥2 = Z 1
REFERENCES,0.38188976377952755,"0
(g′(xk−1 −tpk−1) −g′(xk −pkγ(2)
k
−tpk−1γ(1)
k ))pk−1γ(1)
k dt

2 ≤
Z 1"
REFERENCES,0.3826771653543307,"0
∥g′(xk−1 −tpk−1) −g′(xk −pkγ(2)
k
−tpk−1γ(1)
k )∥2∥pk−1γ(1)
k ∥2dt"
REFERENCES,0.38346456692913383,"≤ˆκ
Z 1"
REFERENCES,0.384251968503937,"0
∥∆xk−1 −pkγ(2)
k
−tpk−1γ(1)
k
+ tpk−1∥2∥pk−1∥2|γ(1)
k |dt"
REFERENCES,0.3850393700787402,"≤ˆκ

∥∆xk−1∥2 + ∥pk∥2|γ(2)
k | + 1"
REFERENCES,0.3858267716535433,"2∥pk−1∥2|1 −γ(1)
k |

∥pk−1∥2|γ(1)
k |"
REFERENCES,0.38661417322834646,"≤ˆκ

∥∆xk−1∥2 + ∥pk∥2
∥rk∥2
∥qk∥2
+ 1"
REFERENCES,0.38740157480314963,"2∥pk−1∥2
∥rk −qk−1∥2"
REFERENCES,0.38818897637795274,∥qk−1∥2
REFERENCES,0.3889763779527559,"
∥pk−1∥2
∥rk∥2
∥qk−1∥2
= ˆκ∥rk∥2 (O(∥∆rk−1∥2) + O(∥rk∥2) + O(∥rk −qk−1∥2) = ˆκ k
X"
REFERENCES,0.38976377952755903,"j=k−mk
O(∥rj∥2
2),
(44)"
REFERENCES,0.3905511811023622,Published as a conference paper at ICLR 2022
REFERENCES,0.39133858267716537,"where the last inequality is due to (34), and the second equality is due to (48a), (49), (50b);"
REFERENCES,0.3921259842519685,"∥Ck∥2 = ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.39291338582677166,"j=k−mk
O(∥∆rj∥2)γ(2)
k"
REFERENCES,0.3937007874015748,"≤ˆκ∥∆rπ(k)−1∥2
∥rk∥2
∥qk∥2 k−1
X"
REFERENCES,0.39448818897637794,"j=k−mk
O(∥∆rj∥2)"
REFERENCES,0.3952755905511811,"= ˆκ∥∆rπ(k)−1∥2
∥rk∥2
∥qπ(k)∥2 k−1
X"
REFERENCES,0.3960629921259842,"j=k−mk
O(∥∆rj∥2) = ˆκ k
X"
REFERENCES,0.3968503937007874,"j=k−mk
O(∥rj∥2
2),
(45)"
REFERENCES,0.39763779527559057,"where the ﬁrst inequality is from (34), and the second equality is due to (50b);"
REFERENCES,0.3984251968503937,"∥Dk∥2 = ˆκ∥∆rυ(k)−1∥2 k−2
X"
REFERENCES,0.39921259842519685,"j=k−mk
O(∥∆rj∥2)γ(1)
k"
REFERENCES,0.4,"≤ˆκ∥∆rυ(k)−1∥2
∥rk∥2
∥qk−1∥2 k−2
X"
REFERENCES,0.40078740157480314,"j=k−mk
O(∥∆rj∥2)"
REFERENCES,0.4015748031496063,"= ˆκ∥∆rυ(k)−1∥2
∥rk∥2
∥qυ(k)∥2 k−2
X"
REFERENCES,0.4023622047244094,"j=k−mk
O(∥∆rj∥2) = ˆκ k
X"
REFERENCES,0.4031496062992126,"j=k−mk
O(∥rj∥2
2),
(46)"
REFERENCES,0.40393700787401576,"where the ﬁrst inequality is from (34), and the second equality is due to (50b). Then with the bounds
(43), (44), (45) and (46), we obtain"
REFERENCES,0.4047244094488189,"∥Hk∥2 = ˆκ k
X"
REFERENCES,0.40551181102362205,"j=k−mk
O(∥rj∥2
2).
(47)"
REFERENCES,0.4062992125984252,"Combining (39) and (47) to (38), we obtain"
REFERENCES,0.40708661417322833,"∥rk+1∥≤∥Lk∥2 + ∥Hk∥2 ≤θk(κβk + |1 −βk|)∥rk∥2 + ˆκ k
X"
REFERENCES,0.4078740157480315,"j=k−mk
O(∥rj∥2
2),"
REFERENCES,0.4086614173228346,"as desired. Since mk ≤m, the higher-order terms are limited. Note that"
REFERENCES,0.4094488188976378,κβk + |1 −βk| ≤κ0 < 1
REFERENCES,0.41023622047244096,"by assumption. Then, for ∥x0−x∗∥2 sufﬁciently small, the residuals {rk} are Q-linearly convergent,
which infers ∥rk∥2 ≤∥r0∥2. Therefore, we complete the induction."
REFERENCES,0.4110236220472441,"Remark 6. There may be some concern about whether ˆκ can be counteracted by the constant hidden
in the Big-O notation. In fact, since ˆκ is the Lipschitz constant of g′ and the constants in O(·) are
composed of κ, cp, cq, it follows that ˆκ is unrelated to O(·). Hence a small ˆκ can lead to a small
uniform boundedness of the higher-order terms. In the extreme case where ˆκ = 0 , e.g. g is a linear
map, the residual only consists of the ﬁrst-order term Lk.
Lemma 1. Under the same assumptions of Theorem 2, for k ≥1, we have the following bounds:"
REFERENCES,0.41181102362204725,"(1 −κ)∥∆xk−1∥2 ≤∥∆rk−1∥2 ≤(1 + κ)∥∆xk−1∥2,
(48a)
(1 −κ)∥xk −x∗∥2 ≤∥rk∥2 ≤(1 + κ)∥xk −x∗∥2,
(48b)"
REFERENCES,0.4125984251968504,"If qk ̸= 0, then
∥pk∥2
∥qk∥2
≤
1 + cp
(1 −κ)(1 −cq)
(49)"
REFERENCES,0.41338582677165353,Published as a conference paper at ICLR 2022
REFERENCES,0.4141732283464567,"If the condition in Line 7 is true, then"
REFERENCES,0.4149606299212598,"∥pk∥2 ≤(1 + cp)∥∆xk−1∥2,
(50a)
qk ̸= 0, (1 −cq)∥∆rk−1∥≤∥qk∥2 ≤∥∆rk−1∥2
(50b)"
REFERENCES,0.415748031496063,"Proof. From the assumption (32a) of g, we have"
REFERENCES,0.41653543307086616,"(1 −κ)∥∆xk−1∥2 ≤∥xk −xk−1∥2 −∥g(xk) −g(xk−1)∥2
≤∥g(xk) −g(xk−1) −(xk −xk−1)∥2
= ∥rk −rk−1∥2 = ∥∆rk−1∥2
≤∥g(xk) −g(xk−1)∥2 + ∥xk −xk−1∥
≤(1 + κ)∥xk −xk−1∥2,"
REFERENCES,0.41732283464566927,"(1 −κ)∥xk −x∗∥2 ≤∥xk −x∗∥2 −∥g(xk) −g(x∗)∥2
≤∥g(xk) −g(x∗) −(xk −x∗)∥2 = ∥rk∥2
≤∥g(xk) −g(x∗)∥2 + ∥xk −x∗∥2
≤(1 + κ)∥xk −x∗∥2."
REFERENCES,0.41811023622047244,"If the condition in Line 7 is true, i.e., ∥Pk−1ζk∥2 ≤cp∥∆xk−1∥2, ∥Qk−1ζk∥2 ≤cq∥∆rk−1∥2, we
have"
REFERENCES,0.4188976377952756,"∥pk∥2 = ∥∆xk−1 −Pk−1ζk∥2 ≤∥∆xk−1∥2 + ∥Pk−1ζk∥2 ≤(1 + cp)∥∆xk−1∥2,"
REFERENCES,0.4196850393700787,∥qk∥2 = ∥∆rk−1 −Qk−1ζk∥2 ≥∥∆rk−1∥2 −∥Qk−1ζk∥2 ≥(1 −cq)∥∆rk−1∥2.
REFERENCES,0.4204724409448819,"The inequality ∥qk∥2 ≤∥∆rk−1∥is due to the fact that qk is the orthogonal projection of ∆rk−1
onto range(Qk−1)⊥. Also, qk ̸= 0 must hold otherwise q = Qk−1ζk which violates the condition
∥Qk−1ζk∥2 ≤cq∥∆rk−1∥2 as cq ∈(0, 1)."
REFERENCES,0.421259842519685,"If qk ̸= 0, then qk must be updated by Line 8 in some previous iteration. We assume the latest
update by Line 8 occurred in the j-th iteration, i.e., qk = qj, pk = pj, hence"
REFERENCES,0.4220472440944882,"∥pk∥2
∥qk∥2
= ∥pj∥2"
REFERENCES,0.42283464566929135,"∥qj∥2
≤(1 + cp)∥∆xj−1∥2"
REFERENCES,0.42362204724409447,"(1 −cq)∥∆rj−1∥2
≤
1 + cp
(1 −cq)(1 −κ)."
REFERENCES,0.42440944881889764,where the ﬁrst inequality is due to (50a) and (50b) and the second inequality is due to (48a).
REFERENCES,0.4251968503937008,"Lemma 2. Under the same assumptions of Theorem 2, in the k-th iteration (k ≥0) of the restarted
MST-AM (Algorithm 3), we have"
REFERENCES,0.4259842519685039,"pk + qk =
Z 1"
REFERENCES,0.4267716535433071,"0
g′(xk −tpk)pkdt + ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.4275590551181102,"j=k−mk
O(∥∆rj∥2),
(51)"
REFERENCES,0.4283464566929134,"where mk = k mod m, ∆rk−mk−1 := rk−mk, π(k) denotes that the latest update of qk by Line 8
occurred in the π(k)-th iteration and π(k) = k−mk if Line 8 is never executed up to the k-iteration."
REFERENCES,0.42913385826771655,"Proof. We prove (51) by induction. Denote ζk = (ζ(1)
k , ζ(2)
k )T."
REFERENCES,0.42992125984251967,"For k = 0, the relation trivially holds."
REFERENCES,0.43070866141732284,"For k = 1, p1 = ∆x0, q1 = ∆r0. It follows that"
REFERENCES,0.431496062992126,"p1 + q1 = g(x1) −g(x0) =
Z 1"
REFERENCES,0.4322834645669291,"0
g′(x1 −tp1)p1dt."
REFERENCES,0.4330708661417323,So (51) holds for k = 1.
REFERENCES,0.4338582677165354,"Suppose that (51) holds for 0 ≤j ≤k −1, where k ≥2. For k ≥2, if mk := k mod m = 0 or 1,
(51) holds because it is the same as the case of k = 0 or k = 1. Now consider the nontrivial cases."
REFERENCES,0.4346456692913386,Published as a conference paper at ICLR 2022
REFERENCES,0.43543307086614175,"For mk ̸= 0 and mk ̸= 1, if the condition in Line 7 in Algorithm 3 is true, then π(k) = k. With the
convention that ∆rk−mk−1 = rk−mk, we have"
REFERENCES,0.43622047244094486,pk + qk = ∆xk−1 −Pk−1ζk + ∆rk−1 −Qk−1ζk
REFERENCES,0.43700787401574803,"= g(xk) −g(xk−1) −(pk−1 + qk−1)ζ(2)
k
−(pk−2 + qk−2)ζ(1)
k =
Z 1"
REFERENCES,0.4377952755905512,"0
g′(xk −t∆xk−1)∆xk−1dt −
Z 1"
REFERENCES,0.4385826771653543,"0
g′(xk−1 −tpk−1)pk−1ζ(2)
k dt + ˆκ∥∆rπ(k−1)−1∥2 k−2
X"
REFERENCES,0.4393700787401575,"j=k−1−mk−1
O(∥∆rj∥2)ζ(2)
k −
Z 1"
REFERENCES,0.4401574803149606,"0
g′(xk−2 −tpk−2)pk−2ζ(1)
k dt + ˆκ∥∆rυ(k−1)−1∥2 k−3
X"
REFERENCES,0.4409448818897638,"j=k−2−mk−2
O(∥∆rj∥2)ζ(1)
k , (52)"
REFERENCES,0.44173228346456694,"where υ(k −1) = π(π(k −1) −1) denotes that qk−2 records qυ(k−1) that is the penultimate update
by Line 8 occurring in the υ(k −1)-th iteration. Considering the terms in pk + qk, we know"
REFERENCES,0.44251968503937006,"g′(xk −t∆xk−1)∆xk−1 −g′(xk−1 −tpk−1)pk−1ζ(2)
k
−g′(xk−2 −tpk−2)pk−2ζ(1)
k
= g′(xk −t∆xk−1)(∆xk−1 −pk−1ζ(2)
k
−pk−2ζ(1)
k )"
REFERENCES,0.44330708661417323,"+ (g′(xk −t∆xk−1) −g′(xk−1 −tpk−1))pk−1ζ(2)
k
+ (g′(xk −t∆xk−1) −g′(xk−2 −tpk−2))pk−2ζ(1)
k .
(53)"
REFERENCES,0.4440944881889764,"For ζk = (QT
k−1Qk−1)†QT
k−1∆rk−1, because QT
k−1Qk−1 = diag{∥qk−2∥2
2, ∥qk−1∥2
2}, it follows
that"
REFERENCES,0.4448818897637795,"ζ(1)
k
= I(qk−2 ̸= 0)∆rT
k−1qk−2
qT
k−2qk−2
, ζ(2)
k
= I(qk−1 ̸= 0)∆rT
k−1qk−1
qT
k−1qk−1
."
REFERENCES,0.4456692913385827,"Therefore,"
REFERENCES,0.4464566929133858,"|ζ(1)
k | ≤I(qk−2 ̸= 0)∥∆rk−1∥2"
REFERENCES,0.44724409448818897,"∥qk−2∥2
, |ζ(2)
k | ≤I(qk−1 ̸= 0)∥∆rk−1∥2"
REFERENCES,0.44803149606299214,"∥qk−1∥2
.
(54)"
REFERENCES,0.44881889763779526,"Now, to bound the terms in (53), we have"
REFERENCES,0.4496062992125984,"∥(g′(xk −t∆xk−1) −g′(xk−1 −tpk−1))pk−1ζ(2)
k ∥2"
REFERENCES,0.4503937007874016,"≤ˆκ∥xk −xk−1 −t(∆xk−1 −pk−1)∥2∥pk−1ζ(2)
k ∥2"
REFERENCES,0.4511811023622047,"≤ˆκ((1 −t)∥∆xk−1∥2 + t∥pk−1∥2)∥pk−1∥2|ζ(2)
k |"
REFERENCES,0.4519685039370079,≤ˆκ((1 −t)∥∆xk−1∥2 + t∥pk−1∥2)∥pk−1∥2I(qk−1 ̸= 0)∥∆rk−1∥2
REFERENCES,0.452755905511811,∥qk−1∥2
REFERENCES,0.45354330708661417,"≤ˆκ∥∆rk−1∥2 k−1
X"
REFERENCES,0.45433070866141734,"j=k−1−mk−1
O(∥∆rj∥2) = ˆκ∥∆rk−1∥2 k−1
X"
REFERENCES,0.45511811023622045,"j=k−mk
O(∥∆rj∥2),
(55)"
REFERENCES,0.4559055118110236,"where the third inequality is due to (54), and the last inequality is due to (48a), (50a) and (49), where
the constants are absorbed into the big-O notation. Similarly, it follows that"
REFERENCES,0.4566929133858268,"∥(g′(xk −t∆xk−1) −g′(xk−2 −tpk−2))pk−2ζ(1)
k ∥2"
REFERENCES,0.4574803149606299,"≤ˆκ∥∆xk−1 + ∆xk−2 −t∆xk−1 + tpk−2∥2∥pk−2ζ(1)
k ∥2"
REFERENCES,0.4582677165354331,"≤ˆκ((1 −t)∥∆xk−1∥2 + ∥∆xk−2∥2 + t∥pk−2∥2)∥pk−2∥2|ζ(1)
k |"
REFERENCES,0.4590551181102362,≤ˆκ((1 −t)∥∆xk−1∥2 + ∥∆xk−2∥2 + t∥pk−2∥2)∥pk−2∥2I(qk−2 ̸= 0)∥∆rk−1∥2
REFERENCES,0.45984251968503936,∥qk−2∥2
REFERENCES,0.46062992125984253,"≤ˆκ∥∆rk−1∥2 k−1
X"
REFERENCES,0.46141732283464565,"j=k−2−mk−2
O(∥∆rj∥2) = ˆκ∥∆rk−1∥2 k−1
X"
REFERENCES,0.4622047244094488,"j=k−mk
O(∥∆rj∥2).
(56)"
REFERENCES,0.462992125984252,Published as a conference paper at ICLR 2022
REFERENCES,0.4637795275590551,"For the remaining terms in (52), we have"
REFERENCES,0.4645669291338583,"ˆκ∥∆rπ(k−1)−1∥2 k−2
X"
REFERENCES,0.4653543307086614,"j=k−1−mk−1
O(∥∆rj∥2)ζ(2)
k
+ ˆκ∥∆rυ(k−1)−1∥2 k−3
X"
REFERENCES,0.46614173228346456,"k=k−2−mk−2
O(∥∆rj∥2)ζ(1)
k"
REFERENCES,0.46692913385826773,≤ˆκ∥∆rπ(k−1)−1∥2I(qk−1 ̸= 0)∥∆rk−1∥2
REFERENCES,0.46771653543307085,"∥qk−1∥2 k−2
X"
REFERENCES,0.468503937007874,"j=k−1−mk−1
O(∥∆rj∥2)"
REFERENCES,0.4692913385826772,+ ˆκ∥∆rυ(k−1)−1∥2I(qk−2 ̸= 0)∥∆rk−1∥2
REFERENCES,0.4700787401574803,"∥qk−2∥2 k−3
X"
REFERENCES,0.47086614173228347,"k=k−2−mk−2
O(∥∆rj∥2)"
REFERENCES,0.4716535433070866,"≤ˆκ∥∆rk−1∥2 k−2
X"
REFERENCES,0.47244094488188976,"k=k−mk
O(∥∆rj∥2),
(57)"
REFERENCES,0.4732283464566929,"where the last inequality is due to (50b) and that qk−1 = qπ(k−1), qk−2 = qυ(k−1)."
REFERENCES,0.47401574803149604,"With relations (52), (53), (55), (56) and (57), and noting that pk = ∆xk−1 −pk−1ζ(2)
k
−pk−2ζ(1)
k ,
and Z 1"
REFERENCES,0.4748031496062992,"0
(g′(xk −t∆xk−1)∆xk−1 −g′(xk−1 −tpk−1)pk−1ζ(2)
k
−g′(xk−2 −tpk−2)pk−2ζ(1)
k )dt

2 ≤
Z 1"
REFERENCES,0.4755905511811024,"0
∥g′(xk −t∆xk−1)∆xk−1 −g′(xk−1 −tpk−1)pk−1ζ(2)
k
−g′(xk−2 −tpk−2)pk−2ζ(1)
k ∥2dt,"
REFERENCES,0.4763779527559055,we can estimate pk + qk as
REFERENCES,0.47716535433070867,"pk + qk =
Z 1"
REFERENCES,0.4779527559055118,"0
g′(xk −t∆xk−1)pkdt + ˆκ∥∆rk−1∥2 k−1
X"
REFERENCES,0.47874015748031495,"j=k−mk
O(∥∆rj∥2).
(58)"
REFERENCES,0.4795275590551181,"To further obtain (51), notice that the difference Z 1"
REFERENCES,0.48031496062992124,"0
g′(xk −tpk)pkdt −
Z 1"
REFERENCES,0.4811023622047244,"0
g′(xk −t∆xk−1)pkdt

2 = Z 1"
REFERENCES,0.4818897637795276,"0
(g′(xk −tpk) −g′(xk −t∆xk−1))pkdt

2 ≤
Z 1"
REFERENCES,0.4826771653543307,"0
∥g′(xk −tpk) −g′(xk −t∆xk−1)∥2∥pk∥2dt"
REFERENCES,0.48346456692913387,"≤ˆκ
Z 1"
REFERENCES,0.484251968503937,"0
t∥pk−1ζ(2)
k
+ pk−2ζ(1)
k ∥2∥pk∥2dt ≤ˆκ"
REFERENCES,0.48503937007874015,"2 cp∥∆xk−1∥2∥pk∥2 = ˆκ∥∆rk−1∥2O(∥∆rk−1∥2),"
REFERENCES,0.4858267716535433,"where the last inequality is due to the condition ∥Pk−1ζk∥2 ≤cp∥p∥2 and inequalities (48a) and
(50a). Hence the difference can be absorbed in the Big-O notation in (58). Thus (51) holds when
the condition in Line 7 is true for the k-th iteration."
REFERENCES,0.48661417322834644,"We consider the case where the condition in Line 7 is false for the k-th iteration. Then Pk =
Pk−1, Qk = Qk−1. In other words, the memory recording pk, qk keeps unchanged from the (π(k)+
1)-th to k-th iteration. Since π(k) < k and π(π(k)) = π(k), with the inductive hypothesis, we have"
REFERENCES,0.4874015748031496,"pk + qk = pπ(k) + qπ(k) =
Z 1"
REFERENCES,0.4881889763779528,"0
g′(xπ(k) −tpπ(k))pπ(k)dt + ˆκ∥∆rπ(k)−1∥2"
REFERENCES,0.4889763779527559,"π(k)−1
X"
REFERENCES,0.48976377952755906,"j=π(k)−mπ(k)
O(∥∆rj∥2) =
Z 1"
REFERENCES,0.4905511811023622,"0
g′(xπ(k) −tpk)pkdt + ˆκ∥∆rπ(k)−1∥ k−1
X"
REFERENCES,0.49133858267716535,"j=k−mk
O(∥∆rj∥2).
(59)"
REFERENCES,0.4921259842519685,Published as a conference paper at ICLR 2022
REFERENCES,0.49291338582677163,"To further obtain (51), note that the difference Z 1"
REFERENCES,0.4937007874015748,"0
g′(xk −tpk)pkdt −
Z 1"
REFERENCES,0.494488188976378,"0
g′(xπ(k) −tpk)pkdt

2 ≤
Z 1"
REFERENCES,0.4952755905511811,"0
∥g′(xk −tpk) −g′(xπ(k) −tpk)∥2∥pk∥2dt"
REFERENCES,0.49606299212598426,"≤ˆκ∥xk −xπ(k)∥2∥pk∥2 = ˆκ k−1
X"
REFERENCES,0.4968503937007874,"j=π(k)
∥∆xj∥2∥pk∥2 = ˆκ∥∆rπ(k)−1∥2 k−1
X"
REFERENCES,0.49763779527559054,"j=π(k)
O(∥∆rj∥2)"
REFERENCES,0.4984251968503937,"can be absorbed into the Big-O notation in (59). Thus (51) also holds when the condition in Line 7
is false for the k-th iteration."
REFERENCES,0.49921259842519683,"As a result, we complete the induction in the k-th iteration."
REFERENCES,0.5,"C.3
REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING"
REFERENCES,0.5007874015748032,"C.3.1
CHECK OF POSITIVE DEFINITENESS"
REFERENCES,0.5015748031496063,"We describe the check of positive deﬁniteness in RST-AM, which follows the same procedure as
that of SAM (Wei et al., 2021). From Line 11-13 in Algorithm 1, one update of xk RST-AM is
xk+1 = xk + Hkrk, where Hk = βkI −αkYkZ†
kQT
k , Yk = Pk + βkQk, Zk = QT
k Qk + δkP T
k Pk.
Hk is generally not symmetric. For the convergence analysis of RST-AM, a critical condition is the
positive deﬁniteness of Hk, i.e."
REFERENCES,0.5023622047244094,"sT
k Hksk ≥βkµ∥sk∥2
2,
∀sk ∈Rd,
(60)"
REFERENCES,0.5031496062992126,"where µ ∈(0, 1) is a constant. Next, we show how to guarantee it."
REFERENCES,0.5039370078740157,"Let λmin(·) denote the smallest eigenvalue, and λmax(·) denote the largest eigenvalue.
Since
sT
k Hksk = 1"
ST,0.5047244094488189,"2sT
k (Hk + HT
k )sk, Condition (60) is equivalent to λmin
  1"
ST,0.5055118110236221,"2
 
Hk + HT
k

≥βkµ. By
some simple algebraic operations, we obtain λmin
  1"
ST,0.5062992125984253,"2
 
Hk + HT
k

= βk −1"
ST,0.5070866141732283,"2αkλmax(YkZ†
kQT
k +
QkZ†
kY T
k ). Let λk := λmax(YkZ†
kQT
k + QkZ†
kY T
k ), then Condition (60) is equivalent to"
ST,0.5078740157480315,"αkλk ≤2βk(1 −µ),
(61)"
ST,0.5086614173228347,"namely, (13) in Remark 4. To check Condition (61), note that"
ST,0.5094488188976378,λk = λmax
ST,0.510236220472441,"
(Yk
Qk)

0
Z†
k
Z†
k
0"
ST,0.511023622047244," 
Y T
k
QT
k"
ST,0.5118110236220472,"
= λmax"
ST,0.5125984251968504,"
Y T
k
QT
k"
ST,0.5133858267716536,"
(Yk
Qk)

0
Z†
k
Z†
k
0"
ST,0.5141732283464567,"
.
(62)"
ST,0.5149606299212598,"Since

Y T
k
QT
k"
ST,0.515748031496063,"
(Yk
Qk) ,

0
Z†
k
Z†
k
0"
ST,0.5165354330708661,"
∈R4×4, λk can be computed cheaply. This cost is negligible"
ST,0.5173228346456693,"compared with those to form P T
k Pk, QT
k Qk, which need O(d) ﬂops. Then, to guarantee the positive
deﬁniteness of Hk, we check whether αk satisﬁes (61) and use a smaller αk if necessary, e.g. αk =
2βk(1 −µ)/λk."
ST,0.5181102362204725,"C.3.2
PROOFS OF THE THEOREMS"
ST,0.5188976377952756,"We ﬁrst give the proof of the boundedness of ∥Pk−1ζk∥2 and ∥Qk−1ζk∥2.
Lemma 3. For P, Q ∈Rd×m(d ≥m), δ > 0, and Z = QTQ + δP TP, we have"
ST,0.5196850393700787,∥PZ†QT∥2 ≤δ−1
ST,0.5204724409448819,"2 ,
(63a)"
ST,0.521259842519685,"∥QZ†QT∥2 ≤1.
(63b)"
ST,0.5220472440944882,"Proof. We ﬁrst consider the case that Z is nonsingular, then PZ†QT = PZ−1QT, QZ†QT =
QZ−1QT. It can be seen that P TP and QTQ are symmetric positive semideﬁnite, and Z is SPD as
it is assumed to be nonsingular. Also, we have δP TP ⪯Z and QTQ ⪯Z, where the notation “⪯”"
ST,0.5228346456692914,Published as a conference paper at ICLR 2022
ST,0.5236220472440944,"denotes the Loewner partial order, i.e., A ⪯B with A, B ∈Rm×m means that B −A is positive
semideﬁnite. Hence, we have Z−1"
ST,0.5244094488188976,2 δP TPZ−1
ST,0.5251968503937008,"2 ⪯I, Z−1"
ST,0.525984251968504,"2 QTQZ−1 2 ⪯I,"
ST,0.5267716535433071,which implies ∥Z−1
ST,0.5275590551181102,"2  
P TP

Z−1"
ST,0.5283464566929134,"2 ∥2 ≤δ−1, ∥Z−1"
ST,0.5291338582677165,"2  
QTQ

Z−1"
ST,0.5299212598425197,2 ∥2 ≤1.
ST,0.5307086614173229,"Let λmax(·) denote the largest eigenvalue, we have"
ST,0.531496062992126,"∥QZ−1QT∥2 = λmax
 
QZ−1QT
= λmax
 
QTQZ−1
= λmax

Z−1"
ST,0.5322834645669291,2 QTQZ−1
ST,0.5330708661417323,"2

≤1,"
ST,0.5338582677165354,"∥PZ−1QT∥2
2 = λmax
 
PZ−1QTQZ−1P T"
ST,0.5346456692913386,"= λmax
 
P TPZ−1QTQZ−1"
ST,0.5354330708661418,"= λmax

Z−1"
ST,0.5362204724409448,"2  
P TP

Z−1"
ST,0.537007874015748,2 · Z−1
ST,0.5377952755905512,"2  
QTQ

Z−1 2
 ≤∥Z−1"
ST,0.5385826771653544,"2  
P TP

Z−1"
ST,0.5393700787401575,2 · Z−1
ST,0.5401574803149606,"2  
QTQ

Z−1 2 ∥2 ≤∥Z−1"
ST,0.5409448818897638,"2  
P TP

Z−1"
ST,0.5417322834645669,2 ∥2∥Z−1
ST,0.5425196850393701,"2  
QTQ

Z−1"
ST,0.5433070866141733,2 ∥2 ≤δ−1.
ST,0.5440944881889764,"Therefore, ∥PZ−1QT∥≤δ−1"
ST,0.5448818897637795,2 and ∥QZ−1QT∥2 ≤1.
ST,0.5456692913385827,"For the case that Z is singular, it can be proved that"
ST,0.5464566929133858,"ker(Z) := {x ∈Rm|Zx = 0} = {x ∈Rm|Px = 0 and Qx = 0}.
(64)"
ST,0.547244094488189,"In fact, if Px = 0 and Qx = 0, it is obvious that Zx = 0. On the other side, if Zx = 0, then
xTZx = 0, i.e. xTQTQx + δxTP TPx = 0. Since 0 ⪯P TP, 0 ⪯QTQ, δ > 0, it follows that
xTQTQx = 0 and δxTP TPx = 0, which further implies Qx = 0 and Px = 0. Hence (64) holds."
ST,0.5480314960629922,"Let U1 satisfy U T
1 U1 = I and range(U1) = ker(Z), i.e. the orthonormal basis of ker(Z), and U2
satisfy U T
2 U2 = I and U T
2 U1 = 0, i.e. the orthonormal basis of ker(Z)⊥. With the equality (64),
we know PU1 = 0, QU1 = 0. Deﬁne U = (U1, U2) ∈Rm×m, then U TU = Im and by direct
computation, we have"
ST,0.5488188976377952,"U TZU =
0
0
0
U T
2 ZU2 
,"
ST,0.5496062992125984,"where U T
2 ZU2 = (QU2)TQU2 + δ(PU2)TPU2 is nonsingular according to the deﬁnition of U2.
So"
ST,0.5503937007874016,"Z† = U
0
0
0
(U T
2 ZU2)−1"
ST,0.5511811023622047,"
U T."
ST,0.5519685039370079,"As a result, we can further compute PZ†QT and QZ†QT as"
ST,0.552755905511811,"PZ†QT = (PU2)(U T
2 ZU2)−1(QU2)T, QZ†QT = (QU2)(U T
2 ZU2)−1(QU2)T."
ST,0.5535433070866141,"Then let ˆP = PU2, ˆQ = QU2, and ˆZ = ˆQT ˆQ + δ ˆP T ˆP, and noting that ˆZ is nonsingular, we can
obtain ∥PZ†QT∥2 = ∥ˆP ˆZ−1 ˆQT∥2 ≤δ−1"
ST,0.5543307086614173,"2 , ∥QZ†QT∥2 = ∥ˆQ ˆZ−1 ˆQT∥2 ≤1."
ST,0.5551181102362205,"As a result of Lemma 3, ∥Pk−1ζk∥2, ∥Qk−1ζk∥2 in Algorithm 1 are bounded by O(∥∆rk−1∥2), as
shown in the following corollary."
ST,0.5559055118110237,"Corollary 2. ∥Pk−1ζk∥2, ∥Qk−1ζ∥2 in Algorithm 1 are bounded, i.e."
ST,0.5566929133858268,"∥Pk−1ζk∥2 ≤(δ(1)
k )−1"
ST,0.5574803149606299,"2 ∥∆rk−1∥2, ∥Qk−1ζk∥2 ≤∥∆rk−1∥2,
(65)"
ST,0.5582677165354331,"where ζk = (QT
k−1Qk−1 + δ(1)
k Pk−1Pk−1)†QT
k−1∆rk−1."
ST,0.5590551181102362,Published as a conference paper at ICLR 2022
ST,0.5598425196850394,"Now, we turn to the proofs of the theorems about RST-AM in Section 3.4. For brevity, we use δk to
denote δ(2)
k , i.e. δk ≡δ(2)
k . The proofs follow those of SAM (Wei et al., 2021). Nonetheless, since
RST-AM uses different historical sequences compared with SAM, we give the detailed proofs for
RST-AM for completeness."
ST,0.5606299212598426,"From Assumption 2, for the mini-batch gradient fSk(xk) =
1
nk
P"
ST,0.5614173228346456,"i∈Sk fξi(xk), where nk = |Sk|,
the following properties hold:"
ST,0.5622047244094488,"E[∇fSk(x)|xk] = ∇f(xk),
(66a)"
ST,0.562992125984252,"E[∥∇fSk(xk) −∇f(xk)∥2
2|xk] ≤σ2"
ST,0.5637795275590551,"nk
.
(66b)"
ST,0.5645669291338583,"Consider the update of RST-AM. From Line 11-13 in Algorithm 1, it can be written as xk+1 =
xk + Hkrk, where rk = −∇fSk(xk), and for k ≥0,"
ST,0.5653543307086614,"Hk = βkI −αk (Pk + βkQk)
 
QT
k Qk + δkP T
k Pk
† QT
k .
(67)"
ST,0.5661417322834645,"To prove the theorems, the critical points are (i) the positive deﬁniteness of the approximate Hessian
Hk and (ii) an adequate suppression of the noise from the gradient estimates in the stochastic case."
ST,0.5669291338582677,We ﬁrst give a lemma related to the projection step.
ST,0.5677165354330709,"Lemma 4. Suppose that {xk} is generated by RST-AM. If αk ≥0, βk > 0, then for any vk ∈Rd,
we have"
ST,0.568503937007874,"∥Hkvk∥2
2 ≤2
 
β2
k
 
1 + 2α2
k −2αk

+ α2
kδ−1
k

∥vk∥2
2.
(68)"
ST,0.5692913385826772,"Proof. The result holds when k = 0 as H0 = β0I. For k ≥1,"
ST,0.5700787401574803,"Hkvk = βkvk −(αkPk + αkβkQk)Γk,
(69)"
ST,0.5708661417322834,"where Γk = (QT
k Qk + δkP T
k Pk)†QT
k vk solves"
ST,0.5716535433070866,"min
Γ ∥vk −QkΓ∥2
2 + δk∥PkΓ∥2
2.
(70)"
ST,0.5724409448818898,"By direct computation, ∥vk −QkΓk∥2
2 +δk∥PkΓk∥2
2 = ∥vk∥2
2 −vT
k Qk(QT
k Qk +δkP T
k Pk)† ·QT
k vk,
thus
∥vk −QkΓk∥2
2 + δk∥PkΓk∥2
2 ≤∥vk∥2
2.
(71)"
ST,0.573228346456693,"Therefore,"
ST,0.574015748031496,"∥Hkvk∥2
2
= ∥βkvk −(αkPk + αkβkQk)Γk∥2
2
= ∥βk (vk −αkQkΓk) −αkPkΓk∥2
2"
ST,0.5748031496062992,"= ∥βk(1 −αk)vk + βkαk(vk −QkΓk) −αkδ
−1"
K,0.5755905511811024,"2
k
δ"
K,0.5763779527559055,"1
2
k PkΓk∥2
2
≤
 
β2
k(1 −αk)2 + β2
kα2
k + α2
kδ−1
k

·
 
∥vk∥2
2 + ∥vk −QkΓk∥2
2 + δk∥PkΓk∥2
2
"
K,0.5771653543307087,"≤
 
β2
k
 
1 + 2α2
k −2αk

+ α2
kδ−1
k
  
∥vk∥2
2 + ∥vk∥2
2
"
K,0.5779527559055118,"= 2
 
β2
k
 
1 + 2α2
k −2αk

+ α2
kδ−1
k

∥vk∥2
2.
(72)"
K,0.5787401574803149,"In the above, the ﬁrst inequality uses the inequality ∥ n
X"
K,0.5795275590551181,"i=1
aixi∥2
2 ≤ n
X"
K,0.5803149606299213,"i=1
|ai|∥xi∥2 !2 ≤ n
X"
K,0.5811023622047244,"i=1
a2
i"
K,0.5818897637795276,"!  n
X"
K,0.5826771653543307,"i=1
∥xi∥2
2 !"
K,0.5834645669291338,",
(73)"
K,0.584251968503937,"where ai ∈R, xi ∈Rd. The second inequality is based on inequality (71)."
K,0.5850393700787402,"With Lemma 4, we can prove the deterministic case of RST-AM."
K,0.5858267716535434,Published as a conference paper at ICLR 2022
K,0.5866141732283464,"Proof of Theorem 3. Since 1+2α2
k −2αk ≤1 for 0 ≤αk ≤1, and δ−1
k
≤C−1β2, with Lemma 4,
we have
∥Hkrk∥2
2 ≤2β2(1 + C−1)∥rk∥2
2."
K,0.5874015748031496,"Since αk satisﬁes (61), we have
rT
k Hkrk ≥βµ∥rk∥2
2."
K,0.5881889763779528,"Then, under Assumption 1, we have"
K,0.5889763779527559,f(xk+1) ≤f(xk) + ∇f(xk)T(xk+1 −xk) + L
K,0.5897637795275591,"2 ∥xk+1 −xk∥2
2"
K,0.5905511811023622,"= f(xk) −rT
k Hkrk + L"
K,0.5913385826771653,"2 ∥Hkrk∥2
2"
K,0.5921259842519685,"≤f(xk) −βµ∥rk∥2
2 + Lβ2(1 + C−1)∥rk∥2
2
= f(xk) −β
 
µ −βL(1 + C−1)

∥rk∥2
2"
K,0.5929133858267717,≤f(xk) −1
K,0.5937007874015748,"2βµ · ∥∇f(xk)∥2
2,
(74)"
K,0.594488188976378,"where the last inequality is due to 0
<
β
≤
µ
2L(1+C−1).
Thus, f(xk+1) −f(xk)
≤
−1"
K,0.5952755905511811,"2βµ∥∇f(xk)∥2
2."
K,0.5960629921259842,"Summing both sides of this inequality for k ∈{0, . . . , N −1} and recalling f(x) > f low in
Assumption 1 gives"
K,0.5968503937007874,"f low −f(x0) ≤f(xN) −f(x0) ≤−1 2βµ N−1
X"
K,0.5976377952755906,"k=0
∥∇f(xk)∥2
2."
K,0.5984251968503937,Rearranging and dividing further by N yields (15).
K,0.5992125984251968,The next lemmas and proofs are about the stochastic case.
K,0.6,"Lemma 5. Suppose that Assumption 2 holds for {xk} generated by RST-AM. In addition, if βk > 0,
and αk ≥0 and satisﬁes (13), then"
K,0.6007874015748031,"ESk[∥Hkrk∥2
2] ≤2

β2
k
 
1 + 2α2
k −2αk

+ α2
k
δk"
K,0.6015748031496063,"
·

∥∇f(xk)∥2
2 + σ2 nk"
K,0.6023622047244095,"
,
(75a)"
K,0.6031496062992125,∇f(xk)TESk[Hkrk] ≤−1
K,0.6039370078740157,"2βkµ∥∇f(xk)∥2
2 + 1"
K,0.6047244094488189,"2
α2
k(δ
−1"
K,0.6055118110236221,"2
k
+ βk)2"
K,0.6062992125984252,"βkµ
· σ2"
K,0.6070866141732284,"nk
,
(75b)"
K,0.6078740157480315,where µ > 0 is the constant introduced in Remark 4.
K,0.6086614173228346,"Proof. (i) From Lemma 4, we have"
K,0.6094488188976378,"ESk[∥Hkrk∥2
2] ≤2

β2
k
 
1 + 2α2
k −2αk

+ α2
k
δk"
K,0.610236220472441,"
ESk[∥rk∥2
2].
(76)"
K,0.6110236220472441,"From Assumption 2, we have"
K,0.6118110236220472,"ESk[∥rk∥2
2] = ESk[∥rk −ESk[rk]∥2
2] + ∥ESk[rk]∥2
2 ≤∥∇f(xk)∥2
2 + σ2/nk.
(77)"
K,0.6125984251968504,"With (76), (77), we obtain (75a)."
K,0.6133858267716535,"(ii) The result holds for k = 0 since H0 = β0I. Consider k ≥1. Deﬁne ϵk = ∇fSk(xk) −
∇f(xk) = −rk −∇f(xk). Then Hkrk = Hk (−ϵk −∇f(xk)) . Since αk satisﬁes (13), it has
λmin
  1"
K,0.6141732283464567,"2
 
Hk + HT
k

≥βkµ. Thus"
K,0.6149606299212599,∇f(xk)THk∇f(xk) = 1
K,0.6157480314960629,"2∇f(xk)T  
Hk + HT
k

∇f(xk) ≥βkµ∥∇f(xk)∥2
2,"
K,0.6165354330708661,"which implies
ESk[∇f(xk)THk∇f(xk)] ≥βkµ∥∇f(xk)∥2
2.
(78)"
K,0.6173228346456693,Published as a conference paper at ICLR 2022
K,0.6181102362204725,"Let Mk = αk (Pk + βkQk)
 
QT
k Qk + δkP T
k Pk
† QT
k , then Hk = βkI −Mk. With the assumption
(66a), i.e. ESk[ϵk] = 0, we have"
K,0.6188976377952756,ESk[∇f(xk)THkϵk] = ESk[∇f(xk)T (βkϵk −Mkϵk)]
K,0.6196850393700788,"= βk∇f(xk)TESk[ϵk] −ESk[∇f(xk)TMkϵk] = −ESk[∇f(xk)TMkϵk].
Using the Cauchy-Schwarz inequality with expectations, we obtain"
K,0.6204724409448819,"|ESk[∇f(xk)THkϵk]| = |ESk[∇f(xk)TMkϵk]| ≤
q"
K,0.621259842519685,"ESk[∥∇f(xk)∥2
2]
q"
K,0.6220472440944882,"ESk[∥Mkϵk∥2
2]"
K,0.6228346456692914,"≤∥∇f(xk)∥2
q"
K,0.6236220472440945,"ESk[∥Mkϵk∥2
2].
(79)"
K,0.6244094488188976,"We now bound ∥Mkϵk∥2
2. For brevity, let Zk = QT
k Qk + δkP T
k Pk, and N1 = PkZ†
kQT
k , N2 =
βkQkZ†
kQT
k , then"
K,0.6251968503937008,"∥Mk∥2 = ∥αk (N1 + N2) ∥2 ≤αk (∥N1∥2 + ∥N2∥2) ≤αk(δ
−1"
K,0.6259842519685039,"2
k
+ βk),
(80)
where the last inequality is from Lemma 3."
K,0.6267716535433071,"With (80), we have ∥Mkϵk∥2 ≤αk(δ
−1"
K,0.6275590551181103,"2
k
+ βk)∥ϵk∥2, which implies"
K,0.6283464566929133,"ESk[∥Mkϵk∥2
2] ≤α2
k(δ
−1"
K,0.6291338582677165,"2
k
+ βk)2ESk[∥ϵk∥2
2] ≤α2
k(δ
−1"
K,0.6299212598425197,"2
k
+ βk)2 σ2"
K,0.6307086614173228,"nk
,
(81)"
K,0.631496062992126,where the last inequality is due to (66b). Now we bound |ESk[∇f(xk)THkϵk]| as follows (cf. (79)):
K,0.6322834645669292,|ESk[∇f(xk)THkϵk]|
K,0.6330708661417322,"≤∥∇f(xk)∥2
q"
K,0.6338582677165354,"ESk[∥Mkϵk∥2
2]"
K,0.6346456692913386,"≤αk(δ
−1"
K,0.6354330708661418,"2
k
+ βk)∥∇f(xk)∥2
q"
K,0.6362204724409449,"ESk[∥ϵk∥2
2]"
K,0.637007874015748,"≤αk(δ
−1"
K,0.6377952755905512,"2
k
+ βk) σ
√nk
∥∇f(xk)∥2 =
p"
K,0.6385826771653543,"βkµ∥∇f(xk)∥2 · αk(δ
−1"
K,0.6393700787401575,"2
k
+ βk)
√βkµ
σ
√nk ≤1"
K,0.6401574803149607,"2βkµ∥∇f(xk)∥2
2 + 1"
K,0.6409448818897637,"2
α2
k(δ
−1"
K,0.6417322834645669,"2
k
+ βk)2"
K,0.6425196850393701,"βkµ
· σ2"
K,0.6433070866141732,"nk
.
(82)"
K,0.6440944881889764,"With the inequality (78) and (82), we obtain
∇f(xk)TESk[Hkrk]"
K,0.6448818897637796,= −∇f(xk)TESk[Hk (ϵk + ∇f(xk))]
K,0.6456692913385826,= −ESk[∇f(xk)THk∇f(xk)] −ESk[∇f(xk)THkϵk]
K,0.6464566929133858,≤−ESk[∇f(xk)THk∇f(xk)] + |ESk[∇f(xk)THkϵk]|
K,0.647244094488189,"≤−βkµ∥∇f(xk)∥2
2 + 1"
K,0.6480314960629922,"2βkµ∥∇f(xk)∥2
2 + 1"
K,0.6488188976377953,"2
α2
k(δ
−1"
K,0.6496062992125984,"2
k
+ βk)2"
K,0.6503937007874016,"βkµ
· σ2 nk = −1"
K,0.6511811023622047,"2βkµ∥∇f(xk)∥2
2 + 1"
K,0.6519685039370079,"2
α2
k(δ
−1"
K,0.6527559055118111,"2
k
+ βk)2"
K,0.6535433070866141,"βkµ
· σ2"
K,0.6543307086614173,"nk
.
(83)"
K,0.6551181102362205,"By imposing one more restriction on αk, we can obtain a convenient corollary:
Corollary 3. Suppose that Assumption 2 holds for {xk} generated by RST-AM. C > 0 is the"
K,0.6559055118110236,"constant in (12). If βk > 0, 0 ≤αk ≤min{1, β"
K,0.6566929133858268,"1
2
k } and satisﬁes (61) , then"
K,0.65748031496063,"ESk[∥Hkrk∥2
2] ≤2β2
k
 
1 + C−1
·

∥∇f(xk)∥2
2 + σ2 nk"
K,0.658267716535433,"
,
(84a)"
K,0.6590551181102362,∇f(xk)TESk[Hkrk] ≤−1
K,0.6598425196850394,"2βkµ∥∇f(xk)∥2
2 + β2
k · µ−1  
1 + C−1 σ2"
K,0.6606299212598425,"nk
.
(84b)"
K,0.6614173228346457,Published as a conference paper at ICLR 2022
K,0.6622047244094488,"Proof. The ﬁrst result (84a) is clear by considering (75a) and noticing that 1+2α2
k −2αk ≤1 when"
K,0.662992125984252,"αk ∈[0, 1] and δ−1
k
≤C−1β2
k. Since αk ≤β"
K,0.6637795275590551,"1
2
k , δk ≥Cβ−2
k
and (1 + C−1"
K,0.6645669291338583,"2 )2 ≤2(1 + C−1) we
have"
K,0.6653543307086615,"1
2
α2
k(δ
−1"
K,0.6661417322834645,"2
k
+ βk)2"
K,0.6669291338582677,"βkµ
· σ2 nk
≤1"
K,0.6677165354330709,"2
βk(C−1"
K,0.668503937007874,2 βk + βk)2
K,0.6692913385826772,"βkµ
· σ2 nk = 1"
K,0.6700787401574804,2µ−1(C−1
K,0.6708661417322834,"2 + 1)2β2
k · σ2 nk"
K,0.6716535433070866,"≤β2
kµ−1(1 + C−1)σ2 nk
."
K,0.6724409448818898,"Substituting it into (75b), we obtain (84b)."
K,0.6732283464566929,"With Corollary 3, we establish the descent property of RST-AM:"
K,0.6740157480314961,Lemma 6. Suppose that Assumptions 1 and 2 hold for {xk} generated by RST-AM. C > 0 is the
K,0.6748031496062992,"constant in (12). If 0 < βk ≤
µ
4L(1+C−1), 0 ≤αk ≤min{1, β"
K,0.6755905511811023,"1
2
k } and satisﬁes (61), then"
K,0.6763779527559055,ESk[f(xk+1)] ≤f(xk) −1
K,0.6771653543307087,"4βkµ∥∇f(xk)∥2
2 + β2
k
 
(L + µ−1)(1 + C−1)
 σ2"
K,0.6779527559055119,"nk
.
(85)"
K,0.6787401574803149,"Proof. According to Assumption 1, we have"
K,0.6795275590551181,f(xk+1) ≤f(xk) + ∇f(xk)T(xk+1 −xk) + L
K,0.6803149606299213,"2 ∥xk+1 −xk∥2
2"
K,0.6811023622047244,= f(xk) + ∇f(xk)THkrk + L
K,0.6818897637795276,"2 ∥Hkrk∥2
2.
(86)"
K,0.6826771653543308,"Taking expectation with respect to the mini-batch Sk on both sides of (86) and using Corollary 3 we
obtain"
K,0.6834645669291338,ESk[f(xk+1)]
K,0.684251968503937,≤f(xk) + ∇f(xk)TESk[Hkrk] + L
K,0.6850393700787402,"2 ESk∥Hkrk∥2
2"
K,0.6858267716535433,≤f(xk) −1
K,0.6866141732283465,"2βkµ∥∇f(xk)∥2
2 + β2
kµ−1(1 + C−1)σ2"
K,0.6874015748031496,"nk
+ Lβ2
k(1 + C−1)

∥∇f(xk)∥2
2 + σ2 nk "
K,0.6881889763779527,= f(xk) −βk 1
K,0.6889763779527559,"2µ −βkL(1 + C−1)

∥∇f(xk)∥2
2 + β2
k(L + µ−1)(1 + C−1)σ2"
K,0.6897637795275591,"nk
.
(87)"
K,0.6905511811023622,"Then (87) combined with the assumption βk ≤
µ
4L(1+C−1) implies (85)."
K,0.6913385826771653,"Lemma 6 suggests that the term related to the noise from gradient estimates is bounded as a second-
order term (i.e. O(β2
k)). Thus, with the diminishing stepsize, the effect of noise also diminishes.
To establish the global convergence, we introduce the deﬁnition of a supermartingale following the
proofs in (Wang et al., 2017; Wei et al., 2021)."
K,0.6921259842519685,"Deﬁnition 1. Let {Fk} be an increasing sequence of σ-algebras. If {Xk} is a stochastic process
satisfying (i) E[|Xk|] < ∞, (ii) Xk ∈Fk for all k, and (iii) E[Xk+1|Fk] ≤Xk for all k, then {Xk}
is called a supermartingale."
K,0.6929133858267716,"Proposition 3 (Supermartingale convergence theorem, see, e.g., Theorem 4.2.12 in (Durrett, 2019)).
If {Xk} is a nonnegative supermartingale, then limk→∞Xk →X almost surely and E[X] ≤
E[X0]."
K,0.6937007874015748,"Now, we prove the convergence theory of RST-AM in the nonconvex stochastic case."
K,0.694488188976378,"Proof of Theorem 4. (i) Deﬁne φk :=
βkµ"
K,0.6952755905511812,"4 ∥∇f(xk)∥2
2 and ˜L := (L + µ−1)(1 + C−1), γk :=
f(xk)+ ˜L σ2"
K,0.6960629921259842,"n
P∞
i=k β2
i . Let Fk be the σ-algebra measuring φk, γk, and xk. From (85) we know that"
K,0.6968503937007874,Published as a conference paper at ICLR 2022
K,0.6976377952755906,"for any k,"
K,0.6984251968503937,"E[γk+1|Fk] = E[f(xk+1)|Fk] + ˜Lσ2 n ∞
X"
K,0.6992125984251969,"i=k+1
β2
i"
K,0.7,≤f(xk) −1
K,0.7007874015748031,"4βkµ∥∇f(xk)∥2
2 + ˜Lσ2 n ∞
X"
K,0.7015748031496063,"i=k
β2
i = γk −φk,
(88)"
K,0.7023622047244095,"which implies that E[γk+1 −f low|Fk] ≤γk −f low −φk. Since φk ≥0, we have 0 ≤E[γk −
f low] ≤γ0 −f low < +∞. As the diminishing condition (14) holds, we obtain E[f(xk)] ≤Mf for
some constant Mf > 0. According to Deﬁnition 1, {γk −f low} is a supermartingale. Therefore,
Proposition 3 indicates that there exists a γ such that limk→∞γk = γ with probability 1, and
E[γ] ≤E[γ0]. Note that from (88) we have E[φk] ≤E[γk] −E[γk+1]. Thus, E "" ∞
X"
K,0.7031496062992126,"k=0
φk # ≤ ∞
X"
K,0.7039370078740157,"k=0
(E[γk] −E[γk+1]) < +∞,"
K,0.7047244094488189,"which further yields that
∞
X"
K,0.705511811023622,"k=0
φk = µ 4 ∞
X"
K,0.7062992125984252,"k=0
βk∥∇f(xk)∥2
2 < +∞with probability 1.
(89)"
K,0.7070866141732284,"Since P∞
k=0 βk = +∞, it follows that (16) holds."
K,0.7078740157480315,"(ii) If the noisy gradient is bounded, i.e.,
Eξk[∥∇fξk(xk)∥2
2] ≤Mg,
(90)
where Mg > 0 is a constant, then a stronger result can be obtained."
K,0.7086614173228346,"For any give ϵ > 0, according to (16), there exist inﬁnitely many iterates xk such that ∥∇f(xk)∥2 ≤
ϵ. Then if (17) does not hold, there must exist two inﬁnite sequences of indices {si}, {ti} with
ti > si, such that for i = 0, 1, . . ., k = si + 1, . . . , ti −1,
∥∇f(xsi)∥2 ≥2ϵ, ∥∇f(xti)∥2 < ϵ, ∥∇f(xk)∥2 ≥ϵ.
(91)
Then from (89) it follows that +∞> ∞
X"
K,0.7094488188976378,"k=0
βk∥∇f(xk)∥2
2 ≥ +∞
X i=0"
K,0.710236220472441,"ti−1
X"
K,0.7110236220472441,"k=si
βk∥∇f(xk)∥2
2 ≥ϵ2
+∞
X i=0"
K,0.7118110236220473,"ti−1
X"
K,0.7125984251968503,"k=si
βk with probability 1,"
K,0.7133858267716535,"which implies that
ti−1
X"
K,0.7141732283464567,"k=si
βk →0 with probability 1, as i →+∞.
(92)"
K,0.7149606299212599,"According to (77) and (72), we have
E[∥xk+1 −xk∥2|xk]
= E[∥Hkrk∥2|xk] ≤
q"
K,0.715748031496063,"2
 
β2
k (1 + 2α2
k −2αk) + α2
kδ−1
k

E[∥rk∥2|xk] ≤βk
p"
K,0.7165354330708661,"2(1 + C−1)E[∥rk∥2|xk] ≤βk
p"
K,0.7173228346456693,"2(1 + C−1)(E[∥rk∥2
2|xk])
1
2 ≤βk
p"
K,0.7181102362204724,2(1 + C−1)M
K,0.7188976377952756,"1
2
g ,
(93)
where the last inequalities are due to Cauchy-Schwarz inequality and (90). Then it follows from (93)
that"
K,0.7196850393700788,"E[∥xti −xsi∥2] ≤
p"
K,0.7204724409448819,"2(1 + C−1)M 1
2
g"
K,0.721259842519685,"ti−1
X"
K,0.7220472440944882,"k=si
βk,"
K,0.7228346456692913,"which together with (92) implies that ∥xti −xsi∥2 →0 with probability 1, as i →+∞. Hence,
from the Lipschitz continuity of ∇f, it follows that ∥∇f(xti) −∇f(xsi)∥2 →0 with probability
1 as i →+∞. However, this contradicts (91). Therefore, the assumption that (17) does not hold is
not true."
K,0.7236220472440945,Published as a conference paper at ICLR 2022
K,0.7244094488188977,"Proof of Theorem 5. According to (87) in Lemma 6, we have N−1
X"
K,0.7251968503937007,"k=0
βk 1"
K,0.7259842519685039,"2µ −βkL(1 + C−1)

E∥∇f(xk)∥2
2"
K,0.7267716535433071,"≤f(x0) −f low + N−1
X"
K,0.7275590551181103,"k=0
β2
k(L + µ−1)(1 + C−1)σ2"
K,0.7283464566929134,"nk
,
(94)"
K,0.7291338582677165,"where the expectation is taken with respect to {Sj}N−1
j=0 . Deﬁne"
K,0.7299212598425197,"PR(k)
def
= Prob{R = k} =
βk
  1"
K,0.7307086614173228,"2µ −βkL(1 + C−1)
"
K,0.731496062992126,"PN−1
j=0 βj
  1"
K,0.7322834645669292,"2µ −βjL(1 + C−1)
, k = 0, . . . , N −1,
(95) then"
K,0.7330708661417323,"E

∥∇f(xR)∥2
2

=
PN−1
k=0 βk
  1"
K,0.7338582677165354,"2µ −βkL(1 + C−1)

E

∥∇f(xk)∥2
2
"
K,0.7346456692913386,"PN−1
j=0 βj
  1"
K,0.7354330708661417,"2µ −βjL(1 + C−1)
"
K,0.7362204724409449,"≤Df + σ2(L + µ−1)(1 + C−1) PN−1
k=0 β2
k/nk
PN−1
j=0 βj
  1"
K,0.7370078740157481,"2µ −βjL(1 + C−1)

.
(96)"
K,0.7377952755905511,"Let ˜D be a problem-independent constant. If we choose βk = β := min{
µ
4L(1+C−1),
˜
D
σ
√"
K,0.7385826771653543,"N }, and
nk = n, then the deﬁnition of PR simpliﬁes to PR(k) = 1/N. From (96) we have"
K,0.7393700787401575,"E[∥∇f(xR)∥2
2] ≤Df + σ2(L + µ−1)(1 + C−1) Nβ2"
K,0.7401574803149606,"n
PN−1
j=0 β( 1 2µ −µ 4 )"
K,0.7409448818897638,= Df + σ2(L + µ−1)(1 + C−1) · Nβ2
K,0.7417322834645669,"n
Nβ · 1 4µ = 4Df"
K,0.74251968503937,"Nβµ + σ2(L + µ−1)(1 + C−1) · β 1
4nµ ≤4Df"
K,0.7433070866141732,Nµ max
K,0.7440944881889764,"(
4L(1 + C−1)"
K,0.7448818897637796,"µ
, σ
√ N
˜D )"
K,0.7456692913385827,+ 4σ2(L + µ−1)(1 + C−1)
K,0.7464566929133858,"nµ
·
˜D
σ
√ N ≤4Df Nµ"
K,0.747244094488189,4L(1 + C−1)
K,0.7480314960629921,"µ
+ σ
√ N
˜D !"
K,0.7488188976377953,"+ 4σ(L + µ−1)(1 + C−1) ˜D nµ
√ N"
K,0.7496062992125985,= 16DfL(1 + C−1)
K,0.7503937007874015,"Nµ2
+
σ
µ
√ N"
DF,0.7511811023622047,4Df
DF,0.7519685039370079,"˜D
+ 4(L + µ−1)(1 + C−1) ˜D n ! ."
DF,0.752755905511811,"Therefore, to ensure E[∥∇f(xR)∥2
2] ≤ϵ, the number of iterations is O(1/ϵ2)."
DF,0.7535433070866142,"D
EXPERIMENTAL DETAILS"
DF,0.7543307086614173,"Our main codes were written based on the PyTorch framework 1 and one GeForce RTX 2080 Ti
GPU was used for the tests in training neural networks. Our methods are ST-AM (MST-AM) and
the regularized version RST-AM."
DF,0.7551181102362204,"D.1
EXPERIMENTAL DETAILS ABOUT ST-AM/MST-AM"
DF,0.7559055118110236,"The experiments about ST-AM were conducted to verify the main theorems, i.e. Theorem 1, Corol-
lary 1 and Theorem 2. Four types of problems were used for the experiments. The conjugate residual
(CR) method is a short-term recurrence version of the full-memory GMRES and needs matrix-vector
products to fulﬁll the algorithm. We give the pseudocode of the CR method (Algorithm 6.20 in
(Saad, 2003)) here for readers who are not familiar with this numerical algorithm."
DF,0.7566929133858268,1 Information about this framework can be found in https://pytorch.org.
DF,0.75748031496063,Published as a conference paper at ICLR 2022
DF,0.7582677165354331,Algorithm 4 CR Algorithm for solving a SPD linear system Ax = b.
DF,0.7590551181102362,"Input: x0 ∈Rd.
Output: x ∈Rd"
DF,0.7598425196850394,"1: r0 = b −Ax0, p0 = r0
2: for k = 0, 1, . . . , until convergence, do"
DF,0.7606299212598425,"3:
αk =
rT
k Ark
(Apk)TApk
4:
xk+1 = xk + αkpk
5:
rk+1 = rk −αkApk"
DF,0.7614173228346457,"6:
βk =
rT
k+1Ark+1"
DF,0.7622047244094489,"rT
k Ark
7:
pk+1 = rk+1 + βkpk
8:
Apk+1 = Ark+1 + βkApk
9: end for
10: return xk"
DF,0.7629921259842519,"According to Theorem 1, the CR method is essentially equivalent to ST-AM for solving strongly
convex quadratic optimization. However, it should be pointed out that the CR method needs to
directly assess the matrix A, and the residual update (Line 5) is based on the linear assumption,
which makes it inapplicable for nonlinear problems or the case that A is unavailable. Also, even
though ﬁnite difference technique can be used to construct the matrix-vector products, the number
of gradient evaluations is twice of that of ST-AM."
DF,0.7637795275590551,"D.1.1
STRONGLY CONVEX QUADRATIC OPTIMIZATION"
DF,0.7645669291338583,"To construct a case of (5), we considered the least squares problem:"
DF,0.7653543307086614,"min
x∈Rd f(x) := 1"
DF,0.7661417322834646,"2∥Ax −b∥2
2,
(97)"
DF,0.7669291338582677,"where A ∈Rℓ×d, b ∈Rℓ, which can be reformulated as a form of (5)."
DF,0.7677165354330708,"We generated a dense random matrix A ∈R500×100 and a dense random vector b ∈R500 for the
test. The gradient descent used a ﬁxed stepsize η ≤
1
∥A∥2
2 that can guarantee convergence, and the
same η was used as the βk for the full-memory AM and ST-AM."
DF,0.768503937007874,"We also conducted additional tests about solving the problem (97) with different condition numbers,
where the eigenvalues of A were set to be uniformly distributed. The results with different condition
numbers (cond(ATA) = 102, 104, 106) are shown in Figure 4. It can be observed that the curves
of CR, AM and ST-AM nearly coincide, which verify the correctness of Theorem 1. Also, since
the eigenvalues are uniformly distributed, the superlinear convergence behaviour may not happen.
Nonetheless, CR, AM and ST-AM are still much faster then the GD method."
DF,0.7692913385826772,"0
10
20
30
40
50
iteration 10
6 10
5 10
4 10
3 10
2 10
1"
DF,0.7700787401574803,"GD: ||rk||2/||r0||2
CR: ||rk||2/||r0||2
AM: ||rk||2/||r0||2
ST-AM: ||rk||2/||r0||2"
DF,0.7708661417322835,(a) cond(ATA) = 1 × 102
DF,0.7716535433070866,"0
10
20
30
40
50
iteration 10
4 10
3 10
2 10
1"
DF,0.7724409448818897,"GD: ||rk||2/||r0||2
CR: ||rk||2/||r0||2
AM: ||rk||2/||r0||2
ST-AM: ||rk||2/||r0||2"
DF,0.7732283464566929,(b) cond(ATA) = 1 × 104
DF,0.7740157480314961,"0
10
20
30
40
50
iteration 10
4 10
3 10
2 10
1"
DF,0.7748031496062993,"GD: ||rk||2/||r0||2
CR: ||rk||2/||r0||2
AM: ||rk||2/||r0||2
ST-AM: ||rk||2/||r0||2"
DF,0.7755905511811023,(c) cond(ATA) = 1 × 106
DF,0.7763779527559055,Figure 4: Solving (97) with different condition numbers: cond(ATA) = λmax(ATA)/λmin(ATA).
DF,0.7771653543307087,Published as a conference paper at ICLR 2022
DF,0.7779527559055118,"D.1.2
SOLVING A NONSYMMETRIC LINEAR SYSTEM"
DF,0.778740157480315,For the solution of a nonsymmetric linear system
DF,0.7795275590551181,"Ax = b,
(98)"
DF,0.7803149606299212,"where A ∈Rd×d, b ∈Rd, the ﬁxed-point iteration (FP) is xk+1 = g(xk) := xk + η(b −Axk).
Theorem 2 requires g to be a contractive map, i.e. ∥I −ηA∥2 < 1. We used a test matrix “ﬁdap029”
from the Matrix Market 2. This matrix is a banded matrix and not symmetric. We used Jacobi
preconditioner for all the tested iterative methods. Since solving linear systems is not the main
focus of this work, we did not do a thorough test of applying ST-AM to solve various nonsymmetric
linear systems."
DF,0.7811023622047244,"D.1.3
CUBIC-REGULARIZED QUADRATIC MINIMIZATION"
DF,0.7818897637795276,The concerned cubic-regularized quadratic minimization is
DF,0.7826771653543307,"min
x∈Rd f(x) := 1"
DF,0.7834645669291339,"2∥Ax −b∥2
2 + M"
DF,0.784251968503937,"3 ∥x∥3
2,
(99)"
DF,0.7850393700787401,"where A ∈Rℓ×d, b ∈Rℓ, and M ≥0 is the regularization parameter. It can be computed that"
DF,0.7858267716535433,∇f(x) = AT(Ax −b) + M∥x∥2x = (ATA + M∥x∥2I)x −ATb.
DF,0.7866141732283465,"Hence, for the gradient descent xk+1 = g(xk) := xk −η∇f(xk), the Jacobian of g is I −ηATA −
ηM(∥x∥−1
2 xxT + ∥x∥2I), which has ˆκ > 0 in the local region B(ρ)."
DF,0.7874015748031497,"We generated a dense random matrix A ∈R500×100 and a dense vector b ∈R500 for the test. The
ofﬁcial implementation of L-BFGS in PyTorch was used for comparison. The historical length m of
L-BFGS was 50, i.e., BFGS was actually used in the test."
DF,0.7881889763779527,"0
10
20
30
40
50
iteration 10
15 10
12 10
9 10
6 10
3 100"
DF,0.7889763779527559,||rk||2/||r0||2
DF,0.789763779527559,"AM: M=100
AM: M=1
AM: M=0.01
MST-AM: M=100
MST-AM: M=1
MST-AM: M=0.01"
DF,0.7905511811023622,(a) MST-AM
DF,0.7913385826771654,"0
10
20
30
40
50
iteration 10
15 10
12 10
9 10
6 10
3 100"
DF,0.7921259842519685,||rk||/||r0||
DF,0.7929133858267716,"AM: M=100
AM: M=1
AM: M=0.01
ST-AM: M=100
ST-AM: M=1
ST-AM: M=0.01"
DF,0.7937007874015748,(b) ST-AM
DF,0.794488188976378,"0
10
20
30
40
50
iteration 10
1 100"
DF,0.7952755905511811,"P : M = 100
P : M = 1
P : M = 0.01
Q : M = 100
Q : M = 1
Q : M = 0.01"
DF,0.7960629921259843,(c) MST-AM
DF,0.7968503937007874,"0
10
20
30
40
50
iteration 10
1 100"
DF,0.7976377952755905,"P : M = 100
P : M = 1
P : M = 0.01
Q : M = 100
Q : M = 1
Q : M = 0.01"
DF,0.7984251968503937,(d) ST-AM
DF,0.7992125984251969,"Figure 5: Solving (99) with different M. (a) ∥rk∥2/∥r0∥2 of MST-AM; (b) ∥rk∥2/∥r0∥2 of ST-AM;
(c) ∥Pk−1ζk∥2/∥∆xk−1∥2 and ∥Qk−1ζk∥2/∥∆rk−1∥2 of MST-AM; (d) ∥Pk−1ζk∥2/∥∆xk−1∥2
and ∥Qk−1ζk∥2/∥∆rk−1∥2 of ST-AM."
DF,0.8,"We also conducted the tests related to different regularization parameters M, and the cases that
M = 0.01, 1, 100 are shown in Figure 5. Figure 5 shows that with the large M = 100, both AM and
MST-AM converge faster. An ablation study was also conducted about the boundedness restriction
of ∥Pk−1ζk∥2, ∥Qk−1ζk∥2 in Theorem 2. In Figure 5(b), we show the convergence behaviour of
ST-AM without the boundedness check. In the case of the rather large regularization M = 100, ST-
AM does not show a monotone decrease of the residual. To further investigate the cause, we plot the
magnitude of ∥Pk−1ζk∥2/∥∆xk−1∥2, ∥Qk−1ζk∥2/∥∆rk−1∥2 in Figure5(c) and Figure 5(d). It can
be observed that the evolutions of ∥Pk−1ζk∥2/∥∆xk−1∥2, ∥Qk−1ζk∥2/∥∆rk−1∥2 are quite oscilla-
tory in ST-AM, while being roughly bounded below 1 in MST-AM. This phenomenon may accounts
for the more stable convergence behaviour of MST-AM and veriﬁes the necessity of the changes of
MST-AM compared with ST-AM. In our experiments, we also found the restarting period can be set
quite large and has little effect on ST-AM."
DF,0.8007874015748031,2 https://math.nist.gov/MatrixMarket/.
DF,0.8015748031496063,Published as a conference paper at ICLR 2022
DF,0.8023622047244094,"D.1.4
ROOT-FINDING PROBLEMS IN THE MULTISCALE DEEP EQUILIBRIUM MODEL"
DF,0.8031496062992126,"The multiscale deep equilibrium (MDEQ) model is a recent extension of the deep equilibrium (DEQ)
model (Bai et al., 2019) for computer vision. One of the central engines for these DEQ models is
the root-ﬁnding problem:"
DF,0.8039370078740158,"fθ(z; x) = gθ(z; x) −z ⇒ﬁnd z∗s.t. fθ(z∗; x) = 0,
(100)"
DF,0.8047244094488188,"where θ and x are parameters and the input representation, respectively. In (Bai et al., 2020), the
Broyden’s method is employed to solve this problem. Since MST-AM is suitable for solving non-
linear systems of equations, we can apply MST-AM to solve (100)."
DF,0.805511811023622,"We implemented the MST-AM method and integrated it into the MDEQ framework 3 . The task was
image classiﬁcation on CIFAR-10 and we used the small model for test. We followed the suggested
experimental setting of the framework. The optimizer was Adam with learning rate of 0.001 and the
weight-decay was 2.5×10−6. The batch size was 128 and the number of epochs was 50. The tested
ﬁxed-point solver was used for the forward root-ﬁnding process and for the backward root-ﬁnding
process. The threshold of the steps for the forward process was 18 and the threshold of the steps for
the backward process was 20."
DF,0.8062992125984252,"We used the built-in AM method and Broyden’s method as the baseline methods. The m for AM
was 20, i.e. using the full-memory AM."
DF,0.8070866141732284,"D.2
EXPERIMENTAL DETAILS ABOUT RST-AM"
DF,0.8078740157480315,"Our experiments on RST-AM focused on training neural networks. Since ST-AM can be regarded
as a special case of RST-AM with δ(1)
k
= δ(2)
k
= 0, the basic ST-AM is also covered. In the
Line 10 in Algorithm 1, αk should be adjusted to meet the positive deﬁniteness check (13), which
can be simpliﬁed to (∆xk)Trk ≥βkµ∥rk∥2
2 in practice. The adjustment of αk can be (i) αk =
min{αk, 2βk(1 −µ)/λk}, or (ii) αk = 0. We used the option (ii) since the violation of (13) seldom
happened in our tests and option (ii) is more simple to apply."
DF,0.8086614173228347,"In the experiments on MNIST, Penn Treebank, we incorporated preconditioning (described in Ap-
pendix A.3) into the baseline method SAM and our method. Preconditioning is found to be effective
for difﬁcult problems, e.g. mini-batch training with very small batch sizes and the scaling of the
model’s parameters being important."
DF,0.8094488188976378,"D.2.1
HYPERPARAMETER SETTING OF RST-AM"
DF,0.8102362204724409,"The hyperparameters of RST-AM are easy to tune. The only hyperparameters that need to be care-
fully tuned are the regularization parameters c1, c2 in δ(1)
k
and δ(2)
k . We found RST-AM is more
sensitive to c1, possibly due to the fact that it inﬂuences the construction of secant equations. c2 can
be quite small in our tests. The hyperparameter C in δ(2)
k
was set to be very small (C = 1 × 10−16)"
DF,0.8110236220472441,"so as to ensure δ(2)
k
=
c2∥rk∥2
2
∥pk∥2
2+ϵ0 almost all the time. ϵ0 is introduced to prevent the denominators
from being zero. We found ∥pk∥2 > 0 and ∥∆xk−1∥2 > 0 always held in the tests, so ϵ0 can be
omitted. In the experiments except for deterministic optimization on MNIST, we kept the setting
c1 = 1, c2 = 1 × 10−7 unchanged and found such setting is quite robust."
DF,0.8118110236220473,"The other hyperparameters are αk, βk, which can be always initially set as 1. So RST-AM has the
same number of hyperparameters to tune as SGDM, since SGDM needs to tune learning rate and
the momentum."
DF,0.8125984251968504,"D.2.2
EXPERIMENTS ON MNIST"
DF,0.8133858267716535,"The experiments on MNIST 4 aimed to validate the effectiveness of RST-AM in deterministic opti-
mization, so we were only concerned about the training loss by regarding it as a nonlinear function
to be optimized. To facilitate the full-batch training, we used a subset of the training dataset by
randomly selecting 10k images from the total 60k images."
DF,0.8141732283464567,"3 https://github.com/locuslab/mdeq.
4Based on the ofﬁcial PyTorch implementation https://github.com/pytorch/examples/blob/master/mnist."
DF,0.8149606299212598,Published as a conference paper at ICLR 2022
DF,0.815748031496063,"The baselines were SGDM, Adam, Adagrad, RMSprop and SAM. We tried our best to ensure that
the baselines had the best performance in the tests. We tuned the learning rates by log-scale grid-
searches from 10−3 to 100. For SGDM, Adam, Adagrad, and RMSprop, the learning rates were
0.2, 0.001, 0.01, 0.001, respectively. For SAM, we used the hyperparameter setting recommended
in (Wei et al., 2021)."
DF,0.8165354330708662,"For RST-AM, we set c1 = 0.05, c2 = 1 × 10−7, αk = βk = 1. When (∆xk)Trk ≤0 occurs,
xk+1 = xk + 0.2rk was used as the new update."
DF,0.8173228346456692,"For the preconditioned RST-AM, we set c1 = 1 × 10−2, c2 = 1 × 10−7 for the Adagrad-
preconditioned RST-AM and c1 = 1 × 10−2, c2 = 1 × 10−8 for the RMSprop-preconditioned
RST-AM."
DF,0.8181102362204724,"For all these tests, we trained the model for 200 epochs."
DF,0.8188976377952756,"0
50
100
150
200
epoch 10
6 10
4 10
2 100"
DF,0.8196850393700787,Train Loss
DF,0.8204724409448819,"Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM"
DF,0.8212598425196851,(a) Train loss
DF,0.8220472440944881,"0
50
100
150
200
epoch 10
9 10
6 10
3 100 103"
DF,0.8228346456692913,Squared norm of gradient
DF,0.8236220472440945,"Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM"
DF,0.8244094488188977,(b) Squared norm of gradient
DF,0.8251968503937008,Figure 6: Training on MNIST with the Adam/RMSprop preconditioner.
DF,0.8259842519685039,"In Figure 2 in the main paper, we report the RMSprop/Adagrad preconditioned SAM/RST-AM. We
give the result about using Adam as the preconditioner in Figure 6. It suggests that Adam does not
perform as well as the RMSprop method to serve as a preconditioner for SAM/RST-AM in this task.
Nevertheless, Adam RST-AM is still better than Adam and Adam SAM(2), which demonstrates the
effect of our proposed short-term recurrence scheme."
DF,0.8267716535433071,"D.2.3
EXPERIMENTS ON CIFAR"
DF,0.8275590551181102,"This group of experiments were the same as those in (Wei et al., 2021) so that we can have a direct
comparison between RST-AM and SAM. We still give the details here for completeness."
DF,0.8283464566929134,"We followed the same way of training ResNet (He et al., 2016). The batchsize was set to be 128.
For N iterations of training, the learning rate of each optimizer was decayed at the (⌊N"
DF,0.8291338582677166,"2 ⌋)-th and the
(⌊3"
DF,0.8299212598425196,"4N⌋)-th iterations. Here, for SAM and RST-AM, the αk and βk serve as the learning rates, so
the learning rate decay denotes decaying αk, βk simultaneously. The experiments were run with 3
random seeds."
DF,0.8307086614173228,"The baseline optimizers were SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang
et al., 2020), Lookahead (Zhang et al., 2019) and AdaHessian (Yao et al., 2021). Adam and the
recently proposed optimizers AdaBound and AdaBelief are adaptive learning rate methods which
use different learning rates for different model parameters. Lookahead is a k-step method and has
an inner optimizer. In each cycle of Lookahead, the inner-optimizer optim is used to iterate for k
steps and then the ﬁrst iterate and the last iterate are interpolated to obtain the starting point of the
next cycle. Compared to Adam, AdaHessian uses Hessian-vector products to construct a diagonal
approximation of the Hessian."
DF,0.831496062992126,"For fair comparison, the hyperparameters of all the optimizers (including RST-AM) were tuned
through experiments on CIFAR-10/ResNet20. For each optimizer, the hyperparameter setting that
attained the highest ﬁnal test accuracy on CIFAR-10/ResNet20 was kept unchanged for training
the other networks on CIFAR. We found the results of hyperparameter tuning were consistent with
those reported in (Wei et al., 2021). For completeness, we list the settings of hyperparameters here
(learning rate is abbreviated as lr, and “*” indicates the same setting as that in (Wei et al., 2021))."
DF,0.8322834645669291,"• SGDM∗: lr = 0.1, momentum = 0.9, weight-decay = 5 × 10−4, lr-decay = 0.1."
DF,0.8330708661417323,Published as a conference paper at ICLR 2022
DF,0.8338582677165355,"0
20
40
60
80
100
120
140
160
epochs 30 40 50 60 70 80 90"
DF,0.8346456692913385,Test Accuracy %
DF,0.8354330708661417,"SGDM
Adam
AdaBound
AdaBelief
Lookahead
Adahessian
SAM(2)
SAM(10)
RST-AM"
DF,0.8362204724409449,"120
140
160 93 94 95"
DF,0.837007874015748,(a) CIFAR-10/ResNet18
DF,0.8377952755905512,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8385826771653543,Test Accuracy %
DF,0.8393700787401575,"SGDM
Adam
AdaBound
AdaBelief
Lookahead
Adahessian
SAM(2)
SAM(10)
RST-AM"
DF,0.8401574803149606,"120
140
160
92 93 94 95"
DF,0.8409448818897638,(b) CIFAR-10/WRN16-4
DF,0.841732283464567,"0
20
40
60
80
100
120
140
160
epochs 0 10 20 30 40 50 60 70 80"
DF,0.84251968503937,Test Accuracy %
DF,0.8433070866141732,"SGDM
Adam
AdaBound
AdaBelief
Lookahead
SAM(2)
SAM(10)
RST-AM"
DF,0.8440944881889764,"120
140
160 74 76 78 80"
DF,0.8448818897637795,(c) CIFAR-100/ResNeXt50
DF,0.8456692913385827,"0
20
40
60
80
100
120
140
160
epochs 10 20 30 40 50 60 70 80"
DF,0.8464566929133859,Test Accuracy %
DF,0.8472440944881889,"SGDM
Adam
AdaBound
AdaBelief
Lookahead
SAM(2)
SAM(10)
RST-AM"
DF,0.8480314960629921,"120
140
160 76 78 80"
DF,0.8488188976377953,(d) CIFAR-100/DenseNet121
DF,0.8496062992125984,"Figure 7: Test accuracy of training ResNet18 and WideResNet16-4 on CIFAR-10 and training
ResNeXt50 and DenseNet121 on CIFAR-100."
DF,0.8503937007874016,"• Adam∗: lr = 0.001, (β1, β2) = (0.9, 0.999), weight-decay = 5 × 10−4, lr-decay = 0.1."
DF,0.8511811023622047,"• AdaBound: lr = 0.001, (β1, β2) = (0.9, 0.999), ﬁnal lr = 0.1, gamma = 0.001, weight-
decay = 5 × 10−4, lr-decay = 0.1."
DF,0.8519685039370078,"• AdaBelief∗: lr = 0.001, (β1, β2) = (0.9, 0.999), eps = 1×10−8, weight-decay = 5×10−4,
lr-decay = 0.1."
DF,0.852755905511811,"• Lookahead∗: optim: SGDM (lr = 0.1, momentum = 0.9, weight-decay = 1 × 10−3),
α = 0.8, steps = 10, lr-decay = 0.1."
DF,0.8535433070866142,"• AdaHessian∗: lr = 0.15, (β1, β2) = (0.9, 0.999), eps=1×10−4, hessian-power: 1, weight-
decay = 5 × 10−4/0.15, lr-decay = 0.1."
DF,0.8543307086614174,"• SAM(2): optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10−3), αk =
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 2, weight-decay = 1.5 × 10−3, lr-decay = 0.06."
DF,0.8551181102362204,"• SAM(10)∗: optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10−3), αk =
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 10, weight-decay = 1.5 × 10−3, lr-decay = 0.06."
DF,0.8559055118110236,"• RST-AM: c1 = 1, c2 = 1 × 10−7, α0 = β0 = 1, weight-decay = 1 × 10−3, lr-decay = 0.1."
DF,0.8566929133858268,"For the tests of SGDM, Adam, AdaBelief, Lookahead, AdaHessian and SAM(10), we also had
consistent numerical results with those reported in (Wei et al., 2021), so we reported their results of
these methods in the main paper for reference."
DF,0.8574803149606299,"Figure 7 shows the curves of test accuracy for training ResNet18 and WRN16-4 on CIFAR-10 and
training ResNeXt50 and DenseNet121 on CIFAR-100. The numerical results of ﬁnal test accuracy
can be found in Table 1(a) in the main paper. It can be found in Figure 7 that the convergence
behaviour of RST-AM is less erratic than SAM(10) and SAM(2). In the ﬁrst 80 epochs, RST-AM"
DF,0.8582677165354331,Published as a conference paper at ICLR 2022
DF,0.8590551181102363,"converges faster than SAM, partly due to using a smaller weight decay. The learning rate schedule
has a large impact on the convergence of each optimizer. Similar to SAM(10), RST-AM can always
climb up and stabilize to a higher test accuracy in the ﬁnal 40 epochs. It is found that RST-AM is
comparable to SAM(10), while improving the short-memory SAM(2)."
DF,0.8598425196850393,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90 100"
DF,0.8606299212598425,Train Accuracy %
DF,0.8614173228346457,"SGDM
RST-AM: c2=0.1
RST-AM: c2=0.001
RST-AM: c2=10
5"
DF,0.8622047244094488,"RST-AM: c2=10
7"
DF,0.862992125984252,"RST-AM: c2=10
9"
DF,0.8637795275590551,(a) Train accuracy
DF,0.8645669291338582,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8653543307086614,Test Accuracy %
DF,0.8661417322834646,"SGDM
RST-AM: c2=0.1
RST-AM: c2=0.001
RST-AM: c2=10
5"
DF,0.8669291338582678,"RST-AM: c2=10
7"
DF,0.8677165354330708,"RST-AM: c2=10
9"
DF,0.868503937007874,(b) Test accuracy
DF,0.8692913385826772,Figure 8: Train accuracy and test accuracy of RST-AM with different c2.
DF,0.8700787401574803,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90 100"
DF,0.8708661417322835,Train Accuracy %
DF,0.8716535433070867,"SGDM
RST-AM: c1=10
RST-AM: c1=1
RST-AM: c1=0.1
RST-AM: c1=0.01"
DF,0.8724409448818897,(a) Train accuracy
DF,0.8732283464566929,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8740157480314961,Test Accuracy %
DF,0.8748031496062992,"SGDM
RST-AM: c1=10
RST-AM: c1=1
RST-AM: c1=0.1
RST-AM: c1=0.01"
DF,0.8755905511811024,(b) Test accuracy
DF,0.8763779527559055,Figure 9: Train accuracy and test accuracy of RST-AM with different c1.
DF,0.8771653543307086,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90 100"
DF,0.8779527559055118,Train Accuracy %
DF,0.878740157480315,"SGDM
RST-AM: wd=0.0005
RST-AM: wd=0.0008
RST-AM: wd=0.001
RST-AM: wd=0.0015"
DF,0.8795275590551181,(a) Train accuracy
DF,0.8803149606299212,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8811023622047244,Test Accuracy %
DF,0.8818897637795275,"SGDM
RST-AM: wd=0.0005
RST-AM: wd=0.0008
RST-AM: wd=0.001
RST-AM: wd=0.0015"
DF,0.8826771653543307,(b) Test accuracy
DF,0.8834645669291339,Figure 10: Train accuracy and test accuracy of RST-AM with different weight-decays.
DF,0.8842519685039371,"Since our hyperparameter tuning was conducted on CIFAR10/ResNet20, we give some results about
the hyperparameters on this model."
DF,0.8850393700787401,"Effect of c2 in RST-AM. Figure 8 shows the effect of c2, where we kept c1 = 1, weight-decay=5 ×
10−4 ﬁxed, and c2 = 10−1, 10−3, 10−5, 10−7, 10−9. It indicates that RST-AM is not sensitive to
c2."
DF,0.8858267716535433,"Effect of c1 in RST-AM. Figure 9 shows the effect of c1, where we kept c2 = 10−7, weight-
decay=5×10−4 ﬁxed, and c1 = 0.01, 0.1, 1, 10. It suggests that with smaller c1, RST-AM converges
faster in terms of train accuracy, but the test accuracy is worse."
DF,0.8866141732283465,Published as a conference paper at ICLR 2022
DF,0.8874015748031496,"Effect of weight decay in RST-AM. Weight-decay is a common hyperparameter that can affect the
generalization of each optimizer. In Figure 10, we show the convergence behaviour of RST-AM
with different weight-decays. It suggests that with a too small weight-decay, RST-AM tends to be
overﬁtting in the test dataset."
DF,0.8881889763779528,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8889763779527559,Test Accuracy %
DF,0.889763779527559,"SGDM: epoch=80
SGDM: epoch=100
SGDM: epoch=160
SAM: epoch=80
SAM: epoch=100
SAM: epoch=160
RST-AM: epoch=80
RST-AM: epoch=100
RST-AM: epoch=160"
DF,0.8905511811023622,"60
80
100
120
140
160
92 93 94 95"
DF,0.8913385826771654,"94.82
94.53 94.93 95.27"
DF,0.8921259842519685,(a) Test accuracy on CIFAR10/ResNet18
DF,0.8929133858267716,"0
20
40
60
80
100
120
140
160
epochs 40 50 60 70 80 90"
DF,0.8937007874015748,Test Accuracy %
DF,0.8944881889763779,"SGDM: epoch=80
SGDM: epoch=100
SGDM: epoch=160
SAM: epoch=80
SAM: epoch=100
SAM: epoch=160
RST-AM: epoch=80
RST-AM: epoch=100
RST-AM: epoch=160"
DF,0.8952755905511811,"60
80
100
120
140
160
92 93 94 95"
DF,0.8960629921259843,"94.90
94.78 94.95
95.21"
DF,0.8968503937007875,(b) Test accuracy on CIFAR10/WideResNet16-4
DF,0.8976377952755905,"0
20
40
60
80
100
120
140
160
epochs 0 10 20 30 40 50 60 70 80"
DF,0.8984251968503937,Test Accuracy %
DF,0.8992125984251969,"SGDM: epoch=80
SGDM: epoch=100
SGDM: epoch=160
SAM: epoch=80
SAM: epoch=100
SAM: epoch=160
RST-AM: epoch=80
RST-AM: epoch=100
RST-AM: epoch=160"
DF,0.9,"60
80
100
120
140
160
76 77 78 79 80"
DF,0.9007874015748032,"78.41
78.39 78.96 79.53"
DF,0.9015748031496063,(c) Test accuracy on CIFAR100/ResNeXt50
DF,0.9023622047244094,"0
20
40
60
80
100
120
140
160
epochs 20 30 40 50 60 70 80"
DF,0.9031496062992126,Test Accuracy %
DF,0.9039370078740158,"SGDM: epoch=80
SGDM: epoch=100
SGDM: epoch=160
SAM: epoch=80
SAM: epoch=100
SAM: epoch=160
RST-AM: epoch=80
RST-AM: epoch=100
RST-AM: epoch=160"
DF,0.9047244094488189,"60
80
100
120
140
160
77 78 79 80 78.49 78.90 79.44 80.36"
DF,0.905511811023622,(d) Test accuracy on CIFAR100/DenseNet121
DF,0.9062992125984252,"Figure 11: Training deep neural networks for 80,100,160 epochs. The results of ﬁnal test accuracy
of RST-AM for training 80,100,160 epochs and the ﬁnal test accuracy of SGDM for training 160
epochs are shown in the nested ﬁgures for comparison."
DF,0.9070866141732283,"Table 5:
The cost and ﬁnal test accuracy compared with SGDM. The notations “m”,“t/e”, “e”,
“t” and “a” are abbreviations of memory, per-epoch time, training epochs, total running time, and
accuracy, respectively. “*” indicates numbers published in (Wei et al., 2021)."
DF,0.9078740157480315,"Cost (× SGDM)
CIFAR10/ResNet18
CIFAR10/WRN16-4
& accuracy
m
t/e
e
t
a(%)
m
t/e
e
t
a(%)"
DF,0.9086614173228347,"SGDM∗
1.00
1.00
1.00
1.00
94.82
1.00
1.00
1.00
1.00
94.90
SAM(10)∗
1.73
1.78
0.56
1.00
94.81
1.26
1.28
0.63
0.80
94.94
RST-AM
1.05
1.46
0.56
0.82
94.84
1.03
1.14
0.63
0.71
94.95"
DF,0.9094488188976378,"Cost (× SGDM)
CIFAR100/ResNeXt50
CIFAR100/DenseNet121
& accuracy
m
t/e
e
t
a(%)
m
t/e
e
t
a(%)"
DF,0.9102362204724409,"SGDM∗
1.00
1.00
1.00
1.00
78.41
1.00
1.00
1.00
1.00
78.49
SAM(10)∗
1.30
1.16
0.50
0.58
78.37
1.16
1.19
0.50
0.60
78.84
RST-AM
1.04
1.07
0.50
0.54
78.39
1.01
1.11
0.50
0.55
78.90"
DF,0.9110236220472441,"Table 1(a) in the main paper reports the test accuracy of each optimizer when training for the same
epochs. In fact, as shown in Figure 11, within 100 epochs, RST-AM can achieve a better test"
DF,0.9118110236220472,Published as a conference paper at ICLR 2022
DF,0.9125984251968504,"accuracy than SGD. So if the running time of the training process matters, it is expected that RST-
AM can use less total running time due to the large number of reduction in training epochs. In
Table 1(b), we set SGD as the baseline, and compare the computation and memory cost with SGD.
In Table 5, we give more details about the saving in training epochs and the ﬁnal test accuracy. It
can be seen that RST-AM can achieve a comparable or better test accuracy than SGDM with less
computation time, while reducing the memory footprint of SAM."
DF,0.9133858267716536,"We also tried using Adam as a preconditioner for RST-AM but found the ﬁnal test accuracy was
often worse. For example, the test accuracy of Adam RST-AM for CIFAR-10/ResNet20 is only
90.79%. We suppose it is the worse generalization ability of Adam (Luo et al., 2018) that makes
Adam not suitable as a preconditioner for RST-AM in the image classiﬁcation task."
DF,0.9141732283464566,"D.2.4
EXPERIMENTS ON PENN TREEBANK"
DF,0.9149606299212598,"This group of experiments were the same as those in (Wei et al., 2021) for direct comparison. Results
in Table 2 were measured with 3 random seeds. The parameter settings of the LSTM models were
the same as those in (Zhuang et al., 2020; Wei et al., 2021). The baseline optimizers were SGDM,
Adam, AdaBelief, and SAM. The validation dataset was used for tuning hyperparameters."
DF,0.915748031496063,"For SGDM, the learning rate (abbr. lr) was tuned via grid-search in {1, 10, 30, 100}. We set lr = 10,
momentum = 0.9 for the 2-layer/3-layer LSTM, and lr = 30, momentum = 0 for the 1-layer LSTM."
DF,0.9165354330708662,"For Adam, the learning rate was tuned via grid-search in {1×10−3, 2×10−3, 5×10−3, 8×10−3, 1×
10−2, 2 × 10−2}. We found the setting that lr = 5 × 10−3 performs best."
DF,0.9173228346456693,"For AdaBelief, we set lr = 5 × 10−3 which is better than the recommended settings of the ofﬁcial
implementation5."
DF,0.9181102362204724,"For SAM, we used the recommended setting in (Wei et al., 2021) and used the baseline Adam as the
preconditioner. The cases m = 2 and m = 20 are denoted as SAM(2) and SAM(10), respectively."
DF,0.9188976377952756,"For our method RST-AM, we kept c1 = 1, c2 = 1 × 10−7 unchanged and used the same precondi-
tioner (Adam) as SAM."
DF,0.9196850393700787,"The batch size was 20. We trained the model for 500 epochs and the learning rate was decayed by
0.1 at the 250th epoch and the 375th epoch."
DF,0.9204724409448819,"Table 6:
The cost to achieve comparable results of Adam. The notations “m”,“t/e” and “t” are
abbreviations of memory, per-epoch time and total running time, respectively."
DF,0.9212598425196851,"Cost
1-Layer
2-Layer
3-Layer
(× Adam)
m
t/e
t
m
t/e
t
m
t/e
t"
DF,0.9220472440944882,"Adam
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
SAM(10)
1.20
1.84
0.74
1.36
1.88
0.75
1.90
1.67
0.67
RST-AM
1.06
1.73
0.69
1.15
1.78
0.71
1.11
1.53
0.61"
DF,0.9228346456692913,"Table 7: Test perplexity of training 1,2,3-layer LSTM on Penn Treebank for 200 epochs. Lower is
better. “*” indicates numbers published in (Wei et al., 2021)."
DF,0.9236220472440945,"Method
1-Layer
2-Layer
3-Layer"
DF,0.9244094488188976,"Adam∗
80.88±.15
64.54±.18
60.34±.22
SAM(2)
81.82±.09
66.62±.26
61.55±.11
SAM(10)
79.30±.12
63.21±.02
59.47±.07
RST-AM
79.49±.11
63.61±.26
59.34±.23"
DF,0.9251968503937008,"In Table 6, we report the memory footprint and per-epoch running time of SAM(10) and RST-AM
compared with Adam. It indicates that RST-AM also largely reduces the memory overhead of the"
DF,0.925984251968504,5https://github.com/juntang-zhuang/Adabelief-Optimizer/tree/update 0.2.0/PyTorch Experiments/LSTM.
DF,0.926771653543307,Published as a conference paper at ICLR 2022
DF,0.9275590551181102,"long-memory SAM(10) in the language modeling task. Since the batch size is very small, the cost of
stochastic gradient evaluation is quite cheap, which makes the additional cost of matrix computation
in RST-AM and SAM(10) become considerable. However, if we consider achieving a comparable
validation/test perplexity to that of Adam, RST-AM does not need to train for the same number of
epochs as Adam. Table 7 shows the test perplexity of Adam, SAM(2), SAM(10), and RST-AM for
training 200 epochs. By comparing the results of Table 2 and Table 7, we see RST-AM is better than
Adam with much fewer training epochs, thus RST-AM can save a large amount of training time, as
shown in Table 6."
DF,0.9283464566929134,"D.2.5
EXPERIMENTS ON ADVERSARIAL TRAINING"
DF,0.9291338582677166,The adversarial training considers the empirical adversarial risk minimization (EARM) problem:
DF,0.9299212598425197,"min
x∈Rd
1
T T
X"
DF,0.9307086614173228,"i=1
max
∥¯ξi−ξi∥2≤ϵ f¯ξi(x),
(101)"
DF,0.931496062992126,"where ¯ξi is the i-th adversarial data within the ϵ-ball centered at ξi. We followed the standard PGD
adversarial training in (Madry et al., 2018), using projection gradient descent (PGD) to solve the
inner maximization problem and the tested optimizers (SGD and RST-AM) to solve the outer mini-
mization problem. The experiments were conducted on CIFAR10/ResNet18, CIFAR10/WRN16-4,
CIFAR100/ResNet18, and CIFAR100/DenseNet121. We trained the neural networks for 200 epochs
and decayed the learning rate at the 100th and 150th epoch."
DF,0.9322834645669291,"Since adversarial training is much more time consuming than the ordinary training in Section D.2.3,
we tuned the hyperparameters in CIFAR10/ResNet20, and applied the same hyperparameters to
other models. We found the setting that c1 = 1, c2 = 1 × 10−7 and weight-decay = 0.001 is still
suitable for this task."
DF,0.9330708661417323,"The CIFAR-10 (CIFAR-100) dataset contains 50K images for training and 10K images for testing.
Since it is found that the phenomenon of overﬁtting is severer (Rice et al., 2020) in adversarial
training, we randomly selected 5K images from the total 50K training dataset as the validation
dataset (the other 45K images remained as the training dataset), and chose the best checkpoint
model in the validation set to evaluate on the test dataset. We consider two types of test accuracy:
(i) the clean test accuracy, where clean data was used for model evaluation;
(ii) the robust test accuracy, where corrupted data was used for model evaluation. The attacking
methods are FGSM (Goodfellow et al., 2014), PGD-20 (Madry et al., 2018), and C&W∞attack
(Carlini & Wagner, 2017)."
DF,0.9338582677165355,Table 8: Test accuracy (%) for adversarial training.
DF,0.9346456692913386,"Optimizer
CIFAR10/ResNet18
CIFAR100/DenseNet121
Clean
FGSM
PGD-20
C&W∞
Clean
FGSM
PGD-20
C&W∞
SGD
82.16
63.23
51.91
50.22
59.45
39.76
30.92
29.00
RST-AM
82.53
63.78
52.43
50.52
60.48
40.41
31.20
29.52"
DF,0.9354330708661417,"Optimizer
CIFAR10/WRN16-4
CIFAR100/ResNet18
Clean
FGSM
PGD-20
C&W∞
Clean
FGSM
PGD-20
C&W∞
SGD
80.84
60.97
49.29
47.62
55.42
36.17
28.18
26.31
RST-AM
81.36
61.38
49.93
47.95
56.49
37.00
28.50
26.66"
DF,0.9362204724409449,"In Tabel 8, we report the average results of tests with three different random seeds. It shows that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy across various
models on CIFAR10/CIFAR100. To see the convergence behaviour of SGD and RST-AM, we plot
the curves of the clean validation accuracy and the PGD-10 attacked validation accuracy in Fig-
ure 12. We can observe the phenomenon of robust overﬁtting (Rice et al., 2020) from these curves,
which justiﬁes our experimental setting with validation set for the checkpoint selection. Nonetheless,
the numerical results suggest that RST-AM can still be better than SGDM in adversarial training."
DF,0.937007874015748,"It is also found that due to the heavy cost of gradient evaluations in PGD adversarial training, the
additional computational cost incurred by RST-AM is negligible and the per-epoch running time of
SGD and RST-AM is roughly the same. So we do not report it here."
DF,0.9377952755905512,Published as a conference paper at ICLR 2022
DF,0.9385826771653544,"0
25
50
75
100
125
150
175
200
epochs 30 40 50 60 70 80"
DF,0.9393700787401574,Validation Accuracy %
DF,0.9401574803149606,"SGDM
RST-AM"
DF,0.9409448818897638,(a) CIFAR10/ResNet18
DF,0.9417322834645669,"0
25
50
75
100
125
150
175
200
epochs 20 25 30 35 40 45 50 55"
DF,0.9425196850393701,Adv-Validation Accuracy %
DF,0.9433070866141732,"SGDM
RST-AM"
DF,0.9440944881889763,(b) CIFAR10/ResNet18
DF,0.9448818897637795,"0
25
50
75
100
125
150
175
200
epochs 30 40 50 60 70 80"
DF,0.9456692913385827,Validation Accuracy %
DF,0.9464566929133859,"SGDM
RST-AM"
DF,0.947244094488189,(c) CIFAR10/WRN16-4
DF,0.9480314960629921,"0
25
50
75
100
125
150
175
200
epochs 25 30 35 40 45 50"
DF,0.9488188976377953,Adv-Validation Accuracy %
DF,0.9496062992125984,"SGDM
RST-AM"
DF,0.9503937007874016,(d) CIFAR10/WRN16-4
DF,0.9511811023622048,"0
25
50
75
100
125
150
175
200
epochs 10 20 30 40 50"
DF,0.9519685039370078,Validation Accuracy %
DF,0.952755905511811,"SGDM
RST-AM"
DF,0.9535433070866142,(e) CIFAR100/ResNet18
DF,0.9543307086614173,"0
25
50
75
100
125
150
175
200
epochs 5 10 15 20 25"
DF,0.9551181102362205,Adv-Validation Accuracy %
DF,0.9559055118110236,"SGDM
RST-AM"
DF,0.9566929133858267,(f) CIFAR100/ResNet18
DF,0.9574803149606299,"0
25
50
75
100
125
150
175
200
epochs 10 20 30 40 50 60"
DF,0.9582677165354331,Validation Accuracy %
DF,0.9590551181102362,"SGDM
RST-AM"
DF,0.9598425196850394,(g) CIFAR100/DenseNet121
DF,0.9606299212598425,"0
25
50
75
100
125
150
175
200
epochs 5 10 15 20 25 30"
DF,0.9614173228346456,Adv-Validation Accuracy %
DF,0.9622047244094488,"SGDM
RST-AM"
DF,0.962992125984252,(h) CIFAR100/DenseNet121
DF,0.9637795275590552,"Figure 12: Clean accuracy and PGD-10 attacked accuracy on the validation set in training different
neural networks."
DF,0.9645669291338582,Published as a conference paper at ICLR 2022
DF,0.9653543307086614,"D.2.6
EXPERIMENTS ON TRAINING A GENERATIVE ADVERSARIAL NETWORK"
DF,0.9661417322834646,"We describe our setting of training a generative adversarial network (GAN) here. Like the adver-
sarial training, the GAN training process is also a min-max problem. The stability of an optimizer
is critical for the training process. To demonstrate the applicability of RST-AM, we conducted
experiments on a GAN which was equipped with spectral normalization (Miyato et al., 2018) (SN-
GAN). The experimental setting was the same as that of AdaBelief (Zhuang et al., 2020): the dataset
was CIFAR-10; ResNets were used as the generator and the discriminator networks; the steps for
optimization in the discriminator and the generator per iteration were 5 and 1, respectively; the min-
batch size was 64 and the total iteration number was 100000. The Frechet Inception Distance (FID)
(Heusel et al., 2017) was used as the evaluation metric: lower FID score means better accuracy."
DF,0.9669291338582677,"The baseline optimizer were Adam and AdaBelief as they perform well in this task (Zhuang et al.,
2020). We also used the recommended hyperparameter settings for the two optimizers. For our
RST-AM method, due to the ill-conditioning of the min-max problem, we used the AdaBelief as the
preconditioner and set the damping parameter αk = 0.6. The regularization parameters c1 = 1 and
c2 = 1 × 10−7 were still kept unchanged just as the previous experiments."
DF,0.9677165354330709,"0
20000
40000
60000
80000
100000
iteration 15 20 25 30 35 40 45"
DF,0.968503937007874,FID score
DF,0.9692913385826771,"Adam
AdaBelief
RST-AM"
DF,0.9700787401574803,Figure 13: FID score for training SN-GAN on CIFAR-10.
DF,0.9708661417322835,"In Figure 13, we show the curve of FID score for each optimizer, which is the average of three
independent runs. It indicates that the RST-AM method is stable for this min-max optimization
problem."
DF,0.9716535433070866,Table 9: The effect of αk for RST-AM.
DF,0.9724409448818898,"Method
Adam
AdaBelief
α = 0.8
α = 0.6
α = 0.5
α = 0.4
α = 0.2
α = 0.1"
DF,0.9732283464566929,"FID score
13.07
12.80
12.48
12.05
12.75
13.13
13.07
12.59"
DF,0.974015748031496,"Since we only tuned the damping parameter αk for RST-AM, we report the FID scores of other
choices of αk in Table 9 during our experiment. It shows that even with the suboptimal choices of
αk, e.g. αk = 0.1, 0.5, 0.8, RST-AM can still outperform the baselines."
DF,0.9748031496062992,"D.3
ADDITIONAL EXPERIMENTS"
DF,0.9755905511811024,"To further test the performance of our method in training neural networks on larger datasets or
different models, we conducted additional experiments of the image classiﬁcation task in ImageNet
(Deng et al., 2009) and the Transformer (Vaswani et al., 2017) based neural machine translation task
in the IWSTL14 DE-EN (Cettolo et al., 2014) dataset."
DF,0.9763779527559056,"D.3.1
EXPERIMENTS ON IMAGENET"
DF,0.9771653543307086,"We trained ResNet50 on ImageNet with SGDM and RST-AM. We used the built-in ResNet50 model
in PyTorch. We ran the tests of each optimizer with three random seeds and four GeForce RTX 2080"
DF,0.9779527559055118,Published as a conference paper at ICLR 2022
DF,0.978740157480315,"Ti GPUs were used for each test. The hyperparameters of SGDM were set as the recommended
setting in PyTorch. For RST-AM, the hyperparameters were kept the same as those in the CIFAR
experiments. The weight-decay was 1 × 10−4. The number of the total training epochs is 90. The
learning rate decay of SGD was at the 30th and 60th epochs. For RST-AM, since the experiments
on CIFAR show it can often converge faster to an acceptable solution than SGDM, we adopted the
early learning rate decay strategy recommended in (Zhang et al., 2019): decay the αk and βk for
RST-AM at the 30th, 50th and 70th epochs."
DF,0.9795275590551181,"Table 10: TOP 1 test accuracy (%) w.r.t. epoch, the best TOP1 test accuracy (%), and the cost. The
memory, per-epoch time and total time of SGDM are set as the units. The total time is the time to
ﬁrst achieve the accuracy ≥75.90%."
DF,0.9803149606299213,"Method
epoch = 72
epoch = 88
epoch = 90
best
memory
per-epoch time
total time"
DF,0.9811023622047244,"SGDM
75.75
75.90
75.81
75.93±.15
1.00
1.00
1.00
RST-AM
75.90
75.95
75.98
76.04±.06
1.06
1.01
0.83"
DF,0.9818897637795275,"In Table 10, we report the TOP1 accuracy in the validation dataset during training. Note that we also
report the epoch number for each optimizer to achieve the accuracy equal or exceeding 75.90%. It
shows RST-AM needs 72 epochs while SGDM needs 88 epochs. The curves of the training accuracy
and test accuracy are shown in Figure 14. The results suggest that RST-AM is still a competitive
optimizer in training a larger model in a larger dataset."
DF,0.9826771653543307,"0
20
40
60
80
epochs 10 20 30 40 50 60 70 80"
DF,0.9834645669291339,Train Accuracy %
DF,0.984251968503937,"SGDM
RST-AM"
DF,0.9850393700787402,(a) Train accuracy
DF,0.9858267716535433,"0
20
40
60
80
epochs 10 20 30 40 50 60 70"
DF,0.9866141732283464,Test Accuracy %
DF,0.9874015748031496,"SGDM
RST-AM"
DF,0.9881889763779528,"50
60
70
80
90
74 75 76 77 75.81 75.98"
DF,0.988976377952756,(b) Test accuracy
DF,0.989763779527559,Figure 14: Train and test accuracy for training ImageNet/ResNet50.
DF,0.9905511811023622,"D.3.2
TRAINING TRANSFORMER"
DF,0.9913385826771653,"We conducted the neural machine translation task with Transformer. We implemented our RST-AM
method and integrated it into the fairseq framework 6. The basic experimental setting was set as
the recommended setting in (Yao et al., 2021). We trained the model for 50 epochs and the BLEU
(Papineni et al., 2002) score was calculated using the average model of the last ﬁve checkpoints. We
added a baseline optimizer RAdam (Liu et al., 2019) which was inspired by the warmup procedure
in training Transformer."
DF,0.9921259842519685,Table 11: The BLEU score of training Transformer on IWSLT14.
DF,0.9929133858267717,"Method
SGD
Adam
AdaBelief
RAdam
RST-AM"
DF,0.9937007874015747,"BLEU score
28.14±.08
35.71±.03
35.15±.14
35.60±.06
35.89±.02"
DF,0.9944881889763779,6 https://github.com/pytorch/fairseq.
DF,0.9952755905511811,Published as a conference paper at ICLR 2022
DF,0.9960629921259843,"The numerical results reported in Table 11 show that Adam is well-suited for this neural machine
translation task, though it does not perform well in the image classiﬁcation task. Also, the results
demonstrate that RST-AM can still outperform Adam in this task."
DF,0.9968503937007874,"Table 12:
BLEU score evaluated at the 40/45/50-th epoch, and the cost. The memory, per-epoch
time and total time of Adam are set as the units. The total time is the time of RST-AM to achieve
a BLEU score matching the ﬁnal BLEU of Adam: | BLEU(RST-AM) −BLEU(Adam) | ≤0.03,
where 0.03 is the standard deviation of the results of Adam."
DF,0.9976377952755906,"Method
epoch = 40
epoch = 45
epoch = 50
memory
per-epoch time
total time"
DF,0.9984251968503937,"Adam
35.42±.10
35.54±.08
35.71±.03
1.00
1.00
1.00
RST-AM
35.59±.14
35.69±.06
35.89±.02
1.16
1.00
0.90"
DF,0.9992125984251968,"In Table 12, we report the BLEU scores evaluated in the test dataset at the 40th, 45th and 50th epochs.
The results show the better performance of RST-AM over Adam and the per-epoch computational
cost is nearly the same. To achieve a comparable solution to Adam, RST-AM can save 10% training
time."
