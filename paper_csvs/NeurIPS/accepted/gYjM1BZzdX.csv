Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0037593984962406013,"Topological Data Analysis (TDA) provides a pipeline to extract quantitative topo-
logical descriptors from structured objects. This enables the definition of topo-
logical loss functions, which assert to what extent a given object exhibits some
topological properties. These losses can then be used to perform topological op-
timization via gradient descent routines. While theoretically sounded, topological
optimization faces an important challenge: gradients tend to be extremely sparse,
in the sense that the loss function typically depends on only very few coordinates
of the input object, yielding dramatically slow optimization schemes in practice.
Focusing on the central case of topological optimization for point clouds, we pro-
pose in this work to overcome this limitation using diffeomorphic interpolation,
turning sparse gradients into smooth vector fields defined on the whole space,
with quantifiable Lipschitz constants. In particular, we show that our approach
combines efficiently with subsampling techniques routinely used in TDA, as the
diffeomorphism derived from the gradient computed on a subsample can be used
to update the coordinates of the full input object, allowing us to perform topo-
logical optimization on point clouds at an unprecedented scale. Finally, we also
showcase the relevance of our approach for black-box autoencoder (AE) regular-
ization, where we aim at enforcing topological priors on the latent spaces associ-
ated to fixed, pre-trained, black-box AE models, and where we show that learning
a diffeomorphic flow can be done once and then re-applied to new data in linear
time (while vanilla topological optimization has to be re-run from scratch). More-
over, reverting the flow allows us to generate data by sampling the topologically-
optimized latent space directly, yielding better interpretability of the model."
INTRODUCTION,0.007518796992481203,"1
Introduction"
INTRODUCTION,0.011278195488721804,"Persistent homology (PH) is a central tool of Topological Data Analysis (TDA) that enables the
extraction of quantitative topological information (such as, e.g., the number and sizes of loops, con-
nected components, branches, cavities, etc) about structured objects (such as graphs, times series or"
INTRODUCTION,0.015037593984962405,"∗Part of this work was done when MT was doing an internship at the Laboratoire d’Informatique Gaspard
Monge and student at Université Paris-Saclay."
INTRODUCTION,0.018796992481203006,"point clouds sampled from, e.g., submanifolds), summarized in compact descriptors called persis-
tence diagrams (PDs). PDs were initially used as features in Machine Learning (ML) pipelines; due
to their strong invariance and stability properties, they have been proved to be powerful descriptors
in the context of classification of time series [42, 19], graphs [5, 23, 24], images [2, 25, 16], shape
registration [7, 6, 36], or analysis of neural networks [22, 3, 26], to name a few."
INTRODUCTION,0.022556390977443608,"Another active line or research at the crossroad of TDA and ML is (persistence-based) topological
optimization, where one wants to modify an object X so that it satisfies some topological constraints
as reflected in its persistence diagram Dgm(X). The first occurrence of this idea appears in [21],
where one wants to deform a point cloud X ∈Rn×d so that Dgm(X) becomes as close as possible
(w.r.t. an appropriate metric denoted by W) to some target diagram Dtarget, hence yielding to the
problem of minimizing X 7→W(Dgm(X), Dtarget). This idea has then been revisited with different
flavors, for instance by adding topology-based terms in standard losses in order to regularize ML
models [14, 33, 29], improving ML model reconstructions by explicitly accounting for topological
features [16], or improving correspondences between 3D shapes by forcing matched regions to have
similar topology [36]. Formally, this goes through the minimization of an objective function"
INTRODUCTION,0.02631578947368421,"L : X 7→ℓ(Dgm(X)) ∈R,"
INTRODUCTION,0.03007518796992481,"where ℓis a user-chosen loss function that quantifies to what extent Dgm(X) reflects some pre-
scribed topological properties inferred from X. Under mild assumptions (see Section 2), the map
L is differentiable generically and its gradients are obtained as a byproduct of the computation of
Dgm(X). However, these approaches are limited in practice by two major issues: (i) the com-
putation of X 7→Dgm(X) scales poorly with the size of X (e.g., number of points n in a point
cloud, number of nodes in a graph, etc), and (ii) the gradient ∇L(X) tends to be very sparse: if
X = (x1, . . . , xn) ∈Rn×d is a point cloud, ∇L(X)i ̸= 0 for only very few indices i ∈{1, . . . , n}
(the corresponding points are called the critical points of the topological gradient, see Section 2.1)."
INTRODUCTION,0.03383458646616541,"Related works.
Several articles have studied topological optimization in the TDA literature. The
standard, or vanilla, framework to define and study gradients obtained from topological losses was
described in [4, 28], where the high sparsity and long computation times were first identified. To
mitigate this issue, the authors of [34] introduced the notion of critical set that extends the usually
sparse set of critical points in order to get a gradient-like object that would update more points in X.
In [39], the authors used an average of the vanilla topological gradients of several subsamples to get
a denser and faster gradient. On the theoretical side, the authors of [27] demonstrated that adapting
the stratified structure induced by PDs to the gradient definition enables faster convergence."
INTRODUCTION,0.03759398496240601,"Limitations.
Despite proposing interesting ways to accelerate gradient descent, the approaches
mentioned above are still limited in the sense that their proposed gradients are not defined on the
whole space, but only on a sparse subset of the current observation X, which still prevents their
use in different contexts, that we investigate in our experiments (Section 4). First, when the data
has more than tens of thousands of points, the number of subsamples needed to capture relevant
topological structures (when using [39]), as well as the critical set computations (when using [34]),
both become practically infeasible. Second, when optimizing the topology of datasets obtained as
latent spaces of a black-box autoencoder model (i.e., an autoencoder with forbidden access to its
architecture, parameters, and training), then (a) the topological gradients of [39, 34] cannot be re-
used to process new such datasets, and topological optimization has to be performed from scratch
every time that new data comes in, (b) this also impedes their transferability, as re-running gradient
descent every time makes it very difficult to guarantee some stability for the final output, and finally
(c) one cannot generate new data by sampling the optimized latent spaces directly, as it would
require to apply the sequence of reverted gradients (which are not well-defined everywhere)."
INTRODUCTION,0.041353383458646614,"Contributions and Outline.
In this article, we propose to replace the standard gradient ∇L(X)
of (??) derived formally by a diffeomorphism v : Rd →Rd that interpolates ∇L(X) on its non-zero
entries, that is v(xi) = ∇L(X)i for all i ∈I := {j | ∇L(X)j ̸= 0} and is, in some sense, as smooth
as possible. More precisely, our contribution is three-fold:"
INTRODUCTION,0.045112781954887216,"• We introduce a diffeomorphic interpolation of the vanilla topological gradient, which ex-
tends this gradient to a smooth and denser vector field defined on the whole space Rd, and
which is able to move a lot more points in X at each iteration,"
INTRODUCTION,0.04887218045112782,"t = 0
t = 1
t = 2
t = 3
t = 5
births"
INTRODUCTION,0.05263157894736842,"deaths t t 0
1 2 2 3 5"
INTRODUCTION,0.05639097744360902,"Figure 1: Illustration of the Vietoris-Rips filtration on a point cloud in Rd, focusing on one-dimensional topo-
logical features (loops). When the filtration parameter t increases, loops appear and disappear in the filtration.
These values are accounted in the resulting persistence diagram (right)."
INTRODUCTION,0.06015037593984962,"• We prove that its updates indeed decrease topological losses, and we quantify its smooth-
ness by upper bounding its Lipschitz constant (again, in the context of topological losses),
• We showcase its practical efficiency: we show that it compares favorably to the main base-
line [34] in terms of convergence speed, that its combination with subsampling [39] allows
to process datasets whose sizes are currently out of reach in TDA, and that it can success-
fully be used for the tasks mentioned above concerning black-box autoencoder models."
INTRODUCTION,0.06390977443609022,"Section 2 provides necessary background in Topological Data Analysis and diffeomorphic interpo-
lations. Section 3 presents our approach and its corresponding guarantees, and Section 4 showcases
our experiments. Limitations and further research directions are discussed in Section 5."
BACKGROUND,0.06766917293233082,"2
Background"
TOPOLOGICAL DATA ANALYSIS,0.07142857142857142,"2.1
Topological Data Analysis"
TOPOLOGICAL DATA ANALYSIS,0.07518796992481203,"In this section, we recall the basic materials of Topological Data Analysis (TDA), and refer the
interested reader to [20, 35] for a thorough overview. We restrict the presentation to our case of
interest: extracting topological information from a point cloud using the standard Vietoris-Rips (VR)
filtration. A more extensive presentation of the TDA machinery is provided in Appendix A."
TOPOLOGICAL DATA ANALYSIS,0.07894736842105263,"Let X = (x1, . . . , xn) ∈Rn×d. The Vietoris-Rips filtration consists of building an increasing
sequence of simplicial complexes (Kt)t≥0 over X by inserting a simplex σ = (xi1, . . . , xip) when-
ever ∀j, j′ ∈{1, . . . , p}, ∥xij −xij′∥≤t. Each time a simplex σ is inserted, it either creates a
topological feature (e.g., inserting a face creates a cavity, that is a 2-dimensional topological feature)
or destroy a pre-existing feature (e.g., the face insertion fills a loop, that is a 1-dimensional feature,
making it topologically trivial). The persistent homology machinary tracks the apparition and de-
struction of such features in the so-called persistence diagram (PD) of X, denoted by Dgm(X).
Therefore, Dgm(X) is a set of points in R2 of the form (tb, td) with td ≥tb, where each such point
accounts for the presence of a topological feature inferred from X that appeared at time tb following
the insertion of an edge (xi1, xi2) with ∥xi1 −xi2∥= tb and disappeared at time td following the
insertion of an edge (xi3, xi4) with ∥xi3 −xi4∥= td. Figure 1 illustrates this construction. From
a computational standpoint, computing the VR diagram of X ∈Rn×d that would reflect topolog-
ical features of dimension d′ ≤d runs in O(nd′+2), making the computation quickly unpractical
when n increases, even when restricting to low-dimensional features such as connected components
(d′ = 0), loops (d′ = 1) or cavities (d′ = 2)."
TOPOLOGICAL DATA ANALYSIS,0.08270676691729323,"Topological optimization.
PDs are made to be used in downstream pipelines, either as static
features (e.g., for classification purposes) or as intermediate representations of X in optimization
schemes. In this work, we focus on the second problem. We formally consider the minimization of
objective functions of the form"
TOPOLOGICAL DATA ANALYSIS,0.08646616541353383,"L : X ∈Rn×d 7→ℓ(Dgm(X)) ∈R.
(1)"
TOPOLOGICAL DATA ANALYSIS,0.09022556390977443,"Here, ℓrepresents a loss function taking value from the space of PDs, denoted by D in the following.
The space D can be equipped with a canonical metric, denoted by W and whose formal definition
is not required in this work, for which a central result is that the map X 7→Dgm(X) is stable
(Lipschitz continuous) [17, 18, 38]. Therefore, if ℓis Lipschitz continuous, so is L, hence it admits
a gradient almost everywhere by Rademacher theorem. Building on these theoretical statements, one"
TOPOLOGICAL DATA ANALYSIS,0.09398496240601503,"can consider the “vanilla” gradient descent update Xk+1 := Xk −λ∇L(Xk) for a given learning-
rate λ > 0 and iterate it in order to minimize (1). Theoretical properties of this seminal scheme (and
natural extensions, e.g., stochastic gradient descent) have been studied in [4, 27], where convergence
(to a local minimum of L) is proved."
TOPOLOGICAL DATA ANALYSIS,0.09774436090225563,"From a computational perspective, deriving ∇L(X) comes in two steps. Let µ := Dgm(X), written
as µ = {(bi, di) | i ∈I} for some finite set of indices I. To each i ∈I correspond four (possibly
coinciding) points xi1, xi2, xi3, xi4 in the input point cloud X. Intuitively, minimizing µ 7→ℓ(µ)
boils down to prescribe a descent direction (δbi, δdi) ∈R2 to each (bi, di) for i ∈I, where δbi =
∂ℓ
∂bi (µ) and δdi =
∂ℓ
∂di (µ). Backpropagating this perturbation to X will move the corresponding
points xi1, xi2, xi3, xi4 in order to increase or decrease the distances ∥xi1 −xi2∥= bi and ∥xi3 −
xi4∥= di accordingly. This yields to the following formula: ∂L"
TOPOLOGICAL DATA ANALYSIS,0.10150375939849623,"∂x (X) =
X"
TOPOLOGICAL DATA ANALYSIS,0.10526315789473684,"i, x→(bi,di)  ∂ℓ"
TOPOLOGICAL DATA ANALYSIS,0.10902255639097744,"∂bi
· ∂bi"
TOPOLOGICAL DATA ANALYSIS,0.11278195488721804,∂x + ∂ℓ
TOPOLOGICAL DATA ANALYSIS,0.11654135338345864,"∂di
· ∂di ∂x"
TOPOLOGICAL DATA ANALYSIS,0.12030075187969924,"
(X),
(2)"
TOPOLOGICAL DATA ANALYSIS,0.12406015037593984,"where the notation x →(bi, di) means that x ∈X appears in (at least) one of the four points yielding
the presence of (bi, di) in the diagram µ = Dgm(X). A fundamental contribution of [28, §3.3] is
to prove that the chain rule formula (2) is indeed valid2. Most of the time, a point x ∈X will not
belong to any critical pair (σb, σd) and the above gradient coordinate is 0. Therefore, the gradient of
L depends on very few points of X, yielding the sparsity phenomenon discussed in Section 1."
TOPOLOGICAL DATA ANALYSIS,0.12781954887218044,"Examples of common topological losses.
Let X = (x1, . . . , xn) ∈Rn×d be a point cloud and
Dgm(X) = {(bi, di) | i ∈I} be its PD. There are several natural losses that have been introduced
in the TDA literature:"
TOPOLOGICAL DATA ANALYSIS,0.13157894736842105,• Topological simplification losses: typically of the form P
TOPOLOGICAL DATA ANALYSIS,0.13533834586466165,"i∈˜I(bi −di)2, where ˜I ⊆I.
Such losses push (some of the) points in Dgm(X) toward the diagonal ∆= {b = d},
hence destroying the corresponding topological features appearing in X.
• Topological augmentation losses [4]: similar to simplification losses, but typically attempt-
ing to push points in Dgm(X) away from ∆, i.e., minimizing −P"
TOPOLOGICAL DATA ANALYSIS,0.13909774436090225,"i∈˜I(bi −di)2, to make
topological features of X more salient. As such losses are not coercive, they are usually
coupled with regularization terms to prevent points in X going to infinity.
• Topological registration losses [21]:
given a target diagram Dtarget, one minimizes
W(Dgm(X), Dtarget) where W denotes a standard metric between persistence diagrams.
This loss attempts to modify X so that it exhibits a prescribed topological structure."
DIFFEOMORPHIC INTERPOLATIONS,0.14285714285714285,"2.2
Diffeomorphic interpolations"
DIFFEOMORPHIC INTERPOLATIONS,0.14661654135338345,"In order to overcome the sparsity of gradients appearing in TDA, we rely on diffeomorphic interpo-
lations (see, e.g., [43, Chapter 8]). Let X = (x1, . . . , xn) ∈Rn×d, let I ⊆{1, . . . , n} denote the set
of indices on which ∇L(X) is non-zero and let ai := (∇L(X))i ∈Rd for i ∈I. Our goal is to find a
smooth vector field ˜v : Rd →Rd such that, for all i ∈I, ˜v(xi) = ai. To formalize this, we consider
a Hilbert space H ⊂(Rd)Rd for which the map δα
x : f 7→⟨α, f(x)⟩Rd = αT f(x) is continuous for
any (α, x) ∈Rd × Rd. Such a space is called a Reproducing Kernel Hilbert Space (RKHS)3. A cru-
cial property is that there exists a matrix-valued kernel operator K : Rd×Rd →Rd×d whose outputs
are symmetric and positive definite, and related to H through the relation ⟨kα
x, kβ
y ⟩H = αT K(x, y)β
for all x, y, α, β ∈Rd, where kα
x ∈H is the unique vector provided by the Riesz representation
theorem such that ⟨kα
x, f⟩= ⟨α, f(x)⟩. Conversely, any such kernel K induces a (unique) RKHS
H (of which K is the reproducing kernel). Now, we can consider the following problem:"
DIFFEOMORPHIC INTERPOLATIONS,0.15037593984962405,"minimize ∥v∥H, s.t. v(xi) = ai, ∀i ∈I,
(3)"
DIFFEOMORPHIC INTERPOLATIONS,0.15413533834586465,"that is, we are seeking for the smoothest (lowest norm) element of H that solves our interpolation
problem. The solution ˜v of this problem is the projection of 0 onto the affine set {v ∈H | v(xi) ="
DIFFEOMORPHIC INTERPOLATIONS,0.15789473684210525,"2This is not trivial, because the intermediate space D is only a metric space.
3In many applications, RKHS are restricted to spaces of functions valued in R or C, but the theory adapts
faithfully to the more general setting of vector-valued maps."
DIFFEOMORPHIC INTERPOLATIONS,0.16165413533834586,"ai, ∀i ∈I}. Observe that ˜v belongs to the orthogonal of {v ∈H | v(xi) = 0, ∀i ∈I}, and thus of
{v ∈H | ⟨kαi
xi , v⟩H = 0, ∀i ∈I, αi ∈Rd}, and therefore ˜v ∈span({kαi
xi | i ∈I}). This justifies to
search for ˜v in the form of ˜v(x) = P
i∈I K(x, xi)αi, and the interpolation that it must satisfy yields
˜v(x) = P"
DIFFEOMORPHIC INTERPOLATIONS,0.16541353383458646,"i∈I K(x, xi)(K−1a)i, where K is the block matrix (K(xi, xj))i,j∈I and a = (ai)i∈I.
See also [43, Theorem 8.8]. In particular, it is important to note that ˜v inherits from the regularity
of K and will typically be a diffeomorphism in this work. If K is the Gaussian kernel defined by
K(x, y) := exp

−∥x−y∥2"
DIFFEOMORPHIC INTERPOLATIONS,0.16917293233082706,"2σ2

Id for some bandwidth σ > 0, a choice to which we stick to in the rest
of this work, the expression of ˜v reduces to"
DIFFEOMORPHIC INTERPOLATIONS,0.17293233082706766,"˜v(x) =
X"
DIFFEOMORPHIC INTERPOLATIONS,0.17669172932330826,"i∈I
ρσ(∥x −xi∥)αi,
(4)"
DIFFEOMORPHIC INTERPOLATIONS,0.18045112781954886,where ρσ(u) := e−u2
DIFFEOMORPHIC INTERPOLATIONS,0.18421052631578946,"2σ2 , and αi := (K−1a)i with K = (ρσ(∥xi −xj∥)Id)i,j∈I. Note that ˜v
can be understood as the convolution of a with a Gaussian kernel, but involving a correction K−1
guaranteeing that after the convolution, the interpolation constraint is satisfied. We will call ˜v the
diffeomorphic interpolation associated to the vectors and indices {ai | i ∈I}."
DIFFEOMORPHIC INTERPOLATION OF THE VANILLA TOPOLOGICAL GRADIENT,0.18796992481203006,"3
Diffeomorphic interpolation of the vanilla topological gradient"
METHODOLOGY,0.19172932330827067,"3.1
Methodology"
METHODOLOGY,0.19548872180451127,"1.0
0.5
0.0
0.5
1.0 1.0 0.5 0.0 0.5 1.0"
METHODOLOGY,0.19924812030075187,Point cloud
METHODOLOGY,0.20300751879699247,"L(X) (Vanilla)
v(X) (Diffeo)"
METHODOLOGY,0.20676691729323307,"Figure
2:
(blue)
A
point
cloud
X, and (black) the negative gradi-
ent −∇L(X) of a simplification loss
which aims at destroying the loop by
collapsing the circle (reduce the loop’s
death time) and tearing it (increase the
birth time). While ∇L(X) only affects
four points in X, the diffeomorphic in-
terpolation ˜v(X) (orange, σ = 0.1) is
defined on Rd, hence extends smoothly
to other points in X."
METHODOLOGY,0.21052631578947367,"We aim at minimizing a loss function L : X 7→ℓ(Dgm(X)) as
in (1), starting from some initialization X0, and assuming that
L is lower bounded (typically by 0) and locally semi-convex.
This assumption is typically satisfied by the topological losses
ℓintroduced in Section 2.1. Gradient descents implemented in
practice are (explicit) discretization of the gradient flow
dX"
METHODOLOGY,0.21428571428571427,"dt ∈−∂L(X(t)),
X(0) = X0,
(5)"
METHODOLOGY,0.21804511278195488,"where ∂L(X) := {v | L(Y ) ≥L(X) + v · (Y −X) + o(Y −
X) for all X, Y } denotes the subdifferential of L at X. Note
that a topological loss L is typically not differentiable every-
where, since the map X 7→Dgm(X) is differentiable almost
everywhere but not in C1,1. However, uniqueness of the gra-
dient flow on a maximal interval [0, +∞[ is guaranteed if L is
lower bounded and locally semi-convex [15, §B.1]."
METHODOLOGY,0.22180451127819548,"In this work, we propose to use the dynamic described by the
diffeomorphism ˜vt introduced in (4) interpolating the current
vanilla topological gradient ∇L(Xt) at each time t, formally
considering solutions ˜X of d ˜X"
METHODOLOGY,0.22556390977443608,"dt = −˜vt( ˜X(t)),
˜X(0) = X0.
(6)"
METHODOLOGY,0.22932330827067668,"Here, slightly overloading notation, ˜vt( ˜X(t)) denotes the n×d
matrix where the i-th line is given by ˜vt( ˜X(t)i). The flow at time T associated to (6) is the map"
METHODOLOGY,0.23308270676691728,"φT : x0 7→x0 −
Z T"
METHODOLOGY,0.23684210526315788,"0
˜vt(x(t))dt,
˙x(t) = −˜vt(x(t)), x(0) = x0,
(7)"
METHODOLOGY,0.24060150375939848,"which can inverted by simply following the flow backward (i.e., by following ˜vt instead of −˜vt). We
now guarantee that at each time t, following ˜vt instead of the vanilla topological gradient ∇L(Xt)
still provides a descent direction for the topological loss L."
METHODOLOGY,0.24436090225563908,"Proposition 3.1. For each t ≥0, it holds that dL( ˜
X(t))
dt
= −∥∇L( ˜X(t))∥2 ≤0."
METHODOLOGY,0.24812030075187969,"Proof. One has dL( ˜
X(t))
dt
= −⟨∇L( ˜X(t)), ˜vt( ˜X(t))⟩= −Pn
i=1(∇L( ˜X(t)))i · (˜vt( ˜X(t)))i. Since
∇L( ˜X(t))i = 0 for i ̸∈I, and ˜vt( ˜X(t))i = −∇L( ˜X(t))i for i ∈I, the result follows."
METHODOLOGY,0.2518796992481203,"Moreover, it is also possible to upper bound the smoothness, i.e., the Lipschitz constant, of ˜v:"
METHODOLOGY,0.2556390977443609,"Proposition 3.2. Let L be the simplification or augmentation loss computed with k = |˜I| PD points,
as defined at the end of Section 2.1. Let ˜v = ˜vt be the diffeomorphic interpolation associated to the
vanilla topological gradient at time t ≥0. Then, one has, ∀x, y ∈Rd and t ≥0:"
METHODOLOGY,0.2593984962406015,"∥˜v(x) −˜v(y)∥2 ≤∥˜v(x) −˜v(y)∥1 ≤Cd · σd−1 · κ(K) · Persk(Dgm( ˜X(t))) · ∥x −y∥2,"
METHODOLOGY,0.2631578947368421,"where Cd =
√"
METHODOLOGY,0.2669172932330827,d · 23+ d+1
METHODOLOGY,0.2706766917293233,"2
· π
d−1"
METHODOLOGY,0.2744360902255639,"2 , κ(K) is the condition number of K, and Persk(Dgm( ˜X(t))) is
the sum of the k largest distances to the diagonal in Dgm( ˜X(t))."
METHODOLOGY,0.2781954887218045,"The proof is deferred to Appendix B. This upper bound can be used to quantify how smooth the
diffeomorphic interpolation is (as characterized with its Lipschitz constant) based on the parameters
it is computed from. In the case of the Gaussian kernel, we found that our upper bound on the Lip-
schitz constant depends on the kernel bandwidth σ: indeed, the more spread the Gaussian function
is, the more critical points can influence other data points potentially far from them, introducing un-
wanted distortions and larger Lipschitz constant. Similarly, if the condition number κ(K) is large,
inverting the kernel matrix might introduce instabilities in Equation (4), and thus a larger Lipschitz
constant as well. Finally, the total persistence also appears, as the more PD points one has to opti-
mize, the more critical pairs will appear, and thus the more constrained Equation (3) is, leading to
more complex diffeomorphic interpolation solutions with larger Lipschitz constants."
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.2819548872180451,"3.2
Subsampling techniques to scale topological optimization"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.2857142857142857,"As a consequence of the limited scaling of the Vietoris-Rips filtration with respect to the number of
points n of the input point cloud X, it often happens in practical applications that computing the
VR diagram Dgm(X) of a large point set X (a fortiori its gradient) turns out to be intractable. A
natural workaround is to randomly sample s-points from X, with s ≪n, yielding a smaller point
cloud X′ ⊂X. Provided that the Hausdorff distance between X′ and X is small, the stability
theorem [18, 17, 8] ensures that Dgm(X′) is close to Dgm(X). See [11, 12, 9] for an overview of
subsampling methods in TDA."
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.2894736842105263,"However, the sparsity of vanilla topological gradients computed from topological losses strikes fur-
ther when relying on subsampling: only a tiny fraction of the seminal point cloud X is likely to be
updated at each gradient step. In contrast, using the diffeomorphic interpolation ˜v (of the vanilla
topological gradient) computed on the subsample X′ still provides a vector field defined on the
whole input space Rd, in particular on each point of X and the update can then be performed in
linear time with respect to n. This yields Algorithm 1. Figure 3 illustrates the qualitative benefits
offered by the joint use of subsampling and diffeomorphic interpolations when compared to vanilla
topological gradients. A larger-scale experiment is provided in Section 4."
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.2932330827067669,Algorithm 1 Diffeomorphic gradient descent for topological loss functions with subsampling
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.29699248120300753,"Input: Initial X0 ∈Rn×d, loss function ℓ, learning rate λ > 0, subsampling size s ∈{1, . . . , n},
max. epoch T ≥1, stopping criterion.
Set L : X 7→ℓ(Dgm(X)) (+ possibly a regularization term in X).
for k = 1, . . . , T do"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.3007518796992481,"Subsample X′
k−1 = {x′
1, . . . , x′
s} uniformly from Xk−1.
Compute ∇L(X′
k−1) (vanilla topological gradient)
Compute the diffeomorphic interpolation ˜v(X′
k−1) from ∇L(X′
k−1) using (4).
Set Xk := Xk−1 −λ˜v(Xk−1).
if stopping criterion is reached then"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.30451127819548873,"Return Xk
end if
end for
Return XT"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.3082706766917293,"Stopping criterion.
A natural stopping criterion for Algorithm 1 is to assess whether the loss
L(Xt) = ℓ(Dgm(Xt)) is smaller than some ε > 0. However, computing Dgm(Xt) can be in-
tractable if Xt is large. Therefore, a tractable loss to consider is ˆL(Xt) := E[ℓ(Dgm(X′
t))], where"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.31203007518796994,"(a) t = 0
(b) t = 100 (Vanilla)
(c) t = 500 (Vanilla)
(d) t = 100 (Diffeo)"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.3157894736842105,"Figure 3: Showcase of the usefulness of subsampling combined with diffeomorphic interpolations to minimize
a topological simplification loss, with parameters λ = 0.1, s = 50, n = 500. (a) Initial point cloud X (blue),
subsample X′ (red), vanilla topological gradient on the subsample (black) and corresponding diffeomorphic
interpolation (orange). (b) and (c), the point cloud Xt after running t = 100 and t = 500 steps of vanilla
gradient descent. (d) the point cloud Xt after running t = 100 steps of diffeomorphic gradient descent."
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.31954887218045114,"X′
t is a uniform s-sample from Xt. Under that perspective, Algorithm 1 can be re-interpreted
as a kind of stochastic gradient descent on ˆL, for which two standard stopping criteria can be
used: (a) compute an exponential moving average of the loss on individual samples X′
t over
iterations, or (b) compute a validation loss, i.e., sample X′
t,(1), . . . , X′
t,(K) and estimate ˆL by"
SUBSAMPLING TECHNIQUES TO SCALE TOPOLOGICAL OPTIMIZATION,0.3233082706766917,"K−1 PK
k=1 ℓ(Dgm(X′
t,(k))). Empirically, we observe that the latter approach with K = n/s (more
repetitions for smaller sample sizes to mitigate variance) yields the most satisfactory results (faster
convergence toward a better objective Xt) overall, and thus stick to this choice in our experiments."
NUMERICAL EXPERIMENTS,0.32706766917293234,"4
Numerical experiments"
NUMERICAL EXPERIMENTS,0.3308270676691729,"We provide numerical evidence for the strength of our diffeomorphic interpolations.
PH-
related computations relies on the library Gudhi [40] and automatic differentiation relies on
tensorflow [1]. The “big-step gradient” baseline [34] implementation is based on oineus4. The
first two experiments were run on a 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz,
the last one on a 2x Xeon SP Gold 5115 @ 2.40GHz. Our code is publicly available at https:
//github.com/tlacombe/topt."
NUMERICAL EXPERIMENTS,0.33458646616541354,"Convergence speed and running times.
We sample uniformly n = 200 points on a unit circle
in R2 with some additional Gaussian noise, and then minimize the simplification loss L : X 7→
P"
NUMERICAL EXPERIMENTS,0.3383458646616541,"(b,d)∈Dgm(X) |d|2, which attempts to destroy the underlying topology in X by reducing the death
times of the loops by collapsing the points. The respective gradient descents are iterated over a
maximum of 250 epochs, possibly interrupted before if a loss of 0 is reached (Dgm(X) is empty),
with a same learning rate λ = 0.1. The bandwidth of the Gaussian kernel in (4) is set to σ = 0.1. We
include the competitor oineus [34], as—even though relying on a fairly different construction—this
method shares a key idea with ours: extending the vanilla gradient to move more points in X. We
stress that both approaches can be used in complementarity: compute first the “big-step gradient”
of [34] using oineus, and then extend it by diffeomorphic interpolation. Results are displayed in
Figure 45. In terms of loss decrease over iterations, both “big-step gradients"" and our diffeomorphic
interpolations significantly outperform vanilla topological gradients, and their combined use yields
the fastest convergence (by a slight margin over our diffeomorphic interpolations alone). However,
in terms of raw running times, the use of oineus involves a significant computational overhead,
making our approach the fastest to reach convergence by a significant margin."
NUMERICAL EXPERIMENTS,0.34210526315789475,"Subsampling.
We now showcase how using our diffeomorphic interpolation jointly with subsam-
pling routines (Algorithm 1) allows to perform topological optimization on point clouds with thou-
sands of points, a new scale in the field. For this, we consider the vertices of the Stanford Bunny
[41], yielding a point cloud X0 ∈Rn×d with n = 35, 947 and d = 3. We consider a topolog-"
NUMERICAL EXPERIMENTS,0.3458646616541353,"4https://github.com/anigmetov/oineus
5Note that the loss computed with oineus has not been normalized in the figure, which is why its values are
larger than the others. This has no influence over its minimum number of iterations needed to reach 0 though."
NUMERICAL EXPERIMENTS,0.34962406015037595,"1.0
0.5
0.0
0.5
1.0 1.0 0.5 0.0 0.5 1.0 Init."
NUMERICAL EXPERIMENTS,0.3533834586466165,"1.0
0.5
0.0
0.5
1.0 1.0 0.5 0.0 0.5 1.0"
NUMERICAL EXPERIMENTS,0.35714285714285715,Vanilla output
NUMERICAL EXPERIMENTS,0.3609022556390977,"1.0
0.5
0.0
0.5
1.0 2 1 0 1"
NUMERICAL EXPERIMENTS,0.36466165413533835,Diffeo output
NUMERICAL EXPERIMENTS,0.3684210526315789,"1.0
0.5
0.0
0.5
1.0 1.0 0.5 0.0 0.5 1.0"
NUMERICAL EXPERIMENTS,0.37218045112781956,Oineus output
NUMERICAL EXPERIMENTS,0.37593984962406013,"0.5
0.0
0.5
1.0 6 4 2 0 2 4"
NUMERICAL EXPERIMENTS,0.37969924812030076,6Oineus+Diffeo output
NUMERICAL EXPERIMENTS,0.38345864661654133,"Figure 4: (Top) From left to right: initial point cloud, and final point cloud for the different flows. (Bottom)
Evolution of the losses with respect to the number of iterations and with respect to running time."
NUMERICAL EXPERIMENTS,0.38721804511278196,"0
200
400
600
800
1000
Epoch 0.008 0.007 0.006 0.005 0.004 0.003 0.002 0.001 0.000 Loss"
NUMERICAL EXPERIMENTS,0.39097744360902253,Evolution of loss over iterations
NUMERICAL EXPERIMENTS,0.39473684210526316,"Vanilla
Diffeo"
NUMERICAL EXPERIMENTS,0.39849624060150374,"Figure 5: From left to right: initial Stanford bunny X0, the point cloud after 1, 000 epochs of vanilla topological
gradient descent (barely any changes), the point cloud after 200 epochs of diffeomorphic gradient descent, after
1,000 epochs, and eventually the evolution of losses for both methods over iterations."
NUMERICAL EXPERIMENTS,0.40225563909774437,"ical augmentation loss (see Section 2.1) for two-dimensional topological features, i.e., we aim at
increasing the persistence of the bunny’s cavity. The size of n makes the computation of Dgm(X0)
untractable (recall that it scales in O(n4)); we thus rely on subsampling with sample size s = 100
and compare the vanilla gradient descent scheme and our scheme described in Algorithm 1. Results
are displayed in Figure 5. Because it only updates a tiny fraction of the initial point cloud at each
iteration, the vanilla topological gradient with subsampling barely changes the point cloud (nor de-
creases the loss) in 1,000 epochs. In sharp contrast, as our diffeomorphic interpolation computed on
subsamples is defined on R3, it updates the whole point cloud at each iteration, making possible to
decrease the objective function where the vanilla gradient descent is completely stuck. Note that a
step of diffeomorphic interpolation, in that case, takes about 10 times longer than a vanilla step. An
additional subsampling experiment can be found in Appendix C."
NUMERICAL EXPERIMENTS,0.40601503759398494,"Black-box autoencoder models.
Next, we apply our diffeomorphic interpolations to black-box
autoencoder models. In their simplest formulation, autoencoders (AE) can be summarized as two
maps E : Rd →Rd′ and D : Rd′ →Rd called encoder and decoder respectively. The intermediate
space Rd′ in which the encoder E is valued is referred to as a latent space (LS), with typically
d′ ≪d. In general, without further care, there is no reason to expect that the LS of a point cloud X,
E(X) = {E(x1), . . . , E(xn)}, reflects any geometric or topological properties of X. While this
can be mitigated by adding a topological regularization term to the loss function during the training
of the autoencoder [29, 4], this cannot work in the setting where one is given a black-box, pre-
trained AE. However, replacing (E, D) by (φ ◦E, D ◦φ−1) for any invertible map φ : Rd′ →Rd′"
NUMERICAL EXPERIMENTS,0.40977443609022557,"yields an AE producing the same outputs yet changing the LS E(X), without explicit access to
the AE’s model. Hence, we propose to learn such a φ with diffeomorphic interpolations: given
some latent space E(X), we apply T steps of our diffeomorphic gradient descent algorithm to X 7→
ℓ(Dgm(X)) initialized at E(X). We thus get a sequence of smooth displacements −˜v1, . . . , −˜vT of
Rd′ that discretizes the flow (7) via φ : x0 7→x0−PT
k=1 ˜vk(xk−1) where xk−xk−1 = −˜vk(xk−1),
and such that Dgm(φ(E(X))) is more topologically satisfying. This fixed diffeomorphism φ can"
NUMERICAL EXPERIMENTS,0.41353383458646614,"Figure 7:
COIL images, their corresponding initial LSs in blue and final LSs obtained with diffeomorphic
gradient descent in orange, and the corresponding topological losses, for both vase (left) and duck (right)."
NUMERICAL EXPERIMENTS,0.41729323308270677,"then be re-applied to any new data coming out of the encoder in a deterministic way. Moreover, any
random sample from the topologically-optimized LS can be inverted without further computations
by following ˜vT , ˜vT −1, . . . , ˜v1, which allows to push the new sample back to the initial LS, and then
apply the decoder on it. Again, this cannot be achieved with baselines [39, 34]."
NUMERICAL EXPERIMENTS,0.42105263157894735,"In order to illustrate these properties, we trained a variational autoencoder (VAE) to project a family
of datasets of images representing rotating objects, named COIL [32], to two-dimensional latent
spaces. Given that every dataset in this family is comprised of 288 pictures of the same object taken
with different angles, one can impose a prior on the topology of the corresponding LSs, namely that
they are sampled from circles. However, the VAE architecture is shallow (the encoder has one fully-
connected layer (100 neurons), and the decoder has two (50 and 100 neurons), all layers use ReLu
activations), and thus the learned latent spaces, although still looking like curves thanks to continuity,
do not necessarily display circular patterns. This makes generating new data more difficult, as latent
spaces are harder to interpret. To improve on this, we learn a flow φ as described above with an
augmentation loss associated to the 1-dimensional PD point which is the most far away from the
diagonal, in order to force latent spaces to have a significant one-dimensional topological feature,
i.e., loop. As the datasets are small, we do not use subsampling, and we use learning rate λ = 0.1,
Gaussian kernels with bandwidth σ = 0.3 and an increase of at least 3. in the topological loss (from
an iteration to the next) to stop the algorithm6."
NUMERICAL EXPERIMENTS,0.424812030075188,"Figure 6:
As the four samples from the
topologically-optimized LS (blue) are far
from the initial LS (orange), the decoded im-
ages are fuzzy. However, reverting φ and
following the corresponding green trajecto-
ries allows to render good-looking images."
NUMERICAL EXPERIMENTS,0.42857142857142855,"We provide some qualitative results in Figure 7 (see also
Appendix C, Figure 10). In order to quantify the improve-
ment, we also computed the Pearson correlation scores
between the ground-truth angles θi and the angles ˆθi com-
puted from the topologically-optimized LSs with"
NUMERICAL EXPERIMENTS,0.4323308270676692,"ˆθi := ∠(φ◦E(xi)−ˆE[φ◦E(X)], φ◦E(x1)−ˆE[φ◦E(X)]),"
NUMERICAL EXPERIMENTS,0.43609022556390975,"where ˆE[φ ◦E(X)] := n−1 Pn
i=1 φ ◦E(xi), and φ de-
notes our flow.
In Table 1, we provide an average of
these scores over 100 test sets obtained by randomly per-
turbing the training set with uniform noise of amplitude
0.05. As expected, correlation becomes better after forc-
ing the latent spaces to have the topology of a circle. This
better interpretability is also illustrated in Figure 6, in
which four angles are specified, which are mapped to the
topologically-optimized LS, then pushed to the initial LS
of the black-box VAE by following the reverted flow of
our learned diffeomorphism φ, and finally decoded back
into realistic, COIL-like images."
NUMERICAL EXPERIMENTS,0.4398496240601504,"Single-cell data.
We also deployed our proposed topological correction of LSs from AEs on a real-
world dataset of single cells. Specifically, we designed an experiment on single cell HiC (scHiC)
data inspired from [27]. The dataset is comprised of single cells characterized by chromatin folding,
that is, each cell is encoded by the spatial distance matrix of its DNA fragments. The dataset we
focus on is taken from [31], in which it was shown that cells are sampled at different phases of the
cell cycle. Thus, similar to COIL images, we expect latent embeddings of this dataset to exhibit a
circular shape, that we can constrain with diffeomorphic topological optimization."
NUMERICAL EXPERIMENTS,0.44360902255639095,"6Indeed, we noticed that 3. was a consistent threshold for detecting whether the representative cycle of the
most persistent PD point changed between iterations."
NUMERICAL EXPERIMENTS,0.4473684210526316,"Dataset
Duck
Cat
Pig
Vase
Teapot
No optim.
0.56 ± 7.5e-04
0.78 ± 1.4e-03
0.17 ± 5.7e-04
0.86 ± 7.2e-03
0.32 ± 1.6e-03
Diffeo
0.61 ± 3.1e-03
0.83 ± 1.2e-03
0.76 ± 2.1e-04
0.93 ± 9.8e-04
0.39 ± 3.4e-03
Dataset
scHiC (augmentation)
scHiC (registration)
No optim.
0.79 ± 8.1e-03
0.792 ± 8.1e-03
Diffeo
0.84 ± 4.3e-03
0.794 ± 8.4e-03
Table 1: Means and variances of correlation scores computed over 100 test sets, for both COIL and scHiC."
NUMERICAL EXPERIMENTS,0.45112781954887216,"Specifically, we processed this single cell dataset of 1, 171 cells with the stratum-adjusted correlation
coefficient (SCC) with 500kb and convolution parameter h = 1 on chromosome 10. Then, we run
kernel PCA on the SCC matrix to obtain a preprocessed dataset in R100, on which we applied
the same VAE architecture than the one described above for COIL images. Finally, we optimized
two losses, the first was the same augmentation loss than for the COIL images, the second was the
following registration loss:"
NUMERICAL EXPERIMENTS,0.4548872180451128,"L : X ∈Rn×d 7→W 2(Dgm1(X), Dtarget),"
NUMERICAL EXPERIMENTS,0.45864661654135336,"where Dgm(X) contains the points of the PD of X with distance-to-diagonal at least τ = 1, Dtarget
is a target PD with only one point p∗= [−3.5, 3.5], and W is the 2-Wasserstein distance between
PDs. We used σ = 0.2 for the augmentation loss and σ = 0.025 for the registration loss (σ is set to a
smaller value for the registration loss in order to mitigate the effects of matching instability), λ = 0.1
on 500 epochs, with subsampling of size s = 300 for computational efficiency (as computing VR
diagrams without radius thresholding on 1, 171 points already takes few minutes on a laptop CPU,
which becomes hardly tractable if done repetitively as in gradient descent), and loss increase of 3.
as a stopping criterion. Qualitative results are displayed in Appendix C, Figure 11, and we also
measured quantitative improvement with the correlation scores between latent space angles and
repli scores7 in Table 1, in which improvements can be observed. An additional experiment on the
influence of the bandwidth parameter σ over these correlation scores, as well as over convergence,
can also be found in Appendix C."
CONCLUSION,0.462406015037594,"5
Conclusion"
CONCLUSION,0.46616541353383456,"In this article, we have presented a way to turn sparse topological gradients into dense diffeomor-
phisms with quantifiable Lipschitz constants, and showcased practical benefits of this approach in
terms of convergence speed, scaling, and applications to black-box AE models on several datasets.
Several questions are still open for future work."
CONCLUSION,0.4699248120300752,"In terms of theoretical results, we plan on working on the stability between the diffeomorphic in-
terpolations computed on a dataset and its subsamples. This requires some control over the loca-
tions of the critical points, which we expect to be possible in statistical estimation; indeed sublevel
sets of density functions are know to have stable critical points [10, Lemma 17]. We also plan to
look at adaptive kernels whose parameters (like the bandwidth σ) depend on the input point cloud
σ = σ(X) (instead of using a fixed kernel and parameters at every iteration), and understand the
convergence properties of our proposed diffeomorphic gradient descent. Finally, applying our dif-
feomorphic interpolation to sparse gradients computed with multiparameter persistent homology
is another natural research direction, provided that differentiability properties have recently been
proved in that setting [30, 37]."
CONCLUSION,0.47368421052631576,"Concerning the AE experiment, we plan to investigate the limitations presented in the figure below:"
CONCLUSION,0.4774436090225564,"as the initial LSs (colored with the ground-truth angles) have zero (left)
or two (right) loops, it is impossible to unfold them with diffeomor-
phisms; instead the optimized latent spaces either also exhibit no topol-
ogy, or mixes different angles. In future work, we plan on investigating
other losses or gradient descent schemes for diffeomorphic topological optimization, including strat-
ified procedures similar to [27] that allow for local topological changes during training."
CONCLUSION,0.48120300751879697,"7The repli-score is a real-valued proxy for the cell cycle, which was introduced in [31], and which can be
computed out of the copy numbers of genome regions associated to the early phases of the cell cycle. Thus,
a large correlation with this score indicates that the circular shape in the optimized latent space is indeed
representative of the cell cycle itself."
CONCLUSION,0.4849624060150376,Acknowledgements
CONCLUSION,0.48872180451127817,"M.C. was supported by ANR grant ""TopModel"", ANR-23-CE23-0014. The authors are grateful to
the OPAL infrastructure from Université Côte d’Azur for providing resources and support."
REFERENCES,0.4924812030075188,References
REFERENCES,0.49624060150375937,"[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Good-
fellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software avail-
able from tensorflow.org.
[2] Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Ship-
man, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence
images: a stable vector representation of persistent homology. Journal of Machine Learning
Research, 18(8):1–35, 2017.
[3] Tolga Birdal, Aaron Lou, Leonidas Guibas, and Umut Simsekli. Intrinsic dimension, persistent
homology and generalization in neural networks. In Advances in Neural Information Process-
ing Systems 34 (NeurIPS 2021), volume 34, pages 6776–6789. Curran Associates, Inc., 2021.
[4] Mathieu Carrière, Frédéric Chazal, Marc Glisse, Yuichi Ike, Hariprasad Kannan, and Yuhei
Umeda. Optimizing persistent homology based functions. In 38th International Conference
on Machine Learning (ICML 2021), volume 139, pages 1294–1303. PMLR, 2021.
[5] Mathieu Carrière, Frédéric Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, and Yuhei
Umeda. PersLay: a neural network layer for persistence diagrams and new graph topological
signatures. In 23rd International Conference on Artificial Intelligence and Statistics (AISTATS
2020), pages 2786–2796. PMLR, 2020.
[6] Mathieu Carrière, Steve Y. Oudot, and Maks Ovsjanikov. Stable topological signatures for
points on 3d shapes. Computer Graphics Forum, 34(5):1–12, 2015.
[7] Frédéric Chazal, David Cohen-Steiner, Leonidas Guibas, Facundo Mémoli, and Steve Oudot.
Gromov-Hausdorff stable signatures for shapes using persistence. Computer Graphics Forum,
28(5):1393–1403, 2009.
[8] Frédéric Chazal, Vin De Silva, and Steve Oudot. Persistence stability for geometric complexes.
Geometriae Dedicata, 173(1):193–214, 2014.
[9] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, and
Larry Wasserman. Subsampling methods for persistent homology. In 32nd International Con-
ference on Machine Learning (ICML 2015), volume 37, pages 2143–2151. PMLR, 2015.
[10] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, and
Larry Wasserman. Robust topological inference: distance to a measure and kernel distance.
Journal of Machine Learning Research, 18(159):1–40, 2018.
[11] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Alessandro Rinaldo, Aarti Singh, and Larry
Wasserman. On the bootstrap for persistence diagrams and landscapes. Modelirovanie i Analiz
Informatsionnykh Sistem, 20(6):111–120, 2013.
[12] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Alessandro Rinaldo, and Larry Wasserman.
Stochastic convergence of persistence landscapes and silhouettes. Journal of Computational
Geometry, 6(2):140–161, 2015.
[13] Frédéric Chazal and Steve Yann Oudot. Towards persistence-based reconstruction in euclidean
spaces. In Proceedings of the twenty-fourth annual symposium on Computational geometry,
pages 232–241. ACM, 2008.
[14] Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang. A topological regularizer for classifiers
via persistent homology. In The 22nd International Conference on Artificial Intelligence and
Statistics, pages 2573–2582. PMLR, 2019."
REFERENCES,0.5,"[15] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. Advances in neural information processing
systems, 31, 2018."
REFERENCES,0.5037593984962406,"[16] James R Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A Zimmer, Julia A Schnabel, and
Andrew P King.
A topological loss function for deep-learning based image segmentation
using persistent homology. IEEE transactions on pattern analysis and machine intelligence,
44(12):8766–8778, 2020."
REFERENCES,0.5075187969924813,"[17] David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence diagrams.
Discrete & Computational Geometry, 37(1):103–120, 2007."
REFERENCES,0.5112781954887218,"[18] David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz func-
tions have l p-stable persistence. Foundations of computational mathematics, 10(2):127–139,
2010."
REFERENCES,0.5150375939849624,"[19] Meryll Dindin, Yuhei Umeda, and Frederic Chazal. Topological data analysis for arrhyth-
mia detection through modular neural networks. In Advances in Artificial Intelligence: 33rd
Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May
13–15, 2020, Proceedings 33, pages 177–188. Springer, 2020."
REFERENCES,0.518796992481203,"[20] Herbert Edelsbrunner and John Harer. Computational topology: an introduction. American
Mathematical Soc., 2010."
REFERENCES,0.5225563909774437,"[21] Marcio Gameiro, Yasuaki Hiraoka, and Ippei Obayashi.
Continuation of point clouds via
persistence diagrams. Physica D: Nonlinear Phenomena, 334:118–132, 2016."
REFERENCES,0.5263157894736842,"[22] Thomas Gebhart and Paul Schrater. Adversary detection in neural networks via persistent
homology. arXiv preprint arXiv:1711.10056, 2017."
REFERENCES,0.5300751879699248,"[23] Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph fil-
tration learning. In 37th International Conference on Machine Learning (ICML 2020), volume
119, pages 4314–4323. PMLR, 2020."
REFERENCES,0.5338345864661654,"[24] Max Horn, Edward de Brouwer, Michael Moor, Bastian Rieck, and Karsten Borgwardt. Topo-
logical Graph Neural Networks. In 10th International Conference on Learning Representations
(ICLR 2022). OpenReviews.net, 2022."
REFERENCES,0.5375939849624061,"[25] Xiaoling Hu, Li Fuxin, Dimitris Samaras, and Chao Chen. Topology-preserving deep image
segmentation. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019),
pages 5657–5668. Curran Associates, Inc., 2019."
REFERENCES,0.5413533834586466,"[26] Théo Lacombe, Yuichi Ike, Mathieu Carrière, Frédéric Chazal, Marc Glisse, and Yuhei Umeda.
Topological Uncertainty: monitoring trained neural networks through persistence of activation
graphs. In 30th International Joint Conference on Artificial Intelligence (IJCAI 2021), pages
2666–2672. International Joint Conferences on Artificial Intelligence Organization, 2021."
REFERENCES,0.5451127819548872,"[27] Jacob Leygonie, Mathieu Carrière, Théo Lacombe, and Steve Oudot. A gradient sampling
algorithm for stratified maps with applications to topological data analysis.
Mathematical
Programming, pages 1–41, 2023."
REFERENCES,0.5488721804511278,"[28] Jacob Leygonie, Steve Oudot, and Ulrike Tillmann. A framework for differential calculus on
persistence barcodes. Foundations of Computational Mathematics, pages 1–63, 2021."
REFERENCES,0.5526315789473685,"[29] Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders.
In 37th International Conference on Machine Learning (ICML 2020), volume 119, pages
7045–7054. PMLR, 2020."
REFERENCES,0.556390977443609,"[30] Soham Mukherjee, Shreyas Samaga, Cheng Xin, Steve Oudot, and Tamal Dey. D-gril: End-
to-end topological learning with 2-parameter persistence. In CoRR. arXiv:2406.07100, 2024."
REFERENCES,0.5601503759398496,"[31] Takashi Nagano, Yaniv Lubling, Csilla Várnai, Carmel Dudley, Wing Leung, Yael Baran, Netta
Mendelson-Cohen, Steven Wingett, Peter Fraser, and Amos Tanay. Cell-cycle dynamics of
chromosomal organization at single-cell resolution. Nature, 547:61–67, 2017."
REFERENCES,0.5639097744360902,"[32] S. A. Nene, S. K. Nayar, and H. Murase. Columbia Object Image Library (COIL-100). In
Technical Report CUCS-005-96, 1996."
REFERENCES,0.5676691729323309,"[33] Arnur Nigmetov, Aditi S Krishnapriyan, Nicole Sanderson, and Dmitriy Morozov. Topological
regularization via persistence-sensitive optimization. arXiv preprint arXiv:2011.05290, 2020."
REFERENCES,0.5714285714285714,"[34] Arnur Nigmetov and Dmitriy Morozov. Topological optimization with big steps. Discrete &
Computational Geometry, pages 1–35, 2024.
[35] Steve Y Oudot. Persistence theory: from quiver representations to data analysis, volume 209.
American Mathematical Society, 2015.
[36] Adrien Poulenard, Primoz Skraba, and Maks Ovsjanikov. Topological function optimization
for continuous shape matching. In Computer Graphics Forum, volume 37, pages 13–25. Wiley
Online Library, 2018.
[37] Luis Scoccola, Siddharth Setlur, David Loiseaux, Mathieu Carrière, and Steve Oudot. Dif-
ferentiability and convergence of filtration learning with multiparameter persistence. In 41st
International Conference on Machine Learning (ICML 2024). PMLR, 2024.
[38] Primoz Skraba and Katharine Turner. Wasserstein stability for persistence diagrams. arXiv
preprint arXiv:2006.16824, 2020.
[39] Yitzchak Solomon, Alexander Wagner, and Paul Bendich. A fast and robust method for global
topological functional optimization. In International Conference on Artificial Intelligence and
Statistics, pages 109–117. PMLR, 2021.
[40] The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.6.0
edition, 2022.
[41] Greg Turk and Marc Levoy. Zippered polygon meshes from range images. In Proceedings of
the 21st annual conference on Computer graphics and interactive techniques, pages 311–318,
1994.
[42] Yuhei Umeda. Time series classification via topological data analysis. Information and Media
Technologies, 12:228–239, 2017.
[43] Laurent Younes. Shapes and diffeomorphisms. Springer-Verlag, 2010."
REFERENCES,0.575187969924812,"A
A more extensive presentation of TDA"
REFERENCES,0.5789473684210527,"The starting point of TDA is to extract quantitative topological information from structured
objects—for example graphs, points sampled on a manifold, time series, etc. Doing so relies on
a piece of algebraic machinery called persistent homology (PH), which informally detects the pres-
ence of underlying topological properties in a multiscale way. Here, estimating topological proper-
ties should be understood as inferring the number of connected components (topology of dimension
0), the presence of loops (topology of dimension 1), of cavities (dimension 2), and so on in higher
dimensional settings."
REFERENCES,0.5827067669172933,"Simplicial filtrations.
Given a finite simplicial complex8 K, a filtration over K is a map t 7→
Kt ⊆K that is non-decreasing (for the inclusion). For each σ ∈K, one can record the value t(σ) :=
inf{t | σ ∈Kt} at which the simplex σ is inserted in the filtration. From a topological perspective,
the insertion of σ has exactly one of two effects: either it creates a new topological feature in Kt
(e.g., the insertion of an edge can create a loop, that is, a one-dimensional topological feature) or it
destroys an existing feature of lower dimension (e.g., two independent connected components are
now connected by the insertion of an edge). Relying on a matrix reduction algorithm [20, §IV.2],
PH identifies, for each topological feature appearing in the filtration, the critical pair of simplices
(σb, σd) that created and destroyed this feature, and as a byproduct the corresponding birth and
death times t(σb), t(σd).9 The collection of intervals (t(σb), t(σd)) is a (finite) subset of the open
half-plane {(b, d) ∈R2 | b < d}, called the persistence diagram (PD) of the filtration (Kt)t. The
distance of such a point (tb, td) to the diagonal {b = d}, namely 2−1"
REFERENCES,0.5864661654135338,"2 |t(σb) −t(σd)|, is called the
persistence of the corresponding topological feature, as an indicator of “for how long” could this
feature be detected in the filtration (Kt)t."
REFERENCES,0.5902255639097744,"Note that if (σb, σd) is a critical pair for our filtration (Kt)t, it holds that |σb| = |σd|+1. The quantity
|σb| −1 is the dimension of the corresponding topological feature (e.g., loops, which are created by
the insertion of an edge and killed by the insertion of a triangle, are topological features of dimension
one). From a computational perspective, deriving the PD of a filtration (Kt)t is empirically10 quasi-
linear with respect to the number of simplices in K (which can still be extremely high in the case of
Vietoris-Rips filtration—see below—where K = 2V with |V | = n being typically quite large)."
REFERENCES,0.5939849624060151,"The Vietoris-Rips filtration.
A particular instance of simplicial filtration that will be extensively
used in this work is the Vietoris-Rips (VR) one. Given X = (x1, . . . , xn) ∈Rn×d a point cloud of
n points in dimension d, one consider the simplicial complex K = 2X and then the filtration (Kt)t
defined by
σ = {xi1 . . . xip} ∈Kt ⇐⇒∀j, j′ ∈{1, . . . , p}, ∥xij −xij′∥≤t.
(8)"
REFERENCES,0.5977443609022557,"The corresponding persistence diagram will be denoted, for the sake of simplicity, by Dgm(X)."
REFERENCES,0.6015037593984962,"Note that for t < 0, Kt = ∅, when t ≥diam(X), Kt = K, and there is always a point with
coordinates (0, +∞) in Dgm(X) accounting for the remaining connected component when t →
∞. This is the unique point in Dgm(X) for which the second coordinate is +∞(and is often
discarded in practice, as it does not play any significant role). Intuitively, Dgm(X) reflects the
topological properties that can be inferred from the geometry of X; this can be formalized by various
results which state, roughly, that if the xi are i.i.d. samples from a regular measure µ supported on
a submanifold M ⊂Rd, then with high probability the topological properties of M are reflected in
Dgm(X) (see [13, 9]). From a computational perspective, note that the VR filtration only depends
on X through the pairwise distance matrix (∥xi−xj∥)1≤i,j≤n, and thus the complexity of computing
Dgm(X) depends only linearly in d11. On the other hand, since Dgm(X) scales (at least) linearly"
REFERENCES,0.6052631578947368,"8A simplicial complex is a combinatorial object generalizing graphs and triangulations. Given a finite set
of vertices V = {v1, . . . , vn}, a finite simplicial complex K is a subset of 2V (whose elements are called
simplices) such that σ ∈K ⇒τ ∈K, ∀τ ⊆σ (if a simplex is in the complex, its faces must be in it as well).
9It may happen that a topological feature appears at some time tb and is never destroyed, in which case
the death time is set to +∞. However, in the context of the Vietoris-Rips filtration, extensively studied in this
work, this (almost) never happens. See the next paragraph.
10The theoretical worst case yields a cubic complexity, but the matrix that has to be reduced is typically very
sparse, enabling this practical speed up.
11However, the statistical efficiency of Dgm(X) when it is used as an estimator for the topology of an
underlying manifold M deteriorate when the intrinsic dimension of M increases."
REFERENCES,0.6090225563909775,"−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00 −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.6127819548872181,Initial and ﬁnal point cloud
REFERENCES,0.6165413533834586,"init
ﬁnal state"
REFERENCES,0.6203007518796992,"0.1
0.2
0.3
0.4
0.5
0.6
Birth 0.1 0.2 0.3 0.4 0.5 0.6 Death"
REFERENCES,0.6240601503759399,Initial and ﬁnal persistence diagrams
REFERENCES,0.6278195488721805,"0
25
50
75
100
125
150
175
200 −0.4 −0.3 −0.2 −0.1"
REFERENCES,0.631578947368421,Evolution of the loss over iterations
REFERENCES,0.6353383458646616,"Figure 8:
Topological optimization of an initial point cloud X (in red) by minimizing X
7→
P"
REFERENCES,0.6390977443609023,"(b,d)∈Dgm(X) −|d|2+P"
REFERENCES,0.6428571428571429,"x∈X dist(x, [−1, 1]2). This loss favors the apparition of topological features (loops)
while the regularization term penalizes points that would go to infinity otherwise.—Experiment reproduced
following the setting of [4], using code available at https://github.com/GUDHI/TDA-tutorial/blob/
master/Tuto-GUDHI-optimization.ipynb."
REFERENCES,0.6466165413533834,"with respect to the number of simplices in K, computing the whole VR diagram of a point cloud
X ∈Rn×d can take up to O(2n) operations. Even if one restricts topological features of dimension
d′ ≤d (e.g. d′ = 1 if one only considers loops)—as commonly done—the complexity is of order
O(nd′+2), which becomes quickly intractable if n is large, even if d′ = 1 or 2."
REFERENCES,0.650375939849624,"B
Delayed proofs"
REFERENCES,0.6541353383458647,"Proof of Proposition 3.2. One
has:
∥˜v(x)
−
˜v(y)∥1
=
∥P"
REFERENCES,0.6578947368421053,"i∈I(K(x, xi)
−
K(y, xi))(−K−1∇L( ˜X(t)))i∥1 ≤P"
REFERENCES,0.6616541353383458,"i∈I |ρσ(∥x−xi∥)−ρσ(∥y −xi∥)|·∥(−K−1∇L( ˜X(t)))i∥1,
since we are using Gaussian kernels.
As |ρσ(∥x −xi∥) −ρσ(∥y −xi∥)| ≤Cd,σ∥x −y∥2,
with Cd,σ
=
2
d+1"
REFERENCES,0.6654135338345865,"2 π
d−1"
REFERENCES,0.6691729323308271,"2 σd−1 (see [2, Theorem 8]), it follows that ∥˜v(x) −˜v(y)∥1
≤
Cd,σ∥x −y∥2 · ∥K−1∥1 · ∥∇L( ˜X(t))∥1."
REFERENCES,0.6729323308270677,"Let us upper bound the term ∥∇L( ˜X(t))∥1. Let us start with the simplification loss, one has ∂ℓ"
REFERENCES,0.6766917293233082,"∂bi =
2(bi −di) and, writing bi = ∥xi1 −xi2∥2 (for some critical points xi1, xi2 ∈
˜X(t)), one has
∂bi
∂xi1 =
xi1−xi2
∥xi1−xi2∥2 and
∂bi
∂xi2 = −
xi1−xi2
∥xi1−xi2∥2 . Similarly, one has
∂ℓ
∂di = −2(bi −di), and writing
di = ∥xi3 −xi4∥2 provides the corresponding partial derivatives."
REFERENCES,0.6804511278195489,"Applying (2), this gives:"
REFERENCES,0.6842105263157895,"∥∇L( ˜X(t))∥1 =
X"
REFERENCES,0.6879699248120301,"x∈˜
X(t)
∥
X"
REFERENCES,0.6917293233082706,"x
b,1
→(bi,di)"
REFERENCES,0.6954887218045113,"2(bi −di)
x −xi2
∥x −xi2∥2
−
X"
REFERENCES,0.6992481203007519,"x
b,2
→(bi,di)"
REFERENCES,0.7030075187969925,"2(bi −di)
xi1 −x
∥xi1 −x∥2 −
X"
REFERENCES,0.706766917293233,"x
d,1
→(bi,di)"
REFERENCES,0.7105263157894737,"2(bi −di)
x −xi4
∥x −xi4∥2
+
X"
REFERENCES,0.7142857142857143,"x
d,2
→(bi,di)"
REFERENCES,0.7180451127819549,"2(bi −di)
xi3 −x
∥xi3 −x∥2
∥1,"
REFERENCES,0.7218045112781954,"where
b,1
→(resp.
b,2
→) means that x appears as left point (resp. right point) in the computation of
the birth filtration value bi of one of the k PD points (bi, di) ∈Dgm( ˜X(t)) associated to the
loss, and similarly for death filtration values. A brutal majoration finally gives ∥∇L( ˜X(t))∥1 ≤
2
√ d P"
REFERENCES,0.7255639097744361,"x∈˜
X(t)
P"
REFERENCES,0.7293233082706767,"x→(bi,di) |bi −di| ≤8
√"
REFERENCES,0.7330827067669173,"d · Persk(Dgm( ˜X(t))), as there are at most four points
associated to every (bi, di) ∈Dgm( ˜X(t)). One can easily see that the same bound applies to the
augmentation loss."
REFERENCES,0.7368421052631579,"Let us finally bound ∥K−1∥1, one has ∥K−1∥1 = κ(K)/∥K∥1 ≤κ(K). Indeed, as we are using
Gaussian kernels, ∥K∥1 = max1≤i≤n
Pn
j=1 ρσ(∥xi −xj∥2) ≥1."
REFERENCES,0.7406015037593985,"C
Complementary experimental results and details"
REFERENCES,0.7443609022556391,"Subsampling and improving over [4].
We reproduce the experiment of [4, §5], see also Figure 8,
but starting from an initial point cloud X0 of size n = 2, 000 instead of n = 300. This makes the"
REFERENCES,0.7481203007518797,"1.0
0.5
0.0
0.5
1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Init"
REFERENCES,0.7518796992481203,"1.0
0.5
0.0
0.5
1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.7556390977443609,"Vanilla, epoch 750"
REFERENCES,0.7593984962406015,"1.0
0.5
0.0
0.5
1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.7631578947368421,"Diffeo, epoch 750"
REFERENCES,0.7669172932330827,"0
200
400
600
Epoch 0.3 0.2 0.1 0.0 0.1 0.2 Loss"
REFERENCES,0.7706766917293233,Evolution of loss
REFERENCES,0.7744360902255639,"Vanilla
Diffeo"
REFERENCES,0.7781954887218046,"Figure 9: Topological optimization with subsampling. From left to right, the initial point cloud X0, the point
cloud after 750 steps of vanilla gradient descent (+subsampling), the point cloud after 750 steps of diffeomor-
phic interpolation gradient descent (+subsampling), loss evolution over epochs. Parameters: λ = 0.1, σ = 0.1."
REFERENCES,0.7819548872180451,"Figure 10: Topologically-optimized LSs and losses for duck, cat, pig, vase and teapot."
REFERENCES,0.7857142857142857,"raw computation of Dgm(Xk) at each gradient step unpractical. Following Section 3.2 we rely on
subsampling with sample size s = 100 and apply Algorithm 1. Results are summarized in Figure 9.
While relying on vanilla gradients and subsampling barely changes the point cloud even after 750
epochs, the diffeomorphic interpolation gradient with subsampling manages to decrease the loss."
REFERENCES,0.7894736842105263,"More extensive reports of running time and comments.
On the hardware used in our exper-
iments (the first two experiments were run on a 11th Gen Intel(R) Core(TM) i5-1135G7 @
2.40GHz, the last one on a 2x Xeon SP Gold 5115 @ 2.40GHz.), we report the approximate fol-
lowing running times:"
REFERENCES,0.793233082706767,"• Small point cloud optimization without subsampling (see Figure 4, n = 200 points): one
gradient descent iteration takes about 1s for the vanilla topological gradient and our diffeo-
morphic interpolation. The use of oineus integrated in our pipeline raises the running time
(per iteration) to 10 to 20 seconds. Note that the diffeomorphic interpolation and oineus
may converge in less steps than the vanilla topological gradient, preserving a competitive
advantage. We also believe that oineus has a significant room for improvement in terms
of running times and may be a promising method in the future to be used jointly with our
approach.
• Iterating over the stanford bunny with subsampling (n = 35, 947, s = 100) takes about 3
seconds per iteration for the vanilla topological gradient and 20 second for our diffeomor-"
REFERENCES,0.7969924812030075,"Figure 11: Decrease of augmentation (up left) and registration (up right) losses for the scHiC dataset, as well
as the corresponding initial (orange) and optimized (blue) LSs displayed below."
REFERENCES,0.8007518796992481,"phic interpolation. The increase in running time with respect to the previous experiment
mostly lies on instantiating and applying the n × d (d = 3) vector field ˜v (requires to com-
pute ρi(x −xi) for each new x (n of them) and sampled xi (|I| of them, which is typically
very small), hence a ∼O(n) complexity)."
REFERENCES,0.8045112781954887,"• Training the VAE for the COIL and scHiC datasets are the most computationally expensive
parts of this work: it takes about 3 hours per image (20 of them), and 3 hours also for
the scHiC dataset. In contrast, performing the topological optimization take few dozen
of minutes (less than one hour) for each image. Applying it is done in few seconds at
most. Recall that our method is designed to handle pre-trained models (which may be way
more sophisticated than the one we used!); and its running time does not depend on the
complexity of the model."
REFERENCES,0.8082706766917294,"Influence of the bandwidth σ.
We reproduce the same experimental setting as in Fig-
ure 3,
i.e.,
sample points uniformly on a circle of radius 1 plus additional noise ∼
N(0, 0.05I2), and consider minimizing the total persistence of the point cloud.
We take σ ∈
{0, 0.1, 0.2, 0.3, 0.5, 0.7, 1, 2, 3, 4, 5} (with the convention that σ = 0 corresponds to the vanilla
topological gradient) and learning rate λ = 0.1. We also rely on a subsampling with batch size
s = 50. To quantify the variability of the scheme with respect to the randomness induced by the
subsampling step, we run each gradient descent 50 times with a fixed initialization X0, up to a max-
imum of 200 iterations, stopped earlier if a loss of 0 (no topology left, global minimum has been
reached) is measured."
REFERENCES,0.8120300751879699,"Figure 12 displays the results of this illustrative experiment. We report the median of both running
time and number of iterations to reach convergence (or reach the 200 iterations limit), along with
the 10 and 90 percentiles. The conclusions are:"
REFERENCES,0.8157894736842105,"• For σ = 0 (vanilla) and σ ≥3, the gradient descent never converges in less than 200 steps.
Since the radius of the diameter of the circle is 2, it is not surprising that taking a bandwidth
larger than that hinders convergence."
REFERENCES,0.8195488721804511,"• For σ ∈(0.1, 1], the convergence occurs within the same order of magnitude (between
0.49 and 1.74s), the best performance being reached at σ = 0.3 (recall that we used in
the paper, testifying that we did not rely on hyperparameter tuning). It suggests that, on
regular structure, the approach is smooth with respect to σ. Empirically, we observe that a
good proxy is to take σ < median({|xi −xj|}i,j). Note that even though in theory, σ →0
should recover the vanilla topological gradients, one is limited by numerical accuracy when
evaluating the Gaussian kernel."
REFERENCES,0.8233082706766918,"• The variation around the median over 50 runs is very small: the randomness of the samples
at each iteration (hence of the trajectory) barely impacts the decrease of the loss and thus
the convergence time."
REFERENCES,0.8270676691729323,"0
1
2
3
4
5
bandwidth 0.0 2.5 5.0 7.5 10.0 12.5 15.0"
REFERENCES,0.8308270676691729,Total running time (s)
REFERENCES,0.8345864661654135,"0
1
2
3
4
5
bandwidth 0 50 100 150 200"
REFERENCES,0.8383458646616542,#Iterations to converge / stop
REFERENCES,0.8421052631578947,Impact of  and sampling on convergence
REFERENCES,0.8458646616541353,"Figure 12: Topological simplification, point cloud of diameter 2 with median pairwise distance ≃
√"
MEDIAN,0.849624060150376,"2. Median
and 10-90 percentiles over 50 runs. (Left) Time to converge for different values of σ ∈[0, 5] (σ = 0 corre-
sponds to Vanilla). (Right) #iterations to converge (or stop after 200 iterations, indicated by the dashed red
line)."
MEDIAN,0.8533834586466166,"We also studied how the bandwidth σ influences the correlation scores of Table 1 in Figure 13. We
observe oscillations for values that are roughly on the sides (very small or very large), and more
stable scores for middle range values. Note that these oscillations could also come from how the
correlation score itself is computed."
MEDIAN,0.8571428571428571,"Figure 13: Influence of the kernel bandwidth σ on correlation scores for a few datasets. The values of σ are
evenly spaced between 0.05 and 1 for the COIL datasets, and between 0.025 and 0.5 for the scHiC dataset."
MEDIAN,0.8609022556390977,NeurIPS Paper Checklist
CLAIMS,0.8646616541353384,1. Claims
CLAIMS,0.868421052631579,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8721804511278195,Answer: [Yes]
CLAIMS,0.8759398496240601,"Justification: Our abstract is quite extensive, and there is a clear “Contribution and Outline”
paragraph at the end of our introduction along with “Related works” and “Limitations” (of
previous works) paragraphs."
LIMITATIONS,0.8796992481203008,2. Limitations
LIMITATIONS,0.8834586466165414,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8872180451127819,Answer: [Yes]
LIMITATIONS,0.8909774436090225,Justification: This is discussed in the Conclusion section.
THEORY ASSUMPTIONS AND PROOFS,0.8947368421052632,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8984962406015038,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9022556390977443,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9060150375939849,"Justification: Our two theoretical results are followed by a proof. The first is pretty short
and included in the main material, the second one is longer and technical and thus deferred
to the appendix."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9097744360902256,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9135338345864662,"Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9172932330827067,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9210526315789473,"Justification: We mentioned important hyper-parameters for our PoC experiments (learning
rate λ, bandwidth σ, number of epochs, stopping criterion if any.). The code is available in
the supplementary material as well."
OPEN ACCESS TO DATA AND CODE,0.924812030075188,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9285714285714286,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9323308270676691,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9360902255639098,"Justification: Code is provided in the supplementary material with an (hopefully) clear
Readme.md file which provides some ready-to-use quick start examples. It is also available
as a public Github repository, see https://github.com/tlacombe/topt."
OPEN ACCESS TO DATA AND CODE,0.9398496240601504,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.943609022556391,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9473684210526315,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9511278195488722,"Justification: We hopefully provided the important experimental details to understand our
experiments."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9548872180451128,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9586466165413534,"Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9624060150375939,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9661654135338346,"Justification: We did not report error bars in our PoC experiments as they were meant for
illustrative purposes. We did report error bars in the form of standard deviations for our
main experiments (topological correction of latent spaces of VAEs for images and single
cells datasets). These were obtained by generating multiple test sets by adding noise to the
training set.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9699248120300752,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Hardware is mentioned in the first paragraph of Section 4, and we reported
running times either in the plots or in the appendix.
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9736842105263158,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We do not use sensitive nor closed datasets or other aspect that may enter in
conflict with NeurIPS Code of Ethics.
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9774436090225563,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: We do not expect that—in its current state—our article may have any neg-
ative societal impact. It is a work that introduced a novel method to perform topological
optimization. This recent research field does not have direct applications that would have
negative societal impact so far. Of course, it falls in the same pitfall as any machine learn-
ing related research problem, but we do not see something specific to our work that would
be worth discussing.
11. Safeguards"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.981203007518797,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We used a single generative model (VAE) to reproduce images from the COIL
dataset and single cells from the Hi-C dataset. We trained it by ourselves.
12. Licenses for existing assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9849624060150376,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We used three non-synthetic datasets: the Stanford Bunny, the COIL dataset
and the Hi-C dataset, for which we give the proper credits.
13. New Assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9887218045112782,"Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: No new asset.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9924812030075187,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9962406015037594,"Answer: [NA]
Justification: the article does not involve crowdsourcing nor research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the article does not involve crowdsourcing nor research with human subjects."
