Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0023923444976076554,"It is well-known that accelerated gradient first order methods possess optimal
complexity estimates for the class of convex smooth minimization problems. In
many practical situations, it makes sense to work with inexact gradients. How-
ever, this can lead to the accumulation of corresponding inexactness in the the-
oretical estimates of the rate of convergence. We propose some modification of
the methods for convex optimization with inexact gradient based on the subspace
optimization sush as Nemirovski’s Conjugate Gradients and Sequential Subspace
Optimization. We research the methods convergence for different condition of in-
exactness both in gradient value and accuracy of subspace optimization problems.
Besides this, we investigate generalization of this result to the class of quasar-
convex (weakly-quasi-convex) functions."
INTRODUCTION,0.004784688995215311,INTRODUCTION
INTRODUCTION,0.007177033492822967,"The first-order methods are an important class of approaches for optimization problems. They have
different advantages: simple implementation, usually low cost of iterations and high performance
for wide class of functions (Beck (2017), Gasnikov (2017), Polyak (1987)). Nevertheless, there
are different areas where method access to only inexact gradient: the gradient free optimization
in infinite dimensional spaces Vasilyev (2002), inverse problems Kabanikhin (2011), saddle-point
problems Lan (2020), L. Hien (2023). Therefore, such methods are interesting for many researchers
Devolder (2013), O. Devolder & Nesterov (2014), d’Aspremont (2008), Polyak (1987), Vasin et al.
(2021)."
INTRODUCTION,0.009569377990430622,"Further, note that there are well-known results about convergence of first order methods and optimal-
ity of accelerated methods for class of convex functions d’Aspremont et al. (2021), Bubeck (2015),
Nemirovsky & Yudin (1979a), Barre et al. (2020), Nemirovsky & Yudin (1979b). On the other hand,
nonconvex optimization appears in many practical problems. Especially, interest to such problems is
growing because of deep learning Lan (2020), Goodfellow et al. (2016). One of possible expansion
of convexity is quasar-convexity (or weakly quasar convexity). This class and non-convex examples
are described in S. Guminov (2008); Hardt et al. (2016); Hinder et al. (2020)."
INTRODUCTION,0.011961722488038277,"This paper continues research of the first order methods based on subspace optimization. Such meth-
ods were considered in S. Guminov (2008); Kuruzov & Stonyakin (2021). One of such methods is
Sequential Subspace Optimization (SESOP) Narkiss & Zibulevsky (2005). This method searches
sequentially minima on subspaces and converges to the solution. Recently, several interesting prop-
erties were demonstrated for this method. Especially, S. Guminov (2008) contains proof for con-
vergence of SESOP for quasar-convex case. It demonstrates that this method converges with rate
similar to accelerated rates. Besides, Kuruzov & Stonyakin (2021) proofs the convergence in the
case of inexact gradient. Moreover, it states that this method does not accumulate error in contrast
to other known accelerated methods. However, there were no works devoted to complexity of such
methods in terms of gradient calculations but not iterations. In this work, we propose to use ellipsoid
method Nemirovski et al. (2010); Gladin et al. (2020) for auxiliary problems. We demonstrate the-
oretical complexity under condition of convexity and with inexactness because of inexact gradient
and inexact solution of subproblems."
INTRODUCTION,0.014354066985645933,"Besides of SESOP, we consider generalization of Nemirovski’s Conjugate Gradient method. In
the Section 3.1 we research it convergence for quasar-convex functions that meet quadratic growth
condition. For this method we also demonstrated its non-accumulation of the additive gradient"
INTRODUCTION,0.01674641148325359,Under review as a conference paper at ICOMP 2024
INTRODUCTION,0.019138755980861243,"inexactness. Nevertheless, note that quality of obtained by CG method solution can be degraded
to O
q"
INTRODUCTION,0.0215311004784689,"ε
µδ1

. Also, we prove that for obtaining quality ε we need O
 1"
INTRODUCTION,0.023923444976076555,"ε

iterations for enough"
INTRODUCTION,0.02631578947368421,"small δ1. The main disadvantage of the result in this section is requirement for enough large norm
of gradient on each iteration. But we introduce stop condition that guarantee compromise between
quality of solution and complexity of algorithm."
INTRODUCTION,0.028708133971291867,Our contributions are the following:
INTRODUCTION,0.03110047846889952,"1. Linear convergence for inexact CG method in non-convex case with. We generalize proof
of convergence for Nemirovski’s conjugate gradient method with inexact gradient. Moreover, we
propose stopping rule to approach required quality."
INTRODUCTION,0.03349282296650718,"2. Complexity of auxiliary problems for SESOP and CG methods in convex case. It is natural
to solve low-dimensional subproblems in these methods by low-dimensional methods. We consider
Ellipsoid Method and Multidimensional dichotomy for these problems and estimate complexity for
convex case."
LIT REVIEW,0.03588516746411483,"1
PROBLEM STATEMENT"
LIT REVIEW,0.03827751196172249,"Let us consider minimization problems of convex and L-smooth function f (∥·∥is a usual Euclidean
norm)
∥∇f(x) −∇f(y)∥⩽L∥x −y∥
∀x, y ∈Rn
(1)
with an inexact gradient g : Rn →Rn:
∥g(x) −∇f(x)∥⩽δ,
(2)
where L > 0 and δ > 0."
LIT REVIEW,0.04066985645933014,"The result for convergence per iteration are formulated for quasar-convex problems.
Definition 1. Assume that γ ∈(0, 1] and let x∗be a minimizer of the differentiable function f :
Rn →R. The function f is γ-quasar-convex with respect to x∗if for all x ∈Rn,"
LIT REVIEW,0.0430622009569378,f(x∗) ≥f(x) + 1
LIT REVIEW,0.045454545454545456,"γ ⟨∇f(x), x∗−x⟩.
(3)"
LIT REVIEW,0.04784688995215311,"This class generalize convex functions. It is also known as weakly-quasi convex functions. In the
case of γ = 1 it is well-known star-convexity. Hinder et al. (2020) demonstrates that functions
f(x) = (x2 + 1/8)1/6 are quasar-convex but not convex or star-convex. Besides, work Wang &
Wibisono (2023) demonstrates that some problems of training general linear models are quasar-
convex."
LIT REVIEW,0.050239234449760764,"Also, in this work, we will consider generalizations of strong convexity. The first considered condi-
tion is PL-condition.
Definition 2. The differentiable function f satisfies the Polyak-Łojasiewicz condition (for brevity,
we write PL-condition) for some constant µ > 0:"
LIT REVIEW,0.05263157894736842,f(x) −f ∗⩽1
LIT REVIEW,0.05502392344497608,"2µ∥∇f(x)∥2
∀x ∈Rn,
(4)"
LIT REVIEW,0.05741626794258373,"where f ∗= f(x∗) is the value of the function f at one of the exact solutions x∗of the optimization
problem under consideration."
LIT REVIEW,0.05980861244019139,"The first present of this condition was in Polyak (1963). Recently (see Karimi et al. (2016); Belkin
(2021)), it was proven that many practical problems satisfy this condition. Especially, it holds for
over-parameterized non-linear systems."
LIT REVIEW,0.06220095693779904,"Moreover, we propose results about works of well-known Conjugate-Gradient methods for more
weak condition that PL-condition - quadratic growth condition (see Karimi et al. (2016)).
Definition 3. The differentiable function f satisfies the quadratic growth condition (for brevity, we
write QC-condition) for some constant µ > 0:"
LIT REVIEW,0.0645933014354067,f(x) −f ∗⩾µ
LIT REVIEW,0.06698564593301436,"2 ∥x −x∗∥2
∀x ∈Rn,
(5)"
LIT REVIEW,0.06937799043062201,"where f ∗= f(x∗) is the value of the function f at one of the exact solutions x∗of the optimization
problem under consideration."
LIT REVIEW,0.07177033492822966,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.07416267942583732,"2
SUBSPACE OPTIMIZATION METHODS"
IMPLEMENTATION/METHODS,0.07655502392344497,"Let us present SESOP (Sequential Subspace Optimization) method from S. Guminov (2008);
Narkiss & Zibulevsky (2005); Kuruzov & Stonyakin (2021) (see Algorithm 1). The first step of
this method is constructing of subspace for further optimization. On each iteration there are three
important directions: gradient at current point, direction from start point to current and weighted sum
of gradients from all iterations. Note, all this directions can be calculated with only xk, x0, g(xk).
In other words, this method does not require too much additional memory in comparison with other
first order methods like Gradient Descent."
IMPLEMENTATION/METHODS,0.07894736842105263,"Algorithm 1 A modification of the SESOP method with an inexact gradient
Require: objective function f with an inexact gradient g, initial point x0, number of iterations T."
IMPLEMENTATION/METHODS,0.08133971291866028,"1: w0 = 1
2: for k = 0, . . . , T −1 do"
IMPLEMENTATION/METHODS,0.08373205741626795,"3:
Construct Subspace: d0
k = g(xk),
d1
k = xk −x0,
d2
k =
kP"
IMPLEMENTATION/METHODS,0.0861244019138756,"i=0
ωig(xi)."
IMPLEMENTATION/METHODS,0.08851674641148326,"4:
Find the optimal step"
IMPLEMENTATION/METHODS,0.09090909090909091,"τk ←arg min
τ∈R3 f  xk +"
IMPLEMENTATION/METHODS,0.09330143540669857,"3
X"
IMPLEMENTATION/METHODS,0.09569377990430622,"i=1
τ1di−1
k ! (6)"
IMPLEMENTATION/METHODS,0.09808612440191387,"5:
xk+1 ←xk +
3P"
IMPLEMENTATION/METHODS,0.10047846889952153,"i=1
τ1di−1
k"
IMPLEMENTATION/METHODS,0.10287081339712918,"6:
Update w : wk+1 = 1 2 +
q"
IMPLEMENTATION/METHODS,0.10526315789473684,"1
4 + w2
k
return xT"
IMPLEMENTATION/METHODS,0.1076555023923445,"The next step is optimization on three-dimensional subspace. It is the most complex step. Note,
even if the original problem is quasar convex, the auxiliary problem may not have good properties.
It is the bottleneck of this method, and it will be discussed in Section 4 for convex case."
IMPLEMENTATION/METHODS,0.11004784688995216,"The final steps are calculation of new point and update of new weight for direction of weighted
gradient sum. This step does not require additional computations. The convergence per iterations
for this method is presented in Narkiss & Zibulevsky (2005). This result were recently generalized
for quasar-convex case S. Guminov (2008). Further, it was proven that this method is robust for
inexactness in gradient in Kuruzov & Stonyakin (2021)."
IMPLEMENTATION/METHODS,0.11244019138755981,"Algorithm 2 A modification of Nemirovski’s Conjugate Gradient method with an inexact gradient
Require: objective function f with an inexact gradient g, initial point x0, number of iterations T."
IMPLEMENTATION/METHODS,0.11483253588516747,"1: q0 = 0
2: for k = 1, . . . , T −1 do
3:
Solve 2-dimensional problem"
IMPLEMENTATION/METHODS,0.11722488038277512,"ˆxk ←arg min
x∈Xk f(x),
where Xk = x0 + Lin(xk −x0, qk)
(7)"
IMPLEMENTATION/METHODS,0.11961722488038277,"4:
Make gradient step: xk = ˆxk −
1
2Lg(ˆxk)
5:
qk = qk−1 + g(ˆxk)
return xT"
IMPLEMENTATION/METHODS,0.12200956937799043,"Another considered in S. Guminov (2008) method was Nemirovski’s Conjugate Gradient Method
(see Algorithm 2). It is well-known method with enough high performance in practice. Besides, it
is known that this method has close form for quadratic minimization problem and it is optimal for
them. There are different variants of generalization of such method for non-quadratic problem. In
this paper, we consider Nemirovski’s Conjugate Gradient Method Nemirovsky & Yudin (1979a). In
Nemirovsky & Yudin (1979a) theoretical convergence rate of CG was consequence of the following
properties: 1) smoothness of function, 2) strong-convexity of function, 3) orthogonality of gradient
at current point and direction from start point to current, 4) orthogonality of gradient at current point
and sum of gradients from all previous iterations."
IMPLEMENTATION/METHODS,0.12440191387559808,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.12679425837320574,"In S. Guminov (2008), it was proven that this method converge if to replace strong-convexity condi-
tion by quasar-convexity and quadratic growth condition. The last two conditions are consequence
of optimization on 2-dimensional subspace (see Step 3 in Algorithm 2). Nevertheless, this orthogo-
nality will be inexact in the case of inexact solution of the auxiliary subproblem. The next section is
devoted to this problem"
IMPLEMENTATION/METHODS,0.1291866028708134,"3
CONVERGENCE OF CG METHOD"
IMPLEMENTATION/METHODS,0.13157894736842105,"3.1
CONVERGENCE WITH INEXACT GRADIENT"
IMPLEMENTATION/METHODS,0.1339712918660287,"In work S. Guminov (2008) the authors obtained the result for convergence rate of Nemirovski’s
Conjugate Gradient Method. In this work, we show that the method 2 can work with additively
inexact gradient too when its inexactness is not large. To do this, we need the following auxiliary
lemma.
Lemma 4. Let the objective function f be L-smooth and γ-quasar-convex with respect to x∗. Also
for the inexact gradient g : Rn →Rn there is some constant δ1 ≥0 such that for all x ∈Rn:
∥g(x) −∇f(x)∥≤δ1.
(8)
Then the following inequality holds:"
IMPLEMENTATION/METHODS,0.13636363636363635,"∥qT ∥≤3δ1T + T
X"
IMPLEMENTATION/METHODS,0.13875598086124402,"k=0
∥g(ˆxk)∥2
! 1"
IMPLEMENTATION/METHODS,0.14114832535885166,"2
.
(9)"
IMPLEMENTATION/METHODS,0.14354066985645933,"Using Lemma 4 we can generalize the result of Theorem 2 from work S. Guminov (2008) for the
case of inexact gradient. Finally, we have the following result.
Theorem 5. Let the objective function f be L-smooth and γ-quasar-convex with respect to x∗. Also
for the inexact gradient g : Rn →Rn there is some constant δ1 ≥0 such that for all x ∈Rn
∥g(x) −∇f(x)∥≤δ1. Moreover, function satisfied condition of quadratic growth f(x) −f ∗≥
µ
2 ∥x −x∗∥. Then if on all iterations ∥g(ˆxk)∥≥2δ1 the CG obtain xT such that"
IMPLEMENTATION/METHODS,0.145933014354067,f(xT ) −f ∗≤βϵ0 + 4 γ r2ε0
IMPLEMENTATION/METHODS,0.14832535885167464,"µ δ1.
(10) after T ="
IMPLEMENTATION/METHODS,0.1507177033492823,"&
2
γβ s"
IMPLEMENTATION/METHODS,0.15311004784688995,2(1 −β)L µ ' .
IMPLEMENTATION/METHODS,0.15550239234449761,"iterations, where R = ∥x∗−x0∥, ϵ0 = f(x0) −f ∗and any constant parameter β ∈(0, 1).
Remark 6. Note, that quadratic growth condition is met when the object function f satisfies well-
known PL-condition equation 4. It’s well known Nesterov & Polyak (2006); Karimi et al. (2016);
Gasnikov (2017) that under additional smoothness assumptions standard non-accelerated iterative
methods for such functions(Gradient Descent, Cubic Regularized Newton method etc.) converge as
if f to be µ-strongly convex function. For accelerated methods, such results are not known. So we
were motivated to find such additional sufficient conditions that guarantee convergence for properly
chosen accelerated methods. In this section we observe that such a condition could be α-weakly-
quasi-convexity of f.
Remark 7. Note, if function f meets PL-condition equation 4 and we will stop our method when
∥g(xk)∥≤2δ1 then we have that f(xk) −f ∗≤4δ2
1
µ . Note, that in work Vorontsova E.A. & F.S."
IMPLEMENTATION/METHODS,0.15789473684210525,"(2021) authors proved that there are no methods that can converges better than O

δ2
1
µ

in general
case."
IMPLEMENTATION/METHODS,0.16028708133971292,"3.2
NEMIROVSKI’S CG METHOD WITH RESTARTS"
IMPLEMENTATION/METHODS,0.16267942583732056,"It is well-known that restart technique can significantly improve convergence of conjugate gradient
methods. To use that, we need to run the algorithm for T iterations, and after that start the same
algorithm from the final point after T iterations (see Algorithm 3)."
IMPLEMENTATION/METHODS,0.16507177033492823,"Similar to S. Guminov (2008), we can obtain the following result."
IMPLEMENTATION/METHODS,0.1674641148325359,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.16985645933014354,"Algorithm 3 Restarted Nemirovski’s Conjugate Gradient method with an inexact gradient
Require: objective function f with an inexact gradient g, initial point x0, number of iterations T,
number of restarts K.
1: q0 = 0
2: for k = 1, . . . , K −1 do
3:
Run Algorithm 2 for start point xk−1 and T iterations:"
IMPLEMENTATION/METHODS,0.1722488038277512,"xk = CG(f, g, xk−1, T)"
IMPLEMENTATION/METHODS,0.17464114832535885,return xK
IMPLEMENTATION/METHODS,0.17703349282296652,"Theorem 8. Let the objective function f be L-smooth and γ-quasar-convex with respect to x∗.
Also for the inexact gradient g : Rn →Rn there is some constant δ1 ≥0 such that for all x ∈
Rn ∥g(x) −∇f(x)∥≤δ1 and δ2
1 ≤
γ2α2µε"
IMPLEMENTATION/METHODS,0.17942583732057416,"32
for some α ∈(0, 1). Moreover, function satisfied
the condition of quadratic growth f(x) −f ∗≥
µ
2 ∥x −x∗∥. Then if on all iterations condition
∥g(xk)∥≥2δ1 is met the CG obtain outer point ˆx such that"
IMPLEMENTATION/METHODS,0.18181818181818182,"f(ˆx) −f ∗≤ε.
(11) after"
IMPLEMENTATION/METHODS,0.18421052631578946,"K =

2
1 −α log 1 ε "
IMPLEMENTATION/METHODS,0.18660287081339713,"restarts and T = &
8
γ s L
µ"
IMPLEMENTATION/METHODS,0.18899521531100477,"√1 + α 1 −α ' ,"
IMPLEMENTATION/METHODS,0.19138755980861244,"iterations, where ϵ0 = f(x0) −f ∗."
IMPLEMENTATION/METHODS,0.1937799043062201,"Proof. We have that the method may degrade the quality on function for enough large δ1. At the
same time, in the case
4
γ r2ε0"
IMPLEMENTATION/METHODS,0.19617224880382775,"µ δ1 ≤αε0,
(12)"
IMPLEMENTATION/METHODS,0.19856459330143542,"for some constant α ∈(0, 1) we have that"
IMPLEMENTATION/METHODS,0.20095693779904306,"f(xT ) −f ∗≤˜βϵ0 after T = &
8
γ s L
µ"
IMPLEMENTATION/METHODS,0.20334928229665072,√1 + α 1 −α ' .
IMPLEMENTATION/METHODS,0.20574162679425836,"iterations, where ˜β = 1+α"
IMPLEMENTATION/METHODS,0.20813397129186603,"2 . Note, that condition equation 12 can be rewritten in the following form:"
IMPLEMENTATION/METHODS,0.21052631578947367,"δ2
1 ≤γ2α2µε0"
IMPLEMENTATION/METHODS,0.21291866028708134,"32
.
(13)"
IMPLEMENTATION/METHODS,0.215311004784689,"So, to approach quality ε we need to require condition equation 13 for ε in the following form:"
IMPLEMENTATION/METHODS,0.21770334928229665,"δ2
1 ≤γ2α2µε"
IMPLEMENTATION/METHODS,0.22009569377990432,"32
.
(14)"
IMPLEMENTATION/METHODS,0.22248803827751196,"In this case, after"
IMPLEMENTATION/METHODS,0.22488038277511962,"K =
 log ε"
IMPLEMENTATION/METHODS,0.22727272727272727,log 1−α 2
IMPLEMENTATION/METHODS,0.22966507177033493,"
≤

2
1 −α log 1 ε "
IMPLEMENTATION/METHODS,0.23205741626794257,restarts the method obtains a point xT K such that:
IMPLEMENTATION/METHODS,0.23444976076555024,f(xT K) −f ∗≤ε.
IMPLEMENTATION/METHODS,0.23684210526315788,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.23923444976076555,"Remark 9. Generally, we can obtain that after K restarts and KT general number of iterations we
obtain the point ˆx such that"
IMPLEMENTATION/METHODS,0.24162679425837322,f(ˆxT ) −f ∗≤βNε0 + 
IMPLEMENTATION/METHODS,0.24401913875598086,"
N−1
X"
IMPLEMENTATION/METHODS,0.24641148325358853,"j=0
βj  4 γ r2ϵ0 µ δ1, or"
IMPLEMENTATION/METHODS,0.24880382775119617,"f(xT ) −f ∗≤βNϵ0 +
4
γ(1 −β) r2ϵ0 µ δ1."
IMPLEMENTATION/METHODS,0.2511961722488038,"Here we can see it cannot be guaranteed that the Nemirovski’s Conjugate Gradient method will
converge to a quality better than O
q ε0"
IMPLEMENTATION/METHODS,0.2535885167464115,"µ δ1

."
IMPLEMENTATION/METHODS,0.25598086124401914,"Remark 10. The algorithm 2 requires the total number of gradient computations O
q"
IMPLEMENTATION/METHODS,0.2583732057416268,"L
µ log 1 ε
"
IMPLEMENTATION/METHODS,0.2607655502392344,to approach quality ε.
IMPLEMENTATION/METHODS,0.2631578947368421,"As we mentioned above, the first-order methods can not approach quality better than O( δ2
1
µ ) for
strong-convex function. Consequently, it is true for functions that meet PL-condition equation 4 or
quadratic growth condition equation 5. So, let us consider estimate f(xk) −f ∗≤δ2
1
µ acceptable for
the function level and agree to terminate algorithm 2 if the condition ∥g(xk)∥≤8"
IMPLEMENTATION/METHODS,0.26555023923444976,γ δ1 is satisfied.
IMPLEMENTATION/METHODS,0.2679425837320574,"Finally, let us state the following results about work of method with stop condition."
IMPLEMENTATION/METHODS,0.2703349282296651,"Theorem 11. Let the objective function f be L-smooth and γ-quasar-convex with respect to x∗.
Also for the inexact gradient g : Rn →Rn there is some constant δ1 ≥0 such that for all x ∈Rn
∥g(x) −∇f(x)∥≤δ1. Moreover, function satisfied PL-condition equation 4."
IMPLEMENTATION/METHODS,0.2727272727272727,Let one of the following alternatives hold:
IMPLEMENTATION/METHODS,0.2751196172248804,1. The Nemirovski’s Conjugate Gradient method 2 makes
IMPLEMENTATION/METHODS,0.27751196172248804,"K =

2
1 −α log 1 ε "
IMPLEMENTATION/METHODS,0.2799043062200957,"restarts and T = &
8
γ s L
µ"
IMPLEMENTATION/METHODS,0.2822966507177033,"√1 + α 1 −α ' ,"
IMPLEMENTATION/METHODS,0.284688995215311,"iterations per each restart, where ε =
64
γ2µδ2
1"
IMPLEMENTATION/METHODS,0.28708133971291866,"2. For some iteration N ≤N ∗, at the N-th iteration of Nemirovski’s Conjugate Gradient
method 2, stopping criterion ∥g(xN)∥≤8"
IMPLEMENTATION/METHODS,0.2894736842105263,γ δ1 is satisfied for the first time.
IMPLEMENTATION/METHODS,0.291866028708134,"Then for the output point bx (bx = xN or bx = xN∗) of Nemirovski’s Conjugate Gradient method 2,
the following inequalities hold:"
IMPLEMENTATION/METHODS,0.2942583732057416,"f(bx) −f ∗⩽64δ2
1
γ2µ ,"
IMPLEMENTATION/METHODS,0.2966507177033493,"We can see that restarts technique allows obtaining optimal convergence rate for considered non-
convex case. Nevertheless, in this case we have additional parameter for tuning - frequency of
restarts."
IMPLEMENTATION/METHODS,0.29904306220095694,"4
AUXILIARY LOW-DIMENSIONAL SUBPROBLEMS"
IMPLEMENTATION/METHODS,0.3014354066985646,"In this section, we assume that all auxiliary subproblems in Algorithms 1 and 2 are convex. Namely,
subproblems in step 4 of Algorithm 1 and in step 3 of Algorithm 2 are convex."
IMPLEMENTATION/METHODS,0.3038277511961722,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.3062200956937799,"4.1
ELLIPSOID METHODS FOR SESOP"
IMPLEMENTATION/METHODS,0.30861244019138756,"To estimate the number of gradient calculations, we need to choose some procedure for optimization
on subspace on the second string of algorithm 1. It is the three-dimensional problem, so we can
use some methods for low-dimensional problems. Examples of such methods are ellipsoid method
(see Nemirovski et al. (2010)), Vaidya method (see Vaidya (1996)) and Dichotomy methods for
hypercube (see Gladin et al. (2020)). For all these methods there are results of method works with
inexact gradient (see Gladin et al. (2020)). The Dichotomy method has worse estimate for number
of calculations than other methods. Nevertheless, it demonstrates enough good performance for
two dimensional case. Therefore, we consider it for CG method below. The Ellipsoid and Vaid’s
methods require O

log 1"
IMPLEMENTATION/METHODS,0.31100478468899523,"ε

number of gradient calculations. So in current work we chose Ellipsoid
method (see 4) for subproblem."
IMPLEMENTATION/METHODS,0.3133971291866029,"For Ellipsoid Methods there is the following estimate (see Theorem 2 in Gladin et al. (2020)). If
algorithm 4 was run on a ball B ⊂Rn in n-dimensional space of radius R, the constant B is such that
maxx f(x) −minx f(x) ≤B then the Ellispoid method with δ-subgradient converges to solution
with the following speed:"
IMPLEMENTATION/METHODS,0.3157894736842105,"f(xN) −f(x∗) ≤B exp

−N 2n2"
IMPLEMENTATION/METHODS,0.3181818181818182,"
+ δ
(15)"
IMPLEMENTATION/METHODS,0.32057416267942584,"In our case n is equal to 3, dimension of subproblem. So, according to equation 15 when δ ≤ε"
IMPLEMENTATION/METHODS,0.3229665071770335,"2 to
approach the quality ε we need"
IMPLEMENTATION/METHODS,0.3253588516746411,N ≥18 ln 2B
IMPLEMENTATION/METHODS,0.3277511961722488,"ε
(16)"
IMPLEMENTATION/METHODS,0.33014354066985646,iterations of method 4.
IMPLEMENTATION/METHODS,0.33253588516746413,"In this section, we will estimate the work of SESOP algorithm in two modes:"
IMPLEMENTATION/METHODS,0.3349282296650718,"• One has exact low-dimensional gradient (gradient for subproblem) but there is only inexact
gradient for full problem
• One has only inexact gradient both in low-dimensional problem and full problem
Theorem 12. Let inexact gradient required condition equation 8 with δ1 ≤
ε
R"
IMPLEMENTATION/METHODS,0.3373205741626794,"γ +10. Also, let us"
IMPLEMENTATION/METHODS,0.3397129186602871,"assume that we have the ball Bk
R ⊂R3 with radius R on each iteration such that τk ∈Bj
R. If we can
use exact gradient of function fk than to approach quality ε on initial problem by SESOP method
one requires not more than N = &s 40LR2 γ2ε '"
IMPLEMENTATION/METHODS,0.34210526315789475,of inexact gradient calculations with respect to x and not more than
IMPLEMENTATION/METHODS,0.3444976076555024,"M =

18N ln 12800LBCN ε4 "
IMPLEMENTATION/METHODS,0.34688995215311,of exact gradient calculations with respect to τ where
IMPLEMENTATION/METHODS,0.3492822966507177,"B = max
k=1,N
max
τ∈Bj
R
fj(τ) −f ∗"
IMPLEMENTATION/METHODS,0.35167464114832536,"CN = 1 +
r"
IMPLEMENTATION/METHODS,0.35406698564593303,"max
k=1,N
(∥Dk∥∥τk∥) +
r"
IMPLEMENTATION/METHODS,0.35645933014354064,"∥max
k=1,N
d1
k−1∥+ max
k=1,N
∥d3
k∥."
IMPLEMENTATION/METHODS,0.3588516746411483,"Remark 13. Note, that the SESOP in such implementation requires O
q LR2 ε"
IMPLEMENTATION/METHODS,0.361244019138756,"
inexact gradient"
IMPLEMENTATION/METHODS,0.36363636363636365,"calculations with respect to x and O
q LR2"
IMPLEMENTATION/METHODS,0.3660287081339713,"ε
ln 1 ε"
IMPLEMENTATION/METHODS,0.3684210526315789,"
inexact gradient calculations with respect to τ."
IMPLEMENTATION/METHODS,0.3708133971291866,"Remark 14. The main theoretical advantage of SESOP with inexact gradient is that there is no
additive part depends on maxk Rk as in early works. It approaches through solving additional
low-dimensional subproblem. Nevertheless, it leads to requirements for high accuracy of solution
of auxiliary problem."
IMPLEMENTATION/METHODS,0.37320574162679426,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.37559808612440193,"Further, let us consider the case of inexact gradient in internal problems. In this case, the quality of
subproblem solution can not be better than inexactness of gradient.
Theorem 15. Let us assume that we have the ball Bk
R ⊂R3 with radius R on each iteration such
that τk ∈Bj
R. Let inexact gradient require condition equation 8 with"
IMPLEMENTATION/METHODS,0.37799043062200954,"δ1 ≤min (
ε
R"
IMPLEMENTATION/METHODS,0.3803827751196172,"γ + 10,
ε4"
IMPLEMENTATION/METHODS,0.3827751196172249,6400AkL ) (17)
IMPLEMENTATION/METHODS,0.38516746411483255,"where AN = CN maxk=1,N ∥Dk∥2 for Ck defined as in Theorem 12."
IMPLEMENTATION/METHODS,0.3875598086124402,Then to approach quality ε on initial problem by SESOP method one requires not more than N = &s 40LR2 γ2ε '
IMPLEMENTATION/METHODS,0.38995215311004783,of inexact gradient calculations with respect to x and not more than
IMPLEMENTATION/METHODS,0.3923444976076555,"M =

18N ln 12800LBCN ε4 "
IMPLEMENTATION/METHODS,0.39473684210526316,of inexact gradient calculations with respect to τ.
IMPLEMENTATION/METHODS,0.39712918660287083,"We can see that the number of gradient calculation is almost the same as in previous theorem in
case of exact low-dimensional gradient but in this case we have significantly more strong conditions
equation 17. This condition allows testing inequalities equation 37 and equation 38 and to approach
quality equation 39 in subproblem. But in this case we can see that the inexactness gradient should
be not more than O(ε4)."
IMPLEMENTATION/METHODS,0.39952153110047844,"Note, that the advantages of SESOP method leads to requirements for extra low inexactness for both
gradient inexactness and solution of auxiliary problem. Nevertheless, the required inexactness can
be easily controlled through values dJ
k during the algorithm work."
IMPLEMENTATION/METHODS,0.4019138755980861,"4.2
MULTIDIMENSIONAL DICHOTOMY"
IMPLEMENTATION/METHODS,0.4043062200956938,"Ellipsoid method can be applied for problems in CG method too. Nevertheless, in Gladin et al.
(2020) it was shown that there is another effective method for two-dimensional subproblem. It is
generalization of one-dimensional dichotomy. Despite the little worse convergence rate in compar-
ison with Ellipsoid method, it demonstrates better performance. So, we provide the result for CG
method with two-dimensional dichotomy in the following theorem.
Theorem 16. Let assumptions of Theorem 11 hold and all subproblems are convex. Besides, there
is R such that ˆxk −xk ∈BRx for all k. Each point ˆxk is output of two-dimensional dichotomy
algorithm (see Gladin et al. (2020)) after M steps, where M is given by: M = &"
IMPLEMENTATION/METHODS,0.40669856459330145,"16

ln CRx ε4 2'"
IMPLEMENTATION/METHODS,0.4090909090909091,Let one of the following alternatives hold:
IMPLEMENTATION/METHODS,0.41148325358851673,1. The Nemirovski’s Conjugate Gradient method 2 makes
IMPLEMENTATION/METHODS,0.4138755980861244,"K =

2
1 −α log 1 ε "
IMPLEMENTATION/METHODS,0.41626794258373206,"restarts and T = &
8
γ s L
µ"
IMPLEMENTATION/METHODS,0.41866028708133973,"√1 + α 1 −α ' ,"
IMPLEMENTATION/METHODS,0.42105263157894735,"iterations per each restart, where ε =
64
γ2µδ2
1"
IMPLEMENTATION/METHODS,0.423444976076555,"2. For some iteration N ≤N ∗, at the N-th iteration of Nemirovski’s Conjugate Gradient
method 2, stopping criterion ∥g(xN)∥≤8"
IMPLEMENTATION/METHODS,0.4258373205741627,γ δ1 is satisfied for the first time.
IMPLEMENTATION/METHODS,0.42822966507177035,Under review as a conference paper at ICOMP 2024
IMPLEMENTATION/METHODS,0.430622009569378,"Then for the output point bx (bx = xN or bx = xN∗) of Nemirovski’s Conjugate Gradient method 2,
the following inequalities hold:"
IMPLEMENTATION/METHODS,0.43301435406698563,"f(bx) −f ∗⩽64δ2
1
γ2µ ."
IMPLEMENTATION/METHODS,0.4354066985645933,"As the result the algorithm requires not more N of calculations of inexact gradient with respect to x
and MN = O(ln3(1/ε)) of low-dimensional inexact gradient calculations."
NUMERICAL EXPERIMENTS,0.43779904306220097,"5
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.44019138755980863,"In this section, we present some preliminary numerical results. We compare SESOP method with
Ellipsoid methods for auxiliary subproblem (see Algorithms 1 and 4), CG with restarts with differ-
ent methods for subproblems (see Algorithms 3, 4 and Gladin et al. (2020))) and Similar Triangle
method (see Gasnikov & Nesterov (2018))."
NUMERICAL EXPERIMENTS,0.44258373205741625,We consider the problem of logistic regression:
NUMERICAL EXPERIMENTS,0.4449760765550239,"f(x) = (1/m) m
X"
NUMERICAL EXPERIMENTS,0.4473684210526316,"j=1
log(1 + exp(−yj⟨fj, x⟩)) + µ∥x∥2
(18)"
NUMERICAL EXPERIMENTS,0.44976076555023925,"on synthetic data in dimensions n = 100, m = 200. In all cases we considered constant inexact-
ness with different norm δ1. Parameter for restarts of CG, Lipschitz constant and strong convexity
parameter were found analytically. It was found that all method approach the close accuracy in our
problem settings. So, in this work we demonstrate time comparison (see 1). The presented time is
time required to approach quality 10δ2
1/µ"
NUMERICAL EXPERIMENTS,0.45215311004784686,"δ1
SESOP
CG+Ellipsoids
CG+Dichotomy
STM
10−3
1
1.4
0.9
1.7
10−5
10.1
15.3
9.5
13.8
10−7
35.3
60.9
36.8
42.1"
NUMERICAL EXPERIMENTS,0.45454545454545453,Table 1: Time comparison (s) for problem equation 18
NUMERICAL EXPERIMENTS,0.4569377990430622,"We can see that multidimensional dichotomy works better than Ellipsoid method for Nemirovski’s
Conjugate Gradient Method in all cases. At the same time, methods based on subspace optimization
outperforms STM method. The result for CG and SESOP method are close enough."
NUMERICAL EXPERIMENTS,0.45933014354066987,CONCLUSION
NUMERICAL EXPERIMENTS,0.46172248803827753,"In this paper, one considered generalization of convexity condition that is known as quasar-convexity
or weakly-quasi-convexity. We propose modification of Nemirovski’s Conjugate Gradient Method
with a δ-additive noise in the gradient equation 2 for γ-quasar convex functions satisfying quadratic
growth condition."
NUMERICAL EXPERIMENTS,0.46411483253588515,"We estimate computational complexity for solving the internal auxiliary problem in SESOP and CG
method. For this, we used well-known low-dimensional optimization methods – Ellipsoid Method
and generalization of dichotomy. We prove that these methods do not significantly increase com-
plexity for convex case. Besides, these methods are still the methods of the first order."
NUMERICAL EXPERIMENTS,0.4665071770334928,"Moreover, we provide numerical experiments which demonstrate the effectiveness of the approaches
proposed in this paper."
REFERENCES,0.4688995215311005,REFERENCES
REFERENCES,0.47129186602870815,"Mathieu Barre, Adrien Taylor, and Alexandre d’Aspremont. Complexity guarantees for polyak steps
with momentum, 02 2020."
REFERENCES,0.47368421052631576,"A. Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics. 01
2017."
REFERENCES,0.47607655502392343,Under review as a conference paper at ICOMP 2024
REFERENCES,0.4784688995215311,"Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the
prism of interpolation. Acta Numerica, 30:203–248, 08 2021. doi: 10.1017/S0962492921000039."
REFERENCES,0.48086124401913877,"S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning, 8:231–357, 01 2015. doi: 10.1561/2200000050."
REFERENCES,0.48325358851674644,"O. Devolder. Exactness, inexactness and stochasticity in first-order methods for large-scale convex
optimization. Ph.D. thesis, ICTEAM and CORE, Universit‘e Catholique de Louvain, 01 2013."
REFERENCES,0.48564593301435405,"A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization
19(3), M 146(1):1171–1183, 01 2008."
REFERENCES,0.4880382775119617,"Alexandre d’Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. 5:1–245, 01
2021. doi: 10.1561/2400000036."
REFERENCES,0.4904306220095694,Alexander Gasnikov. Universal gradient descent. 11 2017.
REFERENCES,0.49282296650717705,"Alexander Gasnikov and Yu Nesterov.
Universal method for stochastic composite optimization
problems.
Computational Mathematics and Mathematical Physics, 58:48–64, 01 2018.
doi:
10.1134/S0965542518010050."
REFERENCES,0.49521531100478466,"Egor Gladin, Ilya Kuruzov, Fedor Stonyakin, Dmitry Pasechnyuk, Mohammad Alkousa, and
Alexander Gasnikov. Solving strongly convex-concave composite saddle point problems with
a small dimension of one of the variables, 10 2020."
REFERENCES,0.49760765550239233,"I. Goodfellow, Y. Bengio, and A. Courville.
Deep learning.
MIT Press, http://www.
deeplearningbook.org, 01 2016."
REFERENCES,0.5,"Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
Journal of Machine Learning Research, 19, 09 2016."
REFERENCES,0.5023923444976076,"Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing star-convex
functions and beyond, 06 2020."
REFERENCES,0.5047846889952153,"Sergey Kabanikhin. Inverse and ill-posed problems: Theory and applications. 01 2011. doi: 10.
1515/9783110224016."
REFERENCES,0.507177033492823,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-Łojasiewicz condition. volume 9851, pp. 795–811, 09 2016.
ISBN 978-3-319-46127-4. doi: 10.1007/978-3-319-46128-1 50."
REFERENCES,0.5095693779904307,"Ilya Kuruzov and Fedor Stonyakin. Sequential Subspace Optimization for Quasar-Convex Opti-
mization Problems with Inexact Gradient, pp. 19–33. 12 2021. ISBN 978-3-030-92710-3. doi:
10.1007/978-3-030-92711-0 2."
REFERENCES,0.5119617224880383,"W¿ Haskell L. Hien, R. Zhao. An inexact primal-dual smoothing framework for large-scale non-
bilinear saddle point problems. ournal of Optimization Theory and Applications. 10.1007/s10957-
023-02351-9, 01 2023."
REFERENCES,0.5143540669856459,"G. Lan.
First-order and stochastic optimization methods for machine learning.
Switzerland:
Springer Series in the Data Sciences, 01 2020."
REFERENCES,0.5167464114832536,"Guy Narkiss and Michael Zibulevsky. Sequential subspace optimization method for large-scale
unconstrained problems. 01 2005."
REFERENCES,0.5191387559808612,"Arkadi Nemirovski, Shmuel Onn, and Uriel Rothblum. Accuracy certificates for computational
problems with convex structure. Math. Oper. Res., 35:52–78, 02 2010. doi: 10.1287/moor.1090.
0427."
REFERENCES,0.5215311004784688,"A.S. Nemirovsky and D.B. Yudin.
Problem complexity and optimization method efficiency [in
russian]. Nauka, Moscow, 11 1979a."
REFERENCES,0.5239234449760766,"A.S. Nemirovsky and D.B. Yudin.
Problem complexity and optimization method efficiency.
Moscow, Nauka, 01 1979b."
REFERENCES,0.5263157894736842,Under review as a conference paper at ICOMP 2024
REFERENCES,0.5287081339712919,"Yurii Nesterov and Boris Polyak. Cubic regularization of newton method and its global performance.
Math. Program., 108:177–205, 08 2006. doi: 10.1007/s10107-006-0706-8."
REFERENCES,0.5311004784688995,"F. Glineur O. Devolder and Y. Nesterov. First-order methods of smooth convex optimization with
inexact oracle. Mathematical Programming M, M 146(1):37–75, 01 2014."
REFERENCES,0.5334928229665071,"B.T. Polyak. Gradient methods for minimizing functionals. Comput. Math. Math. Phys., 3:4, pp.
864 –– 878, 01 1963."
REFERENCES,0.5358851674641149,"B.T. Polyak. Introduction to optimization. Optimization Software, 01 1987."
REFERENCES,0.5382775119617225,"I. Kuruzov S. Guminov, A. Gasnikov. Accelerated methods for weakly-quasi-convex optimization
problems. Computational Management Science. 20. 10.1007/s10287-023-00468-w, 01 2008."
REFERENCES,0.5406698564593302,"Pravin Vaidya. New algorithm for minimizing convex functions over convex sets. Math Program,
73:291–341, 01 1996. doi: 10.1007/BF02592216."
REFERENCES,0.5430622009569378,"F. Vasilyev. Optimization methods. Moscow, Russia: FP, 01 2002."
REFERENCES,0.5454545454545454,"Artem Vasin, Alexander Gasnikov, and Vladimir Spokoiny. Stopping rules for accelerated gradient
methods with additive noise in gradient, 02 2021."
REFERENCES,0.5478468899521531,"Gasnikov A.V. Vorontsova E.A., Hildbrand R.F. and Stonyakin F.S. Convex optimization. Moscow,
MIPT, pp. 251, 11 2021."
REFERENCES,0.5502392344497608,"Jun-Kun Wang and Andre Wibisono. Continuized acceleration for quasar convex functions in non-
convex optimization, 02 2023."
OTHER,0.5526315789473685,Under review as a conference paper at ICOMP 2024
OTHER,0.5550239234449761,"A
CONVERGENCE OF CG METHOD"
OTHER,0.5574162679425837,"A.1
PROOF OF LEMMA 4"
OTHER,0.5598086124401914,"Proof. Note, that qT = qT −1 + 1"
OTHER,0.562200956937799,"Lg(ˆxT ) for T ≥1. So, we have the following expression for ∥qT ∥:"
OTHER,0.5645933014354066,"∥qT ∥2 = ∥g(ˆxT )∥2 + ∥qT −1∥2 + 2⟨g(ˆxT ), qT −1⟩.
Because of exact solution of auxiliary problem on each iteration, we have that"
OTHER,0.5669856459330144,∇f(ˆxT ) ⊥qT −1.
OTHER,0.569377990430622,"At the same time, we have that ∥∇f(ˆxT ) −g(ˆxT )∥2 ≤δ1. So, we have the following estimations:"
OTHER,0.5717703349282297,"∥qT ∥2 ≤ T
X"
OTHER,0.5741626794258373,"k=0
∥g(ˆxk)∥2 + 2δ1"
OTHER,0.5765550239234449,"T −1
X"
OTHER,0.5789473684210527,"k=0
∥qk∥,
(19)"
OTHER,0.5813397129186603,"and
∥qT ∥2 ≥∥qT −1∥2 −2δ1∥qT −1∥.
(20)
From the lower bound equation 20, we have that"
OTHER,0.583732057416268,"∥qT −1∥≤δ1 +
q"
OTHER,0.5861244019138756,"δ2
1 + ∥qT ∥2,"
OTHER,0.5885167464114832,"or
∥qT −1∥≤2δ1 + ∥qT ∥.
Using the inequality above, we can obtain estimation for ∥qj∥for all j:"
OTHER,0.5909090909090909,"∥qj∥≤2δ1(T −j) + ∥qT ∥.
(21)"
OTHER,0.5933014354066986,"Using inequalities equation 19 and equation 21, we have the following estimation:"
OTHER,0.5956937799043063,"∥qT ∥2 ≤ T
X"
OTHER,0.5980861244019139,"k=0
∥g(ˆxk)∥2 + 4δ2
1"
OTHER,0.6004784688995215,"T −1
X"
OTHER,0.6028708133971292,"i=0
(T −j) + 2δ1T∥qT ∥,
(22)"
OTHER,0.6052631578947368,and from equation 22 we have the following quadratic inequality on ∥qT ∥:
OTHER,0.6076555023923444,"∥qT ∥2 ≤ T
X"
OTHER,0.6100478468899522,"k=0
∥g(ˆxk)∥2 + 2δ2
1T 2 + 2δ1T∥qT ∥,
(23)"
OTHER,0.6124401913875598,Using inequality equation 23 we obtain the estimation equation 9:
OTHER,0.6148325358851675,"∥qT ∥≤3δ1T + T
X"
OTHER,0.6172248803827751,"k=0
∥g(ˆxk)∥2
! 1 2
."
OTHER,0.6196172248803827,"A.2
PROOF OF THEOREM 5"
OTHER,0.6220095693779905,"Proof. Let us assume that εT = f(xT ) −f ∗≥βε0 + cδ1, where β ∈(0, 1) and c are some
constants. Using estimation equation 31 for s0 =
1
2L we obtain the following estimation:"
OTHER,0.6244019138755981,"∥g(ˆxk)∥2 ≤4L(f(ˆxk) −f(xk+1)) + 2δ1.
Because of exact solution of auxiliary problem, we have that"
OTHER,0.6267942583732058,"∥g(ˆxk)∥2 ≤4L(εk −εk+1) + 2δ1.
(24)"
OTHER,0.6291866028708134,"Telescoping inequality above, we obtain the following inequality:"
OTHER,0.631578947368421,"T −1
X"
OTHER,0.6339712918660287,"k=0
∥g(ˆxk)∥2 ≤4L(ε0 −εT ) ≤4L(1 −β)ε0.
(25)"
OTHER,0.6363636363636364,Under review as a conference paper at ICOMP 2024
OTHER,0.638755980861244,"On the other hand, from quasar-convexity we have the following estimation:"
OTHER,0.6411483253588517,f(ˆxk) −f ∗≤1
OTHER,0.6435406698564593,"γ ⟨∇f(ˆxk), ˆxk −x∗⟩."
OTHER,0.645933014354067,Because of exact solution of auxiliary problem. we have the following inequality:
OTHER,0.6483253588516746,f(ˆxk) −f ∗≤1
OTHER,0.6507177033492823,"γ ⟨∇f(ˆxk), x0 −x∗⟩."
OTHER,0.65311004784689,"Similarly to proof of results for SESOP, we obtain the final estimations:"
OTHER,0.6555023923444976,f(ˆxk) −f ∗≤1
OTHER,0.6578947368421053,"γ ⟨∇g(ˆxk), x0 −x∗⟩+ δ1
R γ ."
OTHER,0.6602870813397129,"Note, that from equation 24 and condition ∥g(ˆxk)∥≥2δ1, we have that f(xk+1) ≤f(ˆxk). By
construction of ˆxk, we have that f(ˆxk) ≥f(xk). So, when εT = f(xT ) −f ∗≥βε0 + cδ1 the
following inequality holds:"
OTHER,0.6626794258373205,βε0 + cδ1 ≤1
OTHER,0.6650717703349283,"γ ⟨∇g(ˆxk), x0 −x∗⟩+ δ1
R γ ."
OTHER,0.6674641148325359,"When one sum up this inequalities above, we obtain:"
OTHER,0.6698564593301436,Tβε0 + cTδ1 ≤1
OTHER,0.6722488038277512,"γ ⟨qT , x0 −x∗⟩+ δ1
RT γ ,"
OTHER,0.6746411483253588,"−∥qT ∥∥x0 −x∗∥≤−Tγβε0 + δ1T (R + cγ) .
(26)"
OTHER,0.6770334928229665,"Firstly, from Lemma 4 and inequality equation 25, we have that:"
OTHER,0.6794258373205742,"∥qT ∥≤3δ1T +
p"
OTHER,0.6818181818181818,"4L(1 −β)ε0 −4Lcδ1.
(27)"
OTHER,0.6842105263157895,"On the other hand, from quadratic growth we can obtain the following estimation for ∥x0 −x∗∥:"
OTHER,0.6866028708133971,"∥x0 −x∗∥≤
r2ε0"
OTHER,0.6889952153110048,"µ .
(28)"
OTHER,0.6913875598086124,"Uniting inequalities equation 26-equation 28 we obtain the following inequalities for T:

3δ1T +
p"
OTHER,0.69377990430622,"4L(1 −β)ε0
 r2ε0"
OTHER,0.6961722488038278,"µ
≥Tγβε0 + δ1T (−R + cγ)
(29)"
OTHER,0.6985645933014354,Let us rewrite equation 29 in the following form:
OTHER,0.7009569377990431,"Tγβε0 + δ1T

cγ −R −3
r2ε0 µ"
OTHER,0.7033492822966507,"
≤2ε0 s"
OTHER,0.7057416267942583,"2(1 −β)L µ
."
OTHER,0.7081339712918661,"So, when c ≥1"
OTHER,0.7105263157894737,"γ

R + 3
q 2ε0"
OTHER,0.7129186602870813,"µ

, we have the following estimation on T: T ≤2 γβ s"
OTHER,0.715311004784689,"2(1 −β)L µ
."
OTHER,0.7177033492822966,"So, after T =

2
γβ q"
OTHER,0.7200956937799043,2(1−β)L µ
OTHER,0.722488038277512,"
, we have that εT ≤βε0+cδ1. Finally, note, that because of quadratic"
OTHER,0.7248803827751196,growth we have estimation µR2
OTHER,0.7272727272727273,"2
≥ε0. So, we have that after T iterations we have a point xT that
meets the following estimation:"
OTHER,0.7296650717703349,f(xT ) −f ∗≤βε0 + 4 γ r2ε0 µ δ1.
OTHER,0.7320574162679426,Under review as a conference paper at ICOMP 2024
OTHER,0.7344497607655502,"B
SOME RESULTS FOR SESOP METHOD"
OTHER,0.7368421052631579,"On the base of the last inequality and the right part of equation 32 for y := xk + s0g(xk) and
x = xk we can conclude that"
OTHER,0.7392344497607656,"f(xk+1) ≤f(xk) +

s0 + s2
0L

∥g(xk)∥2 + 1"
OTHER,0.7416267942583732,"2Lδ2
1
(30)"
OTHER,0.7440191387559809,"for each s0 ∈R. Further,"
OTHER,0.7464114832535885,"−

s0 + s2
0L

∥g(xk)∥2 ≤f(xk) −f(xk+1) + 1"
OTHER,0.7488038277511961,"2Lδ2
1.
(31)"
OTHER,0.7511961722488039,"f(xk+1) = min
s∈R3 f  xk +"
OTHER,0.7535885167464115,"2
X"
OTHER,0.7559808612440191,"i=0
sidi
k !"
OTHER,0.7583732057416268,"≤f (xk + s0g(xk)) .
(32)"
OTHER,0.7607655502392344,"Theorem 17. Let the objective function f be L-smooth and γ-quasar-convex with respect to x∗. Let
τk be the step value obtained with the inexact solution of the auxiliary problem equation 6 on step 2
in Algorithm 1 on the k-th iteration. Namely, the following conditions for inexactness hold:"
OTHER,0.7631578947368421,"(i) For the inexact gradient g : Rn →Rn there is some constant δ1 ≥0 such that for all
points x ∈Rn condition equation 8 holds."
OTHER,0.7655502392344498,(ii) The inexact solution τk meets the following condition:
OTHER,0.7679425837320574,"|

∇f(xk), d2
k−1

| ≤k2δ2
(33)"
OTHER,0.7703349282296651,for some constant δ2 ≥0 and each k ∈N. Note that xk = xk−1 + Dk−1τk−1.
OTHER,0.7727272727272727,(iii) The inexact solution τk meets the following condition for some constant δ3 ≥0:
OTHER,0.7751196172248804,"| ⟨∇f(xk), xk −x0⟩| ≤δ3.
(34)"
OTHER,0.777511961722488,"(iv) The problem from step 2 in Algorithm 1 is solved with accuracy δ4 ≥0 on the function on
each iteration, i.e. f(xk) −minτ∈Rn f(xk−1 + Dk−1τ) ≤δ4."
OTHER,0.7799043062200957,Then the sequence {xk} generated by Algorithm 1 satisfies
OTHER,0.7822966507177034,f(xk) −f ∗≤8LR2
OTHER,0.784688995215311,"γ2k2 +
R"
OTHER,0.7870813397129187,"γ + 10

δ1 + 4
p"
OTHER,0.7894736842105263,δ2 + δ3 + 5 r Lδ4
OTHER,0.7918660287081339,"k
(35)"
OTHER,0.7942583732057417,"for each k ≥8, where R = ∥x∗−x0∥.
Theorem 18. If condition (iv) from Theorem 17 holds, then we can choose δ2, δ3 ≥0 according to
the following estimates:"
OTHER,0.7966507177033493,"δ3 ≤
p 2Lδ4 q"
OTHER,0.7990430622009569,"max
k (∥Dk∥∥τk∥) +
q"
OTHER,0.8014354066985646,"∥max
k
d1
k−1∥
"
OTHER,0.8038277511961722,"and
δ2 ≤1 k2
q"
OTHER,0.80622009569378,"2L max
k
∥d3
k∥δ4."
OTHER,0.8086124401913876,"C
AUXILLARY LOW-DIMENSIONAL SUBPROBLEMS"
OTHER,0.8110047846889952,"C.1
PROOF OF THEOREM 12"
OTHER,0.8133971291866029,"In the first case we will estimate the number of gradient calculations with respect to x and to τ
separately. In the second case we will estimate this gradients calculation in total. In the second case
we can get the inexact gradient for subproblem through full gradient calculation:"
OTHER,0.8157894736842105,"d
dτ f(xk + Dkτ) = D⊤
k ∇f(x)

x=xk+Dkτ"
OTHER,0.8181818181818182,Under review as a conference paper at ICOMP 2024
OTHER,0.8205741626794258,"for all k. In such scheme, any low-dimensional gradient calculation requires high-dimensional gra-
dient calculation because we can calculate total number of calculations."
OTHER,0.8229665071770335,"Theorem 18 gives correspondence between inaccuracies δ2, δ3 and δ4:"
OTHER,0.8253588516746412,"δ3 ≤
p 2Lδ4 q"
OTHER,0.8277511961722488,"max
k (∥Dk∥∥τk∥) +
q"
OTHER,0.8301435406698564,"∥max
k
d1
k−1∥
 and δ2 ≤1 k2
q"
OTHER,0.8325358851674641,"2L max
k
∥d3
k∥δ4."
OTHER,0.8349282296650717,The condition max
OTHER,0.8373205741626795,"(
8LR2"
OTHER,0.8397129186602871,"γ2k2 ,
R"
OTHER,0.8421052631578947,"γ + 10

δ1, 4
p"
OTHER,0.8444976076555024,"δ2, δ3, 5 r Lδ4 k ) ≤ε"
OTHER,0.84688995215311,"5,
(36)"
OTHER,0.8492822966507177,"is sufficient to approach quality ε on function. So, according to equation 36 we have the following
conditions for inexactness δ2, δ3, δ4:"
OTHER,0.8516746411483254,δ2 ≤ε2
OTHER,0.854066985645933,"400
(37) δ3 ≤ε"
OTHER,0.8564593301435407,"5
(38)"
OTHER,0.8588516746411483,"δ4 ≤
ε2"
OTHER,0.861244019138756,"625L
(39)"
OTHER,0.8636363636363636,"So we obtain the main statement about quality of subproblem solution and condition for iterations
count and inexactness of gradient."
OTHER,0.8660287081339713,Lemma 19. Let inexact gradient meets condition equation 8 with
OTHER,0.868421052631579,"δ1 ≤
ε
R"
OTHER,0.8708133971291866,γ + 10.
OTHER,0.8732057416267942,"To obtain quality ε on function the SESOP method 1 with inexact gradient should have T ≥ s 40LR2 γ2ε
,"
OTHER,0.8755980861244019,"iterations and on each iteration subproblem should be solved such that conditions equation 37,
equation 38 and equation 39 are met."
OTHER,0.8779904306220095,"Moreover, uniting the conditions equation 37-equation 39 with early obtained results of Theorem 18
we can obtain sufficient quality of subproblem for obtaining these results."
OTHER,0.8803827751196173,"Lemma 20. The following quality of subproblem on kth iteration is sufficient for conditions equa-
tion 37, equation 38 and equation 39 are met:"
OTHER,0.8827751196172249,"δ4 ≤min 
 
ε4"
OTHER,0.8851674641148325,"6400L
p"
OTHER,0.8875598086124402,"maxk(∥Dk∥∥τk∥) +
q"
OTHER,0.8899521531100478,"∥maxk d1
k−1∥
,
ε2"
OTHER,0.8923444976076556,"50L maxk ∥d3
k∥,
ε2"
OTHER,0.8947368421052632,"625L 
 ."
OTHER,0.8971291866028708,"In other words, we need to solve on each iteration auxiliary problem with accuracy O

ε4
LCk"
OTHER,0.8995215311004785,"
where"
OTHER,0.9019138755980861,"constant Ck = 1 +
p"
OTHER,0.9043062200956937,"maxk(∥Dk∥∥τk∥) +
q"
OTHER,0.9066985645933014,"∥maxk d1
k−1∥+ maxk ∥d3
k∥is defined by generated
by algorithm sequence {xk}."
OTHER,0.9090909090909091,"This quality is the worst case when the subproblem procedure will be stopped. One supposes that
for the most problems conditions equation 37, equation 38 and equation 39 will be met significantly
early."
OTHER,0.9114832535885168,Under review as a conference paper at ICOMP 2024
OTHER,0.9138755980861244,"The statements above are true for the both modes, and they allow estimating a number of exact
and inexact gradients in these cases. In the following theorem, we suppose that we have the ball
Bk
R ⊂R3 with radius R on each iteration such that τk ∈Bj
R. So for the first mode we have the
following estimations. In this case, to approach quality"
OTHER,0.916267942583732,"δ4 =
ε4"
OTHER,0.9186602870813397,6400CkL
OTHER,0.9210526315789473,"where Ck = 1 +
p"
OTHER,0.9234449760765551,"maxk(∥Dk∥∥τk∥) +
q"
OTHER,0.9258373205741627,"∥maxk d1
k−1∥+ maxk ∥d3
k∥we need"
OTHER,0.9282296650717703,18 ln 12800LBkCk ε4
OTHER,0.930622009569378,"iterations of ellispoid method where Bk = maxτ∈Bj
R fj(τ) −f ∗."
OTHER,0.9330143540669856,"C.2
PROOF OF THEOREM 15"
OTHER,0.9354066985645934,"Proof. Let us estimate inexactness in internal problems. Early, we obtained the following equality:"
OTHER,0.937799043062201,"d
dτ f(xk + Dkτ) = D⊤
k ∇f(x)

x=xk+Dkτ"
OTHER,0.9401913875598086,"So, let us consider ellipsoid method with inexact gradient in the following form:"
OTHER,0.9425837320574163,"gk(τ) = D⊤
k g(xk + Dkτ)."
OTHER,0.9449760765550239,For such gradient we have the following estimation for inexactness:
OTHER,0.9473684210526315,"∥gk(τ) −∇τfk(τ)∥≤∥Dk∥2δ1.
(40)"
OTHER,0.9497607655502392,"So to approach quality ε we need the more hard conditions for inexactness of gradient. To approach
quality δ4 by ellispoid method, we need that the following condition holds:"
OTHER,0.9521531100478469,"δ1 ≤
δ4
∥Dk∥2
."
OTHER,0.9545454545454546,"Under assumptions of Theorem 12, we have that δ4 is given by"
OTHER,0.9569377990430622,"δ4 =
ε4"
OTHER,0.9593301435406698,6400CkL
OTHER,0.9617224880382775,"is sufficient accuracy where Ck = 1 +
p"
OTHER,0.9641148325358851,"maxk(∥Dk∥∥τk∥) +
q"
OTHER,0.9665071770334929,"∥maxk d1
k−1∥+ maxk ∥d3
k∥. So,
for δ1 we have the following condition:"
OTHER,0.9688995215311005,"δ1 ≤
ε4"
OTHER,0.9712918660287081,"6400CkL∥Dk∥2
."
OTHER,0.9736842105263158,for all k or
OTHER,0.9760765550239234,"δ1 ≤
ε4"
OTHER,0.9784688995215312,6400AkL.
OTHER,0.9808612440191388,where Ak = Ck maxk ∥Dk∥2.
OTHER,0.9832535885167464,"C.3
ELLISPOIDS METHOD"
OTHER,0.9856459330143541,Under review as a conference paper at ICOMP 2024
OTHER,0.9880382775119617,"Algorithm 4 Ellipsoids Method with δ-subgradient.
Require: Number of iterations N ⩾1, δ ⩾0, ball BR ⊇Qx, its center c and radius R."
OTHER,0.9904306220095693,"1: E0 := BR,
H0 := R2In,
c0 := c.
2: for k = 0, . . . , N −1 do
3:
if ck ∈Qx then
4:
wk := w ∈∂δg(ck),
5:
if wk = 0 then return ck,
6:
else
7:
wk := w, where w ̸= 0 is such that Qx ⊂{x ∈Ek : ⟨w, x −ck⟩⩽0}.
8:
ck+1 := ck −
1
n+1
Hkwk
√"
OTHER,0.992822966507177,"wT
k Hkwk ,"
OTHER,0.9952153110047847,"9: Hk+1 :=
n2
n2−1

Hk −
2
n+1
HkwkwT
k Hk
wT
k Hkwk 
,"
OTHER,0.9976076555023924,"10: Ek+1 := {x : (x −ck+1)T H−1
k+1(x −ck+1) ⩽1},
Ensure: xN = arg
min
x∈{c0,...,cN}∩Qx g(x)."
