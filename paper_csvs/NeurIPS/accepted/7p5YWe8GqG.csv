Section,Section Appearance Order,Paragraph
TENCENT ROBOTICS X,0.0,"1Tencent Robotics X
2University of Toronto
3Vector Institute
4Tsinghua University
5Hong Kong University of Science and Technology"
ABSTRACT,0.0014005602240896359,Abstract
ABSTRACT,0.0028011204481792717,"Graph neural networks, which typically exchange information between local neigh-
bors, often struggle to capture long-range interactions (LRIs) within the graph.
Building a graph hierarchy via graph pooling methods is a promising approach
to address this challenge; however, hierarchical information propagation cannot
entirely take over the role of local information aggregation. To balance locality
and hierarchy, we integrate the local and hierarchical structures, represented by
intra- and inter-graphs respectively, of a multi-scale graph hierarchy into a single
mega graph. Our proposed MeGraph model consists of multiple layers alternating
between local and hierarchical information aggregation on the mega graph. Each
layer first performs local-aware message-passing on graphs of varied scales via
the intra-graph edges, then fuses information across the entire hierarchy along
the bidirectional pathways formed by inter-graph edges. By repeating this fusion
process, local and hierarchical information could intertwine and complement each
other. To evaluate our model, we establish a new Graph Theory Benchmark de-
signed to assess LRI capture ability, in which MeGraph demonstrates dominant
performance. Furthermore, MeGraph exhibits superior or equivalent performance
to state-of-the-art models on the Long Range Graph Benchmark. The experimental
results on commonly adopted real-world datasets further demonstrate the broad
applicability of MeGraph. 1"
INTRODUCTION,0.004201680672268907,"1
Introduction"
INTRODUCTION,0.0056022408963585435,"Graph-structured data, such as social networks, traffic networks, and biological data, are prevalent
across a plethora of real-world applications. Recently, Graph Neural Networks (GNNs) have emerged
as a powerful tool for modeling and understanding the intricate relationships and patterns present in
such data. Most existing GNNs learn graph representations by iteratively aggregating information
from individual nodes‚Äô local neighborhoods through the message-passing mechanism. Despite their
effectiveness, these GNNs struggle to capture long-range interactions (LRIs) between nodes in the
graph. For instance, when employing a 4-layer vanilla GNN on the 9-node (A to I) graph (as shown
in Fig. 1), the receptive field of node A is limited to 4-hop neighbors, making the aggregation of
information from nodes G, H, and I into node A quite challenging. While GNNs could theoretically
incorporate information from nodes n-hops away with n-layers of message passing, this often leads
to over-smoothing and over-squashing issues [17, 3] when n is large."
INTRODUCTION,0.0070028011204481795,"*Equal Contribution. Work done while HD, JX and YY are interns at Tencent Robotics X.
‚Ä†Corresponding authors, contact honghuad@cs.toronto.edu and lxhan@tencent.com.
1Project website and open-source code can be found at https://sites.google.com/view/megraph."
INTRODUCTION,0.008403361344537815,Connect
INTRODUCTION,0.00980392156862745,Select
INTRODUCTION,0.011204481792717087,"Reduce B
E
H"
INTRODUCTION,0.012605042016806723,"A
D
G
C
F
I 3
2
1 X ùíâ 1 2 3
‚Ä¶ B
E
H"
INTRODUCTION,0.014005602240896359,"A
D
G
C
F
I 3
2
1 X"
INTRODUCTION,0.015406162464985995,IntraEdges
INTRODUCTION,0.01680672268907563,"Graph Pyramid
Mega Graph"
INTRODUCTION,0.018207282913165267,InterEdges 2
INTRODUCTION,0.0196078431372549,"Figure 1: Illustration of the graph pooling operation, graph pyramid, and mega graph. Graph pooling is a
downsampling process comprising SELECT, CONNECT, and REDUCE steps. It begins by selecting subsets for
grouping and each subset collapses into a new node in the pooled graph. Next, it forms new edges by merging
the original ones, and finally calculates the pooled graph‚Äôs features. In this graph, nodes A B C, D E F, and G H
I are pooled into nodes 1, 2, and 3 respectively, while the edges (B, E) and (C, D) are merged into (1, 2). Graph
Pyramid involves multi-scaled graphs derived from iterative graph pooling, with the height indicating different
scales and h = 1 symbolizing the original graph. Mega Graph is formed by connecting the graph pyramid
using inter-graph edges, which are the by-products of graph pooling."
INTRODUCTION,0.02100840336134454,"One mainstream solution to this problem involves constructing a multi-scale graph hierarchy through
graph pooling methods. Previous efforts, such as Graph UNets [22] and HGNet [47], have attempted
to broaden the receptive field using this strategy. They downsample and upsample the graph,
aggregating information along the hierarchy. However, hierarchical information propagation cannot
take over the role of local information aggregation. To illustrate, consider the graph hierarchy depicted
in Fig. 1. The information propagated along the hierarchy from node B to nodes D, E, and F tends to
be similar since they share the common path B-1-X-2. However, in the original graph, node B holds
different degrees of importance to nodes D, E, and F as they are 2, 1, and 3 hops away respectively."
INTRODUCTION,0.022408963585434174,"To balance the importance of locality and hierarchy, we amalgamate the local and hierarchical
structures of a multi-scale graph hierarchy into a single mega graph as depicted in Fig. 1, where we
refer to the local structure as intra-graph edges and hierarchical structure as inter-graph edges. Based
on this mega graph, we introduce our MeGraph model consisting of n Mee layers. Each layer first
performs local-aware message-passing on graphs of varied scales via the intra-graph edges and then
fuses information across the whole hierarchy along the bidirectional pathways formed by inter-graph
edges. This method enables hierarchically fused information to circulate within local structures and
allows locally fused information to distribute across the hierarchy. By repeating this fusion process,
local and hierarchical information could intertwine and complement each other. Moreover, to support
flexible graph pooling ratios when constructing the multi-scale graph hierarchy, we propose a new
graph pooling method S-EdgePool that improves from EdgePool [13]."
INTRODUCTION,0.023809523809523808,"In our experiments, We first evaluate MeGraph‚Äôs capability to capture Long Range interactions
(LRIs). We establish a Graph Theory Benchmark comprising four tasks related to shortest paths
and one related to connected components. MeGraph demonstrates superior performance compared
with many competitive baselines. MeGraph also achieves comparable or superior performance than
the state-of-the-art on the Long Range Graph Benchmark (LRGB) [17]. In addition, we perform
extensive experiments on widely-used real-world datasets that are not explicitly tailored for assessing
the capacity to capture LRIs. These include the GNN benchmark [15] and OGB-G datasets [28].
In these tests, MeGraph demonstrates superior or equivalent performance compared to the baseline
models, suggesting its broad applicability and effectiveness."
INTRODUCTION,0.025210084033613446,"The main contributions of this work are summarized as follows: 1) Mega graph and novel architec-
ture: we propose the mega graph, a multi-scale graph formed by intra- and inter-graph edges, where
the message-passing over the mega graph naturally balances locality and hierarchy. On this basis, we
introduce a novel architecture MeGraph, which alternates information aggregation along the intra-
and inter-edges of the mega graph. This fusion process intertwines local and hierarchical information,
leading to mutual benefits. 2) Hierarchical information fusion: we design a bidirectional pathway
to facilitate information fusion among the hierarchies. 3) S-EdgePool: we enhance EdgePool into
S-EdgePool, allowing an adjustable pooling ratio. 4) Benchmark and Evaluations: we establish a
new graph theory benchmark to evaluate the ability of models to capture LRIs. In these evaluations,
MeGraph exhibits dominant performance. Additionally, MeGraph achieves new SOTA in one task of
LRGB and shows better or comparable performance compared with baselines on popular real-world
datasets."
INTRODUCTION,0.02661064425770308,"2
Notations, Backgrounds and Preliminaries"
INTRODUCTION,0.028011204481792718,"Let G = (V, E) be a graph with node set V (of cardinality N v) and edge set E (of cardinality N e). The
edge set can be represented as E = {(sk, tk)}k=1:Ne, where sk and tk are the indices of the source
and target nodes connected by edge k. We define XG as features of graph G, which is a combination
of global (graph-level) features uG, node features VG, and edge features EG. Accordingly, we
use VG
i to represent the features of a specific node vi, and EG
k denotes the features of a specific
edge (sk, tk). We may abuse the notations by omitting the superscript G when there is no context
ambiguity."
INTRODUCTION,0.029411764705882353,"2.1
Graph Network (GN) Block"
INTRODUCTION,0.03081232492997199,"We adopt the Graph Network (GN) block design in accordance with the GN framework [6]. In our
notation, a GN block accepts a graph G and features X = (u, V, E) as inputs, and produces new
features X‚Ä≤ = (u‚Ä≤, V‚Ä≤, E‚Ä≤). A full GN block [6] includes the following update steps. In each of these
steps, œï denotes an update function, typically implemented as a neural network:"
INTRODUCTION,0.03221288515406162,"Edge features: E‚Ä≤
k = œïe(Ek, Vsk, Vtk, u), ‚àÄk ‚àà[1, N e].
Node features: V‚Ä≤
i = œïv(œÅe‚Üív({E‚Ä≤
k}k‚àà[1,N e],tk=i), Vi, u), ‚àÄi ‚àà[1, N v], where œÅe‚Üív is an aggre-
gation function taking the features of incoming edges as inputs.
Global features: u‚Ä≤ = œïu(œÅe‚Üíu(E‚Ä≤), œÅv‚Üíu(V‚Ä≤), u), where œÅe‚Üíu and œÅv‚Üíu are two global aggrega-
tion functions over edge and node features."
INTRODUCTION,0.03361344537815126,"Given a fixed graph structure G and the consistent input and output formats outlined above, GN
blocks can be seamlessly integrated to construct complex, deep graph networks."
GRAPH POOLING,0.0350140056022409,"2.2
Graph Pooling"
GRAPH POOLING,0.036414565826330535,"Graph pooling operation downsamples the graph structure and its associated features while ensuring
the preservation of structural and semantic information inherent to the graph. Drawing from the
SRC framework [23], we identify graph pooling as a category of functions,POOL, that maps a graph
G = (V, E) with N v nodes and features XG to a reduced graph ÀúG = (ÀúV, ÀúE) with N Àúv nodes and new
features X ÀúG. Here, N Àúv ‚â§N v and ( ÀúG, X ÀúG) = POOL(G, XG)."
GRAPH POOLING,0.037815126050420166,"The SRC framework deconstructs the POOL operation into SELECT, REDUCE, and CONNECT functions,
which encompass most existing graph pooling techniques. We reinterpret these functions in our own
notation as follows:"
GRAPH POOLING,0.0392156862745098,"( ÀÜG, X
ÀÜG) = SELECT(G, XG);
ÀúG = CONNECT(G, ÀÜG, X
ÀÜG); X
ÀúG = REDUCE(XG, ÀÜG, X
ÀÜG).
(1)"
GRAPH POOLING,0.04061624649859944,"As shown in Fig. 1, the SELECT establishes N Àúv nodes for the pooled graph, and each node Àúv
corresponds to a subset of nodes SÀúv ‚äÜV in the input graph. This creates an undirected bipartite
graph ÀÜG = (ÀÜV, ÀÜE), with ÀÜV = V ‚à™ÀúV and (v, Àúv) ‚ààÀÜE if and only if v ‚ààSÀúv. We refer to this graph ÀÜG as
the inter-graph, a larger graph that links nodes in the input graph G with nodes in the pooled graph ÀúG.
The SELECT function can be generalized to include inter-graph features X ÀÜG. As an example, edge
weights can be introduced for some edge (ÀÜsk, ÀÜtk) in graph ÀÜG to gauge the importance of node ÀÜsk from
the input graph contributing to node ÀÜtk in the pooled graph."
GRAPH POOLING,0.04201680672268908,"The CONNECT function reconstructs the edge set ÀúE between the nodes in ÀúV of the pooled graph ÀúG
based on the original edges in E and the inter-graph edges in ÀÜE. The REDUCE function calculates
the graph features X ÀúG of graph ÀúG by aggregating input graph features XG, taking into account both
the inter-graph ÀÜG and features X ÀÜG. In a similar vein to the relationship between graph lifting and
coarsening, we define the EXPAND function for graph features, which serves as the inverse of the
REDUCE function: XG = EXPAND(X ÀúG, ÀÜG, X ÀÜG)."
METHODS,0.04341736694677871,"3
Methods"
METHODS,0.04481792717086835,"We begin with the introduction of the mega graph (Sec.3.1), which amalgamates the local (intra-
edges) and hierarchical (inter-edges) structures of a multi-scale graph hierarchy into a single graph."
METHODS,0.046218487394957986,"Figure 2: Illustration of the MeGraph model, where n‚àídenotes n ‚àí1. The blue and green circles represent
features of intra- and inter-graphs, respectively. In this figure, the horizontal and vertical directions represent the
interaction among the local structure (intra-graph) and graph hierarchy (inter-graph) respectively. The features
of intra- and inter-graphs are represented by blue and green circles, respectively. In this figure, the horizontal
and vertical directions denote the local structure and graph hierarchy respectively. During the encode stage, the
mega graph is constructed using graph pooling. In the process stage, the Mee layer, which features bidirectional
pathways across multiple scales, is stacked n times. In the decode stage, multi-scale features are read out. The
golden inter GN blocks form bidirectional pathways across the whole hierarchy."
METHODS,0.047619047619047616,"Following this, we present the MeGraph model (Sec.3.2), which alternates between the aggregation
of local and hierarchical information along the intra- and inter-edges of the mega graph. We then
discuss the specific choices made for the core modules of the MeGraph, along with the innovations
(Sec.3.3). Finally, we delve into the computational complexity of the MeGraph model (Sec.3.4)."
CONNECTING MULTI-SCALE GRAPHS INTO A MEGA GRAPH,0.049019607843137254,"3.1
Connecting Multi-scale Graphs into a Mega Graph"
CONNECTING MULTI-SCALE GRAPHS INTO A MEGA GRAPH,0.05042016806722689,"Similar to the concept of an image pyramid [1], a graph pyramid is constructed by stacking multi-
scale graphs, which are obtained through iterative downsampling of the graph using a graph pooling
technique. Formally, in alignment with the definition of an image feature pyramid [39], we define
a graph feature pyramid as a set of graphs G1:h := {Gi}i=1,¬∑¬∑¬∑ ,h and their corresponding features
XG1:h := {XGi}i=1,¬∑¬∑¬∑ ,h. Here, G1 represents the original graph, XG1 signifies the initial features, h
stands for the height, and (Gi, XGi) = POOL(Gi‚àí1, XGi‚àí1) for i > 1."
CONNECTING MULTI-SCALE GRAPHS INTO A MEGA GRAPH,0.05182072829131653,"By iteratively applying the POOL function, we can collect the inter-graphs ÀÜG1:h := { ÀÜGi}i=1,¬∑¬∑¬∑ ,h‚àí1
and their features XÀÜG1:h := {X ÀÜGi}i=1,¬∑¬∑¬∑ ,h‚àí1 (since there are h ‚àí1 inter-graphs for h intra-graphs),
where ( ÀÜGi, X ÀÜGi) = SELECT(Gi, XGi) for i < h. The bipartite inter-graph ÀÜG and its features X ÀÜG
essentially depict the relationships between the graphs before and after the pooling process (see
Sec. 2.2)."
CONNECTING MULTI-SCALE GRAPHS INTO A MEGA GRAPH,0.05322128851540616,"Finally, as illustrated in Fig. 1, we wire the graph pyramid G1:h using the edges found in the
bipartite graphs ÀÜG1:h. This results in a mega graph MG = (MV, ME), where MV = Sh
i=1 Vi
and ME = Sh
i=1 Ei ‚à™Sh‚àí1
i=1 ÀÜEi. The structure of the mega graph would vary as the graph pooling
method trains. We denote MGintra = Sh
i=1 Gi as the intra-graph of MG, and refer to the edges
therein as intra-edges. Correspondingly, MGinter = Sh‚àí1
i=1 ÀÜGi is referred to as the inter-graph of MG,
with its corresponding edges termed as inter-edges. The features XMG of the mega graph MG is a
combination of intra-graph features XG1:h and inter-graph features XÀÜG1:h."
MEGA GRAPH MESSAGE PASSING,0.0546218487394958,"3.2
Mega Graph Message Passing"
MEGA GRAPH MESSAGE PASSING,0.056022408963585436,"We introduce the MeGraph architecture, designed to perform local and hierarchical aggregations over
the mega graph alternately. As shown in Fig.2, the architecture follows the encode-process-decode
design [6, 25] and incorporates GN blocks (refer to Sec. 2.1) as fundamental building blocks."
MEGA GRAPH MESSAGE PASSING,0.05742296918767507,"During the encode stage, initial features are inputted into an intra-graph GN block, which is followed
by a sequence of graph pooling operations to construct the mega graph MG and its associated features ùëã1 ùëñ‚àí ùëã2 ùëñ‚àí ùëã3 ùëñ‚àí
ùëã3 ‚Ä≤ ùëã4 ùëñ‚àí
ùëã4 ‚Ä≤ ùëã1 ‚Ä≤‚Ä≤ ùëã2 ‚Ä≤‚Ä≤ ùëã3 ‚Ä≤‚Ä≤ ùëã4 ‚Ä≤‚Ä≤"
MEGA GRAPH MESSAGE PASSING,0.058823529411764705,Inter GN
MEGA GRAPH MESSAGE PASSING,0.06022408963585434,Intra GN
MEGA GRAPH MESSAGE PASSING,0.06162464985994398,Residual Same
MEGA GRAPH MESSAGE PASSING,0.06302521008403361,"Inputs
Outputs
Vertical Updates ùëãùëó+ ùëãùëó
ùëãùëó ‚Ä≤ ùëãùëó+ ‚Ä≤"
MEGA GRAPH MESSAGE PASSING,0.06442577030812324,Cross Update ùëã1 ‚Ä≤ ùëã2 ‚Ä≤ ùëãùëó
MEGA GRAPH MESSAGE PASSING,0.06582633053221289,"ùëñ
Graph
Features
ùíâ 1 2 3 4"
MEGA GRAPH MESSAGE PASSING,0.06722689075630252,"‚Ä¶
‚Ä¶
‚Ä¶‚Ä¶ ùëã1 ùëñ ùëã2 ùëñ ùëã3 ùëñ ùëã4 ùëñ ‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶ ùëã1 ùëñ‚àí
ùëã1 ‚Ä≤
ùëã1 ùëñ ùëãùëó ‚Ä≤ ùëã2 ùëñ‚àí ùëã3 ùëñ‚àí ùëã2 ùëñ ùëã3 ùëñ ùëãùëó ùëñ"
MEGA GRAPH MESSAGE PASSING,0.06862745098039216,"Step 1
Step 2
Step 3 ùëã1 ùëã2 ‚Ä≤
ùëã2 ùëã1 ‚Ä≤‚Ä≤
ùëã1 ‚Ä≤ ùëãùëó ùëã3 ‚Ä≤
ùëã3 ùëã2 ‚Ä≤‚Ä≤
ùëã2 ùëã3 ‚Ä≤‚Ä≤
ùëã3 ‚Ä≤"
MEGA GRAPH MESSAGE PASSING,0.0700280112044818,Pathway
MEGA GRAPH MESSAGE PASSING,0.07142857142857142,"Figure 3: Illustration of the Mee layer, where i‚àídenotes i ‚àí1 and j+ denotes for j + 1. The blue and green
circles represent the features of intra- and inter-graphs, respectively. Grey and golden arrows represent the intra
and inter GN blocks. The cross-update utilizes inter GN blocks to exchange information between consecutive
heights, as elaborated in the main text. The Mee layer first aggregates information locally along inter-graph
edges. It then applies cross-updates sequentially from lower to higher levels, accumulating information along
the pathway to pass to the higher hierarchy. The process is reversed in the last step."
MEGA GRAPH MESSAGE PASSING,0.07282913165266107,"(X0)MG. In the process stage, the Mee layer, which performs both local and hierarchical information
aggregation within the mega graph, is stacked n times. The i-th Mee layer receives (Xi‚àí1)MG as
input and outputs (Xi)MG. Through the stacking of Mee layers, a deeper architecture is created,
enabling a more profound fusion of local and hierarchical information. Lastly, in the decode stage,
the features (Xn)MG are transformed into task-specific representations using readout functions."
MEGA GRAPH MESSAGE PASSING,0.0742296918767507,"Mee Layer. The Mee layer is designed to aggregate local and hierarchical information within the
mega graph. A detailed structure of the Mee layer is depicted in Fig. 3."
MEGA GRAPH MESSAGE PASSING,0.07563025210084033,"For the i-th Mee layer, we consider inputs denoted by (Xi‚àí1)MG = {(Xi‚àí1)G1:h, (Xi‚àí1)ÀÜG1:h}.
For simplicity, we omit the superscript and denote the features of intra- and inter-graphs as
{Xi‚àí1
j
}j=1,¬∑¬∑¬∑ ,h := (Xi‚àí1)G1:h and { ÀÜXi‚àí1
j
}j=1,¬∑¬∑¬∑ ,h‚àí1 := (Xi‚àí1)ÀÜG1:h respectively."
MEGA GRAPH MESSAGE PASSING,0.07703081232492998,"The first step applies GN blocks on intra-graph edges, performing message passing on the local
structure of graphs at each scale: X‚Ä≤
j = GNi,j
intra(Gj, Xi‚àí1
j
). Here, X‚Ä≤
j represents the updated
intra-graph Gj features."
MEGA GRAPH MESSAGE PASSING,0.0784313725490196,"The second and third steps focus on multi-scale information fusion. The second step applies cross-
updates across consecutive heights from 1 to h, while the third step reverses the process, forming a
bidirectional pathway for the information flow across the hierarchy. The cross-update between consec-
utive heights j and j + 1 is denoted by a function (X‚Ä≤
j, ÀÜX‚Ä≤
j, Xj + 1‚Ä≤) = X-UPD(j, Xj, ÀÜXj, Xj + 1).
The prime notation indicates the updated value, and residual links [26] are used in practice."
MEGA GRAPH MESSAGE PASSING,0.07983193277310924,"This cross-update can be implemented via an inter-graph convolution with GNi,j
inter, referred to as X-
Conv (detailed in App.C.1). Alternatively, it can be realized using the REDUCE and EXPAND operations
of POOL (refer to Sec.2.2) by X‚Ä≤
j+1 = REDUCE( ÀÜGj, ÀÜX0
j, Xj) and X‚Ä≤
j = EXPAND( ÀÜGj, ÀÜX0
j, Xj+1),
where ÀÜGj is the j-th inter-graph. We denote this implementation as X-Pool."
MEGA GRAPH MESSAGE PASSING,0.08123249299719888,"The Mee layer outputs features {Xi
j}j=1,¬∑¬∑¬∑ ,h and { ÀÜXi
j}j=1,¬∑¬∑¬∑ ,h‚àí1. Residual links [26] can be added
from Xi‚àí1
j
to Xi
j and from ÀÜXi‚àí1
j
to ÀÜXi
j empirically, creating shortcuts that bypass GN blocks in the
Mee layer. It‚Äôs worth noting that the intra and inter GN blocks can share parameters across all heights
j to accommodate varying heights, or across all Mee layers to handle varying layer numbers."
MODULE CHOICE AND INNOVATION,0.08263305322128851,"3.3
Module Choice and Innovation"
MODULE CHOICE AND INNOVATION,0.08403361344537816,"MeGraph incorporates two fundamental modules: the graph pooling operator and the GN block. This
architecture can accommodate any graph pooling method from the POOL function family (refer to
Sec. 2.2). Furthermore, the GN block is not strictly confined to the graph convolution layer found in
standard GCN, GIN, or GAT."
MODULE CHOICE AND INNOVATION,0.08543417366946779,"Graph Pooling. There are a number of commonly used graph pooling methods, including Diff-
Pool [62], TopKPool [22], EdgePool [13], etc. We opt for EdgePool due to its simplicity, efficiency,"
MODULE CHOICE AND INNOVATION,0.08683473389355742,"and ability to naturally preserve the graph‚Äôs connectivity through edge contraction. However, edge
contraction is applied only to the edges in a specific maximal matching of the graph‚Äôs nodes [13],
thereby setting a lower limit of 50% to the pooling ratio Œ∑v. This constraint implies that a minimum
of log2 N pooling operations is required to reduce a graph of N nodes to a single node. To address
this limitation, we propose the Stridden EdgePool (S-EdgePool), which allows for a variable pooling
stride."
MODULE CHOICE AND INNOVATION,0.08823529411764706,"The principle behind S-EdgePool involves dynamically tracking the clusters of nodes created by the
contraction of selected edges. Similar to EdgePool, edges are processed in descending order based on
their scores. When an edge is contracted, if both ends do not belong to the same node cluster, the
two clusters containing the endpoints of the edge merge. The current edge can be contracted if the
resulting cluster contains no more than œÑc nodes after this edge‚Äôs contraction. The iteration stops
prematurely once a pooling ratio, Œ∑v, is achieved. During pooling, each node cluster is pooled as a
new node. When œÑc = 2, S-EdgePool reverts to the original EdgePool. The algorithm‚Äôs details and
pseudocode are available in App. C.2."
MODULE CHOICE AND INNOVATION,0.0896358543417367,"For efficiency, we employ the disjoint-set data structure to dynamically maintain the node clusters,
which has a complexity of O(EŒ±(E)), where E is the number of edges and Œ±(E) is a function that
grows slower than log(E) [50]. The total time complexity of S-EdgePool is equivalent to EdgePool
and is calculated as O(ED+E log E), where D is the embedding size, O(ED) from computing edge
scores and O(E log E) from sorting the edges."
MODULE CHOICE AND INNOVATION,0.09103641456582633,"GN block. The full GN block, introduced in Sec. 2.1, is implemented as a graph full network (GFuN)
layer. This layer exhibits a highly configurable within-block structure, enabling it to express a variety
of other architectures (see Sec. 4.2 of [6]), like GCN, GIN, GAT, GatedGCN. Thus, modifying the
within-block structure of GFuN is akin to plugging in different GNN cores. Further details can be
found in App. C.3."
MODULE CHOICE AND INNOVATION,0.09243697478991597,"Encoder and decoder. Most preprocessing methods (including positional encodings and graph
rewiring), encoding (input embedding) and decoding (readout functions) schemes applicable to GNNs
can also be applied to MeGraph. We give implementation details in App. C.4."
COMPUTATIONAL COMPLEXITY AND DISCUSSION,0.0938375350140056,"3.4
Computational Complexity and Discussion"
COMPUTATIONAL COMPLEXITY AND DISCUSSION,0.09523809523809523,"The overall complexity of the MeGraph model is contingent on the height h, the number of Mee
layers n, the chosen modules, and the corresponding hyperparameters. Let D be the embedding
size, V the number of nodes, and E the number of edges in the input graph G. The time complexity
of S-Edgepool is O(ED+E log E), and that of a GFuN layer is O(V D2+ED). Assuming both
the pooling ratios of nodes and edges are Œ∑, the total time complexity to construct the mega graph
MG becomes O((ED + E log E)/(1 ‚àíŒ∑)), where Ph‚àí1
i=0 Œ∑i < 1/(1 ‚àíŒ∑). Similarly, the total time
complexity of an Mee layer is O((V D2 + ED)/(1 ‚àíŒ∑)). This complexity is equivalent to a typical
GNN layer if we consider 1/(1 ‚àíŒ∑) as a constant (for instance, it is a constant of 2 when Œ∑ = 0.5)."
COMPUTATIONAL COMPLEXITY AND DISCUSSION,0.09663865546218488,"Theoretically, when using the same number of layers, MeGraph is better at capturing LRIs than
standard message-passing GNNs owning to the hierarchical structure (see App. D.1 for details). On
the other hand, MeGraph can degenerate into standard message-passing GNNs (see App. D.2 for
details), indicating it should not perform worse than them on other tasks."
EXPERIMENTS,0.09803921568627451,"4
Experiments"
EXPERIMENTS,0.09943977591036414,"We conduct extensive experiments to evaluate the MeGraph‚Äôs ability to capture long-range interactions
(LRIs) and its performance in general graph learning tasks."
EXPERIMENTAL SETTINGS,0.10084033613445378,"4.1
Experimental Settings"
EXPERIMENTAL SETTINGS,0.10224089635854341,"Baselines. We compare MeGraph model to three baselines as follows: 1) MeGraph h=1 variant does
not use the hierarchical structure and falls back to standard GNNs. 2) MeGraph n=1 variant gives up
repeating information exchange over the mega graph. 3) Graph U-Nets [22] uses a U-shaped design
and only traverses the multi-scale graphs once."
EXPERIMENTAL SETTINGS,0.10364145658263306,"Due to page limits, statistics of the datasets are provided in App. B.1, hyper-parameters are reported
in Table 9, and the training and implementation details are reported in App. E."
EXPERIMENTAL SETTINGS,0.10504201680672269,"Table 1: Results on Graph Theory Benchmark (medium size). For each task, we report the MSE regression loss
on test set, averaged over different graph generation methods. Darker blue cells denote better performance and
the bold denotes the best one. We provide detailed results on each type of graphs in App. F.7."
EXPERIMENTAL SETTINGS,0.10644257703081232,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC
Average"
EXPERIMENTAL SETTINGS,0.10784313725490197,"Baselines
(h=1)"
EXPERIMENTAL SETTINGS,0.1092436974789916,"n=1
11.184
1.504
11.781
22.786
20.133
13.478
n=5
3.898
1.229
5.750
12.354
18.971
8.440
n=10
2.326
1.264
5.529
7.038
18.876
7.006"
EXPERIMENTAL SETTINGS,0.11064425770308123,"MeGraph (h=5)
EdgePool (œÑc=2)
n=1
1.758
0.907
4.591
5.554
14.030
5.368
n=5
0.790
0.767
2.212
0.712
6.593
2.215"
EXPERIMENTAL SETTINGS,0.11204481792717087,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
EXPERIMENTAL SETTINGS,0.1134453781512605,"œÑc=3
0.660
0.747
0.719
0.459
0.942
0.705
Œ∑v=0.3
2.225
0.778
1.061
3.591
2.009
1.933
Œ∑v=0.3, œÑc=4
0.615
0.702
0.651
0.434
0.975
0.675
Œ∑v=0.5, œÑc=4
1.075
0.769
0.945
1.204
1.992
1.197
Œ∑v=0.3, œÑc=4 (X-Pool)
0.935
0.751
0.864
1.462
2.003
1.203
Œ∑v=0.3, œÑc=4 (w/o pw)
0.632
0.730
0.864
0.765
2.334
1.065"
EXPERIMENTAL SETTINGS,0.11484593837535013,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
1.118
1.008
2.031
1.166
2.584
1.581"
EXPERIMENTAL SETTINGS,0.11624649859943978,Table 2: Results on Graph Theory Benchmark (large size).
EXPERIMENTAL SETTINGS,0.11764705882352941,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC
Average"
EXPERIMENTAL SETTINGS,0.11904761904761904,"Baseline
h=1,n=5
328.014
39.4772
189.577
324.033
219.746
220.169"
EXPERIMENTAL SETTINGS,0.12044817927170869,"MeGraph
h=5,n=5,Œ∑v=0.3,œÑc=4
23.8963
16.8321
19.2185
14.9676
44.9234
23.9676"
EXPERIMENTAL SETTINGS,0.12184873949579832,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
101.009
30.3711
39.8708
100.070
75.1185
69.2879"
EXPERIMENTAL SETTINGS,0.12324929971988796,"Table 3: Results on LRGB [17], numbers are taken from corresponding papers. All methods use around 500K
parameters for a fair comparison. Message-passing-based models have 5 layers, while the Transformer-based
models have 4 layers [17]. PE indicates positional encoding, which makes it easier to distinguish different nodes."
EXPERIMENTAL SETTINGS,0.12464985994397759,"Methods
Use PE
Peptide-func ‚Üë
Peptide-struct ‚Üì"
EXPERIMENTAL SETTINGS,0.12605042016806722,"GCN [17]
59.30 ¬±0.23
0.3496 ¬±0.0013
GINE [17]
55.43 ¬±0.78
0.3547 ¬±0.0045
GatedGCN [17]
58.64 ¬±0.77
0.3420 ¬±0.0013
GatedGCN+RWSE [17]
‚úì
60.69 ¬±0.35
0.3357 ¬±0.0006
GatedGCN+RWSE+VN [10]
‚úì
66.85 ¬±0.62
0.2529 ¬±0.0009
Transformer+LapPE [17]
‚úì
63.26 ¬±1.26
0.2529 ¬±0.0016
SAN+LapPE [17]
‚úì
63.84 ¬±1.21
0.2683 ¬±0.0043
SAN+RWSE [17]
‚úì
64.39 ¬±0.75
0.2545 ¬±0.0012
GPS [46]
‚úì
65.35 ¬±0.41
0.2500 ¬±0.0005
MGT+WavePE [43]
‚úì
68.17 ¬±0.64
0.2453 ¬±0.0025
GNN-AK+ [27]
64.80 ¬±0.89
0.2736 ¬±0.0007
SUN [27]
67.30 ¬±0.78
0.2498 ¬±0.0008
GraphTrans+PE [27]
‚úì
63.13 ¬±0.39
0.2777 ¬±0.0025
GINE+PE [27]
‚úì
64.05 ¬±0.77
0.2780 ¬±0.0021
GINE-MLP-Mixer+PE [27]
‚úì
69.21 ¬±0.54
0.2485 ¬±0.0004"
EXPERIMENTAL SETTINGS,0.12745098039215685,"MeGraph (h=9,n=1)
67.52 ¬±0.78
0.2557 ¬±0.0011
MeGraph (h=9,n=4)
69.45 ¬±0.77
0.2507 ¬±0.0009"
PERFOMANCE ON LRI TASKS,0.12885154061624648,"4.2
Perfomance on LRI Tasks"
PERFOMANCE ON LRI TASKS,0.13025210084033614,"To test MeGraph‚Äôs ability to capture long-range interactions, we establish a Graph Theory Benchmark,
of which four tasks related to shortest path distance, i.e., Single Source Shortest Path (SPss), Single
Source Single Destination Shortest Path (SPsssd), Graph Diameter (Diameter) and Eccentricity of
nodes (ECC); and 1 task related to connected component, i.e., Maximum Connected Component of
the same color (MCC). To generate diversified undirected and unweighted graphs for each task, we
adopt the ten methods used in PNA [12] and add four new methods: cycle graph, pseudotree, SBM,
and geographic threshold graphs. The details of the dataset generation can be found in App. B.2."
PERFOMANCE ON LRI TASKS,0.13165266106442577,"As depicted in Table 1, the MeGraph model with h=5, n=5 significantly outperforms both the h=1
and n=1 baselines in terms of reducing regression error across all tasks. It is worth noting that even"
PERFOMANCE ON LRI TASKS,0.1330532212885154,"Table 4: Results on GNN benchmark. Regression tasks are colored with blue. ‚Üìindicates that smaller numbers
are better. Classification tasks are colored with green. ‚Üëindicates that larger numbers are better. Darker colors
indicate better performance. ‚Ä† denotes the results are reported in [15]."
PERFOMANCE ON LRI TASKS,0.13445378151260504,"Model
ZINC ‚Üì
AQSOL ‚Üì
MNIST ‚Üë
CIFAR10 ‚Üë
PATTERN ‚Üë
CLUSTER ‚Üë"
PERFOMANCE ON LRI TASKS,0.13585434173669467,"GCN‚Ä†
0.416 ¬±0.006
1.372 ¬±0.020
90.120 ¬±0.145
54.142 ¬±0.394
85.498 ¬±0.045
47.828 ¬±1.510
GIN‚Ä†
0.387 ¬±0.015
1.894 ¬±0.024
96.485 ¬±0.252
55.255 ¬±1.527
85.590 ¬±0.011
58.384 ¬±0.236
GAT‚Ä†
0.475 ¬±0.007
1.441 ¬±0.023
95.535 ¬±0.205
64.223 ¬±0.455
75.824 ¬±1.823
57.732 ¬±0.323
GatedGCN‚Ä†
0.435 ¬±0.011
1.352 ¬±0.034
97.340 ¬±0.143
67.312 ¬±0.311
84.480 ¬±0.122
60.404 ¬±0.419"
PERFOMANCE ON LRI TASKS,0.13725490196078433,"Graph-UNets
0.332 ¬±0.010
1.063 ¬±0.018
97.130 ¬±0.227
68.567 ¬±0.339
86.257 ¬±0.078
50.371 ¬±0.243
MeGraph (h=1)
0.323 ¬±0.002
1.075 ¬±0.007
97.570 ¬±0.168
69.890 ¬±0.209
84.845 ¬±0.021
58.178 ¬±0.079
MeGraph (n=1)
0.310 ¬±0.005
1.038 ¬±0.018
96.867 ¬±0.167
68.522 ¬±0.239
85.507 ¬±0.402
50.396 ¬±0.082
MeGraph
0.260 ¬±0.005
1.002 ¬±0.021
97.860 ¬±0.098
69.925 ¬±0.631
86.507 ¬±0.067
68.603 ¬±0.101
MeGraphbest
0.202 ¬±0.007
1.002 ¬±0.021
97.860 ¬±0.098
69.925 ¬±0.631
86.732 ¬±0.023
68.610 ¬±0.164"
PERFOMANCE ON LRI TASKS,0.13865546218487396,"Table 5: Results on OGB-G. ‚Ä† indicates that the results are reported in [28].
Model
molhiv ‚Üë
molbace ‚Üë
molbbbp ‚Üë
molclintox ‚Üë
molsider ‚Üë"
PERFOMANCE ON LRI TASKS,0.1400560224089636,"GCN‚Ä†
76.06 ¬±0.97
79.15 ¬±1.44
68.87 ¬±1.51
91.30 ¬±1.73
59.60 ¬±1.77
GIN‚Ä†
75.58 ¬±1.40
72.97 ¬±4.00
68.17 ¬±1.48
88.14 ¬±2.51
57.60 ¬±1.40"
PERFOMANCE ON LRI TASKS,0.14145658263305322,"Graph-UNets
79.48 ¬±1.06
81.09 ¬±1.66
71.10 ¬±0.52
91.67 ¬±1.69
59.38 ¬±0.63
MeGraph (h=1)
78.54 ¬±1.14
71.77 ¬±2.15
67.56 ¬±1.11
89.77 ¬±3.48
58.28 ¬±0.51
MeGraph (n=1)
78.56 ¬±1.02
79.72 ¬±1.24
67.34 ¬±0.98
91.07 ¬±2.21
58.08 ¬±0.59
MeGraph
77.20 ¬±0.88
78.52 ¬±2.51
69.57 ¬±2.33
92.04 ¬±2.19
59.01 ¬±1.45
MeGraphbest
79.20 ¬±1.80
83.52 ¬±0.47
69.57 ¬±2.33
92.06 ¬±1.32
63.43 ¬±1.10"
PERFOMANCE ON LRI TASKS,0.14285714285714285,"Model
moltox21 ‚Üë
moltoxcast ‚Üë
molesol ‚Üì
molfreesolv ‚Üì
mollipo ‚Üì"
PERFOMANCE ON LRI TASKS,0.14425770308123248,"GCN‚Ä†
75.29 ¬±0.69
63.54 ¬±0.42
1.114 ¬±0.03
2.640 ¬±0.23
0.797 ¬±0.02
GIN‚Ä†
74.91 ¬±0.51
63.41 ¬±0.74
1.173 ¬±0.05
2.755 ¬±0.34
0.757 ¬±0.01"
PERFOMANCE ON LRI TASKS,0.14565826330532214,"Graph-UNets
77.85 ¬±0.81
66.49 ¬±0.45
1.002 ¬±0.04
1.885 ¬±0.07
0.716 ¬±0.01
MeGraph (h=1)
75.89 ¬±0.45
64.49 ¬±0.46
1.079 ¬±0.02
2.017 ¬±0.08
0.768 ¬±0.00
MeGraph (n=1)
77.01 ¬±0.93
66.89 ¬±1.21
0.896 ¬±0.04
1.892 ¬±0.06
0.730 ¬±0.01
MeGraph
78.11 ¬±0.47
67.67 ¬±0.53
0.886 ¬±0.02
1.876 ¬±0.05
0.726 ¬±0.00
MeGraphbest
78.11 ¬±0.47
67.90 ¬±0.19
0.867 ¬±0.02
1.876 ¬±0.05
0.688 ¬±0.01"
PERFOMANCE ON LRI TASKS,0.14705882352941177,"the h=5, n=1 baseline outperforms the h=1, n=10 baseline, indicating that adopting a multi-scale
graph hierarchy is crucial in these tasks. The improvement is also substantial when compared with
our reproduced Graph-UNets using S-EdgePool ([MeGraph] 0.675 vs. [Graph UNets] 1.581). The
improvements are more significant when the size of graphs becomes larger (as shown in Table 2).
These results collectively demonstrate the superior ability of MeGraph to capture LRIs."
PERFOMANCE ON LRI TASKS,0.1484593837535014,"Furthermore, we evaluated MeGraph model and compared it with other recent methods on the
Long Range Graph Benchmark (LRGB) [17] that contains real-world tasks that require capturing
LRIs. As depicted in Table 3, the h=9, n=4 variant of MeGraph achieves superior results on the
Peptide-func task, and comparable performance on the Peptide-struct task, relative to state-of-the-art
models. It is worth noting that the n = 1 variant already surpasses other methods except the recent
MLP-Mixer [27] in the Peptide-func task."
GENERALITY OF MEGRAPH,0.14985994397759103,"4.3
Generality of MeGraph"
GENERALITY OF MEGRAPH,0.15126050420168066,"To verify the generality of MeGraph model, we evaluate MeGraph on widely adopted GNN Bench-
mark [15], Open Graph Benchmark [28] and TU Dataset [42]. Results on TU Datasets are available
in App. F.3. In addition to the standard model that shares hyper-parameters in similar tasks, we also
report MeGraphbest with specifically tuned hyper-parameters for each task."
GENERALITY OF MEGRAPH,0.15266106442577032,"GNN Benchmark. We experiment on chemical data (ZINC and AQSOL), image data (MNIST and
CIFAR10) and social network data (PATTERN and CLUSTER). As shown in Table 4, MeGraph
outperforms the three baselines by a large margin, indicating the effectiveness of repeating both the
local and hierarchical information aggregation."
GENERALITY OF MEGRAPH,0.15406162464985995,"Open Graph Benchmark (OGB). We choose 10 datasets related to molecular graphs from the graph
prediction tasks of OGB. The task of all datasets is to predict some properties of molecule graphs"
GENERALITY OF MEGRAPH,0.15546218487394958,"1
2
3
4
5
6
7
8
9
10
Num Layer"
GENERALITY OF MEGRAPH,0.1568627450980392,"0.60
0.64
0.68
0.72
0.76
0.80
0.84
0.88
0.92
0.96
1.00"
GENERALITY OF MEGRAPH,0.15826330532212884,Accuracy
GENERALITY OF MEGRAPH,0.15966386554621848,TreeCycle
GENERALITY OF MEGRAPH,0.16106442577030813,"EdgePool h=2
EdgePool h=3"
GENERALITY OF MEGRAPH,0.16246498599439776,"Louvain h=2
Louvain h=3"
GENERALITY OF MEGRAPH,0.1638655462184874,"Random h=2
Random h=3 h=1"
GENERALITY OF MEGRAPH,0.16526610644257703,"1
2
3
4
5
6
7
8
9
10
Num Layer"
GENERALITY OF MEGRAPH,0.16666666666666666,"0.80
0.82
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00"
GENERALITY OF MEGRAPH,0.16806722689075632,Accuracy
GENERALITY OF MEGRAPH,0.16946778711484595,TreeGrid
GENERALITY OF MEGRAPH,0.17086834733893558,"EdgePool h=2
EdgePool h=3"
GENERALITY OF MEGRAPH,0.1722689075630252,"Louvain h=2
Louvain h=3"
GENERALITY OF MEGRAPH,0.17366946778711484,"Random h=2
Random h=3 h=1"
GENERALITY OF MEGRAPH,0.17507002801120447,"Figure 4: Node classification accuracy (averaged over 10 random repetitions) for MeGraph on TreeCycle
(left) and TreeGrid (right) datasets by varying the number of Mee layers n, the height h, and the graph pooling
methods (EdgePool, Louvain, Random). Clear gaps can be observed among heights 1, 2, and 3 for EdgePool and
Louvain [8] methods, while the accuracy is almost invariant among different heights for randomized pooling."
GENERALITY OF MEGRAPH,0.17647058823529413,"based on their chemical structures. As shown in Table 5, MeGraph outperforms the h=1 baseline by a
large margin, suggesting that building a graph hierarchy is also essential in molecule graphs. The
performance of MeGraph, n = 1 baseline, and the reproduced Graph U-Nets are comparable. This
observation may be because the information obtained from multi-hop neighbors offers only marginal
improvements compared to the information aggregated hierarchically."
ABLATION STUDY,0.17787114845938376,"4.4
Ablation Study"
ABLATION STUDY,0.1792717086834734,"Hierarchy vs. Locality. We study the impact of the height h and the number of Mee layers n on four
synthetic datasets introduced in [61], which are BAShape, BACommunity, TreeCycle, and TreeGrid.
Each dataset contains one graph formed by attaching multiple motifs to a base graph. The motif can
be a ‚Äòhouse‚Äô-shaped network (BAShape, BACommunity), six-node cycle (TreeCycle), or 3-by-3 grid
(TreeGrid). The task is to identify the nodes of the motifs in the fused graph."
ABLATION STUDY,0.18067226890756302,"As shown in Fig. 4, we can observe clear improvements in performance as the height h and the number
of layers n increase when using EdgePool. Although increasing height shows better improvements,
both hierarchy and locality are indispensable in TreeCycle and TreeGrid tasks. In App. F.5, we show
that the conclusion also holds on BAShape and BACommunity datasets, except that the accuracy
is already saturated with height h = 2. The significance of integrating locality with hierarchy is
also demonstrated in the CLUSTER task, as presented in Table 4. Here, MeGraph reaches 68.6%
accuracy, which is markedly higher than the 50.4% accuracy achieved by both the n=1 baseline and
Graph-UNets."
ABLATION STUDY,0.18207282913165265,"Varying the Pooling Method. We varied the graph pooling with two non-learnable pooling methods,
Louvain [8] and Random. On the TreeCycle and TreeGrid datasets, as depicted in Fig. 4, Louvain
achieves comparable accuracy to EdgePool, while MeGraph using random pooling matches the
performance of the h = 1 variant of MeGraph. These observations indicate that: 1) a well-structured
hierarchy crafted by suitable graph pooling methods enhances MeGraph‚Äôs ability to harness hierarchi-
cal information; and 2) despite the disruption from a randomly constructed hierarchy, the MeGraph
model effectively taps into the local structure of the original graph (also discussed in App. D.2)."
ABLATION STUDY,0.18347338935574228,"We further studied the impact of the node pooling ratio Œ∑v and the maximum cluster size œÑc in
S-EdgePool by perturbing these parameters. As indicated in Table 1, the best variant (Œ∑v=0.3,œÑc=4)
achieved a regression error about 3x smaller (0.675) compared to the original EdgePool (œÑc=2 with
an error of 2.215). This suggests the benefit of having a flexible pooling stride. Moreover, the mega
graph produced by S-EdgePool can vary significantly with different parameters. In App. F.2, we
visualized the resulting graph hierarchy to illustrate the difference between different S-EdgePool
variants. However, irrespective of the parameter set used, MeGraph consistently outperforms h = 1
baselines. This suggests that MeGraph exhibits robustness against different architectures of the mega
graph."
ABLATION STUDY,0.18487394957983194,"Varying GN Block. We varied the aggregation function of the GN block as attention (w/ ATT) and
gated function (w/ GATE). We observe similar results as in Sec. 4.2 and 4.3, verifying the robustness
of MeGraph towards different types of GN blocks. Detailed results can be found in Tables 16, 17 and
18 in App. F.6."
ABLATION STUDY,0.18627450980392157,"Changing cross update function (X-UPD). The unpool operation is frequently used by other hier-
archical architectures that build upon graph pooling. As illustrated in Table 1, we substituted the
X-Conv implementation with the X-Pool implementation of X-UPD, which resulted in a performance
decline from 0.675 to 1.203 (smaller is better). This finding suggests that other hierarchical GNNs
might also benefit from replacing the unpool operation with a convolution over the inter-graph edges."
ABLATION STUDY,0.1876750700280112,"Disabling bidirectional pathway. We verify the effectiveness of the bidirectional pathway design by
replacing steps 2 and 3 of the Mee layer as a standard message-passing along the inter-graph edges
(denoted w/o pw). As shown in Table 1, the performance degrades from 0.675 to 1.065 (smaller is
better), which indicates the contribution of the bidirectional pathway."
RELATED WORK,0.18907563025210083,"5
Related Work"
RELATED WORK,0.19047619047619047,"Long-Range Interactions (LRIs). Various methods have been proposed to address the issue of
LRIs, including making the GNNs deeper [40]. Another way is to utilize attention and gating
mechanism, including GAT [52], jumping knowledge (JK) network [59], incorporating Transformer
structures [35, 60, 56, 46] and MLP-Mixer [27]. Another line of research focuses on multi-scale
graph hierarchy using graph pooling methods [62, 22, 47], or learning representation based on
subgraphs [5, 66]. Recently, Long Range Graph Benchmark [17] has been proposed to better evaluate
models‚Äô ability to capture LRIs."
RELATED WORK,0.19187675070028012,"Feature Pyramids and Multi-Scale Feature Fusion. Multi-scale feature fusion methods on image
feature pyramids have been widely studied in computer vision literature, including the U-Net [48],
FPN [39], UNet++ [67], and some recent approaches [63, 41, 38, 37]. HRNet [53] is a similar method
compared to MeGraph. HRNet alternates between multi-resolution convolutions and multi-resolution
fusion by stridden convolutions. However, the above methods are developed for image data. The key
difference compared to these approaches is that the multi-scale feature fusion in MeGraph is along the
inter-graph edges, which is not as well structured as the pooling operation in image data. For graph
networks, the GraphFPN [65] builds a graph feature pyramid according to the image feature pyramid
and superpixel hierarchy. It applies GNN layers on the hierarchical graph to exchange information
within the graph pyramid. Existing works [22, 20, 47, 30] have also explored similar ideas in graph-
structured data. Our approach aligns with the broader concept of multi-scale information fusion, but
it is the first method that builds a mega graph using graph pooling operations and alternates local and
hierarchical information aggregation."
RELATED WORK,0.19327731092436976,"Graph Pooling Methods. Graph pooling is an important part of hierarchical graph representation
learning. There have been some traditional graph pooling methods like METIS [32] in early liter-
ature. Recently, many learning-based graph pooling methods have been proposed, including the
DiffPool [62], TopKPool [22], SAG pool [36], EdgePool [13], MinCutPool [7], Structpool [64], and
MEWISPool [44], etc. In this work, we utilize S-EdgePool improved from EdgePool to build the
mega graph, while this module can be substituted with any of the above-mentioned pooling methods."
RELATED WORK,0.19467787114845939,"Graph Neural Network (GNN) Layers. The GNN layer is the core module of graph representation
learning models. Typical GNNs include the GCN [34], GraphSage [24], GAT [52, 9], GIN [58],
PNA [12]. MeGraph adopts the full GN block [6] by removing part of links in the module as an
elementary block, and similarly this can be replaced by any one of the popular GNN blocks."
LIMITATIONS AND FUTURE WORK,0.19607843137254902,"6
Limitations and Future Work"
LIMITATIONS AND FUTURE WORK,0.19747899159663865,"The MeGraph model suffers from some limitations. The introduced mega graph architecture inevitably
increases both the number of trainable parameters and tuneable hyper-parameters. The flexible choices
of many modules in MeGraph post burdens on tuning the architecture on specific datasets. For future
research, MeGraph encourages new graph pooling methods to yield edge features in addition to
node features, when mapping the input graph to the pooled graph. It is also possible to improve
MeGraph using adaptive computational steps [49]. Another direction is to apply some expressive but
computationally expensive models like Transformers [51] and Neural Logic Machines [14, 57] (only)
over the pooled small-sized graphs."
LIMITATIONS AND FUTURE WORK,0.19887955182072828,"Acknowledgements
We thank Qizheng He for the discussions on the Graph Theory Dataset, Kirsty You for improving the
illustrations, and Jiafei Lv and the Maddison group for their helpful discussions or feedback on the
paper draft; This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404."
REFERENCES,0.20028011204481794,References
REFERENCES,0.20168067226890757,"[1] Edward H Adelson, Charles H Anderson, James R Bergen, Peter J Burt, and Joan M Ogden.
Pyramid methods in image processing. RCA engineer, 29(6):33‚Äì41, 1984."
REFERENCES,0.2030812324929972,"[2] R√©ka Albert and Albert-L√°szl√≥ Barab√°si. Statistical mechanics of complex networks. Reviews
of modern physics, 74(1):47, 2002."
REFERENCES,0.20448179271708683,"[3] Uri Alon and Eran Yahav.
On the bottleneck of graph neural networks and its practical
implications. In International Conference on Learning Representations, 2020."
REFERENCES,0.20588235294117646,"[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.20728291316526612,"[5] Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations
with graph multiset pooling. In International Conference on Learning Representations."
REFERENCES,0.20868347338935575,"[6] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018."
REFERENCES,0.21008403361344538,"[7] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph
neural networks for graph pooling. In International Conference on Machine Learning, pages
874‚Äì883. PMLR, 2020."
REFERENCES,0.211484593837535,"[8] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast
unfolding of communities in large networks. Journal of statistical mechanics: theory and
experiment, 2008(10):P10008, 2008."
REFERENCES,0.21288515406162464,"[9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In
International Conference on Learning Representations, 2021."
REFERENCES,0.21428571428571427,"[10] Chen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the connection between mpnn and
graph transformer. arXiv preprint arXiv:2301.11956, 2023."
REFERENCES,0.21568627450980393,"[11] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection
on graph classification. arXiv preprint arXiv:1905.04579, 2019."
REFERENCES,0.21708683473389356,"[12] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li√≤, and Petar VeliÀáckovi¬¥c. Principal
neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems,
33:13260‚Äì13271, 2020."
REFERENCES,0.2184873949579832,"[13] Frederik Diehl, Thomas Brunner, Michael Truong Le, and Alois Knoll. Towards graph pooling
by edge contraction. In ICML 2019 workshop on learning and reasoning with graph-structured
data, 2019."
REFERENCES,0.21988795518207283,"[14] Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural
logic machines. In International Conference on Learning Representations, 2018."
REFERENCES,0.22128851540616246,"[15] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier
Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020."
REFERENCES,0.22268907563025211,"[16] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Graph neural networks with learnable structural and positional representations. In International
Conference on Learning Representations, 2021."
REFERENCES,0.22408963585434175,"[17] Vijay Prakash Dwivedi, Ladislav Rampasek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan
Luu, and Dominique Beaini. Long range graph benchmark. In Thirty-sixth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track, 2022."
REFERENCES,0.22549019607843138,"[18] Paul ErdÀùos, Alfr√©d R√©nyi, et al. On the evolution of random graphs. Publ. Math. Inst. Hung.
Acad. Sci, 5(1):17‚Äì60, 1960."
REFERENCES,0.226890756302521,"[19] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428, 2019."
REFERENCES,0.22829131652661064,"[20] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical inter-message passing for
learning on molecular graphs. arXiv preprint arXiv:2006.12179, 2020."
REFERENCES,0.22969187675070027,"[21] Bernard A Galler and Michael J Fisher. An improved equivalence algorithm. Communications
of the ACM, 7(5):301‚Äì303, 1964."
REFERENCES,0.23109243697478993,"[22] Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,
pages 2083‚Äì2092. PMLR, 2019."
REFERENCES,0.23249299719887956,"[23] Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding
pooling in graph neural networks. IEEE Transactions on Neural Networks and Learning
Systems, 2022."
REFERENCES,0.2338935574229692,"[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.23529411764705882,"[25] Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B
Tenenbaum, and Peter W Battaglia. Relational inductive bias for physical construction in
humans and machines. arXiv preprint arXiv:1806.01203, 2018."
REFERENCES,0.23669467787114845,"[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770‚Äì778, 2016."
REFERENCES,0.23809523809523808,"[27] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A
generalization of vit/mlp-mixer to graphs, 2022."
REFERENCES,0.23949579831932774,"[28] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
Advances in neural information processing systems, 33:22118‚Äì22133, 2020."
REFERENCES,0.24089635854341737,"[29] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fr√©do Durand. Taichi:
a language for high-performance computation on spatially sparse data structures. ACM Transac-
tions on Graphics (TOG), 38(6):201, 2019."
REFERENCES,0.242296918767507,"[30] Truong Son Hy and Risi Kondor. Multiresolution equivariant graph variational autoencoder.
Machine Learning: Science and Technology, 4(1):015031, 2023."
REFERENCES,0.24369747899159663,"[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning, pages
448‚Äì456. PMLR, 2015."
REFERENCES,0.24509803921568626,"[32] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM Journal on scientific Computing, 20(1):359‚Äì392, 1998."
REFERENCES,0.24649859943977592,"[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR
(Poster), 2015."
REFERENCES,0.24789915966386555,"[34] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. 2016."
REFERENCES,0.24929971988795518,"[35] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L√©tourneau, and Prudencio Tossou.
Rethinking graph transformers with spectral attention. Advances in Neural Information Pro-
cessing Systems, 34:21618‚Äì21629, 2021."
REFERENCES,0.2507002801120448,"[36] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International
conference on machine learning, pages 3734‚Äì3743. PMLR, 2019."
REFERENCES,0.25210084033613445,"[37] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, and Kuiyuan Yang. Gated
fully fusion for semantic segmentation. In Proceedings of the AAAI conference on artificial
intelligence, volume 34, pages 11418‚Äì11425, 2020."
REFERENCES,0.2535014005602241,"[38] Di Lin, Dingguo Shen, Siting Shen, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, and
Hui Huang. Zigzagnet: Fusing top-down and bottom-up context for object segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
7490‚Äì7499, 2019."
REFERENCES,0.2549019607843137,"[39] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid networks for object detection. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2117‚Äì2125, 2017."
REFERENCES,0.25630252100840334,"[40] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In
Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &
data mining, pages 338‚Äì348, 2020."
REFERENCES,0.25770308123249297,"[41] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for
instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 8759‚Äì8768, 2018."
REFERENCES,0.25910364145658266,"[42] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv
preprint arXiv:2007.08663, 2020."
REFERENCES,0.2605042016806723,"[43] Nhat Khang Ngo, Truong Son Hy, and Risi Kondor. Multiresolution graph transformers and
wavelet positional encoding for learning long-range and hierarchical structures. The Journal of
Chemical Physics, 159(3), 2023."
REFERENCES,0.2619047619047619,"[44] Amirhossein Nouranizadeh, Mohammadjavad Matinkia, Mohammad Rahmati, and Reza
Safabakhsh. Maximum entropy weighted independent set pooling for graph neural networks.
arXiv preprint arXiv:2107.01410, 2021."
REFERENCES,0.26330532212885155,"[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems, 32, 2019."
REFERENCES,0.2647058823529412,"[46] Ladislav Ramp√°≈°ek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and
Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. arXiv preprint
arXiv:2205.12454, 2022."
REFERENCES,0.2661064425770308,"[47] Ladislav Ramp√°≈°ek and Guy Wolf. Hierarchical graph neural nets can capture long-range
interactions. In 2021 IEEE 31st International Workshop on Machine Learning for Signal
Processing (MLSP), pages 1‚Äì6. IEEE, 2021."
REFERENCES,0.26750700280112044,"[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical image computing and
computer-assisted intervention, pages 234‚Äì241. Springer, 2015."
REFERENCES,0.2689075630252101,"[49] Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, and Hao Su. Towards scale-invariant
graph-related problem solving by iterative homogeneous gnns. Advances in Neural Information
Processing Systems, 33:15811‚Äì15822, 2020."
REFERENCES,0.2703081232492997,"[50] Robert E Tarjan and Jan Van Leeuwen. Worst-case analysis of set union algorithms. Journal of
the ACM (JACM), 31(2):245‚Äì281, 1984."
REFERENCES,0.27170868347338933,"[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.27310924369747897,"[52] Petar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.27450980392156865,"[53] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong
Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation
learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence,
43(10):3349‚Äì3364, 2020."
REFERENCES,0.2759103641456583,"[54] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,
Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.
Deep graph library: A graph-centric, highly-performant package for graph neural networks.
arXiv preprint arXiv:1909.01315, 2019."
REFERENCES,0.2773109243697479,"[55] Duncan J WATTS. Networks, dynamics and the small world phenomenon. American Journal
of Sociology, 105(2):50‚Äì59, 2003."
REFERENCES,0.27871148459383754,"[56] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion
Stoica. Representing long-range context for graph neural networks with global attention.
Advances in Neural Information Processing Systems, 34:13266‚Äì13279, 2021."
REFERENCES,0.2801120448179272,"[57] Guangxuan Xiao, Leslie Pack Kaelbling, Jiajun Wu, and Jiayuan Mao. Efficient training and
inference of hypergraph reasoning networks, 2022."
REFERENCES,0.2815126050420168,"[58] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2018."
REFERENCES,0.28291316526610644,"[59] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
International conference on machine learning, pages 5453‚Äì5462. PMLR, 2018."
REFERENCES,0.28431372549019607,"[60] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in
Neural Information Processing Systems, 34:28877‚Äì28888, 2021."
REFERENCES,0.2857142857142857,"[61] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. Advances in neural information processing
systems, 32, 2019."
REFERENCES,0.28711484593837533,"[62] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. Advances in neural
information processing systems, 31, 2018."
REFERENCES,0.28851540616246496,"[63] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2403‚Äì
2412, 2018."
REFERENCES,0.28991596638655465,"[64] Hao Yuan and Shuiwang Ji. Structpool: Structured graph pooling via conditional random fields.
In Proceedings of the 8th International Conference on Learning Representations, 2020."
REFERENCES,0.2913165266106443,"[65] Gangming Zhao, Weifeng Ge, and Yizhou Yu. Graphfpn: Graph feature pyramid network
for object detection. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 2763‚Äì2772, 2021."
REFERENCES,0.2927170868347339,"[66] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any
gnn with local structure awareness. In International Conference on Learning Representations."
REFERENCES,0.29411764705882354,"[67] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang.
Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in
medical image analysis and multimodal learning for clinical decision support, pages 3‚Äì11.
Springer, 2018."
REFERENCES,0.29551820728291317,Appendix
REFERENCES,0.2969187675070028,Table of Contents
REFERENCES,0.29831932773109243,"A Code and Reproducibility
16"
REFERENCES,0.29971988795518206,"B
Dataset Details
16
B.1
Dataset Statistics and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
B.2
Graph Theory Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.3011204481792717,"C Method Details
18
C.1
Cross Update Function
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
C.2
S-EdgePool
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
C.3
GFuN
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
C.4
Encorder and Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
C.5
Architecture Variants
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.3025210084033613,"D Theoretical Discussions
22
D.1
Smaller Number of Aggregation Steps for Capturing Long-Range Interactions . .
22
D.2
MeGraph can degenerate to standard GNNs . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.30392156862745096,"E
Implementation and Training Details
23"
REFERENCES,0.30532212885154064,"F
Additional Experiment Results
23
F.1
Experimental Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
F.2
Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
F.3
Other Real-Wrold Datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
F.4
GFuN
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
F.5
Synthetic Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
F.6
Varying GN block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
F.7
Graph Theory Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27"
REFERENCES,0.3067226890756303,"A
Code and Reproducibility"
REFERENCES,0.3081232492997199,"The code along with the configuration of hyper-parameters to reproduce our experiments can be
found at https://github.com/dhh1995/MeGraph."
REFERENCES,0.30952380952380953,"We set the random seed as 2022 for all experiments to enable reproducible results. We provide dataset
statistics in Table 6 and details for the proposed graph theory benchmark in Appendix B.2. Details of
the hyper-parameters are reported in Table 9. Configuration of all hyper-parameters and the command
lines to reproduce the experiments have been included in the code repository."
REFERENCES,0.31092436974789917,"B
Dataset Details"
REFERENCES,0.3123249299719888,"B.1
Dataset Statistics and Metrics"
REFERENCES,0.3137254901960784,"We provide the statistics of all datasets used in our experiments in Table 6 and introduce the evaluation
metrics for each dataset."
REFERENCES,0.31512605042016806,"For Synthetic datasets, we use classification accuracy (ACC) as the evaluation metric. We use Mean
Square Error (MSE) as the evaluation metric for all datasets in our Graph Theory Benchmark. For
GNN Benchmark, we follow the original work [15] for evaluation, i.e., Mean Absolute Error (MAE)
for ZINC and AQSOL, classification accuracy for MNIST and CIFAR10, and balanced classification
accuracy for PATTERN and CLUSTER. For OGB Benchmark, we follow the original work [28] and
use the ROC-AUC for classification tasks and Root Mean Square Error (RMSE) for regression tasks.
For TU datasets, we follow the setting used by [11] and use classification accuracy as the evaluation
metric."
REFERENCES,0.3165266106442577,"B.2
Graph Theory Benchmark"
REFERENCES,0.3179271708683473,"In this section, we provide the details about the tasks and how the graph features and the labels are
generated given a base graph G = (V, E):"
REFERENCES,0.31932773109243695,"‚Ä¢ Single source single destination shortest path (SPsssd): a source node s ‚ààV and a destination
node t ‚ààV are selected uniform randomly. The feature of each node v contains three
numbers: (1, whether the node v is s, whether the node v is t). The label of a graph is the
length of the shortest path from s to t."
REFERENCES,0.32072829131652664,"‚Ä¢ A maximum connected component of the same color (MCC): each node of the graph is
colored with one of three colors. The feature for each node is the one-hot representation of
its color. The label of graph is the size of the largest connected component of the same color
for each color."
REFERENCES,0.32212885154061627,"‚Ä¢ Graph diameter (Diameter): the label of the graph is the diameter of the graph. The diameter
of a graph G is the maximum of the set of shortest path distances between all pairs of nodes
in the graph. The feature of each node is a uniform number 1."
REFERENCES,0.3235294117647059,"‚Ä¢ Single source shortest path (SPss): a source node s is selected uniformly randomly. The
feature of each node contains two numbers: (1, whether the node is s). The label of each
node is the length of the shortest path from s to this node."
REFERENCES,0.32492997198879553,"‚Ä¢ Graph eccentricity (ECC): the label of each node v is node‚Äôs eccentricity in the graph, which
is the maximum distance from v to the other nodes. The feature of each node is a uniform
number 1."
REFERENCES,0.32633053221288516,"For each task and graph generation method, We generate the dataset by the following steps:"
REFERENCES,0.3277310924369748,"‚Ä¢ Sample N (number of nodes) from an interval with a total number of graphs. These numbers
can be configured. For the medium size setting, the interval is [20, 50], a total of 300 graphs.
For the large size setting, the interval is [100, 200], a total of 500 graphs."
REFERENCES,0.3291316526610644,‚Ä¢ Use the graph generation method to generate a graph of N nodes.
REFERENCES,0.33053221288515405,‚Ä¢ Create graph features and labels according to the task.
REFERENCES,0.3319327731092437,"Table 6: The statistics of the datasets used in experiments. Some statistics (like the average number
of edges) of the Graph Theory datasets may vary depending on different random graph generation
methods. The regression tasks are marked with ‚úìin a separate column. The tasks of 4 synthetic
datasets are transductive, where the same graph is used for both training and testing. We do not use
the node labels as features during the training time. The train-val-test split is over nodes. All other
datasets in the table are inductive, where the testing graphs do not occur during training, and the
train-val-test split is over graphs."
REFERENCES,0.3333333333333333,"Collection
Dataset
#
Graphs"
REFERENCES,0.33473389355742295,"Avg
#
Nodes"
REFERENCES,0.33613445378151263,"Avg
#
Edges"
REFERENCES,0.33753501400560226,"#
Node
Feat"
REFERENCES,0.3389355742296919,"#
Edge
Feat"
REFERENCES,0.3403361344537815,"#
Classes
Task
Reg."
REFERENCES,0.34173669467787116,"Synthetic
BaShape
1
700
1761
1
-
4
Trans-Node
Synthetic
BaCommunity
1
1400
3872
10
-
8
Trans-Node
Synthetic
TreeCycle
1
871
970
1
-
2
Trans-Node
Synthetic
TreeGrid
1
1231
1705
1
-
2
Trans-Node"
REFERENCES,0.3431372549019608,"GraphTheory
SPsssd
300
35.0
-
3
-
-
Graph
‚úì
GraphTheory
Diameter
300
35.0
-
1
-
-
Graph
‚úì
GraphTheory
MCC
300
35.0
-
3
-
-
Graph
‚úì
GraphTheory
SPss
300
35.0
-
2
-
-
Node
‚úì
GraphTheory
ECC
300
35.0
-
1
-
-
Node
‚úì"
REFERENCES,0.3445378151260504,"LRGB
Peptides-func
15535
150.94
307.30
9
3
10
Graph
LRGB
Peptides-struct
15535
150.94
307.30
9
3
-
Graph
‚úì"
REFERENCES,0.34593837535014005,"GNNBenchmark
ZINC
12000
23.16
49.83
28
4
2
Graph
‚úì
GNNBenchmark
AQSOL
9823
17.57
35.76
65
5
2
Graph
‚úì
GNNBenchmark
MNIST
70000
70.57
564.53
3
1
10
Graph
GNNBenchmark
CIFAR10
60000
117.63
941.07
5
1
10
Graph
GNNBenchmark
PATTERN
14000
118.89
6078.57
3
-
2
Node
GNNBenchmark
CLUSTER
12000
117.20
4301.72
7
-
6
Node"
REFERENCES,0.3473389355742297,"OGB Graph
molhiv
41127
25.51
80.45
9
3
2
Graph
OGB Graph
molbace
1513
34.09
107.81
9
3
2
Graph
OGB Graph
molbbbp
2039
24.06
75.97
9
3
2
Graph
OGB Graph
molclintox
1477
26.16
81.93
9
3
2
Graph
OGB Graph
molsider
1427
33.64
104.36
9
3
2
Graph
OGB Graph
moltox21
7831
18.57
57.16
9
3
2
Graph
OGB Graph
moltoxcast
8576
18.78
57.30
9
3
2
Graph
OGB Graph
molesol
1128
13.29
40.64
9
3
-
Graph
‚úì
OGB Graph
molfreesolv
642
8.7
25.50
9
3
-
Graph
‚úì
OGB Graph
mollipo
4200
27.04
86.04
9
3
-
Graph
‚úì"
REFERENCES,0.3487394957983193,"TU
MUTAG
188
17.93
19.79
7
-
3
Graph
TU
NCI1
4110
29.87
32.30
37
-
2
Graph
TU
PROTEINS
1113
39.06
72.82
4
-
2
Graph
TU
D&D
1178
284.32
715.66
89
-
2
Graph
TU
ENZYMES
600
32.63
62.14
21
-
6
Graph
TU
IMDB-B
1000
19.77
96.53
10
-
2
Graph
TU
IMDB-M
1500
13.00
65.94
10
-
3
Graph
TU
RE-B
2000
429.63
497.75
10
-
2
Graph
TU
RE-M5K
4999
508.52
594.87
10
-
5
Graph
TU
RE-M12K
11929
391.41
456.89
10
-
11
Graph"
REFERENCES,0.35014005602240894,"We then provide the details about the random graph generation methods we used to create our Graph
Theory datasets."
REFERENCES,0.35154061624649857,"Following [12], we continue to use undirected and unweighted graphs from a wide variety of
types. We inherit their 10 random graph generation methods and quote their descriptions here for
completeness (the percentage after the name is the approximate proportion of such graphs in the
mixture setting)."
REFERENCES,0.35294117647058826,"‚Ä¢ Erd√∂s-R√©nyi (ER) (20%) [18]: with a probability of presence for each edge equal to p,
where p is independently generated for each graph from U[0, 1]
‚Ä¢ Barab√°si-Albert (BA) (20%) [2]: the number of edges for a new node is k, which is taken
randomly from {1, 2, . . . , N ‚àí1} for each graph
‚Ä¢ Grid (5%): m √ó k 2d grid graph with N = mk and m and k as close as possible
‚Ä¢ Caveman (5%) [55]: with m cliques of size k, with m and k as close as possible
‚Ä¢ Tree (15%): generated with a power-law degree distribution with exponent 3
‚Ä¢ Ladder graphs (5%)
‚Ä¢ Line graphs (5%)
‚Ä¢ Star graphs (5%)
‚Ä¢ Caterpillar graphs (10%): with a backbone of size b (drawn from U[1, N)), and N ‚àíb
pendent vertices uniformly connected to the backbone
‚Ä¢ Lobster graphs (10%): with a backbone of size b (drawn from U[1, N)), p (drawn from
U[1, N ‚àíb] ) pendent vertices uniformly connected to the backbone, and additional N ‚àíb‚àíp
pendent vertices uniformly connected to the previous pendent vertices."
REFERENCES,0.3543417366946779,"Additional, we add three more graph generation methods:"
REFERENCES,0.3557422969187675,"‚Ä¢ Cycle graphs
‚Ä¢ Pseudotree graphs: A tree graph plus an additional edge. The graph is generated by first
generating a cycle graph of size m = sample(0.3N, 0.6N). Then n ‚àím remaining nodes
are sampled to m parts, where i-th part represents the size of the tree hanging on the i-th
node on the cycle. The trees are randomly generated with the given size.
‚Ä¢ Stochastic Block Model (SBM) graphs: graphs with clusters. We randomly sample the size
of each block to be random from (5, 15), and the probability of edge within the block to be
random from (0.3, 0.5) and those for other edges to be random from (0.005, 0.01). To make
all the tasks well-defined, we filtered out the unconnected graphs during the generation.
‚Ä¢ Geographic (Geo) graphs: geographic threshold graphs, but with added edges via a
minimum spanning tree algorithm, to ensure all nodes are connected. This graph generation
method is introduced by [6] in their codebase 2. We use the geographic threshold Œ∏ = 200
instead of the default value Œ∏ = 1000."
REFERENCES,0.35714285714285715,"Note that we do not have randomization after the graph generation as in [12]. Therefore, very long
diameter is preserved for some type of graphs."
REFERENCES,0.3585434173669468,"C
Method Details"
REFERENCES,0.3599439775910364,"C.1
Cross Update Function"
REFERENCES,0.36134453781512604,"The cross update function (X‚Ä≤
j, ÀÜX‚Ä≤
j, X‚Ä≤
j+1) = X-UPD(j, Xj, ÀÜXj, Xj+1) perform information ex-
change in consecutive hierarchies."
REFERENCES,0.3627450980392157,The X-Conv realization contains the following steps:
REFERENCES,0.3641456582633053,"1. Merge the node features of Xj and Xj+1 with the inter-graph feature ÀÜXj, results in ¬ØXj."
REFERENCES,0.36554621848739494,"2. Apply GN blocks on inter-graph ÀÜGj: ÀÜX‚Ä≤
j = GNi,j
inter( ÀÜGj, ¬ØXj)."
REFERENCES,0.36694677871148457,"3. Retrieve X‚Ä≤
j and X‚Ä≤
j+1 from the node features of inter-graph features ÀÜX‚Ä≤
j."
REFERENCES,0.36834733893557425,"2https://github.com/deepmind/graph_nets, the shortest path demo"
REFERENCES,0.3697478991596639,"C.2
S-EdgePool"
REFERENCES,0.3711484593837535,"In this subsection, we introduce the details of S-EdgePool. We first introduce the score generation
method, then give details about the SELECT, CONNECT, REDUCE and EXPAND functions, and lastly
provide pseudocode of the algorithm."
REFERENCES,0.37254901960784315,"C.2.1
Edge Score Generation"
REFERENCES,0.3739495798319328,"Both S-EdgePool and EdgePool methods compute a raw edge score rk for each edge k using a linear
layer:
rk = W ¬∑ (Vsk||Vtk||Ek) + b
where sk and tk are the source and target nodes of edge k, V is node features, E is edge features,
W and b are learned parameters. The raw edge scores are further normalized by a local softmax
function over all edges of a node:"
REFERENCES,0.3753501400560224,"wk = exp(rk)/
X"
REFERENCES,0.37675070028011204,"k‚Ä≤,tk‚Ä≤=tk
exp(rk‚Ä≤),"
REFERENCES,0.37815126050420167,and biased by a constant 0.5 [13].
REFERENCES,0.3795518207282913,"C.2.2
Select, Connect, Reduce and Expand"
REFERENCES,0.38095238095238093,"SELECT step. S-EdgePool shares the same computations as in EdgePool to generate learnable edge
scores, as detailed above. Then, we use a clustering procedure to determine the subset of nodes to be
reduced."
REFERENCES,0.38235294117647056,"Let Iv be the identifier of the cluster containing a set of nodes v. Initially, we let v = {v} for
every single node v. A contraction of an edge merges a pair of nodes (v, v‚Ä≤) connected by this edge
(where v ‚ààv, v‚Ä≤ ‚ààv‚Ä≤ and v Ã∏= v‚Ä≤), and thus unifies the cluster identifiers, i.e., Iv = Iv‚Ä≤ = Ivmerge
and vmerge = v ‚à™v‚Ä≤. That is, once an edge connecting any pair of nodes from two distinct clusters
is contracted, we merge the two clusters and unify their identifiers. Edges are visited sequentially
by a decreasing order on the edge scores, and contractions are implemented if valid. We set the
maximum size of the node clusters to be a parameter œÑc, where œÑc = 2 degenerates to the case of
EdgePool [13]. We further introduce the pooling ratio Œ∑v to control the minimal number of remaining
clusters after edge contractions to be N v ‚àóŒ∑v. Contractions that violate the above two constraints
are invalid and will be skipped. Both parameters control the number of nodes in the pooled graph.
In our implementation, the cluster of nodes is dynamically maintained using the disjoint-set data
structure [21]."
REFERENCES,0.38375350140056025,"Then each node cluster i collapses into a new node Àúv of the pooled graph (i.e. SÀúv = {v|Iv = i}),
with inter-graph edges connect the nodes in the cluster to the new node Àúv."
REFERENCES,0.3851540616246499,"CONNECT step. The CONNECT function rebuilds the edge set ÀúE between the nodes in ÀúV. As aforemen-
tioned, we build the pooled graph‚Äôs nodes according to node clusters. We call this mapping function
from node clusters to new nodes as c2n. After that, we build the pooled graph‚Äôs edges following three
steps: First, for all edges in the original graph, we find out the corresponding node cluster(s) of its
two endpoints (using a disjoint-set‚Äôs find index operation). Then, we find out the corresponding new
nodes by using the mapping function n. Last, we add a new edge between the new nodes."
REFERENCES,0.3865546218487395,"REDUCE and EXPAND step. The REDUCE and EXPAND are generalized from the method mentioned in
[13]. The REDUCE function computes new node features and edge features. We follow their method
to compute new node features by taking the sum of the node features and multiplying it by the edge
score. Specifically, we generalize the computation between two nodes to a node cluster. The node
clusters are maintained with a disjoint-set data structure and a cluster SÀúv consists of |SÀúv| nodes. We
define Eds
Àúv as a set of |SÀúv| ‚àí1 edges, where the edges are the selected edges to be contracted in the
SELECT step. Then,"
REFERENCES,0.38795518207282914,"cÀúv =
1 + P"
REFERENCES,0.38935574229691877,"ek‚ààEds
Àúv wk
|SÀúv|"
REFERENCES,0.3907563025210084,"VÀúv =
cÀúv
X"
REFERENCES,0.39215686274509803,"v‚ààSÀúv
Vv"
REFERENCES,0.39355742296918766,"To integrate the edge features between two node clusters, we first find all the connected edges
between the two node clusters (the edges between node clusters are edges that connect two nodes
from different node clusters). Then, we use the sum of all the connected edges‚Äô features between the
two node clusters as the new edge‚Äôs features."
REFERENCES,0.3949579831932773,"The EXPAND function is also referred as unpool operation. It computes node features of the input
graph Vv given the node features of the pooled graph VÀúv as following:"
REFERENCES,0.3963585434173669,Vv = VÀúv cÀúv
REFERENCES,0.39775910364145656,"C.2.3
Pseudo Code"
REFERENCES,0.39915966386554624,"The pseudo-code includes two parts, where Algorithm 1 describes how to maintain the clusters using
a disjoint-set data structure, and Algorithm 2 describes the procedure of S-EdgePool that generates a
pooled graph ÀúG with configurable node pooling ratio Œ∑v and maximum of cluster sizes œÑc."
REFERENCES,0.4005602240896359,Algorithm 1 Get Cluster Index And Cluster Size of a Node (Using disjoint-set data structure)
REFERENCES,0.4019607843137255,"function InitializeDisjointSet(graph G(V, E))"
REFERENCES,0.40336134453781514,for v ‚ààV do
REFERENCES,0.40476190476190477,"index[v] = v {the identifier of the cluster the node v belongs to}
end for
end function
function FindIndex(node v)"
REFERENCES,0.4061624649859944,if index[v] = v then
REFERENCES,0.40756302521008403,"return v
else"
REFERENCES,0.40896358543417366,"index[v] ‚ÜêFindIndex(index[v])
return index[v]
end if
end function
function FindIndexAndSize(node v)"
REFERENCES,0.4103641456582633,"i ‚ÜêFindIndex(v)
s ‚Üêsize[i]
return i, s
end function
function MERGE(cluster index x, cluster index y)"
REFERENCES,0.4117647058823529,"size[y] ‚Üêsize[x] + size[y]
index[x] ‚Üêindex[y]
end function"
REFERENCES,0.41316526610644255,"C.3
GFuN"
REFERENCES,0.41456582633053224,"We first realize the œïe, œïv, œïu functions in the full GN block (Sec 2.1 and [6]) as neural networks:"
REFERENCES,0.41596638655462187,"E‚Ä≤
k
=
NNe(Ek, Vsk, Vtk, u),
(2)
V‚Ä≤
i
=
NNv(¬ØE‚Ä≤
i, Vi, u),
(3)
u‚Ä≤
=
NNu(¬ØE‚Ä≤, ¬ØV‚Ä≤, u),
(4)"
REFERENCES,0.4173669467787115,"respectively, where
¬ØE‚Ä≤
i
=
œÅe‚Üív({E‚Ä≤
k}k‚àà[1...N e],tk=i),
(5)
¬ØE‚Ä≤
=
œÅe‚Üíu(E‚Ä≤),
(6)
¬ØV‚Ä≤
=
œÅv‚Üíu(V‚Ä≤).
(7)"
REFERENCES,0.41876750700280113,We further decompose the neural networks according to the features in the function:
REFERENCES,0.42016806722689076,"NNe(Ek, Vsk, Vtk, u)
=
NNe‚Üêe(Ek) + NNe‚Üêvs(Vsk) + NNe‚Üêvt(Vtk) + NNe‚Üêu(u),(8)
NNv(¬ØE‚Ä≤
i, Vi, u)
=
NNv‚Üêe(¬ØE‚Ä≤
i) + NNv‚Üêv(Vi) + NNv‚Üêu(u),
(9)
NNu(¬ØE‚Ä≤, ¬ØV‚Ä≤, u)
=
NNu‚Üêe(¬ØE‚Ä≤) + NNu‚Üêv( ¬ØV‚Ä≤) + NNu‚Üêu(u)
(10)"
REFERENCES,0.4215686274509804,"Algorithm 2 Strided EdgePool
input graph G = (V, E), edge scores w, node pooling ratio Œ∑v, maximum cluster sizes œÑc.
output pooled graph ÀúG = (ÀúV, ÀúE) and inter graph ÀÜG = (ÀÜV, ÀÜE)"
REFERENCES,0.42296918767507,"InitializeDisjointSet(G)
remains ‚ÜêN v {N v is the number of nodes in graph G}"
REFERENCES,0.42436974789915966,"¬ØE ‚ÜêSort the edges E according to the edge scores w decreasingly.
for e ‚àà¬ØE do"
REFERENCES,0.4257703081232493,"x, y ‚Üêthe two endpoints of the edge e
rx, sx ‚ÜêFindIndexAndSize(x)
ry, sy ‚ÜêFindIndexAndSize(y)
if rx Ã∏= ry and (sx + sy ‚â§œÑc) then"
REFERENCES,0.4271708683473389,"Merge(x, y)
remains ‚Üêremains ‚àí1
if remains ‚â§N v ‚àóŒ∑v then"
REFERENCES,0.42857142857142855,"break
end if
end if
end for"
REFERENCES,0.42997198879551823,"ÀúV, ÀúE, ÀÜV, ÀÜE ‚Üê{}, {}, {}, {}
create empty mapping c2n from cluster index to nodes
for v ‚ààV do"
REFERENCES,0.43137254901960786,if FindIndex(v) = v then
REFERENCES,0.4327731092436975,"create new node Àúv
c2n[v] = Àúv"
REFERENCES,0.4341736694677871,"ÀúV ‚ÜêÀúV ‚à™{Àúv}
end if
end for
for e ‚ààE do"
REFERENCES,0.43557422969187676,"x, y ‚Üêthe two endpoints of the edge e"
REFERENCES,0.4369747899159664,"Àúx ‚Üêc2n[FindIndex(x)]
Àúy ‚Üêc2n[FindIndex(y)]"
REFERENCES,0.438375350140056,"ÀúE ‚ÜêÀúE ‚à™{(Àúx, Àúy)}
end for
for v ‚ààV do"
REFERENCES,0.43977591036414565,Àúv ‚Üêc2n[FindIndex(v)]
REFERENCES,0.4411764705882353,"ÀÜE ‚ÜêÀÜE ‚à™{(v, Àúv)}
end for"
REFERENCES,0.4425770308123249,ÀÜV ‚ÜêV ‚à™ÀúV
REFERENCES,0.44397759103641454,"However, such GN block uses 10 times the number of parameters as the standard GCN [34] layer
when the node, edge and global embedding dimensions are all equivalent. In practice, we disable all
computations related to global features u, as well as the neural networks NNe‚Üêe and NNe‚Üêvt. We
also set NNv‚Üêe to be Identity."
REFERENCES,0.44537815126050423,"In practice, we use the summation function as the aggregator function œÅe‚Üív by default. But other
choices like MEAN, MAX, gated summation, attention or their combinations can also be used."
REFERENCES,0.44677871148459386,"Overall, we call such GN block as graph full network (GFuN)."
REFERENCES,0.4481792717086835,"C.4
Encorder and Decoder"
REFERENCES,0.4495798319327731,"Encoder. For input embedding, we use the Linear layer or Embedding layer to embed input features.
For example, we follow [15] and use the Linear layer on MNIST and CIFAR10 datasets, and use
the Embedding layer on ZINC and AQSOL datasets. For the molecular graph in OGB, we use the
same embedding method as in the original work [28]. Besides, we can adopt positional encoding
methods like Laplacian [15] and Random Walk [16] to further embed global and local graph structure
information. The embedding of positional encoding can be combined into (like concatenation,
addition, etc.) input features and form new embeddings."
REFERENCES,0.45098039215686275,"Decoder. We can freely choose from the multi-scale features computed during the process stage as
inputs to the decoder module. Empirically, we use the features on the original graph for prediction in
all experiments. For node-level tasks, we apply a last GNN layer on the original graph to get logits
for every node. For graph-level tasks, we first use global pooling functions to aggregate features. We
can use common global pooling methods like SUM, MEAN, MAX, or their combination. After the
global pool, we use MLP layer(s) to generate the prediction."
REFERENCES,0.4523809523809524,"C.5
Architecture Variants"
REFERENCES,0.453781512605042,"We can replace some GN blocks within Mee layers as an Identity block to reduce the time complexity.
We call the height j is reserved if the intra GN block of height j is not replaced by an Identity block.
We prefer to reserve an interval of consecutive heights for the Mee layers. (The inter GN blocks
between these heights remain unchanged while others are replaced as identities) By varying the
heights reserved in each Mee layers, we can create a large number of variants of MeGraph model
including U-Shaped, Bridge-Shaped and Staircase-Shaped."
REFERENCES,0.45518207282913165,"U-Shaped. This variant is similar to Graph U-Net [22]. In this U-Shaped variant, the relationship
between the number of layers n and height h is n = 2h + 1, and there is only one GN block in each
layer. We keep the GN block at height j = i for each layer i at the first half layers and keep the GN
block at height j = n ‚àíi + 1 for each layer i at the later half layers. In the middle layer, only the last
height j = h = (n ‚àí1)/2 has a GN block."
REFERENCES,0.4565826330532213,"Bridge-Shaped. In this variant, all GN blocks are combined like an arch bridge. Describe in detail,
in the first and last layers, there are GN blocks in each height. In other layers, there are GN blocks at
a height of 1 to j (where 1 < j < h)."
REFERENCES,0.4579831932773109,"Staircase-Shaped. There are four forms in this variant, and the number of layers n is equal to the
height h in all forms. The first form is like the ‚Äòdownward‚Äô staircase. In each layer i of this form,
there are GN blocks at the height of j to h (where j = i). The second form is the inverted first form.
In each layer i of this second form, there are GN blocks at height of 1 to h ‚àíi + 1 (where j = i).
The last two forms are the mirror of the first and second forms."
REFERENCES,0.45938375350140054,"D
Theoretical Discussions"
REFERENCES,0.46078431372549017,"D.1
Smaller Number of Aggregation Steps for Capturing Long-Range Interactions"
REFERENCES,0.46218487394957986,We rephrase the analysis provided in [47] as following:
REFERENCES,0.4635854341736695,"We analyze the number of aggregation steps required to capture long-range interactions between
nodes in the original graph while assuming the node representation capacity is large enough."
REFERENCES,0.4649859943977591,"Standard message-passing GNNs require n aggregation steps to capture long-range interactions of n
hops away, therefore requiring a stack of n layers, which could be expensive when n is large."
REFERENCES,0.46638655462184875,"We also assume the height h of the hierarchy is large enough so that all nodes of the original graph
are pooled into a single node. In that case, the information aggregation along the hierarchy captures
all pairs of LRIs into the embedding of the single node. Which means the number of aggregation
steps of MeGraph is h. When we adopt a pooling method that coarsens the graph at least half, h is
at most O(log(|V |)) where |V | is the number of nodes of the input graph. Therefore, the height h is
significantly smaller than the diameter of the graph (which could be O(|V |)) in most cases."
REFERENCES,0.4677871148459384,"D.2
MeGraph can degenerate to standard GNNs"
REFERENCES,0.469187675070028,"MeGraph can learn a gating function (within the X-UPD function) that only reserves the features of
the same scale while performing cross-scale information exchanging. In that case, there will be no
information exchange across multi-scale graphs, and features other than those in the original scale
will not be aggregated. We provide a proof sketch below, while the results of the random pooling
method ablation study in Sec. 4.4 also provide empirical evidence."
REFERENCES,0.47058823529411764,"Proof: The cross update function is (X‚Ä≤
j, ÀÜX‚Ä≤
j, X‚Ä≤
j+1) = X-UPD(j, Xj, ÀÜ
Xj, Xj+1). There is a residual
function applied here, and we assume it is implemented as a gated residual: X‚Ä≤‚Ä≤
j = œÉ(Œ±)Xj +œÉ(Œ≤)X‚Ä≤
j,
where œÉ is the sigmoid function and Œ±, Œ≤ are learnable parameters. Theoremetically, it is possible that"
REFERENCES,0.47198879551820727,"Table 7: Running time (s) for one epoch on the GNN benchmark. See Sec. E for more implementation
details."
REFERENCES,0.4733893557422969,"ZINC
AQSOL
CIFAR10
MNIST
PATTERN
CLUSTER"
REFERENCES,0.47478991596638653,"Megraph (h = 5)
25.69
20.22
336.63
307.23
101.52
69.65
Megraph (h = 1)
2.41
1.67
51.74
38.60
9.21
6.52"
REFERENCES,0.47619047619047616,"Table 8: Running time (s) for one epoch on the OGBG datasets. See Sec. E for more implementation
details."
REFERENCES,0.47759103641456585,"molhiv
molbace
molbbbp
molclintox
molsider"
REFERENCES,0.4789915966386555,"Megraph (h = 5)
393.42
14.70
20.36
14.03
14.12
Megraph (h = 1)
22.50
1.43
1.58
1.26
1.41"
REFERENCES,0.4803921568627451,"moltox21
moltoxcast
molesol
molfreesolv
mollipo"
REFERENCES,0.48179271708683474,"Megraph (h = 5)
70.15
78.77
11.68
6.24
44.77
Megraph (h = 1)
5.27
8.17
0.76
0.41
2.77"
REFERENCES,0.4831932773109244,"œÉ(Œ±) = 1 and œÉ(Œ≤) = 0 after training. In that case, X‚Ä≤‚Ä≤
j = Xj, which means Xj is not changed over
steps 2 and 3 of the Mee layer. Therefore, Xi
0 = GN i,0
intra(G, Xi‚àí1
0
), this is equivalent to a simple
GNN layer that Xi = GNNi(G, Xi‚àí1) as Xi
0 is the features of the original graph and GNintra is a
GNN layer. Therefore, MeGraph degenerates to standard message-passing GNNs in this case. ‚ñ†"
REFERENCES,0.484593837535014,"E
Implementation and Training Details"
REFERENCES,0.48599439775910364,We use PyTorch [45] and Deep Graph Library (DGL) [54] to implement our method.
REFERENCES,0.48739495798319327,"We implement S-EdgePool using DGL, extending from the original implementation of EdgePool in
the Pytorch Geometric library (PYG) [19]. We did Constant optimization over the implementation
to speed up the training and inference of the pooling. We further use Taichi-Lang [29] to speed up
the dynamic node clustering process of S-EdgePool. The practical running time of MeGraph model
with height h > 1 after optimization is about 2h times as the h = 1 baseline. This is still slower
than the theoretical computational complexity due to the constant in the implementation and the
difficulty of paralleling the sequential visitation of edges (according to their scores) in the EdgePool
and S-EdgePool. This process could be further speed up by implementing the operations with the
CUDA library. We provide the practical running time for h > 1 and h = 1 in GNN benchmark and
OGB-G datasets in Tables 7 and 8. The speed is slower than the theoretical one partially due to the
pooling ratio of edges being larger than the nodes, making the number of edges decrease slowly over
the hierarchy. To further speed up, we could use the variants of MeGraph introduced in App. C.5 by
skipping some computation modules."
REFERENCES,0.4887955182072829,"We run all our experiments on V100 GPUs and M40 GPUs. For training the neural networks, we use
Adam [33] as the optimizer. We report the hyper-parameters of the Megraph in Table 9."
REFERENCES,0.49019607843137253,"For models using GFuN layer as the core GN block, we find it benefits from using layer norms [4].
However, for models using GCN layer as the core GN block, we find it performs best when using
batch norms [31]."
REFERENCES,0.49159663865546216,"F
Additional Experiment Results"
REFERENCES,0.49299719887955185,"F.1
Experimental Protocol"
REFERENCES,0.4943977591036415,"We evaluate MeGraph on public real-world graph benchmarks. To fairly compare MeGraph with the
baselines, we use the following experimental protocols. We first report the public baseline results and
our reproduced standard GCN‚Äôs results. We then replace GCN layers with GFuN layers (which is
equivalent to MeGraph (h = 1)) to serve as another baseline. We tune the hyper-parameters (such
as learning rate, dropout rate and the readout global pooling method, etc.) of MeGraph (h = 1) and
choose the best configurations. We then run other diversely configured MeGraph candidates by tuning"
REFERENCES,0.4957983193277311,"Table 9: Hyper-parameters of the standard version of MeGraph for each dataset. It is worth noting
that the total number of GNN layers is equals to one plus the number of Mee layers as n + 1."
REFERENCES,0.49719887955182074,"Hyper-parameters
Synthetic
Datasets"
REFERENCES,0.49859943977591037,"Graph
Theory
Benchmark"
REFERENCES,0.5,"LRGB
Benchmark
GNN
Benchmark
OGB
Benchmark
TU
Datasets"
REFERENCES,0.5014005602240896,"Repeated Runs
10
5
4
4
5
1 for each fold"
REFERENCES,0.5028011204481793,"Epochs per run
200 for BA*
500 for Tree*
300
(200 for MCC)
200"
REFERENCES,0.5042016806722689,"200
(100 for
MNIST,
CIFRA10)"
REFERENCES,0.5056022408963585,"100
100
(200 for
ENZYMES)"
REFERENCES,0.5070028011204482,"Learning rate
0.002
0.002
(0.005 for MCC)
0.001
0.001
0.001
0.002"
REFERENCES,0.5084033613445378,"Weight decay
0.0005
0.0005
0
0
0.0005
0.0005"
REFERENCES,0.5098039215686274,"Node hidden dim
64
128
160
144
300
128"
REFERENCES,0.511204481792717,"Edge hidden dim
(for GFuN)
64
128
160
144
300
128"
REFERENCES,0.5126050420168067,"Num Mee
layers n
-
-
4
3
4
2"
REFERENCES,0.5140056022408963,"Height h
-
-
9
5
5
3 or 5"
REFERENCES,0.5154061624649859,"Batch size
32
32
128
128
32
128"
REFERENCES,0.5168067226890757,"Input embedding
False
True
True
True
True
True"
REFERENCES,0.5182072829131653,"Global pooling
Mean
Mean
Max"
REFERENCES,0.5196078431372549,"Mean
Max
Sum
Mean
Mean
Mean
Max
Sum"
REFERENCES,0.5210084033613446,"Dataset split
(train:val:test)
8:1:1
8:1:1
Original
split
Original
split
Original
split
10-fold cross
validation"
REFERENCES,0.5224089635854342,"other hyper-parameters that only matters for h > 1, and these hyper-parameters are referred to as the
MeGraph hyper-parameters. Detailed configurations are put in Table 9 in App. E."
REFERENCES,0.5238095238095238,"F.2
Visualization"
REFERENCES,0.5252100840336135,"We plotted the graph hierarchies discovered by MeGraph in the shortest path tasks of Graph Theory
Benchmark. In Figure 5, the S-EdgePool with Œ∑v = 0.3, œÑc = 4 well preserves the structure of
the graph after pooling, while the S-EdgePool with Œ∑v = 0.3 (no cluster size limit) sometimes
pooled too many nodes together, breaking the graph structure. The former S-EdgePool leads to
better performance as indicated in Table 1. We also plot the hierarchy for SBM-generated graphs in
Figure 6, indicating that EdgePool can handle graphs that naturally contains clusters of different size."
REFERENCES,0.5266106442577031,"F.3
Other Real-Wrold Datasets"
REFERENCES,0.5280112044817927,"TU dataset consists of over 120 datasets of varying sizes from a wide range of applications. We
choose 10 datasets, 5 of which are molecule datasets (MUTAG, NCI1, PROTEINS, D&D and
ENZYMES) and the other 5 are social networks (IMDB-B, IMDB-M, REDDIT-BINARY, REDDIT-
MULTI-5K and REDDIT-MULTI-12K). They are all graph classification tasks. For more details of
each dataset, please refer to the original work [42]."
REFERENCES,0.5294117647058824,"Our Megraph uses the same network structure and hyper-parameters for the same type of dataset. As
shown in Table 10, our Megraph achieves about 1% absolute gain than the h = 1 Baselines."
REFERENCES,0.530812324929972,"F.4
GFuN"
REFERENCES,0.5322128851540616,"We show our GFuN results on real-world datasets compared to our reproduced GCN in Table 11, 12
and 13. Both GCN and GFuN have the same hyper-parameters except the batch norm for GCN and
layer norm for GFuN as stated in Appendix E. 0
1 2 345 6 7 8 9
10 11 12
13"
REFERENCES,0.5336134453781513,"14
1516"
REFERENCES,0.5350140056022409,"17
18
1920
21
22 23"
REFERENCES,0.5364145658263305,"24
25
26"
REFERENCES,0.5378151260504201,"27
28
29
30
31"
REFERENCES,0.5392156862745098,"32
33
3435
36"
REFERENCES,0.5406162464985994,"37
38
39
40 41 42"
REFERENCES,0.542016806722689,"43
44
45 46 47 48 49 50 51
52"
REFERENCES,0.5434173669467787,"5354
55 56 57 58 59"
REFERENCES,0.5448179271708683,"60
61
62"
REFERENCES,0.5462184873949579,"63
64
65 66 67"
REFERENCES,0.5476190476190477,"68
69
70 71 72 73 74 75 76
77 78"
REFERENCES,0.5490196078431373,"79
80
81
82
83
84 85 86"
REFERENCES,0.5504201680672269,"87
88
89 90 91
92 93
94 95 96"
REFERENCES,0.5518207282913166,"979899
100 101 102 103 104 105"
REFERENCES,0.5532212885154062,"106
107
108"
REFERENCES,0.5546218487394958,"109
110
111
112 113"
REFERENCES,0.5560224089635855,"114
115
116"
REFERENCES,0.5574229691876751,"117
118 119"
REFERENCES,0.5588235294117647,"120
121
122
123"
REFERENCES,0.5602240896358543,"124
125 126"
REFERENCES,0.561624649859944,"127
128
129"
REFERENCES,0.5630252100840336,"130
131 132"
REFERENCES,0.5644257703081232,"133
134
135
136 137 138"
REFERENCES,0.5658263305322129,"139
140
141
142 143"
REFERENCES,0.5672268907563025,"144
145"
REFERENCES,0.5686274509803921,"146
147"
REFERENCES,0.5700280112044818,"148
149
150
151
152
153 154 155"
REFERENCES,0.5714285714285714,"156
157 158"
REFERENCES,0.572829131652661,"159
160
161"
REFERENCES,0.5742296918767507,"162
163
164"
REFERENCES,0.5756302521008403,"165
166 167"
REFERENCES,0.5770308123249299,"168
169"
REFERENCES,0.5784313725490197,"170
171
172 0 1 2 3 4 56
7 8
9"
REFERENCES,0.5798319327731093,"10
1112 13 14 15 16 17 18 19 20 21 22"
REFERENCES,0.5812324929971989,"23
24
25 26 27 28"
REFERENCES,0.5826330532212886,"29 3031
32 33 34"
REFERENCES,0.5840336134453782,"35
36
37 38
39 40
41 42 43 44
45"
REFERENCES,0.5854341736694678,"46
47
48"
REFERENCES,0.5868347338935574,"49
5051 52 53
54 55 5657 0 1 2 3 4 5 6 7 8"
REFERENCES,0.5882352941176471,"9
10 11
12 13 14 15 16 17 18 0 1
2 3
4 5 6 0
1 2 345 6 7 8 9
10 11 12
13"
REFERENCES,0.5896358543417367,"14
1516"
REFERENCES,0.5910364145658263,"17
18
1920
21
22 23"
REFERENCES,0.592436974789916,"24
25
26"
REFERENCES,0.5938375350140056,"27
28
29
30
31"
REFERENCES,0.5952380952380952,"32
33
3435
36"
REFERENCES,0.5966386554621849,"37
38
39
40 41 42"
REFERENCES,0.5980392156862745,"43
44
45 46 47 48 49 50 51
52"
REFERENCES,0.5994397759103641,"5354
55 56 57 58 59"
REFERENCES,0.6008403361344538,"60
61
62"
REFERENCES,0.6022408963585434,"63
64
65 66 67"
REFERENCES,0.603641456582633,"68
69
70 71 72 73 74 75 76
77 78"
REFERENCES,0.6050420168067226,"79
80
81
82
83
84 85 86"
REFERENCES,0.6064425770308123,"87
88
89 90 91
92 93
94 95 96"
REFERENCES,0.6078431372549019,"979899
100 101 102 103 104 105"
REFERENCES,0.6092436974789915,"106
107
108"
REFERENCES,0.6106442577030813,"109
110
111
112 113"
REFERENCES,0.6120448179271709,"114
115
116"
REFERENCES,0.6134453781512605,"117
118 119"
REFERENCES,0.6148459383753502,"120
121
122
123"
REFERENCES,0.6162464985994398,"124
125 126"
REFERENCES,0.6176470588235294,"127
128
129"
REFERENCES,0.6190476190476191,"130
131 132"
REFERENCES,0.6204481792717087,"133
134
135
136 137 138"
REFERENCES,0.6218487394957983,"139
140
141
142 143"
REFERENCES,0.623249299719888,"144
145"
REFERENCES,0.6246498599439776,"146
147"
REFERENCES,0.6260504201680672,"148
149
150
151
152
153 154 155"
REFERENCES,0.6274509803921569,"156
157 158"
REFERENCES,0.6288515406162465,"159
160
161"
REFERENCES,0.6302521008403361,"162
163
164"
REFERENCES,0.6316526610644257,"165
166 167"
REFERENCES,0.6330532212885154,"168
169"
REFERENCES,0.634453781512605,"170
171
172 0 1 2 3 4 5 6 7
8 9
10 11 12 13"
REFERENCES,0.6358543417366946,"14
15
16 17 18 19 20 21 22"
REFERENCES,0.6372549019607843,"23
24
25 26 27 28 29 30 31 32
33 34 35
36 37 38 39 40
41"
REFERENCES,0.6386554621848739,"42
43
44
45 46 47 48 49 50 0 1
2 3 4 5 6
7 8
9 10 11
12 13 14 0 1 2 3 0 123 4
5 6 7"
REFERENCES,0.6400560224089635,"8
9
1011
12"
REFERENCES,0.6414565826330533,"1314
15
16
17
1819
20
21"
REFERENCES,0.6428571428571429,"22 23
24
25
26"
REFERENCES,0.6442577030812325,"27 28
29
30
31
32"
REFERENCES,0.6456582633053222,"33
34
35
36
3738
39 40 41"
REFERENCES,0.6470588235294118,"42
43
44 45"
REFERENCES,0.6484593837535014,"4647
48
49
50"
REFERENCES,0.6498599439775911,"51
52
53
54
55 56 57"
REFERENCES,0.6512605042016807,"58 59
60
61
62
63 64
65 66
67"
REFERENCES,0.6526610644257703,"6869 70
71
72
73
74
75
76
77 78 79 80
81 82
83 84
85 86
87 88 89
90"
REFERENCES,0.65406162464986,"91
92
93
94 95"
REFERENCES,0.6554621848739496,"96
97
98
99100
101"
REFERENCES,0.6568627450980392,"102
103
104
105
106
107"
REFERENCES,0.6582633053221288,"108
109
110 111
112 113"
REFERENCES,0.6596638655462185,"114
115
116
117"
REFERENCES,0.6610644257703081,"118
119
120
121"
REFERENCES,0.6624649859943977,"122
123"
REFERENCES,0.6638655462184874,"124
125
126
127128
129
130
131"
REFERENCES,0.665266106442577,"132
133"
REFERENCES,0.6666666666666666,"134
135
136"
REFERENCES,0.6680672268907563,"137
138"
REFERENCES,0.6694677871148459,"139
140 141"
REFERENCES,0.6708683473389355,"142
143"
REFERENCES,0.6722689075630253,"144
145146
147"
REFERENCES,0.6736694677871149,"148
149"
REFERENCES,0.6750700280112045,"150
151 152"
REFERENCES,0.6764705882352942,"153
154
155
156
157
158
159"
REFERENCES,0.6778711484593838,"160
161
162
163 164"
REFERENCES,0.6792717086834734,"165
166"
REFERENCES,0.680672268907563,"167
168169
170171"
REFERENCES,0.6820728291316527,"172
173"
REFERENCES,0.6834733893557423,"174
175
176177 178"
REFERENCES,0.6848739495798319,"179
180"
REFERENCES,0.6862745098039216,"181
182 183"
REFERENCES,0.6876750700280112,"184
185186187 0
1
2 3 45
6 7
8"
REFERENCES,0.6890756302521008,"910
11 12
13"
REFERENCES,0.6904761904761905,"14
15
16
17 18 19"
REFERENCES,0.6918767507002801,"20
21
22 23 24
25 26"
REFERENCES,0.6932773109243697,"2728
29 30
31 32
33 34 35 36
37 38
39 40
41 42 43 44
45 4647 48 49 50 5152 53
54 55 0 1 2 3 4
5 6 7
8"
REFERENCES,0.6946778711484594,"9
10
11 12
13 14 15 0 1 2 3 4 5 0 123 4
5 6 7"
REFERENCES,0.696078431372549,"8
9
1011
12"
REFERENCES,0.6974789915966386,"1314
15
16
17
1819
20
21"
REFERENCES,0.6988795518207283,"22 23
24
25
26"
REFERENCES,0.7002801120448179,"27 28
29
30
31
32"
REFERENCES,0.7016806722689075,"33
34
35
36
3738
39 40 41"
REFERENCES,0.7030812324929971,"42
43
44 45"
REFERENCES,0.7044817927170869,"4647
48
49
50"
REFERENCES,0.7058823529411765,"51
52
53
54
55 56 57"
REFERENCES,0.7072829131652661,"58 59
60
61
62
63 64
65 66
67"
REFERENCES,0.7086834733893558,"6869 70
71
72
73
74
75
76
77 78 79 80
81 82
83 84
85 86
87 88 89
90"
REFERENCES,0.7100840336134454,"91
92
93
94 95"
REFERENCES,0.711484593837535,"96
97
98
99100
101"
REFERENCES,0.7128851540616247,"102
103
104
105
106
107"
REFERENCES,0.7142857142857143,"108
109
110 111
112 113"
REFERENCES,0.7156862745098039,"114
115
116
117"
REFERENCES,0.7170868347338936,"118
119
120
121"
REFERENCES,0.7184873949579832,"122
123"
REFERENCES,0.7198879551820728,"124
125
126
127128
129
130
131"
REFERENCES,0.7212885154061625,"132
133"
REFERENCES,0.7226890756302521,"134
135
136"
REFERENCES,0.7240896358543417,"137
138"
REFERENCES,0.7254901960784313,"139
140 141"
REFERENCES,0.726890756302521,"142
143"
REFERENCES,0.7282913165266106,"144
145146
147"
REFERENCES,0.7296918767507002,"148
149"
REFERENCES,0.7310924369747899,"150
151 152"
REFERENCES,0.7324929971988795,"153
154
155
156
157
158
159"
REFERENCES,0.7338935574229691,"160
161
162
163 164"
REFERENCES,0.7352941176470589,"165
166"
REFERENCES,0.7366946778711485,"167
168169
170171"
REFERENCES,0.7380952380952381,"172
173"
REFERENCES,0.7394957983193278,"174
175
176177 178"
REFERENCES,0.7408963585434174,"179
180"
REFERENCES,0.742296918767507,"181
182 183"
REFERENCES,0.7436974789915967,"184
185186187 0
1
2 34 5 6
78 9
10 11"
REFERENCES,0.7450980392156863,"12
13
14
15
1617
18 19 20 21 22 23"
REFERENCES,0.7464985994397759,"24
25
2627 28"
REFERENCES,0.7478991596638656,"29
30
31 32
33"
REFERENCES,0.7492997198879552,"34
35
3637
38 39"
REFERENCES,0.7507002801120448,"40
4142
43 44 45 46
47"
REFERENCES,0.7521008403361344,"48
49
50
51
52 53 54 55 0 12 3
4 5 6 7 8"
REFERENCES,0.7535014005602241,"910
1112
13 14 15 0 1 2
3"
REFERENCES,0.7549019607843137,"Figure 5: The visualization of the resulting graph hierarchy of MeGraph. Each row is a graph hierarchy, and the
nodes with the same color in the same graph are in the same cluster and are pooled into a single node in the next
hierarchy. The 1st, 3rd hierarchies use S-EdgePool with Œ∑v = 0.3, œÑc = 4 and the 2nd and 4th hierarchies use
S-EdgePool with Œ∑v = 0.3. The graph in the 1st and 2nd hierarchies is generated by the ‚Äôlobster‚Äô method, the
graph in the 3rd and 4th hierarchies is generated by the ‚Äôgeo‚Äô method. The task is to compute the length of the
shortest path between two nodes. 0 1
23 4 5
6 7 8 9
10 11 12 13 14
15 16"
REFERENCES,0.7563025210084033,"17
18
19 20 21 22"
REFERENCES,0.757703081232493,"23
24252627
28
29 30"
REFERENCES,0.7591036414565826,"31
32
33 34 35
36 37 38 39"
REFERENCES,0.7605042016806722,"40
41
42
43
44
45
46
47 48 49 0 1 2 3
4 5 6 7 8
9 10 11 12 13
14 15 0 1
23 4 5
6 7 8 9
10 11 12 13 14
15 16"
REFERENCES,0.7619047619047619,"17
18
19 20 21 22"
REFERENCES,0.7633053221288515,"23
24252627
28
29 30"
REFERENCES,0.7647058823529411,"31
32
33 34 35
36 37 38 39"
REFERENCES,0.7661064425770309,"40
41
42
43
44
45
46
47 48 49 0 1 2 3 4 5
6 7 8 9 10"
REFERENCES,0.7675070028011205,"Figure 6: The graph hierarchy visualization, using S-EdgePool with Œ∑v = 0.3, œÑc = 4 (left two figures) and
Louvain pooling (right two figures) on the same SBM generated graph. The task is the shortest path."
REFERENCES,0.7689075630252101,"F.5
Synthetic Datasets"
REFERENCES,0.7703081232492998,"Figures 7 and 8 show the influence of the height h and the number of Mee layers n for MeGraph
model on the BAShape and BACommunity datasets. The trends are similar while less significant on
easier datasets BAShape and BACommunity."
REFERENCES,0.7717086834733894,"F.6
Varying GN block"
REFERENCES,0.773109243697479,"We vary the aggregation function of the GN block as attention (w/ ATT) and gated function (w/
GATE). We observe similar results as in Sec. 4.2 and 4.3, verifying the robustness of MeGraph over
different GN blocks. Results are shown in Tables 16, 17 and 18."
REFERENCES,0.7745098039215687,"Table 10: Results on Tu Dataset. ‚Ä† means the results taken from [11] (*: The result of GCN on
ENZYMES is 100 epoch)."
REFERENCES,0.7759103641456583,"Model
MUTAG ‚Üë
NCI1 ‚Üë
PROTEINS ‚Üë
D&D ‚Üë
ENZYMES ‚Üë
Average"
REFERENCES,0.7773109243697479,"GCN‚Ä†
87.20 ¬±5.11
83.65 ¬±1.69
75.65 ¬±3.24
79.12 ¬±3.07
66.50 ¬±6.91*
78.42
GIN‚Ä†
89.40 ¬±5.60
82.70 ¬±1.70
76.20 ¬±2.80
-
-
-"
REFERENCES,0.7787114845938375,"GCN
92.46 ¬±6.55
82.55 ¬±0.99
77.82 ¬±4.52
80.56 ¬±2.40
74.17 ¬±5.59
81.51
MeGraph (h=1)
93.01 ¬±6.83
82.53 ¬±1.89
81.32 ¬±4.08
81.32 ¬±3.17
74.83 ¬±3.20
82.60
MeGraph
93.07 ¬±6.71
83.99 ¬±0.98
81.41 ¬±3.10
81.24 ¬±2.39
75.17 ¬±4.86
82.98
MeGraphbest
94.12 ¬±5.02
84.40 ¬±1.11
81.68 ¬±3.40
82.00 ¬±2.86
75.17 ¬±4.86
83.47"
REFERENCES,0.7801120448179272,"Model
IMDB-B ‚Üë
IMDB-M ‚Üë
RE-B ‚Üë
RE-M5K ‚Üë
RE-M12K ‚Üë
Average"
REFERENCES,0.7815126050420168,"GCN
76.00 ¬±3.44
50.33 ¬±1.89
91.15 ¬±1.63
56.47 ¬±1.54
48.71 ¬±0.88
64.53
MeGraph (h=1)
68.60 ¬±3.53
51.33 ¬±2.23
93.10 ¬±1.16
57.47 ¬±2.31
51.56 ¬±1.06
64.41
MeGraph
72.40 ¬±2.80
51.27 ¬±2.71
93.75 ¬±1.25
57.69 ¬±2.22
52.03 ¬±0.86
65.43
MeGraphbest
74.30 ¬±2.97
52.00 ¬±2.49
93.75 ¬±1.25
58.45 ¬±2.22
52.13 ¬±1.01
66.13"
REFERENCES,0.7829131652661064,Table 11: Comparison between GCN and GFuN on GNN benchmark.
REFERENCES,0.7843137254901961,"Model
ZINC ‚Üì
AQSOL ‚Üì
MNIST ‚Üë
CIFAR10 ‚Üë
PATTERN ‚Üë
CLUSTER ‚Üë"
REFERENCES,0.7857142857142857,"GCN
0.426 ¬±0.015
1.397 ¬±0.029
90.140 ¬±0.140
51.050 ¬±0.390
84.672 ¬±0.054
47.541 ¬±0.940
GFuN
0.364 ¬±0.003
1.386 ¬±0.024
95.560 ¬±0.190
61.060 ¬±0.500
84.845 ¬±0.021
58.178 ¬±0.079"
REFERENCES,0.7871148459383753,"MeGraph (h=1)
0.323 ¬±0.002
1.075 ¬±0.007
97.570 ¬±0.168
69.890 ¬±0.209
84.845 ¬±0.021
58.178 ¬±0.079
MeGraph
0.260 ¬±0.005
1.002 ¬±0.021
97.860 ¬±0.098
69.925 ¬±0.631
86.507 ¬±0.067
68.603 ¬±0.101"
REFERENCES,0.788515406162465,Table 12: Comparison between GCN and GFuN on OGB-G.
REFERENCES,0.7899159663865546,"Model
molhiv ‚Üë
molbace ‚Üë
molbbbp ‚Üë
molclintox ‚Üë
molsider ‚Üë"
REFERENCES,0.7913165266106442,"GCN
75.40 ¬±1.29
76.01 ¬±3.31
67.35 ¬±0.96
89.62 ¬±2.27
58.08 ¬±0.78
GFuN
78.54 ¬±1.14
71.77 ¬±2.15
67.56 ¬±1.11
89.77 ¬±3.48
58.28 ¬±0.51"
REFERENCES,0.7927170868347339,"MeGraph (h=1)
78.54 ¬±1.14
71.77 ¬±2.15
67.56 ¬±1.11
89.77 ¬±3.48
58.28 ¬±0.51
MeGraph
77.20 ¬±0.88
78.52 ¬±2.51
69.57 ¬±2.33
92.04 ¬±2.19
59.01 ¬±1.45"
REFERENCES,0.7941176470588235,"Model
moltox21 ‚Üë
moltoxcast ‚Üë
molesol ‚Üì
molfreesolv ‚Üì
mollipo ‚Üì"
REFERENCES,0.7955182072829131,"GCN
75.11 ¬±0.41
64.13 ¬±0.52
1.141 ¬±0.02
2.407 ¬±0.15
0.788 ¬±0.01
GFuN
75.89 ¬±0.45
64.49 ¬±0.46
1.079 ¬±0.02
2.017 ¬±0.08
0.768 ¬±0.00"
REFERENCES,0.7969187675070029,"MeGraph (h=1)
75.89 ¬±0.45
64.49 ¬±0.46
1.079 ¬±0.02
2.017 ¬±0.08
0.768 ¬±0.00
MeGraph
78.11 ¬±0.47
67.67 ¬±0.53
0.886 ¬±0.02
1.876 ¬±0.05
0.726 ¬±0.00"
REFERENCES,0.7983193277310925,Table 13: Comparison between GCN and GFuN on Tu Dataset.
REFERENCES,0.7997198879551821,"Model
MUTAG ‚Üë
NCI1 ‚Üë
PROTEINS ‚Üë
D&D ‚Üë
ENZYMES ‚Üë
Average"
REFERENCES,0.8011204481792717,"GCN
92.46 ¬±6.55
82.55 ¬±0.99
77.82 ¬±4.52
80.56 ¬±2.40
74.17 ¬±5.59
81.51
GFuN
93.01 ¬±7.96
82.80 ¬±1.30
80.60 ¬±3.83
82.43 ¬±2.60
73.00 ¬±5.31
82.37"
REFERENCES,0.8025210084033614,"Model
IMDB-B ‚Üë
IMDB-M ‚Üë
RE-B ‚Üë
RE-M5K ‚Üë
RE-M12K ‚Üë
Average"
REFERENCES,0.803921568627451,"GCN
76.00 ¬±3.44
50.33 ¬±1.89
91.15 ¬±1.63
56.47 ¬±1.54
48.71 ¬±0.88
64.53
GFuN
68.90 ¬±3.42
51.27 ¬±3.22
92.25 ¬±1.12
57.53 ¬±1.31
51.54 ¬±1.19
64.30"
REFERENCES,0.8053221288515406,"1
2
3
4
5
6
7
8
9
10
Num Layer 0.80 0.84 0.88 0.92 0.96 1.00"
REFERENCES,0.8067226890756303,Accuracy
REFERENCES,0.8081232492997199,TreeCycle
REFERENCES,0.8095238095238095,"height=1
height=2 3
4 5
6"
REFERENCES,0.8109243697478992,"1
2
3
4
5
6
7
8
9
10
Num Layer 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.8123249299719888,Accuracy
REFERENCES,0.8137254901960784,TreeGrid
REFERENCES,0.8151260504201681,"height=1
height=2 3
4 5
6"
REFERENCES,0.8165266106442577,"Figure 7: Node Classification accuracy for MeGraph model on TreeCycle (left) and TreeGrid (right) datasets,
varying the height h and the number of Mee layers n. A clear gap can be observed between heights 1, 2 and ‚â•3.
The concrete number of accuracy can be found in Table 14."
REFERENCES,0.8179271708683473,"1
2
3
4
5
6
7
8
9
10
Num Layer 0.980 0.984 0.988 0.992 0.996 1.000"
REFERENCES,0.819327731092437,Accuracy
REFERENCES,0.8207282913165266,BAShape
REFERENCES,0.8221288515406162,"height=1
height=2 3
4 5
6"
REFERENCES,0.8235294117647058,"1
2
3
4
5
6
7
8
9
10
Num Layer 0.95 0.96 0.97 0.98 0.99 1.00"
REFERENCES,0.8249299719887955,Accuracy
REFERENCES,0.8263305322128851,BACommunity
REFERENCES,0.8277310924369747,"height=1
height=2 3
4 5
6"
REFERENCES,0.8291316526610645,"Figure 8: Node Classification accuracy for MeGraph model on BAShape (left) and BACommunity (right)
datasets, varying the height h and the number of Mee layers n. A clear gap can be observed between heights 1
and ‚â•2. The concrete number of accuracy can be found in Table 15."
REFERENCES,0.8305322128851541,"F.7
Graph Theory Dataset"
REFERENCES,0.8319327731092437,"We provide a list of tables (from Table 19 to 30) showing the individual results of Table 1 for each
possible graph generation method. Each table contains a list of variants of models and 5 tasks. Some
graph generation methods and task combinations are trivial so we filter them out."
REFERENCES,0.8333333333333334,Table 14: Node Classification accuracy for MeGraph model on TreeCycle (above) and TreeGrid (below).
REFERENCES,0.834733893557423,"layer
height
1
2
3
4
5
6"
REFERENCES,0.8361344537815126,"1
61.48 ¬±6.04
76.59 ¬±4.41
91.48 ¬±2.70
98.52 ¬±1.69
97.95 ¬±2.32
98.52 ¬±1.35
2
67.27 ¬±6.91
81.59 ¬±4.03
97.39 ¬±1.25
98.98 ¬±0.94
98.75 ¬±1.29
98.86 ¬±1.14
3
74.43 ¬±3.60
90.80 ¬±2.61
98.64 ¬±1.11
99.09 ¬±1.11
98.75 ¬±1.56
99.09 ¬±0.85
4
79.55 ¬±4.34
93.41 ¬±2.82
99.20 ¬±0.73
99.20 ¬±0.89
99.66 ¬±0.73
99.20 ¬±1.69
5
82.73 ¬±4.06
93.41 ¬±1.89
99.43 ¬±1.05
99.20 ¬±1.35
99.32 ¬±1.16
99.32 ¬±0.56
6
83.18 ¬±3.51
94.09 ¬±2.02
99.43 ¬±0.76
99.09 ¬±0.85
99.20 ¬±1.69
99.20 ¬±0.89
7
84.43 ¬±3.74
94.43 ¬±2.24
99.89 ¬±0.34
99.20 ¬±1.02
99.20 ¬±0.89
99.66 ¬±0.73
8
84.20 ¬±3.82
94.20 ¬±2.00
98.98 ¬±1.19
99.32 ¬±0.75
99.66 ¬±0.52
99.20 ¬±0.73
9
84.43 ¬±3.87
94.20 ¬±2.06
99.77 ¬±0.45
99.20 ¬±1.02
98.98 ¬±1.07
99.32 ¬±0.75
10
84.77 ¬±3.98
94.43 ¬±2.18
99.32 ¬±0.75
98.86 ¬±1.14
99.09 ¬±1.67
99.66 ¬±0.52"
REFERENCES,0.8375350140056023,"layer
height
1
2
3
4
5
6"
REFERENCES,0.8389355742296919,"1
79.11 ¬±3.07
91.13 ¬±2.01
96.85 ¬±1.11
97.18 ¬±1.31
97.10 ¬±1.45
97.42 ¬±1.34
2
89.68 ¬±1.76
93.55 ¬±1.53
98.31 ¬±0.76
97.82 ¬±1.14
97.42 ¬±1.24
97.98 ¬±0.74
3
90.81 ¬±1.36
96.13 ¬±1.48
97.66 ¬±1.22
98.23 ¬±0.94
98.87 ¬±0.82
98.39 ¬±0.62
4
91.53 ¬±1.04
96.69 ¬±1.05
98.06 ¬±1.03
98.55 ¬±1.01
98.63 ¬±1.08
97.98 ¬±1.15
5
93.95 ¬±1.58
96.13 ¬±1.76
98.47 ¬±1.17
98.47 ¬±0.92
98.31 ¬±0.84
97.90 ¬±0.65
6
94.35 ¬±1.25
96.69 ¬±1.46
98.06 ¬±1.03
98.31 ¬±1.05
98.15 ¬±1.20
98.39 ¬±1.20
7
94.76 ¬±1.10
97.02 ¬±1.44
98.47 ¬±0.84
98.47 ¬±1.05
98.71 ¬±0.74
98.87 ¬±0.90
8
95.08 ¬±0.76
97.02 ¬±1.20
98.55 ¬±1.24
98.87 ¬±0.82
98.47 ¬±0.92
98.71 ¬±1.15
9
94.68 ¬±1.09
96.94 ¬±1.19
98.47 ¬±0.43
98.15 ¬±1.20
98.15 ¬±0.89
98.39 ¬±1.08
10
94.84 ¬±1.21
96.77 ¬±1.20
98.47 ¬±0.92
97.98 ¬±1.50
98.15 ¬±1.02
98.23 ¬±1.19"
REFERENCES,0.8403361344537815,Table 15: Node Classification accuracy for MeGraph model on BAShape (above) and BACommunity (below).
REFERENCES,0.8417366946778712,"layer
height
1
2
3
4
5
6"
REFERENCES,0.8431372549019608,"1
98.71 ¬±1.00
99.14 ¬±1.14
99.86 ¬±0.43
99.43 ¬±0.70
99.43 ¬±0.95
99.57 ¬±0.91
2
98.71 ¬±1.00
99.29 ¬±0.96
99.57 ¬±0.91
99.71 ¬±0.57
99.57 ¬±0.91
99.57 ¬±0.91
3
99.00 ¬±0.91
99.43 ¬±0.95
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
4
99.00 ¬±0.91
99.71 ¬±0.57
99.86 ¬±0.43
99.86 ¬±0.43
99.43 ¬±0.95
99.71 ¬±0.57
5
99.00 ¬±0.91
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.43 ¬±0.95
6
99.00 ¬±0.91
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
7
99.00 ¬±0.91
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
8
99.00 ¬±0.91
99.86 ¬±0.43
99.86 ¬±0.43
99.86 ¬±0.43
99.71 ¬±0.57
99.57 ¬±0.65
9
99.00 ¬±0.91
99.86 ¬±0.43
99.57 ¬±0.91
99.57 ¬±0.91
99.57 ¬±0.91
99.57 ¬±0.91
10
99.00 ¬±0.91
99.71 ¬±0.57
99.57 ¬±0.91
99.71 ¬±0.57
99.86 ¬±0.43
99.43 ¬±0.95"
REFERENCES,0.8445378151260504,"layer
height
1
2
3
4
5
6"
REFERENCES,0.84593837535014,"1
94.93 ¬±1.30
97.00 ¬±1.80
96.93 ¬±1.60
97.00 ¬±1.88
97.21 ¬±1.70
96.86 ¬±1.67
2
97.93 ¬±0.87
98.36 ¬±0.72
98.79 ¬±0.46
98.79 ¬±0.46
98.57 ¬±0.55
98.50 ¬±1.03
3
98.07 ¬±0.91
98.64 ¬±0.87
98.86 ¬±0.91
98.86 ¬±0.80
98.50 ¬±0.98
98.93 ¬±0.80
4
98.21 ¬±0.97
98.86 ¬±0.65
98.86 ¬±0.80
98.79 ¬±0.64
99.00 ¬±0.73
99.07 ¬±0.64
5
98.50 ¬±0.87
98.86 ¬±0.91
99.07 ¬±0.64
99.21 ¬±0.67
99.14 ¬±0.70
99.00 ¬±0.73
6
98.71 ¬±0.83
98.64 ¬±0.87
99.07 ¬±0.64
99.14 ¬±0.70
99.07 ¬±0.85
99.14 ¬±0.70
7
98.29 ¬±0.91
98.86 ¬±0.65
99.07 ¬±0.56
98.79 ¬±0.56
98.79 ¬±0.72
98.86 ¬±0.57
8
98.43 ¬±0.77
99.00 ¬±0.47
99.14 ¬±0.53
98.93 ¬±0.58
99.14 ¬±0.29
99.14 ¬±0.43
9
98.79 ¬±0.79
99.07 ¬±0.56
99.21 ¬±0.50
99.00 ¬±0.73
99.29 ¬±0.45
99.36 ¬±0.50
10
98.86 ¬±0.73
98.93 ¬±0.80
99.21 ¬±0.87
99.14 ¬±0.70
99.00 ¬±0.73
99.29 ¬±0.64"
REFERENCES,0.8473389355742297,Table 16: Comparison results of MeGraph with ATT and GATE on Graph Theory Benchmark.
REFERENCES,0.8487394957983193,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.8501400560224089,"MeGraph w. ATT
h = 1
2.990 ¬±3.411
3.346 ¬±3.228
44.41 ¬±36.33
16.39 ¬±13.57
29.04 ¬±27.59
h = 5
0.594 ¬±0.903
1.706 ¬±1.409
3.256 ¬±2.956
1.018 ¬±1.071
14.80 ¬±17.09
h = 5, Œ∑v = 0.3, œÑc = 4
0.749 ¬±1.131
1.128 ¬±0.794
4.430 ¬±4.329
0.640 ¬±0.833
5.649 ¬±4.496"
REFERENCES,0.8515406162464986,"MeGraph w. GATE
h = 1
4.144 ¬±4.181
0.908 ¬±0.934
6.343 ¬±7.152
13.94 ¬±12.78
19.73 ¬±19.47
h = 5
0.809 ¬±0.993
0.660 ¬±0.601
2.506 ¬±2.639
0.669 ¬±0.546
7.508 ¬±7.558
h = 5, Œ∑v = 0.3, œÑc = 4
0.602 ¬±0.622
0.599 ¬±0.520
0.544 ¬±0.490
0.342 ¬±0.193
0.859 ¬±0.712"
REFERENCES,0.8529411764705882,Table 17: Comparison results of MeGraph with ATT and GATE on GNN Benchmark.
REFERENCES,0.8543417366946778,"ZINC
AQSOL
CIFAR10
MNIST
PATTERN
CLUSTER"
REFERENCES,0.8557422969187675,"MeGraph w. ATT (h = 1)
0.4258 ¬±0.0054
1.1421 ¬±0.0270
69.890 ¬±0.209
97.570 ¬±0.168
78.232 ¬±0.827
59.497 ¬±0.207
MeGraph w. ATT (h = 5)
0.3637 ¬±0.0116
1.0767 ¬±0.0105
69.925 ¬±0.631
97.860 ¬±0.098
83.798 ¬±0.885
68.930 ¬±68.76
MeGraph w. GATE (h = 1)
0.3336 ¬±0.0036
1.0766 ¬±1.0556
64.200 ¬±0.586
96.812 ¬±0.205
85.391 ¬±0.029
59.321 ¬±0.290
MeGraph w. GATE (h = 5)
0.2897 ¬±0.0291
1.0240 ¬±0.0098
64.935 ¬±0.829
97.290 ¬±0.140
86.611 ¬±0.041
67.122 ¬±3.323"
REFERENCES,0.8571428571428571,Table 18: Comparison results of MeGraph with ATT and GATE on OGB-G.
REFERENCES,0.8585434173669467,"Model
molhiv ‚Üë
molbace ‚Üë
molbbbp ‚Üë
molclintox ‚Üë
molsider ‚Üë"
REFERENCES,0.8599439775910365,"MeGraph w. ATT (h = 1)
77.33 ¬±0.78
74.83 ¬±4.87
64.74 ¬±1.14
86.34 ¬±1.04
58.12 ¬±0.53
MeGraph w. ATT (h = 5)
77.15 ¬±1.37
76.13 ¬±3.85
68.68 ¬±2.07
87.17 ¬±0.76
58.03 ¬±1.58
MeGraph w. GATE (h = 1)
76.35 ¬±0.70
76.36 ¬±1.55
65.97 ¬±1.98
87.12 ¬±0.74
58.11 ¬±1.29
MeGraph w. GATE (h = 5)
78.14 ¬±0.91
78.90 ¬±1.29
66.53 ¬±0.74
89.02 ¬±2.56
59.58 ¬±1.88"
REFERENCES,0.8613445378151261,"Model
moltox21 ‚Üë
moltoxcast ‚Üë
molesol ‚Üì
molfreesolv ‚Üì
mollipo ‚Üì"
REFERENCES,0.8627450980392157,"MeGraph w. ATT (h = 1)
75.88 ¬±0.64
64.65 ¬±0.59
1.091 ¬±0.030
2.318 ¬±0.089
0.790 ¬±0.012
MeGraph w. ATT (h = 5)
76.71 ¬±0.98
66.98 ¬±0.70
1.007 ¬±0.617
2.065 ¬±0.151
0.736 ¬±0.023
MeGraph w. GATE (h = 1)
75.30 ¬±0.43
64.34 ¬±0.62
1.064 ¬±0.015
2.191 ¬±0.068
0.766 ¬±0.008
MeGraph w. GATE (h = 5)
76.91 ¬±0.25
66.78 ¬±0.13
1.003 ¬±0.086
2.048 ¬±0.199
0.700 ¬±0.014"
REFERENCES,0.8641456582633054,"Table 19: Graph Theory Benchmark results on Grid graphs, all results are obtained using our codebase."
REFERENCES,0.865546218487395,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.8669467787114846,"Baselines
(h=1)"
REFERENCES,0.8683473389355743,"n=1
6.60 ¬±0.541
1.50 ¬±0.050
22.49 ¬±1.36
26.74 ¬±0.347
20.99 ¬±0.232
n=5
4.18 ¬±0.737
1.29 ¬±0.124
5.04 ¬±1.26
15.54 ¬±0.155
20.32 ¬±0.326
n=10
3.70 ¬±0.422
1.33 ¬±0.100
0.737 ¬±0.116
7.24 ¬±0.243
20.32 ¬±0.422"
REFERENCES,0.8697478991596639,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
1.19 ¬±0.486
1.24 ¬±0.154
6.78 ¬±1.95
5.34 ¬±0.265
18.00 ¬±0.910
n=5
0.738 ¬±0.322
1.11 ¬±0.043
0.616 ¬±0.310
0.617 ¬±0.099
13.3 ¬±3.31"
REFERENCES,0.8711484593837535,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.8725490196078431,"œÑc=3
0.361 ¬±0.182
1.24 ¬±0.113
0.382 ¬±0.120
0.442 ¬±0.130
0.918 ¬±0.220
Œ∑v=0.3
4.77 ¬±2.50
1.33 ¬±0.161
0.349 ¬±0.074
5.40 ¬±0.954
3.59 ¬±0.354
Œ∑v=0.3, œÑc=4
0.745 ¬±0.316
1.35 ¬±0.168
0.385 ¬±0.180
0.552 ¬±0.113
0.622 ¬±0.100
Œ∑v=0.5, œÑc=4
1.61 ¬±0.394
1.28 ¬±0.138
0.458 ¬±0.220
1.71 ¬±0.535
1.48 ¬±0.283
Œ∑v=0.3, œÑc=4 (X-Pool)
1.03 ¬±0.365
1.50 ¬±0.142
0.626 ¬±0.216
1.70 ¬±0.185
3.44 ¬±0.991
Œ∑v=0.3, œÑc=4 (w/o pw)
0.616 ¬±0.194
1.66 ¬±0.083
0.361 ¬±0.147
0.678 ¬±0.139
1.70 ¬±0.485"
REFERENCES,0.8739495798319328,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.969 ¬±0.643
2.18 ¬±0.381
0.111 ¬±0.091
0.773 ¬±0.086
0.548 ¬±0.131"
REFERENCES,0.8753501400560224,Table 20: Graph Theory Benchmark results on Tree graphs. All results are obtained using our codebase.
REFERENCES,0.876750700280112,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.8781512605042017,"Baselines
(h=1)"
REFERENCES,0.8795518207282913,"n=1
5.21 ¬±0.209
1.28 ¬±0.050
3.77 ¬±1.22
17.16 ¬±0.168
24.63 ¬±0.427
n=5
3.34 ¬±0.375
0.405 ¬±0.089
0.504 ¬±0.109
7.66 ¬±0.325
18.11 ¬±1.85
n=10
3.16 ¬±0.252
0.338 ¬±0.046
0.100 ¬±0.059
2.28 ¬±0.209
14.93 ¬±0.800"
REFERENCES,0.8809523809523809,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
1.62 ¬±0.314
0.846 ¬±0.071
0.725 ¬±0.249
6.99 ¬±0.610
12.27 ¬±0.843
n=5
0.83 ¬±0.667
0.490 ¬±0.118
0.084 ¬±0.030
1.27 ¬±0.442
2.87 ¬±0.420"
REFERENCES,0.8823529411764706,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.8837535014005602,"œÑc=3
0.599 ¬±0.200
0.483 ¬±0.081
0.075 ¬±0.012
0.497 ¬±0.121
0.429 ¬±0.105
Œ∑v=0.3
0.868 ¬±0.230
0.413 ¬±0.054
0.142 ¬±0.047
0.789 ¬±0.092
0.534 ¬±0.074
Œ∑v=0.3, œÑc=4
0.615 ¬±0.209
0.418 ¬±0.024
0.081 ¬±0.017
0.440 ¬±0.106
0.436 ¬±0.097
Œ∑v=0.5, œÑc=4
1.06 ¬±0.327
0.424 ¬±0.042
0.214 ¬±0.018
1.20 ¬±0.128
2.03 ¬±0.507
Œ∑v=0.3, œÑc=4 (X-Pool)
0.666 ¬±0.118
0.596 ¬±0.067
0.182 ¬±0.057
1.22 ¬±0.281
1.11 ¬±0.122
Œ∑v=0.3, œÑc=4 (w/o pw)
0.771 ¬±0.141
0.455 ¬±0.056
0.124 ¬±0.032
0.700 ¬±0.190
1.07 ¬±0.260"
REFERENCES,0.8851540616246498,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.873 ¬±0.247
0.667 ¬±0.043
0.804 ¬±0.284
0.606 ¬±0.123
1.00 ¬±0.221"
REFERENCES,0.8865546218487395,"Table 21: Graph Theory Benchmark results on Ladder graphs, all results are obtained using our codebase."
REFERENCES,0.8879551820728291,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.8893557422969187,"Baselines
(h=1)"
REFERENCES,0.8907563025210085,"n=1
5.06 ¬±0.330
1.73 ¬±0.249
1.17 ¬±0.149
13.20 ¬±0.126
20.10 ¬±0.583
n=5
0.692 ¬±0.204
0.734 ¬±0.106
1.39 ¬±0.078
5.02 ¬±0.876
19.81 ¬±0.669
n=10
0.257 ¬±0.078
0.691 ¬±0.119
1.55 ¬±0.069
1.60 ¬±0.194
20.40 ¬±0.995"
REFERENCES,0.8921568627450981,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
0.662 ¬±0.165
0.866 ¬±0.071
1.57 ¬±0.992
2.18 ¬±0.181
6.61 ¬±1.32
n=5
0.251 ¬±0.108
0.753 ¬±0.091
0.175 ¬±0.169
0.321 ¬±0.058
1.18 ¬±0.746"
REFERENCES,0.8935574229691877,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.8949579831932774,"œÑc=3
0.296 ¬±0.070
0.754 ¬±0.086
0.226 ¬±0.069
0.228 ¬±0.021
0.285 ¬±0.069
Œ∑v=0.3
0.507 ¬±0.204
0.768 ¬±0.050
0.156 ¬±0.053
0.969 ¬±0.148
0.787 ¬±0.059
Œ∑v=0.3, œÑc=4
0.297 ¬±0.113
0.712 ¬±0.059
0.095 ¬±0.046
0.180 ¬±0.026
0.225 ¬±0.043
Œ∑v=0.5, œÑc=4
0.375 ¬±0.196
0.656 ¬±0.064
0.058 ¬±0.019
0.612 ¬±0.191
0.464 ¬±0.121
Œ∑v=0.3, œÑc=4 (X-Pool)
0.442 ¬±0.108
0.742 ¬±0.047
0.158 ¬±0.074
0.710 ¬±0.076
0.765 ¬±0.089
Œ∑v=0.3, œÑc=4 (w/o pw)
0.245 ¬±0.021
0.682 ¬±0.105
0.106 ¬±0.052
0.271 ¬±0.039
0.618 ¬±0.188"
REFERENCES,0.896358543417367,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.339 ¬±0.11
0.797 ¬±0.138
0.013 ¬±0.005
0.287 ¬±0.023
0.230 ¬±0.054"
REFERENCES,0.8977591036414566,"Table 22: Graph Theory Benchmark results on Line graphs, all results are obtained using our codebase."
REFERENCES,0.8991596638655462,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9005602240896359,"Baselines
(h=1)"
REFERENCES,0.9019607843137255,"n=1
30.37 ¬±1.41
0.458 ¬±0.035
21.49 ¬±8.84
68.99 ¬±0.247
75.46 ¬±1.86
n=5
10.55 ¬±2.40
0.019 ¬±0.004
9.97 ¬±10.85
46.39 ¬±3.09
78.49 ¬±4.38
n=10
3.29 ¬±0.813
0.012 ¬±0.003
10.18 ¬±10.59
35.07 ¬±2.71
77.23 ¬±3.42"
REFERENCES,0.9033613445378151,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
1.45 ¬±0.598
0.056 ¬±0.014
7.62 ¬±4.43
10.13 ¬±2.33
45.19 ¬±8.64
n=5
0.536 ¬±0.149
0.016 ¬±0.007
0.611 ¬±0.238
1.06 ¬±0.341
14.12 ¬±13.82"
REFERENCES,0.9047619047619048,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9061624649859944,"œÑc=3
0.349 ¬±0.206
0.013 ¬±0.003
0.724 ¬±0.479
0.339 ¬±0.102
1.15 ¬±0.267
Œ∑v=0.3
3.65 ¬±2.13
0.017 ¬±0.005
1.75 ¬±1.63
13.99 ¬±2.09
7.45 ¬±0.989
Œ∑v=0.3, œÑc=4
0.283 ¬±0.072
0.019 ¬±0.006
0.584 ¬±0.337
0.515 ¬±0.044
1.27 ¬±1.08
Œ∑v=0.5, œÑc=4
1.81 ¬±0.121
0.022 ¬±0.006
0.711 ¬±0.213
2.64 ¬±0.047
3.77 ¬±0.763
Œ∑v=0.3, œÑc=4 (X-Pool)
1.06 ¬±0.510
0.101 ¬±0.016
0.767 ¬±0.522
2.29 ¬±0.472
3.89 ¬±1.02
Œ∑v=0.3, œÑc=4 (w/o pw)
0.377 ¬±0.106
0.022 ¬±0.007
1.19 ¬±1.17
1.12 ¬±0.115
3.34 ¬±0.904"
REFERENCES,0.907563025210084,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.426 ¬±0.223
0.062 ¬±0.008
2.89 ¬±1.89
0.767 ¬±0.129
4.78 ¬±1.94"
REFERENCES,0.9089635854341737,"Table 23: Graph Theory Benchmark results on Caterpillar graphs, all results are obtained using our codebase."
REFERENCES,0.9103641456582633,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9117647058823529,"Baselines
(h=1)"
REFERENCES,0.9131652661064426,"n=1
24.24 ¬±1.57
1.25 ¬±0.082
28.62 ¬±2.55
19.08 ¬±0.208
35.32 ¬±0.462
n=5
8.32 ¬±2.10
0.561 ¬±0.070
4.59 ¬±0.346
9.62 ¬±0.357
37.01 ¬±1.48
n=10
6.40 ¬±0.652
0.630 ¬±0.127
5.06 ¬±0.499
4.06 ¬±0.297
37.87 ¬±3.22"
REFERENCES,0.9145658263305322,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
5.04 ¬±1.03
0.685 ¬±0.077
6.08 ¬±1.40
5.40 ¬±0.843
28.52 ¬±2.16
n=5
3.44 ¬±1.13
0.533 ¬±0.064
2.00 ¬±1.28
0.921 ¬±0.149
5.20 ¬±1.57"
REFERENCES,0.9159663865546218,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9173669467787114,"œÑc=3
2.47 ¬±0.529
0.607 ¬±0.081
0.591 ¬±0.172
0.574 ¬±0.073
1.21 ¬±0.148
Œ∑v=0.3
3.61 ¬±1.36
0.582 ¬±0.052
0.578 ¬±0.231
1.69 ¬±0.572
1.95 ¬±0.322
Œ∑v=0.3, œÑc=4
1.59 ¬±0.444
0.535 ¬±0.091
0.317 ¬±0.104
0.474 ¬±0.170
1.32 ¬±0.272
Œ∑v=0.5, œÑc=4
2.00 ¬±0.648
0.514 ¬±0.040
1.10 ¬±0.288
0.986 ¬±0.130
2.11 ¬±0.766
Œ∑v=0.3, œÑc=4 (X-Pool)
1.39 ¬±0.478
0.602 ¬±0.110
0.736 ¬±0.230
1.78 ¬±0.254
3.36 ¬±0.873
Œ∑v=0.3, œÑc=4 (w/o pw)
1.82 ¬±0.627
0.628 ¬±0.093
0.604 ¬±0.067
0.797 ¬±0.299
2.25 ¬±0.230"
REFERENCES,0.9187675070028011,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
1.57 ¬±0.670
0.679 ¬±0.098
3.18 ¬±0.583
0.976 ¬±0.270
3.83 ¬±1.06"
REFERENCES,0.9201680672268907,"Table 24: Graph Theory Benchmark results on Lobster graphs, all results are obtained using our codebase."
REFERENCES,0.9215686274509803,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9229691876750701,"Baselines
(h=1)"
REFERENCES,0.9243697478991597,"n=1
23.92 ¬±0.319
1.06 ¬±0.166
11.93 ¬±1.32
38.44 ¬±0.065
40.46 ¬±0.350
n=5
10.89 ¬±1.47
0.544 ¬±0.067
3.66 ¬±0.424
20.12 ¬±0.105
28.81 ¬±1.14
n=10
7.35 ¬±2.50
0.631 ¬±0.067
2.59 ¬±0.517
10.52 ¬±0.619
28.47 ¬±1.65"
REFERENCES,0.9257703081232493,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
6.00 ¬±1.82
0.785 ¬±0.062
4.35 ¬±1.51
13.75 ¬±0.675
30.49 ¬±2.18
n=5
1.93 ¬±0.861
0.543 ¬±0.073
1.07 ¬±0.114
2.05 ¬±0.393
11.39 ¬±5.43"
REFERENCES,0.927170868347339,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9285714285714286,"œÑc=3
2.02 ¬±0.791
0.447 ¬±0.123
0.705 ¬±0.133
1.66 ¬±0.270
2.23 ¬±0.378
Œ∑v=0.3
6.01 ¬±1.52
0.521 ¬±0.028
0.707 ¬±0.202
3.04 ¬±0.250
2.70 ¬±0.212
Œ∑v=0.3, œÑc=4
1.90 ¬±0.449
0.489 ¬±0.069
0.671 ¬±0.165
1.30 ¬±0.106
2.62 ¬±0.849
Œ∑v=0.5, œÑc=4
3.27 ¬±0.716
0.451 ¬±0.090
0.941 ¬±0.324
2.82 ¬±0.803
4.04 ¬±0.527
Œ∑v=0.3, œÑc=4 (X-Pool)
2.67 ¬±0.486
0.494 ¬±0.109
1.01 ¬±0.194
2.79 ¬±0.343
4.16 ¬±0.886
Œ∑v=0.3, œÑc=4 (w/o pw)
1.85 ¬±0.432
0.473 ¬±0.069
0.892 ¬±0.277
1.77 ¬±0.329
4.33 ¬±1.71"
REFERENCES,0.9299719887955182,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
4.85 ¬±1.48
0.782 ¬±0.026
3.74 ¬±0.361
2.96 ¬±0.443
4.25 ¬±0.544"
REFERENCES,0.9313725490196079,"Table 25: Graph Theory Benchmark results on Cycle graphs, all results are obtained using our codebase."
REFERENCES,0.9327731092436975,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9341736694677871,"Baselines
(h=1)"
REFERENCES,0.9355742296918768,"n=1
18.75 ¬±0.066
0.534 ¬±0.022
22.35 ¬±0.149
24.07 ¬±0.009
21.47 ¬±0.060
n=5
3.39 ¬±0.304
0.027 ¬±0.001
25.11 ¬±0.325
12.44 ¬±1.05
21.81 ¬±0.102
n=10
0.352 ¬±0.060
0.011 ¬±0.003
26.54 ¬±1.16
8.65 ¬±1.02
24.09 ¬±0.360"
REFERENCES,0.9369747899159664,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
0.594 ¬±0.212
0.074 ¬±0.029
9.11 ¬±1.88
4.07 ¬±0.364
21.53 ¬±0.070
n=5
0.060 ¬±0.032
0.014 ¬±0.003
13.44 ¬±6.40
0.103 ¬±0.016
24.05 ¬±0.204"
REFERENCES,0.938375350140056,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9397759103641457,"œÑc=3
0.066 ¬±0.036
0.015 ¬±0.006
0.241 ¬±0.049
0.090 ¬±0.037
0.342 ¬±0.186
Œ∑v=0.3
2.45 ¬±0.873
0.015 ¬±0.001
0.709 ¬±0.226
8.36 ¬±0.261
0.488 ¬±0.267
Œ∑v=0.3, œÑc=4
0.060 ¬±0.030
0.019 ¬±0.003
0.312 ¬±0.236
0.226 ¬±0.050
0.562 ¬±0.209
Œ∑v=0.5, œÑc=4
0.451 ¬±0.203
0.014 ¬±0.004
0.252 ¬±0.124
1.05 ¬±0.524
4.30 ¬±1.90
Œ∑v=0.3, œÑc=4 (X-Pool)
0.494 ¬±0.292
0.096 ¬±0.028
0.468 ¬±0.220
1.08 ¬±0.130
0.860 ¬±0.292
Œ∑v=0.3, œÑc=4 (w/o pw)
0.159 ¬±0.209
0.017 ¬±0.008
1.23 ¬±0.928
0.461 ¬±0.118
8.26 ¬±3.70"
REFERENCES,0.9411764705882353,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.144 ¬±0.073
0.035 ¬±0.010
3.21 ¬±0.893
0.439 ¬±0.089
5.91 ¬±1.31"
REFERENCES,0.9425770308123249,"Table 26: Graph Theory Benchmark results on Pseudotree graphs, all results are obtained using our codebase."
REFERENCES,0.9439775910364145,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9453781512605042,"Baselines
(h=1)"
REFERENCES,0.9467787114845938,"n=1
1.93 ¬±0.239
1.71 ¬±0.281
2.78 ¬±0.098
6.27 ¬±0.004
4.23 ¬±0.034
n=5
0.061 ¬±0.024
0.942 ¬±0.094
1.74 ¬±0.299
1.54 ¬±0.006
4.15 ¬±0.086
n=10
0.037 ¬±0.022
0.775 ¬±0.094
1.84 ¬±0.260
0.126 ¬±0.038
4.06 ¬±0.037"
REFERENCES,0.9481792717086834,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
0.404 ¬±0.096
1.75 ¬±0.133
1.50 ¬±0.494
2.25 ¬±0.280
3.97 ¬±0.270
n=5
0.141 ¬±0.022
0.999 ¬±0.054
1.16 ¬±0.069
0.148 ¬±0.034
3.12 ¬±0.202"
REFERENCES,0.9495798319327731,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9509803921568627,"œÑc=3
0.130 ¬±0.069
0.912 ¬±0.073
0.669 ¬±0.080
0.115 ¬±0.015
0.797 ¬±0.079
Œ∑v=0.3
0.048 ¬±0.030
0.839 ¬±0.077
0.758 ¬±0.134
0.246 ¬±0.021
0.838 ¬±0.023
Œ∑v=0.3, œÑc=4
0.106 ¬±0.054
0.814 ¬±0.092
0.663 ¬±0.076
0.133 ¬±0.028
0.845 ¬±0.101
Œ∑v=0.5, œÑc=4
0.071 ¬±0.048
1.03 ¬±0.186
0.583 ¬±0.065
0.171 ¬±0.038
0.868 ¬±0.034
Œ∑v=0.3, œÑc=4 (X-Pool)
0.564 ¬±0.155
0.966 ¬±0.172
0.977 ¬±0.054
0.611 ¬±0.065
1.10 ¬±0.036
Œ∑v=0.3, œÑc=4 (w/o pw)
0.080 ¬±0.033
0.971 ¬±0.072
0.956 ¬±0.230
0.276 ¬±0.017
1.13 ¬±0.321"
REFERENCES,0.9523809523809523,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.467 ¬±0.065
1.09 ¬±0.072
1.71 ¬±0.295
0.721 ¬±0.092
2.25 ¬±0.327"
REFERENCES,0.9537815126050421,"Table 27: Graph Theory Benchmark results on Geo graphs, all results are obtained using our codebase."
REFERENCES,0.9551820728291317,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9565826330532213,"Baselines
(h=1)"
REFERENCES,0.957983193277311,"n=1
5.79 ¬±0.630
0.424 ¬±0.023
11.85 ¬±0.391
12.49 ¬±0.035
14.82 ¬±0.056
n=5
1.02 ¬±0.772
0.407 ¬±0.040
8.37 ¬±0.468
5.10 ¬±0.435
14.33 ¬±0.079
n=10
0.304 ¬±0.125
0.404 ¬±0.061
9.41 ¬±0.759
0.803 ¬±0.162
14.33 ¬±0.136"
REFERENCES,0.9593837535014006,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
1.60 ¬±0.880
0.347 ¬±0.033
10.17 ¬±2.04
4.87 ¬±0.777
11.91 ¬±0.451
n=5
0.232 ¬±0.061
0.273 ¬±0.018
2.70 ¬±0.288
0.575 ¬±0.127
6.92 ¬±2.36"
REFERENCES,0.9607843137254902,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9621848739495799,"œÑc=3
0.188 ¬±0.100
0.288 ¬±0.020
2.04 ¬±0.225
0.562 ¬±0.186
2.42 ¬±0.333
Œ∑v=0.3
1.38 ¬±0.617
0.330 ¬±0.025
4.40 ¬±1.15
1.37 ¬±0.083
5.45 ¬±0.465
Œ∑v=0.3, œÑc=4
0.230 ¬±0.070
0.231 ¬±0.034
1.99 ¬±0.549
0.454 ¬±0.057
2.69 ¬±0.369
Œ∑v=0.5, œÑc=4
0.374 ¬±0.148
0.368 ¬±0.043
3.95 ¬±0.319
0.777 ¬±0.122
4.61 ¬±0.717
Œ∑v=0.3, œÑc=4 (X-Pool)
1.04 ¬±0.502
0.362 ¬±0.031
2.32 ¬±0.440
2.37 ¬±0.260
5.08 ¬±0.737
Œ∑v=0.3, œÑc=4 (w/o pw)
0.233 ¬±0.046
0.261 ¬±0.035
2.58 ¬±0.617
1.09 ¬±0.226
4.85 ¬±0.805"
REFERENCES,0.9635854341736695,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
1.49 ¬±0.451
0.400 ¬±0.020
4.63 ¬±0.647
2.42 ¬±0.458
7.36 ¬±1.62"
REFERENCES,0.9649859943977591,"Table 28: Graph Theory Benchmark results on SBM graphs, all results are obtained using our codebase."
REFERENCES,0.9663865546218487,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9677871148459384,"Baselines
(h=1)"
REFERENCES,0.969187675070028,"n=1
1.14 ¬±0.084
3.28 ¬±0.135
3.05 ¬±0.171
1.43 ¬±0.020
3.47 ¬±0.026
n=5
0.420 ¬±0.018
3.14 ¬±0.131
2.77 ¬±0.383
0.100 ¬±0.050
3.18 ¬±0.093
n=10
0.704 ¬±0.264
3.29 ¬±0.063
2.70 ¬±0.235
0.012 ¬±0.005
2.97 ¬±0.051"
REFERENCES,0.9705882352941176,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
0.786 ¬±0.145
2.79 ¬±0.226
2.49 ¬±0.692
0.547 ¬±0.059
3.35 ¬±0.195
n=5
0.525 ¬±0.136
2.88 ¬±0.300
2.37 ¬±0.336
0.058 ¬±0.031
3.01 ¬±1.07"
REFERENCES,0.9719887955182073,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9733893557422969,"œÑc=3
0.783 ¬±0.149
2.80 ¬±0.172
2.16 ¬±0.449
0.076 ¬±0.044
1.98 ¬±0.497
Œ∑v=0.3
1.16 ¬±0.131
3.51 ¬±0.312
2.03 ¬±0.429
0.058 ¬±0.027
1.86 ¬±0.098
Œ∑v=0.3, œÑc=4
0.926 ¬±0.087
2.62 ¬±0.179
1.99 ¬±0.290
0.062 ¬±0.027
1.57 ¬±0.205
Œ∑v=0.5, œÑc=4
0.798 ¬±0.255
3.14 ¬±0.234
2.05 ¬±0.375
0.063 ¬±0.013
1.70 ¬±0.049
Œ∑v=0.3, œÑc=4 (X-Pool)
0.935 ¬±0.106
2.60 ¬±0.374
2.16 ¬±0.204
0.056 ¬±0.014
1.87 ¬±0.111
Œ∑v=0.3, œÑc=4 (w/o pw)
0.788 ¬±0.063
2.52 ¬±0.100
1.38 ¬±0.131
0.484 ¬±0.063
2.21 ¬±0.169"
REFERENCES,0.9747899159663865,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
1.51 ¬±0.138
3.57 ¬±0.569
1.96 ¬±0.240
1.71 ¬±0.115
2.77 ¬±0.160"
REFERENCES,0.9761904761904762,"Table 29: Graph Theory Benchmark results on BA graphs, all results are obtained using our codebase."
REFERENCES,0.9775910364145658,"Category
Model
SPsssd
MCC
Diameter
SPss
ECC"
REFERENCES,0.9789915966386554,"Baselines
(h=1)"
REFERENCES,0.9803921568627451,"n=1
0.004 ¬±0.001
2.81 ¬±0.142
0.092 ¬±0.021
‚àí
0.128 ¬±0.006
n=5
0.007 ¬±0.002
3.65 ¬±0.660
0.098 ¬±0.014
‚àí
0.091 ¬±0.011
n=10
0.011 ¬±0.006
3.72 ¬±0.376
0.122 ¬±0.038
‚àí
0.080 ¬±0.004"
REFERENCES,0.9817927170868347,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
0.006 ¬±0.004
2.00 ¬±0.380
0.101 ¬±0.020
‚àí
0.084 ¬±0.017
n=5
0.003 ¬±0.001
2.00 ¬±0.240
0.104 ¬±0.011
‚àí
0.052 ¬±0.010"
REFERENCES,0.9831932773109243,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9845938375350141,"œÑc=3
0.007 ¬±0.003
1.77 ¬±0.403
0.089 ¬±0.008
‚àí
0.126 ¬±0.027
Œ∑v=0.3
0.013 ¬±0.004
1.67 ¬±0.333
0.084 ¬±0.008
‚àí
0.086 ¬±0.005
Œ∑v=0.3, œÑc=4
0.011 ¬±0.005
1.42 ¬±0.252
0.073 ¬±0.015
‚àí
0.163 ¬±0.007
Œ∑v=0.5, œÑc=4
0.008 ¬±0.004
1.71 ¬±0.403
0.074 ¬±0.009
‚àí
0.156 ¬±0.021
Œ∑v=0.3, œÑc=4 (X-Pool)
0.009 ¬±0.003
1.22 ¬±0.242
0.088 ¬±0.021
‚àí
0.076 ¬±0.006
Œ∑v=0.3, œÑc=4 (w/o pw)
0.009 ¬±0.003
1.42 ¬±0.209
0.068 ¬±0.017
‚àí
0.068 ¬±0.017"
REFERENCES,0.9859943977591037,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
0.024 ¬±0.009
2.84 ¬±0.777
0.091 ¬±0.01
‚àí
0.179 ¬±0.0227"
REFERENCES,0.9873949579831933,"Table 30: Graph Theory Benchmark results on mixed, ER, Caveman and Star graphs, all results are obtained
using our codebase."
REFERENCES,0.988795518207283,"Category
Model
MCC
ECC"
REFERENCES,0.9901960784313726,"mix
ER
Caveman
Star
mix
ER"
REFERENCES,0.9915966386554622,"Baselines
(h=1)"
REFERENCES,0.9929971988795518,"n=1
3.46 ¬±0.211
2.91 ¬±0.206
0.015 ¬±0.004
0.144 ¬±0.031
0.316 ¬±0.003
0.346 ¬±0.006
n=5
3.29 ¬±0.261
3.35 ¬±0.205
0.014 ¬±0.003
0.078 ¬±0.021
0.228 ¬±0.008
0.289 ¬±0.008
n=10
3.51 ¬±0.323
3.53 ¬±0.375
0.018 ¬±0.006
0.065 ¬±0.005
0.212 ¬±0.008
0.414 ¬±0.102"
REFERENCES,0.9943977591036415,"MeGraph(h=5)
EdgePool(œÑc=2)
n=1
1.25 ¬±0.167
0.749 ¬±0.058
0.018 ¬±0.005
0.135 ¬±0.055
0.150 ¬±0.011
0.320 ¬±0.071
n=5
1.11 ¬±0.143
0.723 ¬±0.073
0.017 ¬±0.005
0.052 ¬±0.017
0.125 ¬±0.010
0.345 ¬±0.064"
REFERENCES,0.9957983193277311,"MeGraph
S-EdgePool
Variants
(h=5, n=5)"
REFERENCES,0.9971988795518207,"œÑc=3
1.07 ¬±0.034
0.714 ¬±0.039
0.017 ¬±0.002
0.072 ¬±0.016
0.137 ¬±0.013
0.232 ¬±0.035
Œ∑v=0.3
0.908 ¬±0.153
0.627 ¬±0.090
0.026 ¬±0.007
0.125 ¬±0.026
0.128 ¬±0.014
0.248 ¬±0.012
Œ∑v=0.3, œÑc=4
1.10 ¬±0.085
0.709 ¬±0.092
0.019 ¬±0.004
0.073 ¬±0.012
0.129 ¬±0.009
0.224 ¬±0.053
Œ∑v=0.5, œÑc=4
1.12 ¬±0.219
0.722 ¬±0.128
0.026 ¬±0.008
0.058 ¬±0.010
0.147 ¬±0.017
0.219 ¬±0.042
Œ∑v=0.3, œÑc=4 (X-Pool)
1.01 ¬±0.166
0.838 ¬±0.078
0.029 ¬±0.007
0.107 ¬±0.021
0.119 ¬±0.008
0.213 ¬±0.027
Œ∑v=0.3, œÑc=4 (w/o pw)
1.13 ¬±0.059
0.622 ¬±0.073
0.019 ¬±0.003
0.075 ¬±0.015
0.126 ¬±0.016
0.307 ¬±0.062"
REFERENCES,0.9985994397759104,"Graph-UNets
h=5,n=9,Œ∑v=0.3,œÑc=4
1.06 ¬±0.171
0.859 ¬±0.092
0.041 ¬±0.007
0.057 ¬±0.010
0.153 ¬±0.012
0.34 5¬±0.133"
