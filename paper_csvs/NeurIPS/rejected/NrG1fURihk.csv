Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015015015015015015,"We introduce a formulation for learning dimensionality-reducing representations
1"
ABSTRACT,0.003003003003003003,"of unlabeled feature vectors, when a prior knowledge on future prediction tasks
2"
ABSTRACT,0.0045045045045045045,"is available. The formulation is based on a three-player game, in which the ﬁrst
3"
ABSTRACT,0.006006006006006006,"player chooses a representation, the second player then adversarially chooses a
4"
ABSTRACT,0.0075075075075075074,"prediction task, and the third player predicts the response based on the represented
5"
ABSTRACT,0.009009009009009009,"features. The ﬁrst and third player aim is to minimize, and the second player to
6"
ABSTRACT,0.010510510510510511,"maximize, the regret: The minimal prediction loss using the representation com-
7"
ABSTRACT,0.012012012012012012,"pared to the same loss using the original features. Our ﬁrst contribution is theoret-
8"
ABSTRACT,0.013513513513513514,"ical and addresses the mean squared error loss function, and the case in which the
9"
ABSTRACT,0.015015015015015015,"representation, the response to predict and the predictors are all linear functions.
10"
ABSTRACT,0.016516516516516516,"We establish the optimal representation in pure strategies, which shows the effec-
11"
ABSTRACT,0.018018018018018018,"tiveness of the prior knowledge, and the optimal regret in mixed strategies, which
12"
ABSTRACT,0.01951951951951952,"shows the usefulness of randomizing the representation. We prove that optimal
13"
ABSTRACT,0.021021021021021023,"randomization requires a precisely characterized ﬁnite number of representations,
14"
ABSTRACT,0.02252252252252252,"which is smaller than the dimension of the feature vector, and potentially much
15"
ABSTRACT,0.024024024024024024,"smaller. Our second contribution is an efﬁcient gradient-based iterative algorithm
16"
ABSTRACT,0.025525525525525526,"that approximates the optimal mixed representation for a general loss function,
17"
ABSTRACT,0.02702702702702703,"and general classes of representations, response functions and predictors.
18"
INTRODUCTION,0.028528528528528527,"1
Introduction
19"
INTRODUCTION,0.03003003003003003,"A common practice in modern data-science is to collect as much data as possible, even without
20"
INTRODUCTION,0.03153153153153153,"an exact knowledge of a subsequent prediction task it will be used for. The data collected is an
21"
INTRODUCTION,0.03303303303303303,"unlabeled set of feature vectors {xi} ⊂Rd . Then, when a speciﬁc prediction task becomes of
22"
INTRODUCTION,0.03453453453453453,"interest, responses yi ∈Y are collected, and a learning algorithm is trained on the pairs {(xi, yi)}.
23"
INTRODUCTION,0.036036036036036036,"Modern sources, such as high-deﬁnition images or genomic sequences, have high dimension d, and
24"
INTRODUCTION,0.03753753753753754,"this raises the question of dimensionality-reduction, either for a better generalization [1], for stor-
25"
INTRODUCTION,0.03903903903903904,"age/communication savings [2–4], or for interpretability [5]. The goal is thus to ﬁnd a representation
26"
INTRODUCTION,0.04054054054054054,"z = R(x) ∈Rr, where d ≫r, that preserves the relevant part of the features, without a full knowl-
27"
INTRODUCTION,0.042042042042042045,"edge of their utility for future prediction tasks. In this paper, we propose an unsupervised-learning
28"
INTRODUCTION,0.04354354354354354,"game-theoretic framework for this goal, whose central aspect is an assumption of prior knowledge
29"
INTRODUCTION,0.04504504504504504,"on the class of future prediction tasks. Our contributions are a theoretical solution in a fully linear
30"
INTRODUCTION,0.046546546546546545,"setting, under the mean squared error (MSE) loss, and an algorithm for the general setting.
31"
INTRODUCTION,0.04804804804804805,"Popular approaches to dimensionality reduction are oblivious to prior knowledge on the prediction
32"
INTRODUCTION,0.04954954954954955,"task. Most prominently, principal component analysis (PCA) [6–9], and non-linear extensions such
33"
INTRODUCTION,0.05105105105105105,"as kernel PCA [10] and auto-encoders (AE) [11–13, 1], aim that the representation z will maximally
34"
INTRODUCTION,0.052552552552552555,"preserve the variation in x. Nonetheless, prior knowledge may indicate that the highly varying di-
35"
INTRODUCTION,0.05405405405405406,"rections in the feature space are irrelevant for future prediction tasks. From the supervised learn-
36"
INTRODUCTION,0.05555555555555555,"ing perspective, it is well established that efﬁcient representations are inherent to efﬁcient learning
37"
INTRODUCTION,0.057057057057057055,"[14, 15]. In this respect, the information bottleneck (IB) principle [16–19] was used to postulate
38"
INTRODUCTION,0.05855855855855856,"that efﬁcient supervised learning learns representations which are both low-complexity and relevant
39"
INTRODUCTION,0.06006006006006006,"[20–24] (this spurred a debate, e.g., [25, 26]). The original IB formulation is based on the mutual in-
40"
INTRODUCTION,0.06156156156156156,"formation functional [27], which is difﬁcult to estimate (especially in high dimensions), and ignores
41"
INTRODUCTION,0.06306306306306306,"complexity constraints on the representation or prediction [28, 29]; see a review in Appendix B. Us-
42"
INTRODUCTION,0.06456456456456457,"ing the notion of usable information, introduced in [28], optimal representations for the supervised
43"
INTRODUCTION,0.06606606606606606,"learning setting were explored in [29] via a two-player game between Alice, which selects a pre-
44"
INTRODUCTION,0.06756756756756757,"diction problem of y given x, and Bob, which then selects the representation z. Alice then uses an
45"
INTRODUCTION,0.06906906906906907,"empirical risk minimizer with the standard goal of minimizing the expected risk. It was established
46"
INTRODUCTION,0.07057057057057058,"in [29] that ideal generalization is obtained for representations that optimize the decodable IB.
47"
INTRODUCTION,0.07207207207207207,"In this paper, we build upon [29], and propose a three-player game for unsupervised representation-
48"
INTRODUCTION,0.07357357357357357,"learning, chosen without a speciﬁc prediction problem (Section 2). First, the representation player
49"
INTRODUCTION,0.07507507507507508,"reduce x ∈Rd to a representation z ∈Rr, where r < d. Second, the response function player
50"
INTRODUCTION,0.07657657657657657,"chooses a response (label) y rule f for x, from a given known class of (random) response functions
51"
INTRODUCTION,0.07807807807807808,"F. The choice of this class manifests the prior knowledge available on the type of prediction prob-
52"
INTRODUCTION,0.07957957957957958,"lems that the representations will be used for. Third, the predictor player optimally predicts y from
53"
INTRODUCTION,0.08108108108108109,"z. The value of the game is determined by the regret: The prediction loss based on the representation
54"
INTRODUCTION,0.08258258258258258,"z compared to prediction loss based on x. The ﬁrst and last player cooperate in order to minimize
55"
INTRODUCTION,0.08408408408408409,"the regret, whereas the response function player aims to maximize it. In other words, the represen-
56"
INTRODUCTION,0.08558558558558559,"tation is chosen to minimize the worst-case prediction loss for any response in F. The output of this
57"
INTRODUCTION,0.08708708708708708,"game is the representation chosen by the ﬁrst player. In order to focus on the representation aspect
58"
INTRODUCTION,0.08858858858858859,"we side-step the generalization problem, and assume that sufﬁcient labeled data will be provided to
59"
INTRODUCTION,0.09009009009009009,"the predictor later on in order to accurately estimate the prediction rule.
60"
INTRODUCTION,0.0915915915915916,"This formulation directly addresses the relevance of a “direction” in the feature space to the pre-
61"
INTRODUCTION,0.09309309309309309,"diction tasks in F, rather than its variability, as in standard unsupervised learning (e.g., PCA and
62"
INTRODUCTION,0.0945945945945946,"AE). Compared to [29], the representation is chosen based only on the class of possible response
63"
INTRODUCTION,0.0960960960960961,"functions, rather than a speciﬁc one. Such knowledge on F may stem from various considerations:
64"
INTRODUCTION,0.09759759759759759,"Domain speciﬁc, imposed by privacy or fairness constraints, or stem from transfer or continual
65"
INTRODUCTION,0.0990990990990991,"learning setting; see Appendix A for an extended discussion. Technically, the game in [29] replaces
66"
INTRODUCTION,0.1006006006006006,"the order of the ﬁrst (representation) and second (response) players. From a different perspective,
67"
INTRODUCTION,0.1021021021021021,"our method is a self-supervised learning method, for which the prior knowledge on F serves as a
68"
INTRODUCTION,0.1036036036036036,"“self-deﬁned signal” for choosing an optimal representation, without any labeled data (see, e.g., [30]
69"
INTRODUCTION,0.10510510510510511,"and [31] for recent surveys). In addition, our game formulation naturally leads to a mixed strategies
70"
INTRODUCTION,0.1066066066066066,"solution [32], that is, allowing the representation player to randomized the representation rule, in
71"
INTRODUCTION,0.10810810810810811,"order to mix up the adversarial response player. This randomization is an inherent aspect of the IB
72"
INTRODUCTION,0.10960960960960961,"formulation, but its usage there is not rigorously justiﬁed. By contrast, for standard unsupervised
73"
INTRODUCTION,0.1111111111111111,"learning, mixed representation does not improve the regret (see Proposition 14 in Appendix E.1 for
74"
INTRODUCTION,0.11261261261261261,"the PCA setting). In Appendix B we provide a thorough discussion of related work.
75"
INTRODUCTION,0.11411411411411411,"Contributions
76"
INTRODUCTION,0.11561561561561562,"• Theoretical: We address the fundamental setting in which the representation, the response, and
77"
INTRODUCTION,0.11711711711711711,"the prediction are linear functions, under the MSE loss function (Section 3). The prior knowledge
78"
INTRODUCTION,0.11861861861861862,"on F is represented by a symmetric matrix S that determines the principal directions of the function
79"
INTRODUCTION,0.12012012012012012,"in the feature space. We establish the optimal representation and regret in pure strategies, which
80"
INTRODUCTION,0.12162162162162163,"shows the utility of the prior information, and in mixed strategies, which shows that randomizing the
81"
INTRODUCTION,0.12312312312312312,"representation yields strictly lower regret. We prove that randomizing between merely ℓ∗different
82"
INTRODUCTION,0.12462462462462462,"representation rules sufﬁces, where r + 1 ≤ℓ∗≤d is a precisely characterized effective dimension.
83"
INTRODUCTION,0.12612612612612611,"• Algorithmic: We develop an iterative gradients-based algorithm that approximates the optimal
84"
INTRODUCTION,0.12762762762762764,"mixed representation (Section 4) for general representations/response/predictors and loss functions.
85"
INTRODUCTION,0.12912912912912913,"The algorithm is greedy, and alternates between ﬁnding a new representation rule and an adversarial
86"
INTRODUCTION,0.13063063063063063,"function. We empirically verify that the output mixed representation has close-to-optimal regret
87"
INTRODUCTION,0.13213213213213212,"in the linear MSE setting. To optimize the weights of the representation, we essentially solve a
88"
INTRODUCTION,0.13363363363363365,"minimax two-player games, and to this end, we utilize the classic multiplicative weights update
89"
INTRODUCTION,0.13513513513513514,"(MWU) algorithm [33] (which is essentially a follow-the-regularized-leader [34, 35]).
90"
PROBLEM FORMULATION,0.13663663663663664,"2
Problem formulation
91"
PROBLEM FORMULATION,0.13813813813813813,"We use mostly conventional notation that is detailed in Appendix C. Speciﬁcally, the eigenval-
92"
PROBLEM FORMULATION,0.13963963963963963,"ues of a positive-semideﬁnite matrix S are denoted as λmax(S) ≡λ1(S) ≥· · · ≥λd(S) =
93"
PROBLEM FORMULATION,0.14114114114114115,"λmin(S) and vi(S) denotes an eigenvector corresponding to λi(S) such that V
= V (S) :=
94"
PROBLEM FORMULATION,0.14264264264264265,"[v1(S), v2(S), · · · , vd(S)] ∈Rd×d and S = V (S)Λ(S)V ⊤(S) is an eigenvalue decomposition.
95"
PROBLEM FORMULATION,0.14414414414414414,"For a matrix W ∈Rd×d we let Wi:j := [wi, . . . , wj] ∈R(j−i+1)×d denote the matrix comprised of
96"
PROBLEM FORMULATION,0.14564564564564564,"the columns indexed by {i, . . . , j}. We denote the probability law of a random variable x as L(x).
97"
PROBLEM FORMULATION,0.14714714714714713,"Let x ∈X be a random feature vector, where Px := L(x) is known. Let y ∈Y be a corresponding
98"
PROBLEM FORMULATION,0.14864864864864866,"response drawn according to a probability kernel y ∼f(· | x = x), where for brevity, we will refer
99"
PROBLEM FORMULATION,0.15015015015015015,"to f as the response function. We assume f ∈F for some known class F. Let z := R(x) ∈Rr be
100"
PROBLEM FORMULATION,0.15165165165165165,"an r-dimensional representation of x where R: X →Rr is chosen from a class R of representation
101"
PROBLEM FORMULATION,0.15315315315315314,"functions, and let Q: Rr →Y be a prediction rule from a class Q, with the loss function loss: Y ×
102"
PROBLEM FORMULATION,0.15465465465465467,"Y →R+. The regret of the representation R for the response function f is
103"
PROBLEM FORMULATION,0.15615615615615616,"regret(R, f | Px) := min
Q∈Q E [loss(y, Q(R(x)))] −
min
Q:Rd→Y E [loss(y, Q(x))] .
(1)"
PROBLEM FORMULATION,0.15765765765765766,"The minimax regret in mixed strategies is deﬁned via the worst case response function in F as
104"
PROBLEM FORMULATION,0.15915915915915915,"regretmix(R, F | Px) :=
min
L(R)∈P(R) max
f∈F E [regret(R, f | Px)] ,
(2)"
PROBLEM FORMULATION,0.16066066066066065,"where P(R) is a set of probability measures on the possible set of representations R. The minimax
105"
PROBLEM FORMULATION,0.16216216216216217,"regret in pure strategies restricts P(R) to degenerated measures (deterministic), and so the expec-
106"
PROBLEM FORMULATION,0.16366366366366367,"tation in (2) is removed. Our main goal is to determine the optimal representation strategy, either
107"
PROBLEM FORMULATION,0.16516516516516516,"in pure R∗∈R or mixed strategies L(R∗) ∈P(R). To this end, we will also utilize the maximin
108"
PROBLEM FORMULATION,0.16666666666666666,"version of (2). Speciﬁcally, let P(F) denote a set of probability measures supported on F, and
109"
PROBLEM FORMULATION,0.16816816816816818,"assume that for any R ∈R, there exists a measure in P(R) that puts all its mass on R. Then, the
110"
PROBLEM FORMULATION,0.16966966966966968,"minimax theorem [32, Chapter 2.4] [36] implies that
111"
PROBLEM FORMULATION,0.17117117117117117,"regretmix(R, F | Px) =
max
L(f)∈P(F) min
R∈R E [regret(R, f | Px)] .
(3)"
PROBLEM FORMULATION,0.17267267267267267,"The right-hand side of (3) is the maximin regret in mixed strategies, and the maximizing prob-
112"
PROBLEM FORMULATION,0.17417417417417416,"ability law L(f ∗) is known as the least favorable prior. In general, regretmix(R, F | Px) ≤
113"
PROBLEM FORMULATION,0.17567567567567569,"regretpure(R, F | Px), and the inequality can be strict. We mention that the use of expectation
114"
PROBLEM FORMULATION,0.17717717717717718,"in the deﬁnition of the mixed regret over the randomized representation, implies that the empirical
115"
PROBLEM FORMULATION,0.17867867867867868,"performance of a system based on this randomized representation achieves the mixed minimax re-
116"
PROBLEM FORMULATION,0.18018018018018017,"gret value in the limit of large number of repeating representation games. The size of the dataset for
117"
PROBLEM FORMULATION,0.1816816816816817,"each of these games should be large enough to allow for accurate learning of f to be used by the
118"
PROBLEM FORMULATION,0.1831831831831832,"predictor. By contrast, the pure minimax regret guarantee is valid for a single representation, and
119"
PROBLEM FORMULATION,0.18468468468468469,"thus more conservative from this aspect.
120"
THE LINEAR SETTING UNDER MSE LOSS,0.18618618618618618,"3
The linear setting under MSE loss
121"
THE LINEAR SETTING UNDER MSE LOSS,0.18768768768768768,"In this section, we focus on linear classes and the MSE loss function. The response function class
122"
THE LINEAR SETTING UNDER MSE LOSS,0.1891891891891892,"is characterized by a quadratic constraint, to wit, the class F is speciﬁed by a matrix S ∈Sd
++ that
123"
THE LINEAR SETTING UNDER MSE LOSS,0.1906906906906907,"represents the relative importance of each direction in the feature space in determining y.
124"
THE LINEAR SETTING UNDER MSE LOSS,0.1921921921921922,"Deﬁnition 1 (The linear MSE setting). Assume that X = Rd, that Y = R and the loss function is
125"
THE LINEAR SETTING UNDER MSE LOSS,0.19369369369369369,"the MSE, loss(y1, y2) = |y1 −y2|2 . Assume that E[x] = 0 and let Σx := E[xxT ] ∈Sd
++ be its
126"
THE LINEAR SETTING UNDER MSE LOSS,0.19519519519519518,"invertible covariance matrix. The classes of representations, response functions, and predictors are
127"
THE LINEAR SETTING UNDER MSE LOSS,0.1966966966966967,"all linear, that is: (1) The representation is z = R(x) = R⊤x for R ∈R := Rd×r where d > r; (2)
128"
THE LINEAR SETTING UNDER MSE LOSS,0.1981981981981982,"The response function is F ∈F ⊂Rd , and y = f ⊤x + n ∈R, where n ∈R is a heteroscedastic
129"
THE LINEAR SETTING UNDER MSE LOSS,0.1996996996996997,"noise that satisﬁes E[n | x] = 0, and given some speciﬁed S ∈Sd
++
130"
THE LINEAR SETTING UNDER MSE LOSS,0.2012012012012012,"f ∈FS :=

f ∈Rd: ∥f∥2
S≤1
	
,
(4)"
THE LINEAR SETTING UNDER MSE LOSS,0.20270270270270271,"where ∥f∥S:= ∥S−1/2f∥2= (f ⊤S−1f)1/2 is the Mahalanobis norm; (3) The predictor is Q(z) =
131"
THE LINEAR SETTING UNDER MSE LOSS,0.2042042042042042,"q⊤z ∈R for q ∈Rr. Since the regret will depend on Px only via Σx, we will abbreviate the
132"
THE LINEAR SETTING UNDER MSE LOSS,0.2057057057057057,"notation of the pure (resp. mixed) minimax regret to regretpure(F | Σx) (resp. regretmix(F | Σx)).
133"
THE LINEAR SETTING UNDER MSE LOSS,0.2072072072072072,"In Appendix E.1 we show that standard PCA can be similarly formulated, by assuming that F
134"
THE LINEAR SETTING UNDER MSE LOSS,0.2087087087087087,"is a singleton containing the noiseless identity function, so that y = x surely holds, and ˆx =
135"
THE LINEAR SETTING UNDER MSE LOSS,0.21021021021021022,"Q(z) ∈Rd. Proposition 14 therein shows that the pure and mixed minimax representations are both
136"
THE LINEAR SETTING UNDER MSE LOSS,0.21171171171171171,"R = V1:r(Σx), and so randomization is not unnecessary. We begin with the pure minimax regret.
137"
THE LINEAR SETTING UNDER MSE LOSS,0.2132132132132132,"Theorem 2. For the linear MSE setting (Deﬁnition 1)
138"
THE LINEAR SETTING UNDER MSE LOSS,0.2147147147147147,"regretpure(FS | Σx) = λr+1

Σ1/2
x SΣ1/2
x

.
(5)"
THE LINEAR SETTING UNDER MSE LOSS,0.21621621621621623,"A minimax representation matrix is
139"
THE LINEAR SETTING UNDER MSE LOSS,0.21771771771771772,"R∗:= Σ−1/2
x
· V1:r

Σ1/2
x SΣ1/2
x

,
(6)"
THE LINEAR SETTING UNDER MSE LOSS,0.21921921921921922,"and the worst case response function is
140"
THE LINEAR SETTING UNDER MSE LOSS,0.22072072072072071,"f ∗:= S1/2 · vr+1

Σ1/2
x SΣ1/2
x

.
(7)"
THE LINEAR SETTING UNDER MSE LOSS,0.2222222222222222,"The optimal representation thus whitens the feature vector x, and then projects it on the top r
141"
THE LINEAR SETTING UNDER MSE LOSS,0.22372372372372373,"eigenvectors of the adjusted covariance matrix Σ1/2
x SΣ1/2
x , which reﬂects the prior knowledge that
142"
THE LINEAR SETTING UNDER MSE LOSS,0.22522522522522523,"f ∈FS. The proof is deferred to Appendix E.2, and its outline is as follows: Plugging the op-
143"
THE LINEAR SETTING UNDER MSE LOSS,0.22672672672672672,"timal predictor into the regret results a quadratic form in f ∈Rd, determined by a matrix which
144"
THE LINEAR SETTING UNDER MSE LOSS,0.22822822822822822,"depends on the subspace spanned by the representation R. The worst-case f is the determined via
145"
THE LINEAR SETTING UNDER MSE LOSS,0.22972972972972974,"the Rayleigh quotient theorem [37, Theorem 4.2.2], and the optimal R is found via the Courant–
146"
THE LINEAR SETTING UNDER MSE LOSS,0.23123123123123124,"Fischer variational characterization [37, Theorem 4.2.6] (see Appendix D for a summary of useful
147"
THE LINEAR SETTING UNDER MSE LOSS,0.23273273273273273,"mathematical results). We next consider the mixed minimax regret.
148"
THE LINEAR SETTING UNDER MSE LOSS,0.23423423423423423,"Theorem 3. For the linear MSE setting (Deﬁnition 1)
149"
THE LINEAR SETTING UNDER MSE LOSS,0.23573573573573572,"regretmix(FS | Σx) =
ℓ∗−r
Pℓ∗
i=1 λ−1
i
,
(8)"
THE LINEAR SETTING UNDER MSE LOSS,0.23723723723723725,"where λi ≡λi(S1/2ΣxS1/2) and ℓ∗is any member of
150 ("
THE LINEAR SETTING UNDER MSE LOSS,0.23873873873873874,"ℓ∈[d]\[r]: (ℓ−r) · λ−1
ℓ
≤ ℓ
X"
THE LINEAR SETTING UNDER MSE LOSS,0.24024024024024024,"i=1
λ−1
i
≤(ℓ−r) · λ−1
ℓ+1 ) (9)"
THE LINEAR SETTING UNDER MSE LOSS,0.24174174174174173,"(with λd+1 ≡0). Furthermore:
151"
THE LINEAR SETTING UNDER MSE LOSS,0.24324324324324326,"• The covariance matrix of the least favorable prior of f: Let Λℓ:= diag(λ1, . . . , λℓ∗, 0, · · · , 0),
152"
THE LINEAR SETTING UNDER MSE LOSS,0.24474474474474475,"and let V ≡V (S1/2ΣxS1/2). Then, the covariance matrix of the least favorable prior of f is
153"
THE LINEAR SETTING UNDER MSE LOSS,0.24624624624624625,"Σ∗
f := V ⊤Λ−1
ℓ∗V
Pℓ∗
i=1 λ−1
i
.
(10)"
THE LINEAR SETTING UNDER MSE LOSS,0.24774774774774774,"• The probability law of the minimax representation: Let A ∈{0, 1}ℓ∗×(
ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.24924924924924924,"r ) be a matrix whose
154"
THE LINEAR SETTING UNDER MSE LOSS,0.25075075075075076,"columns are the members of the set
155"
THE LINEAR SETTING UNDER MSE LOSS,0.25225225225225223,"A := {a ∈{0, 1}ℓ∗: ∥a∥1= ℓ∗−r}
(11)"
THE LINEAR SETTING UNDER MSE LOSS,0.25375375375375375,"(in an arbitrary order). Let b = (b1, . . . , bℓ∗)⊤be such that
156"
THE LINEAR SETTING UNDER MSE LOSS,0.2552552552552553,"bi = (ℓ∗−r) ·
λ−1
i
Pℓ∗
j=1 λ−1
j
.
(12)"
THE LINEAR SETTING UNDER MSE LOSS,0.25675675675675674,"Then, there exists a solution p ∈[0, 1](
ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.25825825825825827,"r ) with support size at most ℓ∗+1 to Ap = b. For j ∈[
 ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.25975975975975973,"r

],
157"
THE LINEAR SETTING UNDER MSE LOSS,0.26126126126126126,"let Ij := {i ∈[ℓ∗]: Aij = 0} be the zero indices on the jth column of A, and let VIj denote the r
158"
THE LINEAR SETTING UNDER MSE LOSS,0.2627627627627628,"columns of V whose index is in Ij. A minimax representation is
159"
THE LINEAR SETTING UNDER MSE LOSS,0.26426426426426425,"R∗= Σ−1/2
x
VIj
(13)"
THE LINEAR SETTING UNDER MSE LOSS,0.26576576576576577,"with probability pj, for j ∈[
 ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.2672672672672673,"r

].
160"
THE LINEAR SETTING UNDER MSE LOSS,0.26876876876876876,"Interestingly, while the eigenvalues λi(Σ1/2
x SΣ1/2
x ) = λi(S1/2ΣxS1/2) are equal, the pure mini-
161"
THE LINEAR SETTING UNDER MSE LOSS,0.2702702702702703,"max regret utilizes the eigenvectors of Σ1/2
x SΣ1/2
x
whereas the mixed minimax regret utilizes those
162"
THE LINEAR SETTING UNDER MSE LOSS,0.27177177177177175,"of S1/2ΣxS1/2, which are possibly different. The proof of Theorem 3 is also in Appendix E.2, and
163"
THE LINEAR SETTING UNDER MSE LOSS,0.2732732732732733,"is substantially more complicated and longer than for the pure regret. We use a two-step indirect
164"
THE LINEAR SETTING UNDER MSE LOSS,0.2747747747747748,"approach, since it seems challenging to directly maximize over L(R). First, we solve the maximin
165"
THE LINEAR SETTING UNDER MSE LOSS,0.27627627627627627,"problem (3), and ﬁnd the least favorable prior L(f ∗). Second, we propose a probability law for the
166"
THE LINEAR SETTING UNDER MSE LOSS,0.2777777777777778,"representation L(R), and show that its regret equals the maximin value, and thus also the minimax.
167"
THE LINEAR SETTING UNDER MSE LOSS,0.27927927927927926,"With more detail, in the ﬁrst step, we show that the regret only depends on L(f) via Σf = E[ff ⊤],
168"
THE LINEAR SETTING UNDER MSE LOSS,0.2807807807807808,"and we explicitly construct a probability law that is both fully supported on FS and has this co-
169"
THE LINEAR SETTING UNDER MSE LOSS,0.2822822822822823,"variance matrix. This reduces the problem from optimizing L(f) to optimizing Σf, whose solution
170"
THE LINEAR SETTING UNDER MSE LOSS,0.28378378378378377,"(Lemma 16) leads to the least favorable Σ∗
f, and then to the maximin value. In the second step,
171"
THE LINEAR SETTING UNDER MSE LOSS,0.2852852852852853,"we explicitly construct a representation that achieves the maximin regret. Concretely, we construct
172"
THE LINEAR SETTING UNDER MSE LOSS,0.28678678678678676,"representation matrices that use r of the ℓ∗principal components of Σ1/2
x SΣ1/2
x , where ℓ∗> r.
173"
THE LINEAR SETTING UNDER MSE LOSS,0.2882882882882883,"The deﬁning property of ℓ∗(9) established in the maximin solution is utilized to ﬁnd weights on the
174
 ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.2897897897897898,"r

possible representations, that achieves the maximin solution, and thus also the minimax. The
175"
THE LINEAR SETTING UNDER MSE LOSS,0.2912912912912913,"proof uses Carathéodory’s theorem (see Appendix D) which also establishes that the optimal {pj}
176"
THE LINEAR SETTING UNDER MSE LOSS,0.2927927927927928,"is supported on at most ℓ∗+ 1 matrices, much less than
 ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.29429429429429427,"r

. We next make a few comments:
177"
THE LINEAR SETTING UNDER MSE LOSS,0.2957957957957958,"1. Computing the mixed minimax probability: This requires solving A
⊤p = b for a probability
178"
THE LINEAR SETTING UNDER MSE LOSS,0.2972972972972973,"vector p, which is a linear-program feasibility problem that is routinely solved [38]. For illustration,
179"
THE LINEAR SETTING UNDER MSE LOSS,0.2987987987987988,"if r = 1 then A ∈{0, 1}ℓ∗×ℓ∗is a square all ones matrix, except for a zero diagonal, and pj =
180"
THE LINEAR SETTING UNDER MSE LOSS,0.3003003003003003,"1 −(ℓ∗−1)λ−1
j /(Pℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.30180180180180183,"i=1 λ−1
i ) for j ∈[ℓ∗]. Similarly, the case ℓ∗= r + 1 is solved by setting
181"
THE LINEAR SETTING UNDER MSE LOSS,0.3033033033033033,"pj = (λ−1
j )/(Pℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.3048048048048048,"j′=1 λ−1
j′ ) on the ℓ∗standard basis vectors. Nonetheless, the dimension of p is
 ℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.3063063063063063,"r

182"
THE LINEAR SETTING UNDER MSE LOSS,0.3078078078078078,"and thus increases fast as Θ((ℓ∗)r) , and this approach may be intractable. However, in this case the
183"
THE LINEAR SETTING UNDER MSE LOSS,0.30930930930930933,"algorithm we present in Section 4 can be used. As we empirically show, it approximately achieves
184"
THE LINEAR SETTING UNDER MSE LOSS,0.3108108108108108,"the optimal regret, and the number of atoms is not much larger than ℓ∗+ 1.
185"
THE LINEAR SETTING UNDER MSE LOSS,0.3123123123123123,"2. Required randomness: The regret formulation (2) assumes that the actual realization of the rep-
186"
THE LINEAR SETTING UNDER MSE LOSS,0.3138138138138138,"resentation rule is known to the predictor. Formally, this can be conveyed to the predictor using an
187"
THE LINEAR SETTING UNDER MSE LOSS,0.3153153153153153,"small header of less than log2(ℓ∗+ 1) ≤log(d + 1) bits. Practically, this is unnecessary and an
188"
THE LINEAR SETTING UNDER MSE LOSS,0.31681681681681684,"efﬁcient predictor can be learned from a labeled data set (z, y).
189"
THE LINEAR SETTING UNDER MSE LOSS,0.3183183183183183,"3. The rank of Σ∗
f: The rank of the covariance matrix of the least favorable prior is an effective
190"
THE LINEAR SETTING UNDER MSE LOSS,0.31981981981981983,"dimension, satisfying (see (8))
191"
THE LINEAR SETTING UNDER MSE LOSS,0.3213213213213213,"ℓ∗= arg max
ℓ∈[d]\[r]"
THE LINEAR SETTING UNDER MSE LOSS,0.3228228228228228,1 −(r/ℓ)
THE LINEAR SETTING UNDER MSE LOSS,0.32432432432432434,"1
ℓ
Pℓ
i=1 λ−1
i
.
(14)"
THE LINEAR SETTING UNDER MSE LOSS,0.3258258258258258,"By convention, {λ−1
i }i∈[d] is a monotonic non-decreasing sequence, and so is the partial Cesàro
192"
THE LINEAR SETTING UNDER MSE LOSS,0.32732732732732733,mean ψ(ℓ) := 1
THE LINEAR SETTING UNDER MSE LOSS,0.32882882882882886,"ℓ
Pℓ
i=1 λ−1
i . For example, if λi = i−α with α > 0 then ψ(ℓ) = Θ(ℓα). If, e.g.,
193"
THE LINEAR SETTING UNDER MSE LOSS,0.3303303303303303,"ψ(ℓ) = ℓα, then it is easily derived that ℓ∗≈min{ α+1"
THE LINEAR SETTING UNDER MSE LOSS,0.33183183183183185,"α r, d}. So, if α ≥
r
d−r is large enough and
194"
THE LINEAR SETTING UNDER MSE LOSS,0.3333333333333333,"the decay rate of {λi} is fast enough then ℓ∗< d, and otherwise ℓ∗= d. As the decay rate of
195"
THE LINEAR SETTING UNDER MSE LOSS,0.33483483483483484,"{λi} becomes faster, the rank of Σ∗
f decreases to r. Importantly, ℓ∗≥r + 1 always holds, and so
196"
THE LINEAR SETTING UNDER MSE LOSS,0.33633633633633636,"the optimal mixed representation is not deterministic even if S1/2ΣxS1/2 has less than r signiﬁcant
197"
THE LINEAR SETTING UNDER MSE LOSS,0.33783783783783783,"eigenvalues (which can be represented by a single matrix R ∈Rd×r). Hence, the mixed minimax
198"
THE LINEAR SETTING UNDER MSE LOSS,0.33933933933933935,"regret is always strictly lower than the pure minimax regret. Thus, even when S = Id, and no
199"
THE LINEAR SETTING UNDER MSE LOSS,0.3408408408408408,"valuable prior knowledge is known on the response function, the mixed minimax representation is
200"
THE LINEAR SETTING UNDER MSE LOSS,0.34234234234234234,"different from the standard PCA solution of top r eigenvectors of Σx.
201"
THE LINEAR SETTING UNDER MSE LOSS,0.34384384384384387,"4. Uniqueness of the optimal representation: Since one can always post-multiply R⊤x by some
202"
THE LINEAR SETTING UNDER MSE LOSS,0.34534534534534533,"invertible matrix, and then pre-multiply z = R⊤x by its inverse, the following simple observation
203"
THE LINEAR SETTING UNDER MSE LOSS,0.34684684684684686,"holds: When R and Q are not further restricted, then if R is a minimax representation, and W(R) ∈
204"
THE LINEAR SETTING UNDER MSE LOSS,0.3483483483483483,"Rr×r is an invertible matrix, then R · W(R) is also a minimax representation.
205"
THE LINEAR SETTING UNDER MSE LOSS,0.34984984984984985,"5. Inﬁnite-dimensional features: Theorems 2 and 3 assume a ﬁnite dimensional feature space, but
206"
THE LINEAR SETTING UNDER MSE LOSS,0.35135135135135137,"as we show in Appendix F, the results can be easily generalized to an inﬁnite dimensional Hilbert
207"
THE LINEAR SETTING UNDER MSE LOSS,0.35285285285285284,"space X, in the more restrictive setting that the noise n is statistically independent of x.
208"
THE LINEAR SETTING UNDER MSE LOSS,0.35435435435435436,"Example 4. Assume S = Id, and denote, for brevity, V ≡V (Σx) := [v1, . . . , vd] and Λ ≡
209"
THE LINEAR SETTING UNDER MSE LOSS,0.35585585585585583,"Λ(Σx) := diag(λ1, . . . , λd). The optimal minimax representation in pure strategies (Theorem 2) is
210"
THE LINEAR SETTING UNDER MSE LOSS,0.35735735735735735,"1
1.5
2
2.5
3
3.5
4
0 1 2 3 4 5 6 7 8"
THE LINEAR SETTING UNDER MSE LOSS,0.3588588588588589,"9
10-3 30 35 40 45 50"
THE LINEAR SETTING UNDER MSE LOSS,0.36036036036036034,"1
1.5
2
2.5
3
3.5
4
0 0.2 0.4 0.6 0.8 1 1.2"
THE LINEAR SETTING UNDER MSE LOSS,0.36186186186186187,"1.4
10-4 38 40 42 44 46 48 50"
THE LINEAR SETTING UNDER MSE LOSS,0.3633633633633634,"Figure 1: Left: Pure and mixed minimax regret and ℓ∗for Example 4, for d = 50, r = 25, with
λi = σ2
i ∝i−α. Right: Pure and mixed minimax regret and ℓ∗for Example 5, for d = 50, r = 25,
with σ2
i ∝i−α and si ∝i2. The trend of ℓ∗is reversed for α > 2."
THE LINEAR SETTING UNDER MSE LOSS,0.36486486486486486,"then
211"
THE LINEAR SETTING UNDER MSE LOSS,0.3663663663663664,"R∗= Σ−1/2
x
· V1:r = V Λ−1/2
x
V ⊤V1:r = V Λ−1/2
x
· [e1, . . . , er] =
h
λ−1/2
1
· v1, . . . , λ−1/2
r
· vr
i
, (15)"
THE LINEAR SETTING UNDER MSE LOSS,0.36786786786786785,"which is comprised of the top r eigenvectors of Σx, scaled so that v⊤
i x has unit variance. By
212"
THE LINEAR SETTING UNDER MSE LOSS,0.36936936936936937,"Comment 4 above, V1:r is also an optimal minimax representation. The worst case response is
213"
THE LINEAR SETTING UNDER MSE LOSS,0.3708708708708709,"f = vr+1(Σx) and, as expected, since R uses the ﬁrst r principal directions
214"
THE LINEAR SETTING UNDER MSE LOSS,0.37237237237237236,"regretpure(F | Σx) = λr+1.
(16)
The minimax regret in mixed strategies (Theorem 3) is different, and given by
215"
THE LINEAR SETTING UNDER MSE LOSS,0.3738738738738739,"regretmix(F | Σx) =
ℓ∗−r
Pℓ∗
i=1 λ−1
i
,
(17)"
THE LINEAR SETTING UNDER MSE LOSS,0.37537537537537535,"where ℓ∗is determined by the decay rate of the eigenvalues of Σx (see (9)). The least favorable
216"
THE LINEAR SETTING UNDER MSE LOSS,0.3768768768768769,"covariance matrix is given by (Theorem 3)
217"
THE LINEAR SETTING UNDER MSE LOSS,0.3783783783783784,"Σ∗
f ="
THE LINEAR SETTING UNDER MSE LOSS,0.37987987987987987,""" ℓ∗
X"
THE LINEAR SETTING UNDER MSE LOSS,0.3813813813813814,"i=1
λ−1
i #−1"
THE LINEAR SETTING UNDER MSE LOSS,0.38288288288288286,"· V diag
 
λ−1
1 , . . . , λ−1
ℓ∗, 0, · · · , 0

· V ⊤.
(18)"
THE LINEAR SETTING UNDER MSE LOSS,0.3843843843843844,"Intuitively, the least favorable Σ∗
f equalizes the ﬁrst ℓ∗eigenvalues of ΣxΣ∗
f (and nulls the other
218"
THE LINEAR SETTING UNDER MSE LOSS,0.3858858858858859,"d −ℓ∗) so that the representation is indifferent to these ℓ∗directions. As evident from the regret, the
219"
THE LINEAR SETTING UNDER MSE LOSS,0.38738738738738737,"“equalization” of the ith eigenvalue adds a term of λ−1
i
to the denominator, and if λi is too small
220"
THE LINEAR SETTING UNDER MSE LOSS,0.3888888888888889,"then vi is not chosen for the representation, as agrees with Comment 3 above (a fast decay of {λi}
221"
THE LINEAR SETTING UNDER MSE LOSS,0.39039039039039036,"reduces ℓ∗away from d). The mixed minimax representation sets
222"
THE LINEAR SETTING UNDER MSE LOSS,0.3918918918918919,"R∗= Σ−1/2
x
· VIj =
h
λ−1/2
ij,1
· vij,1, . . . , λ−1/2
ij,r
· vij,r
i
(19)"
THE LINEAR SETTING UNDER MSE LOSS,0.3933933933933934,"with probability pj, where Ij ≡{ij,1, . . . , ij,r} (the derivation is similar to (15)). Thus, the optimal
223"
THE LINEAR SETTING UNDER MSE LOSS,0.3948948948948949,"representation chooses a random subset of r vectors from {v1, . . . , vℓ∗}. See the left panel of Figure
224"
THE LINEAR SETTING UNDER MSE LOSS,0.3963963963963964,"1 for a numerical example.
225"
THE LINEAR SETTING UNDER MSE LOSS,0.3978978978978979,"Example 5. To demonstrate the effect of prior knowledge on the response function, we assume
226"
THE LINEAR SETTING UNDER MSE LOSS,0.3993993993993994,"Σx = diag(σ2
1, . . . , σ2
d) and S = diag(s1, . . . , sd), where σ2
1 ≥σ2
2 ≥· · · ≥σ2
d (but {si}i∈[d]
227"
THE LINEAR SETTING UNDER MSE LOSS,0.4009009009009009,"are not necessarily ordered). Letting f = (f1, . . . , fd), the class of response functions is FS :=
228"
THE LINEAR SETTING UNDER MSE LOSS,0.4024024024024024,"{f ∈Rd: Pd
i=1(f 2
i /si) ≤1}, and so coordinates i ∈[d] with a large si have large inﬂuence on
229"
THE LINEAR SETTING UNDER MSE LOSS,0.4039039039039039,"the response. Let (i(1), . . . , i(d)) be a permutation of [d] so that σ2
i(j)si(j) it the jth largest value of
230"
THE LINEAR SETTING UNDER MSE LOSS,0.40540540540540543,"(σ2
i si)i∈[d]. The pure minimax regret is (Theorem 2)
231"
THE LINEAR SETTING UNDER MSE LOSS,0.4069069069069069,"regretpure(F | Σx) = σ2
ir+1sir+1.
(20)"
THE LINEAR SETTING UNDER MSE LOSS,0.4084084084084084,"The optimal representation is R = [ei(1), ei(2), . . . , ei(r)], that is, uses the most inﬂuential coordi-
232"
THE LINEAR SETTING UNDER MSE LOSS,0.4099099099099099,"nates, according to {si}, which may be different from the r principal directions of Σx. For the
233"
THE LINEAR SETTING UNDER MSE LOSS,0.4114114114114114,"minimax regret in mixed strategies, Theorem 3 results
234"
THE LINEAR SETTING UNDER MSE LOSS,0.41291291291291293,"regretmix(F | Σx) =
ℓ∗−r
Pℓ∗
j=1(sijσ2
ij)−1
(21)"
THE LINEAR SETTING UNDER MSE LOSS,0.4144144144144144,"for ℓ∗∈[d]\[r] satisfying (9), and the covariance matrix of the least favorable prior is given by
235"
THE LINEAR SETTING UNDER MSE LOSS,0.4159159159159159,"Σ∗
f = Pℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.4174174174174174,"j=1 σ−2
ij · eije⊤
ij
Pℓ∗
j=1(sijσ2
ij)−1 .
(22)"
THE LINEAR SETTING UNDER MSE LOSS,0.4189189189189189,"That is, up to a scale factor (Pℓ∗"
THE LINEAR SETTING UNDER MSE LOSS,0.42042042042042044,"i=1 s−1
i σ−2
i
)−1, the matrix is diagonal so that the kth term on the
236"
THE LINEAR SETTING UNDER MSE LOSS,0.4219219219219219,"diagonal is Σ∗
f(k, k) = σ−2
k
if k = ij for some j ∈[ℓ∗] and Σ∗
f(k, k) = 0 otherwise. As in
237"
THE LINEAR SETTING UNDER MSE LOSS,0.42342342342342343,"Example 4, Σ∗
f equalizes the ﬁrst ℓ∗eigenvalues of ΣxΣf (and nulls the other d −ℓ∗). However, it
238"
THE LINEAR SETTING UNDER MSE LOSS,0.42492492492492495,"does so in a manner that chooses them according to their inﬂuence on f ⊤x. The random minimax
239"
THE LINEAR SETTING UNDER MSE LOSS,0.4264264264264264,"representation in mixed strategies is
240"
THE LINEAR SETTING UNDER MSE LOSS,0.42792792792792794,"R∗=
h
σ−1
ij,1 · eij,1, . . . , σ−1
ij,r · eij,r
i
(23)"
THE LINEAR SETTING UNDER MSE LOSS,0.4294294294294294,"with probability pj. Again, all the ﬁrst ℓ∗coordinates are used, and not just the top r. See the right
241"
THE LINEAR SETTING UNDER MSE LOSS,0.43093093093093093,"panel of Figure 1 for a numerical example. We ﬁnally remark that, naturally, in the non-diagonal
242"
THE LINEAR SETTING UNDER MSE LOSS,0.43243243243243246,"case, the minimax regret will also depend on the relative alignment between S and Σx.
243"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4339339339339339,"4
An iterative algorithm for general classes and loss functions
244"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.43543543543543545,"In this section, we develop an iterative algorithm for ﬁnding the optimal representation in mixed
245"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4369369369369369,"strategies, i.e., solving (2) for general classes and loss functions. Since optimizing general probabil-
246"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.43843843843843844,"ity measures over R is formidable, we restrict the optimization to ﬁnite mixed representations, i.e.,
247"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.43993993993993996,"assume that R = R(j) ∈R with probability p(j), where j ∈[m] (which sufﬁces for the linear MSE
248"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.44144144144144143,"setting of Section 3, but possibly sub-optimal in general). Furthermore, the algorithm’s operation
249"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.44294294294294295,"will require randomization also for the response player, and so we set f = f (i) ∈F with probability
250"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4444444444444444,"o(i) where i ∈[m], and m = m0 + m for some m0 ≥0. The resulting optimization problem then
251"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.44594594594594594,"becomes
252"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.44744744744744747,"min
{p(j),R(j)∈R}
max
{o(i),f (i)∈F}
min
{Q(j,i)∈Q} X j∈[m] X"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.44894894894894893,"i∈[m]
p(j)·o(i)·E
h
loss(f (i)(x), Q(j,i)(R(j)(x)))
i
, (24)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.45045045045045046,under the constraints p(j) ≥0 and P
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4519519519519519,"j p(j) = 1, and o(i) ≥0 and P"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.45345345345345345,"i o(i) = 1. Note that the
253"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.45495495495495497,"prediction rule Q(j,i) is determined based on both R(j) and f (i), and that the ultimate goal of solving
254"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.45645645645645644,"(24) is just to extract the optimal R.
255"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.45795795795795796,"A high level description of the algorithm is to gradually add more representations to the support size
256"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4594594594594595,"of R up to m, where next k will denote the current number of representations, k ∈[m]. Initialization
257"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.46096096096096095,"requires an representation R(1), as well as a set of functions {f (i)}i∈m0, so that the ﬁnal support
258"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4624624624624625,"size of f will be m = m0 + m. Finding this initial representation and the set of functions is based
259"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.46396396396396394,"on the speciﬁc loss function and a possible set of representation/predictors. At iteration k ∈[m], the
260"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.46546546546546547,"main loop of the algorithm has two phases. In the ﬁrst phase, a new adversarial function is added
261"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.466966966966967,"to the set of functions, as the worse function for the current random representation. In the second
262"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.46846846846846846,"phase, a new representation atom is added to the set of possible representations. This representation
263"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.46996996996997,"is determined based on the given set of functions. Concretely, the two phases operate as follows:
264"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.47147147147147145,"• Phase 1 – Given k representations {R(j)}j∈(k) with weights {p(j)}j∈[k], the algorithm determines
265"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.47297297297297297,"the function f (m0+k) as the worst function for this random representation (optimal adversarial action
266"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4744744744744745,"of the response function player). Speciﬁcally,
267"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.47597597597597596,"regk := regretmix({R(j), p(j)}j∈[k], F | Px)
(25)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4774774774774775,":= max
f∈F
min
{Q(j)∈Q}j∈[k] X"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.47897897897897895,"j∈[k]
p(j) · E
h
loss(f(x), Q(j)(R(j)(x)))
i
(26)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4804804804804805,"is solved, and f (m0+k) is set to be the maximizer. This simpliﬁes (24) in the sense that m is replaced
268"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.481981981981982,"by k, the random representation R is kept ﬁxed, and f ∈F is optimized as a pure strategy (the
269"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.48348348348348347,"previous functions {f (i)}i∈[m0+k−1] are ignored).
270"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.484984984984985,"• Phase 2 – Adding a representation atom: Given ﬁxed {f (j)}j∈[m0+k] and {R(j)}j∈[k], a new
271"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4864864864864865,"representation R(k+1) is found as the most incrementally valuable representation atom. Speciﬁcally,
272"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.487987987987988,"min
R(k+1)∈R regretmix({R(j1)}j1∈[k+1], {f (j2)}j2∈[m0+k] | Px)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4894894894894895,":=
min
R(k+1)∈R
min
{p(j1)}j1∈[k+1]
max
{o(j2)}j2∈[m0+k]
min
{Q(j1,j2)∈Q}j1∈[k+1],j2∈[m0+k]
X"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.49099099099099097,j1∈[k+1] X
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4924924924924925,"j2∈[m0+k]
p(j1) · o(j2) · E
h
loss(f (j1)(x), Q(j1,j2)(R(j1)(x)))
i
(27)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.493993993993994,"is solved, the solution R(k+1) is added to the set of representations, and the weights are updated to
273"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4954954954954955,"the optimal {p(j1)}j1∈[k+1]. Compared to (24), here the response functions and current k represen-
274"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.496996996996997,"tations are kept ﬁxed, and only their weights {p(j1)} {o(j2)} are optimized, along with R(k+1).
275"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.4984984984984985,"The procedure is described in Algorithm 1,
where,
following the main loop,
m∗
=
276"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5,"arg mink∈[m] regk representation atoms are chosen and the output is {R(j), p(j)}j∈[m∗]. Algorithm
277"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5015015015015015,"1 relies on solvers for the Phase 1 (26) and Phase 2 (27) problems. In Appendix G we propose two
278"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.503003003003003,"algorithms for these problems, which are based on gradient steps for updating the adversarial re-
279"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5045045045045045,"sponse and the new representation, and on the MWU algorithm [33] (follow-the-regularized-leader
280"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.506006006006006,"[35]) for updating the weights. In short, the Phase 1 algorithm updates the response function f via
281"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5075075075075075,"a projected gradient step of the expected loss, and then adjusts the predictors {Q(j)} to the updated
282"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.509009009009009,"response function f and the current representations {R(j)}j∈[k]. The Phase 2 algorithm only up-
283"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5105105105105106,"dates the new representation R(k+1) via projected gradient steps, while keeping {R(j)}j∈[k] ﬁxed.
284"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.512012012012012,"Given the representations {R(j)}j∈[k+1] and the functions {f (i)}i∈[m0+k], a predictor Q(j,i) is then
285"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5135135135135135,"ﬁtted to each representation-function pair, which also determines the loss for this pair. The weights
286"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.515015015015015,"{p(j)}j∈[k+1] and {o(i)}i∈[m0+k] are updated towards the equilibrium of the two-player game deter-
287"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5165165165165165,"mined by the loss of the predictors {Q(j,i)}j∈[k+1],i∈[m0+k] via the MWU algorithm."
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5180180180180181,Algorithm 1 Solver of (24): An iterative algorithm for learning mixed representations.
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5195195195195195,"1: input Px, R, F, Q, d, r, m, m0
▷Feature distribution, classes, dimensions and parameters
2: input R(1) , {f (j)}j∈[m0]
▷Initial representation and initial function (set)
3: begin
4: for k = 1 to m do
5:
phase 1: f (m0+k) is set by a solver of (26) and"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.521021021021021,"regk ←regretmix({R(j), p(j)}j∈[k], F | Px)
(28)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5225225225225225,▷Solved using Algorithm 2
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.524024024024024,"6:
phase 2: R(k+1), {p(j)
k }j∈[k+1] is set by a solver of (27)
▷Solved using Algorithm 3; step
can be removed if k = m
7: end for
8: set m∗= arg mink∈[m] regk
9: return {R(j)}j∈[m∗] and pm∗= {p(j)
k }j∈[m∗] 288"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5255255255255256,"We next outline two examples, where full details can be found in Appendix H.
289"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.527027027027027,"Example 6. We validate that efﬁciency of Algorithm 1 in the linear MSE setting (Section 3), for
290"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5285285285285285,"which a closed-form solution exists. We ran Algorithm 1 on randomly drawn diagonal Σx, and
291"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.53003003003003,"computed the ratio between the regret obtained by the algorithm to the theoretical value. The left
292"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5315315315315315,"panel of Figure 2 shows that the ratio is between 1.15−1.2 in a wide range of d values. We mention
293"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5330330330330331,"again that Algorithm 1 is useful even for this setting since ﬁnding an (ℓ∗+ 1)-sparse solution to
294"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5345345345345346,"Ap = b is computationally difﬁcult when
 ℓ∗"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.536036036036036,"r

is very large. For example, in the largest dimension
295"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5375375375375375,"of the experiment, the potential number of representation matrices is
 d
r

=
 19
5

= 11, 628.
296"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.539039039039039,"Our next example pertains to a logistic regression setting, under the cross-entropy loss function.
297"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5405405405405406,"Deﬁnition 7 (The linear cross-entropy setting). Assume that X = Rd, that Y = {±1} and that
298"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5420420420420421,"E[x] = 0. Assume that the class of representation is linear z = R(x) = R⊤x for some R ∈R :=
299"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5435435435435435,"Figure 2: Results of Algorithm 1. Left: r = 5, varying d. The ratio between the regret achieved by
Algorithm 1 and the theoretical regret in the linear MSE setting. Right: r = 3, varying d. The regret
achieved by Algorithm 1 in the linear cross entropy setting, various m."
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.545045045045045,"Rd×r where d > r. Assume that a response function and a prediction rule determine the probability
300"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5465465465465466,"that y = 1 via logistic regression modeling, as f(y = ±1 | x) = 1/[1 + exp(∓f ⊤x)]. Assume
301"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5480480480480481,"the cross-entropy loss function, where given that the prediction that y = 1 with probability q results
302"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5495495495495496,"the loss loss(y, q) := −1"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.551051051051051,2(1 + y) log q −1
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5525525525525525,"2 (1 −y) log(1 −q). The set of predictor functions is
303"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5540540540540541,"Q :=

Q(z) = 1/[1 + exp(−q⊤z)], q ∈Rr	
. As for the linear case, we assume that f ∈FS
304"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5555555555555556,"for some S ∈Sd
++. It is not difﬁcult to show that the regret is then given by the expected binary
305"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5570570570570571,"Kullback-Leibler (KL) divergence
306"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5585585585585585,"regret(R, f | Px) = min
q∈Rr E

DKL
 
[1 + exp(−f ⊤x)]−1 || [1 + exp(−q⊤R⊤x)]−1
.
(29)"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.56006006006006,"Example 8. We ran Algorithm 1 on empirical distributions of features drawn from an isotropic
307"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5615615615615616,"normal distribution, in the linear cross-entropy setting. Algorithm 1 is suitable in this setting since
308"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5630630630630631,"gradients of the regret have closed-form (see Appendix H). The right panel of Figure 2 shows the
309"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5645645645645646,"reduced regret obtained by increasing the support size m of the random representation, and thus the
310"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.566066066066066,"effectiveness of mixed representations.
311"
AN ITERATIVE ALGORITHM FOR GENERAL CLASSES AND LOSS FUNCTIONS,0.5675675675675675,"We refer the reader to Appendix I for additional experiments with Algorithm 1.
312"
CONCLUSION,0.5690690690690691,"5
Conclusion
313"
CONCLUSION,0.5705705705705706,"We proposed a game-theoretic formulation for learning representations of unlabeled features when
314"
CONCLUSION,0.5720720720720721,"prior knowledge (or assumptions) on the class of future prediction tasks is available. We focused on
315"
CONCLUSION,0.5735735735735735,"the fundamental of linear MSE setting, and derived the optimal solution. Beyond the lower regret
316"
CONCLUSION,0.575075075075075,"that is directly obtained from utilizing the prior knowledge, our results also revealed the importance
317"
CONCLUSION,0.5765765765765766,"of using randomized representations. We have then proposed an iterative algorithm suitable for
318"
CONCLUSION,0.5780780780780781,"general classes of functions and losses, and exempliﬁed its effectiveness.
319"
CONCLUSION,0.5795795795795796,"We next discuss limitations and potential future research: (1) We have focused on the elementary and
320"
CONCLUSION,0.581081081081081,"simpliﬁed class FS = {f: ∥f∥S≤1}, mainly for theoretical investigations. A natural reﬁnement to
321"
CONCLUSION,0.5825825825825826,"non-linear functions is the general class FSx := E

∥∇xf(x)∥2
Sx

≤1, where {Sx}x∈Rd is now
322"
CONCLUSION,0.5840840840840841,"locally speciﬁed (somewhat similarly to the regularization term used in contractive AE [39], though
323"
CONCLUSION,0.5855855855855856,"for different reasons). (2) Since the proposed iterative algorithm includes optimization over three
324"
CONCLUSION,0.5870870870870871,"players, it is of interest to develop version of the algorithm with lower computational optimization
325"
CONCLUSION,0.5885885885885885,"cost. (3) We have assumed that FS is given in advance, and a natural follow-up goal is to efﬁciently
326"
CONCLUSION,0.5900900900900901,"learn S from previous experience, e.g., improving S from one episode to another in a meta-learning
327"
CONCLUSION,0.5915915915915916,"setup [40]. (4) It is interesting to evaluate the effectiveness of the learned representation in our
328"
CONCLUSION,0.5930930930930931,"formulation, as an initialization for further optimization when labeled data is collected. One may
329"
CONCLUSION,0.5945945945945946,"postulate that since our learned representation is uniformly good for all response functions in the
330"
CONCLUSION,0.5960960960960962,"class, it may serve as a universal initialization for such training.
331"
CONCLUSION,0.5975975975975976,"Broader impact
332"
CONCLUSION,0.5990990990990991,"The research described in this paper is foundational, and does not aim for any speciﬁc application.
333"
CONCLUSION,0.6006006006006006,"Nonetheless, the learned representation is based on a prior assumption on the class of response
334"
CONCLUSION,0.6021021021021021,"functions, and the choice of this prior may have positive or negative impacts: For example, a risk
335"
CONCLUSION,0.6036036036036037,"of this choice of prior is that the represented features completely ignore a viable feature for making
336"
CONCLUSION,0.6051051051051051,"future predictions. A beneﬁt that can stem from choosing a proper prior is that the representation
337"
CONCLUSION,0.6066066066066066,"will null the effect of features that lead to unfair advantages for some particular group, in future
338"
CONCLUSION,0.6081081081081081,"predictions. Anyhow, the results presented in the paper are indifferent to such future utilization, and
339"
CONCLUSION,0.6096096096096096,"any usage of these results should take into account the aforementioned possible implications.
340"
REFERENCES,0.6111111111111112,"References
341"
REFERENCES,0.6126126126126126,"[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
342"
REFERENCES,0.6141141141141141,"[2] John N. Tsitsiklis. Decentralized detection. 1989.
343"
REFERENCES,0.6156156156156156,"[3] XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. On surrogate loss func-
344"
REFERENCES,0.6171171171171171,"tions and f-divergences. The Annals of Statistics, 37(2):876 – 904, 2009. doi: 10.1214/
345"
REFERENCES,0.6186186186186187,"08-AOS595. URL https://doi.org/10.1214/08-AOS595.
346"
REFERENCES,0.6201201201201201,"[4] John Duchi, Khashayar Khosravi, and Feng Ruan. Multiclass classiﬁcation, information,
347"
REFERENCES,0.6216216216216216,"divergence and surrogate risk. Annals of Statistics, 46(6B):3246–3275, 2018. ISSN 0090-
348"
REFERENCES,0.6231231231231231,"5364. doi: 10.1214/17-AOS1657.
349"
REFERENCES,0.6246246246246246,"[5] Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT
350"
REFERENCES,0.6261261261261262,"Press, 2012. ISBN 0262017180.
351"
REFERENCES,0.6276276276276276,"[6] Karl Pearson. On lines and planes of closest ﬁt to systems of points in space. The London,
352"
REFERENCES,0.6291291291291291,"Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559–572, 1901.
353"
REFERENCES,0.6306306306306306,"[7] Ian Jolliffe. Principal component analysis. Encyclopedia of statistics in behavioral science,
354"
REFERENCES,0.6321321321321322,"2005.
355"
REFERENCES,0.6336336336336337,"[8] John P. Cunningham and Zoubin Ghahramani.
Linear dimensionality reduction: Survey,
356"
REFERENCES,0.6351351351351351,"insights, and generalizations. The Journal of Machine Learning Research, 16(1):2859–2900,
357"
REFERENCES,0.6366366366366366,"2015.
358"
REFERENCES,0.6381381381381381,"[9] Iain M. Johnstone and Debashis Paul. PCA in high dimensions: An orientation. Proceedings
359"
REFERENCES,0.6396396396396397,"of the IEEE, 106(8):1277–1292, 2018.
360"
REFERENCES,0.6411411411411412,"[10] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear component anal-
361"
REFERENCES,0.6426426426426426,"ysis as a kernel eigenvalue problem. Neural computation, 10(5):1299–1319, 1998.
362"
REFERENCES,0.6441441441441441,"[11] Mark A. Kramer. Nonlinear principal component analysis using autoassociative neural net-
363"
REFERENCES,0.6456456456456456,"works. AIChE journal, 37(2):233–243, 1991.
364"
REFERENCES,0.6471471471471472,"[12] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with
365"
REFERENCES,0.6486486486486487,"neural networks. science, 313(5786):504–507, 2006.
366"
REFERENCES,0.6501501501501501,"[13] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Unsupervised learning
367"
REFERENCES,0.6516516516516516,"of hierarchical representations with convolutional deep belief networks. Communications of
368"
REFERENCES,0.6531531531531531,"the ACM, 54(10):95–103, 2011.
369"
REFERENCES,0.6546546546546547,"[14] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol,
370"
REFERENCES,0.6561561561561562,"and Léon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep
371"
REFERENCES,0.6576576576576577,"network with a local denoising criterion. Journal of machine learning research, 11(12), 2010.
372"
REFERENCES,0.6591591591591591,"[15] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and
373"
REFERENCES,0.6606606606606606,"new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):
374"
REFERENCES,0.6621621621621622,"1798–1828, 2013.
375"
REFERENCES,0.6636636636636637,"[16] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method.
376"
REFERENCES,0.6651651651651652,"arXiv preprint physics/0004057, 2000.
377"
REFERENCES,0.6666666666666666,"[17] Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for
378"
REFERENCES,0.6681681681681682,"Gaussian variables. Advances in Neural Information Processing Systems, 16, 2003.
379"
REFERENCES,0.6696696696696697,"[18] Noam Slonim, Nir Friedman, and Naftali Tishby. Multivariate information bottleneck. Neural
380"
REFERENCES,0.6711711711711712,"computation, 18(8):1739–1789, 2006.
381"
REFERENCES,0.6726726726726727,"[19] Peter Harremoës and Naftali Tishby. The information bottleneck revisited or how to choose
382"
REFERENCES,0.6741741741741741,"a good distortion measure. In 2007 IEEE International Symposium on Information Theory,
383"
REFERENCES,0.6756756756756757,"pages 566–570. IEEE, 2007.
384"
REFERENCES,0.6771771771771772,"[20] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.
385"
REFERENCES,0.6786786786786787,"In 2015 ieee information theory workshop, pages 1–5. IEEE, 2015.
386"
REFERENCES,0.6801801801801802,"[21] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
387"
REFERENCES,0.6816816816816816,"information. arXiv preprint arXiv:1703.00810, 2017.
388"
REFERENCES,0.6831831831831832,"[22] Ravid Shwartz-Ziv.
Information ﬂow in deep neural networks.
arXiv preprint
389"
REFERENCES,0.6846846846846847,"arXiv:2202.06749, 2022.
390"
REFERENCES,0.6861861861861862,"[23] Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep
391"
REFERENCES,0.6876876876876877,"representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018.
392"
REFERENCES,0.6891891891891891,"[24] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representa-
393"
REFERENCES,0.6906906906906907,"tions through noisy computation. IEEE transactions on pattern analysis and machine intelli-
394"
REFERENCES,0.6921921921921922,"gence, 40(12):2897–2905, 2018.
395"
REFERENCES,0.6936936936936937,"[25] Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
396"
REFERENCES,0.6951951951951952,"dan D. Tracey, and David D. Cox. On the information bottleneck theory of deep learning.
397"
REFERENCES,0.6966966966966966,"Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
398"
REFERENCES,0.6981981981981982,"[26] Bernhard C. Geiger. On information plane analyses of neural network classiﬁers– A review.
399"
REFERENCES,0.6996996996996997,"IEEE Transactions on Neural Networks and Learning Systems, 2021.
400"
REFERENCES,0.7012012012012012,"[27] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 2006.
401"
REFERENCES,0.7027027027027027,"ISBN 0471241954.
402"
REFERENCES,0.7042042042042042,"[28] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of
403"
REFERENCES,0.7057057057057057,"usable information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.
404"
REFERENCES,0.7072072072072072,"[29] Yann Dubois, Douwe Kiela, David J. Schwab, and Ramakrishna Vedantam. Learning optimal
405"
REFERENCES,0.7087087087087087,"representations with the decodable information bottleneck. Advances in Neural Information
406"
REFERENCES,0.7102102102102102,"Processing Systems, 33:18674–18690, 2020.
407"
REFERENCES,0.7117117117117117,"[30] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
408"
REFERENCES,0.7132132132132132,"predictive coding. arXiv preprint arXiv:1807.03748, 2018.
409"
REFERENCES,0.7147147147147147,"[31] Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress–self-supervised learn-
410"
REFERENCES,0.7162162162162162,"ing and information theory: A review. arXiv preprint arXiv:2304.09355, 2023.
411"
REFERENCES,0.7177177177177178,"[32] Guillermo Owen. Game theory. Emerald Group Publishing, 2013.
412"
REFERENCES,0.7192192192192193,"[33] Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights.
413"
REFERENCES,0.7207207207207207,"Games and Economic Behavior, 29(1-2):79–103, 1999.
414"
REFERENCES,0.7222222222222222,"[34] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and
415"
REFERENCES,0.7237237237237237,"Trends R⃝in Machine Learning, 4(2):107–194, 2012.
416"
REFERENCES,0.7252252252252253,"[35] Elad Hazan. Introduction to online convex optimization. Foundations and Trends R⃝in Opti-
417"
REFERENCES,0.7267267267267268,"mization, 2(3-4):157–325, 2016.
418"
REFERENCES,0.7282282282282282,"[36] Maurice Sion. On general minimax theorems. 1958.
419"
REFERENCES,0.7297297297297297,"[37] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university press, 2012.
420"
REFERENCES,0.7312312312312312,"[38] Dimitris Bertsimas and John N. Tsitsiklis. Introduction to linear optimization, volume 6.
421"
REFERENCES,0.7327327327327328,"Athena scientiﬁc Belmont, MA, 1997.
422"
REFERENCES,0.7342342342342343,"[39] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contrac-
423"
REFERENCES,0.7357357357357357,"tive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th
424"
REFERENCES,0.7372372372372372,"international conference on international conference on machine learning, pages 833–840,
425"
REFERENCES,0.7387387387387387,"2011.
426"
REFERENCES,0.7402402402402403,"[40] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in
427"
REFERENCES,0.7417417417417418,"neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence,
428"
REFERENCES,0.7432432432432432,"44(9):5149–5169, 2021.
429"
REFERENCES,0.7447447447447447,"[41] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
430"
REFERENCES,0.7462462462462462,"nifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:
431"
REFERENCES,0.7477477477477478,"151–175, 2010.
432"
REFERENCES,0.7492492492492493,"[42] Friedemann Zenke, Ben Poole, and Surya Ganguli.
Continual learning through synaptic
433"
REFERENCES,0.7507507507507507,"intelligence. In International conference on machine learning, pages 3987–3995. PMLR,
434"
REFERENCES,0.7522522522522522,"2017.
435"
REFERENCES,0.7537537537537538,"[43] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual
436"
REFERENCES,0.7552552552552553,"learning. arXiv preprint arXiv:1710.10628, 2017.
437"
REFERENCES,0.7567567567567568,"[44] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv
438"
REFERENCES,0.7582582582582582,"preprint arXiv:1904.07734, 2019.
439"
REFERENCES,0.7597597597597597,"[45] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars.
Task-free continual learning.
440"
REFERENCES,0.7612612612612613,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
441"
REFERENCES,0.7627627627627628,"pages 11254–11263, 2019.
442"
REFERENCES,0.7642642642642643,"[46] Stephen Boyd, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
443"
REFERENCES,0.7657657657657657,"university press, 2004.
444"
REFERENCES,0.7672672672672672,"[47] Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the infor-
445"
REFERENCES,0.7687687687687688,"mation bottleneck. Theoretical Computer Science, 411(29-30):2696–2711, 2010.
446"
REFERENCES,0.7702702702702703,"[48] XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence
447"
REFERENCES,0.7717717717717718,"functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on In-
448"
REFERENCES,0.7732732732732732,"formation Theory, 56(11):5847–5861, 2010.
449"
REFERENCES,0.7747747747747747,"[49] Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander A. Alemi, and George Tucker. On
450"
REFERENCES,0.7762762762762763,"variational lower bounds of mutual information. In NeurIPS Workshop on Bayesian Deep
451"
REFERENCES,0.7777777777777778,"Learning, 2018.
452"
REFERENCES,0.7792792792792793,"[50] Tailin Wu, Ian Fischer, Isaac L. Chuang, and Max Tegmark. Learnability for the information
453"
REFERENCES,0.7807807807807807,"bottleneck. In Uncertainty in Artiﬁcial Intelligence, pages 1050–1060. PMLR, 2020.
454"
REFERENCES,0.7822822822822822,"[51] David McAllester and Karl Stratos. Formal limitations on the measurement of mutual infor-
455"
REFERENCES,0.7837837837837838,"mation. In International Conference on Artiﬁcial Intelligence and Statistics, pages 875–884.
456"
REFERENCES,0.7852852852852853,"PMLR, 2020.
457"
REFERENCES,0.7867867867867868,"[52] Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational
458"
REFERENCES,0.7882882882882883,"information bottleneck. Advances in Neural Information Processing Systems, 29, 2016.
459"
REFERENCES,0.7897897897897898,"[53] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy.
Deep variational
460"
REFERENCES,0.7912912912912913,"information bottleneck. arXiv preprint arXiv:1612.00410, 2016.
461"
REFERENCES,0.7927927927927928,"[54] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio,
462"
REFERENCES,0.7942942942942943,"Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International
463"
REFERENCES,0.7957957957957958,"conference on machine learning, pages 531–540. PMLR, 2018.
464"
REFERENCES,0.7972972972972973,"[55] Behrooz Razeghi, Flavio P. Calmon, Deniz Gunduz, and Slava Voloshynovskiy. Bottlenecks
465"
REFERENCES,0.7987987987987988,"CLUB: Unifying information-theoretic trade-offs among complexity, leakage, and utility.
466"
REFERENCES,0.8003003003003003,"arXiv preprint arXiv:2207.04895, 2022.
467"
REFERENCES,0.8018018018018018,"[56] Matías Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of information complexity
468"
REFERENCES,0.8033033033033034,"and randomization in representation learning. arXiv preprint arXiv:1802.05355, 2018.
469"
REFERENCES,0.8048048048048048,"[57] Borja Rodriguez Galvez. The information bottleneck: Connections to other problems, learn-
470"
REFERENCES,0.8063063063063063,"ing and exploration of the ib curve, 2019.
471"
REFERENCES,0.8078078078078078,"[58] Peter L. Bartlett and Shahar Mendelson.
Rademacher and Gaussian complexities: Risk
472"
REFERENCES,0.8093093093093093,"bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
473"
REFERENCES,0.8108108108108109,"[59] Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
474"
REFERENCES,0.8123123123123123,"Cambridge University Press, 2019.
475"
REFERENCES,0.8138138138138138,"[60] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
476"
REFERENCES,0.8153153153153153,"algorithms. Cambridge university press, 2014.
477"
REFERENCES,0.8168168168168168,"[61] Karthik Sridharan and Sham M. Kakade. An information theoretic framework for multi-view
478"
REFERENCES,0.8183183183183184,"learning. 2008.
479"
REFERENCES,0.8198198198198198,"[62] Rana Ali Amjad and Bernhard C. Geiger. Learning representations for neural network-based
480"
REFERENCES,0.8213213213213213,"classiﬁcation using the information bottleneck principle. IEEE transactions on pattern anal-
481"
REFERENCES,0.8228228228228228,"ysis and machine intelligence, 42(9):2225–2239, 2019.
482"
REFERENCES,0.8243243243243243,"[63] Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear information bot-
483"
REFERENCES,0.8258258258258259,"tleneck. Entropy, 21(12):1181, 2019.
484"
REFERENCES,0.8273273273273273,"[64] D. J. Strouse and David J. Schwab. The information bottleneck and geometric clustering.
485"
REFERENCES,0.8288288288288288,"Neural computation, 31(3):596–612, 2019.
486"
REFERENCES,0.8303303303303303,"[65] Ankit Pensia, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a
487"
REFERENCES,0.8318318318318318,"robust information bottleneck. IEEE Journal on Selected Areas in Information Theory, 1(1):
488"
REFERENCES,0.8333333333333334,"131–144, 2020.
489"
REFERENCES,0.8348348348348348,"[66] Shahab Asoodeh and Flavio P Calmon. Bottleneck problems: An information and estimation-
490"
REFERENCES,0.8363363363363363,"theoretic view. Entropy, 22(11):1325, 2020.
491"
REFERENCES,0.8378378378378378,"[67] Vudtiwat Ngampruetikorn and David J. Schwab.
Perturbation theory for the information
492"
REFERENCES,0.8393393393393394,"bottleneck. Advances in Neural Information Processing Systems, 34:21008–21018, 2021.
493"
REFERENCES,0.8408408408408409,"[68] Xi Yu, Shujian Yu, and José C Príncipe. Deep deterministic information bottleneck with
494"
REFERENCES,0.8423423423423423,"matrix-based entropy functional. In ICASSP 2021-2021 IEEE International Conference on
495"
REFERENCES,0.8438438438438438,"Acoustics, Speech and Signal Processing (ICASSP), pages 3160–3164. IEEE, 2021.
496"
REFERENCES,0.8453453453453453,"[69] Vudtiwat Ngampruetikorn and David J. Schwab.
Information bottleneck theory of
497"
REFERENCES,0.8468468468468469,"high-dimensional regression:
Relevancy, efﬁciency and optimality.
arXiv preprint
498"
REFERENCES,0.8483483483483484,"arXiv:2208.03848, 2022.
499"
REFERENCES,0.8498498498498499,"[70] Deniz Gündüz, Zhijin Qin, Inaki Estella Aguerri, Harpreet S Dhillon, Zhaohui Yang, Aylin
500"
REFERENCES,0.8513513513513513,"Yener, Kai Kit Wong, and Chan-Byoung Chae. Beyond transmitting bits: Context, semantics,
501"
REFERENCES,0.8528528528528528,"and task-oriented communications. IEEE Journal on Selected Areas in Communications, 41
502"
REFERENCES,0.8543543543543544,"(1):5–41, 2022.
503"
REFERENCES,0.8558558558558559,"[71] Vudtiwat Ngampruetikorn and David J. Schwab.
Generalized information bottleneck for
504"
REFERENCES,0.8573573573573574,"Gaussian variables. arXiv preprint arXiv:2303.17762, 2023.
505"
REFERENCES,0.8588588588588588,"[72] Vudtiwat Ngampruetikorn, William Bialek, and David Schwab.
Information-bottleneck
506"
REFERENCES,0.8603603603603603,"renormalization group for self-supervised representation learning. Bulletin of the American
507"
REFERENCES,0.8618618618618619,"Physical Society, 65, 2020.
508"
REFERENCES,0.8633633633633634,"[73] William B. Johnson. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math.,
509"
REFERENCES,0.8648648648648649,"26:189–206, 1984.
510"
REFERENCES,0.8663663663663663,"[74] Santosh S. Vempala. The random projection method, volume 65. American Mathematical
511"
REFERENCES,0.8678678678678678,"Soc., 2005.
512"
REFERENCES,0.8693693693693694,"[75] Michael W. Mahoney et al. Randomized algorithms for matrices and data. Foundations and
513"
REFERENCES,0.8708708708708709,"Trends R⃝in Machine Learning, 3(2):123–224, 2011.
514"
REFERENCES,0.8723723723723724,"[76] David P. Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and
515"
REFERENCES,0.8738738738738738,"Trends R⃝in Theoretical Computer Science, 10(1–2):1–157, 2014.
516"
REFERENCES,0.8753753753753754,"[77] Fan Yang, Sifan Liu, Edgar Dobriban, and David P. Woodruff. How to reduce dimension with
517"
REFERENCES,0.8768768768768769,"PCA and random projections? IEEE Transactions on Information Theory, 67(12):8154–8189,
518"
REFERENCES,0.8783783783783784,"2021.
519"
REFERENCES,0.8798798798798799,"[78] John F. Nash Jr. Equilibrium points in n-person games. Proceedings of the national academy
520"
REFERENCES,0.8813813813813813,"of sciences, 36(1):48–49, 1950.
521"
REFERENCES,0.8828828828828829,"[79] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
522"
REFERENCES,0.8843843843843844,"equilibrium in generative adversarial nets (gans). In International Conference on Machine
523"
REFERENCES,0.8858858858858859,"Learning, pages 224–232. PMLR, 2017.
524"
REFERENCES,0.8873873873873874,"[80] Paulina Grnarova, Kﬁr Y. Levy, Aurelien Lucchi, Thomas Hofmann, and Andreas
525"
REFERENCES,0.8888888888888888,"Krause. An online learning approach to generative adversarial networks. arXiv preprint
526"
REFERENCES,0.8903903903903904,"arXiv:1706.03269, 2017.
527"
REFERENCES,0.8918918918918919,"[81] Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bern-
528"
REFERENCES,0.8933933933933934,"hard Schölkopf. Adagan: Boosting generative models. Advances in neural information pro-
529"
REFERENCES,0.8948948948948949,"cessing systems, 30, 2017.
530"
REFERENCES,0.8963963963963963,"[82] Max Welling, Richard Zemel, and Geoffrey E. Hinton. Self supervised boosting. Advances
531"
REFERENCES,0.8978978978978979,"in neural information processing systems, 15, 2002.
532"
REFERENCES,0.8993993993993994,"[83] Abraham Wald. Contributions to the theory of statistical estimation and testing hypotheses.
533"
REFERENCES,0.9009009009009009,"The Annals of Mathematical Statistics, 10(4):299–326, 1939.
534"
REFERENCES,0.9024024024024024,"[84] Larry Wasserman. All of statistics: A concise course in statistical inference, volume 26.
535"
REFERENCES,0.9039039039039038,"Springer, 2004.
536"
REFERENCES,0.9054054054054054,"[85] Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of
537"
REFERENCES,0.9069069069069069,"convergence. Annals of Statistics, pages 1564–1599, 1999.
538"
REFERENCES,0.9084084084084084,"[86] Peter D. Grünwald and A. Philip Dawid. Game theory, maximum entropy, minimum dis-
539"
REFERENCES,0.9099099099099099,"crepancy and robust bayesian decision theory. The Annals of Statistics, 32(4):1367–1433,
540"
REFERENCES,0.9114114114114115,"2004.
541"
REFERENCES,0.9129129129129129,"[87] David Haussler and Manfred Opper. Mutual information, metric entropy and cumulative
542"
REFERENCES,0.9144144144144144,"relative entropy risk. The Annals of Statistics, 25(6):2451–2492, 1997.
543"
REFERENCES,0.9159159159159159,"[88] Farzan Farnia and David Tse. A minimax approach to supervised learning. Advances in
544"
REFERENCES,0.9174174174174174,"Neural Information Processing Systems, 29, 2016.
545"
REFERENCES,0.918918918918919,"[89] Jorge Silva and Felipe Tobar. On the interplay between information loss and operation loss in
546"
REFERENCES,0.9204204204204204,"representations for classiﬁcation. In International Conference on Artiﬁcial Intelligence and
547"
REFERENCES,0.9219219219219219,"Statistics, pages 4853–4871. PMLR, 2022.
548"
REFERENCES,0.9234234234234234,"[90] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
549"
REFERENCES,0.924924924924925,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communica-
550"
REFERENCES,0.9264264264264265,"tions of the ACM, 63(11):139–144, 2020.
551"
REFERENCES,0.9279279279279279,"[91] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and
552"
REFERENCES,0.9294294294294294,"Anil A. Bharath. Generative adversarial networks: An overview. IEEE signal processing
553"
REFERENCES,0.9309309309309309,"magazine, 35(1):53–65, 2018.
554"
REFERENCES,0.9324324324324325,"[92] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
555"
REFERENCES,0.933933933933934,"Vladu.
Towards deep learning models resistant to adversarial attacks.
arXiv preprint
556"
REFERENCES,0.9354354354354354,"arXiv:1706.06083, 2017.
557"
REFERENCES,0.9369369369369369,"[93] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski.
Robust optimization, vol-
558"
REFERENCES,0.9384384384384384,"ume 28. Princeton university press, 2009.
559"
REFERENCES,0.93993993993994,"[94] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
560"
REFERENCES,0.9414414414414415,"Xi Chen. Improved techniques for training GANs. Advances in neural information pro-
561"
REFERENCES,0.9429429429429429,"cessing systems, 29, 2016.
562"
REFERENCES,0.9444444444444444,"[95] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret al-
563"
REFERENCES,0.9459459459459459,"gorithms for zero-sum games. In Proceedings of the twenty-second annual ACM-SIAM sym-
564"
REFERENCES,0.9474474474474475,"posium on Discrete Algorithms, pages 235–254. SIAM, 2011.
565"
REFERENCES,0.948948948948949,"[96] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable
566"
REFERENCES,0.9504504504504504,"sequences. Advances in Neural Information Processing Systems, 26, 2013.
567"
REFERENCES,0.9519519519519519,"[97] James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In
568"
REFERENCES,0.9534534534534534,"Proceedings of the 2018 ACM Conference on Economics and Computation, pages 321–338,
569"
REFERENCES,0.954954954954955,"2018.
570"
REFERENCES,0.9564564564564565,"[98] Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger B. Grosse. Near-optimal local
571"
REFERENCES,0.9579579579579579,"convergence of alternating gradient descent-ascent for minimax optimization. In Interna-
572"
REFERENCES,0.9594594594594594,"tional Conference on Artiﬁcial Intelligence and Statistics, pages 7659–7679. PMLR, 2022.
573"
REFERENCES,0.960960960960961,"[99] Florian Schäfer and Anima Anandkumar. Competitive gradient descent. Advances in Neural
574"
REFERENCES,0.9624624624624625,"Information Processing Systems, 32, 2019.
575"
REFERENCES,0.963963963963964,"[100] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. Advances
576"
REFERENCES,0.9654654654654654,"in neural information processing systems, 30, 2017.
577"
REFERENCES,0.9669669669669669,"[101] Alistair Letcher, David Balduzzi, Sébastien Racaniere, James Martens, Jakob Foerster, Karl
578"
REFERENCES,0.9684684684684685,"Tuyls, and Thore Graepel. Differentiable game mechanics. The Journal of Machine Learning
579"
REFERENCES,0.96996996996997,"Research, 20(1):3032–3071, 2019.
580"
REFERENCES,0.9714714714714715,"[102] Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rémi Le Priol, Gabriel
581"
REFERENCES,0.972972972972973,"Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved
582"
REFERENCES,0.9744744744744744,"game dynamics. In The 22nd International Conference on Artiﬁcial Intelligence and Statis-
583"
REFERENCES,0.975975975975976,"tics, pages 1802–1811. PMLR, 2019.
584"
REFERENCES,0.9774774774774775,"[103] Guodong Zhang and Yuanhao Wang. On the suboptimality of negative momentum for mini-
585"
REFERENCES,0.978978978978979,"max optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages
586"
REFERENCES,0.9804804804804805,"2098–2106. PMLR, 2021.
587"
REFERENCES,0.9819819819819819,"[104] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
588"
REFERENCES,0.9834834834834835,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
589"
REFERENCES,0.984984984984985,"tion processing systems, 30, 2017.
590"
REFERENCES,0.9864864864864865,"[105] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
Bert:
Pre-
591"
REFERENCES,0.987987987987988,"training of deep bidirectional transformers for language understanding.
arXiv preprint
592"
REFERENCES,0.9894894894894894,"arXiv:1810.04805, 2018.
593"
REFERENCES,0.990990990990991,"[106] Roman Vershynin. High-dimensional probability: An introduction with applications in data
594"
REFERENCES,0.9924924924924925,"science, volume 47. Cambridge university press, 2018.
595"
REFERENCES,0.993993993993994,"[107] Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. Convex analysis and optimization,
596"
REFERENCES,0.9954954954954955,"volume 1. Athena Scientiﬁc, 2003.
597"
REFERENCES,0.996996996996997,"[108] Ky Fan. On a theorem of Weyl concerning eigenvalues of linear transformations i. Proceed-
598"
REFERENCES,0.9984984984984985,"ings of the National Academy of Sciences, 35(11):652–655, 1949.
599"
