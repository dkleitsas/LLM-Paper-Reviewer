Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008417508417508417,"The remarkable success of Large Language Models (LLMs) across diverse tasks
1"
ABSTRACT,0.0016835016835016834,"has driven the research community to extend their capabilities to molecular appli-
2"
ABSTRACT,0.0025252525252525255,"cations, leading to the development of molecular LLMs. However, most molecular
3"
ABSTRACT,0.003367003367003367,"LLMs employ adapter-based architectures that do not treat molecule and text
4"
ABSTRACT,0.004208754208754209,"modalities equally and lack a supervision signal for the molecule modality. To
5"
ABSTRACT,0.005050505050505051,"address these issues, we introduce UniMoT, a unified molecule-text LLM adopting
6"
ABSTRACT,0.005892255892255892,"a tokenizer-based architecture that expands the vocabulary of LLM with molecule
7"
ABSTRACT,0.006734006734006734,"tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that
8"
ABSTRACT,0.007575757575757576,"incorporates a Q-Former to bridge the modality gap between molecule and text.
9"
ABSTRACT,0.008417508417508417,"This tokenizer transforms molecules into sequences of molecule tokens with causal
10"
ABSTRACT,0.009259259259259259,"dependency, encapsulating high-level molecular and textual information. Equipped
11"
ABSTRACT,0.010101010101010102,"with this tokenizer, UniMoT can unify molecule and text modalities under a shared
12"
ABSTRACT,0.010942760942760943,"token representation and an autoregressive training paradigm, enabling it to in-
13"
ABSTRACT,0.011784511784511785,"terpret molecules as a foreign language and generate them as text. Following a
14"
ABSTRACT,0.012626262626262626,"four-stage training scheme, UniMoT emerges as a multi-modal generalist capable
15"
ABSTRACT,0.013468013468013467,"of performing both molecule-to-text and text-to-molecule tasks. Extensive exper-
16"
ABSTRACT,0.01430976430976431,"iments demonstrate that UniMoT achieves state-of-the-art performance across a
17"
ABSTRACT,0.015151515151515152,"wide range of molecule comprehension and generation tasks.
18"
INTRODUCTION,0.015993265993265993,"1
Introduction
19"
INTRODUCTION,0.016835016835016835,"The incredible capabilities of Large Language Models (LLMs) [5, 44] have led to their widespread
20"
INTRODUCTION,0.017676767676767676,"use as versatile tools for completing diverse real-world tasks. This success has sparked interest in
21"
INTRODUCTION,0.018518518518518517,"Multi-modal LLMs [59, 52], which aim to enhance LLMs by enabling them to process multi-modal
22"
INTRODUCTION,0.01936026936026936,"inputs and outputs. Prior research efforts [26, 41, 12, 6, 33, 35, 25] have focused on adapting LLMs
23"
INTRODUCTION,0.020202020202020204,"to molecular tasks, resulting in the development of molecular LLMs. These molecular LLMs can
24"
INTRODUCTION,0.021043771043771045,"analyze molecule structures [35, 33, 6], address drug-related inquiries [26, 41], assist in synthesis
25"
INTRODUCTION,0.021885521885521887,"and retrosynthesis planning [12], support drug design [12], and more.
26"
INTRODUCTION,0.022727272727272728,"Prevalent molecular LLMs commonly employ adapter-based architectures, adopting either a linear
27"
INTRODUCTION,0.02356902356902357,"projection [26, 41, 6] or a Q-Former [33, 25] as an adapter to translate molecule features into the
28"
INTRODUCTION,0.02441077441077441,"semantic space of LLM, as illustrated in Figure 1a and Figure 1b. Despite demonstrating initial
29"
INTRODUCTION,0.025252525252525252,"capabilities in molecular comprehension and yielding promising results in molecule-to-text generation
30"
INTRODUCTION,0.026094276094276093,"tasks, they still lack molecule generation abilities. The critical issue within these methods is their
31"
INTRODUCTION,0.026936026936026935,"unequal treatment of molecules and text, resulting in a lack of supervision for the molecule modality.
32"
INTRODUCTION,0.027777777777777776,"This limitation significantly constrains model capacity and effectiveness. Due to limitations imposed
33"
INTRODUCTION,0.02861952861952862,"by the training paradigm, they are unable to perform text-to-molecule generation tasks.
34"
INTRODUCTION,0.029461279461279462,"Discretizing continuous molecule features into discrete molecule tokens offers a promising solution
35"
INTRODUCTION,0.030303030303030304,"for conducting both molecule-to-text and text-to-molecule generation tasks. By treating tokens from
36"
INTRODUCTION,0.031144781144781145,Large Language Model S
INTRODUCTION,0.03198653198653199,Text Tokenizer
INTRODUCTION,0.03282828282828283,Molecule Features
INTRODUCTION,0.03367003367003367,Projection Layer Text Text /S
INTRODUCTION,0.034511784511784514,(a) Projection-Based Architecture.
INTRODUCTION,0.03535353535353535,Large Language Model S
INTRODUCTION,0.0361952861952862,Text Tokenizer
INTRODUCTION,0.037037037037037035,Molecule Features
INTRODUCTION,0.03787878787878788,Q-Former Text Text /S
INTRODUCTION,0.03872053872053872,"Molecule
Embedding"
INTRODUCTION,0.03956228956228956,"Text
Token"
INTRODUCTION,0.04040404040404041,Molecule Token
INTRODUCTION,0.041245791245791245,(b) Q-Former-Based Architecture.
INTRODUCTION,0.04208754208754209,Large Language Model S
INTRODUCTION,0.04292929292929293,Text Tokenizer
INTRODUCTION,0.04377104377104377,Molecule
INTRODUCTION,0.04461279461279461,Molecule Tokenizer Text
INTRODUCTION,0.045454545454545456,"Decode into Molecule
Text /S"
INTRODUCTION,0.046296296296296294,(c) Tokenizer-Based Architecture.
INTRODUCTION,0.04713804713804714,"Figure 1: Comparisons among different molecular LLMs. 1a and 1b are adapter-based architectures
that do not treat molecule and text modalities equally and lack a supervision signal for the molecule
modality. 1c is our proposed tokenizer-based architecture, where molecules are presented in the same
discrete token representation as that of text. Molecules and text can be optimized under a unified
next-token-prediction objective."
INTRODUCTION,0.047979797979797977,"different modalities equally, we can predict the next molecule or text token in an autoregressive
37"
INTRODUCTION,0.04882154882154882,"manner. However, directly discretizing molecule features poses several challenges: (i) This approach
38"
INTRODUCTION,0.049663299663299666,"results in long sequences, with lengths equivalent to the number of atoms in a batch. LLMs typically
39"
INTRODUCTION,0.050505050505050504,"experience a quadratic increase in computational complexity with sequence length [46]. (ii) Molecule
40"
INTRODUCTION,0.05134680134680135,"tokens derived from molecule features lack left-to-right causal dependency, which conflicts with
41"
INTRODUCTION,0.05218855218855219,"the unidirectional attention mechanism in LLMs. (iii) Molecule features lack textual information,
42"
INTRODUCTION,0.05303030303030303,"hindering effective molecule-text interactions and alignment.
43"
INTRODUCTION,0.05387205387205387,"To this end, we present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architec-
44"
INTRODUCTION,0.054713804713804715,"ture, integrating molecule comprehension and generation, as depicted in Figure 1c. A pivotal aspect
45"
INTRODUCTION,0.05555555555555555,"of UniMoT’s architecture is the molecule tokenizer for transforming molecules into molecule tokens.
46"
INTRODUCTION,0.0563973063973064,"We introduce a Vector Quantization-driven [45] tokenizer, incorporating a Q-Former [23] to bridge
47"
INTRODUCTION,0.05723905723905724,"the modality gap between molecule and text. Specifically, we incorporate causal masks for the queries,
48"
INTRODUCTION,0.05808080808080808,"enabling the Causal Q-Former to generate a causal sequence of query embeddings compatible with
49"
INTRODUCTION,0.058922558922558925,"the unidirectional attention in LLMs. The sequence of query embeddings is subsequently quantized
50"
INTRODUCTION,0.05976430976430976,"into a sequence of molecule tokens using a learnable codebook. The molecule tokens encapsulate
51"
INTRODUCTION,0.06060606060606061,"high-level molecular and textual information, which are then aligned with the latent space of a
52"
INTRODUCTION,0.061447811447811446,"generative model via an MLP adapter, enabling the generation of desired molecules.
53"
INTRODUCTION,0.06228956228956229,"Pretrained LLMs can integrate the molecule tokenizer by treating molecule tokens as new words and
54"
INTRODUCTION,0.06313131313131314,"constructing a molecule vocabulary through mapping the learned codebook. We adopt the unified
55"
INTRODUCTION,0.06397306397306397,"discrete token representation for molecules and text, coupled with the unified next-token-prediction
56"
INTRODUCTION,0.06481481481481481,"training paradigm of LLM. This unification of representation and training paradigm enhances LLMs’
57"
INTRODUCTION,0.06565656565656566,"ability to understand molecule-text interactions and alignment. UniMoT interprets molecules akin to
58"
INTRODUCTION,0.0664983164983165,"understanding a foreign language, and generates them as if they were text. Following a four-stage
59"
INTRODUCTION,0.06734006734006734,"training scheme, UniMoT serves as a multi-modal generalist capable of performing both molecule
60"
INTRODUCTION,0.06818181818181818,"comprehension and generation tasks.
61"
INTRODUCTION,0.06902356902356903,"Our contributions can be summarized as follows:
62"
INTRODUCTION,0.06986531986531987,"• We introduce a molecule tokenizer specifically designed for LLMs, enabling the tokenization
63"
INTRODUCTION,0.0707070707070707,"of molecules into short sequences of molecule tokens with causal dependency. These tokens
64"
INTRODUCTION,0.07154882154882154,"encapsulate high-level molecular and textual information and can be decoded into desired
65"
INTRODUCTION,0.0723905723905724,"molecules during inference.
66"
INTRODUCTION,0.07323232323232323,"• We present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architecture
67"
INTRODUCTION,0.07407407407407407,"instead of traditional adapter-based architectures. UniMoT unifies the modalities of molecule
68"
INTRODUCTION,0.07491582491582492,"and text under a shared token representation and an autoregressive training paradigm.
69"
INTRODUCTION,0.07575757575757576,"• UniMoT exhibits remarkable capabilities in multi-modal comprehension and generation. Exten-
70"
INTRODUCTION,0.0765993265993266,"sive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide
71"
INTRODUCTION,0.07744107744107744,"spectrum of molecule comprehension tasks and molecule generation tasks.
72"
RELATED WORKS,0.07828282828282829,"2
Related Works
73"
RELATED WORKS,0.07912457912457913,"Molecular Large Language Models.
The recent emergence of Vision Large Language Models
74"
RELATED WORKS,0.07996632996632996,"(VLLMs) [24, 23, 28] has catalyzed advancements in Molecular LLMs, which encompass both
75"
RELATED WORKS,0.08080808080808081,"single modality and multi-modality approaches. In the single modality domain, researchers are
76"
RELATED WORKS,0.08164983164983165,"exploring diverse molecule representations, such as 1D sequences like SMILES strings [47, 8, 17],
77"
RELATED WORKS,0.08249158249158249,"2D molecule graphs [15, 56], 3D geometric conformations [56, 32], and textual information from
78"
RELATED WORKS,0.08333333333333333,"the literature [43, 2, 21]. In the multiple modalities domain, various innovative approaches are being
79"
RELATED WORKS,0.08417508417508418,"employed. MolT5 [11], a T5-based [38] model, is designed for SMILES-to-text and text-to-SMILES
80"
RELATED WORKS,0.08501683501683502,"translations. Other works, such as MoMu [39], MoleculeSTM [31], MolFM [34], and GIT-Mol [29],
81"
RELATED WORKS,0.08585858585858586,"leverage cross-modal contrastive learning to align the representation spaces of molecules and text.
82"
RELATED WORKS,0.0867003367003367,"Additionally, some studies use multi-modal learning architectures to develop molecular LLMs,
83"
RELATED WORKS,0.08754208754208755,"which often adopt adapter-based architectures. For instance, InstructMol [6], GraphGPT [41], and
84"
RELATED WORKS,0.08838383838383838,"DrugChat [26] employ a simple projection layer to map molecule features to LLM’s input space.
85"
RELATED WORKS,0.08922558922558922,"MolCA [33] and 3D-MoLM [25] utilize a Q-Former [23] to bridge the modality gap between
86"
RELATED WORKS,0.09006734006734007,"molecules and text. However, these methods do not treat molecule and text modalities equally and
87"
RELATED WORKS,0.09090909090909091,"lack a supervision signal for the molecule modality, limiting model capacity and effectiveness.
88"
RELATED WORKS,0.09175084175084175,"Vector Quantization.
Vector Quantization (VQ) [13] is a widely used technique in generative
89"
RELATED WORKS,0.09259259259259259,"models. VQ-VAE [45] converts an image into a set of discrete codes within a learnable discrete
90"
RELATED WORKS,0.09343434343434344,"latent space by learning to reconstruct the original image. VQ-GAN [57] enhances the generation
91"
RELATED WORKS,0.09427609427609428,"quality by leveraging adversarial and perceptual objectives. In the context of molecules, VQ has
92"
RELATED WORKS,0.09511784511784512,"been effectively applied to quantize molecule representations. For example, DGAE [4] introduces
93"
RELATED WORKS,0.09595959595959595,"a VQ model specifically for molecular graphs, where molecular graphs are encoded into discrete
94"
RELATED WORKS,0.0968013468013468,"latent codes. Mole-BERT [54] uses VQ to rethink the pre-training of GNNs for molecular tasks.
95"
RELATED WORKS,0.09764309764309764,"IMoLD [60] proposes using VQ to enhance invariant molecule representations, and VQSynergy [51]
96"
RELATED WORKS,0.09848484848484848,"demonstrates the use of VQ for drug discovery.
97"
METHOD,0.09932659932659933,"3
Method
98"
METHOD,0.10016835016835017,"Our objective is to leverage the reasoning and generation capabilities of LLMs to enhance the
99"
METHOD,0.10101010101010101,"comprehension and generation of molecule and text data. To achieve this, we focus on representing
100"
METHOD,0.10185185185185185,"these modalities uniformly within the token representation, utilizing the next-token-prediction training
101"
METHOD,0.1026936026936027,"paradigm of LLMs. As illustrated in Figure 2, we introduce a molecule tokenizer (Section 3.1)
102"
METHOD,0.10353535353535354,"designed to transform molecules into molecule tokens by learning to reconstruct the input molecule.
103"
METHOD,0.10437710437710437,"The molecule sequence can then be concatenated with the text sequence to form a multi-modal
104"
METHOD,0.10521885521885523,"sequence, which is subsequently fed into an LLM for autoregressive pretraining (Section 3.2), as
105"
METHOD,0.10606060606060606,"illustrated in Figure 3. The LLM vocabulary is expanded with molecule codes mapped from the
106"
METHOD,0.1069023569023569,"learned codebook. We introduce a four-stage training scheme for UniMoT (Section 3.3) comprising
107"
METHOD,0.10774410774410774,"Causal Q-Former pretraining, molecule tokenizer pretraining, unified molecule-text pretraining, and
108"
METHOD,0.10858585858585859,"task-specific instruction tuning. UniMoT is capable of performing both molecular comprehension
109"
METHOD,0.10942760942760943,"and generation tasks following the training scheme.
110"
MOLECULE TOKENIZER FOR LLMS,0.11026936026936027,"3.1
Molecule Tokenizer for LLMs
111"
MOLECULE TOKENIZER FOR LLMS,0.1111111111111111,"Molecule encoder.
We represent the structural information of a molecule as a graph, denoted by
112"
MOLECULE TOKENIZER FOR LLMS,0.11195286195286196,"G = (V, E), where V is the set of atoms and |V| = N is the number of atoms. The task of the
113"
MOLECULE TOKENIZER FOR LLMS,0.1127946127946128,"molecule encoder is to extract node representations that are context-aware and encompass diverse
114"
MOLECULE TOKENIZER FOR LLMS,0.11363636363636363,"local neighborhood structural information. By employing a molecule encoder, we obtain molecule
115"
MOLECULE TOKENIZER FOR LLMS,0.11447811447811448,"features X ∈RN×F , where each atom representation contains context-aware structural information.
116"
MOLECULE TOKENIZER FOR LLMS,0.11531986531986532,"Causal Q-Former.
We employ a Q-Former model introduced by BLIP-2 [23] to generate query
117"
MOLECULE TOKENIZER FOR LLMS,0.11616161616161616,"embeddings Z = {zi}M
i=1 ∈RM×d containing high-level molecular and textual information, where
118"
MOLECULE TOKENIZER FOR LLMS,0.117003367003367,"M represents the number of queries and d denotes the dimension of query embeddings. Specifically,
119"
MOLECULE TOKENIZER FOR LLMS,0.11784511784511785,"we incorporate causal masks into the queries, ensuring that they only interact with preceding queries.
120"
MOLECULE TOKENIZER FOR LLMS,0.11868686868686869,"This ensures the sequence of query embeddings maintains a causal dependency, aligning with the
121"
MOLECULE TOKENIZER FOR LLMS,0.11952861952861953,"requirements of LLMs operating on text sequence. Details regarding the Causal Q-Former can be
122"
MOLECULE TOKENIZER FOR LLMS,0.12037037037037036,"found in Appendix A.
123"
MOLECULE TOKENIZER FOR LLMS,0.12121212121212122,Molecule
MOLECULE TOKENIZER FOR LLMS,0.12205387205387205,"Encoder
Causal Q-Former"
MOLECULE TOKENIZER FOR LLMS,0.12289562289562289,"“The molecule is an 
indole phytoalexin 
that …”"
MOLECULE TOKENIZER FOR LLMS,0.12373737373737374,"1
3
7
2"
MOLECULE TOKENIZER FOR LLMS,0.12457912457912458,Learnable
MOLECULE TOKENIZER FOR LLMS,0.12542087542087543,Queries
MOLECULE TOKENIZER FOR LLMS,0.12626262626262627,Causal Query
MOLECULE TOKENIZER FOR LLMS,0.1271043771043771,Embeddings
MOLECULE TOKENIZER FOR LLMS,0.12794612794612795,Learnable
MOLECULE TOKENIZER FOR LLMS,0.12878787878787878,"Molecule
Codebook"
MOLECULE TOKENIZER FOR LLMS,0.12962962962962962,Molecule
MOLECULE TOKENIZER FOR LLMS,0.13047138047138046,Tokens
MOLECULE TOKENIZER FOR LLMS,0.13131313131313133,"Aligning
(MSE Loss)"
MOLECULE TOKENIZER FOR LLMS,0.13215488215488216,"Input
Molecule"
MOLECULE TOKENIZER FOR LLMS,0.132996632996633,"Straight-
Through 
Gradients"
MOLECULE TOKENIZER FOR LLMS,0.13383838383838384,"Adapter
SMILES
Decoder"
MOLECULE TOKENIZER FOR LLMS,0.13468013468013468,Reconstructed
MOLECULE TOKENIZER FOR LLMS,0.13552188552188552,Molecule
MOLECULE TOKENIZER FOR LLMS,0.13636363636363635,Molecule
MOLECULE TOKENIZER FOR LLMS,0.13720538720538722,Caption
MOLECULE TOKENIZER FOR LLMS,0.13804713804713806,"SMILES
Encoder"
MOLECULE TOKENIZER FOR LLMS,0.1388888888888889,"C1=CC=C2C(=C1)C
(=CN2)C3=NC=CS3"
MOLECULE TOKENIZER FOR LLMS,0.13973063973063973,Molecule
MOLECULE TOKENIZER FOR LLMS,0.14057239057239057,SMILES
MOLECULE TOKENIZER FOR LLMS,0.1414141414141414,Feed into LLM
MOLECULE TOKENIZER FOR LLMS,0.14225589225589225,Generated Text
MOLECULE TOKENIZER FOR LLMS,0.14309764309764308,"Trainable
Parameters"
MOLECULE TOKENIZER FOR LLMS,0.14393939393939395,"Frozen
Parameters"
MOLECULE TOKENIZER FOR LLMS,0.1447811447811448,"Figure 2: Illustration of our proposed molecule tokenizer. The tokenizer generates discrete molecule
tokens, which can be fed into LLMs for downstream tasks. The generated molecule tokens can be
decoded into molecules using the adapter and the SMILES decoder during inference."
MOLECULE TOKENIZER FOR LLMS,0.14562289562289563,"Vector Quantization.
The Causal Q-Former converts molecule and text features into a causal
124"
MOLECULE TOKENIZER FOR LLMS,0.14646464646464646,"sequence of query embeddings. Subsequently, we aim to quantize these query embeddings into
125"
MOLECULE TOKENIZER FOR LLMS,0.1473063973063973,"molecule tokens using a variant of VQ-VAE [45]. These discrete molecule tokens can then be
126"
MOLECULE TOKENIZER FOR LLMS,0.14814814814814814,"integrated with text tokens to form a multi-modal sequence suitable for feeding into LLMs. The
127"
MOLECULE TOKENIZER FOR LLMS,0.14898989898989898,"causal sequence of query embeddings {zi}M
i=1 is quantized into a causal sequence of molecule
128"
MOLECULE TOKENIZER FOR LLMS,0.14983164983164984,"tokens {si}M
i=1 by identifying the closest neighbor in a learnable codebook C = {ci}K
i=1, where K
129"
MOLECULE TOKENIZER FOR LLMS,0.15067340067340068,"represents the size of the codebook. The codebook is randomly initialized and optimized during
130"
MOLECULE TOKENIZER FOR LLMS,0.15151515151515152,"pretraining. Specifically, token si is determined as follows:
131"
MOLECULE TOKENIZER FOR LLMS,0.15235690235690236,"si = argminj∈{1,··· ,K} ∥zi −cj∥2 ,
for
i = 1, 2, · · · , M.
(1)"
MOLECULE TOKENIZER FOR LLMS,0.1531986531986532,"Intuitively, the query embedding zi is quantized to the closest neighbor csi in the codebook. As the
132"
MOLECULE TOKENIZER FOR LLMS,0.15404040404040403,"vector quantization process is non-differentiable, we adopt the straight-through estimator [3] to train
133"
MOLECULE TOKENIZER FOR LLMS,0.15488215488215487,"the Causal Q-Former by copying the gradient from the molecule tokens to the query embeddings,
134"
MOLECULE TOKENIZER FOR LLMS,0.15572390572390574,"as shown in Figure 2. The resulting embeddings of molecule tokens, denoted as C = {csi}M
i=1, are
135"
MOLECULE TOKENIZER FOR LLMS,0.15656565656565657,"subsequently utilized for reconstructing molecules.
136"
MOLECULE TOKENIZER FOR LLMS,0.1574074074074074,"Molecule Reconstruction.
An adapter needs to be trained to align the discrete latent space of
137"
MOLECULE TOKENIZER FOR LLMS,0.15824915824915825,"molecule tokens with the continuous latent space of a molecular generative model for molecule
138"
MOLECULE TOKENIZER FOR LLMS,0.1590909090909091,"reconstruction. The embeddings of molecule tokens C can be aligned with the latent space of
139"
MOLECULE TOKENIZER FOR LLMS,0.15993265993265993,"the generative model via an MLP adapter ψ, represented as XR = ψ(C), where XR denotes the
140"
MOLECULE TOKENIZER FOR LLMS,0.16077441077441076,"embeddings for reconstruction. Subsequently, we can reconstruct the molecule from XR using the
141"
MOLECULE TOKENIZER FOR LLMS,0.16161616161616163,"pretrained SMILES decoder To achieve alignment, we minimize the Mean Squared Error (MSE) loss
142"
MOLECULE TOKENIZER FOR LLMS,0.16245791245791247,"between XR and the SMILES [50] embeddings XS produced by the pretrained SMILES encoder.
143"
MOLECULE TOKENIZER FOR LLMS,0.1632996632996633,"The training loss of the tokenizer is expressed as follows:
144"
MOLECULE TOKENIZER FOR LLMS,0.16414141414141414,"LTokenizer = ∥XR −XS∥2
2 + 1 M M
X"
MOLECULE TOKENIZER FOR LLMS,0.16498316498316498,"i=1
∥sg [zi] −csi∥2
2 + β M M
X"
MOLECULE TOKENIZER FOR LLMS,0.16582491582491582,"i=1
∥sg [csi] −zi∥2
2 .
(2)"
MOLECULE TOKENIZER FOR LLMS,0.16666666666666666,"Here, the first term represents the alignment loss, the second term is a codebook loss aimed at
145"
MOLECULE TOKENIZER FOR LLMS,0.1675084175084175,"updating the codebook embeddings, and the third term is a commitment loss that encourages the
146"
MOLECULE TOKENIZER FOR LLMS,0.16835016835016836,"query embedding to stay close to the chosen codebook embedding. sg[·] denotes the stop-gradient
147"
MOLECULE TOKENIZER FOR LLMS,0.1691919191919192,"operator, and the hyperparameter β is set to 0.25.
148"
MOLECULE TOKENIZER FOR LLMS,0.17003367003367004,Unified Molecule-Text LLM (UniMoT)
MOLECULE TOKENIZER FOR LLMS,0.17087542087542087,"MOL T+3
/MOL
SMI
24
/SMI"
MOLECULE TOKENIZER FOR LLMS,0.1717171717171717,Text Tokenizer
MOLECULE TOKENIZER FOR LLMS,0.17255892255892255,Molecule
MOLECULE TOKENIZER FOR LLMS,0.1734006734006734,"C1=CC=C2C(
=C1)C(=CN2)
C3=NC=CS3"
MOLECULE TOKENIZER FOR LLMS,0.17424242424242425,The molecule is
MOLECULE TOKENIZER FOR LLMS,0.1750841750841751,SMILES 3
MOLECULE TOKENIZER FOR LLMS,0.17592592592592593,an indole phytoalexin that …
MOLECULE TOKENIZER FOR LLMS,0.17676767676767677,Prompt
MOLECULE TOKENIZER FOR LLMS,0.1776094276094276,"BOS
37"
MOLECULE TOKENIZER FOR LLMS,0.17845117845117844,Molecule Tokenizer
MOLECULE TOKENIZER FOR LLMS,0.17929292929292928,"16
54
41
T+7"
MOLECULE TOKENIZER FOR LLMS,0.18013468013468015,"T+3
/MOL
SMI
24
/SMI
3
BOS
EOS
16
54
41
T+7
37"
MOLECULE TOKENIZER FOR LLMS,0.18097643097643099,(a) Molecule-to-Text Autoregression.
MOLECULE TOKENIZER FOR LLMS,0.18181818181818182,Adapter
MOLECULE TOKENIZER FOR LLMS,0.18265993265993266,"MOL T+3
BOS
24
43
EOS
16"
MOLECULE TOKENIZER FOR LLMS,0.1835016835016835,Unified Molecule-Text LLM (UniMoT)
MOLECULE TOKENIZER FOR LLMS,0.18434343434343434,"27
T+7"
MOLECULE TOKENIZER FOR LLMS,0.18518518518518517,Text Tokenizer
MOLECULE TOKENIZER FOR LLMS,0.18602693602693604,"Generate a 
molecule with 
the following 
properties."
MOLECULE TOKENIZER FOR LLMS,0.18686868686868688,Prompt
MOLECULE TOKENIZER FOR LLMS,0.18771043771043772,"The molecule 
is an indole 
phytoalexin 
that …"
MOLECULE TOKENIZER FOR LLMS,0.18855218855218855,Caption
MOLECULE TOKENIZER FOR LLMS,0.1893939393939394,Molecule
MOLECULE TOKENIZER FOR LLMS,0.19023569023569023,Decoder
MOLECULE TOKENIZER FOR LLMS,0.19107744107744107,Generated
MOLECULE TOKENIZER FOR LLMS,0.1919191919191919,Molecule
MOLECULE TOKENIZER FOR LLMS,0.19276094276094277,"During
Inference"
MOLECULE TOKENIZER FOR LLMS,0.1936026936026936,"MOL T+3
24
43
EOS
16
27
/MOL
T+7"
MOLECULE TOKENIZER FOR LLMS,0.19444444444444445,(b) Text-to-Molecule Autoregression.
MOLECULE TOKENIZER FOR LLMS,0.19528619528619529,"Figure 3: Illustration of the multi-modal autoregressive pretraining on molecule-text datasets. Uni-
MoT excels in multi-modal comprehension and generation tasks, enabled by the unified LM objective.
T represents the size of the text vocabulary."
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.19612794612794612,"3.2
Unified Molecule-Text Language Model
149"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.19696969696969696,"Expanding Vocabulary.
Employing the molecule tokenizer, a molecule can be tokenized into a
150"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.1978114478114478,"molecule sequence {si}M
i=1 with causal dependency. The molecule sequence can be concatenated with
151"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.19865319865319866,"the text sequence to form a multi-modal sequence {ui}L
i=1, where L is the length of the multi-modal
152"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.1994949494949495,"sequence. To facilitate the representation of the multi-modal sequence, we construct the molecule
153"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20033670033670034,"vocabulary Vm = {vm
i }K
i=1, which maintains the order of the molecule codebook C = {ci}K
i=1.
154"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20117845117845118,"Additionally, Vm includes several special tokens such as boundary indicators, e.g., [MOL] and
155"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20202020202020202,"[/MOL], to mark the beginning and end of the molecule sequence. Next, we merge the original text
156"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20286195286195285,"vocabulary Vt = {vt
i}T
i=1 with the molecule vocabulary Vm. The unified molecule-text vocabulary
157"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2037037037037037,"V = {Vm, Vt} facilitates joint learning from molecules and text under a unified next-token-prediction
158"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20454545454545456,"objective. As the vocabulary is expanded, the corresponding embeddings and prediction layers also
159"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2053872053872054,"need to be extended, with the newly introduced parameters initialized randomly.
160"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20622895622895623,"Unified Molecule-text Modeling.
The multi-modal sequence {ui}L
i=1 is fed into the pretrained
161"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20707070707070707,"LLM for performing multi-modal autoregression. UniMoT adopts the general Language Modeling
162"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2079124579124579,"(LM) objective to directly maximize the log-likelihood of the data distribution:
163"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20875420875420875,"LLM = −
X u∈D X"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.20959595959595959,"i∈I
log p (ui | u1, · · · , ui−1; θ) ,
(3)"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21043771043771045,"where D represents the dataset, I represents the set of indices of the generation target, and θ denotes
164"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2112794612794613,"the parameters of the LLM. The unification of representation and training paradigm for molecules and
165"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21212121212121213,"text enhances the abilities of LLMs to understand molecule-text interactions and alignment. UniMoT
166"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21296296296296297,"can interpret molecules similar to understanding a foreign language, and generate them as if they
167"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2138047138047138,"were text. We conduct autoregressive pretraining on molecule-to-text and text-to-molecule tasks to
168"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21464646464646464,"enhance the molecule comprehension and generation capabilities.
169"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21548821548821548,"Molecule-to-Text Autoregression.
While structural information is embedded in molecule features
170"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21632996632996632,"and captured by the molecule tokens through the tokenizer, we also aim to incorporate sequential
171"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21717171717171718,"information of molecules for better comprehension. Therefore, we concatenate the molecule sequence
172"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21801346801346802,"{si}M
i=1 with the SMILES [50] sequence and a prompt to form the multi-modal input sequence
173"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.21885521885521886,"{ui}L
i=1, as illustrated in Figure 3a. The text sequence of the corresponding molecule caption is used
174"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2196969696969697,"as the generation target.
175"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.22053872053872053,"Text-to-Molecule Autoregression.
For molecule generation, a prompt and the molecule caption
176"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.22138047138047137,"are concatenated, with a [MOL] token appended to signify the beginning of the molecule sequence,
177"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2222222222222222,"as illustrated in Figure 3b. The molecule sequence {si}M
i=1 produced by the tokenizer is used as the
178"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.22306397306397308,"generation target. During inference, given a prompt and the molecule caption, the output molecule
179"
UNIFIED MOLECULE-TEXT LANGUAGE MODEL,0.2239057239057239,"sequence can be decoded into the desired molecule by the pretrained adapter and SMILES decoder.
180"
TRAINING STRATEGY,0.22474747474747475,"3.3
Training Strategy
181"
TRAINING STRATEGY,0.2255892255892256,"The training strategy for UniMoT is structured across four stages. Stage-1 focuses on Causal Q-
182"
TRAINING STRATEGY,0.22643097643097643,"Former pretraining with tailored objectives. In Stage-2, the molecule tokenizer is optimized using the
183"
TRAINING STRATEGY,0.22727272727272727,"frozen encoders and decoder. Stage-3 integrates the tokenizer with a language model for multi-modal
184"
TRAINING STRATEGY,0.2281144781144781,"comprehension and generation. Finally, Stage-4 fine-tunes UniMoT for specific tasks, aligning it with
185"
TRAINING STRATEGY,0.22895622895622897,"human instructions and optimizing performance for various molecular applications. More details
186"
TRAINING STRATEGY,0.2297979797979798,"regarding the training process can be found in Appendix C.
187"
TRAINING STRATEGY,0.23063973063973064,"Stage-1: Causal Q-Former Pretraining.
We connect the molecule encoder and Causal Q-Former,
188"
TRAINING STRATEGY,0.23148148148148148,"leveraging the pretrained MoleculeSTM molecule encoder [31]. The molecule encoder remains
189"
TRAINING STRATEGY,0.23232323232323232,"frozen while only the Causal Q-Former is updated. Both queries and text inputs are used, while
190"
TRAINING STRATEGY,0.23316498316498316,"only queries serve as input in subsequent stages. In our experiments, we utilize 16 queries. We
191"
TRAINING STRATEGY,0.234006734006734,"employ three tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former,
192"
TRAINING STRATEGY,0.23484848484848486,"as detailed in Appendix A.
193"
TRAINING STRATEGY,0.2356902356902357,"Stage-2: Molecule Tokenizer Pretraining.
We connect the Causal Q-Former with subsequent
194"
TRAINING STRATEGY,0.23653198653198654,"blocks and use the objective defined in Equation (2). We employ the pretrained ChemFormer [17] as
195"
TRAINING STRATEGY,0.23737373737373738,"the generative model. Specifically, we leverage the SMILES encoder and SMILES decoder provided
196"
TRAINING STRATEGY,0.2382154882154882,"by ChemFormer. The molecule codebook size is set to K = 2048. As shown in Figure 2, we keep
197"
TRAINING STRATEGY,0.23905723905723905,"the molecule encoder, SMILES encoder, and SMILES decoder frozen, while updating the Causal
198"
TRAINING STRATEGY,0.2398989898989899,"Q-Former, codebook, and adapter.
199"
TRAINING STRATEGY,0.24074074074074073,"Stage-3: Unified Molecule-Text Pretraining.
We integrate the molecule tokenizer with the LLM
200"
TRAINING STRATEGY,0.2415824915824916,"using the unified vocabulary of molecule tokens and text tokens. We employ the LM objective
201"
TRAINING STRATEGY,0.24242424242424243,"defined in Equation (3) to pretrain the LLM. Pretraining involves molecule-to-text autoregression
202"
TRAINING STRATEGY,0.24326599326599327,"and text-to-molecule autoregression, aimed at enhancing UniMoT’s multi-modal comprehension and
203"
TRAINING STRATEGY,0.2441077441077441,"generation capabilities. To enhance efficiency, we train the LLM using LoRA tuning [14].
204"
TRAINING STRATEGY,0.24494949494949494,"Stage-4: Task-Specific Instruction Tuning.
UniMoT is fine-tuned on seven comprehension and
205"
TRAINING STRATEGY,0.24579124579124578,"generation tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-
206"
TRAINING STRATEGY,0.24663299663299662,"guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. We
207"
TRAINING STRATEGY,0.2474747474747475,"also utilize LoRA tuning to improve efficiency. This stage ensures UniMoT can accurately interpret
208"
TRAINING STRATEGY,0.24831649831649832,"and respond to human instructions, making it versatile and effective for molecular tasks.
209"
EXPERIMENTS,0.24915824915824916,"4
Experiments
210"
MOLECULE COMPREHENSION TASKS,0.25,"4.1
Molecule Comprehension Tasks
211"
MOLECULE COMPREHENSION TASKS,0.25084175084175087,"Molecular Property Prediction Task.
The goal of molecular property prediction is to forecast
212"
MOLECULE COMPREHENSION TASKS,0.2516835016835017,"a molecule’s intrinsic physical and chemical properties. For the classification task, we incorporate
213"
MOLECULE COMPREHENSION TASKS,0.25252525252525254,"eight binary classification datasets from MoleculeNet [53]. Models are tasked with generating
214"
MOLECULE COMPREHENSION TASKS,0.25336700336700335,"a single prediction (“yes” or “no”). We compare UniMoT with the following baselines: KV-
215"
MOLECULE COMPREHENSION TASKS,0.2542087542087542,"PLM [58], AttrMask [16], InfoGraph [40], MolCLR [48], GraphMVP [30], MoleculeSTM [31],
216"
MOLECULE COMPREHENSION TASKS,0.255050505050505,"and InstructMol [6]. The ROC-AUC (%) results on the MoleculeNet datasets are shown in Table 1.
217"
MOLECULE COMPREHENSION TASKS,0.2558922558922559,"The performance of the regression task of molecular property prediction is provided in Appendix D.
218"
MOLECULE COMPREHENSION TASKS,0.25673400673400676,"Compared to traditional graph learning methods and molecular LLMs like InstructMol, UniMoT
219"
MOLECULE COMPREHENSION TASKS,0.25757575757575757,"demonstrates consistent improvements across the eight datasets, indicating its robust molecule
220"
MOLECULE COMPREHENSION TASKS,0.25841750841750843,"comprehension abilities.
221"
MOLECULE COMPREHENSION TASKS,0.25925925925925924,"Molecule Captioning Task.
The molecule captioning task involves generating a comprehensive
222"
MOLECULE COMPREHENSION TASKS,0.2601010101010101,"description of a molecule. We compare UniMoT with several baselines: MolT5 [11], MoMu [39],
223"
MOLECULE COMPREHENSION TASKS,0.2609427609427609,"InstructMol [6], MolCA [33], and 3D-MoLM [25]. BLEU [37], ROUGE [27], and METEOR [1] are
224"
MOLECULE COMPREHENSION TASKS,0.2617845117845118,"adopted as evaluation metrics. UniMoT is evaluated for molecule captioning on the PubChem and
225"
MOLECULE COMPREHENSION TASKS,0.26262626262626265,"Table 1: ROC-AUC (%) of molecular property prediction task (classification) on the MoleculeNet
datasets. Bold indicates the best performance and underline indicates the second best performance."
MOLECULE COMPREHENSION TASKS,0.26346801346801346,"Model
BBBP↑
Tox21↑
ToxCast↑
Sider↑
ClinTox↑
MUV↑
HIV↑
BACE↑"
MOLECULE COMPREHENSION TASKS,0.26430976430976433,"KV-PLM [58]
70.50
72.12
55.03
59.83
89.17
54.63
65.40
78.50
AttrMask [16]
67.79
75.00
63.57
58.05
75.44
73.76
75.44
80.28
InfoGraph [40]
64.84
76.24
62.68
59.15
76.51
72.97
70.20
77.64
MolCLR [48]
67.79
75.55
64.58
58.66
84.22
72.76
75.88
71.14
GraphMVP [30]
68.11
77.06
65.11
60.64
84.46
74.38
77.74
80.48
MoleculeSTM [31]
69.98
76.91
65.05
60.96
92.53
73.40
76.93
80.77
InstructMol (Vicuna-7B) [6]
70.00
74.67
64.29
57.80
91.48
74.62
68.90
82.30"
MOLECULE COMPREHENSION TASKS,0.26515151515151514,"UniMoT (LLaMA2-7B)
71.37
76.43
65.78
59.79
92.89
75.97
78.49
83.69"
MOLECULE COMPREHENSION TASKS,0.265993265993266,"Table 2: Performance (%) of molecule captioning task on the PubChem dataset. Bold indicates the
best performance and underline indicates the second best performance."
MOLECULE COMPREHENSION TASKS,0.2668350168350168,"Model
BLEU-2↑
BLEU-4↑
ROUGE-1↑
ROUGE-2↑
ROUGE-L↑
METEOR↑"
MOLECULE COMPREHENSION TASKS,0.2676767676767677,"MolT5-Small (T5-Small) [11]
22.5
15.2
30.4
13.5
20.3
24.0
MolT5-Base (T5-Base) [11]
24.5
16.6
32.2
14.0
21.4
26.1
MolT5-Large (T5-Large) [11]
25.9
17.3
34.1
16.4
23.4
28.0
MoMu-Small (T5-Small) [39]
22.9
16.0
31.0
13.7
20.8
24.4
MoMu-Base (T5-Base) [39]
24.7
16.8
32.5
14.6
22.1
27.2
MoMu-Large (T5-Large) [39]
26.3
18.0
34.8
16.9
24.8
28.7
InstructMol (Vicuna-7B) [6]
18.9
11.7
27.3
11.8
17.8
21.3
MolCA (OPT-125M) [33]
25.9
17.5
34.4
16.6
23.9
28.5
MolCA (OPT-1.3B) [33]
28.6
21.3
36.2
21.4
29.7
32.6
3D-MoLM (LLaMA2-7B) [25]
30.3
22.5
36.8
22.3
31.2
33.1"
MOLECULE COMPREHENSION TASKS,0.26851851851851855,"UniMoT (LLaMA2-7B)
31.3
23.8
37.5
23.7
33.6
34.8"
MOLECULE COMPREHENSION TASKS,0.26936026936026936,"CheBI-20 datasets. Performance on the PubChem dataset is shown in Table 2, while the performance
226"
MOLECULE COMPREHENSION TASKS,0.2702020202020202,"on the CheBI-20 dataset and some concrete examples are presented in Appendix D.
227"
MOLECULE COMPREHENSION TASKS,0.27104377104377103,"From Table 2, we observe that UniMoT consistently outperforms the baselines by a significant margin.
228"
MOLECULE COMPREHENSION TASKS,0.2718855218855219,"This task is more complex than classification or regression, providing a robust measure of the model’s
229"
MOLECULE COMPREHENSION TASKS,0.2727272727272727,"molecule comprehension abilities. Notably, our proposed tokenizer-based architecture surpasses the
230"
MOLECULE COMPREHENSION TASKS,0.2735690235690236,"projection-based architecture (such as InstructMol), Q-Former-based architecture (such as MolCA
231"
MOLECULE COMPREHENSION TASKS,0.27441077441077444,"and 3D-MoLM), and models trained with contrastive learning strategies (such as MoMu). The results
232"
MOLECULE COMPREHENSION TASKS,0.27525252525252525,"demonstrate that the molecule tokenizer can generate molecule tokens with high-level molecular and
233"
MOLECULE COMPREHENSION TASKS,0.2760942760942761,"textual information, enhancing molecule comprehension abilities.
234"
MOLECULE COMPREHENSION TASKS,0.2769360269360269,"Molecule-Text Retrieval Task.
The molecule-text retrieval task involves using a molecule to
235"
MOLECULE COMPREHENSION TASKS,0.2777777777777778,"retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several
236"
MOLECULE COMPREHENSION TASKS,0.2786195286195286,"baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D-
237"
MOLECULE COMPREHENSION TASKS,0.27946127946127947,"MoLM [25]. We report the performance of retrieval using a batch of 64 random samples and the entire
238"
MOLECULE COMPREHENSION TASKS,0.2803030303030303,"test set, evaluated with the metrics of Accuracy and Recall@20. We use the checkpoint from Stage-1
239"
MOLECULE COMPREHENSION TASKS,0.28114478114478114,"of pretraining. UniMoT is evaluated on the datasets of PubChem, PCdes, and MoMu. Performance
240"
MOLECULE COMPREHENSION TASKS,0.281986531986532,"on the PubChem dataset is shown in Table 3, while performance on the PCdes and MoMu datasets is
241"
MOLECULE COMPREHENSION TASKS,0.2828282828282828,"presented in Appendix D. UniMoT can understand complex molecule-text interactions through the
242"
MOLECULE COMPREHENSION TASKS,0.2836700336700337,"introduction of the Causal Q-Former. From Table 3, UniMoT demonstrates superior performance over
243"
MOLECULE COMPREHENSION TASKS,0.2845117845117845,"the baselines on molecule-text retrieval, particularly in molecule-to-text retrieval. This underscores
244"
MOLECULE COMPREHENSION TASKS,0.28535353535353536,"UniMoT’s capability in learning fine-grained alignment between molecules and text.
245"
MOLECULE GENERATION TASKS,0.28619528619528617,"4.2
Molecule Generation Tasks
246"
MOLECULE GENERATION TASKS,0.28703703703703703,"We employ molecule generation tasks, which encompass caption-guided molecule generation, reagent
247"
MOLECULE GENERATION TASKS,0.2878787878787879,"prediction, forward reaction prediction, and retrosynthesis. Caption-guided molecule generation
248"
MOLECULE GENERATION TASKS,0.2887205387205387,"involves generating molecular structures based on textual descriptions. Reagent prediction entails
249"
MOLECULE GENERATION TASKS,0.2895622895622896,"determining suitable reagents given reactants and products. Forward reaction prediction involves
250"
MOLECULE GENERATION TASKS,0.2904040404040404,"predicting probable products given specific reactants and reagents. Retrosynthesis involves decon-
251"
MOLECULE GENERATION TASKS,0.29124579124579125,"structing a target molecule into simpler starting materials. We compare UniMoT with the following
252"
MOLECULE GENERATION TASKS,0.29208754208754206,"Table 3: Performance (%) of molecule-text retrieval task on the PubChem dataset. Bold indicates the
best performance and underline indicates the second best performance. Model"
MOLECULE GENERATION TASKS,0.29292929292929293,"Retrieval in batch
Retrieval in test set
M2T (%)
T2M (%)
M2T (%)
T2M (%)"
MOLECULE GENERATION TASKS,0.2937710437710438,"Acc↑
R@20↑
Acc↑
R@20↑
Acc↑
R@20↑
Acc↑
R@20↑"
MOLECULE GENERATION TASKS,0.2946127946127946,"Sci-BERT [2]
85.3
98.7
84.2
98.4
41.7
87.3
40.2
86.8
KV-PLM [58]
86.1
98.6
85.2
98.5
42.8
88.5
41.7
87.8
MoMu (Sci-BERT) [39]
87.6
99.2
86.4
99.4
47.3
90.8
48.1
89.9
MoMu (KV-PLM) [39]
88.2
99.4
87.3
99.4
48.5
91.6
49.5
90.7
MoleculeSTM [31]
90.5
99.6
88.6
99.5
52.7
92.9
53.2
92.5
MolCA (OPT-1.3B) [33]
92.6
99.8
91.3
99.5
67.9
94.4
68.6
93.3
3D-MoLM (LLaMA2-7B) [25]
93.5
100.0
92.9
99.6
69.1
95.9
70.1
94.9"
MOLECULE GENERATION TASKS,0.29545454545454547,"UniMoT (LLaMA2-7B)
93.6
100.0
92.7
99.4
69.5
96.3
69.8
94.4"
MOLECULE GENERATION TASKS,0.2962962962962963,"Table 4: Performance of molecule generation tasks on the Mol-Instructions dataset, including caption-
guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. Bold
indicates the best performance, and underline indicates the second best performance."
MOLECULE GENERATION TASKS,0.29713804713804715,"Model
Exact↑
BLEU↑
Levenshtein↓
RDK FTS↑
MACCS FTS↑
Morgan FTS↑
Validity↑"
MOLECULE GENERATION TASKS,0.29797979797979796,"Caption-guided Molecule Generation
LLaMA [44]
0.000
0.003
59.864
0.005
0.000
0.000
0.003
Vicuna [7]
0.000
0.006
60.356
0.006
0.001
0.000
0.001
Mol-Instructions [12]
0.002
0.345
41.367
0.231
0.412
0.147
1.000
MolT5 [11]
0.112
0.546
38.276
0.400
0.538
0.295
0.773"
MOLECULE GENERATION TASKS,0.2988215488215488,"UniMoT
0.237
0.698
27.782
0.543
0.651
0.411
1.000"
MOLECULE GENERATION TASKS,0.2996632996632997,"Reagent Prediction
LLaMA [44]
0.000
0.003
28.040
0.037
0.001
0.001
0.001
Vicuna [7]
0.000
0.010
27.948
0.038
0.002
0.001
0.007
Mol-Instructions [12]
0.044
0.224
23.167
0.237
0.364
0.213
1.000
InstructMol [6]
0.129
0.610
19.664
0.444
0.539
0.400
1.000"
MOLECULE GENERATION TASKS,0.3005050505050505,"UniMoT
0.167
0.728
14.588
0.549
0.621
0.507
1.000"
MOLECULE GENERATION TASKS,0.30134680134680136,"Forward Reaction Prediction
LLaMA [44]
0.000
0.020
42.002
0.001
0.002
0.001
0.039
Vicuna [7]
0.000
0.057
41.690
0.007
0.016
0.006
0.059
Mol-Instructions [12]
0.045
0.654
27.262
0.313
0.509
0.262
1.000
InstructMol [6]
0.536
0.967
10.851
0.776
0.878
0.741
1.000"
MOLECULE GENERATION TASKS,0.3021885521885522,"UniMoT
0.611
0.980
8.297
0.836
0.911
0.807
1.000"
MOLECULE GENERATION TASKS,0.30303030303030304,"Retrosynthesis
LLaMA [44]
0.000
0.036
46.844
0.018
0.029
0.017
0.010
Vicuna [7]
0.000
0.057
46.877
0.025
0.030
0.021
0.017
Mol-Instructions [12]
0.009
0.705
31.227
0.283
0.487
0.230
1.000
InstructMol [6]
0.407
0.941
13.967
0.753
0.852
0.714
1.000"
MOLECULE GENERATION TASKS,0.30387205387205385,"UniMoT
0.478
0.974
11.634
0.810
0.909
0.771
1.000"
MOLECULE GENERATION TASKS,0.3047138047138047,"baselines: LLaMA [44], Vicuna [7], Mol-Instructions [12], and InstructMol [6]. The metrics used
253"
MOLECULE GENERATION TASKS,0.3055555555555556,"to evaluate molecule generation tasks include Exact Match, BLEU [37], Levenshtein Distance [22],
254"
MOLECULE GENERATION TASKS,0.3063973063973064,"RDKit Fingerprint Similarity [20], MACCS Fingerprint Similarity [10], and Morgan Fingerprint
255"
MOLECULE GENERATION TASKS,0.30723905723905726,"Similarity [36]. These metrics evaluate structural similarity between generated and target molecules,
256"
MOLECULE GENERATION TASKS,0.30808080808080807,"along with Validity [19], which assesses the proportion of chemically valid molecules generated. We
257"
MOLECULE GENERATION TASKS,0.30892255892255893,"utilize the Mol-Instructions dataset to evaluate the generation capabilities of UniMoT, and the results
258"
MOLECULE GENERATION TASKS,0.30976430976430974,"are presented in Table 4.
259"
MOLECULE GENERATION TASKS,0.3106060606060606,"As the baselines generate SMILES strings and then convert them to molecules, UniMoT directly
260"
MOLECULE GENERATION TASKS,0.3114478114478115,"leverages the generated molecule tokens and obtains their embeddings from the learned codebook.
261"
MOLECULE GENERATION TASKS,0.3122895622895623,"These embeddings can be decoded to desired molecules through the pretrained adapter and SMILES
262"
MOLECULE GENERATION TASKS,0.31313131313131315,"decoder. Regarding the results in Table 4, UniMoT exhibits the capability to generate valid molecules
263"
MOLECULE GENERATION TASKS,0.31397306397306396,"with a higher degree of similarity to the target molecules compared to the baselines. UniMoT can
264"
MOLECULE GENERATION TASKS,0.3148148148148148,"generate molecules as if they were text, demonstrating strong generation capabilities and providing a
265"
MOLECULE GENERATION TASKS,0.31565656565656564,"new perspective to molecule generation tasks.
266"
MOLECULE GENERATION TASKS,0.3164983164983165,"Table 5: Ablation study on the projector and representation form for the molecule captioning task
using the PubChem dataset."
MOLECULE GENERATION TASKS,0.31734006734006737,"Projector
Input to LLM
BLEU-2
BLEU-4
ROUGE-1
ROUGE-2
ROUGE-L
METEOR"
MOLECULE GENERATION TASKS,0.3181818181818182,"Projection Layer
Molecule Emb.
19.3
12.1
27.9
12.3
18.1
21.5
Q-Former
Query Emb.
28.6
21.3
36.2
21.4
29.7
32.6
Causal Q-Former
Causal Emb.
32.8
25.2
39.2
24.8
35.3
36.5
Causal Q-Former
Causal Tokens
31.3
23.8
37.5
23.7
33.6
34.8"
MOLECULE GENERATION TASKS,0.31902356902356904,"Table 6: Ablation study on the model size and tuning strategy for the molecule captioning task using
the PubChem dataset."
MOLECULE GENERATION TASKS,0.31986531986531985,"Model Size
Tuning
BLEU-2
BLEU-4
ROUGE-1
ROUGE-2
ROUGE-L
METEOR"
MOLECULE GENERATION TASKS,0.3207070707070707,"LLaMA2-7B
LoRA Tuning
31.3
23.8
37.5
23.7
33.6
34.8
LLaMA2-7B
Fully Tuning
32.0
24.6
38.3
24.3
34.7
35.6
LLaMA2-13B
LoRA Tuning
31.8
24.3
38.0
24.1
34.4
35.3"
ABLATION STUDIES,0.32154882154882153,"4.3
Ablation Studies
267"
ABLATION STUDIES,0.3223905723905724,"Cross-Modal Projector.
We conducted an ablation study on the cross-modal projector, with the
268"
ABLATION STUDIES,0.32323232323232326,"results on the molecule captioning task shown in Table 5. The linear projection demonstrated the worst
269"
ABLATION STUDIES,0.32407407407407407,"performance, indicating that the molecule features lack textual information, thus hindering effective
270"
ABLATION STUDIES,0.32491582491582494,"molecule-text interactions and alignment. Additionally, we compared the performance of a Q-Former
271"
ABLATION STUDIES,0.32575757575757575,"with bidirectional self-attention to a Causal Q-Former with causal self-attention. The results show
272"
ABLATION STUDIES,0.3265993265993266,"that query embeddings with causal dependency outperform those with bidirectional dependency. This
273"
ABLATION STUDIES,0.3274410774410774,"demonstrates that input with left-to-right causal dependency aligns with the unidirectional attention
274"
ABLATION STUDIES,0.3282828282828283,"mechanism in LLMs, leading to improved performance.
275"
ABLATION STUDIES,0.3291245791245791,"Discrete vs. Continuous Representation.
We compare the performance of continuous causal query
276"
ABLATION STUDIES,0.32996632996632996,"embeddings and discrete tokens quantized from causal embeddings as inputs to LLMs. As shown in
277"
ABLATION STUDIES,0.33080808080808083,"Table 5, continuous embeddings demonstrate better performance than discrete tokens in understanding
278"
ABLATION STUDIES,0.33164983164983164,"molecules. This result is reasonable since the quantization process causes information loss in discrete
279"
ABLATION STUDIES,0.3324915824915825,"tokens. However, we still use discrete token representation to facilitate the autoregressive training
280"
ABLATION STUDIES,0.3333333333333333,"paradigm of LLMs, which supports the unification of comprehension and generation tasks. To achieve
281"
ABLATION STUDIES,0.3341750841750842,"this unification, we unavoidably sacrifice some performance in comprehension tasks.
282"
ABLATION STUDIES,0.335016835016835,"Model Size and Tuning Stategy.
We conducted a comparison of molecule captioning performance
283"
ABLATION STUDIES,0.33585858585858586,"across various model sizes and tuning strategies, as illustrated in Table 6. Our findings indicate that
284"
ABLATION STUDIES,0.3367003367003367,"scaling up the LLM to 13B or adopting a fully tuning strategy yields only marginal improvements
285"
ABLATION STUDIES,0.33754208754208753,"in performance compared to using LLaMA2-7B with LoRA tuning. While larger models and fully
286"
ABLATION STUDIES,0.3383838383838384,"tuning strategies might offer slight gains in performance, they come at a significant cost in terms of
287"
ABLATION STUDIES,0.3392255892255892,"efficiency. Considering the trade-off between achieving high performance and maintaining efficiency,
288"
ABLATION STUDIES,0.3400673400673401,"we have chosen to utilize LLaMA2-7B with LoRA tuning in our experiments. This ensures that our
289"
ABLATION STUDIES,0.3409090909090909,"model remains both powerful and practical.
290"
CONCLUSION,0.34175084175084175,"5
Conclusion
291"
CONCLUSION,0.3425925925925926,"This work introduces UniMoT, an innovation in the field of molecular-textual understanding and
292"
CONCLUSION,0.3434343434343434,"generation, which has successfully unified these two distinct modalities under a single, coherent
293"
CONCLUSION,0.3442760942760943,"framework. By integrating a Vector Quantization-driven tokenizer with a Causal Q-Former, UniMoT
294"
CONCLUSION,0.3451178451178451,"overcomes previous architectural limitations where molecule and text modalities were not treated
295"
CONCLUSION,0.34595959595959597,"equally, lacking a dedicated supervision signal for the molecular domain. This unique tokenizer
296"
CONCLUSION,0.3468013468013468,"transforms molecules into sequences of discrete tokens, embedding high-level molecular and textual
297"
CONCLUSION,0.34764309764309764,"information cohesively. Moreover, by employing a four-stage training scheme, UniMoT has emerged
298"
CONCLUSION,0.3484848484848485,"as a versatile multi-modal LLM, adept at handling molecule-to-text and text-to-molecule tasks.
299"
CONCLUSION,0.3493265993265993,"Extensive empirical evaluations demonstrate that UniMoT attains state-of-the-art performance across
300"
CONCLUSION,0.3501683501683502,"a diverse array of molecule comprehension and generation tasks.
301"
REFERENCES,0.351010101010101,"References
302"
REFERENCES,0.35185185185185186,"[1] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
303"
REFERENCES,0.35269360269360267,"correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation
304"
REFERENCES,0.35353535353535354,"measures for machine translation and/or summarization, pages 65–72, 2005.
305"
REFERENCES,0.3543771043771044,"[2] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv
306"
REFERENCES,0.3552188552188552,"preprint arXiv:1903.10676, 2019.
307"
REFERENCES,0.3560606060606061,"[3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
308"
REFERENCES,0.3569023569023569,"stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
309"
REFERENCES,0.35774410774410775,"[4] Yoann Boget, Magda Gregorova, and Alexandros Kalousis. Vector-quantized graph auto-encoder. arXiv
310"
REFERENCES,0.35858585858585856,"preprint arXiv:2306.07735, 2023.
311"
REFERENCES,0.35942760942760943,"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
312"
REFERENCES,0.3602693602693603,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
313"
REFERENCES,0.3611111111111111,"Advances in neural information processing systems, 33:1877–1901, 2020.
314"
REFERENCES,0.36195286195286197,"[6] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a
315"
REFERENCES,0.3627946127946128,"versatile and reliable molecular assistant in drug discovery. arXiv preprint arXiv:2311.16208, 2023.
316"
REFERENCES,0.36363636363636365,"[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
317"
REFERENCES,0.36447811447811446,"Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4
318"
REFERENCES,0.3653198653198653,"with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.
319"
REFERENCES,0.3661616161616162,"[8] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised
320"
REFERENCES,0.367003367003367,"pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.
321"
REFERENCES,0.36784511784511786,"[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
322"
REFERENCES,0.3686868686868687,"tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
323"
REFERENCES,0.36952861952861954,"[10] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys
324"
REFERENCES,0.37037037037037035,"for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273–1280, 2002.
325"
REFERENCES,0.3712121212121212,"[11] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between
326"
REFERENCES,0.3720538720538721,"molecules and natural language. arXiv preprint arXiv:2204.11817, 2022.
327"
REFERENCES,0.3728956228956229,"[12] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and
328"
REFERENCES,0.37373737373737376,"Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models.
329"
REFERENCES,0.37457912457912457,"arXiv preprint arXiv:2306.08018, 2023.
330"
REFERENCES,0.37542087542087543,"[13] Robert Gray. Vector quantization. IEEE Assp Magazine, 1(2):4–29, 1984.
331"
REFERENCES,0.37626262626262624,"[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
332"
REFERENCES,0.3771043771043771,"Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
333"
REFERENCES,0.3779461279461279,"2021.
334"
REFERENCES,0.3787878787878788,"[15] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
335"
REFERENCES,0.37962962962962965,"Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
336"
REFERENCES,0.38047138047138046,"[16] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
337"
REFERENCES,0.3813131313131313,"Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
338"
REFERENCES,0.38215488215488214,"[17] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained
339"
REFERENCES,0.382996632996633,"transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.
340"
REFERENCES,0.3838383838383838,"[18] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A
341"
REFERENCES,0.3846801346801347,"Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic acids research, 51(D1):D1373–
342"
REFERENCES,0.38552188552188554,"D1380, 2023.
343"
REFERENCES,0.38636363636363635,"[19] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. In
344"
REFERENCES,0.3872053872053872,"International conference on machine learning, pages 1945–1954. PMLR, 2017.
345"
REFERENCES,0.38804713804713803,"[20] Greg Landrum et al. Rdkit: Open-source cheminformatics, 2006.
346"
REFERENCES,0.3888888888888889,"[21] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo
347"
REFERENCES,0.3897306397306397,"Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining.
348"
REFERENCES,0.39057239057239057,"Bioinformatics, 36(4):1234–1240, 2020.
349"
REFERENCES,0.39141414141414144,"[22] Vladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In
350"
REFERENCES,0.39225589225589225,"Soviet physics doklady, volume 10, pages 707–710. Soviet Union, 1966.
351"
REFERENCES,0.3930976430976431,"[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
352"
REFERENCES,0.3939393939393939,"with frozen image encoders and large language models. In International conference on machine learning,
353"
REFERENCES,0.3947811447811448,"pages 19730–19742. PMLR, 2023.
354"
REFERENCES,0.3956228956228956,"[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training
355"
REFERENCES,0.39646464646464646,"for unified vision-language understanding and generation. In International conference on machine learning,
356"
REFERENCES,0.39730639730639733,"pages 12888–12900. PMLR, 2022.
357"
REFERENCES,0.39814814814814814,"[25] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and
358"
REFERENCES,0.398989898989899,"Qi Tian. Towards 3d molecule-text interpretation in language models. arXiv preprint arXiv:2401.13923,
359"
REFERENCES,0.3998316498316498,"2024.
360"
REFERENCES,0.4006734006734007,"[26] Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like
361"
REFERENCES,0.4015151515151515,"capabilities on drug molecule graphs. arXiv preprint arXiv:2309.03907, 2023.
362"
REFERENCES,0.40235690235690236,"[27] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches
363"
REFERENCES,0.4031986531986532,"out, pages 74–81, 2004.
364"
REFERENCES,0.40404040404040403,"[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
365"
REFERENCES,0.4048821548821549,"information processing systems, 36, 2024.
366"
REFERENCES,0.4057239057239057,"[29] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. Git-mol: A multi-modal large language model for
367"
REFERENCES,0.4065656565656566,"molecular science with graph, image, and text. Computers in Biology and Medicine, 171:108073, 2024.
368"
REFERENCES,0.4074074074074074,"[30] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation
369"
REFERENCES,0.40824915824915825,"for graphs, with applications to molecules. Advances in neural information processing systems, 32, 2019.
370"
REFERENCES,0.4090909090909091,"[31] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao,
371"
REFERENCES,0.4099326599326599,"and Animashree Anandkumar. Multi-modal molecule structure–text model for text-based retrieval and
372"
REFERENCES,0.4107744107744108,"editing. Nature Machine Intelligence, 5(12):1447–1457, 2023.
373"
REFERENCES,0.4116161616161616,"[32] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training
374"
REFERENCES,0.41245791245791247,"molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728, 2021.
375"
REFERENCES,0.4132996632996633,"[33] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng
376"
REFERENCES,0.41414141414141414,"Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter.
377"
REFERENCES,0.414983164983165,"arXiv preprint arXiv:2310.12798, 2023.
378"
REFERENCES,0.4158249158249158,"[34] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. Molfm: A multimodal molecular
379"
REFERENCES,0.4166666666666667,"foundation model. arXiv preprint arXiv:2307.09484, 2023.
380"
REFERENCES,0.4175084175084175,"[35] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt:
381"
REFERENCES,0.41835016835016836,"Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442,
382"
REFERENCES,0.41919191919191917,"2023.
383"
REFERENCES,0.42003367003367004,"[36] Harry L Morgan. The generation of a unique machine description for chemical structures-a technique
384"
REFERENCES,0.4208754208754209,"developed at chemical abstracts service. Journal of chemical documentation, 5(2):107–113, 1965.
385"
REFERENCES,0.4217171717171717,"[37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation
386"
REFERENCES,0.4225589225589226,"of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
387"
REFERENCES,0.4234006734006734,"Linguistics, pages 311–318, 2002.
388"
REFERENCES,0.42424242424242425,"[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
389"
REFERENCES,0.42508417508417506,"Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
390"
REFERENCES,0.42592592592592593,"Journal of machine learning research, 21(140):1–67, 2020.
391"
REFERENCES,0.42676767676767674,"[39] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong
392"
REFERENCES,0.4276094276094276,"Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv
393"
REFERENCES,0.42845117845117847,"preprint arXiv:2209.05481, 2022.
394"
REFERENCES,0.4292929292929293,"[40] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
395"
REFERENCES,0.43013468013468015,"supervised graph-level representation learning via mutual information maximization. arXiv preprint
396"
REFERENCES,0.43097643097643096,"arXiv:1908.01000, 2019.
397"
REFERENCES,0.4318181818181818,"[41] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt:
398"
REFERENCES,0.43265993265993263,"Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023, 2023.
399"
REFERENCES,0.4335016835016835,"[42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
400"
REFERENCES,0.43434343434343436,"and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
401"
REFERENCES,0.4351851851851852,"[43] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
402"
REFERENCES,0.43602693602693604,"Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv
403"
REFERENCES,0.43686868686868685,"preprint arXiv:2211.09085, 2022.
404"
REFERENCES,0.4377104377104377,"[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
405"
REFERENCES,0.4385521885521885,"Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
406"
REFERENCES,0.4393939393939394,"language models (2023). arXiv preprint arXiv:2302.13971, 2023.
407"
REFERENCES,0.44023569023569026,"[45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural
408"
REFERENCES,0.44107744107744107,"information processing systems, 30, 2017.
409"
REFERENCES,0.44191919191919193,"[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
410"
REFERENCES,0.44276094276094274,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
411"
REFERENCES,0.4436026936026936,"30, 2017.
412"
REFERENCES,0.4444444444444444,"[47] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale
413"
REFERENCES,0.4452861952861953,"unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international
414"
REFERENCES,0.44612794612794615,"conference on bioinformatics, computational biology and health informatics, pages 429–436, 2019.
415"
REFERENCES,0.44696969696969696,"[48] Y Wang, J Wang, Z Cao, and AB Farimani. Molclr: Molecular contrastive learning of representations via
416"
REFERENCES,0.4478114478114478,"graph neural networks. arxiv 2021. arXiv preprint arXiv:2102.10056, 2021.
417"
REFERENCES,0.44865319865319864,"[49] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar.
418"
REFERENCES,0.4494949494949495,"Retrieval-based controllable molecule generation. arXiv preprint arXiv:2208.11126, 2022.
419"
REFERENCES,0.4503367003367003,"[50] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology
420"
REFERENCES,0.4511784511784512,"and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36, 1988.
421"
REFERENCES,0.45202020202020204,"[51] Jiawei Wu, Mingyuan Yan, and Dianbo Liu. Vqsynery: Robust drug synergy prediction with vector
422"
REFERENCES,0.45286195286195285,"quantization mechanism. arXiv preprint arXiv:2403.03089, 2024.
423"
REFERENCES,0.4537037037037037,"[52] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm.
424"
REFERENCES,0.45454545454545453,"arXiv preprint arXiv:2309.05519, 2023.
425"
REFERENCES,0.4553872053872054,"[53] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu,
426"
REFERENCES,0.4562289562289562,"Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical
427"
REFERENCES,0.45707070707070707,"science, 9(2):513–530, 2018.
428"
REFERENCES,0.45791245791245794,"[54] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z Li.
429"
REFERENCES,0.45875420875420875,"Mole-bert: Rethinking pre-training graph neural networks for molecules. In The Eleventh International
430"
REFERENCES,0.4595959595959596,"Conference on Learning Representations, 2022.
431"
REFERENCES,0.4604377104377104,"[55] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with
432"
REFERENCES,0.4612794612794613,"parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.
433"
REFERENCES,0.4621212121212121,"[56] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive
434"
REFERENCES,0.46296296296296297,"learning with augmentations. Advances in neural information processing systems, 33:5812–5823, 2020.
435"
REFERENCES,0.46380471380471383,"[57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,
436"
REFERENCES,0.46464646464646464,"Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint
437"
REFERENCES,0.4654882154882155,"arXiv:2110.04627, 2021.
438"
REFERENCES,0.4663299663299663,"[58] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure
439"
REFERENCES,0.4671717171717172,"and biomedical text with comprehension comparable to human professionals. Nature communications,
440"
REFERENCES,0.468013468013468,"13(1):862, 2022.
441"
REFERENCES,0.46885521885521886,"[59] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,
442"
REFERENCES,0.4696969696969697,"Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv
443"
REFERENCES,0.47053872053872053,"preprint arXiv:2402.12226, 2024.
444"
REFERENCES,0.4713804713804714,"[60] Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, and
445"
REFERENCES,0.4722222222222222,"Huajun Chen. Learning invariant molecular representation in latent discrete space. Advances in Neural
446"
REFERENCES,0.4730639730639731,"Information Processing Systems, 36, 2024.
447"
REFERENCES,0.4739057239057239,"A
Details of Causal Q-Former
448"
REFERENCES,0.47474747474747475,"The Q-Former operates as a query-based transformer that utilizes learnable query vectors to interact
449"
REFERENCES,0.47558922558922556,"with molecule features extracted by a frozen encoder. These queries are essential for extracting rele-
450"
REFERENCES,0.4764309764309764,"vant information from the molecule features. The Q-Former comprises both a molecule transformer
451"
REFERENCES,0.4772727272727273,"and a text transformer, sharing self-attention layers. The text transformer architecture is based on
452"
REFERENCES,0.4781144781144781,"BERT [9], while the molecule transformer incorporates cross-attention layers between self-attention
453"
REFERENCES,0.47895622895622897,"and feed-forward layers. Q-Former employs a cross-attention mechanism where the query vectors
454"
REFERENCES,0.4797979797979798,"selectively attend to different aspects of the molecule features, allowing the model to capture critical
455"
REFERENCES,0.48063973063973064,"details necessary for understanding and generating textual descriptions of molecular properties.
456"
REFERENCES,0.48148148148148145,"Specifically, we incorporate causal masks into the queries, ensuring that they only interact with
457"
REFERENCES,0.4823232323232323,"preceding queries. This ensures the sequence of query embeddings maintains a causal dependency,
458"
REFERENCES,0.4831649831649832,"aligning with the requirements of LLMs operating on text sequence. The Causal Q-Former is
459"
REFERENCES,0.484006734006734,"illustrated in Figure 4. We employ the Causal Q-Former to generate causal query embeddings
460"
REFERENCES,0.48484848484848486,"Z = {zi}M
i=1 ∈RM×d containing high-level molecular and textual information, where M represents
461"
REFERENCES,0.48569023569023567,"the number of queries and d denotes the dimension of query embeddings. Next, we introduce three
462"
REFERENCES,0.48653198653198654,"tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former.
463"
REFERENCES,0.48737373737373735,Molecule
REFERENCES,0.4882154882154882,Encoder
REFERENCES,0.4890572390572391,"Input
Molecule"
REFERENCES,0.4898989898989899,"“The molecule is an 
indole phytoalexin 
that …”
Learnable Queries
Molecule Caption"
REFERENCES,0.49074074074074076,Self Attention
REFERENCES,0.49158249158249157,Cross Attention
REFERENCES,0.49242424242424243,Feed Forward
REFERENCES,0.49326599326599324,Molecule-Text
REFERENCES,0.4941077441077441,Matching
REFERENCES,0.494949494949495,Self Attention
REFERENCES,0.4957912457912458,Feed Forward
REFERENCES,0.49663299663299665,Molecule-Grounded
REFERENCES,0.49747474747474746,"Text Generation
Molecule-Text"
REFERENCES,0.4983164983164983,Contrastive
REFERENCES,0.49915824915824913,Learning
REFERENCES,0.5,"Every
Block"
REFERENCES,0.5008417508417509,"Causal Self-
Attention Mask"
REFERENCES,0.5016835016835017,"Different Masking
Strategies for
Different Tasks"
REFERENCES,0.5025252525252525,"Figure 4: Illustration of our proposed Causal Q-Former. The Causal Q-Former provides causal query
embeddings for subsequent blocks."
REFERENCES,0.5033670033670034,"Molecule-Text Contrastive Learning (MTC) aims to align molecule and text features by maximizing
464"
REFERENCES,0.5042087542087542,"their mutual information. This is achieved by maximizing the molecule-text similarity of positive
465"
REFERENCES,0.5050505050505051,"pairs against that of negative pairs. We utilize the last query embedding zM of the query sequence
466"
REFERENCES,0.5058922558922558,"{zi}M
i=1 as the query representation, since the output query sequence is causal and the last embedding
467"
REFERENCES,0.5067340067340067,"contains global information from the queries. For text representation, we use the output embedding
468"
REFERENCES,0.5075757575757576,"of the [CLS] token, denoted as y. The contrastive learning loss is expressed as follows:
469"
REFERENCES,0.5084175084175084,"LMTC = −1 B B
X"
REFERENCES,0.5092592592592593,"i=1
log
exp((zi
M)T yi/τ)
PB
j=1 exp((zi
M)T yj/τ)
−1 B B
X"
REFERENCES,0.51010101010101,"i=1
log
exp((yi)T zi
M/τ)
PB
j=1 exp((yi)T zj
M/τ)
,
(4)"
REFERENCES,0.5109427609427609,"where B denotes the batch size, and τ represents the temperature parameter. Here, zi
M and yi refer
470"
REFERENCES,0.5117845117845118,"to the i-th query and text representations in a batch, respectively.
471"
REFERENCES,0.5126262626262627,"Molecule-Text Matching (MTM) focuses on learning fine-grained alignment between molecule and
472"
REFERENCES,0.5134680134680135,"text features. As query embeddings Z = {zi}M
i=1 capture both molecular and textual information
473"
REFERENCES,0.5143097643097643,"through cross-attention and self-attention layers respectively, we utilize the last query embedding zM
474"
REFERENCES,0.5151515151515151,"as input to a binary classifier. This classifier predicts whether a given molecule-text pair is matched
475"
REFERENCES,0.515993265993266,"or unmatched. The corresponding loss function is formulated as follows:
476"
REFERENCES,0.5168350168350169,"LMTM = −1 B B
X"
REFERENCES,0.5176767676767676,"i=1
log
exp(ϕ(zM | Xi, ti))
PB
j=1 exp(ϕ(zM | Xi, tj)) + PB
j=1 exp(ϕ(zM | Xj, ti))
,
(5)"
REFERENCES,0.5185185185185185,"where ϕ represents a binary classifier, and Xi and ti denote the i-th input molecule features and input
477"
REFERENCES,0.5193602693602694,"text in a batch, respectively.
478"
REFERENCES,0.5202020202020202,"Molecule-grounded Text Generation (MTG) focuses on generating textual descriptions given
479"
REFERENCES,0.5210437710437711,"a molecule input. In this task, causal masks for queries are not applied since only textual output
480"
REFERENCES,0.5218855218855218,"is required. However, causal masks are applied for text, allowing each text token to attend to its
481"
REFERENCES,0.5227272727272727,"preceding text tokens and all queries, but not subsequent tokens. The Language Modeling (LM)
482"
REFERENCES,0.5235690235690236,"loss function is applied to model the generation of text ti conditioned on the molecule input Xi,
483"
REFERENCES,0.5244107744107744,"formulated as:
484"
REFERENCES,0.5252525252525253,"LMTG = −1 B B
X i=1 L
X"
REFERENCES,0.5260942760942761,"j=1
log p
 
ti
j | ti
1, · · · , ti
j−1, Xi
,
(6)"
REFERENCES,0.5269360269360269,"where ti
j represents the j-th token in the text sequence ti. Here, Xi and ti denote the i-th input
485"
REFERENCES,0.5277777777777778,"molecule features and generated text in a batch, respectively.
486"
REFERENCES,0.5286195286195287,"The total loss for training the Q-Former encompasses the three aforementioned objectives:
487"
REFERENCES,0.5294612794612794,"LQ-Former = LMTC + LMTM + LMTG.
(7)"
REFERENCES,0.5303030303030303,"B
Details of Datasets
488"
REFERENCES,0.5311447811447811,"This section provides detailed information about the datasets used in evaluating the performance of
489"
REFERENCES,0.531986531986532,"UniMoT across various tasks. The datasets are utilized for molecular property prediction, molecule
490"
REFERENCES,0.5328282828282829,"captioning, molecule-text retrieval, and molecule generation tasks. Each dataset serves a unique
491"
REFERENCES,0.5336700336700336,"purpose in assessing different capabilities of the model.
492"
REFERENCES,0.5345117845117845,"We present the details of the Molecular Property Prediction Datasets below:
493"
REFERENCES,0.5353535353535354,"• BBBP [53]: The Blood-Brain Barrier Penetration dataset predicts the ability of molecules to
494"
REFERENCES,0.5361952861952862,"penetrate the blood-brain barrier.
495"
REFERENCES,0.5370370370370371,"• Tox21 [53]: This dataset is part of the Toxicology in the 21st Century initiative, used for toxicity
496"
REFERENCES,0.5378787878787878,"prediction.
497"
REFERENCES,0.5387205387205387,"• ToxCast [53]: Another toxicity prediction dataset with a broader range of biological assays.
498"
REFERENCES,0.5395622895622896,"• Sider [53]: Side Effect Resource database, used for predicting drug side effects.
499"
REFERENCES,0.5404040404040404,"• ClinTox [53]: Clinical Toxicity dataset for predicting clinical trial toxicity outcomes.
500"
REFERENCES,0.5412457912457912,"• MUV [53]: Maximum Unbiased Validation dataset for virtual screening.
501"
REFERENCES,0.5420875420875421,"• HIV [53]: Human Immunodeficiency Virus dataset for predicting anti-HIV activities.
502"
REFERENCES,0.5429292929292929,"• BACE [53]: Beta-Secretase 1 dataset for predicting inhibitors of the BACE-1 enzyme, relevant
503"
REFERENCES,0.5437710437710438,"for Alzheimer’s research.
504"
REFERENCES,0.5446127946127947,"• QM9 [12]: The quantum mechanics properties dataset, where the objective is to predict key
505"
REFERENCES,0.5454545454545454,"quantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO-
506"
REFERENCES,0.5462962962962963,"LUMO gap.
507"
REFERENCES,0.5471380471380471,"We present the details of the Molecule Captioning Datasets below:
508"
REFERENCES,0.547979797979798,"• PubChem [18]: A large dataset of chemical molecules used for generating textual descriptions
509"
REFERENCES,0.5488215488215489,"of molecular structures.
510"
REFERENCES,0.5496632996632996,"• ChEBI-20 [11]: A subset of the Chemical Entities of Biological Interest database, provides
511"
REFERENCES,0.5505050505050505,"structured and detailed descriptions of molecules, enhancing the model’s ability to generate
512"
REFERENCES,0.5513468013468014,"accurate captions.
513"
REFERENCES,0.5521885521885522,"We present the details of the Molecule-Text Retrieval Datasets below:
514"
REFERENCES,0.553030303030303,"• PubChem [18]: Used for both molecule-to-text (M2T) and text-to-molecule (T2M) retrieval
515"
REFERENCES,0.5538720538720538,"tasks.
516"
REFERENCES,0.5547138047138047,"• PCdes [58]: Another dataset for evaluating M2T and T2M retrieval accuracy.
517"
REFERENCES,0.5555555555555556,"• MoMu [39]: Dataset specifically designed for molecule-text interactions and retrieval tasks.
518"
REFERENCES,0.5563973063973064,"Table 7: Summary of datasets, their types, tasks, descriptions, URLs, and licenses used for evaluating
UniMoT."
REFERENCES,0.5572390572390572,"Dataset
Type
Tasks
Description
URL
License"
REFERENCES,0.5580808080808081,"BBBP
Classification Molecular Prop-
erty Prediction
Predicts
blood-brain
barrier
penetration
ability."
REFERENCES,0.5589225589225589,"BBBP URL
CC-BY 4.0"
REFERENCES,0.5597643097643098,"Tox21
Classification Molecular Prop-
erty Prediction
Toxicity prediction us-
ing the Tox21 initiative
data."
REFERENCES,0.5606060606060606,"Tox21 URL
Public Do-
main"
REFERENCES,0.5614478114478114,"ToxCast
Classification Molecular Prop-
erty Prediction
Broad toxicity predic-
tion with various biolog-
ical assays."
REFERENCES,0.5622895622895623,"ToxCast URL
Public Do-
main"
REFERENCES,0.5631313131313131,"Sider
Classification Molecular Prop-
erty Prediction
Predicts drug side ef-
fects.
Sider URL
CC-BY 4.0"
REFERENCES,0.563973063973064,"ClinTox
Classification Molecular Prop-
erty Prediction
Clinical trial toxicity
prediction.
ClinTox URL
Public Do-
main"
REFERENCES,0.5648148148148148,"MUV
Classification Molecular Prop-
erty Prediction
Virtual screening for un-
biased validation.
MUV URL
CC-BY 4.0"
REFERENCES,0.5656565656565656,"HIV
Classification Molecular Prop-
erty Prediction
Predicts anti-HIV activ-
ity of molecules.
HIV URL
Public Do-
main"
REFERENCES,0.5664983164983165,"BACE
Classification Molecular Prop-
erty Prediction
Predicts inhibitors of
the BACE-1 enzyme.
BACE URL
Public Do-
main"
REFERENCES,0.5673400673400674,"QM9
Regression
Molecular Prop-
erty Prediction
Predicts various molec-
ular
properties
such
as atomization energy,
dipole moment, etc."
REFERENCES,0.5681818181818182,"QM9 URL
CC-BY 4.0"
REFERENCES,0.569023569023569,"PubChem
Captioning,
Retrieval
Molecule
Captioning,
Molecule-Text
Retrieval"
REFERENCES,0.5698653198653199,"Generates
descrip-
tions
and
retrieves
text/molecules based on
input molecules/text."
REFERENCES,0.5707070707070707,"PubChem URL
Public Do-
main"
REFERENCES,0.5715488215488216,"ChEBI-
20
Captioning
Molecule Cap-
tioning
Generates detailed de-
scriptions of molecular
structures."
REFERENCES,0.5723905723905723,"ChEBI-20 URL
CC-BY 4.0"
REFERENCES,0.5732323232323232,"PCdes
Retrieval
Molecule-Text
Retrieval
Used for evaluating ac-
curacy in molecule-text
retrieval tasks."
REFERENCES,0.5740740740740741,"PCdes URL
CC-BY 4.0"
REFERENCES,0.5749158249158249,"MoMu
Retrieval
Molecule-Text
Retrieval
Dataset for molecule-
text interaction and re-
trieval evaluation."
REFERENCES,0.5757575757575758,"MoMu URL
CC-BY 4.0"
REFERENCES,0.5765993265993266,"Mol-
Instructions
Generation
Molecule Gen-
eration
Includes tasks such as
molecule
generation
from
descriptions,
reagent prediction, etc."
REFERENCES,0.5774410774410774,"Mol-
Instructions
URL"
REFERENCES,0.5782828282828283,CC-BY 4.0
REFERENCES,0.5791245791245792,"We present the details of the Molecule Generation Datasets below:
519"
REFERENCES,0.57996632996633,"• Mol-Instructions [12]: This dataset includes tasks such as caption-guided molecule generation,
520"
REFERENCES,0.5808080808080808,"reagent prediction, forward reaction prediction, and retrosynthesis. It is used to evaluate the
521"
REFERENCES,0.5816498316498316,"model’s ability to generate molecular structures based on textual descriptions and other related
522"
REFERENCES,0.5824915824915825,"tasks.
523"
REFERENCES,0.5833333333333334,"We summarize the datasets used for evaluating UniMoT in Table 7. It encompasses various types
524"
REFERENCES,0.5841750841750841,"of datasets, including those for classification, regression, captioning, retrieval, and generation tasks.
525"
REFERENCES,0.585016835016835,"Each dataset is described in terms of its type, tasks it supports, a brief description of its content, its
526"
REFERENCES,0.5858585858585859,"URL for access, and the license under which it is distributed. The licenses vary, with some datasets
527"
REFERENCES,0.5867003367003367,"being in the public domain and others under CC-BY 4.0 license.
528"
REFERENCES,0.5875420875420876,"C
Details of Training
529"
REFERENCES,0.5883838383838383,"Stage-1: Causal Q-Former Pretraining.
During Stage-1, we only connect the molecule encoder
530"
REFERENCES,0.5892255892255892,"and the Causal Q-Former, leaving out other blocks. We leverage the pretrained molecule encoder from
531"
REFERENCES,0.5900673400673401,"MoleculeSTM [31], which has undergone extensive contrastive learning with molecule-text pairs.
532"
REFERENCES,0.5909090909090909,"We utilize the PubChem dataset [18] for pretraining, keeping the molecule encoder frozen while
533"
REFERENCES,0.5917508417508418,"updating only the Causal Q-Former. Both queries and text serve as input to the Causal Q-Former,
534"
REFERENCES,0.5925925925925926,"while only queries serve as input in subsequent stages. Inspired by BLIP-2 [23], we employ three
535"
REFERENCES,0.5934343434343434,"tailored objectives – Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM),
536"
REFERENCES,0.5942760942760943,"and Molecule-grounded Text Generation (MTG) – for the pretraining of the Causal Q-Former, as
537"
REFERENCES,0.5951178451178452,"detailed in Appendix A.
538"
REFERENCES,0.5959595959595959,"The dimension of molecule features is set to 300. We use 16 queries, each with a dimension of 768.
539"
REFERENCES,0.5968013468013468,"The size of Z (16 × 768) is much smaller than the size of molecule features X (e.g., 150 × 300). The
540"
REFERENCES,0.5976430976430976,"Q-former is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05,
541"
REFERENCES,0.5984848484848485,"and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set
542"
REFERENCES,0.5993265993265994,"to 64. The computational overhead for this pretraining is 20 GPU hours on 4 NVIDIA A100 GPUs.
543"
REFERENCES,0.6001683501683501,"Stage-2: Molecule Tokenizer Pretraining.
We connect the Causal Q-Former with the subsequent
544"
REFERENCES,0.601010101010101,"blocks and train the molecule tokenizer using the objective defined in Equation (2). Following
545"
REFERENCES,0.6018518518518519,"the approach of RetMol [49], we utilize SMILES strings [50] to represent molecules, and employ
546"
REFERENCES,0.6026936026936027,"the pretrained ChemFormer [17] as the generative model. Specifically, we leverage the SMILES
547"
REFERENCES,0.6035353535353535,"encoder and SMILES decoder components provided by ChemFormer. We utilize PubChem [18]
548"
REFERENCES,0.6043771043771043,"and CheBI-20 [11] datasets, keeping the molecule encoder, SMILES encoder, and SMILES decoder
549"
REFERENCES,0.6052188552188552,"frozen, while updating the Causal Q-Former, codebook, and adapter. Once optimized, the molecule
550"
REFERENCES,0.6060606060606061,"tokenizer remains unchanged throughout the subsequent stages.
551"
REFERENCES,0.6069023569023569,"The molecule codebook size is set to K = 2048, and the dimension of codebook embedding is 768.
552"
REFERENCES,0.6077441077441077,"The tokenizer is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of
553"
REFERENCES,0.6085858585858586,"0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size
554"
REFERENCES,0.6094276094276094,"is set to 64. The computational overhead for this pretraining is 40 GPU hours on 4 NVIDIA A100
555"
REFERENCES,0.6102693602693603,"GPUs.
556"
REFERENCES,0.6111111111111112,"Stage-3: Unified Molecule-Text Pretraining.
We connect the molecule tokenizer with the LLM
557"
REFERENCES,0.6119528619528619,"and employ the LM objective defined in Equation (3) to pretrain the LLM. We utilize LLaMA [44] as
558"
REFERENCES,0.6127946127946128,"the default LLM. To construct the unified molecule-text vocabulary, we merge 2048 molecule codes
559"
REFERENCES,0.6136363636363636,"with the original text vocabulary. Pretraining the LLM involves molecule-to-text autoregression
560"
REFERENCES,0.6144781144781145,"and text-to-molecule autoregression, aimed at enhancing UniMoT’s multi-modal comprehension
561"
REFERENCES,0.6153198653198653,"and generation capabilities. We utilize datasets PubChem [18], CheBI-20 [11], PCdes [58], and
562"
REFERENCES,0.6161616161616161,"MoMu [39] for this purpose. To enhance efficiency, we train the LLM using LoRA tuning [14].
563"
REFERENCES,0.617003367003367,"The multi-modal LLM is pretrained for 10 epochs. We adopt the AdamW optimizer with a weight
564"
REFERENCES,0.6178451178451179,"decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The
565"
REFERENCES,0.6186868686868687,"batch size is set to 32. The computational overhead for this pretraining is 50 GPU hours on 4 NVIDIA
566"
REFERENCES,0.6195286195286195,"A100 GPUs. To reduce CUDA memory usage, we integrate LoRA with the parameters set to r = 8,
567"
REFERENCES,0.6203703703703703,"α = 32, and dropout = 0.1. This integration is applied to the k_proj, v_proj, q_proj, and o_proj
568"
REFERENCES,0.6212121212121212,"modules.
569"
REFERENCES,0.6220538720538721,"Table 8: Instruction samples for comprehension and generation tasks: molecular property prediction,
molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction,
forward reaction prediction, and retrosynthesis."
REFERENCES,0.622895622895623,"Task
Instruction"
REFERENCES,0.6237373737373737,"Molecular Property Predic-
tion (Regression)
Instruction: Could you give me the LUMO energy value of this molecule?
(Optional: The SMILES sequence is: SMILES)
Output: 0.0576."
REFERENCES,0.6245791245791246,"Molecular Property Predic-
tion (Classification)
Instruction: Evaluate whether the given molecule is able to enter the blood-brain barrier.
(Optional: The SMILES sequence is: SMILES)
Output: Yes."
REFERENCES,0.6254208754208754,"Molecule Captioning
Instruction: Could you give me a brief overview of this molecule?
(Optional: The SMILES sequence is: SMILES)
Output: The molecule is an indole phytoalexin that ..."
REFERENCES,0.6262626262626263,"Molecule-Text Retrieval
Instruction: Retrieve relevant text for the given molecule.
(Optional: The SMILES sequence is: SMILES)
Output: The molecule is associated with ..."
REFERENCES,0.627104377104377,"Caption-Guided Molecule
Generation
Instruction: Create a molecule with the structure as described: The molecule is a primary
arylamine that ...
Output: SMILES of the molecule."
REFERENCES,0.6279461279461279,"Reagent Prediction
Instruction: Please provide possible reagents based on the following chemical reaction.
<REACTANT A> <REACTANT B> ...
» <PRODUCTs>
Output: SMILES of the reagents."
REFERENCES,0.6287878787878788,"Forward Reaction Predic-
tion
Instruction: With the provided reactants and reagents, propose a potential product:
<REACTANT A> <REACTANT B> ...
<REAGENT A> <REAGENT B> ...
Output: SMILES of the products."
REFERENCES,0.6296296296296297,"Retrosynthesis
Instruction: Please suggest potential reactants used in the synthesis of the product:
<PRODUCTs>
Output: SMILES of the reactants and reagents."
REFERENCES,0.6304713804713805,"Stage-4: Task-Specific Instruction Tuning.
We perform instruction tuning to align UniMoT with
570"
REFERENCES,0.6313131313131313,"human instructions through supervised fine-tuning on seven tasks: molecular property prediction,
571"
REFERENCES,0.6321548821548821,"molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction,
572"
REFERENCES,0.632996632996633,"forward reaction prediction, and retrosynthesis. For the molecular property prediction task, we
573"
REFERENCES,0.6338383838383839,"utilize the quantum mechanics properties dataset [12] for regression prediction and the MoleculeNet
574"
REFERENCES,0.6346801346801347,"datasets [53] for property classification. For the molecule captioning and molecule-text retrieval
575"
REFERENCES,0.6355218855218855,"tasks, we employ datasets PubChem [18], CheBI-20 [11], PCdes [58], and MoMu [39]. For the
576"
REFERENCES,0.6363636363636364,"remaining tasks, we utilize the Mol-Instructions dataset [12] to conduct instruction tuning. We
577"
REFERENCES,0.6372053872053872,"fine-tune UniMoT for 10 epochs on each task using the same optimizer, learning rate scheduler, and
578"
REFERENCES,0.6380471380471381,"LoRA configurations as in Stage-3 pretraining. Instruction samples for comprehension and generation
579"
REFERENCES,0.6388888888888888,"tasks are shown in Table 8.
580"
REFERENCES,0.6397306397306397,"We have summarized the detailed training hyperparameters of UniMoT in Table 9.
581"
REFERENCES,0.6405723905723906,"D
Details and More Results of Experiments
582"
REFERENCES,0.6414141414141414,"Molecular Property Prediction Task.
Property prediction aims to anticipate a molecule’s intrinsic
583"
REFERENCES,0.6422558922558923,"physical and chemical properties based on its structural or sequential characteristics. In the regression
584"
REFERENCES,0.6430976430976431,"task, we conduct experiments on the quantum mechanics properties dataset QM9 [12], where the
585"
REFERENCES,0.6439393939393939,"objective is to predict key quantum mechanics properties of a given molecule, such as HUMO, LUMO,
586"
REFERENCES,0.6447811447811448,"and the HUMO-LUMO gap. We compare UniMoT against several baselines, including Alpaca [42],
587"
REFERENCES,0.6456228956228957,"Baize [55], LLaMA2-7B [44], Vicuna-13B [7], Mol-Instructions [12], and InstructMol [6]. Mean
588"
REFERENCES,0.6464646464646465,"Absolute Error (MAE) serves as our evaluation metric. The performance of the regression task on the
589"
REFERENCES,0.6473063973063973,"QM9 dataset is presented in Table 10. Compared to previous single-modal instruction-tuned LLMs
590"
REFERENCES,0.6481481481481481,"and molecular LLMs, UniMoT exhibits further improvement on the regression task, showcasing its
591"
REFERENCES,0.648989898989899,"fundamental comprehension abilities in molecular contexts.
592"
REFERENCES,0.6498316498316499,"Molecule Captioning Task.
The molecule captioning task involves generating a comprehensive
593"
REFERENCES,0.6506734006734006,"description of a molecule. For this task, we compare UniMoT with several baselines: MolT5 [11],
594"
REFERENCES,0.6515151515151515,Table 9: The detailed training hyperparameters of UniMoT.
REFERENCES,0.6523569023569024,"Configuration
Q-Former Pretraining
Tokenizer Pretraining
LLM Pretraining"
REFERENCES,0.6531986531986532,"Molecule Encoder
MoleculeSTM
MoleculeSTM
MoleculeSTM
SMILES Encoder
-
ChemFormer
ChemFormer
SMILES Decoder
-
ChemFormer
ChemFormer
LLM Base
-
-
LLaMA2-7B
Epoch
50
50
10
Optimizer
AdamW
AdamW
AdamW
Codebook Size
2048
2048
2048
Number of Queries
16
16
16
Query Embedding Dim.
768
768
768
Molecule Embedding Dim.
300
300
300
Batch Size
64
64
32
Minimal Learning Rate
1e-5
1e-5
1e-5
Learning Rate Scheduler
Cosine
Cosine
Cosine
Warm-up Steps
1000
1000
1000
Weight Decay
0.05
0.05
0.05
LoRA Config
-
-
r = 8, α = 32, dropout = 0.1
Precision
bfloat16
bfloat16
bfloat16
GPU Usage
4 NVIDIA A100
4 NVIDIA A100
4 NVIDIA A100
Training Time
20 GPU hours
40 GPU hours
50 GPU hours"
REFERENCES,0.6540404040404041,"Table 10: Mean Absolute Error (MAE) of molecular property prediction task (regression) on the QM9
dataset. Bold indicates the best performance and underline indicates the second best performance.
∆ϵ is the HOMO-LUMO energy gap."
REFERENCES,0.6548821548821548,"Model
HOMO↓
LUMO↓
∆ϵ ↓
AVG↓"
REFERENCES,0.6557239057239057,"Alpaca (LLaMA-7B) [42]
-
-
-
322.109
Baize (LLaMA-7B) [55]
-
-
-
261.343
LLaMA2-7B [44]
0.7367
0.8641
0.5152
0.7510
Vicuna-13B [7]
0.7135
3.6807
1.5407
1.9783
Mol-Instructions (LLaMA-7B) [12]
0.0210
0.0210
0.0203
0.0210
InstructMol (Vicuna-7B) [6]
0.0048
0.0050
0.0061
0.0050"
REFERENCES,0.6565656565656566,"UniMoT (LLaMA2-7B)
0.0042
0.0047
0.0055
0.0049"
REFERENCES,0.6574074074074074,"MoMu [39], InstructMol [6], MolCA [33], and 3D-MoLM [25]. We adopt BLEU [37], ROUGE [27],
595"
REFERENCES,0.6582491582491582,"and METEOR [1] as the evaluation metrics. The performance of UniMoT in the molecule captioning
596"
REFERENCES,0.6590909090909091,"task on the CheBI-20 dataset is presented in Table 11. Some concrete examples of molecule captioning
597"
REFERENCES,0.6599326599326599,"task are presented in Table 12. From the results, it is evident that UniMoT consistently outperforms
598"
REFERENCES,0.6607744107744108,"the baselines by a significant margin. These results underscore the effectiveness of the molecule
599"
REFERENCES,0.6616161616161617,"tokenizer in providing molecule tokens with high-level molecular and textual information, thus
600"
REFERENCES,0.6624579124579124,"enhancing molecule comprehension.
601"
REFERENCES,0.6632996632996633,"Molecule-Text Retrieval Task.
The molecule-text retrieval task involves using a molecule to
602"
REFERENCES,0.6641414141414141,"retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several
603"
REFERENCES,0.664983164983165,"baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D-
604"
REFERENCES,0.6658249158249159,"MoLM [25]. We report the performance of retrieval using a batch of 64 random samples and the
605"
REFERENCES,0.6666666666666666,"entire test set, evaluated with the metrics of Accuracy and Recall@20. We use the checkpoint
606"
REFERENCES,0.6675084175084175,"from Stage-1 of pretraining. Performance on the PCdes and MoMu datasets is shown in Table 13.
607"
REFERENCES,0.6683501683501684,"UniMoT demonstrates superior performance over the baselines on molecule-text retrieval, particularly
608"
REFERENCES,0.6691919191919192,"in molecule-to-text retrieval. This demonstrates that UniMoT has learned fine-grained alignment
609"
REFERENCES,0.67003367003367,"between molecules and text, and it can understand molecule-text interactions through the introduction
610"
REFERENCES,0.6708754208754208,"of the Causal Q-Former.
611"
REFERENCES,0.6717171717171717,"Molecule Generation Tasks.
Molecule generation tasks include caption-guided molecule genera-
612"
REFERENCES,0.6725589225589226,"tion, reagent prediction, forward reaction prediction, and retrosynthesis.
613"
REFERENCES,0.6734006734006734,"Table 11: Performance (%) of molecule captioning task on the CheBI-20 dataset. Bold indicates the
best performance and underline indicates the second best performance."
REFERENCES,0.6742424242424242,"Model
BLEU-2↑
BLEU-4↑
ROUGE-1↑
ROUGE-2↑
ROUGE-L↑
METEOR↑"
REFERENCES,0.6750841750841751,"T5-Small [38]
50.1
41.5
60.2
44.6
54.5
53.2
T5-Base [38]
51.1
42.3
60.7
45.1
55.0
53.9
T5-Large [38]
55.8
46.7
63.0
47.8
56.9
58.6
MolT5-Small (T5-Small) [11]
51.9
43.6
62.0
46.9
56.3
55.1
MolT5-Base (T5-Base) [11]
54.0
45.7
63.4
48.5
57.8
56.9
MolT5-Large (T5-Large) [11]
59.4
50.8
65.4
51.0
59.4
61.4
MoMu-Small (T5-Small) [39]
53.2
44.5
-
-
56.4
55.7
MoMu-Base (T5-Base) [39]
54.9
46.2
-
-
57.5
57.6
MoMu-Large (T5-Large) [39]
59.9
51.5
-
-
59.3
59.7
InstructMol (Vicuna-7B) [6]
47.5
37.1
56.6
39.4
50.2
50.9
MolCA (OPT-125M) [33]
61.6
52.9
67.4
53.3
61.5
63.9
MolCA (OPT-1.3B) [33]
63.9
55.5
69.7
55.8
63.6
66.9"
REFERENCES,0.6759259259259259,"UniMoT (LLaMA2-7B)
66.4
58.3
72.2
58.4
66.4
70.3"
REFERENCES,0.6767676767676768,"• Caption-guided molecule generation involves creating molecular structures from textual descrip-
614"
REFERENCES,0.6776094276094277,"tions, leveraging NLP and cheminformatics to interpret and translate descriptions into chemical
615"
REFERENCES,0.6784511784511784,"structures.
616"
REFERENCES,0.6792929292929293,"• Reagent prediction focuses on identifying suitable reagents for given reactants and desired
617"
REFERENCES,0.6801346801346801,"products, optimizing synthetic routes.
618"
REFERENCES,0.680976430976431,"• Forward reaction prediction forecasts probable products from specific reactants and reagents,
619"
REFERENCES,0.6818181818181818,"using knowledge of chemical reactivity.
620"
REFERENCES,0.6826599326599326,"• Retrosynthesis deconstructs target molecules into simpler starting materials.
621"
REFERENCES,0.6835016835016835,"In molecule generation tasks, evaluating the quality of generated molecules involves several metrics
622"
REFERENCES,0.6843434343434344,"that measure different aspects of similarity and validity.
623"
REFERENCES,0.6851851851851852,"• Exact Match checks if the generated molecule is identical to the target molecule, offering a
624"
REFERENCES,0.686026936026936,"stringent criterion for precise replication but potentially overlooking chemically similar variants.
625"
REFERENCES,0.6868686868686869,"• The BLEU score [37], adapted from machine translation, measures the overlap of n-grams (short
626"
REFERENCES,0.6877104377104377,"sequences of atoms or bonds) between generated and target molecules, thus assessing partial
627"
REFERENCES,0.6885521885521886,"similarities.
628"
REFERENCES,0.6893939393939394,"• Levenshtein Distance [22] evaluates the minimum number of edits needed to transform the
629"
REFERENCES,0.6902356902356902,"generated molecule into the target, providing insight into structural changes required.
630"
REFERENCES,0.6910774410774411,"• RDKit [20], MACCS [10], and Morgan [36] Fingerprint Similarities compare the generated and
631"
REFERENCES,0.6919191919191919,"target molecules based on various molecular fingerprinting methods, which capture different
632"
REFERENCES,0.6927609427609428,"aspects of molecular structure and properties.
633"
REFERENCES,0.6936026936026936,"• The Validity [19] metric assesses the proportion of chemically valid molecules generated,
634"
REFERENCES,0.6944444444444444,"ensuring that the output consists of plausible chemical structures.
635"
REFERENCES,0.6952861952861953,"Together, these metrics offer a comprehensive evaluation framework, balancing exact matches with
636"
REFERENCES,0.6961279461279462,"structural and chemical validity.
637"
REFERENCES,0.696969696969697,"E
Limitations
638"
REFERENCES,0.6978114478114478,"While UniMoT demonstrates considerable advancements in unifying molecule and text modalities
639"
REFERENCES,0.6986531986531986,"for comprehensive understanding and generation tasks, several limitations must be acknowledged.
640"
REFERENCES,0.6994949494949495,"Although UniMoT exhibits strong performance in molecule-to-text and text-to-molecule tasks, it has
641"
REFERENCES,0.7003367003367004,"not been extensively tested on more complex molecule generation tasks such as molecule editing,
642"
REFERENCES,0.7011784511784511,"which require precise modifications to molecular structures. Future work could explore extending
643"
REFERENCES,0.702020202020202,"UniMoT’s capabilities to handle such sophisticated molecular manipulations.
644"
REFERENCES,0.7028619528619529,"Due to the scarcity of annotated data in the molecular field, the training of UniMoT is less extensive
645"
REFERENCES,0.7037037037037037,"compared to fields like computer vision. This limitation restricts the model’s ability to fully learn and
646"
REFERENCES,0.7045454545454546,"generalize from diverse molecular structures and properties. In contrast, the visual domain benefits
647"
REFERENCES,0.7053872053872053,"Table 12: Examples of molecule captioning task on the ChEBI-20 dataset. We highlight in blue the
text that accurately describes the molecule structures in the generated caption, ensuring alignment
with the ground truth."
REFERENCES,0.7062289562289562,"Molecule
Generated Molecule Caption
Ground Truth"
REFERENCES,0.7070707070707071,"The molecule is an optically active
form of phenylalaninate having D-
configuration. It is a conjugate base
of a D-phenylalanine. It is an enan-
tiomer of a L-phenylalaninate."
REFERENCES,0.7079124579124579,"The molecule is the D-enantiomer
of phenylalaninate. It is a conjugate
base of a D-phenylalanine. It is an
enantiomer of a L-phenylalaninate."
REFERENCES,0.7087542087542088,"The molecule is an ammonium ion
that is the conjugate acid of 2-
phenylpropylamine arising from pro-
tonation of the primary amino func-
tion; major species at pH 7.3. It
has a role as a human metabolite,
an Escherichia coli metabolite and a
mouse metabolite. It is a conjugate
acid of a 2-phenylpropylamine."
REFERENCES,0.7095959595959596,"The molecule is the cation obtained
by protonation of the amino group
of 2-phenylethylamine. It has a role
as a human metabolite and an Es-
cherichia coli metabolite. It is a con-
jugate acid of a 2-phenylethylamine."
REFERENCES,0.7104377104377104,"The molecule is an enamide ob-
tained by the carboxy group of
trans-cinnamic acid with the sec-
ondary amino group of (2S,5R)-
1,2,5-trimethylpiperazine. It has a
role as an Aspergillus metabolite. It
is an alkaloid, a N-acylpiperazine,
an enamide and a tertiary carboxam-
ide. It derives from a trans-cinnamic
acid."
REFERENCES,0.7112794612794613,"The molecule is an enamide ob-
tained
by
formal
condensation
of the carboxy group of trans-
cinnamic acid with the secondary
amino
group
of
(2R,5R)-1,2,5-
trimethylpiperazine. It has a role as
an Aspergillus metabolite. It is a N-
acylpiperazine, a N-alkylpiperazine,
an alkaloid, an enamide and a ter-
tiary carboxamide. It derives from a
trans-cinnamic acid."
REFERENCES,0.7121212121212122,"The molecule is an (omega-1)-
hydroxy fatty acid ascaroside ob-
tained by formal condensation of the
alcoholic hydroxy group of (10R)-
10-hydroxylauric acid with ascary-
lopyranose (the alpha anomer). It
is a metabolite of the nematode
Caenorhabditis elegans. It has a role
as a Caenorhabditis elegans metabo-
lite. It is a monocarboxylic acid and
an (omega-1)-hydroxy fatty acid as-
caroside. It derives from an (11R)-
11-hydroxylauric acid. It is a conju-
gate acid of an ascr18(1-)."
REFERENCES,0.7129629629629629,"The molecule is an (omega-1)-
hydroxy fatty acid ascaroside ob-
tained by formal condensation of the
alcoholic hydroxy group of (10R)-
10-hydroxyundecanoic acid with as-
carylopyranose (the alpha anomer).
It is a metabolite of the nema-
tode Caenorhabditis elegans.
It
is a monocarboxylic acid and an
(omega-1)-hydroxy fatty acid as-
caroside. It derives from a (10R)-
10-hydroxyundecanoic acid. It is a
conjugate acid of an ascrblue18(1-)."
REFERENCES,0.7138047138047138,"The molecule is a 2-oxo monocar-
boxylic acid that is pyruvic acid in
which one of the methyl hydrogens
is substituted by a 4-vinylcyclohex-
2-en-1-yl group. It has a role as a
plant metabolite. It derives from a
pyruvic acid. It is a conjugate acid
of a 4-[(1E)-4-vinylcyclohex-2-en-
1-yl]pyruvate."
REFERENCES,0.7146464646464646,"The molecule is a 2-oxo monocar-
boxylic acid that is pyruvic acid in
which one of the methyl hydrogens
has been replaced by a methylenecy-
clopropyl group. It has a role as a rat
metabolite and a xenobiotic metabo-
lite. It is a 2-oxo monocarboxylic
acid, a member of cyclopropanes
and an olefinic compound. It derives
from a pyruvic acid."
REFERENCES,0.7154882154882155,"Table 13: Accuracy (%) of molecule-text retrieval task on the PCdes and MoMu datasets. Bold
indicates the best performance and underline indicates the second best performance. We report the
performance of retrieval using a batch of 64 random samples and the entire test set."
REFERENCES,0.7163299663299664,(a) Accuracy (%) of molecule-text retrieval task on the PCdes dataset.
REFERENCES,0.7171717171717171,"Model
Retrieval in batch
Retrieval in test set"
REFERENCES,0.718013468013468,"M2T (%)
T2M (%)
M2T (%)
T2M (%)"
REFERENCES,0.7188552188552189,"Sci-BERT [2]
62.6
61.8
60.7
60.8
KV-PLM [58]
77.9
65.0
75.9
64.3
MoMu (Sci-BERT) [39]
80.6
77.0
79.1
75.5
MoMu (KV-PLM) [39]
81.1
80.2
80.2
79.0
MoleculeSTM [31]
86.2
83.9
84.6
85.1
MolCA (OPT-1.3B) [33]
91.4
88.4
90.5
87.6
3D-MoLM (LLaMA2-7B) [25]
92.3
89.6
91.2
88.5"
REFERENCES,0.7196969696969697,"UniMoT (LLaMA2-7B)
92.6
89.4
91.6
88.3"
REFERENCES,0.7205387205387206,(b) Accuracy (%) of molecule-text retrieval task on the MoMu dataset.
REFERENCES,0.7213804713804713,"Model
Retrieval in batch
Retrieval in test set"
REFERENCES,0.7222222222222222,"M2T (%)
T2M (%)
M2T (%)
T2M (%)"
REFERENCES,0.7230639730639731,"Sci-BERT [2]
1.4
1.6
0.3
0.3
KV-PLM [58]
1.5
1.3
0.5
0.3
MoMu (Sci-BERT) [39]
45.7
40.0
43.3
43.4
MoMu (KV-PLM) [39]
46.2
38.5
43.7
43.5
MoleculeSTM [31]
81.8
81.9
75.8
74.5
MolCA (OPT-1.3B) [33]
83.7
84.3
88.6
87.3
3D-MoLM (LLaMA2-7B) [25]
84.9
85.4
89.9
88.7"
REFERENCES,0.7239057239057239,"UniMoT (LLaMA2-7B)
85.4
85.6
90.3
89.0"
REFERENCES,0.7247474747474747,"from abundant labeled datasets, allowing for more comprehensive training and better performance.
648"
REFERENCES,0.7255892255892256,"Addressing this data scarcity in the molecular domain is crucial for improving UniMoT’s training
649"
REFERENCES,0.7264309764309764,"effectiveness and overall capabilities.
650"
REFERENCES,0.7272727272727273,"The current empirical evaluations, though extensive, are primarily conducted on standard datasets
651"
REFERENCES,0.7281144781144782,"and benchmarks; expanding the evaluation to a broader array of datasets and real-world scenarios
652"
REFERENCES,0.7289562289562289,"will provide a more comprehensive understanding of the model’s robustness and generalizability.
653"
REFERENCES,0.7297979797979798,"F
Broader Impacts
654"
REFERENCES,0.7306397306397306,"The development of UniMoT, a unified model for molecule and text modalities, has significant
655"
REFERENCES,0.7314814814814815,"potential to positively impact various fields. UniMoT can streamline the drug discovery process by
656"
REFERENCES,0.7323232323232324,"enabling efficient molecule generation and optimization based on textual descriptions. In material
657"
REFERENCES,0.7331649831649831,"science, it can aid in discovering new materials with desirable properties. Additionally, UniMoT
658"
REFERENCES,0.734006734006734,"can enhance research collaboration between chemists, biologists, and data scientists by integrating
659"
REFERENCES,0.7348484848484849,"molecular and textual data, leading to comprehensive research insights and innovative solutions.
660"
REFERENCES,0.7356902356902357,"This paper does not pose any ethical concerns. The study does not involve human subjects and follows
661"
REFERENCES,0.7365319865319865,"proper procedures for data set releases. There are no potentially harmful insights, methodologies, or
662"
REFERENCES,0.7373737373737373,"applications. Additionally, there are no conflicts of interest or sponsorship concerns. Discrimination,
663"
REFERENCES,0.7382154882154882,"bias, and fairness issues are not applicable. Privacy and security matters have been appropriately
664"
REFERENCES,0.7390572390572391,"addressed, legal compliance has been maintained, and research integrity has been upheld.
665"
REFERENCES,0.73989898989899,"NeurIPS Paper Checklist
666"
CLAIMS,0.7407407407407407,"1. Claims
667"
CLAIMS,0.7415824915824916,"Question: Do the main claims made in the abstract and introduction accurately reflect the
668"
CLAIMS,0.7424242424242424,"paper’s contributions and scope?
669"
CLAIMS,0.7432659932659933,"Answer: [Yes]
670"
CLAIMS,0.7441077441077442,"Justification: We conduct extensive experiments to verify our claims.
671"
CLAIMS,0.7449494949494949,"Guidelines:
672"
CLAIMS,0.7457912457912458,"• The answer NA means that the abstract and introduction do not include the claims
673"
CLAIMS,0.7466329966329966,"made in the paper.
674"
CLAIMS,0.7474747474747475,"• The abstract and/or introduction should clearly state the claims made, including the
675"
CLAIMS,0.7483164983164983,"contributions made in the paper and important assumptions and limitations. A No or
676"
CLAIMS,0.7491582491582491,"NA answer to this question will not be perceived well by the reviewers.
677"
CLAIMS,0.75,"• The claims made should match theoretical and experimental results, and reflect how
678"
CLAIMS,0.7508417508417509,"much the results can be expected to generalize to other settings.
679"
CLAIMS,0.7516835016835017,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
680"
CLAIMS,0.7525252525252525,"are not attained by the paper.
681"
LIMITATIONS,0.7533670033670034,"2. Limitations
682"
LIMITATIONS,0.7542087542087542,"Question: Does the paper discuss the limitations of the work performed by the authors?
683"
LIMITATIONS,0.7550505050505051,"Answer: [Yes]
684"
LIMITATIONS,0.7558922558922558,"Justification: We discuss the limitations in Appendix E.
685"
LIMITATIONS,0.7567340067340067,"Guidelines:
686"
LIMITATIONS,0.7575757575757576,"• The answer NA means that the paper has no limitation while the answer No means that
687"
LIMITATIONS,0.7584175084175084,"the paper has limitations, but those are not discussed in the paper.
688"
LIMITATIONS,0.7592592592592593,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
689"
LIMITATIONS,0.76010101010101,"• The paper should point out any strong assumptions and how robust the results are to
690"
LIMITATIONS,0.7609427609427609,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
691"
LIMITATIONS,0.7617845117845118,"model well-specification, asymptotic approximations only holding locally). The authors
692"
LIMITATIONS,0.7626262626262627,"should reflect on how these assumptions might be violated in practice and what the
693"
LIMITATIONS,0.7634680134680135,"implications would be.
694"
LIMITATIONS,0.7643097643097643,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
695"
LIMITATIONS,0.7651515151515151,"only tested on a few datasets or with a few runs. In general, empirical results often
696"
LIMITATIONS,0.765993265993266,"depend on implicit assumptions, which should be articulated.
697"
LIMITATIONS,0.7668350168350169,"• The authors should reflect on the factors that influence the performance of the approach.
698"
LIMITATIONS,0.7676767676767676,"For example, a facial recognition algorithm may perform poorly when image resolution
699"
LIMITATIONS,0.7685185185185185,"is low or images are taken in low lighting. Or a speech-to-text system might not be
700"
LIMITATIONS,0.7693602693602694,"used reliably to provide closed captions for online lectures because it fails to handle
701"
LIMITATIONS,0.7702020202020202,"technical jargon.
702"
LIMITATIONS,0.7710437710437711,"• The authors should discuss the computational efficiency of the proposed algorithms
703"
LIMITATIONS,0.7718855218855218,"and how they scale with dataset size.
704"
LIMITATIONS,0.7727272727272727,"• If applicable, the authors should discuss possible limitations of their approach to
705"
LIMITATIONS,0.7735690235690236,"address problems of privacy and fairness.
706"
LIMITATIONS,0.7744107744107744,"• While the authors might fear that complete honesty about limitations might be used by
707"
LIMITATIONS,0.7752525252525253,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
708"
LIMITATIONS,0.7760942760942761,"limitations that aren’t acknowledged in the paper. The authors should use their best
709"
LIMITATIONS,0.7769360269360269,"judgment and recognize that individual actions in favor of transparency play an impor-
710"
LIMITATIONS,0.7777777777777778,"tant role in developing norms that preserve the integrity of the community. Reviewers
711"
LIMITATIONS,0.7786195286195287,"will be specifically instructed to not penalize honesty concerning limitations.
712"
THEORY ASSUMPTIONS AND PROOFS,0.7794612794612794,"3. Theory Assumptions and Proofs
713"
THEORY ASSUMPTIONS AND PROOFS,0.7803030303030303,"Question: For each theoretical result, does the paper provide the full set of assumptions and
714"
THEORY ASSUMPTIONS AND PROOFS,0.7811447811447811,"a complete (and correct) proof?
715"
THEORY ASSUMPTIONS AND PROOFS,0.781986531986532,"Answer: [NA]
716"
THEORY ASSUMPTIONS AND PROOFS,0.7828282828282829,"Justification: The paper does not include theoretical results.
717"
THEORY ASSUMPTIONS AND PROOFS,0.7836700336700336,"Guidelines:
718"
THEORY ASSUMPTIONS AND PROOFS,0.7845117845117845,"• The answer NA means that the paper does not include theoretical results.
719"
THEORY ASSUMPTIONS AND PROOFS,0.7853535353535354,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
720"
THEORY ASSUMPTIONS AND PROOFS,0.7861952861952862,"referenced.
721"
THEORY ASSUMPTIONS AND PROOFS,0.7870370370370371,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
722"
THEORY ASSUMPTIONS AND PROOFS,0.7878787878787878,"• The proofs can either appear in the main paper or the supplemental material, but if
723"
THEORY ASSUMPTIONS AND PROOFS,0.7887205387205387,"they appear in the supplemental material, the authors are encouraged to provide a short
724"
THEORY ASSUMPTIONS AND PROOFS,0.7895622895622896,"proof sketch to provide intuition.
725"
THEORY ASSUMPTIONS AND PROOFS,0.7904040404040404,"• Inversely, any informal proof provided in the core of the paper should be complemented
726"
THEORY ASSUMPTIONS AND PROOFS,0.7912457912457912,"by formal proofs provided in appendix or supplemental material.
727"
THEORY ASSUMPTIONS AND PROOFS,0.7920875420875421,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7929292929292929,"4. Experimental Result Reproducibility
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7937710437710438,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7946127946127947,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7954545454545454,"of the paper (regardless of whether the code and data are provided or not)?
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7962962962962963,"Answer: [Yes]
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7971380471380471,"Justification: We disclose all the information to reproduce the experimental results in
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.797979797979798,"Section 4, Appendix C, and Appendix D.
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7988215488215489,"Guidelines:
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996632996632996,"• The answer NA means that the paper does not include experiments.
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8005050505050505,"• If the paper includes experiments, a No answer to this question will not be perceived
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8013468013468014,"well by the reviewers: Making the paper reproducible is important, regardless of
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8021885521885522,"whether the code and data are provided or not.
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803030303030303,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8038720538720538,"to make their results reproducible or verifiable.
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8047138047138047,"• Depending on the contribution, reproducibility can be accomplished in various ways.
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8055555555555556,"For example, if the contribution is a novel architecture, describing the architecture fully
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8063973063973064,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8072390572390572,"be necessary to either make it possible for others to replicate the model with the same
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8080808080808081,"dataset, or provide access to the model. In general. releasing code and data is often
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089225589225589,"one good way to accomplish this, but reproducibility can also be provided via detailed
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8097643097643098,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8106060606060606,"of a large language model), releasing of a model checkpoint, or other means that are
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8114478114478114,"appropriate to the research performed.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8122895622895623,"• While NeurIPS does not require releasing code, the conference does require all submis-
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8131313131313131,"sions to provide some reasonable avenue for reproducibility, which may depend on the
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.813973063973064,"nature of the contribution. For example
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8148148148148148,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8156565656565656,"to reproduce that algorithm.
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8164983164983165,"(b) If the contribution is primarily a new model architecture, the paper should describe
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8173400673400674,"the architecture clearly and fully.
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"(c) If the contribution is a new model (e.g., a large language model), then there should
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.819023569023569,"either be a way to access this model for reproducing the results or a way to reproduce
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8198653198653199,"the model (e.g., with an open-source dataset or instructions for how to construct
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8207070707070707,"the dataset).
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8215488215488216,"(d) We recognize that reproducibility may be tricky in some cases, in which case
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8223905723905723,"authors are welcome to describe the particular way they provide for reproducibility.
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8232323232323232,"In the case of closed-source models, it may be that access to the model is limited in
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8240740740740741,"some way (e.g., to registered users), but it should be possible for other researchers
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8249158249158249,"to have some path to reproducing or verifying the results.
767"
OPEN ACCESS TO DATA AND CODE,0.8257575757575758,"5. Open access to data and code
768"
OPEN ACCESS TO DATA AND CODE,0.8265993265993266,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
769"
OPEN ACCESS TO DATA AND CODE,0.8274410774410774,"tions to faithfully reproduce the main experimental results, as described in supplemental
770"
OPEN ACCESS TO DATA AND CODE,0.8282828282828283,"material?
771"
OPEN ACCESS TO DATA AND CODE,0.8291245791245792,"Answer: [No]
772"
OPEN ACCESS TO DATA AND CODE,0.82996632996633,"Justification: Once our paper is accepted, we will make the code openly accessible.
773"
OPEN ACCESS TO DATA AND CODE,0.8308080808080808,"Guidelines:
774"
OPEN ACCESS TO DATA AND CODE,0.8316498316498316,"• The answer NA means that paper does not include experiments requiring code.
775"
OPEN ACCESS TO DATA AND CODE,0.8324915824915825,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
776"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"public/guides/CodeSubmissionPolicy) for more details.
777"
OPEN ACCESS TO DATA AND CODE,0.8341750841750841,"• While we encourage the release of code and data, we understand that this might not be
778"
OPEN ACCESS TO DATA AND CODE,0.835016835016835,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
779"
OPEN ACCESS TO DATA AND CODE,0.8358585858585859,"including code, unless this is central to the contribution (e.g., for a new open-source
780"
OPEN ACCESS TO DATA AND CODE,0.8367003367003367,"benchmark).
781"
OPEN ACCESS TO DATA AND CODE,0.8375420875420876,"• The instructions should contain the exact command and environment needed to run to
782"
OPEN ACCESS TO DATA AND CODE,0.8383838383838383,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
783"
OPEN ACCESS TO DATA AND CODE,0.8392255892255892,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
784"
OPEN ACCESS TO DATA AND CODE,0.8400673400673401,"• The authors should provide instructions on data access and preparation, including how
785"
OPEN ACCESS TO DATA AND CODE,0.8409090909090909,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
786"
OPEN ACCESS TO DATA AND CODE,0.8417508417508418,"• The authors should provide scripts to reproduce all experimental results for the new
787"
OPEN ACCESS TO DATA AND CODE,0.8425925925925926,"proposed method and baselines. If only a subset of experiments are reproducible, they
788"
OPEN ACCESS TO DATA AND CODE,0.8434343434343434,"should state which ones are omitted from the script and why.
789"
OPEN ACCESS TO DATA AND CODE,0.8442760942760943,"• At submission time, to preserve anonymity, the authors should release anonymized
790"
OPEN ACCESS TO DATA AND CODE,0.8451178451178452,"versions (if applicable).
791"
OPEN ACCESS TO DATA AND CODE,0.8459595959595959,"• Providing as much information as possible in supplemental material (appended to the
792"
OPEN ACCESS TO DATA AND CODE,0.8468013468013468,"paper) is recommended, but including URLs to data and code is permitted.
793"
OPEN ACCESS TO DATA AND CODE,0.8476430976430976,"6. Experimental Setting/Details
794"
OPEN ACCESS TO DATA AND CODE,0.8484848484848485,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
795"
OPEN ACCESS TO DATA AND CODE,0.8493265993265994,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
796"
OPEN ACCESS TO DATA AND CODE,0.8501683501683501,"results?
797"
OPEN ACCESS TO DATA AND CODE,0.851010101010101,"Answer: [Yes]
798"
OPEN ACCESS TO DATA AND CODE,0.8518518518518519,"Justification: We disclose all the details of our experiments in Appendix C and Appendix D.
799"
OPEN ACCESS TO DATA AND CODE,0.8526936026936027,"Guidelines:
800"
OPEN ACCESS TO DATA AND CODE,0.8535353535353535,"• The answer NA means that the paper does not include experiments.
801"
OPEN ACCESS TO DATA AND CODE,0.8543771043771043,"• The experimental setting should be presented in the core of the paper to a level of detail
802"
OPEN ACCESS TO DATA AND CODE,0.8552188552188552,"that is necessary to appreciate the results and make sense of them.
803"
OPEN ACCESS TO DATA AND CODE,0.8560606060606061,"• The full details can be provided either with the code, in appendix, or as supplemental
804"
OPEN ACCESS TO DATA AND CODE,0.8569023569023569,"material.
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8577441077441077,"7. Experiment Statistical Significance
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8585858585858586,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8594276094276094,"information about the statistical significance of the experiments?
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8602693602693603,"Answer: [No]
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8611111111111112,"Justification: Given the considerable computational resources required for experiments with
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8619528619528619,"LLMs, we adhere to the common practice in the community.
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8627946127946128,"Guidelines:
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8636363636363636,"• The answer NA means that the paper does not include experiments.
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8644781144781145,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8653198653198653,"dence intervals, or statistical significance tests, at least for the experiments that support
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661616161616161,"the main claims of the paper.
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.867003367003367,"• The factors of variability that the error bars are capturing should be clearly stated (for
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8678451178451179,"example, train/test split, initialization, random drawing of some parameter, or overall
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8686868686868687,"run with given experimental conditions).
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8695286195286195,"• The method for calculating the error bars should be explained (closed form formula,
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8703703703703703,"call to a library function, bootstrap, etc.)
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8712121212121212,"• The assumptions made should be given (e.g., Normally distributed errors).
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8720538720538721,"• It should be clear whether the error bar is the standard deviation or the standard error
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.872895622895623,"of the mean.
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8737373737373737,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8745791245791246,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8754208754208754,"of Normality of errors is not verified.
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8762626262626263,"• For asymmetric distributions, the authors should be careful not to show in tables or
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.877104377104377,"figures symmetric error bars that would yield results that are out of range (e.g. negative
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8779461279461279,"error rates).
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8787878787878788,"• If error bars are reported in tables or plots, The authors should explain in the text how
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8796296296296297,"they were calculated and reference the corresponding figures or tables in the text.
832"
EXPERIMENTS COMPUTE RESOURCES,0.8804713804713805,"8. Experiments Compute Resources
833"
EXPERIMENTS COMPUTE RESOURCES,0.8813131313131313,"Question: For each experiment, does the paper provide sufficient information on the com-
834"
EXPERIMENTS COMPUTE RESOURCES,0.8821548821548821,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
835"
EXPERIMENTS COMPUTE RESOURCES,0.882996632996633,"the experiments?
836"
EXPERIMENTS COMPUTE RESOURCES,0.8838383838383839,"Answer: [Yes]
837"
EXPERIMENTS COMPUTE RESOURCES,0.8846801346801347,"Justification: The information regarding compute resources is provided in Appendix C.
838"
EXPERIMENTS COMPUTE RESOURCES,0.8855218855218855,"Guidelines:
839"
EXPERIMENTS COMPUTE RESOURCES,0.8863636363636364,"• The answer NA means that the paper does not include experiments.
840"
EXPERIMENTS COMPUTE RESOURCES,0.8872053872053872,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
841"
EXPERIMENTS COMPUTE RESOURCES,0.8880471380471381,"or cloud provider, including relevant memory and storage.
842"
EXPERIMENTS COMPUTE RESOURCES,0.8888888888888888,"• The paper should provide the amount of compute required for each of the individual
843"
EXPERIMENTS COMPUTE RESOURCES,0.8897306397306397,"experimental runs as well as estimate the total compute.
844"
EXPERIMENTS COMPUTE RESOURCES,0.8905723905723906,"• The paper should disclose whether the full research project required more compute
845"
EXPERIMENTS COMPUTE RESOURCES,0.8914141414141414,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
846"
EXPERIMENTS COMPUTE RESOURCES,0.8922558922558923,"didn’t make it into the paper).
847"
CODE OF ETHICS,0.8930976430976431,"9. Code Of Ethics
848"
CODE OF ETHICS,0.8939393939393939,"Question: Does the research conducted in the paper conform, in every respect, with the
849"
CODE OF ETHICS,0.8947811447811448,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
850"
CODE OF ETHICS,0.8956228956228957,"Answer: [Yes]
851"
CODE OF ETHICS,0.8964646464646465,"Justification: We have carefully reviewed the code of ethics to ensure strict adherence to the
852"
CODE OF ETHICS,0.8973063973063973,"guidelines.
853"
CODE OF ETHICS,0.8981481481481481,"Guidelines:
854"
CODE OF ETHICS,0.898989898989899,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
855"
CODE OF ETHICS,0.8998316498316499,"• If the authors answer No, they should explain the special circumstances that require a
856"
CODE OF ETHICS,0.9006734006734006,"deviation from the Code of Ethics.
857"
CODE OF ETHICS,0.9015151515151515,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
858"
CODE OF ETHICS,0.9023569023569024,"eration due to laws or regulations in their jurisdiction).
859"
BROADER IMPACTS,0.9031986531986532,"10. Broader Impacts
860"
BROADER IMPACTS,0.9040404040404041,"Question: Does the paper discuss both potential positive societal impacts and negative
861"
BROADER IMPACTS,0.9048821548821548,"societal impacts of the work performed?
862"
BROADER IMPACTS,0.9057239057239057,"Answer: [Yes]
863"
BROADER IMPACTS,0.9065656565656566,"Justification: We discuss the broader impacts in Appendix F.
864"
BROADER IMPACTS,0.9074074074074074,"Guidelines:
865"
BROADER IMPACTS,0.9082491582491582,"• The answer NA means that there is no societal impact of the work performed.
866"
BROADER IMPACTS,0.9090909090909091,"• If the authors answer NA or No, they should explain why their work has no societal
867"
BROADER IMPACTS,0.9099326599326599,"impact or why the paper does not address societal impact.
868"
BROADER IMPACTS,0.9107744107744108,"• Examples of negative societal impacts include potential malicious or unintended uses
869"
BROADER IMPACTS,0.9116161616161617,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
870"
BROADER IMPACTS,0.9124579124579124,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
871"
BROADER IMPACTS,0.9132996632996633,"groups), privacy considerations, and security considerations.
872"
BROADER IMPACTS,0.9141414141414141,"• The conference expects that many papers will be foundational research and not tied
873"
BROADER IMPACTS,0.914983164983165,"to particular applications, let alone deployments. However, if there is a direct path to
874"
BROADER IMPACTS,0.9158249158249159,"any negative applications, the authors should point it out. For example, it is legitimate
875"
BROADER IMPACTS,0.9166666666666666,"to point out that an improvement in the quality of generative models could be used to
876"
BROADER IMPACTS,0.9175084175084175,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
877"
BROADER IMPACTS,0.9183501683501684,"that a generic algorithm for optimizing neural networks could enable people to train
878"
BROADER IMPACTS,0.9191919191919192,"models that generate Deepfakes faster.
879"
BROADER IMPACTS,0.92003367003367,"• The authors should consider possible harms that could arise when the technology is
880"
BROADER IMPACTS,0.9208754208754208,"being used as intended and functioning correctly, harms that could arise when the
881"
BROADER IMPACTS,0.9217171717171717,"technology is being used as intended but gives incorrect results, and harms following
882"
BROADER IMPACTS,0.9225589225589226,"from (intentional or unintentional) misuse of the technology.
883"
BROADER IMPACTS,0.9234006734006734,"• If there are negative societal impacts, the authors could also discuss possible mitigation
884"
BROADER IMPACTS,0.9242424242424242,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
885"
BROADER IMPACTS,0.9250841750841751,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
886"
BROADER IMPACTS,0.9259259259259259,"feedback over time, improving the efficiency and accessibility of ML).
887"
SAFEGUARDS,0.9267676767676768,"11. Safeguards
888"
SAFEGUARDS,0.9276094276094277,"Question: Does the paper describe safeguards that have been put in place for responsible
889"
SAFEGUARDS,0.9284511784511784,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
890"
SAFEGUARDS,0.9292929292929293,"image generators, or scraped datasets)?
891"
SAFEGUARDS,0.9301346801346801,"Answer: [NA]
892"
SAFEGUARDS,0.930976430976431,"Justification: The paper poses no such risks.
893"
SAFEGUARDS,0.9318181818181818,"Guidelines:
894"
SAFEGUARDS,0.9326599326599326,"• The answer NA means that the paper poses no such risks.
895"
SAFEGUARDS,0.9335016835016835,"• Released models that have a high risk for misuse or dual-use should be released with
896"
SAFEGUARDS,0.9343434343434344,"necessary safeguards to allow for controlled use of the model, for example by requiring
897"
SAFEGUARDS,0.9351851851851852,"that users adhere to usage guidelines or restrictions to access the model or implementing
898"
SAFEGUARDS,0.936026936026936,"safety filters.
899"
SAFEGUARDS,0.9368686868686869,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
900"
SAFEGUARDS,0.9377104377104377,"should describe how they avoided releasing unsafe images.
901"
SAFEGUARDS,0.9385521885521886,"• We recognize that providing effective safeguards is challenging, and many papers do
902"
SAFEGUARDS,0.9393939393939394,"not require this, but we encourage authors to take this into account and make a best
903"
SAFEGUARDS,0.9402356902356902,"faith effort.
904"
LICENSES FOR EXISTING ASSETS,0.9410774410774411,"12. Licenses for existing assets
905"
LICENSES FOR EXISTING ASSETS,0.9419191919191919,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
906"
LICENSES FOR EXISTING ASSETS,0.9427609427609428,"the paper, properly credited and are the license and terms of use explicitly mentioned and
907"
LICENSES FOR EXISTING ASSETS,0.9436026936026936,"properly respected?
908"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"Answer: [Yes]
909"
LICENSES FOR EXISTING ASSETS,0.9452861952861953,"Justification: The licenses for existing assets are provided in Appendix B.
910"
LICENSES FOR EXISTING ASSETS,0.9461279461279462,"Guidelines:
911"
LICENSES FOR EXISTING ASSETS,0.946969696969697,"• The answer NA means that the paper does not use existing assets.
912"
LICENSES FOR EXISTING ASSETS,0.9478114478114478,"• The authors should cite the original paper that produced the code package or dataset.
913"
LICENSES FOR EXISTING ASSETS,0.9486531986531986,"• The authors should state which version of the asset is used and, if possible, include a
914"
LICENSES FOR EXISTING ASSETS,0.9494949494949495,"URL.
915"
LICENSES FOR EXISTING ASSETS,0.9503367003367004,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
916"
LICENSES FOR EXISTING ASSETS,0.9511784511784511,"• For scraped data from a particular source (e.g., website), the copyright and terms of
917"
LICENSES FOR EXISTING ASSETS,0.952020202020202,"service of that source should be provided.
918"
LICENSES FOR EXISTING ASSETS,0.9528619528619529,"• If assets are released, the license, copyright information, and terms of use in the
919"
LICENSES FOR EXISTING ASSETS,0.9537037037037037,"package should be provided. For popular datasets, paperswithcode.com/datasets
920"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"has curated licenses for some datasets. Their licensing guide can help determine the
921"
LICENSES FOR EXISTING ASSETS,0.9553872053872053,"license of a dataset.
922"
LICENSES FOR EXISTING ASSETS,0.9562289562289562,"• For existing datasets that are re-packaged, both the original license and the license of
923"
LICENSES FOR EXISTING ASSETS,0.9570707070707071,"the derived asset (if it has changed) should be provided.
924"
LICENSES FOR EXISTING ASSETS,0.9579124579124579,"• If this information is not available online, the authors are encouraged to reach out to
925"
LICENSES FOR EXISTING ASSETS,0.9587542087542088,"the asset’s creators.
926"
NEW ASSETS,0.9595959595959596,"13. New Assets
927"
NEW ASSETS,0.9604377104377104,"Question: Are new assets introduced in the paper well documented and is the documentation
928"
NEW ASSETS,0.9612794612794613,"provided alongside the assets?
929"
NEW ASSETS,0.9621212121212122,"Answer: [NA]
930"
NEW ASSETS,0.9629629629629629,"Justification: The paper does not release new assets.
931"
NEW ASSETS,0.9638047138047138,"Guidelines:
932"
NEW ASSETS,0.9646464646464646,"• The answer NA means that the paper does not release new assets.
933"
NEW ASSETS,0.9654882154882155,"• Researchers should communicate the details of the dataset/code/model as part of their
934"
NEW ASSETS,0.9663299663299664,"submissions via structured templates. This includes details about training, license,
935"
NEW ASSETS,0.9671717171717171,"limitations, etc.
936"
NEW ASSETS,0.968013468013468,"• The paper should discuss whether and how consent was obtained from people whose
937"
NEW ASSETS,0.9688552188552189,"asset is used.
938"
NEW ASSETS,0.9696969696969697,"• At submission time, remember to anonymize your assets (if applicable). You can either
939"
NEW ASSETS,0.9705387205387206,"create an anonymized URL or include an anonymized zip file.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713804713804713,"14. Crowdsourcing and Research with Human Subjects
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,"Question: For crowdsourcing experiments and research with human subjects, does the paper
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730639730639731,"include the full text of instructions given to participants and screenshots, if applicable, as
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739057239057239,"well as details about compensation (if any)?
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747474747474747,"Answer: [NA]
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755892255892256,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764309764309764,"Guidelines:
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,"• The answer NA means that the paper does not involve crowdsourcing nor research with
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781144781144782,"human subjects.
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789562289562289,"• Including this information in the supplemental material is fine, but if the main contribu-
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797979797979798,"tion of the paper involves human subjects, then as much detail as possible should be
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806397306397306,"included in the main paper.
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823232323232324,"or other labor should be paid at least the minimum wage in the country of the data
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831649831649831,"collector.
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984006734006734,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"Subjects
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856902356902357,"Question: Does the paper describe potential risks incurred by study participants, whether
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865319865319865,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873737373737373,"approvals (or an equivalent approval/review based on the requirements of your country or
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882154882154882,"institution) were obtained?
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890572390572391,"Answer: [NA]
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98989898989899,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,"Guidelines:
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915824915824916,"• The answer NA means that the paper does not involve crowdsourcing nor research with
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924242424242424,"human subjects.
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932659932659933,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941077441077442,"may be required for any human subjects research. If you obtained IRB approval, you
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949494949494949,"should clearly state this in the paper.
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957912457912458,"• We recognize that the procedures for this may vary significantly between institutions
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966329966329966,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974747474747475,"guidelines for their institution.
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983164983164983,"• For initial submissions, do not include any information that would break anonymity (if
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991582491582491,"applicable), such as the institution conducting the review.
974"
