Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014204545454545455,"Stein variational gradient descent (SVGD) is a deterministic inference algorithm
that evolves a set of particles to ﬁt a target distribution. Despite its computational
efﬁciency, SVGD often underestimates the variance of the target distribution in
high dimensions. In this work we attempt to explain the variance collapse in SVGD.
On the qualitative side, we compare the SVGD update with gradient descent
on the maximum mean discrepancy (MMD) objective; we ﬁnd that the variance
collapse phenomenon relates to the bias from deterministic updates present in the
“driving force” of SVGD, and empirically verify that removal of such bias leads
to more accurate variance estimation. On the quantitative side, we demonstrate
that the variance collapse of SVGD can be accurately predicted in the proportional
asymptotic limit, i.e., when the number of particles n and dimensions d diverge at
the same rate. In particular, for learning high-dimensional isotropic Gaussians, we
derive the exact equilibrium variance for both SVGD and MMD-descent, under
certain empirically veriﬁed near-orthogonality condition on the converged particles,
and conﬁrm that SVGD suffers from the “curse of dimensionality”."
INTRODUCTION,0.002840909090909091,"1
INTRODUCTION"
INTRODUCTION,0.004261363636363636,"A typical challenge in Bayesian learning is to efﬁciently and accurately learn a complex posterior
distribution. Markov Chain Monte Carlo (Robert and Casella, 2013) provides asymptotically accurate
samples, but simulating the chain till convergence can be time-consuming. On the other hand,
variational inference (Wainwright et al., 2008) approximates the intractable target distribution with
parametric families by minimizing the KL divergence, which can be more computationally efﬁcient.
Among a plethora of variational approximations, particle-based inference algorithms are particularly
compelling as they bypass the inherent distributional assumptions on the variational family."
INTRODUCTION,0.005681818181818182,"As a noticeable example, Stein Variational Gradient Descent (SVGD) (Liu and Wang, 2016) is
a deterministic particle-based inference algorithm that iteratively transports the particles by the
functional gradient of KL divergence in the reproducing kernel Hilbert space (RKHS). The functional
gradient takes the form of a kernelized Stein’s operator and only requires access to the unnormalized
target density. Despite the empirical successes of SVGD (Liu, 2017; Haarnoja et al., 2017; Kim
et al., 2018), very few convergence guarantees have been established except in the mean-ﬁeld limit,
i.e., the number of particles n →∞under ﬁxed dimensionality d, for which it has been shown
that the distribution of particles converges to the true invariant solution (Liu, 2017; Lu et al., 2019).
Moreover, it has been observed that as the problem dimensionality d becomes larger, the variance
estimated by SVGD can be much smaller than variance of the target distribution (Zhuo et al., 2017).
This observation, which we refer to as the variance collapse phenomenon (see Figure 1), is highly
undesirable for practitioners due to two reasons: (i) underestimating the variance leads to failure in
explaining the uncertainty of model predictions, which is a key beneﬁt of being Bayesian; (ii) modern
Bayesian inference problems are usually high-dimensional; for instance, training Bayesian neural
networks (BNNs) (MacKay, 1992) requires inferring the posterior distribution of network weights,
which could be more than millions of dimensions in real-world problems (Krizhevsky et al., 2012)."
INTRODUCTION,0.007102272727272727,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008522727272727272,"Our Contribution.
We study the algorithmic bias of SVGD that leads to variance collapse in
high dimensions. We focus on the commonly-used Euclidean distance kernel (e.g., Gaussian RBF
kernel), and provide: (i) qualitative understanding on the pitfall of SVGD by comparing it to another
interacting particle algorithm termed MMD-descent; (ii) quantitative understanding on the variance
underestimation in learning simple target distributions. Our ﬁndings can be summarized as follows."
INTRODUCTION,0.009943181818181818,"• In Section 3 we connect SVGD with MMD-descent, a kernel-based particle inference algorithm
that performs maximum mean discrepancy (MMD) minimization, and empirically show that despite
their similar updates, MMD-descent does not collapse the variance in high dimensions.
• In Section 4 we identify the log derivative driving force as the problematic term in SVGD, and
design experiments to illustrate that the combination of this driving force and the deterministic bias,
i.e. the absence of particle resampling, prevents accurate variance estimation in high dimensions.
• In Section 5 we argue that the proportional asymptotic limit: n, d →∞, d/n →γ ∈(0, ∞), is
more relevant to understanding the variance collapse phenomenon. As an example, we derive the
exact dimension-averaged variance of SVGD and MMD-descent in learning Gaussian distribution
in the proportional limit, under certain concentration assumption which we empirically verify. Our
analysis conﬁrms that variance estimated by SVGD scales inversely with the dimensionality."
BACKGROUND,0.011363636363636364,"2
BACKGROUND"
INTEGRAL PROBABILITY METRIC,0.01278409090909091,"2.1
INTEGRAL PROBABILITY METRIC"
INTEGRAL PROBABILITY METRIC,0.014204545454545454,"To measure the “closeness” between distributions on X ⊆Rd, one may consider the maximum
discrepancy between the target p and sample distribution q over test functions F: DF(p, q) :=
suph∈F Eq[h(x)] −Ep[h(y)], known as the integral probability metric (IPM) (M¨uller, 1997)."
INTEGRAL PROBABILITY METRIC,0.015625,"If F is a unit ball in the reproducing kernel Hilbert space (RKHS) H, the resulting DF is termed the
maximum mean discrepancy (MMD) (Gretton et al., 2012), and its squared value can be evaluated as:"
INTEGRAL PROBABILITY METRIC,0.017045454545454544,"MMD2(p, q) = Ex,x′[k(x, x′)] + Ey,y′k(y, y′) −2Ex,y[k(x, y)]
(1)"
INTEGRAL PROBABILITY METRIC,0.018465909090909092,"where x, x′ ∼p, y, y′ ∼q, and the kernel k : Rd × Rd →R satisﬁes E
p"
INTEGRAL PROBABILITY METRIC,0.019886363636363636,"k(x, x) < ∞."
INTEGRAL PROBABILITY METRIC,0.02130681818181818,"Stein’s Discrepancy.
When integration under the target p is intractable, Stein’s method (Stein et al.,
1972) can be used to construct zero-mean test functions w.r.t. p. For differentiable h in the Stein Class
of p, i.e.,
R"
INTEGRAL PROBABILITY METRIC,0.022727272727272728,"x ∇x(h(x)p(x))dx = 0, the Stein’s discrepancy (Gorham and Mackey, 2015) is given as"
INTEGRAL PROBABILITY METRIC,0.024147727272727272,"DStein
F
(p, q) = suph∈F Eq[h(x)⊤∇x log p(x) + ∇⊤
x h(x)]."
INTEGRAL PROBABILITY METRIC,0.02556818181818182,"Note that the Stein’s discrepancy only involves the score of p and thus the normalization constant
is not required. When h is restricted in the product RKHS Hd with inner product ⟨f, g⟩Hd =
Pd
i=1⟨fi, gi⟩H, the corresponding maximum discrepancy, known as kernel Stein discrepancy (KSD),
can be estimated efﬁciently from samples (Liu et al., 2016; Chwialkowski et al., 2016)."
INTEGRAL PROBABILITY METRIC,0.026988636363636364,"2.2
(DETERMINISTIC) PARTICLE INFERENCE ALGORITHMS"
INTEGRAL PROBABILITY METRIC,0.028409090909090908,"We now consider the approximation of p using particles constructed as follows: starting from the initial
particles X = {xi}n
i=1, we iteratively optimize their positions using the update: xi = xi + η∆(xi),
where η is the step size, and ∆(·) : Rd →Rd is the update direction. Motivated by the SVGD
algorithm, we focus on the setting where the update ∆is deterministic, in which case the particles
may converge to a deterministic ﬁxed point. Note that there exist other particle-based algorithms that
do not ﬁt into this description, such as sequential Monte Carlo (Doucet et al., 2001), and stochastic
variants of the particle gradient descent method (Chen et al., 2018a; Gallego and Insua, 2018), which
our current analysis does not cover (see Appendix A.4 for discussion on these alternative algorithms)."
INTEGRAL PROBABILITY METRIC,0.029829545454545456,"Stein Variational Gradient Descent.
SVGD constructs the update direction ∆as the optimal
perturbation in the RKHS that decreases Kullback-Leibler divergence. In particular, constrain ∆in
RKHS unit ball, and take q = 1"
INTEGRAL PROBABILITY METRIC,0.03125,"n
Pn
i=1 δxi, the update for each particle x is given as:"
INTEGRAL PROBABILITY METRIC,0.032670454545454544,"∆SVGD(x) = Ex′∼q[k(x′, x)∇x′ log p(x′)
|
{z
}
driving force"
INTEGRAL PROBABILITY METRIC,0.03409090909090909,"+ ∇x′k(x′, x)
|
{z
}
repulsive force"
INTEGRAL PROBABILITY METRIC,0.03551136363636364,"] := 1 n n
X"
INTEGRAL PROBABILITY METRIC,0.036931818181818184,"i=1
[S1(xi, x) + S2(xi, x)]. (2)"
INTEGRAL PROBABILITY METRIC,0.03835227272727273,Published as a conference paper at ICLR 2022
INTEGRAL PROBABILITY METRIC,0.03977272727272727,"Intuitively, the log derivative term in the update rule S1(xi, x) := k(xi, x)∇xi log p(xi) corresponds
to a driving force that guides particles towards high likelihood regions, whereas the kernel derivative
term S2(xi, x) := ∇xik(xi, x) provides a repulsive force to prevent the particles from collapsing
into the mode. Typically-used kernels in SVGD include the Gaussian RBF kernel k(x, x′) =
exp(−∥x −x′∥2
2 /2σ2) (Liu and Wang, 2016; Zhuo et al., 2017) and the inverse multi-quadratic
(IMQ) kernel k(x, x′) = 1/
p"
INTEGRAL PROBABILITY METRIC,0.041193181818181816,"1 + ∥x −x′∥2
2/(2σ2) (Gorham and Mackey, 2017)."
CONNECTING SVGD WITH MMD MINIMIZATION,0.04261363636363636,"3
CONNECTING SVGD WITH MMD MINIMIZATION"
CONNECTING SVGD WITH MMD MINIMIZATION,0.04403409090909091,"In this section, we introduce another particle inference algorithm termed MMD-descent, whose update
rule closely relates to that of SVGD. Despite the similarity, we empirically observe MMD-descent
accurately estimates the target variance independent of the dimensionality. By analyzing the similarity
of the update rules and contrast in the algorithmic performances, we identify factors that lead to the
variance collapse of SVGD. In this work we focus on the following class of kernel functions.
Deﬁnition 1. A Euclidean Distance Kernel can be written as: k(x, y) = f
 
∥x −y∥2
2/σ2
."
CONNECTING SVGD WITH MMD MINIMIZATION,0.045454545454545456,"For example, the Gaussian RBF kernel and IMQ kernel both satisfy this deﬁnition. Note that σ is the
tunable bandwidth that usually scales with the distance between particles."
CONNECTING SVGD WITH MMD MINIMIZATION,0.046875,"We now construct a kernel-based particle inference algorithm termed MMD-descent, which draws
inspiration from the kernel herding algorithm (Welling, 2009) and particle gradient descent (Chizat
and Bach, 2018). Instead of greedily reducing the MMD by adding one particle a time, we consider an
update rule similar to SVGD that transport all particles together to approximate the target distribution,
by minimizing the MMD between the particles and the target via gradient descent:"
CONNECTING SVGD WITH MMD MINIMIZATION,0.048295454545454544,"∆MMD(x) = Ey∼p[∇xk(x, y)
|
{z
}
driving force"
CONNECTING SVGD WITH MMD MINIMIZATION,0.04971590909090909,"] + Ex′∼q[−∇xk(x, x′)
|
{z
}
repulsive force ]."
CONNECTING SVGD WITH MMD MINIMIZATION,0.05113636363636364,"This update can be seen as the ﬁnite-particle discretization of the MMD gradient ﬂow (Arbel et al.,
2019). For Euclidean distance kernels we have −∇xk(x, x′) = ∇x′k(x, x′) = S2(x′, x); hence,
MMD-descent and SVGD share the same repulsive force. Furthermore, when k is in the Stein class
of p, the driving force of MMD-descent can be connected SVGD via a simple integration by parts:"
CONNECTING SVGD WITH MMD MINIMIZATION,0.052556818181818184,"Ey∼p[∇xk(x, y)] = −Ey∼p[∇yk(x, y)] = Ey∼p[k(x, y)∇y log p(y)] = Ey∼p[S1(y, x)].
(3)"
CONNECTING SVGD WITH MMD MINIMIZATION,0.05397727272727273,"Therefore, the MMD-descent update for a set of particles {xi}n
i=1 can be equivalently written as:"
CONNECTING SVGD WITH MMD MINIMIZATION,0.05539772727272727,"∆MMD(x) = Ey∼p[S1(y, x)] + 1 n n
X"
CONNECTING SVGD WITH MMD MINIMIZATION,0.056818181818181816,"i=1
S2(xi, x).
(4)"
CONNECTING SVGD WITH MMD MINIMIZATION,0.05823863636363636,"We remark that MMD-descent is not a practical algorithm due to the required integration under p,
which is typically intractable. Instead, the purpose of introducing MMD-descent is to compare the
update with SVGD and understand the cause of the variance collapse phenomenon."
CONNECTING SVGD WITH MMD MINIMIZATION,0.05965909090909091,"SVGD vs. MMD-descent.
By comparing SVGD (2) and MMD-descent (4), we observe that:"
CONNECTING SVGD WITH MMD MINIMIZATION,0.061079545454545456,• SVGD and MMD-descent have identical repulsive force.
CONNECTING SVGD WITH MMD MINIMIZATION,0.0625,"• In MMD-descent, the driving force is integrated under the target distribution p, whereas in SVGD
the expectation is under the current particle distribution q."
CONNECTING SVGD WITH MMD MINIMIZATION,0.06392045454545454,"At the inﬁnite particle limit, p = q is a ﬁxed point for both update rules. In fact, under certain
conditions, uniqueness of this ﬁxed point has been established for both updates in this mean-ﬁeld
limit (Lu et al., 2019; Arbel et al., 2019). However, in the practical setting where the particle size is
not signiﬁcantly greater than the dimensionality, it is not clear if the two algorithms: (i) provide a
reasonable “approximation” to the target distribution; (ii) converge to similar ﬁxed points. Given the
analogous updates, one natural question to ask is: do SVGD and MMD-descent reliably estimate the
target variance in high dimensions, and do they converge to similar solutions?"
CONNECTING SVGD WITH MMD MINIMIZATION,0.06534090909090909,"The empirical answer is in the negative: SVGD and MMD-descent converge to completely different
solutions: SVGD is known to underestimate the marginal variance in high dimensions, even for
simple Gaussian targets (Zhuo et al., 2017). In contrast, MMD-descent does not exhibit the same issue
empirically. Figure 1 illustrates this discrepancy in a Bayesian neural network (BNN) experiment."
CONNECTING SVGD WITH MMD MINIMIZATION,0.06676136363636363,Published as a conference paper at ICLR 2022
CONNECTING SVGD WITH MMD MINIMIZATION,0.06818181818181818,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONNECTING SVGD WITH MMD MINIMIZATION,0.06960227272727272,model output
CONNECTING SVGD WITH MMD MINIMIZATION,0.07102272727272728,HMC (ground truth)
CONNECTING SVGD WITH MMD MINIMIZATION,0.07244318181818182,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONNECTING SVGD WITH MMD MINIMIZATION,0.07386363636363637,model output SVGD
CONNECTING SVGD WITH MMD MINIMIZATION,0.07528409090909091,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONNECTING SVGD WITH MMD MINIMIZATION,0.07670454545454546,model output
CONNECTING SVGD WITH MMD MINIMIZATION,0.078125,MMD descent
CONNECTING SVGD WITH MMD MINIMIZATION,0.07954545454545454,"Figure 1: Comparison of SVGD and MMD-descent in training a two-hidden-layer BNN on synthetic 1D dataset.
The target distribution is approximated via HMC, and 100 particles is used for SVGD and MMD-descent. SVGD
(middle) signiﬁcantly underestimates the target variance, but MMD-descent (right) generates diverse samples."
CONNECTING SVGD WITH MMD MINIMIZATION,0.08096590909090909,"We consider a two-hidden-layer BNN with 100 hidden units each, so that inference in the weight
space is a high-dimensional problem. We obtain (approximate) target samples via Hamiltonian Monte
Carlo (HMC) (Neal et al., 2011), and then initialize 100 particles from the target to be optimized by
SVGD or MMD-descent with the Gaussian RBF kernel (for detailed setup see Appendix D). Observe
that although we used the same number of particles for SVGD and MMD-descent, SVGD severely
underestimates the variance, whereas MMD-descent generates diverse predictions similar to HMC."
UNDERSTANDING THE PITFALL OF SVGD,0.08238636363636363,"4
UNDERSTANDING THE PITFALL OF SVGD"
UNDERSTANDING THE PITFALL OF SVGD,0.08380681818181818,"We now qualitatively characterize the algorithmic bias of SVGD. Motivated by the observation that
SVGD and MMD-descent only differs in the driving force which involves S1, we ﬁrst compare S1
and S2 in SVGD and argue that the former term is more likely problematic in high dimensions due to
larger ﬂuctuation. We then design controlled experiments by modifying the updates of SVGD and
MMD-descent to show that when the log derivative S1 is coupled with the deterministic bias (i.e.,
absence of particle resampling), the algorithm does not reliably estimate the target variance."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.08522727272727272,"4.1
HIGH VARIANCE FROM INTEGRATION BY PARTS"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.08664772727272728,Recall that the update rule of SVGD involves a driving force (log derivative) term 1
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.08806818181818182,"n
Pn
i=1 S1(xi, x)
and a repulsive force (kernel derivative) term 1"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.08948863636363637,"n
Pn
i=1 S2(xi, x). From integration by parts we have
the following equality: Ey∼p[S1(y, x)] = −Ey∼p[S2(y, x)]. Thus the target distribution p is a ﬁxed
point of SVGD when the number of particles tends to inﬁnity. However, in the ﬁnite sample setting,
the ﬂuctuations in S1 and S2 might have impact on the converged solution."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09090909090909091,"In many settings, we expect the log derivative driving force S1 to have higher variance. For instance,
in the case of unit Gaussian target p and Gaussian RBF kernel k(x, x′) = exp(−∥x−x′∥2
2/2σ2), we
know that S1(y, x) = −yk(y, x) and S2(y, x) = σ−2(x−y)k(y, x). Roughly speaking, when the
norm of x, y and the bandwidth σ scale with
√"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09232954545454546,"d, ∥S1∥2
2 increases with the dimensionality, whereas
∥S2∥2
2 remains bounded. The large magnitude of S1 results in large ﬂuctuation in the driving force
term in SVGD. In Figure 2(a), we visualize the distributions of S1 and S2 when the particles x are
drawn i.i.d. from Gaussian distributions, and indeed observe the higher variance of S1. The following
proposition quantiﬁes this discrepancy in an idealized setting."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09375,"Proposition 2. Deﬁne the mean squared error as: MSEp[f(y)] = Ep(y) ∥f(y) −Ep[f(y)]∥2
2. Then
for y ∼N(a, Id) where ∥a∥2
2 = O(d), Gaussian RBF kernel k(x, y)=exp(−∥x−y∥2
2 /2σ2) with
bandwidth σ = Θ(
√"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09517045454545454,"d), and x ∈Sd−1(
√"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09659090909090909,"d), we have"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09801136363636363,"MSEp[S2(y, x)] = Θ(d−1); MSEp[S1(y, x)] = Θ(d)."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.09943181818181818,"More generally, for x ∈Sd−1(
√"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10085227272727272,"d), strongly log-concave p, and Euclidean distance kernel with Lips-
chitz f and lower-bounded by a scaled Gaussian kernel, MSEp[S1(y, x)] = Ω(d) · MSEp[S2(y, x)]."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10227272727272728,"The proposition indicates that when particles are i.i.d. sampled from some target p, then S1 would
have much larger ﬂuctuation than S21. Intuitively speaking, the higher variance in S1 suggests that
more samples are required to estimate the term accurately. Therefore, when the dimensionality is
large compare to the particle size, poor estimation of S1 may relate to the variance collapse. The
following subsection empirically demonstrate that this is indeed the case; in particular, we show that
in the presence of deterministic bias, the high-variance S1 leads to “biased” particles in SVGD."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10369318181818182,"1Note that particles optimized by SVGD cannot be considered as i.i.d. samples from some distribution, and
thus Proposition 2 does not rigorously apply to particles obtained by the actual algorithm."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10511363636363637,Published as a conference paper at ICLR 2022
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10653409090909091,"(a) Distribution of S1, S2.
(b) Deterministic bias in MMD."
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.10795454545454546,"20
40
60
80
100
Dimensionality (d) 0.2 0.4 0.6 0.8 1.0"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.109375,Marginal variance
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.11079545454545454,"SVGD (original)
SVGD (resampled S1)
SVGD (resampled S2)"
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.11221590909090909,target variance
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.11363636363636363,(c) SVGD with resampling.
HIGH VARIANCE FROM INTEGRATION BY PARTS,0.11505681818181818,"Figure 2: (a) Distribution of sample means of S1 and S2 for Gaussian target and Gaussian RBF kernel. (b)
Particle variance of MMD-descent initialized from N(0, 0.2Id). Darker color indicates larger particle size.
Resampling or ∆MMD
2
(ﬁxed) results in correct variance, but ∆MMD
1
(ﬁxed) diverges. (c) SVGD (n=20) with
resampled S1 accurately estimates the variance, but resampled S2 still leads to variance collapse."
BIAS FROM DETERMINISTIC UPDATE,0.11647727272727272,"4.2
BIAS FROM DETERMINISTIC UPDATE"
BIAS FROM DETERMINISTIC UPDATE,0.11789772727272728,"In this subsection we isolate a cause of variance collapse that relates to the deterministic nature of the
update rule. Observe that in the derivation of SVGD, particles {xi}n
i=1 are assumed to be sampled
from some underlying distribution q. But due to the deterministic update, q is entirely represented by
the same set of particles, and redrawing i.i.d. samples is not feasible during optimization. We now
demonstrate that this failure to resample, which we refer to as the deterministic bias, when combined
with the log derivative driving force S1, may cause the algorithm to converge to the wrong target."
BIAS FROM DETERMINISTIC UPDATE,0.11931818181818182,"Deterministic Bias in MMD-descent.
We start with an illustration of deterministic bias in MMD-
descent. Given samples {yi}m
i=1 ∼p drawn from the target, we have two distinct MMD-gradient
updates (connected via integration by parts (3)) that differs in the driving force."
BIAS FROM DETERMINISTIC UPDATE,0.12073863636363637,"∆MMD
1
(x) = 1 m m
X"
BIAS FROM DETERMINISTIC UPDATE,0.12215909090909091,"i=1
S1(yi, x)+ 1 n n
X"
BIAS FROM DETERMINISTIC UPDATE,0.12357954545454546,"i=1
S2(xi, x). ∆MMD
2
(x) = −1 m m
X"
BIAS FROM DETERMINISTIC UPDATE,0.125,"i=1
S2(yi, x)+ 1 n n
X"
BIAS FROM DETERMINISTIC UPDATE,0.12642045454545456,"i=1
S2(xi, x)."
BIAS FROM DETERMINISTIC UPDATE,0.1278409090909091,"We refer to ∆MMD
1
and ∆MMD
2
as the log derivative update and the kernel derivative update, respectively.
Section 4.1 suggests that ∆MMD
1
tend to have larger ﬂuctuation than ∆MMD
2
. For both update rules,
we consider the two variations: (i) ﬁxed particles, where we draw {yi}m
i=1 in the beginning and use
this same set of target samples throughout optimization; (ii) resampled particles, where we redraw
new i.i.d. samples {yi}m
i=1 from p at each iteration. In this construction, the ﬁxed-particle update (i)
emulates the deterministic bias in S1, due to the absence of resampling from p."
BIAS FROM DETERMINISTIC UPDATE,0.12926136363636365,"We simulate the two variants of MMD-descent in Figure 2(b), in which the target is a 50-dimensional
unit Gaussian, n = 50 and we vary m from 50 to 1000. First observe that the log derivative update
with ﬁxed particles (solid red), which combines the high variance S1 and the deterministic bias, results
in diverged particles. In contrast, the log derivative update with resampled particles (dashed red), in
which S1 is present but not the deterministic bias, converges to the desired variance. Moreover, if the
driving force is estimated by S2, then the variance is accurate even in the presence of deterministic
bias, as shown in the kernel derivative update with ﬁxed particles (solid blue). This indicates that the
combination of deterministic bias and the log derivative driving force S1 leads to biased convergence2."
BIAS FROM DETERMINISTIC UPDATE,0.13068181818181818,Algorithm 1 SVGD with Particle Resampling
BIAS FROM DETERMINISTIC UPDATE,0.13210227272727273,"Input: Initial density q0. Number of Steps S.
for s = 1 to S do"
BIAS FROM DETERMINISTIC UPDATE,0.13352272727272727,"Sample {xs
i}n
i=0 ∼q0(x).
for t = 1 to s −1 do"
BIAS FROM DETERMINISTIC UPDATE,0.13494318181818182,"either Compute S1 from resampled particles:
xs
i ←xs
i + η"
BIAS FROM DETERMINISTIC UPDATE,0.13636363636363635,"n
Pn
j=1 S1(xt
j, xs
i)+S2(xs
j, xs
i).
or Compute S2 from resampled particles:
xs
i ←xs
i + η"
BIAS FROM DETERMINISTIC UPDATE,0.1377840909090909,"n
Pn
j=1 S1(xs
j, xs
i)+S2(xt
j, xs
i).
end for
end for
Output {xS
i }n
i=1."
BIAS FROM DETERMINISTIC UPDATE,0.13920454545454544,"Particle Resampling in SVGD.
The previous
experiment on MMD-descent suggests that the
deterministic bias arises from the algorithm not
being able to “redraw” i.i.d. samples. We fur-
ther validate this observation in SVGD by con-
structing a variant that achieves particle resam-
pling. The modiﬁed update exhibits a double-
loop structure (see Algorithm 1): at iteration i of
the outer loop, we obtain a new set of particles
indexed as qi after (i−1) inner loop steps. In
the inner loop, we ﬁrst draw n i.i.d. particles ˆqi
from the initial distribution q0, and then update
ˆqi via (i−1) SVGD steps to obtain qi; at each
step j of the inner loop, either the driving force S1 or the repulsive force S2 is computed between ˆqi
and qj, as opposed to between particles within ˆqi as in the original SVGD."
BIAS FROM DETERMINISTIC UPDATE,0.140625,"2Due to the different driving force, the log derivative update of MMD-descent with ﬁxed particles results in
divergence instead of variance collapse (see Appendix A.3 for more discussion)."
BIAS FROM DETERMINISTIC UPDATE,0.14204545454545456,Published as a conference paper at ICLR 2022
BIAS FROM DETERMINISTIC UPDATE,0.1434659090909091,"We remark that this resampled update resembles transport-based particle algorithms (e.g. Nitanda and
Suzuki (2017)), where old particles deﬁne a transport map that the new particles (initialized from q0)
follow. By construction, at each outer loop iteration, a new set of independent particles are generated
and updated; importantly, one of S1 and S2 is not evaluated on the same set of particles themselves,
and hence deterministic bias is not present. However, the complexity of such resampled updates
scales quadratically with the iterations, which renders the algorithm computationally prohibitive3."
BIAS FROM DETERMINISTIC UPDATE,0.14488636363636365,"Figure 2(c) compares the modiﬁed SVGD updates (with Gaussian RBF kernel) in learning a Gaussian
target. Observe that when the driving force S1 is computed on resampled particles (blue), SVGD
accurately estimates the target variance even with small particle size (n = 20) at each step. In
contrast, if only the repulsive force S2 is evaluated on particles resampled from q0, then the variance
is still underestimated (green), due to deterministic bias in S1. This conﬁrms our argument that
combination of log derivative S1 and deterministic bias leads to the algorithmic bias of SVGD."
SVGD IN THE PROPORTIONAL LIMIT,0.14630681818181818,"5
SVGD IN THE PROPORTIONAL LIMIT"
SVGD IN THE PROPORTIONAL LIMIT,0.14772727272727273,"While the previous discussion provides qualitative understanding of the algorithmic bias of SVGD,
no quantitative characterization is provided. In this section, we make use of the deterministic nature
of the SVGD update, and directly analyze the particle ﬁxed point; this allows us to precisely compute
the variance of SVGD particles and conﬁrm the variance collapse in simple settings."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.14914772727272727,"5.1
SCALING LIMIT AND BASIC ASSUMPTIONS"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.15056818181818182,"Previous works have considered the inﬁnite-particle limit of SVGD under ﬁxed dimensionality, which
is known as the mean-ﬁeld limit (Lu et al., 2019; Duncan et al., 2019). In this regime, the particle
dynamics of SVGD is described by a nonlinear PDE: ∂tρ = ∇· (ρ(K ∗(∇ρ + ∇V ρ))), which
converges weakly to the unique target measure as t →∞under certain non-degeneracy assumptions.
However, since the invariant solution is the desired target distribution, the mean-ﬁeld limit does not
capture the variance collapse phenomenon. Moreover, similar guarantees can also be derived for
MMD-descent under similar assumptions (Arbel et al., 2019). Therefore, the mean-ﬁeld limit does
not explain the observed discrepancy between SVGD and MMD-descent (Figure 1 in Section 3)."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.15198863636363635,"Instead, we propose to analyze the algorithm in the proportional asymptotic limit:"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.1534090909090909,"• (A1) Proportional Limit. n, d →∞and d/n →γ; we also assume γ > 1."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.15482954545454544,"Note that larger γ is analogous to higher dimensionality or smaller particle size; we focus on the
γ > 1 regime, where the number of particles is less than the problem dimensionality — this is
a common feature of modern Bayesian inference problems. In this asymptotic limit, prior works
studied spectral properties of the kernel matrix (e.g., El Karoui et al. (2010)), which the SVGD update
crucially depends upon. However, these random matrix results typically require the particles to be
i.i.d., which is not satisﬁed by SVGD due to the interacting update. To overcome this technical
difﬁculty, we make an additional assumption on the ﬁxed point:"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.15625,"• (A2) Near-orthogonality. The optimized particles {xi}n
i=1 satisfy |x⊤
i xi −dυ| < υϵd, |x⊤
i xj| <
υϵd for all i ̸= j, some υ > 0, and d−1/2ϵd →0 as d →∞with probability 1."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.15767045454545456,"50
100
150
200
250
number of particles 0.04 0.06 0.08 0.10 0.12"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.1590909090909091,"d
1/2
d in (A2)"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16051136363636365,"Gaussian kernel
IMQ kernel
median bandwidth"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16193181818181818,"fixed 
=
d"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16335227272727273,"Figure 3: Empirical veriﬁcation
of (A2) for SVGD in learning
isotropic Gaussian (γ = 3)."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16477272727272727,"The above assumption ensures that the particles tend to equilibrate
at a ﬁxed point where they “repel” one another as much as possible
(due to the repulsive force), as shown in Figure 3 for the case of
Gaussian target. While this is a condition only on the ﬁxed point of
the SVGD algorithm, not on the particles produced by the SVGD
iterations (unless initialized at the ﬁxed point), we note that it is
still stronger than making an assumption on the sampling problem.
We refer to Appendix A.2 for additional empirical veriﬁcation and
leave the rigorous justiﬁcation as future work."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16619318181818182,"Under assumptions (A1) and (A2), we can approximate the Gram
matrix via Taylor expansion of the kernel matrix, which allows
us to simplify the ﬁxed point equation. We restrict ourselves to the following class of kernels:"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.16761363636363635,"3Resampling can also be achieved via kernel density estimation (KDE) at each iteration, as in Dai et al.
(2016). However, KDE is also known to be less effective in high dimensions."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.1690340909090909,Published as a conference paper at ICLR 2022
SCALING LIMIT AND BASIC ASSUMPTIONS,0.17045454545454544,"• (A3) Kernel Function. k(x, y) = f

∥x−y∥2
2
2σ2

, where f : R≥0 →R+ is bounded, monotonically"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.171875,"decreasing, and differentiable4on R≥0."
SCALING LIMIT AND BASIC ASSUMPTIONS,0.17329545454545456,"Remark. (A3) covers common choices of Euclidean distance kernels with decaying tail, such as
the Gaussian RBF kernel, the IMQ kernel and the log-inverse kernel. Our analysis also allows for
different choices of bandwidth σ, which we specify in the sequel."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.1747159090909091,"5.2
LEARNING HIGH-DIMENSIONAL GAUSSIANS"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.17613636363636365,"We now consider a simple example in which SVGD does not reliably estimate the variance – learning
an isotropic Gaussian target (empirically observed in Zhuo et al. (2017)). Note that our goal is to
demonstrate a negative result: we do not claim that SVGD underestimates the variance in all settings;
instead, we show that variance collapse is present even in learning the simplest target distribution."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.17755681818181818,"• (A4) Gaussian Target. p(x) ∝exp
 
−1"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.17897727272727273,"2x⊤x

."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.18039772727272727,"Remark. We assume (A4) for simple and concise presentation, and we believe that the result can
be extended to more general sub-Gaussian targets. In addition, since Gaussian prior is widely used
in Bayesian inference, it is natural to expect that target potentials in many real-world problems are
“Gaussian-like” (i.e., exhibit quadratic growth) outside of some radius (Cheng et al., 2018a). Indeed,
our empirical observations on BNN align with our theoretical predictions for Gaussian target."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.18181818181818182,We compute the dimension-averaged marginal variance: v = 1
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.18323863636363635,"d
Pd
j=1 Varj({xi}n
i=1), where Varj(·)
is the particle variance at the j-th coordinate. Since v is a scalar quantity, accurate estimation in the
proportional limit (A1) is not impossible; in fact, in Appendix A.4 we show that running Langevin
Monte Carlo (LMC) for O(d) iterations using one particle would sufﬁce. In the following subsections
we derive the equilibrium variance of SVGD under two different choices of kernel bandwidth."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.1846590909090909,"Kernels with Adaptive Bandwidth.
We ﬁrst consider the case where the bandwidth σ is adaptively
tuned based on the optimized particles. In particular, we analyze the median heuristic (Scholkopf"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.18607954545454544,"and Smola, 2001): σ =
q"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.1875,"Med{∥xi −xj∥2
2}/2, which is the most common choice made in practice.
Under the previous assumptions, we have the following result for SVGD with median bandwidth."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.18892045454545456,"Proposition 3. Given (A1-4) and the choice of bandwidth σ =
q"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.1903409090909091,"Med{∥xi −xj∥2
2}/2, particles
driven by SVGD (Equation (2)) equilibriate at the dimension-averaged variance"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19176136363636365,vSVGD = γ−1 · f ′(1) [f(1) −f(0)]−1 .
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19318181818181818,"Remark. The proposition precisely captures our observation that SVGD underestimates the
dimension-averaged variance inversely proportional to the problem dimensionality (γ = d/n).
In contrast, previous analysis (Zhang et al., 2020) predicts that SVGD would collapse to zero
variance as t →∞, which does not align with the empirical observations."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19460227272727273,"For Gaussian kernel, we also provide a precise contrast between SVGD and MMD-descent.
Corollary 4. Given (A1-4), for the Gaussian RBF kernel f(x) = exp(−x) with the median band-
width, SVGD converges to the following dimension-averaged variance: vSVGD = (e−1)−1 ·γ−1 < 1,
whereas MMD-descent (Equation (4)) leads to vMMD = 1."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19602272727272727,"In other words, as the problem dimensionality γ increases, more particles are required for SVGD
to reliably estimate the dimension-averaged variance (speciﬁcally, the number of particles should
grow linearly with d). In contrast, the variance estimated by MMD-descent remains accurate and is
independent to γ; this aligns with the empirical observations in Section 3. Remarkably, Figure 4(a)
demonstrates that Proposition 3 is accurate even for reasonably small n, d: observe that once γ > 1
(i.e., d > n), the prediction (black) becomes well-aligned with the empirical value."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19744318181818182,"Kernels with Fixed Bandwidth.
One possibility remains that the culprit of variance collapse is the
median bandwidth. Speciﬁcally in kernel Stein discrepancy (KSD), it has been argued that the IMQ
kernel can outperform the Gaussian RBF kernel (Gorham and Mackey, 2017), and the IMQ kernel is
often employed without an adaptive bandwidth. Due to the connection between KSD and SVGD, one
may speculate that the IMQ kernel with ﬁxed bandwidth can alleviate the variance collapse problem."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.19886363636363635,"4For the median bandwidth, we only require local differentiability around 1, similar to El Karoui et al. (2010)."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2002840909090909,Published as a conference paper at ICLR 2022
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.20170454545454544,"50
100
150
200
Dimensionality (d) 0.2 0.4 0.6 0.8 1.0"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.203125,Marginal variance
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.20454545454545456,"Target variance
SVGD (n=25)
SVGD (n=50)
MMD (n=25)
MMD (n=50)
prediction"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2059659090909091,(a) Gaussian RBF Kernel (median).
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.20738636363636365,"50
100
150
200
Dimensionality (d) 0.2 0.4 0.6 0.8 1.0"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.20880681818181818,Marginal variance
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21022727272727273,"Target variance
median (n=25)
median (n=50)"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21164772727272727,"=
d (n=25)"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21306818181818182,"=
d (n=50)
Prediction"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21448863636363635,"(b) IMQ Kernel (σ = Θ(
√ d))."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2159090909090909,"102
6 × 101
2 × 102"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21732954545454544,"Dimensionality (d) 10
1"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.21875,"3 × 10
2"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22017045454545456,"4 × 10
2"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2215909090909091,"6 × 10
2"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22301136363636365,Marginal variance
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22443181818181818,"= 2
= 3"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22585227272727273,"= 4
= 5"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22727272727272727,"(d
1/3)"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.22869318181818182,(c) IMQ Kernel (σ = 1).
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.23011363636363635,"Figure 4: Stationary variance of SVGD and MMD-descent; predictions (black) are given by Proposition 3 and
5. (a) Gaussian kernel with median heuristic: SVGD underestimates the variance, but MMD-descent (blue) does
not. (b) IMQ-SVGD underestimates the variance under both the median heuristic (red) and ﬁxed σ =
√"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2315340909090909,"d (blue).
(c) When σ = 1, IMQ-SVGD asymptotically collapses the variance to 0 at a rate of d−1/3 (black)."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.23295454545454544,"We show that this is unfortunately not the case. We ﬁrst provide a general characterization:
Proposition 5. Given (A1-4) and ﬁxed bandwidth5 σ =
√"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.234375,"d, the SVGD variance satisﬁes
f ′(vSVGD) = γ ·

f(vSVGD) −f(0)

."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.23579545454545456,"In addition, if f ′ is also monotone on R≥0, then vSVGD decreases as γ > 1 increases."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.2372159090909091,"Note that the monotonicity assumption is again satisﬁed by many Euclidean distance kernels of
interest. While the equation may not provide an explicit expression of the stationary variance, we
can verify that the variance decreases as the problem becomes more high-dimensional (i.e., larger γ).
Our next proposition speciﬁcally handles the IMQ kernel considered in Gorham and Mackey (2017).
Corollary 6. Given (A1-4), for the IMQ kernel f(x) = (1 + x)−1/2 with ﬁxed bandwidth, we have
the following stationary variance of SVGD under two different scalings:"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.23863636363636365,"• When σ =
√"
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.24005681818181818,"d, vSVGD < 1 and is decreasing as γ > 1 increases."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.24147727272727273,"• When σ = 1, vSVGD →0 as n, d →∞at a rate of d−1/3."
LEARNING HIGH-DIMENSIONAL GAUSSIANS,0.24289772727272727,"This corollary suggests that the IMQ kernel with ﬁxed bandwidth is not a remedy to the variance
collapse problem. We remark that the second setting (σ = 1) is considered in Gorham and Mackey
(2017) for KSD. In both cases (and including the median bandwidth covered by Proposition 3), SVGD-
IMQ underestimates the target variance as the dimensionality increases (large γ). The agreement
between the theoretical predictions and the empirical simulations (using ﬁnite particles) is illustrated
in Figure 4(b)(c). Finally, in Appendix A we include a more general (but less precise) characterization
of variance collapse beyond Gaussian target, as well as additional empirical evidence."
A MODIFICATION OF SVGD,0.24431818181818182,"5.3
A MODIFICATION OF SVGD"
A MODIFICATION OF SVGD,0.24573863636363635,"We have thus far shown that in the simple setting of learning high-dimensional Gaussian, SVGD
underestimates the dimension-averaged variance unless the number of particles is larger than the di-
mensionality. Now we further validate our theoretical ﬁndings by introducing a heuristic modiﬁcation
of SVGD that corrects for this variance collapse in the overparameterized regime6."
A MODIFICATION OF SVGD,0.2471590909090909,"The starting observation is that the variance collapse indicates that the deterministic bias causes the
driving force term to dominate. Because during each update every particle xi is most “correlated”
with itself, one should expect S1(xi, xi) to contribute signiﬁcantly to this bias. We thus consider a
modiﬁcation of SVGD which simply shrinks (damps) the term S1(xi, xi) by λ = min{1, (f(1) −
γ−1f ′(1))/f(0)}, where λ is chosen such that when d > n, the equilibrium variance matches the
target variance in the setup of Proposition 3. We refer to this update as damped SVGD:"
A MODIFICATION OF SVGD,0.24857954545454544,"∆Damp(xi) =
X"
A MODIFICATION OF SVGD,0.25,"j̸=i
[S1(xj, xi) + S2(xj, xi)] + λS1(xi, xi).
(5)"
A MODIFICATION OF SVGD,0.25142045454545453,"Remark. At high level, (5) resembles the annealed SVGD algorithm (D’Angelo and Fortuin, 2021),
in that they both ”weaken” the driving force S1. However, annealed SVGD uses a heuristic learning
rate schedule (target-independent) to modify the strength of S1, whereas we derive our λ to ensure
that the dimension-averaged variance is correct under (A1-4) in the d > n regime."
A MODIFICATION OF SVGD,0.2528409090909091,"5Note that the ﬁxed bandwidth can be arbitrary, as constants can be absorbed into the function f.
6The modiﬁed update in Section 4 also alleviate the variance collapse, but is computationally intractable."
A MODIFICATION OF SVGD,0.25426136363636365,Published as a conference paper at ICLR 2022
A MODIFICATION OF SVGD,0.2556818181818182,"Figure 5: Points represent converged particles
of SVGD (middle) and damped SVGD (right).
We also report the dimension-averaged variances
in the ﬁgure. In the Gaussian case (top), SVGD
(damped) exactly learns the target variance."
A MODIFICATION OF SVGD,0.2571022727272727,"Simulating Toy Densities.
We demonstrate the use-
fulness of this bias-correction term in learning sim-
ple target distributions. We stack 250 independent 2-
dimensional distributions to create a high-dimensional
target distribution, and visualize the ﬁrst two dimen-
sions in Figure 5. We optimize 50 particles using
SVGD or SVGD (damped) with the Gaussian RBF
kernel and median bandwidth."
A MODIFICATION OF SVGD,0.2585227272727273,"As shown in Figure 5 (top), SVGD collapses its par-
ticles to the mode, whereas damped SVGD generates
more dispersed particles with variance exactly match-
ing that of the Gaussian target. It is also possible that
the heuristic modiﬁcation leads to more accurate sam-
ples for other “Gaussian-like” target distributions7, as
shown in the second row of Figure 5. We provide
additional experimental results in Appendix A.3."
RELATED WORKS,0.2599431818181818,"6
RELATED WORKS"
RELATED WORKS,0.26136363636363635,"Guarantees and Applications of SVGD.
Liu and Wang (2018); Lu et al. (2019) characterized
SVGD in the mean-ﬁeld limit and showed the weak convergence to the target distribution. Under
further assumptions on the potential and kernel, quantitative convergence rate can be derived (Duncan
et al., 2019; Korba et al., 2020). In addition, Liu and Wang (2018) shows SVGD using kernels with
ﬁnite-dimensional feature maps, exactly estimates the expectations for some set of functions, casting
SVGD as a moment matching method. In the high-dimensional setting, Zhuo et al. (2017); Wang et al.
(2018) observed in experiments that particles driven by SVGD tend to underestimate the marginal
variance, but did not provide any quantitative understanding of the phenomenon."
RELATED WORKS,0.2627840909090909,"On the application side, Haarnoja et al. (2017); Liu et al. (2017) adopt SVGD to learn a stochastic
sampling network to approximate the policy in Q-learning (Sutton et al., 1998), whereas in Gangwani
et al. (2018), SVGD encourages diverse policies for exploration. SVGD can be used in meta-learning
to quickly obtain parameter samples from training sets (Yoon et al., 2018). Recent works also applied
SVGD in Batch Bayesian optimization (Gong et al., 2019) and the learning of mixture models (Wang
and Liu, 2019). Leveraging the Markov blanket structure, SVGD also achieves strong performance in
learning graphical models (Zhuo et al., 2017; Wang et al., 2018)."
RELATED WORKS,0.26420454545454547,"Stein’s Method.
Stein’s method provides powerful tools in approximating probability distributions
and specifying convergence rates (Erdogdu, 2016; Gorham et al., 2016). Liu and Wang (2016) utilizes
the connection between Stein’s operator and the gradient of KL divergence to construct particle
inference algorithm. Stein’s lemma is also useful in implicit variational inference (Husz´ar, 2017)
to estimate the score using samples from an implicit distribution (Li and Turner, 2017; Shi et al.,
2018). Related to our analysis in Section 4, Erdogdu et al. (2016) observed that algorithms that are
equivalent in expectation via Stein’s lemma might have different convergence properties. In addition,
the “curse of dimensionality” of kernel Stein estimators has also been studied in Oates et al. (2016)."
CONCLUSION,0.265625,"7
CONCLUSION"
CONCLUSION,0.26704545454545453,"We analyzed the variance collapse of SVGD in high dimensions based on a connection between
SVGD and a proposed MMD-descent algorithm. We qualitatively identiﬁed factors that lead to this
phenomenon, and also quantitatively characterized the equilibrium variance in the proportional limit
for simple models. Looking forward, we believe that understanding interacting particle systems
(SVGD, two-layer neural nets, etc.) in the proportional limit instead of the mean-ﬁeld limit is an
important direction. In addition, while our analysis conﬁrms the variance collapse of the original
algorithm, it remains possible that certain dimension reduction schemes could alleviate the issue;
Chen and Ghattas (2020); Gorham et al. (2020); Gong et al. (2020) proposed dimension-reduced
versions of SVGD, and it would be interesting to theoretically justify these approaches."
CONCLUSION,0.2684659090909091,"7We however note that due to the distribution-speciﬁc derivation of λ, we should not expect the proposed
modiﬁcation to be a general solution to the variance collapse problem of SVGD."
CONCLUSION,0.26988636363636365,Published as a conference paper at ICLR 2022
CONCLUSION,0.2713068181818182,ACKNOWLEDGEMENT
CONCLUSION,0.2727272727272727,"JB was supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research
Scholar Program and Amazon Research Award. MAE was supported by NSERC Grant [2019-06167],
Connaught New Researcher Award, CIFAR AI Chairs program, and CIFAR AI Catalyst grant. MG
was supported in part by Microsoft Research and a Canadian CIFAR AI Chair held at the Vector
Institute. TS was partially supported by JSPS KAKENHI (18H03201), Japan Digital Design and JST
CREST. SS was supported by a Connaught New Researcher Award and a Connaught Fellowship."
REFERENCES,0.2741477272727273,REFERENCES
REFERENCES,0.2755681818181818,"Arbel, M., Korba, A., Salim, A., and Gretton, A. (2019). Maximum mean discrepancy gradient ﬂow.
In Advances in Neural Information Processing Systems, pages 6484–6494."
REFERENCES,0.27698863636363635,"Bach, F., Lacoste-Julien, S., and Obozinski, G. (2012). On the equivalence between herding and
conditional gradient algorithms. arXiv preprint arXiv:1203.4523."
REFERENCES,0.2784090909090909,"Bordenave, C. et al. (2013). On euclidean random matrices in high dimension. Electronic Communi-
cations in Probability, 18."
REFERENCES,0.27982954545454547,"Chen, C., Zhang, R., Wang, W., Li, B., and Chen, L. (2018a). A uniﬁed particle-optimization
framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659."
REFERENCES,0.28125,"Chen, P. and Ghattas, O. (2020). Projected stein variational gradient descent. arXiv preprint
arXiv:2002.03469."
REFERENCES,0.28267045454545453,"Chen, W. Y., Barp, A., Briol, F.-X., Gorham, J., Girolami, M., Mackey, L., and Oates, C. (2019).
Stein point markov chain monte carlo. In International Conference on Machine Learning, pages
1011–1021. PMLR."
REFERENCES,0.2840909090909091,"Chen, W. Y., Mackey, L., Gorham, J., Briol, F.-X., and Oates, C. (2018b). Stein points. In International
Conference on Machine Learning, pages 844–853. PMLR."
REFERENCES,0.28551136363636365,"Cheng, X., Chatterji, N. S., Abbasi-Yadkori, Y., Bartlett, P. L., and Jordan, M. I. (2018a). Sharp con-
vergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648."
REFERENCES,0.2869318181818182,"Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2018b). Underdamped langevin mcmc:
A non-asymptotic analysis. In Conference on Learning Theory, pages 300–323. PMLR."
REFERENCES,0.2883522727272727,"Chizat, L. and Bach, F. (2018). On the global convergence of gradient descent for over-parameterized
models using optimal transport. arXiv preprint arXiv:1805.09545."
REFERENCES,0.2897727272727273,"Chwialkowski, K., Strathmann, H., and Gretton, A. (2016). A kernel test of goodness of ﬁt. JMLR:
Workshop and Conference Proceedings."
REFERENCES,0.2911931818181818,"Dai, B., He, N., Dai, H., and Song, L. (2016). Provable bayesian inference via particle mirror descent.
In Artiﬁcial Intelligence and Statistics, pages 985–994."
REFERENCES,0.29261363636363635,"Dalalyan, A. S. (2014). Theoretical guarantees for approximate sampling from smooth and log-
concave densities. arXiv preprint arXiv:1412.7392."
REFERENCES,0.2940340909090909,"D’Angelo, F. and Fortuin, V. (2021). Annealed stein variational gradient descent. arXiv preprint
arXiv:2101.09815."
REFERENCES,0.29545454545454547,"Doucet, A., De Freitas, N., Gordon, N. J., et al. (2001). Sequential Monte Carlo methods in practice,
volume 1. Springer."
REFERENCES,0.296875,"Duncan, A., Nuesken, N., and Szpruch, L. (2019). On the geometry of stein variational gradient
descent. arXiv preprint arXiv:1912.00894."
REFERENCES,0.29829545454545453,"Durmus, A. and Moulines, E. (2017). Nonasymptotic convergence analysis for the unadjusted
langevin algorithm. The Annals of Applied Probability, 27(3):1551–1587."
REFERENCES,0.2997159090909091,Published as a conference paper at ICLR 2022
REFERENCES,0.30113636363636365,"El Karoui, N. et al. (2010). The spectrum of kernel random matrices. The Annals of Statistics,
38(1):1–50."
REFERENCES,0.3025568181818182,"Erdogdu, M. A. (2016). Newton-stein method: an optimization method for glms via stein’s lemma.
The Journal of Machine Learning Research, 17(1):7565–7616."
REFERENCES,0.3039772727272727,"Erdogdu, M. A., Bayati, M., and Dicker, L. H. (2016). Scalable approximations for generalized linear
problems. arXiv preprint arXiv:1611.06686."
REFERENCES,0.3053977272727273,"Erdogdu, M. A. and Hosseinzadeh, R. (2020). On the convergence of langevin monte carlo: The
interplay between tail growth and smoothness. arXiv preprint arXiv:2005.13097."
REFERENCES,0.3068181818181818,"Erdogdu, M. A., Hosseinzadeh, R., and Zhang, M. S. (2021). Convergence of langevin monte carlo
in chi-squared and r´enyi divergence."
REFERENCES,0.30823863636363635,"Gallego, V. and Insua, D. R. (2018). Stochastic gradient mcmc with repulsive forces. arXiv preprint
arXiv:1812.00071."
REFERENCES,0.3096590909090909,"Gangwani, T., Liu, Q., and Peng, J. (2018). Learning self-imitating diverse policies. arXiv preprint
arXiv:1805.10309."
REFERENCES,0.31107954545454547,"Garreau, D., Jitkrittum, W., and Kanagawa, M. (2017). Large sample analysis of the median heuristic.
arXiv preprint arXiv:1707.07269."
REFERENCES,0.3125,"Gong, C., Peng, J., and Liu, Q. (2019). Quantile stein variational gradient descent for batch bayesian
optimization. In International Conference on Machine Learning, pages 2347–2356."
REFERENCES,0.31392045454545453,"Gong, W., Li, Y., and Hern´andez-Lobato, J. M. (2020). Sliced kernelized stein discrepancy. arXiv
preprint arXiv:2006.16531."
REFERENCES,0.3153409090909091,"Gorham, J., Duncan, A. B., Vollmer, S. J., and Mackey, L. (2016). Measuring sample quality with
diffusions. arXiv preprint arXiv:1611.06972."
REFERENCES,0.31676136363636365,"Gorham, J. and Mackey, L. (2015). Measuring sample quality with stein’s method. In Advances in
Neural Information Processing Systems, pages 226–234."
REFERENCES,0.3181818181818182,"Gorham, J. and Mackey, L. (2017). Measuring sample quality with kernels. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pages 1292–1301. JMLR. org."
REFERENCES,0.3196022727272727,"Gorham, J., Raj, A., and Mackey, L. (2020).
Stochastic stein discrepancies.
arXiv preprint
arXiv:2007.02857."
REFERENCES,0.3210227272727273,"Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch¨olkopf, B., and Smola, A. (2012). A kernel
two-sample test. Journal of Machine Learning Research, 13(Mar):723–773."
REFERENCES,0.3224431818181818,"Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-
based policies. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 1352–1361. JMLR. org."
REFERENCES,0.32386363636363635,"Husz´ar, F. (2017). Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235."
REFERENCES,0.3252840909090909,"Husz´ar, F. and Duvenaud, D. (2012). Optimally-weighted herding is bayesian quadrature. arXiv
preprint arXiv:1204.1664."
REFERENCES,0.32670454545454547,"Jordan, R., Kinderlehrer, D., and Otto, F. (1998). The variational formulation of the fokker–planck
equation. SIAM journal on mathematical analysis, 29(1):1–17."
REFERENCES,0.328125,"Kim, T., Yoon, J., Dia, O., Kim, S., Bengio, Y., and Ahn, S. (2018). Bayesian model-agnostic
meta-learning. arXiv preprint arXiv:1806.03836."
REFERENCES,0.32954545454545453,"Korba, A., Aubin-Frankowski, P.-C., Majewski, S., and Ablin, P. (2021). Kernel stein discrepancy
descent. arXiv preprint arXiv:2105.09994."
REFERENCES,0.3309659090909091,"Korba, A., Salim, A., Arbel, M., Luise, G., and Gretton, A. (2020). A non-asymptotic analysis for
stein variational gradient descent. arXiv preprint arXiv:2006.09797."
REFERENCES,0.33238636363636365,Published as a conference paper at ICLR 2022
REFERENCES,0.3338068181818182,"Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–1105."
REFERENCES,0.3352272727272727,"Li, Y. and Turner, R. E. (2017).
Gradient estimators for implicit models.
arXiv preprint
arXiv:1705.07107."
REFERENCES,0.3366477272727273,"Liu, Q. (2017). Stein variational gradient descent as gradient ﬂow. In Advances in neural information
processing systems, pages 3115–3123."
REFERENCES,0.3380681818181818,"Liu, Q., Lee, J., and Jordan, M. (2016). A kernelized stein discrepancy for goodness-of-ﬁt tests. In
International Conference on Machine Learning, pages 276–284."
REFERENCES,0.33948863636363635,"Liu, Q. and Wang, D. (2016). Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances In Neural Information Processing Systems, pages 2378–2386."
REFERENCES,0.3409090909090909,"Liu, Q. and Wang, D. (2018). Stein variational gradient descent as moment matching. In Advances in
Neural Information Processing Systems, pages 8868–8877."
REFERENCES,0.34232954545454547,"Liu, Y., Ramachandran, P., Liu, Q., and Peng, J. (2017). Stein variational policy gradient. arXiv
preprint arXiv:1704.02399."
REFERENCES,0.34375,"Lu, J., Lu, Y., and Nolen, J. (2019). Scaling limit of the stein variational gradient descent: The mean
ﬁeld regime. SIAM Journal on Mathematical Analysis, 51(2):648–671."
REFERENCES,0.34517045454545453,"MacKay, D. J. (1992). A practical Bayesian framework for backpropagation networks. Neural
Computation, 4:448–472."
REFERENCES,0.3465909090909091,"Mattingly, J. C., Stuart, A. M., and Higham, D. J. (2002). Ergodicity for sdes and approximations:
locally lipschitz vector ﬁelds and degenerate noise. Stochastic processes and their applications,
101(2):185–232."
REFERENCES,0.34801136363636365,"M¨uller, A. (1997). Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429–443."
REFERENCES,0.3494318181818182,"Neal, R. M. et al. (2011). Mcmc using hamiltonian dynamics. Handbook of markov chain monte
carlo, 2(11):2."
REFERENCES,0.3508522727272727,"Nitanda, A. and Suzuki, T. (2017). Stochastic particle gradient descent for inﬁnite ensembles. arXiv
preprint arXiv:1712.05438."
REFERENCES,0.3522727272727273,"Oates, C. J., Cockayne, J., Briol, F.-X., and Girolami, M. (2016). Convergence rates for a class of
estimators based on stein’s method. arXiv preprint arXiv:1603.03220."
REFERENCES,0.3536931818181818,"Robert, C. and Casella, G. (2013). Monte Carlo statistical methods. Springer Science & Business
Media."
REFERENCES,0.35511363636363635,"Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin distributions and
their discrete approximations. Bernoulli, 2(4):341–363."
REFERENCES,0.3565340909090909,"Scholkopf, B. and Smola, A. J. (2001). Learning with kernels: support vector machines, regulariza-
tion, optimization, and beyond. MIT press."
REFERENCES,0.35795454545454547,"Shi, J., Sun, S., and Zhu, J. (2018). A spectral approach to gradient estimation for implicit distributions.
arXiv preprint arXiv:1806.02925."
REFERENCES,0.359375,"Stein, C. et al. (1972). A bound for the error in the normal approximation to the distribution
of a sum of dependent random variables.
In Proceedings of the Sixth Berkeley Symposium
on Mathematical Statistics and Probability, Volume 2: Probability Theory. The Regents of the
University of California."
REFERENCES,0.36079545454545453,"Sutton, R. S., Barto, A. G., et al. (1998). Introduction to reinforcement learning, volume 2. MIT
press Cambridge."
REFERENCES,0.3622159090909091,"Vempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin algorithm:
Isoperimetry sufﬁces. In Advances in Neural Information Processing Systems, pages 8094–8106."
REFERENCES,0.36363636363636365,Published as a conference paper at ICLR 2022
REFERENCES,0.3650568181818182,"Wainwright, M. J., Jordan, M. I., et al. (2008). Graphical models, exponential families, and variational
inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305."
REFERENCES,0.3664772727272727,"Wang, D. and Liu, Q. (2019). Nonlinear stein variational gradient descent for learning diversiﬁed
mixture models. In International Conference on Machine Learning, pages 6576–6585."
REFERENCES,0.3678977272727273,"Wang, D., Zeng, Z., and Liu, Q. (2018). Stein variational message passing for continuous graphical
models. In International Conference on Machine Learning, pages 5206–5214."
REFERENCES,0.3693181818181818,"Welling, M. (2009). Herding dynamic weights for partially observed random ﬁeld models. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 599–
606. AUAI Press."
REFERENCES,0.37073863636363635,"Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S. (2018). Bayesian model-agnostic
meta-learning. In Advances in Neural Information Processing Systems, pages 7332–7342."
REFERENCES,0.3721590909090909,"Zhang, J., Zhang, R., Carin, L., and Chen, C. (2020). Stochastic particle-optimization sampling and
the non-asymptotic convergence theory. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 1877–1887."
REFERENCES,0.37357954545454547,"Zhuo, J., Liu, C., Shi, J., Zhu, J., Chen, N., and Zhang, B. (2017). Message passing stein variational
gradient descent. arXiv preprint arXiv:1711.04425."
REFERENCES,0.375,Published as a conference paper at ICLR 2022
REFERENCES,0.37642045454545453,TABLE OF CONTENTS
INTRODUCTION,0.3778409090909091,"1
Introduction
1"
BACKGROUND,0.37926136363636365,"2
Background
2"
BACKGROUND,0.3806818181818182,"2.1
Integral Probability Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2"
BACKGROUND,0.3821022727272727,"2.2
(Deterministic) Particle Inference Algorithms . . . . . . . . . . . . . . . . . . . .
2"
CONNECTING SVGD WITH MMD MINIMIZATION,0.3835227272727273,"3
Connecting SVGD with MMD Minimization
3"
UNDERSTANDING THE PITFALL OF SVGD,0.3849431818181818,"4
Understanding the Pitfall of SVGD
4"
UNDERSTANDING THE PITFALL OF SVGD,0.38636363636363635,"4.1
High Variance from Integration by Parts . . . . . . . . . . . . . . . . . . . . . . .
4"
BIAS FROM DETERMINISTIC UPDATE,0.3877840909090909,"4.2
Bias from Deterministic Update
. . . . . . . . . . . . . . . . . . . . . . . . . . .
5"
SVGD IN THE PROPORTIONAL LIMIT,0.38920454545454547,"5
SVGD in the Proportional Limit
6"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.390625,"5.1
Scaling Limit and Basic Assumptions
. . . . . . . . . . . . . . . . . . . . . . . .
6"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.39204545454545453,"5.2
Learning High-dimensional Gaussians . . . . . . . . . . . . . . . . . . . . . . . .
7"
SCALING LIMIT AND BASIC ASSUMPTIONS,0.3934659090909091,"5.3
A Modiﬁcation of SVGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
RELATED WORKS,0.39488636363636365,"6
Related Works
9"
CONCLUSION,0.3963068181818182,"7
Conclusion
9"
CONCLUSION,0.3977272727272727,"A Additional Results
15"
CONCLUSION,0.3991477272727273,"A.1
Additional Results for Section 5
. . . . . . . . . . . . . . . . . . . . . . . . . . .
15"
CONCLUSION,0.4005681818181818,"A.2
Empirical Veriﬁcation of (A2)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
CONCLUSION,0.40198863636363635,"A.3
Additional Results for Modiﬁed SVGD
. . . . . . . . . . . . . . . . . . . . . . .
17"
CONCLUSION,0.4034090909090909,"A.4
Discussion on Other Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . .
18"
CONCLUSION,0.40482954545454547,"A.5
Additional Figures for Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
CONCLUSION,0.40625,"B
Derivation of Propositions in Section 4
20"
CONCLUSION,0.40767045454545453,"C Derivation of Propositions in Section 5
21"
CONCLUSION,0.4090909090909091,"C.1
Gaussian Kernel with the Median Bandwidth
. . . . . . . . . . . . . . . . . . . .
22"
CONCLUSION,0.41051136363636365,"C.2
General Kernel with Adaptive Bandwidth (Median Heuristic) . . . . . . . . . . . .
24"
CONCLUSION,0.4119318181818182,"C.3
General Kernel with Fixed Bandwidth . . . . . . . . . . . . . . . . . . . . . . . .
25"
CONCLUSION,0.4133522727272727,"C.4
“Almost-Gaussian” Target
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
CONCLUSION,0.4147727272727273,"D Experiment Setup
26"
CONCLUSION,0.4161931818181818,Published as a conference paper at ICLR 2022
CONCLUSION,0.41761363636363635,"A
ADDITIONAL RESULTS"
CONCLUSION,0.4190340909090909,"A.1
ADDITIONAL RESULTS FOR SECTION 5"
CONCLUSION,0.42045454545454547,"We include additional ﬁgures to illustrate our analytic results in Section 5. In Figure 6 we plot
the stationary variance of SVGD and MMD-descent (with the Gaussian RBF kernel and median
bandwidth heuristic) under ﬁxed dimensionality d against varying number of particles n. As shown
in Figure 6(a), the particle variance of SVGD scales linearly with 1/d when γ > 1 (i.e., d > n), as
predicted by Proposition 3 and Corollary 4; this again conﬁrms the variance collapse phenomenon.
When n > d, our analysis cannot predict the stationary variance accurately, but the value empirically
approaches the target variance from below as γ decreases."
CONCLUSION,0.421875,"Non-asymptotic Correction for MMD-descent.
As noted in Section 5, there is a small gap be-
tween the empirical particle variance of MMD-descent and the prediction of Proposition 3, due to
ﬁnite-particle error in the empirical simulation. To characterize this discrepancy, we now consider a
regime where n is ﬁxed and d is large. In this case, we show that SVGD collapses all the particles to
0, whereas MMD-descent with the Gaussian RBF kernel can still estimate the marginal variance up
to O(1/n) error (see Appendix C for derivation)."
CONCLUSION,0.42329545454545453,"Proposition 7. Consider the setting of ﬁxed n and d →∞, then given (A2)(A3), particles driven by
SVGD (2) collapses to zero variance: vSVGD →0, whereas MMD-descent (4) with the Gaussian RBF
kernel leads to vMMD →(n −1)/(n + 1)."
CONCLUSION,0.4247159090909091,"Figure 6(b) demonstrates the agreement between the proposition and ﬁnite-particle behavior of MMD-
descent. Observe that the ﬁnite-particle error decays at 1/n and is independent to the dimensionality.
In other words, our analysis suggests that to achieve non-trivial estimation of the target variance,
SVGD requires Θ(d) particles, whereas for MMD-descent Od(1) particles sufﬁce."
CONCLUSION,0.42613636363636365,"Based on this observation, we speculate that to match the accuracy of MMD-descent with n particles
(in terms of marginal variance), SVGD requires N = nd particle. This conjecture is empirically
conﬁrmed in Figure 6(c): when we increase the number of particles in SVGD by an additional factor
of d, then the equilibrium variance on longer depends on the dimensionality and becomes similar to
that of MMD-descent with n particles. This again suggests that SVGD may not be reliable unless the
number of particles scale with the problem dimensionality."
CONCLUSION,0.4275568181818182,"(a) SVGD (n particles).
(b) MMD-descent (n particles).
(c) SVGD (N = nd particles)."
CONCLUSION,0.4289772727272727,"Figure 6: Dimension-averaged marginal variance of particles converged under (a) SVGD, (b) MMD and (c)
SVGD with N = nd particles. The target is a unit Gaussian, and we employ the Gaussian RBF kernel with
median bandwidth. Dashed lines correspond to predicted values. (a) when n, d jointly scales and d > n, the
variance of SVGD scales linearly with n and 1/d as predicted by Proposition 3. (b) for small n, variance of
MMD-descent approaches 1 as n increases (independent to d), which agrees with Proposition 7. (c) if the
particle size of SVGD is up-scaled by a factor of d, then the underestimation of variance is of order O(1/n)."
CONCLUSION,0.4303977272727273,"Variance Collapse in Almost-Gaussian Target.
In Section 5 we focus on Gaussian target for
precise characterization of the stationary variance. For more general target distributions, if the
potential behaves “Gaussian-like” outside of a certain radius (smaller than the true target variance),
then under the same assumption on the ﬁxed point particles (A3), we intuitively expect SVGD to
underestimate the marginal variance when γ = d/n is large."
CONCLUSION,0.4318181818181818,"Corollary 8. Given p(x) ∝exp(−f(x)) satisfying Ep[x] = 0, E[xx⊤] = Id, and assume f
exhibits Gaussian tail growth outside a Euclidean ball with radius 1, then given (A1-3), there exists
γ∗= d/n > 1 such that SVGD underestimates the dimension-averaged variance for γ > γ∗."
CONCLUSION,0.43323863636363635,Published as a conference paper at ICLR 2022
CONCLUSION,0.4346590909090909,"While the proposition above only considers quadratic growth of the potential (similar to Mattingly et al.
(2002)), we empirically verify that SVGD underestimates the target variance under anisotropy as well
as different tail growth condition. In Figure 7(a) we construct the target distribution to be a mixture of
two Gaussians with mean µ1, µ2 such that µ1 + µ2 = 0, ∥µ1 −µ2∥2 = 1, whereas in Figure 7(b)
we consider a factorized distribution with a cubic-growth potential: log p(x) ∝−Qd
i=1(x(i))3/3.
In both cases we observe similar variance collapse phenomenon of SVGD across different choices of
kernel and bandwidth."
CONCLUSION,0.43607954545454547,"Log-inverse Kernel.
We also illustrate the variance collapse phenomenon of SVGD with the log-
inverse kernel proposed in (Chen et al., 2018b). This heavy-tailed kernel is deﬁned as k(x, x′) =

α + log(1 + ∥x −x′∥2
2 /σ2)
−1
. One can easily verify that it is a Euclidean distance kernel
satisfying assumption (A2), and thus we also expect variance collapse to occur in learning the unit
Gaussian target (A3). This is indeed conﬁrmed in Figure 7(c)(d), where we see that the log-inverse
kernel with both the median heuristic and ﬁxed bandwidth (σ = 1) underestimates the marginal
variance, and in the case of median bandwidth, we can analytically predict the equilibrium variance
for γ > 1 via Proposition 3."
CONCLUSION,0.4375,"25
50
75
100
125
Number of particles n 0.2 0.3 0.4 0.5 0.6 0.7"
CONCLUSION,0.43892045454545453,Marginal variance
CONCLUSION,0.4403409090909091,"Gaussian RBF (median)
Gaussian RBF (fixed)
IMQ (median)
IMQ (fixed)"
CONCLUSION,0.44176136363636365,"(a) SVGD
(cubic potential)."
CONCLUSION,0.4431818181818182,"25
50
75
100
125
Number of particles n 0.2 0.4 0.6"
CONCLUSION,0.4446022727272727,Marginal variance
CONCLUSION,0.4460227272727273,"Gaussian RBF (median)
Gaussian RBF (fixed)
IMQ (median)
IMQ (fixed)"
CONCLUSION,0.4474431818181818,"(b) SVGD
(MoG potential)."
CONCLUSION,0.44886363636363635,"50
100
150
200
Dimensionality (d) 0.2 0.4 0.6 0.8 1.0"
CONCLUSION,0.4502840909090909,Marginal variance
CONCLUSION,0.45170454545454547,"Target variance
SVGD (n=25)
SVGD (n=50)
SVGD (n=75)"
CONCLUSION,0.453125,"(c) log-inverse kernel
(median)."
CONCLUSION,0.45454545454545453,"102
6 × 101
2 × 102"
CONCLUSION,0.4559659090909091,Dimensionality (d)
CONCLUSION,0.45738636363636365,"3 × 10
2"
CONCLUSION,0.4588068181818182,"4 × 10
2"
CONCLUSION,0.4602272727272727,"6 × 10
2"
CONCLUSION,0.4616477272727273,Marginal variance
CONCLUSION,0.4630681818181818,"= 2
= 3"
CONCLUSION,0.46448863636363635,"= 4
= 5"
CONCLUSION,0.4659090909090909,"(c) log-inverse kernel
(ﬁxed σ = 1)."
CONCLUSION,0.46732954545454547,"Figure 7: (a)(b) Empirical demonstration of variance collapse beyond Gaussian target. (c)(d) For unit Gaussian
target, SVGD with the log-inverse kernel also underestimates the marginal variance."
CONCLUSION,0.46875,"A.2
EMPIRICAL VERIFICATION OF (A2)"
CONCLUSION,0.47017045454545453,"We empirically validate the near-orthogonality assumption in Section 5 in learning Gaussian targets.
In particular, we plot the following quantities for particles driven by SVGD and MMD-descent till
convergence (from random initialization):"
CONCLUSION,0.4715909090909091,"(i) d−1/2maxi{∥xi∥2
2 −υd};
(ii) d−1/2maxi,j{x⊤
i xj}."
CONCLUSION,0.47301136363636365,"Note that for (A2) to hold, the above quantity should decay with the dimensionality d."
CONCLUSION,0.4744318181818182,"50
100
150
200
250
number of particles 10
1"
CONCLUSION,0.4758522727272727,"maxi{||xi||2
2
vd} /
d"
CONCLUSION,0.4772727272727273,"SVGD (Gaussian)
MMD (Gaussian)
SVGD (IMQ)
MMD (IMQ)"
CONCLUSION,0.4786931818181818,"d−1/2maxi{∥xi∥2
2 −υd}
(Gaussian target)."
CONCLUSION,0.48011363636363635,"50
100
150
200
250
number of particles 10
1"
CONCLUSION,0.4815340909090909,"maxij{xT
i xj} /
d"
CONCLUSION,0.48295454545454547,"d−1/2maxi,j{x⊤
i xj}
(Gaussian target)."
CONCLUSION,0.484375,"50
100
150
200
250
number of particles 10
1"
CONCLUSION,0.48579545454545453,"maxi{||xi||2
2
vd} /
d"
CONCLUSION,0.4872159090909091,"d−1/2maxi{∥xi∥2
2 −υd}
(MoG target)."
CONCLUSION,0.48863636363636365,"50
100
150
200
250
number of particles 10
1"
CONCLUSION,0.4900568181818182,"2 × 10
1"
CONCLUSION,0.4914772727272727,"3 × 10
1"
CONCLUSION,0.4928977272727273,"4 × 10
1"
CONCLUSION,0.4943181818181818,"6 × 10
1"
CONCLUSION,0.49573863636363635,"maxij{xT
i xj} /
d"
CONCLUSION,0.4971590909090909,"d−1/2maxi,j{x⊤
i xj}
(MoG target)."
CONCLUSION,0.49857954545454547,"Figure 8: Quantities of interest in Assumption (A2). We ﬁx γ > 1 and vary n, d to verify the dependence on d.
Particles are randomly initialized from N(0, 2Id) and optimized till convergence. (a)(b) unit Gaussian target
with γ = 2. (c)(d) MoG target with γ = 3."
CONCLUSION,0.5,"In Figure 8 we validate this hypothesis for both SVGD and MMD-descent on two different kernels:
the Gaussian RBF kernel and the IMQ kernel, and two target distributions: unit Gaussian and mixture
of Gaussians (MoG). For the Gaussian RBF kernel we use the median bandwidth heuristic, and for
the IMQ kernel we set the bandwidth σ =
√"
CONCLUSION,0.5014204545454546,"d. The MoG model consists of two Gaussians with
mean µ1 +µ2 = 0, ∥µ1 −µ2∥2 = 1. For both algorithms we initialize the particles from N(0, 2Id).
Observe that in all cases the quantities of interest indeed decrease with d. We leave the rigorous
justiﬁcation of this observation as future work."
CONCLUSION,0.5028409090909091,Published as a conference paper at ICLR 2022
CONCLUSION,0.5042613636363636,"A.3
ADDITIONAL RESULTS FOR MODIFIED SVGD"
CONCLUSION,0.5056818181818182,"In Section 5 we proposed a modiﬁcation of SVGD based on derivations of learning a unit Gaussian
target distribution in high dimensions (and kernels with the median bandwidth),"
CONCLUSION,0.5071022727272727,"∆Damp(xi) =
X"
CONCLUSION,0.5085227272727273,"j̸=i
[S1(xj, xi) + S2(xj, xi)] + λS1(xi, xi),"
CONCLUSION,0.5099431818181818,"where λ = max{1, (f(1) −γ−1f ′(1))/f(0)}. Note that λ = 1 recovers the original SVGD update,
whereas under extreme overparameterization (p ≫n) we have λ = f(1)/f(0) < 1 due to (A3). In
addition, larger λ implies smaller stationary variance of SVGD, vice versa. In the ﬁgures below we
refer to the SVGD update with λmin = min{1, (f(1) −γ−1f ′(1))/f(0)} as “fully-damped”, and the
update with λ in between λmin and 1 as “intermetidate”."
CONCLUSION,0.5113636363636364,"Bayesian Neural Network.
We now apply this modiﬁed update to the BNN setting discussed in
Section 3 (for detailed setup see Section D). We initialize 100 particles from a standard normal
distribution and then optimize them via (modiﬁed) SVGD with the Gaussian RBF kernel and median
bandwidth for 50000 steps with learning rate η = 5e −3. As shown in Figure 9, the original SVGD
update (Figure (b), original) signiﬁcantly underestimates the target variance (estimated using HMC),
and as we decrease λ, the variance of the predictions gradually increases (Figure (c), intermediate).
Observe that at our proposed damping factor (Figure (d), fully-damped), the modiﬁed update produces
diverse samples similar to HMC."
CONCLUSION,0.5127840909090909,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONCLUSION,0.5142045454545454,model output
CONCLUSION,0.515625,HMC (ground truth)
CONCLUSION,0.5170454545454546,(a) HMC (ground truth).
CONCLUSION,0.5184659090909091,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONCLUSION,0.5198863636363636,model output = 1
CONCLUSION,0.5213068181818182,(b) SVGD (original).
CONCLUSION,0.5227272727272727,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONCLUSION,0.5241477272727273,model output
CONCLUSION,0.5255681818181818,"= (f1
10
1f′
1)/f0"
CONCLUSION,0.5269886363636364,(c) SVGD (intermediate).
CONCLUSION,0.5284090909090909,"3
2
1
0
1
2
3
input x 1.0 0.8 0.6 0.4 0.2 0.0"
CONCLUSION,0.5298295454545454,model output
CONCLUSION,0.53125,"= (f1
1f′
1)/f0"
CONCLUSION,0.5326704545454546,(d) SVGD (fully-damped).
CONCLUSION,0.5340909090909091,"Figure 9: BNN experiment (γ = 100) for the proposed modiﬁcation of SVGD (Gaussian RBF kernel). Observe
that the original SVGD update (b) signiﬁcantly underestimates that target variance, whereas the proposed
modiﬁcation (d) leads to more diverse predictions."
CONCLUSION,0.5355113636363636,"In real-world problems, it is not likely that our proposed modiﬁcation (derived for Gaussian target)
always results in optimal performance. This being said, Figure 9 suggests that practitioners could tune
the damping term λ in the range [f(1)/f(0), 1] and adjust the desired level of uncertainty. Theoretical
justiﬁcation of our modiﬁed update is left as future work."
CONCLUSION,0.5369318181818182,"Bayesian Logistic Regression.
We also conduct additional experiment of the modiﬁed (damped)
SVGD update in a Bayesian logistic regression problem. Given input Z = {zi}m
i=1 ∈Rm×d, labels
y = {yi}m
i=1 ∈Rm, and parameter θ ∈Rd, we model the Bernoulli conditional distribution with
probability Pr(yi =1|zi) = 1/(1 + exp(−θ⊤zi)). We place an isotropic Gaussian prior on θ; the
posterior density is given as"
CONCLUSION,0.5383522727272727,p(θ) ∝exp 
CONCLUSION,0.5397727272727273,"y⊤Zθ − m
X"
CONCLUSION,0.5411931818181818,"i=1
log(1 + exp(−θ⊤zi)) −α"
CONCLUSION,0.5426136363636364,"2 ∥θ∥2
2 ! ."
CONCLUSION,0.5440340909090909,"Following Dalalyan (2014), we sample the coordinates of zi from a Rademacher distribution and then
normalize the vector by its Euclidean norm; the labels are generated from a Bernoulli distribution
with true parameters θ∗= 1d; we set m = 500, d = 100, and the regularization parameter α = 1."
CONCLUSION,0.5454545454545454,"We report the dimension-averaged marginal variance of the particles (in the n < d regime) optimized
by variants of SVGD in Figure 10(b). We use the Gaussian RBF kernel with the median bandwidth,
and for the ”intermediate” update we set λmin = min{1, (f(1) −2γ−1f ′(1))/f(0)}. The ground
truth variance is estimated by Langevin Monte Carlo (see Appendix A.4 for more discussion).
Figure 10(b) illustrates that for this problem, the original SVGD update (red) underestimates the
target variance, similar to the Gaussian case. However, our heuristic modiﬁcation (fully damped,"
CONCLUSION,0.546875,Published as a conference paper at ICLR 2022
CONCLUSION,0.5482954545454546,"20
40
60
80
100
number of particles 0.00 0.05 0.10 0.15 0.20 0.25"
CONCLUSION,0.5497159090909091,MMD (IMQ)
CONCLUSION,0.5511363636363636,"SVGD (original)
SVGD (fully damped)
SVGD (intermediate)"
CONCLUSION,0.5525568181818182,(a) BLR (MMD).
CONCLUSION,0.5539772727272727,"20
40
60
80
100
number of particles 0.1 0.2 0.3 0.4"
CONCLUSION,0.5553977272727273,dimension averaged variance
CONCLUSION,0.5568181818181818,ground truth (LMC)
CONCLUSION,0.5582386363636364,(b) BLR (variance).
CONCLUSION,0.5596590909090909,"0
200
400
600
800
1000
Number of iterations 0.0 0.5 1.0 1.5 2.0"
CONCLUSION,0.5610795454545454,Marginal variance
CONCLUSION,0.5625,"SVGD (original)
SVGD ( = 0)
Target variance"
CONCLUSION,0.5639204545454546,(c) damped SVGD (λ = 0).
CONCLUSION,0.5653409090909091,"Figure 10: (a)(b) MMD (IMQ kernel) and dimension-averaged variance of SVGD particles in Bayesian logistic
regression experiment. Observe that the modiﬁed update (blue) leads to smaller MMD (a), but may overestimate
the target variance in the small-particle regime (b). (c) Modiﬁed SVGD with λ = 0 leads to diverging particles.
Darker color represents larger particle size n."
CONCLUSION,0.5667613636363636,"blue) does not remove this bias completely – this is not surprising because the correction term is
derived only for the isotropic Gaussian target. In particular, the damped update might overestimate
the dimension-averaged variance in the small-particle regime. Nevertheless, we observe that the
proposed modiﬁcation generates more diverse samples (with variance closer to the true value)."
CONCLUSION,0.5681818181818182,"To further quantify the convergence of the particles, in Figure 10(a) we report the MMD between
the SVGD particles and (approximate) ground-truth samples obtained from LMC; we employ the
V-statistics MMD with the IMQ kernel (ﬁxed bandwidth). Observe that the modiﬁed update (blue,
green) results in smaller MMD compared to the original algorithm (red)."
CONCLUSION,0.5696022727272727,"Complete Removal of S1(xi, xi).
Recall that the SVGD update can be decomposed into three
terms: 1) driving force from each particle itself S1(xi, xi); 2) driving force from other parti-
cles P"
CONCLUSION,0.5710227272727273,"j̸=i S1(xj, xi); 3) repulsive force P"
CONCLUSION,0.5724431818181818,"j S2(xj, xi) (note that S2(xi, xi) = 0). Speciﬁcally,
S1(xi, xi) has a large impact on the update and the variance collapse: in the modiﬁcation above, we
are able to generate diverse samples just by decreasing the strength of this single term."
CONCLUSION,0.5738636363636364,"One might speculate that instead of using the speciﬁc λ derived for Gaussian target, an easier
alternative would be to completely remove S1(xi, xi), i.e., setting λ = 0. This is however not the
case. Following the same derivation as in Appendix C, one can see that when λ = 0, Equation 13
does not equilibriate at any ﬁnite variance for γ > 1. We empirically validate this ﬁnding in Figure
10(c), where we ﬁx d = 50 and vary the particle size n from 25 to 500, and optimize the particle
using (modiﬁed) SVGD with the Gaussian RBF kernel and the median bandwidth. Observe that while
the original SVGD (red) underestimates the target variance, setting λ = 0 (blue) leads to signiﬁcant
overestimation or even divergence when d > n."
CONCLUSION,0.5752840909090909,"One minor remark is that heuristically speaking, SVGD with λ = 0 roughly resembles MMD-descent
with ﬁxed log derivative S1 (see Figure 2(b) in Section 4), due to the presence of deterministic bias,
and that the driving force in both updates involves the log derivative S1, but does not include the
“self” term S1(xi, xi). As a result, we observe an overestimation of marginal variance (or diverging
particles) in both modiﬁed updates."
CONCLUSION,0.5767045454545454,"A.4
DISCUSSION ON OTHER SAMPLING ALGORITHMS"
CONCLUSION,0.578125,"In addition to SVGD and MMD-descent, there are many other approaches to draw (approximate)
samples from a given target distribution. We brieﬂy discuss two popular methods: Langevin Monte
Carlo (LMC) and Herding. In particular, we show that LMC does not suffer from variance collapse
in high dimensions; in fact, under some conditions, one particle sufﬁces to estimate the dimension-
averaged variance of the target."
CONCLUSION,0.5795454545454546,"Langevin Algorithm.
Langevin Monte Carlo (LMC) is a time discretization of the Langevin
diffusion, which can be interpreted as the (Wasserstein) gradient ﬂow of relative entropy (Jordan et al.,
1998). Convergence rate of the discrete-time algorithm has been analyzed under various metrics
(Dalalyan, 2014; Durmus and Moulines, 2017; Erdogdu et al., 2021) and assumptions on the target
distribution (Vempala and Wibisono, 2019; Erdogdu and Hosseinzadeh, 2020)."
CONCLUSION,0.5809659090909091,Published as a conference paper at ICLR 2022
CONCLUSION,0.5823863636363636,"Here we provide a short sketch that existing quantitative convergence guarantee of LMC implies
that the algorithm does not underestimate the dimension-averaged variance. For strongly convex and
smooth potentials (which includes the Gaussian setting in Section 5), Erdogdu et al. (2021, Theorem
4) showed that when the strong convexity parameter is dimension-free, then under appropriate
choice of step size, running LMC for T = ˜O(d) iterations leads to χ2(qT ||p) = O(1). We denote
the covariance Ep[xx⊤] = Σ, and assume Ep[x] = 0, Tr(Σ) = d WLOG. By properties of the
χ2-divergence and strongly log-concave concentration, given one sample xq drawn from qT ,"
CONCLUSION,0.5838068181818182,"Pr
  ∥xq∥2 −Ep ∥x∥2
 > t

≲exp
 
−t2
."
CONCLUSION,0.5852272727272727,"This concentration directly implies that in the high-dimensional limit d →∞(Assumption 1), we
have ∥xq∥2
2 / Tr(Σ)
p→1. In other words, the dimension-averaged variance Tr(Σ)/d can be reliably
estimated using one single particle driven by LMC for ˜O(d) steps. We remark that similar results
are expected to hold true under more general conditions such as the log-Sobolev inequality on the
target distribution, as well as other variants of the algorithm (Roberts and Tweedie, 1996; Cheng
et al., 2018b). We therefore speculate that variance collapse is a unique property of certain interacting
particle algorithms such as SVGD."
CONCLUSION,0.5866477272727273,"Kernel Herding.
Consider the approximation of an intractable distribution p(x) with a set of
particles X = {xi}n
i=1. To generate these particles, the kernel herding algorithm was introduced by
Welling (2009) to minimize the MMD between the particles and the target distribution. The algorithm
proceeds in a greedy manner: given the current set of selected particles {x1, · · · , xn−1}, the next
particle is chosen based on the following:"
CONCLUSION,0.5880681818181818,"xn ←argmin
x
MMD2 p, 1 n n−1
X"
CONCLUSION,0.5894886363636364,"i=1
δxi + δx !!"
CONCLUSION,0.5909090909090909,"= argmax
x
Ey∼p[k(x, y)] −1 n n−1
X"
CONCLUSION,0.5923295454545454,"i=1
k(x, xi)."
CONCLUSION,0.59375,"Intuitively, the ﬁrst term encourages sampling in high density areas for the target, whereas the second
term discourages drawing samples close to existing ones. It has been shown that the kernel herding
algorithm reduces the MMD at a rate O( 1"
CONCLUSION,0.5951704545454546,"N ) for ﬁnite-dimensional Hilbert spaces H (Welling, 2009;
Bach et al., 2012; Husz´ar and Duvenaud, 2012)."
CONCLUSION,0.5965909090909091,"Note that the same procedure can also be used to greedily minimize the kernel Stein discrepancy
(Chen et al., 2018b; 2019). In addition, the non-greedy gradient descent update on KSD (analogous
to MMD-descent) has been recently analyzed (Korba et al., 2021). We leave the high-dimensional
characterization of these algorithms as future work."
CONCLUSION,0.5980113636363636,"A.5
ADDITIONAL FIGURES FOR SECTION 4"
CONCLUSION,0.5994318181818182,"In Section 4 we conducted experiments that qualitatively identiﬁed the cause of variance collapse,
primarily for the Gaussian RBF kernel. Here we reproduce these ﬁndings for the IMQ kernel:"
CONCLUSION,0.6008522727272727,"k(x, y) = 1/
q"
CONCLUSION,0.6022727272727273,"1 + ∥x −y∥2
2 /2σ2), which is also commonly-used in SVGD. As shown in Figure 11,
results are qualitatively similar to that of Gaussian RBF kernel."
CONCLUSION,0.6036931818181818,"(a) MSE in estimating S1 and S2
(IMQ)."
CONCLUSION,0.6051136363636364,(b) MMD-descent (IMQ).
CONCLUSION,0.6065340909090909,"20
40
60
80
100
Dimensionality (d) 0.2 0.4 0.6 0.8 1.0"
CONCLUSION,0.6079545454545454,Marginal variance
CONCLUSION,0.609375,"SVGD (original)
SVGD (resampled S1)"
CONCLUSION,0.6107954545454546,SVGD (resampled S2)
CONCLUSION,0.6122159090909091,target variance
CONCLUSION,0.6136363636363636,(b) Resampled SVGD (IMQ).
CONCLUSION,0.6150568181818182,"Figure 11: Learning an isotropic Gaussian using the IMQ kernel. (a) Integration by parts with the IMQ kernel
leads to a large discrepancy in the variance of S1 and S2. (b) MMD with IMQ kernel leads to divergence under
∆MMD
1
(log derivative) with ﬁxed target samples. (c) IMQ-SVGD with resampled S1 (blue) correctly estimates
the target variance, but redrawing S2 (green) fails to provide more accurate samples."
CONCLUSION,0.6164772727272727,Published as a conference paper at ICLR 2022
CONCLUSION,0.6178977272727273,"20
40
60
80
100
Dimensionality (d) 0.2 0.4 0.6 0.8"
CONCLUSION,0.6193181818181818,Marginal variance
CONCLUSION,0.6207386363636364,"SVGD (original)
SVGD (resampled S1)
SVGD (resampled S2)"
CONCLUSION,0.6221590909090909,"Figure 12: Resampled SVGD in
learning the cubic-growth potential
deﬁned in Appendix A.1."
CONCLUSION,0.6235795454545454,"Finally, we provide additional empirical evidence that SVGD
with proper resampling procedure (see algorithm 1) reliably es-
timates the target variance. We consider the cubic-growth poten-
tial: p(x) ∝exp

−Qd
i=1(x(i))3/3

, and optimize the particles
using SVGD with the Gaussian RBF kernel (ﬁxed bandwidth
σ =
√"
CONCLUSION,0.625,"d). As shown in Figure 12, the original SVGD update
(red) underestimates the dimension-averaged marginal variance
inversely proportional to the problem dimensionality; in contrast,
when S1 is resampled (blue), then the estimated variance is stable
as d increases."
CONCLUSION,0.6264204545454546,"B
DERIVATION OF PROPOSITIONS IN SECTION 4"
CONCLUSION,0.6278409090909091,"Proof of Proposition 2.
For simplicity we ﬁrst assume the target is zero-centered: y ∼N(0, Id).
In this case when the kernel is Gaussian RBF, the expectation of S1, which we denote as µx, has the
following closed-form: µx = −
σd"
CONCLUSION,0.6292613636363636,"(1+σ2)d/2+1 exp

−∥x∥2
2
2+2σ2

x. We compute the mean squared error
of interest as follow:"
CONCLUSION,0.6306818181818182,"MSEp[S1(y, x)] =
Z"
CONCLUSION,0.6321022727272727,"y
∥S1(y, x) −µx∥2
2 p(y)dy =
Z"
CONCLUSION,0.6335227272727273,"y
∥−yk(x, y) −µx∥2
2 p(y)dy =
Z"
CONCLUSION,0.6349431818181818,"y
k2(x, y)yT y p(y)dy + 2µT
x Z"
CONCLUSION,0.6363636363636364,"y
k(x, y)y p(y)dy + µT
xµx Z"
CONCLUSION,0.6377840909090909,"y
p(y)dy"
CONCLUSION,0.6392045454545454,"=
e−
∥x∥2
2
2+σ2 σd"
CONCLUSION,0.640625,"(2 + σ2)d/2+1 (2∥x∥2
2 + dσ2) −2e−
∥x∥2
2
1+σ2 σ2d"
CONCLUSION,0.6420454545454546,"(1 + σ2)d+2 ∥x∥2
2 + e−
∥x∥2
2
1+σ2 σ2d"
CONCLUSION,0.6434659090909091,"(1 + σ2)d+2 ∥x∥2
2"
CONCLUSION,0.6448863636363636,"=∥x∥2
2
h2e−
∥x∥2
2
2+σ2"
CONCLUSION,0.6463068181818182,"σ2 + 2
 
σ2"
CONCLUSION,0.6477272727272727,"2 + σ2
d/2 −
e−
∥x∥2
2
1+σ2"
CONCLUSION,0.6491477272727273,"(σ2 + 1)2
 
σ2"
CONCLUSION,0.6505681818181818,"1 + σ2
di
+ de−
∥x∥2
2
2+σ2  
σ2"
CONCLUSION,0.6519886363636364,"2 + σ2
d/2+1."
CONCLUSION,0.6534090909090909,"Similarly, for the kernel derivative S2 we have,"
CONCLUSION,0.6548295454545454,"MSEp[S2(y, x)] =
Z"
CONCLUSION,0.65625,"y
∥S2(y, x) −(−µx)∥2
2 p(y)dy =
Z"
CONCLUSION,0.6576704545454546,"y
∥x −y"
CONCLUSION,0.6590909090909091,"σ2
k(x, y) + µx∥2
2 p(y)dy = 1 σ4 Z"
CONCLUSION,0.6605113636363636,"y
k2(x, y)yT y p(y)dy −2x σ4 Z"
CONCLUSION,0.6619318181818182,"y
k2(x, y)y p(y)dy −2µx σ2 Z"
CONCLUSION,0.6633522727272727,"y
k(x, y)y p(y)dy"
CONCLUSION,0.6647727272727273,+ xT x σ4 Z
CONCLUSION,0.6661931818181818,"y
k2(x, y) p(y)dy + 2xT µx σ2 Z"
CONCLUSION,0.6676136363636364,"y
k(x, y) p(y)dy + µT
xµx Z"
CONCLUSION,0.6690340909090909,"y
p(y)dy"
CONCLUSION,0.6704545454545454,"= e−
∥x∥2
2
2+σ2 σd−4"
CONCLUSION,0.671875,"(2 + σ2)d/2+1 (2∥x∥2
2 + dσ2) −4e−
∥x∥2
2
2+σ2 σd−4"
CONCLUSION,0.6732954545454546,"(2 + σ2)d/2+1 ∥x∥2
2 + 2e−
∥x∥2
2
1+σ2 σ2d−2"
CONCLUSION,0.6747159090909091,"(1 + σ2)d+2 ∥x∥2
2"
CONCLUSION,0.6761363636363636,"+ e−
∥x∥2
2
2+σ2 σd−4"
CONCLUSION,0.6775568181818182,"(2 + σ2)d/2+1 (2 + σ2)∥x∥2
2 −2e−
∥x∥2
2
1+σ2 σ2d−2"
CONCLUSION,0.6789772727272727,"(1 + σ2)d+1 ∥x∥2
2 + e−
∥x∥2
2
1+σ2 σ2d"
CONCLUSION,0.6803977272727273,"(1 + σ2)d+2 ∥x∥2
2"
CONCLUSION,0.6818181818181818,"=e−
∥x∥2
2
2+2σ2"
CONCLUSION,0.6832386363636364,"σ4
 
σ2"
CONCLUSION,0.6846590909090909,"2 + σ2
d/2+1(d + ∥x∥2
2) + 3e−
∥x∥2
2
1+σ2"
CONCLUSION,0.6860795454545454,"(1 + σ2)2
 
σ2"
CONCLUSION,0.6875,"1 + σ2
d∥x∥2
2."
CONCLUSION,0.6889204545454546,Published as a conference paper at ICLR 2022
CONCLUSION,0.6903409090909091,"The simpliﬁcation above largely follows from Ex∼N(µ,Σ)[∥x∥2
2] = µT µ + Tr(Σ). For non-centered
y ∼N(a, Id), note that replacing ∥x∥2
2 with ∥x −a∥2
2 does not affect the order dependence on d as
long as ∥a∥2
2 = O(d), and therefore the order that we aim to estimate remains the same."
CONCLUSION,0.6917613636363636,"Given the bandwidth heuristic σ = Θ(
√"
CONCLUSION,0.6931818181818182,"d) and ∥x∥2
2 = d, one can easily verify that:"
CONCLUSION,0.6946022727272727,"MSEp[S2(y, x)] ∈Θ(d−1),
MSEp[S1(y, x)] ∈Θ(d)."
CONCLUSION,0.6960227272727273,"To extend the result to general Euclidean distance kernel k(x, y) = f

∥x−y∥2
2
2σ2

and strong log-"
CONCLUSION,0.6974431818181818,"concave distributions satisfying
Ep(y)[y]
2
2 = O(d) and Ep(y)[∥y∥2
2] = O(d), ﬁrst note that"
CONCLUSION,0.6988636363636364,"Ey∼p[∥µx∥2] = Ey∼p[∥∇yk(y, x)∥2]"
CONCLUSION,0.7002840909090909,= C1Ey∼p
CONCLUSION,0.7017045454545454,"
x −y"
CONCLUSION,0.703125,"σ2
∇zf(z)

2"
CONCLUSION,0.7045454545454546,"
,
z = ∥x −y∥2
2
2σ2"
CONCLUSION,0.7059659090909091,"(i)
≤C2O(d−1) r"
CONCLUSION,0.7073863636363636,"Ey∼p
h
∥x −y∥2i
= O(d−1/2),"
CONCLUSION,0.7088068181818182,"where (i) is by Cauchy-Schwarz and the Lipschitzity of f. Similarly, following the expansion of
MSEp[S2] and the assumptions that k is upper-bounded, one can verify that MSEp[S2(y, x)] = O(1).
Finally, we lower-bound the variance of S1 via the following calculation,"
CONCLUSION,0.7102272727272727,"Ey∼p
h
∥S1(y, x) −µx∥2
2
i
= Ey∼p
h
∥∇y log p(y)∥2
2 k2(x, y)
i
−∥µx∥2
2"
CONCLUSION,0.7116477272727273,"≥C1Ey∼p
h
∥y∥2
2 k2(x, y)
i
−∥µx∥2
2"
CONCLUSION,0.7130681818181818,"(i)
≥C2Ey∼N(0,I)"
CONCLUSION,0.7144886363636364,"
∥y∥2
2 e−
∥x−y∥2
2
σ2

−∥µx∥2
2
(ii)
= Ω(d),"
CONCLUSION,0.7159090909090909,"where (i) is by the strongly log-concavity of p and the fact that the kernel being lower-bounded by a
scaled Gaussian RBF kernel, and (ii) directly follows from result in the Gaussian case. Combining
the calculations yields the desired statement."
CONCLUSION,0.7173295454545454,"C
DERIVATION OF PROPOSITIONS IN SECTION 5"
CONCLUSION,0.71875,"In this section we aim to calculate the equilibrium variance of SVGD and MMD-Descent under the
proportional asymptotics. We ﬁrst restate and comment on the Assumptions in Section 5."
CONCLUSION,0.7201704545454546,"• (A2) Near-orthogonality. Particles at ﬁxed point of SVGD (or MMD-descent) {xi}n
i=1 satisfy
|x⊤
i xi −dυ| < υϵd, |x⊤
i xj| < υϵd for all i ̸= j, some υ > 0, and d−1/2ϵd →0 as d →∞with
probability 1.
• (A4) Gaussian Target. p(x) ∝exp
 
−1"
CONCLUSION,0.7215909090909091,"2x⊤x

."
CONCLUSION,0.7230113636363636,"Under (A4) the goal of SVGD or MMD-descent is to draw samples from a unit Gaussian distribution,
and we aim to calculate the dimension-averaged variance v of the particles evolved by the two
updates. Note that since both SVGD and MMD-descent form an interacting particle system, one
can no longer treat the converged particles as i.i.d Gaussian samples, i.e. xi ∼N(0, vI). This
signiﬁcantly complicates the analysis in the proportional limit."
CONCLUSION,0.7244318181818182,"However, empirical results (see Figure 8) demonstrate that starting from non-degenerate random
initialization, the equilibrium solution of both algorithms admits thin-shell concentration around the
radius, i.e., the norm of xi concentrates around
√"
CONCLUSION,0.7258522727272727,"dv, and particles are almost orthogonal to one
another. This observation is captured by (A2). We conjecture that such property holds for the updates
we consider due to the repulsive force pushing the particles away from one another, and thus leading
to this near-orthogonal conﬁguration."
CONCLUSION,0.7272727272727273,"Under assumptions (A2)(A4), we are able to compute the stationary variance of both SVGD and
MMD-descent in the asymptotic limit. We ﬁrst provide a detailed derivation for the case of Gaussian
RBF kernel, and then extend the analysis to more general settings."
CONCLUSION,0.7286931818181818,Published as a conference paper at ICLR 2022
CONCLUSION,0.7301136363636364,"C.1
GAUSSIAN KERNEL WITH THE MEDIAN BANDWIDTH"
CONCLUSION,0.7315340909090909,"Stationary Variance of SVGD in Proportional Limit.
We aim to solve the stationary point of"
CONCLUSION,0.7329545454545454,"SVGD update (2). Under (A2) and the median heuristic σ =
q"
CONCLUSION,0.734375,"Med{∥xi −xj∥2
2}/2, the ﬁxed point
condition for the Gaussian RBF kernel is given as"
CONCLUSION,0.7357954545454546,"∆(xk) = 1 n n
X i=1"
CONCLUSION,0.7372159090909091,"h
−k(xi, xk)xi + 1"
CONCLUSION,0.7386363636363636,"dv k(xi, xk)(xk −xi)
i
= 0,
(6)"
CONCLUSION,0.7400568181818182,"for all k, or equivalently n
X"
CONCLUSION,0.7414772727272727,"i=1
k(xi, xk)xi =
1
dv + 1 n
X"
CONCLUSION,0.7428977272727273,"i=1
k(xi, xk) · xk.
(7)"
CONCLUSION,0.7443181818181818,"Note that for the left hand side of (7), we have the following equivalence, LHS = n
X"
CONCLUSION,0.7457386363636364,"i=1
k(xi, xk)xi = Xkk,"
CONCLUSION,0.7471590909090909,"where X = [x1, · · · , xn] ∈Rd×n is the data matrix, kk = [k(x1, xk), · · · , k(xn, xk)]⊤∈Rn is
the k-th column of kernel Gram matrix K ∈Rn×n. As for the RHS of Equation (7), note that for
i ̸= k, (A2) allows us to take the following Taylor expansion on entries of the kernel matrix around
its concentrated value, i.e.,"
CONCLUSION,0.7485795454545454,"k(xi, xk) = exp "
CONCLUSION,0.75,"−∥xi −xk∥2
2
2dv !"
CONCLUSION,0.7514204545454546,= e−1 + O(ϵ).
CONCLUSION,0.7528409090909091,"Where ϵ · d1/2 →0. Similarly for i = k we have k(xk, xk) ≈1. Combining the equations above,"
CONCLUSION,0.7542613636363636,"RHS =
1
dv + 1 n
X"
CONCLUSION,0.7556818181818182,"i=1
k(xi, xk) · xk ="
CONCLUSION,0.7571022727272727,"n + e −1
(dv + 1)e + O(ϵ) ! xk."
CONCLUSION,0.7585227272727273,Equating the RHS and LHS of Eq (7) in matrix form (over all k) we have
CONCLUSION,0.7599431818181818,X · K = n + e −1
CONCLUSION,0.7613636363636364,(dv + 1)eX + X · diag(ϵ).
CONCLUSION,0.7627840909090909,"where diag(ϵ) is a squared matrix where the i-th diagonal element is the error from Taylor expansion
ϵi = O(ϵ). Deﬁne"
CONCLUSION,0.7642045454545454,m = n + e −1
CONCLUSION,0.765625,dv + 1 .
CONCLUSION,0.7670454545454546,The ﬁxed point of SVGD thus simpliﬁes to
CONCLUSION,0.7684659090909091,"X · (K −mIn −diag(ϵ)) = 0.
(8)"
CONCLUSION,0.7698863636363636,"Denote A = K −mIn −diag(ϵ), Recall that the K is an Euclidean kernel matrix with Kij ="
CONCLUSION,0.7713068181818182,"k(xi, xk) = exp

−(2dv)−1 ∥xi −xk∥2
2

. From Theorem 4 in Bordenave et al. (2013), it follows"
CONCLUSION,0.7727272727272727,"that the empirical spectrum of A, which we write as µ(A) = n−1 Pn
i=1 δλi(A), converges weakly to
the following quantity,"
CONCLUSION,0.7741477272727273,"µ(A) →

1 −2"
CONCLUSION,0.7755681818181818,"e −m

+ 1"
CONCLUSION,0.7769886363636364,"eµ
 1"
CONCLUSION,0.7784090909090909,"dv X⊤X

+ µ(diag(ϵ)),"
CONCLUSION,0.7798295454545454,"In addition, deﬁne S = n/(n −1)In −1/(n −1)1n1⊤
n , then by the Hoffman-Wielandt inequality
(see Bordenave et al. (2013, Lemma 6)) we have W2"
CONCLUSION,0.78125,"
µ
 1"
CONCLUSION,0.7826704545454546,"dv X⊤X

, µ(S)

≤ s"
NTR,0.7840909090909091,"1
ntr
 1"
NTR,0.7855113636363636,"dv X⊤X −S
2
=
p"
NTR,0.7869318181818182,"n−1 · (nO(ϵ))2 →0,"
NTR,0.7883522727272727,Published as a conference paper at ICLR 2022
NTR,0.7897727272727273,"where we used (A2), and W2(·, ·) is the 2-Wasserstein distance. Hence, we know that"
NTR,0.7911931818181818,"µ(A) →

1 −2"
NTR,0.7926136363636364,"e −m

+ 1"
NTR,0.7940340909090909,"eµ
 1"
NTR,0.7954545454545454,"dv X⊤X

+ µ(diag(ϵ))"
NTR,0.796875,"→

1 −2"
NTR,0.7982954545454546,"e −m

+ 1"
NTR,0.7997159090909091,"eµ (S) + µ(diag(ϵ)).
(9)"
NTR,0.8011363636363636,"When γ > 1 (i.e. d > n), Equation (8) requires µ(A) →0. Consequently, 1 −2"
NTR,0.8025568181818182,"e +
n
e(n −1) −m = 0
⇔
m →1 −e−1."
NTR,0.8039772727272727,"Therefore, from the deﬁnition of m we have the desired result."
NTR,0.8053977272727273,"vSVGD →
n
d(e −1) =
1
e −1
1
γ .
(10)"
NTR,0.8068181818181818,"Stationary Variance of MMD-descent in Proportional Limit.
First note that for the Gaussian
RBF kernel, the driving force of MMD-descent (4) admits the following closed-form,"
NTR,0.8082386363636364,"Ey∼p[S2(y, x)] = −
σd"
NTR,0.8096590909090909,"(1 + σ2)d/2+1 exp

−∥x∥2
2
2 + 2σ2 
x."
NTR,0.8110795454545454,"This can be veriﬁed via simple numerical calculation:
Z
p(y)k(x, y)∇y log p(y)dy"
NTR,0.8125,"=
Z
e−
∥x−y∥2
2
2σ2
(−y)
1
p"
NTR,0.8139204545454546,(2π)d e−y⊤y
DY,0.8153409090909091,2 dy
DY,0.8167613636363636,"= −
1
p (2π)d Z y
ye− 1
√"
DY,0.8181818181818182,1+σ2 x−√ 1+σ2y !2
DY,0.8196022727272727,"2σ2
e−x⊤x"
DY,0.8210227272727273,2+2σ2 dy
DY,0.8224431818181818,"= −
σd"
DY,0.8238636363636364,"(1 + σ2)d/2+1 e−
∥x∥2
2
2+2σ2 x."
DY,0.8252840909090909,"Therefore, the stationary point of MMD-descent satisﬁes"
DY,0.8267045454545454,"∆xk = −
σd"
DY,0.828125,"(1 + σ2)d/2+1 e−∥xk∥2
2
2+2σ2 xk +
1
nσ2
X"
DY,0.8295454545454546,"i̸=k
k(xk, xi)(xk −xi) = 0,"
DY,0.8309659090909091,"∀k, or equivalently, n
X"
DY,0.8323863636363636,"i=1
k(xk, xi)xk −

dv
1 + dv"
DY,0.8338068181818182,"d/2+1
e−∥xk∥2
2
2+2dv nxk = n
X"
DY,0.8352272727272727,"i=1
k(xk, xi)xi."
DY,0.8366477272727273,"Under assumptions (A2)(A3), similar to the SVGD case, we have the matrix form of the ﬁxed point,
"""
DY,0.8380681818181818,"1 + e−1(n −1) −

dv
1 + dv"
DY,0.8394886363636364,"d/2+1
e−
dv
2+2dv n #"
DY,0.8409090909090909,X + Xdiag(ϵ) = XK.
DY,0.8423295454545454,"with ϵ = o(1). Following an anlogous calculation, as n, d →∞with d/n = γ ∈(1, ∞) we obtain,"
DY,0.84375,"1 + e−1(n −1) −

dv
1 + dv"
DY,0.8451704545454546,"d/2+1
e−
dv
2+2dv n →1 −1"
DY,0.8465909090909091,"e.
(11)"
DY,0.8480113636363636,"Moreover, observe that limd→∞(dv/(1 + dv))d/2+1 = e−1/(2v). We arrive at the desired result,"
DY,0.8494318181818182,vMMD →1.
DY,0.8508522727272727,"Following the exact same reasoning, one can show that for ﬁxed bandwidth σ = c
√"
DY,0.8522727272727273,"d for some
constant c ∈Θ(1), MMD-descent also estimates the variance correctly, i.e., vMMD = 1."
DY,0.8536931818181818,Published as a conference paper at ICLR 2022
DY,0.8551136363636364,"SVGD and MMD-descent with Finite n and Large d.
From (A2) we have the following decom-
position for i ̸= j,"
DY,0.8565340909090909,"∥xi −xj∥2
2 = ∥xi∥2
2 + ∥xj∥2
2 −2x⊤
i xj = 2dv + 2(n −1)−1dv + O(ϵ) =
2n
n −1dv + O(dϵ)."
DY,0.8579545454545454,"For ﬁnite n, this indicates that the Euclidean distance between two particles concentrates around
2(n −1)−1ndv (instead of 2dv); this is to say, the Gaussian RBF kernel admits the the following
Taylor expansion for i ̸= j,"
DY,0.859375,"k(xi, xj) = exp "
DY,0.8607954545454546,"−∥xi −xk∥2
2
2dv !"
DY,0.8622159090909091,"= e−
n
n−1 + O(ϵ)."
DY,0.8636363636363636,"Therefore, for the MMD-descent algorithm, (11) reduces to"
DY,0.8650568181818182,"1 + e−
n
n−1 (n −1) −

dv
1 + dv"
DY,0.8664772727272727,"d/2+1
e−
dv
2+2dv n →1 −e−
n
n−1 ."
DY,0.8678977272727273,Hence the converged particles of MMD-descent with ﬁnite n and inﬁnite d has variance
DY,0.8693181818181818,vMMD →n −1
DY,0.8707386363636364,n + 1.
DY,0.8721590909090909,"And for SVGD, one can use the exact same argument to obtain that"
DY,0.8735795454545454,vSVGD →0.
DY,0.875,"Note that this result agrees with our characterization in the proportional asymptotic limit by taking
γ →∞(i.e., d ≫n)."
DY,0.8764204545454546,"C.2
GENERAL KERNEL WITH ADAPTIVE BANDWIDTH (MEDIAN HEURISTIC)"
DY,0.8778409090909091,"Recall the deﬁnition of the (bandwidth-adjusted) Euclidean distance kernel: k(x, y) = f

∥x−y∥2
2
2σ2

."
DY,0.8792613636363636,"Under (A2) and the median heuristic σ =
q"
DY,0.8806818181818182,"Med{∥xi −xj∥2
2}/2, we know that the kernel can"
DY,0.8821022727272727,"be equivalently written as k(x, y) = f

∥x−y∥2
2
2dv

. Following the same procedure, we have the
following ﬁxed point condition,"
DY,0.8835227272727273,"∆(xk) = 1 n n
X i=1 "" −f"
DY,0.8849431818181818,"∥xi −xk∥2
2
2dv ! xi −1"
DY,0.8863636363636364,"dv f ′
 
∥xi −xk∥2
2
2dv !"
DY,0.8877840909090909,(xk −xi) #
DY,0.8892045454545454,"= 0,
(12)"
DY,0.890625,"for all k, or equivalently n
X i=1 "" f"
DY,0.8920454545454546,"∥xi −xk∥2
2
2dv ! −1"
DY,0.8934659090909091,"dv f ′
 
∥xi −xk∥2
2
2dv !#"
DY,0.8948863636363636,"xi = −1 dv n
X"
DY,0.8963068181818182,"i=1
f ′
 
∥xi −xk∥2
2
2dv ! xk."
DY,0.8977272727272727,"Similar to the previous calculation on the Gaussian RBF kernel, applying (A2) and the differentiability
of f around 1, we Taylor-expand the kernel and obtain the following equivalence:"
DY,0.8991477272727273,X · (K −1
DY,0.9005681818181818,dv K′) = −f(0) + (n −1)f ′(1)
DY,0.9019886363636364,"dv
· X + X · diag(ϵ)."
DY,0.9034090909090909,"where K is the Gram matrix of the kernel k, K′ is the Gram matrix of its derivative with K′
ij ="
DY,0.9048295454545454,"f ′  ∥xi−xj∥2
2
2dv

. Apply Theorem 4 of Bordenave et al. (2013) to K and K′, we have the following
relation under (A1)(A2),"
DY,0.90625,f(0) −f(1) −1
DY,0.9076704545454546,dv (f ′(0) −f ′(1)) = −f(0) + (n −1)f ′(1)
DY,0.9090909090909091,"dv
,
(13)"
DY,0.9105113636363636,"which gives the expression of the equilibrium variance,"
DY,0.9119318181818182,"vSVGD →
f ′(1)
f(1) −f(0) · 1 γ ."
DY,0.9133522727272727,Published as a conference paper at ICLR 2022
DY,0.9147727272727273,"Heuristic Derivation of the Modiﬁed (Damped) Update.
We provide a brief sketch of the modi-
ﬁed algorithm in Section 5 for the median bandwidth. Recall that our proposed update introduces
a damping term λf(0)xk in the driving force S1. To derive the optimal λ for unit Gaussian target,
we incorporate this damping term into the ﬁxed point Equation (12), which leads to the following
modiﬁcation of Equation (13):"
DY,0.9161931818181818,f(0) −f(1) −1
DY,0.9176136363636364,"dv (f ′(0) −f ′(1)) = (1 −λ)f(0) −f(0) + (n −1)f ′(1) dv
."
DY,0.9190340909090909,Recall the true target variance v = 1; this gives following choice of λ:
DY,0.9204545454545454,"λ = f(0)−1 ·
 
f(1) −γ−1f ′(1)

.
(14)"
DY,0.921875,"Since our goal is to “weaken” the driving force S1(xk, xk), we take the minimum between the
derived value in (14) and the default λ = 1."
DY,0.9232954545454546,"C.3
GENERAL KERNEL WITH FIXED BANDWIDTH"
DY,0.9247159090909091,"We now consider Euclidean distance kernel with invariant (ﬁxed) bandwidth that scales with the
dimensionality d, which we write as k(x, y) = f(∥x −y∥2
2 /2d). Following (12), we have the
following equilibrium condition for the particles, n
X i=1"
DY,0.9261363636363636,"
f

∥xi −xk∥2
2 /2d

−1"
DY,0.9275568181818182,"df ′ 
∥xi −xk∥2
2 /2d

xi = −1 d n
X"
DY,0.9289772727272727,"i=1
f ′ 
∥xi −xk∥2
2 /2d

xk."
DY,0.9303977272727273,"Under Taylor expansion around v, the ﬁxed point condition entails that the stationary variance
satisﬁes,"
DY,0.9318181818181818,f(0) −f(v) + 1
DY,0.9332386363636364,d(f ′(0) −f ′(v)) = −1
DY,0.9346590909090909,d(f(0) + (n −1)f ′(v)).
DY,0.9360795454545454,"⇐⇒
f ′(v)
f(v) −f(0) = γ.
(15)"
DY,0.9375,"Note that given (A3) and monotone f ′, the numerator of the RHS of (15) takes a negative value that
increases with v, whereas the denominator is also negative but decreases with v. This implies that
when γ becomes larger, v needs to decay towards 0 in order to satisfy the equation."
DY,0.9389204545454546,"For the Gaussian RBF kernel with ﬁxed bandwidth σ =
√"
DY,0.9403409090909091,"d, the equation can be easily solved as,"
DY,0.9417613636363636,"vRBF = log

1 + 1 γ"
DY,0.9431818181818182,"
< 1,"
DY,0.9446022727272727,which is a decreasing function of γ > 1.
DY,0.9460227272727273,"On the other hand, for the IMQ kernel with ﬁxed bandwidth σ =
√"
DY,0.9474431818181818,"d, standard calculation yields,"
DY,0.9488636363636364,vIMQ = 1 6
DY,0.9502840909090909,"4γ(γ + 3)

3γ4p"
DY,0.9517045454545454,"3(8γ + 27) + 8γ6 + 36γ5 + 27γ4
1/3 +"
DY,0.953125,"
3γ4p"
DY,0.9545454545454546,3(8γ + 27) + 8γ6 + 36γ5 + 27γ41/3 γ2 ! −2 3.
DY,0.9559659090909091,One can numerically verify that the value is less than 1 for γ > 1 and also non-increasing.
DY,0.9573863636363636,"IMQ Kernel with Dimension-independent Bandwidth.
Finally, we note that in the context
of kernel Stein discrepancy (KSD), the IMQ kernel is often employed without the dimension-
dependent bandwidth (Gorham and Mackey, 2017). In this case we write the kernel as k(x, y) =
f(∥x −y∥2
2 /2), which gives the following stationary condition, n
X i=1"
DY,0.9588068181818182,"h
f

∥xi −xk∥2
2 /2

−f ′ 
∥xi −xk∥2
2 /2
i
xi = − n
X"
DY,0.9602272727272727,"i=1
f ′ 
∥xi −xk∥2
2 /2

xk."
DY,0.9616477272727273,Published as a conference paper at ICLR 2022
DY,0.9630681818181818,"Taking Taylor expansion around dv gives
f(0) −f(vd) −f ′(0) + f ′(vd) = −f(0) −(n −1)f ′(vd).
Note that the LHS is Θ(1) by (A2); in order for the inequality to hold asymptotically, we need to
have f ′(vd) = Θ(n−1). On the other hand, for the ﬁxed-bandwidth IMQ kernel,
f(a) = (1 + a)−1/2;
2f ′(a) = −(1 + a)−3/2."
DY,0.9644886363636364,"This implies that as d increases, the stationary variance decays to 0 at a rate of v = Θ(d−1/3).
Following the exact same procedure, one can show that the log-inverse kernel with ﬁxed dimension-
independent bandwidth also asymptotically collapses the variance of SVGD particles to 0 when
γ > 1; we omit the derivation."
DY,0.9659090909090909,"C.4
“ALMOST-GAUSSIAN” TARGET"
DY,0.9673295454545454,"For general p(x) ∝exp(−f(x)), the ﬁxed point equation of xk is given as,"
DY,0.96875,"∆(xk) = 1 n n
X i=1 "" −f"
DY,0.9701704545454546,"∥xi −xk∥2
2
2σ2 !"
DY,0.9715909090909091,∇f(xi) −1
DY,0.9730113636363636,"σ2 f ′
 
∥xi −xk∥2
2
2σ2 !"
DY,0.9744318181818182,(xk −xi) # = 0.
DY,0.9758522727272727,"Assume that SVGD does not underestimate that marginal variance, then by (A2) and the quadratic
growth, we know that there exists some α > 0 such that for every particle xk and coordinate m ∈[d],
"" n
X"
DY,0.9772727272727273,"i=1
αf"
DY,0.9786931818181818,"∥xi −xk∥2
2
2σ2 ! xi −1"
DY,0.9801136363636364,"σ2 f ′
 
∥xi −xk∥2
2
2σ2 ! xi #"
DY,0.9815340909090909,"m
= −1 σ2 "" n
X"
DY,0.9829545454545454,"i=1
f ′
 
∥xi −xk∥2
2
2σ2 ! xk # m
."
DY,0.984375,"For kernels with the median bandwidth, followings the same simpliﬁcation as in (13), we get"
DY,0.9857954545454546,"X ·

αK −1"
DY,0.9872159090909091,"dv K′

= −f(0) + (n −1)f ′(1)"
DY,0.9886363636363636,"dv
· X + X · diag(ϵ)."
DY,0.9900568181818182,"Solving the inequality yields, v = α"
DY,0.9914772727272727,"γ ·
f ′(1)
f(1) −f(0)."
DY,0.9928977272727273,"Therefore, given any dimension-independent growth of the potential α, there exists a large enough
γ such that v < 1, i.e., SVGD underestimates the marginal variance. The case for ﬁxed bandwidth
follows from the same line of reasoning, the details of which we omit."
DY,0.9943181818181818,"D
EXPERIMENT SETUP"
DY,0.9957386363636364,"Bayesian Neural Network.
We consider a BNN with two hidden layers of 100 units. In each layer,
the preactivations st+1 are computed via st+1 = (W tat + bt)/√ht + 1, where at ∈Rht are the
input activations. The target function f is a BNN with the same architecture, whose weights and
biases are randomly generated from standard normal distributions. For the training set, we sampled
10 input locations uniformly from [−2.5, −1.5] and [1.5, 2.5], respectively. We add random noises to
the observations, y = f(x) + ϵ, ϵ ∼N(0, 0.01)."
DY,0.9971590909090909,"We ﬁrst adopt the Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) to generate asymptotic
posterior samples. HMC used 50 independent chains and each chain selected 50 particles with the
frequency of 100 iterations after 5k burn-in iterations. HMC generates diverse particles as shown in
Figure 1(a). We then simulate SVGD and MMD-descent dynamics on this problem. For both updates
we use the Gaussian RBF kernel with the median bandwidth heuristic. For SVGD, we evolve 100
particles for 50k iterations using learning rate η = 5e −3, whereas for MMD-descent, we evolve
10 particles using HMC particles as approximate target samples to compute the driving force term.
The particles are either initialized from the (approximate) target distribution (as in Figure 1), or from
standard normal distribution (as in Figure 9). Note that the plots do not include observation variance."
DY,0.9985795454545454,"Hyperparameter Setting in Section 5.
For all experiments in Section 5, we initialize the particles
from N(0, 0.8Id), and run SVGD (or MMD-descent) with learning rate η = 10−1 for 20k iterations.
For experiments in the proportional limit (Figure 4(a)(b)), we ﬁx n = 50 and vary d. For the median
heuristic (following Garreau et al. (2017)), we compute the median of Euclidean distance between all
particles at each iteration."
