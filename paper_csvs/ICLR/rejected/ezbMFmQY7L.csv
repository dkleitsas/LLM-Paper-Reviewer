Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004098360655737705,"Methods for designing organic materials with desired properties have high po-
tential impact across ﬁelds such as medicine, renewable energy, petrochemical
engineering, and agriculture. However, using generative models for this task is
difﬁcult because candidate compounds must satisfy many constraints, including
synthetic accessibility, intellectual property attributes, “chemical beauty” (Bicker-
ton et al., 2012), and other considerations that are intuitive to domain experts but
can be challenging to quantify. We propose C5T5, a novel self-supervised pre-
training method that works in tandem with domain experts by making zero-shot
select-and-replace edits, altering organic substances towards desired property val-
ues. C5T5 operates on IUPAC names—a standardized molecular representation
that intuitively encodes rich structural information for organic chemists but that
has been largely ignored by the ML community. Our technique requires no edited
molecule pairs to train and only a rough estimate of molecular properties, and
it has the potential to model long-range dependencies and symmetric molecular
structures more easily than graph-based methods. We demonstrate C5T5’s ef-
fectiveness on four physical properties relevant for drug discovery, showing that it
learns successful and chemically intuitive strategies for altering molecules towards
desired property values."
ABSTRACT,0.00819672131147541,"Figure 1: Increasing a molecule’s octanol-water partition coefﬁcient with C5T5. (1) A molec-
ular fragment (acetyloxy) is identiﬁed in a molecule of interest. (2) The molecular fragment is
replaced with a mask token (<mask>) and the property value is set to the desired bucket (<high>).
(3) Sampling from C5T5 produces a new fragment (decyl). Substituting in the fragment yields a
new molecule. The long chain of carbons added to the molecule increases its solubility in octanol
while decreasing its solubility in water."
INTRODUCTION,0.012295081967213115,"1
INTRODUCTION"
INTRODUCTION,0.01639344262295082,"Organic molecules are used in countless applications across human society: as medicines, industrial
chemicals, fuels, pesticides, plastics, television screens, solar cells, and many others. Traditionally,
new molecules are designed for particular tasks by hand, but the space of all possible molecules
is so vast (e.g. the total number of drug-like molecules may be as high as 1060) that most useful
materials are probably still undiscovered (Reymond et al., 2012). To automate materials discovery,
domain experts have turned to high-throughput screening, in which a large library of potentially
useful molecules is generated heuristically, and the most promising molecules are chosen for further
study using computational models that estimate how effective each substance will be for the target
application (Hughes et al., 2011). Unfortunately, even high-throughput methods can still only screen
a tiny fraction of all possible molecules."
INTRODUCTION,0.020491803278688523,Under review as a conference paper at ICLR 2022
-ACETYLOXYBENZOIC ACID,0.02459016393442623,2-acetyloxybenzoic acid
-ACETYLOXYBENZOIC ACID,0.028688524590163935,"1
2
3
4"
-ACETYLOXYBENZOIC ACID,0.03278688524590164,"CC(=O)OC1=C(C=CC=C1)C(O)=O
1 2    3  4 5     6  7   8 9  10  11 12  13 13
12"
-ACETYLOXYBENZOIC ACID,0.036885245901639344,"11
6
1 2 3 4 5
7 8 9 10"
-ACETYLOXYBENZOIC ACID,0.040983606557377046,"IUPAC Name
    SMILES"
-ACETYLOXYBENZOIC ACID,0.045081967213114756,"Figure 2: Visual representations of IUPAC names and SMILES. Tokens in IUPAC names corre-
spond to well-known functional groups and moieties. In contrast, tokens in SMILES correspond to
individual atoms and bonds."
-ACETYLOXYBENZOIC ACID,0.04918032786885246,"Generating molecules directly with machine learning addresses this limitation, but de novo genera-
tion can be of limited use in domains like drug discovery, where experts’ intuitions about structure-
activity relationships and external factors like patentability are important to consider in the design
process. These constraints can often be expressed by providing known portions of the molecular
structure; for example a domain expert may be interested in a particular scaffold because it has
favorable intellectual property attributes, or certain parts of a drug may be needed for the desired
biological activity, while other parts can be modiﬁed to increase bioavailability."
-ACETYLOXYBENZOIC ACID,0.05327868852459016,"To address this real-world setting, we consider the problem of learning to make localized modi-
ﬁcations to a molecule that change its physical properties in a desired way. We propose C5T5:
Controllable Characteristic-Conditioned Chemical Changer with T5 (Raffel et al., 2019), a novel
method for generative modeling of organic molecules that gives domain experts ﬁne-grained con-
trol over the molecular optimization process while also providing more understandable predictions
than prior methods (Figure 1). Our two key contributions are 1) recasting molecular modeling as
language modeling on the semantically rich IUPAC name base representation, and 2) the develop-
ment of a novel conditional language modeling strategy using transformers that supports targeted
modiﬁcations to existing molecules."
-ACETYLOXYBENZOIC ACID,0.05737704918032787,"IUPAC Names.
The IUPAC naming system is a systematic way of naming organic molecules
based on functional groups and moieties, or commonly occurring clusters of connected atoms that
have known chemical behaviors. Organic chemists have discovered countless chemical reactions
that operate on functional groups, and they use these reactions to develop synthesis routes for novel
molecules. Despite this, existing generative methods for organic molecules have ignored IUPAC
names as a representation, instead opting for atom-based representations like SMILES (Weininger,
1988) and molecular graphs (Duvenaud et al., 2015). See Figure 2 for a comparison of these rep-
resentations. We argue for several advantages of IUPAC names in Section 3.1. To the best of our
knowledge we are the ﬁrst to use IUPAC names as a base representation for molecular modeling."
-ACETYLOXYBENZOIC ACID,0.06147540983606557,"Self-Supervised Objective for Zero-Shot Editing.
To enable targeted modiﬁcations of molecules
without predeﬁned edit pairs, we train transformers with a conditional variant of a self-supervised
inﬁlling task, where the model must replace masked-out tokens in the IUPAC name. As described
in Section 3.2, we condition the model by prepending IUPAC names with discretized molecular
property values; the model then learns the conditional relationships between the property value and
molecular structure. To the best of our knowledge, C5T5 is the ﬁrst method to use conditional
inﬁlling for select-and-replace editing; we anticipate this method could be broadly applied in other
controlled generation contexts, such as modeling affect, politeness, or topic of natural language
(Ghosh et al., 2017; Ficler & Goldberg, 2017; Niu & Bansal, 2018; Keskar et al., 2019)."
-ACETYLOXYBENZOIC ACID,0.06557377049180328,"As we show in Section 4, C5T5 is able to make interpretable targeted modiﬁcations to molecules
that lead to desired changes across several physical properties important in drug design."
RELATED WORK,0.06967213114754098,"2
RELATED WORK"
RELATED WORK,0.07377049180327869,"Modeling.
A number of machine learning methods have been developed for the task of design-
ing organic molecules, but most do not allow a user to make targeted modiﬁcations to a molecule.
Some methods, like generative adversarial networks and unconditional sequence models, provide no"
RELATED WORK,0.0778688524590164,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08196721311475409,"control over a generated molecule’s structure (Grisoni et al., 2020; Guimaraes et al., 2017; Sanchez-
Lengeling et al.; De Cao & Kipf, 2018; Kajino, 2019), and are therefore more useful for generating
candidate libraries than optimizing a particular molecule. Other methods, like variational autoen-
coders or sequence models that are conditioned on a base molecule, allow specifying that generated
molecules should be similar to a starting molecule in some learned space, but there is no way to
speciﬁcally target a certain part of the molecule to modify (He et al., 2021b; Jin et al., 2019; Shin
et al., 2021; Yang et al., 2020; Kotsias et al., 2020; G´omez-Bombarelli et al., 2018; Lim et al., 2018;
Dollar et al., 2021; Liu et al., 2018; Jin et al., 2018; Maziarka et al., 2020; Olivecrona et al., 2017;
Bagal et al., 2021; You et al., 2018; Shi* et al., 2020). Recognizing the importance of leveraging
domain experts’ intuition about structure-activity relationships, several methods, published mostly
in chemistry venues, have explored constraining generated molecules to contain a scaffold, or a sub-
graph of the full molecular graph (Li et al., 2019; Lim et al., 2020; Maziarz et al., 2021). However,
these methods append to scaffolds arbitrarily instead of allowing domain experts to specify which
part of the molecule they would like to modify or append to, limiting their utility for human-in-the-
loop molecular optimization."
RELATED WORK,0.0860655737704918,"A few methods have explored allowing targeted modiﬁcations, where a domain expert can mask
out a portion of a starting molecule and ask the model to replace the mask with a novel side chain
(Ar´us-Pous et al., 2020; Langevin et al., 2020; He et al., 2021a). These methods are limited because
they only support masking parts of the molecule that can be truncated by cutting a single bond, and
because they require a dataset of paired molecules (scaffolds & decorators) that must be constructed
using hand-crafted rules. In contrast, C5T5 learns in an entirely unsupervised fashion and therefore
requires no paired data; the only limit to what can be masked is what can be represented using
IUPAC tokens."
RELATED WORK,0.09016393442622951,"Representation.
Existing methods all use SMILES (or a derivative representation) or graphs to
represent molecules. There are a number of drawbacks to using the SMILES representation: a small
change in a molecule can lead to a large change in the SMILES string (Jin et al., 2018); ﬂattening the
graph into a list of atoms artiﬁcially creates variable- and long-range dependencies between bonded
atoms; and it is difﬁcult to reason about common substructures, because the same structure can be
represented in many different ways depending on how the graph was ﬂattened. And although graphs
seem like a natural representation for molecules, graphs do a poor job encoding symmetry, long-
range interactions between atoms that are many bonds apart but nearby in 3D space, and long-range
interactions that arise from conjugated systems (Duvenaud et al., 2015). C5T5 operates instead
of IUPAC names, which we argue in Section 3.1 is a more suitable representation for molecular
optimization because tokens have much more semantic meaning. See Appendix A for more details
on how C5T5 relates to prior work."
RELATED WORK,0.0942622950819672,"Transformers for Molecular Modeling
Outside of molecular optimization, transformers have
found a number of applications in molecular modeling tasks, including property prediction (Wang
et al., 2019; Rong et al., 2020), chemical reaction prediction (Schwaller et al., 2019), retrosynthesis
(Karpov et al., 2019) and generating proteins (Elnaggar et al., 2020; Grechishnikova, 2021). A few
works have explored using transformers for generative modeling of organic molecules (He et al.,
2021b; Shin et al., 2021; Dollar et al., 2021). Some works have also proposed using transformers for
scaffold-conditioned generative modeling (He et al., 2021a; Bagal et al., 2021). This work extends
these efforts by proposing a simple yet effective training and zero-shot adaptation method, and by
using IUPAC names instead of SMILES strings."
RELATED WORK,0.09836065573770492,"IUPAC Names
Although we are unaware of prior work using IUPAC names as a base representa-
tion for molecular modeling, several works have explored using machine learning to convert between
IUPAC names and other molecular representations (Rajan et al., 2021; Handsel et al., 2021; Krasnov
et al., 2021)."
METHOD,0.10245901639344263,"3
METHOD"
METHOD,0.10655737704918032,"Molecular optimization is a difﬁcult problem because it requires modifying a molecule that al-
ready satisﬁes a number of requirements. Modiﬁcations need to improve a particular aspect of
the molecule without degrading its performance on other metrics, and without making it too difﬁcult"
METHOD,0.11065573770491803,Under review as a conference paper at ICLR 2022
METHOD,0.11475409836065574,"Figure 3: T-SNE visualization of the word2vec embedding space. “Charge”: tokens that indi-
cate formal charge. “Group”: functional groups and moieties. “Count/Mult”: multipliers. “Ring
loc.”: fused-ring locants. “Stereo”: stereochemistry markers. “Locants”: simple locants. “Ele-
ments”: single-atom tokens. As shown, the 2D location of tokens carries high semantic meaning;
for example, locants are not only collocated, but are approximately in order."
METHOD,0.11885245901639344,"to synthesize. We argue that by using IUPAC names (Section 3.1) and by allowing users to target
particular parts of a molecule to modify (Section 3.2), C5T5 has the potential to support human-
in-the-loop molecular editing that complements domain experts’ intuitions about structure-activity
relationships and synthetic accessibility."
IUPAC NAMING,0.12295081967213115,"3.1
IUPAC NAMING"
IUPAC NAMING,0.12704918032786885,"The International Union of Pure and Applied Chemistry (IUPAC) publishes a set of rules that allow
systematic conversion between a chemical structure and a human-readable name (Favre & Powell,
2013). For example, 2-chloropentane refers unambiguously to ﬁve carbons (“pent”) connected by
single bonds (“ane”) with a chlorine atom (“chloro”) bonded to the second carbon from one end
(“2-”). IUPAC names are used ubiquitously in scholarly articles, patents, and educational materials.
In contrast to other linear molecular representations like SMILES and its derivatives, where single
tokens mostly refer to individual atoms and bonds, tokens in IUPAC names generally have a rich
semantic meaning. For example, the token “ic acid” denotes a carboxylic acid, which is a com-
mon functional group that has well-known physical and chemical properties; there are many known
chemical reactions that either start with or produce carboxylic acids. Other tokens denote additional
functional groups (e.g. “imide,” “imine,” “al,” “one”), locants (e.g. “1,” “2,” “N”), which indicate
connectivity, alkanes (e.g. “meth,” “eth,” “prop”), which denote the lengths of carbon chains, poly-
cyclic rings (e.g. “naphthalene,” “anthracene”), stereochemistry markers (“R,” “S”), and multipliers"
IUPAC NAMING,0.13114754098360656,Under review as a conference paper at ICLR 2022
IUPAC NAMING,0.13524590163934427,"(e.g. “di,” “tri”), which concisely represent duplicated and symmetric structures. Figure 2 shows the
relationships between IUPAC names, graph representations, and SMILES."
IUPAC NAMING,0.13934426229508196,"For molecular optimization, C5T5 supports qualitatively different molecular edits compared to
graph- and SMILES-based methods by virtue of its use of IUPAC names: editing a locant token
corresponds to moving a functional group along a carbon backbone or changing the connectivity
of a fused ring system; and editing a multiplier token corresponds to creating or eliminating du-
plicated and symmetric structures. For example, changing “ethylbenzene“ to “hexaethylbenzene”
replicates the ethyl structure around the entire benzene ring with a single token edit. These sorts of
modiﬁcations require much more extensive editing for SMILES- and graph-based methods.1"
IUPAC NAMING,0.14344262295081966,"We argue that IUPAC names are especially attractive for molecular optimization, since the process
requires interaction between the algorithm and a domain expert, so interpretability is paramount.
Compared to graph- or SMILES-based models, C5T5 makes predictions that can be traced back to
moieties and functional groups that domain experts are more likely to understand, trust, and know
how to synthesize than either arbitrary collections of atoms and bonds or motifs decided upon by
machine learning practitioners."
IUPAC NAMING,0.14754098360655737,"In addition to improved interpretability, we argue that using IUPAC names has advantages purely
from the standpoint of modeling data, since moving from SMILES to IUPAC names is akin to
moving from a character-based to a word-based sequence model. Modeling at this higher level of
abstraction enables the network to direct more of its capacity to structure at the relevant semantic
level, instead of relearning lower-level details like the speciﬁc atomic composition of functional
groups. In this vein, we demonstrate the potential of IUPAC names by learning word2vec represen-
tations of IUPAC name tokens (Mikolov et al., 2013), drawn from a list of over 100 million names
in the PubChem repository (Kim et al., 2016) and tokenized using a list of tokens in OPSIN—an
open-source IUPAC Name parser library (MIT License) (Lowe et al., 2011). For example, as shown
in Figure 2, the chemical “2-acetyloxybenzoic acid” gets tokenized to [“2”, “-”, “acet”, “yl”, “oxy”,
“benzo”, “ic acid”]. As with natural language modeling, we ﬁnd that the embedding space learned
by word2vec encodes the semantic meaning of the tokens, as shown in Figure 3. Different classes
of tokens tend to be clustered together, and similar tokens within clusters are located nearby. For
example, aromatic compounds with two rings are clearly separated from those with three, locants
are ordered roughly correctly from 1 to 100, and multiplier tokens are also roughly in order (zoom
not shown). Following Mikolov et al. (2013), we also ﬁnd that simple arithmetic operations in the
embedding vector space correspond to semantic analogies between tokens. For example, the nearest
neighbor of “phosphonous acid” - “nitrous acid” + “nitroso” is the embedding for “phosphoroso.”2
The nearest neighbor of “diphosphate” - “disulfate” + “sulfate” is “phosphate.” Likewise for “sele-
nate” - “tellurate” + “tellurite” being closest to “selenite.”"
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.15163934426229508,"3.2
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS"
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1557377049180328,"We now present the C5T5 objective, which trains a model to alter the properties of a molecule
through localized edits. Importantly, this behavior does not require training on human-deﬁned edit
pairs, a strategy limited by either a ﬁxed set of hand-speciﬁed chemical alterations or an expensive
experimentally-derived training set. Instead, this editing behavior emerges as a zero-shot side-effect
of our conditional language modeling objective, requiring only a simple forward pass using our
pretrained model without additional gradient updates."
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1598360655737705,"Given a property value P, a fragment of a molecule F, and the rest of the molecule C (the context),
we wish to learn the conditional distribution P(F|C, P). Then, for a new molecule, one could
alter the molecule towards a desired property value by redacting the original F, changing P to P ′
and sampling a new F ′ ∼P(F|C, P ′). Intuitively, this asks our model what kinds of molecular
fragments the model would expect given the context and the new property value."
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.16393442622950818,"1Moving the attachment point of a functional group requires only a small edit of a graph (i.e. changing one
bond), but most graph-based molecular generation methods sequentially generate one node at a time, followed
by any bonds that connect the new node to the molecular graph so far. Moving a side chain therefore requires
removing the entire chain and regenerating it node by node.
2ignoring the embedding for “nitroso”"
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1680327868852459,Under review as a conference paper at ICLR 2022
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1721311475409836,"To learn this conditional distribution, we propose a conditional generalization of the inﬁlling objec-
tive used to train T5 (Raffel et al., 2019) and ILM (Donahue et al., 2020). This process consists of
several steps, also illustrated in Figure 1:"
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1762295081967213,1. Replacing random spans of the tokenized IUPAC name with sentinel tokens.
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.18032786885245902,"2. Prepending the resulting sequence with a token indicating the original molecule’s computed
property value. To obtain these property value tokens, we discretize the distribution of
property values into three buckets, speciﬁed in Table 5."
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.18442622950819673,"3. Training the model as in T5 to produce the sequence of redacted tokens, prepended by their
corresponding sentinel tokens."
CONDITIONAL LANGUAGE MODELING WITH TRANSFORMERS,0.1885245901639344,"This conditional inﬁlling objective incentivizes the model to learn the relationship between the com-
puted property value and the missing tokens. To make a localized edit to a molecule, we then replace
the desired fragments with sentinel tokens, change the property value token, and sample autoregres-
sively from the predictive distribution. Thus, our approach hybridizes the ﬂexible editing capabilities
of ILM (Donahue et al., 2020) with the controllability of CTRL (Keskar et al., 2019). See Appendix
C for experimental details. Code is available at redacted."
RESULTS,0.19262295081967212,"4
RESULTS"
RESULTS,0.19672131147540983,"To demonstrate the promise of combining IUPAC names with conditional modeling using T5, we
explore several molecular optimization tasks relevant to drug discovery.
Speciﬁcally, we train
C5T5 to make localized changes that affect the octanol-water partition and distribution coefﬁcients
(logP, logD), polar surface area (PSA), and refractivity—four properties commonly used to estimate
bioavailability of a candidate drug (Ghose et al., 1999; Veber et al., 2002; Bhal et al., 2007). logP
and logD measure the ratio of a compound’s solubility in octanol, a lipid-like structure, and water;
drugs need to be somewhat soluble in both to be orally absorbed. PSA and refractivity both relate to
charge separation within the molecule. As shown in Appendix B, C5T5 generates mostly valid and
novel molecules, with values of logP that lie outside of the range of a “best in dataset” baseline for
targeted modiﬁcations."
RESULTS,0.20081967213114754,"4.1
C5T5 SUCCESSFULLY MODIFIES PROPERTIES"
RESULTS,0.20491803278688525,"First, we demonstrate that the localized changes proposed by C5T5 do in fact control the property
value as desired. C5T5 allows domain experts to choose where to make changes to a molecule based
on their intuition and particular application: the user masks out tokens in the IUPAC name that can
be modiﬁed, and the model ﬁlls in the masked spans in a way that changes the property value as
directed. There is no canonical way to choose particular tokens to mask for evaluation purposes, so
we simply choose a number of starting molecules randomly from PubChem (Kim et al., 2016), and
then we iteratively mask all length-one to length-ﬁve spans (to match the training distribution) and
run inference. We also experiment with masking multiple spans per molecule during inference, as
is done during training. This is useful in practice when there are multiple areas of the molecule that
can be changed in tandem to achieve the desired property value, but in our evaluation we observe
qualitatively similar results when masking only a single span, so for computational efﬁciency we
limit ourselves to single spans. We expect multi-span masks to be much more important when
optimizing for more complex properties."
RESULTS,0.20901639344262296,"Figure 4 shows that C5T5 successfully generates molecules with higher property values when passed
<high>, and with lower property values when passed <low>. The model is much more successful
at raising property values than lowering them, especially for refractivity and polar surface area. For
both of these properties, increasing the property value is straightforward: just add polar groups to
replace whatever tokens were masked. In contrast, lowering these properties is only possible when
the mask coincides with a polar group, in which case the model must ﬁnd a non-polar substitute
while still maintaining the molecule’s validity. Even if unsuccessful at lowering these two property
values on average, C5T5 can still be used in this case to suggest a number of candidate edits, and
the one with the lowest property value can be selected using a property prediction model. This
is an improvement over high-throughput screening and untargeted machine-learning methods for"
RESULTS,0.21311475409836064,Under review as a conference paper at ICLR 2022
RESULTS,0.21721311475409835,"5
0
5
10
Orig. logP 5 0 5 10 15 20"
RESULTS,0.22131147540983606,"Generated logP _
_
_ _ _ _ _ __ _ _ ___ _"
RESULTS,0.22540983606557377,"__ __ __
_
_
_
__
__
_
_"
RESULTS,0.22950819672131148,"_
_
__"
RESULTS,0.2336065573770492,"__
_
_ __
_ _ __ _ _
_ _
_ _ _
_ _
_
_ _ _ _ _ __ _ _ ___ _"
RESULTS,0.23770491803278687,"__ __ __
_
_
_
__
__
_
_"
RESULTS,0.24180327868852458,"_
_
__"
RESULTS,0.2459016393442623,"__
_
_ __
_ _ __ _ _
_ _
_ _ _
_"
RESULTS,0.25,"Target: <high>
Target: <low>"
RESULTS,0.2540983606557377,"5
0
5
10
Orig. logD 10 5 0 5 10 15 20"
RESULTS,0.2581967213114754,Generated logD __ _ _ _
RESULTS,0.26229508196721313,"_
_
__
_ _ __
_ _ __
__"
RESULTS,0.26639344262295084,"__
_
_
_
__
__
_
_ _
_ __"
RESULTS,0.27049180327868855,"__
_
_
__ _ _ __ _ _ _ _
_ _ _
_ __ _ _ _"
RESULTS,0.27459016393442626,"_
_
__
_ _ __
_ _ __
__"
RESULTS,0.2786885245901639,"__
_
_
_
__
__
_
_ _
_ __"
RESULTS,0.2827868852459016,"__
_
_
__ _ _ __ _ _ _ _
_ _ _
_"
RESULTS,0.28688524590163933,"Target: <high>
Target: <low>"
RESULTS,0.29098360655737704,"50
100
150
200
250
Orig. Refractivity 50 100 150 200 250 300 350 400"
RESULTS,0.29508196721311475,"Generated Refractivity ___ _
_
_ _
__ _ _ _ __
_"
RESULTS,0.29918032786885246,"_
____
__
_
_
_
__
__"
RESULTS,0.30327868852459017,"_
_
__
__ __
__ __
_ _ __
_ _
_ _
_ _
_
_ ___ _
_
_ _
__ _ _ _ __
_"
RESULTS,0.3073770491803279,"_
____
__
_
_
_
__
__"
RESULTS,0.3114754098360656,"_
_
__
__ __
__ __
_ _ __
_ _
_ _
_ _
_
_"
RESULTS,0.3155737704918033,"Target: <high>
Target: <low>"
RESULTS,0.319672131147541,"25
50
75
100
125
150
Orig. TPSA 0 50 100 150 200 250"
RESULTS,0.3237704918032787,"Generated TPSA __ _ _ _ _ _ _ _
_ _ _ _ _"
RESULTS,0.32786885245901637,"__
_
_ __ _ _ _ ___ _
_ _ _"
RESULTS,0.3319672131147541,"___
_
_
_
_ _
_ _
_ _ _ __ _ _ __ _ _ _ _ _ _ _
_ _ _ _ _"
RESULTS,0.3360655737704918,"__
_
_ __ _ _ _ ___ _
_ _ _"
RESULTS,0.3401639344262295,"___
_
_
_
_ _
_ _
_ _ _ __ _ _"
RESULTS,0.3442622950819672,"Target: <high>
Target: <low>"
RESULTS,0.3483606557377049,"Figure 4: Calculated property values of optimized molecules vs. original. Values computed for
30 randomly chosen starting molecules. Top left: octanol-water partition coefﬁcient. Top right:
octanol-water distribution coefﬁcient at pH of 7. Bottom left: molar refractivity. Bottom right: polar
surface area. Blue violins show the distribution of generated molecule properties when the model
was asked to complete molecules to achieve a <high> property value, and red for <low>. Cutoffs
between <high> and <med> and between <med> and <low> are shown as dashed blue and red
lines, respectively. The black dashed line is y=x. Black vertical lines show best in dataset baseline
for each trial molecule."
RESULTS,0.3524590163934426,"molecular optimization, since it isn’t restricted to a predeﬁned library of candidate molecules, and it
still allows the user to choose particular parts of the molecule to modify."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.35655737704918034,"4.2
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.36065573770491804,"One main advantage of C5T5 is that suggested edits are in the intuitive language of IUPAC names,
rather than arbitrary combinations of atoms. Table 1 shows the tokens that the model most pref-
erentially adds to a molecule when asked to produce low vs. high logP. Unsurprisingly, the most
common tokens added when increasing logP are generally long carbon chains (pentadecyl, unde-
cyl, heptadecyl) and other hydrocarbons (trityl, perylen). Conversely, when the model is asked to
produce low-logP modiﬁcations, hydrophilic groups are added. LogP is a simple metric, and by
proposing molecular edits for logP that are obvious and easily understandable to domain experts, we
expect users to gain conﬁdence that C5T5 will suggest reasonable edits for more complex properties."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.36475409836065575,"To further investigate the types of optimizations C5T5 suggests, Figure 5 visualizes two of the start-
ing molecules from Figure 4 with logP values of −2.4 and 5.8. For each molecule, we mask spans
as usual and generate after prepending with <low> (molecules on the left) and <high> (molecules
on the right). The IUPAC name of the top starting molecule is “3,3-bis(aminomethyl)pentane-1,5-
diol,” where “bis” signiﬁes that the CNH2 group should be duplicated, and “diol” means dupli-
cate OH groups at the ends. By virtue of the IUPAC name encoding symmetry, C5T5 is easily
able to generate similarly symmetric molecules. For example, the molecule in the top left is “3,3-
bis(aminomethyl)pentane-1,5-disulﬁnic acid,” where the “ol” has been replaced with “sulﬁnic acid.”"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.36885245901639346,Under review as a conference paper at ICLR 2022
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.3729508196721312,"Table 1: Tokens most preferentially added when C5T5 is asked to make modiﬁcations resulting
high vs. low logP values. Multipliers compare the actual rate of adding tokens compared to the
expected number if the model drew randomly from the data distribution independent of property
value. Blue tokens are hydrocarbons (i.e. lipophilic groups). Red tokens contain hydrogen bonding
donors or acceptors (i.e. hydrophilic groups)"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.3770491803278688,"Target: <high>
Target: <low>"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.38114754098360654,"trityl
77.0x
phospho
48.1x
pentadecyl
20.2x
phosphonato
44.7x
Z
17.7x
sulﬁnam
41.2x
perylen
11.8x
hydrazon
34.3x
undecyl
8.1x
sulﬁnato
26.3x
heptadecyl
7.9x
Z
17.9x
ylium
7.6x
oxonium
10.3x
isoindolo
5.9x
amoyl
9.3x
bH
5.9x
carbamic acid
8.6x
iod
5.8x
sulﬁn
6.9x"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.38524590163934425,"Decrease logP
Increase logP"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.38934426229508196,Original logP: -2.4
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.39344262295081966,logP: -5
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.3975409836065574,logP: -3.5
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4016393442622951,logP: -3
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4057377049180328,logP: 10.9
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4098360655737705,logP: 5.2
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4139344262295082,logP: 14.2
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4180327868852459,logP: 1.1
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.42213114754098363,Original logP: 5.8
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4262295081967213,logP: 1.4
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.430327868852459,logP: 1.4
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4344262295081967,logP: 8.1
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4385245901639344,logP: 8.1 
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4426229508196721,logP: 10.8
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.44672131147540983,"Figure 5: Visualizations of base and logP-optimized molecules. Two molecules from the logP
plot in Figure 4, with three C5T5-optimized molecules for each of <low> and <high> logP."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.45081967213114754,"Although symmetry is often highly desired, C5T5 is not limited to generating symmetric molecules.
For example, the middle molecule on the right is “3,3-bis(aminomethyl)-1-heptadecylpentane-1,5-
diol”—C5T5 added an additional carbon chain non-symmetrically at the end of the pentane.3"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.45491803278688525,"Sometimes C5T5 generates valid but unstable compounds. For example, the neighboring NH2
groups in the bottom left molecule in the top half of Figure 5 are unstable, and would turn into
aldehydes in an aqueous solution. All machine learning methods are susceptible to this sort of mis-
take, underscoring the importance of the type of human-in-the-loop optimization that C5T5 enables."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.45901639344262296,"3Although this is not a preferred IUPAC name, it is still unambiguous, and therefore valid and parseable."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.46311475409836067,Under review as a conference paper at ICLR 2022
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4672131147540984,"0.0
0.5
1.0
Tanimoto Similarity 0 200 400 600"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4713114754098361,# Molecules
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.47540983606557374,"C5T5-IUPAC
C5T5-SMILES
HierG2G"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.47950819672131145,"0
5
10
Synthetic Accessibility 0 200 400 600"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.48360655737704916,# Molecules
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.48770491803278687,"C5T5-IUPAC
C5T5-SMILES
HierG2G"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4918032786885246,"5
0
5
10
LogP Increase 0 200 400 600 800"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.4959016393442623,# Molecules
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5,"C5T5-IUPAC
C5T5-SMILES
HierG2G"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5040983606557377,"Figure 6: Histograms of Tanimoto similarity to base molecules, synthetic accessibility of generated
molecules (both as computed in (Jin et al., 2020)), and logP increase for 30 random base molecules
from the Zinc test set."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5081967213114754,"4.3
C5T5 MAKES SMALL AND SYNTHETICALLY ACCESSIBLE EDITS"
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5122950819672131,"C5T5’s proposed molecular edits are not only intuitive, but also small and synthetically accessible.
Unlike methods that train on edited pairs of molecules or that optimize molecules to be similar and
synthetically accessible in a latent space, C5T5 achieves this purely from self-supervision."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5163934426229508,"For 30 base molecules randomly chosen from the Zinc dataset (Sterling & Irwin, 2015), we compute
the Tanimoto similarity between generated and base molecules and the synthetic accessibility of
generated molecules. Results are shown in Figure 6, along with a comparison to Hierarchical Graph-
to-graph VAE (HierG2G), a state-of-the-art molecular graph translation model that trains on pairs of
Zinc molecules (Jin et al., 2020). We also compare to an ablation of C5T5 that uses SMILES instead
of IUPAC names. C5T5 generates molecules that are more synthetically accessible and similar to
base molecules and that have a wider range of logP increases than HierG2G. An additional advantage
of C5T5 not shown in Figure 6 is that C5T5 allows the user to choose what part of the molecule to
edit, whereas HierG2G makes edits wherever it sees ﬁt."
MODIFIED TOKENS ARE CHEMICALLY INTUITIVE,0.5204918032786885,"Unlike C5T5, methods like HierG2G that train on pairs of similar molecules are fundamentally
limited by a paucity of experimental molecule pairs that have high similarity and high property
value improvement. See Appendix B.2 for a quantitative analysis."
DISCUSSION AND CONCLUSION,0.5245901639344263,"5
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.5286885245901639,"We propose C5T5, a simple and effective zero-shot method for targeted control of molecular proper-
ties with transformers. Unlike prior approaches that make user-targeted modiﬁcations, our method
requires no database of paired edits; instead, it simply trains in a self-supervised fashion on a large
dataset of molecules and coarse estimates of their molecular property values. Core to our method
is the use of IUPAC names as a data representation, which captures molecular structure at an ap-
propriate level of abstraction and enables an intuitive editing interface for domain experts. C5T5
successfully rediscovers chemically-intuitive strategies for altering four drug-related properties in
molecules, a notable feat given the absence of any human demonstration of these editing strategies."
DISCUSSION AND CONCLUSION,0.5327868852459017,"Our work also has several limitations. The select-and-replace interface provided by the inﬁlling
objective may not always match the needs or preferred design process of domain experts. The
interface also only suggests how to ﬁll in missing parts of a molecule, relying on domain expertise or
enumeration to decide which parts of the molecule should be changed to begin with. In addition, we
explore only a coarse-grained bucketing of property values, leaving a more ﬁne-grained treatment
for future work. IUPAC names might also be too limiting in cases where a user wants to edit a
subgraph of a molecule that does not correspond neatly to a small number of IUPAC tokens. Finally,
training C5T5 is computationally expensive and sample inefﬁcient."
DISCUSSION AND CONCLUSION,0.5368852459016393,"Future work will investigate using C5T5 with more molecular properties, such as the power conver-
sion efﬁciency of solar cells. We also leave to future work extending C5T5 to jointly model multiple
properties, and adding a more ﬂexible editing interface."
DISCUSSION AND CONCLUSION,0.5409836065573771,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.5450819672131147,"6
REPRODUCIBILITY"
REPRODUCIBILITY,0.5491803278688525,"In Appendix C, we describe how to download and prepare the training dataset, our tokenization
strategy, masking strategy, optimizer, learning rate schedule, number of iterations, and generation
strategy. Our code is also available online at REDACTED."
REPRODUCIBILITY,0.5532786885245902,"One barrier to reproducibility is that we use ChemAxon’s proprietary software to compute molecular
property values. However, ChemAxon offers a free academic license, and we suspect that all results
would be similar if property values were computed with RDKit instead."
REFERENCES,0.5573770491803278,REFERENCES
REFERENCES,0.5614754098360656,"Josep Ar´us-Pous, Atanas Patronov, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis Reymond,
Hongming Chen, and Ola Engkvist. Smiles-based deep generative scaffold decorator for de-novo
drug design. Journal of cheminformatics, 12:1–18, 2020."
REFERENCES,0.5655737704918032,"Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Liggpt: Molecular generation
using a transformer-decoder model. 2021."
REFERENCES,0.569672131147541,"Sanjivanjit K Bhal, Karim Kassam, Ian G Peirson, and Greg M Pearl. The rule of ﬁve revisited:
applying log d in place of log p in drug-likeness ﬁlters. Molecular pharmaceutics, 4(4):556–560,
2007."
REFERENCES,0.5737704918032787,"G Richard Bickerton, Gaia V Paolini, J´er´emy Besnard, Sorel Muresan, and Andrew L Hopkins.
Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012."
REFERENCES,0.5778688524590164,"Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular
graphs. ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative
Models, 2018."
REFERENCES,0.5819672131147541,"Orion Dollar, Nisarg Joshi, David Beck, and Jim Pfaendtner. Attention-based generative models for
de novo molecular design. Chemical Science, 2021."
REFERENCES,0.5860655737704918,"Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to ﬁll in the blanks. In ACL,
2020."
REFERENCES,0.5901639344262295,"David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli, Tim-
othy Hirzel, Al´an Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular ﬁngerprints. In Proceedings of the 28th International Conference on Neural
Information Processing Systems - Volume 2, NIPS’15, pp. 2224–2232, Cambridge, MA, USA,
2015. MIT Press."
REFERENCES,0.5942622950819673,"Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones,
Tom Gibbs, Tamas Feher, Christoph Angerer, Debsindhu Bhowmik, and Burkhard Rost. Prot-
trans: Towards cracking the language of life’s code through self-supervised deep learning and
high performance computing. bioRxiv, 2020. doi: 10.1101/2020.07.12.199554. URL https:
//www.biorxiv.org/content/early/2020/07/12/2020.07.12.199554."
REFERENCES,0.5983606557377049,"Henri A Favre and Warren H Powell. Nomenclature of organic chemistry: IUPAC recommendations
and preferred names 2013. Royal Society of Chemistry, 2013."
REFERENCES,0.6024590163934426,"Jessica Ficler and Y. Goldberg. Controlling linguistic style aspects in neural language generation.
ArXiv, abs/1707.02633, 2017."
REFERENCES,0.6065573770491803,"Arup K Ghose, Vellarkad N Viswanadhan, and John J Wendoloski. A knowledge-based approach in
designing combinatorial or medicinal chemistry libraries for drug discovery. 1. a qualitative and
quantitative characterization of known drug databases. Journal of combinatorial chemistry, 1(1):
55–68, 1999."
REFERENCES,0.610655737704918,"Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, and Stefan Scherer.
Affect-lm: A neural language model for customizable affective text generation. arXiv preprint
arXiv:1704.06851, 2017."
REFERENCES,0.6147540983606558,Under review as a conference paper at ICLR 2022
REFERENCES,0.6188524590163934,"Daria Grechishnikova. Transformer neural network for protein-speciﬁc de novo drug generation as
a machine translation problem. Scientiﬁc reports, 11(1):1–13, 2021."
REFERENCES,0.6229508196721312,"Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider. Bidirectional molecule
generation with recurrent neural networks. Journal of chemical information and modeling, 60(3):
1175–1183, 2020."
REFERENCES,0.6270491803278688,"Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias,
and Al´an Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ) for se-
quence generation models. arXiv preprint arXiv:1705.10843, 2017."
REFERENCES,0.6311475409836066,"Rafael G´omez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,
Benjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,
Ryan P. Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven con-
tinuous representation of molecules.
ACS Central Science, 4(2):268–276, 2018.
doi: 10.
1021/acscentsci.7b00572. URL https://doi.org/10.1021/acscentsci.7b00572.
PMID: 29532027."
REFERENCES,0.6352459016393442,"Jennifer Handsel, Brian Matthews, Nicola Knight, and Simon Coles. Translating the molecules:
adapting neural machine translation to predict iupac names from a chemical identiﬁer. 2021."
REFERENCES,0.639344262295082,"Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, Christian Tyr-
chan, Werngard Czechtizky, et al. Transformer neural network for structure constrained molecular
optimization. 2021a."
REFERENCES,0.6434426229508197,"Jiazhen He, Huifang You, Emil Sandstr¨om, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan,
Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist’s intuition
using deep neural networks. Journal of cheminformatics, 13(1):1–17, 2021b."
REFERENCES,0.6475409836065574,"Stephen A Hitchcock and Lewis D Pennington. Structure- brain exposure relationships. Journal of
medicinal chemistry, 49(26):7559–7583, 2006."
REFERENCES,0.6516393442622951,"James P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. Principles of early drug
discovery. British journal of pharmacology, 162(6):1239–1249, 2011."
REFERENCES,0.6557377049180327,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In International Conference on Machine Learning, pp. 2323–2332.
PMLR, 2018."
REFERENCES,0.6598360655737705,"Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-
graph translation for molecule optimization. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/forum?id=B1xJAsA5F7."
REFERENCES,0.6639344262295082,"Wengong Jin, Dr.Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs
using structural motifs. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th Inter-
national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Re-
search, pp. 4839–4848. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/
v119/jin20a.html."
REFERENCES,0.6680327868852459,"Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In
International Conference on Machine Learning, pp. 3183–3191. PMLR, 2019."
REFERENCES,0.6721311475409836,"Pavel Karpov, Guillaume Godin, and Igor V Tetko. A transformer model for retrosynthesis. In
International Conference on Artiﬁcial Neural Networks, pp. 817–830. Springer, 2019."
REFERENCES,0.6762295081967213,"N. Keskar, Bryan McCann, L. Varshney, Caiming Xiong, and R. Socher. Ctrl: A conditional trans-
former language model for controllable generation. ArXiv, abs/1909.05858, 2019."
REFERENCES,0.680327868852459,"Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han,
Jane He, Siqian He, Benjamin A Shoemaker, et al. Pubchem substance and compound databases.
Nucleic acids research, 44(D1):D1202–D1213, 2016."
REFERENCES,0.6844262295081968,Under review as a conference paper at ICLR 2022
REFERENCES,0.6885245901639344,"Panagiotis-Christos Kotsias, Josep Ar´us-Pous, Hongming Chen, Ola Engkvist, Christian Tyrchan,
and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor con-
ditional recurrent neural networks. Nature Machine Intelligence, 2(5):254–265, 2020."
REFERENCES,0.6926229508196722,"Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, and Sergey Sosnin. Struct2iupac–transformer-based
artiﬁcial neural network for the conversion between chemical notations. 2021."
REFERENCES,0.6967213114754098,"Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. In EMNLP (Demonstration), 2018."
REFERENCES,0.7008196721311475,"Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the
carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019."
REFERENCES,0.7049180327868853,"Maxime Langevin, Herv´e Minoux, Maximilien Levesque, and Marc Bianciotto.
Scaffold-
constrained molecular generation. Journal of Chemical Information and Modeling, 2020."
REFERENCES,0.7090163934426229,"Yibo Li, Jianxing Hu, Yanxing Wang, Jielong Zhou, Liangren Zhang, and Zhenming Liu. Deep-
scaffold: A comprehensive tool for scaffold-based de novo drug discovery using deep learning.
Journal of chemical information and modeling, 60(1):77–91, 2019."
REFERENCES,0.7131147540983607,"Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based
on conditional variational autoencoder for de novo molecular design. Journal of cheminformatics,
10(1):1–9, 2018."
REFERENCES,0.7172131147540983,"Jaechang Lim, Sang-Yeon Hwang, Seokhyun Moon, Seungsu Kim, and Woo Youn Kim. Scaffold-
based molecular design with a graph generative model. Chemical Science, 11(4):1153–1164,
2020."
REFERENCES,0.7213114754098361,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph varia-
tional autoencoders for molecule design. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, NIPS’18, pp. 7806–7815, Red Hook, NY, USA, 2018.
Curran Associates Inc."
REFERENCES,0.7254098360655737,"Daniel M Lowe, Peter T Corbett, Peter Murray-Rust, and Robert C Glen. Chemical name to struc-
ture: Opsin, an open source solution, 2011."
REFERENCES,0.7295081967213115,"Łukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Michał
Warchoł. Mol-cyclegan: a generative model for molecular optimization. Journal of Cheminfor-
matics, 12(1):1–18, 2020."
REFERENCES,0.7336065573770492,"Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider,
Nikolaus Stieﬂ, and Marc Brockschmidt. Learning to extend molecular scaffolds with structural
motifs. arXiv preprint arXiv:2103.03864, 2021."
REFERENCES,0.7377049180327869,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013."
REFERENCES,0.7418032786885246,"Tong Niu and Mohit Bansal. Polite dialogue generation without parallel data. Transactions of the
Association for Computational Linguistics, 6:373–389, 2018."
REFERENCES,0.7459016393442623,"Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo
design through deep reinforcement learning. Journal of cheminformatics, 9(1):1–14, 2017."
REFERENCES,0.75,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. arXiv preprint arXiv:1912.01703, 2019."
REFERENCES,0.7540983606557377,"Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011."
REFERENCES,0.7581967213114754,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.7622950819672131,Under review as a conference paper at ICLR 2022
REFERENCES,0.7663934426229508,"Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using
neural machine translation. Journal of Cheminformatics, 13(1):1–14, 2021."
REFERENCES,0.7704918032786885,"Jean-Louis Reymond, Lars Ruddigkeit, Lorenz Blum, and Ruud van Deursen. The enumeration
of chemical space. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2(5):
717–733, 2012."
REFERENCES,0.7745901639344263,"Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang.
Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.7786885245901639,"Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Opti-
mizing distributions over molecular space. an objective-reinforced generative adversarial network
for inverse-design chemistry (organic)."
REFERENCES,0.7827868852459017,"Philippe Schwaller, Teodoro Laino, Th´eophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas
Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical
reaction prediction. ACS central science, 5(9):1572–1583, 2019."
REFERENCES,0.7868852459016393,"Chence Shi*, Minkai Xu*, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf:
a ﬂow-based autoregressive model for molecular graph generation.
In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
S1esMkHYPr."
REFERENCES,0.7909836065573771,"Bonggun Shin, Sungsoo Park, JinYeong Bak, and Joyce C Ho. Controlled molecule generator for
optimizing multiple chemical properties. In Proceedings of the Conference on Health, Inference,
and Learning, pp. 146–153, 2021."
REFERENCES,0.7950819672131147,"Teague Sterling and John J Irwin. Zinc 15–ligand discovery for everyone. Journal of chemical
information and modeling, 55(11):2324–2337, 2015."
REFERENCES,0.7991803278688525,"Daniel F Veber, Stephen R Johnson, Hung-Yuan Cheng, Brian R Smith, Keith W Ward, and Ken-
neth D Kopple. Molecular properties that inﬂuence the oral bioavailability of drug candidates.
Journal of medicinal chemistry, 45(12):2615–2623, 2002."
REFERENCES,0.8032786885245902,"Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large
scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th
ACM international conference on bioinformatics, computational biology and health informatics,
pp. 429–436, 2019."
REFERENCES,0.8073770491803278,"David Weininger. Smiles, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36,
1988."
REFERENCES,0.8114754098360656,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019."
REFERENCES,0.8155737704918032,"Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, and Tommi Jaakkola. Improving molec-
ular design by stochastic iterative target augmentation. In International Conference on Machine
Learning, pp. 10716–10726. PMLR, 2020."
REFERENCES,0.819672131147541,"Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy
network for goal-directed molecular graph generation. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, NIPS’18, pp. 6412–6422, Red Hook, NY,
USA, 2018. Curran Associates Inc."
REFERENCES,0.8237704918032787,Under review as a conference paper at ICLR 2022
REFERENCES,0.8278688524590164,"Method
Base Repr.
Model
T?a
UD?b
CO?c"
REFERENCES,0.8319672131147541,"C5T5
IUPAC
T5
✓
✓
✓
He et al. (2021b)
SMILES
Seq2Seq/Transformer
X
X
✓
Langevin et al. (2020)
SMILES
Any
✓
X
X
Ar´us-Pous et al. (2020)
SMILES
LSTM
✓
X
X
Jin et al. (2019)
Graph/Motifs
JT-VAE+GAN
X
X
X
Li et al. (2019)
Graph/Atoms
GNN+VAE
X d
X
X
Lim et al. (2020)
Graph/Atoms
VAE+GNN
X
X
✓
Maziarz et al. (2021)
Graph/Motifs
VAE+GNN
X
✓
✓
Bagal et al. (2021)
SMILES
GPT
X
✓
✓
Shin et al. (2021)
SMILES
Transformer+LSTM
X
X
✓
He et al. (2021a)
SMILES
Transformer
✓
X
✓
Kotsias et al. (2020)
SMILES
cRNN
X
✓
✓
G´omez-Bombarelli et al. (2018)
SMILES
VAE+ConvNet+GRU
X
✓
X
Lim et al. (2018)
SMILES
cVAE
X
✓
✓
Dollar et al. (2021)
SMILES
VAE+Transformer
X
✓
X
Liu et al. (2018)
Graph/Atoms
VAE+GNN
X
✓
X
You et al. (2018)
Graph/Atoms
GCN+GAN+RL
X
✓
X
Jin et al. (2018)
Graph/Motifs
JT-VAE
X
✓
X
Maziarka et al. (2020)
Graph/Motifs
CycleGAN+JT-VAE
X
✓
✓
Shi* et al. (2020)
Graph/Atoms
Flow+RL
X
✓
X
Olivecrona et al. (2017)
SMILES
RNN+RL
X
✓
X"
REFERENCES,0.8360655737704918,Table 2: Comparison of C5T5 to prior methods
REFERENCES,0.8401639344262295,"aWhether or not the model makes targeted modiﬁcations – i.e. whether the user can specify which part of
the molecule the model should modify.
bWhether or not the model can train without using paired molecular data.
cWhether or not the model can switch objectives without re-training or re-optimizing.
dThe authors propose ﬁltering results based on where the user wants to add a side chain, but the method
itself does not target speciﬁc attachment points."
REFERENCES,0.8442622950819673,"A
QUALITATIVE COMPARISON TO PRIOR METHODS"
REFERENCES,0.8483606557377049,Table 2 shows how C5T5 and prior methods for molecular optimization differ along several axes.
REFERENCES,0.8524590163934426,"B
ADDITIONAL EXPERIMENTS"
REFERENCES,0.8565573770491803,"B.1
NOVELTY, VALIDITY, & BEST-IN-DATASET"
REFERENCES,0.860655737704918,"Section 4.1 shows that C5T5 successfully modiﬁes property values. Here, for molecules generated
to optimize logP, we show the novelty and validity of the generated molecules, along with a compar-
ison to a baseline of the best eligible compound in PubChem. To match how the model was trained
and how new molecules were generated, eligible compounds are those that could be generated by
masking any consecutive span of at most 5 IUPAC name tokens and replacing the masked tokens
with any number of replacement tokens.4 Results are shown in Tables 3 and 4. Percent validity is
the fraction of generated molecules that T5 generated with valid sentinel tokens that were consid-
ered chemically valid by the ChemAxon logP calculator. Percent novelty is the fraction of distinct
generated molecules that do not appear in PubChem (excluding when C5T5 re-generated the source
molecule). As shown, despite the comparative difﬁculty of learning IUPAC name syntax compared
to SMILES syntax, C5T5 consistently ﬁnds novel and valid molecules that signiﬁcantly outperform
the best-in-dataset baseline."
REFERENCES,0.8647540983606558,"4For computational efﬁciency, we ﬁlter out molecules that differ in length by more than 15 tokens, or that
have more than 15 non-overlapping tokens in their bag of tokens, before checking whether they could indeed
be generated by masking some length-5 sequence of tokens."
REFERENCES,0.8688524590163934,Under review as a conference paper at ICLR 2022
REFERENCES,0.8729508196721312,"Table 3: For each source molecule, we show the percent of generated molecules that are novel (not
in PubChem, including invalid names) and valid (can be parsed by ChemAxon), the number of gen-
erated molecules, the min/max logP of any generated molecule, the number of eligible compounds
in PubChem and the min/max logP of all molecules in PubChem that could be generated by masking
up to 5 consecutive tokens. IUPAC names of source molecules are listed in Table 4."
REFERENCES,0.8770491803278688,"Src.
# gen.
% novel
% valid
max gen.
min gen.
# elig.
max PC
min PC"
REFERENCES,0.8811475409836066,"1
82
95.1%
81.7%
14.22
-5.04
26
10.81
-2.7
2
133
93.2%
91.7%
9.41
-3.82
28
1.92
-1.46
3
217
98.6%
91.7%
9.15
-2.17
4
3.01
1.66
4
140
100.0%
94.3%
10.21
1.15
3
8.18
3.78
5
128
92.2%
88.3%
6.91
0.86
19
4.24
2.35
6
160
100.0%
75.6%
8.16
0.47
4
2.56
1.6
7
159
99.4%
86.2%
9.05
-2.22
8
3.91
-1.34
8
137
99.3%
84.7%
14.74
-2.97
1
1.08
1.08
9
122
95.1%
81.1%
8.57
-4.89
34
4.75
-0.07
10
112
92.9%
88.4%
8.44
-2.96
112
5.03
-2.3
11
127
97.6%
81.1%
9.52
0.11
9
5.13
2.28
12
114
90.4%
85.1%
7.71
-3.32
515
6.13
-2.95
13
115
94.8%
90.4%
8.12
-1.91
36
4.05
0.3
14
149
97.3%
85.2%
14.09
-4.3
7
2.1
0.44
15
135
100.0%
83.0%
7.52
-0.7
2
5.08
4.12
16
214
100.0%
97.7%
6.86
-1.83
3
1.91
-0.05
17
156
99.4%
92.3%
10.58
-2.88
2
0.76
0.76
18
231
98.7%
83.1%
9.79
-1.35
6
5.6
3.46
19
148
93.2%
82.4%
9.98
-0.19
63
7.01
0.76
20
143
96.5%
72.7%
14.12
0.33
14
4.61
1.31
21
232
99.1%
85.3%
9.9
-1.6
6
3.15
1.14
22
150
96.0%
92.7%
7.96
-0.74
22
4.95
2.71
23
127
94.5%
87.4%
7.54
-3.1
18
4.67
0.37
24
160
99.4%
82.5%
7.54
-1.53
2
1.28
1.28
25
274
100.0%
95.6%
10.82
1.12
2
6.12
5.82"
REFERENCES,0.8852459016393442,"B.2
EDITED MOLECULE PAIRS IN ZINC"
REFERENCES,0.889344262295082,"Methods like HierG2G (Jin et al., 2020) train a model to translate between “unoptimized” and “op-
timized“ molecules, where the training set consists of pairs of similar molecules in an underlying
dataset (e.g. Zinc) that have different property values."
REFERENCES,0.8934426229508197,"Training a model in this way faces the fundamental limitation that there are very few pairs of
molecules that are both similar (e.g. in Tanimoto similarity space) and have very different prop-
erty values. To quantify this effect, we compute similarity and logP increase between all pairs of
molecules in the training set of Zinc used by Jin et al. (2020). Results are shown in Figure 7."
REFERENCES,0.8975409836065574,"C
EXPERIMENTAL DETAILS"
REFERENCES,0.9016393442622951,"Dataset Preparation
We download PubChem from ftp.ncbi.nlm.nih.gov/pubchem/
Compound/CURRENT-Full/XML/ and extracted each molecule’s Preferred IUPAC Name and
computed octanol-water partition coefﬁcient (logP). There are 109M total compounds in the version
of PubChem we downloaded in January 2021. For experiments using logP, we used the XLogP3 val-
ues from PubChem, which were computed using OpenEye’s software. For logD (pH=7), refractivity,
and polar surface area, we computed values using ChemAxon’s calculator with default parameters.
Separately for each target property, we excluded chemicals that had no logP value in PubChem or
that were not parseable by ChemAxon’s calculator. Of the remaining molecules, we randomly split
into a training set with 90M compounds and a validation set with ∼10M-19M compounds."
REFERENCES,0.9057377049180327,Under review as a conference paper at ICLR 2022
REFERENCES,0.9098360655737705,Table 4: IUPAC Name lookup table for Table 3
REFERENCES,0.9139344262295082,"ID
Source Molecule"
REFERENCES,0.9180327868852459,"1
3,3-bis(aminomethyl)pentane-1,5-diol
2
1-(3-hydroxypropyl)-N-(1-methoxybutan-2-yl)pyrazole-4-sulfonamide
3
4-chloro-N-[2-[[2-(4-ﬂuorophenyl)acetyl]amino]ethyl]-1,3-thiazole-5-carboxamide
4
1-[(1S)-1-(3-ﬂuorophenyl)propyl]-3-iodoindole
5
4-(4-ﬂuorophenyl)-N-[(1R,2R)-2-methylcyclohexyl]piperazine-1-carbothioamide
6
N’-(3-ethyl-4-oxophthalazine-1-carbonyl)-4-methyl-2-phenyl-1,3-thiazole-5-carbohydrazide
7
(E)-2-methoxy-3-methylhex-4-en-1-ol
8
N-methyl-1-[2-(4-methylthiadiazol-5-yl)-1,3-thiazol-4-yl]methanamine
9
2-[(7-methyl-[1,2,4]triazolo[1,5-a]pyridin-2-yl)amino]ethylurea
10
4-[[2-(2-oxopyridin-1-yl)acetyl]amino]benzoic acid
11
[6-prop-2-enoxy-4-(triﬂuoromethyl)pyridin-2-yl]hydrazine
12
4-(2-methylphenyl)sulfonylpiperidin-3-amine
13
3-[ethyl(2-methylpropyl)amino]propane-1-thiol
14
6-methoxy-4-N-methyl-4-N-[(2-methylfuran-3-yl)methyl]pyrimidine-4,5-diamine
15
3-phenylmethoxy-5-(triﬂuoromethoxy)quinoline-2-carboxylic acid
16
3-[4-[acetamido-[3-methoxy-4-[(2-methylphenyl)carbamoylamino]phenyl]methyl]piperidin-1-yl]-
3-phenylpropanoic acid
17
(3R)-3-[[(2S)-2-[benzyl(methyl)amino]butanoyl]amino]pyrrolidine-1-carboxamide
18
6-cyclobutyl-2-N-[3-(1-ethylsulﬁnylethyl)phenyl]-5-(triﬂuoromethyl)pyrimidine-2,4-diamine
19
6-ﬂuoro-2-(4-phenylpyridin-2-yl)-1H-benzimidazole
20
4-chloro-3-(2-oxo-1,3-dihydroindol-5-yl)benzonitrile
21
1-(6-tert-butylpyridazin-3-yl)-N-methyl-N-[(2-methyl-1,3-oxazol-4-yl)methyl]azetidin-3-amine
22
2-[(4aR,8aS)-3,4,4a,5,6,7,8,8a-octahydro-1H-isoquinolin-2-yl]-N-(2,4-dimethoxyphenyl)acetamide
23
2-[2-(2,4-dichlorophenoxy)ethoxy]-4-methoxybenzoic acid
24
(2Z)-2-[(1,7-dimethylquinolin-1-ium-2-yl)methylidene]-1-ethyl-7-methylquinoline
25
N’-[(3S)-1-[[3-(2,4-dichlorophenyl)phenyl]methyl]-2-oxoazepan-3-yl]-
3-(2-methylpropyl)-2-prop-2-enylbutanediamide"
REFERENCES,0.9221311475409836,"0.0
0.2
0.4
0.6
0.8
1.0
Tanimoto Similarity Cutoff 101 103 105 107 109"
REFERENCES,0.9262295081967213,# Pairs
REFERENCES,0.930327868852459,"logP diff > 0
logP diff > 1
logP diff > 2
logP diff > 3
logP diff > 4"
REFERENCES,0.9344262295081968,"Figure 7: Number of molecular pairs in Zinc that pass a Tanimoto similarity threshold (x-axis) and a
logP-difference threshold (different curves). Even for moderate similarity and logP thresholds, there
quickly become vanishingly few usable molecular pairs. There are 24 × 109 total unique molecular
pairs."
REFERENCES,0.9385245901639344,"Tokenization
We use HuggingFace’s T5Tokenizer, which is based on the SentencePiece algorithm
Kudo & Richardson (2018). Because our goal is to have tokens that domain experts are familiar with,
we do not train SentencePiece on the IUPAC names, since doing so learns both combinations of and
substrings of moiety names. Instead, we manually specify the tokens to be all the keywords in the
Opsin IUPAC name parsing library Lowe et al. (2011). To these keywords, we add locants 1–100,"
REFERENCES,0.9426229508196722,Under review as a conference paper at ICLR 2022
REFERENCES,0.9467213114754098,"stereochemistry markers (R, S, E, Z,...), and a few miscellaneous tokens to e.g. handle spiro centers.
This leads to a total of 1274 tokens. After tokenization, we truncate all names to at most 128 tokens."
REFERENCES,0.9508196721311475,We choose which property value token to use based on the cutoffs speciﬁed in Table 5.
REFERENCES,0.9549180327868853,"Table 5: Numerical ranges across properties for each property value token. Cutoffs for logP,
PSA, and refractivity were chosen as common thresholds for druglikeness screening following
(Ghose et al., 1999; Veber et al., 2002; Hitchcock & Pennington, 2006). We use the same cutoff
for logD as logP."
REFERENCES,0.9590163934426229,"Property
<low>
<med>
<high>"
REFERENCES,0.9631147540983607,"Octanol-water partition coeff. (logP)
(−∞, −0.4)
(−0.4, 5.6)
(5.6, ∞)
Octanol-water distribution coeff. (logD)
(−∞, −0.4)
(−0.4, 5.6)
(5.6, ∞)
Polar surface area (PSA)
(0, 90)
(90, 140)
(140, ∞)
Refractivity
(0, 40)
(40, 130)
(130, ∞)"
REFERENCES,0.9672131147540983,"Training
We train a t5-large model (∼700M params) available from HuggingFace that was pre-
trained on English text. We keep the ﬁrst 1274 embeddings from the pretrained embedding table,
along with the pretrained embeddings for the 100 sentinel tokens. When training, we mask 15% of
the tokens in each input in spans of mean length 3 tokens, with a minimum span length of 1. We
use a linear warmup of the learning rate for 10,000 steps followed by a 1/T decay. All models were
trained using the AdamW optimizer. We train the logP model for 2.5M iterations using a max learn-
ing rate of 10−3. We train the refractivity/logD/Polar SA models using a maximum learning rate of
2 × 10−4/10−4/2 × 10−4 starting from the logP model after 1M/2.5M/2.5M iterations. (We trained
using the latest model available when we started a run.) All models were trained on 8 NVIDIA
A100s with a batch size of 16 per GPU."
REFERENCES,0.9713114754098361,"Generation
We generate novel molecules greedily from the output of T5’s decoder. We discard
any generations where the sentinel tokens do not line up, and we further discard any molecules that
cannot be parsed by ChemAxon’s calculators. We also discard instances where C5T5 regenerates the
base molecule. Doing so inﬂates our novelty score, but because we choose random masks (including,
e.g. masking out a comma between two locants), in many cases there are few or no other ways to
ﬁll in the masked section besides regenerating the original tokens."
REFERENCES,0.9754098360655737,"Comparison to HierG2G
We trained HierG2G on Zinc using the authors’ code and hyperparam-
eters from (Jin et al., 2020). We generated all pairs of Zinc training molecules with a Tanimoto
similarity of at least 0.4 and an increase in logP of at least 2.5, yielding ∼200k pairs. We trained
HierG2G for 32k iterations; further training did not improve results. To choose the similarity and
logp thresholds, we attempted to use the average similarity and logp increase achieved by C5T5
(similarity of 0.55, logP increase of 2.4). However, there are only 7000 pairs of molecules in Zinc
meeting these constraints, so we relaxed the cutoffs to achieve a reasonably sized training set. See
Appendix B.2 for further details."
REFERENCES,0.9795081967213115,"We were not able to fully reproduce the results in (Jin et al., 2020), either by running the authors’
code as-is or by modifying the hyperparameters to match the description in the paper. Speciﬁcally,
for penalized logP with a similarity cutoff of 0.4, we get an average improvement of ∼2.8, whereas
the results in the paper show an improvement of 3.98. We will update Figure 6 once we are able to
reproduce the paper’s results, but we do not believe the conclusions of our comparison will change
qualitatively even with updated results from HierG2G."
REFERENCES,0.9836065573770492,"Cloud Computing Cost
We train T5-Large models Raffel et al. (2019) on PubChem for each
investigated property value using local computing resources and AWS p4d.24xlarge instances in
the us-east-1 region. The logP model was trained for 2.5M iterations over 8 days; the logD and
refractivity models for 350k iterations over 1 day each (initialized from the logP model after 2.5M
iterations); and the Polar SA model for 1.6M iterations over 5 days (initialized from the logP model
after 1M iterations). The SMILES logP ablation model was trained for 1M iterations over 12 days."
REFERENCES,0.9877049180327869,Under review as a conference paper at ICLR 2022
REFERENCES,0.9918032786885246,"Total equivalent on-demand AWS cost for the models presented here is ∼$19,000, and total carbon
dioxide-equivalent emissions is ∼570 kg Lacoste et al. (2019)."
REFERENCES,0.9959016393442623,"Software
We use HuggingFace 4.2.2 (Apache 2.0 license) (Wolf et al., 2019), PyTorch 1.8.0
(BSD) (Paszke et al., 2019), ChemAxon 20.17.0 (Academic License), scikit-learn 0.24.2 (New BSD)
(Pedregosa et al., 2011) and python 3.9 (PSFL)."
