Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001597444089456869,"Learning efﬁciently a causal model of the environment is a key challenge of
model-based RL agents operating in POMDPs. We consider here a scenario
where the learning agent has the ability to collect online experiences through direct
interactions with the environment (interventional data), but also has access to a large
collection of ofﬂine experiences, obtained by observing another agent interacting
with the environment (observational data). A key ingredient, which makes this
situation non-trivial, is that we allow the observed agent to act based on privileged
information, hidden from the learning agent. We then ask the following questions:
can the online and ofﬂine experiences be safely combined for learning a causal
transition model ? And can we expect the ofﬂine experiences to improve the agent’s
performances ? To answer these, ﬁrst we bridge the ﬁelds of reinforcement learning
and causality, by importing ideas from the well-established causal framework
of do-calculus, and expressing model-based reinforcement learning as a causal
inference problem. Second, we propose a general yet simple methodology for
safely leveraging ofﬂine data during learning. In a nutshell, our method relies on
learning a latent-based causal transition model that explains both the interventional
and observational regimes, and then inferring the standard POMDP transition
model via deconfounding using the recovered latent variable. We prove our method
is correct and efﬁcient in the sense that it attains better generalization guarantees
due to the ofﬂine data (in the asymptotic case), and we assess its effectiveness
empirically on a series of synthetic toy problems."
INTRODUCTION,0.003194888178913738,"1
INTRODUCTION"
INTRODUCTION,0.004792332268370607,"As human beings, a key ingredient in our learning process is experimentation: we perform actions
in our environment and we measure their outcomes. Another ingredient, maybe less understood, is
observation: we observe the behaviour of others acting and evolving in the environment, be it people,
animals, or even plants. It is well-known that observation alone is not sufﬁcient to infer how our
environment works, or more precisely to predict the outcome of our actions, especially when the
behaviours we observe depend on hidden information1. And yet a whole ﬁeld of science, astronomy,
heavily relies on the observation of celestial bodies in the sky, on which experimentation is virtually
impossible. So which role exactly does observation play during learning ? And in particular, how do
we combine observation and experimentation ?"
INTRODUCTION,0.006389776357827476,"In the context of reinforcement learning (RL), a related question is the combination of ofﬂine data,
resulting from observations, with online data resulting from experimentation, in order to improve the
performance of a learning agent. In the Markov Decision Process (MDP) setting, where the agent
observes the entire state of the environment, the answer is straightforward and practical solutions exist,
leading to the fastly growing ﬁeld of ofﬂine reinforcement learning [17; 18] where large databases of
demonstrations can be efﬁciently leveraged. In the more general Partially-Observable MDP (POMDP)
setting however, the question turns out to be much more challenging. A typical example is in the
context of medicine, where ofﬂine data is collected from physicians who may rely on information
absent from their patient’s medical records, such as their wealthiness or their lifestyle. Suppose that
wealthy patients in general get prescribed speciﬁc treatments by their physicians, because they can"
INTRODUCTION,0.007987220447284345,"1Simply put, correlation does not imply causation. Or, citing Pearl [24], “behind every causal conclusion
there must lie some causal assumption that is not testable in observational studies”."
INTRODUCTION,0.009584664536741214,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011182108626198083,"afford it, while being less at risk to develop severe conditions regardless of their treatment, because
they can also afford a healthier lifestyle. This creates a spurious correlation called confounding, and
will cause a naive recommender system to wrongly infer that a treatment has positive health effects. A
second example is in the context of autonomous driving, where ofﬂine data is collected from human
drivers who have a wider ﬁeld of vision than the camera on which the robot driver relies. Suppose
human drivers push the brakes when they see a person waiting to cross the street, and only when the
person walks in front of the car it enters the camera’s ﬁeld of vision. Then, again, a naive robot might
wrongly infer from its observations that whenever brakes are pushed, a person appears in front of
the car. Suppose now that the robot’s objective is to avoid collisions with pedestrians, they it might
get regrettably reluctant to push the brakes. Of course, in both those situations, the learning agent
will eventually infer the right causal effects of its actions if it collects enough online data from its
own interactions. However, in both those situations also, performing many interventions for the sole
purpose of seeing what happens is not really realistic, while collecting ofﬂine data by observing the
behaviour of human agents is much more affordable."
INTRODUCTION,0.012779552715654952,"In this paper we study the question of combining ofﬂine and online data under the Partially-Observable
Markov Decision Process (POMDP) setting, by importing tools and ideas from the well-established
ﬁeld of causality [23] into the model-based RL framework. Our contribution is three-fold:"
WE FORMALIZE MODEL-BASED RL AS A CAUSAL INFERENCE PROBLEM USING THE FRAMEWORK OF DO-,0.01437699680511182,"1. We formalize model-based RL as a causal inference problem using the framework of do-
calculus [25], which allows us to reason formally about online and ofﬂine scenarios in an
intuitive manner (Section 3)."
WE FORMALIZE MODEL-BASED RL AS A CAUSAL INFERENCE PROBLEM USING THE FRAMEWORK OF DO-,0.01597444089456869,"2. We present a generic method for combining ofﬂine and online data in model-based RL
(Section 4), with a formal proof of correctness even when the ofﬂine policy relies on
privileged hidden information (confounding variable), and a proof of efﬁciency in the
asymptotic case (with respect to using online data only)."
WE FORMALIZE MODEL-BASED RL AS A CAUSAL INFERENCE PROBLEM USING THE FRAMEWORK OF DO-,0.01757188498402556,"3. We propose a practical implementation of our method, and illustrate its effectiveness on
three experiments with synthetic toy problems (Section 6)."
WE FORMALIZE MODEL-BASED RL AS A CAUSAL INFERENCE PROBLEM USING THE FRAMEWORK OF DO-,0.019169329073482427,"While our proposed method can be formulated outside of the framework of do-calculus, in this paper
we hope to demonstrate that it offers a principled and intuitive tool to reason about model-based RL.
By relating common concepts from RL and causality, we wish that our contribution will ultimately
help to bridge the gap between the two communities."
BACKGROUND,0.020766773162939296,"2
BACKGROUND"
NOTATION,0.022364217252396165,"2.1
NOTATION"
NOTATION,0.023961661341853034,"In this paper, upper-case letters in italics denote random variables (e.g. X, Y ), while their lower-case
counterpart denote their value (e.g. x, y) and their calligraphic counterpart their domain (e.g., x ∈X).
We consider only discrete random variables. To keep our notation uncluttered, with a slight abuse
of notations and use p(x) to denote sometimes the event probability p(X = x), and sometimes
the whole probability distribution of X, which should be clear from the context. In the context of
sequential models we also distinguish random variables with a temporal index t, which might be ﬁxed
(e.g., o0, o1 ), or undeﬁned (e.g., p(st+1|st, at) denotes at the same time the distributions p(s1|s0, a0)
and p(s2|s1, a1)). We also adopt a compact notation for sequences of contiguous variables (e.g.,
s0→T = (s0, . . . , sT ) ∈ST +1 ), and for summations over sets (P"
NOTATION,0.025559105431309903,"x∈X ⇐⇒PX
x ). We assume the
reader is familiar with the concepts of conditional independence (X ⊥⊥Y | Z) and probabilistic
graphical models based on directed acyclic graphs (DAGs), which can be found in most introductory
textbooks, e.g. Pearl [22]; Studeny [29]; Koller and Friedman [15]. In the following we will use
do-calculus to derive formal solutions to model-based RL in various POMDP settings. We refer the
reader to Pearl [25] for a thorough introduction, and give a description of rules R1, R2 and R3 used
in our derivations in the appendix (Section A.1)."
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.027156549520766772,"2.2
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.02875399361022364,"We consider Partially-Observable Markov Decision Processes (POMDPs) of the form M =
(S, O, A, pinit, pobs, ptrans, r), with hidden states s ∈S, observations o ∈O, actions a ∈A, ini-"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.03035143769968051,Under review as a conference paper at ICLR 2022
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.03194888178913738,"tial state distribution pinit(s0), state transition distribution ptrans(st+1|st, at), observation distribution
pobs(ot|st), and reward2 function r : O →R. For simplicity we assume episodic tasks with ﬁnite
horizon H. We further denote a complete trajectory τ = (o0, a0, . . . , oH), and for convenience we
introduce the concept of a history at time t, ht = (o0, a0, . . . , ot)."
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.03354632587859425,"A common control scenario for POMDPs is when actions are decided based on all the available
information from the past. We call this the standard POMDP setting. The control mechanism can be
represented as a stochastic policy π(at|ht), which together with the POMDP dynamics pinit, pobs and
ptrans deﬁnes a probability distribution over trajectories τ,"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.03514376996805112,pstd(τ) =
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.036741214057507986,"S|τ|+1
X"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.038338658146964855,"s0→|τ|
pinit(s0)pobs(o0|s0)"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.039936102236421724,"|τ|−1
Y"
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.04153354632587859,"t=0
π(at|ht)ptrans(st+1|st, at)pobs(ot+1|st+1)."
PARTIALLY-OBSERVABLE MARKOV DECISION PROCESS,0.04313099041533546,"This whole data-generation mechanism can be represented visually as a DAG, represented in Figure 1.
A key characteristic in this setting is that At ⊥⊥St | Ht is always true, that is, every action is
independent of the current state given the history."
MODEL-BASED RL,0.04472843450479233,"2.3
MODEL-BASED RL"
MODEL-BASED RL,0.0463258785942492,"Assuming the objective is the long-term reward, the POMDP control problem formulates as:"
MODEL-BASED RL,0.04792332268370607,"π⋆= arg max
π
E
τ∼pstd   |τ|
X"
MODEL-BASED RL,0.04952076677316294,"t=0
r(ot) "
MODEL-BASED RL,0.051118210862619806,".
(1)"
MODEL-BASED RL,0.052715654952076675,"Model-based RL relies on the estimation of the POMDP transition model pstd(ot+1|ht, at) to solve
(1), which decomposes into two sub-problems:"
MODEL-BASED RL,0.054313099041533544,"1. learning: given a dataset D, estimate a transition model ˆq(ot+1|ht, at) ≈pstd(ot+1|ht, at);"
MODEL-BASED RL,0.05591054313099041,"2. planning: given a history ht and a transition model ˆq, decide on an optimal action at."
MODEL-BASED RL,0.05750798722044728,"As we will see shortly, the transition model ˆq seeked by model-based RL is inherently causal [9]. In
this work we consider only the ﬁrst problem above, that is, learning the (causal) POMDP transition
model from data."
MODEL-BASED RL AS CAUSAL INFERENCE,0.05910543130990415,"3
MODEL-BASED RL AS CAUSAL INFERENCE"
MODEL-BASED RL AS CAUSAL INFERENCE,0.06070287539936102,"Decision problems, such as those arising in POMDPs, can naturally be formulated in terms of causal
queries where actions directly translate into do statements. For example, given past information about
the POMDP process, what will be the causal effect of an action (intervention) on future rewards ? 3"
THE INTERVENTIONAL REGIME,0.06230031948881789,"3.1
THE INTERVENTIONAL REGIME"
THE INTERVENTIONAL REGIME,0.06389776357827476,"In the interventional regime, we assume a dataset Dint of episodes τ collected in the standard POMDP
setting from an arbitrary decision policy π(at|ht),"
THE INTERVENTIONAL REGIME,0.06549520766773163,"Dint ∼pinit, ptrans, pobs, π."
THE INTERVENTIONAL REGIME,0.0670926517571885,"Let us now adopt a causal perspective and reason in terms of interventions in the causal system,
depicted in Figure 1. Consider that we want to control the system, that is, replace π with π⋆, in
order to maximize a long-term outcome. Then, evaluating the effect of each action on the system is a
causal inference problem. In order to decide on the best ﬁrst action a0 given h0 = (o0), one must
evaluate a series of causal queries in the form pstd(o1|o0, do(a0)), then pstd(o2|o0, do(a0), o1, do(a1)),
and so on, and ﬁnally using those causal distributions for planning by solving a Bellman equation.
Conveniently, in the interventional regime, applying rule R2 of do-calculus on the causal DAG results"
THE INTERVENTIONAL REGIME,0.06869009584664537,"2Without loss of generality we consider the reward to be part of the observation ot to simplify our notation.
3A guiding example accompanying this section can be found in the appendix (Section A.3)."
THE INTERVENTIONAL REGIME,0.07028753993610223,Under review as a conference paper at ICLR 2022 S0 pinit St
THE INTERVENTIONAL REGIME,0.07188498402555911,ptrans St+1
THE INTERVENTIONAL REGIME,0.07348242811501597,ptrans O0 pobs Ot pobs Ot+1 pobs At π At−1 π
THE INTERVENTIONAL REGIME,0.07507987220447285,Figure 1: Standard POMDP setting. S0 pinit St
THE INTERVENTIONAL REGIME,0.07667731629392971,ptrans St+1
THE INTERVENTIONAL REGIME,0.07827476038338659,ptrans O0 pobs Ot pobs Ot+1 pobs At πprv At−1 πprv
THE INTERVENTIONAL REGIME,0.07987220447284345,Figure 2: Privileged POMDP setting.
THE INTERVENTIONAL REGIME,0.08146964856230032,"in those queries being trivially identiﬁable from pstd(τ). In fact, those queries exactly boil down to
the standard POMDP transition model that model-based RL seeks to estimate,"
THE INTERVENTIONAL REGIME,0.08306709265175719,"pstd(ot+1|o0→t, do(a0→t)) = pstd(ot+1|ht, at).
(2)"
THE INTERVENTIONAL REGIME,0.08466453674121406,"As such, model-based RL can be naturally reinterpreted in terms of causal inference. Also, a
convenient property in this regime is that pstd(ot+1|ht, at) does not depend on the control policy
π that was used to build the dataset Dint. The only requirement, in order to estimate transition
probabilities for every ht, at combination, is that π has a non-zero chance to explore every action,
that is, π(at|ht) > 0, ∀at, ht. Then, an unbiased estimate of the standard POMDP transition model
can be obtained simply via log-likelihood maximization:"
THE INTERVENTIONAL REGIME,0.08626198083067092,"ˆq = arg max
q∈Q"
THE INTERVENTIONAL REGIME,0.0878594249201278,"Dint
X τ"
THE INTERVENTIONAL REGIME,0.08945686900958466,"|τ|−1
X"
THE INTERVENTIONAL REGIME,0.09105431309904154,"t=0
log q(ot+1|ht, at).
(3)"
THE INTERVENTIONAL REGIME,0.0926517571884984,"In some situations it is very reasonable to assume an interventional regime, for example when it is
known to hold by construction. This is the case with online RL data, as the learning agent itself
explicitly controls the data-collection policy π(at|ht). But it can also be the case with ofﬂine RL
data, if one knows that the data-collection policy did not use any additional information besides
the information available to the learning agent, ht. In Atari video games for example, it is hard to
imagine a human player using any kind of privileged information related to the machine’s internal
state st other than the video and audio outputs from the game."
THE OBSERVATIONAL REGIME,0.09424920127795527,"3.2
THE OBSERVATIONAL REGIME"
THE OBSERVATIONAL REGIME,0.09584664536741214,"In the observational regime, we assume a dataset Dobs of episodes τ collected in the privileged
POMDP setting, depicted in Figure 2. In this setting episodes are collected from an external agent
who has access to privileged information, in the extreme case the whole POMDP state st, which
the learning agent can not observe4. In this setting we denote the data-generating control policy
πprv(at|ht, st), such that
Dobs ∼pinit, ptrans, pobs, πprv.
We denote the whole episode distribution resulting from pinit, ptrans, pobs and πprv as pprv(τ). A key
characteristic in this setting is that now At ⊥⊥St | Ht can not be assumed to hold any more."
THE OBSERVATIONAL REGIME,0.09744408945686901,"Let us reason here again in terms of causal inference from the causal system depicted in Figure 2. For
the purpose of controlling the POMDP in the standard setting, in the light of past information ht, we
want to evaluate the same series of causal queries as before, in the form pprv(ot+1|o0→t, do(a0→t)).
This time however, those causal queries are not identiﬁable from pprv(τ). Evaluating them would
require knowledge of the POMDP hidden states st, which act as confounding variables. For example,
identifying the ﬁrst query at t = 0 requires at least the observation of s0,"
THE OBSERVATIONAL REGIME,0.09904153354632587,"pprv(o1|o0, do(a0)) =
X"
THE OBSERVATIONAL REGIME,0.10063897763578275,"s0∈S
pprv(s0|o0, do(a0))pprv(o1|s0, o0, do(a0)) =
X"
THE OBSERVATIONAL REGIME,0.10223642172523961,"s0∈S
pprv(s0|o0)pprv(o1|s0, a0)"
THE OBSERVATIONAL REGIME,0.10383386581469649,"4Note that our only assumption is that this external agent has access to privileged information. We do not
assume it acts optimally with respect to the learning agent’s reward, or any other reward."
THE OBSERVATIONAL REGIME,0.10543130990415335,Under review as a conference paper at ICLR 2022 S0 pinit St
THE OBSERVATIONAL REGIME,0.10702875399361023,ptrans St+1
THE OBSERVATIONAL REGIME,0.10862619808306709,ptrans O0 pobs Ot pobs Ot+1 pobs At π At−1 π I
THE OBSERVATIONAL REGIME,0.11022364217252396,"Figure 3: Augmented POMDP setting, with a policy regime indicator I taking values in {0, 1}
(1=interventional regime, no confounding, 0=observational regime, potential confounding), such
that π(at|ht, st, i = 1) = π(at|ht, i = 1). This additional constraint introduces a contextual
independence At ⊥⊥St | Ht, I = 1."
THE OBSERVATIONAL REGIME,0.11182108626198083,"(R3 and R2 of do-calculus, then Ot+1 ⊥⊥Ht | St, At)."
THE OBSERVATIONAL REGIME,0.1134185303514377,"In many ofﬂine RL situations, we believe that it is common to have access to POMDP trajectories
for which At ⊥⊥St | Ht can not be assumed, for example when demonstrations are collected from
a human agent acting in the world (see Section 1 for examples). In such a situation, the observed
trajectories may be confounded, and naively learning a causal transition model by solving (3) might
result in a non-causal model, and in non-optimal planning. A natural question is then: what can be
done in such a situation ? Are confounded trajectories useless ? Is there still a way to use this data ?"
COMBINING OBSERVATIONAL AND INTERVENTIONAL DATA,0.11501597444089456,"4
COMBINING OBSERVATIONAL AND INTERVENTIONAL DATA"
PROBLEM STATEMENT,0.11661341853035144,"4.1
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.1182108626198083,"We consider a generic situation where two datasets of POMDP trajectories Dint and Dobs are available,
sampled respectively in the interventional regime with policy πstd(at|ht), and in the observational
(potentially confounded) regime with policy πprv(at|ht, st). We then ask the following question: is
there a sound way to use the observational data for improving the estimator of the standard POMDP
transition model that would be recovered from the interventional data only ?"
THE AUGMENTED POMDP,0.11980830670926518,"4.2
THE AUGMENTED POMDP"
THE AUGMENTED POMDP,0.12140575079872204,"We formulate the problem of learning the standard POMDP transition model from Dint and Dobs
as that of inferring a structured latent-variable model. Since both datasets are sampled from the
same POMDP (pinit, ptrans and pobs) controlled in different ways (either πprv or πstd), the overall data
generating process can be represented in the form of an augmented DAG, depicted in Figure 3. We
simply introduce an auxiliary variable I ∈{0, 1} that acts as a regime indicator [5], for differentiating
between observational and interventional data. The augmented POMDP policy then simply becomes
π, where π(at|ht, st, i = 0) = πprv(at|ht, st) and π(at|ht, st, i = 1) = πstd(at|ht)."
THE AUGMENTED POMDP,0.12300319488817892,"For simplicity, in the following we will refer to the joint distribution of this augmented POMDP
as the true distribution p, and with a slight abuse of notation we will consider Dobs and Dint two
datasets of augmented POMDP trajectories, sampled respectively under the observational regime
(τ, i) ∼p(τ, i|i = 0), and the interventional regime (τ, i) ∼p(τ, i|i = 1). The causal queries
required to control the augmented POMDP can then be identiﬁed as"
THE AUGMENTED POMDP,0.12460063897763578,"p(ot+1|o0→t, do(a0→t)) = p(ot+1|o0→t, do(a0→t), i = 1)
= p(ot+1|ht, at, i = 1)"
THE AUGMENTED POMDP,0.12619808306709265,"(R1 of do-calculus, then R2 on the contextual causal DAG from Figure 1)."
THE AUGMENTED LEARNING PROBLEM,0.12779552715654952,"4.3
THE AUGMENTED LEARNING PROBLEM"
THE AUGMENTED LEARNING PROBLEM,0.12939297124600638,"In order to learn the standard POMDP transition model p(ot+1|ht, at, i = 1) from the augmented
dataset Dobs ∪Dint = D ∼p(τ, i), we propose the following two-step procedure."
THE AUGMENTED LEARNING PROBLEM,0.13099041533546327,Under review as a conference paper at ICLR 2022
THE AUGMENTED LEARNING PROBLEM,0.13258785942492013,"Learning In the ﬁrst step, we ﬁt a latent probabilistic model ˆq to the training trajectories, constrained
to respect all the independencies of our augmented POMDP. To do so we substitute the actual POMDP
hidden state st ∈S by a latent variable zt ∈Z, with Z the discrete latent space of the model. Our
learning problem then formulates as a standard likelihood maximization5, i.e.,"
THE AUGMENTED LEARNING PROBLEM,0.134185303514377,"ˆq = arg max
q∈Q D
X"
THE AUGMENTED LEARNING PROBLEM,0.13578274760383385,"(τ,i)
log q(τ, i),
(4)"
THE AUGMENTED LEARNING PROBLEM,0.13738019169329074,with Q the family of sequential latent probabilistic models that respect
THE AUGMENTED LEARNING PROBLEM,0.1389776357827476,"q(τ, i) = q(i)"
THE AUGMENTED LEARNING PROBLEM,0.14057507987220447,"Z|τ|+1
X"
THE AUGMENTED LEARNING PROBLEM,0.14217252396166133,"z0→|τ|
q(z0)q(o0|z0)"
THE AUGMENTED LEARNING PROBLEM,0.14376996805111822,"|τ|−1
Y"
THE AUGMENTED LEARNING PROBLEM,0.14536741214057508,"t=0
q(at|ht, zt, i)q(zt+1|at, zt)q(ot+1|zt+1),"
THE AUGMENTED LEARNING PROBLEM,0.14696485623003194,"q(at|ht, zt, i = 1) = q(at|ht, i = 1)."
THE AUGMENTED LEARNING PROBLEM,0.1485623003194888,"Inference In the second step, we recover ˆq(ot+1|ht, at, i = 1) as an estimator of the standard
POMDP transition model. This can be done efﬁciently with a forward algorithm over the augmented
DAG structure6, which unrolls over time as the RL agent evolves in the environment."
THE AUGMENTED LEARNING PROBLEM,0.1501597444089457,"Intuitively,
the
observational
data
Dobs
inﬂuences
the
interventional
transition
model
q(ot+1|ht, at, i = 1) as follows. The learned model q must ﬁt the observational and interven-
tional data by sharing the same building blocs q(z0), q(ot|zt) and q(zt+1|zt, at), while only the
expert policy q(at|ht, zt, i = 0) offers some ﬂexibility that allows to differentiate between both
regimes. As a result, imposing an observational distribution q(τ|i = 0) acts as a regularizer for the
interventional distribution q(τ|i = 1)."
THEORETICAL GUARANTEES,0.15175718849840256,"4.4
THEORETICAL GUARANTEES"
THEORETICAL GUARANTEES,0.15335463258785942,"In this section we show that our two-step approach is 1) correct, in the sense that it yields an unbiased
estimator of the standard POMDP causal transition model and 2) efﬁcient, in the sense that it yields
a better estimator than the one based on interventional data only (asymptotically in the number of
observational data). All proofs are deferred to the appendix (Section A.7)."
THEORETICAL GUARANTEES,0.15495207667731628,"First we show that the recovered estimator is unbiased, and then we derive bounds for
ˆq(ot+1|ht, at, i = 1) in the asymptotic observational scenario, |Dobs| →∞(regardless of the
interventional data Dint)."
THEORETICAL GUARANTEES,0.15654952076677317,"Proposition 1. Assuming |Z|
≥
|S|, ˆq(ot+1|ht, at, i
=
1) is an unbiased estimator of
p(ot+1|ht, at, i = 1)."
THEORETICAL GUARANTEES,0.15814696485623003,"Theorem 1. Assuming |Dobs| →∞, for any Dint the recovered causal model is bounded as follows:"
THEORETICAL GUARANTEES,0.1597444089456869,"T −1
Y"
THEORETICAL GUARANTEES,0.16134185303514376,"t=0
ˆq(ot+1|ht, at, i = 1) ≥"
THEORETICAL GUARANTEES,0.16293929712460065,"T −1
Y"
THEORETICAL GUARANTEES,0.1645367412140575,"t=0
p(at|ht, i = 0)p(ot+1|ht, at, i = 0), and"
THEORETICAL GUARANTEES,0.16613418530351437,"T −1
Y"
THEORETICAL GUARANTEES,0.16773162939297126,"t=0
ˆq(ot+1|ht, at, i = 1) ≤"
THEORETICAL GUARANTEES,0.16932907348242812,"T −1
Y"
THEORETICAL GUARANTEES,0.17092651757188498,"t=0
p(at|ht, i = 0)p(ot+1|ht, at, i = 0) + 1 −"
THEORETICAL GUARANTEES,0.17252396166134185,"T −1
Y"
THEORETICAL GUARANTEES,0.17412140575079874,"t=0
p(at|ht, i = 0),"
THEORETICAL GUARANTEES,0.1757188498402556,"∀hT −1, aT −1, T ≥1 where p(hT −1, aT −1, i = 0) > 0."
THEORETICAL GUARANTEES,0.17731629392971246,"As a direct consequence, in the asymptotic case, using observational data ensures stronger generaliza-
tion guarantees for the recovered transition model than using no observational data."
THEORETICAL GUARANTEES,0.17891373801916932,"Corollary 1. The estimator ˆq(ot+1|ht, at, i = 1) recovered after solving (4) with |Dobs| →∞offers
strictly better generalization guarantees than the one with |Dobs| = 0, for any Dint."
THEORETICAL GUARANTEES,0.1805111821086262,"5Note that, while the problem of learning structured latent variable models is known to be hard in general,
there also exists a wide range of tools and algorithms available in the literature for solving it approximately, such
as the EM algorithm or the method of ELBO maximization.
6See appendix for details (Section A.4)"
THEORETICAL GUARANTEES,0.18210862619808307,Under review as a conference paper at ICLR 2022
RELATED WORK,0.18370607028753994,"5
RELATED WORK"
RELATED WORK,0.1853035143769968,"A whole body of work exists around the question of merging interventional and observational data in
RL, with related results already in econometrics [20]. Bareinboim et al. [2] study a sequential decision
problem similar to ours, but assume that expert intentions are observed both in the interventional and
the observational regimes, i.e., prior to doing interventions the learning agent can ask “what would
the expert do in my situation ?” This introduces an intermediate, observed variable ˆat = f(ot) with
the property that pprv(at = ˆat|ˆat) = 1, which guarantees unconfoundedness in the observational
regime (At ⊥⊥St|Ht), so that observational data can be considered interventional, and the standard
PO-MDP transition model can be directly estimated via (3). Zhang and Bareinboim [31; 34] relax
this assumption in the context of binary bandits, and later on in the more general context of dynamic
treatment regimes [32; 33]. They derive causal bounds similar to ours (Theorem 1), and propose a
two-step approach which ﬁrst extracts causal bounds from observational data, and then uses these
bounds in an online RL algorithm. While their method nicely tackles the question of leveraging
observational data for online exploration, it does not account for uncertainty in the bounds estimated
from the observational data. In comparison, our latent-based approach is more ﬂexible, as it never
computes explicit bounds, but rather lets the learning agent decide through (4) how data from both
regimes inﬂuence the ﬁnal transition model, depending of the number of samples available. Kallus
et al. [13] also propose a two-step learning procedure to combine observational and interventional
data in the context of binary contextual bandits. Their method however relies on a series of strong
parametric assumptions (strong one-way overlap, linearity, non-singularity etc.)."
RELATED WORK,0.1869009584664537,"A speciﬁc instantiation of our framework is off-policy evaluation, i.e., estimating the performance of
a policy π using observational data only. This corresponds to the speciﬁc setting |Dint| = 0, where
it can be shown that the causal transition model is in general not recoverable in the presence of
confounding variables. Still, a growing body of literature studies the question under speciﬁc structural
or parametric assumptions [19; 30; 3]. In the context of imitation learning, de Haan et al. [6] attribute
the issue of causal misidentiﬁcation, that is, ascribing the actions of an agent to the wrong explanatory
variables, to confounding. We argue that this explanation is erroneous, since their imitated experts are
trained in the standard POMDP setting (interventional regime). This reasoning supports Spencer et al.
[28], who shows that causal misidentiﬁcation is simply a manifestation of covariate shift. Finally,
other issues orthogonal to confounding can appear when combining online and ofﬂine data RL, for
example the value function initialisation problem [8], or the bootstrapping error problem [16; 21]."
EXPERIMENTS,0.18849840255591055,"6
EXPERIMENTS"
EXPERIMENTS,0.1900958466453674,"We perform experiments on three synthetic toy problems, each one expressing a different level of
complexity and a different form of hidden information."
EXPERIMENTS,0.19169329073482427,"Door In this toy problem, we consider a closed door, and a light (red or green) indicating which of
two buttons (A and B) should be pressed to open the door. The privileged agent perceives the color of
the light, while the learning agent doesn’t (colorblind). This corresponds to a simple binary bandit,
with a time horizon H = 1 and a hidden state space |S| = 3."
EXPERIMENTS,0.19329073482428116,"Tiger In this classical problem from the literature [4], the agent stands in front of two doors, one with
a treasure behind (+10 reward) and one with a tiger behind (-100 reward). At each time step the agent
can either open one of the doors, or listen (-1 reward) to obtain a noisy estimate of the tiger’s position.
The privileged agent has full knowledge of the tiger’s position, while the learning agent doesn’t. This
toy problem is a small-scale POMDP, with a time horizon H = 50 and a hidden state space |S| = 6."
EXPERIMENTS,0.19488817891373802,"Gridworld This problem is inspired from Alt et al. [1]. Here the agent starts on the top-left corner of
a small 5x5 grid, and tries to get to a target placed on the bottom side behind a large wall. The agent
can use ﬁve actions: top, right, bottom, left and idle, and moves into the desired direction with 50%
chances, or randomly remains in the current tile or slips to one of the 4 adjacent tiles otherwise. The
privileged agent has full knowledge of its position at each time step, while the learning agent is only
revealed this information once in a while, with 20% chances. This toy problem constitutes a more
challenging POMDP, with a time horizon H = 20 and a hidden state space |S| = 42."
EXPERIMENTS,0.1964856230031949,"For each toy problem, we train and evaluate our proposed approach, augmented, using a large amount
of observational data Dobs (512 samples for door, 8192 for tiger and gridworld), and interventional"
EXPERIMENTS,0.19808306709265175,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.19968051118210864,"data Dint of varying size, collected from random explorations. Each time, we compare our approach
to two baseline methods: no obs, where the Dobs is not used at all during training, and naive, where
Dobs is naively combined with Dint as if there was no confounding. As a reference, in the door
experiment we also report the performance of Kallus et al. [13] (only setting in which it applies). We
repeat each experiment over 20 random seeds. In the following we report our main results, and defer
the reader to the appendix (Section A.5) for the complete experimental details and results. 7"
PERFORMANCE OF THE RL AGENTS,0.2012779552715655,"6.1
PERFORMANCE OF THE RL AGENTS"
PERFORMANCE OF THE RL AGENTS,0.20287539936102236,"door
tiger
gridworld"
PERFORMANCE OF THE RL AGENTS,0.20447284345047922,"24
27
210
213"
PERFORMANCE OF THE RL AGENTS,0.20607028753993611,nints (log scale) 600 400 200 0 200
PERFORMANCE OF THE RL AGENTS,0.20766773162939298,"24
27
210
213"
PERFORMANCE OF THE RL AGENTS,0.20926517571884984,nints (log scale) 0.00 0.25 0.50 0.75
"NO OBS
NAIVE
AUGMENTED",0.2108626198083067,"1.00
no obs
naive
augmented
Kallus et al."
"NO OBS
NAIVE
AUGMENTED",0.2124600638977636,"Figure 4: Performance of each RL agent on our three toy problems (the higher the better). We report
the average cumulative reward (mean ± std) obtained on the real environment. Little markers indicate
the signiﬁcance of a two-sided Wilcoxon signed-rank test [7] with α < 5%, between our method,
augmented, and the baselines no obs (down triangles), naive (squares) and Kallus et al. (up triangles)."
"NO OBS
NAIVE
AUGMENTED",0.21405750798722045,"In Figure 4 we report the test performance of an RL agent trained on the transition models recovered
by each method. Here the privileged policy πprv consists of a good but imperfect expert in both door
and tiger, and a shortest-path expert in gridworld. In all three toy problems, our method successfully
leverages the confounded observational data and outperforms the two baseline methods, especially in
the low-sample regime (few interventional samples). Most noticeably, the no obs baseline converges
to the same performance as our method, if given enough interventional samples, while the naive
baseline seems to suffer from the additional observational data, and converges much slower than the
two other methods. As a reference, our approach also performs much better than Kallus et al. [13]."
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.21565495207667731,"6.2
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.21725239616613418,"noisy good expert
perfectly good expert
perfectly bad expert"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.21884984025559107,"random expert
positively biased expert
negatively biased expert"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.22044728434504793,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.2220447284345048,nints (log scale) 0.00 0.05 0.10 0.15
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.22364217252396165,"no obs
naive
augmented"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.22523961661341854,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.2268370607028754,nints (log scale) 0.00 0.05 0.10 0.15 0.20
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.22843450479233227,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.23003194888178913,nints (log scale) 0.00 0.05 0.10 0.15 0.20
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.23162939297124602,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.23322683706070288,nints (log scale) 0.00 0.05 0.10 0.15
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.23482428115015974,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.2364217252396166,nints (log scale) 0.00 0.05 0.10 0.15
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.2380191693290735,"23
25
27"
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.23961661341853036,nints (log scale) 0.00 0.05 0.10 0.15
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.24121405750798722,"Figure 5: Robustness to different degrees of confounding in the door problem (the lower the better).
We report the JS divergence (mean ± std) between the true and the recovered transition model."
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.24281150159744408,7Code available at https://github.com/causal-rl-anonymous/causal-rl
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.24440894568690097,Under review as a conference paper at ICLR 2022
ROBUSTNESS TO DIFFERENT DEGREES OF CONFOUNDING,0.24600638977635783,"Our approach is robust to any kind of confounding, and does not assume the observed expert uses
the privileged information in any speciﬁc way, or performs well or poorly at the task at hand. To
empirically demonstrate this claim, we repeat the door experiment with various expert behaviours,
including perfectly good / bad (always / never press the correct button), positively / negatively biased
(overly optimistic / pessimistic towards the optimal button), and random as a control (no confounding).
The outcome of this experiment is showcased in Figure 5, where our method always results in the
best estimate of the transition model (except in the situation with no confounding, where the naive
approach is slightly more effective). In the appendix (Section A.6.1) we also report gains in terms of
reward, except when there is no confounding or when the confounding induces a positive bias, in
which case the naive approach performs slightly better."
FOCUS ON THE GRIDWORLD PROBLEM,0.2476038338658147,"6.3
FOCUS ON THE GRIDWORLD PROBLEM"
FOCUS ON THE GRIDWORLD PROBLEM,0.24920127795527156,"JS divergence
cum. reward"
FOCUS ON THE GRIDWORLD PROBLEM,0.2507987220447284,"24
27
210
213"
FOCUS ON THE GRIDWORLD PROBLEM,0.2523961661341853,nints (log scale) 0.0 0.2 0.4 0.6
FOCUS ON THE GRIDWORLD PROBLEM,0.2539936102236422,"24
27
210
213"
FOCUS ON THE GRIDWORLD PROBLEM,0.25559105431309903,nints (log scale) 0.00 0.25 0.50 0.75
"NO OBS
NAIVE
AUGMENTED
AUGMENTED",0.2571884984025559,"1.00
no obs
naive
augmented
augmented"
"NO OBS
NAIVE
AUGMENTED
AUGMENTED",0.25878594249201275,"no obs
naive"
"NO OBS
NAIVE
AUGMENTED
AUGMENTED",0.26038338658146964,"Figure 6: The gridworld experiment. Left: the JS divergence and cumulative reward obtained by
each method. Right: the initial grid, and a heatmap of the tiles visited by the RL agents at test time
at the |Dint| = 27 mark. At this point, only the augmented method has learned how to pass the wall."
"NO OBS
NAIVE
AUGMENTED
AUGMENTED",0.26198083067092653,"Let us now have a particular focus on our most complex problem, the gridworld problem, showcased
in Figure 6. Our method, augmented, starts obtaining positive rewards with 27 = 128 interventional
samples, which is two orders of magnitude better than the two other methods, where positive rewards
appear later on at 29 = 512 samples. This impact is clearly noticeable if one looks at the test-time
trajectories of the RL agents at the 27 mark (Figure 6, right side). While the transition models learned
by the no obs and naive approaches result in the agent being stuck around its starting position, the
transition model learned by our approach already allows the agent to learn how to pass the wall and
reach the bottom side of the grid."
DISCUSSIONS,0.26357827476038337,"7
DISCUSSIONS"
DISCUSSIONS,0.26517571884984026,"In this paper we have presented a simple, generic method for combining interventional and observa-
tional (potentially confounded) data in model-based reinforcement learning for POMDPs. We have
demonstrated that our method is correct and efﬁcient in the asymptotic case (inﬁnite observational
data), and we have illustrated its effectiveness on three synthetic toy problems. One limitation
of our method is that it adds an additional challenge on top of model-based RL, that of learning
a latent-based transition model, which can become problematic in high-dimensional RL settings..
Still, the recent success of discrete latent models for solving complex RL tasks [10] or tasks in
high-dimensional domains [26] lets us envision that this difﬁculty can be overcome in practice. A
ﬁrst potential extension to our work could be to use ofﬂine data to guide online exploration, in a
fashion similar to Zhang and Bareinboim [31; 32; 33; 34]. A second direct extension is to consider
that several agents are observed, each with its own privileged policy, leading to multiple observational
regimes. This would lead, in the asymptotic case, to a stronger implicit regularizer for the causal
transition model. A third, obvious extension is to develop a similar approach for model-free RL,
maybe in the form of a value-function regularizer. A fourth direction is to apply the same approach to
POMDPs with continuous observation spaces (e.g., pixel-based problems), which is theoretically
very straightforward. Finally, we hope that our work will help to bridge the gap between the RL and
causality communities, and will convince the RL community that causality is an adequate tool to
reason about observational data, which is plentiful in the world."
DISCUSSIONS,0.26677316293929715,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.268370607028754,ETHICS STATEMENT
ETHICS STATEMENT,0.26996805111821087,"Confounding is a prevalent issue in human-generated data, and can be an important source of bias in
the design of decision policies, if not dealt with properly. This paper, although theoretical, makes
a humble step towards more robustness and fairness in AI-based decision systems, by combining
causality and statistical learning to address the confounding problem. As such this work has potentially
important societal implications, in particular in critical systems where lives are at stake such as
medicine or self-driving cars, where human-generated data is prevalent."
REPRODUCIBILITY STATEMENT,0.2715654952076677,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2731629392971246,"Our notations, our POMDP and our causal frameworks are explicitly introduced in Sections 2
and 3. Our problem statement is clearly laid down in Section 4 before we present our contribution
contribution, and the proofs of all our theoretical results are presented in the appendix, Section A.7.
Our experimental setting is described brieﬂy in the main body of the paper, Section 6, and in details
in the appendix, Section A.5. The experimental results presented in the paper are reproducible, as
both the workﬂow (scripts, parameters and seeds), and the source code are made publicly available
alongside the paper."
REFERENCES,0.2747603833865815,REFERENCES
REFERENCES,0.2763578274760383,"[1] Bastian Alt, Matthias Schultheis, and Heinz Koeppl. Pomdps in continuous time and discrete
spaces. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems, volume 33, pages 13151–13162. Curran Associates,
Inc., 2020."
REFERENCES,0.2779552715654952,"[2] Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A
causal approach. In NIPS, 2015."
REFERENCES,0.2795527156549521,"[3] Andrew Bennett, Nathan Kallus, Lihong Li, and Ali Mousavi. Off-policy evaluation in inﬁnite-
horizon reinforcement learning with latent confounders. In AISTATS, 2021."
REFERENCES,0.28115015974440893,"[4] Anthony R. Cassandra, Leslie P Kaelbling, and Michael L. Littman. Acting optimally in
partially observable stochastic domains. In AAAI, 1994."
REFERENCES,0.2827476038338658,"[5] A. Philip Dawid. Decision-theoretic foundations for statistical causality, 2020."
REFERENCES,0.28434504792332266,"[6] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In
NeurIPS, 2019."
REFERENCES,0.28594249201277955,"[7] Janez Demsar. Statistical comparisons of classiﬁers over multiple data sets. J. Mach. Learn. Res.,
7:1–30, 2006.
URL http://dblp.uni-trier.de/db/journals/jmlr/jmlr7.
html#Demsar06."
REFERENCES,0.28753993610223644,"[8] Sylvain Gelly and David Silver. Combining online and ofﬂine knowledge in uct. In ICML,
2007."
REFERENCES,0.28913738019169327,"[9] Samuel J Gershman. Reinforcement learning and causal models. In Michael R. Waldmann,
editor, The Oxford handbook of causal reasoning, chapter 10, pages 295–306. Oxford University
Press, 2017."
REFERENCES,0.29073482428115016,"[10] Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
discrete world models. In ICLR, 2021."
REFERENCES,0.29233226837060705,"[11] Yimin Huang and Marco Valtorta. Pearl’s calculus of intervention is complete. In UAI, 2006."
REFERENCES,0.2939297124600639,"[12] Guido W. Imbens and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical
Sciences: An Introduction. Cambridge University Press, 2015."
REFERENCES,0.2955271565495208,"[13] Nathan Kallus, Aahlad Manas Puli, and Uri Shalit. Removing hidden confounding by experi-
mental grounding. In NeurIPS, 2018."
REFERENCES,0.2971246006389776,Under review as a conference paper at ICLR 2022
REFERENCES,0.2987220447284345,"[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,
2015."
REFERENCES,0.3003194888178914,"[15] Daphne Koller and Nir Friedman. Probabilistic Graphical Models - Principles and Techniques.
MIT Press, 2009."
REFERENCES,0.3019169329073482,"[16] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning
via bootstrapping error reduction. In NeurIPS, 2019."
REFERENCES,0.3035143769968051,"[17] Sascha Lange, Thomas Gabel, and Martin A. Riedmiller. Batch reinforcement learning. In
Reinforcement Learning, volume 12 of Adaptation, Learning, and Optimization, pages 45–73.
Springer, 2012."
REFERENCES,0.305111821086262,"[18] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint, 2020."
REFERENCES,0.30670926517571884,"[19] Chaochao Lu, Bernhard Schölkopf, and José Miguel Hernández-Lobato. Deconfounding
reinforcement learning in observational settings. arXiv preprint, 2018."
REFERENCES,0.3083067092651757,"[20] Charles F Manski. Nonparametric bounds on treatment effects. The American Economic Review,
80(2):319–323, 1990."
REFERENCES,0.30990415335463256,"[21] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with ofﬂine datasets, 2021."
REFERENCES,0.31150159744408945,"[22] Judea Pearl. Probabilistic reasoning in intelligent systems - networks of plausible inference.
Morgan Kaufmann, 1989."
REFERENCES,0.31309904153354634,"[23] Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2 edition, 2009."
REFERENCES,0.3146964856230032,"[24] Judea Pearl. Causal inference in statistics: An overview. Statistics Surveys, 3:96 – 146, 2009."
REFERENCES,0.31629392971246006,"[25] Judea Pearl. The do-calculus revisited. In UAI, 2012."
REFERENCES,0.31789137380191695,"[26] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint, 2021."
REFERENCES,0.3194888178913738,"[27] Ilya Shpitser and Judea Pearl. Identiﬁcation of joint interventional distributions in recursive
semi-markovian causal models. In AAAI, 2006."
REFERENCES,0.3210862619808307,"[28] Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J. Andrew
Bagnell. Feedback in imitation learning: The three regimes of covariate shift, 2021."
REFERENCES,0.3226837060702875,"[29] Milan Studeny. Probabilistic Conditional Independence Structures. Springer, 2005."
REFERENCES,0.3242811501597444,"[30] Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable
environments. In AAAI, 2020."
REFERENCES,0.3258785942492013,"[31] Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandits: A causal
approach. In IJCAI, 2017."
REFERENCES,0.3274760383386581,"[32] Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment
regimes. In NeurIPS, 2019."
REFERENCES,0.329073482428115,"[33] Junzhe Zhang and Elias Bareinboim. Designing optimal dynamic treatment regimes: A causal
reinforcement learning approach. In Hal Daumé III and Aarti Singh, editors, ICML, volume
119 of Proceedings of Machine Learning Research, 2020."
REFERENCES,0.3306709265175719,"[34] Junzhe Zhang and Elias Bareinboim. Bounding causal effects on continuous outcomes. In
AAAI, 2021."
REFERENCES,0.33226837060702874,Under review as a conference paper at ICLR 2022
REFERENCES,0.33386581469648563,"A
APPENDIX"
REFERENCES,0.3354632587859425,"A.1
DO-CALCULUS"
REFERENCES,0.33706070287539935,"Several frameworks exist in the literature for reasoning about causality [23; 12]. Here we follow the
framework of Judea Pearl, whose concept of ladder of causation is particularly relevant to answer
RL questions. The ﬁrst level of the ladder, association, relates to the observation of an external
agent acting in the environment, while the second level, intervention, relates the question of what
will happen to the environment as a result of one’s own actions. The tool of do-calculus [25] acts
as a bridge between these two levels, and relates interventional distributions, such as p(y|do(x)),
to observational distributions, such as p(y|x), in causal systems that can be expressed as DAGs.
In a nutshell, do-calculus allows for measuring changes in the distribution of random variables
{X, Y, Z, . . . }, when one performs an arbitrary intervention do(x) which forces some variables to
take values X = x regardless of their causal ancestors. It relies on a complete set of rules [11; 27],
which allow for the following equivalences when speciﬁc structural conditions are met in the causal
DAG:"
REFERENCES,0.33865814696485624,"• R1: insertion/deletion of observations p(y|do(x), z, w) = p(y|do(x), w),"
REFERENCES,0.3402555910543131,"• R2: action/observation exchange p(y|do(x), do(z), w) = p(y|do(x), z, w),"
REFERENCES,0.34185303514376997,"• R3: insertion/deletion of actions p(y|do(x), do(z), w) = p(y|do(x), w)."
REFERENCES,0.34345047923322686,We refer the reader to Pearl [25] for a thorough introduction to do-calculus.
REFERENCES,0.3450479233226837,"A.2
ABOUT IGNORABILITY AND EXOGENEITY"
REFERENCES,0.3466453674121406,"In this paper we discuss and use at great length the concept of confounding, which is a core idea
in Judea Pearl’s do-calculus framework. For readers who are more familiar with the framework
of potential outcomes from Donald Rubin [12], the concept of confounding closely relates to the
concepts of ignorability and exogeneity. Indeed, both those concepts are shown to be equivalent to
the unconfoundedness (no confounding) assumption in [23]."
REFERENCES,0.34824281150159747,"A.3
GUIDING EXAMPLE: THE DOOR PROBLEM"
REFERENCES,0.3498402555910543,"This section introduces the door problem, and constitutes a guiding example that is meant to
accompany the paper."
REFERENCES,0.3514376996805112,"The door problem
Consider a door, a light, and two buttons A and B. The light is red 60% of the
time, and green the rest of the time. When the light is red, button A opens the door, while when the
light is green, then button B opens the door. I am told that the mechanism responsible for opening the
door depends on both the light color and the button pressed (light →door ←button), but I am not
given the mechanism itself. Suppose now that I am colorblind, and I want to open the door. Which
button should I press ? In the do-calculus framework, the question I am asking is"
REFERENCES,0.35303514376996803,"arg max
button∈{A,B}
p(door=open|do(button))."
REFERENCES,0.3546325878594249,"Interventional regime
If I am able to observe myself or another colorblind person interacting
with the door, then I know that which button is pressed is unrelated to which color the light is
(light ̸→button). Then I can directly estimate the causal effect of the button on the door,"
REFERENCES,0.3562300319488818,p(door=open|do(button)) = p(door=open|button).
REFERENCES,0.35782747603833864,"Whichever policy is used to collect (button, door) samples8, eventually I realise that button A has
more chances of opening the door (60%) than button B (40%), and thus is the optimal choice."
REFERENCES,0.35942492012779553,"8One assumption though is strict positivity, π(button) > 0 ∀button, so that both buttons are pressed."
REFERENCES,0.3610223642172524,Under review as a conference paper at ICLR 2022
REFERENCES,0.36261980830670926,"Observational regime
Assume now that I observe another person interacting with the door. I
do not know whether that person is colorblind or not (light →button is possible). Then, without
further knowledge, I cannot recover the causal queries p(door=open|do(button)) from the observed
distribution p(door, button). In the do-calculus framework, the queries are said non identiﬁable.
However, if that person was to tell me the light color they see before they press A or B, then I could
recover those queries as follows,"
REFERENCES,0.36421725239616615,"p(door=open|do(button)) =
X"
REFERENCES,0.365814696485623,"light∈{red,green}
p(light)p(door=open|light, button)."
REFERENCES,0.36741214057507987,"This formula, called deconfounding, eventually yields the correct causal transition probabilities
regardless of the observed policy9, given that enough (light, button, door) samples are observed."
REFERENCES,0.36900958466453676,"Merging interventional and observational data
Let us now look at our door example in light of
Theorem 1. Assume this time that I observe many (button, door) interactions from a non-colorblind
person (i = 0), who’s policy is π(button=A|light=red) = 0.9 and π(button=A|light=green) =
0.4. Then I can already infer from Theorem 1 that p(door=open|do(button=A)) ∈[0.54, 0.84]
and p(door=open|do(button=B)) ∈[0.24, 0.94]. I now get a chance to interact with the door
(i = 1), and I decide to press A 10 times and B 10 times.
I am unlucky, and my inter-
ventional study results in the following probabilities: q(door=open|do(button=A)) = 0.5 and
q(door=open|do(button=B)) = 0.5. This does not coincide with my (reliable) observational study,
and therefore I adjust q(door=open|do(button=A)) to its lower bound 0.54. I now believe that
pressing A is more likely to be my optimal strategy."
REFERENCES,0.3706070287539936,"A.4
RECOVERING THE STANDARD POMDP TRANSITION MODEL."
REFERENCES,0.3722044728434505,"Recovering ˆq(ot+1|ht, at, i = 1) can be done as follows:"
REFERENCES,0.3738019169329074,"ˆq(ot+1|ht, at, i = 1) = Z
X"
REFERENCES,0.3753993610223642,"zt
ˆq(zt|ht, i = 1) Z
X"
REFERENCES,0.3769968051118211,"zt+1
ˆq(zt+1|zt, at)ˆq(ot+1|zt+1)."
REFERENCES,0.37859424920127793,"The second and third terms are readily available as components of the augmented POMDP model
ˆq, while the ﬁrst term can be recovered by unrolling a forward algorithm over the augmented DAG
structure. First, we have"
REFERENCES,0.3801916932907348,"ˆq(z0, o0|i = 1) = ˆq(z0)ˆq(o0|z0),"
REFERENCES,0.3817891373801917,"ˆq(z0|h0, i = 1) =
ˆq(z0, o0|i = 1)
PZ
z0 ˆq(z0, o0|i = 1)
."
REFERENCES,0.38338658146964855,"Then, for every t′ from 0 to t −1,"
REFERENCES,0.38498402555910544,"ˆq(zt′+1, ot′+1|ht′, at′, i = 1) = Z
X"
REFERENCES,0.3865814696485623,"zt′
ˆq(zt′|ht′, i = 1)ˆq(zt′+1|zt′, at′)ˆq(ot′+1|zt′+1),"
REFERENCES,0.38817891373801916,"ˆq(zt′+1|ht′+1, i = 1) =
ˆq(zt′+1, ot′+1|ht′, at′, i = 1)
PS
zt′+1 ˆq(zt′+1, ot′+1|ht′, at′, i = 1)
."
REFERENCES,0.38977635782747605,"9The strict positivity condition here is π(button|light) > 0 ∀button, light."
REFERENCES,0.3913738019169329,Under review as a conference paper at ICLR 2022
REFERENCES,0.3929712460063898,"A.5
EXPERIMENTAL DETAILS"
REFERENCES,0.39456869009584666,The code for reproducing our experiments is made available online10.
REFERENCES,0.3961661341853035,"We perform experiments on three synthetic toy problems: the door problem described earlier
(Section A.3), the classical tiger problem from the literature [4], and a 5x5 gridworld problem
inspired from Alt et al. [1]."
REFERENCES,0.3977635782747604,"Data
To assess the performance of our method, we consider a large observational dataset Dobs of
ﬁxed size (512 samples for door, 8192 samples for tiger and gridworld), and an interventional dataset
Dint of varying size, ranging on an exponential scale from 4 to |Dobs|."
REFERENCES,0.3993610223642173,"Baselines
We compare the performance of the transition model ˆq recovered in three different
settings: no obs, when only interventional data (D = Dint) is used for training; naive, when
observational data is naively combined with interventional data as if there was no confounding
(D = Dint ∪{(τ, 1)|(τ, i) ∈Dobs)}); and augmented, our proposed method (D = Dint ∪Dobs).
Note that the only difference between each of those settings is the training dataset, all other aspects
(learning procedure, model architecture, loss function) begin the same."
REFERENCES,0.4009584664536741,"Training
In all our experiments we use a tabular model for ˆq, that is, we use discrete proba-
bility tables for each building blocs of the transition model, q(z0), q(ot|zt), q(zt+1|zt, at), and
q(at|ht, zt, i = 0). We use a latent space |Z| of size 32, 32 and 128 respectively for each toy problem,
while the true latent space |S| is of size 3, 6 and 42. We train ˆq by directly minimizing the negative
log likelihood (4) via gradient descent. We use the Adam optimizer [14] with a learning rate of 10−2,
and train for 500 epochs consisting of 50 gradient descent steps with minibatches of size 32. We
divide the learning rate by 10 after 10 epochs without loss improvement (reduce on plateau), and we
stop training after 20 epochs without improvement (early stopping). In the door experiment we derive
the optimal policy ˆπ⋆exactly, while in the tiger and gridworld experiments we train a “dreamer” RL
agent on imaginary samples τ ∼ˆq(τ|i = 1) obtained from the model, using the belief states ˆq(st|ht)
as features. We use a simple Actor-Critic algorithm for training, and our agents consist of a simple
MLP with one hidden layers for both the critic and the policy parts. RL agents are trained until
convergence or with a maximum number of 1000 epochs, with a learning rate of 10−2, a discount
factor γ = 0.9 and a batch size of 8."
REFERENCES,0.402555910543131,"JS divergence
To evaluate the general quality of the recovered transition models, we compute
the expected Jensen-Shannon divergence between the learned ˆq(ot+1|ht, i = 1) and the true
p(ot+1|ht, i = 1), over transitions generated using a uniformly random policy πrand,"
REFERENCES,0.40415335463258784,"1
2Eτ∼pinit,ptrans,pobs,πrand "
REFERENCES,0.4057507987220447,log p(o0)
REFERENCES,0.4073482428115016,"m(o0) + |τ|
X"
REFERENCES,0.40894568690095845,"t=1
log p(ot+1|ht, i = 1)"
REFERENCES,0.41054313099041534,"m(ot+1|ht, i = 1)   +1"
REFERENCES,0.41214057507987223,"2Eτ∼ˆqinit,ˆqtrans,ˆqobs,πrand "
REFERENCES,0.41373801916932906,log ˆq(o0)
REFERENCES,0.41533546325878595,"m(o0) + |τ|
X"
REFERENCES,0.4169329073482428,"t=1
log ˆq(ot+1|ht, i = 1)"
REFERENCES,0.4185303514376997,"m(ot+1|ht, i = 1)  ,"
REFERENCES,0.42012779552715657,"where m(.) =
1
2 (ˆq(.) + p(.)). In the ﬁrst experiment we compute the JS exactly, while in the
second experiment we use a stochastic approximation over 100 trajectories τ to estimate each of the
expectation terms in the JS empirically."
REFERENCES,0.4217252396166134,"Reward.
To evaluate quality of the recovered transition models for solving the original RL task,
that is, maximizing the expected long-term reward, we evaluate the policy ˆπ⋆, obtained after planning
with the recovered model ˆq, on the true environment p,"
REFERENCES,0.4233226837060703,"Eτ∼pinit,ptrans,pobs,ˆπ⋆   |τ|
X"
REFERENCES,0.4249201277955272,"t=0
R(ot)  ."
REFERENCES,0.426517571884984,"In the ﬁrst experiment we compute this expectation exactly, while in the second experiment we use a
stochastic approximation using 100 trajectories τ."
REFERENCES,0.4281150159744409,10https://supplementary.materials/disclosed.after.acceptance
REFERENCES,0.42971246006389774,Under review as a conference paper at ICLR 2022
REFERENCES,0.43130990415335463,"A.6
COMPLETE EMPIRICAL RESULTS"
REFERENCES,0.4329073482428115,"A.6.1
DOOR EXPERIMENT"
REFERENCES,0.43450479233226835,"The door experiment corresponds to a simple binary bandit setting, that is, a speciﬁc POMDP with
horizon H = 1. The observation space is of size |O| = 0, since the learning agent receives no
observation, and the hidden state space is of minimal size |S| = 3 to encode both the initial light
color and the reward obtained afterwards. The bandit dynamics are described in Table 1."
REFERENCES,0.43610223642172524,"light
red
green
0.6
0.4"
REFERENCES,0.43769968051118213,p(light)
REFERENCES,0.43929712460063897,"door
light
button
closed
open"
REFERENCES,0.44089456869009586,"red
A
0.0
1.0
B
1.0
0.0"
REFERENCES,0.4424920127795527,"green
A
1.0
0.0
B
0.0
1.0"
REFERENCES,0.4440894568690096,"p(door|light, button)"
REFERENCES,0.44568690095846647,Table 1: Probability tables for our door bandit problem.
REFERENCES,0.4472843450479233,"We repeat the door experiment in six different scenarios, corresponding to different privileged policies
πprv ranging from a totally random agent to a perfectly good and a perfectly bad agent. Each time,
we evaluate the performance of the no obs, naive and augmented approaches under different data
regimes, by varying the sample size for both the observational data Dobs and the interventional data
Dint in the range (4, 8, 16, 32, 64, 128, 256, 512)."
REFERENCES,0.4488817891373802,"In each scenario, we report both the expected reward and the JS as heatmaps with |Dint| and |Dobs|
in the x-axis and y-axis respectively, to highlight the combined effect of the sample sizes on each
approach. We also provide as a heatmap the difference between our approach, augmented, and the
two other approaches no obs and naive. We always plot the expected reward in the ﬁrst row, and JS
in the second row. As a remark, shades of green show gains in reward (the higher the better), while
shades of purple show gains in JS (the lower the better)."
REFERENCES,0.4504792332268371,"Finally, we also present two plots which provide a focus on the data regime that corresponds to the
largest number of observational data (|Dobs| = 512), as in the main paper."
REFERENCES,0.4520766773162939,Under review as a conference paper at ICLR 2022
REFERENCES,0.4536741214057508,"Noisy Good Expert
In the noisy good expert setting, the expert plays halfway between a perfect
and a random policy. The diversity of its action leads to a good start for the naive model but the bias
it contains is hard to overcome. In contrast, our method makes good use of the observational data
from the start and is also able to correct the bias as interventional data come in, eventually converging
towards the true transition model."
REFERENCES,0.45527156549520764,πprv(button|light)
REFERENCES,0.45686900958466453,"button
light
A
B
red
0.9
0.1
green
0.4
0.6"
REFERENCES,0.4584664536741214,"23
25
27"
REFERENCES,0.46006389776357826,nints (log scale) 0.00 0.05 0.10 0.15
REFERENCES,0.46166134185303515,JS divergence
REFERENCES,0.46325878594249204,"no obs
naive
augmented"
REFERENCES,0.46485623003194887,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.46645367412140576,no obs
REFERENCES,0.4680511182108626,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.4696485623003195,naive obs+int
REFERENCES,0.4712460063897764,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.4728434504792332,augmented obs+int
REFERENCES,0.4744408945686901,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.476038338658147,augmented - no obs
REFERENCES,0.4776357827476038,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.4792332268370607,augmented - naive
REFERENCES,0.48083067092651754,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.48242811501597443,no obs
REFERENCES,0.4840255591054313,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.48562300319488816,naive obs+int
REFERENCES,0.48722044728434505,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.48881789137380194,augmented obs+int
REFERENCES,0.4904153354632588,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.49201277955271566,augmented - no obs
REFERENCES,0.4936102236421725,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.4952076677316294,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.05 0.00 0.05 0.05 0.00 0.05 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.10 0.15 0.050 0.025 0.000 0.025 0.050 0.050 0.025 0.000 0.025 0.050
REFERENCES,0.4968051118210863,"Figure 7: Noisy good expert setting. Heatmaps correspond respectively to the expected reward (top
row, higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.4984025559105431,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,"Random Expert
A random policy naturally results in unconfounded observational data, since it
does not exploits the privileged information. Hence, the naive approach is unbiased in this case,
and actually makes the best use of the observational data. Our approach, augmented, exhibits an
overall comparable performance, only slightly worse at times. We believe this can be explained by
the additional complexity of our method which tries to disentangle a confounded regime in the data,
and is not best suited to unconfounded data."
REFERENCES,0.5015974440894568,πprv(button|light)
REFERENCES,0.5031948881789138,"button
light
A
B
red
0.5
0.5
green
0.5
0.5"
REFERENCES,0.5047923322683706,"23
25
27"
REFERENCES,0.5063897763578274,nints (log scale) 0.00 0.05 0.10 0.15
REFERENCES,0.5079872204472844,JS divergence
REFERENCES,0.5095846645367412,"no obs
naive
augmented"
REFERENCES,0.5111821086261981,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.512779552715655,no obs
REFERENCES,0.5143769968051118,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5159744408945687,naive obs+int
REFERENCES,0.5175718849840255,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5191693290734825,augmented obs+int
REFERENCES,0.5207667731629393,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5223642172523961,augmented - no obs
REFERENCES,0.5239616613418531,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5255591054313099,augmented - naive
REFERENCES,0.5271565495207667,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5287539936102237,no obs
REFERENCES,0.5303514376996805,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5319488817891374,naive obs+int
REFERENCES,0.5335463258785943,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5351437699680511,augmented obs+int
REFERENCES,0.536741214057508,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5383386581469649,augmented - no obs
REFERENCES,0.5399361022364217,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5415335463258786,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.10 0.05 0.00 0.05 0.10 0.10 0.05 0.00 0.05 0.10 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.00 0.05 0.05 0.00 0.05
REFERENCES,0.5431309904153354,"Figure 8: Random expert setting. Heatmaps correspond respectively to the expected reward (top row,
higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.5447284345047924,Under review as a conference paper at ICLR 2022
REFERENCES,0.5463258785942492,"Perfectly Good Expert
Observing a perfectly good expert playing in the door problem induces a
strong bias, because every observed action always results in a positive reward. As such, the naive
approach struggles to learn a good transition model. The bias however is quickly corrected by our
augmented approach, which eventually converges to the true transition model faster than the no obs
approach."
REFERENCES,0.547923322683706,πprv(button|light)
REFERENCES,0.549520766773163,"button
light
A
B
red
1.0
0.0
green
0.0
1.0"
REFERENCES,0.5511182108626198,"23
25
27"
REFERENCES,0.5527156549520766,nints (log scale) 0.00 0.05 0.10 0.15 0.20
REFERENCES,0.5543130990415336,JS divergence
REFERENCES,0.5559105431309904,"no obs
naive
augmented"
REFERENCES,0.5575079872204473,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5591054313099042,no obs
REFERENCES,0.560702875399361,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5623003194888179,naive obs+int
REFERENCES,0.5638977635782748,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5654952076677316,augmented obs+int
REFERENCES,0.5670926517571885,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5686900958466453,augmented - no obs
REFERENCES,0.5702875399361023,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5718849840255591,augmented - naive
REFERENCES,0.5734824281150159,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5750798722044729,no obs
REFERENCES,0.5766773162939297,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5782747603833865,naive obs+int
REFERENCES,0.5798722044728435,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5814696485623003,augmented obs+int
REFERENCES,0.5830670926517572,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5846645367412141,augmented - no obs
REFERENCES,0.5862619808306709,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.5878594249201278,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.05 0.00 0.05 0.05 0.00 0.05 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20 0.1 0.0 0.1 0.1 0.0 0.1
REFERENCES,0.5894568690095847,"Figure 9: Perfectly good expert setting. Heatmaps correspond respectively to the expected reward
(top row, higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.5910543130990416,Under review as a conference paper at ICLR 2022
REFERENCES,0.5926517571884984,"Perfectly Bad Expert
Similarly to the previous setting, observing an expert that always chooses
a bad action leads to a strong bias, as every action is associated to a low reward. The behaviour in
terms of JS and reward is similar as well."
REFERENCES,0.5942492012779552,πprv(button|light)
REFERENCES,0.5958466453674122,"button
light
A
B
red
0.0
1.0
green
1.0
0.0"
REFERENCES,0.597444089456869,"23
25
27"
REFERENCES,0.5990415335463258,nints (log scale) 0.00 0.05 0.10 0.15 0.20
REFERENCES,0.6006389776357828,JS divergence
REFERENCES,0.6022364217252396,"no obs
naive
augmented"
REFERENCES,0.6038338658146964,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6054313099041534,no obs
REFERENCES,0.6070287539936102,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6086261980830671,naive obs+int
REFERENCES,0.610223642172524,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6118210862619808,augmented obs+int
REFERENCES,0.6134185303514377,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6150159744408946,augmented - no obs
REFERENCES,0.6166134185303515,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6182108626198083,augmented - naive
REFERENCES,0.6198083067092651,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6214057507987221,no obs
REFERENCES,0.6230031948881789,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6246006389776357,naive obs+int
REFERENCES,0.6261980830670927,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6277955271565495,augmented obs+int
REFERENCES,0.6293929712460063,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6309904153354633,augmented - no obs
REFERENCES,0.6325878594249201,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.634185303514377,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.04 0.02 0.00 0.02 0.04 0.04 0.02 0.00 0.02 0.04 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20 0.1 0.0 0.1 0.1 0.0 0.1
REFERENCES,0.6357827476038339,"Figure 10: Perfectly bad expert setting. Heatmaps correspond respectively to the expected reward
(top row, higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.6373801916932907,Under review as a conference paper at ICLR 2022
REFERENCES,0.6389776357827476,"Positively Biased Expert
Here the expert’s policy is considered as positively biased in the sense
that the agent will only obtain a positive reward when playing button A (with 55% chances) and
never by playing button B (0% chances). Because playing button A is actually the optimal policy,
this strong bias has a positive effect on the reward for the naive approach. Hence, although worse in
terms of JS than our approach, the naive approach always results in a very good policy in terms of
reward. Our augmented approach, however, seems more conservative."
REFERENCES,0.6405750798722045,πprv(button|light)
REFERENCES,0.6421725239616614,"button
light
A
B
red
0.8
0.2
green
1.0
0.0"
REFERENCES,0.6437699680511182,"23
25
27"
REFERENCES,0.645367412140575,nints (log scale) 0.00 0.05 0.10 0.15
REFERENCES,0.646964856230032,JS divergence
REFERENCES,0.6485623003194888,"no obs
naive
augmented"
REFERENCES,0.6501597444089456,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6517571884984026,no obs
REFERENCES,0.6533546325878594,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6549520766773163,naive obs+int
REFERENCES,0.6565495207667732,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.65814696485623,augmented obs+int
REFERENCES,0.6597444089456869,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6613418530351438,augmented - no obs
REFERENCES,0.6629392971246006,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6645367412140575,augmented - naive
REFERENCES,0.6661341853035144,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6677316293929713,no obs
REFERENCES,0.6693290734824281,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.670926517571885,naive obs+int
REFERENCES,0.6725239616613419,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6741214057507987,augmented obs+int
REFERENCES,0.6757188498402555,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6773162939297125,augmented - no obs
REFERENCES,0.6789137380191693,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6805111821086262,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.05 0.00 0.05 0.05 0.00 0.05 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.10 0.15 0.050 0.025 0.000 0.025 0.050 0.050 0.025 0.000 0.025 0.050
REFERENCES,0.6821086261980831,"Figure 11: Positively biased expert setting. Heatmaps correspond respectively to the expected reward
(top row, higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.6837060702875399,Under review as a conference paper at ICLR 2022
REFERENCES,0.6853035143769968,"Negatively Biased Expert
In an analogous way, a negatively biased expert will overuse button A,
leading to mixed feelings regarding this button, whereas it will always get a positive reward each
time it uses button B. This leads to the opposite behavior as we had in the previous setting, with the
naive approach always favoring the use of button B, and obtaining a bad performance in terms of
reward. The naive approach only gets better when a lot of interventional data is combined with the
biased observational data, while our augmented approach is able to overcome this pessimistic bias
very early on, and converges faster than both no obs and naive."
REFERENCES,0.6869009584664537,πprv(button|light)
REFERENCES,0.6884984025559105,"button
light
A
B
red
1.0
0.0
green
0.8
0.2"
REFERENCES,0.6900958466453674,"23
25
27"
REFERENCES,0.6916932907348243,nints (log scale) 0.00 0.05 0.10 0.15
REFERENCES,0.6932907348242812,JS divergence
REFERENCES,0.694888178913738,"no obs
naive
augmented"
REFERENCES,0.6964856230031949,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.6980830670926518,no obs
REFERENCES,0.6996805111821086,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7012779552715654,naive obs+int
REFERENCES,0.7028753993610224,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7044728434504792,augmented obs+int
REFERENCES,0.7060702875399361,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.707667731629393,augmented - no obs
REFERENCES,0.7092651757188498,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7108626198083067,augmented - naive
REFERENCES,0.7124600638977636,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7140575079872205,no obs
REFERENCES,0.7156549520766773,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7172523961661342,naive obs+int
REFERENCES,0.7188498402555911,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7204472843450479,augmented obs+int
REFERENCES,0.7220447284345048,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7236421725239617,augmented - no obs
REFERENCES,0.7252396166134185,"4
8
16
32
64 128
nints 4 8 16 32 64 128 256 512 nobs"
REFERENCES,0.7268370607028753,augmented - naive 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.45 0.50 0.55 0.60 0.2 0.1 0.0 0.1 0.2 0.2 0.1 0.0 0.1 0.2 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.10 0.15 0.05 0.00 0.05 0.05 0.00 0.05
REFERENCES,0.7284345047923323,"Figure 12: Pessimistic bias expert setting. Heatmaps correspond respectively to the expected reward
(top row, higher is better) and the JS divergence (bottom row, lower is better)."
REFERENCES,0.7300319488817891,Under review as a conference paper at ICLR 2022
REFERENCES,0.731629392971246,"A.6.2
TIGER EXPERIMENT"
REFERENCES,0.7332268370607029,"The tiger experiment corresponds a synthetic POMDP toy problem proposed by Cassandra et al.
[4]. In short, in this problem the agent stands in front of two doors to open, one of them having
a tiger behind it (-100 reward), and the other one a treasure (+10 reward). The agent also gets a
noisy observation of the system in the form of the roar from the tiger, which seems to originate from
the correct door most of the time (85% chances) and the wrong door sometimes (15% chances). In
order to reduce uncertainty the agent can listen to the tiger’s roar again, at the cost of a small penalty
(-1). We present the simpliﬁed POMDP dynamics in Table 2, and in our experiments we impose a
ﬁxed horizon of size H = 50. The observation space is of size |O| = 6, to encode the roar location
perceived by the agent and the obtained reward, ot = (roart, rewardt), and the hidden state space is
of minimal size |S| = 6 to encode both the tiger position and the reward obtained at each time step,
st = (tigert, rewardt)."
REFERENCES,0.7348242811501597,"tiger0
left
right
0.5
0.5"
REFERENCES,0.7364217252396166,p(tiger0)
REFERENCES,0.7380191693290735,"tigert+1
tigert
actiont
left
right left"
REFERENCES,0.7396166134185304,"listen
1.0
0.0
open left
0.5
0.5
open right
0.5
0.5 right"
REFERENCES,0.7412140575079872,"listen
0.0
1.0
open left
0.5
0.5
open right
0.5
0.5"
REFERENCES,0.7428115015974441,"p(tigert+1|tigert, actiont)"
REFERENCES,0.744408945686901,"roart
tigert
left
right
left
0.85
0.15
right
0.15
0.85"
REFERENCES,0.7460063897763578,p(roart|tigert)
REFERENCES,0.7476038338658147,"rewardt+1
tigert
actiont
-1
-100
+10 left"
REFERENCES,0.7492012779552716,"listen
1.0
0.0
0.0
open left
0.0
1.0
0.0
open right
0.0
0.0
1.0 right"
REFERENCES,0.7507987220447284,"listen
1.0
0.0
0.0
open left
0.0
0.0
1.0
open right
0.0
1.0
0.0"
REFERENCES,0.7523961661341853,"p(rewardt+1|tigert, actiont)"
REFERENCES,0.7539936102236422,Table 2: Probability tables for the tiger problem.
REFERENCES,0.755591054313099,"For the tiger experiment we consider four different privileged policies πprv for the ob-
served agent.
We then evaluate the performance of the no obs, naive and augmented ap-
proaches under different data regimes, by keeping the observational data ﬁxed to |Dobs| =
8192 while varying the varying the number of interventional data for Dint in the range
(4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192)."
REFERENCES,0.7571884984025559,Under review as a conference paper at ICLR 2022
REFERENCES,0.7587859424920128,"Noisy Good Expert
In this scenario the privileged expert adopts a policy that plays the optimal
action most of the time (open the treasure door), but also sometimes decides to just listen or to
open the wrong door. As can be seen, in this scenario our augmented method makes the best use of
the observational data, and is signiﬁcantly better than both the no obs and naive approaches in the
low-sample regime, both in terms of quality of the estimated transition model and obtained reward."
REFERENCES,0.7603833865814696,"action
tiger
listen
open left
open right
Left
0.05
0.3
0.65
right
0.05
0.8
0.15"
REFERENCES,0.7619808306709265,πprv(action|tiger)
REFERENCES,0.7635782747603834,"24
27
210
213"
REFERENCES,0.7651757188498403,nints (log scale) 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.7667731629392971,JS divergence
REFERENCES,0.768370607028754,"no obs
naive
augmented"
REFERENCES,0.7699680511182109,"24
27
210
213"
REFERENCES,0.7715654952076677,nints (log scale) 600 400 200 0 200
REFERENCES,0.7731629392971247,reward
REFERENCES,0.7747603833865815,Figure 13: Noisy good agent.
REFERENCES,0.7763578274760383,"Random Expert
In the random scenario there is no confounding, and observational data can be
safely mixed with interventional data. The naive approach thus does not suffer from any bias, and
in fact is the one that converges the fastest to the optimal transition model and policy. Our method,
while it manages to leverage the observational data to converge faster than no obs, suffers from a
worse performance than naive in the low sample regime, most likely because it tries to recover a
spurious confounding variable to distinguish the observational and interventional regimes, when none
actually exists."
REFERENCES,0.7779552715654952,"action
tiger
listen
open left
open right
left
0.33
0.33
0.33
right
0.33
0.33
0.33"
REFERENCES,0.7795527156549521,πprv(action|tiger)
REFERENCES,0.7811501597444089,"24
27
210
213"
REFERENCES,0.7827476038338658,nints (log scale) 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.7843450479233227,JS divergence
REFERENCES,0.7859424920127795,"no obs
naive
augmented"
REFERENCES,0.7875399361022364,"24
27
210
213"
REFERENCES,0.7891373801916933,nints (log scale) 600 400 200 0
REFERENCES,0.7907348242811502,reward
REFERENCES,0.792332268370607,Figure 14: Random agent.
REFERENCES,0.7939297124600639,Under review as a conference paper at ICLR 2022
REFERENCES,0.7955271565495208,"Very Good Expert
Here the privileged expert never opens the wrong door, and thus never receives
the very penalizing -100 reward. As a result the naive approach seems to be overly optimistic with
respect to the action of opening a door, which strongly affects the expected reward it obtains in the
true environment. While our augmented approach seems also to suffer from this bias in the very low
sample regime (as can be seen on the reward plot), overall the quality of the recovered transition
model is still superior to both other approaches, and converges faster to the true transition model."
REFERENCES,0.7971246006389776,"action
tiger
listen
open left
open right
left
0.05
0.0
0.95
right
0.05
0.95
0.0"
REFERENCES,0.7987220447284346,πprv(action|tiger)
REFERENCES,0.8003194888178914,"24
27
210
213"
REFERENCES,0.8019169329073482,nints (log scale) 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.8035143769968051,JS divergence
REFERENCES,0.805111821086262,"no obs
naive
augmented"
REFERENCES,0.8067092651757188,"24
27
210
213"
REFERENCES,0.8083067092651757,nints (log scale) 2500 2000 1500 1000 500 0
REFERENCES,0.8099041533546326,reward
REFERENCES,0.8115015974440895,Figure 15: Very good agent.
REFERENCES,0.8130990415335463,"Very Bad Expert
Here the privileged expert never opens the correct door, and thus never receives
a positive reward (+10). As a result, the naive approach seems to be very conservative, and prefers
not to take any chances opening a door. It turns out that this strategy is not too bad in terms of reward
(always listening yields a -51 total reward), and as such this causal bias seems to positively affect the
performance of the naive approach in the low sample regime, but prevents it from obtaining a better
policy in the high sample regime too. Our augmented method, on the other hand, is able to escape
this overly conservative strategy earlier on, and converges to a good-performing policy faster than
both other approaches."
REFERENCES,0.8146964856230032,"action
tiger
listen
open left
open right
left
0.05
0.95
0.0
right
0.05
0.0
0.95"
REFERENCES,0.8162939297124601,πprv(action|tiger)
REFERENCES,0.8178913738019169,"24
27
210
213"
REFERENCES,0.8194888178913738,nints (log scale) 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.8210862619808307,JS divergence
REFERENCES,0.8226837060702875,"no obs
naive
augmented"
REFERENCES,0.8242811501597445,"24
27
210
213"
REFERENCES,0.8258785942492013,nints (log scale) 600 400 200 0
REFERENCES,0.8274760383386581,reward
REFERENCES,0.829073482428115,Figure 16: Very bad agent.
REFERENCES,0.8306709265175719,Under review as a conference paper at ICLR 2022
REFERENCES,0.8322683706070287,"A.6.3
GRIDWORLD EXPERIMENT"
REFERENCES,0.8338658146964856,"The gridworld experiment, represented in Figure 17, is inspired from [1]. It consists in a small 5x5
grid where the agent starts on the top-left corner, and tries to get to a target placed on the bottom side
behind a large wall. The agent can use ﬁve actions: top, right, bottom, left and idle, and only receives
a noisy signal about its current position. At each time step, the agent’s position is revealed with
20% chances, and remains completely hidden otherwise. In addition, the agent’s actions only have a
stochastic effect, i.e., the agent moves into the desired direction with 50% chances, and otherwise
slips at random to one of the 5 adjacent tiles or current tile. In case the agent would bump into a wall,
it simply remains at its current position. The observation space is of size |O| = 44, to encode both
the agent’s location (or the indication that the location is hidden) and the reward, and the hidden state
space is of size |S| = 42 to encode both the agent’s location and the reward. In this experiment we
impose a ﬁxed horizon of size H = 20."
REFERENCES,0.8354632587859425,Figure 17: The gridworld problem
REFERENCES,0.8370607028753994,"For the gridworld experiment we consider a single policy πprv for the privileged agent, who acts
optimally (shortest path from current location to target). We then evaluate the performance of the no
obs, naive and augmented approaches under different data regimes, by keeping the observational data
ﬁxed to |Dobs| = 8192 while varying the varying the number of interventional data for Dint in the
range (4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192)."
REFERENCES,0.8386581469648562,"Very Good Expert
In this scenario the privileged agent adopts a perfect policy, and always chooses
an action leading to the shortest path towards the target. As can be seen, here again our augmented
method makes the best use of the observational data, and converges faster than both the no obs and
the naive approaches for recovering the true transition model. This improvement in the transition
model also translates into an improvement in terms of the learned policy, which starts converging
towards high reward values with fewer samples (27) than both no obs and naive (29)."
REFERENCES,0.8402555910543131,"24
27
210
213"
REFERENCES,0.84185303514377,nints (log scale) 0.0 0.2 0.4 0.6
REFERENCES,0.8434504792332268,JS divergence
REFERENCES,0.8450479233226837,"no obs
naive
augmented"
REFERENCES,0.8466453674121406,"24
27
210
213"
REFERENCES,0.8482428115015974,nints (log scale) 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.8498402555910544,reward
REFERENCES,0.8514376996805112,Figure 18: Perfect agent.
REFERENCES,0.853035143769968,Under review as a conference paper at ICLR 2022
REFERENCES,0.854632587859425,"|Dint|
no obs
naive
augmented 22 23 24 25 26 27"
REFERENCES,0.8562300319488818,"Figure 19: Average heat-maps over 100 episodes × 10 seeds, of the tiles visited by each trained
agent (no obs, naive, augmented) for different interventional data sizes (22, 23, 24, 25, 26, 27). The
augmented approach is the fastest (in terms of interventional data) to learn how to properly escape
the top part of the maze through tile (4, 2), and then move towards the treasure on tile (1, 3)."
REFERENCES,0.8578274760383386,Under review as a conference paper at ICLR 2022
REFERENCES,0.8594249201277955,"|Dint|
no obs
naive
augmented 28 29 210 211 212 213"
REFERENCES,0.8610223642172524,"Figure 20: Average heat-maps over 100 episodes × 10 seeds, of the tiles visited by each trained agent
(no obs, naive, augmented) for different interventional data sizes (28, 29, 210, 211, 212, 213). The
augmented approach is the fastest (in terms of interventional data) to learn how to properly escape
the top part of the maze through tile (4, 2), and then move towards the treasure on tile (1, 3)."
REFERENCES,0.8626198083067093,Under review as a conference paper at ICLR 2022
REFERENCES,0.8642172523961661,"A.7
PROOFS."
REFERENCES,0.865814696485623,"Proposition 1. Assuming |Z|
≥
|S|, ˆq(ot+1|ht, at, i
=
1) is an unbiased estimator of
p(ot+1|ht, at, i = 1)."
REFERENCES,0.8674121405750799,"Proof. The proof is straightforward. First, we have that D ∼p(τ, i). Second, we have p ∈Q,
because Q is only restricted to the augmented POMDP constraints, and because its latent space is
sufﬁciently large (|Z| ≥|S|). Therefore, ˆq(τ, i) solution of (4) is an unbiased estimator of p(τ, i),
and in particular ˆq(ot+1|ht, at, i = 1) is an unbiased estimator of p(ot+1|ht, at, i = 1)."
REFERENCES,0.8690095846645367,"Corollary 1. The estimator ˆq(ot+1|ht, at, i = 1) recovered after solving (4) with |Dobs| →∞offers
strictly better generalization guarantees than the one with |Dobs| = 0, for any Dint."
REFERENCES,0.8706070287539937,"Proof. There exists at least one history-action couple (hT −1, aT −1), T
≥1, that has non-
zero probability in the observational regime.
This ensures that there exists a value oT for
which QT −1
t=0 p(at|ht, i = 0)p(ot+1|ht, at, i = 0) is strictly positive, which in turn ensures
ˆq(oT +1|hT , aT , i = 1) > 0. As a result, the family of models {q(ot+1|ht, at, i = 1) | q ∈
Q, q(τ|i = 0) = p(τ|i = 0)} is a strict subset of the unrestricted family {q(ot+1|ht, at, i = 1) | q ∈
Q}, and thus offers strictly better generalization guarantees."
REFERENCES,0.8722044728434505,"Theorem 1. Assuming |Dobs| →∞, for any Dint the recovered causal model is bounded as follows:"
REFERENCES,0.8738019169329073,"T −1
Y"
REFERENCES,0.8753993610223643,"t=0
ˆq(ot+1|ht, at, i = 1) ≥"
REFERENCES,0.8769968051118211,"T −1
Y"
REFERENCES,0.8785942492012779,"t=0
p(at|ht, i = 0)p(ot+1|ht, at, i = 0), and"
REFERENCES,0.8801916932907349,"T −1
Y"
REFERENCES,0.8817891373801917,"t=0
ˆq(ot+1|ht, at, i = 1) ≤"
REFERENCES,0.8833865814696485,"T −1
Y"
REFERENCES,0.8849840255591054,"t=0
p(at|ht, i = 0)p(ot+1|ht, at, i = 0) + 1 −"
REFERENCES,0.8865814696485623,"T −1
Y"
REFERENCES,0.8881789137380192,"t=0
p(at|ht, i = 0),"
REFERENCES,0.889776357827476,"∀hT −1, aT −1, T ≥1 where p(hT −1, aT −1, i = 0) > 0."
REFERENCES,0.8913738019169329,"Proof of Theorem 1. Consider q(τ, i) ∈Q any distribution that follows our augmented POMDP
constraints. Then, for every T ≥1 we have"
REFERENCES,0.8929712460063898,"T −1
Y"
REFERENCES,0.8945686900958466,"t=0
q(at|ht, i)q(ot+1|ht, at, i) = q(τ|i)"
REFERENCES,0.8961661341853036,q(h0|i) =
REFERENCES,0.8977635782747604,"ZT +1
X"
REFERENCES,0.8993610223642172,"z0→T
q(z0|h0, i)"
REFERENCES,0.9009584664536742,"T −1
Y"
REFERENCES,0.902555910543131,"t=0
q(at, zt+1, ot+1|zt, ht, i),"
REFERENCES,0.9041533546325878,"by using At, Zt+1, Ot+1 ⊥⊥Z0→t−1 | Zt, Ht, I, which can be read via d-separation in the augmented
POMDP DAG. Likewise, for every t ≥0 we have"
REFERENCES,0.9057507987220448,"q(ot+1|ht, at, i = 1) = Z
X"
REFERENCES,0.9073482428115016,"zt+1
q(zt+1, ot+1|ht, at, i = 1) = Z
X"
REFERENCES,0.9089456869009584,"zt
q(zt|ht, i = 1) Z
X"
REFERENCES,0.9105431309904153,"zt+1
q(zt+1, ot+1|zt, ht, at, i = 0),"
REFERENCES,0.9121405750798722,"by using Zt ⊥⊥At | Ht, I = 1 and Zt+1, Ot+1 ⊥⊥I | Zt, At, Ht. Then for every t ≥1 we can
further write"
REFERENCES,0.9137380191693291,"q(ot+1|ht, at, i = 1) = Z
X zt"
REFERENCES,0.9153354632587859,"q(zt, ot|ht−1, at−1, i = 1)"
REFERENCES,0.9169329073482428,"q(ot|ht−1, at−1, i = 1) Z
X"
REFERENCES,0.9185303514376997,"zt+1
q(zt+1, ot+1|zt, ht, at, i = 0)."
REFERENCES,0.9201277955271565,"By recursively decomposing every q(zt, ot|ht−1, at−1, i = 1) until t = 0, and ﬁnally by using
Z0 ⊥⊥I | H0, we obtain that for any T ≥1"
REFERENCES,0.9217252396166135,"T −1
Y"
REFERENCES,0.9233226837060703,"t=0
q(ot+1|ht, at, i = 1) ="
REFERENCES,0.9249201277955271,"ZT +1
X"
REFERENCES,0.9265175718849841,"z0→T
q(z0|h0, i = 0)"
REFERENCES,0.9281150159744409,"T −1
Y"
REFERENCES,0.9297124600638977,"t=0
q(zt+1, ot+1|zt, at, ht, i = 0),"
REFERENCES,0.9313099041533547,Under review as a conference paper at ICLR 2022
REFERENCES,0.9329073482428115,which can be re-expressed as
REFERENCES,0.9345047923322684,"T −1
Y"
REFERENCES,0.9361022364217252,"t=0
q(ot+1|ht, at, i = 1) = AT
X"
REFERENCES,0.9376996805111821,"a′
0→T −1"
REFERENCES,0.939297124600639,"ZT +1
X"
REFERENCES,0.9408945686900958,"z0→T
q(z0|h0, i = 0)"
REFERENCES,0.9424920127795527,"T −1
Y"
REFERENCES,0.9440894568690096,"t=0
q(a′
t|zt, ht, i = 0)q(zt+1, ot+1|zt, ht, at, i = 0)."
REFERENCES,0.9456869009584664,"By considering the case a′
0→T −1 = a0→T −1 in isolation, and by assuming probabilities are positive,
we readily obtain our ﬁrst bound,"
REFERENCES,0.9472843450479234,"T −1
Y"
REFERENCES,0.9488817891373802,"t=0
q(ot+1|ht, at, i = 1) ≥"
REFERENCES,0.950479233226837,"T −1
Y"
REFERENCES,0.952076677316294,"t=0
q(at|ht, i = 0)q(ot+1|ht, at, i = 0)."
REFERENCES,0.9536741214057508,"In order to obtain our second bound, we further isolate the cases a′
0 ̸= a0, then a′
0 = a0 ∧a′
1 ̸= a1,
then a′
0 = a0 ∧a′
1 = a1 ∧a′
2 ̸= a2 and so on until a′
0→T −2 = a0→T −2 ∧a′
T −1 ̸= aT −1, which
yields"
REFERENCES,0.9552715654952076,"T −1
Y"
REFERENCES,0.9568690095846646,"t=0
q(ot+1|ht, at, i = 1) ="
REFERENCES,0.9584664536741214,"T −1
Y"
REFERENCES,0.9600638977635783,"t=0
q(at|ht, i = 0)q(ot+1|ht, at, i = 0) +"
REFERENCES,0.9616613418530351,"ZT +1
X"
REFERENCES,0.963258785942492,"z0→T
q(z0|h0, i = 0) (1 −q(a0|z0, h0, i = 0))"
REFERENCES,0.9648562300319489,"T −1
Y"
REFERENCES,0.9664536741214057,"t=0
q(zt+1, ot+1|zt, ht, at, i = 0) +"
REFERENCES,0.9680511182108626,"T −2
X K=0"
REFERENCES,0.9696485623003195,"ZT +1
X"
REFERENCES,0.9712460063897763,"z0→T
q(z0|h0, i = 0) K
Y"
REFERENCES,0.9728434504792333,"t=0
q(at, zt+1, ot+1|zt, ht, i = 0) (1 −q(aK|zK, hK, i = 0))"
REFERENCES,0.9744408945686901,"T −1
Y"
REFERENCES,0.9760383386581469,"t=K+1
q(zt+1, ot+1|zt, ht, at, i = 0)."
REFERENCES,0.9776357827476039,"Then by assuming probabilities are upper bounded by 1, we obtain"
REFERENCES,0.9792332268370607,"T −1
Y"
REFERENCES,0.9808306709265175,"t=0
q(ot+1|ht, at, i = 1) ≤"
REFERENCES,0.9824281150159745,"T −1
Y"
REFERENCES,0.9840255591054313,"t=0
q(at|ht, i = 0)q(ot+1|ht, at, i = 0) + 1 −q(a0|h0, i = 0) +"
REFERENCES,0.9856230031948882,"T −2
X K=0 K
Y"
REFERENCES,0.987220447284345,"t=0
q(ot+1|ht, at, i = 0) K−1
Y"
REFERENCES,0.9888178913738019,"t=0
q(at|ht, i = 0) − K
Y"
REFERENCES,0.9904153354632588,"t=0
q(at|ht, i = 0) ! ≤"
REFERENCES,0.9920127795527156,"T −1
Y"
REFERENCES,0.9936102236421726,"t=0
q(at|ht, i = 0)q(ot+1|ht, at, i = 0) + 1 −"
REFERENCES,0.9952076677316294,"T −1
Y"
REFERENCES,0.9968051118210862,"t=0
q(at|ht, i = 0)."
REFERENCES,0.9984025559105432,"Finally, with ˆq solution of (4) and |Dobs| →∞we have that DKL(p(τ|i = 0)∥ˆq(τ|i = 0)) = 0,
and thus ˆq(at|ht, i = 0) = p(at|ht, i = 0) and ˆq(ot+1|ht, at, i = 0) = p(ot+1|ht, at, i = 0), which
shows the desired result."
