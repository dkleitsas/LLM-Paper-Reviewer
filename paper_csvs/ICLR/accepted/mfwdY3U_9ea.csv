Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017123287671232876,"Reliable out-of-distribution (OOD) detection is fundamental to implementing
safer modern machine learning (ML) systems.
In this paper, we introduce
IGEOOD, an effective method for detecting OOD samples. IGEOOD applies to
any pre-trained neural network, works under various degrees of access to the ML
model, does not require OOD samples or assumptions on the OOD data but can
also beneﬁt (if available) from OOD samples. By building on the geodesic (Fisher-
Rao) distance between the underlying data distributions, our discriminator can
combine conﬁdence scores from the logits outputs and the learned features of a
deep neural network. Empirically, we show that IGEOOD outperforms competing
state-of-the-art methods on a variety of network architectures and datasets."
INTRODUCTION,0.003424657534246575,"1
INTRODUCTION"
INTRODUCTION,0.005136986301369863,"Deep neural networks (DNNs) reach the state-of-the-art in several classiﬁcation tasks as they are
known to generalize well on data with a distribution close to the training set. Whereas, in many prac-
tical applications, the training set does not reﬂect well enough the real-life environment (Quionero-
Candela et al., 2009) which is often non-stationary and sometimes with unpredictable events. There-
fore, matching the training scenario to reality can be impossible or too complex. The inability of
machine learning (ML) models to adapt to non-stationary distributions could limit their adoption in
mission-critical systems (e.g., autonomous devices, healthcare applications)."
INTRODUCTION,0.00684931506849315,"Out-of-Distribution (OOD) or novelty detection is one of the main objectives in conceiving reliable
ML systems (Amodei et al., 2016). A typical application is monitoring ML-based online services
for periodically shifting distributions. However, tracking changes in the underlying data distribution
is challenging as they contain unusual (irregular or unexpected) events and have large dimensions.
For instance, relying on the intrinsic properties of ML models and their statistical behavior in the
presence of in-distribution data is essential to identify OOD samples. Classic approaches to OOD
detection consist of deriving metrics for detecting those abnormalities from the lens of ML models
(e.g., softmax output, latent representations across layers), provided that often only a single test
example is available. Furthermore, these metrics are subject to potential limitations inherent in
practical scenarios depending on the level of access to information in the ML model, e.g., having
access only to the last layer or to all intermediate layers."
INTRODUCTION,0.008561643835616438,"The baseline approach for OOD detection relies on the predictive uncertainty of DNNs. Hendrycks
& Gimpel (2017) demonstrated that OOD samples, in general, induce DNN classiﬁers to output
less conﬁdent softmax scores, while existing state-of-the-art methods on classiﬁcation problems
still output high accuracy even under dataset shift. For instance, Ovadia et al. (2019) show that
as the accuracy of the underlying DNN increases, the supervisors’ outlier detection accuracy also"
INTRODUCTION,0.010273972602739725,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011986301369863013,"improves. Unfortunately, also the variance increases. Henriksson et al. (2021) observed that small
changes in model parameters that marginally impact the accuracy could have a degrading impact on
the performance of the OOD discriminator. This challenge is not exclusive to discriminative models.
Deep generative models also fall short in discerning OOD from in-distribution samples. Nalisnick
et al. (2019) raise awareness of the fact that deep generative models also may output a higher likeli-
hood to OOD samples. They show that, even though the samples from the in-distribution CIFAR-10
(Krizhevsky et al., 2009) dataset (e.g., cats, dogs, airplanes, ships) are conceptually and visually
different from house numbers from SVHN (Netzer et al., 2011) dataset, DNN-based classiﬁers may
still assign a high likelihood to SVHN samples."
INTRODUCTION,0.0136986301369863,"In this paper, we propose IGEOOD, a new uniﬁed and effective method to perform OOD detection by
rigorously exploring the information-geometric properties of the feature space on various depths of
a DNN. IGEOOD provides a ﬂexible framework that applies to any pre-trained softmax neural clas-
siﬁer. A key ingredient of IGEOOD is the Fisher-Rao distance. This distance is used as an effective
differential geometry tool for clustering and as a distance in the context of multivariate Gaussian
pdfs (Pinele et al., 2020; Strapasson et al., 2016). In our context, we measure the dissimilarity be-
tween probability distributions (in and out), as the length of the shortest path within the manifold
induced by the underlying class of distributions (i.e., the softmax probabilities of the neural classi-
ﬁer or the densities modeling the learned representations across the layers). By doing so, we can
explore statistical invariances of the geometric properties of the learned features (Bronstein et al.,
2021). Our method adapts to the various scenarios depending on the level of information access of
the DNN and uses only in-distribution samples but can also beneﬁt (if available) from OOD samples."
CONTRIBUTIONS,0.015410958904109588,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.017123287671232876,Our work investigates the problem of OOD detection and advances state-of-the-art in different ways.
CONTRIBUTIONS,0.018835616438356163,"i To the best of our knowledge, this is the ﬁrst work studying information geometry tools to devise a
uniﬁed metric for OOD detection. We derive an explicit characterization of the Fisher-Rao distance
based on the information-geometric properties of the softmax probabilities of the neural classiﬁer
and the class of multivariate Gaussian pdfs. In general terms, our Fisher-Rao-based metric measures
the mismatch–in the geometry space–between the probability density functions of the pre-trained
DNN classiﬁer conditioned on test and in-distribution samples. Section 3 details IGEOOD."
CONTRIBUTIONS,0.02054794520547945,"ii Experiments on BLACK-BOX and GREY-BOX setups using various datasets, architectures, and
classiﬁcation tasks show that IGEOOD is competitive with state-of-the-art methods. In the BLACK-
BOX setup, we assume that only the outputs, i.e., the logits of the DNN, are available. In the
GREY-BOX setup, we allow access to all parameters of the network; however, the detection must
be performed using only the output softmax probabilities. The latter permits input pre-processing
which introduces a small (additive) noise in the direction of the gradients w.r.t the test sample. This
pre-processing allows for further discrimination between in- and out-of-distribution samples. Our
benchmark contains two DNN architectures, three in-distribution datasets, and nine OOD datasets."
CONTRIBUTIONS,0.02226027397260274,"iii In a WHITE-BOX setting, we combine the logits with the low-level features of the DNN to
leverage further useful statistical information of the encoded in-distribution data. We model the
pre-trained latent representations as a mixture of Gaussian pdfs with a diagonal covariance matrix.
Under this assumption, we derive a conﬁdence score based on the Fisher-Rao distance between con-
ditional pdfs corresponding to the test and the closest in-distribution samples. Experiments based
on various datasets, architectures, and classiﬁcation tasks clearly show consistent improvement of
IGEOOD, achieving new state-of-the-art performance on a couple of benchmarks. In particular, we
increased the average TNR at 95% TPR by 11.2% with tuning on OOD data and by 2.5% with tuning
on adversarial data compared to Lee et al. (2018)."
RELATED WORKS,0.023972602739726026,"1.2
RELATED WORKS"
RELATED WORKS,0.025684931506849314,"OOD discriminators consist of a binary classiﬁer to distinguish between in- and out-of-distribution
samples. A few works (Shalev et al., 2018; Hendrycks et al., 2019; Bitterwolf et al., 2020; Mohseni
et al., 2020; Winkens et al., 2020; Vyas et al., 2018; Hein et al., 2019) propose retraining the base
(or an auxiliary) model with synthetic or ground truth OOD samples to serve as a classiﬁer and
as an OOD discriminator. Disposing of both OOD and in-distribution samples during training en-"
RELATED WORKS,0.0273972602739726,Published as a conference paper at ICLR 2022
RELATED WORKS,0.02910958904109589,"ables the latent representations to learn the decision boundaries to facilitate OOD detection. These
methods will not be compared to ours in this work, as they entail retraining or modifying the base
neural network by using OOD data to further train parameters. Nagarajan et al. (2021) studies fail-
ure modes of OOD detection methods to better understand how to improve them, especially how
spurious features like the background can vastly degrade detection performance. Lee et al. (2021)
leverage OOD data as a regularization technique to improve the generalization and robustness of
current neural networks. References (Schlegl et al., 2017; Kirichenko et al., 2020; Choi & Jang,
2018; Vernekar et al., 2019; Xiao et al., 2020; Ren et al., 2019; Zhang et al., 2021; Mahmood et al.,
2021; Zhang et al., 2020; Zisselman & Tamar, 2020) study OOD detection in the context of gen-
erative models for density estimation. Open set recognition (Bendale & Boult, 2016), outlier or
anomaly detection (Pimentel et al., 2014), concept drift detection (Quionero-Candela et al., 2009),
and adversarial attacks detection (Goodfellow et al., 2015; Madry et al., 2018) are related topics."
RELATED WORKS,0.030821917808219176,"BLACK-BOX and GREY-BOX scenarios. It is often the case on ML as a service (Ribeiro et al.,
2015) that the model’s parameters knowledge and access are not allowed to the end-user, granting
access only to the computation of the forward and the logits or softmax outputs. The baseline work
(Hendrycks & Gimpel, 2017) for BLACK-BOX techniques simply consider the unscaled maximum
value of the softmax (MSP) as OOD score. In some cases, this conﬁdence score is enough to distin-
guish between in-distribution and out-of-distribution examples, but it also may assign overconﬁdent
values to OOD examples (Hein et al., 2019). ODIN’s (Liang et al., 2018) method has two variations.
The BLACK-BOX variation consists of temperature scaling the softmax outputs. While the GREY-
BOX variation also uses an input pre-processing technique that calculates the gradient of the model
parameters and adds to the input in an adversarial manner for a more effective OOD detection. Hsu
et al. (2020) proposes a variation of ODIN that does not need access to OOD data for validation. Liu
et al. (2020) proposes an energy-based OOD score. They substitute the softmax conﬁdence score
with the free energy function with a temperature parameter without retraining. They also propose a
GREY-BOX variation with posterior processing for improved results. Fine-tuning is done differently
across the literature and should be considered when comparing methodologies."
RELATED WORKS,0.032534246575342464,"WHITE-BOX scenario. This class of OOD detectors has access to all intermediate layer outputs.
Naturally, discriminators have access to more information than the BLACK-BOX or GREY-BOX se-
tups, warranting greater detection capacity. Batch-normalization statistics between layers are used
(Quintanilha et al., 2019) to ﬁt a logistic regression that serves as an OOD detection score. Sas-
try & Oore (2020) proposes high order Gram matrices to perform OOD detection by computing
class-conditional pairwise feature correlations between the test sample and the training set across
the hidden layers of the network. Lee et al. (2018) assume that latent features of DNN models
trained under the softmax score follow a class-conditional Gaussian mixture distribution with tied
covariance matrix and different class-conditional mean vectors. They calculate the Mahalanobis
distance between a test sample as a single estimator of the mean of a class-conditional Gaussian
distribution with a tied covariance matrix estimated on the training set. The importance of each low-
level component and hyperparameters are tuned using validation data. Ren et al. (2021) modiﬁes
this method to improve detection of near-OOD data. They ﬁt the layer-wise background distribution
with a Gaussian distribution ﬁt from the training set. They subtract the Mahalanobis distance be-
tween the test example and this distribution from the score proposed in Lee et al. (2018), reducing
the importance of features shared by in- and out-of-distribution data."
BACKGROUND,0.03424657534246575,"2
BACKGROUND"
BACKGROUND,0.03595890410958904,"Let X ⊆Rd be the feature space (continuous) and Y a label space. Moreover, let pXY be the un-
derlying unknown probability density function (pdf) over X × Y. We deﬁne the in-distribution
training dataset as DN ≜

(xi, yi)
	N
i=1 ∼pXY , where xi ∈X is the input feature data,
yi ∈Y ≜{1, . . . , C} is the output class among C possible classes and N denotes the number
of training samples. The training dataset is characterized by the joint pdf pXY with in-distribution
marginals X ∼pX and Y ∼PY . The predictor denoted by fDN : X →Y is based on the
inferred model PbY |X, i.e., fDN (x) ≡fn(x; DN) ≜arg maxy∈Y PbY |X(y|x; DN). In order to
model the underlying problem, we introduce an artiﬁcial binary random variable Z ∈{0, 1} indi-
cating with z = 1 that the test sample x is OOD and otherwise, it is in-distribution. The open-
world data can then be modeled as a mixture distribution pX|Z deﬁned by pX|Z(x|z = 0) ≜"
BACKGROUND,0.03767123287671233,Published as a conference paper at ICLR 2022
BACKGROUND,0.039383561643835614,"(a) Contour lines.
(b) Synthetic data distribution.
(c) OOD detection score histogram."
BACKGROUND,0.0410958904109589,"Figure 1: Example comparing Fisher-Rao with Mahalanobis distances to distinguish between 1D
Gaussian distributions, showcasing the motivation to use of Fisher-Rao metric for OOD detection."
BACKGROUND,0.04280821917808219,"pX(x), and pX|Z(x|z = 1) ≜qX(x). The intrinsic difﬁculty arises from the fact that very lit-
tle can be assumed about the unknown distributions pX and qX, in particular for out-of-distribution."
BACKGROUND,0.04452054794520548,"3
IGEOOD: OOD DETECTION USING THE FISHER-RAO DISTANCE"
BACKGROUND,0.046232876712328765,"This section introduces IGEOOD, a ﬂexible framework for OOD detection. IGEOOD is implemented
in two ways: at the level of the logits using temperature scaling (Section 3.2), which mitigates the
high-conﬁdence scores assigned to OOD examples, and layer-wise level (Section 3.3). The key
ingredient of IGEOOD is the Fisher-Rao distance that allows for effective differentiation between
in-distribution and out-of-distribution samples. This distance measures the dissimilarity between
two probability models within a class of probability distributions by calculating the geodesic dis-
tance between two points on the learned manifold. This measure connects information geometry
and differential geometry through the R. Fisher information matrix (Fisher, 1922). Closed-form ex-
pressions of this distance are known to multivariate normal distributions under certain assumptions,
among others distributions (Pinele et al., 2020)."
MOTIVATION FOR THE USE OF THE FISHER-RAO DISTANCE FOR OOD DETECTION,0.04794520547945205,"3.1
MOTIVATION FOR THE USE OF THE FISHER-RAO DISTANCE FOR OOD DETECTION"
MOTIVATION FOR THE USE OF THE FISHER-RAO DISTANCE FOR OOD DETECTION,0.04965753424657534,"We introduce a simple example to demonstrate conceptually how Fisher-Rao distance is instrumental
to OOD detection. It should be noted that this example is limited to one dimension. However, we
expect similar behavior with more complex data under the Gaussianity assumptions."
MOTIVATION FOR THE USE OF THE FISHER-RAO DISTANCE FOR OOD DETECTION,0.05136986301369863,"Consider the case where we try to distinguish between samples from distinct Gaussian distributions
on 1D. Assume that the in-distribution data follows a Gaussian N(µ1, σ1) while OOD data is drawn
according to either N(µ2, σ1) or N(µ2, σ2). These distributions are illustrated in Figures 1a and
1b. In this setup, distance-based approaches which are invariant to the variance of the distributions
would have the performance limited to the information given by the difference between the means
of the underlying distributions. For instance, in the case of the Mahalanobis distance, we would rely
our discrimination on the difference between the sample and the in-distribution mean, rescaled by
the in-distribution standard deviation only, but nothing further could be obtained. However, if we
can estimate OOD standard deviations from actual or pseudo OOD data, we expect the Fisher-Rao
distance between Gaussian distributions to be more effective in distinguishing between distributions.
Figure 1c shows that the Fisher-Rao distance distinguishes better between “In-dist.” and “OOD II”
samples, while the other distances fail."
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.053082191780821915,"3.2
IGEOOD SCORE USING THE SOFTMAX PROBABILITY"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.0547945205479452,"The Fisher-Rao distance (Atkinson & Mitchell, 1981) takes as input two probability distributions.
For the classiﬁcation problem, we can take the temperature T scaled softmax function (Eq. (1)) as"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.05650684931506849,Published as a conference paper at ICLR 2022
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.05821917808219178,an approximation of a class-conditional probability distribution:
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.059931506849315065,"qθ (y|f(x); T) ≜
exp (fy(x)/T)
P"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.06164383561643835,"y′∈Y exp (fy′(x)/T),
(1)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.06335616438356165,"where f : X →RC is a vectorial function with f ≜
 
f1, f2, . . . , fC

and fy(·) denotes the
y-th logits output value of the DNN classiﬁer. The Fisher-Rao distance dFR−Logits between two
distributions resulting from the softmax probability evaluated at two data points is (see Appendix A):"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.06506849315068493,"dFR−Logits
 
qθ(·|f(x)), qθ(·|f(x′))

≜2 arccos  X y∈Y q"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.06678082191780822,"qθ
 
y|f(x)

qθ
 
y|f(x′)

"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.0684931506849315,".
(2)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.0702054794520548,"Class conditional centroid estimation. We model the training dataset class-conditional posterior
distribution by calculating the centroid of the logits representations of this set. Precisely, we compute
the empirical centroid for the logits of each class y ∈Y = {1, . . . , C} of the in-distribution training
dataset DN corresponding to the Fisher-Rao distance, i.e.,"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.07191780821917808,"µy ≜min
µ∈RC
1
Ny X"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.07363013698630137,"∀i : yi=y
dFR−Logits
 
qθ(·|f(xi)), qθ(·|µ)

,
(3)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.07534246575342465,"where Ny is the amount of training examples with label y. We optimize this expression ofﬂine using
SGD algorithm, where the parameter to be tuned is µ in the logits space. This is equivalent to ﬁnding
the centroid of a cluster using the Fisher-Rao distance, after each example has been assigned to a
cluster. Please refer to the appendix (see Section B) for further details on this optimization."
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.07705479452054795,"OOD and conﬁdence score. Using the softmax probability, we can deﬁne a conﬁdence score to
be the minimum of the Fisher-Rao distance between f(x) and the class-conditional centroids. As a
sanity check, we show empirically in the appendix (see Section C) that this conﬁdence score does
not degrade the in-distribution test classiﬁcation accuracy. Thus, the estimated class byFR follows as:"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.07876712328767123,"byFR(x) ≜arg min
y∈Y dFR−Logits
 
qθ(·|f(x)), qθ(·|µy)

.
(4)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.08047945205479452,"However, we obtained slightly better OOD detection performance by using Eq. (5) instead of the
minimal value. A likely explanation would be that this metric uses extra information from the other
logits dimensions. We provide an empirical study comparing both methods in the appendix (see
Section E.1). Thus, we propose the Fisher-Rao distance-based OOD detection score FR0(x) for the
logits to be the sum of the distances between f(x) and each individual class conditional centroid µy
given by Eq. (3). By taking the sum instead of the minimal distance, we leverage useful information
related to the example’s conﬁdence score for each class y. We denote it by"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.0821917808219178,"FR0(x) ≜
X"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.0839041095890411,"y∈Y
dFR−Logits
 
qθ(·|f(x)), qθ(·|µy)

.
(5)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.08561643835616438,"Input pre-processing. In consonance with the literature (Liang et al., 2018; Liu et al., 2020; Lee
et al., 2018), we also perform input pre-processing to enhance the detection between in-distribution
and OOD samples and potentially improve OOD detection performance for the GREY-BOX dis-
criminator. We add small magnitude perturbations ε in a Fast Gradient-Sign Method-style (FGSM)
(Goodfellow et al., 2015) to each test sample x to increase the proposed metric, that is:"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.08732876712328767,"ex = x + ε ⊙sign

∇xFR0(x)

.
(6)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.08904109589041095,"The OOD detector. The detector consists of a threshold-based function for discriminating between
in-distribution and OOD data. This threshold δ and parameters are set so that the true positive rate,
i.e., the in-distribution samples correctly classiﬁed as in-distribution, becomes 95%. Mathemati-
cally, the BLACK-BOX OOD detector gBB and the GREY-BOX OOD detector gGB writes:"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.09075342465753425,"gBB(x; δ, T) =

1
if FR0 (x) ≤δ
0
if FR0 (x) > δ
and gGB(ex; δ, T, ε) =

1
if FR0 (ex) ≤δ
0
if FR0 (ex) > δ
.
(7)"
IGEOOD SCORE USING THE SOFTMAX PROBABILITY,0.09246575342465753,Published as a conference paper at ICLR 2022
IGEOOD SCORE LEVERAGING LATENT FEATURES,0.09417808219178082,"3.3
IGEOOD SCORE LEVERAGING LATENT FEATURES"
IGEOOD SCORE LEVERAGING LATENT FEATURES,0.0958904109589041,"For each layer, we deﬁne a set of class-conditional Gaussian distributions with diagonal standard
deviation matrix σ(ℓ) and class-conditional mean µ(ℓ)
y , where y ∈{1, . . . , C} and ℓis the index of
the latent feature. We compute the empirical estimates of these parameters according to"
IGEOOD SCORE LEVERAGING LATENT FEATURES,0.0976027397260274,"µ(ℓ)
y
= 1 Ny X"
IGEOOD SCORE LEVERAGING LATENT FEATURES,0.09931506849315068,"∀i : yi=y
f (ℓ) (xi) ,
and σ(ℓ) = diag  
s"
N,0.10102739726027397,"1
N X y∈Y X"
N,0.10273972602739725,∀i : yi=y
N,0.10445205479452055,"
f (ℓ)
j
(xi) −µ(ℓ)
y,j
2
"
N,0.10616438356164383,", (8)"
N,0.10787671232876712,"where j ∈{1, . . . , k}, k is the size of feature ℓ, and f (ℓ)(·) is the output of the network for feature
ℓ. The Fisher-Rao distance ρFR between two arbitrary univariate Gaussian pdfs N(µ1, σ2
1) and
N(µ2, σ2
2) is given by (See Section A)"
N,0.1095890410958904,"ρFR ((µ1, σ1) , (µ2, σ2)) =
√"
LOG,0.1113013698630137,2 log
LOG,0.11301369863013698,"
µ1
√"
LOG,0.11472602739726027,"2, σ1

−

µ2
√"
LOG,0.11643835616438356,"2, −σ2
 +


µ1
√"
LOG,0.11815068493150685,"2, σ1

−

µ2
√"
LOG,0.11986301369863013,"2, σ2



µ1
√"
LOG,0.12157534246575342,"2, σ1

−

µ2
√"
LOG,0.1232876712328767,"2, −σ2
 −


µ1
√"
LOG,0.125,"2, σ1

−

µ2
√"
LOG,0.1267123287671233,"2, σ2

.
(9)"
LOG,0.1284246575342466,"Similarly, the Fisher-Rao distance dFR−Gauss between two multivariate Gaussian pdfs with diagonal
standard deviation matrix is derived from the univariate case and is given by"
LOG,0.13013698630136986,"dFR−Gauss
 
(µ, σ), (µ′, σ′)

="
LOG,0.13184931506849315,"v
u
u
t k
X"
LOG,0.13356164383561644,"i=1
ρFR
 
(µi, σi,i) ,
 
µ′
i, σ′
i,i
2,
(10)"
LOG,0.13527397260273974,"where k is the cardinality of the distributions N(µ, σ) and N(µ′, σ′), µi is the i-th component of
the vector µ, and σi,i is the entry with index (i, i) of the standard deviation matrix σ."
LOG,0.136986301369863,"Experimental support for a diagonal Gaussian mixture model. It is known that intermediate fea-
tures of a DNN can be valuable for detecting abnormal samples as demonstrated by Lee et al. (2018).
Nonetheless, we observed that the latent features covariance matrices are often ill-conditioned and
are diagonal dominant. In other words, the condition number of the covariance matrix often di-
verges, and the magnitude of the diagonal entry in a row is greater than or equal to the sum of all
the other entries in that row for most rows. Thus, a diagonal covariance matrix will be a favorable
compromise for OOD detection. See Appendix, Section B.3 for further details."
LOG,0.1386986301369863,"Fisher-Rao distance-based feature-wise conﬁdence score. We derive a conﬁdence score by ap-
plying the Fisher-Rao distance between the test sample x and the closest class-conditional diagonal
Gaussian distribution. Contrarily to the logits, taking the sum did not improve results, so we kept the
minimal distance. We can consider two scenarios: (i) We do not have access to any validation OOD
data whatsoever. In this case, the natural choice is to model the test samples as Gaussian distribution
with the same diagonal standard deviation as the learned representation, i.e.,"
LOG,0.1404109589041096,"FRℓ(x) = min
y∈Y dFR−Gauss
 
(x, σ(ℓ)), (µ(ℓ)
y , σ(ℓ))

;
(11)"
LOG,0.1421232876712329,"and (ii) we dispose of a validation OOD dataset on which the features’ diagonal standard deviation
matrices σ′(ℓ) and the means µ′(ℓ) can be estimated, as well as the quantity:"
LOG,0.14383561643835616,"FR′
ℓ(x) = min
y∈Y dFR−Gauss
 
(x, σ(ℓ)), (µ′(ℓ), σ′(ℓ))

.
(12)"
LOG,0.14554794520547945,"This validation dataset could be obtained from a synthetic dataset, a dataset different from the
testing one, or even by adversarially creating OOD data by attacking the classiﬁer model on the
training dataset. In the appendix (Section B), we include pseudo-codes for calculating the IGEOOD
score for the BLACK-BOX, GREY-BOX, and WHITE-BOX settings."
LOG,0.14726027397260275,"Feature ensemble. To further improve performance, we combine the conﬁdence scores of the logits
and the ones from the low-level features through a linear combination. Similarly to the strategy in
Lee et al. (2018), we choose the weights α0, αℓand α′
ℓ∈R by training a logistic regression detector
using validation samples. Thus, we ensure that the metric emphasizes features that demonstrate a
greater capacity for detecting abnormal samples. IGEOOD score for the WHITE-BOX setting is:"
LOG,0.14897260273972604,"FR(x) ≜α0FR0(x) +
X"
LOG,0.1506849315068493,"ℓ
αℓ· FRℓ(x) + α′
ℓ· FR′
ℓ(x),
(13)"
LOG,0.1523972602739726,Published as a conference paper at ICLR 2022
LOG,0.1541095890410959,"where FR0 is given by equation (5), FRℓis given by equation (11) and FR′ considers a different
validation diagonal covariance matrix for the test samples (equation (12)). We also apply input
pre-processing similarly to the GREY-BOX setting (equation (6)), obtaining FR(ex) as ﬁnal score."
LOG,0.1558219178082192,"Uniﬁed metric. For the three settings, the metric is the same but has different formulations given
the family of the distributions. For the DNN outputs, we use the softmax posterior probability
distribution formulation. For the intermediate layers, it is under the model of diagonal Gaussian pdfs.
Therefore, we have derived a uniﬁed OOD detection framework that combines a single distance
for both the softmax outputs and the latent features of a neural network. Figure 2 illustrates how
each of the presented techniques contributes towards separating in-distribution and OOD samples.
Additional histograms of the detection scores are relegated to the appendix (see Section F)."
LOG,0.15753424657534246,"Figure 2: Probability distributions of the IGEOOD score under three different settings for a pre-
trained DenseNet on CIFAR-10 for in-distribution and OOD data (TinyImageNet downsampled)."
EXPERIMENTAL RESULTS,0.15924657534246575,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.16095890410958905,"We show the effectiveness of IGEOOD comparing to state-of-the-art methods. Details about the
experimental setup 1 and additional results are given in appendices (see Sections C, D, and E)."
SETUP,0.16267123287671234,"4.1
SETUP"
SETUP,0.1643835616438356,"The experimental setup follows the setting established by Hendrycks & Gimpel (2017), Liang et al.
(2018) and Lee et al. (2018). We use two pre-trained deep neural networks architectures for image
classiﬁcation tasks: a Dense Convolutional Network (DenseNet-BC-100) (Huang et al., 2017) and
a Residual Neural Network (ResNet-34) (He et al., 2016). We take as in-distribution data images
from CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 and SVHN (Netzer et al., 2011) datasets."
SETUP,0.1660958904109589,"For out-of-distribution data, we use natural image examples from the datasets: Tiny-ImageNet (Le &
Yang, 2015), LSUN (Yu et al., 2015), Describable Textures Dataset (Cimpoi et al., 2014), Chars74K
(de Campos et al., 2009), Places365 (Zhou et al., 2017), iSUN (Xu et al., 2015) and a synthetic
dataset generated from Gaussian noise. For models pre-trained on CIFAR-10, data from CIFAR-100
and SVHN are also considered OOD; for models pre-trained on CIFAR-100, data from CIFAR-10
and SVHN are considered OOD, and for models pre-trained on SVHN, CIFAR-10 and CIFAR-
100 datasets are considered OOD. We resize the images to dimension 32 × 32 by downsampling
and applying center crop when needed. We only use test data for evaluation. Even though we ran
experiments with image data, IGEOOD could be applied to any neural-based classiﬁcation task."
SETUP,0.1678082191780822,"We measure the effectiveness of the OOD detectors with three standard evaluation metrics: (i) The
true negative rate at 95% true positive rate (TNR at TPR-95%); (ii) the area under the receiving
operating curve (AUROC); and (iii) the area under the precision-recall curve (AUPR). We use the
scores over the test set of in-distribution and OOD datasets to calculate them. For the BLACK-BOX
and GREY-BOX experimental settings, we tune hyperparameters for all of the OOD detectors only
based on the DNN classiﬁer architecture, the in-distribution dataset, and a validation dataset. The
iSUN (Xu et al., 2015) dataset is chosen as a source of OOD validation data, independently from
OOD test data. We choose the parameters that maximize the TNR at TPR-95% on the validation
OOD dataset. For the WHITE-BOX framework, we allow both the benchmark and our method to"
SETUP,0.1695205479452055,1Our code is publicly available at https://github.com/edadaltocg/Igeood.
SETUP,0.17123287671232876,Published as a conference paper at ICLR 2022
SETUP,0.17294520547945205,"tune either on adversarially generated data from in-distribution training samples or a separate vali-
dation dataset containing 1, 000 images from the OOD test dataset with feature ensemble described
in Section 3.3. In this case, we evaluate performance on the remaining test samples."
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.17465753424657535,"4.2
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS"
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.17636986301369864,"For comparing IGEOOD under the hypothesis of a BLACK-BOX scenario, we consider the Baseline
(Hendrycks & Gimpel, 2017) method, ODIN (Liang et al., 2018) with temperature scaling only,
and the free-energy-based metric (Liu et al., 2020) with temperature scaling only. The results for
the BLACK-BOX setting are available in Table 1, where we show the average and one standard
deviation OOD detection performance for each of the eight OOD detection method in six different
image classiﬁcation contexts (couple DNN model and in-distribution dataset). The extended results
for each OOD dataset can be found in Table 13. For comparison under the GREY-BOX assumption,
we consider ODIN and the free-energy-based methods, both with input pre-processing. The results
for the GREY-BOX setup are provided in the appendix (see Section E and Table 10). For the BLACK-
BOX setting, IGEOOD slight improves the benchmark by less than 1% in TNR at TPR-95%. While
for the GREY-BOX setting, results show IGEOOD is outperformed by <1% in a few benchmarks by
ODIN, which is greatly improved by input pre-processing techniques."
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.1780821917808219,"Table 1: Average and standard deviation OOD detection performance across eight OOD datasets for
each model and in-distribution dataset in a BLACK-BOX setting. IGEOOD is compared to Baseline
(Hendrycks & Gimpel, 2017), ODIN (Liang et al., 2018), and Energy (Liu et al., 2020) methods.
The extended results can be found in Table 13 in the appendix."
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.1797945205479452,"TNR at TPR-95%
AUROC
Model
In-dist.
Baseline / ODIN / Energy / IGEOOD (ours)"
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.1815068493150685,DenseNet
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.1832191780821918,"C-10
52.5±16/66.8±20/65.3±23/65.6±23
91.8±3.2/92.8±4.6/92.1±5.3/92.3±5.1
C-100
15.9±6.8/20.5±9.5/20.3±9.6/20.7±9.8
69.1±15/71.6±20/71.6±20/73.2±17
SVHN
68.4±14/68.8±20/70.2±17/72.1±15
92.3±4.0/87.3±14/90.1±5.9/90.9±5.3"
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.18493150684931506,ResNet
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.18664383561643835,"C-10
41.7±16/51.9±15/56.3±13/56.7±13
89.6±3.1/90.4±3.1/90.4±3.0/90.5±3.0
C-100
15.0±5.5/16.0±6.3/16.3±7.1/16.4±6.8
74.0±1.9/75.2±1.7/75.5±1.9/75.5±1.7
SVHN
76.2±7.8/77.7±7.9/78.0±7.9/78.3±8.0
92.2±2.9/91.4±3.2/91.4±3.2/91.7±3.2
Average and Std.
44.9±24/50.3±24/51.1±24/51.6±24
84.8±9.5/84.8±8.3/85.2±8.4/85.7±8.0"
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.18835616438356165,"Temperature scaling and input pre-processing. We observed that low values of temperature and
moderate noise magnitude yield better detection performance for IGEOOD on the logits. For most
models and datasets, we obtained better results for temperatures between 1 and 6 and noise magni-
tudes below 0.002. Detailed results and the best hyperparameters found for each conﬁguration, as
well as ﬁgures of their impact on performance, are delegated to the appendix (see Section E)."
RESULTS FOR THE BLACK-BOX AND THE GREY-BOX SETUPS,0.19006849315068494,"How the choice of validation dataset impacts performance. We include in the appendix (see
Section E) the average OOD detection performance for each method when we change the validation
set among the nine available ones. We show that the average TNR at TPR-95% for IGEOOD ranges
between 63% and 72% on a BLACK-BOX scenario and between 65% and 74% on a GREY-BOX
scenario. The performances among the compared methods are consistent across validation datasets."
RESULTS FOR THE WHITE-BOX SETTING,0.1917808219178082,"4.3
RESULTS FOR THE WHITE-BOX SETTING"
RESULTS FOR THE WHITE-BOX SETTING,0.1934931506849315,"For benchmarking IGEOOD on the WHITE-BOX setting, we compare results to the Mahalanobis
(Lee et al., 2018) method with input pre-processing and feature ensemble. For both of them, we
extract features from every output of the dense (or residual) block of the DenseNet (or ResNet)
model and the ﬁrst convolutional layer. The size of each feature is reduced by average pooling in
the spatial dimensions. Thus, the initial dimension Fℓ× Wℓ× Hℓis reduced to Fℓ, where Fℓ
is the number of channels in block ℓ. For DenseNet, this reduction translates to features of sizes
F1 = {24, 108, 150, 342}; and for ResNet, to features of sizes F2 = {64, 64, 128, 256, 512}."
RESULTS FOR THE WHITE-BOX SETTING,0.1952054794520548,"We consider two scenarios for tuning hyperparameters for both Mahalanobis and IGEOOD: one with
adversarially generated (FGSM) and in-distribution data and another one with 1,000 OOD samples"
RESULTS FOR THE WHITE-BOX SETTING,0.1969178082191781,Published as a conference paper at ICLR 2022
RESULTS FOR THE WHITE-BOX SETTING,0.19863013698630136,"and in-distribution data. We derive two methods: IGEOOD+, which is given by equation (13) and
considers that we can calculate the statistics from OOD data as additional information; and IGEOOD,
which doesn’t consider any prior on OOD data, i.e., set α′
ℓ= 0 on equation (13)."
RESULTS FOR THE WHITE-BOX SETTING,0.20034246575342465,"Comparison with current literature. For each DNN model and in-distribution dataset pair, we
report the average and one standard deviation OOD detection performance for Mahalanobis (Lee
et al., 2018), IGEOOD and IGEOOD+. Table 2 validates the contributions of our techniques. We
observe substantial performance improvement in all experiments for the left-hand side of the table,
where we outperform Mahalanobis on average for all test cases. IGEOOD+ show improvements
of at least 2.1% up to 23% on TNR at TPR-95%. Since the results are usually above 90%, these
improvements are signiﬁcant. To assess the consistency of IGEOOD to the choice of validation
data, we measured the detection performance when all hyperparameters are tuned only using in-
distribution and generated adversarial data, as observed in the right-hand side of Table 2. IGEOOD
record improvements up to 10.5%, and improves by 2.5% the average TNR at TPR-95% across all
datasets and models. We provide an extra benchmark against other WHITE-BOX methods (Sastry &
Oore, 2020; Hsu et al., 2020; Zisselman & Tamar, 2020) (see Table 11 in the appendix)."
RESULTS FOR THE WHITE-BOX SETTING,0.20205479452054795,"Table 2: Average and standard deviation OOD detection performance for the WHITE-BOX settings.
The abbreviation TNR-95%, C-10 and C-100 stands for TNR at TPR-95%, CIFAR-10 and CIFAR-
100, respectively. The extended results can be found in Tables 15 and 16 in the appendix."
RESULTS FOR THE WHITE-BOX SETTING,0.20376712328767124,"Validation on OOD data
Validation on adversarial data
TNR-95%
AUROC
TNR-95%
AUROC
Model
In-dist.
Mahalanobis / IGEOOD+ (ours)
Mahalanobis / IGEOOD (ours)"
RESULTS FOR THE WHITE-BOX SETTING,0.2054794520547945,DenseNet
RESULTS FOR THE WHITE-BOX SETTING,0.2071917808219178,"C-10
76.6±31/92.6±14
92.1±12/98.4±3.0
75.9±30/77.9±29
91.7±12/94.0±9.0
C-100
67.2±28/90.2±21
90.2±13/97.7±5.0
60.4±34/70.9±35
85.3±19/90.8±13
SVHN
93.3±8.0/98.0±2.0
98.6±1.0/99.6±0.1
93.7±10/92.2±9.0
98.6±2.0/98.4±1.0"
RESULTS FOR THE WHITE-BOX SETTING,0.2089041095890411,ResNet
RESULTS FOR THE WHITE-BOX SETTING,0.2106164383561644,"C-10
82.5±23/91.6±16
96.5±4.0/98.4±3.0
78.6±24/77.3±32
95.3±6.0/90.0±15
C-100
70.4±30/86.4±23
91.9±10/97.1±5.0
57.4±36/65.1±33
86.9±13/88.6±15
SVHN
96.8±6.0/98.9±2.0
99.2±1.0/99.7±0.1
96.3±8.0/93.6±14
99.1±1.0/98.4±3.0
Average and Std.
81.1±11/92.9±4.0
94.8±4.0/98.5±1.0
77.0±15/79.5±10
92.8±5.4/93.4±3.9"
RESULTS FOR THE WHITE-BOX SETTING,0.21232876712328766,"Ablation study. IGEOOD has three components, FR0, FRℓ, and FR′
ℓ, that together compose the
ﬁnal metric of equation (13). The outputs of the network provide limited OOD detection capacity
as observed in Table 1. When available, the intermediate features, i.e., FRℓ, are a valuable resource
for OOD detection. Moreover, when few reliable OOD data are available, calculating FR′
ℓcan
further improve the detection performance (left-hand side column of Table 2). Also, data from a
source other than in-distribution, e.g., adversarial samples, is enough for tuning hyperparameters
and combining features (right-hand side column of Table 2). The detection capacity of each hidden
layer before any tuning is studied in Appendix B.4. Experiments show that the Fisher-Rao metric
effectively separates in- and out-of-distribution data for each of the features individually as well."
SUMMARY AND CONCLUDING REMARKS,0.21404109589041095,"5
SUMMARY AND CONCLUDING REMARKS"
SUMMARY AND CONCLUDING REMARKS,0.21575342465753425,"This paper introduces IGEOOD, an effective and ﬂexible method for OOD detection that applies to
any pre-trained neural network. The main feature of IGEOOD relies on the geodesic distance of
the probabilistic manifold of the learned latent representations that induces an effective measure
for OOD detection. First, in a (GREY-) BLACK-BOX setup, we calculate the sum of the Fisher-
Rao distance between the softmax output, corresponding to the test (pre-processed) sample, and a
reference probability, corresponding to the conditional-class of softmax probabilities. Similarly, in a
WHITE-BOX setup, we model the low-level features of a DNN as a diagonal Gaussian mixture. The
Fisher-Rao distance between the pdf of the latent feature, corresponding to the test sample, and a
reference pdf, corresponding to the conditional-class of pdfs, provides an effective conﬁdence score.
We considered diverse testing environments where prior knowledge of OOD data may or may not
be available, reﬂecting diverse application scenarios. It is observed that IGEOOD signiﬁcantly and
consistently improves the accuracy of OOD detection on several DNN architectures across various
datasets for a WHITE-BOX setting. Some perspectives for future work include studying causal
factors, explainable components for OOD detection, and extensions to textual data."
SUMMARY AND CONCLUDING REMARKS,0.21746575342465754,Published as a conference paper at ICLR 2022
SUMMARY AND CONCLUDING REMARKS,0.2191780821917808,ACKNOWLEDGMENTS
SUMMARY AND CONCLUDING REMARKS,0.2208904109589041,This work has been supported by the project PSPC AIDA: 2019-PSPC-09 funded by BPI-France.
REFERENCES,0.2226027397260274,REFERENCES
REFERENCES,0.2243150684931507,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http://arxiv.org/
abs/1606.06565."
REFERENCES,0.22602739726027396,"Colin Atkinson and Ann F. S. Mitchell. Rao’s distance measure. Sankhy¯a: The Indian Journal of
Statistics, Series A (1961-2002), 43(3):345–365, 1981. ISSN 0581572X. URL http://www.
jstor.org/stable/25050283."
REFERENCES,0.22773972602739725,"Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 1563–1572, 2016. doi: 10.1109/CVPR.
2016.173."
REFERENCES,0.22945205479452055,"Julian Bitterwolf, Alexander Meinke, and Matthias Hein. Certiﬁably adversarially robust detection
of out-of-distribution data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 16085–16095. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/b90c46963248e6d7aab1e0f429743ca0-Paper.pdf."
REFERENCES,0.23116438356164384,"Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges, 2021."
REFERENCES,0.2328767123287671,"Hyun-Jae Choi and Eric Jang.
Generative ensembles for robust anomaly detection.
ArXiv,
abs/1810.01392, 2018."
REFERENCES,0.2345890410958904,"M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014."
REFERENCES,0.2363013698630137,"T. E. de Campos, B. R. Babu, and M. Varma. Character recognition in natural images. In Pro-
ceedings of the International Conference on Computer Vision Theory and Applications, Lisbon,
Portugal, February 2009."
REFERENCES,0.238013698630137,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009."
REFERENCES,0.23972602739726026,"R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions
of the Royal Society of London, A, 222:309–368, 1922."
REFERENCES,0.24143835616438356,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572."
REFERENCES,0.24315068493150685,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778, 2016. doi: 10.1109/CVPR.2016.90."
REFERENCES,0.24486301369863014,"Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
conﬁdence predictions far away from the training data and how to mitigate the problem. 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 41–50, 2019."
REFERENCES,0.2465753424657534,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In International Conference on Learning Representations, 2017."
REFERENCES,0.2482876712328767,"Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=HyxCxhRcY7."
REFERENCES,0.25,Published as a conference paper at ICLR 2022
REFERENCES,0.2517123287671233,"Jens Henriksson, Christian Berger, Markus Borg, Lars Tornberg, Sankar Raman Sathyamoorthy,
and Cristofer Englund. Performance analysis of out-of-distribution detection on trained neural
networks. Information and Software Technology, 130:106409, 2021. ISSN 0950-5849. doi:
https://doi.org/10.1016/j.infsof.2020.106409. URL https://www.sciencedirect.com/
science/article/pii/S0950584919302204."
REFERENCES,0.2534246575342466,"Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-
distribution image without learning from out-of-distribution data. 2020 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 10948–10957, 2020."
REFERENCES,0.2551369863013699,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2261–2269, 2017. doi: 10.1109/CVPR.2017.243."
REFERENCES,0.2568493150684932,"Polina Kirichenko, Pavel Izmailov, and Andrew G Wilson. Why normalizing ﬂows fail to detect
out-of-distribution data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20578–20589. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf."
REFERENCES,0.2585616438356164,Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.2602739726027397,"Balaji Lakshminarayanan,
Alexander Pritzel,
and Charles Blundell.
Simple and scal-
able predictive uncertainty estimation using deep ensembles.
In I. Guyon,
U. V.
Luxburg,
S.
Bengio,
H.
Wallach,
R.
Fergus,
S.
Vishwanathan,
and
R.
Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf."
REFERENCES,0.261986301369863,Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. 2015.
REFERENCES,0.2636986301369863,"Kimin Lee,
Kibok Lee,
Honglak Lee,
and Jinwoo Shin.
A simple uniﬁed frame-
work for detecting out-of-distribution samples and adversarial attacks.
In S. Ben-
gio,
H.
Wallach,
H.
Larochelle,
K.
Grauman,
N.
Cesa-Bianchi,
and
R.
Garnett
(eds.),
Advances
in
Neural
Information
Processing
Systems
31,
pp.
7167–7177.
Curran
Associates,
Inc.,
2018.
URL
http://papers.nips.cc/paper/
7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adve
pdf."
REFERENCES,0.2654109589041096,"Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee, and Sungroh Yoon. Re-
moving undesirable feature contributions using out-of-distribution data. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
eIHYL6fpbkA."
REFERENCES,0.2671232876712329,"Shiyu Liang, Yixuan Li, and R. Srikant.
Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=H1VGkIxRZ."
REFERENCES,0.2688356164383562,"Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detec-
tion. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.2705479452054795,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb."
REFERENCES,0.2722602739726027,"Prasanta Chandra Mahalanobis. On the generalized distance in statistics. Proceedings of the Na-
tional Institute of Sciences (Calcutta), 2:49–55, 1936."
REFERENCES,0.273972602739726,"Ahsan Mahmood, Junier Oliva, and Martin Andreas Styner. Multiscale score matching for out-of-
distribution detection. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=xoHdgbQJohv."
REFERENCES,0.2756849315068493,Published as a conference paper at ICLR 2022
REFERENCES,0.2773972602739726,"Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for
generalizable out-of-distribution detection. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 34(04):5216–5223, Apr. 2020. doi: 10.1609/aaai.v34i04.5966. URL https://
ojs.aaai.org/index.php/AAAI/article/view/5966."
REFERENCES,0.2791095890410959,"Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
Understanding the failure
modes of out-of-distribution generalization.
In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=fSTD6NFIW_b."
REFERENCES,0.2808219178082192,"Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=H1xwNhCcYm."
REFERENCES,0.2825342465753425,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/
housenumbers/nips2011_housenumbers.pdf."
REFERENCES,0.2842465753424658,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evalu-
ating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf."
REFERENCES,0.285958904109589,"Marine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail Ben Ayed, and Pablo
Piantanida. Adversarial robustness via ﬁsher-rao regularization. ArXiv, abs/2106.06685, 2021."
REFERENCES,0.2876712328767123,"Marco Pimentel, David Clifton, Lei Clifton, and L. Tarassenko. A review of novelty detection.
Signal Processing, 99:215–249, 06 2014. doi: 10.1016/j.sigpro.2013.12.026."
REFERENCES,0.2893835616438356,"Julianna Pinele, Jo˜ao E. Strapasson, and Sueli I. R. Costa. The ﬁsher–rao distance between multi-
variate normal distributions: Special cases, bounds and applications. Entropy, 22(4), 2020. ISSN
1099-4300. doi: 10.3390/e22040404. URL https://www.mdpi.com/1099-4300/22/
4/404."
REFERENCES,0.2910958904109589,"Igor M. Quintanilha, Roberto de M. E. Filho, Jos´e Lezama, Mauricio Delbracio, and Leonardo O.
Nunes. Detecting out-of-distribution samples using low-order deep features statistics, 2019. URL
https://openreview.net/forum?id=rkgpCoRctm."
REFERENCES,0.2928082191780822,"Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence.
Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051."
REFERENCES,0.2945205479452055,"Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua
Dillon, and Balaji Lakshminarayanan.
Likelihood ratios for out-of-distribution detection.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1e79596878b2320cac26dd792a6c51c9-Paper.pdf."
REFERENCES,0.2962328767123288,"Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshmi-
narayanan. A simple ﬁx to mahalanobis distance for improving near-ood detection, 2021."
REFERENCES,0.2979452054794521,"Mauro Ribeiro, Katarina Grolinger, and Miriam A.M. Capretz.
Mlaas: Machine learning as a
service. In 2015 IEEE 14th International Conference on Machine Learning and Applications
(ICMLA), pp. 896–902, 2015. doi: 10.1109/ICMLA.2015.152."
REFERENCES,0.2996575342465753,"Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with Gram
matrices. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Con-
ference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
8491–8501. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
sastry20a.html."
REFERENCES,0.3013698630136986,Published as a conference paper at ICLR 2022
REFERENCES,0.3030821917808219,"Thomas Schlegl, Philipp Seeb¨ock, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, and Georg
Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker
discovery, 2017."
REFERENCES,0.3047945205479452,"Gabi Shalev, Yossi Adi, and Joseph Keshet. Out-of-distribution detection using multiple semantic
label representations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
2151b4c76b4dcb048d06a5c32942b6f6-Paper.pdf."
REFERENCES,0.3065068493150685,"S. S. Shapiro and M. B. Wilk.
An analysis of variance test for normality (complete samples).
Biometrika, 52(3/4):591–611, 1965.
ISSN 00063444.
URL http://www.jstor.org/
stable/2333709."
REFERENCES,0.3082191780821918,"Jo˜ao E. Strapasson, Julianna Pinele, and Sueli I. R. Costa. Clustering using the ﬁsher-rao distance.
In 2016 IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM), pp. 1–5, 2016.
doi: 10.1109/SAM.2016.7569717."
REFERENCES,0.3099315068493151,"Sachin
Vernekar,
Ashish
Gaurav,
Vahdat
Abdelzad,
Taylor
Denouden,
Rick
Salay,
and
Krzysztof
Czarnecki.
Out-of-distribution
detection
in
classiﬁers
via
genera-
tion.
In Neural Information Processing Systems (NeurIPS 2019),
Safety and Ro-
bustness
in
Decision
Making
Workshop.
https://sites.google.com/view/neurips19-
safe-robust-workshop,
https://sites.google.com/view/neurips19-safe-robust-workshop,
12/2019
2019.
URL
https://drive.google.com/file/d/0B3mY6u_
lryzdel9WOW1XTVA0aDIwazJDcG9ORlZrZWFOd0xJ/view."
REFERENCES,0.3116438356164384,"Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L.
Willke.
Out-of-distribution detection using an ensemble of self supervised leave-out clas-
siﬁers.
In ECCV (8), pp. 560–574, 2018.
URL https://doi.org/10.1007/
978-3-030-01237-3_34."
REFERENCES,0.3133561643835616,"Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R. Led-
sam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon A. A. Kohl, tay-
lan. cemgil, S. M. Ali Eslami, and Olaf Ronneberger. Contrastive training for improved out-of-
distribution detection. ArXiv, abs/2007.05566, 2020."
REFERENCES,0.3150684931506849,"J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recog-
nition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. 3485–3492, 2010. doi: 10.1109/CVPR.2010.5539970."
REFERENCES,0.3167808219178082,"Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score
for variational auto-encoder. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20685–20696. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf."
REFERENCES,0.3184931506849315,"Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking, 2015."
REFERENCES,0.3202054794520548,"F. Yu, Y. Zhang, Shuran Song, Ari Seff, and J. Xiao. Lsun: Construction of a large-scale image
dataset using deep learning with humans in the loop. ArXiv, abs/1506.03365, 2015."
REFERENCES,0.3219178082191781,"Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In
ECCV, 2020."
REFERENCES,0.3236301369863014,"Yufeng Zhang, Wanwei Liu, Zhenbang Chen, Ji Wang, Zhiming Liu, Kenli Li, and Hongmei Wei.
Out-of-distribution detection with distance guarantee in deep generative models, 2021."
REFERENCES,0.3253424657534247,"Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil-
lion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2017."
REFERENCES,0.3270547945205479,"Ev Zisselman and Aviv Tamar. Deep residual ﬂow for out of distribution detection. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.3287671232876712,Published as a conference paper at ICLR 2022
REFERENCES,0.3304794520547945,"A
REVIEW OF FISHER-RAO DISTANCE (FRD)"
REFERENCES,0.3321917808219178,"In this section, we review some results from references Atkinson & Mitchell (1981); Pinele et al.
(2020). We intend to clarify some basic concepts surrounding the Fisher-Rao distance while moti-
vating this measure in the context of OOD detection."
REFERENCES,0.3339041095890411,"In few words, the Fisher-Rao’s distance is given by the geodesic distance, i.e., the shortest path
between points in a Riemannian space induced by a parametric family. Consider the family C of
probability distributions over the class of discrete concepts or labels: Y = {1, . . . , C}, denoted by
C ≜

qθ(·|x) : x ∈X ⊆RC	
."
REFERENCES,0.3356164383561644,"We are interested in measuring the distance between probability distributions qθ(·|x) with respect
to the testing input x and a population of inputs drawn accordingly to the in-distribution data set. To
this end, we ﬁrst need to characterize the Fisher-Rao distance for two inputs or for two probability
distributions qθ, q′
θ ∈C."
REFERENCES,0.3373287671232877,"Assume that the following regularity conditions hold (Atkinson & Mitchell, 1981):"
REFERENCES,0.339041095890411,"(i) ∇x qθ(y|x) exists for all x, y and θ ∈Θ;
(ii) P"
REFERENCES,0.3407534246575342,"y∈Y
∇x qθ(y|x) = 0 for all x and θ ∈Θ;"
REFERENCES,0.3424657534246575,"(iii) G(x) = EY ∼qθ(·|x)

∇x log qθ(Y |x)∇⊤
x log qθ(Y |x)

is positive deﬁnite for any x and
θ ∈Θ."
REFERENCES,0.3441780821917808,"Notice that if (i) holds, (ii) also holds immediately for discrete distributions over ﬁnite spaces (as-
suming that P"
REFERENCES,0.3458904109589041,"y∈Y and ∇x are interchangeable operations) as in our case. When (i)-(iii) are met, the
variance of the differential form ∇⊤
x log qθ(Y |x)dx can be interpreted as the square of a differential
arc length ds2 in the space C, which yields"
REFERENCES,0.3476027397260274,"ds2 = ⟨dx, dx⟩G(x) = dx⊤G(x)dx.
(14)"
REFERENCES,0.3493150684931507,"Thus, G, which is the Fisher Information Matrix (FIM), can be adopted as a metric tensor. We now
consider a curve γ : [0, 1] →X connecting a pair of arbitrary points x, x′ in the input space X, i.e.,
γ(0) = x and γ(1) = x′. Notice that any curve γ induces a curve qθ(·|γ(t)) for t ∈[0, 1] in the
space C. The Fisher-Rao distance between the distributions qθ = qθ(·|x) and q′
θ = qθ(·|x′) will be
denoted as dR,C(qθ, q′
θ) and is formally deﬁned by the expression:"
REFERENCES,0.351027397260274,"dR,C(qθ, q′
θ) ≜inf
γ Z 1 0 r"
REFERENCES,0.3527397260273973,dγ⊤(t)
REFERENCES,0.3544520547945205,"dt
G(γ(t))dγ(t)"
REFERENCES,0.3561643835616438,"dt
,
(15)"
REFERENCES,0.3578767123287671,"where the inﬁmum is taken over all piecewise smooth curves. This means that the FRD is the
length of the geodesic between points x and x′ using the FIM as the metric tensor. In general, the
minimization of the functional in equation (15) is a problem that can be solved using the well-known
Euler-Lagrange differential equation."
REFERENCES,0.3595890410958904,"A.1
DERIVATION OF FISHER-RAO DISTANCE FOR THE CLASS OF SOFTMAX PROBABILITY
DISTRIBUTIONS"
REFERENCES,0.3613013698630137,"The direct computation of the FIM of the family C with qθ(y|x) in the form of the softmax probabil-
ity distribution function given by equation (1) can be shown to be singular, i.e., rank(G(x)) ≤C−1,
where C −1 is the number of degrees of freedom of the manifold C. To overcome this issue, we
introduce the probability simplex P deﬁned by P = 
"
REFERENCES,0.363013698630137,"q : Y →[0, 1]C :
X"
REFERENCES,0.3647260273972603,"y∈Y
q(y) = 1 
"
REFERENCES,0.3664383561643836,".
(16)"
REFERENCES,0.3681506849315068,"Next, we consider the following parametrization for any distribution q ∈P:"
REFERENCES,0.3698630136986301,"q(y|z) = z2
y
4 ,
y ∈{1, . . . , C}.
(17)"
REFERENCES,0.3715753424657534,Published as a conference paper at ICLR 2022
REFERENCES,0.3732876712328767,"From
this
expression,
we
consider
the
statistical
manifold
D
=

q(·|z) : ∥z∥2 = 4, zy ≥0, ∀y ∈Y
	
.
Note that the parameter vector z belongs to the posi-
tive portion of a sphere of radius 2 and centered at the origin in RC. The computation of the FIM
for z on D yields:
G(z) = Eq(y|z)

∇z log q(y|z)∇⊤
z log q(y|z)
 =
X y∈Y"
REFERENCES,0.375,"z2
y
4  2 zy
ey   2"
REFERENCES,0.3767123287671233,"zy
e⊤
y  =
X"
REFERENCES,0.3784246575342466,"y∈Y
eye⊤
y = I, (18)"
REFERENCES,0.3801369863013699,"where {ey} are the canonical basis vectors in RC and I is the identity matrix. From equation (18)
we can conclude that the Fisher-Rao metric in this parametric space is equal to the Euclidean metric.
Also, since the parameter vector lies on a sphere, the FRD between the distributions q = q(·|z) and
q′ = q (·|z′) can be written as the radius of the sphere times the angle between the vectors z and z′.
Which leads to expression:"
REFERENCES,0.3818493150684932,"dR,D (q, q′) = 2 arccos
z⊤z′ 4"
REFERENCES,0.3835616438356164,"
= 2 arccos  X y∈Y p"
REFERENCES,0.3852739726027397,q(y|z)q (y|z′) 
REFERENCES,0.386986301369863,".
(19)"
REFERENCES,0.3886986301369863,"Finally, we can compute the FRD for softmax distributions in C as"
REFERENCES,0.3904109589041096,"dFR−Logits (qθ, q′
θ) = 2 arccos  X y∈Y p"
REFERENCES,0.3921232876712329,qθ(y|x)qθ (y|x′) 
REFERENCES,0.3938356164383562,",
(20)"
REFERENCES,0.3955479452054795,"obtaining the same form of equation (2).
Notice that 0 ≤dFR−Logits (qθ, q′
θ) ≤π for all
x, x′ ∈X
⊆RC, being zero when qθ(·|x) = qθ (·|x′) and maximum when the vectors
 
qθ(1|x), . . . , qθ(C|x)

and
 
qθ (1|x′) , . . . , qθ (C|x′)

are orthogonal."
REFERENCES,0.3972602739726027,"A.2
DERIVATION OF FISHER-RAO DISTANCE FOR MULTIVARIATE GAUSSIAN
DISTRIBUTIONS"
REFERENCES,0.398972602739726,"Consider a broader statistical manifold S ≜{pθ = p(x; θ) : θ = (θ1, θ2, . . . , θm) ∈Θ} of multi-
variate differential probability density functions. The Fisher information matrix G(θ) = [gij(θ)] in
this parametric space is provided by:"
REFERENCES,0.4006849315068493,gij(θ) = Eθ  ∂
REFERENCES,0.4023972602739726,"∂θi
log p(x; θ) ∂"
REFERENCES,0.4041095890410959,"∂θj
log p(x; θ)
"
REFERENCES,0.4058219178082192,"=
Z
∂
∂θi
log p(x; θ) ∂"
REFERENCES,0.4075342465753425,"∂θj
log p(x; θ)p(x; θ)dx.
(21)"
REFERENCES,0.4092465753424658,"Next, consider a multivariate Gaussian distribution:"
REFERENCES,0.410958904109589,"p(x; µ, Σ) = (2π)−( n 2 )
p"
REFERENCES,0.4126712328767123,"Det(Σ)
exp

−(x −µ)⊤Σ−1(x −µ) 2"
REFERENCES,0.4143835616438356,"
,
(22)"
REFERENCES,0.4160958904109589,"where x ∈Rk is the variable vector, µ ∈Rk is the mean vector, Σ ∈Pk(R) is the covariance
matrix, and Pk(R) is the space of k positive deﬁnite symmetric matrices. We can deﬁne the statistical
manifold composed by these distributions as M = {pθ; θ = (µ, Σ) ∈Rk×Pk(R)}. By substituting
equation (22) in equation (21), we can derive the Fisher information matrix for this parametrization,
obtaining:"
REFERENCES,0.4178082191780822,gij(θ) = ∂µ⊤
REFERENCES,0.4195205479452055,"∂θi
Σ−1 ∂µ"
REFERENCES,0.4212328767123288,"∂θj
+ 1"
TR,0.4229452054794521,"2 tr

Σ−1 ∂Σ"
TR,0.4246575342465753,"∂θi
Σ−1 ∂Σ ∂θi"
TR,0.4263698630136986,"
,
(23)"
TR,0.4280821917808219,which induces the following square differential arc length in M:
TR,0.4297945205479452,ds2 = dµ⊤Σ−1dµ + 1
TR,0.4315068493150685,"2 tr
h 
Σ−1dΣ
2i
.
(24)"
TR,0.4332191780821918,Published as a conference paper at ICLR 2022
TR,0.4349315068493151,"Here, dµ = (dµ1, . . . , dµn) ∈Rk and dΣ = [dσij] ∈Pk(R). We observe that this metric is
invariant to afﬁne transformations (Pinele et al., 2020), i.e., for any (c, Q) ∈Rk × GLk(R), with
GLk(R) the space of non-singular order k matrices, the map (µ, Σ) 7→
 
Qµ + c, QΣQ⊤
is an
isometry in M. Thus, the Fisher-Rao distance between two multivariate normal distributions with
parameters θ1 = (µ1, Σ1) and θ2 = (µ2, Σ2) in M satisﬁes:"
TR,0.4366438356164384,"dR,M (θ1, θ2) = dR,M
  
Qµ1 + c, QΣ1Q⊤
,
 
Qµ2 + c, QΣ2Q⊤
.
(25)
Unfortunately, a closed-form solution for the Fisher-Rao distance remains unknown. This is still
an open problem for an arbitrary covariance matrix Σ and mean vector µ. Fortunately, the FRD is
known for the univariate case and hence, for the submanifold where Σ is diagonal. Notice that in
this case equation (24) admits an additive form."
TR,0.4383561643835616,"From Pinele et al. (2020), we obtain the analytical expression of the Fisher-Rao in the 2-dimensional
submanifold of univariate Gaussian probability distributions M2 = {pθ : θ = (µ, σ2) ∈R ×
(0, +∞)}:"
TR,0.4400684931506849,"ρFR
  
µ1, σ2
1

,
 
µ2, σ2
2

=
√"
LOG,0.4417808219178082,2 log
LOG,0.4434931506849315,"
µ1
√"
LOG,0.4452054794520548,"2, σ1

−

µ2
√"
LOG,0.4469178082191781,"2, −σ2
 +


µ1
√"
LOG,0.4486301369863014,"2, σ1

−

µ2
√"
LOG,0.4503424657534247,"2, σ2



µ1
√"
LOG,0.4520547945205479,"2, σ1

−

µ2
√"
LOG,0.4537671232876712,"2, −σ2
 −


µ1
√"
LOG,0.4554794520547945,"2, σ1

−

µ2
√"
LOG,0.4571917808219178,"2, σ2

, (26)"
LOG,0.4589041095890411,"where | · | is the Euclidian norm in R2 and σ denotes the standard deviation. Consequently, the FRD
for Gaussian distributions with diagonal covariance matrix Σ = diag
 
σ2
1, σ2
2, . . . , σ2
k

in the 2k-
dimensional statistical submanifold MD =

pθ : θ = (µ, Σ), Σ = diag
 
σ2
1, σ2
2, . . . , σ2
k

, σi >
0, i = 1, . . . , k
	
is"
LOG,0.4606164383561644,"dFR−Gauss (θ1, θ2) ="
LOG,0.4623287671232877,"v
u
u
t k
X"
LOG,0.464041095890411,"i=1
dR,M2

(µ1i, σ1i) , (µ2i, σ2i)
2
.
(27)"
LOG,0.4657534246575342,"A.3
FISHER-RAO VS. MAHALANOBIS DISTANCE"
LOG,0.4674657534246575,"There is an intricate relationship between the FRD for multivariate Gaussian distributions and the
Mahalanobis distance. We borrow the result from Pinele et al. (2020), which states that in the k-
dimensional submanifold MΣ of M where Σ is constant, i.e., MΣ = {pθ : θ = (µ, Σ), Σ = Σ0 ∈
Pk(R)}, the Fisher-Rao distance dR,MΣ between two distributions is given by the Mahalanobis
distance (Mahalanobis, 1936):"
LOG,0.4691780821917808,"dR,MΣ
 
N(µ1, Σ), N(µ2, Σ)

=
q"
LOG,0.4708904109589041,"(µ1 −µ2)T Σ−1(µ1 −µ2).
(28)"
LOG,0.4726027397260274,"The Mahalanobis distance is also used for OOD detection (Lee et al., 2018) and its performance is
compared to the FRD through several experiments in Section 4. Since the covariance matrix for the
hidden layers’ outputs is often not full rank, the pseudo-inverse is calculated instead of the inverse."
LOG,0.4743150684931507,"B
IGEOOD ALGORITHMS AND COMPUTATION DETAILS"
LOG,0.476027397260274,"In this section, we provide pseudo-code for calculating the IGEOOD score from the logits (Algo-
rithm 1) and from the latent features (Algorithm 2). The BLACK-BOX IGEOOD score is obtained
with Algorithm 1 by setting ε = 0, while the GREY-BOX IGEOOD score is obtained with ε > 0. We
calculated the centroid of the logits for the in-distribution training set by optimizing the objective
function given by equation (3) through a gradient descent algorithm for each DNN. We used a con-
stant learning rate of 0.01 and a batch size of 128 for 100 epochs. Finally, the WHITE-BOX IGEOOD
score is obtained by combining the outputs of Algorithms 1 and 2 through ﬁtting the multiplicative
weights α through a logistic function classiﬁer on a labeled mixture dataset composed from in- and
out-of-distribution data according to a validation dataset, which leads to expression equation (13)."
LOG,0.4777397260273973,"Note that the calculation of the training logits centroids µy, as well as the latent representations’
mean vectors µ(ℓ)
y
and standard covariance matrices σ(ℓ) is performed beforehand, prior to infer-
ence. In this way, we retrieve the objects from memory at inference time. Also, we deﬁne k as the
cardinality of feature ℓ, or |f (ℓ)| and ρFR as the Fisher-Rao distance between univariate Gaussian
distribution given by expression equation (9)."
LOG,0.4794520547945205,Published as a conference paper at ICLR 2022
LOG,0.4811643835616438,"Algorithm 1: Evaluating IGEOOD score based on the logits.
Input : Test sample x, temperature T and noise magnitude ε parameters, and training set
DN = {(xi, yi)}N
i=1.
Output: FR0: IGEOOD score in the logits level."
LOG,0.4828767123287671,"// Offline computation
Calculate the logits centroids from the training data:"
LOG,0.4845890410958904,"µy ≜minµ∈RC
1
Ny
P"
LOG,0.4863013698630137,"∀i : yi=y 2 arccos
P"
LOG,0.488013698630137,"y′∈Y
q"
LOG,0.4897260273972603,"qθ
 
y′|f(xi)

qθ
 
y′|µ
"
LOG,0.4914383561643836,"// Online computation
Add small perturbation to x:
ex ←x + ε ⊙sign
h
∇x
P"
LOG,0.4931506849315068,"y 2 arccos
P"
LOG,0.4948630136986301,"y′∈Y
p"
LOG,0.4965753424657534,"qθ(y′|f(x))qθ(y′|µy)
i"
LOG,0.4982876712328767,return FR0(ex) ←P
LOG,0.5,"y 2 arccos
P"
LOG,0.5017123287671232,"y′∈Y
p"
LOG,0.5034246575342466,"qθ(y′|f(ex))qθ(y′|µy)
"
LOG,0.5051369863013698,Algorithm 2: Evaluating feature-wise IGEOOD score.
LOG,0.5068493150684932,"Input : Test sample x and training set DN = {(xi, yi)}N
i=1.
Output: FRℓ: feature-wise IGEOOD scores."
LOG,0.5085616438356164,"for each feature ℓ∈{1, . . . , L} do"
LOG,0.5102739726027398,// Offline computation
LOG,0.511986301369863,"Calculate the means: µ(ℓ)
y
←
1
Ny
P"
LOG,0.5136986301369864,"i:yi=y f (ℓ) (xi)
Calculate the diagonal standard deviation matrix:"
LOG,0.5154109589041096,"σ(ℓ)
jj ← r"
"N
P",0.5171232876712328,"1
N
P y∈Y
P"
"N
P",0.5188356164383562,"∀i : yi=y

f (ℓ)
j
(xi) −µ(ℓ)
y,j
2"
"N
P",0.5205479452054794,"// Online computation
Compute the OOD score for ℓ:"
"N
P",0.5222602739726028,FRℓ(x) ←miny r
"N
P",0.523972602739726,"Pk
j=1 ρFR

µ(ℓ)
y,j, σ(ℓ)
jj

,

f (ℓ)
j (x), σ(ℓ)
jj
2"
"N
P",0.5256849315068494,"end
return
 
FR1(x), . . . , FRL(x)
"
"N
P",0.5273972602739726,"B.1
LOGITS CENTROIDS ESTIMATION DETAILS"
"N
P",0.5291095890410958,"In order to obtain the logits centroids given the Fisher-Rao distance in the space of softmax prob-
ability distributions, we designed a simple optimization problem. This problem aims to minimize
the average distance between the class conditional training samples and the centroids as given by
equation (3). We initialized the C centroids, where C is the number of classes of a given model, with
the identity matrix of size C × C. Note that the initial centroid for class i is given by the matrix’s
line number i. We minimized the expression in equation (3) with a gradient descent optimizer for
100 epochs with a ﬁxed learning rate equal to 0.1 for every DNN model and in-distribution dataset."
"N
P",0.5308219178082192,"The computation of the logits centroid is done ofﬂine, and the loss of the centroid estimation con-
verges fast. We show in Table 3 the execution time for some operations in the OOD detection
pipeline accelerated by one GPU. The left-hand column shows the ofﬂine computations needed to
run our setup. They are as follow:"
"N
P",0.5325342465753424,"• Save train set logits: We ﬁrst do a forward pass through all the training sets and save in
memory the resulting logits for a given network, which takes on average 83s for CIFAR-10
and CIFAR-100;"
"N
P",0.5342465753424658,"• Centroid estimation: We load the training logits from memory and run the Gradient Descent
algorithm, which takes on average 1.2s for CIFAR-10 and 11s for CIFAR-100;"
"N
P",0.535958904109589,Published as a conference paper at ICLR 2022
"N
P",0.5376712328767124,Algorithm 3: Evaluating feature-wise IGEOOD+ score.
"N
P",0.5393835616438356,"Input : Test sample x, training set DN = {(xi, yi)}N
i=1 and M OOD samples
OM = {x′
i}M
i=1.
Output: FRℓand FR′
ℓ: feature-wise IGEOOD+ scores."
"N
P",0.541095890410959,"for each feature ℓ∈{1, . . . , L} do"
"N
P",0.5428082191780822,"// Offline computation
Calculate class conditional means: µ(ℓ)
y
←
1
Ny
P"
"N
P",0.5445205479452054,i:yi=y f (ℓ) (xi)
"N
P",0.5462328767123288,"Calculate OOD samples mean: µ(ℓ)′ ←
1
M
PM
i=1 f (ℓ) (x′
i)
Calculate the diagonal standard deviation matrix from training data:"
"N
P",0.547945205479452,"σ(ℓ)
jj ← r"
"N
P",0.5496575342465754,"1
N
P y∈Y
P"
"N
P",0.5513698630136986,"∀i : yi=y

f (ℓ)
j
(xi) −µ(ℓ)
y,j
2"
"N
P",0.553082191780822,Calculate the diagonal standard deviation matrix from OOD data:
"N
P",0.5547945205479452,"σ(ℓ)′
jj
← r"
"M
PM",0.5565068493150684,"1
M
PM
i=i

f (ℓ)
j
(x′
i) −µ(ℓ)′
j
2"
"M
PM",0.5582191780821918,"// Online computation
Compute the OOD scores for ℓ:"
"M
PM",0.559931506849315,FRℓ(x) ←miny r
"M
PM",0.5616438356164384,"Pk
j=1 ρFR

µ(ℓ)
y,j, σ(ℓ)
jj

,

f (ℓ)
j (x), σ(ℓ)
jj
2"
"M
PM",0.5633561643835616,"FR′
ℓ(x) ←miny r"
"M
PM",0.565068493150685,"Pk
j=1 ρFR

µ(ℓ)′
j
, σ(ℓ)′
jj

,

f (ℓ)
j (x), σ(ℓ)
jj
2"
"M
PM",0.5667808219178082,"end
return
 
FR1(x), FR′
1(x) . . . , FRL(x), FR′
L(x)
"
"M
PM",0.5684931506849316,"The right-hand side of Table 3 shows the average online computation time for one test sample in a
BLACK-BOX setting."
"M
PM",0.5702054794520548,"• Model inference: The average time needed to complete one forward pass for a DenseNet-
BC-100 model is 28 ms and 19 ms for CIFAR-10 and CIFAR-100, respectively.
• MSP and BLACK-BOX IGEOOD computations. Computing the OOD detection scores from
the calculated softmax output is roughly 100 to 1000 times faster than the inference time
taken by the model."
"M
PM",0.571917808219178,"Hence, computing the Fisher-Rao distance between a test sample and the class-conditional centroids
does not account for a considerable overhead in execution time."
"M
PM",0.5736301369863014,"Table 3: Execution time analysis for an experimental set accelerated by a single GPU for a
DenseNet-BC-100 architecture pre-trained on CIFAR-10 and CIFAR-100. We show the average
value for 5 runs."
"M
PM",0.5753424657534246,"Ofﬂine computation
Online computation"
"M
PM",0.577054794520548,"In-dist.
Dataset"
"M
PM",0.5787671232876712,"Save
train set logits"
"M
PM",0.5804794520547946,"Centroid
estimation"
"M
PM",0.5821917808219178,"Model
inference"
"M
PM",0.583904109589041,"MSP
computation"
"M
PM",0.5856164383561644,"BLACK-BOX
IGEOOD computation"
"M
PM",0.5873287671232876,"CIFAR-10
83 s
1.2 s
28 ms
63 µs
66 µs
CIFAR-100
83 s
11 s
19 ms
34 µs
171 µs"
"M
PM",0.589041095890411,"B.2
COVARIANCE MATRIX ESTIMATION DETAILS"
"M
PM",0.5907534246575342,"We model the latent output probability distributions as Gaussian distributions with diagonal covari-
ance matrix calculated with equation (8). We chose this model motivated by a closed form for"
"M
PM",0.5924657534246576,Published as a conference paper at ICLR 2022
"M
PM",0.5941780821917808,"the FRD and by observing that the standard covariance matrix for the latent features is often ill-
conditioned and diagonal dominant. The condition number of a matrix correlates to its numerical
stability, i.e., a small rounding error in its estimation may cause a large difference in its values. So,
a matrix with a low condition number is said to be well-conditioned, while a matrix with a high
condition number is said to be ill-conditioned. We calculate the condition number of the covariance
matrices with the formula κ(Σ) =
Σ−1
∞∥Σ∥∞, where ∥· ∥∞is the inﬁnity norm. For each of
the four dense blocks outputs of a DenseNet trained on CIFAR-10, we obtained the condition num-
bers κΣ = {2.8e10, 3.5e6, 3.1e5, 3.5e21}. While for the diagonal covariance matrix, we obtained
smaller values of condition numbers: κΣD = {1.0e3, 3.0e1, 1.4e1, 7.6e20}. We associate the high
value for the last feature mainly because the last feature is high dimensional and coarse, i.e., most
of the values in the diagonal are close to zero."
"M
PM",0.5958904109589042,"B.3
GAUSSIANITY TEST OF THE HIDDEN LAYERS’ OUTPUTS"
"M
PM",0.5976027397260274,"In order to test if the Gaussian assumption is valid for the outputs of the hidden feature, we conduct
a Shapiro & Wilk (1965) normality test for each coordinate of the features for the training data of
a DenseNet model. We calculated the test’s W statistic for each coordinate and class and averaged
them. We chose a univariate normality test because they are often powerful and the problem is high
dimensional, which would be unfavorable for a multivariate statistic test. Thus, this study should be
considered with caution, given the considered hypothesis. In Figure 3, we also show the standardized
histograms for the ﬁrst coordinate of each layer. Note that, apart from the penultimate layer, if we
consider the coordinates of the hidden features independently, the Gaussianity assumption holds, as
we obtain a W statistics close to 1. However, for the last block, this assumption sometimes does not
hold. Hence, modeling the penultimate layer with a more powerful density estimator, and using a
metric that considers this more complex distribution, may be favorable for OOD detection."
"M
PM",0.5993150684931506,"B.4
FEATURE IMPORTANCE REGRESSION DETAILS"
"M
PM",0.601027397260274,"For both Mahalanobis and IGEOOD methods, we ﬁtted a logistic regression model with cross-
validation using 1,000 OOD and 1,000 in-distribution data samples. Each regression parameter
multiplies the layer scores outputs with the objective function of maximizing the TNR at TPR-95%.
We set the maximum number of iterations to 100."
"M
PM",0.6027397260273972,"In order to investigate which hidden feature assists the most in OOD detection, we calculate the
TNR at TPR-95% for the scores in the outputs of Blocks 1, 2 and 3 of a DenseNet pre-trained on
CIFAR-10. We took as OOD data the SVHN dataset. Figure 4 shows the histogram and detection
performance for each layer as well as the results from the logistic regression. Note that for the
IGEOOD score in this study, we did not consider the logits."
"M
PM",0.6044520547945206,"C
DETAILED EXPERIMENTAL SETUP"
"M
PM",0.6061643835616438,"C.1
DNN MODELS AND TRAINING DETAILS"
"M
PM",0.6078767123287672,We describe the DNN models used in the experiments:
"M
PM",0.6095890410958904,"• DenseNet. Densely Connected Convolutional Networks (Huang et al., 2017), or DenseNet
for short, are compositions of dense blocks, which are composed of multiple layers di-
rectly connected to every other layer in a feed-forward fashion. In this work, we use the
DenseNet-BC-100 architecture. The BC stands for a model with 1x1 convolutional bot-
tleneck (B) layers and channel number compression (C) of 0.5. The models have depth
L = 100 and growth rate k = 12. We consider the outputs of each dense block after the
transition layer (3 in total) and the ﬁrst convolutional layer output as the latent features.
After an averaging pooling, the latent features have dimensions F1 = {24, 108, 150, 342}."
"M
PM",0.6113013698630136,"• ResNet. Residual Networks (He et al., 2016), or ResNet, are deep neural networks com-
posed of residual blocks.
Each residual block is composed of layers connected in a
feed-forward manner plus a skip connection.
We use the ResNet with 34 layers pre-
trained on CIFAR-10, CIFAR-100, and SVHN datasets.
We take the output of every
residual block (4 in total) and the ﬁrst convolutional layer for calculating the score on"
"M
PM",0.613013698630137,Published as a conference paper at ICLR 2022
"M
PM",0.6147260273972602,(a) DenseNet pre-trained on CIFAR-10.
"M
PM",0.6164383561643836,(b) DenseNet pre-trained on CIFAR-100.
"M
PM",0.6181506849315068,(c) DenseNet pre-trained on SVHN.
"M
PM",0.6198630136986302,"Figure 3: Histograms of the standardized ﬁrst coordinate output of each hidden feature of a
DenseNet model for in-distribution and out-of-distribution (TinyImageNet) compared to a 1-D Nor-
mal distribution. The Average Shapiro-Wilk test’s W statistics is close to one for Conv 0, Block
1 and Block 2, which indicates that the coordinates, and potentially the feature vector, are prov-
ably Gaussian. The penultimate layer (outputs of Block 3) has a lower test statistic for the given
experiments."
"M
PM",0.6215753424657534,"the WHITE-BOX setting. After an averaging pooling, the latent features have dimensions
F2 = {64, 64, 128, 256, 512}."
"M
PM",0.6232876712328768,"We train each model by minimizing the cross-entropy loss using SGD with Nesterov momentum
equal to 0.9, weight decay equal to 0.0001, and a multi-step learning rate schedule starting at 0.1
for 300 epochs. The pre-trained models is available at 2. We report their test set accuracy in Table
4 with the softmax function and by replacing it with the Fisher-Rao distance between the training
class-conditional centroids and the test sample outputs. Also, it is worth noting that one high-end
GPU is sufﬁcient for running every experiment presented in this work."
"M
PM",0.625,2https://github.com/edadaltocg/Igeood
"M
PM",0.6267123287671232,Published as a conference paper at ICLR 2022
"M
PM",0.6284246575342466,"(a) Block 1.
(b) Block 2.
(c) Block 3."
"M
PM",0.6301369863013698,(d) Logistic regression result.
"M
PM",0.6318493150684932,"Figure 4: Histograms of the Mahalanobis and IGEOOD scores for the output of each hidden block of
a DenseNet model for CIFAR-10 (in-dstribution) and SVHN (out-of-distribution). The title shows
the TNR at TPR-95% considering only the scores of the outputs of the given layer. The logistic
regression found as coefﬁcients: α = (1.0, −3.6, −0.13) for Mahalanobis and α = (1.0, 1.3, 1.2)
for IGEOOD."
"M
PM",0.6335616438356164,"Table 4: Test set accuracy in percentage for ResNet and DenseNet architectures pre-trained on
CIFAR-10, CIFAR-100 and SVHN."
"M
PM",0.6352739726027398,"ResNet-34
DenseNet-BC-100
In-Dataset
Softmax
Fisher-Rao
Softmax
Fisher-Rao
CIFAR-10
93.52
93.53
95.20
95.20
CIFAR-100
77.11
77.09
77.62
77.63
SVHN
96.61
96.61
95.16
95.16"
"M
PM",0.636986301369863,"C.2
EVALUATION METRICS"
"M
PM",0.6386986301369864,"We introduce below standard binary classiﬁcation performance metrics used to evaluate the OOD
discriminators."
"M
PM",0.6404109589041096,"• True Negative Rate at 95% True Positive Rate (TNR at TPR-95% (%)). This metric
measures the true negative rate (TNR) at a speciﬁc true positive rate (TPR). The operating
point is chosen such that the TPR of the in-distribution test set is ﬁxed to some value, 95%
in this case. Mathematically, let TP, TN, FP, and FN denote true positive, true negative,
false positive and false negative, respectively. We measure TNR = TN/(FP + TN), when
TPR = TP/(TP + FN) is 95%."
"M
PM",0.6421232876712328,Published as a conference paper at ICLR 2022
"M
PM",0.6438356164383562,"• Area Under the Receiver Operating Characteristic curve (AUROC (%)). The ROC
curve is constructed by plotting the true positive rate (TPR) against the false positive
rate (= FP/(FP + TN)) at various threshold values. The area under this curve tells how
much the OOD discriminator can distinguish in-distribution and OOD data in a threshold-
independent manner."
"M
PM",0.6455479452054794,"• Area Under the Precision-Recall curve (AUPR (%)). The PR curve plots the precision
(= TP/(TP + FP)) against the recall (= TP/(TP + FN)) by varying a threshold. For the
experiments, in-distribution data are speciﬁed as positives while OOD data as negative."
"M
PM",0.6472602739726028,"Note that the TNR at TPR-95% is signiﬁcant because we want to identify OOD data and preserve a
sufﬁciently good performance on identifying in-distribution data, which is not the case for the other
metrics."
"M
PM",0.648972602739726,"C.3
DATASETS"
"M
PM",0.6506849315068494,"We use natural image examples from the following image classiﬁcation and synthetic datasets in our
experiments. We normalize the test samples with the in-distribution dataset statistics."
"M
PM",0.6523972602739726,"• CIFAR-10. The CIFAR-10 (Krizhevsky et al., 2009) dataset is composed of 32 × 32 natu-
ral images of 10 different classes, e.g., airplane, ship, bird, etc. The training set comprises
50,000 images, and the test set is composed of 10,000 images. The classes are approxi-
mately equally distributed (5,000 examples each label). The CIFAR-10 dataset is under the
MIT license."
"M
PM",0.6541095890410958,"• CIFAR-100. The CIFAR-100 (Krizhevsky et al., 2009) dataset contains similar natural
images to the CIFAR-10 dataset, but with 90 additional categories. Its set repartition is
50,000 for training and 10,000 for the test set. We expect around 500 samples for each
class of the training set. It is also under the MIT license."
"M
PM",0.6558219178082192,"• SVHN. The SVHN (Netzer et al., 2011) dataset collects street house numbers for digit
classiﬁcation. It contains 73,257 training and 26,032 test RGB images of size 32 × 32
of printed digits (from 0 to 9). We take only the ﬁrst 10,000 examples of the test set for
evaluating the methods to have a balanced dataset of in-distribution and out-of-distribution
data. This dataset is subject to a non-commercial license."
"M
PM",0.6575342465753424,"• Tiny-ImageNet. The Tiny-ImageNet (Le & Yang, 2015) dataset is a subset of the large-
scale natural image dataset ImageNet (Deng et al., 2009). It contains 200 different classes
and 10,000 test examples. We downsize the images from their original resolution to images
of dimension 32 × 32 × 3."
"M
PM",0.6592465753424658,"• LSUN. The LSUN (Yu et al., 2015) dataset, which has equally 10,000 test examples, is
used for the large-scale scene classiﬁcation of different scene categories (e.g., bedroom,
bridge, kitchen, etc.). Similarly, we resize the images following the same procedure for the
Tiny-ImageNet dataset. LSUN is under the Apache 2.0 license."
"M
PM",0.660958904109589,"• iSUN. The iSUN (Xu et al., 2015) dataset consists of selected natural scene images from
the SUN (Xiao et al., 2010) dataset. The test set has 8925 images, which we downsample
to 32 × 32 × 3. We use this dataset as a source of OOD for validation purposes as an
independent dataset from the test OOD data."
"M
PM",0.6626712328767124,"• Textures. The Describable Textures Dataset (DTD) (Cimpoi et al., 2014) is a collection of
textural pattern images observed in nature. It contains 47 categories totaling 5640 images
of various sizes, which are resized and center cropped to ﬁt into the input size of 32 × 32."
"M
PM",0.6643835616438356,"• Chars74K. The Chars74K dataset (de Campos et al., 2009) contains 74,000 samples of
62 classes of characters found in natural images, handwritten text, and synthesized from
computer fonts. We used as OOD data only the EnglishImg dataset split, which contains
7705 characters from natural scenes. We resized and center-cropped the images."
"M
PM",0.666095890410959,"• Places365. The Places365 dataset (Zhou et al., 2017) contains images of 365 natural scenes
categories. We used the small images validation split as OOD data in our experiments. It
contains 36,500 RGB images which were downsampled from 256 × 256 to 32 × 32."
"M
PM",0.6678082191780822,Published as a conference paper at ICLR 2022
"M
PM",0.6695205479452054,"• Gaussian. For the Gaussian dataset, we generated 10,000 synthetic RGB images from 2D
Gaussian noise, where each RGB pixel is sampled from an i.i.d Gaussian distribution with
mean 0.5 and variance 1.0. The pixel values are clipped to [0, 1] interval. This synthetic
data was introduced in previous work as an easy benchmark (Hendrycks & Gimpel, 2017)."
"M
PM",0.6712328767123288,"C.4
ADVERSARIAL DATA GENERATION"
"M
PM",0.672945205479452,"We generate adversarial samples from the in-distribution dataset using the fast gradient sign method
(FGSM). This method works by exploiting the gradients of the neural network to create a non-
targeted adversarial attack. For an input image xi, the method computes the sign of the gradients
of the loss function J with respect to the input image to create a new image xadv
i
that maximizes
the loss as given by equation (29). This fabricated image is called an adversarial image, which
we use for tuning the hyperparameters of the OOD detection methods in the WHITE-BOX case.
Mathematically,
xadv
i
= xi + εadv ⊙sign(∇xiJ(θ, xi, yi)),
(29)"
"M
PM",0.6746575342465754,"where εadv > 0 is the additive noise magnitude parameter. Table 5 shows the resulting L∞mean
perturbation and classiﬁcation accuracy on adversarial samples."
"M
PM",0.6763698630136986,"Table 5: The L∞mean perturbation used to generate adversarial data with FGSM algorithm and
classiﬁcation accuracy on adversarial samples for the DNN models and in-distribution datasets."
"M
PM",0.678082191780822,"CIFAR-10
CIFAR-100
SVHN
L∞
Acc.
L∞
Acc.
L∞
Acc.
DenseNet-BC-100
0.21
19.5%
0.20
4.45%
0.32
54.7%
ResNet-34
0.21
23.7%
0.20
12.49%
0.25
50.0%"
"M
PM",0.6797945205479452,"D
BENCHMARK METHODS"
"M
PM",0.6815068493150684,This section brieﬂy introduces the benchmark OOD detection methods with a standardized notation.
"M
PM",0.6832191780821918,"D.1
BASELINE"
"M
PM",0.684931506849315,"DNNs tend to assign lower conﬁdence for OOD samples. So, calculating the Maximum Softmax
Probability (MSP) (Hendrycks & Gimpel, 2017) is a natural baseline for OOD detection. In other
words, provided an input data x, a pre-trained neural network f(·), and a conﬁdence threshold δ,
the OOD score, and the discriminator are given by"
"M
PM",0.6866438356164384,"s(x) = max
y∈Y
efy(x)
P"
"M
PM",0.6883561643835616,"y′∈Y efy′(x)
and S(x; δ) =

1
if s(x) ≤δ
0
if s(x) > δ
,
(30)"
"M
PM",0.690068493150685,"respectively. Here, fy(x) indicates the y-th logits output. A limitation of this method is that un-
scaled softmax posterior distributions are usually spiky, i.e., softmax trained deep neural models are
incorrectly calibrated, which does not favor OOD detection (Lakshminarayanan et al., 2017)."
"M
PM",0.6917808219178082,"D.2
ODIN: OOD DETECTOR FOR NEURAL NETWORKS"
"M
PM",0.6934931506849316,"In summary, ODIN (Liang et al., 2018) explores the weaknesses of the MSP criterion by recalibrat-
ing the output’s conﬁdence to the task of OOD detection. They improve the MSP baseline by using
the temperature scaled softmax function (equation (1)) instead. Also, ODIN adds small adversarial
noise perturbation to the inputs, i.e.,"
"M
PM",0.6952054794520548,"ex = x −ε ⊙sign (−∇x log qθ(y|f(x); T)) ,
(31)"
"M
PM",0.696917808219178,"where ε is the perturbation magnitude. Hyperparameters T and ε are tuned on a validation dataset
without requiring prior knowledge of test OOD data. They calculate the conﬁdence score by taking
the maximum of the perturbed input temperature scaled softmax outputs."
"M
PM",0.6986301369863014,Published as a conference paper at ICLR 2022
"M
PM",0.7003424657534246,"D.3
ENERGY-BASED OOD DETECTOR"
"M
PM",0.702054794520548,"An energy-based OOD discriminator is proposed by Liu et al. (2020), where the differences of ener-
gies between in-distribution and OOD samples allow for distribution distinction. The energy-based
model substitutes the softmax function with the Helmholtz free energy equation to extract a conﬁ-
dence score. They observed that examples with higher energy have a low likelihood of occurrence,
concluding that they are likely OOD. The free energy expression is:"
"M
PM",0.7037671232876712,"E(x; f) = −T · log
X"
"M
PM",0.7054794520547946,"y∈Y
efy(x)/T .
(32)"
"M
PM",0.7071917808219178,"Note that, differently from ODIN and MSP, they use the information of all of the logits output
values through the sum operation. Besides, they apply input pre-processing for further separating
OOD data from in-distribution."
"M
PM",0.708904109589041,"D.4
MAHALANOBIS DISTANCE-BASED CONFIDENCE SCORE"
"M
PM",0.7106164383561644,"The Mahalanobis-based method in Lee et al. (2018) ﬁts the DNN training data features as class-
conditional Gaussian distributions. These use the outputs of every DNN latent block to leverage
useful information for discrimination. For a test sample x, the conﬁdence score from the ℓ-th feature
is calculated based on the Mahalanobis distance between f (ℓ)(x) and the closest class-conditional
distribution:"
"M
PM",0.7123287671232876,"Mℓ(x) = max
y
−

f (ℓ)(x) −bµ(ℓ)
y
⊤bΣ−1
ℓ

f (ℓ)(x) −bµ(ℓ)
y

,
(33)"
"M
PM",0.714041095890411,"where f (ℓ)(x) is the ℓ-th latent feature output, and bµ(ℓ)
y
and bΣℓare, respectively, the empirical class
mean and covariance matrix estimates. The covariance matrix is often not full rank, so the pseudo-
inverse is calculated instead of the inverse. In addition, input pre-processing and feature ensemble
are also used to boost performance. A logistic regression model learns the multiplicative weights αℓ
for each layer score, which predicts 1 for in-distribution and 0 for OOD examples from a mixture
validation dataset. Finally, the Mahalanobis-based discriminator is given by thresholding expression
P"
"M
PM",0.7157534246575342,ℓαℓMℓ(x).
"M
PM",0.7174657534246576,"E
ADDITIONAL OUT-OF-DISTRIBUTION DETECTION RESULTS"
"M
PM",0.7191780821917808,"E.1
FISHER-RAO DISTANCE VERSUS KULLBACK-LEIBLER DIVERGENCE"
"M
PM",0.7208904109589042,"From Picot et al. (2021), the Kullback-Leibler divergence (KL) is connected to the Fisher-Rao dis-
tance between softmax probability distributions (dR,D) by the inequality:"
"M
PM",0.7226027397260274,"1 −cos
dR,D (qθ, q′
θ)
2 
≤1"
"M
PM",0.7243150684931506,"2KL (qθ, q′
θ) .
(34)"
"M
PM",0.726027397260274,"To verify how the KL divergence would behave for OOD detection, we ran experiments with our
BLACK-BOX setting, where we calculated the class conditional centroids with the KL divergence.
We calculated the divergence of the test sample w.r.t each of these centroids during test time, then
aggregated the results with a sum or by taking the minimal value. The results are displayed in Table
6. We can conclude from these experiments that taking the sum of the outputs instead of the minimal
value is overall advantageous for Fisher-Rao distance and KL divergence."
"M
PM",0.7277397260273972,"E.2
HYPERPARAMETERS TUNING"
"M
PM",0.7294520547945206,"For temperature T, we ran a Bayesian optimization for 500 epochs in the interval of temperature
values between 1 and 1000, where the objective function was to maximize the TNR at TPR-95%
metric for the validation set. We took the best temperature among ﬁve runs with different random
seeds. For the input pre-processing noise magnitude ε tuning, we ran a grid search optimization
with 21 equally spaced values in the interval [0, 0.002]. Table 7 shows the best hyperparameters we
found for the methods in the BLACK-BOX, GREY-BOX, and WHITE-BOX settings."
"M
PM",0.7311643835616438,Published as a conference paper at ICLR 2022
"M
PM",0.7328767123287672,"Table 6: Performance comparison between the Fisher-Rao distance and the KL Divergence for OOD
detection in a BLACK-BOX setting. The numerical values in the Table are TNR at TPR-95% in
percentage for a DenseNet and ResNet models pre-trained on CIFAR-10, CIFAR-100 and SVHN
datasets. FISHER-RAO (sum) corresponds to the IGEOOD score."
"M
PM",0.7345890410958904,"OOD
dataset"
"M
PM",0.7363013698630136,"CIFAR-10
CIFAR-100
SVHN
FISHER-RAO (sum) / FISHER-RAO (min) / KL (min) / KL (sum)"
"M
PM",0.738013698630137,DenseNet
"M
PM",0.7397260273972602,"Chars
55.1/45.0/56.9/54.6
17.2/14.6/20.1/17.1
47.9/46.6/50.1/46.9
Gaussian
99.9/97.9/97.9/99.9
0.0/0.0/0.0/0.0
98.0/97.2/73.1/98.1
TinyImgNet
87.8/73.6/72.3/88.1
25.7/18.1/15.7/25.4
85.1/84.1/69.4/85.0
LSUN
93.3/81.9/86.4/93.4
25.4/17.8/15.1/25.2
85.0/83.9/66.2/85.4
Places365
52.2/49.7/57.2/51.5
20.7/20.9/18.2/20.8
71.9/71.1/59.1/71.3
Textures
35.8/46.9/51.0/34.8
22.8/19.5/17.6/23.1
56.5/57.4/65.3/55.4
CIFAR-10
-
17.0/20.0/18.5/17.7
67.0/66.0/56.2/65.8
CIFAR-100
50.8/48.7/49.6/50.3
-
65.4/65.0/59.1/64.1
SVHN
50.1/47.9/50.7/49.6
36.7/29.9/29.1/35.9
-
average
65.6/61.4/65.3/65.3
20.7/17.6/16.8/20.6
72.1/71.4/62.3/71.5"
"M
PM",0.7414383561643836,ResNet
"M
PM",0.7431506849315068,"Chars
51.1/45.0/41.6/49.6
15.2/14.6/14.5/15.5
58.5/57.4/46.2/58.4
Gaussian
89.0/86.4/62.3/86.8
0.6/1.7/3.9/0.9
87.3/87.5/76.7/87.0
TinyImageNet
58.2/51.4/51.4/57.8
23.0/17.8/10.4/21.6
82.2/81.6/68.8/81.9
LSUN
62.0/53.8/56.0/62.0
20.6/15.4/10.2/19.5
77.4/77.5/64.6/77.2
Places365
48.2/40.0/39.6/48.1
16.9/17.3/16.7/17.8
79.0/79.1/67.2/78.8
Textures
50.3/44.0/45.6/49.8
23.4/20.9/14.1/23.2
80.9/80.9/72.8/80.6
CIFAR-10
-
18.0/18.1/16.8/18.8
81.2/81.0/67.8/81.1
CIFAR-100
45.9/38.9/36.8/45.6
-
80.2/79.8/66.2/79.9
SVHN
48.8/31.6/31.5/47.0
13.3/14.3/15.7/14.3
-
average
56.7/48.9/45.6/55.8
16.4/15.0/12.8/16.4
78.3/78.1/66.3/78.1"
"M
PM",0.7448630136986302,"Table 7: Best temperatures T for the BLACK-BOX setup, best temperature and noise magnitude
(T, ε) for the GREY-BOX setup, and best ε for the Mahalanobis score and (T, ε) for IGEOOD and
IGEOOD+ in the WHITE-BOX setup with adversarial tuning."
"M
PM",0.7465753424657534,"In-dist.
dataset
BLACK-BOX
GREY-BOX
WHITE-BOX
Model
ODIN
Energy
IGEOOD
ODIN
Energy
IGEOOD
Maha.
IGEOOD,+"
"M
PM",0.7482876712328768,"DenseNet
C-10
1000
4.6
5.3
(1000, 0.0014)
(4.6, 0.0012)
(5.3, 0.0012)
0
(5, 0.0015)
C-100
1000
1.1
2.1
(1000, 0.0020)
(1.1, 0.0020)
(2.1, 0.0020)
0
(5, 0)
SVHN
1
1.1
1.1
(1, 0.0010)
(1.1, 0.0006)
(1.1, 0.0006)
0.001
(5, 0.0015)"
"M
PM",0.75,"ResNet
C-10
1000
5.4
5.3
(1000, 0.0014)
(5.4, 0.0012)
(5.3, 0.0012)
0.0005
(2, 0)
C-100
1000
1
1
(1000, 0.0020)
(9.1, 0.0024)
(12.7, 0.0024)
0.0005
(1, 0)
SVHN
1000
1.7
1
(1000, 0.0004)
(1.7, 0.0002)
(1.0, 0.0004)
0
(5, 0)"
"M
PM",0.7517123287671232,"E.3
TEMPERATURE SCALING AND NOISE MAGNITUDE PLOTS"
"M
PM",0.7534246575342466,"In Figure 5 and 6, we plot on the left hand side column the effect of the temperature parameter in the
performance for the BLACK-BOX setup. We set the noise magnitude to zero and measured the TNR
at TPR-95% for 500 different temperatures values found by a Bayesian optimization for a variety
of DNN models. The performance is evaluated on the iSUN dataset. The right hand side column
of Figure 5 and 6 show the effect of the noise magnitude parameter in the performance of IGEOOD
score in the GREY-BOX setup. We set the temperature to the best found in the BLACK-BOX case.
Then, we measured the OOD performance for 21 values of noise magnitude ε equally spaced in
the interval [0, 0.004]. The best couple (T, ε) for each method and model is used to evaluate the
GREY-BOX performances. The best hyperparameters found are detailed in Table 7."
"M
PM",0.7551369863013698,Published as a conference paper at ICLR 2022
"M
PM",0.7568493150684932,(a) DenseNet on CIFAR-10.
"M
PM",0.7585616438356164,(b) DenseNet on CIFAR-100.
"M
PM",0.7602739726027398,(c) DenseNet on SVHN.
"M
PM",0.761986301369863,"Figure 5: OOD detection performance against temperature and noise magnitude parameters for
ODIN (Liang et al., 2018), Energy (Liu et al., 2020) and IGEOOD (ours) on the iSUN (Xu et al.,
2015) OOD dataset for a DenseNet-100 architecture."
"M
PM",0.7636986301369864,"E.4
CONSISTENCY OF IGEOOD SCORE CONCERNING THE CHOICE OF THE VALIDATION DATA"
"M
PM",0.7654109589041096,"To verify the consistency of IGEOOD and other methods to the choice of validation data, we mea-
sured the TNR at TPR-95% after tuning our method in a BLACK-BOX and GREY-BOX scenario on
nine validation datasets. In Table 8, the ﬁrst column shows the validation dataset, while we used
the remaining OOD datasets to evaluate performance. We obtained consistent results, ranging from
63.4% to 72.0% the average TNR at TPR-95% in the BLACK-BOX case and from 65.0% to 73.4%
in the GREY-BOX setting. We show that input pre-processing provides mild amelioration for our
method and can be considered a ﬁne-tuning step."
"M
PM",0.7671232876712328,"E.5
ERROR BARS AND STANDARD DEVIATION"
"M
PM",0.7688356164383562,"We conduct all of our experiments during inference time. Provided that we ﬁx the DNN, the in-
distribution, and the out-of-distribution datasets, there is not a source of randomness to our algorithm
because the weights α of the feature ensemble method and centroids are initialized deterministically.
Thus, the OOD scores for the same experimental setting do not change. To conﬁrm this, we ran the
same experiment ﬁve times and obtained the same results in all of them. However, if we allow for
retraining the DNN from scratch, we might obtain different parameters, leading to slightly different
model accuracy and potentially OOD detection performance. With this in mind, we retrained a
DenseNet-BC-100 model on CIFAR-10 ﬁve times with ﬁve different random seeds. The results for
OOD detection in a BLACK-BOX setting for the 5 models can be found in Table 9."
"M
PM",0.7705479452054794,Published as a conference paper at ICLR 2022
"M
PM",0.7722602739726028,(a) ResNet on CIFAR-10.
"M
PM",0.773972602739726,(b) ResNet on CIFAR-100.
"M
PM",0.7756849315068494,(c) ResNet on SVHN.
"M
PM",0.7773972602739726,"Figure 6: Temperature and noise magnitude tuning for OOD detection performance for ODIN (Liang
et al., 2018), Energy (Liu et al., 2020) and IGEOOD (ours) on iSUN (Xu et al., 2015) OOD dataset
for a ResNet-34 architecture."
"M
PM",0.7791095890410958,"Table 8: BLACK-BOX and GREY-BOX settings average performance across different OOD datasets
for validation. The hyperparameters are tuned using one validation dataset (column 1), and evalua-
tion is done on the remaining eight OOD test datasets. The DNN is DenseNet-BC-100 pre-trained
on CIFAR-10, and the values are TNR at TPR-95% in percentage."
"M
PM",0.7808219178082192,"BLACK-BOX
GREY-BOX
Validation set
Baseline
ODIN
Energy
IGEOOD
ODIN
Energy
IGEOOD"
"M
PM",0.7825342465753424,"iSUN
52.5
64.3
64.9
65.6
66.8
64.8
65.3
Chars
55.0
70.8
71.1
71.4
72.5
72.0
73.4
CIFAR-100
55.4
68.6
69.1
72.0
68.6
71.7
71.3
Gaussian
49.4
62.8
65.6
63.4
70.4
64.0
68.0
TinyImgNet
53.0
64.7
65.2
63.5
67.0
65.0
65.5
LSUN
52.1
63.9
63.7
63.6
66.6
65.3
65.0
Places365
55.3
68.5
69.0
71.8
70.0
71.5
70.9
SVHN
55.4
68.7
69.3
69.5
70.0
69.4
70.1
Textures
55.4
71.2
73.1
71.4
71.5
72.4
71.6
average and std.
53.7±2.0
67.1±3.0
67.9±3.0
68.0±3.7
69.3±2.0
68.4±3.4
69.0±3.0"
"M
PM",0.7842465753424658,Published as a conference paper at ICLR 2022
"M
PM",0.785958904109589,"Table 9: Experiment using ﬁve different training seeds for DenseNet-100 on CIFAR-10 for the
BLACK-BOX scenario. The average test accuracy of the 5 models is 94.58%±0.13%. All values are
percentages."
"M
PM",0.7876712328767124,"TNR at TPR-95%
AUROC
Dataset
Baseline / ODIN / Energy / IGEOOD
Chars
34.1±21/54.3±20/59.0±14/59.4±14
88.6±4.3/90.4±3.5/90.2±3.9/90.6±3.4
CIFAR-100
37.1±0.5/47.8±1.4/44.8±2.3/45.2±2.2
88.2±0.3/88.8±0.7/88.0±1.0/88.3±0.8
Gaussian
42.7±49/74.0±28/79.6±23/80.2±23
93.7±4.3/96.6±2.4/96.7±1.8/96.7±1.8
TinyImgNet
50.6±4.3/76.1±4.4/78.3±5.0/78.4±5.0
92.5±1.0/95.7±1.0/96.0±1.1/96.1±1.0
LSUN
58.0±3.6/85.2±3.5/87.3±4.7/87.5±4.5
94.2±0.6/97.4±0.6/97.6±0.7/97.7±0.7
Places365
9.30±1.5/54.2±2.5/52.7±4.2/53.2±3.9
88.4±0.4/89.9±1.2/89.4±1.7/89.7±1.5
SVHN
36.0±3.0/48.1±7.1/46.1±9.9/46.5±9.6
86.8±2.0/86.6±4.8/85.9±5.7/86.4±5.0
Textures
35.6±1.7/38.2±1.4/33.3±2.8/34.0±2.6
87.2±0.6/83.3±1.0/80.8±1.9/82.1±1.3
average and std.
41.7±10/59.7±8.6/60.1±8.3/60.6±8.0
90.0±1.7/91.1±1.9/90.6±2.2/90.9±1.9"
"M
PM",0.7893835616438356,"Table 10: Average and standard deviation OOD detection performance across eight OOD datasets
for each model and in-distribution dataset in a GREY-BOX setting."
"M
PM",0.791095890410959,"TNR at TPR-95%
AUROC
Model
In-dist.
ODIN / Energy / IGEOOD"
"M
PM",0.7928082191780822,DenseNet
"M
PM",0.7945205479452054,"C-10
66.8±23/64.8±25/65.3±24
91.9±6.2/91.5±6.4/91.9±6.0
C-100
25.5±14/24.8±13/25.0±13
76.6±12/76.4±12/78.2±8.2
SVHN
75.4±15/70.6±17/72.4±16
91.6±5.4/89.2±6.9/90.0±6.3"
"M
PM",0.7962328767123288,ResNet
"M
PM",0.797945205479452,"C-10
57.3±20/57.7±19/57.8±19
89.2±5.4/88.7±5.3/89.0±5.2
C-100
31.1±22/30.2±22/30.2±22
76.9±11/74.4±12/74.3±12
SVHN
78.5±7.8/78.5±7.9/78.8±7.8
90.4±3.4/90.9±3.4/90.7±3.3
Average and Std.
55.8±21/54.4±20/54.9±20
86.1±6.7/85.2±7.0/85.7±6.8"
"M
PM",0.7996575342465754,"E.6
IGEOOD COMPARED TO OTHER WHITE-BOX METHODS."
"M
PM",0.8013698630136986,"Even though Lee et al. (2018) shares the closest setup to ours, recent literature also shows promising
results for OOD detection in a WHITE-BOX setting, achieving state-of-the-art in a few benchmarks.
Notably, the works from Sastry & Oore (2020); Hsu et al. (2020); Zisselman & Tamar (2020) achieve
remarkable performance in a range of benchmarks. Thus, we gathered the reported results from the
original works and displayed them in Table 11 and 12, which considers that a few OOD samples
and only adversarial samples are available for tuning, respectively. We highlight that Sastry & Oore
(2020) extracts, in addition to the outputs of the blocks, intra-block features for the ResNet and
DenseNet models."
"M
PM",0.803082191780822,"E.7
EXTENDED OOD DETECTION RESULTS"
"M
PM",0.8047945205479452,"We show in Table 13 extended OOD detection results of Table 1. It contains the OOD detection
performance for each model, in-distribution dataset and OOD dataset in a BLACK-BOX setting. In
Table 14, we show the performance of ODIN, energy-based, and IGEOOD scores in the task of OOD
detection in a GREY-BOX setup for each OOD dataset. In Table 15 and 16, we show additional
results referring to the right-hand column and left-hand column of Table 2, respectively."
"M
PM",0.8065068493150684,"F
HISTOGRAMS"
"M
PM",0.8082191780821918,"Figures 7, 8, 10 and 9 display histograms for the OOD detection score for IGEOOD in the BLOCK-
BOX, GREY-BOX and WHITE-BOX settings, respectively."
"M
PM",0.809931506849315,Published as a conference paper at ICLR 2022
"M
PM",0.8116438356164384,"Table 11: TNR at TPR-95% (%) performance in a WHITE-BOX setting considering the original
results from Lee et al. (2018) and Zisselman & Tamar (2020) with access to OOD samples. The
models are DenseNet-BC-100 and ResNet-34 pre-trained on CIFAR-10, CIFAR-100 and SVHN."
"M
PM",0.8133561643835616,"OOD
dataset"
"M
PM",0.815068493150685,"CIFAR-10
CIFAR-100
SVHN
Mahalanobis / Res-Flow / IGEOOD / IGEOOD+"
"M
PM",0.8167808219178082,DenseNet
"M
PM",0.8184931506849316,"iSUN
95.3/ - /97.7/99.8
87.0/ - /93.8/99.7
99.9/ - /98.3/99.9
LSUN
97.2/98.2/98.5/99.9
91.4/96.3/95.2/99.9
99.9/100/97.1/99.9
TinyImgNet
95.0/96.4/95.7/99.8
86.6/93.0/94.5/99.5
99.9/100/98.2/99.9
SVHN/C-10
90.8/94.9/98.9/99.9
82.5/84.9/93.3/99.6
96.8/99.0/91.6/98.3
average
94.6/96.5/97.7/99.8
86.9/91.4/94.2/99.7
99.1/99.6/96.3/99.5"
"M
PM",0.8202054794520548,ResNet
"M
PM",0.821917808219178,"iSUN
97.8/ - /97.2/99.9
89.9/ - /93.4/99.8
99.7/ - /99.8/100
LSUN
98.8/99.0/98.4/100
90.9/96.2/94.3/100
99.9/100/99.7/99.9
TinyImgNet
97.1/97.8/96.3/99.6
90.9/94.6/90.1/99.6
99.9/100/99.7/99.9
SVHN/C-10
87.8/96.5/98.8/99.8
91.9/93.0/91.6/99.7
98.4/99.4/97.7/99.7
average
95.4/97.8/97.7/99.8
90.9/94.6/92.35/99.8
99.5/99.8/99.2/99.9"
"M
PM",0.8236301369863014,"Table 12: TNR at TPR-95% (%) performance in a WHITE-BOX setting considering the original
results from Lee et al. (2018); Sastry & Oore (2020); Hsu et al. (2020); Zisselman & Tamar (2020)
without access to OOD samples for hyperparameter tuning."
"M
PM",0.8253424657534246,"OOD
dataset"
"M
PM",0.827054794520548,"CIFAR-10
CIFAR-100
SVHN
Mahalanobis / Gram Matrix / DeConf-C / Res-Flow / IGEOOD / IGEOOD+"
"M
PM",0.8287671232876712,DenseNet
"M
PM",0.8304794520547946,"iSUN
94.3/99.0/99.4/ - /94.5/95.8
84.8/95.9/98.4/ - /93.8/92.2
99.9/99.4/ - / - /98.2/98.6
LSUN
97.2/99.5/99.4/98.1/96.4/97.2
91.4/97.2/98.7/95.8/95.1/94.4
100/99.5/ - /100/97.3/97.0
TinyImgNet
94.9/98.8/99.1/96.1/93.4/94.5
87.2/95.7/98.6/91.5/94.3/94.0
99.9/99.1/ - /99.9/98.1/96.8
SVHN/C-10
89.9/96.1/98.8/86.1/94.3/95.7
62.2/89.3/95.9/48.9/90.1/90.6
90.0/80.4/ - /90.0/89.5/86.6
average
94.1/98.3/99.2/93.4/94.6/95.8
81.4/94.5/97.9/78.7/93.3/92.8
97.4/94.6/ - /96.6/95.8/94.8"
"M
PM",0.8321917808219178,ResNet
"M
PM",0.833904109589041,"iSUN
96.8/99.3/88.8/ - /95.3/95.0
87.9/94.8/75.3/ - /89.4/91.0
100/99.4/ - / - /99.8/99.9
LSUN
98.1/99.6/90.9/99.1/97.7/97.7
56.6/96.6/76.8/70.4/88.6/93.9
99.9/99.6/ - /100/99.8/100
TinyImgNet
95.5/98.7/81.4/98.0/94.3/94.2
70.3/94.8/76.5/77.5/86.2/90.1
99.2/99.3/ - /99.9/99.6/99.6
SVHN/C-10
75.8/97.6/89.5/91.0/98.2/97.7
41.9/80.8/55.1/74.1/75.2/78.5
94.1/85.8/ - /96.6/96.7/97.3
average
91.5/98.8/87.6/96.0/96.3/96.2
64.2/91.7/71.0/74.0/84.8/88.4
98.3/96.0/ - /98.8/99.0/99.2"
"M
PM",0.8356164383561644,Published as a conference paper at ICLR 2022
"M
PM",0.8373287671232876,Table 13: Extended BLACK-BOX results for Table1. Parameter tuning on iSUN dataset.
"M
PM",0.839041095890411,"In-dist.
(model)
OOD
dataset
TNR at TPR-95%
AUROC
AUPR"
"M
PM",0.8407534246575342,Baseline / ODIN / Energy / IGEOOD
"M
PM",0.8424657534246576,"CIFAR-10
(DenseNet)"
"M
PM",0.8441780821917808,"Chars
43.5/57.2/54.6/55.0
90.2/91.2/90.4/90.5
93.0/93.1/92.5/92.7
CIFAR-100
40.6/53.1/50.5/50.7
89.4/90.4/89.7/89.8
90.5/90.7/90.1/90.2
Gaussian
88.1/99.8/99.9/99.9
97.6/98.9/98.5/98.5
98.3/99.3/99.1/99.1
TinyImgNet
59.4/85.0/88.0/87.8
94.1/97.3/97.6/97.6
95.4/97.7/97.9/97.9
LSUN
66.9/91.4/93.3/93.3
95.5/98.3/98.5/98.5
96.5/98.5/98.7/98.7
Places365
40.8/54.2/51.5/52.0
88.8/90.2/89.5/89.7
74.4/74.8/73.9/74.3
SVHN
40.4/52.0/49.6/50.1
89.9/90.9/90.2/90.3
84.6/84.6/83.5/83.7
Textures
40.5/42.1/34.9/35.6
88.5/85.1/82.4/83.2
93.1/88.3/86.5/87.8
average
52.5/66.8/65.3/65.6
91.7/92.8/92.1/92.3
90.7/90.9/90.3/90.6"
"M
PM",0.8458904109589042,"CIFAR-100
(DenseNet)"
"M
PM",0.8476027397260274,"Chars
15.1/17.8/17.0/17.2
72.8/78.0/77.9/77.8
79.6/83.8/83.9/83.8
CIFAR-10
17.7/18.1/17.1/17.0
75.6/74.8/74.4/75.4
78.3/74.3/74.0/76.2
Gaussian
0.0/0.0/0.0/0.0
30.2/19.4/19.5/30.7
53.2/44.0/44.1/53.5
TinyImgNet
16.7/24.7/25.0/25.7
72.0/79.4/79.6/79.5
74.8/80.7/80.9/80.7
LSUN
15.5/23.2/24.2/25.4
70.9/80.4/80.8/80.6
74.4/82.6/82.9/82.5
Places365
18.8/21.2/20.6/20.6
75.9/78.0/77.7/78.0
54.2/54.5/54.3/55.7
SVHN
25.7/36.4/36.5/36.7
82.8/88.4/88.4/88.2
75.4/82.5/82.4/82.4
Textures
18.0/22.4/22.2/22.8
72.7/74.4/74.3/75.7
80.8/79.1/79.0/81.5
average
15.9/20.5/20.3/20.7
69.1/71.6/71.6/73.2
71.3/72.7/72.7/74.5"
"M
PM",0.8493150684931506,"SVHN
(DenseNet)"
"M
PM",0.851027397260274,"Chars
46.4/27.0/45.0/47.9
83.9/52.2/79.6/80.9
91.8/70.2/88.9/89.6
CIFAR-10
61.8/66.0/64.7/67.0
92.3/90.9/90.3/90.9
96.2/95.1/94.6/95.0
CIFAR-100
61.3/64.4/63.0/65.4
91.9/90.3/89.5/90.3
95.7/94.3/93.8/94.3
Gaussian
93.6/97.8/97.9/98.0
97.4/98.0/98.0/98.0
99.2/99.4/99.4/99.4
TinyImgNet
80.4/84.4/84.1/85.1
95.5/95.3/94.9/95.3
97.9/97.4/97.2/97.5
LSUN
80.1/84.4/84.3/85.0
95.5/95.3/95.1/95.3
98.0/97.6/97.5/97.6
Places365
66.8/71.0/69.9/71.9
93.0/91.9/91.3/91.9
89.1/85.8/84.6/85.8
Textures
56.4/55.2/52.4/56.5
88.9/84.6/82.5/84.9
95.5/93.3/92.2/93.4
average
68.3/68.8/70.2/72.1
92.3/87.3/90.2/90.9
95.4/91.6/93.5/94.1"
"M
PM",0.8527397260273972,"CIFAR-10
(ResNet)"
"M
PM",0.8544520547945206,"Chars
36.8/45.8/50.7/51.1
89.4/90.1/90.3/90.4
92.7/92.7/92.5/92.7
CIFAR-100
33.6/41.8/45.6/45.9
86.4/87.0/87.0/87.1
87.0/86.6/86.2/86.4
Gaussian
81.5/89.9/88.2/89.0
96.9/97.3/96.7/96.7
97.9/98.3/98.0/98.0
TinyImgNet
42.1/53.4/57.7/58.2
90.3/91.5/91.6/91.7
91.8/92.2/92.1/92.2
LSUN
41.2/55.1/61.7/62.0
90.1/91.5/92.0/92.1
91.5/92.1/92.3/92.3
Places365
32.9/42.4/48.2/48.2
85.8/86.6/86.9/86.9
67.1/66.1/65.6/65.7
SVHN
27.7/39.9/48.6/48.8
89.2/90.2/90.5/90.6
85.8/85.1/84.1/84.5
Textures
37.9/46.6/49.6/50.3
89.0/89.1/88.4/88.6
93.9/93.3/92.4/92.6
average
41.7/51.9/56.3/56.7
89.6/90.4/90.4/90.5
88.5/88.3/87.9/88.1"
"M
PM",0.8561643835616438,"CIFAR-100
(ResNet)"
"M
PM",0.8578767123287672,"Chars
14.3/15.3/15.1/15.2
72.7/73.0/73.1/73.4
77.8/77.2/77.3/77.7
CIFAR-10
18.1/18.4/17.4/18.0
76.5/76.6/76.5/76.8
78.3/77.8/77.8/78.1
Gaussian
1.6/0.8/0.3/0.6
72.8/76.0/76.7/76.7
81.6/83.8/84.4/84.3
TinyImgNet
17.8/20.7/23.8/23.0
73.3/76.6/77.4/76.9
76.4/78.7/79.2/78.8
LSUN
15.5/18.8/21.5/20.6
70.8/74.1/74.9/74.4
72.9/75.2/75.7/75.2
Places365
17.3/17.2/16.2/16.9
74.1/73.2/72.9/73.4
44.6/42.1/41.9/42.7
SVHN
14.2/13.8/12.5/13.3
74.8/74.1/73.9/74.5
59.4/56.9/56.8/57.7
Textures
20.9/22.8/23.3/23.3
77.0/78.0/78.2/78.3
85.9/86.2/86.3/86.4
average
15.0/16.0/16.3/16.4
74.0/75.2/75.4/75.5
72.1/72.3/72.4/72.6"
"M
PM",0.8595890410958904,"SVHN
(ResNet)"
"M
PM",0.8613013698630136,"Chars
56.9/58.2/58.4/58.5
85.1/83.7/83.7/84.0
92.3/91.0/91.0/91.3
CIFAR-10
79.0/80.6/81.0/81.3
93.0/92.1/92.2/92.5
94.7/93.4/93.4/93.7
CIFAR-100
78.0/79.6/79.8/80.2
92.7/91.9/91.9/92.2
94.7/93.4/93.4/93.7
Gaussian
85.3/86.7/86.9/87.3
95.9/95.7/95.7/95.9
97.7/97.1/97.1/97.3
TinyImgNet
79.8/81.4/81.8/82.2
93.5/92.9/92.9/93.2
95.3/94.3/94.3/94.5
LSUN
74.9/76.8/77.2/77.4
91.5/90.5/90.6/90.9
93.5/92.2/92.1/92.4
Places365
77.0/78.4/78.8/79.0
92.0/91.0/91.1/91.4
81.4/77.9/77.8/78.6
Textures
78.5/80.0/80.4/80.9
93.7/93.0/93.0/93.4
97.6/97.0/97.0/97.2
average
76.2/77.7/78.0/78.4
92.2/91.3/91.4/91.7
93.4/92.0/92.0/92.4
Average of the average values
46.0/51.6/52.1/52.6
85.3/85.4/85.1/85.8
85.2/84.7/84.4/85.1"
"M
PM",0.863013698630137,Published as a conference paper at ICLR 2022
"M
PM",0.8647260273972602,Table 14: Extended GREY-BOX results. Parameter tuning on iSUN dataset.
"M
PM",0.8664383561643836,"In-dist.
(model)
OOD
dataset
TNR at TPR-95%
AUROC
AUPR"
"M
PM",0.8681506849315068,ODIN / Energy / IGEOOD
"M
PM",0.8698630136986302,"CIFAR-10
(DenseNet)"
"M
PM",0.8715753424657534,"Chars
52.5/49.2/50.0
89.1/88.2/88.7
90.7/90.2/90.8
CIFAR-100
48.9/49.7/50.0
88.3/88.5/88.7
88.4/88.6/88.9
Gaussian
100/100/100
100/99.9/99.9
100/99.9/99.9
TinyImgNet
92.6/92.4/92.4
98.5/98.5/98.5
98.6/98.5/98.5
LSUN
96.2/96.2/96.2
99.2/99.1/99.2
99.3/99.2/99.2
Places365
52.4/52.0/52.4
89.0/88.9/89.2
88.7/88.7/89.0
SVHN
49.3/41.8/42.5
89.7/88.2/88.6
82.1/80.6/81.4
Textures
42.9/37.0/38.7
81.4/80.8/82.3
84.7/84.6/86.5
average
66.8/64.8/65.3
91.9/91.5/91.9
91.6/91.3/91.8"
"M
PM",0.8732876712328768,"CIFAR-100
(DenseNet)"
"M
PM",0.875,"Chars
19.8/19.2/19.5
76.6/76.3/75.8
80.8/80.6/79.7
CIFAR-10
15.3/16.4/16.9
72.4/72.8/74.9
72.6/72.8/76.1
Gaussian
0.0/0.0/0.0
47.1/46.9/59.9
65.5/65.4/74.9
TinyImgNet
43.8/42.2/40.1
86.5/86.2/84.8
87.2/87.0/85.3
LSUN
42.2/40.6/38.9
86.8/86.4/84.7
87.8/87.7/85.7
Places365
23.4/23.8/23.9
79.1/79.1/79.4
79.4/79.4/80.0
SVHN
34.7/31.4/35.7
87.7/87.2/88.1
81.0/80.8/81.8
Textures
24.9/24.7/24.9
76.7/76.6/77.8
81.7/81.6/83.8
average
25.5/24.8/25.0
76.6/76.4/78.2
79.5/79.4/80.9"
"M
PM",0.8767123287671232,"SVHN
(DenseNet)"
"M
PM",0.8784246575342466,"Chars
53.3/45.8/48.3
81.8/78.0/79.3
90.4/88.1/88.8
CIFAR-10
69.8/65.1/67.1
91.3/89.1/89.9
95.2/93.7/94.2
CIFAR-100
69.8/64.2/66.3
91.1/88.5/89.4
94.8/93.0/93.6
Gaussian
99.4/99.0/99.1
98.9/98.5/98.6
99.7/99.5/99.6
TinyImgNet
88.4/85.5/86.4
96.1/94.9/95.2
97.9/97.0/97.3
LSUN
88.2/85.8/86.5
96.2/95.1/95.4
98.0/97.3/97.5
Places365
74.8/69.9/71.8
92.4/90.3/91.0
95.8/94.2/94.7
Textures
59.4/49.6/53.7
84.9/79.0/81.4
93.4/90.4/91.6
average
75.4/70.6/72.4
91.6/89.2/90.0
95.7/94.2/94.7"
"M
PM",0.8801369863013698,"CIFAR-10
(ResNet)"
"M
PM",0.8818493150684932,"Chars
54.7/54.2/54.8
88.3/87.8/88.2
90.7/90.4/90.6
CIFAR-100
38.9/41.7/41.5
83.8/83.8/84.1
83.7/83.5/83.8
Gaussian
100/100/100
100/99.5/99.5
100/99.7/99.7
TinyImgNet
68.7/66.6/67.0
93.1/92.4/92.6
93.3/92.7/92.9
LSUN
70.4/68.8/68.9
93.2/92.8/92.9
93.3/92.9/93.0
Places365
40.4/43.2/43.0
84.1/84.1/84.2
84.0/83.9/84.0
SVHN
38.6/39.9/40.0
84.8/84.0/84.6
76.4/74.6/75.7
Textures
46.8/47.1/47.5
86.1/85.2/85.7
91.4/90.7/91.0
average
57.3/57.7/57.8
89.2/88.7/89.0
89.1/88.5/88.8"
"M
PM",0.8835616438356164,"CIFAR-100
(ResNet)"
"M
PM",0.8852739726027398,"Chars
14.8/13.8/13.8
68.0/65.1/65.0
71.0/68.5/68.4
CIFAR-10
14.3/11.8/11.8
70.8/65.1/64.9
70.1/64.7/64.5
Gaussian
78.7/73.8/74.3
96.8/95.9/96.0
97.7/97.4/97.4
TinyImgNet
44.9/49.2/49.3
86.4/86.5/86.5
86.1/85.6/85.6
LSUN
40.2/44.3/44.4
83.5/83.8/83.8
82.5/82.5/82.5
Places365
15.9/10.2/10.1
67.7/59.7/59.5
64.5/58.2/58.0
SVHN
10.2/10.6/10.6
64.3/65.4/65.3
42.2/43.6/43.5
Textures
29.5/27.6/27.7
77.9/73.7/73.6
84.4/81.1/81.0
average
31.1/30.2/30.2
76.9/74.4/74.3
74.8/72.7/72.6"
"M
PM",0.886986301369863,"SVHN
(ResNet)"
"M
PM",0.8886986301369864,"Chars
59.9/59.4/60.0
82.8/83.2/83.2
90.4/90.7/90.6
CIFAR-10
80.9/81.0/81.3
90.9/91.5/91.2
92.2/92.8/92.5
CIFAR-100
80.2/80.2/80.5
90.8/91.4/91.1
92.3/92.9/92.6
Gaussian
89.4/88.7/89.4
95.9/95.9/95.9
97.3/97.2/97.2
TinyImgNet
82.6/82.3/82.9
92.2/92.6/92.5
93.5/93.9/93.8
LSUN
77.8/77.6/78.3
89.6/90.1/89.9
91.3/91.7/91.6
Places365
78.7/78.8/79.0
89.8/90.6/90.2
91.2/91.9/91.5
Textures
78.6/79.8/79.2
91.3/92.2/91.7
96.1/96.6/96.3
average
78.5/78.5/78.8
90.4/90.9/90.7
93.0/93.5/93.3
Average of average values
55.8/54.4/54.9
86.1/85.2/85.7
87.3/86.6/87.0"
"M
PM",0.8904109589041096,Published as a conference paper at ICLR 2022
"M
PM",0.8921232876712328,Table 15: WHITE-BOX extended results. Validation on OOD data.
"M
PM",0.8938356164383562,"In-dist.
(model)
OOD
dataset
TNR at TPR-95%
AUROC
AUPR"
"M
PM",0.8955479452054794,"Mahalanobis (Lee et al., 2018) / IGEOOD+"
"M
PM",0.8972602739726028,"CIFAR-10
(DenseNet)"
"M
PM",0.898972602739726,"Chars
91.3/99.4
97.5/99.9
97.7/99.9
CIFAR-100
21.4/56.6
67.3/90.7
64.4/90.8
TinyImgNet
96.9/99.8
99.3/99.9
99.3/99.9
LSUN
98.2/99.9
99.5/100
99.5/100
Places365
18.1/80.2
72.7/95.7
72.8/95.4
SVHN
90.1/99.9
97.3/100
97.3/100
Textures
84.1/97.4
95.6/99.5
94.7/99.5
Gaussian
100/100
100/100
100/100
iSUN
97.3/99.8
99.4/100
99.4/100
average
77.5±31/92.6±14
92.1±12/98.4±3.0
91.7±13/98.4±3.0"
"M
PM",0.9006849315068494,"CIFAR-100
(DenseNet)"
"M
PM",0.9023972602739726,"Chars
62.9/97.5
94.0/99.4
95.8/99.4
CIFAR-10
9.1/22.7
60.8/80.7
60.1/83.0
TinyImgNet
87.1/99.5
97.4/99.9
97.4/99.9
LSUN
91.1/99.9
97.8/100
98.1/100
Places365
5.9/58.2
54.8/90.0
54.7/89.2
SVHN
79.0/99.6
96.8/99.9
94.1/99.9
Textures
70.3/90.2
91.4/98.1
94.3/98.2
Gaussian
100/100
100/100
100/100
iSUN
86.4/99.7
96.8/99.9
97.7/99.9
average
67.7±28/90.2±21
87.8±13/97.7±5.0
88.0±12/97.8±5.0"
"M
PM",0.9041095890410958,"SVHN
(DenseNet)"
"M
PM",0.9058219178082192,"Chars
78.7/92.2
96.1/98.4
98.9/98.5
CIFAR-10
91.6/98.3
98.0/99.6
99.4/99.6
CIFAR-100
92.9/95.3
98.2/99.1
99.4/99.2
TinyImgNet
99.9/99.9
99.8/99.9
99.9/99.9
LSUN
99.9/99.9
99.8/100
99.7/100
Places365
94.7/98.3
98.3/99.6
98.4/99.7
Textures
98.2/98.5
99.4/99.6
99.9/99.6
Gaussian
100/100
100/100
100/100
iSUN
99.9/99.9
99.8/99.9
99.9/99.9
average
95.1±8.0/98.0±2.0
98.8±1.0/99.6±0.1
99.5±1.0/99.6±0.1"
"M
PM",0.9075342465753424,"CIFAR-10
(ResNet)"
"M
PM",0.9092465753424658,"Chars
93.6/99.3
98.6/99.8
99.1/99.8
CIFAR-100
44.9/51.3
87.4/90.9
87.8/91.7
TinyImgNet
96.8/99.6
99.4/99.9
99.4/99.9
LSUN
98.3/99.9
99.6/100
99.6/100
Places365
45.8/77.6
88.1/95.6
88.1/95.5
SVHN
96.1/99.8
99.0/99.9
98.1/99.9
Textures
84.3/97.0
97.3/99.4
98.6/99.4
Gaussian
100/100
100/100
100/100
iSUN
97.2/99.9
99.4/100
99.5/100
average
84.1±23/91.6±16
96.5±4.0/98.4±3.0
96.7±4.0/98.5±3.0"
"M
PM",0.910958904109589,"CIFAR-100
(ResNet)"
"M
PM",0.9126712328767124,"Chars
63.8/97.8
94.0/99.5
96.0/99.5
CIFAR-10
18.0/30.8
76.6/85.3
76.4/87.8
TinyImgNet
90.1/99.6
97.9/99.9
98.0/99.9
LSUN
92.4/100
98.3/100
98.5/100
Places365
23.5/59.1
76.8/91.2
76.0/91.4
SVHN
88.4/99.7
97.7/99.9
95.2/99.9
Textures
71.6/90.7
93.9/98.2
96.6/98.1
Gaussian
100/100
100/100
100/100
iSUN
89.4/99.8
97.7/99.9
98.0/99.9
average
70.8±30/86.4±23
92.5±10/97.1±5.0
92.7±10/97.4±4.0"
"M
PM",0.9143835616438356,"SVHN
(ResNet)"
"M
PM",0.916095890410959,"Chars
84.9/92.4
97.0/98.4
99.0/98.5
CIFAR-10
98.0/99.7
99.2/99.9
99.7/99.9
CIFAR-100
98.3/99.1
99.3/99.7
99.8/99.8
TinyImgNet
99.9/99.9
99.9/100
100/100
LSUN
99.9/99.9
99.9/100
100/100
Places365
98.4/99.6
99.3/99.9
99.8/99.9
Textures
99.0/99.9
99.7/99.9
99.9/99.9
Gaussian
100/100
100/100
100/100
iSUN
100/100
99.9/100
100/100
average
97.6±6.0/98.9±2.0
99.4±1.0/99.7±0.1
99.8±1.0/99.8±0.1
Avg. and std. of avg. values
82.1±11/92.9±4.0
94.5±4.0/98.5±1.0
94.7±4.0/98.6±1.0"
"M
PM",0.9178082191780822,Published as a conference paper at ICLR 2022
"M
PM",0.9195205479452054,Table 16: WHITE-BOX extended results. Validation on adversarial (FGSM) data.
"M
PM",0.9212328767123288,"In-dist.
(model)
OOD
dataset
TNR at TPR-95%
AUROC
AUPR"
"M
PM",0.922945205479452,"Mahalanobis (Lee et al., 2018) / IGEOOD"
"M
PM",0.9246575342465754,"CIFAR-10
(DenseNet)"
"M
PM",0.9263698630136986,"Chars
88.5/87.3
97.7/97.7
98.3/98.3
CIFAR-100
21.5/26.4
68.0/77.7
66.3/75.5
TinyImageNet
93.9/93.4
98.6/98.7
98.6/98.7
LSUN
96.3/96.4
99.1/99.2
99.1/99.2
Places365
17.8/23.2
70.0/77.9
40.1/76.3
SVHN
87.0/94.3
97.2/98.7
93.7/97.3
Textures
83.6/86.0
95.8/97.2
97.3/98.2
Gaussian
100/100
100/100
100/100
iSUN
94.3/94.5
98.8/98.9
98.9/99.0
average
75.9±30/77.9±29
91.7±12/94.0±9.0
88.0±20/93.6±10"
"M
PM",0.928082191780822,"CIFAR-100
(DenseNet)"
"M
PM",0.9297945205479452,"CIFAR-10
1.1/5.7
43.5/62.6
46.7/62.6
Chars
53.9/59.6
92.2/92.0
94.5/93.9
TinyImageNet
86.4/94.3
97.4/98.8
97.5/98.9
LSUN
88.6/95.1
97.6/98.9
97.9/98.9
Places365
5.5/13.0
56.6/71.0
57.5/71.0
SVHN
56.1/90.1
91.8/98.0
85.4/96.2
Textures
67.5/86.7
91.2/97.4
94.4/98.4
Gaussian
100/100
100/100
100/100
iSUN
84.8/93.8
97.2/98.7
97.6/98.8
average
60.4±34/70.9±35
85.3±19/90.8±13
85.7±19/91.0±13"
"M
PM",0.9315068493150684,"SVHN
(DenseNet)"
"M
PM",0.9332191780821918,"CIFAR-10
90.6/89.5
97.7/97.8
99.1/99.2
CIFAR-100
91.8/88.4
98.0/97.7
99.2/99.1
Chars
72.3/70.5
95.2/94.5
98.5/98.3
TinyImageNet
99.5/98.1
99.6/99.3
99.5/99.8
LSUN
99.9/97.3
99.8/99.1
99.9/99.7
Places365
94.3/91.9
98.3/98.2
98.1/99.3
Textures
95.3/97.1
98.8/99.3
99.6/99.8
Gaussian
100/100
100/99.9
100/100
iSUN
99.9/98.2
99.8/99.3
99.9/99.8
average
93.7±8.0/92.3±9.0
98.6±1.0/98.3±2.0
99.3±1.0/99.4±0.5"
"M
PM",0.934931506849315,"CIFAR-10
(ResNet)"
"M
PM",0.9366438356164384,"CIFAR-100
36.5/21.5
84.5/63.3
84.3/58.1
Chars
82.0/90.9
96.9/98.3
97.7/98.7
TinyImageNet
96.2/94.3
99.2/98.0
99.2/96.7
LSUN
98.2/97.7
99.5/99.2
99.5/98.9
Places365
34.8/15.9
85.0/60.1
84.2/24.4
SVHN
81.0/98.2
96.6/99.3
93.7/97.5
Textures
81.7/81.6
96.7/93.4
98.2/94.3
Gaussian
100/100
100/100
100/100
iSUN
96.8/95.3
99.3/98.6
99.3/98.1
average
78.6±24/77.3±32
95.3±6.0/90.0±15
95.1±6.0/85.2±25"
"M
PM",0.9383561643835616,"CIFAR-100
(ResNet)"
"M
PM",0.940068493150685,"CIFAR-10
3.0/5.0
61.0/59.6
63.7/60.6
Chars
39.9/55.1
85.6/90.4
88.1/92.5
TinyImageNet
88.7/86.2
97.6/97.3
97.6/97.3
LSUN
91.3/88.6
98.0/97.8
98.3/98.0
Places365
8.0/8.6
67.9/63.0
66.8/61.7
SVHN
31.6/75.2
82.9/95.8
68.8/92.7
Textures
65.9/78.1
91.9/95.6
95.2/97.6
Gaussian
100/100
100/100
100/100
iSUN
87.9/89.4
97.4/97.8
97.6/97.7
average
57.4±36/65.1±33
86.9±13/88.6±15
86.2±14/88.7±15"
"M
PM",0.9417808219178082,"SVHN
(ResNet)"
"M
PM",0.9434931506849316,"CIFAR-10
97.1/96.7
99.1/99.2
99.7/99.7
CIFAR-100
97.5/96.2
99.1/99.1
99.7/99.6
Chars
75.4/55.1
95.3/89.1
98.5/96.0
TinyImageNet
99.9/99.6
99.9/99.9
99.9/99.9
LSUN
100/99.8
99.9/99.9
100/100
Places365
98.1/97.0
99.2/99.2
99.2/99.0
Textures
98.9/98.4
99.6/99.6
99.9/99.9
Gaussian
100/100
99.9/100
100/100
iSUN
100/99.8
99.8/99.9
99.9/100
average
96.3±8.0/93.6±14
99.1±1.0/98.4±3.0
99.6±0.5/99.3±1.0
Avg. and std. of avg. values
77.0±15/79.5±10
92.8±5.4/93.4±3.9
92.3±5.9/92.9±5.2"
"M
PM",0.9452054794520548,Published as a conference paper at ICLR 2022
"M
PM",0.946917808219178,(a) DenseNet on CIFAR-10.
"M
PM",0.9486301369863014,(b) DenseNet on CIFAR-100.
"M
PM",0.9503424657534246,(c) DenseNet on SVHN.
"M
PM",0.952054794520548,(d) ResNet on CIFAR-10.
"M
PM",0.9537671232876712,(e) ResNet on CIFAR-100.
"M
PM",0.9554794520547946,(f) ResNet on SVHN.
"M
PM",0.9571917808219178,Figure 7: BLACK-BOX setup. TinyImageNet as OOD dataset.
"M
PM",0.958904109589041,Published as a conference paper at ICLR 2022
"M
PM",0.9606164383561644,(a) DenseNet on CIFAR-10.
"M
PM",0.9623287671232876,(b) DenseNet on CIFAR-100.
"M
PM",0.964041095890411,(c) DenseNet on SVHN.
"M
PM",0.9657534246575342,(d) ResNet on CIFAR-10.
"M
PM",0.9674657534246576,(e) ResNet on CIFAR-100.
"M
PM",0.9691780821917808,(f) ResNet on SVHN.
"M
PM",0.9708904109589042,Figure 8: GREY-BOX setup. TinyImageNet as OOD dataset.
"M
PM",0.9726027397260274,Published as a conference paper at ICLR 2022
"M
PM",0.9743150684931506,(a) DenseNet on CIFAR-10.
"M
PM",0.976027397260274,(b) DenseNet on CIFAR-100.
"M
PM",0.9777397260273972,(c) DenseNet on SVHN.
"M
PM",0.9794520547945206,(d) ResNet on CIFAR-10.
"M
PM",0.9811643835616438,(e) ResNet on CIFAR-100.
"M
PM",0.9828767123287672,(f) ResNet on SVHN.
"M
PM",0.9845890410958904,Figure 9: WHITE-BOX setup with adversarial data validation. TinyImageNet as OOD dataset.
"M
PM",0.9863013698630136,Published as a conference paper at ICLR 2022
"M
PM",0.988013698630137,(a) DenseNet on CIFAR-10.
"M
PM",0.9897260273972602,(b) DenseNet on CIFAR-100.
"M
PM",0.9914383561643836,(c) DenseNet on SVHN.
"M
PM",0.9931506849315068,(d) ResNet on CIFAR-10.
"M
PM",0.9948630136986302,(e) ResNet on CIFAR-100.
"M
PM",0.9965753424657534,(f) ResNet on SVHN.
"M
PM",0.9982876712328768,Figure 10: WHITE-BOX setup with validation on OOD data. TinyImageNet as OOD dataset.
