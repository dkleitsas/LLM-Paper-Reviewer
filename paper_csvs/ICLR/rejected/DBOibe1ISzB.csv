Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004149377593360996,"In this paper, we focus on learning-based methods for simulation. Most existing
particle-based simulators adopt graph neural networks (GNNs) to model the un-
derlying physics of particles. However, they force particles to interact with all
neighbors without selection, and they fall short in capturing material semantics,
such as viscosity or plastic deformations, for different particles, leading to unsat-
isfactory performance, especially in generalization. This paper proposes Simula-
tion Transformer (SiT) to simulate particle dynamics with more careful modeling
of particle states, interactions, and their intrinsic properties. Speciﬁcally, besides
the particle tokens, which encode the states of particles into latent space, SiT gen-
erates interaction tokens to extract high-level representations for interactions and
selectively focuses on essential interactions by allowing both tokens to attend to
each other. In addition, SiT learns material-aware representations by learnable
abstract tokens, which will participate in the attention mechanism and boost the
generalization capability further. We evaluate our model on diverse environments,
including ﬂuid, rigid, and deformable objects, which cover domains of different
complexity and materials. Without bells and whistles, SiT shows strong abili-
ties to simulate particles of different materials and achieves superior performance
and generalization across these environments with fewer parameters than existing
methods. Codes and models will be released."
INTRODUCTION,0.008298755186721992,"1
INTRODUCTION"
INTRODUCTION,0.012448132780082987,"Particle-based physics simulation is a classic and important topic in computer science. It not only fa-
cilitates the exploration of underlying principles in physics, chemistry and biology, but also enables
the creation of vivid visual effects such as explosion and ﬂuid dynamic in ﬁlms and games. Dif-
ferent from traditional simulators, such as grid-based (Guo et al., 2016) and mesh-based (Bronstein
et al., 2017) methods, particle-based simulators view a system, which is an example in one domain,
as a composition of particles and imitate system changes over time by predicting the changes of
particle-wise states according to current particle states and particle interactions, of which the latter
represents the inﬂuence of action-reaction forces, such as the collisions. Consequently, they follow
the same forward process without separately considering different constraints to simulate different
domains with varying materials, requiring no domain-speciﬁc physical priors. Moreover, since in
particle-based simulators the dynamics of a system is modeled by the states of particles and their
interactions, they also have the potential to possess a strong generalization ability, where they can
estimate the dynamics of a system with varying number and conﬁguration of particles in a more
robust manner. After learning the dynamics of ﬂuid water in a sandbox, the same particle-based
simulator can be used to simulate a waterfall and a river."
INTRODUCTION,0.016597510373443983,"Recent particle-based simulators (Battaglia et al., 2016; Schenck & Fox, 2018; Mrowca et al., 2018;
Li et al., 2019; Sanchez-Gonzalez et al., 2020; Ummenhofer et al., 2020) often view a system as a
graph, and adopt graph neural network (GNN) (Kipf & Welling, 2016) as the basic network structure.
In these attempts, each particle is treated as a node in the graph, with edges linking it to all its
neighboring particles, assuming interactions mainly occur between particles that are close to each
other. Subsequently, state updates of particles are achieved by combining node features with the
summation of edge features. While such a GNN-based formulation obtains satisfying simulation
results, it faces two issues that affect efﬁciency and generality. First, it forces each particle to interact
with all its nearby particles without providing a selective mechanism, which leads to computational"
INTRODUCTION,0.02074688796680498,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.024896265560165973,"Figure 1: (a). Samples of the datasets. FluidFall contains two drops of water. FluidShake simulates a block
of water in a moving box. RiceGrip has a deformable object squeezed by two grippers. BoxBath contains a
rigid cubic washed by water. (b). Rollouts from generalizations of RiceGrip, where we increase the number of
particles. SiT with abstract tokens, marked by +, can still maintain the object’s shape, while DPI-Net (Li et al.,
2019) fail to simulate the deformable object."
INTRODUCTION,0.029045643153526972,"redundancy and prevents the discovery of inherent patterns of particle interaction. Second, the
GNN-based formulation uses particle-wise attributes to capture both material characteristics, such
as viscosity or plastic deformations, and domain-speciﬁc semantics, such as the shape of a rigid
material. Therefore, it may regard the latter as part of the intrinsic material semantics and fail to
generalize to domains with the same materials but different particle amounts and conﬁgurations."
INTRODUCTION,0.03319502074688797,"In this paper, we propose a novel Transformer-based framework, referred to as Simulation Trans-
former (SiT), for particle-based physics simulation. The model inherits the powerful multi-head
self-attention mechanism in Transformer (Vaswani et al., 2017) to capture particle interactions. To
further encourage efﬁcient modeling of complex interactions, instead of treating particle interac-
tions as attention weights obtained by dot-product , we introduce the notion of interaction tokens,
which are high-dimensional representations for interactions, to model the rich semantics of particle
interactions, such as how the particle is restored after deformations. In addition, to disentangle local
material-speciﬁc characteristics from global domain-speciﬁc semantics, SiT further learns a high-
dimensional abstract token for each type of material to capture material semantics, forcing particles
of the same material to interact with their corresponding abstract token."
INTRODUCTION,0.03734439834024896,"The proposed SiT is more appealing than previous methods in several aspects. First, through cap-
turing particle interactions explicitly with interaction tokens and allowing dynamic inter-token at-
tention, SiT dynamically focuses on essential particle interactions and reduces the computations
for redundant and noisy ones. This is crucial for particle-based simulation, especially for domains
containing hundreds and thousands of densely placed particles, where the modeling of all particle
interactions is redundant, expensive and noisy in practice. Second, thanks to the trainable abstract
tokens that disentangle intrinsic material characteristics from domain-speciﬁc semantics, we can
reuse them to apply SiT to unseen domains of the same materials without retraining. As shown in
our experiments, after training on one domain consisting of ﬂuid water and a rigid cubic, SiT still
yields fairly faithful simulations when the cubic is replaced with a ball or a bunny comparing with
previous work."
INTRODUCTION,0.04149377593360996,"To show the effectiveness of SiT, we perform extensive evaluations on four standard environments
commonly used in the literature (Li et al., 2019; Sanchez-Gonzalez et al., 2020), covering domains
of different complexity and materials. The proposed method achieves superior performance across
all these environments with fewer parameters compared to existing methods. We further demonstrate
the generalization ability of SiT by adjusting the environments to create new domains and applying
SiT to these domains without retraining. In all cases, SiT obtains more realistic simulation results
than previous methods, which tend to overﬁt to the training domains."
RELATED WORK,0.04564315352697095,"2
RELATED WORK"
RELATED WORK,0.04979253112033195,"Physics simulation by neural networks. There are many different kind of representations for
physics simulations. Grid-based methods (Lee & You, 2019; Thuerey et al., 2020; Wang et al., 2020)
adopt convolutional architectures for learning high-dimensional physical system, while mesh-based
simulations (Bronstein et al., 2017; Luo et al., 2020; Hanocka et al., 2019; Nash et al., 2020; Qiao
et al., 2020; Weng et al., 2021; Pfaff et al., 2021) typically simulate objects with continuous surfaces,
such as clothes, rigid objects, surfaces of water and so on."
RELATED WORK,0.05394190871369295,Under review as a conference paper at ICLR 2022
RELATED WORK,0.058091286307053944,"Many works (Battaglia et al., 2016; Schenck & Fox, 2018; Mrowca et al., 2018; Li et al., 2019;
Sanchez-Gonzalez et al., 2020; Ummenhofer et al., 2020) simulate physics on particle-based sys-
tems, where all objects are represented by groups of particles. Speciﬁcally, Interaction Network
(IN) (Battaglia et al., 2016) simulated interactions in object-level. Smooth Particle Networks (SP-
Nets) (Schenck & Fox, 2018) implemented ﬂuid dynamics using position-based ﬂuids (Macklin &
M¨uller, 2013). Hierarchical Relation Network (HRN) (Mrowca et al., 2018) predicted physical dy-
namics based on hierarchical graph convolution. Dynamic Particle Interaction Networks (DPI-Net)
(Li et al., 2019) combined dynamic graphs, multi-step spatial propagation, and hierarchical structure
to simulate particles. CConv (Ummenhofer et al., 2020) used spatial convolutions to simulate ﬂuid
particles. Graph Network-based Simulators (GNS) (Sanchez-Gonzalez et al., 2020) computed dy-
namics via learned message-passing. Similar to particle-based systems, COPINGNet (Shao et al.,
2021) applies graph networks to simulate rod dynamics, where the discretized rod is the basic unit
similar to particle."
RELATED WORK,0.06224066390041494,"Previous work mostly adopted graph networks for simulations and required each particle to inter-
act with all its nearby particles without selective mechanism. In contrast, our SiT employs both
particle and interaction tokens and selectively focus on necessary particle interactions through atten-
tion mechanism. Experiments show that SiT surpasses existing GNN-based methods and has more
robust performances in generalizations."
RELATED WORK,0.06639004149377593,"Transformer.
Transformer (Vaswani et al., 2017) was designed for machine translation and
achieved state-of-the-art performance in many natural langruage processing tasks (Devlin et al.,
2019; Radford et al., 2019; Brown et al., 2020). Recently, Transformer starts to show great expand-
ability and applicability in many other ﬁelds, such as computer vision (Wang et al., 2018; Carion
et al., 2020; Dosovitskiy et al., 2021; Wang et al., 2021; Liu et al., 2021), and graph representations
(Zhou et al., 2020; Zhang et al., 2020; Dwivedi & Bresson, 2020). To our knowledge, no attempt
has been made to apply Transformer on physics simulation."
RELATED WORK,0.07053941908713693,"Our SiT models the interactions between particles by trainable sub-network given corresponding
particle tokens. The same notion of extracting potential semantics between nodes is also applied in
Graph Transformer (Dwivedi & Bresson, 2020), which we refer as GraphTrans for short. However,
there are differences in our formulations. Speciﬁcally, GraphTrans adopts element-wise product
between node representations followed by multi-layer perceptron (MLP) to update the interaction
embeddings, which store the attention scores in each dimension and are reduced to a scalar for fur-
ther attention mechanism to update node embeddings. In contrast, our model learns semantic tokens
for interactions through sub-network instead of element-wise product. When updating the particle
tokens, which are referred as node representations in GraphTrans, both particle and interaction to-
kens will attend to each other. The interaction tokens are not weighted scores any more. We adopt
GraphTrans (Dwivedi & Bresson, 2020) in particle-based simulation and compare it with SiT in
experiments. The quantitative results show that SiT achieves better results than GraphTrans."
METHODOLOGY,0.07468879668049792,"3
METHODOLOGY"
PROBLEM FORMULATION,0.07883817427385892,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.08298755186721991,"For a particle-based system composed of N particles, we use X t = {xt
i}N
i=1 to denote the system
state at time step t, where xt
i denotes the state of i-th particle. Speciﬁcally, xt
i = [pt
i, qt
i, ai], where
pt
i, qt
i ∈R3 refer to position and velocity, and ai ∈Rda represents ﬁxed particle attributes such
as its material type. The goal of a simulator is to learn a model φ(·) from previous rollouts of a
system to causally predict a rollout trajectory in a speciﬁc time period conditioned on the initial
system state X 0. The prediction is run in a recursive manner, where the simulator will predict
the state ˆ
X t+1 = φ(X t) at time step t + 1 based on the state X t = {xt
i} at time step t. In
practice, we will predict the velocities of particles ˆQt+1 = {ˆqt+1
i
}, and obtain their positions via
ˆpt+1
i
= pt
i + ∆t · ˆqt+1
i
, where ∆t is a domain-speciﬁc constant."
SIMULATION VIA VANILLA TRANSFORMER,0.08713692946058091,"3.2
SIMULATION VIA VANILLA TRANSFORMER"
SIMULATION VIA VANILLA TRANSFORMER,0.0912863070539419,"To accurately simulate the changes of a system over time, it is crucial to effectively model the
interactions among particles, as they indicate the energy transition of a system when constrained by"
SIMULATION VIA VANILLA TRANSFORMER,0.0954356846473029,Under review as a conference paper at ICLR 2022
SIMULATION VIA VANILLA TRANSFORMER,0.0995850622406639,"Figure 2: Overview of our transformer-based model with abstract tokens, which are optional. SiT is composed
of an encoding layer, stacks of L blocks extended from Transformer encoder blocks, and a decoding layer.
Given the particles states at time t, the encoding layer will ﬁrst encode the particles’ states and learnable
embeddings into particle tokens and abstract tokens, and initialize interaction tokens by particles’ states. In each
blocks, the sub-network ﬁrst updates interaction tokens given particle tokens. Then all tokens will dynamically
attend to each other via uniform attention and update particle and abstract tokens. An MLP is attached after
attention. Finally, the decoding layer will predict the particles’ velocities at time t+1 given the particle tokens."
SIMULATION VIA VANILLA TRANSFORMER,0.1037344398340249,"material characteristics and physical laws. However, it is infeasible to know a priori how should
particles interact with each other. Thus, a selective mechanism is needed to help the simulator focus
only on necessary interactions."
SIMULATION VIA VANILLA TRANSFORMER,0.1078838174273859,"Since Transformer (Vaswani et al., 2017) is capable of modeling the dynamical attention scores
between tokens via the self-attention module, we can regard the attention weights as the intensity
of connectivities and the importance of interactions, where the larger the attention weight the more
important the interaction is. Thus, it is naturally a good backbone of building an efﬁcient particle-
based simulator. To apply vanilla Transformer in particle-based simulation, we ﬁrst encode the states
of particles into corresponding particle tokens V = {vt
i} by"
SIMULATION VIA VANILLA TRANSFORMER,0.11203319502074689,"vt
i
=
f enc
V
(xt
i),
(1)
where vt
i ∈Rdh is a dh dimensional vector and f enc
V
(·) is an encoding layer implemented as a MLP.
Subsequently, particle interactions are achieved by L blocks of self-attention modules, where in the
l-th block, particle tokens will attend to each other selectively as:"
SIMULATION VIA VANILLA TRANSFORMER,0.11618257261410789,"vl+1,t
i
=
X"
SIMULATION VIA VANILLA TRANSFORMER,0.12033195020746888,"j
ˆwv
ijvl,t
j ,
(2)"
SIMULATION VIA VANILLA TRANSFORMER,0.12448132780082988,"ˆwv
ij
=
exp(wv
ij)
√dh · P"
SIMULATION VIA VANILLA TRANSFORMER,0.12863070539419086,"j exp(wv
ij),
(3)"
SIMULATION VIA VANILLA TRANSFORMER,0.13278008298755187,"wv
ij
=
(vl,t
i )⊤vl,t
j .
(4)
Since a system usually contains hundreds of particles and the interactions among particles occur
within the neighbors in our settings, considering all possible interactions, of which the number
quadratically increases with respect to the number of particles, is computationally redundant and in-
efﬁcient. Therefore, we follow previous literature Li et al. (2019); Sanchez-Gonzalez et al. (2020) to
assume that interactions of distant particles can be omitted, which is realized by a window function:
g(pt
i, pt
j)
=
I
 
||pt
i −pt
j||2 < R

,
(5)"
SIMULATION VIA VANILLA TRANSFORMER,0.13692946058091288,"where I(condition) is a indicator function that returns 1 if the condition is satisﬁed, and R deﬁnes
the extent of the window. This window function will generate a mask to retain only interactions
between neighboring particles as potential candidates in the self-attention modules."
SIMULATION VIA VANILLA TRANSFORMER,0.14107883817427386,"To predict particle states in the next time step, a decoding layer is applied to the updated token of
i-th particle to obtain its velocity:"
SIMULATION VIA VANILLA TRANSFORMER,0.14522821576763487,"ˆqt+1
i
=
f dec
V
(vL,t
i
),
(6)"
SIMULATION VIA VANILLA TRANSFORMER,0.14937759336099585,"where f dec
V
(·) is implemented by another MLP."
SIMULATION VIA VANILLA TRANSFORMER,0.15352697095435686,"3.3
SIMULATION TRANSFORMER (SIT)"
SIMULATION VIA VANILLA TRANSFORMER,0.15767634854771784,"Although the vanilla Transformer provides a ﬂexible approach for particle-based simulation, directly
applying it leads to inferior simulation results as shown in our experiments. In particular, a vanilla"
SIMULATION VIA VANILLA TRANSFORMER,0.16182572614107885,Under review as a conference paper at ICLR 2022
SIMULATION VIA VANILLA TRANSFORMER,0.16597510373443983,"Transformer uses attention weights, which are scalars obtained via dot-product, to represent particle
interactions. A single scalar is insufﬁcient to reﬂect the rich semantics of particle interactions. For
example, in the case of ﬂuid water, when two particles get closer and closer, their interaction will at
ﬁrst act as surface tension to push them towards each other. And when they are sufﬁciently close, the
repulsion between them will dominate the interaction and stop them from further moving towards
each other. In addition, since the semantics of one domain is scattered in the tokens of different par-
ticles, domain-speciﬁc semantics, such as the shape of rigid cubic, are mixed with domain-agnostic
semantics, such as the stickiness attribute of materials. In such cases, the former may be misinter-
preted as the latter, leading to poor generalization ability. To overcome the limitations of vanilla
Transformer, we propose a novel Transformer-based simulator, referred to as Simulation Trans-
former (SiT), that extends vanilla Transformer with two new types of tokens, namely interactions
tokens and abstract tokens. An overview of SiT is included in Figure 2."
SIMULATION VIA VANILLA TRANSFORMER,0.17012448132780084,"Interaction Tokens.
To capture the rich semantics of particle interactions, instead of a scalar, SiT
assigns an interaction token to each potential particle interaction. Speciﬁcally, the token for the
interaction between particles xt
i and xt
j is initialized by"
SIMULATION VIA VANILLA TRANSFORMER,0.17427385892116182,"ut
ij
=
f enc
I
(xt
i, xt
j),
(7)"
SIMULATION VIA VANILLA TRANSFORMER,0.17842323651452283,"where ut
ij ∈Rdh is a dh dimensional vector and f enc
I
(·) is the encoding function for interactions."
SIMULATION VIA VANILLA TRANSFORMER,0.1825726141078838,"Subsequently, in each self-attention block SiT will update the interaction tokens as"
SIMULATION VIA VANILLA TRANSFORMER,0.18672199170124482,"ul+1,t
ij
=
ψ

vl,t
i , vl,t
j , ul,t
ij , g(pt
i, pt
j)

,
(8)"
SIMULATION VIA VANILLA TRANSFORMER,0.1908713692946058,"where ψ(·) is a learnable sub-network shared for all interaction tokens. Inspired by GNN-based
methods (Mrowca et al., 2018; Li et al., 2019; Sanchez-Gonzalez et al., 2020) , SiT adopts the
learnable sub-network to enlarge its capacity for handling complex particle interactions. In practice,
the sub-network is implemented as"
SIMULATION VIA VANILLA TRANSFORMER,0.1950207468879668,"ψ

vl,t
i , vl,t
j , ul,t
ij , g(pt
i, pt
j)

=
MLP([vl,t
i , vl,t
j , ul,t
ij ]) · g(pt
i, pt
j).
(9)"
SIMULATION VIA VANILLA TRANSFORMER,0.1991701244813278,"As is shown in Figure 6, the update of particle tokens in the self-attention module is thus achieved
by"
SIMULATION VIA VANILLA TRANSFORMER,0.2033195020746888,"vl+1,t
i
=
α
X"
SIMULATION VIA VANILLA TRANSFORMER,0.2074688796680498,"j
ˆwv
ijvl,t
j + β
X"
SIMULATION VIA VANILLA TRANSFORMER,0.21161825726141079,"k
ˆwu
ikul+1,t
ik
,
(10)"
SIMULATION VIA VANILLA TRANSFORMER,0.2157676348547718,"ˆwv
ij
=
exp(wv
j )
√dh ·

α P"
SIMULATION VIA VANILLA TRANSFORMER,0.21991701244813278,"j exp(wv
j ) + β P"
SIMULATION VIA VANILLA TRANSFORMER,0.22406639004149378,"k exp(wu
ik)
,
(11)"
SIMULATION VIA VANILLA TRANSFORMER,0.22821576763485477,"ˆwu
ik
=
exp(wu
ik)
√dh ·

α P
j exp(wv
j ) + β P
k exp(wu
ik)
,
(12)"
SIMULATION VIA VANILLA TRANSFORMER,0.23236514522821577,"wv
ij
=
(vl,t
i )⊤vl,t
j ,
(13)"
SIMULATION VIA VANILLA TRANSFORMER,0.23651452282157676,"wu
ik
=
(vl,t
i )⊤ul+1,t
ik
,
(14)
where α, β ∈{0, 1} are the gate controllers, j and k are the indexes of particles within the recep-
tion window deﬁned by Equation 5. Equation 13 computes the attention scores between particle
tokens, and Equation 14 computes the attention scores between particle and interaction tokens. Sub-
sequently, these attention scores are normalized by Equation 11 and Equation 12. They are then used
by Equation 10 to aggregate semantics of particle and interaction tokens, where the gate controllers
α, β are hyper-parameters that work as balancing coefﬁcients. When α = 1, β = 0, Equation 10
becomes the original self-attention formula in vanilla Transformer. The multi-head attention version
of Equation 10 can be found in Appendix 6.1.2."
SIMULATION VIA VANILLA TRANSFORMER,0.24066390041493776,"It is worth noting that SiT provides a more ﬂexible way for further extensions when compared to
vanilla Transformer. Speciﬁcally, we can change the value of α and β in Equation 10 to adjust the
signiﬁcance of different tokens. And changing the window function in Equation 5 allows SiT to
consider different interaction patterns. Finally, by adjusting the learnable sub-network, we can also
generalize SiT to consider more complex particle interactions such as interactions beyond a pair of
particles and interactions of interactions."
SIMULATION VIA VANILLA TRANSFORMER,0.24481327800829875,Under review as a conference paper at ICLR 2022
ABSTRACT,0.24896265560165975,"Abstract Tokens.
To improve the generalization ability of SiT and disentangle domain-speciﬁc se-
mantics from its domain-agnostic counterparts, we further apply SiT with material-speciﬁc abstract
tokens."
ABSTRACT,0.25311203319502074,"For Na types of materials, SiT adopts Na abstract tokens A = {ak}Na
k=1, each of which is a learn-
able vector of the same length of particle tokens. Ideally, the abstract token ak should capture the
domain-agnostic semantics of k-th material. They act as particle tokens for additional abstract par-
ticles, following the same update formula as particle tokens. Therefore with Na abstract tokens SiT
will have N + Na particle tokens in total: {a1, · · · , aNa, xt
1, · · · , xt
N}, as is show in Figure 2.
Although abstract tokens share the same update formula as particle tokens, unlike particle tokens
that decide their potential interactions via the window function in Equation 5, abstract tokens are
forced to interact with all particle tokens of its corresponding material, which is achieved by setting
the reception ﬁeld of abstract tokens to the particles of the same material."
ABSTRACT,0.2572614107883817,"Once abstract tokens capture domain-agnostic semantics, they can be reused by SiT when general-
izing to domains that have same materials but vary in particle amount and conﬁguration."
TRAINING OBJECTIVE,0.26141078838174275,"3.4
TRAINING OBJECTIVE"
TRAINING OBJECTIVE,0.26556016597510373,"To train SiT with previous rollouts, the standard mean square error (MSE) loss is applied on the
output of our SiT:"
TRAINING OBJECTIVE,0.2697095435684647,"MSE( ˆQt+1, Qt+1)
=
1
N X i"
TRAINING OBJECTIVE,0.27385892116182575,"||ˆqt+1
i
−qt+1
i
||2"
TRAINING OBJECTIVE,0.27800829875518673,"2,
(15)"
TRAINING OBJECTIVE,0.2821576763485477,"where ˆQt+1 = {ˆqt+1
i
}N
i=1 is the estimated velocity , Qt+1 = {qt+1
i
}N
i=1 is the ground truth, and
|| · ||2 is L2 norm."
TRAINING OBJECTIVE,0.2863070539419087,"While the MSE loss works well for cases with relatively low complexity, in practice some domains
often contain multiple types of materials with imbalance numbers of particles. In such cases, the
MSE loss will be biased to the material with more particles. Therefore, we further apply a material
weighted MSE (WMSE) loss to reduce the effect of imbalance:"
TRAINING OBJECTIVE,0.29045643153526973,"WMSE( ˆQt+1, Qt+1)
=
1
K X k"
NK,0.2946058091286307,"1
Nk X"
NK,0.2987551867219917,"i
||ˆqt+1
i,k −qt+1
i,k ||2
2,
(16)"
NK,0.3029045643153527,"where K is the number of material types, Nk is the number of particles belonging to the k-th material
and N = P"
NK,0.3070539419087137,"k Nk, ˆQt+1
k
= {ˆqt+1
i,k }Nk
i=1 and Qt+1
k
= {qt+1
i,k }Nk
i=1 denote the estimated velocity and
ground truth for k-th material, and ˆQt+1 = { ˆQt+1
k
}K
k=1, Qt+1 = {Qt+1
k
}K
k=1,"
EXPERIMENTS,0.3112033195020747,"4
EXPERIMENTS"
BASE ENVIRONMENTS,0.3153526970954357,"4.1
BASE ENVIRONMENTS"
BASE ENVIRONMENTS,0.31950207468879666,"We adopt the environments commonly used in the literature (Li et al., 2019; Sanchez-Gonzalez et al.,
2020; Ummenhofer et al., 2020). Samples are displayed in Figure 1. There are four environments
in total: FluidFall is a basic simulation for two drops of water; FluidShake is more complex and
simulate the water in a randomly moving box; BoxBath simulates the water washing a rigid cubic
in ﬁxed box; RiceGrip simulates the interactions between deformable rice and two rigid grippers.
More details of data generalization can be found in Appendix 6.1.3."
BASE ENVIRONMENTS,0.3236514522821577,"We compare SiT with 4 recent approaches: DPI-Net (Li et al., 2019), CConv (Ummenhofer et al.,
2020), GNS (Sanchez-Gonzalez et al., 2020), and GraphTrans (Dwivedi & Bresson, 2020). For fair
comparison, we adopt similar training schedules. Quantitative results and model parameters are in
Table 5. Qualitative results are in Figure 3. More details can be found in Appendix 6.1.3 and 6.2.2"
BASE ENVIRONMENTS,0.3278008298755187,"SiT achieves superior performance on all environments with fewer model parameters. The effec-
tiveness of abstract tokens are more obvious for RiceGrip and BoxBath, which are more complex.
Quantitative results show that SiT can better simulate the dynamics on different environments."
BASE ENVIRONMENTS,0.33195020746887965,"Comparison with DPI-Net and GNS. DPI-Net (Li et al., 2019) and GNS (Sanchez-Gonzalez et al.,
2020) adopt message-passing graph networks for particle-based simulation. As shown in the results,"
BASE ENVIRONMENTS,0.3360995850622407,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.34024896265560167,"Table 1:
Quantitative results and model parameters on four environments. SiT achieves superior and rea-
sonable performance with less model parameters on all environments. When adding trainable abstract tokens,
which introduce a few more parameters, SiT, marked by +, further improves performance in complex envi-
ronments. As CConv is designed for ﬂuid dynamics, we report the results on BoxBath for reference of the
simulation on ﬂuid parts, which is marked by *."
BASE ENVIRONMENTS,0.34439834024896265,"Methods
FluidFall
FluidShake
RiceGrip
BoxBath"
BASE ENVIRONMENTS,0.34854771784232363,"MSE
Params
MSE
Params
MSE
Params
MSE
WMSE
Params"
BASE ENVIRONMENTS,0.35269709543568467,"DPI-Net
0.12±0.06 0.62M 1.43±0.52 1.02M 0.11±0.21 2.17M
1.91±0.08
1.52±0.33
1.98M
CConv
0.08±0.02 0.84M 1.34±0.45 0.84M
N/A
N/A
1.98 ±0.11* 3.06±0.47* 0.84M
GNS
0.05±0.02 1.59M 1.45±0.55 1.59M 0.38±0.25 1.60M
1.77±0.87
2.62±0.87
1.59M
GraphTrans 0.12±0.03 0.49M 8.77±3.97 0.96M 0.36±0.27 0.96M
3.17±0.07
2.33±0.47
0.97M"
BASE ENVIRONMENTS,0.35684647302904565,"SiT (Ours) 0.05±0.02 0.40M 1.08±0.36 0.77M 0.15±0.12 0.82M
1.74±0.08
1.42±0.29
0.77M
SiT+ (Ours) 0.04±0.01 0.40M 1.08±0.39 0.77M 0.07±0.07 0.82M
1.57±0.06
1.39±0.31
0.77M"
BASE ENVIRONMENTS,0.36099585062240663,"Figure 3: Qualitative results on FluidFall and BoxBath. When the two drops are getting closer in FluidFall but
not close enough to merge, previous methods are likely to mix them together due to the newly added neighbors
from different drops and incorrect interactions. But SiT can still predict faithful rollouts."
BASE ENVIRONMENTS,0.3651452282157676,"SiT achieves better performance, as the selective mechanism can eliminate the unnecessary interac-
tions, which is further demonstrated by the heat map in Appendix 6.2.1. SiT with abstract tokens
further improves the performances especially on RiceGrip and BoxBath, suggesting the effective-
ness of abstract tokens in modeling complex deformations and multi-materials interactions.
For
FluidFall, though the MSEs are close to each other, SiT can better maintain the shape of drops while
DPI-Net and GNS fail because they do not have selective mechanism and equally treat the incorrect
interactions, which are introduced by the newly added neighbors from different drops when they get
closer but not close enough to merge."
BASE ENVIRONMENTS,0.36929460580912865,"Comparison with CConv. CConv (Ummenhofer et al., 2020) designs convolutional layers carefully
tailored to modeling ﬂuid dynamics, such as an SPH(Monaghan, 1992)-like local kernel, different
sub-networks for ﬂuid and boundary particles. Without modeling interactions, CConv propagates
information only among particles weighted by distances. On the other hand, SiT assigns high-level
tokens for interactions and achieves better results, suggesting the necessity of interaction tokens.
Notice that CConv is not suitable for rigid box, we only report result on BoxBath for reference of
simulation on ﬂuid parts."
BASE ENVIRONMENTS,0.37344398340248963,"Comparison with GraphTrans (Dwivedi & Bresson, 2020). The GraphTrans is also a Transformer-
based method and updates the interactions using element-wise product from node representations.
We represent particles by nodes and adopt layers in GraphTrans as our backbone, which and use the
same hidden dimensions and number of blocks. Other settings are completely the same as SiT for
better comparison. As is shown in experiments, GraphTrans fails to simulate particles on all cases
with much higher errors. This is because our SiT directly models the interactions by interaction
tokens generated by sub-network, which is necessary to capture high-level semantics."
GENERALIAZTIONS,0.3775933609958506,"4.2
GENERALIAZTIONS"
GENERALIAZTIONS,0.3817427385892116,"To challenge the robustness of our model on more complex settings, we add more particles for
RichGrip and FluidShake, and change the size and shape of rigid object for BoxBath. Table 5 shows
the upper and lower bound for the number of particles in training set and generalization validation
set, as well as the shapes of rigid object."
GENERALIAZTIONS,0.38589211618257263,"We mainly compare SiT with DPI-Net, which has fairly good and reasonable performance on all
environments. Quantitative results are summarized in Table 5, while qualitative results are depicted
in Figure 4 and Figure 1 (b). As is shown, SiT with abstract tokens can better simulate particle dy-"
GENERALIAZTIONS,0.3900414937759336,Under review as a conference paper at ICLR 2022
GENERALIAZTIONS,0.3941908713692946,"Table 2: MSEs on generalizations. The lists of numbers in FluidShake and RiceGrip are the range of particles,
while the tuples in BoxBath denotes number of ﬂuid particles, number of rigid particles, and shape of rigid
objects respectively. Training settings are marked by *. We compare SiT with abstract tokens and DPI-Net.
SiT+ achieves the best results on all cases."
GENERALIAZTIONS,0.3983402489626556,"Methods
FluidShake [450,627]*
RiceGrip [570,980]*"
GENERALIAZTIONS,0.4024896265560166,"[720,900]
[924,1080]
[1104,1368]
[1060,1345]
[1347,1640]"
GENERALIAZTIONS,0.4066390041493776,"DPI-Net
2.14±0.36
2.78±0.58
3.53±0.71
2.33±30.64
0.87±4.00
SiT+ (Ours)
1.54±0.37
1.90±0.63
2.30±0.69
0.14±0.10
0.17±0.15"
GENERALIAZTIONS,0.4107883817427386,"Methods
BoxBath (960,64,cubic)*"
GENERALIAZTIONS,0.4149377593360996,"(1280,64,cubic)
(960,41,bunny)
(960,125,cubic)
(960,136,ball)
(960,120,cuboid)"
GENERALIAZTIONS,0.4190871369294606,"DPI-Net
4.28±0.11
2.49±0.23
2.75±0.42
2.71±0.35
2.97±0.38
SiT+ (Ours)
2.64±0.11
1.72±0.08
1.74±0.14
1.67±0.13
1.92±0.32"
GENERALIAZTIONS,0.42323651452282157,"Figure 4: Rendered rollouts on generalized FluidShake and BoxBath. More particles participate in simulations.
SiT with abstract tokens (SiT+) can still faithfully predict the wave of ﬂuid in FluidShake and the positions of
rigid cubic in BoxBath."
GENERALIAZTIONS,0.42738589211618255,"namics in generalized environment. suggesting that the abstract tokens and the selective mechanism
involving interaction tokens can provide more stable and robust performance. More details about
the results and rollouts can be found in Appendix 6.2.3."
ABLATION STUDIES,0.4315352697095436,"4.3
ABLATION STUDIES"
ABLATION STUDIES,0.43568464730290457,"We comprehensively analyze our SiT and explore the effectiveness of our model in the following
ways: (a) the necessity of interaction tokens; (b) the architectures of sub-network, which generates
interaction tokens; (c) the values for gate controllers α, β; (d) the usage of multi-head in uniform
attention; (e) the sensitiveness of SiT to radius R; (f) the effectiveness of abstract tokens.
We
conduct our experiments on FluidShake for (a) to (e). We verify (f) on BoxBath and RiceGrip. The
quantitative results are in Table 3 and Table 4."
ABLATION STUDIES,0.43983402489626555,"Necessity of interaction tokens. We apply vanilla Transformer encoder by conﬁguration A, which
uses the same hidden dimension and number of blocks as SiT, and SiT by conﬁguration B, which
only changes the output of sub-network to one dimension. As shown in Table 3, both conﬁguration A
and B have low performance, suggesting the scalars are insufﬁcient to describe complex interactions."
ABLATION STUDIES,0.44398340248962653,"Architectures of sub-network. Conﬁguration C adopts element-wise product between particle
tokens, which follows the GraphTrans (Dwivedi & Bresson, 2020); conﬁguration D adopts self-
attention blocks, where q is the interaction token from previous layer, k and v are the neighbor
particle tokens. As shown in Table 3, conﬁguration C and D have worse performance than SiT,
which suggests that the element-wise product is insufﬁcient to model complex interactions, while
self-attention block introduces more parameters and complexities, making it harder to train."
ABLATION STUDIES,0.44813278008298757,"Hyper-paramters for α, β. We set α = 1, β = 0 in conﬁguration E, α = 1, β = 1 in conﬁguration
F, and α = 0, β = 1 in conﬁguration H, Consequently, both conﬁguration E and H achieve good
results, suggesting that the participants of interaction tokens are the key to boost the performance.
In practice, we use conﬁguration H for SiT which shows the best results."
ABLATION STUDIES,0.45228215767634855,"Multi-head uniform attention. We apply multi-head mechanism in conﬁguration G and set head
number as 8. The results show that multi-head mechanism can bring us some improvement. But it
also introduces more parameters and consumes lots of memory when computing."
ABLATION STUDIES,0.45643153526970953,"Sensitiveness to R. Quantitative results are reported on FluidShake. As shown in Table 4, SiT is
more robust when varying the radius R, suggesting the effectiveness of selective mechanism and
robustness of SiT."
ABLATION STUDIES,0.4605809128630705,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.46473029045643155,"Table 3: Ablation studies. We comprehensively explore the effectiveness of SiT, including the necessity of
interaction tokens, the structure of sub-network, the value of α, β, and the usage of multi-head in uniform
attention. We report MSEs(1e-2) on FluidShake, which is a complex environments involving outer forces."
ABLATION STUDIES,0.46887966804979253,"Conﬁgurations
Interaction modeling
α
β
Multi-head
FluidShake"
ABLATION STUDIES,0.4730290456431535,"A(Transformer)
Scalar by dot product
1
0
✓
21.03±10.54
B
Scalar by MLP
0
1
300.43±70.16
C
Tokens by element-wise product
0
1
17.70±6.99
D
Tokens by self attention
0
1
3.17±0.81
E
Tokens by MLP
1
0
14.27±42.08
F
Tokens by MLP
1
1
1.19±0.44
G
Tokens by MLP
0
1
✓
1.04±0.36
H(SiT)
Tokens by MLP
0
1
1.08±0.36"
ABLATION STUDIES,0.47717842323651455,"Table 4: Ablation studies on sensitiveness to radius R and abstract tokens. The left parts are MSEs(1e-2) on
FluidShake, which suggest that SiT is more robust within small range of R. Our default setting on all domains
is marked by ∗. The right parts are MSEs(1e-2) on RiceGrip and BoxBath. We replace abstract tokens with
dummy tokens, which are ﬁxed value vectors but have same connectivities as abstract tokens."
ABLATION STUDIES,0.48132780082987553,"Methods
R = 0.07
R∗= 0.08
R = 0.09
Methods
RiceGrip
BoxBath"
ABLATION STUDIES,0.4854771784232365,"DPI-Net
2.60±0.56
1.43±0.52
1.66±0.48
SiT w dummy tokens
2.12±0.46
3.98±0.09
SiT
1.38±0.36
1.08±0.36
1.37±0.35
SiT +
0.07±0.07
1.57±0.06"
ABLATION STUDIES,0.4896265560165975,"Effectiveness of abstract tokens. We replace the abstract tokens with dummy tokens, which are
randomly initialized vectors with ﬁxed values but have same connectivities as abstract tokens. As
shown in 4, SiT with dummy tokens fails on both domains, suggesting the abstract tokens are able
to learn the materials semantics and boost SiT’s performances."
FEW SHOTS LEARNING,0.49377593360995853,"4.4
FEW SHOTS LEARNING"
FEW SHOTS LEARNING,0.4979253112033195,"As shown in Figure 5, we conduct few shots learning experiments on FluidShake using SiT without
abstract tokens. Even when trained on 60% of data, SiT achieves similar MSEs comparing with
DPI-Net trained on 100% of data. When trained with further less data, SiT learns not well enough.
As the number of particles differs in each rollout, SiT does not have enough training on all possible
cases. On the other hand, the weights are not shared in SiT and would not learn enough on less data
comparing with the shared-weight blocks in DPI-Net. SiT is more robust with no less than 60% of
the whole training data, but is sensitive to extremely less data."
FEW SHOTS LEARNING,0.5020746887966805,"Figure 5: Few shots learning on FluidShake. Figure (a) shows the rollouts. Figure (b) draws the MSE curves
as the number of training examples increases. Our model are robust even with only 60% of training examples."
CONCLUSION,0.5062240663900415,"5
CONCLUSION"
CONCLUSION,0.5103734439834025,"In this paper, we propose Simulation Transformer (SiT) and introduce interaction tokens and abstract
tokens to simulate domains of different complexity and materials, including hundreds and thousands
of particles. Our experimental results show the necessity and effectiveness of interaction and abstract
tokens as well as selective mechanism in attention for particle-based simulation. Generalization
results further suggest the robustness of SiT. SiT is also ﬂexible for extensions and applications,
such as more complex and efﬁcient architectures for sub-network, changing the particle dynamics
by attending to alternative abstract tokens with different materials semantics. Finally, SiT makes a
successful attempt to apply Transformer into physics simulation and achieve superior performances
over existing methods."
CONCLUSION,0.5145228215767634,Under review as a conference paper at ICLR 2022
REFERENCES,0.5186721991701245,REFERENCES
REFERENCES,0.5228215767634855,"Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray
Kavukcuoglu. Interaction networks for learning about objects, relations and physics. In Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, 2016."
REFERENCES,0.5269709543568465,"Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geo-
metric deep learning: Going beyond euclidean data. IEEE Signal Process. Mag., 2017."
REFERENCES,0.5311203319502075,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neu-
ral Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.5352697095435685,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision - ECCV
2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, 2020."
REFERENCES,0.5394190871369294,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers), 2019."
REFERENCES,0.5435684647302904,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021, 2021."
REFERENCES,0.5477178423236515,"Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.
CoRR, 2020."
REFERENCES,0.5518672199170125,"Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady ﬂow ap-
proximation. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, 2016."
REFERENCES,0.5560165975103735,"Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or.
MeshCNN: a network with an edge. ACM Trans. Graph., 2019."
REFERENCES,0.5601659751037344,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. CoRR, 2016."
REFERENCES,0.5643153526970954,"Sangseung Lee and Donghyun You. Data-driven prediction of unsteady ﬂow over a circular cylinder
using deep learning. Journal of Fluid Mechanics, 2019."
REFERENCES,0.5684647302904564,"Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning particle
dynamics for manipulating rigid bodies, deformable objects, and ﬂuids.
In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,
2019."
REFERENCES,0.5726141078838174,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin Transformer: Hierarchical vision transformer using shifted windows. CoRR, 2021."
REFERENCES,0.5767634854771784,"Ran Luo, Tianjia Shao, Huamin Wang, Weiwei Xu, Xiang Chen, Kun Zhou, and Yin Yang. NNWarp:
Neural network-based nonlinear deformation. IEEE Trans. Vis. Comput. Graph., 2020."
REFERENCES,0.5809128630705395,"Miles Macklin and Matthias M¨uller. Position based ﬂuids. ACM Trans. Graph., 2013."
REFERENCES,0.5850622406639004,Under review as a conference paper at ICLR 2022
REFERENCES,0.5892116182572614,"Joe J Monaghan. Smoothed particle hydrodynamics. Annual review of astronomy and astrophysics,
1992."
REFERENCES,0.5933609958506224,"Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei, Josh Tenenbaum, and
Daniel L. Yamins. Flexible neural representation for physics prediction. In Advances in Neu-
ral Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, 2018."
REFERENCES,0.5975103734439834,"Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. PolyGen: An autoregressive
generative model of 3d meshes. In Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event, 2020."
REFERENCES,0.6016597510373444,"Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-
based simulation with graph networks. In 9th International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021."
REFERENCES,0.6058091286307054,"Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C. Lin. Scalable differentiable physics for
learning and control. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, 2020."
REFERENCES,0.6099585062240664,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 2019."
REFERENCES,0.6141078838174274,"Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W.
Battaglia. Learning to simulate complex physics with graph networks. In Proceedings of the 37th
International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,
2020."
REFERENCES,0.6182572614107884,"Connor Schenck and Dieter Fox. SPNets: Differentiable ﬂuid dynamics for deep neural networks.
In 2nd Annual Conference on Robot Learning, CoRL 2018, Z¨urich, Switzerland, 29-31 October
2018, Proceedings, 2018."
REFERENCES,0.6224066390041494,"Han Shao, Tassilo Kugelstadt, Torsten H¨adrich, Wojtek Palubicki, Jan Bender, Soren Pirk, and Do-
minik Michels. Accurately solving rod dynamics with graph learning. In Thirty-Fifth Conference
on Neural Information Processing Systems, 2021."
REFERENCES,0.6265560165975104,"Nils Thuerey, Konstantin Weißenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for
reynolds-averaged navier–stokes simulations of airfoil ﬂows. AIAA Journal, 2020."
REFERENCES,0.6307053941908713,"Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian ﬂuid simula-
tion with continuous convolutions. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020."
REFERENCES,0.6348547717842323,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, December 4-9, 2017, Long Beach, CA, USA, 2017."
REFERENCES,0.6390041493775933,"Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. MaX-DeepLab:
End-to-end panoptic segmentation with mask transformers. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021."
REFERENCES,0.6431535269709544,"Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-
informed deep learning for turbulent ﬂow prediction. In KDD ’20: The 26th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27,
2020, 2020."
REFERENCES,0.6473029045643154,"Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,
UT, USA, June 18-22, 2018, 2018."
REFERENCES,0.6514522821576764,"Zehang Weng, Fabian Paus, Anastasiia Varava, Hang Yin, Tamim Asfour, and Danica Kragic.
Graph-based task-speciﬁc prediction models for interactions between deformable and rigid ob-
jects. CoRR, 2021."
REFERENCES,0.6556016597510373,Under review as a conference paper at ICLR 2022
REFERENCES,0.6597510373443983,"Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-Bert: Only attention is needed
for learning graph representations. CoRR, 2020."
REFERENCES,0.6639004149377593,"Dawei Zhou, Lecheng Zheng, Jiawei Han, and Jingrui He. A data-driven graph generative model for
temporal interaction networks. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, 2020."
REFERENCES,0.6680497925311203,Under review as a conference paper at ICLR 2022
APPENDIX,0.6721991701244814,"6
APPENDIX"
MODEL DETAILS,0.6763485477178424,"6.1
MODEL DETAILS"
UNIFORM ATTENTION,0.6804979253112033,"6.1.1
UNIFORM ATTENTION"
UNIFORM ATTENTION,0.6846473029045643,"Figure 6: Overview of our uniform attention. The abstract tokens are optional. The tokens will
attend to each other and update accordingly."
UNIFORM ATTENTION,0.6887966804979253,"Figure 6 illustrates how our uniform attention works. Our uniform attention extend self-attention
(Vaswani et al., 2017) by additional tokens. The queries are the abstract tokens, which are optional,
and particle tokens, while the keys and values are all three types of tokens. Each token will attend to
tokens within its reception ﬁelds deﬁned by the window function 5. For the abstract tokens, they are
forced to attend only to the particles belonging to the same material. The output of uniform attention
is the updated tokens except interaction tokens."
MULTI-HEAD UNIFORM ATTENTION,0.6929460580912863,"6.1.2
MULTI-HEAD UNIFORM ATTENTION"
MULTI-HEAD UNIFORM ATTENTION,0.6970954356846473,The multi-head version of uniform attention is formulated by
MULTI-HEAD UNIFORM ATTENTION,0.7012448132780082,"vl,t
i
=
W l [SA1, SA2, · · · , SAM]⊤,
(17)"
MULTI-HEAD UNIFORM ATTENTION,0.7053941908713693,"SAm
=
α
X"
MULTI-HEAD UNIFORM ATTENTION,0.7095435684647303,"j
ˆwm,v
ij
(V m,lvl−1,t
j
) + β
X"
MULTI-HEAD UNIFORM ATTENTION,0.7136929460580913,"k
ˆwm,u
ik (V m,lul,t
ik),
(18)"
MULTI-HEAD UNIFORM ATTENTION,0.7178423236514523,"ˆwm,v
ij
=
exp(wm,v
j
)
√dh ·

α P"
MULTI-HEAD UNIFORM ATTENTION,0.7219917012448133,"j exp(wm,v
j
) + β P"
MULTI-HEAD UNIFORM ATTENTION,0.7261410788381742,"k exp(wm,u
ik )
,
(19)"
MULTI-HEAD UNIFORM ATTENTION,0.7302904564315352,"ˆwm,u
ik
=
exp(wm,u
ik )
√dh ·

α P
j exp(wm,v
j
) + β P
k exp(wm,u
ik )
,
(20)"
MULTI-HEAD UNIFORM ATTENTION,0.7344398340248963,"wm,v
ij
=
(Qm,lvl−1,t
i
)⊤(Km,lvl−1,t
j
),
(21)"
MULTI-HEAD UNIFORM ATTENTION,0.7385892116182573,"wm,u
ik
=
(Qm,lvl−1,t
i
)⊤(Km,lul,t
ik),
(22)"
MULTI-HEAD UNIFORM ATTENTION,0.7427385892116183,"where m ∈1, 2, · · · , M is the index of head, W l ∈Rdh×dh is the output weight for the concate-
nation of M heads; Qm,l, Km,l, V m,; are the projection weights of query, key, value for head m at
block l. The rest symbols are the same with those in uniform attention."
IMPLEMENTATION DETAILS,0.7468879668049793,"6.1.3
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.7510373443983402,"Inputs and outputs details. For FluidFall, FluidShake, and BoxBath, we only use particles’ states at
time t as inputs and output the velocities at time t + 1. For RiceGrip, we concatenate particles states"
IMPLEMENTATION DETAILS,0.7551867219917012,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.7593360995850622,"from t−2 to t as inputs and output 6-dim vector for the velocity of the current observed position and
the resting position, which is the same setting as DPI-Net for better comparison. For BoxBath, we
output 7-dim vectors, where 3 dimensions for the predicted velocities, and 4 dimensions for rotation
constrains. The rotation constraints are applied only on rigid particles which predict the rotation
velocities. We use particles’ states at time t as inputs and output the velocities at time t + 1 except
RiceGrip, where we concatenate particles states from t −2 to t as inputs for better comparison with
DPI-Net (Li et al., 2019). All states of particles, such as the positions, velocities, and accelerations,
are ﬁrst normalized by mean and standard deviations calculated on corresponding training set."
IMPLEMENTATION DETAILS,0.7634854771784232,"SiT details. We set R = 0.08 across all environments. All hidden dimensions in SiT are 128
for default. The MLP after uniform attention has two layers with dimensions 512 and 128. We
use 2 blocks shown in Figure 2 for FluidFall and 4 blocks for the other three environments. We set
α = 0, β = 1 in our model for default, which show the best results in ablation studies. The learnable
embeddings for abstract tokens have the same dimensions as inputs."
IMPLEMENTATION DETAILS,0.7676348547717843,"Training. We train four models independently on these four environments. For common settings,
we use Adam optimizer with initial learning rate 0.0008 and plateau scheduler with patience 3 and
decreasing factor 0.8. We choose batch size 16 for all environments and train the models for 5
epochs on FluidShake, 5 epochs for BoxBath, 13 epochs for FluidFall, and 20 epochs for RiceGrip.
No augmentation is involved, such as adding noises to particles states during training. We adopt
WMSE loss on BoxBath and MSE loss on the rest environments for training."
IMPLEMENTATION DETAILS,0.7717842323651453,"Evaluation. We evaluate the positions between predictions and ground truths by MSE as is shown
in equation 15. For BoxBath, we also report the WMSE results by equation 16 to better show the
performance of simulating different materials."
IMPLEMENTATION DETAILS,0.7759336099585062,"Baseline details. For fair comparison, the following settings are the same with SiT: inputs for
models, number of training epochs on different environments, learning rate schedules, and training
loss on velocities. On BoxBath, all baselines adopt the rigid constraints except CConv, which is
designed speciﬁcally for ﬂuid dynamics and we use it to mainly compare the ﬂuid simulations. The
model-related hyper-parameters for baselines are the same as the original papers, such as the number
of message passing is 10 for GNS. Other hyper-parameters for baselines are ﬁrst chosen the same as
their original papers, and then ﬁne-tuned within a small range of changes. For example, in terms of
the initial learning rate, 0.0001 works better on GNS while others adopt 0.0008."
EXPERIMENTS,0.7800829875518672,"6.2
EXPERIMENTS"
ADDTIONAL ABLATION STUDIES,0.7842323651452282,"6.2.1
ADDTIONAL ABLATION STUDIES"
ADDTIONAL ABLATION STUDIES,0.7883817427385892,"We also visualize the attention scores in each block for a speciﬁc particle on BoxBath in Figure
7. The red dot in the center is the given particle. Other dots are the neighors. The red particle
will attend selectively to its neighbors rather than treating them equally. Different layers focus on
different aspect of semantics. Speciﬁcally, the particle will focus mainly on closer neightbors in the
ﬁrst layer as is shown in Figure 7 (b)."
BASE ENVIRONMENTS,0.7925311203319502,"6.2.2
BASE ENVIRONMENTS"
BASE ENVIRONMENTS,0.7966804979253111,"Dataset details. We use the same setting for our datasets as mentioned in previous work (Li et al.,
2019). FluidFall contains two ﬂuid drops with different sizes. The size for each drop are randomly
generated with one drop larger than the other. Position and viscosity for drops are randomly ini-
tialized. This environment contains 189 particles with 121 frames for each rollout. There are 2700
rollouts in training set and 300 rollouts in validation set. FluidShake simulates the water in a mov-
ing box. The speed of the box is randomly generated at each timestamp. In addition, the size of the
box and the number of particles are various for different rollouts. In basic training and validation
sets, the number of particles changes from 450 to 627. This environment has 301 frames for each
rollout. There are 1800 rollouts in training set and 200 rollouts in validation set. RiceGrip contains
two grippers and a sticky rice. The grippers’ positions and orientation are randomly initialized. The
number of particles for rice changes from 570 to 980 with 41 frames for each rollout in training and
validation sets. There are 4500 rollouts in training set and 500 rollouts in validation set. BoxBath
simulates a rigid cubic washed by water in a ﬁxed container. The initial position of ﬂuid block and
rigid cubic are randomly initialized. This environment contains 960 ﬂuid particles and 64 rigid par-"
BASE ENVIRONMENTS,0.8008298755186722,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8049792531120332,(a) Sample in BoxBath at time t = 42.
BASE ENVIRONMENTS,0.8091286307053942,"(b) Attention scores at
block 1."
BASE ENVIRONMENTS,0.8132780082987552,"(c) Attention scores at
block 2."
BASE ENVIRONMENTS,0.8174273858921162,"(d) Attention scores at
block 3."
BASE ENVIRONMENTS,0.8215767634854771,"(e) Attention scores at
block 4."
BASE ENVIRONMENTS,0.8257261410788381,"Figure 7: (a) is a sample frame on BoxBath. We select one particle in the center of red circle to show
its attention scores on interactions from neighbors. (b)(c)(d)(e) show the heat maps from uniform
attention in each block of SiT. The attention scores on interactions are assigned to the corresponding
neighbors, which are shown by different colors. The darker the color, the lower attention score on
particles."
BASE ENVIRONMENTS,0.8298755186721992,"ticles with 151 frames for each rollout. There are 2700 rollouts in training set and 300 rollouts in
validation set."
BASE ENVIRONMENTS,0.8340248962655602,"Results. We display some rollouts for all experiments in this section. The results on RiceGrip for
CConv are not available because CConv was designed speciﬁcally for ﬂuid dynamics. The results
on BoxBath for CConv are only used as reference of the simulation on ﬂuid parts. And we compare
CConv with SiT mainly on the ﬂuid simulations."
BASE ENVIRONMENTS,0.8381742738589212,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8423236514522822,"Figure 8: Rollouts on FluidFall. Here we display the settings where the larger drop is above the smaller drop.
DPI-Net and GNS are likely to mix the drops together, while CConv alters the shape of the upper drop before
falling on the ﬂoor. GraphTrans has worse performance of maintaining the shape of water drops. SiT changes a
little for the shape of the upper drop. When using abstract tokens, which is marked by +, SiT can achieve better
results which are closer to the ground truth."
BASE ENVIRONMENTS,0.8464730290456431,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8506224066390041,"Figure 9: Rollouts on FluidShake. DPI-Net still has some artifacts in predicting the wave of water, such as the
rollout at t = 63. CConv, which is designed speciﬁcally for ﬂuid dynamics, rollouts fairly good results, such as
the surface of water, in this domain. GNS has artifacts when predicting the waves (t = 63) and predicts overly
smooth surfaces of ﬂuid (t = 90). GraphTrans fails in this domain, suggesting the insufﬁciency of capturing
the complex interactions from outer forces. SiT can also achieve compatible results with less parameters."
BASE ENVIRONMENTS,0.8547717842323651,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8589211618257261,"Figure 10: Rollouts on RiceGrip. For the surface of the rice, we can see at t = 38 that DPI-Net seems to put
more pressure to the rice; GNS and GraphTrans has difﬁculties in maintaining the smooth surfaces of the rice.
SiT is capable of predicting faithful rollouts , while SiT with abstract tokens can better simulate the deformable
object."
BASE ENVIRONMENTS,0.8630705394190872,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8672199170124482,"Figure 11:
Rollouts on BoxBath. As CConv is designed for ﬂuid dynamics, it treats all particles as ﬂuid
and fails in this domain. GNS is able to handle the rotations of rigid box, but has difﬁculties in modeling the
motions of rigid box when pushed by the waves of ﬂuid, suggesting the insufﬁcient abilities of dealing with
interactions between different materials. SiT with abstract tokens achieves more faithful results in terms of the
rotation of rigid cubics, the interactions between ﬂuid particles."
BASE ENVIRONMENTS,0.8713692946058091,Under review as a conference paper at ICLR 2022
BASE ENVIRONMENTS,0.8755186721991701,"Table 5: MSE and WMSE results from all models on generalizations. The lists of numbers in
FluidShake and RiceGrip are the range of particles, while the tuples in BoxBath denotes number of
ﬂuid particles, number of rigid particles, and shape of rigid objects respectively. Training settings
are marked by *."
BASE ENVIRONMENTS,0.8796680497925311,"Methods
FluidShake [450,627]*
RiceGrip [570,980]*"
BASE ENVIRONMENTS,0.8838174273858921,"[720,900]
[924,1080]
[1104,1368]
[1060,1345]
[1347,1640]"
BASE ENVIRONMENTS,0.8879668049792531,"DPI-Net
2.14±0.36
2.78±0.58
3.53±0.71
2.33±30.64
0.87±4.00
CConv
1.86±0.42
2.23±0.72
2.50±0.77
N/A
N/A
GNS
2.06±0.75
2.82±1.93
2.98±1.22
0.56±0.31
0.57±0.29
GraphTrans
10.24±3.58
12.27±4.80
12.72±3.75
0.63±0.28
0.68±0.37
SiT+ (Ours)
1.54±0.37
1.90±0.63
2.30±0.69
0.14±0.10
0.17±0.15"
BASE ENVIRONMENTS,0.8921161825726142,"Methods
BoxBath (960,64,cubic)*"
BASE ENVIRONMENTS,0.8962655601659751,"(1280,64,cubic)
(960,41,bunny)
(960,125,cubic)
(960,136,ball)
(960,120,cuboid)"
BASE ENVIRONMENTS,0.9004149377593361,"DPI-Net
4.28±0.11
2.49±0.23
2.75±0.42
2.71±0.35
2.97±0.38
CConv
4.30±1.73
3.03±0.18
3.21±0.82
3.80±0.22
3.05±1.55
GNS
2.87±0.19
2.22±0.28
1.97±0.28
2.08±0.22
1.81±0.18
GraphTrans
3.87±0.08
3.55±0.07
3.92±0.15
3.86±0.17
3.56±0.13
SiT+ (Ours)
2.64±0.11
1.72±0.08
1.74±0.14
1.67±0.13
1.92±0.32"
BASE ENVIRONMENTS,0.9045643153526971,"Methods
BoxBath WMSE (960,64,cubic)*"
BASE ENVIRONMENTS,0.9087136929460581,"(1280,64,cubic)
(960,41,bunny)
(960,125,cubic)
(960,136,ball)
(960,120,cuboid)"
BASE ENVIRONMENTS,0.9128630705394191,"DPI-Net
3.14±0.39
3.65±1.72
3.42±0.90
3.13±0.68
3.96±1.39
CConv
4.30±1.09
3.59±0.51
4.09±0.62
4.43±0.35
3.43±0.90
GNS
4.55±1.53
8.09±3.32
3.40±1.06
3.60±0.74
2.18±0.66
GraphTrans
3.37±0.75
3.59±0.76
5.31±0.73
5.38±0.69
3.81±0.75
SiT+ (Ours)
1.82±0.21
1.81±0.64
1.94±0.51
1.76±0.35
2.86±1.41"
BASE ENVIRONMENTS,0.91701244813278,Table 6: Details of generalization settings.
BASE ENVIRONMENTS,0.921161825726141,"Environments
Number of training particles
Number of generalization particles"
BASE ENVIRONMENTS,0.9253112033195021,"FluidShake Large
[450, 720]
[720, 1500]
RiceGrip Large
[558, 1060]
[1060, 1798]
BoxBath Large Fluid
Fluid: 960. Rigid: 64
Fluid: 1280. Rigid 64
BoxBath Bunny
Fluid: 960. Rigid: 64
Fluid: 960. Rigid: 41
BoxBath Large Cubic
Fluid: 960. Rigid: 64
Fluid: 960. Rigid: 125
BoxBath Large Ball
Fluid: 960. Rigid: 64
Fluid: 960. Rigid: 136
BoxBath Large Cuboid
Fluid: 960. Rigid: 64
Fluid: 960. Rigid: 120"
GENERALIZATION DETAILS,0.9294605809128631,"6.2.3
GENERALIZATION DETAILS"
GENERALIZATION DETAILS,0.9336099585062241,"We report WMSE results on BoxBath and other models’ generalization performance for references
in Table 5. SiT with abstract tokens achieves reasonable and superior performance."
GENERALIZATION DETAILS,0.9377593360995851,"We release the details of generalization settings in Table 6. For the generalizations of FluidShake
and RiceGrip, we display the range of particle numbers. For the generalizations of BoxBath, we
change the shape of rigid box into bunny, ball, cuboid, and larger cubic. Speciﬁcally, the generalized
RiceGrip contains particles from 1060 to 1640, while the training set contains particles from 570
to 980; the generalized FluidShake contains particles from 720 to 1368, while the training set has
450 to 627 particles; For BoxBath, the training set has 960 ﬂuid particles and 64 rigid particles. We
ﬁrst separately increase the ﬂuid particles and rigid particles to 1280 and 125 respectively. Then,
we change the shape of rigid object into bunny, ball, and cuboid with particles 41, 136, and 120
respectively. The followings are the rollouts on all environments."
GENERALIZATION DETAILS,0.941908713692946,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.946058091286307,"Figure 12:
Rollouts on generalized FluidShake. DPI-Net achieves fairly good performance, while it still
has difﬁculties to correctly predict the waves, such as the rollout at t = 123. CConv and GNS have similar
problems, such as the rollout at t = 215 and t = 123 respectively. GraphTrans still fails in this environment.
Our SiT+ can still achieve better performance."
GENERALIZATION DETAILS,0.950207468879668,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.9543568464730291,"Figure 13: Rollouts on generalized RiceGrip. DPI-Net fails and cannot maintain the shape of the deformable
object, as is shown in the prediction at t = 38.
GNS tends to rotate the corner of the rice, as is shown at
t = 38, the left lower corner is twisted when the grippers do not have contact with the rice’s surface. Our SiT+
has better rollouts than GraphTrans, suggesting the effectiveness and robustness of our method."
GENERALIZATION DETAILS,0.9585062240663901,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.9626556016597511,"Figure 14: Rollouts on generalized BoxBath, where we add more ﬂuid particles. DPI-Net misses the cubic
at the end. While GNS is able to simulate the rotations of rigid box, it fails to make further predictions of the
rigid after it falls into the water. While SiT+ predicts wrong motions of several ﬂuid particles at the beginning,
it still achieve better results compared with other models."
GENERALIZATION DETAILS,0.966804979253112,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.970954356846473,"Figure 15:
Rollouts on generalized BoxBath, where we add more rigid particles. DPI-Net starts to predict
wrong rollouts for ﬂuid particles at the beginning, and worse results for the rigid cubic. GNS seems overﬁt to
the rotations of rigid box. SiT+ achieves better results comparing with other models."
GENERALIZATION DETAILS,0.975103734439834,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.979253112033195,"Figure 16: Rollouts on generalized BoxBath, where we change the shape from cubic to bunny. The shape of
bunny is more challenging, as the ears of the bunny do not have direct connections to the body. While DPI-Net
fails in this scene, the bunny is submerged in the rollouts of GNS. SiT+ predicts rollouts as correctly as possible."
GENERALIZATION DETAILS,0.983402489626556,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.9875518672199171,"Figure 17: Rollouts on generalized BoxBath, where we use a rigid ball. For simple comparison, the positions
of the rigid ball suggest models’ abilities of simulation. SiT+ predicts more faithful positions of the ball and
overall shapes of the ﬂuid."
GENERALIZATION DETAILS,0.991701244813278,Under review as a conference paper at ICLR 2022
GENERALIZATION DETAILS,0.995850622406639,"Figure 18: Rollouts on generalized BoxBath, where we use a rigid cubiod. It seems that the cuboid is harder
to move. DPI-Net fails in this scenes by predicting wrong rotations for the cuboid, GNS overﬁts to the rotations
of rigid box. SiT+ still achieves better results."
