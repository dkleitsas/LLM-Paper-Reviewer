Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006920415224913495,"Previous studies have highlighted significant advancements in multimodal fusion.
1"
ABSTRACT,0.001384083044982699,"Nevertheless, such methods often encounter challenges regarding the efficacy of fea-
2"
ABSTRACT,0.0020761245674740486,"ture extraction, data integrity, consistency of feature dimensions, and adaptability
3"
ABSTRACT,0.002768166089965398,"across various downstream tasks. This paper proposes a generalized multimodal fu-
4"
ABSTRACT,0.0034602076124567475,"sion method (GMF) via the Poisson-Nernst-Planck (PNP) equation, which adeptly
5"
ABSTRACT,0.004152249134948097,"addresses the aforementioned issues. Theoretically, the optimization objective for
6"
ABSTRACT,0.004844290657439446,"traditional multimodal tasks is formulated and redefined by integrating information
7"
ABSTRACT,0.005536332179930796,"entropy and the flow of gradient backward step. Leveraging these theoretical
8"
ABSTRACT,0.006228373702422145,"insights, the PNP equation is applied to feature fusion, rethinking multimodal
9"
ABSTRACT,0.006920415224913495,"features through the framework of charged particles in physics and controlling their
10"
ABSTRACT,0.007612456747404845,"movement through dissociation, concentration, and reconstruction. Building on
11"
ABSTRACT,0.008304498269896194,"these theoretical foundations, GMF disassociated features which extracted by the
12"
ABSTRACT,0.008996539792387544,"unimodal feature extractor into modality-specific and modality-invariant subspaces,
13"
ABSTRACT,0.009688581314878892,"thereby reducing mutual information and subsequently lowering the entropy of
14"
ABSTRACT,0.010380622837370242,"downstream tasks. The identifiability of the feature’s origin enables our approach to
15"
ABSTRACT,0.011072664359861591,"function independently as a frontend, seamlessly integrated with a simple concate-
16"
ABSTRACT,0.011764705882352941,"nation backend, or serve as a prerequisite for other modules. Experimental results
17"
ABSTRACT,0.01245674740484429,"on multiple downstream tasks show that the proposed GMF achieves performance
18"
ABSTRACT,0.01314878892733564,"close to the state-of-the-art (SOTA) accuracy while utilizing fewer parameters and
19"
ABSTRACT,0.01384083044982699,"computational resources. Furthermore, by integrating GMF with advanced fusion
20"
ABSTRACT,0.01453287197231834,"methods, we surpass the SOTA results.
21"
INTRODUCTION,0.01522491349480969,"1
Introduction
22"
INTRODUCTION,0.01591695501730104,"The world is inherently multimodal; individuals perceive and integrate diverse sensory inputs to form
23"
INTRODUCTION,0.01660899653979239,"a more comprehensive understanding of their surroundings. Similarly, multimodal learning processes
24"
INTRODUCTION,0.01730103806228374,"inputs from multiple modalities, offering potential applications in complex downstream tasks such as
25"
INTRODUCTION,0.017993079584775088,"cross-modal retrieval and multi-modal classification. Nevertheless, features from different modalities
26"
INTRODUCTION,0.018685121107266434,"often differ significantly, even when describing the same event [1, 2]. Consequently, fusing features
27"
INTRODUCTION,0.019377162629757784,"from different modalities is challenging, requiring a dedicated fusion phase before being applied in
28"
INTRODUCTION,0.020069204152249134,"tasks, bridging the semantic gap between different modalities is crucial for valid feature fusion.
29"
INTRODUCTION,0.020761245674740483,"Theoretical works on multimodal fusion have proposed more generalized schemes. MBT [3] ex-
30"
INTRODUCTION,0.021453287197231833,"changes mutual information between different modalities to enhance understanding. Perceiver [4]
31"
INTRODUCTION,0.022145328719723183,"stacks various features and extracts fusion features from transformer blocks to condense task-related
32"
INTRODUCTION,0.022837370242214532,"features. Uni-Code [2] distinguishes between modality-invariant and modality-specific features,
33"
INTRODUCTION,0.023529411764705882,"optimizing feature utilization. Moreover, in downstream tasks, innovative fusion methods are applied.
34"
INTRODUCTION,0.02422145328719723,"MAP-IVR [5] considered that image features belong to the subset of video features, UAVM [6] fuses
35"
INTRODUCTION,0.02491349480968858,"different modalities using an independent fusion block.
36"
INTRODUCTION,0.02560553633217993,"Although existing methods for feature fusion show considerable improvements, they often rely on
37"
INTRODUCTION,0.02629757785467128,"several incomplete assumptions: 1)Feature dimension consistency: Feature dimensions across
38"
INTRODUCTION,0.02698961937716263,"different modalities are perfectly aligned [7, 8], leading to inefficient representations, thus impairing
39"
INTRODUCTION,0.02768166089965398,"model performance; 2)Data reliability: In reality, poor quality data (e.g. missing modalities) directly
40"
INTRODUCTION,0.02837370242214533,"degrades performance [9, 10], even though datasets are assumed to be complete; 3)Downstream
41"
INTRODUCTION,0.02906574394463668,"task applicability: Feature fusion requirements are uniform across different tasks, but matching
42"
INTRODUCTION,0.02975778546712803,"tasks [11, 12, 13, 14, 5] require modality-invariant features (common to all modalities), whereas
43"
INTRODUCTION,0.03044982698961938,"detection tasks [15, 16] necessitate modality-specific features (specific to each modality) additionally;
44"
INTRODUCTION,0.031141868512110725,"4)Feature extraction effectiveness: Loss function in feature fusion does not affect the feature
45"
INTRODUCTION,0.03183391003460208,"extractor’s gradients [17, 18] (See Appendix A), often results in feature extractor homogenization [17],
46"
INTRODUCTION,0.032525951557093424,"deteriorating performance in downstream tasks [1]. Furthermore, the fixed quantity of modal features
47"
INTRODUCTION,0.03321799307958478,"often limit the generalizability of proposed fusion methods [2].
48"
INTRODUCTION,0.033910034602076124,"This paper introduces a generalized multimodal fusion method (GMF) that operates independently
49"
INTRODUCTION,0.03460207612456748,"of the usual constraints. We formulate the learning objectives for traditional multimodal tasks and
50"
INTRODUCTION,0.03529411764705882,"propose new definitions based on information entropy theory [19, 20]. Taking inspiration from the
51"
INTRODUCTION,0.035986159169550176,"Poisson-Nernst-Planck equation (PNP) [21], treating features as charged particles to disassociate them,
52"
INTRODUCTION,0.03667820069204152,"employing GMF for multimodal feature fusion. Leveraging the principles of the PNP equation, GMF
53"
INTRODUCTION,0.03737024221453287,"orchestrates the guided migration of features within a high-dimensional space, segregating modality-
54"
INTRODUCTION,0.03806228373702422,"invariant from modality-specific features within the disassociated feature landscape, reducing the
55"
INTRODUCTION,0.03875432525951557,"mutual information between features further decreases the relevant entropy of downstream tasks.
56"
INTRODUCTION,0.03944636678200692,"Specifically, the proposed method incorporates a reversible feature dissociation-concentration step
57"
INTRODUCTION,0.04013840830449827,"and applies reasonable regional constraints to the reconstruction gradient, emphasizing the connection
58"
INTRODUCTION,0.04083044982698962,"between the feature extractor and the loss of a downstream task, enabling GMF to generalize
59"
INTRODUCTION,0.04152249134948097,"effectively and serve as the frontend for other fusion modules. We evaluated our method on multiple
60"
INTRODUCTION,0.04221453287197232,"datasets across specific downstream tasks. It consistently demonstrated significant performance and
61"
INTRODUCTION,0.042906574394463666,"generalization capabilities. In summary, our contributions are as follows:
62"
INTRODUCTION,0.04359861591695502,"(1) We propose a novel theory for multimodal feature fusion based on the Poisson-Nernst-Planck
63"
INTRODUCTION,0.044290657439446365,"equation and information entropy with an exhaustive proof, demonstrating its effectiveness
64"
INTRODUCTION,0.04498269896193772,"through theoretical analysis and preliminary experiments.
65"
INTRODUCTION,0.045674740484429065,"(2) We have devised a generalized feature fusion method GMF, grounded in entropy theory and the
66"
INTRODUCTION,0.04636678200692042,"PNP equation, which stands independent of both feature extractors and downstream tasks.
67"
INTRODUCTION,0.047058823529411764,"(3) Experiments demonstrate that GMF achieves comparable performance to SOTA with fewer
68"
INTRODUCTION,0.04775086505190312,"computational demands and parameters, while also showing robustness to missing modalities.
69"
INTRODUCTION,0.04844290657439446,"Moreover, when integrated with advanced fusion methods, its performance and robustness are
70"
INTRODUCTION,0.04913494809688582,"notably enhanced, surpassing SOTA and ensuring greater reliability in real-world applications.
71"
RELATED WORKS,0.04982698961937716,"2
Related Works
72"
RELATED WORKS,0.05051903114186851,"Innovative advancements in multimodal fusion methods, both theoretically [2] and structurally [4],
73"
RELATED WORKS,0.05121107266435986,"have significantly propelled the progress of generalized multimodal tasks (denote as GMTs). Some
74"
RELATED WORKS,0.05190311418685121,"SOTA methods focusing on downstream tasks propose fusion methods specifically tailored for
75"
RELATED WORKS,0.05259515570934256,"them. However, the fusion challenges vary with the diversity of downstream tasks. In this paper,
76"
RELATED WORKS,0.05328719723183391,"we categorize multimodal tasks into two types: Native Multimodal Tasks (denote as NMTs) and
77"
RELATED WORKS,0.05397923875432526,"Extended Multimodal Tasks (denote as EMTs), based on whether corresponding single-modal tasks
78"
RELATED WORKS,0.05467128027681661,"exist. Specifically, cross-modal retrieval and matching tasks such as Image-Video retrieval [14, 5]
79"
RELATED WORKS,0.05536332179930796,"and Image-Text matching [12, 13, 11] usually belong to NMT and only require the similarity of
80"
RELATED WORKS,0.056055363321799306,"modalities. For example, CLIP [22] transforms the image classification task into an image-text
81"
RELATED WORKS,0.05674740484429066,"retrieval task, achieving stunning zero-shot performance. Multi-modal classification, recognition, and
82"
RELATED WORKS,0.057439446366782006,"detection tasks such as emotion recognition [16] and event classification [6] usually belong to EMT.
83"
RELATED WORKS,0.05813148788927336,"Different modalities often have inconsistent perspectives, and fully aligned features will affect the
84"
RELATED WORKS,0.058823529411764705,"performance of such tasks.
85"
RELATED WORKS,0.05951557093425606,"To illustrate the generalization capabilities of these methods and their impact on downstream tasks,
86"
RELATED WORKS,0.060207612456747404,"Tab 1 is presented. The ""Type"" column categorizes methods by GMT support. ""Align."" indicates
87"
RELATED WORKS,0.06089965397923876,"feature alignment across modalities. ""Grad. Ref."" assesses if fusion affects feature extractor gradients.
88"
RELATED WORKS,0.061591695501730104,"""Gene."" denotes uniformity of fusion requirements across tasks. ""Avail."" indicates handling of missing
89"
RELATED WORKS,0.06228373702422145,"modalities during inference. Lastly, ""Complexity"" reflects computational complexity regarding (n)
90"
RELATED WORKS,0.0629757785467128,"modalities. Perceiver [4] does not report multimodal correlation experiments.
91"
RELATED WORKS,0.06366782006920416,"Table 1: Comparison of multimodal method proposed in the fusion phase.
Method
Type
Align.
Grad. Ref.
Gene.
Avail.
Complexity
Mentioned Multimodal Related Task
CLIP [22]
NMT
✓
✓
×
-
O(n2)
I-T, Contrastive Learning
ALBEF [12]
NMT
✓
✓
×
-
O(n2)
I-T, Contrastive Learning and Matching
ViLT [11]
NMT
✓
✓
×
-
O(n2)
I-T, Matching
METER [13]
NMT
✓
✓
×
-
O(n2)
I-T, Matching
APIVR [14]
NMT
✓
✓
×
-
O(n2)
I-V, Retrieval
MAP-IVR [5]
NMT
×
✓
×
-
O(n2)
I-V, Retrieval
AVoiD-DF [15]
EMT
✓
✓
✓
✓
O(n2)
A-V, Deepfake Detection
MISA [16]
EMT
✓
✓
×
×
O(n2)
A-V-T, Emotion Recognition
UAVM [6]
EMT
✓
×
✓
✓
O(n2)
A-V, Event Classification
DrFuse [8]
EMT
×
✓
×
×
O(n2)
EHR-CXR, Representation
MBT [3]
GMT
✓
×
✓
✓
O(n2)
A-V, Event Classification
Perceiver [4]
GMT
✓
×
✓
×
O(n)
-
Uni-Code [2]
GMT
×
✓
✓
✓
O(n2)
A-V, Event Classification; localization"
RELATED WORKS,0.0643598615916955,"It is worth noting that the evaluation of gradient correlation is simply whether there is an explicit
92"
RELATED WORKS,0.06505190311418685,"excitation of the loss function. Some downstream methods introduce ways such as concat (e.g.,
93"
RELATED WORKS,0.0657439446366782,"classifier of AVoiD-DF [15]) in the classification stage, and the modal missing adaptation in the fusion
94"
RELATED WORKS,0.06643598615916955,"stage does not represent the adaptation for this task. In addition, for NMTs, the complete modal input
95"
RELATED WORKS,0.0671280276816609,"is necessary, so the conclusion of this part is ""-""; Here, the complexity takes the highest value, which
96"
RELATED WORKS,0.06782006920415225,"does not represent the final computation cost. (e.g., the disentangled loss of MISA [16] is O(n2).
97"
THEORY,0.0685121107266436,"3
Theory
98"
THEORY,0.06920415224913495,"In this subsection, we briefly introduce the notation system used in this paper and the general structure
99"
THEORY,0.0698961937716263,"of multimodal tasks, representing the information entropy at different stages of multimodal learning.
100"
THEORY,0.07058823529411765,"After that, we generalize the information entropy to multi-modality and redefine the entropy reduction
101"
THEORY,0.07128027681660899,"objective for multi-modal learning. Finally, we evaluate the impact of linear dimension mapping on
102"
THEORY,0.07197231833910035,the performance of downstream tasks and present the preamble theorem.
THEORY,0.0726643598615917,"Extraction
Fusion
Classification"
THEORY,0.07335640138408304,"Figure 1: Stages of information entropy change. Where Zi might be a set of vectors ({ZA
i , . . . , ZM
i })
or a vector, depending on the fusion method F(·), and C(·) stands for classifier. 103"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07404844290657439,"3.1
Formulation and Traditional Objective Definition
104"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07474048442906574,"Consider inputs with d modalities, where j ∈{1, 2, . . . , d} represents different modalities. Ex-
105"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.0754325259515571,"amine a dataset comprising n samples.
Let the input be X = {X1, X2, . . . , Xn}, where a
106"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07612456747404844,"specific sample i ∈{1, 2, . . . , n} is represented as Xi = {X(1)
i
, X(2)
i
, . . . , X(d)
i
}. The output
107"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07681660899653979,"is Y
= {Y1, Y2, . . . , Yn}, and each {Xi, Yi} forms a sample pair.
X(j)
i
represents the orig-
108"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07750865051903114,"inal sample of modality j with varying shapes, while the shape of Yi depends on the specific
109"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.0782006920415225,"datasets and downstream tasks. For each modality j, specific feature extractors f (j)(·, θ(j)) and
110"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07889273356401384,"parameters θ(j) are employed for feature extraction. The fused features capturing multimodal
111"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.07958477508650519,"interactions for sample i are denoted as Zi = {Z(1)
i
, Z(2)
i
, · · · , Z(d)
i
}. The set of global fea-
112"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08027681660899653,"tures is expressed as f(X, θ) = [f (1)(X(1), θ(1)); f (2)(X(2), θ(2)); . . . ; f (d)(X(d), θ(d))], where
113"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.0809688581314879,"θ = {θ(1), θ(2), . . . , θ(d)}.
114"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08166089965397924,"The multimodal task is depicted in Figure 1, delineating three key parameters: the feature extractor
115"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08235294117647059,"θ, fusion parameter θF , and classifier parameter θC. Optimization of these parameters aims at
116"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08304498269896193,"maximizing performance. Regarding entropy, F(·) represents the fused mapping, extending the
117"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.0837370242214533,"learning objective from feature extraction to fusion:
118"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08442906574394464,"min
θ,θF {H(F(f(X, θ), θF ) | F(f(X, θ))}
(1)"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08512110726643599,"Similarly, we employ C(·) to represent the mapping for downstream tasks and generalize it to embody
119"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08581314878892733,"the learning objective fused with downstream tasks:
120"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08650519031141868,"min
θ,θF ,θC{H(Y | C(F[f(X, θ), θF ], θC)])}
(2)"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08719723183391004,"In Eq. ( 2), these parameters are optimized by downstream task losses. If there is a loss in the fusion
121"
FORMULATION AND TRADITIONAL OBJECTIVE DEFINITION,0.08788927335640138,"stage, then it optimizes the parameters in Eq. ( 1).
122"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.08858131487889273,"3.2
Information Entropy and Objective Redefinition
123"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.08927335640138408,"Feature extraction through dimensionality reduction involves reducing data uncertainty [19], as
124"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.08996539792387544,"quantified by information entropy H. In Figure 1, we show a simplified approach to single-modal
125"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09065743944636678,"learning. The feature extractor and classifier (dotted arrow) directly minimize the information
126"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09134948096885813,"entropy of both the input X(j)
i
and the output Yi by adjusting the parameters of the feature extractor
127"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09204152249134948,"f (j)(·, θ(j)) and the classifier C(j)(·, θC(j)) for modality j:
128"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09273356401384084,"min
θ(j),θC H[Yi|C(j)(f (j)(X(j), θ(j)), θC(j))]
(3)"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09342560553633218,"This process, facilitated by feature extractors, condenses data samples into a feature space, preserving
129"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09411764705882353,"pertinent attributes for downstream tasks. Think loss as stimulation of entropy reduction, maximize
130"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09480968858131487,"mutual information about related features [18]. Expanding to the multimodal fusion stage, the
131"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09550173010380623,"objective is to minimize the entropy of the fused features compared to the sum of the entropy of
132"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09619377162629758,"each input feature. In the context of multimodal fusion, where outputs from disparate modalities are
133"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09688581314878893,"integrated post-feature extraction, the total information entropy of the system can be estimated using
134"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09757785467128027,"the joint entropy formula, and for constant X:
135"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09826989619377163,"H(f(X, θ)) = d
X"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09896193771626298,"j=1
H(f (j)(X(j), θ(j))) −I(f(X, θ))
|
{z
}
Mutual information"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.09965397923875433,"=⇒min
θ
H(f(X, θ)) ⇔max
θ
I(f(X, θ))"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10034602076124567,"(4)
Downstream objectives are typically structured to minimize mutual information, consequently leading
136"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10103806228373702,"to a reduction in entropy. However, in fusion stage, disparities observed among the equations (1),
137"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10173010380622838,"(2), and (3) suggest that certain fusion-method might not establish a straightforward correspondence
138"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10242214532871972,"between network inputs and outputs. Achieving complete consistency between modalities, where
139"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10311418685121107,"mutual information is zero, may not always lead to optimal outcomes [1, 20], potentially increasing
140"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10380622837370242,"entropy in downstream task-related features [17]. This observation is substantiated by the diminishing
141"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10449826989619378,"performance of certain multimodal methods [3, 15] compared to earlier unimodal methods, indicating
142"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10519031141868512,"a decline in their capacity to extract distinctive features from individual modalities when confronted
143"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10588235294117647,"with the absence of certain modalities. Thus, optimization objectives for multimodal tasks should
144"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10657439446366782,"balance minimizing entropy during fusion with maintaining or reducing entropy in downstream
145"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10726643598615918,"task-related features. This highlights the necessity of aligning deep learning tasks with downstream
146"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10795847750865052,"objectives and minimizing information entropy when designing loss functions for these tasks.
147"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10865051903114187,"Theorem 3.1: The overarching objective of multimodal tasks lies in minimizing entropy during the
148"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.10934256055363321,"fusion stage without amplifying the entropy of downstream task-related features:
149"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11003460207612457,"min
θ,θF ,θC{H(Y | C(F[f(X, θ), θF ], θC)])}"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11072664359861592,"s.t.
∀j ∈{1, 2, . . . , d},
θ(j) ∈arg min
θ(j) H(Y |f (j)(X(j), θ(j)))
(5)"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11141868512110727,"Some approaches introduce the fused results as residuals, which demonstrate a certain degree of
150"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11211072664359861,"improvement, and this theory provides a better rationale for such enhancement. However, given that
151"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11280276816608996,"the forward pass necessarily involves the operation of F(·), it becomes challenging to fully meet this
152"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11349480968858132,"precondition. During gradient backward, the loss incurred during the fusion stage for the feature
153"
INFORMATION ENTROPY AND OBJECTIVE REDEFINITION,0.11418685121107267,"extractor should align with the loss of the downstream task or be zero.
154"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11487889273356401,"3.3
Modality Feature Dissolution and Concentration
155"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11557093425605536,"Adding too many parameters, or overcharacterization, can improve the model’s ability to fit the data,
156"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11626297577854672,"acting like a parameterized memory function [23]. However, it’s important to balance this with the
157"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11695501730103806,"amount of data available for the next task to prevent learning too much noise and overfitting [7]. On
158"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11764705882352941,"the other hand, having too few parameters may weaken the model’s ability to represent complex
159"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11833910034602076,"patterns, resulting in lower performance across different methods (See Appendix C).
160"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11903114186851212,"Theorem 3.2: The dimension of the feature that is best suited to the downstream task varies, and
161"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.11972318339100346,"there is always an optimal value for this feature. The dimension multiple relationship between each
162"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12041522491349481,"layer of the feature extractor is fixed, and the initial dimension is adjusted. Too low dimension of the
163"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12110726643598616,"final output will lead to inefficient representation, and too high dimension will introduce noise. The
164"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12179930795847752,"existence of an integer lbest such that for any integer l distinct from lbest, the conditional entropy of
165"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12249134948096886,"the model’s predictions fl(X, θl) is greater than that of the model’s predictions flbest(X, θlbest).
166"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12318339100346021,"∃lbest ∈N, ∀l ∈N, l ̸= lbest, H(Y |fl(X, θl)) > H(Y |flbest(X, θlbest))
(6)"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12387543252595155,"Theorem 3.3: The feature extractor is fixed, and its original output feature dimension l is mapped
167"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.1245674740484429,"to nl, and finally back to l. The mapping result is used as the basis for the downstream task. The
168"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12525951557093426,"performance of downstream tasks is infinitely close to the original performance as n increases, but
169"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.1259515570934256,"never greater than the original performance. For magnification n > 1, n ∈Z, mapping matrix
170"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12664359861591695,"U1 ∈Rl×nl and U2 ∈Rnl×l, For the output features f(X, θ) ∈Rl and Y :
171"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.1273356401384083,"H(Y |f(X, θ)) < H(Y |U1 · (U2 · f(X, θ)))
(7)
172"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12802768166089964,"limn→∞H(Y |U1 · (U2 · f(X, θ)))) = H(Y |f(X, θ)
(8)
Conjecture 3.1: Rely on Theorem 3.1, 3.2, 3.3, we propose an conjecture that a boundary of perfor-
173"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.128719723183391,"mance limitation exists, determined by downstream-related entropy. Theoretically, by establishing a
174"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.12941176470588237,"direct correspondence between the extractor and classifier, fusion method can enhance the limitation
175"
MODALITY FEATURE DISSOLUTION AND CONCENTRATION,0.1301038062283737,"boundary, further improve performance.
176"
POISSON-NERNST-PLANCK EQUATION,0.13079584775086506,"3.4
Poisson-Nernst-Planck Equation
177"
POISSON-NERNST-PLANCK EQUATION,0.1314878892733564,"The Nernst-Planck equation represents a mass conservation equation that characterizes the dynamics
178"
POISSON-NERNST-PLANCK EQUATION,0.13217993079584775,"of charged particles within a fluid medium. This equation modifies Fick’s law of diffusion to include
179"
POISSON-NERNST-PLANCK EQUATION,0.1328719723183391,"scenarios where particles are also mobilized by electrostatic forces relative to the fluid. The equation
180"
POISSON-NERNST-PLANCK EQUATION,0.13356401384083044,"accounts for the total flux of particle p ∈{+, −}, denoted as Jp, of charged particles, encompassing
181"
POISSON-NERNST-PLANCK EQUATION,0.1342560553633218,"both diffusion driven by concentration gradients and migration induced by electric fields. Since fusion
182"
POISSON-NERNST-PLANCK EQUATION,0.13494809688581316,"features are usually one-dimensional, we only consider the x direction here. For a given charged
183"
POISSON-NERNST-PLANCK EQUATION,0.1356401384083045,"particle i, the equation describes its movement as follows:
184"
POISSON-NERNST-PLANCK EQUATION,0.13633217993079585,"Jp = −Dp∇cp(x, t)
|
{z
}
Diffusion"
POISSON-NERNST-PLANCK EQUATION,0.1370242214532872,"+ cp(x, t)v
|
{z
}
Advection"
POISSON-NERNST-PLANCK EQUATION,0.13771626297577855,+ Dpzpe
POISSON-NERNST-PLANCK EQUATION,0.1384083044982699,"kBT cp(x, t)E
|
{z
}
Electromigration (9)"
POISSON-NERNST-PLANCK EQUATION,0.13910034602076124,"p is abstracted as elements in the modality-invariant feature and the modality-specific feature. Here,
185"
POISSON-NERNST-PLANCK EQUATION,0.1397923875432526,"cp(x, t) denotes the concentration of particle, while Dp (diffusivity of p), kB (Boltzmann constant),
186"
POISSON-NERNST-PLANCK EQUATION,0.14048442906574393,"zp (valence also electric charge), and e (elementary charge) are constants. T is a hyperparameter,
187"
POISSON-NERNST-PLANCK EQUATION,0.1411764705882353,"represent temperature. E represents the electric field of the entire system, and v represents the flow
188"
POISSON-NERNST-PLANCK EQUATION,0.14186851211072665,"rate. The Poisson equation describes the relationship between the distribution of a field and the
189"
POISSON-NERNST-PLANCK EQUATION,0.14256055363321798,"potential energy it induces, represented by the expression:
190"
POISSON-NERNST-PLANCK EQUATION,0.14325259515570934,∇2ϕ(x) = −ρ
POISSON-NERNST-PLANCK EQUATION,0.1439446366782007,"ε0
, ρ = e(z+c+(x, t) + z−c−(x, t))
(10)"
POISSON-NERNST-PLANCK EQUATION,0.14463667820069204,"ϕ signifies the potential, considered as an external excitation, ε0 represent dielectric constant. By
191"
POISSON-NERNST-PLANCK EQUATION,0.1453287197231834,"integrating the relationship between the concentration of charged particles and the electromigration
192"
POISSON-NERNST-PLANCK EQUATION,0.14602076124567473,"term in the Poisson equation, we derive the Poisson-Nernst-Planck (PNP) equation. Assuming that
193"
POISSON-NERNST-PLANCK EQUATION,0.1467128027681661,"the dissociation process approaches equilibrium, for feature elements without magnetic field and flow
194"
POISSON-NERNST-PLANCK EQUATION,0.14740484429065745,"velocity, we can consider the time-dependent change in concentration cp(x, t) of the charged particle
195"
POISSON-NERNST-PLANCK EQUATION,0.14809688581314878,"i over time t is negligible:
196"
POISSON-NERNST-PLANCK EQUATION,0.14878892733564014,"∂cp(x, t)"
POISSON-NERNST-PLANCK EQUATION,0.14948096885813147,"∂t
= Dp(∂2cp(x, t)"
POISSON-NERNST-PLANCK EQUATION,0.15017301038062283,"∂x2
−zpeF"
POISSON-NERNST-PLANCK EQUATION,0.1508650519031142,"kBTϵ0
cp(x, t)(z+c+(x, t)+z−c−(x, t)+ zpe"
POISSON-NERNST-PLANCK EQUATION,0.15155709342560553,"kBT
∂cp(x, t)"
POISSON-NERNST-PLANCK EQUATION,0.1522491349480969,"∂x
dϕ(x)"
POISSON-NERNST-PLANCK EQUATION,0.15294117647058825,"dx
) ≈0"
POISSON-NERNST-PLANCK EQUATION,0.15363321799307958,"(11)
When the final state is stable, a sufficiently large 1D electrolytic cell of length l, at the potential
197"
POISSON-NERNST-PLANCK EQUATION,0.15432525951557094,"equilibrium boundary b, it can be equivalent to (See Appendix B):
198"
POISSON-NERNST-PLANCK EQUATION,0.15501730103806227,"(ϕ(0) −ϕ(b)) −e
Z b"
POISSON-NERNST-PLANCK EQUATION,0.15570934256055363,"0
c−(x, t)z−dx ≈e
Z l"
POISSON-NERNST-PLANCK EQUATION,0.156401384083045,"0
c+(x, t)z+ −c−(x, t)z−dx
(12)"
POISSON-NERNST-PLANCK EQUATION,0.15709342560553632,"In this context, ϕ(x) represents an external influence from another modality feature. We assume that
199"
POISSON-NERNST-PLANCK EQUATION,0.15778546712802768,"modality-invariant feature elements have a positive charge, while modality-specific feature elements
200"
POISSON-NERNST-PLANCK EQUATION,0.15847750865051904,"have a negative charge. The difference ϕ(0) −ϕ(b) indicates the enrichment potential of modality-
201"
POISSON-NERNST-PLANCK EQUATION,0.15916955017301038,"invariant feature elements for the excitation modality. This potential attracts modality-specific feature
202"
POISSON-NERNST-PLANCK EQUATION,0.15986159169550174,"elements in dissociated modality towards dissociation.
203"
POISSON-NERNST-PLANCK EQUATION,0.16055363321799307,"Theorem 3.4: Following dissociation and Theorem3.3, in line with the principles of matter and
204"
POISSON-NERNST-PLANCK EQUATION,0.16124567474048443,"information conservation, the excitation and attraction features can revert back to their original state.
205"
POISSON-NERNST-PLANCK EQUATION,0.1619377162629758,"A cyclic feature electrolytic cell is generalized, using a loss function as stimulation:
206"
POISSON-NERNST-PLANCK EQUATION,0.16262975778546712,"ˆZ(j)
i
= Udisf (j)(X(j)
i
, θ(j))
(13)
207"
POISSON-NERNST-PLANCK EQUATION,0.16332179930795848,"L = ||U(j)
con[ ˆZ(j)
i
(1 : bj); ˆZ(j+1)
i
(b(j+1) + 1 : nl(j+1))] −f (j)(X(j)
i
, θ(j))||2
(14)
L is loss function. l(j) and b(j) are feature dimension and dissociation boundary of modality j,
208"
POISSON-NERNST-PLANCK EQUATION,0.16401384083044981,"respectively. Around this boundary, features are explicitly distinguished. The mapping matrix
209"
POISSON-NERNST-PLANCK EQUATION,0.16470588235294117,"U(j)
dis ∈Rnl(j)×l(j), U(j)
con ∈Rl(j)×(nl(j+1)+b(j)−b(j+1)) is learnable. ˆZ(j)
i
∈Rnl(j) is the result of
210"
POISSON-NERNST-PLANCK EQUATION,0.16539792387543253,"f (j)(X(j)
i
, θ(j)) ∈Rl(j) being linearly mapped (dissolved) into a higher dimensional space.
211"
METHODOLOGY,0.16608996539792387,"4
Methodology
212"
METHODOLOGY,0.16678200692041523,"Set the dissociation boundary b(j) and feature dimension l(j) of modality j. The feature with the
213"
METHODOLOGY,0.1674740484429066,"smallest dimension is denoted as l∗. The feature dimension of the dissociation is nl(j), with a uniform
214"
METHODOLOGY,0.16816608996539792,"magnification of n > 2.
215"
METHODOLOGY,0.16885813148788928,"Combining information entropy theory with the PNP equation, we propose GMF method to optimize
216"
METHODOLOGY,0.1695501730103806,"fusion feature mutual information on the premise of maintaining the downstream task related infor-
217"
METHODOLOGY,0.17024221453287197,"mation of input features. Following Assumption3.1, GMF has only four learnable matrices for each
218"
METHODOLOGY,0.17093425605536333,"modality, enforces correlations without complex structure, as shown in Fig 2.
219"
METHODOLOGY,0.17162629757785466,"GMF is divided into three stages, for each modality j, applying different learnable mapping ma-
220"
METHODOLOGY,0.17231833910034602,"trices: dissolve matrix P(j)
dis ∈Rnl(j)×l(j), concentrate matrix P(j)
cinv ∈Rb(j)×l∗and P(j)
cspec ∈
221"
METHODOLOGY,0.17301038062283736,"R(nl(j)−b(j))×l(j), reconstruct matrix P(j)
recon ∈Rl(j)×(l(j)+l∗).
222"
METHODOLOGY,0.17370242214532872,"Zi = GMF(f(Xi, θ), θGMF ), θGMF = {P(j)
dis, P(j)
cinv, P(j)
cspec, P(j)
recon}
(15)
First, to make sure the features move, we map (dissolve) them to higher dimensions. Next, for the
223"
METHODOLOGY,0.17439446366782008,"feature of each modality, after dimension elevation, the goal is explicitly divided as specific and
224"
METHODOLOGY,0.1750865051903114,"invariant by abstracting different kinds of features into positive and negative charged particles:
225"
METHODOLOGY,0.17577854671280277,"ˆZ(j)
i
= P(j)
dis(f (j)(X(j)
i
, θ(j))),
( ˆZ(j)
i
)inv = ˆZ(j)
i
(1 : b(j)),
( ˆZ(j)
i
)spec = ˆZ(j)
i
(b(j) + 1 : nl(j))
(16)
f (j)(X(j)
i
, θ(j)) ∈Rl(j), and ˆZ(j)
i
∈Rnl(j). Referencing Eq.( 4), irrespective of the initial length
226"
METHODOLOGY,0.17647058823529413,"l(j) of a feature, partitioning it into invariant (Z(j)
i
)inv ∈Rl∗and specific (Z(j)
i
)spec ∈Rl(j)
227"
METHODOLOGY,0.17716262975778546,"components aims to minimize output feature dimensions, thereby mitigating entropy disturbance.
228"
METHODOLOGY,0.17785467128027682,"After concentrate, finally, the output Z(j)
i
∈R(l(j)+l∗) is obtained:
229"
METHODOLOGY,0.17854671280276815,"(Z(j)
i
)inv = P(j)
cinv( ˆZ(j)
i
)inv,
(Z(j)
i
)spec = P(j)
spec( ˆZ(j)
i
)spec,
Z(j)
i
= [(Z(j+1)
i
)inv; (Z(j)
i
)spec]
(17)"
METHODOLOGY,0.17923875432525951,Multi-Modal input
METHODOLOGY,0.17993079584775087,"Multi-Modal
Sorted output －
+ +
－"
METHODOLOGY,0.1806228373702422,"Concentration
Disassociated"
METHODOLOGY,0.18131487889273357,Modality A
METHODOLOGY,0.18200692041522493,Modality B
METHODOLOGY,0.18269896193771626,"recon A
output A"
METHODOLOGY,0.18339100346020762,Dissociation
METHODOLOGY,0.18408304498269895,"Material
conservation Loss"
METHODOLOGY,0.1847750865051903,"Recon-
sturction"
METHODOLOGY,0.18546712802768167,Reconstruction
METHODOLOGY,0.186159169550173,specific
METHODOLOGY,0.18685121107266436,invariant
METHODOLOGY,0.1875432525951557,Diluted Feature
METHODOLOGY,0.18823529411764706,"linear
target"
METHODOLOGY,0.18892733564013842,Original Feature
METHODOLOGY,0.18961937716262975,"Recon-
sturction"
METHODOLOGY,0.1903114186851211,"Figure 2: Structure of GMF. The input is taken from f(Xi, θ) and the output is taken as Zi. This is
done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be
directly used for classification or can be connected to other fusion modules. See Appendix J ."
METHODOLOGY,0.19100346020761247,"Eventually the entire system can be restored to its original state. A loss function is given as an
230"
METHODOLOGY,0.1916955017301038,"external incentive to force the features to move in different directions. Following the Theorem3.4, we
231"
METHODOLOGY,0.19238754325259516,"use P(j)
recon to map the features back to f (j)(X(j)
i
, θ(j)) and apply the disassociation loss.
232"
METHODOLOGY,0.1930795847750865,"Ldis = d
X"
METHODOLOGY,0.19377162629757785,"j=1
||(f (j)(X(j)
i
, θ(j)) −P(j)
reconZ(j)
i
||2
(18) 233"
EXPERIMENT,0.1944636678200692,"5
Experiment
234"
EXPERIMENT,0.19515570934256055,"In this section we briefly introduce the experimental dataset, evaluation metrics, implementation
235"
EXPERIMENT,0.1958477508650519,"details, experimental results and analysis. Our evaluation focuses on solving the limitations mentioned
236"
EXPERIMENT,0.19653979238754327,"in Section 1 and verifying our theory and hypothesis, so we pay more attention to the fusion
237"
EXPERIMENT,0.1972318339100346,"performance under the same feature extraction ability.
238"
DATASETS AND EXPERIMENTAL TASKS,0.19792387543252596,"5.1
Datasets and experimental tasks
239"
DATASETS AND EXPERIMENTAL TASKS,0.1986159169550173,"We performed the NMT task for image-video retrieval on ActivityNet [24] dataset and the EMT task
240"
DATASETS AND EXPERIMENTAL TASKS,0.19930795847750865,"for audio-video event classification on VGGSound [25] and deepfake detection on FakeAVCeleb [26],
241"
DATASETS AND EXPERIMENTAL TASKS,0.2,"and compared the NMT, EMT and GMT methods (as defined in the Related Work) respectively. We
242"
DATASETS AND EXPERIMENTAL TASKS,0.20069204152249134,"conduct three sets of comparison experiments:
243"
DATASETS AND EXPERIMENTAL TASKS,0.2013840830449827,"(1) Input the same features to simulate the freezing of the feature extractor, and evaluate the entropy
244"
DATASETS AND EXPERIMENTAL TASKS,0.20207612456747404,"reduction effect of the fusion method on the existing information.
245"
DATASETS AND EXPERIMENTAL TASKS,0.2027681660899654,"(2) Complete the training of the whole model including the same feature extractor, and evaluate the
246"
DATASETS AND EXPERIMENTAL TASKS,0.20346020761245676,"impact of the fusion method on the gradient of the feature extractor.
247"
DATASETS AND EXPERIMENTAL TASKS,0.2041522491349481,"(3) Select a set of method-specific feature extractors to test the limitation performance.
248"
DATASETS AND EXPERIMENTAL TASKS,0.20484429065743945,"For EMTs, VGGSound dataset evaluate (1) and (2)1, the evaluation metric is the classification
249"
DATASETS AND EXPERIMENTAL TASKS,0.2055363321799308,"accuracy ACC(%). FakeAVCeleb dataset evaluate (3), due to the imbalance of data samples, the
250"
DATASETS AND EXPERIMENTAL TASKS,0.20622837370242214,"evaluation focuses on the Area under the Curve of ROC (AUC). For NMTs, ActivityNet dataset
251"
DATASETS AND EXPERIMENTAL TASKS,0.2069204152249135,"evaluate (4), the evaluation metric is the matching accuracy mAP, mAP@n represents that the
252"
DATASETS AND EXPERIMENTAL TASKS,0.20761245674740483,"matching task target is selected from n samples.
253"
DATASETS AND EXPERIMENTAL TASKS,0.2083044982698962,"1When the dataset was acquired, 20% of the samples were no longer valid."
IMPLEMENT DETAILS,0.20899653979238755,"5.2
Implement details
254"
IMPLEMENT DETAILS,0.20968858131487889,"For the methods proposed in different papers, we only compare the fusion structures except feature
255"
IMPLEMENT DETAILS,0.21038062283737025,"extractor and classifier. During the evaluation, we set n to 4 and b(j) to always be 1"
IMPLEMENT DETAILS,0.21107266435986158,"2 of l(j). All
256"
IMPLEMENT DETAILS,0.21176470588235294,"experiments were performed on a single RTX4090@2.64GHz, the CPU for testing the inference
257"
IMPLEMENT DETAILS,0.2124567474048443,"time is R9 5900X@4.5GHz, and the random seed was fixed to ’1’ except for dropout proposed
258"
IMPLEMENT DETAILS,0.21314878892733563,"by baseline and some transformer [27]-based methods. There was no data augmentation (such as
259"
IMPLEMENT DETAILS,0.213840830449827,"cropping, rotation) or introduction of any pre-training parameters in the data preprocessing process.
260"
IMPLEMENT DETAILS,0.21453287197231835,"See the Appendix G for details of the training parameters.
261"
IMPLEMENT DETAILS,0.21522491349480968,"The baseline of the multi-modal is all the direct connection of the features of the output of the
262"
IMPLEMENT DETAILS,0.21591695501730104,"single-modal baseline. GMF stands for simple connection as the back-end. ""G-method name"" stands
263"
IMPLEMENT DETAILS,0.21660899653979238,"for GMF as a front-end for the method, See Figure 13 in Appendix for the detailed structure.
264"
EVALUATION,0.21730103806228374,"5.3
Evaluation
265"
EVALUATION,0.2179930795847751,"For EMTs, our experiments, detailed in Table 2 and conducted on the VGGSound dataset [25],
266"
EVALUATION,0.21868512110726643,"employ R(2+1)D [28] as the video feature extractor and ResNet-18 [29] as the audio feature extractor.
267"
EVALUATION,0.2193771626297578,"The ’Training Extractor’ label indicates trainable parameters, while ’Frozen Extractor’ denotes
268"
EVALUATION,0.22006920415224915,"fixed parameters. Columns ’A’ and ’V’ represent audio and video inputs, respectively, while a
269"
EVALUATION,0.22076124567474048,"value of ’0’ for the other modality input indicates its absence. For trainable feature extractors, we
270"
EVALUATION,0.22145328719723184,"introduce additional columns ’A(uni)’ and ’V(uni)’ to evaluate the direct use of extracted features for
271"
EVALUATION,0.22214532871972317,"classification, thereby assessing feature extraction efficacy.
272"
EVALUATION,0.22283737024221453,Table 2: Comparison of EMTs and GMTs methods on VGGSound.
EVALUATION,0.2235294117647059,"Method
Frozen Extractor
Training Extractor
Real-Time
Params
FLOPs
A
V
AV
A(uni)
A
V(uni)
V
AV
CPU(s)"
EVALUATION,0.22422145328719723,"Baseline
23.31
25.14
28.56
23.31
-
25.14
-
28.56
-
-
-
AVoiD-DF [15]
16.56
18.33
31.61
15.32
10.81
17.71
13.44
30.05
0.028
57.45M
0.11G
MISA [16]
20.88
21.67
32.85
20.43
18.65
22.65
20.03
33.77
0.015
50.88M
0.40G
UAVM [6]
23.28
24.98
26.15
21.86
-
23.37
-
30.81
0.006
25.70M
0.05G
DrFuse [8]
20.45
21.92
32.79
20.31
18.77
22.39
20.31
33.23
0.011
37.33M
0.31G
MBT [3]
18.87
20.01
31.88
18.72
16.35
19.98
17.44
33.95
0.013
37.83M
0.15G
Perceiver [4]
17.98
18.31
33.41
21.45
15.31
23.83
16.05
35.21
0.301
45.05M
45.59G"
EVALUATION,0.22491349480968859,"GMF
22.01
24.32
31.64
21.83
21.55
23.93
23.67
32.01
0.001
5.25M
0.01G
G-MBT
21.67
22.98
34.28
19.81
18.33
20.68
19.25
34.97
0.013
43.08M
0.16G
G-Perceiver
20.13
21.66
34.73
21.53
17.92
23.81
18.17
35.85
0.301
50.31M
45.61G"
EVALUATION,0.22560553633217992,"UAVM [6] emphasizes unified expression, highlighting the importance of modality absence. In
273"
EVALUATION,0.22629757785467128,"contrast, AVoiD-DF [15] and MBT [3] prioritize exchanging feature semantics, making them par-
274"
EVALUATION,0.22698961937716264,"ticularly sensitive to missing modalities; MBT further distinguishes itself through the incorporation
275"
EVALUATION,0.22768166089965397,"of bottlenecks. Notably, DrFuse [8] and MISA [16] marginally outperform our method, possibly
276"
EVALUATION,0.22837370242214533,"due to the abundance of learnable cross-modal parameters enabled by their self-attention modules,
277"
EVALUATION,0.2290657439446367,"which also magnifies the impact of modality absence. Perceiver [4], characterized by stacked features
278"
EVALUATION,0.22975778546712802,"without explicit modal differentiation, is notably susceptible to missing modalities. In cases where
279"
EVALUATION,0.23044982698961938,"the feature extractor is trainable, the impact of modality absence becomes more pronounced. At this
280"
EVALUATION,0.23114186851211072,"juncture, this influence arises not only from modal fusion but also from the homogenization of features
281"
EVALUATION,0.23183391003460208,"extracted by the feature extractor. GMF stands out for its minimal parameters and computational load,
282"
EVALUATION,0.23252595155709344,"yet it achieves competitive performance while significantly reducing sensitivity to modality absence.
283"
EVALUATION,0.23321799307958477,"This remarkable trait can be harnessed by integrating it with other methods, imparting them with
284"
EVALUATION,0.23391003460207613,"similar characteristics. This integration leads to performance enhancement and decreased sensitivity
285"
EVALUATION,0.23460207612456746,"to modality absence, showcasing the versatility and applicability of GMF.
286"
EVALUATION,0.23529411764705882,"For NMTs, our performance report on the ActivityNet [24] dataset is presented in Table 3. To be
287"
EVALUATION,0.23598615916955018,"fair, we utilize the features same as AP-IVR [14] (4096-dimensional for video, 128-dimensional for
288"
EVALUATION,0.2366782006920415,"images) as input. We map image features to 4096 dimensions or video features to 128 dimensions.
289"
EVALUATION,0.23737024221453287,"Three combinations are obtained: Image-Video feature dimensions are (1) 128-4096 (denoted as
290"
EVALUATION,0.23806228373702423,"128-4096), (2) 4096-4096 (denoted as 4096), and (3) 128-128 (denoted as 128).
291"
EVALUATION,0.23875432525951557,"We employ CLIP [22] as the baseline, which only requires computing cosine similarity of mapped
292"
EVALUATION,0.23944636678200693,"features without introducing parameters. METER [13] introduces the cross-attention module on this
293"
EVALUATION,0.24013840830449826,Table 3: Comparison of NMTs and GMTs methods on ActivityNet.
EVALUATION,0.24083044982698962,"Method
mAP@10
mAP@20
mAP@50
mAP@100
Params
FLOPs"
EVALUATION,0.24152249134948098,"CLIP (4096)
0.235
0.221
0.213
0.205
-
-
METER (4096)
0.252
0.245
0.235
0.228
62.96M
0.13G"
EVALUATION,0.2422145328719723,"Perceiver (128)
0.264
0.253
0.241
0.232
44.54M
45.56G
MAP-IVR (128)
0.341
0.323
0.306
0.294
3.81M
0.01G
GMF (128)
0.349
0.335
0.323
0.308
0.32M
0.00G"
EVALUATION,0.24290657439446367,"APIVR (128-4096)
0.264
0.255
0.249
0.232
2.19M
0.00G
MAP-IVR (128-4096)
0.349
0.337
0.322
0.311
11.94M
0.02G
GMF (128-4096)
0.355
0.341
0.327
0.315
119.21M
0.23G"
EVALUATION,0.24359861591695503,"basis, but the improvement is limited due to the sparse features. MAP-IVR [5] employs fixed-length
294"
EVALUATION,0.24429065743944636,"mappings, while Perceiver [4] inputs an indistinguishable feature mapping, so the actual number of
295"
EVALUATION,0.24498269896193772,"parameters relative to input dimensions is not apparent. GMF achieving competitive performance
296"
EVALUATION,0.24567474048442905,"in (128) with minimal additional parameters and computations. Furthermore, the experiments (128-
297"
EVALUATION,0.24636678200692042,"4096) demonstrate the necessity of unequal-length fusion, ensuring not only the flexibility of the
298"
EVALUATION,0.24705882352941178,"method but also profoundly impacting its performance and additional parameters. In the experiments
299"
EVALUATION,0.2477508650519031,"of unequal-length fusion, GMF achieved state-of-the-art performance. Given that GMF is composed
300"
EVALUATION,0.24844290657439447,"of linear layers, an increase in input dimensionality leads to an escalation in parameter count.
301"
EVALUATION,0.2491349480968858,Table 4: Comparison of fusion methods based on different feature extractors on FakeAVCeleb.
EVALUATION,0.24982698961937716,"Baseline
MISA [16]
UAVM [6]
DrFuse [8]
Perceiver [4]
GMF
G-Perceiver
GMF-MAE"
EVALUATION,0.2505190311418685,"ACC
97.68
97.68
78.64
97.68
97.68
97.68
98.21
99.99
AUC
69.33
79.22
43.92
78.56
93.45
91.88
96.71
99.97"
EVALUATION,0.2512110726643599,"We performed a theoretical performance evaluation on FakeAVCeleb [26], as shown in Table 4. We
302"
EVALUATION,0.2519031141868512,"use a feature extractor that is more compatible with the proposed method and remove the linear layer,
303"
EVALUATION,0.25259515570934254,"denote as GMF-MAE (in Appendix, Fig. 14). For other SOTA methods involved in the comparison,
304"
EVALUATION,0.2532871972318339,"we choose the feature extractor proposed in the original paper as much as possible (MISA utilizes
305"
EVALUATION,0.25397923875432526,"sLSTM [30], UAVM adopts ConvNeXT-B [31], GMF-MAE employs MAE [32, 33]). The remaining
306"
EVALUATION,0.2546712802768166,"methods, including Baseline employs R(2+1)D-18 [28]. Due to the imbalance in the dataset, with
307"
EVALUATION,0.255363321799308,"a ratio of approximately 1:39, the audio ratio is 1:1 and the video ratio is 1:19. UAVM [6] learns a
308"
EVALUATION,0.2560553633217993,"unified representation, thus the easier classification of audio significantly impacts the overall results.
309"
EVALUATION,0.25674740484429065,"Both DrFuse [8] and MISA [16] perform below our expectations; one potential explanation could be
310"
EVALUATION,0.257439446366782,"the influence of sample imbalance on their performance.
311"
EVALUATION,0.25813148788927337,"The performance of GMF remains consistent with the conclusions drawn from Table 2. Furthermore,
312"
EVALUATION,0.25882352941176473,"GMF’s insensitivity to missing modalities effectively mitigates the impact of sample imbalance,
313"
EVALUATION,0.25951557093425603,"avoiding an excessive emphasis on any particular modality. The combination of GMF and MAE [32,
314"
EVALUATION,0.2602076124567474,"33] demonstrates optimal performance limits, validating our approach’s effectiveness in addressing
315"
EVALUATION,0.26089965397923875,"the challenges posed by downstream tasks. We provide a more comprehensive comparison with
316"
EVALUATION,0.2615916955017301,"methods focused on deepfake detection in Table 7 (in Appendix).
317"
CONCLUSION,0.2622837370242215,"6
Conclusion
318"
CONCLUSION,0.2629757785467128,"In this paper, we combine the PNP equation with information entropy theory to introduce a multimodal
319"
CONCLUSION,0.26366782006920414,"fusion method for unrelated input features and downstream task features. The aim is to reduce the
320"
CONCLUSION,0.2643598615916955,"joint entropy of input features while decreasing the downstream task-related information entropy.
321"
CONCLUSION,0.26505190311418686,"Experimental results demonstrate that the proposed method takes a step forward in the generalization
322"
CONCLUSION,0.2657439446366782,"and robustness of multimodal tasks. Meanwhile, the additional burden can be negligible.
323"
CONCLUSION,0.2664359861591695,"GMF comprises basic linear layers and is consequently susceptible to the inherent characteristics of
324"
CONCLUSION,0.2671280276816609,"linear operations, which exhibit growth in parameter count relative to input dimensionality. However,
325"
CONCLUSION,0.26782006920415224,"as per our theoretical framework, the effective component is proportional to the feature dimension. In
326"
CONCLUSION,0.2685121107266436,"forthcoming research, we intend to concentrate on sparsifying mapping matrices to further diminish
327"
CONCLUSION,0.26920415224913496,"parameter count.
328"
REFERENCES,0.2698961937716263,"References
329"
REFERENCES,0.27058823529411763,"[1] Liang, V. W., Y. Zhang, Y. Kwon, et al. Mind the gap: Understanding the modality gap in
330"
REFERENCES,0.271280276816609,"multi-modal contrastive representation learning. In Advances in Neural Information Processing
331"
REFERENCES,0.27197231833910035,"Systems, pages 17612–17625. 2022.
332"
REFERENCES,0.2726643598615917,"[2] Xia, Y., H. Huang, J. Zhu, et al. Achieving cross modal generalization with multimodal unified
333"
REFERENCES,0.27335640138408307,"representation. In Conference and Workshop on Neural Information Processing Systems, pages
334"
REFERENCES,0.2740484429065744,"63529–63541. 2023.
335"
REFERENCES,0.27474048442906573,"[3] Nagrani, A., S. Yang, A. Arnab, et al. Attention bottlenecks for multimodal fusion. In
336"
REFERENCES,0.2754325259515571,"Conference and Workshop on Neural Information Processing Systems, pages 14200–14213.
337"
REFERENCES,0.27612456747404845,"2021.
338"
REFERENCES,0.2768166089965398,"[4] Jaegle, A., F. Gimeno, A. Brock, et al. Perceiver: General perception with iterative attention. In
339"
REFERENCES,0.2775086505190311,"International Conference on Machine Learning. 2021.
340"
REFERENCES,0.2782006920415225,"[5] Liu, L., J. Li, L. Niu, et al. Activity image-to-video retrieval by disentangling appearance and
341"
REFERENCES,0.27889273356401384,"motion. In Association for the Advancement of Artificial Intelligence. 2021.
342"
REFERENCES,0.2795847750865052,"[6] Gong, Y., A. H. Liu, A. Rouditchenko, et al. Uavm: Towards unifying audio and visual models.
343"
REFERENCES,0.28027681660899656,"IEEE Signal Processing Letters, pages 2437–2441, 2022.
344"
REFERENCES,0.28096885813148786,"[7] Ying, X. An overview of overfitting and its solutions. Journal of Physics: Conference Series,
345"
REFERENCES,0.2816608996539792,"page 022022, 2019.
346"
REFERENCES,0.2823529411764706,"[8] Yao, W., K. Yin, W. K. Cheung, et al. Drfuse: Learning disentangled representation for clinical
347"
REFERENCES,0.28304498269896194,"multi-modal fusion with missing modality and modal inconsistency. In Association for the
348"
REFERENCES,0.2837370242214533,"Advancement of Artificial Intelligence, pages 16416–16424. 2024.
349"
REFERENCES,0.2844290657439446,"[9] Ma, M., J. Ren, L. Zhao, et al. Are multimodal transformers robust to missing modality? In
350"
REFERENCES,0.28512110726643597,"IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
351"
REFERENCES,0.28581314878892733,"[10] Wang, H., Y. Chen, C. Ma, et al. Multi-modal learning with missing modality via shared-specific
352"
REFERENCES,0.2865051903114187,"feature modelling. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
353"
REFERENCES,0.28719723183391005,"pages 15878–15887. 2023.
354"
REFERENCES,0.2878892733564014,"[11] Kim, W., B. Son, I. Kim. Vilt: Vision-and-language transformer without convolution or region
355"
REFERENCES,0.2885813148788927,"supervision. In International Conference on Machine Learning. 2021.
356"
REFERENCES,0.2892733564013841,"[12] Li, J., R. R. Selvaraju, A. D. Gotmare, et al. Align before fuse: Vision and language representa-
357"
REFERENCES,0.28996539792387543,"tion learning with momentum distillation. In NeurIPS, pages 9694–9705. 2021.
358"
REFERENCES,0.2906574394463668,"[13] Dou, Z.-Y., Y. Xu, Z. Gan, et al. An empirical study of training end-to-end vision-and-language
359"
REFERENCES,0.29134948096885815,"transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
360"
REFERENCES,0.29204152249134946,"18145–18155. 2021.
361"
REFERENCES,0.2927335640138408,"[14] Xu, R., L. Niu, J. Zhang, et al. A proposal-based approach for activity image-to-video retrieval.
362"
REFERENCES,0.2934256055363322,"In Association for the Advancement of Artificial Intelligence, pages 12524–12531. 2020.
363"
REFERENCES,0.29411764705882354,"[15] Yang, W., X. Zhou, Z. Chen, et al. Avoid-df: Audio-visual joint learning for detecting deepfake.
364"
REFERENCES,0.2948096885813149,"IEEE Transactions on Information Forensics and Security, pages 2015–2029, 2023.
365"
REFERENCES,0.2955017301038062,"[16] Hazarika, D., R. Zimmermann, S. Poria. Misa: Modality-invariant and -specific representations
366"
REFERENCES,0.29619377162629756,"for multimodal sentiment analysis. In ACM International Conference on Multimedia, pages
367"
REFERENCES,0.2968858131487889,"1122–1131. 2020.
368"
REFERENCES,0.2975778546712803,"[17] Wang, F., H. Liu. Understanding the behaviour of contrastive loss. pages 2495–2504. 2020.
369"
REFERENCES,0.29826989619377164,"[18] Boudiaf, M., J. Rony, I. M. Ziko, et al. A unifying mutual information view of metric learning:
370"
REFERENCES,0.29896193771626295,"Cross-entropy vs. pairwise losses. In European Conference on Computer Vision, page 548–564.
371"
REFERENCES,0.2996539792387543,"2020.
372"
REFERENCES,0.30034602076124567,"[19] Shang, Z. W., W. Li, M. Gao, et al. An intelligent fault diagnosis method of multi-scale deep
373"
REFERENCES,0.30103806228373703,"feature fusion based on information entropy. Chinese Journal of Mechanical Engineering, 2021.
374"
REFERENCES,0.3017301038062284,"[20] Jiang, Q., C. Chen, H. Zhao, et al. Understanding and constructing latent modality structures in
375"
REFERENCES,0.30242214532871975,"multi-modal representation learning. In IEEE/CVF Conference on Computer Vision and Pattern
376"
REFERENCES,0.30311418685121105,"Recognition, pages 7661–7671. 2023.
377"
REFERENCES,0.3038062283737024,"[21] Granada, J. R. G., V. A. Kovtunenko. Entropy method for generalized poisson–nernst–planck
378"
REFERENCES,0.3044982698961938,"equations. Analysis and Mathematical Physics, pages 603–619, 2018.
379"
REFERENCES,0.30519031141868513,"[22] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural
380"
REFERENCES,0.3058823529411765,"language supervision. In International Conference on Machine Learning, pages 8748–8763.
381"
REFERENCES,0.3065743944636678,"2021.
382"
REFERENCES,0.30726643598615916,"[23] Frankle, J., M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
383"
REFERENCES,0.3079584775086505,"In International Conference on Learning Representations. 2019.
384"
REFERENCES,0.3086505190311419,"[24] Heilbron, F. C., V. Escorcia, B. Ghanem, et al. Activitynet: A large-scale video benchmark
385"
REFERENCES,0.30934256055363324,"for human activity understanding. In 2015 IEEE Conference on Computer Vision and Pattern
386"
REFERENCES,0.31003460207612454,"Recognition, pages 961–970. 2015.
387"
REFERENCES,0.3107266435986159,"[25] Chen, H., W. Xie, A. Vedaldi, et al. Vggsound: A large-scale audio-visual dataset. In 2020
388"
REFERENCES,0.31141868512110726,"IEEE International Conference on Acoustics, Speech and Signal Processing, pages 721–725.
389"
REFERENCES,0.3121107266435986,"2020.
390"
REFERENCES,0.31280276816609,"[26] Khalid, H., S. Tariq, M. Kim, et al. Fakeavceleb: A novel audio-video multimodal deepfake
391"
REFERENCES,0.3134948096885813,"dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and
392"
REFERENCES,0.31418685121107265,"Benchmarks. 2021.
393"
REFERENCES,0.314878892733564,"[27] Vaswani, A., N. M. Shazeer, N. Parmar, et al. Attention is all you need. In Neural Information
394"
REFERENCES,0.31557093425605537,"Processing Systems. 2017.
395"
REFERENCES,0.31626297577854673,"[28] Tran, D., H. Wang, L. Torresani, et al. A closer look at spatiotemporal convolutions for
396"
REFERENCES,0.3169550173010381,"action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
397"
REFERENCES,0.3176470588235294,"Recognition, pages 6450–6459. 2018.
398"
REFERENCES,0.31833910034602075,"[29] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In 2016 IEEE
399"
REFERENCES,0.3190311418685121,"Conference on Computer Vision and Pattern Recognition, pages 770–778. 2016.
400"
REFERENCES,0.3197231833910035,"[30] Hochreiter, S., J. Schmidhuber. Long short-term memory. Neural Computation, pages 1735–
401"
REFERENCES,0.32041522491349483,"1780, 1997.
402"
REFERENCES,0.32110726643598614,"[31] Todi, A., N. Narula, M. Sharma, et al. Convnext: A contemporary architecture for convolutional
403"
REFERENCES,0.3217993079584775,"neural networks for image classification. In 2023 3rd International Conference on Innovative
404"
REFERENCES,0.32249134948096886,"Sustainable Computational Technologies (CISCT), pages 1–6. 2023.
405"
REFERENCES,0.3231833910034602,"[32] He, K., X. Chen, S. Xie, et al. Masked autoencoders are scalable vision learners. In Proceedings
406"
REFERENCES,0.3238754325259516,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000–16009.
407"
REFERENCES,0.3245674740484429,"2022.
408"
REFERENCES,0.32525951557093424,"[33] Huang, P.-Y., H. Xu, J. B. Li, et al. Masked autoencoders that listen. In Advances in Neural
409"
REFERENCES,0.3259515570934256,"Information Processing Systems. 2022.
410"
REFERENCES,0.32664359861591696,"[34] Kingma, D. P., M. Welling. Auto-encoding variational bayes. In International Conference on
411"
REFERENCES,0.3273356401384083,"Learning Representations. 2014.
412"
REFERENCES,0.32802768166089963,"[35] Hinton, G. E., R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
413"
REFERENCES,0.328719723183391,"Science, pages 504–507, 2006.
414"
REFERENCES,0.32941176470588235,"[36] Mittal, T., U. Bhattacharya, R. Chandra, et al. Emotions don’t lie: An audio-visual deepfake
415"
REFERENCES,0.3301038062283737,"detection method using affective cues. In Proceedings of the 28th ACM International Conference
416"
REFERENCES,0.33079584775086507,"on Multimedia, page 2823–2832. 2020.
417"
REFERENCES,0.33148788927335643,"[37] Nagrani, A., S. Albanie, A. Zisserman. Seeing voices and hearing faces: Cross-modal biometric
418"
REFERENCES,0.33217993079584773,"matching. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
419"
REFERENCES,0.3328719723183391,"8427–8436. 2018.
420"
REFERENCES,0.33356401384083045,"[38] Hu, Y., C. Chen, R. Li, et al. Mir-gan: Refining frame-level modality-invariant representations
421"
REFERENCES,0.3342560553633218,"with adversarial network for audio-visual speech recognition. In Proceedings of the 61st Annual
422"
REFERENCES,0.3349480968858132,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
423"
REFERENCES,0.3356401384083045,"11610–11625. 2023.
424"
REFERENCES,0.33633217993079584,"[39] Shankar, S., L. Thompson, M. Fiterau. Progressive fusion for multimodal integration, 2024.
425"
REFERENCES,0.3370242214532872,"[40] Huang, G., Z. Liu, L. Van Der Maaten, et al. Densely connected convolutional networks. In
426"
REFERENCES,0.33771626297577856,"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269.
427"
REFERENCES,0.3384083044982699,"2017.
428"
REFERENCES,0.3391003460207612,"[41] Skoog, D., D. West, F. Holler, et al. Fundamentals of Analytical Chemistry. 2021.
429"
REFERENCES,0.3397923875432526,"[42] Nakkiran, P., G. Kaplun, Y. Bansal, et al. Deep double descent: Where bigger models and more
430"
REFERENCES,0.34048442906574394,"data hurt. In International Conference on Learning Representations, page 124003. 2020.
431"
REFERENCES,0.3411764705882353,"[43] Krizhevsky, A. Learning multiple layers of features from tiny images, 2009.
432"
REFERENCES,0.34186851211072666,"[44] Howard, A., M. Sandler, B. Chen, et al. Searching for mobilenetv3. In 2019 IEEE/CVF
433"
REFERENCES,0.34256055363321797,"International Conference on Computer Vision, pages 1314–1324. 2019.
434"
REFERENCES,0.34325259515570933,"[45] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers
435"
REFERENCES,0.3439446366782007,"for image recognition at scale. In International Conference on Learning Representations. 2021.
436"
REFERENCES,0.34463667820069205,"[46] Han, S., J. Pool, J. Tran, et al. Learning both weights and connections for efficient neural
437"
REFERENCES,0.3453287197231834,"network. In Advances in Neural Information Processing Systems. 2015.
438"
REFERENCES,0.3460207612456747,"[47] Paszke, A., S. Gross, F. Massa, et al. Pytorch: An imperative style, high-performance deep
439"
REFERENCES,0.3467128027681661,"learning library. In Advances in Neural Information Processing Systems, pages 8024–8035.
440"
REFERENCES,0.34740484429065743,"2019.
441"
REFERENCES,0.3480968858131488,"[48] Deng, J., W. Dong, R. Socher, et al. Imagenet: A large-scale hierarchical image database. In
442"
REFERENCES,0.34878892733564015,"2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255. 2009.
443"
REFERENCES,0.3494809688581315,"[49] Zhou, Y., S.-N. Lim. Joint audio-visual deepfake detection. In IEEE International Conference
444"
REFERENCES,0.3501730103806228,"on Computer Vision, pages 14780–14789. 2021.
445"
REFERENCES,0.3508650519031142,"[50] Cheng, H., Y. Guo, T. Wang, et al. Voice-face homogeneity tells deepfake. ACM Transactions
446"
REFERENCES,0.35155709342560554,"on Multimedia Computing, Communications, and Applications, 2023.
447"
REFERENCES,0.3522491349480969,"[51] Mittal, T., U. Bhattacharya, R. Chandra, et al. Emotions don’t lie: An audio-visual deepfake
448"
REFERENCES,0.35294117647058826,"detection method using affective cues. In ACM International Conference on Multimedia, page
449"
REFERENCES,0.35363321799307956,"2823–2832. 2020.
450"
REFERENCES,0.3543252595155709,"[52] Zadeh, A., P. P. Liang, N. Mazumder, et al. Memory fusion network for multi-view sequential
451"
REFERENCES,0.3550173010380623,"learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence
452"
REFERENCES,0.35570934256055364,"and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI
453"
REFERENCES,0.356401384083045,"Symposium on Educational Advances in Artificial Intelligence, page 5634–5641. 2018.
454"
REFERENCES,0.3570934256055363,"[53] Chugh, K., P. Gupta, A. Dhall, et al. Not made for each other- audio-visual dissonance-based
455"
REFERENCES,0.35778546712802767,"deepfake detection and localization. In ACM International Conference on Multimedia, page
456"
REFERENCES,0.35847750865051903,"439–447. 2020.
457"
REFERENCES,0.3591695501730104,"[54] Hara, K., H. Kataoka, Y. Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and
458"
REFERENCES,0.35986159169550175,"imagenet? In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
459"
REFERENCES,0.36055363321799305,"6546–6555. 2018.
460"
REFERENCES,0.3612456747404844,"Appendix / supplemental material
461"
REFERENCES,0.3619377162629758,"A
Gradient Backward Flow
462"
REFERENCES,0.36262975778546713,"A.1
Definition and Explaination
463"
REFERENCES,0.3633217993079585,"In the gradient backward stage, the gradient is propagated from the output to the input direction
464"
REFERENCES,0.36401384083044985,according to the adjustment of the downstream task loss. The specific gradient backward diagram is
REFERENCES,0.36470588235294116,"Extraction
Fusion
Classification"
REFERENCES,0.3653979238754325,Proposed objective
REFERENCES,0.3660899653979239,Grad of fusion loss
REFERENCES,0.36678200692041524,Grad of task loss
REFERENCES,0.3674740484429066,"Figure 3: The gradient diagram extended from Figure 1, the notation system is consistent with
Figure 1. The blue arrow represents the loss in the fusion stage (Lfusion), and the red arrow
represents the loss in the downstream task (Ltask). The green arrow is related to our redefined
optimization objective, and the meaning is consistent with the green dashed arrow in Figure 1. Not all
multimodal fusion methods have gradients with blue arrows and green arrows. These are not specific
losses, nor are they necessarily individual losses. 465"
REFERENCES,0.3681660899653979,"shown in Figure 3. The gradient generated by the downstream task loss is propagated through the
466"
REFERENCES,0.36885813148788926,"entire network to the input, and the gradient of the fusion stage loss (if any) is propagated from the
467"
REFERENCES,0.3695501730103806,"fusion output feature to the downstream task. The parameter adjustment of the feature extractor is
468"
REFERENCES,0.370242214532872,"affected by the gradient backward of the loss in the fusion stage and the loss of the downstream task.
469"
REFERENCES,0.37093425605536334,"It is worth discussing that the gradient adjustments from downstream task classification loss and
470"
REFERENCES,0.37162629757785465,"fusion-related loss may not necessarily align. Hence, there typically exists a set of hyperparameters
471"
REFERENCES,0.372318339100346,"to balance the impacts of different losses. For instance, in VAE [34], the KL divergence loss and the
472"
REFERENCES,0.37301038062283737,"reconstruction loss serve distinct purposes. The KL divergence loss facilitates model generalization,
473"
REFERENCES,0.3737024221453287,"a significant divergence between VAE and AE [35], while the reconstruction loss is task-specific,
474"
REFERENCES,0.3743944636678201,"reconstructing a sample from the latent space. However, both the KL divergence loss and the
475"
REFERENCES,0.3750865051903114,"reconstruction loss in VAE often cannot simultaneously be zero. The KL divergence loss encourages
476"
REFERENCES,0.37577854671280275,"some randomness in the latent space features, whereas the reconstruction loss favors more consistency
477"
REFERENCES,0.3764705882352941,"in the latent space features. This balancing act is commendable, yet weighting between the losses
478"
REFERENCES,0.3771626297577855,"poses a significant challenge. Hence, when all losses in multi-stage learning bear significance and
479"
REFERENCES,0.37785467128027683,"the gradient descent directions of feature extractors are incongruent, balancing a hyperparameter
480"
REFERENCES,0.3785467128027682,"becomes necessary to harmonize diverse learning objectives.
481"
REFERENCES,0.3792387543252595,"However, not all losses bear significance. Take contrastive loss, for example. It is a downstream
482"
REFERENCES,0.37993079584775086,"task loss in some NMT tasks [22], yet in most EMT tasks, contrastive loss typically operates in
483"
REFERENCES,0.3806228373702422,"the fusion stage, complementing downstream task-relevant cross-entropy losses, to narrow the gap
484"
REFERENCES,0.3813148788927336,"between positive samples in the latent space and push away negative samples. Some studies [1, 20]
485"
REFERENCES,0.38200692041522494,"have demonstrated the existence of gaps between modalities, and smaller gaps are not necessarily
486"
REFERENCES,0.38269896193771624,"better. There are also analyses of the behavior of contrastive loss [17], aiming to minimize mutual
487"
REFERENCES,0.3833910034602076,"information for positive sample pairs and maximize mutual information for negative sample pairs [18].
488"
REFERENCES,0.38408304498269896,"In EMT tasks, if positive and negative sample pairs coexist, as in Audio-Visual Deepfake Detec-
489"
REFERENCES,0.3847750865051903,"tion [15], the contrastive loss in the fusion stage aims to extract consistent information from positive
490"
REFERENCES,0.3854671280276817,"sample pairs (representing real samples) while ensuring inconsistency in negative sample pairs
491"
REFERENCES,0.386159169550173,"(representing fake samples). It must be emphasized that the significant advantage of EMT tasks lies
492"
REFERENCES,0.38685121107266435,"in modality commonality. Some studies have proven the existence of commonality [36, 37], but
493"
REFERENCES,0.3875432525951557,"this doesn’t alter the fact that auditory and visual modalities are fundamentally distinct (not only
494"
REFERENCES,0.38823529411764707,"the semantic gap), with their enriched information not entirely consistent. In action recognition
495"
REFERENCES,0.3889273356401384,"tasks, there is currently no work that effectively achieves this through audio; in speech recognition
496"
REFERENCES,0.38961937716262973,"tasks [38], even with more complex, advanced feature extractors for extracting video features, or
497"
REFERENCES,0.3903114186851211,"introducing priors to isolate video features solely for lip movements, the results are far inferior to
498"
REFERENCES,0.39100346020761245,"audio single modality. While contrastive loss constrains the feature extractor to extract the most
499"
REFERENCES,0.3916955017301038,"effective synchronous-related features, in the absence of a modality [15], it leads to a significant
500"
REFERENCES,0.3923875432525952,"performance decline.
501"
REFERENCES,0.39307958477508653,"Moreover, not all tasks in EMT tasks involve positive and negative sample contrastive learning, so
502"
REFERENCES,0.39377162629757784,"sometimes contrastive loss is equivalent to operating mutual information. For example, in some
503"
REFERENCES,0.3944636678200692,"EMT methods’ decoupling works [8, 16], each modality enjoys a common encoder and a specific
504"
REFERENCES,0.39515570934256056,"encoder, minimizing mutual information for different modalities’ common encoders to homogenize
505"
REFERENCES,0.3958477508650519,"the extracted content and maximizing mutual information for the same modality’s common encoder
506"
REFERENCES,0.3965397923875433,"and specific encoder to heterogenize them, adapting well to the environment of modality absence.
507"
REFERENCES,0.3972318339100346,"However, this method fixes the dimensions of each feature part, and the introduced losses directly
508"
REFERENCES,0.39792387543252594,"manipulate the behavior of the feature extractor, compelling it to extract a predetermined quantity of
509"
REFERENCES,0.3986159169550173,"common and specific features. The design of hyperparameters (encoder dimensions) will alter the
510"
REFERENCES,0.39930795847750866,"behavior of the feature extractor. Additionally, when expanding to more modalities, the training cost
511"
REFERENCES,0.4,"of this method is also worth discussing.
512"
REFERENCES,0.4006920415224913,"A.2
Combine With Residual
513"
REFERENCES,0.4013840830449827,"ResNet [29] solves the bottleneck of the number of network layers, and this epoch-making work
514"
REFERENCES,0.40207612456747405,"allows the number of network layers to be stacked into thousands. A plausible explanation is that
515"
REFERENCES,0.4027681660899654,"it reduces gradient disappearance or gradient explosion in deep networks. We try to explain this
516"
REFERENCES,0.40346020761245677,problem based on our information entropy related theory (Theory 3.1).
REFERENCES,0.40415224913494807,DownSample
REFERENCES,0.40484429065743943,Residual × n
REFERENCES,0.4055363321799308,Residual Forward
REFERENCES,0.40622837370242215,Residual Backward
REFERENCES,0.4069204152249135,Directly Backward
REFERENCES,0.4076124567474048,Totally Backward
REFERENCES,0.4083044982698962,Directly Forward
REFERENCES,0.40899653979238754,Forward Grad. Sum.
REFERENCES,0.4096885813148789,Backward Grad. Sum.
REFERENCES,0.41038062283737026,Totally Forward
REFERENCES,0.4110726643598616,Figure 4: Structure of Residual in Networks. 517
REFERENCES,0.4117647058823529,"The basic block structure of ResNet [29] and the gradient propagation are illustrated in Figure 4. We
518"
ABSTRACT,0.4124567474048443,"abstract it into a more general structure, where the downsampling block is considered as an arbitrary
519"
ABSTRACT,0.41314878892733564,"function f(·, θf), and the residual block is considered as another arbitrary function g(·, θg). Here,
520"
ABSTRACT,0.413840830449827,"both of these arbitrary functions represent a type of network structure (in fact, this structure can be
521"
ABSTRACT,0.41453287197231836,"further generalized), with θf and θg representing the parameters of the functions f and g, respectively.
522"
ABSTRACT,0.41522491349480967,"Same as Eq.(2), the objective of gradient optimization is to optimize these parameters to minimize
523"
ABSTRACT,0.415916955017301,"the conditional entropy of Input X, Y and Output Ypred:
524"
ABSTRACT,0.4166089965397924,"Ypred = (g(f(X, θf) , θg),
L = H[Y | g(f(X, θf) , θg)]
(19)"
ABSTRACT,0.41730103806228375,"The expression for gradient descent can be derived by computing the partial derivatives of the loss
525"
ABSTRACT,0.4179930795847751,"function with respect to the parameters θf and θg. Denote the loss function as Eq.( 19), the gradient
526"
ABSTRACT,0.4186851211072664,"descent expressions are:
527"
ABSTRACT,0.41937716262975777,"∂L
∂(θf, θg) = ∂L"
ABSTRACT,0.42006920415224913,∂θf + ∂L
ABSTRACT,0.4207612456747405,"∂θg ,
∂L
∂θf = ∂L"
ABSTRACT,0.42145328719723185,∂g · ∂g
ABSTRACT,0.42214532871972316,∂f · ∂f
ABSTRACT,0.4228373702422145,"∂θf ,
∂L
∂θg = ∂L"
ABSTRACT,0.4235294117647059,∂g · ∂g
ABSTRACT,0.42422145328719724,"∂θg
(20)"
ABSTRACT,0.4249134948096886,"For functions f positioned further back, their ultimate gradients are influenced by the partial deriva-
528"
ABSTRACT,0.42560553633217996,"tives of the loss function with respect to functions g positioned earlier. If network g is composed of
529"
ABSTRACT,0.42629757785467126,"g1, g2, ..., gn, then during backward, it will be multiplied by numerous coefficients, making it more
530"
ABSTRACT,0.4269896193771626,"prone to gradient vanishing or exploding. The introduction of residuals can alleviate this problem. It
531"
ABSTRACT,0.427681660899654,"is expressed as:
532"
ABSTRACT,0.42837370242214534,"Ypred = (g(f(X, θf) , θg) + f(X, θf)
(21)"
ABSTRACT,0.4290657439446367,"These derivatives represent the directions of steepest descent with respect to the parameters θf and
533"
ABSTRACT,0.429757785467128,"θg, guiding the optimization process towards minimizing the loss function. Rethinking the associated
534"
ABSTRACT,0.43044982698961937,"gradient of f:
535"
ABSTRACT,0.4311418685121107,"∂L
∂θf = ∂L"
ABSTRACT,0.4318339100346021,∂g · ∂g
ABSTRACT,0.43252595155709345,∂f · ∂f
ABSTRACT,0.43321799307958475,∂θf + ∂L
ABSTRACT,0.4339100346020761,∂f · ∂f
ABSTRACT,0.43460207612456747,"∂θf
(22)"
ABSTRACT,0.43529411764705883,"For the two elements of addition, compared to no residual, the first half of the gradient is numerically
536"
ABSTRACT,0.4359861591695502,"consistent, and the second half of the gradient is used as the residual. Obviously, this gradient is
537"
ABSTRACT,0.4366782006920415,"going to be direct.
538"
ABSTRACT,0.43737024221453286,"Even in multimodal tasks, there exist challenges akin to residual issues yet to be resolved [39]. For
539"
ABSTRACT,0.4380622837370242,"instance, the association between feature extractors and downstream tasks may be compromised by
540"
ABSTRACT,0.4387543252595156,"the presence of feature fusion modules, manifested particularly in the introduction of intermediate
541"
ABSTRACT,0.43944636678200694,"gradients by deep fusion mechanisms, leading to gradient explosion or vanishing gradients. One
542"
ABSTRACT,0.4401384083044983,"approach to addressing this is through the incorporation of residuals. Indeed, some experimental
543"
ABSTRACT,0.4408304498269896,"endeavors have already undertaken this step, demonstrating its efficacy. These inferences may serve
544"
ABSTRACT,0.44152249134948096,"as a possible explanation, offering a generalized perspective.
545"
ABSTRACT,0.4422145328719723,"However, residuals alone cannot entirely resolve the issue. Residuals, as a vector addition method,
546"
ABSTRACT,0.4429065743944637,"demand strict consistency in dimensions between inputs and outputs; moreover, excessive layer-by-
547"
ABSTRACT,0.44359861591695504,"layer transmission of residuals may result in the accumulation of low-level semantics onto high-level
548"
ABSTRACT,0.44429065743944635,"semantics, thereby blurring the representations learned by intermediate layers. While it may be
549"
ABSTRACT,0.4449826989619377,"feasible to employ residuals in a smaller phase within the fusion stage, utilizing residuals across
550"
ABSTRACT,0.44567474048442907,"the entire stage not only imposes stringent constraints on inputs and outputs but also risks semantic
551"
ABSTRACT,0.4463667820069204,"ambiguity.
552"
ABSTRACT,0.4470588235294118,"Another method of applying residuals is akin to DenseNet [40], directly stacking channels. This still
553"
ABSTRACT,0.4477508650519031,"necessitates consistency in residual dimensions across different stages but circumvents the issue of
554"
ABSTRACT,0.44844290657439445,"semantic confusion. However, the final classifier remains a linear layer, requiring the flattening of
555"
ABSTRACT,0.4491349480968858,"multiple channels. Based on our theory, regardless of semantic sophistication, their initial origins
556"
ABSTRACT,0.44982698961937717,"remain consistent. As dimensions accumulate, elements describing the same set of features proliferate,
557"
ABSTRACT,0.45051903114186853,"inevitably leading to mutual information and subsequently reducing the conditional entropy relevant
558"
ABSTRACT,0.45121107266435984,"to downstream tasks.
559"
ABSTRACT,0.4519031141868512,"In light of the foregoing analysis, residual connections at the skip-fusion stage can effectively alleviate
560"
ABSTRACT,0.45259515570934256,"the prevalent gradient issues in deep networks. However, this phased residual connection directly
561"
ABSTRACT,0.4532871972318339,"linking feature extractors to downstream tasks rigorously constrains the form of inputs and outputs,
562"
ABSTRACT,0.4539792387543253,"necessitating equilength features and overly blurred semantics, thus failing to achieve optimal effects.
563"
ABSTRACT,0.45467128027681664,"Furthermore, the nature of multimodal tasks diverges from simple downsampling-residual networks,
564"
ABSTRACT,0.45536332179930794,"as gradients stem not only from downstream tasks but also from multiple sources before the fusion
565"
ABSTRACT,0.4560553633217993,"stage. Our proposed method entails reducing the network layers in the fusion stage to align the fusion
566"
ABSTRACT,0.45674740484429066,"gradients with the descent direction of downstream task gradients. Alternatively, the scope of the
567"
ABSTRACT,0.457439446366782,"fusion stage loss function gradient can be restricted.
568"
ABSTRACT,0.4581314878892734,"B
Proof of Theorem 3.4
569"
ABSTRACT,0.4588235294117647,"We explain the derivation of the PNP equation to the proposed loss in detail. As before, let’s assume
570"
ABSTRACT,0.45951557093425605,"that the cell is one-dimensional, and only the direction x exists. -
+ -
+ + + + + + + + + + + + + + + + - - - -
- -
- - - - - - - - -"
ABSTRACT,0.4602076124567474,Voltage (also stimulation)
ABSTRACT,0.46089965397923877,"Figure 5: Schematic diagram of the electrolytic cell, + (orange) and - (black) represent the charged
species (ions and electrodes). There is a boundary b (black line) in the electrolytic cell, assuming that
the positive potential is U0, the negative potential is −U0, and the boundary b is the zero potential. 571"
ABSTRACT,0.4615916955017301,"For the basic Nernst-Planck equation, as shown in Figure 5, the ion p ∈{+, −} in the cell system
572"
ABSTRACT,0.46228373702422143,"conforms to:
573"
ABSTRACT,0.4629757785467128,"Jp = −Dp∇cp(x, t)
|
{z
}
Diffusion"
ABSTRACT,0.46366782006920415,"+ cp(x, t)v
|
{z
}
Advection"
ABSTRACT,0.4643598615916955,+ Dpzpe
ABSTRACT,0.46505190311418687,"kBT cp(x, t)E
|
{z
}
Electromigration (23)"
ABSTRACT,0.4657439446366782,"We abstract the feature vector into a one-dimensional electrolytic cell and need to correspond each
574"
ABSTRACT,0.46643598615916954,"term of the equation to it. Throughout the system, the fluid remains stationary; The electric field E
575"
ABSTRACT,0.4671280276816609,"that guides the movement of ions is generated by the electric potential ϕ and the magnetic field A.
576"
ABSTRACT,0.46782006920415226,"We need to externally excite ϕ and do not additionally apply a magnetic field. The actual learning
577"
ABSTRACT,0.4685121107266436,"rate is usually not very large (< 100), and the charge of the ion is assumed to be very small. This
578"
ABSTRACT,0.4692041522491349,"gradient can be neglected as the magnetic field generated by the excitation.
579"
ABSTRACT,0.4698961937716263,E=−∇ϕ−∂A
ABSTRACT,0.47058823529411764,"∂t
=========⇒
v≡0,A≡0
−Dp∇cp(x, t)
|
{z
}
Diffusion"
ABSTRACT,0.471280276816609,+ Dpzpe
ABSTRACT,0.47197231833910036,"kBT cp(x, t)(−∇ϕ)
|
{z
}
Electromigration (24)"
ABSTRACT,0.4726643598615917,"Our external excitation electric field is constant, so the potential expression can be expressed by the
580"
ABSTRACT,0.473356401384083,"ion concentration.
581"
ABSTRACT,0.4740484429065744,"ϕ(x) = U0 + e
Z x"
ABSTRACT,0.47474048442906575,"0
(c+(y, t)z+ + c−(y, t)z−)dy
(25)"
ABSTRACT,0.4754325259515571,"The final state of the system is that the flux is fixed with respect to time, that is, the partial differential
582"
ABSTRACT,0.47612456747404847,"is zero. From the ion point of view, diffusion and electromigration are in equilibrium.
583"
ABSTRACT,0.47681660899653977,"∂cp(x, t)"
ABSTRACT,0.47750865051903113,"∂t
= −∇· Jp ≈0
(26)"
ABSTRACT,0.4782006920415225,"=⇒−∇˙{ −Dp∇cp(x, t) + Dpzpe"
ABSTRACT,0.47889273356401385,"kBT cp(x, t)[−∇ϕ(x)]} ≈0
(27)"
ABSTRACT,0.4795847750865052,∇2ϕ(x)=−ρ(x)
ABSTRACT,0.4802768166089965,"ϵ0 ,ρ(x)=P"
ABSTRACT,0.4809688581314879,"j zjcj(x,t)
======================⇒
using Poisson equation
(28)"
ABSTRACT,0.48166089965397924,"Dp(∂2cp(x, t)"
ABSTRACT,0.4823529411764706,"∂x2
−zpeF"
ABSTRACT,0.48304498269896196,"kBTϵ0
cp(x, t)
X"
ABSTRACT,0.48373702422145326,"j
zjcj(x, t) + zpe"
ABSTRACT,0.4844290657439446,"kBT
∂cp(x, t)"
ABSTRACT,0.485121107266436,"∂x
dϕ(x)"
ABSTRACT,0.48581314878892734,"dx
) ≈0
(29)"
ABSTRACT,0.4865051903114187,Initial distribution
ABSTRACT,0.48719723183391006,Ideal distribution
ABSTRACT,0.48788927335640137,Actual distribution
ABSTRACT,0.4885813148788927,"Figure 6: Representation of ion distribution. The ordinate represents the ion concentration and
the abscissa represents the electrolytic cell position. 0 is the position of the positive electrode, l is
the position of the negative electrode, and b is the potential equilibrium boundary. The green line
represents a uniform distribution of initial state ions to conform to macroscopic electrical neutrality,
the yellow line represents the ideal electrolysis target, that is, the foreign ions are completely divided
at the equilibrium boundary, and the red line represents the practically possible situation."
ABSTRACT,0.4892733564013841,"In the initial condition, ions undergo spontaneous and uniform distribution through diffusion driven
584"
ABSTRACT,0.48996539792387545,"by Brownian motion (the green line in Figure 6). This dynamic process leads to the establishment of a
585"
ABSTRACT,0.4906574394463668,"heterogeneous distribution of ions within the system. However, as the system approaches the potential
586"
ABSTRACT,0.4913494809688581,"equilibrium boundary b, the electrostatic forces acting on ions become increasingly influential. At
587"
ABSTRACT,0.49204152249134947,"this boundary, denoted as the end condition [41], the principles of electroneutrality come into play.
588"
ABSTRACT,0.49273356401384083,"Here, positive and negative ions are balanced such that their net charge is neutral, resulting in an
589"
ABSTRACT,0.4934256055363322,"electrically neutral region around the potential equilibrium boundary:
590 X"
ABSTRACT,0.49411764705882355,"j
zjcj(x, t) ≈0,
∂2cp(x, t)"
ABSTRACT,0.49480968858131485,"∂x2
+ zpe"
ABSTRACT,0.4955017301038062,"kBT
∂cp(x, t)"
ABSTRACT,0.4961937716262976,"∂x
dϕ(x)"
ABSTRACT,0.49688581314878894,"dx
≈0
(30) 591"
ABSTRACT,0.4975778546712803,"ϕ(0) −ϕ(B) ≈−e
Z B"
ABSTRACT,0.4982698961937716,"0
c−(x, t)z−dx|t=T ≈ϕ(B + 1) −ϕ(L) ≈e
Z L"
ABSTRACT,0.49896193771626296,"B+1
c+(x, t)z+dx|t=T (31)"
ABSTRACT,0.4996539792387543,"The positive and negative properties of diffusion and electromigration are always opposite. If the
592"
ABSTRACT,0.5003460207612457,"ion species used as the external electrode is the same as that of the original solution, then we can
593"
ABSTRACT,0.501038062283737,"approximately assume that the ion on either side of the zero potential boundary b, combined with the
594"
ABSTRACT,0.5017301038062284,"ion equivalent to the external electrode, can reduce the initial solute.
595"
ABSTRACT,0.5024221453287198,"Assuming features from another modality are perfectly ordered, they can serve as a constant stimulus
596"
ABSTRACT,0.5031141868512111,"guiding the ionization of the awaiting electrolytic modality. However, unlike in deep learning, where
597"
ABSTRACT,0.5038062283737024,"the loss function can be equivalent to an external potential, both serve as stimuli capable of guiding
598"
ABSTRACT,0.5044982698961937,"the respective fundamental ion directional motion.
599"
ABSTRACT,0.5051903114186851,"Beginning with two modalities, initially disordered features prompt GMF to attempt cyclic con-
600"
ABSTRACT,0.5058823529411764,"nections, as depicted in the diagram. The imposition of external guidance induces the movement
601"
ABSTRACT,0.5065743944636678,"of feature particles of different polarities in distinct directions, ultimately coalescing at one end.
602"
ABSTRACT,0.5072664359861592,"According to the law of conservation of mass, these aggregated features can be fully reconstructed
603"
ABSTRACT,0.5079584775086505,"into the original modality representation of the guided modality particles at the opposite end.
604"
ABSTRACT,0.5086505190311419,"Expanding to multiple modalities, electrochemical cells allow for parallel multi-level connectivity,
605"
ABSTRACT,0.5093425605536333,"where applying a set of stimuli can simultaneously guide the movement of ions across multiple cell
606"
ABSTRACT,0.5100346020761246,"groups. These potentials, as per the principles of basic circuitry, are distributed across each cell, as
607"
ABSTRACT,0.510726643598616,"shown in Figure 7. -
+ -
+ + + + + + + + + + + + + + + + - - - -
- -
- - - - - - - - - - + - + + + + +
+ + + + + +
+ + + + + - - -
- - - - - - - - - - - - - + - + + + + +
+ +
+ + + + + + + +
+ - - -
- - - - - - - - - - - -"
ABSTRACT,0.5114186851211072,Figure 7: Example diagram of loop guidance. The modes are excited by each other. 608
ABSTRACT,0.5121107266435986,"The PNP equation provides a theoretical basis for GMF, and then we can propose to model material
609"
ABSTRACT,0.5128027681660899,"conservation with a reconstruction loss. The reconstruction loss can well simulate the motion of
610"
ABSTRACT,0.5134948096885813,"particles, and its reduction condition does not lead to ambiguity due to the existence of modality-
611"
ABSTRACT,0.5141868512110727,"specific, as expressed in Eq.( 31).
612"
ABSTRACT,0.514878892733564,"2
4
8
16
32
64
128 70 75 80 85 90 95"
ABSTRACT,0.5155709342560554,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
ABSTRACT,0.5162629757785467,"(a) ResNet in CIFAR-10, Test ACC"
ABSTRACT,0.5169550173010381,"2
4
8
16
32
64
128 0.92 0.94 0.96 0.98 1.00 1.02 1.04"
ABSTRACT,0.5176470588235295,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
ABSTRACT,0.5183391003460207,"(b) ResNet in CIFAR-10, Ratio"
ABSTRACT,0.5190311418685121,"2
4
8
16
32
64
128 80 82 84 86 88 90 92 94"
ABSTRACT,0.5197231833910034,"small
large"
ABSTRACT,0.5204152249134948,"(c) MobileNet in CIFAR-10, Test ACC"
ABSTRACT,0.5211072664359861,"2
4
8
16
32
64
128 0.90 0.92 0.94 0.96 0.98 1.00"
ABSTRACT,0.5217993079584775,"small
large"
ABSTRACT,0.5224913494809689,"(d) MobileNet in CIFAR-10, Ratio"
ABSTRACT,0.5231833910034602,"32
64
128
256
512
1024 76 78 80 82 84 86"
ABSTRACT,0.5238754325259516,"VIT7,Head=8
VIT12,Head=8
VIT12,Head=16"
ABSTRACT,0.524567474048443,"(e) ViT in CIFAR-10, Test ACC"
ABSTRACT,0.5252595155709343,"32
64
128
256
512
1024 0.76 0.78 0.80 0.82 0.84 0.86"
ABSTRACT,0.5259515570934256,"VIT7,Head=8
VIT12,Head=8
VIT12,Head=16"
ABSTRACT,0.5266435986159169,"(f) ViT in CIFAR-10, Ratio"
ABSTRACT,0.5273356401384083,"Figure 8: Evaluate ResNet, MobileNet and ViT test accuracy and the ratio of test accuracy to training
accuracy (denote as ratio) on the CIFAR-10 dataset."
ABSTRACT,0.5280276816608996,"C
Proof of Theorem 3.2
613"
ABSTRACT,0.528719723183391,"Theorem 3.2: The dimension of the feature that is best suited to the downstream task varies, and
614"
ABSTRACT,0.5294117647058824,"there is always an optimal value for this feature. The dimension multiple relationship between each
615"
ABSTRACT,0.5301038062283737,"layer of the feature extractor is fixed, and the initial dimension is adjusted. Too low dimension of the
616"
ABSTRACT,0.5307958477508651,"final output will lead to inefficient representation, and too high dimension will introduce noise. The
617"
ABSTRACT,0.5314878892733564,"existence of an integer lbest such that for any integer l distinct from lbest, the conditional entropy of
618"
ABSTRACT,0.5321799307958478,"the model’s predictions fl(X, θl) is greater than that of the model’s predictions flbest(X, θlbest).
619"
ABSTRACT,0.532871972318339,"∃lbest ∈N, ∀l ∈N, l ̸= lbest, H(Y |fl(X, θl)) > H(Y |flbest(X, θlbest))
(32)"
ABSTRACT,0.5335640138408304,"C.1
Experiment
620"
ABSTRACT,0.5342560553633218,"There was some previous work [42] that demonstrated that this optimal dimension exists. However,
621"
ABSTRACT,0.5349480968858131,"existing methods do not account particularly well for the conditions under which poor fitting occurs,
622"
ABSTRACT,0.5356401384083045,"so we conduct experiments to demonstrate the existence of this phenomenon. At the end we present
623"
ABSTRACT,0.5363321799307958,"a possible conjecture. The existence of this optimal dimension is universal and at the same time
624"
ABSTRACT,0.5370242214532872,"inconsistent. Specifically, each type of feature extractor, each type of dataset, and each corresponding
625"
ABSTRACT,0.5377162629757786,"downstream task have different optimal dimensions.
626"
ABSTRACT,0.5384083044982699,"Our set of experiments is shown in Fig 8. In addition to the intuitive visualization of the validation
627"
ABSTRACT,0.5391003460207613,"accuracy, we also show the ratio of the validation accuracy to the training accuracy, aiming to measure
628"
ABSTRACT,0.5397923875432526,"the validation accuracy and reflect the fitting effect of the model. The closer the ratio is to 1, the
629"
ABSTRACT,0.5404844290657439,"stronger the generalization ability is, and the better the fit is.
630"
ABSTRACT,0.5411764705882353,"In Figure 8 (a) and (b), the evaluation results of ResNet [29] on CIFAR-10 [43] are presented. As the
631"
ABSTRACT,0.5418685121107266,"dimensionality increases, the testing performance of the model improves, and the performance range
632"
ABSTRACT,0.542560553633218,"stabilizes. However, with a twofold increase in dimensionality, the variation in testing performance
633"
ABSTRACT,0.5432525951557093,"diminishes, approaching zero. In other words, doubling the parameter count does not yield any
634"
ABSTRACT,0.5439446366782007,"improvement. Additionally, for larger networks like ResNet110, performance begins to decline.
635"
ABSTRACT,0.5446366782006921,"Furthermore, while absolute performance is increasing, the ratio is declining, indicating a weakening
636"
ABSTRACT,0.5453287197231834,"in generalization capability.
637"
ABSTRACT,0.5460207612456748,"Figure 8 (c) and (d) depict the evaluation results of MobileNetV3 [44] on CIFAR-10 [43], showing
638"
ABSTRACT,0.5467128027681661,"conclusions similar to those of ResNet. For larger networks like MobileNetV3-Large, at lower
639"
ABSTRACT,0.5474048442906574,"dimensionalities, its generalization capability is significantly lower compared to simpler networks.
640"
ABSTRACT,0.5480968858131487,"Figure 8 (e) and (f) illustrate the evaluation results of ViT [45] on CIFAR-10 [43]. As ViT is based
641"
ABSTRACT,0.5487889273356401,"on transformers [27] and possesses a global receptive field, its base dimensionality is significantly
642"
ABSTRACT,0.5494809688581315,"larger than that of convolutional neural networks. Both in terms of absolute performance and ratio,
643"
ABSTRACT,0.5501730103806228,"its optimal representation dimensionality approaches 256, distinct from other networks.
644"
ABSTRACT,0.5508650519031142,"2
4
8
16
32
64
128 30 40 50 60 70"
ABSTRACT,0.5515570934256055,"ResNet20
ResNet44
ResNet110"
ABSTRACT,0.5522491349480969,"(a) ResNet in CIFAR-100, Test ACC"
ABSTRACT,0.5529411764705883,"2
4
8
16
32
64
128 0.7 0.8 0.9 1.0 1.1"
ABSTRACT,0.5536332179930796,"1.2
ResNet20
ResNet44
ResNet110"
ABSTRACT,0.554325259515571,"(b) ResNet in CIFAR-100, Ratio"
ABSTRACT,0.5550173010380622,"Figure 9: Evaluate ResNet test accuracy and the ratio of test accuracy to training accuracy (denote as
ratio) on the CIFAR-100 dataset."
ABSTRACT,0.5557093425605536,"However, it is worth noting that the presence of optimal features is not only closely related to network
645"
ABSTRACT,0.556401384083045,"type and structure, but also to the dataset and downstream tasks. We chose CIFAR-100 [43] for this
646"
ABSTRACT,0.5570934256055363,"set of comparative experiments. This is because its data volume is consistent with CIFAR-10, but
647"
ABSTRACT,0.5577854671280277,"with more categories and greater difficulty. The experimental results of ResNet [29] evaluated on
648"
ABSTRACT,0.558477508650519,"CIFAR-100 are shown in Figure 9. Compared to the results shown in Figure 8(a) and (b), firstly, the
649"
ABSTRACT,0.5591695501730104,"impact of different dimensions on accuracy is more significant (for example, the maximum difference
650"
ABSTRACT,0.5598615916955018,"in test performance of ResNet-20 on CIFAR-10 is about 25%, exceeding 40% here); for ResNet-110,
651"
ABSTRACT,0.5605536332179931,"excessive dimensions no longer lead to performance stabilization, but rather a visible performance
652"
ABSTRACT,0.5612456747404845,"decline.
653"
ABSTRACT,0.5619377162629757,"The experimental results demonstrate the existence of an optimal dimensionality. This dimensionality
654"
ABSTRACT,0.5626297577854671,"may vary based on the different structures of networks. Hence, the concept of optimal dimensionality
655"
ABSTRACT,0.5633217993079584,"should be discussed in consideration of multiple external conditions.
656"
ABSTRACT,0.5640138408304498,"D
Proof of Theorem 3.3
657"
ABSTRACT,0.5647058823529412,"Theorem 3.3: The feature extractor is fixed, and its original output feature dimension l is mapped
658"
ABSTRACT,0.5653979238754325,"to nl, and finally back to l. The mapping result is used as the basis for the downstream task. The
659"
ABSTRACT,0.5660899653979239,"performance of downstream tasks is infinitely close to the original performance as n increases, but
660"
ABSTRACT,0.5667820069204152,"never greater than the original performance. For magnification n > 1, n ∈Z, mapping matrix
661"
ABSTRACT,0.5674740484429066,"U1 ∈Rl×nl and U2 ∈Rnl×l, For the output features f(X, θ) ∈Rl and Y :
662"
ABSTRACT,0.568166089965398,"H(Y |f(X, θ)) < H(Y |U1 · (U2 · f(X, θ)))
(33)
663"
ABSTRACT,0.5688581314878892,"limn→∞H(Y |U1 · (U2 · f(X, θ)))) = H(Y |f(X, θ)
(34)"
ABSTRACT,0.5695501730103806,"1
4
8
16
64 88 89 90 91 92 93 94 95"
ABSTRACT,0.5702422145328719,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
ABSTRACT,0.5709342560553633,"(a) ResNet in CIFAR-10, Test ACC"
ABSTRACT,0.5716262975778547,"1
4
8
16
64 0.89 0.90 0.91 0.92 0.93 0.94 0.95"
ABSTRACT,0.572318339100346,"ResNet20
ResNet32
ResNet44
ResNet56
ResNet110"
ABSTRACT,0.5730103806228374,"(b) ResNet in CIFAR-10, Ratio"
ABSTRACT,0.5737024221453287,"Figure 10: Evaluate ResNet test accuracy and the ratio of test accuracy to training accuracy (denote
as ratio) on the CIFAR-10 dataset."
ABSTRACT,0.5743944636678201,"D.1
Theoretically
664"
ABSTRACT,0.5750865051903115,"Denote V = f(X, θ), the rank of each stage:
665 V =  "
ABSTRACT,0.5757785467128028,"v1
v2
...
vl "
ABSTRACT,0.5764705882352941,",
r(V ) ≤l,
r(U2) ≤l,
r(U2 · V ) ≤min(r(V ), r(U2)) ≤l,
(35) 666"
ABSTRACT,0.5771626297577854,"r(U1) ≤l
r(U1 · (U2 · f(X, θ)) ≤l
(36)"
ABSTRACT,0.5778546712802768,"The mapped rank is always less than or equal to the original rank. That is, downstream task-relevant
667"
ABSTRACT,0.5785467128027681,"features may be compressed while not generating features out of thin air. Eq. (33) gets the certificate.
668"
ABSTRACT,0.5792387543252595,"For Eq.(34), we discuss the problem from pruning, linear algebra and probability theory. Neural
669"
ABSTRACT,0.5799307958477509,"networks are often overparameterized, requiring more network parameters than needed to get a
670"
ABSTRACT,0.5806228373702422,"good fit. In theory [23], however, only a subset of these parameters are useful in practice. Hence,
671"
ABSTRACT,0.5813148788927336,"some knowledge distillation methods such as teacher-student networks and pruning [46]. These
672"
ABSTRACT,0.582006920415225,"tested models maintain good performance while removing most of the parameters, which proves that
673"
ABSTRACT,0.5826989619377163,"overparameterization is a common phenomenon. We interpret it as a probabilistic problem, that is,
674"
ABSTRACT,0.5833910034602076,"the effective parameters are generated with a certain probability. Overparameterization significantly
675"
ABSTRACT,0.5840830449826989,"improves the effective parameter generation, and knowledge distillation removes these redundant and
676"
ABSTRACT,0.5847750865051903,"invalid parameters.
677"
ABSTRACT,0.5854671280276816,"Let A ∈Rnd×d be a learnable matrix (n >> 1). Act on v ∈Rd to complete the mapping from
678"
ABSTRACT,0.586159169550173,"lower dimension to higher dimension:
679"
ABSTRACT,0.5868512110726644,"A = [a1, a2, . . . , and],
ˆv = Av = [ˆv1, ˆv2, . . . , ˆvnd]T
(37)"
ABSTRACT,0.5875432525951557,"Denote ˆv ∈Rnd as the mapping result. ˆvk represents the k-th row element. For any two of these
680"
ABSTRACT,0.5882352941176471,"row vectors ai and aj (i ̸= j). They have a ratio c for their first elements.A necessary and sufficient
681"
ABSTRACT,0.5889273356401384,"condition for linearity between two vectors can be extended to the following: for any element in the
682"
ABSTRACT,0.5896193771626298,"same row of these two vectors, the ratio should be c.
683"
ABSTRACT,0.5903114186851212,"ai = [ai1, ai2, . . . , aid],
aj = [aj1, aj2, . . . , ajd],
c = ai1"
ABSTRACT,0.5910034602076124,"aj1
(38) 684 d
X t=1"
ABSTRACT,0.5916955017301038,"ajt
ait
= c
(39)"
ABSTRACT,0.5923875432525951,"If A is learnable, each element will have a different weight for each parameter adjustment. denote
685"
ABSTRACT,0.5930795847750865,P( ajt
ABSTRACT,0.5937716262975778,"ait = c) as the probability that the proportion of the t-th element of ai and aj is equal to
686"
ABSTRACT,0.5944636678200692,"c, which cannot be determined directly because the input sample is uncertain. In the context of
687"
ABSTRACT,0.5951557093425606,"neural networks, the adjustment of gradients can be regarded as following a continuous probability
688"
ABSTRACT,0.5958477508650519,"distribution. Consequently, the probability of the adjustment taking on a specific constant value
689"
ABSTRACT,0.5965397923875433,"is zero (does not imply impossibility). By cumulatively multiplying this probability, we get the
690"
ABSTRACT,0.5972318339100346,"probability that the two column vectors are linearly related in gradient descent.
691 d
Y"
ABSTRACT,0.5979238754325259,"t=1
P(ajt"
ABSTRACT,0.5986159169550173,"ait
= c) ≈0
(40)"
ABSTRACT,0.5993079584775086,"However, for a d-dimensional vector, there cannot be more than d linearly independent features. To
692"
ABSTRACT,0.6,"simplify the expression, we assume that Eq.(40) is a fixed value on the interval (0,1). The probability
693"
ABSTRACT,0.6006920415224913,"that exactly d-dimensional features are linearly dependent is given by:
694"
ABSTRACT,0.6013840830449827,"(nd)!
d!(nd −d)!( d
Y"
ABSTRACT,0.6020761245674741,"t=1
P(ajt"
ABSTRACT,0.6027681660899654,"ait
= c))d(1 − d
Y"
ABSTRACT,0.6034602076124568,"t=1
P(ajt"
ABSTRACT,0.6041522491349481,"ait
= c))nd−d
(41) 695"
ABSTRACT,0.6048442906574395,"(nd + 1)!
d!(nd + 1 −d)!( d
Y"
ABSTRACT,0.6055363321799307,"t=1
P(ajt"
ABSTRACT,0.6062283737024221,"ait
= c))d(1 − d
Y"
ABSTRACT,0.6069204152249135,"t=1
P(ajt"
ABSTRACT,0.6076124567474048,"ait
= c))nd+1−d
(42)"
ABSTRACT,0.6083044982698962,"In deep learning methods, the feature dimension is usually not set too small, d is sufficiently
696"
ABSTRACT,0.6089965397923875,"large. Combined with gradient descent, the parameter adjustment is random, the linear correlation
697"
ABSTRACT,0.6096885813148789,"probability of two random features is close to 0.
698 d
Y"
ABSTRACT,0.6103806228373703,"t=1
P(ajt"
ABSTRACT,0.6110726643598616,"ait
= c) ≈0,
Eq.(42)
Eq.(41) =
nd + 1
nd + 1 −d d
Y"
ABSTRACT,0.611764705882353,"t=1
P(ajt"
ABSTRACT,0.6124567474048442,"ait
= c) ≈1 +
d
nd + 1 −d ≥1
(43)"
ABSTRACT,0.6131487889273356,"Consider mapping matrix U2 ∈Rnl×l. As n increases, the probability of rank l increases. The same
699"
ABSTRACT,0.613840830449827,"is true for the matrix U1 ∈Rl×nl. Therefore, as the probability of two correlation matrices being full
700"
ABSTRACT,0.6145328719723183,"rank becomes larger, a larger n helps to restore the original representation under the premise that the
701"
ABSTRACT,0.6152249134948097,"network does not involve unexpected situations such as gradient explosion and vanishing gradients.
702"
ABSTRACT,0.615916955017301,"However, it can be determined that when n is less than 1 (n > 0), there must be information loss. This
703"
ABSTRACT,0.6166089965397924,"is because the upper limit of the rank of a matrix depends on the smaller value of the number of rows,
704"
ABSTRACT,0.6173010380622838,"columns. Furthermore, it is not appropriate to increase the number of parameters blindly, which will
705"
ABSTRACT,0.6179930795847751,"lead to an exponential number of parameters.
706"
ABSTRACT,0.6186851211072665,"D.2
Experiment
707"
ABSTRACT,0.6193771626297578,"64
128
256
512
1024
2048
4096
8192
16384
Dimention 10 20 30 40 50 60 70"
ABSTRACT,0.6200692041522491,Val Acc
ABSTRACT,0.6207612456747404,"ResNet18
ResNet34
ResNet50
ResNet101"
ABSTRACT,0.6214532871972318,"Figure 11: Theoretical validation on ImageNet on the performance impact of raising and then reducing
the original features. The horizontal coordinate represents the mapped feature dimension, the upper
bound is the best performance, the lower bound is the worst performance, and the ordinate represents
the validation set accuracy. The dashed line represents the results reported for directly validating the
performance of the pretrained model."
ABSTRACT,0.6221453287197232,"We employed pre-trained ResNet-18, ResNet-34, ResNet-50, and ResNet-101 [29] models provided
708"
ABSTRACT,0.6228373702422145,"by PyTorch [47], removing their classifiers to obtain raw features with dimensions of 512, 512, 2048,
709"
ABSTRACT,0.6235294117647059,"and 2048 respectively. After freezing the other layers, we mapped these original features to another
710"
ABSTRACT,0.6242214532871972,"dimension and subsequently retrained the classifiers based on these new features. As depicted in
711"
ABSTRACT,0.6249134948096886,"Figure 11, where the abscissa represents the dimensions of the mapped features and the ordinate
712"
ABSTRACT,0.62560553633218,"represents the classification accuracy of the new classifier on the ImageNet [48] validation set. Our
713"
ABSTRACT,0.6262975778546713,"experimental hyperparameter design and optimizer were identical to those reported in the original
714"
ABSTRACT,0.6269896193771626,"paper. We recorded the validation accuracy every 400 iterations, and if the accuracy did not improve
715"
ABSTRACT,0.6276816608996539,"for 10 consecutive validations, training was terminated prematurely. The final results are depicted in
716"
ABSTRACT,0.6283737024221453,"a bar chart, where the upper and lower bounds represent the maximum and minimum values of the
717"
ABSTRACT,0.6290657439446367,"validation accuracy.
718"
ABSTRACT,0.629757785467128,"It can be observed that larger mapping dimensions lead to faster convergence and yield better results.
719"
ABSTRACT,0.6304498269896194,"Smaller mapping dimensions, especially when they are smaller than the original dimensions, not
720"
ABSTRACT,0.6311418685121107,"only exhibit significant differences in upper and lower bounds of validation accuracy but also witness
721"
ABSTRACT,0.6318339100346021,"a substantial decrease in the upper limit. This observation aligns with our theoretical expectations.
722"
ABSTRACT,0.6325259515570935,"When the scaling factor n is close to 4, the performance loss has entered the acceptable range.
723"
ABSTRACT,0.6332179930795848,"E
Different Between Theorem 3.2 and Theorem 3.3
724"
ABSTRACT,0.6339100346020762,"Both Theorem 3.2 and Theorem 3.3 focus on the dimension of presentation. The most significant
725"
ABSTRACT,0.6346020761245674,"difference between the two theories is what the original input was.
726"
ABSTRACT,0.6352941176470588,"Theorem 3.2 is for the case where the sample is known and the representation is unknown, and this
727"
ABSTRACT,0.6359861591695501,"representation contains relevant information and irrelevant information. Therefore, this theory is
728"
ABSTRACT,0.6366782006920415,"more about the number of parameters needed to characterize, the minimum dimension needed to get
729"
ABSTRACT,0.6373702422145329,"the best performance, or the best performance in the minimum dimension. In this paper, this theory
730"
ABSTRACT,0.6380622837370242,"emphasizes the necessity of unequal-length fusion, and points out and proves through experiments
731"
ABSTRACT,0.6387543252595156,"that equal-length fusion may bring the problem of feature redundancy or feature missing, which not
732"
ABSTRACT,0.639446366782007,"only increases the unnecessary amount of computation, but also affects the performance to some
733"
ABSTRACT,0.6401384083044983,"extent.
734"
ABSTRACT,0.6408304498269897,"Theorem 3.3 is to analyze the influence of linear mapping on the representation in the case of known
735"
ABSTRACT,0.6415224913494809,"representation and unknown samples. Our proposed GMF method is very simple and contains only a
736"
ABSTRACT,0.6422145328719723,"number of linear layers, achieving the performance of larger parameter fusion methods of previous
737"
ABSTRACT,0.6429065743944636,"works. However, our original intention is not to be guided by experimental results, but to theoretically
738"
ABSTRACT,0.643598615916955,"analyze whether the possible information loss is acceptable. We expect our work to be interpretable
739"
ABSTRACT,0.6442906574394464,"and applicable.
740"
ABSTRACT,0.6449826989619377,"F
Derivation of Conjecture 3.1
741"
ABSTRACT,0.6456747404844291,"Conjecture 3.1: Rely on Theorem 3.1, 3.2, 3.3, we propose an conjecture that a boundary of perfor-
742"
ABSTRACT,0.6463667820069204,"mance limitation exists, determined by downstream-related entropy. Theoretically, by establishing a
743"
ABSTRACT,0.6470588235294118,"direct correspondence between the extractor and classifier, fusion method can enhance the limitation
744"
ABSTRACT,0.6477508650519032,"boundary, further improve performance.
745"
ABSTRACT,0.6484429065743945,"Based on the proof of Theorem 3.2, one of the foundations of learning in neural networks is gradient
746"
ABSTRACT,0.6491349480968858,"descent, which presuppositions that gradients can be backpropagated. Every tuning of the learnable
747"
ABSTRACT,0.6498269896193771,"parameters will eventually be implemented on the original input. Assuming that the feature extractor
748"
ABSTRACT,0.6505190311418685,"is fixed, the original input at this time is the feature output by the feature extractor. For any learnable
749"
ABSTRACT,0.6512110726643598,"parameter, the value of a certain sample can be expressed by an exact formula. For a completely
750"
ABSTRACT,0.6519031141868512,"consistent input, it is assumed that its downstream task-related information entropy can be efficiently
751"
ABSTRACT,0.6525951557093426,"calculated, and its information entropy minimum is certain. Therefore, there is a performance upper
752"
ABSTRACT,0.6532871972318339,"bound, depending on how the existing features are utilized.
753"
ABSTRACT,0.6539792387543253,"In practical deep learning tasks, the input features are often not fixed, and gradients need to propagate
754"
ABSTRACT,0.6546712802768166,"to be able to fully determine the original samples—which must also be fully determined. We continue
755"
ABSTRACT,0.655363321799308,"to analyze the feature layers outputted by the feature extractor, assuming that the relevant information
756"
ABSTRACT,0.6560553633217993,"entropy of downstream tasks can be manually calculated. Thus, for the output features at a certain
757"
ABSTRACT,0.6567474048442906,"moment, the lower bound of the conditional entropy of downstream tasks can still be computed,
758"
ABSTRACT,0.657439446366782,"which represents the performance upper bound.
759"
ABSTRACT,0.6581314878892733,"Therefore, the entire multimodal learning network is divided into two parts: one is the lower bound of
760"
ABSTRACT,0.6588235294117647,"the conditional entropy of the feature extractor output relative to the original samples, and the other
761"
ABSTRACT,0.6595155709342561,"is the lower bound of the conditional entropy of downstream tasks relative to the feature extractor
762"
ABSTRACT,0.6602076124567474,"output. The former is a prerequisite for the latter sequentially. However, as stated in the formulas,
763"
ABSTRACT,0.6608996539792388,"assuming the existence of fusion loss and downstream task loss, and the gradient descent directions
764"
ABSTRACT,0.6615916955017301,"are not completely consistent, let the weight of the fusion loss Lfusion be λ1, and the loss of the
765"
ABSTRACT,0.6622837370242215,"downstream task Ltask be λ2, the total loss can be expressed as:
766"
ABSTRACT,0.6629757785467129,"L = λ1Lfusion + λ2Ltask
(44)"
ABSTRACT,0.6636678200692041,"The learning task is to minimize the training loss. Assuming that λ1Lfusion > λ2Ltask, then the
767"
ABSTRACT,0.6643598615916955,"gradient of the feature extractor will tend more toward the fusion loss. In severe cases (such as opposite
768"
ABSTRACT,0.6650519031141868,"gradient descent directions), the downstream task-related loss will be completely overshadowed.
769"
ABSTRACT,0.6657439446366782,"This also leads to an increase in the lower bound of the conditional entropy of downstream tasks
770"
ABSTRACT,0.6664359861591695,"and a decrease in the theoretical performance upper limit. Therefore, we assume that there exists a
771"
ABSTRACT,0.6671280276816609,"boundary, which is determined by the theoretical performance upper bound based on a fixed feature
772"
ABSTRACT,0.6678200692041523,"and the conditional entropy of downstream tasks. Regardless of how outstanding the fusion method
773"
ABSTRACT,0.6685121107266436,"design is, just like the principle of energy conservation law for features, the final task performance of
774"
ABSTRACT,0.669204152249135,"this method cannot exceed this upper bound.
775"
ABSTRACT,0.6698961937716263,Perceiver
ABSTRACT,0.6705882352941176,GMF+Perceiver GMF
ABSTRACT,0.671280276816609,Boundary of our proposed objective
ABSTRACT,0.6719723183391003,Boundary of tranditional objective
ABSTRACT,0.6726643598615917,Performance
ABSTRACT,0.673356401384083,Training step
ABSTRACT,0.6740484429065744,Figure 12: Visualizing performance improvements based on conjectures.
ABSTRACT,0.6747404844290658,"The reason why our proposed GMF achieves performance improvement is not due to the performance
776"
ABSTRACT,0.6754325259515571,"enhancement brought by the complex fusion network, but rather from a higher upper bound. However,
777"
ABSTRACT,0.6761245674740485,"in reality, we are still far from this upper bound, and demonstrating our method as a precursor to
778"
ABSTRACT,0.6768166089965398,"other methods can prove this point well. As shown in the Figure 12, we have drawn a hypothetical
779"
ABSTRACT,0.6775086505190312,"graph based on the data reported in the paper. Assuming GMF as the precondition method for the
780"
ABSTRACT,0.6782006920415224,"Perceiver [4], the result that GMF can be on par with complex networks with almost no resource
781"
ABSTRACT,0.6788927335640138,"consumption is interpretable.
782"
ABSTRACT,0.6795847750865052,"G
Experiment Supplement
783"
ABSTRACT,0.6802768166089965,"G.1
Implement Details
784"
ABSTRACT,0.6809688581314879,"For all experiments, we use apex to optimize the v-memory and the parameter is set to ’O1’. The
785"
ABSTRACT,0.6816608996539792,"random seed fixed ’1’ for all GMF related implementation. However, for some dropout design
786"
ABSTRACT,0.6823529411764706,"methods, the reported experimental results may not be fully reproducible. More details are listed in
787"
ABSTRACT,0.683044982698962,"Table 5
788"
ABSTRACT,0.6837370242214533,"(1) torch.manual_seed(seed)
789"
ABSTRACT,0.6844290657439447,"(2) torch.cuda.manual_seed_all(seed)
790"
ABSTRACT,0.6851211072664359,"(3) np.random.seed(seed)
791"
ABSTRACT,0.6858131487889273,"(4) random.seed(seed)
792"
ABSTRACT,0.6865051903114187,"Table 5: Details of GMF. Momentun of SGD = 0.9, weight delay=1e-4. Lr_scheduler = ReduceL-
ROnPlateau, factor=0.1, patience=1000."
ABSTRACT,0.68719723183391,"Dataset
Lr
Optimizer
Batchsize
Epoch
Input Shape"
ABSTRACT,0.6878892733564014,"VGGSound
0.01
SGD
64
20
[512,512]
ActivityNet
0.01
SGD
64
20
[4096,128], [4096,4096], [128,128]
FakeAVCeleb
0.01
SGD
64
20
[512,512], [128,512]"
ABSTRACT,0.6885813148788927,"G.2
Information About Preprocess and Baseline
793"
ABSTRACT,0.6892733564013841,"For the VGGSound dataset, we downsample all currently available samples to 5fps, with videos of
794"
ABSTRACT,0.6899653979238755,"size 192*256 and audio sampled at 16000 Hz, while retaining only the first 9 seconds to accommodate
795"
ABSTRACT,0.6906574394463668,"most samples that are not exactly 10 seconds in duration. Samples without audio or video are removed.
796"
ABSTRACT,0.6913494809688582,"As for FakeAVCeleb, since the fabricated samples exhibit a global range of fabrication, with lengths
797"
ABSTRACT,0.6920415224913494,"distributed from 0.8 seconds and above, and a frame rate between 15 to 30 fps, we only select the
798"
ABSTRACT,0.6927335640138408,"first 8 frames along with their corresponding audio to ensure adaptability to the dataset.
799"
ABSTRACT,0.6934256055363321,"We employ the default testing-training split provided by VGGSound. For FakeAVCeleb, consistent
800"
ABSTRACT,0.6941176470588235,"with much of the prior work focused on audio-visual deepfake detection, we first sort each class
801"
ABSTRACT,0.6948096885813149,"(real audio-real video, real audio-fake video, fake audio-real video, fake audio-fake video), and then
802"
ABSTRACT,0.6955017301038062,"allocate the first 70% of each class to the training set and the remaining 30% to the testing set.
803"
ABSTRACT,0.6961937716262976,"The baseline of VGGSound pretrained on KINETICS400V1. Momentun of SGD = 0.9, weight
804"
ABSTRACT,0.696885813148789,"delay=1e-4. Adam betas=(0.5, 0.9). lr_scheduler = ReduceLROnPlateau, factor=0.1, patience=1000
805"
ABSTRACT,0.6975778546712803,"on VGGSound, factor=0.5, patience=50, verbose=True, min_lr=1e-8 on FakeAVCeleb. The generated
806"
ABSTRACT,0.6982698961937717,"audio sequence is quite long, and the receptive field of the convolutional network is not global. To
807"
ABSTRACT,0.698961937716263,"address this potential issue, we stack the audio into a timing sequence (144000 to 9 × 16000).
808"
ABSTRACT,0.6996539792387543,"Audio wave transform to input tensor by MelSpectrogram(sample_rate=16000, n_fft=400,
809"
ABSTRACT,0.7003460207612456,"win_length=400, hop_length=160, n_mels=192) for VGGSound and log (abs (STFT(n_fft=1024,
810"
ABSTRACT,0.701038062283737,"hop_length=256, win_length=1024,window=blackman_window(1024))) + 1e-8) for FakeAVCeleb.
811"
ABSTRACT,0.7017301038062284,"Video frame directly as the input of network without any preprocess.
812"
ABSTRACT,0.7024221453287197,"The hyperparameter as shown in Table 6
813"
ABSTRACT,0.7031141868512111,Table 6: Model Details of Baseline.
ABSTRACT,0.7038062283737024,"Model
Modality
Dataset
Role
Lr
Optimizer
Batchsize
Epoch
Input Shape"
ABSTRACT,0.7044982698961938,"R2+1D-18
A
VGGSound
Baseline
0.01
SGD
64
20
[9,192,100,1]
R2+1D-18
V
VGGSound
Baseline
0.01
SGD
64
20
[15,128,96,3]
R2+1D-18
A
FakeAVCeleb
Baseline
0.005
Adam
16
5
[1,1,513,60]
R2+1D-18
V
FakeAVCeleb
Baseline
0.005
Adam
16
5
[8,224,224,3]"
ABSTRACT,0.7051903114186852,"G.3
Compared Method Structure
814"
ABSTRACT,0.7058823529411765,"The integration of our method with others is depicted in Figure 1. By bypassing modality-invariant
815"
ABSTRACT,0.7065743944636678,"features and focusing solely on modality-specific features for fusion, the input represents a representa-
816"
ABSTRACT,0.7072664359861591,"tion with reduced mutual information. This leads to a reduction in the conditional entropy magnitude
817"
ABSTRACT,0.7079584775086505,"during the initial stages. The backend component may consist of a simple concatenation or modules
818"
ABSTRACT,0.7086505190311418,"proposed by other methods. Consequently, the inherent characteristics of GMF are constrained by
819"
ABSTRACT,0.7093425605536332,"the limitations of the backend module. Comparatively, the limitations are minimal with a simple
820"
ABSTRACT,0.7100346020761246,"concatenation approach.
821"
ABSTRACT,0.7107266435986159,"H
More Comparison on the FakeAVCeleb Dataset
822"
ABSTRACT,0.7114186851211073,"We expanded the experimental table of FakeAVCeleb (Tab. 4) in the main text, incorporating additional
823"
ABSTRACT,0.7121107266435986,"comparisons focused on deepfake detection methods. Apart from the experiments reported in the
824"
ABSTRACT,0.71280276816609,front-end GMF
ABSTRACT,0.7134948096885814,Extractor Input
ABSTRACT,0.7141868512110726,"GMF
Output"
ABSTRACT,0.714878892733564,back-end
ABSTRACT,0.7155709342560553,Module Task
ABSTRACT,0.7162629757785467,Related
ABSTRACT,0.7169550173010381,Module
ABSTRACT,0.7176470588235294,Output
ABSTRACT,0.7183391003460208,"Final
Output"
ABSTRACT,0.7190311418685121,"Figure 13: G-structure schematic diagram. Yellow feature vectors represent modality-invariant
features, while other colors represent modality-specific features for each modality. Modality-invariant
features are directly connected to downstream task classifiers, while modality-specific features serve
as new inputs to the fusion module."
ABSTRACT,0.7197231833910035,"Table 7: Performance on the FakeAVCeleb dataset. ’A’, ’V’ represents the separate audio and video
modality, and the input of the other modality is 0. ’AV’ stands for the full sample."
ABSTRACT,0.7204152249134949,"Method
Extractor
ACC(%)
AUC(%)"
ABSTRACT,0.7211072664359861,"A
V
AV
A
V
AV"
ABSTRACT,0.7217993079584775,"Baseline
R(2+1)D-18 [28]
98.76
95.36
97.68
99.73
54.38
69.33
MISA [16]
sLSTM [30]
61.75
71.66
97.68
58.98
64.76
79.22
UAVM [6]
ConvNeXT-B [31]
86.59
73.05
78.64
83.98
69.38
43.92"
ABSTRACT,0.7224913494809688,"DrFuse [8]
R(2+1)D-18 [28]
66.83
75.35
97.68
62.86
69.33
78.56
Perceiver [4]
R(2+1)D-18 [28]
56.81
78.84
97.68
51.36
58.20
93.45
Joint-AV [49]
R(2+1)D-18 [28] and 1D CNN
81,77
65.73
71.81
79.25
69.61
75.81
AVoiD-DF [15]
ViT [45]
70.31
55.81
83.71
72.41
57.21
89.21"
ABSTRACT,0.7231833910034602,"VFD [50]
Transformer [27]
-
-
81.52
-
-
86.11
Emo-Foren [51]
2D CNN and MFN [52]
-
-
78.11
-
-
79.81
MDS [53]
3D-ResNet [54] Like
-
-
83.86
-
-
86.71"
ABSTRACT,0.7238754325259515,"GMF
R(2+1)D-18 [28]
71.25
85.33
97.68
67.32
64.91
91.88
GMF+Perceiver
R(2+1)D-18 [28]
64.01
82.15
98.21
66.53
62.42
96.71
GMF-MAE
MAE [32] and Audio-MAE [33]
99.79
97.74
99.99
99.73
89.82
99.97"
ABSTRACT,0.7245674740484429,"original text, the remaining data were sourced from the original paper proposing the method. Here,
825"
ABSTRACT,0.7252595155709343,"VFD [50], Emo-Foren [51], and MDS [53] are grouped together because these methods transform
826"
ABSTRACT,0.7259515570934256,"EMT into NMT. Specifically, these methods emphasize certain aspects of multimodal performance:
827"
ABSTRACT,0.726643598615917,"VFD emphasizes identity, Emo-Foren emphasizes emotion, and MDS, while not emphasizing a
828"
ABSTRACT,0.7273356401384083,"specific mode, relies on computing confidence in matching a certain segment. Therefore, the modal
829"
ABSTRACT,0.7280276816608997,"absence evaluation for these methods is marked as ’-’, indicating absence. Importantly, our method
830"
ABSTRACT,0.728719723183391,"effectively connects representations of different modalities without additional overhead for AE-based
831"
ABSTRACT,0.7294117647058823,"feature extractors, resulting in a highly competitive outcome.
832"
ABSTRACT,0.7301038062283737,"H.1
The reason of choose FakeAVCeleb
833"
ABSTRACT,0.730795847750865,"The FakeAVCeleb dataset is atypical, characterized by severe class imbalance posing significant
834"
ABSTRACT,0.7314878892733564,"challenges to methods. Specifically, the ratio of positive to negative samples is 1:1 for audio and 1:19
835"
ABSTRACT,0.7321799307958478,"for video, resulting in an overall ratio of 1:39. While audio often possesses discriminative capabilities
836"
ABSTRACT,0.7328719723183391,"less susceptible to the impact of sample proportions, most methods evaluated in our tests struggle to
837"
ABSTRACT,0.7335640138408305,"effectively address this bias.
838"
ABSTRACT,0.7342560553633218,"Addressing this imbalance necessitates multimodal methods to learn weight disparities across modali-
839"
ABSTRACT,0.7349480968858132,"ties to mitigate the effects of sample bias. This manifests in high accuracy (ACC) juxtaposed with
840"
ABSTRACT,0.7356401384083044,"mismatched area under the curve (AUC). Methods capable of mitigating this bias often underutilize it,
841"
ABSTRACT,0.7363321799307958,"resulting in suboptimal ACC. However, in real-world scenarios, the distribution of genuine and fake
842"
ABSTRACT,0.7370242214532872,"samples may not be balanced, and a single segment may not adequately represent an event. Hence,
843"
ABSTRACT,0.7377162629757785,"the adaptability of methods to publicly available datasets warrants thorough investigation.
844"
ABSTRACT,0.7384083044982699,"I
GMF with AutoEncoder (GMF-AE/MAE)
845"
ABSTRACT,0.7391003460207612,"AutoEncoder [35] (AE) was initially proposed as a feature dimensionality reduction method, com-
846"
ABSTRACT,0.7397923875432526,"pressing samples into a latent space and then reconstructing them to retain the details of the entire
847"
ABSTRACT,0.740484429065744,"sample in the latent space features. Masked AutoEncoder [32] (MAE) is a more powerful feature
848"
ABSTRACT,0.7411764705882353,"extraction variant of AE, masking most of the original samples and reconstructing them, allowing
849"
ABSTRACT,0.7418685121107267,"the model to learn more sample features. An intriguing point is that this concept can be seamlessly
850"
ABSTRACT,0.742560553633218,integrated with GMF (proposed Generalized Multimodal Fusion). GMF applies reconstruction loss
ABSTRACT,0.7432525951557093,Because [Mask] are so [Mask] Text Audio
ABSTRACT,0.7439446366782007,Visual
ABSTRACT,0.744636678200692,Because you are so beautiful
ABSTRACT,0.7453287197231834,Output of Proposed GMF
ABSTRACT,0.7460207612456747,invariant
ABSTRACT,0.7467128027681661,specific
ABSTRACT,0.7474048442906575,encoder
ABSTRACT,0.7480968858131488,decoder
ABSTRACT,0.7487889273356402,latent X X' Mask
ABSTRACT,0.7494809688581315,Reconstruct
ABSTRACT,0.7501730103806228,Reconstruct
ABSTRACT,0.7508650519031141,Reconstruct
ABSTRACT,0.7515570934256055,Reconstruct
ABSTRACT,0.7522491349480969,Figure 14: Simplified GMF frame diagram with MAE as feature extractor. 851
ABSTRACT,0.7529411764705882,"as an incentive, directing the movement of different types of features towards a relatively ordered
852"
ABSTRACT,0.7536332179930796,"representation. Combining with PNP equations and our theoretical framework, this requires two
853"
ABSTRACT,0.754325259515571,"additional linear layers for feature dimensionality reduction, expansion, and a linear layer for recon-
854"
ABSTRACT,0.7550173010380623,"struction. Thus, the additional overhead includes a reconstruction loss and the mentioned linear layers.
855"
ABSTRACT,0.7557093425605537,"However, due to the nature of AE, this feature-directional movement process can be accomplished
856"
ABSTRACT,0.756401384083045,"during AE’s self-supervised learning. Specifically, instead of feeding complete latent space features
857"
ABSTRACT,0.7570934256055364,"into the Decoder, a combination of features from the corresponding Encoder and another modality
858"
ABSTRACT,0.7577854671280276,"Encoder is used. This allows us to achieve our goal without any additional overhead. However, if
859"
ABSTRACT,0.758477508650519,"done so, explicit boundary delineation is necessary, which may affect model performance; moreover,
860"
ABSTRACT,0.7591695501730104,"this learning process must be conducted in a multi-modal task, and features must be intact during the
861"
ABSTRACT,0.7598615916955017,"learning process.
862"
ABSTRACT,0.7605536332179931,"The specific structural diagram is shown in Figure 14. Here, we also consider the transformer [27]
863"
ABSTRACT,0.7612456747404844,"initially used for text as a variant of MAE, video encoder is MAE [32] and the Audio encoder is
864"
ABSTRACT,0.7619377162629758,"Audio-MAE [33].
865"
ABSTRACT,0.7626297577854672,"J
GMF Architecture
866"
ABSTRACT,0.7633217993079585,Algorithm 1 GMF (Generalized Multimodal Fusion)
ABSTRACT,0.7640138408304499,"1: Input: Dimensions dims, multiple m, boundary b
2: Output: x1, x2, x1recon, x2recon
3: procedure GMF(x1, x2)
4:
x1inv, x1spec ←ELEMENTSPLIT(x1, dims[0], min(dims), m, b)
5:
x2inv, x2spec ←ELEMENTSPLIT(x2, dims[1], min(dims), m, b)
6:
x1 ←concat([x2inv, x1spec])
7:
x2 ←concat([x1inv, x2spec])
8:
x1re ←Linear(x1, dims[0] + min(dims), dims[0])
9:
x2re ←Linear(x2, dims[1] + min(dims), dims[1])
10:
return x1, x2, x1re, x2re
11: end procedure"
ABSTRACT,0.7647058823529411,Algorithm 2 ElementSplit
ABSTRACT,0.7653979238754325,"1: Input: Dimension dim, min_len, multiple m, boundary b
2: Output: xinv, xspec
3: procedure ELEMENTSPLIT(x)
4:
b ←⌊b × m × dim⌋
5:
d ←m × dim
6:
x ←Linear(x, dim, m × dim)
7:
xinv ←Linear(x[:, : b], b, min_len)
8:
xspec ←Linear(x[:, b : d], d −b, dim)
9:
return xinv, xspec
10: end procedure"
ABSTRACT,0.7660899653979238,Algorithm 3 Reconstruction Loss
ABSTRACT,0.7667820069204152,"1: Input: xrecon, xoriginal
2: Output: Reconstruction loss
3: procedure RECONSTRUCTIONLOSS(xrecon, xoriginal)
4:
return MSE(xrecon, xoriginal)
5: end procedure"
ABSTRACT,0.7674740484429066,"NeurIPS Paper Checklist
867"
CLAIMS,0.7681660899653979,"1. Claims
868"
CLAIMS,0.7688581314878893,"Question: Do the main claims made in the abstract and introduction accurately reflect the
869"
CLAIMS,0.7695501730103806,"paper’s contributions and scope?
870"
CLAIMS,0.770242214532872,"Answer: [Yes]
871"
CLAIMS,0.7709342560553634,"Justification: We propose a generalized multimodal fusion model via Poisson-Nernst-Planck
872"
CLAIMS,0.7716262975778547,"Equation, which can greatly improve the fusion performance.
873"
CLAIMS,0.772318339100346,"Guidelines:
874"
CLAIMS,0.7730103806228373,"• The answer NA means that the abstract and introduction do not include the claims
875"
CLAIMS,0.7737024221453287,"made in the paper.
876"
CLAIMS,0.77439446366782,"• The abstract and/or introduction should clearly state the claims made, including the
877"
CLAIMS,0.7750865051903114,"contributions made in the paper and important assumptions and limitations. A No or
878"
CLAIMS,0.7757785467128028,"NA answer to this question will not be perceived well by the reviewers.
879"
CLAIMS,0.7764705882352941,"• The claims made should match theoretical and experimental results, and reflect how
880"
CLAIMS,0.7771626297577855,"much the results can be expected to generalize to other settings.
881"
CLAIMS,0.7778546712802769,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
882"
CLAIMS,0.7785467128027682,"are not attained by the paper.
883"
LIMITATIONS,0.7792387543252595,"2. Limitations
884"
LIMITATIONS,0.7799307958477508,"Question: Does the paper discuss the limitations of the work performed by the authors?
885"
LIMITATIONS,0.7806228373702422,"Answer: [Yes]
886"
LIMITATIONS,0.7813148788927335,"Justification: We provide an outlook on future work in the conclusion section and write a
887"
LIMITATIONS,0.7820069204152249,"separate subsection in the appendix to state limitations.
888"
LIMITATIONS,0.7826989619377163,"Guidelines:
889"
LIMITATIONS,0.7833910034602076,"• The answer NA means that the paper has no limitation while the answer No means that
890"
LIMITATIONS,0.784083044982699,"the paper has limitations, but those are not discussed in the paper.
891"
LIMITATIONS,0.7847750865051903,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
892"
LIMITATIONS,0.7854671280276817,"• The paper should point out any strong assumptions and how robust the results are to
893"
LIMITATIONS,0.7861591695501731,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
894"
LIMITATIONS,0.7868512110726643,"model well-specification, asymptotic approximations only holding locally). The authors
895"
LIMITATIONS,0.7875432525951557,"should reflect on how these assumptions might be violated in practice and what the
896"
LIMITATIONS,0.788235294117647,"implications would be.
897"
LIMITATIONS,0.7889273356401384,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
898"
LIMITATIONS,0.7896193771626298,"only tested on a few datasets or with a few runs. In general, empirical results often
899"
LIMITATIONS,0.7903114186851211,"depend on implicit assumptions, which should be articulated.
900"
LIMITATIONS,0.7910034602076125,"• The authors should reflect on the factors that influence the performance of the approach.
901"
LIMITATIONS,0.7916955017301038,"For example, a facial recognition algorithm may perform poorly when image resolution
902"
LIMITATIONS,0.7923875432525952,"is low or images are taken in low lighting. Or a speech-to-text system might not be
903"
LIMITATIONS,0.7930795847750866,"used reliably to provide closed captions for online lectures because it fails to handle
904"
LIMITATIONS,0.7937716262975778,"technical jargon.
905"
LIMITATIONS,0.7944636678200692,"• The authors should discuss the computational efficiency of the proposed algorithms
906"
LIMITATIONS,0.7951557093425605,"and how they scale with dataset size.
907"
LIMITATIONS,0.7958477508650519,"• If applicable, the authors should discuss possible limitations of their approach to
908"
LIMITATIONS,0.7965397923875432,"address problems of privacy and fairness.
909"
LIMITATIONS,0.7972318339100346,"• While the authors might fear that complete honesty about limitations might be used by
910"
LIMITATIONS,0.797923875432526,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
911"
LIMITATIONS,0.7986159169550173,"limitations that aren’t acknowledged in the paper. The authors should use their best
912"
LIMITATIONS,0.7993079584775087,"judgment and recognize that individual actions in favor of transparency play an impor-
913"
LIMITATIONS,0.8,"tant role in developing norms that preserve the integrity of the community. Reviewers
914"
LIMITATIONS,0.8006920415224914,"will be specifically instructed to not penalize honesty concerning limitations.
915"
THEORY ASSUMPTIONS AND PROOFS,0.8013840830449827,"3. Theory Assumptions and Proofs
916"
THEORY ASSUMPTIONS AND PROOFS,0.802076124567474,"Question: For each theoretical result, does the paper provide the full set of assumptions and
917"
THEORY ASSUMPTIONS AND PROOFS,0.8027681660899654,"a complete (and correct) proof?
918"
THEORY ASSUMPTIONS AND PROOFS,0.8034602076124567,"Answer: [Yes]
919"
THEORY ASSUMPTIONS AND PROOFS,0.8041522491349481,"Justification: We briefly introduce the theory in the main text and present the necessary for-
920"
THEORY ASSUMPTIONS AND PROOFS,0.8048442906574395,"mulas that will help the reader to understand. For each proposed theory and hypothesis, the
921"
THEORY ASSUMPTIONS AND PROOFS,0.8055363321799308,"necessary derivations and experimental results are proved in the corresponding subsections
922"
THEORY ASSUMPTIONS AND PROOFS,0.8062283737024222,"of the experimental section and appendix.
923"
THEORY ASSUMPTIONS AND PROOFS,0.8069204152249135,"Guidelines:
924"
THEORY ASSUMPTIONS AND PROOFS,0.8076124567474049,"• The answer NA means that the paper does not include theoretical results.
925"
THEORY ASSUMPTIONS AND PROOFS,0.8083044982698961,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
926"
THEORY ASSUMPTIONS AND PROOFS,0.8089965397923875,"referenced.
927"
THEORY ASSUMPTIONS AND PROOFS,0.8096885813148789,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
928"
THEORY ASSUMPTIONS AND PROOFS,0.8103806228373702,"• The proofs can either appear in the main paper or the supplemental material, but if
929"
THEORY ASSUMPTIONS AND PROOFS,0.8110726643598616,"they appear in the supplemental material, the authors are encouraged to provide a short
930"
THEORY ASSUMPTIONS AND PROOFS,0.8117647058823529,"proof sketch to provide intuition.
931"
THEORY ASSUMPTIONS AND PROOFS,0.8124567474048443,"• Inversely, any informal proof provided in the core of the paper should be complemented
932"
THEORY ASSUMPTIONS AND PROOFS,0.8131487889273357,"by formal proofs provided in appendix or supplemental material.
933"
THEORY ASSUMPTIONS AND PROOFS,0.813840830449827,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
934"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8145328719723184,"4. Experimental Result Reproducibility
935"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8152249134948096,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
936"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.815916955017301,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
937"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8166089965397924,"of the paper (regardless of whether the code and data are provided or not)?
938"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8173010380622837,"Answer: [Yes]
939"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8179930795847751,"Justification: We fixed the random seed, and used source code that can be directly used as a
940"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8186851211072664,"class, rather than pseudocode, during the introduction to the algorithm. For fixed features,
941"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8193771626297578,"we will provide pre-trained models with the results of feature extraction. In addition, we
942"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8200692041522492,"have added our code in the attachment, and annotate the reference projects in detail. In
943"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8207612456747405,"addition, we present the detailed hyperparameters of the replication method in a tabular
944"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8214532871972319,"form in the appendix, and open source this part of the code.
945"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8221453287197232,"Guidelines:
946"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8228373702422145,"• The answer NA means that the paper does not include experiments.
947"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8235294117647058,"• If the paper includes experiments, a No answer to this question will not be perceived
948"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8242214532871972,"well by the reviewers: Making the paper reproducible is important, regardless of
949"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8249134948096886,"whether the code and data are provided or not.
950"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8256055363321799,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
951"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8262975778546713,"to make their results reproducible or verifiable.
952"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8269896193771626,"• Depending on the contribution, reproducibility can be accomplished in various ways.
953"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.827681660899654,"For example, if the contribution is a novel architecture, describing the architecture fully
954"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8283737024221454,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
955"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8290657439446367,"be necessary to either make it possible for others to replicate the model with the same
956"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.829757785467128,"dataset, or provide access to the model. In general. releasing code and data is often
957"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8304498269896193,"one good way to accomplish this, but reproducibility can also be provided via detailed
958"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8311418685121107,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
959"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.831833910034602,"of a large language model), releasing of a model checkpoint, or other means that are
960"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325259515570934,"appropriate to the research performed.
961"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8332179930795848,"• While NeurIPS does not require releasing code, the conference does require all submis-
962"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8339100346020761,"sions to provide some reasonable avenue for reproducibility, which may depend on the
963"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8346020761245675,"nature of the contribution. For example
964"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8352941176470589,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
965"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8359861591695502,"to reproduce that algorithm.
966"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8366782006920416,"(b) If the contribution is primarily a new model architecture, the paper should describe
967"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8373702422145328,"the architecture clearly and fully.
968"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8380622837370242,"(c) If the contribution is a new model (e.g., a large language model), then there should
969"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8387543252595155,"either be a way to access this model for reproducing the results or a way to reproduce
970"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8394463667820069,"the model (e.g., with an open-source dataset or instructions for how to construct
971"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8401384083044983,"the dataset).
972"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8408304498269896,"(d) We recognize that reproducibility may be tricky in some cases, in which case
973"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.841522491349481,"authors are welcome to describe the particular way they provide for reproducibility.
974"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8422145328719723,"In the case of closed-source models, it may be that access to the model is limited in
975"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8429065743944637,"some way (e.g., to registered users), but it should be possible for other researchers
976"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8435986159169551,"to have some path to reproducing or verifying the results.
977"
OPEN ACCESS TO DATA AND CODE,0.8442906574394463,"5. Open access to data and code
978"
OPEN ACCESS TO DATA AND CODE,0.8449826989619377,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
979"
OPEN ACCESS TO DATA AND CODE,0.845674740484429,"tions to faithfully reproduce the main experimental results, as described in supplemental
980"
OPEN ACCESS TO DATA AND CODE,0.8463667820069204,"material?
981"
OPEN ACCESS TO DATA AND CODE,0.8470588235294118,"Answer: [Yes]
982"
OPEN ACCESS TO DATA AND CODE,0.8477508650519031,"Justification: We will open source all original code (such as the implementation and repro-
983"
OPEN ACCESS TO DATA AND CODE,0.8484429065743945,"duction method of the proposed method), for non-original code, we will mark the reference
984"
OPEN ACCESS TO DATA AND CODE,0.8491349480968858,"project.
985"
OPEN ACCESS TO DATA AND CODE,0.8498269896193772,"Guidelines:
986"
OPEN ACCESS TO DATA AND CODE,0.8505190311418686,"• The answer NA means that paper does not include experiments requiring code.
987"
OPEN ACCESS TO DATA AND CODE,0.8512110726643599,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
988"
OPEN ACCESS TO DATA AND CODE,0.8519031141868512,"public/guides/CodeSubmissionPolicy) for more details.
989"
OPEN ACCESS TO DATA AND CODE,0.8525951557093425,"• While we encourage the release of code and data, we understand that this might not be
990"
OPEN ACCESS TO DATA AND CODE,0.8532871972318339,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
991"
OPEN ACCESS TO DATA AND CODE,0.8539792387543252,"including code, unless this is central to the contribution (e.g., for a new open-source
992"
OPEN ACCESS TO DATA AND CODE,0.8546712802768166,"benchmark).
993"
OPEN ACCESS TO DATA AND CODE,0.855363321799308,"• The instructions should contain the exact command and environment needed to run to
994"
OPEN ACCESS TO DATA AND CODE,0.8560553633217993,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
995"
OPEN ACCESS TO DATA AND CODE,0.8567474048442907,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
996"
OPEN ACCESS TO DATA AND CODE,0.857439446366782,"• The authors should provide instructions on data access and preparation, including how
997"
OPEN ACCESS TO DATA AND CODE,0.8581314878892734,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
998"
OPEN ACCESS TO DATA AND CODE,0.8588235294117647,"• The authors should provide scripts to reproduce all experimental results for the new
999"
OPEN ACCESS TO DATA AND CODE,0.859515570934256,"proposed method and baselines. If only a subset of experiments are reproducible, they
1000"
OPEN ACCESS TO DATA AND CODE,0.8602076124567474,"should state which ones are omitted from the script and why.
1001"
OPEN ACCESS TO DATA AND CODE,0.8608996539792387,"• At submission time, to preserve anonymity, the authors should release anonymized
1002"
OPEN ACCESS TO DATA AND CODE,0.8615916955017301,"versions (if applicable).
1003"
OPEN ACCESS TO DATA AND CODE,0.8622837370242215,"• Providing as much information as possible in supplemental material (appended to the
1004"
OPEN ACCESS TO DATA AND CODE,0.8629757785467128,"paper) is recommended, but including URLs to data and code is permitted.
1005"
OPEN ACCESS TO DATA AND CODE,0.8636678200692042,"6. Experimental Setting/Details
1006"
OPEN ACCESS TO DATA AND CODE,0.8643598615916955,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1007"
OPEN ACCESS TO DATA AND CODE,0.8650519031141869,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1008"
OPEN ACCESS TO DATA AND CODE,0.8657439446366783,"results?
1009"
OPEN ACCESS TO DATA AND CODE,0.8664359861591695,"Answer: [Yes]
1010"
OPEN ACCESS TO DATA AND CODE,0.8671280276816609,"Justification: We briefly describe the hyperparameter Settings and experimental equipment
1011"
OPEN ACCESS TO DATA AND CODE,0.8678200692041522,"used in the experiment section of the main text. For methods not previously available on the
1012"
OPEN ACCESS TO DATA AND CODE,0.8685121107266436,"corresponding dataset, our preset hyperparameters are described in detail in the appendix.
1013"
OPEN ACCESS TO DATA AND CODE,0.8692041522491349,"Guidelines:
1014"
OPEN ACCESS TO DATA AND CODE,0.8698961937716263,"• The answer NA means that the paper does not include experiments.
1015"
OPEN ACCESS TO DATA AND CODE,0.8705882352941177,"• The experimental setting should be presented in the core of the paper to a level of detail
1016"
OPEN ACCESS TO DATA AND CODE,0.871280276816609,"that is necessary to appreciate the results and make sense of them.
1017"
OPEN ACCESS TO DATA AND CODE,0.8719723183391004,"• The full details can be provided either with the code, in appendix, or as supplemental
1018"
OPEN ACCESS TO DATA AND CODE,0.8726643598615917,"material.
1019"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.873356401384083,"7. Experiment Statistical Significance
1020"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8740484429065744,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1021"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8747404844290657,"information about the statistical significance of the experiments?
1022"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8754325259515571,"Answer: [No]
1023"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8761245674740484,"Justification: We fixed all random seeds to 1, and no data augmentation was applied to the
1024"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8768166089965398,"original data, so the results should be similar across multiple runs. Furthermore, feature
1025"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8775086505190312,"extractors are mostly aligned, which has nothing to do with dataset integrity.
1026"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8782006920415225,"Guidelines:
1027"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8788927335640139,"• The answer NA means that the paper does not include experiments.
1028"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8795847750865052,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1029"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8802768166089966,"dence intervals, or statistical significance tests, at least for the experiments that support
1030"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8809688581314878,"the main claims of the paper.
1031"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8816608996539792,"• The factors of variability that the error bars are capturing should be clearly stated (for
1032"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8823529411764706,"example, train/test split, initialization, random drawing of some parameter, or overall
1033"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8830449826989619,"run with given experimental conditions).
1034"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8837370242214533,"• The method for calculating the error bars should be explained (closed form formula,
1035"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8844290657439446,"call to a library function, bootstrap, etc.)
1036"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.885121107266436,"• The assumptions made should be given (e.g., Normally distributed errors).
1037"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8858131487889274,"• It should be clear whether the error bar is the standard deviation or the standard error
1038"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8865051903114187,"of the mean.
1039"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8871972318339101,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1040"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8878892733564013,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1041"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8885813148788927,"of Normality of errors is not verified.
1042"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.889273356401384,"• For asymmetric distributions, the authors should be careful not to show in tables or
1043"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8899653979238754,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1044"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8906574394463668,"error rates).
1045"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8913494809688581,"• If error bars are reported in tables or plots, The authors should explain in the text how
1046"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8920415224913495,"they were calculated and reference the corresponding figures or tables in the text.
1047"
EXPERIMENTS COMPUTE RESOURCES,0.8927335640138409,"8. Experiments Compute Resources
1048"
EXPERIMENTS COMPUTE RESOURCES,0.8934256055363322,"Question: For each experiment, does the paper provide sufficient information on the com-
1049"
EXPERIMENTS COMPUTE RESOURCES,0.8941176470588236,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1050"
EXPERIMENTS COMPUTE RESOURCES,0.8948096885813149,"the experiments?
1051"
EXPERIMENTS COMPUTE RESOURCES,0.8955017301038062,"Answer: [Yes]
1052"
EXPERIMENTS COMPUTE RESOURCES,0.8961937716262975,"Justification: For each set of experiments in the main text, we report not only the evaluation
1053"
EXPERIMENTS COMPUTE RESOURCES,0.8968858131487889,"metrics, but also the number of parameters, the amount of computation, and the time required
1054"
EXPERIMENTS COMPUTE RESOURCES,0.8975778546712803,"for inference.
1055"
EXPERIMENTS COMPUTE RESOURCES,0.8982698961937716,"Guidelines:
1056"
EXPERIMENTS COMPUTE RESOURCES,0.898961937716263,"• The answer NA means that the paper does not include experiments.
1057"
EXPERIMENTS COMPUTE RESOURCES,0.8996539792387543,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1058"
EXPERIMENTS COMPUTE RESOURCES,0.9003460207612457,"or cloud provider, including relevant memory and storage.
1059"
EXPERIMENTS COMPUTE RESOURCES,0.9010380622837371,"• The paper should provide the amount of compute required for each of the individual
1060"
EXPERIMENTS COMPUTE RESOURCES,0.9017301038062284,"experimental runs as well as estimate the total compute.
1061"
EXPERIMENTS COMPUTE RESOURCES,0.9024221453287197,"• The paper should disclose whether the full research project required more compute
1062"
EXPERIMENTS COMPUTE RESOURCES,0.903114186851211,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1063"
EXPERIMENTS COMPUTE RESOURCES,0.9038062283737024,"didn’t make it into the paper).
1064"
CODE OF ETHICS,0.9044982698961938,"9. Code Of Ethics
1065"
CODE OF ETHICS,0.9051903114186851,"Question: Does the research conducted in the paper conform, in every respect, with the
1066"
CODE OF ETHICS,0.9058823529411765,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1067"
CODE OF ETHICS,0.9065743944636678,"Answer: [Yes]
1068"
CODE OF ETHICS,0.9072664359861592,"Justification: We’ve read the spec and followed it to the letter. Our paper does not involve
1069"
CODE OF ETHICS,0.9079584775086506,"human subjects and the datasets used are all open source datasets. These data sets are all
1070"
CODE OF ETHICS,0.9086505190311419,"instant downloads, and we cannot obtain them when the sample provider sets the sample
1071"
CODE OF ETHICS,0.9093425605536333,"private.
1072"
CODE OF ETHICS,0.9100346020761245,"Guidelines:
1073"
CODE OF ETHICS,0.9107266435986159,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1074"
CODE OF ETHICS,0.9114186851211072,"• If the authors answer No, they should explain the special circumstances that require a
1075"
CODE OF ETHICS,0.9121107266435986,"deviation from the Code of Ethics.
1076"
CODE OF ETHICS,0.91280276816609,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1077"
CODE OF ETHICS,0.9134948096885813,"eration due to laws or regulations in their jurisdiction).
1078"
BROADER IMPACTS,0.9141868512110727,"10. Broader Impacts
1079"
BROADER IMPACTS,0.914878892733564,"Question: Does the paper discuss both potential positive societal impacts and negative
1080"
BROADER IMPACTS,0.9155709342560554,"societal impacts of the work performed?
1081"
BROADER IMPACTS,0.9162629757785468,"Answer: [NA]
1082"
BROADER IMPACTS,0.916955017301038,"Justification: Our approach is a deep learning architecture, and the selection of downstream
1083"
BROADER IMPACTS,0.9176470588235294,"tasks does not require a natural person to do it. This is not directly related to society.
1084"
BROADER IMPACTS,0.9183391003460207,"Guidelines:
1085"
BROADER IMPACTS,0.9190311418685121,"• The answer NA means that there is no societal impact of the work performed.
1086"
BROADER IMPACTS,0.9197231833910035,"• If the authors answer NA or No, they should explain why their work has no societal
1087"
BROADER IMPACTS,0.9204152249134948,"impact or why the paper does not address societal impact.
1088"
BROADER IMPACTS,0.9211072664359862,"• Examples of negative societal impacts include potential malicious or unintended uses
1089"
BROADER IMPACTS,0.9217993079584775,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1090"
BROADER IMPACTS,0.9224913494809689,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1091"
BROADER IMPACTS,0.9231833910034603,"groups), privacy considerations, and security considerations.
1092"
BROADER IMPACTS,0.9238754325259516,"• The conference expects that many papers will be foundational research and not tied
1093"
BROADER IMPACTS,0.9245674740484429,"to particular applications, let alone deployments. However, if there is a direct path to
1094"
BROADER IMPACTS,0.9252595155709342,"any negative applications, the authors should point it out. For example, it is legitimate
1095"
BROADER IMPACTS,0.9259515570934256,"to point out that an improvement in the quality of generative models could be used to
1096"
BROADER IMPACTS,0.9266435986159169,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1097"
BROADER IMPACTS,0.9273356401384083,"that a generic algorithm for optimizing neural networks could enable people to train
1098"
BROADER IMPACTS,0.9280276816608997,"models that generate Deepfakes faster.
1099"
BROADER IMPACTS,0.928719723183391,"• The authors should consider possible harms that could arise when the technology is
1100"
BROADER IMPACTS,0.9294117647058824,"being used as intended and functioning correctly, harms that could arise when the
1101"
BROADER IMPACTS,0.9301038062283737,"technology is being used as intended but gives incorrect results, and harms following
1102"
BROADER IMPACTS,0.9307958477508651,"from (intentional or unintentional) misuse of the technology.
1103"
BROADER IMPACTS,0.9314878892733564,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1104"
BROADER IMPACTS,0.9321799307958477,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1105"
BROADER IMPACTS,0.9328719723183391,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1106"
BROADER IMPACTS,0.9335640138408304,"feedback over time, improving the efficiency and accessibility of ML).
1107"
SAFEGUARDS,0.9342560553633218,"11. Safeguards
1108"
SAFEGUARDS,0.9349480968858132,"Question: Does the paper describe safeguards that have been put in place for responsible
1109"
SAFEGUARDS,0.9356401384083045,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1110"
SAFEGUARDS,0.9363321799307959,"image generators, or scraped datasets)?
1111"
SAFEGUARDS,0.9370242214532872,"Answer: [NA]
1112"
SAFEGUARDS,0.9377162629757786,"Justification: The ultimate goal of our proposed method is not to propose any model,
1113"
SAFEGUARDS,0.9384083044982698,"but to propose a valuable theory of multi-modal learning. The training data used only
1114"
SAFEGUARDS,0.9391003460207612,"includes matching, classification and detection, and the data sets are all open source data
1115"
SAFEGUARDS,0.9397923875432526,"sets. Therefore, as far as this article is concerned, there is no risk of abuse.
1116"
SAFEGUARDS,0.9404844290657439,"Guidelines:
1117"
SAFEGUARDS,0.9411764705882353,"• The answer NA means that the paper poses no such risks.
1118"
SAFEGUARDS,0.9418685121107266,"• Released models that have a high risk for misuse or dual-use should be released with
1119"
SAFEGUARDS,0.942560553633218,"necessary safeguards to allow for controlled use of the model, for example by requiring
1120"
SAFEGUARDS,0.9432525951557094,"that users adhere to usage guidelines or restrictions to access the model or implementing
1121"
SAFEGUARDS,0.9439446366782007,"safety filters.
1122"
SAFEGUARDS,0.9446366782006921,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1123"
SAFEGUARDS,0.9453287197231834,"should describe how they avoided releasing unsafe images.
1124"
SAFEGUARDS,0.9460207612456747,"• We recognize that providing effective safeguards is challenging, and many papers do
1125"
SAFEGUARDS,0.946712802768166,"not require this, but we encourage authors to take this into account and make a best
1126"
SAFEGUARDS,0.9474048442906574,"faith effort.
1127"
LICENSES FOR EXISTING ASSETS,0.9480968858131488,"12. Licenses for existing assets
1128"
LICENSES FOR EXISTING ASSETS,0.9487889273356401,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1129"
LICENSES FOR EXISTING ASSETS,0.9494809688581315,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1130"
LICENSES FOR EXISTING ASSETS,0.9501730103806229,"properly respected?
1131"
LICENSES FOR EXISTING ASSETS,0.9508650519031142,"Answer: [Yes]
1132"
LICENSES FOR EXISTING ASSETS,0.9515570934256056,"Justification: We annotated any sources in detail.
1133"
LICENSES FOR EXISTING ASSETS,0.9522491349480969,"Guidelines:
1134"
LICENSES FOR EXISTING ASSETS,0.9529411764705882,"• The answer NA means that the paper does not use existing assets.
1135"
LICENSES FOR EXISTING ASSETS,0.9536332179930795,"• The authors should cite the original paper that produced the code package or dataset.
1136"
LICENSES FOR EXISTING ASSETS,0.9543252595155709,"• The authors should state which version of the asset is used and, if possible, include a
1137"
LICENSES FOR EXISTING ASSETS,0.9550173010380623,"URL.
1138"
LICENSES FOR EXISTING ASSETS,0.9557093425605536,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1139"
LICENSES FOR EXISTING ASSETS,0.956401384083045,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1140"
LICENSES FOR EXISTING ASSETS,0.9570934256055363,"service of that source should be provided.
1141"
LICENSES FOR EXISTING ASSETS,0.9577854671280277,"• If assets are released, the license, copyright information, and terms of use in the
1142"
LICENSES FOR EXISTING ASSETS,0.9584775086505191,"package should be provided. For popular datasets, paperswithcode.com/datasets
1143"
LICENSES FOR EXISTING ASSETS,0.9591695501730104,"has curated licenses for some datasets. Their licensing guide can help determine the
1144"
LICENSES FOR EXISTING ASSETS,0.9598615916955018,"license of a dataset.
1145"
LICENSES FOR EXISTING ASSETS,0.960553633217993,"• For existing datasets that are re-packaged, both the original license and the license of
1146"
LICENSES FOR EXISTING ASSETS,0.9612456747404844,"the derived asset (if it has changed) should be provided.
1147"
LICENSES FOR EXISTING ASSETS,0.9619377162629758,"• If this information is not available online, the authors are encouraged to reach out to
1148"
LICENSES FOR EXISTING ASSETS,0.9626297577854671,"the asset’s creators.
1149"
NEW ASSETS,0.9633217993079585,"13. New Assets
1150"
NEW ASSETS,0.9640138408304498,"Question: Are new assets introduced in the paper well documented and is the documentation
1151"
NEW ASSETS,0.9647058823529412,"provided alongside the assets?
1152"
NEW ASSETS,0.9653979238754326,"Answer: [No]
1153"
NEW ASSETS,0.9660899653979239,"Justification: For the code and resources involved in the full text, we only provide our
1154"
NEW ASSETS,0.9667820069204153,"original parts, such as our methods and our reproduced methods. For resources that already
1155"
NEW ASSETS,0.9674740484429065,"exist (e.g., the feature extractor code), the reader should follow the documentation. Since
1156"
NEW ASSETS,0.9681660899653979,"the concept we propose contains some conclusions that should be tried by the reader (such
1157"
NEW ASSETS,0.9688581314878892,"as the optimal dimension), the specific training procedure should also be designed by the
1158"
NEW ASSETS,0.9695501730103806,"reader.
1159"
NEW ASSETS,0.970242214532872,"Guidelines:
1160"
NEW ASSETS,0.9709342560553633,"• The answer NA means that the paper does not release new assets.
1161"
NEW ASSETS,0.9716262975778547,"• Researchers should communicate the details of the dataset/code/model as part of their
1162"
NEW ASSETS,0.972318339100346,"submissions via structured templates. This includes details about training, license,
1163"
NEW ASSETS,0.9730103806228374,"limitations, etc.
1164"
NEW ASSETS,0.9737024221453288,"• The paper should discuss whether and how consent was obtained from people whose
1165"
NEW ASSETS,0.9743944636678201,"asset is used.
1166"
NEW ASSETS,0.9750865051903114,"• At submission time, remember to anonymize your assets (if applicable). You can either
1167"
NEW ASSETS,0.9757785467128027,"create an anonymized URL or include an anonymized zip file.
1168"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764705882352941,"14. Crowdsourcing and Research with Human Subjects
1169"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771626297577855,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1170"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778546712802768,"include the full text of instructions given to participants and screenshots, if applicable, as
1171"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785467128027682,"well as details about compensation (if any)?
1172"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792387543252595,"Answer: [NA]
1173"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799307958477509,"Justification: Our work does not involve any human subjects.
1174"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806228373702423,"Guidelines:
1175"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813148788927336,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1176"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820069204152249,"human subjects.
1177"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826989619377162,"• Including this information in the supplemental material is fine, but if the main contribu-
1178"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833910034602076,"tion of the paper involves human subjects, then as much detail as possible should be
1179"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840830449826989,"included in the main paper.
1180"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847750865051903,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1181"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854671280276817,"or other labor should be paid at least the minimum wage in the country of the data
1182"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986159169550173,"collector.
1183"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868512110726644,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1184"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875432525951557,"Subjects
1185"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882352941176471,"Question: Does the paper describe potential risks incurred by study participants, whether
1186"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889273356401385,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1187"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896193771626297,"approvals (or an equivalent approval/review based on the requirements of your country or
1188"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903114186851211,"institution) were obtained?
1189"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910034602076124,"Answer: [NA]
1190"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916955017301038,"Justification: Our work does not involve any human subjects.
1191"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923875432525952,"Guidelines:
1192"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930795847750865,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1193"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937716262975779,"human subjects.
1194"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944636678200692,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1195"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951557093425606,"may be required for any human subjects research. If you obtained IRB approval, you
1196"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995847750865052,"should clearly state this in the paper.
1197"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965397923875432,"• We recognize that the procedures for this may vary significantly between institutions
1198"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972318339100346,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1199"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979238754325259,"guidelines for their institution.
1200"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986159169550173,"• For initial submissions, do not include any information that would break anonymity (if
1201"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993079584775086,"applicable), such as the institution conducting the review.
1202"
