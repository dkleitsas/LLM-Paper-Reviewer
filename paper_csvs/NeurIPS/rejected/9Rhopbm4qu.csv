Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016891891891891893,"Causal effect estimation from observational data is an important analytical ap-
1"
ABSTRACT,0.0033783783783783786,"proach for data-driven policy-making. However, due to the inherent lack of ground
2"
ABSTRACT,0.005067567567567568,"truth in causal inference accepting such recommendations requires transparency
3"
ABSTRACT,0.006756756756756757,"and explainability. To date, attempts at transparent causal effect estimation con-
4"
ABSTRACT,0.008445945945945946,"sist of applying post hoc explanation methods to black-box models, which are
5"
ABSTRACT,0.010135135135135136,"not interpretable. In this manuscript, we present BICauseTree: an interpretable
6"
ABSTRACT,0.011824324324324325,"balancing method that identiﬁes clusters where natural experiments occur locally.
7"
ABSTRACT,0.013513513513513514,"Our approach builds on decision trees to reduce treatment allocation bias. As a
8"
ABSTRACT,0.015202702702702704,"result, we can deﬁne subpopulations presenting positivity violations and exclude
9"
ABSTRACT,0.016891891891891893,"them while providing a covariate-based deﬁnition of the target population we can
10"
ABSTRACT,0.018581081081081082,"infer from. We characterize the method’s performance using synthetic and realistic
11"
ABSTRACT,0.02027027027027027,"datasets, explore its bias-interpretability tradeoff, and show that it is comparable
12"
ABSTRACT,0.02195945945945946,"with existing approaches.
13"
INTRODUCTION,0.02364864864864865,"1
Introduction
14"
INTRODUCTION,0.02533783783783784,"The primary task of causal inference is estimating the effect of a treatment or intervention. Evaluating
15"
INTRODUCTION,0.02702702702702703,"the strength of a causal relationship is essential for decision-making, designing interventions, as well
16"
INTRODUCTION,0.028716216216216218,"as evaluating the effect of a policy. As such, causal inference has high applicability across multiple
17"
INTRODUCTION,0.030405405405405407,"ﬁelds including medicine, social sciences and policy-making.
18"
INTRODUCTION,0.03209459459459459,"However, the estimation of a causal effect requires the computation of “potential outcomes” i.e.
19"
INTRODUCTION,0.033783783783783786,"the outcome an individual would experience if they had received some potential treatment, which
20"
INTRODUCTION,0.03547297297297297,"may differ to the observed one (1). When treatment is binary, the quantity of interest is often the
21"
INTRODUCTION,0.037162162162162164,"difference between the average potential outcomes in an all-treated scenario vs an all-untreated
22"
INTRODUCTION,0.03885135135135135,"scenario. Estimating and evaluating causal effect from observational data is thus challenging as we
23"
INTRODUCTION,0.04054054054054054,"only observe a single potential outcome–the one under the observed treatment–and can never observe
24"
INTRODUCTION,0.04222972972972973,"the counterfactual outcome, lacking ground-truth labels. Furthermore, when treatment assignment is
25"
INTRODUCTION,0.04391891891891892,"not randomized, groups that do or do not receive treatment may not be comparable in their attributes,
26"
INTRODUCTION,0.04560810810810811,"and such attributes can inﬂuence the outcome too (i.e. confounding bias).
27"
INTRODUCTION,0.0472972972972973,"In addition to these fundamental challenges, in practical settings where causality is used, decision-
28"
INTRODUCTION,0.048986486486486486,"making can often be safety-sensitive (e.g. healthcare, education). This, in turn, incentivizes “in-
29"
INTRODUCTION,0.05067567567567568,"terpretable” modeling to either comply with ethics requirements or be properly communicated to
30"
INTRODUCTION,0.052364864864864864,"interested parties. Here, interpretability means that each decision in the algorithm is inherently
31"
INTRODUCTION,0.05405405405405406,"explicit and traceable, contrasting with explainability where decisions are justiﬁed post-hoc using an
32"
INTRODUCTION,0.05574324324324324,"external model (2). Moreover, due to the lack of ground truth, interpretability is of greater importance
33"
INTRODUCTION,0.057432432432432436,"in causal inference where understanding a model may be the only way to question it.
34"
INTRODUCTION,0.05912162162162162,"In this paper, we introduce BICauseTree: Bias-balancing Interpretable Causal Tree, an interpretable
35"
INTRODUCTION,0.060810810810810814,"balancing method for observational data with binary treatment that can handle high-dimensional
36"
INTRODUCTION,0.0625,"datasets. We use a binary decision tree to stratify on imblanced covariates and identify subpopulations
37"
INTRODUCTION,0.06418918918918919,"with similar propensities to be treated, when they exist in the data. The resulting clusters act as local
38"
INTRODUCTION,0.06587837837837837,"naturally randomized experiments. This newly formed partition can be used for effect estimation,
39"
INTRODUCTION,0.06756756756756757,"as well as propensity score or outcome estimation. Our method can further identify positivity
40"
INTRODUCTION,0.06925675675675676,"violating regions of the data space, i.e., subsets of the population where treatment allocation is
41"
INTRODUCTION,0.07094594594594594,"highly unbalanced. By doing so, we generate a transparent, covariate-based deﬁnition of the target
42"
INTRODUCTION,0.07263513513513513,"population or “inferentiable” population i.e. the population with sufﬁcient overlap for inferring causal
43"
INTRODUCTION,0.07432432432432433,"effect.
44"
INTRODUCTION,0.07601351351351351,"Our contributions are as follows:
45"
INTRODUCTION,0.0777027027027027,"1. Our BICauseTree method can identify “natural experiments” i.e. subgroups with lower treatment
46"
INTRODUCTION,0.07939189189189189,"imbalance, when they exist.
47"
BICAUSETREE COMPARES WITH EXISTING METHODS FOR CAUSAL EFFECT ESTIMATION IN TERMS OF BIAS,0.08108108108108109,"2. BICauseTree compares with existing methods for causal effect estimation in terms of bias
48"
BICAUSETREE COMPARES WITH EXISTING METHODS FOR CAUSAL EFFECT ESTIMATION IN TERMS OF BIAS,0.08277027027027027,"while maintaining interpretability. Estimation error and consistency of the clusters show good
49"
BICAUSETREE COMPARES WITH EXISTING METHODS FOR CAUSAL EFFECT ESTIMATION IN TERMS OF BIAS,0.08445945945945946,"robustness to subsampling in both our synthetic and realistic benchmark datasets.
50"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.08614864864864864,"3. Our method provides users with built-in prediction abstention mechanism for covariate spaces
51"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.08783783783783784,"lacking common support. We show the value of deﬁning the inferentiable population using a
52"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.08952702702702703,"clinical example with matched twins data.
53"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.09121621621621621,"4. The resulting tree can further be used for propensity or outcome estimation.
54"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.0929054054054054,"5. We release open-source code with detailed documentation for implementation of our method,
55"
OUR METHOD PROVIDES USERS WITH BUILT-IN PREDICTION ABSTENTION MECHANISM FOR COVARIATE SPACES,0.0945945945945946,"and reproducibility of our results.
56"
RELATED WORK,0.09628378378378379,"2
Related work
57"
EFFECT ESTIMATION METHODS,0.09797297297297297,"2.1
Effect estimation methods
58"
EFFECT ESTIMATION METHODS,0.09966216216216216,"Causal inference provides a wide range of methods for effect estimation from data with unbalanced
59"
EFFECT ESTIMATION METHODS,0.10135135135135136,"treatment allocation. There are two modelling strategies: modelling the treatment using the covariates
60"
EFFECT ESTIMATION METHODS,0.10304054054054054,"to balance the groups, and modelling the outcome directly using the covariates and treatment
61"
EFFECT ESTIMATION METHODS,0.10472972972972973,"assignment.
62"
EFFECT ESTIMATION METHODS,0.10641891891891891,"In balancing methods such as matching or weighting methods, the data is pre-processed to create
63"
EFFECT ESTIMATION METHODS,0.10810810810810811,"subgroups with lower treatment imbalance or “natural experiments”. Matching methods consist of
64"
EFFECT ESTIMATION METHODS,0.1097972972972973,"clustering similar units, based on some distance metric, from the treatment and control groups to
65"
EFFECT ESTIMATION METHODS,0.11148648648648649,"reduce imbalance. Euclidean and Mahalanobis distances are commonly used, together with nearest
66"
EFFECT ESTIMATION METHODS,0.11317567567567567,"neighbour search. However, as the notion of distance becomes problematic in high dimensional spaces,
67"
EFFECT ESTIMATION METHODS,0.11486486486486487,"covariate-based matching tends to become ineffective in such settings (3). Weighting methods aim at
68"
EFFECT ESTIMATION METHODS,0.11655405405405406,"balancing the covariate distribution across treatment groups, with Inverse Probability Weighting (IPW)
69"
EFFECT ESTIMATION METHODS,0.11824324324324324,"(4) being the most popular approach. Samples weights are the inverse of the estimated propensity
70"
EFFECT ESTIMATION METHODS,0.11993243243243243,"scores, i.e. the probability of a unit to be assigned to its observed group. However, extreme IPW
71"
EFFECT ESTIMATION METHODS,0.12162162162162163,"weights can also increase the estimation variance.
72"
EFFECT ESTIMATION METHODS,0.12331081081081081,"Contrastingly, in adjustment methods the causal effect is estimated from regression outcome models
73"
EFFECT ESTIMATION METHODS,0.125,"where both treatment and covariates act as predictors of the outcome. These regressions can be ﬁtted
74"
EFFECT ESTIMATION METHODS,0.1266891891891892,"through various methods like linear regression (5), neural networks (6; 7), or tree-based models (8; 9).
75"
EFFECT ESTIMATION METHODS,0.12837837837837837,"Under this taxonomy, BICauseTree is a balancing method, i.e., a data-driven mechanism for achieving
76"
EFFECT ESTIMATION METHODS,0.13006756756756757,"conditional exchangeability. Nonetheless, BICauseTree can be combined with other methods to
77"
EFFECT ESTIMATION METHODS,0.13175675675675674,"achieve superior results. Either as propensity models in established doubly robust methods (10), or
78"
EFFECT ESTIMATION METHODS,0.13344594594594594,"by incorporating arbitrary causal models at leaf nodes (similar to regression trees with linear models
79"
EFFECT ESTIMATION METHODS,0.13513513513513514,"at leaf nodes (11)).
80"
POSITIVITY VIOLATIONS,0.13682432432432431,"2.2
Positivity violations
81"
POSITIVITY VIOLATIONS,0.13851351351351351,"Causal inference is only possible under the positivity assumption, which requires covariate dis-
82"
POSITIVITY VIOLATIONS,0.14020270270270271,"tributions to overlap between treatment arms. Thus, positivity violations (also referred to as no
83"
POSITIVITY VIOLATIONS,0.14189189189189189,"overlap) occur when certain subgroups in a sample do not receive one of the treatments of interest
84"
POSITIVITY VIOLATIONS,0.14358108108108109,"or receive it too rarely (12). Overlap is essential as it guarantees data-driven outcome extrapolation
85"
POSITIVITY VIOLATIONS,0.14527027027027026,"across treatment groups. Having no common support means there are subjects in one group with
86"
POSITIVITY VIOLATIONS,0.14695945945945946,"no counterparts from the other group, and, therefore, no reliable way to pool information on their
87"
POSITIVITY VIOLATIONS,0.14864864864864866,"outcome had they been in the other group. Non-violating samples are thus the only ones for which
88"
POSITIVITY VIOLATIONS,0.15033783783783783,"we can guarantee some validity of the inferred causal effect.
89"
POSITIVITY VIOLATIONS,0.15202702702702703,"There are three common ways to characterize positivity. The most common one consists in estimating
90"
POSITIVITY VIOLATIONS,0.15371621621621623,"propensity scores and excluding the samples associated with extreme values (also known as “trim-
91"
POSITIVITY VIOLATIONS,0.1554054054054054,"ming”) (13). The threshold for propensity scores can be set arbitrarily or dynamically (14). However,
92"
POSITIVITY VIOLATIONS,0.1570945945945946,"since samples are excluded on the basis of their propensity scores and not their covariate values, these
93"
POSITIVITY VIOLATIONS,0.15878378378378377,"methods lack interpretabilty about the excluded subjects and how it may affect the target population
94"
POSITIVITY VIOLATIONS,0.16047297297297297,"on which we can generalize the inference. Consequently, other methods have been developed to
95"
POSITIVITY VIOLATIONS,0.16216216216216217,"overcome this challenge by characterizing the propensity-based exclusion (15; 16; 17). Lastly, the
96"
POSITIVITY VIOLATIONS,0.16385135135135134,"third way tries to characterize the overlap from covariates and treatment assignment directly, without
97"
POSITIVITY VIOLATIONS,0.16554054054054054,"going through the intermediate propensity score e.g. PositiviTree (12). In PositiviTree, a decision
98"
POSITIVITY VIOLATIONS,0.16722972972972974,"tree classiﬁer is ﬁtted to predict treatment allocation. In contrast to their approach, BICause Tree
99"
POSITIVITY VIOLATIONS,0.16891891891891891,"implements a tailor-made optimization function where splits are chosen to maximize balancing
100"
POSITIVITY VIOLATIONS,0.17060810810810811,"in the resulting sub-population, whereas PositiviTree uses off-the-shelf decision trees maximizing
101"
POSITIVITY VIOLATIONS,0.17229729729729729,"separation. Ultimately, the above mentioned methods for positivity identiﬁcation and characterization
102"
POSITIVITY VIOLATIONS,0.17398648648648649,"are model agnostic. In our model, BICauseTree, positivity identiﬁcation and characterization are
103"
POSITIVITY VIOLATIONS,0.17567567567567569,"inherently integrated in the model, and effect estimation comes with a built-in interpretable abstention
104"
POSITIVITY VIOLATIONS,0.17736486486486486,"prediction mechanism.
105"
INTERPRETABILITY AND CAUSAL INFERENCE,0.17905405405405406,"2.3
Interpretability and causal inference
106"
INTERPRETABILITY AND CAUSAL INFERENCE,0.18074324324324326,"A predominant issue in existing effect estimation methods is their lack of interpretability. A model is
107"
INTERPRETABILITY AND CAUSAL INFERENCE,0.18243243243243243,"considered as interpretable if its decisions are inherently transparent (2). Examples of interpretable
108"
INTERPRETABILITY AND CAUSAL INFERENCE,0.18412162162162163,"models include decision trees where the decision can be recovered as a simple logical conjunction.
109"
INTERPRETABILITY AND CAUSAL INFERENCE,0.1858108108108108,"Contrastingly, a model is said to be explainable when its predictions can be justiﬁed a-posteriori by
110"
INTERPRETABILITY AND CAUSAL INFERENCE,0.1875,"examining the black-box using an additional “explanation model”. Popular post-hoc explanation
111"
INTERPRETABILITY AND CAUSAL INFERENCE,0.1891891891891892,"models include Shapley values (18) or LIME (19). However, previous works have shown that existing
112"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19087837837837837,"explainability techniques lack robustness and stability (20). Further, the explanations provided by
113"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19256756756756757,"explanation models inevitably depend on the black-box model’s speciﬁcation and ﬁtness. Given that
114"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19425675675675674,"explanation models only provide unreliable justiﬁcations for black-box model decisions, a growing
115"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19594594594594594,"number of practitioners have been advocating for intrinsically interpretable predictive models (2).
116"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19763513513513514,"We further claim that causal inference, and in particular effect estimation, should be interpretable as
117"
INTERPRETABILITY AND CAUSAL INFERENCE,0.19932432432432431,"it assists high-stake decisions affecting laypeople.
118"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20101351351351351,"Causal Trees (8) are another tree-based model for causal inference that (i) leverages the inherent
119"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20270270270270271,"interpretability of decision trees, and (ii) has a custom objective function for recursively splitting
120"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20439189189189189,"the data. Although both utilize decision trees, BICauseTree and Causal Tree (CT) serve distinct
121"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20608108108108109,"purposes. BICauseTree splits are optimized for balancing treatment allocation while CT splits are
122"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20777027027027026,"optimized for balancing treatment effect, under assumed exchangeability. In other words, CT assumes
123"
INTERPRETABILITY AND CAUSAL INFERENCE,0.20945945945945946,"exchangeability while BICauseTree “ﬁnds” exchangeability. As such, our approach is more suited
124"
INTERPRETABILITY AND CAUSAL INFERENCE,0.21114864864864866,"for ATE estimation while CT is better suited for Conditional Average Treatment Effect estimation (8).
125"
INTERPRETABILITY AND CAUSAL INFERENCE,0.21283783783783783,"Furthermore, in practice, causal effects are often averaged over multiple trees into a so-called Causal
126"
INTERPRETABILITY AND CAUSAL INFERENCE,0.21452702702702703,"Forest (21; 22) that is no longer interpretable, and users are encouraged to use post-hoc explanation
127"
INTERPRETABILITY AND CAUSAL INFERENCE,0.21621621621621623,"methods (23).
128"
INTERPRETABILITY AND CAUSAL INFERENCE,0.2179054054054054,"In addition to effect estimation, positivity violations characterization should also be interpretable for
129"
INTERPRETABILITY AND CAUSAL INFERENCE,0.2195945945945946,"downstream users, such as policy makers. Discarding samples can hurt the external validity of any
130"
INTERPRETABILITY AND CAUSAL INFERENCE,0.22128378378378377,"result, as there can be structural biases leading to entire subpopulation being excluded. Therefore,
131"
INTERPRETABILITY AND CAUSAL INFERENCE,0.22297297297297297,"interpretable characterization of the overlap in a study can help policy makers better assess on
132"
INTERPRETABILITY AND CAUSAL INFERENCE,0.22466216216216217,"whom they expect the study results to apply (15; 12). In our model, BICauseTree, we generate a
133"
INTERPRETABILITY AND CAUSAL INFERENCE,0.22635135135135134,"covariate-based deﬁnition of the violating subpopulation. In other words, we can claim which target
134"
INTERPRETABILITY AND CAUSAL INFERENCE,0.22804054054054054,"population our estimate of the Average Treatment Effect applies to.
135"
BICAUSETREE,0.22972972972972974,"3
BICauseTree
136"
PROBLEM SETTING,0.23141891891891891,"3.1
Problem setting
137"
PROBLEM SETTING,0.23310810810810811,"We consider a dataset of size n where we note each individual sample (Xi, Ti, Yi) with Xi ∈Rd is
138"
PROBLEM SETTING,0.23479729729729729,"a covariate vector for sample i measured prior to treatment allocation, and Ti is a binary variable
139"
PROBLEM SETTING,0.23648648648648649,"denoting treatment allocation. In the potential outcomes framework (24), Yi(1) is the outcome
140"
PROBLEM SETTING,0.23817567567567569,"under Ti = 1, and Yi(0) is the analogous outcome under Ti = 0. Then, assuming the consistency
141"
PROBLEM SETTING,0.23986486486486486,"assumption, the observed outcome is deﬁned as Yi = TiYi(1) + (1 −Ti) Yi(0). In this paper, we
142"
PROBLEM SETTING,0.24155405405405406,"focus on estimating the average treatment effect (ATE), deﬁned as: ATE = E[Y (1) −Y (0)].
143"
MOTIVATION,0.24324324324324326,"3.2
Motivation
144"
MOTIVATION,0.24493243243243243,"We introduce a method for balancing observational datasets with the goal of estimating causal effect
145"
MOTIVATION,0.24662162162162163,"in a subpopulation with sufﬁcient overlap. Our goals are: (i) unbiased estimation of causal effect, (ii)
146"
MOTIVATION,0.2483108108108108,"interpretability of both the balancing and positivity violation identiﬁcation procedures, (iii) ability to
147"
MOTIVATION,0.25,"handle high-dimensional datasets. Our approach utilizes the Absolute Standardized Mean Difference
148"
MOTIVATION,0.2516891891891892,"(ASMD) (25) frequently used for assessing potential confounding bias in observational data. Note
149"
MOTIVATION,0.2533783783783784,"that our balancing procedure is entirely interpretable, although it can be used in combination with
150"
MOTIVATION,0.25506756756756754,"arbitrary black-box outcome models or propensity score models. Finally, our method generates a
151"
MOTIVATION,0.25675675675675674,"covariate-based deﬁnition of the target population on which we make inference. As such, it is tailored
152"
MOTIVATION,0.25844594594594594,"to sensitive domains where inference should be restricted to subpopulations with reasonable overlap.
153"
ALGORITHM,0.26013513513513514,"3.3
Algorithm
154"
ALGORITHM,0.26182432432432434,"The intuition for our algorithm is that, by partitioning the population to maximize treatment allocation
155"
ALGORITHM,0.2635135135135135,"heterogeneity, we may be able to ﬁnd subpopulations that are natural experiments. We recursively
156"
ALGORITHM,0.2652027027027027,"partition the data according to the most imbalanced covariate between treatment groups. Using
157"
ALGORITHM,0.2668918918918919,"decision trees makes our approach transparent and non-parametric.
158"
ALGORITHM,0.2685810810810811,"Splitting criterion The ﬁrst step of our algorithm is to split the data until some stopping criterion is
159"
ALGORITHM,0.2702702702702703,"met. The tree recursively splits on the covariate that maximize treatment allocation heterogeneity. To
160"
ALGORITHM,0.2719594594594595,"do so, we compute the Absolute Standardized Mean Difference (ASMD) for all covariates and select
161"
ALGORITHM,0.27364864864864863,"the covariate with the highest absolute value. The ASMD for a variable Xj is deﬁned as:
162"
ALGORITHM,0.27533783783783783,"ASMDj =
|E[Xj|T =1]−E[Xj|T =0]|
√"
ALGORITHM,0.27702702702702703,"V ar([Xj|T =1])+V ar([Xj|T =0])
163"
ALGORITHM,0.27871621621621623,"The reason for choosing the feature with the highest ASMD is that it is most likely to be a confounder.
164"
ALGORITHM,0.28040540540540543,"Once that next splitting covariate jmax is chosen, we want to ﬁnd a split that is most associated
165"
ALGORITHM,0.28209459459459457,"with treatment assignment, so that we may control for the effect of counfounding. The tree ﬁnds the
166"
ALGORITHM,0.28378378378378377,"optimal splitting value by iterating over covariate values xjmax and taking the value associated with
167"
ALGORITHM,0.28547297297297297,"the lowest p-value according to a Fisher’s exact test or a χ2 test, depending on the sample size.
168"
ALGORITHM,0.28716216216216217,"Stopping criterion The tree building phase stops when either: (i) the maximum ASMD is below
169"
ALGORITHM,0.28885135135135137,"some threshold, (ii) the minimum treatment group size falls below some threshold (iii) the total
170"
ALGORITHM,0.2905405405405405,"population fall below the minimum population size threshold, or (iv) a maximum tree depth is reached.
171"
ALGORITHM,0.2922297297297297,"All of the thresholds are user-deﬁned hyperparameters.
172"
ALGORITHM,0.2939189189189189,"Pruning procedure Once the stopping criterion is met in all leaf nodes, the tree is pruned. A multiple
173"
ALGORITHM,0.2956081081081081,"hypothesis test correction is ﬁrst applied on the p-values of all splits. Following this, the splits with
174"
ALGORITHM,0.2972972972972973,"signiﬁcant p-values or with at least one split with signiﬁcant p-value amongst their descendants are
175"
ALGORITHM,0.2989864864864865,"kept. Ultimately, given that ASMD reduction may not be monotonic, pruning an initially deeper tree
176"
ALGORITHM,0.30067567567567566,"allows us to check if partitioning more renders unbiased subpopulations. The implementation of the
177"
ALGORITHM,0.30236486486486486,"tree allows for user-deﬁned multiple hypothesis test correction, with current experiments using Holm
178"
ALGORITHM,0.30405405405405406,"correction (26). The choice of the pruning and stopping criterion hyperparameters will guide the
179"
ALGORITHM,0.30574324324324326,"bias/variance trade-off of the tree. Deeper trees may have more power to detect treatment effect while
180"
ALGORITHM,0.30743243243243246,"shallower trees will be more likely to have biased effect estimation.
181"
ALGORITHM,0.3091216216216216,"Positivity violation ﬁltering The ﬁnal step evaluates the overlap in the resulting set of leaf nodes
182"
ALGORITHM,0.3108108108108108,"to identify those where inference is possible. The tree checks for treatment balance based on some
183"
ALGORITHM,0.3125,"user-deﬁned overlap estimation method, with the default method being the Crump procedure (14).
184"
ALGORITHM,0.3141891891891892,"The positivity violating leaf nodes are tagged and then used for inference abstention mechanism, i.e.
185"
ALGORITHM,0.3158783783783784,"inference will be restricted to non-violating leaves.
186"
ALGORITHM,0.31756756756756754,"Estimation Once a tree is contracted, it can be used to estimate both counterfactual outcomes
187"
ALGORITHM,0.31925675675675674,"and propensity scores. For each leaf, using the units propagated to that leaf, we can model the
188"
ALGORITHM,0.32094594594594594,"counterfactual outcome by taking the average outcome of those units in both treatment groups.
189"
ALGORITHM,0.32263513513513514,"Alternatively, we can ﬁt any arbitrary causal model (e.g., IPW or an outcome regression) to obtain the
190"
ALGORITHM,0.32432432432432434,"average counterfactual outcomes in that leaf. The ATE is then obtained by averaging the estimation
191"
ALGORITHM,0.3260135135135135,"across leaves. Similarly, we can estimate the propensity score in each leaf by taking the treatment
192"
ALGORITHM,0.3277027027027027,"prevalence or using any other statistical estimator (e.g., logistic regression).
193"
ALGORITHM,0.3293918918918919,"Code and implementation details Code for BICauseTree is released open-source, including de-
194"
ALGORITHM,0.3310810810810811,"tailed documentation under: https://anonymous.4open.science/r/BICause-Trees-F259.
195"
ALGORITHM,0.3327702702702703,"Our ﬂexible implementation allows the user to extend the default stopping criterion as well as the
196"
ALGORITHM,0.3344594594594595,"multiple hypothesis correction method. BICauseTree adheres to causallib’s API, and can accept
197"
ALGORITHM,0.33614864864864863,"various outcome and propensity models.
198"
ALGORITHM,0.33783783783783783,Algorithm 1 BICauseTree
ALGORITHM,0.33952702702702703,"Inputs: root node N0, X, T, Y
Call Build subtree(N0, X, T, Y )
Do multiple hypothesis test correction on all split p-values
Pruning procedure: keep splits with either (i) a signiﬁcant p-value or (ii) at least one descendant
with a signiﬁcant p-value
Mark leaf nodes that violate positivity violation criterion"
ALGORITHM,0.34121621621621623,Algorithm 2 Build subtree
ALGORITHM,0.34290540540540543,"Inputs: current node N, X, T, Y
if Stopping criteria not met then"
ALGORITHM,0.34459459459459457,"Find and record in N the covariate with maximum ASMD: maxASMD := maxi(ASMDi)
Find and record in N the split value with the lowest p-value according to a Fisher test/χ2 test
Record the p-value for this split in N
Split the data X, T, Y into Xleft, Tleft, Yleft and Xright, Tright, Yright according to N’s
splitting covariate and value"
ALGORITHM,0.34628378378378377,"Add two child nodes to N: Nleft and Nright
Call Build subtree(Nleft, Xleft, Tleft, Yleft)
Call Build subtree(Nright, Xright, Tright, Yright)
end if"
EXPERIMENT AND RESULTS,0.34797297297297297,"4
Experiment and results
199"
EXPERIMENTAL SETTINGS,0.34966216216216217,"4.1
Experimental settings
200"
EXPERIMENTAL SETTINGS,0.35135135135135137,"In all experiments–unless stated otherwise–the data was split into a training and testing set with a
201"
EXPERIMENTAL SETTINGS,0.3530405405405405,"50/50 ratio. The training set was used for the construction of the tree and for ﬁtting the outcome
202"
EXPERIMENTAL SETTINGS,0.3547297297297297,"models in leaf nodes, if relevant. Causal effects are estimated by taking a weighted average of the
203"
EXPERIMENTAL SETTINGS,0.3564189189189189,"local treatment effects in each subpopulation. At the testing phase, the data is propagated through
204"
EXPERIMENTAL SETTINGS,0.3581081081081081,"the tree, and potential outcomes are evaluated using the previously ﬁtted leaf outcome model. We
205"
EXPERIMENTAL SETTINGS,0.3597972972972973,"performed 50 random train-test splits, which we will refer to as subsamples to avoid confusion with
206"
EXPERIMENTAL SETTINGS,0.3614864864864865,"the tree partitions. For each subsample, effects are only computed on the non-violating samples
207"
EXPERIMENTAL SETTINGS,0.36317567567567566,"of the population. In order to maintain a fair comparison, these samples are also excluded from
208"
EXPERIMENTAL SETTINGS,0.36486486486486486,"effect estimation with other models and with ground truth. All results are shown after ﬁltering
209"
EXPERIMENTAL SETTINGS,0.36655405405405406,"positivity-violating samples.
210"
EXPERIMENTAL SETTINGS,0.36824324324324326,"Baseline comparisons We compare our method to doubleMahalanobis Matching, Inverse Probability
211"
EXPERIMENTAL SETTINGS,0.36993243243243246,"Weighting (IPW), and Causal Tree (CT). In Mahalanobis Matching (27; 28), the nearest neighbor
212"
EXPERIMENTAL SETTINGS,0.3716216216216216,"search operates on the Mahalanobis distance: d (Xi, Xj) = (Xi −Xj)T Σ−1 (Xi −Xj), where
213"
EXPERIMENTAL SETTINGS,0.3733108108108108,"Σ is alternatively the estimated covariance matrix of the control and treatment group dataset. In
214"
EXPERIMENTAL SETTINGS,0.375,"Inverse Probability Weighting (4), a propensity score model estimates the individual probability
215"
EXPERIMENTAL SETTINGS,0.3766891891891892,"of treatment conditional on the covariates. The data is then weighted by the inverse propensities
216"
EXPERIMENTAL SETTINGS,0.3783783783783784,"P(T = ti | X = xi)−1 to generate a balanced pseudo-population. In Causal Tree (8), the splitting
217"
EXPERIMENTAL SETTINGS,0.38006756756756754,"criterion optimizes for treatment effect heterogeneity (see section 3.1 for further details). We use a
218"
EXPERIMENTAL SETTINGS,0.38175675675675674,"Causal Tree and not a Causal Forest to compare to an estimator which is equally interpretable as
219"
EXPERIMENTAL SETTINGS,0.38344594594594594,"our estimator. We also compare our results to an unadjusted marginal outcome estimator, which
220"
EXPERIMENTAL SETTINGS,0.38513513513513514,"will act as our “dummy” baseline model. As using a single Causal Tree for our interpretability goal
221"
EXPERIMENTAL SETTINGS,0.38682432432432434,"gives rise to high estimation bias, Causal Tree was excluded from the main manuscript for scaling
222"
EXPERIMENTAL SETTINGS,0.3885135135135135,"purposes. We refer the reader to sections A.5, A.6 and A.7 for a comparison with CT. For synthetic
223"
EXPERIMENTAL SETTINGS,0.3902027027027027,"experiments, we use the simplest version of our tree which we term BICauseTree(Marginal) where
224"
EXPERIMENTAL SETTINGS,0.3918918918918919,"the effect is estimated from taking average outcomes in leaf nodes. For real-world experiments,
225"
EXPERIMENTAL SETTINGS,0.3935810810810811,"we compare BICauseTree(Marginal) with BICauseTree(IPW), an augmented version in which an
226"
EXPERIMENTAL SETTINGS,0.3952702702702703,"IPW model is ﬁtted in each leaf node. To compare estimation methods, we compute the difference
227"
EXPERIMENTAL SETTINGS,0.3969594594594595,"between the estimated ATE and the true ATE for each subsample (or train-test partition) and display
228"
EXPERIMENTAL SETTINGS,0.39864864864864863,"the resulting distribution of estimation biases in a box plot. Further experimental details, including
229"
EXPERIMENTAL SETTINGS,0.40033783783783783,"hyperparameters, can be found in the Appendix under section A.9.
230"
SYNTHETIC DATASETS,0.40202702702702703,"4.2
Synthetic datasets
231"
SYNTHETIC DATASETS,0.40371621621621623,"We ﬁrst evaluate the performance of our approach on two synthetic datasets. We ﬁrst demonstrate
232"
SYNTHETIC DATASETS,0.40540540540540543,"BICauseTree’s ability to identify subgroups with lower treatment imbalance on a dataset which we
233"
SYNTHETIC DATASETS,0.40709459459459457,"will refer to as the “natural experiment dataset” in the following. We further exemplify BICauseTree’s
234"
SYNTHETIC DATASETS,0.40878378378378377,"identiﬁcation of positivity violating samples on a dataset we refer to as the “positivity violations
235"
SYNTHETIC DATASETS,0.41047297297297297,"dataset”. Due to the interaction-based nature of the data generation procedure, we additionally
236"
SYNTHETIC DATASETS,0.41216216216216217,"compare our approach to an IPW estimator with a Gradient Boosting classiﬁer propensity model,
237"
SYNTHETIC DATASETS,0.41385135135135137,"referred to as IPW (GBT) in both synthetic experiments. This choice ensures a fair comparison across
238"
SYNTHETIC DATASETS,0.4155405405405405,"estimators.
239"
SYNTHETIC DATASETS,0.4172297297297297,"Identifying natural experiments For the natural experiment dataset, we considered a Death out-
240"
SYNTHETIC DATASETS,0.4189189189189189,"come D, a binary treatment of interest T and two covariates: Sex S and Age A. We deﬁned four
241"
SYNTHETIC DATASETS,0.4206081081081081,"sub-populations, where each constituted a natural experiment with a truncated normal propensity
242"
SYNTHETIC DATASETS,0.4222972972972973,"distribution centered around a pre-deﬁned constant value and variance (see details in Section A.4.1).
243"
SYNTHETIC DATASETS,0.4239864864864865,"Then, individual treatment propensities were sampled from the corresponding distribution and ob-
244"
SYNTHETIC DATASETS,0.42567567567567566,"served treatment values were sampled from a Bernoulli distribution parameterized with the individual
245"
SYNTHETIC DATASETS,0.42736486486486486,"propensities. No positivity violation was modeled in this experiment. Ultimately, X = (S, A) is
246"
SYNTHETIC DATASETS,0.42905405405405406,"the vector of covariate values in R2 with the sample size chosen as n = 20, 000. The marginal
247"
SYNTHETIC DATASETS,0.43074324324324326,"distribution of covariates follows: S ∼Ber(0.5) and A ∼N
 
µ, σ2
where µ = 50 and σ = 20.
248"
SYNTHETIC DATASETS,0.43243243243243246,"Figure A1 in A.5.1 shows the partition obtained from training BICauseTree on the entire dataset. Our
249"
SYNTHETIC DATASETS,0.4341216216216216,"tree successfully identiﬁes the subpopulations in which a natural experiment was simulated. Figure
250"
SYNTHETIC DATASETS,0.4358108108108108,"1a shows the estimation bias across subsamples. In addition to being transparent, BICauseTree has
251"
SYNTHETIC DATASETS,0.4375,"lower bias in causal effect estimation compared to all other methods, excluding IPW(GBT) which has
252"
SYNTHETIC DATASETS,0.4391891891891892,"comparable performance. Despite its higher estimation variance, Matching has low bias, probably
253"
SYNTHETIC DATASETS,0.4408783783783784,"due to covariate space being well-posed and low-dimensional. Contrastingly, the logistic regression
254"
SYNTHETIC DATASETS,0.44256756756756754,"in IPW(LR) is not able to model treatment allocation as the true propensities are generated from a
255"
SYNTHETIC DATASETS,0.44425675675675674,"noisy piecewise constant function of the covariates resulting in a threshold effect that explains its poor
256"
SYNTHETIC DATASETS,0.44594594594594594,"performance. The non-parametric, local nature of both Matching and BICauseTree thus contrasts
257"
SYNTHETIC DATASETS,0.44763513513513514,"with the parametric estimation by IPW(LR). Further results on the BICauseTree’s calibration and
258"
SYNTHETIC DATASETS,0.44932432432432434,"covariate partition can be found in the Appendix, under section A.5.1.
259"
SYNTHETIC DATASETS,0.4510135135135135,"0.00
0.02
0.04
0.06
0.08"
SYNTHETIC DATASETS,0.4527027027027027,"|ATE
ATE|"
SYNTHETIC DATASETS,0.4543918918918919,BICause Tree
SYNTHETIC DATASETS,0.4560810810810811,Marginal
SYNTHETIC DATASETS,0.4577702702702703,IPW (LR)
SYNTHETIC DATASETS,0.4594594594594595,IPW (GBT)
SYNTHETIC DATASETS,0.46114864864864863,Marginal
SYNTHETIC DATASETS,0.46283783783783783,Matching Split
SYNTHETIC DATASETS,0.46452702702702703,"Test
Train"
SYNTHETIC DATASETS,0.46621621621621623,"(a) Estimation bias for the natural experiment
dataset (see subsection 4.2) across 50 subsamples,
with N = 20, 000"
SYNTHETIC DATASETS,0.46790540540540543,"0.0
0.2
0.4
0.6"
SYNTHETIC DATASETS,0.46959459459459457,"|ATE
ATE|"
SYNTHETIC DATASETS,0.47128378378378377,BICause Tree
SYNTHETIC DATASETS,0.47297297297297297,Marginal
SYNTHETIC DATASETS,0.47466216216216217,IPW (LR)
SYNTHETIC DATASETS,0.47635135135135137,IPW (GBT)
SYNTHETIC DATASETS,0.4780405405405405,Marginal
SYNTHETIC DATASETS,0.4797297297297297,Matching
SYNTHETIC DATASETS,0.4814189189189189,"0.00
0.02
0.04
0.06 Split"
SYNTHETIC DATASETS,0.4831081081081081,"Test
Train"
SYNTHETIC DATASETS,0.4847972972972973,"(b) Estimation bias for the positivity violations
dataset (see subsection 4.2) across 50 subsamples,
after excluding positivity violating leaf nodes with
N = 20, 000.
Figure 1: Results on the synthetic datasets"
SYNTHETIC DATASETS,0.4864864864864865,"Identifying positivity violations For the positivity violations dataset, we consider a synthetic dataset
260"
SYNTHETIC DATASETS,0.48817567567567566,"with a Death outcome D, a binary treatment of interest T, and three Bernoulli covariates –Sex S,
261"
SYNTHETIC DATASETS,0.48986486486486486,"cancer C and arrhythmia A– such that X = (S, C, A) (see Section A.4.2 for further details). As
262"
SYNTHETIC DATASETS,0.49155405405405406,"for the natural experiment dataset, we modeled treatment allocation with stochasticity by sampling
263"
SYNTHETIC DATASETS,0.49324324324324326,"propensities from a truncated gaussian distribution ﬁrst. Treatment allocation was simulated to ensure
264"
SYNTHETIC DATASETS,0.49493243243243246,"that overlap is very limited in two subpopulations: females with no cancer and no arrhythmia are
265"
SYNTHETIC DATASETS,0.4966216216216216,"rarely treated, while males with cancer and arrhythmia are almost always treated. Figure A2 in
266"
SYNTHETIC DATASETS,0.4983108108108108,"A.5.2 shows the partition obtained from training BICauseTree on the entire dataset, conﬁrming that
267"
SYNTHETIC DATASETS,0.5,"BICauseTree excludes the subgroups where positivity violations were modeled. On average, 67.1%
268"
SYNTHETIC DATASETS,0.5016891891891891,"of the cohort remained after positivity ﬁltering with very little variability across subsamples. Thanks
269"
SYNTHETIC DATASETS,0.5033783783783784,"to the interpretable nature of our method, we are able to identify these subgroups as a region of
270"
SYNTHETIC DATASETS,0.5050675675675675,"the covariate space. As seen in Figure 1b, after ﬁltering violating samples the effect estimation by
271"
SYNTHETIC DATASETS,0.5067567567567568,"BICauseTree remains unbiased and with low variance. Our estimator compares with IPW(GBT)
272"
SYNTHETIC DATASETS,0.5084459459459459,"while being interpretable. The IPW(LR) estimator is more biased than BICauseTree. This may be
273"
SYNTHETIC DATASETS,0.5101351351351351,"due to the extreme weights in the initial overall cohort. In spite of ﬁltering samples from regions
274"
SYNTHETIC DATASETS,0.5118243243243243,"with lack of overlap–as deﬁned by BICauseTree–the remaining propensity weights may be biased,
275"
SYNTHETIC DATASETS,0.5135135135135135,"which would ultimately induce a biased effect estimation. Estimation variance is comparable across
276"
SYNTHETIC DATASETS,0.5152027027027027,"methods, except for Matching which is both more biased and has higher variance than all other
277"
SYNTHETIC DATASETS,0.5168918918918919,"estimators. Further results on the BICauseTree’s calibration and covariate partition can be found in
278"
SYNTHETIC DATASETS,0.518581081081081,"the Appendix, under section A.5.2.
279"
REALISTIC DATASETS,0.5202702702702703,"4.3
Realistic datasets
280"
REALISTIC DATASETS,0.5219594594594594,"Causal benchmark datasets
We use two causal benchmark datasets to show the value of our
281"
REALISTIC DATASETS,0.5236486486486487,"approach. The twins dataset illustrates the high applicability of our procedure to clinical settings. It
282"
REALISTIC DATASETS,0.5253378378378378,"is based on real-world records of N = 11, 984 pairs of same-sex twin births, and has 75 covariates.
283"
REALISTIC DATASETS,0.527027027027027,"It tests the effect of being born the heavier twin (i.e. the treatment) on death within one year (i.e.
284"
REALISTIC DATASETS,0.5287162162162162,"the outcome), with the outcomes of the twins serving as the two potential outcomes. We use the
285"
REALISTIC DATASETS,0.5304054054054054,"dataset generated by Neal et. al (29), that simulates an observational study from the initial data by
286"
REALISTIC DATASETS,0.5320945945945946,"selectively hiding one of the twins with a generative approach. We also ran our analysis on the 2016
287"
REALISTIC DATASETS,0.5337837837837838,"Atlantic Causal Inference Conference (ACIC) semisynthetic dataset with simulated outcomes (30).
288"
REALISTIC DATASETS,0.535472972972973,"For ACIC, given that trees are data greedy, and due to the smaller sample size (N = 4, 802) relative
289"
REALISTIC DATASETS,0.5371621621621622,"to the number of covariates (d = 79), the models were trained on 70% of the dataset.
290"
REALISTIC DATASETS,0.5388513513513513,"0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07"
REALISTIC DATASETS,0.5405405405405406,"|ATE
ATE|"
REALISTIC DATASETS,0.5422297297297297,BICauseTree (IPW)
REALISTIC DATASETS,0.543918918918919,BICauseTree
REALISTIC DATASETS,0.5456081081081081,(Marginal) IPW
REALISTIC DATASETS,0.5472972972972973,Marginal
REALISTIC DATASETS,0.5489864864864865,Matching
REALISTIC DATASETS,0.5506756756756757,"Test
Train"
REALISTIC DATASETS,0.5523648648648649,"Figure 2: Estimation bias for the twins dataset
(N = 11, 984) across 50 subsamples, exclud-
ing positivity violating leaf nodes."
REALISTIC DATASETS,0.5540540540540541,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75"
REALISTIC DATASETS,0.5557432432432432,"|ATE
ATE|"
REALISTIC DATASETS,0.5574324324324325,BICauseTree (IPW)
REALISTIC DATASETS,0.5591216216216216,BICauseTree
REALISTIC DATASETS,0.5608108108108109,(Marginal) IPW
REALISTIC DATASETS,0.5625,Marginal
REALISTIC DATASETS,0.5641891891891891,Matching
REALISTIC DATASETS,0.5658783783783784,"Test
Train"
REALISTIC DATASETS,0.5675675675675675,"Figure 3:
Estimation bias for the ACIC
dataset (N = 4, 802) across 50 subsamples,
excluding positivity violating leaf nodes."
REALISTIC DATASETS,0.5692567567567568,"Effect estimation Figure 2 shows the distribution of the estimation biases across subsamples on
291"
REALISTIC DATASETS,0.5709459459459459,"the twins dataset, comparing to the baseline models. Here, our BICauseTree(Marginal) estimator
292"
REALISTIC DATASETS,0.5726351351351351,"is less biased than the marginal estimator. Augmenting our tree with an IPW outcome model –
293"
REALISTIC DATASETS,0.5743243243243243,"BICauseTree(IPW) – further decreases estimation bias, making it comparable with IPW, both w.r.t
294"
REALISTIC DATASETS,0.5760135135135135,"bias and estimation variance. Figure 3 compares the estimation biases across estimators on the ACIC
295"
REALISTIC DATASETS,0.5777027027027027,"dataset. Here, both BICauseTree models compare with IPW in terms of bias and estimation variance.
296"
REALISTIC DATASETS,0.5793918918918919,"Bias-interpretability tradeoff We expect a bias-interpretability tradeoff, where deeper trees are
297"
REALISTIC DATASETS,0.581081081081081,"less biased but more complex to understand, while shallower trees are less accurate but easier
298"
REALISTIC DATASETS,0.5827702702702703,"to comprehend. Figure 4 shows how estimation bias in leaf nodes decreases as we increase the
299"
REALISTIC DATASETS,0.5844594594594594,"maximum depth hyperparameter of our BICauseTree(Marginal) in the twins dataset. Here, each
300"
REALISTIC DATASETS,0.5861486486486487,"circle in the plot represents a leaf node, and the dotted line shows the average bias with an IPW
301"
REALISTIC DATASETS,0.5878378378378378,"estimator. The shaded area represents the 95% conﬁdence interval (CI) for IPW. As seen in the
302"
REALISTIC DATASETS,0.589527027027027,"plot, there is some overlap between the 95% CI for IPW and the estimation bias of deeper trees.
303"
REALISTIC DATASETS,0.5912162162162162,"The remaining gap thus represents the need for a more complex outcome model in the leaves, or in
304"
REALISTIC DATASETS,0.5929054054054054,"other words the estimation bias that was traded against interpretability here. Similarly, in ﬁgures 2
305"
REALISTIC DATASETS,0.5945945945945946,"and 3 we notice how augmenting our partition with an IPW leaf outcome models has decreased the
306"
REALISTIC DATASETS,0.5962837837837838,"estimation bias at the cost of transparency. Ultimately, ﬁgure 4 shows that bias reduction is consistent
307"
REALISTIC DATASETS,0.597972972972973,"beyond a maximum depth parameter of 5. The robustness of our estimator w.r.t the maximum depth
308"
REALISTIC DATASETS,0.5996621621621622,"hyperparameter is likely due to our statistical pruning procedure. A similar ﬁgure is shown in Section
309"
REALISTIC DATASETS,0.6013513513513513,"A.7 for the ACIC dataset.
310"
REALISTIC DATASETS,0.6030405405405406,"1
2
3
4
5
6
7
8
9
10
Tree maximum depth 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REALISTIC DATASETS,0.6047297297297297,"|ATE
ATE|"
REALISTIC DATASETS,0.606418918918919,95% confidence interval IPW
REALISTIC DATASETS,0.6081081081081081,"Figure 4:
Estimation bias when comparing
BICauseTree(Marginal) with varying maximum
depth parameters with the average bias of IPW
(dotted), on the twins training set (N = 5, 992)."
REALISTIC DATASETS,0.6097972972972973,"Interpretable positivity violations ﬁltering
311"
REALISTIC DATASETS,0.6114864864864865,"As previously discussed, BICauseTree provides
312"
REALISTIC DATASETS,0.6131756756756757,"a built-in method for identifying positivity vi-
313"
REALISTIC DATASETS,0.6148648648648649,"olations in the covariate space directly. After
314"
REALISTIC DATASETS,0.6165540540540541,"positivity ﬁltering, effect was computed on an
315"
REALISTIC DATASETS,0.6182432432432432,"average of 99.5% (σ = 0.006) of the population
316"
REALISTIC DATASETS,0.6199324324324325,"on the twins dataset, and an average of 85.9%
317"
REALISTIC DATASETS,0.6216216216216216,"(σ = 0.093) of the ACIC dataset.
318"
REALISTIC DATASETS,0.6233108108108109,"Figure A4 in the Appendix shows the tree parti-
319"
REALISTIC DATASETS,0.625,"tion for the twins dataset. One leaf node was de-
320"
REALISTIC DATASETS,0.6266891891891891,"tected as having positivity violations (N = 106).
321"
REALISTIC DATASETS,0.6283783783783784,"The twins example illustrates the real-world im-
322"
REALISTIC DATASETS,0.6300675675675675,"pact of having a covariate-based deﬁnition of the
323"
REALISTIC DATASETS,0.6317567567567568,"non-violating subpopulation. Here, we are able
324"
REALISTIC DATASETS,0.6334459459459459,"to claim that our estimate of the effect of being
325"
REALISTIC DATASETS,0.6351351351351351,"born heavier might not be valid for newborns
326"
REALISTIC DATASETS,0.6368243243243243,"that ﬁt the criteria for this speciﬁc violating node.
327"
REALISTIC DATASETS,0.6385135135135135,"This capability of BICauseTree is highly valu-
328"
REALISTIC DATASETS,0.6402027027027027,"able in any safety-sensitive setting. Consider a
329"
REALISTIC DATASETS,0.6418918918918919,"scenario where the “at-risk” twin beneﬁts from
330"
REALISTIC DATASETS,0.643581081081081,"a follow-up visit after birth, and that the true effect of the intervention is higher in the positivity
331"
REALISTIC DATASETS,0.6452702702702703,"violating subpopulation. Extrapolating the estimated effect of the exposure to the entire cohort may
332"
REALISTIC DATASETS,0.6469594594594594,"be dangerous to the infants in this subgroup. It is thus essential for practitioners to know which
333"
REALISTIC DATASETS,0.6486486486486487,"population the inferred effect applies to, which would not have been possible using alternative
334"
REALISTIC DATASETS,0.6503378378378378,"non-interpretable methods for identifying positivity violations e.g. IPW with weight trimming, as they
335"
REALISTIC DATASETS,0.652027027027027,"provide an opaque exclusion criterion. Additionally, note that the positivity violation identiﬁcation
336"
REALISTIC DATASETS,0.6537162162162162,"remains transparent regardless of the chosen propensity or outcome model at the leaves.
337"
REALISTIC DATASETS,0.6554054054054054,"Propensity score estimation Alternative use-cases for BICauseTree include using the partition as a
338"
REALISTIC DATASETS,0.6570945945945946,"propensity model. Given the importance of calibrated propensity scores (31), Figure 5 compares the
339"
REALISTIC DATASETS,0.6587837837837838,"calibration of the propensity score estimation of BICauseTree with the one from logistic regression
340"
REALISTIC DATASETS,0.660472972972973,"(IPW) on the testing set of the twins dataset. As expected, logsitic regression, which has better data
341"
REALISTIC DATASETS,0.6621621621621622,"efﬁciency, has better, less-noisy calibration. However, BICauseTree still shows satisfying calibration
342"
REALISTIC DATASETS,0.6638513513513513,"on average. Section A.6 in the Appendix shows the calibration plots for the estimation of potential
343"
REALISTIC DATASETS,0.6655405405405406,"outcomes on the twins dataset. Section A.7 shows calibration plots for the ACIC dataset.
344"
REALISTIC DATASETS,0.6672297297297297,"0.5
0.6
0.7
0.8
0.9
1.0
Predicted probability 0.5 0.6 0.7 0.8 0.9 1.0"
REALISTIC DATASETS,0.668918918918919,True probability
REALISTIC DATASETS,0.6706081081081081,"Average
Subsamples"
REALISTIC DATASETS,0.6722972972972973,Optimal
REALISTIC DATASETS,0.6739864864864865,BICause Tree
REALISTIC DATASETS,0.6756756756756757,"0.5
0.6
0.7
0.8
0.9
1.0
Predicted probability"
REALISTIC DATASETS,0.6773648648648649,Average
REALISTIC DATASETS,0.6790540540540541,Subsamples
REALISTIC DATASETS,0.6807432432432432,Optimal
REALISTIC DATASETS,0.6824324324324325,Logistic regression
REALISTIC DATASETS,0.6841216216216216,"Figure 5: Calibration of the propensity score esti-
mation for the twins dataset"
REALISTIC DATASETS,0.6858108108108109,"Tree consistency To evaluate the consistency
345"
REALISTIC DATASETS,0.6875,"of our clustering across subsamples, we train
346"
REALISTIC DATASETS,0.6891891891891891,"our tree on 70% of the dataset and compute the
347"
REALISTIC DATASETS,0.6908783783783784,"adjusted Rand index (32) (see further details in
348"
REALISTIC DATASETS,0.6925675675675675,"section A.2). We chose not to train on 50% of
349"
REALISTIC DATASETS,0.6942567567567568,"the data here as most of the inconsistency would
350"
REALISTIC DATASETS,0.6959459459459459,"then be due to the variance between subsamples.
351"
REALISTIC DATASETS,0.6976351351351351,"For the twins dataset, the Rand index across
352"
REALISTIC DATASETS,0.6993243243243243,"50 subsamples of sample sizes N = 8, 388,
353"
REALISTIC DATASETS,0.7010135135135135,"is equal to 0.633 (σ = 0.208). For the ACIC
354"
REALISTIC DATASETS,0.7027027027027027,"dataset, the Rand index across 50 subsamples
355"
REALISTIC DATASETS,0.7043918918918919,"of sample sizes N = 3, 361, is equal to 0.314
356"
REALISTIC DATASETS,0.706081081081081,"(σ = 0.210) which shows that our tree is not
357"
REALISTIC DATASETS,0.7077702702702703,"consistent across subsamples if sample size is
358"
REALISTIC DATASETS,0.7094594594594594,"not substantial. However, we exemplify consistent identiﬁcation of the positivity population, with
359"
REALISTIC DATASETS,0.7111486486486487,"the variance of the percentage of positive samples equal to σ = 0.006 and σ = 0.093 (see paragraph
360"
REALISTIC DATASETS,0.7128378378378378,"4.3) in the twins and ACIC dataset respectively. Ultimately, throughout our experiments, we noticed
361"
REALISTIC DATASETS,0.714527027027027,"how consistency starts to decrease if the maximum depth hyperparameter increases past a certain
362"
REALISTIC DATASETS,0.7162162162162162,"threshold. As a heuristic, we would recommend users to test tree consistency across subsamples
363"
REALISTIC DATASETS,0.7179054054054054,"when tuning this hyperparameter.
364"
DISCUSSION,0.7195945945945946,"5
Discussion
365"
DISCUSSION,0.7212837837837838,"Strengths and limitations of our approach Following our discussion on the bias-interpretability
366"
DISCUSSION,0.722972972972973,"tradeoff, we acknowledge that in complex data settings where ﬁnding sub-populations that enclose
367"
DISCUSSION,0.7246621621621622,"natural experiments is difﬁcult, the resulting BICauseTree partition may have remaining bias in
368"
DISCUSSION,0.7263513513513513,"some leaf nodes, and ultimately render some estimation bias. This bias is, however, traded-off with
369"
DISCUSSION,0.7280405405405406,"enhanced interpretability, as previously discussed. Nonetheless, as exempliﬁed in this work, the
370"
DISCUSSION,0.7297297297297297,"performances of BICauseTree remains comparable, with estimation bias being only slightly larger
371"
DISCUSSION,0.731418918918919,"than common models such as IPW. We further emphasize the fact that its strength resides in the
372"
DISCUSSION,0.7331081081081081,"combination of (i) the performance of the estimator with (ii) the interpretability of the balancing and
373"
DISCUSSION,0.7347972972972973,"positivity identiﬁcation procedures, and (iii) the ability to handle high-dimensional datasets.
374"
DISCUSSION,0.7364864864864865,"Another advantage of BICauseTree is its ability to identify complex interaction features that are
375"
DISCUSSION,0.7381756756756757,"signiﬁcantly correlated to treatment allocation. Indeed, in leaf nodes that come directly from
376"
DISCUSSION,0.7398648648648649,"a signiﬁcant split, the root-to-leaf path is an interaction signiﬁcantly associated with treatment
377"
DISCUSSION,0.7415540540540541,"allocation after multiple hypothesis test correction. Common alternatives to identify such interactions
378"
DISCUSSION,0.7432432432432432,"include exhaustive enumeration of all pairs of feature interactions, or complex feature engineering
379"
DISCUSSION,0.7449324324324325,"(33). However these approaches either lack transparency or become problematic in high-dimensional
380"
DISCUSSION,0.7466216216216216,"datasets. Furthermore, the tree nature of our approach is a major strength. BICauseTree is a non-
381"
DISCUSSION,0.7483108108108109,"parametric estimator that inherit the desirable empirical properties of regression forests—such as
382"
DISCUSSION,0.75,"stability, ease of use, and ﬂexible adaptation to different functional forms. Finally, the computational
383"
DISCUSSION,0.7516891891891891,"expense induced from ﬁtting a BICauseTree is manageable: it is roughly comparable to IPW and
384"
DISCUSSION,0.7533783783783784,"CausalForest, and substantially lower than for Matching (see detailed compute times in Section A.9)
385"
DISCUSSION,0.7550675675675675,"Our work has the following limitations: (i) due to its tree structure, BICauseTree has lower data
386"
DISCUSSION,0.7567567567567568,"efﬁciency than most other estimators, including IPW. However the data efﬁciency of BICauseTree
387"
DISCUSSION,0.7584459459459459,"was superior to that of CT in our experiments. (ii) our tree design has some lunging dependence
388"
DISCUSSION,0.7601351351351351,"on sample size. While our estimation of ASMD is independent of sample size, the variance of our
389"
DISCUSSION,0.7618243243243243,"estimator, \
ASMD, is dependent on n. Furthermore, having chosen the splitting covariate, the choice
390"
DISCUSSION,0.7635135135135135,"of a split point is biased towards equal split subgroups. (iii) our individual splitting decisions do not
391"
DISCUSSION,0.7652027027027027,"consider interactions and instead only consider the marginal association of covariates with treatment.
392"
DISCUSSION,0.7668918918918919,"Applicability of BICauseTree We claim that BICauseTree is highly relevant when causality is
393"
DISCUSSION,0.768581081081081,"examined in a context with substantial safety and ethical concerns. We consider the transparency
394"
DISCUSSION,0.7702702702702703,"of our built-in approach to positivity violation identiﬁcation particularly relevant to ﬁelds such as
395"
DISCUSSION,0.7719594594594594,"epidemiology, econometrics, medicine, and policy-making. The social impact of our work, and its
396"
DISCUSSION,0.7736486486486487,"relevance to the upcoming policies for Artiﬁcial Intelligence is further discussed in section A.10.
397"
DISCUSSION,0.7753378378378378,"In addition, we claim that our ability to identify violating regions of the covariate space is key for
398"
DISCUSSION,0.777027027027027,"experimental design. Fitting a BICauseTree to an existing dataset will advise practitioners on which
399"
DISCUSSION,0.7787162162162162,"individuals we currently lack data to infer an effect on, which will in turn inform them on the speciﬁc
400"
DISCUSSION,0.7804054054054054,"subpopulations they need to recruit from, in a potential next study.
401"
DISCUSSION,0.7820945945945946,"Conclusion and future work Here, we introduced a model able to detect positivity violations
402"
DISCUSSION,0.7837837837837838,"directly in the covariate space, perform effect estimation comparable to existing methods, while
403"
DISCUSSION,0.785472972972973,"allowing for interpretability. We demonstrated our model’s performance on both synthetic and
404"
DISCUSSION,0.7871621621621622,"realistic data, and showcased its usefulness in the principle challenges of causal inference.
405"
DISCUSSION,0.7888513513513513,"Future work may include extension to a non-binary tree, where we allow splitting to more than
406"
DISCUSSION,0.7905405405405406,"two nodes. This could be done for instance by ﬁtting a piece-wise constant function that predicts
407"
DISCUSSION,0.7922297297297297,"treatment and ﬁnds the potentially multiple thresholds for optimized hetereogenous subgroups. In
408"
DISCUSSION,0.793918918918919,"addition, to reﬁne our pruning procedure, we can account for the intrisic ordering of the p-values
409"
DISCUSSION,0.7956081081081081,"of the splits using sequential multiple hypothesis testing (34; 35; 36). Furthermore, following the
410"
DISCUSSION,0.7972972972972973,"work of (8) on the “honest effect” in Causal Forests, we may use a subset of the data for ﬁtting the
411"
DISCUSSION,0.7989864864864865,"partition of the tree and another distinct subset for ﬁtting the outcome or propensity models in each
412"
DISCUSSION,0.8006756756756757,"leaf node. This procedure however requires having many samples. Another alternative to current
413"
DISCUSSION,0.8023648648648649,"model ﬁtting, which is done independently in each leaf, is to partially pool estimates across the
414"
DISCUSSION,0.8040540540540541,"clusters and ﬁt a multilevel outcome model with varying intercepts or varying slopes for treatment
415"
DISCUSSION,0.8057432432432432,"coefﬁcients (37). In terms of estimation, one may investigate the performance of bagging multiple
416"
DISCUSSION,0.8074324324324325,"BICauseTrees into a BICauseForest, similarly to Causal Forest. Aggregating trees would however
417"
DISCUSSION,0.8091216216216216,"defeat the interpretability purpose. Finally, similarly to positivity-violating nodes, future work may
418"
DISCUSSION,0.8108108108108109,"explore the possibility of excluding leaf nodes with high maximum ASMD, under the premises that
419"
DISCUSSION,0.8125,"these subgroups do not enclose natural experiments.
420"
REFERENCES,0.8141891891891891,"References
421"
REFERENCES,0.8158783783783784,"[1] D. B. Rubin, “Estimating causal effects of treatments in randomized and nonrandomized
422"
REFERENCES,0.8175675675675675,"studies.,” Journal of educational Psychology, vol. 66, no. 5, p. 688, 1974.
423"
REFERENCES,0.8192567567567568,"[2] C. Rudin, “Stop explaining black box machine learning models for high stakes decisions and
424"
REFERENCES,0.8209459459459459,"use interpretable models instead,” Nature Machine Intelligence, vol. 1, no. 5, pp. 206–215,
425"
REFERENCES,0.8226351351351351,"2019.
426"
REFERENCES,0.8243243243243243,"[3] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft, “When is “nearest neighbor” meaning-
427"
REFERENCES,0.8260135135135135,"ful?,” in Database Theory—ICDT’99: 7th International Conference Jerusalem, Israel, January
428"
REFERENCES,0.8277027027027027,"10–12, 1999 Proceedings 7, pp. 217–235, Springer, 1999.
429"
REFERENCES,0.8293918918918919,"[4] D. G. Horvitz and D. J. Thompson, “A generalization of sampling without replacement from a
430"
REFERENCES,0.831081081081081,"ﬁnite universe,” Journal of the American statistical Association, vol. 47, no. 260, pp. 663–685,
431"
REFERENCES,0.8327702702702703,"1952.
432"
REFERENCES,0.8344594594594594,"[5] G. W. Imbens and D. B. Rubin, Causal inference in statistics, social, and biomedical sciences.
433"
REFERENCES,0.8361486486486487,"Cambridge University Press, 2015.
434"
REFERENCES,0.8378378378378378,"[6] C. Shi, D. Blei, and V. Veitch, “Adapting neural networks for the estimation of treatment effects,”
435"
REFERENCES,0.839527027027027,"Advances in neural information processing systems, vol. 32, 2019.
436"
REFERENCES,0.8412162162162162,"[7] U. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual treatment effect: generalization
437"
REFERENCES,0.8429054054054054,"bounds and algorithms,” in International Conference on Machine Learning, pp. 3076–3085,
438"
REFERENCES,0.8445945945945946,"PMLR, 2017.
439"
REFERENCES,0.8462837837837838,"[8] S. Athey and G. Imbens, “Recursive partitioning for heterogeneous causal effects,” Proceedings
440"
REFERENCES,0.847972972972973,"of the National Academy of Sciences, vol. 113, no. 27, pp. 7353–7360, 2016.
441"
REFERENCES,0.8496621621621622,"[9] S. R. Künzel, J. S. Sekhon, P. J. Bickel, and B. Yu, “Metalearners for estimating heterogeneous
442"
REFERENCES,0.8513513513513513,"treatment effects using machine learning,” Proceedings of the national academy of sciences,
443"
REFERENCES,0.8530405405405406,"vol. 116, no. 10, pp. 4156–4165, 2019.
444"
REFERENCES,0.8547297297297297,"[10] J. D. Kang and J. L. Schafer, “Demystifying double robustness: A comparison of alternative
445"
REFERENCES,0.856418918918919,"strategies for estimating a population mean from incomplete data,” 2007.
446"
REFERENCES,0.8581081081081081,"[11] J. R. Quinlan et al., “Learning with continuous classes,” in 5th Australian joint conference on
447"
REFERENCES,0.8597972972972973,"artiﬁcial intelligence, vol. 92, pp. 343–348, World Scientiﬁc, 1992.
448"
REFERENCES,0.8614864864864865,"[12] E. Karavani, P. Bak, and Y. Shimoni, “A discriminative approach for ﬁnding and characterizing
449"
REFERENCES,0.8631756756756757,"positivity violations using decision trees,” arXiv preprint arXiv:1907.08127, 2019.
450"
REFERENCES,0.8648648648648649,"[13] M. L. Petersen, K. E. Porter, S. Gruber, Y. Wang, and M. J. Van Der Laan, “Diagnosing and
451"
REFERENCES,0.8665540540540541,"responding to violations in the positivity assumption,” Statistical methods in medical research,
452"
REFERENCES,0.8682432432432432,"vol. 21, no. 1, pp. 31–54, 2012.
453"
REFERENCES,0.8699324324324325,"[14] R. K. Crump, V. J. Hotz, G. W. Imbens, and O. A. Mitnik, “Dealing with limited overlap in
454"
REFERENCES,0.8716216216216216,"estimation of average treatment effects,” Biometrika, vol. 96, no. 1, pp. 187–199, 2009.
455"
REFERENCES,0.8733108108108109,"[15] M. Oberst, F. Johansson, D. Wei, T. Gao, G. Brat, D. Sontag, and K. Varshney, “Characterization
456"
REFERENCES,0.875,"of overlap in observational studies,” in International Conference on Artiﬁcial Intelligence and
457"
REFERENCES,0.8766891891891891,"Statistics, pp. 788–798, PMLR, 2020.
458"
REFERENCES,0.8783783783783784,"[16] G. Wolf, G. Shabat, and H. Shteingart, “Positivity validation detection and explainability via
459"
REFERENCES,0.8800675675675675,"zero fraction multi-hypothesis testing and asymmetrically pruned decision trees,” arXiv preprint
460"
REFERENCES,0.8817567567567568,"arXiv:2111.04033, 2021.
461"
REFERENCES,0.8834459459459459,"[17] S. Ackerman, E. Farchi, O. Raz, M. Zalmanovici, and P. Dube, “Detection of data drift
462"
REFERENCES,0.8851351351351351,"and outliers affecting machine learning model performance over time,” arXiv preprint
463"
REFERENCES,0.8868243243243243,"arXiv:2012.09258, 2020.
464"
REFERENCES,0.8885135135135135,"[18] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting model predictions,” Advances
465"
REFERENCES,0.8902027027027027,"in neural information processing systems, vol. 30, 2017.
466"
REFERENCES,0.8918918918918919,"[19] M. T. Ribeiro, S. Singh, and C. Guestrin, “Model-agnostic interpretability of machine learning,”
467"
REFERENCES,0.893581081081081,"arXiv preprint arXiv:1606.05386, 2016.
468"
REFERENCES,0.8952702702702703,"[20] B. Mittelstadt, C. Russell, and S. Wachter, “Explaining explanations in ai,” in Proceedings of
469"
REFERENCES,0.8969594594594594,"the conference on fairness, accountability, and transparency, pp. 279–288, 2019.
470"
REFERENCES,0.8986486486486487,"[21] S. Wager and S. Athey, “Estimation and inference of heterogeneous treatment effects using
471"
REFERENCES,0.9003378378378378,"random forests,” Journal of the American Statistical Association, vol. 113, no. 523, pp. 1228–
472"
REFERENCES,0.902027027027027,"1242, 2018.
473"
REFERENCES,0.9037162162162162,"[22] S. Athey, J. Tibshirani, and S. Wager, “Generalized random forests,” Ann. Statist, vol. 47,
474"
REFERENCES,0.9054054054054054,"pp. 1148–1178, 2019.
475"
REFERENCES,0.9070945945945946,"[23] K. Battocchi, E. Dillon, M. Hei, G. Lewis, P. Oka, M. Oprescu, and V. Syrgkanis,
476"
REFERENCES,0.9087837837837838,"“EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.”
477"
REFERENCES,0.910472972972973,"https://github.com/microsoft/EconML, 2019. Version 0.x.
478"
REFERENCES,0.9121621621621622,"[24] D. B. Rubin, “The use of matched sampling and regression adjustment to remove bias in
479"
REFERENCES,0.9138513513513513,"observational studies,” Biometrics, pp. 185–203, 1973.
480"
REFERENCES,0.9155405405405406,"[25] P. C. Austin, “Balance diagnostics for comparing the distribution of baseline covariates between
481"
REFERENCES,0.9172297297297297,"treatment groups in propensity-score matched samples,” Statistics in medicine, vol. 28, no. 25,
482"
REFERENCES,0.918918918918919,"pp. 3083–3107, 2009.
483"
REFERENCES,0.9206081081081081,"[26] S. Holm, “A simple sequentially rejective multiple test procedure,” Scandinavian journal of
484"
REFERENCES,0.9222972972972973,"statistics, pp. 65–70, 1979.
485"
REFERENCES,0.9239864864864865,"[27] D. B. Rubin, “Bias reduction using mahalanobis-metric matching,” Biometrics, pp. 293–298,
486"
REFERENCES,0.9256756756756757,"1980.
487"
REFERENCES,0.9273648648648649,"[28] E. A. Stuart, “Matching methods for causal inference: A review and a look forward,” Statistical
488"
REFERENCES,0.9290540540540541,"science: a review journal of the Institute of Mathematical Statistics, vol. 25, no. 1, p. 1, 2010.
489"
REFERENCES,0.9307432432432432,"[29] B. Neal, C.-W. Huang, and S. Raghupathi, “Realcause: Realistic causal inference benchmarking,”
490"
REFERENCES,0.9324324324324325,"arXiv preprint arXiv:2011.15007, 2020.
491"
REFERENCES,0.9341216216216216,"[30] P. R. Hahn, V. Dorie, and J. S. Murray, “Atlantic causal inference conference (acic) data analysis
492"
REFERENCES,0.9358108108108109,"challenge 2017,” arXiv preprint arXiv:1905.09515, 2019.
493"
REFERENCES,0.9375,"[31] R. Gutman, E. Karavani, and Y. Shimoni, “Propensity score models are better when post-
494"
REFERENCES,0.9391891891891891,"calibrated,” arXiv preprint arXiv:2211.01221, 2022.
495"
REFERENCES,0.9408783783783784,"[32] W. M. Rand, “Objective criteria for the evaluation of clustering methods,” Journal of the
496"
REFERENCES,0.9425675675675675,"American Statistical association, vol. 66, no. 336, pp. 846–850, 1971.
497"
REFERENCES,0.9442567567567568,"[33] A. Zheng and A. Casari, Feature engineering for machine learning: principles and techniques
498"
REFERENCES,0.9459459459459459,"for data scientists. "" O’Reilly Media, Inc."", 2018.
499"
REFERENCES,0.9476351351351351,"[34] M. Behr, M. A. Ansari, A. Munk, and C. Holmes, “Testing for dependence on tree structures,”
500"
REFERENCES,0.9493243243243243,"Proceedings of the National Academy of Sciences, vol. 117, no. 18, pp. 9787–9792, 2020.
501"
REFERENCES,0.9510135135135135,"[35] M. P. Allen, “Testing hypotheses in nested regression models,” Understanding regression
502"
REFERENCES,0.9527027027027027,"analysis, pp. 113–117, 1997.
503"
REFERENCES,0.9543918918918919,"[36] A. Malek, S. Katariya, Y. Chow, and M. Ghavamzadeh, “Sequential multiple hypothesis testing
504"
REFERENCES,0.956081081081081,"with type i error control,” in Artiﬁcial Intelligence and Statistics, pp. 1468–1476, PMLR, 2017.
505"
REFERENCES,0.9577702702702703,"[37] A. Feller and A. Gelman, “Hierarchical models for causal effects,” Emerging Trends in the Social
506"
REFERENCES,0.9594594594594594,"and Behavioral Sciences: An interdisciplinary, searchable, and linkable resource, pp. 1–16,
507"
REFERENCES,0.9611486486486487,"2015.
508"
REFERENCES,0.9628378378378378,"[38] S. M. Iacus, G. King, and G. Porro, “Causal inference without balance checking: Coarsened
509"
REFERENCES,0.964527027027027,"exact matching,” Political analysis, vol. 20, no. 1, pp. 1–24, 2012.
510"
REFERENCES,0.9662162162162162,"[39] A. Abadie and G. W. Imbens, “Large sample properties of matching estimators for average
511"
REFERENCES,0.9679054054054054,"treatment effects,” econometrica, vol. 74, no. 1, pp. 235–267, 2006.
512"
REFERENCES,0.9695945945945946,"[40] P. C. Austin, “Optimal caliper widths for propensity-score matching when estimating differences
513"
REFERENCES,0.9712837837837838,"in means and differences in proportions in observational studies,” Pharmaceutical statistics,
514"
REFERENCES,0.972972972972973,"vol. 10, no. 2, pp. 150–161, 2011.
515"
REFERENCES,0.9746621621621622,"[41] G. King and R. Nielsen, “Why propensity scores should not be used for matching,” Political
516"
REFERENCES,0.9763513513513513,"analysis, vol. 27, no. 4, pp. 435–454, 2019.
517"
REFERENCES,0.9780405405405406,"[42] S. L. Morgan and C. Winship, Counterfactuals and causal inference. Cambridge University
518"
REFERENCES,0.9797297297297297,"Press, 2015.
519"
REFERENCES,0.981418918918919,"[43] S. R. Seaman and S. Vansteelandt, “Introduction to double robust methods for incomplete data,”
520"
REFERENCES,0.9831081081081081,"Statistical science: a review journal of the Institute of Mathematical Statistics, vol. 33, no. 2,
521"
REFERENCES,0.9847972972972973,"p. 184, 2018.
522"
REFERENCES,0.9864864864864865,"[44] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, W. Newey, and J. Robins,
523"
REFERENCES,0.9881756756756757,"“Double/debiased machine learning for treatment and structural parameters,” 2018.
524"
REFERENCES,0.9898648648648649,"[45] A. Knudby and L. Ellsworth, “Bonferroni, ce, 1936. teoria statistica delle classi e calcolo delle
525"
REFERENCES,0.9915540540540541,"probabilita, pubblicazioni del r istituto superiore di scienze economiche e commerciali di ﬁrenze,
526"
REFERENCES,0.9932432432432432,"8: 3- 62. brenning, a., 2009. benchmarking classiﬁers to optimally integrate terrain analysis and
527"
REFERENCES,0.9949324324324325,"multispectral remote sensing in automatic,” Environment, vol. 37, no. 1, pp. 35–46, 2009.
528"
REFERENCES,0.9966216216216216,"[46] R. K. Crump, V. J. Hotz, G. Imbens, and O. Mitnik, “Moving the goalposts: Addressing limited
529"
REFERENCES,0.9983108108108109,"overlap in the estimation of average treatment effects by changing the estimand,” 2006.
530"
