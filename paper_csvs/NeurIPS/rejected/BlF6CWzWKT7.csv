Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007830853563038371,"Estimating individual treatment effects (ITEs) from observational data is relevant
1"
ABSTRACT,0.0015661707126076742,"in many fields such as personalized medicine. However, in practice, the treatment
2"
ABSTRACT,0.0023492560689115116,"assignment is usually confounded by unobserved variables and thus introduces
3"
ABSTRACT,0.0031323414252153485,"bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such
4"
ABSTRACT,0.003915426781519186,"settings are widespread in medicine (e.g., trials where compliance is used as binary
5"
ABSTRACT,0.004698512137823023,"IV). In this paper, we propose a novel, multiply robust machine learning framework,
6"
ABSTRACT,0.00548159749412686,"called MRIV, for estimating ITEs using binary IVs and thus yield an unbiased ITE
7"
ABSTRACT,0.006264682850430697,"estimator. Different from previous work for binary IVs, our framework estimates
8"
ABSTRACT,0.007047768206734534,"the ITE directly via a pseudo outcome regression. (1) We provide a theoretical
9"
ABSTRACT,0.007830853563038372,"analysis where we show that our framework yields multiply robust convergence
10"
ABSTRACT,0.008613938919342208,"rates: our ITE estimator achieves fast convergence even if several nuisance esti-
11"
ABSTRACT,0.009397024275646046,"mators converge slowly. (2) We further show that our framework asymptotically
12"
ABSTRACT,0.010180109631949883,"outperforms state-of-the-art plug-in IV methods for ITE estimation. (3) We build
13"
ABSTRACT,0.01096319498825372,"upon our theoretical results and propose a tailored deep neural network architecture
14"
ABSTRACT,0.011746280344557557,"called MRIV-Net for ITE estimation using binary IVs. Across various compu-
15"
ABSTRACT,0.012529365700861394,"tational experiments, we demonstrate empirically that our MRIV-Net achieves
16"
ABSTRACT,0.01331245105716523,"state-of-the-art performance. To the best of our knowledge, our MRIV is the first
17"
ABSTRACT,0.014095536413469069,"multiply robust machine learning framework tailored to estimating ITEs in the
18"
ABSTRACT,0.014878621769772905,"binary IV setting.
19"
INTRODUCTION,0.015661707126076743,"1
Introduction
20"
INTRODUCTION,0.01644479248238058,"Individual treatment effects (ITEs) are relevant across many disciplines such as marketing [41] and
21"
INTRODUCTION,0.017227877838684416,"personalized medicine [51]. Knowledge about ITEs provides insights into the heterogeneity of
22"
INTRODUCTION,0.018010963194988253,"treatment effects, and thus help in potentially better treatment decisions.
23"
INTRODUCTION,0.018794048551292093,"Many recent works that use machine learning to estimate ITEs are based on the assumption of
24"
INTRODUCTION,0.01957713390759593,"unconfoundedness [1, 15, 27, 36, 42], In practice, however, this assumption is often violated because
25"
INTRODUCTION,0.020360219263899765,"it is common that some confounders are not reported in the data. Typical examples are race,
26"
INTRODUCTION,0.021143304620203602,"income, gender, or the socioeconomic status of patients, which are not stored in medical files. If the
27"
INTRODUCTION,0.02192638997650744,"confounding is sufficiently strong, standard methods for estimating ITEs suffer from confounding
28"
INTRODUCTION,0.022709475332811275,"bias [31], which may lead to inferior treatment decisions.
29"
INTRODUCTION,0.023492560689115115,"To handle unobserved confounders, instrumental variables (IVs) can be leveraged to relax the
30"
INTRODUCTION,0.02427564604541895,"assumption of unconfoundedness and still compute reliable ITE estimates. IV methods were originally
31"
INTRODUCTION,0.025058731401722788,"developed in economics [48], but, only recently, there is a growing interest in combining IV methods
32"
INTRODUCTION,0.025841816758026624,"with machine learning (see Sec. 3). Importantly, IV methods outperform classical ITE estimators
33"
INTRODUCTION,0.02662490211433046,"if a sufficient amount of confounding is not observed [17]. We thus aim at estimating ITEs from
34"
INTRODUCTION,0.0274079874706343,"observational data under unobserved confounding using IVs.
35"
INTRODUCTION,0.028191072826938137,"In this paper, we consider the setting where a single binary instrument is available. This setting is
36"
INTRODUCTION,0.028974158183241974,"widespread in personalized medicine (and other applications such as marketing or public policy)
37"
INTRODUCTION,0.02975724353954581,"[9]. In fact, the setting is encountered in essentially all observational or randomized studies with
38"
INTRODUCTION,0.030540328895849646,"observed non-compliance [19]. As an example, consider a randomized controlled trial (RCT), where
39"
INTRODUCTION,0.031323414252153486,"treatments are randomly assigned to patients and their outcomes are observed. Due to some potentially
40"
INTRODUCTION,0.03210649960845732,"unobserved confounders (e.g., income, education), some patients refuse to take the treatment initially
41"
INTRODUCTION,0.03288958496476116,"assigned to them. Here, the treatment assignment serves as a binary IV. Moreover, such RCTs have
42"
INTRODUCTION,0.033672670321064996,"been widely used by public decision-makers, e.g., to analyze the effect of health insurance on health
43"
INTRODUCTION,0.03445575567736883,"outcome (see the so-called Oregon health insurance experiment) [16] or the effect of military service
44"
INTRODUCTION,0.03523884103367267,"on lifetime earnings [2].
45"
INTRODUCTION,0.036021926389976505,"We propose a novel machine learning framework (called MRIV) for estimating ITEs using binary IVs.
46"
INTRODUCTION,0.03680501174628034,"Our framework takes an initial ITE estimator and nuisance parameter estimators as input to perform a
47"
INTRODUCTION,0.037588097102584185,"pseudo-outcome regression. Importantly, our framework uses a multiply robust parametrization of
48"
INTRODUCTION,0.03837118245888802,"the efficient influence function as pseudo outcome.
49"
INTRODUCTION,0.03915426781519186,"We provide a theoretical analysis, where we use tools from [22] to show that our framework achieves
50"
INTRODUCTION,0.039937353171495694,"a multiply robust convergence rate, i.e., our MRIV converges with a fast rate even if several nuisance
51"
INTRODUCTION,0.04072043852779953,"parameters converge slowly. We further show that, compared to existing plug-in IV methods, the
52"
INTRODUCTION,0.04150352388410337,"performance of our framework is asymptotically superior. Finally, we leverage our framework and,
53"
INTRODUCTION,0.042286609240407204,"on top of it, build a tailored deep neural network called MRIV-Net.
54"
INTRODUCTION,0.04306969459671104,"Differences to existing literature: Our framework is multiply robust1, i.e., it is consistent in
55"
INTRODUCTION,0.04385277995301488,"the union of three different model specifications. This is different from existing methods for ITE
56"
INTRODUCTION,0.04463586530931871,"estimation using IVs, which are only doubly robust (e.g., Syrgkanis et al. [40]) or plug-in estimators
57"
INTRODUCTION,0.04541895066562255,"[5, 19].
58"
INTRODUCTION,0.04620203602192639,"Figure 1:
Underlying
causal graph. The instru-
ment Z has a direct influ-
ence on the treatment A,
but does not have a direct
effect on the outcome Y .
Note that we allow for un-
observed confounders for
both Z–A (dashed line)
and A–Y (given by U).
Our setting is general in
that U can be correlated
or uncorrelated with the
observed confounders X."
INTRODUCTION,0.04698512137823023,"Contributions:2 (1) We propose a novel, multiply robust machine learn-
59"
INTRODUCTION,0.047768206734534066,"ing framework (called MRIV) to learn the ITE using the binary IV setting.
60"
INTRODUCTION,0.0485512920908379,"To the best of our knowledge, ours is the first that is multiply robust, i.e.,
61"
INTRODUCTION,0.04933437744714174,"consistent in the union of three model specifications. For comparison,
62"
INTRODUCTION,0.050117462803445575,"existing works for ITE estimation are only double robust [45, 40]. (2) We
63"
INTRODUCTION,0.05090054815974941,"prove that MRIV achieves a multiply robust convergence rate. This is
64"
INTRODUCTION,0.05168363351605325,"different to methods for IV settings which are only doubly robust, such
65"
INTRODUCTION,0.052466718872357085,"as [40]. We further show that our MRIV is asymptotically superior to ex-
66"
INTRODUCTION,0.05324980422866092,"isting plug-in estimators. (3) We propose a tailored deep neural network,
67"
INTRODUCTION,0.05403288958496476,"called MRIV-Net, which builds upon our framework to estimate ITEs.
68"
INTRODUCTION,0.0548159749412686,"We demonstrate that MRIV-Net achieves state-of-the-art performance.
69"
PROBLEM SETUP,0.05559906029757244,"2
Problem setup
70"
PROBLEM SETUP,0.056382145653876274,"Data generating process: We observe data D = (xi, zi, ai, yi)n
i=1 con-
71"
PROBLEM SETUP,0.05716523101018011,"sisting of n ∈N observations of the tuple (X, Z, A, Y ). Here, X ∈X
72"
PROBLEM SETUP,0.05794831636648395,"are observed confounders, Z ∈{0, 1} is a binary instrument, A ∈{0, 1}
73"
PROBLEM SETUP,0.058731401722787784,"is a binary treatment, and Y ∈R is an outcome of interest. Furthermore,
74"
PROBLEM SETUP,0.05951448707909162,"we assume the existence of unobserved confounders U ∈U, which affect
75"
PROBLEM SETUP,0.060297572435395456,"both the treatment A and the outcome Y . The causal graph is shown in
76"
PROBLEM SETUP,0.06108065779169929,"Fig. 1.
77"
PROBLEM SETUP,0.06186374314800313,"Applicability: Our proposed framework is widely applicable in practice, namely to all settings with
78"
PROBLEM SETUP,0.06264682850430697,"the above data generating process. This includes both (1) observational data and (2) RCTs with
79"
PROBLEM SETUP,0.06342991386061081,"non-compliance. For (1), observational data is commonly encountered in, e.g., personalized medicine.
80"
PROBLEM SETUP,0.06421299921691465,"Here, modeling treatments as binary variables is consistent with previous literature on causal effect
81"
PROBLEM SETUP,0.06499608457321848,"estimation and standard in medical practice [33]. For (2), our setting is further encountered in RCTs
82"
PROBLEM SETUP,0.06577916992952232,"when the instrument Z is a randomized treatment assignment but individuals do not comply with
83"
PROBLEM SETUP,0.06656225528582616,"their treatment assignment. Such RCTs have been extensively used by public decision-makers, e.g.,
84"
PROBLEM SETUP,0.06734534064212999,"1For a detailed introduction to multiple robustness and its importance in treatment effect estimation, we refer
to [46], Section 4.5.
2Codes
are
in
the
supplementary
materials.
Codes
are
also
available
at
https://anonymous.4open.science/r/MRIV-Net-0AC4 (Upon acceptance, we replace the link and point
to a public GitHub repository)."
PROBLEM SETUP,0.06812842599843383,"to analyze the effect of health insurance on health outcome [16] or the effect of military service on
85"
PROBLEM SETUP,0.06891151135473766,"lifetime earnings [2].
86"
PROBLEM SETUP,0.0696945967110415,"We build upon the potential outcomes framework [34] for modeling causal effects. Let Y (a, z)
87"
PROBLEM SETUP,0.07047768206734534,"denote the potential outcome that would have been observed under A = a and Z = z. Following
88"
PROBLEM SETUP,0.07126076742364917,"previous literature on IV estimation [45], we impose the following standard IV assumptions on the
89"
PROBLEM SETUP,0.07204385277995301,"data generating process.
90"
PROBLEM SETUP,0.07282693813625685,"Assumption 1 (Standard IV assumptions [45, 47]). We assume: (1) Exclusion: Y (a, z) = Y (a) for
91"
PROBLEM SETUP,0.07361002349256068,"all a, z ∈{0, 1}, i.e., the instrument has no direct effect on the patient outcome; (2) Independence:
92"
PROBLEM SETUP,0.07439310884886452,"Z ⊥⊥U | X; (3) Relevance: Z ̸⊥⊥A | X, (iv) The model includes all A–Y confounder: Y (a) ⊥⊥
93"
PROBLEM SETUP,0.07517619420516837,"(A, Z) | (X, U) for all a ∈{0, 1}.
94"
PROBLEM SETUP,0.0759592795614722,"Assumption 1 is standard for IV methods and fulfilled in practical settings where IV methods
95"
PROBLEM SETUP,0.07674236491777604,"are applied [2, 4, 19]. Note that Assumption 1 does not prohibit the existence of unobserved Z–
96"
PROBLEM SETUP,0.07752545027407988,"A confounders. On the contrary, it merely prohibits the existence of unobserved counfounders
97"
PROBLEM SETUP,0.07830853563038372,"that affect all Z, A, and Y simultaneously, as it is standard in IV settings [47]. A practical and
98"
PROBLEM SETUP,0.07909162098668755,"widespread example where Assumption 1 is satisfied are randomized controlled trials (RCTs) with
99"
PROBLEM SETUP,0.07987470634299139,"non-compliance [19]. Here, the treatment assignment Z is randomized, but the actual relationship
100"
PROBLEM SETUP,0.08065779169929523,"between treatment A and outcome Y may still be confounded. For instance, in the Oregon health
101"
PROBLEM SETUP,0.08144087705559906,"insurance experiment [16], people were given access to health insurance (Z) by a lottery with aim
102"
PROBLEM SETUP,0.0822239624119029,"to study the effect of health insurance (A) on health outcome (Y ) [16]. Here, non-compliance
103"
PROBLEM SETUP,0.08300704776820673,"information is observed because the lottery winners needed to sign up for health insurance.
104"
PROBLEM SETUP,0.08379013312451057,"Objective: In this paper, we are interested in estimating the individual treatment effect (ITE)
105"
PROBLEM SETUP,0.08457321848081441,"τ(x) = E[Y (1) −Y (0) | X = x].
(1)"
PROBLEM SETUP,0.08535630383711824,"If there is no unobserved confounding (U = ∅), the ITE is identifiable from observational data under
106"
PROBLEM SETUP,0.08613938919342208,"mild positivity assumptions [36]. However, in practice, it is often unlikely that all confounders are
107"
PROBLEM SETUP,0.08692247454972592,"observable. To account for this, we leverage the instrument Z to identify the ITE. We state the
108"
PROBLEM SETUP,0.08770555990602975,"following assumption for identifiability.
109"
PROBLEM SETUP,0.08848864526233359,"Assumption 2 (Identifiability of the ITE [45]). At least one of the following two statements holds
110"
PROBLEM SETUP,0.08927173061863743,"true: (1) E[A | Z = 1, X, U] −E[A | Z = 0, X, U] = E[A | Z = 1, X] −E[A | Z = 0, X]; or
111"
PROBLEM SETUP,0.09005481597494126,"(2) E[Y (1) −Y (0) | X, U] = E[Y (1) −Y (0) | X].
112"
PROBLEM SETUP,0.0908379013312451,"Example: Assumption 1 holds is when the function f(a, X, U) = E[Y (a) | X, U] is additive with
113"
PROBLEM SETUP,0.09162098668754894,"respect to a and U, e.g., f(a, X, U) = g(a, X) + h(U) for measurable functions h and g.
114"
PROBLEM SETUP,0.09240407204385279,"Under Assumptions 1 and 2, the ITE is identifiable [45]. It can be written as
115"
PROBLEM SETUP,0.09318715740015662,"τ(x) = µY
1 (x) −µY
0 (x)
µA
1 (x) −µA
0 (x) = δY (x)"
PROBLEM SETUP,0.09397024275646046,"δA(x),
(2)"
PROBLEM SETUP,0.0947533281127643,"where µY
i (x) = E[Y | Z = i, X = x] and µA
i (x) = E[A | Z = i, X = x]. Even if Assumption 2
116"
PROBLEM SETUP,0.09553641346906813,"does not hold, the quantity on the right-hand side of Eq. (2) still allows for interpretation. If no
117"
PROBLEM SETUP,0.09631949882537197,"unobserved Z–A confounders exist, it can be interpreted as conditional version of the local average
118"
PROBLEM SETUP,0.0971025841816758,"treatment effect (LATE) [19, 5] under a monotonicity assumption. Furthermore, under a no-current-
119"
PROBLEM SETUP,0.09788566953797964,"treatment-value-interaction assumption, it can be interpreted as conditional treatment effect on the
120"
PROBLEM SETUP,0.09866875489428348,"treated (ETT) [45]. 3 This has an important implication for our results: If Assumption 2 does not
121"
PROBLEM SETUP,0.09945184025058731,"hold in practice, our estimates still provide conditional LATE or ETT estimates under the respective
122"
PROBLEM SETUP,0.10023492560689115,"assumptions because they are based on Eq. (2). If Assumption 2 does hold, all three – i.e., ITE,
123"
PROBLEM SETUP,0.10101801096319499,"conditional LATE, and ETT – coincide [45].
124"
RELATED WORK,0.10180109631949882,"3
Related work
125"
RELATED WORK,0.10258418167580266,"ITE methods without unconfoundedness: Various machine learning methods for estimating ITEs
126"
RELATED WORK,0.1033672670321065,"without unobserved confounding have been proposed in recent literature [1, 15, 25, 27, 36, 42, 52,
127"
RELATED WORK,0.10415035238841033,"3The conditional LATE measures the ITE for individuals which are part of the complier subpopulation, i.e.,
the subpopulation for whom A(Z = 1) > A(Z = 0). The conditional ETT measures the ITE for treated
individuals."
RELATED WORK,0.10493343774471417,"53]. To remove plug-in bias, the DR-learner performs a second stage regression on the uncentered
128"
RELATED WORK,0.105716523101018,"influence function of the average treatment effect [22, 14]. However, under unobserved confounding,
129"
RELATED WORK,0.10649960845732184,"all of these methods are biased (see Appendix). As a result, this hampers their performance in our
130"
RELATED WORK,0.10728269381362568,"setting.
131"
RELATED WORK,0.10806577916992952,"ITE methods for unobserved confounding: There is a rich literature for causal effect estimation
132"
RELATED WORK,0.10884886452623337,"under unobserved confounding. Methods include deconfounding methods [46, 7, 18], proxy learning
133"
RELATED WORK,0.1096319498825372,"methods [13, 49], causal sensitivity analysis [21, 20], and IV methods. IV methods address the
134"
RELATED WORK,0.11041503523884104,"problem of unobserved confounding by exploiting the variance in treatment and outcome induced by
135"
RELATED WORK,0.11119812059514488,"the instruments. Traditionally, two-stage least squares (2SLS) has been used for estimating causal
136"
RELATED WORK,0.11198120595144871,"effects [48, 4]. 2SLS was originally developed in economics, and follows a two-stage procedure: it
137"
RELATED WORK,0.11276429130775255,"performs a first stage regression of treatment A on the instrument Z, and then uses the fitted values
138"
RELATED WORK,0.11354737666405638,"for a second stage regression to predict the outcome Y . Several nonparametric methods have been
139"
RELATED WORK,0.11433046202036022,"developed in econometric to generalize 2SLS in order to account for non-linearities within the data
140"
RELATED WORK,0.11511354737666406,"[28, 44], yet these are limited to low-dimensional settings.
141"
RELATED WORK,0.1158966327329679,"Only recently, machine learning has been integrated into IV methods. These are: [37] and [50]
142"
RELATED WORK,0.11667971808927173,"generalize 2SLS by learning complex feature maps using kernel methods and deep learning, re-
143"
RELATED WORK,0.11746280344557557,"spectively. [17] adopts a two-stage neural network architecture that performs the first stage via
144"
RELATED WORK,0.1182458888018794,"conditional density estimation. [6] and [40] leverage moment conditions for IV estimation. However,
145"
RELATED WORK,0.11902897415818324,"the aforementioned methods are not specifically designed for the binary IV setting but, rather, for
146"
RELATED WORK,0.11981205951448708,"multiple IVs or treatment scenarios. In particular, they impose stronger assumptions such as additive
147"
RELATED WORK,0.12059514487079091,"confounding in order to identify the ITE. Note that additive confounding is a special case of when our
148"
RELATED WORK,0.12137823022709475,"Assumption 2 holds. Moreover, they are not multiply robust: Even though doubly robust IV methods
149"
RELATED WORK,0.12216131558339859,"have been proposed (e.g., Syrgkanis et al. [40]), these methods are not consistent in the union of more
150"
RELATED WORK,0.12294440093970242,"than two model specifications [45]. We provide more details below.
151"
RELATED WORK,0.12372748629600626,"Table 1: Key methods for causal effect es-
timation with IVs. This paper: Multiply
robustness for ITEs.
hhhhhhhhhh
Robustness
Estimand
ATE
ITE"
RELATED WORK,0.1245105716523101,"Doubly robust
Okui et al. [30]
Syrgkanis et al. [40]
Multiply robust
Wang et al. [45]
MRIV (ours)"
RELATED WORK,0.12529365700861395,"Doubly robust IV methods: Doubly robust estimators
152"
RELATED WORK,0.12607674236491778,"are commonly used in causal inference as they allow
153"
RELATED WORK,0.12685982772122162,"for consistent estimation under model misspecification
154"
RELATED WORK,0.12764291307752546,"and fast convergence rates [22]. Recently, they also
155"
RELATED WORK,0.1284259984338293,"have been adopted for IV settings: [23] proposes a
156"
RELATED WORK,0.12920908379013313,"pseudo regression estimator for the local average treat-
157"
RELATED WORK,0.12999216914643696,"ment effect using continuous instruments, which has been extended to individual effects by [35].
158"
RELATED WORK,0.1307752545027408,"Furthermore, [38] uses a doubly robust approach to estimate average compiler parameters. Finally,
159"
RELATED WORK,0.13155833985904464,"Ogburn et al. [29] and Syrgkanis et al. [40] propose doubly robust ITE estimators in the IV setting
160"
RELATED WORK,0.13234142521534847,"which both rely on doubly robust parametrizations of the uncentered efficient influence function [30].
161"
RELATED WORK,0.1331245105716523,"However, these estimators are not multiply robust in the sense that they are consistent in the union of
162"
RELATED WORK,0.13390759592795615,"more than two model specifications [45].
163"
RELATED WORK,0.13469068128425998,"Multiply robust IV methods: Multiply robust estimators for IV settings have been proposed only
164"
RELATED WORK,0.13547376664056382,"for average treatment effects (ATEs) [45] and optimal treatment regimes [12] but not for ITEs. In
165"
RELATED WORK,0.13625685199686766,"particular, Wang et al. [45] derive a multiply robust parametrization of the efficient influence function
166"
RELATED WORK,0.1370399373531715,"for the ATE. However, there exists no similar approach for ITE estimation (see Table 1).
167"
RELATED WORK,0.13782302270947533,"We provide a detailed, technical comparison of existing methods and our framework in Appendix E.
168"
RELATED WORK,0.13860610806577917,"Binary IVs: In the binary IV setting, current methods proceed by estimating µY
i (x) and µA
i (x)
169"
RELATED WORK,0.139389193422083,"separately, before plugging them in Eq. 2 [19, 3, 5]. As result, these suffer from plug-in bias and do
170"
RELATED WORK,0.14017227877838684,"not offer robustness properties.
171"
RELATED WORK,0.14095536413469067,"Research gap: To the best of our knowledge, there exists no method for ITE estimation under
172"
RELATED WORK,0.1417384494909945,"unobserved confounding that is multiply robust. To fill this gap, we propose MRIV: a multiply robust
173"
RELATED WORK,0.14252153484729835,"machine learning framework tailored to the binary IV setting. For this, we build upon the approach
174"
RELATED WORK,0.14330462020360218,"by Kennedy [22] to derive robust convergence rates, yet this approach has not been adapted to IV
175"
RELATED WORK,0.14408770555990602,"settings, which is our contribution.
176"
MRIV FOR ESTIMATING ITES USING BINARY INSTRUMENTS,0.14487079091620986,"4
MRIV for estimating ITEs using binary instruments
177"
MRIV FOR ESTIMATING ITES USING BINARY INSTRUMENTS,0.1456538762725137,"In the following, we present our MRIV framework for estimating ITEs under unobserved confounding
178"
MRIV FOR ESTIMATING ITES USING BINARY INSTRUMENTS,0.14643696162881753,"(Sec. 4.1). We then derive an asymptotic convergence rate for MRIV (Sec. 4.2) and finally use our
179"
MRIV FOR ESTIMATING ITES USING BINARY INSTRUMENTS,0.14722004698512137,"framework to develop a tailored deep neural network called MRIV-Net (Sec. 4.4).
180"
FRAMEWORK,0.1480031323414252,"4.1
Framework
181"
FRAMEWORK,0.14878621769772904,"Motivation: A naïve approach to estimate the ITE is to leverage the identification result in Eq. (2).
182"
FRAMEWORK,0.14956930305403288,"Assuming that we have estimated the nuisance components ˆµY
i and ˆµA
i for i ∈{0, 1}, we can simply
183"
FRAMEWORK,0.15035238841033674,"plug them into Eq. (2) to obtain the so-called (plug-in) Wald estimator ˆτW(x) [43].
184"
FRAMEWORK,0.15113547376664058,"However, in practice, the true ITE curve τ(x) is often simpler (e.g., smoother, more sparse) than
185"
FRAMEWORK,0.1519185591229444,"its complements µY
i (x) or µA
i (x) [25]. In this case, ˆτW(x) is inefficient because it models all
186"
FRAMEWORK,0.15270164447924825,"components separately, and, to address this, our proposed framework estimates τ directly using a
187"
FRAMEWORK,0.1534847298355521,"pseudo outcome regression.
188"
FRAMEWORK,0.15426781519185592,"Overview: We now propose MRIV. MRIV is a two-stage meta learner that takes any base method for
189"
FRAMEWORK,0.15505090054815976,"ITE estimation as input. For instance, the base ssssmethod could be the Wald estimator from Eq. (2),
190"
FRAMEWORK,0.1558339859044636,"any other IV method such as 2SLS, or a deep neural network (as we propose in our MRIV-Net later
191"
FRAMEWORK,0.15661707126076743,"in Sec. 4.4). In Stage 1, MRIV produces nuisance estimators ˆµY
0 (x), ˆµA
0 (x), ˆδA(x), and ˆπ(x), where
192"
FRAMEWORK,0.15740015661707127,"ˆπ(x) is an estimator of the propensity score π(x) = P(Z = 1 | X = x). In Stage 2, MRIV estimates
193"
FRAMEWORK,0.1581832419733751,"τ(x) directly using a pseudo outcome ˆYMR as a regression target.
194"
FRAMEWORK,0.15896632732967894,"Given an arbitrary initial ITE estimator ˆτinit(x) and nuisance estimates ˆµY
0 (x), ˆµA
0 (x), ˆδA(x), and
195"
FRAMEWORK,0.15974941268598278,"ˆπ(x), we define the pseudo outcome
196"
FRAMEWORK,0.16053249804228661,ˆYMR =
FRAMEWORK,0.16131558339859045,Z −(1 −Z)
FRAMEWORK,0.1620986687548943,ˆδA(X)
FRAMEWORK,0.16288175411119812,"!  
Y −
 
ˆµY
0 (X) + ˆτinit(X) (A −ˆµA
0 (X))
"
FRAMEWORK,0.16366483946750196,Z ˆπ(X) + (1 −Z)(1 −ˆπ(X)) !
FRAMEWORK,0.1644479248238058,"+ ˆτinit(X).
(3) 197"
FRAMEWORK,0.16523101018010963,"The pseudo outcome ˆYMR in Eq. (3) is a multiply robust parameterization of the (uncentered) efficient
198"
FRAMEWORK,0.16601409553641347,"influence function for the average treatment effect EX[τ(X)] (see the derivation in [45]). The initial
199"
FRAMEWORK,0.1667971808927173,"estimator ˆτinit(X) is corrected by a weighted difference of the observed outcome Y and the term
200"
FRAMEWORK,0.16758026624902114,"ˆµY
0 (X) + ˆτinit(X) (A −ˆµA
0 (X)). Individuals X with small ˆδA(X) (large estimated compliance) or
201"
FRAMEWORK,0.16836335160532498,"small/large π(X) (i.e., low/high probability of receiving treatment Z) receive a larger correction.
202"
FRAMEWORK,0.16914643696162882,"Once we have obtained the pseudo outcome ˆYMR, we regress it on X to obtain the Stage 2 MRIV
203"
FRAMEWORK,0.16992952231793265,"estimator ˆτMRIV(x) for τ(x). The pseudocode for MRIV is given in Algorithm 1. MRIV can be
204"
FRAMEWORK,0.1707126076742365,"interpreted as a way to remove plug-in bias from ˆτinit(x) via the efficient influence function [14]
205"
FRAMEWORK,0.17149569303054032,Algorithm 1: MRIV
FRAMEWORK,0.17227877838684416,"Input : data (X, Z, A, Y ), initial ITE estimator ˆτinit(x)
// Stage 1:
Estimate nuisance components
ˆπ(x) ←ˆE[Z | X = x],
ˆµY
0 (x) ←ˆE[Y | X = x, Z = 0],
ˆµA
0 (x) ←ˆE[A | X = x, Z = 0]
ˆδA(x) ←ˆE[A | X = x, Z = 1] −ˆE[A | X = x, Z = 0]
// Stage 2:
Pseudo outcome regression"
FRAMEWORK,0.173061863743148,"ˆYMR ←

Z−(1−Z)"
FRAMEWORK,0.17384494909945183,ˆδA(X)
FRAMEWORK,0.17462803445575567," 
Y −A ˆτinit(X)−ˆ
µY
0 (X)+ˆ
µA
0 (X) ˆτinit(X)
Z ˆπ(X)+(1−Z)(1−ˆπ(X))"
FRAMEWORK,0.1754111198120595,"
+ ˆτinit(X)"
FRAMEWORK,0.17619420516836334,ˆτMRIV(x) ←ˆE[ ˆYMR | X = x] 206
FRAMEWORK,0.17697729052466718,"Using the fact that ˆYMR is a multiply robust parametrization of the efficient influence function, we
207"
FRAMEWORK,0.17776037588097102,"derive a multiply robustness property of ˆτMRIV(x).
208"
FRAMEWORK,0.17854346123727485,"Theorem 1 (multiply robustness property). Let ˆµY
0 (x), ˆµA
0 (x), ˆδA(x), ˆπ(x), and ˆτinit(x) denote
209"
FRAMEWORK,0.1793265465935787,"estimators of µY
0 (x), µA
0 (x), δA(x), π(x), and τ(x), respectively. Then, for all x ∈X, it holds that
210"
FRAMEWORK,0.18010963194988253,"E[ ˆYMR | X = x] = τ(x),if least one of the following conditions is satisfied: (1) ˆµY
0 = µY
0 , ˆµA
0 = µA
0 ,
211"
FRAMEWORK,0.18089271730618636,"ˆδA = δA, and ˆτinit = τ; or (2) ˆπ = π and ˆδA = δA; or (3) ˆπ = π and ˆτinit = τ.
212"
FRAMEWORK,0.1816758026624902,"Theorem 1 implies that ˆτMRIV(x) is consistent for τ(x) if either condition (1), (2), or (3) holds.
213"
FRAMEWORK,0.18245888801879404,"As a result, our MRIV framework is multiply robust in the sense that our estimator, ˆτMRIV(x), is
214"
FRAMEWORK,0.18324197337509787,"consistent in the union of three different model specifications. Importantly, this is different from
215"
FRAMEWORK,0.18402505873140174,"doubly robust estimators which are only consistent in the union of two model specifications [45].
216"
FRAMEWORK,0.18480814408770557,"Example: We illustrate the robustness under model specification (2) in an example. Let ˆµY
0 (x) =
217"
FRAMEWORK,0.1855912294440094,"ˆµA
0 (x) = ˆτinit(x) = 0 be misspecified and let ˆπ = π and ˆδA = δA be correctly specified. It
218"
FRAMEWORK,0.18637431480031325,"follows E[ ˆYMR | X = x] =
1
δA(X)E
h
ZY −(1−Z)Y
Zπ(x)+(1−Z)(1−π(x)) | X = x
i
= µY
1 (x)−µY
0 (x)
δA(X)
= τ(x). This
219"
FRAMEWORK,0.18715740015661708,"justifies the pseudo-outcome regression in last step of MRIV.
220"
FRAMEWORK,0.18794048551292092,"Our MRIV is directly applicable to RCTs with non-compliance: Then, the treatment assignment is
221"
FRAMEWORK,0.18872357086922475,"randomized and the propensity score π(x) is known. Our MRIV framework can be thus adopted
222"
FRAMEWORK,0.1895066562255286,"by plugging in the known π(x) into the pseudo outcome in Eq. (3). Moreover, ˆτMRIV(x) is already
223"
FRAMEWORK,0.19028974158183243,"consistent if either ˆτinit(x) or ˆδA(x) are.
224"
THEORETICAL ANALYSIS,0.19107282693813626,"4.2
Theoretical analysis
225"
THEORETICAL ANALYSIS,0.1918559122944401,"In the following, we derive the asymptotic convergence rate of MRIV under smoothness assumptions.
226"
THEORETICAL ANALYSIS,0.19263899765074394,"For this, we define s-smooth functions as functions contained in the Hölder class H(s), associated
227"
THEORETICAL ANALYSIS,0.19342208300704777,"with Stone’s minimax rate [39] of n−2s/(2s+p), where p is the dimension of X.
228"
THEORETICAL ANALYSIS,0.1942051683633516,"Assumption 3 (Smoothness). We assume that (1) the nuisance components µY
i (·) are α-smooth,
229"
THEORETICAL ANALYSIS,0.19498825371965545,"µA
i (·) and δA(·) are β-smooth, and π(·) is δ-smooth; (2) all nuisance components are estimated with
230"
THEORETICAL ANALYSIS,0.19577133907595928,their respective minimax rate of n
THEORETICAL ANALYSIS,0.19655442443226312,"−2k
2k+p , where k ∈{α, β, δ}; and (3) the oracle ITE τ(·) is γ-smooth
231"
THEORETICAL ANALYSIS,0.19733750978856696,"and the initial ITE estimator ˆτinit converges with rate rτ(n).
232"
THEORETICAL ANALYSIS,0.1981205951448708,"Assumption 3 for smoothness provides us with a way to quantify the difficulty of the underlying
233"
THEORETICAL ANALYSIS,0.19890368050117463,"nonparametric regression problems. Similar assumptions have been imposed for asymptotic analysis
234"
THEORETICAL ANALYSIS,0.19968676585747847,"of previous ITE estimators in [22, 15]. They can be replaced with other assumptions such as
235"
THEORETICAL ANALYSIS,0.2004698512137823,"assumptions on the level of sparsity of the ITE components. We also provide an asymptotic analysis
236"
THEORETICAL ANALYSIS,0.20125293657008614,"under sparsity assumptions (see Appendix B).
237"
THEORETICAL ANALYSIS,0.20203602192638997,"We additionally impose the following boundedness assumptions on the the underlying data generating
238"
THEORETICAL ANALYSIS,0.2028191072826938,"process and estimators.
239"
THEORETICAL ANALYSIS,0.20360219263899765,"Assumption 4 (Boundedness). We assume that there exist constants C, ρ, eρ, ϵ, K > 0 such that for
240"
THEORETICAL ANALYSIS,0.20438527799530148,"all x ∈X it holds that: (1) |µY
i (x)| ≤C; (2) |δA(x)| = |µA
1 (x) −µA
0 (x)| ≥ρ and |ˆδA(x)| ≥eρ;
241"
THEORETICAL ANALYSIS,0.20516836335160532,"(3) ϵ ≤ˆπ(x) ≤1 −ϵ; and (4) |ˆτinit(x)| ≤K.
242"
THEORETICAL ANALYSIS,0.20595144870790916,"Assumptions 4.1, 4.3, and 4.4 are standard and in line with previous works on theoretical analyses
243"
THEORETICAL ANALYSIS,0.206734534064213,"of ITE estimators [15, 22]. Assumption 4.2 ensures that both the oracle ITE and the estimator are
244"
THEORETICAL ANALYSIS,0.20751761942051683,"bounded. Violations of Assumption 4.2 may occur when working with so-called “weak” instruments,
245"
THEORETICAL ANALYSIS,0.20830070477682067,"which are IVs that are only weakly correlated with the treatment. Using IV methods with weak
246"
THEORETICAL ANALYSIS,0.2090837901331245,"instruments should generally be avoided [26]. However, in many applications such as RCTs with
247"
THEORETICAL ANALYSIS,0.20986687548942834,"non-compliance, weak instruments are unlikely to occur as patients’ decisions to follow the treatment
248"
THEORETICAL ANALYSIS,0.21064996084573218,"are generally correlated with the initial treatment assignments.
249"
THEORETICAL ANALYSIS,0.211433046202036,"We state now our main theoretical result: an upper bound on the oracle risk of the MRIV estimator.
250"
THEORETICAL ANALYSIS,0.21221613155833985,"To derive our bound, we leverage the sample splitting approach from [22]. The approach in [22] has
251"
THEORETICAL ANALYSIS,0.21299921691464369,"been initially used to analyze the DR-learner for ITE estimation under unconfoundedness and allows
252"
THEORETICAL ANALYSIS,0.21378230227094752,"for the derivation of robust convergence rates. It has later been adapted to several other meta learners
253"
THEORETICAL ANALYSIS,0.21456538762725136,"[15], yet not for IV methods.
254"
THEORETICAL ANALYSIS,0.2153484729835552,"Theorem 2 (Oracle upper bound under sample splitting). Let Dℓfor ℓ∈{1, 2, 3} be independent
255"
THEORETICAL ANALYSIS,0.21613155833985903,"samples of size n. Let ˆτinit(x), ˆµY
0 (x), and ˆµA
0 (x) be trained on D1, and let ˆδA(x) and ˆπ(x) be
256"
THEORETICAL ANALYSIS,0.2169146436961629,"trained on D2. We denote ˆYMR as the pseudo outcome from Eq. (3) and Y0 as the corresponding
257"
THEORETICAL ANALYSIS,0.21769772905246673,"oracle. Let ˆτMRIV(x) = ˆEn[ ˆYMR | X = x] and eτMRIV(x) = ˆEn[Y0 | X = x] denote the (oracle)
258"
THEORETICAL ANALYSIS,0.21848081440877057,"pseudo outcome regression on D3 for some generic estimator ˆEn[· | X = x] of E[· | X = x].
259"
THEORETICAL ANALYSIS,0.2192638997650744,"We assume that the second-stage estimator ˆEn yields the minimax rate n−
2γ
2γ+p and satisfies the fol-
260"
THEORETICAL ANALYSIS,0.22004698512137824,"lowing two assumptions from Kennedy [22]: (1) ˆEn[W + c | X = x] = ˆEn[W | X = x] + c
261"
THEORETICAL ANALYSIS,0.22083007047768208,"for any random W and constant c and (2) if E[W
| X = x] = E[V
| X = x], then
262"
THEORETICAL ANALYSIS,0.22161315583398591,"E

ˆEn[W | X = x] −E[W | X = x]
2
≍E

ˆEn[V | X = x] −E[V | X = x]
2
. Then, the
263"
THEORETICAL ANALYSIS,0.22239624119028975,"oracle risk is upper bounded by
264"
THEORETICAL ANALYSIS,0.2231793265465936,"E
h
(ˆτMRIV(x) −τ(x))2i
≲n"
THEORETICAL ANALYSIS,0.22396241190289742,"−2γ
2γ+p + rτ(n)

n"
THEORETICAL ANALYSIS,0.22474549725920126,"−2β
2β+p + n"
THEORETICAL ANALYSIS,0.2255285826155051,"−2δ
2δ+p

+ n−2(
α
2α+p +
δ
2δ+p) + n−2(
β
2β+p +
δ
2δ+p)."
THEORETICAL ANALYSIS,0.22631166797180893,"Proof. See Appendix A.
265"
THEORETICAL ANALYSIS,0.22709475332811277,"Recall that the first summand of the lower bound in Eq. (2) is the minimax rate for the oracle ITE
266"
THEORETICAL ANALYSIS,0.2278778386844166,"τ(x) which cannot be improved upon. Hence, for a fast convergence rate of ˆτMRIV(x), it is sufficient
267"
THEORETICAL ANALYSIS,0.22866092404072044,"if either: (1) rτ(n) decreases fast and δ is large; (2) rτ(n) decreases fast and α and β are large;
268"
THEORETICAL ANALYSIS,0.22944400939702428,"or (3) all α, β, and δ are large. This is in line with the multiply robustness property of MRIV and
269"
THEORETICAL ANALYSIS,0.23022709475332812,"means that MRIV achieves a fast rate of convergence even if the initial estimator or several nuisance
270"
THEORETICAL ANALYSIS,0.23101018010963195,"estimators converge slowly.
271"
THEORETICAL ANALYSIS,0.2317932654659358,"From the bound in Eq. (2), it follows that ˆτMRIV(x) improves on the convergence rate of the initial
272"
THEORETICAL ANALYSIS,0.23257635082223962,"ITE estimator ˆτinit(x) if its rate rτ(n) is lower bounded by
273"
THEORETICAL ANALYSIS,0.23335943617854346,rτ(n) ≳n
THEORETICAL ANALYSIS,0.2341425215348473,"−2γ
2γ+p + n−2(
α
2α+p +
δ
2δ+p) + n−2(
β
2β+p +
δ
2δ+p).
(4)
Hence, our MRIV estimator is more likely to improve on the initial estimator for large α, β, and δ,
274"
THEORETICAL ANALYSIS,0.23492560689115113,"i.e., if the nuisance components are smooth. Note that it is sufficient if either (1) only the propensity
275"
THEORETICAL ANALYSIS,0.23570869224745497,"score π(x) is relatively smooth (large δ) or (2) that all other nuisance components are (large α and
276"
THEORETICAL ANALYSIS,0.2364917776037588,"β). In fact, this is widely fulfilled in practice. For example, the former is fulfilled for RCTs with
277"
THEORETICAL ANALYSIS,0.23727486296006264,"non-compliance, where π(x) is often some known, fixed number p ∈(0, 1). Hence, for RCTs with
278"
THEORETICAL ANALYSIS,0.23805794831636648,"non-compliance, MRIV should (at least asymptotically) improve the performance of most estimators.
279"
THEORETICAL ANALYSIS,0.23884103367267032,"4.3
MRIV vs. Wald estimator
280"
THEORETICAL ANALYSIS,0.23962411902897415,"In the following, we compare ˆτMRIV(x) to the Wald estimator ˆτW(x). First, we derive corresponding
281"
THEORETICAL ANALYSIS,0.240407204385278,"upper bound under smoothness.
282"
THEORETICAL ANALYSIS,0.24119028974158183,"Theorem 3 (Wald oracle upper bound). Given estimators ˆµY
i (x) and ˆµA
i (x). Let ˆδA(x) = ˆµA
1 (x) −
283"
THEORETICAL ANALYSIS,0.24197337509788566,"ˆµA
0 (x) satisfy Assumption 4. Then, the oracle risk of the Wald estimator ˆτW (x) is bounded by
284"
THEORETICAL ANALYSIS,0.2427564604541895,"E

(ˆτW(x) −τ(x))2
≲n−
2α
2α+p + n−
2β
2β+p .
(5)"
THEORETICAL ANALYSIS,0.24353954581049334,"Proof. See Appendix A.
285"
THEORETICAL ANALYSIS,0.24432263116679717,"We now consider the MRIV estimator ˆτMRIV(x) with ˆτinit = ˆτW(x), i.e., initialized with the Wald
286"
THEORETICAL ANALYSIS,0.245105716523101,"estimator (under sample splitting). Plugging the Wald rate from Eq. (5) into the Eq. (2) yields
287"
THEORETICAL ANALYSIS,0.24588880187940484,"E
h
(ˆτMRIV(x) −τ(x))2i
≲n"
THEORETICAL ANALYSIS,0.24667188723570868,"−2γ
2γ+p +n"
THEORETICAL ANALYSIS,0.24745497259201252,"−4β
2β+p +n−2(
α
2α+p +
β
2β+p)+n−2(
δ
2δ+p +
α
2α+p)+n−2(
δ
2δ+p +
β
2β+p).
(6)"
THEORETICAL ANALYSIS,0.24823805794831635,"For α = β = δ, the rates of ˆτMRIV(x) and ˆτW(x) reduce to
288"
THEORETICAL ANALYSIS,0.2490211433046202,"E
h
(ˆτMRIV(x) −τ(x))2i
≲n"
THEORETICAL ANALYSIS,0.24980422866092403,"−2γ
2γ+p + n"
THEORETICAL ANALYSIS,0.2505873140172279,"−4α
2α+p
and
E
h
(ˆτW(x) −τ(x))2i
≲n"
THEORETICAL ANALYSIS,0.2513703993735317,"−2α
2α+p .
(7)"
THEORETICAL ANALYSIS,0.25215348472983556,"Hence, ˆτMRIV(x) outperforms ˆτW(x) asymptotically for γ > α, i.e., when the ITE τ(x) is smoother
289"
THEORETICAL ANALYSIS,0.2529365700861394,"than its components, which is usually the case in practice [25]. For γ = α, the rates of both estimators
290"
THEORETICAL ANALYSIS,0.25371965544244324,"coincide. Hence, we should expect MRIV to improve on the Wald estimator in real-world settings
291"
THEORETICAL ANALYSIS,0.2545027407987471,"with sufficiently large sample size.
292"
MRIV-NET,0.2552858261550509,"4.4
MRIV-Net
293"
MRIV-NET,0.25606891151135475,"Based on our MRIV framwork, we develop a tailored deep neural network called MRIV-Net for ITE
294"
MRIV-NET,0.2568519968676586,"estimation using IVs. Our MRIV-Net produces both an initial ITE estimator ˆτinit(x) and nuisance
295"
MRIV-NET,0.2576350822239624,"estimators ˆµY
0 (x), ˆµA
0 (x), ˆδA(x), and ˆπ(x).
296"
MRIV-NET,0.25841816758026626,"For MRIV-Net, we choose deep neural networks for the nuisance components due to their predictive
297"
MRIV-NET,0.2592012529365701,"power and their ability to learn complex shared representations for several nuisance components.
298"
MRIV-NET,0.25998433829287393,"Sharing representations between nuisance components has been exploited previously for ITE estima-
299"
MRIV-NET,0.26076742364917777,"tion, yet only under unconfoundedness [36, 15]. Building shared representations is more efficient in
300"
MRIV-NET,0.2615505090054816,"finite sample regimes than estimating all nuisance components separately as they usually share some
301"
MRIV-NET,0.26233359436178544,"common structure.
302"
MRIV-NET,0.2631166797180893,"In MRIV-Net, not all nuisance components should share a representation.
Recall that, in
303"
MRIV-NET,0.2638997650743931,"Theorem 2, we assumed that (1) ˆτinit(x), ˆµY
0 (x), and ˆµA
0 (x); and (2) ˆδA(x) and ˆπ(x)
304"
MRIV-NET,0.26468285043069695,"are trained on two independent samples in order to derive the upper bound on the oracle
305"
MRIV-NET,0.2654659357870008,"risk.
Hence, we propose to build two separate representations Φ1 and Φ2, so that (i) Φ1
306"
MRIV-NET,0.2662490211433046,"is used to learn ˆτinit(x), ˆµY
0 (x), and ˆµA
0 (x), and (ii) Φ2 is used to learn ˆδA(x) and ˆπ(x).
307 FF FF FF FF FF FF FF FF FF"
MRIV-NET,0.26703210649960846,"Figure 2: Architecture of
MRIV-Net."
MRIV-NET,0.2678151918559123,"This ensures that the nuisance estimators (1) share minimal information
308"
MRIV-NET,0.26859827721221613,"with nuisance estimators (2) even though they are estimated on the same
309"
MRIV-NET,0.26938136256851997,"data. Intuitively, this should lead to a faster decay of the oracle upper
310"
MRIV-NET,0.2701644479248238,"bound (cf. [15]).
311"
MRIV-NET,0.27094753328112764,"The architecture of MRIV-Net is shown in Fig. 2. MRIV-Net takes the
312"
MRIV-NET,0.2717306186374315,"observed covariates X as input to build the two representations Φ1 and
313"
MRIV-NET,0.2725137039937353,"Φ2. The first representation Φ1 is used to output estimates ˆµY
1 (x), ˆµY
0 (x),
314"
MRIV-NET,0.27329678935003915,"ˆµA
1 (x), and ˆµA
0 (x) of the ITE components. The second representation
315"
MRIV-NET,0.274079874706343,"Φ2 is used to output estimates eµA
1 (x), eµA
0 (x), and ˆπ(x). MRIV-Net is
316"
MRIV-NET,0.2748629600626468,"trained by minimizing an overall loss
317"
MRIV-NET,0.27564604541895066,"L(θ) = n
X i=1"
MRIV-NET,0.2764291307752545,"h 
ˆµY
zi(xi) −yi
2 + BCE
 
ˆµA
zi(xi), ai

+ BCE
 
eµA
zi(xi), ai

+ BCE (ˆπ(xi), zi)
i
,
(8)"
MRIV-NET,0.27721221613155833,"where θ denotes the neural network parameters and BCE is the binary cross entropy loss. After
318"
MRIV-NET,0.27799530148786217,"training MRIV-Net, we obtain the ˆτinit(x) = ˆµY
1 (x)−ˆµY
0 (x)
ˆµA
1 (x)−ˆµA
0 (x) and obtain the nuisance estimators ˆµY
0 (x),
319"
MRIV-NET,0.278778386844166,"ˆµA
0 (x), ˆδA(x) = eµA
1 (x) −eµA
0 (x) and ˆπ(x). Then, we perform, we perform the pseudo regression
320"
MRIV-NET,0.27956147220046984,"(Stage 2) of MRIV to obtain ˆτMRIV(x).
321"
MRIV-NET,0.2803445575567737,"Implementation: We use PyTorch Lightning for our implementation and train MRIV-Net with
322"
MRIV-NET,0.2811276429130775,"the Adam optimizer [24]. Details on the network architecture and hyperparameter tuning are in
323"
MRIV-NET,0.28191072826938135,"Appendix G. We perform both the training of MRIV-Net and the pseudo outcome regression on
324"
MRIV-NET,0.2826938136256852,"the full training data. Needless to say, MRIV-Net can be easily adopted for sample splitting or
325"
MRIV-NET,0.283476898981989,"cross-fitting procedures as in [10], namely, by learning separate networks for each representation
326"
MRIV-NET,0.28425998433829286,"Φ1 and Φ2. However, in our experiments, we do not use sample splitting or cross-fitting, as this can
327"
MRIV-NET,0.2850430696945967,"affect the performance in finite sample regimes. Of note, our choice is consistent with previous work
328"
MRIV-NET,0.28582615505090053,"[15].
329"
COMPUTATIONAL EXPERIMENTS,0.28660924040720437,"5
Computational experiments
330"
SIMULATED DATA,0.2873923257635082,"5.1
Simulated data
331"
SIMULATED DATA,0.28817541111981204,"In causal inference literature, it is common practice to use simulated data for performance evaluations
332"
SIMULATED DATA,0.2889584964761159,"[8, 15, 17]. Simulated data offers the crucial benefit that it provides ground-truth information on the
333"
SIMULATED DATA,0.2897415818324197,"counterfactual outcomes and thus allows for direct benchmarking against the oracle ITE.
334"
SIMULATED DATA,0.29052466718872355,"Data generation: We generate simulated data by sampling the oracle ITE τ(x) and the nuisance
335"
SIMULATED DATA,0.2913077525450274,"components µY
i (x), µA
i (x), and π(x) from Gaussian process priors. Using Gaussian processes has
336"
SIMULATED DATA,0.2920908379013312,"the following advantages: (1) It allows for a fair method comparison, as there is no need to explicitly
337"
SIMULATED DATA,0.29287392325763506,"specify the nuisance components, which could lead to unwanted inductive biases favoring a specific
338"
SIMULATED DATA,0.2936570086139389,"method; (2) the sampled nuisance components are non-linear and thus resemble real-world scenarios
339"
SIMULATED DATA,0.29444009397024273,"where machine learning methods would be applied; and, (3) by sampling from the prior induced by
340"
SIMULATED DATA,0.29522317932654657,"the Matérn kernel [32], we can control the smoothness of the nuisance components, which allows
341"
SIMULATED DATA,0.2960062646828504,"us to confirm our theoretical results from Sec. 4.2. For a detailed description of our data generating
342"
SIMULATED DATA,0.29678935003915424,"process, we refer to Appendix C.
343"
SIMULATED DATA,0.2975724353954581,"Baselines: We compare our MRIV-Net with the following state-of-the-art baselines: (1) ITE methods
344"
SIMULATED DATA,0.2983555207517619,"for unconfoundedness: TARNet [36] and TARNet combined with the DR-learner [22]; (2) general
345"
SIMULATED DATA,0.29913860610806575,"IV methods: 2SLS [48], kernel IV (KIV) [37], DFIV [50], DeepIV [17], DeepGMM [6], DMLIV
346"
SIMULATED DATA,0.2999216914643696,"[40], and DMLIV combined with DRIV (as described in [40]); (3) the (plug-in) Wald estimator using
347"
SIMULATED DATA,0.3007047768206735,"linear models and Bayesian additive regression trees (BART) [11]. Of note, the DR-learner assumes
348"
SIMULATED DATA,0.3014878621769773,"unconfoundedness, which is why we only combine it TARNet in our experiments. Implementation
349"
SIMULATED DATA,0.30227094753328115,"details regarding baselines and nuisance parameter estimation are in Appendix E. Note that many of
350"
SIMULATED DATA,0.303054032889585,"the baselines do not directly aim at ITE estimation but rather at counterfactual outcome prediction.
351"
SIMULATED DATA,0.3038371182458888,"We nevertheless use these methods as baselines and, for this, obtain the ITE by taking the difference
352"
SIMULATED DATA,0.30462020360219266,"between the predictions of the factual and counterfactual outcomes.
353"
SIMULATED DATA,0.3054032889584965,"Performance evaluation: For all experiments, we use a 80/20 split as training/test set. We calcalute
354"
SIMULATED DATA,0.30618637431480034,"the root mean squared errors (RMSE) between the ITE estimates and the oracle ITE on the test set.
355"
SIMULATED DATA,0.3069694596711042,"We report the mean RMSE and the standard deviation over five data sets generated from random
356"
SIMULATED DATA,0.307752545027408,"seeds.
357"
SIMULATED DATA,0.30853563038371185,"Table 2: Performance comparison: our MRIV-Net vs. ex-
isting baselines."
SIMULATED DATA,0.3093187157400157,"Method
n = 3000
n = 5000
n = 8000"
SIMULATED DATA,0.3101018010963195,"(1) STANDARD ITE
TARNet [36]
0.76 ± 0.14
0.70 ± 0.12
0.69 ± 0.17
TARNet + DR [36, 22]
0.78 ± 0.10
0.66 ± 0.09
0.70 ± 0.10"
SIMULATED DATA,0.31088488645262335,"(2) GENERAL IV
2SLS [47]
1.22 ± 0.23
0.79 ± 0.37
1.12 ± 0.29
KIV [37]
1.54 ± 0.53
1.18 ± 1.14
3.80 ± 4.71
DFIV [50]
0.43 ± 0.11
0.40 ± 0.21
0.46 ± 0.54
DeepIV [17]
0.96 ± 0.30
0.28 ± 0.09
0.23 ± 0.04
DeepGMM [6]
0.95 ± 0.38
0.37 ± 0.09
0.42 ± 0.14
DMLIV [40]
1.92 ± 0.71
0.92 ± 0.41
1.14 ± 0.24
DMLIV + DRIV [40]
0.41 ± 0.12
0.22 ± 0.04
0.21 ± 0.06"
SIMULATED DATA,0.3116679718089272,"(3) WALD ESTIMATOR [43]
Linear
1.06 ± 0.63
0.62 ± 0.22
0.81 ± 0.34
BART
0.95 ± 0.30
0.63 ± 0.33
0.88 ± 0.28"
SIMULATED DATA,0.312451057165231,"MRIV-Net (ours)
0.26 ± 0.11
0.15 ± 0.03
0.13 ± 0.03"
SIMULATED DATA,0.31323414252153486,Reported: RMSE for base methods (mean ± standard deviation). Lower = better (best in bold)
SIMULATED DATA,0.3140172278778387,"Results: Table 2 shows the results for
358"
SIMULATED DATA,0.31480031323414254,"all baselines. Here, the DR-learner does
359"
SIMULATED DATA,0.3155833985904464,"not improve the performance of TAR-
360"
SIMULATED DATA,0.3163664839467502,"Net, which is reasonable as both the
361"
SIMULATED DATA,0.31714956930305405,"DR-learner and TARNet assume uncon-
362"
SIMULATED DATA,0.3179326546593579,"foundedness and are thus biased in our
363"
SIMULATED DATA,0.3187157400156617,"setting. Our MRIV-Net outperforms all
364"
SIMULATED DATA,0.31949882537196556,"baselines. Our MRIV-Net also achieves
365"
SIMULATED DATA,0.3202819107282694,"a smaller standard deviation. For addi-
366"
SIMULATED DATA,0.32106499608457323,"tional results, we refer to Appendix H.
367"
SIMULATED DATA,0.32184808144087707,"We further compare the performance of
368"
SIMULATED DATA,0.3226311667971809,"two different meta-learner frameworks –
369"
SIMULATED DATA,0.32341425215348474,"DRIV [40] and our MRIV– across differ-
370"
SIMULATED DATA,0.3241973375097886,"ent base methods. The nuisance param-
371"
SIMULATED DATA,0.3249804228660924,"eters are estimated using feed forward neural networks (DRIV) or TARNets with either binary or
372"
SIMULATED DATA,0.32576350822239625,"continuous outputs (MRIV). The results are in Table 3. Our MRIV improves over the variant without
373"
SIMULATED DATA,0.3265465935787001,"any meta-learner framework across all base methods (both in terms of RMSE and standard deviation).
374"
SIMULATED DATA,0.3273296789350039,"Table 3: Base model with different meta-learners (i.e., none, DRIV, and our MRIV)."
SIMULATED DATA,0.32811276429130776,"n = 3000
n = 5000
n = 8000
hhhhhhhhhhhhhh
Base methods
Meta-learners
None
DRIV
MRIV (ours)
None
DRIV
MRIV (ours)
None
DRIV
MRIV (ours)"
SIMULATED DATA,0.3288958496476116,"(1) STANDARD ITE
TARNet [36]
0.76 ± 0.14
0.31 ± 0.05
0.34 ± 0.13
0.70 ± 0.12
0.17 ± 0.06
0.17 ± 0.05
0.69 ± 0.17
0.21 ± 0.04
0.16 ± 0.04"
SIMULATED DATA,0.32967893500391543,"(2) GENERAL IV
2SLS [47]
1.22 ± 0.23
0.40 ± 0.11
0.31 ± 0.08
0.79 ± 0.37
0.17 ± 0.09
0.19 ± 0.05
1.12 ± 0.29
0.21 ± 0.05
0.16 ± 0.02
KIV [37]
1.54 ± 0.53
0.40 ± 0.10
0.39 ± 0.11
1.18 ± 1.14
0.20 ± 0.08
0.17 ± 0.06
3.80 ± 4.71
0.31 ± 0.18
0.28 ± 0.19
DFIV [50]
0.43 ± 0.11
0.26 ± 0.05
0.27 ± 0.07
0.40 ± 0.21
0.18 ± 0.09
0.16 ± 0.04
0.46 ± 0.54
0.21 ± 0.06
0.18 ± 0.05
DeepIV [17]
0.96 ± 0.30
0.27 ± 0.03
0.26 ± 0.05
0.28 ± 0.09
0.18 ± 0.08
0.18 ± 0.05
0.23 ± 0.04
0.21 ± 0.03
0.16 ± 0.03
DeepGMM [6]
0.95 ± 0.38
0.40 ± 0.15
0.36 ± 0.13
0.37 ± 0.09
0.24 ± 0.12
0.16 ± 0.05
0.42 ± 0.14
0.21 ± 0.03
0.17 ± 0.03
DMLIV [40]
1.92 ± 0.71
0.41 ± 0.12
0.37 ± 0.11
0.92 ± 0.41
0.22 ± 0.05
0.16 ± 0.05
1.14 ± 0.24
0.21 ± 0.06
0.18 ± 0.05"
SIMULATED DATA,0.33046202036021927,"(3) WALD ESTIMATOR [43]
Linear
1.06 ± 0.63
0.42 ± 0.15
0.38 ± 0.14
0.62 ± 0.22
0.19 ± 0.09
0.25 ± 0.09
0.81 ± 0.34
0.19 ± 0.09
0.18 ± 0.04
BART
0.95 ± 0.30
0.48 ± 0.14
0.46 ± 0.12
0.63 ± 0.33
0.26 ± 0.13
0.20 ± 0.07
0.88 ± 0.28
0.31 ± 0.08
0.29 ± 0.04"
SIMULATED DATA,0.3312451057165231,"MRIV-Net\w network only (ours)
0.39 ± 0.13
0.35 ± 0.12
0.26 ± 0.11
0.31 ± 0.04
0.19 ± 0.13
0.15 ± 0.03
0.26 ± 0.06
0.18 ± 0.08
0.13 ± 0.03
Reported: RMSE (mean ± standard deviation). Lower = better (best improvement over none meta-learner in bold)"
SIMULATED DATA,0.33202819107282694,Table 4: Ablation study.
SIMULATED DATA,0.3328112764291308,"Method
n = 3000
n = 5000
n = 8000"
SIMULATED DATA,0.3335943617854346,"MRIV-Net\w network only
0.39 ± 0.13
0.31 ± 0.04
0.26 ± 0.06
MRIV-Net\w single repr.
0.28 ± 0.12
0.21 ± 0.04
0.32 ± 0.10
MRIV-Net (ours)
0.26 ± 0.11
0.15 ± 0.03
0.13 ± 0.03"
SIMULATED DATA,0.33437744714173845,Reported: RMSE (mean ± standard deviation). Lower = better (best in bold)
SIMULATED DATA,0.3351605324980423,"Furthermore, MRIV is clearly superior
375"
SIMULATED DATA,0.3359436178543461,"over DRIV. This demonstrates the effec-
376"
SIMULATED DATA,0.33672670321064996,"tiveness of our MRIV across different
377"
SIMULATED DATA,0.3375097885669538,"base methods (note: MRIV with an ar-
378"
SIMULATED DATA,0.33829287392325763,"bitrary base model is typically superior
379"
SIMULATED DATA,0.33907595927956147,"to DRIV with our custom network from
380"
SIMULATED DATA,0.3398590446358653,"above). MRIV-Net is overall best. We
381"
SIMULATED DATA,0.34064212999216914,"also performed additional experiments where we used cross-fitting approaches for both meta-learners
382"
SIMULATED DATA,0.341425215348473,"(see Appendix I).
383"
SIMULATED DATA,0.3422083007047768,"Ablation study: Table 4 compares different variants of our MRIV-Net. These are: (1) MRIV but
384"
SIMULATED DATA,0.34299138606108065,"network only; (2) MRIV-Net with a single representation for all nuisance estimators; and (3) our
385"
SIMULATED DATA,0.3437744714173845,"MRIV-Net from above. We observe that MRIV-Net is best. This justifies our proposed network
386"
SIMULATED DATA,0.3445575567736883,"architecture for MRIV-Net. Hence, combing the result from above, our performance gain must be
387"
SIMULATED DATA,0.34534064212999216,"attributed to both our framework and the architecture of our deep neural network.
388"
SIMULATED DATA,0.346123727486296,"1
2
3
4
5
Confounding level 
U 0.0 0.2 0.4 0.6 0.8 1.0 RMSE"
SIMULATED DATA,0.34690681284259983,n = 3000
SIMULATED DATA,0.34768989819890367,"1
2
3
4
5
Confounding level 
U RMSE"
SIMULATED DATA,0.3484729835552075,n = 5000
SIMULATED DATA,0.34925606891151134,"1
2
3
4
5
Confounding level 
U RMSE"
SIMULATED DATA,0.3500391542678152,n = 8000
SIMULATED DATA,0.350822239624119,"Method
TARNet
TARNet + DR
MRIV-Net\w netowrk only
MRIV-Net (ours)"
SIMULATED DATA,0.35160532498042285,"Figure 3: Results over different levels of confounding αU. Shaded
area shows standard deviation."
SIMULATED DATA,0.3523884103367267,"Robustness
checks
for
389"
SIMULATED DATA,0.3531714956930305,"unobserved
confounding
390"
SIMULATED DATA,0.35395458104933436,"and smoothness: Here, we
391"
SIMULATED DATA,0.3547376664056382,"demonstrate the importance
392"
SIMULATED DATA,0.35552075176194203,"of
handling
unobserved
393"
SIMULATED DATA,0.35630383711824587,"confounding (as we do in our
394"
SIMULATED DATA,0.3570869224745497,"MRIV framework). For this,
395"
SIMULATED DATA,0.35787000783085354,"Fig. 3 plots the results for
396"
SIMULATED DATA,0.3586530931871574,"our MRIV-Net vs. standard
397"
SIMULATED DATA,0.3594361785434612,"ITE without customization
398"
SIMULATED DATA,0.36021926389976505,"for confounding (i.e., TARNet with and without the DR-learner) over over different levels of
399"
SIMULATED DATA,0.3610023492560689,"unobserved confounding.
The RMSE of both TARNet variants increase almost linearly with
400"
SIMULATED DATA,0.3617854346123727,"increasing confounding. In contrast, the RMSE of our MRIV-Net only marginally. Even for low
401"
SIMULATED DATA,0.36256851996867656,"confounding regimes, our MRIV-Net performs competitively.
402"
SIMULATED DATA,0.3633516053249804,"Fig. 4 varies the smoothness level. This is given by α of µY
i (·) (controlled by the Matérn kernel
403"
SIMULATED DATA,0.36413469068128423,"prior). Here, the performance decreases for the baselines, i.e., DeepIV and our network without
404"
SIMULATED DATA,0.36491777603758807,"MRIV framework. In contrast, the peformance of our MRIV-Net remains robust and outperforms
405"
SIMULATED DATA,0.3657008613938919,"the baselines. This confirms our theoretical results from above. It thus indicates that our MRIV
406"
SIMULATED DATA,0.36648394675019574,"framework works best when the oracle ITE τ(x) is smoother than the nuisance parameters µY
i (x).
407"
SIMULATED DATA,0.36726703210649964,"5
10
Smoothness 0.10 0.15 0.20 0.25 0.30 0.35 RMSE"
SIMULATED DATA,0.36805011746280347,"Method
DeepIV
MRIV-Net\w netowrk only
MRIV-Net (ours)"
SIMULATED DATA,0.3688332028191073,"Figure 4: Results over different lev-
els of smoothness α of µY
i (·), sam-
ple size n = 8000. Larger α =
smoother. Shaded areas show stan-
dard deviation. 408"
CASE STUDY WITH REAL-WORLD DATA,0.36961628817541115,"5.2
Case study with real-world data
409"
CASE STUDY WITH REAL-WORLD DATA,0.370399373531715,"Setting: We demonstrate effectiveness of our framework using
410"
CASE STUDY WITH REAL-WORLD DATA,0.3711824588880188,"a case study with real-world, medical data. Here, we use medi-
411"
CASE STUDY WITH REAL-WORLD DATA,0.37196554424432265,"cal data from the so-called Oregon health insurance experiment
412"
CASE STUDY WITH REAL-WORLD DATA,0.3727486296006265,"(OHIE) [16]. It provides data for an RCT with non-compliance:
413"
CASE STUDY WITH REAL-WORLD DATA,0.3735317149569303,"In 2008, ∼30,000 low-income, uninsured adults in Oregon were
414"
CASE STUDY WITH REAL-WORLD DATA,0.37431480031323416,"offered participation in a health insurance program by a lottery.
415"
CASE STUDY WITH REAL-WORLD DATA,0.375097885669538,"Individuals whose names were drawn could decide to sign up
416"
CASE STUDY WITH REAL-WORLD DATA,0.37588097102584184,"for health insurance. After a period of 12 months, in-person
417"
CASE STUDY WITH REAL-WORLD DATA,0.3766640563821457,"interviews took place to evaluate the health condition of the
418"
CASE STUDY WITH REAL-WORLD DATA,0.3774471417384495,"respective participant.
419"
CASE STUDY WITH REAL-WORLD DATA,0.37823022709475335,"In our analysis, the lottery assignment is the instrument Z, the decision to sign up for health insurance
420"
CASE STUDY WITH REAL-WORLD DATA,0.3790133124510572,"is treatment A, and an overall health score is the outcome Y . We also include five covariates X (age,
421"
CASE STUDY WITH REAL-WORLD DATA,0.379796397807361,"gender, language, the number of emergency visits before the experiment, and the number of people
422"
CASE STUDY WITH REAL-WORLD DATA,0.38057948316366486,"the individual signed up with). It is important to include the latter in our analysis as it is the only
423"
CASE STUDY WITH REAL-WORLD DATA,0.3813625685199687,"variable influencing the propensity score. For details, we refer to Appendix D. We first estimate the
424"
CASE STUDY WITH REAL-WORLD DATA,0.38214565387627253,"ITE function and then report the treatment effect heterogeneity w.r.t. age and gender, while fixing
425"
CASE STUDY WITH REAL-WORLD DATA,0.38292873923257637,"the other covariates (i.e., we consider the English-speaking subpopulation with one emergency visit
426"
CASE STUDY WITH REAL-WORLD DATA,0.3837118245888802,"that signed up alone). We repeat the same procedure for our neural network architecture without the
427"
CASE STUDY WITH REAL-WORLD DATA,0.38449490994518404,"MRIV-Net framework and TARNet. The results are in Fig. 5.
428"
CASE STUDY WITH REAL-WORLD DATA,0.3852779953014879,"Results:
Our
MRIV-Net
estimates
larger
causal
effects
for
an
older
age.
In
429"
CASE STUDY WITH REAL-WORLD DATA,0.3860610806577917,"contrast,
TARNet
does
not
estimate
positive
ITEs
even
for
an
older
age.
430"
CASE STUDY WITH REAL-WORLD DATA,0.38684416601409555,"20
40
60
Age 0.2 0.0 0.2 0.4"
CASE STUDY WITH REAL-WORLD DATA,0.3876272513703994,Estimated ITE
CASE STUDY WITH REAL-WORLD DATA,0.3884103367267032,gender = male
CASE STUDY WITH REAL-WORLD DATA,0.38919342208300706,"20
40
60
Age"
CASE STUDY WITH REAL-WORLD DATA,0.3899765074393109,gender = female
CASE STUDY WITH REAL-WORLD DATA,0.39075959279561473,"Method
TARNet
MRIV-Net\w netowrk only
MRIV-Net (ours)
DMLIV + DRIV"
CASE STUDY WITH REAL-WORLD DATA,0.39154267815191857,Figure 5: Results on real-world medical data.
CASE STUDY WITH REAL-WORLD DATA,0.3923257635082224,"Even though we cannot evaluate the estimation
431"
CASE STUDY WITH REAL-WORLD DATA,0.39310884886452624,"quality on real-world data, our estimates seem
432"
CASE STUDY WITH REAL-WORLD DATA,0.3938919342208301,"reasonable in light of the medical literature: the
433"
CASE STUDY WITH REAL-WORLD DATA,0.3946750195771339,"benefit of health insurance should increase with
434"
CASE STUDY WITH REAL-WORLD DATA,0.39545810493343775,"older age. This showcases that TARNet may
435"
CASE STUDY WITH REAL-WORLD DATA,0.3962411902897416,"suffer from bias induced by unobserved con-
436"
CASE STUDY WITH REAL-WORLD DATA,0.3970242756460454,"founders. We also report the results for DRIV
437"
CASE STUDY WITH REAL-WORLD DATA,0.39780736100234926,"with DMLIV as base method, and observe that
438"
CASE STUDY WITH REAL-WORLD DATA,0.3985904463586531,"in contrast to MRIV-Net, the corresponding ITE
439"
CASE STUDY WITH REAL-WORLD DATA,0.39937353171495693,"does not vary much between ages. Interestingly,
440"
CASE STUDY WITH REAL-WORLD DATA,0.40015661707126077,"both our MRIV-Net estimate a somewhat smaller ITE for middle ages (around 30–50 yrs). One
441"
CASE STUDY WITH REAL-WORLD DATA,0.4009397024275646,"explanation might be that individual in this age group are more likely to have stable jobs and, thus, are
442"
CASE STUDY WITH REAL-WORLD DATA,0.40172278778386844,"also more likely to be able to afford medical care, decreasing the direct effect of health insurance on
443"
CASE STUDY WITH REAL-WORLD DATA,0.4025058731401723,"individuals health. In sum, the findings from our case study are of direct relevance for decision-makers
444"
CASE STUDY WITH REAL-WORLD DATA,0.4032889584964761,"in public health [19], and highlight the practical value of our framework.
445"
CONCLUSION,0.40407204385277995,"6
Conclusion
446"
CONCLUSION,0.4048551292090838,"In this paper, we propose MRIV-Net: a novel ITE estimator based on a deep neural network.
447"
CONCLUSION,0.4056382145653876,"Importantly, our estimator is consistent in the union of three models specifications and, therefore,
448"
CONCLUSION,0.40642129992169146,"multiply robust. This is a crucial difference to existing works: previously, existing ITE estimators
449"
CONCLUSION,0.4072043852779953,"(such es DRIV from Syrgkanis et al. [40]) were only doubly robust. We show both theoretically and
450"
CONCLUSION,0.40798747063429913,"empirically that MRIV-Net is state-of-the-art for estimating ITEs using binary IVs. For future work,
451"
CONCLUSION,0.40877055599060297,"it would be interesting to derive finite sample results for MRIV-Net, because our theoretical analysis
452"
CONCLUSION,0.4095536413469068,"is purely asymptotic. Furthermore, one could develop multiply robust estimators for other IV settings
453"
CONCLUSION,0.41033672670321064,"(e.g., multiple or continuous instruments and treatments).
454"
REFERENCES,0.4111198120595145,"References
455"
REFERENCES,0.4119028974158183,"[1]
Ahmed M. Alaa and Mihaela van der Schaar. “Bayesian inference of individualized treatment
456"
REFERENCES,0.41268598277212215,"effects using multi-task Gaussian processes”. In: NeurIPS. 2017.
457"
REFERENCES,0.413469068128426,"[2]
Joshua D. Angrist. “Lifetime earnings and the vietnam era draft lotter: Evidence from social
458"
REFERENCES,0.4142521534847298,"security administrative records”. In: The American Economic Review 80.3 (1990), pp. 313–336.
459"
REFERENCES,0.41503523884103366,"[3]
Joshua D. Angrist, Guido W. Imbens, and Donald B. Rubin. “Identification of causal effects
460"
REFERENCES,0.4158183241973375,"using instrumental variables”. In: Journal of the American Statistical Association 91.434
461"
REFERENCES,0.41660140955364133,"(1996), pp. 444–455.
462"
REFERENCES,0.41738449490994517,"[4]
Joshua D. Angrist and Alan B. Krueger. “Does compulsory school attendance affect schooling
463"
REFERENCES,0.418167580266249,"and earnings?” In: The Quarterly Journal of Economics 106.4 (1991), pp. 979–1014.
464"
REFERENCES,0.41895066562255284,"[5]
Falco J. Bargagli-Stoffi, Kristof de Witte, and Giorgio Gnecco. “Heterogeneous causal effects
465"
REFERENCES,0.4197337509788567,"with imperfect compliance: A Bayesian machine learning approach”. In: Annals of Applied
466"
REFERENCES,0.4205168363351605,"Statistics (2021).
467"
REFERENCES,0.42129992169146435,"[6]
Andrew Bennett, Nathan Kallus, and Tobias Schnabel. “Deep generalized method of moments
468"
REFERENCES,0.4220830070477682,"for instrumental variable analysis”. In: NeurIPS. 2019.
469"
REFERENCES,0.422866092404072,"[7]
Ioana Bica, Ahmed M. Alaa, and Mihaela van der Schaar. “Time series deconfounder: Esti-
470"
REFERENCES,0.42364917776037586,"mating treatment effects over time in the presence of hidden confounders”. In: ICML. 2020.
471"
REFERENCES,0.4244322631166797,"[8]
Ioana Bica et al. “Estimating counterfactual treatment outcomes over time through adversarially
472"
REFERENCES,0.42521534847298353,"balanced representations”. In: ICLR. 2020.
473"
REFERENCES,0.42599843382928737,"[9]
Howard S. Bloom et al. “The benefits and costs of JTPA title II-A programs: Key Findings
474"
REFERENCES,0.4267815191855912,"from the National Job Training Partnership Act Study”. In: Journal of Human Resources 32.32
475"
REFERENCES,0.42756460454189504,"(1997), pp. 549–586.
476"
REFERENCES,0.4283476898981989,"[10]
Victor Chernozhukov et al. “Double/debiased machine learning for treatment and structural
477"
REFERENCES,0.4291307752545027,"parameters”. In: The Econometrics Journal 21.1 (2018), pp. C1–C68.
478"
REFERENCES,0.42991386061080655,"[11]
Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. “BART: Bayesian additive
479"
REFERENCES,0.4306969459671104,"regression trees”. In: The Annals of Applied Statistics 4.1 (2010), pp. 266–298.
480"
REFERENCES,0.4314800313234142,"[12]
Yifan Cui and Eric Tchetgen Tchetgen. “A semiparametric instrumental variable approach
481"
REFERENCES,0.43226311667971806,"to optimal treatment regimes under endogeneity”. In: Journal of the American Statistical
482"
REFERENCES,0.4330462020360219,"Association 116.553 (2021), pp. 126–137.
483"
REFERENCES,0.4338292873923258,"[13]
Yifan Cui et al. “Semiparametric proximal causal inference”. In: arXiv preprint (2020).
484"
REFERENCES,0.4346123727486296,"[14]
Alicia Curth, Ahmed M. Alaa, and Mihaela van der Schaar. “Estimating structural target
485"
REFERENCES,0.43539545810493346,"functions using machine learning and influence functions”. In: arXiv preprint (2020).
486"
REFERENCES,0.4361785434612373,"[15]
Alicia Curth and Mihaela van der Schaar. “Nonparametric estimation of heterogeneous treat-
487"
REFERENCES,0.43696162881754114,"ment effects: From theory to learning Algorithms”. In: AISTATS. 2021.
488"
REFERENCES,0.437744714173845,"[16]
Amy Finkelstein et al. “The oregon health insurance experiment: Evidence from the first year”.
489"
REFERENCES,0.4385277995301488,"In: The Quarterly Journal of Economics 127.3 (2012), pp. 1057–1106.
490"
REFERENCES,0.43931088488645265,"[17]
Jason Hartford et al. “Deep IV: A flexible approach for counterfactual prediction”. In: ICML.
491"
REFERENCES,0.4400939702427565,"2017.
492"
REFERENCES,0.4408770555990603,"[18]
Tobias Hatt and Stefan Feuerriegel. “Sequential deconfounding for causal inference with
493"
REFERENCES,0.44166014095536416,"unobserved confounders”. In: arXiv preprint (2021).
494"
REFERENCES,0.442443226311668,"[19]
Guido W. Imbens and Joshua D. Angrist. “Identification and estimation of local average
495"
REFERENCES,0.44322631166797183,"treatment effects”. In: Econometrica 62.2 (1994), pp. 467–475.
496"
REFERENCES,0.44400939702427566,"[20]
Andrew Jesson et al. “Quantifying ignorance in individual-level causal-effect estimates under
497"
REFERENCES,0.4447924823805795,"hidden confounding”. In: ICML. 2021.
498"
REFERENCES,0.44557556773688334,"[21]
Nathan Kallus, Xiaojie Mao, and Angela Zhou. “Interval estimation of individual-level causal
499"
REFERENCES,0.4463586530931872,"effects under unobserved confounding”. In: AISTATS. 2019.
500"
REFERENCES,0.447141738449491,"[22]
Edward H. Kennedy. “Optimal doubly robust estimation of heterogeneous causal effects”. In:
501"
REFERENCES,0.44792482380579485,"arXiv preprint (2020).
502"
REFERENCES,0.4487079091620987,"[23]
Edward H. Kennedy, Scott A. Lorch, and Dylan S. Small. “Robust causal inference with
503"
REFERENCES,0.4494909945184025,"continuous instruments using the local instrumental variable curve”. In: Journal of the Royal
504"
REFERENCES,0.45027407987470636,"Statistical Society: Series B 81.1 (2019), pp. 121–143.
505"
REFERENCES,0.4510571652310102,"[24]
Diederik P. Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: ICLR.
506"
REFERENCES,0.45184025058731403,"2015.
507"
REFERENCES,0.45262333594361787,"[25]
Sören R. Künzel et al. “Metalearners for estimating heterogeneous treatment effects using
508"
REFERENCES,0.4534064212999217,"machine learning”. In: Proceedings of the National Academy of Sciences (PNAS) 116.10
509"
REFERENCES,0.45418950665622554,"(2019), pp. 4156–4165.
510"
REFERENCES,0.4549725920125294,"[26]
Chunxiao Li, Cynthia Rudin, and Typer H. McCormick. “Rethinking nonlinear instrumental
511"
REFERENCES,0.4557556773688332,"variable models through prediction validity”. In: Journal of Machine Learning Research 23
512"
REFERENCES,0.45653876272513705,"(2022), pp. 1–55.
513"
REFERENCES,0.4573218480814409,"[27]
Bryan Lim, Ahmed M. Alaa, and Mihaela van der Schaar. “Forecasting treatment responses
514"
REFERENCES,0.4581049334377447,"over time using recurrent marginal structural networks”. In: NeurIPS. 2018.
515"
REFERENCES,0.45888801879404856,"[28]
Whitney K. Newey and James L. Powell. “Instrumental variable estimation of nonparametric
516"
REFERENCES,0.4596711041503524,"models”. In: Econometrica 71.5 (2003), pp. 1565–1578.
517"
REFERENCES,0.46045418950665623,"[29]
Elizabeth L. Ogburn, Andrea Rotnitzky, and James M. Robins. “Doubly robust estimation of
518"
REFERENCES,0.46123727486296007,"the local average treatment effect curve”. In: Journal of the Royal Statistical Society: Series B
519"
REFERENCES,0.4620203602192639,"77.2 (2015), pp. 373–396.
520"
REFERENCES,0.46280344557556774,"[30]
Ryo Okui et al. “Doubly robust instrumental variable regression”. In: Statistica Sinica 22.1
521"
REFERENCES,0.4635865309318716,"(2012), pp. 173–205.
522"
REFERENCES,0.4643696162881754,"[31]
Judea Pearl. Causality. New York City: Cambridge University Press, 2009.
523"
REFERENCES,0.46515270164447925,"[32]
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine
524"
REFERENCES,0.4659357870007831,"learning. 3. print. Adaptive computation and machine learning. Cambridge, Mass.: MIT Press,
525"
REFERENCES,0.4667188723570869,"2008.
526"
REFERENCES,0.46750195771339076,"[33]
James M. Robins, Miguel A. Hernán, and Babette Brumback. “Marginal structural models and
527"
REFERENCES,0.4682850430696946,"causal inference in epidemiology”. In: Epidemiology 11.5 (2000), pp. 550–560.
528"
REFERENCES,0.46906812842599843,"[34]
Donald B. Rubin. “Estimating causal effects of treatments in randomized and nonrandomized
529"
REFERENCES,0.46985121378230227,"studies”. In: Journal of Educational Psychology 66.5 (1974), pp. 688–701.
530"
REFERENCES,0.4706342991386061,"[35]
Vira Semenova and Victor Chernozhukov. “Debiased machine learning of conditional average
531"
REFERENCES,0.47141738449490994,"treatment effects and other causal functions”. In: The Econometrics Journal 24.2 (2021),
532"
REFERENCES,0.4722004698512138,"pp. 264–289.
533"
REFERENCES,0.4729835552075176,"[36]
Uri Shalit, Fredrik D. Johansson, and David Sontag. “Estimating individual treatment effect:
534"
REFERENCES,0.47376664056382145,"Generalization bounds and algorithms”. In: ICML. 2017.
535"
REFERENCES,0.4745497259201253,"[37]
Rahul Singh, Maneesh Sahani, and Arthur Gretton. “Kernel instrumental variable regression”.
536"
REFERENCES,0.4753328112764291,"In: NeurIPS. 2019.
537"
REFERENCES,0.47611589663273296,"[38]
Rahul Singh and Liyang Sun. “Double robustness for complier parameters and a semiparamet-
538"
REFERENCES,0.4768989819890368,"ric test for complier characteristics”. In: arXiv preprint ().
539"
REFERENCES,0.47768206734534063,"[39]
Charles J. Stone. “Optimal rates of convergence for nonparametric estimators”. In: Annals of
540"
REFERENCES,0.47846515270164447,"Statistics 8.6 (1980).
541"
REFERENCES,0.4792482380579483,"[40]
Vasilis Syrgkanis et al. “Machine learning estimation of heterogeneous treatment effects with
542"
REFERENCES,0.48003132341425214,"instruments”. In: NeurIPS. 2019.
543"
REFERENCES,0.480814408770556,"[41]
Hal R. Varian. “Causal inference in economics and marketing”. In: Proceedings of the National
544"
REFERENCES,0.4815974941268598,"Academy of Sciences (PNAS) 113.27 (2016), pp. 7310–7315.
545"
REFERENCES,0.48238057948316365,"[42]
Stefan Wager and Susan Athey. “Estimation and inference of heterogeneous treatment effects
546"
REFERENCES,0.4831636648394675,"using random forests”. In: Journal of the American Statistical Association 113.523 (2018),
547"
REFERENCES,0.4839467501957713,"pp. 1228–1242.
548"
REFERENCES,0.48472983555207516,"[43]
Abraham Wald. “The fitting of straight lines if both variables are subject to error”. In: Annals
549"
REFERENCES,0.485512920908379,"of Mathematical Statistics 11.3 (1940), pp. 284–300.
550"
REFERENCES,0.48629600626468283,"[44]
Guihua Wang, Jun Li, and Wallace J. Hopp. “An instrumental variable forest approach for
551"
REFERENCES,0.48707909162098667,"detecting heterogeneous treatment effects in observational studies”. In: Management Science
552"
REFERENCES,0.4878621769772905,"(2021).
553"
REFERENCES,0.48864526233359434,"[45]
Linbo Wang and Eric J. Tchetgen Tchetgen. “Bounded, efficient and multiply robust estimation
554"
REFERENCES,0.4894283476898982,"of average treatment effects using instrumental variables”. In: Journal of the Royal Statistical
555"
REFERENCES,0.490211433046202,"Society: Series B 80.3 (2018), pp. 531–550.
556"
REFERENCES,0.49099451840250585,"[46]
Yixin Wang and David M. Blei. “The blessings of multiple causes”. In: Journal of the American
557"
REFERENCES,0.4917776037588097,"Statistical Association 114.528 (2019), pp. 1574–1596.
558"
REFERENCES,0.4925606891151135,"[47]
Jeffrey M. Wooldridge. Introductory Econometrics: A modern approach. Routledge, 2013.
559"
REFERENCES,0.49334377447141736,"[48]
Phillip G. Wright. The tariff on animal and vegitable oils. New York: Macmillan, 1928.
560"
REFERENCES,0.4941268598277212,"[49]
Liyuan Xu, Heishiro Kanagawa, and Arthur Gretton. “Deep proxy causal learning and its
561"
REFERENCES,0.49490994518402504,"application to confounded bandid policy evaluation”. In: NeurIPS. 2021.
562"
REFERENCES,0.49569303054032887,"[50]
Liyuan Xu et al. “Learning deep features in instrumental variable regression”. In: ICLR. 2021.
563"
REFERENCES,0.4964761158966327,"[51]
Azam M. Yazdani and Eric Boerwinkle. “Causal inference in the age of decision medicine”.
564"
REFERENCES,0.49725920125293654,"In: Journal of Data Mining in Genomics & Proteomics 6.1 (2015).
565"
REFERENCES,0.4980422866092404,"[52]
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. “GANITE: Estimation of individu-
566"
REFERENCES,0.4988253719655442,"alized treatment effects using generative adversarial nets”. In: ICLR. 2018.
567"
REFERENCES,0.49960845732184805,"[53]
Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. “Learning overlapping representations
568"
REFERENCES,0.5003915426781519,"for the estimation of individualized treatment effects”. In: AISTATS. 2020.
569"
REFERENCES,0.5011746280344558,"Checklist
570"
REFERENCES,0.5019577133907596,"1. For all authors...
571"
REFERENCES,0.5027407987470635,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
572"
REFERENCES,0.5035238841033672,"contributions and scope? [Yes] See here
573"
REFERENCES,0.5043069694596711,"(b) Did you describe the limitations of your work? [Yes] See Sec. 5.2.
574"
REFERENCES,0.5050900548159749,"(c) Did you discuss any potential negative societal impacts of your work? [No]
575"
REFERENCES,0.5058731401722788,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
576"
REFERENCES,0.5066562255285826,"them? [Yes]
577"
REFERENCES,0.5074393108848865,"2. If you are including theoretical results...
578"
REFERENCES,0.5082223962411903,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Sec. 4.2
579"
REFERENCES,0.5090054815974941,"and Appendix A..
580"
REFERENCES,0.5097885669537979,"(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix A.
581"
REFERENCES,0.5105716523101018,"3. If you ran experiments...
582"
REFERENCES,0.5113547376664056,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
583"
REFERENCES,0.5121378230227095,"mental results (either in the supplemental material or as a URL)? [Yes] (both).
584"
REFERENCES,0.5129209083790133,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
585"
REFERENCES,0.5137039937353172,"were chosen)? [Yes] See Appendix E and G
586"
REFERENCES,0.514487079091621,"(c) Did you report error bars (e.g., with respect to the random seed after running ex-
587"
REFERENCES,0.5152701644479248,"periments multiple times)? [Yes] One standard deviation using 5 random seeds, see
588"
REFERENCES,0.5160532498042286,"Sec. 5.
589"
REFERENCES,0.5168363351605325,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
590"
REFERENCES,0.5176194205168363,"of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E.
591"
REFERENCES,0.5184025058731402,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
592"
REFERENCES,0.519185591229444,"(a) If your work uses existing assets, did you cite the creators? [Yes] See Sec. 5.2.
593"
REFERENCES,0.5199686765857479,"(b) Did you mention the license of the assets? [No] No license is provided on the OHIE
594"
REFERENCES,0.5207517619420516,"website.
595"
REFERENCES,0.5215348472983555,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
596"
REFERENCES,0.5223179326546593,"See Appendix D.
597"
REFERENCES,0.5231010180109632,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
598"
REFERENCES,0.523884103367267,"using/curating? [No] We only used simulated and publicly available data.
599"
REFERENCES,0.5246671887235709,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
600"
REFERENCES,0.5254502740798747,"information or offensive content? [No] The OHIE data is publicly available, personally
601"
REFERENCES,0.5262333594361785,"identifiable information are censored.
602"
REFERENCES,0.5270164447924823,"5. If you used crowdsourcing or conducted research with human subjects...
603"
REFERENCES,0.5277995301487862,"(a) Did you include the full text of instructions given to participants and screenshots, if
604"
REFERENCES,0.52858261550509,"applicable? [No] Not applicable.
605"
REFERENCES,0.5293657008613939,"(b) Did you describe any potential participant risks, with links to Institutional Review
606"
REFERENCES,0.5301487862176977,"Board (IRB) approvals, if applicable? [No] Not applicable.
607"
REFERENCES,0.5309318715740016,"(c) Did you include the estimated hourly wage paid to participants and the total amount
608"
REFERENCES,0.5317149569303053,"spent on participant compensation? [No] Not applicable.
609"
REFERENCES,0.5324980422866092,"Estimating individual treatment effects under
unobserved confounding using binary instruments
Appendix"
REFERENCES,0.533281127642913,"Anonymous Author(s)
Affiliation
Address
email"
REFERENCES,0.5340642129992169,"Contents
1"
REFERENCES,0.5348472983555208,"A Proofs
2
2"
REFERENCES,0.5356303837118246,"A.1
Proof of Theorem 1 (multiple robustness property)
. . . . . . . . . . . . . . . . .
2
3"
REFERENCES,0.5364134690681285,"A.2
Proof of Theorem 2 (Convergence rate of MRIV) . . . . . . . . . . . . . . . . . .
3
4"
REFERENCES,0.5371965544244323,"A.3
Proof of Theorem 3 (Convergence rate of the Wald estimator) . . . . . . . . . . . .
4
5"
REFERENCES,0.5379796397807362,"B
Theoretical analysis under sparsity assumptions
5
6"
REFERENCES,0.5387627251370399,"C Simulated data
6
7"
REFERENCES,0.5395458104933438,"D Oregon health insurance experiment
8
8"
REFERENCES,0.5403288958496476,"E
Details for baseline methods
9
9"
REFERENCES,0.5411119812059515,"E.1
ITE methods for unconfoundedness
. . . . . . . . . . . . . . . . . . . . . . . . .
9
10"
REFERENCES,0.5418950665622553,"E.2
General IV methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
11"
REFERENCES,0.5426781519185592,"E.3
Wald estimator
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
12"
REFERENCES,0.543461237274863,"F
Visualization of predicted ITEs
12
13"
REFERENCES,0.5442443226311668,"G Implementation details and hyperparameter tuning
13
14"
REFERENCES,0.5450274079874706,"H Results for semi-synthetic data
15
15"
REFERENCES,0.5458104933437745,"I
Results for cross-fitting
16
16"
REFERENCES,0.5465935787000783,"A
Proofs
17"
REFERENCES,0.5473766640563822,"We start by deriving an auxiliary Lemma. That is, we derive an explicit expression for the Stage 2
18"
REFERENCES,0.548159749412686,"oracle pseudo outcome regression E[ ˆY0 | X = x] of MRIV.
19"
REFERENCES,0.5489428347689899,Lemma 4.
REFERENCES,0.5497259201252936,E[ ˆY0 | X = x]
REFERENCES,0.5505090054815975,"=
π(x)
ˆδA(x)ˆπ(x)"
REFERENCES,0.5512920908379013," 
µY
1 (x) −µA
1 (x) ˆτinit(x)

+
(1 −π(x))
ˆδA(x)(1 −ˆπ(x))"
REFERENCES,0.5520751761942052," 
µA
0 (x) ˆτinit(x) −µY
0 (x)
"
REFERENCES,0.552858261550509,"+ ˆµA
0 (x) ˆτinit(x) −ˆµY
0 (x)
ˆδA(x) π(x)"
REFERENCES,0.5536413469068129,ˆπ(x) −1 −π(x)
REFERENCES,0.5544244322631167,1 −ˆπ(x)
REFERENCES,0.5552075176194206,"
+ ˆτinit(x) (1)"
REFERENCES,0.5559906029757243,Proof.
REFERENCES,0.5567736883320282,"E[ ˆY0 | X = x]
(2)"
REFERENCES,0.557556773688332,=π(x)E
REFERENCES,0.5583398590446359,"""
Y −A ˆτinit(X) −ˆµY
0 (X) + ˆµA
0 (X) ˆτinit(X)
ˆδA(X) ˆπ(X)"
REFERENCES,0.5591229444009397,"X = x, Z = 1 #"
REFERENCES,0.5599060297572436,+ (1 −π(x))E
REFERENCES,0.5606891151135474,"""
Y −A ˆτinit(X) −ˆµY
0 (X) + ˆµA
0 (X) ˆτinit(X)
ˆδA(X) (1 −ˆπ(X))"
REFERENCES,0.5614722004698512,"X = x, Z = 0 #"
REFERENCES,0.562255285826155,+ ˆτinit(x) (3)
REFERENCES,0.5630383711824589,"=
π(x)
ˆδA(x) ˆπ(x)"
REFERENCES,0.5638214565387627," 
µY
1 (x) −µA
1 (x) ˆτinit(x) −ˆµY
0 (x) + ˆµA
0 (x) ˆτinit(x)
"
REFERENCES,0.5646045418950666,"+
1 −π(x)
ˆδA(x) (1 −ˆπ(x))"
REFERENCES,0.5653876272513704," 
µY
0 (x) −µA
0 (x) ˆτinit(x) −ˆµY
0 (x) + ˆµA
0 (x) ˆτinit(x)

+ ˆτinit(x)
(4)"
REFERENCES,0.5661707126076743,"Rearranging the terms yields the desired result.
20"
REFERENCES,0.566953797963978,"A.1
Proof of Theorem 1 (multiple robustness property)
21"
REFERENCES,0.5677368833202819,"We use Lemma 4 to show that under each of the three conditions it follows that E[ ˆY0 | X = x] = τ(x).
22 1."
REFERENCES,0.5685199686765857,"E[ ˆY0 | X = x]
(5)"
REFERENCES,0.5693030540328896,"=
π(x)
δA(x) ˆπ(x)
 
µY
1 (x) −µA
1 (x) τ(x) + µA
0 (x) τ(x) −µY
0 (x)
"
REFERENCES,0.5700861393891934,"+
(1 −π(x))
δA(x) (1 −ˆπ(x))
 
µA
0 (x) τ(x) −µY
0 (x) −µA
0 (x) τ(x) + µY
0 (x)

+ τ(x)
(6)"
REFERENCES,0.5708692247454973,"=
π(x)
δA(x) ˆπ(x) (δY (x) −δY (x)) + τ(x) = τ(x).
(7) 2."
REFERENCES,0.5716523101018011,E[ ˆY0 | X = x] =
REFERENCES,0.572435395458105," 
µY
1 (x) −µA
1 (x) ˆτinit(x)
"
REFERENCES,0.5732184808144087,"δA(x)
+"
REFERENCES,0.5740015661707126," 
µA
0 (x) ˆτinit(x) −µY
0 (x)
"
REFERENCES,0.5747846515270164,"δA(x)
+ ˆτinit(x) (8)"
REFERENCES,0.5755677368833203,= δY (x) −ˆτinit(x) δA(x)
REFERENCES,0.5763508222396241,"δA(x)
+ ˆτinit(x) = τ(x).
(9) 3."
REFERENCES,0.577133907595928,E[ ˆY0 | X = x] =
REFERENCES,0.5779169929522318," 
µY
1 (x) −µA
1 (x) τ(x)
"
REFERENCES,0.5787000783085356,"ˆδA(x)
+"
REFERENCES,0.5794831636648394," 
µA
0 (x) τ(x) −µY
0 (x)
"
REFERENCES,0.5802662490211433,"ˆδA(x)
+ τ(x)
(10)"
REFERENCES,0.5810493343774471,= δY (x)
REFERENCES,0.581832419733751,"ˆδA(x)
−τ(x)δA(x)"
REFERENCES,0.5826155050900548,"ˆδA(x)
+ τ(x) = τ(x)
(11)"
REFERENCES,0.5833985904463587,"A.2
Proof of Theorem 2 (Convergence rate of MRIV)
23"
REFERENCES,0.5841816758026624,"To prove Theorem 2, we need an additional assumption on the second stage regression estimator ˆEn.
24"
REFERENCES,0.5849647611589663,"We refer to Kennedy [8] (Theorem 1) for a detailed discussion on this assumption.
25"
REFERENCES,0.5857478465152701,"Assumption 5 (From Theorem 1 of Kennedy [8]). The following two statements hold:
26"
REFERENCES,0.586530931871574,"1. ˆEn[W + c | X = x] = ˆEn[W | X = x] + c for any random W and constant c
27"
REFERENCES,0.5873140172278778,"2. If E[W | X = x] = E[V | X = x] then
28"
REFERENCES,0.5880971025841817,"E

ˆEn[W | X = x] −E[W | X = x]
2
≍E

ˆEn[V | X = x] −E[V | X = x]
2
. (12)"
REFERENCES,0.5888801879404855,"Proof of Theorem 2. Using Assumption 5, we can apply Theorem 1 of Kennedy [8] and obtain
29"
REFERENCES,0.5896632732967894,"E
h
(ˆτinit(x) −τ(x))2i
≲R(x) + E

ˆr(x)2
,
(13)"
REFERENCES,0.5904463586530931,"where R(x) = E
h
(eτMR(x) −τ(x))2i
is the oracle risk of the second stage regression and r(x) =
30"
REFERENCES,0.591229444009397,"E[ ˆY0 | X = x] −τ(x). We can apply Lemma 4 to obtain
31"
REFERENCES,0.5920125293657008,"ˆr(x) =
π(x)
ˆδA(x) ˆπ(x)"
REFERENCES,0.5927956147220047," 
µY
1 (x) −µA
1 (x) ˆτinit(x)

+
(1 −π(x))
ˆδA(x) (1 −ˆπ(x))"
REFERENCES,0.5935787000783085," 
µA
0 (x) ˆτinit(x) −µY
0 (x)
"
REFERENCES,0.5943617854346124,"+ ˆµA
0 (x) ˆτinit(x) −ˆµY
0 (x)
ˆδA(x) π(x)"
REFERENCES,0.5951448707909162,ˆπ(x) −1 −π(x)
REFERENCES,0.59592795614722,1 −ˆπ(x)
REFERENCES,0.5967110415035238,"
+ ˆτinit(x) −τ(x)
(14) ="
REFERENCES,0.5974941268598277,"µY
1 (x) −µY
0 (x)
ˆδA(x)"
REFERENCES,0.5982772122161315,"!
π(x)"
REFERENCES,0.5990602975724354,"ˆπ(x) + µY
0 (x) −ˆµY
0 (x)
ˆδA(x) π(x)"
REFERENCES,0.5998433829287392,ˆπ(x) −1 −π(x)
REFERENCES,0.6006264682850431,1 −ˆπ(x)
REFERENCES,0.601409553641347,"
+ (ˆτinit(x) −τ(x)) +"
REFERENCES,0.6021926389976507,"(µA
0 (x) −µA
1 (x)) ˆτinit(x)
ˆδA(x)"
REFERENCES,0.6029757243539546,"!
π(x)"
REFERENCES,0.6037588097102584,"ˆπ(x) + (ˆµD
0 (x) −µD
0 (x)) ˆτinit(x)
ˆδA(x) π(x)"
REFERENCES,0.6045418950665623,ˆπ(x) −1 −π(x)
REFERENCES,0.6053249804228661,1 −ˆπ(x)  (15)
REFERENCES,0.60610806577917,= δY (x) π(x)
REFERENCES,0.6068911511354738,"ˆδA(x) ˆπ(x)
+"
REFERENCES,0.6076742364917777," 
µY
0 (x) −ˆµY
0 (x)

(π(x) −ˆπ(x))
ˆδA(x) ˆπ(x) (1 −ˆπ(x))
+ (ˆτinit(x) −τ(x))"
REFERENCES,0.6084573218480814,−δA(x) π(x) ˆτinit(x)
REFERENCES,0.6092404072043853,"ˆδA(x) ˆπ(x)
+"
REFERENCES,0.6100234925606891," 
ˆµA
0 (x) −µA
0 (x)

ˆτinit(x) (π(x) −ˆπ(x))
ˆδA(x) ˆπ(x) (1 −ˆπ(x))
(16)"
REFERENCES,0.610806577916993,"=
(π(x) −ˆπ(x))
ˆδA(x) ˆπ(x) (1 −ˆπ(x))"
REFERENCES,0.6115896632732968," 
µY
0 (x) −ˆµY
0 (x)

+
 
ˆµA
0 (x) −µA
0 (x)

ˆτinit(x)
"
REFERENCES,0.6123727486296007,+ (ˆτinit(x) −τ(x)) + π(x)δA(x)
REFERENCES,0.6131558339859045,"ˆπ(x)ˆδA(x)
(τ(x) −ˆτinit(x))
(17)"
REFERENCES,0.6139389193422083,"=
(π(x) −ˆπ(x))
ˆδA(x) ˆπ(x) (1 −ˆπ(x))"
REFERENCES,0.6147220046985121," 
µY
0 (x) −ˆµY
0 (x)

+
 
ˆµA
0 (x) −µA
0 (x)

ˆτinit(x)
"
REFERENCES,0.615505090054816,"+ (τ(x) −ˆτinit(x))

δA(x) −ˆδA(x)

π(x) + (τ(x) −ˆτinit(x)) (π(x) −ˆπ(x)) ˆδA(x). (18)"
REFERENCES,0.6162881754111198,"Applying the inequality (a+b)2 ≤2(a2 +b2) together with Assumption 4 and the fact that π(x) ≤1
32"
REFERENCES,0.6170712607674237,"yields
33"
REFERENCES,0.6178543461237275,"ˆr(x)2 ≤
4
ϵ4ρ2 (π(x) −ˆπ(x))2 h 
µY
0 (x) −ˆµY
0 (x)
2 +
 
ˆµA
0 (x) −µA
0 (x)
2 K2i"
REFERENCES,0.6186374314800314,"+ 4 (τ(x) −ˆτinit(x))2 
δA(x) −ˆδA(x)
2
+ 4 (τ(x) −ˆτinit(x))2 (π(x) −ˆπ(x))2 . (19)"
REFERENCES,0.6194205168363351,"By setting eK = max{K, 1}, we obtain
34"
REFERENCES,0.620203602192639,ˆr(x)2 ≤4 eK2 ϵ4ρ2
REFERENCES,0.6209866875489428,"
(π(x) −ˆπ(x))2 h 
µY
0 (x) −ˆµY
0 (x)
2 +
 
ˆµA
0 (x) −µA
0 (x)
2 + (ˆτinit(x) −τ(x))2i"
REFERENCES,0.6217697729052467,"+ (τ(x) −ˆτinit(x))2 
δA(x) −ˆδA(x)
2
.
(20)"
REFERENCES,0.6225528582615505,"Applying expectations on both sides yields
35"
REFERENCES,0.6233359436178544,"E
h
(ˆτinit(x) −τ(x))2i
(21)"
REFERENCES,0.6241190289741582,"≲R(x) + E
h
(ˆτinit(x) −τ(x))2i 
E

ˆδA(x) −δA(x)
2
+ E
h
(ˆπ(x) −π(x))2i"
REFERENCES,0.624902114330462,"+ E
h
(ˆπ(x) −π(x))2i 
E
h 
ˆµY
0 (x) −µY
0 (x)
2i
+ E
h 
ˆµA
0 (x) −µA
0 (x)
2i
,
(22)"
REFERENCES,0.6256851996867658,"because (ˆπ(x), ˆδA(x)) ⊥⊥(ˆµY
0 (x), ˆµA
0 (x), ˆτinit(x)) due to sample splitting. The claim follows now
36"
REFERENCES,0.6264682850430697,"by applying Assumption 3.
37"
REFERENCES,0.6272513703993735,"A.3
Proof of Theorem 3 (Convergence rate of the Wald estimator)
38"
REFERENCES,0.6280344557556774,"Proof. We define eC = max{C, 1} and obtain the upper bound
39"
REFERENCES,0.6288175411119812,"(ˆτW (x) −τ(x))2
(23) ="
REFERENCES,0.6296006264682851,"(ˆµY
1 (x) −µY
1 (x)) δA(x) + (µY
0 (x) −ˆµY
0 (x)) δA(x) + (δA(x) −ˆδA(x)) δY (x)"
REFERENCES,0.6303837118245889,δA(x) ˆδA(x) !2 (24)
REFERENCES,0.6311667971808927,≤4 eC2 ρ2eρ2
REFERENCES,0.6319498825371965,"h
(ˆµY
1 (x) −µY
1 (x))2 + (ˆµY
0 (x) −µY
0 (x))2 + (δA(x) −ˆδA(x))2i
(25)"
REFERENCES,0.6327329678935004,≤8 eC2
REFERENCES,0.6335160532498042,"ρ2eρ2

(ˆµY
1 (x) −µY
1 (x))2 + (ˆµY
0 (x) −µY
0 (x))2 + (ˆµA
1 (x) −µA
1 (x))2"
REFERENCES,0.6342991386061081,"+(ˆµA
0 (x) −µA
0 (x))2
,
(26)"
REFERENCES,0.6350822239624119,"where we used the inequality (a + b)2 ≤2(a2 + b2) several times. Taking expectations and applying
40"
REFERENCES,0.6358653093187158,"the smoothness assumptions yields the result.
41"
REFERENCES,0.6366483946750195,"B
Theoretical analysis under sparsity assumptions
42"
REFERENCES,0.6374314800313234,"In Sec. 4.2, we analyzed MRIV theoretically by imposing smoothness assumptions on the underlying
43"
REFERENCES,0.6382145653876272,"data generating process. In particular, we derived a multiple robust convergence rate and showed
44"
REFERENCES,0.6389976507439311,"that MRIV outperforms the Wald estimator if the oracle ITE is smoother than its components. In
45"
REFERENCES,0.6397807361002349,"this section, we derive similar results by relying on a different set of assumptions. Instead of using
46"
REFERENCES,0.6405638214565388,"smoothness, we make assumptions on the level of sparsity of the ITE components. This assumption
47"
REFERENCES,0.6413469068128426,"is often imposed in high-dimensional settings (n < p) and is in line with previous literature on
48"
REFERENCES,0.6421299921691465,"analyzing ITE estimators [4, 8].
49"
REFERENCES,0.6429130775254502,"In the following, we say a function f(x) is k-sparse, if it is linear in x ∈Rp and it only depends
50"
REFERENCES,0.6436961628817541,"on k < min{n, p} predictors. [22] showed, that in this case the minimax rate of f(x) is given by
51"
REFERENCES,0.6444792482380579,k log(p)
REFERENCES,0.6452623335943618,"n
. The linearity assumption can be relaxed to an additive structural assumption, which we omit
52"
REFERENCES,0.6460454189506656,"here for simplicity. In the following, we replace the smoothness conditions in Assumption 3 with
53"
REFERENCES,0.6468285043069695,"sparsity conditions.
54"
REFERENCES,0.6476115896632733,"Assumption 6 (Sparsity). We assume that (1) the nuisance components µY
i (·) are α-sparse, µA
i (·)
55"
REFERENCES,0.6483946750195771,"and δA(·) are β-sparse, and π(·) is δ-sparse; (2) all nuisance components are estimated with their
56"
REFERENCES,0.6491777603758809,respective minimax rate of k log(p)
REFERENCES,0.6499608457321848,"n
, where k ∈{α, β, δ}; and (3) the oracle ITE τ(·) is γ-sparse and
57"
REFERENCES,0.6507439310884886,"the initial ITE estimator ˆτinit converges with rate rτ(n).
58"
REFERENCES,0.6515270164447925,"We restate now our result from Theorem 3 for MRIV using the sparsity assumption.
59"
REFERENCES,0.6523101018010963,"Theorem 5 (MRIV upper bound under sparsity). We consider the same setting as in Theorem 2
60"
REFERENCES,0.6530931871574002,"under the sparsity assumption 6. If the second-stage estimator ˆEn yields the minimax rate γ log(p) n
61"
REFERENCES,0.653876272513704,"and satisfies Assumption 5, the oracle risk is upper bounded by
62"
REFERENCES,0.6546593578700078,"E
h
(ˆτMRIV(x) −τ(x))2i
≲γ log(p)"
REFERENCES,0.6554424432263116,"n
+ rτ(n)(β + δ) log(p)"
REFERENCES,0.6562255285826155,"n
+ (α + β)δ log2(p) n2
."
REFERENCES,0.6570086139389193,"Proof. Follows immediately from the proof of Theorem 2, i.e., from Eq.(21) by applying Ass- 6.
63"
REFERENCES,0.6577916992952232,"Again, we obtain a multiple robust convergence rate for MRIV in the sense that MRIV achieves a fast
64"
REFERENCES,0.658574784651527,"rate even if the initial estimator or several nuisance estimators converge slowly. More precisely, for a
65"
REFERENCES,0.6593578700078309,"fast convergence rate of ˆτMRIV(x), it is sufficient if either: (1) rτ(n) decreases fast and δ is small;
66"
REFERENCES,0.6601409553641346,"(2) rτ(n) decreases fast and α and β are small; or (3) all α, β, and δ are small.
67"
REFERENCES,0.6609240407204385,"We derive now the corresponding rate for the Wald estimator.
68"
REFERENCES,0.6617071260767423,"Theorem 6 (Wald oracle upper bound). Given estimators ˆµY
i (x) and ˆµA
i (x). Let ˆδA(x) = ˆµA
1 (x) −
69"
REFERENCES,0.6624902114330462,"ˆµA
0 (x) satisfy Assumption 4. Then, under Assumption 6 the oracle risk of the Wald estimator ˆτW (x)
70"
REFERENCES,0.66327329678935,"is bounded by
71"
REFERENCES,0.6640563821456539,"E

(ˆτW(x) −τ(x))2
≲(α + β) log(p)"
REFERENCES,0.6648394675019577,"n
(27)"
REFERENCES,0.6656225528582616,"Proof. Follows immediately from the proof of Theorem 3, i.e., from Eq.(23) by applying Ass- 6.
72"
REFERENCES,0.6664056382145653,"If α = β = δ, we obtain the rates
73"
REFERENCES,0.6671887235708692,"E
h
(ˆτMRIV(x) −τ(x))2i
≲γ log(p)"
REFERENCES,0.6679718089271731,"n
+ α2 log2(p)"
REFERENCES,0.6687548942834769,"n2
and
E

(ˆτW(x) −τ(x))2
≲α log(p)"
REFERENCES,0.6695379796397808,"n
,
(28)"
REFERENCES,0.6703210649960846,"which means that ˆτMRIV(x) outperforms ˆτW(x) for γ < α, i.e., if the oracle ITE is more sparse than
74"
REFERENCES,0.6711041503523885,"its components.
75"
REFERENCES,0.6718872357086922,"C
Simulated data
76"
REFERENCES,0.6726703210649961,"In the following, we describe how we simulate synthetic data for the experiments in Sec. 5.1 from the
77"
REFERENCES,0.6734534064212999,"main paper. As mentioned therein, we simulate the ITE components from Gaussian processes using
78"
REFERENCES,0.6742364917776038,"the prior induced by the Matern kernel [12]
79"
REFERENCES,0.6750195771339076,"Kℓ,ν(xi, xj) =
1
Γ(ν)2ν−1 √"
REFERENCES,0.6758026624902115,"2ν
ℓ
∥xi −xj∥2 !ν Kν √"
REFERENCES,0.6765857478465153,"2ν
ℓ
∥xi −xj∥2 !"
REFERENCES,0.6773688332028192,",
(29)"
REFERENCES,0.6781519185591229,"where Γ(·) is the Gamma function and Kν(·) is the modified Bessel function of second kind. Here, ℓ
80"
REFERENCES,0.6789350039154268,"is the length scale of the kernel and ν controls the smoothness of the sampled functions.
81"
REFERENCES,0.6797180892717306,"We set ℓ= 1 and sample functions δY ∼GP(0, Kℓ,γ), µY
0 ∼GP(0, Kℓ,α), f1 ∼GP(0, Kℓ,β),
82"
REFERENCES,0.6805011746280345,"f0 ∼GP(0, Kℓ,β) and g ∼GP(0, Kℓ,β). Then, we define µY
1 = δY + µY
0 , µA
1 = 0.3 · σ ◦f1 + 0.7,
83"
REFERENCES,0.6812842599843383,"µA
0 = 0.3 · σ ◦f0, δA = µA
1 −µA
0 , µY
0 = c0δA, and π = σ ◦g. Finally, we set the oracle ITE to
84"
REFERENCES,0.6820673453406422,"τ = µY
1 −µY
0
µA
1 −µA
0
= δY"
REFERENCES,0.682850430696946,"δA
.
(30)"
REFERENCES,0.6836335160532498,"Note that we can create a setup where the ITE τ is smoother than its components by using a small
85"
REFERENCES,0.6844166014095536,"α/β ratio. An example is shown in Fig. 1.
86"
REFERENCES,0.6851996867658575,"4
3
2
1
0
1
2
3
4 3 2 1 0 1 2 3 4"
REFERENCES,0.6859827721221613,"Y
Z = 1"
REFERENCES,0.6867658574784652,"Y
Z = 0"
REFERENCES,0.687548942834769,"Y
1
Y
0"
REFERENCES,0.6883320281910729,Figure 1: Gaussian process simulation for α = 1.5 and β = 50.
REFERENCES,0.6891151135473766,"In the following, we describe how we generate data the (X, Z, A, Y ) using the ITE components
87"
REFERENCES,0.6898981989036805,"µY
i (x), µA
i (x), and π(x). We begin by sampling n observed confounder X ∼N(0, 1), unobserved
88"
REFERENCES,0.6906812842599843,"confounders U ∼N
 
0, 0.22
, and instruments Z ∼Bernoulli(π(X)). Then, we obtain treatments
89"
REFERENCES,0.6914643696162882,"via
90"
REFERENCES,0.692247454972592,"A = Z 1{U + ϵA > α1(X)} + (1 −Z) 1{U + ϵA > α0(X)}
(31)"
REFERENCES,0.6930305403288959,"with indicator function 1, noise ϵA ∼N
 
0, 0.12
, and αi(X) = Φ−1  
1 −µA
i (X)
 √"
REFERENCES,0.6938136256851997,"0.12 + 0.22,
91"
REFERENCES,0.6945967110415036,"where Φ−1 denotes the quantile function of the standard normal distribution. Finally, we generate the
92"
REFERENCES,0.6953797963978073,"outcomes via
93"
REFERENCES,0.6961628817541112,"Y = A
(µA
1 (X) −1)µY
0 (X) −µA
0 (X)µY
1 (X) + µY
1 (X)
δA(X)"
REFERENCES,0.696945967110415,"
(32)"
REFERENCES,0.6977290524667189,"+ (1 −A)
µA
1 (X)µY
0 (X) −µA
0 (X)µY
1 (X)
δA(X)"
REFERENCES,0.6985121378230227,"
+ αUU + ϵY ,
(33)"
REFERENCES,0.6992952231793266,"where ϵY ∼N
 
0, 0.32
is noise and αU > 0 is a parameter indicating the level of unobserved
94"
REFERENCES,0.7000783085356304,"confounding. This choice of A and Y in Eq. (31) and Eq. (32), respectively, implies that τ(x) is
95"
REFERENCES,0.7008613938919342,"indeed the ITE, i. e., it holds that τ(x) = E[Y (1) −Y (0) | X = x].
96"
REFERENCES,0.701644479248238,"Lemma 7. Let (X, Z, A, Y ) be sampled from the the previously described procedure. Then, it holds
97"
REFERENCES,0.7024275646045419,"that
98"
REFERENCES,0.7032106499608457,"µA
i (x) = E[A | Z = i, X = x]
and
µY
i (x) = E[Y | Z = i, X = x].
(34)"
REFERENCES,0.7039937353171496,"Proof. The first claim follows from
99"
REFERENCES,0.7047768206734534,"E[A | Z = i, X = x] = P (U + ϵA > αi(x)) = 1 −Φ(Φ−1(1 −µA
i (x))) = µA
i (x),
(35)"
REFERENCES,0.7055599060297573,"because U + ϵA ∼N(0,
√"
REFERENCES,0.706342991386061,"0.12 + 0.22). The second claim follows from
100"
REFERENCES,0.7071260767423649,"E[Y | Z = i, X = x] = µA
i (x)
(µA
1 (x) −1)µY
0 (x) −µA
0 (x)µY
1 (x) + µY
1 (x)
δA(x)"
REFERENCES,0.7079091620986687,"
(36)"
REFERENCES,0.7086922474549726,"+ (1 −µA
i (x))
µA
1 (x)µY
0 (x) −µA
0 (x)µY
1 (x)
δA(x)"
REFERENCES,0.7094753328112764,"
(37)"
REFERENCES,0.7102584181675803,"= µY
i (x)δA(x)"
REFERENCES,0.7110415035238841,"δA(x)
= µY
i (x).
(38) 101"
REFERENCES,0.711824588880188,"D
Oregon health insurance experiment
102"
REFERENCES,0.7126076742364917,"The so-called Oregon health insurance experiment1 (OHIE) [6] was an important RCT with non-
103"
REFERENCES,0.7133907595927956,"compliance. It was intentionally conducted as large-scale effort among public health to assess the
104"
REFERENCES,0.7141738449490994,"effect of health insurance on several outcomes such as health or economic status. In 2008, a lottery
105"
REFERENCES,0.7149569303054033,"draw offered low-income, uninsured adults in Oregon participation in a Medicaid program, providing
106"
REFERENCES,0.7157400156617071,"health insurance. Individuals whose names were drawn could decide to sign up for the program.
107"
REFERENCES,0.716523101018011,"In our analysis, the lottery assignment is the instrument Z, the decision to sign up for the Medicaid
108"
REFERENCES,0.7173061863743148,"program is the treatment A, and an overall health score is the outcome Y . The outcome was obtained
109"
REFERENCES,0.7180892717306187,"after a period of 12 months during in-person interviews. We use the following covariates X: age,
110"
REFERENCES,0.7188723570869224,"gender, language, the number of emergency visits before the experiment, and the number of people
111"
REFERENCES,0.7196554424432263,"the individual signed up with. The latter is used to control for peer effects, and it is important to
112"
REFERENCES,0.7204385277995301,"include this variable in our analysis as it is the only variable influencing the propensity score (see
113"
REFERENCES,0.721221613155834,"below). We extract ∼10,000 observations from the OHIE data and plot the histograms of all variables
114"
REFERENCES,0.7220046985121378,"in Fig. 2. We can clearly observe the presence of non-compliance within the data, because the
115"
REFERENCES,0.7227877838684417,"ratio of treated / untreated individuals is much lower than the corresponding ratio for the treatment
116"
REFERENCES,0.7235708692247454,"assignment.
117"
REFERENCES,0.7243539545810493,"20
40
60
Y"
REFERENCES,0.7251370399373531,"Frequency 0
1
A"
REFERENCES,0.725920125293657,"Frequency 0
1
Z"
REFERENCES,0.7267032106499608,Frequency
REFERENCES,0.7274862960062647,"20
30
40
50
60
Age"
REFERENCES,0.7282693813625685,Frequency
REFERENCES,0.7290524667188724,"1
2
3
Num. signed up"
REFERENCES,0.7298355520751761,Frequency
REFERENCES,0.73061863743148,"0
5
10
15
Num. visits"
REFERENCES,0.7314017227877838,Frequency
REFERENCES,0.7321848081440877,"Male
Female
Gender"
REFERENCES,0.7329678935003915,Frequency
REFERENCES,0.7337509788566954,"Other
English
Language"
REFERENCES,0.7345340642129993,Frequency
REFERENCES,0.735317149569303,Figure 2: Histograms of each variable in our sample from OHIE.
REFERENCES,0.7361002349256069,"The data collection in the OHIE was done follows: After excluding individuals below the age
118"
REFERENCES,0.7368833202819107,"of 19, above the age of 64, and individuals with residence outside of Oregon, 74,922 individuals
119"
REFERENCES,0.7376664056382146,"were considered for the lottery. Among those, 29,834 were selected randomly and were offered
120"
REFERENCES,0.7384494909945184,"participation in the program. However, the probability of selection depended on the number of
121"
REFERENCES,0.7392325763508223,"household members on the waiting list: for instance, an individual who signed up with another person
122"
REFERENCES,0.7400156617071261,"was twice as likely to be selected. From the 74,922 individuals, 57,528 signed up alone, 17,236
123"
REFERENCES,0.74079874706343,"signed up with another person, and 158 signed up with two more people on the waiting list. Thus, the
124"
REFERENCES,0.7415818324197337,"probability of being selected conditional on the number of household members on the waiting list
125"
REFERENCES,0.7423649177760376,"follows the multivariate version of Wallenius’ noncentral hypergeometric distribution [2].
126"
REFERENCES,0.7431480031323414,"Propensity score: We computed the propensity score as follows. To account for the Wallenius’
127"
REFERENCES,0.7439310884886453,"noncentral hypergeometric distribution, we use the R package BiasedUrn to calculate the propensity
128"
REFERENCES,0.7447141738449491,"score π(x) = P(Z = 1 | X = x). We obtained
129"
REFERENCES,0.745497259201253,"π(x) = 
 "
REFERENCES,0.7462803445575568,"0.345,
if individual x signed up alone,
0.571,
if individual x signed up with one more person,
0.719,
if individual x signed up with two more people.
(39)"
REFERENCES,0.7470634299138607,"During the training of both MRIV and DRIV, we use the calculated values from Eq. (39) for the
130"
REFERENCES,0.7478465152701644,"propensity score.
131"
REFERENCES,0.7486296006264683,"1Data available here: https://www.nber.org/programs-projects/projects-and-centers/oregon-health-insurance-
experiment"
REFERENCES,0.7494126859827721,"E
Details for baseline methods
132"
REFERENCES,0.750195771339076,"In this section, we give a brief overview on the baselines which we used in our experiments. We
133"
REFERENCES,0.7509788566953798,"implemented: (1) ITE methods for unconfoundedness [8, 13]; (2) general IV methods, i.e., IV
134"
REFERENCES,0.7517619420516837,"methods developed for IV settings with multiple or continuous instruments and treatments [1, 7, 14,
135"
REFERENCES,0.7525450274079875,"15, 20, 21]; and (3) two instantiations of the Wald estimator for the binary IV setting [16].
136"
REFERENCES,0.7533281127642913,"E.1
ITE methods for unconfoundedness
137"
REFERENCES,0.7541111981205951,"Many ITE methods assume unconfoundedness, i.e., that all confounders are observed in the data.
138"
REFERENCES,0.754894283476899,"Formally, the unconfoundedness assumption can be expressed in the potential outcomes framework
139"
REFERENCES,0.7556773688332028,"as
140"
REFERENCES,0.7564604541895067,"Y (1), Y (0) ⊥⊥A | X.
(40)
Under unconfoundedness, the ITE is identified as
141"
REFERENCES,0.7572435395458105,"τ(x) = µ1(x) −µ0(x)
with
µi(x) = E[Y | A = i, X = x].
(41)"
REFERENCES,0.7580266249021144,"Methods that assume unconfoundedness proceed by estimating µi(x) = E[Y | A = i, X = x] from
142"
REFERENCES,0.7588097102584181,"Eq. (41). However, if unobserved confounders U exist, it follows that
143"
REFERENCES,0.759592795614722,"τ(x) = E[Y | A = 1, X = x, U] −E[Y | A = 0, X = x, U] ̸= µ1(x) −µ0(x),
(42)"
REFERENCES,0.7603758809710258,"which means that estimators that assume unconfoundedness are generally biased. Nevertheless, we
144"
REFERENCES,0.7611589663273297,"include two baselines that assume unconfoundedness into our experiments: TARNet [13] and the
145"
REFERENCES,0.7619420516836335,"DR-learner [8].
146"
REFERENCES,0.7627251370399374,"TARNet [13]: TARNet [13] is a neural network that estimates the ITE components µi(x) from
147"
REFERENCES,0.7635082223962412,"Eq. 41 by learning a shared representation Φ(x) and two potential outcome heads hi(Φ(x)). We train
148"
REFERENCES,0.7642913077525451,"TARNet by minimizing the loss
149"
REFERENCES,0.7650743931088488,"L(θ) = n
X"
REFERENCES,0.7658574784651527,"i=1
L (hai(Φ(xi, θΦ), θhi), yi) ,
(43)"
REFERENCES,0.7666405638214565,"where θ = (θh1, θh0, θΦ) denotes the model parameters and L denotes squared loss if Y is continuous
150"
REFERENCES,0.7674236491777604,"or binary cross entropy loss if Y is binary.
151"
REFERENCES,0.7682067345340642,"Note regarding balanced representations: In [13], the authors propose to add an additional regular-
152"
REFERENCES,0.7689898198903681,"ization term inspired from domain adaptation literature, which forces TARNet to learn a balanced
153"
REFERENCES,0.7697729052466719,"representation Φ(x), i.e., that minimizes the distance the treatment and control group in the feature
154"
REFERENCES,0.7705559906029757,"space. They showed that this approach leads to minimization of a generalization bound on the ITE
155"
REFERENCES,0.7713390759592795,"estimation error if the representation is invertible.
156"
REFERENCES,0.7721221613155834,"In our experiments, we refrained from learning balanced representations because minimizing the
157"
REFERENCES,0.7729052466718872,"regularized loss from [13] does not necessarily result in an invertible representation and thus may
158"
REFERENCES,0.7736883320281911,"even harm the estimation performance. For a detailed discussion, we refer to [4]. Furthermore,
159"
REFERENCES,0.7744714173844949,"by leaving out the regularization, we ensure comparability between the different baselines. If
160"
REFERENCES,0.7752545027407988,"balanced representations are desired, the balanced representation approach could also be extended to
161"
REFERENCES,0.7760375880971025,"MRIV-Net, as we also build MRIV-Net on learning shared representations.
162"
REFERENCES,0.7768206734534064,"DR-learner [8]: The DR-learner [8] is a meta learner that takes arbitrary estimators of the ITE
163"
REFERENCES,0.7776037588097102,"componenets µi and the propensity score π(x) = P(A = 1 | X = x) as input and performs a pseudo
164"
REFERENCES,0.7783868441660141,"outcome regression by using the pseudo outcome
165"
REFERENCES,0.7791699295223179,"ˆY0 =

A
ˆπ(X) −
1 −A
1 −ˆπ(X)"
REFERENCES,0.7799530148786218,"
Y +

1 −
A
ˆπ(X)"
REFERENCES,0.7807361002349256,"
ˆµ1(X) −

1 −
1 −A
1 −ˆπ(X)"
REFERENCES,0.7815191855912295,"
ˆµ0(X).
(44)"
REFERENCES,0.7823022709475332,"In our experiments, we use TARNet as base method to provide initial estimators ˆµi(X). We further
166"
REFERENCES,0.7830853563038371,"learn propensity score estimates ˆπ(X) by adding a seperate representation to TARNet as done in
167"
REFERENCES,0.7838684416601409,"[13].
168"
REFERENCES,0.7846515270164448,"E.2
General IV methods
169"
REFERENCES,0.7854346123727486,"2SLS [20]: 2SLS [20] is a linear two-stage approach. First, the treatments A are regressed on the
170"
REFERENCES,0.7862176977290525,"instruments Z and fitted values ˆA are obtained. In the second stage, the outcome Y is regressed on ˆA.
171"
REFERENCES,0.7870007830853563,"We implement 2SLS using the scikit-learn package.
172"
REFERENCES,0.7877838684416602,"KIV [14]: Kernel IV [14] generalizes 2SLS to nonlinear settings. KIV assumes that the data is
173"
REFERENCES,0.7885669537979639,"generated by
174"
REFERENCES,0.7893500391542678,"Y = f(A) + U,
(45)
where U is an additive unobserved confounder and f is some unknown (potentially nonlinear)
175"
REFERENCES,0.7901331245105716,"structural function. KIV then models the structural function via
176"
REFERENCES,0.7909162098668755,"f(a) = µtψ(a)
and
E[ψ(A) | Z = z] = V ϕ(z),
(46)
where ψ andϕ are feature maps. Here, kernel ridge regressions instead of linear regressions are used
177"
REFERENCES,0.7916992952231793,"in both stages to estimate µ and V .
178"
REFERENCES,0.7924823805794832,"Following [14] we use the exponential kernel [12] and set the length scale to the median inter-point
179"
REFERENCES,0.793265465935787,"distance. KIV does not provide a direct way to incorporate the observed confounders X. Hence, we
180"
REFERENCES,0.7940485512920908,"augment both the instrument and the treatment with X, which is consistent with previous work [1,
181"
REFERENCES,0.7948316366483946,"21]. We also use two different samples for each stage as recommended in [14].
182"
REFERENCES,0.7956147220046985,"DFIV [21]: DFIV [21] is a similar approach KIV in generalizing 2SLS to nonlinear setting by
183"
REFERENCES,0.7963978073610023,"assuming Eq. (45) and Eq. (46). However, instead of using kernel methods, DFIV models the features
184"
REFERENCES,0.7971808927173062,"maps ψθA and ϕθZ as neural networks with parameters θA and θZ, respectively. DFIV is trained by
185"
REFERENCES,0.79796397807361,"iteratively updating the parameters θA and θZ. The authors also provide a training algorithm that
186"
REFERENCES,0.7987470634299139,"incorporates observed confounders X, which we implemented for our experiments. During training,
187"
REFERENCES,0.7995301487862176,"we used two different datasets for each of the two stages as described in in the paper.
188"
REFERENCES,0.8003132341425215,"DeepIV [7]: DeepIV [7] also assumes additive unobserved confounding as in Eq. (45), but leverages
189"
REFERENCES,0.8010963194988254,"the identification result [10]
190"
REFERENCES,0.8018794048551292,"E[Y | X = x, Z = z] =
Z
h(a, x) dF(a | x, z),
(47)"
REFERENCES,0.8026624902114331,"where h(a, x) = f(a, x) + E[U | X = x] is the target counterfactual prediction function. DeepIV
191"
REFERENCES,0.8034455755677369,"estimates F(a | x, z), i.e., the conditional distribution function of the treatment A given observed
192"
REFERENCES,0.8042286609240408,"covariates X and instruments Z, by using neural networks. Because we consider only binary
193"
REFERENCES,0.8050117462803446,"treatments, we simply implement a (tunable) feed-forward neural network with sigmoid activation
194"
REFERENCES,0.8057948316366484,"function. Then, DeepIV proceeds by learning a second stage neural network to solve the inverse
195"
REFERENCES,0.8065779169929522,"problem defined by Eq. (47).
196"
REFERENCES,0.8073610023492561,"DeepGMM [1]: DeepGMM [1] adopts neural networks for IV estimation inspired by the (optimally
197"
REFERENCES,0.8081440877055599,"weighted) Generalized Method of Moments. The DeepGMM estimator is defined as the solution of
198"
REFERENCES,0.8089271730618638,"the following minimax game:
199"
REFERENCES,0.8097102584181676,"ˆθ ∈arg min
θ∈Θ
sup
τ∈T"
N,0.8104933437744715,"1
n n
X"
N,0.8112764291307752,"i=1
f(zi, τ)(yi −g(ai, θ)) −1"
N,0.8120595144870791,"4n n
X"
N,0.8128425998433829,"i=1
f 2(zi, τ)(yi −g(ai, eθ))2,
(48)"
N,0.8136256851996868,"where f(zi, ·) and g(ai, ·) are parameterized by neural networks. As recommended in [1], we solve
200"
N,0.8144087705559906,"this optimization via adversarial training with the Optimistic Adam optimizer [5], where we set the
201"
N,0.8151918559122945,"parameter eθ to the previous value of θ.
202"
N,0.8159749412685983,"DMLIV [15]: DMLIV [15] assumes that the data is generated via
203"
N,0.8167580266249022,"Y = τ(X)A + f(X) + U,
(49)"
N,0.8175411119812059,"where τ is the ITE f some function of the observed covariates. First, DMLIV estimates the functions
204"
N,0.8183241973375098,"q(X) = E[Y | X], h(Z, X) = E[A | Z, X], and p(X) = E[A | X]. Then, the ITE is learned by
205"
N,0.8191072826938136,"minimizing the loss
206"
N,0.8198903680501175,"L(θ) =
X"
N,0.8206734534064213,"i=1
(yi −ˆq(xi) −ˆτ(xi, θ)(ˆh(zi, xi) −ˆp(xi))2,
(50)"
N,0.8214565387627252,"where ˆτ(X, ·) is some model for τ(X). In our experiments, we use (tunable) feed-forward neural
207"
N,0.822239624119029,"networks for all estimators.
208"
N,0.8230227094753328,"DRIV [15]: DRIV [15] is a meta learner, originally proposed in combination with DMLIV. It requires
209"
N,0.8238057948316366,"initial estimators for q(X), p(X), π(X) = E[Z | X = x], and f(X) = E[A · Z | X = x] as well
210"
N,0.8245888801879405,"as an initial ITE estimatior ˆτinit(X) (e.g., from DMLIV). The ITE is then estimated by a pseudo
211"
N,0.8253719655442443,"regression on the following doubly robust pseudo outcome:
212"
N,0.8261550509005482,ˆYDR = ˆτinit(X) + (Y −ˆq(X) −ˆτinit(X)(A −ˆp(X))Z −ˆπ(X))
N,0.826938136256852,"ˆf(X) −ˆp(X)ˆr(X)
.
(51)"
N,0.8277212216131559,"We implement all regressions using (tunable) feed-forward neural networks.
213"
N,0.8285043069694596,"Comparison between DRIV vs. MRIV: There are two key differences between our paper and [15]:
214"
N,0.8292873923257635,"(i) Our MRIV is multiply robust, while DRIV is only doubly robust. (ii) We derive a multiple robust
215"
N,0.8300704776820673,"convergence rate, while the rate in [15] is not robust with respect to the nuisance rates.
216"
N,0.8308535630383712,"Ad (i): Both MRIV and DRIV perform a pseudo-outcome regression on the efficient influence
217"
N,0.831636648394675,"function (EIF) of the ATE. The key difference: DRIV uses the doubly robust parametrization of the
218"
N,0.8324197337509789,"EIF from [11], whereas we use the multiply robust parametrization of the EIF from [17] 2. Hence,
219"
N,0.8332028191072827,"our MRIV frameworks extends DRIV in a non-trivial way to achieve multiple robustness (rather
220"
N,0.8339859044635866,"than doubly robustness). Thus, our estimator is consistent in the union of three different model
221"
N,0.8347689898198903,"specifications rather than two.3
222"
N,0.8355520751761942,"Ad (ii): Here, we compare the convergence rates from DRIV and our MRIV and, thereby, show the
223"
N,0.836335160532498,"strengths of our MRIV. To this end, let us assume that the pseudo regression function is γ-smooth and
224"
N,0.8371182458888019,"that we use the same second-stage estimator ˆEn with minimax rate n−
2γ
2γ+p for both DRIV and MRIV.
225"
N,0.8379013312451057,"If the nuisance parameters q(X), p(X), f(X), and π(X) are α-smooth and further are estimated
226"
N,0.8386844166014096,with minimax rate n
N,0.8394675019577134,"−2α
2α+p , Corollary 4 from [15] states that DRIV converges with rate
227"
N,0.8402505873140172,"E
h
(ˆτDRIV(x) −τ(x))2i
≲n"
N,0.841033672670321,"−2γ
2γ+p + n"
N,0.8418167580266249,"−4α
2α+p ."
N,0.8425998433829287,"In contrast, MRIV assumes estimation of the nuisance parameters µY
0 (x) with rate n"
N,0.8433829287392326,"−2α
2α+p , µA
0 (x)
228"
N,0.8441660140955364,and δA(x) with rate n
N,0.8449490994518403,"−2β
2β+p , and π(x) with rate n"
N,0.845732184808144,"−2δ
2δ+p . If the initial estimator ˆτinit(x) converges with
229"
N,0.8465152701644479,"rate rτ(n), our Theorem 2 yields the rate
230"
N,0.8472983555207517,"E
h
(ˆτMRIV(x) −τ(x))2i
≲n"
N,0.8480814408770556,"−2γ
2γ+p + rτ(n)

n"
N,0.8488645262333594,"−2β
2β+p + n"
N,0.8496476115896633,"−2δ
2δ+p

+ n−2(
α
2α+p +
δ
2δ+p) + n−2(
β
2β+p +
δ
2δ+p)."
N,0.8504306969459671,If all nuisance parameters converge with the same minimax rate of n
N,0.851213782302271,"−2α
2α+p , the rates of DRIV and
231"
N,0.8519968676585747,"our MRIV coincide. However, different to DRIV, our rate is additionally multiple robust in spirit of
232"
N,0.8527799530148786,"Theorem 1. This presents a crucial strength of our MRIV over DRIV: For example, if δ is small (slow
233"
N,0.8535630383711824,"convergence of ˆπ(x)), our MRIV still with fast rate as long as α and β are large (i.e., if the other
234"
N,0.8543461237274863,"nuisance parameters are sufficiently smooth).
235"
N,0.8551292090837901,"E.3
Wald estimator
236"
N,0.855912294440094,"Finally, we consider the Wald estimator [16] for the binary IV setting. More precisely, we estimate
237"
N,0.8566953797963978,"the ITE components µY
i (x) and µA
i (x) seperately and plug them into
238"
N,0.8574784651527017,"τ(x) = ˆµY
1 (x) −ˆµY
0 (x)
ˆµA
1 (x) −ˆµA
0 (x) .
(52)"
N,0.8582615505090054,"We consider two versions of the Wald estimator:
239"
N,0.8590446358653093,"Linear: We use linear regressions to estimate the µY
i (x) and logistic regressions to estimate the
240"
N,0.8598277212216131,"µA
i (x).
241"
N,0.860610806577917,"BART: We use Bayesian additive regression trees [3] trees to estimate the µY
i (x) and random forest
242"
N,0.8613938919342208,"classifier to estimate the µA
i (x).
243"
N,0.8621769772905247,"2For a detailed discussion on multiple robustness and the importance of the EIF parametrization, we refer to
[18], Section 4.5.
3On a related note, a similar, important contribution of developing multiply robust method was recently made
for the average treatment effect. Here, the estimator of [11] was extended by the estimator of [17] to allow for
multi robustness. Yet, this different from our work in that it focuses on the average treatment effect, while we
study the individual treatment effect in our paper."
N,0.8629600626468285,"F
Visualization of predicted ITEs
244"
N,0.8637431480031323,"We plot the predicted ITEs for the different baselines and MRIV-Net in Fig. 3 (for n = 3000). As
245"
N,0.8645262333594361,"expected, the linear methods (2SLS and linear Wald) are not flexible enough to provide accurate
246"
N,0.86530931871574,"ITE estimates. We also observe that the curve of MRIV-Net without MRIV is quite wiggly, i.e., the
247"
N,0.8660924040720438,"estimator has a relatively large variance. This variance is reduced when the full MRIV-Net is applied.
248"
N,0.8668754894283477,"As a result, curve is much smoother. This is reasonable because MRIV does not estimate the ITE
249"
N,0.8676585747846516,"components individually, but estimates the ITE directly via the Stage 2 pseudo outcome regression.
250"
N,0.8684416601409554,"Overall, this confirms the superiority of our proposed framework.
251 X 3 2 1 0 1 2 3 Y"
N,0.8692247454972593,Method = TARNet X Y
N,0.870007830853563,Method = 2SLS X Y
N,0.8707909162098669,Method = KIV X Y
N,0.8715740015661707,Method = DFIV X 3 2 1 0 1 2 3 Y
N,0.8723570869224746,Method = DeepIV X Y
N,0.8731401722787784,Method = DeepGMM X Y
N,0.8739232576350823,Method = DMLIV
N,0.874706342991386,"2
0
2
X Y"
N,0.87548942834769,Method = Wald (linear)
N,0.8762725137039937,"2
0
2
X 3 2 1 0 1 2 3 Y"
N,0.8770555990602976,Method = Wald (BART)
N,0.8778386844166014,"2
0
2
X Y"
N,0.8786217697729053,Method = MRIV-Net (network only)
N,0.8794048551292091,"2
0
2
X Y"
N,0.880187940485513,Method = MRIV-Net
N,0.8809710258418167,Figure 3: Predicted ITEs (blue) and oracle ITE (red) for different baselines.
N,0.8817541111981206,"G
Implementation details and hyperparameter tuning
252"
N,0.8825371965544244,"Implementation details for deep learning models: To make the performance of the deep learning
253"
N,0.8833202819107283,"models comparable, we implemented all feed-forward neural networks (including MRIV-Net) as
254"
N,0.8841033672670321,"follows: We use two hidden layers with RELU activation functions. We also incorporated a dropout
255"
N,0.884886452623336,"layer for each hidden layer. We trained all models with the Adam optimizer [9] using 100 epochs.
256"
N,0.8856695379796398,"Exceptions are only DFIV and DeepGMM, where we used 200 epochs for training, accounting for
257"
N,0.8864526233359437,"slower convergence of the respective (adversarial) training algorithms. For DeepGMM, we further
258"
N,0.8872357086922474,"used Optimistic Adam [5] as in the original paper.
259"
N,0.8880187940485513,"Training times: We report the approximate times needed to train the deep learning models on
260"
N,0.8888018794048551,"our simulated data with n = 5000 in Table 1. For training, we used an AMD Ryzen Pro 7 CPU.
261"
N,0.889584964761159,"Compared to DMLIV and DRIV, the training of MRIV-Net is faster because only a single neural
262"
N,0.8903680501174628,"network is trained.
263"
N,0.8911511354737667,Table 1: Training times for deep learning models (in seconds).
N,0.8919342208300705,"TARNet
TARNet + DR
DFIV
DeepIV
DeepGMM
DMLIV
DMLIV + DRIV
MRIV-Net"
N,0.8927173061863743,"∼10.62
∼28.57
∼164.98
∼30.21
∼17.31
∼74.98
∼91.12
∼32.20"
N,0.8935003915426781,"Hyperparameter tuning: We performed hyperparameter tuning for all deep learning models
264"
N,0.894283476898982,"(including MRIV-Net), KIV, and the BART Wald estimator on all datasets. For all methods except
265"
N,0.8950665622552858,"KIV and DFIV, we split the data into a training set (80%) and a validation set (20%). We then
266"
N,0.8958496476115897,"performed 40 random grid search iterations and chose the set of parameters that minimized the
267"
N,0.8966327329678935,"respective training loss on the validation set. In particular, the tuning procedure was the same for
268"
N,0.8974158183241974,"all baselines, which ensures that the performance gain of MRIV-Net is due to the method itself
269"
N,0.8981989036805011,"and not due to larger flexibility. Exceptions are only KIV and DFIV, for which we implemented
270"
N,0.898981989036805,"the customized hyperparameter tuning algorithms proposed in [14] and [21] to ensure consistency
271"
N,0.8997650743931088,"with prior literature. For the meta learners (DR-learner, DRIV, and MRIV), we first performed
272"
N,0.9005481597494127,"hyperparameter tuning for the base methods and nuisance models, before tuning the pseudo outcome
273"
N,0.9013312451057165,"regression neural network by using the input from the tuned models. The tuning ranges for the
274"
N,0.9021143304620204,"hyperparameter are shown in Table 2. These include both the hyperparameter rangers shared across
275"
N,0.9028974158183242,"all neural networks and the model-specific hyperparameters. For reproducibility purposes, we publish
276"
N,0.9036805011746281,"the selected hyperparameters in our GitHub project as .yaml files.4
277"
N,0.9044635865309318,Table 2: Hyperparameter tuning ranges.
N,0.9052466718872357,"MODEL
HYPERPARAMETER
TUNING RANGE"
N,0.9060297572435395,"Feed-forward neural networks
Hidden layer size(es)
p, 5p, 10p, 20p, 30p (simulated data)
(Shared parameter ranges
p, 3p, 5p, 8p, 10p (OHIE)
for all deep learning baselines)
Learning rate
0.0001, 0.0005, 0.001, 0.005, 0.01
Batch size
64, 128, 256
Dropout probability
0, 0.1, 0.2, 0.3"
N,0.9068128425998434,"KIV
λ (Ridge penalty first stage)
5, 6, 7, 8, 9, 10, 12
ξ (Ridge penalty second stage)
5, 6, 7, 8, 9, 10, 12
DFIV
λ1 (Ridge penalty first stage)
0.0001, 0.001, 0.01, 0.1 (simulated data)
0.01, 0.05, 0.1 (OHIE)
λ2 (Ridge penalty second stage)
0.0001, 0.001, 0.01, 0.1 (simulated data)
0.01, 0.05, 0.1 (OHIE)
DeepGMM
λf (learning rate multiplier)
0.5, 1, 1.5, 2, 5
Wald (BART)
Number of trees (BART)
20, 30, 40, 50
Number of trees (Random forest classifier)
20, 30, 40, 50
p = network input size"
N,0.9075959279561472,"Hyperparameter robustness checks: We also investigate the robustness of MRIV-Net with respect
278"
N,0.9083790133124511,"to hyperparameter choice. To to this, we fix the optimal hyperparameter constellation for our simulated
279"
N,0.9091620986687549,"data for n = 3000 and perturb the hidden layer sizes, learning rate, dropout probability, and batch size.
280"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9099451840250588,"4Codes
are
in
the
supplementary
materials.
Codes
are
also
available
at
https://anonymous.4open.science/r/MRIV-Net-0AC4 (Upon acceptance, we replace the link and point
to a public GitHub repository)."
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9107282693813625,"The results are shown in Fig. 4. We observe that the RMSE only changes marginally when perturbing
281"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9115113547376664,"the different hyperparameters, indicating that our method is to a certain degree robust against
282"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9122944400939702,"hyperparameter misspecification. Furthermore, our results indicate that the performance improvement
283"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9130775254502741,"of MRIV-Net over the baselines observed in our experiments is not due to hyperparameter tuning,
284"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9138606108065779,"but to our method itself.
285"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9146436961628818,"20
25
30
35
Value 0.0 0.1 0.2 0.3 0.4 0.5 RMSE"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9154267815191856,Parameter = Hidden size
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9162098668754894,0.0025 0.0050 0.0075 0.0100 Value
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9169929522317932,Parameter = Learning rate
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9177760375880971,"0.0
0.1
0.2
0.3
Value 0.0 0.1 0.2 0.3 0.4 0.5 RMSE"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9185591229444009,Parameter = Dropout prob.
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9193422083007048,"50
100
150
200
250
Value"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9201252936570086,Parameter = Batch size
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9209083790133125,Figure 4: Robustness checks for different hyperparameters of MRIV-Net.
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9216914643696162,"H
Results for semi-synthetic data
286"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9224745497259201,"In the main paper, we evaluated MRIV-Net both on synthetic and real-world data. Here, we provide
287"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9232576350822239,"additional results by constructing a semi-synthetic dataset on the basis of OHIE. It is common practice
288"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9240407204385278,"in causal inference literature to use semi-synthetic data for evaluation, because it combines advantages
289"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9248238057948316,"of both synthetic and real-world data. On the one hand, the real-world data part ensures that the
290"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9256068911511355,"data distribution is realistic and matches those in practice. On the other hand, the counterfactual
291"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9263899765074393,"ground-truth is still available, which makes it possible to measure the performance of ITE methods.
292"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9271730618637432,"We construct our semi-synthetic data as follows: First, we extract the covariates X ∈R5 and instru-
293"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9279561472200469,"ments Z ∈{0, 1} of our OHIE dataset from Sec. D. Then, we construct the treatment components
294"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9287392325763508,"µA
i (x) via
295"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9295223179326546,"µA
1 (X) = 0.3 · σ(X1) + 0.7
and µA
0 (X) = 0.3 · σ(X1),
(53)
where X1 is the (standardized) age and σ(·) is the sigmoid function. The outcome components are
296"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9303054032889585,"constructed via
297"
"CODES
ARE
IN
THE
SUPPLEMENTARY",0.9310884886452623,"µY
1 (X) = 0.5X2
1 +"
X,0.9318715740015662,"5
X"
X,0.93265465935787,"i=2
X2
i
and µY
0 (X) = −0.5X2
1 +"
X,0.9334377447141738,"5
X"
X,0.9342208300704777,"i=2
X2
i .
(54)"
X,0.9350039154267815,"We then sample treatments A and outcomes Y as in Eq. (31) and Eq. (32). Lemma 7 ensures that
298"
X,0.9357870007830854,"µY
i (X) = E[Y | Z = i, X] and µA
i (X) = E[A | Z = i, X].
299"
X,0.9365700861393892,"Given the above, the oracle ITE becomes
300"
X,0.9373531714956931,"τ(X) = X2
1
0.7 .
(55)"
X,0.9381362568519969,"Note that τ(X) is sparse in the sense that it only depends on age, while the outcome components
301"
X,0.9389193422083008,"depend on all five covariates. Following our theoretical analysis in Sec. B, MRIV-Net should thus
302"
X,0.9397024275646045,"outperform methods that aim at estimating the components directly. This is confirmed in Table 3,
303"
X,0.9404855129209084,"where we show the results for all baselines and MRIV-Net on the semi-synthetic data. Indeed, we
304"
X,0.9412685982772122,"observe that MRIV-Net outperforms all other baselines, confirming both the superiority of our method
305"
X,0.9420516836335161,"as well as our theoretical results under sparsity assumptions from Sec. B.
306"
X,0.9428347689898199,Table 3: Results for semi-synthetic data.
X,0.9436178543461238,"Method
n = 3000
n = 5000
n = 8000"
X,0.9444009397024276,"(1) STANDARD ITE
TARNet [13]
1.66 ± 0.11
1.58 ± 0.07
1.57 ± 0.11
TARNet + DR [13, 8]
1.31 ± 0.28
1.22 ± 0.37
1.12 ± 0.15"
X,0.9451840250587314,"(2) GENERAL IV
2SLS [19]
1.34 ± 0.06
1.31 ± 0.03
1.32 ± 0.02
KIV [14]
1.97 ± 0.10
1.92 ± 0.05
1.93 ± 0.05
DFIV [21]
1.67 ± 0.44
1.63 ± 0.47
1.45 ± 0.17
DeepIV [7]
1.24 ± 0.26
0.99 ± 0.22
0.84 ± 0.19
DeepGMM [1]
1.39 ± 0.03
1.37 ± 0.16
1.18 ± 0.16
DMLIV [15]
2.12 ± 0.10
2.09 ± 0.09
2.02 ± 0.11
DMLIV + DRIV [15]
1.22 ± 0.10
1.18 ± 0.19
1.00 ± 0.08"
X,0.9459671104150352,"(3) WALD ESTIMATOR [16]
Linear
1.42 ± 0.24
1.28 ± 0.07
1.32 ± 0.07
BART
1.48 ± 0.24
1.29 ± 0.04
1.06 ± 0.13"
X,0.9467501957713391,"MRIV-Net (network only)
1.11 ± 0.15
0.84 ± 0.14
0.95 ± 0.21
MRIV-Net (ours)
0.71 ± 0.24
0.75 ± 0.18
0.78 ± 0.26
Reported: RMSE (mean ± standard deviation). Lower = better (best in bold)"
X,0.9475332811276429,"I
Results for cross-fitting
307"
X,0.9483163664839468,"Here, we repeat our experiments from the main paper but now make use of cross-fitting. Recall that,
308"
X,0.9490994518402506,"in Theorem 2, we assume that the nuisance parameter estimation and the pseudo-outcome regression
309"
X,0.9498825371965545,"are performed on three independent samples. We now address this through cross-fitting. To this end,
310"
X,0.9506656225528582,"our aim is to show that our proposed MRIV framework is again superior.
311"
X,0.9514487079091621,"For MRIV, we proceeded as follows: We split the sample D into three equally sized samples D1, D2,
312"
X,0.9522317932654659,"and D3. We then trained ˆτinit(x), ˆµY
0 (x), and ˆµA
0 (x) on D1, ˆδA(x) and ˆπ(x) on D2, and performed
313"
X,0.9530148786217698,"the pseudo-outcome regression on D3. Then, we repeated the same training procedure two times, but
314"
X,0.9537979639780736,"performed the pseudo-outcome regression on D2 and D1. Finally, we averaged the resulting three
315"
X,0.9545810493343775,"ITE estimators. For DRIV, we implemented the cross-fitting procedure described in [15]. For the
316"
X,0.9553641346906813,"DR-learner, we followed [8].
317"
X,0.9561472200469852,"The results are in Table H. Importantly, the results confirm the effectiveness of our proposed MRIV.
318"
X,0.9569303054032889,"Overall, we find that our proposed MRIV outperforms DRIV for the vast majority of base methods
319"
X,0.9577133907595928,"when performing cross-fitting. Furthermore, MRIV-Net is highly competitive even when comparing
320"
X,0.9584964761158966,"it with the cross-fitted estimators. This shows that our heuristic to learn separate representations
321"
X,0.9592795614722005,"instead of performing sample splits works in practice. In sum, the results confirm empirically that our
322"
X,0.9600626468285043,"MRIV is superior.
323"
X,0.9608457321848082,"Table 4: Results for base methods with different meta-learners (i.e., DRIV, and our MRIV) using
cross-fitting and results for MRIV-Net without cross-fitting."
X,0.961628817541112,"n = 3000
n = 5000
n = 8000
hhhhhhhhhhhhhh
Base methods
Meta-learners
DRIV
MRIV (ours)
DRIV
MRIV (ours)
DRIV
MRIV (ours)"
X,0.9624119028974158,"(1) STANDARD ITE
TARNet [13]
0.30 ± 0.02
0.36 ± 0.16
0.18 ± 0.06
0.16 ± 0.03
0.21 ± 0.08
0.13 ± 0.04
TARNet + DR-learner [13, 8]
0.85 ± 0.11
0.66 ± 0.08
0.67 ± 0.12"
X,0.9631949882537196,"(2) GENERAL IV
2SLS [19]
0.42 ± 0.11
0.33 ± 0.09
0.20 ± 0.07
0.23 ± 0.11
0.24 ± 0.10
0.14 ± 0.02
KIV [14]
0.47 ± 0.18
0.45 ± 0.15
0.20 ± 0.06
0.19 ± 0.08
0.22 ± 0.04
0.15 ± 0.03
DFIV [21]
0.35 ± 0.05
0.28 ± 0.09
0.22 ± 0.10
0.18 ± 0.08
0.24 ± 0.12
0.16 ± 0.04
DeepIV [7]
0.38 ± 0.09
0.44 ± 0.16
0.20 ± 0.07
0.19 ± 0.07
0.20 ± 0.08
0.12 ± 0.02
DeepGMM [1]
0.42 ± 0.09
0.42 ± 0.16
0.19 ± 0.04
0.19 ± 0.07
0.22 ± 0.06
0.13 ± 0.02
DMLIV [15]
0.44 ± 0.09
0.46 ± 0.16
0.21 ± 0.04
0.19 ± 0.07
0.21 ± 0.05
0.14 ± 0.02"
X,0.9639780736100235,"(3) WALD ESTIMATOR [16]
Linear
0.47 ± 0.23
0.36 ± 0.12
0.24 ± 0.05
0.20 ± 0.08
0.22 ± 0.05
0.15 ± 0.02
BART
0.43 ± 0.12
0.39 ± 0.12
0.14 ± 0.05
0.13 ± 0.05
0.23 ± 0.08
0.15 ± 0.02"
X,0.9647611589663273,"MRIV-Net\w network only (ours)
0.35 ± 0.12
0.26 ± 0.11
0.19 ± 0.13
0.15 ± 0.03
0.18 ± 0.08
0.13 ± 0.03
Reported: RMSE (mean ± standard deviation). Lower = better (best in bold)"
REFERENCES,0.9655442443226312,"References
324"
REFERENCES,0.966327329678935,"[1]
Andrew Bennett, Nathan Kallus, and Tobias Schnabel. “Deep generalized method of moments
325"
REFERENCES,0.9671104150352389,"for instrumental variable analysis”. In: NeurIPS. 2019.
326"
REFERENCES,0.9678935003915426,"[2]
Jean Chesson. “A non-central multivariate hypergeometric distribution arising from biased
327"
REFERENCES,0.9686765857478465,"sampling with application to selective predation”. In: Journal of Applied Probability 13.4
328"
REFERENCES,0.9694596711041503,"(1976), pp. 795–797.
329"
REFERENCES,0.9702427564604542,"[3]
Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. “BART: Bayesian additive
330"
REFERENCES,0.971025841816758,"regression trees”. In: The Annals of Applied Statistics 4.1 (2010), pp. 266–298.
331"
REFERENCES,0.9718089271730619,"[4]
Alicia Curth and Mihaela van der Schaar. “Nonparametric estimation of heterogeneous treat-
332"
REFERENCES,0.9725920125293657,"ment effects: From theory to learning Algorithms”. In: AISTATS. 2021.
333"
REFERENCES,0.9733750978856696,"[5]
Constantinos Daskalakis et al. “Training GANs with optimism”. In: ICLR. 2018.
334"
REFERENCES,0.9741581832419733,"[6]
Amy Finkelstein et al. “The oregon health insurance experiment: Evidence from the first year”.
335"
REFERENCES,0.9749412685982772,"In: The Quarterly Journal of Economics 127.3 (2012), pp. 1057–1106.
336"
REFERENCES,0.975724353954581,"[7]
Jason Hartford et al. “Deep IV: A flexible approach for counterfactual prediction”. In: ICML.
337"
REFERENCES,0.9765074393108849,"2017.
338"
REFERENCES,0.9772905246671887,"[8]
Edward H. Kennedy. “Optimal doubly robust estimation of heterogeneous causal effects”. In:
339"
REFERENCES,0.9780736100234926,"arXiv preprint (2020).
340"
REFERENCES,0.9788566953797964,"[9]
Diederik P. Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: ICLR.
341"
REFERENCES,0.9796397807361003,"2015.
342"
REFERENCES,0.980422866092404,"[10]
Whitney K. Newey and James L. Powell. “Instrumental variable estimation of nonparametric
343"
REFERENCES,0.9812059514487079,"models”. In: Econometrica 71.5 (2003), pp. 1565–1578.
344"
REFERENCES,0.9819890368050117,"[11]
Ryo Okui et al. “Doubly robust instrumental variable regression”. In: Statistica Sinica 22.1
345"
REFERENCES,0.9827721221613156,"(2012), pp. 173–205.
346"
REFERENCES,0.9835552075176194,"[12]
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine
347"
REFERENCES,0.9843382928739233,"learning. 3. print. Adaptive computation and machine learning. Cambridge, Mass.: MIT Press,
348"
REFERENCES,0.985121378230227,"2008.
349"
REFERENCES,0.9859044635865309,"[13]
Uri Shalit, Fredrik D. Johansson, and David Sontag. “Estimating individual treatment effect:
350"
REFERENCES,0.9866875489428347,"Generalization bounds and algorithms”. In: ICML. 2017.
351"
REFERENCES,0.9874706342991386,"[14]
Rahul Singh, Maneesh Sahani, and Arthur Gretton. “Kernel instrumental variable regression”.
352"
REFERENCES,0.9882537196554424,"In: NeurIPS. 2019.
353"
REFERENCES,0.9890368050117463,"[15]
Vasilis Syrgkanis et al. “Machine learning estimation of heterogeneous treatment effects with
354"
REFERENCES,0.9898198903680501,"instruments”. In: NeurIPS. 2019.
355"
REFERENCES,0.990602975724354,"[16]
Abraham Wald. “The fitting of straight lines if both variables are subject to error”. In: Annals
356"
REFERENCES,0.9913860610806577,"of Mathematical Statistics 11.3 (1940), pp. 284–300.
357"
REFERENCES,0.9921691464369616,"[17]
Linbo Wang and Eric J. Tchetgen Tchetgen. “Bounded, efficient and multiply robust estimation
358"
REFERENCES,0.9929522317932654,"of average treatment effects using instrumental variables”. In: Journal of the Royal Statistical
359"
REFERENCES,0.9937353171495693,"Society: Series B 80.3 (2018), pp. 531–550.
360"
REFERENCES,0.9945184025058731,"[18]
Yixin Wang and David M. Blei. “The blessings of multiple causes”. In: Journal of the American
361"
REFERENCES,0.995301487862177,"Statistical Association 114.528 (2019), pp. 1574–1596.
362"
REFERENCES,0.9960845732184808,"[19]
Jeffrey M. Wooldridge. Introductory Econometrics: A modern approach. Routledge, 2013.
363"
REFERENCES,0.9968676585747847,"[20]
Phillip G. Wright. The tariff on animal and vegitable oils. New York: Macmillan, 1928.
364"
REFERENCES,0.9976507439310884,"[21]
Liyuan Xu et al. “Learning deep features in instrumental variable regression”. In: ICLR. 2021.
365"
REFERENCES,0.9984338292873923,"[22]
Yun Yang and Surya T. Tokdar. “Minimax-optimal nonparametric regression in high dimen-
366"
REFERENCES,0.9992169146436961,"sions”. In: The Annals of Statistics 43.2 (2015), pp. 652–674.
367"
