Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009416195856873823,"Transparent models, which are machine learning models that produce inherently
1"
ABSTRACT,0.0018832391713747645,"interpretable predictions, are receiving significant attention in high-stakes domains.
2"
ABSTRACT,0.002824858757062147,"However, despite much real-world data being collected as time series, there is a lack
3"
ABSTRACT,0.003766478342749529,"of studies on transparent time series models. To address this gap, we propose a novel
4"
ABSTRACT,0.004708097928436911,"transparent neural network model for time series called Generalized Additive Time
5"
ABSTRACT,0.005649717514124294,"Series Model (GATSM). GATSM consists of two parts: 1) independent feature
6"
ABSTRACT,0.006591337099811676,"networks to learn feature representations, and 2) a transparent temporal module to
7"
ABSTRACT,0.007532956685499058,"learn temporal patterns across different time steps using the feature representations.
8"
ABSTRACT,0.00847457627118644,"This structure allows GATSM to effectively capture temporal patterns and handle
9"
ABSTRACT,0.009416195856873822,"dynamic-length time series while preserving transparency. Empirical experiments
10"
ABSTRACT,0.010357815442561206,"show that GATSM significantly outperforms existing generalized additive models
11"
ABSTRACT,0.011299435028248588,"and achieves comparable performance to black-box time series models, such as
12"
ABSTRACT,0.01224105461393597,"recurrent neural networks and Transformer. In addition, we demonstrate that
13"
ABSTRACT,0.013182674199623353,"GATSM finds interesting patterns in time series. The source code is available at
14"
ABSTRACT,0.014124293785310734,"https://anonymous.4open.science/r/GATSM-78F4/.
15"
INTRODUCTION,0.015065913370998116,"1
Introduction
16"
INTRODUCTION,0.0160075329566855,"Artificial neural networks excel at learning complex representations and demonstrate remarkable
17"
INTRODUCTION,0.01694915254237288,"predictive performance across various fields. However, their complexity makes interpreting the
18"
INTRODUCTION,0.017890772128060263,"decision-making processes of neural network models challenging. Consequently, post-hoc explainable
19"
INTRODUCTION,0.018832391713747645,"artificial intelligence (XAI) methods, which explain the predictions of trained black-box models,
20"
INTRODUCTION,0.01977401129943503,"have been widely studied in recent years [1, 2, 3, 4]. XAI methods are generally effective at
21"
INTRODUCTION,0.02071563088512241,"providing humans with understandable explanations of model predictions. However, they may
22"
INTRODUCTION,0.021657250470809793,"produce incorrect and unfaithful explanations of the underlying black-box model and cannot provide
23"
INTRODUCTION,0.022598870056497175,"actual contributions of input features to model predictions [5, 6]. Therefore, their applicability to
24"
INTRODUCTION,0.023540489642184557,"high-stakes domains-such as healthcare and fraud detection, where faithfulness to the underlying
25"
INTRODUCTION,0.02448210922787194,"model and actual contributions of features are important-is limited.
26"
INTRODUCTION,0.025423728813559324,"Due to these limitations, transparent (i.e., inherently interpretable) models are attracting attention as
27"
INTRODUCTION,0.026365348399246705,"alternatives to XAI in high-stakes domains [7, 8, 9]. Modern transparent models typically adhere to
28"
INTRODUCTION,0.027306967984934087,"the generalized additive model (GAM) framework [10]. A GAM consists of independent functions,
29"
INTRODUCTION,0.02824858757062147,"each corresponding to an input feature, and makes predictions as a linear combination of these
30"
INTRODUCTION,0.02919020715630885,"functions (e.g., the sum of all functions). Therefore, each function reflects the contribution of its
31"
INTRODUCTION,0.030131826741996232,"respective feature. For this reason, interpreting GAMs is straightforward, making them widely used in
32"
INTRODUCTION,0.031073446327683617,"various fields, such as healthcare [11, 12], survival analysis [13], and model bias discovery [7, 14, 15].
33"
INTRODUCTION,0.032015065913371,"However, despite much real-world data being collected as time series, research on GAMs for time
34"
INTRODUCTION,0.03295668549905838,"series remains scarce. Consequently, the applicability of GAMs in real-world scenarios is still limited.
35"
INTRODUCTION,0.03389830508474576,"To overcome this limitation, we propose a novel transparent model for multivariate time series
36"
INTRODUCTION,0.03483992467043315,"called Generalized Additive Time Series Model (GATSM). GATSM consists of independent feature
37"
INTRODUCTION,0.035781544256120526,"networks to learn feature representations and a transparent temporal module to learn temporal patterns.
38"
INTRODUCTION,0.03672316384180791,"Since employing distinct networks across different time steps requires a massive amount of learnable
39"
INTRODUCTION,0.03766478342749529,"parameters, the feature networks in GATSM share the weights across all time steps, while the
40"
INTRODUCTION,0.038606403013182675,"temporal module independently learns temporal patterns. GATSM then generates final predictions by
41"
INTRODUCTION,0.03954802259887006,"integrating the feature representations with the temporal information from the temporal module. This
42"
INTRODUCTION,0.04048964218455744,"strategy allows GATSM to effectively capture temporal patterns and handle dynamic-length time
43"
INTRODUCTION,0.04143126177024482,"series while preserving transparency. Additionally, this approach facilitates the separate extraction of
44"
INTRODUCTION,0.0423728813559322,"time-independent feature contributions, the importance of individual time steps, and time-dependent
45"
INTRODUCTION,0.04331450094161959,"feature contributions through the feature functions, temporal module, and final prediction. To
46"
INTRODUCTION,0.044256120527306965,"demonstrate the effectiveness of GATSM, we conducted empirical experiments on various time series
47"
INTRODUCTION,0.04519774011299435,"datasets. The experimental results show that GATSM significantly outperforms existing GAMs
48"
INTRODUCTION,0.046139359698681735,"and achieves comparable performances to black-box time series models, such as recurrent neural
49"
INTRODUCTION,0.047080979284369114,"networks and Transformer [16]. In addition, we provide visualizations of GATSM‚Äôs predictions to
50"
INTRODUCTION,0.0480225988700565,"demonstrate that GATSM finds interesting patterns in time series.
51"
RELATED WORKS,0.04896421845574388,"2
Related Works
52"
RELATED WORKS,0.04990583804143126,"Various XAI studies have been conducted over the past decade [7, 8, 9, 17, 18]; however, they are
53"
RELATED WORKS,0.05084745762711865,"less relevant to the transparent model that is the subject of this study. Therefore, we refer readers to
54"
RELATED WORKS,0.051789077212806026,"[19, 20] for more detailed information on recent XAI research. In this section, we review existing
55"
RELATED WORKS,0.05273069679849341,"transparent models closely related to our GATSM and discuss their limitations.
56"
RELATED WORKS,0.05367231638418079,Table 1: Advantages of GATSM.
RELATED WORKS,0.054613935969868174,"Time series input
Temporal pattern
Dynamic time series"
RELATED WORKS,0.05555555555555555,existing GAMs
RELATED WORKS,0.05649717514124294,"NATM
‚úì
GATSM (our)
‚úì
‚úì
‚úì"
RELATED WORKS,0.05743879472693032,"The simple linear model is designed to fit the conditional expectation g (E (y | x)) = PM
i=1 xiwi,
57"
RELATED WORKS,0.0583804143126177,"where g(¬∑) is a link function, M indicates the number of input features, y is the target value for the
58"
RELATED WORKS,0.059322033898305086,"given input features x ‚ààRM, and wi ‚ààR is the learnable weight for xi. This model captures only
59"
RELATED WORKS,0.060263653483992465,"linear relationships between the target y and the inputs x. To address this limitation, GAM [10]
60"
RELATED WORKS,0.06120527306967985,"extends the simple linear model to the generalized form as follows:
61"
RELATED WORKS,0.062146892655367235,"g (E (y | x)) = M
X"
RELATED WORKS,0.06308851224105462,"i=1
fi (xi) ,
(1)"
RELATED WORKS,0.064030131826742,"where each fi(¬∑) is a function that models the effect of a single feature, referred as a feature function.
62"
RELATED WORKS,0.06497175141242938,"Typically, fi (¬∑) becomes a non-linear function such as a decision tree or neural network to capture
63"
RELATED WORKS,0.06591337099811675,"non-linear relationships.
64"
RELATED WORKS,0.06685499058380415,"Originally, GAMs were fitted via the backfitting algorithm using smooth splines [10, 21]. Later, Yin
65"
RELATED WORKS,0.06779661016949153,"Lou et al. [22] and Harsha Nori et al. [23] have proposed boosted decision tree-based GAMs, which
66"
RELATED WORKS,0.0687382297551789,"use boosted decision trees as feature functions. Spline- and tree-based GAMs have less flexibility
67"
RELATED WORKS,0.0696798493408663,"and scalability. Thus, extending them to transfer or multi-task learning is challenging. To overcome
68"
RELATED WORKS,0.07062146892655367,"this problem, various neural network-based GAMs have been proposed in recent years. Potts [24]
69"
RELATED WORKS,0.07156308851224105,"introduced generalized additive neural network, which employs 2-layer neural networks as feature
70"
RELATED WORKS,0.07250470809792843,"functions. Similarly, Rishabh Agarwal et al. [7] proposed neural additive model (NAM) that employs
71"
RELATED WORKS,0.07344632768361582,"multi-layer neural networks. To improve the scalability of NAM, Chun-Hao Chang et al. [8] and
72"
RELATED WORKS,0.0743879472693032,"Filip Radenovic et al. [9] proposed the neural oblivious tree-based GAM and the basis network-based
73"
RELATED WORKS,0.07532956685499058,"GAM, respectively. Xu et al. [25] introduced a sparse version of NAM using the group LASSO. One
74"
RELATED WORKS,0.07627118644067797,"disadvantage of GAMs is their limited predictive power, which stems from the fact that they only
75"
RELATED WORKS,0.07721280602636535,"learn first-order feature interactions-i.e., relationships between the target value and individual features.
76"
RELATED WORKS,0.07815442561205273,"To address this, various studies have been conducted to enhance the predictive powers of GAMs by
77"
RELATED WORKS,0.07909604519774012,"incorporating higher-order feature interactions, while still maintaining transparency. GA2M [26]
78"
RELATED WORKS,0.0800376647834275,"simply takes pairwise features as inputs to learn pairwise interactions. GAMI-Net [27], a neural
79"
RELATED WORKS,0.08097928436911488,"network-based GAM, consists of networks for main effects (i.e., first-order interactions) and pairwise
80"
RELATED WORKS,0.08192090395480225,"interactions. To enhance the interpretability of GAMI-Net, the sparsity and heredity constraints are
81"
RELATED WORKS,0.08286252354048965,"added, and trivial features are pruned in the training process. Sparse interaction additive network [28]
82 ùë•!,!"
RELATED WORKS,0.08380414312617702,"‚Ñé# ùë•!,!
‚Ñé! ùë•!,!
‚Ñé$ ùë•!,!
‚Ä¶ ùë•!,#"
RELATED WORKS,0.0847457627118644,"‚Ñé# ùë•!,#
‚Ñé! ùë•!,#
‚Ñé$ ùë•!,#
‚Ä¶ ùë•!,%"
RELATED WORKS,0.0856873822975518,"‚Ñé# ùë•!,%
‚Ñé! ùë•!,%
‚Ñé$ ùë•!,%
‚Ä¶
‚Ä¶ ùë§!,!"
RELATED WORKS,0.08662900188323917,"#$%
ùë§!,&"
RELATED WORKS,0.08757062146892655,"#$%
ùë§!,'"
RELATED WORKS,0.08851224105461393,"#$%
ùë§!,!"
RELATED WORKS,0.08945386064030132,"#$%
ùë§!,&"
RELATED WORKS,0.0903954802259887,"#$%
ùë§!,'"
RELATED WORKS,0.09133709981167608,"#$%
ùë§!,!"
RELATED WORKS,0.09227871939736347,"#$%
ùë§!,&"
RELATED WORKS,0.09322033898305085,"#$%
ùë§!,' #$%"
RELATED WORKS,0.09416195856873823,"ùë•#!,!
ùë•#!,#
ùë•#!,%"
RELATED WORKS,0.0951035781544256,": shared bases
Time-Sharing NBM"
RELATED WORKS,0.096045197740113,T steps
RELATED WORKS,0.09698681732580038,Time Sharing NBM
RELATED WORKS,0.09792843691148775,Input Time Series
RELATED WORKS,0.09887005649717515,Transformed Time Series
RELATED WORKS,0.09981167608286252,Masked MHA
RELATED WORKS,0.1007532956685499,Attention Map
RELATED WORKS,0.1016949152542373,Output GATSM ùíô%& ùíô%'
RELATED WORKS,0.10263653483992467,Linear
RELATED WORKS,0.10357815442561205,Linear
RELATED WORKS,0.10451977401129943,Softmax
RELATED WORKS,0.10546139359698682,Masked MHA
RELATED WORKS,0.1064030131826742,Positional Encoding
RELATED WORKS,0.10734463276836158,Concat
RELATED WORKS,0.10828625235404897,Linear
RELATED WORKS,0.10922787193973635,Figure 1: Architecture of GATSM.
RELATED WORKS,0.11016949152542373,"is a 3-phase method for exploiting higher-order interactions. Initially, a black-box neural network is
83"
RELATED WORKS,0.1111111111111111,"trained; subsequently, the top-k important features are identified using explainable feature attribution
84"
RELATED WORKS,0.1120527306967985,"methods like LIME [1] and SHAP [2], and finally, NAM is trained with these extracted features.
85"
RELATED WORKS,0.11299435028248588,"Dubey et al. [29] introduced scalable polynomial additive model, an end-to-end model that learns
86"
RELATED WORKS,0.11393596986817325,"higher-order interactions via polynomials. Similarly, Kim et al. [15] proposed higher-order NAM that
87"
RELATED WORKS,0.11487758945386065,"utilizes the feature crossing technique to capture higher-order interactions. Despite their capabilities,
88"
RELATED WORKS,0.11581920903954802,"the aforementioned GAMs cannot process time series data, which limits their applicability in real-
89"
RELATED WORKS,0.1167608286252354,"world scenarios. Recently, neural additive time series Model (NATM) [30], a time-series adaptation
90"
RELATED WORKS,0.11770244821092278,"of NAM, has been proposed. However, NATM handles each time step independently with separate
91"
RELATED WORKS,0.11864406779661017,"feature networks. This approach cannot capture effective temporal patterns and only takes fixed-length
92"
RELATED WORKS,0.11958568738229755,"time series as input. Our GATSM not only captures temporal patterns but also handles dynamic-length
93"
RELATED WORKS,0.12052730696798493,"time series. Table 1 shows the advantages of our GATSM compared to existing GAMs.
94"
PROBLEM STATEMENT,0.12146892655367232,"3
Problem Statement
95"
PROBLEM STATEMENT,0.1224105461393597,"We tackle the problem of the existing GAMs on time series. Equation (1) outlines the GAM framework
96"
PROBLEM STATEMENT,0.12335216572504708,"for tabular data, which fails to capture the interactions between current and previous observations in
97"
PROBLEM STATEMENT,0.12429378531073447,"time series. A straightforward method to extend GAM to time series, adopted in NATM, is applying
98"
PROBLEM STATEMENT,0.12523540489642185,"distinct feature functions to each time step and summing them to produce predictions:
99"
PROBLEM STATEMENT,0.12617702448210924,"g (E (yt | X:t)) = t
X i=1 M
X"
PROBLEM STATEMENT,0.1271186440677966,"j=1
fi,j (xi,j) ,
(2)"
PROBLEM STATEMENT,0.128060263653484,"where X ‚ààRT √óM is a time series with T time steps and M features, and t is the current time step.
100"
PROBLEM STATEMENT,0.12900188323917136,"This method can handle time series data as input but fails to capture effective temporal patterns
101"
PROBLEM STATEMENT,0.12994350282485875,"because the function fi,j (¬∑) still does not interact with previous time steps. To overcome this problem,
102"
PROBLEM STATEMENT,0.13088512241054615,"we suggest a new form of GAM for time series defined as follows:
103"
PROBLEM STATEMENT,0.1318267419962335,"g (E (yt | X:t)) = t
X i=1 M
X"
PROBLEM STATEMENT,0.1327683615819209,"j=1
fi,j (xi,j, X:t) .
(3)"
PROBLEM STATEMENT,0.1337099811676083,"Definition 3.1 GAMs for time series, which capture temporal patterns hold the form of Equation 3.
104"
PROBLEM STATEMENT,0.13465160075329566,"In Equation (3), the function f (¬∑, ¬∑) can capture interactions between current and previous time steps.
105"
PROBLEM STATEMENT,0.13559322033898305,"Therefore, GAMs adhering to Definition 3.1 are capable of capturing temporal patterns. However,
106"
PROBLEM STATEMENT,0.13653483992467044,"implementing such a model while maintaining transparency poses challenges. In the following
107"
PROBLEM STATEMENT,0.1374764595103578,"section, we will describe our approach to implementing a GAM that holds Definition 3.1. To the best
108"
PROBLEM STATEMENT,0.1384180790960452,"of our knowledge, no existing literature addresses Definition 3.1.
109"
PROBLEM STATEMENT,0.1393596986817326,"4
Our Method: Generalized Additive Time Series Model
110"
ARCHITECTURE,0.14030131826741996,"4.1
Architecture
111"
ARCHITECTURE,0.14124293785310735,"Figure 1 shows the overall architecture of GATSM. Our model has two modules: 1) feature networks,
112"
ARCHITECTURE,0.14218455743879474,"called time-sharing neural basis model, for learning feature representations, and 2) masked multi-head
113"
ARCHITECTURE,0.1431261770244821,"attention for learning temporal patterns.
114"
ARCHITECTURE,0.1440677966101695,"Time-Sharing NBM: Assume a time series with T time steps and M features. Applying GAMs
115"
ARCHITECTURE,0.14500941619585686,"to this time series necessitates T √ó M feature functions, which becomes problematic when dealing
116"
ARCHITECTURE,0.14595103578154425,"with large T or M due to increased model size. This limits the applicability of GAMs to real-world
117"
ARCHITECTURE,0.14689265536723164,"datasets. To overcome this problem, we extend neural basis model (NBM) [9] to time series as:
118"
ARCHITECTURE,0.147834274952919,"Àúxi,j = fj (xi,j) = B
X"
ARCHITECTURE,0.1487758945386064,"k=1
hk (xi,j) wnbm
j,k .
(4)"
ARCHITECTURE,0.1497175141242938,"We refer to this extended version of NBM as time-sharing NBM. Time-sharing NBM has B basis
119"
ARCHITECTURE,0.15065913370998116,"functions, with each basis hk(¬∑) taking a feature xi,j as input. The feature-specific weight wnbm
j,k
120"
ARCHITECTURE,0.15160075329566855,"then projects the basis to the transformed feature Àúxi,j. As depicted in Equation 4, the basis functions
121"
ARCHITECTURE,0.15254237288135594,"are shared across all features and time steps, drastically reducing the number of required feature
122"
ARCHITECTURE,0.1534839924670433,"functions T √ó M to B. We use B = 100 and implement hk (¬∑) using multi-layer perceptron (MLP).
123"
ARCHITECTURE,0.1544256120527307,"Masked MHA: GATSM employs multi-head attention (MHA) to learn temporal patterns. Although
124"
ARCHITECTURE,0.1553672316384181,"the dot product attention [16] is popular, simple dot operation has low expressive power [31].
125"
ARCHITECTURE,0.15630885122410546,"Therefore, we adopt the 2-layer attention mechanism proposed by [31] to GATSM. We first transform
126"
ARCHITECTURE,0.15725047080979285,"Àúxi = [Àúxi,1, Àúxi,2, ¬∑ ¬∑ ¬∑ , Àúxi,M] ‚ààRM produced by Equation 4 as follows:
127"
ARCHITECTURE,0.15819209039548024,"vi = Àúx‚ä∫
i Z + pei,
(5)"
ARCHITECTURE,0.1591337099811676,"where Z ‚ààRM√óD is a learnable weight, pei = [pei,1, pei,2, ¬∑ ¬∑ ¬∑ , pei,D] ‚ààRD is the positional
128"
ARCHITECTURE,0.160075329566855,"encoding for i-th step, and D indicates the hidden size. The positional encoding is defined as follows:
129"
ARCHITECTURE,0.16101694915254236,"pei,j =
sin
 
i
100002j/D

if j mod 2 = 1,
cos
 
i
100002j/D

otherwise.
(6)"
ARCHITECTURE,0.16195856873822975,"The positional encoding helps GATSM effectively capture temporal patterns. While learnable position
130"
ARCHITECTURE,0.16290018832391714,"embedding also works in GATSM, we recommend positional encoding because position embedding
131"
ARCHITECTURE,0.1638418079096045,"requires knowledge of the maximum number of time steps, which is often unknown in real-world
132"
ARCHITECTURE,0.1647834274952919,"settings. After computing vi, we calculate the attention scores as follows:
133"
ARCHITECTURE,0.1657250470809793,"ek,i,j = œÉ
 
[vi | vj]‚ä∫wattn
k

mi,j,
(7)"
ARCHITECTURE,0.16666666666666666,"ak,i,j =
exp (ek,i,j)
PT
t=1 exp (ek,i,t)
,
(8)"
ARCHITECTURE,0.16760828625235405,"where k is attention head index, œÉ (¬∑) is an activation function, wattn
k
‚ààR2D, and mi,j ‚ààR is the
134"
ARCHITECTURE,0.16854990583804144,"mask value used to block future information. The time mask is defined as follows:
135"
ARCHITECTURE,0.1694915254237288,"mi,j =
1
if i ‚â§j,
‚àí‚àû
otherwise.
(9)"
ARCHITECTURE,0.1704331450094162,"Inference: The prediction of GATSM is produced by combining the transformed features from
136"
ARCHITECTURE,0.1713747645951036,"time-sharing NBM with the attention scores from masked MHA.
137 ÀÜyt = K
X"
ARCHITECTURE,0.17231638418079095,"k=1
a‚ä∫
k,t ÀúXwout
k ,
(10)"
ARCHITECTURE,0.17325800376647835,"where K is the number of attention heads, ak,t = [ak,i,1, ak,i,2, ¬∑ ¬∑ ¬∑ , ak,i,T ] ‚ààRT is the attention
138"
ARCHITECTURE,0.17419962335216574,"map in Equation 8, ÀúX = [Àúx1, Àúx2, ¬∑ ¬∑ ¬∑ , ÀúxT ] ‚ààRT √óM is the transformed features in Equation 4, and
139"
ARCHITECTURE,0.1751412429378531,"wout
k
‚ààRM is the learnable output weight.
140"
ARCHITECTURE,0.1760828625235405,"Interpretability: We can rewrite Equation 10 as the following scalar form:
141 K
X"
ARCHITECTURE,0.17702448210922786,"k=1
a‚ä∫
k,t ÀúXwout
k
= t
X u=1 M
X m=1 K
X k=1 B
X"
ARCHITECTURE,0.17796610169491525,"b=1
ak,t,uhb (xt,m) wnbm
m,b wout
k,m = t
X u=1 M
X"
ARCHITECTURE,0.17890772128060264,"m=1
fu,m (xu,m, X:t) (11)"
ARCHITECTURE,0.17984934086629,"Equation 11 shows that GATSM satisfying Definition 3.1. We can derive three types of interpretations
142"
ARCHITECTURE,0.1807909604519774,"from GATSM: 1) ak,t,u indicates the importance of time step u at time step t, 2) hb (xt,m) wnbm
m,b wout
k,m
143"
ARCHITECTURE,0.1817325800376648,"represents the time-independent contribution of feature m, and 3) ak,t,uhb (xt,m) wnbm
m,b wout
k,m repre-
144"
ARCHITECTURE,0.18267419962335216,"sents the time-dependent contribution of feature m at time step t.
145"
EXPERIMENTS,0.18361581920903955,"5
Experiments
146"
EXPERIMENTAL SETUP,0.18455743879472694,"5.1
Experimental Setup
147"
EXPERIMENTAL SETUP,0.1854990583804143,"Datasets: We conducted our experiments using eight publicly available real-world time series
148"
EXPERIMENTAL SETUP,0.1864406779661017,"datasets. From the Monash repository [32], we sourced three datasets: Energy, Rainfall, and
149"
EXPERIMENTAL SETUP,0.1873822975517891,"AirQuality. Another three datasets, Heartbeat, LSST, and NATOPS, were downloaded from the
150"
EXPERIMENTAL SETUP,0.18832391713747645,"UCR repository [33]. The remaining two datasets, Mortality and Sepsis, were downloaded from
151"
EXPERIMENTAL SETUP,0.18926553672316385,"the PhysioNet [34]. We perform ordinal encoding for categorical features and standardize features
152"
EXPERIMENTAL SETUP,0.1902071563088512,"to have zero-mean and unit-variance. For forecasting tasks, target value y is also standardized to
153"
EXPERIMENTAL SETUP,0.1911487758945386,"zero-mean and unit-variance. If the dataset contains missing values, we impute categorical features
154"
EXPERIMENTAL SETUP,0.192090395480226,"with their modes and numerical features with their means. The dataset is split into a 60%/20%/20%
155"
EXPERIMENTAL SETUP,0.19303201506591336,"ratio for training, validation, and testing, respectively. Table 2 shows the statistics of the experimental
156"
EXPERIMENTAL SETUP,0.19397363465160075,"datasets. Further details of the experimental datasets can be found in Appendix B.
157"
EXPERIMENTAL SETUP,0.19491525423728814,Table 2: Dataset statistics.
EXPERIMENTAL SETUP,0.1958568738229755,"Dataset
Task
Variable length
# of time series
Avg. length
# of features
# of classes"
EXPERIMENTAL SETUP,0.1967984934086629,"Energy
1-step FCST
No
137
24
24
-
Rainfall
1-step FCST
No
160,267
24
3
-
AirQuality
1-step FCST
No
16,966
24
9
-
Heartbeat
Binary
No
409
405
61
2
Mortality
Binary
Yes
12,000
49.861
41
2
Sepsis
Binary
Yes
40,336
38.482
40
2
LSST
Multi-class
No
4,925
36
6
14
NATOPS
Multi-class
No
360
51
24
6"
EXPERIMENTAL SETUP,0.1977401129943503,FCST: forecasting
EXPERIMENTAL SETUP,0.19868173258003766,"Baselines: We compare our GATSM with 12 baselines, which can be categorized into four groups: 1)
158"
EXPERIMENTAL SETUP,0.19962335216572505,"Black-box tabular models include extreme gradient boosting (XGBoost) [35] and MLP. 2) Black-box
159"
EXPERIMENTAL SETUP,0.20056497175141244,"time series models include simple recurrent neural network (RNN), gated recurrent unit (GRU), long
160"
EXPERIMENTAL SETUP,0.2015065913370998,"short-term memory (LSTM), and Transformer [16]. 3) Transparent tabular models are simple linear
161"
EXPERIMENTAL SETUP,0.2024482109227872,"model (Linear), explainable boosting machine (EBM) [23], NAM [7], NodeGAM [8], and NBM [9].
162"
EXPERIMENTAL SETUP,0.2033898305084746,"4) NATM [30] is a transparent time series model.
163"
EXPERIMENTAL SETUP,0.20433145009416195,"Implementation: We implement XGBoost and EBM models using the xgboost and interpretml
164"
EXPERIMENTAL SETUP,0.20527306967984935,"libraries, respectively. For NodeGAM, we employ the official implementation provided by its authors
165"
EXPERIMENTAL SETUP,0.2062146892655367,"[8]. The remaining models are developed using PyTorch [36]. All models undergo hyperparameter
166"
EXPERIMENTAL SETUP,0.2071563088512241,Table 3: Predictive performance comparison of various models.
EXPERIMENTAL SETUP,0.2080979284369115,"Model Type
Model
Energy
Rainfall
AirQuality
Heartbeat
Mortality
Sepsis
LSST
NATOPS
Avg. Rank"
EXPERIMENTAL SETUP,0.20903954802259886,"Black-box
Tabular Model"
EXPERIMENTAL SETUP,0.20998116760828625,"XGBoost
0.094
0.002
0.532
0.679
0.707
0.816
0.424
0.200
8.500
(¬±0.137)
(¬±0.002)
(¬±0.019)
(¬±0.094)
(¬±0.015)
(¬±0.007)
(¬±0.012)
(¬±0.049)
(¬±4.000)"
EXPERIMENTAL SETUP,0.21092278719397364,"MLP
0.459
0.011
0.423
0.654
0.842
0.786
0.417
0.211
7.375
(¬±0.101)
(¬±0.004)
(¬±0.031)
(¬±0.082)
(¬±0.014)
(¬±0.007)
(¬±0.008)
(¬±0.065)
(¬±2.134)"
EXPERIMENTAL SETUP,0.211864406779661,"Black-box
Time Series Model"
EXPERIMENTAL SETUP,0.2128060263653484,"RNN
0.320
0.068
0.644
0.661
0.581
0.782
0.422
0.592
7.750
(¬±0.122)
(¬±0.020)
(¬±0.032)
(¬±0.078)
(¬±0.040)
(¬±0.009)
(¬±0.029)
(¬±0.110)
(¬±2.712)"
EXPERIMENTAL SETUP,0.2137476459510358,"GRU
0.435
0.089
0.701
0.694
0.818
0.785
0.629
0.931
4.375
(¬±0.107)
(¬±0.034)
(¬±0.018)
(¬±0.052)
(¬±0.014)
(¬±0.010)
(¬±0.013)
(¬±0.045)
(¬±2.669)"
EXPERIMENTAL SETUP,0.21468926553672316,"LSTM
0.359
0.090
0.683
0.648
0.790
0.779
0.491
0.908
6.375
(¬±0.112)
(¬±0.031)
(¬±0.026)
(¬±0.042)
(¬±0.020)
(¬±0.008)
(¬±0.082)
(¬±0.035)
(¬±3.623)"
EXPERIMENTAL SETUP,0.21563088512241055,"Transformer
0.263
0.098
0.711
0.690
0.844
0.789
0.679
0.967
4.000
(¬±0.263)
(¬±0.035)
(¬±0.027)
(¬±0.040)
(¬±0.019)
(¬±0.010)
(¬±0.019)
(¬±0.029)
(¬±3.703)"
EXPERIMENTAL SETUP,0.21657250470809794,"Transparent
Tabular Model"
EXPERIMENTAL SETUP,0.2175141242937853,"Linear
0.482
0.004
0.241
0.637
0.838
0.723
0.311
0.206
10.125
(¬±0.112)
(¬±0.001)
(¬±0.019)
(¬±0.070)
(¬±0.017)
(¬±0.011)
(¬±0.010)
(¬±0.045)
(¬±3.871)"
EXPERIMENTAL SETUP,0.2184557438794727,"EBM
-0.200
0.004
0.324
0.666
0.729
0.802
0.408
0.164
9.750
(¬±0.409)
(¬±0.001)
(¬±0.014)
(¬±0.056)
(¬±0.017)
(¬±0.011)
(¬±0.016)
(¬±0.053)
(¬±3.284)"
EXPERIMENTAL SETUP,0.2193973634651601,"NAM
0.363
0.006
0.300
0.645
0.853
0.800
0.400
0.242
7.875
(¬±0.218)
(¬±0.002)
(¬±0.013)
(¬±0.026)
(¬±0.014)
(¬±0.006)
(¬±0.011)
(¬±0.040)
(¬±3.643)"
EXPERIMENTAL SETUP,0.22033898305084745,"NodeGAM
0.398
0.006
0.380
0.681
0.854
0.802
0.400
0.247
6.375
(¬±0.195)
(¬±0.002)
(¬±0.032)
(¬±0.046)
(¬±0.013)
(¬±0.007)
(¬±0.028)
(¬±0.012)
(¬±3.623)"
EXPERIMENTAL SETUP,0.22128060263653485,"NBM
0.330
0.007
0.301
0.716
0.852
0.799
0.388
0.189
7.875
(¬±0.251)
(¬±0.003)
(¬±0.012)
(¬±0.039)
(¬±0.014)
(¬±0.006)
(¬±0.014)
(¬±0.029)
(¬±3.603)"
EXPERIMENTAL SETUP,0.2222222222222222,"Transparent
Time Series Model"
EXPERIMENTAL SETUP,0.2231638418079096,"NATM
0.304
0.038
0.548
0.724
N/A
N/A
0.452
0.878
5.667
(¬±0.122)
(¬±0.011)
(¬±0.028)
(¬±0.043)
(¬±0.010)
(¬±0.058)
(¬±2.582)"
EXPERIMENTAL SETUP,0.224105461393597,"GATSM (ours)
0.493
0.073
0.583
0.843
0.853
0.797
0.570
0.956
3.125
(¬±0.173)
(¬±0.027)
(¬±0.026)
(¬±0.025)
(¬±0.015)
(¬±0.007)
(¬±0.024)
(¬±0.027)
(¬±1.808)"
EXPERIMENTAL SETUP,0.22504708097928436,"tuning via Optuna [37]. The pytorch-based models are optimized with the Adam with decoupled
167"
EXPERIMENTAL SETUP,0.22598870056497175,"weight decay (AdamW) [38] optimizer on an NVIDIA A100 GPU. Model training is halted if the
168"
EXPERIMENTAL SETUP,0.22693032015065914,"validation loss does not decrease over 20 epochs. We use mean squared error for the forecasting tasks,
169"
EXPERIMENTAL SETUP,0.2278719397363465,"and for classification tasks, we use cross-entropy loss. Further details of the model implementations
170"
EXPERIMENTAL SETUP,0.2288135593220339,"and hyper-parameters are provided in Appendix C.
171"
COMPARISON WITH BASELINES,0.2297551789077213,"5.2
Comparison with baselines
172"
COMPARISON WITH BASELINES,0.23069679849340866,"Table 3 shows the predictive performances of the experimental models. We report mean scores
173"
COMPARISON WITH BASELINES,0.23163841807909605,"and standard deviations over five different random seeds. For the forecasting datasets, we evaluate
174"
COMPARISON WITH BASELINES,0.23258003766478344,"R2 scores. For the binary classification datasets, we assess the area under the receiver operating
175"
COMPARISON WITH BASELINES,0.2335216572504708,"characteristic curve (AUROC). For the multi-class classification datasets, we measure accuracy. We
176"
COMPARISON WITH BASELINES,0.2344632768361582,"highlight the best-performing model in bold and underline the second-best model. Since the tabular
177"
COMPARISON WITH BASELINES,0.23540489642184556,"models cannot handle time series, they only take xt to produce yt.
178"
COMPARISON WITH BASELINES,0.23634651600753295,"On the Energy and Heartbeat datasets, which are small in size, our GATSM demonstrates the best
179"
COMPARISON WITH BASELINES,0.23728813559322035,"performance, indicating strong generalization ability. EBM, XGBoost, and Transformer struggle
180"
COMPARISON WITH BASELINES,0.2382297551789077,"with overfitting on the Energy dataset. For the Mortality and Sepsis datasets, there is no significant
181"
COMPARISON WITH BASELINES,0.2391713747645951,"performance difference between tabular and time series models, nor between black-box and trans-
182"
COMPARISON WITH BASELINES,0.2401129943502825,"parent models. This suggests that these two healthcare datasets lack significant temporal patterns
183"
COMPARISON WITH BASELINES,0.24105461393596986,"and feature interactions. It is likely that seasonal patterns are hard to detect in medical data, and
184"
COMPARISON WITH BASELINES,0.24199623352165725,"the patient‚Äôs current condition already encapsulates previous conditions, making historical data less
185"
COMPARISON WITH BASELINES,0.24293785310734464,"crucial. Since these datasets contain variable-length time series, the performance of NATM, which
186"
COMPARISON WITH BASELINES,0.243879472693032,"can only handle fixed-length time series, is not available. On the Rainfall, AirQuality, LSST, and
187"
COMPARISON WITH BASELINES,0.2448210922787194,"NATOPS datasets, the time series models significantly outperform the tabular models, indicating
188"
COMPARISON WITH BASELINES,0.2457627118644068,"that these datasets contain important temporal patterns that tabular models cannot capture. Addition-
189"
COMPARISON WITH BASELINES,0.24670433145009416,"ally, the black-box models outperform the transparent models, suggesting that these datasets have
190"
COMPARISON WITH BASELINES,0.24764595103578155,"higher-order feature interactions that transparent models cannot capture. Nevertheless, GATSM is the
191"
COMPARISON WITH BASELINES,0.24858757062146894,"best model within the transparent model group and performs comparably to Transformer. Overall,
192"
COMPARISON WITH BASELINES,0.2495291902071563,"GATSM achieved the best average rank in the experiments, followed by the Transformer, indicating
193"
COMPARISON WITH BASELINES,0.2504708097928437,"GATSM‚Äôs superiority. Additional experiments on model throughput and an ablation study on the
194"
COMPARISON WITH BASELINES,0.2514124293785311,"basis functions are presented in Appendix D.
195"
COMPARISON WITH BASELINES,0.2523540489642185,Table 4: Ablation study on different feature functions.
COMPARISON WITH BASELINES,0.2532956685499058,"Feature Function
Energy
Rainfall
AirQuality
Heartbeat
Mortality
Sepsis
LSST
NATOPS"
COMPARISON WITH BASELINES,0.2542372881355932,"Linear
0.283(¬±0.277)
0.071(¬±0.024)
0.563(¬±0.019)
0.766(¬±0.024)
0.832(¬±0.015)
0.735(¬±0.012)
0.398(¬±0.030)
0.972(¬±0.020)
NAM
0.304(¬±0.229)
0.068(¬±0.021)
0.564(¬±0.019)
0.838(¬±0.032)
0.851(¬±0.013)
0.801(¬±0.005)
0.553(¬±0.023)
0.933(¬±0.039)
NBM
0.493(¬±0.173)
0.073(¬±0.027)
0.583(¬±0.026)
0.843(¬±0.025)
0.853(¬±0.015)
0.797(¬±0.007)
0.570(¬±0.024)
0.956(¬±0.027)"
COMPARISON WITH BASELINES,0.2551789077212806,Table 5: Ablation study on the temporal module.
COMPARISON WITH BASELINES,0.256120527306968,"Temporal Module
Energy
Rainfall
AirQuality
Heartbeat
Mortality
Sepsis
LSST
NATOPS"
COMPARISON WITH BASELINES,0.2570621468926554,"Base
0.452(¬±0.087)
0.007(¬±0.002)
0.299(¬±0.012)
0.661(¬±0.043)
0.854(¬±0.013)
0.798(¬±0.008)
0.392(¬±0.006)
0.192(¬±0.027)
Base + PE
0.397(¬±0.054)
0.007(¬±0.003)
0.299(¬±0.012)
0.681(¬±0.068)
0.852(¬±0.013)
0.799(¬±0.007)
0.385(¬±0.027)
0.228(¬±0.029)
Base + MHA
0.368(¬±0.230)
0.048(¬±0.017)
0.555(¬±0.020)
0.821(¬±0.044)
0.847(¬±0.020)
0.779(¬±0.033)
0.595(¬±0.013)
0.856(¬±0.059)
Base + PE + MHA
0.493(¬±0.173)
0.073(¬±0.027)
0.583(¬±0.026)
0.843(¬±0.025)
0.853(¬±0.015)
0.797(¬±0.007)
0.570(¬±0.024)
0.956(¬±0.027)"
ABLATION STUDY,0.2580037664783427,"5.3
Ablation study
196"
ABLATION STUDY,0.2589453860640301,"Choice of feature function: We evaluate the performance of GATSM by changing the feature
197"
ABLATION STUDY,0.2598870056497175,"functions using three models: Linear, NAM, and NBM. Table 4 presents the results of this experiment.
198"
ABLATION STUDY,0.2608286252354049,"The simple linear function performs poorly because it lacks the capability to capture non-linear
199"
ABLATION STUDY,0.2617702448210923,"relationships. In contrast, NAM, which can capture non-linearity, shows improved performance over
200"
ABLATION STUDY,0.2627118644067797,"the linear function. However, NBM stands out by achieving the best performance in six out of eight
201"
ABLATION STUDY,0.263653483992467,"datasets. This indicates that the basis strategy of NBM is highly effective for time series data.
202"
ABLATION STUDY,0.2645951035781544,"Design of temporal module: We evaluate the performance of GATSM by modifying the design of
203"
ABLATION STUDY,0.2655367231638418,"the temporal module. The results are presented in Table 5. GATSM without the temporal module
204"
ABLATION STUDY,0.2664783427495292,"(Base) fails to learn temporal patterns and shows poor performance in the experiment. GATSM with
205"
ABLATION STUDY,0.2674199623352166,"only positional encoding (Base + PE) also shows similar performance to the Base, indicating that
206"
ABLATION STUDY,0.268361581920904,"positional encoding alone is insufficient for capturing effective temporal patterns. GATSM with only
207"
ABLATION STUDY,0.2693032015065913,"multi-head attention (Base + MHA) outperforms the previous two methods, demonstrating that the
208"
ABLATION STUDY,0.2702448210922787,"MHA mechanism is beneficial for capturing temporal patterns. Finally, our full GATSM (Base + PE +
209"
ABLATION STUDY,0.2711864406779661,"MHA) significantly outperforms the other methods, suggesting that the combination of PE and MHA
210"
ABLATION STUDY,0.2721280602636535,"creates a synergistic effect. Consistent with our previous findings in section 5.2, all four methods
211"
ABLATION STUDY,0.2730696798493409,"show similar performances on the Mortality and Sepsis datasets, which lack significant temporal
212"
ABLATION STUDY,0.2740112994350282,"patterns.
213"
INTERPRETATION,0.2749529190207156,"5.4
Interpretation
214"
INTERPRETATION,0.275894538606403,"In this section, we visualize four interpretations of GATSM‚Äôs predictions on the AirQuality dataset.
215"
INTERPRETATION,0.2768361581920904,"In addition, interpretations for the Rainfall dataset can be found in Appendix E.
216"
INTERPRETATION,0.2777777777777778,"0
10
20
Time steps 0.03 0.04 0.05 0.06 0.07"
INTERPRETATION,0.2787193973634652,Avg. attention score
INTERPRETATION,0.2796610169491525,"Figure 2: Average attention
scores of time steps on the
AirQuality dataset. 0
20 0.00 0.05 0.10 SO2 0
5 0.0 0.1 0.2 NO2 0
5 0.0 0.2"
CO,0.2806026365348399,"0.4
CO 0
10 0.00 0.05 0.10 O3"
CO,0.2815442561205273,"2.5
0.0
2.5 0.050 0.025 0.000 0.025"
CO,0.2824858757062147,temperature
CO,0.2834274952919021,"2.5
0.0
2.5 0.05 0.00 0.05"
CO,0.2843691148775895,pressure
CO,0.2853107344632768,"2.5
0.0 0.000 0.002 0.004"
CO,0.2862523540489642,"dew point 0
50 0.6 0.4 0.2 0.0"
CO,0.2871939736346516,rainfall
CO,0.288135593220339,"0
5
0.0 0.1 0.2"
CO,0.2890772128060264,windspeed
CO,0.2900188323917137,Feature value
CO,0.2909604519774011,Feature contribution
CO,0.2919020715630885,Figure 3: Global interpretations of features in the Air Quality dataset.
CO,0.2928436911487759,"0
10
20"
CO,0.2937853107344633,0.0200
CO,0.2947269303201507,0.0175
CO,0.295668549905838,0.0150
CO,0.2966101694915254,0.0125
CO,0.2975517890772128,0.0100
CO,0.2984934086629002,0.0075
CO,0.2994350282485876,0.0050
CO,0.300376647834275,0.0025
CO,0.3013182674199623,"0.0000
SO2"
CO,0.3022598870056497,"0
10
20 0.03 0.02 0.01 0.00 0.01 0.02 0.03"
CO,0.3032015065913371,"0.04
NO2"
CO,0.3041431261770245,"0
10
20 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01"
CO,0.3050847457627119,"0.00
CO"
CO,0.3060263653483992,"0
10
20 0.02 0.00 0.02 0.04 0.06 0.08 O3"
CO,0.3069679849340866,"0
10
20 0.015 0.010 0.005 0.000 0.005 0.010 0.015"
CO,0.307909604519774,temperature
CO,0.3088512241054614,"0
10
20 0.04 0.02 0.00 0.02 0.04"
CO,0.3097928436911488,pressure
CO,0.3107344632768362,"0
10
20"
CO,0.3116760828625235,0.0015
CO,0.3126177024482109,0.0010
CO,0.3135593220338983,0.0005
CO,0.3145009416195857,0.0000
CO,0.3154425612052731,0.0005
CO,0.3163841807909605,0.0010
CO,0.3173258003766478,0.0015
CO,0.3182674199623352,dew point
CO,0.3192090395480226,"0
10
20 0.014 0.012 0.010 0.008 0.006 0.004 0.002"
RAINFALL,0.32015065913371,"0.000
rainfall"
RAINFALL,0.3210922787193974,"0
10
20 0.008 0.006 0.004 0.002 0.000 0.002"
RAINFALL,0.3220338983050847,windspeed 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1 0 1 2 3 4 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50
RAINFALL,0.3229755178907721,Time step
RAINFALL,0.3239171374764595,Feature contribution
RAINFALL,0.3248587570621469,Feature value
RAINFALL,0.3258003766478343,Figure 4: Local time-independent feature contributions.
RAINFALL,0.3267419962335217,"0
10
20 0.20 0.15 0.10 0.05"
RAINFALL,0.327683615819209,"0.00
SO2"
RAINFALL,0.3286252354048964,"0
10
20 0.10 0.05 0.00 0.05 0.10 NO2"
RAINFALL,0.3295668549905838,"0
10
20
0.7 0.6 0.5 0.4 0.3 0.2 0.1"
CO,0.3305084745762712,"0.0
CO"
CO,0.3314500941619586,"0
10
20
0.1 0.0 0.1 0.2 0.3 0.4"
CO,0.332391713747646,"0.5
O3"
CO,0.3333333333333333,"0
10
20 0.10 0.05 0.00 0.05 0.10"
CO,0.3342749529190207,temperature
CO,0.3352165725047081,"0
10
20 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4"
CO,0.3361581920903955,pressure
CO,0.3370998116760829,"0
10
20 0.015 0.010 0.005 0.000 0.005 0.010 0.015"
CO,0.3380414312617702,dew point
CO,0.3389830508474576,"0
10
20 0.12 0.10 0.08 0.06 0.04 0.02"
RAINFALL,0.339924670433145,"0.00
rainfall"
RAINFALL,0.3408662900188324,"0
10
20 0.06 0.04 0.02 0.00 0.02"
RAINFALL,0.3418079096045198,windspeed 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1.0 0.5 0.0 0.5 1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1 0 1 2 3 4 5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50
RAINFALL,0.3427495291902072,Time step
RAINFALL,0.3436911487758945,Feature contribution
RAINFALL,0.3446327683615819,Feature value
RAINFALL,0.3455743879472693,Figure 5: Local time-dependent feature contributions.
RAINFALL,0.3465160075329567,"Time-step importance: We plot the average attention scores at the last time step T in Figure 2.
217"
RAINFALL,0.3474576271186441,"The process for extracting the average attention score of time step u at time step t is formalized as
218
PK
k=1 ak,t,u. This process is repeated over all data samples, and the results are averaged. Based
219"
RAINFALL,0.3483992467043315,"on Figure 2, it seems that GATSM pays more attention to the initial and last states than to the
220"
RAINFALL,0.3493408662900188,"intermediate states. This indicates that the current concentration of particulate matter depends on the
221"
RAINFALL,0.3502824858757062,"initial state.
222"
RAINFALL,0.3512241054613936,"Global feature contribution:
Figure 3 illustrates the global behavior of features in the
223"
RAINFALL,0.352165725047081,"AirQuality dataset, with red bars indicating the density of training samples.
We extract
224
PK
k=1 hb (xt,m) wnbm
m,b wout
k,m from GATSM and repeat this process over the range of minimum to
225"
RAINFALL,0.3531073446327684,"maximum feature values to plot the line. We found that the behavior of SO2, O3, and windspeed is
226"
RAINFALL,0.3540489642184557,"inconsistent with prior human knowledge. Typically, high levels of SO2 and O3 are associated with
227"
RAINFALL,0.3549905838041431,"poor air quality. However, GATSM learned that particulate matter concentration starts to decrease
228"
RAINFALL,0.3559322033898305,"when SO2 exceeds 10 and O3 exceeds 5. This discrepancy may be due to sparse training samples in
229"
RAINFALL,0.3568738229755179,"these regions, leading to insufficient training, or there may be interactions with other features. Another
230"
RAINFALL,0.3578154425612053,"known fact is that high windspeed decreases particulate matter concentration. This is consistent when
231"
RAINFALL,0.3587570621468927,"windspeed is below 0.7 in our observation. However, particulate matter concentration drastically
232"
RAINFALL,0.35969868173258,"increases when windspeed exceeds 0.7, likely due to the wind causing yellow dust.
233"
RAINFALL,0.3606403013182674,"Local time-independent feature contribution: To interpret the prediction of a data sample, we
234"
RAINFALL,0.3615819209039548,"plot the local time-independent feature contributions, PK
k=1 hb (xt,m) wnbm
m,b wout
k,m, in Figure 4. The
235"
RAINFALL,0.3625235404896422,"main x-axis (blue) represents feature contribution, the sub x-axis (red) represents feature value, and
236"
RAINFALL,0.3634651600753296,"the y-axis represents time steps. We found that SO2, NO2, CO, and O3 have positive correlations.
237"
RAINFALL,0.3644067796610169,"In contrast, temperature, pressure, dew point, and windspeed have negative correlations. These are
238"
RAINFALL,0.3653483992467043,"consistent with the global interpretations shown in Figure 3. Rainfall has the same values across all
239"
RAINFALL,0.3662900188323917,"time steps.
240"
RAINFALL,0.3672316384180791,"Local time-dependent feature contribution: We also visualize the local time-dependent feature con-
241"
RAINFALL,0.3681732580037665,"tributions, PK
k=1 ak,t,uhb (xt,m) wnbm
m,b wout
k,m. Figure 5 illustrates the interpretation of the same data
242"
RAINFALL,0.3691148775894539,"sample as in Figure 4. The time-dependent interpretation differs slightly from the time-independent
243"
RAINFALL,0.3700564971751412,"interpretation. We found that there are time lags in SO2, NO2, CO, and O3, meaning previous feature
244"
RAINFALL,0.3709981167608286,"values affect current feature contributions. For example, in the case of SO2, low feature values around
245"
RAINFALL,0.371939736346516,"time step 5 lead to low feature contributions around time step 13.
246"
FUTURE WORKS & CONCLUSION,0.3728813559322034,"6
Future Works & Conclusion
247"
FUTURE WORKS & CONCLUSION,0.3738229755178908,"Although GATSM achieved state-of-the-art performance within the transparent model category,
248"
FUTURE WORKS & CONCLUSION,0.3747645951035782,"it has several limitations. This section discusses these limitations and suggests future work to
249"
FUTURE WORKS & CONCLUSION,0.3757062146892655,"address them. GAMs have relatively slower computational times and larger model sizes compared to
250"
FUTURE WORKS & CONCLUSION,0.3766478342749529,"black-box models because they require the same number of feature functions as input features. To
251"
FUTURE WORKS & CONCLUSION,0.3775894538606403,"address this problem, methods such as the basis strategy can be proposed to reduce the number of
252"
FUTURE WORKS & CONCLUSION,0.3785310734463277,"feature functions, or entirely new methods for transparent models can be developed. The attention
253"
FUTURE WORKS & CONCLUSION,0.3794726930320151,"mechanism in GATSM may be a bottleneck. Fast attention mechanisms proposed in the literature
254"
FUTURE WORKS & CONCLUSION,0.3804143126177024,"[39, 40, 41, 42, 43], or the recently proposed Mamba [44], can help overcome this limitation. Existing
255"
FUTURE WORKS & CONCLUSION,0.3813559322033898,"time series models, including GATSM, only handle discrete time series and have limited length
256"
FUTURE WORKS & CONCLUSION,0.3822975517890772,"generalization ability, resulting in significantly reduced performance when very long sequences,
257"
FUTURE WORKS & CONCLUSION,0.3832391713747646,"unseen during training, are input. Extending GATSM to continuous models using NeuralODE [45]
258"
FUTURE WORKS & CONCLUSION,0.384180790960452,"or HiPPO [46] could address this issue. GATSM still cannot learn higher-order feature interactions
259"
FUTURE WORKS & CONCLUSION,0.3851224105461394,"internally and shows low performance on complex datasets. Feature interaction methods proposed
260"
FUTURE WORKS & CONCLUSION,0.3860640301318267,"for transparent models may help address this problem [29, 15].
261"
FUTURE WORKS & CONCLUSION,0.3870056497175141,"In this papre, we proposed a novel transparent model for time series named GATSM. GATSM
262"
FUTURE WORKS & CONCLUSION,0.3879472693032015,"consists of time-sharing NBM and the temporal module to effectively learn feature representations
263"
FUTURE WORKS & CONCLUSION,0.3888888888888889,"and temporal patterns while maintaining transparency. The experimental results demonstrated that
264"
FUTURE WORKS & CONCLUSION,0.3898305084745763,"GATSM has superior generalization ability and is the only transparent model with performance
265"
FUTURE WORKS & CONCLUSION,0.3907721280602637,"comparable to Transformer. We provided various visual interpretations of GATSM, demonstrated that
266"
FUTURE WORKS & CONCLUSION,0.391713747645951,"GATSM capture interesting patterns in time series data. We anticipate that GATSM will be widely
267"
FUTURE WORKS & CONCLUSION,0.3926553672316384,"adopted in various fields and demonstrate strong performance. The broader impacts of GATSM
268"
FUTURE WORKS & CONCLUSION,0.3935969868173258,"across various fields can be found in Appendix A.
269"
REFERENCES,0.3945386064030132,"References
270"
REFERENCES,0.3954802259887006,"[1] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ""Why Should I Trust You?"": Explain-
271"
REFERENCES,0.3964218455743879,"ing the Predictions of Any Classifier. In ACM SIGKDD International Conference on Knowledge
272"
REFERENCES,0.3973634651600753,"Discovery and Data Mining, 2016.
273"
REFERENCES,0.3983050847457627,"[2] Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In
274"
REFERENCES,0.3992467043314501,"Advances in Neural Information Processing Systems, 2017.
275"
REFERENCES,0.4001883239171375,"[3] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
276"
REFERENCES,0.4011299435028249,"Parikh, and Dhruv Batra. Grad-CAM: Visual Explanations From Deep Networks via Gradient-
277"
REFERENCES,0.4020715630885122,"Based Localization. 2017.
278"
REFERENCES,0.4030131826741996,"[4] Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining Machine Learning
279"
REFERENCES,0.403954802259887,"Classifiers through Diverse Counterfactual Explanations. In Proceedings of the 2020 Conference
280"
REFERENCES,0.4048964218455744,"on Fairness, Accountability, and Transparency, 2020.
281"
REFERENCES,0.4058380414312618,"[5] Cynthia Rudin. Please Stop Explaining Black Box Models for High Stakes Decisions. In
282"
REFERENCES,0.4067796610169492,"Advances in Neural Information Processing Systems, Workshop on Critiquing and Correcting
283"
REFERENCES,0.4077212806026365,"Trends in Machine Learning, 2018.
284"
REFERENCES,0.4086629001883239,"[6] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions
285"
REFERENCES,0.4096045197740113,"and use interpretable models instead. Nature Machine Intelligence, 1:206‚Äì215, May 2019.
286"
REFERENCES,0.4105461393596987,"[7] Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich
287"
REFERENCES,0.4114877589453861,"Caruana, and Geoffrey E. Hinton. Neural Additive Models: Interpretable Machine Learning
288"
REFERENCES,0.4124293785310734,"with Neural Nets. In Advances in Neural Information Processing Systems, 2021.
289"
REFERENCES,0.4133709981167608,"[8] Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural Generalized
290"
REFERENCES,0.4143126177024482,"Additive Model for Interpretable Deep Learning. In International Conference on Learning
291"
REFERENCES,0.4152542372881356,"Representations, 2022.
292"
REFERENCES,0.416195856873823,"[9] Filip Radenovic, Abhimanyu Dubey, and Dhruv Mahajan. Neural Basis Models for Inter-
293"
REFERENCES,0.4171374764595104,"pretability. In Advances in Neural Information Processing Systems, 2022.
294"
REFERENCES,0.4180790960451977,"[10] Trevor Hastie and Robert Tibshirani. Generalized Additive Models. Statistical Science, 1(3):
295"
REFERENCES,0.4190207156308851,"297‚Äì318, August 1986.
296"
REFERENCES,0.4199623352165725,"[11] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and No√©mie Elhadad. Intel-
297"
REFERENCES,0.4209039548022599,"ligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission.
298"
REFERENCES,0.4218455743879473,"In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.
299"
REFERENCES,0.4227871939736347,"[12] Chun-Hao Chang, Sarah Tan, Ben Lengerich, Anna Goldenberg, and Rich Caruana. How
300"
REFERENCES,0.423728813559322,"Interpretable and Trustworthy are GAMs? In ACM SIGKDD International Conference on
301"
REFERENCES,0.4246704331450094,"Knowledge Discovery and Data Mining, 2021.
302"
REFERENCES,0.4256120527306968,"[13] Lev V. Utkin, Egor D. Satyukov, and Andrei V. Konstantinov. SurvNAM: The machine learning
303"
REFERENCES,0.4265536723163842,"survival model explanation. Neural Networks, 147:81‚Äì102, March 2022.
304"
REFERENCES,0.4274952919020716,"[14] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare: Auditing Black-
305"
REFERENCES,0.4284369114877589,"Box Models Using Transparent Model Distillation. In Proceedings of the 2018 AAAI/ACM
306"
REFERENCES,0.4293785310734463,"Conference on AI, Ethics, and Society, 2018.
307"
REFERENCES,0.4303201506591337,"[15] Minkyu Kim, Hyun-Soo Choi, and Jinho Kim. Higher-order Neural Additive Models: An Inter-
308"
REFERENCES,0.4312617702448211,"pretable Machine Learning Model with Feature Interactions. arXiv preprint arXiv:2209.15409,
309"
REFERENCES,0.4322033898305085,"2022.
310"
REFERENCES,0.4331450094161959,"[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
311"
REFERENCES,0.4340866290018832,"≈Åukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural
312"
REFERENCES,0.4350282485875706,"Information Processing Systems, 2017.
313"
REFERENCES,0.435969868173258,"[17] Sebastian Bach, Alexander Binder, Gr√©goire Montavon, Frederick Klauschen, Klaus-Robert
314"
REFERENCES,0.4369114877589454,"M√ºller, and Wojciech Samek. On Pixel-Wise Explanations for Non-Linear Classifier Decisions
315"
REFERENCES,0.4378531073446328,"by Layer-Wise Relevance Propagation. PLoS ONE, 10(7), July 2015.
316"
REFERENCES,0.4387947269303202,"[18] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning Important Features
317"
REFERENCES,0.4397363465160075,"Through Propagating Activation Differences. In International Conference on Machine Learning,
318"
REFERENCES,0.4406779661016949,"2017.
319"
REFERENCES,0.4416195856873823,"[19] Sajid Ali, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M. Alonso-Moral,
320"
REFERENCES,0.4425612052730697,"Roberto Confalonieri, Riccardo Guidotti, Javier Del Ser, Natalia D√≠az-Rodr√≠guez, and Francisco
321"
REFERENCES,0.4435028248587571,"Herrera. Explainable Artificial Intelligence (XAI): What we know and what is left to attain
322"
REFERENCES,0.4444444444444444,"Trustworthy Artificial Intelligence. Information Fusion, 99:101805, November 2023.
323"
REFERENCES,0.4453860640301318,"[20] Vikas Hassija, Vinay Chamola, Atmesh Mahapatra, Abhinandan Singal, Divyansh Goel, Kaizhu
324"
REFERENCES,0.4463276836158192,"Huang, Simone Scardapane, Indro Spinelli, Mufti Mahmud, and Amir Hussain. Interpreting
325"
REFERENCES,0.4472693032015066,"Black-Box Models: A Review on Explainable Artificial Intelligence. Cognitive Computation,
326"
REFERENCES,0.448210922787194,"16(1):45‚Äì74, January 2024.
327"
REFERENCES,0.4491525423728814,"[21] Grace Wahba. Spline Models for Observational Data. SIAM, September 1990.
328"
REFERENCES,0.4500941619585687,"[22] Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible Models for Classification and
329"
REFERENCES,0.4510357815442561,"Regression. In ACM SIGKDD International Conference on Knowledge Discovery and Data
330"
REFERENCES,0.4519774011299435,"Mining, 2012.
331"
REFERENCES,0.4529190207156309,"[23] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. InterpretML: A Unified Framework
332"
REFERENCES,0.4538606403013183,"for Machine Learning Interpretability. arXiv preprint arXiv:1909.09223, 2019.
333"
REFERENCES,0.4548022598870056,"[24] William J. E. Potts. Generalized Additive Neural Networks. In ACM SIGKDD International
334"
REFERENCES,0.455743879472693,"Conference on Knowledge Discovery and Data Mining, 1999.
335"
REFERENCES,0.4566854990583804,"[25] Shiyun Xu, Zhiqi Bu, Pratik Chaudhari, and Ian J. Barnett. Sparse Neural Additive Model:
336"
REFERENCES,0.4576271186440678,"Interpretable Deep Learning with Feature Selection via Group Sparsity. In Joint European
337"
REFERENCES,0.4585687382297552,"Conference on Machine Learning and Knowledge Discovery in Databases, 2023.
338"
REFERENCES,0.4595103578154426,"[26] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with
339"
REFERENCES,0.4604519774011299,"pairwise interactions. In ACM SIGKDD International Conference on Knowledge Discovery and
340"
REFERENCES,0.4613935969868173,"Data Mining, 2013.
341"
REFERENCES,0.4623352165725047,"[27] Zebin Yang, Aijun Zhang, and Agus Sudjianto. GAMI-Net: An Explainable Neural Network
342"
REFERENCES,0.4632768361581921,"based on Generalized Additive Models with Structured Interactions. Pattern Recognition, 120:
343"
REFERENCES,0.4642184557438795,"108192, December 2021.
344"
REFERENCES,0.4651600753295669,"[28] James Enouen and Yan Liu. Sparse Interaction Additive Networks via Feature Interaction
345"
REFERENCES,0.4661016949152542,"Detection and Sparse Selection. Advances in Neural Information Processing Systems, 35, 2022.
346"
REFERENCES,0.4670433145009416,"[29] Abhimanyu Dubey, Filip Radenovic, and Dhruv Mahajan. Scalable Interpretability via Polyno-
347"
REFERENCES,0.467984934086629,"mials. Advances in Neural Information Processing Systems, 2022.
348"
REFERENCES,0.4689265536723164,"[30] Wonkeun Jo and Dongil Kim. Neural additive time-series models: Explainable deep learning
349"
REFERENCES,0.4698681732580038,"for multivariate time-series prediction. Expert Systems with Applications, 228:120307, October
350"
REFERENCES,0.4708097928436911,"2023.
351"
REFERENCES,0.4717514124293785,"[31] Petar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and Yoshua
352"
REFERENCES,0.4726930320150659,"Bengio. Graph Attention Networks. In International Conference on Learning Representations,
353"
REFERENCES,0.4736346516007533,"2018.
354"
REFERENCES,0.4745762711864407,"[32] Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I. Webb. Monash Univer-
355"
REFERENCES,0.4755178907721281,"sity, UEA, UCR Time Series Extrinsic Regression Archive. arXiv preprint arXiv:2006.10996,
356"
REFERENCES,0.4764595103578154,"2020.
357"
REFERENCES,0.4774011299435028,"[33] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom,
358"
REFERENCES,0.4783427495291902,"Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive,
359"
REFERENCES,0.4792843691148776,"2018. arXiv preprint arXiv:1811.00075, 2018.
360"
REFERENCES,0.480225988700565,"[34] Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov,
361"
REFERENCES,0.4811676082862524,"Roger G. Mark, Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley.
362"
REFERENCES,0.4821092278719397,"PhysioBank, PhysioToolkit, and PhysioNet. Circulation, 101(23):e215‚Äìe220, June 2000.
363"
REFERENCES,0.4830508474576271,"[35] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings
364"
REFERENCES,0.4839924670433145,"of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
365"
REFERENCES,0.4849340866290019,"2016.
366"
REFERENCES,0.4858757062146893,"[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
367"
REFERENCES,0.4868173258003766,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
368"
REFERENCES,0.487758945386064,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
369"
REFERENCES,0.4887005649717514,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,
370"
REFERENCES,0.4896421845574388,"High-Performance Deep Learning Library. In Advances in Neural Information Processing
371"
REFERENCES,0.4905838041431262,"Systems, 2019.
372"
REFERENCES,0.4915254237288136,"[37] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
373"
REFERENCES,0.4924670433145009,"A Next-generation Hyperparameter Optimization Framework. In Proceedings of the 25th ACM
374"
REFERENCES,0.4934086629001883,"SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.
375"
REFERENCES,0.4943502824858757,"[38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International
376"
REFERENCES,0.4952919020715631,"Conference on Learning Representations, 2019.
377"
REFERENCES,0.4962335216572505,"[39] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are
378"
REFERENCES,0.4971751412429379,"RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference
379"
REFERENCES,0.4981167608286252,"on Machine Learning, 2020.
380"
REFERENCES,0.4990583804143126,"[40] Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, and Prateek Jain. Treeformer: Dense
381"
REFERENCES,0.5,"Gradient Trees for Efficient Attention Computation. In International Conference on Learning
382"
REFERENCES,0.5009416195856874,"Representations, 2023.
383"
REFERENCES,0.5018832391713748,"[41] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-Attention
384"
REFERENCES,0.5028248587570622,"with Linear Complexity. arXiv preprint arXiv:2006.04768, 2020.
385"
REFERENCES,0.5037664783427496,"[42] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
386"
REFERENCES,0.504708097928437,"Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger,
387"
REFERENCES,0.5056497175141242,"Lucy Colwell, and Adrian Weller. Rethinking Attention with Performers. In International
388"
REFERENCES,0.5065913370998116,"Conference on Learning Representations, 2021.
389"
REFERENCES,0.507532956685499,"[43] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. In
390"
REFERENCES,0.5084745762711864,"International Conference on Learning Representations, 2020.
391"
REFERENCES,0.5094161958568738,"[44] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces,
392"
REFERENCES,0.5103578154425612,"2023.
393"
REFERENCES,0.5112994350282486,"[45] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural Ordinary
394"
REFERENCES,0.512241054613936,"Differential Equations. In Advances in Neural Information Processing Systems, 2018.
395"
REFERENCES,0.5131826741996234,"[46] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R√©. HiPPO: Recurrent Memory
396"
REFERENCES,0.5141242937853108,"with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems,
397"
REFERENCES,0.5150659133709982,"2020.
398"
REFERENCES,0.5160075329566854,"[47] Eric J. Pedersen, David L. Miller, Gavin L. Simpson, and Noam Ross. Hierarchical generalized
399"
REFERENCES,0.5169491525423728,"additive models in ecology: an introduction with mgcv. PeerJ, 7:e6876, May 2019.
400"
REFERENCES,0.5178907721280602,"[48] Trevor Hastie and Robert Tibshirani. Generalized additive models for medical research. Statisti-
401"
REFERENCES,0.5188323917137476,"cal Methods in Medical Research, 4(3):187‚Äì196, September 1995.
402"
REFERENCES,0.519774011299435,"[49] Appliances Energy Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902637.
403"
REFERENCES,0.5207156308851224,"[50] Australia Rainfall Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902654.
404"
REFERENCES,0.5216572504708098,"[51] Beijing PM10 Dataset, 2020. URL https://doi.org/10.5281/zenodo.3902667.
405"
REFERENCES,0.5225988700564972,"[52] Classification of Heart Sound Recordings: The PhysioNet/Computing in Cardiology Challenge
406"
REFERENCES,0.5235404896421846,"2016, 2016. URL https://physionet.org/content/challenge-2016/1.0.0/.
407"
REFERENCES,0.524482109227872,"[53] Predicting Mortality of ICU Patients: The PhysioNet/Computing in Cardiology Challenge 2012,
408"
REFERENCES,0.5254237288135594,"2012. URL https://physionet.org/content/challenge-2012/1.0.0/.
409"
REFERENCES,0.5263653483992468,"[54] Early Prediction of Sepsis from Clinical Data: The PhysioNet/Computing in Cardiology Chal-
410"
REFERENCES,0.527306967984934,"lenge 2019, 2019. URL https://physionet.org/content/challenge-2019/1.0.0/.
411"
REFERENCES,0.5282485875706214,"[55] PLAsTiCC Astronomical Classification, 2018.
URL https://www.kaggle.com/c/
412"
REFERENCES,0.5291902071563088,"PLAsTiCC-2018.
413"
REFERENCES,0.5301318267419962,"[56] AALTD‚Äô16 Time Series Classification Contest, 2016. URL https://aaltd16.irisa.fr/
414"
REFERENCES,0.5310734463276836,"challenge/.
415"
REFERENCES,0.532015065913371,"[57] Ignacio Oguiza. tsai - a state-of-the-art deep learning library for time series and sequential data.
416"
REFERENCES,0.5329566854990584,"Github, 2023. URL https://github.com/timeseriesAI/tsai.
417"
REFERENCES,0.5338983050847458,"[58] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep
418"
REFERENCES,0.5348399246704332,"Convolutional Neural Networks. In Advances in Neural Information Processing Systems, 2012.
419"
REFERENCES,0.5357815442561206,"[59] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated Residual
420"
REFERENCES,0.536723163841808,"Transformations for Deep Neural Networks.
In Proceedings of the IEEE Conference on
421"
REFERENCES,0.5376647834274952,"Computer Vision and Pattern Recognition, 2017.
422"
REFERENCES,0.5386064030131826,"[60] James Bergstra, R√©mi Bardenet, Yoshua Bengio, and Bal√°zs K√©gl. Algorithms for Hyper-
423"
REFERENCES,0.53954802259887,"Parameter Optimization. In Advances in Neural Information Processing Systems, 2011.
424"
REFERENCES,0.5404896421845574,"[61] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast
425"
REFERENCES,0.5414312617702448,"and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135,
426"
REFERENCES,0.5423728813559322,"2022.
427"
REFERENCES,0.5433145009416196,"NeurIPS Paper Checklist
428"
CLAIMS,0.544256120527307,"1. Claims
429"
CLAIMS,0.5451977401129944,"Question: Do the main claims made in the abstract and introduction accurately reflect the
430"
CLAIMS,0.5461393596986818,"paper‚Äôs contributions and scope?
431"
CLAIMS,0.5470809792843692,"Answer: [Yes]
432"
CLAIMS,0.5480225988700564,"Justification: The main claims made in the abstract and introduction accurately reflect the
433"
CLAIMS,0.5489642184557438,"paper‚Äôs contributions and scope.
434"
CLAIMS,0.5499058380414312,"Guidelines:
435"
CLAIMS,0.5508474576271186,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
436"
CLAIMS,0.551789077212806,"made in the paper.
437"
CLAIMS,0.5527306967984934,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
438"
CLAIMS,0.5536723163841808,"contributions made in the paper and important assumptions and limitations. A No or
439"
CLAIMS,0.5546139359698682,"NA answer to this question will not be perceived well by the reviewers.
440"
CLAIMS,0.5555555555555556,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
441"
CLAIMS,0.556497175141243,"much the results can be expected to generalize to other settings.
442"
CLAIMS,0.5574387947269304,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
443"
CLAIMS,0.5583804143126178,"are not attained by the paper.
444"
LIMITATIONS,0.559322033898305,"2. Limitations
445"
LIMITATIONS,0.5602636534839924,"Question: Does the paper discuss the limitations of the work performed by the authors?
446"
LIMITATIONS,0.5612052730696798,"Answer: [Yes]
447"
LIMITATIONS,0.5621468926553672,"Justification: The limitations of our work are described in section 6.
448"
LIMITATIONS,0.5630885122410546,"Guidelines:
449"
LIMITATIONS,0.564030131826742,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
450"
LIMITATIONS,0.5649717514124294,"the paper has limitations, but those are not discussed in the paper.
451"
LIMITATIONS,0.5659133709981168,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
452"
LIMITATIONS,0.5668549905838042,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
453"
LIMITATIONS,0.5677966101694916,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
454"
LIMITATIONS,0.568738229755179,"model well-specification, asymptotic approximations only holding locally). The authors
455"
LIMITATIONS,0.5696798493408662,"should reflect on how these assumptions might be violated in practice and what the
456"
LIMITATIONS,0.5706214689265536,"implications would be.
457"
LIMITATIONS,0.571563088512241,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
458"
LIMITATIONS,0.5725047080979284,"only tested on a few datasets or with a few runs. In general, empirical results often
459"
LIMITATIONS,0.5734463276836158,"depend on implicit assumptions, which should be articulated.
460"
LIMITATIONS,0.5743879472693032,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
461"
LIMITATIONS,0.5753295668549906,"For example, a facial recognition algorithm may perform poorly when image resolution
462"
LIMITATIONS,0.576271186440678,"is low or images are taken in low lighting. Or a speech-to-text system might not be
463"
LIMITATIONS,0.5772128060263654,"used reliably to provide closed captions for online lectures because it fails to handle
464"
LIMITATIONS,0.5781544256120528,"technical jargon.
465"
LIMITATIONS,0.5790960451977402,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
466"
LIMITATIONS,0.5800376647834274,"and how they scale with dataset size.
467"
LIMITATIONS,0.5809792843691148,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
468"
LIMITATIONS,0.5819209039548022,"address problems of privacy and fairness.
469"
LIMITATIONS,0.5828625235404896,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
470"
LIMITATIONS,0.583804143126177,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
471"
LIMITATIONS,0.5847457627118644,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
472"
LIMITATIONS,0.5856873822975518,"judgment and recognize that individual actions in favor of transparency play an impor-
473"
LIMITATIONS,0.5866290018832392,"tant role in developing norms that preserve the integrity of the community. Reviewers
474"
LIMITATIONS,0.5875706214689266,"will be specifically instructed to not penalize honesty concerning limitations.
475"
THEORY ASSUMPTIONS AND PROOFS,0.588512241054614,"3. Theory Assumptions and Proofs
476"
THEORY ASSUMPTIONS AND PROOFS,0.5894538606403014,"Question: For each theoretical result, does the paper provide the full set of assumptions and
477"
THEORY ASSUMPTIONS AND PROOFS,0.5903954802259888,"a complete (and correct) proof?
478"
THEORY ASSUMPTIONS AND PROOFS,0.591337099811676,"Answer: [NA]
479"
THEORY ASSUMPTIONS AND PROOFS,0.5922787193973634,"Justification: Our work does not include theoretical results.
480"
THEORY ASSUMPTIONS AND PROOFS,0.5932203389830508,"Guidelines:
481"
THEORY ASSUMPTIONS AND PROOFS,0.5941619585687382,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
482"
THEORY ASSUMPTIONS AND PROOFS,0.5951035781544256,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
483"
THEORY ASSUMPTIONS AND PROOFS,0.596045197740113,"referenced.
484"
THEORY ASSUMPTIONS AND PROOFS,0.5969868173258004,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
485"
THEORY ASSUMPTIONS AND PROOFS,0.5979284369114878,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
486"
THEORY ASSUMPTIONS AND PROOFS,0.5988700564971752,"they appear in the supplemental material, the authors are encouraged to provide a short
487"
THEORY ASSUMPTIONS AND PROOFS,0.5998116760828626,"proof sketch to provide intuition.
488"
THEORY ASSUMPTIONS AND PROOFS,0.60075329566855,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
489"
THEORY ASSUMPTIONS AND PROOFS,0.6016949152542372,"by formal proofs provided in appendix or supplemental material.
490"
THEORY ASSUMPTIONS AND PROOFS,0.6026365348399246,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
491"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.603578154425612,"4. Experimental Result Reproducibility
492"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6045197740112994,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6054613935969868,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6064030131826742,"of the paper (regardless of whether the code and data are provided or not)?
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6073446327683616,"Answer: [Yes]
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.608286252354049,"Justification: We provided experimental setup and implementation details in section 5.1 and
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6092278719397364,"Appendix C.
498"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6101694915254238,"Guidelines:
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6111111111111112,"‚Ä¢ The answer NA means that the paper does not include experiments.
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6120527306967984,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6129943502824858,"well by the reviewers: Making the paper reproducible is important, regardless of
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6139359698681732,"whether the code and data are provided or not.
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6148775894538606,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.615819209039548,"to make their results reproducible or verifiable.
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6167608286252354,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6177024482109228,"For example, if the contribution is a novel architecture, describing the architecture fully
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6186440677966102,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
508"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6195856873822976,"be necessary to either make it possible for others to replicate the model with the same
509"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.620527306967985,"dataset, or provide access to the model. In general. releasing code and data is often
510"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6214689265536724,"one good way to accomplish this, but reproducibility can also be provided via detailed
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6224105461393596,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.623352165725047,"of a large language model), releasing of a model checkpoint, or other means that are
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6242937853107344,"appropriate to the research performed.
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6252354048964218,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6261770244821092,"sions to provide some reasonable avenue for reproducibility, which may depend on the
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6271186440677966,"nature of the contribution. For example
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.628060263653484,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6290018832391714,"to reproduce that algorithm.
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6299435028248588,"(b) If the contribution is primarily a new model architecture, the paper should describe
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6308851224105462,"the architecture clearly and fully.
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6318267419962336,"(c) If the contribution is a new model (e.g., a large language model), then there should
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.632768361581921,"either be a way to access this model for reproducing the results or a way to reproduce
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6337099811676082,"the model (e.g., with an open-source dataset or instructions for how to construct
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6346516007532956,"the dataset).
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.635593220338983,"(d) We recognize that reproducibility may be tricky in some cases, in which case
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6365348399246704,"authors are welcome to describe the particular way they provide for reproducibility.
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6374764595103578,"In the case of closed-source models, it may be that access to the model is limited in
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6384180790960452,"some way (e.g., to registered users), but it should be possible for other researchers
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6393596986817326,"to have some path to reproducing or verifying the results.
530"
OPEN ACCESS TO DATA AND CODE,0.64030131826742,"5. Open access to data and code
531"
OPEN ACCESS TO DATA AND CODE,0.6412429378531074,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
532"
OPEN ACCESS TO DATA AND CODE,0.6421845574387948,"tions to faithfully reproduce the main experimental results, as described in supplemental
533"
OPEN ACCESS TO DATA AND CODE,0.6431261770244822,"material?
534"
OPEN ACCESS TO DATA AND CODE,0.6440677966101694,"Answer: [Yes]
535"
OPEN ACCESS TO DATA AND CODE,0.6450094161958568,"Justification: We used public datasets and opened our code.
536"
OPEN ACCESS TO DATA AND CODE,0.6459510357815442,"Guidelines:
537"
OPEN ACCESS TO DATA AND CODE,0.6468926553672316,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
538"
OPEN ACCESS TO DATA AND CODE,0.647834274952919,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
539"
OPEN ACCESS TO DATA AND CODE,0.6487758945386064,"public/guides/CodeSubmissionPolicy) for more details.
540"
OPEN ACCESS TO DATA AND CODE,0.6497175141242938,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
541"
OPEN ACCESS TO DATA AND CODE,0.6506591337099812,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
542"
OPEN ACCESS TO DATA AND CODE,0.6516007532956686,"including code, unless this is central to the contribution (e.g., for a new open-source
543"
OPEN ACCESS TO DATA AND CODE,0.652542372881356,"benchmark).
544"
OPEN ACCESS TO DATA AND CODE,0.6534839924670434,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
545"
OPEN ACCESS TO DATA AND CODE,0.6544256120527306,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
546"
OPEN ACCESS TO DATA AND CODE,0.655367231638418,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
547"
OPEN ACCESS TO DATA AND CODE,0.6563088512241054,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
548"
OPEN ACCESS TO DATA AND CODE,0.6572504708097928,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
549"
OPEN ACCESS TO DATA AND CODE,0.6581920903954802,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
550"
OPEN ACCESS TO DATA AND CODE,0.6591337099811676,"proposed method and baselines. If only a subset of experiments are reproducible, they
551"
OPEN ACCESS TO DATA AND CODE,0.660075329566855,"should state which ones are omitted from the script and why.
552"
OPEN ACCESS TO DATA AND CODE,0.6610169491525424,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
553"
OPEN ACCESS TO DATA AND CODE,0.6619585687382298,"versions (if applicable).
554"
OPEN ACCESS TO DATA AND CODE,0.6629001883239172,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
555"
OPEN ACCESS TO DATA AND CODE,0.6638418079096046,"paper) is recommended, but including URLs to data and code is permitted.
556"
OPEN ACCESS TO DATA AND CODE,0.664783427495292,"6. Experimental Setting/Details
557"
OPEN ACCESS TO DATA AND CODE,0.6657250470809792,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
558"
OPEN ACCESS TO DATA AND CODE,0.6666666666666666,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
559"
OPEN ACCESS TO DATA AND CODE,0.667608286252354,"results?
560"
OPEN ACCESS TO DATA AND CODE,0.6685499058380414,"Answer: [Yes]
561"
OPEN ACCESS TO DATA AND CODE,0.6694915254237288,"Justification: We described the experimental setting in section 5.1.
562"
OPEN ACCESS TO DATA AND CODE,0.6704331450094162,"Guidelines:
563"
OPEN ACCESS TO DATA AND CODE,0.6713747645951036,"‚Ä¢ The answer NA means that the paper does not include experiments.
564"
OPEN ACCESS TO DATA AND CODE,0.672316384180791,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
565"
OPEN ACCESS TO DATA AND CODE,0.6732580037664784,"that is necessary to appreciate the results and make sense of them.
566"
OPEN ACCESS TO DATA AND CODE,0.6741996233521658,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
567"
OPEN ACCESS TO DATA AND CODE,0.6751412429378532,"material.
568"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6760828625235404,"7. Experiment Statistical Significance
569"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6770244821092278,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
570"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6779661016949152,"information about the statistical significance of the experiments?
571"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6789077212806026,"Answer: [Yes]
572"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.67984934086629,"Justification: We provided standard deviations with experimental results.
573"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6807909604519774,"Guidelines:
574"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6817325800376648,"‚Ä¢ The answer NA means that the paper does not include experiments.
575"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6826741996233522,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
576"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6836158192090396,"dence intervals, or statistical significance tests, at least for the experiments that support
577"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.684557438794727,"the main claims of the paper.
578"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6854990583804144,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
579"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6864406779661016,"example, train/test split, initialization, random drawing of some parameter, or overall
580"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.687382297551789,"run with given experimental conditions).
581"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6883239171374764,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
582"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6892655367231638,"call to a library function, bootstrap, etc.)
583"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6902071563088512,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
584"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6911487758945386,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
585"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.692090395480226,"of the mean.
586"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6930320150659134,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
587"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6939736346516008,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
588"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6949152542372882,"of Normality of errors is not verified.
589"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6958568738229756,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
590"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.696798493408663,"figures symmetric error bars that would yield results that are out of range (e.g. negative
591"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6977401129943502,"error rates).
592"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6986817325800376,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
593"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.699623352165725,"they were calculated and reference the corresponding figures or tables in the text.
594"
EXPERIMENTS COMPUTE RESOURCES,0.7005649717514124,"8. Experiments Compute Resources
595"
EXPERIMENTS COMPUTE RESOURCES,0.7015065913370998,"Question: For each experiment, does the paper provide sufficient information on the com-
596"
EXPERIMENTS COMPUTE RESOURCES,0.7024482109227872,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
597"
EXPERIMENTS COMPUTE RESOURCES,0.7033898305084746,"the experiments?
598"
EXPERIMENTS COMPUTE RESOURCES,0.704331450094162,"Answer: [Yes]
599"
EXPERIMENTS COMPUTE RESOURCES,0.7052730696798494,"Justification: We provided information on the computational resource used in the experi-
600"
EXPERIMENTS COMPUTE RESOURCES,0.7062146892655368,"ments.
601"
EXPERIMENTS COMPUTE RESOURCES,0.7071563088512242,"Guidelines:
602"
EXPERIMENTS COMPUTE RESOURCES,0.7080979284369114,"‚Ä¢ The answer NA means that the paper does not include experiments.
603"
EXPERIMENTS COMPUTE RESOURCES,0.7090395480225988,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
604"
EXPERIMENTS COMPUTE RESOURCES,0.7099811676082862,"or cloud provider, including relevant memory and storage.
605"
EXPERIMENTS COMPUTE RESOURCES,0.7109227871939736,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
606"
EXPERIMENTS COMPUTE RESOURCES,0.711864406779661,"experimental runs as well as estimate the total compute.
607"
EXPERIMENTS COMPUTE RESOURCES,0.7128060263653484,"‚Ä¢ The paper should disclose whether the full research project required more compute
608"
EXPERIMENTS COMPUTE RESOURCES,0.7137476459510358,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
609"
EXPERIMENTS COMPUTE RESOURCES,0.7146892655367232,"didn‚Äôt make it into the paper).
610"
CODE OF ETHICS,0.7156308851224106,"9. Code Of Ethics
611"
CODE OF ETHICS,0.716572504708098,"Question: Does the research conducted in the paper conform, in every respect, with the
612"
CODE OF ETHICS,0.7175141242937854,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
613"
CODE OF ETHICS,0.7184557438794726,"Answer: [Yes]
614"
CODE OF ETHICS,0.71939736346516,"Justification: Our work conform with the NeurIPS Code of Ethics.
615"
CODE OF ETHICS,0.7203389830508474,"Guidelines:
616"
CODE OF ETHICS,0.7212806026365348,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
617"
CODE OF ETHICS,0.7222222222222222,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
618"
CODE OF ETHICS,0.7231638418079096,"deviation from the Code of Ethics.
619"
CODE OF ETHICS,0.724105461393597,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
620"
CODE OF ETHICS,0.7250470809792844,"eration due to laws or regulations in their jurisdiction).
621"
BROADER IMPACTS,0.7259887005649718,"10. Broader Impacts
622"
BROADER IMPACTS,0.7269303201506592,"Question: Does the paper discuss both potential positive societal impacts and negative
623"
BROADER IMPACTS,0.7278719397363466,"societal impacts of the work performed?
624"
BROADER IMPACTS,0.7288135593220338,"Answer: [Yes]
625"
BROADER IMPACTS,0.7297551789077212,"Justification: We discussed the potential impacts of GATSM in Appendix A.
626"
BROADER IMPACTS,0.7306967984934086,"Guidelines:
627"
BROADER IMPACTS,0.731638418079096,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
628"
BROADER IMPACTS,0.7325800376647834,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
629"
BROADER IMPACTS,0.7335216572504708,"impact or why the paper does not address societal impact.
630"
BROADER IMPACTS,0.7344632768361582,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
631"
BROADER IMPACTS,0.7354048964218456,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
632"
BROADER IMPACTS,0.736346516007533,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
633"
BROADER IMPACTS,0.7372881355932204,"groups), privacy considerations, and security considerations.
634"
BROADER IMPACTS,0.7382297551789078,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
635"
BROADER IMPACTS,0.7391713747645952,"to particular applications, let alone deployments. However, if there is a direct path to
636"
BROADER IMPACTS,0.7401129943502824,"any negative applications, the authors should point it out. For example, it is legitimate
637"
BROADER IMPACTS,0.7410546139359698,"to point out that an improvement in the quality of generative models could be used to
638"
BROADER IMPACTS,0.7419962335216572,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
639"
BROADER IMPACTS,0.7429378531073446,"that a generic algorithm for optimizing neural networks could enable people to train
640"
BROADER IMPACTS,0.743879472693032,"models that generate Deepfakes faster.
641"
BROADER IMPACTS,0.7448210922787194,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
642"
BROADER IMPACTS,0.7457627118644068,"being used as intended and functioning correctly, harms that could arise when the
643"
BROADER IMPACTS,0.7467043314500942,"technology is being used as intended but gives incorrect results, and harms following
644"
BROADER IMPACTS,0.7476459510357816,"from (intentional or unintentional) misuse of the technology.
645"
BROADER IMPACTS,0.748587570621469,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
646"
BROADER IMPACTS,0.7495291902071564,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
647"
BROADER IMPACTS,0.7504708097928436,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
648"
BROADER IMPACTS,0.751412429378531,"feedback over time, improving the efficiency and accessibility of ML).
649"
SAFEGUARDS,0.7523540489642184,"11. Safeguards
650"
SAFEGUARDS,0.7532956685499058,"Question: Does the paper describe safeguards that have been put in place for responsible
651"
SAFEGUARDS,0.7542372881355932,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
652"
SAFEGUARDS,0.7551789077212806,"image generators, or scraped datasets)?
653"
SAFEGUARDS,0.756120527306968,"Answer: [NA]
654"
SAFEGUARDS,0.7570621468926554,"Justification: Our work poses no such risks.
655"
SAFEGUARDS,0.7580037664783428,"Guidelines:
656"
SAFEGUARDS,0.7589453860640302,"‚Ä¢ The answer NA means that the paper poses no such risks.
657"
SAFEGUARDS,0.7598870056497176,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
658"
SAFEGUARDS,0.7608286252354048,"necessary safeguards to allow for controlled use of the model, for example by requiring
659"
SAFEGUARDS,0.7617702448210922,"that users adhere to usage guidelines or restrictions to access the model or implementing
660"
SAFEGUARDS,0.7627118644067796,"safety filters.
661"
SAFEGUARDS,0.763653483992467,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
662"
SAFEGUARDS,0.7645951035781544,"should describe how they avoided releasing unsafe images.
663"
SAFEGUARDS,0.7655367231638418,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
664"
SAFEGUARDS,0.7664783427495292,"not require this, but we encourage authors to take this into account and make a best
665"
SAFEGUARDS,0.7674199623352166,"faith effort.
666"
LICENSES FOR EXISTING ASSETS,0.768361581920904,"12. Licenses for existing assets
667"
LICENSES FOR EXISTING ASSETS,0.7693032015065914,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
668"
LICENSES FOR EXISTING ASSETS,0.7702448210922788,"the paper, properly credited and are the license and terms of use explicitly mentioned and
669"
LICENSES FOR EXISTING ASSETS,0.7711864406779662,"properly respected?
670"
LICENSES FOR EXISTING ASSETS,0.7721280602636534,"Answer: [Yes]
671"
LICENSES FOR EXISTING ASSETS,0.7730696798493408,"Justification: We properly cited the used codes and data.
672"
LICENSES FOR EXISTING ASSETS,0.7740112994350282,"Guidelines:
673"
LICENSES FOR EXISTING ASSETS,0.7749529190207156,"‚Ä¢ The answer NA means that the paper does not use existing assets.
674"
LICENSES FOR EXISTING ASSETS,0.775894538606403,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
675"
LICENSES FOR EXISTING ASSETS,0.7768361581920904,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
676"
LICENSES FOR EXISTING ASSETS,0.7777777777777778,"URL.
677"
LICENSES FOR EXISTING ASSETS,0.7787193973634652,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
678"
LICENSES FOR EXISTING ASSETS,0.7796610169491526,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
679"
LICENSES FOR EXISTING ASSETS,0.78060263653484,"service of that source should be provided.
680"
LICENSES FOR EXISTING ASSETS,0.7815442561205274,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
681"
LICENSES FOR EXISTING ASSETS,0.7824858757062146,"package should be provided. For popular datasets, paperswithcode.com/datasets
682"
LICENSES FOR EXISTING ASSETS,0.783427495291902,"has curated licenses for some datasets. Their licensing guide can help determine the
683"
LICENSES FOR EXISTING ASSETS,0.7843691148775894,"license of a dataset.
684"
LICENSES FOR EXISTING ASSETS,0.7853107344632768,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
685"
LICENSES FOR EXISTING ASSETS,0.7862523540489642,"the derived asset (if it has changed) should be provided.
686"
LICENSES FOR EXISTING ASSETS,0.7871939736346516,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
687"
LICENSES FOR EXISTING ASSETS,0.788135593220339,"the asset‚Äôs creators.
688"
NEW ASSETS,0.7890772128060264,"13. New Assets
689"
NEW ASSETS,0.7900188323917138,"Question: Are new assets introduced in the paper well documented and is the documentation
690"
NEW ASSETS,0.7909604519774012,"provided alongside the assets?
691"
NEW ASSETS,0.7919020715630886,"Answer: [Yes]
692"
NEW ASSETS,0.7928436911487758,"Justification: We opened the source code of GATSM, and the document to run the code is
693"
NEW ASSETS,0.7937853107344632,"provided along with the code.
694"
NEW ASSETS,0.7947269303201506,"Guidelines:
695"
NEW ASSETS,0.795668549905838,"‚Ä¢ The answer NA means that the paper does not release new assets.
696"
NEW ASSETS,0.7966101694915254,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
697"
NEW ASSETS,0.7975517890772128,"submissions via structured templates. This includes details about training, license,
698"
NEW ASSETS,0.7984934086629002,"limitations, etc.
699"
NEW ASSETS,0.7994350282485876,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
700"
NEW ASSETS,0.800376647834275,"asset is used.
701"
NEW ASSETS,0.8013182674199624,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
702"
NEW ASSETS,0.8022598870056498,"create an anonymized URL or include an anonymized zip file.
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.803201506591337,"14. Crowdsourcing and Research with Human Subjects
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8041431261770244,"Question: For crowdsourcing experiments and research with human subjects, does the paper
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8050847457627118,"include the full text of instructions given to participants and screenshots, if applicable, as
706"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8060263653483992,"well as details about compensation (if any)?
707"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8069679849340866,"Answer: [NA]
708"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.807909604519774,"Justification: Our work does not involve crowdsourcing nor research with human subjects.
709"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8088512241054614,"Guidelines:
710"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8097928436911488,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
711"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8107344632768362,"human subjects.
712"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8116760828625236,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
713"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.812617702448211,"tion of the paper involves human subjects, then as much detail as possible should be
714"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8135593220338984,"included in the main paper.
715"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8145009416195856,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.815442561205273,"or other labor should be paid at least the minimum wage in the country of the data
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8163841807909604,"collector.
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8173258003766478,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8182674199623352,"Subjects
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8192090395480226,"Question: Does the paper describe potential risks incurred by study participants, whether
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.82015065913371,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8210922787193974,"approvals (or an equivalent approval/review based on the requirements of your country or
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8220338983050848,"institution) were obtained?
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8229755178907722,"Answer: [NA]
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8239171374764596,"Justification: Our work does not involve crowdsourcing nor research with human subjects.
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8248587570621468,"Guidelines:
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8258003766478342,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8267419962335216,"human subjects.
729"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.827683615819209,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
730"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8286252354048964,"may be required for any human subjects research. If you obtained IRB approval, you
731"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8295668549905838,"should clearly state this in the paper.
732"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8305084745762712,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
733"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8314500941619586,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.832391713747646,"guidelines for their institution.
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8333333333333334,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8342749529190208,"applicable), such as the institution conducting the review.
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.835216572504708,"A
Broader impact
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8361581920903954,"We discuss the expected impacts of GATSM across various fields.
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8370998116760828,"‚Ä¢ Time series adaptation: GATSM extends existing GAMs to time series, enabling tasks that
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8380414312617702,"traditional GAMs could not perform in this context - e.g., better performance on time series and
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8389830508474576,"finding temporal patterns.
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.839924670433145,"‚Ä¢ Improved decision-making system: GATSM can show users their exact decision-making process,
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8408662900188324,"providing trust and confidence in its predictions to users. This enables decision-makers to make
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8418079096045198,"more informed choices, crucial in high-stakes domains such as healthcare.
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8427495291902072,"‚Ä¢ Ethical AI: GATSM can examine that their outcomes are biased or discriminatory by displaying
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8436911487758946,"the shape of feature functions. This is particularly important in ethically sensitive domains, such as
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.844632768361582,"recidivism prediction.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8455743879472694,"‚Ä¢ Scientific discovery: Transparent models have already been used in various research fields for
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8465160075329566,"scientific discovery [47, 48]. GATSM also can be applied to these domains to obtain novel scientific
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.847457627118644,"insights.
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8483992467043314,"Despite these advantages, it is important to remember that the interpretations of transparent models
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8493408662900188,"do not necessarily reflect exact causal relationships. While transparent models provide clear and
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8502824858757062,"faithful interpretations, they are still not capable of identifying causal relationships. Causal discovery
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8512241054613936,"is a complex task that requires further research.
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.852165725047081,"B
Dataset details
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8531073446327684,"We use eight publicly available datasets for our experiments. Three datasets - Energy, Rainfall, and
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8540489642184558,"AirQuality - can be downloaded from the Monash repository [32]. Another three datasets - Heartbeat,
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8549905838041432,"LSST, and NATOPS - are available from the UCR repository [33]. The remaining two datasets can
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8559322033898306,"be downloaded from the PhysioNet [34]. Details of the datasets are provided below:
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8568738229755178,"‚Ä¢ Energy [49]: This dataset consists of 24 features related to temperature and humidity from sensors
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8578154425612052,"and weather conditions. These features are measured every 10 minutes. The goal of this dataset is
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8587570621468926,"to predict total energy usage.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.85969868173258,"‚Ä¢ Rainfall [50]: This dataset consists of temperatures measured hourly. The goal of this dataset is to
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8606403013182674,"predict total daily rainfall in Australia.
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8615819209039548,"‚Ä¢ AirQuality [51]: This dataset consists of features related to air pollutants and meteorological data.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8625235404896422,"The goal of this dataset is to predict the PM10 level in Beijing.
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8634651600753296,"‚Ä¢ Heartbeat [52]: This dataset consists of heart sounds collected from various locations on the body.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.864406779661017,"Each sound was truncated to five seconds, and a spectrogram of each instance was created with a
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8653483992467044,"window size of 0.061 seconds with a 70% overlap. The goal of this dataset is to classify the sounds
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8662900188323918,"as either normal or abnormal.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.867231638418079,"‚Ä¢ Mortality [53] This dataset consists of records of adult patients admitted to the ICU. The input
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8681732580037664,"features include the patient demographics, vital signs, and lab results. The goal of this dataset is to
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8691148775894538,"predict the in-hospital death of patients.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8700564971751412,"‚Ä¢ Sepsis [54]: This dataset consists of records of ICU patients. The input features include patient
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8709981167608286,"demographics, vital signs, and lab results. The goal of this dataset is to predict sepsis six hours in
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.871939736346516,"advance at every time step.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8728813559322034,"‚Ä¢ LSST [55]: This challenge dataset aims to classify astronomical time series. These time series
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8738229755178908,"consist of six different light curves, simulated based on the data expected from the Large Synoptic
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8747645951035782,"Survey Telescope (LSST).
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8757062146892656,"‚Ä¢ NATOPS [56]: This dataset aims to classify the Naval Air Training and Operating Procedures
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.876647834274953,"Standardization (NATOPS) motions used to control aircraft movements. It consists of 24 features
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8775894538606404,"representing the x, y, and z coordinates for each of the eight sensor locations attached to the body.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8785310734463276,"We used get_UCR_data() and get_Monash_regression_data() functions in the tsai library
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.879472693032015,"[57] to load the UCR and Monash datasets.
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8804143126177024,Table 6: Optimal hyper-parameters for GATSM.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8813559322033898,"GATSM: [256, 256, 128] hidden dims, 100 basis functions"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8822975517890772,"Dataset
Batch Size
NBM Batch Norm.
NBM Dropout
Attn. Embedding Size
Attn. Heads
Attn. Dropout
Learning Rate
Weight Decay"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8832391713747646,"Energy
32
False
2.315e-1
110
8
6.924e-2
4.950e-3
1.679e-3
Rainfall
32,768
False
5.936e-3
44
7
1.215e-3
9.225e-3
2.204e-6
AirQuality
4,096
False
2.340e-2
81
8
1.169e-1
6.076e-3
5.047e-6
Heartbeat
64
True
1.749e-1
92
2
1.653e-1
8.061e-3
4.787e-6
Mortality
512
False
7.151e-2
125
8
7.324e-1
7.304e-3
2.181e-4
Sepsis
512
True
6.523e-2
90
6
8.992e-1
4.509e-3
2.259e-2
LSST
1,024
False
2.500e-2
59
7
2.063e-1
5.561e-2
5.957e-3
NATOPS
64
True
4.827e-3
49
8
7.920e-1
8.156e-3
2.748e-2"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.884180790960452,"C
Implementation details
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8851224105461394,"We use 13 models, including GATSM, for our experiments. We implement XGBoost and EBM
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8860640301318268,"using the xgboost [35] and interpretml [23] libraries, respectively. For NodeGAM, we employ
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8870056497175142,"the official implementation provided by its authors [8]. The remaining models are developed using
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8879472693032016,"PyTorch [36]. In addition, we implement the feature functions in NAM and NBM using grouped
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8888888888888888,"convolutions [58, 59] to enhance their efficiency. XGBoost and EBM are trained on two AMD EPYC
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8898305084745762,"7513 CPUs, while the other models are trained on an NVIDIA A100 GPU with 80GB VRAM. All
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8907721280602636,"models undergo hyperparameter tuning via Optuna [37] with the Tree-structured Parzen Estimator
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.891713747645951,"(TPE) algorithm [60] in 100 trials. The hyperparameter search space and the optimal hyperparameters
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8926553672316384,"for the models are provided below:
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8935969868173258,"‚Ä¢ XGBoost: We tune the n_estimators in the integer interval [1, 1000], max_depth in the integer
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8945386064030132,"interval [0, 2000], learning rate in the continuous interval [1e-6, 1], subsample in the continuous
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8954802259887006,"interval [0, 1], and colsample_bytree in the continuous interval [0, 1].
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.896421845574388,"‚Ä¢ MLP, NAM, NBM and NATM: We tune the batchnorm in the descret set {False, True}, dropout
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8973634651600754,"in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3, 1e-2], and
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8983050847457628,"weight_decay in the continuous interval [1e-6, 1e-1] on a log scale.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.89924670433145,"‚Ä¢ RNN, GRU and LSTM: We tune the hidden_size in the integer interval [8, 128], dropout
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9001883239171374,"in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3, 1e-2], and
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9011299435028248,"weight_decay in the continuous interval [1e-6, 1e-1] on a log scale.
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9020715630885122,"‚Ä¢ Transformer: We tune the n_layers in the integer interval [1, 4], emb_size in the integer
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9030131826741996,"interval [8, 32], hidden_size in the integer interval [8, 128], n_heads in the integer interval [1,
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.903954802259887,"8], dropout in the continuous interval [0, 0.9], learning_rate in the continuous interval [1e-3,
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9048964218455744,"1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a log scale.
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9058380414312618,"‚Ä¢ Linear: We tune the learning_rate in the continuous interval [1e-3, 1e-2], and weight_decay
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9067796610169492,"in the continuous interval [1e-6, 1e-1] on a log scale.
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9077212806026366,"‚Ä¢ EBM: We tune max_bins in the integer interval [8, 512], min_samples_leaf and max_leaves
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.908662900188324,"in the integer interval [1, 50], inner_bags and outer_bags in the integer interval [1, 128],
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9096045197740112,"learning_rate in the continuous interval [1e-6, 100] on a log scale, and max_rounds in the
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9105461393596986,"integer interval [1000, 10000].
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.911487758945386,"‚Ä¢ NodeGAM: We tune n_trees in the integer interval [1, 256], n_layers and depth in the integer
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9124293785310734,"intervals [1, 4], dropout in the continuous interval [0, 0.9], learning_rate in the continuous
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9133709981167608,"interval [1e-3, 1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a log scale.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9143126177024482,"‚Ä¢ GATSM: We tune nbm_batchnorm in the descret set {False, True}, nbm_dropout in the con-
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9152542372881356,"tinuous interval [0, 0.9], attn_emb_size in the integer interval [8, 128], attn_n_heads in the
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.916195856873823,"integer interval [1, 8], attn_dropout in the continuous interval [0, 0.9], learning_rate in the
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9171374764595104,"continuous interval [1e-3, 1e-2], and weight_decay in the continuous interval [1e-6, 1e-1] on a
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9180790960451978,"log scale. The optimal hyper-parameters for GATSM across all experimental datasets are provided
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9190207156308852,"in Table 6.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9199623352165726,"D
Additional experiments
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9209039548022598,"D.1
Inference speed
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9218455743879472,"The inference speed of machine learning models is a crucial metric for real-world systems. We
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9227871939736346,"evaluate the throughput of various models. The results are presented in Table 7. Since the datasets
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.923728813559322,"have fewer features than the number of basis functions in NBM, NAM achieves higher throughput
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9246704331450094,"than NBM. Transparent tabular models typically exhibit fast speeds. However, their throughput
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9256120527306968,"significantly decreases in datasets with many features, such as Heartbeat, Mortality, and Sepsis,
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9265536723163842,"because they require the same number of feature functions as the number of input features. Trans-
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9274952919020716,"former shows higher throughput than the transparent time series models because it does not require
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.928436911487759,"feature functions, which are the main bottleneck of transparent models. Additionally, the PyTorch
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9293785310734464,"implementation of Transformer uses the flash attention mechanism [61] to enhance its efficiency.
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9303201506591338,"NATM has slightly higher throughput than GATSM, as it does not require the attention mechanism
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.931261770244821,"and has fewer feature functions compared to the number of basis functions in GATSM.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9322033898305084,Table 7: Inference throughput of different models.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9331450094161958,"Energy
Rainfall
AirQuality
Heartbeat
Mortality
Sepsis
LSST
NATOPS"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9340866290018832,"NAM
65.3K
1.8M
5.1M
139.1K
772.2K
23.9K
2.3M
147.9K
NBM
45.5K
1.1M
1.0M
55.9K
375.8K
6.5K
1.6M
85.6K
Transformer
30.9K
240.5K
174.2K
15.7K
161.9K
134.6K
214.4K
68.3K
NATM
5.3K
699.3K
241.3K
1.3K
N/A
N/A
28.6K
19.2K
GATSM
6.1K
350.6K
192.8K
1.2K
4.9K
3.8K
126.5K
12.5K"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9350282485875706,"D.2
Number of basis functions
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.935969868173258,"We evaluate GATSM by varying the number of basis functions in the time-sharing NBM. The results
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9369114877589454,"for forecasting, binary classification, and multi-class classification datasets are presented in Figure 6.
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9378531073446328,"For the Sepsis dataset, using 200 and 300 basis functions causes the out-of-memory error. For the
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9387947269303202,"Energy and Heartbeat datasets, performance improves up to 100 basis functions but shows no further
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9397363465160076,"benefit when the number of bases exceeds 100. In other datasets, performance changes are not
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.940677966101695,"significant with different numbers of basis functions. In addition, there is a trade-off between the
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9416195856873822,"number of basis functions and computational speed. Therefore, we recommend generally setting the
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9425612052730696,"number of basis functions to 100. Note that the performance of GATSM with this hyper-parameter
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.943502824858757,"depends on the dataset size and complexity. Hence, a larger number of basis functions may benefit
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9444444444444444,"more complex datasets.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9453860640301318,"10
50
100
200
300
Number of bases 0.1 0.2 0.3 0.4 0.5 0.6 R2"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9463276836158192,"Energy
Rainfall
AirQuality"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9472693032015066,(a) Forecasting
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.948210922787194,"10
50
100
200
300
Number of bases 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 AUROC"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9491525423728814,"Heartbeat
Mortality
Sepsis"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9500941619585688,(b) Binary
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9510357815442562,"10
50
100
200
300
Number of bases 0.6 0.7 0.8 0.9"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9519774011299436,Accuracy
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9529190207156308,"LSST
NATOPS"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9538606403013182,(c) Multi-class
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9548022598870056,Figure 6: Performances of GATSM on the different number of basis functions.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.955743879472693,"E
Additional visualizations
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9566854990583804,"In addition to the interpretations on the AirQuality dataset in section 5.4, we present another interesting
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9576271186440678,"interpretations of GATSM on the Rainfall dataset.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9585687382297552,"Time-step importance: Figure 7 illustrates the average importance of all time steps at the final time
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9595103578154426,"step. The importance exhibit a cyclical pattern of rising and falling at regular intervals, indicating
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96045197740113,"that GATSM effectively captures seasonal patterns in the Rainfall dataset.
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9613935969868174,"Global feature contribution: Figure 8 illustrates the global behavior of features in the Rainfall
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9623352165725048,"dataset, with red bars indicating the density of training samples. Our findings indicate that low Max
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.963276836158192,"Temperature and high Min Temperature contribute to an increase in rainfall.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642184557438794,"Local time-independent feature contribution: Figure 9 shows the local time-independent feature
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9651600753295668,"contributions. Consistent with the global interpretation, Avg. Temperature and Min Temperature have
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661016949152542,"positive correlations with rainfall, while Max Temperature has a negative correlation with rainfall.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670433145009416,"Local time-dependent feature contribution: Figure 10 shows the local time-dependent feature
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967984934086629,"contributions. All features exhibit patterns similar to the local time-independent contributions.
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689265536723164,"However, we found that Avg. Temperature and Min Temperature have time lags between feature
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698681732580038,"values and contributions.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708097928436912,"0
10
20
Time steps 0.02 0.04 0.06 0.08 0.10"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717514124293786,Avg. attention score
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972693032015066,Figure 7: Average attention scores of time steps on the Rainfall dataset.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736346516007532,"2.5
0.0
2.5 0.1 0.0"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745762711864406,Avg. Temperature
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975517890772128,"2.5
0.0
2.5 0.0 0.1"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764595103578154,"Max Temperature 5
0 0.00 0.05"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774011299435028,Min Temperature
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783427495291902,Feature value
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792843691148776,Feature contribution
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980225988700565,Figure 8: Global interpretations of features in the Rainfall dataset.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811676082862524,"0
10
20 0.035 0.030 0.025 0.020 0.015 0.010 0.005"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821092278719398,"0.000
Avg. Temperature"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"0
10
20
0.10 0.05 0.00 0.05"
MAX TEMPERATURE,0.9839924670433146,"0.10
Max Temperature"
MAX TEMPERATURE,0.9849340866290018,"0
10
20 0.030 0.025 0.020 0.015 0.010 0.005"
MIN TEMPERATURE,0.9858757062146892,"0.000
Min Temperature 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00"
MIN TEMPERATURE,0.9868173258003766,Time step
MIN TEMPERATURE,0.987758945386064,Feature contribution
MIN TEMPERATURE,0.9887005649717514,Feature value
MIN TEMPERATURE,0.9896421845574388,Figure 9: Local time-independent contributions of features in the Rainfall dataset.
MIN TEMPERATURE,0.9905838041431262,"0
10
20
0.30 0.25 0.20 0.15 0.10 0.05"
MIN TEMPERATURE,0.9915254237288136,"0.00
Avg. Temperature"
MIN TEMPERATURE,0.992467043314501,"0
10
20 0.6 0.4 0.2 0.0 0.2 0.4 0.6"
MIN TEMPERATURE,0.9934086629001884,Max Temperature
MIN TEMPERATURE,0.9943502824858758,"0
10
20
0.25 0.20 0.15 0.10 0.05"
MIN TEMPERATURE,0.995291902071563,"0.00
Min Temperature 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00 1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00"
MIN TEMPERATURE,0.9962335216572504,Time step
MIN TEMPERATURE,0.9971751412429378,Feature contribution
MIN TEMPERATURE,0.9981167608286252,Feature value
MIN TEMPERATURE,0.9990583804143126,Figure 10: Local time-dependent contributions of features in the Rainfall dataset.
