Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005050505050505051,"We study reinforcement learning (RL) agents which can utilize language inputs
and efﬁciently learn on downstream tasks. To investigate this, we propose a new
multimodal benchmark – Text-Conditioned Frostbite – in which an agent must
complete tasks speciﬁed by text instructions in the Atari Frostbite environment.
We curate and release a dataset of 5M text-labelled transitions for training, and
to encourage further research in this direction. On this benchmark, we evaluate
Text Decision Transformer (TDT), a transformer directly operating on text, state,
and action tokens, and ﬁnd it improves upon baseline architectures. Furthermore,
we evaluate the effect of pretraining, ﬁnding unsupervised pretraining can yield
improved results in low-data settings."
ABSTRACT,0.010101010101010102,"Figure 1: The Text-Conditioned Decision Transformer (TDT) architecture for specifying behaviors
via language inputs. Text tokens mi are prepended to the sequence of episode tokens, specifying the
current task. Alternating states st and actions at provide context for the model and are rolled out in
the environment. Actions are predicted from every state in an autoregressive fashion."
INTRODUCTION,0.015151515151515152,"1
INTRODUCTION"
INTRODUCTION,0.020202020202020204,"Whereas most contemporary reinforcement learning (RL) methods require programming a reward
function, we are interested in training RL agents which understand language, enabling a ﬂexible
and interpretable interface and facilitating human-AI interaction. Natural language often carries
tremendous amounts of information and can thus both provide strong learning signals and describe
complex scenarios. Furthermore, agents which are able to learn from a diverse set of tasks – such as
the space of tasks speciﬁed by language – can take advantage of broader data for supervision, access
more diversity in an environment, and perform more effective few-shot learning. One path towards
more general agents is data-driven reinforcement learning, which promises to leverage diverse data
for improved generalization in RL (Levine et al., 2020), similar to approaches in natural language."
INTRODUCTION,0.025252525252525252,"Recent approaches to data-driven RL have proposed leveraging powerful sequence models (Chen
et al., 2021; Janner et al., 2021) that are now mainstream in language (Devlin et al., 2018; Radford
et al., 2019). While initial approaches to training language models on large-scale unsupervised data
focused on learning individual word embeddings (Pennington et al., 2014), newer models, especially
those based on the transformer architecture (Vaswani et al., 2017), can also transfer sequential as-
pects learned during pretraining. Similar work in RL has found great beneﬁt to transferring learned"
INTRODUCTION,0.030303030303030304,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03535353535353535,Task: “jump between the second and third ice ﬂoes”
INTRODUCTION,0.04040404040404041,"Figure 2: We create a new dataset on the Atari Frostbite environment, consisting of 5M timesteps
of labelled text-trajectory pairs. The agent must solve a diverse set of tasks speciﬁed by natural
language instructions. Above are example frames representative of a task from the dataset."
INTRODUCTION,0.045454545454545456,"embeddings via a vision encoder (Yarats et al., 2021), but transferring sequential behavior knowl-
edge is relatively unstudied. However, using transformer sequence modeling approaches, we can
develop methods which can ﬁnetune more effectively from ofﬂine data."
INTRODUCTION,0.050505050505050504,"To better study this, we propose a new benchmark and release a new dataset1, Text-Conditioned
Frostbite, in which the agent must perform tasks speciﬁed by language instructions in the Atari
Frostbite environment (Bellemare et al., 2013). Crucially, our dataset consists of a diverse set of
tasks, ranging from easier tasks such as “try to stay on the right side” to harder tasks such as “spend
as much time as possible on the fourth ice ﬂoe”. Inspired by the proposal from Lake et al. (2017),
we believe Frostbite is a diverse and dynamic environment requiring temporally extended strategies,
whereas traditional evaluations underutilize the environment by ﬁnding agents with single, narrow
strategies. We focus on Atari Frostbite as it allows for combining this naturalistic language with
complex goals and behaviors using a familiar, well-studied environment."
INTRODUCTION,0.05555555555555555,"We introduce a transformer operating on text-state-action sequences – which we call Text Decision
Transformer (TDT) – adapting the single-stream Decision Transformer (Chen et al., 2021) to our
setting of conditioning on language goals by replacing the reward tokens with text tokens. We
perform empirical evaluations on our environment using the TDT, ﬁnding it serves as a reasonable
baseline architecture for study, and evaluate performance with unsupervised pretraining."
INTRODUCTION,0.06060606060606061,Our contributions are as follows:
INTRODUCTION,0.06565656565656566,"1. We propose a new multimodal benchmark, Text-Conditioned Frostbite, and provide a dataset of
5M labelled transitions with 500 different tasks.
2. We evaluate a single-stream architecture, Text Decision Transformer, and ﬁnd it outperforms
MLP baselines, highlighting sequence modeling-based approaches.
3. We investigate unsupervised pretraining, both on language and on trajectories, and ﬁnd trajectory-
based pretraining yields improved performance in low-data settings. Furthermore, we investigate
scaling laws, ﬁnding the task is complex enough that TDT improves with more data."
PRELIMINARIES,0.0707070707070707,"2
PRELIMINARIES"
LANGUAGE-CONDITIONED IMITATION LEARNING,0.07575757575757576,"2.1
LANGUAGE-CONDITIONED IMITATION LEARNING"
LANGUAGE-CONDITIONED IMITATION LEARNING,0.08080808080808081,"In the problem of Language-Conditioned Imitation Learning, we are provided with a tuple of (S,
A, M, P, R)i, which correspond to the states, actions, tasks, transition dynamics, and a reward
or score function. A trajectory τ = (s1, a1, ...st, at) is a sequence of states and actions generated
by the transition dynamics and the agent’s action at every timestep, where t is less than or equal
to the maximum episode length. The score function R is prompted with a trajectory τ and a text"
LANGUAGE-CONDITIONED IMITATION LEARNING,0.08585858585858586,"1Dataset and code are available at: <github will be made public at publication>, tem-
porary link to dataset and code for reproduction are in supplementary material"
LANGUAGE-CONDITIONED IMITATION LEARNING,0.09090909090909091,Under review as a conference paper at ICLR 2022
LANGUAGE-CONDITIONED IMITATION LEARNING,0.09595959595959595,"task m and outputs the degree of success that the trajectory had with respect to the task – note we
only use R for evaluation purposes. A policy/model πθ(at|s≤t, a<t, m) is tasked with outputting
actions which maximize the score function, i.e. Ea∼πθ[R(τ, m)]. The model has access to a dataset
of expert trajectory-language pairs (τ, m) for training."
TRANSFORMERS,0.10101010101010101,"2.2
TRANSFORMERS"
TRANSFORMERS,0.10606060606060606,"Transformers were originally proposed by Vaswani et al. (2017) and have since become the front-
runner in solving sequence modelling problems. The models are built with sequential self-attention
modules. Each self-attention module is given a sequence of n embeddings and outputs another n
embeddings with the same dimensions. The self-attention modules work by performing the inner
product between the current embedding xi with the query matrix Q to give qi and with the key matrix
K to give the key ki. The model takes the inner product between the key and query for j ∈[1, n],
yielding n logits, over which the model takes the softmax and yields a probability distribution. The
ﬁnal result is then a convex combination of the value vectors vi, with the weights dictated by the
probability distribution. More concisely, the i-th component of the output zi is given by zi = n
X"
TRANSFORMERS,0.1111111111111111,"j=1
softmax({⟨qi, kj′⟩}n
j′=1)j · vj.
(1)"
DECISION TRANSFORMER,0.11616161616161616,"2.3
DECISION TRANSFORMER"
DECISION TRANSFORMER,0.12121212121212122,"Our work builds on Decision Transformer (DT) (Chen et al., 2021), which uses a GPT-2 (Radford
et al., 2019) transformer architecture, leveraging a causal mask where the i-th prediction depends
only on input tokens up to i. DT models a trajectory sequence ( ˆR1, s1, a1, . . . , ˆRt, st, at) autoregres-
sively, where ˆRt is the returns-to-go from timestep t to the end of the episode. Test-time evaluation
proceeds by initializing a behavior speciﬁcation – DT uses a desired return ˆR1 – and alternating
generation between states from the environment and actions sampled from the model. To reduce
computation, DT only uses the last K timesteps as input to the model, called the context size. A
context size of 1 represents a Markovian policy."
ENVIRONMENT AND DATASET,0.12626262626262627,"3
ENVIRONMENT AND DATASET"
ENVIRONMENT AND DATASET,0.13131313131313133,"The ﬁrst contribution of our work is in developing a diverse, yet accessible environment for eval-
uating the performance of text-conditioned imitation models. We build on the proposal from Lake
et al. (2017), in which the authors propose the “Frostbite Challenge”. The authors propose Frostbite
is challenging due to the requirement of temporally extended strategies and the diverse landscape
of possible tasks to complete. To construct a dataset for this task, we manually collected several
hundred tasks and trajectories. These tasks range from “don’t move” to “stay as long as you can on
the fourth ice ﬂoe” to “die to a crab on your ﬁrst life” and so on."
ENVIRONMENT DETAILS,0.13636363636363635,"3.1
ENVIRONMENT DETAILS"
ENVIRONMENT DETAILS,0.1414141414141414,"Evaluation via RAM state. The ﬁrst challenge with such an environment is building deterministic
evaluation strategies. For this reason, evaluation of the agent is performed by accessing the RAM
state of the game, calculating the level, position, score, and other related quantities of the agent, and
comparing them with the desired outcomes. This evaluation loop runs after a trajectory is completed
and can be prompted with a number of different desired tasks. The interface is malleable enough to
even generate rewards for speciﬁc tasks, though we have not yet conducted such a study. We use the
underlying state-space of the game to calculate speciﬁc metrics in the environment. These metrics
are then normalized to be in the range (0, 1), so no task gets unequal weighting. For example, in
a task like “spend as much time as possible on the ﬁrst ice ﬂoe”, the assigned success is simply
(number of timesteps on 1st ice ﬂoe) / (number of timesteps)."
ENVIRONMENT DETAILS,0.14646464646464646,"Performance measurement. Performance of the model is measured by prompting the model with
several different tasks and subsequently running the evaluation procedure on the generated trajecto-
ries. The score is normalized to be in the range [0, 1] and is designed to be a success metric. If the"
ENVIRONMENT DETAILS,0.15151515151515152,Under review as a conference paper at ICLR 2022
ENVIRONMENT DETAILS,0.15656565656565657,"Split
Example Tasks"
ENVIRONMENT DETAILS,0.16161616161616163,"Easy
stay on the left side, die on the ﬁrst level, get as close to 500 score as you can
Medium
jump between the second and third ice ﬂoes, reach level 2, die on the second level
Hard
reach level 5, spend as much time as possible on the fourth ice ﬂoe"
ENVIRONMENT DETAILS,0.16666666666666666,"Table 1: Example tasks in our Text-Conditioned Frostbite dataset. Tasks are split into easy, medium,
and hard categories for evaluation purposes."
ENVIRONMENT DETAILS,0.1717171717171717,"model receives a score of 1 on a generated trajectory, then the model has succeeded in obeying the
task, while a score of 0 indicates a failure."
DATASET DETAILS,0.17676767676767677,"3.2
DATASET DETAILS"
DATASET DETAILS,0.18181818181818182,"Overview. The dataset consists of more than 700 labelled missions, for a total of 5m timesteps, each
consisting of the current task, the current state, and the action taken. The Atari environment supports
18 discrete actions corresponding to directional movement and interaction. Game frames are resized
and converted to black-white. The environment does not explicitly perform frame-stacking, as our
agent has access to context, but frame-stacking can be implemented with the interface. We list some
example tasks in Table 1 and include more in the Appendix."
DATASET DETAILS,0.18686868686868688,"Collection. We collected several hundred tasks and trajectories by hand. These trajectories were
generated by taking advantage of the Atari-Gym user interface and playing Frostbite with keyboard
controls. At the beginning of every task, users are prompted with a task that they will try to achieve,
and their states, actions, and tasks are recorded and saved."
DATASET DETAILS,0.1919191919191919,"Task categorizations. For evaluation, a subset of the collected tasks are supported and separated
into distinct task suites depending on the degree of control necessary to perform the task. For
instance, in the easy task suite, there are tasks such as “dont move,” “die on the ﬁrst level,” and
“stay on the left side.” Such tasks are quite simple and do not require advanced visual perception
or control of the agent. In the medium task suite, tasks require higher levels of perception, but are
less demanding than the hard suite. The medium task suite contains “get as close to 1000 score
as you can,” “reach level 2,” and “jump between the second and third ice ﬂoes.” The hard suite
contains tasks that require the most advanced control, such as “reach level 5” and “spend as much
time as possible on the fourth ice ﬂoe.” The task suites include only tasks that support RAM score
evaluation. A list of further training tasks can be found in the Appendix (see Table 3)."
METHOD,0.19696969696969696,"4
METHOD"
METHOD,0.20202020202020202,"Here, we detail the architecture and modiﬁcations of the Text-Conditioned Decision Transformer.
This model uses a causal transformer architecture in the style of GPT-2 (Radford et al., 2019) to
model sequences jointly consisting of text and trajectories."
ARCHITECTURE,0.20707070707070707,"4.1
ARCHITECTURE"
ARCHITECTURE,0.21212121212121213,"Similar to Decision Transformer, we model the trajectories as a sequence of tokens. However,
because we are operating within the framework of imitation learning, for behavior speciﬁcation,
we excise the return-to-go tokens from our architecture and use text tokens instead. These text
tokens are prepended to the sequence of tokens. The original words of the task are encoded using an
embedding table, whereas states and actions are encoded using CNNs / MLPs, respectively. Thus, a
typical sequence of tokens passed into the model will resemble (see Figure 1):"
ARCHITECTURE,0.21717171717171718,"τ = (m1, m2, . . . , mn, s1, a1, s2, a2, . . . , st, at)"
ARCHITECTURE,0.2222222222222222,"The model is prompted with a text tokens and a sequence of states and actions. The inputs are tok-
enized and passed through the model. Action logits for action at correspond to the output from state
st, and due to the causal transformer mask, only tokens preceding st are visible in this prediction.
For generating trajectories, we sample from the predicted action logits, and step the environment"
ARCHITECTURE,0.22727272727272727,Under review as a conference paper at ICLR 2022
ARCHITECTURE,0.23232323232323232,"with the predicted action. At the end of the trajectory, we evaluate performance using the environ-
ment’s score function to determine if the trajectory and original text are well-aligned."
ARCHITECTURE,0.23737373737373738,"We preprocess language inputs by converting to lower case and stripping any punctuation. Words
are then converted to one-hot representation and passed through a learned embedding table. See
Algorithm 1 for more details."
TRAINING,0.24242424242424243,"4.2
TRAINING"
TRAINING,0.2474747474747475,"We operate with a standard supervised training procedure. Random minibatches of paired tasks and
trajectories are sampled from our ofﬂine dataset. The model performs a forward pass over the joint
sequence, outputting the logits of the distribution over next actions in parallel. We optimize the cross
entropy loss between the predictions and the true actions. Pseudocode for the model and training is
included in Algorithm 1."
UNSUPERVISED PRETRAINING,0.25252525252525254,"4.3
UNSUPERVISED PRETRAINING"
UNSUPERVISED PRETRAINING,0.25757575757575757,"In Sections 5.2-5.4, we additionally evaluate the effect of unsupervised pretraining, which refers to
training on unlabelled trajectories (without a text pair) drawn from the dataset. For pretraining, the
model is only given as input states and actions, and otherwise follows the same procedure as the su-
pervised training regime. The pretraining data is drawn from the corpus of all missions simply with
text missions removed. Later, the model is ﬁne-tuned and evaluated on speciﬁc subsets of missions
to measure the effectiveness of the representations learned by pretrainining. This is comparable to
the standard autoregressive language modeling objective used by GPT (Radford et al., 2019), or be-
havior cloning in imitation learning. We can transfer the vision encoder layers and/or the sequence
model (the transformer backbone)."
EMPIRICAL EVALUATIONS,0.26262626262626265,"5
EMPIRICAL EVALUATIONS"
EMPIRICAL EVALUATIONS,0.2676767676767677,"5.1
DOES THE TRANSFORMER ARCHITECTURE MODEL THE JOINT SEQUENCES WELL?"
EMPIRICAL EVALUATIONS,0.2727272727272727,"We ﬁrst evaluate the transformer architecture itself in the fully supervised setting. We compare to
an MLP baseline which removes the ability to attend between modalities: text tokens are processed
by a pretrained BERT model (Devlin et al., 2018), and then concatenated to the state and action
tokens, which are processed by the same encoders as TDT. The MLP with context (K) of 1 is chosen
to have the same number of parameters as TDT. The MLP with context 1 is outperformed by all
but one TDT architecture choice, even those with limited context. The MLP with context 20 is
outperformed by the TDT architectures with context size of at least 10. This is despite the fact that
the MLP with context size 20 uses many more parameters (since it relies on concatenating over
timesteps). The TDT with one context represents a fairly similar architecture to the MLP Baseline
with the key difference that the TDT model learns its own text embeddings and utilizes attention
between different modalities. We see that the standard error bars are larger for the MLP, and we
belive this is the reason for its relatively strong performance on the hard tasks."
EMPIRICAL EVALUATIONS,0.2777777777777778,Our results are shown in Figure 3. The conclusion from this experiment is two-fold:
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.2828282828282828,"1. The causal transformer architecture accessing text and state tokens in a single stream outperforms
a concatenation-based approach with pre-encoded text tokens."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.2878787878787879,"2. Increasing the context of the causal transformer leads to stronger performance, indicating that
longer sequence modeling can lead to a more powerful model."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.29292929292929293,"5.2
HOW EFFECTIVE IS UNSUPERVISED PRETRAINING FOR LEARNING REPRESENTATIONS?"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.29797979797979796,"Perhaps the most enticing prospect of TDT is its ability to harness large-scale unlabelled data for
pretraining, enabling few-shot behavior observed in language. For our unsupervised pretraining
experiments, models are initially pretrained via behavior cloning on a ﬁxed number of unlabelled
tasks. These training tasks are sampled from the same corpus as the labelled tasks, but do not include
text. For each evaluation task, we sample a small subset of the labelled data, including the evaluation"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.30303030303030304,Under review as a conference paper at ICLR 2022
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.30808080808080807,"Easy tasks
Medium tasks
Hard tasks
0.0 0.2 0.4 0.6 0.8 1.0 Score"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.31313131313131315,Architecture Comparison in Supervised Setting
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3181818181818182,"MLP, K = 1
MLP, K = 20
TDT, K = 1
TDT, K = 3
TDT, K = 10
TDT, K = 20"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.32323232323232326,"Figure 3: Performance of transformer architecture (TDT), based on the context size K. We com-
pare to a concatenation-based MLP architecture which receives encoded text latents from pretrained
BERT and concatenates them with the latents of the current state from a learned vision encoder. Text
Decision Transformer outperforms the MLP baseline in every suite of tasks. All experiments use 3
seeds and show standard deviation between seeds."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3282828282828283,"Medium tasks
Hard tasks
0.0 0.2 0.4 0.6 0.8 1.0 Score"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3333333333333333,Transferring Vision Encoder
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3383838383838384,"From scratch
Pretrained on 10
Pretrained on 50
Pretrained on 100
Pretrained on 200"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3434343434343434,"Figure 4: Transferring vision encoder after unsupervised pretraining. After pretraining with behavior
cloning on unlabelled trajectories, we see signiﬁcant beneﬁts on ﬁnetuning for medium and hard
tasks compared to a randomly initialized vision encoder. Note the context size used is 20."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3484848484848485,"task itself. For instance, when evaluating on the “jump back and forth between the second and third
ice ﬂoes” task, the model is trained on this task, as well as some related jumping tasks between
the ﬁrst and second, ﬁrst and third, etc., ice ﬂoes. The data that the model is trained on for each
evaluation task is the same across all variants. We ﬁnetune all the model parameters."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.35353535353535354,"We ﬁrst evaluate transferring the vision encoder, which represents the beneﬁt from better image
representations. This is the currently the most common method of ﬁnetuning RL policies after
unsupervised exploration (Yarats et al., 2021), taking advantage of larger datasets for stronger rep-
resentations, but not transferring learned behaviors. Our results are shown in Figure 4."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.35858585858585856,"We conclude that by pre-training the entire model on an unlabeled dataset of transitions, we learn
important features in our visual encoder (despite no text labels). Using this visual encoder in later
tasks then allows for better performance when ﬁne-tuning on speciﬁc tasks. This improvement
scales with larger quantities of pre-training data."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.36363636363636365,"5.3
DOES SEQUENTIAL CONTEXT IMPROVE PRETRAINING PERFORMANCE?"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3686868686868687,"Additionally, due to the sequential nature of the transformer, we can further investigate how context
length affects pretraining. Since TDT jointly models modalities, there are multiple possible modes
of supervision. On the one hand, the vision encoder may learn to pick up on valuable features
simply due to repeated actions in speciﬁc positions. For instance, whenever the player is located on
the bottom ice ﬂoe, the agent may tend to jump higher. In such a case, there would be a learning
signal from the environment which would allow the encoder to pick-up on features indicating when
the agent is in the bottom ice ﬂoe. On another hand, the vision representations will be processed
sequentially by a transformer, and may learn features which correlate behavior across timesteps. For"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.37373737373737376,Under review as a conference paper at ICLR 2022
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3787878787878788,"instance, if the previous frames indicate an agent jumping between two ice ﬂoes, knowledge of this
past would allow the model to infer the task at hand."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3838383838383838,"To evaluate this, we ablate TDT from Figure 4 by using a context size of 1 instead of 20. Again,
we transfer the vision encoder only. Our results are shown in Table 2 and Figure 5. We observe
that larger contexts do indeed lead to larger percent increases from pre-training. This suggests that
the improvement from pre-training is not just due to seeing repeated state-action pairs, and beneﬁts
from trajectory-level inference."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3888888888888889,"Split
Context = 1
Context = 20"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.3939393939393939,"Hard
43.5%
181.1%
Medium
39.0%
39.0%"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.398989898989899,"Table 2: Comparing beneﬁt of transferring vision encoder for different model context sizes. Im-
provement is measured as percent increase in performance when pre-training with 200 unlabelled
tasks compared to an untrained model. The improved transfer of the 20 context model indicates that
sequential trajectory-level information can be transferred."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.40404040404040403,"Medium tasks
Hard tasks
0.0 0.2 0.4 0.6 0.8 1.0 Score"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4090909090909091,Transferring Vision Encoder (Context = 1)
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.41414141414141414,"From scratch
Pretrained on 10
Pretrained on 50
Pretrained on 100
Pretrained on 200"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.41919191919191917,"Figure 5: Transferring vision encoder with a shorter context (size 1). While pretraining still helps, it
is generally less effective than with the model utilizing a longer context (Figure 4); the numbers for
pretraining on 200 unlabelled tasks are summarized in Table 2."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.42424242424242425,"5.4
CAN WE TRANSFER THE SEQUENCE MODEL BACKBONE?"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4292929292929293,"In the past sections, we investigated transferring the vision encoder only from the pretraining phase.
This has been standard for ﬁnetuning RL agents after unsupervised exploration because it can be
difﬁcult to transfer the policy/value parameters between the distinct exploration/ﬁnetuning regimes.
However, as we have seen in language, we can gain greater expressiveness in ﬁnetuning by trans-
ferring the sequential aspect of the model (Devlin et al., 2018; Radford et al., 2019). We repeat the
same unsupervised pretraining phase, but now transfer the transformer backbone, which captures
learned behaviors and sequential integration of the inputs."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.43434343434343436,"Our results are shown in Figure 6. The sequence model incorporates additional information that is
applicable to the downstream tasks: for instance, after pretraining on 200 unlabelled tasks and then
evaluating on the hard tasks, additionally transferring the sequence model doubles the score of only
transferring the vision encoder. This hints that transferring learned sequence models can serve as a
powerful foundation for few-shot learning in RL settings."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4393939393939394,"5.5
HOW DOES TEXT DECISION TRANSFORMER SCALE WITH DATA?"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4444444444444444,"Finally, we also show additionally scaling results in the supervised setting. In the previous experi-
ments, we showed pretraining on larger amounts of unsupervised data improved performance. Here,
we additionally train the model on larger amounts of labelled data ranging from 10 to 700, and then
evaluate the performance of these models on a heldout validation set. The validation set consists of
new trajectories and some new task statements not in the training data. The validation loss of the
model is shown in Figure 7. A low validation loss indicates that the model is more likely to predict
the correct action, and hence more likely to achieve higher success rates in the environment."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4494949494949495,Under review as a conference paper at ICLR 2022
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.45454545454545453,"Medium tasks
Hard tasks
0.0 0.2 0.4 0.6 0.8 1.0 Score"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4595959595959596,Transferring Sequence Model
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.46464646464646464,"From scratch
Pretrained on 50
Pretrained on 100
Pretrained on 200"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4696969696969697,"Figure 6: Transferring both the sequence model and the vision encoder after unsupervised pretrain-
ing. The sequence model captures additional information that improves performance compared to
only transferring the vision encoder."
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.47474747474747475,"101
102"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4797979797979798,Number of training examples
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.48484848484848486,"3.1 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.4898989898989899,"3.2 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.494949494949495,"3.3 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.5,"3.4 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.5050505050505051,"3.5 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.51010101010101,"3.6 × 10
1"
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.5151515151515151,Validation loss
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.5202020202020202,Scaling Trend for Supervised Setting
THE CAUSAL TRANSFORMER ARCHITECTURE ACCESSING TEXT AND STATE TOKENS IN A SINGLE STREAM OUTPERFORMS,0.5252525252525253,"Figure 7: Validation loss scaling in the supervised setting based on number of labelled training
trajectories. The relationship between loss and data is approximately linear on a log-log plot."
QUALITATIVE EXAMPLES,0.5303030303030303,"5.6
QUALITATIVE EXAMPLES"
QUALITATIVE EXAMPLES,0.5353535353535354,"See Figure 8 for an example rollout with the task “jump between the second and third ice ﬂoes”.
The agent can be seen jumping between the two aforementioned ice ﬂoes."
RELATED WORK,0.5404040404040404,"6
RELATED WORK"
RELATED WORK,0.5454545454545454,"Language-conditioned IL/RL. BabyAI (Chevalier-Boisvert et al., 2019) also proposes a bench-
mark for evaluating language learning on a griworld task. While BabyAI includes a combinatorially
diverse set of tasks using language, the tasks are procedurally generated and thus somewhat less
“natural”, so the tasks and example trajectories are not as diverse. We propose a more open-ended
approach, where the dataset consists of human-generated tasks with no enforced structure of the
text, in hopes of training more realistic and capable models. We also use an environment more re-
liant on skill acquisition, as the model must be able to string together complex action sequences to
reach high success rates. CLIPort (Shridhar et al., 2021) studies language for robotic tasks, using
CLIP (Radford et al., 2021) to transfer supervision from pure language to images, complementary to
our investigation of transferring supervision from pure trajectories to language. Additionally, both
BabyAI and CLIPort, like many other works (Chaplot et al., 2018; Stepputtis et al., 2020; Abramson"
RELATED WORK,0.5505050505050505,"Figure 8: Key frames from a generated agent rollout corresponding to task “jump between the second
and third ice ﬂoes”. States represent a summary of the behavior; see Appendix for longer trajectory."
RELATED WORK,0.5555555555555556,Under review as a conference paper at ICLR 2022
RELATED WORK,0.5606060606060606,"et al., 2020; Nair et al., 2021), use a multi-stream approach, embedding different inputs/modali-
ties with separate models, in contrast to our single-stream approach which simply extends Decision
Transformer (Chen et al., 2021). Our environment differs from CLIPort in that it does not require
advanced attention mechanisms to be successful. Indeed, the Atari interface allows for impressive
performance even with CNNs and MLPs. Another environment addressing language-conditioned
imitation is ALFRED (Shridhar et al., 2020). Unlike ours, this environment provides an overarch-
ing task, along with step-by-step instructions to perform the task. Additionally, the time horizon in
ALFRED is smaller than in our work (some missions take thousands of steps). Our dataset contains
more image-action pairs, and focuses on longer-horizon tasks, but in a simpler environment without
sub-tasks. In comparison to other works such as R2R (Anderson et al., 2018) and Touchdown (Chen
et al., 2020), our work requires less visual feature engineering due to the simpler input, but retains
complex language and allows for longer time horizon missions."
RELATED WORK,0.5656565656565656,"Transformers. Our approach to modeling trajectories follows Chen et al. (2021) and Janner et al.
(2021), except we condition on behaviors by conditioning on language tokens rather than target
returns, which can yield a more diverse set of trajectories. These approaches use sequence mod-
eling, rather than TD learning as other RL transformers works have (Parisotto et al., 2020; Ritter
et al., 2020), which may be simpler and more stable. Transformers have also been more broadly
applied to multimodal tasks, such as vision and text (Lu et al., 2019; Radford et al., 2021). Like the
above discussion, most multimodal approaches use separate models for different models for differ-
ent modalities, e.g. CLIP (Radford et al., 2021) learns separate models to embed text and images
into a similar representation space. Our approach is akin to DALL-E (Ramesh et al., 2021), which
uses a single-stream approach to model text and images, where we replaces images with trajectories,
and do not predict the text. Prompt tuning (Lester et al., 2021; Li and Liang, 2021) has the potential
to turn a single-modality model into a multimodal one with minimal ﬁnetuning (Lu et al., 2021;
Tsimpoukelli et al., 2021), and would be interesting future work. We showcase positive results from
the ability to transfer sequential behavior from unlabeled pretraining, which parallels trends seen in
unsupervised language pretraining (Devlin et al., 2018)."
CONCLUSION,0.5707070707070707,"7
CONCLUSION"
CONCLUSION,0.5757575757575758,"We developed a new benchmark, Text-Conditioned Frostbite, and released a corresponding dataset,
with the aim of accelerating progress in the development of RL agents which capably utilize lan-
guage. We also proposed Text Decision Transformer, a multimodal architecture operating over
text, state, and action tokens. We showed results showing that unsupervised pretraining can im-
prove downstream few-shot learning, both when transferring the vision encoder and the sequence
backbone. We released code and the dataset used, and hope future work will continue exploring
approaches for effective few-shot learning in language and RL settings."
CONCLUSION,0.5808080808080808,Under review as a conference paper at ICLR 2022
REFERENCES,0.5858585858585859,REFERENCES
REFERENCES,0.5909090909090909,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.5959595959595959,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. arXiv preprint arXiv:2106.01345, 2021."
REFERENCES,0.601010101010101,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence mod-
eling problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.6060606060606061,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.6111111111111112,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.6161616161616161,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pages 1532–1543, 2014."
REFERENCES,0.6212121212121212,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017."
REFERENCES,0.6262626262626263,"Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-
totypical representations. arXiv preprint arXiv:2102.11271, 2021."
REFERENCES,0.6313131313131313,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013."
REFERENCES,0.6363636363636364,"Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi:
10.1017/S0140525X16001837."
REFERENCES,0.6414141414141414,"Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,
Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efﬁciency of
grounded language learning, 2019."
REFERENCES,0.6464646464646465,"Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
CLIPort:
What and where pathways for
robotic manipulation.
In 5th Annual Conference on Robot Learning, 2021.
URL https:
//openreview.net/forum?id=9uFiX_HRsIL."
REFERENCES,0.6515151515151515,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021."
REFERENCES,0.6565656565656566,"Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Ra-
jagopal, and Ruslan Salakhutdinov.
Gated-attention architectures for task-oriented language
grounding. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.6616161616161617,"Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben
Amor. Language-conditioned imitation learning for robot manipulation tasks. arXiv preprint
arXiv:2010.12083, 2020."
REFERENCES,0.6666666666666666,"Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita
Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelli-
gence. arXiv preprint arXiv:2012.05672, 2020."
REFERENCES,0.6717171717171717,"Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learn-
ing language-conditioned robot behavior from ofﬂine data and crowd-sourced annotation. arXiv
preprint arXiv:2109.01115, 2021."
REFERENCES,0.6767676767676768,Under review as a conference paper at ICLR 2022
REFERENCES,0.6818181818181818,"Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
for everyday tasks, 2020."
REFERENCES,0.6868686868686869,"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S¨underhauf, Ian Reid,
Stephen Gould, and Anton van den Hengel.
Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2018."
REFERENCES,0.6919191919191919,"Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural
language navigation and spatial reasoning in visual street environments, 2020."
REFERENCES,0.696969696969697,"Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,
Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers
for reinforcement learning. In International Conference on Machine Learning, pages 7487–7498.
PMLR, 2020."
REFERENCES,0.702020202020202,"Sam Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matt Botvinick, and David Raposo.
Rapid task-solving in novel environments. arXiv preprint arXiv:2006.03662, 2020."
REFERENCES,0.7070707070707071,"Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019."
REFERENCES,0.7121212121212122,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021."
REFERENCES,0.7171717171717171,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.7222222222222222,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190, 2021."
REFERENCES,0.7272727272727273,"Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247, 2021."
REFERENCES,0.7323232323232324,"Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multi-
modal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021."
REFERENCES,0.7373737373737373,Under review as a conference paper at ICLR 2022
REFERENCES,0.7424242424242424,Example Tasks
REFERENCES,0.7474747474747475,"dont ﬁnish level 2
do not go below the ﬁrst ice ﬂoe
hit birds
minimize the time spent alive
get to the third level
reach each level as fast as possible
start playing once there are 30 seconds left
make sure to collect ﬁsh
dont enter the ﬁrst igloo
jump via the second ice ﬂoe
complete the ﬁrst igloo
hit a bird on the third level
let the timer run out in every life
run into the left border
fall in the water every life
do not pass the second ice ﬂoe
dont move
build the ﬁrst 6 igloos
prioritize getting as many ﬁsh as possible
dont enter the third igloo
only play on your ﬁrst life
run to the left before starting
dont die from crabs
start playing when there are 20 seconds left
build but dont enter the third igloo
hit a bird before entering the ﬁrst igloo
die from water 4 times"
REFERENCES,0.7525252525252525,"Table 3: Example tasks in the Text-Conditioned Frostbite dataset. Some of these tasks are not
included in evaluation so are not broken into easy, medium, and hard suites."
REFERENCES,0.7575757575757576,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.7626262626262627,"In this section, detail performance on a task-by-task basis, and list the tasks which are used for the
experiments."
REFERENCES,0.7676767676767676,"A.1
TASK SUITES"
REFERENCES,0.7727272727272727,"In the easy task suite, the evaluation tasks are ’dont move’, ’stay on the left side’, ’try to stay on the
right side’, ’die on the ﬁrst level’, and ’get as close to 500 points as you can’. In the medium task
suite, the evaluation tasks are ’jump back and forth between the second and third ice ﬂoes’, ’get as
close as you can to 1000 points’, ’spend as much time as possible on the ﬁrst ice ﬂoe’,"
REFERENCES,0.7777777777777778,"A.2
BASELINES BY TASK"
REFERENCES,0.7828282828282829,"In this section, we include the performances of the baselines by task."
REFERENCES,0.7878787878787878,dont stay above the first ice floe
REFERENCES,0.7929292929292929,get as close to 1000 score as you can
REFERENCES,0.797979797979798,reach level 2
REFERENCES,0.803030303030303,dont move
REFERENCES,0.8080808080808081,get as many points as possible
REFERENCES,0.8131313131313131,alternate jumping between the second and third ice floes
REFERENCES,0.8181818181818182,spend as much time as possible on the first ice floe
REFERENCES,0.8232323232323232,get as close to 500 score as you can
REFERENCES,0.8282828282828283,get as close to 1500 score as you can
REFERENCES,0.8333333333333334,stay on the left side
REFERENCES,0.8383838383838383,try to stay on the right side
REFERENCES,0.8434343434343434,spend as much time as possible on the fourth ice floe
REFERENCES,0.8484848484848485,reach level 5
REFERENCES,0.8535353535353535,die on the second level
REFERENCES,0.8585858585858586,reach level 3
REFERENCES,0.8636363636363636,die on the first level mode 0.0 0.2 0.4 0.6 0.8 1.0 Score
REFERENCES,0.8686868686868687,Performance vs Model on Different Suites of Tasks
REFERENCES,0.8737373737373737,"MLP Baseline
1context
3context
10context
20context"
REFERENCES,0.8787878787878788,"Figure 9: Performance vs. Baseline. Above we see the performance of the model relative to a
concatenation-based MLP architecture on a per-task basis."
REFERENCES,0.8838383838383839,Under review as a conference paper at ICLR 2022
REFERENCES,0.8888888888888888,"Figure 10: Select frames from a generated agent rollout corresponding to mission “jump between
the second and third ice ﬂoes”. States are sampled from every 50 frames, and visualized left to right,
top to bottom. Note that many jumps are still lost by subsampling."
REFERENCES,0.8939393939393939,"A.3
FULL TRAJECTORY VISUALIZATION"
REFERENCES,0.898989898989899,Please see ﬁgure 10 for a more complete sequence of a generated trajectory.
REFERENCES,0.9040404040404041,"A.4
HYPERPARAMETERS FOR BASELINE TEXT DT"
REFERENCES,0.9090909090909091,"In this section, we share the hyperparameters we used for the baseline plots with the Text Decision
Transformer. Please see table 4"
REFERENCES,0.9141414141414141,Table 4: Hyperparameters of TDT for Frostbite experiments.
REFERENCES,0.9191919191919192,"Hyperparameter
Value"
REFERENCES,0.9242424242424242,"Number of layers
6
Number of attention heads
8
Embedding dimension
128
Batch size
128
Context length K
20
Text context length
20
Nonlinearity
ReLU, encoder
GeLU, otherwise
Encoder channels
32, 64, 64
Encoder ﬁlter sizes
8 × 8, 4 × 4, 3 × 3
Encoder strides
4, 2, 1
Max epochs
10
Dropout
0.1
Learning rate
1 ∗10−4"
REFERENCES,0.9292929292929293,"Adam betas
(0.9, 0.95)
Grad norm clip
1.0
Weight decay
0.1"
REFERENCES,0.9343434343434344,Under review as a conference paper at ICLR 2022
REFERENCES,0.9393939393939394,"A.5
ALGORITHM PSEUDOCODE"
REFERENCES,0.9444444444444444,"Here, we provide pseudocode for our implementation."
REFERENCES,0.9494949494949495,Algorithm 1 Text-Conditioned Decision Transformer Pseudocode
REFERENCES,0.9545454545454546,"# m, s, a, t: tasks, states, actions, or timesteps
# transformer: transformer with causal masking (GPT)
# embed_s, embed_a: linear embedding layers
# embed_m, embed_t: learned discrete embedding tables
# pred_a: linear action prediction layer"
REFERENCES,0.9595959595959596,"# main model
def TextDecisionTransformer(m, s, a, t):"
REFERENCES,0.9646464646464646,"# compute embeddings for tokens
m_embedding = embed_m(m)
s_embedding = embed_s(s) + embed_t(t)
a_embedding = embed_a(a) + embed_t(t)"
REFERENCES,0.9696969696969697,"# interleave tokens as (m_1, ..., m_n, s_1, a_1, ..., s_K)
input_embeds = stack(s_embedding, a_embedding)
# interleave (s, a)
input_embeds = concatenate(m_embedding, input_embeds)
# prepend text"
REFERENCES,0.9747474747474747,"# use transformer to get hidden states corresponding to actions
hidden_states = transformer(input_embeds=input_embeds)
a_hidden = unstack(hidden_states).actions"
REFERENCES,0.9797979797979798,"# predict action logits
return pred_a(a_hidden)"
REFERENCES,0.9848484848484849,"# training loop
for (m, s, a, t) in dataloader:
a_preds = TextDecisionTransformer(m, s, a, t)
loss = cross_entropy_loss(a_preds, a)
optimizer.zero_grad(); loss.backward(); optimizer.step()"
REFERENCES,0.98989898989899,"# evaluation loop
m, s, a, t, done = [task], [env.reset()], [], [1], False
while not done:
# autoregressive generation/sampling
# sample next action
action = TextDecisionTransformer(m, s, a, t).sample()
new_s, r, done, _ = env.step(action)"
REFERENCES,0.9949494949494949,"# append new tokens to sequence
s, a, t = s + [new_s], a + [action], t + [t[-1]+1]
s, a, t = s[-K:] ...
# only keep context length of K"
