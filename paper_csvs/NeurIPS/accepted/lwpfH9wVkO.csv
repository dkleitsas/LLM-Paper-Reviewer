Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013869625520110957,"Current PAC-Bayes generalisation bounds are restricted to scalar metrics of perfor-
mance, such as the loss or error rate. However, one ideally wants more information-
rich certificates that control the entire distribution of possible outcomes, such
as the distribution of the test loss in regression, or the probabilities of different
mis-classifications. We provide the first PAC-Bayes bound capable of providing
such rich information by bounding the Kullback-Leibler divergence between the
empirical and true probabilities of a set of M error types, which can either be
discretized loss values for regression, or the elements of the confusion matrix (or a
partition thereof) for classification. We transform our bound into a differentiable
training objective. Our bound is especially useful in cases where the severity of
different mis-classifications may change over time; existing PAC-Bayes bounds
can only bound a particular pre-decided weighting of the error types. In contrast
our bound implicitly controls all uncountably many weightings simultaneously."
INTRODUCTION,0.0027739251040221915,"1
Introduction"
INTRODUCTION,0.004160887656033287,"Generalisation bounds are a core component of the theoretical understanding of machine learning
algorithms. For over two decades now, PAC-Bayesian theory has been at the core of studies on
generalisation abilities of machine learning algorithms. PAC-Bayes originated in the seminal work
of McAllester [1998, 1999] and was further developed by Catoni [2003, 2004, 2007], among other
authors—we refer to the surveys Guedj [2019] and Alquier [2021] for an introduction to the field. The
outstanding empirical success of deep neural networks in the past decade calls for better theoretical
understanding of deep learning, and PAC-Bayes has emerged as one of the few frameworks that
can be used to derive meaningful (and non-vacuous) generalisation bounds for neural networks:
the pioneering work of Dziugaite and Roy [2017] has been followed by a number of contributions,
including Neyshabur et al. [2018], Zhou et al. [2019], Letarte et al. [2019], Pérez-Ortiz et al. [2021],
Perez-Ortiz et al. [2021] and Biggs and Guedj [2022a,b], to name but a few."
INTRODUCTION,0.005547850208044383,"Much of the PAC-Bayes literature focuses on the case of binary classification, or of multiclass
classification where one only distinguishes whether each classification is correct or incorrect. This is
in stark contrast to the complexity of contemporary real-world learning problems, such as medical
diagnosis where the severity of Type I and Type II errors may be crucial and context-dependent.
This work aims to bridge this gap by deriving a generalisation bound that provides information-rich
measures of performance at test time by controlling the probabilities of errors of any finite number of
user-specified types. More precisely, we bound the KL-divergence between the empirical and true"
INTRODUCTION,0.006934812760055479,"distributions over the different error types. From this single bound one can then derive bounds on
arbitrary linear combinations of these error probabilities, which will all hold simultaneously with the
same probability as the original bound. In addition, these bounds are guaranteed to be non-vacuous
(this follows since the KL-divergence blows up on the boundary of the simplex)."
INTRODUCTION,0.008321775312066574,"As a concrete example, if the severity of Type I and Type II errors of a medical test are context-
dependent, one would want to be able to bound arbitrary linear combinations of these error probabili-
ties. Existing bounds could only bound finitely many pre-specified weightings by employing a union
bound, which would also degrade the bound. In contrast, by constraining the KL-divergence between
the true and empirical error probabilities, our bound constrains all uncountably many weightings of
the error probabilities simultaneously."
INTRODUCTION,0.009708737864077669,"The usual setting of PAC-Bayes bounds is that of binary classification, namely an input set X, output
set Y = {−1, 1}, hypothesis space H ⊆YX and a sample S ∈(X × Y)m drawn i.i.d. from a data-
generating distribution D. A number of PAC-Bayes bounds in this setting (e.g. Maurer [2004]) have
been unified by a single general bound found in Bégin et al. [2016]. Briefly, Bégin et al. [2016] prove
a bound on the discrepancy d(RS(Q), RD(Q)) between the error probability RD(Q) of a stochastic
classifier Q (a distribution over H which classifies by first drawing h ∼Q and then classifying
according to h) and its empirical counterpart RS(Q) (the fraction of the sample Q misclassifies).
The bound holds with high probability for all Q simultaneously. The bound in Bégin et al. [2016] is
binary in the sense that Y contains two elements, but a more subtle way to look at this is that only two
cases are distinguished—correct classification and incorrect classification. While it can be applied to
multiclassification provided one maintains the second binary characteristic by only distinguishing
correct and incorrect classifications. It is this heavy restriction that our result lifts, by considering the
new framework of error types."
INTRODUCTION,0.011095700416088766,"A new framework of errors types
We consider a user-specified partition of the space Y × Y
of prediction-truth label-pairs into a finite partition of error types E1, . . . , EM. Our bound then
simultaneously constrains the probability with which errors of each type occur. In multiclass
classification for example, one can choose the error types to be the set of all different possible
mis-classifications, in which case our bound will control the entire confusion matrix, bounding how
far the true confusion matrix (i.e. expected over the data-generating distribution) can diverge from the
empirical one (i.e. on the training set). From this one can then derive bounds on the probabilities
with which each mis-classification may be made, and arbitrary linear combinations of these error
probabilities, and all of these will hold simultaneously with the same probability as the original
bound. Our bound therefore paints a far richer picture of the performance of the final learned model
than can be provided by any existing PAC-Bayes bound."
INTRODUCTION,0.012482662968099861,"Formally, we let SM
j=1 Ej be a user-specified disjoint partition of Y2 into a finite number of M
error types, where we say that a hypothesis h ∈H makes an error of type j on datapoint (x, y) if
(h(x), y) ∈Ej (by convention, every pair (ˆy, y) ∈Y2 is interpreted as a predicted value ˆy followed
by a true value y, in that order). It should be stressed that not all of the Ej need correspond to
mislabellings—indeed, some of the Ej may distinguish different correct labellings."
INTRODUCTION,0.013869625520110958,"Relation to previous results.
Our framework of a finite number of user-specified “error types”
includes multiclass classification as a particular case, and it is in this field that one finds the work
most closely related to ours. Little is known of multiclass classification from a theoretical perspective
and, to the best of our knowledge, only a handful of relevant strategies or generalisation bounds can
be compared to the present paper."
INTRODUCTION,0.015256588072122053,"The closest is the work of Morvant et al. [2012], which establishes a PAC-Bayes bound on the spectral
norm of the difference between the true and empirical confusion matrices. Our bound differs from
theirs in two respects. First, they consider the confusion matrix, whereas ours applies to the more
general setting of a finite number of error types, which can be the set of all mis-classifications, or some
partition thereof. Second, they deal with the spectral norm, whereas we employ the KL-divergence.
Since the KL-divergence follows a simple formula, this means we can much more easily infer bounds
on the individual error probabilities, which would be very challenging for the spectral norm. The
follow-up work Koço and Capponi [2013] shows how a proxy of the spectral norm bound can be
used as a training objective that may deal with imbalanced classes. In the present work, we show how
our bound can be used as a differentiable training objective directly (without the need of a proxy) and"
INTRODUCTION,0.016643550624133148,"that it can more sensitively deal with imbalanced classes, or errors of different severity, by assigning
each error type a user-specified loss value."
INTRODUCTION,0.018030513176144243,"Laviolette et al. [2017] extend the celebrated C-bound in PAC-Bayes to ensembles, obtaining a bound
on the risk of the majority vote classifier in the case of multiclass classification. In this context, our
bound is able to distinguish different mis-classifications and control them, whereas they bound the
scalar risk which lumps all mis-classifications together. The C-bound has alternately been generalised
by Lacasse et al. [2006] (see also Germain et al. [2015]) to simultaneously control three metrics,
namely the so-called expected disagreement, expected joint success and expected joint error of the
posterior. While they restricted themselves to the ternary case, some of their proof techniques share
similarities with ours. In cases where one has exactly three error types, for example the {−1, 0, 1}-
valued excess loss, the work of Wu and Seldin [2022] is applicable; they construct so-called ‘split-kl’
inequalities (both classical and PAC-Bayesian) which deftly handle this specific scenario."
INTRODUCTION,0.019417475728155338,"Pires et al. [2013] present a comprehensive analysis of convex surrogate losses in cost-sensitive
multiclass classification, providing conditions for consistency, bounding the excess loss of a predictor,
and extending the analysis to the “Simplex Coding” scheme. We are considering the generalisation gap
rather than the excess loss. Lei et al. [2019] study data-dependent bounds for multiclass classification.
Their analysis is restricted to SVMs however, whereas ours applies to arbitrary hypothesis spaces."
INTRODUCTION,0.020804438280166437,"Outline.
We fix notation in Section 2. Theorem 1 in Section 3 is our main result—a PAC-Bayes
bound on the KL-divergence between the true and empirical error distributions. For multiclass
classification with a fully refined partition this becomes a bound on the KL-divergence between the
true and empirical confusion matrices. Proposition 1 then bounds the individual error probabilities.
Our second main result, Theorem 2 in Section 4, allows us to use bounds on linear combinations of
error probabilities as training objectives. We prove Theorem 1 in Section 5 via Proposition 4, which
bounds the distribution of errors via a general convex function d and may be of independent interest.
Section 6 outlines positive empirical results1 from using our bound as a training objective for neural
networks and Section 7 gives perspectives for follow-up work.."
NOTATION,0.022191400832177532,"2
Notation"
NOTATION,0.023578363384188627,"For any set A, let M(A) be the set of probability measures on A. Let X and Y be arbitrary input
(e.g., feature) and output (e.g., label) sets respectively, and D ∈M(X × Y) be a data-generating
distribution. For any sample S ∼Dm drawn i.i.d. from D, let ˆD(S) ∈M(X × Y) denote the
empirical distribution ˆD(S) :=
1
m
P"
NOTATION,0.024965325936199722,"(x,y)∈S δ(x,y). We consider the setting where the user has
specified a partition {E1, . . . , EM} of Y2 into M error types."
NOTATION,0.026352288488210817,"We are interested in simple hypotheses h : X →Y and soft hypotheses H : X →M(Y). For
example, a neural network outputting scores (logits) in RY is converted to a simple or soft hypothesis,
respectively, by passing the scores through the argmax or softmax function, respectively. For any
A ⊆Y, H(x)(A) can be interpreted as the probability according to H that the label of x is in A. We
will see in Section 4 that soft hypotheses permit more flexible training procedures and a more fine-
grained analysis. Note that while soft hypotheses output distributions, they do so deterministically,
always returning the same distribution for the same input x, and so are distinct from the stochastic
classifiers introduced shortly."
NOTATION,0.027739251040221916,"For a simple hypothesis h : X →Y and j ∈[M], define the j-risk of h to be Rj
D(h) :=
P(x,y)∼D((h(x), y) ∈Ej), namely the probability that h makes an error of type Ej for a ran-
domly sampled (x, y) ∼D. For a soft hypothesis H : X →M(Y) define the j-risk of H to be
Rj
D(H) := P(x,y)∼D,ˆy∼H(x)((ˆy, y) ∈Ej), namely the probability that one would make an error of
type Ej on a randomly sampled (x, y) ∼D if one predicted by sampling ˆy from the distribution
H(x). From now until Section 4 it will not matter whether we are dealing with simple or soft
hypotheses. So, unless stated explicitly, we will refer to both simply as hypotheses, denote both by
lowercase h, and refer to the hypothesis class H, whether it is a subset of YX or M(Y)X ."
NOTATION,0.02912621359223301,"Our goal is to control the risk vector RD(h) := (R1
D(h), . . . , RM
D (h)), since controlling this
vector controls all linear combinations of j-risks. Since this is unobervable, we will control it by"
NOTATION,0.030513176144244106,1Code available here: https://github.com/reubenadams/PAC-Bayes-Control
NOTATION,0.0319001386962552,"bounding how far it diverges from its empirical counterpart RS(h) := R ˆ
D(S)(h), which we term
the empirical risk vector. Note that ES∼DmRS(h) = RD(h), and that, for a simple hypothesis
h ∈YX , RS(h) is the vector of proportions of the sample on which h makes an error of type
Ej2. Since the Ej partition Y2, RD(h) and RS(h) are elements of the M-dimensional simplex
△M := {u ∈[0, 1]M : u1 + · · · + uM = 1}. Thus we can choose our divergence measure to be
kl(RS(h)∥RD(Q)), where for q, p ∈△M we define kl(q∥p) := PM
j=1 qj ln qj"
NOTATION,0.033287101248266296,"pj .3 When M = 2 we
abbreviate kl((q, 1 −q)∥(p, 1 −p)) to kl(q∥p), which is then the conventional definition of kl(·∥·)
found in the PAC-Bayes literature [as in Seeger, 2002, for example]. We define the risk and empirical
risk of Q as RD(Q) := Eh∼QRD(h) and RS(Q) := Eh∼QRS(h), respectively, and seek a bound
on kl(RS(Q)∥RD(Q)). Note we still have ES[RS(Q)] = RD(Q), this time using Fubini. Moreover,
for a sample S of size m, we have that RS(Q) = K/m where K ∼Mult(m, M, RD(Q)). Recall
that for m, M ∈N and r ∈△M, the multinomial distribution Mult(m, M, r) has probability mass
function Mult(k; m, M, r) :=
 
m
k1 k2 ··· kM
 QM
j=1 rkj
j ,
where
 
m
k1 k2 ··· kM

:=
m!
QM
j=1 kj! for"
NOTATION,0.03467406380027739,"k ∈Sm,M :=

(k1, . . . , kM) ∈NM
0 : k1 + · · · + kM = m
	
, and zero otherwise. As a final piece
of notation, we let △>0
M := △M ∩(0, 1)M and S>0
m,M := Sm,M ∩NM denote the vector elements of
△M and Sm,M, respectively, that have no zero components."
MAIN RESULT,0.036061026352288486,"3
Main result"
MAIN RESULT,0.03744798890429958,"We now state our main result, which bounds the KL-divergence between the true and empirical
risk vectors RD(Q) and RS(Q), interpreted as probability distributions. As is conventional in the
PAC-Bayes literature, we refer to sample independent and dependent distributions on M(H) (i.e.
stochastic hypotheses) as priors (denoted P) and posteriors (denoted Q) respectively, even if they are
not related by Bayes’ theorem."
MAIN RESULT,0.038834951456310676,"Theorem 1. Let X and Y be arbitrary sets and SM
j=1 Ej be a disjoint partition of Y2 into M error
types. Let D ∈M(X × Y) be a data-generating distribution and H be a simple (H ⊆YX ) or soft
(H ⊆M(Y)X ) hypothesis class. For any prior P ∈M(H), δ ∈(0, 1] and sample size m ≥M,
with probability at least 1 −δ over the random draw S ∼Dm, we have that simultaneously for all
posteriors Q ∈M(H), the divergence kl
 
RS(Q)∥RD(Q)

is upper bounded by"
M,0.04022191400832178,"1
m"
M,0.04160887656033287,"
KL(Q∥P) + ln ξ(m, M) δ"
M,0.04299583911234397,"
,
where
(1)"
M,0.044382801664355064,"ξ(m, M) := √πe1/(12m)   m"
M,0.04576976421636616,"2
 M−1"
M,0.047156726768377254,"2
PM−1
z=0
 M
z
   2"
M,0.04854368932038835,"m
z/2 Γ
  M−z"
M,0.049930651872399444,"2
−1 ∈O
 
(mM)M
."
M,0.05131761442441054,"The fact that the logarithmic term is of order O(M ln(mM/δ)) means the bound is linear in M up to
logarithmic terms, while this may seem excessive, one should note that the quantity that our theorem
bounds also depends on M. Further, the bound has been successfully used in by Biggs and Guedj
[2023] to improve on state of the art PAC-Bayes bounds."
M,0.052704576976421634,"To see how our bound compares to existing PAC-Bayes bounds for binary classification, take
Y = {−1, 1}, M = 2, E1 = {(−y, y) : y ∈Y} and E2 = {(y, y) : y ∈Y}. The argument of the
logarithm then reduces to 1"
M,0.05409153952843273,"δ e1/(12m)  
2 + p πm"
M,0.05547850208044383,"2

≤1.25√m when m is large. The corresponding
term in Maurer [2004] is 2√m, which is only larger because he relaxes the term for aesthetics.
Therefore our bound gracefully reduces to Maurer’s in the case of binary classification."
M,0.056865464632454926,"Suppose after a use of Theorem 1 we have a bound of the form kl(RS(Q)∥RD(Q)) ≤B. We
can then derive bounds on the individual j-risks Rj
D(Q) or, more generally, on linear combinations
thereof. While one could obtain such bounds perhaps more directly with existing PAC-Bayes
bounds, the significance of our bound is that all such derived bounds hold with high probability
simultaneously. Existing PAC-Bayes bounds would require the use of a union bound in order to bound
multiple combinations simultaneously, whereas ours bounds all uncountably many combinations
simultaneously, as a package. As for the individual j-risks Rj
D(Q), the following proposition"
M,0.05825242718446602,"2(RS(h))j = Rj
ˆ
D(h) = P(x,y)∼ˆ
D((h(x), y) ∈Ej) =
1
m
P"
M,0.059639389736477116,"(x,y)∈S 1[(h(x), y) ∈Ej].
3We follow the usual convention that 0 ln 0"
M,0.06102635228848821,x = 0 for x ≥0 and x ln x
M,0.06241331484049931,0 = ∞for x > 0.
M,0.0638002773925104,"then yields the bounds Lj ≤Rj
D(Q) ≤Uj, where Lj := inf{p ∈[0, 1] : kl(Rj
S(Q)∥p) ≤
B} and Uj := sup{p ∈[0, 1] : kl(Rj
S(Q)∥p) ≤B}. Moreover, since in the worst case we
have kl(RS(Q)∥RD(Q)) = B, the proposition shows that the lower and upper bounds Lj and
Uj are the tightest possible, since if Rj
D(Q) ̸∈[Lj, Uj] then kl(Rj
S(Q)∥Rj
D(Q)) > B implying
kl(RS(Q)∥RD(Q)) > B. For a more precise version of this argument and a proof of Proposition 1,
see Appendix C.4.
Proposition 1. Let q, p ∈△M. Then kl(qj∥pj) ≤kl(q∥p) for all j ∈[M], with equality when
pi = 1−pj"
M,0.0651872399445215,1−qj qi. for all i ̸= j.
M,0.06657420249653259,"More generally, suppose we can quantify how costly an error of each type is by means of a loss vector
ℓ∈[0, ∞)M, where ℓj is the loss we attribute to an error of type Ej. We may then be interested in
bounding the total risk RT
D(Q) := ℓ· RD(Q). Then, given a bound kl(RS(Q)∥RD(Q)) ≤B from
Theorem 1, we can deduce
RT
D(Q) ≤sup {ℓ· r : r ∈△M, kl(RS(Q)∥r) ≤B} = ℓ· kl−1
ℓ(RS(Q)|B),
(2)"
M,0.06796116504854369,"where we define kl−1
ℓ(u|c) ∈△M as follows. To see that it is indeed well-defined (at least when
u ∈△>0
M ), see the discussion at the beginning of Appendix C.5."
M,0.06934812760055478,"Definition 1. For u ∈△M, c ∈[0, ∞) and ℓ∈[0, ∞)M, define kl−1
ℓ(u|c) to be an element
v ∈△M solving the constrained optimisation problem
Maximise:
fℓ(v) := ℓ· v,
(3)
Subject to:
kl(u∥v) ≤c.
(4)"
M,0.07073509015256588,"This motivates the following training procedure: search for a posterior Q for which the bound
ℓ· kl−1
ℓ(RS(Q)|B) on the total risk RT
D(Q) is minimised. While this requires a particular choice of
loss vector ℓ, we emphasise that at the end of training, Theorem 1 bounds kl(RS(Q)∥RD(Q)) and
so can be used to bound any linear combination of the j-risks, not just the one given the loss vector ℓ
chosen for training. It is this flexibility which is the main advantage of our bound; changes in the
severity of different error types over time do not require union bounds or retraining."
M,0.07212205270457697,"In the next section we provide a theorem for calculating kl−1
ℓ(u|c) and its derivatives so that the
training procedure can be executed."
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.07350901525658807,"4
Construction of a Differentiable Training Objective"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.07489597780859916,"We now state and prove Theorem 2, which provides a speedy method for approximating kl−1
ℓ(u|c)
and its derivatives to arbitrary precision, provided c > 0 and ∀j uj > 0. The only approximation
step required is that of approximating the unique root of a continuous and strictly increasing scalar
function. Thus, provided the uj themselves are differentiable, Theorem 1 combined with Theorem
2 shows that the upper bound on the total risk can be used as a tractable and fully differentiable
training objective. See Appendix A for more details, including a pseudocode algorithm and an
implementation. Since the proof of Theorem 2 is rather long and technical, we defer it to Appendix
C.5. The requirement that the ℓj are not all equal only rules out trivial cases where RT
D(Q) is
independent of RD(Q).
Theorem 2. Fix ℓ∈[0, ∞)M such that not all ℓj are equal, and define fℓ: △M →[0, ∞) by
fℓ(v) := PM
j=1 ℓjvj. For all ˜u = (u, c) ∈△>0
M ×(0, ∞), define v∗(˜u) := kl−1
ℓ(u|c) ∈△M and let
µ∗(˜u) ∈(−∞, −maxj ℓj) be the unique solution to c = ϕℓ(µ), where ϕℓ: (−∞, −maxj ℓj) →R
is given by ϕℓ(µ) := ln(−PM
j=1
uj
µ+ℓj ) + PM
j=1 uj ln(−(µ + ℓj)), which is continuous and strictly"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.07628294036061026,"increasing. Then v∗(˜u) = kl−1
ℓ(u|c) is given by"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.07766990291262135,"v∗(˜u)j =
λ∗(˜u)uj
µ∗(˜u) + ℓj
for j ∈[M], where
λ∗(˜u) =  
M
X j=1"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.07905686546463246,"uj
µ∗(˜u) + ℓj   −1 .
(5)"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08044382801664356,"Further, defining f ∗
ℓ: △>0
M × (0, ∞) →[0, ∞) by f ∗
ℓ(˜u) := fℓ(v∗(˜u)), we have that"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08183079056865465,"∂f ∗
ℓ
∂uj
(˜u) = λ∗(˜u)

1 + ln
uj
v∗(˜u)j"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08321775312066575,"
and
∂f ∗
ℓ
∂c (˜u) = −λ∗(˜u).
(6)"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08460471567267684,"A final wrinkle in evaluating our bound is that while the empirical risk vector RS(Q) = Eh∼QRS(h)
does not depend on the data-generating distribution D, the expectation over Q may still be in-
tractable. This would be the default case when Q is a Gaussian over the weights of a multi-
layer perceptron, for example. In such cases, we can estimate RS(Q) via a Monte Carlo sample
RS( ˆQ) :=
1
N
PN
n=1 RS(hn) (where the hn are drawn i.i.d. from Q) and use the following two
results. Proposition 2 shows that the kl(Rj
S( ˆQ)∥Rj
D(Q)) can be simultaneously bounded, whence
Proposition 3 can be used to obtain a bound on kl(RS( ˆQ)∥RD(Q))."
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08599167822468794,"Proposition 2. Let X ∼Multinomial(N, M, p). Then for any δ ∈(0, 1), with probability at least"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08737864077669903,"1 −δ we have that for all j ∈[M] simultaneously kl
  1"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.08876560332871013,"N Xj
pj

≤ln 2M δ
N
."
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.09015256588072122,"Proof. Each bound holds separately with probability at least 1 −δ/M by Theorem 2.5 in Langford
and Caruana [2001]. They then hold simultaneously by application of a union bound.
Proposition 3. Suppose q, p, ˆq ∈△M are such that kl(q∥p) ≤B1 and kl(ˆqj∥qj) ≤B2 for all
j ∈[M]. For each j, define qj = inf{r ∈[0, 1] : kl(ˆqj∥r) ≤B2}. Then"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.09153952843273232,"kl(ˆq∥p) ≤MB2 − M
X"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.09292649098474341,"j=1
(1 −ˆqj) ln 1 −ˆqj"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.09431345353675451,"1 −qj
+ B1 max
j
ˆqj
qj
→B1
as
B2 →0.
(7)"
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.0957004160887656,Proof. Deferred to C.1.
CONSTRUCTION OF A DIFFERENTIABLE TRAINING OBJECTIVE,0.0970873786407767,"The fact that the bound on kl(ˆq∥p) →B1 as B2 →0 ensures that as we increase the size of our
Monte Carlo sample for estimating RS(Q) the bound on kl(RS( ˆQ)∥RD(Q)) approaches that of
kl(RS(Q)∥RD(Q)), meaning in the limit we pay an arbitrarily small price in the bound for the
approximation."
PROOF OF THE MAIN BOUND,0.09847434119278779,"5
Proof of the main bound"
PROOF OF THE MAIN BOUND,0.09986130374479889,"We split the proof of Theorem 1 into three parts. First, we prove Proposition 4, a bound on
d(RS(Q), RD(Q)) for an arbitrary convex function d, which may be of independent interest. Second,
we prove Corollary 1 by specialising Proposition 4 to the case d(·, ·) = kl(·∥·). Finally, we show that
the bound in Theorem 1 is a loosened version of the bound in Corollary 1."
PROOF OF THE MAIN BOUND,0.10124826629680998,"Proposition 4. Let d : △2
M →R be jointly convex. In the setting of Theorem 1,"
PROOF OF THE MAIN BOUND,0.10263522884882108,"d
 
RS(Q), RD(Q)

≤1 β"
PROOF OF THE MAIN BOUND,0.10402219140083217,"
KL(Q∥P) + ln Id(m, β) δ"
PROOF OF THE MAIN BOUND,0.10540915395284327,"
,
where
(8)"
PROOF OF THE MAIN BOUND,0.10679611650485436,"Id(m, β) := supr∈△M
hP
k∈Sm,M Mult(k; m, M, r) exp

βd
  k"
PROOF OF THE MAIN BOUND,0.10818307905686546,"m, r
i
."
PROOF OF THE MAIN BOUND,0.10957004160887657,"This is a generalisation of the unifying PAC-Bayes bound given in Bégin et al. [2016] where we replace
the scalar risk quantities RS(Q) and RD(Q) with their vector counterparts RS(Q) and RD(Q). To
see this, note that we can recover it by setting Y = {−1, 1}, M = 2, E1 = {(−y, y) : y ∈Y}
and E2 = {(y, y) : y ∈Y}. Then, for any convex function d : [0, 1]2 →R, apply Theorem
4 with the convex function d′ : △2
M →R defined by d′((u1, u2), (v1, v2)) := d(u1, v1) so that
Theorem 4 bounds d′ 
RS(Q), RD(Q)

= d
 
R1
S(Q), R1
D(Q)

which equals d(RS(Q), RD(Q))"
PROOF OF THE MAIN BOUND,0.11095700416088766,"in the notation of Bégin et al. [2016]. Further, P"
PROOF OF THE MAIN BOUND,0.11234396671289876,"k∈Sm,2 Mult(k; m, 2, r) exp

βd′  k"
PROOF OF THE MAIN BOUND,0.11373092926490985,"m, r
 
=
Pm
k=0 Bin(k; m, r1) exp

βd
  k"
PROOF OF THE MAIN BOUND,0.11511789181692095,"m, r1
 
, so that the supremum over r1 ∈[0, 1] of the right hand side
equals the supremum over r ∈△2 of the left hand side, which, when substituted into (8), yields the
bound given in Bégin et al. [2016]."
PROOF OF THE MAIN BOUND,0.11650485436893204,"To prove Proposition 4 we require the following two lemmas. The first is the well-known change of
measure in equality (Csiszár, 1975, Donsker and Varadhan, 1975). The second is a generalisation
from Binomial to Multinomial distributions of a result found in Maurer [2004], the proof of which
we defer to Appendix C.2."
PROOF OF THE MAIN BOUND,0.11789181692094314,"Lemma 1. For any set H, any P, Q ∈M(H) and any measurable function ϕ : H →R, E
h∼Qϕ(h) ≤"
PROOF OF THE MAIN BOUND,0.11927877947295423,"KL(Q∥P) + ln E
h∼P exp(ϕ(h))."
PROOF OF THE MAIN BOUND,0.12066574202496533,"Lemma 2. Let X1, . . . , Xm be i.i.d △M-valued random vectors with mean µ and suppose
that f : △m
M →R is convex. If X′
1, . . . , X′
m are i.i.d. Mult(1, M, µ) random vectors, then
E[f(X1, . . . , Xm)] ≤E[f(X′
1, . . . , X′
m)]."
PROOF OF THE MAIN BOUND,0.12205270457697642,"The consequence of Lemma 2 is that the worst case (in terms of bounding d(RS(Q), RD(Q))) occurs
when R{(x,y)}(h) is a one-hot vector for all (x, y) ∈S and h ∈H, namely when H ⊆M(Y)X only
contains hypotheses that, when labelling S, put all their mass on elements ˆy ∈Y that incur the same
error type4. In particular, this is the case for hypotheses that put all their mass on a single element of
Y, equivalent to the simpler case H ⊆YX as discussed in Section 2. Thus, Lemma 2 shows that the
bound given in Proposition 4 cannot be made tighter only by restricting to such hypotheses."
PROOF OF THE MAIN BOUND,0.12343966712898752,"Proof. (of Proposition 4) The case H ⊆YX follows directly from the more general case by taking
H′ := {h′ ∈M(Y)X : ∃h ∈H such that ∀x ∈X h′(x) = δh(x)}, where δh(x) ∈M(Y) denotes a
point mass on h(x). For the general case H ⊆M(Y)X , using Jensen’s inequality with the convex
function d(·, ·) and Lemma 1 with ϕ(h) = βd(RS(h), RD(h)), we see that for all Q ∈M(H)"
PROOF OF THE MAIN BOUND,0.12482662968099861,"βd
 
RS(Q), RD(Q)

= βd

E
h∼QRS(h), E
h∼QRD(h)
"
PROOF OF THE MAIN BOUND,0.1262135922330097,"≤
E
h∼Qβd
 
RS(h), RD(h)
"
PROOF OF THE MAIN BOUND,0.1276005547850208,"≤KL(Q∥P) + ln

E
h∼P exp

βd
 
RS(h), RD(h)
"
PROOF OF THE MAIN BOUND,0.1289875173370319,"= KL(Q∥P) + ln(ZP (S)),"
PROOF OF THE MAIN BOUND,0.130374479889043,"where ZP (S) := Eh∼P exp
 
βd(RS(h), RD(h))

. Note that ZP (S) is a non-negative random"
PROOF OF THE MAIN BOUND,0.1317614424410541,"variable, so that by Markov’s inequality
P
S∼Dm"
PROOF OF THE MAIN BOUND,0.13314840499306518,"
ZP (S) ≤ES′∼DmZP (S′)"
PROOF OF THE MAIN BOUND,0.13453536754507628,"δ

≥1−δ. Thus, since ln(·)"
PROOF OF THE MAIN BOUND,0.13592233009708737,"is strictly increasing, with probability at least 1 −δ over S ∼Dm, we have that simultaneously for
all Q ∈M(H)"
PROOF OF THE MAIN BOUND,0.13730929264909847,"βd
 
RS(Q), RD(Q)

≤KL(Q∥P) + ln
E
S′∼DmZP (S′)"
PROOF OF THE MAIN BOUND,0.13869625520110956,"δ
.
(9)"
PROOF OF THE MAIN BOUND,0.14008321775312066,"To bound ES′∼DmZP (S′), let Xi := R{(xi,yi)′}(h) ∈△M for i ∈[m], where (xi, yi)′ is the
i’th element of the dummy sample S′. Noting that each Xi has mean RD(h), define the random
vectors X′
i ∼Mult(1, M, RD(h)) and Y := Pm
i=1 X′
i ∼Mult(m, M, RD(h)). Finally let f :
△m
M →R be defined by f(x1, . . . , xm) := exp
 
βd
  1"
PROOF OF THE MAIN BOUND,0.14147018030513175,"m
Pm
i=1 xi, RD(h)

, which is convex since
the average is linear, d is convex and the exponential is non-decreasing and convex. Then, by
swapping expectations (which is permitted by Fubini’s theorem since the argument is non-negative)
and applying Lemma 2, we have that ES′∼DmZP (S′) can be written as"
PROOF OF THE MAIN BOUND,0.14285714285714285,"ES′∼DmZP (S′) =
E
S′∼Dm
E
h∼P exp

βd
 
RS′(h), RD(h)
"
PROOF OF THE MAIN BOUND,0.14424410540915394,"= E
h∼P
E
S′∼Dm exp

βd
 
RS′(h), RD(h)
"
PROOF OF THE MAIN BOUND,0.14563106796116504,"= E
h∼P
E
X1,...,Xm exp  βd"
M,0.14701803051317613,"1
m m
X"
M,0.14840499306518723,"i=1
Xi, RD(h) !!"
M,0.14979195561719832,"≤E
h∼P
E
X′
1,...,X′m
exp  βd"
M,0.15117891816920942,"1
m m
X"
M,0.15256588072122051,"i=1
X′
i, RD(h) !!"
M,0.1539528432732316,"= E
h∼P E
Y exp

βd
 1"
M,0.1553398058252427,"mY , RD(h)
"
M,0.15672676837725383,"4More precisely, when ∀h ∈H ∀(x, y) ∈S ∃j ∈[M] such that h(x)[{ˆy ∈Y : (ˆy, y) ∈Ej)}] = 1."
M,0.15811373092926492,"= E
h∼P X"
M,0.15950069348127602,"k∈Sm,M
Mult
 
k; m, M, RD(h)

exp

βd
  k"
M,0.1608876560332871,"m, RD(h)
"
M,0.1622746185852982,"≤sup
r∈△M  
X"
M,0.1636615811373093,"k∈Sm,M
Mult
 
k; m, M, r

exp

βd
  k"
M,0.1650485436893204,"m, r

 ,"
M,0.1664355062413315,"which is the definition of Id(m, β). Inequality (8) then follows by substituting this bound on
ES′∼DmZP (S′) into (9) and dividing by β."
M,0.1678224687933426,"We now specialise Proposition 4 to the case d(·, ·) = kl(·∥·) to obtain Corollary 1.
Corollary 1. In the setting of Theorem 1,"
M,0.16920943134535368,"kl
 
RS(Q)∥RD(Q)

≤1 m"
M,0.17059639389736478,"
KL(Q∥P) + ln η(m, M) δ"
M,0.17198335644937587,"
,
where
(10)"
M,0.17337031900138697,"η(m, M) := m! mm
X"
M,0.17475728155339806,"k∈Sm,M M
Y j=1"
M,0.17614424410540916,"kkj
j
kj! .
(11)"
M,0.17753120665742025,"Proof. Applying Proposition 4 with d(·, ·) = kl(·∥·) and β = m gives that with probability at least
1 −δ over S ∼Dm, simultaneously for all posteriors Q ∈M(H),"
M,0.17891816920943135,"kl
 
RS(Q)∥RD(Q)

≤1 m"
M,0.18030513176144244,"
KL(Q∥P) + ln Ikl(m, m) δ 
,"
M,0.18169209431345354,"where Ikl(m, m) := supr∈△M [P"
M,0.18307905686546463,"k∈Sm,M Mult(k; m, M, r) exp
 
mkl( k"
M,0.18446601941747573,"m, r)

]. Thus it suffices to
show that Ikl(m, m) ≤η(m, M)."
M,0.18585298196948682,"To prove this, for each fixed r = (r1, . . . , rM) ∈△M let Jr = {j ∈[M] : rj = 0}. Then
Mult(k; m, M, r) = 0 for any k ∈Sm,M such that kj ̸= 0 for some j ∈Jr. For the other
k ∈Sm,M, namely those such that kj = 0 for all j ∈Jr, the probability term can be written as
Mult(k; m, M, r) =
m!
QM
j=1 kj!
QM
j=1 rkj
j
=
m!
Q"
M,0.18723994452149792,"j̸∈Jr kj!
Q"
M,0.18862690707350901,"j̸∈Jr rkj
j , and (recalling the convention that"
M,0.1900138696255201,0 ln 0
M,0.1914008321775312,0 = 0) the term exp(mkl( k
M,0.1927877947295423,"m, r)) can be written as exp  m M
X j=1"
M,0.1941747572815534,"kj
m ln"
M,0.1955617198335645,"kj
m
rj "
M,0.19694868238557559,= exp  X
M,0.19833564493758668,"j̸∈Jr
kj ln kj mrj  =
Y j̸∈Jr  kj mrj"
M,0.19972260748959778,"kj
=
1
mm
Y j̸∈Jr kj rj kj
,"
M,0.20110957004160887,"where the last equality is obtained by recalling that the kj sum to m. Substituting these two
expressions into the definition of Ikl(m, m) and only summing over those k ∈Sm,M with non-zero
probability, we obtain
X"
M,0.20249653259361997,"k∈Sm,M
Mult(k; m, M, r) exp
 
mkl
  k"
M,0.20388349514563106,"m, r

=
X"
M,0.20527045769764216,"k∈Sm,M :
∀j∈Jr kj=0"
M,0.20665742024965325,"Mult(k; m, M, r) exp
 
mkl
  k"
M,0.20804438280166435,"m, r
 =
X"
M,0.20943134535367544,"k∈Sm,M :
∀j∈Jr kj=0 m!
Q"
M,0.21081830790568654,j̸∈Jr kj! Y
M,0.21220527045769763,"j̸∈Jr
rkj
j
1
mm
Y j̸∈Jr kj rj kj = m! mm
X"
M,0.21359223300970873,"k∈Sm,M :
∀j∈Jr kj=0 Y j̸∈Jr"
M,0.21497919556171982,"kkj
j
kj! = m! mm
X"
M,0.21636615811373092,"k∈Sm,M :
∀j∈Jr kj=0 M
Y j=1"
M,0.217753120665742,"kkj
j
kj!
(because 00"
M,0.21914008321775313,"0! = 1) ≤m! mm
X"
M,0.22052704576976423,"k∈Sm,M M
Y j=1"
M,0.22191400832177532,"kkj
j
kj! ,"
M,0.22330097087378642,"which is η(m, M). Since this is independent of r, it also holds after taking the supremum over
r ∈△M of the left hand side, showing that Ikl(m, m) ≤η(m, M)."
M,0.22468793342579751,"The final step in obtaining Theorem 1 is to loosen the bound given in Corollary 1 (which is intractable
when m is large) to the tractable form given in Theorem 1. For this we require the following technical
lemma, the proof of which we defer to Appendix C.3."
M,0.2260748959778086,"Lemma 3. For integers M ≥1 and m ≥M, P"
M,0.2274618585298197,"k∈S>0
m,M
1
QM
j=1
√"
M,0.2288488210818308,"kj ≤π
M"
M,0.2302357836338419,"2 m
M−2"
M,0.231622746185853,"2
Γ( M 2 )
."
M,0.23300970873786409,"Proof. (Of Theorem 1) It suffices to show that for all m ≥M ≥1 we have η(m, M) ≤ξ(m, M).
We achieve this by applying Stirling’s approximation
√"
M,0.23439667128987518,"2πn
  n"
M,0.23578363384188628,"e
n < n! <
√"
M,0.23717059639389737,"2πn
  n"
M,0.23855755894590847,"e
n e
1
12n (valid
for n ≥1) to the factorials in η(m, M) and then using Lemma 3."
M,0.23994452149791956,"Since Stirling’s approximation requires that all the kj are at least one, we partition the sum in
η(m, M) according to the number of coordinates of k at which kj = 0. Let z index the number of
such coordinates. Defining f : S∞
M=2 Sm,M →R by f(k) = Q|k|
j=1 kkj
j /kj! and noting that f is
symmetric under permutations of its arguments, we then have"
M,0.24133148404993066,"η(m, M) = m! mm
X"
M,0.24271844660194175,"k∈Sm,M
f(k) = m! mm M−1
X z=0 M
z 
X"
M,0.24410540915395285,"k∈S>0
m,M−z"
M,0.24549237170596394,"f(k).
(12)"
M,0.24687933425797504,"Stirling’s approximation can now be applied to each k ∈S>0
m,M f(k) ≤QM
j=1
k
kj
j
√"
M,0.24826629680998613,"2πkj
 kj"
M,0.24965325936199723,"e
kj ="
M,0.2510402219140083,"QM
j=1
ekj
√"
M,0.2524271844660194,"2πkj =
em"
M,0.2538141470180305,"(2π)M/2
QM
j=1
1
√"
M,0.2552011095700416,kj . An application of Lemma 3 now gives X
M,0.2565880721220527,"k∈S>0
m,M−z"
M,0.2579750346740638,"f(k) ≤
X"
M,0.2593619972260749,"k∈S>0
m,M−z em"
M,0.260748959778086,"(2π)
M−z 2 M−z
Y j=1"
P,0.2621359223300971,"1
p"
P,0.2635228848821082,"kj
≤
em"
P,0.26490984743411927,"(2π)
M−z"
P,0.26629680998613037,"2
π
M−z"
M,0.26768377253814146,"2
m
M−z−2 2"
M,0.26907073509015256,"Γ
  M−z"
M,0.27045769764216365,"2

=
emm
M−z−2 2 2
M−z"
M,0.27184466019417475,"2
Γ
  M−z 2
."
M,0.27323162274618584,"Substituting this into equation (12) and bounding m! using Stirling’s approximation, we have"
M,0.27461858529819694,"η(m, M) ≤
√"
M,0.27600554785020803,2πme1/(12m)
M,0.27739251040221913,"em
PM−1
z=0
 M
z
 emm
M−z−2 2 2
M−z"
M,0.2787794729542302,"2
Γ( M−z"
M,0.2801664355062413,"2 )
= ξ(m, M), which completes the proof of"
M,0.2815533980582524,"the bound. As for the order of the bound, it is sufficient to bound ln ξ(m, M) using the crude
approximations
 M
z

≤M M, (2/m)z/2 ≤1 and Γ((M −z)/2) ≥1."
NUMERICAL EXPERIMENTS,0.2829403606102635,"6
Numerical experiments"
NUMERICAL EXPERIMENTS,0.2843273231622746,"We use binarised versions of MNIST, and HAM10000 Tschandl [2018]. In both cases we partition
Y2 into E0 = {(0, 0), (1, 1)}, E1 = {(0, 1)} and E2 = {(1, 0)}, and take ℓ= (0, 1, 3). Each
dataset is split into prior and certification sets. We take H to be two-layer MLPs. As is common in
the PAC-Bayes literature, we restrict P and Q to be isotropic and diagonal Gaussian distributions
over the parameter space, respectively. The means of P and Q are set to the parameters of an MLP
trained on the prior set. The mean and variances of Q and the variance of P are tuned via Theorem
2 to minimize the bound on the total risk RT
D(Q). See Appendix A for pseudocode, Appendix B
for full experimental details and https://github.com/reubenadams/PAC-Bayes-Control for
code. The results for MNIST can be seen in Figure 1."
NUMERICAL EXPERIMENTS,0.2857142857142857,"We estimate RS(Q) with a Monte Carlo and obtain a PAC-Bayes bound on RT
D(Q) by combining
Proposition 2 (with δ = 0.01 and N = 100000) and Proposition 3. We obtain RT
D(Q) ≤0.2640 for
MNIST and RT
D(Q) ≤0.8379 for HAM10000, where both bounds hold with probability at least
1 −0.05 −0.01 = 0.94. While these bounds are far from vacuous—the maximum possible value of
RT
D(Q) is 3 for our choice of ℓ—one might wonder whether one can do better by bounding each error
probability individually using Maurer’s inequality Maurer [2004], and then unioning these bounds.
As with our Theorem 1, this would also constrain the entire distribution of error types since for any ℓ,
one could then calculate the maximimum value of RT
D(Q) that satisfies all of these constraints. Both"
NUMERICAL EXPERIMENTS,0.2871012482662968,"Dataset
Volume Our Region
Volume Maurer Region
MNIST
0.0025 (0.002498, 0.002504)
0.0028 (0.002793, 0.002800)
HAM10000
0.0012 (0.001207, 0.001211)
0.0011 (0.001142, 0.001146)"
NUMERICAL EXPERIMENTS,0.2884882108183079,"Table 1: Point estimates and 95% confidence intervals for the volumes of the confidence regions
for RD(Q) given by Theorem 1 and a union over M individual Maurer bounds, respectively. Our
method is superior for MNIST and inferior for HAM100000."
NUMERICAL EXPERIMENTS,0.289875173370319,"(a)
(b)
(c)"
NUMERICAL EXPERIMENTS,0.2912621359223301,"Figure 1: Experimental results for binarised MNIST. (a) The PAC-Bayes bound on the total risk
decreases when tuning the posterior via Theorem 2. (b) This is achieved by a shift in the empirical
error probabilities. (c) The bound on kl(RS(Q)∥RD(Q)) is not substantially increased, meaning we
still retain good control of RD(Q) after optimizing Q for this particular choice of ℓ."
NUMERICAL EXPERIMENTS,0.2926490984743412,"methods constrain the region of the simplex in which RD(Q) can lie (with high probability), and a
reasonable metric by which to compare them is the volumes of these regions. This can be estimated
via a MC sample by uniformly sampling points r from △M and counting how samples are legal
values of RD(Q) according to each method. The 95% confidence intervals for the volumes of the
two regions are given in Table 1. A more comprehensive table for synthetic values of RS(Q) can be
found in Appendix B."
PERSPECTIVES,0.29403606102635227,"7
Perspectives"
PERSPECTIVES,0.29542302357836336,"We introduce the framework of error types, considering the vectors RS(Q) and RD(Q) of empirical
and true probabilities of errors of different types. We prove a PAC-Bayes bound (Theorem 1) on
kl(RS(Q)∥RD(Q)) which controls the entire distribution of error probabilities, and hence can be
used to derive bounds on arbitrary linear combinations of the error probabilities, all of which hold
simultaneously with high probability; this cannot be achieved with any existing PAC-Bayes bound."
PERSPECTIVES,0.29680998613037446,"We construct a differential training objective based on our bound by introducing the the vectorised
kl inverse, providing a recipe for quickly computing its value and derivatives (Theorem 2). Our
framework is flexible enough to encompass multiclass classification or discretised regression, but
also structured output prediction, multi-task learning and learning-to-learn."
PERSPECTIVES,0.29819694868238555,"Another potential application of our work is to the excess risk, since under a misclassification loss
there are three different error types, corresponding to excess losses of {−1, 0, 1}. Biggs and Guedj
[2023] adapted Theorems 1 and 2 to this setting, leading to an empirically tighter PAC-Bayes bound
for certain classification tasks."
PERSPECTIVES,0.29958391123439665,"We require i.i.d. data, which in practice is frequently not the case or is hard to verify. Further, the
number of error types M must be finite. In continuous scenarios it would be preferable to be able to
control the entire distribution of loss values without having to discretise into finitely many error types.
We leave this direction to future work."
PERSPECTIVES,0.30097087378640774,Acknowledgments and Disclosure of Funding
PERSPECTIVES,0.30235783633841884,"We warmly thank reviewers and the Area Chair who provided insigthful comments and suggestions
which greatly helped us improve our manuscript. R.A. was supported by the UKRI grant number
EP/S021566/1 and gratefully thanks Felix Biggs for his insights. J.S-T gratefully acknowledges the
European Union’s Horizon 2020 Research and Innovation Program through the grant numbers 951847
(European learning and intelligent systems excellence, ELISE) and 952026 (Human-centred artificial
intelligence, HumanE-AI-Net). B.G. acknowledges partial support by the U.S. Army Research
Laboratory and the U.S. Army Research Office, and by the U.K. Ministry of Defence and the U.K.
Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/R013616/1.
B.G. acknowledges partial support from the French National Agency for Research, through grants
ANR-18-CE40-0016-01 and ANR-18- CE23-0015-02, and through the programme “France 2030”
and PEPR IA on grant SHARP ANR-23-PEIA-0008."
REFERENCES,0.30374479889042993,References
REFERENCES,0.30513176144244103,"Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv preprint arXiv:2110.11216,
2021."
REFERENCES,0.3065187239944521,"Amiran Ambroladze, Emilio Parrado-Hernández, and John Shawe-Taylor. Tighter PAC-Bayes
bounds. In Bernhard Schölkopf, John C. Platt, and Thomas Hofmann, editors, Advances in Neural
Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural
Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006,
pages 9–16. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2006/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
REFERENCES,0.3079056865464632,"Felix Biggs and Benjamin Guedj. On margins and derandomisation in pac-bayes. In International
Conference on Artificial Intelligence and Statistics, pages 3709–3731. PMLR, 2022a."
REFERENCES,0.3092926490984743,"Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks.
In International Conference on Machine Learning, pages 1963–1981. PMLR, 2022b."
REFERENCES,0.3106796116504854,"Felix Biggs and Benjamin Guedj. Tighter pac-bayes generalisation bounds by leveraging example
difficulty. In International Conference on Artificial Intelligence and Statistics, pages 8165–8182.
PMLR, 2023."
REFERENCES,0.3120665742024965,"Luc Bégin, Pascal Germain, François Laviolette, and Jean-Francis Roy. PAC-Bayesian Bounds based
on the Rényi Divergence. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the
19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings
of Machine Learning Research, pages 435–444, Cadiz, Spain, 09–11 May 2016. PMLR. URL
https://proceedings.mlr.press/v51/begin16.html."
REFERENCES,0.31345353675450766,"Olivier Catoni. A PAC-Bayesian approach to adaptive classification. preprint, 840, 2003."
REFERENCES,0.31484049930651875,"Olivier Catoni. Statistical Learning Theory and Stochastic Optimization: Ecole d’Eté de Probabilités
de Saint-Flour XXXI-2001. Springer, 2004."
REFERENCES,0.31622746185852985,"Olivier Catoni. PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning,
volume 56 of Institute of Mathematical Statistics (IMS) Lecture Notes - Monograph Series. Institute
of Mathematical Statistics, 2007. ISBN 9780940600720. URL https://books.google.fr/
books?id=acnaAAAAMAAJ."
REFERENCES,0.31761442441054094,"Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Conditionally gaussian pac-bayes. In
International Conference on Artificial Intelligence and Statistics, pages 2311–2329. PMLR, 2022."
REFERENCES,0.31900138696255204,"Imre Csiszár. I-divergence geometry of probability distributions and minimization problems. The
Annals of Probability, pages 146–158, 1975."
REFERENCES,0.32038834951456313,"MD Donsker and SRS Varadhan. Large deviations for Markov processes and the asymptotic evaluation
of certain markov process expectations for large times. In Probabilistic Methods in Differential
Equations, pages 82–88. Springer, 1975."
REFERENCES,0.3217753120665742,"Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Conference on
Uncertainty in Artificial Intelligence [UAI], 2017."
REFERENCES,0.3231622746185853,"Gintare Karolina Dziugaite and Daniel M. Roy. Entropy-SGD optimizes the prior of a PAC-Bayes
bound: Generalization properties of entropy-SGD and data-dependent priors. In Jennifer G.
Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80
of Proceedings of Machine Learning Research, pages 1376–1385. PMLR, 2018. URL http:
//proceedings.mlr.press/v80/dziugaite18a.html."
REFERENCES,0.3245492371705964,"Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On
the role of data in pac-bayes bounds. In International Conference on Artificial Intelligence and
Statistics, pages 604–612. PMLR, 2021."
REFERENCES,0.3259361997226075,"Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy.
Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm. arXiv
preprint arXiv:1503.08329, 2015."
REFERENCES,0.3273231622746186,"Benjamin Guedj. A Primer on PAC-Bayesian Learning. In Proceedings of the second congress of the
French Mathematical Society, 2019. URL https://arxiv.org/abs/1901.05353."
REFERENCES,0.3287101248266297,"Sokol Koço and Cécile Capponi. On multi-class classification through the minimization of the
confusion matrix norm. In Asian Conference on Machine Learning, pages 277–292. PMLR, 2013."
REFERENCES,0.3300970873786408,"Alexandre Lacasse, François Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier.
Pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier. Advances
in Neural information processing systems, 19, 2006."
REFERENCES,0.3314840499306519,"John Langford and Rich Caruana. (not) bounding the true error. Advances in Neural Information
Processing Systems, 14, 2001."
REFERENCES,0.332871012482663,"François Laviolette, Emilie Morvant, Liva Ralaivola, and Jean-Francis Roy. Risk upper bounds
for general ensemble methods with an application to multiclass classification. Neurocomputing,
219:15–25, 2017. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2016.09.016. URL
https://www.sciencedirect.com/science/article/pii/S0925231216310177."
REFERENCES,0.3342579750346741,"Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization
bounds for multi-class classification. IEEE Transactions on Information Theory, 65(5):2995–3021,
2019. doi: 10.1109/TIT.2019.2893916."
REFERENCES,0.3356449375866852,"Gaël Letarte, Pascal Germain, Benjamin Guedj, and Francois Laviolette. Dichotomize and generalize:
PAC-Bayesian binary activated deep neural networks. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. dAlché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 6872–6882. Curran Associates, Inc., 2019."
REFERENCES,0.33703190013869627,"Guy Lever, François Laviolette, and John Shawe-Taylor. Distribution-dependent PAC-Bayes priors.
In International Conference on Algorithmic Learning Theory, pages 119–133. Springer, 2010."
REFERENCES,0.33841886269070737,"Guy Lever, François Laviolette, and John Shawe-Taylor.
Tighter PAC-Bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4–28, February 2013. ISSN
0304-3975.
doi: 10.1016/j.tcs.2012.10.013.
URL https://linkinghub.elsevier.com/
retrieve/pii/S0304397512009346."
REFERENCES,0.33980582524271846,"Andreas Maurer. A note on the PAC-Bayesian theorem. arXiv preprint cs/0411099, 2004."
REFERENCES,0.34119278779472956,"David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference
on Computational Learning Theory, pages 230–234. ACM, 1998."
REFERENCES,0.34257975034674065,"David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference
on Computational Learning Theory, pages 164–170. ACM, 1999."
REFERENCES,0.34396671289875175,"Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient
estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020."
REFERENCES,0.34535367545076284,"Emilie Morvant, Sokol Koço, and Liva Ralaivola. PAC-Bayesian generalization bound on confusion
matrix for multi-class classification. In Proceedings of the 29th International Conference on
Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc /
Omnipress, 2012. URL http://icml.cc/2012/papers/434.pdf."
REFERENCES,0.34674063800277394,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=Skz_
WfbCZ."
REFERENCES,0.34812760055478503,"Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-
Bayes bounds with data dependent priors. J. Mach. Learn. Res., 13:3507–3531, 2012. URL
http://dl.acm.org/citation.cfm?id=2503353."
REFERENCES,0.34951456310679613,"María Pérez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-
Taylor, Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic neural
networks. arXiv preprint arXiv:2109.10304, 2021."
REFERENCES,0.3509015256588072,"Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk cer-
tificates for neural networks. Journal of Machine Learning Research, 22(227):1–40, 2021. URL
http://jmlr.org/papers/v22/20-879.html."
REFERENCES,0.3522884882108183,"Bernardo Avila Pires, Csaba Szepesvari, and Mohammad Ghavamzadeh. Cost-sensitive multiclass
classification risk bounds. In International Conference on Machine Learning, pages 1391–1399.
PMLR, 2013."
REFERENCES,0.3536754507628294,"Omar Rivasplata, Csaba Szepesvári, John Shawe-Taylor, Emilio Parrado-Hernández, and Shiliang
Sun. PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Samy Ben-
gio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada, pages 9234–9244, 2018. URL https://proceedings.neurips.cc/paper/2018/
hash/386854131f58a556343e056f03626e00-Abstract.html."
REFERENCES,0.3550624133148405,"Matthias Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification.
Journal of Machine Learning Research, 3(Oct):233–269, 2002."
REFERENCES,0.3564493758668516,"Akira Takayama and Takayama Akira. Mathematical economics. Cambridge university press, 1985."
REFERENCES,0.3578363384188627,"Philipp Tschandl. The HAM10000 dataset, a large collection of multi-source dermatoscopic images
of common pigmented skin lesions, 2018. URL https://doi.org/10.7910/DVN/DBW86T."
REFERENCES,0.3592233009708738,"Yi-Shan Wu and Yevgeny Seldin. Split-kl and pac-bayes-split-kl inequalities for ternary random
variables. Advances in Neural Information Processing Systems, 35:11369–11381, 2022."
REFERENCES,0.3606102635228849,"Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=BJgqqsAct7."
REFERENCES,0.361997226074896,"A
Recipe for implementing Theorems 1 and 2"
REFERENCES,0.3633841886269071,"We here outline more explicitly how Theorem 1 and Theorem 2 may be used to formulate a fully
differentiable objective by which a model may be trained."
REFERENCES,0.3647711511789182,"First, if one wishes to make hard labels, namely H ⊆YX , it will first be necessary to use a surrogate
class of soft hypotheses H′ ⊆M(Y)X during training, before reverting to hard labels for example
by taking the mean label or the one with highest probability. Using soft hypotheses during training
is necessary to ensure that the empirical j-risks Rj
S(Q) are differentiable with respect to the model
parameters. Since how one chooses to do this will depend on the specific use case, we restrict our
attention here to the case of soft hypotheses. Specifically, we consider a class of soft hypotheses
H = {hθ : θ ∈RN} ⊆M(Y)X parameterised by the weights θ ∈RN of some neural network
of a given architecture with N parameters in such a way that the Rj
S(hθ) are differentiable in θ. A
concrete example would be multiclass classification using a fully connected neural network with
output being softmax probabilities on the classes so that the Rj
S(hθ) are differentiable."
REFERENCES,0.36615811373092927,"Second, it is necessary to restrict the prior and posterior P, Q ∈M(H) to a parameterised subset
of M(H) in which KL(Q∥P) has a closed form which is differentiable in the parameterisation. A
simple choice for our case of a neural network with N parameters is P, Q ∈{N(w, diag(s)) : w ∈
RN, s ∈RN
>0}. For prior a Pv,r = N(v, diag(r)) and posterior Qw,s = N(w, diag(s)) we have
the closed form"
REFERENCES,0.36754507628294036,"KL(Qw,s∥Pv,r) = 1 2 "" N
X n=1 sn"
REFERENCES,0.36893203883495146,"rn
+ (wn −vn)2"
REFERENCES,0.37031900138696255,"rn
+ ln rn sn 
−N # ,"
REFERENCES,0.37170596393897365,"which is indeed differentiable in v, r, w and s. While Qw,s and Pv,r are technically distributions on
RD rather than H, the KL-divergence between the distributions they induce on H will be at most as
large as the expression above. Thus, substituting the expression above into the bounds we prove in
Section 3 can only increase the value of the bounds, meaning the enlarged bounds certainly still hold
with probability at least 1 −δ."
REFERENCES,0.37309292649098474,"Third, in all but the simplest cases Rj
S(Qw,s) will not have a closed form, much less one that is
differentiable in w and s. A common solution to this is to use the so-called pathwise gradient
estimator. In our case, this corresponds to drawing ϵ ∼N(0, I), where I is the N × N identity
matrix, and estimating"
REFERENCES,0.37447988904299584,"∇w,sRj
S(Qw,s) = ∇w,s
h
Eϵ′∼N(0,I)Rj
S(hw+ϵ′⊙√s)
i
≈∇w,sRj
S(hw+ϵ⊙√s),"
REFERENCES,0.37586685159500693,"where hw denotes the function expressed by the neural network with parameters w. For a proof that
this is an unbiased estimator, and for other methods for estimating the gradients of expectations, see
the survey Mohamed et al. [2020]."
REFERENCES,0.37725381414701803,"Fourth, one must choose the prior. Designing priors which are optimal in some sense (i.e., minimising
the Kullback-Leibler term in the right-hand side of generalisation bounds) has been at the core of an
active line of work in the PAC-Bayesian literature. For the sake of simplicity, and since it is out of the
scope of our contributions, we assume here that the prior is given beforehand, although we stress that
practitioners should pay great attention to its tuning. For our purposes, it suffices to say that if one
is using a data-dependent prior then it is necessary to partition the sample into S = SPrior ∪SBound,
where SPrior is used to train the prior and SBound is used to evaluate the bound. Since our bound holds
uniformly over posteriors Q ∈M(H), the entire sample S is free to be used to train the posterior
Q. For a more in-depth discussion on the choice of prior, we refer to the following body of work:
Ambroladze et al. [2006], Lever et al. [2010, 2013], Parrado-Hernández et al. [2012], Dziugaite and
Roy [2017, 2018], Rivasplata et al. [2018], Letarte et al. [2019], Pérez-Ortiz et al. [2021], Dziugaite
et al. [2021], Biggs and Guedj [2022a,b]."
REFERENCES,0.3786407766990291,"Finally, given a confidence level δ ∈(0, 1], one may use Algorithm 1 to obtain a posterior Qw,s
with minimal upper bound on the total risk. Note we take the pointwise logarithm of the variances
r and s to obtain unbounded parameters on which to perform stochastic gradient descent or some
other minimisation algorithm. We use ⊕to denote vector concatenation. The algorithm can be
straightforwardly adapted to permit mini-batches by, for each epoch, sequentially repeating the steps
with S equal to each mini-batch."
REFERENCES,0.3800277392510402,"Input:
X, Y /* Arbitrary input and output spaces
*/
SM
j=1 Ej = Y2 /* A finite partition into error types
*/
ℓ∈[0, ∞)M /* A vector of losses, not all equal
*/
S = SPrior ∪SBound ∈(X × Y)m /* A partitioned i.i.d.
sample
*/
N ∈N /* The number of model parameters
*/
Pv,r, v(SPrior) ∈RN, r(SPrior) ∈RN
≥0 /* A (data-dependent) prior
*/
Qw0,s0, w0 ∈RN, s0 ∈RN
≥0 /* An initial posterior
*/
δ ∈(0, 1] /* A confidence level
*/
λ > 0 /* A learning rate
*/
T /* The number of epochs to train for
*/"
REFERENCES,0.3814147018030513,"Output:
Qw,s, w ∈RN, s ∈RN
≥0 /* A trained posterior
*/"
REFERENCES,0.3828016643550624,"Procedure:
ζ0 ←log s0 /* Transform to unbounded scale parameters
*/
p ←w0 ⊕ζ0 /* Collect mean and scale parameters
*/
for t ←1 to T do"
REFERENCES,0.3841886269070735,"Draw ϵ ∼N(0, I)"
REFERENCES,0.3855755894590846,"u ←RS

hw+ϵ⊙√"
REFERENCES,0.3869625520110957,exp(ζ)  B ←
M,0.3883495145631068,"1
m"
M,0.3897364771151179,"
KL
 
Qw,exp(ζ)
Pv,r

+ ln

1
δ
√πe1/12m   m"
M,0.391123439667129,"2
 M−1"
M,0.3925104022191401,"2
PM−1
z=0
 M
z

1
(πm)z/2Γ( M−z 2 ) "
M,0.39389736477115117,"˜u ←(u1, . . . , uM, B)
G ←02N×(M+1) /* Initialise gradient matrix
*/
F ←0M+1 /* Initialise gradient vector
*/
for j ←1 to M + 1 do"
M,0.39528432732316227,"Fj ←∂f ∗
ℓ
∂˜uj (˜u) /* Gradients of total loss from Theorem 2
*/
for i ←1 to 2N do"
M,0.39667128987517336,"Gi,j ←∂˜uj"
M,0.39805825242718446,"∂pi (p) /* Gradients of empirical risks and bound
*/
end
end
H ←GF /* Gradients of total loss w.r.t.
parameters
*/
p ←p −λH /* Gradient step
*/
end
w = (p1, . . . , pN)
s = (exp(pN+1), . . . , exp(p2N))
return w, s"
M,0.39944521497919555,Algorithm 1: Calculating a posterior with minimal bound on the total risk.
M,0.40083217753120665,"B
Additional Experimental Details"
M,0.40221914008321774,"For MNIST we map labels {0, 1, 2, 3, 4} to 0 and {5, 6, 7, 8, 9} to 1. For HAM10000 we map the
cancerous or pre-cancerous labels {Melanoma, Basal Cell Carcinoma, Actinic Keratosis}
to 1 and the other labels to 0. In both cases we partition Y2 into E0 = {(0, 0), (1, 1)}, E1 = {(0, 1)}
and E2 = {(1, 0)}, and take ℓ= (0, 1, 3). For HAM10000, E1 and E2 then refer to Type I and Type
II errors, respectively, and ℓreflects the greater severity of false negatives."
M,0.40360610263522884,"Each dataset is split into prior and certification sets SPrior and SBound, respectively. For MNIST, we
use the conventional training set of size 60000 as the prior set, and the conventional test set of size
10000 as the certification set. For HAM10000 we pool the conventional train, validation and test sets
together and then split 50-50 to obtain prior and certification sets each of size 5860. For HAM10000"
M,0.40499306518723993,"we resize the images to (28, 28) and use just the first channel so that the data dimension is the same
for both datasets."
M,0.406380027739251,"We take H to be two-layer MLPs with 784, 100 and 2 units in the input, hidden and output layers,
respectively. As is common in the PAC-Bayes literature, we restrict P to be an isotropic Gaussian
N(v, λI) and Q to be a diagonal Gaussian N(w, diag(s)). Further, as in Dziugaite and Roy [2017],
we restrict λ to be of the form λj = c exp(−j/b) for some j ∈N, taking c = 0.1 and b = 100.
Since, at the end of training, we will then have one prior Pj for each j ∈N, we can choose the j that
minimizes the PAC-Bayes bound provided we take a union over all of them, taking δj =
6δ
π2j2 so that
P"
M,0.4077669902912621,"j δj = 1 and all the bounds hold simultaneously with probability at least 1 −δ. After applying
Algorithm 1 we round λ to a discrete λj, either up or down depending on which gives the smaller
bound."
M,0.4091539528432732,"For both datasets we set the prior mean v to be the parameters of an MLP trained on the prior
set. In both cases we use SGD with learning rate 0.01 to minimise the cross-entropy loss, using
a portion of the prior set as a validation set. For MNIST we train the MLP for 20 epochs to get
an error rate of 14%, for HAM10000 we train the MLP for 5 epochs to get an error rate of 22%.
We then apply Algorithm 1. By combining Proposition 2 (with δ = 0.01 and N = 100000) and
Proposition 3. We obtain RS( ˆQ) = (0.8879, 0.0919, 0.0203) and RT
D(Q) ≤0.2640 for MNIST and
RS( ˆQ) = (0.7860, 0.0146, 0.1995) and RT
D(Q) ≤0.8379 for HAM10000, where both bounds hold
with probability at least 1 −0.05 −0.01 = 0.94."
M,0.4105409153952843,"The full results are shown in Figure 2. Figures 2a, 2c and 2e are the same as Figures 1a, 1b and
1c, and are repeated here for easier comparison with the HAM10000 results. Figure 2b shows that
Algorithm 1 has failed to reduce the bound on the total risk beyond the initialisation of Q to P, with
the small variation being explained by different MC samples being drawn from Q during training
rather than Q changing substantially. Indeed, Figure 2h shows that Q does not appreciably move from
its initialisation at P—KL(Q∥P) remains below 0.1 whereas in the MNNIST experiment, which
has the same number of parameters, exceeds 30. It is therefore unsurprising that Figures 2d and 2f
show negligible change in the empirical error probabilities and the bound on kl(RS(Q)∥RD(Q)),
respectively. The divergence in the results is likely due to the difference in sample size; the certification
set for the MNIST experiment contains 10000 samples, whereas for the HAM10000 dataset there are
only 5000, which, all else equal, makes an increase in KL(Q∥P) twice as expensive."
M,0.4119278779472954,"Recall from Section 6 that while RD(Q) can be effectively constrained to a sub-region of the simple
△M using our Theorem 1, this can also be achieved by unioning M Maurer bounds, one for each
error probability. Table 1 gave the 95% confidence intervals for the volumes of the confidence regions
in which RD(Q) was likely to lie for experiments on MNIST and HAM10000, but neither region
was uniformly smaller, making it unclear which method should be preferred."
M,0.4133148404993065,"Table 2 provides additional data by taking synthetic values for RS(Q) and KL(Q∥P), for different
values of m (the size of the certification set) and M (the number of error types). ‘Individual’ denotes
unioning individual Maurer bounds, ‘Ours’ is our method, ‘Intersection’ is the intersection of the
confidence regions given by the previous two methods (but loosened so that they now both hold
simultaneously with probability at least 0.95), and ‘Morv.’ is the confidence region produced by
Morvant’s bound Morvant et al. [2012]. The 95% confidence intervals for the volumes of all the
regions have been produced by Monte Carlo samples. We see that our confidence region is tighter
than the individual one in 4/9 cases (green), worse in 3/9 cases (red) and ties in 2/9 cases (orange).
Interestingly, union bounding the naive CR and our CR and intersecting often beats both of these
(bold). Morvant’s result is either not applicable or their confidence region is much larger than ours
and essentially takes up the entire simplex, hence the volume estimate of 1.000. The reason their
bound is sometimes inapplicable is because it requires every class to contain at least 8L instances,
where L is the number of labels—in the L = 5, M = 25, m = 100 case this would require each class
to contain at least 5 × 8 = 40 instances which is impossible with m = 100 samples."
M,0.4147018030513176,"(a)
(b)"
M,0.4160887656033287,"(c)
(d)"
M,0.4174757281553398,"(e)
(f)"
M,0.4188626907073509,"(g)
(h)"
M,0.420249653259362,Figure 2: MNIST (first column) and HAM10000 (second column) experiments.
M,0.42163661581137307,"M
m
Vol. Individual
Vol. Ours
Vol. Intersection
Vol. Morv.
100
(0.1195, 0.1196)
(0.1165, 0.1166)
(0.1160, 0.1161)
(1.0, 1.0)
22
300
(0.02920, 0.02926)
(0.03071, 0.03078)
(0.02893, 0.02900)
(1.0, 1.0)
1000
(5.635e-3, 5.664e-3)
(6.475e-3, 6.507e-3)
(5.706e-3, 5.735e-3)
(1.0, 1.0)
100
(0.3190, 0.3192)
(0.1757, 0.1758)
(0.1582, 0.1584)
N/A
52
300
(1.306e-3, 1.320e-3)
(3.672e-4, 3.748e-4)
(2.515e-4, 2.578e-4)
(1.0, 1.0)
1000
(1.090e-08, 1.024-07)
(2.422e-09, 7.225e-08)
(0.000, 3.689e-08)
(1.0, 1.0)
100
(0.9990, 0.9990)
(1.000, 1.000)
(0.9995, 0.9995)
N/A
102
300
(0.3534, 0.3536)
(0.1688, 0.1689)
(0.1306, 0.1307)
N/A
1000
(3.454e-8, 1.5763e-7)
(0.000, 3.688e-8)
(0.000, 3.688e-8)
(1.0, 1.0)"
M,0.42302357836338417,"Table 2: 95% confidence intervals for the volumes of the confidence regions for RD(Q). We set
KL(Q∥P) = 0, δ = 0.05, RS(Q) = (1/M, ..., 1/M) and use 108 Monte Carlo samples."
M,0.42441054091539526,"C
Proofs"
M,0.42579750346740636,"C.1
Proof of Proposition 3"
M,0.42718446601941745,"Write kl(ˆq∥p) as M
X"
M,0.42857142857142855,"j=1
ˆqj ln ˆqj qj
+ M
X"
M,0.42995839112343964,"j=1
ˆqj ln qj"
M,0.43134535367545074,"pj
.
(13)"
M,0.43273231622746183,"The result then follows by bounding the two sums by M
X"
M,0.43411927877947293,"j=1
ˆqj ln ˆqj qj
= M
X"
M,0.435506241331484,"j=1
kl(ˆqj∥qj) −(1 −ˆqj) ln 1 −ˆqj"
M,0.4368932038834951,"1 −qj
≤MB2 − M
X"
M,0.43828016643550627,"j=1
(1 −ˆqj) ln 1 −ˆqj"
M,0.43966712898751736,"1 −qj
(14) and M
X"
M,0.44105409153952846,"j=1
ˆqj ln qj pj
= M
X j=1"
M,0.44244105409153955,"ˆqj
qj
qj ln qj"
M,0.44382801664355065,"pj
≤max
j
ˆqj
qj M
X"
M,0.44521497919556174,"j=1
qj ln qj"
M,0.44660194174757284,"pj
≤B1 max
j
ˆqj
qj
.
(15)"
M,0.44798890429958393,"Putting these together we obtain the bound on kl(ˆq∥p). The limit follows because each qj →ˆqj as
B2 →0."
M,0.44937586685159503,"C.2
Proof of Lemma 2"
M,0.4507628294036061,"Let EM := {e1, . . . , eM}, namely the set of M-dimensional basis vectors. We will denote a typical
element of Em
M by η(m) = (η1, . . . , ηm). For any x(m) = (x1, . . . , xm) ∈△m
M, a straightforward
induction on m yields X"
M,0.4521497919556172,"η(m)∈Em
M m
Y"
M,0.4535367545076283,"i=1
xi · ηi !"
M,0.4549237170596394,"= 1.
(16)"
M,0.4563106796116505,"To see this, for m = 1 we have E1
M = {(e1, ), . . . , (eM, )}, where we have been pedantic in using
1-tuples to maintain consistency with larger values of m. Thus, for any x(1) = (x1, ) ∈△1
M, the left
hand side of equation (16) can be written as M
X"
M,0.4576976421636616,"j=1
x1 · ej = M
X"
M,0.4590846047156727,"j=1
(x1)j = 1."
M,0.4604715672676838,"Now suppose that equation (16) holds for any x(m) ∈△m
M and let x(m+1) = (x1, . . . , xm+1) ∈
△m+1
M
. Then the left hand side of equation (16) can be written as X"
M,0.4618585298196949,"η(m+1)∈Em+1
M m+1
Y"
M,0.463245492371706,"i=1
xi · ηi ! =
X"
M,0.4646324549237171,"η(m)∈Em
M M
X j=1 m
Y"
M,0.46601941747572817,"i=1
xi · ηi !"
M,0.46740638002773927,"(xm+1 · ej) =
X"
M,0.46879334257975036,"η(m)∈Em
M m
Y"
M,0.47018030513176146,"i=1
xi · ηi ! M
X"
M,0.47156726768377255,"j=1
(xm+1 · ej) = 1."
M,0.47295423023578365,"We now show that any x(m) = (x1, . . . , xm) ∈△m
M can be written as a convex combination of the
elements of Em
M in the following way"
M,0.47434119278779474,"x(m) =
X"
M,0.47572815533980584,"η(m)∈Em
M m
Y"
M,0.47711511789181693,"i=1
xi · ηi !"
M,0.478502080443828,"η(m).
(17)"
M,0.4798890429958391,"We have already shown that the weights sum to one, and they are clearly elements of [0, 1], so the
right hand side of equation (17) is indeed a convex combination of the elements of Em
M. We now
show that equation (17) holds, again by induction."
M,0.4812760055478502,"For m = 1 and any x(1) = (x1, ) ∈△1
M, the right hand side of equation (17) can be written as M
X"
M,0.4826629680998613,"j=1
(x1 · ej)(ej, ) = (x1, ) = x."
M,0.4840499306518724,"For the inductive hypothesis, suppose equation (17) holds for some arbitrary m ≥1, and denote
elements of Em+1
M
by η(m) ⊕(e, ) for some η(m) ∈Em
M and e ∈EM, where ⊕denotes vector
concatenation. Then for any x(m+1) = x(m) ⊕(xm+1, ) = (x1, . . . , xm+1) ∈△m+1
M
, the right
hand side of equation (17) can be written as
X"
M,0.4854368932038835,"η(m+1)∈Em+1
M m+1
Y"
M,0.4868238557558946,"i=1
xi · ηi !"
M,0.4882108183079057,"η(m+1) =
X"
M,0.4895977808599168,"η(m)∈Em
M M
X j=1 m
Y"
M,0.4909847434119279,"i=1
xi · ηi !"
M,0.492371705963939,"(xm+1 · ej)η(m) ⊕(ej, ) =
X"
M,0.49375866851595007,"η(m)∈Em
M M
X j=1 m
Y"
M,0.49514563106796117,"i=1
xi · ηi !"
M,0.49653259361997226,"(xm+1 · ej)η(m) ⊕
X"
M,0.49791955617198336,"η(m)∈Em
M M
X j=1 m
Y"
M,0.49930651872399445,"i=1
xi · ηi !"
M,0.5006934812760055,"(xm+1 · ej)(ej, ) = M
X"
M,0.5020804438280166,"j=1
(xm+1 · ej)
X"
M,0.5034674063800277,"η(m)∈Em
M m
Y"
M,0.5048543689320388,"i=1
xi · ηi ! η(m) ⊕
X"
M,0.5062413314840499,"η(m)∈Em
M m
Y"
M,0.507628294036061,"i=1
xi · ηi ! M
X"
M,0.5090152565880721,"j=1
(xm+1 · ej)(ej, )"
M,0.5104022191400832,"= 1 · x(m) ⊕1 · (xm+1, ) = x(m+1),
where in the penultimate equality we have used the inductive hypothesis and (twice) the result of the
previous induction."
M,0.5117891816920943,"We can now prove the statement of the Lemma. Applying Jensen’s inequality to equation (17) with
the convex function f, we have that"
M,0.5131761442441054,"f(x1, . . . , xm) = f  
X"
M,0.5145631067961165,"η(m)∈Em
M m
Y"
M,0.5159500693481276,"i=1
xi · ηi ! η(m)   ≤
X"
M,0.5173370319001387,"η(m)∈Em
M m
Y"
M,0.5187239944521498,"i=1
xi · ηi !"
M,0.5201109570041609,"f

η(m)
."
M,0.521497919556172,Let µ = E[X1] denote the mean of the i.i.d. random vectors Xi. Then the above inequality implies
M,0.5228848821081831,"E[f(X1, . . . , Xm)] ≤
X"
M,0.5242718446601942,"η(m)∈Em
M m
Y"
M,0.5256588072122053,"i=1
µ · ηi !"
M,0.5270457697642164,"f

η(m) =
X"
M,0.5284327323162274,"η(m)∈Em
M m
Y"
M,0.5298196948682385,"i=1
P(X′
i = ηi) !"
M,0.5312066574202496,"f

η(m)"
M,0.5325936199722607,"= E[f(X′
1, . . . , X′
m)]."
M,0.5339805825242718,"C.3
Proof of Lemma 3"
M,0.5353675450762829,"The proof of Lemma 3 itself requires two technical helping lemmas which we now state and prove.
Lemma 4. For any integers n ≥2 and p ≥−1, n−1
X k=1"
M,0.536754507628294,(n −k)p/2 √
M,0.5381414701803051,"k
≤n
p+1 2
Z 1 0"
M,0.5395284327323162,(1 −x)p/2
M,0.5409153952843273,"√x
dx."
M,0.5423023578363384,"Proof. The case of p = −1, namely n−1
X k=1"
P,0.5436893203883495,"1
p"
P,0.5450762829403606,"k(n −k)
≤
Z 1 0"
P,0.5464632454923717,"1
p"
P,0.5478502080443828,"x(1 −x)
dx,"
P,0.5492371705963939,"has already been demonstrated in Maurer [2004]. For p > −1, let"
P,0.550624133148405,"fp(x) := (1 −x)p/2 √x
."
P,0.5520110957004161,"We will show that each fp(·) is monotonically decreasing on (0, 1). Indeed, dfp"
P,0.5533980582524272,"dx (x) = −(1 −x)
p
2 −1(px + 1 −x)"
P,0.5547850208044383,"2x3/2
≤−(1 −x)p/2"
P,0.5561719833564494,"2x3/2
< 0,"
P,0.5575589459084604,"where for the inequalities we have used the fact that p > −1 and x ∈(0, 1). We therefore see that n−1
X k=1"
P,0.5589459084604715,"(n −k)p/2 √ k
= n−1
X k=1"
P,0.5603328710124826,"np/2(1 −k n)p/2 √n
q k
n"
P,0.5617198335644937,"= n
p+1 2 n−1
X k=1"
N,0.5631067961165048,"1
n
(1 −k"
N,0.5644937586685159,"n)p/2
q k
n"
N,0.565880721220527,"= n
p+1 2 n−1
X k=1"
NFP,0.5672676837725381,"1
nfp k n "
NFP,0.5686546463245492,"≤n
p+1 2 n−1
X k=1 Z
k
n k−1"
NFP,0.5700416088765603,"n
fp(x)dx"
NFP,0.5714285714285714,"= n
p+1"
NFP,0.5728155339805825,"2
Z 1−1 n"
NFP,0.5742024965325936,"0
fp(x)dx"
NFP,0.5755894590846047,"≤n
p+1 2
Z 1"
NFP,0.5769764216366158,"0
fp(x)dx."
NFP,0.5783633841886269,"Intuitively, the proof of the above lemma works by bounding the integral below by a Riemann sum.
In the following lemma we actually calculate this integral, yielding a more explicit bound on the sum
in Lemma 4. We found it is easier to calculate a slightly more general integral, where the 1 in the
limit and the integrand is replaced by a positive constant a."
NFP,0.579750346740638,"Lemma 5. For any real number a > 0 and integer n ≥−1,
Z a 0"
NFP,0.5811373092926491,(a −x)n/2
NFP,0.5825242718446602,"√x
dx = √π Γ( n+2"
NFP,0.5839112343966713,"2 )
Γ( n+3"
NFP,0.5852981969486823,"2 )a
n+1 2 ."
NFP,0.5866851595006934,Proof. Define
NFP,0.5880721220527045,"In(a) :=
Z a 0"
NFP,0.5894590846047156,(a −x)n/2
NFP,0.5908460471567267,"√x
dx
and
fn(a) := √π Γ( n+2"
NFP,0.5922330097087378,"2 )
Γ( n+3"
NFP,0.5936199722607489,"2 )a
n+1 2 ."
NFP,0.59500693481276,"We proceed by induction, increasing n by 2 each time. This means we need two base cases. First, for
n = −1, we have"
NFP,0.5963938973647711,"I−1(a) =
Z a 0"
P,0.5977808599167822,"1
p"
P,0.5991678224687933,"x(a −x)
dx =

2 arcsin
rx a a"
P,0.6005547850208044,"0
= π = f−1(a),"
P,0.6019417475728155,since Γ( 1
P,0.6033287101248266,"2) = √π and Γ(1) = 1. Second, for n = 0,"
P,0.6047156726768377,"I0(a) =
Z a 0"
P,0.6061026352288488,"1
√xdx =

2√x
a
0 = 2√a = f0(a),"
P,0.6074895977808599,since Γ( 3
P,0.608876560332871,"2) =
√π"
P,0.6102635228848821,"2 . Now, by the Leibniz integral rule, we have"
P,0.6116504854368932,"d
daIn+2(a) =
Z a 0"
P,0.6130374479889042,"∂
∂a
(a −x)
n+2"
P,0.6144244105409153,"2
√x
dx = n + 2 2 Z a 0"
P,0.6158113730929264,"(a −x)
n
2
√x
dx = n + 2"
P,0.6171983356449375,"2
In(a). Thus"
P,0.6185852981969486,In+2(a) = n + 2 2 Z a
P,0.6199722607489597,"0
In(t)dt + In(0)

= n + 2 2 Z a"
P,0.6213592233009708,"0
In(t)dt,"
P,0.6227461858529819,since In(0) = 0.
P,0.624133148404993,"Now, for the inductive step, suppose In(a) = fn(a) for some n ≥−1. Then, using the previous
calculation, we have"
P,0.6255201109570042,In+2(a) = n + 2 2 Z a
P,0.6269070735090153,"0
fn(t)dt"
P,0.6282940360610264,= n + 2 2 Z a 0
P,0.6296809986130375,√π Γ( n+2
P,0.6310679611650486,"2 )
Γ( n+3"
P,0.6324549237170597,"2 )t
n+1"
DT,0.6338418862690708,2 dt = √π n+2
DT,0.6352288488210819,2 Γ( n+2 2 ) n+3
DT,0.636615811373093,2 Γ( n+3
DT,0.6380027739251041,"2 )a
n+3 2"
DT,0.6393897364771152,= √π Γ( n+2
DT,0.6407766990291263,"2
+ 1)
Γ( n+3"
DT,0.6421636615811374,"2
+ 1)a
n+3 2"
DT,0.6435506241331485,"= √π
Γ

(n+2)+2 2
"
DT,0.6449375866851595,"Γ

(n+2)+3 2
a"
DT,0.6463245492371706,(n+2)+1 2
DT,0.6477115117891817,= fn+2(a).
DT,0.6490984743411928,This completes the proof.
DT,0.6504854368932039,"We are now ready to prove Lemma 3 which, for ease of reference, we restate here. For integers
M ≥1 and m ≥M,
X"
DT,0.651872399445215,"k∈S>0
m,M"
QM,0.6532593619972261,"1
QM
j=1
p"
QM,0.6546463245492372,"kj
≤π
M"
M,0.6560332871012483,"2 m
M−2 2 Γ( M 2 )
."
M,0.6574202496532594,"Proof. (of Lemma 3) We proceed by induction on M. For M = 1, the set Sm,M contains a single
element, namely the one-dimensional vector k = (k1, ) = (m, ). In this case, the left hand side is
1/√m while the right hand side is √π/(√mΓ(1/2)) = 1/√m, since Γ(1/2) = √π."
M,0.6588072122052705,"Now, as the inductive hypothesis, assume the inequality of Lemma 3 holds for some fixed M ≥1
and all m ≥M. Then for all m ≥M + 1, we have X"
M,0.6601941747572816,"k∈S>0
m,M+1"
M,0.6615811373092927,"1
QM+1
j=1
p kj
= m−M
X k1=1 1
√k1 X"
M,0.6629680998613038,"k′∈S>0
m−k1,M"
QM,0.6643550624133149,"1
QM
j=1
q k′
j ≤ m−M
X k1=1 1
√k1 π
M"
QM,0.665742024965326,"2 (m −k1)
M−2 2 Γ( M"
QM,0.6671289875173371,"2 )
(by the inductive hypothesis) =
π
M 2 Γ( M 2 ) m−M
X k1=1"
QM,0.6685159500693482,"(m −k1)
M−2 2
√k1 ≤
π
M 2 Γ( M 2 ) m−1
X k1=1"
QM,0.6699029126213593,"(m −k1)
M−2"
QM,0.6712898751733704,"2
√k1
(enlarging the sum domain) ≤
π
M 2 Γ( M"
QM,0.6726768377253814,"2 )m
M−1 2
Z 1 0"
QM,0.6740638002773925,"(1 −x)
M−2"
QM,0.6754507628294036,"2
√x
dx
(by Lemma 4) =
π
M 2 Γ( M"
QM,0.6768377253814147,"2 )m
M−1"
QM,0.6782246879334258,2 √π Γ( M
QM,0.6796116504854369,"2 )
Γ( M+1"
QM,0.680998613037448,"2
)
(by Lemma 5)"
QM,0.6823855755894591,"= π
M+1"
M,0.6837725381414702,"2 m
M−1 2"
M,0.6851595006934813,"Γ( M+1 2
)
,"
M,0.6865464632454924,as required.
M,0.6879334257975035,"C.4
Proof of Proposition 1"
M,0.6893203883495146,"Proof. The case where qj = 1 or pj = 1 can be dealt with trivially by splitting into the three
following subcases"
M,0.6907073509015257,• qj = pj = 1 =⇒kl(qj∥pj) = kl(q∥p) = 0
M,0.6920943134535368,"• qj = 1, pj ̸= 1 =⇒kl(qj∥pj) = kl(q∥p) = −log pj"
M,0.6934812760055479,"• qj ̸= 1, pj = 1 =⇒kl(qj∥pj) = kl(q∥p) = ∞."
M,0.694868238557559,"For qj ̸= 1 and pj ̸= 1 define the distributions ˜q, ˜p ∈△M by ˜qj = ˜pj = 0 and"
M,0.6962552011095701,"˜qi =
qi
1 −qj
and
˜pi =
pi
1 −pj"
M,0.6976421636615812,for i ̸= j. Then X
M,0.6990291262135923,"i̸=j
qi log qi"
M,0.7004160887656034,"pi
=
X"
M,0.7018030513176144,"i̸=j
(1 −qj)˜qi log (1 −qj)˜qi"
M,0.7031900138696255,(1 −pj)˜pi
M,0.7045769764216366,"= (1 −qj)
X"
M,0.7059639389736477,"i̸=j
˜qi log ˜qi"
M,0.7073509015256588,"˜pi
+ ˜qi log 1 −qj 1 −pj"
M,0.7087378640776699,= (1 −qj)kl(˜q∥˜p) + (1 −qj) log 1 −qj 1 −pj
M,0.710124826629681,≥(1 −qj) log 1 −qj
M,0.7115117891816921,"1 −pj
."
M,0.7128987517337032,"The final inequality holds since kl(˜q∥˜p) ≥0. Further, note that we have equality if and only if ˜q = ˜p,
which, by their definitions, translates to"
M,0.7142857142857143,pi = 1 −pj
M,0.7156726768377254,"1 −qj
qi"
M,0.7170596393897365,for all i ̸= j. If we now add qj log qj
M,0.7184466019417476,"pj to both sides, we obtain"
M,0.7198335644937587,kl(q∥p) ≥(1 −qj) log 1 −qj
M,0.7212205270457698,"1 −pj
+ qj log qj"
M,0.7226074895977809,"pj
= kl(qj∥pj),"
M,0.723994452149792,with the same condition for equality.
M,0.7253814147018031,"The following proposition makes more precise the argument found at the beginning of Section 4
for how Proposition 1 can be used to derive the tightest possible lower and upper bounds on each
Rj
D(Q).
Proposition 5. Suppose that q, p ∈△M are such that kl(q∥p) ≤B, where q is known and p is
unknown. Then, in the absence of any further information, the tightest bound that can be obtained on
each pj is
pj ≤kl−1(qj, B)."
M,0.7267683772538142,"Proof. Suppose pj > kl−1(qj, B). Then, by definition of kl−1, we have that kl(qj∥pj) > B.
By Proposition 1, this would then imply kl(q∥p) > B, contradicting our assumption. Therefore
pj ≤kl−1(qj, B). Now, with the information we have, we cannot rule out that"
M,0.7281553398058253,pi = 1 −pj
M,0.7295423023578363,"1 −qj
qi"
M,0.7309292649098474,"for all i ̸= j and thus, by Proposition 1, that kl(qj∥pj) = kl(q∥p). Further, we cannot rule out that
kl(q∥p) = B. Thus, it is possible that kl(qj∥pj) = B, in which case pj = kl−1(qj, B). We therefore
see that kl−1(qj, B) is the tightest possible upper bound on pj, for each j ∈[M]."
M,0.7323162274618585,"C.5
Proof of Theorem 2"
M,0.7337031900138696,"Before proving the proposition, we first argue that kl−1
ℓ(u|c) given by Definition 1 is well-defined.
First, note that Au := {v ∈△M : kl(u∥v) ≤c} is compact (boundedness is clear and it is closed
because it is the preimage of the closed set [0, c] under the continuous map v 7→kl(u∥v)) and so the
continuous function fℓachieves its supremum on Au. Further, note that Au is a convex subset of
△M (because the map v 7→kl(u∥v) is convex) and fℓis linear, so the supremum of fℓover Au is
achieved and is located on the boundary of Au. This means we can replace the inequality constraint
kl(u∥v) ≤c in Definition 1 with the equality constraint kl(u∥v) = c. Finally, if u ∈△>0
M then Au
is a strictly convex subset of △M (because the map v 7→kl(u∥v) is then strictly convex) and so the
supremum of fℓoccurs at a unique point on the boundary of Au. In other words, if u ∈△>0
M then
kl−1
ℓ(u|c) is defined uniquely."
M,0.7350901525658807,"We now prove Theorem 2. While our proof technique is somewhat analogous to the technique used
in Clerico et al. [2022] to obtain derivatives of the one-dimensional kl-inverse, our theorem directly
yields derivatives on the total risk by (implicitly) employing the envelope theorem (see for example
Takayama and Akira, 1985)."
M,0.7364771151178918,"Proof Outline:
We first derive the expression given for v∗(˜u) = kl−1
ℓ(u|c) given on line (5) of the
theorem using the method of Lagrange multipliers. Since we are working on the simplex, we make
things easier for ourselves by first making the substitution tj = ln vj to make the vj > 0 constraints
unnecessary. The method of Lagrange multipliers yields both the maximum and the minimum (recall
that kl−1
ℓ(u|c) is defined as the location of a maximum) for the two values of the Lagrange multiplier
µ. We show that exactly one of these values lies in the interval µ ∈(−∞, −maxj ℓj) and that this
one corresponds to the maximum. This shows that the value µ∗Theorem 2 instructs us to find indeed
yields v∗(˜u) = kl−1
ℓ(u|c). Finally, we derive the partial derivatives of kl−1
ℓ(u|c) with respect the ˜uj
to obtain the second part of the theorem, namely line (6) by employing the envelope theorem."
M,0.7378640776699029,"Proof. (of Theorem 2) We start by deriving the implicit expression for v∗(˜u) = kl−1
ℓ(u|c) given in
the proposition by solving a transformed version of the optimisation problem given by Definition
1 using the method of Lagrange multipliers. We obtain two solutions to the Lagrangian equations,
which must correspond to the maximum and minimum total risk over the set Au := {v ∈△M :
kl(u∥v) ≤c} because, as argued in the main text (see the discussion after Definition 1), Au is
compact and so the linear total risk fℓ(v) attains its maximum and minimum on Au."
M,0.739251040221914,"By definition of v∗(˜u) = kl−1
ℓ(u|c), we know that kl(v∗(˜u)∥u) ≤c. Since, by assumption,
uj > 0 for all j, we see that v∗(˜u)j > 0 for all j, otherwise we would have kl(v∗(˜u)∥u) = ∞, a
contradiction. Thus v∗(˜u) ∈△>0
M and we are permitted to instead optimise over the unbounded
variable t ∈RM, where tj := ln vj. With this transformation, the constraint v ∈△M can be
replaced simply by P"
M,0.7406380027739251,j etj = 1 and the optimisation problem becomes
M,0.7420249653259362,"Maximise:
F(t) := M
X"
M,0.7434119278779473,"j=1
ℓjetj"
M,0.7447988904299584,"Subject to:
g(t; u, c) := kl(u∥et) −c = 0,"
M,0.7461858529819695,"h(t) := M
X"
M,0.7475728155339806,"j=1
etj −1 = 0,"
M,0.7489597780859917,"where et ∈RM is defined by (et)j := etj. Note that F(t) = fℓ(et). Following the terminology
of mathematical economics, we call the tj the optimisation variables, and the ˜uj (namely the uj
and c) the choice variables. The vector ℓis considered fixed—we neither want to optimise over
it nor differentiate with respect to it—which is why we occasionally suppress it from the notation
henceforth."
M,0.7503467406380028,"For each ˜u, let v∗(˜u) and t∗(˜u) be the solutions to the original and transformed optimisation
problems respectively. Since the map v = et is one-to-one, it is clear that since v∗(˜u) exists uniquely,
so does t∗(˜u), and that they are related by v∗(˜u) = et∗(˜u). We therefore have the identity"
M,0.7517337031900139,fℓ(v∗(˜u)) ≡F(t∗(˜u)).
M,0.753120665742025,"Recalling that f ∗
ℓ(˜u) := fℓ(v∗(˜u)), we see that"
M,0.7545076282940361,"∇˜uf ∗
ℓ(˜u) ≡∇˜uF(t∗(˜u)).
(18)"
M,0.7558945908460472,"the derivatives of fℓ(kl−1
ℓ(u|c)) with respect to u and c are given by ∇˜uF(t∗(˜u))."
M,0.7572815533980582,"Using the method of Lagrange multipliers, there exist real numbers λ∗= λ∗(˜u) and µ∗= µ∗(˜u)
such that (t∗, λ∗, µ∗) is a stationary point (with respect to t, λ and µ) of the Lagrangian function"
M,0.7586685159500693,"L(t, λ, µ; ˜u) := F(t) + λg(t; ˜u) + µh(t)."
M,0.7600554785020804,"Let Ft(·) and ht(·) denote the gradient vectors of F and h respectively, and let gt( · ; ˜u) and g˜u(t; · )
denote the gradient vectors of g with respect to t only and ˜u only, respectively. Simple calculation
yields"
M,0.7614424410540915,"gt(t; ˜u) =
 ∂g"
M,0.7628294036061026,"∂t1
(t; ˜u), . . . , ∂g"
M,0.7642163661581137,"∂tM
(t; ˜u)

= −u
and"
M,0.7656033287101248,"g˜u(t; ˜u) =
 ∂g"
M,0.7669902912621359,"∂˜u1
(t; ˜u), . . . ,
∂g
∂˜uM+1
(t; ˜u)

=

1 −t1 + log u1, . . . , 1 −tM + log uM, −1

. (19)"
M,0.768377253814147,"Then, taking the partial derivatives of L with respect to λ, µ and the tj, we have that (t, λ, µ) =
(t∗(˜u), λ∗(˜u), µ∗(˜u)) solves the simultaneous equations"
M,0.7697642163661581,"Ft(t) + λgt(t; ˜u) + µht(t) = 0,
(20)"
M,0.7711511789181692,"g(t; ˜u) = 0,
and
h(t) = 0,
where the last two equations recover the constraints. Substituting the gradients Ft, gt and ht, the first
equation reduces to
ℓ⊙et −λu + µet = 0,"
M,0.7725381414701803,which implies that for all j ∈[M]
M,0.7739251040221914,"etj =
λuj
µ + ℓj
.
(21)"
M,0.7753120665742025,"Substituting this into the constraints g = h = 0 yields the following simultaneous equations in λ and
µ"
M,0.7766990291262136,"c = kl(u∥et) = M
X"
M,0.7780859916782247,"j=1
uj log uj etj = M
X"
M,0.7794729542302358,"j=1
uj log µ + ℓj"
M,0.7808599167822469,"λ
and
λ M
X j=1"
M,0.782246879334258,"uj
µ + ℓj
= 1."
M,0.7836338418862691,"Substituting the second into the first and rearranging the second, this is equivalent to solving c = M
X"
M,0.7850208044382802,"j=1
uj log "
M,0.7864077669902912,"(µ + ℓj) M
X k=1"
M,0.7877947295423023,"uk
µ + ℓk !"
M,0.7891816920943134,"and
λ =  
M
X j=1"
M,0.7905686546463245,"uj
µ + ℓj   −1"
M,0.7919556171983356,".
(22)"
M,0.7933425797503467,"It has already been established in the discussion after Definition 1 that fℓ(v) attains its maximum
on the set Au := {v ∈△M : kl(u∥v) ≤c}. Therefore F(t) also attains its maximum on RM and
one of the solutions to these simultaneous equations corresponds to this maximum. We first show
that there is a single solution to the first equation in the set (−∞, −maxj ℓj), referred to as µ∗(˜u) in
the proposition. Second, we show that any other solution corresponds to a smaller total risk, so that
µ∗(˜u) corresponds to the maximum total risk and yields v∗(˜u) = kl−1
ℓ(u|c) when µ∗(˜u) and the
associated λ∗(˜u) are substituted into Equation 21."
M,0.7947295423023578,"For the first step, note that since the etj are probabilities, we see from Equation 21 that either
µ + ℓj > 0 for all j (in the case that λ > 0), or µ + ℓj < 0 for all j (in the case that λ < 0).
Thus any solutions µ to the first equation must be in (−∞, −maxj ℓj) or (−minj ℓj, ∞). If
µ ∈(−∞, −maxj ℓj) then the first equation can be written as c = ϕℓ(µ), with ϕℓas defined in the
statement of the proposition. We now show that ϕℓis strictly increasing in µ, and that ϕℓ(µ) →0 as
µ →−∞and ϕℓ(µ) →∞as µ →−maxj ℓj, so that c = ϕℓ(µ) does indeed have a single solution
in the set (−∞, −maxj ℓj). Straightforward differentiation and algebra shows that"
M,0.7961165048543689,"ϕ′
ℓ(µ) = M
X j=1"
M,0.79750346740638,"uj
(µ + ℓj) PM
k=1
uk
µ+ℓk M
X k′=1"
M,0.7988904299583911,"uk′
µ + ℓk′ −(µ + ℓj) M
X k′=1"
M,0.8002773925104022,"uk′
(µ + ℓk′)2 ! ="
M,0.8016643550624133,"PM
j=1
uj
µ+ℓj"
M,0.8030513176144244,"2
−PM
j=1
uj
(µ+ℓj)2
PM
k=1
uk
µ+ℓk
."
M,0.8044382801664355,"Jensen’s inequality demonstrates that the numerator is strictly negative, where strictness is due to
the assumption that the ℓj are not all equal. Further, since the denominator is strictly negative (since
we are dealing with the case where µ ∈(−∞, −maxj ℓj)), we see that ϕℓis strictly increasing for
µ ∈(−∞, −maxj ℓj).5 Turning to the limits, we first show that ϕℓ(µ) →∞as µ →−maxj ℓj."
M,0.8058252427184466,"We now determine the left hand limit. Define J = {j ∈[M] : ℓj = maxk ℓk}, noting that
this is a strict subset of [M] since by assumption the ℓj are not all equal. We then have that for"
M,0.8072122052704577,"5Incidentally, this argument also shows that there is at most one solution to the first equation in (22) in the
range (−minj ℓj, ∞). There indeed exists a unique solution, which corresponds to the minimum total risk, but
we do not prove this."
M,0.8085991678224688,"µ ∈(−∞, maxj ℓj)"
M,0.8099861303744799,"eϕℓ(µ) =  − M
X j=1"
M,0.811373092926491,"uj
µ + ℓj   M
Y k=1"
M,0.812760055478502," 
−(µ + ℓk)
uk
! =  −
X j∈J"
M,0.8141470180305131,"uj
µ + ℓj
−
X j′̸∈J"
M,0.8155339805825242,"uj′
µ + ℓj′  Y k∈J"
M,0.8169209431345353," 
−(µ + ℓk)
uk Y k′̸∈J"
M,0.8183079056865464," 
−(µ + ℓk′)
uk′ ≥  −
X j∈J"
M,0.8196948682385575,"uj
µ + ℓj  Y k∈J"
M,0.8210818307905686," 
−(µ + ℓk)
uk Y k′̸∈J"
M,0.8224687933425797," 
−(µ + ℓk′)
uk′ = P"
M,0.8238557558945908,"j∈J uj
 Q"
M,0.8252427184466019,"k′̸∈J
 
−(µ + ℓk′)
uk′"
M,0.826629680998613," 
−(µ + maxj ℓj)
1−P"
M,0.8280166435506241,"k∈J uk
."
M,0.8294036061026352,"The first term in the numerator is a positive constant, independent of µ. The second term in the
numerator tends to a finite positive limit as µ ↑−maxj ℓj. Since [M] \ J is non-empty, the power
in the denominator is positive and the term in the outer brackets is positive and tends to zero as
µ ↑−maxj ℓj. Thus eϕℓ(µ) →∞as µ ↑−maxj ℓj and, by the continuity of the logarithm, ϕℓ(µ)
as µ ↑−maxj ℓj."
M,0.8307905686546463,"We now determine limµ→−∞ϕℓ(µ) by sandwiching ϕ(µ) between two functions that both tend to
zero as µ →−∞. First, since ℓj ≥0 for all j, for µ ∈(−∞, −maxj ℓj) we have log  − M
X j=1"
M,0.8321775312066574,"uj
µ + ℓj  ≥log  − M
X j=1 uj µ "
M,0.8335644937586685,"= −log(−µ) = − M
X"
M,0.8349514563106796,"j=1
uj log(−µ),"
M,0.8363384188626907,and so
M,0.8377253814147018,"ϕℓ(µ) ≥− M
X"
M,0.8391123439667129,"j=1
uj log(−µ)+ M
X"
M,0.840499306518724,"j=1
uj log
 
−(µ+ℓj)

= M
X"
M,0.841886269070735,"j=1
uj log

1 + ℓj µ"
M,0.8432732316227461,"
→0
as
µ →−∞."
M,0.8446601941747572,"Similarly,
M
X"
M,0.8460471567267683,"j=1
uj log
 
−(µ + ℓj)

≤ M
X"
M,0.8474341192787794,"j=1
uj log(−µ) = log(−µ),"
M,0.8488210818307905,and so
M,0.8502080443828016,"ϕℓ(µ) ≤log  µ M
X j=1"
M,0.8515950069348127,"uj
µ + ℓj "
M,0.8529819694868238,"= log  
M
X j=1"
M,0.8543689320388349,"uj
1 + ℓj µ "
M,0.855755894590846,"→0
as
µ →−∞."
M,0.8571428571428571,"This completes the first step, namely showing that there does indeed exist a unique solution µ∗(˜u) in
the set (−ℓ1, ∞) to the first equation in line (22)."
M,0.8585298196948682,"We now turn to the second step, namely showing that this solution corresponds to the maximum total
risk. Given a value of the Lagrange multiplier µ, substitution into Equation 21 gives"
M,0.8599167822468793,etj(µ) =
M,0.8613037447988904,"uj
µ+ℓj
PM
k=1
uk
µ+ℓk
and therefore total risk"
M,0.8626907073509015,R(µ) =
M,0.8640776699029126,"PM
j=1
ujℓj
µ+ℓj
PM
k=1
uk
µ+ℓk
."
M,0.8654646324549237,"To prove that the solution µ∗(˜u) ∈(−∞, −maxj ℓj) is the solution to the first equation in line (22)
that maximises R, it suffices to show that R(µ) →PM
j=1 ujℓj as |µ| →∞and R′(µ) ≥0 for all
µ ∈(−∞, −maxj ℓj) ∪(−minj ℓj, ∞), so that"
M,0.8668515950069348,"inf
µ∈(−∞,−maxj ℓj) R(µ) ≥
sup
µ∈(−minj ℓj,∞)
R(µ)."
M,0.8682385575589459,"This suffices as we have already proved that µ∗(˜u) is the only solution in (−∞, −maxj ℓj) to the
first equation in line (22), and that no solutions exists in the set [−maxj ℓj, −minj ℓj]."
M,0.869625520110957,"The limit can be easily evaluated by first rewriting R(µ) and then taking the limit as |µ| →∞as
follows"
M,0.871012482662968,R(µ) =
M,0.8723994452149791,"PM
j=1
ujℓj
1+
ℓj"
M,0.8737864077669902,"µ
PM
k=1
uk
1+
ℓk µ →"
M,0.8751733703190014,"PM
j=1 ujℓj
PM
k=1 uk
= M
X"
M,0.8765603328710125,"j=1
ujℓj."
M,0.8779472954230236,"To show that R′(µ) ≥0, let ℓ(j) denote the j’th smallest component of ℓ(breaking ties arbitrarily),
so that ℓ(1) ≤· · · ≤ℓ(M), and use the quotient rule to see that"
M,0.8793342579750347,R′(µ) ≥0 ⇐⇒
M,0.8807212205270458,"PM
k=1
uk
µ+ℓk"
M,0.8821081830790569," PM
j=1
−ujℓj
(µ+ℓj)2

−
PM
j=1
ujℓj
µ+ℓj"
M,0.883495145631068," PM
k=1
−uk
(µ+ℓk)2
"
M,0.8848821081830791,"PM
p=1
up
µ+ℓp 2
≥0 ⇐⇒ M
X j=1 M
X k=1"
M,0.8862690707350902,"ujukℓj
(µ + ℓj)(µ + ℓk)"
M,0.8876560332871013,"
1
µ + ℓk
−
1
µ + ℓj 
≥0 ⇐⇒
X"
M,0.8890429958391124,"j,k∈[M]
k<j"
M,0.8904299583911235,"ujukℓ(j)
(µ + ℓ(j))(µ + ℓ(k))"
M,0.8918169209431346,"
1
µ + ℓ(k)
−
1
µ + ℓ(j)  +
X"
M,0.8932038834951457,"j,k∈[M]
k>j"
M,0.8945908460471568,"ujukℓ(j)
(µ + ℓ(j))(µ + ℓ(k))"
M,0.8959778085991679,"
1
µ + ℓ(k)
−
1
µ + ℓ(j) 
≥0,"
M,0.897364771151179,"where in the final line we have dropped the summands where k = j since they equal zero as the terms
in the bracket cancel. This final inequality holds since the first sum can be bounded below by the
negative of the second sum as follows
X"
M,0.8987517337031901,"j,k∈[M]
k<j"
M,0.9001386962552012,"ujukℓ(j)
(µ + ℓ(j))(µ + ℓ(k))"
M,0.9015256588072122,"
1
µ + ℓ(k)
−
1
µ + ℓ(j)  ≥
X"
M,0.9029126213592233,"j,k∈[M]
k<j"
M,0.9042995839112344,"ujukℓ(k)
(µ + ℓ(j))(µ + ℓ(k))"
M,0.9056865464632455,"
1
µ + ℓ(k)
−
1
µ + ℓ(j)"
M,0.9070735090152566,"
(since ℓ(k) ≤ℓ(j) for k < j) =
X"
M,0.9084604715672677,"j,k∈[M]
k>j"
M,0.9098474341192788,"ukujℓ(j)
(µ + ℓ(k))(µ + ℓ(j))"
M,0.9112343966712899,"
1
µ + ℓ(j)
−
1
µ + ℓ(k)"
M,0.912621359223301,"
(swapping dummy variables j, k)."
M,0.9140083217753121,"We now turn to finding the partial derivatives of F(t∗(˜u)) with respect the ˜uj, which in turn will
allow us to find the partial derivatives of kl−1
ℓ(u|c). Let ∇˜u denote the gradient operator with respect
to ˜u. Then the quantity we are after is ∇˜uF(t∗(˜u)) ∈RM+1, the j’th component of which is"
M,0.9153952843273232," 
∇˜uF(t∗(˜u))
 j = M+1
X k=1"
M,0.9167822468793343,"∂F
∂tk
(t∗(˜u)) ∂t∗
k
∂˜uj
(˜u) = Ft(t∗(˜u)) · ∂t∗"
M,0.9181692094313454,"∂˜uj
(˜u) ∈R."
M,0.9195561719833565,Thus the full gradient vector is
M,0.9209431345353676,"∇˜uF(t∗(˜u)) = Ft(t∗(˜u))∇˜ut∗(˜u),
(23)"
M,0.9223300970873787,where ∇˜ut∗(˜u) is the M × (M + 1) matrix given by
M,0.9237170596393898," 
∇˜ut∗(˜u)
"
M,0.9251040221914009,"j,k = ∂t∗
k
∂˜uj
(˜u)."
M,0.926490984743412,"Finding an expression for this matrix is difficult. Fortunately we can avoid needing to by using a trick
from mathematical economics referred to as the envelope theorem, as we now show."
M,0.9278779472954231,"First, note that since, for all ˜u, the constraints g = h = 0 are satisfied by t∗(˜u), we have the identities"
M,0.9292649098474342,"g(t∗(˜u), ˜u) ≡0
and
h(t∗(˜u)) ≡0."
M,0.9306518723994452,Differentiating these identities with respect to ˜uj then yields
M,0.9320388349514563,"gt(t∗(˜u), ˜u) · ∂t∗"
M,0.9334257975034674,"∂˜uj
(˜u) + g˜uj(t∗(˜u), ˜u) ≡0
and
ht(t∗(˜u)) · ∂t∗"
M,0.9348127600554785,"∂˜uj
(˜u) ≡0."
M,0.9361997226074896,"As before, we can write these M + 1 pairs of equations as the following pair of matrix equations"
M,0.9375866851595007,"gt(t∗(˜u), ˜u)∇˜ut∗(˜u) + g˜u(t∗(˜u), ˜u) ≡0
and
ht(t∗(˜u))∇˜ut∗(˜u) ≡0."
M,0.9389736477115118,"Multiplying these identities by λ∗(˜u) and µ∗(˜u) respectively, and combining with equation (23),
yields"
M,0.9403606102635229,"∇˜uF(t∗(˜u)) =

Ft(t∗(˜u)) + λ∗(˜u)gt(t∗(˜u), ˜u) + µ∗(˜u)ht(t∗(˜u))

∇˜ut∗(˜u)"
M,0.941747572815534,"+ λ∗(˜u)g˜u(t∗(˜u), ˜u)
= λ∗(˜u)g˜u(t∗(˜u), ˜u),"
M,0.9431345353675451,"where the final equality comes from noting that the terms in the large bracket vanish due to equation
(20). Recalling the expression for g˜u(t; ˜u) given by Equation 19 and that v∗(˜u) = exp(t∗(˜u)) we
obtain"
M,0.9445214979195562,"∇˜uF(t∗(˜u)) = λ∗(˜u)

1 −t∗(˜u)1 + log u1, . . . , 1 −t∗(˜u)M + log uM, −1
"
M,0.9459084604715673,"= λ∗(˜u)

1 + log
u1
v∗(˜u)1
, . . . , 1 + log
uM
v∗(˜u)M
, −1
"
M,0.9472954230235784,"Finally, recalling Equivalence (18), namely ∇˜uf ∗
ℓ(˜u) ≡∇˜uF(t∗(˜u)), we see that the above
expression gives the derivatives ∂f ∗
ℓ
∂uj (˜u) and ∂f ∗
ℓ
∂c (˜u) stated in the proposition, thus completing the
proof."
M,0.9486823855755895,NeurIPS Paper Checklist
CLAIMS,0.9500693481276006,1. Claims
CLAIMS,0.9514563106796117,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9528432732316228,Answer: [Yes]
CLAIMS,0.9542302357836339,"Justification: Our Theorem 1 is a PAC-Bayes bound of the form we claim to prove in the
abstract. Further, Proposition 2 is a recipe for the differentiable training objective we claim
to derive in the abstract."
LIMITATIONS,0.955617198335645,2. Limitations
LIMITATIONS,0.957004160887656,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9583911234396671,Answer: [Yes]
LIMITATIONS,0.9597780859916782,Justification: We discuss these in Section 7.
THEORY ASSUMPTIONS AND PROOFS,0.9611650485436893,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9625520110957004,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9639389736477115,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9653259361997226,"Justification: The proof of our main result Theorem 1 is found in Section 5. Proposition 1 is
proved in Appendix C.4. Our second main result, Theorem 2 is proved in Appendix C.5.
Proposition 2 is proved directly after its statement. Proposition 3 is proved in Appendix C.1.
Proposition 4 is proved in Section 5. Lemma 1 does not require proof as it is the classic
change of measure inequality (Csiszár, 1975, Donsker and Varadhan, 1975). Lemma 2 is
proved in Appendix C.2. Finally, Corollary 1 and Lemma 3 are proved directly after their
statement."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9667128987517337,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9680998613037448,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9694868238557559,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.970873786407767,"Justification: Appendix A is devoted to a detailed explanation of how to implement our
training regime, and Appendix B gives the details of the specific experiments we run."
OPEN ACCESS TO DATA AND CODE,0.9722607489597781,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9736477115117892,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9750346740638003,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9764216366158114,Justification: URL is provided.
OPEN ACCESS TO DATA AND CODE,0.9778085991678225,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9791955617198336,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9805825242718447,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9819694868238558,Justification: This is outlined in Appendix B.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9833564493758669,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.984743411927878,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.986130374479889,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9875173370319001,"Justification: The paper concerns bounds, meaning the results themselves are confidence
regions."
EXPERIMENTS COMPUTE RESOURCES,0.9889042995839112,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9902912621359223,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: The compute resources required are not stated as they are negligible.
9. Code Of Ethics"
EXPERIMENTS COMPUTE RESOURCES,0.9916782246879334,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: There are no data-related concerns or societal impact concerns.
10. Broader Impacts"
EXPERIMENTS COMPUTE RESOURCES,0.9930651872399445,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The introduction describes a class of concrete real-world problems for which
our method may have positive impact.
11. Safeguards"
EXPERIMENTS COMPUTE RESOURCES,0.9944521497919556,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
12. Licenses for existing assets"
EXPERIMENTS COMPUTE RESOURCES,0.9958391123439667,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper uses only the MNIST dataset.
13. New Assets"
EXPERIMENTS COMPUTE RESOURCES,0.9972260748959778,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENTS COMPUTE RESOURCES,0.9986130374479889,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: None required.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: None required."
