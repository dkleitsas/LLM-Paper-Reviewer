Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004048582995951417,"Many problems in machine learning rely on multi-task learning (MTL), in which
the goal is to solve multiple related machine learning tasks simultaneously. MTL is
particularly relevant for privacy-sensitive applications in areas such as healthcare,
ﬁnance, and IoT computing, where sensitive data from multiple, varied sources are
shared for the purpose of learning. In this work, we formalize notions of task-level
privacy for MTL via joint differential privacy (JDP), a relaxation of differential
privacy for mechanism design and distributed optimization. We then propose an
algorithm for mean-regularized MTL, an objective commonly used for applications
in personalized federated learning, subject to JDP. We analyze our objective and
solver, providing certiﬁable guarantees on both privacy and utility. Empirically,
we ﬁnd that our method allows for improved privacy/utility trade-offs relative to
global baselines across common federated learning benchmarks."
INTRODUCTION,0.008097165991902834,"1
INTRODUCTION"
INTRODUCTION,0.012145748987854251,"Multi-task learning (MTL) aims to solve multiple learning tasks simultaneously while exploiting
similarities/differences across tasks (Caruana, 1997). Multi-task learning is commonly used in
applications that warrant strong privacy guarantees. For example, MTL has been used in healthcare,
as a way to learn over diverse populations or between multiple institutions (Baytas et al., 2016;
Suresh et al., 2018; Harutyunyan et al., 2019); in ﬁnancial forecasting, to combine knowledge from
multiple indicators or across organizations (Ghosn & Bengio, 1997; Cheng et al., 2020); and in IoT
computing, as an approach for learning in federated networks of heterogeneous devices (Smith et al.,
2017; Hanzely & Richt´arik, 2020; Hanzely et al., 2020; Ghosh et al., 2020; Sattler et al., 2020; Deng
et al., 2020; Mansour et al., 2020). While MTL can signiﬁcantly improve accuracy when learning in
these applications, there is a dearth of work studying the privacy implications of multi-task learning."
INTRODUCTION,0.016194331983805668,"In this work, we develop and theoretically analyze methods for MTL with formal privacy guarantees.
Motivated by applications in federated learning, we aim to provide task-level privacy1, where each
task corresponds to a client/device/data silo, and the goal is to protect the sensitive information in each
task’s data (McMahan et al., 2018). We focus on incorporating differential privacy (DP) (Dwork et al.,
2006), which (informally) requires an algorithm’s output to be insensitive to the change of any single
entity’s data. For MTL, using task-level DP directly would require the entire set of predictive models
across all tasks to be insensitive to changes in the private data of any single task. This requirement is
too stringent for most applications, as it implies that the predictive model for task k must have little
dependence on the training data for task k, thus preventing the usefulness of the model (see Figure 1)."
INTRODUCTION,0.020242914979757085,"To circumvent this limitation, we leverage a meaningful relaxation of DP known as joint differential
privacy (JDP) (Kearns et al., 2014), which requires that for each task k, the set of output predictive
models for all other tasks except k is insensitive to k’s private data. As a consequence, the client’s
private data in task k is protected even if all other clients/tasks collude and share their private data and
output models (as long as client k keeps their data private). In contrast to standard DP, JDP allows
the predictive model for task k to depend on k’s private data, helping to preserve the task’s utility."
INTRODUCTION,0.024291497975708502,"Using JDP, we then develop new learning algorithms for MTL with rigorous privacy and utility
guarantees. Speciﬁcally, we propose Private Mean-Regularized MTL, a simple framework for learning"
INTRODUCTION,0.02834008097165992,"1In federated learning applications, where each MTL task typically corresponds to a client’s local training
task, this can be equivalently viewed as ‘client-level’ or ‘user-level’ privacy (McMahan et al., 2018)."
INTRODUCTION,0.032388663967611336,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03643724696356275,Algorithm D1
INTRODUCTION,0.04048582995951417,"w1
w2
wm D2
Dm"
INTRODUCTION,0.044534412955465584,Algorithm D′ 1
INTRODUCTION,0.048582995951417005,"w′ 1 ≈w1
w′ 2 ≈w2
w′ m ≈wm D2
Dm"
INTRODUCTION,0.05263157894736842,Algorithm D′ 1
INTRODUCTION,0.05668016194331984,"w′ 1 ≉w1
w′ 2 ≈w2
w′ m ≈wm D2
Dm"
INTRODUCTION,0.06072874493927125,"Figure 1: An MTL problem consists of m different tasks and a learning algorithm that jointly produces one
model for each task (Left). For example, in cross-device federated learning, each ‘task’ may represent data
from a mobile phone client (as depicted), and MTL can be used to learn shared, yet personalized models for
each client (Smith et al., 2017). In traditional differential privacy, if the private data of task k (e.g., Task 1)
changes, the models produced by the MTL algorithm should be indistinguishable from the models derived
without changing data from task k (Middle). In contrast, JDP allows the model of task k to be dependent on task
k’s data while still protecting other tasks from leaking information about their private data (Right)."
INTRODUCTION,0.06477732793522267,"multiple tasks while ensuring task-level privacy. We show that our method achieves (ϵ, δ)-JDP. Our
scalable solver builds on FedAvg (McMahan et al., 2017), a common method for communication-
efﬁcient federated optimization. We analyze the convergence of our solver on both nonconvex and
convex objectives, demonstrating a tradeoff between privacy and utility, and evaluate this trade-off
empirically on multiple federated learning benchmarks. We summarize our contributions below:"
INTRODUCTION,0.06882591093117409,"• Our work is the ﬁrst we are aware of to provide formal deﬁnitions of task-level differential
privacy for multi-task learning objectives (Section 3). Our deﬁnitions rely on joint differential
privacy and are applicable to commonly-used multi-task relationship learning objectives.
• Using our privacy deﬁnitions, we propose Private Mean-Regularized MTL, a simple MTL frame-
work that provides task-level privacy (Section 4). We prove that our method achieves (ϵ, δ)-JDP,
and we analyze the convergence of our communication-efﬁcient solver on convex and nonconvex
objectives. Our convergence analysis extends to non-private settings with partial participation,
which may be of independent interest for problems in cross-device federated learning.
• Finally, we explore the performance of our approach on common federated learning benchmarks
(Section 5). Our results show that it is possible to retain the accuracy beneﬁts of MTL in these
settings relative to global baselines while still providing meaningful privacy guarantees. Further,
even in cases where the MTL objective achieves similar accuracy to the global objective, we ﬁnd
that privacy/utility beneﬁts exist when employing the private MTL formulation."
BACKGROUND AND RELATED WORK,0.0728744939271255,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.07692307692307693,"Multi-task learning. Multi-task learning considers jointly solving multiple related ML tasks. Our
work focuses on the general and widely-used formulation of multi-task relationship learning (Zhang &
Yeung, 2010), as discussed in Section 3. This form of MTL is particularly useful in privacy-sensitive
applications where datasets are shared among multiple heterogeneous entities (Baytas et al., 2016;
Smith et al., 2017; Ghosn & Bengio, 1997). In these cases, it is natural to view each data source
(e.g., ﬁnancial institution, hospital, mobile phone) as a separate ‘task’ that is learned in unison with
the other tasks. This allows data to be shared, but the models to be personalized to each data silo.
For example, in the setting of cross-device federated learning, MTL is commonly used to train a
personalized model for each device in a distributed network (Smith et al., 2017; Liu et al., 2017)."
BACKGROUND AND RELATED WORK,0.08097165991902834,"Federated learning. A motivation for our work is the application of federated learning (FL), in
which the goal is to collaboratively learn from data that has been generated by, and resides on, a
number of private data silos, such as remote devices or servers (McMahan et al., 2017; Kairouz et al.,
2019; Li et al., 2020a). To ensure client-level differential privacy in FL, a common technique is to
learn one global model across the distributed data and then add noise to the aggregated model to
sufﬁciently mask any speciﬁc client’s update (Kairouz et al., 2019; McMahan et al., 2018; Geyer
et al., 2017). However, a deﬁning characteristic of federated learning is that the distributed data are
likely to be heterogeneous, i.e., each client may generate data via a distinct data distribution (Kairouz
et al., 2019; Li et al., 2020a). To model the (possibly) varying data distributions on each client, it is
natural to instead consider learning a separate model for each client’s local dataset."
BACKGROUND AND RELATED WORK,0.08502024291497975,"To this end, a number of recent works have explored multi-task learning as a way to improve the
accuracy of learning in federated networks (Smith et al., 2017; Hanzely & Richt´arik, 2020; Hanzely"
BACKGROUND AND RELATED WORK,0.08906882591093117,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.0931174089068826,"et al., 2020; Ghosh et al., 2020; Sattler et al., 2020; Deng et al., 2020; Mansour et al., 2020). Despite
the prevalence of multi-task federated learning, we are unaware of any work that has explored
task-level privacy for commonly-used multi-task relationship models (Section 3) in federated settings."
BACKGROUND AND RELATED WORK,0.09716599190283401,"Differentially private MTL. Prior work in private MTL differs from our own either in terms of the
privacy formulation or MTL objective. For example, Wu et al. (2020) explore a speciﬁc MTL setting
where a feature representation shared by all tasks is ﬁrst learned, followed by task-speciﬁc models on
top of this private representation. We instead study multi-task relationship learning (Section 3), which
is a general and widely-used MTL framework, particularly in federated learning (Smith et al., 2017).
While our work focuses on task-level privacy, there has been work on data-level privacy for MTL,
which aims to protect any single piece of local data rather than protecting the entire local dataset. For
example, Xie et al. (2017) propose a method for data-level privacy by representing the model for
each task as a sum of a public, shared weight and a task-speciﬁc weight that is only updated locally,
and Gupta et al. (2016) study data-level privacy for a mean estimation MTL problem. Finally, Li
et al. (2019) studies multiple notions of differential privacy for meta-learning. Although similarly
motivated by personalization, their framework does not cover the multi-task setting, where there
exists a separate model for each task."
MULTI-TASK LEARNING AND PRIVACY FORMULATION,0.10121457489878542,"3
MULTI-TASK LEARNING AND PRIVACY FORMULATION"
MULTI-TASK LEARNING AND PRIVACY FORMULATION,0.10526315789473684,"In this section, we ﬁrst formalize our multi-task learning objective, which is a form of mean-
regularized multi-task learning (Section 3.1), and then provide our privacy formulation (Section 3.2)."
PROBLEM SETUP,0.10931174089068826,"3.1
PROBLEM SETUP"
PROBLEM SETUP,0.11336032388663968,"In the classical setting of multi-task relationship learning (Zhang & Yang, 2017; Zhang & Yeung,
2010), there are m different task learners with their own task-speciﬁc data. The aim is to solve:"
PROBLEM SETUP,0.11740890688259109,"min
W,Ω ("
PROBLEM SETUP,0.1214574898785425,"F(W, Ω) = (
1
m m
X k=1 nk
X"
PROBLEM SETUP,0.12550607287449392,"i=1
lk(xi, wk) + R(W, Ω) )) ,
(1)"
PROBLEM SETUP,0.12955465587044535,"where wk is model for task learner k; {x1, . . . , xnk} is the local data for the kth task; lk(·) is the
empirical loss for task k; W = [w1; · · · ; wm]; and Ω∈Rm×m characterizes the relationship between
every pair of task learners. A common choice for setting the regularization term R(W, Ω) in previous
works (Zhang & Yeung, 2010; Smith et al., 2017) is:"
PROBLEM SETUP,0.13360323886639677,"R(W, Ω) = λ1tr(WΩW T ) ,"
PROBLEM SETUP,0.13765182186234817,"where Ωcan be viewed as a covariance matrix, used to learn/encode positive, negative, or unrelated
task relationships (Zhang & Yeung, 2010). In this paper, we focus on studying the mean-regularized
multi-task learning objective (Evgeniou & Pontil, 2004): a special case of (1) where Ω= (Im×m −
1
m1m1T
m)2 is ﬁxed. Here Im×m is the identity matrix of size m × m and 1m ∈Rm is the vector
with all entries equal to 1. By picking λ1 = λ"
PROBLEM SETUP,0.1417004048582996,"2 , we can rewrite the objective as: min
W ("
PROBLEM SETUP,0.145748987854251,"F(W) = (
1
m m
X k=1"
PROBLEM SETUP,0.14979757085020243,"λ
2 ∥wk −¯w∥2 + nk
X"
PROBLEM SETUP,0.15384615384615385,"i=1
lk(xi, wk) )) ,
(2)"
PROBLEM SETUP,0.15789473684210525,where ¯w is the average of task-speciﬁc models: ¯w = 1
PROBLEM SETUP,0.16194331983805668,"m
Pm
i=1 wk. Note that ¯w is shared across all
tasks, and each wk is kept locally for task learner k. During optimization, each task learner k solves:"
PROBLEM SETUP,0.1659919028340081,"min
wk"
PROBLEM SETUP,0.1700404858299595,"
fk(wk; ¯w) = λ"
PROBLEM SETUP,0.17408906882591094,"2 ∥wk −¯w∥2 + nk
X"
PROBLEM SETUP,0.17813765182186234,"i=1
lk(xi, wk)

.
(3)"
PROBLEM SETUP,0.18218623481781376,"Despite the prevalence of this simple form of multi-task learning and its recent use in applications
such as federated learning with strong privacy motivations (e.g., Hanzely & Richt´arik, 2020; Hanzely
et al., 2020; Dinh et al., 2020), we are unaware of prior work that has formalized task-level differential
privacy in the context of solving Objective (2)."
PROBLEM SETUP,0.1862348178137652,Under review as a conference paper at ICLR 2022
PRIVACY FORMULATION,0.1902834008097166,"3.2
PRIVACY FORMULATION"
PRIVACY FORMULATION,0.19433198380566802,"We start by introducing the deﬁnition of differential privacy (DP) before discussing its generalization
to joint differential privacy (JDP). In the context of multi-task learning, each of the m task learners
owns a private dataset Di ∈Ui ⊂U. We deﬁne D = {D1, · · · , Dm} and D′ = {D′
1, · · · , D′
m}.
We call two sets D, D′ neighboring sets if they only differ on the index i, i.e., Dj = D′
j for all j
except i. With this setup in mind, we deﬁne differential privacy more formally below.
Deﬁnition 1 (Differential Privacy (DP) for MTL (Dwork et al., 2006)). A randomized algorithm
M : Um →Rm is (ϵ, δ)-differentially private if for every pair of neighboring sets that only differ in
arbitrary index i: D, D′ ∈U and for every set of subsets of outputs S ⊂R,"
PRIVACY FORMULATION,0.19838056680161945,"Pr(M(D) ∈S) ≤eϵPr(M(D′) ∈S) + δ.
(4)
In the context of MTL, a learning algorithm outputs one model for every task learner. As mentioned
previously, since the output of MTL is a collection of models, traditional DP would require that all
the models produced by an MTL learning algorithm are insensitive to changes that happen to the
private dataset of any single task."
PRIVACY FORMULATION,0.20242914979757085,"In this work we are interested in studying task-level privacy, where the purpose is to protect one
task learner’s data from leakage to any other task learners. In this setting, DP incurs an additional
restriction that the model of any task learner should also be insensitive to changes in its own data,
which would render each of the models useless. To overcome this limitation of DP, we suggest
employing joint differential privacy (JDP) (Kearns et al., 2014), a relaxed notion of DP, to formalize
the guarantee that an MTL algorithm should provide in order to protect task-level privacy. Intuitively,
JDP requires that for each task k, the set of output predictive models for all other tasks except k is
insensitive to k’s private data. We provide a formal deﬁnition below.
Deﬁnition 2 (Joint Differential Privacy (JDP) (Kearns et al., 2014)). A randomized algorithm
M : Um →Rm is (ϵ, δ)-joint differentially private if for every i, for every pair of neighboring
datasets that only differ in index i: D, D′ ∈Um and for every set of subsets of outputs S ⊂Rm,"
PRIVACY FORMULATION,0.20647773279352227,"Pr(M(D)−i ∈S) ≤eϵPr(M(D′)−i ∈S) + δ,
(5)"
PRIVACY FORMULATION,0.21052631578947367,where M(D)−i represents the vector M(D) with the i-th entry removed.
PRIVACY FORMULATION,0.2145748987854251,"JDP allows the predictive model for task k to depend on the private data of k, while still providing a
strong guarantee: even if all the clients from all the other tasks collude and share their information,
they still will not be able to learn much about the private data in the task k. JDP has mostly been used
in applications related to mechanism design (Hsu et al., 2016a; Kannan et al., 2015; Hsu et al., 2016b;
Cummings et al., 2015; Rogers & Roth, 2014; Kearns et al., 2014). Although it is a natural choice for
achieving task-level privacy in MTL, we are unaware of any work that studies MTL subject to JDP."
PRIVACY FORMULATION,0.21862348178137653,"We also note that we can naturally connect joint differential privacy to standard differential privacy.
Informally, if we take the output of a differentially private process and run some algorithm on top of
that locally for each task learner without communicating to the global learner or other task learners,
this whole process can be shown to be joint differentially private. This is formalized as the Billboard
Lemma (Kearns et al., 2014), presented in Lemma 1 below.
Lemma 1 (Billboard Lemma). Suppose M : D →W is (ϵ, δ)-differentially private. Consider any
set of functions: fi : Di ×W →W′. The composition {fi(ΠiD, M(D))} is (ϵ, δ)-joint differentially
private, where Πi : D →Di is the projection of D onto Di."
PRIVACY FORMULATION,0.22267206477732793,"With the Billboard Lemma, we are able to obtain joint differential privacy by ﬁrst training a differen-
tially private model with data from all tasks, and then ﬁnetuning on each task with its local data. We
formally introduce our algorithm and corresponding JDP guarantee by using Lemma 1 in Section 4."
PRIVACY FORMULATION,0.22672064777327935,"Finally, note that our privacy formulation itself is not limited to the multi-task relationship learning
framework. For any form of multi-task learning where each task-speciﬁc model is obtained by training
a combination of global component and local component(e.g. Li et al. (2021)), we can provide a JDP
guarantee for the MTL training process by using a differentially private global component."
PRIVACY FORMULATION,0.23076923076923078,"4
PMTL: PRIVATE MULTI-TASK LEARNING"
PRIVACY FORMULATION,0.23481781376518218,"We now present PMTL, a method for performing joint differentially-private MTL (Section 4.1). We
provide both a privacy guarantee (Section 4.2) and utility guarantee (Section 4.3) for our approach."
PRIVACY FORMULATION,0.2388663967611336,Under review as a conference paper at ICLR 2022
PRIVACY FORMULATION,0.242914979757085,Algorithm 1 PMTL: Private Mean-Regularized MTL
PRIVACY FORMULATION,0.24696356275303644,"1: Input: m, T, λ, η, {w0
1, · · · , w0
m}, ew0 = 1"
PRIVACY FORMULATION,0.25101214574898784,"m
Pm
k=1 w0
k
2: for t = 0, · · · , T −1 do
3:
Global Learner randomly selects a set of tasks St and broadcasts the mean weight ewt"
PRIVACY FORMULATION,0.2550607287449393,"4:
for k ∈St in parallel do
5:
Each task updates its weight wk for E iterations, ok is the last iteration task k is selected
wt+1
k
= ClientUpdate(wok
k )"
PRIVACY FORMULATION,0.2591093117408907,"6:
Each task sends gt+1
k
= wt+1
k
−wt
k back to the global learner.
7:
end for
8:
Global Learner computes a noisy aggregator of the weights"
PRIVACY FORMULATION,0.2631578947368421,"ewt+1 = ewt +
1
|St| X"
PRIVACY FORMULATION,0.26720647773279355,"k∈St
gt+1
k
min

1,
γ
∥gt+1
k
∥2"
PRIVACY FORMULATION,0.27125506072874495,"
+ N(0, σ2Id×d)"
PRIVACY FORMULATION,0.27530364372469635,"9: end for
10: return w1, · · · , wm as differentially private personalized models"
PRIVACY FORMULATION,0.2793522267206478,"11: ClientUpdate(w)
12: for j = 0, · · · , E −1 do
13:
Task learner performs SGD locally
w = w −η(∇wlk(w) + λ(w −ewt))
14: end for"
ALGORITHM,0.2834008097165992,"4.1
ALGORITHM"
ALGORITHM,0.2874493927125506,"We summarize our solver for private multi-task learning in Algorithm 1. Our method is based off of
FedAvg (McMahan et al., 2017), a communication-efﬁcient method widely used in federated learning.
FedAvg alternates between two steps: (i) each task learner selected at one communication round
solves its own local objective by running stochastic gradient descent for E iterations and sending the
updated model to the global learner; (ii) the global learner aggregates the local updates and broadcasts
the aggregated mean. By performing local updating in this manner, FedAvg has been shown to
empirically reduce the total number of communication rounds needed for convergence in federated
settings relative to baselines such as mini-batch FedSGD (McMahan et al., 2017). Our private MTL
algorithm differs from FedAvg in that: (i) instead of learning a single global model, all task learners
collaboratively learn separate, personalized models for each task; (ii) each task learner solves the
local objective with the mean-regularization term; (iii) individual model updates are clipped and
random Gaussian noise is added to the aggregated model updates to ensure task-level privacy."
ALGORITHM,0.291497975708502,"To aggregate updates from each task, we assume that we have access to a trusted global learner, i.e.,
it is safe for some global entity to observe/collect the individual model updates from each task. This
is a standard assumption in federated learning, where access to a trusted central server is assumed in
order to collect client updates (Kairouz et al., 2019). However, even with this assumption, note that it
is possible for any single task learner to infer information about other tasks from the global model,
since it is a linear combination of all task speciﬁc models and is shared among all task learners."
ALGORITHM,0.29554655870445345,"There are several ways to overcome this privacy risk and thus achieve (ϵ, δ)-differential privacy. In
this paper, we use the Gaussian Mechanism (Dwork & Roth, 2014) during global aggregation as a
simple yet effective method, highlighted in the red portion of line 8 in Algorithm 1. In this case, each
task learner receives a noisy aggregated global model, making it difﬁcult for any task to leak private
information to the others. To apply the Gaussian mechanism, we need to bound the ℓ2-sensitivity of
each local model update that is communicated to lie in B = {∆w|∥∆w∥2 ≤γ}, as highlighted in the
blue part of line 8 in Algorithm 1. Hence, at each communication round, the global learner receives
the model updates from each task, and clips the model updates to B before aggregation. Note that
different from DPSGD (Abadi et al., 2016), when we solve the local objective for each selected task
at each communication round, our algorithm doesn’t clip and perturb the gradient used to update the
task-speciﬁc model. Instead, since the purpose is to protect task or client-level privacy in multi-task
learning, we perform standard SGD locally for each task and only clip and perturb the model update
that is sent to the global learner. We formalize the privacy guarantee of Algorithm 1 in Section 4.2."
ALGORITHM,0.29959514170040485,Under review as a conference paper at ICLR 2022
PRIVACY ANALYSIS,0.30364372469635625,"4.2
PRIVACY ANALYSIS
We now rigorously explore the privacy guarantee provided by Algorithm 1. In our optimization
scheme, for each task k, at the end of each communication round, a shared global model is received.
After that the task speciﬁc model is updated by optimizing the local objective. We formalize this local
task learning process as hk : Dk × W →W. Here we simply assume W ⊂Rd is closed. Deﬁne the
mechanism for communication round t to be"
PRIVACY ANALYSIS,0.3076923076923077,"Mt({Di}, {hi(·)}, ewt, σ) = ewt +
1
|St| X"
PRIVACY ANALYSIS,0.3117408906882591,"k∈St
hk(Dk, ewt) + βt,
(6)"
PRIVACY ANALYSIS,0.3157894736842105,"where βt ∼N(0, σ2Id×d). Note that Mt characterizes a Sampled Gaussian Mechanism given ewt
as a ﬁxed model rather than the output of a composition of Mj for j < t. To analyze the privacy
guarantee of Algorithm 1 over T communication rounds, we deﬁne the composition of M1 to MT
recursively as M1:T = MT ({Di}, {hi(·)}, MT −1, σ).
Theorem 1. Assume |St| = q for all t and the total number of communication rounds is T. There
exists constants c1, c2 such that for any ϵ < c1
q2"
PRIVACY ANALYSIS,0.31983805668016196,"m2 T, the mechanism M1:T is (ϵ, δ)-differentially"
PRIVACY ANALYSIS,0.32388663967611336,"private for any δ > 0 if we choose σ ≥c2
γ√"
PRIVACY ANALYSIS,0.32793522267206476,T log(1/δ)
PRIVACY ANALYSIS,0.3319838056680162,"ϵm
. When q = m, M1:T is (ϵ, δ)-differentially"
PRIVACY ANALYSIS,0.3360323886639676,"private if we choose σ =
4γ√"
PRIVACY ANALYSIS,0.340080971659919,"T log(1/δ) ϵm
."
PRIVACY ANALYSIS,0.3441295546558704,"Theorem 1 provides a provable privacy guarantee on the learned global model. When all tasks
participate in every communication round, i.e. q = m, the global aggregation step in Algorithm 1
reduce to applying Gaussian Mechanism without sampling rather than Sampled Gaussian Mechanism
on the average model updates. We provide a detailed proof of Theorem 1 in Appendix A.1."
PRIVACY ANALYSIS,0.3481781376518219,"Note that Theorem 1 doesn’t rely on how task learners optimize their local objective. Hence, Theorem
1 is not limited to Algorithm 1 and could be generalized to other local objectives and other global
aggregation methods that produce a single model aggregate."
PRIVACY ANALYSIS,0.3522267206477733,"Now we show that Algorithm 1, which outputs m separate models, satisﬁes joint differential privacy.
Given ewt for any t ≤T, we formally deﬁne the process that each task learner k optimize its local
objective to be h′
k : Dk × W →W. Note that h′
k is not restricted to be hk and could represent the
optimization process for any local objective. In order to show that Algorithm 1 satisﬁes JDP, we
would apply the Billboard Lemma introduced in Section 3. In our case, the average model that is
broadcast by the global learner at every communication round is the output of a differentially private
learning process. Task learners then individually train their task speciﬁc models on the respective
private data to obtain personalized models. We now present our main theorem of the JDP guarantee
provided by Algorithm 1:"
PRIVACY ANALYSIS,0.3562753036437247,"Theorem 2. There exists constants c1, c2, for any 0 < ϵ < c1
q2"
PRIVACY ANALYSIS,0.3603238866396761,"m2 T and δ > 0, let σ ≥
c2γ√"
PRIVACY ANALYSIS,0.3643724696356275,T log(1/δ)
PRIVACY ANALYSIS,0.3684210526315789,"ϵm
.
Algorithm 1 that outputs h′
k(Dk, M1:T ) for each task is (ϵ, δ)-joint differentially private."
PRIVACY ANALYSIS,0.3724696356275304,"From Theorem 2, for any ﬁxed δ, the more tasks involved in the learning process, the smaller σ we
need in order to keep the privacy parameter ϵ the same. In other words, less noise is required to keep
the task-speciﬁc data private. When we have inﬁnitely many tasks (m →∞), we have σ →0, in
which case only a negligible amount of noise is needed to add to the model aggregates to make the
global model private to all tasks. We provide a detailed proof in Appendix A.1."
PRIVACY ANALYSIS,0.3765182186234818,"Remark. Note that privacy guarantee provided by our Theorem 2 is not limited to mean-regularized
multi-task learning. For any form of multi-task relationship learning with ﬁxed relationship matrix Ω,
as long as we ﬁx the ℓ2-sensitivity of model updates and the noise scale of the Gaussian mechanism
applied to the statistics broadcast to all task learners, the privacy guarantee induced by this aggregation
step is ﬁxed, regardless of the local objective being optimized. For example, as a natural extension of
our mean-regularized MTL objective, consider the case where task learners are partitioned into ﬁxed
clusters and optimize the mean-regularized MTL objective within each cluster, as in Evgeniou et al.
(2005). In this scenario, Theorem 2 directly applies to the algorithm run on each cluster."
CONVERGENCE ANALYSIS,0.3805668016194332,"4.3
CONVERGENCE ANALYSIS
As discussed in Section 3, we are interested in the following task-speciﬁc objective:"
CONVERGENCE ANALYSIS,0.38461538461538464,fk(wk; ew) = lk(wk) + λ
CONVERGENCE ANALYSIS,0.38866396761133604,"2 ∥wk −ew∥2
2
(7)"
CONVERGENCE ANALYSIS,0.39271255060728744,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.3967611336032389,where ew is an estimate for the average model w; lk(wk) is the empirical loss for task k; wk ∈Rd.
CONVERGENCE ANALYSIS,0.4008097165991903,"Here, we analyze the convergence behavior in the setting where a set St of q tasks participate in
the optimization process at every communication round. Further, we assume the number of local
optimization steps E = 1. We present the following convergence result:
Theorem 3 (Convergence under nonconvex loss). Let fk be (L+λ)-smooth. Assume γ is sufﬁciently
large such that γ ≥maxk,t ∥∇wt
kfk(wt
k; ewt)∥2. Further let f ∗
k = minw, ¯
w fk(w; ¯w) and p = q"
CONVERGENCE ANALYSIS,0.4048582995951417,"m. If
we use a ﬁxed learning rate ηt = η =
1
pL+(p−1"
CONVERGENCE ANALYSIS,0.4089068825910931,"p)λ, Algorithm 1 satisﬁes:"
MT,0.41295546558704455,"1
mT"
MT,0.41700404858299595,"T −1
X t=0 m
X"
MT,0.42105263157894735,"k=1
∥∇fk(wt
k; ewt)∥2 ≤O
 1 mT"
MT,0.4251012145748988,"
+
O

L + λ + λ"
MT,0.4291497975708502,"p2
 PT −1
t=0 Bt+1"
MT,0.4331983805668016,"T
+ O
 
dσ2
.
(8) where"
MT,0.43724696356275305,"Bt = max
k
fk(wt
k; ewt).
(9)"
MT,0.44129554655870445,"Let σ chosen as we set in Theorem 2. Take T = O

m
λdγ2

, the right hand side is bounded by λdγ2 m2"
MT,0.44534412955465585,"T −1
X t=0 m
X"
MT,0.4493927125506073,"k=1
∥∇fk(wt
k; ewt)∥2 ≤O
dγ2 m2"
MT,0.4534412955465587,"
+ O
 
dγ2 T −1
X"
MT,0.4574898785425101,"t=0
Bt+1 + O
 1 m"
MT,0.46153846153846156, log(1/δ)
MT,0.46558704453441296,"ϵ2
.
(10)"
MT,0.46963562753036436,"We provide a formal statement and full proof of Theorem 3 in Appendix A.2. The upper bound in
(30) consists of two parts: error induced by the gradient descent algorithm and error induced by the
Gaussian Mechanism. When σ = 0, Algorithm 1 recovers a non-private mean-regularized multi-task
learning solver.
Corollary 4. When σ = 0, Algorithm 1 with (L + λ)-smooth and nonconvex fk satisﬁes"
MT,0.47368421052631576,"1
mT"
MT,0.4777327935222672,"T −1
X t=0 m
X"
MT,0.4817813765182186,"k=1
∥∇fk(wt
k; ewt)∥2 ≤O
 1 mT"
MT,0.48582995951417,"
+
O

L + λ + λ"
MT,0.4898785425101215,"p2
 PT −1
t=0 Bt+1"
MT,0.4939271255060729,"T
.
(11)"
MT,0.4979757085020243,"By Theorem 2, given ﬁxed ϵ, σ2 grows linearly with respect to T. Hence, given the same privacy
guarantee, larger noise is required if the algorithm is run for more communication rounds. Note that in
Theorem 3, the upper bound consists of O(
1
mϵ2 ), which means when there are more tasks, the upper
bound becomes smaller while the privacy parameter remains the same. On the other hand, Theorem
3 also shows a privacy-utility tradeoff using our Algorithm 1: the upper bound grows inversely
proportional to the privacy parameter ϵ. We also provide a convergence analysis of Algorithm 1 with
strongly-convex losses in Theorem 5 below (formal statement and proof in Appendix A.3).
Theorem 5 (Convergence under strongly-convex loss). Let fk be (L + λ)-smooth and (µ + λ)-
strongly convex. Assume γ is sufﬁciently large such that γ ≥maxk,t ∥∇wt
kfk(wt
k; ewt)∥2. Further
let w∗
k = arg minw fk(w; ¯w∗), where ¯w∗= 1"
MT,0.5020242914979757,"m
Pm
k=1 w∗
k and p = q"
MT,0.5060728744939271,"m. If we use a ﬁxed learning rate
ηt = η =
c
L−2"
MT,0.5101214574898786,"p
+λp for some constant c such that 0 ≤ηp(c −2)(µ + λ) ≤1, Algorithm 1 satisﬁes:"
MT,0.5141700404858299,"1
m∆T ≤(1 −ηp(c −2)(µ + λ))T
 1"
MT,0.5182186234817814,"m∆0 −C

+ C,
(12)"
MT,0.5222672064777328,"where
∆t
=
Pm
k=1 fk(wt
k; ewt) −fk(w∗
k; ew∗),
C
=
O √ dσ+√"
MT,0.5263157894736842,"2
λ B
2 η ! ,
B
="
MT,0.5303643724696356,"maxt maxk fk(wt
k; ewt)."
MT,0.5344129554655871,"Let σ be chosen as in Theorem 2, then there exists T = O

m(c2−2c)(µ+λ)"
MT,0.5384615384615384,"λ

L−2"
MT,0.5425101214574899,"p2 +λ

dγ2"
MT,0.5465587044534413,"
such that"
MT,0.5506072874493927,"1
m∆T ≤(1 −ηp(c −2)(µ + λ))T
 1"
MT,0.5546558704453441,m∆0 −log(1/δ)
MT,0.5587044534412956,"mϵ2
−O
B η"
MT,0.562753036437247,"
+ log(1/δ)"
MT,0.5668016194331984,"mϵ2
+ O
B η 
. (13)"
MT,0.5708502024291497,Under review as a conference paper at ICLR 2022
MT,0.5748987854251012,"As with Corollary 4, we recover the bound of the non-private mean-regularized MTL solver for σ=0.
Corollary 6. When σ = 0, Algorithm 1 with (L+λ)-smooth and (µ+λ)-strongly convex fk satisﬁes"
MT,0.5789473684210527,"1
m∆T ≤(1 −ηp(c −2)(µ + λ))T
 1"
MT,0.582995951417004,"m∆0 −
B
ηp(c −2)(µ + λ)"
MT,0.5870445344129555,"
+
B
ηp(c −2)(µ + λ).
(14)"
EXPERIMENTS,0.5910931174089069,"5
EXPERIMENTS"
EXPERIMENTS,0.5951417004048583,"In this section, we empirically evaluate our private MTL solver on several federated learning bench-
marks. Speciﬁcally, we demonstrate the privacy-utility trade-off of training a MTL objective compared
with training a global model. We then compare results of performing local ﬁnetuning after learning
an MTL objective with local ﬁnetuning after learning a global objective."
SETUP,0.5991902834008097,"5.1
SETUP"
SETUP,0.6032388663967612,"For all experiments, we evaluate the test accuracy and privacy parameter of our private MTL solver
given a ﬁxed clipping bound γ, variance of Gaussian noise σ2, and communication rounds T. All
experiments are performed on common federated learning benchmarks as a natural application of
multi-task learning. We provide a detailed description of datasets and models in Appendix A.4. Each
dataset is naturally partitioned among m different clients. Under such a scenario, each client can be
viewed as a task and the data that a client generates is only visible to the local task learner."
SETUP,0.6072874493927125,"0
200
400
600
Communication rounds 5.0 5.5 6.0"
SETUP,0.611336032388664,Losses 0 5 10 15
SETUP,0.6153846153846154,Privacy parameter( )
SETUP,0.6194331983805668,"=0.01, =0.5"
SETUP,0.6234817813765182,(a) StackOverﬂow tag prediction
SETUP,0.6275303643724697,"0
200
400
600
800
1000
Communication rounds 0 1 2 3 4"
SETUP,0.631578947368421,Losses 0.0 2.5 5.0 7.5 10.0 12.5
SETUP,0.6356275303643725,Privacy parameter( )
SETUP,0.6396761133603239,"=0.01, =0.2"
SETUP,0.6437246963562753,(b) FEMNIST
SETUP,0.6477732793522267,"0
200
400
600
800
1000
Communication rounds 0 1 2 3 4"
SETUP,0.6518218623481782,Losses 0 5 10 15
SETUP,0.6558704453441295,Privacy parameter( )
SETUP,0.659919028340081,"=0.02, =1"
SETUP,0.6639676113360324,(c) CelebA
SETUP,0.6680161943319838,"Figure 2: Loss and privacy parameter vs. communication rounds for PMTL. The blue line shows the change of
privacy parameter ϵ in terms of number of communication rounds during training. The orange line shows the
average training loss across all tasks."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6720647773279352,"5.2
PRIVACY-UTILITY TRADE-OFF OF PMTL"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6761133603238867,"We ﬁrst explore the training loss (orange) and privacy parameter ϵ (blue) as a function of communi-
cation rounds across three datasets (Figure 2). Speciﬁcally, we evaluate the average loss for all the
tasks and ϵ given a ﬁxed δ after each round, where δ is set to be 1"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.680161943319838,"m for all experiments. In general, for
a ﬁxed clipping bound γ and σ, we see that the method converges fairly quickly with respect to the
resulting privacy, but that privacy guarantees may be sacriﬁced in order to achieve very small losses."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6842105263157895,"To put these results in context, we also compare the test performance of our private MTL solver with
that of training a global model. In particular, we use FedAvg (McMahan et al., 2017) to train a global
model. At each communication round, task learners solve their local objective individually. Assuming
the global learner is trustworthy, while aggregating the model updates from all tasks, the global learner
applies a Gaussian Mechanism and sends the noisy aggregation back to the task learners. As a result,
private FedAvg differs from our private MTL solver in the following two places: (i) the MTL objective
solved locally by each task learner has an additional mean-regularized term; (ii) the MTL method
evaluates on one task-speciﬁc model for every task while the global method evaluates all tasks on one
global model. For each dataset, we select privacy parameter ϵ ∈[0.05, 0.1, 0.2, 0.4, 0.8, 1.6, 2.0, 4.0].
For each ϵ, we select the γ, σ, and T that result in the best validation accuracy for a given ϵ and
record the test accuracy. A detailed description of hyperparameters is listed in Appendix A.5. We plot
the test accuracy with respect to the highest validation accuracy given one ϵ for both private MTL
model and private global model. The results are shown in Figure 3."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6882591093117408,"In all three datasets, our private MTL solver achieves higher test accuracy compared with training a
private global model with FedAvg given the same ϵ. Moreover, the proposed mean regularized MTL
solver is able to retain an advantage over global model even with noisy aggregation. In particular, for
small ϵ < 1, adding random Gaussian noise during global aggregation ampliﬁes the test accuracy"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6923076923076923,Under review as a conference paper at ICLR 2022
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.6963562753036437,"0
1
2
3
4
0.0 0.1 0.2 0.3"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7004048582995951,Test accuracy
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7044534412955465,"MTL non-private
Global non-private
MTL
Global"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.708502024291498,(a) StackOverﬂow tag prediction
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7125506072874493,"0
1
2
3
4
0.0 0.2 0.4 0.6 0.8"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7165991902834008,Test accuracy
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7206477732793523,(b) FEMNIST
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7246963562753036,"0
1
2
3
4 0.5 0.6 0.7 0.8 0.9"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.728744939271255,Test accuracy
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7327935222672065,"(c) CelebA
Figure 3: Comparison of PMTL and training a private global model."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7368421052631579,"difference between our MTL solver and FedAvg. Under the StackOverﬂow task, both methods obtain
test accuracy close to the non private baseline for large ϵ. To demonstrate that applying private MTL
has an advantage over private global training more generally, we also compared our PMTL method
with private FedProx (Li et al., 2020b). The results (which mirror Figure 3) are in Appendix A.6."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7408906882591093,"Table 1: Comparison of private MTL and private Global model with different local ﬁnetuning methods.
ϵ = ∞corresponds to the case where no noise and clipping happened, i.e., training non-privately.
The higher accuracy between MTL and Global given the same ϵ and ﬁnetuning method is bolded."
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7449392712550608,"FEMNIST
ϵ = 0.1
ϵ = 0.8
ϵ = 2.0
ϵ = ∞"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7489878542510121,"MTL
Global
MTL
Global
MTL
Global
MTL
Global"
PRIVACY-UTILITY TRADE-OFF OF PMTL,0.7530364372469636,"Vanilla Finetuning
0.645 ± 0.013
0.606 ± 0.017
0.640 ± 0.016
0.648 ± 0.017
0.677 ± 0.008
0.653 ± 0.010
0.832 ± 0.005
0.812 ± 0.009
Mean-regularization
0.608 ± 0.011
0.581 ± 0.011
0.605 ± 0.008
0.574 ± 0.006
0.656 ± 0.009
0.633 ± 0.003
0.826 ± 0.011
0.839 ± 0.006
Symmetrized KL
0.486 ± 0.012
0.348 ± 0.005
0.584 ± 0.012
0.481 ± 0.016
0.662 ± 0.016
0.565 ± 0.019
0.839 ± 0.006
0.829 ± 0.015
EWC
0.663 ± 0.002
0.556 ± 0.001
0.595 ± 0.004
0.607 ± 0.007
0.681 ± 0.002
0.666 ± 0.001
0.837 ± 0.001
0.823 ± 0.005"
PMTL WITH LOCAL FINETUNING,0.757085020242915,"5.3
PMTL WITH LOCAL FINETUNING"
PMTL WITH LOCAL FINETUNING,0.7611336032388664,"Finally, in federated learning, previous works have shown local ﬁnetuning with different objectives is
helpful for improving utility while training a differentially private global model (Yu et al., 2020). In
this section, after obtaining a private global model, we explore locally ﬁnetuning the task speciﬁc
models by optimizing different local objective functions. In particular, we use common objectives
which (i) naively optimize the local empirical risk (Vanilla Finetuning), or (ii) encourage minimizing
the distance between local and global model under different distance metrics (Mean-regularization,
Symmetrized KL, EWC (Kirkpatrick et al., 2017; Yu et al., 2020)). The results are listed in Table 1.
When ϵ = ∞(the non-private setting), global with mean-regularization ﬁnetuning outperforms all
MTL+ﬁnetuning methods. However, when we add privacy to both methods, private MTL+ﬁnetuning
has an advantage over global with ﬁnetuning on different ﬁnetuning objectives. In some cases, e.g.
using Symmetrized KL as the ﬁnetuning objective, the test accuracy gap between private MTL with
ﬁntuning and private global with ﬁnetuning is ampliﬁed when ϵ is small compared to the case where
no privacy is added during training."
CONCLUSION AND FUTURE WORK,0.7651821862348178,"6
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.7692307692307693,"In this work, we deﬁne notions of task-level privacy for multi-task learning and propose a simple
method for differentially private mean-regularized MTL. Theoretically, we provide both privacy
and utility guarantees for our approach. Empirically, we show that private mean-regularized MTL
retains advantages over training a private global on common federated learning benchmarks. In future
work, we are interested in building on our results to explore privacy for more general forms of MTL,
e.g., the family of objectives in (1) with arbitrary matrix Ω. We are also interested in studying how
task-level privacy relates to algorithmic fairness in the MTL setting."
CONCLUSION AND FUTURE WORK,0.7732793522267206,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.7773279352226721,"Ethics Statement.
In this work we aim to raise awareness about privacy issues that potentially exist
when performing multi-task learning, which is commonly used to model heterogeneous data in areas
such as healthcare, ﬁnance, and IoT computing. We are particularly motivated by the application of
cross-device federated learning, where numerous forms of multi-task learning have been proposed to
improve accuracy, but the privacy implications remain unclear. To ensure common notions of task or
client-level privacy in this setting while retaining the demonstrated accuracy beneﬁts of MTL, we
have provided formal privacy deﬁnitions for multi-task learning. Building on these deﬁnitions, we
proposed a practical algorithm and studied its performance both theoretically and empirically for the
special case of mean-regularized multi-task learning objective. We hope this work will be a starting
point for future extensions that study the issue of privacy in multi-task learning."
REPRODUCIBILITY STATEMENT,0.7813765182186235,"Reproducibility Statement.
To enable reproducibility in our theoretical and empirical results, we
provide:"
REPRODUCIBILITY STATEMENT,0.7854251012145749,"• Proofs of Theorem 1, 2, 3, 5 in Appendix A.1, A.2, A.3."
REPRODUCIBILITY STATEMENT,0.7894736842105263,• Dataset and model details in Appendix A.4.
REPRODUCIBILITY STATEMENT,0.7935222672064778,• Hyperparameters for tuning in Appendix A.5.
REPRODUCIBILITY STATEMENT,0.7975708502024291,• Code used for running experiments in Supplementary Material.
REFERENCES,0.8016194331983806,REFERENCES
REFERENCES,0.805668016194332,"Tensorﬂow federated: Machine learning on decentralized data. URL https://www.tensorflow.org/
federated."
REFERENCES,0.8097165991902834,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In ACM SIGSAC Conference on Computer and Communications
Security, 2016."
REFERENCES,0.8137651821862348,"Inci M Baytas, Ming Yan, Anil K Jain, and Jiayu Zhou. Asynchronous multi-task learning. In International
Conference on Data Mining (ICDM), 2016."
REFERENCES,0.8178137651821862,"Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar.
Leaf: A benchmark for federated settings, https://leaf.cmu.edu/. arXiv preprint arXiv:1812.01097,
2018."
REFERENCES,0.8218623481781376,"Rich Caruana. Multitask learning. Machine Learning, 28:41–75, 1997."
REFERENCES,0.8259109311740891,"Yong Cheng, Yang Liu, Tianjian Chen, and Qiang Yang. Federated learning for privacy-preserving ai. Commu-
nications of the ACM, 63(12), 2020."
REFERENCES,0.8299595141700404,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten
letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921–2926, 2017."
REFERENCES,0.8340080971659919,"Rachel Cummings, Michael J. Kearns, Aaron Roth, and Zhiwei Steven Wu. Privacy and truthful equilibrium
selection for aggregative games. In Web and Internet Economics Conference (WINE), 2015."
REFERENCES,0.8380566801619433,"Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning.
arXiv preprint arXiv:2003.13461, 2020."
REFERENCES,0.8421052631578947,"Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau envelopes.
In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.8461538461538461,"Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in
Theoretical Computer Science, 9(3-4):211–407, 2014."
REFERENCES,0.8502024291497976,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Conference on Theory of Cryptography (TCC), 2006."
REFERENCES,0.854251012145749,"Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Conference on Knowledge
Discovery and Data Mining, 2004."
REFERENCES,0.8582995951417004,"Theodoros Evgeniou, Charles A Micchelli, Massimiliano Pontil, and John Shawe-Taylor. Learning multiple
tasks with kernel methods. Journal of Machine Learning Research, 6(4), 2005."
REFERENCES,0.8623481781376519,Under review as a conference paper at ICLR 2022
REFERENCES,0.8663967611336032,"Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective.
arXiv preprint arXiv:1712.07557, 2017."
REFERENCES,0.8704453441295547,"Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efﬁcient framework for clustered
federated learning. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.8744939271255061,"Joumana Ghosn and Yoshua Bengio. Multi-task learning for stock selection. In Advances in Neural Information
Processing Systems, 1997."
REFERENCES,0.8785425101214575,"Sunil Kumar Gupta, Santu Rana, and Svetha Venkatesh. Differentially private multi-task learning. In Paciﬁc-Asia
Workshop on Intelligence and Security Informatics, pp. 101–113. Springer, 2016."
REFERENCES,0.8825910931174089,"Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models. arXiv preprint
arXiv:2002.05516, 2020."
REFERENCES,0.8866396761133604,"Filip Hanzely, Slavom´ır Hanzely, Samuel Horv´ath, and Peter Richtarik. Lower bounds and optimal algorithms
for personalized federated learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.8906882591093117,"Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning
and benchmarking with clinical time series data. Scientiﬁc data, 6(1):1–18, 2019."
REFERENCES,0.8947368421052632,"Justin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, and Zhiwei Steven Wu. Private matchings and
allocations. SIAM Journal of Computing, 45(6), 2016a."
REFERENCES,0.8987854251012146,"Justin Hsu, Zhiyi Huang, Aaron Roth, and Zhiwei Steven Wu. Jointly private convex programming. In
Symposium on Discrete Algorithms, SODA, 2016b."
REFERENCES,0.902834008097166,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.9068825910931174,"Sampath Kannan, Jamie Morgenstern, Aaron Roth, and Zhiwei Steven Wu. Approximately stable, school
optimal, and student-truthful many-to-one matchings (via differential privacy). In Symposium on Discrete
Algorithms, SODA, 2015."
REFERENCES,0.9109311740890689,"Michael J. Kearns, Mallesh M. Pai, Aaron Roth, and Jonathan R. Ullman. Mechanism design in large games:
incentives and privacy. In Moni Naor (ed.), Innovations in Theoretical Computer Science, ITCS’14, Princeton,
NJ, USA, January 12-14, 2014, pp. 403–410. ACM, 2014."
REFERENCES,0.9149797570850202,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017."
REFERENCES,0.9190283400809717,"Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private meta-learning. In
International Conference on Learning Representations, 2019."
REFERENCES,0.9230769230769231,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and
future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a."
REFERENCES,0.9271255060728745,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In Conference on Machine Learning and Systems, 2020b."
REFERENCES,0.9311740890688259,"Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through
personalization. In International Conference on Machine Learning, pp. 6357–6368. PMLR, 2021."
REFERENCES,0.9352226720647774,"Sulin Liu, Sinno Jialin Pan, and Qirong Ho. Distributed multi-task relationship learning. In International
Conference on Knowledge Discovery and Data Mining (ICDM), pp. 937–946, 2017."
REFERENCES,0.9392712550607287,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision, 2015."
REFERENCES,0.9433198380566802,"Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization
with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020."
REFERENCES,0.9473684210526315,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics, pp.
1273–1282. PMLR, 2017."
REFERENCES,0.951417004048583,"H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent
language models. In International Conference on Learning Representations, 2018."
REFERENCES,0.9554655870445344,Under review as a conference paper at ICLR 2022
REFERENCES,0.9595141700404858,"Ilya Mironov. R´enyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium (CSF),
pp. 263–275. IEEE, 2017."
REFERENCES,0.9635627530364372,"Ryan M. Rogers and Aaron Roth. Asymptotically truthful equilibrium selection in large congestion games. In
ACM Conference on Economics and Computation, 2014."
REFERENCES,0.9676113360323887,"Felix Sattler, Klaus-Robert M¨uller, and Wojciech Samek.
Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and
Learning Systems, 2020."
REFERENCES,0.97165991902834,"Virginia Smith, Chaokai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In
Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.9757085020242915,"Harini Suresh, Jen J Gong, and John V Guttag. Learning tasks for multitask learning: Heterogenous patient
populations in the icu. In International Conference on Knowledge Discovery & Data Mining, 2018."
REFERENCES,0.979757085020243,"Huiwen Wu, Cen Chen, and Li Wang. A Theoretical Perspective on Differentially Private Federated Multi-task
Learning. arXiv e-prints, art. arXiv:2011.07179, November 2020."
REFERENCES,0.9838056680161943,"Liyang Xie, Inci M Baytas, Kaixiang Lin, and Jiayu Zhou. Privacy-preserving distributed multi-task learning
with asynchronous updates. In International Conference on Knowledge Discovery and Data Mining, 2017."
REFERENCES,0.9878542510121457,"Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation. arXiv
preprint arXiv:2002.04758, 2020."
REFERENCES,0.9919028340080972,"Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017."
REFERENCES,0.9959514170040485,"Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task learning. In
Conference on Uncertainty in Artiﬁcial Intelligence, 2010."
