Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021598272138228943,"Predict and optimize is an increasingly popular decision-making paradigm that em-
1"
ABSTRACT,0.004319654427645789,"ploys machine learning to predict unknown parameters of optimization problems.
2"
ABSTRACT,0.0064794816414686825,"Instead of minimizing the prediction error of the parameters, it trains predictive
3"
ABSTRACT,0.008639308855291577,"models using task performance as a loss function. In the convex optimization
4"
ABSTRACT,0.01079913606911447,"domain, predict and optimize has seen significant progress due to recently devel-
5"
ABSTRACT,0.012958963282937365,"oped methods for differentiating optimization problem solutions over the problem
6"
ABSTRACT,0.01511879049676026,"parameters. This paper identifies a yet unnoticed drawback of this approach – the
7"
ABSTRACT,0.017278617710583154,"zero-gradient problem – and introduces a method to solve it. The suggested method
8"
ABSTRACT,0.019438444924406047,"is based on the mathematical properties of differential optimization and is verified
9"
ABSTRACT,0.02159827213822894,"using two real-world benchmarks.
10"
INTRODUCTION,0.023758099352051837,"1
Introduction
11"
INTRODUCTION,0.02591792656587473,"Mathematical programming is one of the fundamental tools of applied mathematics. It is utilized in
12"
INTRODUCTION,0.028077753779697623,"various domains, such as finance [Cornuejols and Tütüncü, 2006], power systems [Bansal, 2005],
13"
INTRODUCTION,0.03023758099352052,"robotics [Raja and Pugazhenthi, 2012], and many others. The main practical limitation of mathemati-
14"
INTRODUCTION,0.032397408207343416,"cal programming is that it requires a fully-defined model describing the problem which is not always
15"
INTRODUCTION,0.03455723542116631,"available in reality. A promising approach to overcome this limitation is to employ machine learning
16"
INTRODUCTION,0.0367170626349892,"(ML) to predict missing parts of the model [Ning and You, 2019].
17"
INTRODUCTION,0.038876889848812095,"Predict and optimize (P&O) [Elmachtoub and Grigas, 2017] is a decision-making paradigm that
18"
INTRODUCTION,0.04103671706263499,"combines ML with mathematical programming. It considers optimization problems where some
19"
INTRODUCTION,0.04319654427645788,"parameters are unknown and should be predicted prior to solving the problem. The P&O approach
20"
INTRODUCTION,0.04535637149028078,"builds upon the observation that naively training an ML algorithm to match the distribution of
21"
INTRODUCTION,0.047516198704103674,"unknown parameters is inefficient [Elmachtoub and Grigas, 2017], as this approach does not take the
22"
INTRODUCTION,0.04967602591792657,"actual task performance into account. Instead, P&O aims at using task performance as the objective
23"
INTRODUCTION,0.05183585313174946,"function for ML models directly.
24"
INTRODUCTION,0.05399568034557235,"The standard approach to training models in machine learning is to use gradient-based algorithms,
25"
INTRODUCTION,0.056155507559395246,"such as stochastic gradient descent Kiefer and Wolfowitz [1952]. In predict and optimize, computing
26"
INTRODUCTION,0.058315334773218146,"the gradient of the task performance involves differentiating the solution of the optimization problem
27"
INTRODUCTION,0.06047516198704104,"with respect to the parameters, which is a non-trivial task. In their seminal work, Agrawal et al.
28"
INTRODUCTION,0.06263498920086392,"[2019a] have shown that a large class of convex optimization problems indeed can be differentiated.
29"
INTRODUCTION,0.06479481641468683,"In this paper, we identify a fundamental drawback of differential optimization – the zero-gradient
30"
INTRODUCTION,0.06695464362850972,"problem. Specifically, we show that the Jacobian of convex problems often has a large null space,
31"
INTRODUCTION,0.06911447084233262,"and hence the task performance, as a function of the ML model parameters, is flat in a significant part
32"
INTRODUCTION,0.07127429805615551,"of its domain. Therefore, it can not be optimized using gradient-based methods. Consequently, we
33"
INTRODUCTION,0.0734341252699784,"introduce a way to compute an approximate gradient that is zero only in the optimal solution and is
34"
INTRODUCTION,0.0755939524838013,"guaranteed to not decrease performance. Finally, we validate the superiority of this method using two
35"
INTRODUCTION,0.07775377969762419,"real-world problems: the portfolio optimization problem and the optimal power flow problem.
36"
PREDICT AND OPTIMIZE,0.07991360691144708,"2
Predict and optimize
37"
PREDICT AND OPTIMIZE,0.08207343412526998,"In this section, we provide an overview of the existing research in the predict and optimize domain.
38"
PREDICT AND OPTIMIZE,0.08423326133909287,"Then, we define the P&O problem and introduce the solution approach that we are going to investigate
39"
PREDICT AND OPTIMIZE,0.08639308855291576,"later in this manuscript.
40"
RELATED WORK,0.08855291576673865,"2.1
Related work
41"
RELATED WORK,0.09071274298056156,"To the best of our knowledge, the predict and optimize framework was first introduced by Elmachtoub
42"
RELATED WORK,0.09287257019438445,"and Grigas [2017]. They consider optimization problems with linear objectives and derive a convex
43"
RELATED WORK,0.09503239740820735,"approximation of the task performance function. Then, they optimize the prediction model by
44"
RELATED WORK,0.09719222462203024,"using sub-gradients of this approximation. Later, this method was extended onto combinatorial
45"
RELATED WORK,0.09935205183585313,"problems by Mandi et al. [2020]. Several other approximations were introduced in other studies
46"
RELATED WORK,0.10151187904967603,"focusing on combinatorial problems. Vlastelica et al. [2019] derive a differentiable piecewise-linear
47"
RELATED WORK,0.10367170626349892,"approximation for the task performance; Berthet et al. [2020] employ stochastic perturbations to
48"
RELATED WORK,0.10583153347732181,"approximate derivative of combinatorial problems.
49"
RELATED WORK,0.1079913606911447,"Unlike in the combinatorial case, continuous convex optimization problems do allow exact differenti-
50"
RELATED WORK,0.1101511879049676,"ation of the loss function. The sequence of works [Amos and Kolter, 2017], [Agrawal et al., 2019b],
51"
RELATED WORK,0.11231101511879049,"[Agrawal et al., 2019a] developed a differential optimization technique to compute the derivative of
52"
RELATED WORK,0.11447084233261338,"convex optimization problems. In their latest work [Agrawal et al., 2019a], the authors delivered a
53"
RELATED WORK,0.11663066954643629,"general method that allows differentiating disciplined convex programs [Grant et al., 2006]. This
54"
RELATED WORK,0.11879049676025918,"result gave rise to new applications of P&O to convex optimization: Uysal et al. [2021] applied
55"
RELATED WORK,0.12095032397408208,"convex differential optimization to the risk budgeting portfolio optimization problem; Wang et al.
56"
RELATED WORK,0.12311015118790497,"[2020] utilized it to learn surrogate models for predict and optimize; Donti et al. [2017] applied
57"
RELATED WORK,0.12526997840172785,"the method to three different real-world benchmarks. Moreover, several studies applied differential
58"
RELATED WORK,0.12742980561555076,"optimization to predict and optimize for other problem classes. In Wilder et al. [2019], it was used in
59"
RELATED WORK,0.12958963282937366,"linear optimization via constructing a quadratic approximation of the problem. Later, Mandi and Guns
60"
RELATED WORK,0.13174946004319654,"[2020] improved upon this result by using logarithmic approximations. Ferber et al. [2020] combined
61"
RELATED WORK,0.13390928725701945,"a similar idea with the cutting plane approach and used differential optimization in combinatorial
62"
RELATED WORK,0.13606911447084233,"problems.
63"
RELATED WORK,0.13822894168466524,"Outside of predict and optimize, differential optimization also has found several applications. Chen
64"
RELATED WORK,0.14038876889848811,"et al. [2021] used it to train reinforcement learning agents in the action space with convex constraints,
65"
RELATED WORK,0.14254859611231102,"and Agrawal et al. [2019c], employed it for tuning model predictive control algorithms.
66"
RELATED WORK,0.1447084233261339,"While the benefits of the differential optimization approach to predict and optimize are numerous, it
67"
RELATED WORK,0.1468682505399568,"is still not fully understood. It was reported in several studies Vlastelica et al. [2019], Wilder et al.
68"
RELATED WORK,0.1490280777537797,"[2019], that the gradient of a linear problem is zero everywhere, except for the finite set of points
69"
RELATED WORK,0.1511879049676026,"where it is undefined. Since any linear problem is convex, this observation suggests that the gradients
70"
RELATED WORK,0.15334773218142547,"of convex problems should be also thoroughly investigated.
71"
PROBLEM FORMULATION,0.15550755939524838,"2.2
Problem formulation
72"
PROBLEM FORMULATION,0.15766738660907129,"In this section, we introduce the P&O problem. We refer readers to Elmachtoub and Grigas [2017]
73"
PROBLEM FORMULATION,0.15982721382289417,"for further details. In predict and optimize, we solve optimization problems of the form
74"
PROBLEM FORMULATION,0.16198704103671707,"arg max
x
f(x, w) s. t. x ∈C,
(True problem)"
PROBLEM FORMULATION,0.16414686825053995,"where x ∈Rn is the decision variable, w ∈Ru is a vector of unknown parameters, f : Rn × Ru →R
75"
PROBLEM FORMULATION,0.16630669546436286,"is the objective function, and C is the feasibility region. The defining feature of this problem is that
76"
PROBLEM FORMULATION,0.16846652267818574,"the parameters w are unknown at the moment when the decision must be made. Therefore, the true
77"
PROBLEM FORMULATION,0.17062634989200864,"optimization problem is under-defined and cannot be solved directly.
78"
PROBLEM FORMULATION,0.17278617710583152,"One way to deal with the unknown parameters w is to use a prediction ˆw instead. Then, the decision
79"
PROBLEM FORMULATION,0.17494600431965443,"can be computed by solving the following problem, which we refer to as the internal problem:
80"
PROBLEM FORMULATION,0.1771058315334773,"x∗( ˆw) = arg max
x
f(x, ˆw) s. t. x ∈C.
(Internal problem)"
PROBLEM FORMULATION,0.17926565874730022,"A commonly made assumption is that instead of w, we observe a feature vector o that contains some
81"
PROBLEM FORMULATION,0.18142548596112312,"information about w. Also, we have a dataset D = {(ok, wk)}, e.g., of historical data, which we
82"
PROBLEM FORMULATION,0.183585313174946,"can use to learn the relation between w and o. This setup enables using ML models to compute the
83"
PROBLEM FORMULATION,0.1857451403887689,"prediction. We denote the prediction model by ϕθ, and thus we have ˆw = ϕθ(o).
84"
PROBLEM FORMULATION,0.1879049676025918,"The problem described above is not specific to predict and optimize. What separates the P&O
85"
PROBLEM FORMULATION,0.1900647948164147,"paradigm from earlier works is the approach to training the model ϕθ. In the past, machine learning
86"
PROBLEM FORMULATION,0.19222462203023757,"models would be trained to predict w as accurately as possible, e.g., in Mukhopadhyay and Vorobey-
87"
PROBLEM FORMULATION,0.19438444924406048,"chik [2017]. However, the parameter prediction error is merely an artificial objective and our true
88"
PROBLEM FORMULATION,0.19654427645788336,"goal is to derive a decision x that maximizes the task performance f(x, w). The main goal of the
89"
PROBLEM FORMULATION,0.19870410367170627,"P&O approach is to utilize this objective for training the model ϕθ. The task performance achieved
90"
PROBLEM FORMULATION,0.20086393088552915,"by ϕθ on the dataset D can be quantified by the following loss function:
91"
PROBLEM FORMULATION,0.20302375809935205,L(θ) = −1 |D| X
PROBLEM FORMULATION,0.20518358531317496,"(o,w)∈D
f

x∗ 
ϕθ(o)

, w

(1)"
PROBLEM FORMULATION,0.20734341252699784,"Most machine learning algorithms for training models are based on computing the gradient of
92"
PROBLEM FORMULATION,0.20950323974082075,"the loss function (Kiefer and Wolfowitz [1952]). To train ϕθ with a gradient-based algorithm, we
93"
PROBLEM FORMULATION,0.21166306695464362,"need to differentiate L over θ, and hence we need to compute the gradient ∇θf

x∗( ˆw), w

, where
94"
PROBLEM FORMULATION,0.21382289416846653,"ˆw = ϕθ(o). Applying the chain rule, it can be decomposed into three terms:
95"
PROBLEM FORMULATION,0.2159827213822894,"∇θf

x∗( ˆw), w

= ∇xf
 
x∗( ˆw), w

∇ˆ
wx∗( ˆw) ∇θ ˆw.
(2)"
PROBLEM FORMULATION,0.21814254859611232,"The second term, ∇ˆ
wx∗( ˆw), is the Jacobian of the solution of the optimization problem over the
96"
PROBLEM FORMULATION,0.2203023758099352,"prediction ˆw. An exact method to compute this Jacobian was introduced in Agrawal et al. [2019a],
97"
PROBLEM FORMULATION,0.2224622030237581,"but it has never been thoroughly analyzed. In the next section, we show that ∇ˆ
wx∗( ˆw) has a large
98"
PROBLEM FORMULATION,0.22462203023758098,"null space, thereby causing the total gradient in Eq. 2 to be zero even outside of the optimum.
99"
DIFFERENTIABLE OPTIMIZATION,0.2267818574514039,"3
Differentiable optimization
100"
DIFFERENTIABLE OPTIMIZATION,0.22894168466522677,"In this section, we study the derivative of convex optimization programs over the parameters of the
101"
DIFFERENTIABLE OPTIMIZATION,0.23110151187904968,"objective function. We show that the gradient in Eq. 2 is often zero outside of the optimum, and
102"
DIFFERENTIABLE OPTIMIZATION,0.23326133909287258,"hence it causes gradient-following methods to get stuck in suboptimal solutions. In the second part of
103"
DIFFERENTIABLE OPTIMIZATION,0.23542116630669546,"this section, we introduce a method to solve this problem.
104"
DIFFERENTIABLE OPTIMIZATION,0.23758099352051837,"Without loss of generality, we consider a single instance of the problem, i.e., one sample (o, w) ∈D.
105"
DIFFERENTIABLE OPTIMIZATION,0.23974082073434125,"Everywhere in this section, we denote the prediction by ˆw = ϕθ(o). Then, the decision is computed
106"
DIFFERENTIABLE OPTIMIZATION,0.24190064794816415,"as a solution of the internal optimization problem defined as follows:
107"
DIFFERENTIABLE OPTIMIZATION,0.24406047516198703,"x∗( ˆw) = arg max
x
f(x, ˆw) s.t. x ∈C.
(3)"
DIFFERENTIABLE OPTIMIZATION,0.24622030237580994,"We use ˆx to denote the value of x∗( ˆw) for a given prediction ˆw. As we are interested in convex
108"
DIFFERENTIABLE OPTIMIZATION,0.24838012958963282,"optimization problems, we make the following assumptions:
109"
DIFFERENTIABLE OPTIMIZATION,0.2505399568034557,"Assumption 1. The objective function f(x, w) is concave and twice continuously differentiable in x
110"
DIFFERENTIABLE OPTIMIZATION,0.2526997840172786,"for any w.
111"
DIFFERENTIABLE OPTIMIZATION,0.2548596112311015,"Assumption 2. The feasibility region C is convex, i.e., {C = {x|gi(x) ≤0, i = 1, . . . , l}, where gi(x)
112"
DIFFERENTIABLE OPTIMIZATION,0.2570194384449244,"are convex differentiable functions. Moreover, for any x ∈C, the gradients {∇xgi(x)|gi(x) = 0} of
113"
DIFFERENTIABLE OPTIMIZATION,0.2591792656587473,"the active constraints are linearly independent. 1
114"
DIFFERENTIABLE OPTIMIZATION,0.2613390928725702,"Additionally, we make an assumption about how f depends on w, which holds for many real-world
115"
DIFFERENTIABLE OPTIMIZATION,0.2634989200863931,"problems, including linear and quadratic optimization problems.
116"
DIFFERENTIABLE OPTIMIZATION,0.265658747300216,"Assumption 3. The objective function f(x, w) is twice continuously differentiable in w.
117"
DIFFERENTIABLE OPTIMIZATION,0.2678185745140389,"Throughout this paper, we use derivatives of different objects. For clarity, we first provide an overview
118"
DIFFERENTIABLE OPTIMIZATION,0.26997840172786175,"of them: the gradient of the true objective function over the decision, ∇xf(ˆx, w); the Jacobian of the
119"
DIFFERENTIABLE OPTIMIZATION,0.27213822894168466,"decision over the prediction, ∇ˆ
wx∗( ˆw); the Jacobian of the prediction over the ML model parameters,
120"
DIFFERENTIABLE OPTIMIZATION,0.27429805615550756,"∇θ ˆw; and the gradient of the predicted objective in the internal problem, ∇xf(x, ˆw). In the next
121"
DIFFERENTIABLE OPTIMIZATION,0.27645788336933047,"section, we establish some crucial properties of the Jacobian ∇ˆ
wx∗( ˆw).
122"
DIFFERENTIABLE OPTIMIZATION,0.2786177105831533,"1As is, Assumption 2 does not allow equality constraints. For clarity, we use this formulation in the main
body of the paper. In the appendix, we show that our results hold for the equality constraints as well."
DIFFERENTIABLE OPTIMIZATION,0.28077753779697623,"Figure 1: Gradient cones ˆx + G(ˆx) (orange cones) and internal gradients ∇xf(ˆx, ˆw) (black arrows)
at different points ˆx (red dots) in different feasibility regions C (blue cube and cylinder)."
THE ZERO-GRADIENT THEOREM,0.28293736501079914,"3.1
The zero-gradient theorem
123"
THE ZERO-GRADIENT THEOREM,0.28509719222462204,"We begin by investigating the relation between the values of the function x∗( ˆw) and the gradient
124"
THE ZERO-GRADIENT THEOREM,0.28725701943844495,"of the internal objective, ∇xf(x, ˆw). Let ni := ∇xgi(ˆx), i = 1, . . . , l be the normal vectors of the
125"
THE ZERO-GRADIENT THEOREM,0.2894168466522678,"constraints at ˆx, Then, the KKT conditions Kuhn and Tucker [1951] at ˆx state that there exist real
126"
THE ZERO-GRADIENT THEOREM,0.2915766738660907,"values α1, . . . , αl such that the following holds:
127"
THE ZERO-GRADIENT THEOREM,0.2937365010799136,"∇xf(ˆx, ˆw) = l
X"
THE ZERO-GRADIENT THEOREM,0.2958963282937365,"i=1
αini,
αigi(ˆx) = 0,
αi ≥0,
gi(ˆx) ≤0,
i = 1, . . . , l."
THE ZERO-GRADIENT THEOREM,0.2980561555075594,"Under Assumptions 1 and 2, the KKT multipliers αi are uniquely defined by ˆw and ˆx. Thus, as ˆx is
128"
THE ZERO-GRADIENT THEOREM,0.3002159827213823,"defined by ˆw, we sometimes write αi( ˆw) to emphasize that it is, in fact, a function of ˆw. To provide
129"
THE ZERO-GRADIENT THEOREM,0.3023758099352052,"a geometrical perspective on the KKT conditions, we introduce the following definition:
130"
THE ZERO-GRADIENT THEOREM,0.3045356371490281,"Definition 3.1. Let x ∈C and let I(x) = {i|gi(x) = 0} be the set of indices of the constraints active
131"
THE ZERO-GRADIENT THEOREM,0.30669546436285094,"at x. Let ni = ∇xgi(x), ∀i ∈I(x), be the normal vectors of these constraints. The gradient cone,
132"
THE ZERO-GRADIENT THEOREM,0.30885529157667385,"G(x) :=
n P"
THE ZERO-GRADIENT THEOREM,0.31101511879049676,"i∈I αini|αi ≥0
o
, is the positive linear span of normal vectors ni.
133"
THE ZERO-GRADIENT THEOREM,0.31317494600431967,"Combining the KKT conditions with Definition 3.1, we immediately arrive at the following property:
134"
THE ZERO-GRADIENT THEOREM,0.31533477321814257,"Property 3.2. Let x ∈C and let ∇xf(x, ˆw) be the internal gradient at x. Then, x is a solution to the
135"
THE ZERO-GRADIENT THEOREM,0.3174946004319654,"problem in Eq. 3 if and only if ∀i ∈I(x), ∃αi ≥0, such that ∇xf(x, ˆw) = P"
THE ZERO-GRADIENT THEOREM,0.31965442764578833,"i∈I(x) αini ∈G(x),
136"
THE ZERO-GRADIENT THEOREM,0.32181425485961124,"where I(x) is the set of indices of active constraints, I(x) = {i|gi(x) = 0}.
137"
THE ZERO-GRADIENT THEOREM,0.32397408207343414,"While trivial, this property provides a geometrical interpretation of the problem. Effectively, a point
138"
THE ZERO-GRADIENT THEOREM,0.326133909287257,"x is a solution to the problem in Eq. 3 if and only if the internal gradient at this point lies inside its
139"
THE ZERO-GRADIENT THEOREM,0.3282937365010799,"gradient cone. Figure 1 illustrates this property.
140"
THE ZERO-GRADIENT THEOREM,0.3304535637149028,"Before studying the Jacobian ∇ˆ
wx∗( ˆw), we first need to address the question of when this Jacobian
141"
THE ZERO-GRADIENT THEOREM,0.3326133909287257,"exists. Sufficient conditions for existence are given in Fiacco [1976]. Under Assumptions 1-3, these
142"
THE ZERO-GRADIENT THEOREM,0.3347732181425486,"conditions can be reformulated as follows:
143"
THE ZERO-GRADIENT THEOREM,0.3369330453563715,"Lemma 3.3 (Theorem 2.1 in Fiacco [1976]). Let Assumptions 1-3 hold and let
144"
THE ZERO-GRADIENT THEOREM,0.3390928725701944,"∇xf(ˆx, ˆw) =
X"
THE ZERO-GRADIENT THEOREM,0.3412526997840173,"i∈I(ˆx)
αi( ˆw)ni"
THE ZERO-GRADIENT THEOREM,0.3434125269978402,"be the representation of the internal gradient with the normals of the active constraints. Then,
145"
THE ZERO-GRADIENT THEOREM,0.34557235421166305,"suppose that the strict complementary slackness condition holds, i.e., αi( ˆw) > 0, ∀i ∈I(ˆx). Then,
146"
THE ZERO-GRADIENT THEOREM,0.34773218142548595,"the Jacobian ∇ˆ
wx∗( ˆw) exists at ˆw. Moreover, αi(·) is continuous around ˆw for any i ∈I(ˆx).
147"
THE ZERO-GRADIENT THEOREM,0.34989200863930886,"Proof of this lemma is given in Fiacco [1976]. This result establishes that strict complementary
148"
THE ZERO-GRADIENT THEOREM,0.35205183585313177,"slackness is sufficient for the Jacobian ∇ˆ
wx∗( ˆw) to exist. In most cases, the points that violate strict
149"
THE ZERO-GRADIENT THEOREM,0.3542116630669546,"complementary slackness form a zero-measure set and hence can be neglected in practice.
150"
THE ZERO-GRADIENT THEOREM,0.3563714902807775,"Now, we have all the necessary tools to describe the structure of the Jacobian ∇ˆ
wx∗( ˆw). Suppose
151"
THE ZERO-GRADIENT THEOREM,0.35853131749460043,"that the strict complementary slackness condition holds at ˆx and hence the Jacobian exists. Assume
152"
THE ZERO-GRADIENT THEOREM,0.36069114470842334,"that we perturb ˆw and obtain ˆw′. Let ˆx′ = x∗( ˆw′) denote the solution corresponding to ˆw′. What
153"
THE ZERO-GRADIENT THEOREM,0.36285097192224625,"can be said about ˆx′? Strict complementary slackness implies that the constraints active at ˆx will
154"
THE ZERO-GRADIENT THEOREM,0.3650107991360691,"remain active at ˆx′ if the difference ∥ˆw′ −ˆw∥2
2 is small enough. Therefore, the decision ˆx′ can only
155"
THE ZERO-GRADIENT THEOREM,0.367170626349892,"move within the tangent space of C at ˆx, i.e., orthogonally to all ni, i ∈I(ˆx.) Hence, when more
156"
THE ZERO-GRADIENT THEOREM,0.3693304535637149,"constraints are active, ˆx′ can move in less directions. Formally, we obtain the following lemma:
157"
THE ZERO-GRADIENT THEOREM,0.3714902807775378,"Lemma 3.4. Suppose that the strict complementary slackness conditions hold at ˆx and let
158"
THE ZERO-GRADIENT THEOREM,0.37365010799136067,"∇xf(ˆx, ˆw) = P"
THE ZERO-GRADIENT THEOREM,0.3758099352051836,"i∈I(ˆx) αini, αi > 0,
∀i ∈I(ˆx) be the internal gradient.
Let N(ˆx) =
159"
THE ZERO-GRADIENT THEOREM,0.3779697624190065,"span({ni | i ∈I(ˆx)}) be the linear span of the gradient cone. Then N(ˆx) is contained in the
160"
THE ZERO-GRADIENT THEOREM,0.3801295896328294,"left null space of ∇ˆ
wx∗( ˆw), i.e., v ∇ˆ
wx∗( ˆw) = 0, ∀v ∈N(ˆx)
161"
THE ZERO-GRADIENT THEOREM,0.38228941684665224,"The formal proof of this result can be found in the appendix. Lemma 3.4 is very important, as it
162"
THE ZERO-GRADIENT THEOREM,0.38444924406047515,"specifies in what directions x∗( ˆw) can move as a consequence of changing ˆw. Now, the first term in
163"
THE ZERO-GRADIENT THEOREM,0.38660907127429806,"the chain rule in Eq. 2, ∇xf(ˆx, w), specifies in what directions x∗( ˆw) should move in order for the
164"
THE ZERO-GRADIENT THEOREM,0.38876889848812096,"true objective to increase. Naturally, if these directions are contained in the null space of ∇ˆ
wx∗( ˆw),
165"
THE ZERO-GRADIENT THEOREM,0.39092872570194387,"then the total gradient in Eq. 2 is zero. This observation constitutes the main theorem of this paper –
166"
THE ZERO-GRADIENT THEOREM,0.3930885529157667,"the zero-gradient theorem.
167"
THE ZERO-GRADIENT THEOREM,0.3952483801295896,"Theorem 3.5 (Zero-gradient theorem). Let ˆw be a prediction, and let ˆx be the solution of the internal
168"
THE ZERO-GRADIENT THEOREM,0.39740820734341253,"optimization problem defined in Eq. 3. Suppose that the strict complementary slackness conditions
169"
THE ZERO-GRADIENT THEOREM,0.39956803455723544,"hold at ˆx and let N(ˆx) = span({ni | i ∈I(ˆx)}) be the linear span of the gradient cone at ˆx. Then,
170"
THE ZERO-GRADIENT THEOREM,0.4017278617710583,"∇xf(ˆx, w) ∈N(ˆx) =⇒∇θf(ˆx, w) = 0.
171"
THE ZERO-GRADIENT THEOREM,0.4038876889848812,"The proof of this theorem is obtained by simply applying Lemma 3.4 to the chain rule in Eq. 2.
172"
THE ZERO-GRADIENT THEOREM,0.4060475161987041,"The theorem claims that the gradient of the P&O loss in Eq. 1 can be zero in the points outside
173"
THE ZERO-GRADIENT THEOREM,0.408207343412527,"of the optimal solution. Hence, any gradient-following method “shall not pass” these points. In
174"
THE ZERO-GRADIENT THEOREM,0.4103671706263499,"particular, the zero-gradient phenomenon happens in such points ˆx where the true gradient ∇xf(ˆx, w)
175"
THE ZERO-GRADIENT THEOREM,0.41252699784017277,"is contained in the space N(ˆx) spanned by the gradient cone G(ˆx). As the dimensionality of this
176"
THE ZERO-GRADIENT THEOREM,0.4146868250539957,"space grows with the number of active constraints, the zero-gradient issue is particularly important
177"
THE ZERO-GRADIENT THEOREM,0.4168466522678186,"for problems with a large number of constraints. In the worst case, N(ˆx) can be as big as the whole
178"
THE ZERO-GRADIENT THEOREM,0.4190064794816415,"decision space Rn, thereby making the total gradient ∇θf(ˆx, w) from Eq. 2 zero for any value of
179"
THE ZERO-GRADIENT THEOREM,0.42116630669546434,"the true gradient ∇xf(ˆx, w). In the following sections, we introduce a method that resolves the
180"
THE ZERO-GRADIENT THEOREM,0.42332613390928725,"zero-gradient problem and provides theoretical guarantees for its performance.
181"
QUADRATIC PROGRAMMING APPROXIMATION,0.42548596112311016,"3.2
Quadratic programming approximation
182"
QUADRATIC PROGRAMMING APPROXIMATION,0.42764578833693306,"The fundamental assumption of the predict and optimize framework is that training ϕθ using the
183"
QUADRATIC PROGRAMMING APPROXIMATION,0.4298056155507559,"task performance loss is better than fitting it to the true values of w. Hence, the models trained with
184"
QUADRATIC PROGRAMMING APPROXIMATION,0.4319654427645788,"predict and optimize might output ˆw that is significantly different from the true w and yet produces
185"
QUADRATIC PROGRAMMING APPROXIMATION,0.43412526997840173,"good decisions. Taking this argument one step further, we claim that the objective function f(x, ˆw) in
186"
QUADRATIC PROGRAMMING APPROXIMATION,0.43628509719222464,"the internal optimization problem in Eq. 3 does not need to be the same as the true objective f(x, w).
187"
QUADRATIC PROGRAMMING APPROXIMATION,0.43844492440604754,"In particular, we suggest computing decisions using a simple quadratic program (QP):
188"
QUADRATIC PROGRAMMING APPROXIMATION,0.4406047516198704,"x∗
QP ( ˆw) = arg max
x
−∥x −ˆw∥2
2 s.t. x ∈C.
(4)"
QUADRATIC PROGRAMMING APPROXIMATION,0.4427645788336933,"The reasons for this choice are manyfold. First, the internal objective fQP (x, ˆw) = −∥x −ˆw∥2
2, is
189"
QUADRATIC PROGRAMMING APPROXIMATION,0.4449244060475162,"strictly concave and hence x∗
QP ( ˆw) is always uniquely-defined. Moreover, the range of xQP ( ˆw) is
190"
QUADRATIC PROGRAMMING APPROXIMATION,0.4470842332613391,"C, i.e., ∀x ∈C, ∃ˆw such that x = x∗
QP ( ˆw). Hence, it can represent any optimal solution. However,
191"
QUADRATIC PROGRAMMING APPROXIMATION,0.44924406047516197,"the most important property of QP is that its Jacobian is very simple, which we explain below.
192"
QUADRATIC PROGRAMMING APPROXIMATION,0.4514038876889849,"The problem in Eq. 4 has a simple geometrical interpretation: the point x = ˆw is the unconstrained
193"
QUADRATIC PROGRAMMING APPROXIMATION,0.4535637149028078,"maximum of fQP (x, ˆw) and x∗
QP ( ˆw) is its Euclidean projection on the feasibility set C, see Figure 2.
194"
QUADRATIC PROGRAMMING APPROXIMATION,0.4557235421166307,"To compute the Jacobian ∇ˆ
w x∗
QP , we need to understand how perturbations of ˆw affect x∗
QP .
195"
QUADRATIC PROGRAMMING APPROXIMATION,0.45788336933045354,"Employing the geometrical intuition above, we obtain the following lemma:
196"
QUADRATIC PROGRAMMING APPROXIMATION,0.46004319654427644,"Lemma 3.6. Let ˆw be a prediction and ˆx be the optimal solution of the QP problem defined in
197"
QUADRATIC PROGRAMMING APPROXIMATION,0.46220302375809935,"Eq. 4. Let the strict complementary slackness condition hold and let {ni|i ∈I(ˆx)} be the normals
198"
QUADRATIC PROGRAMMING APPROXIMATION,0.46436285097192226,"of the active constraints. Let {ej|j = 1, . . . , n −|I(ˆx|)} be an orthogonal complement of vectors
199"
QUADRATIC PROGRAMMING APPROXIMATION,0.46652267818574517,"{ni|i ∈I(ˆx)} to a basis of Rn. Then, the representation of the Jacobian ∇ˆ
wxQP ( ˆw) in the basis
200"
QUADRATIC PROGRAMMING APPROXIMATION,0.468682505399568,"{ni} ∪{ej} is a diagonal matrix. Its first |I(ˆx)| diagonal entries are zero, and the others are one.
201"
QUADRATIC PROGRAMMING APPROXIMATION,0.4708423326133909,"Proof of this lemma can be found in the appendix. Lemma 3.6 implies that the Jacobian ∇ˆ
wxQP ( ˆw)
202"
QUADRATIC PROGRAMMING APPROXIMATION,0.47300215982721383,"has a simple form and can be easily computed by hand. While providing computational benefits, this
203"
QUADRATIC PROGRAMMING APPROXIMATION,0.47516198704103674,"approach does not address the zero-gradient problem. In the next section, we introduce a method to
204"
QUADRATIC PROGRAMMING APPROXIMATION,0.4773218142548596,"compute an approximate of the Jacobian ∇ˆ
wxQP ( ˆw) that has a strictly one-dimensional null space.
205"
QUADRATIC PROGRAMMING APPROXIMATION,0.4794816414686825,"Combined with the QP approximation, it is guaranteed to at least not decrease the task performance.
206 A"
QUADRATIC PROGRAMMING APPROXIMATION,0.4816414686825054,"Figure 2: Left: Illustration of QP. The internal gradient (black arrow) at the solution of the QP ˆx
(red point) is orthogonal to the feasibility region C (blue area) and points towards the unconstrained
maximum ˆw (purple cross). Right: Illustration of the r−smoothed problem. The internal gradient
(black arrow) is orthogonal to the r−smoothed feasibility region Cr(ˆx, ˆw) (green circle) at the
decision ˆx (red point)."
LOCAL SMOOTHING,0.4838012958963283,"3.3
Local smoothing
207"
LOCAL SMOOTHING,0.48596112311015116,"We identified a fundamental issue of differential optimization – the zero-gradient problem. We
208"
LOCAL SMOOTHING,0.48812095032397407,"showed that the null space of the Jacobian ∇ˆ
wx( ˆw) depends on the number of constraints active at
209"
LOCAL SMOOTHING,0.490280777537797,"ˆx. Generally, this number can be as large as the number of optimized variables n, and the gradient-
210"
LOCAL SMOOTHING,0.4924406047516199,"descent algorithms can get stuck in certain points on the boundary of the feasibility region.
211"
LOCAL SMOOTHING,0.4946004319654428,"In this section, we propose a simple way to modify the feasibility region – we smooth C locally
212"
LOCAL SMOOTHING,0.49676025917926564,"around the point for which we compute the Jacobian, thereby ensuring that its null space becomes
213"
LOCAL SMOOTHING,0.49892008639308855,"one dimensional. First, we define a method for the general setup, without imposing any assumptions
214"
LOCAL SMOOTHING,0.5010799136069114,"on the optimization problem. Then, we demonstrate that combined with the QP approximation from
215"
LOCAL SMOOTHING,0.5032397408207343,"Section 3.2, this smoothing approach has theoretical guarantees.
216"
LOCAL SMOOTHING,0.5053995680345572,"We begin with the general case – the problem in Eq. 3. Let ∇xf(ˆx, ˆw) = P
i∈I(ˆx) αini be the
217"
LOCAL SMOOTHING,0.5075593952483801,"internal gradient at ˆx for some αi ≥0, ∀i ∈I(ˆx). Then, we introduce the following definition:
218"
LOCAL SMOOTHING,0.509719222462203,"Definition 3.7. Let r > 0 be a positive real number.
Let c = ˆx −r
∇xf(ˆx, ˆ
w)
∥∇xf(ˆx, ˆ
w)∥2 . The lo-
219"
LOCAL SMOOTHING,0.5118790496760259,"cal r-smoothed feasibility region, Cr(ˆx, ˆw) := {y|y ∈Rn, ∥y −c∥2 ≤r}, is a ball of ra-
220"
LOCAL SMOOTHING,0.5140388768898488,"dius r around c. The local r−smoothed problem Pr(ˆx, ˆw) with parameters ˆx, ˆw is defined as
221"
LOCAL SMOOTHING,0.5161987041036717,"x∗
r( ˆw) := arg maxx∈Cr(ˆx, ˆ
w) f(x, ˆw).
222"
LOCAL SMOOTHING,0.5183585313174947,"Figure 2 shows an example of the local r−smoothed problem. Now, let ˆxr = x∗
r( ˆw) denote the
223"
LOCAL SMOOTHING,0.5205183585313174,"solution of Pr(ˆx, ˆw). By construction, the internal gradient at ˆxr lies in the one-dimensional gradient
224"
LOCAL SMOOTHING,0.5226781857451404,"cone, and hence, by Property 3.2, ˆxr = ˆx. The main purpose of smoothing is to approximate the
225"
LOCAL SMOOTHING,0.5248380129589633,"gradient in Eq. 2 by substituting ∇ˆ
wx∗( ˆw) with ∇ˆ
wx∗
r( ˆw). We highlight that the decisions are still
226"
LOCAL SMOOTHING,0.5269978401727862,"computed using the non-smoothed problem x∗( ˆw) and x∗
r(ˆx, ˆw) is used exclusively to perform the
227"
LOCAL SMOOTHING,0.5291576673866091,"gradient update step. In other words, we use the following expression to compute the gradient:
228"
LOCAL SMOOTHING,0.531317494600432,"∇θf(x∗( ˆw), w) ≈∇xf
 
ˆx, w

∇ˆ
wx∗
r( ˆw) ∇θ ˆw
(5)
It is worth mentioning that the strict complementary slackness in the original problem is a stronger
229"
LOCAL SMOOTHING,0.5334773218142549,"condition than the strict complementary slackness on Pr(ˆx, ˆw). Therefore, the Jacobian of the
230"
LOCAL SMOOTHING,0.5356371490280778,"r−smoothed problem can exist even for predictions ˆw where the true Jacobian does not.
231"
LOCAL SMOOTHING,0.5377969762419006,"Generally, the efficiency of r−smoothing depends on the form of the internal problem in Eq. 3. Below,
232"
LOCAL SMOOTHING,0.5399568034557235,"we show that combining r−smoothing with the QP approximation has guarantees on its performance.
233"
LOCAL SMOOTHING,0.5421166306695464,"First, we notice that Lemma 3.6 prescribes the Jacobian of the r−smoothed QP problem:
234"
LOCAL SMOOTHING,0.5442764578833693,"Property 3.8. Let ˆx = x∗
QP ( ˆw) be a decision derived via QP. Suppose that the complementary
235"
LOCAL SMOOTHING,0.5464362850971922,"slackness conditions hold for Pr(ˆx, ˆw) and let e1 = ∇xfQP (ˆx, ˆw) be the internal gradient. Let
236"
LOCAL SMOOTHING,0.5485961123110151,"{e2, . . . , en} be a complement of e1 to an orthogonal basis of Rn. Then, the Jacobian ∇ˆ
wx∗
r( ˆw) of
237"
LOCAL SMOOTHING,0.550755939524838,"the local r−smoothed problem expressed in the basis {e1, e2, . . . , en} is a diagonal matrix. Its first
238"
LOCAL SMOOTHING,0.5529157667386609,"entry is zero, others are ones.
239"
LOCAL SMOOTHING,0.5550755939524838,"As Cr(ˆx, ˆw) is defined by a single constraint, the null space of ∇ˆ
wx∗
r(ˆx, ˆw) is always one-dimensional.
240"
LOCAL SMOOTHING,0.5572354211663066,"Hence, the zero-gradient problem can only occur when the internal gradient ∇xfQP (ˆx, ˆw) and the
241"
LOCAL SMOOTHING,0.5593952483801296,"true gradient ∇xf(ˆx, w) are exactly collinear. Hence, we expect r−smoothing to significantly
242"
LOCAL SMOOTHING,0.5615550755939525,"improve upon the zero-gradient problem. Next, we show that the r−smoothed Jacobian is actually a
243"
LOCAL SMOOTHING,0.5637149028077754,"good approximation. In the following theorem, we demonstrate that the local r−smoothing of the
244"
LOCAL SMOOTHING,0.5658747300215983,"QP approach indeed yields a “good” direction for the gradient steps.
245"
LOCAL SMOOTHING,0.5680345572354212,"Theorem 3.9. Let ˆx = x∗
QP ( ˆw) be the decision obtained via QP and let ∇ˆ
wx∗
r( ˆw) be the Ja-
246"
LOCAL SMOOTHING,0.5701943844492441,"cobian of the r−smoothed QP problem. Let ∆ˆw = ∇xf(ˆx, w) ∇ˆ
wx∗
r( ˆw) be the prediction per-
247"
LOCAL SMOOTHING,0.572354211663067,"turbation obtained by using this Jacobian and let ˆw′(t) = ˆw + t∆ˆw be the updated prediction.
248"
LOCAL SMOOTHING,0.5745140388768899,"Then, for t →0+, using ˆw′(t) results in a non-decrease in the task performance. In other words,
249"
LOCAL SMOOTHING,0.5766738660907127,"f
 
x∗
QP ( ˆw′(t)), w

≥f
 
x∗
QP ( ˆw), w

.
250"
LOCAL SMOOTHING,0.5788336933045356,"Interestingly, this result does not depend on r. However, this is to be expected – no matter the
251"
LOCAL SMOOTHING,0.5809935205183585,"radius of Cr, the Jacobian of Pr(ˆx, ˆw) is still the same by Lemma 3.6. Theorem 3.10 shows that
252"
LOCAL SMOOTHING,0.5831533477321814,"using r−smoothing together with the QP approximation results in analytically computable Jacobian
253"
LOCAL SMOOTHING,0.5853131749460043,"that has a strictly one-dimensional null space. Therefore, we are much less likely to encounter the
254"
LOCAL SMOOTHING,0.5874730021598272,"zero-gradient problem when using this approximation. However, the resulting one-dimensional null
255"
LOCAL SMOOTHING,0.5896328293736501,"space contains the only direction that can move the prediction ˆw, and hence the decision ˆx, inside C.
256"
LOCAL SMOOTHING,0.591792656587473,"This might become crucial, for example, when the optimal solution with respect to the true objective
257"
LOCAL SMOOTHING,0.593952483801296,"lies in the interior of C. To resolve this problem, we use the projection distance regularization method
258"
LOCAL SMOOTHING,0.5961123110151187,"first suggested in Chen et al. [2021]. Specifically, we add a penalty term
259"
LOCAL SMOOTHING,0.5982721382289417,"p( ˆw) = α∥ˆx −ˆw∥2
2,
(6)"
LOCAL SMOOTHING,0.6004319654427646,"where α ∈R+ is a hyperparameter. Minimizing this term, we push ˆw along the null-space of the
260"
LOCAL SMOOTHING,0.6025917926565875,"Jacobian towards the feasibility region and eventually move ˆx inside C.
261"
THE TRAINING PROCESS,0.6047516198704104,"3.4
The training process
262"
THE TRAINING PROCESS,0.6069114470842333,"In this section, we summarize the results of Sections 3.1-3.3 and describe the final algorithm we
263"
THE TRAINING PROCESS,0.6090712742980562,"use to solve the P&O problems. For each problem instance (o, w), we first compute the prediction,
264"
THE TRAINING PROCESS,0.6112311015118791,"ˆw = ϕθ(o), and the decision using the QP approximation method, ˆx = x∗
QP ( ˆw). Then, we obtain the
265"
THE TRAINING PROCESS,0.6133909287257019,"achieved objective value, f(ˆx, w). During training, we update the model parameters θ by performing
266"
THE TRAINING PROCESS,0.6155507559395248,"the steps described in Algorithm 1.
267"
THE TRAINING PROCESS,0.6177105831533477,Algorithm 1
THE TRAINING PROCESS,0.6198704103671706,"for (o, w) ∈D do"
THE TRAINING PROCESS,0.6220302375809935,"ˆx ←x∗
QP
 
ϕ(o)

▷Compute the decision"
THE TRAINING PROCESS,0.6241900647948164,"fx ←∇xf(ˆx, w)
▷Compute the true gradient
ˆfx ←∇xf(ˆx, ˆw)
▷Compute the internal gradient"
THE TRAINING PROCESS,0.6263498920086393,"f 0 ←f ⊤
x ˆ
fx
∥ˆ
fx∥2
▷Project the true gradient on the null space of ∇ˆ
wx∗
r( ˆw)"
THE TRAINING PROCESS,0.6285097192224622,"∆ˆw ←fx ∇ˆ
wx∗
r( ˆw) = fx −f 0.
▷Compute the prediction perturbation"
THE TRAINING PROCESS,0.6306695464362851,"∆ˆwreg ←2α(ˆx −ˆw)
▷Compute the anti-gradient of the penalty from Eq. 6"
THE TRAINING PROCESS,0.6328293736501079,"∆θ ←(∆ˆw + ∆ˆwreg)∇θ ϕθ(o)
▷Approximate the total gradient"
THE TRAINING PROCESS,0.6349892008639308,"θ ←θ + η∆θ
▷Perform the gradient step of size η"
EXPERIMENTS,0.6371490280777538,"4
Experiments
268"
EXPERIMENTS,0.6393088552915767,"The main result of Section 3 is the zero-gradient theorem, which describes when the gradient
269"
EXPERIMENTS,0.6414686825053996,"∇θf
 
x∗( ˆw), w

is zero. To deal with it, we introduced the QP approach for computing the decisions,
270"
EXPERIMENTS,0.6436285097192225,"r−smoothing for approximating the Jacobian ∇ˆ
wx∗( ˆw), and projection distance regularization to
271"
EXPERIMENTS,0.6457883369330454,"deal with the remaining null space dimension. Our solution deals with the zero gradient problem
272"
EXPERIMENTS,0.6479481641468683,"by combining these methods. In this section, we use two real-world P&O problems to evaluate the
273"
EXPERIMENTS,0.6501079913606912,"efficiency of our method.
274"
PORTFOLIO OPTIMIZATION,0.652267818574514,"4.1
Portfolio optimization
275"
PORTFOLIO OPTIMIZATION,0.6544276457883369,"Following Wang et al. [2020], we apply the predict and optimize framework to the Markowitz
276"
PORTFOLIO OPTIMIZATION,0.6565874730021598,"mean-variance stock market optimization problem Markowitz and Todd [2000]. In this problem, we
277"
PORTFOLIO OPTIMIZATION,0.6587473002159827,"act as an investor who seeks to maximize the immediate return but minimize the risk penalty. The
278"
PORTFOLIO OPTIMIZATION,0.6609071274298056,"(a)
(b)
(c)"
PORTFOLIO OPTIMIZATION,0.6630669546436285,"Figure 3: Comparison of different methods on the portfolio optimization problem. y−axis represents
the mean and the standard deviation of the regret on the test set for four seeds. The lower the better.
(a) Regret for different λ. (b) Regret during training for λ = 0.25. (c) Regret during training for
λ = 2."
PORTFOLIO OPTIMIZATION,0.6652267818574514,"decision variable, x ∈Rn, is a positive vector representing our investment in different securities. The
279"
PORTFOLIO OPTIMIZATION,0.6673866090712743,"budget constraint forces the investments to add up to one, i.e., P
i xi = 1. The objective is defined as
280"
PORTFOLIO OPTIMIZATION,0.6695464362850972,"f(x, p, Q) = p⊤x −λx⊤Qx, where p ∈Rn is the immediate return of the securities, λ ≥0 is the
281"
PORTFOLIO OPTIMIZATION,0.67170626349892,"risk-aversion weight, and Q ∈Rn×n is the positive definite matrix of covariance between securities.
282"
PORTFOLIO OPTIMIZATION,0.673866090712743,"The portfolio optimization problem is then defined as follows:
283"
PORTFOLIO OPTIMIZATION,0.6760259179265659,"arg max
x
p⊤x −λ x⊤Qx
|
{z
}
f(x,p,Q)"
PORTFOLIO OPTIMIZATION,0.6781857451403888,"s. t.
nP"
PORTFOLIO OPTIMIZATION,0.6803455723542117,"i=1
xi = 1,
x ≥0.
(7)"
PORTFOLIO OPTIMIZATION,0.6825053995680346,"This is a quadratic optimization problem with unknown parameters (p, Q), as neither the immediate
284"
PORTFOLIO OPTIMIZATION,0.6846652267818575,"return nor the true covariance matrix is known at the decision-making moment. Following Wang
285"
PORTFOLIO OPTIMIZATION,0.6868250539956804,"et al. [2020], we use historical data from QUANDL WIKI prices QUANDL [2020] for 505 largest
286"
PORTFOLIO OPTIMIZATION,0.6889848812095032,"companies on the American market for the period 2014-2017. The dataset is processed and for every
287"
PORTFOLIO OPTIMIZATION,0.6911447084233261,"day we obtain a feature vector summarizing the recent price dynamic. For further details on the
288"
PORTFOLIO OPTIMIZATION,0.693304535637149,"processing we refer readers to the code2 and to Wang et al. [2020]. For each run, we randomly split
289"
PORTFOLIO OPTIMIZATION,0.6954643628509719,"data into train, validation, and test sets by using 70%, 15%, and 15% of the whole dataset respectively.
290"
PORTFOLIO OPTIMIZATION,0.6976241900647948,"To evaluate the performance of different algorithms, we use regret, defined as
291"
PORTFOLIO OPTIMIZATION,0.6997840172786177,"regret(o, w) = f

x∗ 
ϕθ(o), w

−max
x
f
 
x, w

.
(8)"
PORTFOLIO OPTIMIZATION,0.7019438444924406,"In the experiments, we used λ from the set {0, 0.1, 0.25, 0.5, 1, 2}. For the larger value of λ, the true
292"
PORTFOLIO OPTIMIZATION,0.7041036717062635,"objective f(x, p, Q) is “more” quadratic, and hence its maximum is more likely to lie in the interior
293"
PORTFOLIO OPTIMIZATION,0.7062634989200864,"of C. For smaller λ’s, on the other hand, the true objective becomes almost linear and hence it usually
294"
PORTFOLIO OPTIMIZATION,0.7084233261339092,"attains its maximum on the boundary of C.
295"
PORTFOLIO OPTIMIZATION,0.7105831533477321,"First of all, we define the QP approximation of the portfolio optimization problem:
296"
PORTFOLIO OPTIMIZATION,0.712742980561555,"x∗( ˆw) = arg max
x
−(x −ˆw)⊤I(x −ˆw)
s. t.
nP"
PORTFOLIO OPTIMIZATION,0.714902807775378,"i=1
xi = 1,
x ≥0.
(9)"
PORTFOLIO OPTIMIZATION,0.7170626349892009,Table 1: Performance of the QP approximation
PORTFOLIO OPTIMIZATION,0.7192224622030238,Final test regret
PORTFOLIO OPTIMIZATION,0.7213822894168467,"λ
QP approximation
Predict both Q and p"
PORTFOLIO OPTIMIZATION,0.7235421166306696,"0
0.061 ± 0.002
0.064 ± 0.002
0.1
0.047 ± 0.002
0.052 ± 0.002
0.25
0.040 ± 0.002
0.045 ± 0.002
0.5
0.039 ± 0.001
0.041 ± 0.001
1
0.040 ± 0.002
0.041 ± 0.002
2
0.039 ± 0.001
0.04 ± 0.001"
PORTFOLIO OPTIMIZATION,0.7257019438444925,"As the problem is quadratic, the only differ-
297"
PORTFOLIO OPTIMIZATION,0.7278617710583153,"ence introduced by the QP approximation comes
298"
PORTFOLIO OPTIMIZATION,0.7300215982721382,"from using the identity matrix I instead of Q.
299"
PORTFOLIO OPTIMIZATION,0.7321814254859611,"The results in Table 1 indicate that it performs
300"
PORTFOLIO OPTIMIZATION,0.734341252699784,"at least as well as learning to predict both p and
301"
PORTFOLIO OPTIMIZATION,0.7365010799136069,"Q, and hence in all other experiments we used
302"
PORTFOLIO OPTIMIZATION,0.7386609071274298,"the QP approximation to compute the decisions.
303"
PORTFOLIO OPTIMIZATION,0.7408207343412527,"To investigate the zero-gradient effect, we
304"
PORTFOLIO OPTIMIZATION,0.7429805615550756,"compared four ways to train the predictor:
305"
PORTFOLIO OPTIMIZATION,0.7451403887688985,"with/without r−smoothing and with/without the
306"
PORTFOLIO OPTIMIZATION,0.7473002159827213,"penalty term from Eq. 6. The model used for
307"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7494600431965442,2Placeholder for the link to the GitHub repository
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7516198704103672,"the predictor is a 2-layer neural network, further details on the training process are described in the
308"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7537796976241901,"appendix. The results in Figure 3 indicate that r−smoothing significantly improves the performance
309"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.755939524838013,"when the true objective is more linear. This result matches the theory from Section 3, as linear true
310"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7580993520518359,"objective pushes the decision ˆx towards the boundary of C, and hence it is more likely to enter points
311"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7602591792656588,"with a large gradient cone. For the more quadratic objectives, the true maximum is often in the
312"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7624190064794817,"interior of C, and hence r−smoothing alone is not sufficient to reach it. In this case, the regularization
313"
PLACEHOLDER FOR THE LINK TO THE GITHUB REPOSITORY,0.7645788336933045,"term from Eq 6 becomes crucial, as it is the only method that can push ˆx inside C.
314"
OPTIMAL POWER FLOW IN A DC GRID,0.7667386609071274,"4.2
Optimal power flow in a DC grid
315"
OPTIMAL POWER FLOW IN A DC GRID,0.7688984881209503,"Figure 4: Comparison of different
methods on the DC grid OPF prob-
lem. y−axis represents the mean
and standard deviation of the test re-
gret for twelve random seeds."
OPTIMAL POWER FLOW IN A DC GRID,0.7710583153347732,"To further understand the zero-gradient phenomenon, we con-
316"
OPTIMAL POWER FLOW IN A DC GRID,0.7732181425485961,"sider the optimal power flow problem (OPF) for DC grids [Li
317"
OPTIMAL POWER FLOW IN A DC GRID,0.775377969762419,"et al., 2018]. Due to power losses, the constraints in this prob-
318"
OPTIMAL POWER FLOW IN A DC GRID,0.7775377969762419,"lem are non-linear, thus making it computationally hard. In
319"
OPTIMAL POWER FLOW IN A DC GRID,0.7796976241900648,"our experiments, we used a linearized version of the problem
320"
OPTIMAL POWER FLOW IN A DC GRID,0.7818574514038877,"that represents a DC grid without power losses. The deci-
321"
OPTIMAL POWER FLOW IN A DC GRID,0.7840172786177105,"sion variable is the vector of nodal voltages v ∈Rn, and the
322"
OPTIMAL POWER FLOW IN A DC GRID,0.7861771058315334,"unknown parameter w represents either the value gained by
323"
OPTIMAL POWER FLOW IN A DC GRID,0.7883369330453563,"serving power to a customer or the price paid for utilizing
324"
OPTIMAL POWER FLOW IN A DC GRID,0.7904967602591793,"a generator. The reference voltage v0 ∈R, the admittance
325"
OPTIMAL POWER FLOW IN A DC GRID,0.7926565874730022,"matrix Y, and the constraint bounds represent the physical
326"
OPTIMAL POWER FLOW IN A DC GRID,0.7948164146868251,"properties of the grid.
327"
OPTIMAL POWER FLOW IN A DC GRID,0.796976241900648,"max
v
f(v, w) = −v0w⊤(Y v)"
OPTIMAL POWER FLOW IN A DC GRID,0.7991360691144709,"subject to:
V ≤v ≤¯V"
OPTIMAL POWER FLOW IN A DC GRID,0.8012958963282938,P ≤−v0Y v ≤¯P
OPTIMAL POWER FLOW IN A DC GRID,0.8034557235421166,I ≤Yij(vi −vj) ≤¯I
OPTIMAL POWER FLOW IN A DC GRID,0.8056155507559395,"We refer the reader to Li et al. [2018] for further details of the
328"
OPTIMAL POWER FLOW IN A DC GRID,0.8077753779697624,"problem. Importantly, the feasibility region is defined by multiple linear constraints, and therefore
329"
OPTIMAL POWER FLOW IN A DC GRID,0.8099352051835853,"we expect it to have numerous vertices with large gradient cones. The objective function f(v, w)
330"
OPTIMAL POWER FLOW IN A DC GRID,0.8120950323974082,"quantifies the social welfare [Veviurko et al., 2022] generated by all the users of the power grid.
331"
OPTIMAL POWER FLOW IN A DC GRID,0.8142548596112311,"Importantly, f(v, w) is linear, and hence its maximum lies on the boundary of the feasibility region.
332"
OPTIMAL POWER FLOW IN A DC GRID,0.816414686825054,"We compared the same four methods as before on this problem using randomly generated grids
333"
OPTIMAL POWER FLOW IN A DC GRID,0.8185745140388769,"with four generators and twelve loads. Same as before, we use QP approximation, x∗
QP ( ˆw) =
334"
OPTIMAL POWER FLOW IN A DC GRID,0.8207343412526998,"arg maxx∈C(−∥ˆw −x∥2
2), to compute the decisions. The results in Figure 4 confirm our hypothesis –
335"
OPTIMAL POWER FLOW IN A DC GRID,0.8228941684665226,"even though we differentiate through a quadratic problem, the linearity of the true objective causes
336"
OPTIMAL POWER FLOW IN A DC GRID,0.8250539956803455,"the zero-gradient effect as the decision is pushed towards the boundary of the feasibility region. Then,
337"
OPTIMAL POWER FLOW IN A DC GRID,0.8272138228941684,"due to a large number of constraints, it is likely to enter a vertex with a large gradient cone and get
338"
OPTIMAL POWER FLOW IN A DC GRID,0.8293736501079914,"stuck there. In this case, r−smoothing greatly outperforms the standard differential optimization
339"
OPTIMAL POWER FLOW IN A DC GRID,0.8315334773218143,"method from Agrawal et al. [2019a], while the projection distance regularization does not help a lot.
340"
CONCLUSION,0.8336933045356372,"5
Conclusion
341"
CONCLUSION,0.8358531317494601,"In this work, we discover and explain the zero-gradient problem in P&O for convex optimization.
342"
CONCLUSION,0.838012958963283,"In particular, we show that the null space of the Jacobian of a convex optimization problem can
343"
CONCLUSION,0.8401727861771058,"get arbitrarily large, especially in the case with numerous constraints. This phenomenon prevents
344"
CONCLUSION,0.8423326133909287,"gradient-following algorithms from learning optimal solutions in convex P&O problems.
345"
CONCLUSION,0.8444924406047516,"To resolve this issue, we introduce a method to compute an approximation of the Jacobian. It
346"
CONCLUSION,0.8466522678185745,"is done by smoothing the feasibility region around the current solution and thereby reducing the
347"
CONCLUSION,0.8488120950323974,"dimensionality of the null space to one. We prove that the combination of smoothing with the QP
348"
CONCLUSION,0.8509719222462203,"approximation results in the gradient update steps that at least do not decrease the task performance,
349"
CONCLUSION,0.8531317494600432,"but often allow to escape the zero-gradient cones. To enable movement along the remaining one-
350"
CONCLUSION,0.8552915766738661,"dimensional null space, we add a projection distance regularization term. The suggested method
351"
CONCLUSION,0.857451403887689,"leads to significantly better results for the convex P&O problems that suffer from the zero-gradient
352"
CONCLUSION,0.8596112311015118,"problem the most – those with many constraints and with the true optimum lying on the boundary of
353"
CONCLUSION,0.8617710583153347,"the feasibility set.
354"
REFERENCES,0.8639308855291576,"References
355"
REFERENCES,0.8660907127429806,"Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter.
356"
REFERENCES,0.8682505399568035,"Differentiable convex optimization layers. Adv. Neural Inf. Process. Syst., 32, 2019a.
357"
REFERENCES,0.8704103671706264,"Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M Moursi. Differentiating
358"
REFERENCES,0.8725701943844493,"through a cone program. April 2019b.
359"
REFERENCES,0.8747300215982722,"Akshay Agrawal, Shane Barratt, Stephen Boyd, and Bartolomeo Stellato. Learning convex optimiza-
360"
REFERENCES,0.8768898488120951,"tion control policies. pages 1–26, 2019c.
361"
REFERENCES,0.8790496760259179,"Brandon Amos and J Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks.
362"
REFERENCES,0.8812095032397408,"34th International Conference on Machine Learning, ICML 2017, 1:179–191, 2017.
363"
REFERENCES,0.8833693304535637,"RC Bansal. Optimization methods for electric power systems: An overview. International Journal of
364"
REFERENCES,0.8855291576673866,"Emerging Electric Power Systems, 2(1), 2005.
365"
REFERENCES,0.8876889848812095,"Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean Philippe Vert, and Francis
366"
REFERENCES,0.8898488120950324,"Bach. Learning with differentiable perturbed optimizers. Adv. Neural Inf. Process. Syst., 2020-
367"
REFERENCES,0.8920086393088553,"Decem:1–24, 2020.
368"
REFERENCES,0.8941684665226782,"Bingqing Chen, Priya L Donti, Kyri Baker, J Zico Kolter, and Mario Bergés. Enforcing policy
369"
REFERENCES,0.896328293736501,"feasibility constraints through differentiable projection for energy optimization. e-Energy 2021 -
370"
REFERENCES,0.8984881209503239,"Proceedings of the 2021 12th ACM International Conference on Future Energy Systems, pages
371"
REFERENCES,0.9006479481641468,"199–210, 2021.
372"
REFERENCES,0.9028077753779697,"Gerard Cornuejols and Reha Tütüncü. Optimization methods in finance, volume 5. Cambridge
373"
REFERENCES,0.9049676025917927,"University Press, 2006.
374"
REFERENCES,0.9071274298056156,"Priya L Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in stochastic
375"
REFERENCES,0.9092872570194385,"optimization. Adv. Neural Inf. Process. Syst., 2017-Decem:5485–5495, 2017.
376"
REFERENCES,0.9114470842332614,"Adam N Elmachtoub and Paul Grigas.
Smart"" predict, then optimize"".
arXiv preprint
377"
REFERENCES,0.9136069114470843,"arXiv:1710.08005, 2017.
378"
REFERENCES,0.9157667386609071,"Aaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. MIPaaL: Mixed integer program as a
379"
REFERENCES,0.91792656587473,"layer. AAAI 2020 - 34th AAAI Conference on Artificial Intelligence, pages 1504–1511, 2020.
380"
REFERENCES,0.9200863930885529,"Anthony V Fiacco. Sensitivity analysis for nonlinear programming using penalty methods. Mathe-
381"
REFERENCES,0.9222462203023758,"matical programming, 10(1):287–311, 1976.
382"
REFERENCES,0.9244060475161987,"Michael Grant, Stephen Boyd, and Yinyu Ye. Disciplined Convex Programming, pages 155–210.
383"
REFERENCES,0.9265658747300216,"Springer US, Boston, MA, 2006. ISBN 978-0-387-30528-8.
384"
REFERENCES,0.9287257019438445,"Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function.
385"
REFERENCES,0.9308855291576674,"The Annals of Mathematical Statistics, pages 462–466, 1952.
386"
REFERENCES,0.9330453563714903,"Harold W Kuhn and Albert W Tucker. Nonlinear programming. In Berkeley Symposium on Mathe-
387"
REFERENCES,0.9352051835853131,"matical Statistics and Probability, 2:481–492, 1951.
388"
REFERENCES,0.937365010799136,"Jia Li, Feng Liu, Zhaojian Wang, Steven H. Low, and Shengwei Mei. Optimal Power Flow in
389"
REFERENCES,0.9395248380129589,"Stand-Alone DC Microgrids. IEEE Transactions on Power Systems, 33(5):5496–5506, 2018. ISSN
390"
REFERENCES,0.9416846652267818,"08858950. doi: 10.1109/TPWRS.2018.2801280.
391"
REFERENCES,0.9438444924406048,"J Mandi, E Demirovic, P J Stuckey, and T Guns. Smart predict-and-optimize for hard combinatorial
392"
REFERENCES,0.9460043196544277,"optimization problems. Proc. Conf. AAAI Artif. Intell., 2020.
393"
REFERENCES,0.9481641468682506,"Jayanta Mandi and Tias Guns. Interior point solving for LP-based prediction+optimisation. October
394"
REFERENCES,0.9503239740820735,"2020.
395"
REFERENCES,0.9524838012958964,"Harry M Markowitz and G Peter Todd. Mean-variance analysis in portfolio choice and capital
396"
REFERENCES,0.9546436285097192,"markets, volume 66. John Wiley & Sons, 2000.
397"
REFERENCES,0.9568034557235421,"Ayan Mukhopadhyay and Yevgeniy Vorobeychik. Prioritized allocation of emergency responders
398"
REFERENCES,0.958963282937365,"based on a continuous-time incident prediction model. In International Conference on Autonomous
399"
REFERENCES,0.9611231101511879,"Agents and MultiAgent Systems, 2017.
400"
REFERENCES,0.9632829373650108,"Chao Ning and Fengqi You. Optimization under uncertainty in the era of big data and deep learning:
401"
REFERENCES,0.9654427645788337,"When machine learning meets mathematical programming. Computers & Chemical Engineering,
402"
REFERENCES,0.9676025917926566,"125:434–448, 2019.
403"
REFERENCES,0.9697624190064795,"QUANDL. Quandl wiki prices, 2020., 2020.
404"
REFERENCES,0.9719222462203023,"Purushothaman Raja and Sivagurunathan Pugazhenthi. Optimal path planning of mobile robots: A
405"
REFERENCES,0.9740820734341252,"review. International journal of physical sciences, 7(9):1314–1320, 2012.
406"
REFERENCES,0.9762419006479481,"Ayse Sinem Uysal, Xiaoyue Li, and John M Mulvey. End-to-End risk budgeting portfolio optimization
407"
REFERENCES,0.978401727861771,"with neural networks. July 2021.
408"
REFERENCES,0.980561555075594,"Grigorii Veviurko, Wendelin Böhmer, Laurens Mackay, and Mathijs de Weerdt. Surrogate dc micro-
409"
REFERENCES,0.9827213822894169,"grid models for optimization of charging electric vehicles under partial observability. Energies, 15
410"
REFERENCES,0.9848812095032398,"(4):1389, 2022.
411"
REFERENCES,0.9870410367170627,"Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, and Michal Rolínek. Differentiation of
412"
REFERENCES,0.9892008639308856,"blackbox combinatorial solvers. pages 1–19, 2019.
413"
REFERENCES,0.9913606911447084,"Kai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. Automatically learning compact
414"
REFERENCES,0.9935205183585313,"quality-aware surrogates for optimization problems. June 2020.
415"
REFERENCES,0.9956803455723542,"Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the Data-Decisions pipeline: Decision-
416"
REFERENCES,0.9978401727861771,"Focused learning for combinatorial optimization. AAAI, 33(01):1658–1665, July 2019.
417"
