Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035211267605633804,"Graph neural networks (GNNs) have drawn signiï¬cant research attention recently,
mostly under the setting of semi-supervised learning. When task-agnostic rep-
resentations are preferred or supervision is simply unavailable, the auto-encoder
framework comes in handy with a natural graph reconstruction objective for un-
supervised GNN training. However, existing graph auto-encoders are designed to
reconstruct the direct links, so GNNs trained in this way are only optimized towards
proximity-oriented graph mining tasks, and will fall short when the topological
structures matter. In this work, we revisit the graph encoding process of GNNs
which essentially learns to encode the neighborhood information of each node into
an embedding vector, and propose a novel graph decoder to reconstruct the entire
neighborhood information regarding both proximity and structure via Neighbor-
hood Wasserstein Reconstruction (NWR). Speciï¬cally, from the GNN embedding
of each node, NWR jointly predicts its node degree and neighbor feature distribu-
tion, where the distribution prediction adopts an optimal-transport loss based on
the Wasserstein distance. Extensive experiments on both synthetic and real-world
network datasets show that the unsupervised node representations learned with
NWR have much more advantageous in structure-oriented graph mining tasks,
while also achieving competitive performance in proximity-oriented ones.1"
INTRODUCTION,0.007042253521126761,"1
INTRODUCTION"
INTRODUCTION,0.01056338028169014,"Network/Graph representation learning (a.k.a. embedding) aims to preserve the high-dimensional
complex graph information involving node features and link structures in a low-dimensional em-
bedding space, which requires effective feature selection and dimension reduction (Hamilton et al.,
2017b). Graph neural networks (GNNs) have done great jobs to this end, but most of them rely
on node labels from speciï¬c downstream tasks to be trained in a semi-supervised fashion (Kipf &
Welling, 2017; Hamilton et al., 2017a; Wu et al., 2019; VeliË‡ckoviÂ´c et al., 2018a; Klicpera et al.,
2019; Chien et al., 2021). However, similar to other domains, unsupervised representation learning is
preferred in many cases, not only because labeled data is not always available (Hu et al., 2020; Xie
et al., 2021), but also task-agnostic representations can better transfer and generalize among different
scenarios (Erhan et al., 2010; Bengio, 2012; Radford et al., 2016)."
INTRODUCTION,0.014084507042253521,"To train GNNs in an unsupervised fashion, the classic auto-encoder framework (Baldi, 2012; Good-
fellow et al., 2016) provides a natural solution and has been widely explored such as the prominent
work (V)GAE (Kipf & Welling, 2016). Speciï¬cally, classic auto-encoders aim to decode from the
low-dimensional representations information in the entire receptive ï¬eld of the neural networks. For
GNNs, the receptive ï¬eld of a node representation is its entire neighborhood. However, existing graph
auto-encoders appear away from such a motivation and are designed to merely decode the direct
links between the node pairs by minimizing a link reconstruction loss. The fundamental difï¬culty to
reconstruct the entire receptive ï¬elds of GNNs is due to the non-trivial design of a reconstruction loss
on the irregular graph structures. Unfortunately, the over-simpliï¬cation into link reconstruction makes
the learned node representations drop much information and thus provides undesired performance in
many downstream tasks."
INTRODUCTION,0.017605633802816902,"âˆ—Equal contribution. â€ Corresponding author.
1Code available at https://github.com/mtang724/NWR-GAE."
INTRODUCTION,0.02112676056338028,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02464788732394366,"Figure 1: A toy example (redrawn from real experiments) illustrating different types of information
in real-world graphs and the two-dimensional node embedding spaces learned by different models.
Colors indicates node features. The nodes with same digits are transitive in some graph automorphism."
INTRODUCTION,0.028169014084507043,"Take Figure 1 as an example, where different types of information are mixed in a graph (e.g., proximity
and structure information as illustrated in Figure 5 in Appendix B (Cui et al., 2021)). The node
representations learned by existing graph auto-encoders such as GAE (Kipf & Welling, 2016) are
driven too much to be similar on linked nodes due to their simple link reconstruction objective,
and thus fail to distinguish node pairs like (2, 4) and (3, 5) in the cliques, though they clearly have
different structural roles and node features. On the other hand, structure-oriented embedding models
like GraphWave (Donnat et al., 2018) cannot consider node features and spatial proximity, and thus
fail to distinguish node pairs like (0, 1), (2, 4) and (3, 5) though they have different features, as well
as (2, 5) and (3, 4) though they are further apart. An ideal unsupervised node representation learning
model as we advocate in this work is expected to be task-agnostic and encode as much information as
possible of all types in a low-dimensional embedding space."
INTRODUCTION,0.03169014084507042,"In this work, we aim to fundamentally address the above limitations of existing unsupervised node
representation learning models by proposing a novel graph auto-encoder framework for unsupervised
GNN training. The new framework is equipped with a powerful decoder that fully reconstructs the
information from the entire receptive ï¬eld of a node representation. Our key technical contribution
lies in designing a principled and easy-to-compute loss to reconstruct the entire irregular structures of
the node neighborhood. Speciï¬cally, we characterize the decoding procedure as iteratively sampling
from a series of probability distributions deï¬ned over multi-hop neighborsâ€™ representations obtained
through the GNN encoder. Then, the reconstruction loss can be decomposed into three parts, for
sampling numbers (node degrees), neighbor-representation distributions and node features. All of
these terms are easy to compute but may represent the entire receptive ï¬eld of a node instead of just
the linkage information to its direct neighbors. For the most novel and important term, neighbor-
representation distribution reconstruction, we adopt an optimal-transport loss based on Wasserstein
distance (Frogner et al., 2015) and thus name this new framework as Neighborhood Wasserstein
Reconstruction Graph Auto-Encoder (NWR-GAE). As also illustrated in Figure 1, NWR-GAE can
effectively distinguish all pairs of nodes dissimilar in different perspectives, and concisely reï¬‚ect
their similarities in the low-dimensional embedding space."
INTRODUCTION,0.035211267605633804,"We have conducted extensive experiments on four synthetic datasets and nine real-world datasets.
Among the real-world datasets, three have proximity-oriented tasks, three have structure-oriented
tasks, and three have proximity-structure-mixed tasks. We can observe signiï¬cant improvements
brought by NWR-GAE over the best method among the state-of-the-art baselines on all structure-
oriented tasks (8.74% to 18.48%) and proximity-structure-mixed tasks (-2.98% to 8.62%), and
competitive performance on proximity-oriented tasks (-3.21% to -0.32%). In-depth ablation and
hyper-parameter studies further consolidate the claimed advantages of NWR-GAE."
INTRODUCTION,0.03873239436619718,"2
PRELIMINARIES, MOTIVATIONS & OTHER RELATED WORKS"
INTRODUCTION,0.04225352112676056,"In this work, we focus on the auto-encoder framework for unsupervised task-agnostic graph repre-
sentation learning. The original motivation of auto-encoders is to perform neural-network-based
dimension reduction of the data that originally lies in a high-dimensional space (Hinton & Salakhutdi-
nov, 2006). Speciï¬cally, an auto-encoder consists of two components, an encoder and a decoder. The
encoder works to compress each data point into a low-dimensional vector representation, while the
decoder works to reconstruct the original information from this vector. By minimizing the reconstruc-"
INTRODUCTION,0.045774647887323945,"Published as a conference paper at ICLR 2022 ğ‘£ ğ‘ ğ‘ ğ‘
 ğ‘’ ğ‘“"
-HOP NEIGHBORHOOD,0.04929577464788732,2-hop neighborhood ğ‘£
-HOP NEIGHBORHOOD,0.0528169014084507,"ğ‘
 ğ‘
 ğ‘ ğ‘£"
-HOP NEIGHBORHOOD,0.056338028169014086,"ğ‘
 ğ‘’
 ğ‘£
 ğ‘ ğ‘“ ğ‘£
 ğ‘’ â„à¯©"
-HOP NEIGHBORHOOD,0.05985915492957746,"(à¬¶)
Two GNN layers
1-hop neighborhood â„à¯©"
-HOP NEIGHBORHOOD,0.06338028169014084,"(à¬¶)obtained via two 
GNN layers has the 
information from 2-hop
neighborhood of ğ‘£. 
Figure 2: The information source/receptive ï¬eld of the node representation h(2)
v ."
-HOP NEIGHBORHOOD,0.06690140845070422,"tion error, the encoder automatically converges to a good compressor that allows the low-dimensional
representations to capture as much information as possible from the original data."
-HOP NEIGHBORHOOD,0.07042253521126761,"Although the above high-level idea of auto-encoders is clear, when it is applied to graph structured
data, the problem becomes challenging. This is because in graph-structured data, information of data
points (nodes to be speciï¬c as most widely studied) is correlated due to the ambient graph structure.
Without a speciï¬c task needed in a priori, the learned low-dimensional representation of a node
should carry as much information as possible from not only its own features but also the features of
the nodes it connects to (both directly and indirectly)."
-HOP NEIGHBORHOOD,0.07394366197183098,"This implies that when building auto-encoders for graph-structure data, we expect the node represen-
tations to be able to reconstruct all correlated node features. However, existing graph auto-encoders
seem to be away from this motivation. Previous prominent works such as unsupervised Graph-
SAGE (Hamilton et al., 2017a), GAE (Kipf & Welling, 2016), their generative variants such as
VGAE (Kipf & Welling, 2016), CondGen (Yang et al., 2019) (), and many others (Grover et al.,
2019; Pan et al., 2018; Shi et al., 2020; Yang et al., 2021), use GNNs to encode graph structured
data into node representations. Without exception, they follow the rationale of traditional network
embedding techniques (Perozzi et al., 2014; Qiu et al., 2018; Grover & Leskovec, 2016) and adopt
link reconstruction in the decoder as the main drive to optimize their GNN encoders. The obtained
node representations best record the network linkage information but lose much of other important
information, such as local structures, neighborsâ€™ features, etc. Hence, these auto-encoders will most
likely fail in other tasks such as node classiï¬cations (especially structure-oriented ones as manifested
in Figure 1)."
-HOP NEIGHBORHOOD,0.07746478873239436,"To better understand this point, we carefully analyze the source of information encoded in each
node representation via a GNN. Suppose a standard message-passing GNN (Gilmer et al., 2017) is
adopted as the encoder, which is a general framework that includes GCN (Kipf & Welling, 2017),
GraphSAGE (Hamilton et al., 2017a), GAT (VeliË‡ckoviÂ´c et al., 2018a), GIN (Xu et al., 2019c) and so
on. After k-hop message passing, the source of information encoded in the representation of a node v
essentially comes from the k-hop neighborhood of v (Fig. 2). Therefore, a good representation of
node v should capture the information of features from all nodes in its k-hop neighborhood, which is
agnostic to downstream tasks. Note that this may not be ideal as nodes out of k-hop neighborhood may
also provide useful information, but this is what GNN-based graph auto-encoders can be expected
to do due to the architectures of GNN encoders. This observation motivates our study on a novel
graph decoder that can better facilitate the goal of GNN-based graph auto-encoders, based on the
neighborhood reconstruction principle. We will formalize this principle in Sec. 3."
-HOP NEIGHBORHOOD,0.08098591549295775,"Relation to the InfoMax principle. Recently, DGI (VeliË‡ckoviÂ´c et al., 2018b), EGI (Zhu et al.,
2021) and others (Sun et al., 2020; Hu et al., 2020; You et al., 2020; Hassani & Khasahmadi, 2020;
Suresh et al., 2021) have used constrasive learning for unsupervised GNN training methods and
may capture information beyond the directed links. They adopt the rule of mutual information
maximization (InfoMax), which essentially works to maximize certain correspondence between the
learned representations and the original data. For example, DGI (VeliË‡ckoviÂ´c et al., 2018b) maximizes
the correspondence between a node representation and which graph the node belongs to, but this has
no guarantee to reconstruct the structural information of node neighborhoods. Recent works even
demonstrate that maximizing such correspondence risks capturing only the noisy information that
is irrelevant to the downsteam tasks because noisy information itself is sufï¬cient for the models to
achieve InfoMax (Tschannen et al., 2020; Suresh et al., 2021), which gets demonstrated again by
our experiments. Our goal instead is to let node representations not just capture the information to
distinguish nodes but capture as much information as possible to reconstruct the features and structure
of the neighborhood."
-HOP NEIGHBORHOOD,0.08450704225352113,Published as a conference paper at ICLR 2022
-HOP NEIGHBORHOOD,0.0880281690140845,"ğœ‰à¬µ, ğœ‰à¬¶, â€¦ , ğœ‰à¯¤~ğºğ‘ğ‘¢ğ‘ ğ‘ ğ‘–ğ‘ğ‘›(ğœ‡à¯©, Î£à¯©) ğ¹ğ‘ğ‘à¯£"
-HOP NEIGHBORHOOD,0.09154929577464789,"(à¬´)
ğ¹ğ‘ğ‘à¯£"
-HOP NEIGHBORHOOD,0.09507042253521127,"(à¬µ)
ğœ“à¯£ ğ‘¥à¯© â„à¯© (à¬´) â„à¯”"
-HOP NEIGHBORHOOD,0.09859154929577464,(à¬´) â„à¯•
-HOP NEIGHBORHOOD,0.10211267605633803,(à¬´) â„à¯– (à¬´) ğ»ğ’©à³¡ (à¬´) ğœ™(à¬µ) â„à¯© (à¬µ) â„à¯”
-HOP NEIGHBORHOOD,0.1056338028169014,(à¬µ) â„à¯•
-HOP NEIGHBORHOOD,0.10915492957746478,(à¬µ) â„à¯– (à¬µ) ğ»ğ’©à³¡ (à¬µ) ğœ™(à¬¶) â„à¯© (à¬¶)
-HOP NEIGHBORHOOD,0.11267605633802817,"ğœ“à¯¦
||â„à¯©"
-HOP NEIGHBORHOOD,0.11619718309859155,"à¬´âˆ’ğœ“à¯¦â„à¯© à¬¶
||à¬¶"
-HOP NEIGHBORHOOD,0.11971830985915492,"ğœ“à¯—
ğ‘‘à¯©âˆ’ğœ“à¯—â„à¯© à¬¶
à¬¶"
-HOP NEIGHBORHOOD,0.12323943661971831,"ğ‘£à¬µ, ğ‘£à¬¶, â€¦ , ğ‘£à¯¤~ğ’©à¯©
min"
-HOP NEIGHBORHOOD,0.1267605633802817,à°—à·à·||â„à¯©à³• à¯œâˆ’â„à· à¯©
-HOP NEIGHBORHOOD,0.13028169014084506,"à¯œ,à°—à¯||à¬¶ à¯¤ à¯à­€à¬µ à¬µ à¯œà­€à¬´"
-HOP NEIGHBORHOOD,0.13380281690140844,"+
ğ‘™ğ‘œğ‘ ğ‘ 
ğœ†à¯— ğœ†à¯¦"
-HOP NEIGHBORHOOD,0.13732394366197184,sample neighbors
-HOP NEIGHBORHOOD,0.14084507042253522,decode degree
-HOP NEIGHBORHOOD,0.1443661971830986,"decode initial 
feature"
-HOP NEIGHBORHOOD,0.14788732394366197,"decode 
neighborhood
Encoder"
-HOP NEIGHBORHOOD,0.15140845070422534,Decoder ğ¹ğ‘ğ‘à°“ ğ¹ğ‘ğ‘à°™
-HOP NEIGHBORHOOD,0.15492957746478872,Figure 3: The diagram of our model using an example with 2 GNN layers as the encoder.
-HOP NEIGHBORHOOD,0.15845070422535212,"Optimal-transport (OT) losses. Many machine learning problems depend on the characterization
of the distance between two probability measures. The family f-divergence has the non-continuous
issue when the two measures of interest have non-overlapped support (Ali & Silvey, 1966). Therefore,
OT-losses are often adopted and have shown great success in generative models (Gulrajani et al.,
2017) and domain adaptation (Courty et al., 2016). OT-losses have been used to build variational
auto-encoders for non-graph data (Tolstikhin et al., 2018; Kolouri et al., 2018; Patrini et al., 2020).
But one should note the our work is not a graph-data-oriented generalization of these works: They
use OT-losses to characterize the distance between the variational distribution and the data empirical
distribution while our model even does not use a variational distribution. Our model may be further
improved by being reformulated as a variational autoencoder but we leave it as a future direction."
-HOP NEIGHBORHOOD,0.1619718309859155,"Here, we give a frequently-used OT loss based on 2-Wasserstein distance that will be used later."
-HOP NEIGHBORHOOD,0.16549295774647887,"Deï¬nition 2.1. Let P, Q denote two probability distributions with ï¬nite second moment deï¬ned on
Z âŠ†Rm. The 2-Wasserstein distance between P and Q deï¬ned on Z, Zâ€² âŠ†Rm is the solution to
the optimal mass transportation problem with â„“2 transport cost (Villani, 2008):"
-HOP NEIGHBORHOOD,0.16901408450704225,"W2(P, Q) =

inf
Î³âˆˆÎ“(P,Q) Z"
-HOP NEIGHBORHOOD,0.17253521126760563,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥2
2dÎ³(Z, Zâ€²)
1/2
(1)"
-HOP NEIGHBORHOOD,0.176056338028169,"where Î“(P, Q) contains all joint distributions of (Z, Zâ€²) with marginals P and Q respectively."
METHODS,0.1795774647887324,"3
METHODS"
METHODS,0.18309859154929578,"3.1
NEIGHBORHOOD RECONSTRUCTION PRINCIPLE (NRP)"
METHODS,0.18661971830985916,"Let G = (V, E, X) denote the input graph where V is the node set, E is the edge set and X =
{xv|v âˆˆV } includes the node features. Given a node v âˆˆV , we deï¬ne its k-hop neighborhood as
the nodes which have shortest paths of length no greater than k from v. Let Nv denote the 1-hop
neighborhood of v, and dv denote the degree of node v."
METHODS,0.19014084507042253,"We are to build an auto-encoder to learn a low-dimensional representation of each node. Speciï¬-
cally, the auto-encoder will have an encoder Ï† and a decoder Ïˆ. Ï† could be any message-passing
GNNs (Gilmer et al., 2017). The decoder further contains three parts Ïˆ = (Ïˆs, Ïˆp, Ïˆd). The physical
meaning of each part will be clear as we introduce them in the later subsections."
METHODS,0.1936619718309859,"First-hop neighborhood reconstruction. To simplify the exposition, we start from 1-hop neigh-
borhood reconstruction. We initialize the node representation by H(0) based on X. For every node
v âˆˆV , after being encoded by one GNN layer, its node representation h(1)
v
collects information from
h(0)
v
and its neighborsâ€™ representations H(0)
Nv = {h(0)
u |u âˆˆNv}. Hence, we consider the following"
METHODS,0.19718309859154928,"principle that reconstructs the information from both h(0)
v
and H(0)
Nv. Therefore, we have"
METHODS,0.2007042253521127,"min
Ï†,Ïˆ X"
METHODS,0.20422535211267606,"vâˆˆV
M((h(0)
v , H(0)
Nv), Ïˆ(h(1)
v )),
s.t. h(1)
v
= Ï†(h(0)
v , H(0)
Nv), âˆ€v âˆˆV,
(2)"
METHODS,0.20774647887323944,Published as a conference paper at ICLR 2022
METHODS,0.2112676056338028,"where M(Â·, Â·) deï¬nes the reconstruction loss. M can be split into two parts that measure self-feature
reconstruction and neighborhood reconstruction respectively:"
METHODS,0.2147887323943662,"M((h(0)
v , H(0)
Nv), Ïˆ(h(1)
v )) = Ms(h(0)
v , Ïˆ(h(1)
v )) + Mn(H(0)
Nv, Ïˆ(h(1)
v )).
(3)"
METHODS,0.21830985915492956,"Note that Ms works just as the reconstruction loss of a standard feedforward neural network (FNN)-
based auto-encoder, so we adopt"
METHODS,0.22183098591549297,"Ms(h(0)
v , Ïˆ(h(1)
v )) = âˆ¥h(0)
v
âˆ’Ïˆs(h(1)
v )âˆ¥2.
(4)"
METHODS,0.22535211267605634,"Mn is much harder to characterize, as it measures the loss to reconstruct a set of features H(0)
Nv. There
are two fundamental challenges: First, as in real-world networks the distribution of node degrees is
often long-tailed, the sets of node neighbors may have very different sizes. Second, to compare even
two equal-sized sets, a matching problem has to be solved, as no ï¬xed orders of elements in the sets
can be assumed. The complexity to solve a matching problem is high if the size of the set is large. We
solve the above challenges by decoupling the neighborhood information into a probability distribution
and a sampling number. Speciï¬cally, for node v the neighborhood information is represented as
an empirical realization of i.i.d. sampling dv elements from P(0)
v , where P(0)
v
â‰œ
1
dv
P"
METHODS,0.22887323943661972,"uâˆˆNv Î´h(0)
u .
Based on this view, we are able to decompose the reconstruction into the part for the node degree
and that for the distribution respectively, where the various sizes of node neighborhoods are handled
properly. Speciï¬cally, we adopt"
METHODS,0.2323943661971831,"Mn(H(0)
Nv, Ïˆ(h(1)
v )) = (dv âˆ’Ïˆd(h(1)
v ))2 + W2
2(P(0)
v , Ïˆ(1)
p (h(1)
v )),
(5)"
METHODS,0.23591549295774647,"where W2(Â·, Â·) is the 2-Wasserstein distance as deï¬ned in Eq.1. Plug Eqs.4,5 into Eq.3, we obtain the
objective for ï¬rst-hop neighborhood reconstruction."
METHODS,0.23943661971830985,"Generalizing to k-hop neighborhood reconstruction. Following the above derivation for the ï¬rst-
hop case, one can similarly generalize the loss for reconstructing (h(iâˆ’1)
v
, H(iâˆ’1)
Nv
) based on h(i)
v
for all 1 â‰¤i â‰¤k. Then, if we sum such losses over all nodes v âˆˆV and all hops 1 â‰¤i â‰¤k, we
may achieve the objective for k-hop neighborhood reconstruction. We may further simplify such an
objective. Typically, only the ï¬nal layer representation H(k) is used as the output dimension-reduced
representations. Too many intermediate hops make the model hard to train and slow to converge.
Therefore, we adopt a more economic way by merging the multi-step reconstruction: â„à¯©"
METHODS,0.24295774647887325,(à¯) â†’â„à¯©
METHODS,0.24647887323943662,(à¯à¬¿à¬µ) â†’â‹¯â†’â„à¯©
METHODS,0.25,(à¬µ) â†’â„à¯© (à¬´) ğ»ğ’©à³¡
METHODS,0.2535211267605634,"(à¯à¬¿à¬µ)
ğ»ğ’©à³¡"
METHODS,0.25704225352112675,"(à¬´)
â€¦
ğ»ğ’©à³¡ (à¯à¬¿à¬¶) â„à¯© (à¯) â„à¯© (à¬´) ğ»ğ’©à³¡"
METHODS,0.2605633802816901,"(à¯à¬¿à¬µ)
ğ»ğ’©à³¡"
METHODS,0.2640845070422535,"(à¬´)
â€¦
ğ»ğ’©à³¡ (à¯à¬¿à¬¶)"
METHODS,0.2676056338028169,"That is, we expect h(k)
v
to directly reconstruct H(i)
Nv even if i < k âˆ’1. Speciï¬cally, for each node
v âˆˆV , we use the reconstruction loss"
METHODS,0.2711267605633803,"Mâ€²((h(0)
v , {H(i)
Nv|0 â‰¤i â‰¤k âˆ’1}), Ïˆ(h(k)
v )) = Ms(h(0)
v , Ïˆ(h(k)
v )) + kâˆ’1
X"
METHODS,0.2746478873239437,"i=0
Mn(H(i)
Nv, Ïˆ(h(k)
v ))"
METHODS,0.27816901408450706,"= Î»sâˆ¥h(0)
v
âˆ’Ïˆs(h(k)
v )âˆ¥2 + Î»d(dv âˆ’Ïˆd(h(k)
v ))2 + kâˆ’1
X"
METHODS,0.28169014084507044,"i=0
W2
2(P(i)
v , Ïˆ(i)
p (h(k)
v )),
(6)"
METHODS,0.2852112676056338,"where Ïˆs is to decode the initial features, Ïˆd is to decode the degree and Ïˆ(i)
p , 0 â‰¤i â‰¤k âˆ’1 is
to decode the i-layer neighbor representation distribution P(i)
v (:â‰œ
1
dv
P"
METHODS,0.2887323943661972,"uâˆˆNv Î´h(i)
u ). Î»s and Î»d are
non-negative hyperparameters. Hence, the entire objective for k-hop neighborhood reconstruction is"
METHODS,0.29225352112676056,"min
Ï†,Ïˆ X"
METHODS,0.29577464788732394,"vâˆˆV
Mâ€²((h(0)
v , {H(iâˆ’1)
Nv
|1 â‰¤i â‰¤k}), Ïˆ(h(k)
v ))
s.t. H(i) = Ï†(i)(H(iâˆ’1)), 1 â‰¤i â‰¤k,
(7)"
METHODS,0.2992957746478873,where Ï† = {Ï†(i)|1 â‰¤i â‰¤k} include k GNN layers and Mâ€² is deï¬ned in Eq.6.
METHODS,0.3028169014084507,"Remark 3.1. The loss in Eq. 6 does not directly push the k-layer representation h(k)
v
to reconstruct
the features of all nodes that are direct neighbors but lie in k-hop neighborhood of v. However,
by deï¬nition, there exists a shortest path of length no greater than k from v to every node in this
k-hop neighborhood. As every two adjacent nodes along the path will introduce at least one term
of reconstruction loss in the sum of Eq. 6, i.e., the objective in Eq. 7. This guarantees that the
optimization in Eq. 7 pushes h(k)
v
to reconstruct the entire k-hop neighborhood of v."
METHODS,0.30633802816901406,Published as a conference paper at ICLR 2022
METHODS,0.30985915492957744,"3.2
DECODING DISTRIBUTIONS â€” DECODERS Ïˆ(i)
p , 0 â‰¤i â‰¤k âˆ’1"
METHODS,0.31338028169014087,"Our NRP essentially represents the node neighborhood H(i)
Nv as a sampling number (node degree dv)"
METHODS,0.31690140845070425,"plus a distribution of neighborsâ€™ representations P(i)
v
(Eqs.5,6). We adopt Wasserstein distance to
characterize the distribution reconstruction loss because P(i)
v
has atomic non-zero measure supports
in a continuous space, where the family of f-divergences such as KL-divergence cannot be applied.
Maximum mean discrepancy may be applied but it needs to specify a kernel function."
METHODS,0.3204225352112676,"We set the decoded distribution Ïˆ(i)
p (h(k)
v ) as an FNN-based transformation of a Gaussian distribution
parameterized by h(k)
v . The rationale of this setting is due to the universal approximation capability of
FNNs to (approximately) reconstruct any distributions in 1-Wasserstein distance, as formally stated
in Theorem 3.1 (See the proof in Appendix A, reproduced by Theorem 2.1 in (Lu & Lu, 2020)). For
a better empirical performance, our case adopts the 2-Wasserstein distance and an FNN with m-dim
output instead of the gradient of an FNN with 1-dim outout. Here, the reparameterization trick needs
to be used (Kingma & Welling, 2014), i.e.,
Ïˆ(i)
p (h(k)
v ) = FNN(i)
p (Î¾), Î¾ âˆ¼N(Âµv, Î£v), where Âµv = FNNÂµ(h(k)
v ), Î£v = diag(exp(FNNÏƒ(h(k)
v )))."
METHODS,0.323943661971831,"Theorem 3.1. For any Ïµ > 0, if the support of the distribution P(i)
v
lies in a bounded space of Rm,
there exists a FNN u(Â·) : Rm â†’R (and thus its gradient âˆ‡u(Â·) : Rm â†’Rm) with large enough
width and depth (depending on Ïµ) such that W2
2(P(i)
v , âˆ‡u(G)) < Ïµ where âˆ‡u(G) is the distribution
generated via the mapping âˆ‡u(Î¾), Î¾ âˆ¼a m-dim non-degenerate Gaussian distribution."
METHODS,0.3274647887323944,"A further challenge is that the Wasserstein distance between P(i)
v
and Ïˆ(i)
p (h(k)
v ) does not have a
closed form. Therefore, we adopt the empirical Wasserstein distance that can provably approximate
the population one as stated in Sec.8.4.1 in (PeyrÃ© et al., 2019). For every forward pass, the
model will get q sampled nodes Nv, denoted by v1, v2, ..., vq and thus {h(i)
vj |1 â‰¤j â‰¤q} are
q samples from P(i)
v ; Next, get q samples from N(Âµv, Î£v), denoted by Î¾1, Î¾2, ..., Î¾q, and thus
{Ë†h(i,j)
v
= FNN(i)
p (Î¾j)|1 â‰¤j â‰¤q} are q samples from Ïˆ(i)
p (h(k)
v ); Adopt the following empirical"
METHODS,0.33098591549295775,"surrogated loss of Pkâˆ’1
i=0 W2
2(P(i)
v , Ïˆ(i)
p (h(k)
v )) in Eq.6, min
Ï€ kâˆ’1
X i=0 q
X"
METHODS,0.3345070422535211,"j=1
âˆ¥h(i)
vj âˆ’Ë†h(i,Ï€(j))
v
âˆ¥2, s.t. Ï€ is a bijective mapping:[q] â†’[q].
(8)"
METHODS,0.3380281690140845,"Computing the above loss is based on solving a matching problem and needs the Hungarian algorithm
with O(q3) complexity (Jonker & Volgenant, 1987). More efï¬cient types of surrogate loss may be
adopted, such as Chamfer loss based on greedy approximation (Fan et al., 2017) or Sinkhorn loss
based on continuous relaxation (Cuturi, 2013), whose complexities are O(q2). In our experiments, as
q is ï¬xed as a small constant, say 5, we use Eq.8 based on a Hungarian matching and do not introduce
much computational overhead."
METHODS,0.3415492957746479,"Although not directly related, we would like to highlight some recent works that use OT losses as
distance between two graphs, where Wasserstein distance between the two sets of node embeddings
of these two graphs is adopted (Xu et al., 2019a;b). Borrowing such a concept, one can view our
OT-loss as also to measure the distance between the original graph and the decoded graph."
METHODS,0.34507042253521125,"3.3
FURTHER DISCUSSION â€” DECODERS Ïˆs AND Ïˆd"
METHODS,0.3485915492957746,"The decoder Ïˆs to reconstruct the initial features h(0)
v
is an FNN. The decoder Ïˆd to reconstruct the
node degree is an FNN plus exponential neuron to make the value non-negative.
Ïˆs(h(k)
v ) = FNNs(h(k)
v ),
Ïˆd(h(k)
v ) = exp(FNNd(h(k)
v )).
(9)
In practice, the original node features X could be very high-dimensional and reconstructing them
directly may introduce a lot of noise into the node representations. Instead, we may ï¬rst map X
into a latent space to initialize H(0). However, if H(0) is used in both representation learning and
reconstruction, it has the risk to collapse to trivial points. Hence, we normalize H(0) = {h(0)
v |v âˆˆV }
properly via pair-norm (Zhao & Akoglu, 2020) to avoid this trap as follows
{h(0)
v |v âˆˆV } = pair-norm({xvW|v âˆˆV }), where W is a learnable parameter matrix.
(10)"
METHODS,0.352112676056338,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.35563380281690143,"4
EXPERIMENTS"
EXPERIMENTS,0.3591549295774648,"We design our experiments to evaluate NWR-GAE, focusing on the following research questions:
RQ1: How does NWR-GAE perform on structure-role-based synthetic datasets in comparison to
state-of-the-art unsupervised graph embedding baselines? RQ2: How do NWR-GAE and its ablations
compare to the baselines on different types of real-world graph datasets? RQ3: What are the impacts
of the major model parameters including embedding size d and sampling size q on NWR-GAE?"
EXPERIMENTAL SETUP,0.3626760563380282,"4.1
EXPERIMENTAL SETUP"
DATASETS,0.36619718309859156,"4.1.1
DATASETS"
DATASETS,0.36971830985915494,"Synthetic datasets. Following existing works on structure role oriented graph embedding (Henderson
et al., 2012; Ribeiro et al., 2017; Donnat et al., 2018), we construct multiple sets of planted structural
equivalences graphs, which are generated by regularly placing different basic shapes (House, Fan,
Star) along a cycle of certain length. We use node degrees as the node features, and generate the
structural-role labels for nodes in the graphs in the same way as (Donnat et al., 2018).
Real-world graph Datasets. We use a total of nine public real-world graph datasets, which roughly
belong to three types, one with proximity-oriented (assortative (Liu et al., 2020)) labels, one with
structure-oriented (disassortative) labels, and one with proximity-structure-mixed labels. Among
them, Cora, Citeseer, Pubmed are publication networks (Namata et al., 2012); Cornell, Texas, and
Wisconsin are school department webpage networks (Pei et al., 2020); Chameleon, Squirrel are
page-page networks in Wikipedia (Rozemberczki et al., 2021); Actor is an actor co-ï¬lming network
(Tang et al., 2009). More details of the datasets are put in Appendix B."
BASELINES,0.3732394366197183,"4.1.2
BASELINES"
BASELINES,0.3767605633802817,"We compare NWR-GAE with a total of 11 baselines, representing four types of state-of-the-art on
unsupervised graph embedding, 1) Random walk based (DeepWalk, node2vec), 2) Structural role
based (RoleX, struc2vec, GraphWave), 3) Graph auto-encoder based (GAE, VGAE, ARGVA), 4)
Contrastive learning based (DGI, GraphCL, MVGRL)."
BASELINES,0.38028169014084506,"â€¢ DeepWalk (Perozzi et al., 2014): a two-phase method which generates random walks from all
nodes in the graph and then treats all walks as input to a Skipgram language model.
â€¢ node2vec (Grover & Leskovec, 2016): a method similar to DeepWalk but uses two hyper-paramters
p and q to control the behavior of the random walker.
â€¢ RolX (Henderson et al., 2012): a structure role discovery method which extracts features and group
features of each node, and then generates structure roles via matrix factorization.
â€¢ struc2vec (Ribeiro et al., 2017): a graph structural embedding method that encodes the structural
similarities of nodes and generates structural context for each node.
â€¢ GraphWave (Donnat et al., 2018): a graph structural embedding method that models the diffusion
of a spectral graph wavelet and learns embeddings from network diffusion spreads.
â€¢ GAE (Kipf & Welling, 2016): a GCN encoder trained with a link reconstruction decoder.
â€¢ VGAE (Kipf & Welling, 2016): the variational version of GAE.
â€¢ ARGVA (Pan et al., 2018): a method which considers topological structure and node content,
trained by reconstructing the graph structure.
â€¢ DGI (VeliË‡ckoviÂ´c et al., 2018b): a GCN encoder trained via graph mutual information maximization.
â€¢ GraphCL (You et al., 2020): a contrastive learning method via four types of graph augmentation.
â€¢ MVGRL (Hassani & Khasahmadi, 2020): a constastive learning method by contrasting encodings
from ï¬rst-order neighbors and a graph diffusion."
EVALUATION METRICS,0.38380281690140844,"4.1.3
EVALUATION METRICS"
EVALUATION METRICS,0.3873239436619718,"Following (Donnat et al., 2018), we measure the performance of compared algorithms on the synthetic
datasets based on agglomerative clustering (with single linkage) with the following metrics"
EVALUATION METRICS,0.3908450704225352,"â€¢ Homogeneity: conditional entropy of ground-truth among predicting clusters.
â€¢ Completeness: ratio of nodes with the same ground-truth labels assigned to the same cluster.
â€¢ Silhouette score: intra-cluster distance vs. inter-cluster distance."
EVALUATION METRICS,0.39436619718309857,Published as a conference paper at ICLR 2022
EVALUATION METRICS,0.397887323943662,Table 1: Performance of compared algorithms on structure role identiï¬cation with synthetic data.
EVALUATION METRICS,0.4014084507042254,"Dataset
Metrics
DeepWalk
node2vec
RolX
struc2vec
GraphWave
GAE
VGAE
ARGVA
DGI
GraphCL
MVGRL
NWR-GAE"
EVALUATION METRICS,0.40492957746478875,"House
Homogeneity
0.01
0.01
1.0
0.99
1.0
1.0
0.25
0.28
1.0
1.0
1.0
1.0
Completeness
0.01
0.01
1.0
0.99
1.0
1.0
0.27
0.28
1.0
1.0
1.0
1.0
Silhouette
0.29
0.33
0.99
0.45
0.99
0.99
0.21
0.19
0.99
0.99
0.99
0.99"
EVALUATION METRICS,0.4084507042253521,"House Perturbed
Homogeneity
0.06
0.03
0.65
0.21
0.52
0.36
0.29
0.24
0.24
0.41
0.63
0.65
Completeness
0.06
0.03
0.69
0.24
0.53
0.37
0.29
0.24
0.25
0.42
0.63
0.72
Silhouette
0.25
0.28
0.51
0.18
0.39
0.67
0.21
0.19
0.44
0.43
0.69
0.94"
EVALUATION METRICS,0.4119718309859155,"Varied
Homogeneity
0.26
0.24
0.91
0.63
0.87
0.65
0.50
0.66
0.36
0.93
0.93
0.93
Completeness
0.23
0.22
0.93
0.58
0.88
0.69
0.36
0.57
0.37
0.89
0.89
0.94
Silhouette
0.35
0.40
0.82
0.24
0.66
0.66
0.21
0.23
0.94
0.93
0.90
0.95"
EVALUATION METRICS,0.4154929577464789,"Perturbed & Varied
Homogeneity
0.30
0.30
0.74
0.46
0.63
0.44
0.42
0.57
0.36
0.70
0.73
0.78
Completeness
0.27
0.27
0.72
0.43
0.60
0.45
0.43
0.49
0.36
0.63
0.67
0.81
Silhouette
0.33
0.36
0.61
0.29
0.50
0.52
0.21
0.20
0.45
0.69
0.61
0.84"
EVALUATION METRICS,0.41901408450704225,Table 2: Performance of compared algorithms on different types of real-world graph datasets.
EVALUATION METRICS,0.4225352112676056,"Proximity
Structure
Mixed
Algorithms
Cora
Citeseer
Pubmed
Cornell
Texas
Wisconsin
Chameleon
Squirrel
Actor
DeepWalk
82.97 Â± 1.67
68.99 Â± 0.95
82.39 Â± 4.88
41.21 Â± 3.40
41.89 Â± 7.81
43.62 Â± 2.46
68.03 Â± 2.13
59.22 Â± 2.35
23.84 Â± 2.14
node2vec
81.93 Â± 1.43
64.56 Â± 1.65
81.02 Â± 1.48
40.54 Â± 1.62
48.64 Â± 2.92
36.27 Â± 2.08
65.67 Â± 2.31
48.29 Â± 1.67
24.14 Â± 1.02
RolX
29.70 Â± 2.89
20.90 Â± 0.72
39.85 Â± 2.33
25.67 Â± 11.78
42.56 Â± 7.13
24.92 Â± 13.43
22.75 Â± 2.12
20.50 Â± 1.18
25.42 Â± 0.55
struc2vec
41.46 Â± 1.49
51.70 Â± 0.67
81.49 Â± 0.33
23.72 Â± 13.69
47.29 Â± 7.21
24.59 Â± 12.14
60.63 Â± 2.90
52.59 Â± 0.69
25.13 Â± 0.79
GraphWave
28.83 Â± 2.39
20.79 Â± 1.59
20.96 Â± 2.35
45.96 Â± 2.20
37.45 Â± 7.09
39.24 Â± 5.16
17.59 Â± 3.42
25.69 Â± 0.53
27.29 Â± 3.09
GAE
72.06 Â± 2.54
57.10 Â± 1.62
73.24 Â± 0.88
45.40 Â± 9.99
58.78 Â± 3.41
34.11 Â± 8.06
22.03 Â± 1.09
29.34 Â± 1.12
28.63 Â± 1.05
VGAE
72.87 Â± 1.48
60.78 Â± 1.92
81.34 Â± 0.79
49.32 Â± 9.19
39.18 Â± 8.96
38.27 Â± 6.12
20.17 Â± 1.30
19.57 Â± 1.63
26.41 Â± 1.07
ARGVA
72.88 Â± 3.83
63.36 Â± 2.08
75.32 Â± 0.63
41.08 Â± 4.85
43.24 Â± 5.38
41.17 Â± 5.20
21.17 Â± 0.78
20.61 Â± 0.73
28.97 Â± 1.17
DGI
84.76 Â± 1.39
71.68 Â± 1.54
84.29 Â± 1.07
46.48 Â± 7.97
52.97 Â± 5.64
55.68 Â± 2.97
25.89 Â± 1.49
25.89 Â± 1.62
20.45 Â± 1.32
GraphCL
84.23 Â± 1.51
73.51 Â± 1.73
82.59 Â± 0.71
44.86 Â± 3.73
46.48 Â± 5.85
53.72 Â± 1.07
26.27 Â± 1.53
21.32 Â± 1.66
28.64 Â± 1.28
MVGRL
86.23 Â± 2.71
73.81 Â± 1.53
83.94 Â± 0.75
53.51 Â± 3.26
56.75 Â± 5.97
57.25 Â± 5.94
58.73 Â± 2.03
40.64 Â± 1.15
31.07 Â± 0.29
NWR-GAE w/o deg.
79.75 Â± 2.41
70.68 Â± 0.51
81.04 Â± 0.57
54.05 Â± 5.97
65.13 Â± 6.79
65.09 Â± 5.04
70.54 Â± 2.63
64.20 Â± 1.07
29.57 Â± 1.14
NWR-GAE w/o Hun.
82.38 Â± 2.11
68.95 Â± 1.79
80.30 Â± 0.36
50.81 Â± 4.44
67.59 Â± 8.67
62.35 Â± 4.88
70.98 Â± 2.05
62.63 Â± 2.27
29.47 Â± 1.32
NWR-GAE w/o dist.
80.67 Â± 2.49
69.78 Â± 2.42
81.13 Â± 0.27
49.72 Â± 5.04
65.94 Â± 6.53
68.12 Â± 4.41
71.64 Â± 1.85
62.07 Â± 1.41
28.59 Â± 0.96
NWR-GAE full
83.62 Â± 1.61
71.45 Â± 2.41
83.44 Â± 0.92
58.64 Â± 5.61
69.62 Â± 6.66
68.23 Â± 6.11
72.04 Â± 2.59
64.81 Â± 1.83
30.17 Â± 0.17"
EVALUATION METRICS,0.426056338028169,"For the experiments on real-world graph datasets, we learn the graph embeddings with different
algorithms and then follow the standard setting of supervised classiï¬cation. To be consistent across
all datasets in our experiments, we did not follow the standard semi-supervised setting (20 labels per
class for training) on Cora, Citeseer and Pubmed, but rather randomly split all datasets with 60%
training set, 20% validation set, and 20% testing set, which is a common practice on WebKB and
Wikipedia network datasets (i.e. Cornell, Texas, Chameleon, etc.) (Liu et al., 2020; Pei et al., 2020;
Ma et al., 2021). The overall performance is the average performance over 10 random splits. Due to
space limitation, we put details of the compared models and hyper-parameter settings in Appendix C."
EVALUATION METRICS,0.4295774647887324,"4.2
PERFORMANCE ON SYNTHETIC DATASETS (RQ1)"
EVALUATION METRICS,0.43309859154929575,"We compare NWR-GAE with all baselines on the synthetic datasets. As can be observed in Table 1,
NWR-GAE has the best performance over all compared algorithms regarding all evaluation metrics,
corroborating its claimed advantages in capturing the neighborhood structures of nodes on graphs."
EVALUATION METRICS,0.43661971830985913,"Speciï¬cally, (1) on the purely symmetrical graphs of House, NWR-GAE achieves almost perfect
predictions on par with RolX, GraphWave and GAE; (2) on the asymmetrical graphs of House
Perturbed (by randomly adding or deleting edges between nodes), Varied (by placing varied shapes
around the base circle such as Fans and Stars) and Varied perturbed, the performances of all
algorithms degrade signiï¬cantly, while NWR-GAE is most robust to such perturbation and variance;
(3) among the baselines, proximity oriented methods of DeepWalk and node2vec perform extremely
poorly due to the ignorance of structural roles, while structural role oriented methods of RolX,
strc2vec and GraphWave are much more competitive; (4) GNN-based baselines of GAE, VGAE and
DGI still enjoy certain predictive power, due to the structure representativeness of their GIN encoder,
but such representativeness is actually impeded by their improper loss functions for structural role
identiï¬cation, thus still leading to rather inferior performance."
EVALUATION METRICS,0.44014084507042256,"4.3
PERFORMANCE ON REAL-WORLD DATASETS (RQ2)"
EVALUATION METRICS,0.44366197183098594,"All 11 baselines are included in the real-world datasets performance comparison. As can be observed
in Table 2, NWR-GAE achieves signiï¬cantly better results compared to all baselines on all structure-
oriented and proximity-structure-mixed graph datasets, and maintains close performance to the best
baselines of DGI, GraphCL and MVGRL on all proximity-oriented graph datasets. Note that, most of
the strong baselines can only perform well on either of the proximity-oriented or structure-oriented
datasets, while NWR-GAE is the only one that performs consistently well on all three types. Such
results again indicate the superiority of NWR-GAE in capturing the structural information on graphs,
yet without signiï¬cantly sacriï¬cing the capturing of node proximity."
EVALUATION METRICS,0.4471830985915493,Published as a conference paper at ICLR 2022
EVALUATION METRICS,0.4507042253521127,"Speciï¬cally, (1) Most GNN-based methods (both auto-encoder based, and contrastive learning based),
like GAE, VGAE, ARGVA, DGI and GraphCL only perform well on graphs with proximity-oriented
labels due to their strongly proximity-preserving loss functions, and perform rather poorly on most
graphs with structure-oriented and mixed labels; MVGRL achieves relatively better but still inferior
performance to ours on structure oriented datasets due to its structure aware contrastive training. (2)
the random walk based methods of DeepWalk and node2vec perform well mostly on graphs with
proximity-oriented and mixed labels; (3) the structural role oriented methods of RolX, struc2vec
and GraphWave perform rather poorly on graphs with proximity-oriented and mixed labels, and
sometimes also the structure-oriented ones, due to their incapability of modeling node features."
EVALUATION METRICS,0.45422535211267606,"4.4
IN-DEPTH ANALYSIS OF NWR-GAE (RQ3)"
EVALUATION METRICS,0.45774647887323944,"In Table 2, we also compare several important ablations of NWR-GAE, i.e., the one without degree
predictor, the one without Hungarian loss (replaced by a greedy-matching based loss), and the one
without feature distribution reconstructor, which all lead to inferior performance on all datasets,
demonstrating the effectiveness of our novel model designs."
EVALUATION METRICS,0.4612676056338028,"In Figure 4, we present results on hyper-parameter and efï¬ciency studies of NWR-GAE. Embedding
size efï¬ciency is an important metric for unsupervised representation learning, which reï¬‚ects the
effectiveness in information compression. As can be seen in Figure 4 (a), NWR-GAE quickly converges
when the embedding size reaches 200-300 on the mid-scale datasets of Chameleon and Squirrel,
and maintains good performance as the sizes further increase without overï¬tting. Moreover, as we
can observe from the training curves of NWR-GAE on Chameleon in Figure 4 (b), the performance
of NWR-GAE is robust to the neighborhood sampling sizes, where the larger size of 15 only leads
to slightly better performance than the smaller size of 5. NWR-GAE also converges rapidly after
1-2 training epochs, achieving signiï¬cantly better performance compared with the best baseline of
DeepWalk and typical baselines like DGI. As we can observe from the runtimes of NWR-GAE on
Chameleon in Figure 4 (c), smaller neighborhood sampling sizes do lead to signiï¬cantly shorter
runtimes, while our runtimes are much better compared with typical baselines like DeepWalk and
DGI, especially under the fact that we can converge rather rapidly observed from Figure 4 (b)."
EVALUATION METRICS,0.4647887323943662,"Finally, to further understand the effectiveness of our proposed neighborhood Wasserstein reconstruc-
tion method, we provide detailed results on how well the generated neighbor features can approximate
the real ones in Appendix D. The results show that the method can indeed recover the entire real
neighbor features, and the approximation gets better as we use a larger sampling factor q especially
on larger datasets, while the performances on downstream tasks are already satisfactory even with
smaller q values, allowing the training to be rather efï¬cient."
EVALUATION METRICS,0.46830985915492956,"(a) Perf. vs. emb. sizes
(b) Train curves vs. samp. sizes
(c) Runtimes vs. samp. sizes"
EVALUATION METRICS,0.47183098591549294,Figure 4: Hyper-parameter and efï¬ciency studies of NWR-GAE
CONCLUSION,0.4753521126760563,"5
CONCLUSION"
CONCLUSION,0.4788732394366197,"In this work, we address the limitations of existing unsupervised graph representation methods and
propose the ï¬rst model that can properly capture the proximity, structure and feature information of
nodes in graphs, and discriminatively encode them in the low-dimensional embedding space. The
model is extensively tested on both synthetic and real-world benchmark datasets, and the results
strongly support its claimed advantages. Since it is general, efï¬cient, and also conceptually well-
understood, we believe it to have the potential to serve as the de facto method for unsupervised graph
representation learning. In the future, it will be promising to see its applications studied in different
domains, as well as its potential issues such as robustness and privacy carefully analyzed."
CONCLUSION,0.4823943661971831,Published as a conference paper at ICLR 2022
REFERENCES,0.4859154929577465,REFERENCES
REFERENCES,0.4894366197183099,"Syed Mumtaz Ali and Samuel D Silvey. A general class of coefï¬cients of divergence of one
distribution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28
(1):131â€“142, 1966."
REFERENCES,0.49295774647887325,"Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In ICML workshop on
unsupervised and transfer learning, 2012."
REFERENCES,0.4964788732394366,"Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In ICML
workshop on unsupervised and transfer learning, 2012."
REFERENCES,0.5,"Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In ICLR, 2021."
REFERENCES,0.5035211267605634,"Nicolas Courty, RÃ©mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. TPAMI, 39(9):1853â€“1865, 2016."
REFERENCES,0.5070422535211268,"Hejie Cui, Zijie Lu, Pan Li, and Carl Yang. On positional and structural node features for graph
neural networks on non-attributed graphs. In KDD-DLG, 2021."
REFERENCES,0.5105633802816901,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS, 2013."
REFERENCES,0.5140845070422535,"Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node embed-
dings via diffusion wavelets. In SIGKDD, 2018."
REFERENCES,0.5176056338028169,"Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised
pre-training help deep learning? In Proceedings of the thirteenth international conference on
artiï¬cial intelligence and statistics, 2010."
REFERENCES,0.5211267605633803,"Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object
reconstruction from a single image. In CVPR, 2017."
REFERENCES,0.5246478873239436,"Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso A Poggio.
Learning with a wasserstein loss. In NeurlPS, 2015."
REFERENCES,0.528169014084507,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, 2017."
REFERENCES,0.5316901408450704,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016."
REFERENCES,0.5352112676056338,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In SIGKDD,
2016."
REFERENCES,0.5387323943661971,"Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs.
In ICML, pp. 2434â€“2444. PMLR, 2019."
REFERENCES,0.5422535211267606,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. In NeurIPS, 2017."
REFERENCES,0.545774647887324,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
NeurlPS, 2017a."
REFERENCES,0.5492957746478874,"William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and
applications. IEEE Data Engineering Bulletin, 40(3), 2017b."
REFERENCES,0.5528169014084507,"Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In ICML, 2020."
REFERENCES,0.5563380281690141,"Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu,
Danai Koutra, Christos Faloutsos, and Lei Li. Rolx: structural role extraction & mining in large
graphs. In SIGKDD, 2012."
REFERENCES,0.5598591549295775,"Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313:504â€“507, 2006."
REFERENCES,0.5633802816901409,Published as a conference paper at ICLR 2022
REFERENCES,0.5669014084507042,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. ICLR, 2020."
REFERENCES,0.5704225352112676,"Roy Jonker and Anton Volgenant. A shortest augmenting path algorithm for dense and sparse linear
assignment problems. Computing, 38(4):325â€“340, 1987."
REFERENCES,0.573943661971831,"Diederik P Kingma and Max Welling. Auto-encoding variational bayess. In ICLR, 2014."
REFERENCES,0.5774647887323944,"Thomas N Kipf and Max Welling. Variational graph auto-encoders. NeurIPS Bayesian Deep Learning
Workshop, 2016."
REFERENCES,0.5809859154929577,"Thomas N Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional networks.
In ICLR, 2017."
REFERENCES,0.5845070422535211,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. In ICLR, 2019."
REFERENCES,0.5880281690140845,"Soheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. Sliced wasserstein auto-
encoders. In ICLR, 2018."
REFERENCES,0.5915492957746479,"Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. arXiv preprint
arXiv:2005.14612, 2020."
REFERENCES,0.5950704225352113,"Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for
expressing probability distributions. NeurIPS, 2020."
REFERENCES,0.5985915492957746,"Liheng Ma, Reihaneh Rabbany, and Adriana Romero-Soriano. Graph attention networks with
positional embeddings. In PAKDD (1), 2021."
REFERENCES,0.602112676056338,"Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active
surveying for collective classiï¬cation. In 10th International Workshop on Mining and Learning
with Graphs, 2012."
REFERENCES,0.6056338028169014,"S Pan, R Hu, G Long, J Jiang, L Yao, and C Zhang. Adversarially regularized graph autoencoder for
graph embedding. In IJCAI, 2018."
REFERENCES,0.6091549295774648,"Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In UAI, 2020."
REFERENCES,0.6126760563380281,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. ICLR, 2020."
REFERENCES,0.6161971830985915,"Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In SIGKDD, 2014."
REFERENCES,0.6197183098591549,"Gabriel PeyrÃ©, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and TrendsÂ® in Machine Learning, 11(5-6):355â€“607, 2019."
REFERENCES,0.6232394366197183,"Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk, line, pte, and node2vec. In WSDM, 2018."
REFERENCES,0.6267605633802817,"Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016."
REFERENCES,0.6302816901408451,"Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node
representations from structural identity. In SIGKDD, 2017."
REFERENCES,0.6338028169014085,"Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021."
REFERENCES,0.6373239436619719,"Han Shi, Haozheng Fan, and James T Kwok. Effective decoding in graph auto-encoder using triadic
closure. In AAAI, 2020."
REFERENCES,0.6408450704225352,"Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
supervised graph-level representation learning via mutual information maximization. In ICLR,
2020."
REFERENCES,0.6443661971830986,Published as a conference paper at ICLR 2022
REFERENCES,0.647887323943662,"Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve
graph contrastive learning. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.6514084507042254,"Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social inï¬‚uence analysis in large-scale networks. In
SIGKDD, 2009."
REFERENCES,0.6549295774647887,"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
In ICLR, 2018."
REFERENCES,0.6584507042253521,"Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. In ICLR, 2020."
REFERENCES,0.6619718309859155,"Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018a."
REFERENCES,0.6654929577464789,"Petar VeliË‡ckoviÂ´c, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In ICLR, 2018b."
REFERENCES,0.6690140845070423,"CÃ©dric Villani. Optimal transport: old and new, volume 338. 2008."
REFERENCES,0.6725352112676056,"Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,
Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.
Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv
preprint arXiv:1909.01315, 2019."
REFERENCES,0.676056338028169,"Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplify-
ing graph convolutional networks. In ICML, 2019."
REFERENCES,0.6795774647887324,"Yaochen Xie, Zhao Xu, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural
networks: A uniï¬ed review. arXiv preprint arXiv:2102.10757, 2021."
REFERENCES,0.6830985915492958,"Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph
partitioning and matching. NeurIPS, 2019a."
REFERENCES,0.6866197183098591,"Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning
for graph matching and node embedding. In ICML, 2019b."
REFERENCES,0.6901408450704225,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019c."
REFERENCES,0.6936619718309859,"Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, and Pan Li. Conditional structure generation
through graph variational generative adversarial nets. In NeurIPS, 2019."
REFERENCES,0.6971830985915493,"Carl Yang, Haonan Wang, Ke Zhang, Liang Chen, and Lichao Sun. Secure deep graph generation
with link differential privacy. In IJCAI, 2021."
REFERENCES,0.7007042253521126,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. NeurIPS, 2020."
REFERENCES,0.704225352112676,"Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020."
REFERENCES,0.7077464788732394,"Qi Zhu, Carl Yang, Yidan Xu, Haonan Wang, Chao Zhang, and Jiawei Han. Transfer learning of
graph neural networks with ego-graph information maximization. In NeurIPS, 2021."
REFERENCES,0.7112676056338029,Published as a conference paper at ICLR 2022
REFERENCES,0.7147887323943662,"A
PROOF OF THEOREM 3.1"
REFERENCES,0.7183098591549296,"Theorem 3.1 can be obtained by properly revising Theorem 2.1 in (Lu & Lu, 2020). We ï¬rst state
Theorem 2.1 in (Lu & Lu, 2020)."
REFERENCES,0.721830985915493,"Theorem A.1 (Theorem 2.1 in Lu & Lu (2020)). Let P and Q be the target and the source distri-
butions respectively, both deï¬ned on Rm. Assume that Exâˆ¼P âˆ¥xâˆ¥3 is bounded and Q is absolutely
continuous with respect to the Lebesgue measure. It holds that for any given approximation error Ïµ,
setting n = O( 1"
REFERENCES,0.7253521126760564,"Ïµm ), there is a fully connected and feed-forward deep neural network u(Â·) of depth
L = âŒˆlog2 nâŒ‰and width N = 2L, with d inputs and a single output and with ReLU activation such
that W1(P, âˆ‡u(Q)) < Ïµ. Here, âˆ‡u(Â·) is the function Rd â†’Rd induced by the gradient of u while
âˆ‡u(Q) is the distribution that is generated from the distribution Q through the mapping âˆ‡u(Â·)."
REFERENCES,0.7288732394366197,"To prove Theorem 3.1, we just need to verify the conditions in the above theorem. We need to show
that P = P(i)
v
has a bounded Exâˆ¼P âˆ¥xâˆ¥3, and Q as a non-degenerate m-dim Gaussian distribution
is absolutely continuous with respect to the Lebesgue measure. Also, the original statement is for
W1(Â·, Â·) while we use W2(Â·, Â·). We need to show the connection between W1(Â·, Â·) and W2(Â·, Â·).
Fortunately, all these aspects are easy to be veriï¬ed."
REFERENCES,0.7323943661971831,"P = P(i)
v
has a bounded 3-order moment because the support P(i)
v
are in a bounded space of Rm."
REFERENCES,0.7359154929577465,"Q as a Gaussian distribution is absolutely continuous with respect to the Lebesgue measure, as long
as Q is not degenerate."
REFERENCES,0.7394366197183099,"Lastly we show the connection between W1(Â·, Â·) and W2(Â·, Â·). The support P = P(i)
v
is bounded, i.e.,
Î´ âˆˆsupp(P), âˆ¥Î´âˆ¥2 â‰¤B. According to Lu & Lu (2020), the Q = âˆ‡u(G) also has bounded support.
Without loss of generality, Î´ âˆˆsupp(Q), âˆ¥Î´âˆ¥2 â‰¤B. Then, we may show that"
REFERENCES,0.7429577464788732,"W2
2(P, Q) =
inf
Î³âˆˆÎ“(P,Q) Z"
REFERENCES,0.7464788732394366,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥2
2dÎ³(Z, Zâ€²)
"
REFERENCES,0.75,"â‰¤2B
inf
Î³âˆˆÎ“(P,Q) Z"
REFERENCES,0.7535211267605634,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥2dÎ³(Z, Zâ€²)
"
REFERENCES,0.7570422535211268,"â‰¤2Bâˆšm
inf
Î³âˆˆÎ“(P,Q) Z"
REFERENCES,0.7605633802816901,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥1dÎ³(Z, Zâ€²)
"
REFERENCES,0.7640845070422535,"= 2BâˆšmW1(P, Q) < 2BâˆšmÏµ.
As B and m are just constant, so we may have W2
2(P, Q) = O(Ïµ). Note that the above ï¬rst inequality
is because âˆ¥Z âˆ’Zâ€²âˆ¥2 â‰¤âˆ¥Zâˆ¥2 + âˆ¥Zâ€²âˆ¥2 = 2B. The second inequality is because
âˆšm
inf
Î³âˆˆÎ“(P,Q) Z"
REFERENCES,0.7676056338028169,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥1dÎ³(Z, Zâ€²)
"
REFERENCES,0.7711267605633803,"= lim
iâ†’âˆ Z"
REFERENCES,0.7746478873239436,"ZÃ—Zâ€²
âˆšmâˆ¥Z âˆ’Zâ€²âˆ¥1dÎ³i(Z, Zâ€²)
"
REFERENCES,0.778169014084507,"(There exists a sequence of measures {Î³i}âˆ
i=1 achieving the inï¬mum)"
REFERENCES,0.7816901408450704,"â‰¥lim
iâ†’âˆ Z"
REFERENCES,0.7852112676056338,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥2dÎ³i(Z, Zâ€²)
"
REFERENCES,0.7887323943661971,"â‰¥
inf
Î³âˆˆÎ“(P,Q) Z"
REFERENCES,0.7922535211267606,"ZÃ—Zâ€² âˆ¥Z âˆ’Zâ€²âˆ¥2dÎ³(Z, Zâ€²)
"
REFERENCES,0.795774647887324,"B
ADDITIONAL DATASET DETAILS"
REFERENCES,0.7992957746478874,"In this section, we provide some additional important details about the synthetic and real-world
network datasets we use in the node classiï¬cation experiments."
REFERENCES,0.8028169014084507,"From table B, the node labels shows how the node are classiï¬ed. For example, the node labels of
citation networks are academic topics, which is highly related to proximity information. But the
identity roles of website is more structural oriented since similar identity have similar hyperlinks
structure. Detailed statistics of the real-world graph datasets are provided in Table 4."
REFERENCES,0.8063380281690141,Published as a conference paper at ICLR 2022
REFERENCES,0.8098591549295775,"Figure 5: Toy example of proximity and structure information: A and B are â€œclose in proximityâ€
since they are relatively close in terms of node distances in the global network, whereas A and C are
â€œclose in structureâ€ since they have relatively similar local neighborhood structures."
REFERENCES,0.8133802816901409,"(a) Houses on a Cycle
(b) Perturbed
(c) Varied (house, star, fan, etc.)"
REFERENCES,0.8169014084507042,"Figure 6: Synthetic dataset examples (Colors denote structural role labels and dashed lines denote
randomly adding or deleting edges)"
REFERENCES,0.8204225352112676,"C
DETAILED SETTINGS OF THE COMPARED MODELS"
REFERENCES,0.823943661971831,"DeepWalk: for all datasets, we ran 10 random walks of length 40 and input these walks into a
skipgram langauge model. We implement the DeepWalk method based on Python code (GPLv3
license) provided by Perozzi et al. (2014)."
REFERENCES,0.8274647887323944,"node2vec: similar to DeepWalk, we also ran 10 random walks of length 40 with p and q equal to
0.25. The implementation is provided by Grover & Leskovec (2016)."
REFERENCES,0.8309859154929577,"RolX: by using Henderson et al. (2012)â€™s code, we set the initial learning rate as 0.01 and ran the
algorithm with batch size 32."
REFERENCES,0.8345070422535211,"struc2vec: similar to DeepWalk and node2vec, we ran the same number of random walks with walk
length 40. We set window size 5 as context size for optimization. The reference implementation is
provided by Ribeiro et al. (2017)"
REFERENCES,0.8380281690140845,"GraphWave: the implementation is under the MIT license and provided by Stanford SNAP lab
(Donnat et al., 2018). All the parameters are automatically determined by the algorithm itself."
REFERENCES,0.8415492957746479,"GAE: we use a two layer GCN as graph encoder for node classiï¬cation task and a three layer GIN for
synthetic structural identiï¬cation task. The decoder hyperparameters are selected by cross-validation,
the implementation is based on the code provided by Kipf & Welling (2016)"
REFERENCES,0.8450704225352113,"VGAE: same encoder structure and similar decoder as GAE, the implementation is also based on
Kipf & Welling (2016)."
REFERENCES,0.8485915492957746,"ARGVA: same encoder structure as GAE and VGAE. We use the Tensorï¬‚ow implementation based
on the implementation of Pan et al. (2018)."
REFERENCES,0.852112676056338,"DGI: same encoder structure as GAE and VGAE, the initial number of epochs is 10000 but may
early stop when the algorithm is converged. We use the DGI method based on the implementation of
VeliË‡ckoviÂ´c et al. (2018b)."
REFERENCES,0.8556338028169014,Table 3: Real-world network datasets description
REFERENCES,0.8591549295774648,"Datasets
Type
Nodes
Edges
Node Features
Node Labels
Cora
Citeseer
Citation Networks
Papers
Citations
Bag-of-words of papers
Academic topics
Pubmed"
REFERENCES,0.8626760563380281,Cornell
REFERENCES,0.8661971830985915,"Texas
WebKB
Web pages
Hyperlinks
Bag-of-words of web pages
Identity roles (Student, Project, etc.)
Wisconsin
Chameleon"
REFERENCES,0.8697183098591549,"Squirrel
Wikipedia Network
Web pages
Mutual links
Informative nouns
Number of monthly trafï¬c"
REFERENCES,0.8732394366197183,Published as a conference paper at ICLR 2022
REFERENCES,0.8767605633802817,Table 4: Statistics of the real-world graph datasets.
REFERENCES,0.8802816901408451,"Task type
Proximity
Structure
Mixed
Dataset
Cora
Citeseer
Pubmed
Texas
Wisconsin
Cornell
Chameleon
Squirrel
Actor
Nodes
2708
3327
19717
183
183
251
2277
5201
7600
Edges
5429
4732
44338
295
309
499
36101
217073
33544
Features
1433
3703
500
1703
1703
1703
2325
2089
931
Classes
7
6
3
5
5
5
5
5
5"
REFERENCES,0.8838028169014085,"GraphCL: use the default encoder GraphSage and all the four different types of augumentation. The
implementation is based on You et al. (2020)."
REFERENCES,0.8873239436619719,"MVGRL: the same default GCN encoder as GAE, We use the PyTorch implementation of Hassani &
Khasahmadi (2020)."
REFERENCES,0.8908450704225352,"NWR-GAE: same encoder structure as the above GNN-based methods, learning rate, trade-off
weight between degree loss and neighborhood reconstruction loss and sample size q are tuned using
grid search."
REFERENCES,0.8943661971830986,"Encoder setting. We used the same GCN encoders for almost all baselines because our contribution
in this work is on the decoder and we wanted to be fair and focus on the comparison of different
decoders. Speciï¬cally, we use the default encoders in their original work for all baselines that
involve GNN encoders (GraphSAGE (Hamilton et al., 2017a) for GraphCL (You et al., 2020), GCN
(Kipf & Welling, 2017) for GAE (Kipf & Welling, 2016), VGAE (Kipf & Welling, 2016), DGI
(VeliË‡ckoviÂ´c et al., 2018b), MVGRL (You et al., 2020)). The GCN encoders all have the same two-
layer architecture, which is the same as we used for NWR-GAE. In our experiments, we have actually
tried to use other GNNs as the encoders for all methods including NWR-GAE and the baselines, but
found GCN to have the best performance. For example, in the following Table C, we provide the
results of GAE with the GIN (Xu et al., 2019c) and GraphSage (Hamilton et al., 2017a) encoders,
which are clearly worse than that with GCN."
REFERENCES,0.897887323943662,"Hyper-parameter tuning. For all compared models, we performed hyper-parameter selection on
learning rate {5e-3, 5e-4, 5e-5, 5e-6, 5e-7} and epoch size {100, 200, 300, 400, 500, 600}. For
NWR-GAE, we selected the sample size q from {3, 5, 8, 10}, and the trade-off weight parameters
Î»d, Î»s from {10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5}."
REFERENCES,0.9014084507042254,"Notes on fairness. We set all random walk based method (DeepWalk, node2vec, struc2vec) with
same number of random walks and walk lengths. For all graph neural network based methods (GAE,
VGAE, DGI, NWR-GAE), we use the same GCN encoder with same number of encoder layer from
DGL library (Wang et al., 2019). For the node classiï¬cation task, all methods use the same 4-layer
MLP predictor with learning rate 0.01. For all settings, we use Adam optimizer and backward
propagation from PyTorch Python package, and a ï¬x dimension size as same as the graph node
feature size. Most experiments are performed on a 8GB NVIDIA GeForce RTX 3070 GPU."
REFERENCES,0.9049295774647887,"Table 5: Selected baseline with different encoders
Algorithms
Cornell
Texas
Wisconsin
Chameleon
Squirrel
Actor
GAE w/GCN
45.40 Â± 9.99
58.78 Â± 3.41
34.11 Â± 8.06
22.03 Â± 1.09
29.34 Â± 1.12
28.63 Â± 1.05
GAE w/GIN
39.33 Â± 4.17
53.30 Â± 2.75
32.84 Â± 3.94
24.99 Â± 2.56
23.46 Â± 1.04
25.34 Â± 1.43
GAE w/GraphSage
44.97 Â± 4.61
47.29 Â± 5.43
37.25 Â± 6.25
22.97 Â± 1.29
20.05 Â± 0.34
28.34 Â± 0.51"
REFERENCES,0.9084507042253521,"Notes on neighborhood sampling. due to the heavy tailed nature of node distribution, we down
sample the number of neighbors in NWR-GAE method. We only sampled at most 10 neighbors per
node. This provides a good trade-off between efï¬ciency and performance, which makes the method
have reasonable running time on one GPU machine and also achieve great enough accuracy."
REFERENCES,0.9119718309859155,"D
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.9154929577464789,"To further understand the effectiveness of our proposed neighborhood Wasserstein reconstruction
method, we provide detailed results on how well the generated neighbor features can approximate the
real ones."
REFERENCES,0.9190140845070423,"Speciï¬cally, for every node v, to reconstruct k-hops neighborhood, the model will sample q neighbor
nodes from Nv in layer i, where 0 â‰¤i â‰¤k, thus {h(i)
vj |1 â‰¤j â‰¤q} denotes the q ground truth
embeddings of vâ€™s neighbor. Next the model gets q samples {Î¾j, 1 â‰¤j â‰¤q} from Gaussian"
REFERENCES,0.9225352112676056,Published as a conference paper at ICLR 2022
REFERENCES,0.926056338028169,"distribution parameterized by GNN encoder, and uses an FNN-based transformation {Ë†h(i,j)
v
=
FNN(i)(Î¾j)} to reconstruct h(i)
vj ."
REFERENCES,0.9295774647887324,"Based on above, we create x-y pairs with x ="
REFERENCES,0.9330985915492958,"Pq
j=1(||Ë†h(k,j)
v
||2) q
,"
REFERENCES,0.9366197183098591,"y =
minÏ€:[q]â†’[q]
Pkâˆ’1
i=0
Pq
j=1 ||h(i)
vj âˆ’Ë†h(i,Ï€(j))
v
||2"
REFERENCES,0.9401408450704225,"q Â· k
."
REFERENCES,0.9436619718309859,Table 6: Box-plot like approximation power table.
REFERENCES,0.9471830985915493,"Dataset
5%
25%
50%
75%
95%
q"
REFERENCES,0.9507042253521126,"Cornell
0.0086
0.0336
0.0618
0.0822
0.1095
5
0.0089
0.0344
0.0617
0.0813
0.1130
15
0.0027
0.0398
0.0848
0.1204
0.1436
30"
REFERENCES,0.954225352112676,"Texas
0.0025
0.0477
0.0869
0.1304
0.1792
5
0.0078
0.0559
0.0897
0.1445
0.1828
15
0.0077
0.0285
0.0489
0.0869
0.1048
30"
REFERENCES,0.9577464788732394,"Wisconsin
0.0060
0.0472
0.0723
0.0966
0.1451
5
0.0364
0.0526
0.0671
0.0848
0.1080
15
0.0124
0.0529
0.0716
0.0946
0.1398
30"
REFERENCES,0.9612676056338029,"Chameleon
0.1590
0.2587
0.3183
0.3858
0.5067
5
0.0958
0.1180
0.1504
0.2120
0.2704
15
0.0423
0.0624
0.0884
0.1384
0.2272
30"
REFERENCES,0.9647887323943662,"In Table 6, we ranked all x-y pairs by the ratio , and presented the ratio at 5%, 25%, 50%, 75%, 95%
(like a box-plot). As we can observe, our model can get a reasonably good approximation to the
ground-truth neighborhood information (the smaller the better approximation). We tried different
sample sizes q such as 5, 15, 30, which do not make a large difference in the ï¬rst three datasets
since most nodes in the graphs have rather small numbers of neighbors. Our model shows better
approximation power on the neighborhood information with larger sample sizes such as Chameleon
with denser links. However, the performances on downstream tasks are already satisfactory even
with smaller q values (the results we show in Table 2 are with q = 5, and the results we show in
Figure 4 (b) are with q = 5, 10, 15). This indicates that while our model has the ability to more
ideally approximate the entire neighborhood information, it often may not be necessary to guarantee
satisfactory performance in downstream tasks, which allows us to train the model with small sample
sizes and achieve good efï¬ciency."
REFERENCES,0.9683098591549296,"To better understand the effect of number of GNN layers k in our proposed model, we vary k from 1
to 5 on the synthetic dataset of â€œHouseâ€. The model performance only gets better when k grows over
3, because the important graph structures there cannot be captured by GNNs with less than 3 layers.
Motivated by such observations and understandings, we set k to 4 for all real-world experiments,
which can empirically capture most important local graph structures, to achieve a good trade-off
between model effectiveness and efï¬ciency."
REFERENCES,0.971830985915493,Table 7: The effect of k (exempliï¬ed on the synthetic dataset of â€œHouseâ€).
REFERENCES,0.9753521126760564,"Metrics
k=1
k=2
k=3
k=4
k=5
Homogeneity
0.89
0.94
0.99
1.0
1.0
Completeness
0.92
0.93
1.0
1.0
1.0
Silhouette
0.88
0.87
0.91
0.99
0.98"
REFERENCES,0.9788732394366197,"To gain more insight into the behavior of different unsupervised graph embedding methods, we
visualize the embedding space learned by several representative methods on Chameleon in Figure 7
(reduced to two-dimensional via PCA). As we can observe, in these unsupervised two-dimensional
embedding spaces, our baselines of node2vec, struc2vec, GraphWave, GAE and DGI can hardly
distinguish the actual node classes, whereas NWR-GAE can achieve relatively better class separation.
Note that since all algorithms are unsupervised and the visualized embedding space is very low-
dimensional, it is hard for any algorithm to achieve perfectly clear class separation, but NWR-GAE"
REFERENCES,0.9823943661971831,Published as a conference paper at ICLR 2022
REFERENCES,0.9859154929577465,"can capture more discriminative information and deliver less uniform node distribution, which is
more useful for the node classiï¬cation task as consistent with our results in Table 2."
REFERENCES,0.9894366197183099,"(a) node2vec
(b) struc2vec
(c) GraphWave"
REFERENCES,0.9929577464788732,"(d) GAE
(e) DGI
(f) NWR-GAE"
REFERENCES,0.9964788732394366,Figure 7: Embedding spaces reduced to two-dimensional through PCA on Chameleon dataset.
