Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009852216748768472,"Recently, many studies have demonstrated that exclusively incorporating OCR-
1"
ABSTRACT,0.0019704433497536944,"derived text and spatial layouts with large language models (LLMs) can be highly
2"
ABSTRACT,0.002955665024630542,"effective for document understanding tasks. However, existing methods that in-
3"
ABSTRACT,0.003940886699507389,"tegrate spatial layouts with text have limitations, such as producing overly long
4"
ABSTRACT,0.0049261083743842365,"text sequences or failing to fully leverage the autoregressive traits of LLMs. In
5"
ABSTRACT,0.005911330049261084,"this work, we introduce Interleaving Layout and Text in a Large Language Model
6"
ABSTRACT,0.006896551724137931,"(LayTextLLM) for document understanding. In particular, LayTextLLM projects
7"
ABSTRACT,0.007881773399014778,"each bounding box to a single embedding and interleaves it with text, efficiently
8"
ABSTRACT,0.008866995073891626,"avoiding long sequence issues while leveraging autoregressive traits of LLMs.
9"
ABSTRACT,0.009852216748768473,"LayTextLLM not only streamlines the interaction of layout and textual data but
10"
ABSTRACT,0.01083743842364532,"also shows enhanced performance in Key Information Extraction (KIE) and Visual
11"
ABSTRACT,0.011822660098522168,"Question Answering (VQA). Comprehensive benchmark evaluations reveal signifi-
12"
ABSTRACT,0.012807881773399015,"cant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks
13"
ABSTRACT,0.013793103448275862,"compared to previous state-of-the-art document understanding MLLMs, as well as
14"
ABSTRACT,0.014778325123152709,"a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.
15"
INTRODUCTION,0.015763546798029555,"1
Introduction
16"
INTRODUCTION,0.016748768472906402,"Recent research has increasingly focused on applying Large Language Models (LLMs) [1–17] to
17"
INTRODUCTION,0.017733990147783252,"document-oriented Visual Question Answering (VQA) and Key Information Extraction (KIE) scenar-
18"
INTRODUCTION,0.0187192118226601,"ios. Efforts to build a text-sensitive MultiModal Large Language Models (MLLMs) based on existing
19"
INTRODUCTION,0.019704433497536946,"LLMs, particularly aimed at enhancing Visually Rich Document Understanding (VRDU), have made
20"
INTRODUCTION,0.020689655172413793,"significant progress [6, 12, 18]. Although existing MLLMs show promising results in document
21"
INTRODUCTION,0.02167487684729064,"understanding, they often encounter challenges related to image resolution. When the input image is
22"
INTRODUCTION,0.022660098522167486,"of low resolution, it is too blurry to extract visual features effectively. Conversely, high-resolution
23"
INTRODUCTION,0.023645320197044337,"images require additional computational resources to capture detailed textual information [12].
24"
INTRODUCTION,0.024630541871921183,"Concurrently, another line of research employs off-the-shelf OCR tools to extract text and spatial
25"
INTRODUCTION,0.02561576354679803,"layouts, which are then combined with LLMs to address VRDU tasks. These approaches assume that
26"
INTRODUCTION,0.026600985221674877,"most valuable information for document comprehension can be derived from the text and its spatial
27"
INTRODUCTION,0.027586206896551724,"layouts, viewing spatial layouts as “lightweight visual information” [19]. Following this premise,
28"
INTRODUCTION,0.02857142857142857,"several studies [12, 20–23] have explored various approaches that integrate spatial layouts with text
29"
INTRODUCTION,0.029556650246305417,"for LLMs, achieving results that are competitive with, or even surpass, those of MLLMs.
30"
INTRODUCTION,0.030541871921182268,"The most natural method to incorporate layout information is by treating spatial layouts as tokens,
31"
INTRODUCTION,0.03152709359605911,"which allows for the seamless interleaving of text and layout into a unified text sequence [20, 22, 23].
32"
INTRODUCTION,0.03251231527093596,"For example, Perot et al. [20] employ format such as “HARRISBURG 78|09” to represent OCR text
33"
INTRODUCTION,0.033497536945812804,"and corresponding layout, where “HARRISBURG” is OCR text and “78|09” indicates the mean of
34"
INTRODUCTION,0.034482758620689655,"the horizontal and vertical coordinates, respectively. Similarly, He et al. [23] use “[x_min, y_min,
35"
INTRODUCTION,0.035467980295566505,"x_max, y_max]” to represent layout information. These approaches can effectively take advantage of
36"
INTRODUCTION,0.03645320197044335,"autoregressive characteristics of LLMs and is known as the “coordinate-as-tokens” scheme [20]. In
37"
INTRODUCTION,0.0374384236453202,"contrast, DocLLM [19] explores interacting spatial layouts with text through a disentangled spatial
38"
INTRODUCTION,0.03842364532019704,"attention mechanism that captures cross-alignment between text and layout modalities.
39 103"
INTRODUCTION,0.03940886699507389,Sequence Length (log scale) 20 40 60 80 100
INTRODUCTION,0.04039408866995074,Performance CORD FUNSD SROIE
INTRODUCTION,0.041379310344827586,DocVQA CORD FUNSD SROIE
INTRODUCTION,0.042364532019704436,DocVQA CORD FUNSD SROIE
INTRODUCTION,0.04334975369458128,DocVQA CORD SROIE
INTRODUCTION,0.04433497536945813,"LayTextLLM-7B(ours)
DocLLM-7B
Coord-as-tokens-7B(Llama2)
Coord-as-tokens-ICL-175B(Davinci-003)"
INTRODUCTION,0.04532019704433497,"Figure 1: The performance against input sequence
length of different datasets across various OCR-
based methods where data is from Tab. 2 and 5."
INTRODUCTION,0.04630541871921182,"However, we argue that both of the previous ap-
40"
INTRODUCTION,0.04729064039408867,"proaches have limitations. As shown in Fig. 1,
41"
INTRODUCTION,0.04827586206896552,"coordinate-as-tokens significantly increases the
42"
INTRODUCTION,0.04926108374384237,"number of tokens. Additionally, to accurately
43"
INTRODUCTION,0.05024630541871921,"comprehend coordinates and enhance zero-shot
44"
INTRODUCTION,0.05123152709359606,"capabilities, this scheme often requires few-
45"
INTRODUCTION,0.052216748768472904,"shot in-context demonstrations and large-scale
46"
INTRODUCTION,0.053201970443349754,"language models, such as ChatGPT Davinci-
47"
INTRODUCTION,0.054187192118226604,"003 (175B) [23], which exacerbates issues re-
48"
INTRODUCTION,0.05517241379310345,"lated to sequence length and GPU resource de-
49"
INTRODUCTION,0.0561576354679803,"mands. Meanwhile, although DocLLM does not
50"
INTRODUCTION,0.05714285714285714,"increase sequence length and integrates spatial
51"
INTRODUCTION,0.05812807881773399,"layouts through attention, its generalizability is
52"
INTRODUCTION,0.059113300492610835,"limited. We believe that spatial cross attention
53"
INTRODUCTION,0.060098522167487685,"and masked span tasks in DocLLM cannot fully
54"
INTRODUCTION,0.061083743842364535,"utilize the autoregressive traits of LLMs.
55"
INTRODUCTION,0.06206896551724138,"To address these problems, this paper explores a
56"
INTRODUCTION,0.06305418719211822,"simple yet effective approach to enhance the in-
57"
INTRODUCTION,0.06403940886699508,"teraction between spatial layouts and text — In-
58"
INTRODUCTION,0.06502463054187192,"terleaving Layout and Text in a Large Language
59"
INTRODUCTION,0.06600985221674877,"Model (LayTextLLM) for document understand-
60"
INTRODUCTION,0.06699507389162561,"ing. Adhering to the common practice of inter-
61"
INTRODUCTION,0.06798029556650247,"leaving any modality with text [15, 24, 25], we
62"
INTRODUCTION,0.06896551724137931,"specifically apply this principle to spatial lay-
63"
INTRODUCTION,0.06995073891625615,"outs. In particular, we maps each bounding box
64"
INTRODUCTION,0.07093596059113301,"to a single embedding, which is then interleaved with its corresponding text. Then we propose a
65"
INTRODUCTION,0.07192118226600985,"tailored pre-training task—Layout-aware Next Token Prediction—a completely self-supervised task
66"
INTRODUCTION,0.0729064039408867,"that enhances the alignment between layout and textual modalities without using synthetic data.
67"
INTRODUCTION,0.07389162561576355,"Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly
68"
INTRODUCTION,0.0748768472906404,"improves performance on downstream document-related VQA and KIE tasks. As shown in Fig. 1,
69"
INTRODUCTION,0.07586206896551724,"LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even
70"
INTRODUCTION,0.07684729064039408,"reducing the sequence length compared to DocLLM. Our contributions can be listed as follows:
71"
INTRODUCTION,0.07783251231527094,"• We propose LayTextLLM for document understanding. To the best of the authors’ knowl-
72"
INTRODUCTION,0.07881773399014778,"edge, this is the first work to employ a unified embedding approach (Sec. 3.1.1) that
73"
INTRODUCTION,0.07980295566502463,"interleaves spatial layouts directly with textual data within a LLM. By representing each
74"
INTRODUCTION,0.08078817733990148,"bounding box with one token, LayTextLLM efficiently addresses sequence length issues
75"
INTRODUCTION,0.08177339901477833,"brought by coordiante-as-tokens while fully leveraging autoregressive traits for enhanced
76"
INTRODUCTION,0.08275862068965517,"document understanding.
77"
INTRODUCTION,0.08374384236453201,"• We propose two tailored training tasks: (1) Layout-aware Next Token Prediction (Sec. 3.2.1),
78"
INTRODUCTION,0.08472906403940887,"a completely self-supervised training task to enhance the alignment between layout and
79"
INTRODUCTION,0.08571428571428572,"textual modality; (2) Shuffled-OCR Supervised Fine-tuning task (Sec. 3.2.2) to better elicit
80"
INTRODUCTION,0.08669950738916256,"the model generalizability in downstream tasks.
81"
INTRODUCTION,0.08768472906403942,"• Comprehensive experimental results demonstrate quantitatively that LayTextLLM signifi-
82"
INTRODUCTION,0.08866995073891626,"cantly outperforms previous state-of-the-art (SOTA) OCR-free MLLMs by a large margin in
83"
INTRODUCTION,0.0896551724137931,"zero-shot scenarios, particularly in KIE tasks with an improvement of 27.0%. Additionally,
84"
INTRODUCTION,0.09064039408866995,"we illustrate that LayTextLLM competes effectively or even surpasses previous SOTA OCR-
85"
INTRODUCTION,0.0916256157635468,"based methods in both zero-shot and SFT scenarios. Specifically, it surpasses DocLLM by
86"
INTRODUCTION,0.09261083743842365,"19.8% on VQA and 15.5% on KIE tasks (Sec. 4).
87"
INTRODUCTION,0.09359605911330049,"• Extensive ablations demonstrate the utility of the proposed component, with analysis show-
88"
INTRODUCTION,0.09458128078817735,"ing that LayTextLLM not only improves performance but also reduces input sequence length
89"
INTRODUCTION,0.09556650246305419,"compared to current OCR-based models.
90"
RELATED WORK,0.09655172413793103,"2
Related Work
91"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.09753694581280788,"2.1
OCR-based LLMs for Document Understanding
92"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.09852216748768473,"Early document understanding methods [26–30] tend to solve the task in a two-stage manner, i.e., first
93"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.09950738916256158,"reading texts from input document images using off-the-shelf OCR engines and then understanding
94"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10049261083743842,"the extracted texts. Considering the advantages of LLMs (e.g., high generalizability), some recent
95"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10147783251231528,"methods endeavor to combine LLMs with OCR-derived results to solve document understanding.
96"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10246305418719212,"For example, inspired by the “coordinate-as-tokens” scheme [20], He et al. [23] propose to use
97"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10344827586206896,"“[x_min, y_min, x_max, y_max]” to introduce the layout information, which can fuse the layout
98"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10443349753694581,"information and texts into a unified text sequence and fully exploit the autoregressive merit of LLMs.
99"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10541871921182266,"To reinforce the layout information while avoiding increasing the number of tokens, DocLLM [19]
100"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10640394088669951,"designs a disentangled spatial attention mechanism to capture cross-alignment between text and
101"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10738916256157635,"layout modalities. Recently, LayoutLLM [21] utilizes the pre-trained layout-aware model [31], to
102"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10837438423645321,"insert the visual information, layout information and text information. However, the aforementioned
103"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.10935960591133005,"methods neither suffer from the computational overhead leading by the increasing tokens or hardly
104"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.1103448275862069,"take advantage of autoregressive characteristics of LLMs. Thus, it is an urgent problem to address
105"
OCR-BASED LLMS FOR DOCUMENT UNDERSTANDING,0.11133004926108374,"how to better incorporate layout information without significantly increasing the number of tokens.
106"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.1123152709359606,"2.2
OCR-free MLLMs for Document Understanding
107"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11330049261083744,"Another approach to solve document understanding tasks is the OCR-free method. Benefiting from
108"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11428571428571428,"the end-to-end training framework, it involves processing the text content of documents directly,
109"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11527093596059114,"without relying on OCR engines. Donut [32] first presents an OCR-free method through mapping
110"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11625615763546798,"a text-rich document image into the desired answers. Pix2Struct [33] is trained to parse masked
111"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11724137931034483,"screenshots of web pages into simplified HTML, where variable resolution inputs are supported.
112"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11822660098522167,"While these approaches eliminate the need for OCR tools, they still necessitate task-specific fine-
113"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.11921182266009853,"tuning. With the increasing popularity of LLMs/MLLMs [10–17], various methods are proposed to
114"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12019704433497537,"solve the document understanding task through explicitly training models on visual text understanding
115"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12118226600985221,"datasets and fine-tuning them with instructions to perform a zero-shot prediction. LLaVAR [34]
116"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12216748768472907,"and UniDoc [10] are notable examples that expand upon the document-oriented VQA capabilities
117"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12315270935960591,"of LLaVA [35] by incorporating document-based tasks. These models pioneer the use of MLLMs
118"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12413793103448276,"for predicting texts and coordinates from document images, enabling the development of OCR-
119"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12512315270935961,"free document understanding methods. Additionally, DocPedia [9] operates document images in
120"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12610837438423644,"the frequency domain, allowing for higher input resolution without increasing the input sequence
121"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.1270935960591133,"length. Recent advancements in this field, including mPLUG-DocOwl [18], Qwen-VL [6], and
122"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.12807881773399016,"TextMonkey [12], leverage publicly available document-related VQA datasets to further enhance
123"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.129064039408867,"the document understanding capability. Although these OCR-free methods have exhibited their
124"
OCR-FREE MLLMS FOR DOCUMENT UNDERSTANDING,0.13004926108374384,"advantages, they still struggle with the high-resolution input to reserve more text-related details.
125"
METHOD,0.1310344827586207,"3
Method
126"
METHOD,0.13201970443349753,"In this section, we present our LayTextLLM. First, we introduce a innovative Spatial Layout Projector
127"
METHOD,0.1330049261083744,"(Sec. 3.1.1) converts four-dimensional layout coordinates into a single-token embedding. To reduce
128"
METHOD,0.13399014778325122,"parameter overhead, we apply Partial Low-Rank Adaptation (Sec. 3.1.2). We also introduce two
129"
METHOD,0.13497536945812807,"specific training tasks: Layout-aware Next Token Prediction (Sec. 3.2.1) to align layouts with
130"
METHOD,0.13596059113300493,"text during pre-training, and Shuffled-OCR Supervised Fine-tuning (Sec. 3.2.2) to enhance the
131"
METHOD,0.13694581280788176,"generalizability of the model. An illustration of our approach is shown in Fig. 2.
132"
MODEL ARCHITECTURE,0.13793103448275862,"3.1
Model Architecture
133"
MODEL ARCHITECTURE,0.13891625615763548,"LayTextLLM is built on the Llama2-7B-base model, which was originally designed to accept only
134"
MODEL ARCHITECTURE,0.1399014778325123,"text inputs [36, 37]. To enable the model to interleave spatial layouts with text, we introduce a
135"
MODEL ARCHITECTURE,0.14088669950738916,"novel Spatial Layout Projector. This projector converts OCR-derived coordinates into bounding box
136"
MODEL ARCHITECTURE,0.14187192118226602,"tokens. We also adopt the Partial Low-Rank Adaptation, a minimally invasive method to incorporate
137"
MODEL ARCHITECTURE,0.14285714285714285,"additional modalities while preserving the LLM’s inherent knowledge intact.
138"
MODEL ARCHITECTURE,0.1438423645320197,Large Language Model
MODEL ARCHITECTURE,0.14482758620689656,OCR-Engine Pre-processing
MODEL ARCHITECTURE,0.1458128078817734,"Q:""What is the
title of this paper?"""
MODEL ARCHITECTURE,0.14679802955665025,"""A Bounding Box....""
""Document Understanding"""
MODEL ARCHITECTURE,0.1477832512315271,"A:""A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large"
MODEL ARCHITECTURE,0.14876847290640394,"Language Model for Document Understanding"""
MODEL ARCHITECTURE,0.1497536945812808,". . .
. . . 
. . . SLP . . ."
MODEL ARCHITECTURE,0.15073891625615762,"Tokenizer
. . . 
Tokenizer
SLP"
MODEL ARCHITECTURE,0.15172413793103448,". . . 
. . . . . ."
MODEL ARCHITECTURE,0.15270935960591134,Text Token
MODEL ARCHITECTURE,0.15369458128078817,Bbox Token
MODEL ARCHITECTURE,0.15467980295566502,Legend
MODEL ARCHITECTURE,0.15566502463054188,"Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (bi) with text
tokens (ti), where the superscripts represent the sequence positions of the tokens."
MODEL ARCHITECTURE,0.1566502463054187,"3.1.1
Spatial Layout Projector (SLP)
139"
MODEL ARCHITECTURE,0.15763546798029557,"A key innovation in LayTextLLM is the Spatial Layout Projector (SLP), which transforms a spatial
140"
MODEL ARCHITECTURE,0.15862068965517243,"layout into a singular bounding box token. This enhancement enables the model to process both
141"
MODEL ARCHITECTURE,0.15960591133004925,"spatial layouts and textual inputs simultaneously. To be specifically, each OCR-derived spatial layout
142"
MODEL ARCHITECTURE,0.1605911330049261,"is represented by a bounding box defined by four-dimensional coordinates [x1, y1, x2, y2], these
143"
MODEL ARCHITECTURE,0.16157635467980297,"coordinates represent the normalized minimum and maximum horizontal and vertical extents of the
144"
MODEL ARCHITECTURE,0.1625615763546798,"box, respectively. The SLP maps these coordinates into a high-dimensional space that the language
145"
MODEL ARCHITECTURE,0.16354679802955666,"model can process as a single token. The process can be computed as z = W · c + b, where c ∈R4
146"
MODEL ARCHITECTURE,0.16453201970443349,"is the vector of the bounding box coordinates. W ∈Rd×4 is a weight matrix with d represents
147"
MODEL ARCHITECTURE,0.16551724137931034,"the dimension of the embedding, b ∈Rd×1 is a bias vector, z is the resulting bounding box token
148"
MODEL ARCHITECTURE,0.1665024630541872,"represented as an d-dimensional embedding. As illustrated in Fig. 2, the resulting bounding box
149"
MODEL ARCHITECTURE,0.16748768472906403,"token z will be interleaved with corresponding textual embeddings to put into LLMs. Note that the
150"
MODEL ARCHITECTURE,0.1684729064039409,"SLP is shared by all bounding box tokens so very limited number of parameters are introduced.
151"
MODEL ARCHITECTURE,0.16945812807881774,"Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single
152"
MODEL ARCHITECTURE,0.17044334975369457,"token. This approach significantly reduces the number of input tokens and adheres to the practice
153"
MODEL ARCHITECTURE,0.17142857142857143,"of interleaving any modality with text, effectively integrating layout and textual information into a
154"
MODEL ARCHITECTURE,0.1724137931034483,"unified sequence. This allows the model to process both modalities simultaneously and coherently,
155"
MODEL ARCHITECTURE,0.17339901477832512,"fully leveraging the autoregressive traits of LLMs.
156"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.17438423645320197,"3.1.2
Layout Partial Low-Rank Adaptation
157"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.17536945812807883,Pre-trained
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.17635467980295566,Weights
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.17733990147783252,"Bbox Token
Text Token"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.17832512315270935,"Figure 3: The illustration of P-LoRA,
adapted from [15]."
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1793103448275862,"After using the SLP to generate bounding box tokens and
158"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18029556650246306,"a tokenizer to produce text tokens, these two modalities
159"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1812807881773399,"are then communicated using a Layout Partial Low-Rank
160"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18226600985221675,"Adaptation (P-LoRA) module in LLMs. P-LoRA, introduced
161"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1832512315270936,"in InternLM-XComposer2 [15], is originally used to adapt
162"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18423645320197043,"LLMs to visual modality. It applies plug-in low-rank modules
163"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1852216748768473,"specified to the visual tokens, which adds minimal parameters
164"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18620689655172415,"while preserving the LLMs inherent knowledge.
165"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18719211822660098,"Formally, as shown in Fig. 3 for a linear layer in the LLM,
166"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.18817733990147784,"the original weights WO ∈RCout×Cin and bias BO ∈RCout
167"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1891625615763547,"are specified for input and output dimensions Cin and Cout.
168"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19014778325123152,"P-LoRA modifies this setup by incorporating two additional
169"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19113300492610838,"matrices, WA ∈RCr×Cin and WB ∈RCout×Cr. These
170"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1921182266009852,"matrices are lower-rank, with Cr being considerably smaller
171"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19310344827586207,"than both Cin and Cout, and are specifically designed to
172"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19408866995073892,"interact with new modality tokens, which in our case are
173"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19507389162561575,"bounding box tokens. For example, given an input x =
174"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1960591133004926,"LLM
LLM"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19704433497536947,"<s>
..."
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.1980295566502463,"</s>
...
</s>
..."
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.19901477832512315,"<s>
..."
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.2,"(b) Layout-aware Next Token Prediction
(a) Next Token Prediction"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20098522167487684,Text Token
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.2019704433497537,Bbox Token
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20295566502463055,Bbox Token
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20394088669950738,w/o Supervision
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20492610837438424,Figure 4: Comparison of Layout-aware Next Token Prediction and normal Next Token Prediction.
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20591133004926107,"[xb, xt] comprising of bounding box tokens (xb) and textual tokens (xt) is fed into the system, the
175"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20689655172413793,"forward process is as follows, where ˆxt, ˆxb and ˆx are outputs:
176"
LAYOUT PARTIAL LOW-RANK ADAPTATION,0.20788177339901479,"ˆxt = W0xt + B0
ˆxb = W0xb + WBWAxb + B0
ˆx = [ˆxb, ˆxt]
(1)"
TRAINING PROCEDURE,0.20886699507389161,"3.2
Training Procedure
177"
TRAINING PROCEDURE,0.20985221674876847,"LayTextLLM is trained with innovative layout-aware training procedure, which consists of two stages:
178"
TRAINING PROCEDURE,0.21083743842364533,"Layout-aware Next Token Prediction pre-training and Shuffled-OCR Supervised Fine-tuning.
179"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21182266009852216,"3.2.1
Layout-aware Next Token Prediction
180"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21280788177339902,"Inspired by the next token prediction commonly used in current LLM pre-training [1–7], we propose
181"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21379310344827587,"the Layout-aware Next Token Prediction (LNTP). Fig. 4 presents the contrast of the proposed Layout-
182"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2147783251231527,"aware Next Token Prediction and the conventional next token prediction task. The traditional next
183"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21576354679802956,"token prediction (Fig. 4(a)) relies solely on the textual content, predicting each subsequent token
184"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21674876847290642,"based on the prior sequence of tokens without considering their spatial layouts. Layout-aware next
185"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21773399014778325,"token prediction (Fig. 4(b)), however, interleaves the spatial information encoded by SLP (i.e., bi)
186"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2187192118226601,"with the text tokens (i.e., ti). This integration considers both the content and its layout within the
187"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.21970443349753693,"document, leading to a richer, more precise understanding of both the structure and the content.
188"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2206896551724138,Figure 5: Receipt layout example.
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22167487684729065,"Similarly, primary objective of LNTP is to maximize the
189"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22266009852216748,"likelihood of its predictions for the next token. Thus the
190"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22364532019704433,"loss function is defined as
191"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2246305418719212,"L = −1 T T
X"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22561576354679802,"i=1
log P
 
ti | t1, t2, . . . , ti−1
(2)"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22660098522167488,"where P
 
ti | t1, t2, . . . , ti−1
represents the probability
192"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22758620689655173,"of ith token ti given the sequence of preceding tokens
193"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22857142857142856,"t1, t2, . . . , ti−1, as predicted by the model. Note that we
194"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.22955665024630542,"compute the loss only for text tokens, excluding bounding
195"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.23054187192118228,"box tokens. During pre-training, our goal is to enhance the
196"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2315270935960591,"alignment between spatial layouts and textual modality,
197"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.23251231527093597,"while preserving the LLM’s inherent knowledge as much
198"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2334975369458128,"as possible. Thus, we freeze the LLMs and only update
199"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.23448275862068965,"the parameters of SLP and P-LoRA.
200"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2354679802955665,"It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-
201"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.23645320197044334,"supervised pre-training procedure, unlike previous works that require human annotations of document
202"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.2374384236453202,"structure data or synthetic data generated by larger LLMs such as GPT-4 [21]. Thus, LNTP facilitates
203"
LAYOUT-AWARE NEXT TOKEN PREDICTION,0.23842364532019705,"the creation of large-scale, high-fidelity pre-training datasets at minimal cost.
204"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.23940886699507388,"3.2.2
Shuffled-OCR Supervised Fine-tuning
205"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24039408866995074,"OCR engines typically process text from top to bottom and left to right. This order is also adopted as
206"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2413793103448276,"the input sequence for current OCR-based LLMs [19, 21]. However, modern LLMs often exhibit
207"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24236453201970443,"a strong inductive bias toward the positions of input tokens, influenced by designs such as Rotary
208"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24334975369458128,"Position Embeddings (RoPE) [38]. Specifically, tokens that are close together in the input sequence
209"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24433497536945814,"are likely to receive higher attention scores, which is advantageous for processing standard text
210"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24532019704433497,"sequences. Such inductive bias brings cons and pros.
211"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24630541871921183,"Consider the example illustrated in Fig. 5, where the OCR input text reads: “ ... Change, 1.30, GST%,
212"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24729064039408866,"Amt(RM), GST(RM), Total(RM), SR, 6, 17.64, 1.06, 18.70 ... ”. If the question posed is “What is the
213"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2482758620689655,"value of the field Change?” (highlighted in a blue box), the model easily identifies “1.30” as it is
214"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.24926108374384237,"closely positioned to the word “Change” in the sequence. However, for a more challenging query like
215"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.25024630541871923,"“What is the value of the field Total(RM)?” (highlighted in a red box), the model struggles to determine
216"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2512315270935961,"the correct answer due to the presence of multiple subsequent numbers closed to “Total(RM)”.
217"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2522167487684729,"LayTextLLM integrates spatial layouts with textual data, reducing reliance on input sequence order.
218"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.25320197044334974,"Thus, we posit that shuffling the OCR input order could enhance the resilience of LayTextLLM in
219"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2541871921182266,"discerning relevant information irrespective of token proximity in the sequence.
220"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.25517241379310346,"Specifically, we propose Shuffled-OCR Supervised Fine-tuning (SSFT) that randomly shuffles the
221"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2561576354679803,"order of OCR-derived text in a certain proportion of examples. The range of exploration for the
222"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2571428571428571,"shuffling ratio can be found in Tab. 7 and 20% shuffled ratio is applied. The training objective is
223"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.258128078817734,"equivalent to predicting the next tokens, but in this scenario, only the tokens of the response are used
224"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.25911330049261083,"to compute loss. During SSFT, we unfreeze all parameters including those of LLMs. Experimental
225"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.2600985221674877,"results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making
226"
SHUFFLED-OCR SUPERVISED FINE-TUNING,0.26108374384236455,"it more robust to disruptions in input token order.
227"
EXPERIMENTS,0.2620689655172414,"4
Experiments
228"
DATASETS,0.2630541871921182,"4.1
Datasets
229"
DATASETS,0.26403940886699506,"Pre-training data
In our training process, we exclusively use open-source data to facilitate replica-
230"
DATASETS,0.2650246305418719,"tion. We collect data from two datasets for pre-training: (1) IIT-CDIP Test Collection 1.0 [39] and
231"
DATASETS,0.2660098522167488,"(2) DocBank [40]. The IIT-CDIP Test Collection 1.0 comprises an extensive repository of more than
232"
DATASETS,0.26699507389162563,"16 million document pages. DocBank consists of 500K documents, each presenting distinct layouts
233"
DATASETS,0.26798029556650244,"with a single page per document. For training efficiency, we choose to utilize the entire DocBank
234"
DATASETS,0.2689655172413793,"dataset and only subsample 5 million pages from the IIT-CDIP collection 1.0.
235"
DATASETS,0.26995073891625615,"SFT data
For document-oriented VQA, we select Document Dense Description (DDD) and
236"
DATASETS,0.270935960591133,"Layout-aware SFT data used in Luo et al. [21], which are two synthetic datasets generated by
237"
DATASETS,0.27192118226600986,"GPT-4. Besides, DocVQA [41], InfoVQA [42], ChartQA [43], VisualMRC [44] is included
238"
DATASETS,0.2729064039408867,"following [12]. For KIE task, we select SROIE [45], CORD [46], FUNSD [47], POIE [48] datasets
239"
DATASETS,0.2738916256157635,"following [12, 19, 21].
240"
IMPLEMENTATION DETAIL,0.2748768472906404,"4.2
Implementation Detail
241"
IMPLEMENTATION DETAIL,0.27586206896551724,"The LLM component of LayTextLLM is initialized from the Llama2-7B-base [36], which is a widely-
242"
IMPLEMENTATION DETAIL,0.2768472906403941,"used backbone. Other parameters including SLP and P-LoRA are randomly initialized. During
243"
IMPLEMENTATION DETAIL,0.27783251231527095,"pre-training, the LLM is frozen, and the parameters of SLP and P-LoRA modules are updated. During
244"
IMPLEMENTATION DETAIL,0.2788177339901478,"SFT, all parameters are fine-tuned. Other detailed setup can be found in Appendix B.
245"
IMPLEMENTATION DETAIL,0.2798029556650246,"We have configured the model with three versions of LayTextLLM for a side-by-side comparison
246"
IMPLEMENTATION DETAIL,0.28078817733990147,"under different settings. Aligned with Luo et al. [21], the first version, LayTextLLMzero, is
247"
IMPLEMENTATION DETAIL,0.2817733990147783,"trained exclusively with DDD and Layout-aware SFT data. Building upon this, and in alignment
248"
IMPLEMENTATION DETAIL,0.2827586206896552,"with the setting of Liu et al. [12], we introduce the DocVQA, InfoVQA, and ChartQA training
249"
IMPLEMENTATION DETAIL,0.28374384236453204,"sets to the dataset pool for our second version, termed LayTextLLMvqa. Finally, we incorporate
250"
IMPLEMENTATION DETAIL,0.28472906403940884,"a comprehensive suite of KIE datasets—FUNSD, CORD, POIE, SROIE, and VisualMRC—as
251"
IMPLEMENTATION DETAIL,0.2857142857142857,"described by Wang et al. [19], creating our most extensive version, LayTextLLMall. Note that all
252"
IMPLEMENTATION DETAIL,0.28669950738916256,"versions are based on the same pre-trained LayTextLLM weight.
253"
IMPLEMENTATION DETAIL,0.2876847290640394,"Document-Oriented VQA
KIE
DocVQA
InfoVQA
Avg
FUNSD
SROIE
POIE
Avg"
IMPLEMENTATION DETAIL,0.28866995073891627,"Metric
Accuracy %"
IMPLEMENTATION DETAIL,0.2896551724137931,"OCR-free
UniDoc [10]
7.7
14.7
11.2
1.0
2.9
5.1
3.0
DocPedia [9]
47.1∗
15.2∗
31.2
29.9
21.4
39.9
30.4
Monkey [49]
50.1∗
25.8∗
38.0
24.1
41.9
19.9
28.6
InternVL [50]
28.7∗
23.6∗
26.2
6.5
26.4
25.9
19.6
InternLM-XComposer2 [15]
39.7
28.6
34.2
15.3
34.2
49.3
32.9
TextMonkey [12]
64.3∗
28.2∗
46.3
32.3
47.0
27.9
35.7
TextMonkey+ [12]
66.7∗
28.6∗
47.7
42.9
46.2
32.0
40.4"
IMPLEMENTATION DETAIL,0.29064039408866993,"text + polys
LayTextLLMzero (Ours)
71.8
33.8
52.8
49.4
86.7
66.1
67.4
LayTextLLMvqa (Ours)
77.4∗
66.1∗
71.8
48.9
74.6
70.6
64.7
Table 1: Comparison with SOTA OCR-free MLLMs. ∗indicates the training set used."
IMPLEMENTATION DETAIL,0.2916256157635468,"Document-Oriented VQA
KIE
DocVQA
VisualMRC
Avg
FUNSD
CORD
SROIE
Avg"
IMPLEMENTATION DETAIL,0.29261083743842364,"Metric
ANLS % / CIDEr
F-score %"
IMPLEMENTATION DETAIL,0.2935960591133005,"Text
Llama2-7B-base
34.0
182.7
108.3
25.6
51.9
43.4
40.3
Llama2-7B-chat
20.5
6.3
13.4
23.4
51.8
58.6
44.6"
IMPLEMENTATION DETAIL,0.29458128078817736,"Text + Polys
Llama2-7B-basecoor [23]
8.4
3.8
6.1
6.0
46.4
34.7
29.0
Llama2-7B-chatcoor [23]
12.3
28.0
20.1
14.4
38.1
50.6
34.3
Davinci-003-175Bcoor [23]
-
-
-
-
92.6
95.8
-
DocLLM [19]
69.5∗
264.1∗
166.8
51.8∗
67.4∗
91.9∗
70.3
LayTextLLMzero (Ours)
65.4
260.7
163.0
48.6
74.5
86.4
69.8
LayTextLLMvqa (Ours)
75.7∗
260.2∗
168.0
52.7
70.9
78.6
67.4
LayTextLLMall (Ours)
77.3∗
295.9∗
186.6
64.2∗
96.5∗
95.8∗
85.8
Table 2: Comparison with other OCR-based methods. ∗indicates the training set used."
BASELINES,0.2955665024630542,"4.3
Baselines
254"
BASELINES,0.296551724137931,"OCR-free baselines
In the category of OCR-free MLLMs, we have chosen the following SOTA
255"
BASELINES,0.2975369458128079,"models as our strong baselines due to their superior performance in both document-oriented VQA
256"
BASELINES,0.29852216748768473,"and KIE tasks. These include UniDoc [10], DocPedia [9], Monkey [49], InternVL [50], InternLM-
257"
BASELINES,0.2995073891625616,"XComposer2 [15], TextMonkey, and TextMonkey+ [12].
258"
BASELINES,0.30049261083743845,"OCR-based baselines
For OCR-based baseline models, we implemented a basic approach using
259"
BASELINES,0.30147783251231525,"only OCR-derived text as input. This was done using two versions: Llama2-7B-base and Llama2-
260"
BASELINES,0.3024630541871921,"7B-chat. We also adapted the coordinate-as-tokens scheme from He et al. [23] for these models,
261"
BASELINES,0.30344827586206896,"resulting in two new variants: Llama2-7B-basecoor and Llama2-7B-chatcoor. It’s important to note
262"
BASELINES,0.3044334975369458,"that we did not employ the ICL strategy with these models, as it would significantly exceed their
263"
BASELINES,0.3054187192118227,"maximum sequence length constraints. Additionally, we included results from a stronger baseline
264"
BASELINES,0.30640394088669953,"using the ChatGPT Davinci-003 (175B) model [23], termed Davinci-003-175Bcoor. One other recent
265"
BASELINES,0.30738916256157633,"SOTA OCR-based approach, DocLLM [19] is also considered in our analysis. Finally, LayoutLLM
266"
BASELINES,0.3083743842364532,"and LayoutLLMCoT [21], which integrates visual cues, text and layout is also included.
267"
EVALUATION METRICS,0.30935960591133005,"4.4
Evaluation Metrics
268"
EVALUATION METRICS,0.3103448275862069,"To ensure a fair comparison with OCR-free methods, we adopted the accuracy metric, where a
269"
EVALUATION METRICS,0.31133004926108376,"response from the model is considered correct if it fully captures the ground truth. This approach
270"
EVALUATION METRICS,0.31231527093596056,"aligns with the evaluation criteria described by [9, 10, 12]. To further enhance the comparability with
271"
EVALUATION METRICS,0.3133004926108374,"other OCR-based methods, we conducted additional evaluations using original metrics specific to
272"
EVALUATION METRICS,0.3142857142857143,"certain datasets, such as F1 score [19, 23], ANLS [19, 21, 51] and CIDEr [19, 52].
273"
EVALUATION METRICS,0.31527093596059114,"Document-Oriented VQA
KIE
DocVQA
VisualMRC
Avg
FUNSD−
CORD−
SROIE−
Avg"
EVALUATION METRICS,0.316256157635468,"Metric
ANLS %"
EVALUATION METRICS,0.31724137931034485,"Visual + Text + Polys
LayoutLLM [21]
72.3
-
-
74.0
-
-
-
LayoutLLMCoT [21]
74.2
55.7
64.9
79.9
63.1
72.1
71.7"
EVALUATION METRICS,0.31822660098522165,"Text
Llama2-7B-base
34.0
25.4
29.7
42.1
46.7
60.6
49.8
Llama2-7B-chat
20.5
9.9
15.2
15.1
20.0
35.6
23.5"
EVALUATION METRICS,0.3192118226600985,"Text + Polys
Llama2-7B-basecoor [23]
8.4
6.7
7.5
4.3
33.0
47.2
28.1
Llama2-7B-chatcoor [23]
12.3
12.2
12.2
11.9
6.4
39.4
19.2
LayTextLLMzero (Ours)
65.4
36.2
50.8
71.0
66.9
89.2
75.7
LayTextLLMall (Ours)
77.3∗
41.7∗
59.5
81.3∗
82.6∗
96.2∗
86.7
Table 3: Comparison with LayoutLLM. −indicates that the cleaned test set used in Luo et al. [21]."
QUANTITATIVE RESULTS,0.32019704433497537,"4.5
Quantitative Results
274"
QUANTITATIVE RESULTS,0.3211822660098522,"Comparison with SOTA OCR-free Methods
The experimental results shown in Tab. 1 demon-
275"
QUANTITATIVE RESULTS,0.3221674876847291,"strate the outstanding performance of the LayTextLLM series across various tasks. Note that the
276"
QUANTITATIVE RESULTS,0.32315270935960594,"results for ChartQA are reported in Appendix E due to concerns about fairness in comparison, as
277"
QUANTITATIVE RESULTS,0.32413793103448274,"the dataset does not include OCR-derived results and we used in-house OCR tools instead. Firstly,
278"
QUANTITATIVE RESULTS,0.3251231527093596,"LayTextLLMzero significantly outperforms previous SOTA OCR-free methods, such as TextMon-
279"
QUANTITATIVE RESULTS,0.32610837438423645,"key [12], in zero-shot capabilities, even when these methods use the training set of the dataset. For
280"
QUANTITATIVE RESULTS,0.3270935960591133,"example, in the DocVQA and InfoVQA datasets, LayTextLLMzero achieves accuracies of 71.8% and
281"
QUANTITATIVE RESULTS,0.32807881773399017,"33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey
282"
QUANTITATIVE RESULTS,0.32906403940886697,"and InternLM-XComposer2. When fine-tuned with corresponding datasets, LayTextLLM shows even
283"
QUANTITATIVE RESULTS,0.33004926108374383,"greater performance improvements, particularly in document-oriented VQA datasets. Specifically,
284"
QUANTITATIVE RESULTS,0.3310344827586207,"its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating
285"
QUANTITATIVE RESULTS,0.33201970443349754,"the model’s strong ability to leverage task-specific data. Additionally, LayTextLLMzero excels in
286"
QUANTITATIVE RESULTS,0.3330049261083744,"KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and
287"
QUANTITATIVE RESULTS,0.33399014778325126,"66.1%, respectively. These results significantly surpass those of previous SOTA OCR-free model (i.e.,
288"
QUANTITATIVE RESULTS,0.33497536945812806,"TextMonkey+) by margins of 40.5% and 34.1%, respectively. This significant performance gain is
289"
QUANTITATIVE RESULTS,0.3359605911330049,"likely due to these datasets containing low-resolution images that are too blurred for current MLLMs
290"
QUANTITATIVE RESULTS,0.3369458128078818,"to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.
291"
QUANTITATIVE RESULTS,0.33793103448275863,"Comparison with SOTA OCR-based Methods
For comprehensive comparison, we have also
292"
QUANTITATIVE RESULTS,0.3389162561576355,"conducted correspinding experiments to align with OCR-based methods [19, 21]. The experimental
293"
QUANTITATIVE RESULTS,0.3399014778325123,"results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM
294"
QUANTITATIVE RESULTS,0.34088669950738915,"models compared to pure OCR-based SOTA methods such as DocLLM [19]. Specifically, when
295"
QUANTITATIVE RESULTS,0.341871921182266,"comparing with DocLLM, LayTextLLMzero demonstrates notably superior performance, with even its
296"
QUANTITATIVE RESULTS,0.34285714285714286,"zero-shot capabilities being competitive with supervised SFT approaches. We believe that the subpar
297"
QUANTITATIVE RESULTS,0.3438423645320197,"performance of DocLLM is likely due to its use of cross-attention and the masked span pre-training
298"
QUANTITATIVE RESULTS,0.3448275862068966,"tasks [53], which fail to leverage the autoregressive features of LLMs effectively. Similarly, when
299"
QUANTITATIVE RESULTS,0.3458128078817734,"contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLMzero again outperforms
300"
QUANTITATIVE RESULTS,0.34679802955665023,"significantly. This disparity in performance can be attributed to the following three reasons: (1) The
301"
QUANTITATIVE RESULTS,0.3477832512315271,"coordinate-as-tokens approach tends to introduce an excessive number of tokens, often exceeding the
302"
QUANTITATIVE RESULTS,0.34876847290640395,"pre-defined maximum length of Llama2-7B (i.e., 4096). Consequently, this leads to a lack of crucial
303"
QUANTITATIVE RESULTS,0.3497536945812808,"OCR information, resulting in hallucination and subpar performance. (2) When re-implementing the
304"
QUANTITATIVE RESULTS,0.35073891625615766,"coordinate-as-tokens method with Llama2-7B, we did not introduce the ICL strategy, as it would
305"
QUANTITATIVE RESULTS,0.35172413793103446,"contribute additional length to the input sequence. (3) The coordinate-as-tokens approach necessitates
306"
QUANTITATIVE RESULTS,0.3527093596059113,"a considerably larger-sized LLM to comprehend the numerical tokens effectively.
307"
QUANTITATIVE RESULTS,0.3536945812807882,"In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as
308"
QUANTITATIVE RESULTS,0.35467980295566504,"shown in Tab. 3. In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating
309"
QUANTITATIVE RESULTS,0.3556650246305419,"our capability to leverage OCR-based results effectively. However, we fall short on document-
310"
QUANTITATIVE RESULTS,0.3566502463054187,"oriented VQA tasks since answering some questions that are strongly related to vision information
311"
QUANTITATIVE RESULTS,0.35763546798029555,"may challenge our approach. Two main reasons may well explain this performance discrepancy:
312"
QUANTITATIVE RESULTS,0.3586206896551724,"(1) The visual encoder in LayoutLLM provides additional visual information. (2) LayoutLLM
313"
QUANTITATIVE RESULTS,0.35960591133004927,"incorporates the Chain-of-Thought (CoT) mechanism to model contextual information while it is
314"
QUANTITATIVE RESULTS,0.3605911330049261,"not used in our approach. However, when fine-tuned with tailored data, LayTextLLM significantly
315"
QUANTITATIVE RESULTS,0.361576354679803,"outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data. More qualitative
316"
QUANTITATIVE RESULTS,0.3625615763546798,"example demonstrates can be found in Appendix A.
317"
ANALYSIS,0.36354679802955664,"4.6
Analysis
318"
ANALYSIS,0.3645320197044335,"Document-Oriented VQA
KIE"
ANALYSIS,0.36551724137931035,"LNTP
SSFT
DocVQA
InfoVQA
VisualMRC
Avg
FUNSD
CORD
SROIE
POIE
Avg"
ANALYSIS,0.3665024630541872,"✓
72.3
35.7
24.4
44.2
50.6
91.6
92.8
58.6
73.4
✓
74.5
38.0
23.9
45.5
56.4
95.8
93.2
59.6
76.3
✓
✓
78.8
42.7
35.1
52.2
62.9
95.9
95.2
61.7
78.9
Table 4: Ablations on pre-training and SFT component of LayTextLLM (Accuracy)."
ANALYSIS,0.367487684729064,"Ablations
To better assess the utility of Layout-aware Next Token Prediction and Shuffled-OCR
319"
ANALYSIS,0.36847290640394087,"Supervised Fine-tuning in LayTextLLM, an ablation study was performed (see Tab. 4). Details on
320"
ANALYSIS,0.3694581280788177,"the training setup for all variants are provided in Appendix B. It is evident that both LNTP and
321"
ANALYSIS,0.3704433497536946,"SSFT significantly enhance the utility of LayTextLLM. Specifically, disabling LNTP results in an 8%
322"
ANALYSIS,0.37142857142857144,"decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks. Disabling SSFT leads to a
323"
ANALYSIS,0.3724137931034483,"decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively.
324"
ANALYSIS,0.3733990147783251,"Sequence Length
Tab. 5 presents statistics on the average input sequence length across different
325"
ANALYSIS,0.37438423645320196,"datasets. Intriguingly, despite interleaving bounding box tokens, LayTextLLM consistently exhibits
326"
ANALYSIS,0.3753694581280788,"the shortest sequence length in three out of four datasets, even surpassing DocLLM, which is coun-
327"
ANALYSIS,0.37635467980295567,"terintuitive. We attribute this to the tokenizer mechanism. For example, using tokenizer.encode(), a
328"
ANALYSIS,0.37733990147783253,"single word from the OCR engine, like “International” is encoded into a single ID [4623]. Conversely,
329"
ANALYSIS,0.3783251231527094,"when the entire OCR output is processed as one sequence, such as “... CPC,International,Inc...”, the
330"
ANALYSIS,0.3793103448275862,"word “International” is split into two IDs [17579, 1288], corresponding to “Intern” and “ational”
331"
ANALYSIS,0.38029556650246304,"respectively. This type of case occurs frequently, more discussion in Appendix C.
332"
ANALYSIS,0.3812807881773399,"Dataset
LayTextLLM
DocLLM [19]
Coor-as-tokens [23]"
ANALYSIS,0.38226600985221676,"DocVQA
664.3
827.5
4085.7
CORD
137.9
153.2
607.3
FUNSD
701.9
847.5
4183.4
SROIE
529.2
505.1
1357.7
Table 5: Average sequence length of each data for different methods using Llama2 tokenizer."
LIMITATION,0.3832512315270936,"5
Limitation
333"
LIMITATION,0.3842364532019704,"Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone
334"
LIMITATION,0.3852216748768473,"does not suffice for all real-world applications. There are some instances, particularly in chart analysis,
335"
LIMITATION,0.38620689655172413,"where reasoning must be based solely on visual cues (e.g. size, color)—a challenge that remains
336"
LIMITATION,0.387192118226601,"unmet. Questions such as “What is the difference between the highest and the lowest green bar?”
337"
LIMITATION,0.38817733990147785,"illustrate this gap. The ChartQA results, detailed in Appendix E, also underscore these limitations.
338"
LIMITATION,0.3891625615763547,"Addressing these challenges highlights the urgent need for future enhancements that integrate visual
339"
LIMITATION,0.3901477832512315,"cue within the capabilities of LayTextLLM.
340"
CONCLUSION,0.39113300492610836,"6
Conclusion
341"
CONCLUSION,0.3921182266009852,"We propose LayTextLLM for various VRDU tasks, in which spatial layouts and textual data are
342"
CONCLUSION,0.3931034482758621,"seamlessly interleaved to make more accurate prediction by introducing a innovative Spatial Layout
343"
CONCLUSION,0.39408866995073893,"Projector. Two tailored training tasks — Layout-aware Next Token Prediction and Shuffled-OCR
344"
CONCLUSION,0.39507389162561574,"Supervised Fine-tuning — are designed to improve the comprehension of document layouts. Extensive
345"
CONCLUSION,0.3960591133004926,"experiments confirm the effectiveness of LayTextLLM.
346"
REFERENCES,0.39704433497536945,"References
347"
REFERENCES,0.3980295566502463,"[1] OpenAI:Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
348"
REFERENCES,0.39901477832512317,"ciaLeoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red
349"
REFERENCES,0.4,"Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavar-
350"
REFERENCES,0.4009852216748768,"ian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner,
351"
REFERENCES,0.4019704433497537,"Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim
352"
REFERENCES,0.40295566502463054,"Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
353"
REFERENCES,0.4039408866995074,"Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
354"
REFERENCES,0.40492610837438425,"Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,
355"
REFERENCES,0.4059113300492611,"HyungWon Chung, Dave Cummings, and Jeremiah Currier. Gpt-4 technical report. arXiv
356"
REFERENCES,0.4068965517241379,"preprint arXiv:2303.08774, Dec 2023.
357"
REFERENCES,0.40788177339901477,"[2] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and
358"
REFERENCES,0.4088669950738916,"Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). arXiv preprint
359"
REFERENCES,0.4098522167487685,"arXiv:2309.17421, Sep 2023.
360"
REFERENCES,0.41083743842364534,"[3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
361"
REFERENCES,0.41182266009852214,"Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly
362"
REFERENCES,0.412807881773399,"capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
363"
REFERENCES,0.41379310344827586,"[4] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.
364"
REFERENCES,0.4147783251231527,"[5] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
365"
REFERENCES,0.41576354679802957,"baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.
366"
REFERENCES,0.41674876847290643,"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv
367"
REFERENCES,0.41773399014778323,"preprint arXiv:2403.05530, 2024.
368"
REFERENCES,0.4187192118226601,"[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
369"
REFERENCES,0.41970443349753694,"Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
370"
REFERENCES,0.4206896551724138,"[7] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng
371"
REFERENCES,0.42167487684729066,"Ren, Zhuoshu Li, Yaofeng Sun, et al.
Deepseek-vl: towards real-world vision-language
372"
REFERENCES,0.42266009852216746,"understanding. arXiv preprint arXiv:2403.05525, 2024.
373"
REFERENCES,0.4236453201970443,"[8] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
374"
REFERENCES,0.4246305418719212,"Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv
375"
REFERENCES,0.42561576354679803,"preprint arXiv:2403.04652, 2024.
376"
REFERENCES,0.4266009852216749,"[9] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang. Docpedia: Un-
377"
REFERENCES,0.42758620689655175,"leashing the power of large multimodal model in the frequency domain for versatile document
378"
REFERENCES,0.42857142857142855,"understanding. arXiv preprint arXiv:2311.11810, 2023.
379"
REFERENCES,0.4295566502463054,"[10] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.
380"
REFERENCES,0.43054187192118226,"Unidoc: A universal large multimodal model for simultaneous text detection, recognition,
381"
REFERENCES,0.4315270935960591,"spotting and understanding. arXiv preprint arXiv:2308.11592, 2023.
382"
REFERENCES,0.432512315270936,"[11] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin
383"
REFERENCES,0.43349753694581283,"Jin, Fei Huang, et al. mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document
384"
REFERENCES,0.43448275862068964,"understanding. arXiv preprint arXiv:2403.12895, 2024.
385"
REFERENCES,0.4354679802955665,"[12] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.
386"
REFERENCES,0.43645320197044335,"Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint
387"
REFERENCES,0.4374384236453202,"arXiv:2403.04473, 2024.
388"
REFERENCES,0.43842364532019706,"[13] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li,
389"
REFERENCES,0.43940886699507387,"Siqi Wang, Lei Liao, et al. Textsquare: Scaling up text-centric visual instruction tuning. arXiv
390"
REFERENCES,0.4403940886699507,"preprint arXiv:2404.12803, 2024.
391"
REFERENCES,0.4413793103448276,"[14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong,
392"
REFERENCES,0.44236453201970444,"Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to
393"
REFERENCES,0.4433497536945813,"commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821,
394"
REFERENCES,0.44433497536945815,"2024.
395"
REFERENCES,0.44532019704433495,"[15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei,
396"
REFERENCES,0.4463054187192118,"Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering
397"
REFERENCES,0.44729064039408867,"free-form text-image composition and comprehension in vision-language large model. arXiv
398"
REFERENCES,0.4482758620689655,"preprint arXiv:2401.16420, 2024.
399"
REFERENCES,0.4492610837438424,"[16] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
400"
REFERENCES,0.45024630541871924,"Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
401"
REFERENCES,0.45123152709359604,"language models. arXiv preprint arXiv:2403.18814, 2024.
402"
REFERENCES,0.4522167487684729,"[17] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae
403"
REFERENCES,0.45320197044334976,"Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https:
404"
REFERENCES,0.4541871921182266,"//llava-vl.github.io/blog/2024-01-30-llava-next/.
405"
REFERENCES,0.45517241379310347,"[18] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai
406"
REFERENCES,0.45615763546798027,"Xu, Chenliang Li, Junfeng Tian, et al. mPLUG-DocOwl: Modularized multimodal large
407"
REFERENCES,0.45714285714285713,"language model for document understanding. arXiv:2307.02499, 2023.
408"
REFERENCES,0.458128078817734,"[19] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur,
409"
REFERENCES,0.45911330049261084,"Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. Docllm: A layout-aware generative
410"
REFERENCES,0.4600985221674877,"language model for multimodal document understanding. arXiv preprint arXiv:2401.00908,
411"
REFERENCES,0.46108374384236456,"2023.
412"
REFERENCES,0.46206896551724136,"[20] Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu Sun, Ramya Sree Boppana,
413"
REFERENCES,0.4630541871921182,"Zilong Wang, Jiaqi Mu, Hao Zhang, and Nan Hua. Lmdx: Language model-based document
414"
REFERENCES,0.4640394088669951,"information extraction and localization. arXiv preprint arXiv:2309.10952, 2023.
415"
REFERENCES,0.46502463054187193,"[21] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout
416"
REFERENCES,0.4660098522167488,"instruction tuning with large language models for document understanding. CVPR 2024, 2024.
417"
REFERENCES,0.4669950738916256,"[22] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
418"
REFERENCES,0.46798029556650245,"Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195,
419"
REFERENCES,0.4689655172413793,"2023.
420"
REFERENCES,0.46995073891625616,"[23] Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and Heng Tao Shen. Icl-d3ie:
421"
REFERENCES,0.470935960591133,"In-context learning with diverse demonstrations updating for document information extraction.
422"
REFERENCES,0.4719211822660099,"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19485–
423"
REFERENCES,0.4729064039408867,"19494, 2023.
424"
REFERENCES,0.47389162561576353,"[24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao
425"
REFERENCES,0.4748768472906404,"Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning
426"
REFERENCES,0.47586206896551725,"perception with language models. arXiv:2302.14045, 2023.
427"
REFERENCES,0.4768472906403941,"[25] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
428"
REFERENCES,0.47783251231527096,"Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824,
429"
REFERENCES,0.47881773399014776,"2023.
430"
REFERENCES,0.4798029556650246,"[26] Wonseok Hwang, Jinyeong Yim, Seung-Hyun Park, Sohee Yang, and Minjoon Seo. Spatial
431"
REFERENCES,0.4807881773399015,"dependency parsing for semi-structured document information extraction. Cornell University -
432"
REFERENCES,0.48177339901477834,"arXiv,Cornell University - arXiv, May 2020.
433"
REFERENCES,0.4827586206896552,"[27] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm:
434"
REFERENCES,0.483743842364532,"Pre-training of text and layout for document image understanding. In Proceedings of the 26th
435"
REFERENCES,0.48472906403940885,"ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Aug 2020.
436"
REFERENCES,0.4857142857142857,"doi: 10.1145/3394486.3403172. URL http://dx.doi.org/10.1145/3394486.3403172.
437"
REFERENCES,0.48669950738916257,"[28] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei
438"
REFERENCES,0.4876847290640394,"Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi-
439"
REFERENCES,0.4886699507389163,"modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual
440"
REFERENCES,0.4896551724137931,"Meeting of the Association for Computational Linguistics and the 11th International Joint
441"
REFERENCES,0.49064039408866994,"Conference on Natural Language Processing (Volume 1: Long Papers), Jan 2021. doi: 10.18653/
442"
REFERENCES,0.4916256157635468,"v1/2021.acl-long.201. URL http://dx.doi.org/10.18653/v1/2021.acl-long.201.
443"
REFERENCES,0.49261083743842365,"[29] Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park.
444"
REFERENCES,0.4935960591133005,"Bros: A pre-trained language model focusing on text and layout for better key information
445"
REFERENCES,0.4945812807881773,"extraction from documents. Proceedings of the AAAI Conference on Artificial Intelligence,
446"
REFERENCES,0.49556650246305417,"page 10767–10775, Jul 2022. doi: 10.1609/aaai.v36i10.21322. URL http://dx.doi.org/
447"
REFERENCES,0.496551724137931,"10.1609/aaai.v36i10.21322.
448"
REFERENCES,0.4975369458128079,"[30] Zineng Tang, Zhenfeng Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Zhu C, Michael Zeng,
449"
REFERENCES,0.49852216748768474,"Zhang Cha, and Mohit Bansal. Unifying vision, text, and layout for universal document
450"
REFERENCES,0.4995073891625616,"processing. Cornell University - arXiv,Cornell University - arXiv, Dec 2022.
451"
REFERENCES,0.5004926108374385,"[31] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for
452"
REFERENCES,0.5014778325123153,"document ai with unified text and image masking. In Proceedings of the 30th ACM International
453"
REFERENCES,0.5024630541871922,"Conference on Multimedia, pages 4083–4091, 2022.
454"
REFERENCES,0.503448275862069,"[32] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,
455"
REFERENCES,0.5044334975369458,"Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document
456"
REFERENCES,0.5054187192118227,"understanding transformer. In European Conference on Computer Vision, pages 498–517, 2022.
457"
REFERENCES,0.5064039408866995,"[33] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisensch-
458"
REFERENCES,0.5073891625615764,"los, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct:
459"
REFERENCES,0.5083743842364532,"Screenshot parsing as pretraining for visual language understanding. In International Confer-
460"
REFERENCES,0.50935960591133,"ence on Machine Learning, pages 18893–18912. PMLR, 2023.
461"
REFERENCES,0.5103448275862069,"[34] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.
462"
REFERENCES,0.5113300492610837,"Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint
463"
REFERENCES,0.5123152709359606,"arXiv:2306.17107, 2023.
464"
REFERENCES,0.5133004926108374,"[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances
465"
REFERENCES,0.5142857142857142,"in neural information processing systems, 36, 2024.
466"
REFERENCES,0.5152709359605911,"[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
467"
REFERENCES,0.516256157635468,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
468"
REFERENCES,0.5172413793103449,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
469"
REFERENCES,0.5182266009852217,"[37] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan
470"
REFERENCES,0.5192118226600986,"Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction
471"
REFERENCES,0.5201970443349754,"model. arXiv:2304.15010, 2023.
472"
REFERENCES,0.5211822660098522,"[38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
473"
REFERENCES,0.5221674876847291,"Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
474"
REFERENCES,0.5231527093596059,"[39] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. Building a test
475"
REFERENCES,0.5241379310344828,"collection for complex document information processing. In Proceedings of the 29th annual
476"
REFERENCES,0.5251231527093596,"international ACM SIGIR conference on Research and development in information retrieval,
477"
REFERENCES,0.5261083743842364,"Aug 2006. doi: 10.1145/1148170.1148307. URL http://dx.doi.org/10.1145/1148170.
478"
REFERENCES,0.5270935960591133,"1148307.
479"
REFERENCES,0.5280788177339901,"[40] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou.
480"
REFERENCES,0.529064039408867,"Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th
481"
REFERENCES,0.5300492610837438,"International Conference on Computational Linguistics, Jan 2020. doi: 10.18653/v1/2020.
482"
REFERENCES,0.5310344827586206,"coling-main.82. URL http://dx.doi.org/10.18653/v1/2020.coling-main.82.
483"
REFERENCES,0.5320197044334976,"[41] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on
484"
REFERENCES,0.5330049261083744,"document images. In Proceedings of the IEEE/CVF winter conference on applications of
485"
REFERENCES,0.5339901477832513,"computer vision, pages 2200–2209, 2021.
486"
REFERENCES,0.5349753694581281,"[42] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawa-
487"
REFERENCES,0.5359605911330049,"har. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of
488"
REFERENCES,0.5369458128078818,"Computer Vision, pages 1697–1706, 2022.
489"
REFERENCES,0.5379310344827586,"[43] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA:
490"
REFERENCES,0.5389162561576355,"A benchmark for question answering about charts with visual and logical reasoning.
In
491"
REFERENCES,0.5399014778325123,"Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association
492"
REFERENCES,0.5408866995073892,"for Computational Linguistics: ACL 2022, pages 2263–2279, Dublin, Ireland, May 2022.
493"
REFERENCES,0.541871921182266,"Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL
494"
REFERENCES,0.5428571428571428,"https://aclanthology.org/2022.findings-acl.177.
495"
REFERENCES,0.5438423645320197,"[44] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehen-
496"
REFERENCES,0.5448275862068965,"sion on document images. In Proceedings of the AAAI Conference on Artificial Intelligence,
497"
REFERENCES,0.5458128078817734,"volume 35, pages 13878–13888, 2021.
498"
REFERENCES,0.5467980295566502,"[45] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and
499"
REFERENCES,0.547783251231527,"CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019
500"
REFERENCES,0.548768472906404,"International Conference on Document Analysis and Recognition (ICDAR), pages 1516–1520.
501"
REFERENCES,0.5497536945812808,"IEEE, 2019.
502"
REFERENCES,0.5507389162561577,"[46] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and
503"
REFERENCES,0.5517241379310345,"Hwalsuk Lee. Cord: a consolidated receipt dataset for post-ocr parsing. In Workshop on
504"
REFERENCES,0.5527093596059113,"Document Intelligence at NeurIPS 2019, 2019.
505"
REFERENCES,0.5536945812807882,"[47] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form
506"
REFERENCES,0.554679802955665,"understanding in noisy scanned documents. In 2019 International Conference on Document
507"
REFERENCES,0.5556650246305419,"Analysis and Recognition Workshops (ICDARW), volume 2, pages 1–6. IEEE, 2019.
508"
REFERENCES,0.5566502463054187,"[48] Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang
509"
REFERENCES,0.5576354679802956,"Bai. Visual information extraction in the wild: practical dataset and end-to-end solution. In
510"
REFERENCES,0.5586206896551724,"International Conference on Document Analysis and Recognition, pages 36–53. Springer, 2023.
511"
REFERENCES,0.5596059113300492,"[49] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang
512"
REFERENCES,0.5605911330049261,"Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large
513"
REFERENCES,0.5615763546798029,"multi-modal models. arXiv preprint arXiv:2311.06607, 2023.
514"
REFERENCES,0.5625615763546798,"[50] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong
515"
REFERENCES,0.5635467980295567,"Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning
516"
REFERENCES,0.5645320197044335,"for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.
517"
REFERENCES,0.5655172413793104,"[51] Liangcai Gao, Yilun Huang, Herve Dejean, Jean-Luc Meunier, Qinqin Yan, Yu Fang, Florian
518"
REFERENCES,0.5665024630541872,"Kleber, and Eva Lang. Icdar 2019 competition on table detection and recognition (ctdar). In
519"
REFERENCES,0.5674876847290641,"International Conference on Document Analysis and Recognition, 2019.
520"
REFERENCES,0.5684729064039409,"[52] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
521"
REFERENCES,0.5694581280788177,"description evaluation. In Proceedings of the IEEE conference on computer vision and pattern
522"
REFERENCES,0.5704433497536946,"recognition, pages 4566–4575, 2015.
523"
REFERENCES,0.5714285714285714,"[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
524"
REFERENCES,0.5724137931034483,"Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
525"
REFERENCES,0.5733990147783251,"text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.
526"
REFERENCES,0.574384236453202,"[54] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
527"
REFERENCES,0.5753694581280788,"with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual
528"
REFERENCES,0.5763546798029556,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
529"
REFERENCES,0.5773399014778325,"1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:
530"
REFERENCES,0.5783251231527093,"10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162.
531"
REFERENCES,0.5793103448275863,"Appendix
532"
REFERENCES,0.5802955665024631,"A
Qualitative Examples
533"
REFERENCES,0.5812807881773399,"Qualitative examples of document-oriented VQA (upper row) and KIE (bottom row) are shown in
534"
REFERENCES,0.5822660098522168,"Fig. 6. The results indicate that LayTextLLM is highly effective in utilizing spatial layout information
535"
REFERENCES,0.5832512315270936,"to make more accurate predictions for these challenging examples. For example, in the upper
536"
REFERENCES,0.5842364532019705,"right figure, many numeric texts in the receipt act as noise for the baseline method. In contrast,
537"
REFERENCES,0.5852216748768473,"LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by
538"
REFERENCES,0.5862068965517241,"the other examples, underscoring the utility of LayTextLLM.
539"
REFERENCES,0.587192118226601,What is the date in this receipt? 2003
REFERENCES,0.5881773399014778,Anwsered  by LayTextLLM (Ours)
REFERENCES,0.5891625615763547,Anwsered  by Llama-7B-base
REFERENCES,0.5901477832512315,29 JUN 18
REFERENCES,0.5911330049261084,What is the total price in this receipt?
REFERENCES,0.5921182266009852,100.00
REFERENCES,0.593103448275862,Anwsered  by LayTextLLM (Ours)
REFERENCES,0.594088669950739,Anwsered  by Llama-7B-base 79.50 ✓ ✖ ✓ ✖
REFERENCES,0.5950738916256157,"what is the 10th service provided
under the 'services provided by
child welfare staff'?"
REFERENCES,0.5960591133004927,Anwsered  by LayTextLLM (Ours)
REFERENCES,0.5970443349753695,Anwsered  by Llama-7B-base ✖
REFERENCES,0.5980295566502463,"10th service provided under the 'services
provided by child welfare staff' is 'Licenses
children's institutions, agencies and day"
REFERENCES,0.5990147783251232,"✓
licenses independent full time and day
care homes"
REFERENCES,0.6,"Which is the last financial year
(FY) listed under sub-heading
""Funding""? 1977"
REFERENCES,0.6009852216748769,Anwsered  by LayTextLLM (Ours)
REFERENCES,0.6019704433497537,Anwsered  by Llama-7B-base
REFERENCES,0.6029556650246305,FY 1978 ✓ ✖
REFERENCES,0.6039408866995074,Figure 6: Qualitative comparison with the baseline method.
REFERENCES,0.6049261083743842,"B
Implementation Detail
540"
REFERENCES,0.6059113300492611,"All training and inference procedures are conducted on eight NVIDIA A100 GPUs.
541"
REFERENCES,0.6068965517241379,"Training
LayTextLLM is initialized with Llama2-7B-Base model, the pre-training, SFT, and other
542"
REFERENCES,0.6078817733990147,"model hyper-parameters can be seen in Tab. 6. Please note that all variants of LayTextLLM, including
543"
REFERENCES,0.6088669950738916,"those utilized in ablation studies, are trained in accordance with the SFT settings. All baseline results
544"
REFERENCES,0.6098522167487684,"are sourced from their respective original papers, with the exception of the Llama2-7B series and the
545"
REFERENCES,0.6108374384236454,"Llama2-7Bcoor series. These were re-implemented and can be referenced in [21, 23].
546"
REFERENCES,0.6118226600985222,"Backbone
Plora rank
Batch size
Max length
Precision
Train params
Fix params"
REFERENCES,0.6128078817733991,"Pretrain
Llama2-7B-base
256
128
2048
bf16
648 M
6.7 B
SFT
Llama2-7B-base
256
256
4096
bf16
7.4 B
0B"
REFERENCES,0.6137931034482759,"Learning rate
Weight decay
Scheduler
Adam betas
Adam epsilon
Warm up
Epoch"
REFERENCES,0.6147783251231527,"Pretrain
1.0e-04
0.01
cosine
[0.9, 0.999]
1.0e-08
0.005
2
SFT
2.0e-05
0.01
cosine
[0.9, 0.999]
1.0e-08
0.005
2
Table 6: LayTextLLM trainng Hyper-parameters."
REFERENCES,0.6157635467980296,"Inference
For the document-oriented VQA test set, we use the original question-answer pairs as
547"
REFERENCES,0.6167487684729064,"the prompt and ground truth, respectively. For Key Information Extraction (KIE) tasks, we reformat
548"
REFERENCES,0.6177339901477833,"the key-value pairs into a question-answer format, as described in [12, 19, 21]. Additionally, for the
549"
REFERENCES,0.6187192118226601,"FUNSD dataset, we focus our testing on the entity linking annotations as described in [21].
550"
REFERENCES,0.6197044334975369,"To eliminate the impact of randomness on evaluation, no sampling methods are employed during
551"
REFERENCES,0.6206896551724138,"testing for any of the models. Instead, beam search with a beam size of 1 is used for generation across
552"
REFERENCES,0.6216748768472906,"all models. Additionally, the maximum number of new tokens is set to 512, while the maximum
553"
REFERENCES,0.6226600985221675,"number of input tokens is set to 4096.
554"
REFERENCES,0.6236453201970443,"C
Discussion of Input Sequence Length
555"
REFERENCES,0.6246305418719211,"As mentioned in Section 4.6, it is intriguing that LayTextLLM has fewer input sequences than Do-
556"
REFERENCES,0.625615763546798,"cLLM, which is counterintuitive given that LayTextLLM interleaves bounding box tokens, typically
557"
REFERENCES,0.6266009852216748,"resulting in longer sequence lengths. We attribute this to the Byte Pair Encoding (BPE) tokenizers [54]
558"
REFERENCES,0.6275862068965518,"prevalently used in modern LLMs such as Llama2.
559"
REFERENCES,0.6285714285714286,"BPE operates by building a vocabulary of commonly occurring subwords (or token pieces) derived
560"
REFERENCES,0.6295566502463055,"from the training data. Initially, it tokenizes the text at the character level and then progressively
561"
REFERENCES,0.6305418719211823,"merges the most frequent adjacent pairs of characters or sequences. The objective is to strike a
562"
REFERENCES,0.6315270935960591,"balance between minimizing vocabulary size and maximizing encoding efficiency.
563"
REFERENCES,0.632512315270936,"Thus, when tokenizing a single word like “International” on its own, the tokenizer might identify it
564"
REFERENCES,0.6334975369458128,"as a common sequence in the training data and encode it as a single token. This is especially likely if
565"
REFERENCES,0.6344827586206897,"“International” frequently appears as a standalone word in the training contexts. However, when the
566"
REFERENCES,0.6354679802955665,"word “International” is part of a larger sequence of words such as including in a long sequence of
567"
REFERENCES,0.6364532019704433,"OCR-derived texts like “...335 CPC,International,Inc...”, the context changes. The tokenizer might
568"
REFERENCES,0.6374384236453202,"split “International” into sub-tokens like “Intern” and “ational” because, in various contexts within
569"
REFERENCES,0.638423645320197,"the training data, these subwords might appear more frequently in different combinations or are more
570"
REFERENCES,0.6394088669950739,"useful for the model to understand variations in meaning or syntax.
571"
REFERENCES,0.6403940886699507,"When using LayTextLLM, we input word-level OCR results into the tokenizer, typically resulting in
572"
REFERENCES,0.6413793103448275,"the former situation, where words are encoded as single tokens. Conversely, with DocLLM, the entire
573"
REFERENCES,0.6423645320197044,"OCR output is processed as one large sequence, leading to the latter situation and a longer sequence
574"
REFERENCES,0.6433497536945813,"length than in LayTextLLM. This difference underscores the utility of LayTextLLM in achieving
575"
REFERENCES,0.6443349753694582,"both accuracy and inference efficiency due to its shorter sequence length.
576"
REFERENCES,0.645320197044335,"D
Shuffle Ratio Exploration
577"
REFERENCES,0.6463054187192119,"Tab. 7 presents the results of exploring training and testing shuffling ratios on the FUNSD dataset
578"
REFERENCES,0.6472906403940887,"using two different models: Llama2-7B-base and LayTextLLM. The table shows the performance of
579"
REFERENCES,0.6482758620689655,"these models at various shuffling ratios (100%, 50%, 20%, and 0%).
580"
REFERENCES,0.6492610837438424,"LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further
581"
REFERENCES,0.6502463054187192,"underscores the significance of interleaving spatial layouts with text. Particularly at the 100% shuffle
582"
REFERENCES,0.6512315270935961,"level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a
583"
REFERENCES,0.6522167487684729,"relatively higher performance. It is also interesting to note that Llama2-7B-base generally improves as
584"
REFERENCES,0.6532019704433497,"the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples
585"
REFERENCES,0.6541871921182266,"with OCR-derived text are shuffled. This observation suggests that LayTextLLM effectively utilizes
586"
REFERENCES,0.6551724137931034,"spatial layouts and is less dependent on the sequence of input tokens. Therefore, a certain proportion
587"
REFERENCES,0.6561576354679803,"of shuffled examples can serve as adversarial examples to enhance the model’s robustness, addressing
588"
REFERENCES,0.6571428571428571,"situations such as errors in the text order from the OCR engine, which are caused by subtle differences
589"
REFERENCES,0.6581280788177339,"in horizontal or vertical coordinates.
590 FUNSD"
REFERENCES,0.6591133004926109,"Ratio
Llama2-7B-base
LayTextLLM"
REFERENCES,0.6600985221674877,"100%
20.3
44.7
50%
49.1
62.1
20%
50.2
65.4
0%
52.3
65.1
Table 7: Shuffling ratio exploration in FUNSD dataset."
REFERENCES,0.6610837438423646,"E
Results of ChartQA
591"
REFERENCES,0.6620689655172414,"As shown in Fig. 7, the question-answer pairs in ChartQA [43] tend to involve the visual cues for
592"
REFERENCES,0.6630541871921182,"reasoning. However, with only text and layout information as input, the proposed LayTextLLM
593"
REFERENCES,0.6640394088669951,"inevitably have difficulties in reasoning visual-related information. Thus, on the ChartQA dataset,
594"
REFERENCES,0.6650246305418719,"LayTextLLM can hardly achieve better performance than previous methods that include visual inputs.
595"
REFERENCES,0.6660098522167488,"Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability
596"
REFERENCES,0.6669950738916256,"than UniDoc [10]. After incorporating the training set of ChartQA, the performance of LayTextLLM
597"
REFERENCES,0.6679802955665025,"can be boosted to 35.7%. Considering the importance of visual cues in ChartQA-like tasks, we will
598"
REFERENCES,0.6689655172413793,"try to involve the visual information into LayTextLLM in future work.
599"
REFERENCES,0.6699507389162561,Question: What is the difference between
REFERENCES,0.670935960591133,the highest and the lowest green bar?
REFERENCES,0.6719211822660098,GroundTruth: 6
REFERENCES,0.6729064039408867,Our Prediction: 40
REFERENCES,0.6738916256157635,Figure 7: A failure case of LayTextLLM on CharQA.
REFERENCES,0.6748768472906403,ChartQA
REFERENCES,0.6758620689655173,"OCR-free
UniDoc [10]
10.9
DocPedia [9]
46.9∗"
REFERENCES,0.6768472906403941,"Monkey [49]
54.0∗"
REFERENCES,0.677832512315271,"InternVL [50]
45.6∗"
REFERENCES,0.6788177339901478,"InternLM-XComposer2 [15]
51.6∗"
REFERENCES,0.6798029556650246,"TextMonkey [12]
58.2∗"
REFERENCES,0.6807881773399015,"TextMonkey+ [12]
59.9∗"
REFERENCES,0.6817733990147783,"text + polys
LayTextLLMzero (Ours)
21.4
LayTextLLMvqa (Ours)
29.8∗"
REFERENCES,0.6827586206896552,"LayTextLLMall (Ours)
35.7∗"
REFERENCES,0.683743842364532,Table 8: Comparison with SOTA OCR-free MLLMs on ChartQA. ∗indicates the training set used.
REFERENCES,0.6847290640394089,"NeurIPS Paper Checklist
600"
CLAIMS,0.6857142857142857,"1. Claims
601"
CLAIMS,0.6866995073891625,"Question: Do the main claims made in the abstract and introduction accurately reflect the
602"
CLAIMS,0.6876847290640394,"paper’s contributions and scope?
603"
CLAIMS,0.6886699507389162,"Answer: [Yes]
604"
CLAIMS,0.6896551724137931,"Justification: We have detailed the contributions accurately in the abstract and introduction.
605"
CLAIMS,0.69064039408867,"Guidelines:
606"
CLAIMS,0.6916256157635468,"• The answer NA means that the abstract and introduction do not include the claims
607"
CLAIMS,0.6926108374384237,"made in the paper.
608"
CLAIMS,0.6935960591133005,"• The abstract and/or introduction should clearly state the claims made, including the
609"
CLAIMS,0.6945812807881774,"contributions made in the paper and important assumptions and limitations. A No or
610"
CLAIMS,0.6955665024630542,"NA answer to this question will not be perceived well by the reviewers.
611"
CLAIMS,0.696551724137931,"• The claims made should match theoretical and experimental results, and reflect how
612"
CLAIMS,0.6975369458128079,"much the results can be expected to generalize to other settings.
613"
CLAIMS,0.6985221674876847,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
614"
CLAIMS,0.6995073891625616,"are not attained by the paper.
615"
LIMITATIONS,0.7004926108374384,"2. Limitations
616"
LIMITATIONS,0.7014778325123153,"Question: Does the paper discuss the limitations of the work performed by the authors?
617"
LIMITATIONS,0.7024630541871921,"Answer: [Yes]
618"
LIMITATIONS,0.7034482758620689,"Justification: As discussed in Section. 5, we have listed some limitations of our work and
619"
LIMITATIONS,0.7044334975369458,"shown corresponding failure cases in the supplementary material.
620"
LIMITATIONS,0.7054187192118226,"Guidelines:
621"
LIMITATIONS,0.7064039408866996,"• The answer NA means that the paper has no limitation while the answer No means that
622"
LIMITATIONS,0.7073891625615764,"the paper has limitations, but those are not discussed in the paper.
623"
LIMITATIONS,0.7083743842364532,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
624"
LIMITATIONS,0.7093596059113301,"• The paper should point out any strong assumptions and how robust the results are to
625"
LIMITATIONS,0.7103448275862069,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
626"
LIMITATIONS,0.7113300492610838,"model well-specification, asymptotic approximations only holding locally). The authors
627"
LIMITATIONS,0.7123152709359606,"should reflect on how these assumptions might be violated in practice and what the
628"
LIMITATIONS,0.7133004926108374,"implications would be.
629"
LIMITATIONS,0.7142857142857143,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
630"
LIMITATIONS,0.7152709359605911,"only tested on a few datasets or with a few runs. In general, empirical results often
631"
LIMITATIONS,0.716256157635468,"depend on implicit assumptions, which should be articulated.
632"
LIMITATIONS,0.7172413793103448,"• The authors should reflect on the factors that influence the performance of the approach.
633"
LIMITATIONS,0.7182266009852217,"For example, a facial recognition algorithm may perform poorly when image resolution
634"
LIMITATIONS,0.7192118226600985,"is low or images are taken in low lighting. Or a speech-to-text system might not be
635"
LIMITATIONS,0.7201970443349753,"used reliably to provide closed captions for online lectures because it fails to handle
636"
LIMITATIONS,0.7211822660098522,"technical jargon.
637"
LIMITATIONS,0.722167487684729,"• The authors should discuss the computational efficiency of the proposed algorithms
638"
LIMITATIONS,0.723152709359606,"and how they scale with dataset size.
639"
LIMITATIONS,0.7241379310344828,"• If applicable, the authors should discuss possible limitations of their approach to
640"
LIMITATIONS,0.7251231527093596,"address problems of privacy and fairness.
641"
LIMITATIONS,0.7261083743842365,"• While the authors might fear that complete honesty about limitations might be used by
642"
LIMITATIONS,0.7270935960591133,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
643"
LIMITATIONS,0.7280788177339902,"limitations that aren’t acknowledged in the paper. The authors should use their best
644"
LIMITATIONS,0.729064039408867,"judgment and recognize that individual actions in favor of transparency play an impor-
645"
LIMITATIONS,0.7300492610837438,"tant role in developing norms that preserve the integrity of the community. Reviewers
646"
LIMITATIONS,0.7310344827586207,"will be specifically instructed to not penalize honesty concerning limitations.
647"
THEORY ASSUMPTIONS AND PROOFS,0.7320197044334975,"3. Theory Assumptions and Proofs
648"
THEORY ASSUMPTIONS AND PROOFS,0.7330049261083744,"Question: For each theoretical result, does the paper provide the full set of assumptions and
649"
THEORY ASSUMPTIONS AND PROOFS,0.7339901477832512,"a complete (and correct) proof?
650"
THEORY ASSUMPTIONS AND PROOFS,0.734975369458128,"Answer: [NA]
651"
THEORY ASSUMPTIONS AND PROOFS,0.7359605911330049,"Justification: This work does not include theoretical results.
652"
THEORY ASSUMPTIONS AND PROOFS,0.7369458128078817,"Guidelines:
653"
THEORY ASSUMPTIONS AND PROOFS,0.7379310344827587,"• The answer NA means that the paper does not include theoretical results.
654"
THEORY ASSUMPTIONS AND PROOFS,0.7389162561576355,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
655"
THEORY ASSUMPTIONS AND PROOFS,0.7399014778325124,"referenced.
656"
THEORY ASSUMPTIONS AND PROOFS,0.7408866995073892,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
657"
THEORY ASSUMPTIONS AND PROOFS,0.741871921182266,"• The proofs can either appear in the main paper or the supplemental material, but if
658"
THEORY ASSUMPTIONS AND PROOFS,0.7428571428571429,"they appear in the supplemental material, the authors are encouraged to provide a short
659"
THEORY ASSUMPTIONS AND PROOFS,0.7438423645320197,"proof sketch to provide intuition.
660"
THEORY ASSUMPTIONS AND PROOFS,0.7448275862068966,"• Inversely, any informal proof provided in the core of the paper should be complemented
661"
THEORY ASSUMPTIONS AND PROOFS,0.7458128078817734,"by formal proofs provided in appendix or supplemental material.
662"
THEORY ASSUMPTIONS AND PROOFS,0.7467980295566502,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7477832512315271,"4. Experimental Result Reproducibility
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7487684729064039,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497536945812808,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507389162561576,"of the paper (regardless of whether the code and data are provided or not)?
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7517241379310344,"Answer: [Yes]
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7527093596059113,"Justification: In Section 4.1, 4.2 and Appendix B, we have described the details of imple-
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7536945812807881,"menting and training the proposed model to ensure the reproducibility of our work.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546798029556651,"Guidelines:
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7556650246305419,"• The answer NA means that the paper does not include experiments.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566502463054188,"• If the paper includes experiments, a No answer to this question will not be perceived
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576354679802956,"well by the reviewers: Making the paper reproducible is important, regardless of
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7586206896551724,"whether the code and data are provided or not.
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596059113300493,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7605911330049261,"to make their results reproducible or verifiable.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761576354679803,"• Depending on the contribution, reproducibility can be accomplished in various ways.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7625615763546798,"For example, if the contribution is a novel architecture, describing the architecture fully
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7635467980295566,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7645320197044335,"be necessary to either make it possible for others to replicate the model with the same
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7655172413793103,"dataset, or provide access to the model. In general. releasing code and data is often
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7665024630541872,"one good way to accomplish this, but reproducibility can also be provided via detailed
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767487684729064,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7684729064039408,"of a large language model), releasing of a model checkpoint, or other means that are
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7694581280788177,"appropriate to the research performed.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704433497536946,"• While NeurIPS does not require releasing code, the conference does require all submis-
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714285714285715,"sions to provide some reasonable avenue for reproducibility, which may depend on the
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724137931034483,"nature of the contribution. For example
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733990147783252,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.774384236453202,"to reproduce that algorithm.
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7753694581280788,"(b) If the contribution is primarily a new model architecture, the paper should describe
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7763546798029557,"the architecture clearly and fully.
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7773399014778325,"(c) If the contribution is a new model (e.g., a large language model), then there should
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7783251231527094,"either be a way to access this model for reproducing the results or a way to reproduce
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7793103448275862,"the model (e.g., with an open-source dataset or instructions for how to construct
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.780295566502463,"the dataset).
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7812807881773399,"(d) We recognize that reproducibility may be tricky in some cases, in which case
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7822660098522167,"authors are welcome to describe the particular way they provide for reproducibility.
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832512315270936,"In the case of closed-source models, it may be that access to the model is limited in
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842364532019704,"some way (e.g., to registered users), but it should be possible for other researchers
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852216748768472,"to have some path to reproducing or verifying the results.
702"
OPEN ACCESS TO DATA AND CODE,0.7862068965517242,"5. Open access to data and code
703"
OPEN ACCESS TO DATA AND CODE,0.787192118226601,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
704"
OPEN ACCESS TO DATA AND CODE,0.7881773399014779,"tions to faithfully reproduce the main experimental results, as described in supplemental
705"
OPEN ACCESS TO DATA AND CODE,0.7891625615763547,"material?
706"
OPEN ACCESS TO DATA AND CODE,0.7901477832512315,"Answer: [No]
707"
OPEN ACCESS TO DATA AND CODE,0.7911330049261084,"Justification: We would release our code after this paper is accepted.
708"
OPEN ACCESS TO DATA AND CODE,0.7921182266009852,"Guidelines:
709"
OPEN ACCESS TO DATA AND CODE,0.7931034482758621,"• The answer NA means that paper does not include experiments requiring code.
710"
OPEN ACCESS TO DATA AND CODE,0.7940886699507389,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
711"
OPEN ACCESS TO DATA AND CODE,0.7950738916256158,"public/guides/CodeSubmissionPolicy) for more details.
712"
OPEN ACCESS TO DATA AND CODE,0.7960591133004926,"• While we encourage the release of code and data, we understand that this might not be
713"
OPEN ACCESS TO DATA AND CODE,0.7970443349753694,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
714"
OPEN ACCESS TO DATA AND CODE,0.7980295566502463,"including code, unless this is central to the contribution (e.g., for a new open-source
715"
OPEN ACCESS TO DATA AND CODE,0.7990147783251231,"benchmark).
716"
OPEN ACCESS TO DATA AND CODE,0.8,"• The instructions should contain the exact command and environment needed to run to
717"
OPEN ACCESS TO DATA AND CODE,0.8009852216748768,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
718"
OPEN ACCESS TO DATA AND CODE,0.8019704433497536,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
719"
OPEN ACCESS TO DATA AND CODE,0.8029556650246306,"• The authors should provide instructions on data access and preparation, including how
720"
OPEN ACCESS TO DATA AND CODE,0.8039408866995074,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
721"
OPEN ACCESS TO DATA AND CODE,0.8049261083743843,"• The authors should provide scripts to reproduce all experimental results for the new
722"
OPEN ACCESS TO DATA AND CODE,0.8059113300492611,"proposed method and baselines. If only a subset of experiments are reproducible, they
723"
OPEN ACCESS TO DATA AND CODE,0.8068965517241379,"should state which ones are omitted from the script and why.
724"
OPEN ACCESS TO DATA AND CODE,0.8078817733990148,"• At submission time, to preserve anonymity, the authors should release anonymized
725"
OPEN ACCESS TO DATA AND CODE,0.8088669950738916,"versions (if applicable).
726"
OPEN ACCESS TO DATA AND CODE,0.8098522167487685,"• Providing as much information as possible in supplemental material (appended to the
727"
OPEN ACCESS TO DATA AND CODE,0.8108374384236453,"paper) is recommended, but including URLs to data and code is permitted.
728"
OPEN ACCESS TO DATA AND CODE,0.8118226600985222,"6. Experimental Setting/Details
729"
OPEN ACCESS TO DATA AND CODE,0.812807881773399,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
730"
OPEN ACCESS TO DATA AND CODE,0.8137931034482758,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
731"
OPEN ACCESS TO DATA AND CODE,0.8147783251231527,"results?
732"
OPEN ACCESS TO DATA AND CODE,0.8157635467980295,"Answer: [Yes]
733"
OPEN ACCESS TO DATA AND CODE,0.8167487684729065,"Justification: We have detailed the experimental setting and implementation details in
734"
OPEN ACCESS TO DATA AND CODE,0.8177339901477833,"Section 4.1, 4.2 and Appendix B of the supplementary material.
735"
OPEN ACCESS TO DATA AND CODE,0.81871921182266,"Guidelines:
736"
OPEN ACCESS TO DATA AND CODE,0.819704433497537,"• The answer NA means that the paper does not include experiments.
737"
OPEN ACCESS TO DATA AND CODE,0.8206896551724138,"• The experimental setting should be presented in the core of the paper to a level of detail
738"
OPEN ACCESS TO DATA AND CODE,0.8216748768472907,"that is necessary to appreciate the results and make sense of them.
739"
OPEN ACCESS TO DATA AND CODE,0.8226600985221675,"• The full details can be provided either with the code, in appendix, or as supplemental
740"
OPEN ACCESS TO DATA AND CODE,0.8236453201970443,"material.
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8246305418719212,"7. Experiment Statistical Significance
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825615763546798,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8266009852216749,"information about the statistical significance of the experiments?
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275862068965517,"Answer: [No]
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8285714285714286,"Justification: Our deep learning model is designed for a complex task (requiring huge
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295566502463054,"computing resources) where traditional error bars are less informative due to the high
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8305418719211822,"variability in model training and initialization. We ensured the robustness of our model
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8315270935960591,"by fix the random seed during inference. In addition, comparative analysis with baseline
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8325123152709359,"models demonstrated improvements in key performance areas, underscoring the practical
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8334975369458129,"effectiveness of our approach. We acknowledge the limitation of not using traditional
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8344827586206897,"statistical tests and suggest that future work could explore statistical significance in more
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8354679802955665,"controlled settings.
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364532019704434,"Guidelines:
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374384236453202,"• The answer NA means that the paper does not include experiments.
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384236453201971,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8394088669950739,"dence intervals, or statistical significance tests, at least for the experiments that support
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8403940886699507,"the main claims of the paper.
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8413793103448276,"• The factors of variability that the error bars are capturing should be clearly stated (for
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423645320197044,"example, train/test split, initialization, random drawing of some parameter, or overall
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433497536945813,"run with given experimental conditions).
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8443349753694581,"• The method for calculating the error bars should be explained (closed form formula,
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8453201970443349,"call to a library function, bootstrap, etc.)
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463054187192118,"• The assumptions made should be given (e.g., Normally distributed errors).
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472906403940886,"• It should be clear whether the error bar is the standard deviation or the standard error
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482758620689655,"of the mean.
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492610837438423,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8502463054187193,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8512315270935961,"of Normality of errors is not verified.
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522167487684729,"• For asymmetric distributions, the authors should be careful not to show in tables or
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8532019704433498,"figures symmetric error bars that would yield results that are out of range (e.g. negative
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8541871921182266,"error rates).
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8551724137931035,"• If error bars are reported in tables or plots, The authors should explain in the text how
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561576354679803,"they were calculated and reference the corresponding figures or tables in the text.
774"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"8. Experiments Compute Resources
775"
EXPERIMENTS COMPUTE RESOURCES,0.858128078817734,"Question: For each experiment, does the paper provide sufficient information on the com-
776"
EXPERIMENTS COMPUTE RESOURCES,0.8591133004926108,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
777"
EXPERIMENTS COMPUTE RESOURCES,0.8600985221674877,"the experiments?
778"
EXPERIMENTS COMPUTE RESOURCES,0.8610837438423645,"Answer: [Yes]
779"
EXPERIMENTS COMPUTE RESOURCES,0.8620689655172413,"Justification: We have reported the needed computer resources in Section B of the supple-
780"
EXPERIMENTS COMPUTE RESOURCES,0.8630541871921182,"mentary material.
781"
EXPERIMENTS COMPUTE RESOURCES,0.864039408866995,"Guidelines:
782"
EXPERIMENTS COMPUTE RESOURCES,0.865024630541872,"• The answer NA means that the paper does not include experiments.
783"
EXPERIMENTS COMPUTE RESOURCES,0.8660098522167488,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
784"
EXPERIMENTS COMPUTE RESOURCES,0.8669950738916257,"or cloud provider, including relevant memory and storage.
785"
EXPERIMENTS COMPUTE RESOURCES,0.8679802955665025,"• The paper should provide the amount of compute required for each of the individual
786"
EXPERIMENTS COMPUTE RESOURCES,0.8689655172413793,"experimental runs as well as estimate the total compute.
787"
EXPERIMENTS COMPUTE RESOURCES,0.8699507389162562,"• The paper should disclose whether the full research project required more compute
788"
EXPERIMENTS COMPUTE RESOURCES,0.870935960591133,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
789"
EXPERIMENTS COMPUTE RESOURCES,0.8719211822660099,"didn’t make it into the paper).
790"
CODE OF ETHICS,0.8729064039408867,"9. Code Of Ethics
791"
CODE OF ETHICS,0.8738916256157635,"Question: Does the research conducted in the paper conform, in every respect, with the
792"
CODE OF ETHICS,0.8748768472906404,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
793"
CODE OF ETHICS,0.8758620689655172,"Answer: [Yes]
794"
CODE OF ETHICS,0.8768472906403941,"Justification: The research in this paper conforms with the NeurIPS Code of Ethics in every
795"
CODE OF ETHICS,0.8778325123152709,"respect.
796"
CODE OF ETHICS,0.8788177339901477,"Guidelines:
797"
CODE OF ETHICS,0.8798029556650246,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
798"
CODE OF ETHICS,0.8807881773399014,"• If the authors answer No, they should explain the special circumstances that require a
799"
CODE OF ETHICS,0.8817733990147784,"deviation from the Code of Ethics.
800"
CODE OF ETHICS,0.8827586206896552,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
801"
CODE OF ETHICS,0.8837438423645321,"eration due to laws or regulations in their jurisdiction).
802"
BROADER IMPACTS,0.8847290640394089,"10. Broader Impacts
803"
BROADER IMPACTS,0.8857142857142857,"Question: Does the paper discuss both potential positive societal impacts and negative
804"
BROADER IMPACTS,0.8866995073891626,"societal impacts of the work performed?
805"
BROADER IMPACTS,0.8876847290640394,"Answer: [NA]
806"
BROADER IMPACTS,0.8886699507389163,"Justification: There is no societal impact of the work performed.
807"
BROADER IMPACTS,0.8896551724137931,"Guidelines:
808"
BROADER IMPACTS,0.8906403940886699,"• The answer NA means that there is no societal impact of the work performed.
809"
BROADER IMPACTS,0.8916256157635468,"• If the authors answer NA or No, they should explain why their work has no societal
810"
BROADER IMPACTS,0.8926108374384236,"impact or why the paper does not address societal impact.
811"
BROADER IMPACTS,0.8935960591133005,"• Examples of negative societal impacts include potential malicious or unintended uses
812"
BROADER IMPACTS,0.8945812807881773,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
813"
BROADER IMPACTS,0.8955665024630541,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
814"
BROADER IMPACTS,0.896551724137931,"groups), privacy considerations, and security considerations.
815"
BROADER IMPACTS,0.8975369458128079,"• The conference expects that many papers will be foundational research and not tied
816"
BROADER IMPACTS,0.8985221674876848,"to particular applications, let alone deployments. However, if there is a direct path to
817"
BROADER IMPACTS,0.8995073891625616,"any negative applications, the authors should point it out. For example, it is legitimate
818"
BROADER IMPACTS,0.9004926108374385,"to point out that an improvement in the quality of generative models could be used to
819"
BROADER IMPACTS,0.9014778325123153,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
820"
BROADER IMPACTS,0.9024630541871921,"that a generic algorithm for optimizing neural networks could enable people to train
821"
BROADER IMPACTS,0.903448275862069,"models that generate Deepfakes faster.
822"
BROADER IMPACTS,0.9044334975369458,"• The authors should consider possible harms that could arise when the technology is
823"
BROADER IMPACTS,0.9054187192118227,"being used as intended and functioning correctly, harms that could arise when the
824"
BROADER IMPACTS,0.9064039408866995,"technology is being used as intended but gives incorrect results, and harms following
825"
BROADER IMPACTS,0.9073891625615763,"from (intentional or unintentional) misuse of the technology.
826"
BROADER IMPACTS,0.9083743842364532,"• If there are negative societal impacts, the authors could also discuss possible mitigation
827"
BROADER IMPACTS,0.90935960591133,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
828"
BROADER IMPACTS,0.9103448275862069,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
829"
BROADER IMPACTS,0.9113300492610837,"feedback over time, improving the efficiency and accessibility of ML).
830"
SAFEGUARDS,0.9123152709359605,"11. Safeguards
831"
SAFEGUARDS,0.9133004926108375,"Question: Does the paper describe safeguards that have been put in place for responsible
832"
SAFEGUARDS,0.9142857142857143,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
833"
SAFEGUARDS,0.9152709359605912,"image generators, or scraped datasets)?
834"
SAFEGUARDS,0.916256157635468,"Answer: [NA]
835"
SAFEGUARDS,0.9172413793103448,"Justification: All the datasets used in this paper are publicly available and they contain no
836"
SAFEGUARDS,0.9182266009852217,"unsafe images.
837"
SAFEGUARDS,0.9192118226600985,"Guidelines:
838"
SAFEGUARDS,0.9201970443349754,"• The answer NA means that the paper poses no such risks.
839"
SAFEGUARDS,0.9211822660098522,"• Released models that have a high risk for misuse or dual-use should be released with
840"
SAFEGUARDS,0.9221674876847291,"necessary safeguards to allow for controlled use of the model, for example by requiring
841"
SAFEGUARDS,0.9231527093596059,"that users adhere to usage guidelines or restrictions to access the model or implementing
842"
SAFEGUARDS,0.9241379310344827,"safety filters.
843"
SAFEGUARDS,0.9251231527093596,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
844"
SAFEGUARDS,0.9261083743842364,"should describe how they avoided releasing unsafe images.
845"
SAFEGUARDS,0.9270935960591133,"• We recognize that providing effective safeguards is challenging, and many papers do
846"
SAFEGUARDS,0.9280788177339901,"not require this, but we encourage authors to take this into account and make a best
847"
SAFEGUARDS,0.929064039408867,"faith effort.
848"
LICENSES FOR EXISTING ASSETS,0.9300492610837439,"12. Licenses for existing assets
849"
LICENSES FOR EXISTING ASSETS,0.9310344827586207,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
850"
LICENSES FOR EXISTING ASSETS,0.9320197044334976,"the paper, properly credited and are the license and terms of use explicitly mentioned and
851"
LICENSES FOR EXISTING ASSETS,0.9330049261083744,"properly respected?
852"
LICENSES FOR EXISTING ASSETS,0.9339901477832512,"Answer: [Yes]
853"
LICENSES FOR EXISTING ASSETS,0.9349753694581281,"Justification: For the used datasets and pre-trained models, we have cited their corresponding
854"
LICENSES FOR EXISTING ASSETS,0.9359605911330049,"works.
855"
LICENSES FOR EXISTING ASSETS,0.9369458128078818,"Guidelines:
856"
LICENSES FOR EXISTING ASSETS,0.9379310344827586,"• The answer NA means that the paper does not use existing assets.
857"
LICENSES FOR EXISTING ASSETS,0.9389162561576355,"• The authors should cite the original paper that produced the code package or dataset.
858"
LICENSES FOR EXISTING ASSETS,0.9399014778325123,"• The authors should state which version of the asset is used and, if possible, include a
859"
LICENSES FOR EXISTING ASSETS,0.9408866995073891,"URL.
860"
LICENSES FOR EXISTING ASSETS,0.941871921182266,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
861"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"• For scraped data from a particular source (e.g., website), the copyright and terms of
862"
LICENSES FOR EXISTING ASSETS,0.9438423645320198,"service of that source should be provided.
863"
LICENSES FOR EXISTING ASSETS,0.9448275862068966,"• If assets are released, the license, copyright information, and terms of use in the
864"
LICENSES FOR EXISTING ASSETS,0.9458128078817734,"package should be provided. For popular datasets, paperswithcode.com/datasets
865"
LICENSES FOR EXISTING ASSETS,0.9467980295566503,"has curated licenses for some datasets. Their licensing guide can help determine the
866"
LICENSES FOR EXISTING ASSETS,0.9477832512315271,"license of a dataset.
867"
LICENSES FOR EXISTING ASSETS,0.948768472906404,"• For existing datasets that are re-packaged, both the original license and the license of
868"
LICENSES FOR EXISTING ASSETS,0.9497536945812808,"the derived asset (if it has changed) should be provided.
869"
LICENSES FOR EXISTING ASSETS,0.9507389162561576,"• If this information is not available online, the authors are encouraged to reach out to
870"
LICENSES FOR EXISTING ASSETS,0.9517241379310345,"the asset’s creators.
871"
NEW ASSETS,0.9527093596059113,"13. New Assets
872"
NEW ASSETS,0.9536945812807882,"Question: Are new assets introduced in the paper well documented and is the documentation
873"
NEW ASSETS,0.954679802955665,"provided alongside the assets?
874"
NEW ASSETS,0.9556650246305419,"Answer: [NA]
875"
NEW ASSETS,0.9566502463054187,"Justification: This paper does not release new assets
876"
NEW ASSETS,0.9576354679802955,"Guidelines:
877"
NEW ASSETS,0.9586206896551724,"• The answer NA means that the paper does not release new assets.
878"
NEW ASSETS,0.9596059113300492,"• Researchers should communicate the details of the dataset/code/model as part of their
879"
NEW ASSETS,0.9605911330049262,"submissions via structured templates. This includes details about training, license,
880"
NEW ASSETS,0.961576354679803,"limitations, etc.
881"
NEW ASSETS,0.9625615763546798,"• The paper should discuss whether and how consent was obtained from people whose
882"
NEW ASSETS,0.9635467980295567,"asset is used.
883"
NEW ASSETS,0.9645320197044335,"• At submission time, remember to anonymize your assets (if applicable). You can either
884"
NEW ASSETS,0.9655172413793104,"create an anonymized URL or include an anonymized zip file.
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665024630541872,"14. Crowdsourcing and Research with Human Subjects
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967487684729064,"Question: For crowdsourcing experiments and research with human subjects, does the paper
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684729064039409,"include the full text of instructions given to participants and screenshots, if applicable, as
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9694581280788177,"well as details about compensation (if any)?
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704433497536946,"Answer: [NA]
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724137931034482,"Guidelines:
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733990147783251,"• The answer NA means that the paper does not involve crowdsourcing nor research with
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743842364532019,"human subjects.
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753694581280788,"• Including this information in the supplemental material is fine, but if the main contribu-
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763546798029556,"tion of the paper involves human subjects, then as much detail as possible should be
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773399014778326,"included in the main paper.
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783251231527094,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793103448275862,"or other labor should be paid at least the minimum wage in the country of the data
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802955665024631,"collector.
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812807881773399,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822660098522168,"Subjects
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832512315270936,"Question: Does the paper describe potential risks incurred by study participants, whether
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842364532019704,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852216748768473,"approvals (or an equivalent approval/review based on the requirements of your country or
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862068965517241,"institution) were obtained?
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987192118226601,"Answer: [NA]
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881773399014778,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891625615763546,"Guidelines:
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901477832512315,"• The answer NA means that the paper does not involve crowdsourcing nor research with
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911330049261083,"human subjects.
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921182266009853,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993103448275862,"may be required for any human subjects research. If you obtained IRB approval, you
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994088669950739,"should clearly state this in the paper.
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950738916256158,"• We recognize that the procedures for this may vary significantly between institutions
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960591133004926,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970443349753695,"guidelines for their institution.
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980295566502463,"• For initial submissions, do not include any information that would break anonymity (if
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990147783251232,"applicable), such as the institution conducting the review.
919"
