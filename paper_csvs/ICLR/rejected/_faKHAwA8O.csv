Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013297872340425532,"A library of diverse expert models transfers better to a novel task than a single
generalist model. However, growing such a library indeﬁnitely is impractical.
Hence, we explore the problem of learning a consolidated image feature represen-
tation from a collection of related task-speciﬁc teachers that transfer well on novel
recognition tasks. This differs from traditional knowledge distillation in which
a student model is trained to emulate the input/output functionality of a teacher.
Indeed, we observe experimentally that standard distillation of task-speciﬁc teach-
ers, or using these teacher representations directly, reduces downstream transfer-
ability compared to a task-agnostic generalist model. We show that a simple multi-
head, multi-task distillation method using an unlabeled proxy dataset and adding
a generalist teacher is sufﬁcient to consolidate representations from task-speciﬁc
teacher(s). We improve downstream performance, outperforming the teacher (or
best of all teachers) as well as the strong baseline of ImageNet pre-trained fea-
tures. Our method almost reaches the performance of a multi-task joint training
oracle, reaping the beneﬁt of the teachers without replaying their training data."
INTRODUCTION,0.0026595744680851063,"1
INTRODUCTION"
INTRODUCTION,0.003989361702127659,"A promising approach to scale transfer learning to diverse downstream vision tasks is to maintain
a library of diverse experts pre-trained on different tasks. (Deshpande et al., 2021) When presented
with a novel downstream task, one can select an appropriate expert and ﬁne-tune the representation
with small amounts of task-speciﬁc data. This strategy has many practical beneﬁts: ﬁne-tuning a
task-relevant pre-trained representation is fast, there is no need to store or revisit expert pre-training
data. How is such a library of expert representations populated and maintained? Previous work
(e.g., Puigcerver et al. (2020); Achille et al. (2019); Deshpande et al. (2021)) has assumed a static
collection of experts created by training on domain-speciﬁc datasets or domain-speciﬁc subsets of
general dataset. Instead, we would like to automatically enrich the diversity of the expert library by
accumulating knowledge from transferred downstream tasks, so that the overall system performance
continually increases over time (life-long meta-learning)."
INTRODUCTION,0.005319148936170213,"One approach to growing the library would be to add ﬁne-tuned downstream task-speciﬁc models
back into the library as a candidate expert for transfer on future tasks. Such a na¨ıve approach
clearly does not scale and, at a minimum, requires developing techniques for selecting experts that
is sub-linear in the size of the library (Puigcerver et al., 2020; Achille et al., 2019; Deshpande et al.,
2021). More importantly, as our experiments show in Sec. 4, task-speciﬁc models ﬁne-tuned on
small amounts of data do not provide transferable representations. Task-speciﬁc models tend to
overspecialize and degrade under-utilized features in their representations, and thus under-perform
on new tasks compared to generic pre-trained models."
INTRODUCTION,0.006648936170212766,"To address this, we introduce representation consolidation where the goal is to consolidate knowl-
edge from multiple task-speciﬁc teacher models into a single expert student representation which
transfers to downstream tasks better than any of the individual teacher representations. There are
three factors that make the problem of representation consolidation unique relative to the existing
literature: (1) we assume a representation is to be consolidated from a collection of multiple task-
speciﬁc models, (2) we avoid the need to revisit task-speciﬁc training data of these models, and (3)
we focus on improving downstream transfer instead of simply replicating performance on the up-
stream teacher’s task(s). To the best of our knowledge, no prior work has analyzed the downstream
transfer aspect of distilled domain-expert representations."
INTRODUCTION,0.007978723404255319,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.009308510638297872,evaluate &
INTRODUCTION,0.010638297872340425,deploy
INTRODUCTION,0.011968085106382979,"teacher
student"
INTRODUCTION,0.013297872340425532,(proxy) dataset
INTRODUCTION,0.014627659574468085,soft label
INTRODUCTION,0.015957446808510637,"ℒ!""#$""%%"
INTRODUCTION,0.017287234042553192,(proxy) dataset
INTRODUCTION,0.018617021276595744,"ℒ!""#$""%%"
INTRODUCTION,0.0199468085106383,teacher 1
INTRODUCTION,0.02127659574468085,teacher N
INTRODUCTION,0.022606382978723406,"soft label
soft label"
INTRODUCTION,0.023936170212765957,"…
student …"
INTRODUCTION,0.02526595744680851,evaluate &
INTRODUCTION,0.026595744680851064,deploy
INTRODUCTION,0.027925531914893616,target
INTRODUCTION,0.02925531914893617,target dataset
INTRODUCTION,0.030585106382978722,fine-tune
INTRODUCTION,0.031914893617021274,"or
fixed 
backbone"
INTRODUCTION,0.03324468085106383,evaluate &
INTRODUCTION,0.034574468085106384,deploy
INTRODUCTION,0.03590425531914894,"(a) Traditional (proxy) distillation
(b) Traditional (proxy) distillation w/ multi-task"
INTRODUCTION,0.03723404255319149,(c) Traditional (proxy) multi-task distill used for transfer
INTRODUCTION,0.03856382978723404,target
INTRODUCTION,0.0398936170212766,target dataset
INTRODUCTION,0.041223404255319146,fine-tune
INTRODUCTION,0.0425531914893617,"or
fixed 
backbone"
INTRODUCTION,0.043882978723404256,evaluate &
INTRODUCTION,0.04521276595744681,deploy
INTRODUCTION,0.04654255319148936,(d) Representation consolidation (ours)
INTRODUCTION,0.047872340425531915,generalist
INTRODUCTION,0.04920212765957447,teacher
INTRODUCTION,0.05053191489361702,(proxy) dataset
INTRODUCTION,0.05186170212765957,soft label
INTRODUCTION,0.05319148936170213,"ℒ!""#$""%%"
INTRODUCTION,0.05452127659574468,teacher 1
INTRODUCTION,0.05585106382978723,teacher
INTRODUCTION,0.057180851063829786,"N
student"
INTRODUCTION,0.05851063829787234,"soft label
soft label … …
…"
INTRODUCTION,0.0598404255319149,(proxy) dataset
INTRODUCTION,0.061170212765957445,"ℒ!""#$""%%"
INTRODUCTION,0.0625,teacher 1
INTRODUCTION,0.06382978723404255,teacher N
INTRODUCTION,0.06515957446808511,"soft label
soft label"
INTRODUCTION,0.06648936170212766,"…
student …"
INTRODUCTION,0.0678191489361702,"Figure 1: Knowledge distillation (a,b) seeks to copy the end-to-end functionality of one or more
teachers but often fails to transfer (c). We propose a simple yet effective method to learn a consol-
idated representation from N teachers that transfers well to downstream tasks (d). Given a large
unlabeled proxy dataset, we train a single student model using multi-task distillation with a separate
classiﬁer head for each of the teacher tasks. To limit student forgetting and representation collapse,
we always include an additional generalist teacher (ImageNet model). We show that the resulting
consolidated representation transfers better to downstream tasks than any of the individual teachers
(including the generalist)."
INTRODUCTION,0.06914893617021277,"To carry out representation consolidation, we utilize multi-teacher multi-task model distillation (see
Fig. 1). We use a simple yet effective method to jointly distill one or several task-speciﬁc teacher
with a generalist one. Each teacher may operate on a different set of classes, and a multi-head
student is trained to emulate all teachers. Previous work on knowledge distillation has focused on
the student model’s performance on the teacher’s task. Instead, we evaluate how well the student
representation generalizes to new downstream tasks (whether related or unrelated to the teachers’
tasks). In this new setting we demonstrate several surprising results:"
INTRODUCTION,0.07047872340425532,• Task-speciﬁc model representations often transfer poorly even to a related downstream task.
INTRODUCTION,0.07180851063829788,"• Consolidating a task-speciﬁc teacher with a generalist teacher (ImageNet) is sufﬁcient to
rescue the student. The resulting representation matches or improves the downstream
performance over both task-speciﬁc and generalist representations, getting the best of both
worlds even though no task-speciﬁc data is replayed."
INTRODUCTION,0.07313829787234043,"• Consolidating multiple related task-speciﬁc teacher models can yield a better student rep-
resentation that exceeds the performance of any one teacher on downstream tasks."
INTRODUCTION,0.07446808510638298,"• Consolidation performs similarly to a multi-task joint training oracle, therefore gaining
almost all of the transfer beneﬁt from the teacher’s dataset without needing to replay it."
INTRODUCTION,0.07579787234042554,"Symbol
Deﬁnition"
INTRODUCTION,0.07712765957446809,"N
The number of tasks / task-speciﬁc teachers
Di
t
The i-th task-speciﬁc teacher’s dataset
φi
t
The i-th task-speciﬁc teacher’s backbone
(i = 0: the generalist’s backbone)
hi
t
The i-th task-speciﬁc teacher’s head (classiﬁer layer)
(i = 0: the generalist’s head)
Dproxy
The large unlabeled proxy dataset used for
distillation/consolidation
φs
The distilled/consolidated student’s backbone
hi
s
The distilled/consolidated student’s i-th head (classiﬁer)"
INTRODUCTION,0.07845744680851063,"Symbol
Deﬁnition"
INTRODUCTION,0.0797872340425532,"Dj
d
The j-th downstream task’s dataset
φj
d
The consolidated student backbone ﬁne-tuned on dataset Dj
d
hj
d
The downstream model’s head (classiﬁer layer
or linear SVM) for dataset Dj
d
λi
Loss weight for the i-th teacher
t-split
First 50% random split of classes of a dataset, used as Di
t
d-split
Second 50% random split of classes of a dataset, used as Dj
d"
INTRODUCTION,0.08111702127659574,Table 1: Glossary of symbols.
INTRODUCTION,0.08244680851063829,Under review as a conference paper at ICLR 2022
REPRESENTATION CONSOLIDATION,0.08377659574468085,"2
REPRESENTATION CONSOLIDATION"
REPRESENTATION CONSOLIDATION,0.0851063829787234,"Problem statement.
We start with a collection of one or more task-speciﬁc image classiﬁcation
models {Mi
t}N
i=1 as teachers, trained on corresponding datasets {Di
t } belonging to some domain
(satellite images, images of ﬂowers, etc.). We assume models consist of a feature extractor or back-
bone φi
t(·), composed with a classiﬁer head hi
t(·) so that Mi
t = hi
t(φi
t(·)). We consolidate the knowl-
edge of these task-speciﬁc teachers into a single student representation φs(·) using a proxy dataset
Dproxy (e.g., ImageNet), and then evaluate φs(·) by training a SVM on top of it (or ﬁne-tuning it)
on a given downstream Dj
d chosen from some set {Dj
d}. Our goal is that the resulting downstream
model hj
d(φj
d(·)) achieves good performance, where φj
d denotes the student φs as-is or potentially
after tuning on Dj
d. Fig. 1 highlights how this differs from standard distillation in which the student
model hs(φs(·)) is simply evaluated on the same task its teachers were once trained to perform."
REPRESENTATION CONSOLIDATION,0.08643617021276596,"Forgetting and representation collapse during distillation.
Though our problem statement is
novel, can existing approaches solve our problem statement? We observe (Sec. 4) that neither the
task-speciﬁc teachers nor students distilled with standard or state-of-the-art methods provide sufﬁ-
ciently transferable representations. They under-perform general pre-training representations (e.g.
with ImageNet) when evaluated on downstream tasks from the teachers’ domain, and drastically
under-perform on tasks outside the teachers domain. Surprisingly, this holds true even though we
use ImageNet-pretrained weights to initialize the teacher and student networks. Thus, we argue that
training task-speciﬁc teachers or distilling from them suffers from catastrophic forgetting of general
knowledge that is crucial for transfer learning."
REPRESENTATION CONSOLIDATION,0.08776595744680851,"Intuitively, transfer performance depends on how distinguishable different classes are represented
in the penultimate layer feature space. In standard distillation, the student only learns from task-
speciﬁc teachers trained on smaller datasets and is only required to discriminate the classes in those
datasets. This is well suited for the traditional distillation problem that evaluates the student on
these same tasks. But for representation consolidation, distinguishing unknown downstream classes
requires preserving general features that may not be relevant to the teachers’ speciﬁc task. Our
strategy is thus to ensure that the student maintains general features as it learns task-speciﬁc ones."
REPRESENTATION CONSOLIDATION,0.08909574468085106,"Method formulation.
A key challenge to our approach is avoiding replay of the images used to
train the task-speciﬁc teachers. To overcome this, we propose using a proxy dataset and a generalist
model that transfers well (e.g. one pre-trained on ImageNet). This model is referred to as a generalist
as it is not task speciﬁc and contains a wide variety of classes, and use it to help avoid forgetting.
More speciﬁcally, we use multi-head multi-task distillation. We put N + 1 heads, h0
s , . . . , hN
s , on
top of the student backbone φs(·). In addition to the task-speciﬁc teachers hi
t(φi
t(·)), i ∈{1, . . . , N}
used in traditional distillation, we also include the generalist teacher trained on ImageNet (denoted
as h0
t (φ0
t (·))). The rationale behind this construction is that we learn task knowledge from the
task-speciﬁc model on the proxy dataset, but also force the student to retain knowledge from the
generalist teacher to prevent over-specialization. We learn the student by optimizing the loss: L =
X"
REPRESENTATION CONSOLIDATION,0.09042553191489362,x∈Dproxy
REPRESENTATION CONSOLIDATION,0.09175531914893617,"
λ0 Ldistill
 
h0
t (φ0
t (x)), h0
s (φs(x))
"
REPRESENTATION CONSOLIDATION,0.09308510638297872,"|
{z
}
generalist teacher term + N
X"
REPRESENTATION CONSOLIDATION,0.09441489361702128,"i=1
λi Ldistill
 
hi
t(φi
t(x)), hi
s(φs(x))
"
REPRESENTATION CONSOLIDATION,0.09574468085106383,"|
{z
}
traditional distillation 
(1)"
REPRESENTATION CONSOLIDATION,0.09707446808510638,"with a distillation loss Ldistill. Our key difference from traditional distillation is the generalist teacher
term. Setting λ0 = 0 while keeping all other conditions (proxy dataset, network initialization, etc.)
yields a standard multi-teacher distillation baseline. Note that this is also distinct from ensemble
knowledge distillation, where one averages the homogeneous output of every model and distills the
mean onto a single student head, since our teacher outputs are heterogeneous (different tasks)."
REPRESENTATION CONSOLIDATION,0.09840425531914894,"Our method is agnostic to the underlying distillation method.
We can use the standard
knowledge distillation (KD) loss (Hinton et al., 2015) which is cross-entropy with temperature
T = 2, i.e. Ldistill(pt, ps) = −PC
c=1 p(c)
t
log(p(c)
s ) where c indexes the C classes, and pt =
softmax
 
hi
t(φi
t(x))/T

, ps = softmax
 
hi
s(φs(x))/T

. Alternatively, we can use a state-of-the-
art distillation method, e.g., CRD (Tian et al., 2020). CRD uses the KD loss and adds a contrastive
embedding loss between hi
CRD,t(φi
t(x)) and hi
CRD,s(φs(x)) with a pair of trainable linear embedding
layers hi
CRD,t and hi
CRD,s that are used only in training and discarded afterwards. The teacher φi
t’s"
REPRESENTATION CONSOLIDATION,0.09973404255319149,Under review as a conference paper at ICLR 2022
REPRESENTATION CONSOLIDATION,0.10106382978723404,"are trained on completely different tasks, so for CRD we use different embedding layers for each
teacher-student-head pair."
REPRESENTATION CONSOLIDATION,0.1023936170212766,"We initialize the student backbone φs and its 0-th head h0
s using the generalist model’s weights
φ0
t , h0
t , whereas other heads are randomly initialized. Since it is important to maintain pre-trained
model’s representational power, we simply set the loss weights λ0 = 1 and λi =
1
N for 1 ≤
i ≤N. This forces the learned representation φs to balance between learning general and task-
speciﬁc features, which beneﬁts future downstream transfer. After training the student we evaluate
the resulting representation φs(·) on multiple downstream tasks {Di
d}. For each task j we can either
ﬁne-tune the whole model hj
d(φj
d(·)) or keep the student representation ﬁxed and only learn the
classiﬁer head hj
d(φs(·)) which is often referred to as a linear probe."
EXPERIMENTAL SETUP,0.10372340425531915,"3
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.10505319148936171,"Datasets and downstream tasks.
We utilize datasets from a variety of domains to generate teach-
ers and downstream tasks: Cars196 (Krause et al., 2013), Resisc45 (Cheng et al., 2017) (remote
sensing images), iFood (Kaur et al., 2019) and Food101 (Bossard et al., 2014), iFashion (Guo et al.,
2019), DTD (Cimpoi et al., 2014) (describable textures), iNaturalist (Horn et al., 2018) (species clas-
siﬁcation, 2019 challenge version), CUB Birds (Wah et al., 2011), Flowers (Nilsback & Zisserman,
2008), Caltech256 (Grifﬁn et al., 2007), and Aircrafts (Maji et al., 2013). Among these, iFood and
Food101 are the same domain, and Birds and Flowers are subdomains of iNaturalist. We checked
for near-duplicates between these datasets using perceptual hash (Buchner, 2021), and found negli-
gible duplication: 1 out of 134k iNaturalist (50% classes we used) is a duplicate of CUB, and 8 out
of 130k/100k images are duplicates between iFood and Food101."
EXPERIMENTAL SETUP,0.10638297872340426,"To evaluate if a consolidated student representation has learned features relevant to a speciﬁc do-
main, we require downstream tasks that are related (in the same domain) to that of each task-speciﬁc
teacher. Except for Food101 and iFashion, we split each dataset at random into two disjoint sets
which each contain only 50% of the classes. We take the ﬁrst half of the dataset, named “t-split”,
and use as Di
t , to train a task-speciﬁc teacher. We use the second half of each dataset, named “d-
split”, as one of the downstream tasks Dj
d. We primarily evaluate on few-shot downstream transfer,
where we randomly sample 5 training images from each available class in Dj
d, but always use the
entire test set for evaluation. We use all samples for those classes with < 5 images. For iFashion,
which is a multi-task multi-label dataset, we use all images but with a subset of labels (e.g., those
related to clothing category) as Di
t for teacher training. We then use all images with a disjoint set
of labels (e.g., those related to sleeve style) as a downstream task Dj
d for evaluating transfer. For
iFashion’s few-shot scenario, we randomly subsample 1000 images for downstream training, and
evaluate on all test data. We do not train any teacher on Food101 so the complete set of classes are
used as a downstream task."
EXPERIMENTAL SETUP,0.1077127659574468,"During distillation and consolidation, we use ImageNet (Russakovsky et al., 2015) (ILSVRC12)
as the proxy dataset Dproxy for most experiments. However, we also show results with Places365-
standard (Zhou et al., 2018) or iNaturalist (Horn et al., 2018) as the proxy."
EXPERIMENTAL SETUP,0.10904255319148937,"Compared methods and Criteria.
As a reminder, our goal is not to duplicate upstream teacher
predictions, but rather to improve transfer performance of distilled representations.
Therefore, we
evaluate each student’s φs using its performance when transferred to various downstream datasets
{Dj
d} and compare multiple methods for initializing the downstream representation φj
d:"
EXPERIMENTAL SETUP,0.11037234042553191,"(1) ImageNet-pretrained φ0
t , a strong baseline for transfer learning. (2) ImageNet-pretrained φ0
t
ﬁne-tuned on the soft-labels produced by the ImageNet-pretrained model with batchnorms in test
mode. This baseline aims to isolate the effect of soft-labels / self-distillation on model performance.
(3) The task-speciﬁc teacher φ1
t (or one of the teachers when N > 1) without further distillation.
(4) Distillation with N teachers using either standard knowledge distillation (KD) or contrastive
representation distillation (CRD). This distills only task-speciﬁc teacher(s) on Dproxy, without the
ImageNet teacher h0
t (φ0
t (·)). (5) Our consolidated representation φs which includes h0
t (φ0
t (·)) as a
teacher, using either KD or CRD as the underlying distillation loss. (6) A multi-task learning oracle
(MTL) that jointly trains on ImageNet and teacher data Dt rather than on unlabeled proxy data."
EXPERIMENTAL SETUP,0.11170212765957446,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.11303191489361702,"Dataset
iFood
iFood
Food101
Resisc45
(t-split)
(d-split)
(t-split)
5-shot
5-shot
5-shot
evaluated head
h1
t or h1
s
SVM hd
SVM hd
SVM hd
ImageNet pre-train
–
28.9
37.0
70.7
Teacher (t-split)
74.4
34.8
42.8
53.3
KD (trad. distill)
72.8
35.3
44.0
53.6
Ours + KD
68.6
38.8
47.2
69.5"
EXPERIMENTAL SETUP,0.11436170212765957,"MTL (oracle)
71.6
38.9
46.7
69.4"
EXPERIMENTAL SETUP,0.11569148936170212,(a) iFood as task-speciﬁc teacher dataset
EXPERIMENTAL SETUP,0.11702127659574468,"Dataset
Resisc45
Resisc45
iFood
Food101
(t-split)
(d-split)
(d-split)
5-shot
5-shot
5-shot
evaluated head
h1
t or h1
s
SVM hd
SVM hd
SVM hd
ImageNet pre-train
–
70.7
28.9
37.0
Teacher (t-split)
98.2
67.9
14.8
17.9
KD (trad. distill)
97.9
61.6
9.9
12.1
Ours + KD
97.0
72.6
28.8
36.4"
EXPERIMENTAL SETUP,0.11835106382978723,"MTL (oracle)
96.9
73.1
29.1
36.2"
EXPERIMENTAL SETUP,0.1196808510638298,(b) Resisc45 as task-speciﬁc teacher dataset
EXPERIMENTAL SETUP,0.12101063829787234,"Table 2: Exp. 0: Accuracies of baselines and representation consolidation, as judged with tra-
ditional criteria (ﬁrst column, original network head’s old task performance) and transfer learning
criteria (last 3 columns, performance of linear SVM head trained on related (bold) & unrelated
downstream tasks). Baselines work well for the original task, but underperform in transfer learning.
Ours matches or outperforms the best of all baselines in all transfer scenarios and matches the oracle."
EXPERIMENTAL SETUP,0.12234042553191489,"For comparison fairness, (2-6) are all ImageNet pre-trained (i.e. initialized with (1)) before further
training. Both (4) and (5) are using the same proxy dataset (i.e. ImageNet for most experiments)."
EXPERIMENTAL SETUP,0.12367021276595745,"We use the transfer accuracy on downstream tasks Dj
d to measure each representation’s power. We
primarily use the few-shot linear probe (train a 5-shot linear SVM as hj
d over ﬁxed φ) on Dj
d’s
training set (single training run for full dataset, few-shot performance averaged over 50 random
subsampling trials). We also verify our results hold when ﬁne-tuning the student representation
hj
d(φd(·)) on few-shot or full dataset splits."
EXPERIMENTAL SETUP,0.125,"Implementation details.
We will release our code to reproduce this paper upon acceptance. For
experimental fairness, we use the same network (ResNet50 (He et al., 2016)) / hyperparameters /
ImageNet initialization for the baselines and ours. For more details, please see appendix A."
RESULTS,0.12632978723404256,"4
RESULTS"
RESULTS,0.1276595744680851,"Exp. 0: Motivational analysis – traditional distill vs. representation consolidation
To high-
light the difference between representation consolidation and traditional distillation, we use either
iFood or Resisc45 (t-split) as D1
t to train teachers, and run distillation/consolidation with N = 1.
We test on iFood (d-split), Food101 (full), and Resisc45 (d-split) as downstream tasks (t-split and
d-split are disjoint 50% classes of each dataset; see Section 3)"
RESULTS,0.12898936170212766,"On downstream tasks Dj
d, we follow representation learning’s evaluation protocol (few-shot linear
probe): train linear SVM hd on φ(x), x ∈Dj
d, evaluate on Dj
d’s test set. We also evaluate each
network following the traditional distillation evaluation protocol, i.e. directly evaluate h1
t (φ1
t (·)) or
h1
s (φs(·)) on the upstream task D1
t ’s test set. The results are shown in Table 2."
RESULTS,0.13031914893617022,"If we only focus on the upstream task D1
t , then traditional (proxy) distillation almost matches the
teacher’s performance, and representation consolidation performs worse than both. However, if we
instead focus on the downstream transfer performance on Dj
d, we see the opposite trend. In Table 2a
with iFood representations, for the teacher-related downstream tasks Food101 and iFood d-split (the
disjoint classes split from the same dataset as D1
t ), we see a beneﬁt of consolidation over both task-
speciﬁc teacher and generic pre-trained features. On the unrelated downstream task (Resisc45),
where the generalist teacher excels, we notice that our method almost matches the generalist perfor-
mance even though we improve over it for the food tasks. We see a similar trend in Table 2b: the
consolidated student outperforms the teacher and the generalist on the teacher-related task (Resisc45
d-split) but still retains the generalist teacher’s performance on unrelated food tasks. This clearly
demonstrates the signiﬁcant difference between upstream and downstream transfer."
RESULTS,0.13164893617021275,"We perform nearly the same as the MTL oracle which trains also on the teacher datasets Dt, indicat-
ing that while we avoid replaying Dt, our simple consolidation method with Dproxy can extract nearly
all of its beneﬁt. Note that although we have carefully optimized the MTL oracle for downstream
performance, adding state-of-the-art MTL techniques could outperform our MTL implementation."
RESULTS,0.13297872340425532,Under review as a conference paper at ICLR 2022
RESULTS,0.13430851063829788,"Cars196 
(d-split) 
few-shot"
RESULTS,0.1356382978723404,Resisc45
RESULTS,0.13696808510638298,"(d-split) 
few-shot"
RESULTS,0.13829787234042554,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
RESULTS,0.13962765957446807,few-shot
RESULTS,0.14095744680851063,"iFashion 
(sleeve task)"
RESULTS,0.1422872340425532,few-shot
RESULTS,0.14361702127659576,"DTD 
(d-split) 
few-shot"
RESULTS,0.1449468085106383,"Flowers 
(d-split) 
few-shot 
Caltech256"
RESULTS,0.14627659574468085,"(d-split) 
few-shot"
RESULTS,0.14760638297872342,"Birds 
(d-split) 
few-shot"
RESULTS,0.14893617021276595,"Aircrafts 
(d-split) 
few-shot −40 −30 −20 −10 0 10 20 30 40"
RESULTS,0.1502659574468085,Teacher: Cars196 (t-split) model
RESULTS,0.15159574468085107,"Cars196 
(d-split) 
few-shot"
RESULTS,0.1529255319148936,Resisc45
RESULTS,0.15425531914893617,"(d-split) 
few-shot"
RESULTS,0.15558510638297873,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
RESULTS,0.15691489361702127,few-shot
RESULTS,0.15824468085106383,"iFashion 
(sleeve task)"
RESULTS,0.1595744680851064,few-shot
RESULTS,0.16090425531914893,"DTD 
(d-split) 
few-shot"
RESULTS,0.1622340425531915,"Flowers 
(d-split) 
few-shot 
Caltech256"
RESULTS,0.16356382978723405,"(d-split) 
few-shot"
RESULTS,0.16489361702127658,"Birds 
(d-split) 
few-shot"
RESULTS,0.16622340425531915,"Aircrafts 
(d-split) 
few-shot −40 −30 −20 −10 0 10 20 30 40"
RESULTS,0.1675531914893617,Teacher: iFood (t-split) model
RESULTS,0.16888297872340424,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
teacher"
RESULTS,0.1702127659574468,"KD
repr. consolid. KD
CRD"
RESULTS,0.17154255319148937,"repr. consolid. CRD
MTL oracle"
RESULTS,0.17287234042553193,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.17420212765957446,"(a) Excerpt (2 of 10 D1
t scenarios). Full ﬁgure in appendix Fig. 6."
RESULTS,0.17553191489361702,"related Dj
d
unrelated Dj
d
>
≈
<
>
≈
<"
RESULTS,0.1768617021276596,"ours + KD φs vs. ImageNet φ0
t
6
4
0
0
10
0
ours + KD φs vs. KD (trad.) φs
7
1
2
10
0
0
KD (trad.) φs vs. ImageNet φ0
t
5
1
4
0
0
10"
RESULTS,0.17819148936170212,"ours + CRD φs vs. ImageNet φ0
t
7
3
0
0
10
0
ours + CRD φs vs. CRD (trad.) φs
7
0
3
10
0
0
CRD (trad.) φs vs. ImageNet φ0
t
5
2
3
0
0
10"
RESULTS,0.17952127659574468,"(b) Tally of comparisons (representation
consolidation (ours) vs. traditional distill
vs. ImageNet pre-trained model) among
all 10 teacher dataset D1
t scenarios."
RESULTS,0.18085106382978725,"Figure 2: Exp. 1 with N = 1, 5-shot linear SVM downstream transfer. Left: Comparing different
representations on ten downstream tasks. Teacher-related downstream tasks in bold. Performance
relative to ImageNet representation baseline. Excerpt (2 of 10 teacher domain scenarios). Right:
Tally of comparisons among all ten D1
t domains. On teacher-related downstream tasks, we outper-
form or match ImageNet-pretrained, and on other tasks we match ImageNet. Traditional distill often
underperforms ours (7/10 related, 10/10 unrelated) and ImageNet (3-4/10 related, 10/10 unrelated).
See appendix for full results."
RESULTS,0.18218085106382978,"We only demonstrate that it is not trivial to outperform our method using original teacher training
data."
RESULTS,0.18351063829787234,"Exp. 1: Improving student representation when N = 1.
We show that this advantage of con-
solidation over baselines holds for a wide range of upstream and downstream datasets. We also
compare to CRD to show state-of-the-art distillation often underperforms even when speciﬁcally
targeting representation learning. Fig. 2 summarizes few-shot SVM accuracy using different repre-
sentations relative to ImageNet-pretrained (See appendix for the full Fig. 6 and raw numbers)."
RESULTS,0.1848404255319149,"The conclusions are similar regardless of using KD or CRD – consolidation outperforms (Cars196,
Resisc45, iFood, CUB, Aircrafts, iNaturalist teachers) or matches (iFashion, DTD, Flowers, Cal-
tech256 teachers) ImageNet pre-trained model performance on related downstream tasks, and
matches its performance on unrelated ones. In contrast, both traditional distillation methods (1)
underperform consolidation on related downstream tasks for all teachers except Cars196, iFashion,
and Aircraft, and (2) drastically underperform both consolidated and ImageNet features on unrelated
downstream tasks – although CRD is better than KD, it still degrades performance. Notably, tradi-
tional distillation methods underperform ImageNet even on related downstream tasks for Resisc45,
DTD, Caltech256, and (for KD only) Flowers teachers. We match the MTL oracle performance
across the board, reaping the teacher dataset’s beneﬁt without replaying it."
RESULTS,0.18617021276595744,"Exp. 2: Consolidating representations with N > 1.
We can merge multiple task-speciﬁc teacher
representations that are related to get a better one. In addition, our method is not constrained to
merging models with the same architecture or φi
t’s feature space dimensions like Geyer et al. (2019).
To illustrate this, we split the “t-split” of Cars196 and iFood into ﬁve random splits containing
10% of the original classes. We train a ResNet18 teacher on each of the ﬁve splits. Then, we use
either traditional distillation or representation consolidation to merge these models with a ResNet50
generalist teacher. We compare the resulting representations with the teacher model’s and ImageNet.
Fig. 3a shows this result. For downstream tasks, we obtain better performance than using only one
of the ﬁve teachers on both related and unrelated tasks, especially for iFood and Food101 where
the teachers themselves underperform on related downstream task. This shows that our method can
beneﬁt from even teachers whose representation is weaker, as long as they have domain knowledge."
RESULTS,0.1875,"Full comparison with traditional distillation and representation consolidation from only one of the
ﬁve teachers are in the appendix – we gain performance on similar Dj
d by using ﬁve task-speciﬁc
teachers instead of one, and we outperform both traditional distill methods on related downstream"
RESULTS,0.18882978723404256,Under review as a conference paper at ICLR 2022
RESULTS,0.1901595744680851,"Cars196 
(d-split) 
few-shot"
RESULTS,0.19148936170212766,Resisc45
RESULTS,0.19281914893617022,"(d-split) 
few-shot"
RESULTS,0.19414893617021275,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
RESULTS,0.19547872340425532,few-shot
RESULTS,0.19680851063829788,"iFashion 
(sleeve task)"
RESULTS,0.1981382978723404,few-shot
RESULTS,0.19946808510638298,"DTD 
(d-split) 
few-shot"
RESULTS,0.20079787234042554,"Flowers 
(d-split) 
few-shot 
Caltech256"
RESULTS,0.20212765957446807,"(d-split) 
few-shot"
RESULTS,0.20345744680851063,"Birds 
(d-split) 
few-shot"
RESULTS,0.2047872340425532,Aircrafts
RESULTS,0.20611702127659576,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
RESULTS,0.2074468085106383,Teacher(s): Cars196 10% classes model
RESULTS,0.20877659574468085,"Cars196 
(d-split) 
few-shot"
RESULTS,0.21010638297872342,Resisc45
RESULTS,0.21143617021276595,"(d-split) 
few-shot"
RESULTS,0.2127659574468085,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
RESULTS,0.21409574468085107,few-shot
RESULTS,0.2154255319148936,"iFashion 
(sleeve task)"
RESULTS,0.21675531914893617,few-shot
RESULTS,0.21808510638297873,"DTD 
(d-split) 
few-shot"
RESULTS,0.21941489361702127,"Flowers 
(d-split) 
few-shot 
Caltech256"
RESULTS,0.22074468085106383,"(d-split) 
few-shot"
RESULTS,0.2220744680851064,"Birds 
(d-split) 
few-shot"
RESULTS,0.22340425531914893,Aircrafts
RESULTS,0.2247340425531915,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
RESULTS,0.22606382978723405,Teacher(s): iFood 10% classes model
RESULTS,0.22739361702127658,"ImageNet-pretrained model
ImageNet-pretrained, soft-label"
RESULTS,0.22872340425531915,"x 5 (Res18) repr. consolid. CRD
x 5 (Res18) repr. consolid. KD"
RESULTS,0.2300531914893617,(Res18) teacher (best of 5)
RESULTS,0.23138297872340424,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.2327127659574468,(a) Merging same-domain ResNet18 teachers
RESULTS,0.23404255319148937,"Cars196 
(d-split) 
few-shot"
RESULTS,0.23537234042553193,Resisc45
RESULTS,0.23670212765957446,"(d-split) 
few-shot"
RESULTS,0.23803191489361702,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
RESULTS,0.2393617021276596,few-shot
RESULTS,0.24069148936170212,"iFashion 
(sleeve task)"
RESULTS,0.24202127659574468,few-shot
RESULTS,0.24335106382978725,"DTD 
(d-split) 
few-shot"
RESULTS,0.24468085106382978,"Flowers 
(d-split) 
few-shot 
Caltech256"
RESULTS,0.24601063829787234,"(d-split) 
few-shot"
RESULTS,0.2473404255319149,"Birds 
(d-split) 
few-shot"
RESULTS,0.24867021276595744,Aircrafts
RESULTS,0.25,"(d-split) 
few-shot −15 −10 −5 0 5 10 15 20"
RESULTS,0.25132978723404253,Teachers: Cars196+Resisc45+iFood (t-split) models
RESULTS,0.2526595744680851,"ImageNet-pretrained model
ImageNet-pretrained, soft-label"
RESULTS,0.25398936170212766,repr. consolid. KD
RESULTS,0.2553191489361702,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.2566489361702128,"(b) Merge multi-domain Res50 teach-
ers"
RESULTS,0.2579787234042553,"Figure 3: Exp. 2 with N > 1 multi-model merging, 5-shot linear SVM downstream transfer. Left:
In the same domain, we are able to consolidate from models with different architectures and improve
transfer performance over every single teacher. Right: We can consolidate different domain models
and improve over the ImageNet representation. See appendix for full comparisons."
RESULTS,0.25930851063829785,"Cars196 
(d-split)"
RESULTS,0.26063829787234044,Resisc45
RESULTS,0.261968085106383,"(d-split) 
iFood 
(d-split)"
RESULTS,0.2632978723404255,"Food101 
(all classes)"
RESULTS,0.2646276595744681,Aircrafts
RESULTS,0.26595744680851063,(d-split)
RESULTS,0.26728723404255317,"Flowers 
(all classes)"
RESULTS,0.26861702127659576,"Birds 
(all classes) −5 −4 −3 −2 −1 0 1 2"
RESULTS,0.2699468085106383,Teacher: iFood (t-split) model
RESULTS,0.2712765957446808,"Cars196 
(d-split)"
RESULTS,0.2726063829787234,Resisc45
RESULTS,0.27393617021276595,"(d-split) 
iFood 
(d-split)"
RESULTS,0.2752659574468085,"Food101 
(all classes)"
RESULTS,0.2765957446808511,Aircrafts
RESULTS,0.2779255319148936,(d-split)
RESULTS,0.27925531914893614,"Flowers 
(all classes)"
RESULTS,0.28058510638297873,"Birds 
(all classes) −5 −4 −3 −2 −1 0 1 2"
RESULTS,0.28191489361702127,Teacher: iNaturalist (t-split) model
RESULTS,0.28324468085106386,"ImageNet-pretrained model
ImageNet-pretrained, soft-label"
RESULTS,0.2845744680851064,"repr. consolid. KD
KD"
RESULTS,0.2859042553191489,teacher
RESULTS,0.2872340425531915,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.28856382978723405,"Figure 4: Exp. 3 Excerpt of N = 1, ﬁne-tuning results. Showing full-shot (transfer to all classes
for Flowers and Birds). Our conclusions are the same between ﬁne-tuning and ﬁxed representation
(Fig. 2). See appendix for the full results (few-shot, full-shot, more datasets)."
RESULTS,0.2898936170212766,"tasks for iFood/Food101. We also show results when using all ResNet50 teachers for the ﬁve splits
in the appendix. The gaps are closer but the results are similar and the conclusions are identical."
RESULTS,0.2912234042553192,"Finally, we explore merging models from different domains to form a multi-domain consolidated
student. See Fig. 3b. We observe that we can outperform ImageNet on all related downstream tasks,
but the performance gain is smaller than representation consolidation in just one domain. We show
in the appendix that ours+KD outperforms KD on most downstream tasks except Cars196."
RESULTS,0.2925531914893617,"Exp. 3: Fine-tuning downstream.
We also verify that our conclusions generalize to ﬁne-tuning
as well, especially without few-shot sampling. Fig. 4 shows an excerpt of our results that include full
dataset transfer. See appendix for ﬁne-tuning using few-shot and full d-splits, whose conclusions
are the same as this section’s. Fine-tuning allows the representation to change into one that better
suits the downstream task, so the beneﬁt of teachers’ domain knowledge shrinks compared to ﬁxed
φs with linear SVM hj
d. Despite this, the conclusions are the same as the ﬁxed φs scenario."
RESULTS,0.29388297872340424,"Exp. 4: Inﬂuence of the Loss weights.
Fig. 5a shows the effect of the choice of loss weight on our
method – when we use λ0 = 1"
RESULTS,0.29521276595744683,"2, λi =
3
2N (i > 0), we gain performance on the related downstream
task but lose performance on unrelated ones, whereas using λ0 = 3"
RESULTS,0.29654255319148937,"2, λi =
1
2N (i > 0) gives us the
opposite. We can potentially use this trade-off to suppress unreliable teachers’ inﬂuence."
RESULTS,0.2978723404255319,Under review as a conference paper at ICLR 2022
RESULTS,0.2992021276595745,"old:new = 1:3 
old:new = 1:1 
old:new = 3:1 
−5 0 5 10 15 20 25 30"
RESULTS,0.300531914893617,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.30186170212765956,"Cars196 (d-split) few-shot
Resisc45 (d-split) few-shot
iFood (d-split) few-shot
Food101 (all classes) few-shot
iFashion (sleeve task) few-shot"
RESULTS,0.30319148936170215,"(a) Exp. 4 Tune loss weights λ0 (old) and λ1 (new). D1
t =Cars196."
RESULTS,0.3045212765957447,"Cars196 
(d-split) 
few-shot"
RESULTS,0.3058510638297872,Resisc45
RESULTS,0.3071808510638298,"(d-split) 
few-shot"
RESULTS,0.30851063829787234,"iFood 
(d-split) 
few-shot"
RESULTS,0.3098404255319149,"Food101 
(all classes)"
RESULTS,0.31117021276595747,few-shot
RESULTS,0.3125,"iFashion 
(sleeve task)"
RESULTS,0.31382978723404253,few-shot −30 −20 −10 0 10 20 30 40
RESULTS,0.3151595744680851,Teacher: Cars196 (t-split) model
RESULTS,0.31648936170212766,"ImageNet-pretrained model
repr. consolid. KD (ImNet proxy)
repr. consolid. KD (Places proxy)"
RESULTS,0.3178191489361702,"repr. consolid. KD (Places proxy + label)
KD (Places proxy)
repr. consolid. KD (iNaturalist proxy)"
RESULTS,0.3191489361702128,Accuracy (rel. to ImageNet-pretrained model)
RESULTS,0.3204787234042553,(b) Exp. 5 Dproxy. More in appendix.
RESULTS,0.32180851063829785,"Figure 5: Additional studies. 5-shot linear SVM downstream transfer. Left: Tuning loss weights
result in different balance between related and unrelated downstream task performance. Right:
Places365 as proxy has similar results as ImageNet proxy. However, training on Places365’s super-
vised labels or using a less general proxy (iNaturalist) hinders performance."
RESULTS,0.32313829787234044,"Exp. 5: Proxy data choice.
Fig. 5b shows results replacing ImageNet with Places365 or iNat-
uralist as the Dproxy dataset. Places365 yields similar performances as using ImageNet, while a
narrower-scope iNaturalist is weaker on unrelated downstream tasks. Jointly training on Places365
labels is not helpful, meaning that the improvement comes from the generalist teacher, not the data."
RELATED WORK,0.324468085106383,"5
RELATED WORK"
RELATED WORK,0.3257978723404255,"Our goal is to maximize the transferability of a representation consolidated from multiple teachers.
This downstream transfer aspect has received little attention in the literature (Liu et al., 2019; Geyer
et al., 2019; Tian et al., 2020), but this problem formulation is closely related to prior work on
multi-model merging and distillation with proxy data."
RELATED WORK,0.3271276595744681,"Multi-model merging.
It is useful to combine separate models that perform different tasks into a
single model for efﬁciency and performance beneﬁts. Knowledge Concentration (Gao et al., 2017)
combine teachers trained on subsets of 100k classes in the EFT dataset (Gao et al., 2017) into one
improved single model, using handcrafted sparse connections for the ﬁnal student layers. Chou
et al. (2018) merge CNNs by combining the kernel weights of different models by clustering and
lookup, followed by ﬁne-tuning. Zhao et al. (2020) merge object detection models when some
classes may be the background to other models. Vongkulbhisal et al. (2019) combine models with
overlapping classes using the combined train set, by deriving soft labels from intra-model class
correspondence. Ye et al. (2020) progressively train a GAN to regenerate proxy data for all teachers,
and combine teachers layer by layer. Chakraborty et al. (2018) and Park & Kwak (2020) aggregate
ensembles of members trained on the same task. In one-shot federated learning (Guha et al., 2019),
multiple clients with their own private data train a model each, and ﬁnally merge the models using
an ensemble or distillation in a way that protects their data privacy."
RELATED WORK,0.32845744680851063,"Unlike our approach, these methods merge models in order to perform exactly the same task as
the teacher models, are not concerned with the performance of the student when transferred to a
downstream task, and often (Gao et al., 2017; Chou et al., 2018; Zhao et al., 2020; Vongkulbhisal
et al., 2019; Chakraborty et al., 2018; Park & Kwak, 2020) require revisiting the original training
images. We have shown that these teachers themselves have poor transferability compared to a
simple pre-trained baseline, and learning only from them (even using state-of-the-art CRD (Tian
et al., 2020)) yields suboptimal transfer learning performance."
RELATED WORK,0.32978723404255317,"Multi-model merging for transfer.
Knowledge Flow (Liu et al., 2019) connects the student to
multiple teachers’ intermediate layers to kick-start target task training, and gradually penalizes its
reliance on teacher models over time. Geyer et al. (2019) uses IMM (Lee et al., 2017) to merge mul-
tiple models using their diagonal approximated Fisher information matrix (FIM) to balance different"
RELATED WORK,0.33111702127659576,Under review as a conference paper at ICLR 2022
RELATED WORK,0.3324468085106383,"teacher’s weight importance, merge, and ﬁne-tune. Computing the FIM requires reprocessing the
original teacher training data (unlike our approach that only needs generic proxy data). Furthermore,
this method requires homogeneous architectures for students and teachers, while ours works on any
architecture combinations. These methods directly optimize performance on the downstream task
and thus require re-running for each different target dataset. Our approach is more efﬁcient, as it
only requires consolidating teachers once to improve the pre-trained representation independently of
the downstream task. Finally, we note that the representations learned were not compared to a strong
baseline (i.e., pre-training on ImageNet), which we argue is a prerequisite for being useful in real-
world applications. One of our main contributions is observing the need for including a generalist
teacher, an insight which is orthogonal to, and could be combined with these previous approaches."
RELATED WORK,0.3337765957446808,"Distillation with representation losses (“representation distillation”)
tries to capture additional
structure of feature representations by aligning student and teacher feature activations during distil-
lation. Koratana et al. (2019) compress models by adding L2 losses between intermediate represen-
tations of the teacher and students. Aguilar et al. (2020) use KL divergence and cosine similarity
to make the attention and representation of intermediate features of the student and teacher similar.
Tian et al. (2020) adds contrastive representation learning loss to the penultimate layer to preserve
feature structures, by maximizing each image’s student and teacher features’ mutual information.
Despite their focus on yielding better representations, these methods fully rely on the teacher’s
transferability. We show that this yields downstream performance worse than generalist baselines."
RELATED WORK,0.3351063829787234,"Proxy data (“data-free”) distillation
transfers the input-output function of a teacher network to
a student network without using the teacher’s training data. (Yalniz et al., 2019; Orekondy et al.,
2019) opt to use a large general proxy dataset to query the teacher, and their teacher outputs to on
this data to train the student. Other methods (Nayak et al., 2019; Chen et al., 2019; Haroush et al.,
2020; Chawla et al., 2021) generate proxy data directly from the trained models, and use this data to
train the students. Further, Micaelli & Storkey (2019); Yin et al. (2020) also encourage generating
samples the student and teacher disagree on. Lastly, other methods require the original dataset to
compute meta-data information such as feature cluster mean. (Lopes et al., 2017; Bhardwaj et al.,
2019) Some train a GAN from the teachers to maximize chosen class predictions (Fang et al., 2019;
Yoo et al., 2019; Ye et al., 2020), sometimes also batchnorm statistics (Luo et al., 2020; Xu et al.,
2020b), and sometimes on proxy data instead. (Addepalli et al., 2020; Besnier et al., 2020) Some
work combines self-supervised learning with distillation (Tian et al., 2020; Xu et al., 2020a; Fang
et al., 2021) to improve the representation performance on the original task."
RELATED WORK,0.33643617021276595,"These works try to emulate the original data for standard distillation, and do not concern the student’s
transferability or merging multiple teachers. Our simple method already performs comparably to a
multi-task oracle that uses the original teacher dataset, indicating that these more complex methods
that improve upstream task distillation may be unnecessary when the goal is downstream transfer."
RELATED WORK,0.3377659574468085,"Incremental learning
is also related to our overall goal of growing a library of expert representa-
tions. These methods continually learn tasks or classes but often with limited access to past training
data (Li & Hoiem, 2018; Rebufﬁet al., 2017; Kirkpatrick et al., 2017; Zenke et al., 2017; Hu et al.,
2019; Yin et al., 2020; Prabhu et al., 2020). Our approach addresses many of the same challenges
by consolidating knowledge in the form of feature representations without revisiting old data used
to train teachers, but our “increments” are whole tasks given in the form of trained models instead
of data, whose labels may overlap. Multi-task learning (Caruana, 1998) assumes all datasets are
available and trains jointly on them whereas we assume task-speciﬁc datasets absent. We perform
comparably to this oracle."
SUMMARY,0.3390957446808511,"6
SUMMARY"
SUMMARY,0.3404255319148936,"In this paper, we show that traditional distillation can result in a representation suboptimal for down-
stream task transfer learning, because it only focuses on preserving the end-to-end input-output
mapping of the old task. We propose representation consolidation with the generalist model as an
additional teacher. Our method preserves the wide-range transferability of the strong ImageNet
baseline and improve the performance for both related and unrelated downstream tasks over tradi-
tionally distilled networks. We show that we can merge multiple models in the same domain to get
a better representation than any single model."
SUMMARY,0.34175531914893614,Under review as a conference paper at ICLR 2022
REFERENCES,0.34308510638297873,REFERENCES
REFERENCES,0.34441489361702127,"Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Char-
less C Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6430–6439,
2019."
REFERENCES,0.34574468085106386,"Sravanti Addepalli, Gaurav Kumar Nayak, Anirban Chakraborty, and Venkatesh Babu Radhakrish-
nan. Degan: Data-enriching gan for retrieving representative samples from a trained classiﬁer. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 3130–3137, 2020."
REFERENCES,0.3470744680851064,"Gustavo Aguilar, Yuan Ling, Y. Zhang, Benjamin Yao, Xing Fan, and Edward Guo. Knowledge
distillation from internal representations. In AAAI, 2020."
REFERENCES,0.3484042553191489,"Victor Besnier, Himalaya Jain, Andrei Bursuc, M. Cord, and P. P’erez. This dataset does not exist:
Training models from generated images. ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5, 2020."
REFERENCES,0.3497340425531915,"Kartikeya Bhardwaj, Naveen Suda, and R. Marculescu. Dream distillation: A data-independent
model compression framework. In Joint Workshop on On-Device Machine Learning & Compact
Deep Neural Network Representations, NeurIPS Workshop, 2019."
REFERENCES,0.35106382978723405,"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-
ponents with random forests. In European Conference on Computer Vision, 2014."
REFERENCES,0.3523936170212766,"Johannes Buchner.
Imagehash library.
https://github.com/JohannesBuchner/
imagehash, 2021. Accessed: 2021-05-27."
REFERENCES,0.3537234042553192,"R. Caruana. Multitask learning. In Encyclopedia of Machine Learning and Data Mining, 1998."
REFERENCES,0.3550531914893617,"Rudrasis Chakraborty, Chun-Hao Yang, and B. Vemuri. A mixture model for aggregation of mul-
tiple pre-trained weak classiﬁers. 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), pp. 454–4547, 2018."
REFERENCES,0.35638297872340424,"Akshay Chawla, Hongxu Yin, Pavlo Molchanov, and J. ´Alvarez. Data-free knowledge distillation
for object detection. In WACV, 2021."
REFERENCES,0.35771276595744683,"Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu,
Chao Xu, and Qi Tian. Daﬂ: Data-free learning of student networks. In ICCV, 2019."
REFERENCES,0.35904255319148937,"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classiﬁcation: Bench-
mark and state of the art. Proceedings of the IEEE, 105(10):1865–1883, Oct 2017. ISSN 1558-
2256. doi: 10.1109/jproc.2017.2675998. URL http://dx.doi.org/10.1109/JPROC.
2017.2675998."
REFERENCES,0.3603723404255319,"Yi-Min Chou, Yi-Ming Chan, Jia-Hong Lee, Chih-Yi Chiu, and Chu-Song Chen. Unifying and
merging well-trained deep neural networks for inference stage. In Proceedings of the 27th Inter-
national Joint Conference on Artiﬁcial Intelligence, pp. 2049–2056. AAAI Press, 2018."
REFERENCES,0.3617021276595745,"M. Cimpoi, Subhransu Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the
wild. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606–3613, 2014."
REFERENCES,0.363031914893617,"Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless
Fowlkes, Rahul Bhotika, Stefano Soatto, and Pietro Perona. A linearized framework and a new
benchmark for model selection for ﬁne-tuning. arXiv preprint arXiv:2102.00084, 2021."
REFERENCES,0.36436170212765956,"Gongfan Fang, J. Song, Chengchao Shen, X. Wang, D. Chen, and Mingli Song. Data-free adversarial
distillation. ArXiv, abs/1912.11006, 2019."
REFERENCES,0.36569148936170215,"Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, L. Zhang, Yezhou Yang, and Zicheng Liu. Seed:
Self-supervised distillation for visual representation. ICLR, 2021."
REFERENCES,0.3670212765957447,"J. Gao, Zijian Guo, Z. Li, and R. Nevatia. Knowledge concentration: Learning 100k object classiﬁers
in a single cnn. ArXiv, abs/1711.07607, 2017."
REFERENCES,0.3683510638297872,Under review as a conference paper at ICLR 2022
REFERENCES,0.3696808510638298,"Robin Geyer, Luca Corinzia, and Viktor Wegmayr. Transfer learning by adaptive merging of multi-
ple models. In MIDL, 2019."
REFERENCES,0.37101063829787234,"G. Grifﬁn, Alex Holub, and P. Perona.
Caltech-256 object category dataset.
Technical report,
California Institute of Technology, 2007."
REFERENCES,0.3723404255319149,"Neel Guha, Ameet S. Talwalkar, and Virginia Smith. One-shot federated learning. Machine Learning
on Devices Workshop at NeurIPS, 2019."
REFERENCES,0.37367021276595747,"S. Guo, Weilin Huang, X. Zhang, Prasanna Srikhanta, Yin Cui, Y. Li, M. Scott, Hartwig Adam,
and Serge J. Belongie. The imaterialist fashion attribute dataset. 2019 IEEE/CVF International
Conference on Computer Vision Workshop (ICCVW), pp. 3113–3116, 2019."
REFERENCES,0.375,"Matan Haroush, Itay Hubara, E. Hoffer, and Daniel Soudry.
The knowledge within: Methods
for data-free model compression. 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 8491–8499, 2020."
REFERENCES,0.37632978723404253,"Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016."
REFERENCES,0.3776595744680851,"Geoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural network. ArXiv,
abs/1503.02531, 2015."
REFERENCES,0.37898936170212766,"Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, C. Sun, Alexander Shepard, Hartwig
Adam, P. Perona, and Serge J. Belongie.
The inaturalist species classiﬁcation and detection
dataset. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769–
8778, 2018."
REFERENCES,0.3803191489361702,"Wenpeng Hu, Z. Lin, B. Liu, Chongyang Tao, Zhengwei Tao, Jinwen Ma, Dongyan Zhao, and Rui
Yan. Overcoming catastrophic forgetting for continual learning via model adaptation. In ICLR,
2019."
REFERENCES,0.3816489361702128,"Parneet Kaur, , Karan Sikka, Weijun Wang, serge Belongie, and Ajay Divakaran. Foodx-251: A
dataset for ﬁne-grained food classiﬁcation. arXiv preprint arXiv:1907.06167, 2019."
REFERENCES,0.3829787234042553,"J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, Andrei A. Rusu,
K. Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, D. Hassabis, C. Clopath,
D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings
of the National Academy of Sciences, 114:3521 – 3526, 2017."
REFERENCES,0.38430851063829785,"Animesh Koratana, Daniel Kang, Peter Bailis, and M. Zaharia. Lit: Learned intermediate represen-
tation training for model compression. In ICML, 2019."
REFERENCES,0.38563829787234044,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.386968085106383,"Sang-Woo Lee Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcom-
ing Catastrophic Forgetting by Incremental Moment Matching (IMM). In Advances In Neural
Information Processing Systems 30, 2017."
REFERENCES,0.3882978723404255,"Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 40:2935–2947, 2018."
REFERENCES,0.3896276595744681,"Iou-Jen Liu, J. Peng, and A. Schwing. Knowledge ﬂow: Improve upon your teachers. 7th Interna-
tional Conference on Learning Representations, ICLR 2019, 2019."
REFERENCES,0.39095744680851063,"Raphael Gontijo Lopes, Stefano Fenu, and T. Starner. Data-free knowledge distillation for deep
neural networks. ArXiv, abs/1710.07535, 2017."
REFERENCES,0.39228723404255317,"Liangchen Luo, M. Sandler, Zi Lin, A. Zhmoginov, and Andrew G. Howard. Large-scale generative
data-free distillation. ArXiv, abs/2012.05578, 2020."
REFERENCES,0.39361702127659576,"Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and A. Vedaldi. Fine-grained
visual classiﬁcation of aircraft. ArXiv, abs/1306.5151, 2013."
REFERENCES,0.3949468085106383,Under review as a conference paper at ICLR 2022
REFERENCES,0.3962765957446808,"P. Micaelli and A. Storkey.
Zero-shot knowledge transfer via adversarial belief matching.
In
NeurIPS, 2019."
REFERENCES,0.3976063829787234,"G. K. Nayak, K. R. Mopuri, V. Shaj, R. V. Babu, and A. Chakraborty. Zero-shot knowledge dis-
tillation in deep networks. In International Conference on Machine Learning, pp. 4743–4751,
2019."
REFERENCES,0.39893617021276595,"Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number
of classes. 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp.
722–729, 2008."
REFERENCES,0.4002659574468085,"Tribhuvanesh Orekondy, B. Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-
box models. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 4949–4958, 2019."
REFERENCES,0.4015957446808511,"Seonguk Park and Nojun Kwak.
Feature-level ensemble knowledge distillation for aggregating
knowledge from multiple networks. In ECAI, 2020."
REFERENCES,0.4029255319148936,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. 2019."
REFERENCES,0.40425531914893614,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011."
REFERENCES,0.40558510638297873,"Ameya Prabhu, P. Torr, and P. Dokania. Gdumb: A simple approach that questions our progress in
continual learning. In ECCV, 2020."
REFERENCES,0.40691489361702127,"Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´e Susano Pinto, Sylvain
Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. arXiv
preprint arXiv:2009.13239, 2020."
REFERENCES,0.40824468085106386,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, G. Sperl, and Christoph H. Lampert. icarl: Incre-
mental classiﬁer and representation learning. 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 5533–5542, 2017."
REFERENCES,0.4095744680851064,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y."
REFERENCES,0.4109042553191489,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In Inter-
national Conference on Learning Representations, 2020."
REFERENCES,0.4122340425531915,"J. Vongkulbhisal, Phongtharin Vinayavekhin, and M. V. Scarzanella. Unifying heterogeneous clas-
siﬁers with distillation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3170–3179, 2019."
REFERENCES,0.41356382978723405,"C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011."
REFERENCES,0.4148936170212766,"Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge distillation meets self-
supervision. In ECCV, 2020a."
REFERENCES,0.4162234042553192,"Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui
Tan. Generative low-bitwidth data free quantization. In ECCV, 2020b."
REFERENCES,0.4175531914893617,"I. Z. Yalniz, H. J´egou, K. Chen, Manohar Paluri, and D. Mahajan. Billion-scale semi-supervised
learning for image classiﬁcation. ArXiv, abs/1905.00546, 2019."
REFERENCES,0.41888297872340424,Under review as a conference paper at ICLR 2022
REFERENCES,0.42021276595744683,"Jingwen Ye, Yixin Ji, X. Wang, Xin Gao, and Mingli Song. Data-free knowledge amalgamation via
group-stack dual-gan. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 12513–12522, 2020."
REFERENCES,0.42154255319148937,"Hongxu Yin, Pavlo Molchanov, Zhizhong Li, Jose M. Alvarez, Arun Mallya, Derek Hoiem, N. Jha,
and J. Kautz.
Dreaming to distill: Data-free knowledge transfer via deepinversion.
2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8712–8721,
2020."
REFERENCES,0.4228723404255319,"Jaemin Yoo, Minyong Cho, T. Kim, and U. Kang. Knowledge extraction with no observable data.
In NeurIPS, 2019."
REFERENCES,0.4242021276595745,"Friedemann Zenke, Ben Poole, and S. Ganguli. Continual learning through synaptic intelligence.
Proceedings of machine learning research, 70:3987–3995, 2017."
REFERENCES,0.425531914893617,"X. Zhao, S. Schulter, G. Sharma, Yi-Hsuan Tsai, Manmohan Chandraker, and Ying Wu. Object
detection with a uniﬁed label space from multiple datasets. In ECCV, 2020."
REFERENCES,0.42686170212765956,"B. Zhou, `A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:
1452–1464, 2018."
REFERENCES,0.42819148936170215,"A
IMPLEMENTATION DETAILS"
REFERENCES,0.4295212765957447,"We will release our code to reproduce this paper upon acceptance. We use PyTorch. (Paszke et al.,
2019) For all network training, we use SGD with momentum of 0.9, weight decay of 10−4, a batch
size of 32. Unless noted, we use a learning rate decay of 0.1 at 50% and 80% of total training. We ini-
tialize task-speciﬁc teachers’ training and all students’ distillation with a ResNet50 (He et al., 2016)
pre-trained on ImageNet, and we freeze batchnorm statistics during distill/consolidation/MTL. We
learn each φi
t on the task-speciﬁc Di
t by ﬁne-tuning 120 epochs (learning rate decay at 70th and
100th epoch) while doing a log-scale grid search on the learning rate. For distilling φs, our method
is less sensitive to the choice of learning rate. We use a ﬁxed learning rate of 0.01 for each hi
s
and 0.001 for φs, and a schedule of 40 epochs. Note that ImageNet pre-training in PyTorch uses
0.001 as the ﬁnal epoch learning rate. This takes us roughly 4 days on an AWS instance with one
NVIDIA V100 for each distilled/consolidated representations. When the downstream transfer uses
a ﬁxed φj
d, we extract φd(x) on the center image crop, and search hj
d’s SVM hyperparameters using
a 5-fold cross-validation in scikit-learn (Pedregosa et al., 2011). When the downstream transfer uses
ﬁne-tuning, we run a log-scale grid search of learning rate with a 50 epoch schedule."
REFERENCES,0.4308510638297872,"B
COMPLETE FIGURES FOR SECTION 4 EXPERIMENTS"
REFERENCES,0.4321808510638298,"We now provide the full graphs and table for our experiments. Note that we have summarized these
results and all conclusions in the main paper."
REFERENCES,0.43351063829787234,"Exp. 1: Improving student representation when N = 1.
Fig. 6 and Table 3 show full results
for Fig. 2. Please see the main paper for analysis and conclusions."
REFERENCES,0.4348404255319149,"Exp. 2: Consolidating representations with N > 1.
Figs. 7, 8, 9 shows full results for Fig. 3."
REFERENCES,0.43617021276595747,"For Figs. 7 and 8 same-domain model merging, in addition to the main paper results merging
ResNet18 φi
t and ResNet50 φ0
t , we show similar results for all teachers being ResNet50 in Fig. 8. We
also show comparison to traditional distill with 5 teachers, representation consolidation with only 1
teacher, and the teacher itself (best out of 5 according to related Dj
d performance). The conclusions
are the same, and merging ﬁve teachers using representation consolidation outperforms all baselines
on both related and unrelated downstream tasks (except Cars196 with related Dj
d against traditional
distill)."
REFERENCES,0.4375,"For multi-domain model merging in Fig. 9, we compare to traditional distill with multi-task learning.
We outperform or match it on downstream Dj
d except for Cars196. We also explore using a concate-
nation of multiple large unlabeled datasets as Dproxy. With a more diverse proxy, the performance"
REFERENCES,0.43882978723404253,Under review as a conference paper at ICLR 2022
REFERENCES,0.4401595744680851,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.44148936170212766,Resisc45
REFERENCES,0.4428191489361702,"(d-split) 
few-shot"
REFERENCES,0.4441489361702128,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.4454787234042553,few-shot
REFERENCES,0.44680851063829785,"iFashion 
(sleeve task)"
REFERENCES,0.44813829787234044,few-shot
REFERENCES,0.449468085106383,"DTD 
(d-split) 
few-shot"
REFERENCES,0.4507978723404255,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.4521276595744681,"(d-split) 
few-shot"
REFERENCES,0.45345744680851063,"Birds 
(d-split) 
few-shot"
REFERENCES,0.45478723404255317,Aircrafts
REFERENCES,0.45611702127659576,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.4574468085106383,Teacher: Cars196 (t-split) model
REFERENCES,0.4587765957446808,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.4601063829787234,Resisc45
REFERENCES,0.46143617021276595,"(d-split) 
few-shot"
REFERENCES,0.4627659574468085,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.4640957446808511,few-shot
REFERENCES,0.4654255319148936,"iFashion 
(sleeve task)"
REFERENCES,0.46675531914893614,few-shot
REFERENCES,0.46808510638297873,"DTD 
(d-split) 
few-shot"
REFERENCES,0.46941489361702127,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.47074468085106386,"(d-split) 
few-shot"
REFERENCES,0.4720744680851064,"Birds 
(d-split) 
few-shot"
REFERENCES,0.4734042553191489,Aircrafts
REFERENCES,0.4747340425531915,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.47606382978723405,Teacher: Resisc45 (t-split) model
REFERENCES,0.4773936170212766,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.4787234042553192,Resisc45
REFERENCES,0.4800531914893617,"(d-split) 
few-shot"
REFERENCES,0.48138297872340424,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.48271276595744683,few-shot
REFERENCES,0.48404255319148937,"iFashion 
(sleeve task)"
REFERENCES,0.4853723404255319,few-shot
REFERENCES,0.4867021276595745,"DTD 
(d-split) 
few-shot"
REFERENCES,0.488031914893617,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.48936170212765956,"(d-split) 
few-shot"
REFERENCES,0.49069148936170215,"Birds 
(d-split) 
few-shot"
REFERENCES,0.4920212765957447,Aircrafts
REFERENCES,0.4933510638297872,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.4946808510638298,Teacher: iFood (t-split) model
REFERENCES,0.49601063829787234,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.4973404255319149,Resisc45
REFERENCES,0.49867021276595747,"(d-split) 
few-shot"
REFERENCES,0.5,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.5013297872340425,few-shot
REFERENCES,0.5026595744680851,"iFashion 
(sleeve task)"
REFERENCES,0.5039893617021277,few-shot
REFERENCES,0.5053191489361702,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5066489361702128,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.5079787234042553,"(d-split) 
few-shot"
REFERENCES,0.5093085106382979,"Birds 
(d-split) 
few-shot"
REFERENCES,0.5106382978723404,Aircrafts
REFERENCES,0.511968085106383,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.5132978723404256,Teacher: iFashion (category task) model
REFERENCES,0.5146276595744681,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.5159574468085106,Resisc45
REFERENCES,0.5172872340425532,"(d-split) 
few-shot"
REFERENCES,0.5186170212765957,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.5199468085106383,few-shot
REFERENCES,0.5212765957446809,"iFashion 
(sleeve task)"
REFERENCES,0.5226063829787234,few-shot
REFERENCES,0.523936170212766,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5252659574468085,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.526595744680851,"(d-split) 
few-shot"
REFERENCES,0.5279255319148937,"Birds 
(d-split) 
few-shot"
REFERENCES,0.5292553191489362,Aircrafts
REFERENCES,0.5305851063829787,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.5319148936170213,Teacher: DTD (t-split) model
REFERENCES,0.5332446808510638,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.5345744680851063,Resisc45
REFERENCES,0.535904255319149,"(d-split) 
few-shot"
REFERENCES,0.5372340425531915,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.538563829787234,few-shot
REFERENCES,0.5398936170212766,"iFashion 
(sleeve task)"
REFERENCES,0.5412234042553191,few-shot
REFERENCES,0.5425531914893617,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5438829787234043,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.5452127659574468,"(d-split) 
few-shot"
REFERENCES,0.5465425531914894,"Birds 
(d-split) 
few-shot"
REFERENCES,0.5478723404255319,Aircrafts
REFERENCES,0.5492021276595744,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.550531914893617,Teacher: Flowers (t-split) model
REFERENCES,0.5518617021276596,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.5531914893617021,Resisc45
REFERENCES,0.5545212765957447,"(d-split) 
few-shot"
REFERENCES,0.5558510638297872,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.5571808510638298,few-shot
REFERENCES,0.5585106382978723,"iFashion 
(sleeve task)"
REFERENCES,0.5598404255319149,few-shot
REFERENCES,0.5611702127659575,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5625,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.5638297872340425,"(d-split) 
few-shot"
REFERENCES,0.5651595744680851,"Birds 
(d-split) 
few-shot"
REFERENCES,0.5664893617021277,Aircrafts
REFERENCES,0.5678191489361702,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.5691489361702128,Teacher: Caltech256 (t-split) model
REFERENCES,0.5704787234042553,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.5718085106382979,Resisc45
REFERENCES,0.5731382978723404,"(d-split) 
few-shot"
REFERENCES,0.574468085106383,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.5757978723404256,few-shot
REFERENCES,0.5771276595744681,"iFashion 
(sleeve task)"
REFERENCES,0.5784574468085106,few-shot
REFERENCES,0.5797872340425532,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5811170212765957,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.5824468085106383,"(d-split) 
few-shot"
REFERENCES,0.5837765957446809,"Birds 
(d-split) 
few-shot"
REFERENCES,0.5851063829787234,Aircrafts
REFERENCES,0.586436170212766,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.5877659574468085,Teacher: Birds (t-split) model
REFERENCES,0.589095744680851,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.5904255319148937,Resisc45
REFERENCES,0.5917553191489362,"(d-split) 
few-shot"
REFERENCES,0.5930851063829787,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.5944148936170213,few-shot
REFERENCES,0.5957446808510638,"iFashion 
(sleeve task)"
REFERENCES,0.5970744680851063,few-shot
REFERENCES,0.598404255319149,"DTD 
(d-split) 
few-shot"
REFERENCES,0.5997340425531915,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.601063829787234,"(d-split) 
few-shot"
REFERENCES,0.6023936170212766,"Birds 
(d-split) 
few-shot"
REFERENCES,0.6037234042553191,Aircrafts
REFERENCES,0.6050531914893617,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.6063829787234043,Teacher: Aircrafts (t-split) model
REFERENCES,0.6077127659574468,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.6090425531914894,Resisc45
REFERENCES,0.6103723404255319,"(d-split) 
few-shot"
REFERENCES,0.6117021276595744,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.613031914893617,few-shot
REFERENCES,0.6143617021276596,"iFashion 
(sleeve task)"
REFERENCES,0.6156914893617021,few-shot
REFERENCES,0.6170212765957447,"DTD 
(d-split) 
few-shot"
REFERENCES,0.6183510638297872,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.6196808510638298,"(d-split) 
few-shot"
REFERENCES,0.6210106382978723,"Birds 
(d-split) 
few-shot"
REFERENCES,0.6223404255319149,Aircrafts
REFERENCES,0.6236702127659575,"(d-split) 
few-shot −40 −20 0 20 40"
REFERENCES,0.625,Teacher: iNaturalist (t-split) model
REFERENCES,0.6263297872340425,"ImageNet-pretrained model
ImageNet-pretrained, soft-label"
REFERENCES,0.6276595744680851,"KD
repr. consolid. KD"
REFERENCES,0.6289893617021277,"CRD
repr. consolid. CRD"
REFERENCES,0.6303191489361702,MTL oracle
REFERENCES,0.6316489361702128,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.6329787234042553,"Figure 6: Exp. 1 Full results for Fig. 2a (N = 1 single task-speciﬁc teacher, 5-shot linear SVM
downstream transfer). All 10 Di
t cases. This extensive set of experiments have the same conclusions
as the main paper: we match or outperform ImageNet representation while traditional distill often
underperforms. See also Table 3 for a comparison tally."
REFERENCES,0.6343085106382979,Under review as a conference paper at ICLR 2022
REFERENCES,0.6356382978723404,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.636968085106383,Resisc45
REFERENCES,0.6382978723404256,"(d-split) 
few-shot"
REFERENCES,0.6396276595744681,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.6409574468085106,few-shot
REFERENCES,0.6422872340425532,"iFashion 
(sleeve task)"
REFERENCES,0.6436170212765957,few-shot
REFERENCES,0.6449468085106383,"DTD 
(d-split) 
few-shot"
REFERENCES,0.6462765957446809,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.6476063829787234,"(d-split) 
few-shot"
REFERENCES,0.648936170212766,"Birds 
(d-split) 
few-shot"
REFERENCES,0.6502659574468085,Aircrafts
REFERENCES,0.651595744680851,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
REFERENCES,0.6529255319148937,Teacher(s): Cars196 10% classes model
REFERENCES,0.6542553191489362,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.6555851063829787,Resisc45
REFERENCES,0.6569148936170213,"(d-split) 
few-shot"
REFERENCES,0.6582446808510638,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.6595744680851063,few-shot
REFERENCES,0.660904255319149,"iFashion 
(sleeve task)"
REFERENCES,0.6622340425531915,few-shot
REFERENCES,0.663563829787234,"DTD 
(d-split) 
few-shot"
REFERENCES,0.6648936170212766,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.6662234042553191,"(d-split) 
few-shot"
REFERENCES,0.6675531914893617,"Birds 
(d-split) 
few-shot"
REFERENCES,0.6688829787234043,Aircrafts
REFERENCES,0.6702127659574468,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
REFERENCES,0.6715425531914894,Teacher(s): iFood 10% classes model
REFERENCES,0.6728723404255319,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
x 5 (Res18) repr. consolid. CRD"
REFERENCES,0.6742021276595744,"x 5 (Res18) repr. consolid. KD
x 5 (Res18) CRD
x 5 (Res18) KD"
REFERENCES,0.675531914893617,"(Res18) repr. consolid. CRD
(Res18) repr. consolid. KD
(Res18) teacher (best of 5)"
REFERENCES,0.6768617021276596,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.6781914893617021,"Figure 7: Exp. 2 Merging N > 1 same-domain ResNet18 teachers, 5-shot linear SVM downstream
transfer. Part 1/2 of full results for Fig. 3a. We are able to consolidate from models with different
architectures (ResNet50 φ0
t and ResNet18 φi
t) and improve transfer performance over every single
teacher."
REFERENCES,0.6795212765957447,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.6808510638297872,Resisc45
REFERENCES,0.6821808510638298,"(d-split) 
few-shot"
REFERENCES,0.6835106382978723,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.6848404255319149,few-shot
REFERENCES,0.6861702127659575,"iFashion 
(sleeve task)"
REFERENCES,0.6875,few-shot
REFERENCES,0.6888297872340425,"DTD 
(d-split) 
few-shot"
REFERENCES,0.6901595744680851,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.6914893617021277,"(d-split) 
few-shot"
REFERENCES,0.6928191489361702,"Birds 
(d-split) 
few-shot"
REFERENCES,0.6941489361702128,Aircrafts
REFERENCES,0.6954787234042553,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
REFERENCES,0.6968085106382979,Teacher(s): Cars196 10% classes model
REFERENCES,0.6981382978723404,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.699468085106383,Resisc45
REFERENCES,0.7007978723404256,"(d-split) 
few-shot"
REFERENCES,0.7021276595744681,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.7034574468085106,few-shot
REFERENCES,0.7047872340425532,"iFashion 
(sleeve task)"
REFERENCES,0.7061170212765957,few-shot
REFERENCES,0.7074468085106383,"DTD 
(d-split) 
few-shot"
REFERENCES,0.7087765957446809,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.7101063829787234,"(d-split) 
few-shot"
REFERENCES,0.711436170212766,"Birds 
(d-split) 
few-shot"
REFERENCES,0.7127659574468085,Aircrafts
REFERENCES,0.714095744680851,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
REFERENCES,0.7154255319148937,Teacher(s): iFood 10% classes model
REFERENCES,0.7167553191489362,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
x 5 repr. consolid. CRD"
REFERENCES,0.7180851063829787,"x 5 repr. consolid. KD
x 5 CRD
x 5 KD"
REFERENCES,0.7194148936170213,"repr. consolid. CRD
repr. consolid. KD
teacher (best of 5)"
REFERENCES,0.7207446808510638,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.7220744680851063,"Figure 8: Exp. 2 Merging N > 1 same-domain ResNet50 teachers, 5-shot linear SVM downstream
transfer. Part 2/2 of full results for Fig. 3a. Our conclusions generalize to using all ResNet50
teachers."
REFERENCES,0.723404255319149,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.7247340425531915,Resisc45
REFERENCES,0.726063829787234,"(d-split) 
few-shot"
REFERENCES,0.7273936170212766,"iFood 
(d-split) 
few-shot 
Food101 
(all classes)"
REFERENCES,0.7287234042553191,few-shot
REFERENCES,0.7300531914893617,"iFashion 
(sleeve task)"
REFERENCES,0.7313829787234043,few-shot
REFERENCES,0.7327127659574468,"DTD 
(d-split) 
few-shot"
REFERENCES,0.7340425531914894,"Flowers 
(d-split) 
few-shot 
Caltech256"
REFERENCES,0.7353723404255319,"(d-split) 
few-shot"
REFERENCES,0.7367021276595744,"Birds 
(d-split) 
few-shot"
REFERENCES,0.738031914893617,Aircrafts
REFERENCES,0.7393617021276596,"(d-split) 
few-shot −30 −20 −10 0 10 20 30 40"
REFERENCES,0.7406914893617021,Teacher: Cars196+Resisc45+iFood (t-split) model
REFERENCES,0.7420212765957447,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
repr. consolid. KD"
REFERENCES,0.7433510638297872,"KD
repr. consolid. KD (Places+iNat+logo2k+ImNet proxy)
KD (Places+iNat+logo2k+ImNet proxy)"
REFERENCES,0.7446808510638298,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.7460106382978723,"Figure 9: Exp. 2 Merging N > 1 different domain ResNet50 teachers, 5-shot linear SVM down-
stream transfer. Full results for Fig. 3b (N > 1 multiple model merging, multiple domains). We can
consolidate different domain models and improve over the ImageNet representation and (for most
related/unrelated downstream datasets) multi-task traditional distillation."
REFERENCES,0.7473404255319149,Under review as a conference paper at ICLR 2022
REFERENCES,0.7486702127659575,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.75,Resisc45
REFERENCES,0.7513297872340425,"(d-split) 
few-shot"
REFERENCES,0.7526595744680851,"iFood 
(d-split) 
few-shot"
REFERENCES,0.7539893617021277,"Food101 
(all classes)"
REFERENCES,0.7553191489361702,few-shot
REFERENCES,0.7566489361702128,"iFashion 
(sleeve task)"
REFERENCES,0.7579787234042553,few-shot −20 −10 0 10 20 30
REFERENCES,0.7593085106382979,Teacher: Cars196 (t-split) model
REFERENCES,0.7606382978723404,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.761968085106383,Resisc45
REFERENCES,0.7632978723404256,"(d-split) 
few-shot"
REFERENCES,0.7646276595744681,"iFood 
(d-split) 
few-shot"
REFERENCES,0.7659574468085106,"Food101 
(all classes)"
REFERENCES,0.7672872340425532,few-shot
REFERENCES,0.7686170212765957,"iFashion 
(sleeve task)"
REFERENCES,0.7699468085106383,few-shot −20 −10 0 10 20 30
REFERENCES,0.7712765957446809,Teacher: Resisc45 (t-split) model
REFERENCES,0.7726063829787234,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.773936170212766,Resisc45
REFERENCES,0.7752659574468085,"(d-split) 
few-shot"
REFERENCES,0.776595744680851,"iFood 
(d-split) 
few-shot"
REFERENCES,0.7779255319148937,"Food101 
(all classes)"
REFERENCES,0.7792553191489362,few-shot
REFERENCES,0.7805851063829787,"iFashion 
(sleeve task)"
REFERENCES,0.7819148936170213,few-shot −20 −10 0 10 20 30
REFERENCES,0.7832446808510638,Teacher: iFood (t-split) model
REFERENCES,0.7845744680851063,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.785904255319149,Resisc45
REFERENCES,0.7872340425531915,"(d-split) 
few-shot"
REFERENCES,0.788563829787234,"iFood 
(d-split) 
few-shot"
REFERENCES,0.7898936170212766,"Food101 
(all classes)"
REFERENCES,0.7912234042553191,few-shot
REFERENCES,0.7925531914893617,"iFashion 
(sleeve task)"
REFERENCES,0.7938829787234043,few-shot −20 −10 0 10 20 30
REFERENCES,0.7952127659574468,Teacher: iFashion (category task) model
REFERENCES,0.7965425531914894,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.7978723404255319,Resisc45
REFERENCES,0.7992021276595744,"(d-split) 
few-shot"
REFERENCES,0.800531914893617,"iFood 
(d-split) 
few-shot"
REFERENCES,0.8018617021276596,"Food101 
(all classes)"
REFERENCES,0.8031914893617021,few-shot
REFERENCES,0.8045212765957447,"iFashion 
(sleeve task)"
REFERENCES,0.8058510638297872,few-shot −20 −10 0 10 20 30
REFERENCES,0.8071808510638298,Teacher: Cars196+Resisc45+iFood (t-split) model
REFERENCES,0.8085106382978723,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
repr. consolid. KD
KD"
REFERENCES,0.8098404255319149,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.8111702127659575,"Figure 10: Exp. 3 N = 1 (ﬁrst 4 graphs) and N > 1 (last graph) teacher(s), 5-shot ﬁne-tuning
downstream transfer. Part 1/2 of full results for Fig. 4. The same conclusions as the ﬁxed represen-
tation scenario in Figs. 2, 6 hold."
REFERENCES,0.8125,"drops a little for related Dj
d and stays similar for unrelated Dj
d, suggesting we are somewhat insen-
sitive to choice of datasets, but a more diverse proxy data may not provide better model merging
performance."
REFERENCES,0.8138297872340425,"related Dj
d
unrelated Dj
d
>
≈
<
>
≈
<"
REFERENCES,0.8151595744680851,"ours + KD φs vs. ImageNet φ0
t
(6 others)
iFashion,
DTD, Flowers,
Caltech256.
(all 10)"
REFERENCES,0.8164893617021277,"ours + KD φs vs. KD (trad. distill) φs
(7 others)
Cars196.
iFashion, Aircraft.
(all 10)"
REFERENCES,0.8178191489361702,"KD (trad. distill) φs vs. ImageNet φ0
t
(5 others)
iNaturalist.
Resisc45,
DTD, Flowers,
Caltech256.
(all 10)"
REFERENCES,0.8191489361702128,"ours + CRD φs vs. ImageNet φ0
t
(7 others)
iFashion, DTD,
Caltech256.
(all 10)"
REFERENCES,0.8204787234042553,"ours + CRD φs vs. CRD (trad. distill) φs
(7 others)
Cars196, iFashion,
Aircraft.
(all 10)"
REFERENCES,0.8218085106382979,"CRD (trad. distill) φs vs. ImageNet φ0
t
(5 others)
Flowers,
iNaturalist.
Resisc45, DTD,
Caltech256.
(all 10)"
REFERENCES,0.8231382978723404,"Table 3: Exp. 1 Detailed tally of Fig. 6 (N = 1 single task-speciﬁc teacher, 5-shot linear SVM
downstream transfer) explaining Fig. 2b. On teacher-related downstream tasks, we outperform or
match ImageNet, and on other tasks we match ImageNet performance. Traditional distill often
underperforms ours (7/10 related, 10/10 unrelated) and ImageNet (3-4/10 related, 10/10 unrelated)."
REFERENCES,0.824468085106383,Under review as a conference paper at ICLR 2022
REFERENCES,0.8257978723404256,"Cars196 
(d-split)"
REFERENCES,0.8271276595744681,Resisc45
REFERENCES,0.8284574468085106,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8297872340425532,"Food101 
(all classes)"
REFERENCES,0.8311170212765957,Aircrafts
REFERENCES,0.8324468085106383,(d-split)
REFERENCES,0.8337765957446809,"Flowers 
(all classes)"
REFERENCES,0.8351063829787234,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.836436170212766,Teacher: Cars196 (t-split) model
REFERENCES,0.8377659574468085,"Cars196 
(d-split)"
REFERENCES,0.839095744680851,Resisc45
REFERENCES,0.8404255319148937,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8417553191489362,"Food101 
(all classes)"
REFERENCES,0.8430851063829787,Aircrafts
REFERENCES,0.8444148936170213,(d-split)
REFERENCES,0.8457446808510638,"Flowers 
(all classes)"
REFERENCES,0.8470744680851063,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.848404255319149,Teacher: Resisc45 (t-split) model
REFERENCES,0.8497340425531915,"Cars196 
(d-split)"
REFERENCES,0.851063829787234,Resisc45
REFERENCES,0.8523936170212766,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8537234042553191,"Food101 
(all classes)"
REFERENCES,0.8550531914893617,Aircrafts
REFERENCES,0.8563829787234043,(d-split)
REFERENCES,0.8577127659574468,"Flowers 
(all classes)"
REFERENCES,0.8590425531914894,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.8603723404255319,Teacher: iFood (t-split) model
REFERENCES,0.8617021276595744,"Cars196 
(d-split)"
REFERENCES,0.863031914893617,Resisc45
REFERENCES,0.8643617021276596,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8656914893617021,"Food101 
(all classes)"
REFERENCES,0.8670212765957447,Aircrafts
REFERENCES,0.8683510638297872,(d-split)
REFERENCES,0.8696808510638298,"Flowers 
(all classes)"
REFERENCES,0.8710106382978723,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.8723404255319149,Teacher: Aircrafts (t-split) model
REFERENCES,0.8736702127659575,"Cars196 
(d-split)"
REFERENCES,0.875,Resisc45
REFERENCES,0.8763297872340425,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8776595744680851,"Food101 
(all classes)"
REFERENCES,0.8789893617021277,Aircrafts
REFERENCES,0.8803191489361702,(d-split)
REFERENCES,0.8816489361702128,"Flowers 
(all classes)"
REFERENCES,0.8829787234042553,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.8843085106382979,Teacher: iNaturalist (t-split) model
REFERENCES,0.8856382978723404,"Cars196 
(d-split)"
REFERENCES,0.886968085106383,Resisc45
REFERENCES,0.8882978723404256,"(d-split) 
iFood 
(d-split)"
REFERENCES,0.8896276595744681,"Food101 
(all classes)"
REFERENCES,0.8909574468085106,Aircrafts
REFERENCES,0.8922872340425532,(d-split)
REFERENCES,0.8936170212765957,"Flowers 
(all classes)"
REFERENCES,0.8949468085106383,"Birds 
(all classes) −10 −8 −6 −4 −2 0 2 4"
REFERENCES,0.8962765957446809,Teacher: Cars196+Resisc45+iFood (t-split) model
REFERENCES,0.8976063829787234,"ImageNet-pretrained model
ImageNet-pretrained, soft-label
repr. consolid. KD
KD"
REFERENCES,0.898936170212766,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.9002659574468085,"Figure 11: Exp. 3 N = 1 (ﬁrst 4 graphs) and N > 1 (last graph) teacher(s), full-shot ﬁne-
tuning downstream transfer. Part 2/2 of full results for Fig. 4. The same conclusions as the ﬁxed
representation scenario in Figs. 2, 6 hold."
REFERENCES,0.901595744680851,Under review as a conference paper at ICLR 2022
REFERENCES,0.9029255319148937,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.9042553191489362,Resisc45
REFERENCES,0.9055851063829787,"(d-split) 
few-shot"
REFERENCES,0.9069148936170213,"iFood 
(d-split) 
few-shot"
REFERENCES,0.9082446808510638,"Food101 
(all classes)"
REFERENCES,0.9095744680851063,few-shot
REFERENCES,0.910904255319149,"iFashion 
(sleeve task)"
REFERENCES,0.9122340425531915,few-shot −30 −20 −10 0 10 20 30 40
REFERENCES,0.913563829787234,Teacher: Cars196 (t-split) model
REFERENCES,0.9148936170212766,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.9162234042553191,Resisc45
REFERENCES,0.9175531914893617,"(d-split) 
few-shot"
REFERENCES,0.9188829787234043,"iFood 
(d-split) 
few-shot"
REFERENCES,0.9202127659574468,"Food101 
(all classes)"
REFERENCES,0.9215425531914894,few-shot
REFERENCES,0.9228723404255319,"iFashion 
(sleeve task)"
REFERENCES,0.9242021276595744,few-shot −30 −20 −10 0 10 20 30 40
REFERENCES,0.925531914893617,Teacher: Resisc45 (t-split) model
REFERENCES,0.9268617021276596,"Cars196 
(d-split) 
few-shot"
REFERENCES,0.9281914893617021,Resisc45
REFERENCES,0.9295212765957447,"(d-split) 
few-shot"
REFERENCES,0.9308510638297872,"iFood 
(d-split) 
few-shot"
REFERENCES,0.9321808510638298,"Food101 
(all classes)"
REFERENCES,0.9335106382978723,few-shot
REFERENCES,0.9348404255319149,"iFashion 
(sleeve task)"
REFERENCES,0.9361702127659575,few-shot −30 −20 −10 0 10 20 30 40
REFERENCES,0.9375,Teacher: iFood (t-split) model
REFERENCES,0.9388297872340425,"ImageNet-pretrained model
repr. consolid. KD (ImNet proxy)
repr. consolid. KD (Places proxy)
repr. consolid. KD (iNaturalist proxy)"
REFERENCES,0.9401595744680851,Accuracy (rel. to ImageNet-pretrained model)
REFERENCES,0.9414893617021277,"Figure 12: Exp. 5 N = 1, 5-shot linear SVM downstream transfer, different proxies. Full results for
Fig. 5b. Conclusions similar – as the proxy, Places365 is similar to ImageNet, but a narrower-scoped
iNaturalist underperforms."
REFERENCES,0.9428191489361702,"Exp. 3: Fine-tuning downstream.
Figs. 10, 11 show full results for Fig. 4. These extensive
results show that in both few-shot and full-shot scenarios, and both single-domain and multi-domain
scenarios, we have the same conclusions as the ﬁxed representation experiments."
REFERENCES,0.9441489361702128,"Exp. 5: Proxy data choice.
Fig. 12 shows full results for Fig. 5b. This shows similar results with
different teacher Dt, i.e. Places365 performs similarly to ImageNet as the proxy, but the narrower-
scope iNaturalist underperforms."
REFERENCES,0.9454787234042553,"C
RAW NUMBER ACCURACY TABLES FOR ALL EXPERIMENTS"
REFERENCES,0.9468085106382979,"• Table 4: ﬁxed representation few-shot results for Figs. 2, 3 (Figs. 6,7,8,9 in the appendix).
• Table 5: ﬁxed representation few-shot results for Fig. 5.
• Table 6: ﬁne-tuning few-shot results for Fig. 10 in the appendix.
• Table 7: ﬁne-tuning full-shot results for Fig. 4 (Figs. 11 in the appendix.)"
REFERENCES,0.9481382978723404,"D
LIMITATIONS"
REFERENCES,0.949468085106383,"We assume we have perfect knowledge of which tasks form the same domain and which tasks do not
belong to a domain. Our performance drops when teachers from different domains are consolidated.
In future work, we plan to automatically determine how to cluster a large amount of teachers into
domains. In addition, one of our contributions assume the existence of a strong representation
baseline such as the ImageNet pre-trained model, which is true for ﬁelds like images and language,
but not for others (e.g., 3D reconstruction). Our method also takes the teachers as is and learn from
them, and any mistakes made by the teachers can be propagated during the consolidation. Possible
mitigations include using a better teacher that makes less such mistakes, or using regularization on
both teacher and student training, such as making similar inputs map to similar outputs."
REFERENCES,0.9507978723404256,Under review as a conference paper at ICLR 2022
REFERENCES,0.9521276595744681,"Cars196
Resisc45
iFood
Food101
(all)
iFashion
(sleeve)
DTD
Flowers
Caltech256
Birds
Aircrafts"
REFERENCES,0.9534574468085106,"ImageNet-pretrained model
31.94
70.66
28.94
36.95
89.06
60.98
83.84
80.28
54.73
30.71
ImageNet-pretrained, soft-label
31.47
69.99
29.21
37.29
89.18
60.44
82.86
81.35
57.19
29.77"
REFERENCES,0.9547872340425532,"Cars196 (t-split) KD
59.39
51.56
9.75
11.54
84.27
36.07
59.91
35.51
16.90
16.82
Cars196 (t-split) repr. consolid. KD
59.21
70.00
28.42
35.28
89.00
58.99
83.01
80.26
54.02
29.85
Cars196 (t-split) CRD
62.42
55.96
12.42
14.39
85.22
39.54
66.76
41.09
20.33
19.13
Cars196 (t-split) repr. consolid. CRD
60.97
71.02
29.14
36.15
89.10
59.47
84.33
80.23
54.09
32.04
Cars196 (t-split) MTL oracle
60.49
69.23
28.44
35.15
89.20
60.46
82.79
79.79
55.37
30.89"
REFERENCES,0.9561170212765957,"Resisc45 (t-split) KD
8.97
61.62
9.85
12.11
84.97
37.80
56.06
34.14
11.23
11.57
Resisc45 (t-split) repr. consolid. KD
30.80
72.64
28.77
36.37
89.03
60.08
82.75
80.77
55.47
29.17
Resisc45 (t-split) CRD
16.81
68.62
16.95
19.84
86.42
45.00
72.36
49.02
20.89
18.31
Resisc45 (t-split) repr. consolid. CRD
32.66
74.03
29.61
36.99
89.10
60.46
84.27
80.73
55.19
31.08
Resisc45 (t-split) MTL oracle
32.51
73.06
29.07
36.21
89.11
59.82
83.74
80.04
57.11
31.19"
REFERENCES,0.9574468085106383,"iFood (t-split) KD
12.85
53.57
35.26
44.05
85.39
36.62
70.99
39.94
17.95
14.90
iFood (t-split) repr. consolid. KD
29.44
69.49
38.85
47.19
89.04
57.16
82.66
79.17
51.07
27.31
iFood (t-split) CRD
14.80
56.57
36.30
44.67
86.03
38.95
74.03
42.56
19.91
16.18
iFood (t-split) repr. consolid. CRD
30.54
70.51
39.52
47.58
88.89
57.48
84.02
78.67
50.35
28.57
iFood (t-split) MTL oracle
32.57
69.41
38.88
46.65
89.13
59.46
82.82
79.52
56.37
31.50"
REFERENCES,0.9587765957446809,"iFashion (category task) KD
6.41
37.97
6.26
8.09
90.70
23.45
44.01
30.32
9.28
9.27
iFashion (category task) repr. consolid. KD
31.89
70.02
29.23
37.45
89.15
60.76
83.28
81.29
57.10
30.24
iFashion (category task) CRD
16.11
57.91
14.74
18.35
90.93
41.77
72.11
50.08
20.87
17.52
iFashion (category task) repr. consolid. CRD
32.36
71.75
29.91
37.60
89.72
60.98
84.65
80.91
55.51
31.39
iFashion (category task) MTL oracle
31.91
69.35
28.78
36.27
88.75
60.50
83.40
79.93
57.01
31.28"
REFERENCES,0.9601063829787234,"DTD (t-split) KD
11.29
53.12
14.11
19.00
86.39
46.80
62.48
46.38
17.14
14.75
DTD (t-split) repr. consolid. KD
31.33
69.78
29.17
37.38
89.08
61.43
82.80
81.28
56.09
29.59
DTD (t-split) CRD
23.39
64.77
24.59
31.08
87.82
55.80
79.29
66.36
33.82
24.15
DTD (t-split) repr. consolid. CRD
33.10
71.58
29.93
38.27
89.10
61.47
84.96
81.08
56.02
31.35
DTD (t-split) MTL oracle
32.76
69.56
29.24
36.95
89.24
61.10
83.22
80.57
57.66
31.05"
REFERENCES,0.961436170212766,"Flowers (t-split) KD
16.72
58.08
16.68
20.67
87.03
44.70
77.35
54.49
31.32
20.26
Flowers (t-split) repr. consolid. KD
31.81
69.94
29.46
37.01
89.13
60.06
84.79
81.18
57.07
30.41
Flowers (t-split) CRD
26.84
64.86
22.78
27.54
88.18
50.05
82.66
66.27
42.05
27.22
Flowers (t-split) repr. consolid. CRD
33.79
71.38
30.05
37.68
89.25
60.42
86.37
81.10
56.56
32.52
Flowers (t-split) MTL oracle
32.15
68.89
27.85
35.19
89.11
58.78
82.73
79.72
55.97
30.79"
REFERENCES,0.9627659574468085,"Caltech256 (t-split) KD
21.65
59.67
19.44
25.10
87.35
51.59
75.52
68.98
36.09
23.32
Caltech256 (t-split) repr. consolid. KD
31.71
69.36
29.18
36.97
89.08
61.07
83.14
81.28
56.63
29.98
Caltech256 (t-split) CRD
27.63
63.52
23.77
30.08
88.25
55.35
80.37
73.42
43.88
27.22
Caltech256 (t-split) repr. consolid. CRD
33.44
70.98
30.19
37.89
89.27
61.20
84.70
81.22
56.68
31.61
Caltech256 (t-split) MTL oracle
30.61
67.73
27.64
34.92
89.09
58.92
83.04
78.83
56.40
29.21"
REFERENCES,0.964095744680851,"Birds (t-split) KD
17.15
57.95
17.06
20.72
87.03
46.37
72.16
53.26
62.41
19.00
Birds (t-split) repr. consolid. KD
31.50
70.35
29.16
36.83
89.20
60.32
83.30
80.89
66.37
29.65
Birds (t-split) CRD
23.22
62.63
21.31
25.92
88.10
50.80
77.85
61.92
64.00
23.10
Birds (t-split) repr. consolid. CRD
32.70
71.65
29.72
37.52
89.23
60.38
84.63
80.56
66.35
31.54
Birds (t-split) MTL oracle
31.89
69.85
29.12
36.54
89.30
60.11
83.00
80.25
64.79
30.26"
REFERENCES,0.9654255319148937,"Aircrafts (t-split) KD
10.56
44.30
7.81
9.09
84.17
29.16
48.32
29.18
10.87
58.51
Aircrafts (t-split) repr. consolid. KD
30.71
69.58
28.25
35.54
89.06
58.67
81.36
80.54
53.95
55.26
Aircrafts (t-split) CRD
14.06
50.09
10.56
11.86
85.17
33.13
57.85
35.16
14.11
60.44
Aircrafts (t-split) repr. consolid. CRD
32.54
70.84
28.91
36.15
89.17
59.31
83.01
80.27
53.29
57.25
Aircrafts (t-split) MTL oracle
31.55
69.92
28.91
36.30
89.28
60.40
83.04
79.65
56.31
54.82"
REFERENCES,0.9667553191489362,"iNaturalist (t-split) KD
13.51
57.25
14.66
18.68
84.46
39.49
79.74
41.59
57.49
16.98
iNaturalist (t-split) repr. consolid. KD
30.08
69.67
27.35
34.52
88.70
57.28
84.77
78.01
67.58
27.94
iNaturalist (t-split) CRD
14.92
58.76
15.30
19.41
84.84
40.42
80.98
42.71
58.48
18.00
iNaturalist (t-split) repr. consolid. CRD
30.87
70.34
27.25
33.97
88.81
56.53
86.06
77.08
67.45
28.93
iNaturalist (t-split) MTL oracle
32.14
69.37
28.45
35.00
89.25
58.80
85.42
79.05
68.53
29.93"
REFERENCES,0.9680851063829787,"Cars196 10% classes x 5 (Res18) repr. consolid. CRD
44.04
71.89
30.45
37.80
89.20
60.56
85.60
81.01
58.09
32.86
Cars196 10% classes x 5 (Res18) repr. consolid. KD
42.03
69.95
29.44
36.97
89.05
60.15
83.23
81.48
58.18
31.10
Cars196 10% classes x 5 (Res18) CRD
48.45
66.00
23.35
27.10
88.42
52.48
80.29
66.12
47.90
29.24
Cars196 10% classes x 5 (Res18) KD
45.37
60.74
17.53
20.80
87.11
48.16
73.66
58.56
39.02
23.85
Cars196 10% classes (Res18) repr. consolid. CRD
39.56
72.50
29.86
37.12
89.36
60.73
85.07
80.00
57.67
33.79
Cars196 10% classes (Res18) repr. consolid. KD
36.97
69.64
28.62
35.95
88.92
59.88
82.79
80.37
56.80
30.87
Cars196 10% classes (Res18) teacher (best of 5)
35.39
56.47
16.98
20.50
87.59
45.77
70.58
60.41
41.87
23.40"
REFERENCES,0.9694148936170213,"iFood 10% classes x 5 (Res18) repr. consolid. CRD
32.36
71.87
35.93
42.38
89.24
58.56
84.70
79.37
52.72
31.52
iFood 10% classes x 5 (Res18) repr. consolid. KD
31.14
70.69
35.82
42.97
88.98
58.98
83.51
80.70
53.91
29.80
iFood 10% classes x 5 (Res18) CRD
22.26
64.98
34.87
39.92
87.67
48.94
79.60
58.12
33.24
24.30
iFood 10% classes x 5 (Res18) KD
17.60
60.26
34.10
39.71
86.58
45.93
76.18
51.55
27.91
20.27
iFood 10% classes (Res18) repr. consolid. CRD
32.75
71.36
32.22
38.97
89.29
59.18
84.45
78.88
54.07
32.17
iFood 10% classes (Res18) repr. consolid. KD
30.45
69.54
31.25
38.47
88.99
58.59
81.97
79.37
54.33
30.25
iFood 10% classes (Res18) teacher (best of 5)
14.27
52.35
23.11
26.40
86.45
38.34
65.88
44.94
22.58
16.11"
REFERENCES,0.9707446808510638,"Cars196 10% classes x 5 repr. consolid. CRD
46.69
71.65
30.10
37.72
89.09
60.48
85.50
80.75
57.01
33.00
Cars196 10% classes x 5 repr. consolid. KD
45.99
70.10
29.23
37.02
89.16
60.40
83.82
81.21
57.40
31.42
Cars196 10% classes x 5 CRD
53.22
67.42
23.59
28.19
88.17
54.09
81.42
66.77
45.41
29.87
Cars196 10% classes x 5 KD
50.33
62.63
18.52
22.32
86.77
49.31
75.66
59.74
37.27
25.48
Cars196 10% classes repr. consolid. CRD
41.67
71.77
29.77
37.10
89.32
60.52
85.37
80.05
56.80
33.08
Cars196 10% classes repr. consolid. KD
39.23
69.62
28.42
36.01
89.03
59.84
82.76
80.34
56.43
31.20
Cars196 10% classes teacher (best of 5)
42.92
64.48
20.87
25.95
87.58
51.79
78.69
64.35
43.20
25.01"
REFERENCES,0.9720744680851063,"iFood 10% classes x 5 repr. consolid. CRD
32.12
72.04
37.14
44.30
89.09
58.97
85.69
79.26
52.23
31.47
iFood 10% classes x 5 repr. consolid. KD
30.74
70.64
37.34
44.77
88.91
59.17
84.58
80.20
52.47
29.83
iFood 10% classes x 5 CRD
21.95
65.63
37.04
43.06
87.12
49.35
81.66
56.65
30.24
24.08
iFood 10% classes x 5 KD
17.70
61.33
35.88
42.71
86.18
44.63
77.50
50.98
25.36
20.49
iFood 10% classes repr. consolid. CRD
32.64
70.85
32.81
39.57
89.19
58.40
84.36
79.19
53.73
31.94
iFood 10% classes repr. consolid. KD
30.05
68.62
32.06
39.37
88.83
58.67
82.09
79.43
53.83
29.68
iFood 10% classes teacher (best of 5)
13.85
54.31
25.16
29.22
86.48
39.39
69.26
44.21
20.54
16.08"
REFERENCES,0.973404255319149,"Cars196+Resisc45+iFood (t-split) repr. consolid. KD
47.85
72.39
34.57
41.92
89.09
58.63
83.64
79.90
51.79
29.18
Cars196+Resisc45+iFood (t-split) KD
53.27
66.27
34.44
41.84
85.32
43.74
75.84
48.16
23.16
19.99
Cars196 + Resisc45 + iFood (t-split) repr. consolid.
(Places+iNat+logo2k+ImNet proxy 10 epochs)
45.71
73.00
32.87
39.80
89.00
59.09
83.89
79.83
52.40
29.46"
REFERENCES,0.9747340425531915,"Cars196 + Resisc45 + iFood (t-split) trad. distill
(Places+iNat+logo2k+ImNet proxy 10 epochs)
53.26
66.34
32.80
39.10
85.43
43.81
75.92
47.75
23.15
20.15"
REFERENCES,0.976063829787234,"Table 4: Fixed representation + linear SVM, 5-shot results (Part 1/2) raw numbers for Figs. 2, 3 in
the main paper (Figs. 6, 7, 8, 9 in the appendix)"
REFERENCES,0.9773936170212766,Under review as a conference paper at ICLR 2022
REFERENCES,0.9787234042553191,"Cars196
Resisc45
iFood
Food101
(all)
iFashion
(sleeve)"
REFERENCES,0.9800531914893617,"Cars196 (t-split) repr. consolid. KD old:new = 1:3
61.21
67.83
26.70
33.35
88.92
Cars196 (t-split) repr. consolid. KD old:new = 1:1
59.21
70.00
28.42
35.28
89.00
Cars196 (t-split) repr. consolid. KD old:new = 3:1
52.80
70.82
29.18
36.53
89.13"
REFERENCES,0.9813829787234043,"Cars196 (t-split) repr. consolid. KD (ImNet proxy)
59.21
70.00
28.42
35.28
89.00
Cars196 (t-split) repr. consolid. KD (Places proxy)
61.18
69.59
26.69
33.54
88.68
Cars196 (t-split) repr. consolid. KD (Places proxy + label)
53.16
69.32
23.17
28.54
88.23
Cars196 (t-split) KD (Places proxy)
59.98
51.11
9.54
11.10
84.38
Cars196 (t-split) repr. consolid. KD (iNaturalist proxy)
47.76
67.90
25.08
31.07
87.80"
REFERENCES,0.9827127659574468,"Resisc45 (t-split) repr. consolid. KD (ImNet proxy)
30.80
72.64
28.77
36.37
89.03
Resisc45 (t-split) repr. consolid. KD (Places proxy)
30.08
72.56
27.59
34.68
88.99
Resisc45 (t-split) repr. consolid. KD (iNaturalist proxy)
26.17
72.11
25.71
32.38
88.40"
REFERENCES,0.9840425531914894,"iFood (t-split) repr. consolid. KD (ImNet proxy)
29.44
69.49
38.85
47.19
89.04
iFood (t-split) repr. consolid. KD (Places proxy)
28.43
68.94
36.30
43.97
88.50
iFood (t-split) repr. consolid. KD (iNaturalist proxy)
22.87
67.04
33.47
40.18
87.79"
REFERENCES,0.9853723404255319,"Table 5: Fixed representation + linear SVM, 5-shot results (Part 2/2) raw numbers for Fig. 5 in the
main paper."
REFERENCES,0.9867021276595744,"Cars196
Resisc45
iFood
Food101
(all)
iFashion
(sleeve)"
REFERENCES,0.988031914893617,"ImageNet-pretrained model
37.18
68.91
28.25
34.72
89.68
ImageNet-pretrained, soft-label
39.84
69.88
28.72
35.31
89.92"
REFERENCES,0.9893617021276596,"Cars196 (t-split) repr. consolid.
55.80
69.91
28.41
34.00
89.91
Cars196 (t-split) trad. distill
60.19
61.30
18.98
19.79
87.90
Resisc45 (t-split) repr. consolid.
38.30
71.71
28.16
34.33
89.83
Resisc45 (t-split) trad. distill
21.12
60.93
17.57
19.46
88.05
iFood (t-split) repr. consolid.
35.94
68.39
35.27
42.14
89.54
iFood (t-split) trad. distill
27.89
61.58
31.23
39.33
87.91
iFashion category task repr. consolid.
38.57
68.45
28.80
35.88
89.29
iFashion category task trad. distill
16.71
54.53
11.49
14.93
89.80"
REFERENCES,0.9906914893617021,"Cars196 + Resisc45 + iFood (t-split) repr. consolid.
45.31
70.84
30.90
37.62
89.87
Cars196 + Resisc45 + iFood (t-split) trad. distill
45.29
67.05
30.14
35.44
87.97"
REFERENCES,0.9920212765957447,"Table 6: Fine-tuning, 5-shot results raw numbers for Fig. 10 in the appendix"
REFERENCES,0.9933510638297872,"Cars196
Resisc45
iFood
Food101
(all)
Aircrafts
Flowers
(all)
Birds
(all)"
REFERENCES,0.9946808510638298,"ImageNet-pretrained model
90.90
96.93
78.55
88.15
87.57
92.32
80.82
ImageNet-pretrained, soft-label
90.85
97.20
78.92
88.13
87.87
92.54
81.76"
REFERENCES,0.9960106382978723,"Cars196 (t-split) repr. consolid.
91.84
97.08
78.12
87.91
87.81
92.45
81.46
Cars196 (t-split) trad. distill
91.35
96.43
75.76
86.10
86.91
85.35
75.39
Resisc45 (t-split) repr. consolid.
91.03
97.14
78.57
88.02
87.75
92.08
81.55
Resisc45 (t-split) trad. distill
87.41
96.71
75.90
86.09
85.42
85.25
71.57
iFood (t-split) repr. consolid.
90.75
96.86
78.34
88.00
87.75
92.26
80.20
iFood (t-split) trad. distill
89.44
96.34
77.24
87.33
85.65
88.42
76.30
iFood (t-split) teacher
89.09
96.21
77.76
87.92
84.46
87.97
75.92
Aircrafts (t-split) repr. consolid.
90.93
97.08
78.20
88.01
88.94
91.97
81.15
Aircrafts (t-split) trad. distill
87.31
95.87
75.06
85.29
89.54
81.79
72.30
iNaturalist (t-split) repr. consolid.
91.08
96.74
78.16
87.92
87.81
93.62
82.36
iNaturalist (t-split) trad. distill
88.62
96.02
76.64
86.60
86.61
90.89
79.89
iNaturalist (t-split) teacher
89.37
96.37
76.89
87.09
85.59
91.22
79.62"
REFERENCES,0.9973404255319149,"Cars196 + Resisc45 + iFood (t-split) repr. consolid.
91.05
96.74
78.42
88.15
88.16
92.76
81.43
Cars196 + Resisc45 + iFood (t-split) trad. distill
91.05
96.71
77.47
87.38
87.27
89.71
77.29"
REFERENCES,0.9986702127659575,"Table 7: Fine-tuning, full-shot results raw numbers for Fig. 4 in the main paper (Figs. 11 in the
appendix."
