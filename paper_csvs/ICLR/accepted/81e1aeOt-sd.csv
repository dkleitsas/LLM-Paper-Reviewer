Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011806375442739079,"Model-free reinforcement learning algorithms can compute policy gradients given
sampled environment transitions, but require large amounts of data. In contrast,
model-based methods can use the learned model to generate new data, but model
errors and bias can render learning unstable or suboptimal. In this paper, we present
a novel method that combines real-world data and a learned model in order to get
the best of both worlds. The core idea is to exploit the real-world data for on-
policy predictions and use the learned model only to generalize to different actions.
Speciﬁcally, we use the data as time-dependent on-policy correction terms on top of
a learned model, to retain the ability to generate data without accumulating errors
over long prediction horizons. We motivate this method theoretically and show that
it counteracts an error term for model-based policy improvement. Experiments on
MuJoCo- and PyBullet-benchmarks show that our method can drastically improve
existing model-based approaches without introducing additional tuning parameters."
INTRODUCTION,0.0023612750885478157,"1
INTRODUCTION"
INTRODUCTION,0.0035419126328217238,"Model-free reinforcement learning (RL) has made great advancements in diverse domains such as
single- and multi-agent game playing (Mnih et al., 2015; Silver et al., 2016; Vinyals et al., 2019),
robotics (Kalashnikov et al., 2018), and neural architecture search (Zoph & Le, 2017). All of these
model-free approaches rely on large numbers of interactions with the environment to ensure successful
learning. While this issue is less severe for environments that can easily be simulated, it limits the
applicability of model-free RL to (real-world) domains where data is scarce."
INTRODUCTION,0.004722550177095631,"Model-based RL (MBRL) reduces the amount of data required for policy optimization by approximat-
ing the environment with a learned model, which we can use to generate simulated state transitions
(Sutton, 1990; Racanière et al., 2017; Moerland et al., 2020). While early approaches on low-
dimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models
with closed-form posteriors, recent methods rely on neural networks to scale to complex tasks on
discrete (Kaiser et al., 2020) and continuous (Chua et al., 2018; Kurutach et al., 2018) action spaces.
However, the learned representation of the true environment always remains imperfect, which intro-
duces approximation errors to the RL problem (Atkeson & Santamaria, 1997; Abbeel et al., 2006).
Hence, a key challenge in MBRL is model-bias; small errors in the learned models that compound
over multi-step predictions and lead to lower asymptotic performance than model-free methods."
INTRODUCTION,0.0059031877213695395,"To address these challenges with both model-free and model-based RL, Levine & Koltun (2013);
Chebotar et al. (2017) propose to combine the merits of both. While there are multiple possibilities to"
INTRODUCTION,0.0070838252656434475,∗Work done partially while at the Bosch Center for Artiﬁcial Intelligence.
INTRODUCTION,0.008264462809917356,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009445100354191263,"combine the two methodologies, in this work we focus on improving the model’s predictive state
distribution such that it more closely resembles the data distribution of the true environment."
INTRODUCTION,0.010625737898465172,"Contributions
The main contribution of this paper is on-policy corrections (OPC), a novel
hyperparameter-free methodology that uses on-policy transition data on top of a separately learned
model to enable accurate long-term predictions for MBRL. A key strength of our approach is that it
does not introduce any new parameters that need to be hand-tuned for speciﬁc tasks. We theoretically
motivate our approach by means of a policy improvement bound and show that we can recover the
true state distribution when generating trajectories on-policy with the model. We illustrate how OPC
improves the quality of policy gradient estimates in a simple toy example and evaluate it on various
continuous control tasks from the MuJoCo control suite and their PyBullet variants. There, we
demonstrate that OPC improves current state-of-the-art MBRL algorithms in terms of data-efﬁciency,
especially for the more difﬁcult PyBullet environments."
INTRODUCTION,0.011806375442739079,"Related Work
To counteract model-bias, several approaches combine ideas from model-free and
model-based RL. For example, Levine & Koltun (2013) guide a model-free algorithm via model-
based planning towards promising regions in the state space, Kalweit & Boedecker (2017) augment
the training data by an adaptive ratio of simulated transitions, Talvitie (2017) use ‘hallucinated’
transition tuples from simulated to observed states to self-correct the model, and Feinberg et al.
(2018); Buckman et al. (2018) use a learned model to improve the value function estimate. Janner
et al. (2019) mitigate the issue of compounding errors for long-term predictions by simulating short
trajectories that start from real states. Cheng et al. (2019) extend ﬁrst-order model-free algorithms
via adversarial online learning to leverage prediction models in a regret-optimal manner. Clavera
et al. (2020) employ a model to augment an actor-critic objective and adapt the planning horizon to
interpolate between a purely model-based and a model-free approach. Morgan et al. (2021) combine
actor-critic methods with model-predictive rollouts to guarantee near-optimal simulated data and
retain exploration on the real environment. A downside of most existing approaches is that they
introduce additional hyperparameters that are critical to the learning performance (Zhang et al., 2021)."
INTRODUCTION,0.012987012987012988,"In addition to empirical performance, recent work builds on the theoretical guarantees for model-free
approaches by Kakade & Langford (2002); Schulman et al. (2015) to provide guarantees for MBRL.
Luo et al. (2019) provide a general framework to show monotonic improvement towards a local
optimum of the value function, while Janner et al. (2019) present a lower-bound on performance
for different rollout schemes and horizon lengths. Yu et al. (2020) show guaranteed improvement in
the ofﬂine MBRL setting by augmenting the reward with an uncertainty penalty, while Clavera et al.
(2020) present improvement guarantees in terms of the model’s and value function’s gradient errors."
INTRODUCTION,0.014167650531286895,"Moreover, Harutyunyan et al. (2016) propose a similar correction term as the one introduced in this
paper in the context of off-policy policy evaluation and correct the state-action value function instead
of the transition dynamics. Similarly, Fonteneau et al. (2013) consider the problem of off-policy
policy evaluation but in the batch RL setting and propose to generate ‘artiﬁcial’ trajectories from
observed transitions instead of using an explicit model for the dynamics."
INTRODUCTION,0.015348288075560802,"A related ﬁeld to MBRL that also combines models with data is iterative learning control (ILC)
(Bristow et al., 2006). While RL typically focuses on ﬁnding parametric feedback policies for general
reward functions, ILC instead seeks an open-loop sequence of actions with ﬁxed length to improve
state tracking performance. Moreover, the model in ILC is often derived from ﬁrst principles and then
kept ﬁxed, whereas in MBRL the model is continuously improved upon observing new data. The
method most closely related to RL and our approach is optimization-based ILC (Owens & Hätönen,
2005; Schöllig & D’Andrea, 2009), in which a linear dynamics model is used to guide the search
for optimal actions. Recently, Baumgärtner & Diehl (2020) extended the ILC setting to nonlinear
dynamics and more general reward signals. Little work is available that draws connections between
RL and ILC (Zhang et al., 2019) with one notable exception: Abbeel et al. (2006) use the observed
data from the last rollout to account for a mismatch in the dynamics model. The limitations of this
approach are that deterministic dynamics are assumed, the policy optimization itself requires a line
search procedure with rollouts on the true environment and that it was not combined with model
learning. We build on this idea and extend it to the stochastic setting of MBRL by making use of
recent advances in RL and model learning."
INTRODUCTION,0.01652892561983471,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01770956316410862,Algorithm 1 General Model-based Reinforcement Learning
INTRODUCTION,0.018890200708382526,"1: for n = 1, . . . do
2:
for b = 1, . . . , B do
3:
Rollout policy πn on environment and store transitions Db
n = {(ˆsn,b
t
, ˆan,b
t
,ˆsn,b
t+1)}T −1
t=0
4:
˜pn(st+1 | st, at): Learn a global dynamics model given all data D1:n = Sn
i=1
SB
b=1 Db
i
5:
θn+1 = θn + α∇˜ηn: Optimize the policy based on the model ˜p with any RL algorithm"
PROBLEM STATEMENT AND BACKGROUND,0.020070838252656435,"2
PROBLEM STATEMENT AND BACKGROUND"
PROBLEM STATEMENT AND BACKGROUND,0.021251475796930343,"We consider the Markov decision process (MDP) (S, A, p, r, γ, ρ), where S ⊆RdS and A ⊆RdA
are the continuous state and action spaces, respectively. The unknown environment dynamics are
described by the transition probability p(st+1 | st, at), an initial state distribution ρ(s0) and the
reward signal r(s, a). The goal in RL is to ﬁnd a policy πθ(at | st) parameterized by θ that maximizes
the expected return discounted by γ ∈[0, 1] over episodes of length T,"
PROBLEM STATEMENT AND BACKGROUND,0.02243211334120425,"η =
E
s0:T ,a0:T XT"
PROBLEM STATEMENT AND BACKGROUND,0.023612750885478158,"t=0 γtr(st, at)

,
st+1 ∼p(st+1 | st, at),
s0 ∼ρ,
at ∼πθ(at | st). (1)"
PROBLEM STATEMENT AND BACKGROUND,0.024793388429752067,"The expectation is taken with respect to the trajectory under the stochastic policy πθ starting from a
stochastic initial state s0. Direct maximization of Eq. (1) is challenging, since we do not know the
environment’s transition model p. In MBRL, we learn a model for the transitions and reward function
from data, ˜p(st+1 | st, at) ≈p(st+1 | st, at) and ˜r(st, at) ≈r(st, at), respectively. Subsequently,
we maximize the model-based expected return ˜η as a surrogate problem for the true RL setting, where
˜η is deﬁned as in Eq. (1) but with ˜p and ˜r instead. For ease of exposition, we assume a known reward
function ˜r = r, even though we learn it jointly with ˜p in our experiments."
PROBLEM STATEMENT AND BACKGROUND,0.025974025974025976,"We let ηn denote the return under the policy πn = πθn at iteration n and use ˆs and ˆa for states and
actions that are observed on the true environment. Algorithm 1 summarizes the overall procedure
for MBRL: At each iteration n we store B on-policy trajectories Db
n = {(ˆsn,b
t
, ˆan,b
t
,ˆsn,b
t+1)}T −1
t=0
obtained by rolling out the current policy πn on the real environment in Line 3. Afterwards, we
approximate the environment with a learned model ˜pn(st+1 | st, at) based on the data D1:n in Line 4,
and optimize the policy based on the proxy objective ˜η in Line 5. Note that the policy optimization
algorithm can be off-policy and employ its own, separate replay buffer."
PROBLEM STATEMENT AND BACKGROUND,0.02715466351829988,"Model Choices
The choice of model ˜p plays a key role, since it is used to predict sequences τ
of states transitions and thus deﬁnes the surrogate problem in MBRL. We assume that the model
comes from a distribution family P, which for each state-action pair (st, at) models a distribution
over the next state st+1. The model is then trained to summarize all past data D1:n = ∪n
i=1 ∪B
b=1 Db
i
by maximizing the marginal log-likelihood L,"
PROBLEM STATEMENT AND BACKGROUND,0.02833530106257379,"˜pmodel
n
(st+1 | st, at) = arg max
˜p∈P X"
PROBLEM STATEMENT AND BACKGROUND,0.0295159386068477,"(ˆst,ˆat,ˆst+1)∈D1:n
L(ˆst+1;ˆst, ˆat).
(2)"
PROBLEM STATEMENT AND BACKGROUND,0.030696576151121605,"For a sampled trajectory index b ∼U({1, . . . , B}), sequences τ start from the initial state ˆsn,b
0
and are
distributed according to ˜pmodel
n
(τ | b) = δ(s0 −ˆsn,b
0 ) QT −1
t=0 ˜pmodel
n
(st+1 | st, at)πθ(at | st), where
δ(·) denotes the Dirac-delta distribution. Using model-data for policy optimization is in contrast to
model-free methods, which only use observed environment data by replaying past transitions from
a recent on-policy trajectory b ∈{1, . . . , B}. In our model-based framework, this replay buffer is
equivalent to the non-parametric model"
PROBLEM STATEMENT AND BACKGROUND,0.031877213695395513,"˜pdata
n
(τ | b) = δ(s0 −ˆsn,b
0 )"
PROBLEM STATEMENT AND BACKGROUND,0.03305785123966942,"T −1
Y"
PROBLEM STATEMENT AND BACKGROUND,0.03423848878394333,"t=0
˜pdata
n
(st+1 | t, b), where ˜pdata
n
(st+1 | t, b) = δ(st+1 −ˆsn,b
t+1), (3)"
PROBLEM STATEMENT AND BACKGROUND,0.03541912632821724,where we only replay observed transitions instead of sampling new actions from πθ.
ON-POLICY CORRECTIONS,0.03659976387249114,"3
ON-POLICY CORRECTIONS"
ON-POLICY CORRECTIONS,0.03778040141676505,"In this section, we analyze how the choice of model impacts policy improvement, develop OPC as
a model that can eliminate one term in the improvement bound, and analyze its properties. In the
following, we drop the n sub- and superscript when the iteration is clear from context."
ON-POLICY CORRECTIONS,0.03896103896103896,Published as a conference paper at ICLR 2022
POLICY IMPROVEMENT,0.04014167650531287,"3.1
POLICY IMPROVEMENT"
POLICY IMPROVEMENT,0.04132231404958678,"Independent of whether we use the data directly in ˜pdata or summarize it in a world model ˜pmodel,
our goal is to ﬁnd an optimal policy that maximizes Eq. (1) via the corresponding model-based proxy
objective. To this end, we would like to know how a policy improvement ˜ηn+1 −˜ηn ≥0 based on
the model ˜p, which is what we optimize in MBRL, relates to the true gain in performance ηn+1 −ηn
on the environment with unknown transitions p. While the two are equal without model errors, in
general the larger the model error, the worse we expect the proxy objective to be (Lambert et al.,
2020). Speciﬁcally, we show in Appendix B.1 that the policy improvement can be decomposed as"
POLICY IMPROVEMENT,0.04250295159386069,"ηn+1 −ηn
|
{z
}
True policy improvement"
POLICY IMPROVEMENT,0.043683589138134596,"≥
˜ηn+1 −˜ηn
|
{z
}
Model policy improvement"
POLICY IMPROVEMENT,0.0448642266824085,"−
|ηn+1 −˜ηn+1|
|
{z
}
Off-policy model error"
POLICY IMPROVEMENT,0.04604486422668241,"−
|ηn −˜ηn|
|
{z
}
On-policy model error ,
(4)"
POLICY IMPROVEMENT,0.047225501770956316,"where a performance improvement on our model-based objective ˜η only translates to a gain in Eq. (1)
if two error terms are sufﬁciently small. These terms depend on how well the performance estimate
based on our model, ˜η, matches the true performance, η. If the reward function is known, this term
only depends on the model quality of ˜p relative to p. Note that in contrast to the result by Janner et al.
(2019), Eq. (4) is a bound on the policy improvement instead of a lower bound on ηn+1."
POLICY IMPROVEMENT,0.048406139315230225,"The ﬁrst error term compares ηn+1 and ˜ηn+1, the performance estimation gap under the optimized
policy πn+1 that we obtain in Line 5 of Algorithm 1. Since at this point we have only collected data
with πn in Line 3, this term depends on the generalization properties of our model to new data; what
we call the off-policy model error. For our data-based model ˜pdata that just replays data under πn
independently of the action, this term can be bounded for stochastic policies. For example, Schulman
et al. (2015) bound it by the average KL-divergence between πn and πn+1. For learned models
˜pmodel, it depends on the generalization properties of the model (Luo et al., 2019; Yu et al., 2020).
While understanding model generalization better is an interesting research direction, we will assume
that our learned model is able to generalize to new actions in the following sections."
POLICY IMPROVEMENT,0.049586776859504134,"While the ﬁrst term hinges on model-generalization, the second term is the on-policy model error, i.e.,
the deviation between ηn and ˜ηn under the current policy πn. This error term goes to zero for ˜pdata as
we use more on-policy data B →∞, since the transition data are sampled from the true environment,
c.f., Appendix B.2. While the learned model is also trained with on-policy data, small errors in our
model compound as we iteratively predict ahead in time. Consequently, the on-policy error term
grows as O(min(γ/(1 −γ)2, H/(1 −γ), H2)), c.f., (Janner et al., 2019) and Appendix B.3."
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05076741440377804,"3.2
COMBINING LEARNED MODELS AND REPLAY BUFFER"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05194805194805195,"The key insight of this paper is that the learned model in Eq. (2) and the replay buffer in Eq. (3)
have opposing strengths: The replay buffer has low error on-policy, but high error off-policy since
it replays transitions from past data, i.e., they are independent of the actions chosen under the new
policy. In contrast, the learned model can generalize to new actions by extrapolating from the data
and thus has lower error off-policy, but errors compound over multi-step predictions."
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.053128689492325853,"An ideal model would combine the model-free and model-based approaches in a way such that it
retains the unbiasedness of on-policy generated data, but also generalizes to new policies via the model.
To this end, we propose to use the model to predict how observed transitions would change for a new
state-action pair. In particular, we use the model’s mean prediction ˜fn(s, a) = E[˜pmodel
n
( · | s, a)] to
construct the joint model"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05430932703659976,"˜popc
n (st+1 | st, at, b) = δ
 
st+1 −ˆsn,b
t+1
"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05548996458087367,"|
{z
}
˜pdata
n
(st+1 | t, b)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05667060212514758,"∗δ
 
st+1 −
 ˜fn(st, at) −˜fn(ˆsn,b
t
, ˆan,b
t
)

,
|
{z
}
Model mean correction to generalize to st, at (5)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.05785123966942149,"where ∗denotes the convolution of the two distributions and b refers to a speciﬁc rollout stored
in the replay buffer that was observed in the true environment. Given a trajectory-index b, ˜popc
n
in Eq. (5) transitions deterministically according to st+1 = ˆsn,b
t+1 + ˜fn(st, at) −˜fn(ˆsn,b
t
, ˆan,b
t
),
resembling the equations in ILC (c.f., Baumgärtner & Diehl (2020) and Appendix E). If we roll
out ˜popc
n
along a trajectory, starting from a state ˆsn,b
t
and apply the recorded actions from the
replay buffer, ˆan,b
t
, the correction term on the right of Eq. (5) cancels out and we have ˜popc
n (st+1 |
ˆsn,b
t
, ˆan,b
t
, b) = ˜pdata
n
(st+1 | t, b) = δ(st+1 −ˆsn,b
t+1). Thus OPC retrieves the true on-policy data"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.0590318772136954,Published as a conference paper at ICLR 2022
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.0602125147579693,"ˆsb
0 = s0"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06139315230224321,"ˆsb
1 = ˜popc(s1|ˆsb
0, ˆab
0, b)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06257378984651712,"˜pmodel(s1|ˆsb
0, ˆab
0)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06375442739079103,"˜pmodel(s1|s0, a0)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06493506493506493,"˜popc(s1|s0, a0, b)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06611570247933884,"ˆsb
2 = ˜popc(s2|ˆsb
1, ˆab
1, b)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06729634002361275,"˜pmodel(s2|ˆsb
1, ˆab
1)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06847697756788666,"˜pmodel(s2|s1, a1)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.06965761511216056,"˜popc(s2|s1, a1, b)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07083825265643448,(a) Multi-step predictions with OPC for a given n.
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07201889020070838,Time Steps State
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07319952774498228,on-policy
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.0743801652892562,off-policy
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.0755608028335301,"Model (on-policy)
Model (off-policy)"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07674144037780402,"True environment
Model + OPC"
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07792207792207792,(b) State distribution over time.
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.07910271546635184,"Figure 1: Illustration to compare predictions of the three models Eqs. (2), (3) and (5) starting from the
same state ˆsb
0. In Fig. 1a, we see that on-policy, i.e., using actions (ˆab
0, ˆab
1), ˜pdata returns environment
data, while ˜pmodel (blue) is biased. We correct this on-policy bias in expectation to obtain ˜popc. This
allows us to retain the true state distribution when predicting with these models recursively (c.f.,
bottom three lines in Fig. 1b). When using OPC for off-policy actions (a0, a1), ˜popc does not recover
the true off-policy state distribution since it relies on the biased model. However, the corrections
generalize locally and reduce prediction errors in Fig. 1b (top three lines)."
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.08028335301062574,"distribution independent of the prediction quality of the model, which is why we refer to this method
as on-policy corrections (OPC). This behavior is illustrated in Fig. 1a, where the model (blue) is
biased on-policy, but OPC corrects the model’s prediction to match the true data. In Fig. 1b, we show
how this affects predicted rollouts on a simple stochastic double-integrator environment. Although
small on-policy errors in ˜pmodel (blue) compound over time, the corresponding ˜popc matches the
ground-truth environment data closely. Note that even though the model in Eq. (5) is deterministic,
we retain the environment’s stochasticity from the data in the transitions to ˆst+1, so that we recover
the on-policy aleatoric uncertainty (noise) from sampling different reference trajectories via indexes b."
COMBINING LEARNED MODELS AND REPLAY BUFFER,0.08146399055489964,"When our actions at are different from ˆab
t, ˜popc still uses the data from the environment’s transitions,
but the correction term in Eq. (5) uses the learned model to predict how the next state changes in
expectation relative to the prediction under ˆab
t. That is, in Fig. 1a for a new at the model predicts the
state distribution shown in red. Correspondingly, we shift the static prediction ˆsb
t+1 by the difference
in means (gray arrow) between the two predictions; i.e., the change in trajectory by changing from
ˆab
t to at. Since we shift the model predictions by a time-dependent but constant offset, this does not
recover the true state distribution unless the model has zero error. However, empirically it can still
help with long-term predictions in Fig. 1b by shifting the model off-policy predictions (red) to the
OPC predictions (green), which are closer to the environment’s state distribution under the new policy."
THEORETICAL ANALYSIS,0.08264462809917356,"3.3
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.08382526564344746,"In the previous sections, we have introduced OPC to decrease the on-policy model error in Eq. (4) and
tighten the improvement bound. In this section, we analyze the on-policy performance gap from a
theoretical perspective and show that with OPC this error can be reduced independently of the learned
model’s error. To this end, we assume inﬁnitely many on-policy reference trajectories, B →∞,
which is equivalent to a variant of ˜popc that considers ˆsb
t+1 as a random variable that follows the true
environment’s transition dynamics. While impossible to implement in practice, this formulation is
useful to understand our method. We deﬁne the generalized OPC-model as"
THEORETICAL ANALYSIS,0.08500590318772137,"˜popc
⋆
(st+1 | st, at, b) =
p(ˆst+1 | ˆsb
t, ˆab
t)
|
{z
}
Environment on-policy transition"
THEORETICAL ANALYSIS,0.08618654073199528,"∗
δ

st+1 −
h
˜f(st, at) −˜f(ˆsb
t, ˆab
t)
i"
THEORETICAL ANALYSIS,0.08736717827626919,"|
{z
}"
THEORETICAL ANALYSIS,0.0885478158205431,"OPC correction term ,
(6)"
THEORETICAL ANALYSIS,0.089728453364817,"which highlights that it transitions according to the true on-policy dynamics conditioned on data
from the replay buffer, combined with a correction term. We provide a detailed derivation for the
generalized model in Appendix B, Lemma 4. With Eq. (6), we have the following result:
Theorem 1. Let ˜ηopc
⋆
and η be the expected return under the generalized OPC-model Eq. (6) and
the true environment, respectively. Assume that the learned model’s mean transition function
˜f(st, at) = E[˜pmodel(st+1 | st, at)] is Lf-Lipschitz and the reward r(st, at) is Lr-Lipschitz. Further,"
THEORETICAL ANALYSIS,0.09090909090909091,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.09208972845336481,"if the policy π(at | st) is Lπ-Lipschitz with respect to st under the Wasserstein distance and
its (co-)variance Var[π(at | st)] = Σπ(st) ∈SdA
+
is ﬁnite over the complete state space, i.e.,"
THEORETICAL ANALYSIS,0.09327036599763873,"maxst∈S trace{Σπ(st)} ≤¯σ2
π, then with C1 =
p"
THEORETICAL ANALYSIS,0.09445100354191263,"2(1 + L2π)LfLr and C2 =
q"
THEORETICAL ANALYSIS,0.09563164108618655,"L2
f + L2π"
THEORETICAL ANALYSIS,0.09681227863046045,"|η −˜ηopc
⋆
| ≤
¯σπ
1 −γ dA"
THEORETICAL ANALYSIS,0.09799291617473435,"1
4 C1CT
2
√"
THEORETICAL ANALYSIS,0.09917355371900827,"T.
(7)"
THEORETICAL ANALYSIS,0.10035419126328217,"We provide a proof in Appendix B.4. From Theorem 1, we can observe the key property of OPC:
for deterministic policies, the on-policy model error from Eq. (4) is zero and independent of the
learned models’ predictive distribution ˜pmodel, so that η = ˜ηopc
⋆
. For policies with non-zero variance,
the bound scales exponentially with T, highlighting the problem of compounding errors. In this
case, as in the off-policy case, the model quality determines how well we can generalize to different
actions. We show in Appendix B.5 that, for one-step predictions, OPC’s prediction error scales as the
minimum of policy variance and model error. To further alleviate the issue of compounding errors,
one could extend Theorem 1 with a branched rollout scheme similarly to the results by Janner et al.
(2019), such that the rollouts are only of length H ≪T."
THEORETICAL ANALYSIS,0.10153482880755609,"In practice, ˜popc
⋆
cannot be realized as it requires the true (unknown) state transition model p. However,
as we use more on-policy reference trajectories for ˜popc in Eq. (5), it also converges to zero on-policy
error in probability for deterministic policies.
Lemma 1. Let M be a MDP with dynamics p(st+1 | st, at) and reward r < rmax. Let ˜
M be another
MDP with dynamics ˜pmodel ̸= p. Assume a deterministic policy π : S 7→A and a set of trajectories
D = SB
b=1{(ˆsb
t, ˆab
t,ˆsb
t+1)}T −1
t=0 collected from M under π. If we use OPC Eq. (5) with data D, then"
THEORETICAL ANALYSIS,0.10271546635182999,"lim
B→∞Pr (|η −˜ηopc| > ε) = 0
∀ε > 0
with convergence rate
O(1/B).
(8)"
THEORETICAL ANALYSIS,0.1038961038961039,"We provide a proof in Appendix B.4. Lemma 1 states that given sufﬁcient reference on-policy data,
the performance gap due to model errors becomes arbitrarily small for any model ˜pmodel when
using OPC. While the assumption of inﬁnite on-policy data in Lemma 1 is unrealistic for practical
applications, we found empirically that OPC drastically reduces the on-policy model error even when
the assumptions are violated. In our implementation, we use stochastic policies as well as trajectories
from previous policies, i.e., off-policy data, for the corrections in OPC (see also Section 4.2)."
DISCUSSION,0.1050767414403778,"3.4
DISCUSSION"
DISCUSSION,0.10625737898465171,"Epistemic uncertainty
So far, we have only considered aleatoric uncertainty (noise) in our
transition models. In practice, modern methods additionally distinguish epistemic uncertainty that
arises from having seen limited data (Deisenroth & Rasmussen, 2011; Chua et al., 2018). This leads
to a distribution (or an ensemble) of models, where each sample could explain the data. In this setting,
we apply OPC by correcting each sample individually. This allows us to retain epistemic uncertainty
estimates after applying OPC, while the epistemic uncertainty is zero on-policy."
DISCUSSION,0.10743801652892562,"Limitations
Since OPC uses on-policy data, it is inherently limited to local policy optimization
where the policy changes slowly over time. As a consequence, it is not suitable for global exploration
schemes like the one proposed by Curi et al. (2020). Similarly, since OPC uses the observed data
and corrects it only with the expected learned model, ˜popc always uses the on-policy transition
noise (aleatoric uncertainty) from the data, even if the model has learned to represent it. While not
having to learn a representation for aleatoric uncertainty can be a strength, it limits our approach to
environments where the aleatoric uncertainty does not vary signiﬁcantly with states/actions. It is
possible to extend the method to the heteroscedastic noise setting under additional assumptions that
enable distinguishing model error from transition noise (Schöllig & D’Andrea, 2009). Lastly, our
method applies directly only to MDPs, since we rely on state-observations. Extending the ideas to
partially observed environments is an interesting direction for future research."
EXPERIMENTAL RESULTS,0.10861865407319952,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.10979929161747344,"We begin the experimental section with a motivating example on a toy problem to highlight the
impact of OPC on the policy gradient estimates in the presence of model errors. The remainder of the
section focuses on comparative evaluations and ablation studies on complex continuous control tasks."
EXPERIMENTAL RESULTS,0.11097992916174734,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.11216056670602124,"Figure 2: Signed gradient error when using inaccurate models Eq. (9) to estimate the policy gradient
without (left) and with (right) OPC. The background’s opacity depicts the error’s magnitude, whereas
color denotes if the sign of estimated and true gradient differ (red) or coincide (blue). OPC improves
the gradient estimate in the presence of model errors. Note that the optimal policy is θ∗= −1.0."
ILLUSTRATIVE EXAMPLE,0.11334120425029516,"4.1
ILLUSTRATIVE EXAMPLE"
ILLUSTRATIVE EXAMPLE,0.11452184179456906,"In Section 3.3, we investigate the inﬂuence of the model error directly on the expected return
from a theoretical perspective. From a practical standpoint, another relevant question is how the
policy optimization and the respective policy gradients are inﬂuenced by model errors. For general
environments and reward signals, this question is difﬁcult to answer, due to the typically high-
dimensional state/action spaces and large number of parameters governing the dynamics model as
well as the policy. To shed light on this open question, we resort to a simple low-dimensional example
and investigate how OPC improves the gradient estimates under a misspeciﬁed dynamics model."
ILLUSTRATIVE EXAMPLE,0.11570247933884298,"In particular, we assume a one-dimensional deterministic environment with linear transitions and
a linear policy. The beneﬁts of this example are that we can 1) compute the true policy gradient
based on a single rollout (determinism), 2) determine the environment’s closed-loop stability under
the policy (linearity), and 3) visualize the gradient error as a function of all relevant parameters
(low dimensionality). The dynamics and initial state distribution are speciﬁed by p(st+1 | st, at) =
δ(Ast + Bat | st, at) with ρ(s0) = δ(s0) where A, B ∈R and δ(·) denotes the Dirac-delta
distribution. We deﬁne a deterministic linear policy πθ(at | st) = δ(θst | st) that is parameterized
by the scalar θ ∈R. The objective is to drive the state to zero, which we encode with an exponential
reward r(st, at) = exp

−(st/σr)2	
. Further, we assume an approximate dynamics model"
ILLUSTRATIVE EXAMPLE,0.11688311688311688,"˜p(st+1 | st, at) = δ((A + ∆A)st + (B + ∆B)at | st, at),
(9)"
ILLUSTRATIVE EXAMPLE,0.1180637544273908,"where ∆A, ∆B quantify the mismatch between the approximate model and the true environment. In
practice, the mismatch can arise due to noise-corrupted observations of the true state or, in the case of
stochastic environments, due to a ﬁnite amount of training data."
ILLUSTRATIVE EXAMPLE,0.1192443919716647,"With the setting outlined above, we investigate how model errors inﬂuence the estimation of policy
gradients. To this end, we roll out different policies under models with varying degrees of error ∆B.
For each policy/model combination, we compute the model-based policy gradient and compare it to
the true gradient. The results are summarized in Fig. 2, where the background’s opacity depicts the
gradient error’s magnitude and its color indicates whether the respective signs of the gradients are
the same (blue, ≥0) or differ (red, < 0). For policy optimization, the sign of the gradient estimate
is paramount. However, we see in the left-hand image that even small model errors can lead to
the wrong sign of the gradient. OPC signiﬁcantly reduces the magnitude of the gradient error and
increases the robustness towards model errors. See also Appendix C for a more in-depth analysis."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1204250295159386,"4.2
EVALUATION ON CONTINUOUS CONTROL TASKS"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12160566706021252,"In the following section, we investigate the impact of OPC on a range of continuous control tasks.
To this end, we build upon the current state-of-the-art model-based RL algorithm MBPO (Janner
et al., 2019). Further, we aim to answer the important question about how data diversity affects
MBRL, and OPC in particular. While having a model that can generate (theoretically) an inﬁnite
amount of data is intriguing, the beneﬁt of having more data is limited by the model quality in terms"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12278630460448642,Published as a conference paper at ICLR 2022 0 5 10
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12396694214876033,MuJoCo
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12514757969303425,Evaluation return ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12632821723730814,HalfCheetah 0 1 2 3
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12750885478158205,Hopper 0 2 4
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12868949232585597,Walker2d 0 2 4 6
EVALUATION ON CONTINUOUS CONTROL TASKS,0.12987012987012986,AntTruncatedObs
EVALUATION ON CONTINUOUS CONTROL TASKS,0.13105076741440377,"0
100
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1322314049586777,# Steps ×103 0 1 2
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1334120425029516,PyBullet
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1345926800472255,"Evaluation return ×103 0
100"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1357733175914994,"# Steps ×103 0 1 2 0
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.13695395513577333,"# Steps ×103 0.0 0.5 1.0 1.5 0
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1381345926800472,# Steps ×103 1 2 3 4
EVALUATION ON CONTINUOUS CONTROL TASKS,0.13931523022432113,"Figure 3: Comparison of OPC (
), MBPO(⋆) (
) and SAC (
) on four environments from the
MuJoCo control suite (top row) and their respective PyBullet implementations (bottom row)."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14049586776859505,"of being representative of the true environment. For OPC, the following questions arise from this
consideration: Do longer rollouts help to generate better data? And is there a limit to the value of
simulated transition data, i.e., is more always better?"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14167650531286896,"For the dynamics model ˜pmodel, we follow Chua et al. (2018); Janner et al. (2019) and use a proba-
bilistic ensemble of neural networks, where each head predicts a Gaussian distribution over the next
state and reward. For policy optimization, we employ the soft actor critic (SAC) algorithm by Haarnoja
et al. (2018). All learning curves are presented in terms of the median (lines) and interquartile range
(shaded region) across ten independent experiments, where we smooth the evaluation return with a
moving average ﬁlter to accentuate the results of particularly noisy environments. Apart from small
variations in the hyperparameters, the only difference between OPC and MBPO is that our method
uses ˜popc, while MBPO uses ˜pmodel. We provide pseudo-code for the model rollouts of MBPO and
OPC in Algorithm 2 in Appendix A.1. Generally, we found that OPC was more robust to the choice of
hyperparameters. The rollout horizon to generate training data is set to H = 10 for all experiments.
Note that when using ˜pdata to generate data, we retain the standard (model-free) SAC algorithm."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14285714285714285,"Our implementation is based upon the code from Janner et al. (2019). We made the following changes
to the original implementation: 1) The policy is only updated at the end of an epoch, not during
rollouts on the true environment. 2) The replay buffer retains data for a ﬁxed number of episodes, to
clearly distinguish on- and off-policy data. 3) For policy optimization, MBPO uses a small number of
environment transitions in addition to those from the model. We found that this design choice did
not consistently improve performance and added another level of complexity. Therefore, we refrain
from mixing environment and model transitions and only use simulated data for policy optimization.
While we stay true to the key ideas of MBPO under these changes, we denote our variant as MBPO(⋆)
to avoid ambiguity. See Appendices D.6 and D.7 for a comparison to the original MBPO algorithm."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14403778040141677,"Comparative Evaluation
We begin our analysis with a comparison of our method to MBPO(⋆)
and SAC on four continuous control benchmark tasks from the MuJoCo control suite (Todorov et al.,
2012) and their respective PyBullet variants (Ellenberger, 2018–2019). The results are presented
in Fig. 3. We see that the difference in performance between both methods is only marginal when
evaluated on the MuJoCo environments (Fig. 3, top row). Notably, the situation changes drastically
for the PyBullet environments (Fig. 3, bottom row). Here, MBPO(⋆) exhibits little to no learning
progress, whereas OPC succeeds at learning a good policy with few interactions in the environment.
One of the main differences between the environments (apart from the physics engine itself) is that
the PyBullet variants have initial state distributions with signiﬁcantly larger variance."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14521841794569068,"Inﬂuence of State Representation
In general, the success of RL algorithms should be agnostic to
the way an environment represents its state. In robotics, joint angles ϑ are often re-parameterized by
a sine/cosine transformation, ϑ 7→[sin(ϑ), cos(ϑ)]. We show that even for the simple CartPole envi-"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14639905548996457,Published as a conference paper at ICLR 2022
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14757969303423848,"0
2
4
6"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1487603305785124,# Steps ×103 0.25 0.50 0.75 1.00
EVALUATION ON CONTINUOUS CONTROL TASKS,0.14994096812278632,Evaluation return ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1511216056670602,RoboSchool (original)
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15230224321133412,"[sin(ϑ),cos(ϑ)]"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15348288075560804,"0
2
4
6"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15466351829988192,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15584415584415584,"RoboSchool (transf.)
[sin(ϑ),cos(ϑ)] 7→ϑ"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15702479338842976,"0
2
4
6"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15820543093270367,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.15938606847697756,PyBullet (original) ϑ
EVALUATION ON CONTINUOUS CONTROL TASKS,0.16056670602125148,"0
2
4
6"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1617473435655254,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.16292798110979928,"Pybullet (transf.)
ϑ 7→[sin(ϑ),cos(ϑ)]"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1641086186540732,"Figure 4: Comparison of OPC (
) and MBPO(⋆) (
) on different variants of the CartPole
environment. When the pole’s angle ϑ is observed directly (center plots), both algorithms successfully
learn a policy. With the sine/cosine transformations (outer plots), MBPO(⋆) fails to solve the task."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1652892561983471,"0
100
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.16646989374262103,# Steps ×103 0 3 6 9 12
EVALUATION ON CONTINUOUS CONTROL TASKS,0.16765053128689492,Evaluation return ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.16883116883116883,N = 20 ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17001180637544275,"0
100
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17119244391971664,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17237308146399055,N = 40 ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17355371900826447,"0
100
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17473435655253838,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.17591499409681227,N = 100 ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1770956316410862,"0
100
200"
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1782762691853601,# Steps ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.179456906729634,N = 200 ×103
EVALUATION ON CONTINUOUS CONTROL TASKS,0.1806375442739079,"Figure 5: Ablation study for OPC on the HalfCheetah environment. In each plot, we ﬁx the number
of simulated transitions N and vary the rollout lengths H = {1(
), 5(
), 10(
)}."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.18181818181818182,"ronment, the parameterization of the pole’s angle has a large inﬂuence on the performance of MBRL.
In particular, we compare OPC and MBPO(⋆) on the RoboSchool and PyBullet variants of the CartPole
environment, which represent the pole’s angle with and without the sine/cosine transformation. The
results are shown in Fig. 4. To rule out other effects than the angle’s representation, we repeat the
experiment for each implementation but transform the state to the other representation, respectively.
Notably, OPC successfully learns a policy irrespective of the state’s representation, whereas MBPO(⋆)
fails if the angle of the pole is represented by the sine/cosine transformation."
EVALUATION ON CONTINUOUS CONTROL TASKS,0.18299881936245574,"Inﬂuence of Data Diversity
Here, we investigate whether multi-step predictions are in fact
beneﬁcial compared to single-step predictions during the data generation process. To this end,
we keep the total number of simulated transitions N for training constant, but choose different
horizon lengths H = {1, 5, 10}. The corresponding numbers of simulated rollouts are then given by
nrollout = N/H. The results for N = {20, 40, 100, 200} × 103 on the HalfCheetah environment are
shown in Fig. 5. First, note that more data leads to a higher asymptotic return, but after a certain point
more data only leads to diminishing returns. Further, the results indicate that one-step predictions
are not enough to generate sufﬁciently diverse data. Note that this result contradicts the ﬁndings by
Janner et al. (2019) that one-step predictions are often sufﬁcient for MBPO."
CONCLUSION,0.18417945690672963,"5
CONCLUSION"
CONCLUSION,0.18536009445100354,"In this paper, we have introduced on-policy corrections (OPC), a novel method that combines observed
transition data with model-based predictions to mitigate model-bias in MBRL. In particular, we extend
a replay buffer with a learned model to account for state-action pairs that have not been observed on
the real environment. This approach enables the generation of more realistic transition data to more
closely match the true state distribution, which was further motivated theoretically by a tightened
improvement bound on the expected return. Empirically, we demonstrated superior performance on
high-dimensional continuous control tasks as well as robustness towards state representations."
CONCLUSION,0.18654073199527746,Published as a conference paper at ICLR 2022
REFERENCES,0.18772136953955135,REFERENCES
REFERENCES,0.18890200708382526,"Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using Inaccurate Models in Reinforcement
Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1–8,
2006."
REFERENCES,0.19008264462809918,"Christopher G. Atkeson and Juan Carlos Santamaria. A Comparison of Direct and Model-Based
Reinforcement Learning. In Proceedings of the IEEE International Conference on Robotics and
Automation (ICRA), pp. 3557–3564, 1997."
REFERENCES,0.1912632821723731,"K. Baumgärtner and M. Diehl. Zero-Order Optimization-Based Iterative Learning Control. In
Proceedings of the IEEE Conference on Decision and Control, pp. 3751–3757, 2020."
REFERENCES,0.19244391971664698,"Joseph K. Blitzstein and Jessica Hwang. Introduction to Probability. CRC Press, 2019."
REFERENCES,0.1936245572609209,"Douglas A. Bristow, Marina Tharayil, and Andrew G. Alleyne. A Survey of Iterative Learning
Control. IEEE Control Systems Magazine, 26(3):96–114, 2006."
REFERENCES,0.19480519480519481,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efﬁcient
Reinforcement Learning with Stochastic Ensemble Value Expansion. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 8224–8234, 2018."
REFERENCES,0.1959858323494687,"Frank M. Callier and Charles A. Desoer. Linear System Theory. Springer, 1991."
REFERENCES,0.19716646989374262,"Yevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey
Levine. Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement
Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp.
703–711, 2017."
REFERENCES,0.19834710743801653,"Ching-An Cheng, Xinyan Yan, Nathan Ratliff, and Byron Boots. Predictor-Corrector Policy Op-
timization. In Proceedings of the International Conference on Machine Learning (ICML), pp.
1151–1161, 2019."
REFERENCES,0.19952774498229045,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep Reinforcement
Learning in a Handful of Trials Using Probabilistic Dynamics Models. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 4754–4765, 2018."
REFERENCES,0.20070838252656434,"Ignasi Clavera, Yao Fu, and Pieter Abbeel. Model-Augmented Actor-Critic: Backpropagating
Through Paths. In Proceedings of the International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.20188902007083825,"Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efﬁcient Model-Based Reinforcement
Learning Through Optimistic Policy Search and Planning. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 14156–14170, 2020."
REFERENCES,0.20306965761511217,"Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A Model-Based and Data-Efﬁcient
Approach to Policy Search . In Proceedings of the International Conference on Machine Learning
(ICML), pp. 465–472, 2011."
REFERENCES,0.20425029515938606,"Benjamin
Ellenberger.
PyBullet
Gymperium.
https://github.com/benelot/
pybullet-gym, 2018–2019."
REFERENCES,0.20543093270365997,"Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine.
Model-Based Value Estimation for Efﬁcient Model-Free Reinforcement Learning.
arXiv:1803.00101 [cs.LG], 2018."
REFERENCES,0.2066115702479339,"Raphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch Mode Reinforce-
ment Learning Based on the Synthesis of Artiﬁcial Trajectories. Annals of Operations Research,
208(1):383–416, 2013."
REFERENCES,0.2077922077922078,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the
International Conference on Learning Representations (ICLR), pp. 1861–1870, 2018."
REFERENCES,0.2089728453364817,Published as a conference paper at ICLR 2022
REFERENCES,0.2101534828807556,"Anna Harutyunyan, Marc G. Bellemare, Tom Stepleton, and Rémi Munos. Q(λ) with Off-Policy
Corrections. In International Conference on Algorithmic Learning Theory, pp. 305–320, 2016."
REFERENCES,0.21133412042502953,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to Trust Your Model: Model-
Based Policy Optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
12519–12530, 2019."
REFERENCES,0.21251475796930341,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-Based Reinforcement Learning
for Atari. In Proceedings of the International Conference on Learning Representations (ICLR),
2020."
REFERENCES,0.21369539551357733,"Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement Learning. In
Proceedings of the International Conference on Machine Learning (ICML), pp. 267–274, 2002."
REFERENCES,0.21487603305785125,"Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable
Deep Reinforcement Learning for Vision-Based Robotic Manipulation. In Proceedings of the
Conference on Robot Learning (CoRL), pp. 651–673, 2018."
REFERENCES,0.21605667060212513,"Gabriel Kalweit and Joschka Boedecker. Uncertainty-Driven Imagination for Continuous Deep
Reinforcement Learning. In Proceedings of the Conference on Robot Learning (CoRL), pp.
195–206, 2017."
REFERENCES,0.21723730814639905,"Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of
the International Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.21841794569067297,"Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shaﬁeezadeh-Abadeh.
Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning.
In Operations Research & Management Science in the Age of Analytics, pp. 130–166. INFORMS,
2019."
REFERENCES,0.21959858323494688,"Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-Ensemble
Trust-Region Policy Optimization. In Proceedings of the International Conference on Learning
Representations (ICLR), 2018."
REFERENCES,0.22077922077922077,"Nathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in
model-based reinforcement learning. In Proceedings of the Annual Learning for Dynamics and
Control Conference (L4DC), pp. 761–770, 2020."
REFERENCES,0.22195985832349469,"Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 1–9, 2013."
REFERENCES,0.2231404958677686,"Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
Framework for Model-Based Deep Reinforcement Learning with Theoretical Guarantees. In
Proceedings of the International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.2243211334120425,"Ester Mariucci and Markus Reiß. Wasserstein and Total Variation Distance Between Marginals of
Lévy Processes. Elecronic Journal of Statistics, 12(2):2482–2514, 2018."
REFERENCES,0.2255017709563164,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning.
Nature, 518(7540):529–533, 2015."
REFERENCES,0.22668240850059032,"Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-Based Reinforcement Learning:
A Survey. arXiv:2006.16712 [cs.LG], 2020."
REFERENCES,0.22786304604486424,"Andrew Morgan, Daljeet Nandha, Georgia Chalvatzaki, Carlo D’Eramo, Aaron Dollar, and Jan Peters.
Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement
Learning. In Proceedings of the IEEE International Conference on Robotics and Automation
(ICRA), pp. 6672–6678, 2021."
REFERENCES,0.22904368358913813,Published as a conference paper at ICLR 2022
REFERENCES,0.23022432113341204,"David H. Owens and Jari Hätönen. Iterative Learning Control - An Optimization Paradigm. Annual
Reviews in Control, 29(1):57–70, 2005."
REFERENCES,0.23140495867768596,"Victor M. Panaretos and Yoav Zemel. Statistical Aspects of Wasserstein Distances. Annual Review of
Statistics and Its Application, 6:405–431, 2019."
REFERENCES,0.23258559622195984,"Sébastien Racanière, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo
Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pas-
canu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-Augmented
Agents for Deep Reinforcement Learning. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 5694–5705, 2017."
REFERENCES,0.23376623376623376,"R. Tyrrell Rockafellar. Integral Functionals, Normal Integrands and Measurable Selections. In
Nonlinear operators and the calculus of Variations, pp. 157–207. Springer, 1976."
REFERENCES,0.23494687131050768,"Jeff G. Schneider. Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning.
Advances in Neural Information Processing Systems (NeurIPS), pp. 1047–1053, 1997."
REFERENCES,0.2361275088547816,"Angela P. Schöllig and Raffaello D’Andrea. Optimization-Based Iterative Learning Control for
Trajectory Tracking. In Proceedings of the European Control Conference (ECC), pp. 1505–1510,
2009."
REFERENCES,0.23730814639905548,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region
Policy Optimization. In Proceedings of the International Conference on Machine Learning (ICML),
pp. 1889–1897, 2015."
REFERENCES,0.2384887839433294,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv:1707.06347 [cs.LG], 2017."
REFERENCES,0.2396694214876033,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game of Go with
Deep Neural Networks and Tree Search. Nature, 529:484–489, 2016."
REFERENCES,0.2408500590318772,"Richard S. Sutton. Integrated Architectures for Learning, Planning, and Reacting Based on Approx-
imating Dynamic Programming. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 216–224, 1990."
REFERENCES,0.24203069657615112,"Erik Talvitie. Self-Correcting Models for Model-Based Reinforcement Learning. In Proceedings of
the AAAI National Conference on Artiﬁcial Intelligence, pp. 2597–2603, 2017."
REFERENCES,0.24321133412042503,"Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics Engine for Model-Based Control.
In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 5026–5033, 2012."
REFERENCES,0.24439197166469895,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom
Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
Grandmaster Level in StarCraft II Using Multi-Agent Reinforcement Learning. Nature, 575(7782):
350–354, 2019."
REFERENCES,0.24557260920897284,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: Model-Based Ofﬂine Policy Optimization. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 14129–14142, 2020."
REFERENCES,0.24675324675324675,"Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, André Biedenkapp, Kurtland Chua, Frank
Hutter, and Roberto Calandra. On the Importance of Hyperparameter Optimization for Model-
Based Reinforcement Learning. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pp. 4015–4023, 2021."
REFERENCES,0.24793388429752067,Published as a conference paper at ICLR 2022
REFERENCES,0.24911452184179456,"Yueqing Zhang, Bing Chu, and Zhan Shu. A Preliminary Study on the Relationship Between Iterative
Learning Control and Reinforcement Learning. IFAC-PapersOnLine, 52(29):314–319, 2019."
REFERENCES,0.2502951593860685,"Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. In Proceed-
ings of the International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.2514757969303424,Published as a conference paper at ICLR 2022
REFERENCES,0.2526564344746163,SUPPLEMENTARY MATERIAL
REFERENCES,0.2538370720188902,"In the appendix we provide additional details on our method, ablation studies, and the detailed
hyperparameter conﬁgurations used in the paper. An overview is shown below."
REFERENCES,0.2550177095631641,Table of Contents
REFERENCES,0.256198347107438,"A Implementation Details and Computational Resources
15
A.1
Detailed Algorithm for Rollouts with OPC
. . . . . . . . . . . . . . . . . . . .
15
A.2
Hyperparameter Settings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.3
Implementation and Computational Resources . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.25737898465171194,"B
Theoretical Analysis of On-Policy Corrections
17
B.1
General Policy Improvement Bound . . . . . . . . . . . . . . . . . . . . . . . .
17
B.2
Properties of the Replay Buffer . . . . . . . . . . . . . . . . . . . . . . . . . .
17
B.3
Properties of the Learned Model . . . . . . . . . . . . . . . . . . . . . . . . . .
18
B.4
Properties of OPC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
B.5
Model Errors in OPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.2585596221959858,"C Motivating Example – In-depth Analysis
27
C.1
Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
C.2
Reward Landscapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
C.3
Inﬂuence of Model Error
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
C.4
Inﬂuence of Off-Policy Error
. . . . . . . . . . . . . . . . . . . . . . . . . . .
29
C.5
Additional Information
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
REFERENCES,0.2597402597402597,"D Additional Experimental Results
32
D.1
Comparison with Other Baseline Algorithms . . . . . . . . . . . . . . . . . . .
32
D.2
Ablation - Retain Epochs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
D.3
Inﬂuence of State Representation: In-Depth Analysis . . . . . . . . . . . . . . .
33
D.4
Improvement Bound: Empirical Investigation . . . . . . . . . . . . . . . . . . .
35
D.5
Off-Policy Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
D.6
Implementation Changes to Original MBPO . . . . . . . . . . . . . . . . . . . .
37
D.7
Results for Original MBPO Implementation . . . . . . . . . . . . . . . . . . . .
38"
REFERENCES,0.26092089728453366,"E
Connection between MBRL and ILC
41
E.1
Norm-optimal ILC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
E.2
Model-based RL to Norm-optimal ILC . . . . . . . . . . . . . . . . . . . . . .
42
E.3
Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43"
REFERENCES,0.26210153482880755,Published as a conference paper at ICLR 2022
REFERENCES,0.2632821723730815,"A
IMPLEMENTATION DETAILS AND COMPUTATIONAL RESOURCES"
REFERENCES,0.2644628099173554,"A.1
DETAILED ALGORITHM FOR ROLLOUTS WITH OPC"
REFERENCES,0.26564344746162927,"In Section 3.2 and Fig. 1a, we have introduced the OPC transition model and how to roll out trajectories
with the model. Here, we will give more details on the algorithmic implementation for the generation
of simulated data. Algorithm 2 follows the branched rollout scheme from MBPO (Janner et al., 2019).
Differences to MBPO are highlighted in blue."
REFERENCES,0.2668240850059032,"Generally, OPC only requires a deterministic transition function ˜f to compute the corrective term
in Line 8 in Algorithm 2. For models that include aleatoric uncertainty, we choose ˜f(st, at) =
Est′[˜p(st′ | st, at)]. If, in addition, the model includes epistemic uncertainty, we refer to the
comment in Section 3.4 in the main part of the paper."
REFERENCES,0.2680047225501771,"In practice, rollouts on the true environment are terminated early if, for instance, a particular state
exceeds a user-deﬁned boundary. Consequently, not all trajectories in the replay buffer are necessarily
of length T. Since the prediction in Line 8 requires valid transition tuples for the correction term, we
additionally check in Line 10 whether the next reference state is a terminal state. Thus, in contrast to
MBPO, for OPC we terminate the inner loop in Algorithm 2 early if either a simulated or reference
state is a terminal state."
REFERENCES,0.269185360094451,Algorithm 2 Branched rollout scheme with OPC model (differences to MBPO highlighted in blue)
REFERENCES,0.27036599763872493,Input: Required parameters:
REFERENCES,0.2715466351829988,"• Set of trajectories Db
n = {(ˆsn,b
t
, ˆan,b
t
,ˆsn,b
t+1)}T −1
t=0 for b ∈{1, . . . , B}"
REFERENCES,0.2727272727272727,"• Environment model ˜p(st′ | st, at). Deﬁne ˜f(s, a) = Est′[˜p(st′ | s, a)].
• Policy πθ
• Prediction horizon H
• Number of simulated transitions N
1: Dsim ←∅: Initialize empty buffer for simulated transitions
2: while |Dsim| < N do
3:
b ∼U{1, B}: Sample random reference trajectory
4:
t ∼U{0, T −H}: Sample random starting state
5:
h ←0, st ←ˆsb
t: Initialize starting state
6:
while h < H do
7:
at+h ∼πθ(a | st+h): Sample action from policy
8:
st+h+1 ←ˆst+h+1 + ˜f(st+h, at+h) −˜f(ˆsb
t+h, ˆab
t+h): Do one-step prediction with OPC
9:
Dsim ←Dsim ∪(st+h, at+h, st+h+1): Store new transition tuple
10:
if (st+h+1 is terminal) or (ˆsb
t+h+1 is terminal) then
11:
break
12:
h ←h + 1: Increase counter
return Dsim"
REFERENCES,0.27390791027154665,Published as a conference paper at ICLR 2022
REFERENCES,0.27508854781582054,"A.2
HYPERPARAMETER SETTINGS"
REFERENCES,0.2762691853600944,"Below, we list the most important hyperparameter settings that were used to generate the results in
the main paper."
REFERENCES,0.27744982290436837,"Table 1: Hyperparameter settings for OPC (blue) and MBPO(⋆) (red) for results shown in Fig. 3.
Note that the respective hyperparameters for each environment are shared across the different
implementations, i.e., MuJoCo and PyBullet."
REFERENCES,0.27863046044864226,"HalfCheetah
Hopper
Walker2D
AntTruncatedObs"
REFERENCES,0.2798110979929162,"epochs
200
150
300
300"
REFERENCES,0.2809917355371901,"env steps per
epoch 1000"
REFERENCES,0.282172373081464,"retain epochs
50 / 5
50
50
5"
REFERENCES,0.2833530106257379,"policy updates
per epoch
40
20
20
20"
REFERENCES,0.2845336481700118,"model horizon
10"
REFERENCES,0.2857142857142857,"model rollouts
per epoch
100’000"
REFERENCES,0.28689492325855964,"mix-in ratio
0.0"
REFERENCES,0.28807556080283353,"model network
ensemble of 7 with 5 elites"
REFERENCES,0.2892561983471074,"policy network
MLP with 2 hidden layers of size 64"
REFERENCES,0.29043683589138136,"A.3
IMPLEMENTATION AND COMPUTATIONAL RESOURCES"
REFERENCES,0.29161747343565525,"Our implementation is based on the code from MBPO (Janner et al., 2019), which is open-sourced
under the MIT license. All experiments were run on an HPC cluster, where each individual experiment
used one Nvidia V100 GPU and four Intel Xeon CPUs. All experiments (including early debugging
and evaluations) amounted to a total of 84‘713 hours, which corresponds to roughly 9.7 years if
the jobs ran sequentially. Most of this compute was required to ensure reproducibility (ten random
seeds per job and ablation studies over the effects of parameters). The Bosch Group is carbon-neutral.
Administration, manufacturing and research activities do no longer leave a carbon footprint. This
also includes GPU clusters on which the experiments have been performed."
REFERENCES,0.29279811097992914,Published as a conference paper at ICLR 2022
REFERENCES,0.2939787485242031,"B
THEORETICAL ANALYSIS OF ON-POLICY CORRECTIONS"
REFERENCES,0.29515938606847697,"In this section, we analyze OPC from a theoretical perspective and how it affects policy improvement."
REFERENCES,0.29634002361275086,"Notation
In the following, we drop the n superscript for states and actions for ease of exposition.
That is, ˆsn,b = ˆsb and ˆan,b = ˆab."
REFERENCES,0.2975206611570248,"B.1
GENERAL POLICY IMPROVEMENT BOUND"
REFERENCES,0.2987012987012987,"We begin by deriving inequality Eq. (4), which serves as motivation for OPC and is the foundation for
the theoretical analysis. Our goal is to bound the difference in expected return for the policies before
and after the policy optimization step, i.e., ηn+1 −ηn. Since we are considering the MBRL setting, it
is natural to express the improvement bound in terms of the expected return under the model ˜η and
thus obtain the following"
REFERENCES,0.29988193624557263,ηn+1 −ηn = ηn+1 −ηn + ˜ηn+1 −˜ηn+1 + ˜ηn −˜ηn
REFERENCES,0.3010625737898465,"= ˜ηn+1 −˜ηn + ηn+1 −˜ηn+1 + ˜ηn −ηn
ηn+1 −ηn
|
{z
}
True policy
improvement"
REFERENCES,0.3022432113341204,"≥˜ηn+1 −˜ηn
|
{z
}
Model policy
improvement"
REFERENCES,0.30342384887839435,"−|ηn+1 −˜ηn+1|
|
{z
}
Off-policy
model error"
REFERENCES,0.30460448642266824,"−|˜ηn −ηn|
|
{z
}
On-policy
model error ."
REFERENCES,0.30578512396694213,"According to this bound, the improvement of the policy under the true environment is governed by
the three terms on the LHS:"
REFERENCES,0.3069657615112161,"• Model policy improvement: This term is what we are directly optimizing in MBRL offset
by the return of the previous iteration ˜ηn, which is constant given the current policy πn.
Assuming that we are not at an optimum, standard policy optimization algorithms guarantee
that this term is non-negative.
• Off-policy model error: The last term is the difference in return for the true environment
and model under the improved policy πn+1. This depends largely on the generalization
properties of our model, since it is not trained on data under πn+1.
• On-policy model error: This term compares the on-policy return under πn between the true
environment and the model and it is zero for any model ˜p = p. Since we have access to
transitions from the true environment under the πn, the replay buffer Eq. (3) fulﬁlls this
condition under certain circumstances and the on-policy model error vanishes, see Lemma 2."
REFERENCES,0.30814639905548996,"Note that the learned model Eq. (2) is able to generalize to unseen state-action pairs better than the
replay buffer Eq. (3) and accordingly will achieve a lower off-policy model error. The motivation
behind OPC is therefore to combine the best of the learned model and the replay buffer to reduce both
on- and off-policy model errors."
REFERENCES,0.30932703659976385,"B.2
PROPERTIES OF THE REPLAY BUFFER"
REFERENCES,0.3105076741440378,"The beneﬁt of the replay buffer Eq. (3) is that it can never introduce any model-bias such that any
trajectory sampled from this model is guaranteed to come from the true state distribution. Accordingly,
if we have collected sufﬁcient data under the same policy, the on-policy model error vanishes.
Lemma 2. Let M be the true MDP with (stochastic) dynamics p, bounded reward r < rmax and
let ˜pdata be the transition model for the replay buffer Eq. (3). Further, consider a set of trajectories
D = SB
b=1{(ˆsb
t, ˆab
t,ˆsb
t+1)}T −1
t=0 collected from M under policy π. If we collect more and more
on-policy training data under the same policy, then"
REFERENCES,0.3116883116883117,"lim
B→∞Pr
 
|η −˜ηreplay| > ε

= 0
∀ε > 0"
REFERENCES,0.31286894923258557,where ˜ηreplay is the expected return under the replay buffer using the collected trajectories D.
REFERENCES,0.3140495867768595,"Proof. First, note that the corresponding expected return for the replay-buffer model is given by"
REFERENCES,0.3152302243211334,"˜ηreplay = 1 B B
X b=1"
REFERENCES,0.31641086186540734,"T −1
X"
REFERENCES,0.31759149940968123,"t=0
r(ˆsb
t, ˆab
t),"
REFERENCES,0.3187721369539551,Published as a conference paper at ICLR 2022
REFERENCES,0.31995277449822906,"which is a sample-based approximation of the true reward η. By the weak law of large numbers (see
e.g., Blitzstein & Hwang (2019, Theorem 10.2.2)), the Lemma then holds."
REFERENCES,0.32113341204250295,"B.3
PROPERTIES OF THE LEARNED MODEL"
REFERENCES,0.32231404958677684,"Following Janner et al. (2019, Lemma B.3), a general bound on the performance gap between two
MDPs with different dynamics can be given by"
REFERENCES,0.3234946871310508,"|η1 −η2| ≤2rmaxϵm H
X"
REFERENCES,0.3246753246753247,"t=1
tγt.
(10)"
REFERENCES,0.32585596221959856,"where ϵm ≥maxt Es∼pt
1(s) KL(p1(s′, a)||p2(s′, a)) bounds the mismatch between the respective
transition models. Now, the ﬁnal form of Eq. (10) depends on the horizon length H. For H →∞, we
obtain the original result form Janner et al. (2019) with P"
REFERENCES,0.3270365997638725,"t≥1 tγt = γ/(1−γ)2. For the ﬁnite horizon
case one can obtain tigther bounds when H is smaller than the effective horizon, H < γ/(1 −γ),
encoded in the discount factor: H
X"
REFERENCES,0.3282172373081464,"t=1
tγt ≤min
nH(H + 1)"
REFERENCES,0.3293978748524203,"2
,
H
1 −γ ,
γ
(1 −γ)2"
REFERENCES,0.3305785123966942,"o
,
(11)"
REFERENCES,0.3317591499409681,"which can be veriﬁed by upper bounding t ≤H to obtain H/(1 −γ) or by bounding γ ≤1 to obtain
O(H2)."
REFERENCES,0.33293978748524206,"Note that this bound is vacuous for deterministic policies, since the KL divergence between two
distributions with non-overlapping support is inﬁnite. In the following we focus on the Wasserstein
metric under the Euclidean distance."
REFERENCES,0.33412042502951594,"B.4
PROPERTIES OF OPC"
REFERENCES,0.33530106257378983,"In this section, we analyze the properties of OPC relative to the true, unknown environment’s transition
distribution p and a learned representation ˜p. In general, the OPC-model mixes observed transitions
from the environment with the learned model. The resulting transitions are then a combination of the
mean transitions from the learned model, the aleatoric noise from the data (the environment), and the
mean-error between our learned model and the environment."
REFERENCES,0.3364817001180638,"B.4.1
PROOF OF LEMMA 1"
REFERENCES,0.33766233766233766,"In this section, we prove Lemma 1 by showing that the OPC-model coincides with the replay-buffer
Eq. (3) in the case of a deterministic policy and thus lead to the same expected return ˜η."
REFERENCES,0.33884297520661155,"Lemma 3. Let M be the true MDP with (stochastic) dynamics p and let ˜
M be a MDP with the same
reward function r and initial state distribution ρ0, but different dynamics ˜pmodel, respectively. Further,
assume a deterministic policy π : S 7→A and a set of trajectories D = SB
b=1{(ˆsb
t, ˆab
t,ˆsb
t+1)}T −1
t=0
collected from M under π. If we extend the approximate dynamics ˜pmodel by OPC with data D, then"
REFERENCES,0.3400236127508855,"˜ηreplay = ηopc,"
REFERENCES,0.3412042502951594,"where ˜ηreplay and ˜ηopc are the model-based returns following models Eqs. (3) and (5), respectively."
REFERENCES,0.34238488783943327,"Proof. For the proof, it sufﬁces to show that the resulting state distributions of the two transition
models ˜pdata and ˜popc under the deterministic policy π are the same for all b with 1 ≤b ≤B. We
show this by induction:"
REFERENCES,0.3435655253837072,Published as a conference paper at ICLR 2022
REFERENCES,0.3447461629279811,Theorem 1
REFERENCES,0.345926800472255,Lemma 5
REFERENCES,0.34710743801652894,Lemma 8
REFERENCES,0.3482880755608028,Lemma 7
REFERENCES,0.34946871310507677,Lemma 10
REFERENCES,0.35064935064935066,Lemma 6
REFERENCES,0.35182998819362454,"Lemma 4
Lemma 9
Lemma 11"
REFERENCES,0.3530106257378985,Corollary 1
REFERENCES,0.3541912632821724,"Theorem 2
Lemma 12"
REFERENCES,0.35537190082644626,Lemma 13
REFERENCES,0.3565525383707202,Figure 6: Overview about the supporting Lemmas for the proof of Theorem 1.
REFERENCES,0.3577331759149941,"For t = 0 the states are sampled from the initial state distribution, thus we have s0 = ˆsb
0 by deﬁnition.
Assume that st = ˆsb
t as an induction hypothesis. Then"
REFERENCES,0.358913813459268,"˜popc(st+1 | st, π(st), b) = δ

st+1 −ˆsb
t+1

∗δ

st+1 −
h
f(st, π(st)) −f(ˆsb
t, ˆab
t)
i"
REFERENCES,0.3600944510035419,"= δ

st+1 −
h
ˆsb
t+1 + f(st, π(st)) −f(ˆsb
t, ˆab
t)
i"
REFERENCES,0.3612750885478158,"= δ

st+1 −
h
ˆsb
t+1 + f(ˆsb
t, ˆab
t) −f(ˆsb
t, ˆab
t)
i"
REFERENCES,0.3624557260920897,"= δ

st+1 −ˆsb
t+1
"
REFERENCES,0.36363636363636365,"= ˜pdata(st+1 | t + 1, b)"
REFERENCES,0.36481700118063753,"where the second step follows by the induction hypothesis and due to the deterministic policy. Thus,
for any index b we have τ opc
b
= τb and the result follows."
REFERENCES,0.3659976387249115,"Now, combining Lemmas 2 and 3 proofs the result in Lemma 1."
REFERENCES,0.36717827626918537,"B.4.2
PROOF OF THEOREM 1"
REFERENCES,0.36835891381345925,In this section we prove our main result. An overview of the lemma dependencies is shown in Fig. 6.
REFERENCES,0.3695395513577332,"Remark on Notation
In the main paper, we unify the notation for state sequence probabilities of
the different models Eqs. (2), (3) and (5) as ˜px(τt:t+H | t, b) with x ∈{replay, model, opc}. This
allows for a consistent description of the respective rollouts independent of the model being a learned
representation, a replay buffer or the OPC-model. For that notation, the index b denotes the sampled
trajectory from the collected data on the real environment. Implicitly, we therefore condition the state
sequence probability on the observed transition tuples [(ˆsb
t+1,ˆsb
t, ˆab
t), . . . , (ˆsb
t+H,ˆsb
t+H−1, ˆab
t+H−1)],
i.e.,"
REFERENCES,0.3707201889020071,"˜px(τt:t+H | t, b) = ˜px(τt:t+H | (ˆsb
t+1,ˆsb
t, ˆab
t), . . . , (ˆsb
t+H,ˆsb
t+H−1, ˆab
t+H−1)),
(12)"
REFERENCES,0.371900826446281,"where we omit the explicit conditioning for the sake of brevity in the main paper. Similarly, we can
write the one-step model Eq. (5) for OPC in an explicit form as"
REFERENCES,0.3730814639905549,"˜popc(st+1|st, at, b) = ˜popc(st+1|st, at,ˆsb
t+1,ˆsb
t, ˆab
t).
(13)"
REFERENCES,0.3742621015348288,"Note that with the explicit notation, the relation between the OPC-model Eq. (5) and generalized OPC-
model Eq. (6) becomes clear:"
REFERENCES,0.3754427390791027,"˜popc
⋆
(st+1 | st, at, b) = ˜popc(st+1 | st, at,ˆsb
t, ˆab
t) =
Z
˜popc(st+1 | st, at,ˆsb
t+1ˆsb
t, ˆab
t) dˆsb
t+1. (14)"
REFERENCES,0.37662337662337664,Published as a conference paper at ICLR 2022
REFERENCES,0.3778040141676505,"For the following proofs, we stay with the explicit notation for sake of clarity and instead omit the
conditioning on b."
REFERENCES,0.3789846517119244,"Generalized OPC-Model
In this section, we have a closer look at the generalized OPC-model
Eq. (6). The main difference between Eqs. (5) and (6) is that the former is in fact transitioning
deterministically (the stochasticity arises from the environment’s aleatoric uncertainty which manifests
itself in the observed transitions). The two models can be related via marginalization of ˆsb
t+1, see
Eq. (14). The resulting generalized OPC-model can then be related to the true transition distribution
according to the following Lemma."
REFERENCES,0.38016528925619836,"Lemma 4. For all ˆsb
t, ˆab
t ∈S × A with ˆsb
t+1 ∼p(ˆst+1 | ˆsb
t, ˆab
t) it holds that"
REFERENCES,0.38134592680047225,"˜popc(st+1 | st, at,ˆsb
t, ˆab
t) = p

st+1 −
 ˜f(st, at) −˜f(ˆsb
t, ˆab
t)
"
REFERENCES,0.3825265643447462,"|
{z
}
Mean correction"
REFERENCES,0.3837072018890201,"| st, at,ˆsb
t, ˆab
t

,
(15)"
REFERENCES,0.38488783943329397,"where ˜f(st, at) = E[˜pmodel(st+1 | st, at)] is the mean transition of the learned model Eq. (2) and
˜popc(st+1 | st, at,ˆsb
t, ˆab
t) denotes the OPC-model if we marginalize over the distribution for ˆsb
t+1
instead of using its observed value."
REFERENCES,0.3860684769775679,"Proof. Using the explicit notation Eq. (13), the OPC-model from Eq. (5) is deﬁned as"
REFERENCES,0.3872491145218418,"˜popc(st+1 | st, at, b) = ˜popc(st+1 | st, at,ˆsb
t, ˆab
t,ˆsb
t+1)
(16)"
REFERENCES,0.3884297520661157,"= δ

st+1 −ˆsb
t+1

∗δ

st+1 −
h
˜f(st, at) −˜f(ˆsb
t, ˆab
t)
i 
,
(17)"
REFERENCES,0.38961038961038963,"= δ

st+1 −
h
ˆsb
t+1 + ˜f(st, at) −˜f(ˆsb
t, ˆab
t)
i 
.
(18)"
REFERENCES,0.3907910271546635,"With ˆsb
t+1 ∼p(ˆst+1 | ˆsb
t, ˆab
t), marginalizing ˆst+1 yields (note that we use ˆst+1 instead of st+1 to
denote the random variable for the next state in order to distinguish it from the random variable for
˜popc(st+1 | st, at, b) under the integral)"
REFERENCES,0.3919716646989374,"˜popc(st+1 | st, at,ˆsb
t, ˆab
t) =
Z
δ

st+1 −
h
ˆst+1 + ˜f(st, at) −˜f(ˆsb
t, ˆab
t)
i 
p(ˆst+1 | ˆsb
t, ˆab
t) dˆst+1,"
REFERENCES,0.39315230224321135,"= p

st+1 −
 ˜f(st, at) −˜f(ˆsb
t, ˆab
t)

| ˆsb
t, ˆab
t

."
REFERENCES,0.39433293978748524,"Remark 1. An alternative way of writing the general OPC-model is the following,"
REFERENCES,0.3955135773317591,"˜popc(st+1 | st, at,ˆsb
t, ˆab
t) = p(ˆst+1 | ˆsb
t, ˆab
t)
|
{z
}
On-policy transition"
REFERENCES,0.39669421487603307,"∗δ

st+1 −
h
˜f(st, at) −˜f(ˆsb
t, ˆab
t)
i"
REFERENCES,0.39787485242030696,"|
{z
}
Mean correction term"
REFERENCES,0.3990554899645809,",
(19)"
REFERENCES,0.4002361275088548,"which highlights that the ˜popc transitions according to the true on-policy dynamics conditioned on
data from the replay buffer, combined with a correction term. We can further explicitly see why
an implementation of this model wouldn’t be possible due to its dependency on the true transition
probabilities. Thus, in practice, we’re limited to the sample-based approximation shown in the paper."
REFERENCES,0.4014167650531287,"The fundamental idea for the proof of Theorem 1 lies in the following Lemma 5, which is the
foundation for bounding the on-policy error. The Wasserstein distance naturally arises in bounding
this type of error model as it depends on the expected return under two different distributions. The
ﬁnal result is then summarized in Theorem 1."
REFERENCES,0.4025974025974026,"Lemma 5. Let ˜popc be the generalized OPC-model (cf. Lemma 4) and ˜ηopc be its corresponding
expected return. Assume that the return r(st, at) is Lr-Lipschitz and the policy π(at | st) is
Lπ-Lipschitz with respect to st under the Wasserstein distance, then"
REFERENCES,0.4037780401416765,"|η −˜ηopc| ≤Lr
p"
REFERENCES,0.4049586776859504,"1 + L2π
X"
REFERENCES,0.40613931523022434,"t≥0
γtW2(p(st), ˜popc(st))
(20)"
REFERENCES,0.40731995277449823,Published as a conference paper at ICLR 2022
REFERENCES,0.4085005903187721,Proof.
REFERENCES,0.40968122786304606,"|η −˜ηopc| =
 E
τ∼p h X"
REFERENCES,0.41086186540731995,"t≥0
γtr(ˆst, ˆat)
i
−
E
τ∼˜popc h X"
REFERENCES,0.41204250295159384,"t≥0
γtr(st, at)
i
(21)"
REFERENCES,0.4132231404958678,"=

X"
REFERENCES,0.41440377804014167,"t≥0
γt
E
ˆst,ˆat∼p(ˆst,ˆat)"
REFERENCES,0.4155844155844156,"
r(ˆst, ˆat)

−
E
st,at∼˜popc(st,at)"
REFERENCES,0.4167650531286895,"
r(st, at)

(22) ≤
X"
REFERENCES,0.4179456906729634,"t≥0
γt
E
ˆst,ˆat∼p(ˆst,ˆat)"
REFERENCES,0.41912632821723733,"
r(ˆst, ˆat)

−
E
st,at∼˜popc(st,at)"
REFERENCES,0.4203069657615112,"
r(st, at)

(23)"
REFERENCES,0.4214876033057851,"Applying Lemma 8: ≤
X"
REFERENCES,0.42266824085005905,"t≥0
γtLrW2(p(ˆst, ˆat), ˜popc(st, at))
(24)"
REFERENCES,0.42384887839433294,"Writing the joint distributions for state/action in terms of their conditional (i.e., policy) and marginal
distributions p(st, at) = π(at | st)p(st): =
X"
REFERENCES,0.42502951593860683,"t≥0
γtLrW2(π(ˆat | ˆst)p(ˆst), π(at | st)˜popc(st))
(25)"
REFERENCES,0.42621015348288077,"Under the assumption that the policies π are Lπ-Lipschitz under the Wasserstein distance, application
of Lemma 10 concludes the proof: ≤
X"
REFERENCES,0.42739079102715466,"t≥0
γtLr
p"
REFERENCES,0.42857142857142855,"1 + L2πW2(p(ˆst), ˜popc(st)).
(26)"
REFERENCES,0.4297520661157025,"Lemma 6 (Wasserstein Distance between Marginal State Distributions). Let ˜popc(st) and p(ˆst) be
the marginal state distributions at time t when rolling out from the same initial state ˆs0 under the
same policy with the OPC-model and the true environment, respectively. Assume that the underlying
learned dynamics model is Lf-Lipschitz continuous with respect to both its arguments and the policy
π(a | s) is Lπ-Lipschitz with respect to s under the Wasserstein distance. If it further holds that
the policy’s (co-)variance Var[π(a | s)] = Σπ(s) ∈SdA
+ is ﬁnite over the complete state space, i.e.,
maxs∈S trace{Σπ(s)} ≤¯σ2
π, then the discrepancy between the marginal state distributions of the
two models is bounded"
REFERENCES,0.4309327036599764,"W2
2(˜popc(st), p(ˆst)) ≤2
p"
REFERENCES,0.43211334120425027,"dA¯σ2
πL2
f t−1
X"
REFERENCES,0.4332939787485242,"t′=0
(L2
f + L2
π)t′
(27)"
REFERENCES,0.4344746162927981,Proof. We proof the Lemma by induction.
REFERENCES,0.43565525383707204,"Base Case: t = 1
For the base case, we need to show that starting from the same initial state ˆs0
the following condition holds:"
REFERENCES,0.43683589138134593,"W2
2(˜popc(s1), p(ˆs1)) ≤2
p"
REFERENCES,0.4380165289256198,"dA¯σ2
πL2
f"
REFERENCES,0.43919716646989376,"For ease of readability, we deﬁne z = (s, a) and use the notation dp(x, y) = p(x, y) dx dy whenever
no explicit assumptions are made about the distributions."
REFERENCES,0.44037780401416765,"W2(˜popc(s1), p(ˆs1))
(28) = W2"
REFERENCES,0.44155844155844154,"ZZ
˜popc(s1 | ˆz0, z0) dp(ˆz0, z0),
Z
p(ˆs1 | ˆz0) dp(ˆz0)

(29)"
REFERENCES,0.4427390791027155,"Recall that we can write both ˜popc and p as convolution between pϵ and a Dirac delta (see Lemma 4).
Together with the identity Eq. (54), the Wasserstein distance for sums of random variables Eq. (53)
and noting Wp(pϵ(ϵ), pϵ(ϵ)) = 0: ≤W2"
REFERENCES,0.44391971664698937,"ZZ
δ(s1 −[f(ˆz0) + ˜f(z0) −˜f(ˆz0)]) dp(ˆz0, z0),
Z
δ(ˆs1 −f(ˆz0)) dp(ˆz0)

(30)"
REFERENCES,0.44510035419126326,Published as a conference paper at ICLR 2022
REFERENCES,0.4462809917355372,"Squaring and using Lemma 9: W2
2"
REFERENCES,0.4474616292798111,"ZZ
δ(s1 −[f(ˆz0) + ˜f(z0) −˜f(ˆz0)]) dp(ˆz0)p(z0),
Z
δ(ˆs1 −f(ˆz0)) dp(ˆz0)
"
REFERENCES,0.448642266824085,"≤
ZZ
∥f(ˆz0) + ˜f(z0) −˜f(ˆz0) −f(ˆz0)∥2 dp(ˆz0, z0)
(31)"
REFERENCES,0.4498229043683589,"=
ZZ
∥˜f(z0) −˜f(ˆz0)∥2 dp(ˆz0, z0)
(32) ≤L2
f"
REFERENCES,0.4510035419126328,"Z
∥s0 −ˆs0∥2 + ∥a0 −ˆa0∥2 dp(ˆs0, ˆa0, s0, a0)
(33)"
REFERENCES,0.45218417945690675,"We are assuming that the initial states of the trajectory rollouts coincide. The joint state/action distri-
bution can then be written as p(ˆs0, ˆa0, s0, a0) = p(ˆs0)π(ˆa0 | ˆs0)δ(s0 −ˆs0)π(a0 | s0). Integrating
with respect to s0 leads to:"
REFERENCES,0.45336481700118064,"= L2
f"
REFERENCES,0.45454545454545453,"Z
∥a0 −ˆa0∥2p(ˆs0)π(ˆa0 | ˆs0)π(a0 | ˆs0) dˆs0 dˆa0 da0
(34)"
REFERENCES,0.4557260920897285,"This term describes the mean squared distance between two random actions. Since we condition π on
the same state ˆs0, the policy distributions coincide. Deﬁne ∆a = a0 −ˆa0,"
REFERENCES,0.45690672963400236,"= L2
f E
ˆs0"
REFERENCES,0.45808736717827625,"
E
∆a[∥∆a∥2]

(35)"
REFERENCES,0.4592680047225502,"= L2
f E
ˆs0"
REFERENCES,0.4604486422668241,"
trace{Var[∆a]

}
(36)"
REFERENCES,0.46162927981109797,Now Var[∆a] = Var[π(ˆa0 | ˆs0)] + Var[π(a0 | ˆs0)] = 2 Var[π(a0 | ˆs0)]
REFERENCES,0.4628099173553719,"= 2L2
f E
ˆs0"
REFERENCES,0.4639905548996458,"
trace{Var[π(a0 | ˆs0)]}

(37)"
REFERENCES,0.4651711924439197,"This term is in fact less than Eq. (27) for t = 0, thus prooﬁng the base case. ≤2
p"
REFERENCES,0.46635182998819363,"dA¯σ2
πL2
f
(38)"
REFERENCES,0.4675324675324675,"Inductive Step
We will show that if the hypothesis holds for t then it holds for t + 1 as well.
We explicitly write the following intermediate bound such that its application in the proof is more
apparent, i.e.,"
REFERENCES,0.46871310507674147,"W2
2(p(ˆst), ˜popc(st)) ≤
Z
∥˜f(zt−1) −˜f(ˆzt−1)∥2 dp(ˆzt−1, zt−1)
(39) ≤2
p"
REFERENCES,0.46989374262101535,"dA¯σ2
πL2
f t−1
X"
REFERENCES,0.47107438016528924,"t′=0
(L2
f + L2
π)t′,
(40)"
REFERENCES,0.4722550177095632,"where the ﬁrst inequality immediately follows from the same reasoning as in the base case Eq. (28)–
Eq. (32)."
REFERENCES,0.4734356552538371,"W2
2(p(ˆst+1, p(st+1))
(41)"
REFERENCES,0.47461629279811096,"≤
Z
∥˜f(zt) −˜f(ˆzt)∥2 dp(ˆzt, zt)
(42) ≤L2
f"
REFERENCES,0.4757969303423849,"Z
∥st −ˆst∥2 dp(ˆzt, zt) + L2
f"
REFERENCES,0.4769775678866588,"Z
∥at −ˆat∥2 dp(ˆzt, zt)
(43)"
REFERENCES,0.4781582054309327,Applying Lemma 11 to the second integral
REFERENCES,0.4793388429752066,"≤(L2
f + L2
π)
Z
∥st −ˆst∥2 dp(ˆzt, zt) + 2
p"
REFERENCES,0.4805194805194805,"dAL2
f ¯σ2
π
(44)"
REFERENCES,0.4817001180637544,"We predict along a consistent trajectory, i.e., st = ˆst + ˜f(st−1, at−1) −˜f(ˆst−1, ˆat−1)"
REFERENCES,0.48288075560802834,"≤(L2
f + L2
π)
Z
∥˜f(zt−1) −˜f(ˆzt−1)∥2 dp(ˆzt−1, zt−1) + 2
p"
REFERENCES,0.48406139315230223,"dAL2
f ¯σ2
π
(45)"
REFERENCES,0.4852420306965762,Published as a conference paper at ICLR 2022
REFERENCES,0.48642266824085006,Assume that the hypothesis Eq. (39) holds for t
REFERENCES,0.48760330578512395,"≤(L2
f + L2
π) × 2
p"
REFERENCES,0.4887839433293979,"dA¯σ2
πL2
f t−1
X"
REFERENCES,0.4899645808736718,"t′=0
(L2
f + L2
π)t′ + 2
p"
REFERENCES,0.4911452184179457,"dAL2
f ¯σ2
π
(46) = 2
p"
REFERENCES,0.4923258559622196,"dA¯σ2
πL2
f

1 + t−1
X"
REFERENCES,0.4935064935064935,"t′=0
(L2
f + L2
π)t′
(47) = 2
p"
REFERENCES,0.4946871310507674,"dA¯σ2
πL2
f t
X"
REFERENCES,0.49586776859504134,"t′=0
(L2
f + L2
π)t′
(48)"
REFERENCES,0.4970484061393152,"Theorem 1. Let ˜ηopc
⋆
and η be the expected return under the generalized OPC-model Eq. (6) and
the true environment, respectively. Assume that the learned model’s mean transition function
˜f(st, at) = E[˜pmodel(st+1 | st, at)] is Lf-Lipschitz and the reward r(st, at) is Lr-Lipschitz. Further,
if the policy π(at | st) is Lπ-Lipschitz with respect to st under the Wasserstein distance and
its (co-)variance Var[π(at | st)] = Σπ(st) ∈SdA
+
is ﬁnite over the complete state space, i.e.,"
REFERENCES,0.4982290436835891,"maxst∈S trace{Σπ(st)} ≤¯σ2
π, then with C1 =
p"
REFERENCES,0.49940968122786306,"2(1 + L2π)LfLr and C2 =
q"
REFERENCES,0.500590318772137,"L2
f + L2π"
REFERENCES,0.5017709563164109,"|η −˜ηopc
⋆
| ≤
¯σπ
1 −γ dA"
REFERENCES,0.5029515938606848,"1
4 C1CT
2
√"
REFERENCES,0.5041322314049587,"T.
(7)"
REFERENCES,0.5053128689492326,Proof. From combining Lemmas 5 and 6 it follows that
REFERENCES,0.5064935064935064,"|η −˜ηopc
⋆
| ≤
q"
P,0.5076741440377804,"2
p"
P,0.5088547815820543,"dA(1 + L2π)¯σπLfLr
X"
P,0.5100354191263282,"t≥0
γt"
P,0.5112160566706021,"v
u
u
t t
X"
P,0.512396694214876,"t′=0
(L2
f + L2π)t′
(49)"
P,0.51357733175915,"with the shorthand notations C1 =
p"
P,0.5147579693034239,"2(1 + L2π)LfLr and C2 = L2
f + L2
π"
P,0.5159386068476978,"= C1|A|
1
4 ¯σπ T
X"
P,0.5171192443919717,"t=0
γt"
P,0.5182998819362455,"v
u
u
t t
X"
P,0.5194805194805194,"t′=0
Ct′
2
(50)"
P,0.5206611570247934,"Since t ≤T, we have that Pt
t′=0 Ct′
2 ≤TCT
2"
P,0.5218417945690673,"≤C1|A|
1
4 ¯σπC T 2
2
√ T T
X"
P,0.5230224321133412,"t=0
γt
(51)"
P,0.5242030696576151,"Since T ≤∞, we have with the geometric series PT
t=0 γt ≤1/(1 −γ)"
P,0.525383707201889,"≤
C1
1 −γ |A|
1
4 C T 2
2
√"
P,0.526564344746163,"T ¯σπ
(52)"
P,0.5277449822904369,"B.4.3
DEFINITIONS, HELPFUL IDENTITIES AND SUPPORTING LEMMAS"
P,0.5289256198347108,"Here we brieﬂy summarize some basic deﬁnitions and properties that will be used throughout the
following."
P,0.5301062573789846,"• The Wasserstein distance fulﬁlls the properties of a metric: Wp(p1, p3) ≤Wp(p1, p2) +
Wp(p2, p3).
• Wasserstein distance of sums of random variables (see, e.g., Mariucci & Reiß (2018,
Corollary 1) for a proof):"
P,0.5312868949232585,"Wp(p1 ∗· · · ∗pn, q1 ∗· · · ∗qn) ≤ n
X"
P,0.5324675324675324,"i=1
Wp(pi, qi)
(53)"
P,0.5336481700118064,Published as a conference paper at ICLR 2022
P,0.5348288075560803,"• For any function g(z, ˆz) we have
ZZ
p(st+1 −g(z, ˆz))ν(z, ˆz) dz dˆz = p ∗
ZZ
δ(st+1 −g(z, ˆz))ν(z, ˆz) dz dˆz
(54)"
P,0.5360094451003542,"• For any multivariate random variable z1 and z2 with probability distributions p(z1) =
p1(x)q(y) and p(z2) = p2(x)q(y), respectively, we have that (Panaretos & Zemel, 2019)"
P,0.5371900826446281,"W2
2(p1(x)q(y), p2(x)q(y)) = W2
2(p1(x), p2(x)).
(55)"
P,0.538370720188902,"Further, the following Lemmas are helpful for the proof of Theorem 1."
P,0.5395513577331759,"Lemma 7 (Kantorovich-Rubinstein (cf. Mariucci & Reiß (2018) Proposition 1.3)). Let X and
Y be integrable real random variables. Denote by µ and µ their laws [...]. Then the following
characterization of the Wasserstein distance of order 1 holds:"
P,0.5407319952774499,"W1(ν, µ) =
sup
∥φ∥Lip≤1
E
x∼ν(·)[φ(x)] −
E
y∼µ(·)[φ(y)],
(56)"
P,0.5419126328217237,"where the supremum is being taken over all φ satisfying the Lipschitz condition |φ(x)−φ(y)| ≤|x−y|,
for all x, y ∈R."
P,0.5430932703659976,Lemma 8. Let f be Lf-Lipschitz with respect to a metric d. Then
P,0.5442739079102715,"|
E
x∼ν(·)[f(x)] −
E
y∼µ(·)[f(y)]| ≤LfW1(ν, µ) ≤LfW2(ν, µ)
(57)"
P,0.5454545454545454,"Proof. The ﬁrst inequality is a direct consequence of Lemma 7 and the second inequality comes
from the well-known fact that if 1 ≤p ≤q, then Wp(µ, ν) ≤Wq(µ, ν) (cf. Mariucci & Reiß (2018,
Lemma 1.2))."
P,0.5466351829988194,"Lemma 9. For any two functions f(s) and g(s) and probability density p(s) that govern the distri-
butions deﬁned by"
P,0.5478158205430933,"p1(x1) =
Z
δ(x1 −f(s))p(s) ds
and
p2(x2) =
Z
δ(x2 −g(s))p(s) ds,
(58)"
P,0.5489964580873672,it holds for any q ≥1 that
P,0.5501770956316411,"Wq
q (p1, p2) ≤
Z
∥f(s) −g(s)∥qp(s) ds.
(59)"
P,0.551357733175915,Proof. We have
P,0.5525383707201889,"Wq
q (p1(x1), p2(x2)) =
inf
γ∈Γ(p1,p2)"
P,0.5537190082644629,"ZZ
∥ξ1 −ξ2∥qγ(ξ1, ξ2) dξ1 dξ2
(60)"
P,0.5548996458087367,"Enforcing the following structure on γ(ξ1, ξ2) reduces the space of possible distributions: γ(ξ1, ξ2) =
R
δ(ξ1 −f(s))δ(ξ2 −g(s))p(s) ds, so that γ(ξ1, ξ2) ∈Γ(p1, p2) and thus"
P,0.5560802833530106,"≤
Z
∥ξ1 −ξ2∥q
Z
δ(ξ1 −f(s))δ(ξ2 −g(s))p(s) ds dξ1 dξ2
(61)"
P,0.5572609208972845,Integrating over ξ1 and ξ2 yields
P,0.5584415584415584,"=
Z
∥f(s) −g(s)∥qp(s) ds
(62)"
P,0.5596221959858324,"Lemma 10. If the policy π(at | st) is Lπ-Lipschitz with respect to st under the Wasserstein distance,
then with p(ˆs, ˆa) = π(ˆa | ˆs)p(ˆs) and ˜p(s, a) = π(a | s)˜p(s),"
P,0.5608028335301063,"W2
2(p(ˆs, ˆa), ˜p(s, a)) ≤(1 + L2
π)W2
2(p(ˆs), ˜p(s))
(63)"
P,0.5619834710743802,Published as a conference paper at ICLR 2022
P,0.5631641086186541,Proof.
P,0.564344746162928,"W2
2(p(ˆs, ˆa), ˜p(s, a))
(64)"
P,0.5655253837072018,"=
inf
γ∈Γ(p(ˆs,ˆa),˜p(s,a))"
P,0.5667060212514758,"Z 
∥ˆa −a∥2 + ∥ˆs −s∥2
γ(ˆa, a,ˆs, s) dˆa da dˆs ds
(65)"
P,0.5678866587957497,"Enforcing the following structure on γ reduces the space of possible distributions: γ(ˆs, s, ˆa, a) =
γ(ˆs, s)γ(ˆa, a | ˆs, s) with γ(ˆs, s) ∈Γ(p(ˆs), ˜p(s)) and γ(ˆa, a|ˆs, s) ∈Γ(π(ˆa | ˆs), π(a | s))."
P,0.5690672963400236,"≤
inf
γ(ˆs,s)∈Γ(p(ˆs),˜p(s))
γ(ˆa,a|ˆs,s)∈Γ(π(ˆa|ˆs),π(a|s))"
P,0.5702479338842975,"Z n Z h
∥ˆa −a∥2γ(ˆa, a | ˆs, s) dˆa da
o
+ ∥ˆs −s∥2i
γ(ˆs, s) dˆs ds (66)"
P,0.5714285714285714,"Interchange inﬁmum and the integral: Rockafellar (1976, Theorem 3A)"
P,0.5726092089728453,"= inf
γ(ˆs,s)"
P,0.5737898465171193,"Z n
inf
γ(ˆa,a|ˆs,s)"
P,0.5749704840613932,"Z
∥ˆa −a∥2γ(ˆa, a | ˆs, s) dˆa da + ∥ˆs −s∥2o
γ(ˆs, s) dˆs ds
(67)"
P,0.5761511216056671,"=
inf
γ(ˆs,s)∈Γ(p(ˆs),˜p(s))"
P,0.577331759149941,"Z n
W2
2(π(ˆa | ˆs), π(a | s)) + ∥ˆs −s∥2o
γ(ˆs, s) dˆs ds
(68)"
P,0.5785123966942148,"Using the assumption that the action distribution is Lπ-Lipschitz continuous under the Wasserstein
metric with respect to the state s:"
P,0.5796930342384888,"≤
inf
γ(ˆs,s)∈Γ(p(ˆs),˜p(s))"
P,0.5808736717827627,"Z
(1 + L2
π)∥ˆs −s∥2]γ(ˆs, s) dˆs ds
(69)"
P,0.5820543093270366,"= (1 + L2
π)W2
2(p(ˆs), ˜p(s))
(70)"
P,0.5832349468713105,"Lemma 11 (Average Squared Euclidean Distance Between Actions). If the policy π(a | s) is Lπ-
Lipschitz with respect to s under the Wasserstein distance and the policy’s (co-)variance Var[π(a |
s)] = Σπ(s) ∈SdA
+ is ﬁnite over the complete state space, i.e., maxs∈S trace{Σπ(s)} ≤¯σ2
π, then"
P,0.5844155844155844,"E
ˆat∼π(ˆst)
at∼π(st)"
P,0.5855962219598583,"
∥ˆat −at∥2
≤L2
π∥ˆst −st∥2 + 2
p"
P,0.5867768595041323,"dA¯σ2
π
(71)"
P,0.5879574970484062,Proof. Straightforward application of Corollary 1 and Lemma 13
P,0.58913813459268,"Lemma 12 (Average Squared Euclidean Distance). Consider two random variables x, y with
distributions px, py, mean vectors µx, µy ∈Rm and covariance matrices Σx, Σy ∈Sm
+, respectively.
Then the average squared Euclidean distance between the two is"
P,0.5903187721369539,"E
x,y

∥x −y∥2
= ∥µx −µy∥2 + trace

Σx + Σy
	
.
(72)"
P,0.5914994096812278,Proof. Deﬁne z = x −y with mean µz = µx −µy and variance Σz = Σx + Σy.
P,0.5926800472255017,"E

∥z∥2
= E
 X"
P,0.5938606847697757,"i
z2
i
 =
X"
P,0.5950413223140496,"i
E

z2
i
 =
X"
P,0.5962219598583235,"i
E[zi]2 + Var[zi]"
P,0.5974025974025974,"= µ⊤
z µz + trace

Σz"
P,0.5985832349468713,"= ∥µx −µy∥2 + trace

Σx + Σy
	
."
P,0.5997638724911453,Published as a conference paper at ICLR 2022
P,0.6009445100354192,"Theorem 2 (Gelbrich Bound (from Kuhn et al. (2019))). If ∥· ∥is the Euclidean norm, and the
distributions px and py have mean vectors µx, µy ∈Rm and covariance matrices Σx, Σy ∈Sm
+,
respectively, then"
P,0.602125147579693,"W2(px, py) ≥
q"
P,0.6033057851239669,"∥µx −µy∥2 + trace

Σx + Σy −2
 
Σ1/2
x ΣyΣ1/2
x
1/2	
.
(73)"
P,0.6044864226682408,The bound is exact if px and py are elliptical distributions with the same density generator.
P,0.6056670602125147,"Corollary 1. Consider the same setting as in Lemma 12, then the average squared Euclidean distance
is bounded by"
P,0.6068476977567887,"E
x,y

∥x −y∥2
≤W2
2(px, py) + 2 trace
 
Σ1/2
x ΣyΣ1/2
x
1/2	
.
(74)"
P,0.6080283353010626,Proof. Straightforward application of the results from Lemma 12 and Theorem 2.
P,0.6092089728453365,"Lemma 13. If the policy’s (co-)variance Var[π(a | s)] = Σπ(s) ∈SdA
+ is ﬁnite over the complete
state space, i.e., maxs∈S trace{Σπ(s)} ≤¯σ2
π, then"
P,0.6103896103896104,"trace
 
Σπ(ˆs)1/2Σπ(s)Σπ(ˆs)1/21/2	
≤
p"
P,0.6115702479338843,"dA¯σ2
π
(75)"
P,0.6127508854781583,Proof.
P,0.6139315230224321,"trace
 
Σπ(ˆs)1/2Σπ(s)Σπ(ˆs)1/21/2	
(76)"
P,0.615112160566706,"The trace of a matrix is the same as the sum of its eigenvalues, and the square root of a ma-
trix has eigenvalues that are square root of its eigenvalues. From Jensen’s inequality we know"
P,0.6162927981109799,"that PdA
i=1
√λi ≤
q"
P,0.6174734356552538,"dA
PdA
i=1 λi and consequently it holds for a matrix M ∈RdA×dA that"
P,0.6186540731995277,"trace{M1/2} ≤
p"
P,0.6198347107438017,"dA trace{M} , so that ≤
q"
P,0.6210153482880756,"dA trace{Σπ(ˆs)1/2Σπ(s)Σπ(ˆs)1/2}
(77)"
P,0.6221959858323495,"The trace is invariant under cyclic permutation =
p"
P,0.6233766233766234,"dA trace{Σπ(ˆs)Σπ(s)}
(78)"
P,0.6245572609208973,"Since both matrices are positive semi-deﬁnite, it follows from the Cauchy-Schwartz inequality that ≤
p"
P,0.6257378984651711,"dA trace{Σπ(ˆs)} trace{Σπ(s)}
(79)"
P,0.6269185360094451,"By assumption, the covariance matrices’ traces are bounded ≤
p"
P,0.628099173553719,"dA¯σ2
π
(80)"
P,0.6292798110979929,"B.5
MODEL ERRORS IN OPC"
P,0.6304604486422668,"While Theorem 1 highlights that OPC counteracts the on-policy error in predicted performance, for
stochastic policies we use the Lipschitz continuity of the model to upper-bound errors. In this section,
we look at the impact of model errors in combination with OPC. Speciﬁcally we focus on the one-step
prediction case from a known initial state ˆs0. There, while for the model ˜pmodel without OPC the
prediction error only depends on the quality of the model, with OPC it is instead the minimum of the
model error and the policy variance. This is advantageous, since typical environments tend to have
more states than actions, so that the trace of the policy variance can be signiﬁcantly smaller than the
full-state model error.
Lemma 14. Under the assumptions of Theorem 1, starting from an initial state ˆs0 the following
condition holds:"
P,0.6316410861865407,"W2
2(˜popc
⋆
(s1), p(ˆs1)) ≤min "
P,0.6328217237308147,"O

trace{Var[π(· | ˆs0)]}

, O
Z
∥f(ˆs0, a) −˜f(ˆs0, a)∥2 dπ(a | ˆs0)
|
{z
}
One-step model error !"
P,0.6340023612750886,Published as a conference paper at ICLR 2022
P,0.6351829988193625,"Proof. The ﬁrst term in the minimum follows directly from the base-case of Lemma 6. For the second
term, follow the same derivation, but note that under the distribution p(ˆs0, ˆa0, s0, a0) = p(ˆs0)π(ˆa0 |
ˆs0)δ(s0 −ˆs0)π(a0 | s0) we have
R
p(ˆs1 | ˆz0) dp(ˆz0) =
R
p(s1 | z0) dp(z0). Inserting this into the
r.h.s. of Eq. (28) and following the same steps we obtain"
P,0.6363636363636364,"W2
2(˜popc
⋆
(s1), p(ˆs1)) ≤W2
2"
P,0.6375442739079102,"ZZ
δ(s1 −[f(ˆz0) + ˜f(z0) −˜f(ˆz0)]) dp(ˆz0)p(z0),
Z
δ(s1 −f(z0)) dp(z0)

(81)"
P,0.6387249114521841,"≤
ZZ
∥f(ˆz0) + ˜f(z0) −˜f(ˆz0) −f(z0)∥2 dp(ˆz0, z0)
(82)"
P,0.6399055489964581,"=
ZZ
∥f(ˆz0) −˜f(ˆz0)∥2 + ∥f(z0) −˜f(z0)∥2 dp(ˆz0, z0)
(83)"
P,0.641086186540732,"= 2
Z
∥f(ˆz0) −˜f(ˆz0)∥2 dp(z0)
(84)"
P,0.6422668240850059,"= 2
Z
∥f(ˆs0, a) −˜f(ˆs0, a)∥2 dp(a | ˆs0)
(85)"
P,0.6434474616292798,"Note the additional factor of two in front of the upper bound on the model error, which comes from
using the model ‘twice’: once with ˆz and once with z. In practice we do not see any adverse effects
of this error, presumably because either the variance of the policy is sufﬁciently small, or due to the
upper bound being lose in practice."
P,0.6446280991735537,"C
MOTIVATING EXAMPLE – IN-DEPTH ANALYSIS"
P,0.6458087367178277,"In this section, we re-visit the motivating example presented in Section 4.1 of the main paper. For
completeness, we re-state all assumptions that lead to the simpliﬁed system at hand. We continue
with an analysis of the reward landscape and how OPC inﬂuences its shape. Next, we investigate how
an increasing mismatch of the dynamics model impacts the gradient error. In addition to the result
presented in the main paper, we here show the inﬂuence of different model errors, i.e., ∆A as well as
∆B. While OPC is motivated for the use case of on-policy RL algorithms, we further show that the
resulting gradients are robust with respect to differences in data-generating and evaluation policy,
i.e., the off-policy setting. Lastly, we state the the signed gradient distance that we use for evaluation
of the gradient errors, state the relevant theorem for determining the closed-loop stability of linear
systems, as well as all numerical values used for the motivating example."
P,0.6469893742621016,"C.1
SETUP"
P,0.6481700118063755,"Here, we assume a linear system with deterministic dynamics"
P,0.6493506493506493,"p(st+1 | st, at) = δ(Ast + Bat | st, at),
ρ0(s0) = δ(s0)
(86)"
P,0.6505312868949232,"with A, B ∈R and δ(·) denoting the Dirac-delta distribution. The linear policy and bell-shaped
reward are given by the following equations"
P,0.6517119244391971,"πθ(at | st) = δ(θst | st) with θ ∈R
and
r(st, at) = exp ("
P,0.6528925619834711,"−
 st σr 2)"
P,0.654073199527745,".
(87)"
P,0.6552538370720189,"Further, we assume to have access to an approximate dynamics model ˜p"
P,0.6564344746162928,"˜p(st+1 | st, at) = δ((A + ∆A)st + (B + ∆B)at | st, at),
(88)"
P,0.6576151121605667,"where ∆A, ∆B quantify the mismatch between the approximate model and the true system. For
completeness, the (deterministic) policy gradient is deﬁned as"
P,0.6587957497048406,"∇θ
1
T"
P,0.6599763872491146,"T −1
X"
P,0.6611570247933884,"t=0
r(st, at),
(89)"
P,0.6623376623376623,Published as a conference paper at ICLR 2022
P,0.6635182998819362,"True system
Model
Model + OPC
Reference policy πn"
P,0.6646989374262101,"Figure 7: Cumulative reward for different systems as a function of the policy parameter. The reference
trajectory that is used for OPC was generated by πn
θ (denoted by the black dashed line). The model
mismatch between the true system and the approximated model is ∆A = 0.5, ∆B = 0.0."
P,0.6658795749704841,"where the state/action pairs are obtained by simulating any of the two above models for T time-steps
and following policy πn
θ resulting in the trajectory ˜τ n = {(sn
t , an
t )}T −1
t=0 . Because both the model
and policy are deterministic, we can compute the analytical policy gradient from only one rollout."
P,0.667060212514758,"C.2
REWARD LANDSCAPES"
P,0.6682408500590319,"In a ﬁrst step, we will look at the cumulative rewards as a function of the policy parameter for the
different systems at hand: 1) the true system, 2) the approximate model without OPC and 3) the
approximate model with OPC. Further, let’s assume that the model mismatch is ﬁxed to some arbitrary
value. The resulting reward landscapes are shown in Fig. 7. We would like to emphasize several
key aspects in the plots: First, as one would expect the model mismatch leads to different optimal
policies as well as misleading policy gradients for large parts of the policy parameter space. Second,
the reward landscape for the model with OPC depends on the respective reference policy πn that
was used to generate the data for the corrections. Consequently, the correct reward is recovered at
θ = θn. More importantly, the OPC reshape the reward landscape such that the policy gradients point
towards the correct optimum (left plot). Lastly, even when using OPC the policy gradient’s sign is
not guaranteed to have the correct sign (right plot). The extent of this effect strongly depends on the
model mismatch, which we will investigate in the next section."
P,0.6694214876033058,"C.3
INFLUENCE OF MODEL ERROR"
P,0.6706021251475797,"As shown in the previous section, the estimated policy gradient depends on the current policy as well
as the mismatch between the true system and the approximate model. Fig. 2 depicts the (signed)
differences between the true policy gradient as well as the approximated gradient as a function of
model mismatch and the reference policy. Here, the opacity of the background denotes the magnitude
of the error and the color denotes if the true and estimated gradient have the same (blue) or oppposite
(red) sign. In the context of policy learning, the sign of the gradient is more relevant than the
actual magnitude due to internal re-scaling of the gradients in modern implementations of stochastic
optimizers such as Adam (Kingma & Ba, 2015). In our example, even for negligible model errors
(either in ∆A or ∆B), the model-based approach can lead to gradient estimates with the opposite
sign, indicated by the large red areas for the left ﬁgures in Fig. 2. On the other hand, applying OPC to
the model, we gradient estimates are signiﬁcantly more robust with respect to errors in the dynamics.x"
P,0.6717827626918536,Published as a conference paper at ICLR 2022
P,0.6729634002361276,(a) Gradient error when varying model error ∆A
P,0.6741440377804014,(b) Gradient error when varying model error ∆B
P,0.6753246753246753,"Figure 8: Signed gradient error (see Equation equation 90) when using the approximate model to
estimate the policy gradient without (left) and with (right) on-policy corrections (OPCs). Using OPCs
increases the robustness of the gradient estimate with respect to the model error."
P,0.6765053128689492,"C.4
INFLUENCE OF OFF-POLICY ERROR"
P,0.6776859504132231,"Until now we have considered the case in which the reference trajectory used for OPC is generated
with the same policy as the one used for gradient estimation, i.e., the on-policy setting. In this case,
we have observed that the true return could be recovered (see Fig. 7) when using OPC and that the
gradient estimates are less sensitive to model errors (see Fig. 8). The off-policy case corresponds
to the policy gains in Fig. 7 that are different from the reference policy πn indicated by the dashed
line. Fig. 9 summarizes the results for the off-policy setting. Here, we varied the policy error and the
reference policy itself for varying model errors. Note that for the correct model, we always recover
the true gradient. But also for inaccurate models, the gradient estimates retain a good quality in most
cases, with the exception for some model/policy combinations that are close to unstable."
P,0.6788665879574971,Published as a conference paper at ICLR 2022
P,0.680047225501771,(a) Model error ∆A.
P,0.6812278630460449,(b) Model error ∆B.
P,0.6824085005903188,"Figure 9: Signed gradient error due to off-policy data when using OPC. Note that we retain the true
gradient in case of no model error."
P,0.6835891381345927,"C.5
ADDITIONAL INFORMATION"
P,0.6847697756788665,"Next, we provide some additional information about how we compute gradient distances, properties
of linear systems, and exact numerical values used."
P,0.6859504132231405,"C.5.1
COMPUTING THE SIGNED GRADIENT DISTANCE g1
g2 ∆g"
P,0.6871310507674144,"Figure 10: Sketch depicting the signed gradient distance Eq. (90). In this particular case, gradient g1
is positive and g2 is negative."
P,0.6883116883116883,"In order to compare two (1-dimensional) gradients in terms of sign and magnitude, we use the
following formula"
P,0.6894923258559622,"d(g1, g2) = 1 π 
 "
P,0.6906729634002361,"sign(g2) · ∆g,
if g1 = 0
sign(g1) · ∆g,
if g2 = 0
sign(g1 · g2) · ∆g,
otherwise
,
with
∆g = |arctan g1 −arctan g2| . (90)"
P,0.69185360094451,"The magnitude of this quantity depends on the normalized difference between the tangent’s angles ∆g
and is positive for gradients with the same sign and vice versa it is negative for gradients with opposing
signs. See also Fig. 10 for a sketch."
P,0.693034238488784,Published as a conference paper at ICLR 2022
P,0.6942148760330579,"C.5.2
DETERMINING THE CLOSED-LOOP STABILITY FOR LINEAR SYSTEMS"
P,0.6953955135773318,"For linear and deterministic systems, we can easily check if the system is (asymptotically) stable for
a particular linear policy using the following standard result from linear system theory:"
P,0.6965761511216056,"Theorem 3 (Exponential stability for linear time-invariant systems (Callier & Desoer, 1991)). The
solution of xt+1 = Fxt is exponentially stable if and only if σ(F) ⊂D(0, 1), i.e., every eigenvalue
of F has magnitude strictly less than one."
P,0.6977567886658795,"In our setting, this means that the closed-loop systems fulﬁlling the following are unstable,"
P,0.6989374262101535,"|A + ∆A + (B + ∆B)θ| > 1,
(91)"
P,0.7001180637544274,"i.e., the state and input grow exponentially. We therefore refrain from including unstable system in
the results to avoid numerical issues for the gradients’ computation. The respective areas in the plots
are not colored, see e.g., bottom left corner in Fig. 8b."
P,0.7012987012987013,"C.5.3
NUMERICAL VALUES"
P,0.7024793388429752,The numerical values for all parameters used in the motivating example are given as follows:
P,0.7036599763872491,"• True system dynamics: A = 1.0, B = 1.0"
P,0.704840613931523,• Initial condition: s0 = 1.0
P,0.706021251475797,• Reward width parameter: σr = 0.05
P,0.7072018890200709,• Optimal policy gain: θ∗= −1.0
P,0.7083825265643447,• Rollout horizon: T = 60
P,0.7095631641086186,Published as a conference paper at ICLR 2022
P,0.7107438016528925,"D
ADDITIONAL EXPERIMENTAL RESULTS"
P,0.7119244391971665,"In this section, we provide additional experimental results that did not ﬁt into the main body of the
paper."
P,0.7131050767414404,"D.1
COMPARISON WITH OTHER BASELINE ALGORITHMS"
P,0.7142857142857143,"Fig. 11 shows our method compared to a range of baseline algorithms. The results for all baselines
were obtained from Janner et al. (2019) via personal communication. Note that all results are
presented in terms of mean and standard deviation. The comparison includes the following methods:"
P,0.7154663518299882,"• MBPO (Janner et al., 2019),"
P,0.7166469893742621,"• PETS (Chua et al., 2018),"
P,0.717827626918536,"• SAC (Haarnoja et al., 2018),"
P,0.71900826446281,"• PPO (Schulman et al., 2017),"
P,0.7201889020070839,"• STEVE (Buckman et al., 2018),"
P,0.7213695395513577,"• SLBO (Luo et al., 2019)."
P,0.7225501770956316,"0
50
100
150
200"
P,0.7237308146399055,# Steps ×103 0 5 10
P,0.7249114521841794,Evaluation return ×103
P,0.7260920897284534,HalfCheetah
P,0.7272727272727273,"0
20
40
60
80
100
120"
P,0.7284533648170012,# Steps ×103 0 1 2 3
P,0.7296340023612751,Evaluation return ×103
P,0.730814639905549,Hopper
P,0.731995277449823,"0
50
100
150
200
250
300"
P,0.7331759149940968,# Steps ×103 0 2 4
P,0.7343565525383707,Evaluation return ×103
P,0.7355371900826446,Walker2d
P,0.7367178276269185,"0
50
100
150
200
250
300"
P,0.7378984651711924,# Steps ×103 0 2 4 6
P,0.7390791027154664,Evaluation return ×103
P,0.7402597402597403,AntTruncatedObs
P,0.7414403778040142,"OPC
MBPO(⋆)
MBPO
PETS"
P,0.7426210153482881,"STEVE
SAC
PPO
SLBO"
P,0.743801652892562,"Figure 11: Comparison of OPC against a range of baseline methods on three MuJoCo environments.
We present the mean and standard deviation across 5 independent experiments (10 for OPC and
MBPO(⋆)). The original data for MBPO and the other baselines were provided by Janner et al. (2019).
Solid lines represent the mean and the shaded areas correspond to mean ± one standard deviation."
P,0.7449822904368358,Published as a conference paper at ICLR 2022
P,0.7461629279811098,"D.2
ABLATION - RETAIN EPOCHS"
P,0.7473435655253837,"One of the hyperparameters that we found is critical to both OPC and MBPO(⋆), is retain epochs,
i.e., the number of epochs that are kept in the data buffer for the simulated data generated with the ˜px
with x = {OPC, model}. The results for a comparison are shown in Fig. 12. For MBPO(⋆), we found
that for some environments (HalfCheetah, AntTruncatedObs) smaller values for retain epochs
are helpful, i.e., simulated data is almost only on-policy, and for other environments larger values are
beneﬁcial (Hopper, Walker2d). For OPC on the other hand, we found that retain epochs = 50
almost always leads to better results. 0 5 10"
P,0.7485242030696576,MuJoCo
P,0.7497048406139315,Evaluation return ×103
P,0.7508854781582054,HalfCheetah 1 2 3
P,0.7520661157024794,Hopper 0 1 2 3 4
P,0.7532467532467533,Walker2d 0 2 4 6
P,0.7544273907910272,AntTruncatedObs
P,0.755608028335301,"0
100
200"
P,0.7567886658795749,# Steps ×103 0 1 2
P,0.7579693034238488,PyBullet
P,0.7591499409681228,"Evaluation return ×103 0
100"
P,0.7603305785123967,"# Steps ×103 0 1 2 0
200"
P,0.7615112160566706,"# Steps ×103 0.0 0.5 1.0 1.5 0
200"
P,0.7626918536009445,# Steps ×103 1 2 3 4
P,0.7638724911452184,"OPC (50)
OPC (5)
MBPO(⋆) (50)
MBPO(⋆) (5)"
P,0.7650531286894924,"Figure 12: Ablation study for OPC and MBPO(⋆) on four environments from the MuJoCo con-
trol suite (top row) and their respective PyBullet implementations (bottom row). We vary the
retain epochs hyperparameter (indicated by the number in the parentheses behind the legend
entries), i.e., the number of epochs that are kept in the data buffer for the simulated data."
P,0.7662337662337663,"D.3
INFLUENCE OF STATE REPRESENTATION: IN-DEPTH ANALYSIS"
P,0.7674144037780402,"In the following, we will have a closer look at the surprising result from Fig. 4 (left). In there, the
results indicate that MBPO(⋆) is not able to learn a stabilizing policy within the ﬁrst 7’500 steps on
the RoboSchool variant of the CartPole environment. We hypothesize that the failure of MBPO(⋆) is
due to a mismatch of the simulated data and the true state distribution. As a result, the policy that is
optimized with the simulated data cannot stabilize the pole in the true environment."
P,0.768595041322314,"To validate our hypothesis, we perform the following experiment: First, we train a policy π∗that
we know performs well on the true environment, leading to the maximum evaluation return of 1000.
With this policy, we roll out a reference trajectory on the true environment τ ref = {(ˆst, ˆat)}T
t=0.
To perform branched rollouts with the respective methods, OPC and MBPO(⋆), we use the learned
transition models after 20 epochs (5’000 time steps) that were logged during a full learning process
for each method. We then perform 100 branched rollouts of length H = 20 starting from randomly
sampled initial states of the reference trajectories."
P,0.7697756788665879,Fig. 13 shows the difference between the true and predicted state trajectories (median and 95th
P,0.7709563164108618,"percentiles) for each state in [x, cos(ϑ), sin(ϑ), ˙x, ˙ϑ]. Since we start each branched rollout from
a state on the real environment, the initial error is always zero. Across all states, the errors are
drastically reduced when using OPC. Fig. 14 shows the predicted trajectories for the cosine of the
pole’s angle, cos(ϑ). For MBPO(⋆), we observe that the trajectories often diverge and attain values
that are clearly out of distribution, i.e., cos(ϑ) > 1."
P,0.7721369539551358,Published as a conference paper at ICLR 2022
P,0.7733175914994097,"5
10
15
20
time steps −0.1 0.0 0.1"
P,0.7744982290436836,"State difference: strue
t
−spred
t"
P,0.7756788665879575,Lateral Position
P,0.7768595041322314,"5
10
15
20
time steps"
P,0.7780401416765053,cos(Angle)
P,0.7792207792207793,"5
10
15
20
time steps"
P,0.7804014167650531,sin(Angle)
P,0.781582054309327,"5
10
15
20
time steps −0.5 0.0 0.5"
P,0.7827626918536009,"State difference: strue
t
−spred
t"
P,0.7839433293978748,Lateral Velocity
P,0.7851239669421488,"5
10
15
20
time steps"
P,0.7863046044864227,Angular Velocity
P,0.7874852420306966,"Figure 13: Difference in state trajectories strue
t
−spred
t
from branched rollouts using a ﬁxed policy of
length H = 20 on the RoboSchool environment (cf. Fig. 4 (left)) with s = [x, cos(ϑ), sin(ϑ), ˙x, ˙ϑ].
The solid lines show the median across 100 rollouts and the shaded areas represent the 95th percentiles.
With OPC (
), the simulated rollouts follows the true state trajectories much closer, whereas with
MBPO(⋆) (
) the prediction errors quickly accumulate over time."
P,0.7886658795749705,"5
10
15
20 0.8 1.0 1.2 1.4"
P,0.7898465171192444,Transformed Angle: cos(ϑ)
P,0.7910271546635183,"5
10
15
20 0.98 0.99 1.00 1.01 1.02 1.03"
P,0.7922077922077922,"Figure 14: Trajectories of the second state (cos(ϑ)) from 100 branched rollouts using a ﬁxed policy
of length H = 20 on the RoboSchool environment (cf. Fig. 4 (left)). Both plots present the same
data but the differ in terms of scaling of the ordinate. With OPC (
), the respective trajectories
remain around values close to one, which corresponds to the upright position of the pendulum. When
using the standard predictive model from MBPO(⋆) (
), the state trajectories often diverge and the
rollouts are terminated prematurely."
P,0.7933884297520661,Published as a conference paper at ICLR 2022 500 1000
P,0.79456906729634,True return ηn 0 100 200
P,0.7957497048406139,On-Policy
P,0.7969303423848878,Model Error
P,0.7981109799291618,|ηn −˜ηn|
P,0.7992916174734357,"6
7
8
9
10
11
12
13
Iteration n of Policy Optimization 0 100 200 300"
P,0.8004722550177096,Off-Policy
P,0.8016528925619835,Model Error
P,0.8028335301062574,|ηn+1 −˜ηn+1|
P,0.8040141676505312,"Figure 15: Empirical evaluation of the error terms in the policy improvement bound in Eq. (4) for the
CartPole environment (PyBullet). We evaluate the respective terms for a sequence of policies that
were obtained during different iterations n from a full policy optimization run. The respective returns
˜ηn+1, ηn, ˜ηn+1, ηn+1 are approximated as the mean from 100 rollouts on the true environment and
the respective models, ˜popc
n
(
) and ˜pmodel
n
(
). For the return on the true environment ηn (top),
we additionally show the sample distribution of the rollouts’ returns. This nicely demonstrates how
the policy smoothly transitions from failing consistently (n ≤8) to successfully stabilizing the
pole (n ≥12). Additionally, note that the on-policy model error is almost always smaller for OPC
compared to MBPO(⋆), which supports the theoretical motivation that our method is build upon."
P,0.8051948051948052,"D.4
IMPROVEMENT BOUND: EMPIRICAL INVESTIGATION"
P,0.8063754427390791,"In this section, we empirically investigate to what extent OPC is able to tighten the policy improvement
bound Eq. (4) compared to pure model-based approaches. As mentioned in the main paper, the
motivation behind OPC is to reduce the on-policy error and we assume that the off-policy error is
not affected too badly by the corrected transitions. Generally speaking, it is difﬁcult to quantify
a-priori how OPC compares to a purely model-based approach as this depends on the generalization
capabilities of the learned dynamics model, the environment itself and the reward signal."
P,0.807556080283353,"Here, we analyze the CartPole environment (PyBullet implementation) and estimate the respective
error terms that appear in the policy improvement bound Eq. (4). To this end, we roll out a sequence
of policies πn on the true environment (to estimate ηn and ηn+1) and on the learned model with and
without OPC (to estimate ˜ηn and ˜ηn+1). The sequence of policies was obtained during a full policy
optimization run (the policy was logged after each update) and we roll out each policy 100 times
on the respective environment/models. The corresponding learned transition models were similarly
logged during a full policy optimization. For OPC, we estimate the off-policy return ˜ηn+1 using
the learned model from iteration n and reference trajectories from the true environment that were
collected under πn, but then roll out the model with πn+1. The results are shown in Fig. 15."
P,0.8087367178276269,Published as a conference paper at ICLR 2022 0 250 500 750 1000
P,0.8099173553719008,Off-policy Return OPC
P,0.8110979929161747,"1.1
1.3
1.5
1.7
1.9
2.1
2.3
2.5
2.7
2.9
3.1
Policy Stochasticity Multiplier β 0 250 500 750 1000"
P,0.8122786304604487,Off-policy Return
P,0.8134592680047226,MBPO(*)
P,0.8146399055489965,"Figure 16: Sample distributions of the return on the CartPole environment (PyBullet) with in-
creasing stochasticity of the behaviour policy πrollout when rolling out with ˜popc
n /OPC (top) and
˜pmodel
n
/MBPO(⋆) (bottom). The multiplier β quantiﬁes the stochasticity of πrollout relative to the
reference policy πn (Eq. (92)) such that higher values lead to more ‘off-policy-ness’."
P,0.8158205430932703,"D.5
OFF-POLICY ANALYSIS"
P,0.8170011806375442,"In this section, we investigate the robustness of OPC towards ‘off-policy-ness’ of the reference
trajectories that are simulated under the data-generating policy πn. To this end, we manually increase
the stochasticity of the behavior policy πrollout by a factor β such that"
P,0.8181818181818182,"V[πrollout(· | s)] = β2V[πn(· | s)],
(92)"
P,0.8193624557260921,"and we keep the mean the same for both policies. Fig. 16 shows the distributions of the returns from
100 rollouts with varying degree of ‘off-policy-ness’ on the CartPole environment (PyBullet). Note
that the data-generating policy consistently leads to the maximum return of 1000. For β close to
one, both OPC and MBPO(⋆) lead to almost ideal behavior and correctly predict the return in more
than 95% of the cases. As we increase the policy’s stochasticity (from left to right), the respective
performance of the policy decreases until all rollouts terminate prematurely (β ≥2.3). Notably,
the extent of this effect is almost identical between OPC and MBPO(⋆). We conclude that OPC is, at
least empirically, robust towards ‘off-policy-ness’ of the reference trajectories. Otherwise, we should
observe a more pronounced degradation of the policy’s performance with increased stochasticity of
the behavior policy, since this leads to observing more off-policy state/actions."
P,0.820543093270366,Published as a conference paper at ICLR 2022
P,0.8217237308146399,Algorithm 3 Original MBPO algorithm
P,0.8229043683589138,"1: Initialize policy πφ, predictive model pθ, environment dataset Denv, model dataset Dmodel
2: for N epochs do
3:
Train model pθ on Denv via maximum likelihood
4:
for E steps do
5:
Take action in environment according to πφ; add to Denv
6:
for M model rollouts do
7:
Sample st uniformly from Denv
8:
Perform k-step model rollout starting from st using policy πφ; add to Dmodel
9:
for G gradient updates do
10:
Update policy parameters on model data: φ ←φ −λπ ˆ∇φJπ(φ, Dmodel)"
P,0.8240850059031877,"Algorithm 4 Our version of MBPO algorithm, denoted as MBPO(⋆)"
P,0.8252656434474617,"1: Initialize policy πφ, predictive model pθ, environment dataset Denv, model dataset Dmodel
2: for N epochs do
3:
Train model pθ on Denv via maximum likelihood
4:
for E steps do
5:
Take action in environment according to πφ; add to Denv
6:
for M model rollouts do
7:
Sample st uniformly from Denv
8:
Perform k-step model rollout starting from st using policy πφ; add to Dmodel
9:
for G gradient updates do
10:
Update policy parameters on model data: φ ←φ −λπ ˆ∇φJπ(φ, Dmodel)"
P,0.8264462809917356,"D.6
IMPLEMENTATION CHANGES TO ORIGINAL MBPO"
P,0.8276269185360094,"Here, we provide details to the changes we made to the original implementation of MBPO. We denote
our variant as MBPO(⋆)."
P,0.8288075560802833,"Episodic Setting
MBPO updates the policy during rollouts on the real environment. Algorithms 3
and 4 show the original and our version of MBPO, respectively. While the original version might be
more sample efﬁcient because it allows for more policy gradient updates based on more recent data,
it is not realistic to update the policy during a rollout on the real environment."
P,0.8299881936245572,"Mix-in of Real Data
As mentioned in the main paper, one of the key problems with MBRL is
that the generated data might exhibit so-called model-bias. Biased data can be problematic for
policy optimization as the transition tuples possibly do not come from the true state distribution
of the real environment, thus misleading the RL algorithm. In the original implementation of
MBPO, the authors use a mix of simulated data from the model as well as observed data from the
true environment for policy optimization (https://github.com/jannerm/mbpo/blob/
22cab517c1be7412ec33fbe5c510e018d5813ebf/mbpo/algorithms/mbpo.py#
L430) to alleviate the issue of model bias. While this design choice might help in practice, it adds
another hyperparameter and the exact inﬂuence of this parameter is difﬁcult to interpret. We therefore
refrain from mixing in data from the true environment, but instead only use simulated transition
tuples – staying true to the spirit of MBRL."
P,0.8311688311688312,"Fix Replay Buffer
The replay buffer that stores the simulated data Dmodel for policy optimization
is allocated to a ﬁxed size of rollout length×rollout batch size×retain epochs,
meaning that per epoch rollout length×rollout batch size transition tuples are simu-
lated and only the data from the last retain epochs are kept in the buffer (https://github.
com/jannerm/mbpo/blob/22cab517c1be7412ec33fbe5c510e018d5813ebf/
mbpo/algorithms/mbpo.py#L351). As the buffer is implemented as a FIFO queue and
rollouts might terminate early such that the actual number of simulated transitions is less than
rollout length, data from older episodes as speciﬁed by retain epochs are retained in the
buffer. We assume that this is not the intended behavior and correct for that in our implementation."
P,0.8323494687131051,Published as a conference paper at ICLR 2022
P,0.833530106257379,"0.0
2.5
5.0
7.5"
P,0.8347107438016529,# Steps ×103 0.5 1.0
P,0.8358913813459268,Evaluation return ×103
P,0.8370720188902007,InvertedPendulum
P,0.8382526564344747,"0
50
100
150"
P,0.8394332939787486,# Steps ×103 1 2 3
P,0.8406139315230224,Evaluation return ×103
P,0.8417945690672963,Hopper
P,0.8429752066115702,"0
100
200"
P,0.8441558441558441,# Steps ×103 0 5 10
P,0.8453364817001181,Evaluation return ×103
P,0.846517119244392,HalfCheetah
P,0.8476977567886659,"OPC (ours)
MBPO β = 5%
MBPO β = 0%
SAC"
P,0.8488783943329398,"Figure 17: Results based on original implementation Appendix D.7: Comparison of OPC to the
model-based MBPO and model-free SAC algorithms. The two MBPO variants differ in terms of the
mix-in ratio β of real off-policy transitions in the training data – a critical hyperparameter. All
model-based approaches outperform SAC in terms of convergence for the high-dimensional tasks.
Moreover, on the highly stochastic Hopper environment, OPC outperforms both MBPO variants and
does not require additional real off-policy data."
P,0.8500590318772137,"D.7
RESULTS FOR ORIGINAL MBPO IMPLEMENTATION"
P,0.8512396694214877,"The following results were obtained with the original MBPO implementation without the changes
described in Appendix D.6. Consequently, the results for OPC in this section also do not include these
changes."
P,0.8524203069657615,Comparative Evaluation
P,0.8536009445100354,"We evaluate the original implementation on three continuous control benchmark tasks from the
MuJoCo control suite (Todorov et al., 2012). The results for OPC, two variants of MBPO and SAC are
presented in Fig. 3. Both OPC and MBPO use a rollout horizon of H = 10 to generate the training data.
The difference between the two MBPO variants lies in the mix-in ratio β of real off-policy transitions
to the simulated training data. Especially for highly stochastic environments such as the Hopper,
this mix-in ratio is a critical hyperparameter that requires careful tuning (see also Fig. 18). Fig. 3
indicates that OPC is on par with MBPO on both the InvertedPendulum and HalfCheetah environments
that exhibit little stochasticity. On the Hopper environment, OPC outperforms both MBPO variants.
Note that the mix-in ratio for MBPO is critical for successful learning (the original implementation
uses β = 5%). OPC on the other hand, does not require any mixed-in real data. SAC learns slower on
the more complex Hopper and HalfCheetah environments, re-iterating that model-based approaches
are signiﬁcantly more data-efﬁcient than model-free methods."
P,0.8547815820543093,Large Ablation Study – Hopper
P,0.8559622195985832,The full study investigates the inﬂuence of the following hyperparameters and design choices:
P,0.8571428571428571,• Rollout length H and total number of simulated transitions N.
P,0.8583234946871311,• Mix-in ratio of real transitions into the training data β.
P,0.859504132231405,"• Deterministic or stochastic rollouts (for MBPO): Current state-of-the-art methods in MBRL
rely on probabilistic dynamic models to capture both aleatoric and epistemic uncertainty.
Accordingly, when rolling out the model, these two sources of uncertainty are accounted for.
However, we show that in terms of evaluation return, the stochastic rollouts do not always
lead to the best outcome."
P,0.8606847697756789,"• Re-setting the buffer of simulated data after a policy optimization step: We found that
re-setting the replay buffer for simulated data after each iteration of Algorithm 3 can have
a large inﬂuence. In particular, the replay buffer is implemented as a FIFO queue with a
ﬁxed size. Hence, if the buffer is not emptied after each iteration, it still contains (simulated)
off-policy transitions."
P,0.8618654073199528,Fig. 18 presents the results of the ablation study. We want to highlight a few core insights:
P,0.8630460448642266,Published as a conference paper at ICLR 2022
P,0.8642266824085005,"1. When choosing the best setting for each method, OPC improves MBPO by a large margin
(bottom row, right). Generally, for long rollouts H = 20 (bottom row), OPC improves
MBPO."
P,0.8654073199527745,"2. Across all settings, OPC performs well and is more robust with respect to the choices of
hyperparameters (e.g., bottom row left and center, third row left). Only for few exceptions,
re-setting the buffer can have detrimental effects (e.g., top right, third row right)."
P,0.8665879574970484,"3. Mixing in real transition data can be highly beneﬁcial for MBPO (second row left and center)
but it can also have the opposite effect (bottom row)."
P,0.8677685950413223,"4. Using deterministic rollouts can be beneﬁcial (third row right and left), detrimental (third
row center) or have no inﬂuence (bottom row left) for MBPO."
P,0.8689492325855962,"5. It is not clear, if re-setting the buffer after each iteration should overall be recommended or
not. This remains an open question left for future research."
P,0.8701298701298701,Published as a conference paper at ICLR 2022
P,0.8713105076741441,"0
50
100
150"
P,0.872491145218418,# Steps ×103 1 2 3
P,0.8736717827626919,Evaluation return ×103
P,0.8748524203069658,"0
50
100
150"
P,0.8760330578512396,# Steps ×103 1 2 3
P,0.8772136953955135,Evaluation return ×103
P,0.8783943329397875,"0
50
100
150"
P,0.8795749704840614,# Steps ×103 1 2 3
P,0.8807556080283353,Evaluation return ×103
P,0.8819362455726092,"0
50
100
150"
P,0.8831168831168831,# Steps ×103 1 2 3
P,0.8842975206611571,Evaluation return ×103
P,0.885478158205431,"0
50
100
150"
P,0.8866587957497049,# Steps ×103 1 2 3
P,0.8878394332939787,Evaluation return ×103
P,0.8890200708382526,"0
50
100
150"
P,0.8902007083825265,# Steps ×103 1 2 3
P,0.8913813459268005,Evaluation return ×103
P,0.8925619834710744,"0
50
100
150"
P,0.8937426210153483,# Steps ×103 1 2 3
P,0.8949232585596222,Evaluation return ×103
P,0.8961038961038961,"0
50
100
150"
P,0.89728453364817,# Steps ×103 1 2 3
P,0.898465171192444,Evaluation return ×103
P,0.8996458087367178,"0
50
100
150"
P,0.9008264462809917,# Steps ×103 1 2 3
P,0.9020070838252656,Evaluation return ×103
P,0.9031877213695395,"0
50
100
150"
P,0.9043683589138135,# Steps ×103 1 2 3
P,0.9055489964580874,Evaluation return ×103
P,0.9067296340023613,"0
50
100
150"
P,0.9079102715466352,# Steps ×103 1 2 3
P,0.9090909090909091,Evaluation return ×103
P,0.910271546635183,"0
50
100
150"
P,0.911452184179457,# Steps ×103 1 2 3
P,0.9126328217237308,Evaluation return ×103
P,0.9138134592680047,"OPC + reset buffer
OPC + not reset buffer"
P,0.9149940968122786,"MBPO + reset buffer + deterministic
MBPO + not reset buffer + deterministic"
P,0.9161747343565525,"MBPO + reset buffer + stochastic
MBPO + not reset buffer + stochastic"
P,0.9173553719008265,"Figure 18: Results based on original implementation Appendix D.7: Large ablation study on the
Hopper environment, investigating the inﬂuence various hyperparameters and design choices. Mix-in
ratio of real data β (columns): 0%, 5%, 10% from left to right. Rollout length H (rows): 1, 5, 10, 20
from top to bottom."
P,0.9185360094451004,Published as a conference paper at ICLR 2022
P,0.9197166469893743,"E
CONNECTION BETWEEN MBRL AND ILC"
P,0.9208972845336482,"In this section we compare optimization-based or so-called norm-optimal ILC (NO-ILC) with MBRL.
In particular, we show that under certain assumptions we can reduce the MBRL setting to NO-ILC.
This comparison is structured as follows: First, we review the basic assumptions and notations for
NO-ILC. While there are many variations on NO-ILC, we will only consider the very basic setting,
i.e., linear dynamics, fully observed state and deterministic state evolution. Then, based on the lifted
state representation of the problem, we derive the solution to the optimization problem that leads to
the input sequence of the next iteration / rollout. Next, we will state the general MBRL problem and
pose the simpliﬁcations that we need to make in order to be equivalent to the NO-ILC problem. Last,
we show that the solution to the reduced MBRL problem is equivalent to the one of NO-ILC."
P,0.922077922077922,"E.1
NORM-OPTIMAL ILC"
P,0.9232585596221959,"The goal of NO-ILC is to ﬁnd a sequence of inputs a = [a⊤
0 , . . . , a⊤
T −1]⊤with length T + 1 such
that the outputs y = [y⊤
0 , . . . , y⊤
T ]⊤follow a desired output trajectory ˆy = [ˆy⊤
0 , . . . , ˆy⊤
T ]⊤. In the
simplest setting we assume that the system evolves according to the following linear dynamics"
P,0.9244391971664699,"st+1 = Ast + Bat + dt
(93)
yt = Cst,
(94)"
P,0.9256198347107438,"with state st ∈RdS, action at ∈RdA, output yt ∈Rp and disturbance dt ∈RdS. One of
the major assumption in ILC is that the disturbances dt are repetitive, meaning that the sequence
d = [d⊤
0 , . . . , d⊤
T ]⊤does not change (or only varies slightly) across multiple rollouts. While these
disturbances can be considered to come from some exogenous error source, one can also interpret
these as unmodeled effects of the dynamics, e.g., nonlinearities stemming from friction, aerodynamic
effects, etc."
P,0.9268004722550177,"For the derivation, let’s assume C = I so that we’re operating on an MDP. Consequently, the goal
of ILC is to track a sequence of desired states ˆs instead of outputs ˆy. In order to ﬁnd an optimal
input sequence, we minimize the squared 2-norm of a state’s deviation from the reference for each
time-step t of the sequence, i.e., et = ˆst −st. As is common in the ILC literature, we make use of
the so-called lifted system formulation such that we can conveniently write the state evolution for one
rollout using a single matrix/vector multiplication. Assuming zero initial state (which can always
be done in the linear setting by just shifting the state by a constant offset), we obtain the following
formulation"
P,0.9279811097992916,"s = Fa + d,
with
F = "
P,0.9291617473435655,
P,0.9303423848878394,"0
· · ·
B
0
AB
B
0"
P,0.9315230224321134,"A2B
AB
B
...
...
...
... "
P,0.9327036599763873,"
∈RdS(T +1)×dAT .
(95)"
P,0.9338842975206612,"Thus, we can write the state-error at the j-th iteration of ILC in the lifted representation as (recall that
the disturbance d does not change across iterations)"
P,0.935064935064935,"e(i) = ˆs −s(i) = ˆs −Fa(i) −d,
(96)"
P,0.9362455726092089,"e(i+1) = e(i) −F(a(i+1) −a(i)).
(97)"
P,0.9374262101534829,"Now, the resulting optimization problem then becomes"
P,0.9386068476977568,"a(i+1)
∗
= arg min
a(i+1) J

a(i+1)
with J

a(i+1)
= 1"
P,0.9397874852420307,2∥e(i+1)∥2.
P,0.9409681227863046,"In order to be less sensitive to noise and the inherent stochasticity of real-world problems, one
typically also adds a regularizing term that penalizes the changes in the input sequence. While this
additional penalization term can slow down the learning process it makes it more robust by avoiding
overcompensation to the disturbances. The full objective then becomes"
P,0.9421487603305785,"J

a(i+1)
= 1"
P,0.9433293978748524,"2∥e(i+1)∥2
M + 1"
P,0.9445100354191264,"2∥a(i+1) −a(i)∥2
W,
(98)"
P,0.9456906729634003,Published as a conference paper at ICLR 2022
P,0.9468713105076741,"where we additionally added positive semi-deﬁnite cost matrices M, W for the respective norms
in order to facilitate tuning of the corresponding terms in the objective. Given the regularized cost
function we obtain the optimal sequence of inputs at the next iteration as"
P,0.948051948051948,"a(i+1)
∗
= a(i)
∗+
 
F⊤MF + W
−1 F⊤Me(i).
(99)"
P,0.9492325855962219,"E.2
MODEL-BASED RL TO NORM-OPTIMAL ILC"
P,0.9504132231404959,"Assumptions
In order to show the equivalence of MBRL and NO-ILC we need to make some
assumptions to the above stated optimization problem."
P,0.9515938606847698,"• No aleatoric uncertainty in the model, i.e., no transition noise ωt = 0
∀t.
• Typically in RL we assume the policy to be stationary, however, in this setting we will allow
for non-stationary policies that are indexed by time t (the result will not necessarily depend
on the state such that we essentially just obtain a feedforward control sequence. To include
feedback one can always just combine the feedforward signal with a local controller that
tracks the desired state/action trajectory).
• The reward is given as the negative quadratic error of the state w.r.t. a desired state trajectory,
r(st, at) = −1"
P,0.9527744982290437,2∥ˆst −st∥2
P,0.9539551357733176,"• Typically in RL we have constraints on the policy such that it does not change too much
after every iteration, see e.g. TRPO, REPS, etc. While in the mentioned approaches, the
policy are often constrained in terms of their parameterization vectors, we constrain it as
∥π(i+1)
t
−π(i)
t ∥2 ≤ϵπ
∀t."
P,0.9551357733175915,"• Assume that the model is given by ˜ft(s, a) = As + Ba + dt, where A and B are ﬁxed
system matrices and dt is a time-dependent offset that we learn."
P,0.9563164108618654,"The learned dynamics model
Given state/input pairs of the i-th trajectory τ (i) = {(s(i)
t , a(i)
t )}T
t=0
from the true system, we can now improve our dynamics model. In particular, if we minimize the
prediction error over τ (i), we obtain"
P,0.9574970484061394,"d(i)
t
= s(i)
t+1 −

As(i)
t
+ Ba(i)
t

(100)"
P,0.9586776859504132,"The resulting dynamics for the optimal control problem in Eq. (1) become
˜f (i)
t (s, a) = s(i)
t+1 + A(s −s(i)
t ) + B(a −a(i)
t )
(101)
which are the error dynamics around the trajectory. Now in the noisy case, taking the last trajectory is
not necessarily the best thing one can do. E.g., (Schöllig & D’Andrea, 2009) instead integrate the
information of all past trajectories via Kalman-ﬁltering. In the fully observed case, one way to think
of this is as low-pass ﬁltering dt in order to account for the transition noise ω."
P,0.9598583234946871,"The resulting MBRL problem
Now, let’s plug in all assumptions into the MBRL problem and
have a look at how to solve it."
P,0.961038961038961,"π(i+1)
∗
=
arg min
π={π0,...,πT } T
X t=0"
P,0.9622195985832349,"1
2∥ˆst −st∥2"
P,0.9634002361275088,"st+1 = s(i)
t+1 + A(s −s(i)
t ) + B(a −a(i)
t )"
P,0.9645808736717828,at = πt
P,0.9657615112160567,"∥π(i)
t
−πt∥2 ≤ϵπ
∀t (102)"
P,0.9669421487603306,"where we have ﬂipped the reward’s sign to transform it into a minimization problem. In the presented
form, this optimization problem has a well-deﬁned unique solution, however, it is a-priori not clear
if the trust-region constraint is active for some time-steps. To facilitate an analytical solution to
equation 102, we incorporate the constrain on the policy’s stepsize by a soft-constraint such that"
P,0.9681227863046045,"π(i+1)
∗
=
arg min
π={π0,...,πT } T
X t=0"
P,0.9693034238488784,"1
2∥ˆst −st∥2 + Cπ
t
2 ∥π(i)
t
−πt∥2"
P,0.9704840613931524,"st+1 = s(i)
t+1 + A(s −s(i)
t ) + B(a −a(i)
t )"
P,0.9716646989374262,"at = πt, (103)"
P,0.9728453364817001,Published as a conference paper at ICLR 2022
P,0.974025974025974,"with Cπ
t being constants that weight the respective trust-region terms."
P,0.9752066115702479,"Generally, the MBRL problem cannot be solved analytically due to the (possibly highly non-linear,
non-differentiable, etc.) reward signal and the need for propagating the (uncertain) dynamics model
forwards in time. However, using the simplifying assumptions above, the reduced MBRL problem
equation 103 can in fact be solved analytically. We can circumvent the equality constraints by
predicting the state trajectory using the error corrected dynamics such that we obtain a lifted dynamics
formulation similar to the analysis for ILC,"
P,0.9763872491145218,"s(i) = Fa(i) + d(i),
with
d(i) = s(i) −Fa(i),
(104)"
P,0.9775678866587958,"with s, a denoting the stacked states and actions of the recorded trajectory. Using this notation, the
optimization problem reduces to"
P,0.9787485242030697,"π(i+1)
∗
= arg min
π
1
2∥ˆs −s(i)∥2 + 1"
P,0.9799291617473436,"2∥π(i) −π∥2
Cπ,
(105)"
P,0.9811097992916175,"with Cπ = diag[Cπ
0 , . . . , Cπ
H]. By inserting equation 104 into equation 105 we obtain the closed-
form solution as"
P,0.9822904368358913,"π(i+1)
∗
=
 
F⊤F + Cπ−1 F⊤
ˆs −d(i)
= π(i)
∗
+
 
F⊤F + Cπ−1 F⊤
ˆs −s(i)
,
(106)"
P,0.9834710743801653,"which, clearly, is equivalent to equation 99 for M = I and W = Cπ."
P,0.9846517119244392,"E.3
EXTENSIONS"
P,0.9858323494687131,"In the previous section, we analyzed the most basic setting for NO-ILC, i.e., linear dynamics, no
transition noise in the dynamics and no state/input constraints. Some of these assumptions can easily
be liftd to genearlized the presented framework."
P,0.987012987012987,"Nonlinear Dynamics
Instead of the system being deﬁned by ﬁxed matrices A, B, we can just
linearize a non-linear dynamics model f(s, a) around the last state/input trajectory such that"
P,0.9881936245572609,"A(i)
t
= ∂f ∂s"
P,0.9893742621015348,"st=s(i)
t
,at=a(i)
t
,
B(i)
t
= ∂f ∂a"
P,0.9905548996458088,"st=s(i)
t
,at=a(i)
t
(107)"
P,0.9917355371900827,"and the corresponding dynamics matrix for the lifted representation becomes (dropping the superscript
notation indicating the iteration for clarity) F = "
P,0.9929161747343566,
P,0.9940968122786304,"0
· · ·
B0
0
A0B0
B1
0"
P,0.9952774498229043,"A0A1B0
A0B1
B2
...
...
...
... "
P,0.9964580873671782,"
∈RdS(T +1)×dAT .
(108)"
P,0.9976387249114522,"State/Input Constraints
Recall that we could solve the quadratic problems equation 98 and
equation 103 analytically because we assumed no explicit inequality constraints on the state
and inputs. In practice, however, both states and actions are limited by physical or safety con-
straints typically given by polytopes. While a closed-form solution is not readily available for
this case, one can easily employ any numerical solver to deal with such constraints. See, e.g.,
https://en.wikipedia.org/w/index.php?title=Quadratic_programming for
a comprehensive list of available solvers."
P,0.9988193624557261,"Stochastic Dynamics
Schöllig & D’Andrea (2009) generalize the ILC setting by considering two
separate sources of noise: 1) transition noise in the dynamics, i.e., st+1 = f(st, at) + ωf
t as well
as 2) varying disturbances, i.e., d(i+1) = d(i) + ωd. Based on this model, a Kalman-Filter in the
iteration-domain is developed that estimates the respective random variables and the ILC scheme is
adapted to account for the estimated quantities."
