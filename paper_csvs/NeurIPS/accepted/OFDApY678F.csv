Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000980392156862745,"Most linear experimental design problems assume homogeneous variance, even
though heteroskedastic noise is present in many realistic settings. Let a learner have
access to a finite set of measurement vectors X ⊂Rd that can be probed to receive
noisy linear responses of the form y = x⊤θ∗+ η. Here θ∗∈Rd is an unknown
parameter vector, and η is independent mean-zero σ2
x-strictly-sub-Gaussian noise
defined by a flexible heteroskedastic variance model, σ2
x = x⊤Σ∗x. Assuming that
Σ∗∈Rd×d is an unknown matrix, we propose, analyze and empirically evaluate a
novel design for uniformly bounding estimation error of the variance parameters,
σ2
x. We demonstrate the benefits of this method with two adaptive experimental
design problems under heteroskedastic noise, fixed confidence transductive best-
arm identification, and level-set identification; proving the first instance-dependent
lower bounds in these settings. Lastly, we construct near-optimal algorithms and
empirically demonstrate the large improvements in sample complexity gained from
accounting for heteroskedastic variance in these designs."
INTRODUCTION,0.00196078431372549,"1
Introduction"
INTRODUCTION,0.0029411764705882353,"Accurate data analysis pipelines require decomposing observed data into signal and noise components.
To facilitate this, the noise is commonly assumed to be additive and homogeneous. However, in
applied fields this assumption is often untenable, necessitating the development of novel design and
analysis tools. Specific examples of deviations from homogeneity include the variability in sales
figures of different products in e-commerce [24, 31], the volatility of stocks across sectors [5], spatial
data [58, 43], and genomics [48, 10]. This heteroskedasticity is frequently structured as a function of
observable features. In this work, we develop two near-optimal adaptive experimental designs that
take advantage of such structure."
INTRODUCTION,0.00392156862745098,"There are many approaches, primarily developed in the statistics and econometrics literature, for
efficient mean estimation in the presence of heteroskedastic noise (see [56, 16] for a survey). These
methods concentrate on settings where data have already been collected and do not consider sampling
to better understand the underlying heteroskedasticity. Some early work exists on experimental design
in the presence of heteroskedastic noise, however these methods are non-adaptive (see Section 2 for
details). Importantly, using naive data collection procedures designed for homoskedastic environments
may lead to sub-optimal inference downstream (see Section 1.2 for examples). A scientist may fail
to take samples that allow them to observe the effect of interest with a fixed sampling budget, thus
wasting time and money. This work demonstrates how to efficiently and adaptively collect data"
INTRODUCTION,0.004901960784313725,"∗Department of Statistical Science, Duke University, justin.weltz@duke.edu, work conducted at Amazon
†Amazon.com, USA, fieztann@amazon.com
‡Department of Statistical Science, Duke University, eric.laber@duke.edu
§Department of Statistical Science, Duke University, alexander.volfovsky@duke.edu
¶Amazon.com, USA, bjmason@amazon.com
∥Meta, USA, houssamn@meta.com
∗∗Michael G. Foster School of Business, University of Washington, lalitj@uw.edu, work conducted at Amazon"
INTRODUCTION,0.0058823529411764705,"in the presence of heteroskedastic noise. It bridges the gap between heteroskedasticity-robust (but
potentially inefficient) passive data collection techniques, and powerful (but at times brittle) active
algorithms that largely ignore the presence of differing variances."
PROBLEM SETTING,0.006862745098039216,"1.1
Problem Setting"
PROBLEM SETTING,0.00784313725490196,"To ground our discussion of heteroskedasticity in active data collection, we focus on adaptive
experimentation with covariates; also known as pure-exploration linear bandits [7, 45]. Let a learner
be given a finite set of measurement vectors X ⊂Rd, span(X) = Rd, which can be probed to
receive linear responses with additive Gaussian noise of the form:"
PROBLEM SETTING,0.008823529411764706,"y = x⊤θ∗+ η
with
η ∼N(0, σ2
x),
σ2
x := x⊤Σ∗x.
(1)"
PROBLEM SETTING,0.00980392156862745,"Here, θ∗∈Rd is an unknown parameter vector, η is independent mean-zero Gaussian noise with
variance σ2
x = x⊤Σ∗x, and Σ∗∈Rd×d is an unknown positive semi-definite matrix with the
assumption that σ2
min ≤σ2
x ≤σ2
max for all x ∈X, where σ2
max, σ2
min > 0 are known. We focus
on Gaussian error for ease of exposition but extend to strictly sub-Gaussian noise in the Appendix.
The linear model in Eq. (1) with unknown noise parameter Σ∗directly generalizes the standard
multi-armed bandit to the case where the arms have different variances [4]. Additionally, this
heteroskedastic variance structure arises naturally in linear mixed effects models [20], which are used
in a variety of experimental settings to account for multi-level heterogeneity (patient, spatial and block
design variation [41]). Throughout, Z ⊂Rd will be a set (that need not be equal to X) over which
our adaptive designs will be evaluated. While the theory holds true for σ2
min ≤σ2
x ≤σ2
max in general,
we consider the case where σ2
max = maxx∈X σ2
x and σ2
min = minx∈X σ2
x so that κ := σ2
max/σ2
min
summarizes the level of heteroskedasticity."
HETEROSKEDASTICITY CHANGES OPTIMAL ADAPTIVE EXPERIMENTATION,0.010784313725490196,"1.2
Heteroskedasticity Changes Optimal Adaptive Experimentation"
HETEROSKEDASTICITY CHANGES OPTIMAL ADAPTIVE EXPERIMENTATION,0.011764705882352941,"To underscore the importance of adapting to heteroskedasticity, we consider the performance of
RAGE [12], a best arm identification algorithm designed for homoskedastic settings. This algorithm
does not account for differing variances, so it is necessary to upper bound σ2
x ≤σ2
max for each x to
identify the best arm with probability 1 −δ for δ ∈(0, 1). In Fig. 1, we compare this approach to the
optimal sampling allocation accounting for heteroskedasticity in a setting that is a twist on a standard
benchmark [46] for best-arm identification algorithms. Let X = Z ⊂R2 and |X| = 3. We take
x1 = e1 and x2 = e2 to be the first and second standard bases and set x3 = {cos(0.5), sin(0.5)}.
Furthermore, we take θ∗= e1 such that x1 has the highest reward and x3 has the second highest.
In the homoskedastic case, an optimal algorithm will focus on sampling x2 because it is highly
informative for distinguishing between x1 and x3 as shown in Fig 1b. However, if the errors are
heteroskedastic, such a sampling strategy may not be optimal. If σ2 ≫σ1, σ3, the optimal allocation
for heteroskedastic noise focuses nearly all samples on x1 and x3 (as shown in Fig. 1a), which are
more informative than x2 because they have less noise. Hence, ignoring heteroskedasticity and upper-
bounding the variances leads to a suboptimal sampling allocation and inefficient performance. This
effect worsens as the amount of heteroskedasticity increases, leading to an additional multiplicative
factor of κ (cf., [12], Thm. 1) as demonstrated in Fig. 1c, In this important example, we see that the
sample complexity of an algorithm that ignores heteroskedasticity suffers a linear dependence on κ,
while a method that adapts to the heteroskedasticity in the data does not."
PAPER CONTRIBUTIONS,0.012745098039215686,"1.3
Paper Contributions"
PAPER CONTRIBUTIONS,0.013725490196078431,"1. Finite Sample Noise Estimation Guarantees. We use an experimental design technique to
construct estimates of the heteroskedastic noise variances with bounded maximum absolute error.
This adaptive experimental design is a two-phase sample splitting procedure based on a pair of
distinct G-optimal experimental designs [28], with bounds that scale favorably with the underlying
problem dimension."
PAPER CONTRIBUTIONS,0.014705882352941176,"2. Near-Optimal Experimental Design Algorithms. We propose adaptive experimental design
algorithms for transductive linear best-arm and level-set identification with heteroskedastic noise,
that combine our variance estimation method with optimal experimental design techniques. We
prove sample complexity upper bounds for unknown Σ∗that nearly match instance dependent
lower bounds when Σ∗is known. Importantly, we show that in contrast to naive methods which"
PAPER CONTRIBUTIONS,0.01568627450980392,"x1
x2
x3
Arms 0.0 0.1 0.2 0.3 0.4 0.5"
PAPER CONTRIBUTIONS,0.016666666666666666,Density
PAPER CONTRIBUTIONS,0.01764705882352941,(a) Heteroskedastic design
PAPER CONTRIBUTIONS,0.018627450980392157,"x1
x2
x3
Arms 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
PAPER CONTRIBUTIONS,0.0196078431372549,Density
PAPER CONTRIBUTIONS,0.020588235294117647,(b) Homoskedastic design
PAPER CONTRIBUTIONS,0.021568627450980392,"0
20
40
60
80
100 0 500 1000 1500 2000 2500"
PAPER CONTRIBUTIONS,0.022549019607843137,Oracle Sample Complexity
PAPER CONTRIBUTIONS,0.023529411764705882,"Heteroskedastic Oracle
Homoskedastic Oracle"
PAPER CONTRIBUTIONS,0.024509803921568627,(c) Sample complexity as κ increases
PAPER CONTRIBUTIONS,0.025490196078431372,"Figure 1: We compare the sampling strategies of algorithms that do and do not account for heteroskedastic
variance in Figs. 1a and 1b respectively, when κ = 20. Figure 1c shows that the gap in the optimal sample
complexity between the two algorithms becomes linear as κ increases."
PAPER CONTRIBUTIONS,0.026470588235294117,"have a multiplicative dependence on κ, we only suffer an additive dependence. Lastly, we show
that the theoretical improvements translate to stronger empirical performance."
RELATED WORK,0.027450980392156862,"2
Related Work"
RELATED WORK,0.028431372549019607,"Experimental Design and Active Learning for Heteroskedastic Regression. Many experimental
design methods for heteroskedastic regression assume known weights (or efficiency functions) [55,
54]. Procedures that estimate the heteroskedastic variance focus on optimizing the design for the
weighted least squares estimator given a burn-in period used to estimate the noise parameters [37, 53,
52]. These contributions assume that the variance is unstructured, which leads to sample complexities
that scale with the number of arms. Similarly, past active learning work on linear bandits with
heteroskedastic noise either assume the variances are known [30, 62], or that the variances are
unstructured [14, 8, 4]. The works that assume structured heteroskedastic noise in a linear bandit
framework focus on active maximum likelihood estimation and G-optimal design [7, 45], but do not
exploit the structure of the heteroskedastic noise to improve performance, leading to complexities
scaling poorly in the problem dimension. Lastly, Mukherjee et al. [38] study policy value estimation
under the reward model described in Eq. 1. However, their variance estimator is different from ours."
RELATED WORK,0.029411764705882353,"Best-Arm and Level Set Identification in Linear and Logistic Bandits. Best-arm identification is
a fundamental problem in the bandit literature [13, 46, 18]. Soare [44] constructed the first passive
and adaptive algorithms for pure-exploration in linear bandits using G-optimal and XY-allocation
experimental designs. Fiez et al. [12] built on these results, developing the first pure exploration
algorithm (RAGE) for the transductive linear bandit setting with nearly matching upper and lower
bounds. Recent work specializes these methods to different settings [47, 57, 25, 26], improves
time complexity [60, 50, 33], and targets different (minimax, asymptotically optimal, etc.) lower
bound guarantees [59, 22, 9]. In the level set literature, Mason et al. [35] provides the first instance
optimal algorithm with matching upper and lower bounds. Critically, none of these contributions
account for heteroskedastic variance. Finally, we note the connection between this work and the
logistic bandits literature [23, 11, 1, 36]. In that setting, the observations are Bernoulli random
variables with a probability given by P(y = 1|x) =

1 + exp(−x⊤θ∗)
	−1 for x ∈Rd, θ∗∈Rd and
y ∈{0, 1}. Because of the mean-variance relationship for Bernoulli random variables, points x for
which P(y = 1|x) is near 0 or 1 have lower variance than other x’s. While the tools are different
in these two cases, we highlight two common ideas: First, Mason et al. [36] present a method that
explicitly handles the heteroskedasticity in logistic bandits. Second, a core focus of optimal logistic
bandit papers is reducing the dependence on the worst case variance to only an additive penalty,
similar to the guarantee of κ herein, which has been shown to be unavoidable in general [23]."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.030392156862745098,"3
Heteroskedastic Variance Estimation"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03137254901960784,"In this section, we describe an algorithm for heteroskedastic noise variance estimation with error
bounds that scale favorably in the problem dimension. We then evaluate this method empirically."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03235294117647059,"Given a sampling budget Γ, our goal is to choose {xt}Γ
t=1 ⊂X to control the tail of the maximum
absolute error (MAE): maxx∈X |bσ2
x −σ2
x| by exploiting the heteroskedastic variance model. Alg. 1,
HEAD (Heteroskedasticity Estimation by Adaptive Designs), provides an experimental design for
finite-time bounds on structured heteroskedastic variance estimation."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03333333333333333,"Notation. Let M := d(d + 1)/2. Define W = {ϕx, ∀x ∈X} ⊂RM where ϕx = vec(xx⊤)G and
G ∈Rd2×M is the duplication matrix such that G vech(xx⊤) = vec(xx⊤), with vech(·) denoting
the half-vectorization. For a set V, let PV := {λ ∈R|V| : P"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03431372549019608,"v∈V λv = 1, λv ≥0 ∀v} denote
distributions over V and Y(V) := {v′ −v : v, v′ ∈V, v ̸= v′} represent the differences between
members of set V."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03529411764705882,"General Approach. Given observations from yt = x⊤
t θ∗+ ηt for t ∈{1, 2, . . . , Γ} and the true
value of θ∗, we can estimate σ2
x for each x ∈X from the squared residuals η2
t = (yt −x⊤
t θ∗)2 (e.g.,
using method of moments or another estimating equation approach [15]). However, since θ∗is not
known, we cannot directly observe η2
t , making estimation of σ2
x more difficult. Consider a general
procedure that first uses Γ/2 samples from the budget to construct a least-squares estimator for θ∗,
bθΓ/2, and then uses the final Γ/2 samples to collect error estimates of the form bη2
t = (yt −x⊤
t bθΓ/2)2.
Define ΦΓ ∈RΓ/2×M as a matrix with rows {ϕxt}Γ
t=Γ/2+1 and bη ∈RΓ/2 as a vector with values
{bηt}Γ
t=Γ/2+1. With bη, we can estimate Σ∗by least squares as"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03627450980392157,"vech(bΣΓ) = arg min
s∈RM
∥ΦΓ s −bη2∥2,"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03725490196078431,"where we minimize over the space of symmetric matrices. Splitting samples ensures that the errors in
estimating θ∗and Σ∗are independent. Note that (bΣΓ, bθΓ/2) is an M-estimator of (Σ∗, θ∗) derived
from unbiased estimating functions, a common approach in heteroskedastic regression [21, 51, 34].
Finally, by plugging in the estimate of Σ∗, one can obtain an estimate of the variance, bσ2
x =
min{max{x⊤c
ΣΓx, σ2
min}, σ2
max}. We show that this approach leads to the following general error
decomposition for any x ∈X with z := ϕ⊤
x (Φ⊤
Γ ΦΓ)−1Φ⊤
Γ :"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.03823529411764706,"|x⊤(Σ∗−bΣΓ)x| ≤
n
x⊤(bθΓ/2 −θ∗)
o2"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.0392156862745098,"|
{z
}
A +  Γ
X"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.04019607843137255,"t=Γ/2
zt
 
η2
t −x⊤
t Σ∗xt


|
{z
}
B +  Γ
X"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.041176470588235294,"t=Γ/2
2ztx⊤
t (bθΓ/2 −θ∗)ηt"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.04215686274509804,"|
{z
}
C ."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.043137254901960784,"Experimental Design for Precise Estimation. The analysis above gives an error decomposition for
any phased non-adaptive data collection method that splits samples between estimation of θ∗and
Σ∗. Intuitively, we wish to sample {x1, . . . , xΓ/2} and {xΓ/2+1, . . . , xΓ} to control the maximum
absolute error by minimizing the expression above. We achieve this via two phases of G-optimal
experimental design [28]– first over the X space to estimate θ∗, and then over the W space to estimate
Σ∗. Precisely, we control quantity A via stage 1 of Alg. 1, which draws from a G-optimal design λ∗
to minimize the maximum predictive variance"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.04411764705882353,"max
x∈X E
n
x⊤(bθΓ/2 −θ∗)
o2
."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.045098039215686274,"Quantity B is another sub-exponential quantity (Definition B.2 in Appendix B), which we control by
drawing from a G-optimal design λ† over W to minimize maxϕ∈W ϕ⊤(Φ⊤
Γ ΦΓ)−1ϕ. Finally, C is
controlled by a combination of the guarantees associated with λ∗and λ†. This leads to the following
error bound on bσ2
x = min
n
max{x⊤c
ΣΓx, σ2
min}, σ2
max
o
."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.04607843137254902,"Theorem 3.1. Assume Γ = Ω

max

σ2
max log(|X|/δ)d2, d2	
. For any x ∈X and δ ∈(0, 1),
Alg. 1 guarantees the following:"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.047058823529411764,"P
 
|bσ2
x −σ2
x| ≤CΓ,δ

≥1 −δ/2
where
CΓ,δ = O
np"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.04803921568627451,"log(|X|/δ)σ2maxd2/Γ
o
."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.049019607843137254,"The proof of Theorem 3.1 is in Appendix D and follows from the decomposition in Eq. 3; treating A,
B and C as sub-exponential random variables with bounds that leverage the experimental designs
of Alg. 1. The details on the constants involved in Theorem 3.1 are also included in Appendix D."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.05,Algorithm 1: HEAD (Heteroskedasticity Estimation by Adaptive Designs)
HETEROSKEDASTIC VARIANCE ESTIMATION,0.050980392156862744,Result: Find bΣΓ
HETEROSKEDASTIC VARIANCE ESTIMATION,0.05196078431372549,"1 Input: Arms X ∈Rd, Γ ∈N"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.052941176470588235,"2 //Stage 1:
Take half the samples to estimate θ∗"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.05392156862745098,3 Determine λ∗according to λ∗= arg minλ∈PX maxx∈X x⊤P
HETEROSKEDASTIC VARIANCE ESTIMATION,0.054901960784313725,"x′∈X λx′x′x′⊤−1
x"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.05588235294117647,"4 Pull arm x ∈X ⌈λ∗
xΓ/2⌉times and collect observations {xt, yt}Γ/2
t=1"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.056862745098039215,"5 Define A∗= PΓ/2
t=1 xtx⊤
t and b∗= PΓ/2
t=1 xtyt and estimate bθΓ/2 = A∗−1b"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.05784313725490196,"6 //Stage 2:
Take half the samples to estimate Σ∗given bθΓ/2"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.058823529411764705,"7 Determine λ† ←λ†
x = arg minλ∈PX maxx∈X ϕ⊤
x
 P"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.059803921568627454,"x′∈X λx′ϕx′ϕ⊤
x′
−1 ϕx"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.060784313725490195,"8 Pull arm x ∈X ⌈λ†
xΓ/2⌉times8and collect observations {xt, yt}Γ
t=Γ/2+1"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.061764705882352944,"9 Let A† = PΓ
t=Γ/2+1 ϕxtϕ⊤
xt, b† = PΓ
t=Γ/2+1 ϕxt

yt −x⊤
t bθΓ/2
2"
HETEROSKEDASTIC VARIANCE ESTIMATION,0.06274509803921569,10 Output: vech(bΣΓ) = A†−1b†.
HETEROSKEDASTIC VARIANCE ESTIMATION,0.06372549019607843,"Note that we design to directly control the MAE of bσ2
x. This is in contrast to an inefficient alternative
procedure that allocates samples to minimize the error of bΣΓ in general, and then extends this bound
to x⊤bΣΓx for x ∈X."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.06470588235294118,"Theoretical Comparison of Variance Estimation Methods. In contrast to previous approaches
that naively scale in the size of X [14], the above result has a dimension dependence of d2, which
intuitively scales with the degrees of freedom of Σ∗. To highlight the tightness of this result, consider
estimating the noise parameters for each arm, x ∈X, only using the samples of x (like the method
in [14]). We improve this approach by adapting it to our heteroskedastic variance structure and
strategically pick d2 points to avoid the dependence on |X|. We refer to this method as the Separate
Arm Estimator, and in Appendix D, we show that it suffers a dependence of d4. This comparison
point, along with our empirical results below, suggest that the error bounds established in Theorem 3.1
scale favorably with the problem dimension."
HETEROSKEDASTIC VARIANCE ESTIMATION,0.06568627450980392,"Empirical Comparison of Variance Estimation Methods. Define two sets of arms X1, X2 such
that x ∈X1 is drawn uniformly from a unit sphere and x ∈X2 is drawn uniformly from a sphere
with radius 0.1; with |X1| = 200 and |X2| = 1800. Let Σ∗= diag(1, 0.1, 1, 0.1, . . .) ∈Rd×d, and
θ∗= (1, 1, . . . , 1) ∈Rd. This setting provides the heteroskedastic variation needed to illustrate the
advantages of using HEAD (Alg. 1). An optimal algorithm will tend to target orthogonal vectors in
X1 because these arms have higher magnitudes in informative directions. Defining X = X1 ∪X2,
we perform 32 simulations in which we randomly sample the arm set and construct estimators for
each σ2
x, x ∈X. We compare HEAD, the Separate Arm Estimator, and the Uniform Estimator (see
Alg. 3 in Appendix D). The Uniform Estimator is based on Alg. 1 but samples arms uniformly
from X and does not split samples between estimation of θ∗and Σ∗. HEAD should outperform its
competitors in two ways: 1) optimally allocating samples, and 2) efficiently sharing information
across arms in estimation. The Uniform Estimator exploits the problem structure for estimation but
not when sampling. In contrast, the Seperate Arm Estimator optimally samples, but does not use the
relationships between arms for estimation. Fig. 2 depicts the average MAE and standard errors for
each estimator. In Fig. 2a, we see that HEAD outperforms its competitors over a series of sample sizes
for d = 15. Fig. 2b compares the estimators over a range of dimensions for a sample size of 95k.
While HEAD continues to accurately estimate the heteroskedastic noise parameters at high dimensions,
the Separate Arm Estimator error scales poorly with dimension as the analysis suggests."
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.06666666666666667,"4
Adaptive Experimentation with Covariates in Heteroskedastic Settings"
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.06764705882352941,"As an application of the novel sampling procedure and resulting variance estimation bounds developed
in section 3, we now study adaptive experimentation with covariates in the presence of heteroskedastic"
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.06862745098039216,"8As λxΓ/2 /∈N, it is not possible in reality to take λxΓ/2 samples. This problem can easily be handled by
rounding procedures detailed in Appendix A."
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.0696078431372549,"(a)
(b)"
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.07058823529411765,"Figure 2: The proposed experimental design estimator outperforms competitors in terms of maximum absolute
error maxx∈X |σ2
x −bσ2
x| over a range of sample sizes (a) and dimensions (b)."
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.07156862745098039,"noise through the lens of transductive linear identification problems [12]. Recalling the problem
setting from Section 1.1, we consider the following fixed confidence identification objectives:"
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.07254901960784314,1. Best-Arm Identification (BAI). Identify the singleton set {z∗} where z∗= arg maxz∈Z z⊤θ∗.
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.07352941176470588,2. Level-Set Identification (LS). Identify the set Gα = {z : z⊤θ∗> α} given a threshold α ∈R.
ADAPTIVE EXPERIMENTATION WITH COVARIATES IN HETEROSKEDASTIC SETTINGS,0.07450980392156863,"This set of objectives, together with a learning protocol, allow us to capture a number of practical
problems of interest where heteroskedastic noise naturally arises in the assumed form. Consider
multivariate testing problems in e-commerce advertisement applications [17, 39]. The expected feed-
back is modeled through a linear combination of primary effects from content variation dimensions
(e.g., features or locations that partition the advertisement), and interaction effects between content
variation in pairs of dimensions. The noise structure is naturally heteroskedastic and dependent
on the combination of variation choices. Moreover, the transductive objectives allow for flexible
experimentation such as identifying the optimal combination of variations or the primary effects that
exceed some threshold. We return to this setting in Experiment 3 of Section 5. In the remainder of
this section, we characterize the optimal sample complexities for the heteroskedastic transductive
linear identification problems and design algorithms that nearly match these lower bounds."
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.07549019607843137,"4.1
Linear Estimation Methods with Heteroskedastic Noise"
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.07647058823529412,"To draw inferences in the experimentation problems of interest, it is necessary to consider estimation
methods for the unknown parameter vector θ∗∈Rd. Given a matrix of covariates XT ∈RT ×d, a
vector of observations YT ∈RT , and a user-defined weighting matrix WT = diag(w1, . . . , wT ) ∈
RT ×T , the weighted least squares (WLS) estimator is defined as:"
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.07745098039215687,"bθWLS =
 
X⊤
T WT XT
−1 X⊤
T WT YT = minθ∈Rd ∥YT −XT θ∥2
WT ."
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.0784313725490196,"Let ΣT = diag(σ2
x1, σ2
x2, . . . , σ2
xT ) ∈RT ×T denote the variance parameters of the noise observa-
tions. The WLS estimator is unbiased regardless of the weight selections, with variance"
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.07941176470588235,"V

bθWLS

=
 
X⊤
T WT XT
−1 X⊤
T WT ΣT W ⊤
T XT
 
X⊤
T WT XT
−1 WT →Σ−1
T
=
 
X⊤
T Σ−1
T XT
−1 .
(2)
The WLS estimator with WT = Σ−1
T
is the minimum variance linear unbiased estimator [2]. In
contrast, the ordinary least squares (OLS) estimator, obtained by taking WT = I in the WLS
estimator, is unbiased but with a higher variance,"
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.0803921568627451,"V

bθOLS

=
 
X⊤
T XT
−1 X⊤
T ΣT XT
 
X⊤
T XT
−1 ⪯σ2
max(X⊤
T XT )−1."
LINEAR ESTIMATION METHODS WITH HETEROSKEDASTIC NOISE,0.08137254901960785,"Consequently, WLS is a natural choice with heteroskedastic noise for sample efficient estimation."
OPTIMAL SAMPLE COMPLEXITIES,0.08235294117647059,"4.2
Optimal Sample Complexities"
OPTIMAL SAMPLE COMPLEXITIES,0.08333333333333333,"We now motivate the optimal sample complexity for any algorithm that returns the correct quantity
with a fixed probability greater than 1 −δ, δ ∈(0, 1), in transductive identification problems with"
OPTIMAL SAMPLE COMPLEXITIES,0.08431372549019608,"heteroskedastic noise. We seek to identify a set of vectors HOBJ, where"
OPTIMAL SAMPLE COMPLEXITIES,0.08529411764705883,"HBAI = {z∗} = {arg max
z∈Z z⊤θ∗} and HLS = {z ∈Z : z⊤θ∗−α > 0}."
OPTIMAL SAMPLE COMPLEXITIES,0.08627450980392157,"Algorithms consist of a sampling strategy and a rule to return a set b
H at time τ."
OPTIMAL SAMPLE COMPLEXITIES,0.08725490196078431,"Definition 4.1. An algorithm is δ-PAC for OBJ ∈{BAI, LS} if ∀θ ∈Θ, Pθ( b
H = HOBJ) ≥1 −δ."
OPTIMAL SAMPLE COMPLEXITIES,0.08823529411764706,"Throughout, we will only consider algorithms that are δ-PAC. For drawing parallels between best-arm
and level set identification, it will be useful to define a set of values QOBJ, where"
OPTIMAL SAMPLE COMPLEXITIES,0.0892156862745098,"QBAI = {z : z ∈Z, z ̸= z∗} and QLS = {0},"
OPTIMAL SAMPLE COMPLEXITIES,0.09019607843137255,"and a constant bOBJ such that bBAI = 0 and bLS = α. Both problems amount to verifying that for
all h ∈HOBJ and q ∈QOBJ, (h −q)⊤bθWLS > bOBJ, or equivalently (h −q)⊤θ∗−bOBJ −(h −
q)⊤(θ∗−bθWLS) > 0. Using a sub-Gaussian tail bound and a union bound [32], this verification
requires the following to hold for all h ∈HOBJ and q ∈QOBJ for the data collected:"
OPTIMAL SAMPLE COMPLEXITIES,0.09117647058823529,"|(h −q)⊤(θ∗−bθWLS)| ≤
r"
OPTIMAL SAMPLE COMPLEXITIES,0.09215686274509804,"2∥h −q∥2
V(bθWLS) log(2|Z|/δ) ≤(h −q)⊤θ∗−bOBJ.
(3)"
OPTIMAL SAMPLE COMPLEXITIES,0.09313725490196079,"Let λx denote the proportion of T samples given to a measurement vector x ∈X so that when
WT = Σ−1
T
in Eq. (4.1),"
OPTIMAL SAMPLE COMPLEXITIES,0.09411764705882353,"V

bθWLS

=
 
X⊤
T Σ−1
T XT
−1 = X"
OPTIMAL SAMPLE COMPLEXITIES,0.09509803921568627,"x∈X
λxσ−2
x xx⊤
!−1 /T"
OPTIMAL SAMPLE COMPLEXITIES,0.09607843137254903,"as in Eq. (2). Now, we reformulate the condition in Eq. (3) and minimize over designs λ ∈PX to see
that δ-PAC verification requires"
OPTIMAL SAMPLE COMPLEXITIES,0.09705882352941177,"T ≥2ψ∗
OBJ log(2|Z|/δ), where ψ∗
OBJ = min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
OPTIMAL SAMPLE COMPLEXITIES,0.09803921568627451,"||h −q||2
(
P"
OPTIMAL SAMPLE COMPLEXITIES,0.09901960784313725,"x∈X λxσ−2
x
xx⊤)
−1"
OPTIMAL SAMPLE COMPLEXITIES,0.1,"{(h −q)⊤θ∗−bOBJ}2
. (4)"
OPTIMAL SAMPLE COMPLEXITIES,0.10098039215686275,"This motivating analysis gives rise to the following sample complexity lower bound.
Theorem 4.1. Consider an objective, OBJ, of best-arm identification (BAI) or level-set identification
(LS). For any δ ∈(0, 1), a δ-PAC algorithm must satisfy"
OPTIMAL SAMPLE COMPLEXITIES,0.10196078431372549,"Eθ[τ] ≥2 log(1/2.4δ)ψ∗
OBJ."
OPTIMAL SAMPLE COMPLEXITIES,0.10294117647058823,"Remark 4.1. This lower bound assumes that the noise variance parameters are known. We compare
to the existing lower bounds for the identification objectives with homoskedastic noise [12, 44, 35]
in Appendix G. These apply in this setting by taking the variance parameter σ2 = σ2
max and are
characterized by an instance-dependent parameter, ρ∗
OBJ, such that κ−1 ≤ψ∗
OBJ/ρ∗
OBJ ≤1 (recall
that κ := σ2
max/σ2
min). In general, κ can be quite large in many problems with heteroskedastic noise.
Consequently, this implies that near-optimal algorithms for linear identification with homoskedastic
noise can be highly suboptimal (by up to a multiplicative factor of κ) in this setting."
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10392156862745099,"4.3
Adaptive Designs with Unknown Heteroskedastic Variance"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10490196078431373,"We now present Alg. 2, H-RAGE (Heteroskedastic Randomized Adaptive Gap Elimination), as a
solution for adaptive experimentation with covariates in heteroskedastic settings. The approach
is composed of two components: 1) Obtain accurate estimates of the unknown variance pa-
rameters {σ2
x}x∈X using HEAD; 2) Use adaptive experimental design methods for minimizing
the uncertainty in directions of interest given the estimated variance parameters.
Denote by
∆= minh∈HOBJ minq∈QOBJ(h −q)⊤θ∗−bOBJ the minimum gap for the objective and assume
maxz∈Zℓ|⟨z −z∗, θ∗⟩| ≤2. We prove the following guarantees for Alg. 2.
Theorem 4.2. Consider an objective, OBJ, of best-arm identification (BAI) or level-set identification
(LS). The set returned from Alg. 2 at a time τ is δ-PAC and"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10588235294117647,"τ = O
 
ψ∗
OBJ log(∆−1)

log(|Z|/δ) + log

log(∆−1)
	
+ log(∆−1)d2 + log(|X|/δ)κ2d2
."
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10686274509803921,Algorithm 2: (H-RAGE) Heteroskedastic Randomized Adaptive Gap Elimination
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10784313725490197,Result: Find z∗:= arg maxz∈Z z⊤θ∗for BAI or Gα := {z ∈Z : z⊤θ∗> α} for LS
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10882352941176471,"1 Input: X ∈Rd, Z ∈Rd, confidence δ ∈(0, 1), OBJ ∈{BAI,LS}, threshold α ∈R"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.10980392156862745,"2 Initialize: ℓ←1, Z1 ←Z, G1 ←∅, B1 ←∅"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11078431372549019,3 //Variance estimation
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11176470588235295,"4 Call Alg. 1 such that
bσ2
x = min
n
max{x⊤bΣΓx, σ2
min}, σ2
max
o
−σ2
x
 ≤σ2
x/2"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11274509803921569,5 while (|Zℓ| > 1 and OBJ=BAI) or (|Zℓ| > 0 and OBJ=LS) do
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11372549019607843,"6
//Determine the design"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11470588235294117,"7
Let bλℓ∈PX be a minimizer of q {λ, Y(Zℓ)} if OBJ=BAI and q(λ, Zℓ) if OBJ=LS where"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11568627450980393,"q(V) = infλ∈PX q(λ; V) = infλ∈PX maxz∈V ||z||2
(P"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11666666666666667,"x∈X bσ−2
x
λxxx⊤)−1"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11764705882352941,"8
Set ϵℓ= 2−ℓ, τℓ= 3ϵ−2
ℓq(Zℓ) log(8ℓ2|Z|/δ) //Determine stepsize"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11862745098039215,"9
Pull arm x ∈X exactly ⌈τℓbλℓ,x⌉times for nℓsamples and collect {xℓ,i, yℓ,i}nℓ
i=1"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.11960784313725491,"10
Define Aℓ:= Pnℓ
i=1 bσ−2
i
xℓ,ix⊤
ℓ,i, bℓ= Pnℓ
i=1 bσ−2
i
xℓ,iyℓ,i and construct bθℓ= A−1
ℓbℓ"
ADAPTIVE DESIGNS WITH UNKNOWN HETEROSKEDASTIC VARIANCE,0.12058823529411765,"11
//Eliminate arms"
IF OBJ IS BAI THEN,0.12156862745098039,"12
if OBJ is BAI then"
IF OBJ IS BAI THEN,0.12254901960784313,"13
Zℓ+1 ←Zℓ\{z ∈Zℓ: maxz′∈Zℓ⟨z′ −z, bθℓ⟩> ϵℓ}"
ELSE,0.12352941176470589,"14
else"
ELSE,0.12450980392156863,"15
Gℓ+1 ←Gℓ∪{z ∈Zℓ: ⟨z, bθℓ⟩−ϵℓ> α}"
ELSE,0.12549019607843137,"16
Bℓ+1 ←Bℓ∪{z ∈Zℓ: ⟨z, bθℓ⟩+ ϵℓ< α}"
ELSE,0.1264705882352941,"17
Zℓ+1 ←Zℓ\{{Gℓ\ Gℓ+1} ∪{Bℓ\ Bℓ+1}}"
END,0.12745098039215685,"18
end"
END,0.1284313725490196,"19
ℓ←ℓ+ 1"
END,0.12941176470588237,20 end
END,0.1303921568627451,21 Output: Zℓfor BAI or Gℓfor LS
END,0.13137254901960785,"Remark 4.2. This result shows that Alg. 2 is nearly instance-optimal. Critically, the impact of not
knowing the noise variances ahead of time only has an additive impact on the sample complexity
relative to the lower bound. In contrast, existing near-optimal methods for identification problems
with homoskedastic noise would be suboptimal by a multiplicative factor depending on κ relative
to the lower bound. Given that the lower bound assumes the variances are known, an interesting
question for future work is a tighter lower bound assuming the variance parameters are unknown."
END,0.1323529411764706,"Algorithm Overview. Observe that for ψ∗
OBJ, the optimal allocation depends on both the unknown
gaps ((h−q)⊤θ∗−bOBJ) for h ∈HOBJ and q ∈QOBJ, and the unknown noise variance parameters
{σ2
x}x∈X . To handle this, the algorithm begins with an initial burn-in phase using the procedure from
Alg. 1 to produce estimates {bσ2
x}x∈X of the unknown parameters {σ2
x}x∈X , achieving a multiplicative
error bound of the form |σ2
x −bσ2
x| ≤σ2
x/2 for all x ∈X. From this point, the estimates {bσ2
x}x∈X
are used as plug-ins to the experimental designs and the weighted least squares estimates. In each
round ℓ∈N of the procedure, we maintain a set of uncertain items Zℓ, and obtain the sampling
distribution bλℓ, by minimizing the maximum predictive variance of the WLS estimator over all
directions y = h −q for h ∈HOBJ and q ∈QOBJ. This is known as an XY-allocation and
G-optimal-allocation in the best-arm and level set identification problems respectively. Critically, the
number of samples taken in round ℓguarantees that the error in all estimates is bounded by ϵℓ, such
that for any h ∈HOBJ and q ∈QOBJ, (h −q)⊤θ∗> 2ϵℓis eliminated from the active set at the end
of the round. By progressively honing in on the optimal item set HOBJ, the sampling allocation gets
closer to approximating the oracle allocation."
EXPERIMENTS,0.13333333333333333,"5
Experiments"
EXPERIMENTS,0.13431372549019607,"We now present experiments focusing on the transductive linear bandit setting. We compare H-RAGE
and its corresponding oracle to their homoskedastic counterparts, RAGE [12] and the oracle allocation"
EXPERIMENTS,0.13529411764705881,"with noise equal to σ2
max. RAGE is provably near-optimal in the homoskedastic setting and known
to perform well empirically. The algorithm is similar in concept to Alg. 2, but lacks the initial
exploratory phase to estimate the variance parameters. We include the pseudo-code of RAGE in
Appendix H. The homoskedastic and heteroskedastic oracles sample with respect to the optimal
design with a sub-Gaussian interval stopping condition. All algorithms are run at a confidence level
of δ = 0.05, and we use the Franke-Wolfe method to compute the designs. We demonstrate that
H-RAGE has superior empirical performance over a range of settings."
EXPERIMENTS,0.13627450980392156,"Linear bandits: Experiment 1 (Signal-to-Noise). We begin by presenting an experiment that
is a variation of the standard benchmark problem for BAI in linear bandits [46] introduced in
Sec. 1.2. Define ei ∈Rd as the standard basis vectors in d dimensions. In the standard example,
X = Z = {e1, e2, x′} where x′ = {e1 cos(ω) + e2 sin(ω)} for ω →0, and θ∗= e1. Thus x1 = e1
is optimal, x3 = x′ is near-optimal, and x2 = e2 is informative to discriminate between x1 and x3.
We consider an extension of this benchmark in multiple dimensions that highlights the performance
gains from accounting for heteroskedastic variance. First, define an arm set H that is comprised of
the standard example but with varied magnitudes. For q ∈(0, 1),"
EXPERIMENTS,0.13725490196078433,"H = {e1} ∪{e2} ∪{q × ei}d
i=3 ∪{e1 cos(ω) + ei sin(ω)}d
i=2."
EXPERIMENTS,0.13823529411764707,"Then define G to be a series of vectors such that the rank of span
 
{ϕx : x ∈G ∪H}

= RM. G
allows for variance estimation. Finally, let the arm and item set be X = Z = G ∪H and θ∗= e1."
EXPERIMENTS,0.1392156862745098,"We define Σ∗as the d-dimensional identity matrix. Observe that x1 = e1 is optimal, the d −1
arms given by {e1 cos(ω) + ei sin(ω)}d
i=2 are near-optimal with equal gaps ∆= 1 −cos(ω), and
the d −1 arms given by {e2} ∪q ∗{ei}d
i=3 are informative to sample for discriminating between
x1 = e1 and xi = e1 cos(ω) + ei sin(ω) for each i ∈{2, . . . , d}, respectively. To identify x1, we
must estimate the ∆gap adequately in every dimension with arms {e2} ∪q ∗{ei}d
i=3. Therefore,
performing BAI without accounting for heteroskedastic variances will tend toward a design that
prioritizes sampling informative arms {q × ei}d
i=3, since these have smaller magnitudes and seem
less informative about the near-optimal arms in dimensions i ∈{3, 4 . . . d}. However, the signal
to noise ratio is actually constant across arms (because the variance of q ∗{ei}d
i=3 is equal to q2)
leading to an oracle distribution that equally samples across {e2} ∪q ∗{ei}d
i=3 when we account for
heteroskedastic variances. This change in allocation is illustrated in Appendix H for d = 4, ω = 0.01,
q = 0.4 and G =

0.5(e1 + e2) + 0.1e3, 0.5(e1 + e2) + 0.1(e3 + e4)
	
. In the same setting, Fig. 3a
shows that the heteroskedastic variance experimental design and the tighter predictive confidence
intervals of the weighted least squares estimator result in a large decrease in the sample size needed
to identify the best arm."
EXPERIMENTS,0.14019607843137255,"Experiment 2: Benchmark. This experiment is also similar to the benchmark example introduced in
Sec. 1.2. In this variation, the informative arms have the same magnitude but are bent at a 45◦angle,"
EXPERIMENTS,0.1411764705882353,"X = {e1} ∪{e1 cos(ω) + e2 sin(ω)} ∪{ei}d
i=3 ∪

{ei/
√"
EXPERIMENTS,0.14215686274509803,"2 + ej/
√"
EXPERIMENTS,0.14313725490196078,"2}d
i=1}d
j>i."
EXPERIMENTS,0.14411764705882352,"Let the arm and item set be X = Z and θ∗= e1. In Experiment 1, we have many near-optimal
arms and H-RAGE reveals that each of these are equally difficult to distinguish from the best. In
contrast, Experiment 2 contains one near-optimal arm along with many potentially informative arms,
and we are interested in identifying the one that is most helpful. Intuitively, Experiment 1 uses
heteroskedastic variance estimates to assess the difficulty of many problems, while Experiment 2
uses the same information to assess the benefits of many solutions. Let the unknown noise matrix
be given by Σ∗= diag(σ2
1, σ2
2, . . . , σ2
d) where σ2
1 = α2, (σ2
2, σ2
3) = β2 and (σ2
4, σ2
5 . . . , σ2
d) = α2.
x1 = e1 is again optimal and x2 = {e1 cos(ω) + e2 sin(ω)} is a near optimal arm. In this case,
{e2/
√"
EXPERIMENTS,0.1450980392156863,"2 + e1/
√"
EXPERIMENTS,0.14607843137254903,"2} ∪{e2/
√"
EXPERIMENTS,0.14705882352941177,"2 + ei/
√"
EXPERIMENTS,0.1480392156862745,"2}d
i=3 are informative for distinguishing between x1 and x2,
and the oracle allocation assuming homoskedastic noise (Homoskedastic Oracle) picks three of these
vectors to sample. However, if α2 ≫β2, then it is optimal to sample {e2/
√"
EXPERIMENTS,0.14901960784313725,"2 + e3/
√"
EXPERIMENTS,0.15,"2, e3} over
other potential informative arm combinations. Appendix H shows this contrast in allocations for
d = 3, ω = 0.01, α2 = 1 and β2 = 0.2. In the same setting, Fig. 3b shows that estimating and
accounting for heteroskedastic noise contributes to a reduction in sample complexity."
EXPERIMENTS,0.15098039215686274,"Experiment 3: Multivariate Testing. In this experiment, we return to the motivating example
of Section 4, multivariate testing in e-commerce advertising [17, 39]. We divide an advertisement
into natural locations or features called dimensions, each of which has different content options or
variations. We induce heteroskedastic variance by allowing both the expected value and the variance"
EXPERIMENTS,0.15196078431372548,"(a) Experiment 1
(b) Experiment 2
(c) Experiment 3"
EXPERIMENTS,0.15294117647058825,"Figure 3: Experimental results show the mean sample complexity of each algorithm over 100 simulations with
95% Monte Carlo confidence intervals."
EXPERIMENTS,0.153921568627451,"of an advertisement’s reward to depend on the n variations in each of the m dimensions. Let X = Z
denote the set of layouts corresponding to the combinations of variant choices in the dimensions so
that |X| = nm. The multivariate testing problem is often modeled in the linear bandit framework
using the expected feedback for any layout x ∈X ⊂{0, 1}d, where d = 1 + mn + n2m(m + 1)/2,
given by"
EXPERIMENTS,0.15490196078431373,"x⊤θ∗:= θ∗
0 +
  Pm
i=1
Pn
j=1 θ∗
i,jxi,j

+
  Pm
i=1
Pm
i′=i+1
Pn
j=1
Pn
j′=1 θ∗
i,i′,j,j′xi,jxi′,j′
."
EXPERIMENTS,0.15588235294117647,"In this model, xi,j ∈{0, 1} is an indicator for variation j ∈{1, . . . , n} being placed in dimension
i ∈{1, . . . , m}, and Pn
j=1 xi,j = 1 for all i ∈{1, . . . , n} for any layout x ∈X. Observe that the
expected feedback for a layout is modeled as a linear combination of a common bias term, primary
effects from the variation choices within each dimension, and secondary interaction effects from the
combinations of variation choices between dimensions."
EXPERIMENTS,0.1568627450980392,"We consider an environment where the effect of variation changes in one dimension, call it di-
mension j, has much higher variance than others; resulting in the best variation for dimension j
being harder to identify. An algorithm that accounts for heteroskedastic variance will devote a
greater number of samples to compare variations in dimension j, whereas an algorithm that up-
per bounds the variance will split samples equally between dimensions. For a simulation setting
with 2 variations in 3 dimensions, we define Σ∗= diag(0.3, 0.7, 10−3, 10−3, . . .) ∈R7×7 and
θ∗= (0, 0.005, 0.0075, 0.01, −0.1, −0.1, . . .) ∈R7. This simulation setting implies that the second
variation in each of the dimensions is better than the first, but the positive effect in the first dimension
is hardest to identify. In Appendix H we can see that this causes the heteroskedastic oracle to devote
more weight than the homoskedastic oracle to vectors that include the second variation in the first
dimension. Fig. 3c depicts the improvement in sample complexity resulting from accounting for the
heteroskedastic variances."
CONCLUSION,0.15784313725490196,"6
Conclusion"
CONCLUSION,0.1588235294117647,"This paper presents an investigation of online linear experimental design problems with heteroskedas-
tic noise. We propose a two-phase sample splitting procedure for estimating the unknown noise
variance parameters based on G-optimal experimental designs and show error bounds that scale
efficiently with the dimension of the problem. The proposed approach is then applied to fixed
confidence transductive best-arm and level set identification with heteroskedastic noise, where we
present instance-dependent lower bounds with provably near-optimal algorithms."
CONCLUSION,0.15980392156862744,"This paper leads to a number of interesting future research directions. For example, there may be
situations where the noise structure is not fully characterized by Σ∗. Future work can explore the
development of methods that handle more general noise structures (including correlated or heavy-
tailed noise) while remaining near-optimal. Furthermore, it is possible that an integrated approach to
variance estimation and adaptive experimentation with covariates would be more sample efficient than
our two-step method. Lastly, it would be interesting to extend our algorithm to nonlinear response
models."
REFERENCES,0.1607843137254902,References
REFERENCES,0.16176470588235295,"[1] Marc Abeille, Louis Faury, and Clément Calauzènes. Instance-wise minimax-optimal algorithms
for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages
3691–3699. PMLR, 2021.
[2] Alexander C Aitken. IV.— On least squares and linear combination of observations. Proceedings
of the Royal Society of Edinburgh, 55:42–48, 1936.
[3] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimiza-
tion for experimental design: A regret minimization approach. Mathematical Programming,
186:439–478, 2021.
[4] András Antos, Varun Grover, and Csaba Szepesvári. Active learning in heteroscedastic noise.
Theoretical Computer Science, 411(29-30):2712–2728, 2010.
[5] David Blitz and Pim Van Vliet. The volatility effect: Lower risk without lower return. Journal
of portfolio management, pages 102–113, 2007.
[6] Romain Camilleri, Kevin Jamieson, and Julian Katz-Samuels. High-dimensional experimental
design and kernel bandits. In International Conference on Machine Learning, pages 1227–1237.
PMLR, 2021.
[7] Kamalika Chaudhuri, Prateek Jain, and Nagarajan Natarajan. Active heteroscedastic regression.
In International Conference on Machine Learning, pages 694–702. PMLR, 2017.
[8] Wesley Cowan, Junya Honda, and Michael N Katehakis. Normal bandits of unknown means
and variances. Journal of Machine Learning Research, 18(154):1–28, 2018.
[9] Rémy Degenne, Pierre Ménard, Xuedong Shang, and Michal Valko. Gamification of pure
exploration for linear bandits. In International Conference on Machine Learning, pages 2432–
2442. PMLR, 2020.
[10] Bianca Dumitrascu, Gregory Darnell, Julien Ayroles, and Barbara E Engelhardt. Statistical tests
for detecting variance effects in quantitative trait studies. Bioinformatics, 35(2):200–210, 2019.
[11] Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic
algorithms for logistic bandits. In International Conference on Machine Learning, pages
3052–3060. PMLR, 2020.
[12] Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design
for transductive linear bandits. Advances in neural information processing systems, 32:10667—
-10677, 2019.
[13] Tanner Fiez, Sergio Gamez, Arick Chen, Houssam Nassif, and Lalit Jain. Adaptive experimental
design and counterfactual inference. In Workshops of Conference on Recommender Systems,
2022.
[14] Xavier Fontaine, Pierre Perrault, Michal Valko, and Vianney Perchet. Online a-optimal design
and active linear regression. In International Conference on Machine Learning, pages 3374–
3383. PMLR, 2021.
[15] VP Godambe and ME Thompson. Some aspects of the theory of estimating equations. Journal
of Statistical Planning and Inference, 2(1):95–104, 1978.
[16] William H Greene. Econometric analysis. Pearson Education India, 2003.
[17] Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. An efficient bandit
algorithm for realtime multivariate optimization. In International Conference on Knowledge
Discovery and Data Mining, pages 1813–1821, 2017.
[18] Matthew Hoffman, Bobak Shahriari, and Nando Freitas. On correlation and budget constraints
in model-based bandit optimization with application to automatic machine learning. In Artificial
Intelligence and Statistics, pages 365–374. PMLR, 2014.
[19] Jean Honorio and Tommi Jaakkola. Tight bounds for the expected risk of linear classifiers and
PAC-Bayes finite-sample guarantees. In Artificial Intelligence and Statistics, pages 384–392.
PMLR, 2014.
[20] Francis L Huang, Wolfgang Wiedermann, and Bixi Zhang. Accounting for heteroskedastic-
ity resulting from between-group differences in multilevel models. Multivariate Behavioral
Research, pages 1–21, 2022."
REFERENCES,0.1627450980392157,"[21] Peter J Huber. The behavior of maximum likelihood estimates under nonstandard conditions.
In Berkeley Symposium on Mathematical Statistics and Probability, pages 221–233. University
of California Press, 1967.
[22] Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. Ad-
vances in Neural Information Processing Systems, 33:10007–10017, 2020.
[23] Kwang-Sung Jun, Lalit Jain, Blake Mason, and Houssam Nassif. Improved confidence bounds
for the linear logistic model and applications to bandits. In International Conference on Machine
Learning, pages 5148–5157. PMLR, 2021.
[24] Maurits Kaptein and Petri Parvinen. Advancing e-commerce personalization: Process frame-
work and case study. International Journal of Electronic Commerce, 19(3):7–33, 2015.
[25] Zohar S Karnin. Verification based solution for structured mab problems. Advances in Neural
Information Processing Systems, 29, 2016.
[26] Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the
union bound: Practical algorithms for combinatorial and linear bandits. Advances in Neural
Information Processing Systems, 33:10371–10382, 2020.
[27] Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the complexity of best-arm
identification in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):
1–42, 2016.
[28] Jack Kiefer. Optimum experimental designs. Journal of the Royal Statistical Society: Series B
(Methodological), 21(2):272–304, 1959.
[29] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian
Journal of Mathematics, 12:363–366, 1960.
[30] Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with
heteroscedastic noise. In Conference On Learning Theory, pages 358–384. PMLR, 2018.
[31] Fanjie Kong, Yuan Li, Houssam Nassif, Tanner Fiez, Shreya Chakrabarti, and Ricardo Henao.
Neural insights for digital marketing content design. In International Conference on Knowledge
Discovery and Data Mining, pages 4320–4332, 2023.
[32] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
[33] Zhaoqi Li, Lillian Ratliff, Houssam Nassif, Kevin Jamieson, and Lalit Jain. Instance-optimal
pac algorithms for contextual bandits. In Conference on Neural Information Processing Systems,
pages 37590–37603, 2022.
[34] James G MacKinnon and Halbert White. Some heteroskedasticity-consistent covariance matrix
estimators with improved finite sample properties. Journal of econometrics, 29(3):305–325,
1985.
[35] Blake Mason, Lalit Jain, Subhojyoti Mukherjee, Romain Camilleri, Kevin Jamieson, and Robert
Nowak. Nearly optimal algorithms for level set estimation. In International Conference on
Artificial Intelligence and Statistics, pages 7625–7658. PMLR, 2022.
[36] Blake Mason, Kwang-Sung Jun, and Lalit Jain. An experimental design approach for regret
minimization in logistic bandits. In AAAI Conference on Artificial Intelligence, volume 36,
pages 7736–7743, 2022.
[37] Darcy P Mays and Raymond H Myers. Design and analysis for a two-level factorial experiment
in the presence of variance heterogeneity. Computational statistics & data analysis, 26(2):
219–233, 1997.
[38] Subhojyoti Mukherjee, Qiaomin Xie, Josiah Hanna, and Robert Nowak. SPEED: Experimental
design for policy evaluation in linear heteroscedastic bandits. arXiv preprint arXiv:2301.12357,
2023.
[39] Sareh Nabi, Houssam Nassif, Joseph Hong, Hamed Mamani, and Guido Imbens. Bayesian
meta-prior learning using Empirical Bayes. Management Science, 68(3):1737–1755, 2022.
[40] H Neudecker. Symmetry, 0-1 matrices and jacobians. Econometric Theory, 2:157–190, 1986.
[41] Ann L Oberg and Douglas W Mahoney. Linear mixed effects models. Topics in biostatistics,
pages 213–234, 2007.
[42] Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006."
REFERENCES,0.16372549019607843,"[43] Takaki Sato and Yasumasa Matsuda. Spatial extension of generalized autoregressive conditional
heteroskedasticity models. Spatial Economic Analysis, 16(2):148–160, 2021.
[44] Marta Soare. Sequential resource allocation in linear stochastic bandits. PhD thesis, Université
Lille 1-Sciences et Technologies, 2015.
[45] Marta Soare, Alessandro Lazaric, and Rémi Munos. Active learning in linear stochastic bandits.
Bayesian Optimization in Theory and Practice, 2013.
[46] Marta Soare, Alessandro Lazaric, and Rémi Munos. Best-arm identification in linear bandits.
In Advances in Neural Information Processing Systems, 2014.
[47] Chao Tao, Saúl Blanco, and Yuan Zhou. Best arm identification in linear bandits with linear
dimension dependency. In International Conference on Machine Learning, pages 4877–4886.
PMLR, 2018.
[48] Bryan W Ting, Fred A Wright, and Yi-Hui Zhou. Simultaneous modeling of multivariate het-
erogeneous responses and heteroskedasticity via a two-stage composite likelihood. Biometrical
Journal, page 2200029, 2023.
[49] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge university press, 2019.
[50] Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via Frank-Wolfe.
Advances in Neural Information Processing Systems, 34:5810–5821, 2021.
[51] Halbert White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for
heteroskedasticity. Econometrica: journal of the Econometric Society, pages 817–838, 1980.
[52] Douglas P Wiens. Designs for weighted least squares regression, with estimated weights.
Statistics and Computing, 23:391–401, 2013.
[53] Douglas P Wiens and Pengfei Li. V-optimal designs for heteroscedastic regression. Journal of
Statistical Planning and Inference, 145:125–138, 2014.
[54] Weng Kee Wong and R Dennis Cook. Heteroscedastic g-optimal designs. Journal of the Royal
Statistical Society: Series B (Methodological), 55(4):871–880, 1993.
[55] Weng Kee Wong and Wei Zhu. Optimum treatment allocation rules under a variance hetero-
geneity model. Statistics in Medicine, 27(22):4581–4595, 2008.
[56] Jeffrey M Wooldridge. Econometric analysis of cross section and panel data. MIT press, 2010.
[57] Liyuan Xu, Junya Honda, and Masashi Sugiyama. A fully adaptive algorithm for pure explo-
ration in linear bandits. In International Conference on Artificial Intelligence and Statistics,
pages 843–851. PMLR, 2018.
[58] Dan Yan, Xiaohang Ren, Wanli Zhang, Yiying Li, and Yang Miao. Exploring the real con-
tribution of socioeconomic variation to urban PM2.5 pollution: New evidence from spatial
heteroscedasticity. Science of the Total Environment, 806:150929, 2022.
[59] Junwen Yang and Vincent Tan. Minimax optimal fixed-budget best arm identification in linear
bandits. Advances in Neural Information Processing Systems, 35:12253–12266, 2022.
[60] Mohammadi Zaki, Avinash Mohan, and Aditya Gopalan. Towards optimal and efficient best
arm identification in linear bandits. arXiv preprint arXiv:1911.01695, 2019.
[61] Huiming Zhang and Song Xi Chen. Concentration inequalities for statistical inference. Commu-
nications in Mathematical Research, 37(1):1–85, 2021.
[62] Heyang Zhao, Dongruo Zhou, Jiafan He, and Quanquan Gu. Bandit learning with general
function classes: Heteroscedastic noise and variance-dependent regret bounds. arXiv preprint
arXiv:2202.13603, 2022."
REFERENCES,0.16470588235294117,Contents
REFERENCES,0.16568627450980392,"Appendices
14"
REFERENCES,0.16666666666666666,"A Background on Experimental Designs and Rounding
14"
REFERENCES,0.1676470588235294,"B Technical Preliminaries
15"
REFERENCES,0.16862745098039217,"C Baseline Variance Estimation Procedures
16"
REFERENCES,0.1696078431372549,"D Proofs of Variance Estimation
17"
REFERENCES,0.17058823529411765,"D.1 Analysis of the Separate Arm Estimator . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.1715686274509804,"D.2 Analysis of the HEAD Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.17254901960784313,"E
Proofs of Best-Arm Identification
25"
REFERENCES,0.17352941176470588,"E.1
Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.17450980392156862,"E.2
Upper Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.17549019607843136,"E.2.1
Proof of Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.17647058823529413,"E.2.2
Proof of Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . .
28"
REFERENCES,0.17745098039215687,"F
Proofs of Level Set Identification
33"
REFERENCES,0.1784313725490196,"F.1
Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33"
REFERENCES,0.17941176470588235,"F.2
Upper Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34"
REFERENCES,0.1803921568627451,"F.2.1
Proof of Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34"
REFERENCES,0.18137254901960784,"F.2.2
Proof of Upper Bound on Sample Complexity . . . . . . . . . . . . . . . .
35"
REFERENCES,0.18235294117647058,"G Comparing Identification Lower Bounds
37"
REFERENCES,0.18333333333333332,"H Experiment Details
38"
REFERENCES,0.1843137254901961,"H.1
Oracle Designs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38"
REFERENCES,0.18529411764705883,"H.2 Assuming Homosekedastic Noise
. . . . . . . . . . . . . . . . . . . . . . . . . .
38"
REFERENCES,0.18627450980392157,"A
Background on Experimental Designs and Rounding"
REFERENCES,0.18725490196078431,"We use optimal experiment design [29, 42] to minimize uncertainty in the estimator of interest. Con-
sider the WLS estimator with the weight matrix WT = Σ−1
T
so that V(bθWLS) = (X⊤
T Σ−1
T XT )−1 =
(P⊤
t=1 σ−2
xt xtx⊤
t )−1. Let λx denote the proportion of the total samples T given to the measurement
vector x ∈X so that V(bθWLS) = (P"
REFERENCES,0.18823529411764706,"x∈X λxσ−2
x xx⊤)−1/T. We wish to take T samples so as
to construct the design P"
REFERENCES,0.1892156862745098,"x∈X Tλxσ−2
x xx⊤, however Tλx is often not an integer. As has been
discussed at depth [46, 12, 35, 42, 3], efficient rounding schemes can solve this problem. Given some
constant tolerance threshold ϵ, a design λ ∈PX , and a minimum sample size r(ϵ), efficient rounding
procedures return a fixed allocation {xt}T
t=1 that yields a (1 + ϵ) approximation to the intended
design. A simple, well known procedure with r(ϵ) ≤{d(d + 1) + 2} /ϵ comes from Pukelsheim
[42], while the scheme of Allen-Zhu et al. [3] yields r(ϵ) ≤O(d/ϵ2). In our experimental design
algorithms, we leverage the aforementioned rounding schemes."
REFERENCES,0.19019607843137254,"We adapt the guarantees of [3] for Alg. 1 as an example in the following proposition. For sample
size N, define X ∗as a set composed of the elements of X ⊂Rd duplicated N times and SN =
n
s ∈{0, 1, . . . , N}N|X| : PN|X|
t=1 si ≤N
o
as the set of discrete sample allocations."
REFERENCES,0.19117647058823528,"Proposition A.1. Suppose ϵ ∈(0, 1/3] and N ≥5d/ϵ2. Let λ ∈PX , then in polynomial-time (in n
and d), we can round λN to an integral solution bλ ∈SN satisfying"
REFERENCES,0.19215686274509805,"max
x∈X x⊤
 X"
REFERENCES,0.1931372549019608,"x′∈X ∗
bλx′x′x′⊤
!−1"
REFERENCES,0.19411764705882353,"x ≤(1 + 6ϵ) min
λ∈PX max
x∈X x⊤
 X"
REFERENCES,0.19509803921568628,"x′∈X
Nλx′x′x′⊤
!−1 x."
REFERENCES,0.19607843137254902,"Proof. This
follows
directly
from
Theorem
2.1
in
[3]
letting
CN
=
n
s ∈[0, N]N|X| : PN|X|
i=1 si ≤N
o
be a continuous relaxation of SN and setting k
=
N"
REFERENCES,0.19705882352941176,"and n = N|X|.
■"
REFERENCES,0.1980392156862745,"Alternatively, one could use the robust inverse propensity score (RIPS) estimator [6], which avoids
the need for rounding schemes via robust mean estimation."
REFERENCES,0.19901960784313724,"B
Technical Preliminaries"
REFERENCES,0.2,"For our analysis, we do not assume that the error, η, in Eq. 1 is Gaussian. We generalize by extending
to strictly sub-Gaussian noise, in which the variance σ2
x is equal to the optimal variance proxy for
ηt, ∀t ∈N. This paradigm allows for noise distributions such as the symmetrized Beta, symmetrized
Gamma, and Uniform."
REFERENCES,0.20098039215686275,"The following definitions and propositions will be used in the proofs of Appendices D-F.
Definition B.1. A real-valued random variable X is sub-Gaussian if there exists a positive constant
σ2 such that
E
 
eλX
≤eλ2σ2/2"
REFERENCES,0.2019607843137255,"for all λ ≥0.
Proposition B.1 (Equation 2.9, [49]). Let X be a sub-Gaussian random variable with sub-Gaussian
parameter σ2 and mean E(X) = µ. Then, for all t > 0, we have"
REFERENCES,0.20294117647058824,P (|X −µ| ≥t) ≤2e−t2 2σ2 .
REFERENCES,0.20392156862745098,"Definition B.2. A real-valued random variable X with mean E(X) = µ is sub-exponential if there
are non-negative parameters (ν, α) such that"
REFERENCES,0.20490196078431372,"E
n
eλ(X−µ)o
≤e
ν2λ2"
REFERENCES,0.20588235294117646,"2 , ∀|λ| < 1 α."
REFERENCES,0.2068627450980392,"Proposition B.2 (Proposition 2.9, [49]). Suppose that X is sub-exponential with parameters (ν, α)
and mean E(X) = µ. Then, for t > 0,"
REFERENCES,0.20784313725490197,"P (X −µ ≥t) ≤ (
e −t2"
REFERENCES,0.2088235294117647,"ν2 ,
if 0 ≤t ≤ν2 α
e−t"
REFERENCES,0.20980392156862746,"α ,
if t > ν2 α ."
REFERENCES,0.2107843137254902,"This implies that for δ ∈(0, 1),"
REFERENCES,0.21176470588235294,"P
h
|X −µ| < max
np"
REFERENCES,0.21274509803921568,"2 log(2/δ)ν2, 2 log(2/δ)α
oi
= 1 −δ."
REFERENCES,0.21372549019607842,"Proposition B.3 (Equation 37 in Appendix B, [19]). Let X be a sub-Gaussian random variable with
sub-Gaussian parameter σ2. Then X2 is sub-exponential with parameters (ν = 4σ2√"
REFERENCES,0.21470588235294116,"2, α = 4σ2).
Proposition B.4 (Equation 2.18, [49]). Suppose that Xi for i ∈{1, 2, . . . , n} are sub-exponential
with parameters (νi, αi) such that E(Xi) = µi. Then, Pn
i=1(Xi −µi) is sub-exponential with
parameters (ν∗, α∗) such that"
REFERENCES,0.21568627450980393,"α∗=
max
i=1,2,...,n αi and ν∗:="
REFERENCES,0.21666666666666667,"v
u
u
t n
X"
REFERENCES,0.21764705882352942,"i=1
ν2
i ."
REFERENCES,0.21862745098039216,"Definition B.3. A G-optimal design [28], λ∗∈PX , for a set of arms X ⊂Rd is such that"
REFERENCES,0.2196078431372549,"λ∗= arg min
λ∈PX max
x∈X x⊤
 X"
REFERENCES,0.22058823529411764,"x∈X
λxxx⊤
!−1 x."
REFERENCES,0.22156862745098038,"Proposition B.5 (Lemma 4.6, [28]). For any finite X ⊂Rd with span(X) = Rd,"
REFERENCES,0.22254901960784312,"d = min
λ∈PX max
x∈X x⊤
 X"
REFERENCES,0.2235294117647059,"x∈X
λxxx⊤
!−1 x."
REFERENCES,0.22450980392156863,"This implies that if we sample each arm x ∈X ⌈τλ∗
x⌉times, then"
REFERENCES,0.22549019607843138,"max
x∈X x⊤
 X"
REFERENCES,0.22647058823529412,"x∈X
⌈τλ∗
x⌉xx⊤
!−1 x ≤1"
REFERENCES,0.22745098039215686,"τ max
x∈X x⊤
 X"
REFERENCES,0.2284313725490196,"x∈X
λ∗
xxx⊤
!−1 x = d τ ."
REFERENCES,0.22941176470588234,Definition B.4. The Forbenius norm of A ∈Rn×m is
REFERENCES,0.23039215686274508,"∥A∥F =
q"
REFERENCES,0.23137254901960785,tr(A⊤A).
REFERENCES,0.2323529411764706,Definition B.5. The Spectral norm of A ∈Rn×m is
REFERENCES,0.23333333333333334,"∥A∥2 = sup
x̸=0 ∥Ax∥2"
REFERENCES,0.23431372549019608,"∥x∥2
."
REFERENCES,0.23529411764705882,"Proposition B.6 (Matrix Norm Properties). For A ∈Rn×m, we have sub-multiplicativity for the
Frobenius norm:
∥A⊤A∥F ≤∥A∥2
F ,
and a bound on the Spectral norm:
∥A∥2 ≤∥A∥F .
Proposition B.7 (Corollary 4.7, [61]). Let ξ1, . . . , ξn be zero-mean σ2-sub-Gaussian and ∆∈Rn×n.
For any t > 0,
P
h
ξ⊤∆ξ ≥σ2 n
tr(∆) + 2tr(∆2t)1/2 + 2∥∆∥2t
oi
≤e−t,"
REFERENCES,0.23627450980392156,"which implies that with probability 1 −δ for δ ∈(0, 1),"
REFERENCES,0.2372549019607843,"ξ⊤Σξ ≤σ2 h
tr(∆) + 2tr

∆2 log(1/δ)
	1/2 + 2∥∆∥2 log(1/δ)
i
."
REFERENCES,0.23823529411764705,"Proposition B.8 (Equation 20.3, [32]). Assume the setup of Eq. 1 and that a data set of size Γ,
D = {xt, yt}Γ
t=1, has been collected through a fixed design. Define VΓ = PΓ
t=1 xtx⊤
t and construct"
REFERENCES,0.23921568627450981,"the least squares estimator for D, bθΓ = V −1
Γ
PΓ
t=1 xtyt

. Then, with probability 1 −δ for"
REFERENCES,0.24019607843137256,"δ ∈(0, 1),
∥bθΓ −θ∗∥VΓ ≤2
p"
REFERENCES,0.2411764705882353,2 {d log(6) + log(1/δ)}.
REFERENCES,0.24215686274509804,"C
Baseline Variance Estimation Procedures"
REFERENCES,0.24313725490196078,These are the algorithms we reference in the theoretical and empirical comparisons of Section 3.
REFERENCES,0.24411764705882352,"The Uniform Estimator. The Uniform Estimator draws arms uniformly at random and uses all
samples to construct estimators bθΓ of θ∗and bΣΓ of Σ∗."
REFERENCES,0.24509803921568626,"The Separate Arm Estimator. Define U as the set of subsets of W of size M = d(d + 1)/2, and
for U ∈U construct ΦU such that its rows are composed of ϕx ∈U. Let ζmin(A) be the minimum
singular value of matrix A ∈Rd×d. The Seperate Arm Estimator picks a set of M arms such that"
REFERENCES,0.246078431372549,"U ∗= arg max
U∈U ζmin(Φ−1
U )."
REFERENCES,0.24705882352941178,"It then splits Γ samples evenly between these arms, estimates the sample variance for each and finds
the least squares estimator bΣSA
Γ ."
REFERENCES,0.24803921568627452,Algorithm 3: Uniform Estimator of Heteroskedastic Variance
REFERENCES,0.24901960784313726,"Result: Find bΣUE
Γ
1 Input: Arms X ⊆Rd and Γ ∈N"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25,2 Sample arms uniformly at random from X
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25098039215686274,"3 Define A∗= PΓ
t=1 xtx⊤
t and b∗= PΓ
t=1 xtyt and estimate bθΓ = A∗−1b∗."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2519607843137255,"4 Define A† = PΓ
t=1 ϕxtϕ⊤
xt and b† = PΓ
t=1 ϕxt

yt −x⊤
t bθΓ
2
."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2529411764705882,"5 Output: bΣUE
Γ
= A†−1b†."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25392156862745097,Algorithm 4: Seperate Arm Estimator of Heteroskedastic Variance
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2549019607843137,"Result: Find bΣSA
Γ
1 Input: Arms X ⊆Rd and Γ ∈N"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25588235294117645,"2 Define U ∗= arg maxU∈U ζmin(Φ−1
U )."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2568627450980392,"3 Equally distribute Γ samples among arms XU ∗= {m : ϕm ∈U ∗} and observe rewards
{ym,t}Γ/M
t=1 for m ∈XU ∗"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.257843137254902,"4 Define the sample average for each arm ¯ym = (M/Γ) PΓ/M
t=1 ym,t"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25882352941176473,"5 Output: bΣSA
Γ
←arg mins∈RM P"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.25980392156862747,"m∈XU∗
PΓ/M
t=1
n
ϕm
⊤s −(¯ym −ym,t)2o2"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2607843137254902,"D
Proofs of Variance Estimation"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.26176470588235295,"In this appendix, we analyze the maximum absolute error of estimates associated with the Seperate
Arm Estimator and the HEAD Estimator. Recall that M := d(d+1)/2. Define W = {ϕx, ∀x ∈X} ⊂
RM where ϕx = vec(xx⊤)G and G ∈Rd2×M is the duplication matrix such that Gvech(xx⊤) =
vec(xx⊤) with vech(·) denoting the half-vectorization."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2627450980392157,"D.1
Analysis of the Separate Arm Estimator"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.26372549019607844,"We begin by proving a general result bounding the absolute error of the sample variance for any
sub-Gaussian random variable with parameter γ2."
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2647058823529412,"Proposition D.1. Let X be a γ2-sub-Gaussian random variable with mean µ. Assume that we
have collected n independent copies of X, {X1, X2, . . . , Xn}, and label the sample mean as ¯X =
Pn
i=1 Xi/n. Defining the sample variance as bσ2 = Pn
i=1(Xi −¯X)2/n, the true variance as
σ2 = E(X2
i ) −µ2 and δ ∈(0, 1), we find that with probability greater than 1 −δ,"
SAMPLE ARMS UNIFORMLY AT RANDOM FROM X,0.2656862745098039,|σ2 −bσ2| ≤5
MAX,0.26666666666666666,"2 max (
4
√"
MAX,0.2676470588235294,"2γ2 log(4/δ) n
, r"
MAX,0.26862745098039215,32γ4 log(4/δ) n ) .
MAX,0.2696078431372549,"Proof. We begin by upper bounding the absolute error using the triangle inequality,"
MAX,0.27058823529411763,"σ2 −bσ2 =
σ2 −
Pn
i=1(Xi −¯X)2 n "
MAX,0.27156862745098037,"=
E(X2
i ) −µ2 −
Pn
i=1 X2
i −2Xi ¯X + ¯X2 n "
MAX,0.2725490196078431,"=
E(X2
i ) −µ2 −
Pn
i=1 X2
i
n
+ 2 ¯X2 −¯X2"
MAX,0.2735294117647059,"=
E(X2
i ) −
Pn
i=1 X2
i
n
+ ¯X2 −σ2"
MAX,0.27450980392156865,n −µ2 + σ2 n 
MAX,0.2754901960784314,"≤
E(X2
i ) −
Pn
i=1 X2
i
n"
MAX,0.27647058823529413,"|
{z
}
A"
MAX,0.2774509803921569,"+
 ¯X2 −σ2"
MAX,0.2784313725490196,"n −µ2

|
{z
}
B"
MAX,0.27941176470588236,"+

σ2 n"
MAX,0.2803921568627451,"|{z}
C ."
MAX,0.28137254901960784,"We first bound quantity A. Leveraging Proposition B.3, we establish that X2
i −E(X2
i ) is sub-
exponential with parameters (4γ2√"
MAX,0.2823529411764706,"2, 4γ2). We can then use Proposition B.4 to analyze the sum
of independent sub-exponential random variables, finding that Pn
i=1

X2
i −E(X2
i )
	
/n is sub-
exponential with parameters (4γ2√"
MAX,0.2833333333333333,"2/√n, 4γ2/n). Finally, we invoke Proposition B.2 to bound
quantity A with probability 1 −δ/2,"
MAX,0.28431372549019607,"E(X2
i ) −
Pn
i=1 X2
i
n ≤max"
MAX,0.2852941176470588,"(
4γ2 log(4/δ) n
, r"
MAX,0.28627450980392155,32γ4 log(4/δ) n ) .
MAX,0.2872549019607843,"Now we bound quantity B. Appealing to Proposition B.3, we find that ¯X2 −σ2"
MAX,0.28823529411764703,"n −µ2 is sub-
exponential with parameters (4
√"
MAX,0.28921568627450983,"2γ2/n, 4γ2/n). We then use Proposition B.2 to bound B with
probability 1 −δ/2,"
MAX,0.2901960784313726,¯X2 −σ2
MAX,0.2911764705882353,"n −µ2
 ≤max"
MAX,0.29215686274509806,"(
4γ2 log(4/δ) n
, r"
MAX,0.2931372549019608,32γ4 log(4/δ) n2 ) = max
MAX,0.29411764705882354,"(
4γ2 log(4/δ)"
MAX,0.2950980392156863,"n
, 4γ2 log(4/δ) n √"
P,0.296078431372549,"2
p"
P,0.29705882352941176,log(4/δ) )
P,0.2980392156862745,"(a)
≤4
√"
P,0.29901960784313725,"2γ2 log(4/δ) n
,"
P,0.3,"where (a) follows because
√ 2/
p"
P,0.30098039215686273,"log(4/δ) ≤
√"
P,0.30196078431372547,"2. Finally, we combine quantities A, B and C,
apply a union bound, and conclude that with probability 1 −δ for δ ∈(0, 1),"
P,0.3029411764705882,"σ2 −bσ2 ≤
E(X2
i ) −
Pn
i=1 X2
i
n"
P,0.30392156862745096,"+
 ¯X2 −σ2"
P,0.30490196078431375,"n −µ2
 +

σ2 n  ≤max"
P,0.3058823529411765,"(
4γ2 log(4/δ) n
, r"
P,0.30686274509803924,"32γ4 log(4/δ) n ) + 4
√"
P,0.307843137254902,2γ2 log(4/δ)
P,0.3088235294117647,"n
+ γ2 n ≤5"
MAX,0.30980392156862746,"2 max (
4
√"
MAX,0.3107843137254902,"2γ2 log(4/δ) n
, r"
MAX,0.31176470588235294,32γ4 log(4/δ) n ) . ■
MAX,0.3127450980392157,"We can now use Proposition D.1 to control the sample variance estimates of the arms chosen by the
Separate Arm Estimator. Theorem D.1 derives a concentration bound of the maximum absolute error
of eσ2
x = min
n
max
n
σ2
min, x⊤bΣSA
Γ x
o
, σ2
max
o
. This bound, which is O(d4), scales unfavorably
with dimension as highlighted in the theoretical and empirical comparisons conducted in Section 3."
MAX,0.3137254901960784,"Theorem D.1. Set bΣSA
Γ
as the output of Alg. 4 and define B such that for all x ∈X, ∥x∥2 ≤B."
MAX,0.31470588235294117,"Defining eσ2
x = min
n
max
n
σ2
min, x⊤bΣSA
Γ x
o
, σ2
max
o
, σ2
x = x⊤Σ∗x and δ ∈[0, 1], for any x ∈X, P """
MAX,0.3156862745098039,"|eσ2
x −σ2
x| ≤
5B2√"
MAX,0.31666666666666665,"2
ζmin(ΦU ∗) max (
4
√"
MAX,0.3176470588235294,"2σ2
maxd3 log(8M/δ) Γ
, r"
MAX,0.31862745098039214,32σ4maxd4 log(8M/δ) Γ )#
MAX,0.3196078431372549,≥1 −δ/2.
MAX,0.3205882352941177,"Note that we set the probability bound to 1 −δ/2 instead of 1 −δ because we want to be able to
leverage these variance estimation bounds with the identification algorithms of Section 4 (and so are
anticipating another union bound)."
MAX,0.3215686274509804,"Proof. Label M = d(d + 1)/2 and XU ∗= {m : ϕm ∈U ∗}, where U ∗is defined in Alg. 4.
Additionally, in Alg. 4 we observe {ym,t}Γ/M
t=1 for each arm m ∈XU ∗. Define the sample average
for m ∈XU ∗as"
MAX,0.32254901960784316,"¯ym =
PΓ/M
t=1 ym,t Γ/M
."
MAX,0.3235294117647059,We obtain an estimate of Σ∗via
MAX,0.32450980392156864,"vech(bΣSA
Γ ) = arg min
s∈RM
X m∈XU∗ Γ/M
X t=1"
MAX,0.3254901960784314,"
⟨ϕm, s⟩−(¯ym −ym,t)2	2 .
(5)"
MAX,0.3264705882352941,"Label zm,t = (¯ym −ym,t)2 ∈R, Ψ∗= vech(Σ∗) ∈RM, eΨΓ = vech(bΣSA
Γ ) ∈RM and"
MAX,0.32745098039215687,"¯zm =
PΓ/M
t=1 zm,t Γ/M
."
MAX,0.3284313725490196,We reformulate Eq. 5 as follows
MAX,0.32941176470588235,"eΨΓ = arg min
s∈RM
X m∈XU∗ Γ/M
X"
MAX,0.3303921568627451,"t=1
(⟨ϕm, s⟩−zm,t)2"
MAX,0.33137254901960783,"= arg min
s∈RM
X m∈XU∗ Γ/M
X"
MAX,0.3323529411764706,"t=1
⟨ϕm, s⟩2 −2zm,t⟨ϕm, s⟩+ z2
m,t"
MAX,0.3333333333333333,"= arg min
s∈RM
X"
MAX,0.33431372549019606,"m∈XU∗
(Γ/M)⟨ϕm, s⟩2 −2⟨ϕm, s⟩ Γ/M
X"
MAX,0.3352941176470588,"t=1
zm,t + Γ/M
X"
MAX,0.3362745098039216,"t=1
z2
m,t"
MAX,0.33725490196078434,"= arg min
s∈RM
X"
MAX,0.3382352941176471,"m∈XU∗
⟨ϕm, s⟩2 −2⟨ϕm, s⟩¯zm +"
MAX,0.3392156862745098,"PΓ/M
t=1 z2
m,t
Γ/M"
MAX,0.34019607843137256,"(a)
= arg min
s∈RM
X"
MAX,0.3411764705882353,"m∈XU∗
⟨ϕm, s⟩2 −2⟨ϕm, s⟩¯zm + ¯z2
m"
MAX,0.34215686274509804,"= arg min
s∈RM
X"
MAX,0.3431372549019608,"m∈XU∗
(⟨ϕm, s⟩−¯zm)2 ,"
MAX,0.34411764705882353,"where (a) follows because PΓ/M
t=1 z2
m,t/(Γ/M) and ¯z2
m do not depend on s.
Defining ¯z =
[¯zm]m∈XU∗∈RM, Eq. 5 becomes"
MAX,0.34509803921568627,"eΨΓ = arg min
s∈RM
X"
MAX,0.346078431372549,"m∈XU∗
(⟨ϕm, s⟩−¯zm)2 = arg min
s∈RM ∥ΦU ∗s −¯z∥2
2."
MAX,0.34705882352941175,"We find that
∥ΦU ∗(eΨΓ −Ψ∗)∥2 = ∥ΦU ∗eΨΓ −ΦU ∗Ψ∗−¯z + ¯z∥2"
MAX,0.3480392156862745,= ∥ΦU ∗eΨΓ −¯z + ¯z −ΦU ∗Ψ∗∥2
MAX,0.34901960784313724,"≤∥ΦU ∗eΨΓ −¯z∥2 + ∥¯z −ΦU ∗Ψ∗∥2
≤∥ΦU ∗Ψ∗−¯z∥2 + ∥¯z −ΦU ∗Ψ∗∥2
= 2∥ΦU ∗Ψ∗−¯z∥2."
MAX,0.35,"Consequently, we can focus on bounding ∥ΦU ∗Ψ∗−¯z∥2 in order to control ∥ΦU ∗(eΨΓ −Ψ∗)∥2.
Note that the mth entry of (ΦU ∗Ψ∗−¯z) is (m⊤Σ∗m −¯zm) and that ¯zm is the sample variance of
observations {ym,t}Γ/M
t=1 . Using Proposition D.1 and the fact that the ym,t are σ2
max-sub-Gaussian,
we show that with probability 1 −δ/(2M),"
MAX,0.3509803921568627,"|ϕ⊤
mΨ∗−¯zm| ≤5"
MAX,0.3519607843137255,"2 max (
4
√"
MAX,0.35294117647058826,"2σ2
maxM log(8M/δ) Γ
, r"
MAX,0.353921568627451,"32σ4maxM log(8M/δ) Γ ) .
(6)"
MAX,0.35490196078431374,"We apply Eq. 6 to quantity ∥ΦU ∗Ψ∗−¯z∥2 and a union bound to find that with probability 1 −δ/2,"
MAX,0.3558823529411765,"∥ΦU ∗Ψ∗−¯z∥2 ≤
√ M 5"
MAX,0.3568627450980392,"2 max (
4
√"
MAX,0.35784313725490197,"2σ2
maxM log(8M/δ) Γ
, r"
MAX,0.3588235294117647,32σ4maxM log(8M/δ) Γ ) = 5
MAX,0.35980392156862745,"2 max (
4
√"
MAX,0.3607843137254902,"2σ2
maxM
√"
MAX,0.36176470588235293,"M log(8M/δ)
Γ
, r"
MAX,0.3627450980392157,32σ4maxM 2 log(8M/δ) Γ ) .
MAX,0.3637254901960784,"Consequently,"
MAX,0.36470588235294116,"∥ΦU ∗(eΨΓ −Ψ∗)∥2 ≤5 max (
4
√"
MAX,0.3656862745098039,"2σ2
maxM
√"
MAX,0.36666666666666664,"M log(8M/δ)
Γ
, r"
MAX,0.36764705882352944,32σ4maxM 2 log(8M/δ) Γ ) .
MAX,0.3686274509803922,"We now analyze the absolute error of the estimate eσx for an arbitrary action x ∈X. Since ΦU ∗is
non-singular by construction, we know that for q := (ΦU ∗−1)⊤ϕx ∈RM,"
MAX,0.3696078431372549,"|eΨ⊤
Γ ϕx −σ2
x| = ∥(eΨΓ −Ψ∗)⊤ϕx∥2"
MAX,0.37058823529411766,= ∥(eΨΓ −Ψ∗)⊤ΦU ∗⊤q∥2
MAX,0.3715686274509804,≤∥(eΨΓ −Ψ∗)⊤ΦU ∗⊤∥2∥q∥2
MAX,0.37254901960784315,= ∥ΦU ∗(eΨΓ −Ψ∗)∥2∥(ΦU ∗−1)⊤ϕx∥2
MAX,0.3735294117647059,≤∥ΦU ∗(eΨΓ −Ψ∗)∥2∥(ΦU ∗−1)⊤∥2∥ϕx∥2
MAX,0.37450980392156863,"(a)
=
2
ζmin(ΦU ∗)∥ΦU ∗(eΨΓ −Ψ∗)∥2∥ϕx∥2,"
MAX,0.37549019607843137,"where (a) follows from the definition of the matrix norm. We now leverage the nature of the
duplication matrix, G, and the fact that ∥x∥2 ≤B to bound ∥ϕx∥2
2."
MAX,0.3764705882352941,"∥ϕx∥2
2 = vec(xx⊤)⊤GG⊤vec(xx⊤)"
MAX,0.37745098039215685,= vec(xx⊤)⊤G(G⊤G)(G⊤G)−1G⊤vec(xx⊤)
MAX,0.3784313725490196,= vec(xx⊤)⊤G(G⊤G)−1/2(G⊤G)(G⊤G)−1/2G⊤vec(xx⊤)
MAX,0.37941176470588234,"(a)
≤2vec(xx⊤)⊤G(G⊤G)−1G⊤vec(xx⊤)"
MAX,0.3803921568627451,"(b)
= 2vec(xx⊤)⊤vec(xx⊤)"
MAX,0.3813725490196078,"= 2x⊤xx⊤x ≤2B4,"
MAX,0.38235294117647056,"where (a) follows from the fact that G⊤G ⪯2I as shown in Equation (61) of [40], and (b) uses
the fact that G(G⊤G)−1G⊤vec(xx⊤) = vec(xx⊤) as shown in Equations (54) and (52) of [40].
Consequently, with probability greater than 1 −δ/2, we have that for any x ∈X,"
MAX,0.38333333333333336,"|eσ2
x −σ2
x| ≤
5B2√"
MAX,0.3843137254901961,"2
ζmin(ΦU ∗) max (
4
√"
MAX,0.38529411764705884,"2σ2
maxM
√"
MAX,0.3862745098039216,"M log(8M/δ)
Γ
, r"
MAX,0.3872549019607843,32σ4maxM 2 log(8M/δ) Γ )
MAX,0.38823529411764707,"≤
5B2√"
MAX,0.3892156862745098,"2
ζmin(ΦU ∗) max (
4
√"
MAX,0.39019607843137255,"2σ2
maxd3 log(8M/δ) Γ
, r"
MAX,0.3911764705882353,32σ4maxd4 log(8M/δ) Γ ) . ■
MAX,0.39215686274509803,"D.2
Analysis of the HEAD Estimator"
MAX,0.3931372549019608,"We now present the maximum absolute error guarantees of the HEAD procedure, Alg. 1."
MAX,0.3941176470588235,"Proof of Theorem 3.1. In this proof, we bound the estimation error |bσ2
x −σ2
x|, ∀x ∈X. Define
σ2
max = maxx∈X σ2
x, Ψ∗= vech(Σ∗), N1 = {1, . . . , Γ/2}, N2 = {Γ/2 + 1, . . . , Γ} and"
MAX,0.39509803921568626,"bΨΓ = arg min
s∈RM
X t∈N2"
MAX,0.396078431372549,"n
s⊤ϕxt −(yt −x⊤
t bθΓ/2)2o2
,"
MAX,0.39705882352941174,"where bΨΓ = vech(bΣΓ). Furthermore, define vector e with entries et = (yt −x⊤
t bθΓ/2)2 and Φ such
that the rows of Φ are composed of {ϕxt}t∈N2. We find that
bΨΓ = (Φ⊤Φ)−1Φ⊤e."
MAX,0.3980392156862745,"For any ϕx ∈W, we are interested in bounding
ϕ⊤
x (bΨΓ −Ψ∗)
,"
MAX,0.3990196078431373,"|ϕ⊤
x (bΨΓ −Ψ∗)| =
ϕ⊤
x
n 
Φ⊤Φ
−1 Φ⊤e −Ψ∗o"
MAX,0.4,"= | ϕ⊤
x (Φ⊤Φ)−1Φ⊤
|
{z
}
z⊤
(e −ΦΨ∗)|."
MAX,0.40098039215686276,"Define z⊤= ϕ⊤
x (Φ⊤Φ)−1Φ⊤and X such that the rows of X are composed of {xt}t∈N2. We can
bound this expression with the triangle inequality as"
MAX,0.4019607843137255,"|ϕ⊤
x (bΨΓ −Ψ∗)| =  X"
MAX,0.40294117647058825,"t∈N2
zt
n
x⊤
t (bθΓ/2 −θ∗)
o2
+
X"
MAX,0.403921568627451,"t∈N2
zt
 
η2
t −ϕ⊤
t Ψ∗
+
X"
MAX,0.40490196078431373,"t∈N2
2ztx⊤
t (bθΓ/2 −θ∗)ηt  ≤  X"
MAX,0.40588235294117647,"t∈N2
zt
n
x⊤
t (bθΓ/2 −θ∗)
o2
 +  X"
MAX,0.4068627450980392,"t∈N2
zt
 
η2
t −ϕ⊤
t Ψ∗
 +  X"
MAX,0.40784313725490196,"t∈N2
2ztx⊤
t (bθΓ/2 −θ∗)ηt ."
MAX,0.4088235294117647,"We can simplify the first term as, X"
MAX,0.40980392156862744,"t∈N2
zt
n
x⊤
t (bθΓ/2 −θ∗)
o2
 =  X"
MAX,0.4107843137254902,"t∈N2
ztϕ⊤
xtvech
n
(bθΓ/2 −θ∗)(bθΓ/2 −θ∗)⊤o"
MAX,0.4117647058823529,"=
ϕ⊤
x (Φ⊤Φ)−1Φ⊤Φ × vech
n
(bθΓ/2 −θ∗)(bθΓ/2 −θ∗)⊤o"
MAX,0.41274509803921566,"=
x⊤(bθΓ/2 −θ∗)(bθΓ/2 −θ∗)⊤x"
MAX,0.4137254901960784,"=
n
x⊤(bθΓ/2 −θ∗)
o2
."
MAX,0.4147058823529412,"Additionally, the third term is equivalent to 2|(η ◦z)⊤X(bθΓ/2 −θ∗)|, where “◦"" is element-wise
multiplication. Consequently, the absolute error is equal to"
MAX,0.41568627450980394,"|ϕ⊤
x (bΨΓ −Ψ∗)| =
n
x⊤(bθΓ/2 −θ∗)
o2"
MAX,0.4166666666666667,"|
{z
}
A +  X"
MAX,0.4176470588235294,"t∈N2
zt
 
η2
t −ϕ⊤
t Ψ∗

|
{z
}
B"
MAX,0.41862745098039217,"+ 2
(η ◦z)⊤X(bθΓ/2 −θ∗)

|
{z
}
C ."
MAX,0.4196078431372549,"Note that to bound |ϕ⊤
x (bΨΓ −Ψ∗)| with probability 1 −δ/2, it is necessary to bound quantities A, B
and C with probability 1 −δ/6 in anticipation of a union bound."
MAX,0.42058823529411765,"Quantity A. We sample {xt, t ∈N1} according to the rounded G-optimal design over X constructed
in Stage 1 of Alg. 1. We use the concentration bounds of Proposition B.1, to state that with probability
1 −δ/6,"
MAX,0.4215686274509804,"A ≤2 x⊤
 X"
MAX,0.42254901960784313,"t∈N1
xtx⊤
t !−1 x"
MAX,0.4235294117647059,"|
{z
}
G"
MAX,0.4245098039215686,"σ2
max log(12|X|/δ)."
MAX,0.42549019607843136,"We use Proposition B.5 in combination with the rounding procedure guarantees of Proposition A.1 to
bound quantity G. For ϵ ∈(0, 1/3], with probability 1 −δ/6,"
MAX,0.4264705882352941,"A ≤4d(1 + 6ϵ)σ2
max log(12|X|/δ) Γ
."
MAX,0.42745098039215684,"Quantity B.
According to Proposition B.3,
η2
t
is sub-exponential with parameters
(4
√"
MAX,0.4284313725490196,"2σ2
max, 4σ2
max). Consequently, by Proposition B.4, we know that B is sub-exponential with
parameters
 
s X"
MAX,0.4294117647058823,"t∈N2
z2
t 32σ4max, max
t∈N2 |zt|4σ2
max  ."
MAX,0.4303921568627451,"We now invoke the G-optimal design over W space conducted in Stage 2 of Alg. 1 and Propositions
B.5 and A.1 to find that for ϵ ∈(0, 1/3],
X"
MAX,0.43137254901960786,"t∈N2
z2
t =
X"
MAX,0.4323529411764706,"t∈N2
ϕ⊤
x (Φ⊤Φ)−1ϕtϕ⊤
t (Φ⊤Φ)−1ϕx"
MAX,0.43333333333333335,"= ϕ⊤
x (Φ⊤Φ)−1Φ⊤Φ(Φ⊤Φ)−1ϕx"
MAX,0.4343137254901961,"≤ϕ⊤
x (Φ⊤Φ)−1ϕx ≤2d2(1 + 6ϵ) Γ
."
MAX,0.43529411764705883,"Consequently, X"
MAX,0.4362745098039216,"t∈N2
z2
t ≤2d2(1 + 6ϵ)"
MAX,0.4372549019607843,"Γ
,
(7) and"
MAX,0.43823529411764706,"|zt| = |ϕ⊤
x (Φ⊤Φ)−1ϕt| ≤
q"
MAX,0.4392156862745098,"ϕ⊤
x (Φ⊤Φ)−1ϕx
q"
MAX,0.44019607843137254,"ϕ⊤
t (Φ⊤Φ)−1ϕt"
MAX,0.4411764705882353,"≤2d2(1 + 6ϵ) Γ
."
MAX,0.442156862745098,"Therefore, B has sub-exponential parameters (
p"
MAX,0.44313725490196076,"64σ4maxd2(1 + 6ϵ)/Γ, 8σ2
maxd2(1 + 6ϵ)/Γ). Using
Proposition B.2, with probability 1 −δ/6,"
MAX,0.4441176470588235,B ≤max (r
MAX,0.44509803921568625,128 log(12/δ)σ4maxd2(1 + 6ϵ)
MAX,0.44607843137254904,"Γ
, 16 log(12/δ)σ2
maxd2(1 + 6ϵ)
Γ ) = max (
√ 8σmax r"
MAX,0.4470588235294118,16 log(12/δ)σ2maxd2(1 + 6ϵ)
MAX,0.44803921568627453,"Γ
, 16 log(12/δ)σ2
maxd2(1 + 6ϵ)
Γ ) ."
MAX,0.44901960784313727,"Quantity C. Construct V ∈RΓ/2×d such that the rows of V correspond to {xt : t ∈N1}. We can
upper bound quantity C using the Cauchy-Schwartz inequality in the following manner,"
MAX,0.45,|(η ◦z)⊤X(bθΓ/2 −θ∗)| = |(η ◦z)⊤X(V ⊤V )−1/2(V ⊤V )1/2(bθΓ/2 −θ∗)|
MAX,0.45098039215686275,≤∥(η ◦z)⊤X(V ⊤V )−1/2∥2∥(V ⊤V )1/2(bθΓ/2 −θ∗)∥2.
MAX,0.4519607843137255,"By Proposition B.8, with probability 1 −δ/12,"
MAX,0.45294117647058824,"∥(V ⊤V )1/2(bθΓ/2 −θ∗)∥2 ≤2
p"
MAX,0.453921568627451,2σ2max {d log(6) + log(12/δ)}.
MAX,0.4549019607843137,"Now construct Dz = diag({zt : t ∈N2}) and express,"
MAX,0.45588235294117646,"∥(η ◦z)⊤X(V ⊤V )−1/2∥2 =
q"
MAX,0.4568627450980392,η⊤DzX(V ⊤V )−1X⊤Dzη.
MAX,0.45784313725490194,"We first establish that by the G-optimal designs in both X and W space along with Propositions B.5
and A.1,"
MAX,0.4588235294117647,"tr

DzX(V ⊤V )−1X⊤Dz
	
= tr

D2
zX(V ⊤V )−1X⊤ =
X"
MAX,0.4598039215686274,"t∈N2
z2
t x⊤
t (V ⊤V )−1xt"
MAX,0.46078431372549017,≤2d(1 + 6ϵ) Γ X
MAX,0.46176470588235297,"t∈N2
z2
t"
MAX,0.4627450980392157,"(a)
≤4d3(1 + 6ϵ)2 Γ2
,"
MAX,0.46372549019607845,"where ϵ ∈(0, 1/3] and (a) follows from Eq. 7. Consequently, we find that"
MAX,0.4647058823529412,"tr

DzX(V ⊤V )−1X⊤Dz
	
≤4d3(1 + 6ϵ)2"
MAX,0.46568627450980393,"Γ2
.
(8)"
MAX,0.4666666666666667,"This inequality will be useful in the following derivation. Labeling ∆= DzX(V ⊤V )−1X⊤Dz, we
use Proposition B.7 to bound η⊤∆η with probability 1 −δ/12 and explain below."
MAX,0.4676470588235294,"η⊤∆η < σ2
max
n
tr(∆) + 2∥∆∥F log(12/δ)1/2 + 2∥∆∥2 log(12/δ)
o
(9)"
MAX,0.46862745098039216,"≤σ2
max"
MAX,0.4696078431372549,4d3(1 + 6ϵ)2
MAX,0.47058823529411764,"Γ2
+ 2∥∆1/2∥2
F log(12/δ)1/2 + 2∥∆∥2 log(12/δ)

(10)"
MAX,0.4715686274509804,"≤σ2
max"
MAX,0.4725490196078431,4d3(1 + 6ϵ)2
MAX,0.47352941176470587,"Γ2
+ 2∥∆1/2∥2
F log(12/δ)1/2 + 2∥∆∥F log(12/δ)

(11)"
MAX,0.4745098039215686,"≤σ2
max"
MAX,0.47549019607843135,4d3(1 + 6ϵ)2
MAX,0.4764705882352941,"Γ2
+ 2∥∆1/2∥2
F log(12/δ)1/2 + 2∥∆1/2∥2
F log(12/δ)

(12)"
MAX,0.4774509803921569,"≤σ2
max"
MAX,0.47843137254901963,4d3(1 + 6ϵ)2
MAX,0.47941176470588237,"Γ2
+ 2tr(∆) log(12/δ)1/2 + 2tr(∆) log(12/δ)

(13)"
MAX,0.4803921568627451,"≤σ2
max"
MAX,0.48137254901960785,4d3(1 + 6ϵ)2
MAX,0.4823529411764706,"Γ2
+ 24d3(1 + 6ϵ)2"
MAX,0.48333333333333334,"Γ2
log(12/δ)1/2 + 24d3(1 + 6ϵ)2"
MAX,0.4843137254901961,"Γ2
log(12/δ)
 (14)"
MAX,0.4852941176470588,"≤5σ2
max
4d3(1 + 6ϵ)2"
MAX,0.48627450980392156,"Γ2
log(12/δ),
(15)"
MAX,0.4872549019607843,"where line (9) follows from Eq. 8, Proposition B.6 and Proposition B.7. In lines (10) and (11), we
use Proposition B.6 again and then the definition of the Frobenius norm in line (12). Consequently,
with probability 1 −δ/6,"
MAX,0.48823529411764705,C = |(η ◦z)⊤X(bθΓ/2 −θ∗)|
MAX,0.4892156862745098,= |(η ◦z)⊤X(V ⊤V )−1/2(V ⊤V )1/2(bθΓ/2 −θ∗)|
MAX,0.49019607843137253,≤∥(η ◦z)⊤X(V ⊤V )−1/2∥2∥(V ⊤V )1/2(bθΓ/2 −θ∗)∥2 ≤2 r
MAX,0.49117647058823527,"10σ4max
4d3(1 + 6ϵ)2"
MAX,0.492156862745098,"Γ2
log(12/δ) {d log(6) + log(12/δ)}."
MAX,0.4931372549019608,"Combining quantities A, B and C together and applying a union bound, we have with probability
1 −δ/2,"
MAX,0.49411764705882355,"|ϕ⊤
x (bΨΓ −Ψ∗)| ≤max (
√ 8σmax r"
MAX,0.4950980392156863,16 log(12/δ)σ2maxd2(1 + 6ϵ)
MAX,0.49607843137254903,"Γ
, 16 log(12/δ)σ2
maxd2(1 + 6ϵ)
Γ )"
MAX,0.4970588235294118,"+
4dσ2
max log(12|X|/δ)(1 + 6ϵ) Γ
+ +
2 r"
MAX,0.4980392156862745,"10σ4max
4d3(1 + 6ϵ)2"
MAX,0.49901960784313726,"Γ2
log(12/δ) {d log(6) + log(12/δ)}"
MAX,0.5,"≤(3/10) max (
√ 8σmax r"
MAX,0.5009803921568627,16 log(12/δ)σ2maxd2(1 + 6ϵ)
MAX,0.5019607843137255,"Γ
, 16 log(12/δ)σ2
maxd2(1 + 6ϵ)
Γ )"
MAX,0.5029411764705882,"|
{z
}
W"
MAX,0.503921568627451,"+
4dσ2
max log(12|X|/δ)(1 + 6ϵ)"
MAX,0.5049019607843137,"Γ
|
{z
}
V ."
MAX,0.5058823529411764,"We can exploit the dependencies of V and W to simplify this bound in two ways under different
conditions. Note that we only have dependency on |X| through term V. If d−1 > log(|X|)/ log(1/δ)
and Γ > 16 log(12/δ)σ2
maxd2(1 + 6ϵ), then with probability 1 −δ/2,"
MAX,0.5068627450980392,"|ϕ⊤
x (bΨΓ −Ψ∗)| ≤ r"
MAX,0.5078431372549019,"256 log(12/δ)σ4maxd2(1 + 6ϵ) Γ
."
MAX,0.5088235294117647,"Alternatively, if just Γ > 16 log(12|X|/δ)σ2
maxd2(1 + 6ϵ), then"
MAX,0.5098039215686274,"|ϕ⊤
x (bΨΓ −Ψ∗)| ≤ r"
MAX,0.5107843137254902,256 log(12|X|/δ)σ4maxd2(1 + 6ϵ)
MAX,0.5117647058823529,"Γ
.
(16)"
MAX,0.5127450980392156,"Since σ2
max ≥σ2
x ≥σ2
min for all x ∈X,"
MAX,0.5137254901960784,"|ϕ⊤
x (bΨΓ −Ψ∗)| = |x⊤bΣΓx −x⊤Σ∗x| ≥| min
n
max
n
x⊤bΣΓx, σ2
min
o
, σ2
max
o
−σ2
x|."
MAX,0.5147058823529411,"In summary, if Γ > 16 log(12|X|/δ)σ2
maxd2(1 + 6ϵ), P "
MAX,0.515686274509804,"|bσ2
x −σ2
x| ≤ r"
MAX,0.5166666666666667,C′ log(|X|/δ)σ4maxd2 Γ !
MAX,0.5176470588235295,"≥1 −δ,"
MAX,0.5186274509803922,"where C′ = 256(1 + 6ϵ).
■"
MAX,0.5196078431372549,"We now prove a simple lemma relating the additive bound on |bσ2
x −σ2
x| to a multiplicative bound
1 −CΓ,δ < σ2
x/bσ2
x < 1 + CΓ,δ."
MAX,0.5205882352941177,"Lemma D.1. Define bσ2
x = min
n
max
n
x⊤bΣΓx, σ2
min
o
, σ2
max
o
. Letting σ2
min = minx∈X σ2
x,"
MAX,0.5215686274509804,"κ = σ2
max/σ2
min, and"
MAX,0.5225490196078432,"P

∀x ∈X, 1 −CΓ,δ < σ2
x
bσ2x
< 1 + CΓ,δ"
MAX,0.5235294117647059,"
≥1 −δ/2, where CΓ,δ = r"
MAX,0.5245098039215687,"C′ log(|X|/δ)κ2d2 Γ
."
MAX,0.5254901960784314,"Proof. Using Lemma D.1, we know that with probability greater than 1 −δ/2 the following event
holds."
MAX,0.5264705882352941,"|σ2
x −bσ2
x| ≤ r"
MAX,0.5274509803921569,"C′ log(|X|/δ)σ4maxd2 Γ
⇒"
MAX,0.5284313725490196,"bσ2
x − r"
MAX,0.5294117647058824,C′ log(|X|/δ)σ4maxd2
MAX,0.5303921568627451,"Γ
≤σ2
x ≤bσ2
x + r"
MAX,0.5313725490196078,"C′ log(|X|/δ)σ4maxd2 Γ
⇒"
MAX,0.5323529411764706,"bσ2
x −bσ2
x s"
MAX,0.5333333333333333,C′ log(|X|/δ)σ4maxd2
MAX,0.5343137254901961,"bσ4xΓ
≤σ2
x ≤bσ2
x + bσ2
x s"
MAX,0.5352941176470588,C′ log(|X|/δ)σ4maxd2
MAX,0.5362745098039216,"bσ4xΓ
⇒"
MAX,0.5372549019607843,"bσ2
x −bσ2
x s"
MAX,0.538235294117647,C′ log(|X|/δ)σ4maxd2
MAX,0.5392156862745098,"σ4
minΓ
≤σ2
x ≤bσ2
x + bσ2
x s"
MAX,0.5401960784313725,C′ log(|X|/δ)σ4maxd2
MAX,0.5411764705882353,"σ4
minΓ
⇒ 1 − r"
MAX,0.542156862745098,C′ log(|X|/δ)κ2d2
MAX,0.5431372549019607,"Γ
≤σ2
x
bσ2x
≤1 + r"
MAX,0.5441176470588235,"C′ log(|X|/δ)κ2d2 Γ
. ■"
MAX,0.5450980392156862,"E
Proofs of Best-Arm Identification"
MAX,0.546078431372549,"We divide the proof of Theorem 4.1 and 4.2 between Appendices E and F for ease of exposition. In
Appendix E, we consider the transductive linear best-arm identification problem (BAI) in which we
are interested in identifying z∗= maxz∈Z z⊤θ∗with probability 1−δ for δ ∈(0, 1). In Appendix F,
we consider the level set (LS) counterpart. Recall that for a set V, let PV := {λ ∈R|V| : P"
MAX,0.5470588235294118,"v∈V λv =
1, λv ≥0 ∀v} denote distributions over V and Y(V) := {v′ −v : v, v′ ∈V, v ̸= v′} be an operator
giving differences. We begin with the lower bound."
MAX,0.5480392156862746,"E.1
Lower Bound"
MAX,0.5490196078431373,"Proof of Theorem 4.1 for transductive best-arm identification. This proof is similar to the lower
bound proofs of [35] and [12]. Let C := {eθ ∈Rd : ∃i s.t. eθ⊤(z∗−zi) ≤0}, i.e., eθ ∈C if
and only if z∗is not the best arm in the linear bandit instance (X, Z, eθ). Additionally, label K = |X|."
MAX,0.55,"We first invoke Lemma 1 of [27]. Under a δ-PAC strategy for finding the best arm for the bandit
instance (X, Z, θ∗), let Ti denote the random variable that is the number of times arm i is pulled.
Defining σ2
x = x⊤Σ∗x for all x ∈X, we let νθ,i denote the reward distribution of the i-th arm of X,
i.e., νθ,i = N(x⊤
i θ, σ2
xi). For any eθ ∈C we know from [27] that K
X"
MAX,0.5509803921568628,"i=1
E(Ti)KL(νθ∗,i||νeθ,i) ≥log(1/2.4δ)."
MAX,0.5519607843137255,"Following the steps of [12], we find that K
X"
MAX,0.5529411764705883,"i=1
E(Ti) ≥log(1/2.4δ) min
λ∈PX max
eθ∈C"
PK,0.553921568627451,"1
PK
i=1 λiKL(νθ∗,i||νeθ,i)
.
(17)"
PK,0.5549019607843138,"Label Z = {z1, z2, . . . , zn}, where z1 = z∗without loss of generality. For j ̸= 1, λ ∈PX and"
PK,0.5558823529411765,"ϵ > 0, define A(λ) := PK
i=1 λi
xix⊤
i
σ2xi , wj = z∗−zj and"
PK,0.5568627450980392,"θj(ϵ, λ) = θ∗−(w⊤
j θ∗+ ϵ)A(λ)−1wj"
PK,0.557843137254902,"w⊤
j A(λ)−1wj
."
PK,0.5588235294117647,"Note that w⊤
j θj(ϵ, λ) = −ϵ < 0, implying that θj ∈C. We find that the KL divergence between νθ,i
and νθj(ϵ,λ),i is given by:"
PK,0.5598039215686275,"KL(νθ,i||νθj(ϵ,λ),i) ="
PK,0.5607843137254902,"
x⊤
i {θ∗−θj(ϵ, λ)}
2 2σ2xi"
PK,0.5617647058823529,"= w⊤
j A(λ)−1 (w⊤
j θ∗+ ϵ)2 xix⊤
i
2σ2xi

w⊤
j A(λ)−1wj
	2 A(λ)−1wj."
PK,0.5627450980392157,"Returning to Eq. 17, K
X"
PK,0.5637254901960784,"i=1
E(Ti) ≥log(1/2.4δ) min
λ∈PX max
eθ∈C"
PK,0.5647058823529412,"1
PK
i=1 λiKL(νθ∗,i||νeθ,i)"
PK,0.5656862745098039,"≥log(1/2.4δ) min
λ∈PX
max
j=2,··· ,m
1
PK
i=1 λiKL(νθ∗,i||νθj(ϵ,λ),i)"
PK,0.5666666666666667,"≥log(1/2.4δ) min
λ∈PX
max
j=2,··· ,m"
PK,0.5676470588235294,"
w⊤
j A(λ)−1wj
	2"
PK,0.5686274509803921,"PK
i=1(w⊤
j θ∗+ ϵ)2w⊤
j A(λ)−1λi
xix⊤
i
2σ2xi A(λ)−1wj"
PK,0.5696078431372549,"(a)
= 2 log(1/2.4δ) min
λ∈PX
max
j=2,··· ,m"
PK,0.5705882352941176,"
w⊤
j A(λ)−1wj
	2"
PK,0.5715686274509804,"(w⊤
j θ∗+ ϵ)2w⊤
j A(λ)−1
PK
i=1 λi
xix⊤
i
σ2xi"
PK,0.5725490196078431,"
A(λ)−1wj"
PK,0.5735294117647058,"= 2 log(1/2.4δ) min
λ∈PX
max
j=2,··· ,m"
PK,0.5745098039215686,"
w⊤
j A(λ)−1wj
	2"
PK,0.5754901960784313,"(w⊤
j θ∗+ ϵ)2
,"
PK,0.5764705882352941,"where (a) uses the fact that PK
i=1 λi
xix⊤
i
σ2xi = A(λ). Letting ϵ →0 establishes the result.
■"
PK,0.5774509803921568,"E.2
Upper Bound"
PK,0.5784313725490197,"We now prove Theorem 4.2 by first establishing that Alg. 2 is δ-PAC for the transductive linear
bandits best-arm identification problem."
PK,0.5794117647058824,"E.2.1
Proof of Correctness"
PK,0.5803921568627451,"Overview of Correctness Proof. Lemma D.1 along with Lemmas E.1 through E.3 are combined to
prove that Algorithm 2 is δ-PAC for the best-arm identification problem. Lemma E.1 leverages Lemma
D.1 and begins by establishing that in any round ℓ∈N, the estimation error for the difference in
mean rewards between z ∈Z and z∗= arg maxz∈Z z⊤θ∗is bounded by ϵℓ= 2−ℓwith probability
1 −δ for δ ∈(0, 1). Lemma E.2 uses this result to prove that starting at round ℓ, z∗∈Zℓ+1 with
probability 1 −δ. This establishes that z∗is not eliminated from the set of items we are considering
at any step with high probability. Lastly, Lemma E.3 uses Lemma E.1 to show that Algorithm 2
will eventually identify z∗. This is done by proving that Alg. 2 eliminates items with mean rewards
that are outside a shrinking radius (halves at every step of the algorithm) of the highest reward,
z∗⊤θ∗. In other words, with probability 1 −δ for δ ∈(0, 1), we prove that items in Zℓwith expected
rewards that are 2ϵℓaway from z∗⊤θ∗will not be in Zℓ+1. Since ϵℓ→0 as ℓ→∞, we know that
limℓ→∞P(Zℓ= {z∗}) = 1 −δ."
PK,0.5813725490196079,"Lemma E.1. With probability at least 1 −δ for δ ∈(0, 1),"
PK,0.5823529411764706,"|⟨z −z∗, bθℓ−θ∗⟩| ≤ϵℓ"
PK,0.5833333333333334,for all ℓ∈N and z ∈Z.
PK,0.5843137254901961,"Proof. Define σ2
x = x⊤Σ∗x, bθℓas the weighted least squares estimator constructed in Algorithm 2
at stage ℓ, and events
Ez,ℓ= |⟨z −z∗, bθℓ−θ∗⟩| ≤ϵℓ and"
PK,0.5852941176470589,"J =

σ2
x ≤bσ2
x (1 + CΓ,δ) = 3"
PK,0.5862745098039216,"2bσ2
x, ∀x ∈X
	
."
PK,0.5872549019607843,"Label nℓas the number of data points collected during stage ℓ, Υ = diag({σ2
i }nℓ
i=1), bΥ =
diag({bσ2
i }nℓ
i=1), Xℓas a matrix whose rows are composed of the arms xi,ℓdrawn during round
ℓ, Yℓas the vector of rewards drawn during round ℓ, Aℓ:= Pnℓ
i=1 bσ−2
i
xℓ,ix⊤
ℓ,i, and eΩ=
A−1
ℓX⊤
ℓbΥ−1ΥbΥ−1XℓA−1
ℓ
as the weighted least squares estimator’s covariance. We first estab-
lish that for z ∈Rd,"
PK,0.5882352941176471,"E
n
z⊤(bθℓ−θ∗)
o
= z⊤n
A−1
t XℓbΥ−1E(Yℓ) −θ∗o
= z⊤
A−1
t XℓbΥ−1Xℓθ∗−θ∗
= 0, and"
PK,0.5892156862745098,"V
n
z⊤(bθℓ−θ∗)
o
= z⊤eΩz."
PK,0.5901960784313726,"Furthermore, defining u⊤= z⊤A−1
ℓX⊤
ℓbΥ−1/2 ∈Rnℓand conditioning on event J ,"
PK,0.5911764705882353,"z⊤eΩz = z⊤A−1
ℓX⊤
ℓbΥ−1ΥbΥ−1XℓA−1
ℓz"
PK,0.592156862745098,"= (z⊤A−1
ℓX⊤
ℓbΥ−1/2)bΥ−1/2ΥbΥ−1/2(bΥ−1/2XℓA−1
ℓz)"
PK,0.5931372549019608,"= u⊤bΥ−1/2ΥbΥ−1/2u = nℓ
X"
PK,0.5941176470588235,"i=1
u2
i
σ2
xi
bσ2
i"
PK,0.5950980392156863,"(a)
≤3 2 nℓ
X"
PK,0.596078431372549,"i=1
u2
i = 3"
PK,0.5970588235294118,"2z⊤A−1
ℓX⊤
ℓbΥ−1XℓA−1
ℓz = 3"
PK,0.5980392156862745,"2z⊤A−1
ℓz,"
PK,0.5990196078431372,"where (a) is the result of event J . Using this result and Proposition B.1, with probability at least
1 −
δ
4ℓ2|Z|,"
PK,0.6,"Ez,ℓ= |⟨z −z∗, bθℓ−θ⟩| ≤||z −z∗||eΩ
p"
PK,0.6009803921568627,2 log(8ℓ2|Z|/δ) ≤3
PK,0.6019607843137255,"2||z −z∗||A−1
ℓ
p"
PK,0.6029411764705882,2 log(8ℓ2|Z|/δ).
PK,0.6039215686274509,"Conditioned on event J , we have with probability 1 −
δ
4ℓ2|Z|,"
PK,0.6049019607843137,"|⟨z −z∗, bθℓ−θ⟩| ≤3"
PK,0.6058823529411764,2||z −z∗||P
PK,0.6068627450980392,"x∈V⌈τℓλℓ,x⌉xx⊤ bσ2x −1
p"
PK,0.6078431372549019,2 log(8ℓ2|Z|/δ) ≤3
PK,0.6088235294117647,2||z −z∗||P
PK,0.6098039215686275,"x∈X ⌈τℓλℓ,x⌉xx⊤ bσ2x −1
p"
PK,0.6107843137254902,2 log(8ℓ2|Z|/δ) ≤
PK,0.611764705882353,"3
2||z −z∗||P"
PK,0.6127450980392157,"x∈X λℓ,x xx⊤ bσ2x −1 √τℓ p"
PK,0.6137254901960785,2 log(8ℓ2|Z|/δ) =
PK,0.6147058823529412,"v
u
u
u
t"
PK,0.615686274509804,"3
2||z −z∗||2P"
PK,0.6166666666666667,"x∈X λℓ,x xx⊤ bσ2x −1 2
  3"
PK,0.6176470588235294,"2

ϵ−2
ℓq {Y(Zℓ)} log(8ℓ2|Z|/δ) p"
PK,0.6186274509803922,2 log(8ℓ2|Z|/δ) ≤ϵℓ.
PK,0.6196078431372549,"Applying a union bound, we find that"
PK,0.6205882352941177,"P {(∩∞
ℓ=1 ∩z∈ZℓEz,ℓ|J ) ∩J } = 1 −P
 
∪∞
ℓ=1 ∪z∈ZℓEc
z,ℓ|J

∪J c ≤1 − ( ∞
X ℓ=1 X"
PK,0.6215686274509804,"z∈Zℓ
P(Ec
z,ℓ|J ) + P(J c) ) = 1 − ∞
X ℓ=1 X z∈Zℓ"
PK,0.6225490196078431,"δ
4ℓ2|Z| + δ 2 !"
PK,0.6235294117647059,"≤1 −δ,"
PK,0.6245098039215686,"which proves the result.
■"
PK,0.6254901960784314,"Now we establish that, with probability 1 −δ for δ ∈(0, 1), the best arm z∗will not be eliminated
from Zℓfor any round ℓ∈N."
PK,0.6264705882352941,"Lemma E.2. With probability at least 1 −δ for δ ∈(0, 1), for any ℓ∈N and z′ ∈Zℓ,"
PK,0.6274509803921569,"⟨z′ −z∗, bθℓ⟩≤ϵℓ."
PK,0.6284313725490196,"Proof. We know that with probability 1 −δ for all ℓ∈N and z′ ∈Zℓ,"
PK,0.6294117647058823,"⟨z′ −z∗, bθℓ⟩= ⟨z′ −z∗, bθℓ−θ∗⟩+ ⟨z′ −z∗, θ∗⟩"
PK,0.6303921568627451,"≤⟨z′ −z∗, bθℓ−θ∗⟩
≤ϵℓ,"
PK,0.6313725490196078,"where the last inequality follows from Lemma E.1.
■"
PK,0.6323529411764706,"Then, we establish that in round ℓ, items in Zℓthat have mean rewards farther than 2ϵℓfrom the
highest reward will be eliminated."
PK,0.6333333333333333,"Lemma E.3. With probability at least 1 −δ for δ ∈(0, 1), for any ℓ∈N and z ∈Zℓsuch that
⟨z∗−z, θ∗⟩> 2ϵℓ,"
PK,0.634313725490196,"max
z′∈Zℓ⟨z′ −z, bθℓ⟩> ϵℓ."
PK,0.6352941176470588,"Proof. We know that with probability 1 −δ for all ℓ∈N,"
PK,0.6362745098039215,"max
z′∈Zℓ⟨z′ −z, bθℓ⟩≥⟨z∗−z, bθℓ⟩"
PK,0.6372549019607843,"= ⟨z∗−z, bθℓ−θ∗⟩+ ⟨z∗−z, θ∗⟩"
PK,0.638235294117647,"(a)
> −ϵℓ+ 2ϵℓ
= ϵℓ,"
PK,0.6392156862745098,"where (a) follows from Lemma E.1.
■"
PK,0.6401960784313725,"E.2.2
Proof of Sample Complexity"
PK,0.6411764705882353,"Overview of Sample Complexity Proof. In this section, we prove an upper bound on the sample
complexity of Algorithm 2 for transductive best-arm identification with linear bandits. We begin
by establishing some basic facts about the partial ordering of Hermetian positive definite matrices
in Lemmas E.4 and E.5. Let A and B be Hermitian positive definite matrices in Rd×d and define
A ⪰B if (A −B) is positive definite."
PK,0.6421568627450981,"Lemma E.4. Let A and B be Hermetian Positive Definite matrices in Rd×d,"
PK,0.6431372549019608,A ⪰B ⇐⇒I ⪰A−1/2BA−1/2.
PK,0.6441176470588236,Proof. For any x ∈Rd we define y = A1/2x and
PK,0.6450980392156863,x⊤Ax = x⊤A1/2A−1/2AA−1/2A1/2x
PK,0.6460784313725491,= y⊤Iy
PK,0.6470588235294118,= y⊤A−1/2AA−1/2y ≥x⊤Bx
PK,0.6480392156862745,= y⊤A−1/2BA−1/2y
PK,0.6490196078431373,"■
Lemma E.5. Let A and B be Hermitian positive definite matrices in Rd×d,"
PK,0.65,A ⪰B ⇐⇒B−1 ⪰A−1.
PK,0.6509803921568628,Proof. We use Lemma E.4 twice in the following proof.
PK,0.6519607843137255,A ⪰B ⇐⇒I ⪰A−1/2BA−1/2
PK,0.6529411764705882,⇐⇒I ⪰B1/2A−1/2A−1/2B1/2
PK,0.653921568627451,⇐⇒I ⪰B1/2A−1B1/2
PK,0.6549019607843137,⇐⇒B−1 ⪰A−1.
PK,0.6558823529411765,"where line 2 follows because A−1/2BA−1/2 is similar to B1/2A−1/2A−1/2B1/2 and both are
Hermetian positive definite.
■"
PK,0.6568627450980392,"We now use Lemmas E.4 and E.5 in our problem context. For V ⊂Rd, define g : (V, PX ) →R such
that g(v, λ) = ∥v∥(P"
PK,0.657843137254902,"x λxxx⊤/bσ2x)−1, f : (V, PX ) →R such that f(v, λ) = ∥v∥(P"
PK,0.6588235294117647,x λxxx⊤/σ2x)−1 and
PK,0.6598039215686274,"q∗(V) = min
λ∈PX max
v∈V f(v, λ),
q(V) = min
λ∈PX max
v∈V g(v, λ)."
PK,0.6607843137254902,"Note that the function, q, is also defined and leveraged in Algorithm 2. Lemma E.6 establishes a
multiplicative bound of the form"
PK,0.6617647058823529,"(1 −CΓ,δ) g(v, λ) ≤f(v, λ) ≤(1 + CΓ,δ) g(v, λ)"
PK,0.6627450980392157,using Lemmas D.1 and E.5.
PK,0.6637254901960784,"Lemma E.6. Define bσ2
x = min
n
max
n
x⊤bΣΓx, σ2
min
o
, σ2
max
o
. For a given λ, define"
PK,0.6647058823529411,"Υ = diag({σ2
x}x∈X ) × diag({1/λx}x∈X ), bΥ−1 = diag({bσ2
x}x∈X ) × diag({1/λx}x∈X )."
PK,0.6656862745098039,"Assume CΓ,δ < 1 and v ∈Rd, then with probability 1 −δ/2 for δ ∈(0, 1) the following holds true:"
PK,0.6666666666666666,"(1 −CΓ,δ) v⊤(X⊤bΥ−1X)−1v ≤v⊤(X⊤Υ−1X)−1v"
PK,0.6676470588235294,"≤(1 + CΓ,δ) v⊤(X⊤bΥ−1X)−1v,"
PK,0.6686274509803921,or equivalently
PK,0.6696078431372549,"(1 −CΓ,δ) g(v, λ) ≤f(v, λ) ≤(1 + CΓ,δ) g(v, λ)."
PK,0.6705882352941176,"Proof. Using Lemma D.1, we establish that"
PK,0.6715686274509803,v⊤X⊤bΥ−1Xv = v⊤X⊤bΥ−1ΥΥ−1Xv
PK,0.6725490196078432,= v⊤X⊤Υ−1/2 bΥ−1ΥΥ−1/2Xv
PK,0.6735294117647059,"≤(1 + CΓ,δ) v⊤X⊤Υ−1Xv and"
PK,0.6745098039215687,v⊤X⊤bΥ−1Xv = v⊤X⊤bΥ−1ΥΥ−1Xv
PK,0.6754901960784314,= v⊤X⊤Υ−1/2 bΥ−1ΥΥ−1/2Xv
PK,0.6764705882352942,"≥(1 −CΓ,δ) v⊤X⊤Υ−1Xv."
PK,0.6774509803921569,"Leveraging the partial ordering of Hermitian positive definite matrices and Lemma E.5, we can then
represent these inequalities as"
PK,0.6784313725490196,"(1 −CΓ,δ) X⊤Υ−1X ⪯X⊤bΥ−1X ⪯(1 + CΓ,δ) X⊤Υ−1X ⇐⇒
1
1 −CΓ,δ"
PK,0.6794117647058824," 
X⊤Υ−1X
−1 ⪰

X⊤bΥ−1X
−1
⪰
1
1 + CΓ,δ"
PK,0.6803921568627451," 
X⊤Υ−1X
−1 ."
PK,0.6813725490196079,"This implies that
 
X⊤Υ−1X
−1 ⪰(1 −CΓ,δ)

X⊤bΥ−1X
−1"
PK,0.6823529411764706,"and
 
X⊤Υ−1X
−1 ⪯(1 + CΓ,δ)

X⊤bΥ−1X
−1
."
PK,0.6833333333333333,"By combining these inequalities we arrive at the result.
■"
PK,0.6843137254901961,"We now want to use the multiplicative bound of Lemma E.6 to define the relationship between
q(V) and q∗(V) and eventually relate the sample complexity upper bound of Alg. 2 to the lower
bound established in Theorem 4.1. In order to accomplish this goal, we prove some general results
concerning minimax problems in Lemmas E.7 and E.8.
Lemma E.7. For c > 0, v ∈Rd and λ ∈PX , if"
PK,0.6852941176470588,"(1 −c)g(v, λ) ≤f(v, λ) ≤(1 + c)g(v, λ), then"
PK,0.6862745098039216,"(1 −c) max
v∈V g(v, λ) ≤max
v∈V f(v, λ)"
PK,0.6872549019607843,"≤(1 + c) max
v∈V g(v, λ)."
PK,0.6882352941176471,"Proof. For a given λ ∈PX , define v†(λ) = arg maxv∈V g(v, λ) and v∗(λ) = arg maxv∈V f(v, λ),
then"
PK,0.6892156862745098,"(1 −c)g

v†(λ), λ
	
≤f

v†(λ), λ"
PK,0.6901960784313725,"≤f {v∗(λ), λ}
≤(1 + c)g {v∗(λ), λ}"
PK,0.6911764705882353,"≤(1 + c)g

v†(λ), λ
	
."
PK,0.692156862745098,"■
Lemma E.8. For c > 0, v ∈Rd and λ ∈PX , if"
PK,0.6931372549019608,"(1 −c)g(v, λ) ≤f(v, λ) ≤(1 + c)g(v, λ), then"
PK,0.6941176470588235,"(1 −c) min
λ∈PX max
v∈V g(v, λ) ≤min
λ∈PX max
v∈V f(v, λ)"
PK,0.6950980392156862,"≤(1 + c) min
λ∈PX max
v∈V g(v, λ)."
PK,0.696078431372549,"Proof. For a given λ ∈PX , define v†(λ) = arg maxv∈V g(v, λ), v∗(λ) = arg maxv∈V f(v, λ),
λ∗= arg minλ∈PX maxv∈V f(v, λ) and λ† = arg minλ∈PX maxv∈V g(v, λ), then"
PK,0.6970588235294117,"(1 −c)g

v†(λ†), λ†	
≤(1 −c)g

v†(λ∗), λ∗"
PK,0.6980392156862745,"(a)
≤f {v∗(λ∗), λ∗}"
PK,0.6990196078431372,"≤f

v∗(λ†), λ†"
PK,0.7,"≤(1 + c)g

v∗(λ†), λ†"
PK,0.7009803921568627,"(b)
≤(1 + c)g

v†(λ†), λ†"
PK,0.7019607843137254,"where lines (a) and (b) follows from Lemma E.7.
■"
PK,0.7029411764705882,"Leveraging Lemma E.8, we bound q∗(V) in terms of q(V) in Lemma E.9."
PK,0.703921568627451,"Lemma E.9. With probability 1 −δ/2 for δ ∈(0, 1),"
PK,0.7049019607843138,"(1 −CΓ,δ) q(V) ≤q∗(V) ≤(1 + CΓ,δ) q(V)."
PK,0.7058823529411765,"Proof. From Lemma E.6, we know that"
PK,0.7068627450980393,"(1 −CΓ,δ) g(v, λ) ≤f(v, λ) ≤(1 + CΓ,δ) g(v, λ)."
PK,0.707843137254902,"According to Lemma E.8, this implies"
PK,0.7088235294117647,"(1 −CΓ,δ) min
λ∈PX max
v∈V g(v, λ) ≤min
λ∈PX max
v∈V f(v, λ) ≤(1 + CΓ,δ) min
λ∈PX max
v∈V g(v, λ). ■"
PK,0.7098039215686275,"In round ℓof Algorithm 2, we sample arms in X according to a given design, bλℓ. Although other
rounding procedures are possible (see Proposition A.1), Theorem 4.2 assumes that we take ⌈τℓbλℓ,x⌉
samples of arm x ∈X to accomplish this goal. This results in more than τℓsamples being taken
in round ℓ. We define the sparsity of any bλℓdesign using Caratheodory’s Theorem to motivate the
number of extra samples needed by sampling in this manner."
PK,0.7107843137254902,"Lemma E.10. [Appendix 8, [44]] In Algorithm 2, the support of bλℓfor all ℓ∈N is d(d + 1)/2 + 1."
PK,0.711764705882353,"Proof. Any design matrix in Rd×d is symmetric and can consequently be expressed as a point in RM,
M = d(d + 1)/2. The design matrices leveraged in this paper belong to a convex hull of a subset of
points since they are a convex combination of xx⊤for x ∈X. According to Caratheodory’s theorem,
any point in the convex hull of any subset of points in RD can be defined as a convex combination
of D + 1 points. The optimal experimental designs in this paper can therefore be represented using
only M + 1 points [44]. This sparsity holds regardless of the design’s form and so applies to both the
homoskedastic and heteroskedastic variance settings.
■"
PK,0.7127450980392157,"Finally, we connect the sample complexity upper bound of Algorithm 2 to the lower bound for δ-PAC
algorithms established in Theorem 4.2 for transductive best-arm identification."
PK,0.7137254901960784,"Proof of Theorem 4.2 for transductive best-arm identification. Assume
that
maxz∈Zℓ|⟨z
−
z∗, θ∗⟩| ≤2. Define Sℓ= {z ∈Zℓ: ⟨z∗−z, θ∗⟩≤4ϵℓ}. Note that by assumption Z = Z1 = S1.
Lemma E.3 implies that with probability at least 1 −δ for δ ∈(0, 1), we know Zℓ⊂Sℓfor all ℓ∈N.
This means that"
PK,0.7147058823529412,"q(Y(Zℓ)) = inf
λ∈PX max
z,z′∈Zℓ||z −z′||2
(P"
PK,0.7156862745098039,x∈X λx xx⊤
PK,0.7166666666666667,bσ2x )−1
PK,0.7176470588235294,"≤inf
λ∈PX max
z,z′∈Sℓ||z −z′||2
(P"
PK,0.7186274509803922,x∈X λx xx⊤
PK,0.7196078431372549,bσ2x )−1
PK,0.7205882352941176,= q(Sℓ).
PK,0.7215686274509804,"For ℓ≥⌈log2(4∆−1)⌉, we know that Sℓ= {z∗}. Thus, the sample size needed to identify z∗is
upper bounded as follows."
PK,0.7225490196078431,"⌈log2(4∆−1)⌉
X ℓ=1 X"
PK,0.7235294117647059,"x∈X
⌈τℓbλℓ,x⌉
(a)
="
PK,0.7245098039215686,"⌈log2(4∆−1)⌉
X ℓ=1"
PK,0.7254901960784313,d(d + 1)
PK,0.7264705882352941,"2
+ τℓ  ="
PK,0.7274509803921568,"⌈log2(4∆−1)⌉
X ℓ=1"
PK,0.7284313725490196,d(d + 1)
PK,0.7294117647058823,"2
+ 2
3 2"
PK,0.7303921568627451,"
ϵ−2
ℓq {Y(Zℓ)} log(8ℓ2|Z|/δ)
"
PK,0.7313725490196078,≤d(d + 1)
PK,0.7323529411764705,"2
⌈log2(4∆−1)⌉+"
PK,0.7333333333333333,"⌈log2(4∆−1)⌉
X"
PK,0.734313725490196,"ℓ=1
3ϵ−2
ℓq(Sℓ) log(8ℓ2|Z|/δ)"
PK,0.7352941176470589,≤d(d + 1)
PK,0.7362745098039216,"2
⌈log2(4∆−1)⌉"
PK,0.7372549019607844,"+
3 log
8⌈log2(4∆−1)⌉2|Z| δ"
PK,0.7382352941176471," ⌈log2(4∆−1)⌉
X"
PK,0.7392156862745098,"ℓ=1
22ℓq(Sℓ),"
PK,0.7401960784313726,where (a) follows from Lemma E.10.
PK,0.7411764705882353,"We relate this quantity to the lower bound in Theorem 4.1 below. With probability 1 −δ,"
PK,0.7421568627450981,"ψ∗
BAI = inf
λ∈PX
max
z∈Z\{z∗}"
PK,0.7431372549019608,||z∗−z||2P
PK,0.7441176470588236,x∈X λx xx⊤ σ2x −1
PK,0.7450980392156863,{(z∗−z)⊤θ∗}2
PK,0.746078431372549,"= inf
λ∈PX
max
ℓ≤⌈log2(4∆−1)⌉max
z∈Sℓ"
PK,0.7470588235294118,||z∗−z||2P
PK,0.7480392156862745,x∈X λx xx⊤ σ2x −1
PK,0.7490196078431373,{(z∗−z)⊤θ∗}2
PK,0.75,"≥inf
λ∈PX
max
ℓ≤⌈log2(4∆−1)⌉max
z∈Sℓ"
PK,0.7509803921568627,||z∗−z||2P
PK,0.7519607843137255,x∈X λx xx⊤ σ2x −1
PK,0.7529411764705882,(4 × 2−ℓ)2
PK,0.753921568627451,"(a)
≥
1
⌈log2(4∆−1)⌉inf
λ∈PX"
PK,0.7549019607843137,"⌈log2(4∆−1)⌉
X"
PK,0.7558823529411764,"ℓ=1
max
z∈Sℓ"
PK,0.7568627450980392,||z∗−z||2P
PK,0.7578431372549019,x∈X λx xx⊤ σ2x −1
PK,0.7588235294117647,(4 × 2−ℓ)2
PK,0.7598039215686274,"(b)
≥
1
16⌈log2(4∆−1)⌉"
PK,0.7607843137254902,"⌈log2(4∆−1)⌉
X"
PK,0.7617647058823529,"ℓ=1
22ℓinf
λ∈PX max
z∈Sℓ||z∗−z||2P"
PK,0.7627450980392156,x∈X λx xx⊤ σ2x −1
PK,0.7637254901960784,"(c)
≥
1
64⌈log2(4∆−1)⌉"
PK,0.7647058823529411,"⌈log2(4∆−1)⌉
X"
PK,0.765686274509804,"ℓ=1
22ℓinf
λ∈PX max
z,z′∈Sℓ||z′ −z||2P"
PK,0.7666666666666667,x∈X λx xx⊤ σ2x −1
PK,0.7676470588235295,"(d)
≥
1
64⌈log2(4∆−1)⌉"
PK,0.7686274509803922,"⌈log2(4∆−1)⌉
X"
PK,0.7696078431372549,"ℓ=1
22ℓinf
λ∈PX max
z,z′∈Sℓ
1
2||z′ −z||2P"
PK,0.7705882352941177,x∈X λx xx⊤ bσ2x −1
PK,0.7715686274509804,"≥
1
128⌈log2(4∆−1)⌉"
PK,0.7725490196078432,"⌈log2(4∆−1)⌉
X"
PK,0.7735294117647059,"ℓ=1
22ℓinf
λ∈PX max
z,z′∈Sℓ||z′ −z||2P"
PK,0.7745098039215687,x∈X λx xx⊤ bσ2x −1
PK,0.7754901960784314,"=
1
128⌈log2(4∆−1)⌉"
PK,0.7764705882352941,"⌈log2(4∆−1)⌉
X"
PK,0.7774509803921569,"ℓ=1
22ℓq(Sℓ)"
PK,0.7784313725490196,"here (a) follows from the fact that the maximum of a set of numbers is always greater than the
average and (b) by the fact that the minimum of a sum is greater than the sum of the minimums. We
used the fact that"
PK,0.7794117647058824,"max
z,z′∈Sℓ||z′ −z||2P"
PK,0.7803921568627451,x∈X λx xx⊤ σ2x
PK,0.7813725490196078,"−1 ≤4 max
z∈Sℓ||z∗−z||2P"
PK,0.7823529411764706,x∈X λx xx⊤ σ2x −1
PK,0.7833333333333333,"by the triangle inequality in (c). Finally, (d) follows from Lemma E.9."
PK,0.7843137254901961,"Consequently, putting the previous pieces together we find that the sample complexity can be upper
bounded as"
PK,0.7852941176470588,"⌈log2(4∆−1)⌉
X ℓ=1 X"
PK,0.7862745098039216,"x∈X
⌈τℓbλℓ,x⌉≤d(d + 1)"
PK,0.7872549019607843,"2
⌈log2(4∆−1)⌉"
PK,0.788235294117647,"+
3 log
8⌈log2(4∆−1)⌉2|Z| δ"
PK,0.7892156862745098," ⌈log2(4∆−1)⌉
X"
PK,0.7901960784313725,"ℓ=1
22ℓq(Sℓ)"
PK,0.7911764705882353,≤d(d + 1)
PK,0.792156862745098,"2
⌈log2(4∆−1)⌉"
PK,0.7931372549019607,"+
384⌈log2(4∆−1)⌉log
8⌈log2(4∆−1)⌉2|Z| δ"
PK,0.7941176470588235,"
ψ∗
BAI,"
PK,0.7950980392156862,where the dependency on d(d + 1)/2 can reduced to d by Proposition A.1. This proves the result. ■
PK,0.796078431372549,"F
Proofs of Level Set Identification"
PK,0.7970588235294118,"This appendix contains the proofs for the results on transductive linear bandit level set identification.
In this problem, we are interested in identifying the set Gα(θ∗) = {z ∈Z : z⊤θ∗> α}, i.e.,
the items with mean rewards that exceed threshold α. We prove the lower bound on the sample
complexity from Theorem 4.1 in App. F.1. The proof of correctness and the sample complexity upper
bound for Alg. 2 are presented in App. F.2. These proofs resemble the transductive linear bandit
identification proofs with slight modifications accounting for the the problem structure. Recalling
Bα(θ∗) = {z ∈Z : z⊤θ∗< α}, we assume that Z = Gα(θ∗) ∪Bα(θ∗), which is equivalent to the
condition {z ∈Z : z⊤θ∗= α} = ∅. Let ∆= minz∈Z |z⊤θ∗−α| denote the minimum absolute
gap for the threshold α."
PK,0.7980392156862746,"F.1
Lower Bound"
PK,0.7990196078431373,"Proof of Theorem 4.1 for transductive level set identification. Label X ⊂Rd as the measurement
vectors such that K = |X| and Z as the decision item set. This proof is similar to the lower bound
proofs of [35] and [12]. Let C be the set of alternatives such that for eθ ∈C, Gα(θ∗) ̸= Gα(eθ). This
set of alternatives is decomposed by [35] as,"
PK,0.8,"C =

∪z∈Gα(θ∗){eθ : z⊤eθ < α}

∪

∪z /∈Gα(θ∗){eθ : z⊤eθ > α}

."
PK,0.8009803921568628,"We now recall Lemma 1 of [27]. Under a δ-PAC strategy for bandit instance (X, Z, θ∗), let Ti denote
the random variable that is the number of times arm i is pulled. Defining σ2
x = x⊤Σ∗x for all x ∈X,
we denote νθ,i as the reward distribution of the i-th arm of X, i.e., νθ,i = N
 
x⊤
i θ, σ2
xi

. For any
eθ ∈C, we know from [27] that K
X"
PK,0.8019607843137255,"i=1
E(Ti)KL(νθ∗,i||νeθ,i) ≥log(1/2.4δ)."
PK,0.8029411764705883,"Following the steps of [12], we find that K
X"
PK,0.803921568627451,"i=1
E(Ti) ≥log(1/2.4δ) min
λ∈PX max
eθ∈C"
PK,0.8049019607843138,"1
PK
i=1 λiKL(νθ∗,i||νeθ,i)
.
(18)"
PK,0.8058823529411765,"For λ ∈PX , define A(λ) := PK
i=1 λi
xix⊤
i
σ2xi . For ϵ > 0 and z ∈Gα(θ∗), define"
PK,0.8068627450980392,"θz(ϵ, λ) := θ∗−(θ∗⊤z −α + ϵ)A(λ)−1z"
PK,0.807843137254902,"∥z∥2
A(λ)−1
,"
PK,0.8088235294117647,"where z⊤θz(ϵ, λ) = α −ϵ, implying that θz(ϵ, λ) ∈C. For z ∈Gc
α(θ∗), define"
PK,0.8098039215686275,"θz(ϵ, λ) := θ∗−(θ∗⊤z −α −ϵ)A(λ)−1z"
PK,0.8107843137254902,"∥z∥2
A(λ)−1
,"
PK,0.8117647058823529,"where z⊤θz(ϵ, λ) = α + ϵ, implying that θz(ϵ, λ) ∈C."
PK,0.8127450980392157,"Defining ϵz = ϵ if z ∈Gα(θ∗) and ϵz = −ϵ if z ∈Gc
α(θ∗), we find that the KL divergence between
νθ,i and νθz(ϵ,λ),i is given by:"
PK,0.8137254901960784,"KL(νθ,i||νθz(ϵ,λ),i) ="
PK,0.8147058823529412,"
x⊤
i {θ∗−θz(ϵ, λ)}
2 2σ2xi"
PK,0.8156862745098039,"= z⊤A(λ)−1 (z⊤θ∗−α + ϵz)2 xix⊤
i
2σ2
xi
{z⊤A(λ)−1z}2
A(λ)−1z."
PK,0.8166666666666667,"Returning to Eq. 18, K
X"
PK,0.8176470588235294,"i=1
E(Ti) ≥log(1/2.4δ) min
λ∈PX max
eθ∈C"
PK,0.8186274509803921,"1
PK
i=1 λiKL(νθ∗,i||νeθ,i)"
PK,0.8196078431372549,"≥log(1/2.4δ) min
λ∈PX max
z∈Z
1
PK
i=1 λiKL(νθ∗,i||νθz(ϵz,λ),i)"
PK,0.8205882352941176,"≥log(1/2.4δ) min
λ∈PX max
z∈Z"
PK,0.8215686274509804,"
z⊤A(λ)−1z
	2"
PK,0.8225490196078431,"PK
i=1(z⊤θ∗−α + ϵz)2z⊤A(λ)−1λi
xix⊤
i
2σ2xi A(λ)−1z"
PK,0.8235294117647058,"(a)
= 2 log(1/2.4δ) min
λ∈PX max
z∈Z"
PK,0.8245098039215686,"
z⊤A(λ)−1z
	2"
PK,0.8254901960784313,"(z⊤θ∗−α + ϵz)2z⊤A(λ)−1
PK
i=1 λi
xix⊤
i
σ2xi"
PK,0.8264705882352941,"
A(λ)−1z"
PK,0.8274509803921568,"= 2 log(1/2.4δ) min
λ∈PX max
z∈Z
z⊤A(λ)−1z
(z⊤θ∗−α + ϵz)2 ,"
PK,0.8284313725490197,"where (a) uses the fact that PK
i=1 λi
xix⊤
i
σ2xi = A(λ). Letting ϵz →0 establishes the result.
■"
PK,0.8294117647058824,"F.2
Upper Bound"
PK,0.8303921568627451,"We now prove the upper bound on the sample complexity of Alg. 2 for level set estimation. This proof
follows similarly to the upper bound on the sample complexity of Alg. 2 for best-arm identification
shown in App. E.2. Note that in each round ℓ∈N, Alg. 2 maintains a set Zℓof undecided items, a
set Gℓof items estimated to have value exceeding the threshold α, and a set Bℓof items estimated to
have value falling below the threshold α."
PK,0.8313725490196079,"F.2.1
Proof of Correctness"
PK,0.8323529411764706,"To begin, we show that that for all rounds ℓ∈N of Alg. 2, the error in the estimates of z⊤θ∗for all
z ∈Z are bounded by ϵℓwith probability at least 1 −δ for δ ∈(0, 1)."
PK,0.8333333333333334,"Lemma F.1. For all ℓ∈N and z ∈Z, with probability at least 1 −δ for δ ∈(0, 1),"
PK,0.8343137254901961,"|⟨z, bθℓ−θ⟩| ≤ϵℓ."
PK,0.8352941176470589,"Proof. This proof follows identically to Lemma E.1 by replacing z∗with the zero vector.
■"
PK,0.8362745098039216,"We now apply the preceding result to show that with probability 1 −δ for δ ∈(0, 1), Alg. 2 does not
place items in the wrong category."
PK,0.8372549019607843,"Lemma F.2. With probability at least 1 −δ for δ ∈(0, 1), for any z ∈Zℓ, ℓ∈N, such that
z⊤θ∗> α,"
PK,0.8382352941176471,"⟨z, bθℓ⟩+ ϵℓ> α,"
PK,0.8392156862745098,"and for any z ∈Zℓsuch that z⊤θ∗< α,"
PK,0.8401960784313726,"⟨z, bθℓ⟩−ϵℓ< α."
PK,0.8411764705882353,"Proof. Consider z ∈Gα so that z⊤θ∗> α. Using Lemma F.1, with probability at least 1 −δ for
δ ∈(0, 1) we have"
PK,0.842156862745098,"⟨z, bθℓ⟩+ ϵℓ= ⟨z, bθℓ−θ∗⟩+ ϵℓ+ ⟨z, θ∗⟩"
PK,0.8431372549019608,"> ⟨z, bθℓ−θ∗⟩+ ϵℓ+ α
≥α."
PK,0.8441176470588235,"By the procedure of Alg. 2, this guarantees Gℓ+1 ⊆Gα."
PK,0.8450980392156863,"Now, consider z ∈Bα so that z⊤θ∗< α. Using Lemma F.1, with probability at least 1 −δ for
δ ∈(0, 1) we have"
PK,0.846078431372549,"⟨z, bθℓ⟩−ϵℓ= ⟨z, bθℓ−θ∗⟩−ϵℓ+ ⟨z, θ∗⟩"
PK,0.8470588235294118,"< ⟨z, bθℓ−θ∗⟩−ϵℓ+ α
≤α."
PK,0.8480392156862745,"By the procedure of Alg. 2, this guarantees Bℓ+1 ⊆Bα.
■"
PK,0.8490196078431372,"We now show that any z ∈Z such that |z⊤θ∗−α| > 2ϵℓis removed from the active set, Zℓ, in
round ℓ∈N of Alg. 2 with probability at least 1 −δ for δ ∈(0, 1). Since ϵℓ→0, this proves that we
will eventually identify Gα correctly."
PK,0.85,"Lemma F.3. With probability at least 1 −δ for δ ∈(0, 1), for any ℓ∈N and z ∈Zℓsuch that
|⟨z, θ∗⟩−α| > 2ϵℓ,"
PK,0.8509803921568627,"|⟨z, bθℓ⟩−α| > ϵℓ."
PK,0.8519607843137255,"Proof. Set δ ∈(0, 1). Let us begin by considering any z ∈Zℓsuch that ⟨z, θ∗⟩−α > 2ϵℓ. Using
Lemma F.1, with probability at least 1 −δ,"
PK,0.8529411764705882,"⟨z, bθℓ⟩−ϵℓ= ⟨z, bθℓ−θ∗⟩+ ⟨z, θ∗⟩−ϵℓ
≥−ϵℓ+ ⟨z, θ∗⟩−ϵℓ
> α."
PK,0.8539215686274509,"Now, consider any z ∈Zℓsuch that ⟨z, θ∗⟩−α < −2ϵℓ. Using Lemma F.1, with probability at least
1 −δ,"
PK,0.8549019607843137,"⟨z, bθℓ⟩+ ϵℓ= ⟨z, bθℓ−θ∗⟩+ ⟨z, θ∗⟩+ ϵℓ
≤ϵℓ+ ⟨z, θ∗⟩+ ϵℓ
< α."
PK,0.8558823529411764,"Hence, for z ∈Zℓsuch that |⟨z, θ∗⟩−α| > 2ϵℓ, we have that |⟨z, bθℓ⟩−ϵℓ| > α with probability at
least 1 −δ.
■"
PK,0.8568627450980392,"By the elimination conditions of Alg. 2, this guarantees that the z ∈Zℓsuch that |⟨z, θ∗⟩−α| > 2ϵℓ
are not included in Zℓ+1."
PK,0.8578431372549019,"F.2.2
Proof of Upper Bound on Sample Complexity"
PK,0.8588235294117647,"Now we prove the upper bound on the sample complexity of Alg. 2 for the transductive level set
identification problem."
PK,0.8598039215686275,"Proof of Theorem 4.2 for transductive level set identification. Set δ
∈
(0, 1).
Assume that
maxz∈Zℓ|⟨z, θ∗⟩−α| ≤2. Define Sℓ= {z ∈Zℓ: |⟨z, θ∗⟩−α| ≤4ϵℓ} for ℓ∈N. Note that
by assumption Z = S1 = Z1. Lemma F.3 implies that with probability at least 1 −δ, we know
Zℓ⊂Sℓfor all ℓ∈N. This means"
PK,0.8607843137254902,"q(Zℓ) = inf
λ∈PX max
z∈Zℓ||z||2P"
PK,0.861764705882353,x∈X λx xx⊤ bσ2x −1
PK,0.8627450980392157,"≤inf
λ∈PX max
z∈Sℓ||z||2P"
PK,0.8637254901960785,x∈X λx xx⊤ bσ2x −1
PK,0.8647058823529412,= q(Sℓ).
PK,0.865686274509804,"For ℓ≥⌈log2(4∆−1)⌉, we know that Sℓ= ∅, thus the sample complexity to identify the set Gα is
upper bounded as follows."
PK,0.8666666666666667,"⌈log2(4∆−1)⌉
X ℓ=1 X"
PK,0.8676470588235294,"x∈X
⌈τℓbλℓ,x⌉
(a)
="
PK,0.8686274509803922,"⌈log2(4∆−1)⌉
X ℓ=1"
PK,0.8696078431372549,d(d + 1)
PK,0.8705882352941177,"2
+ τℓ  ="
PK,0.8715686274509804,"⌈log2(4∆−1)⌉
X ℓ=1"
PK,0.8725490196078431,d(d + 1)
PK,0.8735294117647059,"2
+ 2
3 2"
PK,0.8745098039215686,"
ϵ−2
ℓq(Zℓ) log(8ℓ2|Z|/δ)
"
PK,0.8754901960784314,≤d(d + 1)
PK,0.8764705882352941,"2
⌈log2(4∆−1)⌉+"
PK,0.8774509803921569,"⌈log2(4∆−1)⌉
X"
PK,0.8784313725490196,"ℓ=1
3ϵ−2
ℓq(Sℓ) log(8ℓ2|Z|/δ)"
PK,0.8794117647058823,≤d(d + 1)
PK,0.8803921568627451,"2
⌈log2(4∆−1)⌉+"
LOG,0.8813725490196078,"3 log
8⌈log2(4∆−1)⌉2|Z| δ"
LOG,0.8823529411764706," ⌈log2(4∆−1)⌉
X"
LOG,0.8833333333333333,"ℓ=1
22ℓq(Sℓ),"
LOG,0.884313725490196,"where (a) follows from Lemma E.10. We now relate this quantity to the lower bound and explain
below. With probability 1 −δ,"
LOG,0.8852941176470588,"ψ∗
LS = inf
λ∈PX max
z∈Z"
LOG,0.8862745098039215,||z||2P
LOG,0.8872549019607843,x∈X λx xx⊤ σ2x −1
LOG,0.888235294117647,(z⊤θ∗−α)2
LOG,0.8892156862745098,"= inf
λ∈PX
max
ℓ≤⌈log2(4∆−1)⌉max
z∈Sℓ"
LOG,0.8901960784313725,||z||2P
LOG,0.8911764705882353,x∈X λx xx⊤ σ2x −1
LOG,0.8921568627450981,(z⊤θ∗−α)2
LOG,0.8931372549019608,"≥inf
λ∈PX
max
ℓ≤⌈log2(4∆−1)⌉max
z∈Sℓ"
LOG,0.8941176470588236,||z||2P
LOG,0.8950980392156863,x∈X λx xx⊤ σ2x −1
LOG,0.8960784313725491,(4 × 2−ℓ)2
LOG,0.8970588235294118,"(a)
≥
1
16⌈log2(4∆−1)⌉inf
λ∈PX"
LOG,0.8980392156862745,"⌈log2(4∆−1)⌉
X"
LOG,0.8990196078431373,"ℓ=1
22ℓmax
z∈Sℓ||z||2P"
LOG,0.9,x∈X λx xx⊤ σ2x −1
LOG,0.9009803921568628,"(b)
≥
1
16⌈log2(4∆−1)⌉"
LOG,0.9019607843137255,"⌈log2(4∆−1)⌉
X"
LOG,0.9029411764705882,"ℓ=1
22ℓinf
λ∈PX max
z∈Sℓ||z||2P"
LOG,0.903921568627451,x∈X λx xx⊤ σ2x −1
LOG,0.9049019607843137,"(c)
≥
1
32⌈log2(4∆−1)⌉"
LOG,0.9058823529411765,"⌈log2(4∆−1)⌉
X"
LOG,0.9068627450980392,"ℓ=1
22ℓinf
λ∈PX max
z∈Sℓ||z||2P"
LOG,0.907843137254902,x∈X λx xx⊤ bσ2x −1
LOG,0.9088235294117647,"=
1
32⌈log2(4∆−1)⌉"
LOG,0.9098039215686274,"⌈log2(4∆−1)⌉
X"
LOG,0.9107843137254902,"ℓ=1
22ℓq(Sℓ). ■"
LOG,0.9117647058823529,"where (a) follows from the fact that the maximum of a set of numbers is always greater than the
average, (b) by the fact that the minimum of a sum is greater than the sum of the minimums, and
(c) from Lemma E.9. Consequently, putting the previous pieces together we find that the sample"
LOG,0.9127450980392157,complexity can be upper bounded as
LOG,0.9137254901960784,"⌈log2(4∆−1)⌉
X ℓ=1 X"
LOG,0.9147058823529411,"x∈X
⌈τℓbλℓ,x⌉≤d(d + 1)"
LOG,0.9156862745098039,"2
⌈log2(4∆−1)⌉"
LOG,0.9166666666666666,"+
3 log
8⌈log2(4∆−1)⌉2|Z| δ"
LOG,0.9176470588235294," ⌈log2(4∆−1)⌉
X"
LOG,0.9186274509803921,"ℓ=1
22ℓq(Sℓ)"
LOG,0.9196078431372549,≤d(d + 1)
LOG,0.9205882352941176,"2
⌈log2(4∆−1)⌉"
LOG,0.9215686274509803,"+
96⌈log2(4∆−1)⌉log
8⌈log2(4∆−1)⌉2|Z| δ"
LOG,0.9225490196078432,"
ψ∗
LS,"
LOG,0.9235294117647059,where the dependency on d(d + 1)/2 can reduced to d by Proposition A.1. This proves the result.
LOG,0.9245098039215687,"G
Comparing Identification Lower Bounds"
LOG,0.9254901960784314,"In this section, we use the general notation for the level set and best-arm identification problems
developed in Section 4. Define"
LOG,0.9264705882352942,"ρ∗
OBJ := σ2
max min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
LOG,0.9274509803921569,"||h −q||2
(
P"
LOG,0.9284313725490196,"x∈X λxxx⊤)
−1"
LOG,0.9294117647058824,"{(h −q)⊤θ∗−bOBJ}2 ,"
LOG,0.9303921568627451,"which is the lower bound for both BAI and LS by [12] and [35] in the homoskedastic case when
upper bounding the variance.
Lemma G.1. In the same setting as Theorem 4.1,"
LOG,0.9313725490196079,"σ2
min · ρ∗
OBJ ≤ψ∗
OBJ ≤σ2
max · ρ∗
OBJ,"
LOG,0.9323529411764706,and both the upper and lower bounds are tight.
LOG,0.9333333333333333,"Proof. Consider semidefinite matrices A, B, and C in Rd×d such that A ⪯B ⪯C. Then C−1 ⪯
B−1 ⪯A−1. Hence, for x ∈Rd, ∥x∥C−1 ≤∥x∥B−1 ≤∥x∥A−1. Defining"
LOG,0.9343137254901961,"A(λ) := K
X"
LOG,0.9352941176470588,"i=1
λi
xix⊤
i
σ2xi
,"
LOG,0.9362745098039216,"we note that
1
σ2max K
X"
LOG,0.9372549019607843,"i=1
λixix⊤
i ⪯A(λ) ⪯
1
σ2
min K
X"
LOG,0.9382352941176471,"i=1
λixix⊤
i ."
LOG,0.9392156862745098,"Hence,
σ2
min∥x∥2
(
PK
i=1 λixix⊤
i )
−1 ≤∥x∥2
A(λ)−1 ≤σ2
max∥x∥2
(
PK
i=1 λixix⊤
i )
−1."
LOG,0.9401960784313725,"Applying this to the result of Theorem 4.1 and invoking Lemma E.8, we see that"
LOG,0.9411764705882353,"σ2
min min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
LOG,0.942156862745098,"||h −q||2
(P"
LOG,0.9431372549019608,x∈X λxxx⊤)−1
LOG,0.9441176470588235,{(h −q)⊤θ∗−bOBJ}2
LOG,0.9450980392156862,"≤min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
LOG,0.946078431372549,"||h −q||2
A(λ)−1"
LOG,0.9470588235294117,{(h −q)⊤θ∗−bOBJ}2
LOG,0.9480392156862745,"≤σ2
max min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
LOG,0.9490196078431372,"||h −q||2
(P"
LOG,0.95,x∈X λxxx⊤)−1
LOG,0.9509803921568627,{(h −q)⊤θ∗−bOBJ}2 .
LOG,0.9519607843137254,Recalling that
LOG,0.9529411764705882,"ρ∗
OBJ := σ2
max min
λ∈PX
max
h∈HOBJ,q∈QOBJ"
LOG,0.953921568627451,"||h −q||2
(P"
LOG,0.9549019607843138,x∈X λxxx⊤)−1
LOG,0.9558823529411765,{(h −q)⊤θ∗−bOBJ}2
LOG,0.9568627450980393,"is the lower bound in both the best arm and level set problems for the homoskedastic variance
algorithm in a heteroskedastic noise setting (when upper bounding the variance), we see that"
LOG,0.957843137254902,"κ−1 ≤ψ∗
OBJ/ρ∗
OBJ ≤1."
LOG,0.9588235294117647,"Moreover, both the upper and lower bounds are tight by taking σ2
x = σ2
min, ∀x ∈X (to match the
lower bound) and σ2
x = σ2
max, ∀x ∈X (to match the upper bound).
■"
LOG,0.9598039215686275,"H
Experiment Details"
LOG,0.9607843137254902,This Appendix gives more details on the experiments presented in Section 5.
LOG,0.961764705882353,"H.1
Oracle Designs"
LOG,0.9627450980392157,"We demonstrate the difference in oracle allocations referenced by Experiments 1-3 in Section 5.
We begin with Experiment 1 in Fig. 4, where the oracle distribution that assumes homoskedastic
variance devotes more attention to the informative directions with smaller magnitude. In contrast, the
heteroskedastic oracle balances evenly because each informative arm has an equivalent signal-noise
ratio when accounting for variance."
LOG,0.9637254901960784,"Next, we analyze Experiment 2 in Fig. 5. In this setting the oracle distribution that assumes
homoskedastic variance allocates evenly between three informative arms, while the heteroskedastic
oracle prioritizes the informative directions with less variance."
LOG,0.9647058823529412,"1
2
3
Informative Arms 0.0 0.1 0.2 0.3 0.4"
LOG,0.9656862745098039,Density
LOG,0.9666666666666667,"Homoskedastic
Heteroskedastic"
LOG,0.9676470588235294,"Figure 4: In Experiment 1, the Heteroskedastic Ora-
cle balances evenly between the informative arms."
LOG,0.9686274509803922,"1
2
3
4
Informative Arms 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
LOG,0.9696078431372549,Density
LOG,0.9705882352941176,"Homoskedastic
Heteroskedastic"
LOG,0.9715686274509804,"Figure 5: In Experiment 2, the Heteroskedastic Ora-
cle prioritizes informative arms with less variance."
LOG,0.9725490196078431,"Lastly, we examine the oracle distributions for the multivariate testing experiment in Fig. 6. Note
that the first four arm have higher variance than the last four arms. We can see in Fig. 6 that the
Heteroskedastic Oracle prioritizes the second group of arms because these will require more samples
to identify their effect sizes."
LOG,0.9735294117647059,"H.2
Assuming Homosekedastic Noise"
LOG,0.9745098039215686,"If we assume that there is homoskedastic noise in a heteroskedastic setting, then we need to upper-
bound the noise with σ2
max in the sample size calculation of Alg. 2 in order to maintain correctness.
In Section 5, we compare H-RAGE to RAGE; the latter uses the σ2
max upper-bound and is described in
Alg. 5."
LOG,0.9754901960784313,"1
2
3
4
5
6
7
8
Arms 0.0 0.1 0.2 0.3 0.4"
LOG,0.9764705882352941,Density
LOG,0.9774509803921568,"Homoskedastic
Heteroskedastic"
LOG,0.9784313725490196,"Figure 6: In the multivariate testing experiment, the Heteroskedastic Oracle prioritizes the arms that have higher
variance, i.e., the arms that include the second variation in the first dimension."
LOG,0.9794117647058823,Algorithm 5: (RAGE) Randomized Adaptive Gap Elimination
LOG,0.9803921568627451,Result: Find z∗:= arg maxz∈Z z⊤θ∗for BAI or Gα := {z ∈Z : z⊤θ∗> α} for LS
LOG,0.9813725490196078,"1 Input: X ∈Rd, Z ∈Rd, confidence δ ∈(0, 1), OBJ ∈{BAI,LS}, threshold α ∈R"
LOG,0.9823529411764705,"2 Initialize: ℓ←1, Z1 ←Z, G1 ←∅, B1 ←∅"
LOG,0.9833333333333333,3 while (|Zℓ| > 1 and OBJ=BAI) or (|Zℓ| > 0 and OBJ=LS) do
LOG,0.984313725490196,"4
Let bλℓ∈PX be a minimizer of q {λ, Y(Zℓ)} if OBJ=BAI and q(λ, Zℓ) if OBJ=LS where"
LOG,0.9852941176470589,"q(V) = infλ∈PX q(λ; V) = infλ∈PX maxz∈V ||z||2
(P"
LOG,0.9862745098039216,x∈X λxxx⊤)−1
LOG,0.9872549019607844,"5
Set ϵℓ= 2−ℓ, τℓ= 2ϵ−2
ℓσ2
maxq(Zℓ) log(8ℓ2|Z|/δ)"
LOG,0.9882352941176471,"6
Pull arm x ∈X exactly ⌈τℓbλℓ,x⌉times for nℓsamples and collect {xℓ,i, yℓ,i}nℓ
i=1"
LOG,0.9892156862745098,"7
Define Aℓ:= Pnℓ
i=1 xℓ,ix⊤
ℓ,i, bℓ= Pnℓ
i=1 xℓ,iyℓ,i and construct bθℓ= A−1
ℓbℓ"
IF OBJ IS BAI THEN,0.9901960784313726,"8
if OBJ is BAI then"
IF OBJ IS BAI THEN,0.9911764705882353,"9
Zℓ+1 ←Zℓ\{z ∈Zℓ: maxz′∈Zℓ⟨z′ −z, bθℓ⟩> ϵℓ}"
ELSE,0.9921568627450981,"10
else"
ELSE,0.9931372549019608,"11
Gℓ+1 ←Gℓ∪{z ∈Zℓ: ⟨z, bθℓ⟩−ϵℓ> α}"
ELSE,0.9941176470588236,"12
Bℓ+1 ←Bℓ∪{z ∈Zℓ: ⟨z, bθℓ⟩+ ϵℓ< α}"
ELSE,0.9950980392156863,"13
Zℓ+1 ←Zℓ\{{Gℓ\ Gℓ+1} ∪{Bℓ\ Bℓ+1}}"
END,0.996078431372549,"14
end"
END,0.9970588235294118,"15
ℓ←ℓ+ 1"
END,0.9980392156862745,16 end
END,0.9990196078431373,17 Output: Zℓfor BAI or Gℓfor LS
