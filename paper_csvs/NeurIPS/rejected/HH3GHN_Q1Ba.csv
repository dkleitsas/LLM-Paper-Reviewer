Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014124293785310734,"Despite evidence from sociolinguistics that larger groups of speakers tend to
1"
ABSTRACT,0.002824858757062147,"develop more structured languages, the use of populations has failed to yield
2"
ABSTRACT,0.00423728813559322,"significant benefits in emergent multi-agent communication. In this paper we
3"
ABSTRACT,0.005649717514124294,"reassess the validity of the standard training protocol and illustrate its limitations.
4"
ABSTRACT,0.007062146892655367,"Specifically, we analyze population-level communication at the equilibrium in
5"
ABSTRACT,0.00847457627118644,"sender-receiver Lewis games. We find that receivers co-adapt to senders they
6"
ABSTRACT,0.009887005649717515,"are interacting with, which limits the effect of the population. Informed by this
7"
ABSTRACT,0.011299435028248588,"analysis, we propose an alternative training protocol based on “partitioning” agents.
8"
ABSTRACT,0.012711864406779662,"Partitioning isolates sender-receiver pairs, limits co-adaptation, and results in a new
9"
ABSTRACT,0.014124293785310734,"global optimization objective where agents maximize (1) their respective ""internal""
10"
ABSTRACT,0.015536723163841809,"communication accuracy and (2) their alignment with other agents. In experiments,
11"
ABSTRACT,0.01694915254237288,"we find that agents trained in partitioned populations are able to communicate
12"
ABSTRACT,0.018361581920903956,"successfully with new agents which they have never interacted with and tend to
13"
ABSTRACT,0.01977401129943503,"develop a shared language. Moreover, we observe that larger populations develop
14"
ABSTRACT,0.0211864406779661,"languages that are more compositional. Our findings suggest that scaling up to
15"
ABSTRACT,0.022598870056497175,"populations in multi-agent communication can be beneficial, but that it matters
16"
ABSTRACT,0.02401129943502825,"how we scale up.
17"
INTRODUCTION,0.025423728813559324,"1
Introduction
18"
INTRODUCTION,0.026836158192090395,"Uncovering the mechanisms that underlie our ability to communicate using language is an important
19"
INTRODUCTION,0.02824858757062147,"stepping stone towards developing machine learning models that are capable of coordinating and
20"
INTRODUCTION,0.029661016949152543,"interacting via natural language. Over the last few years, there has been increasing interest in
21"
INTRODUCTION,0.031073446327683617,"simulating the emergence of language using artificial agents trained with reinforcement learning to
22"
INTRODUCTION,0.03248587570621469,"communicate to achieve a cooperative task [33]. Typically, agents are trained to perform a variant
23"
INTRODUCTION,0.03389830508474576,"of the Lewis signaling game [37, 51] wherein a sender emits a message describing an object and a
24"
INTRODUCTION,0.03531073446327684,"receiver attempts to reconstruct the object based on the description. This line of work has applications
25"
INTRODUCTION,0.03672316384180791,"to semi-supervised learning applied to concrete tasks such as image captioning or representation
26"
INTRODUCTION,0.038135593220338986,"learning [36, 18].
27"
INTRODUCTION,0.03954802259887006,"Most previous research has focused on communication between a single pair of agents. However,
28"
INTRODUCTION,0.04096045197740113,"there is mounting evidence that the communication protocols developed in this restricted setting
29"
INTRODUCTION,0.0423728813559322,"become highly specialized and exhibit properties that are at odds with those found in human languages
30"
INTRODUCTION,0.043785310734463276,"[4, 8]: for example agents are able to solve the task successfully while using languages that are not
31"
INTRODUCTION,0.04519774011299435,"compositional [32, 9]. As a possible solution, a growing body of work is advocating for scaling
32"
INTRODUCTION,0.046610169491525424,"up the emergent communication literature to populations of more than two agents communicating
33"
INTRODUCTION,0.0480225988700565,"simultaneously [24, 30, 49, 10]. Indeed, there is substantial evidence within the language sciences
34"
INTRODUCTION,0.04943502824858757,"that population dynamics shape the language structure [47, 42]. In spite of this fact, several negative
35"
INTRODUCTION,0.05084745762711865,"results have been obtained, showing that training agents in population yield marginal benefits without
36"
INTRODUCTION,0.052259887005649715,"explicit pressure towards e.g. population diversity [49] or emulation mechanisms [10].
37"
INTRODUCTION,0.05367231638418079,"In this paper, we call into question the way such populations are trained. By studying a simple
38"
INTRODUCTION,0.05508474576271186,"referential game, we evaluate populations on two desirable features observed in natural language:
39"
INTRODUCTION,0.05649717514124294,"• Agents are able to communicate with new partners within the same population [23]
40"
INTRODUCTION,0.05790960451977401,"• Larger populations tend to develop more structured languages [42].
41"
INTRODUCTION,0.059322033898305086,"We provide evidence that populations of artificial agents do not always possess these features (as
42"
INTRODUCTION,0.06073446327683616,"also attested by previous work, e.g. Kim and Oh [30], Chaabouni et al. [10]). To shed light on this
43"
INTRODUCTION,0.062146892655367235,"phenomenon, we analyze the behaviour of agents in a population at the equilibrium. We find that with
44"
INTRODUCTION,0.0635593220338983,"the standard training procedure, the functional form of the objective is the same as that of a single pair
45"
INTRODUCTION,0.06497175141242938,"of agents, due to receivers co-adapting to their training partners. As our main contribution, we propose
46"
INTRODUCTION,0.06638418079096045,"an alternative training procedure which partitions sender-receiver pairs and limits co-adaptation of
47"
INTRODUCTION,0.06779661016949153,"receiver agents. We show that this new training paradigm maximizes a different objective at the
48"
INTRODUCTION,0.0692090395480226,"population level. In particular, it explicitly promotes mutual-intelligibility across different agents.
49"
INTRODUCTION,0.07062146892655367,"In experiments, we find that agents trained in partitioned populations are able to communicate
50"
INTRODUCTION,0.07203389830508475,"successfully with new communication partners with which they have never interacted during training,
51"
INTRODUCTION,0.07344632768361582,"and that languages spoken by various agents tend to be similar to one another. In addition, we observe
52"
INTRODUCTION,0.0748587570621469,"that (1) languages developed in partitioned populations tend to be more compositional and (2) there
53"
INTRODUCTION,0.07627118644067797,"is a population size effect whereby larger populations develop more structured languages. Our results
54"
INTRODUCTION,0.07768361581920905,"show that there are multiple ways to generalize from single agent pairs to larger populations, and that
55"
INTRODUCTION,0.07909604519774012,"these design choices matter when it comes to studying the emergent language.
56"
COMMUNICATION GAME,0.08050847457627118,"2
Communication Game
57"
COMMUNICATION GAME,0.08192090395480225,"We study communication in referential games, a variant of the Lewis signaling game [37] proposed
58"
COMMUNICATION GAME,0.08333333333333333,"by Lazaridou et al. [34]. The game proceeds as follows: during each round, a sender agent π observes
59"
COMMUNICATION GAME,0.0847457627118644,"an object x ∈X (e.g., an arbitrary categorical entity, or a natural images) sampled from input
60"
COMMUNICATION GAME,0.08615819209039548,"space X according to distribution p and generates a message m ∼π(· | x). Messages consist of
61"
COMMUNICATION GAME,0.08757062146892655,"variable length sequences of tokens picked from a discrete vocabulary V . Note that the tokens
62"
COMMUNICATION GAME,0.08898305084745763,"themselves are arbitrary and meaningless (typically they are represented as numbers from 1 to |V |).
63"
COMMUNICATION GAME,0.0903954802259887,"A receiver agent ρ then observes message m and must predict the original object from among a set of
64"
COMMUNICATION GAME,0.09180790960451977,"candidates C = {x, y1, . . . y|C−1|} containing x and |C| −1 distractors, where each distractor y is
65"
COMMUNICATION GAME,0.09322033898305085,"sampled uniformly without replacement from the input space excluding the original object, X \ {x}.
66"
COMMUNICATION GAME,0.09463276836158192,"Concretely, this is implemented by calculating a score f(y, m) for each candidate y and defining
67"
COMMUNICATION GAME,0.096045197740113,"the probability of a candidate conditioned on the message ρ(· | m, C) as
ef(x,m)
P"
COMMUNICATION GAME,0.09745762711864407,"y∈C f(y,m). Based on the
68"
COMMUNICATION GAME,0.09887005649717515,"receiver’s success, the sender agent receives a reward R(x, ρ(· | m, C)).
69"
COMMUNICATION GAME,0.10028248587570622,"In practice, both senders and receivers are implemented as neural networks πθ and ρψ with parameters
70"
COMMUNICATION GAME,0.1016949152542373,"θ and ψ estimated by gradient descent. The sender is trained to maximize its expected reward using
71"
COMMUNICATION GAME,0.10310734463276836,"the REINFORCE algorithm [57], while the receiver maximizes the expected log-likelihood of
72"
COMMUNICATION GAME,0.10451977401129943,"identifying the original object, log ρψ(x | m, C) (also known as the InfoNCE objective; Oord et al.
73"
COMMUNICATION GAME,0.1059322033898305,"[45]). Denoting as Ex∼p the expectation over x sampled from p, the corresponding training objectives
74"
COMMUNICATION GAME,0.10734463276836158,"are:
75"
COMMUNICATION GAME,0.10875706214689265,"Js(θ) = Ex∼p Em∼πθ(·|x) EC∼p R(x, ρψ(· | m, C))
(1)"
COMMUNICATION GAME,0.11016949152542373,"Jr(ψ) = Ex∼p Em∼πθ(·|x) EC∼p log ρψ(x | m, C)
(2)"
POPULATION LEVEL TRAINING,0.1115819209039548,"2.1
Population Level Training
76"
POPULATION LEVEL TRAINING,0.11299435028248588,"The two-player referential game can be generalized to larger populations of agents [41, 10]. In
77"
POPULATION LEVEL TRAINING,0.11440677966101695,"the most general case, we consider a population of Ns senders and Nr receivers that are linked by
78"
POPULATION LEVEL TRAINING,0.11581920903954802,"a bipartite communication graph G defining connections between senders and receiver (πθi, ρψj)
79"
POPULATION LEVEL TRAINING,0.1172316384180791,"[24, 30]. At training time, sender-receiver pairs are repeatedly sampled and trained to perform a
80"
POPULATION LEVEL TRAINING,0.11864406779661017,"round of the game. Importantly, only agent pairs that are connected in the communication graph are
81"
POPULATION LEVEL TRAINING,0.12005649717514125,"sampled. Throughout this paper, we will refer to this type of training as Standard training.
82"
POPULATION LEVEL TRAINING,0.12146892655367232,"With this training procedure, agents are trained to maximize their communicative success with all
83"
POPULATION LEVEL TRAINING,0.1228813559322034,"their neighbors in the communication graph. Let N G(i) refer to the neighbors of the i-th node in
84"
POPULATION LEVEL TRAINING,0.12429378531073447,"the graph, and Js,i→j (respectively Jr,i→j) denote the objective of πθi (respectively ρψj)) in the
85"
POPULATION LEVEL TRAINING,0.12570621468926554,"pairwise communication from sender i to receiver j. We can write the overall objective for sender i
86"
POPULATION LEVEL TRAINING,0.1271186440677966,"(and receiver j, respectively) as:
87"
POPULATION LEVEL TRAINING,0.1285310734463277,"Js,i(θi) =
1
| N G(i)| X"
POPULATION LEVEL TRAINING,0.12994350282485875,"j∈N G(i)
Js,i→j(θi)
and
Jr,j(ψj) =
1
| N G(j)| X"
POPULATION LEVEL TRAINING,0.13135593220338984,"i∈N G(j)
Jr,i→j(ψj).
(3)"
POPULATION LEVEL TRAINING,0.1327683615819209,"At test time, the population is evaluated by averaging the referential accuracy across all possible
88"
POPULATION LEVEL TRAINING,0.134180790960452,"sender-receiver pairings. Following previous work, in this paper we focus on populations with an
89"
POPULATION LEVEL TRAINING,0.13559322033898305,"equal number N := Ns = Nr of senders and receivers, meaning that there are up to N 2 possible
90"
POPULATION LEVEL TRAINING,0.1370056497175141,"pairings.
91"
POPULATION LEVEL TRAINING,0.1384180790960452,"2.2
What does Population-level Training Optimize?
92"
POPULATION LEVEL TRAINING,0.13983050847457626,"To shed light on the differences between training a single agent pair and training a population of
93"
POPULATION LEVEL TRAINING,0.14124293785310735,"agents, we analyze the objective optimized by the population. Inspired by [1]’s analysis in the
94"
POPULATION LEVEL TRAINING,0.1426553672316384,"two-player case, we study the behaviour of the population at the optimum, that is when senders and
95"
POPULATION LEVEL TRAINING,0.1440677966101695,"receivers have reached a Nash equilibrium [46].
96"
POPULATION LEVEL TRAINING,0.14548022598870056,"In this section, we make the simplifying assumption that C = X. In other words, receiver agents must
97"
POPULATION LEVEL TRAINING,0.14689265536723164,"pick the correct candidate out of all possible objects in X. This allows us to remove the conditioning
98"
POPULATION LEVEL TRAINING,0.1483050847457627,"on C and write ρψ(x | m, C) = ρψ(x | m). We make this simplification to reduce clutter in notations.
99"
POPULATION LEVEL TRAINING,0.1497175141242938,"Nevertheless, our key observations still hold for C ̸= X (see Appendix B for a detailed discussion).
100"
POPULATION LEVEL TRAINING,0.15112994350282485,"At a Nash equilibrium, the optimal receiver parameters ψ∗
j satisfy
101"
POPULATION LEVEL TRAINING,0.15254237288135594,"ρψ∗
j = arg max
ψj
Jr,j(ψj) = arg max
ψj"
POPULATION LEVEL TRAINING,0.153954802259887,"1
| N G(j)| X"
POPULATION LEVEL TRAINING,0.1553672316384181,"i∈N G(j)
Jr,i→j(ψj).
(4)"
POPULATION LEVEL TRAINING,0.15677966101694915,"Assuming that receiver ρψj has high enough capacity, and training is able to reach the global optimum,
102"
POPULATION LEVEL TRAINING,0.15819209039548024,"the solution of the optimization problem in Equation 4 has an analytical solution ρψ∗
j which can be
103"
POPULATION LEVEL TRAINING,0.1596045197740113,"written as a function of π∗
N G(j)(m | x) :=
1
| N G(j)|
P"
POPULATION LEVEL TRAINING,0.16101694915254236,"i∈N G(j) πθ∗
i (m | x), the mixture of all senders
104"
POPULATION LEVEL TRAINING,0.16242937853107345,"communicating with receiver j:
105"
POPULATION LEVEL TRAINING,0.1638418079096045,"ρψ∗
j (x | m) = π∗
N G(j)(x | m) =
π∗
N G(j)(m | x)p(x)"
POPULATION LEVEL TRAINING,0.1652542372881356,"Ey∼p π∗
N G(j)(m | y)."
POPULATION LEVEL TRAINING,0.16666666666666666,"In other words, ρψ∗
j is the posterior associated with π∗
N G(j) (full derivation in appendix A).
106"
POPULATION LEVEL TRAINING,0.16807909604519775,"An important implication of this result is that when the population graph is fully connected (all
107"
POPULATION LEVEL TRAINING,0.1694915254237288,"senders are connected to all receivers), each receiver converges to the same optimum π∗(x | m) =
108
Pn
i=1 πθi(m|x)p(x)
Ey∼p
Pn
i=1 πθi(m|x), the posterior of the mixture of all senders in the population. Plugging this back
109"
POPULATION LEVEL TRAINING,0.1709039548022599,"into each sender’s objective, we have
110"
POPULATION LEVEL TRAINING,0.17231638418079095,"Js,i(θ∗
i ) = Ex∼p Em∼πθ∗
i (·|x) R(x, π∗(· | m))"
POPULATION LEVEL TRAINING,0.17372881355932204,"Summing across all senders, we can rewrite the global objective optimized by the senders as
111"
POPULATION LEVEL TRAINING,0.1751412429378531,"max
θ∗Ex∼p Em∼π∗R(x, π∗(· | m)).
(5)"
POPULATION LEVEL TRAINING,0.1765536723163842,"In other words, at the equilibrium, the population maximizes the expected reward of the “sender
112"
POPULATION LEVEL TRAINING,0.17796610169491525,"ensemble” π∗, rather than that of individual agents πθ∗
i : the objective of a population N agents is
113"
POPULATION LEVEL TRAINING,0.17937853107344634,"functionally the same irrespective of N. We postulate that this indifference to the population size
114"
POPULATION LEVEL TRAINING,0.1807909604519774,"may account for the surprising lack of effect of larger populations observed in some previous work
115"
POPULATION LEVEL TRAINING,0.18220338983050846,"[49, 10]. Differences in behaviour must be attributed to effects stemming from training dynamics
116"
POPULATION LEVEL TRAINING,0.18361581920903955,"(e.g. it becomes more difficult for receivers to learn the posterior π∗(x | m)), or be imposed through
117"
POPULATION LEVEL TRAINING,0.1850282485875706,"extraneous modifications of the population objective (for example explicit imitation components;
118"
POPULATION LEVEL TRAINING,0.1864406779661017,"Chaabouni et al. [10]).
119"
POPULATION LEVEL TRAINING,0.18785310734463276,"A second observation is that there is no direct pressure for agents that communicate at training time
120"
POPULATION LEVEL TRAINING,0.18926553672316385,"to develop the same language. Indeed, it is entirely possible that all senders develop different but
121"
POPULATION LEVEL TRAINING,0.1906779661016949,"non-overlapping languages: it suffices that no two senders communicating with a shared receiver
122"
POPULATION LEVEL TRAINING,0.192090395480226,"use the same message m to describe a different object. In this case receivers can simply learn their
123"
POPULATION LEVEL TRAINING,0.19350282485875706,"neighboring sender’s languages and there is no need for the senders to converge to a unified language.
124"
POPULATION LEVEL TRAINING,0.19491525423728814,"Receiver
Sender"
POPULATION LEVEL TRAINING,0.1963276836158192,"Receiver gradient (
)"
POPULATION LEVEL TRAINING,0.1977401129943503,"Sender gradient (
)"
POPULATION LEVEL TRAINING,0.19915254237288135,"Figure 1: In the standard setting (left hand side), both receivers (in blue) are trained by maximizing
their discrimination objective with respect to both senders. With partitioning, receiver ρψ1 (resp.
ρψ2) is only trained to maximize its communication objective with sender πθ1 (resp. πθ2)"
PARTITIONING AGENTS,0.20056497175141244,"3
Partitioning Agents
125"
PARTITIONING AGENTS,0.2019774011299435,"A key difference between the usual population setting and populations of humans in laboratory
126"
PARTITIONING AGENTS,0.2033898305084746,"experiments is that agents are not usually split into “senders” and “receivers”. Rather, each participant
127"
PARTITIONING AGENTS,0.20480225988700565,"in the experiment assumes both a sender and receiver role [21]. Our hypothesis is that, counter to
128"
PARTITIONING AGENTS,0.2062146892655367,"what is customary in the emergent communication literature, tying senders and receivers is key in
129"
PARTITIONING AGENTS,0.2076271186440678,"surfacing useful population-level dynamics in multi-agent communication.
130"
PARTITIONING AGENTS,0.20903954802259886,"To operationalize this sender-receiver coupling, we identify an “agent” as a sender-receiver pair.
131"
PARTITIONING AGENTS,0.21045197740112995,"During training, we only train receiver ρψi with its associated sender πθi. In other words, Jr,i(ψi) :=
132"
PARTITIONING AGENTS,0.211864406779661,"Jr,i→i(ψi). In doing so, we “partition” the agents by preventing receiver i from co-adapting to other
133"
PARTITIONING AGENTS,0.2132768361581921,"senders j ̸= i. This procedure is illustrated in Figure 1. Note that senders can nevertheless still
134"
PARTITIONING AGENTS,0.21468926553672316,"train with rewards from neighboring receivers, and so communication across agents can still emerge.
135"
PARTITIONING AGENTS,0.21610169491525424,"Importantly, partitioning prevents receivers from learning to recognize multiple languages, as they
136"
PARTITIONING AGENTS,0.2175141242937853,"are now only trained on messages emitted by a single sender.
137"
PARTITIONING AGENTS,0.2189265536723164,"Following a similar analysis as Section 2.2, we derive that at the optimum, receiver ρψ∗
i (x | m) now
138"
PARTITIONING AGENTS,0.22033898305084745,"takes the form of the posterior associated with its respective sender, πθ∗
i (x | m) =
πθ∗
i (m|x)p(x)"
PARTITIONING AGENTS,0.22175141242937854,"Ey∼p πm|y
139"
PARTITIONING AGENTS,0.2231638418079096,"(derivation in Appendix A). We can thus write the population-level objective at the equilibrium as
140"
N,0.2245762711864407,"1
N N
X i=1 "
N,0.22598870056497175,"
Ex∼p Em∼πθ∗
i (·|x) R(x, πθ∗
i (· | m))
|
{z
}
Internal communication +
X"
N,0.2274011299435028,"j∈N G(i)
Ex∼p Em∼πθ∗
i (·|x) R(x, πθ∗
j (· | m))"
N,0.2288135593220339,"|
{z
}
Mutual intelligibility "
N,0.23022598870056496,"
. (6)"
N,0.23163841807909605,"Note that the functional form of the objective can now be decomposed into two parts: an “internal
141"
N,0.2330508474576271,"communication” objective which takes the same form as that of a single pair of agents, and a “mutual
142"
N,0.2344632768361582,"intelligibility” objective which enforces that neighboring agents are able to communicate successfully.
143"
N,0.23587570621468926,"In experiments, we show that this explicit pressure towards mutual intelligibility promotes the
144"
N,0.23728813559322035,"emergence of a single language within the population, which in turn enables agents to communicate
145"
N,0.2387005649717514,"with new partners outside of their training neighborhood.
146"
EXPERIMENTAL SETTING,0.2401129943502825,"4
Experimental Setting
147"
DATASETS,0.24152542372881355,"4.1
Datasets
148"
DATASETS,0.24293785310734464,"We perform experiments on two datasets: a simple, synthetic “attribute/values” dataset and a more
149"
DATASETS,0.2443502824858757,"realistic image dataset.
150"
DATASETS,0.2457627118644068,"Attribute/Values
In this dataset, each object is represent by a collection of abstract “attributes”.
151"
DATASETS,0.24717514124293785,"Specifically, each input x is a vector of 4 attributes, each of which can take 10 total values. This results
152"
DATASETS,0.24858757062146894,"in 104 total attribute/value combinations [32, 9]. In each setting we hold out 1, 000 combinations
153"
DATASETS,0.25,"to be used as a validation, and 1, 000 more for use as a test set. We can thus ensure that we are
154"
DATASETS,0.2514124293785311,"evaluating the agents’ ability to generalize to unseen combinations of attributes.
155"
DATASETS,0.2528248587570621,"ImageNet
In addition to toy objects, we perform experiments with referential games based on more
156"
DATASETS,0.2542372881355932,"realistic objects. Following Chaabouni et al. [10], we use the ImageNet [17] dataset of natural images.
157"
DATASETS,0.2556497175141243,"The dataset consists of about 1.4M training images collected on the internet and annotated for 1,000
158"
DATASETS,0.2570621468926554,"labels from the WordNet database [40]. Images are first encoded as 2048-sized real-valued vectors
159"
DATASETS,0.2584745762711864,"with a (frozen) ResNet pre-trained with BYOL [22] before being passed to sender and receivers.
160"
GAME ARCHITECTURE,0.2598870056497175,"4.2
Game Architecture
161"
GAME ARCHITECTURE,0.2612994350282486,"Both sender and receiver agents are based on 1 layer LSTMs [26] with embedding and hidden
162"
GAME ARCHITECTURE,0.2627118644067797,"dimensions of size 256. Specifically, the sender first encodes the object x into a vector of size 256,
163"
GAME ARCHITECTURE,0.2641242937853107,"which is concatenated to the input of the LSTM. At each step, the output of the LSTM cell is passed
164"
GAME ARCHITECTURE,0.2655367231638418,"through a fully connected layer to produce logits of size |V |. A softmax function is then applied
165"
GAME ARCHITECTURE,0.2669491525423729,"to obtain normalized probabilities over the vocabulary. During training, messages are generated
166"
GAME ARCHITECTURE,0.268361581920904,"by sampling from the distribution whereas at test time we generate messages deterministically via
167"
GAME ARCHITECTURE,0.269774011299435,"greedy decoding. In both cases, generation stops whenever a special “<EOS>” is generated, or when
168"
GAME ARCHITECTURE,0.2711864406779661,"the number of tokens reaches a fixed limit L.
169"
GAME ARCHITECTURE,0.2725988700564972,"The receiver encodes the message with an LSTM encoder, the output of which is the fed into a
170"
GAME ARCHITECTURE,0.2740112994350282,"fully connected layer to yield a vector of size 512. The candidate objects C are then scored by
171"
GAME ARCHITECTURE,0.2754237288135593,"computing the dot product of this vector with a 512-dimensional encoding of each candidate. The
172"
GAME ARCHITECTURE,0.2768361581920904,"conditional distribution over candidates is then obtained by taking a softmax. We set the reward
173"
GAME ARCHITECTURE,0.2782485875706215,"function for the sender to the log-likelihood assigned by the receiver to the correct candidate,
174"
GAME ARCHITECTURE,0.2796610169491525,"R(x, ρψ(· | m)) = log ρψ(x | m).
175"
GAME ARCHITECTURE,0.2810734463276836,"Throughout all experiments, we set the vocabulary size |V | to 20 and the maximum length of the
176"
GAME ARCHITECTURE,0.2824858757062147,"messages, L, to 10. This means that the communication channel used by the agents has a capacity
177"
GAME ARCHITECTURE,0.2838983050847458,"of about 2010 which ensures that there is no communication bottleneck (the size of the channel is
178"
GAME ARCHITECTURE,0.2853107344632768,"several orders of magnitude larger than the size of our datasets). Our implementation, based on the
179"
GAME ARCHITECTURE,0.2867231638418079,"EGG toolkit [29], will be open-sourced upon de-anonymization.
180"
POPULATION TRAINING,0.288135593220339,"4.3
Population training
181"
POPULATION TRAINING,0.2895480225988701,"(a) Fully-connected
(b) Circular"
POPULATION TRAINING,0.2909604519774011,"Figure 2: Example of communication
graphs used in this paper"
POPULATION TRAINING,0.2923728813559322,"We train populations following the procedure outlined by
182"
POPULATION TRAINING,0.2937853107344633,"Chaabouni et al. [10]: for each minibatch of data, we
183"
POPULATION TRAINING,0.2951977401129944,"sample K pairs from the population (uniformly among
184"
POPULATION TRAINING,0.2966101694915254,"the pairs linked in the communication graph). Each pair
185"
POPULATION TRAINING,0.2980225988700565,"plays an episode of the game, and the agents are updated
186"
POPULATION TRAINING,0.2994350282485876,"simultaneously following the gradients of their respective
187"
POPULATION TRAINING,0.3008474576271186,"objectives. We take K = max(10, N) to ensure that each
188"
POPULATION TRAINING,0.3022598870056497,"agent plays the game at least once at every step on aver-
189"
POPULATION TRAINING,0.3036723163841808,"age. This procedure needs to be modified for partitioned
190"
POPULATION TRAINING,0.3050847457627119,"populations: since receiver j is only with its respective
191"
POPULATION TRAINING,0.3064971751412429,"sender instead of with all of its neighbors, there is now
192"
POPULATION TRAINING,0.307909604519774,"only a
1
|NG(j)| chance that receiver j will be updated every
193"
POPULATION TRAINING,0.3093220338983051,"step (the probability that the pair (j, j) is sampled). For
194"
POPULATION TRAINING,0.3107344632768362,"larger populations, especially those that are fully-connected, this dramatically slows down training as
195"
POPULATION TRAINING,0.3121468926553672,"receivers are updated very infrequently. To address this issue, we modify the procedure as follows:
196"
POPULATION TRAINING,0.3135593220338983,"for every sampled agent pair (πθi, ρψj), we calculate both Js,i→j and Jr,i→i and update both πθi and
197"
POPULATION TRAINING,0.3149717514124294,"ρψi. Note that this necessitates calculating both ρψj(x | m, C) and ρψi(x | m, C) and therefore we
198"
POPULATION TRAINING,0.3163841807909605,"incur a small computational overhead. However we only observe a ∼5% increase in training time
199"
POPULATION TRAINING,0.3177966101694915,"due to the fact that we are back-propagating through only one of the two receivers, ρψi(x | m, C).
200"
POPULATION TRAINING,0.3192090395480226,"With this modification, we recover the property that each agent (sender or receiver) is updated once
201"
POPULATION TRAINING,0.3206214689265537,"every step on average.
202"
POPULATION TRAINING,0.3220338983050847,"In all experiments we train with a batch size of 1024 with the Adam optimizer [31] using a learning
203"
POPULATION TRAINING,0.3234463276836158,"rate of 0.001 for the attribute/value dataset and 0.0001 for Imagenet. The other parameters are set to
204"
POPULATION TRAINING,0.3248587570621469,"β1 = 0.9, β2 = 0.999 and ε = 10−8. We apply ℓ2 regularization with a coefficient of 10−5.
205"
POPULATION TRAINING,0.326271186440678,"We systematically augment the sender objectives with an entropy maximizing term, which has
206"
POPULATION TRAINING,0.327683615819209,"been found to encourage exploration [58]. The coefficient for this entropy term is set to 0.1 in all
207"
POPULATION TRAINING,0.3290960451977401,"Table 1: Accuracies with training partners and new partners on both datasets. Numbers are reported
with standard deviation across all pairs for 3 independent experiments"
POPULATION TRAINING,0.3305084745762712,"ImageNet
Attribute/Values
Standard
Partitioned
Standard
Partitioned"
POPULATION TRAINING,0.3319209039548023,"Training partners
97.09 ±1.10
99.75 ± 0.08
99.88 ± 0.15
99.81 ± 0.22
New partners
5.41 ±13.57
96.24 ± 3.25
7.81 ± 18.28
40.37 ± 29.44"
POPULATION TRAINING,0.3333333333333333,"Table 2: Language similarity between training partners and new partners on both datasets. Numbers
are reported with standard deviation across all pairs for 3 independent experiments"
POPULATION TRAINING,0.3347457627118644,"ImageNet
Attribute/Values
Standard
Partitioned
Standard
Partitioned"
POPULATION TRAINING,0.3361581920903955,"Training partners
0.28 ± 0.07
0.40 ± 0.02
0.28 ± 0.05
0.36 ± 0.01
New partners
0.22 ± 0.19
0.37 ± 0.15
0.23 ± 0.19
0.31 ± 0.17"
POPULATION TRAINING,0.3375706214689266,"experiments. To reduce the variance of the policy gradient in REINFORCE, we substract a baseline
208"
POPULATION TRAINING,0.3389830508474576,"computed by taking the average reward within a given mini-batch for each pair [54].
209"
POPULATION TRAINING,0.3403954802259887,"We evaluate the population every epoch (every 5 epochs for the Attribute/Value dataset) on the
210"
POPULATION TRAINING,0.3418079096045198,"validation set. We only evaluate on up to 100 unique pairs sampled uniformly within the population,
211"
POPULATION TRAINING,0.3432203389830508,"this time without consideration for the communication graph. We train for a fixed number of epochs,
212"
POPULATION TRAINING,0.3446327683615819,"selecting the best model based on the average validation accuracy across all evaluation pairs.
213"
COMMUNICATION WITH NEW PARTNERS,0.346045197740113,"5
Communication with New Partners
214"
COMMUNICATION WITH NEW PARTNERS,0.3474576271186441,"In our first set of experiments, we evaluate the ability of agents trained in populations to communicate
215"
COMMUNICATION WITH NEW PARTNERS,0.3488700564971751,"with partners they haven’t interacted with during training.
216"
CIRCULAR POPULATIONS,0.3502824858757062,"5.1
Circular Populations
217"
CIRCULAR POPULATIONS,0.3516949152542373,"Specifically, we study “circular” populations of agents arranged on a ring lattice. Each agent (sender-
218"
CIRCULAR POPULATIONS,0.3531073446327684,"receiver pair) i is only trained with neighboring agents i −1, . . . , i + 1 and the graph is cyclical (see
219"
CIRCULAR POPULATIONS,0.3545197740112994,"Figure 2b). We choose this type of population because it is an extreme case of a population where
220"
CIRCULAR POPULATIONS,0.3559322033898305,"each agent has the same, minimal amount of neighbors (two), yet there is still a path between any two
221"
CIRCULAR POPULATIONS,0.3573446327683616,"agents. In this context, training partners are sender-receiver pairs that are connected in the graph and
222"
CIRCULAR POPULATIONS,0.3587570621468927,"have interacted during the training phase whereas new partners refers to pairs that have not interacted
223"
CIRCULAR POPULATIONS,0.3601694915254237,"during training.
224"
CIRCULAR POPULATIONS,0.3615819209039548,"We report results along two metrics:
225"
CIRCULAR POPULATIONS,0.3629943502824859,"• Communication Accuracy of sender/receiver pairs on an evaluation set. This measures how
226"
CIRCULAR POPULATIONS,0.3644067796610169,"successful the pair is in communicating.
227"
CIRCULAR POPULATIONS,0.365819209039548,"• Language Similarity between senders. This metric (also called synchronization in Rita et al. [49])
228"
CIRCULAR POPULATIONS,0.3672316384180791,"is calculated as 1 −δi,j, where δi,j is the normalized edit distance between messages output by
229"
CIRCULAR POPULATIONS,0.3686440677966102,"two senders, averaged across all objects in our evaluation set.
230"
CIRCULAR POPULATIONS,0.3700564971751412,"We report these metrics for both training partners and new partners. Note that high communication
231"
CIRCULAR POPULATIONS,0.3714689265536723,"accuracy does not always entail similar languages: it is possible for the receivers to achieve high
232"
CIRCULAR POPULATIONS,0.3728813559322034,"accuracy despite all senders sending different messages for any given object (it is only necessary for
233"
CIRCULAR POPULATIONS,0.3742937853107345,"a given message to unambiguously refer to one object across senders).
234"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3757062146892655,"5.2
Partitioning Enables Successful Zero-Shot Communication
235"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3771186440677966,"In Table 1 and 2, we report accuracies and similarities for circular populations of 20 sender-receiver
236"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3785310734463277,"pairs trained on ImageNet and the Attribute/Values dataset. All metrics are calculated on the test set
237"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3799435028248588,"and averaged across 3 independent experiments.
238"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3813559322033898,"0
1
2
3
4
5
6
7
8
9
10
Distance 0 20 40 60 80 100"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3827683615819209,Accuracy
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.384180790960452,"standard
partitioned"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3855932203389831,(a) Accuracy
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3870056497175141,"0
1
2
3
4
5
6
7
8
9
10
Distance 0.2 0.3 0.4"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3884180790960452,Similarity
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3898305084745763,"standard
partitioned"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3912429378531073,(b) Similarity1
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3926553672316384,"Figure 3: Accuracy and language similarity as a function of the distance between two agents in the
communication graph."
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3940677966101695,"0
25
50
75
100
125
150
epoch 0 20 40 60 80 100 acc"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3954802259887006,(a) Standard
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3968926553672316,"0
20
40
60
80
100
120
140
epoch 0 20 40 60 80 100 acc"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3983050847457627,Distance
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.3997175141242938,"0
2
4
6
8
10
All"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4011299435028249,(b) Partitioned
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4025423728813559,"Figure 4: Evolution of validation accuracy during training across agent pairs at various distances in
the communication graph. Results are aggregated over all agent pairs and 3 populations."
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.403954802259887,"We observe that in populations following the standard training paradigm (Standard), there is a stark
239"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4053672316384181,"discrepancy between training and new partners. Indeed, on both datasets the accuracy with training
240"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4067796610169492,"partners reaches a very high value, above 95%. Yet, the accuracy when agents communicate with
241"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4081920903954802,"new partners drops down to less than 10%. On the other hand, in Partitioned populations, agents
242"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4096045197740113,"reach a much higher accuracy with non-neighbors, up to 96% on ImageNet and 40%. A similar trend
243"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4110169491525424,"holds for language similarity.
244"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4124293785310734,"Note that all metrics on new partners exhibit high standard deviation. An explanation is that among
245"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4138418079096045,"non-neighboring pairs there is a different behaviour depending on how far the two agents are in the
246"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4152542372881356,"population. This is verified in Figure 3, which displays a breakdown as a function of the distance
247"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4166666666666667,"between two agents in the communication graph (on ImageNet). We find that without partitioning,
248"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4180790960451977,"accuracy drops off sharply to close to 0 for agents at a distance ≥2, whereas it decreases almost
249"
PARTITIONING ENABLES SUCCESSFUL ZERO-SHOT COMMUNICATION,0.4194915254237288,"linearly with the distance in the partitioned case, down to about 95% for the most distant agents.
250"
TRAINING DYNAMICS,0.4209039548022599,"5.3
Training dynamics
251"
TRAINING DYNAMICS,0.422316384180791,"We further investigate the evolution of accuracies during training. In Figure 4, we plot the evaluation
252"
TRAINING DYNAMICS,0.423728813559322,"accuracies of both standard and partitioned populations broken down by distance between pairs,
253"
TRAINING DYNAMICS,0.4251412429378531,"focusing on the ImageNet dataset. Note that there are two training phases in the standard case. Up to
254"
TRAINING DYNAMICS,0.4265536723163842,"epoch ≈10, the accuracy for all training pairs increases, after which agents over-fit to their training
255"
TRAINING DYNAMICS,0.4279661016949153,"partners (distances 0 and 1) and the accuracy on other pairs decreases to a plateau.
256"
TRAINING DYNAMICS,0.4293785310734463,"On the other hand, Figure 4b illustrates the pressure for mutual-intelligibility in partitioned popula-
257"
TRAINING DYNAMICS,0.4307909604519774,"tions: as accuracy between training pairs reaches close to 99% accuracy (around epoch 20), accuracies
258"
TRAINING DYNAMICS,0.4322033898305085,"across distant pairs increases rapidly before plateauing above 90%. In fact, our results show that the
259"
TRAINING DYNAMICS,0.4336158192090395,"most distant accuracies are still increasing after 150 epochs, albeit very slowly.
260"
TRAINING DYNAMICS,0.4350282485875706,"1By construction, the similarity of a sender with itself (corresponding to a distance of 0) is always one. We
omit this value from the figure to better illustrate the trends for distance ≥1."
TRAINING DYNAMICS,0.4364406779661017,"2 3 4 5
10
15
20
25
Population size 25 30 35 40"
TRAINING DYNAMICS,0.4378531073446328,Topographic similarity
TRAINING DYNAMICS,0.4392655367231638,"standard
partitioned
Single Pair"
TRAINING DYNAMICS,0.4406779661016949,"(a) Topographic similarity as a func-
tion of population size on an at-
tribute/value communication game."
TRAINING DYNAMICS,0.442090395480226,"0.0
0.2
0.4
0.6
0.8
1.0
Co-adaptation probability 30 32 34 36 38 40"
TRAINING DYNAMICS,0.4435028248587571,Topographic similarity
TRAINING DYNAMICS,0.4449152542372881,"(b)
Topographic similarity with
varying degrees of partitioning (pop-
ulations of size 10)."
TRAINING DYNAMICS,0.4463276836158192,"0.0
0.1
0.2
0.3
0.4
0.5
Mutual Intelligibility relative weight 25 30 35 40"
TRAINING DYNAMICS,0.4477401129943503,Topographic similarity
TRAINING DYNAMICS,0.4491525423728814,"(c) Topographic similarity when ab-
lating the mutual-intelligibility term
(populations of size 10)."
TRAINING DYNAMICS,0.4505649717514124,Figure 5: Influence of partitioning on the topographic similarity of the emergent languages.
PARTITIONED POPULATION DEVELOP MORE COMPOSITIONAL LANGUAGES,0.4519774011299435,"6
Partitioned Population Develop More Compositional Languages
261"
PARTITIONED POPULATION DEVELOP MORE COMPOSITIONAL LANGUAGES,0.4533898305084746,"In this section, we investigate the effect of partitioning on the structure of the language, with a focus
262"
PARTITIONED POPULATION DEVELOP MORE COMPOSITIONAL LANGUAGES,0.4548022598870056,"on compositionality.
263"
MEASURING COMPOSITIONALITY,0.4562146892655367,"6.1
Measuring Compositionality
264"
MEASURING COMPOSITIONALITY,0.4576271186440678,"A language is said to be compositional when the meaning of a whole utterance can be systematically
265"
MEASURING COMPOSITIONALITY,0.4590395480225989,"deduced from the meaning of its components (i.e. words). The notion of compositionality is widely
266"
MEASURING COMPOSITIONALITY,0.4604519774011299,"construed to underlay the near infinite productivity of human languages [55].
267"
MEASURING COMPOSITIONALITY,0.461864406779661,"A common metric for measuring compositionality in emergent languages is the topographic similarity
268"
MEASURING COMPOSITIONALITY,0.4632768361581921,"[5, 35]. Topographic similarity captures the intuition that a compositional language will map similar
269"
MEASURING COMPOSITIONALITY,0.4646892655367232,"“meanings” to similar messages: the phrase “a red bird” is more similar to the phrase “a blue bird”
270"
MEASURING COMPOSITIONALITY,0.4661016949152542,"than to “a powerful computer”. In practice, the topographic similarity is computed by measuring the
271"
MEASURING COMPOSITIONALITY,0.4675141242937853,"Spearman rank correlation coefficient [52] between (1) the pairwise distances across all objects and
272"
MEASURING COMPOSITIONALITY,0.4689265536723164,"(2) the pairwise distance across all messages.
273"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4703389830508475,"6.2
Effect of Population Size on Compositionality
274"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4717514124293785,"We run experiments on our Attribute/Values dataset, with both standard and partitioned populations
275"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4731638418079096,"that are fully-connected (see Figure 2a). Population sizes range from 2 to 25 sender-receiver pairs.
276"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4745762711864407,"We compute topographic similarity using the Hamming distance in the object space (i.e. the distance
277"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4759887005649718,"between two objects is the number of attributes in which they differ) and the normalized edit distance
278"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4774011299435028,"between messages.
279"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4788135593220339,"In Figure 5a, we observe that while standard population-level training does increase the topographic
280"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.480225988700565,"similarity of the language overall, population size has very little effect: populations of sizes 3 and 20
281"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.481638418079096,"both reach about the same value of 30 on average. On the other hand, partitioning greatly increases
282"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4830508474576271,"the effect of population size on compositionality: populations of size 20 have a significantly higher
283"
EFFECT OF POPULATION SIZE ON COMPOSITIONALITY,0.4844632768361582,"topographic similarity than populations of size 5, with a ≈10 points difference.
284"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4858757062146893,"6.3
Co-adaptation is Responsible for the Decrease in Compositionality
285"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4872881355932203,"Up until this point, we have described partitioning (or lack thereof) as a binary choice. However, it is
286"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4887005649717514,"possible to partition a population only partially, by allowing receiver j to train with senders i ̸= j
287"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4901129943502825,"occasionally with probability α > 0. In doing so, the optimal receiver now becomes the posterior
288"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4915254237288136,"associated with a mixture between πθ∗
i (m | x) and π∗(m | x) (see Appendix A for the derivation). If
289"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4929378531073446,"0 < α < 1, receivers are now optimizing for a different objective (as in partitioned populations), but
290"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4943502824858757,"some amount of co-adaptation is still allowed.
291"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4957627118644068,"We perform this experiment on the Attribute/Values dataset with a fully connected population of
292"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4971751412429379,"size 10, varying the degree of co-adaptation α ranging in {0, 0.1, 0.5, 0.9, 1}. α = 0 corresponds
293"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.4985875706214689,"to partitioned training whereas α = 1 is equivalent to standard training. All populations converge
294"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.5,"to > 99% accuracy. However, in Figure 5b we find that topographic similarity drops as soon as we
295"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.501412429378531,"introduce minimal amounts of co-adaptation (α = 0.1) and decreases steadily to the level of standard
296"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.5028248587570622,"populations as α grows to 1. This further corroborates our hypothesis that reducing co-adaptation
297"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.5042372881355932,"promotes the emergence of a more structured language, and that eliminating it altogether (in a
298"
CO-ADAPTATION IS RESPONSIBLE FOR THE DECREASE IN COMPOSITIONALITY,0.5056497175141242,"partitioned population) yields the best results.
299"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5070621468926554,"6.4
Importance of Mutual Intelligibility
300"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5084745762711864,"Recall that the objective of a partitioned population at the equilibrium (Equation 6) can be decomposed
301"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5098870056497176,"in two terms: an “internal communication” corresponding to the single agent pair objective and a
302"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5112994350282486,"“mutual intelligibility” term which encourages senders to align their languages. Importantly, the latter
303"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5127118644067796,"is the only element that separates a partitioned population from a collection of isolated agents.
304"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5141242937853108,"To measure its effect on the compositionality of the emergent language, we train fully connected
305"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5155367231638418,"populations of size 10 and decrease the relative weight of the mutual intelligibility term. This is
306"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5169491525423728,"implemented by making the pair (πθi, ρθi) more likely to be sampled than other pairs (πθi, ρθj),
307"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.518361581920904,j ̸= i by a factor × 1−β
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.519774011299435,"β . We let β range from 0.5 (partitioned population) to 0.0 (collection of
308"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5211864406779662,"isolated sender-receiver pairs). In Figure 5c, we find that emergent languages retain high topographic
309"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5225988700564972,"similarity even at small β, and the sharp drop-off occurs only when β is very close to 0. This confirms
310"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5240112994350282,"that the mutual intelligibility term exerts a strong pressure towards compositionality. We investigate
311"
IMPORTANCE OF MUTUAL INTELLIGIBILITY,0.5254237288135594,"the evolution of the two terms during training in Appendix C.
312"
RELATED WORK,0.5268361581920904,"7
Related Work
313"
RELATED WORK,0.5282485875706214,"There is a rich history of modeling the emergence of language as the solution to a cooperative game
314"
RELATED WORK,0.5296610169491526,"that can be traced back to functional theories of language [59, 2, 13]. With a regain of interest
315"
RELATED WORK,0.5310734463276836,"for the study of language evolution [15, 12], a rich literature has developed around computational
316"
RELATED WORK,0.5324858757062146,"simulations of the emergence of language based on simple language games [37, 51, 3, 6]. Examples
317"
RELATED WORK,0.5338983050847458,"include studying evolutionary models of the emergence of grammar [44], the influence of cultural
318"
RELATED WORK,0.5353107344632768,"transmission [5], game theoretical considerations [27] or linguistic diversity [39] among others.
319"
RELATED WORK,0.536723163841808,"The recent success of deep learning in natural language processing has spurred interest in studying
320"
RELATED WORK,0.538135593220339,"signaling games between deep neural network trained with reinforcement learning to solve a signaling
321"
RELATED WORK,0.53954802259887,"game [34, 20]. Several follow-ups have taken this idea further by extending it to more complex games
322"
RELATED WORK,0.5409604519774012,"or environment [53, 25, 28, 16] or by adding an element of competition [50, 43] or negotiation [7] or
323"
RELATED WORK,0.5423728813559322,"even explicit pressure towards certain desirable properties [32, 11, 38, 48]. In parallel, several efforts
324"
RELATED WORK,0.5437853107344632,"have been made to understand the properties of the emergent languages [4, 8, 9].
325"
RELATED WORK,0.5451977401129944,"Within this growing literature, multiple authors have explicitly studied the use of populations of more
326"
RELATED WORK,0.5466101694915254,"than two agents. Various works have argued for augmenting populations with an explicit pressure
327"
RELATED WORK,0.5480225988700564,"towards more structure languages, via e.g. generational transmission [14], adversarial regularization
328"
RELATED WORK,0.5494350282485876,"[56], varying learning speeds [49] or imitation learning and voting [10]. Although the focus is often
329"
RELATED WORK,0.5508474576271186,"on fully-connected populations, some authors have also explored more complex communication
330"
RELATED WORK,0.5522598870056498,"graphs, for the purpose of modeling contact linguistics [24] or the effect of social network structure
331"
RELATED WORK,0.5536723163841808,"on the language [19]. Recent work from Kim and Oh [30] is perhaps closest to our own: the authors
332"
RELATED WORK,0.5550847457627118,"study the effect of population size and connectivity in the standard training paradigm. In contrast, the
333"
RELATED WORK,0.556497175141243,"purpose of this paper is to highlight the impact of the training procedure on these very effects.
334"
CONCLUSION,0.557909604519774,"8
Conclusion
335"
CONCLUSION,0.559322033898305,"Empirical findings in socio-linguistics suggest that population dynamics should help in simple
336"
CONCLUSION,0.5607344632768362,"sender-receiver communication games. In this paper, we observed that populations trained by naively
337"
CONCLUSION,0.5621468926553672,"extending the simple 1-1 protocol to N × N agent pairs fail to exhibit some of the properties that
338"
CONCLUSION,0.5635593220338984,"are observed in human populations. Motivated by an analysis of populations at the equilibrium, we
339"
CONCLUSION,0.5649717514124294,"described an alternative training paradigm, based on agents partitioning to reduce co-adaptation.
340"
CONCLUSION,0.5663841807909604,"Empirically, we find that partitioning enables us to recover some of the aforementioned properties.
341"
CONCLUSION,0.5677966101694916,"Our findings call attention to the fact that there is more than one way to generalize two single to many
342"
CONCLUSION,0.5692090395480226,"agents, and simple design choices can have a great impact on the training dynamics and ultimately the
343"
CONCLUSION,0.5706214689265536,"effect of population on the emergent language. Beyond emergent communication, we hope that this
344"
CONCLUSION,0.5720338983050848,"observation can inspire similar work in other cooperative multi-agent problems where co-adaptation
345"
CONCLUSION,0.5734463276836158,"between agents may counteract population effects.
346"
REFERENCES,0.5748587570621468,"References
347"
REFERENCES,0.576271186440678,"[1] Anonymous. Emergent communication: Generalization and overfitting in lewis games. Under
348"
REFERENCES,0.577683615819209,"Review, 2022.
349"
REFERENCES,0.5790960451977402,"[2] John Langshaw Austin. How to do things with words. Harvard University Press, Cambridge,
350"
REFERENCES,0.5805084745762712,"MA, 1962.
351"
REFERENCES,0.5819209039548022,"[3] John Batali. Computational simulations of the emergence of grammar. Approaches to the
352"
REFERENCES,0.5833333333333334,"evolution of language: Social and cognitive bases, page 405, 1998.
353"
REFERENCES,0.5847457627118644,"[4] Diane Bouchacourt and Marco Baroni. How agents see things: On visual representations in
354"
REFERENCES,0.5861581920903954,"an emergent language game. In Proceedings of the 2018 Conference on Empirical Methods in
355"
REFERENCES,0.5875706214689266,"Natural Language Processing (EMNLP), pages 981–985, 2018.
356"
REFERENCES,0.5889830508474576,"[5] Henry Brighton and Simon Kirby.
Understanding linguistic evolution by visualizing the
357"
REFERENCES,0.5903954802259888,"emergence of topographic mappings. Artificial life, 12(2):229–242, 2006.
358"
REFERENCES,0.5918079096045198,"[6] Angelo Cangelosi and Domenico Parisi. Computer simulation: A new scientific approach to the
359"
REFERENCES,0.5932203389830508,"study of language evolution. In Simulating the evolution of language, pages 3–28. Springer,
360"
REFERENCES,0.594632768361582,"2002.
361"
REFERENCES,0.596045197740113,"[7] Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark.
362"
REFERENCES,0.597457627118644,"Emergent communication through negotiation. In Proceedings of the International Conference
363"
REFERENCES,0.5988700564971752,"on Learning Representations (ICLR), 2018.
364"
REFERENCES,0.6002824858757062,"[8] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient
365"
REFERENCES,0.6016949152542372,"encoding in emergent communication. In Proceedings of the 33rd Annual Conference on Neural
366"
REFERENCES,0.6031073446327684,"Information Processing Systems (NeurIPS), volume 32, 2019.
367"
REFERENCES,0.6045197740112994,"[9] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco
368"
REFERENCES,0.6059322033898306,"Baroni. Compositionality and generalization in emergent languages. In Proceedings of the 8th
369"
REFERENCES,0.6073446327683616,"Annual Meeting of the Association for Computational Linguistics (ACL), pages 4427–4442,
370"
REFERENCES,0.6087570621468926,"2020.
371"
REFERENCES,0.6101694915254238,"[10] Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz
372"
REFERENCES,0.6115819209039548,"Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot.
373"
REFERENCES,0.6129943502824858,"Emergent communication at scale. In Proceedings of the International Conference on Learning
374"
REFERENCES,0.614406779661017,"Representations (ICLR), 2022.
375"
REFERENCES,0.615819209039548,"[11] Edward Choi, Angeliki Lazaridou, and Nando de Freitas. Compositional obverter communica-
376"
REFERENCES,0.617231638418079,"tion learning from raw visual input. In Proceedings of the International Conference on Learning
377"
REFERENCES,0.6186440677966102,"Representations (ICLR), 2018.
378"
REFERENCES,0.6200564971751412,"[12] Morten H Christiansen and Simon Ed Kirby. Language evolution. Oxford University Press,
379"
REFERENCES,0.6214689265536724,"2003.
380"
REFERENCES,0.6228813559322034,"[13] Hebert Clark. Using Language. Cambridge University Press, Cambridge, UK, 1996.
381"
REFERENCES,0.6242937853107344,"[14] Michael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, and Dhruv Batra. Emergence of
382"
REFERENCES,0.6257062146892656,"compositional language with deep generational transmission. arXiv preprint arXiv:1904.09067,
383"
REFERENCES,0.6271186440677966,"2019.
384"
REFERENCES,0.6285310734463276,"[15] Vincent P Crawford and Joel Sobel. Strategic information transmission. Econometrica: Journal
385"
REFERENCES,0.6299435028248588,"of the Econometric Society, pages 1431–1451, 1982.
386"
REFERENCES,0.6313559322033898,"[16] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat,
387"
REFERENCES,0.632768361581921,"and Joelle Pineau. Tarmac: Targeted multi-agent communication. In Proceedings of the 36th
388"
REFERENCES,0.634180790960452,"International Conference on Machine Learning (ICML), pages 1538–1546. PMLR, 2019.
389"
REFERENCES,0.635593220338983,"[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
390"
REFERENCES,0.6370056497175142,"hierarchical image database. In Proceedings of the 22nd IEEE Conference on Computer Vision
391"
REFERENCES,0.6384180790960452,"and Pattern Recognition (CVPR), pages 248–255, 2009.
392"
REFERENCES,0.6398305084745762,"[18] Roberto Dessì, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from
393"
REFERENCES,0.6412429378531074,"scratch (with a generic visual processor emerging on the side). arXiv preprint arXiv:2106.04258,
394"
REFERENCES,0.6426553672316384,"2021.
395"
REFERENCES,0.6440677966101694,"[19] Marina Dubova, Arsenii Kirillovich Moskvichev, and Rob Goldstone. Reinforcement communi-
396"
REFERENCES,0.6454802259887006,"cation learning in different social network structures. In Language in Reinforcement Learning
397"
REFERENCES,0.6468926553672316,"Workshop 2020, 2020.
398"
REFERENCES,0.6483050847457628,"[20] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning
399"
REFERENCES,0.6497175141242938,"to communicate with deep multi-agent reinforcement learning. In Annual Conference on Neural
400"
REFERENCES,0.6511299435028248,"Information Processing Systems (NIPS), volume 29, 2016.
401"
REFERENCES,0.652542372881356,"[21] Lukas Galke, Yoav Ram, and Limor Raviv. Emergent communication for understanding human
402"
REFERENCES,0.653954802259887,"language evolution: What’s missing? In Emergent Communication Workshop at ICLR 2022,
403"
REFERENCES,0.655367231638418,"2022.
404"
REFERENCES,0.6567796610169492,"[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
405"
REFERENCES,0.6581920903954802,"Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
406"
REFERENCES,0.6596045197740112,"et al. Bootstrap your own latent-a new approach to self-supervised learning. In Proceedings of
407"
REFERENCES,0.6610169491525424,"the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), volume 33,
408"
REFERENCES,0.6624293785310734,"pages 21271–21284, 2020.
409"
REFERENCES,0.6638418079096046,"[23] Abhinav Gupta, Marc Lanctot, and Angeliki Lazaridou. Dynamic population-based meta-
410"
REFERENCES,0.6652542372881356,"learning for multi-agent communication with natural language. In Proceedings of the 35th
411"
REFERENCES,0.6666666666666666,"Annual Conference on Neural Information Processing Systems (NeurIPS), volume 34, 2021.
412"
REFERENCES,0.6680790960451978,"[24] Laura Harding Graesser, Kyunghyun Cho, and Douwe Kiela. Emergent linguistic phenomena
413"
REFERENCES,0.6694915254237288,"in multi-agent communication games. In Proceedings of the 2019 Conference on Empirical
414"
REFERENCES,0.6709039548022598,"Methods in Natural Language Processing (EMNLP), pages 3700–3710, Hong Kong, China,
415"
REFERENCES,0.672316384180791,"November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1384. URL
416"
REFERENCES,0.673728813559322,"https://aclanthology.org/D19-1384.
417"
REFERENCES,0.6751412429378532,"[25] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to
418"
REFERENCES,0.6765536723163842,"communicate with sequences of symbols. In Proceedings of the 31st Annual Conference on
419"
REFERENCES,0.6779661016949152,"Neural Information Processing Systems (NIPS), volume 30, 2017.
420"
REFERENCES,0.6793785310734464,"[26] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
421"
REFERENCES,0.6807909604519774,"1735–1780, 1997.
422"
REFERENCES,0.6822033898305084,"[27] Simon Huttegger, Brian Skyrms, Pierre Tarres, and Elliott Wagner. Some dynamics of signaling
423"
REFERENCES,0.6836158192090396,"games. Proceedings of the National Academy of Sciences, 111(Supplement 3):10873–10880,
424"
REFERENCES,0.6850282485875706,"2014.
425"
REFERENCES,0.6864406779661016,"[28] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega,
426"
REFERENCES,0.6878531073446328,"DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation
427"
REFERENCES,0.6892655367231638,"for multi-agent deep reinforcement learning. In Proceedings of the 36th International Confer-
428"
REFERENCES,0.690677966101695,"ence on Machine Learning (ICML), pages 3040–3049. PMLR, 2019.
429"
REFERENCES,0.692090395480226,"[29] Eugene Kharitonov, Roberto Dessì, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni.
430"
REFERENCES,0.693502824858757,"EGG: a toolkit for research on Emergence of lanGuage in Games. https://github.com/
431"
REFERENCES,0.6949152542372882,"facebookresearch/EGG, 2021.
432"
REFERENCES,0.6963276836158192,"[30] Jooyeon Kim and Alice Oh. Emergent communication under varying sizes and connectivities.
433"
REFERENCES,0.6977401129943502,"In Proceedings of the 35th Annual Conference on Neural Information Processing Systems
434"
REFERENCES,0.6991525423728814,"(NeurIPS), volume 34, 2021.
435"
REFERENCES,0.7005649717514124,"[31] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
436"
REFERENCES,0.7019774011299436,"of the International Conference on Learning Representations (ICLR), 2014.
437"
REFERENCES,0.7033898305084746,"[32] Satwik Kottur, José Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge
438"
REFERENCES,0.7048022598870056,"‘naturally’in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods
439"
REFERENCES,0.7062146892655368,"in Natural Language Processing (EMNLP), pages 2962–2967, 2017.
440"
REFERENCES,0.7076271186440678,"[33] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep
441"
REFERENCES,0.7090395480225988,"learning era. arXiv preprint arXiv:2006.02419, 2020.
442"
REFERENCES,0.71045197740113,"[34] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation
443"
REFERENCES,0.711864406779661,"and the emergence of (natural) language. In Proceedings of the International Conference on
444"
REFERENCES,0.713276836158192,"Learning Representations (ICLR), 2017.
445"
REFERENCES,0.7146892655367232,"[35] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of
446"
REFERENCES,0.7161016949152542,"linguistic communication from referential games with symbolic and pixel input. In Proceedings
447"
REFERENCES,0.7175141242937854,"of the International Conference on Learning Representations (ICLR), 2018.
448"
REFERENCES,0.7189265536723164,"[36] Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication
449"
REFERENCES,0.7203389830508474,"meets natural language: Synergies between functional and structural language learning. In
450"
REFERENCES,0.7217514124293786,"Proceedings of the 8th Annual Meeting of the Association for Computational Linguistics (ACL),
451"
REFERENCES,0.7231638418079096,"pages 7663–7674, 2020.
452"
REFERENCES,0.7245762711864406,"[37] David Lewis. Convention: A philosophical study. John Wiley & Sons, 1969.
453"
REFERENCES,0.7259887005649718,"[38] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent com-
454"
REFERENCES,0.7274011299435028,"munication. In Proceedings of the 33rd Annual Conference on Neural Information Processing
455"
REFERENCES,0.7288135593220338,"Systems (NeurIPS), volume 32, 2019.
456"
REFERENCES,0.730225988700565,"[39] Daniel Livingstone and Colin Fyfe. Modelling the evolution of linguistic diversity. In European
457"
REFERENCES,0.731638418079096,"Conference on Artificial Life, pages 704–708. Springer, 1999.
458"
REFERENCES,0.7330508474576272,"[40] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):
459"
REFERENCES,0.7344632768361582,"39–41, 1995.
460"
REFERENCES,0.7358757062146892,"[41] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-
461"
REFERENCES,0.7372881355932204,"agent populations. In Proceedings of the 32nd Meeting of the Association for Advancement of
462"
REFERENCES,0.7387005649717514,"Artificial Intelligence (AAAI), volume 32, 2018.
463"
REFERENCES,0.7401129943502824,"[42] Jonas Nölle, Riccardo Fusaroli, Gregory J Mills, and Kristian Tylén. Language as shaped by
464"
REFERENCES,0.7415254237288136,"the environment: linguistic construal in a collaborative spatial task. Palgrave Communications,
465"
REFERENCES,0.7429378531073446,"6(1):1–10, 2020.
466"
REFERENCES,0.7443502824858758,"[43] Michael Noukhovitch, Travis LaCroix, Angeliki Lazaridou, and Aaron Courville. Emergent
467"
REFERENCES,0.7457627118644068,"communication under competition. In Proceedings of the 20th International Conference on
468"
REFERENCES,0.7471751412429378,"Autonomous Agents and MultiAgent Systems, pages 974–982, 2021.
469"
REFERENCES,0.748587570621469,"[44] Martin A Nowak and Natalia L Komarova. Towards an evolutionary theory of language. Trends
470"
REFERENCES,0.75,"in cognitive sciences, 5(7):288–295, 2001.
471"
REFERENCES,0.751412429378531,"[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
472"
REFERENCES,0.7528248587570622,"predictive coding. arXiv preprint arXiv:1807.03748, 2018.
473"
REFERENCES,0.7542372881355932,"[46] Martin J. Osborne and Ariel Rubinstein. A Course in Game Theory. The MIT Press, 1994.
474"
REFERENCES,0.7556497175141242,"ISBN 0262150417.
475"
REFERENCES,0.7570621468926554,"[47] Limor Raviv, Antje Meyer, and Shiri Lev-Ari. Larger communities create more systematic
476"
REFERENCES,0.7584745762711864,"languages. Proceedings of the Royal Society B, 286(1907):20191262, 2019.
477"
REFERENCES,0.7598870056497176,"[48] Yi Ren, Shangmin Guo, Serhii Havrylov, Shay Cohen, and Simon Kirby. Enhance the composi-
478"
REFERENCES,0.7612994350282486,"tionality of emergent language by iterated learning. In 3rd NeurIPS Workshop on Emergent
479"
REFERENCES,0.7627118644067796,"Communication (EmeCom@ NeurIPS 2019). URL https://papers. nips. cc/book/advances-in-
480"
REFERENCES,0.7641242937853108,"neural-information-processing-systems-32-2019, 2019.
481"
REFERENCES,0.7655367231638418,"[49] Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel Dupoux.
482"
REFERENCES,0.7669491525423728,"On the role of population heterogeneity in emergent communication. In Proceedings of the
483"
REFERENCES,0.768361581920904,"International Conference on Learning Representations (ICLR), 2022.
484"
REFERENCES,0.769774011299435,"[50] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate
485"
REFERENCES,0.7711864406779662,"at scale in multiagent cooperative and competitive tasks. In Proceedings of the International
486"
REFERENCES,0.7725988700564972,"Conference on Learning Representations (ICLR), 2018.
487"
REFERENCES,0.7740112994350282,"[51] Brian Skyrms. Signals: Evolution, learning, and information. OUP Oxford, 2010.
488"
REFERENCES,0.7754237288135594,"[52] C Spearman. The proof and measurement of association between two things. The American
489"
REFERENCES,0.7768361581920904,"Journal of Psychology, 1904.
490"
REFERENCES,0.7782485875706214,"[53] Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backprop-
491"
REFERENCES,0.7796610169491526,"agation. In Proceedings of the 30th Annual Conference on Neural Information Processing
492"
REFERENCES,0.7810734463276836,"Systems (NIPS), volume 29, 2016.
493"
REFERENCES,0.7824858757062146,"[54] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
494"
REFERENCES,0.7838983050847458,"methods for reinforcement learning with function approximation. Proceedings of the 13th
495"
REFERENCES,0.7853107344632768,"Annual Conference on Neural Information Processing Systems (NIPS), 12, 1999.
496"
REFERENCES,0.786723163841808,"[55] Zoltán Gendler Szabó. Compositionality. In Edward N. Zalta, editor, The Stanford Encyclo-
497"
REFERENCES,0.788135593220339,"pedia of Philosophy. 2020. URL https://plato.stanford.edu/archives/fall2020/
498"
REFERENCES,0.78954802259887,"entries/compositionality/. Accessed: 2022-05-13.
499"
REFERENCES,0.7909604519774012,"[56] Olivier Tieleman, Angeliki Lazaridou, Shibl Mourad, Charles Blundell, and Doina Precup.
500"
REFERENCES,0.7923728813559322,"Community size effect in artificial learning systems. In ViGIL@ NeurIPS, 2019.
501"
REFERENCES,0.7937853107344632,"[57] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
502"
REFERENCES,0.7951977401129944,"ment learning. Machine learning, 8(3):229–256, 1992.
503"
REFERENCES,0.7966101694915254,"[58] Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement
504"
REFERENCES,0.7980225988700564,"learning algorithms. Connection Science, 3(3):241–268, 1991.
505"
REFERENCES,0.7994350282485876,"[59] Ludwig Wittgenstein. Philosophical Investigations. Blackwell, Oxford, UK, 1953. Translated
506"
REFERENCES,0.8008474576271186,"by G.E.M. Anscombe.
507"
REFERENCES,0.8022598870056498,"Checklist
508"
REFERENCES,0.8036723163841808,"1. For all authors...
509"
REFERENCES,0.8050847457627118,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
510"
REFERENCES,0.806497175141243,"contributions and scope? [Yes]
511"
REFERENCES,0.807909604519774,"(b) Did you describe the limitations of your work? [Yes] The marginal computational
512"
REFERENCES,0.809322033898305,"overhead incurred by training partitioned populations is described in details in Section
513"
REFERENCES,0.8107344632768362,"4.3
514"
REFERENCES,0.8121468926553672,"(c) Did you discuss any potential negative societal impacts of your work? [No] As our
515"
REFERENCES,0.8135593220338984,"work primarily focuses on artificial languages developed by simple agents, we do not
516"
REFERENCES,0.8149717514124294,"expect any immediate negative societal impact.
517"
REFERENCES,0.8163841807909604,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
518"
REFERENCES,0.8177966101694916,"them? [Yes]
519"
REFERENCES,0.8192090395480226,"2. If you are including theoretical results...
520"
REFERENCES,0.8206214689265536,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
521"
REFERENCES,0.8220338983050848,"(b) Did you include complete proofs of all theoretical results? [Yes] Derivations are
522"
REFERENCES,0.8234463276836158,"provided in appendices
523"
REFERENCES,0.8248587570621468,"3. If you ran experiments...
524"
REFERENCES,0.826271186440678,"(a) Did you include the code, data, and instructions needed to reproduce the main ex-
525"
REFERENCES,0.827683615819209,"perimental results (either in the supplemental material or as a URL)? [No] Code to
526"
REFERENCES,0.8290960451977402,"reproduce our experiments will be released upon deanonymization.
527"
REFERENCES,0.8305084745762712,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
528"
REFERENCES,0.8319209039548022,"were chosen)? [Yes]
529"
REFERENCES,0.8333333333333334,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
530"
REFERENCES,0.8347457627118644,"ments multiple times)? [Yes]
531"
REFERENCES,0.8361581920903954,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
532"
REFERENCES,0.8375706214689266,"of GPUs, internal cluster, or cloud provider)? [No] Our work was carried out on GPUs
533"
REFERENCES,0.8389830508474576,"located on an institutional cluster. Each experiment runs on a single V100-32G GPU
534"
REFERENCES,0.8403954802259888,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
535"
REFERENCES,0.8418079096045198,"(a) If your work uses existing assets, did you cite the creators? [Yes] The ImageNet paper
536"
REFERENCES,0.8432203389830508,"was cited
537"
REFERENCES,0.844632768361582,"(b) Did you mention the license of the assets? [No] We were not able to find the license of
538"
REFERENCES,0.846045197740113,"ImageNet
539"
REFERENCES,0.847457627118644,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
540 541"
REFERENCES,0.8488700564971752,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
542"
REFERENCES,0.8502824858757062,"using/curating? [N/A]
543"
REFERENCES,0.8516949152542372,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
544"
REFERENCES,0.8531073446327684,"information or offensive content? [N/A]
545"
REFERENCES,0.8545197740112994,"5. If you used crowdsourcing or conducted research with human subjects...
546"
REFERENCES,0.8559322033898306,"(a) Did you include the full text of instructions given to participants and screenshots, if
547"
REFERENCES,0.8573446327683616,"applicable? [N/A]
548"
REFERENCES,0.8587570621468926,"(b) Did you describe any potential participant risks, with links to Institutional Review
549"
REFERENCES,0.8601694915254238,"Board (IRB) approvals, if applicable? [N/A]
550"
REFERENCES,0.8615819209039548,"(c) Did you include the estimated hourly wage paid to participants and the total amount
551"
REFERENCES,0.8629943502824858,"spent on participant compensation? [N/A]
552"
REFERENCES,0.864406779661017,"A
Derivation of the Optimal Receiver
553"
REFERENCES,0.865819209039548,"We first prove a more general result from which the optimal receiver both in the standard and
554"
REFERENCES,0.867231638418079,"partitioned can be derived.
555"
REFERENCES,0.8686440677966102,"A.1
General Case
556"
REFERENCES,0.8700564971751412,"Consider a receiver j trained to maximize
557"
REFERENCES,0.8714689265536724,"Jr,j(ψj) =
X"
REFERENCES,0.8728813559322034,"i∈senders
αiJr,i→j(ψj)
(7)"
REFERENCES,0.8742937853107344,"where αi=1...n are arbitrary weights for the senders (we assume that the αi are positive and sum to
558"
REFERENCES,0.8757062146892656,"one). We can rewrite the objective as:
559"
REFERENCES,0.8771186440677966,"Jr,j(ψj) =
X"
REFERENCES,0.8785310734463276,"i∈senders
αiJr,i→j(ψj) =
X"
REFERENCES,0.8799435028248588,"i∈senders
αi Em∼πθi(·|x) log ρψj(x | m)"
REFERENCES,0.8813559322033898,"Note that by linearity of expectation we can pass the αi weighted average over the senders inside
560"
REFERENCES,0.882768361581921,"of the expectation and rewrite the second expectation in terms of the mixture π∗
α(m | x) :=
561
P"
REFERENCES,0.884180790960452,"i∈senders αiπθ∗
i (m | x):
562"
REFERENCES,0.885593220338983,"Jr,j(ψj) = Ex∼p Em∼P"
REFERENCES,0.8870056497175142,"i∈senders αiπθ∗
i (m|x) log ρψj(x | m)"
REFERENCES,0.8884180790960452,= Ex∼p Em∼π∗α(·|x) log ρψj(x | m)
REFERENCES,0.8898305084745762,"With slight abuse of notation, let us now denote by π∗
α(m) := Ex∼p π∗
α(m | x) the marginal
563"
REFERENCES,0.8912429378531074,"distribution over messages and π∗
α(x | m) := π∗
α(m|x)p(x)"
REFERENCES,0.8926553672316384,"π∗α(m)
the associated posterior. Notice that
564"
REFERENCES,0.8940677966101694,"since by definition π∗
α(m | x)p(x) = π∗
α(x | m)π∗
α(m), we can rewrite the double expectation
565"
REFERENCES,0.8954802259887006,"Ex∼p Em∼π∗α(·|x) as Em∼π∗α(·) Ex∼π∗α(·|m) by inverting the order of summation. We can therefore
566"
REFERENCES,0.8968926553672316,"rewrite
567"
REFERENCES,0.8983050847457628,"Jr,j(ψj) = Em∼π∗α(·) H(π∗
α(· | m), ρψj(· | m))"
REFERENCES,0.8997175141242938,"where H(p, q) denotes the cross-entropy Eq [−log p] of two distributions p and q. Importantly the
568"
REFERENCES,0.9011299435028248,"cross-entropy is non-negative and H(p, q) = 0 if and only if p = q.
569"
REFERENCES,0.902542372881356,"Consequently, the receiver ρψ will be optimal (Jr,j(ψj) = 0) if and only if for all m:2
570"
REFERENCES,0.903954802259887,"ρψ∗
j (x | m) = π∗
α(x | m) = π∗
α(m | x)p(x)
Ey∼p π∗α(m | y).
(8) □
571"
REFERENCES,0.905367231638418,"A.2
Optimal Receiver in Standard Populations
572"
REFERENCES,0.9067796610169492,"Recall that in standard populations, the training objective for receiver j is:
573"
REFERENCES,0.9081920903954802,"Jr,j(ψj) =
1
| N G(j)| X"
REFERENCES,0.9096045197740112,"i∈N G(j)
Jr,i→j(ψj)."
REFERENCES,0.9110169491525424,"Note that this is a special case of Equation 7 with
574 αi ="
REFERENCES,0.9124293785310734,"(
1
| N G(j)|
if i ∈N G(j)
0
otherwise"
REFERENCES,0.9138418079096046,"2More accurately, if the message space is not finite then the condition holds not for all m, but almost surely.
However throughout the paper we are experimenting with finite (albeit large) message spaces."
REFERENCES,0.9152542372881356,"Consequently, the derivation in Section A.1 tells us that the optimal receiver is
575"
REFERENCES,0.9166666666666666,"ρψ∗
j (x | m) = π∗
N G(j)(x | m) =
π∗
N G(j)(m | x)p(x)"
REFERENCES,0.9180790960451978,"Ey∼p π∗
N G(j)(m | y).
(9)"
REFERENCES,0.9194915254237288,"Where π∗
N G(j)(m | x) :=
1
| N G(j)|
P"
REFERENCES,0.9209039548022598,"i∈N G(j) πθ∗
i (m | x)
576"
REFERENCES,0.922316384180791,"A.3
Optimal Receiver in Partitioned Populations
577"
REFERENCES,0.923728813559322,"In partitioned populations, the training objective for receiver j is:
578"
REFERENCES,0.9251412429378532,"Jr,j(ψj) = Jr,j→j(ψj)."
REFERENCES,0.9265536723163842,"This is also a special case of Equation 7 with
579"
REFERENCES,0.9279661016949152,"αi =
1
if i = j
0
otherwise
The derivation in Section A.1 thus yields the optimal receiver
580"
REFERENCES,0.9293785310734464,"ρψ∗
j (x | m) = π∗
j (x | m) = π∗
j (m | x)p(x)
Ey∼p π∗
j (m | y).
(10)"
REFERENCES,0.9307909604519774,"A.4
Optimal Receiver in Partially Partitioned Populations
581"
REFERENCES,0.9322033898305084,"In the partially partitioned populations used in Section 6.3, each receiver’s objective is a mixture
582"
REFERENCES,0.9336158192090396,"between the standard and partitioned objective. This can also be rewritten as a special case of
583"
REFERENCES,0.9350282485875706,"Equation 7 with
584 αi = 

 
"
REFERENCES,0.9364406779661016,"1 −α +
α
| N G(j)|
if i = j
α
| N G(j)|
if i ∈N G(j) \ {i}
0
otherwise
The optimal receiver can then be rewritten as the posterior distribution associated with the mixture
585"
REFERENCES,0.9378531073446328,"sender
586"
REFERENCES,0.9392655367231638,"α × +(1 −α) × π∗
j (x | m)"
REFERENCES,0.940677966101695,"B
The Case of Referential Games
587"
REFERENCES,0.942090395480226,"In the analysis from Section 2.2 onward, we assumed C = X to simplify notation. We can relax this
588"
REFERENCES,0.943502824858757,"assumption without changing our key observation that all receivers are the same at the optimum.
589"
REFERENCES,0.9449152542372882,"Indeed, in this case the receiver’s objective in a standard population is:
590"
REFERENCES,0.9463276836158192,"Jr,j(ψj) =
1
| N G(j)| X"
REFERENCES,0.9477401129943502,"i∈N G(j)
Jr,i→j(ψj)"
REFERENCES,0.9491525423728814,"=
1
| N G(j)| X"
REFERENCES,0.9505649717514124,"i∈N G(j)
Ex∼p Em∼πθi(·|x) EC∼p log ρψj(x | m, C)"
REFERENCES,0.9519774011299436,"= Ex∼p Em∼π∗
N G(j)(·|x) EC∼p log ρψj(x | m, C)"
REFERENCES,0.9533898305084746,"This objective, called InfoNCE [45] also has an analytical solution that can be expressed as a function
591"
REFERENCES,0.9548022598870056,"of π∗
N G(j), of the form:
592"
REFERENCES,0.9562146892655368,"ρψ∗
j (x | m, C) ="
REFERENCES,0.9576271186440678,"π∗
N G(j)(x|m)"
REFERENCES,0.9590395480225988,"p(x)
P"
REFERENCES,0.96045197740113,"y∈C
π∗
N G(j)(y|m) p(y) (11)"
REFERENCES,0.961864406779661,"Despite the more complicated form of the optimal receiver, the key ingredients to our analysis
593"
REFERENCES,0.963276836158192,"in Sections 2.2 and 3 are preserved: at the optimum, each receiver is a function of the posterior
594"
REFERENCES,0.9646892655367232,"πN G(j)(x | m) associated with the communication partners to which it co-adapts. A similar analysis
595"
REFERENCES,0.9661016949152542,"in partitioned populations shows that the optimum for receiver j then only depends on the posterior
596"
REFERENCES,0.9675141242937854,"associated with its respective sender πθ∗
j instead.
597"
REFERENCES,0.9689265536723164,"0
50
100
epoch 0 20 40 60 80 loss"
REFERENCES,0.9703389830508474,"= 0
Topsim = 25.6"
REFERENCES,0.9717514124293786,"0
50
100
epoch"
REFERENCES,0.9731638418079096,"= 0.001
Topsim = 22.7"
REFERENCES,0.9745762711864406,"0
50
100
epoch"
REFERENCES,0.9759887005649718,"= 0.01
Topsim = 33.3"
REFERENCES,0.9774011299435028,"0
50
100
epoch"
REFERENCES,0.9788135593220338,"= 0.1
Topsim = 37.2"
REFERENCES,0.980225988700565,"0
50
100
epoch"
REFERENCES,0.981638418079096,"= 0.5
Topsim = 38.1 Loss"
REFERENCES,0.9830508474576272,"Mutual
intelligibility
Internal
communication"
REFERENCES,0.9844632768361582,"Figure 6: Evolution of internal communication and mutual intelligibility terms with different weight-
ings β (populations of size 10)."
REFERENCES,0.9858757062146892,"C
Further Analysis of the Effect of Mutual Intelligibility
598"
REFERENCES,0.9872881355932204,"In Section 6.4, we find that languages stay highly compositional until the mutual intelligibility weight
599"
REFERENCES,0.9887005649717514,"β is decreased to almost 0. Our hypothesis is that even with small amounts of mutual intelligibility,
600"
REFERENCES,0.9901129943502824,"agents will eventually have to optimize this part of the objective after they have maximized their
601"
REFERENCES,0.9915254237288136,"respective internal communication to the point where the main contributor to the training gradient is
602"
REFERENCES,0.9929378531073446,"the mutual intelligibility term.
603"
REFERENCES,0.9943502824858758,"To verify this hypothesis, in Figure 6 we report the evolution of both internal communication and
604"
REFERENCES,0.9957627118644068,"mutual intelligibility losses during training for various values of the mutual intelligibility weight
605"
REFERENCES,0.9971751412429378,"β. As expected, we observe that for all but very small values of β, the mutual intelligibility loss
606"
REFERENCES,0.998587570621469,"eventually decreases (although it decreases faster for high β).
607"
