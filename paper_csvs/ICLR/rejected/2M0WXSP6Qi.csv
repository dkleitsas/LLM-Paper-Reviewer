Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0055248618784530384,"Conditional generation is a subclass of generative problems when the output of
generation is conditioned by a class attributes’ information. In this paper, we
present a new stochastic contrastive conditional generative adversarial network
(InfoSCC-GAN) with explorable latent space. The InfoSCC-GAN architecture
is based on an unsupervised contrastive encoder built on the InfoNCE paradigm,
attributes’ classiﬁer, and stochastic EigenGAN generator. We propose two ap-
proaches for selecting the class attributes: external attributes from the dataset an-
notations and internal attributes from the clustered latent space of the encoder.
We propose a novel training method based on a generator regularization using ex-
ternal or internal attributes every n-th iteration using the pre-trained contrastive
encoder and pre-trained attributes’ classiﬁer. The proposed InfoSCC-GAN is de-
rived from an information-theoretic formulation of mutual information maximiza-
tion between the input data and latent space representation for the encoder and
the latent space and generated data for the decoder. Thus, we demonstrate a link
between the training objective functions and the above information-theoretic for-
mulation. The experimental results show that InfoSCC-GAN outperforms vanilla
EigenGAN in image generation on several popular datasets, yet providing an
interpretable latent space. In addition, we investigate the impact of regulariza-
tion techniques and each part of the system by performing an ablation study.
Finally, we demonstrate that thanks to the stochastic EigenGAN generator, the
proposed framework enjoys a truly stochastic generation in contrast to vanilla de-
terministic GANs yet with the independent training of an encoder, a classiﬁer,
and a generator. The code, supplementary materials, and demos are available
https://anonymous.4open.science/r/InfoSCC-GAN-D113"
INTRODUCTION,0.011049723756906077,"1
INTRODUCTION"
INTRODUCTION,0.016574585635359115,"Conditional image generation is the task of generating images, based on some attributes. The idea
of conditional GAN (cGAN) was proposed by Mirza & Osindero (2014). The authors modiﬁed
the classic GAN architecture by adding the attribute as a parameter to the input of the generator to
generate the corresponding image. They also added attributes to the discriminator input to better
distinguish real data. Since then, a lot of other methods have been developed. ACGAN (Odena
et al. (2017)) has an auxiliary classiﬁer to guide the generator to synthesize well-classiﬁable images.
ProjGAN (Miyato & Koyama (2018)) improves the approach proposed in ACGAN by utilizing
the inner product of an embedded image and the corresponding attribute embeddings. ContraGAN
(Kang & Park (2020)) utilizes contrastive 2C loss with multiple positive and negative pairs to update
the generator."
INTRODUCTION,0.022099447513812154,"While these methods have shown impressive results in conditional image generation, they are known
to be difﬁcult to train, to lack the image diversity within the same input attribute, and not to always
have meaningful data exploration in the latent space."
INTRODUCTION,0.027624309392265192,"We propose a new stochastic contrastive conditional generative adversarial network (InfoSCC-GAN)
with an explorable latent space. As a baseline generator, we use EigenGAN (He et al. (2021)) gen-
erator with interpretable and controllable input dimensions, yet trained in an unsupervised way.
EigenGAN ensures that different layers of a generative CNN, controlled by noise vectors, hold dif-
ferent semantics of the synthesized images. EigenGAN is able to mine interpretable and controllable"
INTRODUCTION,0.03314917127071823,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03867403314917127,"dimensions in an unsupervised way from different generator layers by embedding one linear sub-
space with an orthogonal basis into each generator layer. These layer-wise subspaces automatically
discover a set of ”eigen-dimensions” at each layer corresponding to a set of semantic attributes or in-
terpretable variations, via generative adversarial training to learn a target distribution. By traversing
the coefﬁcient of a speciﬁc ”eigen-dimension”, the generator can generate samples with continuous
changes corresponding to a speciﬁc semantic attribute. The ”core” latent space of EigenGAN is gen-
erated from the Gaussian distribution. In contrast to that, we use the latent space of the contrastive
encoder as the ”core” latent space for the generator. By using the contrastive encoder, we have the
ability to discover the ”inner” attributes from the dataset by clustering the latent space. The ”inner”
attributes are useful for datasets without external annotations, unbalanced datasets, and datasets with
subclasses. By using the encoder, we have an opportunity to compare latent spaces of the real im-
ages and the generated images. At the same time, the classiﬁer ensures the correspondence between
the attributes of training data and conditionally generated ones. Also, by training the encoder and
the classiﬁer independently from the generator, we reduce the general training complexity of the
system. It allows avoiding training the encoder and classiﬁer on unrealistic synthetic data, when
training it jointly with the generator and discriminator."
INTRODUCTION,0.04419889502762431,"The information-theoretical interpretation of the proposed model is provided in Section 2. The
experiments and ablation studies are provided in Section 4."
INTRODUCTION,0.049723756906077346,We summarize our contributions as follows:
INTRODUCTION,0.055248618784530384,"• We proposed a novel Stochastic Contrastive Conditional Generative Adversarial Network
(InfoSCC-GAN) for stochastic conditional image generation with controllable and inter-
pretable latent space. It is based on an EigenGAN, an independent contrastive encoder, and
an independent attribute classiﬁer."
INTRODUCTION,0.06077348066298342,"• We introduce a novel classiﬁcation regularization technique, which is based on updating
the model with classiﬁcation loss each n-th iteration and updating the generator using the
adversarial and classiﬁcation loss separately."
INTRODUCTION,0.06629834254143646,"• We propose a novel method for the attribute selection, based on clustering the embeddings,
computed using the pre-trained contrastive encoder."
INTRODUCTION,0.0718232044198895,• We provide an information-theoretic interpretation of the proposed system.
INTRODUCTION,0.07734806629834254,"• We perform an ablation study to determine the contribution of each part of the model to
overall performance."
INFORMATION-THEORETICAL FORMULATION,0.08287292817679558,"2
INFORMATION-THEORETICAL FORMULATION"
INFORMATION-THEORETICAL FORMULATION,0.08839779005524862,"2.1
THE TRAINING OF THE ENCODER (STAGE 1)"
INFORMATION-THEORETICAL FORMULATION,0.09392265193370165,"The encoder training is schematically shown in Figure 1, stage 1. The encoder training is based on
the maximization problem:"
INFORMATION-THEORETICAL FORMULATION,0.09944751381215469,"ˆ
φε = argmax
φε
Iφε(X; E),
(1)"
INFORMATION-THEORETICAL FORMULATION,0.10497237569060773,"where Iφε(X; E) = Ep(x,ε)
h
log qφε(ε|x)"
INFORMATION-THEORETICAL FORMULATION,0.11049723756906077,"qφε(ε)
i
, where qφε(ε|x) denotes the encoder and qφε(ε) - the
marginal latent space distribution."
INFORMATION-THEORETICAL FORMULATION,0.11602209944751381,"In the framework of contrastive learning, (1) is maximized based on the infoNCE framework
(van den Oord et al. (2018)). In the practical implementation, one can use approaches similar to
SimCLR (Chen et al. (2020)) where the inner product between the positive pairs created from the
augmented views originating from the same image is maximized and the inner product between the
negative pairs originating from different images is minimized. Alternatively, one can use other ap-
proaches to learn the representation ε such as BYOL (Grill et al. (2020)), Barlow Twins (Zbontar
et al. (2021)), etc. without loose of the generality of the proposed approach. It should be pointed out
that the encoder is trained independently from the decoder in the scope of the considered setup."
INFORMATION-THEORETICAL FORMULATION,0.12154696132596685,Under review as a conference paper at ICLR 2022 y
INFORMATION-THEORETICAL FORMULATION,0.1270718232044199,"T8MwED2Xr1K+CowsFi0SU5V0AMYKFsYi0Q+pjSrHdVqrjhNsB6lE/RMsDCDEyt9h49/gtBmg5UknPb13p7t7fiy4No7zjQpr6xubW8Xt0s7u3v5B+fCoraNEUdaikYhU1yeaCS5Zy3AjWDdWjIS+YB1/cpP5nUemNI/kvZnGzAvJSPKAU2Ks1K32/QA/VUuDcsWpOXPgVeLmpAI5moPyV38Y0SRk0lBtO65Tmy8lCjDqWCzUj/RLCZ0QkasZ6kIdNeOr93hs+sMsRBpGxJg+fq74mUhFpPQ92hsSM9bKXif95vcQEV17KZwYJuliUZAIbCKcPY+HXDFqxNQSQhW3t2I6JopQYyPKQnCX14l7XrNvai5d/VK4zqPowgncArn4MIlNOAWmtACgKe4RXe0AN6Qe/oY9FaQPnMfwB+vwBXLGO3g=</latexit>z ˜"" ˆ""
"""
INFORMATION-THEORETICAL FORMULATION,0.13259668508287292,+GmXl+wpnSjvNtLS2vrK6tFzbsza3tnd3i3n5dxaktEZiHsumjxXlTNCaZprTZiIpjnxOG37/JvcbAyoVi8W9HibUi3BXsJARrI3ktf0QtTXjAUWPdqdYcsrOBGiRuDNSuvqwL5Pxl13tFD/bQUzSiApNOFaq5TqJ9jIsNSOcjux2qmiCSR93actQgSOqvGxy9AgdGyVAYSxNCY0m6u+JDEdKDSPfdEZY9S8l4v/ea1UhxdexkSairIdFGYcqRjlCeAiYp0XxoCaSmVsR6WGJiTY5SG48y8vkvp2T0ru3dOqXINUxTgEI7gBFw4hwrcQhVqQOABnuAFXq2B9WyNrbdp65I1mzmAP7DefwD4i5ST</latexit>˜x pz(z) py(y)
INFORMATION-THEORETICAL FORMULATION,0.13812154696132597,"r (""|y, z)"
INFORMATION-THEORETICAL FORMULATION,0.143646408839779,"ecq3v8WNGlbtb6u0tr6xuVXeruzs7u0fVA+PukokEpMOFkzIvo8UYZSTjqakX4sCYp8Rnr+5DbXe1MiFRX8Qc9i4kVoxGlIMdKGlZrcpi6saJZPXV9wQI1i8wH3SmSxNBM8OzcuOyGPS+4CpwC1EBR7WH1yw0ETiLCNWZIqYFjx9pLkdQUM5JV3ESRGOEJGpGBgRxFRHnp/JgMnhkmgKGQ5nEN5+zvjhRFKt/SOCOkx2pZy8n/tEGiw2svpTxONOF4MShMGNQC5snAgEqCNZsZgLCkZleIx0girE1+FROCs3zyKuheNJzLRvO+WvdFHGUwQk4BXgCvQAnegDToAg0fwDF7Bm/VkvVjv1sfCWrKnmPwp6zPH9UznB4=</latexit>r ("")"
INFORMATION-THEORETICAL FORMULATION,0.14917127071823205,"z1
zK
· · ·"
INFORMATION-THEORETICAL FORMULATION,0.15469613259668508,"x
Dx˜x(x|y)
px(x)"
INFORMATION-THEORETICAL FORMULATION,0.16022099447513813,qC2/jDw8Z9zmHN+L+ZMKsv6MgpLyura8X10sbm1vaOubvXklEiCG2SiEei42FJOQtpUzHFaScWFAcep21vdJXV23dUSBaFt2ocUyfAg5D5jGClLdc8iN20F2A19Pz0fjKp/PCJa5atqjUVWgQ7hzLkarjmZ68fkSgoSIcS9m1rVg5KRaKEU4npV4iaYzJCA9oV2OIAyqdHrDB1rp4/8SOgXKjR1f0+kOJByHi6M1tRztcy879aN1H+hZOyME4UDcnsIz/hSEUoCwT1maBE8bEGTATuyIyxAITpWMr6RDs+ZMXoXVatc+qtZtauX6Zx1GEQziCthwDnW4hgY0gcADPMELvBqPxrPxZrzPWgtGPrMPf2R8fAPYrJhM</latexit>px(x)
INFORMATION-THEORETICAL FORMULATION,0.16574585635359115,"x
p✓y(y|"")
ˆy
Ly(y, ˆy)"
INFORMATION-THEORETICAL FORMULATION,0.1712707182320442,ZemEph0HW/ndLa+sbmVnm7srO7t39QPTxqmSTjPskYnuhNRwKRT3UaDknVRzGoeSt8Px3cxvP3FtRKIeMU95ENOhEpFgFK3k98KI5P1qza27c5BV4hWkBgWa/epXb5CwLOYKmaTGdD03xWBCNQom+bTSywxPKRvTIe9aqmjMTCZHzslZ1YZkCjRthSufp7YkJjY/I4tJ0xZFZ9mbif143w+gmAiVZsgVWyKMkwIbPyUBozlDmlCmhb2VsBHVlKHNp2JD8JZfXiWti7p3Vb98uKw1bos4ynACp3AOHlxDA+6hCT4wEPAMr/DmKOfFeXc+Fq0lp5g5hj9wPn8AbQaOcg=</latexit>y py(y)
INFORMATION-THEORETICAL FORMULATION,0.17679558011049723,"jDw8Z9zmHN+N+JMKsv6NAoLi0vLK8XV0tr6xuaWub3TkmEsCG2SkIei42JOQtoUzHFaScSFPsup213fJnV27dUSBYGNyqJqOPjYcA8RrDSVt/ciyo9H6uR6V3k2P0zcnkqG+Wrao1FfoLdg5lyNXomx+9QUhinwaKcCxl17Yi5aRYKEY4nZR6saQRJmM8pF2NAfapdNLpCRN0qJ0B8kKhX6DQ1P05kWJfysR3dWe2opyvZeZ/tW6svHMnZUEUKxqQ2UdezJEKUZYHGjBieKJBkwE07siMsICE6VTK+kQ7PmT/0LrpGqfVmvXtXL9Io+jCPtwABWw4QzqcAUNaAKBe3iEZ3gxHown49V4m7UWjHxmF37JeP8C1OXOA=</latexit>p(x, y)"
INFORMATION-THEORETICAL FORMULATION,0.18232044198895028,"Ly(y, ˜y)"
INFORMATION-THEORETICAL FORMULATION,0.1878453038674033,"njJ4xKZdvfRm1tfWNzq7d2Nnd2z8wD5t9GacCkx6OWSyGPpKEU56ipGhokgKPIZGfiz29IfPBIhacwfVJYQL0ITkOKkdLS2Gy6irKA5G6E1NQP86woxmbLbtzWKvEqUgLKnTH5pcbxDiNCFeYISlHjp0oL0dCUcxI0XBTSRKEZ2hCRpyFBHp5fPshXWqlcAKY6EfV9Zc/b2Ro0jKLPL1ZBlRLnul+J83SlV47eWUJ6kiHC8OhSmzVGyVRVgBFQrlmCsKA6q4WnSCsdF0NXYKz/OV0j9vO5fti/uLVuemqMOx3ACZ+DAFXTgDrQAwxP8Ayv8GYUxovxbnwsRmtGtXMEf2B8/gD1apUN</latexit>˜y
p⇤"
INFORMATION-THEORETICAL FORMULATION,0.19337016574585636,"✓y(y|"")
p✓x(x|"")"
INFORMATION-THEORETICAL FORMULATION,0.19889502762430938,"p✓x(x)
p✓y(y) q⇤"
INFORMATION-THEORETICAL FORMULATION,0.20441988950276244,"φ""(""|x) q⇤"
INFORMATION-THEORETICAL FORMULATION,0.20994475138121546,"φ""(""|x)"
INFORMATION-THEORETICAL FORMULATION,0.2154696132596685,"ˆqφ""("")"
INFORMATION-THEORETICAL FORMULATION,0.22099447513812154,"qφ""("")"
INFORMATION-THEORETICAL FORMULATION,0.2265193370165746,Stage 3
INFORMATION-THEORETICAL FORMULATION,0.23204419889502761,"Stage 2
Stage 1 x"
INFORMATION-THEORETICAL FORMULATION,0.23756906077348067,"'t(x)
ˆ""
qφ""(""|x)
h
qφh(h|"")"
INFORMATION-THEORETICAL FORMULATION,0.2430939226519337,"qφ""(""|x)
qφh(h|"")
't0(x)
ˆ""0
h0"
INFORMATION-THEORETICAL FORMULATION,0.24861878453038674,"Lh(h, h0)"
INFORMATION-THEORETICAL FORMULATION,0.2541436464088398,"Figure 1: Overview of the proposed InfoSCC-GAN training. Stage 1. Contrastive encoder training.
Stage 2. Classiﬁer training. Stage 3. Conditional generator training."
INFORMATION-THEORETICAL FORMULATION,0.2596685082872928,"2.2
THE TRAINING OF THE CLASS ATTRIBUTE CLASSIFIER (STAGE 2)"
INFORMATION-THEORETICAL FORMULATION,0.26519337016574585,"The class attribute classiﬁer training is schematically shown in Figure 1, stage 2. The training of the
class attribute classiﬁer is based on the maximization problem:"
INFORMATION-THEORETICAL FORMULATION,0.27071823204419887,"ˆθy = argmax
θy
Iφ∗ε,θy(Y; E),
(2)"
INFORMATION-THEORETICAL FORMULATION,0.27624309392265195,"where Iφ∗ε,θy(Y; E) = H(Y)−Hφ∗ε,θy(Y|E) and H(Y) = −Epy(y) log py(y) and the conditional"
INFORMATION-THEORETICAL FORMULATION,0.281767955801105,"entropy is deﬁned as Hφ∗ε,θy(Y|E) = −Epx(x)
h
Eqφ∗ε (ε|x)

log pθy(y|ε)
i
. Since H(Y) is inde-
pendent of the parameters of the encoder and classiﬁer, (2) reduces to the lower bound minimization:"
INFORMATION-THEORETICAL FORMULATION,0.287292817679558,"ˆθy = argmin
θy
Hφ∗ε,θy(Y|E),
(3)"
INFORMATION-THEORETICAL FORMULATION,0.292817679558011,"that under the categorical conditional distribution pθy(y|ε) can be expressed as the categorical cross-
entropy Ly(y, ˆy)."
INFORMATION-THEORETICAL FORMULATION,0.2983425414364641,"2.3
THE TRAINING OF THE DECODER, I.E., THE MAPPER AND GENERATOR (STAGE 3)"
INFORMATION-THEORETICAL FORMULATION,0.30386740331491713,"The training of decoder is shown in Figure 1, stage 3. The decoder is trained ﬁrst to maximize
the mutual information between the class attributes ˜y predicted from the generated images and true
class attributes y:"
INFORMATION-THEORETICAL FORMULATION,0.30939226519337015,"(ˆθx, ˆψ) = argmax
θx,ψ
Iψ,θx,φ∗ε,θ∗y(Y; E),
(4)"
INFORMATION-THEORETICAL FORMULATION,0.3149171270718232,"where
Iψ,θx,φ∗ε,θ∗y(Y; E)
=
H(Y)
−
Hψ,θx,φ∗ε,θ∗y(Y|E)
and
H(Y)
=
−Epy(y) log py(y)
and
the
conditional
entropy
is
deﬁned
as
Hψ,θx,φ∗
ε,θ∗
y(Y|E)
="
INFORMATION-THEORETICAL FORMULATION,0.32044198895027626,"−Epy(y)
h
Epz(z)
h
Erψ(ε|y,z)
h
Epθx(x|ε)
h
Eqφ∗ε (ε|x)
h
log pθ∗y(y|ε)
iiiii
, where p∗
θy(y
|
ε) cor-"
INFORMATION-THEORETICAL FORMULATION,0.3259668508287293,"responds to the classiﬁer and q∗
φε(ε|x) denotes the pre-trained encoder. Since H(Y) is independent
of the parameters of the encoder and classiﬁer, (4) reduces to the lower bound minimization:"
INFORMATION-THEORETICAL FORMULATION,0.3314917127071823,"(ˆθx, ˆψ) = argmin
θx,ψ
Hψ,θx,φ∗ε,θ∗y(Y|E),
(5)"
INFORMATION-THEORETICAL FORMULATION,0.3370165745856354,Under review as a conference paper at ICLR 2022
INFORMATION-THEORETICAL FORMULATION,0.3425414364640884,"that under the categorical conditional distribution pθy(y|ε) can be expressed as the categorical cross-
entropy Ly(y, ˜y)."
INFORMATION-THEORETICAL FORMULATION,0.34806629834254144,"Finally, the decoder should produce samples that follow the distribution of training data px(x) that
corresponds to the maximization of mutual information:"
INFORMATION-THEORETICAL FORMULATION,0.35359116022099446,"(ˆθx, ˆψ) = argmax
θx,ψ
Iψ,θx(X; E),
(6)"
INFORMATION-THEORETICAL FORMULATION,0.35911602209944754,"where
Iψ,θx(X; E)
=
Epx(x)
h
Epy(y)
h
Epz(z)
h
Erψ(ε|y,z)
h
Epθx(x|ε)
h
log pθx(x|ε)"
INFORMATION-THEORETICAL FORMULATION,0.36464088397790057,"px(x)
iiiii
="
INFORMATION-THEORETICAL FORMULATION,0.3701657458563536,"Epy(y)

Epz(z)

Erψ(ε|y,z) [DKL(pθx(x|E = ε)||pθx(x))]

−DKL(px(x)||pθx(x)), where pθx(x)
denotes the distribution of generated samples ˜x. Since DKL(pθx(x|E = ε)||pθx(x)) ≥0, the maxi-
mization of the above mutual information reduces to the minimization problem:"
INFORMATION-THEORETICAL FORMULATION,0.3756906077348066,"(ˆθx, ˆψ) = argmin
θx,ψ
DKL(px(x)||pθx(x)).
(7)"
INFORMATION-THEORETICAL FORMULATION,0.3812154696132597,"The above discriminator is denoted as Dx˜x(x). At the same time, one can also envision the discrim-
inator conditioned on the attribute class y Dx˜x(x | y) that is implemented as a set of discriminators
for each subset of generated and original samples deﬁned by y."
IMPLEMENTATION DETAILS,0.3867403314917127,"3
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.39226519337016574,"Dataset We test the proposed method on AFHQ (Choi et al. (2020)) and CelebA (Liu et al. (2015))
datasets. AFHQ dataset contains 16130 images belonging to 3 classes: cats, dogs, and wilds animals,
CelebA dataset contains 202599 face images with 40 binary attributes. We use AFHQ and CelebA
dataset for visual result inspection and AFHQ dataset for the ablations studies."
ENCODER,0.39779005524861877,"3.1
ENCODER"
ENCODER,0.40331491712707185,"The proposed encoder is designed to produce the interpretable latent space and it can be used for:
(i) internal latent exploration, (ii) feature metric like the VGG-loss (Ledig et al. (2017)), (iii) feature
extraction for the classiﬁcation of the generated samples with ”external” and ”internal” labels."
ENCODER,0.4088397790055249,"We have selected the SimCLR unsupervised encoder since it has shown state-of-the-art performance
in unsupervised learning on diverse datasets. By training the encoder in an unsupervised way, it is
trained to learn the inner data distribution, which is then used to compare real and generated data. For
both AFHQ and CelebA datasets we use Resnet50 (He et al. (2016)) as a base model. We pretrain
the SimCLR model for each dataset using contrastive NT-Xent loss (Sohn (2016)). We apply the
same augmentations as in the original SimCLR paper. The 2D t-SNE of the extracted features for
the AFHQ dataset is shown in Figure 2. The 2D t-SNEs of the extracted features for CelebA dataset
for selected attributes are shown in the Appendix A."
CLASSIFIER,0.4143646408839779,"3.2
CLASSIFIER"
CLASSIFIER,0.4198895027624309,"The initial idea of using a pre-trained classiﬁer to regularize the generative model is based on the
need to generate images that belong to the speciﬁc class. Training the classiﬁer jointly with the
generator and discriminator requires more time, and is inefﬁcient since in the early iterations the
generator network produces poorly generated images that are not similar to the real ones. While
it is possible to use L2 or other distance-based metrics to regularize the generator by comparing
embeddings between real and generated images computed using the encoder, it requires having
predeﬁned pairs, and since our goal is to develop a generative model, we use the pre-trained classiﬁer
to regularize the generator. We use a one-layer linear classiﬁer for classiﬁcation. As an input, we
use features extracted using the pre-trained encoder. When training on the AFHQ dataset we use the
cross-entropy loss, since each image has one attribute per image, when training on CelebA dataset,
we use the binary cross-entropy loss, since each image has multiple attributes per image."
CLASSIFIER,0.425414364640884,Under review as a conference paper at ICLR 2022
CLASSIFIER,0.430939226519337,"Figure 2: 2D t-SNE of the computed features from the AFHQ dataset. Color represents the class in
the dataset: yellow - cat; green - dog; dark violet - wild animal."
CLASSIFICATION REGULARIZATION,0.43646408839779005,"3.3
CLASSIFICATION REGULARIZATION"
CLASSIFICATION REGULARIZATION,0.4419889502762431,"Classiﬁcation regularization is used to force the generator to generate images conditionally. For
every image, that is generated by generator with given input attributes, the embeddings are computed
using the pre-trained encoder, and then the embeddings are classiﬁed using the pre-trained attributes’
classiﬁer. Generator weights are updated so that attributes predicted by the attribute classiﬁer are the
same as the input attributes. For the AFHQ dataset with one attribute per image, we use the cross-
entropy loss and for the CelebA dataset with multiple attributes per image, we use the binary cross-
entropy loss. Unlike other conditional generation methods, which apply classiﬁcation regularization
at each iteration, we apply the classiﬁcation at each n-th iteration, since it allows to balance between
the generation of diverse samples and the generation of the samples with speciﬁed attributes. The
frequency is selected for each experiment with the dataset and attributes. Unlike other methods,
where generator weights are updated on all losses at once, we update the generator on each loss
separately, so the adversarial loss would not saturate the classiﬁcation loss."
CLUSTERS,0.44751381215469616,"3.4
CLUSTERS"
CLUSTERS,0.4530386740331492,"We propose two approaches for selecting the dataset attributes using: (i) ”external” attributes either
provided with a dataset or annotated manually, (ii) ”inner” attributes, assigned by using K-means
(Lloyd (1982)) clustering on features extracted using the pre-trained encoder. The 2D t-SNE of the
AFHQ dataset features computed using the pre-trained encoder with the ”external” attributes from
the dataset is shown in Figure 2. There are more than 3 distinct clusters and some of the images
from the class of wild animals are semantically closer to the images from other classes. That means
that the pre-trained encoder produces semantic similarity. In Figure 3 2D t-SNE features from the
AFHQ dataset with the different numbers of the ”inner” attributes are shown."
EXPERIMENTS,0.4585635359116022,"4
EXPERIMENTS"
EXPERIMENTS,0.46408839779005523,"In this section, we describe the experiments and the results obtained for the ablation studies. For the
evaluation, we use 3 performance metrics: Fr´echet inception distance (FID) (Heusel et al. (2017)),
which is used to compare the distribution of generated images with the distribution of real images,
inception score (IS) (Salimans et al. (2016)), which is an objective metric for evaluating the quality of
generated images and Chamfer distance (Ravi et al. (2020)), which calculates the distance between
features of the images. To compute the Chamfer distance, we compute features of the real and
generated image by the pre-trained encoder, then compute the 3D t-SNEs of these features, which are
used to compute the Chamfer distance. To determine whether the conditional generated images obey
the needed attributes, we use attribute control accuracy. The attribute control accuracy is computed
as the percentage of the images for which the output of the attribute classiﬁer is the same as an
input attribute. The attribute control accuracy measures how good the generator is at conditionally
generating samples."
EXPERIMENTS,0.4696132596685083,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.47513812154696133,"(1) 3 clusters
(2) 4 clusters"
EXPERIMENTS,0.48066298342541436,"(3) 6 clusters
(4) 15 clusters"
EXPERIMENTS,0.4861878453038674,Figure 3: 2D TSNE of AFHQ dataset with different numbers of clusters.
EIGENGAN,0.49171270718232046,"4.1
EIGENGAN"
EIGENGAN,0.4972375690607735,"We compare the proposed InfoSCC-GAN with the ”vanilla” EigenGAN (He et al. (2021)) on the
AFHQ dataset. Our model is based on the same generator while using different inputs and con-
ditional regularization. In the current setup, EigenGAN has 6 layers each with 6 dimensions that
are used for interpretable and controllable features exploration. The ”vanilla” EigenGAN achieves
FID score of 29.02 and IS of 8.52 after 200000 training iterations on AFHQ dataset using global
discriminator and Hinge loss (Lim & Ye (2017)) (8, 9). The EigenGAN does not allow for in-
terpretable feature exploration for the wild animal images. It can be explained by the imbalance
since the ”wild” animals class includes multiple distinct subclasses such as tiger, lion, fox, and wolf,
which are not semantically close. The interpretable dimensions of the ”vanilla” EigenGAN trained
on AFHQ dataset are shown in Appendix B. For visualization purposes, we only show the layers
and dimensions with visually observable interpretations."
CONDITIONAL GENERATION,0.5027624309392266,"4.2
CONDITIONAL GENERATION"
CONDITIONAL GENERATION,0.5082872928176796,"We achieve the best FID score of 11.59 and IS of 11.06 using the InfoSCC-GAN approach after
200000 training iterations using Patch discriminator (Isola et al. (2017)) and LSGAN (Mao et al.
(2017)) loss (12, 13). In the current setup, we have 6 layers with 6 explorable dimensions. We use 3
cluster classes selected as 3 main clusters using K-means clustering from embeddings computed by
the pre-trained encoder. For visualization purposes, the only shown layers are the ones with visually
observable interpretations. The interpretable dimensions of the proposed InfoSCC-GAN are shown
in Appendix C."
ABLATION STUDIES,0.5138121546961326,"4.3
ABLATION STUDIES"
ABLATION STUDIES,0.5193370165745856,"In this section, we describe the ablation studies we have performed on the type of discriminator, the
discriminator loss, and the number of clusters in the dataset."
ABLATION STUDIES,0.5248618784530387,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.5303867403314917,Table 1: Discriminator ablation studies
ABLATION STUDIES,0.5359116022099447,"Discriminator
Loss
FID ↓
IS ↑
Chamfer distance ↓
Global
Hinge
13.08
10.71
4030
Global
Non saturating
25.62
10.33
28595
Global
LSGAN
29.02
9.89
45583
Patch
Hinge
15.95
10.51
7327
Patch
Non saturating
14.83
10.21
5114
Patch
LSGAN
11.59
11.06
3645"
DISCRIMINATOR ABLATION STUDIES,0.5414364640883977,"4.3.1
DISCRIMINATOR ABLATION STUDIES"
DISCRIMINATOR ABLATION STUDIES,0.5469613259668509,"In this section, we describe the discriminator and loss ablation studies. For experiments, we use
the AFHQ dataset with 3 ”inner” clusters. We compare two discriminators: global discrimina-
tor and patch discriminator.
The global discriminator outputs one value that is the probability
of the image being real. The architecture of the global discriminator is inspired by the Eigen-
GAN paper. The patch discriminator outputs a tensor of values that represent the probability of
the image patch being real, the architecture of the patch discriminator is inspired by the pix2pix
GAN (Isola et al. (2017)).
We compare these discriminators in combination with discrimina-
tor losses: Hinge loss (8,9), non-saturating loss(10, 11) and LSGAN loss(12, 13). The results
of the studies are presented in the Table. 1. The visual results are presented in the supplement
https://anonymous.4open.science/r/InfoSCC-GAN-D113/README.md. The 2D
t-SNEs computed from the embeddings are shown in Figure 4. For all of the discriminators and
losses, used in the study, the attribute control accuracy is in the range of 99-100%."
DISCRIMINATOR ABLATION STUDIES,0.5524861878453039,"In the formulas below ˆx = pθx(ε|y) - is a generated image, Dx˜x - is the discriminator."
DISCRIMINATOR ABLATION STUDIES,0.5580110497237569,The Hinge discriminator loss is deﬁned as following:
DISCRIMINATOR ABLATION STUDIES,0.56353591160221,"LDx˜x = Epx(x) [max(0, 1 −Dx˜x(x))] + Eqφε(ε) [max(0, 1 −Dx˜x(ˆx))] .
(8)"
DISCRIMINATOR ABLATION STUDIES,0.569060773480663,The Hinge generator loss is deﬁned as following:
DISCRIMINATOR ABLATION STUDIES,0.574585635359116,"Lpθx = Eqφε(ε) [max(0, 1 −Dx˜x(ˆx))] .
(9)"
DISCRIMINATOR ABLATION STUDIES,0.580110497237569,The non-saturating discriminator loss is deﬁned as following (Lucic et al. (2018)):
DISCRIMINATOR ABLATION STUDIES,0.585635359116022,"LDx˜x = −Epx(x) [log(Dx˜x(x))] −Eqφε(ε) [log(1 −Dx˜x(ˆx))] .
(10)"
DISCRIMINATOR ABLATION STUDIES,0.5911602209944752,The non-saturating generator loss is deﬁned as following:
DISCRIMINATOR ABLATION STUDIES,0.5966850828729282,"Lpθx = −Eqφε(ε) [Dx˜x(ˆx)] .
(11)"
DISCRIMINATOR ABLATION STUDIES,0.6022099447513812,The LSGAN discriminator loss is deﬁned as following:
DISCRIMINATOR ABLATION STUDIES,0.6077348066298343,"LDx˜x = −Epx(x)

(Dx˜x(x) −1)2
+ Eqφε(ε)

Dx˜x(ˆx)2
.
(12)"
DISCRIMINATOR ABLATION STUDIES,0.6132596685082873,The LSGAN generator loss is deﬁned as following:
DISCRIMINATOR ABLATION STUDIES,0.6187845303867403,"Lpθx = −Eqφε(ε)

(Dx˜x(ˆx) −1)2
.
(13)"
NUMBER OF CLUSTERS ABLATION STUDY,0.6243093922651933,"4.3.2
NUMBER OF CLUSTERS ABLATION STUDY"
NUMBER OF CLUSTERS ABLATION STUDY,0.6298342541436464,"First, we compare the performance of the InfoSCC-GAN for the AFHQ dataset with the ”external”
attributes and with the ”inner” attributes of the same number of clusters as classes in ”external”
attributes. The results of the comparison study are shown in Table 2. The attribute control accuracy
metrics are not included in the table, since they are 100% for both types of attributes. As it is shown
in Table 2, we achieve better performance with the ”inner” attributes for all of the metrics: FID, IS,
and Chamfer distance. It can be explained by the fact that when selecting the ”inner” attributes the
images with the same attributes are semantically closer."
NUMBER OF CLUSTERS ABLATION STUDY,0.6353591160220995,Under review as a conference paper at ICLR 2022
NUMBER OF CLUSTERS ABLATION STUDY,0.6408839779005525,"(1) Global discriminator, Hinge loss
Chamfer distance: 4030"
NUMBER OF CLUSTERS ABLATION STUDY,0.6464088397790055,"(2) Patch discriminator, Hinge loss
Chamfer distance: 7327"
NUMBER OF CLUSTERS ABLATION STUDY,0.6519337016574586,"(3) Global discriminator, Non saturating loss
Chamfer distance: 28595"
NUMBER OF CLUSTERS ABLATION STUDY,0.6574585635359116,"(4) Patch discriminator, Non saturating loss
Chamfer distance: 5114"
NUMBER OF CLUSTERS ABLATION STUDY,0.6629834254143646,"(5) Global discriminator, Non LSGAN loss
Chamfer distance: 45583"
NUMBER OF CLUSTERS ABLATION STUDY,0.6685082872928176,"(6) Patch discriminator, LSGAN loss
Chamfer distance: 3645"
NUMBER OF CLUSTERS ABLATION STUDY,0.6740331491712708,"Figure 4: 2D TSNE of AFHQ dataset and conditionally generated samples using different methods.
On each plot 0 - cat images from the dataset; 1 - dog images from the dataset; 2 - wild images from
the dataset; 3 - generated cat images; 4 - generated dog images; 5 - generated wild images."
NUMBER OF CLUSTERS ABLATION STUDY,0.6795580110497238,Table 2: Comparison of ”external” and ”inner” attributes
NUMBER OF CLUSTERS ABLATION STUDY,0.6850828729281768,"Attribute type
FID ↓
IS ↑
Chamfer distance ↓
Inner
13.08
10.71
4030
External
14.30
10.11
4423"
NUMBER OF CLUSTERS ABLATION STUDY,0.6906077348066298,"In this section, we compare generation results on InfoSCC-GAN on the AFHQ dataset, when work-
ing with different numbers of clusters. By default, we use 3 clusters. We compare image generation
when selecting 4, 6, and 15 clusters. For the experiments, we train the generator with a global dis-"
NUMBER OF CLUSTERS ABLATION STUDY,0.6961325966850829,Under review as a conference paper at ICLR 2022
NUMBER OF CLUSTERS ABLATION STUDY,0.7016574585635359,Table 3: Number of clusters ablation study
NUMBER OF CLUSTERS ABLATION STUDY,0.7071823204419889,"Number of clusters
FID ↓
IS ↑
Chamfer distance ↓
3
13.08
10.71
4030
4
97.37
6.77
44113
6
148.7
5.45
94858
15
101.4
7.15
54016"
NUMBER OF CLUSTERS ABLATION STUDY,0.712707182320442,Table 4: Conditional generation results on CelebA dataset with 5 selected attributes
NUMBER OF CLUSTERS ABLATION STUDY,0.7182320441988951,"FID ↓
IS ↑
Attribute Control Accuracy ↑
Bald
Eyeglasses
Mustache
Wearing Hat
Wearing Necktie
27.84
9.91
93.27%
99.88%
95.68%
94.62%
98.62%"
NUMBER OF CLUSTERS ABLATION STUDY,0.7237569060773481,"criminator, with Hinge loss for 200000 iterations. The comparison results of the numbers of clusters
are presented in Table 3, we also compute an attribute control accuracy for each experiment, and for
all of them the metrics are 99-100%. The best generation results are achieved when using 3 ”inner”
classes, which can be explained by the balanced number of samples in each class - approximately
5000 of semantically similar images in each class, since classes were selected by clustering the
embeddings."
CELEBA EXPERIMENTS,0.7292817679558011,"4.4
CELEBA EXPERIMENTS"
CELEBA EXPERIMENTS,0.7348066298342542,"We run the experiments with CelebA dataset with the different numbers of attributes: 5, 10, and 15
attributes. We have selected the attributes, which can easily and unambiguously be distinguished, so
we can perform a visual check of how the proposed conditional generation method performs. For
the evaluation, we use the FID score, IS, and attribute control accuracy metrics. We have not used
Chamfer distance for the embeddings computed using the pre-trained encoder, since the embeddings
are very dense and do not have visible inner cluster structure, but they are still explorable as it is
shown in Figure 5. The results on the conditional generation using InfoSCC-GAN with 5 selected
attributes are shown in Table 4. When training the model, we have used discrete binary attributes,
but we have discovered that the model is able to generate from continuous input attributes from [0, 1]
range in an interpretable way, meaning that model learns to apply each attribute to a different degree,
depending on the input attribute, if value is closer to 1, the effect of attribute is bigger. This feature
is explored in both CelebA demos. We show that proposed InfoSCC-GAN is able to stochastically
conditionaly generate samples with multiple attributes. Results with 10 and 15 attributes are shown
in the Appendix D."
CONCLUSIONS,0.7403314917127072,"5
CONCLUSIONS"
CONCLUSIONS,0.7458563535911602,"In this paper, we propose a novel stochastic contrastive conditional GAN InfoSCC-GAN, which
produces stochastic conditional image generation with an explorable latent space. We provide the
information-theoretical formulation of the proposed system. Unlike other contrastive image genera-
tion approaches, our method is truly a stochastic generator, that is controlled by the class attributes
and by the set of stochastic parameters. We apply a novel training methodology based on using a pre-
trained unsupervised contrastive encoder and a pre-trained classiﬁer with every n-th iteration using
a classiﬁcation regularization. We propose an information-theoretical interpretation of the proposed
system. We propose a novel attribute selection approach based on clustering embeddings computed
using an encoder. The proposed model outperforms ”vanilla” EigenGAN on AFHQ dataset, while it
also provides conditional image generation. We have performed ablations studies to determine the
best setup for conditional image generation. Finally, we have performed experiments on AFHQ and
CelebA datasets."
CONCLUSIONS,0.7513812154696132,Under review as a conference paper at ICLR 2022
REFERENCES,0.7569060773480663,REFERENCES
REFERENCES,0.7624309392265194,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. ArXiv, abs/2002.05709, 2020."
REFERENCES,0.7679558011049724,"Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis
for multiple domains. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 8185–8194, 2020."
REFERENCES,0.7734806629834254,"Jean-Bastien Grill, Florian Strub, Florent Altch’e, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo ´Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, Bilal Piot, Koray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own latent:
A new approach to self-supervised learning. ArXiv, abs/2006.07733, 2020."
REFERENCES,0.7790055248618785,"Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016."
REFERENCES,0.7845303867403315,"Zhenliang He, Meina Kan, and S. Shan. Eigengan: Layer-wise eigen-learning for gans. ArXiv,
abs/2104.12476, 2021."
REFERENCES,0.7900552486187845,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017."
REFERENCES,0.7955801104972375,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with
conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pp. 5967–5976, 2017."
REFERENCES,0.8011049723756906,"Mingu Kang and Jaesik Park. Contrastive generative adversarial networks. ArXiv, abs/2006.12681,
2020."
REFERENCES,0.8066298342541437,"Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani,
Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution us-
ing a generative adversarial network. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 105–114, 2017."
REFERENCES,0.8121546961325967,"Jae Hyun Lim and J. C. Ye. Geometric gan. ArXiv, abs/1705.02894, 2017."
REFERENCES,0.8176795580110497,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.8232044198895028,"S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):
129–137, 1982. doi: 10.1109/TIT.1982.1056489."
REFERENCES,0.8287292817679558,"Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. In NeurIPS, 2018."
REFERENCES,0.8342541436464088,"Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. 2017 IEEE International Conference on Computer
Vision (ICCV), pp. 2813–2821, 2017."
REFERENCES,0.8397790055248618,"Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. ArXiv, abs/1411.1784,
2014."
REFERENCES,0.8453038674033149,"Takeru Miyato and Masanori Koyama. cgans with projection discriminator. ArXiv, abs/1802.05637,
2018."
REFERENCES,0.850828729281768,"Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classiﬁer gans. In ICML, 2017."
REFERENCES,0.856353591160221,"Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson,
and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020."
REFERENCES,0.861878453038674,"Tim Salimans, I. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Im-
proved techniques for training gans. In NIPS, 2016."
REFERENCES,0.8674033149171271,Under review as a conference paper at ICLR 2022
REFERENCES,0.8729281767955801,"Kihyuk
Sohn.
Improved
deep
metric
learning
with
multi-class
n-pair
loss
objec-
tive.
In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Ad-
vances
in
Neural
Information
Processing
Systems,
volume
29.
Curran
Associates,
Inc.,
2016.
URL
https://proceedings.neurips.cc/paper/2016/file/
6b180037abbebea991d8b1232f8a8ca9-Paper.pdf."
REFERENCES,0.8784530386740331,"A¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. ArXiv, abs/1807.03748, 2018."
REFERENCES,0.8839779005524862,"Jure Zbontar, Li Jing, Ishan Misra, Yann Andr´e LeCun, and St´ephane Deny. Barlow twins: Self-
supervised learning via redundancy reduction. In ICML, 2021."
REFERENCES,0.8895027624309392,Under review as a conference paper at ICLR 2022
REFERENCES,0.8950276243093923,"A
2D TSNE OF CELEBA FEATURES COMPUTED USING ENCODER"
REFERENCES,0.9005524861878453,"(1) Bald
(2) Black hair
(3) Blond hair"
REFERENCES,0.9060773480662984,"(4) Blurry
(5) Brown hair
(6) Chubby"
REFERENCES,0.9116022099447514,"(7) Double chin
(8) Eyeglasses
(9) Goatee"
REFERENCES,0.9171270718232044,"(10) Gray hair
(11) Male
(12) Mustache"
REFERENCES,0.9226519337016574,"(13) Narrow eyes
(14) Pale skin
(15) Receding hairline"
REFERENCES,0.9281767955801105,"(16) Rosy cheeks
(17) Sideburns
(18) Wearing hat"
REFERENCES,0.9337016574585635,Figure 5: 2D TSNE of CelebA features computed using encoder.
REFERENCES,0.9392265193370166,Under review as a conference paper at ICLR 2022
REFERENCES,0.9447513812154696,"B
”VANILLA” EIGENGAN GENERATION RESULTS ON AFHQ DATASET"
REFERENCES,0.9502762430939227,Figure 6: Interpretable dimensions in ”vanilla” EigenGAN
REFERENCES,0.9558011049723757,"C
INFOSCC-GAN CONDITIONAL GENERATION RESULTS ON AFHQ
DATASET"
REFERENCES,0.9613259668508287,Figure 7: Interpretable dimensions in InfoSCC-GAN
REFERENCES,0.9668508287292817,Under review as a conference paper at ICLR 2022
REFERENCES,0.9723756906077348,"D
INFOSCC-GAN CONDITIONAL GENERATION RESULTS ON CELEBA
DATASET"
REFERENCES,0.9779005524861878,Table 5: Conditional generation results on CelebA dataset with 10 selected attributes
REFERENCES,0.9834254143646409,"FID↓
IS↑
Attribute Control Accuracy↑
Bald
Black Hair
Blond Hair
Brown Hair
Double Chin
32.39
9.04
89.74%
89.61%
86.86%
85.55%
84.82%
Eyeglasses
Gray Hair
Mustache
Wearing Hat
Wearing Necktie
99.6%
81.71%
92.27%
92.83%
89.26%"
REFERENCES,0.988950276243094,Table 6: Conditional generation results on CelebA dataset with 15 selected attributes
REFERENCES,0.994475138121547,"FID↓
IS↑
Attribute Control Accuracy↑
Bald
Blurry
Chubby
Double Chin
Eyeglasses
34.97
8.87
83.6%
96.46%
80.1%
95.74%
98.11%
Goatee
Gray Hair
Mustache
Narrow Eyes
Pale Skin
89.09%
90.78%
87.64%
74.22%
86.91%
Receding Hairline
Rosy Chicks
Sideburns
Wearing Hat
Wearing Necktie
86.46%
78.88%
74.9%
97.64%
94.87%"
