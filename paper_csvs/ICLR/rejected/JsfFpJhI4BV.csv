Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0031847133757961785,"Many machine learning techniques incorporate identity-preserving transforma-
tions into their models to generalize their performance to previously unseen data.
These transformations are typically selected from a set of functions that are known
to maintain the identity of an input when applied (e.g., rotation, translation, ﬂip-
ping, and scaling). However, there are many natural variations that cannot be
labeled for supervision or deﬁned through examination of the data. As suggested
by the manifold hypothesis, many of these natural variations live on or near a low-
dimensional, nonlinear manifold. Several techniques represent manifold varia-
tions through a set of learned Lie group operators that deﬁne directions of motion
on the manifold. However theses approaches are limited because they require
transformation labels when training their models and they lack a method for de-
termining which regions of the manifold are appropriate for applying each speciﬁc
operator. We address these limitations by introducing a learning strategy that does
not require transformation labels and developing a method that learns the local
regions where each operator is likely to be used while preserving the identity of
inputs. Experiments on MNIST and Fashion MNIST highlight our model’s ability
to learn identity-preserving transformations on multi-class datasets. Additionally,
we train on CelebA to showcase our model’s ability to learn semantically mean-
ingful transformations on complex datasets in an unsupervised manner."
INTRODUCTION,0.006369426751592357,"1
INTRODUCTION"
INTRODUCTION,0.009554140127388535,"A goal of many machine learning models is to accurately identify objects as they undergo natu-
ral transformations – a task that humans are adept at. According to the manifold hypothesis, natural
variations in high-dimensional data lie on or near a low-dimensional, nonlinear manifold (Fefferman
et al., 2016). Additionally, the manifolds representing different classes are separated by low density
regions (Rifai et al., 2011a). Natural physical laws govern the possible transformations that objects
can undergo and many of the identity-preserving transformations (e.g., changes in viewpoint, color,
and lighting) are shared among classes of data. Sharing of transformations between classes enables
increased efﬁciency in deﬁning data variations – a model can represent a limited set of transforma-
tions that can describe a majority of variations in many classes. Several machine learning models
incorporate speciﬁc identity-preserving transformations that are shared among a large number of
classes to generalize the performance of their model to unseen data. These include equivariant mod-
els that incorporate transformations like translation and rotation into intermediate network layers
(Cohen & Welling, 2016; Cohen et al., 2018) and data augmentation techniques that apply known
identity-preserving variations to data while training (Cubuk et al., 2019; Ho et al., 2019; Lim et al.,
2019; Sohn et al., 2020; He et al., 2020; Chen et al., 2020). However, many datasets have natu-
ral transformations shared among classes that are not easily prespeciﬁed from intuition, making it
critical that we develop a model that can learn both 1) a representation for these transformations
without explicit transformation labels and 2) the context for which locations in the data space each
transformation is likely to be relevant."
INTRODUCTION,0.012738853503184714,"Manifold learning strategies estimate the low-dimensional manifold structure of data. A subset
of these techniques learn to transform points on the manifold through nonlinear Lie group opera-
tors (Rao & Ruderman, 1999; Miao & Rao, 2007; Culpepper & Olshausen, 2009; Sohl-Dickstein
et al., 2010; Cohen & Welling, 2014; Hauberg et al., 2016; Connor & Rozell, 2020; Connor et al.,
2021). Lie group operators represent inﬁnitesimal transformations which can be applied to data"
INTRODUCTION,0.01592356687898089,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01910828025477707,"through an exponential mapping to transform points along a manifold, and a manifold can be glob-
ally deﬁned by a set of operators that each move in different directions along it (Hoffman, 1966;
Dodwell, 1983). A Lie group operator model is well-suited for representing natural data variations
because the operators can be learned from the data, applied to data points to transform them beyond
their local neighborhoods, and used to estimate geodesic paths."
INTRODUCTION,0.022292993630573247,"While the Lie group operator models have many beneﬁts, previous approaches demonstrate the two
shortcomings noted above. First, to learn Lie group operators that represent a data manifold, pairs
of training points are selected which lie within a neighborhood of one another. The training objec-
tive encourages efﬁcient paths between these nearby points and the choice of training point pairs
inﬂuences the types of manifold transformations that are learned. Recent papers incorporating Lie
group operators into machine learning models have either used predeﬁned operators that represent
known transformations groups (e.g., the 3D rotational group SO(3) (Falorsi et al., 2019)), required
transformation labels for selecting point pairs when training (Connor & Rozell, 2020), or randomly
selected pairs of points from the same class (Connor et al., 2021). To learn an effective model with
datasets having no labeled transformation structure, we require a point pair selection strategy that
identiﬁes points that are related through the transformations the model aims to learn. Second, exist-
ing Lie group operator models have lacked a method for determining which regions of the manifold
are appropriate for each operator, meaning that every operator is equally likely to be used at every
point on the manifold. This is a ﬂawed assumption because, while many transformations are shared
between classes, there are also data variations that are unique to speciﬁc data classes. Additionally,
in a dataset with several manifolds (each representing a different class), there is a limited extent to
which a transformation can be applied without moving a point onto another manifold."
INTRODUCTION,0.025477707006369428,"The main contributions of this paper are the development of methods to address the two critical
shortcomings of Lie group operator-based manifold models noted above. Speciﬁcally, motivated
by ﬁnding perceptually similar training samples without transformation labels, we ﬁrst introduce a
point pair selection strategy to learn a manifold representation of natural variations shared across
multiple data classes without requiring transformations labels. Second, we develop a method that
uses a pretrained classiﬁer (measuring identity-preservation of transformed samples) to learn the
local regions where each operator is likely to be used while preserving the identity of transformed
samples. This approach enables us to analyze the local structure of the data manifold in the context
of the learned operators and to describe the invariances of the classiﬁer. We demonstrate the efﬁcacy
of these strategies in the context of the Manifold Autoencoder (MAE) model (Connor & Rozell,
2020) to learn semantically meaningful transformations on MNIST (LeCun et al., 1998), Fashion
MNIST (Xiao et al., 2017), and CelebA (Liu et al., 2015)."
BACKGROUND,0.028662420382165606,"2
BACKGROUND"
BACKGROUND,0.03184713375796178,"Manifold Learning
Manifold learning models estimate the low-dimensional structure of high-
dimensional data by utilizing the property that local neighborhoods on the manifold are approxi-
mately linear. Traditional techniques represent the manifold through a low-dimensional embedding
of the data points (Tenenbaum et al., 2000; Roweis & Saul, 2000; Belkin & Niyogi, 2003; Maaten
& Hinton, 2008) or through estimates of linear tangent planes that represent local directions of man-
ifold motion (Doll´ar et al., 2007; Bengio & Monperrus, 2005; Park et al., 2015). While these tra-
ditional manifold learning approaches are useful for understanding low-dimensional data structure,
in many cases the input data space is an inefﬁcient representation of the data. For example, data in
the pixel space suffers from the curse of dimensionality and cannot be smoothly interpolated while
maintaining identity (Bengio et al., 2005). Many approaches have used neural networks to learn
a low-dimensional latent space in which manifold models can be incorporated. The contractive
autoencoder (CAE) estimates manifold tangents by minimizing the the Jacobian of the encoder net-
work, encouraging invariance of latent vectors to image space perturbations (Rifai et al., 2011c;b;a;
Kumar et al., 2017). Several methods estimate geodesic paths in the latent space of a trained vari-
ational autoencoder (VAE) model (Arvanitidis et al., 2018; Chen et al., 2018; Shao et al., 2018;
Arvanitidis et al., 2019) and use this approach to learn VAEs with priors that are estimated using the
Riemannian metrics computed in the latent space (Arvanitidis et al., 2021; Kalatzis et al., 2020)."
BACKGROUND,0.03503184713375796,"Lie Group Operators
A Lie group is a group of continuous transformations which also deﬁnes a
manifold by representing inﬁnitesimal transformations that can be applied to input data (Hoffman,"
BACKGROUND,0.03821656050955414,Under review as a conference paper at ICLR 2022
BACKGROUND,0.041401273885350316,"1966; Dodwell, 1983). Several methods incorporate Lie groups into neural networks to represent
data transformations that are identity-preserving within the model (Cohen & Welling, 2014; Cohen
et al., 2018; Cosentino et al., 2021). A prevalent strategy is to learn a dictionary of Lie group
operators that are mapped to a speciﬁc group element through the matrix exponential expm(·) (Rao
& Ruderman, 1999; Ham & Lee, 2006; Miao & Rao, 2007; Culpepper & Olshausen, 2009; Sohl-
Dickstein et al., 2010; Cohen & Welling, 2014; Connor & Rozell, 2020; Connor et al., 2021). In
these models, each operator Ψm, called a transport operator, describes a single direction along the
manifold and is parameterized by a single coefﬁcient cm. Given an initial data point z, the transport
operators deﬁne a generative model where transformations can be derived from sampling sparse
coefﬁcients cm ∼Laplace (0, ζ):"
BACKGROUND,0.044585987261146494,"bz = expm M
X"
BACKGROUND,0.04777070063694268,"m=1
Ψmcm !"
BACKGROUND,0.050955414012738856,"z + ϵ,
(1)"
BACKGROUND,0.054140127388535034,"where ϵ ∼N(0, σ2
ϵ I)."
BACKGROUND,0.05732484076433121,"The manifold autoencoder (MAE) incorporates the transport operator model into the latent space
of an autoencoder to learn a dictionary of operators that represent the global, nonlinear manifold
structure in the latent space (Connor & Rozell, 2020). This model has been shown to effectively learn
reusable operators with transformation supervision, and it will provide the context for demonstrating
the effectiveness of the methods developed in this paper."
BACKGROUND,0.06050955414012739,"Disentanglement
One class of methods that also aim to identify factors of variation in data is dis-
entanglement methods which learn features that each vary one independent characteristic of the data
(Higgins et al., 2018). In a supervised learning scenario, there are many disentangling techniques
that separate data style from content by encouraging similarity between the content features in sam-
ples from the same class (Tenenbaum & Freeman, 2000; Reed et al., 2014; Mathieu et al., 2016;
Bouchacourt et al., 2018) or encouraging features to map to known class (Cheung et al., 2015)
or transformation labels (Hinton et al., 2011; Kulkarni et al., 2015; Yang et al., 2015). There are
also techniques that can disentangle latent representations without labels like InfoGAN (Chen et al.,
2016) and β-VAE (Higgins et al., 2017). The goal of our work is distinct from disentanglement
work because, rather than identifying independently varying factors of variation, we aim to learn
non-linear operators that correspond to transformations on the data manifold (which may not vary
independently). Our approach is advantageous because it can faithfully represent longer transfor-
mation paths, learn variations while maintaining image reconstruction, and change the number of
learned variations by increasing or decreasing the number of learned operators."
METHODS,0.06369426751592357,"3
METHODS"
METHODS,0.06687898089171974,"The MAE learns a low-dimensional latent representation of the data by deﬁning an encoder function
f : X →Z that maps high-dimensional data points x ∈RD to low-dimensional latent vectors z ∈
Rd and a decoder function g : Z →X that maps the latent vectors back into the data space (Connor
& Rozell, 2020). Transport operators Ψ are incorporated into the latent space to learn manifold-
based transformations. Before learning the transport operators, the autoencoder is pretrained to
extract a latent representation of the data using the traditional autoencoder reconstruction objective."
METHODS,0.07006369426751592,"After pretraining, the autoencoder weights are ﬁxed and the operators are trained with the following
objective, which encourages the learning of transport operators that generate efﬁcient paths between
the latent vectors z0 and z1 (coinciding with f(x0) and f(x1)) that are nearby on the manifold:"
METHODS,0.0732484076433121,LΨ = 1 2
METHODS,0.07643312101910828,"z1 −expm M
X"
METHODS,0.07961783439490445,"m=1
Ψmcm ! z0  2 2
+ γ 2 X"
METHODS,0.08280254777070063,"m
∥Ψm∥2
F + ζ∥c∥1,
(2)"
METHODS,0.08598726114649681,"where γ, ζ > 0."
METHODS,0.08917197452229299,"Objective (2) is minimized via an alternating minimization scheme. Speciﬁcally, at each training
iteration, points pairs x0 and x1 are selected in the input space and encoded into the latent space
z0 and z1. Then, coefﬁcients c are inferred between the encoded latent vectors (by minimizing (2)
with respect to c) to estimate the best path between z0 and z1. After inference, the coefﬁcients are"
METHODS,0.09235668789808917,Under review as a conference paper at ICLR 2022
METHODS,0.09554140127388536,"ﬁxed and a gradient step is taken to minimize (2) with respect to transport operator weights. Once
learned, these operators represent different types of motion that traverse the manifold and they can
be combined to generate natural paths on the manifold."
METHODS,0.09872611464968153,"After ﬁtting transport operators to the latent space of a ﬁxed autoencoder network, there is a ﬁnal
ﬁne-tuning training phase that updates the network weights and transport operators simultaneously
using a joint objective that combines the autoencoder reconstruction loss with LΨ. This ﬁne-tuning
step addresses the potential of a mismatch between the data manifold and the learned latent structure
by adapting the latent structure to ﬁt the transport operators learned between the selected training
point pairs. We empirically ﬁnd that breaking up training into these three phases increases the
stability with which the transport operators can be learned. Given the context of this MAE model,
in the following sections we describe our contributions."
UNSUPERVISED TRANSFORMATION LEARNING,0.10191082802547771,"3.1
UNSUPERVISED TRANSFORMATION LEARNING"
UNSUPERVISED TRANSFORMATION LEARNING,0.10509554140127389,"There are many possible strategies for selecting training point pairs for a Lie group operator model
and the choice of a strategy can dictate the types of learned transformations. Point pairs may be
selected as random samples from the dataset or from within the same class. These points, however,
are likely to be outside of local manifold neighborhoods and may not provide representations of the
natural, perceptually smooth variations. Point pairs can also be selected as nearest neighbors in the
pixel or autoencoder latent space (denoted with h(x) = x and h(x) = f(x) in (3), respectively):"
UNSUPERVISED TRANSFORMATION LEARNING,0.10828025477707007,"x1 = arg min
x∈D
∥h(x0) −h(x)∥2
2.
(3)"
UNSUPERVISED TRANSFORMATION LEARNING,0.11146496815286625,"However, semantic transformations of interest may not be exhibited through pixel similarity nor
through unsupervised autoencoder feature similarity. In some applications there is additional infor-
mation that can aid in point pair selection, such as rotation angle, semantic label, or time index in a
temporal sequence. However, for most complex datasets, information about the transformations of
interest is not available. This necessitates a strategy for selecting training point pairs which can be
used to learn natural transformations without requiring additional transformation supervision."
UNSUPERVISED TRANSFORMATION LEARNING,0.11464968152866242,"We generalize transport operator training to incorporate an unsupervised learning strategy that can
be applied to a wide array of datasets. In particular, we assume access to a classiﬁer pretrained on
a large-scale dataset (likely taken from a model zoo) with similar statistics as our training dataset of
interest (Nguyen et al., 2020). In this approach, we select point pairs by setting h(x) from (3) equal
to the penultimate layer of the classiﬁer, similar to the previously proposed perceptual loss (Johnson
et al., 2016). It has been shown that high-level features in such classiﬁer networks correspond to
semantic characteristics of the data and ﬁnding points nearby in this feature space can result in image
pairs that are perceptually similar without being exactly the same (Johnson et al., 2016; Yosinski
et al., 2015). Incorporating the classiﬁer features as a supervision strategy for point pair selection
enables our model to learn transport operators without requiring transformation labels or class labels.
We show that this metric leads to learned operators that correspond to semantically identiﬁable
transformations. Discussion of different point pair strategies can be found in Appendix E."
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.1178343949044586,"3.2
LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.12101910828025478,"A trained transport operator model provides a dictionary of operators that can be applied glob-
ally across the entire data space. This model has ﬂexibility to deﬁne local manifold characteristics
through the coefﬁcients c which specify the combination of operators to apply to a given point. The
standard prior on the coefﬁcients is a factorial Laplace distribution parameterized by a single scale
parameter ζ, which encourages transformations to be deﬁned by a sparse set of operators:"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.12420382165605096,"pζ(c) = M
Y m=1"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.12738853503184713,"1
2ζ exp

−|cm| ζ"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.1305732484076433,"
.
(4)"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.1337579617834395,"By setting a ﬁxed prior on coefﬁcients across the entire manifold, there is an implicit assumption
that the manifold structure is the same over the whole dataset and every operator is equally likely
to be applied at every point. However, this is a ﬂawed assumption because not all transformations"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.13694267515923567,Under review as a conference paper at ICLR 2022
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.14012738853503184,"in one class of data are present in all other classes and within classes there may be regions with
different manifold structure. Failing to capture the local statistics of transport operator usage could
result in transformed points that depart from the data manifold and change their identity."
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.14331210191082802,"To address this limitation, we introduce a coefﬁcient encoder network which maps latent vectors
to scale parameters that specify the Laplace distribution on the coefﬁcients at those latent points.
The goal of this network is to estimate local coefﬁcient distributions that maximize the identity-
preservation of points (as measured by a pretrained classiﬁer) transformed with operators character-
ized by sampled coefﬁcients as in (1). Given labeled observations (z, y) and a pretrained classiﬁer
on the latent space r(·), we aim to learn a network qφ(c|z) that outputs parameters that can be used
to produce augmented samples bz without changing the classiﬁer output r(bz) = y."
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.1464968152866242,"To maximize the likelihood of augmented input data maintaining the same classiﬁer output, we adapt
the concept of consistency regularization from semi-supervised learning (Bachman et al., 2014; Sohn
et al., 2020). In our context, we have a pretrained classiﬁer and a dictionary of transport operators
and we aim to ﬁnd a distribution on the coefﬁcients at individual input points that result in consistent
classiﬁcation outputs when TΨ(c) is applied. The speciﬁc objective can be chosen from a variety of
loss functions that encourage similarity in classiﬁer probability outputs. We speciﬁcally minimize
the KL-divegence between the classiﬁer output r(z) and the estimated probability of the transformed
output r(bz)."
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.14968152866242038,"Unfortunately, the consistency regularization objective can be trivially minimized by setting c = 0,
resulting in an identity transformation and the same classiﬁer output. Therefore, we want to encour-
age the largest coefﬁcient values possible while maintaining the identity of our initial data point.
This motivates the addition of a KL-divergence regularizer that encourages the distribution qφ(c|z)
to be similar to a speciﬁed Laplace prior with a ﬁxed scale ζ like in (4). Our ﬁnal objective for
training the coefﬁcient encoder network is: (a more detailed derivation can be found in Appendix A)"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.15286624203821655,"E = DKL(r(z)∥r(bz)) + DKL (qφ(c|z)∥pζ(c)) .
(5)"
"LEARNING LOCAL TRANSPORT OPERATOR STATISTICS TO ENCOURAGE IDENTITY
PRESERVATION",0.15605095541401273,"The coefﬁcient encoder introduces a principled way to build identity-preservation directly into our
model and to identify local manifold characteristics throughout the latent space. Additionally, it
can quantify the extent with which operators can be applied in different parts of the latent space
without changing class prediction, indicating transformations to which the classiﬁer is invariant.
Likewise, points or region that have small encoded coefﬁcient scale weights indicate closeness to
class boundaries and can undergo only small transformations without changing identity."
DECREASING COMPUTATIONAL COMPLEXITY OF COEFFICIENT INFERENCE,0.1592356687898089,"3.3
DECREASING COMPUTATIONAL COMPLEXITY OF COEFFICIENT INFERENCE"
DECREASING COMPUTATIONAL COMPLEXITY OF COEFFICIENT INFERENCE,0.1624203821656051,"To compute the gradient for the matrix exponential during training and inference, previous works
have used a linear approximation (Rao & Ruderman, 1999), learned the operators in a factored form
(Sohl-Dickstein et al., 2010), or used analytic gradients that are not favorable for parallel compu-
tations (Culpepper & Olshausen, 2009; Connor & Rozell, 2020; Connor et al., 2021). The main
computational bottleneck for learning transport operators is the coefﬁcient inference step, where (2)
is minimized with respect to c. The ℓ1 norm term is non-smooth, leading to slower convergence
when using subgradients found via automatic differentiation (Boyd & Vandenberghe, 2004). In this
work we employ a forward-backward splitting scheme (Beck & Teboulle, 2009) that alternates be-
tween automatic differentiation of the matrix exponential term, and proximal gradients for the ℓ1
norm (Parikh & Boyd, 2014). This signiﬁcantly speeds up the training process and allows for scal-
ing up the learning of transport operators to complex datasets. To the best knowledge of the authors,
this is the ﬁrst work to demonstrate learning of Lie group operators on datasets as large and complex
as CelebA. Additional details on the inference algorithm are provided in Appendix B."
ANALYSIS,0.16560509554140126,"4
ANALYSIS"
ANALYSIS,0.16878980891719744,"For our experiments, we examine the ability of our proposed approaches to enable the MAE model to
learn natural transformations of datasets where the underlying identity-preserving transformations
are not easily identiﬁable. We also highlight the beneﬁts of incorporating the coefﬁcient encoder
network that captures the transport operator local usage statistics by encoding coefﬁcient scale values
that best maintain the identity of latent vectors."
ANALYSIS,0.17197452229299362,Under review as a conference paper at ICLR 2022
ANALYSIS,0.1751592356687898,"We work with three datasets: MNIST (LeCun et al., 1998), Fashion MNIST (Xiao et al., 2017),
and CelebA (Liu et al., 2015). We select MNIST and Fashion MNIST because they contain several
classes that share natural transformations but they do not have transformation labels. We select
CelebA to highlight our ability learn natural transformations in a larger, more complex dataset. As
a classic dataset used in papers that aim to disentangle dataset features (Higgins et al., 2017; Chen
et al., 2016; Hu et al., 2018; Lin et al., 2019), CelebA contains semantically meaningful natural
transformations that may be amenable to qualitative labeling."
ANALYSIS,0.17834394904458598,"In all experiments, we followed the general training procedure put forth previously (Connor &
Rozell, 2020) by separating the network training into three phases: the autoencoder training phase,
the transport operator training phase, and the ﬁne-tuning phase. The ﬁne-tuning phase adapts the au-
toencoder latent space to ﬁt the learned transformations in the event that that pretrained latent space
does not match the desired data manifold. We select training point pairs that are nearest neighbors in
the feature space of the ﬁnal, pre-logit layer of a ResNet-18 (He et al., 2015) classiﬁer pretrained on
ImageNet (Russakovsky et al., 2015). After completely training the MAE, we ﬁx the autoencoder
network weights and transport operator weights and train the coefﬁcient encoder network with the
objective derived in Section 3.2. Additional details on the datasets, network architectures, and train-
ing procedure are available in the Appendix."
ANALYSIS,0.18152866242038215,"We compare against the contractive autoencoder (CAE) (Rifai et al., 2011c) and β-VAE (Higgins
et al., 2017), two other methods for incorporating data structure into the latent space.. The CAE
represents another technique that learns a manifold representation in a neural network latent space.
In their model, the manifold is represented by estimated tangent planes at latent point locations.
The β-VAE learns to disentangle factors of variation along latent dimensions through an increase in
the weighted penalty on the KL-divergence term in the VAE objective. We choose these methods
because they also learn natural dataset variations in the latent space without transformation labels."
LEARNING NATURAL DATA VARIATIONS,0.18471337579617833,"4.1
LEARNING NATURAL DATA VARIATIONS"
LEARNING NATURAL DATA VARIATIONS,0.18789808917197454,"First, we show how well the perceptual point pair selection strategy described in Section 3.1 enables
the MAE model to learn natural data variations in datasets and we highlight the usefulness of a
nonlinear manifold model for generating latent space paths. Fig. 1 shows the paths generated by
transport operators trained in this model. The image in the middle column in each block of images
is the reconstructed version of the input image x0. The images to the left and right of the middle
show the reconstructed images generated by an individual learned operator applied to the encoded
latent vector z0: zc = expm(Ψmc)z0, c = −Nc, ..., Nc. We see an individual operator generates
a similar transformation across multiple inputs, and in many cases, the transformations induced by
the transport operators are semantically meaningful. We also show that the perceptual point pair
selection strategy is effective over a range of datasets."
LEARNING NATURAL DATA VARIATIONS,0.1910828025477707,"While many of the operators can be assigned a semantic label through qualitative visual inspection,
the CelebA dataset has attribute labels for the images which enable a quantitative analysis of the
connection between learned transport operators and dataset attributes. To classify attributes, we
ﬁne-tune a ResNet-18 pretrained on ImageNet with 16 classiﬁcation heads for attributes including
smile, beard, hair color, and pale skin (Mao et al., 2020). With this classiﬁer, we are able to identify
transport operators that correspond to speciﬁc dataset attributes. Fig. 2 shows the classiﬁcation
outputs of the attribute classiﬁer for example transport operators. Our model learns operators that
vary hair color and skin paleness as well as several others which are shown in the Appendix."
LEARNING NATURAL DATA VARIATIONS,0.1942675159235669,"In Fig. 1 we compare transport operator paths to those generated by the CAE and the β-VAE. The
CAE-generated paths are the directions of motion on the tangent planes estimated at individual
points. The β-VAE paths are generated by varying the value of one latent dimension while the others
remain ﬁxed. While our method and the β-VAE learn several qualitatively similar transformations,
our method is capable of doing so without signiﬁcantly sacriﬁcing reconstruction performance. In
the Appendix, we include examples of the transformations generated by each learned operator."
LEARNING NATURAL DATA VARIATIONS,0.19745222929936307,"A primary motivation for learning the data manifold is to generate new views of data that cannot
arise from pre-deﬁned functions. To highlight the ability of our model to generate new views that
preserve identity, we compare both interpolated and extrapolated paths from the MAE to those from
the CAE, β-VAE, and a traditional auto-encoder in Fig. 3. Interpolated paths offer insight in the
smooth variations between two data points, while extrapolated paths show the capacity to generate"
LEARNING NATURAL DATA VARIATIONS,0.20063694267515925,Under review as a conference paper at ICLR 2022 (a) (b)
LEARNING NATURAL DATA VARIATIONS,0.20382165605095542,"(c)
Figure 1: Paths generated by applying a subset of learned transport operators on three datasets. In
each ﬁgure, images in the middle column of the image block are the reconstructed inputs and im-
ages to the right and left are images decoded from transformed latent vectors in positive and negative
directions, respectively. (a) Comparing the transformations generated by three learned transport op-
erators to transformations generated by β-VAE and CAE. The transport operators learn semantically
meaningful transformations similar to the disentangled β-VAE representation while maintaining a
higher resolution in image outputs. (b-c) Transport operators learned using the perceptual point pair
selection strategy generate natural transformations on both MNIST and Fashion MNIST."
LEARNING NATURAL DATA VARIATIONS,0.2070063694267516,"(a)
(b)
Figure 2: Paths generated by the application of two learned transport operators with the associated
attribute classiﬁer probability outputs for the transformed images. Our model learns operators that
correspond to meaningful CelebA dataset attributes like (a) hair color (b) pale skin."
LEARNING NATURAL DATA VARIATIONS,0.21019108280254778,"new views of data. For MAE, the paths are estimated by inferring coefﬁcients c∗between two
latent points z0 and z1 and then generating the path: zt = expm
PM
m=1 Ψmc∗
mt

z0. When
the path multiplier t is between 0 and 1 that indicates interpolation and path multipliers beyond
1 indicate extrapolation. For all other methods, Eucilidean distance between features is used for
both interpolation and extrapolation. In these ﬁgures, the ﬁrst block of images corresponds to the
interpolated path with the selected ﬁnal point x1 surrounded in an orange box. The second block of
images corresponds to the extrapolated paths."
LEARNING NATURAL DATA VARIATIONS,0.21337579617834396,"To quantify the identity preservation of each transformation, we input each generated image into a
pretrained classiﬁer and plot the prediction of the correct class. All four methods perform interpola-
tion effectively but our trained model estimates the extrapolated paths more accurately. Fig. 4 shows
how that accuracy varies during extrapolation sequences for 4000 samples. The MAE is better at
generating extrapolated outputs that maintain class identity. The lower classiﬁcation accuracy in
fashion MNIST is due to both a more challenging dataset and the lower resolution of autoencoder
image outputs when compared to input images."
LEARNING NATURAL DATA VARIATIONS,0.21656050955414013,Under review as a conference paper at ICLR 2022 (a)
LEARNING NATURAL DATA VARIATIONS,0.2197452229299363,"(b)
Figure 3: Identity preservation of transformed paths as quantiﬁed by a pretrained classiﬁer output.
In the ﬁgures on the top, the ﬁrst block of images corresponds to the interpolated path with selected
ﬁnal point x1 surrounded in an orange box. The second block of images corresponds to the extrapo-
lated paths. Below the images are plots of the probability of the class label associated with the inputs
z0 and z1. A path multiplier between 0 and 1 indicates interpolation and path multipliers beyond 1
indicate extrapolation. (a) MNIST (b) Fashion MNIST."
LEARNING NATURAL DATA VARIATIONS,0.2229299363057325,"(a)
(b)
Figure 4: The average accuracy of the classiﬁer output for images at each point along the interpola-
tion/extrapolation sequence. When the path multiplier is between 0 and 1 that indicates interpolation
and path multipliers beyond 1 indicate extrapolation. (a) MNIST (b) Fashion MNIST."
LEARNING LOCAL MANIFOLD STRUCTURE,0.22611464968152867,"4.2
LEARNING LOCAL MANIFOLD STRUCTURE"
LEARNING LOCAL MANIFOLD STRUCTURE,0.22929936305732485,"After the MAE is trained, we have a dictionary of transport operators that describe manifold trans-
formations and a network with a latent space that is adapted to the manifold. We then train the
coefﬁcient encoder network to estimate the coefﬁcient scale weights as a function of points in the
latent space. To visualize how the use of the transport operators varies over the latent space, we
generate an Isomap embedding (Tenenbaum et al., 2000) of latent vectors and color each point by
the encoded scale parameter for coefﬁcients associated with each of the transport operators. Fig. 5a
shows these embeddings for MNIST data. Each operator has regions of the latent space where their
use is concentrated."
LEARNING LOCAL MANIFOLD STRUCTURE,0.23248407643312102,"By training the coefﬁcient encoder to maximize the similarity between the classiﬁcation outputs for
an input sample and for a transformed version of that sample, the network aids in identifying which
transport operators can be applied to inputs in regions of the data space without changing the iden-
tity of the input. This helps signiﬁcantly with data augmentation where the goal is to create new
samples with in-class variations. To highlight this beneﬁt, in Fig. 6 we show samples augmented by
applying transport operators with randomly sampled coefﬁcients to an input latent vector. In each
block of images, the leftmost image (in a green box) is the input image and the images to the right
are decoded augmentations. The top row shows samples augmented with transport operators con-
trolled by coefﬁcients sampled from Laplace distributions with encoded coefﬁcient scale weights.
The bottom row shows samples augmented with transport operators controlled by coefﬁcients sam-
pled from Laplace distributions with a ﬁxed scale parameter. While both strategies generate some
realistic variations of the data, using the encoded scale weights improves identity-preservation of
the transformed output. The augmentations with the encoded scale weights are better at maintaining
the identity of the sampled points."
LEARNING LOCAL MANIFOLD STRUCTURE,0.2356687898089172,Under review as a conference paper at ICLR 2022
LEARNING LOCAL MANIFOLD STRUCTURE,0.23885350318471338,"(a)
(b)
Figure 5: (a) Visualizations of the encoded coefﬁcient scale weights. The leftmost image shows
an Isomap embedding of the latent vectors with input images overlaid. The scatter plots on the
right show the same Isomap embedding colored by the encoded coefﬁcient scale weights for several
operators (yellow indicates large scale weights and blue small scale weights). We see operators
whose use is localized in regions of the manifold space. (b) The average coefﬁcient scale weights for
each class on each transport operator for MNIST. High scale weights for a given operator (yellow)
indicate it can be applied to a given class without easily changing identity. The images on the right
show examples of the operators applied to classes with high encoded scale weights (in the top yellow
boxes) and classes with low encoded scale weights (in bottom blue boxes). The examples with low
coefﬁcient scale weights change classes more easily than other examples."
LEARNING LOCAL MANIFOLD STRUCTURE,0.24203821656050956,"Figure 6: Examples of samples generated by transport operators using coefﬁcients sampled with
encoded scale weights (top row) and with a ﬁxed scale weight (bottom row). Images in the green
box are the input images and the remaining images in each row are transformed outputs."
LEARNING LOCAL MANIFOLD STRUCTURE,0.24522292993630573,"Because the coefﬁcient encoder uses a pretrained classiﬁer to determine the coefﬁcient distributions
in the latent space, the resulting encoded scale weights can inform about the types of manifold
transformations the classiﬁer is invariant to. Fig. 5b shows the average coefﬁcient scale weights
for each class (rows) and each transport operator (columns) for MNIST. From this we can identify
classes for which the classiﬁer is both sensitive and robust to transformations, represented by small
and large scale weights respectively. We can also examine which classes share the use of the same
transformations. The images to the right in Fig. 5b show transport operators being applied to samples
with high encoded scale weights (in a yellow box) and samples with low encoded scale weights (in
a blue box). We can examine the characteristics of transformations that are better suited to some
classes than others. For instance, operator 3 in Fig. 5b increases the curve at the bottom of digits.
This is a natural transformation for classes 3, 5, and 6 which all have higher coefﬁcient scale weights
for this operator, but when this is applied to a 1, that makes it look like an 8."
CONCLUSION,0.2484076433121019,"5
CONCLUSION"
CONCLUSION,0.2515923566878981,"In this work we develop methods to improve the effectiveness and utility of Lie group operator
models for learning manifold representations of datasets with complex transformations that cannot
be labeled. We do this by introducing a perceptual point pair selection strategy for training operators
and by developing a method that uses a pretrained classiﬁer to learn local regions where opera-
tors are likely to be used while preserving the identity of transformed samples. We demonstrate
the efﬁcacy of our approach in learning natural dataset variations with the MAE. While this is a
powerful model, users should be mindful of the biases that are introduced through training with spe-
ciﬁc supervision methods when drawing conclusions about learned data transformations. This work
presents a promising technique for learning representations of natural data variations that can im-
prove model robustness. In future work we can address some limitations by expanding this work to
consider methods for localizing manifold structure with limited or no class labels and improving the
reconstruction ﬁdelity by applying the model within more complex generative model embeddings."
CONCLUSION,0.25477707006369427,Under review as a conference paper at ICLR 2022
REFERENCES,0.25796178343949044,REFERENCES
REFERENCES,0.2611464968152866,"Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. Latent space oddity: on the curvature
of deep generative models. In International Conference on Learning Representations, 2018."
REFERENCES,0.2643312101910828,"Georgios Arvanitidis, Soren Hauberg, Philipp Hennig, and Michael Schober. Fast and robust short-
est paths on manifolds learned from data. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1506–1515. PMLR, 2019."
REFERENCES,0.267515923566879,"Georgios Arvanitidis, Bogdan Georgiev, and Bernhard Sch¨olkopf. A prior-based approximate latent
riemannian metric. arXiv preprint arXiv:2103.05290, 2021."
REFERENCES,0.27070063694267515,"Philip Bachman, Ouais Alsharif, and Doina Precup.
Learning with pseudo-ensembles.
arXiv
preprint arXiv:1412.4864, 2014."
REFERENCES,0.27388535031847133,"Amir Beck and Marc Teboulle.
A Fast Iterative Shrinkage-Thresholding Algorithm for Linear
Inverse Problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, January 2009.
ISSN
1936-4954. doi: 10.1137/080716542. URL http://epubs.siam.org/doi/10.1137/
080716542."
REFERENCES,0.2770700636942675,"Mikhail Belkin and Partha Niyogi.
Laplacian eigenmaps for dimensionality reduction and data
representation. Neural computation, 15(6):1373–1396, 2003."
REFERENCES,0.2802547770700637,"Yoshua Bengio and Martin Monperrus. Non-local manifold tangent learning. Advances in Neural
Information Processing Systems, 17:129–136, 2005."
REFERENCES,0.28343949044585987,"Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The curse of dimensionality for local
kernel machines. Techn. Rep, 1258:12, 2005."
REFERENCES,0.28662420382165604,"Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder:
Learning disentangled representations from grouped observations. In AAAI Conference on Artiﬁ-
cial Intelligence, volume 32, 2018."
REFERENCES,0.2898089171974522,"Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press,
Cambridge, UK ; New York, 2004. ISBN 978-0-521-83378-3."
REFERENCES,0.2929936305732484,"Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick Smagt. Met-
rics for deep generative models. In International Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 1540–1550. PMLR, 2018."
REFERENCES,0.2961783439490446,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations, 2020."
REFERENCES,0.29936305732484075,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets,
2016."
REFERENCES,0.30254777070063693,"Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, and Bruno A. Olshausen. Discovering hidden
factors of variation in deep networks. In Yoshua Bengio and Yann LeCun (eds.), International
Conference on Learning Representations, ICLR Workshop Track Proceedings, 2015."
REFERENCES,0.3057324840764331,"Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
In International Conference on Machine Learning, pp. 1755–1763. PMLR, 2014."
REFERENCES,0.3089171974522293,"Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990–2999. PMLR, 2016."
REFERENCES,0.31210191082802546,"Taco S Cohen, Mario Geiger, Jonas K¨ohler, and Max Welling. Spherical cnns. In International
Conference on Learning Representations, 2018."
REFERENCES,0.31528662420382164,"Marissa Connor and Christopher Rozell. Representing closed transformation paths in encoded net-
work latent space. In AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 3666–3675,
2020."
REFERENCES,0.3184713375796178,Under review as a conference paper at ICLR 2022
REFERENCES,0.321656050955414,"Marissa Connor, Gregory Canal, and Christopher Rozell. Variational autoencoder with learned latent
structure. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2359–2367.
PMLR, 2021."
REFERENCES,0.3248407643312102,"Romain Cosentino, Randall Balestriero, Richard Baraniuk, and Behnaam Aazhang. Deep autoen-
coders: From understanding to generalization guarantees, 2021."
REFERENCES,0.32802547770700635,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113–123, 2019."
REFERENCES,0.33121019108280253,"Benjamin J Culpepper and Bruno A Olshausen. Learning transport operators for image manifolds.
In Advances in Neural Information Processing Systems, pp. 423–431, 2009."
REFERENCES,0.3343949044585987,"Peter C Dodwell. The lie transformation group model of visual perception. Perception & Psy-
chophysics, 34(1):1–16, 1983."
REFERENCES,0.3375796178343949,"Piotr Doll´ar, Vincent Rabaud, and Serge J Belongie. Learning to traverse image manifolds. In
Advances in Neural Information Processing Systems, pp. 361–368, 2007."
REFERENCES,0.34076433121019106,"Luca Falorsi, Pim de Haan, Tim R Davidson, and Patrick Forr´e. Reparameterizing distributions
on lie groups. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp.
3244–3253. PMLR, 2019."
REFERENCES,0.34394904458598724,"Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan.
Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983–1049, 2016."
REFERENCES,0.3471337579617834,"Manuel Gil. On R´enyi divergence measures for continuous alphabet sources. PhD thesis, Citeseer,
2011."
REFERENCES,0.3503184713375796,"Jihun Ham and Daniel D. Lee. Separating pose and expression in face images: A manifold learning
approach, 2006."
REFERENCES,0.3535031847133758,"Søren Hauberg, Oren Freifeld, Anders Boesen Lindbo Larsen, John Fisher, and Lars Hansen.
Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data aug-
mentation. In Artiﬁcial Intelligence and Statistics, pp. 342–350. PMLR, 2016."
REFERENCES,0.35668789808917195,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition, 2015."
REFERENCES,0.35987261146496813,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning, 2020."
REFERENCES,0.3630573248407643,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017."
REFERENCES,0.3662420382165605,"Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018."
REFERENCES,0.36942675159235666,"Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In Interna-
tional Conference on Artiﬁcial Neural Networks, pp. 44–51. Springer, 2011."
REFERENCES,0.37261146496815284,"Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation:
Efﬁcient learning of augmentation policy schedules. In International Conference on Machine
Learning, pp. 2731–2741. PMLR, 2019."
REFERENCES,0.37579617834394907,"William C Hoffman. The lie algebra of visual perception. Journal of mathematical Psychology, 3
(1):65–98, 1966."
REFERENCES,0.37898089171974525,"Qiyang Hu, Attila Szab´o, Tiziano Portenier, Matthias Zwicker, and Paolo Favaro. Disentangling
factors of variation by mixing them, 2018."
REFERENCES,0.3821656050955414,Under review as a conference paper at ICLR 2022
REFERENCES,0.3853503184713376,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694–711. Springer, 2016."
REFERENCES,0.3885350318471338,"Dimitris Kalatzis, David Eklund, Georgios Arvanitidis, and Søren Hauberg. Variational autoen-
coders with riemannian brownian motion priors. In International Conference on Machine Learn-
ing, 2020."
REFERENCES,0.39171974522292996,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, 2015."
REFERENCES,0.39490445859872614,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2013."
REFERENCES,0.3980891719745223,"Tejas D Kulkarni, William F. Whitney, Pushmeet Kohli, and Josh Tenenbaum.
Deep convo-
lutional inverse graphics network.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran As-
sociates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf."
REFERENCES,0.4012738853503185,"Abhishek Kumar, Prasanna Sattigeri, and P Thomas Fletcher. Semi-supervised learning with gans:
Manifold invariance with improved inference. Advances in Neural Information Processing Sys-
tems, 2017."
REFERENCES,0.40445859872611467,"Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.40764331210191085,"Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment.
Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.410828025477707,"Jianxin Lin, Zhibo Chen, Yingce Xia, Sen Liu, Tao Qin, and Jiebo Luo. Exploring explicit domain
supervision for latent space disentanglement in unpaired image-to-image translation, 2019."
REFERENCES,0.4140127388535032,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.4171974522292994,"Laurens van der Maaten and Geoffrey Hinton.
Visualizing Data using t-SNE.
Journal of Ma-
chine Learning Research, 1:1–48, 2008. URL http://www.cs.toronto.edu/˜hinton/
absps/tsne.pdf."
REFERENCES,0.42038216560509556,"Longbiao Mao, Yan Yan, Jing-Hao Xue, and Hanzi Wang. Deep multi-task multi-label cnn for
effective facial attribute classiﬁcation, 2020."
REFERENCES,0.42356687898089174,"Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann
LeCun. Disentangling factors of variation in deep representation using adversarial training. In
Advances in Neural Information Processing Systems, pp. 5040–5048, 2016."
REFERENCES,0.4267515923566879,"Xu Miao and Rajesh PN Rao. Learning the lie groups of visual invariance. Neural computation, 19
(10):2665–2693, 2007."
REFERENCES,0.4299363057324841,"Cuong V. Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau. Leep: A new measure to
evaluate transferability of learned representations, 2020."
REFERENCES,0.43312101910828027,"PACE. Partnership for an Advanced Computing Environment (PACE), 2017. URL http://www.
pace.gatech.edu."
REFERENCES,0.43630573248407645,"Neal Parikh and Stephen Boyd. Proximal algorithms. Found. Trends Optim., 1(3):127–239, January
2014. ISSN 2167-3888. doi: 10.1561/2400000003. URL https://doi.org/10.1561/
2400000003."
REFERENCES,0.4394904458598726,"Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zolt´an Szab´o, Lars Buesing, and Maneesh Sahani.
Bayesian manifold learning: The locally linear latent variable model (ll-lvm). In Advances in
Neural Information Processing Systems, pp. 154–162, 2015."
REFERENCES,0.4426751592356688,"Rajesh PN Rao and Daniel L Ruderman. Learning lie groups for invariant visual perception. In
Advances in neural information processing systems, pp. 810–816, 1999."
REFERENCES,0.445859872611465,Under review as a conference paper at ICLR 2022
REFERENCES,0.44904458598726116,"Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of
variation with manifold interaction. In International Conference on Machine Learning, pp. 1431–
1439. PMLR, 2014."
REFERENCES,0.45222929936305734,"Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows, 2016."
REFERENCES,0.4554140127388535,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278–1286. PMLR, 2014."
REFERENCES,0.4585987261146497,"Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classiﬁer. Advances in Neural Information Processing Systems, 24:2294–2302, 2011a."
REFERENCES,0.46178343949044587,"Salah Rifai, Gr´egoire Mesnil, Pascal Vincent, Xavier Muller, Yoshua Bengio, Yann Dauphin, and
Xavier Glorot. Higher order contractive auto-encoder. In Joint European conference on machine
learning and knowledge discovery in databases, pp. 645–660. Springer, 2011b."
REFERENCES,0.46496815286624205,"Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-
encoders: Explicit invariance during feature extraction. In International Conference on Machine
Learning, 2011c."
REFERENCES,0.4681528662420382,"Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. Science, 290(5500):2323–2326, 2000."
REFERENCES,0.4713375796178344,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.4745222929936306,"Hang Shao, Abhishek Kumar, and P Thomas Fletcher. The riemannian geometry of deep generative
models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 315–323, 2018."
REFERENCES,0.47770700636942676,"Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A Olshausen. An unsupervised algorithm for
learning lie group transformations. arXiv preprint arXiv:1001.1027, 2010."
REFERENCES,0.48089171974522293,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and conﬁdence, 2020."
REFERENCES,0.4840764331210191,"Steven H. Strogatz. Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chem-
istry and Engineering. Westview Press, 2000."
REFERENCES,0.4872611464968153,"Joshua B Tenenbaum and William T Freeman. Separating style and content with bilinear models.
Neural computation, 12(6):1247–1283, 2000."
REFERENCES,0.49044585987261147,"Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000."
REFERENCES,0.49363057324840764,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017."
REFERENCES,0.4968152866242038,"Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling
with recurrent transformations for 3d view synthesis. Advances in Neural Information Processing
Systems, 28:1099–1107, 2015."
REFERENCES,0.5,"Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015."
REFERENCES,0.5031847133757962,Under review as a conference paper at ICLR 2022
REFERENCES,0.5063694267515924,SUPPLEMENTARY MATERIAL
REFERENCES,0.5095541401273885,"A
COEFFICIENT ENCODER DETAILS"
REFERENCES,0.5127388535031847,Figure 7: System diagram for learning local coefﬁcient statistics.
REFERENCES,0.5159235668789809,"Our motivation in training an encoder to learn the coefﬁcient statistics is to understand the regions
of the manifold in which speciﬁc transport operators are applicable and the invariances of a pre-
trained classiﬁer to the learned transport operator-induced transformations. In this section, we will
outline a derivation of our coefﬁcient encoder training objective from a variational inference per-
spective by learning a variational posterior for the coefﬁcients using a deep neural network (Kingma
& Welling, 2013; Rezende et al., 2014). Given observations (zi, yi) for i = 1, . . . , N, and a pre-
trained classiﬁer r(·), we aim to learn a distribution qφ(c|z) from which we can sample to deﬁne the
transformation TΨ(c) that can be applied to z while maintaining class identity. In other words, we
want the augmented bz = TΨ(c)z + ϵ to result in r(bz) = y."
REFERENCES,0.5191082802547771,"To encourage identity-preservation of vectors transformed with transport operators that are con-
trolled by sampled coefﬁcients, we maximize the likelihood of a class output for an observation
under the system model diagrammed in Figure 7. This system connects the observed latent vector z
to an output y through a transformation deﬁned by c. We follow the derivation used in Rezende &
Mohamed (Rezende & Mohamed, 2016) which introduces the variational posterior to estimate the
log likelihood of the data."
REFERENCES,0.5222929936305732,Consider a parameterized distribution for the conditional likelihood of our observations:
REFERENCES,0.5254777070063694,"log pθ(y|z) = log Epζ(c) [pθ(y|c, z)]
(6)"
REFERENCES,0.5286624203821656,"= log
Z"
REFERENCES,0.5318471337579618,"c
pζ(c)pθ(y|c, z)dc
(7)"
REFERENCES,0.535031847133758,"= log
Z"
REFERENCES,0.5382165605095541,"c
qφ(c|z) pζ(c)"
REFERENCES,0.5414012738853503,"qφ(c|z)pθ(y|c, z)dc
(8) ≥
Z"
REFERENCES,0.5445859872611465,"c
qφ(c|z) log
 pζ(c)"
REFERENCES,0.5477707006369427,"qφ(c|z)pθ(y|c, z)

dc
(9)"
REFERENCES,0.5509554140127388,"= Eqφ [log pθ(y|c, z)] −DKL (qφ(c|z)|pζ(c))
(10)"
REFERENCES,0.554140127388535,"= Eqφ [log Eϵpθ(y|c, z, ϵ,bz)] −DKL (qφ(c|z)|pζ(c)) .
(11)"
REFERENCES,0.5573248407643312,"In equation 6 and equation 7, we marginalize over the coefﬁcients c as required under our system
model in Figure 7. In equation 9, we lower bound the conditional likelihood with a variational lower
bound derived from Jensen’s inequality. Finally, we use the reparameterization trick to deﬁne bz as a
deterministic function of c, z, and the parameter-free random variable ϵ in equation 11."
REFERENCES,0.5605095541401274,"To sample from qφ(c | z) when computing the expectations, we ﬁrst use the reparameterization
trick (Connor et al., 2021; Kingma & Welling, 2013; Rezende et al., 2014) to deﬁne the sampled
coefﬁcients bc as a function of a uniform random variable u ∼Unif
 
−1 2, 1 2
M:"
REFERENCES,0.5636942675159236,"bc = lφ(u, z) = −hφ(z) sgn(u) log(1 −2 |u|),
(12)"
REFERENCES,0.5668789808917197,"where lφ is a deﬁned mapping from a uniform distribution to a Laplace distribution and the Laplace
scale parameters are deﬁned by the output of the coefﬁcient encoder hφ that we aim to learn."
REFERENCES,0.5700636942675159,"Using the reparameterization of bc and bz, we can deﬁne the expectation:"
REFERENCES,0.5732484076433121,Under review as a conference paper at ICLR 2022
REFERENCES,0.5764331210191083,"Eqφ [log Eϵpθ(y|c, z, ϵ,bz)] = Eu [log Eϵpθ(y|bc = lφ(u, z), ϵ,bz)]
(13)"
REFERENCES,0.5796178343949044,"≈
1
JK J
X"
REFERENCES,0.5828025477707006,"j=1
log K
X"
REFERENCES,0.5859872611464968,"k=1
pθ

y|bc(j) = lφ(u(j), z), ϵ(k),bz

.
(14)"
REFERENCES,0.589171974522293,"Our pre-trained classiﬁer deﬁnes pθ which allows us to specify ﬁnal objective using of r(·). Using a
KL-divergence as the likelihood function between the ground truth labels yi and the classiﬁer output
for the augmented inputs r(bz(j)
i ) and simplifying the model by setting ϵ to 0, we get:"
REFERENCES,0.5923566878980892,"log pθ(y|z) ≥−1 J J
X"
REFERENCES,0.5955414012738853,"j=1
DKL

yi|r(bz(j)
i )

−DKL (qφ(c|z)|pζ(c)) ,
(15)"
REFERENCES,0.5987261146496815,"where in practice we use a single sample J
=
1 and we estimate DKL

yi|r(bz(j)
i )

≈"
REFERENCES,0.6019108280254777,"DKL

r(z(j)
i )|r(bz(j)
i )

for test samples for which we may not have class labels. These practical
simpliﬁcations result in the objective in 5 which we want to minimize. The KL divergence between
the qφ(c|z) and pζ(c) has a closed form expression (Gil, 2011):"
REFERENCES,0.6050955414012739,"DKL (qφ(c | z)∥pζ(c)) = log(hφ(z)) −log(ζ) +
ζ
hφ(z) −1
(16)"
REFERENCES,0.60828025477707,"B
COEFFICIENT INFERENCE DETAILS"
REFERENCES,0.6114649681528662,"Figure 8:
Comparison in time taken for various coefﬁcient inference algorithms on fully trained
transport operators. Each inference trial is performed over random batches of 100 point pairs drawn
from the CelebA dataset (with a latent dimension of 32). 30 trials are run with the same c0 and
tolerance. We used the proximal gradient (GPU) method."
REFERENCES,0.6146496815286624,"To perform coefﬁcient inference, previous Lie group operator methods (Connor & Rozell, 2020;
Connor et al., 2021) use the analytic gradient proposed in (Culpepper & Olshausen, 2009) with a
conjugate gradient descent solver. This method requires an eigenvalue decomposition for computing
the gradient which is not favorable for parallel computations. Alternatively, we use the PyTorch
implementation of the matrix exponential that allows for automatic differentiation. Furthermore, to
handle the non-smooth ℓ1 norm in objective (2), we apply a proximal gradient step in a forward-
backward splitting scheme (Beck & Teboulle, 2009). For the ℓ1 norm, the proximal gradient has a
known, closed-form solution in the soft-thresholding function (Parikh & Boyd, 2014):"
REFERENCES,0.6178343949044586,"Tλ(c) = sign(c) ∗max

|c| −λ, 0

(17)"
REFERENCES,0.6210191082802548,Under review as a conference paper at ICLR 2022
REFERENCES,0.6242038216560509,Let ∇c 1
REFERENCES,0.6273885350318471,"2
z1 −expm
PM
m=1 Ψmcm

z0

2"
REFERENCES,0.6305732484076433,"2 ≈∇ef(c) be a numerical approximation to the gradi-
ent of the ℓ2 term, found through automatic differentiation. Given initial coefﬁcient values c0 drawn
from an isotropic Gaussian with variance 4 × 10−4, our gradient descent step is:"
REFERENCES,0.6337579617834395,"ck+1 = Tζαk

ck −αk∇ef(ck)

(18)"
REFERENCES,0.6369426751592356,"These steps are iterated upon until either a max iteration count is reached, or the change in coefﬁ-
cients, ∥ck+1−ck∥2, falls below some threshold. For all experiments we use a max iteration count of
800 and a tolerance of 1×10−5. For our step-size, we use αk = (0.985)kα0 with α0 = 1×10−2. We
experimented with different acceleration methods (Kingma & Ba, 2015; Beck & Teboulle, 2009),
and found that in many cases they resulted in worse performance."
REFERENCES,0.6401273885350318,"The main parameter to select for inference is the sparsity-inducing parameter on the coefﬁcient prior,
ζ. This value is sensitive to the dataset, latent dimension, and γ hyper-parameter value. In practice,
we ﬁx a γ value and pick ζ to be as high as possible, inducing sparsity in the inferred coefﬁcients,
while maintaining good reconstruction performance of bz1. If ζ is too large, that will result in all
coefﬁcients going to zero, preventing proper reconstructive performance. On the other hand, setting
ζ to be too small will result in all operators being used to represent the transformation between each
point pair, preventing any meaningful structure from being learned during training. Parameters used
in speciﬁc training runs are included in their respective Appendix sections."
REFERENCES,0.643312101910828,"When compared against the coefﬁcient inference implementation from (Connor & Rozell, 2020), we
perform inference for a batch of 100 samples in an average of 1.67 seconds over 100 trials whereas
the inference from (Connor & Rozell, 2020) took an average of 101.01 seconds. We also compare
the speed-up over a baseline that strictly uses automatic differentiation (subgradients) and compare
the beneﬁts from moving to GPU hardware in Figure 8. We use the proximal gradient (GPU) method
from this ﬁgure. Experiments were run on a machine with an Intel i7-6700 CPU with 4.00 GHz and
a Nvidia TITAN RTX."
REFERENCES,0.6464968152866242,"C
TRAINING STRATEGY"
REFERENCES,0.6496815286624203,"In all experiments, we follow the general training procedure put forth previously (Connor & Rozell,
2020). We train the MAE with three training phases: the autoencoder training phase, the trans-
port operator training phase, and the ﬁne-tuning phase. During the autoencoder training phase, the
network weights are updated using a reconstruction loss objective: EAE = ∥x −ˆx∥2
2."
REFERENCES,0.6528662420382165,"During the transport operator training phase, the network weights are ﬁxed and the transport oper-
ators are trained between pairs of points using the objective (2). Pairs of images x0, x1 are chosen
using the perceptual point pair selection strategy described in Section 3.1. The images are then en-
coded into the latent space z0, z1. For each batch, the ﬁrst step is to infer the coefﬁcients between all
pairs of latent vectors. Coefﬁcient inference is best performed when the entries of the latent vectors
are close to the range [−1, 1]. Because of this, we deﬁne a scale factor that can be applied to encoded
latent vectors to reduce the magnitude of their entries prior to performing coefﬁcient inference. In
practice, we inspect the latent vector magnitudes after the autoencoder training phase and choose a
scale that will adjust the magnitudes of the latent vector entries to be in the range [−1, 1]. This does
not have to be a precise range for the latent vector magnitudes but instead is a practical guideline.
Coefﬁcient inference is performed on the scaled latent vectors as described in Sections 3.3 and Ap-
pendix B. After the coefﬁcients are inferred for a batch, the weights on the dictionary elements are
updated. This phase of training is performed until the loss values reach a plateau and the dictionary
magnitudes plateau."
REFERENCES,0.6560509554140127,"The ﬁne-tuning training phase begins after the transport operator training phase is complete. In
this phase, both the network weights and the transport operator weights are updated using the joint
objective:"
REFERENCES,0.6592356687898089,"E = λ

∥x0 −ˆx0∥2
2 + ∥x1 −ˆx1∥2
2

+ (1 −λ) EΨ,
(19)"
REFERENCES,0.6624203821656051,"where EΨ is deﬁned in (2). Using this objective, we alternate between taking steps on the transport
operator weights while the network weights are ﬁxed and taking steps on the network weights while
the transport operator weights are ﬁxed. Additionally, during the ﬁne-tuning phase we incorporate"
REFERENCES,0.6656050955414012,Under review as a conference paper at ICLR 2022
REFERENCES,0.6687898089171974,"occasional steps in which we update the network weights using only the reconstruction loss to ensure
effective image reconstruction."
REFERENCES,0.6719745222929936,"In most cases, it is necessary to reduce the γ parameter in front of the Frobenius norm dictionary
regularizer prior to ﬁne-tuning or the dictionary magnitudes will reduce to zero. We report the γ we
use for transport operator training and ﬁne-tuning in the experimental details sections below. It may
also be necessary to decrease the network learning rate during ﬁne-tuning."
REFERENCES,0.6751592356687898,"The coefﬁcient encoder training requires a network that is trained to classify data from our selected
dataset. We train this classiﬁer using training data from a given dataset. For datasets with worse
autoencoder reconstruction quality, we train the classiﬁer in the latent space. Otherwise we train the
classiﬁer on images in the data space. With the MAE and classiﬁer trained, we train the coefﬁcient
encoder network following the strategy described in Section 3.2 and Appendix A."
REFERENCES,0.678343949044586,"To train the CAE, we use the same autoencoder architecture used with the MAE with the addition
of a Frobenius norm regularizer on the encoder Jacobian, weighted by a selected λ value (different
from the λ in equation 19). The Jacobian is computed using PyTorch automatic differentiation. We
ﬁnd the Jacobian norm decreases to the same value irrespective of our choice of λ, leading us to
choose λ = 1. For the β-VAE we use the same architectures outlined in (Higgins et al., 2017) with
β = 10 for CelebA and β = 5 for MNIST and FMNIST. We ﬁnd that setting β any higher results in
poor reconstructive performance. Scripts for training both comparison methods are included in the
code repository."
REFERENCES,0.6815286624203821,"Hyper-parameter tuning for all experiments was performed on the Georgia Tech Partnership for
Advanced Computing Environment (PACE) clusters (PACE, 2017). Experiments were performed
using a Nvidia Quadro RTX 6000. Runs training the CAE, and β-VAE on CelebA were all run on a
separate machine with a Nvidia TITAN RTX."
REFERENCES,0.6847133757961783,"D
PARAMETER SELECTION"
REFERENCES,0.6878980891719745,"The MAE model has several hyperparameters that must be tuned and we will provide guidance
to determining ideal parameter values for our experiments and future experiments. First we will
describe some signs to look out for to identify if a run is succeeding or failing. One indicator that
we compute is the transport operator difference which is:"
REFERENCES,0.6910828025477707,EΨdiff = 1 2
REFERENCES,0.6942675159235668,"z1 −expm M
X"
REFERENCES,0.697452229299363,"m=1
Ψmcm ! z0  2 2
−1 2"
REFERENCES,0.7006369426751592,"z1 −expm M
X"
REFERENCES,0.7038216560509554,"m=1
bΨmcm ! z0  2"
REFERENCES,0.7070063694267515,"2
,
(20)"
REFERENCES,0.7101910828025477,"where bΨm is the dictionary after the gradient step is taken. A gradient step should decrease the
transport operator objective meaning the EΨdiff value should be positive for an effective step. If
there are many gradient steps that result in negative EΨdiff values, that indicates that the parameters
are not optimal or that the learning rate is too large. This is an important metric to observe during
ﬁne-tuning to determine whether to select a smaller γ or smaller network learning rate."
REFERENCES,0.7133757961783439,Signs of failure of a training run with selected training parameters:
REFERENCES,0.7165605095541401,• All operator magnitudes reduce towards zero.
REFERENCES,0.7197452229299363,• All inferred coefﬁcients between point pairs are zero.
REFERENCES,0.7229299363057324,"• Most of the operators generate transformation paths with latent values that increase quickly
to inﬁnity."
REFERENCES,0.7261146496815286,• Many steps have negative values for EΨdiff.
REFERENCES,0.7292993630573248,"• The operator magnitudes increase which results in unstable training steps with NaN values
in the computed objective."
REFERENCES,0.732484076433121,"Dictionary regularizer parameter
The dictionary regularizer parameter γ is the weight on the
Frobenius norm term in equation 2. This objective term serves two purposes. First, it balances the
effect of the coefﬁcient sparsity regularizer with the parameter ζ. If ζ is large, the sparsity regularizer"
REFERENCES,0.7356687898089171,Under review as a conference paper at ICLR 2022
REFERENCES,0.7388535031847133,"encourages small coefﬁcient magnitudes and one way to achieve that while still effectively inferring
paths is to increase the magnitude of the operators. The Frobenius norm term must have a large
enough inﬂuence to counterbalance this force or the operators will increase in magnitude to the
point of being unstable. If a run is becoming unstable, we recommend decreasing ζ or increasing γ."
REFERENCES,0.7420382165605095,"The second purpose of the dictionary regularizer term is to identify which operators are necessary
to represent the transformations on the data manifold. If an operator is not being used to represent
a transformation between z0 and z1 then the dictionary regularizer reduces its magnitude to zero.
Therefore, during training we are able to estimate the model order based on how many dictionary
elements remain non-zero. In our tests we vary γ between 2 × 10−8 and 2 × 10−4. If γ is too small,
it will not counterbalance the coefﬁcient sparsity term and the dictionaries will grow to unstable
magnitudes. If γ is too large it will reduce the magnitude of all operators to zero. During the ﬁne-
tuning steps, we have often found it necessary to reduce the γ because, with a larger γ, both the
operator magnitudes and the latent vector magnitudes can decrease substantially which leads to an
ineffective manifold model."
REFERENCES,0.7452229299363057,"Coefﬁcient sparsity parameter
The coefﬁcient sparsity parameter ζ in equation 2 controls the
sparsity of the coefﬁcients that are used to estimate paths between z0 and z1. We run tests with
values of ζ between 0.005 and 2. From dataset to dataset the ideal value varies. If ζ values are too
small then all of the operators are used to represent all of the paths between point pairs. The ζ should
be increased so fewer than M coefﬁcients are used for each inferred path. When ζ is too large, all
the coefﬁcients go to zero during inference. This means there is no path inferred between z0 and z1
because of overweighting the sparsity constraint."
REFERENCES,0.7484076433121019,"Number of dictionary elements
As mentioned above, the dictionary regularizer acts as a model
order selection tool so our strategy for selecting number of dictionary elements M is to increase the
number of dictionary elements until some of their magnitudes begin reducing to zero during training.
This indicates that some of the operators are not necessary for representing transformations."
REFERENCES,0.7515923566878981,"Latent dimension
The latent dimension of the autoencoder is selected to ensure quality recon-
structed image outputs."
REFERENCES,0.7547770700636943,"Relative weight of reconstruction and transport operator objectives
The parameter λ deter-
mines the weight of the reconstruction term relative to the transport operator term. We observe good
performance of the model for λ between 0.5 and 0.75."
REFERENCES,0.7579617834394905,"E
COMPARING POINT PAIR SELECTION STRATEGIES"
REFERENCES,0.7611464968152867,"In the past, transport operators have been trained using point pairs that were selected randomly from
the same class (Connor et al., 2021) or with some knowledge of transformation labels (Connor &
Rozell, 2020). In this work, we establish a method that learns natural transformations without re-
quiring transformation labels using a perceptual point pair selection strategy. The perceptual loss
metric (which compares the the features values from the penultimate layer of a pretrained classi-
ﬁer model) enables us to select point pairs that may share semantically meaningful transformations
without being exactly the same. Fig. 9 shows examples of nearest neighbors selected using pixel
similarity, similarity in the autoencoder latent space, and the perceptual loss metric. This highlights
how the perceptual loss metric can be useful for identifying inputs with similar qualitative charac-
teristics. For instance, in the set of images on the left, the perceptual loss metric identiﬁes another
bald man as a nearest neighbor which has similar characteristics of the initial image even though the
exact image looks quite different."
REFERENCES,0.7643312101910829,"When examining the success of learned operators, one characteristic we care about is how well
the operators maintain the stability of generated paths. We consider generated paths stable if the
latent vectors do not expand quickly to inﬁnity. We have deﬁned this as a useful characteristic for
identity-preservation of applied operators – if operators expand latent vectors to inﬁnity, that will
very likely lead to a change in identity. Each of the operators can be viewed as the dynamics matrix
of a continuous time linear dynamical systems model and we can analyze their stability as dynamical
systems by observing their eigenvalues (Strogatz, 2000)."
REFERENCES,0.767515923566879,Under review as a conference paper at ICLR 2022
REFERENCES,0.7707006369426752,"Figure 9: Examples of nearest neighbors identiﬁed through pixel similarity, latent similarity, and
the perceptual loss metric. In each set of 12 images, the images in the ﬁrst column (contained in
green boxes) are the initial reference images and the images to the right of those show three selected
nearest neighbors. The perceptual loss metric identiﬁes neighbors that share similar characteristics,
like hair style, without being exactly the same."
REFERENCES,0.7738853503184714,"In continuous time linear dynamical systems, there are three classiﬁcations of system stability. If a
system is stable, the real parts of all eigenvalues are below zero. In a stable system, the magnitude
of an input shrinks as time progresses forward. If a system is unstable, the real part of at least one
eigenvalue is above zero. In unstable systems, the magnitude of an input expands to inﬁnity as
time progresses forward. Finally, if a system is marginally stable, then the magnitude of an input
remains constant and the real parts of all of its eigenvalues are zero1 (Strogatz, 2000). The transport
operator model is unique because operators can represent dynamics matrices that deﬁne the natural
data variations which do not have the same directionality as temporal systems (where time moves
forward). Therefore transport operators can be applied with positive or negative coefﬁcients and the
traditional view of stability of temporal systems no longer applies."
REFERENCES,0.7770700636942676,"To maintain the stability of generated paths, the operators need to generate cyclic transformation
paths that neither increase nor decrease the latent vector magnitudes. This corresponds to marginal
stability in linear dynamical systems. A marginally stable system has only imaginary eigenval-
ues with no real parts (Strogatz, 2000). Therefore, we can identify transport operators approach-
ing marginal stability by investigating the magnitudes of the real parts of their eigenvalues. The
maximum real magnitude of an eigenvalue of a dynamics matrix drives the speed with which the
generated paths expand to inﬁnity. Therefore this maximum real magnitude is the focus of our inves-
tigation into the stability of paths generated by learned operators. When the maximum magnitude
of the real part for all eigenvectors associated with an operator is close to zero, that indicates that
the operator is closer to marginal stability. Therefore, we quantify the stability of transport operator
generated paths by looking at the maximum magnitude of real parts of eigenvalues associated with
each transport operator. Fig. 10a shows the sorted maximum magnitude of the real parts of eigenval-
ues in each of the 16 operators learned in the experiment in Section 4.1 which learns natural MNIST
variations. The blue x’s are associated with the operators trained using the perceptual point pair
selection strategy and the red dots are associated with the operators trained using a simple strategy
of selecting point pairs as nearest neighbors in the latent space. With the exception of one operator,
all the operators trained using latent space similarity for supervision have larger maximum magni-
tudes of real eigenvalue components than the operators trained using the perceptual loss metric. This
indicates that the operators trained using the latent space similarity to select training point pairs are
farther from marginal stability and can be seen as less stable by our deﬁnition of transport operator
stability."
REFERENCES,0.7802547770700637,"To view the effect of these learned operators more intuitively, we plot the values of the latent di-
mensions as individual operators are applied in Fig. 11. Each line in these plots shows the inﬂuence
of the operators on a single latent dimension (Note that this experiment utilizes a latent dimension
of 10). Fig. 11a shows the paths generated by transport operators trained with the perceptual point
pair selection strategy . The plots with straight lines (i.e., operators 3, 4, 7, 12, 14, and 16) indicate
that those operators had their magnitudes reduced to zero during training and therefore they have no
impact on the latent vectors when applied. Most of the operators learned using the perceptual point
pair selection strategy are close to cyclic except for operator 2 (the operator with the large real com-
ponent magnitude in Fig. 10a). By contrast, several of the operators trained with point pairs selected"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7834394904458599,"1Note that a system can also be marginally stable with at least one pair of eigenvalues with a zero real part
and non-zero imaginary parts and the rest of the eigenvalues with non-positive real parts. For simpliﬁcation of
this analysis, we will focus on striving for systems which have eigenvalues with all real parts equal to 0."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7866242038216561,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7898089171974523,"(a)
(b)
Figure 10: Analysis of eigenvalues of operators learned in the MNIST experiment. (a) The maxi-
mum absolute value of the real parts of eigenvalues computed from each learned operator. (b) Plot
of the real and imaginary parts of the eigenvalues for each operator."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7929936305732485,"as nearest neighbors in the latent space extend to inﬁnity (Fig. 11b). This can explain the larger
maximum real value magnitudes in Fig. 10a. This analysis leads us to conclude that the perceptual
point pair selection strategy has a greater potential to yield identity-preserving transport operators. (a)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7961783439490446,"(b)
Figure 11: Visualizations of the effect of each learned transport operator on each dimension of an
encoded latent vector. In each plot, each line represents a single latent dimension and the coefﬁcient
magnitude of the transformation varies on the x-axis. (a) Paths from operators learned with per-
ceptual point pair supervision. (b) Paths from operators learned with point pairs selected as nearest
neighbors in the latent space."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.7993630573248408,"F
MNIST EXPERIMENT DETAILS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.802547770700637,"The MNIST dataset is made available under the terms of the Creative Commons Attribution-Share
Alike 3.0 license. We split the MNIST dataset into training, validation, and testing sets. The training
set contains 50,000 images from the traditional MNIST training set. The validation set is made up
of the remaining 10,000 images. The traditional MNIST testing set is used for our testing set. The
input images are normalized so their pixel values are between 0 and 1. The network architecture
used for the autoencoder is shown in Table 1. The training parameters for the transport operator
training phase and the ﬁne-tuning phase are shown in Tables 2 and 3."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8057324840764332,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8089171974522293,"Prior to training the coefﬁcient encoder for the MNIST dataset, we train a classiﬁer on the labeled
MNIST image data which we use to encourage identity-preservation during coefﬁcient encoder
training. The training parameters for the coefﬁcient encoder are shown in Table 4. The image
classiﬁer we use is based on the simple LeNet architecture with two convolutional layers and three
fully connected layers (LeCun et al., 1998)."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8121019108280255,"Table 1: Network Architecture for MNIST and Fashion MNIST Experiments
Encoder Network
Decoder Network
Input ∈R28×28
Input ∈R2
conv: chan: 64 , kern: 4, stride: 2, pad: 1
Linear: 3136 Units
BatchNorm: feat: 64
ReLU
ReLU
convTranpose: chan: 64, kern: 4, stride: 1, pad: 1
conv: chan: 64, kern: 4, stride: 2, pad: 1
BatchNorm: feat: 64
BatchNorm: feat: 64
ReLU
ReLU
convTranpose: chann: 64, kern: 4, stride: 2, pad: 2
conv: chan: 64, kern: 4, stride: 1, pad: 0
BatchNorm: feat: 64
BatchNorm: feat: 64
ReLU
ReLU
convTranpose: chan: 1, kernel: 4, stride: 2, pad: 1
Linear: 2 Units
Sigmoid"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8152866242038217,Table 2: Training parameters for the transport operator training phase of the MNIST experiment
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8184713375796179,"MNIST Transport Operator Training Parameters
batch size: 250
autoencoder training epochs: 300
transport operator training epochs: 50
latent space dimension (zdim): 10
M : 16
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.821656050955414,"lrΨ : 10−3
ζ : 0.1
γ : 2 × 10−6
initialization variance for Ψ: 0.05
number of restarts for coefﬁcient inference: 1
nearest neighbor count: 5
latent scale: 30"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8248407643312102,"Table 3: Training parameters for the ﬁne-tuning
phase of the MNIST experiment"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8280254777070064,"MNIST Fine-tuning Parameters
batch size: 250
transport operator training epochs: 100
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8312101910828026,"lrΨ : 10−3
ζ : 0.1
γ : 2 × 10−6
λ: 0.75
number of network update steps: 50
number of Ψ update steps: 50"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8343949044585988,"Table 4:
Training parameters for the
MNIST Coefﬁcient Encoder"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8375796178343949,"MNIST Coefﬁcient Encoder Parameters
batch size: 250
training epochs: 300
lr: 10−3
ζprior : 0.1
λkl: 0.5
coefﬁcient spread scale: 0.1
classiﬁer domain: image"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8407643312101911,"G
MNIST EXPERIMENT ADDITIONAL RESULTS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8439490445859873,"Here we show additional experimental details and results for the MNIST experiment. Fig. 12 shows
the magnitude of all 16 operators after the ﬁne-tuning phase. Six of the operators have their magni-
tudes reduced to zero. Fig. 13 shows the paths generated by transport operators trained on MNIST
data."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8471337579617835,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8503184713375797,Figure 12: The magnitudes of the learned operators after ﬁne-tuning.
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8535031847133758,"(a)
(b)
(c)
(d)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.856687898089172,"(e)
(f)
(g)
(h)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8598726114649682,"(i)
(j)
Figure 13: Paths generated by all non-zero transport operators trained on the MNIST dataset. Im-
ages in the middle column of the image block are the reconstructed inputs and images to the right
and left are images decoded from transformed latent vectors in positive and negative directions,
respectively"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8630573248407644,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8662420382165605,"H
FASHION-MNIST EXPERIMENT DETAILS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8694267515923567,"The Fashion-MNIST dataset is made available under the terms of the MIT license. We split the
Fashion MNIST dataset into training, validation, and testing sets. The training set contains 50,000
images from the Fashion-MNIST training set. The validation set is made up of the remaining 10,000
images. The traditional Fashion-MNIST testing set is used for our testing set. The input images
are normalized so their pixel values are between 0 and 1. The network architecture used for the
autoencoder is the same as in the MNIST experiment and it is shown in Table 1. The training
parameters for the transport operator training phase and the ﬁne-tuning phase are shown in Tables 5
and 6."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8726114649681529,"Prior to training the coefﬁcient encoder for the Fashion-MNIST dataset, we train a classiﬁer the
latent vectors associated with labeled Fashion-MNIST data which we use to encourage identity-
preservation during coefﬁcient encoder training. The training parameters for the coefﬁcient encoder
are shown in Table 7. The latent vector classiﬁer has a simple architecture of Linear(512), ReLU,
Linear(10), Softmax."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8757961783439491,"Table 5: Training parameters for the transport operator training phase of the Fashion-MNIST exper-
iment"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8789808917197452,"Fashion-MNIST Transport Operator Training Parameters
batch size: 200
autoencoder training epochs: 300
transport operator training epochs: 50
latent space dimension (zdim): 10
M : 16
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8821656050955414,"lrΨ : 10−3
ζ : 0.5
γ : 2 × 10−5
initialization variance for Ψ: 0.05
number of restarts for coefﬁcient inference: 1
nearest neighbor count: 5
latent scale: 30"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8853503184713376,"Table 6: Training parameters for the ﬁne-tuning
phase of the Fashion-MNIST experiment"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8885350318471338,"Fashion-MNIST Fine-tuning Parameters
batch size: 200
transport operator training epochs: 150
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.89171974522293,"lrΨ : 10−3
ζ : 0.5
γ : 2 × 10−6
λ: 0.75
number of network update steps: 50
number of Ψ update steps: 50"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8949044585987261,"Table 7: Training parameters for the Fashion-
MNIST Coefﬁcient Encoder"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.8980891719745223,"Fashion-MNIST Coefﬁcient Encoder Parameters
batch size: 200
training epochs: 300
lr: 10−3
ζprior : 0.5
λkl: 0.5
coefﬁcient spread scale: 0.1
classiﬁer domain: latent"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9012738853503185,"I
FASHION MNIST EXPERIMENT ADDITIONAL RESULTS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9044585987261147,"Here we show additional experimental details and results for the Fashion-MNIST experiment.
Fig. 14 shows the magnitude of all 16 operators after the ﬁne-tuning phase. Six of the operators
had their magnitudes reduced to zero."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9076433121019108,"To visualize how the use of the transport operators varies over the latent space, we generate an
Isomap embedding (Tenenbaum et al., 2000) of latent vectors and color each point by the encoded
scale parameter for coefﬁcients associated with each of the transport operators. Fig. 15a shows
these embeddings for Fashion-MNIST data. Each operator has regions of the latent space where
their use is concentrated. Fig. 15b shows the average coefﬁcient scale weights for each class (rows)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.910828025477707,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9140127388535032,Figure 14: The resulting magnitude of the learned operators after ﬁne-tuning the MAE.
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9171974522292994,"and each transport operator (columns) for Fashion-MNIST. There are some classes like trouser and
sandal (classes 1 and 5) which have large encoded coefﬁcient scale weights for most of the transport
operators. This means they are robust to many natural transformations. Other classes like coat and
shirt (classes 4 and 6) have smaller encoded coefﬁcient scale weights which means they are sensitive
to many transformations. The images to the right in Fig. 15b show transport operators being applied
to samples with high encoded scale weights (in a yellow box) and samples with low encoded scale
weights (in a blue box). Fig. 16 shows the paths generated by non-zero transport operators trained
on Fashion-MNIST data. (a)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9203821656050956,"(b)
Figure 15: (a) Visualizations of the encoded coefﬁcient scale weights. The leftmost image shows
an Isomap embedding of the latent vectors with input images overlaid. The scatter plots on the
right show the same Isomap embedding colored by the encoded coefﬁcient scale weights for several
operators (yellow indicates large scale weights and blue small scale weights). We see operators
whose use is localized in regions of the manifold space. (b) The average coefﬁcient scale weights
for each class on each transport operator for Fashion-MNIST. High scale weights for a given operator
(yellow) indicate it can be applied to a given class without easily changing identity. The images on
the right show examples of the operators applied to classes with high encoded scale weights (in the
top yellow boxes) and classes with low encoded scale weights (in bottom blue boxes). The examples
with low coefﬁcient scale weights change classes more easily than other examples."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9235668789808917,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9267515923566879,"(a)
(b)
(c)
(d)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9299363057324841,"(e)
(f)
(g)
(h)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9331210191082803,"(i)
(j)
Figure 16:
Paths generated by all non-zero transport operators trained on the Fashion-MNIST
dataset. Images in the middle column of the image block are the reconstructed inputs and images
to the right and left are images decoded from transformed latent vectors in positive and negative
directions, respectively"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9363057324840764,"J
CELEBA EXPERIMENT DETAILS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9394904458598726,"The CelebA dataset is publicly available for non-commercial research purposes. We split the CelebA
dataset into training and testing sets. The training set contains the ﬁrst 150,000 images accompanied
with the entire test set. The input images are normalized so their pixel values are between 0 and 1.
The network architecture used for the autoencoder is shown in Table 8. The training parameters for
the transport operator training phase and the ﬁne-tuning phase are shown in Table 9."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9426751592356688,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.945859872611465,"The
attribute
classiﬁer
is
a
ResNet-18
model
adapted
from
https://github.com/d-li14/face-attribute-prediction
with
16
classiﬁer
heads after the layer with 512 hidden units. Each classiﬁer head corresponds to a single attribute
and is modeled as a fully-connected linear layer with 256 hidden units, followed by batch normal-
ization, dropout, and ReLU layers. Afterwards, two logits are output for a 0/1 prediction for each
classiﬁer output. The training procedure, including the dynamic loss weighting, follows (Mao et al.,
2020)."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9490445859872612,"Table 8: Network Architecture for CelebA Experiments
Encoder Network
Decoder Network
Input ∈R64×64
Input ∈R32
conv: chan: 32 , kern: 4, stride: 2, pad: 1
Linear: 80,000 Units
BatchNorm: feat: 32
ReLU
ReLU
convTranpose: chan: 256, kern: 3, stride: 1, pad: 0
conv: chan: 64, kern: 4, stride: 2, pad: 1
BatchNorm: feat: 256
BatchNorm: feat: 64
ReLU
ReLU
convTranpose: chann: 256, kern: 3, stride: 1, pad: 0
conv: chan: 128, kern: 3, stride: 2, pad: 1
BatchNorm: feat: 256
BatchNorm: feat: 128
ReLU
ReLU
convTranpose: chan: 256, kernel: 3, stride: 1, pad: 1
conv: chan: 256, kern: 3, stride: 1, pad: 1
BatchNorm: feat: 256
BatchNorm: feat: 128
ReLU
ReLU
convTranpose: chan: 128, kernel: 3, stride: 1, pad: 1
conv: chan: 256, kern: 4, stride: 2, pad: 1
BatchNorm: feat: 128
BatchNorm: feat: 256
ReLU
ReLU
convTranpose: chan: 128, kernel: 3, stride: 1, pad: 0
conv: chan: 128, kern: 4, stride: 2, pad: 1
BatchNorm: feat: 128
BatchNorm: feat: 128
ReLU
ReLU
convTranpose: chan: 3, kernel: 4, stride: 2, pad: 0
Linear: 32 Units
Sigmoid"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9522292993630573,"Table 9: Training parameters for the CelebA experiment
CelebA Transport Operator Training Parameters
batch size: 500
autoencoder training epochs: 300
transport operator training epochs: 50
latent space dimension (zdim): 32
M : 40
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9554140127388535,"lrΨ : 10−3
ζ : 1.5
γ : 1 × 10−5
initialization variance for Ψ: 0.05
number of restarts for coefﬁcient inference: 1
nearest neighbor count: 5
latent scale: 2"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9585987261146497,"CelebA Fine-tuning Parameters
batch size: 500
ﬁne-tuning epochs: 10
lrnet : 10−4"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9617834394904459,"lrΨ : 10−3
ζ : 0.8
γ : 5 × 10−7
λ: 0.75
number of network update steps: 50
number of Ψ update steps: 50"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.964968152866242,"K
CELEBA EXPERIMENT ADDITIONAL RESULTS"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9681528662420382,"Here we show additional experimental results for the CelebA experiment. Fig. 17 shows the paths
generated by the 40 transport operators trained on celebA data. Each row represents a different
operator acting on the same input image. Images in the middle column of the image block are the
reconstructed inputs and images to the right and left are images decoded from transformed latent
vectors in positive and negative directions, respectively. Fig. 18 shows the classiﬁcation outputs of
the attribute classiﬁer for example transport operators. These two operators that vary smiling, beard,
and sunglasses attributes."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9713375796178344,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9745222929936306,"(a)
(b)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9777070063694268,"(c)
(d)
Figure 17: Paths generated by all 40 transport operators trained on the CelebA dataset. Images in
the middle column of the image block are the reconstructed inputs and images to the right and left
are images decoded from transformed latent vectors in positive and negative directions, respectively"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9808917197452229,"(a)
(b)
Figure 18: Paths generated by the application of two learned transport operators with the associated
attribute classiﬁer probability outputs for the transformed images. Our model learns operators that
correspond to meaningful CelebA dataset attributes like (a) smiling (b) sunglasses/beard"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9840764331210191,"Fig. 19 shows an interesting feature in our learned operators – many of them generate cyclic paths
that begin and end at nearly the same point. Also, in these cyclic sequences, the transformations
seem to lead to a change in gender. The image sequence in Fig. 19a shows the path generated by a
single operator. The image in the middle is the reconstructed input image and the images to the left
and right are the paths generated by negative and positive coefﬁcients respectively. This operator
changes the hairline and quantity of bangs. As we apply the transport operator with a negative
coefﬁcient (to the left of center), the woman gains bangs and then becomes a man with a moustache
on the far left of the image. As we apply the transport operator with a positive coefﬁcient (to the
right of center), the woman’s forehead gets higher and then she becomes a man with a high forehead
and eventually, on the far right the woman transforms into a man with bangs, similar to the man
on the far left. The similarity between the generated images on the far left and far right is notable"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9872611464968153,Under review as a conference paper at ICLR 2022
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9904458598726115,"because this indicates the transformation path is nearly closed. Fig. 19b shows the change in ﬁve of
the 32 latent dimensions over the generated path. These paths have a nearly cyclic structure."
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9936305732484076,"This is particularly interesting because in (Connor & Rozell, 2020) they learn closed transformation
paths by selecting point pairs on known cyclic transformation paths and highlight the beneﬁt of the
transport operator model for representing these types of paths. In this case, we learn this cyclic path
with only perceptual point pair supervision. Additionally, this identiﬁes a beneﬁt of the transport
operator approach over other models of the manifold in a neural network latent space. We can
learn nonlinear paths that keep the generated points in the same neighborhood in the latent space
without extending to inﬁnity which is inevitable when linear paths that are used to represent natural
transformations. (a)"
NOTE THAT A SYSTEM CAN ALSO BE MARGINALLY STABLE WITH AT LEAST ONE PAIR OF EIGENVALUES WITH A ZERO REAL PART,0.9968152866242038,"(b)
Figure 19:
An example of an operator that generates a nearly cyclic path in the latent space (a)
Image outputs along a path generated by a single learned operator. The images on the far left and
far right look similar which indicates that this operator generates a nearly closed path that begins
and ends at the same point. (b) Paths of ﬁve of the 32 latent dimensions as the learned operator is
applied to them. This again highlights the cyclic nature of the transport operator generated paths."
