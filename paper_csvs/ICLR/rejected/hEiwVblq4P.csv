Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010570824524312897,"In the quantized network, its gradient shows vanishing except for non-
differentiable points. The network thus cannot be learned by the standard back-
propagation, so that an alternative approach called Straight Through Estimator
(STE), which replaces the part of the gradient with a simple differentiable func-
tion, is used. While STE is known to work well for learning the quantized network
empirically, it has not been established theoretically. A recent study by Yin et al.
(2019) has provided theoretical support for STE. However, its justiﬁcation is still
limited to the model in the one-hidden layer network with the binary activation
where Gaussian generates the input data, and the true labels are output from the
teacher network with the same binary network architecture. In this paper, we dis-
cuss the effectiveness of STEs in more general situations without assuming the
shape of the input distribution and the labels. By considering the scale symmetry
of the network and speciﬁc properties of the STEs, we ﬁnd that STE with clipped
Relu is superior to STEs with identity function and vanilla Relu. The clipped
Relu STE, which breaks the scale symmetry, may pick up one of the local minima
degenerated in scales, while the identity STE and vanilla Relu STE, which keep
the scale symmetry, may not pick it up. To conﬁrm this observation, we further
present an analysis of a simple misspeciﬁed model as an example. We ﬁnd that
all the stationary points are identical with the vanishing points of the cRelu STE
gradient, while some of them are not identical with the vanishing points of the
identity and Relu STE. Finally we have numerically conﬁrmed the observation
for the mixture Gaussian model with various teacher network."
INTRODUCTION,0.0021141649048625794,"1
INTRODUCTION"
INTRODUCTION,0.003171247357293869,"Quantization of the weights and the activations is a promising technique to save the memory and
accelerate the inference speed in deep neural networks (DNNs) which have been getting wider and
deeper in recent years. There are two main approaches of the quantization for DNNs (Krishnamoor-
thi (2018)): Post-training quantization (PTQ) and quantization-aware training (QAT). In the PTQ,
the pre-trained networks are simply quantized without re-training the model. This approach allows
us to achieve the nearly ﬂoating point accuracy at 8-bits, while below 8-bits, this results in signiﬁcant
accuracy degradation. Although there have been recent attempts to alleviate the accuracy degrada-
tion in PTQ (Banner et al. (2018); Choukroun et al. (2019); Zhao et al. (2019); Kryzhanovskiy et al.
(2021)), the QAT, where the quantized weights and activations are trained (e.g., Zhou et al. (2016);
Hubara et al. (2017); Rastegari et al. (2016)), usually leads to better accuracy."
INTRODUCTION,0.004228329809725159,"The difﬁculty in QAT is that the weights and the activations are discretized, and intrinsically non-
differentiable. If we take the derivatives forcibly, they either vanish or diverge. To avoid this prob-
lem, we replace them with the derivatives of some differentiable function in the backward pass only,
called the Straight-Through Estimator (STE) (Hinton et al. (2012); Bengio et al. (2013)). Since the
replacement leads to bias, it is not always possible to learn the network successfully. However this
approach can be applied to the low bits below 8-bits with tolerable accuracy degradation (Zhou et al.
(2016); Choi et al. (2018); Esser et al. (2019); Bhalgat et al. (2020))."
INTRODUCTION,0.005285412262156448,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006342494714587738,"Originally, STE was introduced as the replacement with the derivative of the identity function by
Hinton et al. (2012). Later, the term ”STE” has been extensively used as the replacement with vari-
ous functions. In the binary network, Bengio et al. (2013) studied the replacement with the derivative
of sigmoid function as well as the original STE, and Hubara et al. (2017) used the derivative of the
identity function clipped in the region of |x| ≤1. In low-bits network, Zhou et al. (2016) used the
quantized gradient after the replacement with the identity function, and Choi et al. (2018), Esser
et al. (2019), and Bhalgat et al. (2020) used the the derivative of the clipped Relu (cRelu) for leaning
the step size."
MAIN CONTRIBUTION,0.007399577167019027,"1.1
MAIN CONTRIBUTION"
MAIN CONTRIBUTION,0.008456659619450317,"In this paper, we refer to the proxy of the gradient as the STE gradient 1. Although STE has been
much success in training quantized DNNs empirically, the theoretical justiﬁcation is very limited to
the speciﬁc situations. It also remains unclear what differentiable functions should be used for the
STE gradient. Our aim is to shed light on a new factor to determine the functions."
MAIN CONTRIBUTION,0.009513742071881607,"Without assuming the shape of the input distribution and the loss function, we ﬁrst discuss the
properties of the three STEs, identity STE, Relu STE, and cRelu STE in one-hidden-layer network
with binary activation from the perspective of the intrinsic symmetry. We ﬁnd that because the
identity STE and Relu STE keep the scale symmetry, the STE gradients should be zero for any scale
to converge to the true (local) minimum of the loss function, while in the case of the cRelu STE,
which breaks the scale symmetry, even if its gradient is not zero at the minimum at some scale and
it becomes zero at other scale, the cRelu STE converges to the minimum. Therefore, the back-
propagation using the cRelu STE is most likely to converge to the minimum of the loss function
among three STEs. This result reveals differences between Relu STE and cRelu STE, which could
not be found in the models by Yin et al. (2019) and Long et al. (2021)."
MAIN CONTRIBUTION,0.010570824524312896,"Recently, Kunin et al. (2020) discussed the symmetry embedded in non-quantized neural networks,
and the effect of its breaking such as the discretization, the weight decay, and the stochasticity during
training. We discuss the new effect of breaking symmetry by the proxy of the gradient in training
the quantized neural networks."
MAIN CONTRIBUTION,0.011627906976744186,"Next, to conﬁrm this observation, we have studied a similar model of the Gaussian input as discussed
in Yin et al. (2019) as an example. We have employed a misspeciﬁed model suitable for most
practical situations. Unlike the previous study, the true labels are presumed to be generated by the
non-quantized network architecture. Consequently, we ﬁnd that ignoring the scale degeneracy for
the weight in front of the activation, all the stationary points are identical with the vanishing points
in the cRelu STE gradient, while some of them are not identical with the vanishing points in the
identity and the Relu STE gradients. In particular, at the global minimum point, both identity and
Relu STE gradients do not vanish. Finally, to conﬁrm it in more general case, we have numerically
studied the model of the mixture Gaussian input."
RELATED WORKS,0.012684989429175475,"1.2
RELATED WORKS"
RELATED WORKS,0.013742071881606765,"Recently, there have been a few theoretical studies on the justiﬁcation of the STEs. Shekhovtsov &
Yanush (2020) derived STE by the linearization of their proposed estimator in a stochastic binary
deep network where the noises are injected before the binary activation to be a smooth network.
Cheng et al. (2019) argued that STE can be interpreted as a projected Wasserstein gradient ﬂow
under certain conditions. Yin et al. (2019) that inspired our study examined which of the three
STE’s, identity STE, Relu STE, and cRelu STE, converges to the true minimum in one-hidden-
layer network with binarized activation and Gaussian input data. They clariﬁed that if the labels
are obtained from a teacher network of the same architecture (Du et al. (2018)), all the three STEs
show the non-negative correlation with the population gradient, but only the identity STE gradient
does not become zero in a local minimum. This indicates that the back-propagation using either
Relu STE or cRelu STE may convergence to the local minimum, while it is impossible to show
the convergence to the local minimum using the identity STE. Furthermore, they showed that all
the three STEs gradients vanish at the global minimum, indicating that they may achieve the global
minimum independent of the choice of the STE, if we start with an appropriate initial value."
RELATED WORKS,0.014799154334038054,1It is called “coarse” gradient in Yin et al. (2019).
RELATED WORKS,0.015856236786469344,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.016913319238900635,"Very recently, Long et al. (2021) have discussed the justiﬁcation of a class of STEs with certain
monotonicity, which generalizes the Relu STE, in one-hidden-layer network for the hinge loss,
claiming that the STE gradients vanish at the global minimum."
NOTATIONS,0.017970401691331923,"1.3
NOTATIONS"
NOTATIONS,0.019027484143763214,"|·| denotes the Euclidean norm of a vector. A capital letter in bold denotes the matrix, e.g., Z, and
its component is given by the letter with lower indices, e.g., Zij. A small letter in bold denotes the
vector, e.g., g, and its component is given by the letter with lower indices, e.g., gj. Im×m is the
m × m identity matrix, and 1m is the m-dimensional vector of all ones."
"GENERAL DISCUSSION ON STES IN ONE-HIDDEN LAYER CNN WITH
BINARY ACTIVATION",0.0200845665961945,"2
GENERAL DISCUSSION ON STES IN ONE-HIDDEN LAYER CNN WITH
BINARY ACTIVATION"
PRELIMINARIES,0.021141649048625793,"2.1
PRELIMINARIES"
PRELIMINARIES,0.022198731501057084,The one-hidden layer convolutional neural network with a binary activation function is realized as
PRELIMINARIES,0.023255813953488372,"y(Z, v, w) = vT σ(Zw) =
X"
PRELIMINARIES,0.024312896405919663,"i
viσ(
X"
PRELIMINARIES,0.02536997885835095,"j
Zijwj),
(1)"
PRELIMINARIES,0.026427061310782242,"where σ(·) is the step function,"
PRELIMINARIES,0.02748414376321353,"σ(x) =

1
x > 0,
0
x ≤0,
(2)"
PRELIMINARIES,0.02854122621564482,"Z ∈Rm×n is constructed by deividing the input data, e.g. an image, into m patches with size n,
and w ∈Rn and v ∈Rm are the trainable weights, i.e. kernels, in the ﬁrst and second layers,
respectively (Du et al. (2018)). We assume that Z is generated by a continuous distribution P(Z).
Due to the property of the step function, the network is invariant under the scale transformation for
w, w →kw (k > 0)."
PRELIMINARIES,0.02959830866807611,"Using the loss for each sample, ℓ(y(v, w, Z); y∗) with y∗being the labels generated by the true
distribution q(y∗|Z), we can express the population loss function as
L(v, w; y∗) = EZ [ℓ(y(v, w, Z); y∗)] .
(3)"
BACK-PROPAGATION AND STE GRADIENTS,0.0306553911205074,"2.2
BACK-PROPAGATION AND STE GRADIENTS"
BACK-PROPAGATION AND STE GRADIENTS,0.03171247357293869,"The gradients of the loss for each sample w.r.t v and w can be formally expressed as
∂ℓ
∂vi
= ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.03276955602536998,"∂y
∂y
∂vi
= ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.03382663847780127,"∂y σ(
X"
BACK-PROPAGATION AND STE GRADIENTS,0.03488372093023256,"j
Zijwj),
(4)"
BACK-PROPAGATION AND STE GRADIENTS,0.035940803382663845,"∂ℓ
∂wj
=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.03699788583509514,"∂ℓ
∂xi"
BACK-PROPAGATION AND STE GRADIENTS,0.03805496828752643,"∂xi
∂wj
=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.039112050739957716,"∂ℓ
∂y viδ  X"
BACK-PROPAGATION AND STE GRADIENTS,0.040169133192389,"j′
Zij′wj′ "
BACK-PROPAGATION AND STE GRADIENTS,0.0412262156448203,"Zij,
(5)"
BACK-PROPAGATION AND STE GRADIENTS,0.042283298097251586,where xi = σ(P
BACK-PROPAGATION AND STE GRADIENTS,0.04334038054968287,"j Zijwj) and we used the property that the derivative of the step function is Dirac’s
delta function, dσ(z)/dz = δ(z). The gradient w.r.t v can be utilized for learning the network, while
it is impossible to use the gradient w.r.t w due to the presence of the delta function. Remarkably,
both population gradients w.r.t v and w are smooth functions due to its average over the continuous
distribution, EZ  ∂ℓ ∂vi"
BACK-PROPAGATION AND STE GRADIENTS,0.04439746300211417,"
=
Z  Y"
BACK-PROPAGATION AND STE GRADIENTS,0.045454545454545456,"i′,j′
d ˜Zi′j′ "
BACK-PROPAGATION AND STE GRADIENTS,0.046511627906976744,"P(
n
˜Zi′j′
o
) ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.04756871035940803,"∂y σ

˜Zi1|w|

,
(6) EZ  ∂ℓ ∂wj"
BACK-PROPAGATION AND STE GRADIENTS,0.048625792811839326,"
=
1
|w| X"
BACK-PROPAGATION AND STE GRADIENTS,0.049682875264270614,"i
Fij(v, w, ˜Zi1 = 0),"
BACK-PROPAGATION AND STE GRADIENTS,0.0507399577167019,"Fij(v, w, ˜Zi1) ≡
Z  
Y"
BACK-PROPAGATION AND STE GRADIENTS,0.05179704016913319,"(i′,j′)̸=(i,1)
d ˜Zi′j′ "
BACK-PROPAGATION AND STE GRADIENTS,0.052854122621564484,"P(
n
˜Zi′j′
o
)"
BACK-PROPAGATION AND STE GRADIENTS,0.05391120507399577,"""
∂ℓ
∂y vi
X"
BACK-PROPAGATION AND STE GRADIENTS,0.05496828752642706,"k′
Oi
k′j ˜Zik′ # ,
(7)"
BACK-PROPAGATION AND STE GRADIENTS,0.056025369978858354,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.05708245243128964,"where we used the orthogonal transformation, ˜Zik′ = P"
BACK-PROPAGATION AND STE GRADIENTS,0.05813953488372093,"j′ Oi
k′j′Zij′, Zij′ = P"
BACK-PROPAGATION AND STE GRADIENTS,0.05919661733615222,"k′ Oi
k′j′ ˜Zik′, with
Oi
1j′ = wj′/|w| and P"
BACK-PROPAGATION AND STE GRADIENTS,0.06025369978858351,"j′ Oi
k′j′Oi
l′j′ = δk′,l′. We can easily see that the gradient w.r.t. w does not"
BACK-PROPAGATION AND STE GRADIENTS,0.0613107822410148,"have the component in w-direction, P"
BACK-PROPAGATION AND STE GRADIENTS,0.06236786469344609,"j wjEZ
h
∂ℓ
∂wj"
BACK-PROPAGATION AND STE GRADIENTS,0.06342494714587738,"i
= 0. Its vanishing originally follows from
the scale symmetry of the network: Because the loss has the scale symmetry, its gradient in the
w-direction is obviously zero."
BACK-PROPAGATION AND STE GRADIENTS,0.06448202959830866,"Instead of the population gradient, we use the STE in practice, which replaces the delta function
with the derivative of some differentiable function µSTE, to train the kernel w,"
BACK-PROPAGATION AND STE GRADIENTS,0.06553911205073996,"gSTE
j
(v, w, Z) ≡
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.06659619450317125,"∂ℓ
∂y viµ′
STE  X"
BACK-PROPAGATION AND STE GRADIENTS,0.06765327695560254,"j′
Zij′wj′ "
BACK-PROPAGATION AND STE GRADIENTS,0.06871035940803383,"Zij.
(8)"
BACK-PROPAGATION AND STE GRADIENTS,0.06976744186046512,"We consider three types of µSTE(x): (i) the identity type µSTE(x) = x , (ii) vanilla ReLU type
µSTE(x) = xσ(x) and (iii) the cReLU type µSTE(x) = xσ(x)σ(r −x) with the upper clipping
value r. The surrogate back-propagation using the STE is described in Algorithm 1 (Yin et al.
(2019))."
BACK-PROPAGATION AND STE GRADIENTS,0.0708245243128964,"Algorithm 1 Surrogate back-propagation using STE for learning one-hidden-layer CNN.
Input: initialization v0 ∈Rm, w0 ∈Rn, learning rate η."
BACK-PROPAGATION AND STE GRADIENTS,0.07188160676532769,"for t = 0, 1, , . . . do"
BACK-PROPAGATION AND STE GRADIENTS,0.07293868921775898,"vt+1 = vt −η EZ
 ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.07399577167019028,"∂v(vt, wt; Z)
"
BACK-PROPAGATION AND STE GRADIENTS,0.07505285412262157,"wt+1 = wt −ηEZ

gSTE(vt, wt; Z)
"
BACK-PROPAGATION AND STE GRADIENTS,0.07610993657505286,end for
BACK-PROPAGATION AND STE GRADIENTS,0.07716701902748414,The average of the STE gradients over the input data is also expressed as
BACK-PROPAGATION AND STE GRADIENTS,0.07822410147991543,"EZ

gSTE
j
(v, w; Z)

=
X i"
BACK-PROPAGATION AND STE GRADIENTS,0.07928118393234672,"Z
d ˜Zi1µ′
STE

˜Zi1|w|

Fij(v, w, ˜Zi1).
(9)"
BACK-PROPAGATION AND STE GRADIENTS,0.080338266384778,"Note that in general, the STE gradients have the non-zero component in the w-direction due to their
partial collapse of the form of the gradient, P"
BACK-PROPAGATION AND STE GRADIENTS,0.08139534883720931,"j wjEZ

gSTE
j
(v, w; Z)

̸= 0."
BACK-PROPAGATION AND STE GRADIENTS,0.0824524312896406,"It is obvious from Eq.(9) that the only difference between the STE gradients with the identity func-
tion, Relu, and cRelu is the region of the integration over ˜Zi1,"
BACK-PROPAGATION AND STE GRADIENTS,0.08350951374207188,"EZ

gA
j (v, w; Z)

=
X i Z"
BACK-PROPAGATION AND STE GRADIENTS,0.08456659619450317,"RA
d ˜Zi1Fij(v, w, ˜Zi1),
(10)"
BACK-PROPAGATION AND STE GRADIENTS,0.08562367864693446,"where A = id, Relu, cRelu, and the integration reigion RA are given as Rid = (−∞, +∞),
RRelu = [0, +∞), and RcRelu = [0,
r
|w|]."
BACK-PROPAGATION AND STE GRADIENTS,0.08668076109936575,"Following the scale invariance of Fij(v, w, ˜Zi1) for w, Fi′j(v, kw, ˜Zi′1) = Fi′j(v, w, ˜Zi′1) (k >
0), the STE gradients have the following interesting features:"
BACK-PROPAGATION AND STE GRADIENTS,0.08773784355179703,"1. The identity and Relu STE gradients have scale invariance for w,"
BACK-PROPAGATION AND STE GRADIENTS,0.08879492600422834,"EZ
h
gid/Relu
j
(v, kw; Z)
i
= EZ
h
gid/Relu
j
(v, w; Z)
i
.
(11)"
BACK-PROPAGATION AND STE GRADIENTS,0.08985200845665962,"2. The cRelu STE gradient does not have scale invariance for w due to the clipping effect,"
BACK-PROPAGATION AND STE GRADIENTS,0.09090909090909091,"EZ

gcRelu
j
(v, kw, r; Z)

̸= EZ

gcRelu
j
(v, w, r; Z)

,
(12)"
BACK-PROPAGATION AND STE GRADIENTS,0.0919661733615222,"where we have explicitly shown the dependence of the upper clipping value r.
3. Instead, the scale transformation of w for the cRelu STE gradient can be compensated by
that of r:"
BACK-PROPAGATION AND STE GRADIENTS,0.09302325581395349,"EZ

gcRelu
j
(v, w, kr; Z)

= EZ

gcRelu
j
(v, w/k, r; Z)

.
(13)"
BACK-PROPAGATION AND STE GRADIENTS,0.09408033826638477,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.09513742071881606,"If we take kr with ﬁxed r large enough to cover most of the distribution P(Z), the left-hand
side is identical with the Relu STE gradient. Here we deﬁne the spread of the distribution
as ρ. When we redeﬁne w/k as w on the right hand, the cRelu STE gradient for |w| < r/ρ
is approximately identical with the Relu STE gradient,"
BACK-PROPAGATION AND STE GRADIENTS,0.09619450317124736,"EZ

gcRelu
j
(v, w, r; Z)

≃EZ

gRelu
j
(v, w; Z)

for |w| < r/ρ.
(14)"
BACK-PROPAGATION AND STE GRADIENTS,0.09725158562367865,"This result is intuitively obvious: when w is small enough to keep most of the pre-activation
values in the clipped range, the cRelu gradient and the Relu gradient are approximately
identical."
BACK-PROPAGATION AND STE GRADIENTS,0.09830866807610994,"The important issue is that there is no guarantee that the weights obtained by the STE back-
propagation are the ones at the (local) minimum of the loss function. In other words, at the stationary
points, (v, w) = (vs, ws), deﬁned as the vanishing points of the population gradient, EZ  ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.09936575052854123,"∂vi
(v = vs, w = ws)

= 0,
EZ  ∂ℓ"
BACK-PROPAGATION AND STE GRADIENTS,0.10042283298097252,"∂wj
(v = vs, w = ws)

= 0,
(15)"
BACK-PROPAGATION AND STE GRADIENTS,0.1014799154334038,"the STE gradient is not zero in general. Here we consider the local minimum point that the STE
gradient does not vanish,"
BACK-PROPAGATION AND STE GRADIENTS,0.10253699788583509,"EZ

gSTE
j
(vs, ws; Z)

̸= 0.
(16)"
BACK-PROPAGATION AND STE GRADIENTS,0.10359408033826638,"Note that (v, w) = (vs, kws) for any k > 0 is also the local minimum point due to the scale
invariance of the loss. In the case of the Relu/identity STEs, their gradient has the same ﬁnite value
for any scales due to the scale invariance shown in Eq.(11),"
BACK-PROPAGATION AND STE GRADIENTS,0.10465116279069768,"EZ
h
gid/Relu
j
(vs, kws; Z)
i
= EZ
h
gid/Relu
j
(vs, ws; Z)
i
̸= 0.
(17)"
BACK-PROPAGATION AND STE GRADIENTS,0.10570824524312897,"In the case of the cRelu STE, however, the value of its gradient changes as the scale changes due to
the breaking of the scale symmetry shown in Eq.(12). If it crosses zero as a function of scale,"
BACK-PROPAGATION AND STE GRADIENTS,0.10676532769556026,"EZ

gcRelu
j
(vs, k0ws, r; Z)

= 0 for some k0,
(18)"
BACK-PROPAGATION AND STE GRADIENTS,0.10782241014799154,"the back propagation using cRelu STE can converge at the local minimum as was done by using
the population gradient. Therefore, we conjecture that due to breaking scale symmetry, the back-
propagation using cRelu STE is the most likely to achieve the (local) minimum of the loss function
in the three STEs. This also implies that the cRelu STE is less biased than the other. We conﬁrm
the conjecture analytically for the Gaussian input with Relu-type teacher network in Sec.3, and
numerically for the mixture Gaussian input with various teacher network in Sec. 4."
BACK-PROPAGATION AND STE GRADIENTS,0.10887949260042283,"Since the above discussion is focused only on symmetry, the conjecture can be generalized to net-
works with more symmetries in a straightforward way. In that case, if we employ an STE that breaks
all of those symmetries instead of cRelu STE, it is more likely to achieve the minimum."
BACK-PROPAGATION AND STE GRADIENTS,0.10993657505285412,"The similar mechanism can be also found in physical phenomena. For instance, this is the case of
the ferromagnet, which has rotational symmetry in energy (or Hamiltonian). In the system, if we
impose the magnetic ﬁeld, which breaks the rotational symmetry, the spin states, corresponding to
minimum points, become aligned with the same direction."
BACK-PROPAGATION AND STE GRADIENTS,0.1109936575052854,"3
ONE-HIDDEN LAYER CNN WITH BINARY ACTIVATION: GAUSSIAN INPUT
AND LABELS GENERATED BY NON-QUANTIZED RELU NETWORK"
BACK-PROPAGATION AND STE GRADIENTS,0.11205073995771671,We consider the simple model similar to Yin et al. (2019) given by the Gaussian input with
BACK-PROPAGATION AND STE GRADIENTS,0.113107822410148,"the variance ˆσ2, Z ∼P(Z) =

1
√"
BACK-PROPAGATION AND STE GRADIENTS,0.11416490486257928,"2πˆσ2
mn
e−P i,j"
BACK-PROPAGATION AND STE GRADIENTS,0.11522198731501057,"Z2
ij
2ˆσ2 . We use the squared loss function,"
BACK-PROPAGATION AND STE GRADIENTS,0.11627906976744186,"ℓ(y(v, w, Z); y∗(v∗, w∗, Z)) =
1
2 (y −y∗)2 . Unlike in Yin et al. (2019), the true labels are as-
sumed to be generated by the non-quantized Relu network,"
BACK-PROPAGATION AND STE GRADIENTS,0.11733615221987315,"y∗(v∗, w∗, Z) =
X"
BACK-PROPAGATION AND STE GRADIENTS,0.11839323467230443,"i
v∗
i fRelu  X"
BACK-PROPAGATION AND STE GRADIENTS,0.11945031712473574,"j
Zijw∗
j "
BACK-PROPAGATION AND STE GRADIENTS,0.12050739957716702,",
(19)"
BACK-PROPAGATION AND STE GRADIENTS,0.12156448202959831,Under review as a conference paper at ICLR 2022
BACK-PROPAGATION AND STE GRADIENTS,0.1226215644820296,with the Relu activation deﬁned as fRelu(x) = xσ(x).
BACK-PROPAGATION AND STE GRADIENTS,0.12367864693446089,"Note that the models are misspeciﬁed, i.e., the population loss cannnot be zero even at the global
minimum. In fact, it is inferred from the accuracy degradation in most practical situations (Hubara
et al. (2017)) that the binary networks are considered as misspeciﬁed models. On the other hand, if
the true labels are generated by the same binarized architecture as was used in Yin et al. (2019), the
network becomes a speciﬁed model."
BACK-PROPAGATION AND STE GRADIENTS,0.12473572938689217,"As shown below, this misspeciﬁed model leads to a more striking difference between three STEs
than the speciﬁed model. Particularly, the behaviors of three STE gradients at the global minimum
are completely different from each other."
BACK-PROPAGATION AND STE GRADIENTS,0.12579281183932348,"3.1
POPULATION LOSS, ITS GRADIENT AND STATIONARY POINTS"
BACK-PROPAGATION AND STE GRADIENTS,0.12684989429175475,"The population loss L(v, w; v∗, w∗) is derived in Appendix A.1 as"
BACK-PROPAGATION AND STE GRADIENTS,0.12790697674418605,"L(v, w; v∗, w∗) = EZ [ℓ(v, w, Z; v∗, w∗)] = 1"
BACK-PROPAGATION AND STE GRADIENTS,0.12896405919661733,"8vT  
Im×m + 1m1T
m

v −ˆσ|w∗| 2
√"
BACK-PROPAGATION AND STE GRADIENTS,0.13002114164904863,"2π vT  
cos ϕIm×m + 1m1T
m

v∗ + ˆσ2"
BACK-PROPAGATION AND STE GRADIENTS,0.13107822410147993,"4π |w∗|2v∗T  
(π −1)Im×m + 1m1T
m

v∗
(20)"
BACK-PROPAGATION AND STE GRADIENTS,0.1321353065539112,"where ϕ is the angle between w∗and w. Note that the loss is scale invariant for w, and thus its
derivative satisﬁes ew · ∂L"
BACK-PROPAGATION AND STE GRADIENTS,0.1331923890063425,∂w = 0. The stationary points are given by
BACK-PROPAGATION AND STE GRADIENTS,0.13424947145877378,"∂L
∂v = 0 ⇔v = 2ˆσ|w∗|
√ 2π"
BACK-PROPAGATION AND STE GRADIENTS,0.13530655391120508,"
cos ϕIm×m + 1 −cos ϕ"
BACK-PROPAGATION AND STE GRADIENTS,0.13636363636363635,"m + 1 1m1T
m"
BACK-PROPAGATION AND STE GRADIENTS,0.13742071881606766,"
v∗
(21)"
BACK-PROPAGATION AND STE GRADIENTS,0.13847780126849896,"∂L
∂w = 0 ⇔vT v∗= 0 or ϕ = 0 or ϕ = π
(22)"
BACK-PROPAGATION AND STE GRADIENTS,0.13953488372093023,"where we used
 
Im×m + 1m1T
m
−1 = Im×m −
1
m+11m1T
m."
BACK-PROPAGATION AND STE GRADIENTS,0.14059196617336153,There are three stationary points 2 as classiﬁed in Appendix A.2:
BACK-PROPAGATION AND STE GRADIENTS,0.1416490486257928,"1. middle saddle point given by v = ¯v ≡
2ˆσ|w∗|
√ 2π"
BACK-PROPAGATION AND STE GRADIENTS,0.1427061310782241,"
cos ¯ϕIm×m + 1−cos ¯ϕ"
BACK-PROPAGATION AND STE GRADIENTS,0.14376321353065538,"m+1 1m1T
m

v∗, ϕ ="
BACK-PROPAGATION AND STE GRADIENTS,0.14482029598308668,"¯ϕ ≡arccos

(v∗T 1m)
2"
BACK-PROPAGATION AND STE GRADIENTS,0.14587737843551796,−(m+1)(v∗)2+(v∗T 1m)2
BACK-PROPAGATION AND STE GRADIENTS,0.14693446088794926,"
, which satisﬁes ¯vT v∗= 0. This exists if and only"
BACK-PROPAGATION AND STE GRADIENTS,0.14799154334038056,"if (m + 1) (v∗)2 ≥2
 
v∗T 1m
2."
BACK-PROPAGATION AND STE GRADIENTS,0.14904862579281183,"2. local minimum if (m + 1) (v∗)2 ≥2
 
v∗T 1m
2 , otherwise saddle point given by v ="
BACK-PROPAGATION AND STE GRADIENTS,0.15010570824524314,"vπ ≡2ˆσ|w∗|
√ 2π"
BACK-PROPAGATION AND STE GRADIENTS,0.1511627906976744,"
−Im×m +
2
m+11m1T
m

v∗, ϕ = π."
BACK-PROPAGATION AND STE GRADIENTS,0.1522198731501057,"3. global minimum given by v = v0 ≡
2ˆσ|w∗|
√"
BACK-PROPAGATION AND STE GRADIENTS,0.15327695560253699,"2π v∗, ϕ = 0. At the global minimum, the"
BACK-PROPAGATION AND STE GRADIENTS,0.1543340380549683,"population loss does not become zero, L(v = v0, w = w∗) = ˆσ2|w∗|2(π−2)"
BACK-PROPAGATION AND STE GRADIENTS,0.1553911205073996,"4π
v∗T v."
STE GRADIENT,0.15644820295983086,"3.2
STE GRADIENT"
STE GRADIENT,0.15750528541226216,The STE gradient is given by
STE GRADIENT,0.15856236786469344,"gSTE
j
=
X"
STE GRADIENT,0.15961945031712474,"i
viµ′
STE  X"
STE GRADIENT,0.160676532769556,"j′
Zij′wj′  Zij ×  X"
STE GRADIENT,0.16173361522198731,"i′
vi′σ  X"
STE GRADIENT,0.16279069767441862,"j′
Zi′j′wj′  −
X"
STE GRADIENT,0.1638477801268499,"i′
v∗
i′fRelu  X"
STE GRADIENT,0.1649048625792812,"j′
Zi′j′w∗
j′   "
STE GRADIENT,0.16596194503171247,".
(23)"
STE GRADIENT,0.16701902748414377,"2The deﬁnition of the stationary points is not mathematically rigorous in our paper. Even if points are
non-differentiable, but the gradient gives zero at the points in the one-side limit, we call them the stationary
points."
STE GRADIENT,0.16807610993657504,Under review as a conference paper at ICLR 2022
STE GRADIENT,0.16913319238900634,"The converging points by the back-propagation in Algorithm 1 are characterized by vanishing the
gradients, EZ  ∂ℓ"
STE GRADIENT,0.17019027484143764,"∂vi
(v, w; Z)

= 0, EZ

gSTE
j
(v, w; Z)

= 0.
(24)"
STE GRADIENT,0.17124735729386892,"We discuss the explicit form of the three STE gradients and the associated vanishing points. The
detailed derivation of the STE gradients is shown in Appendix A.3-A.5."
IDENTITY STE,0.17230443974630022,"3.2.1
IDENTITY STE"
IDENTITY STE,0.1733615221987315,The identity STE gradient is given by
IDENTITY STE,0.1744186046511628,"EZ

gid
=
ˆσ
√"
IDENTITY STE,0.17547568710359407,"2π
w
|w|
 
vT v

−ˆσ2"
IDENTITY STE,0.17653276955602537,"2 w∗ 
vT v∗
(25)"
IDENTITY STE,0.17758985200845667,"Combined with Eq.(21), we ﬁnd that the equation in Eq.(24) has no solution. Therefore, if we use
the back-propagation by identity STE, it can not converge to any points."
RELU STE,0.17864693446088795,"3.2.2
RELU STE"
RELU STE,0.17970401691331925,The Relu STE gradient is expressed as
RELU STE,0.18076109936575052,"EZ [grelu] =
ˆσ
2
√"
RELU STE,0.18181818181818182,"2π
w
|w|

vT  
Im×m + 1m1T
m

v

−ˆσ2"
RELU STE,0.1828752642706131,"2π
|w∗|"
RELU STE,0.1839323467230444,"|w| w

vT  
−Im×m + 1m1T
m

v∗"
RELU STE,0.1849894291754757,"−ˆσ2

(π −ϕ) w∗+ |w∗| sin ϕ"
RELU STE,0.18604651162790697,"|w|
w
  
vT v∗"
RELU STE,0.18710359408033828,"2π
.
(26)"
RELU STE,0.18816067653276955,"Combined with Eq.(21), the gradients vanish at the following points:"
RELU STE,0.18921775898520085,"1. If v∗satisﬁes (m + 1) (v∗)2 ≥2
 
v∗T 1m
2 , ϕ = ¯ϕ = arccos

(v∗T 1m)
2"
RELU STE,0.19027484143763213,"−(m+1)(v∗)2+(v∗T 1m)2 
,"
RELU STE,0.19133192389006343,"v = ¯v = 2ˆσ|w∗|
√ 2π"
RELU STE,0.19238900634249473,"
cos ϕIm×m + 1−cos ϕ"
RELU STE,0.193446088794926,"m+1 1m1T
m

v∗."
RELU STE,0.1945031712473573,"2. ϕ = π, v = ¯v = 2ˆσ|w∗|
√ 2π"
RELU STE,0.19556025369978858,"
−Im×m +
2
m+11m1T
m

."
RELU STE,0.19661733615221988,"We ﬁnd the solutions are identical with two stationary points shown in item 1 and 2 in Sec.3.1,
leading to the the saddle point or local minimum, while the global minimum point at ϕ = 0, v = v0
cannot be obtained. Therefore, by using the back-propagation with Relu STE, it always converges
to ϕ = π if (m + 1) (v∗)2 ≥2
 
v∗T 1m
2 , and it does not converge to any points, otherwise."
CLIPPED RELU STE,0.19767441860465115,"3.2.3
CLIPPED RELU STE"
CLIPPED RELU STE,0.19873150105708245,The cRelu STE is given by
CLIPPED RELU STE,0.19978858350951373,"EZ [gcrelu] =
ˆσ
2
√"
CLIPPED RELU STE,0.20084566596194503,"2π
w
|w|"
CLIPPED RELU STE,0.20190274841437633,"
1 −e−1"
CLIPPED RELU STE,0.2029598308668076,"2(
r
ˆσ|w|)
2
vT  
Im×m + 1m1T
m

v −ˆσ2"
CLIPPED RELU STE,0.2040169133192389,"2π
|w∗| |w|"
CLIPPED RELU STE,0.20507399577167018,"
1 −e−1"
CLIPPED RELU STE,0.20613107822410148,"2(
r
ˆσ|w|)
2
w

vT  
−Im×m + 1m1T
m

v∗"
CLIPPED RELU STE,0.20718816067653276,"−ˆσ2

C(w, ϕ) w∗"
CLIPPED RELU STE,0.20824524312896406,"|w∗| + S(w, ϕ)

1
sin ϕ
w
|w| −cot ϕ w∗ |w∗|"
CLIPPED RELU STE,0.20930232558139536,"  
vT v∗
,
(27)"
CLIPPED RELU STE,0.21035940803382663,"where C(|w|, ϕ) and S(|w|, ϕ) are given in Eq.(106).
At ϕ = 0, π they are simpliﬁed as"
CLIPPED RELU STE,0.21141649048625794,"C(|w|, 0) =
|w∗|"
CLIPPED RELU STE,0.2124735729386892,"2π

π −
r
ˆσ|w|e−1"
CLIPPED RELU STE,0.2135306553911205,"2(
r
ˆσ|w|)
2√"
CLIPPED RELU STE,0.21458773784355178,"2π −πerfc

r
√"
CLIPPED RELU STE,0.2156448202959831,2ˆσ|w|
CLIPPED RELU STE,0.2167019027484144,"
, C(|w|, π) = S(|w|, π) ="
CLIPPED RELU STE,0.21775898520084566,"S(|w|, 0) = 0."
CLIPPED RELU STE,0.21881606765327696,"Remarkably, the cRelu STE gradient is proportional to the Relu STE gradient in the case of vT v∗=
0 or ϕ = π:"
CLIPPED RELU STE,0.21987315010570824,"EZ [gcrelu] |vT v∗=0 or ϕ=π =

1 −e−1"
CLIPPED RELU STE,0.22093023255813954,"2(
r
ˆσ|w|)
2
EZ [grelu] |vT v∗=0 or ϕ=π.
(28)"
CLIPPED RELU STE,0.2219873150105708,Under review as a conference paper at ICLR 2022
CLIPPED RELU STE,0.22304439746300211,"As shown in Sec.3.2.2, all the vanishing points in the Relu STE can be found at vT v∗= 0 or ϕ = π,
leading to the saddle point or the local minimum, so that they are also found in the vanishing points
of cRelu STE. On the other hand, the behavior of cRelu STE gradient at ϕ = 0 is not related to that
of Relu STE gradient. The cRelu STE gradient at ϕ = 0 is written as"
CLIPPED RELU STE,0.22410147991543342,"EZ [gcrelu] =
ˆσ
2
√"
CLIPPED RELU STE,0.2251585623678647,"2π w∗
1 −e−1"
CLIPPED RELU STE,0.226215644820296,"2(
r
σ|w|)
2
vT

1
|w∗|
 
Im×m + 1m1T
m

v"
CLIPPED RELU STE,0.22727272727272727,"−2ˆσ
√ 2π"
CLIPPED RELU STE,0.22832980972515857," 
(−1 + λ) Im×m + 1m1T
m

v∗

,
(29) where"
CLIPPED RELU STE,0.22938689217758984,"λ = λ(|w|) =
π −
r
ˆσ|w|e−1"
CLIPPED RELU STE,0.23044397463002114,"2(
r
ˆσ|w|)
2√"
CLIPPED RELU STE,0.23150105708245244,"2π −πerfc

r
√"
CLIPPED RELU STE,0.23255813953488372,2ˆσ|w| 
CLIPPED RELU STE,0.23361522198731502,1 −e−1
CLIPPED RELU STE,0.2346723044397463,"2(
r
ˆσ|w|)
2
.
(30)"
CLIPPED RELU STE,0.2357293868921776,This becomes zero if
CLIPPED RELU STE,0.23678646934460887,"v =
2ˆσ
√"
CLIPPED RELU STE,0.23784355179704017,"2π |w∗|
 
Im×m + 1m1T
m
−1  
(−1 + λ) Im×m + 1m1T
m

v∗"
CLIPPED RELU STE,0.23890063424947147,"=
2ˆσ
√"
CLIPPED RELU STE,0.23995771670190275,"2π |w∗|

(−1 + λ) Im×m + 2 −λ"
CLIPPED RELU STE,0.24101479915433405,"m + 11m1T
m"
CLIPPED RELU STE,0.24207188160676532,"
v∗.
(31)"
CLIPPED RELU STE,0.24312896405919662,"To be consistent with Eq.(21) at ϕ = 0, only λ = 2 is allowed. The solution provides the global
minimum shown in item 3 in Sec.3.1."
CLIPPED RELU STE,0.2441860465116279,"In fact, λ monotonically increases from 0 to π as
r
ˆσ|w| increases, and thus the solution at λ =
2, corresponding to the global minimum point, can be obtained for any clipping value r and the
variance ˆσ2 by changing the scale |w|. Consequently, we get all the stationary points are found in
cRelu STE:"
CLIPPED RELU STE,0.2452431289640592,"1. If v∗satisﬁes (m + 1) (v∗)2 ≥2
 
v∗T 1m
2 , ϕ = ¯ϕ = arccos

(v∗T 1m)
2"
CLIPPED RELU STE,0.2463002114164905,"−(m+1)(v∗)2+(v∗T 1m)2 
,"
CLIPPED RELU STE,0.24735729386892177,"v = ¯v = 2ˆσ|w∗|
√ 2π"
CLIPPED RELU STE,0.24841437632135308,"
cos ϕIm×m + 1−cos ϕ"
CLIPPED RELU STE,0.24947145877378435,"m+1 1m1T
m

v∗."
CLIPPED RELU STE,0.25052854122621565,"2. ϕ = π, v = ¯v = 2ˆσ|w∗|
√ 2π"
CLIPPED RELU STE,0.25158562367864695,"
−Im×m +
2
m+11m1T
m

."
CLIPPED RELU STE,0.2526427061310782,"3. ϕ = 0, |w| = w0 ≡r"
CLIPPED RELU STE,0.2536997885835095,"ˆσc0, v = v0 ≡2ˆσ|w∗|
√"
CLIPPED RELU STE,0.2547568710359408,"2π v∗, where w0 (or c0) is given by λ(w0) = 2."
CLIPPED RELU STE,0.2558139534883721,"Note that while the global minimum point is degenerated in scales for w, i.e. if (w, v) = (w0, v0)
gives the global minimum, (w, v) = (kw0, v0) for any k > 0 also gives the global minimum, the
cRelu STE picks up the point at the scale determined by λ = 2."
EXPERIMENTS,0.2568710359408034,"4
EXPERIMENTS"
EXPERIMENTS,0.25792811839323465,"To conﬁrm our conjecture in more general case, we have numerically studied the Gaussian mixture
input with various mean values. As teacher networks, we have tested tanh-type and sin-type as
well as Relu-type. For all the examined setups, cRelu STE behaves like the population gradient,
while id/Relu STEs show qualitatively different behaviors as described below. To calculate the
population loss and STE gradients, we have generated the mixture Gaussian samples and have taken
their average. The population gradient has been obtained by calculating the ﬁnite difference of the
population loss. We have demonstrated the back propagation with learning rate η = 0.01 given in
Algorithm 1."
EXPERIMENTS,0.25898520084566595,"Shown in Fig. 1 are the results of ten mixture Gaussian input with random mean values for each
components of Z. We employed m = 20, n = 25, and the tanh-type teacher network. We ﬁnd
that the population gradient and cRelu STE show similar results, while id/Relu STEs are completely
different. This indicates cRelu STE is less biased than id/Relu STEs."
EXPERIMENTS,0.26004228329809725,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.26109936575052856,"In the case of id/Relu STEs, at early steps up to around 500 step, |w| decreases around the magnitude
of the update quantity, so that it begins to oscillate. Then it escapes the oscillation, and the loss
function shows the convergence to a point different from the local minimum achieved by population
gradient, while |w| becomes larger and larger due to their scale invariance. Interestingly, the values
of the loss function are small compared to the one obtained by population gradient, which implies
that id/Relu STEs avoid being trapped in the local solution due to their large bias. However, note
that even at that point, the magnitude of w continues to grow and eventually become numerically
unstable."
EXPERIMENTS,0.26215644820295986,"Figure 1: Numerical results of the back-propagation by population gradient and three STEs. We
generate 10000 samples which follow ten mixture Gaussian input with random mean values. We
employ the tanh-type teacher network."
SUMMARY,0.2632135306553911,"5
SUMMARY"
SUMMARY,0.2642706131078224,"We have found that breaking symmetry embedded in the network by STEs enhances the possibility
of convergence to the true (local) minimum of the loss function. We have demonstrated that if an
STE breaks the scale symmetry embedded in the one-hidden-layer network with a binary activation,
it is more likely to achieve the local minimum than the one which keeps the symmetry. The discus-
sion can be generalized to the network with more symmetries in a straightforward way. The more
symmetries embedded in the network an STE breaks, the more likely it is to converge. To conﬁrm
the mechanism, we have studied three STEs, identity STE, Relu STE, and cRelu STE, in a sim-
ple misspeciﬁed model with Gaussian input. We have found that the back-propagation by the cRelu
STE, which breaks the scale symmetry, can converge to the global minimum, while identy/Relu STE
cannot. Finally we have numerically conﬁrmed the mechanism for the mixture Gaussian model with
various teacher network."
REFERENCES,0.2653276955602537,REFERENCES
REFERENCES,0.266384778012685,"Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of
convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018."
REFERENCES,0.26744186046511625,"Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.26849894291754756,Under review as a conference paper at ICLR 2022
REFERENCES,0.26955602536997886,"Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving
low-bit quantization through learnable offsets and better initialization.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696–697,
2020."
REFERENCES,0.27061310782241016,"Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao, and Lawrence
Carin.
Straight-through estimator as projected wasserstein gradient ﬂow.
arXiv preprint
arXiv:1910.02176, 2019."
REFERENCES,0.27167019027484146,"Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018."
REFERENCES,0.2727272727272727,"Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net-
works for efﬁcient inference. In ICCV Workshops, pp. 3009–3018, 2019."
REFERENCES,0.273784355179704,"Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on
Machine Learning, pp. 1339–1348. PMLR, 2018."
REFERENCES,0.2748414376321353,"Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019."
REFERENCES,0.2758985200845666,"Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky.
Neural networks for machine learning.
Coursera, video lectures, 264(1):2146–2153, 2012."
REFERENCES,0.2769556025369979,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Quan-
tized neural networks: Training neural networks with low precision weights and activations. The
Journal of Machine Learning Research, 18(1):6869–6898, 2017."
REFERENCES,0.27801268498942916,"Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A
whitepaper. arXiv preprint arXiv:1806.08342, 2018."
REFERENCES,0.27906976744186046,"Vladimir Kryzhanovskiy, Gleb Balitskiy, Nikolay Kozyrskiy, and Aleksandr Zuruev. Qpp: Real-
time quantization parameter prediction for deep neural networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 10684–10692, 2021."
REFERENCES,0.28012684989429176,"Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
preprint arXiv:2012.04728, 2020."
REFERENCES,0.28118393234672306,"Ziang Long, Penghang Yin, and Jack Xin. Learning quantized neural nets by coarse gradient method
for nonlinear classiﬁcation. Research in the Mathematical Sciences, 8(3):1–19, 2021."
REFERENCES,0.2822410147991543,"Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European conference on computer
vision, pp. 525–542. Springer, 2016."
REFERENCES,0.2832980972515856,"Alexander Shekhovtsov and Viktor Yanush. Reintroducing straight-through estimators as principled
methods for stochastic binary networks. arXiv preprint arXiv:2006.06880, 2020."
REFERENCES,0.2843551797040169,"Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. arXiv preprint
arXiv:1903.05662, 2019."
REFERENCES,0.2854122621564482,"Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network
quantization without retraining using outlier channel splitting. In International conference on
machine learning, pp. 7543–7552. PMLR, 2019."
REFERENCES,0.2864693446088795,"Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients.
arXiv preprint
arXiv:1606.06160, 2016."
REFERENCES,0.28752642706131076,Under review as a conference paper at ICLR 2022
REFERENCES,0.28858350951374206,"A
DERIVATION OF SEC.3"
REFERENCES,0.28964059196617337,"In the Appendix, we provide the results of the Gaussian with variance 1, N(0, 1). For some function
f(Z), the expectation value of the Gaussian with the generic variance ˆσ, N(0, ˆσ2), shown in Sec.3
is written as"
REFERENCES,0.29069767441860467,"EZ∼N(0,σ2)[f(Z)] = r"
REFERENCES,0.2917547568710359,"1
2πˆσ2 Z Y"
REFERENCES,0.2928118393234672,"i,j
dZijf(Z)e−"
REFERENCES,0.2938689217758985,"P
i,j Z2
ij
2ˆσ2 = r 1
2π Z Y"
REFERENCES,0.2949260042283298,"i,j
dZ′
ijf(ˆσZ′)e−"
REFERENCES,0.2959830866807611,"P
i,j Z
′2
ij
2"
REFERENCES,0.29704016913319237,"= EZ∼N(0,1)[f(ˆσZ)]
(32)"
REFERENCES,0.29809725158562367,"where we have changed the integral variables from Z to Z′ = Z/σ. Therefore we get the results
with the variance ˆσ by only changing from f(Z) to f(ˆσZ)."
REFERENCES,0.29915433403805497,"A.1
DERIVATION OF LOSS FUNCTION"
REFERENCES,0.30021141649048627,The expectaion of the loss is divided into the following three terms:
REFERENCES,0.3012684989429176,"L(v, w; v∗, w∗) =1"
EZ,0.3023255813953488,"2EZ

vT σ(Zw)vT σ(Zw)

−EZ

vT σ(Zw)v∗T fRelu(Zw∗)
 + 1"
EZ,0.3033826638477801,"2EZ

v∗T fRelu(Zw∗)v∗T fRelu(Zw∗)

(33)"
EZ,0.3044397463002114,"The calculation of the ﬁrst term and the third term are given in “Proof of Lemma 1” in Yin et al.
(2019) and “Proof of Section 3” in Ref. Du et al. (2018), respectively:"
EZ,0.3054968287526427,"1
2EZ

vT σ(Zw)vT σ(Zw)

+ 1"
EZ,0.30655391120507397,"2EZ

v∗T fRelu(Zw∗)v∗T fRelu(Zw∗)
 = 1"
EZ,0.30761099365750527,"8vT  
Im×m + 1m1T
m

v + 1"
EZ,0.3086680761099366,"4π (w∗)2v∗T  
(π −1)Im×m + 1m1T
m

v∗
(34)"
EZ,0.3097251585623679,"To discuss the second term, we evaluate the following quantity:"
EZ,0.3107822410147992,Fij = EZ [σ(Zw)fRelu(Zw∗)]ij
EZ,0.3118393234672304,"=

1
√ 2π"
EZ,0.3128964059196617,mn Z  Y
EZ,0.313953488372093,"k,l
dZkl  e−1"
P,0.3150105708245243,"2
P"
P,0.31606765327695563,"k,l Z2
klσ X"
P,0.3171247357293869,"m′
Zim′wm′ !  X"
P,0.3181818181818182,"m′
Zjm′w∗
m′ ! σ X"
P,0.3192389006342495,"m′
Zjm′w∗
m′ ! (35)"
P,0.3202959830866808,(i) i ̸= j case:
P,0.321353065539112,"Fij =

1
√ 2π"
P,0.3224101479915433,2n Z  Y
P,0.32346723044397463,"l
dZildZjl ! e−1"
P,0.32452431289640593,"2
P"
P,0.32558139534883723,"l(Z2
il+Z2
jl)σ X"
P,0.3266384778012685,"m′
Zim′wm′ !  X"
P,0.3276955602536998,"m′
Zjm′w∗
m′ ! σ X"
P,0.3287526427061311,"m′
Zjm′w∗
m′ !"
P,0.3298097251585624,"=

1
√ 2π"
P,0.3308668076109937,2n Z  Y
P,0.33192389006342493,"l
d ˜Zild ˜Zjl ! e−1"
P,0.33298097251585623,"2
P"
P,0.33403805496828753,"l( ˜
Z2
il+ ˜
Z2
jl)σ

˜Zi1|w|
 
˜Zj1|w∗|

σ

˜Zj1|w∗|
"
P,0.33509513742071884,"=

1
√ 2π"
P,0.3361522198731501,"2 Z
d ˜Zi1d ˜Zj1e−1"
P,0.3372093023255814,"2( ˜
Z2
i1+ ˜
Z2
j1)σ

˜Zi1
 
˜Zj1|w∗|

σ

˜Zj1
"
P,0.3382663847780127,"=
1
2
√"
P,0.339323467230444,"2π |w∗|,
(36)"
P,0.3403805496828753,where we used the orthogonal transformation in the second line:
P,0.34143763213530653,"˜Zil =
X"
P,0.34249471458773784,"m′
Oi
lm′Zim′,"
P,0.34355179704016914,"˜Zjl =
X"
P,0.34460887949260044,"m′
Oj
lm′Zjm′
(37)"
P,0.3456659619450317,Under review as a conference paper at ICLR 2022
P,0.346723044397463,"with Oi
1m′ = wm′/|w| and Oj
1m′ = w∗
m′/|w∗|."
P,0.3477801268498943,(ii) i = j case:
P,0.3488372093023256,"Fii =

1
√ 2π"
P,0.3498942917547569,n Z Y
P,0.35095137420718814,"l
dZile−1"
P,0.35200845665961944,"2
P"
P,0.35306553911205074,"l Z2
ilσ X"
P,0.35412262156448204,"m
Zimwm !  X"
P,0.35517970401691334,"m
Zimw∗
m ! σ X"
P,0.3562367864693446,"m
Zimw∗
m !"
P,0.3572938689217759,. (38)
P,0.3583509513742072,"Without loss of generality, we can choose"
P,0.3594080338266385,"w∗= (w∗, 0n−1),
w = (w1, w2, 0n−2)
(39)"
P,0.36046511627906974,"with w∗> 0, so that this can be written as"
P,0.36152219873150104,"Fii =

1
√ 2π"
P,0.36257928118393234,"2 Z
dZi1dZi2e−1"
P,0.36363636363636365,"2(Z2
i1+Z2
i2)σ"
X,0.36469344608879495,"2
X"
X,0.3657505285412262,"m=1
Zimwm !"
X,0.3668076109936575,Zi1w∗σ (Zi1)
X,0.3678646934460888,"=

1
√ 2π"
X,0.3689217758985201,"2 Z
dZdθZe−1"
X,0.3699788583509514,2 Z2σ (cos(ϕ −θ)) Z cos θw∗σ (cos θ) = 1 2π Z ∞
DZ,0.37103594080338265,"0
dZ
Z π/2"
DZ,0.37209302325581395,"ϕ−π/2
dθZ2e−1"
DZ,0.37315010570824525,2 Z2 cos θw∗
DZ,0.37420718816067655,"=
1
2
√"
DZ,0.3752642706131078,"2π w∗(1 + cos ϕ)
(40)"
DZ,0.3763213530655391,where ϕ is the angle between w∗and w. Eqs.(36) and (40) are combined as
DZ,0.3773784355179704,"Fij =
1
2
√"
DZ,0.3784355179704017,"2π |w∗|(1 + cos ϕ)δij +
1
2
√"
DZ,0.379492600422833,"2π |w∗|(1 −δij),
(41)"
DZ,0.38054968287526425,so that the second term in Eq. (33) is expressed as
DZ,0.38160676532769555,"EZ

vT σ(Zw)v∗T fRelu(Zw∗)

= |w∗| 2
√"
DZ,0.38266384778012685,"2π vT  
cos ϕIm×m + 1m1T
m

v∗.
(42)"
DZ,0.38372093023255816,"Consequently, the loss function is summarized as"
DZ,0.38477801268498946,"L(v, w; v∗, w∗) = 1"
DZ,0.3858350951374207,"8vT  
Im×m + 1m1T
m

v −|w∗| 2
√"
DZ,0.386892177589852,"2π vT  
cos ϕIm×m + 1m1T
m

v∗ + 1"
DZ,0.3879492600422833,"4π |w∗|2v∗T  
(π −1)Im×m + 1m1T
m

v∗.
(43)"
DZ,0.3890063424947146,The population gradient w.r.t v and w is thus given by
DZ,0.39006342494714585,"∂L
∂v = 1"
DZ,0.39112050739957716,"4
 
Im×m + 1m1T
m

v −ˆσ|w∗| 2
√ 2π"
DZ,0.39217758985200846," 
cos ϕIm×m + 1m1T
m

v∗,
(44)"
DZ,0.39323467230443976,"∂L
∂w =
1
|w|
∂L
∂ϕeϕ =
ˆσ|w∗|
2
√"
DZ,0.39429175475687106,"2π|w| sin ϕ
 
vT v∗
eϕ,
(45)"
DZ,0.3953488372093023,"where without loss of generality,
we can take ew
=
(cos ϕ, sin ϕ, 0n−2),
eϕ
=
(−sin ϕ, cos ϕ, 0n−2). Note that ew · ∂L"
DZ,0.3964059196617336,∂w = 0 is satisﬁed due to the scale symmetry for the loss.
DZ,0.3974630021141649,"A.2
CLASSIFICATION OF STATIONARY POINTS"
DZ,0.3985200845665962,"Using Eq.(21), the loss at the three stationary points is rewritten as"
DZ,0.39957716701902746,L = −|w∗|2
DZ,0.40063424947145876,4π v∗T
DZ,0.40169133192389006,cos2 ϕIm×m +
DZ,0.40274841437632136,−(cos ϕ −1)2
DZ,0.40380549682875266,"m + 1
+ 1 !"
DZ,0.4048625792811839,"1m1T
m ! v∗"
DZ,0.4059196617336152,+ |w∗|2
DZ,0.4069767441860465,"2π v∗T  
(π −1) Im×m + 1m1T
m

v∗.
(46)"
DZ,0.4080338266384778,Under review as a conference paper at ICLR 2022
DZ,0.4090909090909091,"Obviously, this shows that ϕ = 0 is the global minimum point,"
DZ,0.41014799154334036,L ≥L|ϕ=0 = |w∗|2
DZ,0.41120507399577166,"4π (π −2)v∗T v.
(47)"
DZ,0.41226215644820297,"At ϕ = π, this becomes"
DZ,0.41331923890063427,L|ϕ=π = −|w∗|2
DZ,0.4143763213530655,"4π v∗T

Im×m +
 −4"
DZ,0.4154334038054968,"m + 1 + 1

1m1T
m 
v∗"
DZ,0.4164904862579281,+ |w∗|2
DZ,0.4175475687103594,"4π v∗T  
(π −1) Im×m + 1m1T
m

v∗"
DZ,0.4186046511627907,= |w∗|2
DZ,0.41966173361522197,"4π v∗T

(π −2) Im×m +
4
m + 11m1T
m"
DZ,0.42071881606765327,"
v∗
(48)"
DZ,0.42177589852008457,"At ϕ = ¯ϕ = arccos

(v∗T 1m)
2"
DZ,0.42283298097251587,−(m+1)(v∗)2+(v∗T 1m)2
DZ,0.42389006342494717,"
, this becomes"
DZ,0.4249471458773784,L|ϕ= ¯ϕ = −|w∗|2
DZ,0.4260042283298097,4π v∗T  
DZ,0.427061310782241,"(
 
v∗T 1m
2"
DZ,0.4281183932346723,−(m + 1) (v∗)2 + (v∗T 1m)2 )2 Im×m + 
DZ,0.42917547568710357,−(m + 1)
DZ,0.43023255813953487,"(
(v∗)2"
DZ,0.4312896405919662,−(m + 1) (v∗)2 + (v∗T 1m)2 )2 + 1 
DZ,0.4323467230443975,"1m1T
m  v∗"
DZ,0.4334038054968288,+ |w∗|2
DZ,0.43446088794926,"4π v∗T  
(π −1) Im×m + 1m1T
m

v∗"
DZ,0.4355179704016913,= −|w∗|2 4π
DZ,0.4365750528541226,"(
 
v∗T 1m
2 (v∗)2"
DZ,0.4376321353065539,"−(m + 1) (v∗)2 + (v∗T 1m)2 +
 
v∗T 1m
2
)"
DZ,0.43868921775898523,+ |w∗|2
DZ,0.4397463002114165,"4π v∗T  
(π −1) Im×m + 1m1T
m

v∗.
(49)"
DZ,0.4408033826638478,"Using these results, we obtain"
DZ,0.4418604651162791,L|ϕ= ¯ϕ −L|ϕ=π
DZ,0.4429175475687104,= −|w∗|2 4π
DZ,0.4439746300211416," 
v∗T 1m
2 (v∗)2"
DZ,0.4450317124735729,−(m + 1) (v∗)2 + (v∗T 1m)2 + |w∗|2
DZ,0.44608879492600423,"4π
(v∗)2 −|w∗|2"
DZ,0.44714587737843553,"4π
 
v∗T 1m
2
4
m + 1"
DZ,0.44820295983086683,= |w∗|2 4π
DZ,0.4492600422832981,"n
(m + 1) (v∗)2 −2
 
v∗T 1m
2o2"
DZ,0.4503171247357294,"n
(m + 1) (v∗)2 −(v∗T 1m)2o
(m + 1)
.
(50)"
DZ,0.4513742071881607,"Note that because (m + 1) (v∗)2 ≥2
 
v∗T 1m
2 is satisﬁed to exist the stationary point at ϕ = ¯ϕ,
the loss at ϕ = π is smaller than the one at at ϕ = ¯ϕ,"
DZ,0.452431289640592,"L|v=¯v,ϕ= ¯ϕ ≥L|v=vπ,ϕ=π.
(51)"
DZ,0.45348837209302323,"is satisﬁed. When (m + 1) (v∗)2 = 2
 
v∗T 1m
2, L|ϕ= ¯ϕ = L|ϕ=π, because the middle stationary
point at ϕ = ¯ϕ is merged into the point at ϕ = π."
DZ,0.45454545454545453,"This implies that ϕ = 0 is the global minimum, ϕ = π is the local minimum, and ϕ = ¯ϕ is the
saddle point or the local maximum if (m + 1) (v∗)2 ≥2
 
v∗T 1m
2."
DZ,0.45560253699788583,"To fully clarify whether the stationary points are local minimum, maximum or saddle point, we have
calculated the hessian matrix, H ="
DZ,0.45665961945031713,"∂2L
∂v2
∂2L
∂v∂ϕ
∂2L
∂ϕ∂v
∂2L
∂ϕ2 ! = 1"
DZ,0.45771670190274844,"4
 
Im×m + 1m1T
m

|w∗|
2
√"
DZ,0.4587737843551797,2π sin ϕv∗
DZ,0.459830866807611,"|w∗|
2
√"
DZ,0.4608879492600423,"2π sin ϕv∗T
|w∗|
2
√"
DZ,0.4619450317124736,2πvT v∗cos ϕ !
DZ,0.4630021141649049,".
(52)"
DZ,0.46405919661733613,Under review as a conference paper at ICLR 2022
DZ,0.46511627906976744,"(i) v = ¯v, ϕ = ¯ϕ:"
DZ,0.46617336152219874,"For z = (x, y)T (x ∈Rm, y ∈R),"
DZ,0.46723044397463004,zT Hz = 1
DZ,0.4682875264270613,"4xT  
Im×m + 1m1T
m

x + |w∗|
√"
DZ,0.4693446088794926,"2π y sin ¯ϕ
 
xT v∗ = 1"
DZ,0.4704016913319239,"4
 
xT 1m
2 +
1"
DZ,0.4714587737843552,"2x + |w∗|y sin ¯ϕ
√"
DZ,0.4725158562367865,"2π
v∗
2
−|w∗|2y2 sin2 ¯ϕ (v∗)2"
DZ,0.47357293868921774,"2π
,
(53)"
DZ,0.47463002114164904,"where we used ¯vT v∗= 0. Therefore, depending on x and y, the Hessian can be either positive or
negative, which means the stationary point is a saddle point."
DZ,0.47568710359408034,"(ii) v = vπ ≡2|w∗|
√ 2π"
DZ,0.47674418604651164,"
−Im×m +
2
m+11m1T
m

v∗, ϕ = π H = 1"
DZ,0.47780126849894294,"4
 
Im×m + 1m1T
m

0
0
−|w∗| 2
√"
DZ,0.4788583509513742,"2πvT
π v∗ !"
DZ,0.4799154334038055,".
(54)"
DZ,0.4809725158562368,"Note that vT
π v∗= 2|w∗|
√"
DZ,0.4820295983086681,"2π v∗T 
−Im×m +
2
m+11m1T
m

v∗. If (m + 1) (v∗)2 ≥2
 
v∗T 1m
2, then"
DZ,0.48308668076109934,"vT
π v∗≤0, so that the matrix becomes a positive deﬁnite matrix: For any x ∈Rm, y ∈R, this
satisﬁes"
DZ,0.48414376321353064,"1
4xT  
Im×m + 1m1T
m

x −|w∗| 2
√"
DZ,0.48520084566596194,"2π vT
π v∗y2 ≥0,
(55)"
DZ,0.48625792811839325,"which means the stationary point is the local minimum. On the other hand, if (m + 1) (v∗)2 ≤
2
 
v∗T 1m
2, the matrix becomes either positive or negative, depending on x and y, so that this
stationary point becomes a saddle point."
DZ,0.48731501057082455,"(iii) v = v0 ≡2|w∗|
√"
DZ,0.4883720930232558,"2π v∗, ϕ = 0 H = 1"
DZ,0.4894291754756871,"4
 
Im×m + 1m1T
m

0
0
|w∗|
2
√"
DZ,0.4904862579281184,"2πvT
0 v∗ !"
DZ,0.4915433403805497,".
(56)"
DZ,0.492600422832981,"The Hessian becomes a positive deﬁnite matrix: For any x ∈Rm, y ∈R, this satisﬁes"
DZ,0.49365750528541225,"1
4xT  
Im×m + 1m1T
m

x + |w∗| 2
√"
DZ,0.49471458773784355,"2π vT
0 v∗y2 ≥0.
(57)"
DZ,0.49577167019027485,"A.3
DERIVATION OF IDENTITY STE GRADIENT"
DZ,0.49682875264270615,"By substituting µ′(x) = 1 into Eq.(23), we obtain"
DZ,0.4978858350951374,"EZ [g]i =

1
√ 2π"
DZ,0.4989429175475687,mn Z  Y
DZ,0.5,"k,l
dZkl  e−1"
P,0.5010570824524313,"2
P"
P,0.5021141649048626,"k,l Z2
kl  X"
P,0.5031712473572939,"j
Zjivj   × X k
vkσ X"
P,0.5042283298097252,"l
Zklwl ! −
X"
P,0.5052854122621564,"k
v∗
kfRelu X"
P,0.5063424947145877,"l
Zklw∗
l !!"
P,0.507399577167019,".
(58)"
P,0.5084566596194503,We calculate each term as follows.
P,0.5095137420718816,"A.3.1
CALCULATION OF THE FIRST TERM"
P,0.5105708245243129,We calculate the ﬁrst term as was done in Lemma 9 (and also Lemma 11) in Yin et al. (2019).
P,0.5116279069767442,"To discuss the ﬁrst term, we evaluate"
P,0.5126849894291755,"F id
ijk ≡EZ [Zji [σ(Zw)]k] .
(59)"
P,0.5137420718816068,Under review as a conference paper at ICLR 2022
P,0.514799154334038,"Using F id
ijk, the ﬁrst term can be expressed as"
P,0.5158562367864693,"ﬁrst term =
X"
P,0.5169133192389006,"j,k
F id
ijkvjvk.
(60)"
P,0.5179704016913319,(i) j ̸= k case:
P,0.5190274841437632,"F id
ijk = 0,
(61)"
P,0.5200845665961945,because of the odd symmetry for i-th and j-th components in the integrand.
P,0.5211416490486258,(ii) j = k case:
P,0.5221987315010571,"F id
ijj =

1
√ 2π"
P,0.5232558139534884,n Z  Y
P,0.5243128964059197,"l
dZjl ! e−1"
P,0.5253699788583509,"2
P"
P,0.5264270613107822,"l Z2
jlZjiσ X"
P,0.5274841437632135,"l
Zjlwl !"
P,0.5285412262156448,"=

1
√ 2π"
P,0.5295983086680761,n Z  Y
P,0.5306553911205074,"l
d ˜Zjl ! e−1"
P,0.5317124735729387,"2
P"
P,0.53276955602537,"l ˜
Z2
jl
 X"
P,0.5338266384778013,"m′
Oj T
im′ ˜Zjm′ !"
P,0.5348837209302325,"σ

˜Zj1
"
P,0.5359408033826638,"=

1
√ 2π"
P,0.5369978858350951,n Z  Y
P,0.5380549682875264,"l
d ˜Zjl ! e−1"
P,0.5391120507399577,"2
P"
P,0.540169133192389,"l ˜
Z2
jlOj T
i1
˜Zj1σ

˜Zj1
"
P,0.5412262156448203,"=

1
√ 2π"
P,0.5422832980972516," Z
d ˜Zj1e−1"
P,0.5433403805496829,"2 ˜
Z2
j1Oj T
i1
˜Zj1σ

˜Zj1
"
P,0.5443974630021141,"=

1
√ 2π"
P,0.5454545454545454,"
Oj T
i1
=

1
√ 2π"
P,0.5465116279069767,"
Oj
1i =
1
√"
P,0.547568710359408,"2π
wi
|w|
(62)"
P,0.5486257928118393,where we used the orthogonal transformation in the second line:
P,0.5496828752642706,"˜Zjl =
X"
P,0.5507399577167019,"m′
Oj
lm′Zjm′
(63)"
P,0.5517970401691332,"with Oi
1m′ = wm′/|w|."
P,0.5528541226215645,The ﬁrst term is reduced to
P,0.5539112050739958,"ﬁrst term =
1
√"
P,0.554968287526427,"2π
wi
|w|
 
vT v

.
(64)"
P,0.5560253699788583,"A.3.2
CALCULATION OF THE SECOND TERM"
P,0.5570824524312896,"To discuss the second term, we evaluate"
P,0.5581395348837209,"Gid
ijk = EZ [Zji [fRelu(Zw∗)]k] .
(65)"
P,0.5591966173361522,"Using Gid
ijk, the second term can be expressed as"
P,0.5602536997885835,"second term =
X"
P,0.5613107822410148,"j,k
Gid
ijkvjv∗
k.
(66)"
P,0.5623678646934461,(i) j ̸= k case:
P,0.5634249471458774,"Gid
ijk = 0,
(67)"
P,0.5644820295983086,because of the odd symmetry.
P,0.5655391120507399,Under review as a conference paper at ICLR 2022
P,0.5665961945031712,(ii) j = k case:
P,0.5676532769556025,"Gid
ijj =

1
√ 2π"
P,0.5687103594080338,n Z  Y
P,0.5697674418604651,"l
dZjl ! e−1"
P,0.5708245243128964,"2
P"
P,0.5718816067653277,"l Z2
jlZjifRelu X"
P,0.572938689217759,"l
Zjlw∗
l !"
P,0.5739957716701902,"=

1
√ 2π"
P,0.5750528541226215,n Z  Y
P,0.5761099365750528,"l
d ˜Zjl ! e−1"
P,0.5771670190274841,"2
P"
P,0.5782241014799154,"l ˜
Z2
jl
 X"
P,0.5792811839323467,"m′
Oj T
im′ ˜Zjm′ !"
P,0.580338266384778,"fRelu

|w∗| ˜Zj1
"
P,0.5813953488372093,"=

1
√ 2π"
P,0.5824524312896406,n Z  Y
P,0.5835095137420718,"l
d ˜Zjl ! e−1"
P,0.5845665961945031,"2
P"
P,0.5856236786469344,"l ˜
Z2
jlOj T
i1
˜Zj1fRelu

|w∗| ˜Zj1
"
P,0.5866807610993657,"=

1
√ 2π"
P,0.587737843551797," Z
d ˜Zj1e−1"
P,0.5887949260042283,"2 ˜
Z2
j1Oj T
i1
˜Zj1fRelu

|w∗| ˜Zj1
"
P,0.5898520084566596,"=

1
√ 2π"
P,0.5909090909090909," Z
d ˜Zj1e−1"
P,0.5919661733615222,"2 ˜
Z2
j1Oj T
i1
˜Z2
j1|w∗|σ

|w∗| ˜Zj1
"
P,0.5930232558139535,= |w∗|
OJ T,0.5940803382663847,"2
Oj T
i1
= |w∗|"
OJ,0.595137420718816,"2
Oj
1i = 1"
OJ,0.5961945031712473,"2w∗
i
(68)"
OJ,0.5972515856236786,where we have used the orthogonal transformation in the second line:
OJ,0.5983086680761099,"˜Zjl =
X"
OJ,0.5993657505285412,"m′
Oj
lm′Zjm′
(69)"
OJ,0.6004228329809725,"with Oi
1m′ = w∗
m′/|w∗|, and also have used
Z ∞"
OJ,0.6014799154334038,"0
dxx2e−(1/2)x2 = √"
OJ,0.6025369978858351,"2π
2
.
(70)"
OJ,0.6035940803382663,The second term is written as
OJ,0.6046511627906976,second term = 1
WI,0.6057082452431289,"2wi
 
vT v∗
.
(71)"
WI,0.6067653276955602,"A.4
DERIVATION OF RELU STE GRADIENT"
WI,0.6078224101479915,"By substituting µ′(x) = σ(x) into Eq.(23), we obtain"
WI,0.6088794926004228,"EZ [grelu]i =

1
√ 2π"
WI,0.6099365750528541,mn Z  Y
WI,0.6109936575052854,"k,l
dZkl  e−1"
P,0.6120507399577167,"2
P"
P,0.6131078224101479,"k,l Z2
kl  X"
P,0.6141649048625792,"j
Zjivjσ X"
P,0.6152219873150105,"l
Zjlwl !  × X k
vkσ X"
P,0.6162790697674418,"l
Zklwl ! −
X"
P,0.6173361522198731,"k
v∗
kfRelu X"
P,0.6183932346723044,"l
Zklw∗
l !!"
P,0.6194503171247357,".
(72)"
P,0.620507399577167,We calculate each term as follows.
P,0.6215644820295984,"A.4.1
CALCULATION OF THE FIRST TERM"
P,0.6226215644820295,"We calculate the ﬁrst term as was done in Lemma9 (and also Lemma11) in Yin et al. (2019). We
evaluate"
P,0.6236786469344608,"F relu
ijk = EZ
h
Zji [σ(Zw)]j [σ(Zw)]k
i
.
(73)"
P,0.6247357293868921,"Using F relu
ijk , the ﬁrst term can be expressed as"
P,0.6257928118393234,"ﬁrst term =
X"
P,0.6268498942917548,"j,k
F relu
ijk vjvk.
(74)"
P,0.627906976744186,Under review as a conference paper at ICLR 2022
P,0.6289640591966174,(i) j ̸= k case:
P,0.6300211416490487,"F relu
ijk =

1
√ 2π"
P,0.63107822410148,2n Z  Y
P,0.6321353065539113,"l
dZjldZkl ! e−1"
P,0.6331923890063424,"2
P"
P,0.6342494714587738,"l Z2
jl+Z2
klZjiσ X"
P,0.635306553911205,"l
Zjlwl ! σ X"
P,0.6363636363636364,"l
Zklwl !"
P,0.6374207188160677,"=

1
√ 2π"
P,0.638477801268499,2n Z  Y
P,0.6395348837209303,"l
d ˜Zjld ˜Zkl ! e−1"
P,0.6405919661733616,"2
P"
P,0.6416490486257929,"l ˜
Z2
jl+ ˜
Z2
kl
 X"
P,0.642706131078224,"m′
Oj T
im′ ˜Zjm′ !"
P,0.6437632135306554,"σ

˜Zj1

σ

˜Zk1
"
P,0.6448202959830867,"=

1
√ 2π"
P,0.645877378435518,"2 Z
d ˜Zj1d ˜Zk1e−1"
P,0.6469344608879493,"2( ˜
Z2
j1+ ˜
Z2
k1)Oj T
i1
˜Zj1σ

˜Zj1

σ

˜Zk1
"
P,0.6479915433403806,"=
1
2
√"
P,0.6490486257928119,"2π Oj T
i1"
P,0.6501057082452432,"=
1
2
√"
P,0.6511627906976745,"2π
wi
|w|,
(75)"
P,0.6522198731501057,where we used the orthogonal transformation in the second line:
P,0.653276955602537,"˜Zjl =
X"
P,0.6543340380549683,"m′
Oj
lm′Zjm′
(76)"
P,0.6553911205073996,"with Oi
1m′ = wm′/|w|."
P,0.6564482029598309,(ii) j = k case:
P,0.6575052854122622,"We get the same expression given in Eq.(62),"
P,0.6585623678646935,"F relu
ijj
=

1
√ 2π"
P,0.6596194503171248,n Z  Y
P,0.6606765327695561,"l
dZjl ! e−1"
P,0.6617336152219874,"2
P"
P,0.6627906976744186,"l Z2
jlZjiσ X"
P,0.6638477801268499,"l
Zjlwl ! =
1
√"
P,0.6649048625792812,"2π
wi
|w|.
(77)"
P,0.6659619450317125,The ﬁrst term is written as
P,0.6670190274841438,"ﬁrst term =
1
2
√"
P,0.6680761099365751,"2π
wi
|w|vT  
Im×m + 1m1T
m

v.
(78)"
P,0.6691331923890064,"A.4.2
CALCULATION OF SECOND TERM"
P,0.6701902748414377,"To discuss the second term, we evaluate"
P,0.671247357293869,"Grelu
ijk = EZ
h
Zji
h
[σ(Zw)]j fRelu(Zw∗)
i k"
P,0.6723044397463002,"i
.
(79)"
P,0.6733615221987315,"Using Grelu
ijk , the second term can be expressed as"
P,0.6744186046511628,"second term =
X"
P,0.6754756871035941,"j,k
Grelu
ijk vjv∗
k.
(80)"
P,0.6765327695560254,(i) j ̸= k case:
P,0.6775898520084567,"Grelu
ijk =

1
√ 2π"
P,0.678646934460888,2n Z  Y
P,0.6797040169133193,"l
dZjldZkl ! e−1"
P,0.6807610993657506,"2
P"
P,0.6818181818181818,"l Z2
jl+Z2
klZjiσ X"
P,0.6828752642706131,"l
Zjlwl ! fRelu X"
P,0.6839323467230444,"l
Zklw∗
l !"
P,0.6849894291754757,"=

1
√ 2π"
P,0.686046511627907,2n Z  Y
P,0.6871035940803383,"l
d ˜Zjld ˜Zkl ! e−1"
P,0.6881606765327696,"2
P"
P,0.6892177589852009,"l ˜
Z2
jl+ ˜
Z2
kl
 X"
P,0.6902748414376322,"m′
Oj T
im′ ˜Zjm′ !"
P,0.6913319238900634,"σ

˜Zj1

fRelu

|w∗| ˜Zk1
"
P,0.6923890063424947,"=

1
√ 2π"
P,0.693446088794926,"2 Z
d ˜Zj1d ˜Zk1e−1"
P,0.6945031712473573,"2( ˜
Z2
j1+ ˜
Z2
k1)Oj T
i1
˜Zj1σ

˜Zj1

|w∗| ˜Zk1σ

˜Zk1
"
P,0.6955602536997886,= |w∗|
P,0.6966173361522199,"2π Oj T
i1 = 1"
P,0.6976744186046512,"2π
|w∗|"
P,0.6987315010570825,"|w| wi,
(81)"
P,0.6997885835095138,Under review as a conference paper at ICLR 2022
P,0.7008456659619451,"where we used the orthogonal transformation in the second line:
˜Zjl =
X"
P,0.7019027484143763,"m′
Oj
lm′Zjm′,"
P,0.7029598308668076,"˜Zkl =
X"
P,0.7040169133192389,"m′
Ok
lm′Zkm′
(82)"
P,0.7050739957716702,"with Oi
1m′ = wm′/|w| and Oj
1m′ = w∗
m′/|w∗|."
P,0.7061310782241015,(ii) j = k case:
P,0.7071881606765328,"Grelu
ijj =

1
√ 2π"
P,0.7082452431289641,m Z  Y
P,0.7093023255813954,"l
dZjl ! e−1"
P,0.7103594080338267,"2
P"
P,0.7114164904862579,"l Z2
jlZjiσ X"
P,0.7124735729386892,"l
Zjlwl ! fRelu X"
P,0.7135306553911205,"l
Zjlw∗
l !"
P,0.7145877378435518,"=

1
√ 2π"
P,0.7156448202959831,m Z  Y
P,0.7167019027484144,"l
dZjl ! e−1"
P,0.7177589852008457,"2
P"
P,0.718816067653277,"l Z2
jlZjiσ"
X,0.7198731501057083,"2
X"
X,0.7209302325581395,"l=1
Zjlwl !"
X,0.7219873150105708,fRelu (w∗Zj1) = 1 2π
X,0.7230443974630021,"Z  2
Y"
X,0.7241014799154334,"l=1
dZjl ! e−1"
X,0.7251585623678647,"2
P2
l=1 Z2
jlZji (δi1 + δi2) σ"
X,0.726215644820296,"2
X"
X,0.7272727272727273,"l=1
Zjlwl !"
X,0.7283298097251586,fRelu (w∗Zj1) = 1 2π
X,0.7293868921775899,"Z
dZdθZe−1"
X,0.7304439746300211,2 Z2 (Z cos θδi1 + Z sin θδi2) σ (Zw cos (θ −ϕ)) fRelu (w∗Z cos θ) = 1 2π
X,0.7315010570824524,"Z
dZdθZe−1"
X,0.7325581395348837,2 Z2 (Z cos θδi1 + Z sin θδi2) σ (Zw cos (θ −ϕ)) w∗Z cos θσ (cos θ) (83) = w∗ 2π Z ∞
X,0.733615221987315,"0
dZZ3e−1"
X,0.7346723044397463,2 Z2 Z π/2
X,0.7357293868921776,"−π/2+ϕ
dθ
 
cos2 θδi1 + cos θ sin θδi2
 = w∗ π"
X,0.7367864693446089,"sin 2ϕ 4
+ 1"
X,0.7378435517970402,"2 (π −ϕ)

δi1 + 1 −cos 2ϕ 4
δi2  = w∗ π 1"
X,0.7389006342494715,2 (π −ϕ) δi1 + sin ϕ
X,0.7399577167019028,"2
(cos ϕδi1 + sin ϕδi2)

(84)"
X,0.741014799154334,"where we have chosen
w∗= (w∗, 0n−1),
w = (w1, w2, 0n−2)
(85)
with w∗> 0 and ϕ is deﬁned as the angle between w∗and w. We also have used the formula,
Z ∞"
X,0.7420718816067653,"0
dxx3e−(1/2)x2 = 2,
Z
dθ cos2 θ = sin 2θ 4
+ 1"
X,0.7431289640591966,"2θ,
Z
dθ cos θ sin θ = −1"
X,0.7441860465116279,"4 cos 2θ.
(86)"
X,0.7452431289640592,"Note that
w∗
i
|w∗| = δi1,"
X,0.7463002114164905,"wi
|w| = cos ϕδi1 + sin ϕδi2.
(87)"
X,0.7473572938689218,This reduces to
X,0.7484143763213531,"Grelu
ijj = 1 π 1"
X,0.7494714587737844,"2 (π −ϕ) w∗
i + sin ϕ"
X,0.7505285412262156,"2
|w∗|"
X,0.7515856236786469,|w| wi
X,0.7526427061310782,"
(88)"
X,0.7536997885835095,"In summary, the second term is written as"
X,0.7547568710359408,second term = 1
X,0.7558139534883721,"2π
|w∗|"
X,0.7568710359408034,"|w| wi

vT  
−Im×m + 1m1T
m

v∗"
X,0.7579281183932347,"+

(π −ϕ) w∗
i + |w∗| sin ϕ"
X,0.758985200845666,"|w|
wi"
X,0.7600422832980972,"  
vT v∗"
X,0.7610993657505285,"2π
.
(89)"
X,0.7621564482029598,Under review as a conference paper at ICLR 2022
X,0.7632135306553911,"A.5
DERIVATION OF CLIPPED RELU STE GRADIENT"
X,0.7642706131078224,"By substituting µ′(x) = σ(x)σ(r −x) into Eq.(23), we obtain"
X,0.7653276955602537,"EZ [gcrelu]i =

1
√ 2π"
X,0.766384778012685,mn Z  Y
X,0.7674418604651163,"k,l
dZkl  e−1"
P,0.7684989429175476,"2
P"
P,0.7695560253699789,"k,l Z2
kl ×  X"
P,0.7706131078224101,"j
Zjivjσ X"
P,0.7716701902748414,"l
Zjlwl ! σ  r −
X"
P,0.7727272727272727,"l
Zjlwl !  × X k
vkσ X"
P,0.773784355179704,"l
Zklwl ! −
X"
P,0.7748414376321353,"k
v∗
kfRelu X"
P,0.7758985200845666,"l
Zklw∗
l !!"
P,0.7769556025369979,".
(90)"
P,0.7780126849894292,We calculate each term as follows.
P,0.7790697674418605,"A.5.1
CALCULATION OF THE FIRST TERM"
P,0.7801268498942917,"To discuss the ﬁrst term, we evaluate"
P,0.781183932346723,"F crelu
ijk
= EZ
h
Zji [σ(Zw) ⊙σ(r −Zw)]j [σ(Zw)]k
i
.
(91)"
P,0.7822410147991543,"Using F crelu
ijk
, the ﬁrst term can be expressed as"
P,0.7832980972515856,"ﬁrst term =
X"
P,0.7843551797040169,"j,k
F crelu
ijk
vjvk.
(92)"
P,0.7854122621564482,(i) j ̸= k case:
P,0.7864693446088795,"F crelu
ijk
=

1
√ 2π"
P,0.7875264270613108,2n Z  Y
P,0.7885835095137421,"l
dZjldZkl ! e−1"
P,0.7896405919661733,"2
P"
P,0.7906976744186046,"l Z2
jl+Z2
klZji × σ X"
P,0.7917547568710359,"l
Zjlwl ! σ  r −
X"
P,0.7928118393234672,"l
Zjlwl ! σ X"
P,0.7938689217758985,"l
Zklwl !"
P,0.7949260042283298,"=

1
√ 2π"
P,0.7959830866807611,2n Z  Y
P,0.7970401691331924,"l
d ˜Zjld ˜Zkl ! e−1"
P,0.7980972515856237,"2
P"
P,0.7991543340380549,"l ˜
Z2
jl+ ˜
Z2
kl
 X"
P,0.8002114164904862,"m′
Oj T
im′ ˜Zjm′ !"
P,0.8012684989429175,"× σ

˜Zj1

σ

r −|w| ˜Zj1

σ

˜Zk1
"
P,0.8023255813953488,"=

1
√ 2π"
P,0.8033826638477801,"2 Z
d ˜Zj1d ˜Zk1e−1"
P,0.8044397463002114,"2( ˜
Z2
j1+ ˜
Z2
k1)Oj T
i1
˜Zj1σ

˜Zj1

σ

r −|w| ˜Zj1

σ

˜Zk1
"
P,0.8054968287526427,"=
1
2
√ 2π"
P,0.806553911205074,Z r/|w|
P,0.8076109936575053,"0
d ˜Zj1e−1"
P,0.8086680761099366,"2 ˜
Z2
j1Oj T
i1
˜Zj1"
P,0.8097251585623678,"=
1
2
√"
P,0.8107822410147991,"2π Oj T
i1

1 −e−1"
P,0.8118393234672304,"2(
r
|w|)
2"
P,0.8128964059196617,"=
1
2
√ 2π"
P,0.813953488372093,"
1 −e−1"
P,0.8150105708245243,"2(
r
|w|)
2 wi"
P,0.8160676532769556,"|w|,
(93)"
P,0.8171247357293869,where we used the orthogonal transformation in the second line:
P,0.8181818181818182,"˜Zjl =
X"
P,0.8192389006342494,"m′
Oj
lm′Zjm′
(94)"
P,0.8202959830866807,"with Oi
1m′ = wm′/|w|."
P,0.821353065539112,Under review as a conference paper at ICLR 2022
P,0.8224101479915433,(ii) j = k case:
P,0.8234672304439746,"F crelu
ijj
=

1
√ 2π"
P,0.8245243128964059,n Z  Y
P,0.8255813953488372,"l
dZjl ! e−1"
P,0.8266384778012685,"2
P"
P,0.8276955602536998,"l Z2
jlZjiσ X"
P,0.828752642706131,"l
Zjlwl ! σ  r −
X"
P,0.8298097251585623,"l
Zjlwl !"
P,0.8308668076109936,"=

1
√ 2π"
P,0.8319238900634249,n Z  Y
P,0.8329809725158562,"l
d ˜Zjl ! e−1"
P,0.8340380549682875,"2
P"
P,0.8350951374207188,"l ˜
Z2
jl
 X"
P,0.8361522198731501,"m′
Oj T
im′ ˜Zjm′ !"
P,0.8372093023255814,"σ

˜Zj1

σ

r −|w| ˜Zj1
 =
1
√ 2π"
P,0.8382663847780126,"Z
d ˜Zj1e−1"
P,0.8393234672304439,"2 ˜
Z2
j1Oj T
i1
˜Zj1σ

˜Zj1

σ

r −|w| ˜Zj1
 =
1
√ 2π"
P,0.8403805496828752,Z r/|w|
P,0.8414376321353065,"0
d ˜Zj1e−1"
P,0.8424947145877378,"2 ˜
Z2
j1Oj T
i1
˜Zj1 =
1
√"
P,0.8435517970401691,"2π Oj T
i1

1 −e−1"
P,0.8446088794926004,"2(
r
|w|)
2 =
1
√ 2π"
P,0.8456659619450317,"
1 −e−1"
P,0.846723044397463,"2(
r
|w|)
2 wi"
P,0.8477801268498943,"|w|,
(95)"
P,0.8488372093023255,"In summary, the ﬁrst term is written as"
P,0.8498942917547568,"ﬁrst term =
1
2
√"
P,0.8509513742071881,"2π
wi
|w|"
P,0.8520084566596194,"
1 −e−1"
P,0.8530655391120507,"2(
r
|w|)
2
vT  
Im×m + 1m1T
m

v.
(96)"
P,0.854122621564482,"A.5.2
CALCULATION OF THE SECOND TERM"
P,0.8551797040169133,"To discuss the second term, we evaluate"
P,0.8562367864693446,"Gcrelu
ijk
= EZ
h
Zji
h
[σ(Zw) ⊙σ(r −Zw)]j fRelu(Zw∗)
i k"
P,0.857293868921776,"i
.
(97)"
P,0.8583509513742071,"Using Gcrelu
ijk , the second term can be expressed as"
P,0.8594080338266384,"second term =
X"
P,0.8604651162790697,"j,k
Gcrelu
ijk vjv∗
k.
(98)"
P,0.861522198731501,(i) j ̸= k case:
P,0.8625792811839323,"Gcrelu
ijk
=

1
√ 2π"
P,0.8636363636363636,2n Z  Y
P,0.864693446088795,"l
dZjldZkl ! e−1"
P,0.8657505285412262,"2
P"
P,0.8668076109936576,"l Z2
jl+Z2
klZji × σ X"
P,0.8678646934460887,"l
Zjlwl ! σ  r −
X"
P,0.86892177589852,"l
Zjlwl ! fRelu X"
P,0.8699788583509513,"l
Zklw∗
l !"
P,0.8710359408033826,"=

1
√ 2π"
P,0.872093023255814,2n Z  Y
P,0.8731501057082452,"l
d ˜Zjld ˜Zkl ! e−1"
P,0.8742071881606766,"2
P"
P,0.8752642706131079,"l ˜
Z2
jl+ ˜
Z2
kl
 X"
P,0.8763213530655392,"m′
Oj T
im′ ˜Zjm′ !"
P,0.8773784355179705,"× σ

˜Zj1

σ

r −|w| ˜Zj1

fRelu

|w∗| ˜Zk1
"
P,0.8784355179704016,"=

1
√ 2π"
P,0.879492600422833,"2 Z
d ˜Zj1d ˜Zk1e−1"
P,0.8805496828752643,"2( ˜
Z2
j1+ ˜
Z2
k1)Oj T
i1
˜Zj1"
P,0.8816067653276956,"× σ

˜Zj1

σ

r −|w| ˜Zj1

|w∗| ˜Zk1σ

˜Zk1
"
P,0.8826638477801269,"=

1
√ 2π"
P,0.8837209302325582,"2 Z
d ˜Zj1e−1"
P,0.8847780126849895,"2 ˜
Z2
j1Oj T
i1
˜Zj1σ

˜Zj1

σ

r −|w| ˜Zj1

|w∗|"
P,0.8858350951374208,"=

1
√ 2π"
P,0.8868921775898521,2 Z r/|w|
P,0.8879492600422833,"0
d ˜Zj1e−1"
P,0.8890063424947146,"2 ˜
Z2
j1Oj T
i1
˜Zj1|w∗|"
P,0.8900634249471459,= |w∗|
P,0.8911205073995772,"2π Oj T
i1

1 −e−1"
P,0.8921775898520085,"2(
r
|w|)
2"
P,0.8932346723044398,= |w∗| 2π
P,0.8942917547568711,"
1 −e−1"
P,0.8953488372093024,"2(
r
|w|)
2 wi"
P,0.8964059196617337,"|w|,
(99)"
P,0.8974630021141649,Under review as a conference paper at ICLR 2022
P,0.8985200845665962,"where we used the orthogonal transformation in the second line:
˜Zil =
X"
P,0.8995771670190275,"m′
Oi
lm′Zim′,"
P,0.9006342494714588,"˜Zkl =
X"
P,0.9016913319238901,"m′
Ok
lm′Zkm′
(100)"
P,0.9027484143763214,"with Oi
1m′ = wm′/|w| and Oj
1m′ = w∗
m′/|w∗|."
P,0.9038054968287527,(ii) j = k case:
P,0.904862579281184,"Gcrelu
ijj
=

1
√ 2π"
P,0.9059196617336153,m Z  Y
P,0.9069767441860465,"l
dZjl ! e−1"
P,0.9080338266384778,"2
P"
P,0.9090909090909091,"l Z2
jlZji × σ X"
P,0.9101479915433404,"l
Zjlwl ! σ  r −
X"
P,0.9112050739957717,"l
Zjlwl ! fRelu X"
P,0.912262156448203,"l
Zjlw∗
l !"
P,0.9133192389006343,"=

1
√ 2π"
P,0.9143763213530656,m Z  Y
P,0.9154334038054969,"l
dZjl ! e−1"
P,0.9164904862579282,"2
P"
P,0.9175475687103594,"l Z2
jlZji × σ"
X,0.9186046511627907,"2
X"
X,0.919661733615222,"l=1
Zjlwl ! σ  r −"
X,0.9207188160676533,"2
X"
X,0.9217758985200846,"l=1
Zjlwl !"
X,0.9228329809725159,fRelu (w∗Zj1) = 1 2π
X,0.9238900634249472,"Z  2
Y"
X,0.9249471458773785,"l=1
dZjl ! e−1"
X,0.9260042283298098,"2
P2
l=1 Z2
jlZji (δi1 + δi2) × σ"
X,0.927061310782241,"2
X"
X,0.9281183932346723,"l=1
Zjlwl ! σ  r −"
X,0.9291754756871036,"2
X"
X,0.9302325581395349,"l=1
Zjlwl !"
X,0.9312896405919662,fRelu (w∗Zj1) = 1 2π
X,0.9323467230443975,"Z
dZdθZe−1"
X,0.9334038054968288,2 Z2 (Z cos θδi1 + Z sin θδi2)
X,0.9344608879492601,× σ (Zw cos (θ −ϕ)) σ  r −
X,0.9355179704016914,"2
X"
X,0.9365750528541226,"l=1
Zjlwl !"
X,0.9376321353065539,fRelu (w∗Z cos θ) = 1 2π
X,0.9386892177589852,"Z
dZdθZe−1"
X,0.9397463002114165,2 Z2 (Z cos θδi1 + Z sin θδi2)
X,0.9408033826638478,"× σ (Zw cos (θ −ϕ)) σ (r −Zw cos (θ −ϕ)) w∗Z cos θσ (cos θ)
(101) = w∗ 2π Z π/2"
X,0.9418604651162791,"−π/2+ϕ
dθ
 
cos2 θδi1 + cos θ sin θδi2
 Z
r
w cos(θ−ϕ)"
X,0.9429175475687104,"0
dZZ3e−1 2 Z2 = w∗ 2π Z π/2"
X,0.9439746300211417,"−π/2+ϕ
dθ
 
cos2 θδi1 + cos θ sin θδi2
 × ( 2 −"
X,0.945031712473573,"
r
w cos (θ −ϕ)"
X,0.9460887949260042,"2
+ 2 ! e−1"
X,0.9471458773784355,"2(
r
w cos(θ−ϕ))
2) (102)"
X,0.9482029598308668,"where we have chosen
w∗= (w∗, 0n−1),
w = (w1, w2, 0n−2)
(103)
with w∗> 0 and ϕ is deﬁned as the angle between w∗and w. We also have used the formula,
Z t"
X,0.9492600422832981,"0
dxx3e−1"
X,0.9503171247357294,2 x2 = 2 −(t2 + 2)e−1
X,0.9513742071881607,"2 t2.
(104)"
X,0.952431289640592,"Note that
w∗
i
|w∗| = δi1,"
X,0.9534883720930233,"wi
|w| = cos ϕδi1 + sin ϕδi2.
(105)"
X,0.9545454545454546,Under review as a conference paper at ICLR 2022
X,0.9556025369978859,"Using C(w, ϕ), S(w, ϕ) deﬁned as"
X,0.9566596194503171,"C(w, ϕ) ≡w∗ 2π Z π/2"
X,0.9577167019027484,"−π/2+ϕ
dθ cos2 θ ( 2 −"
X,0.9587737843551797,"
r
w cos (θ −ϕ)"
X,0.959830866807611,"2
+ 2 ! e−1"
X,0.9608879492600423,"2(
r
w cos(θ−ϕ))
2) ,"
X,0.9619450317124736,"S(w, ϕ) ≡w∗ 2π Z π/2"
X,0.9630021141649049,"−π/2+ϕ
dθ sin θ cos θ ( 2 −"
X,0.9640591966173362,"
r
w cos (θ −ϕ)"
X,0.9651162790697675,"2
+ 2 ! e−1"
X,0.9661733615221987,"2(
r
w cos(θ−ϕ))
2) , (106)"
X,0.96723044397463,"If ϕ ̸= 0, π, Gcrelu
ijj
is formally written as"
X,0.9682875264270613,"Gcrelu
ijj
= C(w, ϕ) w∗
i
|w∗| + S(w, ϕ)

1
sin ϕ
wi
|w| −cot ϕ w∗
i
|w∗|"
X,0.9693446088794926,"
(107)"
X,0.9704016913319239,"If ϕ = 0 or π, Gcrelu
ijj
is formally written as"
X,0.9714587737843552,"Gcrelu
ijj
= C(w, 0) w∗
i
|w∗|
for ϕ = 0,
(108)"
X,0.9725158562367865,"Gcrelu
ijj
= 0 for ϕ = π
(109)"
X,0.9735729386892178,"where we used S(w, 0) = S(w, π) = C(w, π) = 0 and"
X,0.9746300211416491,"C(w, 0) = w∗ 2π Z π/2"
X,0.9756871035940803,"−π/2
dθ cos2 θ

2 −

r
w cos θ"
X,0.9767441860465116,"2
+ 2

e−1"
X,0.9778012684989429,"2(
r
w cos θ)
2 = w∗ 2π"
X,0.9788583509513742,"
π −r we−1 2( r w)
2√"
X,0.9799154334038055,"2π −πerfc

r
√"
W,0.9809725158562368,2w
W,0.9820295983086681,"
(110)"
W,0.9830866807610994,"with the following formula
Z π/2"
W,0.9841437632135307,"−π/2
dθe−
a2"
W,0.985200845665962,"2 cos2 θ = πerfc
 a
√ 2"
W,0.9862579281183932,"
,
(111) Z π/2"
W,0.9873150105708245,"−π/2
dθ cos2 θe−
a2"
W,0.9883720930232558,2 cos2 θ = 1 2
W,0.9894291754756871,"
ae−a2 2 √"
W,0.9904862579281184,"2π +
 
1 −a2
πerfc
 a
√ 2"
W,0.9915433403805497,"
.
(112)"
W,0.992600422832981,"In summary, the second term is written as"
W,0.9936575052854123,second term = 1
W,0.9947145877378436,"2π
|w∗| |w|"
W,0.9957716701902748,"
1 −e−1"
W,0.9968287526427061,"2(
r
|w|)
2
wi

vT  
−Im×m + 1m1T
m

v∗"
W,0.9978858350951374,"+

C(w, ϕ) w∗
i
|w∗| + S(w, ϕ)

1
sin ϕ
wi
|w| −cot ϕ w∗
i
|w∗|"
W,0.9989429175475687,"  
vT v∗
.
(113)"
