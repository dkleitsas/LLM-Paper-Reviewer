Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002976190476190476,"Better-supervised models might have better performance. In this paper, we ﬁrst
clarify what makes for good supervision for a classiﬁcation problem, and then
explain two existing label reﬁning methods, label smoothing and knowledge dis-
tillation, in terms of our proposed criterion. To further answer why and how bet-
ter supervision emerges, we observe the learning path, i.e., the trajectory of the
model’s predictions during training, for each training sample. We ﬁnd that the
model can spontaneously reﬁne “bad” labels through a “zig-zag” learning path,
which occurs on both toy and real datasets. Observing the learning path not only
provides a new perspective for understanding knowledge distillation, overﬁtting,
and learning dynamics, but also reveals that the supervisory signal of a teacher
network can be very unstable near the best points in training on real tasks. In-
spired by this, we propose a new knowledge distillation scheme, Filter-KD, which
improves downstream classiﬁcation performance in various settings."
INTRODUCTION,0.005952380952380952,"1
INTRODUCTION"
INTRODUCTION,0.008928571428571428,"In multi-class classiﬁcation problems, we usually supervise our model with “one-hot” labels: label
vectors y which have yi = 1 for one i, and 0 for all other dimensions. Over time, however, it
has gradually become clear that this “default” setup is not always the best choice in practice, in
that other schemes can yield better performance on held-out test sets. One such alternative is to
summarize a distribution of human annotations, as Peterson et al. (2019) did for CIFAR10. An
alternative approach is label smoothing (e.g. Szegedy et al., 2016), mixing between a one-hot label
and the uniform distribution. Knowledge distillation (KD), ﬁrst training a teacher network on the
training set and then a student network on the teacher’s output probabilities, was originally proposed
for model compression (Hinton et al., 2015) but can also be thought of as reﬁning the supervision
signal: it provides “soft” teacher outputs rather than hard labels to the student."
INTRODUCTION,0.011904761904761904,"Knowledge distillation is promising because it requires no additional annotation effort, but – unlike
label smoothing – can still provide sample-speciﬁc reﬁnement. Perhaps surprisingly, knowledge
distillation can improve student performance even when the teacher is of exactly the same form as
the student and trained on the same data; this is known as self-distillation (Furlanello et al., 2018;
Zhang et al., 2019). There have been many recent attempts to explain knowledge distillation and
speciﬁcally self-distillation (e.g. Menon et al., 2021; Allen-Zhu & Li, 2020; Tang et al., 2020), from
both optimization and supervision perspectives. We focus on the latter area, where it is usually
claimed that the teacher provides useful “dark knowledge” to the student through its labels."
INTRODUCTION,0.01488095238095238,"Inspired by this line of work, we further explore why and how this improved supervisory signal
emerges during the teacher’s one-hot training. Speciﬁcally, we ﬁrst clarify that given any input
sample x, good supervision signals should be close (in L2 distance) to the ground truth categorical
distribution, i.e., p∗(y | x). We then show that a neural network (NN) can automatically reﬁne “bad
labels”, those where p∗(y | x) is far from the training set’s one-hot vector.1 During one-hot training,
the model prediction on such a sample ﬁrst moves towards p∗(y | x), and then slowly converges to
its supervisory label, following a “zig-zag” pattern. A well-trained teacher, one that does not overﬁt"
INTRODUCTION,0.017857142857142856,"1This might be because x is ambiguous (perhaps p∗(y | x) is ﬂat, or we simply got a sample from a less-
likely class), or because the one-hot label has been corrupted through label noise or otherwise is “wrong.”"
INTRODUCTION,0.020833333333333332,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023809523809523808,"to particular training labels, can thus provide supervisory signals closer to p∗(y | x). We justify
analytically (Section 3.3) that this pattern is common in gradient descent training. Our explanations
cause us to recognize that this signal can be better identiﬁed by taking a moving average of the
teacher’s prediction, an algorithm we term Filter-KD. This approach yields better supervision and
hence better downstream performance, especially when there are many bad labels."
INTRODUCTION,0.026785714285714284,"After completing this work, we became aware of an earlier paper (Liu et al., 2020) studying almost
the same problem in a similar way. We discuss differences between the papers throughout."
SUPERVISION INFLUENCES GENERALIZATION,0.02976190476190476,"2
SUPERVISION INFLUENCES GENERALIZATION"
SUPERVISION INFLUENCES GENERALIZATION,0.03273809523809524,We begin by clarifying how the choice of supervisory signal affects the learned model.
CHOICES OF SUPERVISION SIGNAL,0.03571428571428571,"2.1
CHOICES OF SUPERVISION SIGNAL"
CHOICES OF SUPERVISION SIGNAL,0.03869047619047619,"In K-way classiﬁcation, our goal is to learn a mapping f : X →∆K that can minimize the risk"
CHOICES OF SUPERVISION SIGNAL,0.041666666666666664,"R(f) ≜
E
(x,y)∼P[L(y, f(x))] =
Z"
CHOICES OF SUPERVISION SIGNAL,0.044642857142857144,"p(x) p(y|x) L(y, f(x)) dx dy,
(1)"
CHOICES OF SUPERVISION SIGNAL,0.047619047619047616,"Here the label y is an integer ranging from 1 to K, and f gives predictions in the probability simplex
∆K (a nonnegative vector of length K summing to one); L(y, f(x)) is the loss function, e.g. cross-
entropy or square loss. The input signal x is usually high dimensional, e.g., an image or a sequence of
word embeddings. The joint distribution of (x, y) is P, whose density2 can be written as p(x)p(y|x).
In practice, as P is unknown, we instead (approximately) minimize the empirical risk"
CHOICES OF SUPERVISION SIGNAL,0.050595238095238096,"Remp(f, D) ≜ N
X n=1 K
X k=1"
CHOICES OF SUPERVISION SIGNAL,0.05357142857142857,"1
N 1(yn =k) L(k, f(xn)) = N
X n=1"
N ET,0.05654761904761905,"1
N eT
ynL(f(xn)),
(2)"
N ET,0.05952380952380952,"where 1(yn = k) is an indicator function which equals 1 if yn = k or 0 otherwise, and eyn ∈
{0, 1}K is its one-hot vector form. L(f(xn)) = (L(1, f(xn)), . . . , L(K, f(xn))) ∈RK is the loss
for each possible label. In Remp, the N training pairs D ≜{(xn, yn)}N
n=1 are sampled i.i.d. from P."
N ET,0.0625,"Comparing (2) to (1), we can see p(x) is approximated by an uniform distribution over the samples,
which is reasonable. However, using an indicator function (i.e., one-hot distribution) to approximate
p(y | x) bears more consideration. For example, if a data point x is quite vague and its true p(y|x)
is ﬂat or multimodal, we might hope to see x multiple times with different label y during training.
But actually, most datasets have only one copy of each x, so we only ever see one corresponding ey.
Although Remp is an unbiased estimator for R, if we used a better (e.g. lower-variance) estimate of
p(y | x), we could get a better estimate for R and thus, hopefully, better generalization."
N ET,0.06547619047619048,"Speciﬁcally, suppose we were provided a “target” distribution ptar(y | x) (written in vector form as
ptar(x)) for each training point x, as D′ = {(xn, ptar(xn))}N
n=1. Then we could use"
N ET,0.06845238095238096,"Rtar(f, D′) ≜ N
X n=1 K
X k=1"
N ET,0.07142857142857142,"1
N ptar(yn = k | xn) L(k, f(xn)) = N
X n=1"
N ET,0.0744047619047619,"1
N ptar(xn)TL(f(xn)).
(3)"
N ET,0.07738095238095238,"Standard training with Remp is a special case of Rtar, using ptar(xn) = eyn. The CIFAR10H dataset
(Peterson et al., 2019) is one attempt at a different ptar, using multiple human annotators to estimate
ptar. Label smoothing (e.g. Szegedy et al., 2016) sets ptar to a convex combination of ey and the
constant vector 1"
N ET,0.08035714285714286,"K 1. In knowledge distillation (KD; Hinton et al., 2015), a teacher is ﬁrst trained on
D, then a student learns from D′ with ptar based on the teacher’s outputs. All three approaches yield
improvements over standard training with Remp."
N ET,0.08333333333333333,"2Because we are working with classiﬁcation problems, we use densities with respect to a product of some
arbitrary measure on x (probably Lebesgue) with counting measure on y, and assume that these densities exist
for notational convenience. None of our arguments will depend on the choice of base measure."
N ET,0.08630952380952381,Published as a conference paper at ICLR 2022
N ET,0.08928571428571429,"60
40
20
0
20
40
60 60 40 20 0 20 40"
N ET,0.09226190476190477,"60
Class0
Class1
Class2"
N ET,0.09523809523809523,(a) tSNE of toy dataset.
N ET,0.09821428571428571,"0
5
10
15
20
25
30
L2-distance of p_tar and p* 0.830 0.832 0.834 0.836 0.838 0.840"
N ET,0.10119047619047619,Accuracy on test set
N ET,0.10416666666666667,"Noisy p
OHT
LS
GT
KD
ESKD"
N ET,0.10714285714285714,(b) ACC vs ∥ptar −p∗∥2
N ET,0.11011904761904762,"0
10
20
30
40
50
L2-distance of p_tar and p* 0.00 0.01 0.02 0.03 0.04 0.05"
N ET,0.1130952380952381,ECE on test set
N ET,0.11607142857142858,"Noisy p
OHT
LS
GT
KD
ESKD"
N ET,0.11904761904761904,(c) ECE v.s. ∥ptar −p∗∥2
N ET,0.12202380952380952,"Figure 1: Experiments on a toy dataset when learning from different ptar. In (b-c), the horizontal
axis represents ∥ptar −p∗∥2, and the vertical axis is the generalization performance. OHT means
one-hot training (on Remp), LS means label smoothing, GT means ground truth training with p∗, KD
is knowledge distillation, and ESKD is early-stopped KD. The Spearman correlation coefﬁcient for
results in (b) is -0.930 with p-value 1.9 × 10−53; for (c) is 0.895 with p-value 2.7 × 10−43."
MEASURING THE QUALITY OF SUPERVISION,0.125,"2.2
MEASURING THE QUALITY OF SUPERVISION"
MEASURING THE QUALITY OF SUPERVISION,0.12797619047619047,"Choosing a different ptar, then, can lead to a better ﬁnal model. Can we characterize which ptar will
do well? We propose the following, as a general trend."
MEASURING THE QUALITY OF SUPERVISION,0.13095238095238096,"Hypothesis 1. Suppose we train a model supervised by ptar, that is, we minimize Rtar(f, D′). Then,
smaller average L2 distance between ptar and the ground truth p∗on these samples, i.e. small
Ex [∥ptar(x) −p∗(x)∥2], will in general lead to better generalization performance."
MEASURING THE QUALITY OF SUPERVISION,0.13392857142857142,"This hypothesis is suggested by Proposition 3 of Menon et al. (2021), which shows (tracking con-
stants omitted in their proof) that for any predictor f and loss bounded as L(y, ˆy) ≤ℓ,"
MEASURING THE QUALITY OF SUPERVISION,0.13690476190476192,"E
D′

(Rtar(f, D′) −R(f))2
≤1"
MEASURING THE QUALITY OF SUPERVISION,0.13988095238095238,"N Var
x

pT
tarL(f(x))

+ ℓ2K

E
x ∥ptar(x) −p∗(x)∥2
2
.
(4)"
MEASURING THE QUALITY OF SUPERVISION,0.14285714285714285,"When N is large, the second term will dominate the right-hand side, implying smaller average
∥ptar −p∗∥will lead to Rtar being a better approximation of the true risk R; minimizing it should
then lead to a better learned model. This suggests that the quality of the supervision signal can be
roughly measured by its L2 distance to the ground truth p∗. Appendix B slightly generalizes the
result of Menon et al. with bounds based on total variation (L1) and KL divergences; we focus on
the L2 version here for simplicity."
MEASURING THE QUALITY OF SUPERVISION,0.14583333333333334,"To further support this hypothesis, we conduct experiments on a synthetic Gaussian problem (Fig-
ure 1 (a); details in Appendix C), where we can easily calculate p∗(y | x) for each sample. We
ﬁrst generate several different ptar by adding noise3 to the ground truth p∗, then train simple 3-layer
NNs under that supervision. We also show ﬁve baselines: one-hot training (OHT), label smoothing
(LS), KD, early-stopped KD (ESKD), and ground truth (GT) supervision (using p∗). KD refers to a
teacher trained to convergence, while ESKD uses a teacher stopped early based on validation accu-
racy. We early-stop the student’s training in all settings. From Figure 1 (b-c), it is clear that smaller
∥ptar −p∗∥2 leads to better generalization performance, as measured either by accuracy (ACC) or
expected calibration error (ECE)4 on a held-out test set. Appendix C has more detailed results."
INSIGHTS FROM THE LEARNING PATH,0.1488095238095238,"3
INSIGHTS FROM THE LEARNING PATH"
INSIGHTS FROM THE LEARNING PATH,0.15178571428571427,"In the toy example of Section 2, we see that ESKD outperforms other baselines in accuracy by
a substantial margin (and all baselines are roughly tied in ECE). We expect that supervision with
smaller ∥ptar −p∗∥2 leads to better generalization performance, but it is not clear how better ptar
emerges from when the teacher in ESKD is trained using one-hot labels. This section will answer
this, by observing the learning paths of training samples."
INSIGHTS FROM THE LEARNING PATH,0.15476190476190477,"3Whenever we mention adding noise to p∗, we mean we add independent noise to each dimension, and then
re-normalize it to be a distribution. Large noise can thus ﬂip the “correct” label.
4ECE measures the calibration of a model (Guo et al., 2017). Brieﬂy, lower ECE means the model’s conﬁ-
dence in its predictions is more accurate. See Appendix A for details."
INSIGHTS FROM THE LEARNING PATH,0.15773809523809523,Published as a conference paper at ICLR 2022
INSIGHTS FROM THE LEARNING PATH,0.16071428571428573,"Figure 2: Normalized (divided by
√"
INSIGHTS FROM THE LEARNING PATH,0.1636904761904762,"2) distance between output distribution q and p∗during the
one-hot training in different stages (left to right: initial, early stop, convergence). In these ﬁgures,
Ex ∥q(x) −p∗(x)∥2 of Hypothesis 1 is the mean height of all points in the ﬁgure. We provide more
results about the NNs trained under different supervisions using this fashion in Appendix F."
PAY MORE ATTENTION TO HARDER SAMPLES,0.16666666666666666,"3.1
PAY MORE ATTENTION TO HARDER SAMPLES"
PAY MORE ATTENTION TO HARDER SAMPLES,0.16964285714285715,"For a ﬁner-grained understanding of early stopping the teacher, we would like to better under-
stand how the teacher’s predictions evolve in training. Assuming Hypothesis 1, the main factor
is Ex∥q(x) −p∗(x)∥2, where q is the teacher’s output probability distribution. We expect, though,
that this term will vary for different x, in part because some samples are simply more difﬁcult to
learn. As a proxy for this, we deﬁne base difﬁculty as ∥ey −p∗(x)∥2, which is large if:"
PAY MORE ATTENTION TO HARDER SAMPLES,0.17261904761904762,"• x is ambiguous: p∗has several large components, so there is no one-hot label near p∗.
• x is not very ambiguous (there is a one-hot label near p∗), but the sample was “unlucky” and
drew y from a low-probability class."
PAY MORE ATTENTION TO HARDER SAMPLES,0.17559523809523808,"Figure 2 shows these two quantities at three points in training: initialization, the point where ESKD’s
teacher stops, and convergence. At initialization, most points5 have large ∥q(x) −p∗(x)∥2. By the
point of early stopping, most q(x) values are roughly near p∗. At convergence, however, q(x) ≈ey,
as the classiﬁer has nearly memorized the training inputs, leading to a diagonal line in the plot."
PAY MORE ATTENTION TO HARDER SAMPLES,0.17857142857142858,"It is clear the biggest contributors to Ex∥q(x) −p∗(x)∥2 when training a model to convergence are
the points with high base difﬁculty. Per Hypothesis 1, these are the points we should most focus on."
LEARNING PATH OF DIFFERENT SAMPLES,0.18154761904761904,"3.2
LEARNING PATH OF DIFFERENT SAMPLES"
LEARNING PATH OF DIFFERENT SAMPLES,0.18452380952380953,"To better understand how q changes for each sample, we track the model’s outputs on all training
samples at each training step. Figure 3 shows four samples with different base difﬁculty, with the
vectors of three probabilities plotted as points on the simplex (details in Appendix A)."
LEARNING PATH OF DIFFERENT SAMPLES,0.1875,"The two easy samples very quickly move to the correct location near ey (as indicated by the light
color until reaching the corner). The medium sample takes a less direct route, drifting off slightly to-
wards p∗, but still directly approaches ey. The hard sample, however, does something very different:
it ﬁrst approaches p∗, but then veers off towards ey, giving a “zig-zag” path. In both the medium
and hard cases, there seems to be some “unknown force” dragging the learning path towards p∗; in
both cases, the early stopping point is not quite perfect, but is noticeably closer to p∗than the ﬁnal
converged point near ey. In other words, during one-hot training, the NNs can spontaneously reﬁne
the “bad labels.” Under Hypothesis 1, this partly explains the superior performance of ESKD to
KD with a converged teacher.6"
LEARNING PATH OF DIFFERENT SAMPLES,0.19047619047619047,"These four points are quite representative of all samples under different toy-dataset settings. In
Appendix E, we also deﬁne a “zig-zagness score” to numerically summarize the learning path shape,
and show it is closely correlated to base difﬁculty."
LEARNING PATH OF DIFFERENT SAMPLES,0.19345238095238096,"5The curve structure is expected: points with p∗≈( 1 3, 1 3, 1"
LEARNING PATH OF DIFFERENT SAMPLES,0.19642857142857142,"3) are near the middle of the base difﬁculty
range, and all points are initialized with fairly ambiguous predictions q.
6KD’s practical success is also related to the temperature and optimization effects, among others; better
supervision is not the whole story. We discuss the effect of various hyperparameters in Appendix D."
LEARNING PATH OF DIFFERENT SAMPLES,0.19940476190476192,Published as a conference paper at ICLR 2022
LEARNING PATH OF DIFFERENT SAMPLES,0.20238095238095238,"Figure 3: Learning path of samples with different base difﬁculty. Corners correspond to one-hot
vectors. Colors represent training time: transparent at initialization, dark blue at the end of training."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.20535714285714285,"3.3
EXPLANATION OF PATTERNS IN THE LEARNING PATH"
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.20833333333333334,The following decomposition will help explain the “unknown force” pushing q towards p∗.
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.2113095238095238,"Proposition 1. Let zt(x) ≜f(wt, x) denote the network output logits with parameters wt, and
qt(x) = Softmax(zt(x)) the probabilities. Let wt+1 ≜wt −η ∇w
 
ptar(xu)TL(qt(xu))

be the
result of applying one step of SGD to wt using the data point (xu, ptar(xu)) with learning rate η.
Then the change in network predictions for a particular sample xo is"
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.21428571428571427,"qt+1(xo) −qt(xo) = η At(xo) Kt(xo, xu)
 
ptar(xu) −qt(xu)

+ O(η2∥∇wz(xu)∥2
op),"
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.21726190476190477,"where At(xo) = ∇zqt(xo) and Kt(xo, xu) = (∇wz(xo)|wt) (∇wz(xu)|wt)T are K × K matrices."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.22023809523809523,"The matrix Kt is the empirical neural tangent kernel, which we can think of roughly as a notion of
“similarity” between xo and xu based on the network’s representation at time t, which can change
during training in potentially complex ways. In very wide networks, though, Kt is nearly invariant
throughout training and nearly independent of the initialization (Jacot et al., 2018; Arora et al.,
2019). The gradient norm can be controlled by gradient clipping, or bounded with standard SGD
analyses when optimization doesn’t diverge. Appendix G has more details and the proof."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.22321428571428573,"Figure 4 shows the learning path of a hard sample during one-hot training, say xo, where the label
eyo is far from p∗(xo). In each epoch, w will receive N −1 updates based on the loss of xu ̸= xo
(the small blue path), and one update based on xo (the big red path). At any time t, “dissimilar”
samples will have small Kt(xo, xu) (as measured, e.g., by its trace), and hence only slightly affect
the predicted label for xo. Similar samples will have large Kt(xo, xu), and hence affect its updates
much more; because p∗is hopefully similar for similar x values, it is reasonable to expect that the
mean of ey for data points with similar x will be close to p∗(xo). Thus the net effect of updates for
xu ̸= xo should be to drag q(xo) towards p∗(xo). This is the “unknown force” we observed earlier."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.2261904761904762,Figure 4: Updates of q(xo) over training.
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.22916666666666666,"The other force affecting q(xo) during training
is, of course, the update based on xo, driving
q(xo) towards eyo. How do these two forces
balance over the course training?
Early on,
∥eyu −qt(xu)∥2 is relatively large for nearly
any xu, because near initialization qt(xu) will
be relatively ﬂat. Hence the size of the updates
for “similar” xu and the update for xo should
be comparable, meaning that, if there are at
least a few “similar” training points, qt(xo)
will move towards p∗(xo). Throughout train-
ing, some of these similar samples will become
well-classiﬁed, so that ∥eyu −qt(xu)∥2 be-
comes small, and their updates will no longer
exert much force on q(xo). Thus, the xo up-
dates begin to dominate, causing the zig-zag
pattern as the learning path turns towards eyo.
For easy samples, where p∗and eyo are in the same direction, these forces agree and lead to fast
convergence. On samples like the “medium” point in Figure 3, the two forces broadly agree early
on, but the learning path deviates slightly towards p∗en route to eyo."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.23214285714285715,Published as a conference paper at ICLR 2022
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.23511904761904762,"Figure 5: Learning path on CIFAR10. In the ﬁrst two panels we record qt for each batch, while in
the last panel we record it for each epoch. See Appendix E for the learning paths of more samples."
EXPLANATION OF PATTERNS IN THE LEARNING PATH,0.23809523809523808,"Liu et al. (2020) prove that a similar pattern occurs while training a particular linear model on data
with some mislabeled points, and speciﬁcally that stopping training at an appropriate early point
will give better predictions than continuing training."
LEARNING PATHS ON REAL TASKS,0.24107142857142858,"4
LEARNING PATHS ON REAL TASKS"
LEARNING PATHS ON REAL TASKS,0.24404761904761904,"Our analysis in Sections 2 and 3 demonstrate that the model can spontaneously reﬁne the bad labels
during one-hot training: the zig-zag learning path ﬁrst moves towards the unknown p∗. But is this
also true on real data, for more complex network architectures? We will now show that they do,
although seeing the patterns requires a little more subtlety."
LEARNING PATHS ON REAL TASKS,0.24702380952380953,"We visualize the learning path of data points while training a ResNet18 (He et al., 2016) on CIFAR10
for 200 epochs as an example. The ﬁrst panel of Figure 5 shows the learning path of an easy sample.7
We can see that qt converges quickly towards the left corner, the one-hot distribution for the correct
class, because the color of the scattered points in that ﬁgure are quite light. At the early stopping
epoch, qt has already converged to ey. However, for a hard sample, it is very difﬁcult to observe
any patterns from the raw path (blue points): there are points almost everywhere in this plot. This
is likely caused by the more complex network and dataset, as well as a high learning rate in early
training. To ﬁnd the hidden pattern in this high-variance path, we treat qt as a time series signal with
many high-frequency components. Thus we can collect its low-frequency components via a low-
pass ﬁlter and then draw that, i.e., taking a exponential moving average (EMA) on qt (red points).
This makes it clear that, overall, the pattern zig-zags, ﬁrst moving towards the unknown true label
before eventually turning to memorize the wrong label."
LEARNING PATHS ON REAL TASKS,0.25,"This ﬁltering method not only helps us observe the zig-zag pattern, but can also reﬁne the labels.
We use the following two experiments to verify this. First, we train the model on CIFAR10H using
one-hot labels. As CIFAR10H provides phum, which can be considered as an approximation of p∗,
we track the mean distance between q and phum during training. In the ﬁrst panel of Figure 6, which
averages the distance of all 10k training samples, the difference between the blue qkd curve and red
qﬁlt curve is quite small. However, we can still observe that the red curve is lower than the blue
one before overﬁtting: ﬁltering can indeed reﬁne the labels. This trend is more signiﬁcant when
considering the most difﬁculty samples, as in the second panel: the gap between curves is larger."
LEARNING PATHS ON REAL TASKS,0.25297619047619047,"To further verify the label reﬁnement ability of ﬁltering, we randomly choose 1,000 of the 50,000
CIFAR10 training samples, and ﬂip their labels to a different y′ ̸= y. The last panel in Figure 6 tracks
how often the most likely prediction of the network, argmax qt, recovers the true original label,
rather than the provided random label, for the corrupted data points. At the beginning of the model’s
training, the initialized model randomly guesses labels, getting about 10% of the 1,000 ﬂipped labels
correct. During training, the model spontaneously corrects some predictions, as the learning path
ﬁrst moves towards p∗. Eventually, though, the model memorizes all its training labels. Training
with the predictions from the 400th epoch corresponds to standard KD (no corrected points). Early
stopping as in ESKD would choose a point with around 70% of labels corrected. The ﬁltered path,
though, performs best, with over 80% corrected."
LEARNING PATHS ON REAL TASKS,0.25595238095238093,"7Without knowing p∗, we instead use zig-zag score – see Appendix E – to measure the difﬁculty."
LEARNING PATHS ON REAL TASKS,0.25892857142857145,Published as a conference paper at ICLR 2022
LEARNING PATHS ON REAL TASKS,0.2619047619047619,Figure 6: Filtering can reﬁne the labels in both clean and noisy label case.
LEARNING PATHS ON REAL TASKS,0.2648809523809524,"Figure 7: Test accuracy under different noise ratio σ. Solid lines are the means while shade region
are the standard errors for 3 runs with different random seeds (shaded range is the standard error).
Last panel compare the inﬂuence of different temperatures. Each thin rectangle plot represents a
different σ = {0, 0.1, ..., 0.8}, in which we plot the results with different τ = {0.5, 1, 2, 4, 10}."
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION,0.26785714285714285,"5
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION"
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION,0.2708333333333333,"Figure 6 gives a clear motivation for a new knowledge distillation method, which we call Filter-KD
(Algorithm 1): train the student from the smoothed predictions of the teacher network. Speciﬁcally,
we maintain a look-up table qsmooth ∈RN×K to store a moving average of qt for each training
sample. Note that in one epoch, each qsmooth(xn) will be updated only once. We check the early
stopping criterion with the help of a validation set. Afterwards, the teaching supervision qsmooth is
ready, and we can train a student network under its supervision. This corresponds to using a moving
average of the teacher model “in function space,” i.e. averaging the outputs of the function over time."
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION,0.27380952380952384,"Compared to ESKD, Filter-KD can avoid the extremely high variance of qt during training. Unlike
Figure 5, which plots qt after every iteration of training, most practical implementations only con-
sider early-stopping at the end of each epoch. This is equivalent to down-sampling the noisy learning
path (as in the last panel of Figure 5), further exacerbating the variance of qt. Thus ESKD will likely
select a bad ptar for many data points. Filter-KD, by contrast, has much more stable predictions."
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION,0.2767857142857143,"We will show the effectiveness of this algorithm shortly, but ﬁrst we discuss its limitations. Com-
pared to ESKD, the running time of Filter-KD might be slightly increased. Furthermore, compared
to the teaching model in ESKD, the Filter-KD requires a teaching table qsmooth, which will require
substantial memory when the dataset is large. One alternative avoiding the need for this table would
be to instead take an average “in parameter space,” like e.g. momentum parameter updating as in
Szegedy et al. (2015). We empirically ﬁnd that, although this helps the model converge faster, it does
not lead to a better teacher network; see Appendix I. Thus, although Filter-KD has clear drawbacks,
we hope that our explanations here may lead to better practical algorithms in the future."
FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION,0.27976190476190477,"Similarly inspired by this spontaneous label reﬁning mechanism (or early stopping regularization),
Liu et al. (2020) and Huang et al. (2020) each propose algorithms aiming at the noisy label problem.
We discuss the relationship between these three methods in Appendix H."
QUANTITATIVE RESULTS OF FILTER-KD,0.28273809523809523,"5.1
QUANTITATIVE RESULTS OF FILTER-KD"
QUANTITATIVE RESULTS OF FILTER-KD,0.2857142857142857,"We now compare the performance of Filter-KD and other baselines on a real dataset. We focus on
self-distillation and a ﬁxed temperature τ = 1 (except Table 2 and last panel in Figure 7), as we"
QUANTITATIVE RESULTS OF FILTER-KD,0.28869047619047616,Published as a conference paper at ICLR 2022
QUANTITATIVE RESULTS OF FILTER-KD,0.2916666666666667,"Noise σ
0%
5%
10%
20%
OHT
56.95
53.02
52.02
30.52
ESKD
58.61
53.53
52.99
36.55
FilterKD
59.32
56.43
55.51
40.81"
QUANTITATIVE RESULTS OF FILTER-KD,0.29464285714285715,Table 1: Results on TinyImageNet dataset.
QUANTITATIVE RESULTS OF FILTER-KD,0.2976190476190476,"Eff→Res Res→Mob Res→VGG
Teacher
86.83
77.23
77.98
Student
78.09
72.58
71.04
ESKD
81.16
73.13
72.64
FilterKD
83.03
75.79
74.49"
QUANTITATIVE RESULTS OF FILTER-KD,0.3005952380952381,"Table 2: Teacher→student, on CIFAR100."
QUANTITATIVE RESULTS OF FILTER-KD,0.30357142857142855,"want the only difference among these methods to be ptar. Thus we can conclude the improvement
we achieved comes purely from the reﬁned supervision. See Appendix D for other temperatures."
QUANTITATIVE RESULTS OF FILTER-KD,0.30654761904761907,"Accuracy
ECE
OHT
KD
ESKD
FilterKD
OHT
KD
ESKD
FilterKD"
QUANTITATIVE RESULTS OF FILTER-KD,0.30952380952380953,"CIFAR10
95.34
95.39
95.42
95.63
0.026
0.027
0.027
0.007
CIFAR100
78.07
78.40
78.83
80.09
0.053
0.061
0.067
0.029"
QUANTITATIVE RESULTS OF FILTER-KD,0.3125,"Table 3: Quantitative comparison of generalization performance for ResNet18 self-distillation
(mean value of 5 runs). Refer to Table 6 for detailed results."
QUANTITATIVE RESULTS OF FILTER-KD,0.31547619047619047,"Input: Dataset {(xn, yn)}N
n=1, where In = {1, 2, ..., N} is the index for each pair
# Train the teacher
Initialize a network model, initialize an N × K matrix called qsmooth
Go through the entire dataset, calculate qsmooth[n] = Softmax(model(xn))
for epoch = 1, 2, ..., T do"
QUANTITATIVE RESULTS OF FILTER-KD,0.31845238095238093,"for n ∈{1, 2, . . . , N} in random order do"
QUANTITATIVE RESULTS OF FILTER-KD,0.32142857142857145,"ˆp = Softmax(model(xn))
qsmooth[n] = (1 −α) · qsmooth[n] + α · ˆp
Update parameters based on loss = CrossEntropy(ˆp, yn),
end for
Check the early stopping criterion; stop training if satisﬁed
end for
# Train the student
For each input xn, set ptar = qsmooth[n]; train the network under the supervision of ptar
Algorithm 1: Filter-KD. α controls the cut-off frequency of low-pass ﬁlter (0.05 here)."
QUANTITATIVE RESULTS OF FILTER-KD,0.3244047619047619,"The ﬁrst task we consider is noisy-label classiﬁcation, where we train and validate the model on a
dataset with some labels randomly ﬂipped. After training, all the models are evaluated on a clean
held-out test set. The experiments are conducted on CIFAR (Figure 7) and TinyImageNet (Table 1),
under different noise ratios σ: σ = 0.1 means 10% of the labels are ﬂipped. In Figure 7, an
interesting trend can be observed: the enhancement brought by Filter-KD is not signiﬁcant when
σ is too small or too large. That is reasonable because for small σ, few samples have bad labels,
thus the possible enhancement might not be large. When σ is too high, the labels of similar points
become less reliable, and the learning path will no longer head as reliably towards p∗. Thus, for
very high noise ratios, the performance of Filter-KD decays back towards that of OHT."
QUANTITATIVE RESULTS OF FILTER-KD,0.3273809523809524,"Filter-KD also outperforms other methods in both accuracy and calibration on the clean dataset, as
illustrated in Table 3. This remains true in the more common KD case when the teacher network is
bigger than the student; see results in Table 2. Note that in Table 2, we no longer keep τ = 1, as the
baselines are more sensitive to τ when the teacher is large (see Appendix D for more discussion);
instead we optimize τ for each setting. Here “Eff” is short for EfﬁcientNet-B1 (Tan & Le, 2019),
“Res” is short for ResNet18, “Mobi” is short for MobileNetV2 (Sandler et al., 2018), “VGG” is
short for VGG-11 (Simonyan & Zisserman, 2015)."
QUANTITATIVE RESULTS OF FILTER-KD,0.33035714285714285,"In summary, the explanation in Section 4 and experimental results in this section demonstrate that
better supervisory signal, which can be obtained by Filter-KD, can enhance the prediction perfor-
mance in both clean and noisy-label case. (It is also possible that the algorithm improves perfor-"
QUANTITATIVE RESULTS OF FILTER-KD,0.3333333333333333,Published as a conference paper at ICLR 2022
QUANTITATIVE RESULTS OF FILTER-KD,0.33630952380952384,"mance for other reasons as well, especially in the clean label case; the inﬂuence of temperature may
be particularly relevant.)"
RELATED WORK,0.3392857142857143,"6
RELATED WORK"
RELATED WORK,0.34226190476190477,"Human label reﬁnement
Peterson et al. (2019) use a distribution of labels obtained from multiple
annotators to replace the one-hot label in training, and ﬁnd that doing so enhances both the general-
ization performance of trained models and their robustness under adversarial attacks. However, this
approach is relatively expensive, as it requires more annotation effort than typical dataset creation
techniques; several experienced annotators are required to get a good estimate of p∗."
RELATED WORK,0.34523809523809523,"Label smoothing
Another popular method is label smoothing (Szegedy et al., 2016), discussed
in the previous sections. We believe that suitable smoothing can decrease ∥ptar −p∗∥2 to some
extent, but the lower bound of this gap should be large, because label smoothing treats each class
and each sample in the same way. Much prior research (e.g. M¨uller et al., 2019) shows that KD
usually outperforms label smoothing as well."
RELATED WORK,0.3482142857142857,"KD and ESKD
Knowledge distillation, by contrast, can provide a different ptar for each training
sample. KD was ﬁrst proposed by Hinton et al. (2015) for model compression. Later, Furlanello
et al. (2018); Cho & Hariharan (2019); Zhang et al. (2019); Yuan et al. (2020) found that distillation
can help even in “self-distillation,” when the teacher and student have identical structure. Our work
applies to both self-distillation and larger-teacher cases."
RELATED WORK,0.35119047619047616,"Another active direction in research about KD is ﬁnding explanations for its success, which is often
still considered somewhat mysterious. Tang et al. (2020) decompose the “dark knowledge” provided
by the teacher into three parts: uniform, intra-class and inter-class. This explanation also overlaps
with ours: if we observe the samples with different difﬁculty in the same class, we are discussing
intra-class knowledge; if we observe samples with two semantically similar classes, we then have
inter-class knowledge. Among previous work, the most relevant study is that of Menon et al. (2021).
Our Hypothesis 1 is suggested by their main claim, which focuses on the question of what is a good
ptar. Our work builds off of theirs by helping explain why this good ptar emerges, and how to ﬁnd
a better one. Besides, by looking deeper into the learning path of samples with different difﬁculty,
our work might shed more light on KD, anti-noisy learning, supervised learning, overﬁtting, etc."
RELATED WORK,0.3541666666666667,"Noisy label task
Learning useful information from noisy labels is a long-standing task (Angluin
& Laird, 1988; Natarajan et al., 2013). There are various ways to cope with it; for instance, Menon
et al. (2019) use gradient clipping, Patrini et al. (2017) use loss correction, Huang et al. (2020)
change the supervision during training, and Zhang et al. (2020) employ extra information. It is
possible to combine KD-based methods with traditional ones to further enhance performance; our
perspective of learning paths also provides further insight into why KD can help in this setting."
DISCUSSION,0.35714285714285715,"7
DISCUSSION"
DISCUSSION,0.3601190476190476,"In this paper, we ﬁrst claim that better supervision signal, i.e., ptar that is closer to p∗, leads to
better generalization performance; this is supported by results of Menon et al. (2021) and further
empirical suggestions given here. Based on this hypothesis, we explain why LS and KD outperform
one-hot training using experiments on a toy Gaussian dataset. To further understand how such
better supervision emerges, we directly observe the behavior of samples with different difﬁculty by
projecting them on a 2-D plane. The observed zig-zag pattern is well-explained by considering the
tradeoff between two forces, one pushing the prediction to be near that of similar inputs, the other
pushing the prediction towards its training label; we give an intuitive account based on Proposition 1
of why this leads to the observed zig-zag pattern."
DISCUSSION,0.3630952380952381,"To apply these ﬁndings on real tasks, in which the data and network are more complex, we conduct
low-pass ﬁlter on the learning path, and propose Filter-KD to further enhance ptar. Experimental
results on various settings (datasets, network structures, and label noise) not only show the advantage
of the proposed method as a method for a teacher in knowledge distillation methods, but also help
verify our analysis of how generalization and supervision work in training neural network classiﬁers."
DISCUSSION,0.36607142857142855,Published as a conference paper at ICLR 2022
DISCUSSION,0.36904761904761907,ACKNOWLEDGEMENTS
DISCUSSION,0.37202380952380953,"This research was enabled in part by support provided by the Canada CIFAR AI Chairs program,
WestGrid, and Compute Canada."
REFERENCES,0.375,REFERENCES
REFERENCES,0.37797619047619047,"Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020."
REFERENCES,0.38095238095238093,"Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343–370,
1988."
REFERENCES,0.38392857142857145,"Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information
Processing Systems, 2019."
REFERENCES,0.3869047619047619,"Jang Hyun Cho and Bharath Hariharan. On the efﬁcacy of knowledge distillation. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 4794–4802, 2019."
REFERENCES,0.3898809523809524,"Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. In Proceedings of International Conference on Machine Learning,
pp. 1607–1616. PMLR, 2018."
REFERENCES,0.39285714285714285,"Mengya Gao, Yujun Shen, Quanquan Li, Junjie Yan, Liang Wan, Dahua Lin, Chen Change Loy,
and Xiaoou Tang. An embarrassingly simple approach for knowledge distillation. arXiv preprint
arXiv:1812.01819, 2018."
REFERENCES,0.3958333333333333,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.39880952380952384,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.4017857142857143,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729–9738, 2020."
REFERENCES,0.40476190476190477,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.40773809523809523,"Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.4107142857142857,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.41369047619047616,"Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regu-
larities of labeled data in overparameterized models. Proceedings of International Conference on
Machine Learning, 2021."
REFERENCES,0.4166666666666667,"Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, and Se-Young Yun.
Understanding
knowledge distillation. https://openreview.net/forum?id=tcjMxpMJc95, 2019."
REFERENCES,0.41964285714285715,"Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.4226190476190476,"Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient
clipping mitigate label noise? In International Conference on Learning Representations, 2019."
REFERENCES,0.4255952380952381,Published as a conference paper at ICLR 2022
REFERENCES,0.42857142857142855,"Aditya Krishna Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar.
A statistical perspective on distillation. In International Conference on Machine Learning, pp.
7632–7642. PMLR, 2021."
REFERENCES,0.43154761904761907,"Rafael M¨uller, Simon Kornblith, and Geoffrey Hinton.
When does label smoothing help?
In
Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.43452380952380953,"Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. Advances in Neural Information Processing Systems, 26:1196–1204, 2013."
REFERENCES,0.4375,"Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017."
REFERENCES,0.44047619047619047,"Joshua C Peterson, Ruairidh M Battleday, Thomas L Grifﬁths, and Olga Russakovsky. Human
uncertainty makes classiﬁcation more robust.
In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 9617–9626, 2019."
REFERENCES,0.44345238095238093,"Karsten Roth, Timo Milbich, Bj¨orn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. S2sd:
Simultaneous similarity-based self-distillation for deep metric learning. Proceedings of Interna-
tional Conference on Machine Learning, 2021."
REFERENCES,0.44642857142857145,"David Ruppert. Efﬁcient estimations from a slowly convergent Robbins-Monro process. Technical
report, Cornell University Operations Research and Industrial Engineering, 1988."
REFERENCES,0.4494047619047619,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bileNetV2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4510–4520, 2018."
REFERENCES,0.4523809523809524,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Proceedings of International Conference on Learning Representations, 2015."
REFERENCES,0.45535714285714285,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015."
REFERENCES,0.4583333333333333,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the Inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818–2826, 2016."
REFERENCES,0.46130952380952384,"Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019."
REFERENCES,0.4642857142857143,"Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Under-
standing and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020."
REFERENCES,0.46726190476190477,"Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation
via label smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 3903–3911, 2020."
REFERENCES,0.47023809523809523,"Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the per-
formance of convolutional neural networks via attention transfer. Proceedings of International
Conference on Learning Representations, 2017."
REFERENCES,0.4732142857142857,"Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
ICCV, 2019."
REFERENCES,0.47619047619047616,"Zhilu Zhang and Mert R Sabuncu. Self-distillation as instance-speciﬁc label smoothing. Advances
in Neural Information Processing Systems, 2020."
REFERENCES,0.4791666666666667,"Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pﬁster. Distilling effective
supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9294–9303, 2020."
REFERENCES,0.48214285714285715,Published as a conference paper at ICLR 2022
REFERENCES,0.4851190476190476,"Code, including the experiments producing the ﬁgures and a Filter-KD implementation, is available
at https://github.com/Joshua-Ren/better_supervisory_signal."
REFERENCES,0.4880952380952381,"A
MISCELLANEOUS BACKGROUND"
REFERENCES,0.49107142857142855,"Calculation of ECE
Expected calibration error is a measurement about how well the predicted
conﬁdence represent the true correctness likelihood (Guo et al., 2017). For example, if our model
gives 100 predictions, each with conﬁdence, say, q(y = k|x) ∈[0.7, 0.8]). Then we might expect
there are 70 ∼80 correct predictions among those 100 ones. To calculate this, we ﬁrst uniformly
divide [0, 1] into M bins, with each bin represented by Im ∈
  m−1 M , m"
REFERENCES,0.49404761904761907,"M

and Bm is all the x samples
whose conﬁdence falls into Im. Then, ECE is calculated as: ECE = M
X m=1 |Bm|"
REFERENCES,0.49702380952380953,"n
|acc(Bm) −conf(Bm)| ,
(5)"
REFERENCES,0.5,"where acc(Bm) =
1
|Bm|
P"
REFERENCES,0.5029761904761905,"i∈Bm 1(ˆyi = ki), conf(Bm) =
1
|Bm|
P"
REFERENCES,0.5059523809523809,"i∈Bm q(y = ki|xi), ki is the
true label and ˆyi = argmax[q(y|xi)] is the model’s prediction. All the ECE mentioned in this paper
is calculated by setting M = 10."
REFERENCES,0.5089285714285714,"Barycentric coordinate system
When visualizing the learning path, it is problematic to directly
choose two dimensions and draw them in the Cartesian coordinate system, as illustrated in the ﬁrst
panel in Figure 8. In geometry, a suitable way to project a 3-simplex vector onto a 2-D plane is
converting it to a point in a barycentric coordinate system. Speciﬁcally, we have three basis vectors:"
REFERENCES,0.5119047619047619,"v0 = [0, 0]T, v1 = [1, 0]T, and v2 =
h
1
2,
√"
REFERENCES,0.5148809523809523,"3
2
iT
, which are the corner and two edges of an equilateral"
REFERENCES,0.5178571428571429,"triangle respectively. Then the 2D-coordinate is calculated as (x, y) = [v0; v1; v2] · q. So every
points in the left corner of a Cartesian system plane can be converted to the triangle in a barycentric
system, as illustrated in the last panel in Figure 8."
REFERENCES,0.5208333333333334,Figure 8: How to project a probability vector on to the plane of Barycentric coordinate system.
REFERENCES,0.5238095238095238,"B
RISK ESTIMATE VARIANCE BOUND"
REFERENCES,0.5267857142857143,"The following result is a generalization of Proposition 3 of Menon et al. (2021), whose proof we
replicate and extend here:"
REFERENCES,0.5297619047619048,"Proposition 2. Let L be any bounded loss, L(y, ˆy) ≤ℓ< ∞, and consider Rtar of (3). For any
predictor f : X →∆K, we have that"
REFERENCES,0.5327380952380952,"E
D′

(Rtar(f, D′) −R(f))2
≤1"
REFERENCES,0.5357142857142857,"N Var
x

pT
tarL(f(x))

+ ξ,"
REFERENCES,0.5386904761904762,Published as a conference paper at ICLR 2022
REFERENCES,0.5416666666666666,where ξ can be any of the following seven quantities:
REFERENCES,0.5446428571428571,"ℓ2K

E
x ∥ptar(x) −p∗(x)∥2
2"
REFERENCES,0.5476190476190477,"ℓ2 
E
x ∥ptar(x) −p∗(x)∥1
2"
REFERENCES,0.5505952380952381,"2ℓ2 
E
x p"
REFERENCES,0.5535714285714286,"KL(ptar(x) ∥p∗(x))
2
2ℓ2 E
x KL(ptar(x) ∥p∗(x))"
REFERENCES,0.5565476190476191,"2ℓ2 
E
x p"
REFERENCES,0.5595238095238095,"KL(p∗(x) ∥ptar(x))
2
2ℓ2 E
x KL(ptar(x) ∥p∗(x))"
REFERENCES,0.5625,"ℓ2 E
x

KL(ptar(x) ∥p∗(x)) + KL(p∗(x) ∥ptar(x))

."
REFERENCES,0.5654761904761905,"Proof. To begin,"
REFERENCES,0.5684523809523809,"E
D′

(Rtar(f, D′) −R(f))2
= Var
D′ [Rtar(f, D′) −R(f)] +

E
D′ [Rtar(f, D′) −R(f)]
2
."
REFERENCES,0.5714285714285714,"For the variance term, since R(f) is a constant and Rtar is an average of N i.i.d. terms, we get"
REFERENCES,0.5744047619047619,"Var
D′ [Rtar(f, D′) −R(f)] = 1"
REFERENCES,0.5773809523809523,"N Var
x [ptar(x)TL(f(x))]."
REFERENCES,0.5803571428571429,"The other term, as Rtar is an average of i.i.d. terms and R(f) = Ex p∗(x)⊤L(f(x)), is
 
E
D′ [Rtar(f, D′) −R(f)]
2 =

E
x (ptar(x) −p∗(x))⊤L(f(x))
2
."
REFERENCES,0.5833333333333334,"For the ﬁrst bound, we apply the Cauchy-Schwarz inequality,"
REFERENCES,0.5863095238095238,(ptar(x) −p∗(x))⊤L(f(x)) ≤∥ptar(x) −p∗(x)∥2∥L(f(x))∥2;
REFERENCES,0.5892857142857143,"since the elements of L(f(x)) are each at most ℓ, the term ∥L(f(x))∥2 is at most
√ Kℓ."
REFERENCES,0.5922619047619048,"For the other bounds, we instead apply H¨older’s inequality, yielding"
REFERENCES,0.5952380952380952,(ptar(x) −p∗(x))⊤L(f(x)) ≤∥ptar(x) −p∗(x)∥1∥L(f(x))∥∞≤ℓ∥ptar(x) −p∗(x)∥1.
REFERENCES,0.5982142857142857,"The KL bounds follow by Pinsker’s inequality and then Jensen’s inequality. The last bound, for the
Jeffreys divergence, combines the two KL bounds."
REFERENCES,0.6011904761904762,"C
TOY GAUSSIAN DATASET"
REFERENCES,0.6041666666666666,Figure 9: Correlation between test accuracy and ∥ptar −p∗∥2 for different settings.
REFERENCES,0.6071428571428571,Figure 10: Correlation between test ECE and ∥ptar −p∗∥2 for different settings.
REFERENCES,0.6101190476190477,"In Sections 2 and 3, we apply a toy Gaussian dataset to verify two facts derived from Hypothesis 1,
so as the learning path of speciﬁc samples. Here we provide more details about this dataset."
REFERENCES,0.6130952380952381,Published as a conference paper at ICLR 2022
REFERENCES,0.6160714285714286,"Run1
Run2
Run3
τ=0.5
τ=1
τ=2
τ=10
τ=0.5
τ=1
τ=2
τ=10
τ=0.5
τ=1
τ=2
τ=10
OHT
94.87
-
-
-
95.15
-
-
-
94.48
-
-
-
ESKD
95.02
95.27
95.34
95.05
95.41
95.41
95.39
94.76
94.11
94.65
94.88
94.75
FilterKD
95.18
95.87
95.66
94.81
95.11
95.56
95.71
94.81
95.09
95.31
95.22
95.16
OHT
77.85
-
-
-
78.23
-
-
-
77.99
-
-
-
ESKD
78.28
78.56
79.28
79.09
78.50
78.20
79.3
78.81
78.12
78.31
78.84
78.36
FilterKD
78.43
79.57
79.72
79.65
79.23
79.61
79.45
79.01
79.75
80.32
80.41
79.99"
REFERENCES,0.6190476190476191,Table 4: The inﬂuence of temperature in self-distillation on CIFAR10/100 dataset.
REFERENCES,0.6220238095238095,"Generate the dataset
Here, we have N samples, each sample a 3-tuple (x, y, p∗). To get one
sample, we ﬁrst select the label y = k following an uniform distribution over all K classes. After
that, we sample the input signal x|y=k ∼N(µk, σ2I), where σ is the noisy level for all the samples.
µk is the mean vector for all the samples in class k. Each µk is a 30-dim vector, in which each
dimension is randomly selected from {−δµ, 0, δµ}. Such a process is similar to selecting 30 different
features for each class. Finally, we calculate the true Bayesian probability of this sample, i.e.,
p∗(y|x)."
REFERENCES,0.625,"Calculate the ground truth probability
We use the fact that p∗(y|x) ∝p(x|y)p(y). As y follows
an uniform distribution, we have p∗(y|x) =
p(x|y=k)
P"
REFERENCES,0.6279761904761905,"j̸=k p(x|y=j). Following p(x|y = k) ∼N(µk, σ2I),"
REFERENCES,0.6309523809523809,"we ﬁnd p∗(y|x) should have a Softmax form, i.e., p =
esk
P"
REFERENCES,0.6339285714285714,"j̸=k esj . Speciﬁcally, we have:"
REFERENCES,0.6369047619047619,"p∗(y = k|x) =
esk
P"
REFERENCES,0.6398809523809523,"j̸=k esj ;
si = −1"
REFERENCES,0.6428571428571429,"2σ2 ∥x −µi∥2
2.
(6)"
REFERENCES,0.6458333333333334,"Setup of experiments in Figure 1
in this experiment, we generate a toy Gaussian dataset with
K = 3, σ = 2 and N = 105. To reduce the variance of test error, we make a train/valid/test
split with ratio [0.05 0.05, 0.9]. We train an MLP with 3 hidden layers, each with 128 hidden
units and ReLU activations. We ﬁrst conduct experiments on some baseline settings, i.e., learning
from one-hot supervision (OHT for short), from smoothed label supervision (LS for short), from
a converged teacher’s prediction (KD for short), and from an early-stopped teacher’s prediction
(ESKD for short). In OHT case, an NN is trained under the supervision of ey. If we train this
NN until the convergence of training accuracy, we obtain the ptar for the KD case. If we select the
snapshot of that NN based on the best validation accuracy, we obtain the ptar for the ESKD case. If
we directly set ptar = 0.9ey +0.1u, where u is an uniform distribution over K classes, we obtain the
supervision for LS case. With different supervisions, we train a new network with identical structure
until the validation accuracy no longer increase. Furthermore, to see a trend between generalization
ability and ∥ptar −p∗∥2, we run the experiment 200 times under different noisy supervisions."
REFERENCES,0.6488095238095238,"D
TEMPERATURE IN DIFFERENT KD METHODS"
REFERENCES,0.6517857142857143,"Temperature is an important hyper-parameter in different kinds of KD methods, which might inﬂu-
ence the performance a lot. In this appendix, we will discuss why we prefer τ = 1."
REFERENCES,0.6547619047619048,"The role played by τ
In general, the loss function in KD has the form:"
REFERENCES,0.6577380952380952,"L = β ·
 1 τ 2"
REFERENCES,0.6607142857142857,"
· H(qτ, pτ
tar) + (1 −β) · H(q, ey),
(7)"
REFERENCES,0.6636904761904762,"where β ∈[0, 1] is another hyper-parameter to trade-off the importance between one-hot label and
teacher’s predictions. Furthermore, for this loss, the gradient of L to logits z is:"
REFERENCES,0.6666666666666666,"∂L
∂zi
= β ·
1 τ"
REFERENCES,0.6696428571428571,"
· (qτ
i −pτ
tari) + (1 −β) · (qi −eyi).
(8)"
REFERENCES,0.6726190476190477,Published as a conference paper at ICLR 2022
REFERENCES,0.6755952380952381,"Figure 11: The inﬂuence of label smoothing factor on LS (ﬁrst row) and different τ on KD (other
rows) under two different settings of the toy dataset. It is clear that these hyper-parameters won’t
inﬂuence the performance too much as long as they are in a good region."
REFERENCES,0.6785714285714286,"Why we use τ = 1
The most important reason for us to choose τ = 1 (as well as β = 1), even with
the risk of providing sub-optimal performance, is that we want to observe how much enhancement
is brought by reﬁning the label. In our mind, KD’s success comes from the following two aspects:"
REFERENCES,0.6815476190476191,"(a) better label (i.e., ptar is better than ey);
(b) better learning dynamics (soften q to qτ make the training easier)."
REFERENCES,0.6845238095238095,"The ﬁrst one provides the student with more useful knowledge, and the second one helps the student
to extract it. The focus of our paper is the improvement in labels. When the temperature is not 1,
both (a) and (b) are inﬂuenced, as we are using qτ (i.e., the smoothed student output) to match pτ
tar
(i.e., the smoothed target) during training and using q to inference during testing. If we ﬁx τ = 1,
the only difference between Filter-KD, ESKD, KD, LS, and OHT training is ptar. Under such a
condition, we believe the fact that Filter-KD/ESKD outperforms OHT (strictly better in each run) is
enough to conclude KD methods can provide better labels. Furthermore, as Filter-KD is proposed
to mitigate the high-variance issue in ESKD, and we can directly observe the zig-zag learning path
of samples with bad labels, we believe it is reasonable to make the conclusion in the paper."
REFERENCES,0.6875,"0
5
10
15
20
0.70 0.75 0.80 0.85 0.90"
REFERENCES,0.6904761904761905,Test accuracy
REFERENCES,0.6934523809523809,"1
2
3
4
5 0.898 0.899 0.900 0.901 0.902"
REFERENCES,0.6964285714285714,Test accuracy
REFERENCES,0.6994047619047619,"0
5
10
15
20
Temperature"
REFERENCES,0.7023809523809523,"0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90"
REFERENCES,0.7053571428571429,Test accuracy
REFERENCES,0.7083333333333334,"1
2
3
4
5
Temperature 0.898 0.899 0.900 0.901 0.902 0.903"
REFERENCES,0.7113095238095238,Test accuracy
REFERENCES,0.7142857142857143,"Figure 12: First row: large teacher to small stu-
dent; second row: self-distill. Left column: τ in a
larger range; right column: τ in a small range."
REFERENCES,0.7172619047619048,"Furthermore, using τ ̸= 1 doesn’t substantially
change our analysis.
As illustrated in Equa-
tion (8), the gradient shows that qτ moves to-
wards pτ
tar in each update. Hence our analysis
in Proposition 1 still holds, which means the
trade-off between the two forces still exists."
REFERENCES,0.7202380952380952,"τ = 1 is reasonably good in our settings
In fact, the optimal τ depends on many set-
tings, e.g., teacher’s size, optimizer, learning
rate (scheduler), and etc: there is still no con-
sensus on what is the best choice of τ for all
settings. So for the experiments in the main
context, we ﬁx τ and ﬁne-tune other hyper-
parameters to achieve good results. However,
to verify this choice of τ is not terrible, we per-
formed a coarse grid search on both CIFAR and
toy examples. As illustrated in Table 4 and Fig-
ure 11, choosing τ ∈[1, 2] seems to be good choice: the performance doesn’t signiﬁcantly decay
until τ is too large."
REFERENCES,0.7232142857142857,"Some readers might also curious about why our results don’t match the common notion that the
optimal τ should be 4, which is ﬁrst provided in Hinton et al. (2015) and followed by much later"
REFERENCES,0.7261904761904762,Published as a conference paper at ICLR 2022
REFERENCES,0.7291666666666666,"Wrong labels
Correct labels
0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7321428571428571,zigzagness
REFERENCES,0.7351190476190477,Figure 13: The learning path of samples with correct and wrong labels.
REFERENCES,0.7380952380952381,"work (e.g. Gao et al., 2018; Zagoruyko & Komodakis, 2017; Cho & Hariharan, 2019). Some works
also use a grid search to conclude that a temperature as high as 20 should be the best choice (Yuan
et al., 2020; Kim et al., 2019). We ﬁnd this mismatch comes from the relative difference between
the network size of teacher and student. In short, when distilling from a large teacher to a small
student (as most of the cases in aforementioned works), high τ is preferred. When conducting self-
distillation, τ need not be that high. For example, Zhang et al. (2019) and Roth et al. (2021) claim
τ = 1 is the best choice, while Zhang & Sabuncu (2020) claim τ ≈2 is the best. To further
verify this, we compare the temperature trend between two cases on the toy dataset. In Figure 12,
the ﬁrst row is distilling a 10-layer, 256-width MLP to a 3-layer, 32-width MLP; the second row is
self-distillation between 3-layer, 32-width MLPs."
REFERENCES,0.7410714285714286,"At the same time, we also run a grid search the smoothing factor α of our Filter-KD in Table 5."
REFERENCES,0.7440476190476191,"Smoothing α
0.01
0.05
0.1
0.2
0.5
1 (ESKD)
FilterKD
79.50
80.00
79.83
79.59
78.48
78.39"
REFERENCES,0.7470238095238095,"Table 5: The inﬂuence of the smoothing factor in FilterKD on CIFAR100 (mean of 3 runs). For
comparison, OHT obtained 77.64."
REFERENCES,0.75,"E
HOW REPRESENTATIVE IS THE ZIG-ZAG PATTERN?"
REFERENCES,0.7529761904761905,"In the main paper, we only visualize the zig-zag learning path of a few samples in Figures 3 and 5.
The readers might then wonder whether this pattern is representative across the whole dataset. To
verify this, we deﬁne a quantitative metric called zig-zag score. Speciﬁcally, we ﬁrst calculate the
integral of each dimension of the prediction:"
REFERENCES,0.7559523809523809,"Qi∈{1,...,K} ≜ T
X"
REFERENCES,0.7589285714285714,"t=1
qt
i(xn).
(9)"
REFERENCES,0.7619047619047619,"We then use the highest Qi among i ∈{1, ..., K} \ {y}, where y is the label’s class, as the zig-zag
score. In other words, we focus on the behavior of q on those dimensions that are not the training
label. If this score is large, we might expect the neighbouring samples exert high inﬂuence on the
path, and vice versa. Note that as we have P"
REFERENCES,0.7648809523809523,"i Qi = constant for any sample (as qt is a K-simplex),
this zig-zag score might correlated with C-score of Jiang et al. (2021). However, their focuses are
different: C-score is similar to −Qi=y and focuses more on how fast the prediction converge to the
label; zig-zag score, on the other hand, is more about how much the path deviates towards a different
class, possibly the label close to p∗."
REFERENCES,0.7678571428571429,"With this score, we test the correlation between base difﬁculty and zig-zag score on toy datasets
under different settings. From Figure 14, it is safe to conclude that samples with higher base dif-
ﬁculty will have a more zig-zagging path during training. We also compare the expected zig-zag
score of the 1000 samples with ﬂipped label in CIFAR10 (recall the experiment in Figure 6). In
Figure 13, it is clear that the zig-zagness of these wrong label samples (which we are sure have high
base difﬁculty) is signiﬁcantly higher than the average."
REFERENCES,0.7708333333333334,"Finally, we also randomly select some data samples in each class of CIFAR10 and visualize their
learning paths. Figure 15 shows random samples for the noisy label experiment. It is clear that the"
REFERENCES,0.7738095238095238,Published as a conference paper at ICLR 2022
REFERENCES,0.7767857142857143,Figure 14: Correlation between base difﬁculty and zig-zag score in four different toy datasets.
REFERENCES,0.7797619047619048,"learning path of easy samples with correct labels converge fast while that of samples with wrong
labels is zig-zag. In Figure 16, which shows the learning path with high zig-zag score when the
training set is clean, we can still observe some samples with zig-zag path. These samples might be
ambiguous, with a quite ﬂat p∗. However, as we do not know the true p∗of these samples, it is
impossible to provide a result like Figure 13."
REFERENCES,0.7827380952380952,"F
DISTANCE GAP UNDER DIFFERENT SUPERVISIONS"
REFERENCES,0.7857142857142857,"Figure 2 provides the distance gap between q and corresponding p∗for each training sample in
different training stages, under the supervision of one-hot label ey. From the results in this ﬁgure, we
notice that the behavior of hard samples contributes more to the success of ESKD. Here we further
visualize how these gaps changes when the model is trained under different types of supervision,
i.e., ground truth, LS and ESKD."
REFERENCES,0.7886904761904762,"In the ﬁrst row, which is the result of label smoothing, we see the ∥ptar −p∗∥2 (i.e., red dots) has
a “V”-shape. The vertex location depends on the smoothing parameter we choose. However, as
discussed previously, label smoothing has only one parameter to control ptar, which is like using a
linear model to ﬁt a high-order function. So, although a proper smoothing value can bring better su-
pervision than one-hot label, its upper bound might be limited. Regarding the training dynamics, we
can see a similar trend as the results shown in the one-hot case, i.e., all the samples ﬁrst move down
and then converge to ptar. From the middle panel, we might expect label smoothing to outperform
the one-hot case, because the scatters are closer to the x-axis, which represents the ground truth p∗."
REFERENCES,0.7916666666666666,"The second and the third rows demonstrate the KD and ESKD case. Results in the KD case are
quite similar to the one-hot case in Figure 2, because the converged ptar is close to ey. However, we
can still expect KD to outperform one-hot training, because the ptar is closer to p∗than ey is. The
last panel in this row also demonstrates that ∥q −p∗∥2 might be smaller than ∥ptar −p∗∥2, which
can be considered as an explanation for why iterated self-distillation, e.g., Born Again Networks
(Furlanello et al., 2018), can improve the performance. For the ESKD case, we see the overﬁtting
almost disappear: the distribution of the blue points do not change too much after the early stopping
criterion is satisﬁed."
REFERENCES,0.7946428571428571,"In the last row, which illustrates the training under the supervision of p∗, we ﬁnd all the blue points
move toward the x-axis, i.e., their ptar = p∗, and ﬁnally converge to it. There is also no overﬁtting
in this case. From the last two panels, we see the disperse of blue points is signiﬁcantly smaller than
all other settings, which means the network’s prediction is quite close to p∗. Hence the performance
of this case is the best."
REFERENCES,0.7976190476190477,Published as a conference paper at ICLR 2022
REFERENCES,0.8005952380952381,(a) Samples with clean labels.
REFERENCES,0.8035714285714286,(b) Samples with wrong labels.
REFERENCES,0.8065476190476191,Figure 15: Random selection of samples in CIFAR10 with 1000 ﬂipped labels.
REFERENCES,0.8095238095238095,Published as a conference paper at ICLR 2022
REFERENCES,0.8125,Figure 16: Random selection of samples with high zig-zag score in clean CIFAR10.
REFERENCES,0.8154761904761905,Published as a conference paper at ICLR 2022
REFERENCES,0.8184523809523809,"Figure 17: Distance gap of each sample under different supervision: label smoothing, KD, ESKD,
and ground-truth training."
REFERENCES,0.8214285714285714,Published as a conference paper at ICLR 2022
REFERENCES,0.8244047619047619,"G
MORE ABOUT THE DECOMPOSITION AND NTK MODEL"
REFERENCES,0.8273809523809523,"Proof of Proposition 1. Recall z(x) = f(w, x) is the vector of output logits, and q = Softmax(z)
is the output probability. We are taking a step of SGD observing xu, and observing the change in
predictions on xo. We begin with a Taylor expansion,"
REFERENCES,0.8303571428571429,"qt+1(xo)
|
{z
}
K×1"
REFERENCES,0.8333333333333334,"−qt(xo)
| {z }
K×1"
REFERENCES,0.8363095238095238,"= ∇wqt(xo)|wt
|
{z
}
K×d"
REFERENCES,0.8392857142857143,"·
 
wt+1 −wt"
REFERENCES,0.8422619047619048,"|
{z
}
d×1"
REFERENCES,0.8452380952380952,"+ O
 
∥wt+1 −wt∥2
."
REFERENCES,0.8482142857142857,"To evaluate the leading term, we plug in the deﬁnition of SGD and repeatedly use the chain rule:"
REFERENCES,0.8511904761904762,"∇wqt(xo)|wt
|
{z
}
K×d"
REFERENCES,0.8541666666666666,"·
 
wt+1 −wt"
REFERENCES,0.8571428571428571,"|
{z
}
d×1"
REFERENCES,0.8601190476190477,"=
 
∇zqt(xo)|zt
|
{z
}
K×K"
REFERENCES,0.8630952380952381,"· ∇wzt(xo)|wt
|
{z
}
K×d"
REFERENCES,0.8660714285714286,"
·
 
−η ∇wL(xu)|wt
|
{z
}
1×d T"
REFERENCES,0.8690476190476191,"= ∇zqt(xo)|zt
|
{z
}
K×K"
REFERENCES,0.8720238095238095,"· ∇wzt(xo)|wt
|
{z
}
K×d"
REFERENCES,0.875,"·
 
−η∇zL(xu)|zt
|
{z
}
1×K"
REFERENCES,0.8779761904761905,"· ∇wzt(xu)|wt
|
{z
}
K×d T"
REFERENCES,0.8809523809523809,"= −η ∇zqt(xo)|zt
|
{z
}
K×K"
REFERENCES,0.8839285714285714,"·

∇wzt(xo)|wt
|
{z
}
K×d"
REFERENCES,0.8869047619047619,"·
 
∇wzt(xu)|wtT
|
{z
}
d×K"
REFERENCES,0.8898809523809523,"
·
 
∇zL(xu)|ztT
|
{z
}
K×1
= η · At(xo) · Kt(xo, xu) ·
 
ptar(xu) −qt(xu)

."
REFERENCES,0.8928571428571429,"For the higher-order term, using as above that"
REFERENCES,0.8958333333333334,"wt+1 −wt = −η∇wzt(xu)|T
wt ·
 
ptar(xu) −qt(xu)
"
REFERENCES,0.8988095238095238,"and noting that, since the vectors are probabilities, ∥ptar(xu) −qt(xu)∥is bounded, we have that"
REFERENCES,0.9017857142857143,"O
 
∥wt+1−wt∥2
= O
 
η2 ∥(∇wz(xu)|wt)T∥2
op ∥ptar(xu)−qt(xu)∥2
= O
 
η2∥∇wz(xu)∥2
op

."
REFERENCES,0.9047619047619048,"In the decomposition,"
REFERENCES,0.9077380952380952,At(xo) = ∇zqt(xo)|zt =  
REFERENCES,0.9107142857142857,"q1(1 −q1)
−q1q2
· · ·
−q1qK
−q2q1
q2(1 −q2)
· · ·
−q2qK
...
...
...
...
−qKq1
−qKq2
· · ·
qK(1 −qK) "
REFERENCES,0.9136904761904762,",
(10)"
REFERENCES,0.9166666666666666,"which is a symmetric positive semi-deﬁnite (PSD) matrix8 with trace tr(At(xo)) = 1 −PK
i=1 q2
i .
As we have P"
REFERENCES,0.9196428571428571,"i qi = 1, it is easy to check the trace of this matrix is larger at the beginning of training
(when q tends to be ﬂat) than that at the end of training (q tends to be peaky), as illustrated by most
panels in Figure 18. Given that the trace of a matrix is the sum of its eigenvalues and At(xo) is PSD,
smaller tr(At(xo)) = 1 −PK
i=1 q2
i means this matrix will tend to shrink its inputs more. Hence the
change in predictions tends to decrease when qt becomes more peaky."
REFERENCES,0.9226190476190477,"The second term in that expression, Kt(xo, xu), is the outer product of gradients at xo and xu.
Intuitively, if their gradients have similar directions, this matrix is large, and vice versa. This matrix
is known as the empirical neural tangent kernel, and it can change through the course of training
as the network’s notion of “similarity” evolves. For appropriately initialized very wide networks
trained with very small learning rates, Kt remains almost constant during the course of training, and
is almost independent of the initialization of the network parameters; the kernel it converges to is
known as the neural tangent kernel (Jacot et al., 2018; Arora et al., 2019)."
REFERENCES,0.9255952380952381,"Learning Dynamics of q(xo)
In Proposition 1, we decompose qt+1(xo)−qt(xo) into three parts;
we use this to analyze what the learning path of a training sample might be like in Section 3.3. Here
we will provide more detailed illustration of the two groups of force imposed on q(xo)."
REFERENCES,0.9285714285714286,"Speciﬁcally, qt+1(xo) −qt(xo) will be inﬂuenced by two variables, i.e., updating sample xu and
time t. We discuss their effects separately. For xu, only the last two terms, i.e., Kt(xo, xu) and
(ptar(xu) −qt(xu)) depends on it."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9315476190476191,"8The matrix A can be observed to be the covariance matrix of a categorical distribution with item probabil-
ities q, and hence PSD."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9345238095238095,Published as a conference paper at ICLR 2022
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9375,"Figure 18: Upper panels: how tr(At(xn)) changes during training. Each panel represents a speciﬁc
xn. The panels are ordered by their integral difﬁculty, from left-to-right and up-to-down. The x-
axis is the number of epochs, and the y-axis is tr(At(xn)). Lower panels: the correlation between
cos(xo, xu) and tr(K0). Panels are ordered by the integral difﬁculty of xo. The subtitle of the panel
gives the sample ID, the class it belongs to (i.e., A, B, or C) and the color of their corresponding
class (i.e., A is blue, B is orange and C is green)."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9404761904761905,"In Section 3.3, we claim that if xo and xu are similar, the norm of K0(xo, xu) might be large, and vice
versa. Here we empirically demonstrate this using a toy Gaussian dataset, as illustrated in Figure 18.
The ﬁgure shows how the similarity between xo and xu correlates with tr(K0(xo, xu)). Each panel
represents one xo, which is claimed in the title of the subﬁgures. The x-axis represents the rank of
the cosine similarity between observed xo and all N training samples (including itself). The y-axis
is the trace of K0(xo, xu). The color of each scatter point is the true label of xu. From the ﬁgure, we
can observe a clear decreasing trend, which means smaller similarity leads to larger tr(K0(xo, xu)).
Additionally, the term (ptar(xu) −qt(xu)) provides a direction that qt(xo) should move towards."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9434523809523809,"We claim in Section 3.3 that at any point in the input space, the labels of these input samples might
follow the ground truth distribution, i.e., p∗(y|x). Hence most of the neighbouring x might pull
q(xo) towards its ground truth p∗(y|xo). We will discuss the norm of this term when discussing the
inﬂuence of t. In short, at any time, the neighboring xu will impose stronger force on xo and the
direction of the force roughly points to the ground truth p∗(y|xo)."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9464285714285714,Published as a conference paper at ICLR 2022
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9494047619047619,"As discussed, Kt is roughly constant over t in the very-wide limit. For ﬁnite width networks, how-
ever, it adapts to reﬂect the network’s new “understanding” of similarities. For instance, it might
learn that certain types of images are more semantically similar than the randomly-initialized net-
work thought. This does not fundamentally change our intuitions as long as it doesn’t happen too
often, but could potentially lead to more complicated zig-zag patterns as the network’s estimate of
p∗from neighboring points perhaps improves over time."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9523809523809523,"Over the course of training, At(xo) and (ptar(xu) −qt(xu)) will also change. In practical regimes,
none of these terms have an easy analytical expression w.r.t. t: qt is quite complicated. Thus, we pro-
vide some intuition, with experimental veriﬁcation. In Figure 18, we show how tr(At(xo)) depends
on qt: ﬂat qt leads to larger trace. A similar trend also exists in the norm of (ptar(xu) −qt(xu)).
As the initialized q tend to be ﬂat, updates of any samples will inﬂuence network’s parameters a
lot. When the training progresses, those easy samples converge fast, so their ∥ptar(xu) −qt(xu)∥2
and tr(At) become small. However, as the qt for the hard sample is still far away from its ey, the
large ∥ptar(xu) −qt(xu)∥2 and tr(At) will ﬁnally drag qt back towards the one-hot distribution, as
illustrated in Figure 3."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9553571428571429,"H
COMPARISON TO LIU ET AL. (2020) AND HUANG ET AL. (2020)"
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9583333333333334,"The main claim of this paper is that better supervisory signals can enhance the generalization ability
of the trained model. Inspired by the success of KD, we ﬁnd that the neural network can sponta-
neously reﬁne those “bad” labels during training by observing their learning path. The learning path
of those hard samples will ﬁrst move towards their true p(y|x) and then converge to their label ptar
or ey. We explain why this phenomenon occurs by expanding the gradients of each training sample.
This phenomenon is also explained in Proposition 1, and formally proved for a particular softmax
regression model by Liu et al. (2020). As a complement, we propose an explanation using an NTK
model, and experimentally verify it by observing the learning path and distance gap during training."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9613095238095238,"Another difference between these two works is that we consider the problem of reﬁning supervisory
signals, while Liu et al. (2020) consider correcting wrong labels (a special case of “bad” supervi-
sion). Our work provides additional emphasis and empirical study for the clean-label case."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9642857142857143,"Regarding the algorithm, Liu et al. (2020) design an effective regularization term inspired by early
stopping regularization. They apply exponential moving average (EMA) on the model’s output when
calculating this regularization term to further enhance the performance. This method is similar to
that proposed by Huang et al. (2020), who switch between optimizing the training loss and an
objective based on the EMA of the model over the course of training."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9672619047619048,"Although it bears signiﬁcant similarity to these methods, Filter-KD does not change the course of
training the teacher model. Rather, we propose (based on the high variance of the zig-zag learning
paths) to simply use the the EMA of that model’s outputs as a teacher for later distillation."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9702380952380952,"We suspect that these three algorithms work because of essentially the same underlying principle,
whether we think of this as being based on the zig-zag learning path or as early-stopping regulariza-
tion. We expect that this principle will be helpful moving forward in the ﬁeld’s understanding of the
learning dynamics of SGD methods for neural networks."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9732142857142857,"I
LOW PASS FILTER ON NETWORK PARAMETERS"
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9761904761904762,"In Section 4, we point out the high variance issue of the traditional KD methods after observing
the learning path of those hard samples. We then propose a Filter-KD algorithm to smooth the
output of each training sample during training. Such a low pass ﬁltering method is quite similar
to the momentum mechanism used in self-supervised learning, e.g., from the classic method of
Ruppert (1988) to MOCO (He et al., 2020), which conduct low pass ﬁltering on each parameter of
the network. As mentioned before, the proposed Filter-KD algorithm might require more memory
when the dataset becomes larger, because qsmooth would record every training sample’s prediction."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9791666666666666,"Conducting low pass ﬁltering on network parameters might be a good way to solve this. To verify
whether this method works, we train a ResNet18 on CIFAR100, using one-hot supervision. At the
beginning of training, we copy the training model and call it tracking model. At the end of each"
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9821428571428571,Published as a conference paper at ICLR 2022
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9851190476190477,"Figure 19: Behavior of the tracking model (with low pass ﬁlter on each parameters). ResNet18
trained for 150 epochs on CIFAR100."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9880952380952381,"update (i.e., each batch), the parameters of training model is updated as usual while the tracking
model’s parameters are updated with momentum. Speciﬁcally, for the training model, wt+1
train =
wt
train + η∇L, for the tracking model, wt+1
track = (1 −α)wt
track + αwt+1
train. We train the model for
150 epochs, and show the learning curve of test accuracy in the left panel of Figure 19. We see the
performance of the tracking model converges faster than the training model, which is reasonable
because ﬁltering parameters is regularizing the training process. However, the converging accuracy
of this two models are the same. At the same time, we ﬁnd this tracking model is not as good as
qsmooth when teaching a new model, as illustrated in the right panel of Figure 19."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9910714285714286,"As this paper mainly focuses on the behavior of the model’s prediction, the discussion and ex-
periments of ﬁltering in parameter space is limited. However, as many papers demonstrates the
effectiveness of this momentum mechanism, we think it is important to explore the relationship
further."
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9940476190476191,"Test ACC
Test ECE
Run1
Run2
Run3
Run4
Run5
Run1
Run2
Run3
Run4
Run5
OHT
95.35
95.30
95.42
95.23
95.42
0.027
0.027
0.026
0.025
0.025
KD
95.30
95.38
95.42
95.44
95.42
0.027
0.027
0.027
0.026
0.026
ESKD
95.29
95.41
95.39
95.58
95.42
0.026
0.029
0.025
0.028
0.027
FilterKD
95.66
95.68
95.49
95.76
95.58
0.005
0.006
0.008
0.011
0.006
OHT
78.27
78.31
77.97
77.78
78.02
0.053
0.053
0.052
0.053
0.054
KD
78.64
78.55
78.03
78.18
78.49
0.060
0.057
0.062
0.066
0.059
ESKD
78.84
78.73
78.85
78.97
78.74
0.065
0.066
0.067
0.070
0.066
FilterKD
79.87
79.93
80.19
80.22
80.23
0.028
0.026
0.034
0.028
0.031"
THE MATRIX A CAN BE OBSERVED TO BE THE COVARIANCE MATRIX OF A CATEGORICAL DISTRIBUTION WITH ITEM PROBABIL-,0.9970238095238095,"Table 6: Each run of results in Table 3. In each run, the initialization of student networks for different
methods are controlled to be the same."
