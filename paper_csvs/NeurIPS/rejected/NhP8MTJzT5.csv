Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010141987829614604,"The advancement of autonomous driving is increasingly reliant on high-quality
1"
ABSTRACT,0.002028397565922921,"annotated datasets, especially in the task of 3D occupancy prediction, where the
2"
ABSTRACT,0.0030425963488843813,"occupancy labels require dense 3D annotation with significant human effort. In
3"
ABSTRACT,0.004056795131845842,"this paper, we propose SytheOcc, which denotes a diffusion model that Synthesize
4"
ABSTRACT,0.005070993914807302,"photorealistic and geometric-controlled images by conditioning Occupancy labels
5"
ABSTRACT,0.006085192697768763,"in driving scenarios. This yields an unlimited amount of diverse, annotated, and
6"
ABSTRACT,0.007099391480730223,"controllable datasets for applications like training perception models and simu-
7"
ABSTRACT,0.008113590263691683,"lation. SyntheOcc addresses the critical challenge of how to efficiently encode
8"
ABSTRACT,0.009127789046653144,"3D geometric information as conditional input to a 2D diffusion model. Our ap-
9"
ABSTRACT,0.010141987829614604,"proach innovatively incorporates 3D semantic multi-plane images (MPIs) to pro-
10"
ABSTRACT,0.011156186612576065,"vide comprehensive and spatially aligned 3D scene descriptions for conditioning.
11"
ABSTRACT,0.012170385395537525,"As a result, SyntheOcc can generate photorealistic multi-view images and videos
12"
ABSTRACT,0.013184584178498986,"that faithfully align with the given geometric labels (semantics in 3D voxel space).
13"
ABSTRACT,0.014198782961460446,"Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes
14"
ABSTRACT,0.015212981744421906,"dataset prove its effectiveness in generating controllable occupancy datasets that
15"
ABSTRACT,0.016227180527383367,"serve as an effective data augmentation to perception models.
16"
INTRODUCTION,0.017241379310344827,"1
Introduction
17"
INTRODUCTION,0.018255578093306288,"With the rapid development of generative models, they have shown realistic image synthesis and
18"
INTRODUCTION,0.019269776876267748,"diverse controllability. This progress has opened up new avenues for dataset generation in autonomous
19"
INTRODUCTION,0.02028397565922921,"driving [5, 12, 23, 30]. The task of dataset generation is usually modeled as controllable image
20"
INTRODUCTION,0.02129817444219067,"generation, where the ground truth (e.g. 3D Box) is employed to control the generation of new datasets
21"
INTRODUCTION,0.02231237322515213,"in downstream tasks (e.g. 3D detection). This approach helps to mitigate the data collection and
22"
INTRODUCTION,0.02332657200811359,"annotation effort as it can generate labeled data for free. However, a novel task of vital importance,
23"
INTRODUCTION,0.02434077079107505,"occupancy prediction [24, 27], poses new challenges for dataset generation compared with 3D
24"
INTRODUCTION,0.02535496957403651,"detection. It requires finer and more nuanced geometry controllability, which refers to use the
25"
INTRODUCTION,0.02636916835699797,"occupancy state and semantics of voxels in the whole 3D space to control the image generation.
26"
INTRODUCTION,0.02738336713995943,"We argue that solving this problem not only allows us to synthesize occupancy datasets, but also
27"
INTRODUCTION,0.028397565922920892,"empowers valuable applications such as editing geometry to generate rare data for corner case
28"
INTRODUCTION,0.029411764705882353,"evaluation, as shown in Fig. 1. In the following, we first illustrate why prior work struggles to achieve
29"
INTRODUCTION,0.030425963488843813,"the above objective, and then demonstrate how we address these challenges.
30"
INTRODUCTION,0.03144016227180527,"In the area of diffusion models, several representative works have displayed high-quality image
31"
INTRODUCTION,0.032454361054766734,"synthesis; however, they are constrained by limited 3D controllability: they are incapable of editing 3D
32"
INTRODUCTION,0.033468559837728194,"voxels for precise control. For example, BEVGen [23] generates street view images by conditioning
33"
INTRODUCTION,0.034482758620689655,"BEV layouts using diffusion models. MagicDrive [5] extend BEVGen and additionally converts the
34"
INTRODUCTION,0.035496957403651115,"3D box parameters into text embedding through Fourier mapping that is similar to NeRF [19], and
35"
INTRODUCTION,0.036511156186612576,"uses cross-attention to learn conditional generation. Although these methods achieve satisfactory
36"
INTRODUCTION,0.037525354969574036,"results in image generation, their 3D controllability is inherently limited. These approaches are
37"
INTRODUCTION,0.038539553752535496,Add traffic cone
INTRODUCTION,0.03955375253549696,"Testing
End-to-end"
INTRODUCTION,0.04056795131845842,Planner
INTRODUCTION,0.04158215010141988,"VAD/
UniAD"
INTRODUCTION,0.04259634888438134,Corner
INTRODUCTION,0.0436105476673428,"Case
Evaluation"
INTRODUCTION,0.04462474645030426,Predicted waypoints
INTRODUCTION,0.04563894523326572,Predicted waypoints
INTRODUCTION,0.04665314401622718,Original occupancy
INTRODUCTION,0.04766734279918864,"Edited occupancy
Generated image"
INTRODUCTION,0.0486815415821501,Generated image
INTRODUCTION,0.04969574036511156,"Geometric
Controlled
Generation"
INTRODUCTION,0.05070993914807302,SyntheOcc
INTRODUCTION,0.05172413793103448,User Editing
INTRODUCTION,0.05273833671399594,"To Create
Corner Case"
INTRODUCTION,0.0537525354969574,"Figure 1: A showcase of application of SytheOcc. We enable geometric-controlled generation that
conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we
create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of
autonomous systems, such as the end-to-end planner VAD [9], in simulated corner case scenes."
INTRODUCTION,0.05476673427991886,"restricted to manipulating the scene in types of 3D boxes and BEV layouts, and hardly adapt to finer
38"
INTRODUCTION,0.055780933062880324,"geometry control such as editing the shape of objects and scenes. Meanwhile, they usually convert
39"
INTRODUCTION,0.056795131845841784,"conditional input into 1D embedding that aligns with prompt embedding, which is less effective in
40"
INTRODUCTION,0.057809330628803245,"3D-aware generation due to lack of spatial alignment with the generated images. This limitation
41"
INTRODUCTION,0.058823529411764705,"hinders their utility in downstream applications, such as occupancy prediction and editing scene
42"
INTRODUCTION,0.059837728194726165,"geometry to create long-tailed scenes, where granular volumetric control is paramount in both tasks.
43"
INTRODUCTION,0.060851926977687626,"ControlNet [41] and GLIGEN [14] is another type of prominent method in the field of controllable
44"
INTRODUCTION,0.061866125760649086,"image generation. These approaches exhibit several desirable attributes in terms of controllability.
45"
INTRODUCTION,0.06288032454361055,"They leverage conditional images such as semantic masks for control, thereby offering a unified
46"
INTRODUCTION,0.06389452332657201,"framework to manipulate both foreground and background. However, despite its precise spatial
47"
INTRODUCTION,0.06490872210953347,"control, ControlNet does not align with our specific requirements. Their conditions of pixel-level
48"
INTRODUCTION,0.06592292089249494,"images differ fundamentally from what we require in 3D contexts. Our experimental results also find
49"
INTRODUCTION,0.06693711967545639,"that ControlNet struggles to handle overlapping objects with varying depths (see Fig. 6 (a)), as it only
50"
INTRODUCTION,0.06795131845841786,"utilizes an ambiguous 2D semantic map as conditional input. As a result, it is non-trivial to extend
51"
INTRODUCTION,0.06896551724137931,"the ControlNet framework and convey their desirable attributes for 3D conditioning.
52"
INTRODUCTION,0.06997971602434078,"To address the above challenges, we propose an innovative representation, 3D semantic multi-plane
53"
INTRODUCTION,0.07099391480730223,"images (MPIs), which contribute to image generation with finer geometric control. In detail, we
54"
INTRODUCTION,0.0720081135902637,"employ multi-plane images [43] to represent the occupancy, where each plane represents a slice of
55"
INTRODUCTION,0.07302231237322515,"semantic label at a specific depth. Our 3D semantic MPIs not only preserve accurate and authentic 3D
56"
INTRODUCTION,0.07403651115618662,"information, but also keep pixel-wise alignment with the generated images. We additionally introduce
57"
INTRODUCTION,0.07505070993914807,"the MPI encoder to encode features, and the reweighing methods to ease the training with long-tailed
58"
INTRODUCTION,0.07606490872210954,"cases. As a collection, our framework enables 3D geometry and semantic control for image generation
59"
INTRODUCTION,0.07707910750507099,"and further facilitates corner case evaluation as depicted in Fig. 1. Finally, experimental results
60"
INTRODUCTION,0.07809330628803246,"demonstrate that our synthetic data achieve better recognizability, and are effective in improving the
61"
INTRODUCTION,0.07910750507099391,"perception model on occupancy prediction. In summary, our contributions include:
62"
INTRODUCTION,0.08012170385395538,"• We present SytheOcc, a novel image generation framework to attain finer and precise 3D
63"
INTRODUCTION,0.08113590263691683,"geometric control, thereby unlocking a spectrum of applications such as 3D editing, dataset
64"
INTRODUCTION,0.0821501014198783,"generation, and long-tailed scene generation.
65"
INTRODUCTION,0.08316430020283976,"• Incorporating the proposed 3D semantic MPI, MPI encoder, and reweighing strategy, we
66"
INTRODUCTION,0.08417849898580122,"deliver a substantial advancement in image quality and recognizability over prior works.
67"
INTRODUCTION,0.08519269776876268,"• Our extensive experimental results demonstrate that our synthetic data yields an effective
68"
INTRODUCTION,0.08620689655172414,"data augmentation in the realm of 3D occupancy prediction.
69"
RELATED WORK,0.0872210953346856,"2
Related Work
70"
RELATED WORK,0.08823529411764706,"2.1
3D Occupancy Prediction
71"
RELATED WORK,0.08924949290060852,"The task of 3D occupancy prediction aims to predict the occupancy status of each voxel in 3D space,
72"
RELATED WORK,0.09026369168356999,"as well as its semantic label if occupied. Compared with previous perception methods like 3D object
73"
RELATED WORK,0.09127789046653144,"detection, occupancy prediction offers a more detailed and nuanced understanding of the environment,
74"
RELATED WORK,0.0922920892494929,"as it provides finer geometric details, is capable of handling general, out-of-vocabulary objects, and
75"
RELATED WORK,0.09330628803245436,"finally, enriches the planning stack with comprehensive 3D information. Early methods exploited
76"
RELATED WORK,0.09432048681541583,"LiDAR as inputs to complete the 3D occupancy of the entire 3D scene [18, 33]. Recent methods
77"
RELATED WORK,0.09533468559837728,"began to explore the more challenging vision-based 3D occupancy prediction [24,25,27,29]. By
78"
RELATED WORK,0.09634888438133875,"predicting the geometric and semantic properties of both dynamic and static elements, 3D occupancy
79"
RELATED WORK,0.0973630831643002,"prediction offers a more comprehensive understanding of the surrounding environment.
80"
DIFFUSION-BASED IMAGE GENERATION,0.09837728194726167,"2.2
Diffusion-based Image Generation
81"
DIFFUSION-BASED IMAGE GENERATION,0.09939148073022312,"Recent advancements in diffusion models (DMs) have achieved remarkable progress in image
82"
DIFFUSION-BASED IMAGE GENERATION,0.10040567951318459,"generation. In particular, Stable Diffusion (SD) [21] employs DMs within the latent space of
83"
DIFFUSION-BASED IMAGE GENERATION,0.10141987829614604,"autoencoders, striking a balance between computational efficiency and high image quality. Beyond
84"
DIFFUSION-BASED IMAGE GENERATION,0.10243407707910751,"text control, there is also the introduction of additional control signals. A noteworthy work is
85"
DIFFUSION-BASED IMAGE GENERATION,0.10344827586206896,"ControlNet [41], which incorporates a trainable copy of the SD encoder to extract the feature of
86"
DIFFUSION-BASED IMAGE GENERATION,0.10446247464503043,"conditional images and adds it to the UNet feature. It significantly enhances the controllability and
87"
DIFFUSION-BASED IMAGE GENERATION,0.10547667342799188,"unlocking pathways for advanced applications. We refer readers to recent survey [35] for more details.
88"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.10649087221095335,"2.3
Image Generation in Autonomous Driving
89"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.1075050709939148,"As training neural networks relies heavily on labeled data, numerous studies are delving into dataset
90"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.10851926977687627,"generation to boost training. Lift3D [12] designs generative NeRF to synthesize labeled datasets
91"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.10953346855983773,"for 3D detection for the first time. Several other works employ BEV layouts to synthesize image
92"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.1105476673427992,"data, proving beneficial for perception models. For example, BEVGen [23] conditions BEV layouts
93"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11156186612576065,"to generate multi-view street images, while BEVControl [34] separately generates foregrounds and
94"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11257606490872211,"backgrounds from BEV layouts. MagicDrive [5] generates images with 3D geometry controls by
95"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11359026369168357,"independently encoding objects and maps through a text encoder or map encoder. Compared with
96"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11460446247464504,"MagicDrive, our geometry control is characterized by a more detailed and lossless representation of
97"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11561866125760649,"3D scenes for control, which poses significant challenges than projected layout or box embedding.
98"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11663286004056796,"Recently, DriveDreamer [26], DrivingDiffusion [13], Drive-WM [28] and Panacea [30] use a Con-
99"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11764705882352941,"trolNet framework, which involves projecting bounding boxes and road maps onto 2D FoV images as
100"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11866125760649088,"a conditioning input. This approach has proven to be effective for geometric control. However, it is
101"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.11967545638945233,"limited in that it only achieves alignment at the 2D-pixel level. Consequently, this method falls short
102"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.1206896551724138,"in capturing the depth hierarchy and fails to account for the occlusion relationships present in the 3D
103"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12170385395537525,"real world. Besides, adding a depth channel like Panacea [30] may address the limitations of depth
104"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12271805273833672,"order, but it discards the occluded part and only contains partial observation. UrbanGiraffe [37] train
105"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12373225152129817,"a generative NeRF to perform image generation. WoVoGen [17] creates a 4D world volume feature
106"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12474645030425964,"using occupancy to guide the generation, but seems to rely on object mask guidance.
107"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.1257606490872211,"As described above, most of the prior work is restricted by only modeling a projected primitive of 3D
108"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12677484787018256,"boxes and road maps as conditions. They suffer from ill-posed un-projection ambiguity. In contrast,
109"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12778904665314403,"we model 3D occupancy labels as conditions, as they provide finer geometric details and semantic
110"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12880324543610547,"information. However, designing an input representation of 3D occupancy labels into a 2D diffusion
111"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.12981744421906694,"model is challenging. In this paper, we propose a novel representation: 3D semantic Multi-Plane
112"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.1308316430020284,"Images (MPIs) as conditional inputs, which not only provide spatial alignment that improves visual
113"
IMAGE GENERATION IN AUTONOMOUS DRIVING,0.13184584178498987,"consistency, but also encode comprehensive 3D geometric information including occluded parts.
114"
METHOD,0.1328600405679513,"3
Method
115"
METHOD,0.13387423935091278,"Overview
The overview of our method is depicted in Fig. 2. Built upon the SD pipeline, we
116"
METHOD,0.13488843813387424,"aim to perform geometry-controlled image generation by conditioning on 3D geometry labels with
117"
METHOD,0.1359026369168357,"semantics (occupancy labels). One requirement is that the images should faithfully align with the
118"
METHOD,0.13691683569979715,"given label. This task is more challenging than conditioned on 3D box due to the sparse and irregular
119"
METHOD,0.13793103448275862,"nature of occupancy. We first discuss how to efficiently represent occupancy in Sec. 3.2, followed
120"
METHOD,0.1389452332657201,"by our designed MPI encoder to enhance generation quality in Sec. 3.3, and reweighing strategy to
121"
METHOD,0.13995943204868155,"handle the long-tailed depth and category in Sec. 3.5.
122"
METHOD,0.140973630831643,"3.1
Representation of Condition: Local Control Aligns Better than Global Control
123"
METHOD,0.14198782961460446,"One of the key challenges is how to represent our conditional occupancy input. A straightforward
124"
METHOD,0.14300202839756593,"method [3,5] is to convert the 3D occupancy voxel to 1D global embedding that is similar to text
125"
METHOD,0.1440162271805274,"embedding, and then use cross-attention to learn controllable generation. However, these global
126"
METHOD,0.14503042596348883,"methods can be less effective when dealing with dense or irregular data due to the following reasons:
127"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1460446247464503,"3D Semantic 
Multiplane Images"
"D SEMANTIC 
MULTIPLANE IMAGES",0.14705882352941177,"Prompt: show a realistic
street view image"
"D SEMANTIC 
MULTIPLANE IMAGES",0.14807302231237324,"N×D×H×W
Occupancy Label …"
"D SEMANTIC 
MULTIPLANE IMAGES",0.14908722109533468,Latent Noise
"D SEMANTIC 
MULTIPLANE IMAGES",0.15010141987829614,"N×4×H×W
…"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1511156186612576,"VAE 
Decoder"
"D SEMANTIC 
MULTIPLANE IMAGES",0.15212981744421908,Conv1×1
"D SEMANTIC 
MULTIPLANE IMAGES",0.15314401622718052,Conv1×1 ReLU
"D SEMANTIC 
MULTIPLANE IMAGES",0.15415821501014199,UNet Encoder
"D SEMANTIC 
MULTIPLANE IMAGES",0.15517241379310345,MPI Encoder
"D SEMANTIC 
MULTIPLANE IMAGES",0.15618661257606492,"Intra
View
Diffusion UNet"
"D SEMANTIC 
MULTIPLANE IMAGES",0.15720081135902636,"Cross
View/
Frame"
"D SEMANTIC 
MULTIPLANE IMAGES",0.15821501014198783,"Training 
Perception 
Model"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1592292089249493,"Corner 
Case 
Evaluation"
"D SEMANTIC 
MULTIPLANE IMAGES",0.16024340770791076,"Long-tailed
Scene 
Generation
Optional 
User Edit:"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1612576064908722,"Add
Delete …."
"D SEMANTIC 
MULTIPLANE IMAGES",0.16227180527383367,Applications
"D SEMANTIC 
MULTIPLANE IMAGES",0.16328600405679514,Multi-view images ……
"D SEMANTIC 
MULTIPLANE IMAGES",0.1643002028397566,Simulation
"D SEMANTIC 
MULTIPLANE IMAGES",0.16531440162271804,"Figure 2: The overall architecture of SytheOcc. We achieve 3D geometric control in image generation
by utilizing our proposed 3D semantic multiplane images to encode scene occupancy. In our
framework, we can edit the occupied state and semantics of every voxel in 3D space to control the
image generation, thereby opening up a wide spectrum of applications as shown in the top right."
"D SEMANTIC 
MULTIPLANE IMAGES",0.1663286004056795,"(i) They perform controllable generation through hard encoding the spatial relationship between 1D
128"
"D SEMANTIC 
MULTIPLANE IMAGES",0.16734279918864098,"global embedding and 2D UNet features. (ii) Ignore the underlying geometry alignment between the
129"
"D SEMANTIC 
MULTIPLANE IMAGES",0.16835699797160245,"conditional input and the generated image. In contrast, local methods like ControlNet, directly add
130"
"D SEMANTIC 
MULTIPLANE IMAGES",0.16937119675456389,"spatial features to the UNet features, providing 2D local control with pixel-level spatial alignment.
131"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17038539553752535,"They are better than the global method (see Tab. 1), but suffer from 3D ambiguity (see Fig. 6 (a)).
132"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17139959432048682,"Consequently, this comparison motivates us to seek a more compact and efficient manner to encode
133"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1724137931034483,"and condition our 3D occupancy labels.
134"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17342799188640973,"3.2
Represent Occupancy as 3D Semantic Multiplane Images
135"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1744421906693712,"It is non-trivial to design a 3D representation for conditioning. To efficiently store both the semantic
136"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17545638945233266,"and geometric information of the irregular occupancy input, we propose to use multiplane images
137"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17647058823529413,"(MPIs) [43] as representation. An MPI is composed of a series of fronto-parallel RGBA layers within
138"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17748478701825557,"the frustum of the source camera with a specific viewpoint. These planes are arranged at varying
139"
"D SEMANTIC 
MULTIPLANE IMAGES",0.17849898580121704,"depths, from dmin to dmax, starting from the nearest to the farthest. Each layer of these images
140"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1795131845841785,"contains both an RGB image and an alpha map, which collectively capture the visual and geometric
141"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18052738336713997,"details of the scene at the respective depth. In our work, instead of storing RGB value and alpha map
142"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1815415821501014,"in the original MPI, we store our 3D semantic labels. Each layer of MPI represents the semantic
143"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18255578093306288,"index at the corresponding depth. We display the colored MPI in the top row of Fig. 2 for visual
144"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18356997971602435,"clarity, but we actually use the integer index for learning. We obtain our 3D semantic MPI by:
145"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1845841784989858,"Pl = (u × dl, v × dl, dl)T , dl = dmin + (dmax −dmin) × l/D,
(1)"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18559837728194725,"MPIn,l = Interpolate(Occupancy, Tn · K−1
n
· Pl),
(2)
MPI = Concatenate(MPIi,j), i ∈(0, N), j ∈(0, D),
(3)"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18661257606490872,"where (u, v) is a pixel coordinate in image space, dl is depth value of the lth layer, n denotes the nth
146"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1876267748478702,"camera view. This equation implies we first back project points P in camera frustum space (u, v, d)
147"
"D SEMANTIC 
MULTIPLANE IMAGES",0.18864097363083165,"to Euclid space (x, y, z) by multiplying inverse intrinsic K−1. Then we use transformation matrix T
148"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1896551724137931,"to map points from camera coordinates to occupancy coordinates. We then use the point coordinates
149"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19066937119675456,"to interpolate the nearest semantic index from the dense occupancy voxel to form a slice of MPI.
150"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19168356997971603,"Finally, we concatenate all slices to form MPI ∈RN×D×H×W , where D is the number of layers that
151"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1926977687626775,"is set at 256, N is the number of camera views in the case of batch size = 1.
152"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19371196754563894,"By representing occupancy as 3D semantic MPI, every pixel in MPI contains geometry and semantic
153"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1947261663286004,"information with implicit depth, seamlessly integrating occluded elements, and ensuring a precise
154"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19574036511156187,"spatial alignment with the generated images.
155"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19675456389452334,"3.3
3D Semantic MPI Encoder
156"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19776876267748478,"To enable local control with spatially aligned conditions, we develop a simple but effective MPI
157"
"D SEMANTIC 
MULTIPLANE IMAGES",0.19878296146044624,"encoder that aligns the 3D multi-plane feature to the latent space of the diffusion model. The
158"
"D SEMANTIC 
MULTIPLANE IMAGES",0.1997971602434077,"purpose of the MPI encoder is to obtain features from multi-plane images to perform 3D-aware
159"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20081135902636918,"(c) Editing: Foreground removal
(b) Editing: Object manipulation (copy object)
(a) Raw generation"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20182555780933062,"Figure 3: Visualizations of geometric controlled generation. Top row: Fusion of 3D semantic MPI.
Bottom row: our generation concatenated from neighboring views."
"D SEMANTIC 
MULTIPLANE IMAGES",0.2028397565922921,"image synthesis. Unlike the original ControlNet which downsampling conditional input through 3×3
160"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20385395537525355,"convolutions with padding, we design a 1×1 convolutional encoder without downsampling to encode
161"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20486815415821502,"features. In detail, the 3D multiplane features which have the sample resolution with latent features,
162"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20588235294117646,"are transformed by a 1×1 convolution layer and ReLU activation [1] in the MPI encoder.
163"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20689655172413793,"After obtaining the multi-scale feature after the MPI encoder, we add the feature to the decoder of
164"
"D SEMANTIC 
MULTIPLANE IMAGES",0.2079107505070994,"diffusion UNet to provide spatial features. Experimental results in Tab. 3 will show that our 1×1 conv
165"
"D SEMANTIC 
MULTIPLANE IMAGES",0.20892494929006086,"in MPI encoder is more effective than 3×3 conv, as the 1×1 conv with receptive field = 1 provides a
166"
"D SEMANTIC 
MULTIPLANE IMAGES",0.2099391480730223,"spatial align feature to the latent feature in the diffusion UNet. In contrast, 3×3 conv is conducted
167"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21095334685598377,"in a camera frustum space rather than Euclid space, making an imprecise correspondence between
168"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21196754563894524,"3D multiplane features and 2D image features. Moreover, using 3×3 conv to process 3D semantic
169"
"D SEMANTIC 
MULTIPLANE IMAGES",0.2129817444219067,"MPI will introduce a large computational burden as the channel number increases from 3 channels of
170"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21399594320486814,"RGB to 256 planes. We display our 3D geometry and semantic control property in Fig. 3.
171"
"D SEMANTIC 
MULTIPLANE IMAGES",0.2150101419878296,"In summary, we chose MPIs as the representation because they (i) Incorporate lossless 3D information,
172"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21602434077079108,"including scene geometry rather than 2.5D depth. (ii) Provide spatially aligned conditional features
173"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21703853955375255,"that naturally extend the ControlNet framework from image level to 3D level. (iii) Capable of
174"
"D SEMANTIC 
MULTIPLANE IMAGES",0.21805273833671399,"representing geometry and semantics including occluded elements.
175"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.21906693711967545,"3.4
Cross-View and Cross-Frame Attention
176"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22008113590263692,"The sensor arrangement in a self-driving car usually requires a full surround view of cameras to capture
177"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.2210953346855984,"the entire 360-degree environment. To effectively simulate the multi-view and subsequent multi-frame
178"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22210953346855983,"generation, zero-initialized [41] cross-view and cross-frame attention are integrated into the diffusion
179"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.2231237322515213,"model to maintain consistency between views and frames. Following prior work [5, 28, 30, 31],
180"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22413793103448276,"each cross-view attention allows the target view to access information from its neighboring left and
181"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22515212981744423,"right views, thus training cross-view attention using multi-view consistent images will enforce it to
182"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22616632860040567,"generate the same instance in the overlapping region of multi-view cameras.
183"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22718052738336714,"Attention(Q, K, V ) = softmax( QKT √"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.2281947261663286,"d ) · V ,
(4)"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.22920892494929007,hout = hin + P
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.2302231237322515,"i∈{l,r}Attention(Qin, Ki, Vi),
(5)"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.23123732251521298,"where l, and r is the camera view of left and right. Qin and hin denotes the query and the hidden
184"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.23225152129817445,"state of input view. Similarly, we add cross-frame attention that attend previous frame and future
185"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.2332657200811359,"frame to enable video generation. In this case, we use the same formulation while i ∈{f, h}, where
186"
CROSS-VIEW AND CROSS-FRAME ATTENTION,0.23427991886409735,"f and h is the camera view of future and history frames.
187"
IMPORTANCE REWEIGHING,0.23529411764705882,"3.5
Importance Reweighing
188"
IMPORTANCE REWEIGHING,0.2363083164300203,"0
200
400
600
800
1000 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75"
IMPORTANCE REWEIGHING,0.23732251521298176,"3.00
m = 3, n = 700
m = 3, n = 1000
m = 2, n = 1000"
IMPORTANCE REWEIGHING,0.2383367139959432,"Figure 4: Visualizations of the reweighing
function in Eq. 6."
IMPORTANCE REWEIGHING,0.23935091277890466,"To deal with the extreme imbalance problem between
189"
IMPORTANCE REWEIGHING,0.24036511156186613,"foreground, background, and object categories, and
190"
IMPORTANCE REWEIGHING,0.2413793103448276,"also to ease the training, we propose three types of
191"
IMPORTANCE REWEIGHING,0.24239350912778904,"reweighting methods to improve the generation quality
192"
IMPORTANCE REWEIGHING,0.2434077079107505,"of foreground objects.
193"
IMPORTANCE REWEIGHING,0.24442190669371197,"Progressive Foreground Enhancement
To miti-
194"
IMPORTANCE REWEIGHING,0.24543610547667344,"gate the complexity of the learning task, we propose a
195"
IMPORTANCE REWEIGHING,0.24645030425963488,"progressive reweighting method that incrementally en-
196"
IMPORTANCE REWEIGHING,0.24746450304259635,"hances the loss associated with the foreground regions
197"
IMPORTANCE REWEIGHING,0.2484787018255578,"(based on semantic class) as the training progresses.
198"
IMPORTANCE REWEIGHING,0.24949290060851928,"The detailed formulation is:
199"
IMPORTANCE REWEIGHING,0.25050709939148075,"w(x, m, n) = (m −1)"
IMPORTANCE REWEIGHING,0.2515212981744422,"2
·(1+cos( x"
IMPORTANCE REWEIGHING,0.2525354969574036,"n ·π+π))+1, (6)"
IMPORTANCE REWEIGHING,0.2535496957403651,(a) Top: Fusion of 3D semantic MPI. Bottom: GT
IMPORTANCE REWEIGHING,0.25456389452332656,(b) Generation1: Ordinary scenes. Red rectangle denotes geometry alignment of trees
IMPORTANCE REWEIGHING,0.25557809330628806,"(c) Generation2, Weather variation: Snow (top) and Sandstorm (bottom)"
IMPORTANCE REWEIGHING,0.2565922920892495,"(d) Generation3, Style control: Minecraft style (top) and Diablo style (bottom)"
IMPORTANCE REWEIGHING,0.25760649087221094,"CAM_FRONT_LEFT
CAM_FRONT
CAM_FRONT_RIGHT
CAM_BACK_RIGHT
CAM_BACK
CAM_BACK_LEFT"
IMPORTANCE REWEIGHING,0.25862068965517243,"Figure 5: Visualizations of generated multi-view images. The generation conditions (occupancy
labels) are from nuScenes validation set. We highlight that (i) Geometry alignment of trees in red
rectangle in (b). (ii) Use text prompt to control high-level appearance in (c,d)."
IMPORTANCE REWEIGHING,0.25963488843813387,"where x is the current training step, m is the maximum value of weights that set at 2, and n is the
200"
IMPORTANCE REWEIGHING,0.2606490872210953,"total training steps. This approach is engineered to facilitate a learning trajectory that progresses
201"
IMPORTANCE REWEIGHING,0.2616632860040568,"from simplicity to complexity, thereby aiding in the convergence of the model. This curve can be
202"
IMPORTANCE REWEIGHING,0.26267748478701824,"interpreted as a cosine annealing but inverted to amplify the importance of the foreground region.
203"
IMPORTANCE REWEIGHING,0.26369168356997974,"Depth-aware Foreground Reweighing
In the meantime, we acknowledge the learning difficulty
204"
IMPORTANCE REWEIGHING,0.2647058823529412,"in different depth places in 3D scenes. Following GeoDiffusion [3], we perform depth reweighing to
205"
IMPORTANCE REWEIGHING,0.2657200811359026,"foreground objects by adaptively assigning higher weights to farther foreground areas. This enables
206"
IMPORTANCE REWEIGHING,0.2667342799188641,"the model to focus more thoroughly on hard examples with depth-aware importance reweighting.
207"
IMPORTANCE REWEIGHING,0.26774847870182555,"Instead of using their exponential function to increase weights, we use our designed cosine function
208"
IMPORTANCE REWEIGHING,0.268762677484787,"Eq. 6 for stability. Here x is the input depth value, and n is the maximum depth that set at 50.
209"
IMPORTANCE REWEIGHING,0.2697768762677485,"CBGS Sampling
To deal with the class imbalance problem in driving scenarios, where cer-
210"
IMPORTANCE REWEIGHING,0.27079107505070993,"tain object categories appear infrequently, we employ the Class-Balanced Grouping and Sampling
211"
IMPORTANCE REWEIGHING,0.2718052738336714,"(CBGS) [44] to better handle the long-tailed classes. CBGS addresses the challenge of class imbal-
212"
IMPORTANCE REWEIGHING,0.27281947261663286,"ance by grouping and re-sampling training data to ensure each group has a balanced distribution of
213"
IMPORTANCE REWEIGHING,0.2738336713995943,"sample frequency across different object categories. This method reduces the bias towards more
214"
IMPORTANCE REWEIGHING,0.2748478701825558,"frequent classes and enables better generalization to rare scenarios.
215"
MODEL TRAINING,0.27586206896551724,"3.6
Model Training
216"
MODEL TRAINING,0.2768762677484787,"To ease the training of the MPI encoder and added attention module, we use a two stage training
217"
MODEL TRAINING,0.2778904665314402,"pipeline. We first train MPI encoder and cross-view attention in a multi-view image generation setting.
218"
MODEL TRAINING,0.2789046653144016,"Then we train cross-frame attention and freeze other components in a video generation setting.
219"
MODEL TRAINING,0.2799188640973631,"Method
Train
Val mIoU"
MODEL TRAINING,0.28093306288032455,■barrier
MODEL TRAINING,0.281947261663286,■bicycle ■bus ■car
MODEL TRAINING,0.2829614604462475,■cons. veh.
MODEL TRAINING,0.2839756592292089,■moto.
MODEL TRAINING,0.28498985801217036,■pedes.
MODEL TRAINING,0.28600405679513186,■traf. cone
MODEL TRAINING,0.2870182555780933,■trailer
MODEL TRAINING,0.2880324543610548,■truck
MODEL TRAINING,0.28904665314401623,■drive. suf.
MODEL TRAINING,0.29006085192697767,■other flat
MODEL TRAINING,0.29107505070993916,■sidewalk
MODEL TRAINING,0.2920892494929006,■terrain
MODEL TRAINING,0.29310344827586204,■manmade
MODEL TRAINING,0.29411764705882354,■vegetation
MODEL TRAINING,0.295131845841785,"Oracle (FB-Occ [15])
Real
Real 39.3 45.4 28.2 44.1 49.4 25.9 28.8 28.0 27.7 32.4 37.3 80.4 42.2 49.9 55.2 42.0 37.7
SytheOcc-Aug
Real+Gen Real 40.3 45.4 27.2 46.6 49.5 26.4 27.8 28.4 29.4 34.0 37.2 81.3 46.0 52.4 56.5 43.3 38.9"
MODEL TRAINING,0.2961460446247465,"MagicDrive
Real
Gen 13.4
0.7 0.0 11.8 32.4 0.0 6.6 2.8 0.3 2.6 19.6 60.1 12.1 26.2 23.4 15.5 12.8
ControlNet
Real
Gen 17.3 17.7 0.2 13.6 21.0 0.6 0.8 8.6 10.4 6.9 11.9 67.4 18.8 36.4 36.9 20.8 22.4
ControlNet+depth
Real
Gen 17.5 19.3 0.3 14.0 23.7 1.0 0.6 9.2 9.2 5.7 12.1 68.8 19.2 36.0 35.3 19.8 22.8
SytheOcc-Gen
Real
Gen 25.5 32.6 13.8 27.7 33.4 7.5 6.5 15.7 16.5 16.5 25.6 74.3 24.5 39.4 40.5 28.6 28.8"
MODEL TRAINING,0.2971602434077079,"Table 1: Downstream evaluation on the nuScenes-Occupancy validation set. Based on the used train
and val data, two types of settings are reported. The first is to use generated training set to augment the
real training set, and evaluate on the real validation set, denoted as Aug. The second is to use pretrained
models trained on the real training datasets to test on the generated validation set, denoted as Gen."
MODEL TRAINING,0.29817444219066935,"Objective Function
Our final objective function can be formulated as a standard denoising
220"
MODEL TRAINING,0.29918864097363085,"objective with reweighing:
221"
MODEL TRAINING,0.3002028397565923,"L = EE(x),ϵ,t∥ϵ −ϵθ(zt, t, τθ(y))∥2 ⊙w,
(7)
where w is the multiplication of progressive reweighing and depth-aware reweighing.
222"
EXPERIMENTS,0.3012170385395537,"4
Experiments
223"
DATASET AND SETUPS,0.3022312373225152,"4.1
Dataset and Setups
224"
DATASET AND SETUPS,0.30324543610547666,"We conduct our experiments on the nuScenes dataset [2], which is collected using 6 surrounded-view
225"
DATASET AND SETUPS,0.30425963488843816,"cameras that cover the full 360° field of view around the ego-vehicle. It contains 700 scenes for
226"
DATASET AND SETUPS,0.3052738336713996,"training and 150 scenes for validation. We resize the original image from 1600 × 900 to 800 × 448 for
227"
DATASET AND SETUPS,0.30628803245436104,"training. In our work, we use the occupancy label with a resolution of 0.2m from OpenOccupancy [27]
228"
DATASET AND SETUPS,0.30730223123732253,"as condition input, while the benchmark of occupancy prediction uses a resolution of 0.4m from
229"
DATASET AND SETUPS,0.30831643002028397,"Occ3D [24] dataset for its popularity.
230"
DATASET AND SETUPS,0.3093306288032454,"Networks
We use Stable Diffusion [21] v2.1 checkpoint as initialization and only train occupancy
231"
DATASET AND SETUPS,0.3103448275862069,"encoder, cross-view attention. We additionally add cross-frame attention if in video experiments. We
232"
DATASET AND SETUPS,0.31135902636916835,"adopt FB-Occ [15] as the target model for occupancy prediction for its SOTA performance in this task.
233"
DATASET AND SETUPS,0.31237322515212984,"The pretrained checkpoint of the network is obtained from their official repository. Since FB-Occ
234"
DATASET AND SETUPS,0.3133874239350913,"predicts occupancy using only single frame images, we thus train SyntheOcc without cross-frame
235"
DATASET AND SETUPS,0.3144016227180527,"attention in related experiments. For video generation, we provide experimental results in appendix.
236"
DATASET AND SETUPS,0.3154158215010142,"Metrics
We use Frechet Inception Distance (FID) [6] to measure the perceptual quality of generated
237"
DATASET AND SETUPS,0.31643002028397565,"images, and use mIoU to measure the precision of occupancy prediction.
238"
DATASET AND SETUPS,0.3174442190669371,"Hyperparameters
We set D = 256, dmin = 0 and dmax = 50. The depth resolution of MPI is
239"
DATASET AND SETUPS,0.3184584178498986,"thus higher than occupancy voxel. We train our model in 6 epochs with batch size = 8. The learning
240"
DATASET AND SETUPS,0.31947261663286003,"rate is set at 2e−5. The training phase takes around 1 day using 8 NVIDIA A100 80G GPUs. We use
241"
DATASET AND SETUPS,0.3204868154158215,"UniPC scheduler [42] with the classifier-free guidance (CFG) [7] that is set as 7.0. During inference,
242"
DATASET AND SETUPS,0.32150101419878296,"we use 20 denoising steps for dataset generation.
243"
DATASET AND SETUPS,0.3225152129817444,"Baselines
We compare our method with prior methods in Tab. 1. ControlNet denotes we train
244"
DATASET AND SETUPS,0.3235294117647059,"a ControlNet using an RGB semantic mask as the condition. ControlNet+depth denotes we add a
245"
DATASET AND SETUPS,0.32454361054766734,"depth channel after the semantic mask to provide 2.5D depth information. The depth map rendered
246"
DATASET AND SETUPS,0.3255578093306288,"by occupancy is normalized to [0-255] to accommodate the RGB value. The ControlNet+depth can
247"
DATASET AND SETUPS,0.3265720081135903,"be regarded as a degradation of SytheOcc which is reduced to a single plane. Then we evaluate
248"
DATASET AND SETUPS,0.3275862068965517,"MagicDrive since it is the only open-sourced method in this area. MagicDrive separately encodes
249"
DATASET AND SETUPS,0.3286004056795132,"foreground and background using prompt and BEV layout. Furthermore, we evaluate the image
250"
DATASET AND SETUPS,0.32961460446247465,"quality (FID [6]) of our method in Tab. 2. Compared with prior methods, we use a unified 3D
251"
DATASET AND SETUPS,0.3306288032454361,"representation that seamlessly handles foreground and background, surpassing them by a large margin.
252"
QUALITATIVE RESULTS,0.3316430020283976,"4.2
Qualitative Results
253"
QUALITATIVE RESULTS,0.332657200811359,"High-level Control using Prompt
In Fig. 5 (c,d) and Fig. 6 (c), we demonstrate the capability
254"
QUALITATIVE RESULTS,0.33367139959432046,"to employ user-defined prompts to generate images with specific weather conditions and high-level
255"
QUALITATIVE RESULTS,0.33468559837728196,"(a0) Fusion of MPI
(a1) Occupancy in 3D space
(a4) Ground truth
(a3) Ours generation
(a2) ControlNet generation"
QUALITATIVE RESULTS,0.3356997971602434,(b) A scene with human
QUALITATIVE RESULTS,0.3367139959432049,(d) A scene with hinged-articulated trucks
QUALITATIVE RESULTS,0.33772819472616633,(c) A scene of seasonal changes use prompt control
QUALITATIVE RESULTS,0.33874239350912777,(e) A scene with excavator use geometry control
QUALITATIVE RESULTS,0.33975659229208927,"Figure 6: Top row: Comparison with ControlNet. We achieve a precise alignment between condi-
tional labels and synthesized images, while ControlNet generates objects with incorrect pose due
to ambiguous 2D condition. Mid and Bottom row: Visualizations of geometry-controlled image
generation. We can faithfully generate objects with the desired topology in a specific 3D position."
QUALITATIVE RESULTS,0.3407707910750507,"style. Although the nuScenes dataset doesn’t contain rare weather images like snow and sandstorms,
256"
QUALITATIVE RESULTS,0.34178498985801214,"our method successfully conveys prior knowledge pretrained from stable diffusion to our scenes.
257"
QUALITATIVE RESULTS,0.34279918864097364,"Compared with visualization results in prior work like Fig. 8 of MagicDrive, our method shows better
258"
QUALITATIVE RESULTS,0.3438133874239351,"alignment with the text prompt, demonstrating the cross-domain generalization ability of our method.
259"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3448275862068966,"3D Geometric Control
Our flexible framework enables us to create novel scenes by manipulating
260"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.345841784989858,"voxels as displayed in Fig. 1 and Fig. 3. Basically, we can edit the occupied state and semantics of
261"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.34685598377281945,"every voxel in our scenes for generation. We highlight that we can create a hinged-articulated truck
262"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.34787018255578095,"and an excavator as shown in Fig. 6 (d,e). The generated excavator image exhibits a remarkable
263"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3488843813387424,"alignment with the input occupancy that is delineated by a black outline.
264"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.34989858012170383,"Long-tailed Scene Generation
The flexibility of 3D semantic MPI has conferred significant
265"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3509127789046653,"advantages upon our approach. In the following, we create long-tail scenes that rarely occur in
266"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.35192697768762676,"our real world for evaluation. In Fig. 1, we show that we manually add parallel traffic cones in
267"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.35294117647058826,"front of the ego vehicle. This scene has never happened in the training dataset, but our geometric
268"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3539553752535497,"controllability provides us the capability to create such data. We then use the created scene to test
269"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.35496957403651114,"autonomous driving systems such as end-to-end planner VAD [9] to validate its effectiveness. In
270"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.35598377281947263,"this case, VAD successfully predicts correct waypoints with the high-level command ‘turn left’.
271"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.35699797160243407,"Moreover, in appendix Sec. B, we generate long-tailed scenes with extreme weather such as snow
272"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3580121703853955,"and sandstorms, and evaluate perception model on it to examine its generalizability of rare weather.
273"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.359026369168357,"Comparison with Baselines
In Fig. 6 (a), we visualize a comparison with ControlNet. We find
274"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.36004056795131845,"that ControlNet struggles to distinguish the overlapping instances in 2D-pixel space. This leads to the
275"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.36105476673427994,"two parked cars being merged into a single car with incorrect pose. In contrast, our 3D semantic MPIs
276"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3620689655172414,"contain more than 2D semantic mask, but also account for complete scene geometry with occluded
277"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3630831643002028,"Method
Condition Type
FID"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3640973630831643,"BEVGen [23]
BEV map
25.54
BEVControl [34]
BEV map
24.85
DriveDreamer [26]
Box + FoV map
52.60
MagicDrive [5]
Box + BEV map
16.20
Panacea [30]
Box + FoV map
16.96
Ours
3D Semantic MPI
14.75"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.36511156186612576,"Table 2: Comparison of FID with previous methods
on the nuScenes dataset."
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3661257606490872,"MPI Encoder
Reweighing Method
Metric"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3671399594320487,"design
Progressive
Depth
CBGS
mIoU"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.36815415821501013,"3×3
-
-
-
21.96
1×1
-
-
-
23.05
1×1
✓
-
-
23.63
1×1
✓
✓
-
24.40
1×1
✓
✓
✓
25.50"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3691683569979716,"Table 3: Ablation of different designs of the MPI encoder and
reweighing methods."
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.37018255578093306,"parts. Together with our proposed MPI encoder and reweighing strategy, our framework yields a
278"
"D GEOMETRIC CONTROL
OUR FLEXIBLE FRAMEWORK ENABLES US TO CREATE NOVEL SCENES BY MANIPULATING",0.3711967545638945,"realistic image generation with high-quality label alignment. More comparison is provided in Sec. D.
279"
QUANTITATIVE RESULTS,0.372210953346856,"4.3
Quantitative Results
280"
QUANTITATIVE RESULTS,0.37322515212981744,"Recognizability, Realism and Controllability Evaluation
To evaluate whether our generated
281"
QUANTITATIVE RESULTS,0.3742393509127789,"images aligned with given annotations, we provide Gen experiment in Tab. 1. Using the annotation of
282"
QUANTITATIVE RESULTS,0.3752535496957404,"val set, we synthesize a copy of val set’s images, then use perception model trained on real training set
283"
QUANTITATIVE RESULTS,0.3762677484787018,"to perform evaluation. The performance will be more effective as it is close to the oracle performance.
284"
QUANTITATIVE RESULTS,0.3772819472616633,"We find that local method (ControlNet) perform better than global method (MagicDrive). Furthermore,
285"
QUANTITATIVE RESULTS,0.37829614604462475,"SytheOcc generalizes the locality for 3D conditioning to yield better performance.
286"
QUANTITATIVE RESULTS,0.3793103448275862,"Data Augmentation for 3D Occupancy Prediction
Notably, we conduct experiments using our
287"
QUANTITATIVE RESULTS,0.3803245436105477,"synthesized dataset to enhance the real training set in Tab. 1. We first use the occupancy labels from
288"
QUANTITATIVE RESULTS,0.3813387423935091,"training set to create a synthetic training set. Then we modify the loading pipeline in perception model
289"
QUANTITATIVE RESULTS,0.38235294117647056,"to randomly sample images from real dataset or synthetic dataset and train network from scratch.
290"
QUANTITATIVE RESULTS,0.38336713995943206,"Therefore, our approach preserves the inherent training dynamics of the neural network by solely
291"
QUANTITATIVE RESULTS,0.3843813387423935,"modifying the training images, without any alteration to the number of training iterations or epochs.
292"
QUANTITATIVE RESULTS,0.385395537525355,"As MagicDrive-Aug exhibits numerical overflow when training FB-Occ, which may attributed to
293"
QUANTITATIVE RESULTS,0.38640973630831643,"unsatisfactory recognizability, we have to omit it and only provide MagicDrive-Gen experiments.
294"
QUANTITATIVE RESULTS,0.38742393509127787,"As shown in Tab. 1, where SytheOcc-Aug denotes the augmentation experiments using our generated
295"
QUANTITATIVE RESULTS,0.38843813387423937,"dataset, shows a satisfactory improvement over the prior state of the art. We emphasize that surpassing
296"
QUANTITATIVE RESULTS,0.3894523326572008,"the performance of the original dataset is not the primary objective of our work; rather, it is an
297"
QUANTITATIVE RESULTS,0.39046653144016225,"ancillary benefit that emerges from our framework for geometry-controlled generation.
298"
QUANTITATIVE RESULTS,0.39148073022312374,"Ablations
In Tab. 3, we present ablation studies across several design spaces of our model, analo-
299"
QUANTITATIVE RESULTS,0.3924949290060852,"gous to the Gen experiment in Tab. 1. We find that our designed MPI encoder of 1×1 conv have sig-
300"
QUANTITATIVE RESULTS,0.3935091277890467,"nificant improvement when compared to the conventional 3×3 conv approach. Besides, our proposed
301"
QUANTITATIVE RESULTS,0.3945233265720081,"three types of reweighing methods demonstrate a consistent improvement over the baseline. As a
302"
QUANTITATIVE RESULTS,0.39553752535496955,"result, the improved image quality and label alignment enable higher precision in downstream tasks.
303"
LIMITATION AND BROADER IMPACTS,0.39655172413793105,"5
Limitation and Broader Impacts
304"
LIMITATION AND BROADER IMPACTS,0.3975659229208925,"Layout Genereation
Our method is restricted in a conditional generation framework that should
305"
LIMITATION AND BROADER IMPACTS,0.39858012170385393,"have a conditional input at first. Our condition signal is from the original dataset annotation. Thus
306"
LIMITATION AND BROADER IMPACTS,0.3995943204868154,"most of the augmented data is generated using the same occupancy layout, or with minimal human
307"
LIMITATION AND BROADER IMPACTS,0.40060851926977686,"editing. Future research can incorporate the recent research [10,17,32,40] that generates occupancy
308"
LIMITATION AND BROADER IMPACTS,0.40162271805273836,"descriptions of the scenes to synthesize images with novel occupancy layouts.
309"
LIMITATION AND BROADER IMPACTS,0.4026369168356998,"Closed-loop Simulation
Given the underlying diverse and controllable image generation of our
310"
LIMITATION AND BROADER IMPACTS,0.40365111561866124,"method, it would be advantageous and valuable to extend our work to a broader domain such as closed-
311"
LIMITATION AND BROADER IMPACTS,0.40466531440162273,"loop simulation [16,38], to enable high-fidelity autonomous systems testing. This line of work can
312"
LIMITATION AND BROADER IMPACTS,0.4056795131845842,"be conducted by utilizing motion conditions to generate future frames as in world model [17,28,36],
313"
LIMITATION AND BROADER IMPACTS,0.4066937119675456,"or by explicitly modeling scene graph as in the case of UniSim [20,38] and NeuroNCAP [16].
314"
LIMITATION AND BROADER IMPACTS,0.4077079107505071,"Long-tailed Scene Generation
In this paper, we only investigate a limited number of long-tailed
315"
LIMITATION AND BROADER IMPACTS,0.40872210953346855,"scene generation and corner case evaluations such as rare layout in Fig. 1 and extreme weather in
316"
LIMITATION AND BROADER IMPACTS,0.40973630831643004,"Sec. B. Future work can extend our framework to (i) Synthesize more samples for tail classes to boost
317"
LIMITATION AND BROADER IMPACTS,0.4107505070993915,"performance. (ii) Generate or replicate large-scale databases of corner cases [11] for robust perception.
318"
CONCLUSION,0.4117647058823529,"6
Conclusion
319"
CONCLUSION,0.4127789046653144,"In this paper, we propose SytheOcc, an innovative image generation framework that is empowered
320"
CONCLUSION,0.41379310344827586,"with geometry-controlled capabilities using occupancy. We introduce a novel 3D representation,
321"
CONCLUSION,0.4148073022312373,"3D semantic MPIs, to address the critical challenge of how to efficiently encode occupancy. This
322"
CONCLUSION,0.4158215010141988,"representation not only preserves the authentic and complete 3D geometry details with semantics, but
323"
CONCLUSION,0.41683569979716023,"also provides a spatial-align feature representation for 2D diffusion models. With this property, our
324"
CONCLUSION,0.4178498985801217,"method enjoys photorealistic appearances and fine-grained 3D controllability, serves as a generative
325"
CONCLUSION,0.41886409736308317,"data engine to enable a broad range of applications. Extensive experiments demonstrate that our
326"
CONCLUSION,0.4198782961460446,"synthetic data facilitate the training for perception models on occupancy prediction, and provide
327"
CONCLUSION,0.4208924949290061,"valuable corner case evaluation in a simulated world.
328"
REFERENCES,0.42190669371196754,"References
329"
REFERENCES,0.422920892494929,"[1] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375,
330"
REFERENCES,0.4239350912778905,"2018. 5
331"
REFERENCES,0.4249492900608519,"[2] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,
332"
REFERENCES,0.4259634888438134,"Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.
333"
REFERENCES,0.42697768762677485,"In CVPR, 2020. 7
334"
REFERENCES,0.4279918864097363,"[3] Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan Yeung. Integrating geometric
335"
REFERENCES,0.4290060851926978,"control into text-to-image diffusion models for high-quality detection data generation via text prompt.
336"
REFERENCES,0.4300202839756592,"arXiv preprint arXiv:2306.04607, 2023. 3, 6
337"
REFERENCES,0.43103448275862066,"[4] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-
338"
REFERENCES,0.43204868154158216,"free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 12
339"
REFERENCES,0.4330628803245436,"[5] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive:
340"
REFERENCES,0.4340770791075051,"Street view generation with diverse 3d geometry control. In ICLR, 2024. 1, 3, 5, 8, 14
341"
REFERENCES,0.43509127789046653,"[6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
342"
REFERENCES,0.43610547667342797,"trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. 7
343"
REFERENCES,0.43711967545638947,"[7] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint:2207.12598, 2022. 7
344"
REFERENCES,0.4381338742393509,"[8] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting
345"
REFERENCES,0.43914807302231235,"textured 3d meshes from 2d text-to-image models. In ICCV, 2023. 12
346"
REFERENCES,0.44016227180527384,"[9] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu,
347"
REFERENCES,0.4411764705882353,"Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving.
348"
REFERENCES,0.4421906693711968,"In ICCV, 2023. 2, 8, 12, 13
349"
REFERENCES,0.4432048681541582,"[10] Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic
350"
REFERENCES,0.44421906693711966,"scene generation with triplane diffusion. arXiv preprint arXiv:2403.07773, 2024. 9
351"
REFERENCES,0.44523326572008115,"[11] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang,
352"
REFERENCES,0.4462474645030426,"Chunjing Xu, Dit-Yan Yeung, et al. Coda: A real-world road corner case dataset for object detection in
353"
REFERENCES,0.44726166328600403,"autonomous driving. In ECCV, 2022. 9
354"
REFERENCES,0.4482758620689655,"[12] Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, and Ying-Cong Chen. Lift3d: Synthesize 3d training
355"
REFERENCES,0.44929006085192696,"data by lifting 2d gan to 3d generative radiance field. In CVPR, 2023. 1, 3
356"
REFERENCES,0.45030425963488846,"[13] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: Layout-guided multi-view driving scene
357"
REFERENCES,0.4513184584178499,"video generation with latent diffusion model. arXiv preprint arXiv:2310.07771, 2023. 3
358"
REFERENCES,0.45233265720081134,"[14] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and
359"
REFERENCES,0.45334685598377283,"Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. 2
360"
REFERENCES,0.4543610547667343,"[15] Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, and Jose M Alvarez.
361"
REFERENCES,0.4553752535496957,"Fb-occ: 3d occupancy prediction based on forward-backward view transformation.
arXiv preprint
362"
REFERENCES,0.4563894523326572,"arXiv:2307.01492, 2023. 7
363"
REFERENCES,0.45740365111561865,"[16] William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle Åström, Michael Felsberg,
364"
REFERENCES,0.45841784989858014,"and Christoffer Petersson. Neuroncap: Photorealistic closed-loop safety testing for autonomous driving.
365"
REFERENCES,0.4594320486815416,"arXiv preprint arXiv:2404.07762, 2024. 9
366"
REFERENCES,0.460446247464503,"[17] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang. Wovogen: World volume-aware diffusion
367"
REFERENCES,0.4614604462474645,"for controllable multi-camera driving scene generation. arXiv preprint arXiv:2312.02934, 2023. 3, 9
368"
REFERENCES,0.46247464503042596,"[18] Jianbiao Mei, Yu Yang, Mengmeng Wang, Tianxin Huang, Xuemeng Yang, and Yong Liu. Ssc-rs: Elevate
369"
REFERENCES,0.4634888438133874,"lidar semantic scene completion with representation separation and bev fusion. In IROS, 2023. 3
370"
REFERENCES,0.4645030425963489,"[19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren
371"
REFERENCES,0.46551724137931033,"Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1
372"
REFERENCES,0.4665314401622718,"[20] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic
373"
REFERENCES,0.46754563894523327,"scenes. In CVPR, 2021. 9
374"
REFERENCES,0.4685598377281947,"[21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
375"
REFERENCES,0.4695740365111562,"image synthesis with latent diffusion models. In CVPR, 2022. 3, 7
376"
REFERENCES,0.47058823529411764,"[22] Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, and Yang Zhao.
377"
REFERENCES,0.4716024340770791,"Roomdreamer: Text-driven 3d indoor scene synthesis with coherent geometry and texture. arXiv preprint
378"
REFERENCES,0.4726166328600406,"arXiv:2305.11337, 2023. 12
379"
REFERENCES,0.473630831643002,"[23] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-view image generation from a bird’s-eye view
380"
REFERENCES,0.4746450304259635,"layout. IEEE RAL, 2024. 1, 3, 8
381"
REFERENCES,0.47565922920892495,"[24] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang
382"
REFERENCES,0.4766734279918864,"Zhao. Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving. NeurIPS, 2024.
383"
REFERENCES,0.4776876267748479,"1, 3, 7
384"
REFERENCES,0.4787018255578093,"[25] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping
385"
REFERENCES,0.47971602434077076,"Luo, Dahua Lin, et al. Scene as occupancy. In ICCV, 2023. 3
386"
REFERENCES,0.48073022312373226,"[26] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Drivedreamer: Towards real-world-
387"
REFERENCES,0.4817444219066937,"driven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 3, 8
388"
REFERENCES,0.4827586206896552,"[27] Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang, Yi Wei, Xu Chi, Yun Ye, Dalong Du, Jiwen
389"
REFERENCES,0.48377281947261663,"Lu, and Xingang Wang. Openoccupancy: A large scale benchmark for surrounding semantic occupancy
390"
REFERENCES,0.4847870182555781,"perception. In ICCV, 2023. 1, 3, 7
391"
REFERENCES,0.48580121703853957,"[28] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future:
392"
REFERENCES,0.486815415821501,"Multiview visual forecasting and planning with world model for autonomous driving. arXiv preprint
393"
REFERENCES,0.48782961460446245,"arXiv:2311.17918, 2023. 3, 5, 9
394"
REFERENCES,0.48884381338742394,"[29] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc: Multi-camera
395"
REFERENCES,0.4898580121703854,"3d occupancy prediction for autonomous driving. In ICCV, 2023. 3
396"
REFERENCES,0.4908722109533469,"[30] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang,
397"
REFERENCES,0.4918864097363083,"Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous
398"
REFERENCES,0.49290060851926976,"driving. arXiv preprint arXiv:2311.16813, 2023. 1, 3, 5, 8
399"
REFERENCES,0.49391480730223125,"[31] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying
400"
REFERENCES,0.4949290060851927,"Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for
401"
REFERENCES,0.49594320486815413,"text-to-video generation. In ICCV, 2023. 5, 14
402"
REFERENCES,0.4969574036511156,"[32] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu,
403"
REFERENCES,0.49797160243407707,"Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane
404"
REFERENCES,0.49898580121703856,"extrapolation. arXiv preprint arXiv:2401.17053, 2024. 9
405"
REFERENCES,0.5,"[33] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparse single sweep
406"
REFERENCES,0.5010141987829615,"lidar point cloud segmentation via learning contextual shape priors from scene completion. In AAAI, 2021.
407 3
408"
REFERENCES,0.5020283975659229,"[34] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. Bevcontrol: Accurately
409"
REFERENCES,0.5030425963488844,"controlling street-view elements with multi-perspective consistency via bev sketch layout. arXiv preprint
410"
REFERENCES,0.5040567951318459,"arXiv:2308.01661, 2023. 3, 8
411"
REFERENCES,0.5050709939148073,"[35] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui,
412"
REFERENCES,0.5060851926977687,"and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM
413"
REFERENCES,0.5070993914807302,"Computing Surveys, 2023. 3
414"
REFERENCES,0.5081135902636917,"[36] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.
415"
REFERENCES,0.5091277890466531,"Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. 9
416"
REFERENCES,0.5101419878296146,"[37] Yuanbo Yang, Yifei Yang, Hanlei Guo, Rong Xiong, Yue Wang, and Yiyi Liao. Urbangiraffe: Representing
417"
REFERENCES,0.5111561866125761,"urban scenes as compositional generative neural feature fields. In ICCV, 2023. 3
418"
REFERENCES,0.5121703853955375,"[38] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel
419"
REFERENCES,0.513184584178499,"Urtasun. Unisim: A neural closed-loop sensor simulator. In CVPR, 2023. 9
420"
REFERENCES,0.5141987829614605,"[39] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester
421"
REFERENCES,0.5152129817444219,"Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere.
422"
REFERENCES,0.5162271805273834,"arXiv preprint arXiv:2312.03884, 2023. 12
423"
REFERENCES,0.5172413793103449,"[40] Junge Zhang, Qihang Zhang, Li Zhang, Ramana Rao Kompella, Gaowen Liu, and Bolei Zhou. Urban
424"
REFERENCES,0.5182555780933062,"scene diffusion through semantic occupancy map. arXiv preprint arXiv:2403.11697, 2024. 9
425"
REFERENCES,0.5192697768762677,"[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
426"
REFERENCES,0.5202839756592292,"models. In ICCV, 2023. 2, 3, 5
427"
REFERENCES,0.5212981744421906,"[42] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector
428"
REFERENCES,0.5223123732251521,"framework for fast sampling of diffusion models. NeurIPS, 2023. 7
429"
REFERENCES,0.5233265720081136,"[43] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:
430"
REFERENCES,0.5243407707910751,"Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 2, 4
431"
REFERENCES,0.5253549695740365,"[44] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and
432"
REFERENCES,0.526369168356998,"sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492, 2019. 6
433"
REFERENCES,0.5273833671399595,"Appendix
434"
REFERENCES,0.5283975659229209,"In the appendix, we provide the following content:
435"
REFERENCES,0.5294117647058824,"Sec. A: Statement of Geometric Control.
Sec. E: Results of Video Generation.
Sec. B: Long-Tailed Scene Evaluation.
Sec. F: Generalize to New Cameras.
Sec. C: Ablation of plane number in MPIs.
Sec. G: Impact of Amount of Augment Data.
Sec. D: Additional Qualitative Comparison.
Sec. H: Visualization of Failure Cases."
REFERENCES,0.5304259634888439,"A
Statement of Geometric Control
436"
REFERENCES,0.5314401622718052,"In our paper, we refer the geometric controllable generation as using a voxel grid in 3D space to
437"
REFERENCES,0.5324543610547667,"control the image generation. Although the voxel is a quantized representation of the 3D world,
438"
REFERENCES,0.5334685598377282,"when the resolution goes larger, it can already faithfully represent the geometry detail of scenes.
439"
REFERENCES,0.5344827586206896,"Currently, we are limited by the precision of ground truth labels. The 0.2m occupancy grid is a tensor
440"
REFERENCES,0.5354969574036511,"of 500×500×40 that cover a space in x-axis spanning [−50m, 50m], y-axis spanning [−50m, 50m],
441"
REFERENCES,0.5365111561866126,"z-axis spanning [−5m, 3m]. In the future, we plan to explore a higher resolution of geometric control
442"
REFERENCES,0.537525354969574,"to refine our generation.
443"
REFERENCES,0.5385395537525355,"Except for occupancy, several other 3D representations can be expressed by 3D semantic MPI,
444"
REFERENCES,0.539553752535497,"such as mesh, dense point clouds, and even 3D boxes or HD maps. The underlying mechanism is
445"
REFERENCES,0.5405679513184585,"to cast several slices of multi-plane images at different depths to retrieve geometric information.
446"
REFERENCES,0.5415821501014199,"Thus, our 3D semantic MPI can be regarded as a general 3D conditioning representation to benefit
447"
REFERENCES,0.5425963488843814,"a wide spectrum of practical systems. These encompass but are not limited to 3D generation such
448"
REFERENCES,0.5436105476673428,"as text2room [8], RoomDreamer [22], WonderJourney [39], and LucidDreamer [4], each of which
449"
REFERENCES,0.5446247464503042,"stands to benefit from the rich geometric context provided by our approach.
450"
REFERENCES,0.5456389452332657,"B
Long-Tailed Scene Evaluation
451"
REFERENCES,0.5466531440162272,"In this section, we explore to use SytheOcc to create long-tailed scenes for downstream evaluation.
452"
REFERENCES,0.5476673427991886,"This also stands for evaluating our model using several corner cases. Similar to the SytheOcc-Gen
453"
REFERENCES,0.5486815415821501,"experiment in Tab. 1, we generate a synthetic validation set but use prompts control to manipulate
454"
REFERENCES,0.5496957403651116,"weather patterns or the intensity of illumination.
455"
REFERENCES,0.550709939148073,"As depicted in Fig. 7. We create a variety of weather conditions including sandstorms, snow, foggy,
456"
REFERENCES,0.5517241379310345,"rainy, day night, and day time. The motivation behind the creation of these scenes lies in their extreme
457"
REFERENCES,0.552738336713996,"rarity compared to the ordinary scenes we have captured. The generation of such data is of significant
458"
REFERENCES,0.5537525354969574,"value, as it aids in addressing the long-tailed distribution of scenes, thereby enriching the diversity of
459"
REFERENCES,0.5547667342799188,"our dataset. More visualization is provided in Fig. 13 to Fig. 14.
460"
REFERENCES,0.5557809330628803,"In Tab. 4, we observe that all kinds of extreme weather lead to a degradation in performance. This
461"
REFERENCES,0.5567951318458418,"observation underscores the limitations of the perception model in terms of its generalizability to
462"
REFERENCES,0.5578093306288032,"infrequent weather scenarios. Among them, we find that foggy, rainy, and day night exert the most
463"
REFERENCES,0.5588235294117647,"severe impact, as they contribute to a large reduction in visibility as shown in Fig. 7. To improve the
464"
REFERENCES,0.5598377281947262,"generalizability to handle various weather conditions, future work can leverage our generated data to
465"
REFERENCES,0.5608519269776876,"cover the long-tailed scenes, or use adversarial search to find severe scenes based on our framework.
466"
REFERENCES,0.5618661257606491,"Scenes
Sandstorm
Snow
Foggy
Rainy
Day night
Day time (raw data)"
REFERENCES,0.5628803245436106,"FB-Occ mIOU
22.88
18.25
10.29
9.71
9.95
25.50"
REFERENCES,0.563894523326572,Table 4: Experiments of downstream evaluation on long-tailed scenes with extreme weather.
REFERENCES,0.5649087221095335,"Furthermore, we perform long-tailed scene evaluation in Fig. 8. We display the failure of the
467"
REFERENCES,0.565922920892495,"downstream model VAD [9] in our synthetic long-tailed scene. In this case, we simulate a foggy
468"
REFERENCES,0.5669371196754563,"environment that the dense fog obscures the majority of the ego view. Our experiment reveals that
469"
REFERENCES,0.5679513184584178,"due to the lack of training images of foggy scenes, VAD erroneously predicts waypoints that would
470"
REFERENCES,0.5689655172413793,"result in a collision with the bus. This experiment elucidates the boundaries and failure cases of the
471"
REFERENCES,0.5699797160243407,"VAD model [9]. It exposes the limitations of the system under certain conditions, thereby providing
472"
REFERENCES,0.5709939148073022,"insights into scenarios where the model’s performance may be compromised.
473"
REFERENCES,0.5720081135902637,"Figure 7: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images
of sandstorm, snow, foggy, rainy, day night, day time, and ground truth."
REFERENCES,0.5730223123732252,"Testing
End-to-end"
REFERENCES,0.5740365111561866,Planner VAD
REFERENCES,0.5750507099391481,Corner
REFERENCES,0.5760649087221096,"Case
Evaluation"
REFERENCES,0.577079107505071,Predicted waypoints (Static)
REFERENCES,0.5780933062880325,"Predicted waypoints
Generated image"
REFERENCES,0.579107505070994,Raw image
REFERENCES,0.5801217038539553,Prompt-level
REFERENCES,0.5811359026369168,"Control: 
Add fog"
REFERENCES,0.5821501014198783,SyntheOcc
REFERENCES,0.5831643002028397,User Editing
REFERENCES,0.5841784989858012,"To Create
Corner Case"
REFERENCES,0.5851926977687627,Scene geometry
REFERENCES,0.5862068965517241,"Figure 8: Use SytheOcc to create long-tailed scenes for testing. Top: In the ordinary scene of a
bus placed in front of the ego vehicle, the end-to-end planner VAD [9] predicts future waypoints
without movement, thus not plotted in the image. Bottom: By harnessing the prompt-level control in
our framework, we simulate a scene with the same layout but filled with fog. VAD predicts wrong
waypoints that will collide with the bus."
REFERENCES,0.5872210953346856,"C
Ablation of plane number of MPIs
474"
REFERENCES,0.5882352941176471,"In our proposed 3D semantic MPIs, the number of planes is a hyperparameter that affects the precision
475"
REFERENCES,0.5892494929006086,"of 3D representation. The plane number can be regarded as the 3D resolution in depth axis. The
476"
REFERENCES,0.59026369168357,"larger the plane number, the MPI will contain more details. We find that an increase in the number of
477"
REFERENCES,0.5912778904665315,"planes is associated with improved accuracy in downstream tasks. This finding denotes that more
478"
REFERENCES,0.592292089249493,"condition information leads to better downstream task performance.
479"
REFERENCES,0.5933062880324543,"Fusion of MPIs
MagicDrive
ControlNet
ControlNet+depth
SyntheOcc
GT"
REFERENCES,0.5943204868154158,Figure 9: Comparison with baselines.
REFERENCES,0.5953346855983773,"Number of Planes
96
128
256"
REFERENCES,0.5963488843813387,"FB-Occ mIOU
23.36
24.28
25.50"
REFERENCES,0.5973630831643002,Table 5: Ablation of the number of multi-plane images.
REFERENCES,0.5983772819472617,"D
Qualitative Comparison with Baselines and SOTA
480"
REFERENCES,0.5993914807302231,"In Fig. 9, we conduct a qualitative comparison of our method against MagicDrive, ControlNet, and
481"
REFERENCES,0.6004056795131846,"ControlNet+depth. We find that all the methods display a satisfactory image quality, as they build upon
482"
REFERENCES,0.6014198782961461,"the foundation of the stable diffusion model. The generation of MagicDrive fails to synthesize barriers
483"
REFERENCES,0.6024340770791075,"as shown in the bottom row. ControlNet struggles to generate objects with the correct pose solely
484"
REFERENCES,0.603448275862069,"from only 2D conditions as shown in the second row. ControlNet+depth, a degradation of our method,
485"
REFERENCES,0.6044624746450304,"an enhancement over ControlNet in terms of alignment, nevertheless suffers from a loss of finer detail
486"
REFERENCES,0.6054766734279919,"in scenes with heavy occlusion, as shown in the human of the third row. Our method, in contrast, aims
487"
REFERENCES,0.6064908722109533,"to address these challenges and provide a more nuanced and accurate generation of complex scenes.
488"
REFERENCES,0.6075050709939148,"E
Extend to Video Generation
489"
REFERENCES,0.6085192697768763,"As described in the main paper Sec. 3.4, we further extend the cross-view attention to cross-frame
490"
REFERENCES,0.6095334685598377,"attention to perform video generation. Our generation results are Fig. 11, Fig. 12 and Fig. 16.
491"
REFERENCES,0.6105476673427992,"Our implementation is adopted from MagicDrive [5] which is similar to Tune-a-video [31]. The
492"
REFERENCES,0.6115618661257607,"formulation of cross-frame attention is:
493"
REFERENCES,0.6125760649087221,"Attention(Q, K, V ) = softmax( QKT √"
REFERENCES,0.6135902636916836,"d ) · V ,
(8)"
REFERENCES,0.6146044624746451,hout = hin + P
REFERENCES,0.6156186612576064,"i∈{f,h}Attention(Qin, Ki, Vi),
(9)"
REFERENCES,0.6166328600405679,"where f, and h are the camera view of future and history frames. Qin and hin denotes the query and
494"
REFERENCES,0.6176470588235294,"the hidden state of input view. We train our model in a two-stage pipeline. We first train the MPI
495"
REFERENCES,0.6186612576064908,"encoder and cross-view attention in a multi-view image generation setting. Then we train cross-frame
496"
REFERENCES,0.6196754563894523,"attention and freeze other components in a video generation setting.
497"
REFERENCES,0.6206896551724138,"In practice, we use the keyframe annotation of the nuScenes dataset to train our video model. We start
498"
REFERENCES,0.6217038539553753,"with our pretrained MPI encoder and cross-view attention and only train our cross-frame attention
499"
REFERENCES,0.6227180527383367,"while keeping others frozen. We employ a sequence of 7 frames as a batch, resulting in a batch size
500"
REFERENCES,0.6237322515212982,"of 42 images for the training process.
501"
REFERENCES,0.6247464503042597,"Given that our primary contribution does not lie in video generation, this experiment serves as a
502"
REFERENCES,0.6257606490872211,"proof of concept, demonstrating the potential of our framework. Future research may extend our
503"
REFERENCES,0.6267748478701826,"methodology to facilitate the generation of longer video sequences, thereby expanding the scope and
504"
REFERENCES,0.6277890466531441,"applicability of our framework.
505"
REFERENCES,0.6288032454361054,(a) Generation with original intrinsic
REFERENCES,0.6298174442190669,(b) Generation with intrinsic modification (focal length * 0.8)
REFERENCES,0.6308316430020284,(c) Generation with intrinsic modification (focal length * 1.2)
REFERENCES,0.6318458417849898,"Figure 10: We demonstrate the generalizability of SytheOcc to new camera intrinsic. We multiply
factors to the focal length while keeping the resolution the same. In (b,c), focal length ×0.8 denotes
a camera with a larger field of view similar to zoom out, focal length ×1.2 denotes a camera with a
smaller field of view similar to zoom in."
REFERENCES,0.6328600405679513,"F
Generalize to New Cameras
506"
REFERENCES,0.6338742393509128,"In this section, we investigate the adaptability of our method to a new set of cameras with different
507"
REFERENCES,0.6348884381338742,"intrinsic. Given that our training set has a fixed camera intrinsic and extrinsic, generalizing to novel
508"
REFERENCES,0.6359026369168357,"cameras indicates that our approach possesses robust generalization capabilities. As shown in Fig. 10,
509"
REFERENCES,0.6369168356997972,"benefiting from our local type of condition, SytheOcc generates images that faithfully align with
510"
REFERENCES,0.6379310344827587,"the new intrinsic, proving that SytheOcc do not over-fit certain parameters. Regarding extrinsic
511"
REFERENCES,0.6389452332657201,"parameters, we can cast our MPI at the desirable locations to retrieve geometric information, thus
512"
REFERENCES,0.6399594320486816,"inherently ensuring generalizability without doubt.
513"
REFERENCES,0.640973630831643,"G
The Influence of the Amount of Augmented Data
514"
REFERENCES,0.6419878296146044,"As SytheOcc is capable of generating an infinite number of synthetic data, we investigate the influence
515"
REFERENCES,0.6430020283975659,"of the amount of augmented data on downstream tasks in Tab. 6. We find that when our augmented
516"
REFERENCES,0.6440162271805274,"data is expanded from one-fold to two-fold of the training dataset, the performance of perception
517"
REFERENCES,0.6450304259634888,"model slightly decreases. This may indicate the generated data has an optimal ratio for downstream
518"
REFERENCES,0.6460446247464503,"tasks. Due to limited computational resources, we only experiment with a limited amount of ratio.
519"
REFERENCES,0.6470588235294118,"Future work can conduct more thorough experiments to find a universal theorem.
520"
REFERENCES,0.6480730223123732,"Amount of Augmented Data
0 (no augmentation)
1
2"
REFERENCES,0.6490872210953347,"FB-Occ mIOU
39.3
40.3
40.1"
REFERENCES,0.6501014198782962,Table 6: Ablation of the amount of augmented data.
REFERENCES,0.6511156186612576,"Figure 11: Video generation results. In the temporal progression, the distant buildings maintain a
high degree of consistency, and objects retain their identical shapes and textures across different
views and frames."
REFERENCES,0.652129817444219,"Figure 12: Video generation results of large dynamics scenes. The white car comes across different
views and frames depicting consistent shapes with only a slight appearance change."
REFERENCES,0.6531440162271805,"Figure 13: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images
of sandstorm, snow, foggy, rainy, day night, day time, and ground truth."
REFERENCES,0.654158215010142,Figure 14: Weather variation. Same structure with Fig. 13.
REFERENCES,0.6551724137931034,"Figure 15: Out of distribution generation. We use prompts to control the high-level appearance of
images with specific styles. From top to bottom, we display (1) fusion of 3D semantic MPI. (2) Sunny
day. (3) Science fiction style. (4) 8-bit pixel art style. (5) Snowfall. (6) Minecraft style. (7) Pokémon
style. (8) Diablo style. (9) Ghibli style. (10) Metropolis style. (11) Gotham style. (12) Ground truth."
REFERENCES,0.6561866125760649,"H
Failure Cases
521"
REFERENCES,0.6572008113590264,"We display several failure cases of our method. In Fig. 16, we show a crowd scenes. In this scenario,
522"
REFERENCES,0.6582150101419878,"the excessive number of pedestrians presents a challenge to the cross-view attention and cross-frame
523"
REFERENCES,0.6592292089249493,"attention modules. We find our method incapable of discerning individual entities with clarity. Future
524"
REFERENCES,0.6602434077079108,"research can improve the model capacity or enrich high-quality data to mitigate this problem.
525"
REFERENCES,0.6612576064908722,"Figure 16: Failure case of video generation results. Our cross-frame attention module is challenging
to distinguish a crowd of people across different views and frames."
REFERENCES,0.6622718052738337,"NeurIPS Paper Checklist
526"
REFERENCES,0.6632860040567952,"The checklist is designed to encourage best practices for responsible machine learning research,
527"
REFERENCES,0.6643002028397565,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
528"
REFERENCES,0.665314401622718,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
529"
REFERENCES,0.6663286004056795,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
530"
REFERENCES,0.6673427991886409,"towards the page limit.
531"
REFERENCES,0.6683569979716024,"Please read the checklist guidelines carefully for information on how to answer these questions. For
532"
REFERENCES,0.6693711967545639,"each question in the checklist:
533"
REFERENCES,0.6703853955375254,"• You should answer [Yes] , [No] , or [NA] .
534"
REFERENCES,0.6713995943204868,"• [NA] means either that the question is Not Applicable for that particular paper or the
535"
REFERENCES,0.6724137931034483,"relevant information is Not Available.
536"
REFERENCES,0.6734279918864098,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
537"
REFERENCES,0.6744421906693712,"The checklist answers are an integral part of your paper submission. They are visible to the
538"
REFERENCES,0.6754563894523327,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
539"
REFERENCES,0.6764705882352942,"(after eventual revisions) with the final version of your paper, and its final version will be published
540"
REFERENCES,0.6774847870182555,"with the paper.
541"
REFERENCES,0.678498985801217,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
542"
REFERENCES,0.6795131845841785,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
543"
REFERENCES,0.6805273833671399,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
544"
REFERENCES,0.6815415821501014,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
545"
REFERENCES,0.6825557809330629,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
546"
REFERENCES,0.6835699797160243,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
547"
REFERENCES,0.6845841784989858,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
548"
REFERENCES,0.6855983772819473,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
549"
REFERENCES,0.6866125760649088,"please point to the section(s) where related material for the question can be found.
550"
REFERENCES,0.6876267748478702,"IMPORTANT, please:
551"
REFERENCES,0.6886409736308317,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
552"
REFERENCES,0.6896551724137931,"• Keep the checklist subsection headings, questions/answers and guidelines below.
553"
REFERENCES,0.6906693711967545,"• Do not modify the questions and only use the provided macros for your answers.
554"
CLAIMS,0.691683569979716,"1. Claims
555"
CLAIMS,0.6926977687626775,"Question: Do the main claims made in the abstract and introduction accurately reflect the
556"
CLAIMS,0.6937119675456389,"paper’s contributions and scope?
557"
CLAIMS,0.6947261663286004,"Answer: [Yes]
558"
CLAIMS,0.6957403651115619,"Justification: Please find this part in Sec. 3.
559"
CLAIMS,0.6967545638945233,"Guidelines:
560"
CLAIMS,0.6977687626774848,"• The answer NA means that the abstract and introduction do not include the claims
561"
CLAIMS,0.6987829614604463,"made in the paper.
562"
CLAIMS,0.6997971602434077,"• The abstract and/or introduction should clearly state the claims made, including the
563"
CLAIMS,0.7008113590263692,"contributions made in the paper and important assumptions and limitations. A No or
564"
CLAIMS,0.7018255578093306,"NA answer to this question will not be perceived well by the reviewers.
565"
CLAIMS,0.7028397565922921,"• The claims made should match theoretical and experimental results, and reflect how
566"
CLAIMS,0.7038539553752535,"much the results can be expected to generalize to other settings.
567"
CLAIMS,0.704868154158215,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
568"
CLAIMS,0.7058823529411765,"are not attained by the paper.
569"
LIMITATIONS,0.7068965517241379,"2. Limitations
570"
LIMITATIONS,0.7079107505070994,"Question: Does the paper discuss the limitations of the work performed by the authors?
571"
LIMITATIONS,0.7089249492900609,"Answer: [Yes]
572"
LIMITATIONS,0.7099391480730223,"Justification: Please find this part in Sec. 5.
573"
LIMITATIONS,0.7109533468559838,"Guidelines:
574"
LIMITATIONS,0.7119675456389453,"• The answer NA means that the paper has no limitation while the answer No means that
575"
LIMITATIONS,0.7129817444219066,"the paper has limitations, but those are not discussed in the paper.
576"
LIMITATIONS,0.7139959432048681,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
577"
LIMITATIONS,0.7150101419878296,"• The paper should point out any strong assumptions and how robust the results are to
578"
LIMITATIONS,0.716024340770791,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
579"
LIMITATIONS,0.7170385395537525,"model well-specification, asymptotic approximations only holding locally). The authors
580"
LIMITATIONS,0.718052738336714,"should reflect on how these assumptions might be violated in practice and what the
581"
LIMITATIONS,0.7190669371196755,"implications would be.
582"
LIMITATIONS,0.7200811359026369,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
583"
LIMITATIONS,0.7210953346855984,"only tested on a few datasets or with a few runs. In general, empirical results often
584"
LIMITATIONS,0.7221095334685599,"depend on implicit assumptions, which should be articulated.
585"
LIMITATIONS,0.7231237322515213,"• The authors should reflect on the factors that influence the performance of the approach.
586"
LIMITATIONS,0.7241379310344828,"For example, a facial recognition algorithm may perform poorly when image resolution
587"
LIMITATIONS,0.7251521298174443,"is low or images are taken in low lighting. Or a speech-to-text system might not be
588"
LIMITATIONS,0.7261663286004056,"used reliably to provide closed captions for online lectures because it fails to handle
589"
LIMITATIONS,0.7271805273833671,"technical jargon.
590"
LIMITATIONS,0.7281947261663286,"• The authors should discuss the computational efficiency of the proposed algorithms
591"
LIMITATIONS,0.72920892494929,"and how they scale with dataset size.
592"
LIMITATIONS,0.7302231237322515,"• If applicable, the authors should discuss possible limitations of their approach to
593"
LIMITATIONS,0.731237322515213,"address problems of privacy and fairness.
594"
LIMITATIONS,0.7322515212981744,"• While the authors might fear that complete honesty about limitations might be used by
595"
LIMITATIONS,0.7332657200811359,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
596"
LIMITATIONS,0.7342799188640974,"limitations that aren’t acknowledged in the paper. The authors should use their best
597"
LIMITATIONS,0.7352941176470589,"judgment and recognize that individual actions in favor of transparency play an impor-
598"
LIMITATIONS,0.7363083164300203,"tant role in developing norms that preserve the integrity of the community. Reviewers
599"
LIMITATIONS,0.7373225152129818,"will be specifically instructed to not penalize honesty concerning limitations.
600"
THEORY ASSUMPTIONS AND PROOFS,0.7383367139959433,"3. Theory Assumptions and Proofs
601"
THEORY ASSUMPTIONS AND PROOFS,0.7393509127789046,"Question: For each theoretical result, does the paper provide the full set of assumptions and
602"
THEORY ASSUMPTIONS AND PROOFS,0.7403651115618661,"a complete (and correct) proof?
603"
THEORY ASSUMPTIONS AND PROOFS,0.7413793103448276,"Answer: [NA]
604"
THEORY ASSUMPTIONS AND PROOFS,0.742393509127789,"Justification: The paper does not include theoretical results.
605"
THEORY ASSUMPTIONS AND PROOFS,0.7434077079107505,"Guidelines: Do not have theoretical results.
606"
THEORY ASSUMPTIONS AND PROOFS,0.744421906693712,"• The answer NA means that the paper does not include theoretical results.
607"
THEORY ASSUMPTIONS AND PROOFS,0.7454361054766734,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
608"
THEORY ASSUMPTIONS AND PROOFS,0.7464503042596349,"referenced.
609"
THEORY ASSUMPTIONS AND PROOFS,0.7474645030425964,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
610"
THEORY ASSUMPTIONS AND PROOFS,0.7484787018255578,"• The proofs can either appear in the main paper or the supplemental material, but if
611"
THEORY ASSUMPTIONS AND PROOFS,0.7494929006085193,"they appear in the supplemental material, the authors are encouraged to provide a short
612"
THEORY ASSUMPTIONS AND PROOFS,0.7505070993914807,"proof sketch to provide intuition.
613"
THEORY ASSUMPTIONS AND PROOFS,0.7515212981744422,"• Inversely, any informal proof provided in the core of the paper should be complemented
614"
THEORY ASSUMPTIONS AND PROOFS,0.7525354969574036,"by formal proofs provided in appendix or supplemental material.
615"
THEORY ASSUMPTIONS AND PROOFS,0.7535496957403651,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7545638945233266,"4. Experimental Result Reproducibility
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.755578093306288,"Question: Does the paper fully disclose all the information needed to reproduce the main
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7565922920892495,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757606490872211,"sions of the paper (regardless of whether the code and data are provided or not)?
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7586206896551724,"Answer: [Yes]
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596348884381339,"Justification: Please find this part in Sec. 4.
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7606490872210954,"Guidelines:
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7616632860040567,"• The answer NA means that the paper does not include experiments.
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7626774847870182,"• If the paper includes experiments, a No answer to this question will not be perceived
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7636916835699797,"well by the reviewers: Making the paper reproducible is important, regardless of
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647058823529411,"whether the code and data are provided or not.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657200811359026,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667342799188641,"to make their results reproducible or verifiable.
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677484787018256,"• Depending on the contribution, reproducibility can be accomplished in various ways.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.768762677484787,"For example, if the contribution is a novel architecture, describing the architecture fully
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7697768762677485,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.77079107505071,"be necessary to either make it possible for others to replicate the model with the same
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718052738336714,"dataset, or provide access to the model. In general. releasing code and data is often
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7728194726166329,"one good way to accomplish this, but reproducibility can also be provided via detailed
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7738336713995944,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748478701825557,"of a large language model), releasing of a model checkpoint, or other means that are
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7758620689655172,"appropriate to the research performed.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7768762677484787,"• While NeurIPS does not require releasing code, the conference does require all submis-
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778904665314401,"sions to provide some reasonable avenue for reproducibility, which may depend on the
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7789046653144016,"nature of the contribution. For example
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7799188640973631,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7809330628803245,"to reproduce that algorithm.
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.781947261663286,"(b) If the contribution is primarily a new model architecture, the paper should describe
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829614604462475,"the architecture clearly and fully.
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783975659229209,"(c) If the contribution is a new model (e.g., a large language model), then there should
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7849898580121704,"either be a way to access this model for reproducing the results or a way to reproduce
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7860040567951319,"the model (e.g., with an open-source dataset or instructions for how to construct
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870182555780934,"the dataset).
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7880324543610547,"(d) We recognize that reproducibility may be tricky in some cases, in which case
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890466531440162,"authors are welcome to describe the particular way they provide for reproducibility.
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900608519269777,"In the case of closed-source models, it may be that access to the model is limited in
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7910750507099391,"some way (e.g., to registered users), but it should be possible for other researchers
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7920892494929006,"to have some path to reproducing or verifying the results.
654"
OPEN ACCESS TO DATA AND CODE,0.7931034482758621,"5. Open access to data and code
655"
OPEN ACCESS TO DATA AND CODE,0.7941176470588235,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
656"
OPEN ACCESS TO DATA AND CODE,0.795131845841785,"tions to faithfully reproduce the main experimental results, as described in supplemental
657"
OPEN ACCESS TO DATA AND CODE,0.7961460446247465,"material?
658"
OPEN ACCESS TO DATA AND CODE,0.7971602434077079,"Answer: [Yes]
659"
OPEN ACCESS TO DATA AND CODE,0.7981744421906694,"Justification: Please find this part in Sec. 4.
660"
OPEN ACCESS TO DATA AND CODE,0.7991886409736308,"Guidelines:
661"
OPEN ACCESS TO DATA AND CODE,0.8002028397565923,"• The answer NA means that paper does not include experiments requiring code.
662"
OPEN ACCESS TO DATA AND CODE,0.8012170385395537,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
663"
OPEN ACCESS TO DATA AND CODE,0.8022312373225152,"public/guides/CodeSubmissionPolicy) for more details.
664"
OPEN ACCESS TO DATA AND CODE,0.8032454361054767,"• While we encourage the release of code and data, we understand that this might not be
665"
OPEN ACCESS TO DATA AND CODE,0.8042596348884381,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
666"
OPEN ACCESS TO DATA AND CODE,0.8052738336713996,"including code, unless this is central to the contribution (e.g., for a new open-source
667"
OPEN ACCESS TO DATA AND CODE,0.8062880324543611,"benchmark).
668"
OPEN ACCESS TO DATA AND CODE,0.8073022312373225,"• The instructions should contain the exact command and environment needed to run to
669"
OPEN ACCESS TO DATA AND CODE,0.808316430020284,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
670"
OPEN ACCESS TO DATA AND CODE,0.8093306288032455,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
671"
OPEN ACCESS TO DATA AND CODE,0.8103448275862069,"• The authors should provide instructions on data access and preparation, including how
672"
OPEN ACCESS TO DATA AND CODE,0.8113590263691683,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
673"
OPEN ACCESS TO DATA AND CODE,0.8123732251521298,"• The authors should provide scripts to reproduce all experimental results for the new
674"
OPEN ACCESS TO DATA AND CODE,0.8133874239350912,"proposed method and baselines. If only a subset of experiments are reproducible, they
675"
OPEN ACCESS TO DATA AND CODE,0.8144016227180527,"should state which ones are omitted from the script and why.
676"
OPEN ACCESS TO DATA AND CODE,0.8154158215010142,"• At submission time, to preserve anonymity, the authors should release anonymized
677"
OPEN ACCESS TO DATA AND CODE,0.8164300202839757,"versions (if applicable).
678"
OPEN ACCESS TO DATA AND CODE,0.8174442190669371,"• Providing as much information as possible in supplemental material (appended to the
679"
OPEN ACCESS TO DATA AND CODE,0.8184584178498986,"paper) is recommended, but including URLs to data and code is permitted.
680"
OPEN ACCESS TO DATA AND CODE,0.8194726166328601,"6. Experimental Setting/Details
681"
OPEN ACCESS TO DATA AND CODE,0.8204868154158215,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperpa-
682"
OPEN ACCESS TO DATA AND CODE,0.821501014198783,"rameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
683"
OPEN ACCESS TO DATA AND CODE,0.8225152129817445,"Answer: [Yes]
684"
OPEN ACCESS TO DATA AND CODE,0.8235294117647058,"Justification: Please find this part in Sec. 4.
685"
OPEN ACCESS TO DATA AND CODE,0.8245436105476673,"Guidelines:
686"
OPEN ACCESS TO DATA AND CODE,0.8255578093306288,"• The answer NA means that the paper does not include experiments.
687"
OPEN ACCESS TO DATA AND CODE,0.8265720081135902,"• The experimental setting should be presented in the core of the paper to a level of detail
688"
OPEN ACCESS TO DATA AND CODE,0.8275862068965517,"that is necessary to appreciate the results and make sense of them.
689"
OPEN ACCESS TO DATA AND CODE,0.8286004056795132,"• The full details can be provided either with the code, in appendix, or as supplemental
690"
OPEN ACCESS TO DATA AND CODE,0.8296146044624746,"material.
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306288032454361,"7. Experiment Statistical Significance
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316430020283976,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326572008113591,"information about the statistical significance of the experiments?
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336713995943205,"Answer: [Yes]
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834685598377282,"Justification: Please find this part in Sec. 4.
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356997971602435,"Guidelines:
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367139959432048,"• The answer NA means that the paper does not include experiments.
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377281947261663,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387423935091278,"dence intervals, or statistical significance tests, at least for the experiments that support
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397565922920892,"the main claims of the paper.
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8407707910750507,"• The factors of variability that the error bars are capturing should be clearly stated (for
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8417849898580122,"example, train/test split, initialization, random drawing of some parameter, or overall
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427991886409736,"run with given experimental conditions).
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438133874239351,"• The method for calculating the error bars should be explained (closed form formula,
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8448275862068966,"call to a library function, bootstrap, etc.)
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845841784989858,"• The assumptions made should be given (e.g., Normally distributed errors).
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8468559837728195,"• It should be clear whether the error bar is the standard deviation or the standard error
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847870182555781,"of the mean.
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8488843813387424,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498985801217038,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8509127789046653,"of Normality of errors is not verified.
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8519269776876268,"• For asymmetric distributions, the authors should be careful not to show in tables or
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8529411764705882,"figures symmetric error bars that would yield results that are out of range (e.g. negative
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8539553752535497,"error rates).
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8549695740365112,"• If error bars are reported in tables or plots, The authors should explain in the text how
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8559837728194726,"they were calculated and reference the corresponding figures or tables in the text.
717"
EXPERIMENTS COMPUTE RESOURCES,0.8569979716024341,"8. Experiments Compute Resources
718"
EXPERIMENTS COMPUTE RESOURCES,0.8580121703853956,"Question: For each experiment, does the paper provide sufficient information on the com-
719"
EXPERIMENTS COMPUTE RESOURCES,0.859026369168357,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
720"
EXPERIMENTS COMPUTE RESOURCES,0.8600405679513184,"the experiments?
721"
EXPERIMENTS COMPUTE RESOURCES,0.8610547667342799,"Answer: [Yes]
722"
EXPERIMENTS COMPUTE RESOURCES,0.8620689655172413,"Justification: Please find this part in Sec. 4.
723"
EXPERIMENTS COMPUTE RESOURCES,0.8630831643002028,"Guidelines:
724"
EXPERIMENTS COMPUTE RESOURCES,0.8640973630831643,"• The answer NA means that the paper does not include experiments.
725"
EXPERIMENTS COMPUTE RESOURCES,0.8651115618661258,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
726"
EXPERIMENTS COMPUTE RESOURCES,0.8661257606490872,"or cloud provider, including relevant memory and storage.
727"
EXPERIMENTS COMPUTE RESOURCES,0.8671399594320487,"• The paper should provide the amount of compute required for each of the individual
728"
EXPERIMENTS COMPUTE RESOURCES,0.8681541582150102,"experimental runs as well as estimate the total compute.
729"
EXPERIMENTS COMPUTE RESOURCES,0.8691683569979716,"• The paper should disclose whether the full research project required more compute
730"
EXPERIMENTS COMPUTE RESOURCES,0.8701825557809331,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
731"
EXPERIMENTS COMPUTE RESOURCES,0.8711967545638946,"didn’t make it into the paper).
732"
CODE OF ETHICS,0.8722109533468559,"9. Code Of Ethics
733"
CODE OF ETHICS,0.8732251521298174,"Question: Does the research conducted in the paper conform, in every respect, with the
734"
CODE OF ETHICS,0.8742393509127789,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
735"
CODE OF ETHICS,0.8752535496957403,"Answer: [Yes]
736"
CODE OF ETHICS,0.8762677484787018,"Justification: It should be fine.
737"
CODE OF ETHICS,0.8772819472616633,"Guidelines:
738"
CODE OF ETHICS,0.8782961460446247,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
739"
CODE OF ETHICS,0.8793103448275862,"• If the authors answer No, they should explain the special circumstances that require a
740"
CODE OF ETHICS,0.8803245436105477,"deviation from the Code of Ethics.
741"
CODE OF ETHICS,0.8813387423935092,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
742"
CODE OF ETHICS,0.8823529411764706,"eration due to laws or regulations in their jurisdiction).
743"
BROADER IMPACTS,0.8833671399594321,"10. Broader Impacts
744"
BROADER IMPACTS,0.8843813387423936,"Question: Does the paper discuss both potential positive societal impacts and negative
745"
BROADER IMPACTS,0.8853955375253549,"societal impacts of the work performed?
746"
BROADER IMPACTS,0.8864097363083164,"Answer: [Yes]
747"
BROADER IMPACTS,0.8874239350912779,"Justification: Please find this part in Sec. 5.
748"
BROADER IMPACTS,0.8884381338742393,"Guidelines:
749"
BROADER IMPACTS,0.8894523326572008,"• The answer NA means that there is no societal impact of the work performed.
750"
BROADER IMPACTS,0.8904665314401623,"• If the authors answer NA or No, they should explain why their work has no societal
751"
BROADER IMPACTS,0.8914807302231237,"impact or why the paper does not address societal impact.
752"
BROADER IMPACTS,0.8924949290060852,"• Examples of negative societal impacts include potential malicious or unintended uses
753"
BROADER IMPACTS,0.8935091277890467,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
754"
BROADER IMPACTS,0.8945233265720081,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
755"
BROADER IMPACTS,0.8955375253549696,"groups), privacy considerations, and security considerations.
756"
BROADER IMPACTS,0.896551724137931,"• The conference expects that many papers will be foundational research and not tied
757"
BROADER IMPACTS,0.8975659229208925,"to particular applications, let alone deployments. However, if there is a direct path to
758"
BROADER IMPACTS,0.8985801217038539,"any negative applications, the authors should point it out. For example, it is legitimate
759"
BROADER IMPACTS,0.8995943204868154,"to point out that an improvement in the quality of generative models could be used to
760"
BROADER IMPACTS,0.9006085192697769,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
761"
BROADER IMPACTS,0.9016227180527383,"that a generic algorithm for optimizing neural networks could enable people to train
762"
BROADER IMPACTS,0.9026369168356998,"models that generate Deepfakes faster.
763"
BROADER IMPACTS,0.9036511156186613,"• The authors should consider possible harms that could arise when the technology is
764"
BROADER IMPACTS,0.9046653144016227,"being used as intended and functioning correctly, harms that could arise when the
765"
BROADER IMPACTS,0.9056795131845842,"technology is being used as intended but gives incorrect results, and harms following
766"
BROADER IMPACTS,0.9066937119675457,"from (intentional or unintentional) misuse of the technology.
767"
BROADER IMPACTS,0.907707910750507,"• If there are negative societal impacts, the authors could also discuss possible mitigation
768"
BROADER IMPACTS,0.9087221095334685,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
769"
BROADER IMPACTS,0.90973630831643,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
770"
BROADER IMPACTS,0.9107505070993914,"feedback over time, improving the efficiency and accessibility of ML).
771"
SAFEGUARDS,0.9117647058823529,"11. Safeguards
772"
SAFEGUARDS,0.9127789046653144,"Question: Does the paper describe safeguards that have been put in place for responsible
773"
SAFEGUARDS,0.9137931034482759,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
774"
SAFEGUARDS,0.9148073022312373,"image generators, or scraped datasets)?
775"
SAFEGUARDS,0.9158215010141988,"Answer: [NA]
776"
SAFEGUARDS,0.9168356997971603,"Justification: Our paper poses no such risks.
777"
SAFEGUARDS,0.9178498985801217,"Guidelines:
778"
SAFEGUARDS,0.9188640973630832,"• The answer NA means that the paper poses no such risks.
779"
SAFEGUARDS,0.9198782961460447,"• Released models that have a high risk for misuse or dual-use should be released with
780"
SAFEGUARDS,0.920892494929006,"necessary safeguards to allow for controlled use of the model, for example by requiring
781"
SAFEGUARDS,0.9219066937119675,"that users adhere to usage guidelines or restrictions to access the model or implementing
782"
SAFEGUARDS,0.922920892494929,"safety filters.
783"
SAFEGUARDS,0.9239350912778904,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
784"
SAFEGUARDS,0.9249492900608519,"should describe how they avoided releasing unsafe images.
785"
SAFEGUARDS,0.9259634888438134,"• We recognize that providing effective safeguards is challenging, and many papers do
786"
SAFEGUARDS,0.9269776876267748,"not require this, but we encourage authors to take this into account and make a best
787"
SAFEGUARDS,0.9279918864097363,"faith effort.
788"
LICENSES FOR EXISTING ASSETS,0.9290060851926978,"12. Licenses for existing assets
789"
LICENSES FOR EXISTING ASSETS,0.9300202839756593,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
790"
LICENSES FOR EXISTING ASSETS,0.9310344827586207,"the paper, properly credited and are the license and terms of use explicitly mentioned and
791"
LICENSES FOR EXISTING ASSETS,0.9320486815415822,"properly respected?
792"
LICENSES FOR EXISTING ASSETS,0.9330628803245437,"Answer: [Yes]
793"
LICENSES FOR EXISTING ASSETS,0.934077079107505,"Justification: Please find this part in Sec. 4.
794"
LICENSES FOR EXISTING ASSETS,0.9350912778904665,"Guidelines:
795"
LICENSES FOR EXISTING ASSETS,0.936105476673428,"• The answer NA means that the paper does not use existing assets.
796"
LICENSES FOR EXISTING ASSETS,0.9371196754563894,"• The authors should cite the original paper that produced the code package or dataset.
797"
LICENSES FOR EXISTING ASSETS,0.9381338742393509,"• The authors should state which version of the asset is used and, if possible, include a
798"
LICENSES FOR EXISTING ASSETS,0.9391480730223124,"URL.
799"
LICENSES FOR EXISTING ASSETS,0.9401622718052738,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
800"
LICENSES FOR EXISTING ASSETS,0.9411764705882353,"• For scraped data from a particular source (e.g., website), the copyright and terms of
801"
LICENSES FOR EXISTING ASSETS,0.9421906693711968,"service of that source should be provided.
802"
LICENSES FOR EXISTING ASSETS,0.9432048681541582,"• If assets are released, the license, copyright information, and terms of use in the package
803"
LICENSES FOR EXISTING ASSETS,0.9442190669371197,"should be provided. For popular datasets, paperswithcode.com/datasets has
804"
LICENSES FOR EXISTING ASSETS,0.9452332657200812,"curated licenses for some datasets. Their licensing guide can help determine the license
805"
LICENSES FOR EXISTING ASSETS,0.9462474645030426,"of a dataset.
806"
LICENSES FOR EXISTING ASSETS,0.947261663286004,"• For existing datasets that are re-packaged, both the original license and the license of
807"
LICENSES FOR EXISTING ASSETS,0.9482758620689655,"the derived asset (if it has changed) should be provided.
808"
LICENSES FOR EXISTING ASSETS,0.949290060851927,"• If this information is not available online, the authors are encouraged to reach out to
809"
LICENSES FOR EXISTING ASSETS,0.9503042596348884,"the asset’s creators.
810"
NEW ASSETS,0.9513184584178499,"13. New Assets
811"
NEW ASSETS,0.9523326572008114,"Question: Are new assets introduced in the paper well documented and is the documentation
812"
NEW ASSETS,0.9533468559837728,"provided alongside the assets?
813"
NEW ASSETS,0.9543610547667343,"Answer: [NA]
814"
NEW ASSETS,0.9553752535496958,"Justification: Our paper does not release new assets.
815"
NEW ASSETS,0.9563894523326572,"Guidelines:
816"
NEW ASSETS,0.9574036511156186,"• The answer NA means that the paper does not release new assets.
817"
NEW ASSETS,0.9584178498985801,"• Researchers should communicate the details of the dataset/code/model as part of their
818"
NEW ASSETS,0.9594320486815415,"submissions via structured templates. This includes details about training, license,
819"
NEW ASSETS,0.960446247464503,"limitations, etc.
820"
NEW ASSETS,0.9614604462474645,"• The paper should discuss whether and how consent was obtained from people whose
821"
NEW ASSETS,0.962474645030426,"asset is used.
822"
NEW ASSETS,0.9634888438133874,"• At submission time, remember to anonymize your assets (if applicable). You can either
823"
NEW ASSETS,0.9645030425963489,"create an anonymized URL or include an anonymized zip file.
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9655172413793104,"14. Crowdsourcing and Research with Human Subjects
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665314401622718,"Question: For crowdsourcing experiments and research with human subjects, does the paper
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675456389452333,"include the full text of instructions given to participants and screenshots, if applicable, as
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685598377281948,"well as details about compensation (if any)?
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695740365111561,"Answer: [NA]
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9705882352941176,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716024340770791,"Guidelines:
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726166328600405,"• The answer NA means that the paper does not involve crowdsourcing nor research with
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973630831643002,"human subjects.
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746450304259635,"• Including this information in the supplemental material is fine, but if the main contribu-
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756592292089249,"tion of the paper involves human subjects, then as much detail as possible should be
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766734279918864,"included in the main paper.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776876267748479,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787018255578094,"or other labor should be paid at least the minimum wage in the country of the data
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797160243407708,"collector.
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807302231237323,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817444219066938,"Subjects
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827586206896551,"Question: Does the paper describe potential risks incurred by study participants, whether
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837728194726166,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847870182555781,"approvals (or an equivalent approval/review based on the requirements of your country or
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858012170385395,"institution) were obtained?
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986815415821501,"Answer: [NA]
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878296146044625,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888438133874239,"Guidelines:
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898580121703854,"• The answer NA means that the paper does not involve crowdsourcing nor research with
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908722109533469,"human subjects.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918864097363083,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929006085192698,"may be required for any human subjects research. If you obtained IRB approval, you
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939148073022313,"should clearly state this in the paper.
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949290060851927,"• We recognize that the procedures for this may vary significantly between institutions
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959432048681541,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969574036511156,"guidelines for their institution.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979716024340771,"• For initial submissions, do not include any information that would break anonymity (if
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989858012170385,"applicable), such as the institution conducting the review.
858"
