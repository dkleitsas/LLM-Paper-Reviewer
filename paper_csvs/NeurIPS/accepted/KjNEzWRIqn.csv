Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0023148148148148147,"LLMs can now act as autonomous agents that interact with digital environments and
complete specific objectives (e.g., arranging an online meeting). However, accuracy
is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations
for digital tasks. Obtaining supervised data from humans is costly, and automatic
data collection through exploration or reinforcement learning relies on complex
environmental and content setup, resulting in datasets that lack comprehensive
coverage of various scenarios. On the other hand, there is abundant knowledge that
may indirectly assist task completion, such as online tutorials that were created
for human consumption. In this work, we present Synatra, an approach that
effectively transforms this indirect knowledge into direct supervision at scale. We
define different types of indirect knowledge, and carefully study the available
sources to obtain it, methods to encode the structure of direct demonstrations, and
finally methods to transform indirect knowledge into direct demonstrations. We
use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama,
and demonstrate that the resulting agent surpasses all comparably sized models on
three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well
as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic
demonstrations prove to be only 3% the cost of human demonstrations (at $0.031
each), we show that the synthetic demonstrations can be more effective than an
identical number of human demonstrations collected from limited domains.1"
INTRODUCTION,0.004629629629629629,"1
Introduction"
INTRODUCTION,0.006944444444444444,"AI agents that operate within a digital environment (e.g., a browser in a computer) intelligently
to accomplish complex tasks (e.g., “Create my July online shopping expense report.”) have the
potential to improve the productivity across a broad swath of tasks performed by humans every
day [2, 33, 6, 58]. However, agents still lack the ability to complete tasks with a high degree of
reliability, partly due to a paucity of training data for such agentic tasks. Typically, supervised
finetuning is a standard way to adapt large language models (LLMs) to tasks such as text generation
or classification, large-scale demonstration collections for digital agents are not readily available."
INTRODUCTION,0.009259259259259259,"For AI agents, demonstrations typically involve specifying a sequence of actions and observations
that results in successful task completion, as shown on the right of Figure 1. Existing works that
automatically collect demonstrations (1) set up environments for an agent to interact with, (2) run
a baseline agent within this environment, and (3) employ scoring functions to remove low quality
demonstrations [9] or perform relabeling [1, 26]. All three of these requirements limit applicability
to a variety of practical applications. Setting up an environment that is representative of the actual"
INTRODUCTION,0.011574074074074073,1Data and code are publically available at https://oootttyyy.github.io/synatra
INTRODUCTION,0.013888888888888888,"goto(""https://www.paypal.com"") [...]"
INTRODUCTION,0.016203703703703703,"click(“login"")"
INTRODUCTION,0.018518518518518517,"type(“username"",""john@example.com”) [...]"
INTRODUCTION,0.020833333333333332,"type(""search bar"",""Amazon Prime"")"
INTRODUCTION,0.023148148148148147,"action history a1, . . . , at−1"
INTRODUCTION,0.02546296296296296,observation ot
INTRODUCTION,0.027777777777777776,"click(""Amazon Inc.”, id=156)"
INTRODUCTION,0.03009259259259259,next action at
INTRODUCTION,0.032407407407407406,Cancel Amazon Prime membership on Paypal
INTRODUCTION,0.034722222222222224,task intent i
INTRODUCTION,0.037037037037037035,"<!DOCTYPE html>
<html lang=""en"">
<head>
    [...]
</head>
<body>
    [...]
</body>
</html>"
INTRODUCTION,0.03935185185185185,Cancel a PayPal payment
INTRODUCTION,0.041666666666666664,"[…]
Log in with your 
credentials.
[…]
Enter the keyword 
Select the payment 
[…]"
INTRODUCTION,0.04398148148148148,Indirect knowledge
INTRODUCTION,0.046296296296296294,Synthetic
INTRODUCTION,0.04861111111111111,"agent  
trajectory  
generation"
INTRODUCTION,0.05092592592592592,(Synatra)
INTRODUCTION,0.05324074074074074,id=156
INTRODUCTION,0.05555555555555555,"Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the
immediate next actions based on previous actions and current observations. The sources comprise
only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and
randomly sampled observations that lack associated tasks and actions."
INTRODUCTION,0.05787037037037037,"environments in which we would like agents to act is a difficult task, and existing environments
for digital agents are generally limited in scope to a few websites or digital apps [58, 7, 45]. Even
within these constrained settings, strong LLMs such as GPT-4 struggle on tackling tasks [58], making
collecting successful demonstrations with LLMs inefficient. In addition, human annotation is costly
and its scope can still be limited [17, 6]. For example, gathering a demonstration for canceling a
PayPal order requires an actual PayPal account with a legitimate subscription history."
INTRODUCTION,0.06018518518518518,"In this work, we propose Synatra, a data generation approach that synthesizes high-quality tra-
jectories for complex digital tasks at scale. This approach is based on the intuition that there is
a rich trove of existing knowledge that encodes indirect supervision about how to perform digital
tasks (§2.2). One example of a piece of indirect knowledge is a tutorial that details the sequential
breakdown of a complex web task, such as “how to cancel a recurring payment on Paypal” for human
readers (Figure 1 upper left) [53, 57]. While this tutorial provides some procedural guidance, such as
“Enter the keyword,” it does not specify the executable actions or the tangible observations associated
with them. The key idea of Synatra is that, given this indirect knowledge, we can leverage an LLM
to re-purpose it into more usable form that directly demonstrates the exact action to take given a
concrete observation (§3). In doing so, we leverage LLMs’ ability to paraphrase and code, as well as
its general knowledge of how tasks are performed (Figure 1). The main benefit of this approach is
that it can scale with the availability of indirect knowledge created for humans, rather than rely on
direct human annotations or LLM trajectories."
INTRODUCTION,0.0625,"We carefully study the sources of indirect knowledge, the design of demonstration formats, and
the mechanisms for iterative refinement in order to synthesize high-quality demonstrations using
an LLM (§3, §4). We generate demonstrations of 100k tasks from 21 domains, and finetune a 7b
Codellama-instruct model with this synthetic data. The resulting agent, Synatra-CodeLlama,
surpasses existing open-source models of similar size, excluding those finetuned with human annota-
tions, on three web-based task benchmarks: Mind2Web, MiniWoB++, and WebArena. Moreover, it
consistently outperforms models that are ten times larger and have been finetuned with interactive
data (§6.1). Our findings also indicate that our model is on-par or a more accurate option in browser
copilot settings, in comparison to GPT-3.5. Importantly, while each synthetic example incurs only
approximately 3% of the cost of a human-annotated demonstration, we demonstrate that synthetic
data with good domain coverage can be more effective than an identical quantity of limited-domain
human demonstrations in real-world web-based tasks in WebArena (§6.2)."
PROBLEM FORMULATION,0.06481481481481481,"2
Problem Formulation"
CONTROLLING DIGITAL AGENTS THROUGH NATURAL LANGUAGE,0.06712962962962964,"2.1
Controlling Digital Agents through Natural Language"
CONTROLLING DIGITAL AGENTS THROUGH NATURAL LANGUAGE,0.06944444444444445,"An agent interacts with a computer environment E = ⟨S, A, O, T ⟩with state space S, action space
A, observation space O and the environment dynamics T : S × A −→S. While this framework can
be applied to different types of task, in this work, we consider a web browser as the unified entry point
to access different web applications. We follow WebArena [58] to define the observation space as the
contents on screen, represented as in text-based accessibility tree format. We use the same universal
action space as in WebArena. This action space resembles the keyboard and mouse operations of a"
CONTROLLING DIGITAL AGENTS THROUGH NATURAL LANGUAGE,0.07175925925925926,"computer (e.g., click, type), and is applicable to arbitrary web applications. Appendix A lists all
valid actions. The environment dynamics (e.g., the effect of clicking a button) and the states (e.g., the
database status) are decided by the implementation of each web application."
CONTROLLING DIGITAL AGENTS THROUGH NATURAL LANGUAGE,0.07407407407407407,"Given the natural language intent i, at each time step t, an agent issues an action at ∈A based on
st. The environment state is updated to st+1 with new observation ot+1. This process ends when
the agent predicts the stop action. We follow existing works [6, 47, 58, 54] to represent st with
current observation and all previous actions in the form of (i, a1, ..., at−1, ot). A benchmark (e.g.,
WebArena) supplies a scoring function r(i, sn) that examines the final state and returns 1 if the
desired goal state is satisfied, and 0 otherwise."
DEFINITION OF DIRECT DEMONSTRATIONS AND INDIRECT KNOWLEDGE,0.0763888888888889,"2.2
Definition of Direct Demonstrations and Indirect Knowledge"
DEFINITION OF DIRECT DEMONSTRATIONS AND INDIRECT KNOWLEDGE,0.0787037037037037,"We consider the expected action at given st as a form of direct demonstration, i.e., (st, at). This
allows an agent to directly learn how to predict the next action under a given state. On the other hand,
indirect knowledge is broadly defined as resources that can benefit the task execution, but is not in
the format of state and expected action tuple. We mainly focus on three types of indirect knowledge:"
DEFINITION OF DIRECT DEMONSTRATIONS AND INDIRECT KNOWLEDGE,0.08101851851851852,"1. Procedural knowledge details the sequence of steps ⟨a′
1, a′
2, ..., a′
n⟩required to complete a specific
task i. Unlike an action at in trajectories, the steps in procedural knowledge are ungrounded, they
lack a direct association with any particular observation and are not tied to specific action spaces.
For instance, the tutorial in Figure 1 instructs the user to “login with your credentials” without
providing the concrete PayPal login page and the input fields to type."
ENVIRONMENT KNOWLEDGE T THAT DESCRIBE THE EFFECTS OF APPLYING DIFFERENT ACTIONS IN SOME HY-,0.08333333333333333,"2. Environment knowledge T that describe the effects of applying different actions in some hy-
pothetical states. Example knowledge includes verbal descriptions such as “... after clicking the
cancel button, you will see a pop up window ...”."
ENVIRONMENT KNOWLEDGE T THAT DESCRIBE THE EFFECTS OF APPLYING DIFFERENT ACTIONS IN SOME HY-,0.08564814814814815,"3. Ungrounded observations o that are not associated with particular tasks or trajectories. In the
context of web-based tasks, an observation could be any random web page with different content
and status (e.g., a product page with a query in the search field)."
SCALABLE DEMONSTRATION SYNTHESIS FOR DIGITAL AGENTS,0.08796296296296297,"3
Scalable Demonstration Synthesis for Digital Agents"
SCALABLE DEMONSTRATION SYNTHESIS FOR DIGITAL AGENTS,0.09027777777777778,"In this section, we first introduce our design choices on the canonical formalization of trajectories,
which account for the structural nature of procedures. Then, we delve into the sources for acquiring
indirect knowledge, and the mechanisms for re-purposing this knowledge into direct supervision."
TRAJECTORIES AS PROGRAMS,0.09259259259259259,"3.1
Trajectories as Programs"
TRAJECTORIES AS PROGRAMS,0.09490740740740741,"website = ""<url>""
observation = ""<AXtree of page>""
objective = ""[...]"""
TRAJECTORIES AS PROGRAMS,0.09722222222222222,"# past actions
def solve():"
TRAJECTORIES AS PROGRAMS,0.09953703703703703,"# sub-task 1: [...]
# <action NL explanation>
action(arg=value)
[...]
# sub-task 2: [...]
[...]"
TRAJECTORIES AS PROGRAMS,0.10185185185185185,"# <action CoT reasoning>
action(arg=value)
# <action summary>"
TRAJECTORIES AS PROGRAMS,0.10416666666666667,"Existing works demonstrate that representing task-related
procedures as programs is beneficial for several reasons.
This includes benefits from the structural nature of pro-
grams compared to free-form text [56, 23], and the flex-
ibility of using tools [4, 10, 41]. Inspired by these obser-
vations, we represent a Python function that interleaves
natural language planning articulated in comments and
actions as API calls, as shown on the right. The pink back-
ground represents the prompt, while the blue background
corresponds to the model’s response format."
TRAJECTORIES AS PROGRAMS,0.10648148148148148,"The planning process includes both task-level planning,
which breaks the task into multiple sub-tasks, and action-
level planning, which explains the low-level goals of each
executable action. The model’s generation consists of
chain-of-thought (CoT, [43, 47]) reasoning that analyzes
the objective, previous actions, and current observations.
Since CoT reasoning includes detailed information about
the current step that may not be relevant for future steps, we further design the model response to
include an action summary. This summary serves as a description of the predicted action that is added"
TRAJECTORIES AS PROGRAMS,0.1087962962962963,"to the prompt for future action predictions. A concrete example can be found in Appendix B.2 We
study the empirical effect of program formalization in §6.3."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.1111111111111111,"3.2
Synthesizing from Text Procedural Knowledge with Generative Environment"
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.11342592592592593,"The Internet offers fairly extensive procedural knowledge that describes how to perform high-level
tasks by breaking down the task into detailed lower-level steps, such as how-tos and tutorials."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.11574074074074074,"Source
We use wikiHow3 as our main source for these tutorials due to its comprehensive coverage
of diverse tasks as well as its consistent format. Each article consists of a high-level task description
and step-by-step instructions. We performed a filtering step and only kept the articles that involve
navigation through the graphical user interface (GUI) of a computer or a mobile phone. We prompted
GPT-3.5-turbo with six examples mixing the target articles (e.g., How to redeem an Amazon gift
card online4) and non-target articles (e.g., How to make a pizza) to perform the classification of all
wikiHow articles. The prompt is shown in Appendix C. As a result, we obtained 25k articles that can
be used to perform data synthesis."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.11805555555555555,"Synthesis Approach
We want to bridge two gaps to re-purpose ⟨a′
1, a′
2, ..., a′
n⟩for task i into
⟨a1, ..., at−1, ot⟩. When re-purposing a sequence of actions ⟨a′
1, a′
2, ..., a′
n⟩for task i into a new
sequence ⟨a1, ..., at −1, ot⟩, many challenges arise. First, the action descriptions provided in tutorials
are not constrained to specific action spaces. Instead, they are presented as free-form natural language
(NL) expressions, which can lead to ambiguity. For instance, various verbs such as “enter,” “input,”
and others may all correspond to the same underlying action, type. Second, NL descriptions are
often abstract, omitting concrete actions. For example, the process of “logging in” involves a series of
actions, including typing in a username and password, but these specific actions may not be explicitly
mentioned. Finally, the steps outlined in tutorials are ungrounded, meaning they are not directly
associated with observable states or outcomes. Tutorials typically employ generic descriptions to
accommodate various instances of conceptually similar tasks. For example, as illustrated in Figure 1,
the tutorial merely instructs to “enter the keyword” without addressing any specific scenario."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.12037037037037036,"Based on these findings, we propose an iterative approach that first uses an LLM to rewrite an article
into a hypothetical trajectory in the format shown in §3.1, then we leverage a generative model to
synthesize the intermediate observation between two consecutive actions. First, in the rewriting step,
we ask the assistant LM to perform: (1) propose a hypothetical concrete scenario relevant to the task
(2) perform basic parsing such as translating “enter the keyword [...]” into type(“search bar”,
“Amazon Prime”); (3) categorize actions into groups that reflect the sub-task structures outlined
by coding blocks. These tasks mainly demand a LLMs’s creativity, language processing ability,
and event understanding respectively. An example of rewriting a how-to article into a trajectory in
program format is showed in Appendix E. The prompt for the rewriting step is in Appendix D."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.12268518518518519,"Because the previous procedures still only result in a sequence of ungrounded actions, we next
leverage the assistant LM to generate the observations between randomly sampled consecutive actions.
We use the consecutive actions of type(“search bar”, “Amazon Prime”) and click(“Amazon
Inc”, id=156) in Figure 1 as an example. There are mainly two requirements for generated
observations. First, the observation must reflect the outcomes of past actions. In the example, this
corresponds to a page with a user logged in, and a search input field filled with “Amazon Prime”.
Second, the observation encodes the necessary elements to perform the next action. In the example,
this corresponds to a payment history list with a payment to Amazon. We prompt the assistant
LM with the action sequence to generate a HTML snippet that fulfills the above requirements.
Since the next action requires the concrete element to interact with, we ask the model to insert a
tag of id=“next-action-target-element” in the corresponding HTML node to indicate the
grounding.5 This step mainly requires a model’s coding capabilities, particularly in web development.
However, we find that it is not necessary for the LLM to generate HTML with high fidelity and"
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.125,"2Some existing works also study using program formalization for web-based tasks [11, 46], but focus on
action-level interventions without incorporating task-level planning.
3https://www.wikihow.com/Main-Page
4https://www.wikihow.com/Apply-a-Gift-Card-Code-to-Amazon#Online
5The tag is removed through post-processing."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.12731481481481483,"complexity, which is a open research question [36]. The full prompt is in Appendix D and an example
for this step is in Figure 5."
SYNTHESIZING FROM TEXT PROCEDURAL KNOWLEDGE WITH GENERATIVE ENVIRONMENT,0.12962962962962962,"Note that in addition to wikiHow, there are other resources that share similar a similar procedural
flavor, such as the captions of YouTube how-to videos [25]. Our transformation mechanism is
generally applicable to such resources, but we leave concrete examination of them as future work."
SYNTHESIZING FROM RANDOM OBSERVATIONS,0.13194444444444445,"3.3
Synthesizing from Random Observations"
SYNTHESIZING FROM RANDOM OBSERVATIONS,0.13425925925925927,"While the procedure in the previous section results in real procedures, the LLM-based method
generates simplified observations. To compensate for this, we also perform data synthesis with real
observations, and use synthesis to generate simplified procedures. We show that these two sources can
compensate each other by examining the generated data in §4 and comparing the actual web-based
task performance in §6.3."
SYNTHESIZING FROM RANDOM OBSERVATIONS,0.13657407407407407,"Source
We utilize ClueWeb [27] as our data source, which comprises HTML snapshots of more than
10 billion web pages. Our initial analysis indicates that a random sampling approach would likely lead
to a homogeneous distribution dominated by less interactive pages, such as news articles. In contrast,
more complex web-based tasks typically require interactions with various web elements to advance
the tasks. To diversify the sampled web pages, we employed a temperature sampling approach to
select pages based on their content categories. In general, web pages from higher frequency top-level
domains in ClueWeb are typically more interactive, such as Amazon and Reddit, while domains
with lower frequency are less interactive, such as news article. We use a temperature sampling with
T = 0.6 so that the sample probability of choosing a page in domain i, P ′
i = p"
T,0.1388888888888889,"1
T
i /P k p"
T,0.1412037037037037,"1
T
k , where pi
is the original probability of choosing a page in domain i, and k is all the available domains. In doing
so, we up-sample more interactive sites while maintaining diversity on more rare sites. More details
are listed in Appendix J."
T,0.14351851851851852,"Synthesis Approach
We treat each sampled web page as an intermediate observation at time
step t, aiming to synthesize the specific task i, the previous actions a1, ..., at−1 and the subsequent
action at consisting of an action, a corresponding target element in the observation, and a natural
language summary of the action. We first convert a web page into its corresponding accessibility
tree at the beginning of each node, and sample a segment to present to the assistant LM. We follow
the WebArena convention of assigning a unique ID to each node in the tree to ease the challenge of
referencing the nodes. To increase the diversity of the tasks, we first instruct the model to brainstorm k
task categories relevant to the web domain. Then the model randomly selects three of these categories
and develops them into concrete scenarios with past actions leading up to the current observation and
the next action to take. The prompt is in Appendix F and an example generation is in Appendix G."
DATA FILTERING,0.14583333333333334,"3.4
Data Filtering"
DATA FILTERING,0.14814814814814814,"To ensure the quality of the training set, we apply a two-part filtering pipeline. In the first part, we
ensure that data samples are both complete and coherent. For a sample to pass this filter, it must
include all required components in the correct format. These components include: (1) an action
that falls within the defined action space, (2) a valid and meaningful action target element in the
corresponding web page, (3) NL texts that are well formed, e.g. without the use of “...” in the texts as
an abbreviation by the generative model, (4) overall comprehensive generation without placeholders
from the prompt (e.g., <brief step description>)."
DATA FILTERING,0.15046296296296297,"To further eliminate accurately formatted but unresponsive actions, we apply a second filtering step
using next state prediction. Here, we use the LLM to predict the next state, ot+1, based on the current
state ot and action at in our synthesized data. If the model predicts that ot+1 = ot, the action is
deemed to have no impact, and we filter out the action accordingly."
DATA STATISTICS,0.1527777777777778,"4
Data Statistics"
DATA STATISTICS,0.1550925925925926,"We assess the distribution of history lengths, task objectives, and observations in the form of accessi-
bility trees. The first statistic reflects general task complexity, as longer trajectories typically indicate"
DATA STATISTICS,0.1574074074074074,"Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings
more complex tasks. The latter two measure the domain diversity of our synthetic data. We also
present these statistics for the Mind2Web dataset as an example of human-collected trajectories."
DATA STATISTICS,0.1597222222222222,"1-2
3-4
5-6
7-8
9-10 11-12 13-14 15-16 17-18 19-20 21-22
History Length 0.0 0.1 0.2 0.3 0.4"
DATA STATISTICS,0.16203703703703703,Proportion
DATA STATISTICS,0.16435185185185186,Distribution of History Lengths
DATA STATISTICS,0.16666666666666666,"Tutorials
Random Observation
Mind2Web"
DATA STATISTICS,0.16898148148148148,"As shown on the right, the majority of our
synthetic data consists of trajectories with
a history length of fewer than eight steps,
regardless of the source. Fewer trajectories
have longer histories, with the longest ex-
ceeding 20 steps. Examples with longer
histories represent more complex scenarios
such as “Create a new YouTube ad cam-
paign for the Summer Collection with a
focus on the 25-34 age group interested
in sports, fitness, fashion, and style”. In
addition, we analyze the distribution of
generated intents by projecting their high-
dimensional vectors using the embedding model all-mpnet-base-v2 [30]. We visualize these
embeddings with t-SNE [40]. As shown on the left of Figure 2, intents synthesized from random
observations exhibit broader coverage compared to those from tutorials. This may be because humans
often prefer to write tutorials for critical domains, while randomly sampled observations offer a more
objective reflection of the diverse internet. Although our data synthesis process is entirely independent
of Mind2Web, the generated intents show substantial coverage of Mind2Web tasks, which were
created by human annotators from top-use websites. Finally, we apply the same embedding and
visualization approach for accessibility trees and display the results on the right of Figure 2. The
generated observations from tutorials overlap significantly with real web pages, both from random
observations and those in Mind2Web."
DATA STATISTICS,0.1712962962962963,"Cost
The average end-to-end cost per sample is $0.031, out of which $0.028 is used to generate the
actual sample, and $0.003 is used by our prediction-based filtering pipeline. These values represent
the final average cost to generate one valid sample, including the additional cost for samples that
were generated but later filtered out, which we distributes across all non-filtered samples."
EXPERIMENTAL SETUP,0.1736111111111111,"5
Experimental Setup"
EXPERIMENTAL SETUP,0.17592592592592593,"Agent training
We finetune CodeLlama-7b [31] with 99, 920 web-navigation instruction exam-
ples, sourced from WikiHow articles and ClueWeb web pages 6. Training details can be found in
Appendix K."
EXPERIMENTAL SETUP,0.17824074074074073,"Evaluation tasks
We select three evaluation tasks in the domain of web navigation. (1) the
Mind2Web test set [6]. It consists of three categories, namely, cross-task, cross-website, and cross-
domain—ranked by their degree of deviation from the training data. Since our model is not trained"
EXPERIMENTAL SETUP,0.18055555555555555,"6We select CodeLlama-7b after comparing Llama-2, Llama-3, Llama-3-Instruct, CodeLlama,
CodeLlama-Instruct, DeepSeek Coder, and Mistral by finetuning and evaluating on Mind2Web’s train
and test set."
EXPERIMENTAL SETUP,0.18287037037037038,"Table 1: Performance of various models in three web-based task benchmarks. We measure step
accuracy (%) for Mind2Web, and task success rate (%) for MiniWoB++ and WebArena. The numbers
of FireAct-7b is taken from [5]; AutoWebGLM-7b(S1) represents the model trained with only
synthetic data in [19]. All other numbers are produced by our work."
EXPERIMENTAL SETUP,0.18518518518518517,"Model
Mind2Web
MiniWoB++
WebArena"
EXPERIMENTAL SETUP,0.1875,"Single step
Short
Long
Reference-based
Execution-based
Execution-based"
EXPERIMENTAL SETUP,0.18981481481481483,API-based Models
EXPERIMENTAL SETUP,0.19212962962962962,"GPT-3.5
12.79
39.57
6.16
GPT-4
29.09
53.04
14.41"
EXPERIMENTAL SETUP,0.19444444444444445,Open-source Instructed Models
EXPERIMENTAL SETUP,0.19675925925925927,"CodeLlama-instruct-7b
6.62
23.04
0.00
Llama3-chat-8b
11.50
31.74
3.32
Llama3-chat-70b
22.27
48.70
7.02"
EXPERIMENTAL SETUP,0.19907407407407407,Open-source Interactive Data Finetuned Models
EXPERIMENTAL SETUP,0.2013888888888889,"FireAct-7b [3]
-
-
0.25∗"
EXPERIMENTAL SETUP,0.2037037037037037,"AgentLM-7b [51]
2.99
15.65
0.86
CodeActAgent-7b [41]
3.13
9.78
2.34
AutoWebGLM-7b(S1) [19]
-
-
2.50∗"
EXPERIMENTAL SETUP,0.20601851851851852,"AgentFlan-7b [5]
3.80
20.87
0.62
Lemur-chat-70b [46]
14.28
21.30
2.95
AgentLM-70b [51]
10.61
36.52
3.07"
EXPERIMENTAL SETUP,0.20833333333333334,"Synatra-CodeLlama-7b
15.85
38.20
6.28"
EXPERIMENTAL SETUP,0.21064814814814814,"on the Mind2Web training set, all categories are treated as held-out sets. Therefore, we report an
overall step accuracy as the average across all examples. In addition, we simplify the two-stage setup
used in Mind2Web, where the model first selects the top 50 candidate elements from the HTML
before predicting the action on a selected element. Instead, we input in-viewport accessibility trees
that include the ground-truth element. This approach is more challenging because it introduces more
irrelevant elements, but it streamlines the prediction process by eliminating the need for an additional
element selection model. (2) MiniWoB++ [21]. It is an interactive benchmark with simplified
web pages and low-level tasks (e.g., “Enter BUL to the box”). On average, completing a task in
MiniWoB++ requires fewer steps than that of WebArena. We tested on a text-only subset of tasks
used in [51, 41]. To get the final score, we take the average score of five runs. (3) WebArena. In
WebArena, agents are required to navigate in real-world web applications to accomplish high-levels
web-based tasks (e.g., “How much I spent last month on grocery”). Both MiniWoB++ and WebArena
offer outcome-based evaluations, using programmatic checkers to validate key features after execution
and assess whether the task has been successfully completed."
EXPERIMENTAL SETUP,0.21296296296296297,"Baselines
We compare the performance of our model with 12 baseline models, including API-based
models such as GPT-4, open-source instructed models like CodeLlama, and models finetuned with
interactive data. For instance, Agent-LM [51] is finetuned on supervised data for tasks such as web
navigation and interactive coding. Since most baseline models are not optimized for code generation,
we follow Zhou et al. [58] and prompt the models to generate natural language responses instead of
programs. The same prompt, providing general web navigation instructions without dataset-specific
explanations, is used across three datasets. We show the prompt in Appendix H. Notably, we adopt
the original prompts of AgentLM and CodeActAgent [41] when evaluating them on MiniWoB++ to
be consistent with their original evaluations. We further remove the task-specific in-context examples
to ensure a fair comparison with other models in zero-shot settings."
RESULTS,0.2152777777777778,"6
Results"
MAIN RESULTS,0.2175925925925926,"6.1
Main Results"
MAIN RESULTS,0.2199074074074074,"Table 1 presents the performance of various models across three web-based task benchmarks. Overall,
Synatra-CodeLlama achieves the best performance among models of comparable size. Notably,"
MAIN RESULTS,0.2222222222222222,Table 2: Error rates (%) on five fine-grained metrics.
MAIN RESULTS,0.22453703703703703,"Error Type
GPT-4-turbo
Llama3-chat-70b
Llama3-chat-8b
CodeLlama-Instruct-7b
Ours"
MAIN RESULTS,0.22685185185185186,"Nonexistent element
0.56
0.37
2.40
94.70
0.70
Invalid actions
1.75
2.17
9.74
58.88
4.91"
MAIN RESULTS,0.22916666666666666,"Click non-clickable
5.34
3.40
15.20
100.00
5.77
Type non-typable
1.01
1.34
11.97
32.62
4.34"
MAIN RESULTS,0.23148148148148148,"Repeated type
26.85
44.72
32.14
46.32
6.79"
MAIN RESULTS,0.2337962962962963,"Synatra-CodeLlama significantly outperforms its base model CodeLlama-instruct-7b. While
CodeLlama-instruct-7b fails to complete any tasks in WebArena, Synatra-CodeLlama suc-
cessfully executes 6.28% of them. Furthermore, Synatra-CodeLlama elevates the performance
of CodeLlama-instruct-7b from 6.62% to 15.85% in Mind2Web (a 139.42% relative improve-
ment) and from 23.04% to 38.20% in MiniWoB++. More encouragingly, Synatra-CodeLlama
demonstrates superior performance on Mind2Web and WebArena compared to GPT-3.5. It also
outperforms Lemur-chat-70b, which is finetuned with interactive data and is ten times larger, across
all three benchmarks. The results suggest that our data synthesis approach is effective in helping the
model predict the next action (as in Mind2Web) and performing simple tasks with a few steps (as in
MiniWoB++). The synthesized data can also guide the model towards executing real-world complex
tasks more accurately."
MAIN RESULTS,0.2361111111111111,"Synatra-CodeLlama surpassed the performance of all open-source model finetuned with interactive
data. Among these models, AgentLM, CodeActAgent and AgentFlan include demonstrations to
perform web-based tasks in their instruction finetuning dataset. However, we find that these models
may not serve as capable agents to perform web-based tasks due to the special design choice encoded
in the finetuned models. For instance, AgentLM and CodeActAgent use Regex expression to match
interactive element on a web page and require carefully selected in-context examples to showcase
which are the proper Regex expression for different examples. However, Regex expressions only
work for simple web pages with a few elements as in MiniWoB++, while it is prohibitive to do
pattern matching in complex web pages as in Mind2Web and WebArena. As a result, when we
experiment with the more generic action space which is suitable for all three benchmarks without
in-context examples, we see these models have a significant performance degradation. On the other
hand, Synatra-CodeLlama targets at the generic web-based tasks and does not encode any dataset
artifacts during training. Even though all three benchmarks are completely held-out during data
generation, Synatra-CodeLlama achieves consistent superior performance on all benchmarks."
ANALYSIS,0.23842592592592593,"6.2
Analysis"
ANALYSIS,0.24074074074074073,"What does the model learn from the synthetic data?
Our analysis suggests that the baseline
acquires essential knowledge at multiple levels from synthetic data. The error rates for different error
types are shown in Table 2. At the low level, synthetic data helps the model emit valid actions. For ex-
ample, while approximately 95% of actions predicted by the base model CodeLlama-instruct-7b
involve interacting with non-existent elements on the web page, Synatra reduces this error to just
0.7%. This low error rate is closer to that of larger models such as Llama3-chat-70b and GPT-4,
which are better at following complex instructions with multiple requirements. Additionally, the
synthetic data improves the model’s ability to understand web pages more accurately. To assess
this, we measure the ratio of invalid actions, including both invalid click and type, where the
predicted action attempts to interact with an element that is not clickable (e.g., a piece of text) or
not typable (e.g., a button). We observe that models in the 7b–8b range have high error rates in
interpreting basic web elements. For instance, the strong 8b model Llama3-chat-8b mistakenly
click a non-clickable element over 15% of the time, while Synatra reduces this error to under
6% approaching the performance of GPT-4. Finally, at task completion level, Synatra shows a
stronger ability to track progress accurately. Specifically, when filling out forms on web pages,
Synatra-CodeLlama is 74% less likely than GPT-4-turbo to repeatedly input the same words into
the same field, a common error in models. This indicates that our synthetic data enables the model to
more precisely recognize completed actions and identify the remaining steps needed to achieve the
goal. More qualitative study can be found in Appendix I."
ANALYSIS,0.24305555555555555,"Can Synatra be as effective as human annotations?
We compare models trained with 9k human
demonstrations (human only) from Mind2Web [6] and 9k demonstrations generated by Synatra. The"
ANALYSIS,0.24537037037037038,"results are shown in Figure 3. Simply training CodeLlama with the Mind2Web human annotations
provides modest improvement of 3.96% on MiniWoB++ , while failing entirely on WebArena."
ANALYSIS,0.24768518518518517,MiniWoB++
ANALYSIS,0.25,Task Success Rate (%) 40.87 36.09 27.00 23.04
ANALYSIS,0.2523148148148148,WebArena 4.56 2.09
ANALYSIS,0.25462962962962965,"0.00
0.00"
ANALYSIS,0.2569444444444444,"Synatra
Human + synthesis"
ANALYSIS,0.25925925925925924,"Human only
CodeLlama"
ANALYSIS,0.26157407407407407,"Figure 3: the comparison between the models
trained with trajectories generated by our approach
and the data collected from human."
ANALYSIS,0.2638888888888889,"In contrast, Synatra led to a substantial im-
provement of 17.81% on MiniWoB++ and
4.56% on WebArena. The limited performance
of the human annotations can be partially at-
tributed to their restricted task coverage: notably,
Mind2Web lacks information seeking tasks that
require a string answer. Furthermore, the hu-
man trajectories did not specify conditions for
terminating task execution. Despite adding such
trajectories from Synatra (human + synthesis),
the model still under-performed."
ANALYSIS,0.2662037037037037,"We hypothesize the diversity of tasks plays a
role in this discrepancy, since many tasks cannot
be covered without pre-set environment (e.g.,
return an order) (Figure 2). These findings un-
derscore the efficacy of Synatra, which also
exempts from the complexities of developing recording tools and managing communication with
annotators. However, it is important to recognize that the quality of human demonstrations is
presumably higher, but they require meticulous design and control during the data collection process."
ABLATIONS,0.26851851851851855,"6.3
Ablations"
ABLATIONS,0.2708333333333333,"In this section, we perform an ablation to validate the design choices of our data synthesis approach. 0 0 0 0 0 1"
K,0.27314814814814814,"18k
50k
100k
Count of Training Samples 4.5 5.0 5.5 6.0"
K,0.27546296296296297,WebArena Success Rate
K,0.2777777777777778,"Model performance improves with more synthetic data
To assess the impact of scaling our synthetic data, we trained
three CodeLlama 7B models with 18k, 50k, and 100k sam-
ples respectively, while keeping all other parameters con-
stant. We then evaluated the models on WebArena. As
shown on the right, the success rates on WebArena steadily
increase as the synthetic training data scales from 18k to
100k samples. This highlights the potential of scaling syn-
thetic data with our approach."
K,0.2800925925925926,"Representing trajectories as programs is beneficial
To verify if the programs format is helpful,
we convert 30k trajectories to the NL format similar to setting in WebArena [58] and compare its
performance with model trained with the exact data, but in our program format. The results are shown
in Figure 4a. We can see that performance drops on both MiniWoB++ and WebArena when using
the NL format. We hypothesize that program, in the form of API calls, is potentially a more natural
format to represent sequence of actions. Our observation also echo the observations of using program
representation for non-programming tasks [23, 29, 41], while our experiments further contributes
insights towards finetuning setups for interactive tasks."
K,0.2824074074074074,"Different sources of indirect knowledge complement each other
Our indirect knowledge comes
from two sources: tutorials and randomly sampled web pages. In the former source, the procedural
knowledge from tutorials are written and verified by human. However, there is no guarantee on
the authenticity of the generated observations of web pages. In contrast, the observations from
the latter sources are completely real, while there is no guarantee of the trajectory accuracy. We
hypothesize that the two sources can compensate each other. To test this hypothesis, we trained three
models: one using 9k synthetic data mixed from both sources, and two others each using 9k data
exclusively from one of the sources and the results are shown at Figure 4b. We observe a noticeable
performance degradation when models are trained with data from only one source. This indicates that
utilizing multiple sources yields a more comprehensive dataset by integrating the precise procedural
knowledge from tutorials with the realistic observations of web snapshot data."
K,0.2847222222222222,MiniWoB++
K,0.28703703703703703,Task Success Rate (%) 36.52 36.09
K,0.28935185185185186,WebArena 4.31 4.19
K,0.2916666666666667,"Program Format
NL Format"
K,0.29398148148148145,"(a) Comparison between differ-
ent trajectory formats."
K,0.2962962962962963,MiniWoB++
K,0.2986111111111111,Task Success Rate (%) 40.87 36.25 26.09
K,0.30092592592592593,WebArena 4.56 3.33 1.48
K,0.30324074074074076,"Both sources
Random observation only"
K,0.3055555555555556,Tutorials only
K,0.30787037037037035,"(b) Comparison between different sources of indi-
rect knowledge."
K,0.3101851851851852,WebArena
K,0.3125,Task Success Rate (%) 6.28
K,0.3148148148148148,"4.20
4.06"
K,0.31712962962962965,"Finetune w/ Synatra
RAG-Llama3.1
Llama3.1"
K,0.3194444444444444,"(c) Different ways to
use knowledge."
K,0.32175925925925924,"Figure 4: ablation on trajectory formats, sources of knowledge, and ways to use knowledge."
K,0.32407407407407407,"Models have difficulty learning from indirect sources alone
To access if turning knowledge
into trajectories is helpful, we tested a retrieval-augmented approach that directly uses the col-
lection of tutorials as the knowledge base. We first projected text tutorials to embeddings with
all-mpnet-base-v2 [30]. Then a retriever retrieves the most relevant three tutorials measured by
cosine similarity. These tutorials were included as additional context in the prompt. We tested this
approach with LLama3.1-instruct-8B. As shown in Figure 4c, feeding in tutorials directly im-
proves model performance on WebArena marginally , and finetuning on our data shows a substantial
advantage. This comparison indicates that models have difficulty making use of indirect knowledge
to solve agent tasks, and trajectories is a preferable format for agent learning."
RELATED WORK,0.3263888888888889,"7
Related Work"
RELATED WORK,0.3287037037037037,"Learning from Indirect Supervision
Due to the costly nature of human supervision, many digital
agent learning works explore learning from existing yet indirect knowledge sources [8, 35, 12, 55],
reinforcement learning optimization that learn from environment feedback [37, 21, 32, 39]. More
recently, LLM self-improvement during inference time [34, 49, 28, 44, 14, 52] has been applied on
creating more complex digital agents in the wild. Our work fills the gap of training with synthetic
data generated from existing resources, i.e., indirect supervision, on complex web navigation tasks."
RELATED WORK,0.33101851851851855,"Prompting Approaches for AI Agents
Existing methods include performing reasoning about
the current statues before proceeding to next actions [43, 47, 22], search and planning [48, 13],
and self-verification [18, 34, 24]. Our focus on instruction tuning data generation without human
supervision can hopefully enable synthetic data generation recipes for various kinds of instruction
prompts for agent tasks."
RELATED WORK,0.3333333333333333,"Data Generation for Interactive Agents and Instruction Finetuning
Existing works design
ways of generating training data that adapt LLMs to agent-specific tasks [11, 19], while our work
aim to generate more realistic data without human intervention. Considering more broadly over all
instruction finetuning: [59, 50, 38, 16] generate instructions of a specific task given a few examples.
[42, 15] generates task-agnostic large-scale instruction tuning data without given examples. Li
et al. [20] adopts an iterative instruction data generation and model finetuning approach, While our
work generates instruction tuning datasets from readily available, indirect, unstructured knowledge
resources."
CONCLUSION,0.33564814814814814,"8
Conclusion"
CONCLUSION,0.33796296296296297,"We propose a data synthesis approach Synatra. Empirically we showcase that finetuning with
data generated from our approach can improve existing general-purpose LLMs to perform better on
digital agent tasks. Since Synatra can synthesize trajectories given a single, static piece of indirect
knowledge, we argue that when equipped with a capable LLM, a regular tutorial designed for human
consumption and a random observation, can also be re-purposed a trajectory. We show that even
considering the relative high cost of calling state-of-the-art LLMs such as GPT-4, synthesizing is
more cost-effective than collecting human demonstrations of similar quantity for model training."
CONCLUSION,0.3402777777777778,Acknowledgements
CONCLUSION,0.3425925925925926,"This research project has benefitted from the Microsoft Accelerate Foundation Models Research
(AFMR) grant program, and a grant from Amazon Web Services. We would also like to thank Center
of AI Safety (CAIS) and CMU FLAME center for compute support."
REFERENCES,0.3449074074074074,References
REFERENCES,0.3472222222222222,"[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.34953703703703703,"[2] S. Branavan, H. Chen, L. Zettlemoyer, and R. Barzilay. Reinforcement learning for mapping
instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP, pages 82–90, Suntec, Singapore, 2009. Association for Computational Linguistics.
URL https://aclanthology.org/P09-1010."
REFERENCES,0.35185185185185186,"[3] B. Chen, C. Shu, E. Shareghi, N. Collier, K. Narasimhan, and S. Yao. Fireact: Toward language
agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023."
REFERENCES,0.3541666666666667,"[4] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling
computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,
2022."
REFERENCES,0.35648148148148145,"[5] Z. Chen, K. Liu, Q. Wang, W. Zhang, J. Liu, D. Lin, K. Chen, and F. Zhao. Agent-flan:
Designing data and methods of effective agent tuning for large language models. arXiv preprint
arXiv:2403.12881, 2024."
REFERENCES,0.3587962962962963,"[6] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web:
Towards a generalist agent for the web, 2023."
REFERENCES,0.3611111111111111,"[7] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. D. Verme, T. Marty, L. Boisvert, M. Thakkar,
Q. Cappart, D. Vazquez, N. Chapados, and A. Lacoste. Workarena: How capable are web
agents at solving common knowledge work tasks?, 2024."
REFERENCES,0.36342592592592593,"[8] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,
and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale
knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2022. URL https://openreview.net/forum?id=rc8o_j8I8PX."
REFERENCES,0.36574074074074076,"[9] H. Furuta, O. Nachum, K.-H. Lee, Y. Matsuo, S. S. Gu, and I. Gur. Multimodal web navigation
with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023."
REFERENCES,0.3680555555555556,"[10] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-
aided language models. In International Conference on Machine Learning, pages 10764–10799.
PMLR, 2023."
REFERENCES,0.37037037037037035,"[11] I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. A real-world
webagent with planning, long context understanding, and program synthesis. arXiv preprint
arXiv:2307.12856, 2023."
REFERENCES,0.3726851851851852,"[12] J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang. Reinforcement cutting-agent learning for
video object segmentation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 9080–9089, 2018."
REFERENCES,0.375,"[13] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language
model is planning with world model. arXiv preprint arXiv:2305.14992, 2023."
REFERENCES,0.3773148148148148,"[14] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an
end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024."
REFERENCES,0.37962962962962965,"[15] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language
models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022."
REFERENCES,0.3819444444444444,"[16] O. Honovich, U. Shaham, S. R. Bowman, and O. Levy. Instruction induction: From few
examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022."
REFERENCES,0.38425925925925924,"[17] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson,
P. Georgiev, A. Santoro, and T. Lillicrap. A data-driven approach for learning to control
computers. In International Conference on Machine Learning, pages 9466–9482. PMLR, 2022."
REFERENCES,0.38657407407407407,"[18] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. ArXiv preprint,
abs/2303.17491, 2023. URL https://arxiv.org/abs/2303.17491."
REFERENCES,0.3888888888888889,"[19] H. Lai, X. Liu, I. L. Iong, S. Yao, Y. Chen, P. Shen, H. Yu, H. Zhang, X. Zhang, Y. Dong, et al.
Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent.
arXiv preprint arXiv:2404.03648, 2024."
REFERENCES,0.3912037037037037,"[20] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Self-
alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023."
REFERENCES,0.39351851851851855,"[21] E. Z. Liu, K. Guu, P. Pasupat, T. Shi, and P. Liang. Reinforcement learning on web interfaces us-
ing workflow-guided exploration. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=ryTp3f-0-."
REFERENCES,0.3958333333333333,"[22] C.-f. Lo, A. Sridhar, H. Zhu, F. F. Xu, and S. Zhou. Hierarchical prompting assists large language
model on web navigation. In The 61st Annual Meeting Of The Association For Computational
Linguistics, 2023."
REFERENCES,0.39814814814814814,"[23] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig. Language models of code are few-shot
commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pages 1384–1403, Abu Dhabi, United Arab Emirates, 2022. Association
for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.90."
REFERENCES,0.40046296296296297,"[24] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,
S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in
Neural Information Processing Systems, 36, 2024."
REFERENCES,0.4027777777777778,"[25] A. Miech, D. Zhukov, J. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning a
text-video embedding by watching hundred million narrated video clips. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27
- November 2, 2019, pages 2630–2640. IEEE, 2019. doi: 10.1109/ICCV.2019.00272. URL"
REFERENCES,0.4050925925925926,https://doi.org/10.1109/ICCV.2019.00272.
REFERENCES,0.4074074074074074,"[26] S. Murty, C. Manning, P. Shaw, M. Joshi, and K. Lee. Bagel: Bootstrapping agents by guiding
exploration with language. arXiv preprint arXiv:2403.08140, 2024."
REFERENCES,0.4097222222222222,"[27] A. Overwijk, C. Xiong, X. Liu, C. VandenBerg, and J. Callan. Clueweb22: 10 billion web
documents with visual and semantic information. arXiv preprint arXiv:2211.15848, 2022."
REFERENCES,0.41203703703703703,"[28] J. Pan, Y. Zhang, N. Tomlin, Y. Zhou, S. Levine, and A. Suhr. Autonomous evaluation and
refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024."
REFERENCES,0.41435185185185186,"[29] H. Puerto, M. Tutek, S. Aditya, X. Zhu, and I. Gurevych. Code prompting elicits conditional
reasoning abilities in text+ code llms. arXiv preprint arXiv:2401.10065, 2024."
REFERENCES,0.4166666666666667,"[30] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.
10084."
REFERENCES,0.41898148148148145,"[31] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,
J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950,
2023."
REFERENCES,0.4212962962962963,"[32] P. Shaw, M. Joshi, J. Cohan, J. Berant, P. Pasupat, H. Hu, U. Khandelwal, K. Lee, and K. N.
Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user
interfaces. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.4236111111111111,"[33] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain
platform for web-based agents. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3135–3144.
PMLR, 2017. URL http://proceedings.mlr.press/v70/shi17a.html."
REFERENCES,0.42592592592592593,"[34] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory
and self-reflection. ArXiv preprint, abs/2303.11366, 2023. URL https://arxiv.org/abs/
2303.11366."
REFERENCES,0.42824074074074076,"[35] M. Shridhar, X. Yuan, M. Côté, Y. Bisk, A. Trischler, and M. J. Hausknecht. Alfworld: Aligning
text and embodied environments for interactive learning. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,
2021. URL https://openreview.net/forum?id=0IOX0YcCdTn."
REFERENCES,0.4305555555555556,"[36] C. Si, Y. Zhang, Z. Yang, R. Liu, and D. Yang. Design2code: How far are we from automating
front-end engineering? arXiv preprint arXiv:2403.03163, 2024."
REFERENCES,0.43287037037037035,"[37] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.4351851851851852,"[38] C. Singh, J. X. Morris, J. Aneja, A. Rush, and J. Gao. Explaining data patterns in natu-
ral language with language models. In Y. Belinkov, S. Hao, J. Jumelet, N. Kim, A. Mc-
Carthy, and H. Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyz-
ing and Interpreting Neural Networks for NLP, pages 31–55, Singapore, Dec. 2023. As-
sociation for Computational Linguistics.
doi: 10.18653/v1/2023.blackboxnlp-1.3.
URL
https://aclanthology.org/2023.blackboxnlp-1.3."
REFERENCES,0.4375,"[39] Y. Song, D. Yin, X. Yue, J. Huang, S. Li, and B. Y. Lin. Trial and error: Exploration-based
trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502, 2024."
REFERENCES,0.4398148148148148,"[40] L. van der Maaten and G. E. Hinton. Visualizing data using t-sne. Journal of Machine Learn-
ing Research, 9:2579–2605, 2008. URL https://api.semanticscholar.org/CorpusID:
5855042."
REFERENCES,0.44212962962962965,"[41] X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable code actions elicit
better llm agents, 2024."
REFERENCES,0.4444444444444444,"[42] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:
Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560,
2022."
REFERENCES,0.44675925925925924,"[43] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-
thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems, 35:24824–24837, 2022."
REFERENCES,0.44907407407407407,"[44] Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, and L. Kong. Os-copilot: Towards
generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024."
REFERENCES,0.4513888888888889,"[45] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei,
Y. Liu, Y. Xu, S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu. Osworld: Benchmarking
multimodal agents for open-ended tasks in real computer environments, 2024."
REFERENCES,0.4537037037037037,"[46] Y. Xu, H. Su, C. Xing, B. Mi, Q. Liu, W. Shi, B. Hui, F. Zhou, Y. Liu, T. Xie, et al. Lemur:
Harmonizing natural language and code for language agents. arXiv preprint arXiv:2310.06830,
2023."
REFERENCES,0.45601851851851855,"[47] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing
reasoning and acting in language models. ArXiv preprint, abs/2210.03629, 2022. URL https:
//arxiv.org/abs/2210.03629."
REFERENCES,0.4583333333333333,"[48] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:
Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023.
URL https://arxiv.org/abs/2305.10601."
REFERENCES,0.46064814814814814,"[49] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:
Deliberate problem solving with large language models. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.46296296296296297,"[50] S. Ye, D. Kim, J. Jang, J. Shin, and M. Seo. Guess the instruction! flipped learning makes
language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022."
REFERENCES,0.4652777777777778,"[51] A. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang. Agenttuning: Enabling
generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023."
REFERENCES,0.4675925925925926,"[52] C. Zhang, Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, and G. Yu. Appagent: Multimodal
agents as smartphone users, 2023."
REFERENCES,0.4699074074074074,"[53] L. Zhang, Q. Lyu, and C. Callison-Burch. Reasoning about goals, steps, and temporal ordering
with wikihow. arXiv preprint arXiv:2009.07690, 2020."
REFERENCES,0.4722222222222222,"[54] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v (ision) is a generalist web agent, if grounded.
arXiv preprint arXiv:2401.01614, 2024."
REFERENCES,0.47453703703703703,"[55] S. Zhou, U. Alon, F. F. Xu, Z. Wang, Z. Jiang, and G. Neubig. Docprompting: Generating code
by retrieving the docs. ArXiv preprint, abs/2207.05987, 2022. URL https://arxiv.org/
abs/2207.05987."
REFERENCES,0.47685185185185186,"[56] S. Zhou, P. Yin, and G. Neubig. Hierarchical control of situated agents through natural language.
In Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI),
pages 67–84, Seattle, USA, 2022. Association for Computational Linguistics. doi: 10.18653/
v1/2022.suki-1.8. URL https://aclanthology.org/2022.suki-1.8."
REFERENCES,0.4791666666666667,"[57] S. Zhou, L. Zhang, Y. Yang, Q. Lyu, P. Yin, C. Callison-Burch, and G. Neubig. Show me more
details: Discovering hierarchies of procedures from semi-structured web data. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 2998–3012, Dublin, Ireland, 2022. Association for Computational Linguistics.
doi: 10.18653/v1/2022.acl-long.214. URL https://aclanthology.org/2022.acl-long.
214."
REFERENCES,0.48148148148148145,"[58] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried,
U. Alon, and G. Neubig. Webarena: A realistic web environment for building autonomous
agents. In International Conference on Learning Representations (ICLR), Vienna, Austria, May
2024. URL https://arxiv.org/abs/2307.13854."
REFERENCES,0.4837962962962963,"[59] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models
are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022."
REFERENCES,0.4861111111111111,Table of Contents in Appendix
REFERENCES,0.48842592592592593,"A Action Space
16"
REFERENCES,0.49074074074074076,"B Trajectory Representation
16"
REFERENCES,0.4930555555555556,"C Prompt to Filter wikiHow Articles
16"
REFERENCES,0.49537037037037035,"D Prompt to Generate Demonstrations from Tutorials
17"
REFERENCES,0.4976851851851852,"E
Example Synthetic Demonstration from Tutorials from a wikiHow Article
19"
REFERENCES,0.5,"F
Prompt to Generate Direct Demonstrations from Random Observations
20"
REFERENCES,0.5023148148148148,"G Example Generated Trajectories from Random Observation
22"
REFERENCES,0.5046296296296297,"H Prompt Example
25"
REFERENCES,0.5069444444444444,"I
Case Study
26"
REFERENCES,0.5092592592592593,"J
Data Selection from Random Observations in ClueWeb
28"
REFERENCES,0.5115740740740741,"K Training Settings
28"
REFERENCES,0.5138888888888888,"L
Broader Impacts
29"
REFERENCES,0.5162037037037037,"M Limitations
29"
REFERENCES,0.5185185185185185,"A
Action Space"
REFERENCES,0.5208333333333334,"Table 3: Action Space of WebArena
Action Type
Description"
REFERENCES,0.5231481481481481,"noop
Do nothing
click(elem)
Click at an element
hover(elem)
Hover on an element
type(elem, text)
Type to an element
press(key_comb)
Press a key comb
scroll(dir)
Scroll up and down"
REFERENCES,0.5254629629629629,"tab_focus(index)
focus on i-th tab
new_tab
Open a new tab
tab_close
Close current tab"
REFERENCES,0.5277777777777778,"go_back
Visit the last URL
go_forward
Undo go_back
goto(URL)
Go to URL"
REFERENCES,0.5300925925925926,"B
Trajectory Representation"
REFERENCES,0.5324074074074074,"website = ""<url>""
observation = ""<AXtree of the page>""
objective = ""Find and review the estimated value of your property on the website."""
REFERENCES,0.5347222222222222,"# past actions
def solve():
# sub-task 1: Look up your property on Zillow
# step 1: Search for your address on Zillow’s homepage search
bar to open the property page.
type(element_id=""6135"", string=""Main St"")
# step 2: From the property details page, navigate to the ""Zestimate"" section.
scroll(down)"
REFERENCES,0.5370370370370371,"# sub-task 2: Begin adjusting the estimated value
# step 3: Click on 'Edit home facts' to adjust details that
might affect the home's estimated value.
click(element_id=""9945"")"
REFERENCES,0.5393518518518519,"C
Prompt to Filter wikiHow Articles"
REFERENCES,0.5416666666666666,Prompt to filter wikiHow articles
REFERENCES,0.5439814814814815,"Given the title of an article, determine if it is about performing a task solely with computer or mobile phone’s
graphical user interface, and without any physical world configurations."
REFERENCES,0.5462962962962963,"input: How to Set Up Chromecast WiFi (Using an Android Phone or Tablet)
output: Set Up Chromecast WiFi involves both a mobile application and physical interactions with the
Chromecast device such as plug in the device, so the answer is ""No"""
REFERENCES,0.5486111111111112,"input: How to Change Your Desktop Wallpaper on Linux Mint (Using the Linux Mint Wallpapers)
output: Linux Mint is a desktop operating system, and changing the desktop wallpaper is typically done
through the system settings or desktop environment’s configuration tools, which are desktop applications, so
the answer is ""Yes"""
REFERENCES,0.5509259259259259,"input: How to delete a file using command line in Linux
output: Command line interface (CLI) in Linux is a text-based interface not a graphical user interface (GUI),
so the answer is ""No"""
REFERENCES,0.5532407407407407,"input: How to Reboot an iPad (Frozen iPads)
output: Rebooting an iPad usually involves physical actions like pressing and holding buttons on the iPad, so
the answer is ""No"""
REFERENCES,0.5555555555555556,"input: How to Connect the Kindle Fire to the
Internet (Connecting to an Existing Wi-Fi Network)
output: Kindle is neither a computer nor a mobile phone, so the answer is ""No"""
REFERENCES,0.5578703703703703,"input: How to Pair AirPods to an iPhone (If Your AirPods Won’t Connect)
output: Pairing AirPods with an iPhone typically includes physical actions such as opening the AirPods case
near the iPhone and possibly pressing a button on the AirPods case, so the answer is ""No"""
REFERENCES,0.5601851851851852,"input: {{Title of the article}}
output: {{Model prediction}}"
REFERENCES,0.5625,"D
Prompt to Generate Demonstrations from Tutorials"
REFERENCES,0.5648148148148148,Prompt to rewrite an article to a trajectory in program format
REFERENCES,0.5671296296296297,"# Task overview
You are given an article about performing a task in a web browser. Your goal is to make this article as
accessible as possible to a user who is not familiar with the functionalities of the websites and the task at all."
REFERENCES,0.5694444444444444,"# Guideline
Read the article carefully and follow the instructions below:
- Assume you start with the home page of the web application, skip the initial ‘goto‘ action.
- Break down the article into a sequence of steps.
- In every step, provide a concrete example that reflects a real execution. This example should clearly describe
the element you are interacting with, the concrete value of an element you select, the precise content you
type and other details. Never use broad descriptions. The example should be creative and realistic, avoid
boilerplate text such as email@example.com. Make sure that the example is consistent across steps.
- Following the concrete example, provide the Python API call corresponding to the example.
- Group all API calls into multiple sub-sections, each section corresponds to a logical and actionable sub-task."
REFERENCES,0.5717592592592593,"There are special scenarios and here are the ways to deal with them: - If the article describes multiple
scenarios or multiple ways to approach the same goal, you can use your own judgement to choose the most
common one to describe. - If there are repeated steps, make sure to unroll the steps and describe each of
them canonically. - Always assume you perform this task using a web browser, if the original article uses
a desktop app or mobile phone app, simply assume the corresponding web app exists. Hence, any steps
regarding installation or login can be skipped."
REFERENCES,0.5740740740740741,"# APIs
The APIs are as follows: ‘click(element_desc: str)‘ - Click on an element.
‘element_desc‘ is the the displayed text or the most representative attribute of the HTML element.
‘hover(element_desc: str)‘ - Hover over an element.
‘click_and_type(element_desc: str, content: str)‘ - Click an input element and type ‘content‘ into it.
‘key_press(key_comb: str)‘ - Press a key combination. ‘key_comb‘ is the combination of keys you want to
press on. The default OS is MacOS if there is no explicit specification in the article.
‘goto(url: str)‘ - Navigate to ‘url‘
‘go_back()‘ - Go back to the previous page.
‘go_forward()‘ - Go forward to the next page."
REFERENCES,0.5763888888888888,"‘new_tab()‘ - Open a new tab.
‘close_tab()‘ - Close the current tab.
‘switch_tab(tab_index: int)‘ - Switch to the tab with index ‘tab_index‘, starting from 0."
REFERENCES,0.5787037037037037,"# Response format
Your response should follow the following format.
“‘python
sub-task <index>: <sub-task description>
# step <index>: <the real execution with concrete values for each argument>
<API, do not skip the keys in the API calls>"
REFERENCES,0.5810185185185185,"# step <index>: <the real execution with concrete values for each argument>
<API, do not skip the keys in the API calls>"
REFERENCES,0.5833333333333334,<repeat for all sub tasks>
REFERENCES,0.5856481481481481,"# task:
<task command given to a smart assistant, only the necessary details on expectation are
needed.>
“‘"
REFERENCES,0.5879629629629629,"# Article
{{Article here}}"
REFERENCES,0.5902777777777778,Prompt to generate observation for two consecutive actions
REFERENCES,0.5925925925925926,"# HTML Background Knowledge
Commonly used interactable elements in HTML:
[’a’, ’button’, ’input’, ’textarea’, ’select’, ’option’, ’label’, ’form’, ’details’, ’summary’, ’map’, ’area’,
’iframe’, ’embed’, ’object’, ’dialog’, ’menu’, ’fieldset’, ’legend’, ’datalist’, ’output’, ’progress’, ’meter’,
’keygen’]"
REFERENCES,0.5949074074074074,"# Task Overview
You are given:
- A browser-based task
- A seuqnece of past actions to perform the task and
- The next action to perform the task."
REFERENCES,0.5972222222222222,"Your goal is to recover the HTML and the dynamic of a web application with the following requirements:
- The web page embodies a same level of content richness as advanced web applications on the internet. That
is, the web page should have around 80 elements and at least 20 interactable elements. The depth of the
DOM tree should be around 7. The length is at least 3000 tokens.
- Analyze the past actions and determine which of these actions have visible or functional impacts on the web
page you design. Reflect the effects of these past actions in your HTML code. This may involve updating
text, adding new elements, or modifying the layout or styles to represent the state of the web page after these
actions.
- Design the interactable element that enables the next action. Make sure the choice of element type,
attributes, and other essential characteristics are correct. For example, a text field is not interactable. Once
the element is designed, assign the attribute id=""next-action-target-element"" to this interactable element.
- Please focus on making the static HTML visually rich. Ignore CSS animations & style and JavaScript
functionality in your HTML code. - Provide the concrete reason to perform the next action."
REFERENCES,0.5995370370370371,"# Response format
“‘html
<HTML that fullfils the requirements, make sure ‘next-action-target-element‘ is always included>
“‘
<Summarize the progress by analyzing past actions. Provide a brief reason for performing the next action.
Keep it short. Use imperative sentences.>"
REFERENCES,0.6018518518518519,"# Provided information
task: {{task description}}"
REFERENCES,0.6041666666666666,"past actions:
“‘python
{{past actions}}
“‘"
REFERENCES,0.6064814814814815,"next action:
“‘python
{{next action}}
“‘"
REFERENCES,0.6087962962962963,"E
Example Synthetic Demonstration from Tutorials from a wikiHow Article"
REFERENCES,0.6111111111111112,A wikiHow article
REFERENCES,0.6134259259259259,How to Use Google Chat on iPhone (Enabling Google Chat in Gmail)
REFERENCES,0.6157407407407407,"Open Gmail on your iPhone or iPad. This app icon looks like a white and red envelope. If you don’t have it,
you can download the Gmail app for free from the App Store."
REFERENCES,0.6180555555555556,"Tap and tap Settings.
The three-line menu icon is in the top left corner of your screen, and the
option is generally at the bottom of the menu next to a gear icon. If you have multiple Gmail accounts, select
the one you want to use Google Chats with. You can repeat these steps if you want to use Google Chats with
multiple Gmail accounts."
REFERENCES,0.6203703703703703,"Tap to check the box next to ""Chat.
A checked box indicates that Google Chat is active in your
Gmail account and you’ll see the Chat and Spaces tabs near the bottom of your screen. If Google Chat is
enabled, you can tap these tabs to move between conversations in Chat and your emails in Gmail. If this is
turned off, you’ll need to use the Google Chats app to see your conversations in Chat."
REFERENCES,0.6226851851851852,The output from the rewriting step
REFERENCES,0.625,"# sub-task 1: Navigate to Settings to activate Google Chat in another account
# step 1: Click on the menu icon to reveal options
click(element=""Menu Icon"")"
REFERENCES,0.6273148148148148,"# step 2: Go into Settings by clicking on it
click(element=""Settings"")"
REFERENCES,0.6296296296296297,"# step 3: Select another Gmail account, this time for jane.doe@gmail.com
click(element=""jane.doe@gmail.com"")"
REFERENCES,0.6319444444444444,"# sub-task 2: Activate Google Chat in the chosen account
# step 4: Click to enable Google Chat by checking the box
click(element=""Check box next to Chat"")"
REFERENCES,0.6342592592592593,"# step 5: Ensure the Chat feature is activated
stop(answer=""Google Chat activated for jane.doe@gmail.com"")"
REFERENCES,0.6365740740740741,# task: Enable Google Chat on web version of Gmail for jane.doe@gmail.com
REFERENCES,0.6388888888888888,"Figure 5: The rendered generated HTML between step 2 and step 3, where step 2 is goto(“Google
Chat”) and step 3 is click(“jane.doe@gmail.com”). The concrete element to interact with is
tagged with id=""next-action-target-element""."
REFERENCES,0.6412037037037037,"F
Prompt to Generate Direct Demonstrations from Random Observations"
REFERENCES,0.6435185185185185,Prompt to Generate Direct Demonstrations from Random Observations
REFERENCES,0.6458333333333334,"## Task overview
Given the accessibility tree of a web page, your goal is to propose creative and diverse browser-based tasks
that involves interacting with this page, along with the previous actions that lead to the current state and the
next action needed to be taken to accomplish the task."
REFERENCES,0.6481481481481481,"## Action space
Here are the allowed actions that you can take to interact with the web page:
‘click(element: str, element_id: int=0)‘ - Click on an element. ‘element‘ is the displayed text or the most
representative attribute of the HTML element. ‘element_id‘ is the index of the element at the beginning of
the node.
‘click_and_type(element: str, content: str, element_id: int=0)‘ - Click and type ‘content‘ into an ‘element‘.
‘key_press(key_comb: str)‘ - Press a key combination. ‘key_comb‘ is the combination of keys you want to
press on. The default OS is MacOS if there is no explicit specification in the article.
‘goto(url: str)‘ - Navigate to ‘url‘
‘go_back()‘ - Go back to the previous page.
‘go_forward()‘ - Go forward to the next page.
‘new_tab()‘ - Open a new tab.
‘close_tab()‘ - Close the current tab.
‘switch_tab(tab_index: int)‘ - Switch to the tab with index ‘tab_index‘, starting from 0.
‘scroll(up|down)‘ - Scroll the page up or down.
‘stop(answer: str=”)‘ - The task is completed. If the task is to seek information, include the answer as a string.
Otherwise, leave it empty."
REFERENCES,0.6504629629629629,"## Guidelines
You will follow the guidelines below to perform the task:
1. Examine the web page to understand the the domain of the web page.
2. Brainstorm 8 task categories that could be performed the website. Be creative.
3. For each task category, propose a concrete task that has this web page as one of its steps. You want the
concrete task to be unambiguous and clear so that no further clarification is needed to perform the task.
4. Given a concrete task, you are ask to come up with the past actions that leads to the current page, as well
as the next action.
* Requirement for past actions: You should write down each past action in the details. You want to group all
actions into multiple sub-sections, each section corresponds to a logical and actionable sub-task. The next
action could start with a new sub-task. You can omit the ‘elemement_id‘ if they are not in the current page.
There should only be one action at each step. DO NOT give goto() or new_tab() as first step.
* Requirement for next action: Provide the reasoning behind your past actions and the progress in completing
the task. Also, describe your understanding of the current page and the concrete reason to execute the next
action. If the action takes an element as the argument, it is important that you understand the role and the
attributes of that element so that the action can be appropriately applied. Make sure to always include the
‘element_id‘ in your next action if there is any. Any ‘element_id‘ must come from the given Accessibility Tree."
REFERENCES,0.6527777777777778,"## Format of the response
You are asked to provide the action sequence for task #1 with roughly 7 past actions; task #2 with roughly 0
past actions; task #4 with roughly 6 past actions; task #5 with roughly 4 past actions; task #6 with roughly 10
past actions. Your answer should follow the following format:"
REFERENCES,0.6550925925925926,"<Analysis and understanding about the domain and the concrete content of the web page>
<The list of 8 creative task categories>
<The concrete tasks for task category #1 #2 #4 #5 #6. Remember, a concrete task needs to include concrete
details so that no further clarification is required when performing the task. Use imperative sentences.>"
REFERENCES,0.6574074074074074,"“‘python
# task: <repeat concrete task #1>"
REFERENCES,0.6597222222222222,"# ——————–
# past actions (history)
# sub-task 1: <sub-task description>
# step 1: <step description>
<action>
# step 2: <step description>
<action>
# sub-task 2: <sub-task description>
# step 3: <step description>
<action>
# step 4: <step description>
<action>
# sub-task 3: <sub-task description>
# step 5: <step description>
<action>
# step 6: <step description>
<action>
# sub-task 4: <sub-task description>
# step 7: <step description>
<action>"
REFERENCES,0.6620370370370371,# ——————–
REFERENCES,0.6643518518518519,"# next action
# step <index>: <summarize the progress so far and analyze the current state of the web page. Provide the
concrete reason to perform the next action>
<action>
# step summary: <brief step description>
“‘
“‘python
# task: <repeat concrete task #2>"
REFERENCES,0.6666666666666666,"# ——————–
# past actions (history)"
REFERENCES,0.6689814814814815,"# ——————–
# sub-task <index>: <sub-task description>
# next action
# step <index>: <summarize the progress so far and analyze the current state of the web page. Provide the
concrete reason to perform the next action>
<action>
# step summary: <brief step description>
“‘
......
## The Accessibility Tree
[4812] link ’Shopbop Designer Fashion Brands’
[4815] LineBreak
[4819] LineBreak
[1742] LayoutTableCell ”
[1743] LayoutTableCell ’Amazon Warehouse Great Deals on Quality Used Products’
[4822] link ’Amazon Warehouse Great Deals on Quality Used Products’
[4825] LineBreak
[4829] LineBreak"
REFERENCES,0.6712962962962963,"G
Example Generated Trajectories from Random Observation"
REFERENCES,0.6736111111111112,Generated trajectories from random observation
REFERENCES,0.6759259259259259,Model Brainstorming
REFERENCES,0.6782407407407407,"The domain appears to be that of a major online retail website, likely Amazon, considering the
references to ""Prime,"" ""Best Sellers,"" and ""Amazon Basics,"" as well as services like ""Magazine
Subscriptions"" and ""Prime Video."" The structure of the page includes a variety of product categories,
a search textbox, and links to other services such as account sign-in and shopping cart. This suggests
that users on this page can shop for a wide range of items, manage their account, or engage with
support services."
REFERENCES,0.6805555555555556,"Creative Task Categories: Below are eight creative task categories that could be performed on this
type of website:"
REFERENCES,0.6828703703703703,• Product Searching: Looking up products in a specific category or using a search query.
REFERENCES,0.6851851851851852,"• Account Management: Signing into an account, creating a new account, or managing
existing account details."
REFERENCES,0.6875,"• Navigational Inquiry: Using the site’s navigation to find different departments or special
features."
REFERENCES,0.6898148148148148,• Customer Support: Accessing customer service features or disability support services.
REFERENCES,0.6921296296296297,"• Deal Hunting: Searching for best sellers, deals of the day, or exclusive prime deals."
REFERENCES,0.6944444444444444,"• Media Consumption: Navigating to and consuming digital media such as eBooks, Prime
Video, or music."
REFERENCES,0.6967592592592593,"• Educational Browsing: Exploring educational materials or subscribing to learning re-
sources."
REFERENCES,0.6990740740740741,• Wishlist Management: Adding items to a wishlist or gift registry for future purchase.
REFERENCES,0.7013888888888888,"Concrete Tasks The concrete tasks for task categories #1, #2, #6, #7, #8 are as follows:"
REFERENCES,0.7037037037037037,"Concrete Task #1: Product Searching
Find and search for popular fitness trackers within the ""Sports & Outdoors"" category."
REFERENCES,0.7060185185185185,"#
t a s k :
Find
and
search
f o r
popular
f i t n e s s
t r a c k e r s
w ithin
the
"" Sports & Outdoors ""
c a t e g o r y ."
REFERENCES,0.7083333333333334,"# −−−−−−−−−−−−−−−−−−−−
#
p a s t
a c t i o n s
( h i s t o r y )
# sub −t a s k
1: Open the
s i t e
and
n a v i g a t e
to
the
product
c a t e g o r i e s .
#
s t e p
1:
The
user
opens
the
main page
of
the
r e t a i l
website .
go_forward ( )
# sub −t a s k
2:
Navigate
to
the
"" Sp orts & Outdoors ""
c a t e g o r y .
#
s t e p
2:
The
user
a c c e s s e s
the menu to
view
the
c a t e g o r y
l i s t .
c l i c k ( element= ' Open Menu ' )
#
s t e p
3:
The
user
browses
through
the
c a t e g o r i e s .
s c r o l l ( down )
#
s t e p
4:
The
user
s e l e c t s
the
"" Sp orts & Outdoors ""
c a t e g o r y .
c l i c k ( element= ' Sports & Outdoors ' ,
element_id =7964)"
REFERENCES,0.7106481481481481,"# −−−−−−−−−−−−−−−−−−−−
# next
a c t i o n
#
s t e p
5:
The
user now needs
to
sea rc h
f o r
f i t n e s s
t r a c k e r s
within
the
s e l e c t e d
c a t eg o r y .
c l i c k _ a n d _ t y p e ( element= ' Search ' ,
c o n t e n t = ' f i t n e s s
t r a c k e r s ' ,
element_id =7657)
#
s t e p
summary :
I n i t i a t e
a
sea rc h
f o r
f i t n e s s
t r a c k e r s
in
the
Sports & Outdoors
c at e g o r y ."
REFERENCES,0.7129629629629629,"Concrete Task #2: Account Management
Sign into your personal account to check on active subscriptions."
REFERENCES,0.7152777777777778,"#
t a s k :
Sign
i n t o
your
p e r s o n a l
account
to
check on
a c t i v e
s u b s c r i p t i o n s ."
REFERENCES,0.7175925925925926,"# −−−−−−−−−−−−−−−−−−−−
#
p a s t
a c t i o n s
( h i s t o r y )"
REFERENCES,0.7199074074074074,"# −−−−−−−−−−−−−−−−−−−−
# sub −t a s k
1:
Sign
i n t o
the
account .
# next
a c t i o n
#
s t e p
1:
The
user
i s
a l r e a d y
on a page where
they
can
i n i t i a t e
the
sign −in
process .
c l i c k _ a n d _ t y p e ( element= ' Search ' ,
c o n t e n t = ' myusername@example . com ' ,
element_id =7657)
#
s t e p
summary :
Try
to
sign
in
by
using
the
se arc h
bar
to
e n t e r
the
username ."
REFERENCES,0.7222222222222222,"Concrete Task #6: Media Consumption
Navigate to Prime Video to browse the latest movie releases."
REFERENCES,0.7245370370370371,"#
t a s k :
Navigate
to
Prime Video
to
browse
the
l a t e s t
movie
r e l e a s e s ."
REFERENCES,0.7268518518518519,"# −−−−−−−−−−−−−−−−−−−−
#
p a s t
a c t i o n s
( h i s t o r y )
# sub −t a s k
1:
Access
the
main page .
#
s t e p
1:
The
user
s t a r t s
by
r e a c h i n g
the home page ,
where
they
can
f i n d
v a r i o u s
s e r v i c e s .
go_back ( )
#
s t e p
2:
The
user
c o n s i d e r s
d i f f e r e n t
media
o p t i o n s
a v a i l a b l e
on
the
s i t e .
s c r o l l ( up )
# sub −t a s k
2: Go to
Prime Video .
#
s t e p
3:
The
user
c l i c k s
on
the
"" Prime Video ""
l i n k .
c l i c k ( element= ' Prime Video ' ,
element_id =7961)"
REFERENCES,0.7291666666666666,"# −−−−−−−−−−−−−−−−−−−−
# next
a c t i o n
#
s t e p
4: To search
f o r
the
l a t e s t
movie
r e l e a s e s
w ithin
Prime
Video ,
the
user
needs
to
use
the
s ea rch
f u n c t i o n .
c l i c k _ a n d _ t y p e ( element= ' Search ' ,
c o n t e n t = ' l a t e s t
movie
r e l e a s e s ' ,
element_id =7657)
#
s t e p
summary :
Search
f o r
the
l a t e s t
movie
r e l e a s e s
on Prime
Video ."
REFERENCES,0.7314814814814815,"Concrete Task #7: Educational Browsing
Subscribe to a magazine about technology within the ""Magazine Subscriptions"" category."
REFERENCES,0.7337962962962963,"#
t a s k :
Subscribe
to
a magazine
about
technology
w ithin
the
"" Magazine
S u b s c r i p t i o n s ""
c a t e g o r y ."
REFERENCES,0.7361111111111112,"# −−−−−−−−−−−−−−−−−−−−
#
p a s t
a c t i o n s
( h i s t o r y )
# sub −t a s k
1:
Enter
the
s i t e
and
l o c a t e
the
magazine
s e c t i o n .
#
s t e p
1:
The
user
s t a r t s
on
the home page where
they
can
browse
p ro d uc t s .
go_forward ( )
#
s t e p
2:
The
user
opens
the
c a t e g o r i e s
menu to
look
f o r
s u b s c r i p t i o n s .
c l i c k ( element= ' Open Menu ' )
#
s t e p
3:
The
user
s c r o l l s
through
the menu to
f i n d
the
magazine
s u b s c r i p t i o n s .
s c r o l l ( down )
# sub −t a s k
2:
S e l e c t
the
"" Magazine
S u b s c r i p t i o n s ""
c a t e g o r y .
#
s t e p
4:
The
user
s e l e c t s
the
"" Magazine
S u b s c r i p t i o n s ""
o ption
from
the
c a t e g o r i e s .
c l i c k ( element= ' Magazine
S u b s c r i p t i o n s ' ,
element_id =7955)
#
s t e p
5:
The
user
i s now p r e s e n t e d
with
d i f f e r e n t
types
of
magazines
but
wants
to
f i n d
technology −r e l a t e d
ones .
s c r o l l ( down )"
REFERENCES,0.7384259259259259,"# −−−−−−−−−−−−−−−−−−−−
# next
a c t i o n
#
s t e p
6:
With
the
magazine
s u b s c r i p t i o n s
displayed ,
i t
i s
time
to
search
f o r
technology
magazines
s p e c i f i c a l l y .
c l i c k _ a n d _ t y p e ( element= ' Search ' ,
c o n t e n t = ' technology
magazine ' ,
element_id =7657)
#
s t e p
summary :
Search
f o r
technology
magazines
in
the
Magazine
S u b s c r i p t i o n s
s e c t i o n ."
REFERENCES,0.7407407407407407,"Concrete Task #8: Wishlist Management
Add a popular sci-fi novel to your wishlist for future purchasing."
REFERENCES,0.7430555555555556,"#
t a s k : Add a
popular
sci −f i
novel
to
your
w i s h l i s t
f o r
f u t u r e
purchasing ."
REFERENCES,0.7453703703703703,"# −−−−−−−−−−−−−−−−−−−−
#
p a s t
a c t i o n s
( h i s t o r y )
# sub −t a s k
1:
Access
the
website ' s book
s e c t i o n .
#
s t e p
1:
The
user
has
s t a r t e d
on
the
homepage and
i s
looking
f o r
books .
c l i c k ( element= ' Books ' ,
element_id =6007)"
REFERENCES,0.7476851851851852,"# −−−−−−−−−−−−−−−−−−−−
# next
a c t i o n
#
s t e p
2:
The
user
wants
to
f i n d
and add a
sci −f i
novel
to
t h e i r
w i s h l i s t .
c l i c k _ a n d _ t y p e ( element= ' Search ' ,
c o n t e n t = ' popular
sci −f i
novels ' ,
element_id =7657)
#
s t e p
summary :
Begin
the
sea rc h
f o r
popular
sci −f i
novels
to
add
to
the
w i s h l i s t ."
REFERENCES,0.75,"H
Prompt Example"
REFERENCES,0.7523148148148148,Example prompt used in baseline
REFERENCES,0.7546296296296297,System
REFERENCES,0.7569444444444444,"You are an autonomous intelligent agent tasked with navigating a web browser.
You will be given web-based tasks.
These tasks will be accomplished through the use of specific actions you can issue.
Here’s the information you’ll have:
The user’s objective: This is the task you’re trying to complete.
The current web page’s accessibility tree: This is a simplified representation of the webpage,
providing key information.
The current web page’s website: This is the page you’re currently navigating.
The open tabs: These are the tabs you have open.
The previous action: These are the actions you have performed. It may be helpful to track your
progress.
The actions you can perform fall into several categories:
Page Operation Actions:
click [id]: This action clicks on an element with a specific id on the webpage.
type [id] [content] [press_enter_after=0|1]: Use this to type the content into the field
with id. By default, the ""Enter"" key is pressed after typing unless press_enter_after is set to 0.
hover [id]: Hover over an element with id.
To be successful, it is very important to follow the following rules:
1. You should only issue an action that is valid given the current observation.
2. You should only issue one action at a time.
3. You should follow the examples to reason step by step and then issue the next action.
4. Generate the action in the correct format. Start with a ""In summary, the next action I will perform
is"" phrase, followed by action inside “““. For example, ""In summary, the next action I will perform
is “‘click [1234]“‘"". User"
REFERENCES,0.7592592592592593,"OBSERVATION:
[1744] link ’HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)’
[1749] StaticText ’$279.49’
[1757] button ’Add to Cart’
[1760] button ’Add to Wish List’
[1761] button ’Add to Compare’
WEBSITE: onestopmarket
OBJECTIVE: Add HP Inkjet Fax Machine to cart.
PREVIOUS ACTION: None"
REFERENCES,0.7615740740740741,Assistant
REFERENCES,0.7638888888888888,"Let’s think step-by-step.
This page lists the information of HP Inkjet Fax Machine, which is the product identified in the
objective.
To add it to the cart, I will click the ""Add to Cart"" button.
In summary, the next action I will perform is click [1757]. User"
REFERENCES,0.7662037037037037,"OBSERVATION:
Tab 1 (current): Curry Brand Shoes & Gear | Under Armour
[4976] RootWebArea ’Curry Brand Shoes & Gear | Under Armour’ focused: True
[5059] link ’Skip to main content’
[5060] StaticText ’Skip to main content’
[5400] link ’FREE U.S. Shipping Orders $99+ & FREE Returns’
[5066] link ’Need Help?’
[5067] button ’US’ hasPopup: menu expanded: False controls: country-dropdown
[5422] button ’Welcome James’ hasPopup: menu
[5073] navigation ’Main Navigation’
[5660] menuitem ’Men’ hasPopup: menu expanded: False controls: men
[5668] menuitem ’Curry’ expanded: False
[5672] textbox ’Search by Keyword or Item No.’ hasPopup: listbox required: False controls:
search-results
[5099] button ’add to shopping bag’
[5030] main ”
WEBSITE: underarmour
OBJECTIVE: Search the cheapest Curry brand unisex athletic shoes with the number 5.5, add to
cart and checkout.
PREVIOUS ACTION: # step 1: Click menuitem Curry on the main navigation bar, # step 2: click
on link Shoes"
REFERENCES,0.7685185185185185,"I
Case Study"
REFERENCES,0.7708333333333334,"We conducted a detailed examination of instances where Synatra-CodeLlama successfully com-
pletes tasks that GPT-4-turbo fails to accomplish on WebArena. Two key patterns emerged from
these cases, which we outline here. Our analysis focuses on hard-level tasks requiring multiple steps
to complete."
REFERENCES,0.7731481481481481,"Synatra-CodeLlama
identifies
details
more
effectively
In
several
scenarios,
Synatra-CodeLlama outperforms GPT-4-turbo by detecting and utilizing detailed infor-
mation such as hidden links and buttons on the page, whereas GPT-4-turbo focuses only on the
most prominent components. In the scenario illustrated in Figure 6, both agents are tasked with
displaying issues labeled as “bug.” GPT-4-turbo immediately attempts to use the search box, but
lacks the knowledge of how to search effectively in this case and guesses the keyword, which does
not yield results. In contrast, Synatra-CodeLlama pays closer attention to the content displayed on
the page and accurately identifies a link that lists all issues labeled as “bug.”"
REFERENCES,0.7754629629629629,"# step 2: Since we are already in the 
Issues section, we need to filter the 
issues by the 'Bug' label to list only 
the relevant issues.
click(element_id=""2682"")
# step summary: Filter issues by the 
'Bug' label."
REFERENCES,0.7777777777777778,"Tab 0 (current): Issues · Dashboard · 
GitLab
Dashboard · GitLab' focused: True"
REFERENCES,0.7800925925925926,"[2374] link 'Skip to content'
[2496] link 'Dashboard'
[2548] button '' hasPopup: menu 
expanded: False"
REFERENCES,0.7824074074074074,"[3669] textbox 'Search GitLab' 
required: False"
REFERENCES,0.7847222222222222,"[3576] generic 'Use the shortcut 
key / to start a search'"
REFERENCES,0.7870370370370371,(a lot of links and texts here)
REFERENCES,0.7893518518518519,"'Administrator'
[2680] StaticText ' \xa0 '
[2682] link 'type: bug 🐞'
[3436] link 'Assigned to 
Byte Blaze'"
REFERENCES,0.7916666666666666,"Let's think step-by-step. This page 
has a textbox with ID [3669] to 
filter results. To get all the opened 
issues that report bugs, I can search 
for 'type: bug'. The textbox can 
filter the issues based on labels, and 
in GitLab, the label for bugs is 
'type: bug'. The pressing of the 
'Enter' key is necessary after typing 
to submit the search. In summary, 
the next action I will perform is 
```type [3669] [type: bug] [1]```"
REFERENCES,0.7939814814814815,"Synatra-CodeLlama
GPT-4-turbo"
REFERENCES,0.7962962962962963,"Figure 6: An example task where Synatra-CodeLlama is successful by paying more attention to
the displayed web page while GPT-4-turbo is not."
REFERENCES,0.7986111111111112,"Similarly, in the scenario shown in Figure 7, both agents are tasked with “finding the number of
commits Killian made on 3/5/2023.” At this stage, both agents have already navigated to the page
displaying Killian’s commits, and they can now provide the answer by simply counting. However,
GPT-4-turbo fails to register the displayed information: in its reasoning, it attempts to “view all
the commits,” even though they are already visible. In contrast, Synatra-CodeLlama pays closer
attention to the information on the page and accurately identifies the relevant link showing Killian’s
commits."
REFERENCES,0.8009259259259259,"# step 3: Identify the commit made on 
3/5/2023 by Kilian Valkhof and count 
the number of commits.
click(element_id=""5964"")
# step summary: Click on the commit 
message to see the details of the 
commit."
REFERENCES,0.8032407407407407,"Tab 0 (current): Commits · main · The 
A11Y Project / a11yproject.com · 
GitLab"
REFERENCES,0.8055555555555556,"[2892] textbox 'Search GitLab' 
required: False"
REFERENCES,0.8078703703703703,"[5720] generic 'Use the shortcut 
key / to start a search'"
REFERENCES,0.8101851851851852,(a lot of links and texts here)
REFERENCES,0.8125,"[4758] StaticText '1 
commit'"
REFERENCES,0.8148148148148148,"[5963] img ""Kilian 
Valkhof's avatar"""
REFERENCES,0.8171296296296297,"[5964] link 'Fix typos in 
the code examples (#1524)'"
REFERENCES,0.8194444444444444,"[5968] link 'Kilian 
Valkhof'"
REFERENCES,0.8217592592592593,"Let's think step-by-step. I need to 
look for the commits made by 
Kilian on 3/5/2023. First, I need to 
find a way to view all the commits. 
The search box has an id of [2892], 
which I can use to find the 
commits. I can type ""kilian 
commits on 3/5/2023"" into this box 
and press enter. In summary, the 
next action I will perform is ```type 
[2892] [kilian commits on 
3/5/2023] [1]```"
REFERENCES,0.8240740740740741,"Synatra-CodeLlama
GPT-4-turbo"
REFERENCES,0.8263888888888888,"Figure 7: An example task where Synatra-CodeLlama is successful paying more attention to the
displayed web page while GPT-4-turbo is not."
REFERENCES,0.8287037037037037,"Synatra-CodeLlama demonstrates clearer logical flow between steps
In the example shown
in Figure 8, both agents are filling out a form and have correctly entered the start and end dates.
However, GPT-4-turbo incorrectly predicts that the next step is to re-enter the starting date, while
Synatra-CodeLlama accurately interprets the status of the web page (“with the date range set to
include the entirety of Q1 2023”), plans accordingly (“we are ready to generate the refund report”),
and executes the correct action. This example illustrates that data generated by Synatra enhances"
REFERENCES,0.8310185185185185,"the model’s capacity to accurately interpret and respond to context, leading to more efficient and
reliable task execution in dynamic settings."
REFERENCES,0.8333333333333334,"# step 5: With the date range set to 
include the entirety of Q1 2023, we 
are ready to generate the refund 
report. The next action is to click the 
'Show Report' button to display the 
data.
click(element_id=""1504"")
# step summary: Generate the refund 
report for Q1 2023."
REFERENCES,0.8356481481481481,"[1504] button 'Show Report'
…….
[2278] StaticText 'Date Used'
[2279] combobox 'Date Used' 
hasPopup: menu expanded: False
[2283] combobox 'Period' hasPopup: 
menu expanded: False
[2324] StaticText 'From'
[1281] textbox 'From *' required: True
    [2346] StaticText '01/01/2023'
[2286] button 'undefined \ue627'
[2333] StaticText 'To'
[1284] textbox 'To *' focused: True 
required: True
    [2497] StaticText '03/31/2023'
[2289] button 'undefined \ue627'
[2292] StaticText 'Order Status'
[2293] combobox 'Order Status' 
hasPopup: menu expanded: False"
REFERENCES,0.8379629629629629,"Let's think step-by-step. The task is 
to generate a refund report for Q1, 
and the end date is 3/31/2023, 
which has been correctly entered in 
the ""To"" textbox. The ""From"" 
textbox with id [1281] should 
contain the date which marks the 
beginning of Q1, which is 
01/01/2023. I will input this date 
into the ""From"" textbox. After 
filling in both the ""From"" and ""To"" 
dates, I will need to click the 'Show 
Report' button with id [1504] to 
generate the report.
In summary, the next action I will 
perform is ```type [1281] 
[01/01/2023] [1]```"
REFERENCES,0.8402777777777778,"Synatra-CodeLlama
GPT-4-turbo"
REFERENCES,0.8425925925925926,Figure 8: An example task where Synatra-CodeLlama is successful while GPT-4-turbo is not.
REFERENCES,0.8449074074074074,"J
Data Selection from Random Observations in ClueWeb"
REFERENCES,0.8472222222222222,"We inspect a random sample of ClueWeb on its domain distribution. As shown in Figure 9, the
majority of domains appear only once, making up 69.1% of all the web pages. 1 2 3"
REFERENCES,0.8495370370370371,"4
5
6-12
12-25
25-50
50-100 >100"
REFERENCES,0.8518518518518519,"Categories
1 - 64.2%
2 - 7.9%
3 - 3.7%
4 - 2.2%
5 - 1.3%
6-12 - 3.6%
12-25 - 4.5%
25-50 - 2.9%
50-100 - 3.5%
>100 - 6.1%"
REFERENCES,0.8541666666666666,Figure 9: Frequencies of domains and proportion of each frequency in ClueWeb
REFERENCES,0.8564814814814815,"K
Training Settings"
REFERENCES,0.8587962962962963,"The CodeLlama checkpoints are fine-tuned with A100 GPUs with deepspeed 7 acceleration frame-
work. We set the context length to 4096 tokens. To train on 100k dataset, we train with 6 x A100"
REFERENCES,0.8611111111111112,7https://github.com/microsoft/DeepSpeed
REFERENCES,0.8634259259259259,"80G GPUs for about 40 hours. We use a batch size of 48, and learning rate of 4e-5. We use cosine
annealing and a warm-up ratio of 0.03."
REFERENCES,0.8657407407407407,"L
Broader Impacts"
REFERENCES,0.8680555555555556,"The broader impacts of this work extend across several domains. Firstly, the approach has the
potential to democratize the development of digital agents by making the training process more
affordable and accessible. Organizations with limited resources can utilize existing public data to
train competent agents without the need for expensive data collection efforts. This could lead to a
more widespread adoption and innovation in AI applications, particularly in regions or sectors that
previously could not afford such technology."
REFERENCES,0.8703703703703703,"Secondly, by enabling digital agents to perform more complex tasks effectively, this work can
significantly enhance productivity and efficiency in various industries. For example, customer service,
online troubleshooting, and data management tasks could be accelerated, allowing human workers to
focus on more creative or complex problem-solving tasks."
REFERENCES,0.8726851851851852,"Furthermore, the technology has implications for accessibility, as it could help develop more intuitive
and user-friendly interfaces for people with disabilities or those who are not tech-savvy. Agents
trained with a diverse range of demonstrations can offer more personalized and context-aware
assistance, improving user experience across digital platforms."
REFERENCES,0.875,"Lastly, the ethical and societal implications of this technology also constitute a critical area of impact.
While the technology can lead to significant efficiencies and capabilities, it also raises questions about
the potential for job displacement, and the need for robust guidelines to ensure that the deployment
of such agents aligns with ethical standards. These broader impacts underscore the importance of
interdisciplinary approaches to the development and governance of AI technologies, ensuring they
contribute positively to society."
REFERENCES,0.8773148148148148,"M
Limitations"
REFERENCES,0.8796296296296297,"One major limitation to the work is the potential variability in the quality and relevance of the
indirect knowledge sources. We attempted to mitigate this through use of high-quality sources such
as WikiHow and broad sources such as ClueWeb with intelligent sampling strategies, but still the
danger of unrepresentative data remains."
REFERENCES,0.8819444444444444,"Another concern is the generalizability of the synthesized demonstrations. While the approach allows
for the generation of a large volume of training data, the synthetic nature of these demonstrations may
not fully capture the complexity and nuances of real human interactions with digital environments.
As a result, agents trained on this data may still struggle with unexpected or less typical scenarios not
covered in the training data."
REFERENCES,0.8842592592592593,"Furthermore, there is the risk of overfitting to the specific formats and tasks represented in the indirect
knowledge sources. If the diversity of these sources is limited, the agents may not develop the
flexibility needed to handle a broad range of tasks across different platforms or environments."
REFERENCES,0.8865740740740741,"Lastly, the reliance on large language models and complex synthesis processes might introduce
significant computational costs and environmental impacts. The energy consumption and carbon
footprint associated with training such large models are concerns that need to be addressed to
ensure sustainable development in AI technologies. These limitations highlight the need for ongoing
research, improved data curation methods, and the development of more robust models that can better
generalize from synthetic training environments to real-world applications."
REFERENCES,0.8888888888888888,NeurIPS Paper Checklist
CLAIMS,0.8912037037037037,1. Claims
CLAIMS,0.8935185185185185,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We empirically show the claims.
Guidelines:"
CLAIMS,0.8958333333333334,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8981481481481481,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations in the analysis of empirical results, as well as
in Appendix M.
Guidelines:"
CLAIMS,0.9004629629629629,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9027777777777778,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]"
CLAIMS,0.9050925925925926,"Justification: No theoretical results.
Guidelines:"
CLAIMS,0.9074074074074074,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.9097222222222222,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide hyperparameters, experimental settings, and data generation
prompts for LLMs throughout the paper.
Guidelines:"
CLAIMS,0.9120370370370371,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.9143518518518519,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.9166666666666666,"Answer: [Yes]
Justification: All the code and data is submitted through Supplementary Material. We will
publicly release all the data and code on GitHub.
Guidelines:"
CLAIMS,0.9189814814814815,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.9212962962962963,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We detail experimental settings in our experiment section.
Guidelines:"
CLAIMS,0.9236111111111112,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
CLAIMS,0.9259259259259259,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: Due to costly nature of LLMs (expensive pricing for competitive API-based
models and GPU requirements for open-weight models), we do not perform multiple runs
for the same experiment setting.
Guidelines:"
CLAIMS,0.9282407407407407,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)"
CLAIMS,0.9305555555555556,"• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9328703703703703,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9351851851851852,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9375,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9398148148148148,Justification: We detail compute resource usage in Appendix K.
EXPERIMENTS COMPUTE RESOURCES,0.9421296296296297,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9444444444444444,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9467592592592593,9. Code Of Ethics
CODE OF ETHICS,0.9490740740740741,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9513888888888888,Answer: [Yes]
CODE OF ETHICS,0.9537037037037037,Justification: Our research conform with NeurIPS Code of Ethics
CODE OF ETHICS,0.9560185185185185,Guidelines:
CODE OF ETHICS,0.9583333333333334,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9606481481481481,10. Broader Impacts
BROADER IMPACTS,0.9629629629629629,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9652777777777778,Answer: [Yes]
BROADER IMPACTS,0.9675925925925926,Justification: In Appendix L.
BROADER IMPACTS,0.9699074074074074,Guidelines:
BROADER IMPACTS,0.9722222222222222,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9745370370370371,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9768518518518519,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We use existing models (CodeLlama, OpenAI GPT) and datasets (ClueWeb22,
WikiHow) that have been either safeguarded or processed with moderation when released.
We consider the issue not applicable to our work as we all work with existing safeguarded
datasets and models.
Guidelines:"
BROADER IMPACTS,0.9791666666666666,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9814814814814815,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We properly linked and cited the base model and source data corpus used in
our experiments. They all have permissive licenses.
Guidelines:"
BROADER IMPACTS,0.9837962962962963,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
BROADER IMPACTS,0.9861111111111112,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
BROADER IMPACTS,0.9884259259259259,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: There are generated synthetic data as part of our proposed data synthesis
method, and finetuned models using these data.
Guidelines:"
BROADER IMPACTS,0.9907407407407407,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
BROADER IMPACTS,0.9930555555555556,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No human subjects involved.
Guidelines:"
BROADER IMPACTS,0.9953703703703703,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No human subjects involved.
Guidelines:"
BROADER IMPACTS,0.9976851851851852,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
