Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025974025974025974,"With the growth of model and data sizes, a broad effort has been made to design
pruning techniques that reduce the resource demand of deep learning pipelines,
while retaining model performance. In order to reduce both inference and training
costs, a prominent line of work uses low-rank matrix factorizations to represent
the network weights. Although able to retain accuracy, we observe that low-rank
methods tend to compromise model robustness against adversarial perturbations.
By modeling robustness in terms of the condition number of the neural network,
we argue that this loss of robustness is due to the exploding singular values of
the low-rank weight matrices. Thus, we introduce a robust low-rank training
algorithm that maintains the network’s weights on the low-rank matrix manifold
while simultaneously enforcing approximate orthonormal constraints. The resulting
model reduces both training and inference costs while ensuring well-conditioning
and thus better adversarial robustness, without compromising model accuracy. This
is shown by extensive numerical evidence and by our main approximation theorem
that shows the computed robust low-rank network well-approximates the ideal full
model, provided a highly performing low-rank sub-network exists."
INTRODUCTION,0.005194805194805195,"1
Introduction"
INTRODUCTION,0.007792207792207792,"Deep learning and neural networks have achieved great success in a variety of applications in computer
vision, signal processing, and scientific computing, to name a few. However, their robustness with
respect to perturbations of the input data may considerably impact security and trustworthiness and
poses a major drawback to their real-world application. Moreover, the memory and computational
requirements for both training and inferring phases render them impractical in application settings
with limited resources. While a broad literature on pruning methods and adversarial robustness has
been developed to address these two issues in isolation, much less has been done to design neural
networks that are both energy-saving and robust. Actually, in many approaches the two problems
seem to compete against each other as most adversarial robustness improving-techniques require
even larger networks [36, 41, 46, 47, 79] or computationally more demanding loss functions, and
thus more expensive training phases [13, 23, 34, 44, 66]."
INTRODUCTION,0.01038961038961039,"The limited work available so far on robust pruned networks is mostly focused on reducing memory
and computational costs of the inference phase, while retaining adversarial robustness [20, 28, 42,
56, 72, 76]. However, the inference phase amounts to only a very limited fraction of the cost of the"
INTRODUCTION,0.012987012987012988,"whole deep learning pipeline, which is instead largely dominated by the training phase. Reducing
both inference and training costs is a challenging but desirable goal, especially in view of a more
accessible AI and its effective use on limited-resource and limited-connectivity devices such as drones
or satellites."
INTRODUCTION,0.015584415584415584,"Some of the most effective techniques for the reduction of training costs so far have been based
on low-rank weights parametrizations [29, 55, 71]. These methods exploit the intrinsic low-rank
structure of parameter matrices and large data matrices in general [17, 49, 59, 69]. Thus, assuming a
low-rank structure for the neural network’s weights W = USV ⊤, the resulting training procedures
only use the small individual factors U, S, V . This results in a training cost that scales linearly with
the number of neurons, as opposed to a quadratic scaling required by training full-rank weights.
Despite significantly reducing training parameters, these methods achieve accuracy comparable with
the original full networks. However, their robustness with respect to adversarial perturbations has
been largely unexplored so far."
INTRODUCTION,0.01818181818181818,Contributions
INTRODUCTION,0.02077922077922078,"In this paper, we observe that the adversarial robustness of low-rank networks may actually deteriorate
with respect to the full baseline. By modeling the robustness of the network in terms of the neural
network’s condition number, we argue that this loss of robustness is due to the exploding condition
number of the low-rank weight matrices, whose singular values grow very large in order to match
the baseline accuracy and to compensate for the lack of parameters. Thus, to mitigate this growing
instability, we design an algorithm that trains the network using only the low-rank factors U, S, V
while simultaneously ensuring the condition number of the network remains small. To this end, we
interpret the loss optimization problem as a continuous-time gradient flow and use techniques from
geometric integration theory on manifolds [12, 31, 55, 70] to derive three separate projected gradient
flows for U, S, V , individually, which ensure the condition number of the network remains bounded
to a desired tolerance 1 + τ, throughout the epochs. For a fixed small constant ε > 0, this is done by
bounding the singular values of the small rank matrices within a narrow band [s −ε, s + ε] around a
value s, chosen to best approximate the original singular values."
INTRODUCTION,0.023376623376623377,"We provide several experimental evaluations on different architectures and datasets, where the
robust low-rank networks are compared against a variety of baselines. The results show that the
proposed technique allows us to compute from scratch low-rank weights with bounded singular
values, significantly reducing the memory demand and computational cost of training while at the
same time retaining or improving both the accuracy and the robust accuracy of the original model.
On top of the experimental evidence, we provide a key approximation theorem that shows that if a
high-performing low-rank network with bounded singular values exists, then our algorithm computes
it up to a first-order approximation error."
INTRODUCTION,0.025974025974025976,"This paper focuses on feed-forward neural networks. However, our techniques and analysis apply
straightforwardly to convolutional filters reshaped in matrix form, as done in e.g. [29, 55, 71]. Other
ways exist to promote orthogonality of convolutional filters, e.g. [60, 65, 77], which we do not
consider in this work."
RELATED WORK,0.02857142857142857,"2
Related work"
RELATED WORK,0.03116883116883117,"Neural networks’ robustness against adversarial perturbations has been extensively studied in the
machine learning community. It is well-known that the adversarial robustness of a neural network
is closely related to its Lipschitz continuity [13, 23, 62, 68], see also Section 3. Accordingly,
training neural networks with bounded Lipschitz constant is a widely employed strategy to address
the problem. A variety of works studied Lipschitz architectures [37, 60, 65, 68], and a number
of certified robustness guarantees have been proposed [23, 51, 61]. While scaling each layer to
impose 1-Lipschitz constraints is a possibility, this approach may lead to vanishing gradients and
it is known that a more effective way to reduce the Lipschitz constant and increase robustness is
obtained by promoting orthogonality on each layer [5, 13]. On top of robustness, small Lipschitz
constants and orthogonal layers are known to lead to improved generalization bounds [11, 45] and
more interpretable gradients [67]. Orthogonality was also shown to improve signal propagation in
(very) deep networks [52, 74]."
RELATED WORK,0.033766233766233764,"A variety of methods to integrate orthogonal constraints in deep neural networks have been developed
over the years. Notable example approaches include methods based on regularization and landing
[1, 13], cheap parametrizations of the orthogonal group [6, 38, 39, 48, 50], Riemannian and projected
gradient descent schemes [2, 3, 10]."
RELATED WORK,0.03636363636363636,"In parallel to the development of methods to promote orthogonality, an active line of research
has grown to develop effective training strategies to enforce low-rank weights. Unlike sparsity-
promoting pruning strategies that primarily aim at reducing the parameters required for inference
[8, 21, 22, 30, 43], low-rank neural network models are designed to train directly on the low-
parametric manifold of low-rank matrices and are particularly effective to reduce the number of
parameters required by both inference and training phases. Similar to orthogonal training, methods
for low-rank training include methods based on regularization [26, 29], as well as methods based on
efficient parametrizations of the low-rank manifold using the SVD, randomized tensor dropout or the
polar decomposition [32, 71, 75], and Riemannian optimization-based training models [55, 57]."
RELATED WORK,0.03896103896103896,"By combining low-rank training with approximate orthogonal constraints, in this work we propose
a strategy that simultaneously enforces robustness while only requiring a reduced percentage of
the network’s parameters during training. The method is based on a gradient flow differential
formulation of the training problem, and the use of geometric integration theory to derive the
governing equations of the low-rank factors. With this formulation, we are able to reduce the
sensitivity of the network during training at almost no cost, yielding well-conditioned low-rank neural
networks. Our experimental findings are supported by an approximation theorem that shows that, if
the ideal full network can be approximated by a low-rank one, then our method computes a good
approximation. This is well-aligned with recent work that shows the existence of high-performing
low-rank nets in e.g. deep linear models [7, 17, 25, 49]. Moreover, as orthogonality helps in training
really deep networks, low-rank orthogonal models may be used to mitigate the effect of increased
effective depth when training low-rank networks [54]."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.04155844155844156,"3
The condition number of a neural network"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.04415584415584416,"The adversarial robustness of a neural network model f can be measured by the worst-case sensitivity
of f with respect to small perturbations of the input data x. In an absolute sense, this boils down
to measuring the best global and local Lipschitz constant of f with respect to suitable distances, as
discussed in a variety of papers [13, 15, 23, 62]. However, as the model and the data may assume
arbitrary large and arbitrary small values in general, a relative measure of the sensitivity of f may
be more informative. In other words, if we assume a perturbation δ of small size as compared to
x, we would like to quantify the largest relative change in f(x + δ), as compared to f(x). This
is a well-known problem of conditioning, as we review next, and naturally leads to the concept of
condition number of a neural network."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.046753246753246755,"In the linear setting, the condition number of a matrix is a widely adopted relative measure of
the worst-case sensitivity of linear problems with respect to noise in the data. For a matrix A
and the matrix operator norm ∥A∥= supx̸=0 ∥Ax∥/∥x∥the condition number of A is defined as
cond(A) = ∥A∥∥A+∥, where A+ denotes the pseudo-inverse of A. Note that it is immediate to
verify that cond(A) ≥1. Now, if for example u and uε are the solutions to the linear system
Au = b, when A and b are exact data or when they are perturbed with noise δA, δb of relative norm
∥δA∥/∥A∥≤ε and ∥δb∥/∥b∥≤ε, respectively, then the following relative error bound holds"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.04935064935064935,∥u −uε∥
THE CONDITION NUMBER OF A NEURAL NETWORK,0.05194805194805195,"∥u∥
≲cond(A) ε ."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.05454545454545454,"Thus, small perturbations in the data A, b imply small alterations in the solution if and only if A is
well conditioned, i.e. cond(A) is close to one."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.05714285714285714,"As in the linear case, it is possible to define the concept of condition number for general functions f,
[24, 53]. Let us start by defining the relative error ratio of a function f : Rd →Rm in the point x:"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.05974025974025974,"R(f, x; δ) = ∥f(x + δ) −f(x)∥"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.06233766233766234,∥f(x)∥ . ∥δ∥
THE CONDITION NUMBER OF A NEURAL NETWORK,0.06493506493506493,"∥x∥.
(1)"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.06753246753246753,"In order to take into account the worst-case scenario, the local condition number of f at x is defined
by taking the sup of (1) over all perturbations of relative size ε, i.e. such that ∥δ∥≤ε ∥x∥, in the"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.07012987012987013,"limit of small ε. Namely, cond(f; x) = limε↓0 supδ̸=0:∥δ∥≤ε∥x∥R(f, x; δ). This quantity is a local
measure of the “infinitesimal” conditioning of f around the point x. In fact, a direct computation
reveals that
∥f(x + δ) −f(x)∥"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.07272727272727272,"∥f(x)∥
≲cond(f; x) ε ,
(2)"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.07532467532467532,"as long as ∥δ∥≤ε ∥x∥. Thus, cond(f; x) provides a form of relative local Lipschitz constant for f
which in particular shows that, if ∥δ∥/∥x∥is smaller than cond(f; x)−1, we expect limited change
in f when x is perturbed with δ. A similar conclusion is obtained using an absolute local Lipschitz
constant in e.g. [23]. Similarly to the absolute case, a global relative Lipschitz constant can be
obtained by looking at the worst-case over x, setting cond(f) = supx∈X cond(f; x). Clearly, the
same bound (2) holds for cond(f). Note that this effectively generalizes the linear case, as when
f(x) = Ax we have cond(f) = cond(f, x) = cond(A)."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.07792207792207792,"When f is a neural network, cond(f) is a function of the network’s weights and robustness may
be enforced by reducing cond(f) while training. In fact, cond(f) is the relative equivalent of the
network’s Lipschitz constant and thus standard Lipschitz-based robustness certificates [23, 40, 68]
can be recast in terms of cond(f). However, for general functions f and general norms ∥· ∥, cond(f)
may be (very) expensive to compute, it may be non-differentiable, and cond(f) > 1 can hold
[24]. Fortunately, for feed-forward neural networks, it holds (proof and additional details moved to
Appendix B in the supplementary material)
Proposition 1. Let X be the input space and let f(x) = zL+1 be a network with L linear layers
zi+1 = σi(Wizi), i = 1, . . . , L. Then,"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.08051948051948052,"cond(f) =
sup
x∈X\{0}
cond(f; x) ≤
 L
Y"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.08311688311688312,"i=1
sup
x∈Xi\{0}
cond(σi; x)
 L
Y"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.08571428571428572,"i=1
cond(Wi)

."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.08831168831168831,"In particular, for typical Xi and typical choices of σi, including σi ∈{leakyReLU, sigmoid, tanh,
hardtanh, softplus, siLU}, we have
supx∈Xi\{0} cond(σi; x) ≤C < +∞
for a positive constant C > 0 that depends only on the activation function σi."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.09090909090909091,"Note that for entrywise nonlinearities σ, the condition number cond(σ; x) can be computed straight-
forwardly. In fact, when σ is Lipschitz, the problem can be reduced to a one-dimensional function,
and it follows directly from its definition that (see also [64])
cond(f; x) =
sup
νx∈∂σ(x)
|νx||x||σ(x)|−1,
x ∈R"
THE CONDITION NUMBER OF A NEURAL NETWORK,0.09350649350649351,"where ∂σ(x) denotes Clarke’s generalized gradient [14] of σ at the point x. Thus, for example,
if σ is LeakyRelu with slope α, we have cond(σ) = 1; if σ is the logistic sigmoid (1 + e−x)−1
and the feature space Xi is such that Xi = WiXi−1, then if |zi−1| ≤ci−1 entry-wise, we have
|xi| ≤ci−1 maxuv |Wi|uv := ci and cond(σ) ≤supx≥−ci |x|e−x(1 + e−x)−1 ≤max{ci, 1/e}."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.09610389610389611,"From Proposition 1 we see that when f is a feed-forward network, to reduce the condition number of f
it is enough to reduce the conditioning of all its weights. When ∥·∥= ∥·∥2 is the Euclidean L2 norm,
we have cond2(W) = smax(W)/smin(W), the ratio between the largest and the smallest singular
value of W. This implies that orthogonal weight matrices, for example, are optimally conditioned
with respect to the L2 metric. Thus, a notable and well-known consequence of Proposition 1 is that
imposing orthogonality constraints on W improves the robustness of the network [13, 27, 38, 50]."
THE CONDITION NUMBER OF A NEURAL NETWORK,0.0987012987012987,"While orthogonal constraints are widely studied in the literature, orthogonal matrices are not the
only optimally conditioned ones. In fact, cond2(W) = 1 for any W with constant singular values.
In the next section, we will use this observation to design a low-rank and low-cost algorithm that
trains well-conditioned networks by ensuring cond2(W) ≤1 + τ, for all layers W and a desired
tolerance τ > 0."
ROBUST LOW-RANK TRAINING,0.1012987012987013,"4
Robust low-rank training"
INSTABILITY OF LOW-RANK NETWORKS,0.1038961038961039,"4.1
Instability of low-rank networks"
INSTABILITY OF LOW-RANK NETWORKS,0.10649350649350649,"Low-rank methods are popular strategies to reduce the memory storage and the computational cost
of both training and inference phases of deep learning models [29, 55, 71]. Leveraging the intrinsic"
INSTABILITY OF LOW-RANK NETWORKS,0.10909090909090909,"0
2000 4000 6000 8000
# optimization step 101"
INSTABILITY OF LOW-RANK NETWORKS,0.11168831168831168,layers condition numbers
INSTABILITY OF LOW-RANK NETWORKS,0.11428571428571428,Baseline
INSTABILITY OF LOW-RANK NETWORKS,0.11688311688311688,"0
2000 4000 6000 8000
# optimization step 100 101 102 103"
INSTABILITY OF LOW-RANK NETWORKS,0.11948051948051948,layers condition numbers
INSTABILITY OF LOW-RANK NETWORKS,0.12207792207792208,Low-rank gradient flow
INSTABILITY OF LOW-RANK NETWORKS,0.12467532467532468,"0
2000 4000 6000 8000"
INSTABILITY OF LOW-RANK NETWORKS,0.12727272727272726,# optimization step
INSTABILITY OF LOW-RANK NETWORKS,0.12987012987012986,"100
101
102
103
104
105
106
107"
INSTABILITY OF LOW-RANK NETWORKS,0.13246753246753246,layers condition numbers DLRT
INSTABILITY OF LOW-RANK NETWORKS,0.13506493506493505,"0
200
400
600
# optimization step"
INSTABILITY OF LOW-RANK NETWORKS,0.13766233766233765,"101
102
103
104
105
106
107"
INSTABILITY OF LOW-RANK NETWORKS,0.14025974025974025,layers condition numbers
INSTABILITY OF LOW-RANK NETWORKS,0.14285714285714285,Layer decomposition
INSTABILITY OF LOW-RANK NETWORKS,0.14545454545454545,"Figure 1: Evolution of layers’ condition numbers during training for LeNet5 on MNIST. From left
to right: standard full-rank baseline model; [71] vanilla low-rank training; [55] dynamical low-rank
training based on gradient flow; [75] low-rank training through regularization. All low-rank training
strategies are set to 80% compression ratio (percentage of removed parameters with respect to the
full baseline model)."
INSTABILITY OF LOW-RANK NETWORKS,0.14805194805194805,"low-rank structure of parameter matrices [7, 17, 49, 59], these methods train a subnetwork with
weight matrices parametrized as W = USV ⊤, for “tall and skinny” matrices U, V with r columns,
and a small r × r matrix S. Training low-rank weight matrices has proven to effectively reduce
training parameters while retaining performance comparable to those of the full model. However,
while a variety of contributions have analyzed and refined low-rank methods to match the full model’s
accuracy, the robust accuracy of low-rank models has been partially overlooked in the literature."
INSTABILITY OF LOW-RANK NETWORKS,0.15064935064935064,"Here we observe that reducing the rank of the layer may actually deteriorate the network’s robustness.
We argue that this phenomenon is imputable to the exploding condition number of the network.
In Figure 1 we plot the evolution of the condition number cond2 for the four internal layers of
LeNet5 during training using different low-rank training strategies and compare them with the full
model. While the condition number of the full model grows moderately with the iteration count, the
condition number of low-rank layers blows up drastically. This singular value instability leads to
poor robustness performance of the methods, as observed in the experimental evaluation of Section 5."
INSTABILITY OF LOW-RANK NETWORKS,0.15324675324675324,"In the following, we design a low-rank training model that allows imposing simple yet effective
training constraints, bounding the condition number of the trained network to a desired tolerance
1 + τ, and improving the network robustness without affecting training nor inference costs."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.15584415584415584,"4.2
Low-rank gradient flow with bounded singular values"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.15844155844155844,"Let W ∈Rn×m be the weight matrix of a linear layer within f. For an integer r ≤min{m, n} let
Mr = {W : rank(W) = r} be the manifold of rank-r matrices which we parametrize as"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.16103896103896104,"Mr =

USV ⊤: U ∈Rn×r, V ∈Rm×r with orthonormal columns, S ∈Rr×r invertible
	
."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.16363636363636364,"Obviously, the singular values of W = USV ⊤∈Mr coincide the singular values of S. For s, ε such
that 0 < ε < s, define Σs(ε) as the set of matrices with singular values in the interval [s −ε, s + ε].
Note that Σs(0) is a Riemannian manifold obtained essentially by an s scaling of the standard Stiefel
manifold (the manifold of matrices with orthonormal columns) and any A ∈Σs(0) is optimally
conditioned, i.e. cond2(A) = 1. Thus, ε can be interpreted as an approximation parameter that
controls how close Σs(ε) is to the “optimal” manifold Σs(0). To enhance the network robustness, in
the following we will constrain the parameter weight matrix S to Σs(ε). With this constraint, we
get cond2(W) ≤(s −ε)−1(s + ε) = 1 + τ, with τ = 2(s −ε)−1ε, so that the tolerance τ on the
network’s conditioning can be tuned by suitably choosing the approximation parameter ε."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.16623376623376623,"Given the loss function L, we are interested in the constrained optimization problem"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.16883116883116883,"min L
s.t.
W = USV ⊤∈Mr and S ∈Σs(ε), for all layers W .
(3)"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.17142857142857143,"To approach (3), we use standard arguments from geometric integration theory [31, 70] to design a
training scheme that updates only the factors U, S, V and the gradient of L with respect to U, S, V ,
without ever forming the full weights nor the full gradients. To this end, following [55], we first
recast the optimization of L with respect to each layer W as a continuous-time gradient flow"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.17402597402597403,"˙W(t) = −∇W L(W(t)),
(4)"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.17662337662337663,"where “dot” denotes the time derivative and where we write L as a function of W only, for brevity.
Along the solution of the differential equation above, the loss decreases and a stationary point is"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.17922077922077922,"Algorithm 1: Pseudocode of robust well-Conditioned Low-Rank (CondLR ) training scheme
Input: Chosen compression rate, i.e. for each layer W choose a rank r;
Initial layers’ weights parametrized as W = USV ⊤, with S ∼r × r;
Second singular value moment of S, s =
pP"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.18181818181818182,"k sk(S)2/r
Conditioning tolerance τ > 0"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.18441558441558442,1 for each iteration and each layer do (each block in parallel)
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.18701298701298702,"2
U ←one optimization step with gradient G1 and initial point U"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.18961038961038962,"3
U ←project U on Stiefel manifold with r orthonormal columns"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.19220779220779222,"4
V ←one optimization step with gradient G2 and initial point V"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.19480519480519481,"5
V ←project V on Stiefel manifold with r orthonormal columns"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.1974025974025974,"6
S ←one optimization step with gradient G3 and initial point S"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.2,"7
s ←
pP"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.2025974025974026,"k sk(S)2/r, squareroot of second moment of the singular values of S"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.2051948051948052,"8
ε ←τs/(2 + τ)"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.2077922077922078,"9
S ←project S onto Σs(ε)"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.21038961038961038,"approached as t →∞. Now, if we assume W ∈Mr, then ˙W ∈TW Mr, the tangent space of Mr
at the point W. Thus, to ensure the whole trajectory W(t) ∈Mr, we can consider the projected
gradient flow ˙W(t) = −PW (t)∇W L(W(t)), where PW denotes the orthogonal projection (in the
ambient space of matrices) onto TW Mr. Next, we notice that the projection PW ∇W L can be defined
by imposing orthogonality with respect to any point Y ∈TW Mr, namely"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.21298701298701297,"⟨PW ∇W L −∇W L, Y ⟩= 0
for all Y ∈TW Mr"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.21558441558441557,"where ⟨·, ·⟩is the Frobenius inner product. As discussed in e.g. [31, 55], the above equations combined
with the well-known representation of TW Mr yield a system of three gradient flow equations for the
individual factors

 "
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.21818181818181817,"˙U = −G1(U),
G1(U) = P ⊥
U ∇UL(USV ⊤)(SS⊤)−1
˙V = −G2(V ),
G2(V ) = P ⊥
V ∇V L(USV ⊤)(S⊤S)−⊤
˙S = −G3(S),
G3(S) = ∇SL(USV ⊤)
(5)"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.22077922077922077,"where P ⊥
U = (I −UU ⊤) and P ⊥
V
= (I −V V ⊤) are the projection operators onto the space
orthogonal to the span of U and V , respectively."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.22337662337662337,"Based on the system of gradient flows above, we propose a training scheme that at each iteration and
for each layer parametrized by the tuple {U, S, V } proceeds as follows:"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.22597402597402597,"1. update U and V by numerically integrating the gradient flows ˙U = −G1(U) and ˙V = −G2(V )
2. project the resulting U, V onto the Stiefel manifold of matrices with r orthonormal columns
3. update the r × r weight S by integrating ˙S = −G3(S)
4. for a fixed robustness tolerance τ, project the computed S onto Σs(ε), choosing s and ε so that"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.22857142857142856,"• s is the best constant approximation to S⊤S, i.e. s = argminα ∥S⊤S −α2I∥F
• ε is such that the cond2 of the projection of S does not exceed 1 + τ"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.23116883116883116,"Note that the coefficients s, ε at point 4 can be obtained explicitly by setting s =
qPr
j=1 sj(S)2/r,"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.23376623376623376,"the second moment of the singular values sj(S) of S, and ε = τs/(2 + τ). Note also that, in the
differential equations for U, S, V in (5), the four steps above can be implemented in parallel for each
of the three variables. The detailed pseudocode of the training scheme is presented in Algorithm 1.
We conclude with several remarks about its implementation."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.23636363636363636,"Remarks, implementation details, and limitations"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.23896103896103896,"Each step of Algorithm 1 requires three optimization steps at lines 2, 4, 6. These steps can be
implemented using standard first-order optimizers such as SGD with momentum or ADAM. Standard
techniques can be used to project onto the Stiefel manifold at lines 3 and 5 of Algorithm 1, see
e.g. [4, 70]. Here, we use the QR decomposition. As for the projection onto Σs(ε) at line 9, we"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.24155844155844156,"compute the SVD of the small factor S and set to s + ε or s −ε the singular values that fall outside
the interval [s −ε, s + ε]. Note that, when τ = 0, i.e. when we require perfect conditioning for the
layer weight W = USV ⊤, then the SVD of S can be replaced by a QR step or any other Stiefel
manifold projection. Indeed, we can equivalently set s =
p"
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.24415584415584415,"trace(S⊤S)/r, and then project onto
Σs(0) by rescaling by a factor s the projection of S onto the Stiefel manifold. In this case, the system
(5) further simplifies, as we can replace (SS⊤)−1 and (S⊤S)−⊤with the scalar 1/s2."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.24675324675324675,"Overall, the compressed low-rank network has r(n + m + r) parameters per each layer, where n and
m are the number of input and output neurons. Thus, choosing r so that 1−r(n+m+r)/(nm) = α
can yield a desired compression rate 0 < α < 1 on the number of network parameters, i.e. the number
of parameters one eliminates with respect to the full baseline. For example, in our experiments we
will choose r so that α = 0.5 or α = 0.8."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.24935064935064935,"Computational complexity. Each pass of Alg.1 is done against a batch xbatch. In order to obtain
minimal computational costs for each step in the algorithm, we evaluate USV ⊤xbatch sequentially:
first v = V ⊤xbatch, then u = Sv, and finally Uu. Assuming the size of the batch is negligible
with respect to n and m, the cost of these steps is O(rm), O(r), O(rn), respectively. Adding the
bias term and evaluating the activation function requires O(n) operations. Hence, overall we have
a cost per layer of O(r(n + m + 1)). Taping the forward evaluation to compute the gradient with
respect to U, S, V does not affect the asymptotic costs. The QR decompositions used for U and V
require O(r2n) and O(r2m) operations respectively, O(r2(n + m)) overall. Finally, computing the
SVD in the projection step for S requires a worst-case cost of O(r3). Hence the overall cost per
layer is O(r(1 + r)(n + m) + r3) as opposed to the dense network training, which requires O(nm)
operations per layer. If r ≪n, m then the low-rank method is cheaper than the full baseline. For
example, if n = m, this happens provided r < √n."
LOW-RANK GRADIENT FLOW WITH BOUNDED SINGULAR VALUES,0.2519480519480519,"Limitations. As the rank parameter r has to be chosen a-priori for each layer of the network,
a limitation of the proposed approach is the potential need for fine-tuning such parameter, even
though the proposed analysis in Table 1 shows competitive performance for both 50% and 80%
compression rates. Also, if the layer size n × m is not large enough, the compression ratio cr =
1 −r(n + m + r)/(nm) might be limited. Thus the method works well only for wide-enough
networks (n, m ≫r, so that cr > 0). Finally, a standard way to obtain better adversarial performance
would be to combine the proposed conditioning-based robustness with adversarial training strategies
[16, 63, 73]. However, the cost of producing adversarial examples during training is not negligible,
especially when based on multi-step attacks, and thus the way to incorporate adversarial training
without affecting the benefits obtained with low-rank compression is not straightforward."
APPROXIMATION GUARANTEES,0.2545454545454545,"4.3
Approximation guarantees"
APPROXIMATION GUARANTEES,0.2571428571428571,"Optimization methods over the manifold of low-rank matrices are well-known to be affected by the
stiff intrinsic geometry of the constraint manifold which has very high curvature around points where
W ∈Mr is almost singular [4, 31, 70]. This implies that even very small changes in W may yield
very different tangent spaces, and thus different training trajectories, as shown by the result below:"
APPROXIMATION GUARANTEES,0.2597402597402597,"Lemma 1 (Curvature bound, Lemma 4.2 [31]). For W ∈Mr let smin(W) > 0 be its smallest
singular value. For any W ′ ∈Mr arbitrarily close to W and any matrix B, it holds"
APPROXIMATION GUARANTEES,0.2623376623376623,"∥PW B −PW ′B∥F ≤C smin(W)−1∥W −W ′∥F ,"
APPROXIMATION GUARANTEES,0.2649350649350649,where C > 0 depends only on B.
APPROXIMATION GUARANTEES,0.2675324675324675,"In our gradient flow terminology, this phenomenon is shown by the presence of the matrix inversion in
(5). While this is often an issue that may dramatically affect the performance of low-rank optimizers
(see also Section 5), the proposed regularization step that enforces bounded singular values allows
us to move along paths that avoid stiffness points. Using this observation, here we provide a bound
on the quality of the low-rank neural network computed via Algorithm 1, provided there exists an
optimal trajectory leading to an approximately low-rank network. We emphasize that this assumption
is well-aligned with recent work showing the existence of high-performing low-rank nets in e.g. deep
linear models [7, 17, 25, 49]."
APPROXIMATION GUARANTEES,0.2701298701298701,"Assume the training is performed via gradient descent with learning rate λ > 0, and let W(t) be the
full gradient flow (4). Further, assume that for t ∈[0, λ] and a given ε > 0, for each layer there exists"
APPROXIMATION GUARANTEES,0.2727272727272727,"E(t) and f
W(t) ∈Mr ∩Σs(ε) such that"
APPROXIMATION GUARANTEES,0.2753246753246753,"W(t) = f
W(t) + E(t) ,"
APPROXIMATION GUARANTEES,0.2779220779220779,"where s is the second moment of the singular values of W(t) and E(t) is a perturbation that has
bounded variation in time, namely ∥˙E(t)∥≤η. In other words, we assume there exists a training
trajectory that leads to an approximately low-rank weight matrix W(t) with almost constant singular
values. Because the value s is bounded by construction, the parameter-dependent matrix f
W(t)
possesses singular values exhibiting moderate lower bound. Thus, W(t) is far from the stiffness
region of (5) and we obtain the following bound, based on [31] (proof moved to Appendix C in the
supplementary material)
Theorem 1. Let UkSkV ⊤
k be a solution to (5) computed with k steps of Algorithm 1. Assume that
• The low-rank initialization U0S0V ⊤
0 coincides with the low-rank approximation f
W(0).
• The norm of the full gradient is bounded, i.e., ∥∇W L(W(t))∥≤µ.
• The learning rate is bounded as λ ≤
s−ε
4√2µη.
Then, assuming no numerical errors, the following error bound holds"
APPROXIMATION GUARANTEES,0.2805194805194805,"∥UkSkV ⊤
k −W(λk)∥≤3λη ."
APPROXIMATION GUARANTEES,0.2831168831168831,"Note that if f is smooth enough (e.g. Lipschitz) then by the previous theorem we directly obtain an
equivalent bound for the functional distance ∥f(UkSkV ⊤
k ) −f(W(λk))∥."
EXPERIMENTS,0.2857142857142857,"5
Experiments"
EXPERIMENTS,0.2883116883116883,"We illustrate the performance of Algorithm 1 on a variety of test cases. All the experiments can be
reproduced with the code in PyTorch available at https://github.com/COMPiLELab/CondLR. In
order to assess the combined compression and robustness performance of the proposed method, we
compare it against both full and low-rank baselines."
EXPERIMENTS,0.2909090909090909,"For all models, we compute natural accuracy and robust accuracy. Let {(xi, yi)}i=1,...,n be the set of
test images and the corresponding labels and let f be the neural network model, with output f(x) on
the input x. We quantify the test set robust accuracy as:"
EXPERIMENTS,0.2935064935064935,robust_acc(δ) = 1
EXPERIMENTS,0.2961038961038961,"n
Pn
i=1 1{yi}(f(xi + δi))"
EXPERIMENTS,0.2987012987012987,"where δ = (δi)i=1,...,n are the adversarial perturbation associated to each sample. Notice that, in
the unperturbed case with ∥δi∥= 0, the definition of robust accuracy exactly coincides with the
definition of test accuracy. In our experiments, adversarial perturbations are produced by both the
fast gradient sign method (FGSM) [19] and the projected gradient descent attack (PGD) [47], with
∥δi∥∞= ϵ, and ϵ controls perturbation strength. As images in our set-up have input entries in [0, 1],
the perturbed input is then clamped to that interval. Note that, for the same reason, the value of ϵ
controls in our case the relative size of the perturbation."
EXPERIMENTS,0.3012987012987013,"Datasets. We consider MNIST , CIFAR10, and CIFAR100 [33] datasets for evaluation purposes.
The first contains 60,000 training images, the second one contains 50,000 training images, while the
third one contains 50,000 training images. All the datasets have 10,000 test images, first two have 10
classes and last one has 100 classes. No data-augmentation is performed."
EXPERIMENTS,0.3038961038961039,"Models. We use LeNet5 [35] for MNIST dataset, VGG16 [58] and WideResnet (WRN16-4) [78] for
CIFAR10 and CIFAR100. The general architecture for all the used networks is preserved across the
models, while the weight-storing structures and optimization frameworks differ."
EXPERIMENTS,0.3064935064935065,"Methods. Our baseline network is the one done with the standard implementation. Cayley SGD
[39] and Projected SGD [3] are Riemannian optimization-based methods that train the network
weights over the Stiefel manifold of matrices with orthonormal columns. Thus, both methods ensure
cond2(W) = 1 for all layers. The former uses an iterative estimation of the Cayley transform, while
the latter uses QR-based projection to retract the Riemannian gradient onto the Stiefel manifold.
Both methods have no compression and use full-weight matrices. DLRT, SVD prune, and Vanilla are
low-rank methods that ensure compression of the model parameters during training. DLRT [55] is
based on a low-rank gradient flow model similar to the proposed Algorithm 1. SVD prune [75] is
based on a regularized loss with a low-rank-promoting penalty term. This approach was designed to"
EXPERIMENTS,0.3090909090909091,"compress the ranks of the network after training, but we imposed a fixed compression rate from the
beginning for a fair comparison. “Vanilla” denotes the obvious low-rank approach, that parametrizes
the layers as W = UV ⊤and performs alternate descent steps over U and V , as done in e.g. [29, 71].
All models are implemented with fixed training compression ratios α = 0.5 and α = 0.8, i.e. we
only use 50% and 20% of the parameters the full model would use during training, respectively.
Finally, we implement CondLR as in Algorithm 1 for three choices of the conditioning tolerance
τ ∈{0, 0.1, 0.5}. We also implement a modified version of Algorithm 1 in which S is directly
projected onto the Stiefel manifold Σ1(0), i.e. the parameter s is fixed to one."
EXPERIMENTS,0.3116883116883117,"Training. Each method and model was trained for 120 epochs of stochastic gradient descent with a
minibatch size of 128. We used a learning rate of 0.1 for LeNet5 and 0.05 for VGG16 with momentum
0.3 and 0.45, respectively, and a learning rate scheduler with factor = 0.3 at 70 and 100 epochs."
EXPERIMENTS,0.3142857142857143,"Results. For comparison, we measure robust accuracy for all chosen combinations of datasets,
models, and methods using FGSM and PGD attacks with relative perturbation budget ϵ ∈
{0.01, 0.02, 0.030.04, 0.05, 0.06} for MNIST, ϵ ∈{0.001, 0.002, 0.003, 0.004, 0.005, 0.006} for
CIFAR10 VGG16, ϵ ∈{0.0003, 0.0006, 0.001, 0.0013, 0.0016} for CIFAR10 and CIFAR100 with
WRN16-4."
EXPERIMENTS,0.3168831168831169,"The results are summarized in Table 1 for LeNet5 MNIST and VGG16 CIFAR10 with FGSM
and a subset of the perturbation budget, and in Table 2 for WRN16-4 CIFAR10 with both FGSM
and PGD. The additional results are presented in Tables 3–6 and are moved to Appendix A. In
the tables, we highlight (in gray) the best-performing method for each range of compression rates
α ∈{0%, 50%, 80%}, and the best method overall (in bold). The experiments show that the proposed
method meaningfully conserves the accuracy as compared to the baseline for all datasets; moreover,
the robustness is improved from the baseline for 50% compression rate and, additionally, in the case
of MNIST for 80% compression rate. Compared to orthogonal non-compressed methods, CondLR
with 50% compression rate also outperforms Cayley SGD and Projected SGD. At the same time
CondLR is compressed by design, so we reduce memory costs alongside gaining robustness. As
anticipated in Section 4.1, low-rank methods without regularization deteriorate the condition number
and thus the robustness of the model. This is consistent with the results of Table 1, where we observe
that compression without regularization does not work well in terms of robust accuracy, in particular
DLRT, Vanilla and SVD prune exhibit a drop in the performance in comparison with baseline and,
consequently, CondLR . Specifically, per each fixed compression ratio α, our method exhibits the
highest robustness; moreover, the robustness of CondLR with α = 0.8 is higher than the robustness
of any other method with an even lower compression ratio, α = 0.5."
EXPERIMENTS,0.3194805194805195,"As a further experimental evaluation, we analyze the robustness of low-rank training models with
respect to small singular values. As shown by Theorem 1, CondLR is not affected by the steep
curvature of the low-rank manifold Mr, i.e. accuracy and convergence rate of CondLR do not
deteriorate when the conditioning of the weight matrices explodes. In contrast, small singular values
may significantly affect alternative low-rank training models. In Figure 2 we show the behavior
of loss, accuracy, and condition number as functions of the iteration steps, when the network is
initialized with ill-conditioned layer matrices. Precisely, similar to what is done in [29], we randomly
sample Gaussian initial weights, compute their SVD to define the initial U and V factors, and then
force the singular values of the initial S factor to decay exponentially. We observe that CondLR and
DLRT (which is also based on a low-rank gradient flow formulation) are the most robust, while the
performance of the other methods deteriorates dramatically. This confirms that, as also observed in
[29, 55], most low-rank approaches require specific fine-tuned choices of the initialization, while the
proposed robust low-rank training model ensures solid performance independent of the initialization."
CONCLUSIONS,0.3220779220779221,"6
Conclusions"
CONCLUSIONS,0.3246753246753247,"In recent years, extensive effort has been put into (a) studying different forms of implicit bias in deep
learning, including bias towards low-rank parameter weights [7, 9, 18], and (b) designing modified
training techniques that take advantage of the implicit low-rank structure to reduce training costs and
compress the network [29, 55, 71, 75]. Based on the notion of condition number of a network, as
a form of local Lipschitz constant, in this work we observe that training directly on the manifold
of matrices with a fixed rank may lead to instabilities due to bad conditioning, going against the
intuition that low-rank networks may be more robust to adversarial attacks. Thus, we propose an"
CONCLUSIONS,0.32727272727272727,"algorithm able to mitigate this phenomenon in a computationally affordable way, allowing us to train
directly on the manifold of matrices of a fixed rank while controlling the condition number."
CONCLUSIONS,0.32987012987012987,Table 1: Method comparison results
CONCLUSIONS,0.33246753246753247,"LeNet5 MNIST
VGG16 Cifar10
c.r.
(%)
Rel. perturbation ϵ
0.0
0.02
0.04
0.06
0.0
0.002
0.004
0.006"
CONCLUSIONS,0.33506493506493507,"Baseline
0.9872
0.9793
0.9655
0.9407
0.9104
0.752
0.5822
0.4592
0
Cayley SGD
0.9874
0.9804
0.9688
0.9486
0.8962
0.7446
0.5816
0.4529
0
Projected SGD
0.9878
0.9807
0.968
0.9476
0.897
0.7455
0.5832
0.4574
0"
CONCLUSIONS,0.33766233766233766,CondLR
CONCLUSIONS,0.34025974025974026,"τ = 0
0.9883
0.9825
0.9732
0.958
0.9099
0.7456
0.5711
0.4292
50
τ = 0.1
0.9877
0.9828
0.9763
0.9677
0.9093
0.7411
0.5985
0.4878
50
τ = 0.5
0.9867
0.9802
0.9724
0.9598
0.8997
0.7225
0.6019
0.5017
50
stief
0.986
0.9809
0.9721
0.9586
0.9138
0.7322
0.546
0.4054
50"
CONCLUSIONS,0.34285714285714286,"DLRT
0.967
0.9573
0.939
0.9078
0.8425
0.599
0.441
0.3691
50
Vanilla
0.9875
0.9773
0.9603
0.94
0.8997
0.6771
0.4886
0.3849
50
SVD prune
0.9883
0.9793
0.9639
0.9414
0.8992
0.673
0.4777
0.3698
50"
CONCLUSIONS,0.34545454545454546,CondLR
CONCLUSIONS,0.34805194805194806,"τ = 0.0
0.9882
0.9801
0.9676
0.9452
0.9066
0.7263
0.541
0.4
80
τ = 0.1
0.9877
0.9795
0.966
0.9444
0.9048
0.7123
0.5262
0.4013
80
τ = 0.5
0.9858
0.9768
0.9613
0.9342
0.8933
0.6823
0.4854
0.3666
80
stief
0.9815
0.9729
0.9581
0.9399
0.9067
0.7184
0.5289
0.3861
80"
CONCLUSIONS,0.35064935064935066,"DLRT
0.9649
0.9517
0.9281
0.8865
0.8092
0.5839
0.4178
0.3086
80
Vanilla
0.9862
0.972
0.9464
0.9194
0.881
0.6424
0.4266
0.299
80
SVD prune
0.9864
0.9737
0.9512
0.9281
0.8799
0.6357
0.4206
0.2927
80"
CONCLUSIONS,0.35324675324675325,"Table 2: WRN16-4 Cifar10
0.0
0.0003 0.0006
0.001
0.0013 0.0016 cr (%)"
CONCLUSIONS,0.35584415584415585,"0.9129
0.885
0.8531 0.8056 0.7667 0.7263
0
0.9254 0.8958
0.865
0.8092 0.7669 0.7236
50
0.9271 0.8974 0.8612 0.8117 0.7697 0.7223
50
0.9106 0.8807
0.844
0.7874 0.7413
0.697
80
0.9146 0.8833 0.8497 0.8004 0.7568 0.7142
80"
CONCLUSIONS,0.35844155844155845,"0.9129 0.8898 0.8619 0.8188 0.7828 0.7435
0
0.9254 0.8997 0.8714 0.8241 0.7791 0.7375
50
0.9271 0.9018 0.8684 0.8233
0.782
0.7374
50
0.9106 0.8838 0.8512 0.8014 0.7565 0.7064
80
0.9146 0.8861 0.8582
0.813
0.7705
0.724
80"
CONCLUSIONS,0.36103896103896105,Rel. perturbation ϵ
CONCLUSIONS,0.36363636363636365,"Baseline, FGSM"
CONCLUSIONS,0.36623376623376624,CondLR
CONCLUSIONS,0.36883116883116884,"τ = 0.0
τ = 0.1
τ = 0.0
τ = 0.1"
CONCLUSIONS,0.37142857142857144,"Baseline, PGD10"
CONCLUSIONS,0.37402597402597404,CondLR
CONCLUSIONS,0.37662337662337664,"τ = 0.0
τ = 0.1
τ = 0.0
τ = 0.1"
CONCLUSIONS,0.37922077922077924,"0
200
400
600
800 1000 1200 1400
Optimization step"
CONCLUSIONS,0.38181818181818183,0.0000
CONCLUSIONS,0.38441558441558443,0.0025
CONCLUSIONS,0.38701298701298703,0.0050
CONCLUSIONS,0.38961038961038963,0.0075
CONCLUSIONS,0.3922077922077922,0.0100
CONCLUSIONS,0.3948051948051948,0.0125
CONCLUSIONS,0.3974025974025974,0.0150
CONCLUSIONS,0.4,0.0175
CONCLUSIONS,0.4025974025974026,0.0200 Loss
CONCLUSIONS,0.4051948051948052,"0
200
400
600
800 1000 1200 1400
Optimization step 0.0 0.2 0.4 0.6 0.8 1.0"
CONCLUSIONS,0.4077922077922078,Accuracy
CONCLUSIONS,0.4103896103896104,"0
200
400
600
800 1000 1200 1400
Optimization step 103 108 1013 1018 1023 1028 1033 1038"
CONCLUSIONS,0.412987012987013,Upper bound cond(f)
CONCLUSIONS,0.4155844155844156,"CondLR, = 0"
CONCLUSIONS,0.41818181818181815,"DLRT
SVD Prune
Vanilla"
CONCLUSIONS,0.42077922077922075,"Figure 2: Evolution of loss, accuracy, and Q"
CONCLUSIONS,0.42337662337662335,"i cond(Wi) for Lenet5 on MNIST dataset, for ill-
conditioned initial layers whose singular values are forced to decay exponentially with powers of two."
REFERENCES,0.42597402597402595,References
REFERENCES,0.42857142857142855,"[1] P. Ablin and G. Peyré. Fast and accurate optimization on the orthogonal manifold without
retraction. In International Conference on Artificial Intelligence and Statistics, pages 5636–5657.
PMLR, 2022."
REFERENCES,0.43116883116883115,"[2] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2008."
REFERENCES,0.43376623376623374,"[3] P.-A. Absil and J. Malick. Projection-like retractions on matrix manifolds. SIAM Journal on
Optimization, 22(1):135–158, 2012."
REFERENCES,0.43636363636363634,"[4] P.-A. Absil and I. V. Oseledets. Low-rank retractions: a survey and new results. Computational
Optimization and Applications, 62(1):5–29, 2015."
REFERENCES,0.43896103896103894,"[5] C. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In International
Conference on Machine Learning, pages 291–301. PMLR, 2019."
REFERENCES,0.44155844155844154,"[6] M. Arjovsky, A. Shah, and Y. Bengio.
Unitary evolution recurrent neural networks.
In
International conference on machine learning, pages 1120–1128. PMLR, 2016."
REFERENCES,0.44415584415584414,"[7] S. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization.
Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.44675324675324674,"[8] A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2n learning: Network to network com-
pression via policy gradient reinforcement learning. In International Conference on Learning
Representations, 2018."
REFERENCES,0.44935064935064933,"[9] B. Bah, H. Rauhut, U. Terstiege, and M. Westdickenberg. Learning deep linear neural networks:
Riemannian gradient flows and convergence to global minimizers, 2020."
REFERENCES,0.45194805194805193,"[10] N. Bansal, X. Chen, and Z. Wang. Can we gain more from orthogonality regularizations in
training deep networks? Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.45454545454545453,"[11] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.45714285714285713,"[12] G. Ceruti and C. Lubich. An unconventional robust integrator for dynamical low-rank approxi-
mation. BIT Numerical Mathematics, 62(1):23–44, 2022."
REFERENCES,0.4597402597402597,"[13] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In International Conference on Machine Learning, pages
854–863. PMLR, 2017."
REFERENCES,0.4623376623376623,"[14] F. H. Clarke. Optimization and nonsmooth analysis. SIAM, 1990."
REFERENCES,0.4649350649350649,"[15] J. Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smooth-
ing. In international conference on machine learning, pages 1310–1320. PMLR, 2019."
REFERENCES,0.4675324675324675,"[16] J. Ding, T. Bu, Z. Yu, T. Huang, and J. Liu. Snn-rat: Robustness-enhanced spiking neural
network through regularized adversarial training. Advances in Neural Information Processing
Systems, 35:24780–24793, 2022."
REFERENCES,0.4701298701298701,"[17] R. Feng, K. Zheng, Y. Huang, D. Zhao, M. Jordan, and Z.-J. Zha. Rank diminishing in deep
neural networks. arXiv:2206.06072, 2022."
REFERENCES,0.4727272727272727,"[18] T. Galanti, Z. S. Siegel, A. Gupte, and T. Poggio. Sgd and weight decay provably induce a
low-rank bias in neural networks, 2023."
REFERENCES,0.4753246753246753,"[19] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples,
2015."
REFERENCES,0.4779220779220779,"[20] S. Gui, H. Wang, H. Yang, C. Yu, Z. Wang, and J. Liu. Model Compression with Adversarial
Robustness: A Unified Optimization Framework. 2019."
REFERENCES,0.4805194805194805,"[21] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for model compression
and acceleration on mobile devices. In Proceedings of the European conference on computer
vision, pages 784–800, 2018."
REFERENCES,0.4831168831168831,"[22] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In
IEEE International Conference on Computer Vision, pages 1389–1397, 2017."
REFERENCES,0.4857142857142857,"[23] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.4883116883116883,"[24] D. J. Higham. Condition numbers and their condition numbers. Linear Algebra and its
Applications, 214:193–213, 1995."
REFERENCES,0.4909090909090909,"[25] M. Huh, H. Mobahi, R. Zhang, B. Cheung, P. Agrawal, and P. Isola. The low-rank simplicity
bias in deep networks. Transactions on Machine Learning Research, 2023."
REFERENCES,0.4935064935064935,"[26] Y. Idelbayev and M. A. Carreira-Perpinán. Low-rank compression of neural nets: Learning
the rank of each layer. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 8049–8059, 2020."
REFERENCES,0.4961038961038961,"[27] K. Jia, D. Tao, S. Gao, and X. Xu. Improving training of deep neural networks via singular value
bounding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 4344–4352, 2017."
REFERENCES,0.4987012987012987,"[28] A. Jordao and H. Pedrini. On the effect of pruning on adversarial robustness. In 2021 IEEE/CVF
International Conference on Computer Vision Workshops (ICCVW). IEEE Computer Society,
2021."
REFERENCES,0.5012987012987012,"[29] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of
factorized neural layers. In International Conference on Learning Representations, 2021."
REFERENCES,0.5038961038961038,"[30] H. Kim, M. U. K. Khan, and C.-M. Kyung. Efficient neural network compression. In Pro-
ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
12569–12577, 2019."
REFERENCES,0.5064935064935064,"[31] O. Koch and C. Lubich. Dynamical low-rank approximation. SIAM Journal on Matrix Analysis
and Applications, 29(2):434–454, 2007."
REFERENCES,0.509090909090909,"[32] A. Kolbeinsson, J. Kossaifi, Y. Panagakis, A. Bulat, A. Anandkumar, I. Tzoulaki, and P. M.
Matthews. Tensor dropout for robust learning. IEEE Journal of Selected Topics in Signal
Processing, 15(3):630–640, 2021."
REFERENCES,0.5116883116883116,"[33] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.5142857142857142,"[34] P. Langenberg, E. R. Balda, A. Behboodi, and R. Mathar. On the effect of low-rank weights on
adversarial robustness of neural networks. arXiv preprint arXiv:1901.10371, 2019."
REFERENCES,0.5168831168831168,"[35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.5194805194805194,"[36] H. Lee, S. Han, and J. Lee. Generative adversarial trainer: Defense to adversarial perturbations
with gan, 2017."
REFERENCES,0.522077922077922,"[37] K. Leino, Z. Wang, and M. Fredrikson. Globally-robust neural networks. In International
Conference on Machine Learning, pages 6212–6222. PMLR, 2021."
REFERENCES,0.5246753246753246,"[38] M. Lezcano-Casado and D. Martınez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In International Conference on
Machine Learning, pages 3794–3803. PMLR, 2019."
REFERENCES,0.5272727272727272,"[39] J. Li, L. Fuxin, and S. Todorovic. Efficient Riemannian optimization on the Stiefel manifold via
the Cayley transform. 2019."
REFERENCES,0.5298701298701298,"[40] Q. Li, S. Haque, C. Anil, J. Lucas, R. B. Grosse, and J.-H. Jacobsen. Preventing gradient
attenuation in lipschitz constrained convolutional networks. Advances in neural information
processing systems, 32, 2019."
REFERENCES,0.5324675324675324,"[41] Y. Li, Z. Yang, Y. Wang, and C. Xu. Neural architecture dilation for adversarial robustness. In
Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.535064935064935,"[42] N. Liao, S. Wang, L. Xiang, N. Ye, S. Shao, and P. Chu. Achieving adversarial robustness via
sparsity. Machine Learning, pages 1–27, 2022."
REFERENCES,0.5376623376623376,"[43] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representa-
tions for efficient architecture search. In International Conference on Learning Representations,
2018."
REFERENCES,0.5402597402597402,"[44] X. Liu, Y. Li, C. Wu, and C.-J. Hsieh. Adv-BNN: Improved adversarial defense through robust
bayesian neural network. In International Conference on Learning Representations, 2019."
REFERENCES,0.5428571428571428,"[45] P. M. Long and H. Sedghi. Generalization bounds for deep convolutional neural networks. In
International Conference on Learning Representations, 2020."
REFERENCES,0.5454545454545454,"[46] D. Madaan, J. Shin, and S. J. Hwang. Adversarial neural pruning with latent vulnerability
suppression. In ICML, 2020."
REFERENCES,0.548051948051948,"[47] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations, 2018."
REFERENCES,0.5506493506493506,"[48] K. D. Maduranga, K. E. Helfrich, and Q. Ye. Complex unitary recurrent neural networks using
scaled cayley transform. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pages 4528–4535, 2019."
REFERENCES,0.5532467532467532,"[49] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning
Research, 22(165):1–73, 2021."
REFERENCES,0.5558441558441558,"[50] E. Massart. Orthogonal regularizers in deep learning: how to handle rectangular matrices? In
2022 26th International Conference on Pattern Recognition (ICPR), pages 1294–1299. IEEE,
2022."
REFERENCES,0.5584415584415584,"[51] L. Meunier, B. J. Delattre, A. Araujo, and A. Allauzen. A dynamical system perspective for
lipschitz neural networks. In International Conference on Machine Learning, pages 15484–
15500. PMLR, 2022."
REFERENCES,0.561038961038961,"[52] J. Pennington, S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning
through dynamical isometry: theory and practice. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.5636363636363636,"[53] J. R. Rice. A theory of condition. SIAM Journal on Numerical Analysis, 3(2):287–310, 1966."
REFERENCES,0.5662337662337662,"[54] D. A. Roberts, S. Yaida, and B. Hanin. The Principles of Deep Learning Theory. Cambridge
University Press, may 2022."
REFERENCES,0.5688311688311688,"[55] S. Schotthöfer, E. Zangrando, J. Kusch, G. Ceruti, and F. Tudisco. Low-rank lottery tickets:
finding efficient low-rank neural networks via matrix differential equations. In Advances in
Neural Information Processing Systems, 2022."
REFERENCES,0.5714285714285714,"[56] V. Sehwag, S. Wang, P. Mittal, and S. Jana. Hydra: Pruning adversarially robust neural networks.
NeurIPS, 2020."
REFERENCES,0.574025974025974,"[57] U. Shalit, D. Weinshall, and G. Chechik. Online learning in the manifold of low-rank matrices.
Advances in neural information processing systems, 23, 2010."
REFERENCES,0.5766233766233766,"[58] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.5792207792207792,"[59] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural
network Hessian maps. In Advances in Neural Information Processing Systems, volume 34,
2021."
REFERENCES,0.5818181818181818,"[60] S. Singla and S. Feizi. Skew orthogonal convolutions. In International Conference on Machine
Learning, pages 9756–9766. PMLR, 2021."
REFERENCES,0.5844155844155844,"[61] S. Singla, S. Singla, and S. Feizi. Improved deterministic l2 robustness on cifar-10 and cifar-100.
In International Conference on Learning Representations, 2021."
REFERENCES,0.587012987012987,"[62] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intrigu-
ing properties of neural networks. In International Conference on Learning Representations
(ICLR), 2014."
REFERENCES,0.5896103896103896,"[63] D. Terjék. Adversarial lipschitz regularization. In International Conference on Learning
Representations, 2020."
REFERENCES,0.5922077922077922,"[64] L. N. Trefethen and D. Bau. Numerical Linear Algebra. SIAM, 1997."
REFERENCES,0.5948051948051948,"[65] A. Trockman and J. Z. Kolter. Orthogonalizing convolutional layers with the cayley transform.
In International Conference on Learning Representations, 2021."
REFERENCES,0.5974025974025974,"[66] T. Tsiligkaridis and J. Roberts. On frank-wolfe adversarial training. In ICML 2021 Workshop
on Adversarial Machine Learning, 2021."
REFERENCES,0.6,"[67] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds
with accuracy. In International Conference on Learning Representations, 2018."
REFERENCES,0.6025974025974026,"[68] Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. Advances in neural information processing
systems, 31, 2018."
REFERENCES,0.6051948051948052,"[69] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal
on Mathematics of Data Science, 1(1):144–160, 2019."
REFERENCES,0.6077922077922078,"[70] A. Uschmajew and B. Vandereycken. Geometric methods on low-rank matrix and tensor
manifolds. Handbook of variational methods for nonlinear geometric data, pages 261–313,
2020."
REFERENCES,0.6103896103896104,"[71] H. Wang, S. Agarwal, and D. Papailiopoulos. Pufferfish: communication-efficient models at no
extra cost. Proceedings of Machine Learning and Systems, 3:365–386, 2021."
REFERENCES,0.612987012987013,"[72] X. Wei, Y. Xu, Y. Huang, H. Lv, H. Lan, M. Chen, and X. Tang. Learning extremely lightweight
and robust model with differentiable constraints on sparsity and condition number. In European
Conference on Computer Vision, pages 690–707. Springer, 2022."
REFERENCES,0.6155844155844156,"[73] Y.-L. Wu, H.-H. Shuai, Z.-R. Tam, and H.-Y. Chiu. Gradient normalization for generative
adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 6373–6382, 2021."
REFERENCES,0.6181818181818182,"[74] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. Schoenholz, and J. Pennington. Dynamical isometry and
a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks.
In International Conference on Machine Learning, pages 5393–5402. PMLR, 2018."
REFERENCES,0.6207792207792208,"[75] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y. Chen. Learning low-rank deep
neural networks via singular vector orthogonality regularization and singular value sparsifica-
tion. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), pages 2899–2908, 2020."
REFERENCES,0.6233766233766234,"[76] S. Ye, K. Xu, S. Liu, H. Cheng, J.-H. Lambrechts, H. Zhang, A. Zhou, K. Ma, Y. Wang,
and X. Lin. Adversarial robustness vs. model compression, or both? In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), October 2019."
REFERENCES,0.625974025974026,"[77] T. Yu, J. Li, Y. Cai, and P. Li. Constructing orthogonal convolutions in an explicit manner. In
International Conference on Learning Representations, 2022."
REFERENCES,0.6285714285714286,"[78] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016."
REFERENCES,0.6311688311688312,"[79] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled
trade-off between robustness and accuracy. In International conference on machine learning,
pages 7472–7482. PMLR, 2019."
REFERENCES,0.6337662337662338,"A
Additional results"
REFERENCES,0.6363636363636364,"In this section, we report the results on natural and robust accuracy for the broader range of per-
turbation budgets for Table 1 ϵ ∈{0.01, 0.02, 0.03, 0.04, 0.05, 0.06} for LeNet5 on MNIST, and
ϵ ∈{0.001, 0.002, 0.003, 0.004, 0.005, 0.006} for VGG16 on CIFAR10. See Table 3 and Table 4
(standard deviation is shown in gray). Additionally we provide results for Projected Gradient
Descent attack with L2 norm and 10 steps for VGG16 on CIFAR10 with perturbation budget
ϵ ∈{0.1, 0.13, 0.16, 0.2, 0.23, 0.26, 0.3}. See Table 5. The results confirm the same findings as
the ones reported in the main paper. Table 6 provides results of comparison between baseline and
Algorithm 1 for WideResnet model with depth 16 and width 4 on CIFAR100 for both FGSM and
PGD attack with L∞norm with perturbation budget ϵ ∈{0.0003, 0.0006, 0.001, 0.0013, 0.0016}."
REFERENCES,0.638961038961039,"B
Proof of Proposition 1"
REFERENCES,0.6415584415584416,"Lemma 2. Let ϕ : (Y, ∥· ∥Y ) →(Z, ∥· ∥Z) and ψ : (X, ∥· ∥X) →(Y, ∥· ∥Y ) be continuous
mappings between finite-dimensional Banach spaces, and assume cond(ψ) and cond(ϕ) are well
defined. Then the following inequality holds:"
REFERENCES,0.6441558441558441,cond(ϕ ◦ψ) ≤cond(ϕ) cond(ψ)
REFERENCES,0.6467532467532467,"Proof. To prove this lemma, we will pass through the definition of R(ϕ ◦ψ, x, δ). Let us recall that
it is defined as"
REFERENCES,0.6493506493506493,"R(ϕ ◦ψ, x, δ) = ∥ϕ ◦ψ(x + δ) −ϕ ◦ψ(x)∥Z∥x∥X"
REFERENCES,0.6519480519480519,"∥δ∥X∥ϕ ◦ψ(x)∥Z
Now, if ψ(x + δ) −ψ(x) = 0 for δ ̸= 0, then R(ϕ ◦ψ, x, δ) = 0. Since we’re interested in the
supremum, we can restrict to δ such that ψ(x + δ) −ψ(x) ̸= 0. Moreover, by multiplying and"
REFERENCES,0.6545454545454545,"Table 3: LeNet5 MNIST, FGSM
0.0
0.001
0.002
0.003
0.004
0.005
0.006
cr (%)"
REFERENCES,0.6571428571428571,"0.9872 0.9836 0.9793 0.9735 0.9655 0.9543 0.9407
0
8.1e-04
5.e-04
7.7e-04
6.4e-04
5.1e-04
1.0e-03
1.7e-03
0.9874 0.9843 0.9804 0.9753 0.9688 0.9602 0.9486
0
3.8e-04
5.2e-04
7.1e-04
4.9e-04
5.1e-04
7.4e-04
1.3e-03
0.9878 0.9845 0.9807 0.9753
0.968
0.9594 0.9476
0
2.1e-04
5.2e-04
8.6e-04
7.8e-04
1.0e-03
8.1e-04
1.1e-03"
REFERENCES,0.6597402597402597,"0.9883 0.9857 0.9825 0.9783 0.9732 0.9661
0.958
50
3.2e-04
8.5e-04
4.9e-04
9.9e-04
9.e-04
4.8e-04
6.0e-04
0.9877 0.9854 0.9828
0.98
0.9763
0.972
0.9677
50
4.1e-04
5.5e-04
8.4e-04
1.2e-03
8.9e-04
4.9e-04
1.2e-03
0.9867 0.9831 0.9802 0.9772 0.9724 0.9675 0.9598
50
4.4e-04
5.1e-04
6.4e-04
5.1e-04
6.4e-04
1.2e-03
1.2e-03
0.986
0.9838 0.9809 0.9773 0.9721 0.9655 0.9586
50
2.6e-04
2.4e-04
2.4e-04
2.8e-04
3.6e-04
6.2e-04
1.4e-04
0.967
0.9627 0.9573 0.9499
0.939
0.9253 0.9078
50
6.8e-04
6.9e-04
7.5e-04
1.0e-03
6.6e-04
1.1e-03
1.5e-03
0.9875
0.983
0.9773
0.97
0.9603 0.9503
0.94
50
5.8e-04
8.2e-04
1.2e-03
3.e-03
4.1e-03
4.2e-03
3.9e-03
0.9883 0.9841 0.9793 0.9733 0.9639 0.9526 0.9414
50
9.6e-04
8.6e-04
1.1e-03
1.4e-03
1.2e-03
1.5e-03
2.6e-03"
REFERENCES,0.6623376623376623,"0.9882 0.9846 0.9801 0.9743 0.9676
0.958
0.9452
80
7.5e-04
2.0e-03
3.4e-03
4.3e-03
6.3e-03
8.e-03
1.2e-02
0.9877
0.984
0.9795 0.9736
0.966
0.9564 0.9444
80
5.1e-04
5.3e-04
6.1e-04
1.0e-03
8.1e-04
1.4e-03
1.8e-03
0.9858
0.982
0.9768
0.97
0.9613 0.9487 0.9342
80
6.5e-04
8.e-04
8.2e-04
1.3e-03
1.9e-03
3.5e-03
4.0e-03
0.9815 0.9779 0.9729 0.9659 0.9581 0.9499 0.9399
80
3.8e-04
4.6e-04
1.2e-04
3.7e-04
5.7e-04
4.5e-04
1.0e-03
0.9649 0.9596 0.9517 0.9414 0.9281 0.9108 0.8865
80
5.8e-04
9.7e-04
1.7e-03
2.6e-03
4.6e-03
7.6e-03
1.1e-02
0.9862 0.9802
0.972
0.9602 0.9464 0.9331 0.9194
80
6.9e-04
9.7e-04
1.9e-03
3.4e-03
3.1e-03
4.1e-03
5.2e-03
0.9864
0.981
0.9737 0.9634 0.9512 0.9392 0.9281
80
8.0e-04
1.1e-03
2.2e-03
3.e-03
4.5e-03
5.1e-03
5.9e-03"
REFERENCES,0.6649350649350649,Rel. perturbation ϵ
REFERENCES,0.6675324675324675,Baseline
REFERENCES,0.6701298701298701,Cayley SGD
REFERENCES,0.6727272727272727,Projected SGD
REFERENCES,0.6753246753246753,CondLR
REFERENCES,0.6779220779220779,τ = 0.0
REFERENCES,0.6805194805194805,"τ = 0.1
τ = 0.5 stief DLRT"
REFERENCES,0.6831168831168831,vanilla
REFERENCES,0.6857142857142857,SVD prune
REFERENCES,0.6883116883116883,CondLR
REFERENCES,0.6909090909090909,"τ = 0.0
τ = 0.1
τ = 0.5 stief DLRT"
REFERENCES,0.6935064935064935,vanilla
REFERENCES,0.6961038961038961,SVD prune
REFERENCES,0.6987012987012987,dividing by ∥ψ(x)∥Y we obtain:
REFERENCES,0.7012987012987013,"R(ϕ ◦ψ, x, δ) =
∥ϕ ◦ψ(x + δ) −ϕ ◦ψ(x)∥Z∥ψ(x)∥Y"
REFERENCES,0.7038961038961039,∥ψ(x + δ) −ψ(x)∥Y ∥ϕ ◦ψ(x)∥Z
REFERENCES,0.7064935064935065,∥ψ(x + δ) −ψ(x)∥Y ∥x∥X
REFERENCES,0.7090909090909091,∥δ∥X∥ψ(x)∥Y 
REFERENCES,0.7116883116883117,"Now, if we define cond(f, x, ε) =
sup
δ̸=0:∥δ∥X≤ε
R(f, x, δ), we take the supremum both sides on the set"
REFERENCES,0.7142857142857143,{δ ∈X | ∥δ∥X ≤ε} and we can upper bound it with the product of the suprema of the two blocks
REFERENCES,0.7168831168831169,"sup
δ̸=0:∥δ∥X≤ε
R(ϕ ◦ψ, x, δ) = cond(ψ, x, ε)
sup
δ̸=0:∥δ∥X≤ε"
REFERENCES,0.7194805194805195,∥ϕ ◦ψ(x + δ) −ϕ ◦ψ(x)∥Z∥ψ(x)∥Y
REFERENCES,0.7220779220779221,"∥ψ(x + δ) −ψ(x)∥Y ∥ϕ ◦ψ(x)∥Z 
= ⋆"
REFERENCES,0.7246753246753247,"Now, letting η = ψ(x + δ) −ψ(δ), we can rewrite the second part of the last equation as a sort of
restriction of cond(ϕ, ψ(x), ε) to the particular set of perturbation directions of the form η. Thus, we
can lower-bound it as:"
REFERENCES,0.7272727272727273,"⋆≤cond(ψ, x, ε)
sup
η̸=0:∥η∥Y ≤ε"
REFERENCES,0.7298701298701299,∥ϕ(ψ(x) + η) −ϕ(ψ(x))∥Z∥ψ(x)∥Y
REFERENCES,0.7324675324675325,"∥η∥Y ∥ϕ(ψ(x))∥Z 
="
REFERENCES,0.7350649350649351,"= cond(ψ, x, ε) cond(ϕ, ψ(x), ε)"
REFERENCES,0.7376623376623377,"By hypothesis, cond(ϕ) and cond(ψ) are finite, so we can take the limit ε ↓0 to get:"
REFERENCES,0.7402597402597403,"cond(ϕ ◦ψ, x) ≤cond(ϕ, ψ(x)) cond(ψ, x)"
REFERENCES,0.7428571428571429,"Finally, taking the supremum over x on both members of the last equation, we can upper bound it
with the original condition number without constraint on the directions:"
REFERENCES,0.7454545454545455,"cond(ϕ ◦ψ) ≤sup
x cond(ϕ, ψ(x)) cond(ψ, x) ≤cond(ϕ) cond(ψ)"
REFERENCES,0.7480519480519481,and thus conclude.
REFERENCES,0.7506493506493507,"Table 4: VGG16 Cifar10, FGSM
0.0
0.001
0.002
0.003
0.004
0.005
0.006
cr (%)"
REFERENCES,0.7532467532467533,"0.9104 0.8397
0.752
0.6631 0.5822 0.5154 0.4592
0
1.3e-03
2.e-03
3.3e-03
2.7e-03
3.9e-03
6.6e-03
8.9e-03
0.8962 0.8262 0.7446 0.6624 0.5816 0.5132 0.4529
0
1.7e-03
3.9e-03
3.5e-03
4.1e-03
4.4e-03
4.2e-03
4.3e-03
0.897
0.8271 0.7455
0.661
0.5832 0.5159 0.4574
0
1.8e-03
3.1e-03
3.0e-03
4.1e-03
5.3e-03
4.2e-03
4.5e-03"
REFERENCES,0.7558441558441559,"0.9099 0.8366 0.7456 0.6564 0.5711 0.4955 0.4292
50
7.4e-03
9.2e-03
9.8e-03
1.4e-02
1.5e-02
1.8e-02
1.6e-02
0.9093 0.8291 0.7411 0.6678 0.5985 0.5386 0.4878
50
1.9e-03
3.1e-03
6.e-03
1.2e-02
1.2e-02
8.4e-03
5.6e-03
0.8997 0.8136 0.7225 0.6577 0.6019 0.5485 0.5017
50
1.0e-03
1.9e-03
5.3e-03
7.5e-03
6.8e-03
7.7e-03
9.1e-03
0.9138 0.8331 0.7322 0.6337
0.546
0.4702 0.4054
50
1.3e-03
3.3e-03
5.4e-03
3.9e-03
3.7e-03
4.1e-03
3.9e-03
0.8425 0.7179
0.599
0.5065
0.441
0.3979 0.3691
50
1.6e-03
6.2e-03
8.8e-03
8.3e-03
1.2e-02
1.6e-02
1.8e-02
0.8997 0.7962 0.6771 0.5708 0.4886 0.4276 0.3849
50
3.2e-03
4.4e-03
6.1e-03
8.3e-03
9.9e-03
1.0e-02
1.1e-02
0.8992 0.7936
0.673
0.5643 0.4777 0.4161 0.3698
50
3.1e-03
1.7e-03
2.5e-03
7.6e-03
1.1e-02
1.3e-02
1.3e-02"
REFERENCES,0.7584415584415585,"0.9066
0.825
0.7263 0.6289
0.541
0.4633
0.4
80
2.2e-03
1.8e-03
2.8e-03
3.2e-03
5.2e-03
5.3e-03
5.6e-03
0.9048 0.8152 0.7123 0.6112 0.5262 0.4574 0.4013
80
1.6e-03
3.8e-03
5.4e-03
6.3e-03
9.1e-03
9.9e-03
8.6e-03
0.8933 0.7952 0.6823 0.5748 0.4854 0.4177 0.3666
80
1.3e-03
1.4e-03
4.2e-03
6.e-03
5.3e-03
5.3e-03
5.3e-03
0.9067 0.8202 0.7184 0.6185 0.5289 0.4515 0.3861
80
4.4e-04
2.5e-03
5.9e-03
5.5e-03
7.6e-03
8.9e-03
9.3e-03
0.8092 0.6916 0.5839 0.4936 0.4178 0.3577 0.3086
80
2.e-02
1.9e-02
2.2e-02
2.5e-02
2.4e-02
2.3e-02
2.2e-02
0.881
0.7687 0.6424
0.523
0.4266 0.3533
0.299
80
2.4e-03
5.e-03
7.4e-03
8.5e-03
1.1e-02
1.2e-02
1.2e-02
0.8799 0.7634 0.6357 0.5168 0.4206 0.3474 0.2927
80
3.7e-03
8.2e-03
1.2e-02
1.4e-02
1.2e-02
1.1e-02
1.1e-02"
REFERENCES,0.7610389610389611,Rel. perturbation ϵ
REFERENCES,0.7636363636363637,Baseline
REFERENCES,0.7662337662337663,Cayley SGD
REFERENCES,0.7688311688311689,Projected SGD
REFERENCES,0.7714285714285715,CondLR
REFERENCES,0.7740259740259741,τ = 0.0
REFERENCES,0.7766233766233767,τ = 0.1
REFERENCES,0.7792207792207793,τ = 0.5 stief DLRT
REFERENCES,0.7818181818181819,vanilla
REFERENCES,0.7844155844155845,SVD prune
REFERENCES,0.787012987012987,CondLR
REFERENCES,0.7896103896103897,τ = 0.0
REFERENCES,0.7922077922077922,"τ = 0.1
τ = 0.5 stief DLRT"
REFERENCES,0.7948051948051948,vanilla
REFERENCES,0.7974025974025974,SVD prune
REFERENCES,0.8,"Now, the proof of Proposition 1 follows directly by applying the Lemma above recursively."
REFERENCES,0.8025974025974026,"Proof of Proposition1. By defining Ti(z) = Wiz, we can write the neural network f as"
REFERENCES,0.8051948051948052,f(x) = (σL ◦TL ◦σL−1 ◦· · · ◦T1)(x)
REFERENCES,0.8077922077922078,"for nonlinear activation functions σi. Thus, for L = 1, the thesis follows directly from Lemma2, as
long as cond(σ1) ≤C < ∞. For L > 1, one can unwrap the compositional structure of f from the
left, defining ϕ(z) = (σL ◦TL)(z) and ψ(x) = (σL−1 ◦· · · ◦T1)(x). Then by using Lemma2 we
have that
cond(f) ≤cond(ϕ) cond(ψ) ≤cond(σL) cond(TL) cond(ψ).
Now, since ψ is a network of depth L −1, we can proceed inductively to obtain that cond(f) ≤
C Q"
REFERENCES,0.8103896103896104,"i cond(Ti), with C = ΠL
i=1 cond(σi), and we conclude."
REFERENCES,0.812987012987013,"Note that the result above is of interest only if cond(σ) = supx∈X cond(σ; x) < ∞. When σ is
Lipschitz, using the formula"
REFERENCES,0.8155844155844156,"cond(f; x) =
sup
νx∈∂σ(x)
∥νx∥∥x∥∥σ(x)∥−1,"
REFERENCES,0.8181818181818182,"Table 5: VGG16 Cifar10, PGD
0.0
0.1
0.13
0.16
0.2
0.23
0.26
0.3
cr (%)"
REFERENCES,0.8207792207792208,"0.9104 0.6704 0.5799 0.5021 0.4222 0.3788
0.347
0.3188
0
1.3e-03
5.4e-03
8.1e-03
9.0e-03
1.4e-02
1.4e-02
1.4e-02
1.4e-02
0.8962 0.6768 0.5938 0.5109 0.4087 0.3446 0.2921 0.2362
0
1.7e-03
4.4e-03
5.3e-03
7.1e-03
6.9e-03
8.4e-03
8.9e-03
1.0e-02
0.897
0.6764 0.5925 0.5106 0.4086 0.3451 0.2919 0.2364
0
1.8e-03
5.0e-03
6.4e-03
5.6e-03
6.8e-03
9.e-03
7.5e-03
7.6e-03"
REFERENCES,0.8233766233766234,"0.9099 0.6608
0.562
0.4712 0.3665
0.305
0.2551 0.2078
50
7.4e-03
1.8e-02
2.4e-02
2.8e-02
2.7e-02
2.2e-02
1.8e-02
1.5e-02
0.9093 0.6703 0.6208 0.5915 0.5692 0.5596 0.5528 0.5458
50
1.9e-03
1.6e-02
3.4e-02
5.1e-02
6.5e-02
7.1e-02
7.6e-02
7.9e-02
0.8997 0.6484 0.6025 0.5775 0.5603 0.5521 0.5475 0.5425
50
1.0e-03
9.8e-03
1.9e-02
2.6e-02
3.1e-02
3.2e-02
3.3e-02
3.5e-02
0.9138 0.6318 0.5253 0.4266 0.3118 0.2445
0.192
0.1388
50
1.3e-03
6.1e-03
7.0e-03
6.9e-03
7.8e-03
6.4e-03
6.9e-03
7.2e-03
0.8997 0.5574 0.4511 0.3686 0.2962 0.2628 0.2402 0.2184
50
3.2e-03
1.2e-02
1.5e-02
1.5e-02
1.8e-02
2.0e-02
2.1e-02
2.2e-02
0.8992 0.5487 0.4385 0.3523 0.2795 0.2438 0.2206 0.1994
50
3.1e-03
5.4e-03
8.9e-03
1.5e-02
2.0e-02
2.5e-02
2.9e-02
3.2e-02"
REFERENCES,0.825974025974026,"0.9066 0.6324 0.5323 0.4357
0.325
0.2599 0.2096 0.1603
80
2.2e-03
4.8e-03
6.2e-03
5.3e-03
8.9e-03
1.2e-02
1.5e-02
1.5e-02
0.9048
0.61
0.5084 0.4219
0.337
0.2944 0.2655 0.2397
80
1.6e-03
8.7e-03
9.7e-03
1.1e-02
1.1e-02
1.1e-02
1.1e-02
1.1e-02
0.8933 0.5745 0.4635
0.372
0.283
0.2382 0.2065 0.1784
80
1.3e-03
9.1e-03
9.7e-03
1.0e-02
1.3e-02
1.3e-02
1.6e-02
1.7e-02
0.9067 0.6153 0.5094 0.4082 0.2924 0.2236 0.1716
0.118
80
4.4e-04
6.7e-03
9.5e-03
1.e-02
9.4e-03
9.1e-03
8.7e-03
7.5e-03
0.881
0.514
0.397
0.3012 0.2097 0.1629 0.1315 0.1037
80
2.4e-03
1.3e-02
1.5e-02
1.4e-02
1.3e-02
1.4e-02
1.4e-02
1.4e-02
0.8799 0.5064 0.3906 0.2957 0.2016 0.1549 0.1222 0.0957
80
3.7e-03
2.e-02
2.1e-02
1.8e-02
1.5e-02
1.2e-02
1.1e-02
1.2e-02"
REFERENCES,0.8285714285714286,Rel. perturbation ϵ
REFERENCES,0.8311688311688312,Baseline
REFERENCES,0.8337662337662337,Cayley SGD
REFERENCES,0.8363636363636363,Projected SGD
REFERENCES,0.8389610389610389,CondLR
REFERENCES,0.8415584415584415,τ = 0.0
REFERENCES,0.8441558441558441,"τ = 0.1
τ = 0.5 stief"
REFERENCES,0.8467532467532467,vanilla
REFERENCES,0.8493506493506493,SVD prune
REFERENCES,0.8519480519480519,CondLR
REFERENCES,0.8545454545454545,τ = 0.0
REFERENCES,0.8571428571428571,"τ = 0.1
τ = 0.5 stief"
REFERENCES,0.8597402597402597,vanilla
REFERENCES,0.8623376623376623,SVD prune
REFERENCES,0.8649350649350649,Table 6: WRN16-4 Cifar100
REFERENCES,0.8675324675324675,0.0003 0.0006 0.001 0.0013 0.0016 cr (%)
REFERENCES,0.8701298701298701,"0.5845 0.5267 0.4606 0.4187 0.3841
0
0.6492 0.5806 0.5009 0.4499 0.4011
50
0.6062 0.5442 0.4693 0.4199 0.3774
50"
REFERENCES,0.8727272727272727,"0.5925 0.5401 0.4713 0.4289 0.3878
0
0.6552 0.5918 0.5127 0.4531 0.3972]
50
0.6160 0.5547 0.4823 0.4290 0.3810
50"
REFERENCES,0.8753246753246753,Rel. perturbation ϵ
REFERENCES,0.8779220779220779,"Baseline, FGSM"
REFERENCES,0.8805194805194805,CondLR
REFERENCES,0.8831168831168831,"τ = 0.1
τ = 0.5"
REFERENCES,0.8857142857142857,"Baseline, PGD10"
REFERENCES,0.8883116883116883,CondLR
REFERENCES,0.8909090909090909,"τ = 0.1
τ = 0.5"
REFERENCES,0.8935064935064935,"with ∂being the Clarke’s generalized gradient operator [14], we observe below that this is the case
for a broad list of activation functions σ and feature spaces X."
REFERENCES,0.8961038961038961,"• LeakyReLU. For x ∈R,α > 0, let σ(x) = max{0, x} + α min{0, x}. Then any νx ∈∂σ(x) is
such that νx = 1 if x > 0; νx = α if x < 0; νx = [min(α, 1), max(α, 1)] otherwise. Thus"
REFERENCES,0.8987012987012987,"cond(σ) = sup
x̸=0
cond(σ; x) = sup
x̸=0
sup
β∈∂σ(x)"
REFERENCES,0.9012987012987013,|x||1x>0 + α1x<0 + β1x=0|
REFERENCES,0.9038961038961039,"| max{0, x} + α min{0, x}| = max(α, 1)"
REFERENCES,0.9064935064935065,"• Tanh. For x ∈R, let σ(x) = tanh(x). Then σ′(x) =
1
cosh2(x) and thus"
REFERENCES,0.9090909090909091,"cond(σ) = sup
x
|x|
| tanh(x)|| cosh2(x)| = sup
x
|4x|
|ex −e−x||ex + e−x| = 1"
REFERENCES,0.9116883116883117,"Since the maximum of cond(σ, x) is reached at zero, where the function can be extended by
continuity.
• Hardtanh. For x ∈[−a, a] and a > 0, let σ(x) = a1x>a −a1x<−a + x1x∈[−a,a]. Then, we have
that ∂σ(x) coincides with the derivative values in all points but x = ±a. In those two points, we
have ∂σ(±a) = [0, 1]. Thus, for any νx ∈∂σ(x), we have"
REFERENCES,0.9142857142857143,"cond(σ) =
sup
x∈[−a,a]"
REFERENCES,0.9168831168831169,|νx||x|
REFERENCES,0.9194805194805195,"|σ(x)| ≤
sup
x∈[−a,a]"
REFERENCES,0.922077922077922,"|x|
|σ(x)| = a"
REFERENCES,0.9246753246753247,• Logistic sigmoid. For x ∈R let σ(x) = (1 + e−x)−1. Then σ′(x) = σ(x)(1 −σ(x)) and thus
REFERENCES,0.9272727272727272,cond(σ; x) = |x|(1 −σ(x)) = |x|e−x(1 + e−x)−1 .
REFERENCES,0.9298701298701298,"Therefore, when x ≥0, we have |x|e−x ≤1/e and (1 + e−x) ≥1, thus cond(σ; x) ≤1/e.
• Softplus. For x ∈R, let σ(x) = ln(1+ex). Then σ′(x) = S(x) = (1+e−x)−1 and cond(σ; x) =
|x|S(x)σ(x)−1. Thus, for x ≥0, we have cond(σ; x) ≤1.
• SiLU. For x ∈R let σ(x) = x(1 + e−x)−1 = xS(x). Then, σ′(x) = S(x) + xS(x)(1 −S(x))
and thus for any x ≥0 we have"
REFERENCES,0.9324675324675324,cond(σ; x) = |1 + x(1 −S(x))| ≤1 + 1 e
REFERENCES,0.935064935064935,"C
Proof of Theorem 1"
REFERENCES,0.9376623376623376,"In the following, the proof of the main approximation result is presented. We underline that the core
part of the proof relies on [31, Theorem 5.2]. For completeness, we repropose here main elements of
the argument. We refer the interested reader to [31] and references therein for further details."
REFERENCES,0.9402597402597402,"Proof. Let Y (t) be the solution of (5) at time t ∈[0, λ]. First, we observe that the projected subflows
of W(t) = f
W(t) + E(t) and f
W(t) satisfy the differential equations

 "
REFERENCES,0.9428571428571428,"˙Y = P(Y ) ˙f
W + P(Y ) ˙E ,
˙f
W = P

f
W
 ˙f
W ."
REFERENCES,0.9454545454545454,"where P(·) denotes the orthogonal projection into the tangent space of the low-rank manifold Mr.
Next, we observe that the following identities hold"
REFERENCES,0.948051948051948,"(P(Y ) −P(f
W)) ˙f
W = −(P ⊥(Y ) −P ⊥(f
W)) ˙f
W = −P ⊥(Y ) ˙f
W = −P ⊥(Y )2 ˙f
W ."
REFERENCES,0.9506493506493506,where P ⊥(·) = I −P(·) represents the complementary orthogonal projection. The latter implies that
REFERENCES,0.9532467532467532,"⟨Y −f
W, (P(Y ) −P(f
W)) ˙f
W⟩= ⟨P ⊥(Y )(Y −f
W), (P(Y ) −P(f
W)) ˙f
W⟩."
REFERENCES,0.9558441558441558,"Let γ = 32µ(s −ε)−2. It follows from [31, Lemma 4.2] that"
REFERENCES,0.9584415584415584,"⟨Y −f
W, ˙Y −˙f
W⟩= ⟨P ⊥(Y )(Y −f
W), (P(Y ) −P(f
W)) ˙f
W⟩+ ⟨Y −f
W, P(Y ) ˙E⟩"
REFERENCES,0.961038961038961,"≤γ∥Y −f
W∥3 + η∥Y −f
W∥."
REFERENCES,0.9636363636363636,"Further, we remind that"
REFERENCES,0.9662337662337662,"⟨Y −f
W, ˙Y −˙f
W⟩= 1"
D,0.9688311688311688,"2
d
dt∥Y −f
W∥2 = ∥Y −f
W∥d"
D,0.9714285714285714,"dt∥Y −f
W∥."
D,0.974025974025974,"Hence, the error e(t) = ∥Y (t) −f
W(t)∥satisfies the differential inequality"
D,0.9766233766233766,"˙e ≤γe2 + η,
e(0) = 0 ."
D,0.9792207792207792,"The error e(t) for t ∈[0, λ] admits an upper bound given by the solution of"
D,0.9818181818181818,"˙z = γz2 + η,
z(0) = 0 ."
D,0.9844155844155844,The last differential initial-value problem admits a closed solution given by
D,0.987012987012987,"z(t) =
p"
D,0.9896103896103896,"η/γ tan (t
p ηγ) ,"
D,0.9922077922077922,where the last term is bounded by 2tη for t√γη ≤1. The proof thus concludes as follows
D,0.9948051948051948,"∥Y (t) −W(t)∥≤∥Y (t) −f
W(t)∥+ ∥E(t)∥≤2tη + tη = 3tη ,"
D,0.9974025974025974,"where the last estimate arise by the integral identity E(t) =
R t
0 ˙E(s)ds."
