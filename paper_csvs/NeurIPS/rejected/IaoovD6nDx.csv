Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002331002331002331,"Although it is well known that exploration plays a key role in Reinforcement
1"
ABSTRACT,0.004662004662004662,"Learning (RL), prevailing exploration strategies for continuous control tasks in
2"
ABSTRACT,0.006993006993006993,"RL are mainly based on naive isotropic Gaussian noise regardless of the causality
3"
ABSTRACT,0.009324009324009324,"relationship between action space and the task and consider all dimensions of
4"
ABSTRACT,0.011655011655011656,"actions equally important. In this work, we propose to conduct interventions on
5"
ABSTRACT,0.013986013986013986,"the primal action space to discover the causal relationship between the action
6"
ABSTRACT,0.016317016317016316,"space and the task reward. We propose the method of State-Wise Action Reﬁned
7"
ABSTRACT,0.018648018648018648,"(SWAR), which addresses the issue of action space redundancy and promote
8"
ABSTRACT,0.02097902097902098,"causality discovery in RL. We formulate causality discovery in RL tasks as a state-
9"
ABSTRACT,0.023310023310023312,"dependent action space selection problem and propose two practical algorithms
10"
ABSTRACT,0.02564102564102564,"as solutions. The ﬁrst approach, TD-SWAR, detects task-related actions during
11"
ABSTRACT,0.027972027972027972,"temporal difference learning, while the second approach, Dyn-SWAR, reveals
12"
ABSTRACT,0.030303030303030304,"important actions through dynamic model prediction. Empirically, both methods
13"
ABSTRACT,0.03263403263403263,"provide approaches to understand the decisions made by RL agents and improve
14"
ABSTRACT,0.03496503496503497,"learning efﬁciency in action-redundant tasks.
15"
INTRODUCTION,0.037296037296037296,"1
Introduction
16"
INTRODUCTION,0.039627039627039624,"Although model-free RL has achieved great success in various challenging tasks and outperforms
17"
INTRODUCTION,0.04195804195804196,"experts in most cases [21, 26, 17, 34, 4], the design of action space always requires elaboration. For
18"
INTRODUCTION,0.04428904428904429,"example, in the game StarCraftII, hundreds of units can be selected and controlled to perform various
19"
INTRODUCTION,0.046620046620046623,"actions. To tackle the difﬁculty in exploration caused by the extremely large action and state space,
20"
INTRODUCTION,0.04895104895104895,"hierarchical action space design and imitation learning are used [27, 34] to reduce the exploration
21"
INTRODUCTION,0.05128205128205128,"space. Both of those approaches require expert knowledge of the task. On the other hand, even in the
22"
INTRODUCTION,0.053613053613053616,"context of imitation learning where expert data is assumed to be accessible, causal confusion will still
23"
INTRODUCTION,0.055944055944055944,"hinder the performance of an agent [8]. Those defects motivate us to explore the causality-awareness
24"
INTRODUCTION,0.05827505827505827,"of an agent that permits an agent to discover the causal relationship for the environment and select
25"
INTRODUCTION,0.06060606060606061,"useful dimensions of action space during policy learning in pursuance of improved learning efﬁciency.
26"
INTRODUCTION,0.06293706293706294,"Another motivating example is the in-hand manipulation tasks [2]: robotics equipped with touch
27"
INTRODUCTION,0.06526806526806526,"sensors outperforms the policies learned without sensors by a clear margin in hand-in manipulation
28"
INTRODUCTION,0.0675990675990676,"tasks [20], showing the importance of causality discovery between actions and feedbacks in RL. A
29"
INTRODUCTION,0.06993006993006994,"similar example can be found in human learning: knowing nothing about how to control the ﬁnger
30"
INTRODUCTION,0.07226107226107226,"joints ﬂexibly may not hinder a baby learns to walk, and a baby has not learned how to walk can still
31"
INTRODUCTION,0.07459207459207459,"learn to use forks and spoons skillfully, inspiring us to believe that the challenge for exploration can
32"
INTRODUCTION,0.07692307692307693,"be greatly eased after the causality between action space and the given task is learned.
33"
INTRODUCTION,0.07925407925407925,"In this work, the recent advance of instance-wise feature selection technique [38] is improved to be
34"
INTRODUCTION,0.08158508158508158,"more suitable in large-scale state-wise action selection tasks and adapted to the time-series causal
35"
INTRODUCTION,0.08391608391608392,"discovery setting to select state-conditioned action space in RL with redundant action space. With the
36"
INTRODUCTION,0.08624708624708624,"Figure 1: Block diagram of INVASE in temporal difference learning. States and actions sampled
from replay buffer are fed into the selector network that predicts the selection probabilities of different
dimensions of actions. A selection mask is then generated according to such a selection probability
vector. The critic network and the baseline network are trained to minimize temporal difference error
with states and the selected dimension of actions and primal action respectively. The difference of
TD-Error is used to conduct a policy gradient to update the selector network."
INTRODUCTION,0.08857808857808858,"proposed method, the agent learns to perform intervention, discover the true structural causal model
37"
INTRODUCTION,0.09090909090909091,"(SCM) and select task-related actions for a given task, remarkably reduces the burden of exploration
38"
INTRODUCTION,0.09324009324009325,"and obtains on-par learning efﬁciency as well as asymptotic performance compared with agents
39"
INTRODUCTION,0.09557109557109557,"trained in the oracle settings where the action spaces are pruned according to given tasks manually.
40"
PRELIMINARY,0.0979020979020979,"2
Preliminary
41"
PRELIMINARY,0.10023310023310024,"Markov Decision Processes
RL tasks can be formally deﬁned as Markov Decision Processes
42"
PRELIMINARY,0.10256410256410256,"(MDPs), where an agent interacts with the environment and learns to make decision at every timestep.
43"
PRELIMINARY,0.1048951048951049,"Formally, we consider the deterministic MDP with a ﬁxed horizon H 2 N+ denoted by a tuple
44"
PRELIMINARY,0.10722610722610723,"(S, A, H, r, γ, T , ⇢0), where S and A are the |S|-dimensional state and |A|-dimensional action space;
45"
PRELIMINARY,0.10955710955710955,"r : S ⇥A 7! R denotes the reward function; γ 2 (0, 1] is the discount factor indicating importance
46"
PRELIMINARY,0.11188811188811189,"of present returns compared with long-term returns; T : S ⇥A 7! S denotes the transition dynamics;
47"
PRELIMINARY,0.11421911421911422,"⇢0 is the initial state distribution.
48"
PRELIMINARY,0.11655011655011654,"We use ⇧to represent the stationary deterministic policy class, i.e., ⇧= {⇡: S 7! A}. The
49"
PRELIMINARY,0.11888111888111888,"learning objective of an RL algorithm is to ﬁnd ⇡⇤2 ⇧as the solution of the following optimization
50"
PRELIMINARY,0.12121212121212122,"problem: max⇡2⇧E⌧⇠⇢0,⇡,T [PH"
PRELIMINARY,0.12354312354312354,"t=1 γtrt] where the expectation is taken over the trajectory ⌧=
51"
PRELIMINARY,0.1258741258741259,"(s1, a1, r1, . . . , sH, aH, rH) generated by policy ⇡under the environment T , starting from s0 ⇠⇢0.
52"
PRELIMINARY,0.1282051282051282,"INVASE
INVASE is proposed by [38] to perform instance-wise feature selection to reduce over-
53"
PRELIMINARY,0.13053613053613053,"ﬁtting in predictive models. The learning objective is to minimize the KL-Divergence of the full-
54"
PRELIMINARY,0.13286713286713286,"conditional distribution and the minimal-selected-features-only conditional distribution of the out-
55"
PRELIMINARY,0.1351981351981352,"come, i.e., minF L, with
56"
PRELIMINARY,0.13752913752913754,"L = DKL(p(Y |X = x)||p(Y |X(F (x)) = x(F (x)))) + λ|F(x)|0.
(1)"
PRELIMINARY,0.13986013986013987,"where F : X ! {0, 1}d is a feature selection function and |F(x)|0 denotes the cardinality (l0
57"
PRELIMINARY,0.14219114219114218,"norm) of selected features, i.e., the number of 1’s in F(x). 1 d is the dimension of input features.
58"
PRELIMINARY,0.1445221445221445,"1To avoid confusion between state notion s 2 S and the selector notion S used in [38], F is used in this
work to represent the selector (i.e., mask generator)."
PRELIMINARY,0.14685314685314685,"x(F (x)) = F(x) ⊙x denotes the element-wise product of x and generated mask m = F(x).
59"
PRELIMINARY,0.14918414918414918,"Ideally, the optimal selection function F should be able to minimize the two terms in Equation (1)
60"
PRELIMINARY,0.15151515151515152,"simultaneously.
61"
PRELIMINARY,0.15384615384615385,"INVASE applies the Actor-Critic framework in the optimization of F through sampling, where
62"
PRELIMINARY,0.1561771561771562,"f✓(·|x), parameterized by a neural network ✓2, is used as a stochastic actor. Two predictive networks
63"
PRELIMINARY,0.1585081585081585,"Cφ(·), B (·) are considered as the critic and the baseline network used for variance reduction [36]
64"
PRELIMINARY,0.16083916083916083,"and trained with the Cross-Entropy loss to produce return signal L, based on which f✓(·|x) can be
65"
PRELIMINARY,0.16317016317016317,"optimized through policy gradient:
66"
PRELIMINARY,0.1655011655011655,"E(x,y)⇠p[Em⇠f✓(·|x)[Lr✓log f✓(·|x)]].
(2)"
PRELIMINARY,0.16783216783216784,"Finally, F(x) = (F1(x), ..., Fd(x)) can be get by sampling from f(·|x) = (f1(x), ..., fd(x)), with
67"
PRELIMINARY,0.17016317016317017,Fi(x) =
PRELIMINARY,0.17249417249417248,"⇢1,
w.p.
fi(·|x).
0,
w.p.
1 −fi(·|x).
(3)"
PROPOSED METHOD,0.17482517482517482,"3
Proposed Method
68"
PROPOSED METHOD,0.17715617715617715,"The objective of this work is to carry out state-wise action selection in RL through intervention,
69"
PROPOSED METHOD,0.1794871794871795,"and thereby enhance the learning efﬁciency with a pruned task-related action space after ﬁnding the
70"
PROPOSED METHOD,0.18181818181818182,"correct causal model. Section 3.1 starts with the formalization of the action space reﬁnery objective
71"
PROPOSED METHOD,0.18414918414918416,"in RL tasks under the framework of causal discovery. Section 3.2 introduces SWAR, which improves
72"
PROPOSED METHOD,0.1864801864801865,"the scalability of INVASE in high dimensional variable selection tasks. We integrate SWAR with
73"
PROPOSED METHOD,0.1888111888111888,"deterministic policy gradient methods [25] in Section 3.3 to perform state-wise action space pruning,
74"
PROPOSED METHOD,0.19114219114219114,"resulting in two practical causality-aware RL algorithms.
75"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.19347319347319347,"3.1
Temporal Difference Objective with Structural Causal Models
76"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.1958041958041958,"In modern RL algorithms, the most general approach is based on the Actor-Critic framework [15],
77"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.19813519813519814,"where the critic Qw(s, a) approximates the return of given state-action pair (s, a) and guides the
78"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.20046620046620048,"Actor to maximize the approximated return at state s. The Critic is optimized to reduce Temporal
79"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.20279720279720279,"Difference (TD) error [29], deﬁned as
80"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.20512820512820512,"LT D = Esi,ai,ri,s0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.20745920745920746,"i⇠B[(ri + γQw(s0 i, a0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2097902097902098,"i) −Qw(si, ai))2].
(4)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.21212121212121213,"where B = (si, ai, ri, s0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.21445221445221446,"i)i=1,2,... is the replay buffer used for off-policy learning [17, 10, 12, 28],
81"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.21678321678321677,and a0
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2191142191142191,i = ⇡(s0
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.22144522144522144,i) is the predicted action for state s0
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.22377622377622378,"i. In practice, the calculations of Qw(s0 i, a0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2261072261072261,"i) are
82"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.22843822843822845,"usually based on another set of slowly updated target networks for stability [10, 12]. Henceforth,
83"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.23076923076923078,"TD-learning can be roughly simpliﬁed as regression with notion yi = ri + γQw(s0 i, a0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2331002331002331,"i):
84"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.23543123543123542,"LT D = Esi,ai,ri,s0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.23776223776223776,"i⇠B[(yi −Qw(si, ai))2].
(5)
Assume there are only M < L actions are related to a speciﬁc task among the L-dimensional
85"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2400932400932401,actions ai = a(1)
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.24242424242424243,"i , ..., a(L)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.24475524475524477,"i
, i.e., Qw(·, ·) is function of si, a(1)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.24708624708624707,"i , ..., a(M)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2494172494172494,"i
. Learning with the primal
86"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2517482517482518,redundant action space will lead to around L+|S|
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2540792540792541,"M+|S| times sample complexity [9, 39]. Therefore, we are
87"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2564102564102564,"motivated to improve the learning efﬁciency of Q by pruning those task-irrelevant action dimensions
88"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.25874125874125875,a(M+1)
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.26107226107226106,"i
, ..., a(L)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2634032634032634,"i
by ﬁnding an action selection function G, satisfying
89"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.26573426573426573,"min
G,Qw Esi,ai,ri,s0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2680652680652681,i⇠B[(y0
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2703962703962704,"i −Qw(si, a(G(ai|si))"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2727272727272727,"i
))2] + λ|G(ai|si)|0.
(6)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.27505827505827507,where y0
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2773892773892774,"i = ri + γQw(s0 i, a 0G(a0"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.27972027972027974,"i|si)
i
).
90"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.28205128205128205,"Such a problem can be addressed from the perspective of causal discovery. Formally, we can use
91"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.28438228438228436,"the Structural Causal Models (SCMs) to represent the underlying causal structure of a sequential
92"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2867132867132867,"decision making process, as shown in Figure 2. Under this language, we use the notion of causal
93"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.289044289044289,"actions to denote a(1,...,M)"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2913752913752914,"i
, and nuisance actions for other dimension of actions. In our work, we use
94"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.2937062937062937,"IC-INVASE for causal discovery. Ideally, the action selection function G should be able to distinguish
95"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.29603729603729606,"between nuisance action dimensions and the causal ones that has causal relation with either dynamics
96"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.29836829836829837,"or reward mechanism. We present in the next section our causal discovery algorithms.
97"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.3006993006993007,"2In this work, subscripts φ,  , ✓, w are used to denote the parameter of neural networks."
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.30303030303030304,"Figure 2: SCM of temporal difference learning. Among all executable actions, there can be only
a subset have effect on the dynamical changes or the reward mechanism. In our work, we use
IC-INVASE as a causal discovery tool to distinguish the causal irrelevant actions and hence improve
learning efﬁciency."
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.30536130536130535,"3.2
Iterative Curriculum INVASE (IC-INVASE)
98"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.3076923076923077,"Instead of directly applying INVASE to solve Equation (6). We ﬁrst propose two improvements
99"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.31002331002331,"to make the vanilla INVASE more suitable for large-scale variable selection tasks as the action
100"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.3123543123543124,"dimension in RL might be extremely large [34]. Speciﬁcally, the ﬁrst improvement, based on
101"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.3146853146853147,"curriculum learning, is introduced to tackle the exploration difﬁculty when λ in Equation (1) is large,
102"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.317016317016317,"where INVASE tends to converge to poor sub-optimal solutions and prune all variables including the
103"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.31934731934731936,"useful ones [38]. The second improvement is based on the iterative structure of variable selection
104"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.32167832167832167,"tasks: the feature selection operator G can be applied multiple times to conduct hierarchical feature
105"
TEMPORAL DIFFERENCE OBJECTIVE WITH STRUCTURAL CAUSAL MODELS,0.32400932400932403,"selection without introducing extra computation expenses.
106"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.32634032634032634,"3.2.1
Curriculum Learning For High Dimensional Variable Selection
107"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.32867132867132864,"The work of [3] ﬁrst introduces Curriculum Learning to mimic human learning by gradually learn
108"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.331002331002331,"more complex concepts or handle more difﬁcult tasks. Effectiveness of the method has been
109"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3333333333333333,"demonstrated in various set-ups [3, 19, 7, 35, 37]. In general, it should be easier to select M useful
110"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3356643356643357,"variables out of L input variables when M is larger. The most trivial case is to select all L variables,
111"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.337995337995338,"with an identical mapping x(G(x)) = G(x) ⊙x = x. Formally, we have
112"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.34032634032634035,"Proposition 1 (Curriculum Property in Variable Selection). Assume M out of L variables are
113"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.34265734265734266,"outcome-related, let M N1 < N2 L, GN1(x) minimizes DKL(p(Y |X = x)||p(Y |X(G(x)) =
114"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.34498834498834496,"x(G(x)))) + λ||G(x)|0 −N1|. Then
115"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3473193473193473,"GN2(x) minimizes DKL(p(Y |X = x)||p(Y |XG(x) = xG(x)))+λ||G(x)|0−N2| can be get through:
116"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.34965034965034963,"GN2(x) 2 {GN1(x) _ [GN1(x)XOR 1]1N2−N1 },
117"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.351981351981352,"where [·]1N2−N1 means keep N2 −N1 none-zero elements unchanged while replacing other elements
118"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3543123543123543,"by 0.
119"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.35664335664335667,"Proof. By the deﬁnition of the [·]1N2−N1 operator, ||G(x)|0 −N2| = 0 is minimized. On the other
120"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.358974358974359,"hand, starting from N1 = M, minimizing DKL(p(Y |X = x)||p(Y |X(G(x)) = x(G(x)))) requires
121"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3613053613053613,"all the M outcome-related variables being selected by GN1. Therefore, GN2 also minimizes the
122"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.36363636363636365,"KL-divergence by the independent assumption of the other L −M variables with the outcomes.
123"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.36596736596736595,"The proposition indicates the difﬁculty of selecting N useful out of L variables decreases monotoni-
124"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3682983682983683,"cally as N ≥M increase from M, M + 1, ..., L. In this work, two classes of practical curriculum
125"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3706293706293706,"are designed: 1. curriculum on the l0 penalty coefﬁcient, and 2. curriculum on the proportion of
126"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.372960372960373,"variables to be selected.
127"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3752913752913753,"Curriculum on l0 Penalty Coefﬁcient
In this curriculum design, the penalty coefﬁcient λ in
128"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3776223776223776,"Equation (1) is increased from 0 to a pre-determined number (e.g., 1.0). Increasing the value of λ
129"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.37995337995337997,"will lead to a larger penalty on the number of variables selected by the feature selector. Experiments
130"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3822843822843823,"in [38] has shown a large λ always lead to a trivial selector that does not select any variable.
131"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.38461538461538464,"Curriculum on the Proportion of Selected Features
In this curriculum design, the proportion of
132"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.38694638694638694,"variables to be selected, denoted by pr, is adjusted from the default setting 0 to a decreasing number
133"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.38927738927738925,Algorithm 1 TD3 with TD-SWAR
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3916083916083916,"Initialize critic networks Cφ1, Cφ2, baseline networks B 1, B 2 and actor network ⇡⌫, IC-INVASE
selector network G✓
Initialize target networks φ0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3939393939393939,"1  φ1, φ0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3962703962703963,"2  φ2,  0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.3986013986013986,"1   1,  0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.40093240093240096,"2   2, ⌫0  ⌫
Initialize replay buffer B
for t = 1, H do"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.40326340326340326,"Interact with environment and store transition tuple (s, a, r, s0) in B
Sample mini-batch of transitions {(s, a, r, s0)} from B
Calculate perturbed next action by ˜a  ⇡⌫0(s0) + ✏, ✏is sampled from a clipped Gaussian.
Select actions with target selector network"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.40559440559440557,"˜a(G(˜a|s0))  G✓0(˜a|s0) ⊙˜a
Calculate target critic value yc and baseline value yb:"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.40792540792540793,"yc  r + γ mini=1,2 Cφ0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.41025641025641024,"i(s0, ˜a(G(˜a|s0)))
yb  r + γ mini=1,2 B 0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4125874125874126,"i(s0, ˜a)
Update critics and baselines with selected actions:"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4149184149184149,"a(G(a|s))  G✓(a|s0) ⊙a
φi  arg minφi MSE(yc, Cφi(s, a(G(a|s))))
 i  arg min i MSE(yb, B i(s, a))
Update IC-INVASE selector network by the policy gradient, with learning rate ⌘1:"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4172494172494173,"✓ ✓−⌘1(lb −lc)r✓log G✓(a|s), lb, lc are MSE"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4195804195804196,"losses in the previous step.
Update ⌫by the deterministic policy gradient, with learning rate ⌘2:"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4219114219114219,"⌫ ⌫−⌘2raCφ1(s, a)|a=⇡⌫(s)r⌫⇡⌫(s)
Update target networks, with ⌧2 (0, 1): φ0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.42424242424242425,"i  ⌧φi + (1 −⌧)φ0 i
 0"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.42657342657342656,i  ⌧ i + (1 −⌧) 0
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4289044289044289,"i
⌫0  ⌧⌫+ (1 −⌧)⌫0
end for"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.43123543123543123,"from a pre-determined value (e.g., 0.5) to 0. i.e., the l0 penalty term λ|G(x)|0 in Equation (1) is
134"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.43356643356643354,"revised to be λ||G(x)|0 −d · pr|, where d is the dimension of input x. When the proportion is set
135"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4358974358974359,"to be pr = 0.5, the selector will be penalized whenever less or more than half of all variables are
136"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4382284382284382,"selected. Such a curriculum design forces the feature selector to learn to select less but increasingly
137"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4405594405594406,"more important variables gradually.
138"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.4428904428904429,"Thus, we get the learning objective of curriculum-INVASE:
139"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.44522144522144524,"L = DKL(p(Y |X = x)||p(Y |X(G(x)) = x(G(x)))) + λ||G(x)|0 −d · pr|.
(7)"
CURRICULUM LEARNING FOR HIGH DIMENSIONAL VARIABLE SELECTION,0.44755244755244755,"where λ increases from 0 to some value and pr decreases from a value in [0, 1) to 0.
140"
ITERATIVE VARIABLE SELECTION,0.44988344988344986,"3.2.2
Iterative Variable Selection
141"
ITERATIVE VARIABLE SELECTION,0.4522144522144522,"The second improvement proposed in this work is based on the iterative structure of variable selection
142"
ITERATIVE VARIABLE SELECTION,0.45454545454545453,"tasks. Speciﬁcally, the G(x) mapping x 2 X to {0, 1}d is an iterative operator, which can be applied
143"
ITERATIVE VARIABLE SELECTION,0.4568764568764569,"for multiple times to perform coarse-to-ﬁne variable selection. Although in practice we follow [38]
144"
ITERATIVE VARIABLE SELECTION,0.4592074592074592,"to apply an element-wise product in producing x(G(x)): x(G(x)) = G(x) ⊙x 2 X. In more general
145"
ITERATIVE VARIABLE SELECTION,0.46153846153846156,"cases, the i-th element of x(G(x))"
ITERATIVE VARIABLE SELECTION,0.46386946386946387,"i
is
146"
ITERATIVE VARIABLE SELECTION,0.4662004662004662,"x(G(x)) i
="
ITERATIVE VARIABLE SELECTION,0.46853146853146854,"⇢1,
if
Gi(x) = 1.
⇤,
if
Gi(x) = 0.
(8)"
ITERATIVE VARIABLE SELECTION,0.47086247086247085,"where ⇤can be an arbitrary identiﬁable indicator that represents the variable is not selected.
147"
ITERATIVE VARIABLE SELECTION,0.4731934731934732,"On the other hand, once the outputs G(x) of the selector have been recorded, ⇤can be replaced by
148"
ITERATIVE VARIABLE SELECTION,0.4755244755244755,"any label-independent variable G(x) ⊙z, where z ⇠pz(·) is outcome-independent. Then x(G(x))
149"
ITERATIVE VARIABLE SELECTION,0.47785547785547783,"can be regarded as a new sample and be fed into the variable selector, resulting in a hierarchical
150"
ITERATIVE VARIABLE SELECTION,0.4801864801864802,"variable selection process:
151"
ITERATIVE VARIABLE SELECTION,0.4825174825174825,"x(1) = (G(x) ⊙x) ⊕(G(x) ⊙z),
x(2) = (G(x(1)) ⊙x(1)) ⊕(G(x(1)) ⊙z),
...
x(n) = (G(x(n−1)) ⊙x(n−1)) ⊕(G(x(n−1)) ⊙z), (9)"
ITERATIVE VARIABLE SELECTION,0.48484848484848486,"where z ⇠pz(·), and ⊕is the element-wise sum operator. Moreover, if the distribution of irrelevant
152"
ITERATIVE VARIABLE SELECTION,0.48717948717948717,"variable px(·) is known, applying the variable selection operator obtained from Equation (7) for
153"
ITERATIVE VARIABLE SELECTION,0.48951048951048953,multiple times with pz(·)
ITERATIVE VARIABLE SELECTION,0.49184149184149184,"d= px(·) has the meaning of hierarchical variable selection: after each
154"
ITERATIVE VARIABLE SELECTION,0.49417249417249415,"operation, the most obvious 1−pr irrelevant variables are discarded. e.g., when pr = 0.5, ideally top-
155"
ITERATIVE VARIABLE SELECTION,0.4965034965034965,"50%, 25%, 12.5% most important variables will be selected after the ﬁrst three selection operations.
156"
ITERATIVE VARIABLE SELECTION,0.4988344988344988,"In this work, a coarse approximation is utilized by selecting z to be z = 0 for simplicity. 3
157"
ITERATIVE VARIABLE SELECTION,0.5011655011655012,"Combining those two improvements lead to an Iterative Curriculum version of INVASE (IC-INVASE)
158"
ITERATIVE VARIABLE SELECTION,0.5034965034965035,"that addresses the exploration difﬁculty in high-dimensional variable selection tasks. Curriculum
159"
ITERATIVE VARIABLE SELECTION,0.5058275058275058,"learning helps IC-INVASE to achieve better asymptotic performance, i.e., achieve higher True Positive
160"
ITERATIVE VARIABLE SELECTION,0.5081585081585082,"Rate (TPR) and lower False Discovery Rate (FDR), while iterative application of the selection operator
161"
ITERATIVE VARIABLE SELECTION,0.5104895104895105,"contributes to higher learning efﬁciency: selectors models with different level of TPR/FDR can be
162"
ITERATIVE VARIABLE SELECTION,0.5128205128205128,"generated on-the-ﬂy.
163"
ITERATIVE VARIABLE SELECTION,0.5151515151515151,"3.3
State-Wise Action Reﬁnery with IC-INVASE
164"
ITERATIVE VARIABLE SELECTION,0.5174825174825175,"3.3.1
Temporal Difference State-Wise Action Reﬁnery
165"
ITERATIVE VARIABLE SELECTION,0.5198135198135199,"With the techniques introduced in the previous section, higher dimensional variable selection tasks
166"
ITERATIVE VARIABLE SELECTION,0.5221445221445221,"can be better solved, therefore we are ready to use IC-INVASE to solve Equation (6). The resulting
167"
ITERATIVE VARIABLE SELECTION,0.5244755244755245,"algorithm is called Temporal Difference State-Wise Action Reﬁnery (TD-SWAR).
168"
ITERATIVE VARIABLE SELECTION,0.5268065268065268,"In this work, TD3 [10] is used as the basic algorithm we build TD-SWAR up on. In addition to the
169"
ITERATIVE VARIABLE SELECTION,0.5291375291375291,"policy network ⇡⌫, double critic networks Cφ1, Cφ2 and their corresponding target networks used
170"
ITERATIVE VARIABLE SELECTION,0.5314685314685315,"in vanilla TD3, TD-SWAR includes an action selector model G✓and two baseline networks B 1,
171"
ITERATIVE VARIABLE SELECTION,0.5337995337995338,"B 2 following [38] to reduce the variance in policy gradient learning. Pseudo-code for the proposed
172"
ITERATIVE VARIABLE SELECTION,0.5361305361305362,"algorithm is shown in Algorithm 1. And the block diagram in Figure 1 illustrates how different
173"
ITERATIVE VARIABLE SELECTION,0.5384615384615384,"modules in TD-SWAR updates their parameters.
174"
ITERATIVE VARIABLE SELECTION,0.5407925407925408,"3.3.2
Static Approximation: Model-Based Action Selection
175"
ITERATIVE VARIABLE SELECTION,0.5431235431235432,"While IC-INVASE can be formally integrated with temporal difference learning, the learning stability
176"
ITERATIVE VARIABLE SELECTION,0.5454545454545454,"is not guaranteed. Different from general regression tasks where the label for every instance is ﬁxed
177"
ITERATIVE VARIABLE SELECTION,0.5477855477855478,"across training, in temporal difference learning, the regression target is closely related to the present
178"
ITERATIVE VARIABLE SELECTION,0.5501165501165501,"critic function Cφ, the policy ⇡⌫that generates the transition tuples used for training, and the selector
179"
ITERATIVE VARIABLE SELECTION,0.5524475524475524,"model of IC-INVASE itself. In this section, a static approach is proposed to approximately solve the
180"
ITERATIVE VARIABLE SELECTION,0.5547785547785548,"challenge of instability in TD-SWAR 4.
181"
ITERATIVE VARIABLE SELECTION,0.5571095571095571,"Other than applying the IC-INVASE algorithm to solve Equation (6), another way of leveraging
182"
ITERATIVE VARIABLE SELECTION,0.5594405594405595,"IC-INVASE in action space pruning is to combine it with the model-based methods [11, 16, 13, 14],
183"
ITERATIVE VARIABLE SELECTION,0.5617715617715617,"where a dynamic model P : S ⇥A 7! S is learned through regression:
184"
ITERATIVE VARIABLE SELECTION,0.5641025641025641,P = arg min
ITERATIVE VARIABLE SELECTION,0.5664335664335665,"P E(s,a,s0)⇠⇡,T (s0 −P(s, a))2
(10)"
ITERATIVE VARIABLE SELECTION,0.5687645687645687,"Although the task of precise model-based prediction is in general challenging [24], in this work, we
185"
ITERATIVE VARIABLE SELECTION,0.5710955710955711,"only adopt model-based prediction in action selection, and the target is action discovery other than
186"
ITERATIVE VARIABLE SELECTION,0.5734265734265734,"precise prediction. As the dynamic models are always static across learning, such an approach can be
187"
ITERATIVE VARIABLE SELECTION,0.5757575757575758,"much more stable than TD-SWAR. We name this method as Dyn-SWAR and present the pseudo-code
188"
ITERATIVE VARIABLE SELECTION,0.578088578088578,"in Algorithm 2, where we infuse IC-INVASE to Equation (10) and get the learning objective:
189 min"
ITERATIVE VARIABLE SELECTION,0.5804195804195804,"G,P E(s,a,s0)⇠⇡,T (s0 −P(s, a(G(a|s))))2
(11)"
ITERATIVE VARIABLE SELECTION,0.5827505827505828,"3pz(·) may be learned through generative models to approximate px(·), and Equation (9) can be regarded as
a kind of data-augmentation or ensemble method. This idea is left for the future work."
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.585081585081585,4Analysis on the approximation is provided in Appendix A
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5874125874125874,Algorithm 2 TD3 with Dyn-SWAR
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5897435897435898,"Initialize critic networks Qw1, Qw2, Dynamics critic model Cφ, dynamic baseline model B , actor
network ⇡⌫, and IC-INVASE selector network G✓
Initialize target networks w0"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5920745920745921,"1  w1, w0"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5944055944055944,"2  w2, ⌫0  ⌫
Initialize replay buffer B
for t = 1, H do"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5967365967365967,"Interact with environment and store transition tuple (s, a, r, s0) in B
Sample mini-batch of transitions {(s, a, r, s0)} from B
Update dynamic critics and dynamic baselines with equation (10):"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.5990675990675991,"φ  arg minφ MSE(s0, Cφ(s, a(G(a|s))))
  arg min MSE(s0, B (s, a))
Update IC-INVASE selector network by the policy gradient, with learning rate ⌘1:"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6013986013986014,"✓ ✓−⌘1(lb −lc)r✓log G✓(a|s), lb, lc are MSE"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6037296037296037,"losses in the previous step.
Calculate perturbed next action by ˜a  ⇡⌫0(s0) + ✏, ✏is sampled from a clipped Gaussian.
Select actions with selector network"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6060606060606061,"˜a(G(˜a|s0))  G✓0(˜a|s0) ⊙˜a
Calculate target critic value y and update critic networks:"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6083916083916084,"y  r + γ mini=1,2 Qw0"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6107226107226107,"i(s0, ˜a(G(˜a|s0)))
wi  arg minwi MSE(y, Qwi(s, a(G(a|s))))
Update ⌫by the deterministic policy gradient, with learning rate ⌘2:"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6130536130536131,"⌫ ⌫−⌘2raQw1(s, a)|a=⇡⌫(s)r⌫⇡⌫(s)
Update target networks, with ⌧2 (0, 1): w0"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6153846153846154,i  ⌧wi + (1 −⌧)w0
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6177156177156177,"i
⌫0  ⌧⌫+ (1 −⌧)⌫0
end for +10 +10 +10 +10"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.62004662004662,"(a) 4Rew.-Maze
(b) Pendulum
(c) Walker2d
(d) LunarLander
(e) BipedalWalker"
ANALYSIS ON THE APPROXIMATION IS PROVIDED IN APPENDIX A,0.6223776223776224,Figure 3: Environments used in experiments
EXPERIMENT,0.6247086247086248,"4
Experiment
190"
EXPERIMENT,0.627039627039627,"In this section, we apply our proposed methodologies to ﬁve continuous control RL tasks characterized
191"
EXPERIMENT,0.6293706293706294,"by redundant action spaces, wherein our methods facilitate causality-aware RL. We also present a
192"
EXPERIMENT,0.6317016317016317,"quantitative comparison between IC-INVASE and the standard INVASE on synthetic datasets in
193"
EXPERIMENT,0.634032634032634,"Appendix B, which serves to underscore the enhanced scalability of our approach.
194"
EXPERIMENT,0.6363636363636364,"In the present set of experiments, we employed ﬁve RL environments (Figure 5), detailed in Table
195"
EXPERIMENT,0.6386946386946387,"1 5. The symbol |S| designates the dimension of the state space for each task, while |A| signiﬁes
196"
EXPERIMENT,0.6410256410256411,"the dimension of the action space relevant to the task, and |Ared.| represents the dimension of the
197"
EXPERIMENT,0.6433566433566433,"redundant action space incorporated into each task. These surplus dimensions of actions don’t impact
198"
EXPERIMENT,0.6456876456876457,"state transitions or reward calculations, but it is essential for an agent to identify these redundant
199"
EXPERIMENT,0.6480186480186481,"dimensions for efﬁcient learning.
200"
EXPERIMENT,0.6503496503496503,"We assessed both TD-SWAR, which combines IC-INVASE with temporal difference learning, and
201"
EXPERIMENT,0.6526806526806527,"its static counterpart, Dyn-SWAR, which employs IC-INVASE in dynamics prediction. The results
202"
EXPERIMENT,0.655011655011655,"are benchmarked against two base conditions: the Oracle, where redundant action dimensions are
203"
EXPERIMENT,0.6573426573426573,"5For comprehensive descriptions of the environments, please consult Appendix C"
EXPERIMENT,0.6596736596736597,Table 1: Tasks used in evaluating SWAR in temporal difference learning
EXPERIMENT,0.662004662004662,"TASK/DIMENSION
|S|
|A|
|Ared.|"
EXPERIMENT,0.6643356643356644,"PENDULUM-V0
3
1
100
FOURREWARDMAZE
2
2
100
LUNARLANDERCONTINUOUS-V2
8
2
100
BIPEDALWALKER-V3
24
4
100
WALKER2D-V2
17
6
100"
EXPERIMENT,0.6666666666666666,"(a) FourRewardMaze
(b) Pendulum
(c) Walker2d"
EXPERIMENT,0.668997668997669,"(d) LunarLander
(e) BipedalWalker"
EXPERIMENT,0.6713286713286714,"Figure 4: Performance of agents in ﬁve different environments. The curves shows averaged learning
progress and the shaded areas show standard deviation."
EXPERIMENT,0.6736596736596736,"manually removed; and TD3, which is the standard TD3 algorithm devoid of any explicit action
204"
EXPERIMENT,0.675990675990676,"redundancy reduction.
205"
EXPERIMENT,0.6783216783216783,"In our experimental ﬁndings, we observed that Dyn-SWAR’s deployment demonstrates superior
206"
EXPERIMENT,0.6806526806526807,"efﬁciency with respect to both sample complexity and computational cost. In contrast, TD-SWAR
207"
EXPERIMENT,0.682983682983683,"requires a persistent update of all parameters for the IC-INVASE selector to maintain congruence with
208"
EXPERIMENT,0.6853146853146853,"the real-time policy and value networks, given the ﬂuctuating regression label over time. However,
209"
EXPERIMENT,0.6876456876456877,"the Dyn-SWAR selector necessitates a signiﬁcantly reduced data set for training, speciﬁcally between
210"
EXPERIMENT,0.6899766899766899,"10,000 and 25,000 timesteps of environmental interaction. This attribute can seamlessly integrate
211"
EXPERIMENT,0.6923076923076923,"with the warm-up technique utilized in TD3 [10]. Namely, the Dyn-SWAR selector could be trained
212"
EXPERIMENT,0.6946386946386947,"with warm-up transition tuples gathered during the random exploration phase, and then remain static
213"
EXPERIMENT,0.696969696969697,"throughout the subsequent learning process. Compared to traditional RL conﬁgurations that generally
214"
EXPERIMENT,0.6993006993006993,"require millions of environmental interactions, the training of Dyn-SWAR incurs only a minuscule
215"
EXPERIMENT,0.7016317016317016,"computational cost.
216"
EXPERIMENT,0.703962703962704,"These ﬁndings are illustrated in Figure 4. Across all environments, agent learning with IC-INVASE
217"
EXPERIMENT,0.7062937062937062,"in both TD- and Dyn- methods exceeds the performance of the standard TD3 baseline. Dyn-SWAR
218"
EXPERIMENT,0.7086247086247086,"achieves a learning efﬁciency that is on par with oracle benchmarks. However, the performance of
219"
EXPERIMENT,0.710955710955711,"TD-SWAR in tasks of higher dimensions (Walker2d-v2 and BipedalWalker-v3) indicates signiﬁcant
220"
EXPERIMENT,0.7132867132867133,"potential for enhancement. Accordingly, future work should prioritize enhancing the stability and
221"
EXPERIMENT,0.7156177156177156,"scalability of instance-wise variable selection within temporal difference learning.
222"
RELATED WORK,0.717948717948718,"5
Related Work
223"
RELATED WORK,0.7202797202797203,"Instance-Wise Feature Selection
While traditional feature selection method like LASSO [31]
224"
RELATED WORK,0.7226107226107226,"aims at ﬁnding globally important features across the whole dataset, instance-wise feature selection
225"
RELATED WORK,0.7249417249417249,"try to discover the feature-label dependency on a case-by-case basis. L2X [5] performs instance-
226"
RELATED WORK,0.7272727272727273,"wise feature selection through mutual information maximization with the technique of Gumbel
227"
RELATED WORK,0.7296037296037297,"softmax. L2X requires pre-determined hyper-parameter k to indicate how many features should be
228"
RELATED WORK,0.7319347319347319,"selected for each instance, which limits its performance while the number of label-relevant features
229"
RELATED WORK,0.7342657342657343,"varies across instances. In this work, we build our instance-wise action selection model on top of
230"
RELATED WORK,0.7365967365967366,"INVASE [38], where policy gradient is applied to replace the Gumbel softmax trick and the size of
231"
RELATED WORK,0.7389277389277389,"chosen features per instance is more ﬂexible. [32] considers instance-wise feature selection problems
232"
RELATED WORK,0.7412587412587412,"in time-series setting, and build generative models to capture counterfactual effects in time series
233"
RELATED WORK,0.7435897435897436,"data. Their work enables evaluation of the importance of features over time, which is crucial in the
234"
RELATED WORK,0.745920745920746,"context of healthcare. [18] formally deﬁnes different types of feature redundancy and leverages
235"
RELATED WORK,0.7482517482517482,"mutual information maximization in instance-wise feature group discovery and introduces theoretical
236"
RELATED WORK,0.7505827505827506,"guidance to ﬁnd the optimal number of different groups.
237"
RELATED WORK,0.752913752913753,"Our work is distinguished from previous works for instance-wise feature selection in two aspects.
238"
RELATED WORK,0.7552447552447552,"First, while previous works focus on static scenarios like classiﬁcation and regression, this work focus
239"
RELATED WORK,0.7575757575757576,"on temporal difference learning where there is no static label. Second, the scalability of previous
240"
RELATED WORK,0.7599067599067599,"methods in variable selection is challenged as there might exist hundreds of redundant actions in the
241"
RELATED WORK,0.7622377622377622,"context of RL.
242"
RELATED WORK,0.7645687645687645,"Dimension Reduction in RL
In the context of RL, attention models [33] have been applied to
243"
RELATED WORK,0.7668997668997669,"interpret the behaviors of learned policies. [30] proposes to perceive the state information through a
244"
RELATED WORK,0.7692307692307693,"self-attention bottleneck in vision-based RL tasks, which concentrates on the state space redundancy
245"
RELATED WORK,0.7715617715617715,"reduction with image inputs. The work of [22] also applies the attention mechanism to learn task-
246"
RELATED WORK,0.7738927738927739,"relevant information. The proposed method achieves state-of-the-art performance on Atari games
247"
RELATED WORK,0.7762237762237763,"with image input while being more understandable with top-down attention models.
248"
RELATED WORK,0.7785547785547785,"Different from those papers, this work considers relatively tight state representations (vector input),
249"
RELATED WORK,0.7808857808857809,"and focuses on the task-irrelevant action reduction. We aim at ﬁnding the task-related actions and
250"
RELATED WORK,0.7832167832167832,"improving the learning efﬁciency without wasting samples in learning the task-irrelevant dimensions
251"
RELATED WORK,0.7855477855477856,"of actions. Our work is most closely related to AE-DQN [39] in that we both consider the problem of
252"
RELATED WORK,0.7878787878787878,"redundant action elimination. AE-DQN tackles action space redundancy with an action-elimination
253"
RELATED WORK,0.7902097902097902,"network that eliminates sub-optimal actions. Yet its discussion is limited in the discrete settings. In
254"
RELATED WORK,0.7925407925407926,"contrast, our work focuses on action elimination in continuous control tasks.
255"
CONCLUSION AND FUTURE WORK,0.7948717948717948,"6
Conclusion and Future Work
256"
CONCLUSION AND FUTURE WORK,0.7972027972027972,"In this study, we address the issue of pruning the action space in action redundant RL tasks. We employ
257"
CONCLUSION AND FUTURE WORK,0.7995337995337995,"the recent advancements in instance-wise feature selection technology (INVASE), incorporating both
258"
CONCLUSION AND FUTURE WORK,0.8018648018648019,"curriculum learning and iterative processes, to aim for improved scalability and efﬁciency. This
259"
CONCLUSION AND FUTURE WORK,0.8041958041958042,"leads to the creation of the IC-INVASE method, which is then adapted to the RL environment where
260"
CONCLUSION AND FUTURE WORK,0.8065268065268065,"we introduce two novel algorithms, TD-SWAR and Dyn-SWAR, to implement causality-conscious
261"
CONCLUSION AND FUTURE WORK,0.8088578088578089,"RL. The former algorithm directly addresses the issue of action redundancy in temporal difference
262"
CONCLUSION AND FUTURE WORK,0.8111888111888111,"learning, whereas the latter algorithm leverages model-based prediction to capture dynamic causality.
263"
CONCLUSION AND FUTURE WORK,0.8135198135198135,"Experimental evidence from a range of tasks underscores the importance of causality-awareness for
264"
CONCLUSION AND FUTURE WORK,0.8158508158508159,"RL agents to achieve efﬁcient learning in action-redundant settings.
265"
CONCLUSION AND FUTURE WORK,0.8181818181818182,"As for future research, the iterative characteristic of this method could be further investigated to
266"
CONCLUSION AND FUTURE WORK,0.8205128205128205,"apply ensemble methods in variable selection. Additionally, the design of a more appropriate
267"
CONCLUSION AND FUTURE WORK,0.8228438228438228,"curriculum could enhance the fusion of multiple curricula. From the RL perspective, the stability of
268"
CONCLUSION AND FUTURE WORK,0.8251748251748252,"TD-SWAR could be further optimized to enhance sample efﬁciency. The design of the curriculum
269"
CONCLUSION AND FUTURE WORK,0.8275058275058275,"could potentially offer beneﬁts. For instance, an agent might initially learn to identify actions of
270"
CONCLUSION AND FUTURE WORK,0.8298368298368298,"general importance before concentrating on discerning state-dependent crucial actions. Furthermore,
271"
CONCLUSION AND FUTURE WORK,0.8321678321678322,"the selection process can be extended to include both the state space and action space, allowing for
272"
CONCLUSION AND FUTURE WORK,0.8344988344988346,"efﬁcient temporal difference learning that is mindful of the causal relationships among states, actions,
273"
CONCLUSION AND FUTURE WORK,0.8368298368298368,"and the task at hand. Additionally, model-based prediction could be broadened to anticipate future
274"
CONCLUSION AND FUTURE WORK,0.8391608391608392,"returns.
275"
REFERENCES,0.8414918414918415,"References
276"
REFERENCES,0.8438228438228438,"[1] Martín Abadi, Paul Barham, Jianmin Chen, et al. Tensorﬂow: A system for large-scale machine
277"
REFERENCES,0.8461538461538461,"learning. In OSDI, 2016.
278"
REFERENCES,0.8484848484848485,"[2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, et al. Learning dexterous
279"
REFERENCES,0.8508158508158508,"in-hand manipulation. IJRR, 2020.
280"
REFERENCES,0.8531468531468531,"[3] Yoshua Bengio, Jérôme Louradour, et al. Curriculum learning. In ICML, 2009.
281"
REFERENCES,0.8554778554778555,"[4] Christopher Berner, Greg Brockman, Brooke Chan, et al. Dota 2 with large scale deep reinforce-
282"
REFERENCES,0.8578088578088578,"ment learning. arXiv:1912.06680, 2019.
283"
REFERENCES,0.8601398601398601,"[5] Jianbo Chen, Le Song, et al. Learning to explain: An information-theoretic perspective on model
284"
REFERENCES,0.8624708624708625,"interpretation. arXiv:1802.07814, 2018.
285"
REFERENCES,0.8648018648018648,"[6] François Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
286"
REFERENCES,0.8671328671328671,"[7] Wojciech Marian Czarnecki, Siddhant M Jayakumar, Max Jaderberg, et al. Mix&match-agent
287"
REFERENCES,0.8694638694638694,"curricula for reinforcement learning. arXiv:1806.01780, 2018.
288"
REFERENCES,0.8717948717948718,"[8] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In
289"
REFERENCES,0.8741258741258742,"NeurIPS, pages 11698–11709, 2019.
290"
REFERENCES,0.8764568764568764,"[9] Eyal Even-Dar, Shie M., et al. Action elimination and stopping conditions for the multi-armed
291"
REFERENCES,0.8787878787878788,"bandit and reinforcement learning problems. JMLR, 2006.
292"
REFERENCES,0.8811188811188811,"[10] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error
293"
REFERENCES,0.8834498834498834,"in actor-critic methods. arXiv:1802.09477, 2018.
294"
REFERENCES,0.8857808857808858,"[11] David Ha and Jürgen Schmidhuber. World models. arXiv:1803.10122, 2018.
295"
REFERENCES,0.8881118881118881,"[12] Tuomas Haarnoja, Aurick Zhou, et al. Soft actor-critic: Off-policy maximum entropy deep
296"
REFERENCES,0.8904428904428905,"reinforcement learning with a stochastic actor. arXiv:1801.01290, 2018.
297"
REFERENCES,0.8927738927738927,"[13] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, et al. Dream to control: Learning behaviors by
298"
REFERENCES,0.8951048951048951,"latent imagination. arXiv:1912.01603, 2019.
299"
REFERENCES,0.8974358974358975,"[14] Michael Janner, Justin Fu, Marvin Zhang, et al. When to trust your model: Model-based policy
300"
REFERENCES,0.8997668997668997,"optimization. In NeurIPS, 2019.
301"
REFERENCES,0.9020979020979021,"[15] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In NeurIPS, 2000.
302"
REFERENCES,0.9044289044289044,"[16] Eric Langlois, Shunshi Zhang, Guodong Zhang, et al. Benchmarking model-based reinforcement
303"
REFERENCES,0.9067599067599068,"learning. arXiv:1907.02057, 2019.
304"
REFERENCES,0.9090909090909091,"[17] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, et al. Continuous control with deep
305"
REFERENCES,0.9114219114219114,"reinforcement learning. arXiv:1509.02971, 2015.
306"
REFERENCES,0.9137529137529138,"[18] Aria Masoomi, Chieh Wu, Tingting Zhao, et al. Instance-wise feature grouping. NeurIPS, 2020.
307"
REFERENCES,0.916083916083916,"[19] Tambet Matiisen, Avital Oliver, et al. Teacher-student curriculum learning. TNNLS, 2019.
308"
REFERENCES,0.9184149184149184,"[20] Andrew Melnik, Luca Lach, Matthias Plappert, et al. Tactile sensing and deep reinforcement
309"
REFERENCES,0.9207459207459208,"learning for in-hand manipulation tasks.
310"
REFERENCES,0.9230769230769231,"[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. Human-level control through deep
311"
REFERENCES,0.9254079254079254,"reinforcement learning. Nature, 2015.
312"
REFERENCES,0.9277389277389277,"[22] Alexander Mott, Daniel Zoran, et al. Towards interpretable reinforcement learning using
313"
REFERENCES,0.9300699300699301,"attention augmented agents. In NeurIPS, 2019.
314"
REFERENCES,0.9324009324009324,"[23] Adam Paszke, Sam Gross, Soumith Chintala, et al. Automatic differentiation in pytorch. 2017.
315"
REFERENCES,0.9347319347319347,"[24] Archit Sharma, Shixiang Gu, Sergey Levine, et al. Dynamics-aware unsupervised discovery of
316"
REFERENCES,0.9370629370629371,"skills. arXiv:1907.01657, 2019.
317"
REFERENCES,0.9393939393939394,"[25] David Silver, Guy Lever, et al. Deterministic policy gradient algorithms. In ICML, 2014.
318"
REFERENCES,0.9417249417249417,"[26] David Silver, Aja Huang, Chris J Maddison, et al. Mastering the game of go with deep neural
319"
REFERENCES,0.9440559440559441,"networks and tree search. nature, 2016.
320"
REFERENCES,0.9463869463869464,"[27] Peng Sun, Xinghai Sun, Lei Han, et al. Tstarbots: Defeating the cheating level builtin ai in
321"
REFERENCES,0.9487179487179487,"starcraft ii in the full game. arXiv:1809.07193, 2018.
322"
REFERENCES,0.951048951048951,"[28] Hao Sun, Ziping Xu, Yuhang Song, Meng Fang, Jiechao Xiong, Bo Dai, and Bolei Zhou.
323"
REFERENCES,0.9533799533799534,"Zeroth-order supervised policy improvement. arXiv preprint arXiv:2006.06600, 2020.
324"
REFERENCES,0.9557109557109557,"[29] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 1998.
325"
REFERENCES,0.958041958041958,"[30] Yujin Tang, Duong Nguyen, and David Ha.
Neuroevolution of self-interpretable agents.
326"
REFERENCES,0.9603729603729604,"arXiv:2003.08165, 2020.
327"
REFERENCES,0.9627039627039627,"[31] Robert Tibshirani. Regression shrinkage and selection via the lasso. JRSS, 1996.
328"
REFERENCES,0.965034965034965,"[32] Sana Tonekaboni, S. Joshi, et al. What went wrong and when? instance-wise feature importance
329"
REFERENCES,0.9673659673659674,"for time-series black-box models. NeurIPS, 2020.
330"
REFERENCES,0.9696969696969697,"[33] Ashish Vaswani, Noam Shazeer, et al. Attention is all you need. In NeurIPS, 2017.
331"
REFERENCES,0.972027972027972,"[34] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, et al. Grandmaster level in starcraft ii
332"
REFERENCES,0.9743589743589743,"using multi-agent reinforcement learning. Nature, 2019.
333"
REFERENCES,0.9766899766899767,"[35] Daphna Weinshall, Gad Cohen, et al. Curriculum learning by transfer learning: Theory and
334"
REFERENCES,0.9790209790209791,"experiments with deep networks. arXiv:1802.03796, 2018.
335"
REFERENCES,0.9813519813519813,"[36] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
336"
REFERENCES,0.9836829836829837,"ment learning. Machine learning, 8(3-4):229–256, 1992.
337"
REFERENCES,0.986013986013986,"[37] Benfeng Xu, L. Zhang, et al. Curriculum learning for natural language understanding. In ACL,
338"
REFERENCES,0.9883449883449883,"2020.
339"
REFERENCES,0.9906759906759907,"[38] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable
340"
REFERENCES,0.993006993006993,"selection using neural networks. In ICLR, 2018.
341"
REFERENCES,0.9953379953379954,"[39] Tom Zahavy, Matan Haroush, et al. Learn what not to learn: Action elimination with deep
342"
REFERENCES,0.9976689976689976,"reinforcement learning. In NeurIPS, 2018.
343"
