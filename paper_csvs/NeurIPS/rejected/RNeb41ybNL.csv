Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005995203836930455,"The Langevin Dynamics framework, which aims to generate samples from the
1"
ABSTRACT,0.001199040767386091,"score function of a probability distribution, is widely used for analyzing and
2"
ABSTRACT,0.0017985611510791368,"interpreting score-based generative modeling. While the convergence behavior of
3"
ABSTRACT,0.002398081534772182,"Langevin Dynamics under unimodal distributions has been extensively studied in
4"
ABSTRACT,0.002997601918465228,"the literature, in practice the data distribution could consist of multiple distinct
5"
ABSTRACT,0.0035971223021582736,"modes. In this work, we investigate Langevin Dynamics in producing samples
6"
ABSTRACT,0.004196642685851319,"from multimodal distributions and theoretically study its mode-seeking properties.
7"
ABSTRACT,0.004796163069544364,"We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is
8"
ABSTRACT,0.00539568345323741,"unlikely to find all mixture components within a sub-exponential number of steps in
9"
ABSTRACT,0.005995203836930456,"the data dimension. To reduce the mode-seeking tendencies of Langevin Dynamics,
10"
ABSTRACT,0.006594724220623501,"we propose Chained Langevin Dynamics, which divides the data vector into patches
11"
ABSTRACT,0.007194244604316547,"of constant size and generates every patch sequentially conditioned on the previous
12"
ABSTRACT,0.0077937649880095924,"patches. We perform a theoretical analysis of Chained Langevin Dynamics by
13"
ABSTRACT,0.008393285371702638,"reducing it to sampling from a constant-dimensional distribution. We present
14"
ABSTRACT,0.008992805755395683,"the results of several numerical experiments on synthetic and real image datasets,
15"
ABSTRACT,0.009592326139088728,"supporting our theoretical results on the iteration complexities of sample generation
16"
ABSTRACT,0.010191846522781775,"from mixture distributions using the chained and vanilla Langevin Dynamics.
17"
INTRODUCTION,0.01079136690647482,"1
Introduction
18"
INTRODUCTION,0.011390887290167866,"A central task in unsupervised learning involves learning the underlying probability distribution of
19"
INTRODUCTION,0.011990407673860911,"training data and efficiently generating new samples from the distribution. Score-based generative
20"
INTRODUCTION,0.012589928057553957,"modeling (SGM) (Song et al., 2020c) has achieved state-of-the-art performance in various learning
21"
INTRODUCTION,0.013189448441247002,"tasks including image generation (Song and Ermon, 2019, 2020; Ho et al., 2020; Song et al., 2020a;
22"
INTRODUCTION,0.013788968824940047,"Ramesh et al., 2022; Rombach et al., 2022), audio synthesis (Chen et al., 2020; Kong et al., 2020),
23"
INTRODUCTION,0.014388489208633094,"and video generation (Ho et al., 2022; Blattmann et al., 2023). In addition to the successful empirical
24"
INTRODUCTION,0.01498800959232614,"results, the convergence analysis of SGM has attracted significant attention in the recent literature
25"
INTRODUCTION,0.015587529976019185,"(Lee et al., 2022, 2023; Chen et al., 2023; Li et al., 2023, 2024).
26"
INTRODUCTION,0.01618705035971223,"Stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011), as a fundamental method-
27"
INTRODUCTION,0.016786570743405275,"ology to implement and interpret SGM, can produce samples from the (Stein) score function of a
28"
INTRODUCTION,0.01738609112709832,"probability density, i.e., the gradient of the log probability density function with respect to data. It
29"
INTRODUCTION,0.017985611510791366,"has been widely recognized that a pitfall of SGLD is its slow mixing rate (Wooddard et al., 2009;
30"
INTRODUCTION,0.01858513189448441,"Raginsky et al., 2017; Lee et al., 2018). Specifically, Song and Ermon (2019) shows that under a
31"
INTRODUCTION,0.019184652278177457,"multi-modal data distribution, the samples from Langevin dynamics may have an incorrect relative
32"
INTRODUCTION,0.019784172661870502,"density across the modes. Based on this finding, Song and Ermon (2019) proposes anneal Langevin
33"
INTRODUCTION,0.02038369304556355,"dynamics, which injects different levels of Gaussian noise into the data distribution and samples with
34"
INTRODUCTION,0.020983213429256596,"SGLD on the perturbed distribution. While outputting the correct relative density across modes can
35"
INTRODUCTION,0.02158273381294964,"be challenging for SGLD, a natural question is whether SGLD would be able to find all the modes of
36"
INTRODUCTION,0.022182254196642687,"a multi-modal distribution.
37"
INTRODUCTION,0.022781774580335732,"In this work, we study this question by analyzing the mode-seeking properties of SGLD. The notion
38"
INTRODUCTION,0.023381294964028777,"of mode-seekingness (Bishop, 2006; Ke et al., 2021; Li and Farnia, 2023) refers to the property that a
39"
INTRODUCTION,0.023980815347721823,"generative model captures only a subset of the modes of a multi-modal distribution. We note that
40"
INTRODUCTION,0.024580335731414868,"a similar problem, known as metastability, has been studied in the context of Langevin diffusion,
41"
INTRODUCTION,0.025179856115107913,"a continuous-time version of SGLD described by stochastic differential equation (SDE) (Bovier
42"
INTRODUCTION,0.02577937649880096,"et al., 2002, 2004; Gayrard et al., 2005). Specifically, Bovier et al. (2002) gave a sharp bound on
43"
INTRODUCTION,0.026378896882494004,"the mean hitting time of Langevin diffusion and proved that it may require exponential (in the space
44"
INTRODUCTION,0.02697841726618705,"dimensionality d) time for transition between modes. Regarding discrete SGLD, Lee et al. (2018)
45"
INTRODUCTION,0.027577937649880094,"constructed a probability distribution whose density is close to a mixture of two well-separated
46"
INTRODUCTION,0.02817745803357314,"isotropic Gaussians, and proved that SGLD could not find one of the two modes within an exponential
47"
INTRODUCTION,0.02877697841726619,"number of steps. However, further exploration of mode-seeking tendencies of SGLD and its variants
48"
INTRODUCTION,0.029376498800959234,"such as annealed Langevin dynamics for general distributions is still lacking in the literature.
49"
INTRODUCTION,0.02997601918465228,"In this work, we theoretically formulate and demonstrate the potential mode-seeking tendency of
50"
INTRODUCTION,0.030575539568345324,"SGLD. We begin by analyzing the convergence under a variety of Gaussian mixture probability
51"
INTRODUCTION,0.03117505995203837,"distributions, under which SGLD could fail to visit all the mixture components within sub-exponential
52"
INTRODUCTION,0.03177458033573141,"steps (in the data dimension). Subsequently, we generalize this result to mixture distributions with
53"
INTRODUCTION,0.03237410071942446,"sub-Gaussian modes. This generalization extends our earlier result on Gaussian mixtures to a
54"
INTRODUCTION,0.03297362110311751,"significantly larger family of mixture models, as the sub-Gaussian family includes any distribution
55"
INTRODUCTION,0.03357314148681055,"over an ℓ2-norm-bounded support set. Furthermore, we extend our theoretical results to anneal
56"
INTRODUCTION,0.0341726618705036,"Langevin dynamics with bounded noise scales.
57"
INTRODUCTION,0.03477218225419664,"To reduce SGLD’s large iteration complexity shown under a high-dimensional input vector, we
58"
INTRODUCTION,0.03537170263788969,"propose Chained Langevin Dynamics (Chained-LD). Since SGLD could suffer from the curse of
59"
INTRODUCTION,0.03597122302158273,"dimensionality, we decompose the sample x ∈Rd into d/Q patches x(1), · · · , x(d/Q), each of
60"
INTRODUCTION,0.03657074340527578,"constant size Q, and sequentially generate every patch x(q) for all q ∈[d/Q] statistically conditioned
61"
INTRODUCTION,0.03717026378896882,"on previous patches, i.e., P(x(q) | x(0), · · · x(q−1)). The combination of all patches generated from
62"
INTRODUCTION,0.03776978417266187,"the conditional distribution faithfully follows the probability density P(x), while learning each patch
63"
INTRODUCTION,0.03836930455635491,"requires less cost due to the reduced dimension. We also provide a theoretical analysis of Chained-LD
64"
INTRODUCTION,0.03896882494004796,"by reducing the convergence of a d-dimensional sample to the convergence of each patch.
65"
INTRODUCTION,0.039568345323741004,"Finally, we present the results of several numerical experiments to validate our theoretical findings.
66"
INTRODUCTION,0.04016786570743405,"For synthetic experiments, we consider moderately high-dimensional Gaussian mixture models,
67"
INTRODUCTION,0.0407673860911271,"where the vanilla and annealed Langevin dynamics could not find all the components within a million
68"
INTRODUCTION,0.04136690647482014,"steps, while Chained-LD could capture all the components with correct frequencies in O(104) steps.
69"
INTRODUCTION,0.04196642685851319,"For experiments on real image datasets, we consider a mixture of two modes by using the original
70"
INTRODUCTION,0.042565947242206234,"images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects)
71"
INTRODUCTION,0.04316546762589928,"as the first mode and constructing the second mode by i.i.d. flipping the images (white background
72"
INTRODUCTION,0.043764988009592325,"and black digits/objects) with probability 0.5. Following from Song and Ermon (2019), we trained
73"
INTRODUCTION,0.04436450839328537,"a Noise Conditional Score Network (NCSN) to estimate the score function. Our numerical results
74"
INTRODUCTION,0.044964028776978415,"indicate that vanilla Langevin dynamics can fail to capture the two modes, as also observed by Song
75"
INTRODUCTION,0.045563549160671464,"and Ermon (2019). On the other hand, Chained-LD was capable of finding both modes regardless of
76"
INTRODUCTION,0.046163069544364506,"initialization. We summarize the contributions of this work as follows:
77"
INTRODUCTION,0.046762589928057555,"• Theoretically studying the mode-seeking properties of vanilla and annealed Langevin dynamics,
78"
INTRODUCTION,0.047362110311750596,"• Proposing Chained Langevin Dynamics (Chained-LD), which decomposes the sample into patches
79"
INTRODUCTION,0.047961630695443645,"and sequentially generates each patch conditioned on previous patches,
80"
INTRODUCTION,0.048561151079136694,"• Providing a theoretical analysis of the convergence behavior of Chained-LD,
81"
INTRODUCTION,0.049160671462829736,"• Numerically comparing the mode-seeking properties of vanilla, annealed, and chained Langevin
82"
INTRODUCTION,0.049760191846522785,"dynamics.
83"
INTRODUCTION,0.050359712230215826,"Notations: We use [n] to denote the set {1, 2, · · · , n}. Also, in the paper, ∥·∥refers to the ℓ2 norm.
84"
INTRODUCTION,0.050959232613908875,"We use 0n and 1n to denote a 0-vector and 1-vector of length n. We use In to denote the identity
85"
INTRODUCTION,0.05155875299760192,"matrix of size n × n. In the text, TV stands for the total variation distance.
86"
RELATED WORKS,0.052158273381294966,"2
Related Works
87"
RELATED WORKS,0.05275779376498801,"Langevin Dynamics: The convergence guarantees for Langevin diffusion, a continuous version of
88"
RELATED WORKS,0.053357314148681056,"Langevin dynamics, are classical results extensively studied in the literature (Bhattacharya, 1978;
89"
RELATED WORKS,0.0539568345323741,"Roberts and Tweedie, 1996; Bakry and Émery, 1983; Bakry et al., 2008). Langevin dynamics, also
90"
RELATED WORKS,0.05455635491606715,"known as Langevin Monte Carlo, is a discretization of Langevin diffusion typically modeled as a
91"
RELATED WORKS,0.05515587529976019,"Markov Chain Monte Carlo (Welling and Teh, 2011). For unimodal distributions, e.g., the probability
92"
RELATED WORKS,0.05575539568345324,"density function that is log-concave or satisfies log-Sobolev inequality, the convergence of Langevin
93"
RELATED WORKS,0.05635491606714628,"dynamics is provably fast (Dalalyan, 2017; Durmus and Moulines, 2017; Vempala and Wibisono,
94"
RELATED WORKS,0.05695443645083933,"2019). However, for multimodal distributions, the non-asymptotic convergence analysis is much more
95"
RELATED WORKS,0.05755395683453238,"challenging (Cheng et al., 2018). Raginsky et al. (2017) gave an upper bound on the convergence time
96"
RELATED WORKS,0.05815347721822542,"of Langevin dynamics for arbitrary non-log-concave distributions with certain regularity assumptions,
97"
RELATED WORKS,0.05875299760191847,"which, however, could be exponentially large without imposing more restrictive assumptions. Lee
98"
RELATED WORKS,0.05935251798561151,"et al. (2018) studied the special case of a mixture of Gaussians of equal variance and provided
99"
RELATED WORKS,0.05995203836930456,"heuristic analysis of sampling from general non-log-concave distributions.
100"
RELATED WORKS,0.0605515587529976,"Mode-Seekingness of Langevin Dynamics: The investigation of the mode-seekingness of gener-
101"
RELATED WORKS,0.06115107913669065,"ative models starts with different generative adversarial network (GAN) (Goodfellow et al., 2014)
102"
RELATED WORKS,0.06175059952038369,"model formulations and divergence measures, from both the practical (Goodfellow, 2016; Poole
103"
RELATED WORKS,0.06235011990407674,"et al., 2016) and theoretical (Shannon et al., 2020; Li and Farnia, 2023) perspectives. In the context
104"
RELATED WORKS,0.06294964028776978,"of Langevin dynamics, mode-seekingness is closely related to a lower bound on the transition time
105"
RELATED WORKS,0.06354916067146282,"between two modes, e.g., two local maximums. Bovier et al. (2002, 2004); Gayrard et al. (2005)
106"
RELATED WORKS,0.06414868105515588,"studied the mean hitting time of the continuous Langevin diffusion. Lee et al. (2018) proved the
107"
RELATED WORKS,0.06474820143884892,"existence of a mixture of two Gaussian distributions whose covariance matrices differ by a constant
108"
RELATED WORKS,0.06534772182254196,"factor, Langevin dynamics cannot find both modes in polynomial time.
109"
RELATED WORKS,0.06594724220623502,"Score-based Generative Modeling:
Since Song et al. (2020b) proposed sliced score matching
110"
RELATED WORKS,0.06654676258992806,"which can train deep models to learn the score functions of implicit probability distributions on high-
111"
RELATED WORKS,0.0671462829736211,"dimensional data, score-based generative modeling (SGM) has been going through a spurt of growth.
112"
RELATED WORKS,0.06774580335731414,"Annealed Langevin dynamics (Song and Ermon, 2019) estimates the noise score of the probability
113"
RELATED WORKS,0.0683453237410072,"density perturbed by Gaussian noise and utilizes stochastic gradient Langevin dynamics to generate
114"
RELATED WORKS,0.06894484412470024,"samples from a sequence of decreasing noise scales. Song and Ermon (2020) conducted a heuristic
115"
RELATED WORKS,0.06954436450839328,"analysis of the effect of noise levels on the performance of annealed Langevin dynamics. Denoising
116"
RELATED WORKS,0.07014388489208633,"diffusion probabilistic model (DDPM) (Ho et al., 2020) incorporates a step-by-step introduction of
117"
RELATED WORKS,0.07074340527577938,"random noise into data, followed by learning to reverse this diffusion process in order to generate
118"
RELATED WORKS,0.07134292565947242,"desired data samples from the noise. Song et al. (2020c) unified anneal Langevin dynamics and
119"
RELATED WORKS,0.07194244604316546,"DDPM via a stochastic differential equation. A recent line of work focuses on the non-asymptotic
120"
RELATED WORKS,0.0725419664268585,"convergence guarantees for SGM with an imperfect score estimation under various assumptions on
121"
RELATED WORKS,0.07314148681055156,"the data distribution (Block et al., 2020; De Bortoli et al., 2021; Lee et al., 2022; Chen et al., 2023;
122"
RELATED WORKS,0.0737410071942446,"Benton et al., 2023; Li et al., 2023, 2024).
123"
PRELIMINARIES,0.07434052757793765,"3
Preliminaries
124"
LANGEVIN DYNAMICS,0.0749400479616307,"3.1
Langevin Dynamics
125"
LANGEVIN DYNAMICS,0.07553956834532374,"Generative modeling aims to produce samples such that their distribution is close to the underlying true
126"
LANGEVIN DYNAMICS,0.07613908872901679,"distribution P. For a continuously differentiable probability density P(x) on Rd, its score function is
127"
LANGEVIN DYNAMICS,0.07673860911270983,"defined as the gradient of the log probability density function (PDF) ∇x log P(x). Langevin diffusion
128"
LANGEVIN DYNAMICS,0.07733812949640288,"is a stochastic process defined by the stochastic differential equation (SDE)
129"
LANGEVIN DYNAMICS,0.07793764988009592,"dxt = −∇x log P(xt) dt +
√"
LANGEVIN DYNAMICS,0.07853717026378897,"2 dwt,"
LANGEVIN DYNAMICS,0.07913669064748201,"where wt is the Wiener process on Rd. To generate samples from Langevin diffusion, Welling and
130"
LANGEVIN DYNAMICS,0.07973621103117506,"Teh (2011) proposed stochastic gradient Langevin dynamics (SGLD), a discretization of the SDE for
131"
LANGEVIN DYNAMICS,0.0803357314148681,"T iterations. Each iteration of SGLD is defined as
132"
LANGEVIN DYNAMICS,0.08093525179856115,xt = xt−1 + δt
LANGEVIN DYNAMICS,0.0815347721822542,"2 ∇x log P(xt−1) +
p"
LANGEVIN DYNAMICS,0.08213429256594725,"δtϵt,
(1)"
LANGEVIN DYNAMICS,0.08273381294964029,"where δt is the step size and ϵt ∼N(0d, Id) is Gaussian noise. It has been widely recognized
133"
LANGEVIN DYNAMICS,0.08333333333333333,"that Langevin diffusion could take exponential time to mix without additional assumptions on the
134"
LANGEVIN DYNAMICS,0.08393285371702638,"probability density (Bovier et al., 2002, 2004; Gayrard et al., 2005; Raginsky et al., 2017; Lee et al.,
135"
LANGEVIN DYNAMICS,0.08453237410071943,"2018). To combat the slow mixing, Song and Ermon (2019) proposed annealed Langevin dynamics
136"
LANGEVIN DYNAMICS,0.08513189448441247,"by perturbing the probability density with Gaussian noise of variance σ2, i.e.,
137"
LANGEVIN DYNAMICS,0.08573141486810551,"Pσ(x) :=
Z
P(z)N(x | z, σ2Id) dz,
(2)"
LANGEVIN DYNAMICS,0.08633093525179857,"and running SGLD on the perturbed data distribution Pσt(x) with gradually decreasing noise levels
138"
LANGEVIN DYNAMICS,0.08693045563549161,"{σt}t∈[T ], i.e.,
139"
LANGEVIN DYNAMICS,0.08752997601918465,xt = xt−1 + δt
LANGEVIN DYNAMICS,0.08812949640287769,"2 ∇x log Pσt(xt−1) +
p"
LANGEVIN DYNAMICS,0.08872901678657075,"δtϵt,
(3)"
LANGEVIN DYNAMICS,0.08932853717026379,"where δt is the step size and ϵt ∼N(0d, Id) is Gaussian noise. When the noise level σ is vanishingly
140"
LANGEVIN DYNAMICS,0.08992805755395683,"small, the perturbed distribution is close to the true distribution, i.e., Pσ(x) ≈P(x). Since we do
141"
LANGEVIN DYNAMICS,0.09052757793764989,"not have direct access to the (perturbed) score function, Song and Ermon (2019) proposed the Noise
142"
LANGEVIN DYNAMICS,0.09112709832134293,"Conditional Score Network (NCSN) sθ(x, σ) to jointly estimate the scores of all perturbed data
143"
LANGEVIN DYNAMICS,0.09172661870503597,"distributions, i.e.,
144"
LANGEVIN DYNAMICS,0.09232613908872901,"∀σ ∈{σt}t∈[T ] , sθ(x, σ) ≈∇x log Pσ(x)."
LANGEVIN DYNAMICS,0.09292565947242207,"To train the NCSN, Song and Ermon (2019) adopted denoising score matching, which minimizes the
145"
LANGEVIN DYNAMICS,0.09352517985611511,"following loss
146"
LANGEVIN DYNAMICS,0.09412470023980815,"L

θ; {σt}t∈[T ]

:= 1"
T,0.09472422062350119,2T X
T,0.09532374100719425,"t∈[T ]
σ2
t Ex∼P E˜x∼N(x,σ2
t Id)"
T,0.09592326139088729,"sθ(˜x, σt) −˜x −x σ2
t  2
."
T,0.09652278177458033,"Assuming the NCSN has enough capacity, sθ∗(x, σ) minimizes the loss L

θ; {σt}t∈[T ]

if and only
147"
T,0.09712230215827339,"if sθ∗(x, σt) = ∇x log Pσt(x) almost surely for all t ∈[T].
148"
MULTI-MODAL DISTRIBUTIONS,0.09772182254196643,"3.2
Multi-Modal Distributions
149"
MULTI-MODAL DISTRIBUTIONS,0.09832134292565947,Our work focuses on multi-modal distributions. We use P = P
MULTI-MODAL DISTRIBUTIONS,0.09892086330935251,"i∈[k] wiP (i) to represent a mixture
150"
MULTI-MODAL DISTRIBUTIONS,0.09952038369304557,"of k modes, where each mode P (i) is a probability density with frequency wi such that wi > 0
151"
MULTI-MODAL DISTRIBUTIONS,0.10011990407673861,for all i ∈[k] and P
MULTI-MODAL DISTRIBUTIONS,0.10071942446043165,"i∈[k] wi = 1. In our theoretical analysis, we consider Gaussian mixtures and
152"
MULTI-MODAL DISTRIBUTIONS,0.1013189448441247,"sub-Gaussian mixtures, i.e., every component P (i) is a Gaussian or sub-Gaussian distribution. A
153"
MULTI-MODAL DISTRIBUTIONS,0.10191846522781775,"probability distribution p(z) of dimension d is defined as a sub-Gaussian distribution with parameter
154"
MULTI-MODAL DISTRIBUTIONS,0.10251798561151079,"ν2 if, given the mean vector µ := Ez∼p[z], the moment generating function (MGF) of p satisfies the
155"
MULTI-MODAL DISTRIBUTIONS,0.10311750599520383,"following inequality for every vector α ∈Rd:
156"
MULTI-MODAL DISTRIBUTIONS,0.10371702637889688,"Ez∼p

exp
 
αT (z −µ

≤exp
ν2 ∥α∥2
2
2"
MULTI-MODAL DISTRIBUTIONS,0.10431654676258993,"
.
(4)"
MULTI-MODAL DISTRIBUTIONS,0.10491606714628297,"We remark that sub-Gaussian distributions include a wide variety of distributions such as Gaussian
157"
MULTI-MODAL DISTRIBUTIONS,0.10551558752997602,"distributions and any distribution within a bounded ℓ2-norm distance from the mean µ. From
158"
MULTI-MODAL DISTRIBUTIONS,0.10611510791366907,"equation 2 we note that the perturbed distribution is the convolution of the original distribution
159"
MULTI-MODAL DISTRIBUTIONS,0.10671462829736211,"and a Gaussian random variable, i.e., for random variables z ∼p and t ∼N(0d, Id), their sum
160"
MULTI-MODAL DISTRIBUTIONS,0.10731414868105515,"z+t ∼pσ follows the perturbed distribution with noise level σ. Therefore, a perturbed (sub)Gaussian
161"
MULTI-MODAL DISTRIBUTIONS,0.1079136690647482,"distribution remains (sub)Gaussian. We formalize this property in Proposition 1 and defer the proof
162"
MULTI-MODAL DISTRIBUTIONS,0.10851318944844125,"to Appendix A for completeness.
163"
MULTI-MODAL DISTRIBUTIONS,0.1091127098321343,"Proposition 1. Suppose the perturbed distribution of a d-dimensional probability distribution p with
164"
MULTI-MODAL DISTRIBUTIONS,0.10971223021582734,"noise level σ is pσ, then the mean of the perturbed distribution is the same as the original distribution,
165"
MULTI-MODAL DISTRIBUTIONS,0.11031175059952038,"i.e., Ez∼pσ[z] = Ez∼p[z]. If p = N(µ, Σ) is a Gaussian distribution, pσ = N(µ, Σ + σ2Id) is also
166"
MULTI-MODAL DISTRIBUTIONS,0.11091127098321343,"a Gaussian distribution. If p is a sub-Gaussian distribution with parameter ν2, pσ is a sub-Gaussian
167"
MULTI-MODAL DISTRIBUTIONS,0.11151079136690648,"distribution with parameter (ν2 + σ2).
168"
THEORETICAL ANALYSIS OF THE MODE-SEEKING PROPERTIES OF LANGEVIN DYNAMICS,0.11211031175059952,"4
Theoretical Analysis of the Mode-Seeking Properties of Langevin Dynamics
169"
THEORETICAL ANALYSIS OF THE MODE-SEEKING PROPERTIES OF LANGEVIN DYNAMICS,0.11270983213429256,"In this section, we theoretically investigate the mode-seeking properties of vanilla and annealed
170"
THEORETICAL ANALYSIS OF THE MODE-SEEKING PROPERTIES OF LANGEVIN DYNAMICS,0.11330935251798561,"Langevin dynamics. We begin with analyzing Langevin dynamics in Gaussian mixtures.
171"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11390887290167866,"4.1
Langevin Dynamics in Gaussian Mixtures
172"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1145083932853717,"Assumption 1. Consider a data distribution P := Pk
i=0 wiP (i) as a mixture of Gaussian distribu-
173"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11510791366906475,"tions, where 1 ≤k = o(d) and wi > 0 is a positive constant such that Pk
i=0 wi = 1. Suppose that
174"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1157074340527578,"P (i) = N(µi, ν2
i Id) is a Gaussian distribution over Rd for all i ∈{0} ∪[k] such that for all i ∈[k],
175"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11630695443645084,"νi < ν0 and ∥µi −µ0∥2 ≤ν2
0−ν2
i
2

log

ν2
i
ν2
0"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11690647482014388,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11750599520383694,"
d. Denote νmax := maxi∈[k] νi.
176"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11810551558752998,"Regarding the first requirement νi < ν0, we first note that the probability density p(z) of a Gaussian
177"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11870503597122302,"distribution N(µ, ν2Id) decays exponentially in terms of ∥z−µ∥2"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11930455635491606,"ν2
. When a state z is sufficiently far
178"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.11990407673860912,"from all modes (i.e., ∥z∥≫∥µi∥), the Gaussian distribution with the largest variance (i.e., P (0) in
179"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12050359712230216,Assumption 1) dominates all other modes because ∥z−µ0∥2
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1211031175059952,"ν2
0
≈∥z∥2"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12170263788968826,"ν2
0
≫∥z∥2"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1223021582733813,"ν2
i
≈∥z−µi∥2"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12290167865707434,"ν2
i
. We call
180"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12350119904076738,"such mode P (0) the universal mode. Therefore, if z is initialized far from all modes, it can only
181"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12410071942446044,"converge to the universal mode because the gradient information of other modes is masked. Once
182"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12470023980815348,"z enters the universal mode P (0), if the step size δt of Langevin dynamics is small (i.e., δt ≤ν2
0),
183"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12529976019184652,"it would take exponential steps to escape the local mode P (0); while if the step size is large (i.e.,
184"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12589928057553956,"δt > ν2
0), the state z would again be far from all modes and thus the universal mode P (0) dominates
185"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1264988009592326,"all other modes. Hence, z can only visit the universal mode unless the stochastic noise ϵt miraculously
186"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12709832134292565,"leads it to the region of another mode. In addition, it can be verified that log

ν2
i
ν2
0"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12769784172661872,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i is a
187"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12829736211031176,"positive constant for νi < ν0, thus the second requirement of Assumption 1 essentially represents
188"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1288968824940048,"∥µi −µ0∥2 ≤O(d). We formalize the intuition in Theorem 1 and defer the proof to Appendix A.1.
189"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.12949640287769784,"Theorem 1. Consider a data distribution P satisfying Assumption 1. We follow Langevin dynamics
190"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13009592326139088,"for T = exp(O(d)) steps. Suppose the sample is initialized in P (0), then with probability at least
191"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13069544364508393,"1 −T · exp(−Ω(d)), we have ∥xt −µi∥2 > ν2
0+ν2
max
2
d for all t ∈{0} ∪[T] and i ∈[k].
192"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13129496402877697,"We note that ∥xt −µi∥2 > ν2
0+ν2
max
2
d is a strong notion of mode-seekingness, since the probability
193"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13189448441247004,"density of mode P (i) = N(µi, ν2
i Id) concentrates around the ℓ2-norm ball
n
z : ∥z −µi∥2 ≤ν2
i d
o
.
194"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13249400479616308,"This notion can also easily be translated into a lower bound in terms of other distance measures such
195"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13309352517985612,"as total variation distance and Wasserstein 2-distance. Moreover, in Theorem 2 we extend the result
196"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13369304556354916,"to annealed Langevin dynamics with bounded noise level, and the proof is deferred to Appendix A.2.
197"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1342925659472422,"Theorem 2. Consider a data distribution P satisfying Assumption 1. We follow annealed Langevin
198"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13489208633093525,"dynamics for T = exp(O(d)) steps with noise levels cσ ≥σ0 ≥· · · ≥σT ≥0 for constant cσ > 0.
199"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1354916067146283,"In addition, assume for all i ∈[k], ∥µi −µ0∥2 ≤
ν2
0−ν2
i
2

log

ν2
i +c2
σ
ν2
0+c2σ"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13609112709832133,"
−ν2
i +c2
σ
2ν2
0+c2σ + ν2
0+c2
σ
2ν2
i +c2σ"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.1366906474820144,"
d.
200"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13729016786570744,"Suppose that the sample is initialized in P (0)
σ0 , then with probability at least 1 −T · exp(−Ω(d)), we
201"
LANGEVIN DYNAMICS IN GAUSSIAN MIXTURES,0.13788968824940048,"have ∥xt −µi∥2 > ν2
0+ν2
max+2σ2
t
2
d for all t ∈{0} ∪[T] and i ∈[k].
202"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.13848920863309352,"4.2
Langevin Dynamics in Sub-Gaussian Mixtures
203"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.13908872901678657,"We further generalize our results to sub-Gaussian mixtures. We impose the following assumptions on
204"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1396882494004796,"the mixture. It is worth noting that these assumptions automatically hold for Gaussian mixtures.
205"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14028776978417265,"Assumption 2. Consider a data distribution P := Pk
i=0 wiP (i) as a mixture of sub-Gaussian
206"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14088729016786572,"distributions, where 1 ≤k = o(d) and wi > 0 is a positive constant such that Pk
i=0 wi = 1.
207"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14148681055155876,"Suppose that P (0) = N(µ0, ν2
0Id) is Gaussian and for all i ∈[k], P (i) satisfies
208"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1420863309352518,"i. P (i) is a sub-Gaussian distribution of mean µi with parameter ν2
i ,
209"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14268585131894485,"ii. P (i) is differentiable and ∇P (i)(µi) = 0d,
210"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1432853717026379,iii. the score function of P (i) is Li-Lipschitz such that Li ≤cL
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14388489208633093,"ν2
i for some constant cL > 0,
211"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14448441247002397,"iv. ν2
0 > max
n
1, 4(c2
L+cνcL)
cν(1−cν)
o
ν2
max
1−cν for constant cν ∈(0, 1), where νmax := maxi∈[k] νi,
212"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.145083932853717,"v. ∥µi −µ0∥2 ≤(1−cν)ν2
0−ν2
i
2(1−cν)

log
cνν2
i
(c2
L+cνcL)ν2
0 −
ν2
i
2(1−cν)ν2
0 + (1−cν)ν2
0
2ν2
i"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14568345323741008,"
d.
213"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14628297362110312,"We validate the feasibility of Assumption 2.v. in Lemma 9 in the Appendix. With Assumption 2, we
214"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14688249400479617,"show the mode-seeking tendency of Langevin dynamics under sub-Gaussian distributions in Theorem
215"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1474820143884892,"3 and defer the proof to Appendix A.3.
216"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14808153477218225,"Algorithm 1 Chained Langevin Dynamics (Chained-LD)
Require: Patch size Q, dimension d, conditional score function estimator sθ, number of iterations
T, noise levels {σt}t∈[T Q/d], step size {δt}t∈[T Q/d]."
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1486810551558753,"1: Initialize x0, and divide x0 into d/Q patches x(1)
0 , · · · x(d/Q)
0
of equal size Q"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.14928057553956833,2: for q ←1 to d/Q do
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1498800959232614,"3:
for t ←1 to TQ/d do"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15047961630695444,"4:
x(q)
t
←x(q)
t−1 + δt"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1510791366906475,"2 sθ

x(q)
t
| σt, x(1)
t , · · · , x(q−1)
t

+ √δtϵt, where ϵt ∼N(0Q, IQ)"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15167865707434053,"5:
end for"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15227817745803357,"6:
x(q)
0
←x(q)
T Q/d
7: end for"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1528776978417266,8: return xT Q/d
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15347721822541965,"Theorem 3. Consider a data distribution P satisfying Assumption 2. We follow Langevin dynamics
217"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15407673860911272,"for T = exp(O(d)) steps. Suppose the sample is initialized in P (0), then with probability at least
218"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15467625899280577,"1 −T · exp(−O(d)), we have ∥xt −µi∥2 >

ν2
0
2 +
ν2
max
2(1−cν)

d for all t ∈{0} ∪[T] and i ∈[k].
219"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1552757793764988,"Finally, we slightly modify Assumption 2 and extend our results to annealed Langevin dynamics
220"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15587529976019185,"under sub-Gaussian mixtures in Theorem 4. The details of Assumption 3 and the proof of Theorem 4
221"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.1564748201438849,"are deferred to Appendix A.4.
222"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15707434052757793,"Theorem 4. Consider a data distribution P satisfying Assumption 3. We follow annealed Langevin
223"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15767386091127097,"dynamics for T = exp(O(d)) steps with noise levels cσ ≥σ0 ≥· · · ≥σT ≥0. Suppose
224"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15827338129496402,"the sample is initialized in P (0)
σ0 , then with probability at least 1 −T · exp(−O(d)), we have
225"
LANGEVIN DYNAMICS IN SUB-GAUSSIAN MIXTURES,0.15887290167865709,"∥xt −µi∥2 >

ν2
0+σ2
t
2
+ ν2
max+σ2
t
2(1−cν)

d for all t ∈{0} ∪[T] and i ∈[k].
226"
CHAINED LANGEVIN DYNAMICS,0.15947242206235013,"5
Chained Langevin Dynamics
227"
CHAINED LANGEVIN DYNAMICS,0.16007194244604317,"To reduce the mode-seeking tendencies of vanilla and annealed Langevin dynamics, we propose
228"
CHAINED LANGEVIN DYNAMICS,0.1606714628297362,"Chained Langevin Dynamics (Chained-LD) in Algorithm 1. While vanilla and annealed Langevin
229"
CHAINED LANGEVIN DYNAMICS,0.16127098321342925,"dynamics apply gradient updates to all coordinates of the sample in every step, we decompose the
230"
CHAINED LANGEVIN DYNAMICS,0.1618705035971223,"sample into patches of constant size and generate each patch sequentially to alleviate the exponen-
231"
CHAINED LANGEVIN DYNAMICS,0.16247002398081534,"tial dependency on the dimensionality. More precisely, we divide a sample x into d/Q patches
232"
CHAINED LANGEVIN DYNAMICS,0.1630695443645084,"x(1), · · · x(d/Q) of some constant size Q, and apply annealed Langevin dynamics to sample each
233"
CHAINED LANGEVIN DYNAMICS,0.16366906474820145,"patch x(q) (for q ∈[d/Q]) from the conditional distribution P(x(q) | x(1), · · · x(q−1)).
234"
CHAINED LANGEVIN DYNAMICS,0.1642685851318945,"An ideal conditional score function estimator sθ could jointly estimate the scores of all perturbed
235"
CHAINED LANGEVIN DYNAMICS,0.16486810551558753,"conditional patch distribution, i.e., ∀σ ∈{σt}t∈[T Q/d] , q ∈[d/Q],
236"
CHAINED LANGEVIN DYNAMICS,0.16546762589928057,"sθ

x(q) | σ, x(1), · · · , x(q−1)
≈∇x(q) log Pσ(x(q) | x(1), · · · x(q−1))."
CHAINED LANGEVIN DYNAMICS,0.16606714628297362,"Following from Song and Ermon (2019), we use the denoising score matching to train the estimator.
237"
CHAINED LANGEVIN DYNAMICS,0.16666666666666666,"For a given σ, the denoising score matching objective is
238"
CHAINED LANGEVIN DYNAMICS,0.1672661870503597,ℓ(θ; σ) := 1
CHAINED LANGEVIN DYNAMICS,0.16786570743405277,"2Ex∼P E˜x∼N(x,σ2Id)
X"
CHAINED LANGEVIN DYNAMICS,0.1684652278177458,q∈[d/Q]
CHAINED LANGEVIN DYNAMICS,0.16906474820143885,"""sθ

x(q) | σ, x(1), · · · , x(q−1)
−˜x(q) −x(q) σ2  2# ."
CHAINED LANGEVIN DYNAMICS,0.1696642685851319,"Then, combining the objectives gives the following loss
239"
CHAINED LANGEVIN DYNAMICS,0.17026378896882494,"L

θ; {σt}t∈[T Q/d]

:=
d
TQ X"
CHAINED LANGEVIN DYNAMICS,0.17086330935251798,"t∈[T Q/d]
σ2
t ℓ(θ; σt)."
CHAINED LANGEVIN DYNAMICS,0.17146282973621102,"As shown in Vincent (2011), an estimator sθ with enough capacity minimizes the loss L if and only if
240"
CHAINED LANGEVIN DYNAMICS,0.1720623501199041,"sθ outputs the scores of all perturbed conditional patch distribution almost surely. Ideally, if a sampler
241"
CHAINED LANGEVIN DYNAMICS,0.17266187050359713,"Figure 1: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and
chained Langevin dynamics. Three axes are ℓ2 distance from samples to the mean of the three modes.
The samples are initialized in mode 0."
CHAINED LANGEVIN DYNAMICS,0.17326139088729017,"perfectly generates every patch, combining all patches gives a sample from the original distribution
242"
CHAINED LANGEVIN DYNAMICS,0.17386091127098321,since P(x) = Q
CHAINED LANGEVIN DYNAMICS,0.17446043165467626,"q∈[d/Q] P(x(q) | x(1), · · · x(q−1)). In Theorem 5 we give a linear reduction from
243"
CHAINED LANGEVIN DYNAMICS,0.1750599520383693,"producing samples of dimension d using Chained-LD to learning the distribution of a Q-dimensional
244"
CHAINED LANGEVIN DYNAMICS,0.17565947242206234,"variable for constant Q. The proof of Theorem 5 is deferred to Appendix A.5.
245"
CHAINED LANGEVIN DYNAMICS,0.17625899280575538,"Theorem 5. Consider a sampler algorithm taking the first q −1 patches x(1), · · · , x(q−1) as input
246"
CHAINED LANGEVIN DYNAMICS,0.17685851318944845,"and outputing a sample of the next patch x(q) with probability ˆP
 
x(q) | x(1), · · · , x(q−1)
for all
247"
CHAINED LANGEVIN DYNAMICS,0.1774580335731415,"q ∈[d/Q]. Suppose that for every q ∈[d/Q] and any given previous patches x(1), · · · , x(q−1), the
248"
CHAINED LANGEVIN DYNAMICS,0.17805755395683454,"sampler algorithm can achieve
249"
CHAINED LANGEVIN DYNAMICS,0.17865707434052758,"TV

ˆP

x(q) | x(1), · · · , x(q−1)
, P

x(q) | x(1), · · · , x(q−1)
≤ε · Q"
CHAINED LANGEVIN DYNAMICS,0.17925659472422062,"d
in τ(ε, d) iterations for some ε > 0. Then, equipped with the sampler algorithm, the Chained-LD
250"
CHAINED LANGEVIN DYNAMICS,0.17985611510791366,algorithm in d
CHAINED LANGEVIN DYNAMICS,0.1804556354916067,"Q · τ(ε, d) iterations can achieve
251"
CHAINED LANGEVIN DYNAMICS,0.18105515587529977,"TV

ˆP(x), P(x)

≤ε."
NUMERICAL RESULTS,0.18165467625899281,"6
Numerical Results
252"
NUMERICAL RESULTS,0.18225419664268586,"In this section, we empirically evaluated the mode-seeking tendencies of vanilla, annealed, and
253"
NUMERICAL RESULTS,0.1828537170263789,"chained Langevin dynamics. We performed numerical experiments on synthetic Gaussian mixture
254"
NUMERICAL RESULTS,0.18345323741007194,"models and real image datasets including MNIST (LeCun, 1998) and Fashion-MNIST (Xiao et al.,
255"
NUMERICAL RESULTS,0.18405275779376498,"2017). Details on the experiment setup are deferred to Appendix B.
256"
NUMERICAL RESULTS,0.18465227817745802,"Figure 2: Samples from a mixture distribution of the original and flipped images from the MNIST
dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized
as original images from MNIST."
NUMERICAL RESULTS,0.18525179856115107,"Synthetic Gaussian mixture model: We define the data distribution P as a mixture of three Gaussian
257"
NUMERICAL RESULTS,0.18585131894484413,"components in dimension d = 100, where mode 0 defined as P (0) = N(0d, 3Id) is the universal
258"
NUMERICAL RESULTS,0.18645083932853718,"mode with the largest variance, and mode 1 and mode 2 are respectively defined as P (1) = N(1d, Id)
259"
NUMERICAL RESULTS,0.18705035971223022,"and P (2) = N(−1d, Id). The frequencies of the three modes are 0.2, 0.4 and 0.4, i.e.,
260"
NUMERICAL RESULTS,0.18764988009592326,"P = 0.2P (0) + 0.4P (1) + 0.4P (2) = 0.2N(0d, 3Id) + 0.4N(1d, Id) + 0.4N(−1d, Id)."
NUMERICAL RESULTS,0.1882494004796163,"As shown in Figure 1, vanilla and annealed Langevin dynamics cannot find mode 1 or 2 within 106
261"
NUMERICAL RESULTS,0.18884892086330934,"iterations if the sample is initialized in mode 0, while chained Langevin dynamics can find the other
262"
NUMERICAL RESULTS,0.18944844124700239,"two modes in 1000 steps and correctly recover their frequencies as gradually increasing the number
263"
NUMERICAL RESULTS,0.19004796163069546,"of iterations. In Appendix B.1 we present additional experiments on samples initialized in mode 1 or
264"
NUMERICAL RESULTS,0.1906474820143885,"2, which also verify the mode-seeking tendencies of vanilla and annealed Langevin dynamics.
265"
NUMERICAL RESULTS,0.19124700239808154,"Image datasets: We construct the distribution as a mixture of two modes by using the original images
266"
NUMERICAL RESULTS,0.19184652278177458,"from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the
267"
NUMERICAL RESULTS,0.19244604316546762,"first mode and constructing the second mode by i.i.d. randomly flipping an image (white background
268"
NUMERICAL RESULTS,0.19304556354916066,"and black digits/objects) with probability 0.5. Regarding the neural network architecture of the score
269"
NUMERICAL RESULTS,0.1936450839328537,"function estimator, for vanilla and annealed Langevin dynamics we use U-Net (Ronneberger et al.,
270"
NUMERICAL RESULTS,0.19424460431654678,"2015) following from Song and Ermon (2019). For chained Langevin dynamics, we proposed to use
271"
NUMERICAL RESULTS,0.19484412470023982,"Recurrent Neural Network (RNN) architectures. We note that for a sequence of inputs, the output of
272"
NUMERICAL RESULTS,0.19544364508393286,"RNN from the previous step is fed as input to the current step. Therefore, in the scenario of chained
273"
NUMERICAL RESULTS,0.1960431654676259,"Langevin dynamics, the hidden state of RNN contains information about the previous patches and
274"
NUMERICAL RESULTS,0.19664268585131894,"allows the network to estimate the conditional score function ∇x(q) log P(x(q) | x(1), · · · x(q−1)).
275"
NUMERICAL RESULTS,0.19724220623501199,"More implementation details are deferred to Appendix B.2.
276"
NUMERICAL RESULTS,0.19784172661870503,"Figure 3: Samples from a mixture distribution of the original and flipped images from the Fashion-
MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are
initialized as original images from Fashion-MNIST."
NUMERICAL RESULTS,0.19844124700239807,"The numerical results on image datasets are shown in Figures 2 and 3. Vanilla Langevin dynamics
277"
NUMERICAL RESULTS,0.19904076738609114,"fails to generate reasonable samples, as also observed in Song and Ermon (2019). When the sample
278"
NUMERICAL RESULTS,0.19964028776978418,"is initialized as original images from the datasets, annealed Langevin dynamics tends to generate
279"
NUMERICAL RESULTS,0.20023980815347722,"samples from the same mode, while chained Langevin dynamics can generate samples from both
280"
NUMERICAL RESULTS,0.20083932853717026,"modes. Additional experiments are deferred to Appendix B.2.
281"
CONCLUSION,0.2014388489208633,"7
Conclusion
282"
CONCLUSION,0.20203836930455635,"In this work, we theoretically and numerically studied the mode-seeking properties of vanilla and
283"
CONCLUSION,0.2026378896882494,"annealed Langevin dynamics sampling methods under a multi-modal distribution. We characterized
284"
CONCLUSION,0.20323741007194246,"Gaussian and sub-Gaussian mixture models under which Langevin dynamics are unlikely to find all
285"
CONCLUSION,0.2038369304556355,"the components within a sub-exponential number of iterations. To reduce the mode-seeking tendency
286"
CONCLUSION,0.20443645083932854,"of vanilla Langevin dynamics, we proposed Chained Langevin Dynamics (Chained-LD) and analyzed
287"
CONCLUSION,0.20503597122302158,"its convergence behavior. Studying the connections between Chained-LD and denoising diffusion
288"
CONCLUSION,0.20563549160671463,"models will be an interesting topic for future exploration.
289"
CONCLUSION,0.20623501199040767,"Limitations
290"
CONCLUSION,0.2068345323741007,"Our RNN-based implementation of Chained-LD is currently limited to image data generation tasks.
291"
CONCLUSION,0.20743405275779375,"An interesting future direction is to extend the application of Chained-LD to other domains such as
292"
CONCLUSION,0.20803357314148682,"audio and text data. Another future direction could be to study the convergence of Chained-LD under
293"
CONCLUSION,0.20863309352517986,"an imperfect score estimation which we did not address in our analysis.
294"
REFERENCES,0.2092326139088729,"References
295"
REFERENCES,0.20983213429256595,"Bakry, D., Barthe, F., Cattiaux, P., and Guillin, A. (2008). A simple proof of the poincaré inequality
296"
REFERENCES,0.210431654676259,"for a large class of probability measures. Electronic Communications in Probability [electronic
297"
REFERENCES,0.21103117505995203,"only], 13:60–66.
298"
REFERENCES,0.21163069544364507,"Bakry, D. and Émery, M. (1983). Diffusions hypercontractives. Seminaire de Probabilites XIX, page
299"
REFERENCES,0.21223021582733814,"177.
300"
REFERENCES,0.21282973621103118,"Benton, J., De Bortoli, V., Doucet, A., and Deligiannidis, G. (2023). Linear convergence bounds for
301"
REFERENCES,0.21342925659472423,"diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686.
302"
REFERENCES,0.21402877697841727,"Bhattacharya, R. (1978). Criteria for recurrence and existence of invariant measures for multidimen-
303"
REFERENCES,0.2146282973621103,"sional diffusions. The Annals of Probability, pages 541–553.
304"
REFERENCES,0.21522781774580335,"Bishop, C. M. (2006). Pattern recognition and machine learning. Springer google schola, 2:645–678.
305"
REFERENCES,0.2158273381294964,"Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. (2023).
306"
REFERENCES,0.21642685851318944,"Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings
307"
REFERENCES,0.2170263788968825,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–22575.
308"
REFERENCES,0.21762589928057555,"Block, A., Mroueh, Y., and Rakhlin, A. (2020). Generative modeling with denoising auto-encoders
309"
REFERENCES,0.2182254196642686,"and langevin sampling. arXiv preprint arXiv:2002.00107.
310"
REFERENCES,0.21882494004796163,"Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2002). Metastability and low lying spectra in
311"
REFERENCES,0.21942446043165467,"reversible markov chains. Communications in mathematical physics, 228:219–255.
312"
REFERENCES,0.22002398081534771,"Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2004). Metastability in reversible diffusion
313"
REFERENCES,0.22062350119904076,"processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical
314"
REFERENCES,0.22122302158273383,"Society, 6(4):399–424.
315"
REFERENCES,0.22182254196642687,"Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. (2020). Wavegrad: Estimating
316"
REFERENCES,0.2224220623501199,"gradients for waveform generation. In International Conference on Learning Representations.
317"
REFERENCES,0.22302158273381295,"Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2023). Sampling is as easy as learning
318"
REFERENCES,0.223621103117506,"the score: theory for diffusion models with minimal data assumptions. In International Conference
319"
REFERENCES,0.22422062350119903,"on Learning Representations.
320"
REFERENCES,0.22482014388489208,"Cheng, X., Chatterji, N. S., Abbasi-Yadkori, Y., Bartlett, P. L., and Jordan, M. I. (2018). Sharp con-
321"
REFERENCES,0.22541966426858512,"vergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648.
322"
REFERENCES,0.2260191846522782,"Dalalyan, A. S. (2017). Theoretical guarantees for approximate sampling from smooth and log-
323"
REFERENCES,0.22661870503597123,"concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology,
324"
REFERENCES,0.22721822541966427,"79(3):651–676.
325"
REFERENCES,0.2278177458033573,"De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schrödinger bridge with
326"
REFERENCES,0.22841726618705036,"applications to score-based generative modeling. Advances in Neural Information Processing
327"
REFERENCES,0.2290167865707434,"Systems, 34:17695–17709.
328"
REFERENCES,0.22961630695443644,"Durmus, A. and Moulines, É. (2017). Nonasymptotic convergence analysis for the unadjusted
329"
REFERENCES,0.2302158273381295,"langevin algorithm. The Annals of Applied Probability, 27(3):1551–1587.
330"
REFERENCES,0.23081534772182255,"Gayrard, V., Bovier, A., and Klein, M. (2005). Metastability in reversible diffusion processes
331"
REFERENCES,0.2314148681055156,"ii: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society,
332"
REFERENCES,0.23201438848920863,"7(1):69–99.
333"
REFERENCES,0.23261390887290168,"Goodfellow, I. (2016).
Nips 2016 tutorial: Generative adversarial networks.
arXiv preprint
334"
REFERENCES,0.23321342925659472,"arXiv:1701.00160.
335"
REFERENCES,0.23381294964028776,"Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,
336"
REFERENCES,0.23441247002398083,"and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing
337"
REFERENCES,0.23501199040767387,"systems, 27.
338"
REFERENCES,0.2356115107913669,"Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi,
339"
REFERENCES,0.23621103117505995,"M., Fleet, D. J., et al. (2022). Imagen video: High definition video generation with diffusion
340"
REFERENCES,0.236810551558753,"models. arXiv preprint arXiv:2210.02303.
341"
REFERENCES,0.23741007194244604,"Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural
342"
REFERENCES,0.23800959232613908,"information processing systems, 33:6840–6851.
343"
REFERENCES,0.23860911270983212,"Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. (2021). Imitation learning
344"
REFERENCES,0.2392086330935252,"as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the
345"
REFERENCES,0.23980815347721823,"Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pages 313–329. Springer.
346"
REFERENCES,0.24040767386091128,"Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2020). Diffwave: A versatile diffusion
347"
REFERENCES,0.24100719424460432,"model for audio synthesis. In International Conference on Learning Representations.
348"
REFERENCES,0.24160671462829736,"Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model selection.
349"
REFERENCES,0.2422062350119904,"Annals of statistics, pages 1302–1338.
350"
REFERENCES,0.24280575539568344,"LeCun, Y. (1998). The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.
351"
REFERENCES,0.2434052757793765,"Lee, H., Lu, J., and Tan, Y. (2022). Convergence for score-based generative modeling with polynomial
352"
REFERENCES,0.24400479616306955,"complexity. Advances in Neural Information Processing Systems, 35:22870–22882.
353"
REFERENCES,0.2446043165467626,"Lee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling for general
354"
REFERENCES,0.24520383693045564,"data distributions. In International Conference on Algorithmic Learning Theory, pages 946–985.
355"
REFERENCES,0.24580335731414868,"PMLR.
356"
REFERENCES,0.24640287769784172,"Lee, H., Risteski, A., and Ge, R. (2018). Beyond log-concavity: Provable guarantees for sampling
357"
REFERENCES,0.24700239808153476,"multi-modal distributions using simulated tempering langevin monte carlo. Advances in neural
358"
REFERENCES,0.2476019184652278,"information processing systems, 31.
359"
REFERENCES,0.24820143884892087,"Li, C. T. and Farnia, F. (2023). Mode-seeking divergences: theory and applications to gans. In
360"
REFERENCES,0.24880095923261392,"International Conference on Artificial Intelligence and Statistics, pages 8321–8350. PMLR.
361"
REFERENCES,0.24940047961630696,"Li, G., Huang, Y., Efimov, T., Wei, Y., Chi, Y., and Chen, Y. (2024). Accelerating convergence of
362"
REFERENCES,0.25,"score-based diffusion models, provably. arXiv preprint arXiv:2403.03852.
363"
REFERENCES,0.25059952038369304,"Li, G., Wei, Y., Chen, Y., and Chi, Y. (2023). Towards non-asymptotic convergence for diffusion-based
364"
REFERENCES,0.2511990407673861,"generative models. In The Twelfth International Conference on Learning Representations.
365"
REFERENCES,0.2517985611510791,"Lin, G., Milan, A., Shen, C., and Reid, I. (2017). Refinenet: Multi-path refinement networks for
366"
REFERENCES,0.25239808153477217,"high-resolution semantic segmentation. In Proceedings of the IEEE conference on computer vision
367"
REFERENCES,0.2529976019184652,"and pattern recognition, pages 1925–1934.
368"
REFERENCES,0.25359712230215825,"Poole, B., Alemi, A. A., Sohl-Dickstein, J., and Angelova, A. (2016). Improved generator objectives
369"
REFERENCES,0.2541966426858513,"for gans. arXiv preprint arXiv:1612.02780.
370"
REFERENCES,0.2547961630695444,"Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017). Non-convex learning via stochastic gradient
371"
REFERENCES,0.25539568345323743,"langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674–
372"
REFERENCES,0.2559952038369305,"1703. PMLR.
373"
REFERENCES,0.2565947242206235,"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional
374"
REFERENCES,0.25719424460431656,"image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3.
375"
REFERENCES,0.2577937649880096,"Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin distributions and
376"
REFERENCES,0.25839328537170264,"their discrete approximations. Bernoulli, pages 341–363.
377"
REFERENCES,0.2589928057553957,"Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image
378"
REFERENCES,0.2595923261390887,"synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
379"
REFERENCES,0.26019184652278177,"vision and pattern recognition, pages 10684–10695.
380"
REFERENCES,0.2607913669064748,"Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical
381"
REFERENCES,0.26139088729016785,"image segmentation. In Medical image computing and computer-assisted intervention–MICCAI
382"
REFERENCES,0.2619904076738609,"2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III
383"
REFERENCES,0.26258992805755393,"18, pages 234–241. Springer.
384"
REFERENCES,0.263189448441247,"Sak, H., Senior, A., and Beaufays, F. (2014). Long short-term memory based recurrent neural network
385"
REFERENCES,0.2637889688249401,"architectures for large vocabulary speech recognition. arXiv preprint arXiv:1402.1128.
386"
REFERENCES,0.2643884892086331,"Shannon, M., Poole, B., Mariooryad, S., Bagby, T., Battenberg, E., Kao, D., Stanton, D., and
387"
REFERENCES,0.26498800959232616,"Skerry-Ryan, R. (2020). Non-saturating gan training as divergence minimization. arXiv preprint
388"
REFERENCES,0.2655875299760192,"arXiv:2010.08029.
389"
REFERENCES,0.26618705035971224,"Song, J., Meng, C., and Ermon, S. (2020a). Denoising diffusion implicit models. arXiv preprint
390"
REFERENCES,0.2667865707434053,"arXiv:2010.02502.
391"
REFERENCES,0.2673860911270983,"Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.
392"
REFERENCES,0.26798561151079137,"Advances in neural information processing systems, 32.
393"
REFERENCES,0.2685851318944844,"Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models.
394"
REFERENCES,0.26918465227817745,"Advances in neural information processing systems, 33:12438–12448.
395"
REFERENCES,0.2697841726618705,"Song, Y., Garg, S., Shi, J., and Ermon, S. (2020b). Sliced score matching: A scalable approach to
396"
REFERENCES,0.27038369304556353,"density and score estimation. In Uncertainty in Artificial Intelligence, pages 574–584. PMLR.
397"
REFERENCES,0.2709832134292566,"Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2020c). Score-
398"
REFERENCES,0.2715827338129496,"based generative modeling through stochastic differential equations. In International Conference
399"
REFERENCES,0.27218225419664266,"on Learning Representations.
400"
REFERENCES,0.27278177458033576,"Vempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin algorithm:
401"
REFERENCES,0.2733812949640288,"Isoperimetry suffices. Advances in neural information processing systems, 32.
402"
REFERENCES,0.27398081534772184,"Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural
403"
REFERENCES,0.2745803357314149,"computation, 23(7):1661–1674.
404"
REFERENCES,0.2751798561151079,"Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In
405"
REFERENCES,0.27577937649880097,"Proceedings of the 28th international conference on machine learning (ICML-11), pages 681–688.
406"
REFERENCES,0.276378896882494,"Citeseer.
407"
REFERENCES,0.27697841726618705,"Wooddard, D. B., Schmidler, S. C., and Huber, M. (2009). Conditions for rapid mixing of parallel and
408"
REFERENCES,0.2775779376498801,"simulated tempering on multimodal distributions. The Annals of Applied Probability, 19(2):617–
409"
REFERENCES,0.27817745803357313,"640.
410"
REFERENCES,0.2787769784172662,"Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking
411"
REFERENCES,0.2793764988009592,"machine learning algorithms. arXiv preprint arXiv:1708.07747.
412"
REFERENCES,0.27997601918465226,"A
Theoretical Analysis on the Mode-Seeking Tendency of Langevin Dynamics
413"
REFERENCES,0.2805755395683453,"We begin by introducing some well-established lemmas used in our proof. We first provide the proof
414"
REFERENCES,0.28117505995203834,"of Proposition 1 for completeness:
415"
REFERENCES,0.28177458033573144,"Proof of Proposition 1. By the definition in equation 2, we have
416"
REFERENCES,0.2823741007194245,"pσ(z) =
Z
p(t)N(z | t, σ2Id) dt =
Z
p(t)N(z −t | 0d, σ2Id) dt."
REFERENCES,0.2829736211031175,"For random variables t ∼p and y ∼N(0d, Id), their sum z = t + y ∼pσ follows the perturbed
417"
REFERENCES,0.28357314148681056,"distribution with noise level σ. Therefore,
418"
REFERENCES,0.2841726618705036,"Ez∼pσ[z] = E(t+y)∼pσ[t + y] = Et∼p[t] + Ey∼N(0d,Id)[y] = Et∼p[t]."
REFERENCES,0.28477218225419665,"If t ∼p = N(µ, Σ) follows a Gaussian distribution, we have z = t + y ∼pσ = N(µ, Σ + σ2Id).
419"
REFERENCES,0.2853717026378897,"If p is a sub-Gaussian distribution with parameter ν2, we have z = t + y ∼pσ is a sub-Gaussian
420"
REFERENCES,0.28597122302158273,"distribution with parameter (ν2 + σ2). Hence we obtain Proposition 1.
421"
REFERENCES,0.2865707434052758,"We use the following lemma on the tail bound for multivariate Gaussian random variables.
422"
REFERENCES,0.2871702637889688,"Lemma 1 (Lemma 1, Laurent and Massart (2000)). Suppose that a random variable z ∼N(0d, Id).
423"
REFERENCES,0.28776978417266186,"Then for any λ > 0,
424"
REFERENCES,0.2883693045563549,"P

∥z∥2 ≥d + 2
√"
REFERENCES,0.28896882494004794,"dλ + 2λ

≤exp(−λ),"
REFERENCES,0.289568345323741,"P

∥z∥2 ≤d −2
√"
REFERENCES,0.290167865707434,"dλ

≤exp(−λ)."
REFERENCES,0.2907673860911271,"We also use a tail bound for one-dimensional Gaussian random variables and provide the proof here
425"
REFERENCES,0.29136690647482016,"for completeness.
426"
REFERENCES,0.2919664268585132,"Lemma 2. Suppose a random variable Z ∼N(0, 1). Then for any t > 0,
427"
REFERENCES,0.29256594724220625,"P(Z ≥t) = P(Z ≤−t) ≤exp(−t2/2)
√ 2πt
."
REFERENCES,0.2931654676258993,Proof of Lemma 2. Since z
REFERENCES,0.29376498800959233,"t ≥1 for all z ∈[t, ∞), we have
428"
REFERENCES,0.2943645083932854,"P(Z ≥t) =
1
√ 2π Z ∞"
REFERENCES,0.2949640287769784,"t
exp

−z2 2"
REFERENCES,0.29556354916067146,"
dz ≤
1
√ 2π Z ∞ t z"
REFERENCES,0.2961630695443645,"t exp

−z2 2"
REFERENCES,0.29676258992805754,"
dz = exp(−t2/2)
√ 2πt
."
REFERENCES,0.2973621103117506,"Since the Gaussian distribution is symmetric, we have P(Z ≥t) = P(Z ≤−t). Hence we obtain the
429"
REFERENCES,0.2979616306954436,"desired bound.
430"
REFERENCES,0.29856115107913667,"A.1
Proof of Theorem 1: Langevin Dynamics under Gaussian Mixtures
431"
REFERENCES,0.29916067146282976,"Without loss of generality, we assume that µ0 = 0d for simplicity. Let r and n respectively denote
432"
REFERENCES,0.2997601918465228,"the rank and nullity of the vector space {µi}i∈[k], then we have r + n = d and 0 ≤r ≤k = o(d).
433"
REFERENCES,0.30035971223021585,"Denote R ∈Rd×r an orthonormal basis of the vector space {µi}i∈[k], and denote N ∈Rd×n an
434"
REFERENCES,0.3009592326139089,"orthonormal basis of the null space of {µi}i∈[k]. Now consider decomposing the sample xt by
435"
REFERENCES,0.30155875299760193,"rt := RT xt, and nt := NT xt,"
REFERENCES,0.302158273381295,"where rt ∈Rr, nt ∈Rn. Then we have
436"
REFERENCES,0.302757793764988,xt = Rrt + Nnt.
REFERENCES,0.30335731414868106,"Similarly, we decompose the noise ϵt into
437"
REFERENCES,0.3039568345323741,"ϵ(r)
t
:= RT ϵt, and ϵ(n)
t
:= NT ϵt,"
REFERENCES,0.30455635491606714,"where ϵ(r)
t
∈Rr, ϵ(n)
t
∈Rn. Then we have
438"
REFERENCES,0.3051558752997602,"ϵt = Rϵ(r)
t
+ Nϵ(n)
t
."
REFERENCES,0.3057553956834532,"Since a linear combination of a Gaussian random variable still follows Gaussian distribution, by
439"
REFERENCES,0.30635491606714627,"ϵt ∼N(0d, Id), RT R = Ir, and NT N = In we obtain
440"
REFERENCES,0.3069544364508393,"ϵ(r)
t
∼N(0r, Ir), and ϵ(n)
t
∼N(0n, In)."
REFERENCES,0.30755395683453235,"By the definition of Langevin dynamics in equation 1, the two components of xt follow from the
441"
REFERENCES,0.30815347721822545,"update rule:
442"
REFERENCES,0.3087529976019185,nt = nt−1 + δt
REFERENCES,0.30935251798561153,"2 NT ∇x log P(xt−1) +
p"
REFERENCES,0.30995203836930457,"δtϵ(n)
t
,
(5)"
REFERENCES,0.3105515587529976,rt = rt−1 + δt
REFERENCES,0.31115107913669066,"2 RT ∇x log P(xt−1) +
p"
REFERENCES,0.3117505995203837,"δtϵ(r)
t ."
REFERENCES,0.31235011990407674,"It is worth noting that since NT µi = 0n. To show ∥xt −µi∥2 > ν2
0+ν2
max
2
d, it suffices to prove
443"
REFERENCES,0.3129496402877698,"∥nt∥2 > ν2
0 + ν2
max
2
d."
REFERENCES,0.3135491606714628,"We start by proving that the initialization of the state x0 has a large norm on the null space with high
444"
REFERENCES,0.31414868105515587,"probability in the following proposition.
445"
REFERENCES,0.3147482014388489,"Proposition 2. Suppose that a sample x0 is initialized in the distribution P (0), i.e., x0 ∼P (0), then
446"
REFERENCES,0.31534772182254195,"for any constant νmax < ν0, with probability at least 1 −exp(−Ω(d)), we have ∥n0∥2 ≥3ν2
0+ν2
max
4
d.
447"
REFERENCES,0.315947242206235,"Proof of Proposition 2. Since x0 ∼P (0) = N(0d, ν2
0Id) and NT N = In, we know n0 = NT x0 ∼
448"
REFERENCES,0.31654676258992803,"N(0n, ν2
0In). Therefore, by Lemma 1 we can bound
449"
REFERENCES,0.31714628297362113,"P

∥n0∥2 ≤3ν2
0 + ν2
max
4
d

= P "
REFERENCES,0.31774580335731417,∥n0∥2
REFERENCES,0.3183453237410072,"ν2
0
≤d −2 s"
REFERENCES,0.31894484412470026,"d ·
ν2
0 −ν2max 8ν2
0 2
d   ≤P "
REFERENCES,0.3195443645083933,∥n0∥2
REFERENCES,0.32014388489208634,"ν2
0
≤n −2 s"
REFERENCES,0.3207434052757794,"n
ν2
0 −ν2max 8ν2
0 2 d 2   ≤exp "
REFERENCES,0.3213429256594724,"−
ν2
0 −ν2
max
8ν2
0 2 d 2 ! ,"
REFERENCES,0.32194244604316546,"where the second last step follows from the assumption d −n = r = o(d). Hence we complete the
450"
REFERENCES,0.3225419664268585,"proof of Proposition 2.
451"
REFERENCES,0.32314148681055155,"Then, with the assumption that the initialization satisfies ∥n0∥2 ≥3ν2
0+ν2
max
4
d, the following proposi-
452"
REFERENCES,0.3237410071942446,"tion shows that ∥nt∥remains large with high probability.
453"
REFERENCES,0.32434052757793763,"Proposition 3. Consider a data distribution P satisfies the constraints specified in Theorem 1.
454"
REFERENCES,0.3249400479616307,"We follow the Langevin dynamics for T = exp(O(d)) steps. Suppose that the initial sample
455"
REFERENCES,0.3255395683453237,"satisfies ∥n0∥2 ≥
3ν2
0+ν2
max
4
d, then with probability at least 1 −T · exp(−Ω(d)), we have that
456"
REFERENCES,0.3261390887290168,"∥nt∥2 > ν2
0+ν2
max
2
d for all t ∈{0} ∪[T].
457"
REFERENCES,0.32673860911270985,"Proof of Proposition 3. To establish a lower bound on ∥nt∥, we consider different cases of the step
458"
REFERENCES,0.3273381294964029,"size δt. Intuitively, when δt is large enough, nt will be too noisy due to the introduction of random
459"
REFERENCES,0.32793764988009594,"noise √δtϵ(n)
t
in equation 5. While for small δt, the update of nt is bounded and thus we can
460"
REFERENCES,0.328537170263789,"iteratively analyze nt. We first handle the case of large δt in the following lemma.
461"
REFERENCES,0.329136690647482,"Lemma 3. If δt > ν2
0, with probability at least 1 −exp(−Ω(d)), for nt satisfying equation 5, we
462"
REFERENCES,0.32973621103117506,"have ∥nt∥2 ≥3ν2
0+ν2
max
4
d regardless of the previous state xt−1.
463"
REFERENCES,0.3303357314148681,Proof of Lemma 3. Denote v := nt−1 + δt
REFERENCES,0.33093525179856115,"2 NT ∇x log P(xt−1) for simplicity. Note that v is fixed
464"
REFERENCES,0.3315347721822542,"for any given xt−1. We decompose ϵ(n)
t
into a vector aligning with v and another vector orthogonal
465"
REFERENCES,0.33213429256594723,"to v. Consider an orthonormal matrix M ∈Rn×(n−1) such that MT v = 0n−1 and MT M = In−1.
466"
REFERENCES,0.3327338129496403,"By denoting u := ϵ(n)
t
−MMT ϵ(n)
t
we have MT u = 0n−1, thus we obtain
467"
REFERENCES,0.3333333333333333,"∥nt∥2 =
v +
p"
REFERENCES,0.33393285371702636,"δtϵ(n)
t

2"
REFERENCES,0.3345323741007194,"=
v +
p"
REFERENCES,0.3351318944844125,"δtu +
p"
REFERENCES,0.33573141486810554,"δtMMT ϵ(n)
t

2"
REFERENCES,0.3363309352517986,"=
v +
p"
REFERENCES,0.3369304556354916,"δtu

2
+

p"
REFERENCES,0.33752997601918466,"δtMMT ϵ(n)
t

2"
REFERENCES,0.3381294964028777,"≥

p"
REFERENCES,0.33872901678657075,"δtMMT ϵ(n)
t

2"
REFERENCES,0.3393285371702638,"≥ν2
0
MT ϵ(n)
t

2
."
REFERENCES,0.33992805755395683,"Since ϵ(n)
t
∼N(0n, In) and MT M = In−1, we obtain MT ϵ(n)
t
∼N(0n−1, In−1). Therefore, by
468"
REFERENCES,0.3405275779376499,"Lemma 1 we can bound
469"
REFERENCES,0.3411270983213429,"P

∥nt∥2 ≤3ν2
0 + ν2
max
4
d

≤P
MT ϵ(n)
t

2
≤3ν2
0 + ν2
max
4ν2
0
d
 = P "
REFERENCES,0.34172661870503596,"
MT ϵ(n)
t

2
≤d −2 s"
REFERENCES,0.342326139088729,"d ·
ν2
0 −ν2max 8ν2
0 2
d   ≤P "
REFERENCES,0.34292565947242204,"
MT ϵ(n)
t

2
≤(n −1) −2 s"
REFERENCES,0.3435251798561151,"(n −1)
ν2
0 −ν2max 8ν2
0 2 d 2   ≤exp "
REFERENCES,0.3441247002398082,"−
ν2
0 −ν2
max
8ν2
0 2 d 2 ! ,"
REFERENCES,0.3447242206235012,"where the second last step follows from the assumption d −n = r = o(d). Hence we complete the
470"
REFERENCES,0.34532374100719426,"proof of Lemma 3.
471"
REFERENCES,0.3459232613908873,"We then consider the case when δt ≤ν2
0. Let r := RT x and n := NT x, then x = Rr + Nn. We
472"
REFERENCES,0.34652278177458035,"first show that when ∥n∥2 ≥ν2
0+ν2
max
2
d, P (i)(x) is exponentially smaller than P (0)(x) for all i ∈[k]
473"
REFERENCES,0.3471223021582734,"in the following lemma.
474"
REFERENCES,0.34772182254196643,"Lemma 4. Given that ∥n∥2 ≥ν2
0+ν2
max
2
d and ∥µi∥2 ≤ν2
0−ν2
i
2

log

ν2
i
ν2
0"
REFERENCES,0.34832134292565947,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i"
REFERENCES,0.3489208633093525,"
d for all
475"
REFERENCES,0.34952038369304556,"i ∈[k], we have P (i)(x)"
REFERENCES,0.3501199040767386,"P (0)(x) ≤exp(−Ω(d)) for all i ∈[k].
476"
REFERENCES,0.35071942446043164,"Proof of Lemma 4. For all i ∈[k], define ρi(x) := P (i)(x)"
REFERENCES,0.3513189448441247,"P (0)(x), then
477"
REFERENCES,0.3519184652278177,ρi(x) = P (i)(x)
REFERENCES,0.35251798561151076,"P (0)(x) =
(2πν2
i )−d/2 exp

−1"
REFERENCES,0.35311750599520386,"2ν2
i ∥x −µi∥2"
REFERENCES,0.3537170263788969,"(2πν2
0)−d/2 exp

−1"
REFERENCES,0.35431654676258995,"2ν2
0 ∥x∥2"
REFERENCES,0.354916067146283,"=
ν2
0
ν2
i"
REFERENCES,0.35551558752997603,"d/2
exp
 1"
REFERENCES,0.35611510791366907,"2ν2
0
∥x∥2 −
1
2ν2
i
∥x −µi∥2
"
REFERENCES,0.3567146282973621,"=
ν2
0
ν2
i"
REFERENCES,0.35731414868105515,"d/2
exp  1"
REFERENCES,0.3579136690647482,"2ν2
0
−
1
2ν2
i"
REFERENCES,0.35851318944844124,"
∥Nn∥2 + ∥Rr∥2"
REFERENCES,0.3591127098321343,"2ν2
0
−∥Rr −µi∥2 2ν2
i !!"
REFERENCES,0.3597122302158273,"=
ν2
0
ν2
i"
REFERENCES,0.36031175059952036,"d/2
exp  1"
REFERENCES,0.3609112709832134,"2ν2
0
−
1
2ν2
i"
REFERENCES,0.36151079136690645,"
∥n∥2 + ∥r∥2"
REFERENCES,0.36211031175059955,"2ν2
0
−"
REFERENCES,0.3627098321342926,"r −RT µi
2 2ν2
i !! ,"
REFERENCES,0.36330935251798563,"where the last step follows from the definition that R ∈Rd×r an orthonormal basis of the vector space
478"
REFERENCES,0.36390887290167867,"{µi}i∈[k] and NT N = In. Since ν2
0 > ν2
i , the quadratic term ∥r∥2"
REFERENCES,0.3645083932853717,"2ν2
0 −∥r−RT µi∥
2"
REFERENCES,0.36510791366906475,"2ν2
i
is maximized at
479"
REFERENCES,0.3657074340527578,"r = ν2
0RT µi
ν2
0−ν2
i . Therefore,
480 ∥r∥2"
REFERENCES,0.36630695443645084,"2ν2
0
−"
REFERENCES,0.3669064748201439,"r −RT µi
2"
REFERENCES,0.3675059952038369,"2ν2
i
≤ν4
0
RT µi
2"
REFERENCES,0.36810551558752996,"2ν2
0(ν2
0 −ν2
i )2 −
1
2ν2
i"
REFERENCES,0.368705035971223,"
ν2
0
ν2
0 −ν2
i
−1
2 RT µi
2 =
∥µi∥2"
REFERENCES,0.36930455635491605,"2(ν2
0 −ν2
i )."
REFERENCES,0.3699040767386091,"Hence, for ∥n∥2 ≥ν2
0+ν2
max
2
d and ∥µi∥2 ≤ν2
0−ν2
i
2

log

ν2
i
ν2
0"
REFERENCES,0.37050359712230213,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i"
REFERENCES,0.37110311750599523,"
d, we have
481"
REFERENCES,0.37170263788968827,"ρi(x) =
ν2
0
ν2
i"
REFERENCES,0.3723021582733813,"d/2
exp  1"
REFERENCES,0.37290167865707435,"2ν2
0
−
1
2ν2
i"
REFERENCES,0.3735011990407674,"
∥n∥2 + ∥r∥2"
REFERENCES,0.37410071942446044,"2ν2
0
−"
REFERENCES,0.3747002398081535,"r −RT µi
2 2ν2
i !!"
REFERENCES,0.3752997601918465,"≤
ν2
0
ν2
i"
REFERENCES,0.37589928057553956,"d/2
exp  1"
REFERENCES,0.3764988009592326,"2ν2
0
−
1
2ν2
i"
REFERENCES,0.37709832134292565," ν2
0 + ν2
i
2
d +
∥µi∥2"
REFERENCES,0.3776978417266187,"2(ν2
0 −ν2
i ) ! = exp "
REFERENCES,0.37829736211031173,"−

log
ν2
i
ν2
0"
REFERENCES,0.37889688249400477,"
−ν2
i
2ν2
0
+ ν2
0
2ν2
i  d"
REFERENCES,0.37949640287769787,"2 +
∥µi∥2"
REFERENCES,0.3800959232613909,"2(ν2
0 −ν2
i ) !"
REFERENCES,0.38069544364508395,"≤exp

−

log
ν2
i
ν2
0"
REFERENCES,0.381294964028777,"
−ν2
i
2ν2
0
+ ν2
0
2ν2
i  d 4 
."
REFERENCES,0.38189448441247004,Notice that for function f(z) = log z −z 2 + 1
REFERENCES,0.3824940047961631,"2z, we have f(1) = 0 and
d
dzf(z) = 1 z −1"
REFERENCES,0.3830935251798561,"2 −
1
2z2 =
482 −1 2
  1"
REFERENCES,0.38369304556354916,"z −1
2 < 0 when z ∈(0, 1). Thus, log

ν2
i
ν2
0"
REFERENCES,0.3842925659472422,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i is a positive constant for νi < ν0,
483"
REFERENCES,0.38489208633093525,"i.e., ρi(x) = exp(−Ω(d)). Therefore we finish the proof of Lemma 4.
484"
REFERENCES,0.3854916067146283,"Lemma 4 implies that when ∥n∥is large, the Gaussian mode P (0) dominates other modes P (i). To
485"
REFERENCES,0.38609112709832133,"bound ∥nt∥, we first consider a simpler case that ∥nt−1∥is large. Intuitively, the following lemma
486"
REFERENCES,0.38669064748201437,"proves that when the previous state nt−1 is far from a mode, a single step of Langevin dynamics with
487"
REFERENCES,0.3872901678657074,"bounded step size is not enough to find the mode.
488"
REFERENCES,0.38788968824940045,"Lemma 5. Suppose δt ≤ν2
0 and ∥nt−1∥2 > 36ν2
0d, then for nt following from equation 5, we have
489"
REFERENCES,0.38848920863309355,"∥nt∥2 ≥ν2
0d with probability at least 1 −exp(−Ω(d)).
490"
REFERENCES,0.3890887290167866,"Proof of Lemma 5. From the recursion of nt in equation 5 we have
491"
REFERENCES,0.38968824940047964,nt = nt−1 + δt
REFERENCES,0.3902877697841727,"2 NT ∇x log P(xt−1) +
p"
REFERENCES,0.3908872901678657,"δtϵ(n)
t"
REFERENCES,0.39148681055155876,"= nt−1 −δt 2 k
X i=0"
REFERENCES,0.3920863309352518,P (i)(xt−1)
REFERENCES,0.39268585131894485,"P(xt−1)
· NT (xt−1 −µi)"
REFERENCES,0.3932853717026379,"ν2
i
+
p"
REFERENCES,0.39388489208633093,"δtϵ(n)
t =  1 −δt 2 k
X i=0"
REFERENCES,0.39448441247002397,P (i)(xt−1)
REFERENCES,0.395083932853717,"P(xt−1)
· 1 ν2
i !"
REFERENCES,0.39568345323741005,"nt−1 +
p"
REFERENCES,0.3962829736211031,"δtϵ(n)
t
.
(6)"
REFERENCES,0.39688249400479614,"By Lemma 4, we have P (i)(xj−1)"
REFERENCES,0.39748201438848924,"P (0)(xj−1) ≤exp(−Ω(d)) for all i ∈[k], therefore
492 1−δt 2 k
X i=0"
REFERENCES,0.3980815347721823,P (i)(xt−1)
REFERENCES,0.3986810551558753,P(xt−1) · 1
REFERENCES,0.39928057553956836,"ν2
i
≥1−δt 2 · 1"
REFERENCES,0.3998800959232614,"ν2
0
−δt 2 X i∈[k]"
REFERENCES,0.40047961630695444,"wiP (i)(xt−1)
w0P (0)(xt−1) · 1"
REFERENCES,0.4010791366906475,"ν2
i
≥1−1"
REFERENCES,0.40167865707434053,2 −exp(−Ω(d)) > 1 3. (7)
REFERENCES,0.40227817745803357,"On the other hand, from ϵ(n)
t
∼N(0n, In) we know ⟨nt−1,ϵ(n)
t
⟩
∥nt−1∥
∼N(0, 1) for any fixed nt−1 ̸= 0n,
493"
REFERENCES,0.4028776978417266,"hence by Lemma 2 we have
494 P"
REFERENCES,0.40347721822541965,"⟨nt−1, ϵ(n)
t
⟩
∥nt−1∥
≥ √ d
4 ! = P"
REFERENCES,0.4040767386091127,"⟨nt−1, ϵ(n)
t
⟩
∥nt−1∥
≤− √ d
4 ! ≤
4
√"
REFERENCES,0.40467625899280574,"2πd
exp

−d 32 
(8)"
REFERENCES,0.4052757793764988,"Combining equation 6, equation 7 and equation 8 gives that
495"
REFERENCES,0.4058752997601918,"∥nt∥2 ≥
1 3"
REFERENCES,0.4064748201438849,"2
∥nt−1∥2 −2ν0|⟨nt−1, ϵ(n)
t
⟩| ≥1"
REFERENCES,0.40707434052757796,"9 ∥nt−1∥2 −ν0
√"
REFERENCES,0.407673860911271,"d
2
∥nt−1∥ ≥1"
REFERENCES,0.40827338129496404,"9 · 36ν2
0d −ν0
√"
REFERENCES,0.4088729016786571,"d
2
· 6ν0
√ d"
REFERENCES,0.4094724220623501,"= ν2
0d"
REFERENCES,0.41007194244604317,"with probability at least 1 −
8
√"
REFERENCES,0.4106714628297362,"2πd exp
 
−d"
REFERENCES,0.41127098321342925,"32

= 1 −exp(−Ω(d)). This proves Lemma 5.
496"
REFERENCES,0.4118705035971223,"We then proceed to bound ∥nt∥iteratively for ∥nt−1∥2 ≤36ν2
0d. Recall that equation 5 gives
497"
REFERENCES,0.41247002398081534,nt = nt−1 + δt
REFERENCES,0.4130695443645084,"2 NT ∇x log P(xt−1) +
p"
REFERENCES,0.4136690647482014,"δtϵ(n)
t
."
REFERENCES,0.41426858513189446,"We notice that the difficulty of solving nt exhibits in the dependence of log P(xt−1) on rt−1. Since
498"
REFERENCES,0.4148681055155875,"P = Pk
i=0 wiP (i) = Pk
i=0 wiN(µi, ν2
i Id), we can rewrite the score function as
499"
REFERENCES,0.4154676258992806,∇x log P(x) = ∇xP(x)
REFERENCES,0.41606714628297364,"P(x)
= − k
X i=0"
REFERENCES,0.4166666666666667,P (i)(x)
REFERENCES,0.4172661870503597,"P(x)
· x −µi"
REFERENCES,0.41786570743405277,"ν2
i
= −x"
REFERENCES,0.4184652278177458,"ν2
0
+
X i∈[k]"
REFERENCES,0.41906474820143885,P (i)(x) P(x)  x
REFERENCES,0.4196642685851319,"ν2
0
−x −µi ν2
i 
. (9)"
REFERENCES,0.42026378896882494,"Now, instead of directly working with nt, we consider a surrogate recursion ˆnt such that ˆn0 = n0
500"
REFERENCES,0.420863309352518,"and for all t ≥1,
501"
REFERENCES,0.421462829736211,ˆnt = ˆnt−1 −δt
REFERENCES,0.42206235011990406,"2ν2
0
ˆnt−1 +
p"
REFERENCES,0.4226618705035971,"δtϵ(n)
t
.
(10)"
REFERENCES,0.42326139088729015,"The advantage of the surrogate recursion is that ˆnt is independent of r, thus we can obtain the
502"
REFERENCES,0.4238609112709832,"closed-form solution to ˆnt. Before we proceed to bound ˆnt, we first show that ˆnt is sufficiently close
503"
REFERENCES,0.4244604316546763,"to the original recursion nt in the following lemma.
504"
REFERENCES,0.4250599520383693,"Lemma 6. For any t ≥1, given that δj ≤ν2
0 and ν2
0+ν2
max
2
d ≤∥nj−1∥2 ≤36ν2
0d for all j ∈[t] and
505"
REFERENCES,0.42565947242206237,"∥µi∥2 ≤ν2
0−ν2
i
2

log

ν2
i
ν2
0"
REFERENCES,0.4262589928057554,"
−ν2
i
2ν2
0 + ν2
0
2ν2
i"
REFERENCES,0.42685851318944845,"
d for all i ∈[k], we have ∥ˆnt −nt∥≤
t
exp(Ω(d))
√"
REFERENCES,0.4274580335731415,"d.
506"
REFERENCES,0.42805755395683454,"Proof of Lemma 6. Upon comparing equation 5 and equation 10, by equation 9 we have that for all
507"
REFERENCES,0.4286570743405276,"j ∈[t],
508"
REFERENCES,0.4292565947242206,"∥ˆnj −nj∥=
ˆnj−1 −δj"
REFERENCES,0.42985611510791366,"2ν2
0
ˆnj−1 −nj−1 −δj"
REFERENCES,0.4304556354916067,2 NT ∇x log P(xj−1) = 
REFERENCES,0.43105515587529974,"
1 −δj 2ν2
0"
REFERENCES,0.4316546762589928,"
(ˆnj−1 −nj−1) + δj 2 X i∈[k]"
REFERENCES,0.43225419664268583,P (i)(xj−1)
REFERENCES,0.43285371702637887,P(xj−1)  1
REFERENCES,0.43345323741007197,"ν2
i
−1 ν2
0"
REFERENCES,0.434052757793765,"
nj−1 "
REFERENCES,0.43465227817745805,"≤

1 −δj 2ν2
0"
REFERENCES,0.4352517985611511,"
∥ˆnj−1 −nj−1∥+
X i∈[k] δj"
REFERENCES,0.43585131894484413,"2
P (i)(xj−1)"
REFERENCES,0.4364508393285372,P(xj−1)  1
REFERENCES,0.4370503597122302,"ν2
i
−1 ν2
0"
REFERENCES,0.43764988009592326,"
∥nj−1∥"
REFERENCES,0.4382494004796163,"≤∥ˆnj−1 −nj−1∥+
X i∈[k] δj"
REFERENCES,0.43884892086330934,"2
P (i)(xj−1)
P (0)(xj−1)  1"
REFERENCES,0.4394484412470024,"ν2
i
−1 ν2
0"
REFERENCES,0.44004796163069543,"
6ν0
√ d."
REFERENCES,0.44064748201438847,"By Lemma 4, we have P (i)(xj−1)"
REFERENCES,0.4412470023980815,"P (0)(xj−1) ≤exp(−Ω(d)) for all i ∈[k], hence we obtain a recursive bound
509"
REFERENCES,0.44184652278177455,"∥ˆnj −nj∥≤∥ˆnj−1 −nj−1∥+
1
exp(Ω(d)) √ d."
REFERENCES,0.44244604316546765,"Finally, by ˆn0 = n0, we have
510"
REFERENCES,0.4430455635491607,"∥ˆnt −nt∥=
X"
REFERENCES,0.44364508393285373,"j∈[t]
(∥ˆnj −nj∥−∥ˆnj−1 −nj−1∥) ≤
t
exp(Ω(d)) √ d."
REFERENCES,0.4442446043165468,"Hence we obtain Lemma 6.
511"
REFERENCES,0.4448441247002398,"We then proceed to analyze ˆnt, The following lemma gives us the closed-form solution of ˆnt. We
512"
REFERENCES,0.44544364508393286,"slightly abuse the notations here, e.g., Qc2
i=c1"
REFERENCES,0.4460431654676259,"
1 −
δi
2ν2
0"
REFERENCES,0.44664268585131894,"
= 1 and Pc2
j=c1 δj = 0 for c1 > c2.
513"
REFERENCES,0.447242206235012,"Lemma 7. For all t ≥0, ˆnt ∼N
Qt
i=1

1 −
δi
2ν2
0"
REFERENCES,0.447841726618705,"
n0, Pt
j=1
Qt
i=j+1

1 −
δi
2ν2
0"
REFERENCES,0.44844124700239807,"2
δjIn"
REFERENCES,0.4490407673860911,"
, where
514"
REFERENCES,0.44964028776978415,"the mean and covariance satisfy Qt
i=1

1 −
δi
2ν2
0"
REFERENCES,0.4502398081534772,"2
+
1
ν2
0
Pt
j=1
Qt
i=j+1

1 −
δi
2ν2
0"
REFERENCES,0.45083932853717024,"2
δj ≥1.
515"
REFERENCES,0.45143884892086333,"Proof of Lemma 7. We prove the two properties by induction. When t = 0, they are trivial. Suppose
516"
REFERENCES,0.4520383693045564,"they hold for t −1, then for the distribution of ˆnt, we have
517"
REFERENCES,0.4526378896882494,ˆnt = ˆnt−1 −δt
REFERENCES,0.45323741007194246,"2ν2
0
ˆnt−1 +
p"
REFERENCES,0.4538369304556355,"δtϵ(n)
t ∼N "
REFERENCES,0.45443645083932854,"

1 −δt 2ν2
0"
REFERENCES,0.4550359712230216," t−1
Y i=1"
REFERENCES,0.4556354916067146,"
1 −δi 2ν2
0"
REFERENCES,0.45623501199040767,"
n0,

1 −δt 2ν2
0"
REFERENCES,0.4568345323741007,"2 t−1
X j=1 t−1
Y i=j+1"
REFERENCES,0.45743405275779375,"
1 −δi 2ν2
0"
REFERENCES,0.4580335731414868,"2
δjIn + δtIn   = N  
tY i=1"
REFERENCES,0.45863309352517984,"
1 −δi 2ν2
0 
n0, t
X j=1 tY i=j+1"
REFERENCES,0.4592326139088729,"
1 −δi 2ν2
0"
REFERENCES,0.459832134292566,"2
δjIn  ."
REFERENCES,0.460431654676259,"For the second property,
518 tY i=1"
REFERENCES,0.46103117505995206,"
1 −δi 2ν2
0"
REFERENCES,0.4616306954436451,"2
+ 1 ν2
0 t
X j=1 tY i=j+1"
REFERENCES,0.46223021582733814,"
1 −δi 2ν2
0 2
δj"
REFERENCES,0.4628297362110312,"=

1 −δt 2ν2
0 2
"
REFERENCES,0.4634292565947242,"
t−1
Y i=1"
REFERENCES,0.46402877697841727,"
1 −δi 2ν2
0"
REFERENCES,0.4646282973621103,"2
+ 1 ν2
0 t−1
X j=1 t−1
Y i=j+1"
REFERENCES,0.46522781774580335,"
1 −δi 2ν2
0 2
δj  + 1"
REFERENCES,0.4658273381294964,"ν2
0
δt"
REFERENCES,0.46642685851318944,"≥

1 −δt 2ν2
0"
REFERENCES,0.4670263788968825,"2
+ 1"
REFERENCES,0.4676258992805755,"ν2
0
δt = 1 + δ2
t
4ν4
0
≥1."
REFERENCES,0.46822541966426856,"Hence we finish the proof of Lemma 7.
519"
REFERENCES,0.46882494004796166,"Armed with Lemma 7, we are now ready to establish the lower bound on ∥ˆnt∥. For simplicity,
520"
REFERENCES,0.4694244604316547,"denote α := Qt
i=1

1 −
δi
2ν2
0"
REFERENCES,0.47002398081534774,"2
and β :=
1
ν2
0
Pt
j=1
Qt
i=j+1

1 −
δi
2ν2
0"
REFERENCES,0.4706235011990408,"2
δj. By Lemma 7 we know
521"
REFERENCES,0.4712230215827338,"ˆnt ∼N(αn0, βν2
0In), so we can write ˆnt = αn0 + √βν0ϵ, where ϵ ∼N(0n, In).
522"
REFERENCES,0.47182254196642687,"Lemma 8. Given that ∥ˆn0∥2 ≥3ν2
0+ν2
max
4
d, we have ∥ˆnt∥2 ≥5ν2
0+3ν2
max
8
d with probability at least
523"
REFERENCES,0.4724220623501199,"1 −exp (−Ω(d)).
524"
REFERENCES,0.47302158273381295,"Proof of Lemma 8. By ˆnt = αn0 + √βν0ϵ we have
525"
REFERENCES,0.473621103117506,"∥ˆnt∥2 = α2 ∥n0∥2 + βν2
0 ∥ϵ∥2 + 2α
p"
REFERENCES,0.47422062350119903,"βν0⟨n0, ϵ⟩"
REFERENCES,0.4748201438848921,"By Lemma 1 we can bound
526"
REFERENCES,0.4754196642685851,"P

∥ϵ∥2 ≤3ν2
0 + ν2
max
4ν2
0
d

= P "
REFERENCES,0.47601918465227816,∥ϵ∥2 ≤d −2 s
REFERENCES,0.4766187050359712,"d ·
ν2
0 −ν2max 8ν2
0 2
d   ≤P "
REFERENCES,0.47721822541966424,∥ϵ∥2 ≤(n −1) −2 s
REFERENCES,0.47781774580335734,"(n −1)
ν2
0 −ν2max 8ν2
0 2 d 2   ≤exp "
REFERENCES,0.4784172661870504,"−
ν2
0 −ν2
max
8ν2
0 2 d 2 ! ,"
REFERENCES,0.4790167865707434,"where the second last step follows from the assumption d −n = r = o(d). Since ϵ ∼N(0n, In),
527"
REFERENCES,0.47961630695443647,"we know ⟨n0,ϵ⟩"
REFERENCES,0.4802158273381295,"∥n0∥∼N(0, 1). Therefore by Lemma 2,
528 P"
REFERENCES,0.48081534772182255,"⟨n0, ϵ⟩"
REFERENCES,0.4814148681055156,"∥n0∥≤−
ν2
0 −ν2
max
4ν0
p"
REFERENCES,0.48201438848920863,"3ν2
0 + ν2max √ d !"
REFERENCES,0.4826139088729017,"≤
4ν0
p"
REFERENCES,0.4832134292565947,"3ν2
0 + ν2max
√"
REFERENCES,0.48381294964028776,"2π(ν2
0 −ν2max)
√"
REFERENCES,0.4844124700239808,"d
exp

−
(ν2
0 −ν2
max)2d
32ν2
0(3ν2
0 + ν2max) "
REFERENCES,0.48501199040767384,"Conditioned on ∥ˆn0∥2 ≥3ν2
0+ν2
max
4
d, ∥ϵ∥2 > 3ν2
0+ν2
max
4ν2
0
d and
1
∥n0∥⟨n0, ϵ⟩> −
ν2
0−ν2
max
4ν0√"
REFERENCES,0.4856115107913669,"3ν2
0+ν2max √"
REFERENCES,0.4862110311750599,"d,
529"
REFERENCES,0.486810551558753,"since Lemma 7 gives α2 + β ≥1 we have
530"
REFERENCES,0.48741007194244607,"∥ˆnt∥2 = α2 ∥n0∥2 + βν2
0 ∥ϵ∥2 + 2α
p"
REFERENCES,0.4880095923261391,"βν0⟨n0, ϵ⟩"
REFERENCES,0.48860911270983215,"≥α2 ∥n0∥2 + βν2
0 ∥ϵ∥2 −2α
p"
REFERENCES,0.4892086330935252,"βν0 ∥n0∥
ν2
0 −ν2
max
4ν0
p"
REFERENCES,0.48980815347721823,"3ν2
0 + ν2max √ d"
REFERENCES,0.4904076738609113,"≥α2 ∥n0∥2 + βν2
0 ∥ϵ∥2 −2α
p"
REFERENCES,0.4910071942446043,"βν0 ∥n0∥∥ϵ∥·
ν2
0 −ν2
max
6ν2
0 + 2ν2max"
REFERENCES,0.49160671462829736,"≥

1 −
ν2
0 −ν2
max
6ν2
0 + 2ν2max"
REFERENCES,0.4922062350119904," 
α2 ∥n0∥2 + βν2
0 ∥ϵ∥2"
REFERENCES,0.49280575539568344,"≥5ν2
0 + 3ν2
max
6ν2
0 + 2ν2max"
REFERENCES,0.4934052757793765," 
α2 + β

· 3ν2
0 + ν2
max
4
d"
REFERENCES,0.4940047961630695,"≥5ν2
0 + 3ν2
max
8
d."
REFERENCES,0.49460431654676257,"Hence by union bound, we complete the proof of Lemma 8.
531"
REFERENCES,0.4952038369304556,"Upon having all the above lemmas, we are now ready to establish Proposition 3 by induction. Suppose
532"
REFERENCES,0.4958033573141487,"the theorem holds for all T values of 1, · · · , T −1. We consider the following 3 cases:
533"
REFERENCES,0.49640287769784175,"• If there exists some t ∈[T] such that δt > ν2
0, by Lemma 3 we know that with probability
534"
REFERENCES,0.4970023980815348,"at least 1 −exp(−Ω(d)), we have ∥nt∥2 ≥3ν2
0+ν2
max
4
d, thus the problem reduces to the two
535"
REFERENCES,0.49760191846522783,"sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can be solved by induction.
536"
REFERENCES,0.4982014388489209,"• Suppose δt ≤ν2
0 for all t ∈[T]. If there exists some t ∈[T] such that ∥nt−1∥2 > 36ν2
0d,
537"
REFERENCES,0.4988009592326139,"by Lemma 5 we know that with probability at least 1 −exp(−Ω(d)), we have ∥nt∥2 ≥
538"
REFERENCES,0.49940047961630696,"ν2
0d > 3ν2
0+ν2
max
4
d, thus the problem similarly reduces to the two sub-arrays n0, · · · , nt−1
539"
REFERENCES,0.5,"and nt, · · · , nT , which can be solved by induction.
540"
REFERENCES,0.500599520383693,"• Suppose δt ≤ν2
0 and ∥nt−1∥2 ≤36ν2
0d for all t ∈[T]. Conditioned on ∥nt−1∥2 >
541"
REFERENCES,0.5011990407673861,"ν2
0+ν2
max
2
d for all t ∈[T], by Lemma 6 we have that for T = exp(O(d)),
542"
REFERENCES,0.5017985611510791,∥ˆnT −nT ∥< r
REFERENCES,0.5023980815347722,"5ν2
0 + 3ν2max 8
− r"
REFERENCES,0.5029976019184652,"ν2
0 + ν2max 2 !
√ d."
REFERENCES,0.5035971223021583,"By Lemma 8 we have that with probability at least 1 −exp(−Ω(d)),
543"
REFERENCES,0.5041966426858513,"∥ˆnT ∥2 ≥5ν2
0 + 3ν2
max
8
d."
REFERENCES,0.5047961630695443,"Combining the two inequalities implies the desired bound
544"
REFERENCES,0.5053956834532374,∥nT ∥≥∥ˆnT ∥−∥ˆnT −nT ∥> r
REFERENCES,0.5059952038369304,"ν2
0 + ν2max 2
d."
REFERENCES,0.5065947242206235,"Hence by induction we obtain ∥nt∥2 > ν2
0+ν2
max
2
d for all t ∈[T] with probability at least
545"
REFERENCES,0.5071942446043165,(1 −(T −1) exp(−Ω(d))) · (1 −exp(−Ω(d))) ≥1 −T exp(−Ω(d)).
REFERENCES,0.5077937649880095,"Therefore we complete the proof of Proposition 3.
546"
REFERENCES,0.5083932853717026,"Finally, combining Propositions 2 and 3 finishes the proof of Theorem 1.
547"
REFERENCES,0.5089928057553957,"A.2
Proof of Theorem 2: Annealed Langevin Dynamics under Gaussian Mixtures
548"
REFERENCES,0.5095923261390888,"To establish Theorem 2, we first note from Proposition 1 that perturbing a Gaussian distribution
549"
REFERENCES,0.5101918465227818,"N(µ, ν2Id) with noise level σ results in a Gaussian distribution N(µ, (ν2 + σ2)Id). Therefore, for
550"
REFERENCES,0.5107913669064749,"a Gaussian mixture P = Pk
i=0 wiP (i) = Pk
i=0 wiN(µi, ν2
i Id), the perturbed distribution of noise
551"
REFERENCES,0.5113908872901679,"level σ is
552 Pσ = k
X"
REFERENCES,0.511990407673861,"i=0
wiN(µi, (ν2
i + σ2)Id)."
REFERENCES,0.512589928057554,"Similar to the proof of Theorem 1, we decompose
553"
REFERENCES,0.513189448441247,"xt = Rrt + Nnt, and ϵt = Rϵ(r)
t
+ Nϵ(n)
t
,"
REFERENCES,0.5137889688249401,"where R ∈Rd×r an orthonormal basis of the vector space {µi}i∈[k] and N ∈Rd×n an orthonormal
554"
REFERENCES,0.5143884892086331,"basis of the null space of {µi}i∈[k]. Now, we prove Theorem 2 by applying the techniques developed
555"
REFERENCES,0.5149880095923262,"in Appendix A.1 via substituting ν2 with ν2 + σ2
t at time step t.
556"
REFERENCES,0.5155875299760192,"First, by Proposition 2, suppose that the sample is initialized in the distribution P (0)
σ0 , then with
557"
REFERENCES,0.5161870503597122,"probability at least 1 −exp(−Ω(d)), we have
558"
REFERENCES,0.5167865707434053,"∥n0∥2 ≥3(ν2
0 + σ2
0) + (ν2
max + σ2
0)
4
d = 3ν2
0 + ν2
max + 4σ2
0
4
d.
(11)"
REFERENCES,0.5173860911270983,"Then, with the assumption that the initialization satisfies ∥n0∥2 ≥3ν2
0+ν2
max+4σ2
0
4
d, the following
559"
REFERENCES,0.5179856115107914,"proposition similar to Proposition 3 shows that ∥nt∥remains large with high probability.
560"
REFERENCES,0.5185851318944844,"Proposition 4. Consider a data distribution P satisfies the constraints specified in Theorem 2.
561"
REFERENCES,0.5191846522781774,"We follow annealed Langevin dynamics for T = exp(O(d)) steps with noise level cσ ≥σ0 ≥
562"
REFERENCES,0.5197841726618705,"σ1 ≥σ2 ≥· · · ≥σT ≥0 for some constant cσ > 0. Suppose that the initial sample satisfies
563"
REFERENCES,0.5203836930455635,"∥n0∥2 ≥3ν2
0+ν2
max+4σ2
0
4
d, then with probability at least 1 −T · exp(−Ω(d)), we have that ∥nt∥2 >
564"
REFERENCES,0.5209832134292566,"ν2
0+ν2
max+2σ2
t
2
d for all t ∈{0} ∪[T].
565"
REFERENCES,0.5215827338129496,"Proof of Proposition 4. We prove Proposition 4 by induction. Suppose the theorem holds for all T
566"
REFERENCES,0.5221822541966427,"values of 1, · · · , T −1. We consider the following 3 cases:
567"
REFERENCES,0.5227817745803357,"• If there exists some t ∈[T] such that δt > ν2
0 + σ2
t , by Lemma 3 we know that with proba-
568"
REFERENCES,0.5233812949640287,"bility at least 1 −exp(−Ω(d)), we have ∥nt∥2 ≥3(ν2
0+σ2
t )+(ν2
max+σ2
t )
4
d = 3ν2
0+ν2
max+4σ2
t
4
d,
569"
REFERENCES,0.5239808153477218,"thus the problem reduces to the two sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can
570"
REFERENCES,0.5245803357314148,"be solved by induction.
571"
REFERENCES,0.5251798561151079,"• Suppose δt ≤ν2
0 + σ2
t for all t ∈[T]. If there exists some t ∈[T] such that ∥nt−1∥2 >
572"
REFERENCES,0.5257793764988009,"36(ν2
0 + σ2
t−1)d ≥36(ν2
0 + σ2
t )d, by Lemma 5 we know that with probability at least
573"
REFERENCES,0.526378896882494,"1 −exp(−Ω(d)), we have ∥nt∥2 ≥(ν2
0 + σ2
t )d >
3ν2
0+ν2
max+4σ2
t
4
d, thus the problem
574"
REFERENCES,0.5269784172661871,"similarly reduces to the two sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can be solved
575"
REFERENCES,0.5275779376498801,"by induction.
576"
REFERENCES,0.5281774580335732,"• Suppose δt ≤ν2
0 + σ2
t and ∥nt−1∥2 ≤36(ν2
0 + σ2
t−1)d for all t ∈[T]. Consider a surrogate
577"
REFERENCES,0.5287769784172662,"sequence ˆnt such that ˆn0 = n0 and for all t ≥1,
578"
REFERENCES,0.5293764988009593,"ˆnt = ˆnt−1 −
δt
2ν2
0 + 2σ2
t
ˆnt−1 +
p"
REFERENCES,0.5299760191846523,"δtϵ(n)
t
."
REFERENCES,0.5305755395683454,"Since ν0 > νi and cσ ≥σt for all t ∈{0} ∪[T], we have ν2
i +c2
σ
ν2
0+c2σ ≥ν2
i +σ2
t
ν2
0+σ2
t . Notice that for
579"
REFERENCES,0.5311750599520384,function f(z) = log z −z
REFERENCES,0.5317745803357314,"2 +
1
2z, we have
d
dzf(z) = 1 z −1"
REFERENCES,0.5323741007194245,"2 −
1
2z2 = −1 2
  1"
REFERENCES,0.5329736211031175,"z −1
2 ≤0.
580"
REFERENCES,0.5335731414868106,"Thus, by the assumption
581"
REFERENCES,0.5341726618705036,"∥µi −µ0∥2 ≤ν2
0 −ν2
i
2"
REFERENCES,0.5347721822541966,"
log
ν2
i + c2
σ
ν2
0 + c2σ"
REFERENCES,0.5353717026378897,"
−ν2
i + c2
σ
2ν2
0 + c2σ
+ ν2
0 + c2
σ
2ν2
i + c2σ 
d,"
REFERENCES,0.5359712230215827,"we have that for all t ∈[T],
582"
REFERENCES,0.5365707434052758,"∥µi −µ0∥2 ≤ν2
0 −ν2
i
2"
REFERENCES,0.5371702637889688,"
log
ν2
i + σ2
t
ν2
0 + σ2
t"
REFERENCES,0.5377697841726619,"
−ν2
i + σ2
t
2ν2
0 + σ2
t
+ ν2
0 + σ2
t
2ν2
i + σ2
t 
d."
REFERENCES,0.5383693045563549,"Conditioned on ∥nt−1∥2 >
ν2
0+ν2
max+2σ2
t−1
2
d for all t ∈[T], by Lemma 6 we have that for
583"
REFERENCES,0.5389688249400479,"T = exp(O(d)),
584"
REFERENCES,0.539568345323741,∥ˆnT −nT ∥< r
REFERENCES,0.540167865707434,"5ν2
0 + 3ν2max + 8σ2
T
8
− r"
REFERENCES,0.5407673860911271,"ν2
0 + ν2max + 2σ2
T
2 !
√ d."
REFERENCES,0.5413669064748201,"By Lemma 8 we have that with probability at least 1 −exp(−Ω(d)),
585"
REFERENCES,0.5419664268585132,"∥ˆnT ∥2 ≥5ν2
0 + 3ν2
max + 8σ2
T
8
d."
REFERENCES,0.5425659472422062,"Combining the two inequalities implies the desired bound
586"
REFERENCES,0.5431654676258992,∥nT ∥≥∥ˆnT ∥−∥ˆnT −nT ∥> r
REFERENCES,0.5437649880095923,"ν2
0 + ν2max + 2σ2
T
2
d."
REFERENCES,0.5443645083932853,"Hence by induction we obtain ∥nt∥2 > ν2
0+ν2
max+2σ2
t
2
d for all t ∈{0} ∪[T] with probability
587"
REFERENCES,0.5449640287769785,"at least
588"
REFERENCES,0.5455635491606715,(1 −(T −1) exp(−Ω(d))) · (1 −exp(−Ω(d))) ≥1 −T exp(−Ω(d)).
REFERENCES,0.5461630695443646,"Therefore we complete the proof of Proposition 4.
589"
REFERENCES,0.5467625899280576,"Finally, combining equation 11 and Proposition 4 finishes the proof of Theorem 2.
590"
REFERENCES,0.5473621103117506,"A.3
Proof of Theorem 3: Langevin Dynamics under Sub-Gaussian Mixtures
591"
REFERENCES,0.5479616306954437,"The proof framework is similar to the proof of Theorem 1. To begin with, we validate Assumption
592"
REFERENCES,0.5485611510791367,"2.v. in the following lemma:
593"
REFERENCES,0.5491606714628298,"Lemma 9. For constants ν0, νi, cν, cL satisfying Assumptions 2.iii. and 2.iv., we have (1−cν)ν2
0−ν2
i
2(1−cν)
>
594"
AND LOG,0.5497601918465228,"0 and log
cνν2
i
(c2
L+cνcL)ν2
0 −
ν2
i
2(1−cν)ν2
0 + (1−cν)ν2
0
2ν2
i
> 0 are both positive constants.
595"
AND LOG,0.5503597122302158,"Proof of Lemma 9. From Assumption 2.iv. that ν2
0 > ν2
max
1−cν ≥
ν2
i
1−cν , we easily obtain (1−cν)ν2
0−ν2
i
2(1−cν)
>
596"
AND LOG,0.5509592326139089,"0 is a positive constant. For the second property, let f(z) := log
cνν2
i
(c2
L+cνcL)z −
ν2
i
2(1−cν)z + (1−cν)z"
AND LOG,0.5515587529976019,"2ν2
i
.
597"
AND LOG,0.552158273381295,"For any z >
ν2
i
1−cν , the derivative of f(z) satisfies
598"
AND LOG,0.552757793764988,"d
dz f(z) = −1"
AND LOG,0.5533573141486811,"z +
ν2
i
2(1 −cν)z2 + 1 −cν"
AND LOG,0.5539568345323741,"2ν2
i
=
ν2
i
2(1 −cν)"
AND LOG,0.5545563549160671,1 −cν
AND LOG,0.5551558752997602,"ν2
i
−1 z"
AND LOG,0.5557553956834532,"2
> 0."
AND LOG,0.5563549160671463,"Therefore, when 4(c2
L+cνcL)
cν(1−cν)
≤1, we have
599"
AND LOG,0.5569544364508393,"f(ν2
0) > f

ν2
i
1 −cν"
AND LOG,0.5575539568345323,"
= log cν(1 −cν)"
AND LOG,0.5581534772182254,"c2
L + cνcL
≥log 4 > 0."
AND LOG,0.5587529976019184,"When 4(c2
L+cνcL)
cν(1−cν)
> 1, we have
600"
AND LOG,0.5593525179856115,"f(ν2
0) > f
4(c2
L + cνcL)
cν(1 −cν)
ν2
i
1 −cν"
AND LOG,0.5599520383693045,"
= 2 log
cν(1 −cν)
2(c2
L + cνcL) −
cν(1 −cν)
8(c2
L + cνcL) + 2(c2
L + cνcL)
cν(1 −cν)"
AND LOG,0.5605515587529976,"≥2 −2 log 2 −2(c2
L + cνcL)
cν(1 −cν)
−
cν(1 −cν)
8(c2
L + cνcL) + 2(c2
L + cνcL)
cν(1 −cν)
> 2 −2 log 2 −1"
AND LOG,0.5611510791366906,2 > 0.
AND LOG,0.5617505995203836,"Thus we obtain Lemma 9.
601"
AND LOG,0.5623501199040767,"Without loss of generality, we assume µ0 = 0d. Similar to the proof of Theorem 1, we decompose
602"
AND LOG,0.5629496402877698,"xt = Rrt + Nnt, and ϵt = Rϵ(r)
t
+ Nϵ(n)
t
,"
AND LOG,0.5635491606714629,"where R ∈Rd×r an orthonormal basis of the vector space {µi}i∈[k] and N ∈Rd×n an orthonormal
603"
AND LOG,0.5641486810551559,"basis of the null space of {µi}i∈[k]. To show ∥xt −µi∥2 >

ν2
0
2 +
ν2
max
2(1−cν)

d, it suffices to prove
604"
AND LOG,0.564748201438849,"∥nt∥2 >

ν2
0
2 +
ν2
max
2(1−cν)

d. By Proposition 2, if x0 is initialized in the distribution P (0), i.e.,
605"
AND LOG,0.565347721822542,"x0 ∼P (0), since ν2
0 >
1
1−cν ν2
max, with probability at least 1 −exp(−Ω(d)) we have
606"
AND LOG,0.565947242206235,"∥n0∥2 ≥
3ν2
0
4
+
ν2
max
4(1 −cν)"
AND LOG,0.5665467625899281,"
d.
(12)"
AND LOG,0.5671462829736211,"Then, conditioned on ∥n0∥2 ≥

3ν2
0
4 +
ν2
max
4(1−cν)

d, the following proposition shows that ∥nt∥
607"
AND LOG,0.5677458033573142,"remains large with high probability.
608"
AND LOG,0.5683453237410072,"Proposition 5. Consider a distribution P satisfying Assumption 2. We follow the Langevin dynamics
609"
AND LOG,0.5689448441247003,"for T = exp(O(d)) steps. Suppose that the initial sample satisfies ∥n0∥2 ≥

3ν2
0
4 +
ν2
max
4(1−cν)

d,
610"
AND LOG,0.5695443645083933,"then with probability at least 1 −T · exp(−Ω(d)), we have that ∥nt∥2 >

ν2
0
2 +
ν2
max
2(1−cν)

d for all
611"
AND LOG,0.5701438848920863,"t ∈{0} ∪[T].
612"
AND LOG,0.5707434052757794,"Proof of Proposition 5. Firstly, by Lemma 3, if δt > ν2
0, since ν2
0 > ν2
max
1−cν , we similarly have that
613"
AND LOG,0.5713429256594724,"∥nt∥2 ≥

3ν2
0
4 +
ν2
max
4(1−cν)

d with probability at least 1 −exp(−Ω(d)) regardless of the previous
614"
AND LOG,0.5719424460431655,"state xt−1. We then consider the case when δt ≤ν2
0. Intuitively, we aim to prove that the score
615"
AND LOG,0.5725419664268585,function is close to −x
AND LOG,0.5731414868105515,"ν2
0 when ∥n∥2 ≥

ν2
0
2 +
ν2
max
2(1−cν)

d. Towards this goal, we first show that
616"
AND LOG,0.5737410071942446,"P (0)(x) is exponentially larger than P (i)(x) for all i ∈[k] in the following lemma:
617"
AND LOG,0.5743405275779376,"Lemma 10. Suppose P satisfies Assumption 2. Then for any ∥n∥2 ≥

ν2
0
2 +
ν2
max
2(1−cν)

d, we have
618"
AND LOG,0.5749400479616307,"P (i)(x)
P (0)(x) ≤exp(−Ω(d)) and ∥∇xP (i)(x)∥"
AND LOG,0.5755395683453237,"P (x)
≤exp(−Ω(d)) for all i ∈[k].
619"
AND LOG,0.5761390887290168,"Proof of Lemma 10. We first give an upper bound on the sub-Gaussian probability density. For any
620"
AND LOG,0.5767386091127098,"vector v ∈Rd, by considering some vector m ∈Rd, from Markov’s inequality and the definition in
621"
AND LOG,0.5773381294964028,"equation 4 we can bound
622"
AND LOG,0.5779376498800959,"Pz∼P (i)
 
mT (z −µi) ≥mT (v −µi)

≤Ez∼P (i)

exp
 
mT (z −µi)
"
AND LOG,0.5785371702637889,exp (mT (v −µi)) ≤exp
AND LOG,0.579136690647482,"ν2
i ∥m∥2"
AND LOG,0.579736211031175,"2
−mT (v −µi) ! ."
AND LOG,0.580335731414868,Upon optimizing the last term at m = v−µi
AND LOG,0.5809352517985612,"ν2
i
, we obtain
623"
AND LOG,0.5815347721822542,"Pz∼P (i)
 
(v −µi)T (v −z) ≤0

≤exp "
AND LOG,0.5821342925659473,"−∥v −µi∥2 2ν2
i !"
AND LOG,0.5827338129496403,".
(13)"
AND LOG,0.5833333333333334,"Denote B :=

z : (v −µi)T (v −z) ≤0
	
. To bound Pz∼P (i)(z ∈B), we first note that
624"
AND LOG,0.5839328537170264,"log P (i)(v) −log P (i)(z) =
Z 1"
AND LOG,0.5845323741007195,"0
⟨v −z, ∇log P (i)(v + λ(z −v))⟩dλ"
AND LOG,0.5851318944844125,"= ⟨v −z, ∇log P (i)(v)⟩+
Z 1"
AND LOG,0.5857314148681055,"0
⟨v −z, ∇log P (i)(v + λ(z −v)) −∇log P (i)(v)⟩dλ"
AND LOG,0.5863309352517986,"≤∥v −z∥
∇log P (i)(v)
 +
Z 1"
AND LOG,0.5869304556354916,"0
∥v −z∥
∇log P (i)(v + λ(z −v)) −∇log P (i)(v)
 dλ"
AND LOG,0.5875299760191847,"≤∥v −z∥· Li ∥v −µi∥+
Z 1"
AND LOG,0.5881294964028777,"0
∥v −z∥· Li ∥λ(z −v)∥dλ
(14) ≤Licν"
CL,0.5887290167865707,"2cL
∥v −µi∥2 +
cL + cν 2cν"
CL,0.5893285371702638,"
Li ∥v −z∥2 ,"
CL,0.5899280575539568,"where equation 14 follows from Assumption 2.ii. that ∇log P (i)(µi) = 0d and Assumption 2.iii.
625"
CL,0.5905275779376499,"that the score function ∇log P (i) is Li-Lipschitz. Therefore we obtain
626"
CL,0.5911270983213429,"Pz∼P (i)(z ∈B) =
Z"
CL,0.591726618705036,"z∈B
P (i)(z) dz ≥
Z"
CL,0.592326139088729,"z∈B
P (i)(v) exp

−Licν"
CL,0.592925659472422,"2cL
∥v −µi∥2 −cL + cν"
CL,0.5935251798561151,"2cν
Li ∥v −z∥2

dz"
CL,0.5941247002398081,"= P (i)(v) exp

−Licν"
CL,0.5947242206235012,"2cL
∥v −µi∥2
 Z"
CL,0.5953237410071942,"z∈B
exp

−cL + cν"
CL,0.5959232613908872,"2cν
Li ∥v −z∥2

dz.
(15)"
CL,0.5965227817745803,"By observing that g : B →

z : (v −µi)T (v −z) ≥0
	
with g(z) = 2v −z is a bijection such that
627"
CL,0.5971223021582733,"∥v −z∥= ∥v −g(z)∥for any z ∈B, we have
628 Z"
CL,0.5977218225419664,"z∈B
exp

−cL + cν"
CL,0.5983213429256595,"2cν
Li ∥v −z∥2

dz = 1 2 Z"
CL,0.5989208633093526,"z∈Rd exp

−cL + cν"
CL,0.5995203836930456,"2cν
Li ∥v −z∥2

dz = 1 2"
CL,0.6001199040767387,"
2πcν
(cL + cν)Li  d"
CL,0.6007194244604317,"2
.
(16)"
CL,0.6013189448441247,"Hence, by combining equation 13, equation 15, and equation 16, we obtain
629 exp "
CL,0.6019184652278178,"−∥v −µi∥2 2ν2
i !"
CL,0.6025179856115108,"≥Pz∼P (i)
 
(v −µi)T (v −z) ≤0
"
CL,0.6031175059952039,"≥P (i)(v) exp

−Licν"
CL,0.6037170263788969,"2cL
∥v −µi∥2

· 1 2"
CL,0.60431654676259,"
2πcν
(cL + cν)Li  d 2
."
CL,0.604916067146283,By Assumption 2.iii. that Li ≤cL
CL,0.605515587529976,"ν2
i we obtain the following bound on the probability density:
630"
CL,0.6061151079136691,"P (i)(v) ≤2

2πcνν2
i
(cL + cν)cL −d"
EXP,0.6067146282973621,"2
exp

−1 −cν"
EXP,0.6073141486810552,"2ν2
i
∥v −µi∥2

.
(17)"
EXP,0.6079136690647482,"Then we can bound the ratio of P (i) and P (0). For all i ∈[k], define ρi(x) := P (i)(x)"
EXP,0.6085131894484412,"P (0)(x), then we have
631"
EXP,0.6091127098321343,ρi(x) = P (i)(x)
EXP,0.6097122302158273,"P (0)(x) ≤
2(2πcνν2
i /(c2
L + cνcL))−d/2 exp

−(1 −cν) ∥x −µi∥2 /2ν2
i
"
EXP,0.6103117505995204,"(2πν2
0)−d/2 exp

−∥x∥2 /2ν2
0
"
EXP,0.6109112709832134,"= 2
(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.6115107913669064,"2
exp ∥x∥2"
EXP,0.6121103117505995,"2ν2
0
−(1 −cν) ∥x −µi∥2 2ν2
i !"
EXP,0.6127098321342925,"= 2
(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.6133093525179856,"2
exp  1"
EXP,0.6139088729016786,"2ν2
0
−1 −cν 2ν2
i"
EXP,0.6145083932853717,"
∥Nn∥2 + ∥Rr∥2"
EXP,0.6151079136690647,"2ν2
0
−(1 −cν) ∥Rr −µi∥2 2ν2
i !!"
EXP,0.6157074340527577,"= 2
(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.6163069544364509,"2
exp  1"
EXP,0.6169064748201439,"2ν2
0
−1 −cν 2ν2
i"
EXP,0.617505995203837,"
∥n∥2 + ∥r∥2"
EXP,0.61810551558753,"2ν2
0
−(1 −cν)
r −RT µi
2 2ν2
i !! ,"
EXP,0.6187050359712231,"where the last step follows from the definition that R ∈Rd×r an orthogonal basis of the vector space
632"
EXP,0.6193045563549161,"{µi}i∈[k] and NT N = In. Since ν2
i < (1 −cν)ν2
0, the quadratic term ∥r∥2"
EXP,0.6199040767386091,"2ν2
0 −
(1−cν)∥r−RT µi∥
2"
EXP,0.6205035971223022,"2ν2
i
is
633"
EXP,0.6211031175059952,"maximized at r = (1−cν)ν2
0RT µi
(1−cν)ν2
0−ν2
i . Therefore, we obtain
634 ∥r∥2"
EXP,0.6217026378896883,"2ν2
0
−(1 −cν)
r −RT µi
2"
EXP,0.6223021582733813,"2ν2
i
≤
(1 −cν) ∥µi∥2"
EXP,0.6229016786570744,"2((1 −cν)ν2
0 −ν2
i )."
EXP,0.6235011990407674,"Hence, for ∥µi −µ0∥2 ≤(1−cν)ν2
0−ν2
i
2(1−cν)

log
cνν2
i
(c2
L+cνcL)ν2
0 −
ν2
i
2(1−cν)ν2
0 + (1−cν)ν2
0
2ν2
i"
EXP,0.6241007194244604,"
d and ∥n∥2 ≥
635

ν2
0
2 +
ν2
max
2(1−cν)

d, we have
636"
EXP,0.6247002398081535,"ρi(x) ≤2
(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.6252997601918465,"2
exp  1"
EXP,0.6258992805755396,"2ν2
0
−1 −cν 2ν2
i"
EXP,0.6264988009592326,"
∥n∥2 +
(1 −cν) ∥µi∥2"
EXP,0.6270983213429256,"2((1 −cν)ν2
0 −ν2
i ) !"
EXP,0.6276978417266187,"≤2
(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.6282973621103117,"2
exp  1"
EXP,0.6288968824940048,"2ν2
0
−1 −cν 2ν2
i"
EXP,0.6294964028776978," ν2
0
2 +
ν2
i
2(1 −cν)"
EXP,0.6300959232613909,"
d +
(1 −cν) ∥µi∥2"
EXP,0.6306954436450839,"2((1 −cν)ν2
0 −ν2
i ) !"
EXP,0.6312949640287769,= 2 exp 
EXP,0.63189448441247,"−

log
cνν2
i
(c2
L + cνcL)ν2
0
−
ν2
i
2(1 −cν)ν2
0
+ (1 −cν)ν2
0
2ν2
i  d"
EXP,0.632494004796163,"2 +
(1 −cν) ∥µi∥2"
EXP,0.6330935251798561,"2((1 −cν)ν2
0 −ν2
i ) !"
EXP,0.6336930455635491,"≤2 exp

−

log
cνν2
i
(c2
L + cνcL)ν2
0
−
ν2
i
2(1 −cν)ν2
0
+ (1 −cν)ν2
0
2ν2
i  d 4 
."
EXP,0.6342925659472423,"From Lemma 9, we obtain ρi(x) ≤exp(−Ω(d)).
637"
EXP,0.6348920863309353,To show ∥∇xP (i)(x)∥
EXP,0.6354916067146283,"P (x)
≤exp(−Ω(d)), from Assumptions 2.ii. and 2.iii. we have
638"
EXP,0.6360911270983214,∇xP (i)(x)
EXP,0.6366906474820144,P (i)(x)
EXP,0.6372901678657075,"=

∇xP (i)(x)"
EXP,0.6378896882494005,"P (i)(x)
−∇xP (i)(µi)"
EXP,0.6384892086330936,P (i)(µi)
EXP,0.6390887290167866,"=
∇x log P (i)(x) −∇x log P (i)(µi)"
EXP,0.6396882494004796,≤Li ∥x −µi∥≤cL
EXP,0.6402877697841727,"ν2
i
∥x −µi∥."
EXP,0.6408872901678657,"Therefore, we can bound ∥∇xP (i)(x)∥"
EXP,0.6414868105515588,"P (x)
≤
cL
ν2
i ρi(x) ∥x −µi∥. When ∥x −µi∥= exp(o(d)) is
639"
EXP,0.6420863309352518,"small, by ρi(x) ≤exp(−Ω(d)) we directly have ∥∇xP (i)(x)∥"
EXP,0.6426858513189448,"P (x)
≤exp(−Ω(d)). When ∥x −µi∥=
640"
EXP,0.6432853717026379,"exp(Ω(d)) is exceedingly large, from equation 17 we have
641"
EXP,0.6438848920863309,∇xP (i)(x)
EXP,0.644484412470024,"P(x)
≤2cL ν2
i"
EXP,0.645083932853717,"(c2
L + cνcL)ν2
0
cνν2
i  d"
EXP,0.64568345323741,"2
exp ∥x∥2"
EXP,0.6462829736211031,"2ν2
0
−(1 −cν) ∥x −µi∥2 2ν2
i !"
EXP,0.6468824940047961,∥x −µi∥.
EXP,0.6474820143884892,"Since ν2
0 >
ν2
i
1−cν , when ∥x −µi∥= exp(Ω(d)) ≫∥µi∥we have
642 exp ∥x∥2"
EXP,0.6480815347721822,"2ν2
0
−(1 −cν) ∥x −µi∥2 2ν2
i !"
EXP,0.6486810551558753,= exp(−Ω(∥x −µi∥2)).
EXP,0.6492805755395683,Therefore ∥∇xP (i)(x)∥
EXP,0.6498800959232613,"P (x)
≤exp(−Ω(d)). Thus we complete the proof of Lemma 10.
643"
EXP,0.6504796163069544,"Similar to Lemma 5, the following lemma proves that when the previous state nt−1 is far from a
644"
EXP,0.6510791366906474,"mode, a single step of Langevin dynamics with bounded step size is not enough to find the mode.
645"
EXP,0.6516786570743405,"Lemma 11. Suppose δt ≤ν2
0 and ∥nt−1∥2 > 36ν2
0d, then we have ∥nt∥2 ≥ν2
0d with probability at
646"
EXP,0.6522781774580336,"least 1 −exp(−Ω(d)).
647"
EXP,0.6528776978417267,"Proof of Lemma 11. For simplicity, denote v := nt−1 + δt"
EXP,0.6534772182254197,"2 NT ∇x log P(xt−1).
Since P =
648
Pk
i=0 wiP (i) and P (0) = N(µ0, ν2
0Id), the score function can be written as
649"
EXP,0.6540767386091128,∇x log P(x) = ∇xP(x)
EXP,0.6546762589928058,"P(x)
= ∇xw0P (0)(x)"
EXP,0.6552757793764988,"P(x)
+
X i∈[k]"
EXP,0.6558752997601919,∇xwiP (i)(x) P(x)
EXP,0.6564748201438849,= −w0P (0)(x)
EXP,0.657074340527578,"P(x)
· x"
EXP,0.657673860911271,"ν2
0
+
X i∈[k]"
EXP,0.658273381294964,wi∇xP (i)(x) P(x) = −x
EXP,0.6588729016786571,"ν2
0
+
X i∈[k]"
EXP,0.6594724220623501,wiP (i)(x)
EXP,0.6600719424460432,"P(x)
· x"
EXP,0.6606714628297362,"ν2
0
+
X i∈[k]"
EXP,0.6612709832134293,wi∇xP (i)(x)
EXP,0.6618705035971223,"P(x)
.
(18)"
EXP,0.6624700239808153,"For ∥nt−1∥2 > 36ν2
0d by Lemma 10 we have ∥∇xP (i)(xt−1)∥"
EXP,0.6630695443645084,"P (xt−1)
≤exp(−Ω(d)). Since δt ≤ν2
0, we
650"
EXP,0.6636690647482014,"can bound the norm of v by
651"
EXP,0.6642685851318945,"∥v∥=
nt−1 + δt"
EXP,0.6648681055155875,2 NT ∇x log P(xt−1) =
EXP,0.6654676258992805,nt−1 −δt
EXP,0.6660671462829736,"2ν2
0
nt−1 +
X i∈[k] wiδt 2ν2
0"
EXP,0.6666666666666666,P (i)(xt−1)
EXP,0.6672661870503597,"P(xt−1) nt−1 +
X i∈[k] wiδt"
EXP,0.6678657074340527,"2
NT ∇xP (i)(xt−1)"
EXP,0.6684652278177458,P(xt−1)  ≥  
EXP,0.6690647482014388,1 −δt
EXP,0.669664268585132,"2ν2
0
+
X i∈[k] wiδt 2ν2
0"
EXP,0.670263788968825,P (i)(xt−1)
EXP,0.670863309352518,"P(xt−1)  nt−1 −
X i∈[k] wiδt 2"
EXP,0.6714628297362111,∇xP (i)(xt−1)
EXP,0.6720623501199041,P(xt−1) ≥1
EXP,0.6726618705035972,"2 ∥nt−1∥−
X i∈[k] wiδt"
EXP,0.6732613908872902,"2
exp(−Ω(d))"
EXP,0.6738609112709832,"> 2ν0
√ d."
EXP,0.6744604316546763,"On the other hand, from ϵ(n)
t
∼N(0n, In) we know ⟨v,ϵ(n)
t
⟩
∥v∥
∼N(0, 1) for any fixed v ̸= 0n, hence
652"
EXP,0.6750599520383693,"by Lemma 2 we have
653 P"
EXP,0.6756594724220624,"⟨v, ϵ(n)
t
⟩
∥v∥
≥ √ d
4 ! = P"
EXP,0.6762589928057554,"⟨v, ϵ(n)
t
⟩
∥v∥
≤− √ d
4 ! ≤
4
√"
EXP,0.6768585131894485,"2πd
exp

−d 32 "
EXP,0.6774580335731415,"Combining the above inequalities gives
654"
EXP,0.6780575539568345,"∥nt∥2 =
v +
p"
EXP,0.6786570743405276,"δtϵ(n)
t

2
≥∥v∥2 −2ν0|⟨v, ϵ(n)
t
⟩| ≥∥v∥2 −ν0
√"
EXP,0.6792565947242206,"d
2
∥v∥> ν2
0d"
EXP,0.6798561151079137,"with probability at least 1 −
8
√"
EXP,0.6804556354916067,"2πd exp
 
−d"
EXP,0.6810551558752997,"32

= 1 −exp(−Ω(d)). This proves Lemma 11.
655"
EXP,0.6816546762589928,"When ∥nt−1∥2 ≤36ν2
0d, similar to Theorem 1, we consider a surrogate recursion ˆnt such that
656"
EXP,0.6822541966426858,"ˆn0 = n0 and for all t ≥1,
657"
EXP,0.6828537170263789,ˆnt = ˆnt−1 −δt
EXP,0.6834532374100719,"2ν2
0
ˆnt−1 +
p"
EXP,0.684052757793765,"δtϵ(n)
t
.
(19)"
EXP,0.684652278177458,"The following Lemma shows that ˆnt is sufficiently close to the original recursion nt.
658"
EXP,0.685251798561151,"Lemma 12. For any t ≥1, given that for all j ∈[t], δj ≤ν2
0 and

ν2
0
2 +
ν2
max
2(1−cν)

d ≤∥nj−1∥2 ≤
659"
EXP,0.6858513189448441,"36ν2
0d, if µi satisfies Assumption 2.v. for all i ∈[k], we have ∥ˆnt −nt∥≤
t
exp(Ω(d))
√"
EXP,0.6864508393285371,"d.
660"
EXP,0.6870503597122302,"Proof of Lemma 12. By equation 18 we have that for all j ∈[t],
661"
EXP,0.6876498800959233,"∥ˆnj −nj∥=
ˆnj−1 −nj−1 −δj"
EXP,0.6882494004796164,"2ν2
0
ˆnj−1 −δj"
EXP,0.6888489208633094,2 NT ∇x log P(xj−1) =
EXP,0.6894484412470024,"ˆnj−1 −nj−1 −
X i∈[k]"
EXP,0.6900479616306955,wiP (i)(xj−1)
EXP,0.6906474820143885,"ν2
0P(xj−1) nj−1 −
X i∈[k]"
EXP,0.6912470023980816,wiNT ∇xP (i)(xj−1)
EXP,0.6918465227817746,P(xj−1) 
EXP,0.6924460431654677,"≤∥ˆnj−1 −nj−1∥+
X i∈[k]"
EXP,0.6930455635491607,wiP (i)(xj−1)
EXP,0.6936450839328537,"ν2
0P(xj−1) ∥nj−1∥+
X i∈[k]"
EXP,0.6942446043165468,"wi
∇xP (i)(xj−1)"
EXP,0.6948441247002398,"P(xj−1)
."
EXP,0.6954436450839329,"By Lemma 10, we have P (i)(xj−1)"
EXP,0.6960431654676259,P (0)(xj−1) ≤exp(−Ω(d)) and ∥∇xP (i)(xj−1)∥
EXP,0.6966426858513189,"P (xj−1)
≤exp(−Ω(d)) for all
662"
EXP,0.697242206235012,"i ∈[k], hence from ∥nj−1∥≤6ν0
√"
EXP,0.697841726618705,"d we obtain a recursive bound
663"
EXP,0.6984412470023981,"∥ˆnj −nj∥≤∥ˆnj−1 −nj−1∥+
1
exp(Ω(d)) √ d."
EXP,0.6990407673860911,"Finally, by ˆn0 = n0, we have
664"
EXP,0.6996402877697842,"∥ˆnt −nt∥=
X"
EXP,0.7002398081534772,"j∈[t]
(∥ˆnj −nj∥−∥ˆnj−1 −nj−1∥) ≤
t
exp(Ω(d)) √ d."
EXP,0.7008393285371702,"Hence we obtain Lemma 12.
665"
EXP,0.7014388489208633,"Armed with the above lemmas, we are now ready to establish Proposition 5 by induction. Please
666"
EXP,0.7020383693045563,"note that we also apply some lemmas from the proof of Theorem 1 by substituting ν2
max with ν2
max
1−cν .
667"
EXP,0.7026378896882494,"Suppose the theorem holds for all T values of 1, · · · , T −1. We consider the following 3 cases:
668"
EXP,0.7032374100719424,"• If there exists some t ∈[T] such that δt > ν2
0, by Lemma 3 we know that with probability
669"
EXP,0.7038369304556354,"at least 1 −exp(−Ω(d)), we have ∥nt∥2 ≥

3ν2
0
4 +
ν2
max
4(1−cν)

d, thus the problem reduces
670"
EXP,0.7044364508393285,"to the two sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can be solved by induction.
671"
EXP,0.7050359712230215,"• Suppose δt ≤ν2
0 for all t ∈[T]. If there exists some t ∈[T] such that ∥nt−1∥2 > 36ν2
0d, by
672"
EXP,0.7056354916067147,"Lemma 11 we know that with probability at least 1−exp(−Ω(d)), we have ∥nt∥2 ≥ν2
0d >
673

3ν2
0
4 +
ν2
max
4(1−cν)

d, thus the problem similarly reduces to the two sub-arrays n0, · · · , nt−1
674"
EXP,0.7062350119904077,"and nt, · · · , nT , which can be solved by induction.
675"
EXP,0.7068345323741008,"• Suppose δt ≤ν2
0 and ∥nt−1∥2 ≤36ν2
0d for all t ∈[T]. Conditioned on ∥nt−1∥2 >
676

ν2
0
2 +
ν2
max
2(1−cν)

d for all t ∈[T], by Lemma 12 we have that for T = exp(O(d)),
677"
EXP,0.7074340527577938,∥ˆnT −nT ∥< s
EXP,0.7080335731414868,"5ν2
0
8
+
3ν2max
8(1 −cν) − s"
EXP,0.7086330935251799,"ν2
0
2 +
ν2max
2(1 −cν) !
√ d."
EXP,0.7092326139088729,"By Lemma 8 we have that with probability at least 1 −exp(−Ω(d)),
678"
EXP,0.709832134292566,"∥ˆnT ∥2 ≥
5ν2
0
8
+
3ν2
max
8(1 −cν) 
d."
EXP,0.710431654676259,"Combining the two inequalities implies the desired bound
679"
EXP,0.7110311750599521,∥nT ∥≥∥ˆnT ∥−∥ˆnT −nT ∥>
EXP,0.7116306954436451,"sν2
0
2 +
ν2max
2(1 −cν) 
d."
EXP,0.7122302158273381,"Hence by induction we obtain ∥nt∥2 >

ν2
0
2 +
ν2
max
2(1−cν)

d for all t ∈[T] with probability
680"
EXP,0.7128297362110312,"at least
681"
EXP,0.7134292565947242,(1 −(T −1) exp(−Ω(d))) · (1 −exp(−Ω(d))) ≥1 −T exp(−Ω(d)).
EXP,0.7140287769784173,"Therefore we complete the proof of Proposition 5.
682"
EXP,0.7146282973621103,"Finally, combining equation 12 and Proposition 5 finishes the proof of Theorem 3.
683"
EXP,0.7152278177458034,"A.4
Proof of Theorem 4: Annealed Langevin Dynamics under Sub-Gaussian Mixtures
684"
EXP,0.7158273381294964,"Assumption 3. Consider a data distribution P := Pk
i=0 wiP (i) as a mixture of sub-Gaussian
685"
EXP,0.7164268585131894,"distributions, where 1 ≤k = o(d) and wi > 0 is a positive constant such that Pk
i=0 wi = 1.
686"
EXP,0.7170263788968825,"Suppose that P (0) = N(µ0, ν2
0Id) is Gaussian and for all i ∈[k], P (i) satisfies
687"
EXP,0.7176258992805755,"i. P (i) is a sub-Gaussian distribution of mean µi with parameter ν2
i ,
688"
EXP,0.7182254196642686,"ii. P (i) is differentiable and ∇P (i)
σt (µi) = 0d for all t ∈{0} ∪[T],
689"
EXP,0.7188249400479616,"iii. for all t ∈{0} ∪[T], the score function of P (i)
σt is Li,t-Lipschitz such that Li,t ≤
cL
ν2
i +σ2
t for
690"
EXP,0.7194244604316546,"some constant cL > 0,
691"
EXP,0.7200239808153477,"iv. ν2
0 > max
n
1, 4(c2
L+cνcL)
cν(1−cν)
o
ν2
max+c2
σ
1−cν
−c2
σ for constant cν ∈(0, 1), where νmax := maxi∈[k] νi,
692 693"
EXP,0.7206235011990407,"v. ∥µi −µ0∥2 ≤(1−cν)ν2
0−ν2
i −cνc2
σ
2(1−cν)

log
cν(ν2
i +c2
σ)
(c2
L+cνcL)(ν2
0+c2σ) −
(ν2
i +c2
σ)
2(1−cν)(ν2
0+c2σ) + (1−cν)(ν2
0+c2
σ)
2(ν2
i +c2σ)

d.
694"
EXP,0.7212230215827338,"The feasibility of Assumption 3.v. can be validated by substituting ν2 in Lemma 9 with ν2 + c2
σ.
695"
EXP,0.7218225419664268,"To establish Theorem 4, we first note from Proposition 1 that for a sub-Gaussian mixture P =
696
Pk
i=0 wiP (i), the perturbed distribution of noise level σ is Pσ = Pk
i=0 wiP (i)
σ , where P (0) =
697"
EXP,0.7224220623501199,"N(µ0, (ν2
i +σ2)Id) and P (i) is a sub-Gaussian distribution with mean µi and sub-Gaussian parameter
698"
EXP,0.7230215827338129,"(ν2
i + σ2). Similar to the proof of Theorem 1, we decompose
699"
EXP,0.723621103117506,"xt = Rrt + Nnt, and ϵt = Rϵ(r)
t
+ Nϵ(n)
t
,"
EXP,0.7242206235011991,"where R ∈Rd×r an orthonormal basis of the vector space {µi}i∈[k] and N ∈Rd×n an orthonormal
700"
EXP,0.7248201438848921,"basis of the null space of {µi}i∈[k]. Now, we prove Theorem 4 by applying the techniques developed
701"
EXP,0.7254196642685852,"in Appendix A.1 and A.3 via substituting ν2 and
ν2
1−cν with ν2+σ2
t
1−cν at time step t. Note that for all
702"
EXP,0.7260191846522782,"t ∈{0} ∪[T], Assumption 3.iv. implies ν2
0 + σ2
t > max
n
1, 4(c2
L+cνcL)
cν(1−cν)
o
ν2
max+σ2
t
1−cν
because cσ ≥σt.
703"
EXP,0.7266187050359713,"First, by Proposition 2, suppose that the sample is initialized in the distribution P (0)
σ0 , then with
704"
EXP,0.7272182254196643,"probability at least 1 −exp(−Ω(d)), we have
705"
EXP,0.7278177458033573,"∥n0∥2 ≥
3(ν2
0 + σ2
0)
4
+ ν2
max + σ2
0
4(1 −cν)"
EXP,0.7284172661870504,"
d.
(20)"
EXP,0.7290167865707434,"Then, with the assumption that the initialization satisfies ∥n0∥2 ≥

3(ν2
0+σ2
0)
4
+ ν2
max+σ2
0
4(1−cν)

d, the
706"
EXP,0.7296163069544365,"following proposition similar to Proposition 5 shows that ∥nt∥remains large with high probability.
707"
EXP,0.7302158273381295,"Proposition 6. Consider a distribution P satisfying Assumption 3. We follow annealed Langevin
708"
EXP,0.7308153477218226,"dynamics for T = exp(O(d)) steps with noise level cσ ≥σ0 ≥σ1 ≥· · · ≥σT ≥0 for some
709"
EXP,0.7314148681055156,"constant cσ > 0. Suppose that the initial sample satisfies ∥n0∥2 ≥

3(ν2
0+σ2
0)
4
+ ν2
max+σ2
0
4(1−cν)

d, then
710"
EXP,0.7320143884892086,"with probability at least 1 −T · exp(−Ω(d)), we have that ∥nt∥2 >

ν2
0+σ2
t
2
+ ν2
max+σ2
t
2(1−cν)

d for all
711"
EXP,0.7326139088729017,"t ∈{0} ∪[T].
712"
EXP,0.7332134292565947,"Proof of Proposition 6. We prove Proposition 6 by induction. Suppose the theorem holds for all T
713"
EXP,0.7338129496402878,"values of 1, · · · , T −1. We consider the following 3 cases:
714"
EXP,0.7344124700239808,"• If there exists some t ∈[T] such that δt > ν2
0 + σ2
t , by Lemma 3 we know that with
715"
EXP,0.7350119904076738,"probability at least 1 −exp(−Ω(d)), we have ∥nt∥2 ≥

3(ν2
0+σ2
t )
4
+ ν2
max+σ2
t
4(1−cν)

d, thus the
716"
EXP,0.7356115107913669,"problem reduces to the two sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can be solved
717"
EXP,0.7362110311750599,"by induction.
718"
EXP,0.736810551558753,"• Suppose δt ≤ν2
0 + σ2
t for all t ∈[T]. If there exists some t ∈[T] such that ∥nt−1∥2 >
719"
EXP,0.737410071942446,"36(ν2
0 + σ2
t−1)d ≥36(ν2
0 + σ2
t )d, by Lemma 11 we know that with probability at least
720"
EXP,0.738009592326139,"1 −exp(−Ω(d)), we have ∥nt∥2 ≥(ν2
0 + σ2
t )d >

3(ν2
0+σ2
t )
4
+ ν2
max+σ2
t
4(1−cν)

d, thus the
721"
EXP,0.7386091127098321,"problem similarly reduces to the two sub-arrays n0, · · · , nt−1 and nt, · · · , nT , which can
722"
EXP,0.7392086330935251,"be solved by induction.
723"
EXP,0.7398081534772182,"• Suppose δt ≤ν2
0 + σ2
t and ∥nt−1∥2 ≤36(ν2
0 + σ2
t−1)d for all t ∈[T]. Consider a surrogate
724"
EXP,0.7404076738609112,"sequence ˆnt such that ˆn0 = n0 and for all t ≥1,
725"
EXP,0.7410071942446043,"ˆnt = ˆnt−1 −
δt
2ν2
0 + 2σ2
t
ˆnt−1 +
p"
EXP,0.7416067146282974,"δtϵ(n)
t
."
EXP,0.7422062350119905,"Since ν0 > νi and cσ ≥σt for all t ∈{0} ∪[T], we have ν2
i +c2
σ
ν2
0+c2σ > ν2
i +σ2
t
ν2
0+σ2
t . Notice that for
726"
EXP,0.7428057553956835,function f(z) = log z −z 2 + 1
EXP,0.7434052757793765,"2z, we have
d
dzf(z) = 1 z −1"
EXP,0.7440047961630696,"2 −
1
2z2 = −1 2
  1"
EXP,0.7446043165467626,"z −1
2 ≤0.
727"
EXP,0.7452038369304557,"Thus, by Assumption 3.v. we have that for all t ∈[T],
728"
EXP,0.7458033573141487,"∥µi −µ0∥2 ≤(1 −cν)ν2
0 −ν2
i −cνc2
σ
2(1 −cν)"
EXP,0.7464028776978417,"
log
cν(ν2
i + c2
σ)
(c2
L + cνcL)(ν2
0 + c2σ)"
EXP,0.7470023980815348,"−
(ν2
i + c2
σ)
2(1 −cν)(ν2
0 + c2σ) + (1 −cν)(ν2
0 + c2
σ)
2(ν2
i + c2σ) 
d"
EXP,0.7476019184652278,"≤(1 −cν)ν2
0 −ν2
i −cνσ2
t
2(1 −cν)"
EXP,0.7482014388489209,"
log
cν(ν2
i + σ2
t )
(c2
L + cνcL)(ν2
0 + σ2
t )"
EXP,0.7488009592326139,"−
(ν2
i + σ2
t )
2(1 −cν)(ν2
0 + σ2
t ) + (1 −cν)(ν2
0 + σ2
t )
2(ν2
i + σ2
t ) 
d"
EXP,0.749400479616307,"Conditioned on ∥nt−1∥2 >
 ν2
0+σ2
t−1
2
+
ν2
max+σ2
t−1
2(1−cν)

d for all t ∈[T], by Lemma 12 we
729"
EXP,0.75,"have that for T = exp(O(d)),
730"
EXP,0.750599520383693,∥ˆnT −nT ∥< s
EXP,0.7511990407673861,"5(ν2
0 + σ2
T )
8
+ 3(ν2max + σ2
T )
8(1 −cν)
− s"
EXP,0.7517985611510791,"ν2
0 + σ2
T
2
+ ν2max + σ2
T
2(1 −cν) !
√ d."
EXP,0.7523980815347722,"By Lemma 8 we have that with probability at least 1 −exp(−Ω(d)),
731"
EXP,0.7529976019184652,"∥ˆnT ∥2 ≥
5(ν2
0 + σ2
T )
8
+ 3(ν2
max + σ2
T )
8(1 −cν) 
d."
EXP,0.7535971223021583,"Combining the two inequalities implies the desired bound
732"
EXP,0.7541966426858513,∥nT ∥≥∥ˆnT ∥−∥ˆnT −nT ∥>
EXP,0.7547961630695443,"sν2
0 + σ2
T
2
+ ν2max + σ2
T
2(1 −cν) 
d."
EXP,0.7553956834532374,"Hence by induction we obtain ∥nt∥2 >

ν2
0+σ2
T
2
+ ν2
max+σ2
T
2(1−cν)

d for all t ∈[T] with proba-
733"
EXP,0.7559952038369304,"bility at least
734"
EXP,0.7565947242206235,(1 −(T −1) exp(−Ω(d))) · (1 −exp(−Ω(d))) ≥1 −T exp(−Ω(d)).
EXP,0.7571942446043165,"Therefore we complete the proof of Proposition 6.
735"
EXP,0.7577937649880095,"Finally, combining equation 20 and Proposition 6 finishes the proof of Theorem 4.
736"
EXP,0.7583932853717026,"A.5
Proof of Theorem 5: Convergence Analysis of Chained Langevin Dynamics
737"
EXP,0.7589928057553957,"For simplicity, denote x[q] =

x(1), · · · , x(q)	
. By the definition of total variation distance, for all
738"
EXP,0.7595923261390888,"q ∈[d/Q] we have
739"
EXP,0.7601918465227818,"TV

ˆP

x[q]
, P

x[q] = 1 2"
EXP,0.7607913669064749,"Z  ˆP

x[q]
−P

x[q] dx[q] = 1 2"
EXP,0.7613908872901679,"Z  ˆP

x(q) | x[q−1]
ˆP

x[q−1]
−P

x(q) | x[q−1]
P

x[q−1] dx[q] ≤1 2"
EXP,0.761990407673861,"Z  ˆP

x(q) | x[q−1]
ˆP

x[q−1]
−ˆP

x(q) | x[q−1]
P

x[q−1] dx[q] + 1 2"
EXP,0.762589928057554,"Z  ˆP

x(q) | x[q−1]
P

x[q−1]
−P

x(q) | x[q−1]
P

x[q−1] dx[q] = 1 2"
EXP,0.763189448441247,"Z
ˆP

x(q) | x[q−1]
dx(q)
Z  ˆP

x[q−1]
−P

x[q−1] dx[q−1] + 1 2"
EXP,0.7637889688249401,"Z  ˆP

x(q) | x[q−1]
−P

x(q) | x[q−1] dx(q)
Z
P

x[q−1]
dx[q−1]"
EXP,0.7643884892086331,"= TV

ˆP

x[q−1]
, P

x[q−1]
+ TV

ˆP

x(q) | x[q−1]
, P

x(q) | x[q−1]"
EXP,0.7649880095923262,"≤TV

ˆP

x[q−1]
, P

x[q−1]
+ ε · Q d ."
EXP,0.7655875299760192,"Upon summing up the above inequality for all q ∈[d/Q], we obtain
740"
EXP,0.7661870503597122,"TV

ˆP(x), P(x)

= d/Q
X q=1"
EXP,0.7667865707434053,"
TV

ˆP

x[q]
, P

x[q]
−TV

ˆP

x[q−1]
, P

x[q−1] ≤ d/Q
X"
EXP,0.7673860911270983,"q=1
ε · Q d = ε"
EXP,0.7679856115107914,"Thus we finish the proof of Theorem 5.
741"
EXP,0.7685851318944844,"B
Additional Experiments
742"
EXP,0.7691846522781774,"Algorithm Setup: Our choices of algorithm hyperparameters are based on Song and Ermon (2019).
743"
EXP,0.7697841726618705,"We consider L = 10 different standard deviations such that {λi}i∈[L] is a geometric sequence with
744"
EXP,0.7703836930455635,"λ1 = 1 and λ10 = 0.01. For annealed Langevin dynamics with T iterations, we choose the noise
745"
EXP,0.7709832134292566,"levels {σt}t∈[T ] by repeating every element of {λi}i∈[L] for T/L times and we set the step size as
746"
EXP,0.7715827338129496,"δt = 2×10−5 ·σ2
t /σ2
T for every t ∈[T]. For vanilla Langevin dynamics with T iterations, we use the
747"
EXP,0.7721822541966427,"same step size as annealed Langevin dynamics. For chained Langevin dynamics with T iterations, the
748"
EXP,0.7727817745803357,"patch size Q is chosen depending on different tasks. For every patch of chained Langevin dynamics,
749"
EXP,0.7733812949640287,"we choose the noise levels {σt}t∈[T Q/d] by repeating every element of {λi}i∈[L] for TQ/dL times
750"
EXP,0.7739808153477218,"and we set the step size as δt = 2 × 10−5 · σ2
t /σ2
T Q/d for every t ∈[TQ/d].
751"
EXP,0.7745803357314148,"B.1
Synthetic Gaussian Mixture Model
752"
EXP,0.7751798561151079,"We choose the data distribution P as a mixture of three Gaussian components in dimension d = 100:
753"
EXP,0.7757793764988009,"P = 0.2P (0) + 0.4P (1) + 0.4P (2) = 0.2N(0d, 3Id) + 0.4N(1d, Id) + 0.4N(−1d, Id)."
EXP,0.776378896882494,"Since the distribution is given, we assume that the sampling algorithms have access to the ground-truth
754"
EXP,0.7769784172661871,"score function. We set the batch size as 1000 and patch size Q = 10 for chained Langevin dynamics.
755"
EXP,0.7775779376498801,"We use T ∈

103, 104, 105, 106	
iterations for vanilla, annealed, and chained Langevin dynamics.
756"
EXP,0.7781774580335732,"The initial samples are i.i.d. chosen from P (0), P (1), or P (2), and the results are presented in Figures
757"
EXP,0.7787769784172662,"Figure 4: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and
chained Langevin dynamics. Three axes are ℓ2 distance from samples to the mean of the three modes.
The samples are initialized in mode 1."
EXP,0.7793764988009593,"1, 4, and 5 respectively. The two subfigures above the dashed line illustrate the samples from the
758"
EXP,0.7799760191846523,"initial distribution and target distribution, and the subfigures below the dashed line are the samples
759"
EXP,0.7805755395683454,"generated by different algorithms. A sample x is clustered in mode 1 if it satisfies ∥x −µ1∥2 ≤5d
760"
EXP,0.7811750599520384,"and ∥x −µ1∥2 ≤∥x −µ2∥2; in mode 2 if ∥x −µ2∥2 ≤5d and ∥x −µ1∥2 > ∥x −µ2∥2; and in
761"
EXP,0.7817745803357314,"mode 0 otherwise. The experiments were run on an Intel Xeon CPU with 2.90GHz.
762"
EXP,0.7823741007194245,"B.2
Image Datasets
763"
EXP,0.7829736211031175,"Our implementation and hyperparameter selection are based on Song and Ermon (2019). During
764"
EXP,0.7835731414868106,"training, we i.i.d. randomly flip an image with probability 0.5 to construct the two modes (i.e., original
765"
EXP,0.7841726618705036,"and flipped images). All models are optimized by Adam with learning rate 0.001 and batch size 128
766"
EXP,0.7847721822541966,"for a total of 200000 training steps, and we use the model at the last iteration to generate the samples.
767"
EXP,0.7853717026378897,"We perform experiments on MNIST (LeCun, 1998) (CC BY-SA 3.0 License) and Fashion-MNIST
768"
EXP,0.7859712230215827,"(Xiao et al., 2017) (MIT License) datasets and we set the patch size as Q = 14.
769"
EXP,0.7865707434052758,"For the score networks of vanilla and annealed Langevin dynamics, following from Song and Ermon
770"
EXP,0.7871702637889688,"(2019), we use the 4-cascaded RefineNet (Lin et al., 2017), a modern variant of U-Net (Ronneberger
771"
EXP,0.7877697841726619,"et al., 2015) with residual design. For the score networks of chained Langevin dynamics, we use the
772"
EXP,0.7883693045563549,"official PyTorch implementation of an LSTM network (Sak et al., 2014) followed by a linear layer.
773"
EXP,0.7889688249400479,"For MNIST and Fashion-MNIST datasets, we set the input size of the LSTM as Q = 14, the number
774"
EXP,0.789568345323741,"of features in the hidden state as 1024, and the number of recurrent layers as 2. The inputs of LSTM
775"
EXP,0.790167865707434,"include inputting tensor, hidden state, and cell state, and the outputs of LSTM include the next hidden
776"
EXP,0.7907673860911271,"state and cell state, which can be fed to the next input. To estimate the noisy score function, we first
777"
EXP,0.7913669064748201,"input the noise level σ (repeated for Q times to match the input size of LSTM) and all-0 hidden and
778"
EXP,0.7919664268585132,"Figure 5: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and
chained Langevin dynamics. Three axes are ℓ2 distance from samples to the mean of the three modes.
The samples are initialized in mode 2."
EXP,0.7925659472422062,"cell states to obtain an initialization of the hidden and cell states. Then, we divide a sample into d/Q
779"
EXP,0.7931654676258992,"patches and input the sequence of patches to the LSTM. For every output hidden state corresponding
780"
EXP,0.7937649880095923,"to one patch, we apply a linear layer of size 1024 × Q to estimate the noisy score function of the
781"
EXP,0.7943645083932853,"patch.
782"
EXP,0.7949640287769785,"To generate samples, we use T ∈{3000, 10000, 30000, 100000} iterations for vanilla, annealed, and
783"
EXP,0.7955635491606715,"chained Langevin dynamics. The initial samples are chosen as either original or flipped images
784"
EXP,0.7961630695443646,"from the dataset, and the results for MNIST and Fashion-MNIST datasets are presented in Figures 2,
785"
EXP,0.7967625899280576,"6, 3, and 7 respectively. The two subfigures above the dashed line illustrate the samples from the
786"
EXP,0.7973621103117506,"initial distribution and target distribution, and the subfigures below the dashed line are the samples
787"
EXP,0.7979616306954437,"generated by different algorithms. High-quality figures generated by annealed and chained Langevin
788"
EXP,0.7985611510791367,"dynamics for T = 100000 iterations are presented in Figures 8 and 9.
789"
EXP,0.7991606714628298,"All experiments were run with one RTX3090 GPU. It is worth noting that the training and inference
790"
EXP,0.7997601918465228,"time of chained Langevin dynamics using LSTM is considerably faster than vanilla/annealed Langevin
791"
EXP,0.8003597122302158,"dynamics using RefineNet. For a course of 200000 training steps on MNIST/Fashion-MNIST, due
792"
EXP,0.8009592326139089,"to the different network architectures, LSTM takes around 2.3 hours while RefineNet takes around
793"
EXP,0.8015587529976019,"9.2 hours. Concerning image generation, chained Langevin dynamics is significantly faster than
794"
EXP,0.802158273381295,"vanilla/annealed Langevin dynamics since every iteration of chained Langevin dynamics only updates
795"
EXP,0.802757793764988,"a patch of constant size, while every iteration of vanilla/annealed Langevin dynamics requires
796"
EXP,0.8033573141486811,"computing all coordinates of the sample. One iteration of chained Langevin dynamics using LSTM
797"
EXP,0.8039568345323741,"takes around 1.97 ms, while one iteration of vanilla/annealed Langevin dynamics using RefineNet
798"
EXP,0.8045563549160671,"takes around 43.7 ms.
799"
EXP,0.8051558752997602,"Figure 6: Samples from a mixture distribution of the original and flipped images from the MNIST
dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized
as flipped images from MNIST."
EXP,0.8057553956834532,"C
Boarder Impacts
800"
EXP,0.8063549160671463,"This paper presents work whose goal is to advance the field of machine learning. No potential societal
801"
EXP,0.8069544364508393,"consequence of this work needs to be highlighted here.
802"
EXP,0.8075539568345323,"Figure 7: Samples from a mixture distribution of the original and flipped images from the Fashion-
MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are
initialized as flipped images from Fashion-MNIST."
EXP,0.8081534772182254,"Figure 8: Samples from a mixture distribution of the original and flipped images from the MNIST
dataset generated by annealed and chained Langevin dynamics for T = 100000 iterations. The
samples are initialized as the original or flipped images from MNIST."
EXP,0.8087529976019184,"Figure 9: Samples from a mixture distribution of the original and flipped images from the Fashion-
MNIST dataset generated by annealed and chained Langevin dynamics for T = 100000 iterations.
The samples are initialized as the original or flipped images from Fashion-MNIST."
EXP,0.8093525179856115,"NeurIPS Paper Checklist
803"
CLAIMS,0.8099520383693045,"1. Claims
804"
CLAIMS,0.8105515587529976,"Question: Do the main claims made in the abstract and introduction accurately reflect the
805"
CLAIMS,0.8111510791366906,"paper’s contributions and scope?
806"
CLAIMS,0.8117505995203836,"Answer: [Yes]
807"
CLAIMS,0.8123501199040767,"Justification: We list the paper’s contributions about the mode-seeking tendencies of vanilla,
808"
CLAIMS,0.8129496402877698,"annealed, and chained Langevin Dynamics in the abstract and at the end of Section 1. The
809"
CLAIMS,0.8135491606714629,"scope of this work is also discussed in Section 1.
810"
CLAIMS,0.8141486810551559,"Guidelines:
811"
CLAIMS,0.814748201438849,"• The answer NA means that the abstract and introduction do not include the claims
812"
CLAIMS,0.815347721822542,"made in the paper.
813"
CLAIMS,0.815947242206235,"• The abstract and/or introduction should clearly state the claims made, including the
814"
CLAIMS,0.8165467625899281,"contributions made in the paper and important assumptions and limitations. A No or
815"
CLAIMS,0.8171462829736211,"NA answer to this question will not be perceived well by the reviewers.
816"
CLAIMS,0.8177458033573142,"• The claims made should match theoretical and experimental results, and reflect how
817"
CLAIMS,0.8183453237410072,"much the results can be expected to generalize to other settings.
818"
CLAIMS,0.8189448441247003,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
819"
CLAIMS,0.8195443645083933,"are not attained by the paper.
820"
LIMITATIONS,0.8201438848920863,"2. Limitations
821"
LIMITATIONS,0.8207434052757794,"Question: Does the paper discuss the limitations of the work performed by the authors?
822"
LIMITATIONS,0.8213429256594724,"Answer: [Yes]
823"
LIMITATIONS,0.8219424460431655,"Justification: The limitations of this work are discussed in Section 7.
824"
LIMITATIONS,0.8225419664268585,"Guidelines:
825"
LIMITATIONS,0.8231414868105515,"• The answer NA means that the paper has no limitation while the answer No means that
826"
LIMITATIONS,0.8237410071942446,"the paper has limitations, but those are not discussed in the paper.
827"
LIMITATIONS,0.8243405275779376,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
828"
LIMITATIONS,0.8249400479616307,"• The paper should point out any strong assumptions and how robust the results are to
829"
LIMITATIONS,0.8255395683453237,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
830"
LIMITATIONS,0.8261390887290168,"model well-specification, asymptotic approximations only holding locally). The authors
831"
LIMITATIONS,0.8267386091127098,"should reflect on how these assumptions might be violated in practice and what the
832"
LIMITATIONS,0.8273381294964028,"implications would be.
833"
LIMITATIONS,0.8279376498800959,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
834"
LIMITATIONS,0.8285371702637889,"only tested on a few datasets or with a few runs. In general, empirical results often
835"
LIMITATIONS,0.829136690647482,"depend on implicit assumptions, which should be articulated.
836"
LIMITATIONS,0.829736211031175,"• The authors should reflect on the factors that influence the performance of the approach.
837"
LIMITATIONS,0.830335731414868,"For example, a facial recognition algorithm may perform poorly when image resolution
838"
LIMITATIONS,0.8309352517985612,"is low or images are taken in low lighting. Or a speech-to-text system might not be
839"
LIMITATIONS,0.8315347721822542,"used reliably to provide closed captions for online lectures because it fails to handle
840"
LIMITATIONS,0.8321342925659473,"technical jargon.
841"
LIMITATIONS,0.8327338129496403,"• The authors should discuss the computational efficiency of the proposed algorithms
842"
LIMITATIONS,0.8333333333333334,"and how they scale with dataset size.
843"
LIMITATIONS,0.8339328537170264,"• If applicable, the authors should discuss possible limitations of their approach to
844"
LIMITATIONS,0.8345323741007195,"address problems of privacy and fairness.
845"
LIMITATIONS,0.8351318944844125,"• While the authors might fear that complete honesty about limitations might be used by
846"
LIMITATIONS,0.8357314148681055,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
847"
LIMITATIONS,0.8363309352517986,"limitations that aren’t acknowledged in the paper. The authors should use their best
848"
LIMITATIONS,0.8369304556354916,"judgment and recognize that individual actions in favor of transparency play an impor-
849"
LIMITATIONS,0.8375299760191847,"tant role in developing norms that preserve the integrity of the community. Reviewers
850"
LIMITATIONS,0.8381294964028777,"will be specifically instructed to not penalize honesty concerning limitations.
851"
THEORY ASSUMPTIONS AND PROOFS,0.8387290167865707,"3. Theory Assumptions and Proofs
852"
THEORY ASSUMPTIONS AND PROOFS,0.8393285371702638,"Question: For each theoretical result, does the paper provide the full set of assumptions and
853"
THEORY ASSUMPTIONS AND PROOFS,0.8399280575539568,"a complete (and correct) proof?
854"
THEORY ASSUMPTIONS AND PROOFS,0.8405275779376499,"Answer: [Yes]
855"
THEORY ASSUMPTIONS AND PROOFS,0.8411270983213429,"Justification: The assumptions and proof of every theorem are clearly stated. For Theorem 1,
856"
THEORY ASSUMPTIONS AND PROOFS,0.841726618705036,"the assumptions are listed in Assumption 1 and the proof is in Appendix A.1; for Theorem 2,
857"
THEORY ASSUMPTIONS AND PROOFS,0.842326139088729,"the assumptions are listed in Assumption 1 and the proof is in Appendix A.2; for Theorem 3,
858"
THEORY ASSUMPTIONS AND PROOFS,0.842925659472422,"the assumptions are listed in Assumption 2 and the proof is in Appendix A.3; for Theorem
859"
THEORY ASSUMPTIONS AND PROOFS,0.8435251798561151,"4, the assumptions are listed in Assumption 3 and the proof is in Appendix A.4.
860"
THEORY ASSUMPTIONS AND PROOFS,0.8441247002398081,"Guidelines:
861"
THEORY ASSUMPTIONS AND PROOFS,0.8447242206235012,"• The answer NA means that the paper does not include theoretical results.
862"
THEORY ASSUMPTIONS AND PROOFS,0.8453237410071942,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
863"
THEORY ASSUMPTIONS AND PROOFS,0.8459232613908872,"referenced.
864"
THEORY ASSUMPTIONS AND PROOFS,0.8465227817745803,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
865"
THEORY ASSUMPTIONS AND PROOFS,0.8471223021582733,"• The proofs can either appear in the main paper or the supplemental material, but if
866"
THEORY ASSUMPTIONS AND PROOFS,0.8477218225419664,"they appear in the supplemental material, the authors are encouraged to provide a short
867"
THEORY ASSUMPTIONS AND PROOFS,0.8483213429256595,"proof sketch to provide intuition.
868"
THEORY ASSUMPTIONS AND PROOFS,0.8489208633093526,"• Inversely, any informal proof provided in the core of the paper should be complemented
869"
THEORY ASSUMPTIONS AND PROOFS,0.8495203836930456,"by formal proofs provided in appendix or supplemental material.
870"
THEORY ASSUMPTIONS AND PROOFS,0.8501199040767387,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
871"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8507194244604317,"4. Experimental Result Reproducibility
872"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8513189448441247,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
873"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8519184652278178,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
874"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8525179856115108,"of the paper (regardless of whether the code and data are provided or not)?
875"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8531175059952039,"Answer: [Yes]
876"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8537170263788969,"Justification: We provide all the information about our numerical experiments in Section 6
877"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.85431654676259,"and Appendix B that enable readers to reproduce our results.
878"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.854916067146283,"Guidelines:
879"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.855515587529976,"• The answer NA means that the paper does not include experiments.
880"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8561151079136691,"• If the paper includes experiments, a No answer to this question will not be perceived
881"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8567146282973621,"well by the reviewers: Making the paper reproducible is important, regardless of
882"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8573141486810552,"whether the code and data are provided or not.
883"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8579136690647482,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
884"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8585131894484412,"to make their results reproducible or verifiable.
885"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8591127098321343,"• Depending on the contribution, reproducibility can be accomplished in various ways.
886"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8597122302158273,"For example, if the contribution is a novel architecture, describing the architecture fully
887"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8603117505995204,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
888"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8609112709832134,"be necessary to either make it possible for others to replicate the model with the same
889"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8615107913669064,"dataset, or provide access to the model. In general. releasing code and data is often
890"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8621103117505995,"one good way to accomplish this, but reproducibility can also be provided via detailed
891"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8627098321342925,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
892"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8633093525179856,"of a large language model), releasing of a model checkpoint, or other means that are
893"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8639088729016786,"appropriate to the research performed.
894"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8645083932853717,"• While NeurIPS does not require releasing code, the conference does require all submis-
895"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8651079136690647,"sions to provide some reasonable avenue for reproducibility, which may depend on the
896"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8657074340527577,"nature of the contribution. For example
897"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8663069544364509,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
898"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8669064748201439,"to reproduce that algorithm.
899"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.867505995203837,"(b) If the contribution is primarily a new model architecture, the paper should describe
900"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.86810551558753,"the architecture clearly and fully.
901"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8687050359712231,"(c) If the contribution is a new model (e.g., a large language model), then there should
902"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8693045563549161,"either be a way to access this model for reproducing the results or a way to reproduce
903"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8699040767386091,"the model (e.g., with an open-source dataset or instructions for how to construct
904"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8705035971223022,"the dataset).
905"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8711031175059952,"(d) We recognize that reproducibility may be tricky in some cases, in which case
906"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8717026378896883,"authors are welcome to describe the particular way they provide for reproducibility.
907"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8723021582733813,"In the case of closed-source models, it may be that access to the model is limited in
908"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8729016786570744,"some way (e.g., to registered users), but it should be possible for other researchers
909"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8735011990407674,"to have some path to reproducing or verifying the results.
910"
OPEN ACCESS TO DATA AND CODE,0.8741007194244604,"5. Open access to data and code
911"
OPEN ACCESS TO DATA AND CODE,0.8747002398081535,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
912"
OPEN ACCESS TO DATA AND CODE,0.8752997601918465,"tions to faithfully reproduce the main experimental results, as described in supplemental
913"
OPEN ACCESS TO DATA AND CODE,0.8758992805755396,"material?
914"
OPEN ACCESS TO DATA AND CODE,0.8764988009592326,"Answer: [Yes]
915"
OPEN ACCESS TO DATA AND CODE,0.8770983213429256,"Justification: We submitted our code in supplementary materials for the readers to reproduce
916"
OPEN ACCESS TO DATA AND CODE,0.8776978417266187,"our numerical results.
917"
OPEN ACCESS TO DATA AND CODE,0.8782973621103117,"Guidelines:
918"
OPEN ACCESS TO DATA AND CODE,0.8788968824940048,"• The answer NA means that paper does not include experiments requiring code.
919"
OPEN ACCESS TO DATA AND CODE,0.8794964028776978,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
920"
OPEN ACCESS TO DATA AND CODE,0.8800959232613909,"public/guides/CodeSubmissionPolicy) for more details.
921"
OPEN ACCESS TO DATA AND CODE,0.8806954436450839,"• While we encourage the release of code and data, we understand that this might not be
922"
OPEN ACCESS TO DATA AND CODE,0.8812949640287769,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
923"
OPEN ACCESS TO DATA AND CODE,0.88189448441247,"including code, unless this is central to the contribution (e.g., for a new open-source
924"
OPEN ACCESS TO DATA AND CODE,0.882494004796163,"benchmark).
925"
OPEN ACCESS TO DATA AND CODE,0.8830935251798561,"• The instructions should contain the exact command and environment needed to run to
926"
OPEN ACCESS TO DATA AND CODE,0.8836930455635491,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
927"
OPEN ACCESS TO DATA AND CODE,0.8842925659472423,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
928"
OPEN ACCESS TO DATA AND CODE,0.8848920863309353,"• The authors should provide instructions on data access and preparation, including how
929"
OPEN ACCESS TO DATA AND CODE,0.8854916067146283,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
930"
OPEN ACCESS TO DATA AND CODE,0.8860911270983214,"• The authors should provide scripts to reproduce all experimental results for the new
931"
OPEN ACCESS TO DATA AND CODE,0.8866906474820144,"proposed method and baselines. If only a subset of experiments are reproducible, they
932"
OPEN ACCESS TO DATA AND CODE,0.8872901678657075,"should state which ones are omitted from the script and why.
933"
OPEN ACCESS TO DATA AND CODE,0.8878896882494005,"• At submission time, to preserve anonymity, the authors should release anonymized
934"
OPEN ACCESS TO DATA AND CODE,0.8884892086330936,"versions (if applicable).
935"
OPEN ACCESS TO DATA AND CODE,0.8890887290167866,"• Providing as much information as possible in supplemental material (appended to the
936"
OPEN ACCESS TO DATA AND CODE,0.8896882494004796,"paper) is recommended, but including URLs to data and code is permitted.
937"
OPEN ACCESS TO DATA AND CODE,0.8902877697841727,"6. Experimental Setting/Details
938"
OPEN ACCESS TO DATA AND CODE,0.8908872901678657,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
939"
OPEN ACCESS TO DATA AND CODE,0.8914868105515588,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
940"
OPEN ACCESS TO DATA AND CODE,0.8920863309352518,"results?
941"
OPEN ACCESS TO DATA AND CODE,0.8926858513189448,"Answer: [Yes]
942"
OPEN ACCESS TO DATA AND CODE,0.8932853717026379,"Justification: All details of the numerical experiments are listed in Appendix B.
943"
OPEN ACCESS TO DATA AND CODE,0.8938848920863309,"Guidelines:
944"
OPEN ACCESS TO DATA AND CODE,0.894484412470024,"• The answer NA means that the paper does not include experiments.
945"
OPEN ACCESS TO DATA AND CODE,0.895083932853717,"• The experimental setting should be presented in the core of the paper to a level of detail
946"
OPEN ACCESS TO DATA AND CODE,0.89568345323741,"that is necessary to appreciate the results and make sense of them.
947"
OPEN ACCESS TO DATA AND CODE,0.8962829736211031,"• The full details can be provided either with the code, in appendix, or as supplemental
948"
OPEN ACCESS TO DATA AND CODE,0.8968824940047961,"material.
949"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8974820143884892,"7. Experiment Statistical Significance
950"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8980815347721822,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
951"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8986810551558753,"information about the statistical significance of the experiments?
952"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8992805755395683,"Answer: [Yes]
953"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8998800959232613,"Justification: For the results of synthetic data, we report 1000 samples for every experiment.
954"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9004796163069544,"For the results of image datasets, we report 49 or 100 samples for every experiment.
955"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9010791366906474,"Guidelines:
956"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016786570743405,"• The answer NA means that the paper does not include experiments.
957"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9022781774580336,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
958"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9028776978417267,"dence intervals, or statistical significance tests, at least for the experiments that support
959"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9034772182254197,"the main claims of the paper.
960"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9040767386091128,"• The factors of variability that the error bars are capturing should be clearly stated (for
961"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9046762589928058,"example, train/test split, initialization, random drawing of some parameter, or overall
962"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9052757793764988,"run with given experimental conditions).
963"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9058752997601919,"• The method for calculating the error bars should be explained (closed form formula,
964"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9064748201438849,"call to a library function, bootstrap, etc.)
965"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.907074340527578,"• The assumptions made should be given (e.g., Normally distributed errors).
966"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.907673860911271,"• It should be clear whether the error bar is the standard deviation or the standard error
967"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.908273381294964,"of the mean.
968"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9088729016786571,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
969"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9094724220623501,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
970"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9100719424460432,"of Normality of errors is not verified.
971"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9106714628297362,"• For asymmetric distributions, the authors should be careful not to show in tables or
972"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9112709832134293,"figures symmetric error bars that would yield results that are out of range (e.g. negative
973"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9118705035971223,"error rates).
974"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9124700239808153,"• If error bars are reported in tables or plots, The authors should explain in the text how
975"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9130695443645084,"they were calculated and reference the corresponding figures or tables in the text.
976"
EXPERIMENTS COMPUTE RESOURCES,0.9136690647482014,"8. Experiments Compute Resources
977"
EXPERIMENTS COMPUTE RESOURCES,0.9142685851318945,"Question: For each experiment, does the paper provide sufficient information on the com-
978"
EXPERIMENTS COMPUTE RESOURCES,0.9148681055155875,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
979"
EXPERIMENTS COMPUTE RESOURCES,0.9154676258992805,"the experiments?
980"
EXPERIMENTS COMPUTE RESOURCES,0.9160671462829736,"Answer: [Yes]
981"
EXPERIMENTS COMPUTE RESOURCES,0.9166666666666666,"Justification: Information on the computational resources of the numerical experiments is
982"
EXPERIMENTS COMPUTE RESOURCES,0.9172661870503597,"provided in Appendix B.
983"
EXPERIMENTS COMPUTE RESOURCES,0.9178657074340527,"Guidelines:
984"
EXPERIMENTS COMPUTE RESOURCES,0.9184652278177458,"• The answer NA means that the paper does not include experiments.
985"
EXPERIMENTS COMPUTE RESOURCES,0.9190647482014388,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
986"
EXPERIMENTS COMPUTE RESOURCES,0.919664268585132,"or cloud provider, including relevant memory and storage.
987"
EXPERIMENTS COMPUTE RESOURCES,0.920263788968825,"• The paper should provide the amount of compute required for each of the individual
988"
EXPERIMENTS COMPUTE RESOURCES,0.920863309352518,"experimental runs as well as estimate the total compute.
989"
EXPERIMENTS COMPUTE RESOURCES,0.9214628297362111,"• The paper should disclose whether the full research project required more compute
990"
EXPERIMENTS COMPUTE RESOURCES,0.9220623501199041,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
991"
EXPERIMENTS COMPUTE RESOURCES,0.9226618705035972,"didn’t make it into the paper).
992"
CODE OF ETHICS,0.9232613908872902,"9. Code Of Ethics
993"
CODE OF ETHICS,0.9238609112709832,"Question: Does the research conducted in the paper conform, in every respect, with the
994"
CODE OF ETHICS,0.9244604316546763,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
995"
CODE OF ETHICS,0.9250599520383693,"Answer: [Yes]
996"
CODE OF ETHICS,0.9256594724220624,"Justification: We read the guidelines and wrote the paper according to the NeurIPS Code of
997"
CODE OF ETHICS,0.9262589928057554,"Ethics.
998"
CODE OF ETHICS,0.9268585131894485,"Guidelines:
999"
CODE OF ETHICS,0.9274580335731415,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1000"
CODE OF ETHICS,0.9280575539568345,"• If the authors answer No, they should explain the special circumstances that require a
1001"
CODE OF ETHICS,0.9286570743405276,"deviation from the Code of Ethics.
1002"
CODE OF ETHICS,0.9292565947242206,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1003"
CODE OF ETHICS,0.9298561151079137,"eration due to laws or regulations in their jurisdiction).
1004"
BROADER IMPACTS,0.9304556354916067,"10. Broader Impacts
1005"
BROADER IMPACTS,0.9310551558752997,"Question: Does the paper discuss both potential positive societal impacts and negative
1006"
BROADER IMPACTS,0.9316546762589928,"societal impacts of the work performed?
1007"
BROADER IMPACTS,0.9322541966426858,"Answer: [Yes]
1008"
BROADER IMPACTS,0.9328537170263789,"Justification: As discussed in Appendix C, this work has potential societal consequences,
1009"
BROADER IMPACTS,0.9334532374100719,"none of which needs to be specially highlighted.
1010"
BROADER IMPACTS,0.934052757793765,"Guidelines:
1011"
BROADER IMPACTS,0.934652278177458,"• The answer NA means that there is no societal impact of the work performed.
1012"
BROADER IMPACTS,0.935251798561151,"• If the authors answer NA or No, they should explain why their work has no societal
1013"
BROADER IMPACTS,0.9358513189448441,"impact or why the paper does not address societal impact.
1014"
BROADER IMPACTS,0.9364508393285371,"• Examples of negative societal impacts include potential malicious or unintended uses
1015"
BROADER IMPACTS,0.9370503597122302,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1016"
BROADER IMPACTS,0.9376498800959233,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1017"
BROADER IMPACTS,0.9382494004796164,"groups), privacy considerations, and security considerations.
1018"
BROADER IMPACTS,0.9388489208633094,"• The conference expects that many papers will be foundational research and not tied
1019"
BROADER IMPACTS,0.9394484412470024,"to particular applications, let alone deployments. However, if there is a direct path to
1020"
BROADER IMPACTS,0.9400479616306955,"any negative applications, the authors should point it out. For example, it is legitimate
1021"
BROADER IMPACTS,0.9406474820143885,"to point out that an improvement in the quality of generative models could be used to
1022"
BROADER IMPACTS,0.9412470023980816,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1023"
BROADER IMPACTS,0.9418465227817746,"that a generic algorithm for optimizing neural networks could enable people to train
1024"
BROADER IMPACTS,0.9424460431654677,"models that generate Deepfakes faster.
1025"
BROADER IMPACTS,0.9430455635491607,"• The authors should consider possible harms that could arise when the technology is
1026"
BROADER IMPACTS,0.9436450839328537,"being used as intended and functioning correctly, harms that could arise when the
1027"
BROADER IMPACTS,0.9442446043165468,"technology is being used as intended but gives incorrect results, and harms following
1028"
BROADER IMPACTS,0.9448441247002398,"from (intentional or unintentional) misuse of the technology.
1029"
BROADER IMPACTS,0.9454436450839329,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1030"
BROADER IMPACTS,0.9460431654676259,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1031"
BROADER IMPACTS,0.9466426858513189,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1032"
BROADER IMPACTS,0.947242206235012,"feedback over time, improving the efficiency and accessibility of ML).
1033"
SAFEGUARDS,0.947841726618705,"11. Safeguards
1034"
SAFEGUARDS,0.9484412470023981,"Question: Does the paper describe safeguards that have been put in place for responsible
1035"
SAFEGUARDS,0.9490407673860911,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1036"
SAFEGUARDS,0.9496402877697842,"image generators, or scraped datasets)?
1037"
SAFEGUARDS,0.9502398081534772,"Answer: [NA]
1038"
SAFEGUARDS,0.9508393285371702,"Justification: The models used in this work do not have a high risk of misuse.
1039"
SAFEGUARDS,0.9514388489208633,"Guidelines:
1040"
SAFEGUARDS,0.9520383693045563,"• The answer NA means that the paper poses no such risks.
1041"
SAFEGUARDS,0.9526378896882494,"• Released models that have a high risk for misuse or dual-use should be released with
1042"
SAFEGUARDS,0.9532374100719424,"necessary safeguards to allow for controlled use of the model, for example by requiring
1043"
SAFEGUARDS,0.9538369304556354,"that users adhere to usage guidelines or restrictions to access the model or implementing
1044"
SAFEGUARDS,0.9544364508393285,"safety filters.
1045"
SAFEGUARDS,0.9550359712230215,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1046"
SAFEGUARDS,0.9556354916067147,"should describe how they avoided releasing unsafe images.
1047"
SAFEGUARDS,0.9562350119904077,"• We recognize that providing effective safeguards is challenging, and many papers do
1048"
SAFEGUARDS,0.9568345323741008,"not require this, but we encourage authors to take this into account and make a best
1049"
SAFEGUARDS,0.9574340527577938,"faith effort.
1050"
LICENSES FOR EXISTING ASSETS,0.9580335731414868,"12. Licenses for existing assets
1051"
LICENSES FOR EXISTING ASSETS,0.9586330935251799,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1052"
LICENSES FOR EXISTING ASSETS,0.9592326139088729,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1053"
LICENSES FOR EXISTING ASSETS,0.959832134292566,"properly respected?
1054"
LICENSES FOR EXISTING ASSETS,0.960431654676259,"Answer: [Yes]
1055"
LICENSES FOR EXISTING ASSETS,0.9610311750599521,"Justification: All assets are properly cited in our work.
1056"
LICENSES FOR EXISTING ASSETS,0.9616306954436451,"Guidelines:
1057"
LICENSES FOR EXISTING ASSETS,0.9622302158273381,"• The answer NA means that the paper does not use existing assets.
1058"
LICENSES FOR EXISTING ASSETS,0.9628297362110312,"• The authors should cite the original paper that produced the code package or dataset.
1059"
LICENSES FOR EXISTING ASSETS,0.9634292565947242,"• The authors should state which version of the asset is used and, if possible, include a
1060"
LICENSES FOR EXISTING ASSETS,0.9640287769784173,"URL.
1061"
LICENSES FOR EXISTING ASSETS,0.9646282973621103,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1062"
LICENSES FOR EXISTING ASSETS,0.9652278177458034,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1063"
LICENSES FOR EXISTING ASSETS,0.9658273381294964,"service of that source should be provided.
1064"
LICENSES FOR EXISTING ASSETS,0.9664268585131894,"• If assets are released, the license, copyright information, and terms of use in the
1065"
LICENSES FOR EXISTING ASSETS,0.9670263788968825,"package should be provided. For popular datasets, paperswithcode.com/datasets
1066"
LICENSES FOR EXISTING ASSETS,0.9676258992805755,"has curated licenses for some datasets. Their licensing guide can help determine the
1067"
LICENSES FOR EXISTING ASSETS,0.9682254196642686,"license of a dataset.
1068"
LICENSES FOR EXISTING ASSETS,0.9688249400479616,"• For existing datasets that are re-packaged, both the original license and the license of
1069"
LICENSES FOR EXISTING ASSETS,0.9694244604316546,"the derived asset (if it has changed) should be provided.
1070"
LICENSES FOR EXISTING ASSETS,0.9700239808153477,"• If this information is not available online, the authors are encouraged to reach out to
1071"
LICENSES FOR EXISTING ASSETS,0.9706235011990407,"the asset’s creators.
1072"
NEW ASSETS,0.9712230215827338,"13. New Assets
1073"
NEW ASSETS,0.9718225419664268,"Question: Are new assets introduced in the paper well documented and is the documentation
1074"
NEW ASSETS,0.9724220623501199,"provided alongside the assets?
1075"
NEW ASSETS,0.9730215827338129,"Answer: [NA]
1076"
NEW ASSETS,0.973621103117506,"Justification: This paper does not release new assets.
1077"
NEW ASSETS,0.9742206235011991,"Guidelines:
1078"
NEW ASSETS,0.9748201438848921,"• The answer NA means that the paper does not release new assets.
1079"
NEW ASSETS,0.9754196642685852,"• Researchers should communicate the details of the dataset/code/model as part of their
1080"
NEW ASSETS,0.9760191846522782,"submissions via structured templates. This includes details about training, license,
1081"
NEW ASSETS,0.9766187050359713,"limitations, etc.
1082"
NEW ASSETS,0.9772182254196643,"• The paper should discuss whether and how consent was obtained from people whose
1083"
NEW ASSETS,0.9778177458033573,"asset is used.
1084"
NEW ASSETS,0.9784172661870504,"• At submission time, remember to anonymize your assets (if applicable). You can either
1085"
NEW ASSETS,0.9790167865707434,"create an anonymized URL or include an anonymized zip file.
1086"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796163069544365,"14. Crowdsourcing and Research with Human Subjects
1087"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802158273381295,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1088"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808153477218226,"include the full text of instructions given to participants and screenshots, if applicable, as
1089"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814148681055156,"well as details about compensation (if any)?
1090"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820143884892086,"Answer: [NA]
1091"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826139088729017,"Justification: Crowdsourcing or research with human subjects is not involved in this work.
1092"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832134292565947,"Guidelines:
1093"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838129496402878,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1094"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844124700239808,"human subjects.
1095"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850119904076738,"• Including this information in the supplemental material is fine, but if the main contribu-
1096"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856115107913669,"tion of the paper involves human subjects, then as much detail as possible should be
1097"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862110311750599,"included in the main paper.
1098"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986810551558753,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1099"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987410071942446,"or other labor should be paid at least the minimum wage in the country of the data
1100"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988009592326139,"collector.
1101"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886091127098321,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1102"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892086330935251,"Subjects
1103"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898081534772182,"Question: Does the paper describe potential risks incurred by study participants, whether
1104"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904076738609112,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1105"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910071942446043,"approvals (or an equivalent approval/review based on the requirements of your country or
1106"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916067146282974,"institution) were obtained?
1107"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922062350119905,"Answer: [NA]
1108"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928057553956835,"Justification: Crowdsourcing or research with human subjects is not involved in our paper.
1109"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934052757793765,"Guidelines:
1110"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940047961630696,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1111"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946043165467626,"human subjects.
1112"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952038369304557,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1113"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958033573141487,"may be required for any human subjects research. If you obtained IRB approval, you
1114"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964028776978417,"should clearly state this in the paper.
1115"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970023980815348,"• We recognize that the procedures for this may vary significantly between institutions
1116"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976019184652278,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1117"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982014388489209,"guidelines for their institution.
1118"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988009592326139,"• For initial submissions, do not include any information that would break anonymity (if
1119"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999400479616307,"applicable), such as the institution conducting the review.
1120"
