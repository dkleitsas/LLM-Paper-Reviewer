Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016051364365971107,"Spiking neural networks (SNNs) offer a promising avenue to implement deep neural
1"
ABSTRACT,0.0032102728731942215,"networks in a more energy-efficient way. However, the network architectures of
2"
ABSTRACT,0.004815409309791332,"existing SNNs for language tasks are too simplistic, and deep architectures have
3"
ABSTRACT,0.006420545746388443,"not been fully explored, resulting in a significant performance gap compared to
4"
ABSTRACT,0.008025682182985553,"mainstream transformer-based networks such as BERT. To this end, we improve
5"
ABSTRACT,0.009630818619582664,"a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to
6"
ABSTRACT,0.011235955056179775,"process language tasks and propose a two-stage knowledge distillation method for
7"
ABSTRACT,0.012841091492776886,"training it, which combines pre-training by distilling knowledge from BERT with
8"
ABSTRACT,0.014446227929373997,"a large collection of unlabelled texts and fine-tuning with task-specific instances
9"
ABSTRACT,0.016051364365971106,"via knowledge distillation again from the BERT fine-tuned on the same training
10"
ABSTRACT,0.01765650080256822,"examples. Through extensive experimentation, we show that the models trained
11"
ABSTRACT,0.019261637239165328,"with our method, named SpikeBERT, outperform state-of-the-art SNNs and even
12"
ABSTRACT,0.02086677367576244,"achieve comparable results to BERTs on text classification tasks for both English
13"
ABSTRACT,0.02247191011235955,"and Chinese with much less energy consumption.
14"
INTRODUCTION,0.024077046548956663,"1
Introduction
15"
INTRODUCTION,0.025682182985553772,"Modern artificial neural networks (ANNs) have been highly successful for a wide range of natural
16"
INTRODUCTION,0.027287319422150885,"language processing (NLP) and computer vision (CV) tasks. However, it requires too much compu-
17"
INTRODUCTION,0.028892455858747994,"tational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent
18"
INTRODUCTION,0.030497592295345103,"increase of energy consumption per model over the past decade. The energy consumption of large
19"
INTRODUCTION,0.03210272873194221,"language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable
20"
INTRODUCTION,0.033707865168539325,"even during inference. In recent years, spiking neural networks (SNNs), arguably known as the third
21"
INTRODUCTION,0.03531300160513644,"generation of neural network [Maas, 1997], have attracted a lot of attention due to their high biological
22"
INTRODUCTION,0.03691813804173355,"plausibility, event-driven property and low energy consumption [Roy et al., 2019]. Like biological
23"
INTRODUCTION,0.038523274478330656,"neurons, SNNs use discrete spikes to process and transmit information. Nowadays, neuromorphic
24"
INTRODUCTION,0.04012841091492777,"hardware can be used to fulfill spike-based computing, which provides a promising way to implement
25"
INTRODUCTION,0.04173354735152488,"artificial intelligence with much lower energy consumption.
26"
INTRODUCTION,0.04333868378812199,"Spiking neural networks have achieved great success in image classification task [Hu et al., 2018, Yin
27"
INTRODUCTION,0.0449438202247191,"et al., 2020, Fang et al., 2021, Ding et al., 2021, Kim et al., 2022a, Zhou et al., 2022] and there have
28"
INTRODUCTION,0.04654895666131621,"been some works [Plank et al., 2021, Lv et al., 2023, Zhu et al., 2023] that have demonstrated the
29"
INTRODUCTION,0.048154093097913325,"efficacy of SNNs in language tasks partially. However, the backbone networks employed in SNNs for
30"
INTRODUCTION,0.04975922953451043,"language tasks are overly simplistic, which significantly lowers the upper bound on the performance
31"
INTRODUCTION,0.051364365971107544,"of their SNN models. For instance, the SNN used by Lv et al. [2023], which is built upon TextCNN
32"
INTRODUCTION,0.052969502407704656,"[Kim, 2014], demonstrates a notable performance gap compared to those built on Transfomer-based
33"
INTRODUCTION,0.05457463884430177,(a) Spikformer SPS
INTRODUCTION,0.056179775280898875,Conv 2D BN MP
INTRODUCTION,0.05778491171749599,Images D-SSA BN
INTRODUCTION,0.0593900481540931,Conv 2D
INTRODUCTION,0.060995184590690206,Encoder Block √ó ùêø √ó 2 MLP Words
INTRODUCTION,0.06260032102728733,"Word
Embedding"
INTRODUCTION,0.06420545746388442,(b) SpikeBERT N-SSA LN
INTRODUCTION,0.06581059390048154,Linear
INTRODUCTION,0.06741573033707865,Encoder Block √ó ùêø‚Ä≤ √ó 2 MLP
INTRODUCTION,0.06902086677367576,Spiking Neuron Layer
INTRODUCTION,0.07062600321027288,Element-wise Add
INTRODUCTION,0.07223113964686999,"BN
Batch Normalization"
INTRODUCTION,0.0738362760834671,"LN
Layer Normalization SSA"
INTRODUCTION,0.0754414125200642,"SPS
Spiking Patch Splitting"
INTRODUCTION,0.07704654895666131,Spiking Self Attention
INTRODUCTION,0.07865168539325842,"MP
Max Pooling"
INTRODUCTION,0.08025682182985554,"Figure 1:
(a) The architecture of Spikformer [Zhou et al., 2022]. (b) The architecture of our
SpikeBERT. We improve Spikformer in its architecture, making it possible to process languages.
Firstly, Spiking Patch Splitting (SPS) module was replaced with a word embedding layer so that the
network can take discrete words (or tokens) as input. Secondly, we make the shape of the attention
map yielded by Spiking Self Attention (SSA) to be N √ó N, rather than D √ó D, where D and N
denote dimensionality of hidden layers and the length of inputs respectively. Lastly, the convolution
layers were replaced with linear layers, and the batch normalization with layer normalization. L and
L
‚Ä≤ denote the number of encoder blocks in Spikfomer and SpikeBERT, respectively."
INTRODUCTION,0.08186195826645265,"[Vaswani et al., 2017] large language models like BERT [Devlin et al., 2019] and RoBERTa [Liu
34"
INTRODUCTION,0.08346709470304976,"et al., 2019] on multiple classification benchmarks.
35"
INTRODUCTION,0.08507223113964688,"Recently, Spikformer was proposed by Zhou et al. [2022], which first introduced Transformer
36"
INTRODUCTION,0.08667736757624397,"architecture to SNNs and significantly narrowed the gap between SNNs and ViT [Dosovitskiy et al.,
37"
INTRODUCTION,0.08828250401284109,"2020] on ImageNet [Deng et al., 2009] and CIFAR-10. We think that Spikformer provides the
38"
INTRODUCTION,0.0898876404494382,"possibility to construct complex language representation models. As shown in Figure 1, considering
39"
INTRODUCTION,0.09149277688603531,"the discrete nature of textual data, we improve the architecture of Spikformer to make it suitable
40"
INTRODUCTION,0.09309791332263243,"for language tasks. we replace certain modules that were originally designed for image processing
41"
INTRODUCTION,0.09470304975922954,"with language-friendly modules. Please see Section 3.2 for details on the improvement in network
42"
INTRODUCTION,0.09630818619582665,"architecture. In general, a deeper ANN model often implies better performance. Increasing the
43"
INTRODUCTION,0.09791332263242375,"depth of a ANN allows for the extraction of more complex and abstract features from the input data.
44"
INTRODUCTION,0.09951845906902086,"However, Fang et al. [2020a] have shown that deep SNNs directly trained with backpropagation
45"
INTRODUCTION,0.10112359550561797,"through time (BPTT) [Werbos, 1990] using surrogate gradients (See Section2.1) could suffer from
46"
INTRODUCTION,0.10272873194221509,"the problem of gradient vanishing or exploding due to ‚Äúself-accumulating dynamics‚Äù. Therefore, we
47"
INTRODUCTION,0.1043338683788122,"proposed to use knowledge distillation [Hinton et al., 2015] to train language Spikformers so that the
48"
INTRODUCTION,0.10593900481540931,"deviation of surrogate gradients in Spikformer would not be rapidly accumulated[Qiu et al., 2023].
49"
INTRODUCTION,0.10754414125200643,"Inspired by the widely-used ‚Äúpre-training + fine-tuning‚Äù recipe [Sun et al., 2019, Liu, 2019, Gururan-
50"
INTRODUCTION,0.10914927768860354,"gan et al., 2020], we present a two-stage knowledge distillation strategy. In stage 1, we choose BERT
51"
INTRODUCTION,0.11075441412520064,"as teacher model and the improved Spikformer as student model. We utilize a large collection of
52"
INTRODUCTION,0.11235955056179775,"unlabelled texts to align features produced by two models in the embedding layer and multiple hidden
53"
INTRODUCTION,0.11396468699839486,"layers. In stage 2, we use a BERT fine-tuned on a task-specific dataset as teacher and the model
54"
INTRODUCTION,0.11556982343499198,"that completes stage 1 as student. At this stage, we first do the data augmentation for task-specific
55"
INTRODUCTION,0.11717495987158909,"dataset and then employ the logits predicted by the teacher model to further guide the student model.
56"
INTRODUCTION,0.1187800963081862,"After two-stage knowledge distillation, a spiking language model, named SpikeBERT, can be built
57"
INTRODUCTION,0.12038523274478331,"by distilling knowledge from BERT. The experiment results show that SpikeBERT not only can
58"
INTRODUCTION,0.12199036918138041,"outperform the state-of-the-art SNNs-like frameworks in text classification task but also achieve
59"
INTRODUCTION,0.12359550561797752,"competitive performance to BERTs. The experiments of the ablation study (Section 4.5) also show
60"
INTRODUCTION,0.12520064205457465,"that ‚Äúpre-training distillation‚Äù plays an important role in training SpikeBERT.
61"
INTRODUCTION,0.12680577849117175,"The major contribution of this study can be summarized as follows:
62"
INTRODUCTION,0.12841091492776885,"‚Ä¢ We improve the architecture of Spikformer for language processing and propose a two-stage,
63"
INTRODUCTION,0.13001605136436598,"‚Äúpre-training + task-specific‚Äù knowledge distillation training method, in which the improved
64"
INTRODUCTION,0.13162118780096307,"Spikformers are pre-trained on a huge collection of unlabelled texts before they are further
65"
INTRODUCTION,0.1332263242375602,"fine-tuned on task-specific datasets by distilling the knowledge of feature extractions and
66"
INTRODUCTION,0.1348314606741573,"predictive powers from BERTs.
67"
INTRODUCTION,0.13643659711075443,"‚Ä¢ We empirically show that SpikeBERT achieved significantly higher performance than
68"
INTRODUCTION,0.13804173354735153,"existing SNNs on 6 different language benchmark datasets for both English and Chinese.
69"
INTRODUCTION,0.13964686998394862,"‚Ä¢ This study is among the first to show the feasibility of transferring the knowledge of BERT-
70"
INTRODUCTION,0.14125200642054575,"like large language models to spiking-based architectures that can achieve comparable
71"
INTRODUCTION,0.14285714285714285,"results but with much less energy consumption.
72"
RELATED WORK,0.14446227929373998,"2
Related Work
73"
SPIKING NEURAL NETWORKS,0.14606741573033707,"2.1
Spiking Neural Networks
74"
SPIKING NEURAL NETWORKS,0.1476725521669342,"SNNs use discrete spike trains instead of continuous decimal values to compute and transmit infor-
75"
SPIKING NEURAL NETWORKS,0.1492776886035313,"mation. Spiking neurons, such as Izhikevich neuron [Izhikevich, 2003] and Leaky Integrate-and-Fire
76"
SPIKING NEURAL NETWORKS,0.1508828250401284,"(LIF) neuron [Wu et al., 2017], are usually applied to generate spike trains. However, due to the
77"
SPIKING NEURAL NETWORKS,0.15248796147672553,"non-differentiability of spikes, training SNNs has been a great challenge for the past two decades.
78"
SPIKING NEURAL NETWORKS,0.15409309791332262,"Currently, there are two mainstream approaches to address this problem.
79"
SPIKING NEURAL NETWORKS,0.15569823434991975,"ANN-to-SNN Conversion
ANN-to-SNN conversion method [Diehl et al., 2015, Cao et al., 2015,
80"
SPIKING NEURAL NETWORKS,0.15730337078651685,"Rueckauer et al., 2017, Hu et al., 2018] aims to convert weights of a well-trained ANN to its SNN
81"
SPIKING NEURAL NETWORKS,0.15890850722311398,"counterpart by replacing the activation function with spiking neuron layers and adding scaling rules
82"
SPIKING NEURAL NETWORKS,0.16051364365971107,"such as weight normalization [Diehl et al., 2016] and threshold constraints [Hu et al., 2018]. This
83"
SPIKING NEURAL NETWORKS,0.16211878009630817,"approach suffers from a large number of time steps during the conversion.
84"
SPIKING NEURAL NETWORKS,0.1637239165329053,"Backpropagation with Surrogate Gradients
Another popular approach is to utilize surrogate
85"
SPIKING NEURAL NETWORKS,0.1653290529695024,"gradients [Neftci et al., 2019] during error backpropagation, enabling the entire procedure to be
86"
SPIKING NEURAL NETWORKS,0.16693418940609953,"differentiable. Multiple surrogate gradients functions have been proposed, including the Sigmoid
87"
SPIKING NEURAL NETWORKS,0.16853932584269662,"surrogate function [Zenke and Ganguli, 2017], Fast-Sigmoid [Zheng and Mazumder, 2018], and ATan
88"
SPIKING NEURAL NETWORKS,0.17014446227929375,"[Fang et al., 2020a]. Backpropagation through time (BPTT) [Werbos, 1990] is one of the most popular
89"
SPIKING NEURAL NETWORKS,0.17174959871589085,"methods for directly training SNNs[Shrestha and Orchard, 2018, Kang et al., 2022], which applies
90"
SPIKING NEURAL NETWORKS,0.17335473515248795,"the traditional backpropagation algorithm [LeCun et al., 1989] to the unrolled computational graph.
91"
SPIKING NEURAL NETWORKS,0.17495987158908508,"In recent years, several BPTT-like training strategies have been proposed, including SpatioTemporal
92"
SPIKING NEURAL NETWORKS,0.17656500802568217,"Backpropagation (STBP) [Wu et al., 2017], STBP with Temporal Dependent Batch Normalization
93"
SPIKING NEURAL NETWORKS,0.1781701444622793,"(STBP-tdBN) [Zheng et al., 2020], and Spatio-Temporal Dropout Backpropagation (STDB) [Rathi
94"
SPIKING NEURAL NETWORKS,0.1797752808988764,"et al., 2020]. These strategies have demonstrated high performance under specific settings. For more
95"
SPIKING NEURAL NETWORKS,0.18138041733547353,"detailed information about Backpropagation Through Time (BPTT), please refer to Appendix A.
96"
KNOWLEDGE DISTILLATION,0.18298555377207062,"2.2
Knowledge Distillation
97"
KNOWLEDGE DISTILLATION,0.18459069020866772,"Hinton et al. [2015] proposed the concept of knowledge distillation by utilizing the ‚Äúresponse-
98"
KNOWLEDGE DISTILLATION,0.18619582664526485,"based‚Äù knowledge (i.e., soft labels) of the teacher model to transfer knowledge. However, when this
99"
KNOWLEDGE DISTILLATION,0.18780096308186195,"concept was first proposed, the features captured in the hidden layers were neglected, as they only
100"
KNOWLEDGE DISTILLATION,0.18940609951845908,"focused on the final probability distribution at that time. To better learn from teacher models, some
101"
KNOWLEDGE DISTILLATION,0.19101123595505617,"works [Zagoruyko and Komodakis, 2016, Heo et al., 2019, Chen et al., 2021] have advocated for
102"
KNOWLEDGE DISTILLATION,0.1926163723916533,"incorporating hidden feature alignment during the distillation process. In addition, relation-based
103"
KNOWLEDGE DISTILLATION,0.1942215088282504,"knowledge distillation has been introduced by Park et al. [2019], demonstrating that the interrelations
104"
KNOWLEDGE DISTILLATION,0.1958266452648475,"between training data examples were also essential.
105"
KNOWLEDGE DISTILLATION,0.19743178170144463,"Recently, there have been a few studies [Kushawaha et al., 2020, Takuya et al., 2021, Qiu et al., 2023]
106"
KNOWLEDGE DISTILLATION,0.19903691813804172,"in which knowledge distillation approaches were introduced to train SNNs. However, most of them
107"
KNOWLEDGE DISTILLATION,0.20064205457463885,"focused on image classification task only, which cannot be trivially applied to language tasks. In
108"
KNOWLEDGE DISTILLATION,0.20224719101123595,"this study, we propose a two-stage knowledge distillation approach to train the proposed SpikeBERT
109"
KNOWLEDGE DISTILLATION,0.20385232744783308,"for text classification tasks, which is among the first ones to show the feasibility of transferring the
110"
KNOWLEDGE DISTILLATION,0.20545746388443017,"knowledge to SNNs from large language models.
111"
METHOD,0.20706260032102727,"3
Method
112"
METHOD,0.2086677367576244,"In this section, we describe how we improve the architecture of Spikformer and introduce our two-
113"
METHOD,0.2102728731942215,"stage distillation approach for training SpikeBERT. Firstly, we will depict how spiking neurons and
114"
METHOD,0.21187800963081863,"surrogate gradients work in spiking neural networks. Then we will show the simple but effective
115"
METHOD,0.21348314606741572,"modification of Spikformer to enable it to represent text information. Lastly, we will illustrate
116"
METHOD,0.21508828250401285,"‚Äúpre-training + task-specific‚Äù distillation in detail.
117"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.21669341894060995,"3.1
Spiking Neurons and Surrogate Gradients
118"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.21829855537720708,"Leaky integrate-and-fire (LIF) neuron [Wu et al., 2017] is one of the most widely used spiking
119"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.21990369181380418,"neurons. Similar to the traditional activation function such as ReLU, LIF neurons operate on a
120"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.22150882825040127,"weighted sum of inputs, which contributes to the membrane potential Ut of the neuron at time step t.
121"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2231139646869984,"If membrane potential of the neuron reaches a threshold Uthr, a spike St will be generated:
122 St ="
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2247191011235955,"(
1,
if Ut ‚â•Uthr;
0,
if Ut < Uthr.
(1)"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.22632423756019263,"We can regard the dynamics of the neuron‚Äôs membrane potential as a resistor-capacitor circuit [Maas,
123"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.22792937399678972,"1997]. The approximate solution to the differential equation of this circuit can be represented as
124"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.22953451043338685,"follows:
125"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.23113964686998395,"Ut = It + Œ≤Ut‚àí1 ‚àíSt‚àí1Uthr,
It = WXt
(2)"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.23274478330658105,"where Xt are inputs to the LIF neuron at time step t, W is a set of learnable weights used to integrate
126"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.23434991974317818,"different inputs, It is the weighted sum of inputs, Œ≤ is the decay rate of membrane potential, and
127"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.23595505617977527,"Ut‚àí1 is the membrane potential at time t ‚àí1. The last term of St‚àí1Uthr is introduced to model the
128"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2375601926163724,"spiking and membrane potential reset mechanism.
129"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2391653290529695,"In addition, we follow Fang et al. [2020b] and use Arctangent-like surrogate gradients function,
130"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24077046548956663,"which regards the Heaviside step function (Equation 1) as:
131 S ‚âà1"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24237560192616373,œÄ arctan(œÄ
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24398073836276082,"2 Œ±U) + 1 2
(3)"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24558587479935795,"Therefore, the gradients of S in Equation 3 are:
132"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24719101123595505,"‚àÇS
‚àÇU = Œ±"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.24879614767255218,"2
1
(1 + ( œÄ"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2504012841091493,"2 Œ±U)2)
(4)"
SPIKING NEURONS AND SURROGATE GRADIENTS,0.2520064205457464,"where Œ± defaults to 2.
133"
SPIKEBERT ARCHITECTURE,0.2536115569823435,"3.2
SpikeBERT Architecture
134"
SPIKEBERT ARCHITECTURE,0.2552166934189406,"Spikformer [Zhou et al., 2022] is the first hardware-friendly Transformer-based spiking neural
135"
SPIKEBERT ARCHITECTURE,0.2568218298555377,"network, whose architecture is shown in Figure 1 (a). The most crucial module is the Spiking Self
136"
SPIKEBERT ARCHITECTURE,0.25842696629213485,"Attention (SSA), which utilizes discrete spikes to implement the self-attention mechanism without
137"
SPIKEBERT ARCHITECTURE,0.26003210272873195,"employing a softmax function:
138"
SPIKEBERT ARCHITECTURE,0.26163723916532905,"SSA (Qs, Ks, Vs) = S(BN(MLP(QsKT
s Vs ‚àóœÑ)))
Qs = SQs (BN (XsWQs)) ,
Ks = SKs (BN (XsWKs)) ,
Vs = SVs (BN (XsWVs))
(5)"
SPIKEBERT ARCHITECTURE,0.26324237560192615,"where S is Heaviside step function like Equation 1, Xs ‚ààRT √óL√óD is the input of SSA, T is number
139"
SPIKEBERT ARCHITECTURE,0.26484751203852325,"of time steps, BN is batch normalization, œÑ is a scaling factor. Outputs of SSA and Qs, Ks, Vs are all
140"
SPIKEBERT ARCHITECTURE,0.2664526484751204,"matrix containing 0 and 1. WQs , WKs , WVs and MLP are all learnable decimal parameters.
141"
SPIKEBERT ARCHITECTURE,0.2680577849117175,"We modify Spikformer so that it can effectively process textual data. Firstly, we replace Spiking
142"
SPIKEBERT ARCHITECTURE,0.2696629213483146,"Patch Splitting (SPS) module with a word embedding layer and a spiking neuron layer so that it can
143"
SPIKEBERT ARCHITECTURE,0.2712680577849117,"process sentences. Meanwhile, we find that the shape of the attention map in vanilla Spikformer is
144"
SPIKEBERT ARCHITECTURE,0.27287319422150885,"D √ó D where D is the dimensionality of the hidden layers, which is unreasonable in language tasks.
145"
SPIKEBERT ARCHITECTURE,0.27447833065810595,"For language tasks, the features shared with words in different positions by attention mechanism
146"
SPIKEBERT ARCHITECTURE,0.27608346709470305,"are more important than those in different dimensions. Therefore, we reshape the attention map in
147"
SPIKEBERT ARCHITECTURE,0.27768860353130015,"Spiking Self Attention (SSA) module to N ‚àóN where N is the length of inputs. Lastly, we use linear
148"
SPIKEBERT ARCHITECTURE,0.27929373996789725,"layers and layer normalization (LN) instead of convolution layers and batch normalization(BN). We
149"
SPIKEBERT ARCHITECTURE,0.2808988764044944,"show the architecture of SpikeBERT in Figure 1 (b).
150"
SPIKEBERT ARCHITECTURE,0.2825040128410915,"pos:87% neg:13%
pos:100% neg: 0%
Stage 2"
SPIKEBERT ARCHITECTURE,0.2841091492776886,"Logits Loss: ùêøùëôùëúùëîùëñùë°ùë†
Cross-entropy Loss: ùêøùëêùëí"
SPIKEBERT ARCHITECTURE,0.2857142857142857,We like this movie! ‚Ä¶ T
SPIKEBERT ARCHITECTURE,0.28731942215088285,Feature Alignment Loss: ùêøùëìùëíùëé ùëñ
SPIKEBERT ARCHITECTURE,0.28892455858747995,Embedding Alignment Loss: ùêøùëíùëöùëè ‚Ä¶ T
SPIKEBERT ARCHITECTURE,0.29052969502407705,Feature Alignment Loss: ùêøùëìùëíùëé 1
SPIKEBERT ARCHITECTURE,0.29213483146067415,We like this movie! ‚Ä¶ ‚Ä¶
SPIKEBERT ARCHITECTURE,0.29373996789727125,"Teacher: BERT
Student: SpikeBERT"
SPIKEBERT ARCHITECTURE,0.2953451043338684,Backpropagation
SPIKEBERT ARCHITECTURE,0.2969502407704655,Feature Transformation
SPIKEBERT ARCHITECTURE,0.2985553772070626,SpikeBERT Feedforward Loss
SPIKEBERT ARCHITECTURE,0.3001605136436597,Optional Loss
SPIKEBERT ARCHITECTURE,0.3017656500802568,BERT Feedforward
SPIKEBERT ARCHITECTURE,0.30337078651685395,Transformed Features of Student
SPIKEBERT ARCHITECTURE,0.30497592295345105,Embeddings of Student
SPIKEBERT ARCHITECTURE,0.30658105939004815,Features of Teacher
SPIKEBERT ARCHITECTURE,0.30818619582664525,Embeddings of Teacher
SPIKEBERT ARCHITECTURE,0.3097913322632424,Spiking Neuron Layer
SPIKEBERT ARCHITECTURE,0.3113964686998395,Spiking Features of Student
SPIKEBERT ARCHITECTURE,0.3130016051364366,pos:64% neg:36%
SPIKEBERT ARCHITECTURE,0.3146067415730337,Stage 1
SPIKEBERT ARCHITECTURE,0.3162118780096308,Two-stage Knowledge Distillation
SPIKEBERT ARCHITECTURE,0.31781701444622795,For SpikeBERT
SPIKEBERT ARCHITECTURE,0.31942215088282505,"Figure 2: Overview of our two-stage distillation method (pre-training + task-specific distillation) for
training SpikeBERT. T is the number of time steps of features in every layer. Notice that the logits
loss and cross-entropy loss are only considered in stage 2. The varying shades of color represent the
magnitude of the floating-point values. The dotted line under Li
fea indicates that features of some
hidden layers can be ignored when calculating feature alignment loss. If the student model contains
different numbers of layers from the teacher model, we will align features every few layers."
TWO-STAGE DISTILLATION,0.32102728731942215,"3.3
Two-stage Distillation
151"
TWO-STAGE DISTILLATION,0.32263242375601925,"Two-stage distillation is the key to enabling the student model with language processing ability. The
152"
TWO-STAGE DISTILLATION,0.32423756019261635,"first stage is to align the embeddings and hidden features between BERT and the improved Spikformer
153"
TWO-STAGE DISTILLATION,0.3258426966292135,"using a large-scale corpus. The second stage is to distill logits and cross-entropy information on a
154"
TWO-STAGE DISTILLATION,0.3274478330658106,"task-specific dataset from a fine-tuned BERT to the model finishing stage 1. We show the overview
155"
TWO-STAGE DISTILLATION,0.3290529695024077,"of our method in Figure 2.
156"
TWO-STAGE DISTILLATION,0.3306581059390048,"3.3.1
Stage 1. Pre-training Distillation
157"
TWO-STAGE DISTILLATION,0.33226324237560195,"Given a pre-trained BERT [Devlin et al., 2019] irrelevant to downstream tasks as teacher TM and
158"
TWO-STAGE DISTILLATION,0.33386837881219905,"an improved Spikformer as student SM, our goal in this stage is to align the embeddings and
159"
TWO-STAGE DISTILLATION,0.33547351524879615,"hidden features of TM and SM with a collection of unlabelled texts. We will introduce embedding
160"
TWO-STAGE DISTILLATION,0.33707865168539325,"alignment loss and feature alignment loss in the following.
161"
TWO-STAGE DISTILLATION,0.33868378812199035,"Feature Alignment Loss
This loss Lfea is to measure the similarity of features between TM and
162"
TWO-STAGE DISTILLATION,0.3402889245585875,"SM at every hidden layer. However, the shape of the student model‚Äôs feature Fsm at every layer
163"
TWO-STAGE DISTILLATION,0.3418940609951846,"is T √ó N √ó D but that of BERT‚Äôs feature Ftm is N √ó D, where T is the number of time steps, D
164"
TWO-STAGE DISTILLATION,0.3434991974317817,"is the dimensionality of hidden layers and L is sample length. What‚Äôs more, Fsm is a matrix only
165"
TWO-STAGE DISTILLATION,0.3451043338683788,"containing 0 and 1 but Ftm is a decimal matrix. To address the issue of different dimensions between
166"
TWO-STAGE DISTILLATION,0.3467094703049759,"Ftm and Fsm, as well as the disparity between continuous features of TM and discrete features of
167"
TWO-STAGE DISTILLATION,0.34831460674157305,"SM, a transformation strategy is necessary. We follow the feature transformation approaches of Heo
168"
TWO-STAGE DISTILLATION,0.34991974317817015,"et al. [2019], Chen et al. [2021], Qiu et al. [2023] to map the features of TM and SM to the same
169"
TWO-STAGE DISTILLATION,0.35152487961476725,"content space:
170"
TWO-STAGE DISTILLATION,0.35313001605136435,"F
‚Ä≤
tm = Ftm,
F
‚Ä≤
sm = LayerNorm(MLP( T
X"
TWO-STAGE DISTILLATION,0.3547351524879615,"t
(F t
sm)))
(6)"
TWO-STAGE DISTILLATION,0.3563402889245586,"However, we find it hard to align the features generated by the student model with those generated by
171"
TWO-STAGE DISTILLATION,0.3579454253611557,"BERT for the first few layers in this stage. We think that‚Äôs because the student model might require
172"
TWO-STAGE DISTILLATION,0.3595505617977528,"more network layers to capture the essential features via the interaction among the inputs. As shown
173"
TWO-STAGE DISTILLATION,0.3611556982343499,"in Figure 2, we choose to ignore some front layers when calculating feature alignment loss. Assume
174"
TWO-STAGE DISTILLATION,0.36276083467094705,"BERT contains B Transformer blocks (i.e., B layers) and assume the student model contains M
175"
TWO-STAGE DISTILLATION,0.36436597110754415,"Spike Transformer Block. Therefore, we will align features every ‚åàB"
TWO-STAGE DISTILLATION,0.36597110754414125,"M ‚åâlayers if B > M. For layer i
176"
TWO-STAGE DISTILLATION,0.36757624398073835,"in student model, its feature alignment loss is Li
fea = ||F
‚Ä≤
tm ‚àíF
‚Ä≤
sm||2.
177"
TWO-STAGE DISTILLATION,0.36918138041733545,"Embedding Alignment Loss
As discussed in Section 3.2, the embeddings of the input sentences
178"
TWO-STAGE DISTILLATION,0.3707865168539326,"are not in the form of spikes until they are fed forward into the Heaviside step function. Define Etm
179"
TWO-STAGE DISTILLATION,0.3723916532905297,"and Esm as the embeddings of teacher and student, respectively so the feature alignment loss is
180"
TWO-STAGE DISTILLATION,0.3739967897271268,"Li
fea = ||Etm ‚àíMLP(Esm)||2. The MLP layer is a transformation playing a similar role as that in
181"
TWO-STAGE DISTILLATION,0.3756019261637239,"Equation 6.
182"
TWO-STAGE DISTILLATION,0.37720706260032105,"To sum up, in stage 1, the total loss L1 is the sum of chosen layer‚Äôs feature alignment loss:
183"
TWO-STAGE DISTILLATION,0.37881219903691815,"L1 = œÉ1
X"
TWO-STAGE DISTILLATION,0.38041733547351525,"i
Li
fea + œÉ2Lemb
(7)"
TWO-STAGE DISTILLATION,0.38202247191011235,"where the hyperparameters œÉ1 and œÉ2 are used to balance the learning of embeddings and features.
184"
TWO-STAGE DISTILLATION,0.38362760834670945,"3.3.2
Stage 2. Task-specific Distillation
185"
TWO-STAGE DISTILLATION,0.3852327447833066,"In stage 2, we take a BERT fine-tuned on a task-specific dataset as the teacher model, and the
186"
TWO-STAGE DISTILLATION,0.3868378812199037,"model completed stage 1 as the student. To accomplish a certain language task, there should be a
187"
TWO-STAGE DISTILLATION,0.3884430176565008,"task-specific head over the basic language model as shown in Figure 2. For example, it is necessary
188"
TWO-STAGE DISTILLATION,0.3900481540930979,"to add an MLP layer over BERT for text classification. Besides, data augmentation is a commonly
189"
TWO-STAGE DISTILLATION,0.391653290529695,"used and highly effective technique in knowledge distillation[Jiao et al., 2019, Tang et al., 2019, Liu
190"
TWO-STAGE DISTILLATION,0.39325842696629215,"et al., 2022]. In the following, we will discuss our approach to data augmentation, as well as the
191"
TWO-STAGE DISTILLATION,0.39486356340288925,"logits loss and cross-entropy loss.
192"
TWO-STAGE DISTILLATION,0.39646869983948635,"Data Augmentation
In the distillation approach, a small dataset may be insufficient for the teacher
193"
TWO-STAGE DISTILLATION,0.39807383627608345,"model to fully express its knowledge[Ba and Caruana, 2013]. To tackle this issue, we augment the
194"
TWO-STAGE DISTILLATION,0.3996789727126806,"training set in order to facilitate effective knowledge distillation. We follow Tang et al. [2019] to
195"
TWO-STAGE DISTILLATION,0.4012841091492777,"augment the training set:
196"
TWO-STAGE DISTILLATION,0.4028892455858748,"‚Ä¢ Firstly, we randomly replace a word with [MASK] token with probability pmask.
197"
TWO-STAGE DISTILLATION,0.4044943820224719,"‚Ä¢ Secondly, we replace a word with another of the same POS tag with probability ppos.
198"
TWO-STAGE DISTILLATION,0.406099518459069,"‚Ä¢ Thirdly, we randomly sample an n-gram from a training example with probability png,
199"
TWO-STAGE DISTILLATION,0.40770465489566615,"where n is randomly selected from {1, 2, ..., 5}.
200"
TWO-STAGE DISTILLATION,0.40930979133226325,"Logits Loss
Following Hinton et al. [2015], we take logits, also known as soft labels, into con-
201"
TWO-STAGE DISTILLATION,0.41091492776886035,"sideration, which lets the student learn the prediction distribution of the teacher. To measure the
202"
TWO-STAGE DISTILLATION,0.41252006420545745,"distance between two distributions, we choose KL-divergence: Llogits = Pc
i pilog

pi
qi"
TWO-STAGE DISTILLATION,0.41412520064205455,"
, where c
203"
TWO-STAGE DISTILLATION,0.4157303370786517,"is the number of categories, pi and qi denote the prediction distribution of the teacher model and
204"
TWO-STAGE DISTILLATION,0.4173354735152488,"student model.
205"
TWO-STAGE DISTILLATION,0.4189406099518459,"Cross-entropy Loss
Cross-entropy loss can help the student model learn from the samples in
206"
TWO-STAGE DISTILLATION,0.420545746388443,"task-specific datasets: Lce = ‚àíPc
i ÀÜqilog (qi), where ÀÜqi represents the one-hot label vector.
207"
TWO-STAGE DISTILLATION,0.42215088282504015,"Therefore, the total loss L2 of stage 2 contains three terms:
208"
TWO-STAGE DISTILLATION,0.42375601926163725,"L2 = Œª1
X"
TWO-STAGE DISTILLATION,0.42536115569823435,"i
Li
fea + Œª2Lemb + Œª3Llogits + Œª4Lce
(8)"
TWO-STAGE DISTILLATION,0.42696629213483145,"where Œª1, Œª2, Œª3, and Œª4 are the hype-parameters that control the weight of these loss.
209"
TWO-STAGE DISTILLATION,0.42857142857142855,"For both stages, we adopt backpropagation through time (BPTT), which is suitable for training
210"
TWO-STAGE DISTILLATION,0.4301765650080257,"spiking neural networks. You can see the detailed derivation in Appendix A if interested.
211"
EXPERIMENTS,0.4317817014446228,"4
Experiments
212"
EXPERIMENTS,0.4333868378812199,"We conduct four sets of experiments. The first is to evaluate the accuracy of SpikeBERT trained
213"
EXPERIMENTS,0.434991974317817,"with the proposed method on 6 datasets of text classification datasets. The second experiment is to
214"
EXPERIMENTS,0.43659711075441415,"compare the theoretical energy consumption of BERT and that of SpikeBERT. The third experiment is
215"
EXPERIMENTS,0.43820224719101125,"an ablation study about the training process. The last experiment is to figure out how the performance
216"
EXPERIMENTS,0.43980738362760835,"of SpikeBERT is impacted by the number of time steps and model depth.
217"
DATASETS,0.44141252006420545,"4.1
Datasets
218"
DATASETS,0.44301765650080255,"As mentioned in Section 3.3.1, a large-scale parallel corpus will be used to train student models in
219"
DATASETS,0.4446227929373997,"Stage 1. For the English corpus, we choose the ‚Äú20220301.en‚Äù subset of Wikipedia1 and the whole
220"
DATASETS,0.4462279293739968,"Bookcorpus[Zhu et al., 2015], which are both utilized to pre-train a BERT [Devlin et al., 2019]. For
221"
DATASETS,0.4478330658105939,"the Chinese corpus, we choose Chinese-Wikipedia dump2 (as of Jan. 4, 2023). Additionally, we
222"
DATASETS,0.449438202247191,"follow Lv et al. [2023] to evaluate the SpikeBERT trained with the proposed distillation method on
223"
DATASETS,0.4510433386837881,"six text classification datasets: MR[Pang and Lee, 2005], SST-2[Socher et al., 2013], SST-5, Subj,
224"
DATASETS,0.45264847512038525,"ChnSenti, and Waimai. The dataset details are provided in Appendix B.
225"
IMPLEMENTATION DETAILS,0.45425361155698235,"4.2
Implementation Details
226"
IMPLEMENTATION DETAILS,0.45585874799357945,"Firstly, we set the number of encoder blocks in SpikeBERT to 12. Additionally, we set the threshold
227"
IMPLEMENTATION DETAILS,0.45746388443017655,"of common spiking neurons Uthr as 1.0 but set the threshold of neurons in the spiking self-attention
228"
IMPLEMENTATION DETAILS,0.4590690208667737,"block as 0.25 in SpikeBERT. In addition, we set decay rate Œ≤ = 0.9 and scaling factor œÑ as 0.125.
229"
IMPLEMENTATION DETAILS,0.4606741573033708,"We also set the time step T of spiking inputs as 4 and sentence length to 256 for all datasets.
230"
IMPLEMENTATION DETAILS,0.4622792937399679,"To construct SpikeBERT, we use two Pytorch-based frameworks: SnnTorch [Eshraghian et al., 2021]
231"
IMPLEMENTATION DETAILS,0.463884430176565,"and SpikingJelly [Fang et al., 2020b]. Besides, we utilize bert-base-cased 3 from Huggingface as
232"
IMPLEMENTATION DETAILS,0.4654895666131621,"teacher model for English datasets and Chinese-bert-wwm-base4 [Cui et al., 2019] for Chinese
233"
IMPLEMENTATION DETAILS,0.46709470304975925,"datasets.
234"
IMPLEMENTATION DETAILS,0.46869983948635635,"In addition, we conduct pre-training distillation on 4 NVIDIA A100-PCIE GPUs and task-specific
235"
IMPLEMENTATION DETAILS,0.47030497592295345,"distillation on 4 NVIDIA GeForce RTX 3090 GPUs. Since surrogate gradients are required during
236"
IMPLEMENTATION DETAILS,0.47191011235955055,"backpropagation, we set Œ± in Equation 3 as 2. In stage 1, we set the batch size as 128 and adopt
237"
IMPLEMENTATION DETAILS,0.47351524879614765,"AdamW [Loshchilov and Hutter, 2017] optimizer with a learning rate of 5e‚àí4 and a weight decay
238"
IMPLEMENTATION DETAILS,0.4751203852327448,"rate of 5e‚àí3. The hyperparameters œÉ1 and œÉ2 in Equation 7 are both set to 1.0. In stage 2, we set the
239"
IMPLEMENTATION DETAILS,0.4767255216693419,"batch size as 32 and the learning rate to 5e‚àí5. For data augmentation, we set pmask = ppos = 0.1,
240"
IMPLEMENTATION DETAILS,0.478330658105939,"png = 0.25. To balance the weights of the four types of loss in Equation 8, we set Œª1 = 0.1, Œª2 = 0.1,
241"
IMPLEMENTATION DETAILS,0.4799357945425361,"Œª3 = 1.0, and Œª4 = 0.1.
242"
RESULTS,0.48154093097913325,"4.3
Results
243"
RESULTS,0.48314606741573035,"We report in Table 1 the accuracy achieved by SpikeBERT trained with ‚Äúpre-training + task-specific‚Äù
244"
RESULTS,0.48475120385232745,"distillation on 6 datasets, compared to 2 baselines: 1) SNN-TextCNN proposed by Lv et al. [2023];
245"
RESULTS,0.48635634028892455,"2) improved Spikformer directly trained with gradient descent algorithm using surrogate gradients.
246"
RESULTS,0.48796147672552165,"Table 1:
Classification accuracy achieved by different methods on 6 datasets. A BERT model
fine-tuned on the dataset is denoted as ‚ÄúFT BERT‚Äù. The improved Spikformer directly trained
with surrogate gradients on the dataset is denoted as ‚ÄúDirectly-trained Spikformer‚Äù. All reported
experimental results are averaged across 10 random seeds."
RESULTS,0.4895666131621188,"Model
English Dataset
Chinese Dataset
Avg.
MR
SST-2
Subj
SST-5
ChnSenti
Waimai
TextCNN [Kim, 2014]
77.41
83.25
94.00
45.48
86.74
88.49
79.23
FT BERT [Devlin et al., 2019]
87.63
92.31
95.90
50.41
89.48
90.27
84.33
SNN-TextCNN [Lv et al., 2023]
75.45
80.91
90.60
41.63
85.02
86.66
76.71
Directly-trained Spikformer
76.38
81.55
91.80
42.02
85.45
86.93
77.36
SpikeBERT [Ours]
80.69
85.39
93.00
46.11
86.36
89.66
80.20"
RESULTS,0.4911717495987159,"Table 1 demonstrates that the SpikeBERT trained with two-stage distillation achieves state-out-of-
247"
RESULTS,0.492776886035313,"art performance across 6 text classification datasets. Compared to SNN-TextCNN, SpikeBERT
248"
RESULTS,0.4943820224719101,"achieved up to 5.42% improvement in accuracy (3.49% increase on average) for all text classification
249"
RESULTS,0.4959871589085072,"benchmarks. Furthermore, SpikeBERT outperforms TextCNN, which is considered a representative
250"
RESULTS,0.49759229534510435,"1https://dumps.wikimedia.org/
2https://dumps.wikimedia.org/zhwiki/latest/
3https://huggingface.co/bert-base-cased
4https://huggingface.co/hfl/chinese-bert-wwm"
RESULTS,0.49919743178170145,"artificial neural network, and even achieves comparable results to the fine-tuned BERT by a small
251"
RESULTS,0.5008025682182986,"drop of 4.13% on average in accuracy for text classification task. What‚Äôs more, Table 1 demonstrates
252"
RESULTS,0.5024077046548957,"that SpikeBERT can also be applied well in Chinese datasets (ChnSenti and Waimai).
253"
RESULTS,0.5040128410914928,"Fang et al. [2020a] propose that, in image classification task, surrogate gradients of SNNs may lead
254"
RESULTS,0.5056179775280899,"to gradient vanishing or exploding and it is even getting worse with the increase of model depth. We
255"
RESULTS,0.507223113964687,"found this phenomenon in language tasks as well. Table 1 reveals that the accuracy of directly-trained
256"
RESULTS,0.5088282504012841,"Spikformer is noticeably lower than SpikeBERT on some benchmarks, such as MR, SST-5, and
257"
RESULTS,0.5104333868378812,"ChnSenti. This is likely because the directly-trained Spikformer models have not yet fully converged
258"
RESULTS,0.5120385232744783,"due to gradient vanishing or exploding.
259"
ENERGY CONSUMPTION,0.5136436597110754,"4.4
Energy Consumption
260"
ENERGY CONSUMPTION,0.5152487961476726,"An essential advantage of SNNs is the low consumption of energy during inference. We compare the
261"
ENERGY CONSUMPTION,0.5168539325842697,"theoretical energy consumption per sample of fine-tuned BERT and SpikeBERT on 6 test datasets
262"
ENERGY CONSUMPTION,0.5184590690208668,"and report the results in Table 2. The way to calculate floating point operations (FLOPs), synaptic
263"
ENERGY CONSUMPTION,0.5200642054574639,"operations (SOPs), and the theoretical energy consumption (Power) is shown in Appendix C.
264"
ENERGY CONSUMPTION,0.521669341894061,"Table 2: Energy consumption per sample of fine-tuned BERT and SpikeBERT during inference on 6
text classification benchmarks. ‚ÄúFLOPs‚Äù denotes the floating point operations of fine-tuned BERT.
‚ÄúSOPs‚Äù denotes the synaptic operations of SpikeBERT. ‚ÄúPower‚Äù denotes the average theoretical
energy required for each test example prediction."
ENERGY CONSUMPTION,0.5232744783306581,"Dataset
Model
FLOPs / SOPs(G)
Power (mJ)
Energy Reduction
Accuracy (%)"
ENERGY CONSUMPTION,0.5248796147672552,"ChnSenti
FT BERT
22.46
103.38
73.28% ‚Üì
89.48
SpikeBERT
28.47
27.62
86.36"
ENERGY CONSUMPTION,0.5264847512038523,"Waimai
FT BERT
22.46
103.38
73.91% ‚Üì
90.27
SpikeBERT
27.81
26.97
89.66"
ENERGY CONSUMPTION,0.5280898876404494,"MR
FT BERT
22.23
102.24
74.93% ‚Üì
87.63
SpikeBERT
26.94
25.63
80.69"
ENERGY CONSUMPTION,0.5296950240770465,"SST-2
FT BERT
22.23
102.24
73.78% ‚Üì
92.31
SpikeBERT
27.46
26.81
85.39"
ENERGY CONSUMPTION,0.5313001605136437,"Subj
FT BERT
22.23
102.24
77.17% ‚Üì
95.90
SpikeBERT
25.92
23.34
93.00"
ENERGY CONSUMPTION,0.5329052969502408,"SST-5
FT BERT
22.23
102.24
76.92% ‚Üì
50.41
SpikeBERT
26.01
23.60
46.11"
ENERGY CONSUMPTION,0.5345104333868379,"It is worth noting that the energy consumption of SpikeBERT is significantly lower than that of
265"
ENERGY CONSUMPTION,0.536115569823435,"fine-tuned BERT, which is an important advantage of SNNs over ANNs in terms of energy efficiency.
266"
ENERGY CONSUMPTION,0.5377207062600321,"As shown in Table 2, SpikeBERT demands only 25.00% of the energy that fine-tuned BERT needs to
267"
ENERGY CONSUMPTION,0.5393258426966292,"achieve comparable performance on average. Moreover, on the Subj dataset, SpikeBERT can reduce
268"
ENERGY CONSUMPTION,0.5409309791332263,"energy consumption by up to 77.17% compared to fine-tuned BERT for predicting each text example.
269"
ENERGY CONSUMPTION,0.5425361155698234,"This indicates that SpikeBERT is a promising candidate for energy-efficient text classification in
270"
ENERGY CONSUMPTION,0.5441412520064205,"resource-constrained scenarios.
271"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5457463884430177,"4.5
Ablation Study and Impact of Hyper-parameters
272"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5473515248796148,"In this section, we conduct ablation studies to investigate the contributions of: a) different stages of
273"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5489566613162119,"the proposed knowledge distillation method, and b) different types of loss in Equation 8.
274"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.550561797752809,"As we can see in Table 4.5, SpikeBERTs without either stage 1 or stage 2 experience about 3.20%
275"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5521669341894061,"performance drop on average. Therefore, we conclude that the two distillation stages are both
276"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5537720706260032,"essential for training SpikeBERT. Furthermore, we observed that the average performance dropped
277"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5553772070626003,"from 76.30 to 73.27 when excluding the logits loss, demonstrating that the logits loss Llogits has the
278"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5569823434991974,"greatest impact on task-specific distillation. Meanwhile, data augmentation (DA) plays an important
279"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5585874799357945,"role in Stage 2, contributing to an increase in average performance from 75.54 to 76.30.
280"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5601926163723917,"We investigate how the performance of SpikeBERT is affected by the two important hyperparameters:
281"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5617977528089888,"time steps T and model depth. To this end, we conduct two experiments: (a) varying the number of
282"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5634028892455859,"Table 3: Ablation studies of the two-stage distillation method. Row 3 and 4 show ablation experiment
results on the two steps of our proposed method. Row 5 to 9 are ablation experiment results on
different parts of Equation 8. ‚ÄúDA‚Äù stands for data augmentation."
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.565008025682183,"Models
MR
SST-2
Subj
SST-5
Avg.
Drop
SpikeBERT
80.69
85.39
93.00
46.11
76.30
‚àí
w/o Stage 1
76.04
82.26
91.80
42.16
73.07
-3.23
w/o Stage 2
75.91
82.26
91.90
42.58
73.14
-3.16"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5666131621187801,Stage 2
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5682182985553772,"w/o DA
80.22
84.90
92.20
44.84
75.54
-0.76
w/o Lfea
78.35
83.48
92.20
43.57
74.40
-1.90
w/o Lemb
79.67
83.10
92.00
43.48
74.56
-1.74
w/o Llogits
76.19
82.64
91.90
42.35
73.27
-3.03
w/o Lce
80.43
85.23
93.00
45.86
76.13
-0.17"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5698234349919743,"the time steps of spike inputs when training SpikeBERT; and (b) training a variant of SpikeBERT
283"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5714285714285714,"with different encoder block depths, specifically 6, 12, 18, using our proposed two-stage method.
284"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5730337078651685,"Figure 3 (a) shows how the accuracy of SpikeBERT varies with the increase of time steps. We find
285"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5746388443017657,"that, with the increase of time steps, the accuracy increases first, then remains unchanged, and reaches
286"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5762439807383628,"its maximum roughly at T = 4. Theoretically, the performance of SpikeBERT should be higher
287"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5778491171749599,"with bigger time steps. However, the performance of models with 8 and 12 time steps is even worse
288"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.579454253611557,"than that with 4 time steps on ChnSenti and Waimai datasets. A plausible explanation is that using
289"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5810593900481541,"excessively large time steps may introduce too much noise in the spike trains.
290"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5826645264847512,"1
2
4
8
12
Time Steps 80 85 90 95"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5842696629213483,Accuracy %
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5858747993579454,"sst2
Subj
ChnSenti
Waimai (a)"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5874799357945425,"MR
SST2
Subj
SST5
Dataset Name 40 60 80"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5890850722311396,Accuracy %
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5906902086677368,"Depth 6
Depth 12
Depth 18 (b)"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5922953451043339,Figure 3: (a) Accuracy versus the number of time steps. (b) Accuracy influenced by model depth.
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.593900481540931,"In addition, as we can see from Figure 3 (b), the accuracy of SpikeBERT is generally insensitive to the
291"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5955056179775281,"model depths and even gets lower in some datasets. We think that‚Äôs because more spike Transformer
292"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5971107544141252,"blocks mean more spiking neurons (See Section 2.1), introducing more surrogate gradients when
293"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.5987158908507223,"error backpropagation through time. Higher model depth often brings better model performance for
294"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.6003210272873194,"traditional deep neural networks. However, it seems that deeper spiking neural networks cannot make
295"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.6019261637239165,"further progress in performance. Many previous SNNs works [Zheng et al., 2020, Fang et al., 2020a,
296"
ABLATION STUDY AND IMPACT OF HYPER-PARAMETERS,0.6035313001605136,"Kim et al., 2022b] have proved this deduction.
297"
CONCLUSION,0.6051364365971108,"5
Conclusion
298"
CONCLUSION,0.6067415730337079,"In this study, we extended and improved Spikformer to process language tasks and proposed a new
299"
CONCLUSION,0.608346709470305,"promising training paradigm for training SpikeBERT inspired by the notion of knowledge distillation.
300"
CONCLUSION,0.6099518459069021,"We presented a two-stage, ‚Äúpre-training + task-specific‚Äù knowledge distillation method by transferring
301"
CONCLUSION,0.6115569823434992,"the knowledge from BERTs to SpikeBERT for text classification tasks. We empirically show that
302"
CONCLUSION,0.6131621187800963,"our SpikeBERT outperforms the state-of-the-art SNNs and can even achieve comparable results to
303"
CONCLUSION,0.6147672552166934,"BERTs with much less energy consumption across multiple datasets for both English and Chinese,
304"
CONCLUSION,0.6163723916532905,"leading to future energy-efficient implementations of BERTs or large language models.
305"
REFERENCES,0.6179775280898876,"References
306"
REFERENCES,0.6195826645264848,"OpenAI. Introducing chatgpt. 2022. URL https://openai.com/blog/chatgpt.
307"
REFERENCES,0.6211878009630819,"OpenAI. GPT-4 technical report. 2023. URL https://arxiv.org/abs/2303.08774.
308"
REFERENCES,0.622792937399679,"Wofgang Maas. Networks of spiking neurons: the third generation of neural network models. Neural
309"
REFERENCES,0.6243980738362761,"Networks, 14:1659‚Äì1671, 1997.
310"
REFERENCES,0.6260032102728732,"Kaushik Roy, Akhilesh R. Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelli-
311"
REFERENCES,0.6276083467094703,"gence with neuromorphic computing. Nature, 575:607‚Äì617, 2019.
312"
REFERENCES,0.6292134831460674,"Yangfan Hu, Huajin Tang, and Gang Pan. Spiking deep residual networks. IEEE Transactions on
313"
REFERENCES,0.6308186195826645,"Neural Networks and Learning Systems, 2018.
314"
REFERENCES,0.6324237560192616,"Bojian Yin, Federico Corradi, and Sander M. Boht‚Äôe. Effective and efficient computation with
315"
REFERENCES,0.6340288924558587,"multiple-timescale spiking recurrent neural networks. International Conference on Neuromorphic
316"
REFERENCES,0.6356340288924559,"Systems 2020, 2020.
317"
REFERENCES,0.637239165329053,"Wei Fang, Zhaofei Yu, Yanqing Chen, Tiejun Huang, Timoth√©e Masquelier, and Yonghong Tian.
318"
REFERENCES,0.6388443017656501,"Deep residual learning in spiking neural networks. In Neural Information Processing Systems,
319"
REFERENCES,0.6404494382022472,"2021.
320"
REFERENCES,0.6420545746388443,"Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang. Optimal ann-snn conversion for fast
321"
REFERENCES,0.6436597110754414,"and accurate inference in deep spiking neural networks. ArXiv, abs/2105.11654, 2021.
322"
REFERENCES,0.6452648475120385,"Youngeun Kim, Yuhang Li, Hyoungseob Park, Yeshwanth Venkatesha, and Priyadarshini Panda.
323"
REFERENCES,0.6468699839486356,"Neural architecture search for spiking neural networks. ArXiv, abs/2201.10355, 2022a.
324"
REFERENCES,0.6484751203852327,"Zhaokun Zhou, Yue sheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Liuliang
325"
REFERENCES,0.6500802568218299,"Yuan. Spikformer: When spiking neural network meets transformer. ArXiv, abs/2209.15425, 2022.
326"
REFERENCES,0.651685393258427,"Philipp Plank, A. Rao, Andreas Wild, and Wolfgang Maass. A long short-term memory for ai
327"
REFERENCES,0.6532905296950241,"applications in spike-based neuromorphic hardware. Nat. Mach. Intell., 4:467‚Äì479, 2021.
328"
REFERENCES,0.6548956661316212,"Changze Lv, Jianhan Xu, and Xiaoqing Zheng. Spiking convolutional neural networks for text
329"
REFERENCES,0.6565008025682183,"classification. In The Eleventh International Conference on Learning Representations, 2023.
330"
REFERENCES,0.6581059390048154,"Rui-Jie Zhu, Qihang Zhao, and Jason K Eshraghian. Spikegpt: Generative pre-trained language
331"
REFERENCES,0.6597110754414125,"model with spiking neural networks. arXiv preprint arXiv:2302.13939, 2023.
332"
REFERENCES,0.6613162118780096,"Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP, 2014.
333"
REFERENCES,0.6629213483146067,"Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
334"
REFERENCES,0.6645264847512039,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.
335"
REFERENCES,0.666131621187801,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
336"
REFERENCES,0.6677367576243981,"bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.
337"
REFERENCES,0.6693418940609952,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
338"
REFERENCES,0.6709470304975923,"Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
339"
REFERENCES,0.6725521669341894,"approach. ArXiv, abs/1907.11692, 2019.
340"
REFERENCES,0.6741573033707865,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
341"
REFERENCES,0.6757624398073836,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
342"
REFERENCES,0.6773675762439807,"and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
343"
REFERENCES,0.6789727126805778,"ArXiv, abs/2010.11929, 2020.
344"
REFERENCES,0.680577849117175,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale
345"
REFERENCES,0.6821829855537721,"hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition,
346"
REFERENCES,0.6837881219903692,"pages 248‚Äì255, 2009.
347"
REFERENCES,0.6853932584269663,"Wei Fang, Zhaofei Yu, Yanqing Chen, Timoth√©e Masquelier, Tiejun Huang, and Yonghong Tian.
348"
REFERENCES,0.6869983948635634,"Incorporating learnable membrane time constant to enhance learning of spiking neural networks.
349"
REFERENCES,0.6886035313001605,"2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2641‚Äì2651, 2020a.
350"
REFERENCES,0.6902086677367576,"Paul J. Werbos. Backpropagation through time: What it does and how to do it. Proc. IEEE, 78:
351"
REFERENCES,0.6918138041733547,"1550‚Äì1560, 1990.
352"
REFERENCES,0.6934189406099518,"Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
353"
REFERENCES,0.695024077046549,"ArXiv, abs/1503.02531, 2015.
354"
REFERENCES,0.6966292134831461,"Haonan Qiu, Munan Ning, Li Yuan, Wei Fang, Yanqi Chen, Changlin Li, Tao Sun, Zhengyu Ma, and
355"
REFERENCES,0.6982343499197432,"Yonghong Tian. Self-architectural knowledge distillation for spiking neural networks, 2023.
356"
REFERENCES,0.6998394863563403,"Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to fine-tune bert for text classification? In
357"
REFERENCES,0.7014446227929374,"China National Conference on Chinese Computational Linguistics, 2019.
358"
REFERENCES,0.7030497592295345,"Yang Liu. Fine-tune bert for extractive summarization. ArXiv, abs/1903.10318, 2019.
359"
REFERENCES,0.7046548956661316,"Suchin Gururangan, Ana Marasovi¬¥c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
360"
REFERENCES,0.7062600321027287,"and Noah A. Smith. Don‚Äôt stop pretraining: Adapt language models to domains and tasks. ArXiv,
361"
REFERENCES,0.7078651685393258,"abs/2004.10964, 2020.
362"
REFERENCES,0.709470304975923,"Eugene M. Izhikevich. Simple model of spiking neurons. IEEE transactions on neural networks, 14
363"
REFERENCES,0.7110754414125201,"6:1569‚Äì72, 2003.
364"
REFERENCES,0.7126805778491172,"Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for
365"
REFERENCES,0.7142857142857143,"training high-performance spiking neural networks. Frontiers in Neuroscience, 12, 2017.
366"
REFERENCES,0.7158908507223114,"Peter Udo Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer.
367"
REFERENCES,0.7174959871589085,"Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.
368"
REFERENCES,0.7191011235955056,"2015 International Joint Conference on Neural Networks (IJCNN), pages 1‚Äì8, 2015.
369"
REFERENCES,0.7207062600321027,"Yongqiang Cao, Yang Chen, and Deepak Khosla. Spiking deep convolutional neural networks for
370"
REFERENCES,0.7223113964686998,"energy-efficient object recognition. International Journal of Computer Vision, 113:54‚Äì66, 2015.
371"
REFERENCES,0.723916532905297,"Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver-
372"
REFERENCES,0.7255216693418941,"sion of continuous-valued deep networks to efficient event-driven networks for image classification.
373"
REFERENCES,0.7271268057784912,"Frontiers in Neuroscience, 11, 2017.
374"
REFERENCES,0.7287319422150883,"Peter Udo Diehl, Guido Zarrella, Andrew S. Cassidy, Bruno U. Pedroni, and Emre O. Neftci.
375"
REFERENCES,0.7303370786516854,"Conversion of artificial recurrent neural networks to spiking neural networks for low-power
376"
REFERENCES,0.7319422150882825,"neuromorphic hardware. 2016 IEEE International Conference on Rebooting Computing (ICRC),
377"
REFERENCES,0.7335473515248796,"pages 1‚Äì8, 2016.
378"
REFERENCES,0.7351524879614767,"Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
379"
REFERENCES,0.7367576243980738,"neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
380"
REFERENCES,0.7383627608346709,"IEEE Signal Processing Magazine, 36:51‚Äì63, 2019.
381"
REFERENCES,0.7399678972712681,"Friedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multilayer spiking neural
382"
REFERENCES,0.7415730337078652,"networks. Neural Computation, 30:1514 ‚Äì 1541, 2017.
383"
REFERENCES,0.7431781701444623,"Nan Zheng and Pinaki Mazumder. A low-power hardware architecture for on-line supervised learning
384"
REFERENCES,0.7447833065810594,"in multi-layer spiking neural networks. 2018 IEEE International Symposium on Circuits and
385"
REFERENCES,0.7463884430176565,"Systems (ISCAS), pages 1‚Äì5, 2018.
386"
REFERENCES,0.7479935794542536,"S. Shrestha and G. Orchard. Slayer: Spike layer error reassignment in time. ArXiv, abs/1810.08646,
387"
REFERENCES,0.7495987158908507,"2018.
388"
REFERENCES,0.7512038523274478,"Taewook Kang, Kwang-Il Oh, Jaejin Lee, and Wangrok Oh. Comparison between stdp and gradient-
389"
REFERENCES,0.7528089887640449,"descent training processes for spiking neural networks using mnist digits. 2022 13th International
390"
REFERENCES,0.7544141252006421,"Conference on Information and Communication Technology Convergence (ICTC), pages 1732‚Äì
391"
REFERENCES,0.7560192616372392,"1734, 2022.
392"
REFERENCES,0.7576243980738363,"Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E.
393"
REFERENCES,0.7592295345104334,"Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition.
394"
REFERENCES,0.7608346709470305,"Neural Computation, 1:541‚Äì551, 1989.
395"
REFERENCES,0.7624398073836276,"Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger
396"
REFERENCES,0.7640449438202247,"spiking neural networks. In AAAI Conference on Artificial Intelligence, 2020.
397"
REFERENCES,0.7656500802568218,"Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Enabling deep
398"
REFERENCES,0.7672552166934189,"spiking neural networks with hybrid conversion and spike timing dependent backpropagation.
399"
REFERENCES,0.7688603531300161,"ArXiv, abs/2005.01807, 2020.
400"
REFERENCES,0.7704654895666132,"Sergey Zagoruyko and Nikos Komodakis.
Paying more attention to attention: Improving the
401"
REFERENCES,0.7720706260032103,"performance of convolutional neural networks via attention transfer. ArXiv, abs/1612.03928, 2016.
402"
REFERENCES,0.7736757624398074,"Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A
403"
REFERENCES,0.7752808988764045,"comprehensive overhaul of feature distillation. 2019 IEEE/CVF International Conference on
404"
REFERENCES,0.7768860353130016,"Computer Vision (ICCV), pages 1921‚Äì1930, 2019.
405"
REFERENCES,0.7784911717495987,"Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge
406"
REFERENCES,0.7800963081861958,"review. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
407"
REFERENCES,0.7817014446227929,"5006‚Äì5015, 2021.
408"
REFERENCES,0.78330658105939,"Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. 2019
409"
REFERENCES,0.7849117174959872,"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3962‚Äì3971,
410"
REFERENCES,0.7865168539325843,"2019.
411"
REFERENCES,0.7881219903691814,"R. K. Kushawaha, S. Kumar, Biplab Banerjee, and Rajbabu Velmurugan. Distilling spikes: Knowledge
412"
REFERENCES,0.7897271268057785,"distillation in spiking neural networks. 2020 25th International Conference on Pattern Recognition
413"
REFERENCES,0.7913322632423756,"(ICPR), pages 4536‚Äì4543, 2020.
414"
REFERENCES,0.7929373996789727,"Sugahara Takuya, Renyuan Zhang, and Yasuhiko Nakashima. Training low-latency spiking neural
415"
REFERENCES,0.7945425361155698,"network through knowledge distillation. 2021 IEEE Symposium in Low-Power and High-Speed
416"
REFERENCES,0.7961476725521669,"Chips (COOL CHIPS), pages 1‚Äì3, 2021.
417"
REFERENCES,0.797752808988764,"Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Yonghong Tian, and
418"
REFERENCES,0.7993579454253612,"other contributors. Spikingjelly, 2020b. Accessed: YYYY-MM-DD.
419"
REFERENCES,0.8009630818619583,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
420"
REFERENCES,0.8025682182985554,"Tinybert: Distilling bert for natural language understanding. In Findings, 2019.
421"
REFERENCES,0.8041733547351525,"Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy J. Lin. Distilling
422"
REFERENCES,0.8057784911717496,"task-specific knowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019.
423"
REFERENCES,0.8073836276083467,"Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao. Multi-granularity structural knowl-
424"
REFERENCES,0.8089887640449438,"edge distillation for language model compression. In Annual Meeting of the Association for
425"
REFERENCES,0.8105939004815409,"Computational Linguistics, 2022.
426"
REFERENCES,0.812199036918138,"Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS, 2013.
427"
REFERENCES,0.8138041733547352,"Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
428"
REFERENCES,0.8154093097913323,"and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
429"
REFERENCES,0.8170144462279294,"movies and reading books. 2015 IEEE International Conference on Computer Vision (ICCV),
430"
REFERENCES,0.8186195826645265,"pages 19‚Äì27, 2015.
431"
REFERENCES,0.8202247191011236,"Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
432"
REFERENCES,0.8218298555377207,"with respect to rating scales. In ACL, 2005.
433"
REFERENCES,0.8234349919743178,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and
434"
REFERENCES,0.8250401284109149,"Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
435"
REFERENCES,0.826645264847512,"In EMNLP, 2013.
436"
REFERENCES,0.8282504012841091,"Jason Kamran Eshraghian, Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi,
437"
REFERENCES,0.8298555377207063,"Bennamoun, Doo Seok Jeong, and Wei D. Lu. Training spiking neural networks using lessons
438"
REFERENCES,0.8314606741573034,"from deep learning. ArXiv, abs/2109.12894, 2021.
439"
REFERENCES,0.8330658105939005,"Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu.
440"
REFERENCES,0.8346709470304976,"Pre-training with whole word masking for chinese bert. IEEE/ACM Transactions on Audio, Speech,
441"
REFERENCES,0.8362760834670947,"and Language Processing, 29:3504‚Äì3514, 2019.
442"
REFERENCES,0.8378812199036918,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
443"
REFERENCES,0.8394863563402889,"ence on Learning Representations, 2017.
444"
REFERENCES,0.841091492776886,"Youngeun Kim, Joshua Chough, and Priyadarshini Panda. Beyond classification: Directly training
445"
REFERENCES,0.8426966292134831,"spiking neural networks for semantic segmentation. Neuromorphic Computing and Engineering, 2
446"
REFERENCES,0.8443017656500803,"(4):044015, 2022b.
447"
REFERENCES,0.8459069020866774,"G. Indiveri, Federico Corradi, and Ning Qiao. Neuromorphic architectures for spiking deep neural
448"
REFERENCES,0.8475120385232745,"networks. 2015 IEEE International Electron Devices Meeting (IEDM), pages 4.2.1‚Äì4.2.4, 2015.
449"
REFERENCES,0.8491171749598716,"Appendix
450"
REFERENCES,0.8507223113964687,"A
Backpropagation Through Time in Spiking Neural Networks
451"
REFERENCES,0.8523274478330658,"The content of this section is mostly referred to Lv et al. [2023].
452"
REFERENCES,0.8539325842696629,"Given a loss function L like Eqution 7 and 8, the losses at every time step can be summed together to
453"
REFERENCES,0.85553772070626,"give the following global gradient:
454"
REFERENCES,0.8571428571428571,"‚àÇL
‚àÇW =
X t"
REFERENCES,0.8587479935794543,"‚àÇLt
‚àÇW =
X i X j‚â§i"
REFERENCES,0.8603531300160514,"‚àÇLi
‚àÇWj
‚àÇWj"
REFERENCES,0.8619582664526485,"‚àÇW
(9)"
REFERENCES,0.8635634028892456,"where i and j denote different time steps, and Lt is the loss calculated at time step t. No matter which
455"
REFERENCES,0.8651685393258427,"time step is, the weights of an SNN are shared across all steps. Therefore, we have W0 = W1 =
456"
REFERENCES,0.8667736757624398,"¬∑ ¬∑ ¬∑ = W, which also indicates that ‚àÇWj"
REFERENCES,0.8683788121990369,"‚àÇW = 1. Thus, Equation (9) can be written as follows:
457"
REFERENCES,0.869983948635634,"‚àÇL
‚àÇW =
X i X j‚â§i"
REFERENCES,0.8715890850722311,"‚àÇLi
‚àÇWj
(10)"
REFERENCES,0.8731942215088283,"Based on the chain rule of derivatives, we obtain:
458"
REFERENCES,0.8747993579454254,"‚àÇL
‚àÇW =
X i X j‚â§i"
REFERENCES,0.8764044943820225,"‚àÇLi
‚àÇSi
‚àÇSi
‚àÇUi
‚àÇUi
‚àÇWj =
X i"
REFERENCES,0.8780096308186196,"‚àÇLi
‚àÇSi
‚àÇSi
‚àÇUi X j‚â§i"
REFERENCES,0.8796147672552167,"‚àÇUi
‚àÇWj (11)"
REFERENCES,0.8812199036918138,where ‚àÇLi
REFERENCES,0.8828250401284109,"‚àÇSi is the derivative of the cross-entropy loss at the time step i with respect to Si, and ‚àÇSi"
REFERENCES,0.884430176565008,"‚àÇUi can
459"
REFERENCES,0.8860353130016051,be easily derived using surrogate gradients like Equation 3. As to the last term of P
REFERENCES,0.8876404494382022,"j‚â§i
‚àÇUi
‚àÇWj , we can
460"
REFERENCES,0.8892455858747994,"split it into two parts:
461 X j‚â§i"
REFERENCES,0.8908507223113965,"‚àÇUi
‚àÇWj = ‚àÇUi"
REFERENCES,0.8924558587479936,"‚àÇWi +
X j‚â§i‚àí1"
REFERENCES,0.8940609951845907,"‚àÇUi
‚àÇWj
(12)"
REFERENCES,0.8956661316211878,"From Equation (2), we know that ‚àÇUi"
REFERENCES,0.8972712680577849,"‚àÇWi = Xi. Therefore, Equation (9) can be simplified as follows:
462"
REFERENCES,0.898876404494382,"‚àÇL
‚àÇW =
X i"
REFERENCES,0.9004815409309791,"‚àÇLi
‚àÇSi
‚àÇSi
‚àÇUi
|
{z
}
constant Ô£´"
REFERENCES,0.9020866773675762,"Ô£¨
Ô£¨
Ô£¨
Ô£≠
‚àÇUi
‚àÇWj
| {z }
constant +
X j‚â§i‚àí1"
REFERENCES,0.9036918138041734,"‚àÇUi
‚àÇWj Ô£∂"
REFERENCES,0.9052969502407705,"Ô£∑
Ô£∑
Ô£∑
Ô£∏
(13)"
REFERENCES,0.9069020866773676,"By the chain rule of derivatives over time, ‚àÇUi"
REFERENCES,0.9085072231139647,"‚àÇWj can be factorized into two parts:
463"
REFERENCES,0.9101123595505618,"‚àÇUi
‚àÇWj =
‚àÇUi
‚àÇUi‚àí1
‚àÇUi‚àí1"
REFERENCES,0.9117174959871589,"‚àÇWj
(14)"
REFERENCES,0.913322632423756,"It is easy to see that
‚àÇUi
‚àÇUi‚àí1 is equal to Œ≤ from Equation (2), and Equation (9) can be written as:
464"
REFERENCES,0.9149277688603531,"‚àÇL
‚àÇW =
X i"
REFERENCES,0.9165329052969502,"‚àÇLi
‚àÇSi
‚àÇSi
‚àÇUi
|
{z
}
constant Ô£´"
REFERENCES,0.9181380417335474,"Ô£¨
Ô£¨
Ô£¨
Ô£≠
‚àÇUi
‚àÇWj
| {z }
constant +
X j‚â§i‚àí1"
REFERENCES,0.9197431781701445,"‚àÇUi
‚àÇUi‚àí1
| {z }
constant ‚àÇUi‚àí1 ‚àÇWj Ô£∂"
REFERENCES,0.9213483146067416,"Ô£∑
Ô£∑
Ô£∑
Ô£∏
(15)"
REFERENCES,0.9229534510433387,We can treat ‚àÇUi‚àí1
REFERENCES,0.9245585874799358,"‚àÇWj recurrently as Equation (12). Finally, we can update the weights W by the rule of
465"
REFERENCES,0.9261637239165329,W = W ‚àíŒ∑ ‚àÇL
REFERENCES,0.92776886035313,"‚àÇW , where Œ∑ is a learning rate.
466"
REFERENCES,0.9293739967897271,"B
Datasets
467"
REFERENCES,0.9309791332263242,"The benchmark we used in Table 1 includes the following datasets:
468"
REFERENCES,0.9325842696629213,"‚Ä¢ MR: MR stands for Movie Review and it consists of movie-review documents labeled with
469"
REFERENCES,0.9341894060995185,"respect to their overall sentiment polarity (positive or negative) or subjective rating [Pang
470"
REFERENCES,0.9357945425361156,"and Lee, 2005].
471"
REFERENCES,0.9373996789727127,"‚Ä¢ SST-5: SST-5 contains 11, 855 sentences extracted from movie reviews for sentiment
472"
REFERENCES,0.9390048154093098,"classification [Socher et al., 2013]. There are 5 categories (very negative, negative, neutral,
473"
REFERENCES,0.9406099518459069,"positive, and very positive).
474"
REFERENCES,0.942215088282504,"‚Ä¢ SST-2: The binary version of SST-5. There are just 2 classes (positive and negative).
475"
REFERENCES,0.9438202247191011,"‚Ä¢ Subj: The task of this dataset is to classify a sentence as being subjective or objective5.
476"
REFERENCES,0.9454253611556982,"‚Ä¢ ChnSenti: ChnSenti comprises about 7, 000 Chinese hotel reviews annotated with positive
477"
REFERENCES,0.9470304975922953,"or negative labels6.
478"
REFERENCES,0.9486356340288925,"‚Ä¢ Waimai: There are about 12, 000 Chinese user reviews collected by a food delivery platform
479"
REFERENCES,0.9502407704654896,"for binary sentiment classification (positive and negative)7 in this dataset.
480"
REFERENCES,0.9518459069020867,"C
Theoretical Energy Consumption Calculation
481"
REFERENCES,0.9534510433386838,"For spiking neural networks (SNNs), the theoretical energy consumption of layer Œæ can be calculated
482"
REFERENCES,0.9550561797752809,"as
483"
REFERENCES,0.956661316211878,"Power(Œæ) = 0.9pJ √ó SOPs(Œæ)
(16)"
REFERENCES,0.9582664526484751,"where 0.9pJ is the energy consumption per synaptic operation (SOP) [Indiveri et al., 2015, Hu et al.,
484"
REFERENCES,0.9598715890850722,"2018, Zhou et al., 2022]. The number of synaptic operations at the layer Œæ of an SNN is estimated as
485"
REFERENCES,0.9614767255216693,"SOPs(Œæ) = T √ó Œ≥ √ó FLOPs(Œæ)
(17)"
REFERENCES,0.9630818619582665,"where T is the number of times step required in the simulation, Œ≥ is the firing rate of input spike train
486"
REFERENCES,0.9646869983948636,"of the layer Œæ, and FLOPs(Œæ) is the estimated floating point operations at the layer Œæ.
487"
REFERENCES,0.9662921348314607,"For classical artificial neural networks, the theoretical energy consumption required by the layer Œæ
488"
REFERENCES,0.9678972712680578,"can be estimated by
489"
REFERENCES,0.9695024077046549,"Power(Œæ) = 4.6pJ ‚àóFLOPs(Œæ)
(18)"
REFERENCES,0.971107544141252,"Note that 1J = 103 mJ = 1012 pJ.
490"
REFERENCES,0.9727126805778491,"D
Discussion of Limitations
491"
REFERENCES,0.9743178170144462,"In the image classification task, spiking neural networks have demonstrated comparable performance
492"
REFERENCES,0.9759229534510433,"to ViT on CIFAR-10-DVS and DVS-128-Gesture datasets, which are neuromorphic event-based
493"
REFERENCES,0.9775280898876404,"image datasets created using dynamic vision sensors. We think that the performance gap bewteen
494"
REFERENCES,0.9791332263242376,"SNNs and ANNs in language tasks is mainly due to the lack of neuromorphic language datasets. It is
495"
REFERENCES,0.9807383627608347,"unfair to evaluate SNNs on the datasets that were created to train and evaluate ANNs because these
496"
REFERENCES,0.9823434991974318,"datasets are mostly processed by continuous values. However, it is quite hard to convert language
497"
REFERENCES,0.9839486356340289,"to neuromorphic information without information loss. We hope there will be a new technology to
498"
REFERENCES,0.985553772070626,"transfer senteces to neuromorphic spikes.
499"
REFERENCES,0.9871589085072231,"In addition, GPU memory poses a limitation in our experiments. Spiking neural networks have an
500"
REFERENCES,0.9887640449438202,"additional dimension, denoted as T (time step), compared to artificial neural networks. Increasing
501"
REFERENCES,0.9903691813804173,"the number of time steps allows for capturing more information but results in an increased demand
502"
REFERENCES,0.9919743178170144,"for GPU memory by a factor of T. During our experiments, we observe that maintaining the
503"
REFERENCES,0.9935794542536116,"same number of time steps during training requires reducing the sentence length of input sentences,
504"
REFERENCES,0.9951845906902087,"which significantly constrains the performance of our models. We remain optimistic that future
505"
REFERENCES,0.9967897271268058,"advancements will provide GPUs with sufficient memory to support the functionality of SNNs.
506"
REFERENCES,0.9983948635634029,"5https://www.cs.cornell.edu/people/pabo/movie-review-data/
6https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/
ChnSentiCorp_htl_all.csv
7https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv"
