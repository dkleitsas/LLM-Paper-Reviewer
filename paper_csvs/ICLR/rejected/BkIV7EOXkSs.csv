Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038022813688212928,"Bregman proximal point algorithm (BPPA), as one of the centerpieces in the op-
timization toolbox, has been witnessing emerging applications. With simple and
easy to implement update rule, the algorithm bears several compelling intuitions
for empirical successes, yet rigorous justiﬁcations are still largely unexplored. We
study the computational properties of BPPA through classiﬁcation tasks with sepa-
rable data, and demonstrate provable algorithmic regularization effects associated
with BPPA. We show that BPPA attains non-trivial margin, which closely depends
on the condition number of the distance generating function inducing the Bregman
divergence. We further demonstrate that the dependence on the condition number
is tight for a class of problems, thus showing the importance of divergence in
affecting the quality of the obtained solutions. In addition, we extend our ﬁndings
to mirror descent (MD), for which we establish similar connections between the
margin and Bregman divergence. We demonstrate through a concrete example,
and show BPPA/MD converges in direction to the maximal margin solution with
respect to the Mahalanobis distance. Our theoretical ﬁndings are among the ﬁrst
to demonstrate the benign learning properties BPPA/MD, and also provide strong
corroborations for a careful choice of divergence in the algorithmic design."
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.0076045627376425855,"1
INTRODUCTION
The role of optimization algorithms has become arguably one of the most critical factors in the
empirical successes of training deep models. As the go-to choice for modern machine learning,
ﬁrst-order algorithms, including (stochastic) gradient descent and their adaptive counterparts (Kingma
and Ba, 2014; Duchi et al., 2011), have received tremendous attention, with detailed investigations
dedicated to understanding the effect of batch size (Goyal et al., 2017; Smith et al., 2018; Keskar
et al., 2016), learning rate (Li et al., 2019; He et al., 2019; Lewkowycz et al., 2020), and momentum
(Sutskever et al., 2013; Smith, 2018) across a broad spectrum of applications."
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.011406844106463879,"Meanwhile, Bregman proximal point algorithm (Eckstein, 1993; Kiwiel, 1997) has been drawing
substantial interests. The resounding successes of this classical algorithm are particularly evident
for applications including knowledge distillation (Furlanello et al., 2018), mean-teacher learning
paradigm (Tarvainen and Valpola, 2017), few-shot learning (Zhou et al., 2019), policy optimization
(Green et al., 2019), and ﬁne-tuning pre-trained models (Jiang et al., 2020), yielding competitive
performance compared to its ﬁrst-order counterparts. In the general form, Bregman proximal point
algorithm updates parameters by minimizing a loss L(·), while regularizing the weighted distance to
the previous iterate measured by some divergence function D(·, ·),"
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.015209125475285171,"θt+1 = argminθ L(θ) + 1/(2ηt)D(θ, θt).
(1.1)"
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.019011406844106463,"Popular choices of divergence function used in practice include the squared ℓ2-norm distance
DLS(θ, θt) = ED ∥fθ(x) −fθt(x)∥2
2 (Tarvainen and Valpola, 2017), and Kullback-Leibler based
divergence DKL(θ, θt) = EDKL (fθ′(x)∥fθ(x)) (Furlanello et al., 2018), where D denotes the data
distribution.
Such a simple update is of great practical purposes, as it is easy to describe, and
admits simple implementation by adopting suitable off-the-shelf black-box optimization algorithms
(Solodov and Svaiter, 2000; Monteiro and Svaiter, 2010; Zaslavski, 2010). The updating form
also suggests plausible intuitions for its empirical successes, including iteratively constraining the
search space, alleviating aggressive updates, and preventing catastrophic forgetting (Schulman et al.,
2015; Li and Hoiem, 2017). However, none of the intuitions have been rigorously justiﬁed, and"
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.022813688212927757,"theoretical understandings for the empirical successes of Bregman proximal point algorithm remains
underexplored.
A ﬁrst, and a natural question is whether Bregman proximal point algorithm beneﬁts from the same
kind of mechanism that (stochastic) gradient descent (GD/SGD) enjoys for having the generalization
properties. In particular, in many important applications, GD/SGD is widely believed as the “the
algorithm that ﬁnds the right kind of solutions” for problems with non-unique solutions. Such a claim
is supported with numerous provable examples: GD/SGD converges to the minimum-norm solution
of under-determined linear systems (Gunasekar et al., 2018), converges to the max-margin solution
for separable data (Soudry et al., 2018; Nacson et al., 2019), aligns layers of deep linear networks (Ji
and Telgarsky, 2018), and converges to a generalizable solution for nonlinear networks (Brutzkus
et al., 2017; Allen-Zhu et al., 2018) in the presence of inﬁnitely many overﬁtting solutions. Given the
emerging successes of Bregman proximal point algorithm, and the aforementioned evidences on its
ﬁrst-order counterparts (e.g. GD/SGD) ﬁnding generalizable solutions, one would naturally wonder"
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.026615969581749048,Does Bregman proximal point algorithm converge to a solution with favorable qualities?
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.030418250950570342,"Another important question with great practical implications for Bregman proximal point algorithm
is how the divergence measure D(·, ·) affects the solution. Instead of directly applying the Euclidean
distance based divergence, it is widely observed that the successful application of Bregman proximal
point algorithm is contingent on the careful design of divergence measure, based on the task at
hand (Li and Hoiem, 2017; Hinton et al., 2015). Take the example of ﬁne-tuning language model,
the symmetrized Kullback-Leibler based divergence evaluated on the predictions of the updated
model (i.e., θt+1) and previous model (i.e., θt) yields the state-of-the-art result (Jiang et al., 2020).
Identifying the underlying mechanism for the success or failure of a given divergence choice is not
only of theoretical interest, but also can signiﬁcantly reduce human effort in searching/designing the
suitable divergence for a given task. As an important addition, one may also ask whether the impact of
divergence on the Bregman proximal point algorithm ﬁnd natural counterparts in commonly adopted
ﬁrst-order algorithms (e.g. mirror descent, (Nemirovski and Yudin, 1983)). In such cases, better
task-dependent algorithmic designs could be proposed in conjunction with the suitable divergence.
To this end, we raise our second question."
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.034220532319391636,"How does divergence affect the qualities of the solution obtained by Bregman proximal point
algorithm (and other ﬁrst-order algorithms)?"
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.03802281368821293,"In this paper, we initiate our study to address our previously proposed questions. We focus on
a non-trivial example of an under-determined system – training linear classiﬁers using separable
data. In particular, for exponential tail losses (e.g., exponential loss), the empirical loss function has
inﬁmum zero that is asymptotically attainable at inﬁnity along inﬁnitely many directions. The natural
candidate for measuring the quality of the obtained classiﬁer is its margin, i.e., the minimum distance
between the samples and the decision hyperplane. For such a problem, we summarize our theoretical
ﬁndings below as concrete answers to the previous questions."
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.04182509505703422,"• We show that Bregman proximal point algorithm (BPPA) obtains a solution with non-trivial margin
lower-bound. As a concrete demonstration, we tailor our main theorem for Mahalanobis distance,
and show that BPPA converges in direction to the maximal margin solution. We provide non-
asymptotic analyses of the margin and empirical loss for constant stepsize BPPA, and propose a
more aggressive stepsize rule for a provable exponential speed-up.
• We establish a dependence of such a margin lower-bound on the condition number of the distance
generating function for deﬁning the divergence. In addition, we provide a class of problems where
the margin lower-bound is tight, demonstrating that the Bregman divergence is crucial in affecting
the quality of the obtained solution.
• We extend our ﬁndings to ﬁrst-order algorithms. Speciﬁcally, we show that mirror descent
(MD) enjoys the same previously mentioned margin properties. We also provide non-asymptotic
convergence analyses of the margin and empirical loss for constant stepsize MD, and its exponential
speed-up using a varying stepsize scheme. Our ﬁndings for MD strictly complement prior works
on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019)."
"INTRODUCTION
THE ROLE OF OPTIMIZATION ALGORITHMS HAS BECOME ARGUABLY ONE OF THE MOST CRITICAL FACTORS IN THE",0.045627376425855515,"Notations. We denote [n] := {1, . . . , n}; sgn(z) = 1 if z ≥0 and −1 elsewhere. We use w.r.t in
short for “with respect to”. For any ∥·∥in Euclidean space Rd, we use ∥·∥∗= max∥y∥≤1 ⟨·, y⟩to
denote its dual norm. Note that we have (∥·∥∗)∗= ∥·∥."
PROBLEM SETUP,0.049429657794676805,"2
PROBLEM SETUP"
PROBLEM SETUP,0.053231939163498096,"We study the binary classiﬁcation on linearly separable data. Speciﬁcally, the dataset is S =
{(xi, yi)}n
i=1 ⊂Rd × {+1, −1}, where xi is the feature vector, and yi is the label. In addition, there
exists a linear classiﬁer u ∈Rd such that yi ⟨u, xi⟩> 0 for all i ∈[n]. That is, the decision rule
fu(·) = sgn(⟨u, ·⟩) achieves the perfect accuracy on the dataset, with yi = fu(xi) for all i ∈[n]."
PROBLEM SETUP,0.057034220532319393,"For each linear classiﬁer fu(·) with perfect accuracy, we deﬁne its ∥·∥∗-norm margin as the minimum
distance in ∥·∥∗-norm from the feature vectors to the decision boundary Hu = {x : ⟨x, u⟩= 0}. It is
well known that the ∥·∥∗-norm margin, denoted as γu, only depends on the direction of the classiﬁer"
PROBLEM SETUP,0.060836501901140684,"and satisﬁes γu = mini∈[n]
D
xiyi,
u
∥u∥
E
, where ∥·∥is the dual norm of ∥·∥∗. The ∥·∥∗-norm margin
measures how well the data is separated by decision rule fu(·), measured in ∥·∥∗-norm, and is an
important measure on the generalizability and robustness of the decision rule. Given a norm ∥·∥∗on
Rd, we deﬁne the optimal linear classiﬁer with the maximum ∥·∥∗-margin below.
Deﬁnition 2.1 (Maximum ∥·∥∗-norm Margin Classiﬁer). Given a linearly separable dataset
{(xi, yi)}i∈[n], we deﬁne the maximum ∥·∥∗-norm margin classiﬁer u∥·∥∗, and its associated maxi-
mum ∥·∥∗-norm margin γ∥·∥∗as"
PROBLEM SETUP,0.06463878326996197,"u∥·∥∗= argmax
∥u∥≤1
min
i∈[n] ⟨u, yixi⟩,
γ∥·∥∗= max
∥u∥≤1 min
i∈[n] ⟨u, yixi⟩."
PROBLEM SETUP,0.06844106463878327,"For a separable dataset, we consider ﬁnding the classiﬁer by minimizing the empirical loss"
PROBLEM SETUP,0.07224334600760456,LS(θ) = 1
PROBLEM SETUP,0.07604562737642585,"n
Pn
i=1 ℓ(⟨θ, yixi⟩) .
(2.1)"
PROBLEM SETUP,0.07984790874524715,"Here we focus on the exponential loss ℓ(x) = exp(−x), and our analyses can be readily extended to
other losses with tight exponential tail (e.g., logistic loss)."
PROBLEM SETUP,0.08365019011406843,"Observation. One can readily verify that with a separable dataset S, the empirical loss has inﬁmum 0
but possesses no ﬁnite solution that attains the inﬁmum. Thus any optimization algorithm minimizing
the loss LS(·) will observe the explosion on the norm of iterate."
PROBLEM SETUP,0.08745247148288973,"It has been shown that various optimization algorithms, including (stochastic) gradient descent and
steepest descent, converge in direction to the maximum margin classiﬁer in different norms (Soudry
et al., 2018; Nacson et al., 2019; Gunasekar et al., 2018; Ji and Telgarsky, 2019; 2021). Connections
between gradient descent and the regularization path of homotopy method have also been established
(Ji et al., 2020). A striking feature behind such phenomena is that there is no explicit regularization
in the loss function, and such effects have been termed as the implicit (algorithmic) regularization."
PROBLEM SETUP,0.09125475285171103,"Up to date, most of the implicit regularization effects are attributed to (stochastic) gradient descent,
given their prevalence in applications. However, as Bregman proximal point algorithm (BPPA)
becomes increasingly popular in various domains, there exists considerable lack of understanding on
the computational properties of BPPA. In addition, practitioners often ﬁnd the choice of divergence
function crucially important for the performance of BPPA (Jiang et al., 2020; Furlanello et al.,
2018). This empirical evidence thus calls for a detailed characterization on the connection between
computational properties and the divergence function of BPPA."
PROBLEM SETUP,0.09505703422053231,"In what follows, we study the BPPA for solving problem (2.1) in detail. The BPPA (Algorithm 1) is an
adaptation of the vanilla proximal point algorithm (Rockafellar, 1976a;b) to non-euclidean geometry,
by using Bregman divergence as the divergence measure in (1.1). Speciﬁcally, given a distance
generating function w(·) that is convex and differentiable, we deﬁne the Bregman divergence Dw(·, ·)
associated with w(·) as Dw(θ, θ′) = w(θ) −w(θ′) −⟨∇w(θ′), θ −θ′⟩. Throughout our discussions,
we only impose the following mild assumption on Bregman divergence function Dw(·, ·)."
PROBLEM SETUP,0.09885931558935361,Algorithm 1 Bregman Proximal Point Algorithm (BPPA)
PROBLEM SETUP,0.10266159695817491,"Input: Distance generating function w(·), stepsizes {ηt}t≥0, samples {xi, yi}n
i=1.
Initialize: θ0 ←0.
for t = 0, . . . do"
PROBLEM SETUP,0.10646387832699619,"Update θt+1 = argmin
θ
LS(θ) + 1"
PROBLEM SETUP,0.11026615969581749,"2ηt
Dw(θ, θt).
(2.2)
end for"
PROBLEM SETUP,0.11406844106463879,"Assumption 1. We assume that the distance generating function of Bregman divergence Dw(·, ·) is
Lw-smooth and µw-strongly convex w.r.t. ∥·∥-norm. That is,
µw"
PROBLEM SETUP,0.11787072243346007,"2 ∥θ −θ′∥2 ≤w(θ) −w(θ′) −⟨∇w(θ′), θ −θ′⟩≤Lw"
PROBLEM SETUP,0.12167300380228137,2 ∥θ −θ′∥2 .
ALGORITHMIC REGULARIZATION OF BPPA,0.12547528517110265,"3
ALGORITHMIC REGULARIZATION OF BPPA"
ALGORITHMIC REGULARIZATION OF BPPA,0.12927756653992395,"We show BPPA achieves a ∥·∥∗-norm margin that is at least
p"
ALGORITHMIC REGULARIZATION OF BPPA,0.13307984790874525,"µw/Lw-fraction of the maximal one.
Theorem 3.1 (Constant Stepsize BPPA). Let D∥·∥∗= maxi∈[n] ∥xi∥∗, where ∥·∥∗denotes the dual
norm of ∥·∥. Then under Assumption 1, for any constant stepsize ηt = η > 0, the following hold."
ALGORITHMIC REGULARIZATION OF BPPA,0.13688212927756654,"(1) We have limt→∞LS(θt) = 0. Speciﬁcally, we have that LS(θt) diminishes at the following rate,"
ALGORITHMIC REGULARIZATION OF BPPA,0.14068441064638784,"LS(θt) ≤
1
γ∥·∥∗ηt + Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF BPPA,0.1444866920152091,"4γ2
∥·∥∗ηt
= O"
ALGORITHMIC REGULARIZATION OF BPPA,0.1482889733840304,"Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF BPPA,0.1520912547528517,"γ2
∥·∥∗ηt ! ."
ALGORITHMIC REGULARIZATION OF BPPA,0.155893536121673,(2) We have that the margin is asymptotically lower bounded by
ALGORITHMIC REGULARIZATION OF BPPA,0.1596958174904943,"lim
t→∞min
i∈[n] θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.1634980988593156,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF BPPA,0.16730038022813687,"≥
r µw"
ALGORITHMIC REGULARIZATION OF BPPA,0.17110266159695817,"Lw
γ∥·∥∗,
(3.1)"
ALGORITHMIC REGULARIZATION OF BPPA,0.17490494296577946,"where γ∥·∥∗is deﬁned in Deﬁnition 2.1. In addition, for any given ϵ > 0, there exists a t0 satisfying"
ALGORITHMIC REGULARIZATION OF BPPA,0.17870722433460076,"t0 := e
O "
ALGORITHMIC REGULARIZATION OF BPPA,0.18250950570342206,"max
 D2
∥·∥∗
ϵ2γ2
∥·∥∗
, exp"
ALGORITHMIC REGULARIZATION OF BPPA,0.18631178707224336,"D2
∥·∥∗
γ2
∥·∥∗ϵ2 s Lw
µw"
ALGORITHMIC REGULARIZATION OF BPPA,0.19011406844106463,"!
1
γ2
∥·∥∗η ! ,"
ALGORITHMIC REGULARIZATION OF BPPA,0.19391634980988592,"such that for t ≥t0 number of iterations, we have
 θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.19771863117870722,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF BPPA,0.20152091254752852,"≥(1 −ϵ)
r µw"
ALGORITHMIC REGULARIZATION OF BPPA,0.20532319391634982,"Lw
γ∥·∥∗,
∀i ∈[n]."
ALGORITHMIC REGULARIZATION OF BPPA,0.20912547528517111,"We highlight that (1) The choice of Bregman divergence in BPPA is ﬂexible and can be data
dependent. Properly chosen data-dependent divergence can adapt to data geometry much better than
data-independent divergence, leading to better separation and margin. In Section 5 we demonstrate
how BPPA can beneﬁt signiﬁcantly from such an adaptivity of carefully designed data-dependent
divergence. (2) Our analysis on the convergence requires handling non-ﬁnite minimizers, which
implies divergence of iterate ∥θt∥→∞. The optimization problem of our interest does not meet the
standard assumptions in the classical analysis of BPPA in the literature, and requires a careful choice
of reference point in order to derive non-trivial convergence results. (3) Our result is closely related,
but should not be confused with the homotopy method in (Rosset et al., 2004), which can be viewed
as performing only one proximal step at the origin, with an extremely large stepsize. (4) Finally, our
result is a generalization of Telgarsky (2013); Gunasekar et al. (2018) to non-euclidean settings with
Bregman divergence. Working with Bregman divergence poses unique challenges, as it is previously
unclear how to relate the primal margin progress to the per-iteration progress over the dual space."
ALGORITHMIC REGULARIZATION OF BPPA,0.21292775665399238,"Theorem 3.1 shows that if the distance generating function w(·) is well-conditioned w.r.t. ∥·∥-norm,
then Bregman proximal point algorithm will output a solution with near optimal ∥·∥∗-norm margin.
As a concrete realization of Theorem 3.1, we consider the Mahalanobis distance ∥·∥A :=
p"
ALGORITHMIC REGULARIZATION OF BPPA,0.21673003802281368,"⟨·, A·⟩
induced by a positive deﬁnite matrix A.
Corollary 3.1. Let ∥·∥= ∥·∥A for some positive deﬁnite matrix A. Under the same conditions as
in Theorem 3.1, BPPA with distance generating function w(·) = ⟨·, A·⟩converges to the maximum
∥·∥∗-margin solution, where ∥·∥∗= ∥·∥A−1. Speciﬁcally, we have"
ALGORITHMIC REGULARIZATION OF BPPA,0.22053231939163498,"LS(θt) ≤
1
γ∥·∥∗ηt + Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF BPPA,0.22433460076045628,"4γ2
∥·∥∗ηt
= O"
ALGORITHMIC REGULARIZATION OF BPPA,0.22813688212927757,"Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF BPPA,0.23193916349809887,"γ2
∥·∥∗ηt ! ."
ALGORITHMIC REGULARIZATION OF BPPA,0.23574144486692014,"In addition, we have limt→∞mini∈[n]
D
θt
∥θt∥, yixi
E
= γ∥·∥∗. Speciﬁcally, for any given ϵ > 0,"
ALGORITHMIC REGULARIZATION OF BPPA,0.23954372623574144,"there exists a t0 satisfying t0 := e
O

max

D2
∥·∥∗
ϵ2γ2
∥·∥∗
, exp

D2
∥·∥∗
γ2
∥·∥∗ϵ2"
ALGORITHMIC REGULARIZATION OF BPPA,0.24334600760456274,"
1
γ2
∥·∥∗η"
ALGORITHMIC REGULARIZATION OF BPPA,0.24714828897338403,"
, such that for t ≥t0"
ALGORITHMIC REGULARIZATION OF BPPA,0.2509505703422053,"number of iterations, we have θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.25475285171102663,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF BPPA,0.2585551330798479,"≥(1 −ϵ)γ∥·∥∗,
∀i ∈[n]."
ALGORITHMIC REGULARIZATION OF BPPA,0.2623574144486692,"Finally, we have the direction convergence that limt→∞
θt
∥θt∥= u∥·∥∗."
ALGORITHMIC REGULARIZATION OF BPPA,0.2661596958174905,"Note that similar directional convergence results have been shown in Gunasekar et al. (2018) for
steepest descent w.r.t. ∥·∥A norm. The directional convergence of BPPA obtained here, however, is
not a simple corollary of known results, since existing analyses focus on ﬁrst-order algorithms in
euclidean setting (e.g., GD/SGD, steepest descent). Such existing analyses do not simply extend to
non-ﬁrst-order algorithms in non-euclidean setting, such as BPPA."
ALGORITHMIC REGULARIZATION OF BPPA,0.26996197718631176,"When the distance generating function w(·) is ill-conditioned w.r.t. ∥·∥-norm (i.e.,
p"
ALGORITHMIC REGULARIZATION OF BPPA,0.2737642585551331,"µw/Lw ≪1),
it might be tempting to suggest that the margin lower bound in (3.1) is loose, and what really happens
is limt→∞mini∈[n]
D
θt
∥θt∥, yixi
E
= γ∥·∥∗. However, as we show in the following proposition, there
exists a class of problems, where the lower bound in (3.1) is in fact a tight upper bound (up to a factor
of 2), demonstrating that the dependence on condition number of distance generating function w(·)
w.r.t. ∥·∥-norm is not a proof artifact.
Proposition 3.1 (Tight Dependence on Condition Number). There exists a sequence of problems
{P(m)}m≥1, where each P(m) =

S(m), ∥·∥(m) , w(m)
denotes the dataset, the norm, and the
distance generating function of the m-th problem. For each m, the distance generating function
w(m)(·) is µ(m)
w -strongly convex and L(m)
w -smooth w.r.t. ∥·∥-norm. Then Bregman proximal point
algorithm applied to each problem in {P(m)}m≥1 yields"
ALGORITHMIC REGULARIZATION OF BPPA,0.27756653992395436,"lim
t→∞
min
(x,y)∈S(m) θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.2813688212927757,"∥θt∥, yx
 
γ∥·∥(m)
∗
≤2 s"
ALGORITHMIC REGULARIZATION OF BPPA,0.28517110266159695,"µ(m)
w
L(m)
w
, ∀m ≥1,
(3.2)"
ALGORITHMIC REGULARIZATION OF BPPA,0.2889733840304182,"In addition, for any m ≥4, we have limt→∞min(x,y)∈S(m)
D
θt
∥θt∥, yx
E 
γ∥·∥(m)
∗
≤2
r"
ALGORITHMIC REGULARIZATION OF BPPA,0.29277566539923955,"µ(m)
w
L(m)
w
< 1."
ALGORITHMIC REGULARIZATION OF BPPA,0.2965779467680608,"In fact,"
ALGORITHMIC REGULARIZATION OF BPPA,0.30038022813688214,"lim
t→∞
min
(x,y)∈S(m) θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.3041825095057034,"∥θt∥, yx
 
γ∥·∥(m)
∗
→0,
as m →∞.
(3.3)"
ALGORITHMIC REGULARIZATION OF BPPA,0.30798479087452474,"Combine Theorem 3.1, Corollary 3.1 and Proposition 3.1, we conclude that the margin of the obtained
solution by BPPA has non-trivial dependence on the condition number of the distance generating
function w(·). This observation provides a strong evidence that the Bregman divergence Dw(·, ·)
in BPPA is highly important to the quality of the obtained solution, and advocates a careful design
of Bregman divergence when using the BPPA. Our theoretical ﬁndings also aligns the empirical
evidences on the importance of divergence found in knowledge distillation and model ﬁne-tuning
(Jiang et al., 2020; Furlanello et al., 2018)."
ALGORITHMIC REGULARIZATION OF BPPA,0.311787072243346,"We have shown that BPPA with constant stepsize achieves a margin that is at least
p"
ALGORITHMIC REGULARIZATION OF BPPA,0.3155893536121673,"µw/Lw-fraction
of the maximal one. Meanwhile, our complexity bound in Theorem 3.1 shows that to obtain such a
margin lower bound, it might take an exponential number of iterations. We next show by employing a
more aggressive stepsize scheme, we can attain the same margin lower bound in a polynomial number
of iterations, while speeding up the convergence of the empirical loss {LS(θt)}t≥0 drastically.
Theorem 3.2 (Varying Stepsize BPPA). Given any positive sequence {αt}t≥0, letting the stepsizes
{ηt}t≥0 be ηt =
αt
LS(θt), then the following facts hold."
ALGORITHMIC REGULARIZATION OF BPPA,0.3193916349809886,"(1) limt→∞LS(θt) = 0. Speciﬁcally, for any t ≥0, we have LS(θt+1) ≤LS(θt)β(αt), where"
ALGORITHMIC REGULARIZATION OF BPPA,0.3231939163498099,"β(α) = minβ∈(0,1) max

β, exp

−
2αβ2γ2
∥·∥∗
Lw"
ALGORITHMIC REGULARIZATION OF BPPA,0.3269961977186312," 
< 1."
ALGORITHMIC REGULARIZATION OF BPPA,0.33079847908745247,"(2) Letting αt =
1
√t+1, we have limt→∞mini∈[n]
D
θt
∥θt∥, yixi
E
≥
q"
ALGORITHMIC REGULARIZATION OF BPPA,0.33460076045627374,"µw
Lw γ∥·∥∗. In particular, for any"
ALGORITHMIC REGULARIZATION OF BPPA,0.33840304182509506,"ϵ ∈(0, 1"
ALGORITHMIC REGULARIZATION OF BPPA,0.34220532319391633,"2), there exists a t0 satisfying"
ALGORITHMIC REGULARIZATION OF BPPA,0.34600760456273766,t0 = O  
ALGORITHMIC REGULARIZATION OF BPPA,0.34980988593155893,"Lw
γ∥·∥∗
√µwϵ !8"
ALGORITHMIC REGULARIZATION OF BPPA,0.35361216730038025,",
(3.4)"
ALGORITHMIC REGULARIZATION OF BPPA,0.3574144486692015,"such that in t ≥t0 number of iterations, we have
 θt"
ALGORITHMIC REGULARIZATION OF BPPA,0.3612167300380228,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF BPPA,0.3650190114068441,≥(1 −ϵ) s
ALGORITHMIC REGULARIZATION OF BPPA,0.3688212927756654,"Lw
µw
γ∥·∥∗,
∀i ∈[n]."
ALGORITHMIC REGULARIZATION OF BPPA,0.3726235741444867,"Additionally, the convergence rate of {LS(θt)}t≥0 is given by LS(θt) = O

exp

−
γ2
∥·∥∗
Lw
√"
ALGORITHMIC REGULARIZATION OF BPPA,0.376425855513308,"t

."
ALGORITHMIC REGULARIZATION OF BPPA,0.38022813688212925,"We remark that (1) We do not optimize for the best polynomial dependence on 1/ϵ in the iteration
complexity (3.4), as our main goal is to show the exponential gap between the complexity presented
in Theorem 3.1 and here. We refer interested readers to Appendix B, where we show that we can
improve the polynomial dependence with more tailored analysis. (2) We also demonstrate that the
empirical loss converges almost exponentially faster with our choice of stepsizes. (3) We reiterate
that using the aggressive stepsizes does not change our established margin lower bound, and the
exact convergence to the maximum margin solution demonstrated in Corollary 3.1 still holds for this
scheme of stepsizes, which can be achieved with a polynomial number of iterations."
ALGORITHMIC REGULARIZATION OF BPPA,0.3840304182509506,"• Inexact Implementation of BPPA. The proximal update (2.2) requires solving a non-trivial
optimization problem, and there has been fruitful results of inexact implementation of BPPA in
optimization literature (Rockafellar, 1976b; Yang and Toh, 2021; Solodov and Svaiter, 2000; Monteiro
and Svaiter, 2010). Here based on the varying stepsize scheme proposed in Theorem 3.2, we discuss
the feasibility of a gradient descent based inexact BPPA that: (1) admits a simple implementation
and achieves polynomial complexity, (2) retains the margin properties of exact BPPA. Speciﬁcally, at
the t-th iteration, the gradient descent based inexact BPPA solves the proximal step
bθt+1 ≈argminθ φt(θ) := 1"
ALGORITHMIC REGULARIZATION OF BPPA,0.38783269961977185,"n
Pn
i=1 exp (−⟨θ, yixi⟩) +
1
2ηt Dw(θ, bθt)
(3.5)"
ALGORITHMIC REGULARIZATION OF BPPA,0.3916349809885932,"up to a pre-speciﬁed accuracy δt with gradient descent. Our key observation comes from the fact
that when applying gradient descent to φt(·) with small enough stepsizes, the iterate would stay in a
region that has relative smoothness Mt and relative strong convexity µt bounded by"
ALGORITHMIC REGULARIZATION OF BPPA,0.39543726235741444,Mt ≤LS(bθt) + 1
ALGORITHMIC REGULARIZATION OF BPPA,0.39923954372623577,"ηt = LS(bθt)

1 +
1
αt"
ALGORITHMIC REGULARIZATION OF BPPA,0.40304182509505704,"
,
µt ≥
1
ηt = L(bθt) αt ,"
ALGORITHMIC REGULARIZATION OF BPPA,0.4068441064638783,"both measured w.r.t. Bregman divergence Dh(·, ·) (Lu et al., 2018). Note that the ﬁrst inequality
follows by our choice of stepsize ηt in Theorem 3.2. Thus the effective condition number κt := Mt/µt
of φt(·) is bounded by κt = 1 + αt = O(1), which implies that the t-th proximal step requires"
ALGORITHMIC REGULARIZATION OF BPPA,0.41064638783269963,"O

κt log( 1"
ALGORITHMIC REGULARIZATION OF BPPA,0.4144486692015209,"δt )

= O

log( 1"
ALGORITHMIC REGULARIZATION OF BPPA,0.41825095057034223,"δt )

number of gradient descent steps. Summing up across t0 iterations"
ALGORITHMIC REGULARIZATION OF BPPA,0.4220532319391635,"(3.4), we need up to O
Pt0
t=1 log( 1"
ALGORITHMIC REGULARIZATION OF BPPA,0.42585551330798477,"δt )

gradient descent steps, which depends polynomially on t0
even if we choose an extremely high accuracy δt = O(exp(−t)) for each inexact proximal step (3.5)."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4296577946768061,"4
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.43346007604562736,"Inspired by the results in the previous section, we further show that mirror descent (MD, Algorithm 2),
as a generalization of gradient descent to non-euclidean geometry, possesses similar connection be-
tween the margin and Bregman divergence. We remark that our results are the ﬁrst to characterize the
algorithmic regularization effect of MD for classiﬁcation tasks, while previous literature exclusively
focus on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019)."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4372623574144487,Algorithm 2 Mirror Descent Algorithm (MD)
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.44106463878326996,"Input: Distance generating function w(·), stepsizes {ηt}t≥0, samples {xi, yi}n
i=1.
Initialize: θ0 ←0.
for t = 0, . . . do"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4448669201520912,Compute gradient ∇LS(θt) = 1
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.44866920152091255,"n
Pn
i=1 exp (−⟨θt, yixi⟩) (−yixi).
Update θt+1 = argminθ ⟨∇LS(θt), θ −θt⟩+
1
2ηt Dw(θ, θt).
end for"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4524714828897338,"Theorem 4.1 (Constant Stepsize MD). Let D∥·∥∗= maxi∈[n] ∥xi∥∗, where ∥·∥∗denotes the dual
norm of ∥·∥, and D∥·∥2 = maxi∈[n] ∥xi∥2. Under Assumption 1, let µ2 be the strong convexity
parameter of w(·) w.r.t. ∥·∥2-norm. Then for any constant stepsize ηt = η ≤
µ2
2D∥·∥2 , we have that"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.45627376425855515,"(1) limt→∞LS(θt) = 0. Speciﬁcally, we have that LS(θt) diminishes at the following rate,"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4600760456273764,"LS(θt) ≤
1
γ∥·∥∗ηt + Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.46387832699619774,"4γ2
∥·∥∗ηt
= O"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.467680608365019,"Lw log2  
γ∥·∥∗ηt
"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4714828897338403,"γ2
∥·∥∗ηt ! ."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4752851711026616,(2) We have that the margin is asymptotically lower bounded by
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4790874524714829,"lim
t→∞min
i∈[n] θt"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4828897338403042,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.4866920152091255,"≥
r µw"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.49049429657794674,"Lw
γ∥·∥∗."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.49429657794676807,"In addition, for any ϵ > 0, there exists a t0 satisfying"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.49809885931558934,t0 = O  exp 
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5019011406844106,"D3/2
∥·∥∗D∥·∥2Lwη"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5057034220532319,"γ2
∥·∥∗µ1/2
w µ3/2
2
ϵ3/2 log
1 ϵ   "
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5095057034220533,",
(4.1)"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5133079847908745,"such that any t ≥t0, we have  θt"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5171102661596958,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5209125475285171,"≥(1 −ϵ)
r µw"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5247148288973384,"Lw
γ∥·∥∗,"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5285171102661597,"Theorem 4.1 shows that mirror descent attains the same ∥·∥∗-norm margin lower bound as BPPA,
which is
p"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.532319391634981,"µw/Lw-fraction of the maximal margin. Note that µ2 > 0 is a direct consequence of
Assumption 1 and the equivalence of norm on ﬁnite-dimensional vector space."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5361216730038023,"Similar to Corollary 3.1, let ∥·∥= ∥·∥A be the Mahalanobis distance, then MD equipped with
distance generating function w(·) = ⟨·, A·⟩converges to the maximum ∥·∥∗-norm margin classiﬁer.
Corollary 4.1. Let ∥·∥= ∥·∥A for some positive deﬁnite matrix A. Then under the same conditions as
in Theorem 4.1, the MD with distance generating function w(·) = ⟨·, A·⟩converges to the maximum"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5399239543726235,"∥·∥∗-margin solution, where ∥·∥∗= ∥·∥A−1. Speciﬁcally, we have LS(θt) = O

Lw log2(γ∥·∥∗ηt)"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5437262357414449,"γ2
∥·∥∗ηt 
."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5475285171102662,"In addition, we have limt→∞mini∈[n]
D
θt
∥θt∥, yixi
E
= γ∥·∥∗. Speciﬁcally, for any given ϵ > 0,"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5513307984790875,"there exists a t0 with t0 = O

exp

D3/2
∥·∥∗D∥·∥2Lwη"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5551330798479087,"γ2
∥·∥∗µ1/2
w
µ3/2
2
ϵ3/2 log
  1"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.55893536121673,"ϵ

, such that for t ≥t0 number of"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5627376425855514,"iterations, we have
 θt"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5665399239543726,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5703422053231939,"≥(1 −ϵ)γ∥·∥∗,
∀i ∈[n]."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5741444866920152,"Finally, we have direction convergence that limt→∞
θt
∥θt∥= u∥·∥∗."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5779467680608364,"Note that Corollary 4.1 recovers the directional convergence of steepest descent in Gunasekar et al.
(2018) w.r.t ∥·∥A, which coincides with MD with distance generating function w(·) = ⟨·, A·⟩."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5817490494296578,"Theorem 4.1 guarantees the near optimal ∥·∥∗-norm margin when the distance generating function
w(·) is well-conditioned w.r.t. ∥·∥-norm. For cases when w(·) is ill-conditioned, we demonstrate that
there exists a class of problem for which the margin lower bound is tight.
Proposition 4.1. There exists a sequence of problems {P(m)}m≥1 by the same construction as in
Proposition 3.1, such that the margin lower bound in Theorem 4.1 is tight up to a non-trivial factor of
2. Speciﬁcally, we have (3.2) and (3.3) also hold for MD."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5855513307984791,"Finally, we propose a more aggressive stepsize scheme for MD that achieves the same margin lower
bound. In addition, instead of requiring an exponential number of iterations (4.1) as constant stepsize
MD, such a stepsize scheme only needs a polynomial number of iterations, and achieves an almost
exponential speedup for the empirical loss {LS(θt)}t≥0.
Theorem 4.2 (Varying Stepsize MD). Let the stepsizes {ηt}t≥0 be given by ηt =
αt
LS(θt), where
αt = min{
µ2
2D∥·∥2 ,
1
√t+1}. Then under the same conditions as in Theorem 4.1,"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5893536121673004,"(1) We have limt→∞mini∈[n]
D
θt
∥θt∥, yixi
E
≥
q"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.5931558935361216,"µw
Lw γ∥·∥∗. In addition, for any ϵ > 0, there exists a"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.596958174904943,"t0 satisfying t0 = O

D∥·∥2Lw
γ∥·∥∗µ2√µwϵ
4
, such that for any t ≥t0, we have
 θt"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6007604562737643,"∥θt∥, yixi"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6045627376425855,"≥(1 −ϵ)
r µw"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6083650190114068,"Lw
γ∥·∥∗,
∀i ∈[n]."
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6121673003802282,"(2) We have limt→∞LS(θt) = 0. In addition, the convergence rate is given by"
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6159695817490495,LS(θt) = O  exp 
ALGORITHMIC REGULARIZATION OF MIRROR DESCENT,0.6197718631178707,"−
γ2
∥·∥∗
Lw √ t !! ."
EXPERIMENTS,0.623574144486692,"5
EXPERIMENTS"
EXPERIMENTS,0.6273764258555133,"Synthetic Data. We take S = {((−0.5, 1), +1) , ((−0.5, −1), −1) , ((−0.75, −1), −1) , ((2, 1), +1)}.
One can readily verify that the maximum ∥·∥2-norm margin classiﬁer is u∥·∥2 = (0, 1). For both
BPPA and MD, we take the Bregman divergence as Dw(x, y) = ∥x −y∥2
2, which corresponds to
the vanilla proximal point algorithm and gradient descent algorithm. Note that both algorithms are
guaranteed to converge in direction towards u∥·∥2 = (0, 1), following Corollary 3.1 and 4.1. L(✓t)"
EXPERIMENTS,0.6311787072243346,"h✓t/k✓tk, uk·k2i"
EXPERIMENTS,0.6349809885931559,(a) BPPA L(✓t)
EXPERIMENTS,0.6387832699619772,"h✓t/k✓tk, uk·k2i"
EXPERIMENTS,0.6425855513307985,"(b) MD
Figure 1: BPPA and MD run on the simple data set S."
EXPERIMENTS,0.6463878326996197,"We take ηt = η = 1 for the constant stepsize BPPA/MD, and ηt =
1
L(θt)√t+1 for the varying
stepsize BPPA/MD, following the stepsize choices in Theorem 3.1, 3.2, 4.1 and 4.2. To implement
the proximal step in BPPA at the t-th iteration, we take 128 number of gradient descent steps with
stepsize 0.2ηt, following our discussion at the end of Section 3. We initialize all algorithms at the
origin and run 1200 iterations. From Figure 1, we can clearly observe that both BPPA and MD
converge in direction to the maximum ∥·∥2-norm margin classiﬁer u∥·∥2, which is consistent with our
theoretical ﬁndings. In addition, by adopting the varying stepsize scheme proposed in Theorem 3.2
and 4.2, both BPPA and MD converge exponentially faster than their constant stepsize counterparts."
EXPERIMENTS,0.6501901140684411,"Data-dependent Bregman Divergence. We illustrate through an example on how properly chosen
data-dependent divergence can lead to much improved separation compared to data-independent
divergence, even on simple linear models."
EXPERIMENTS,0.6539923954372624,"We have n labeled data {(xi, yi)}m
i=1 sampled from a mixture of sphere distribution: yi ∼
Bernoulli(1/2), xi ∼Unif (Syiµ(r)), where Sz(r) denotes the sphere centered at z with radius
r in Rd. In addition, we also have m unlabeled data {exj}m
j=1, following the same distribution as
{xi}n
i=1, with no labels given. Clearly, the maximum ∥·∥2-margin classiﬁer for the mixture of sphere
distribution considered here is given by the linear classiﬁer f ∗(·) = sign(⟨·, µ⟩)."
EXPERIMENTS,0.6577946768060836,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.6615969581749049,"sgn(
, x )"
EXPERIMENTS,0.6653992395437263,"Unlabeled
Positive
Negative"
EXPERIMENTS,0.6692015209125475,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.6730038022813688,"sgn(
, x )"
EXPERIMENTS,0.6768060836501901,"Unlabeled
Positive
Negative"
EXPERIMENTS,0.6806083650190115,"2
1
0
1
2 2 1 0 1 2"
EXPERIMENTS,0.6844106463878327,"sgn(
, x )"
EXPERIMENTS,0.688212927756654,"Unlabeled
Positive
Negative"
EXPERIMENTS,0.6920152091254753,"Figure 2: BPPA with Bregman divergence D(3) (right) signiﬁcantly
improves alignment with optimal classiﬁer µ, compared to D(1) (left)
and D(2) (middle)."
EXPERIMENTS,0.6958174904942965,"Divergence
Alignment
D(1)(·, ·)
0.8703
D(2)(·, ·)
0.8175
D(3)(·, ·)
0.9754"
EXPERIMENTS,0.6996197718631179,"Table 1:
D
θT
∥θT ∥2 , µ
E
av-
eraged over 8 runs."
EXPERIMENTS,0.7034220532319392,"We choose n = d = 2, m = 100, r = 0.8, and generate µ ∼Unif (S0(1)). We compare three types
of Bregman divergence, given by D(1)(θ, θ′) = ∥θ −θ′∥2
2 (vanilla proximal point), D(2)(θ, θ′) =
(θ −θ′)⊤bΣ(θ −θ′), and D(3)(θ, θ′) = (θ −θ′)⊤bΣ−1(θ −θ′), where bΣ =
1
m
Pm
j=1 xjx⊤
j denotes
the empirical covariance matrix. Note that D(2) and D(3) are data-dependent from their construction.
For each divergence function, we run BPPA with 8 independent runs, the results are reported in
Figure 2 and Table 1. We make two important remarks on the empirical results:"
EXPERIMENTS,0.7072243346007605,"• Data-dependent divergence D(3) gives the best separation despite limited labeled data (in fact only
2!), much improved over data-independent squared ℓ2-distance D(1).
• Not all data-dependent divergence helps, D(2) shows degradation compared to D(1)."
EXPERIMENTS,0.7110266159695817,"We further remark that by utilizing Corollary 3.1, one can completely characterize the solution
obtained by BPPA for each of the divergence in closed form. Using such a characterization allows
one to corroborate the empirical phenomenon with our developed theories, deferred in Appendix A."
EXPERIMENTS,0.714828897338403,"CIFAR-100. We demonstrate the potential of extending our theoretical ﬁndings for linear models
to practical networks, using ResNet-18 (He et al., 2016), ShufﬂeNetV2 (Ma et al., 2018), Mo-
bileNetV2 (Sandler et al., 2018), with CIFAR-100 dataset (Krizhevsky et al., 2009). At each
iteration of BPPA, the updated model parameter θt+1 is given by solving the proximal step
θt+1 = argminθ 1/n Pn
i=1 ℓ(fθ(xi); yi) + 1/(2ηt)D(θ; θt) for all t ≥0, where D denotes di-
vergence function, and θ0 is obtained by standard training with SGD. We consider inexact imple-
mentation of the proximal step, discussed in (3.5). Speciﬁcally, each proximal step is solved by
using SGD, with a batch size of 128, an initial learning rate of 0.1 which is subsequently divided
by 5 at the 60th, 120th, and 160th epoch. We consider two divergence functions widely used in
practice, deﬁned by DLS(θ′, θ) = 1/(2n) Pn
i=1 ∥fθ(xi) −fθ′(xi)∥2
2 (Tarvainen and Valpola, 2017),
and DKL(θ, θ′) = 1/(2n) Pn
i=1 KL (fθ′(xi)∥fθ(xi)) (Furlanello et al., 2018). For each of the diver-
gence, we run BPPA with 3 proximal steps, with the proximal stepsize ηt = η = 0.025 for DKL, and
ηt = η = 0.2 for DLS (ηt = 0.025 gives signiﬁcantly worse performance). For standard training
with SGD, we use a batch size of 128, an initial learning rate of 0.1 further divided by 5 at the 60th,
120th, and 160th epoch. The results are reported in Figure 3."
EXPERIMENTS,0.7186311787072244,"50
100
150
200
Epoch 50 55 60 65 70 75"
EXPERIMENTS,0.7224334600760456,Test Accuracy
EXPERIMENTS,0.7262357414448669,ResNet18
EXPERIMENTS,0.7300380228136882,"SGD
KL-Prox-3
LS-Prox-3"
EXPERIMENTS,0.7338403041825095,"50
100
150
200
Epoch 30 40 50 60 70"
EXPERIMENTS,0.7376425855513308,Test Accuracy
EXPERIMENTS,0.7414448669201521,MobileNetV2
EXPERIMENTS,0.7452471482889734,"SGD
KL-Prox-3
LS-Prox-3"
EXPERIMENTS,0.7490494296577946,"50
100
150
200
Epoch 50 55 60 65 70"
EXPERIMENTS,0.752851711026616,Test Accuracy
EXPERIMENTS,0.7566539923954373,ShuffleNetV2
EXPERIMENTS,0.7604562737642585,"SGD
KL-Prox-3
LS-Prox-3"
EXPERIMENTS,0.7642585551330798,"75
100
125
150
175
200
Epoch 65.0 67.5 70.0 72.5 75.0 77.5"
EXPERIMENTS,0.7680608365019012,Test Accuracy
EXPERIMENTS,0.7718631178707225,ResNet18
EXPERIMENTS,0.7756653992395437,"SGD
KL-Prox-1
KL-Prox-2
KL-Prox-3
LS-Prox-1
LS-Prox-2
LS-Prox-3"
EXPERIMENTS,0.779467680608365,"Figure 3: BPPA with divergences DKL and DLS on CIFAR-100 dataset. KL-Prox-k denotes learning
curve of the k-th proximal step with DKL; LS-Prox-k denotes learning curve of the k-th proximal
step with DLS.
One can clearly see from Figure 3: (1) Across different model architectures, BPPA with DKL
outperforms standard training with SGD; (2) BPPA with DLS yields negligible differences compared
to SGD. The qualitative difference of DKL and DLS strongly indicates that the divergence function
serves an important role in affecting the model performance learned by BPPA, which we view as an
important evidence showing broader applicability of our developed divergence-dependent margin
theories. In addition, the learned model with DKL improves gradually w.r.t the total number of
proximal steps. For ResNet-18, the accuracy increases from 75.83% (standard training) to 78.56%
after 3 proximal steps – an additional 1.4% improvement over Tf-KDself (see Table 2), which can be
viewed as BPPA with 1 proximal step. We view such ﬁndings as the evidence suggesting the scope of
algorithmic regularization associated with BPPA goes beyond simple linear models."
EXPERIMENTS,0.7832699619771863,"Model
SGD
Tf-KDself
MobileNetV2
68.38
70.96 (+2.58)
ShufﬂeNetV2
70.34
72.23 (+1.89)
ResNet18
75.87
77.10 (+1.23)
GoogLeNet
78.72
80.17 (+1.45)
DenseNet121
79.04
80.26 (+1.22)"
EXPERIMENTS,0.7870722433460076,"Table 2: Comparison of Tf-KDself (2-step
BPPA) and SGD on CIFAR-100."
EXPERIMENTS,0.7908745247148289,"We make further remarks on the previously proposed
method in Yuan et al. (2019), named Teacher-free
Knowledge Distillation via self-training (Tf-KDself) ,
which is equivalent to BPPA with 1 proximal steps, us-
ing DKL(θ, θ′) as the divergence function. Tf-KDself
was shown to improve over SGD for various network
architectures on CIFAR-100 and Tiny-ImageNet. We
include the reported results on CIFAR-100 therein in
Table 2 for completeness."
CONCLUSION AND FUTURE DIRECTION,0.7946768060836502,"6
CONCLUSION AND FUTURE DIRECTION"
CONCLUSION AND FUTURE DIRECTION,0.7984790874524715,"To conclude, we have shown that for binary classiﬁcation task with linearly separable data, the
Bregman proximal point algorithm and mirror descent attain a ∥·∥∗-norm margin that is closely
related to the condition number of the distance generating function w.r.t. ∥·∥-norm. We list two
directions worthy of future investigations. (1) Our analyses exploit the fact that the Bregman
divergence is deﬁned over the model parameters, while many popular data-dependent divergences
are deﬁned over the model output (e.g. prediction conﬁdence). Making this non-trivial extension to
data-dependent divergence can also help demystify the mechanism of the data-dependent divergence.
(2) Our current analyses focus on linear models, and the extension to nonlinear neural networks
requires more delicate deﬁnitions of margin and divergence. We leave this direction as our long-term
investigation plan."
REFERENCES,0.8022813688212928,REFERENCES
REFERENCES,0.8060836501901141,"ALLEN-ZHU, Z., LI, Y. and LIANG, Y. (2018). Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918 ."
REFERENCES,0.8098859315589354,"AZIZAN, N. and HASSIBI, B. (2019). Stochastic gradient/mirror descent: Minimax optimality and
implicit regularization. In International Conference on Learning Representations."
REFERENCES,0.8136882129277566,"BRUTZKUS, A., GLOBERSON, A., MALACH, E. and SHALEV-SHWARTZ, S. (2017). Sgd learns
over-parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174 ."
REFERENCES,0.8174904942965779,"DEVLIN, J., CHANG, M.-W., LEE, K. and TOUTANOVA, K. (2018). Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ."
REFERENCES,0.8212927756653993,"DUCHI, J., HAZAN, E. and SINGER, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. Journal of machine learning research 12."
REFERENCES,0.8250950570342205,"ECKSTEIN, J. (1993). Nonlinear proximal point algorithms using bregman functions, with applica-
tions to convex programming. Mathematics of Operations Research 18 202–226."
REFERENCES,0.8288973384030418,"FRENCH, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive
sciences 3 128–135."
REFERENCES,0.8326996197718631,"FURLANELLO, T., LIPTON, Z., TSCHANNEN, M., ITTI, L. and ANANDKUMAR, A. (2018). Born
again neural networks. In International Conference on Machine Learning. PMLR."
REFERENCES,0.8365019011406845,"GOYAL, P., DOLLÁR, P., GIRSHICK, R., NOORDHUIS, P., WESOLOWSKI, L., KYROLA, A.,
TULLOCH, A., JIA, Y. and HE, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 ."
REFERENCES,0.8403041825095057,"GREEN, S., VINEYARD, C. M. and KOÇ, C. K. (2019). Distillation strategies for proximal policy
optimization. arXiv preprint arXiv:1901.08128 ."
REFERENCES,0.844106463878327,"GUNASEKAR, S., LEE, J., SOUDRY, D. and SREBRO, N. (2018). Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning. PMLR."
REFERENCES,0.8479087452471483,"HE, K., ZHANG, X., REN, S. and SUN, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition."
REFERENCES,0.8517110266159695,"HE, T., ZHANG, Z., ZHANG, H., ZHANG, Z., XIE, J. and LI, M. (2019). Bag of tricks for image
classiﬁcation with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition."
REFERENCES,0.8555133079847909,"HINTON, G., VINYALS, O. and DEAN, J. (2015). Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 ."
REFERENCES,0.8593155893536122,"JI, Z., DUDÍK, M., SCHAPIRE, R. E. and TELGARSKY, M. (2020). Gradient descent follows the
regularization path for general losses. In Proceedings of Thirty Third Conference on Learning
Theory (J. Abernethy and S. Agarwal, eds.), vol. 125 of Proceedings of Machine Learning Research.
PMLR."
REFERENCES,0.8631178707224335,"JI, Z. and TELGARSKY, M. (2018). Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032 ."
REFERENCES,0.8669201520912547,"JI, Z. and TELGARSKY, M. (2019). The implicit bias of gradient descent on nonseparable data. In
Proceedings of the Thirty-Second Conference on Learning Theory (A. Beygelzimer and D. Hsu,
eds.), vol. 99 of Proceedings of Machine Learning Research. PMLR, Phoenix, USA."
REFERENCES,0.870722433460076,"JI, Z. and TELGARSKY, M. (2021). Characterizing the implicit bias via a primal-dual analysis. In
Algorithmic Learning Theory. PMLR."
REFERENCES,0.8745247148288974,"JIANG, H., HE, P., CHEN, W., LIU, X., GAO, J. and ZHAO, T. (2020). SMART: Robust and efﬁcient
ﬁne-tuning for pre-trained natural language models through principled regularized optimization.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online."
REFERENCES,0.8783269961977186,"KAKADE, S., SHALEV-SHWARTZ, S., TEWARI, A. ET AL. (2009). On the duality of strong convexity
and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript,
http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf 2."
REFERENCES,0.8821292775665399,"KESKAR, N. S., MUDIGERE, D., NOCEDAL, J., SMELYANSKIY, M. and TANG, P. T. P. (2016).
On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
arXiv:1609.04836 ."
REFERENCES,0.8859315589353612,"KINGMA, D. P. and BA, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 ."
REFERENCES,0.8897338403041825,"KIRKPATRICK, J., PASCANU, R., RABINOWITZ, N., VENESS, J., DESJARDINS, G., RUSU, A. A.,
MILAN, K., QUAN, J., RAMALHO, T., GRABSKA-BARWINSKA, A. ET AL. (2017). Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114
3521–3526."
REFERENCES,0.8935361216730038,"KIWIEL, K. C. (1997). Proximal minimization methods with generalized bregman functions. SIAM
journal on control and optimization 35 1142–1168."
REFERENCES,0.8973384030418251,"KRIZHEVSKY, A., HINTON, G. ET AL. (2009). Learning multiple layers of features from tiny images
."
REFERENCES,0.9011406844106464,"LEWKOWYCZ, A., BAHRI, Y., DYER, E., SOHL-DICKSTEIN, J. and GUR-ARI, G. (2020). The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 ."
REFERENCES,0.9049429657794676,"LI, Y., WEI, C. and MA, T. (2019). Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595 ."
REFERENCES,0.908745247148289,"LI, Z. and HOIEM, D. (2017). Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence 40 2935–2947."
REFERENCES,0.9125475285171103,"LU, H., FREUND, R. M. and NESTEROV, Y. (2018). Relatively smooth convex optimization by
ﬁrst-order methods, and applications. SIAM Journal on Optimization 28 333–354."
REFERENCES,0.9163498098859315,"MA, N., ZHANG, X., ZHENG, H.-T. and SUN, J. (2018). Shufﬂenet v2: Practical guidelines for
efﬁcient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV)."
REFERENCES,0.9201520912547528,"MCCLOSKEY, M. and COHEN, N. J. (1989). Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation, vol. 24. Elsevier,
109–165."
REFERENCES,0.9239543726235742,"MONTEIRO, R. D. and SVAITER, B. F. (2010). Convergence rate of inexact proximal point methods
with relative error criteria for convex optimization. submitted to SIAM Journal on Optimization ."
REFERENCES,0.9277566539923955,"NACSON, M. S., LEE, J., GUNASEKAR, S., SAVARESE, P. H. P., SREBRO, N. and SOUDRY, D.
(2019). Convergence of gradient descent on separable data. In Proceedings of the Twenty-Second
International Conference on Artiﬁcial Intelligence and Statistics (K. Chaudhuri and M. Sugiyama,
eds.), vol. 89 of Proceedings of Machine Learning Research. PMLR."
REFERENCES,0.9315589353612167,"NEMIROVSKI, A. S. and YUDIN, D. B. (1983). Problem complexity and method efﬁciency in
optimization ."
REFERENCES,0.935361216730038,"ROCKAFELLAR, R. T. (1976a). Augmented lagrangians and applications of the proximal point
algorithm in convex programming. Mathematics of operations research 1 97–116."
REFERENCES,0.9391634980988594,"ROCKAFELLAR, R. T. (1976b). Monotone operators and the proximal point algorithm. SIAM journal
on control and optimization 14 877–898."
REFERENCES,0.9429657794676806,"ROSSET, S., ZHU, J. and HASTIE, T. (2004). Boosting as a regularized path to a maximum margin
classiﬁer. The Journal of Machine Learning Research 5 941–973."
REFERENCES,0.9467680608365019,"SANDLER, M., HOWARD, A., ZHU, M., ZHMOGINOV, A. and CHEN, L.-C. (2018). Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer
vision and pattern recognition."
REFERENCES,0.9505703422053232,"SCHULMAN, J., LEVINE, S., ABBEEL, P., JORDAN, M. and MORITZ, P. (2015). Trust region policy
optimization. In International conference on machine learning. PMLR."
REFERENCES,0.9543726235741445,"SMITH, L. N. (2018). A disciplined approach to neural network hyper-parameters: Part 1–learning
rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820 ."
REFERENCES,0.9581749049429658,"SMITH, S. L., KINDERMANS, P.-J. and LE, Q. V. (2018). Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations."
REFERENCES,0.9619771863117871,"SOLODOV, M. V. and SVAITER, B. F. (2000). Error bounds for proximal point subproblems and
associated inexact proximal point algorithms. Mathematical programming 88 371–389."
REFERENCES,0.9657794676806084,"SOUDRY, D., HOFFER, E. and SREBRO, N. (2018). The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations."
REFERENCES,0.9695817490494296,"SUTSKEVER, I., MARTENS, J., DAHL, G. and HINTON, G. (2013). On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference
on Machine Learning (S. Dasgupta and D. McAllester, eds.), vol. 28 of Proceedings of Machine
Learning Research. PMLR, Atlanta, Georgia, USA."
REFERENCES,0.973384030418251,"TARVAINEN, A. and VALPOLA, H. (2017).
Mean teachers are better role models: Weight-
averaged consistency targets improve semi-supervised deep learning results. arXiv preprint
arXiv:1703.01780 ."
REFERENCES,0.9771863117870723,"TELGARSKY, M. (2013). Margins, shrinkage, and boosting. In International Conference on Machine
Learning. PMLR."
REFERENCES,0.9809885931558935,"YANG, L. and TOH, K.-C. (2021). Bregman proximal point algorithm revisited: a new inexact
version and its variant. arXiv preprint arXiv:2105.10370 ."
REFERENCES,0.9847908745247148,"YUAN, L., TAY, F. E., LI, G., WANG, T. and FENG, J. (2019). Revisit knowledge distillation: a
teacher-free framework ."
REFERENCES,0.9885931558935361,"ZASLAVSKI, A. J. (2010). Convergence of a proximal point method in the presence of computational
errors in hilbert spaces. SIAM Journal on Optimization 20 2413–2421."
REFERENCES,0.9923954372623575,"ZHANG, T., WU, F., KATIYAR, A., WEINBERGER, K. Q. and ARTZI, Y. (2021). Revisiting
few-sample {bert} ﬁne-tuning. In International Conference on Learning Representations."
REFERENCES,0.9961977186311787,"ZHOU, P., YUAN, X., XU, H., YAN, S. and FENG, J. (2019). Efﬁcient meta learning via mini-
batch proximal update. In Advances in Neural Information Processing Systems (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox and R. Garnett, eds.), vol. 32. Curran
Associates, Inc."
