Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018115942028985507,"Real-time recurrent learning (RTRL) for sequence-processing recurrent neural net-
1"
ABSTRACT,0.0036231884057971015,"works (RNNs) offers certain conceptual advantages over backpropagation through
2"
ABSTRACT,0.005434782608695652,"time (BPTT). RTRL requires neither caching past activations nor truncating con-
3"
ABSTRACT,0.007246376811594203,"text, and enables online learning. However, RTRL’s time and space complexity
4"
ABSTRACT,0.009057971014492754,"makes it impractical. To overcome this problem, most recent work on RTRL fo-
5"
ABSTRACT,0.010869565217391304,"cuses on approximation theories, while experiments are often limited to diagnostic
6"
ABSTRACT,0.012681159420289856,"settings. Here we explore the practical promise of RTRL in more realistic settings.
7"
ABSTRACT,0.014492753623188406,"We study actor-critic methods that combine RTRL and policy gradients, and test
8"
ABSTRACT,0.016304347826086956,"them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On
9"
ABSTRACT,0.018115942028985508,"DMLab memory tasks, our system is competitive with or outperforms well-known
10"
ABSTRACT,0.019927536231884056,"IMPALA and R2D2 baselines trained on 10 B frames, while using fewer than 1.2 B
11"
ABSTRACT,0.021739130434782608,"environmental frames. To scale to such challenging tasks, we focus on certain well-
12"
ABSTRACT,0.02355072463768116,"known neural architectures with element-wise recurrence, allowing for tractable
13"
ABSTRACT,0.025362318840579712,"RTRL without approximation. We also discuss rarely addressed limitations of
14"
ABSTRACT,0.02717391304347826,"RTRL in real-world applications, such as its complexity in the multi-layer case.1
15"
INTRODUCTION,0.028985507246376812,"1
Introduction
16"
INTRODUCTION,0.030797101449275364,"There are two classic learning algorithms to compute exact gradients for sequence-processing recur-
17"
INTRODUCTION,0.03260869565217391,"rent neural networks (RNNs): real-time recurrent learning (RTRL; [1, 2, 3, 4]) and backpropagation
18"
INTRODUCTION,0.034420289855072464,"through time (BPTT; [5, 6]) (reviewed in Sec. 2). In practice, BPTT is the only one commonly used
19"
INTRODUCTION,0.036231884057971016,"today, simply because BPTT is tractable while RTRL is not. In fact, the time and space complexities
20"
INTRODUCTION,0.03804347826086957,"of RTRL for a fully recurrent NN are quadratic and cubic in the number of hidden units, respectively,
21"
INTRODUCTION,0.03985507246376811,"which are prohibitive for any RNNs of practical sizes in real applications. Despite such an obvious
22"
INTRODUCTION,0.041666666666666664,"complexity bottleneck, RTRL has certain attractive conceptual advantages over BPTT. BPTT requires
23"
INTRODUCTION,0.043478260869565216,"to cache activations for each new element of the sequence processed by the model, for later gradient
24"
INTRODUCTION,0.04528985507246377,"computation. As the amount of these past activations to be stored grows linearly with the sequence
25"
INTRODUCTION,0.04710144927536232,"length, practitioners (constrained by the actual memory limit of their hardware) use the so-called trun-
26"
INTRODUCTION,0.04891304347826087,"cated BPTT (TBPTT; [7]) where they specify the maximum number of time steps for this storage,
27"
INTRODUCTION,0.050724637681159424,"giving up gradient components—and therefore credit assignments—that go beyond this time span. In
28"
INTRODUCTION,0.05253623188405797,"contrast, RTRL does not require storing past activations, and enables computation of untruncated
29"
INTRODUCTION,0.05434782608695652,"gradients for sequences of any arbitrary length. In addition, RTRL is an online learning algorithm
30"
INTRODUCTION,0.05615942028985507,"(more efficient than BPTT to process long sequences in the online scenario) that allows for updating
31"
INTRODUCTION,0.057971014492753624,"weights immediately after consuming every new input (assuming that the external error feedback to
32"
INTRODUCTION,0.059782608695652176,"the model output is also available for each input). These attractive advantages of RTRL still actively
33"
INTRODUCTION,0.06159420289855073,"motivate researchers to work towards practical RTRL (e.g., [8, 9, 10, 11, 12]).
34"
INTRODUCTION,0.06340579710144928,"1Upon acceptance, we will add a GitHub link to our public code here."
INTRODUCTION,0.06521739130434782,"The root of RTRL’s high complexities is the computation and storage of the so-called sensitivity
35"
INTRODUCTION,0.06702898550724638,"matrix whose entries are derivatives of the hidden activations w.r.t. each trainable parameter of the
36"
INTRODUCTION,0.06884057971014493,"model involved in the recurrence (see Sec. 2). Most recent research on RTRL focuses on introducing
37"
INTRODUCTION,0.07065217391304347,"approximation methods into the computation and storage of this matrix. For example, Menick
38"
INTRODUCTION,0.07246376811594203,"et al. [11] introduce sparsity in both the weights of the RNN and updates of the temporal Jacobian
39"
INTRODUCTION,0.07427536231884058,"(which is an intermediate matrix needed to compute the sensitivity matrix). Another line of work
40"
INTRODUCTION,0.07608695652173914,"[8, 9, 10] proposes estimators based on low-rank decompositions of the sensitivity matrix that are less
41"
INTRODUCTION,0.07789855072463768,"expensive to compute and store than the original one. Silver et al. [12] explore random projections
42"
INTRODUCTION,0.07971014492753623,"of the sensitivity. The main research question in these lines of work is naturally focused around the
43"
INTRODUCTION,0.08152173913043478,"quality of the proposed approximation method. Consequently, the central goal of their experiments is
44"
INTRODUCTION,0.08333333333333333,"typically to test hyper-parameters and configurational choices that control the approximation quality
45"
INTRODUCTION,0.08514492753623189,"in diagnostic settings, rather than evaluating the full potential of RTRL in realistic tasks. In the end,
46"
INTRODUCTION,0.08695652173913043,"we still know very little about the true empirical promise of RTRL. Also, assuming that a solution is
47"
INTRODUCTION,0.08876811594202899,"found to the complexity bottleneck, what actual applications or algorithms would RTRL unlock? In
48"
INTRODUCTION,0.09057971014492754,"what scenarios would RTRL be able to replace BPTT in today’s deep learning?
49"
INTRODUCTION,0.09239130434782608,"Here we propose to study RTRL by looking ahead beyond research on approximations. We explore
50"
INTRODUCTION,0.09420289855072464,"the full potential of RTRL in the settings where no approximation is needed, while at the same time,
51"
INTRODUCTION,0.09601449275362318,"not restricting ourselves to toy tasks. For that, we focus on special RNN architectures with element-
52"
INTRODUCTION,0.09782608695652174,"wise recurrence, that allow for tractable RTRL without any approximation. In fact, the quadratic/cubic
53"
INTRODUCTION,0.09963768115942029,"complexities of the fully recurrent NNs can be simplified for certain neural architectures. Many well-
54"
INTRODUCTION,0.10144927536231885,"known RNN architectures, such as Quasi-RNNs [13] and Simple Recurrent Units [14], and even
55"
INTRODUCTION,0.10326086956521739,"certain Linear Transformers [15, 16, 17], belong to this class of models (see Sec. 3.1). Note that the
56"
INTRODUCTION,0.10507246376811594,"core idea underlying this observation is technically not new: Mozer [18, 19] already explore an RNN
57"
INTRODUCTION,0.1068840579710145,"architecture with this property in the late 1980s to derive his focused backpropagation, and Javed
58"
INTRODUCTION,0.10869565217391304,"et al. [20, 21] also exploit this in the architectural design of their RNNs (even though the problematic
59"
INTRODUCTION,0.1105072463768116,"multi-layer case is ignored; we discuss it in Sec. 5). While such special RNNs may suffer from
60"
INTRODUCTION,0.11231884057971014,"limited computational capabilities on certain tasks (i.e., one can come up with a synthetic/algorithmic
61"
INTRODUCTION,0.11413043478260869,"task where such models fail; see Appendix B.1), they also often perform on par with fully recurrent
62"
INTRODUCTION,0.11594202898550725,"NNs on many tasks (at least, this is the case for the tasks we explore in our experiments). For the
63"
INTRODUCTION,0.11775362318840579,"purpose of this work, the RTRL-tractability property outweighs the potentially limited computational
64"
INTRODUCTION,0.11956521739130435,"capabilities: these architectures allow us to focus on evaluating RTRL on challenging tasks with a
65"
INTRODUCTION,0.1213768115942029,"scale that goes beyond the one typically used in prior RTRL work, and to draw conclusions without
66"
INTRODUCTION,0.12318840579710146,"worrying about the quality of approximation. We study an actor-critic algorithm [22, 23, 24] that
67"
INTRODUCTION,0.125,"combines RTRL and recurrent policy gradients [25], allowing credit assignments throughout an
68"
INTRODUCTION,0.12681159420289856,"entire episode in reinforcement learning (RL) with partially observable Markov decision processes
69"
INTRODUCTION,0.1286231884057971,"(POMDPs; [26, 27]). We test the resulting algorithm, Real-Time Recurrent Actor-Critic method
70"
INTRODUCTION,0.13043478260869565,"(R2AC), in several subsets of DMLab-30 [28], ProcGen [29], and Atari 2600 [30] environments, with
71"
INTRODUCTION,0.1322463768115942,"a focus on memory tasks but also including reactive ones. In particular, on two memory environments
72"
INTRODUCTION,0.13405797101449277,"of DMLab-30, our system is competitive with or outperforms the well-known IMPALA [31] and
73"
INTRODUCTION,0.1358695652173913,"R2D2 [32] baselines, demonstrating certain practical benefits of RTRL at scale. Finally, working
74"
INTRODUCTION,0.13768115942028986,"with concrete real-world tasks also sheds lights on further limitations of RTRL that are rarely (if not
75"
INTRODUCTION,0.13949275362318841,"never) discussed in prior work. These observations are important for future research on practical
76"
INTRODUCTION,0.14130434782608695,"RTRL. We highlight and discuss these general challenges of RTRL (Sec. 5).
77"
BACKGROUND,0.1431159420289855,"2
Background
78"
BACKGROUND,0.14492753623188406,"Here we first review real-time recurrent learning (RTRL; [1, 2, 3, 4]), which is a gradient-based
79"
BACKGROUND,0.14673913043478262,"learning algorithm for sequence-processing RNNs—an alternative to the now standard BPTT.
80"
BACKGROUND,0.14855072463768115,"Preliminaries.
Let t, T, N, and D be positive integers. We describe the corresponding learning
81"
BACKGROUND,0.1503623188405797,"algorithm for the following standard RNN architecture [33] that transforms an input x(t) ∈RD to an
82"
BACKGROUND,0.15217391304347827,"output h(t) ∈RN at every time step t as
83"
BACKGROUND,0.1539855072463768,"s(t) = W x(t) + Rh(t −1)
;
h(t) = σ(s(t))
(1)"
BACKGROUND,0.15579710144927536,"where W ∈RN×D and R ∈RN×N are trainable parameters, s(t) ∈RN, and σ denotes the
84"
BACKGROUND,0.15760869565217392,"element-wise sigmoid function (we omit biases). For the derivation, it is convenient to describe each
85"
BACKGROUND,0.15942028985507245,"component sk(t) ∈R of vector s(t) for k ∈{1, ..., N},
86"
BACKGROUND,0.161231884057971,"sk(t) = D
X"
BACKGROUND,0.16304347826086957,"n=1
Wk,nxn(t) + N
X"
BACKGROUND,0.16485507246376813,"n=1
Rk,nσ(sn(t −1))
(2)"
BACKGROUND,0.16666666666666666,"In addition, we consider some loss function Ltotal(1, T) = PT
t=1 L(t) ∈R computed on an arbitrary
87"
BACKGROUND,0.16847826086956522,"sequence of length T where L(t) ∈R is the loss at each time step t, which is a function of h(t)
88"
BACKGROUND,0.17028985507246377,"(we omit writing down explicit dependencies over the model parameters). Importantly, we assume
89"
BACKGROUND,0.1721014492753623,"that L(t) can be computed solely from h(t) at step t (i.e., L(t) has no dependency on any other past
90"
BACKGROUND,0.17391304347826086,"activations apart from h(t −1) which is needed to compute h(t)).
91"
BACKGROUND,0.17572463768115942,"The role of a gradient-based learning algorithm is to efficiently compute the gradients of the loss
92"
BACKGROUND,0.17753623188405798,"w.r.t. the trainable parameters of the model, i.e., ∂Ltotal(1, T)"
BACKGROUND,0.1793478260869565,"∂Wi,j
∈R for all i ∈{1, ..., N} and
93"
BACKGROUND,0.18115942028985507,"j ∈{1, ..., D}, and ∂Ltotal(1, T)"
BACKGROUND,0.18297101449275363,"∂Ri,j
∈R for all i, j ∈{1, ..., N}. RTRL and BPTT differ in the way
94"
BACKGROUND,0.18478260869565216,"to compute these quantities. While we focus on RTRL here, for the sake of completeness, we also
95"
BACKGROUND,0.18659420289855072,"provide an analogous derivation for BPTT in Appendix A.3.
96"
BACKGROUND,0.18840579710144928,"Real-Time Recurrent Learning (RTRL).
RTRL can be derived by first decomposing the total
97"
BACKGROUND,0.19021739130434784,"loss Ltotal(1, T) over time, and then summing all derivatives of each loss component L(t) w.r.t. inter-
98"
BACKGROUND,0.19202898550724637,"mediate variables sk(t) for all k ∈{1, ..., N}:
99"
BACKGROUND,0.19384057971014493,"∂Ltotal(1, T)"
BACKGROUND,0.1956521739130435,"∂Wi,j
= T
X t=1"
BACKGROUND,0.19746376811594202,"∂L(t)
∂Wi,j
= T
X t=1 N
X k=1"
BACKGROUND,0.19927536231884058,"∂L(t)
∂sk(t) × ∂sk(t) ∂Wi,j ! (3)"
BACKGROUND,0.20108695652173914,"In fact, unlike BPTT that can only compute the derivative of the total loss Ltotal(1, T) efficiently,
100"
BACKGROUND,0.2028985507246377,RTRL is an online algorithm that computes each term ∂L(t)
BACKGROUND,0.20471014492753623,"∂Wi,j
through the decomposition above.
101"
BACKGROUND,0.20652173913043478,The first factor ∂L(t)
BACKGROUND,0.20833333333333334,"∂sk(t) can be straightforwardly computed through standard backpropagation (as
102"
BACKGROUND,0.21014492753623187,"stated above, we assume there is no recurrent computation between s(t) and L(t)). For the second
103"
BACKGROUND,0.21195652173913043,factor ∂sk(t)
BACKGROUND,0.213768115942029,"∂Wi,j
, which is an element of the so-called sensitivity matrix/tensor, we can derive a forward
104"
BACKGROUND,0.21557971014492755,"recursion formula, which can be obtained by directly differentiating Eq. 2:
105"
BACKGROUND,0.21739130434782608,∂sk(t)
BACKGROUND,0.21920289855072464,"∂Wi,j
= xj(t)1k=i + N
X"
BACKGROUND,0.2210144927536232,"n=1
Rk,nσ′(sn(t −1))∂sn(t −1)"
BACKGROUND,0.22282608695652173,"∂Wi,j
(4)"
BACKGROUND,0.2246376811594203,"where 1k=i denotes the indicator function: 1k=i = 1 if k = i, and 0 otherwise, and σ′ denotes the
106"
BACKGROUND,0.22644927536231885,"derivative of the sigmoid, i.e, σ′(sn(t −1)) = σ(sn(t −1))(1 −σ(sn(t −1))). The derivation
107"
BACKGROUND,0.22826086956521738,is similar for ∂L(t)
BACKGROUND,0.23007246376811594,"∂Ri,j
where we obtain a recurrent formula to compute ∂sk(t)"
BACKGROUND,0.2318840579710145,"∂Ri,j
. As this algorithm
108"
BACKGROUND,0.23369565217391305,requires to store ∂sk(t)
BACKGROUND,0.23550724637681159,"∂Wi,j
and ∂sk(t)"
BACKGROUND,0.23731884057971014,"∂Ri,j
, its space complexity is O((D + N)N 2) ∼O(N 3). The time
109"
BACKGROUND,0.2391304347826087,"complexity to update the sensitivity matrix/tensor via Eq. 4 is O(N 4). To be fair with BPTT, it should
110"
BACKGROUND,0.24094202898550723,"be noted that O(N 4) is the complexity for one update; this means that the time complexity to process
111"
BACKGROUND,0.2427536231884058,"a sequence of length T is O(TN 4).
112"
BACKGROUND,0.24456521739130435,"Thanks to the forward recursion, the update frequency of RTRL is flexible: one can opt for the
113"
BACKGROUND,0.2463768115942029,"fully online learning, where we update the weights using ∂L(t)"
BACKGROUND,0.24818840579710144,"∂W
at every time step, or accumulate
114"
BACKGROUND,0.25,"gradients for several time steps. It should be noted that frequent updates may result in staleness of
115"
BACKGROUND,0.25181159420289856,"the sensitivity matrix, as it accumulates updates computed using old weights (Eq. 4).
116"
BACKGROUND,0.2536231884057971,"Note that algorithms similar to RTRL have been derived from several independent authors (see, e.g.,
117"
BACKGROUND,0.2554347826086957,"[3, 18], or [34, 35] for the continuous-time version).
118"
METHOD,0.2572463768115942,"3
Method
119"
METHOD,0.25905797101449274,"Our main algorithm is an actor-critic method that combines RTRL with recurrent policy gradients,
120"
METHOD,0.2608695652173913,"using a special RNN architecture that allows for tractable RTRL. Here we describe its main com-
121"
METHOD,0.26268115942028986,"ponents: an element-wise LSTM with tractable RTRL (Sec. 3.1), and the actor-critic algorithm that
122"
METHOD,0.2644927536231884,"builds upon IMPALA [31] (Sec. 3.2).
123"
METHOD,0.266304347826087,"3.1
RTRL for LSTM with Element-wise Recurrence (eLSTM)
124"
METHOD,0.26811594202898553,"The core RNN architecture we use in this work is a variant of long short-term memory (LSTM; [36])
125"
METHOD,0.26992753623188404,"RNN with element-wise recurrence. Let ⊙denote element-wise multiplication. At each time step t,
126"
METHOD,0.2717391304347826,"it first transforms an input vector x(t) ∈RD to a recurrent hidden state c(t) ∈RN as follows:
127"
METHOD,0.27355072463768115,"f(t) = σ(F x(t) + wf ⊙c(t −1))
;
z(t) = tanh(Zx(t) + wz ⊙c(t −1))
(5)
c(t) = f(t) ⊙c(t −1) + (1 −f(t)) ⊙z(t)
(6)"
METHOD,0.2753623188405797,"where f(t) ∈RN, z(t) ∈RN are activations, F ∈RN×D and Z ∈RN×D are trainable weight
128"
METHOD,0.27717391304347827,"matrices, and wf ∈RN and wz ∈RN are trainable weight vectors. These operations are followed
129"
METHOD,0.27898550724637683,"by a gated feedforward NN to obtain an output h(t) ∈RN as follows:
130"
METHOD,0.2807971014492754,"o(t) = σ(Ox(t) + W oc(t));
h(t) = o(t) ⊙c(t)
(7)"
METHOD,0.2826086956521739,"where O ∈RN×D and W o ∈RN×N are trainable weight matrices. This architecture can be seen as
131"
METHOD,0.28442028985507245,"an extension of Quasi-RNN [13] with element-wise recurrence in the gates, or Simple Recurrent Units
132"
METHOD,0.286231884057971,"[14] without depth gating, and also relates to IndRNN [37]. While one could further discuss myriads
133"
METHOD,0.28804347826086957,"of architectural details [38], most of them are irrelevant to our discussion on the complexity reduction
134"
METHOD,0.2898550724637681,"in RTRL; the only essential property here is that “recurrence” is element-wise. We use this simple
135"
METHOD,0.2916666666666667,"architecture above, an LSTM with element-wise recurrence (or eLSTM), for all our experiments.
136"
METHOD,0.29347826086956524,"Furthermore, we restrict ourselves to the one-layer case (we discuss the multi-layer case later in Sec. 5),
137"
METHOD,0.29528985507246375,"where we assume that there is no recurrence after this layer. Based on this assumption, gradients for
138"
METHOD,0.2971014492753623,"the parameters O and W o in Eq. 7 can be computed by the standard backpropagation, as they are
139"
METHOD,0.29891304347826086,"not involved in recurrence. Hence, the sensitivity matrices we need for RTRL (Sec. 2) are: ∂c(t)"
METHOD,0.3007246376811594,"∂F ,
140 ∂c(t)"
METHOD,0.302536231884058,"∂Z
∈RN×N×N, and ∂c(t)"
METHOD,0.30434782608695654,"∂wf , ∂c(t)"
METHOD,0.3061594202898551,"∂wz ∈RN×N. Through trivial derivations, we can show that each
141"
METHOD,0.3079710144927536,"of these sensitivity matrices can be computed using a tractable forward recursion formula (we provide
142"
METHOD,0.30978260869565216,the full derivation in Appendix A.1). For example for ∂c(t)
METHOD,0.3115942028985507,"∂F , we have, for i, j, k ∈{1, ..., N},
143"
METHOD,0.3134057971014493,"ˆfi(t) = (ci(t −1) −zi(t))fi(t)(1 −fi(t))
(8)
∂ci(t)"
METHOD,0.31521739130434784,"∂Fi,j
= (fi(t) + wf
i ˆfi(t))∂ci(t −1)"
METHOD,0.3170289855072464,"∂Fi,j
+ ˆfi(t)xj(t) ; and
∂ck(t)"
METHOD,0.3188405797101449,"∂Fi,j
= 0
for all k ̸= i.
(9)"
METHOD,0.32065217391304346,"where we introduce an intermediate vector ˆf(t) ∈RN with components ˆfi(t) ∈R for convenience.
144"
METHOD,0.322463768115942,"Consequently, the gradients for the weights can be computed as:
145 ∂L(t)"
METHOD,0.3242753623188406,"∂Fi,j
= N
X k=1"
METHOD,0.32608695652173914,"∂L(t)
∂ck(t) × ∂ck(t)"
METHOD,0.3278985507246377,"∂Fi,j
= ∂L(t)"
METHOD,0.32971014492753625,∂ci(t) × ∂ci(t)
METHOD,0.33152173913043476,"∂Fi,j
(10)"
METHOD,0.3333333333333333,"Finally, we can compactly summarise these equations using the standard matrix operations. By
146"
METHOD,0.3351449275362319,"introducing notations ˆF (t) ∈RN×N with ˆFi,j(t) = ∂ci(t)"
METHOD,0.33695652173913043,"∂Fi,j
∈R, and e(t) ∈RN with ei(t) =
147"
METHOD,0.338768115942029,"∂L(t)
∂ci(t) ∈R for i ∈{1, ..., N} and j ∈{1, ..., D}, Eqs. 8-10 above can be written as:
148"
METHOD,0.34057971014492755,"ˆf(t) = (c(t −1) −z(t)) ⊙f(t) ⊙(1 −f(t))
(11)"
METHOD,0.3423913043478261,"ˆF (t) = diag

f(t) + wf ⊙ˆf(t)

ˆF (t −1) + ˆf(t) ⊗x(t)
;
∂L(t)"
METHOD,0.3442028985507246,"∂F
= diag(e(t)) ˆF (t) (12)"
METHOD,0.34601449275362317,"where, for notational convenience, we introduce a function diag : RN →RN×N that constructs
149"
METHOD,0.34782608695652173,"a diagonal matrix whose diagonal elements are those of the input vector; however, in practical
150"
METHOD,0.3496376811594203,"implementations (e.g., in PyTorch), this can be directly handled as vector-matrix multiplications with
151"
METHOD,0.35144927536231885,"broadcasting (this is an important note for complexity analysis). ⊗denotes outer-product.
152"
METHOD,0.3532608695652174,"Analogously, we can derive compact update equations of sensitivity matrices and gradient computa-
153"
METHOD,0.35507246376811596,"tions for other parameters Z, wf and wz (as well as biases which are omitted here). The complete
154"
METHOD,0.35688405797101447,"list of these equations is provided in Appendix A.1.
155"
METHOD,0.358695652173913,"The RTRL algorithm above requires maintaining sensitivity matrices ˆF (t) ∈RN×N, and analogously
156"
METHOD,0.3605072463768116,"defined ˆZ(t) ∈RN×N, ˆwf(t) ∈RN, and ˆwz(t) ∈RN (see Appendix A.1); thus, the space
157"
METHOD,0.36231884057971014,"complexity is O(N 2). The per-step time complexity is O(N 2) (see Eqs. 8-10). This is all tractable.
158"
METHOD,0.3641304347826087,"Importantly, these equations 11-12 can be implemented as simple PyTorch code (just like the forward
159"
METHOD,0.36594202898550726,"pass of the same model; Eqs. 5-7) without any non-standard logics. Note that many approximations of
160"
METHOD,0.3677536231884058,"RTRL often involve computations that are not well supported yet in the standard deep learning library
161"
METHOD,0.3695652173913043,"(e.g., efficiently handling custom sparsity), which is an extra barrier for scaling RTRL in practice.
162"
METHOD,0.3713768115942029,"Note that the derivation of RTRL for element-wise recurrent nets is not novel: similar methods can be
163"
METHOD,0.37318840579710144,"found in Mozer [18, 19] from the late 1980s. This result itself is also not very surprising, since element-
164"
METHOD,0.375,"wise recurrence introduces obvious sparsity in the temporal Jacobian (which is part of the second
165"
METHOD,0.37681159420289856,"term in Eq. 4). Nethertheless, we are not aware of any prior work pointing out that several modern
166"
METHOD,0.3786231884057971,"RNN architectures such Quasi-RNN [13] or Simple Recurrent Units [14] yield tractable RTRL (in the
167"
METHOD,0.3804347826086957,"one-layer case). Also, while this is not the focus of our experiments, we show an example of Linear
168"
METHOD,0.3822463768115942,"Transformers/Fast Weight Programmers [15, 16, 17] that have tractable RTRL (details can be found in
169"
METHOD,0.38405797101449274,"Appendix A.2), which is another conceptually interesting result. We also note that the famous LSTM-
170"
METHOD,0.3858695652173913,"algorithm [36] (companion learning algorithm for the LSTM architecture) is a diagonal approximation
171"
METHOD,0.38768115942028986,"of RTRL, so is the more recent SnAp-1 of Menick et al. [11]. Unlike in these works, the gradients
172"
METHOD,0.3894927536231884,"computed by our RTRL algorithm above are exact for our eLSTM architecture. This allows us
173"
METHOD,0.391304347826087,"to draw conclusions from experimental results without worrying about the potential influence of
174"
METHOD,0.39311594202898553,"approximation quality. We can evaluate the full potential of RTRL for this specific architecture.
175"
METHOD,0.39492753623188404,"Finally, this is also an interesting system from the biological standpoint. Each weight in the weight
176"
METHOD,0.3967391304347826,"matrix/synaptic connections (e.g., F ∈RN×N) is augmented with the corresponding ""memory""
177"
METHOD,0.39855072463768115,"( ˆF (t) ∈RN×N) tied to its own learning process, which is updated in an online fashion, as the model
178"
METHOD,0.4003623188405797,"observes more and more examples, through an Hebbian/outer product-based update rule (Eq. 12/Left).
179"
METHOD,0.40217391304347827,"3.2
Real-Time Recurrent Actor-Critic Policy Gradient Algorithm (R2AC)
180"
METHOD,0.40398550724637683,"The main algorithm we study in this work, Real-Time Recurrent Actor-Critic method (R2AC), com-
181"
METHOD,0.4057971014492754,"bines RTRL with recurrent policy gradients. Our algorithm builds upon IMPALA [31]. Essentially,
182"
METHOD,0.4076086956521739,"we replace the RNN archicture and its learning algorithm, LSTM/TBPTT in the standard recurrent
183"
METHOD,0.40942028985507245,"IMPALA algorithm, by our eLSTM/RTRL (Sec. 3.1). While we refer to the original paper [31] for
184"
METHOD,0.411231884057971,"basic details of IMPALA, here we recapitulate some crucial aspects. Let M denote a positive integer.
185"
METHOD,0.41304347826086957,"IMPALA is a distributed actor-critic algorithm where each actor interacts with the environment for a
186"
METHOD,0.4148550724637681,"fixed number of steps M to obtain a state-action-reward trajectory segment of length M to be used by
187"
METHOD,0.4166666666666667,"the learner to update the model parameters. M is an important hyper-parameter that is used to specify
188"
METHOD,0.41847826086956524,"the number of steps M for M-step TD learning [39] of the critic, and the frequency of weight updates.
189"
METHOD,0.42028985507246375,"Given the same number of environmental steps used for training, systems trained with a smaller M
190"
METHOD,0.4221014492753623,"apply more weight updates than those trained with a higher M. For recurrent policies trained with
191"
METHOD,0.42391304347826086,"TBPTT, M also represents the BPTT span (i.e., BPTT is carried out on the M-length trajectory seg-
192"
METHOD,0.4257246376811594,"ment; no gradient is propagated farther than M steps back in time; while the last state of the previous
193"
METHOD,0.427536231884058,"segment is used as the initial state of the new segment in the forward pass). In the case of RTRL, there
194"
METHOD,0.42934782608695654,"is no gradient truncation, but since M controls the update frequency, the greater the M, the less fre-
195"
METHOD,0.4311594202898551,"quently we update the parameters, and it potentially suffers less from sensitivity matrix staling. This
196"
METHOD,0.4329710144927536,"setting allows for comparing TBPTT and RTRL in the setting where everything is equal (including the
197"
METHOD,0.43478260869565216,"number of updates) except the actual gradients applied to the weights: truncated vs. untruncated ones.
198"
METHOD,0.4365942028985507,"Note that for R2AC with M = 1, one could obtain a fully online recurrent actor-critic method.
199"
METHOD,0.4384057971014493,"However, in practice, it is known that M > 1 is crucial (for TD learning of the critic) for optimal
200"
METHOD,0.44021739130434784,"performance. In all our experiments, we have M > 1. The main focus of this work is to evaluate
201"
METHOD,0.4420289855072464,"learning with untruncated gradients, rather than the potential for online learning.
202"
EXPERIMENTS,0.4438405797101449,"4
Experiments
203"
DIAGNOSTIC TASK,0.44565217391304346,"4.1
Diagnostic Task
204"
DIAGNOSTIC TASK,0.447463768115942,"Since the main focus of this work is to evaluate RTRL-based algorithms beyond diagnostic tasks, we
205"
DIAGNOSTIC TASK,0.4492753623188406,"only conduct brief experiments on a classic diagnostic task used in recent RTRL research work focused
206"
DIAGNOSTIC TASK,0.45108695652173914,"on approximation methods [8, 9, 10, 11, 12]: the copy task. Since our RTRL algorithm (Sec. 3.1)
207"
DIAGNOSTIC TASK,0.4528985507246377,"requires no approximation, and the task is trivial, we achieve 100% accuracy provided that the RNN
208"
DIAGNOSTIC TASK,0.45471014492753625,"size is large enough and that training hyper-parameters are properly chosen. We confirm this for
209"
DIAGNOSTIC TASK,0.45652173913043476,"sequences with lengths of up to 1000. Additional experimental details can be found in Appendix B.1.
210"
MEMORY TASKS,0.4583333333333333,"4.2
Memory Tasks
211"
MEMORY TASKS,0.4601449275362319,"Here we present the main experiments of this work: RL in POMDPs using realistic game environments
212"
MEMORY TASKS,0.46195652173913043,"requiring memory.
213"
MEMORY TASKS,0.463768115942029,"DMLab Memory Tasks.
DMLab-30 [28] is a collection of 30 first-person 3D game environ-
214"
MEMORY TASKS,0.46557971014492755,"ments, with a mix of both memory and reactive tasks. Here we focus on two well-known environ-
215"
MEMORY TASKS,0.4673913043478261,"ments, rooms_select_nonmatching_object and rooms_watermaze, which are both categorised
216"
MEMORY TASKS,0.4692028985507246,"as “memory” tasks according to Parisotto et al. [40]. The mean episode lengths of these tasks are
217"
MEMORY TASKS,0.47101449275362317,"about 100 and 1000 steps, respectively. As we apply an action repetition of 4, each “step” corre-
218"
MEMORY TASKS,0.47282608695652173,"sponds to 4 environmental frames here. We refer to Appendix B.2 for further descriptions of these
219"
MEMORY TASKS,0.4746376811594203,"tasks, and experimental details. Our model architecture is based on that of IMPALA [31]. Both RTRL
220"
MEMORY TASKS,0.47644927536231885,"and TBPTT systems use our eLSTM (Sec. 3.1) as the recurrent layer with a hidden state size of 512.
221"
MEMORY TASKS,0.4782608695652174,"Everything is equal between these two systems except that the gradients are truncated in TBPTT
222"
MEMORY TASKS,0.48007246376811596,"but not in RTRL. To reduce the overall compute needed for the experiments, we first pre-train one
223"
MEMORY TASKS,0.48188405797101447,"TBPTT model for 50 M steps for rooms_select_nonmatching_object, and for 200 M steps for
224"
MEMORY TASKS,0.483695652173913,"rooms_watermaze. Then, for all main training runs in this experiment, we initialise the parameters
225"
MEMORY TASKS,0.4855072463768116,"of the convolutional vision module from the same pre-trained model, and keep these parameters frozen
226"
MEMORY TASKS,0.48731884057971014,"(and thus, only train the recurrent layer and everything above it). For these main training runs, we
227"
MEMORY TASKS,0.4891304347826087,"train for 30 M and 100 M steps for rooms_select_nonmatching_object and rooms_watermaze,
228"
MEMORY TASKS,0.49094202898550726,"respectively; resulting in the total of 320 M and 1.2 B environmental frames. We compare RTRL
229"
MEMORY TASKS,0.4927536231884058,"and TBPTT for different values of M ∈{10, 50, 100} (Sec. 3.2). We recall that M influences: the
230"
MEMORY TASKS,0.4945652173913043,"frequency of weight updates, M-step TD learning, as well as the backpropagation span for TBPTT.
231"
MEMORY TASKS,0.4963768115942029,"Table 1 shows the corresponding scores, and the left part of Figure 1 shows the training curves. We
232"
MEMORY TASKS,0.49818840579710144,"observe that for select_nonmatching_object which has a short mean episode length of 100 steps,
233"
MEMORY TASKS,0.5,"the performance of TBPTT and RTRL is similar even with M = 50. The benefit of RTRL is only
234"
MEMORY TASKS,0.5018115942028986,"visible in the case with M = 10. In contrast, for the more challenging rooms_watermaze task with
235"
MEMORY TASKS,0.5036231884057971,"a mean episode length of 1000 steps, RTRL outperforms TBPTT for all values of M ∈{10, 50, 100}.
236"
MEMORY TASKS,0.5054347826086957,"Furthermore, with M = 50 or 100, our RTRL system outperforms the IMPALA and R2D2 systems
237"
MEMORY TASKS,0.5072463768115942,"from prior work [32], while trained on fewer than 1.2 B frames. Note that R2D2 systems [32] are
238"
MEMORY TASKS,0.5090579710144928,"trained without action repetitions, and with a BPTT span of 80. This effectively demonstrates the
239"
MEMORY TASKS,0.5108695652173914,"practical benefit of RTRL in a realistic task requiring long-span credit assignments.
240"
MEMORY TASKS,0.5126811594202898,"ProcGen.
We test R2AC in another domain: ProcGen [29]. Most ProcGen environments are solv-
241"
MEMORY TASKS,0.5144927536231884,"able using a feedforward policy even without frame-stacking [29]. There is a so-called memory-mode
242"
MEMORY TASKS,0.5163043478260869,"for certain games, making the task partially observable by making the world bigger, and restricting
243"
MEMORY TASKS,0.5181159420289855,"agents’ observations to a limited area around them. However, in our preliminary experiments, we ob-
244"
MEMORY TASKS,0.519927536231884,"serve that even in these POMDP settings, both the feedforward and LSTM baselines perform similarly
245"
MEMORY TASKS,0.5217391304347826,"(see Appendix B.3). Nevertheless, we find one environment in the standard hard-mode, Chaser, which
246"
MEMORY TASKS,0.5235507246376812,"shows clear benefits of recurrent policies over those without memory. Chaser is similar to the classic
247"
MEMORY TASKS,0.5253623188405797,"game “Pacman,” effectively requiring some counting capabilities to fully exploit power pellets valid
248"
MEMORY TASKS,0.5271739130434783,"for a limited time span. The mean episode length for this task is about 200 steps, where each step is
249"
MEMORY TASKS,0.5289855072463768,"an environmental frame as we apply no action repeat for ProcGen. Unlike in the DMLab experiments
250"
MEMORY TASKS,0.5307971014492754,"above, here we train all models from scratch for 200 M steps without pre-training the vision module
251"
MEMORY TASKS,0.532608695652174,"(since training the vision parameters using RTRL is intractable, they are trained with truncated gradi-
252"
MEMORY TASKS,0.5344202898550725,"ents, i.e., only the recurrent layer is trained using RTRL; we discuss this further in Sec. 5). We com-
253"
MEMORY TASKS,0.5362318840579711,"pare RTRL and TBPTT with M = 5 or 50. The training curves are shown in the right part of Figure 1.
254"
MEMORY TASKS,0.5380434782608695,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Steps
1e7 10 20 30 40 50 60"
MEMORY TASKS,0.5398550724637681,Return
MEMORY TASKS,0.5416666666666666,"TBPTT
RTRL"
MEMORY TASKS,0.5434782608695652,"(a) Non-matching, M = 10"
MEMORY TASKS,0.5452898550724637,"0.0
0.2
0.4
0.6
0.8
1.0
Steps
1e8 10 20 30 40 50 60"
MEMORY TASKS,0.5471014492753623,Return
MEMORY TASKS,0.5489130434782609,"TBPTT
RTRL"
MEMORY TASKS,0.5507246376811594,"(b) Watermaze, M = 10"
MEMORY TASKS,0.552536231884058,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 0 1 2 3 4 5 6 7 8"
MEMORY TASKS,0.5543478260869565,Return
MEMORY TASKS,0.5561594202898551,"TBPTT
RTRL
Feedforward"
MEMORY TASKS,0.5579710144927537,"(c) Chaser, M = 5"
MEMORY TASKS,0.5597826086956522,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Steps
1e7 10 20 30 40 50 60"
MEMORY TASKS,0.5615942028985508,Return
MEMORY TASKS,0.5634057971014492,"TBPTT
RTRL"
MEMORY TASKS,0.5652173913043478,"(d) Non-matching, M = 100"
MEMORY TASKS,0.5670289855072463,"0.0
0.2
0.4
0.6
0.8
1.0
Steps
1e8 35 40 45 50 55 60"
MEMORY TASKS,0.5688405797101449,Return
MEMORY TASKS,0.5706521739130435,"TBPTT
RTRL"
MEMORY TASKS,0.572463768115942,"(e) Watermaze, M = 100"
MEMORY TASKS,0.5742753623188406,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 1 2 3 4 5 6 7 8 9"
MEMORY TASKS,0.5760869565217391,Return
MEMORY TASKS,0.5778985507246377,"TBPTT
RTRL
Feedforward"
MEMORY TASKS,0.5797101449275363,"(f) Chaser, M = 50"
MEMORY TASKS,0.5815217391304348,"Figure 1: Training curves on DMLab-30 rooms_select_nonmatching_object (Non-matching)
and rooms_watermaze (Watermaze), and Procgen Chaser environments."
MEMORY TASKS,0.5833333333333334,"Table
1:
Final
game
scores
on
two
memory
environments
of
DMLab-30:
rooms_select_nonmatching_object and rooms_watermaze.
Numbers on the top part are
copied from the respective papers for reference. We report mean and standard deviation computed
over 3 training seeds (each using 3 sets of 100 test episodes; see Appendix B.2). “frames” indicates the
number of environmental frames used for training. M is the hyper-parameter that controls weight up-
date frequency, M-step TD learning, and backpropagation span for TBPTT in IMPALA (see Sec. 3.2)."
MEMORY TASKS,0.5851449275362319,"frames
M
select_nonmatching_object
watermaze"
MEMORY TASKS,0.5869565217391305,"IMPALA ([31])
1 B
100
7.3
26.9
IMPALA ([32])
10 B
100
39.0
47.0
R2D2 ([32])
10 B
-
2.3
45.9
R2D2+ ([32])
10 B
-
63.6
49.0"
MEMORY TASKS,0.5887681159420289,"TBPTT
< 1.2B
10
54.5 ± 1.1
15.8 ± 0.9
RTRL
61.8 ± 0.5
40.2 ± 5.6"
MEMORY TASKS,0.5905797101449275,"TBPTT
< 1.2B
50
61.4 ± 0.5
44.5 ± 1.5
RTRL
62.0 ± 0.4
52.3 ± 1.9"
MEMORY TASKS,0.592391304347826,"TBPTT
< 1.2B
100
61.7 ± 0.1
45.6 ± 4.7
RTRL
62.2 ± 0.3
54.8 ± 4.3"
MEMORY TASKS,0.5942028985507246,"Similar to the rooms_select_nonmatching_object case above, with a sufficiently large M = 50,
255"
MEMORY TASKS,0.5960144927536232,"there is no difference between RTRL and TBPTT, while we observe benefits of RTRL when M = 5.
256"
GENERAL EVALUATION,0.5978260869565217,"4.3
General Evaluation
257"
GENERAL EVALUATION,0.5996376811594203,"Here we evaluate R2AC more broadly, including environments which are mostly reactive.
258"
GENERAL EVALUATION,0.6014492753623188,"Atari.
Apart from some exceptions (such as Solaris [32]), many of the Atari game environments are
259"
GENERAL EVALUATION,0.6032608695652174,"considered to be fully observable when observations consist of a stack of 4 frames [41, 42]. However,
260"
GENERAL EVALUATION,0.605072463768116,"it is also empirically known that, for certain games, recurrent policies yield higher performance
261"
GENERAL EVALUATION,0.6068840579710145,"than the feedforward ones having only access to 4 past frames (see, e.g., [43, 32, 44]). Here our
262"
GENERAL EVALUATION,0.6086956521739131,"general goal is to compare RTRL to TBPTT more broadly. We use five Atari environments: Breakout,
263"
GENERAL EVALUATION,0.6105072463768116,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 40 60 80 100 120 140 160 180 200"
GENERAL EVALUATION,0.6123188405797102,Return
GENERAL EVALUATION,0.6141304347826086,"TBPTT
RTRL
Feedforward"
GENERAL EVALUATION,0.6159420289855072,(a) Breakout
GENERAL EVALUATION,0.6177536231884058,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 0 200 400 600 800 1000 1200 1400"
GENERAL EVALUATION,0.6195652173913043,Return
GENERAL EVALUATION,0.6213768115942029,"TBPTT
RTRL
Feedforward"
GENERAL EVALUATION,0.6231884057971014,(b) Gravitar
GENERAL EVALUATION,0.625,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 500 1000 1500 2000 2500 3000 3500"
GENERAL EVALUATION,0.6268115942028986,Return
GENERAL EVALUATION,0.6286231884057971,"TBPTT
RTRL
Feedforward"
GENERAL EVALUATION,0.6304347826086957,(c) MsPacman
GENERAL EVALUATION,0.6322463768115942,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 0 2000 4000 6000 8000 10000 12000"
GENERAL EVALUATION,0.6340579710144928,Return
GENERAL EVALUATION,0.6358695652173914,"TBPTT
RTRL
Feedforward"
GENERAL EVALUATION,0.6376811594202898,(d) Q*Bert
GENERAL EVALUATION,0.6394927536231884,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Steps
1e8 2000 0 2000 4000 6000 8000 10000 12000 14000"
GENERAL EVALUATION,0.6413043478260869,Return
GENERAL EVALUATION,0.6431159420289855,"TBPTT
RTRL
Feedforward"
GENERAL EVALUATION,0.644927536231884,(e) Seaquest
GENERAL EVALUATION,0.6467391304347826,Figure 2: Learning curves on five Atari environments
GENERAL EVALUATION,0.6485507246376812,Table 2: Scores on Atari and DMLab-reactive (rooms_keys_doors_puzzle) environments.
GENERAL EVALUATION,0.6503623188405797,"Breakout
Gravitar
MsPacman
Q*bert
Seaquest
keys_doors"
GENERAL EVALUATION,0.6521739130434783,"Feedforward
234 ± 12
1084 ± 54
3020 ± 305
7746 ± 1356
4640 ± 3998
26.6 ± 1.1
TBPTT
305 ± 29
1269 ± 11
3953 ± 497
11298 ± 615
12401 ± 1694
26.1 ± 0.4
RTRL
275 ± 53
1670 ± 358
3346 ± 442
12484 ± 1524
12862 ± 961
26.1 ± 0.9"
GENERAL EVALUATION,0.6539855072463768,"Gravitar, MsPacman, Q*bert, and Seaquest, following Kapturowski et al. [32]’s selection for ablations
264"
GENERAL EVALUATION,0.6557971014492754,"of their R2D2. Here we use M = 50, and train for 200 M steps (with the action repeat of 4) from
265"
GENERAL EVALUATION,0.657608695652174,"scratch. The learning curves are shown in Figure 2. With the exception of MsPacman (note that,
266"
GENERAL EVALUATION,0.6594202898550725,"unlike ProcGen/Chaser above, 4-frame stacking is used) where we observe a slight performance
267"
GENERAL EVALUATION,0.6612318840579711,"degradation, RTRL performs equally well or better than TBPTT in all other environments.
268"
GENERAL EVALUATION,0.6630434782608695,"DMLab Reactive Task.
Finally, we also test our system on one environment of DMLab-30,
269"
GENERAL EVALUATION,0.6648550724637681,"room_keys_doors_puzzle, which is categorised as a reactive task according to Parisotto et al. [40].
270"
GENERAL EVALUATION,0.6666666666666666,"We train with M = 100 for 100 M steps (with the action repeat of 4). The mean episode length is
271"
GENERAL EVALUATION,0.6684782608695652,"about 450 steps. Table 2/right shows the scores. Effectively, all feedforward, TBPTT, and RTRL
272"
GENERAL EVALUATION,0.6702898550724637,"systems perform nearly the same (at least within 100 M steps/400 M frames). We note that these
273"
GENERAL EVALUATION,0.6721014492753623,"scores are comparable to the one reported by the original IMPALA [31] which is 28.0 after training
274"
GENERAL EVALUATION,0.6739130434782609,"on 1 B frames, which is much worse than the score reported by Kapturowski et al. [32] for IMPALA
275"
GENERAL EVALUATION,0.6757246376811594,"trained using 10 B frames (54.6). We show this example to confirm that RTRL is effectively not
276"
GENERAL EVALUATION,0.677536231884058,"helpful on a reactive task, unlike in the memory tasks above.
277"
LIMITATIONS AND DISCUSSION,0.6793478260869565,"5
Limitations and Discussion
278"
LIMITATIONS AND DISCUSSION,0.6811594202898551,"Here we discuss limitations of this work, which also sheds light on more general challenges of RTRL.
279"
LIMITATIONS AND DISCUSSION,0.6829710144927537,"Multi-layer case of our RTRL.
The most crucial limitation of our tractable-RTRL algorithm
280"
LIMITATIONS AND DISCUSSION,0.6847826086956522,"for element-wise recurrent nets (Sec. 3.1) is its restriction to the one-layer case. By stacking two
281"
LIMITATIONS AND DISCUSSION,0.6865942028985508,"such layers, the corresponding RTRL algorithm becomes intractable as we end up with the same
282"
LIMITATIONS AND DISCUSSION,0.6884057971014492,"complexity bottleneck as in fully recurrent networks. This is simply because by composing two such
283"
LIMITATIONS AND DISCUSSION,0.6902173913043478,"element-wise recurrent layers, we obtain a fully recurrent NN as a whole. This can be easily seen by
284"
LIMITATIONS AND DISCUSSION,0.6920289855072463,"writing down the following equations. By introducing extra superscripts to denote the layer number,
285"
LIMITATIONS AND DISCUSSION,0.6938405797101449,"in a stack of two element-wise LSTM layers of Eqs. 5-6 (we remove the output gate), we can express
286"
LIMITATIONS AND DISCUSSION,0.6956521739130435,"the recurrent state c(2)(t) of the second layer at step t as a function of the recurrent state c(1)(t −1)
287"
LIMITATIONS AND DISCUSSION,0.697463768115942,"of the first layer from the previous step as follows:
288"
LIMITATIONS AND DISCUSSION,0.6992753623188406,"c(2)(t) = f (2)(t) ⊙c(2)(t −1) + (1 −f (2)(t)) ⊙z(2)(t)
(13)"
LIMITATIONS AND DISCUSSION,0.7010869565217391,"f (2)(t) = σ(F (2)c(1)(t) + wf(2) ⊙c(2)(t −1))
(14)"
LIMITATIONS AND DISCUSSION,0.7028985507246377,"= σ(F (2) 
f (1)(t) ⊙c(1)(t −1) + (1 −f (1)(t)) ⊙z(1)(t)

+ ...)
(15)"
LIMITATIONS AND DISCUSSION,0.7047101449275363,"= σ(F (2)f (1)(t) ⊙c(1)(t −1) + F (2)(1 −f (1)(t)) ⊙z(1)(t) + ...)
(16)
By looking at the first term of Eq. 13 and that of Eq. 16, one can see that there is full recurrence
289"
LIMITATIONS AND DISCUSSION,0.7065217391304348,"between c(2)(t) and c(1)(t −1) via F (2), which brings back the quadratic/cubic time and space
290"
LIMITATIONS AND DISCUSSION,0.7083333333333334,"complexity for the sensitivity of the recurrent state in the second layer w.r.t. parameters of the first
291"
LIMITATIONS AND DISCUSSION,0.7101449275362319,"layers. This limitation is not discussed in prior work [18, 19].
292"
LIMITATIONS AND DISCUSSION,0.7119565217391305,"Complexity of multi-layer RTRL in general.
Generally speaking, RTRL for the multi-layer case is
293"
LIMITATIONS AND DISCUSSION,0.7137681159420289,"rarely discussed (except Meert and Ludik [45]; 1997). This case is important in modern deep learning
294"
LIMITATIONS AND DISCUSSION,0.7155797101449275,"where stacking multiple layers is a standard. There are two important remarks to be made here.
295"
LIMITATIONS AND DISCUSSION,0.717391304347826,"First of all, even in an NN with a single RNN layer, if there is a layer with trainable parameters whose
296"
LIMITATIONS AND DISCUSSION,0.7192028985507246,"output is connected to the input of the RNN layer, a sensitivity matrix needs to be computed and stored
297"
LIMITATIONS AND DISCUSSION,0.7210144927536232,"for each of these parameters. A good illustration is the policy net used in all our RL experiments
298"
LIMITATIONS AND DISCUSSION,0.7228260869565217,"where our eLSTM layer takes the output of a deep (feedforward) convolutional net (the vision stem)
299"
LIMITATIONS AND DISCUSSION,0.7246376811594203,"as input. As training this vision stem using RTRL requires dealing with the corresponding sensitivity
300"
LIMITATIONS AND DISCUSSION,0.7264492753623188,"matrix, which is intractable, we train/pretrain the vision stem using TBPTT (Sec. 4.2;4.3). This is an
301"
LIMITATIONS AND DISCUSSION,0.7282608695652174,"important remark for RTRL research in general. For example, approximation methods proposed for
302"
LIMITATIONS AND DISCUSSION,0.730072463768116,"the single-layer case may not scale to the multi-layer case; e.g., to exploit sparsity in the policy net
303"
LIMITATIONS AND DISCUSSION,0.7318840579710145,"above, it is not enough to assume weight sparsity in the RNN layer, but also in the vision stem.
304"
LIMITATIONS AND DISCUSSION,0.7336956521739131,"Second, the multi-layer case [45] introduces more complexity growth to RTRL than to BPTT. Let
305"
LIMITATIONS AND DISCUSSION,0.7355072463768116,"L denote the number of layers. We seemlessly use BPTT with deep NNs, as its time and space
306"
LIMITATIONS AND DISCUSSION,0.7373188405797102,"complexity is linear in L. This is not the case for RTRL. With RTRL, for each recurrent layer, we need
307"
LIMITATIONS AND DISCUSSION,0.7391304347826086,"to store sensitivities of all parameters of all preceding layers. This implies that, for an L-layer RNN,
308"
LIMITATIONS AND DISCUSSION,0.7409420289855072,"parameters in the first layer require L sensitivity matrices, L−1 for the second layer, ..., etc., resulting
309"
LIMITATIONS AND DISCUSSION,0.7427536231884058,"in L + (L −1) + (L −2) + ... + 2 + 1 = L(L + 1)/2 sensitivity matrices to be computed and stored.
310"
LIMITATIONS AND DISCUSSION,0.7445652173913043,"Given that multi-layer NNs are crucial today, this remains a big challenge for practical RTRL research.
311"
LIMITATIONS AND DISCUSSION,0.7463768115942029,"Principled vs. practical solution.
Another important aspect of RTRL research is that many realis-
312"
LIMITATIONS AND DISCUSSION,0.7481884057971014,"tic memory tasks have actual dependencies/credit assignment paths that are shorter than the max-
313"
LIMITATIONS AND DISCUSSION,0.75,"imum BPTT span we can afford in practice. In our experiments, with the exception of DMLab
314"
LIMITATIONS AND DISCUSSION,0.7518115942028986,"rooms_watermaze (Sec. 4.2), no task actually absolutely requires RTRL in practice; TBPTT with a
315"
LIMITATIONS AND DISCUSSION,0.7536231884057971,"large span suffices. Future improvements of the hardware may give a further advantage to TBPTT;
316"
LIMITATIONS AND DISCUSSION,0.7554347826086957,"the practical (simple) solution offered by TBPTT might be prioritised over the principled (complex)
317"
LIMITATIONS AND DISCUSSION,0.7572463768115942,"RTRL solution for dealing with long-span credit assignments. This is also somewhat reminiscent of
318"
LIMITATIONS AND DISCUSSION,0.7590579710144928,"the Transformer vs. RNN discussion regarding sequence processing with limited vs. unlimited context.
319"
LIMITATIONS AND DISCUSSION,0.7608695652173914,"Sequence-level parallelism.
While our study focuses on evaluation of untruncated gradients,
320"
LIMITATIONS AND DISCUSSION,0.7626811594202898,"another potential benefit of RTRL is online learning. For most standard self/supervised sequence-
321"
LIMITATIONS AND DISCUSSION,0.7644927536231884,"processing tasks such as language modelling, however, modern implementations are optimised to
322"
LIMITATIONS AND DISCUSSION,0.7663043478260869,"exploit access to the “full” sequence, and to leverage parallel computation across the time axis (at
323"
LIMITATIONS AND DISCUSSION,0.7681159420289855,"least for training). While some hybrid RTRL-BPTT approaches [46] may still be able to exploit such
324"
LIMITATIONS AND DISCUSSION,0.769927536231884,"a parallelism, fast online learning remains open engineering challenge even with tractable RTRL.
325"
CONCLUSION,0.7717391304347826,"6
Conclusion
326"
CONCLUSION,0.7735507246376812,"We demonstrate the empirical promise of RTRL in realistic settings. By focusing on RNNs with
327"
CONCLUSION,0.7753623188405797,"element-wise recurrence, we obtain tractable RTRL without approximation. We evaluate our rein-
328"
CONCLUSION,0.7771739130434783,"forcement learning RTRL-based actor-critic in several popular game environments. In one of the
329"
CONCLUSION,0.7789855072463768,"challenging DMLab-30 memory environments, our system outperforms the well-known IMPALA
330"
CONCLUSION,0.7807971014492754,"and R2D2 baselines which use many more environmental steps. We also highlight general important
331"
CONCLUSION,0.782608695652174,"limitations and further challenges of RTRL rarely discussed in prior work.
332"
REFERENCES,0.7844202898550725,"References
333"
REFERENCES,0.7862318840579711,"[1] Ronald J Williams and David Zipser. A learning algorithm for continually running fully
334"
REFERENCES,0.7880434782608695,"recurrent neural networks. Neural computation, 1(2):270–280, 1989.
335"
REFERENCES,0.7898550724637681,"[2] Ronald J Williams and David Zipser. Experimental analysis of the real-time recurrent learning
336"
REFERENCES,0.7916666666666666,"algorithm. Connection science, 1(1):87–111, 1989.
337"
REFERENCES,0.7934782608695652,"[3] Anthony J Robinson and Frank Fallside. The utility driven dynamic error propagation network,
338"
REFERENCES,0.7952898550724637,"volume 1. University of Cambridge Department of Engineering Cambridge, 1987.
339"
REFERENCES,0.7971014492753623,"[4] Anthony J Robinson. Dynamic error propagation networks. PhD thesis, University of Cam-
340"
REFERENCES,0.7989130434782609,"bridge, 1989.
341"
REFERENCES,0.8007246376811594,"[5] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
342"
REFERENCES,0.802536231884058,"back-propagating errors. nature, 323(6088):533–536, 1986.
343"
REFERENCES,0.8043478260869565,"[6] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of
344"
REFERENCES,0.8061594202898551,"the IEEE, 78(10):1550–1560, 1990.
345"
REFERENCES,0.8079710144927537,"[7] Ronald J Williams and Jing Peng. An efficient gradient-based algorithm for online training of
346"
REFERENCES,0.8097826086956522,"recurrent network trajectories. Neural computation, 2(4):490–501, 1990.
347"
REFERENCES,0.8115942028985508,"[8] Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. In Int. Conf. on
348"
REFERENCES,0.8134057971014492,"Learning Representations (ICLR), Vancouver, Canada, April 2018.
349"
REFERENCES,0.8152173913043478,"[9] Asier Mujika, Florian Meier, and Angelika Steger. Approximating real-time recurrent learning
350"
REFERENCES,0.8170289855072463,"with random kronecker factors. In Proc. Advances in Neural Information Processing Systems
351"
REFERENCES,0.8188405797101449,"(NeurIPS), pages 6594–6603, Montréal, Canada, December 2018.
352"
REFERENCES,0.8206521739130435,"[10] Frederik Benzing, Marcelo Matheus Gauy, Asier Mujika, Anders Martinsson, and Angelika
353"
REFERENCES,0.822463768115942,"Steger. Optimal kronecker-sum approximation of real time recurrent learning. In Proc. Int. Conf.
354"
REFERENCES,0.8242753623188406,"on Machine Learning (ICML), volume 97, pages 604–613, Long Beach, CA, USA, June 2019.
355"
REFERENCES,0.8260869565217391,"[11] Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex Graves.
356"
REFERENCES,0.8278985507246377,"Practical real time recurrent learning with a sparse approximation. In Int. Conf. on Learning
357"
REFERENCES,0.8297101449275363,"Representations (ICLR), Virtual only, May 2021.
358"
REFERENCES,0.8315217391304348,"[12] David Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, and Hado van Hasselt. Learning
359"
REFERENCES,0.8333333333333334,"by directional gradient descent. In Int. Conf. on Learning Representations (ICLR), Virtual only,
360"
REFERENCES,0.8351449275362319,"April 2022.
361"
REFERENCES,0.8369565217391305,"[13] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural
362"
REFERENCES,0.8387681159420289,"networks. In Int. Conf. on Learning Representations (ICLR), Toulon, France, April 2017.
363"
REFERENCES,0.8405797101449275,"[14] Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly
364"
REFERENCES,0.842391304347826,"parallelizable recurrence. In Proc. Conf. on Empirical Methods in Natural Language Processing
365"
REFERENCES,0.8442028985507246,"(EMNLP), pages 4470–4481, Brussels, Belgium, November 2018.
366"
REFERENCES,0.8460144927536232,"[15] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
367"
REFERENCES,0.8478260869565217,"are RNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on Machine
368"
REFERENCES,0.8496376811594203,"Learning (ICML), Virtual only, July 2020.
369"
REFERENCES,0.8514492753623188,"[16] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent
370"
REFERENCES,0.8532608695652174,"nets. Technical Report FKI-147-91, Institut für Informatik, Technische Universität München,
371"
REFERENCES,0.855072463768116,"March 1991.
372"
REFERENCES,0.8568840579710145,"[17] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear Transformers are secretly fast
373"
REFERENCES,0.8586956521739131,"weight programmers. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July 2021.
374"
REFERENCES,0.8605072463768116,"[18] Michael C. Mozer. A focused backpropagation algorithm for temporal pattern recognition.
375"
REFERENCES,0.8623188405797102,"Complex Systems, 3(4):349–381, 1989.
376"
REFERENCES,0.8641304347826086,"[19] Michael Mozer. Induction of multiscale temporal structure. In Proc. Advances in Neural
377"
REFERENCES,0.8659420289855072,"Information Processing Systems (NIPS), pages 275–282, Denver, CO, USA, December 1991.
378"
REFERENCES,0.8677536231884058,"[20] Khurram Javed, Martha White, and Richard S. Sutton. Scalable online recurrent learning using
379"
REFERENCES,0.8695652173913043,"columnar neural networks. Preprint arXiv:2103.05787, 2021.
380"
REFERENCES,0.8713768115942029,"[21] Khurram Javed, Haseeb Shah, Rich Sutton, and Martha White. Online real-time recurrent
381"
REFERENCES,0.8731884057971014,"learning using sparse connections and selective learning. Preprint arXiv:2302.05326, 2023.
382"
REFERENCES,0.875,"[22] Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Proc. Advances in Neural
383"
REFERENCES,0.8768115942028986,"Information Processing Systems (NIPS), pages 1008–1014, Denver, CO, USA, November 1999.
384"
REFERENCES,0.8786231884057971,"[23] Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
385"
REFERENCES,0.8804347826086957,"methods for reinforcement learning with function approximation. In Proc. Advances in Neural
386"
REFERENCES,0.8822463768115942,"Information Processing Systems (NIPS), pages 1057–1063, Denver, CO, USA, November 1999.
387"
REFERENCES,0.8840579710144928,"[24] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
388"
REFERENCES,0.8858695652173914,"Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
389"
REFERENCES,0.8876811594202898,"ment learning. In Proc. Int. Conf. on Machine Learning (ICML), pages 1928–1937, New York
390"
REFERENCES,0.8894927536231884,"City, NY, USA, June 2016.
391"
REFERENCES,0.8913043478260869,"[25] Daan Wierstra, Alexander Förster, Jan Peters, and Jürgen Schmidhuber. Recurrent policy
392"
REFERENCES,0.8931159420289855,"gradients. Logic Journal of IGPL, 18(2):620–634, 2010.
393"
REFERENCES,0.894927536231884,"[26] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
394"
REFERENCES,0.8967391304347826,"partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998.
395"
REFERENCES,0.8985507246376812,"[27] Jürgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in
396"
REFERENCES,0.9003623188405797,"reactive environments. In Proc. Int. Joint Conf. on Neural Networks (IJCNN), pages 253–258,
397"
REFERENCES,0.9021739130434783,"San Diego, CA, USA, June 1990.
398"
REFERENCES,0.9039855072463768,"[28] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich
399"
REFERENCES,0.9057971014492754,"Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab.
400"
REFERENCES,0.907608695652174,"Preprint arXiv:1612.03801, 2016.
401"
REFERENCES,0.9094202898550725,"[29] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural
402"
REFERENCES,0.9112318840579711,"generation to benchmark reinforcement learning. In Proc. Int. Conf. on Machine Learning
403"
REFERENCES,0.9130434782608695,"(ICML), pages 2048–2056, Virtual only, July 2020.
404"
REFERENCES,0.9148550724637681,"[30] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
405"
REFERENCES,0.9166666666666666,"environment: An evaluation platform for general agents. Journal of Artificial Intelligence
406"
REFERENCES,0.9184782608695652,"Research, 47:253–279, 2013.
407"
REFERENCES,0.9202898550724637,"[31] Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward,
408"
REFERENCES,0.9221014492753623,"Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.
409"
REFERENCES,0.9239130434782609,"IMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures.
410"
REFERENCES,0.9257246376811594,"In Proc. Int. Conf. on Machine Learning (ICML), pages 1406–1415, Stockholm, Sweden, July
411"
REFERENCES,0.927536231884058,"2018.
412"
REFERENCES,0.9293478260869565,"[32] Steven Kapturowski, Georg Ostrovski, John Quan, Rémi Munos, and Will Dabney. Recurrent
413"
REFERENCES,0.9311594202898551,"experience replay in distributed reinforcement learning. In Int. Conf. on Learning Representa-
414"
REFERENCES,0.9329710144927537,"tions (ICLR), New Orleans, LA, USA, May 2019.
415"
REFERENCES,0.9347826086956522,"[33] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.
416"
REFERENCES,0.9365942028985508,"[34] Barak A Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural
417"
REFERENCES,0.9384057971014492,"Computation, 1(2):263–269, 1989.
418"
REFERENCES,0.9402173913043478,"[35] Michael Gherrity. A learning algorithm for analog, fully recurrent neural networks. In Proc. Int.
419"
REFERENCES,0.9420289855072463,"Joint Conf. on Neural Networks (IJCNN), volume 643, 1989.
420"
REFERENCES,0.9438405797101449,"[36] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
421"
REFERENCES,0.9456521739130435,"1735–1780, 1997.
422"
REFERENCES,0.947463768115942,"[37] Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural
423"
REFERENCES,0.9492753623188406,"network (indrnn): Building a longer and deeper RNN. In Proc. IEEE Conf. on Computer Vision
424"
REFERENCES,0.9510869565217391,"and Pattern Recognition (CVPR), pages 5457–5466, Salt Lake City, UT, USA, June 2018.
425"
REFERENCES,0.9528985507246377,"[38] Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber.
426"
REFERENCES,0.9547101449275363,"LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems,
427"
REFERENCES,0.9565217391304348,"28(10):2222–2232, 2016.
428"
REFERENCES,0.9583333333333334,"[39] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
429"
REFERENCES,0.9601449275362319,"1998.
430"
REFERENCES,0.9619565217391305,"[40] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayaku-
431"
REFERENCES,0.9637681159420289,"mar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick,
432"
REFERENCES,0.9655797101449275,"Nicolas Heess, and Raia Hadsell. Stabilizing Transformers for reinforcement learning. In Proc.
433"
REFERENCES,0.967391304347826,"Int. Conf. on Machine Learning (ICML), pages 7487–7498, Virtual only, July 2020.
434"
REFERENCES,0.9692028985507246,"[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
435"
REFERENCES,0.9710144927536232,"Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
436"
REFERENCES,0.9728260869565217,"Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
437"
REFERENCES,0.9746376811594203,"[42] Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable
438"
REFERENCES,0.9764492753623188,"mdps. In AAAI Fall Symposia, pages 29–37, Arlington, VA, USA, November 2015.
439"
REFERENCES,0.9782608695652174,"[43] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende.
440"
REFERENCES,0.980072463768116,"Towards interpretable reinforcement learning using attention augmented agents.
In Proc.
441"
REFERENCES,0.9818840579710145,"Advances in Neural Information Processing Systems (NeurIPS), pages 12329–12338, Vancouver,
442"
REFERENCES,0.9836956521739131,"Canada, December 2019.
443"
REFERENCES,0.9855072463768116,"[44] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear
444"
REFERENCES,0.9873188405797102,"transformers with recurrent fast weight programmers. In Proc. Advances in Neural Information
445"
REFERENCES,0.9891304347826086,"Processing Systems (NeurIPS), Virtual only, December 2021.
446"
REFERENCES,0.9909420289855072,"[45] Kürt Meert and Jacques Ludik. A multilayer real-time recurrent learning algorithm for improved
447"
REFERENCES,0.9927536231884058,"convergence. In Proc. Int. Conf. on Artificial Neural Networks (ICANN), pages 445–450,
448"
REFERENCES,0.9945652173913043,"Lausanne, Switzerland, October 1997.
449"
REFERENCES,0.9963768115942029,"[46] Jürgen Schmidhuber. A fixed size storage o(n3) time complexity learning algorithm for fully
450"
REFERENCES,0.9981884057971014,"recurrent continually running networks. Neural Computation, 4(2):243–248, 1992.
451"
