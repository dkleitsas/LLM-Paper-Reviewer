Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00234192037470726,"While generative models continue to evolve, the field of evaluation metrics has
1"
ABSTRACT,0.00468384074941452,"largely remained stagnant. Despite the annual publication of metric papers, the
2"
ABSTRACT,0.00702576112412178,"majority of these metrics share a common characteristic: they measure distributional
3"
ABSTRACT,0.00936768149882904,"distance using pre-trained embeddings without considering the interpretability of
4"
ABSTRACT,0.0117096018735363,"the underlying information. This limits their usefulness and makes it difficult to gain
5"
ABSTRACT,0.01405152224824356,"a comprehensive understanding of the data. To address this issue, we propose using
6"
ABSTRACT,0.01639344262295082,"a new type of interpretable embedding. We demonstrate how we can transform
7"
ABSTRACT,0.01873536299765808,"deeply encoded embeddings into interpretable embeddings by measuring their
8"
ABSTRACT,0.02107728337236534,"correspondence with text attributes. With this new type of embedding, we introduce
9"
ABSTRACT,0.0234192037470726,"two novel metrics that measure and explain the diversity of the generator: the first
10"
ABSTRACT,0.02576112412177986,"metric compares the frequency of appearance of the training set and the attribute,
11"
ABSTRACT,0.02810304449648712,"and the second metric evaluates whether the relationships between attributes in the
12"
ABSTRACT,0.03044496487119438,"training set are preserved. By introducing these new metrics, we hope to enhance
13"
ABSTRACT,0.03278688524590164,"the interpretability and usefulness of evaluation metrics in the field of generative
14"
ABSTRACT,0.0351288056206089,"models.
15"
INTRODUCTION,0.03747072599531616,"1
Introduction
16"
INTRODUCTION,0.03981264637002342,"Significant advancements have been achieved in the image generation field, from the pioneering
17"
INTRODUCTION,0.04215456674473068,"introduction of generative adversarial networks (GANs) to the more recent emergence of diffusion
18"
INTRODUCTION,0.04449648711943794,"models (DMs). [5, 10, 27] In recent years, generated images are hardly distinguishable from real
19"
INTRODUCTION,0.0468384074941452,"images. In this context, evaluating the generated images for a given training dataset has played a
20"
INTRODUCTION,0.04918032786885246,"critical role in the development.
21"
INTRODUCTION,0.05152224824355972,"Envision an evaluation scenario where the outputs of two generative models are compared against
22"
INTRODUCTION,0.053864168618266976,"a common training dataset. What would be the underlying factors for judging a set as superior to
23"
INTRODUCTION,0.05620608899297424,"another set? As the goal of generative models is mimicking the real data distribution, various metrics
24"
INTRODUCTION,0.0585480093676815,"have been designed to assess the similarity between the generated images and the training dataset, e.g.,
25"
INTRODUCTION,0.06088992974238876,"Fréchet Inception Distance (FID)[9], Precision and Recall[25][17], and Density and Coverage[22].
26"
INTRODUCTION,0.06323185011709602,"Most of these evaluation metrics capture the disparity between the training data distribution and the
27"
INTRODUCTION,0.06557377049180328,"distribution of generated images by examining the differences in feature representations within the
28"
INTRODUCTION,0.06791569086651054,"embedding space of a pre-trained network[26, 28]. FID is a widely used metric that quantifies the
29"
INTRODUCTION,0.0702576112412178,"dissimilarity in visual features to assess the quality and diversity of the generated images. Specifically,
30"
INTRODUCTION,0.07259953161592506,"it measures the distance between the real and fake distributions in the embedding space of Inception-
31"
INTRODUCTION,0.07494145199063232,"V3[28].
32"
INTRODUCTION,0.07728337236533958,"An important question arises regarding the suitability of the embedding space employed for evaluating
33"
INTRODUCTION,0.07962529274004684,"generated images. The embedding space of the pre-trained model may vary depending on the dataset
34"
INTRODUCTION,0.08196721311475409,"and task it was trained on. For instance, Inception V3 was trained for image classification on
35"
INTRODUCTION,0.08430913348946135,"Training dataset 
Model 1
Model 2"
INTRODUCTION,0.08665105386416862,"(b) proposed metric
(a) existing metrics"
INTRODUCTION,0.08899297423887588,"Model 1
Model 2"
INTRODUCTION,0.09133489461358314,"Figure 1: Conceptual illustration of our method. We design the scenario, Model 2 lacks diversity.
(a) Although existing metrics distinguish the inferiority of Model 2, they provide no explanation
about judgment. (b) Our attribute-based proposed metric has interpretation; Model 2 is biased with
‘long hair’ and ‘makeup’."
INTRODUCTION,0.0936768149882904,"ImageNet[3], suggesting that its embedding space is designed to compress image information and
36"
INTRODUCTION,0.09601873536299765,"discern essential patterns for classification. Consequently, the appropriateness of employing this
37"
INTRODUCTION,0.09836065573770492,"embedding space for evaluating generated images remains an open question.
38"
INTRODUCTION,0.10070257611241218,"Returning to the fundamental question at hand, Figure 1 makes an evaluation scenario a little bit more
39"
INTRODUCTION,0.10304449648711944,"specific. Suppose there are realistic generated images from two distinct models. As shown in the
40"
INTRODUCTION,0.1053864168618267,"example images, it is evident that Model 2 generates biased images, i.e., there are only women, while
41"
INTRODUCTION,0.10772833723653395,"Model 1 successfully generates various images that are close to training data. Fortunately, although
42"
INTRODUCTION,0.11007025761124122,"there remains an open question about embedding space, the values of various metrics in Figure 1 (b)
43"
INTRODUCTION,0.11241217798594848,"align reasonably well with our interpretation; Model 1 is perceived as superior.
44"
INTRODUCTION,0.11475409836065574,"However, what are the underlying factors that contribute to such judgment? Although the results
45"
INTRODUCTION,0.117096018735363,"are consistent with a person’s conclusion, it far fails to provide a comprehensive explanation. The
46"
INTRODUCTION,0.11943793911007025,"interpretation of distances within the embedding space from a pre-trained classification model remains
47"
INTRODUCTION,0.12177985948477751,"elusive, posing challenges in evaluation. On the contrary, humans readily discern certain factors for
48"
INTRODUCTION,0.12412177985948478,"judgment; individuals easily recognize the bias of Model 2. These factors suggest more information
49"
INTRODUCTION,0.12646370023419204,"and a direction beyond simple ranking. In this paper, we propose an evaluation metric that aims to
50"
INTRODUCTION,0.1288056206088993,"interpret the underlying factors behind such judgments.
51"
INTRODUCTION,0.13114754098360656,"To address this objective, we begin by examining attribute comparison methods in human judgment.
52"
INTRODUCTION,0.13348946135831383,"When evaluating two generated image distributions, humans compare the attributes present in the
53"
INTRODUCTION,0.1358313817330211,"training dataset with those exhibited by the generated images. Key attributes under consideration
54"
INTRODUCTION,0.13817330210772832,"include gender, facial representation, and age distribution. Ideally, with well-defined training data, we
55"
INTRODUCTION,0.1405152224824356,"anticipate the attributes in the generated images to align with those in the training data. If the model
56"
INTRODUCTION,0.14285714285714285,"lacks essential attributes (e.g., gender, age, glasses, or hats), it is insufficient to generate visually
57"
INTRODUCTION,0.1451990632318501,"realistic images. Incorporating these attributes into the evaluation process may enable a more explicit
58"
INTRODUCTION,0.14754098360655737,"and comprehensive assessment.
59"
INTRODUCTION,0.14988290398126464,"This paper presents a novel approach for evaluating generative models by leveraging a newly proposed
60"
INTRODUCTION,0.1522248243559719,"embedding space that incorporates attribute-specific information. Similar to human visual judgment,
61"
INTRODUCTION,0.15456674473067916,"our metrics evaluate images in terms of various characteristic attributes. Figure 1 (b) illustrates the
62"
INTRODUCTION,0.15690866510538642,"concept of our metric; it captures the distribution differences of attributes. We use pre-trained CLIP
63"
INTRODUCTION,0.1592505854800937,"[24], a language-image model trained on a huge dataset, to define a new embedding space that can
64"
INTRODUCTION,0.16159250585480095,"quantify images for multiple attributes.
65"
INTRODUCTION,0.16393442622950818,"To facilitate our embedding space, we introduce the ""Directional CLIPScore"" (DCS), a method for
66"
INTRODUCTION,0.16627634660421545,"quantifying each attribute based on the training data. Within our proposed embedding space, each
67"
INTRODUCTION,0.1686182669789227,"channel comprises DCS values that explicitly indicate the relevance of an image to specific attributes.
68"
INTRODUCTION,0.17096018735362997,"The use of a perceptible embedding space offers the advantage of interpretability.
69"
INTRODUCTION,0.17330210772833723,"We introduce two novel evaluation metrics to use the newly proposed embedding space. Firstly,
70"
INTRODUCTION,0.1756440281030445,"the ""Single attribute KL Divergence (SaKLD)"" compares attribute distributions between training
71"
INTRODUCTION,0.17798594847775176,"data and the generated images, providing a quantitative measure of the similarity between attribute
72"
INTRODUCTION,0.18032786885245902,"distributions. It quantifies how closely the attributes of generated images align with the attribute
73"
INTRODUCTION,0.18266978922716628,"distribution in training data. Secondly, we introduce the ""Paired attribute KL divergence (PaKLD)""
74"
INTRODUCTION,0.18501170960187355,"that considers correlations among multiple attributes. This metric accounts for the relationship
75"
INTRODUCTION,0.1873536299765808,"between attributes, such as the presence of a beard in an image of a woman. PaKLD successfully
76"
INTRODUCTION,0.18969555035128804,"evaluates the generated images while taking into consideration attribute relationships.
77"
INTRODUCTION,0.1920374707259953,"We validate our metrics through a series of carefully designed experiments, demonstrating their
78"
INTRODUCTION,0.19437939110070257,"effectiveness and interpretability. By employing our metric, we conduct a comprehensive analysis of
79"
INTRODUCTION,0.19672131147540983,"prominent generation models currently considered state-of-the-art [11, 13, 12, 14, 23]. Interestingly,
80"
INTRODUCTION,0.1990632318501171,"our findings reveal variations in performance across different datasets. For instance, diffusion models
81"
INTRODUCTION,0.20140515222482436,"exhibit superior performance on datasets with a large number of samples, such as FFHQ. In contrast,
82"
INTRODUCTION,0.20374707259953162,"GANs outperform diffusion models on datasets with relatively smaller sample size, such as MetFaces.
83"
INTRODUCTION,0.20608899297423888,"In summary, this paper presents a novel approach for evaluating generative models using a new
84"
INTRODUCTION,0.20843091334894615,"embedding space that incorporates attribute-specific information. Our proposed method, along with
85"
INTRODUCTION,0.2107728337236534,"the introduced evaluation metrics, allows for a comprehensive assessment of generated images by
86"
INTRODUCTION,0.21311475409836064,"considering attribute distributions and correlations. Our findings contribute to the research field by
87"
INTRODUCTION,0.2154566744730679,"advancing the understanding and evaluation of generative models, offering insights into their strengths
88"
INTRODUCTION,0.21779859484777517,"and limitations. Moreover, our work opens avenues for future research and potential improvements in
89"
INTRODUCTION,0.22014051522248243,"the field of generative image synthesis by comprehensive evaluation metrics.
90"
RELATED WORK,0.2224824355971897,"2
Related Work
91"
RELATED WORK,0.22482435597189696,"Fréchet Inception Distance
Fréchet Inception Distance (FID) [9] measures the distance between
92"
RELATED WORK,0.22716627634660422,"the estimated Gaussian distributions of two datasets by passing them through a pre-trained Inception-
93"
RELATED WORK,0.22950819672131148,"v3[28] model. However, Kynkäänniemi et al. [18] revealed that when generated images are far from
94"
RELATED WORK,0.23185011709601874,"training data, the embeddings may incorrectly highlight irrelevant parts of images. To address this
95"
RELATED WORK,0.234192037470726,"issue, the researchers proposed using the CLIP [24] image encoder instead of Inception-v3 to calculate
96"
RELATED WORK,0.23653395784543327,"the 2-Wasserstein distance, which provides reliable results regardless of the dataset being measured.
97"
RELATED WORK,0.2388758782201405,"Fidelity and diversity
Sajjadi et al. [25] introduced precision and recall for evaluating generative
98"
RELATED WORK,0.24121779859484777,"model, and subsequent studies by Kynkäänniemi et al. [17] and Naeem et al. [22] have further refined
99"
RELATED WORK,0.24355971896955503,"this approach. Most of these methods use a pre-trained network to examine whether the embedding
100"
RELATED WORK,0.2459016393442623,"of generated images falls within the boundary of real image embedding (precision) and whether
101"
RELATED WORK,0.24824355971896955,"the embedding of real images falls within the boundary of generated image embedding (recall) for
102"
RELATED WORK,0.2505854800936768,"assessing fidelity and diversity.
103"
RELATED WORK,0.2529274004683841,"Rarity score
Han et al. [6] proposed a metric for measuring the rarity of generated images. They
104"
RELATED WORK,0.25526932084309134,"quantified how rare the generated images are within a k-NN sphere to assess their rarity. The key
105"
RELATED WORK,0.2576112412177986,"difference between the rarity score and diversity in precision and recall is that the rarity score
106"
RELATED WORK,0.25995316159250587,"considers only the generated samples that fall within the manifold of real samples. In other words, it
107"
RELATED WORK,0.26229508196721313,"focuses on how well the generated images fit within the distribution of real images in terms of rarity,
108"
RELATED WORK,0.2646370023419204,"rather than capturing the overall diversity of generated samples.
109"
RELATED WORK,0.26697892271662765,"However, we note that the concept of using raw embeddings from a pre-trained classifier remains
110"
RELATED WORK,0.2693208430913349,"consistent among all these metrics.
111"
RELATED WORK,0.2716627634660422,"A call for explainable evaluation
Existing evaluation metrics in the field of generative models lack
112"
RELATED WORK,0.27400468384074944,"the ability to provide detailed insights into the diversity of generated images. As shown in Figure 1,
113"
RELATED WORK,0.27634660421545665,"even though metrics like FID, Precision and Recall indicate poor performance for a biased generator
114"
RELATED WORK,0.2786885245901639,"towards specific attributes (e.g., ""makeup"" and ""long hair""), they do not provide an explanation
115"
RELATED WORK,0.2810304449648712,"for judgment factors. Therefore, researchers manually identified the underlying factors by visual
116"
RELATED WORK,0.28337236533957844,"inspection but it becomes increasingly challenging with larger sample sizes. To address this issue,
117"
RELATED WORK,0.2857142857142857,"we propose novel explainable evaluation metrics that provide in-depth analysis and insights into the
118"
RELATED WORK,0.28805620608899296,"diverse generation abilities of models.
119 CS"
RELATED WORK,0.2903981264637002,"(a) CLIPScore
(b) Directional CLIPScore DCS"
RELATED WORK,0.2927400468384075,"Figure 2: Difference between CS and DCS. (a) CLIPScore[8] exhibits similar values, making it
difficult to discern. (b) Directional CLIPScore has an intuitive value based on zero. We design a new
embedding space; each channel represents the intensity of a specific attribute by DCS, informing
explanations about the single image."
ATTRIBUTE-DRIVEN EMBEDDING,0.29508196721311475,"3
Attribute-Driven Embedding
120"
ATTRIBUTE-DRIVEN EMBEDDING,0.297423887587822,"Existing metrics for evaluating generated images commonly utilize embeddings before FCN, from
121"
ATTRIBUTE-DRIVEN EMBEDDING,0.2997658079625293,"Inception-V3 or CLIP image encoder[7, 4]. However, these approaches lack interpretability as the
122"
ATTRIBUTE-DRIVEN EMBEDDING,0.30210772833723654,"meaning of each channel in the embedding. Additionally, Kynkäänniemi et al. [18] have shown the
123"
ATTRIBUTE-DRIVEN EMBEDDING,0.3044496487119438,"FID scores improve significantly when the classification distribution matches that of the training
124"
ATTRIBUTE-DRIVEN EMBEDDING,0.30679156908665106,"set, irrespective of the quality, highlighting another limitation of the existing embedding. To address
125"
ATTRIBUTE-DRIVEN EMBEDDING,0.3091334894613583,"these issues and develop an explainable evaluation metric, we design each embedding of images to
126"
ATTRIBUTE-DRIVEN EMBEDDING,0.3114754098360656,"possess an ’interpretation’. Section 3.1 presents the process of generating explainable embedding for
127"
ATTRIBUTE-DRIVEN EMBEDDING,0.31381733021077285,"individual images using the CLIP encoder, and Section 3.2 introduces the Directional CLIPScore, a
128"
ATTRIBUTE-DRIVEN EMBEDDING,0.3161592505854801,"novel embedding approach that enhances interpretability and accuracy.
129"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3185011709601874,"3.1
Attribute-driven embeddings for better representations
130"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.32084309133489464,"To achieve an interpretable embedding, we utilized each channel of the embedding as a measure of
131"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3231850117096019,"the attribute’s prominence in the image. A straightforward approach to quantify attribute strength is
132"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3255269320843091,"by employing CLIPScore;
133"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.32786885245901637,"CLIPScore(x, a) = 100 ∗sim(EI(x), ET(a)),
(1)"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.33021077283372363,"where x is a single image, a is a given text of attribute, sim(∗, ∗) is cosine similarity, and EI and ET
134"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3325526932084309,"are CLIP image encoder and text encoder respectively. We selected multiple attributes that effectively
135"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.33489461358313816,"represent image characteristics as textual descriptions and measured CLIPScore with individual
136"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3372365339578454,"images and selected attributes. The way to select attributes will refer to Section 3.3. By assigning
137"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3395784543325527,"these CLIPScores as the values for each channel in the embedding, we obtained an interpretable
138"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.34192037470725994,"representation. However, relying solely on CLIPScore has challenges as the cosine similarity values
139"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.3442622950819672,"tend to be similar, making it difficult to discern the relative differences between attribute strengths.
140"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.34660421545667447,"Intuitively, selected human-related attributes tend to cluster closely in the CLIP embedding, resulting
141"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.34894613583138173,"in smaller variations in cosine similarity. To address this limitation, subsequent subsections introduce
142"
ATTRIBUTE-DRIVEN EMBEDDINGS FOR BETTER REPRESENTATIONS,0.351288056206089,"the Directional CLIPScore, which offers a more precise scoring approach.
143"
DIRECTIONAL CLIPSCORE,0.35362997658079626,"3.2
Directional CLIPScore
144"
DIRECTIONAL CLIPSCORE,0.3559718969555035,"As discussed, CLIPScore exhibits a narrow distribution of values, which can be attributed to measuring
145"
DIRECTIONAL CLIPSCORE,0.3583138173302108,"similarity between human-related attributes, resulting in their dense clustering on the CLIP embedding.
146"
DIRECTIONAL CLIPSCORE,0.36065573770491804,"Figure 3 (a) visualizes it. To address this issue, we propose Directional CLIPScore (DCS), which
147"
DIRECTIONAL CLIPSCORE,0.3629976580796253,"leverages the centers of training images and predefined attribute texts on the CLIP embedding.
148"
DIRECTIONAL CLIPSCORE,0.36533957845433257,"Given training data, denoted as {x1, x2, x3, ...} ∈X, we define CX as the center of images and CT
149"
DIRECTIONAL CLIPSCORE,0.36768149882903983,"as another center of images for text attributes on the CLIP embedding, respectively. By using the
150"
DIRECTIONAL CLIPSCORE,0.3700234192037471,"image captioning model, BLIP[19], we define CT as the center of images in text respect;
151"
DIRECTIONAL CLIPSCORE,0.37236533957845436,"CX = 1 N N
X"
DIRECTIONAL CLIPSCORE,0.3747072599531616,"i=1
EI(xi),
CT = 1 N N
X"
DIRECTIONAL CLIPSCORE,0.3770491803278688,"i=1
ET(BLIP(xi)).
(2)"
DIRECTIONAL CLIPSCORE,0.3793911007025761,"(a) CLIPScore
(b) Directional CLIPScore"
DIRECTIONAL CLIPSCORE,0.38173302107728335,"ܥ̶ܵ̶ ൌͳͲͲ כ  ߠଵൌʹͻǤ͵
ܥ̶ܵ̶ ൌͳͲͲ כ  ߠଶൌʹͳǤͳ"
DIRECTIONAL CLIPSCORE,0.3840749414519906,"ܦܥ̶ܵ̶ ൌͳͲͲ כ  ߠଵൌͺǤʹ
ܦܥ̶ܵ̶ ൌͳͲͲ כ  ߠଵൌെ͹Ǥ
𝐷𝐶𝑆""makeup"" = 100 ∗cos 𝜃! = 8.2
𝐷𝐶𝑆""mustache"" = 100 ∗cos 𝜃"" = −7.6
𝐶𝑆""makeup"" = 100 ∗cos 𝜃! = 29.3
𝐶𝑆""mustache"" = 100 ∗cos 𝜃"" = 21.1"
DIRECTIONAL CLIPSCORE,0.3864168618266979,"(b) Directional CLIPScore
(a) CLIPScore"
DIRECTIONAL CLIPSCORE,0.38875878220140514,"Figure 3: Illustration of CLIPScore and Directional CLIPScore. (a) CLIPScore measures the
similarity between vectors with coordinate origin. (b) Directional CLIPScore measures the similarity
between vectors with a defined mean of the images, CX , as the origin. In the figure, we illustrate CX
and CT as the same point for ease of clarity and comprehension."
DIRECTIONAL CLIPSCORE,0.3911007025761124,Table 1: CLIPSCore and Directional CLIPScore’s mean accuracy on CelebA dataset.
DIRECTIONAL CLIPSCORE,0.39344262295081966,"All attributes
Refined attributes
CLIPScore
Directional CLIPScore
CLIPScore
DirectionalCLIPScore
mean accuracy
0.395
0.409
0.501
0.530"
DIRECTIONAL CLIPSCORE,0.3957845433255269,"These centers serve as reference points in the embedding space and aid more accurate attribute
152"
DIRECTIONAL CLIPSCORE,0.3981264637002342,"scores. We define DCS as the measure of similarity between two directions, Vx and Va where a set
153"
DIRECTIONAL CLIPSCORE,0.40046838407494145,"of attributes defined as {a1, a2, a3, ...} ∈A. The first direction spans from the center of the image
154"
DIRECTIONAL CLIPSCORE,0.4028103044496487,"to the image itself, and the second direction extends from the center of the attributes to the desired
155"
DIRECTIONAL CLIPSCORE,0.405152224824356,"attribute.
156"
DIRECTIONAL CLIPSCORE,0.40749414519906324,"Vx = EI(x) −CX ,
Va = ET(a) −CT ,
(3) 157"
DIRECTIONAL CLIPSCORE,0.4098360655737705,"DCS(x, a) = 100 ∗sim(Vx, Va),
(4)"
DIRECTIONAL CLIPSCORE,0.41217798594847777,"where sim(∗, ∗) is cosine similarity. For extending DCS from a single sample to data we denote the
158"
DIRECTIONAL CLIPSCORE,0.41451990632318503,"probability density function (PDF) of DCS(xi, ai) for all xi ∈X as DCSX (ai) for brevity.
159"
DIRECTIONAL CLIPSCORE,0.4168618266978923,"Figure 3 visually illustrates the distinction between DCS (Directional CLIPScore) and CS (CLIP-
160"
DIRECTIONAL CLIPSCORE,0.41920374707259955,"Score). Unlike CS, which lacks a clear reference point, DCS is based on the center, enabling the
161"
DIRECTIONAL CLIPSCORE,0.4215456674473068,"determination of attribute magnitudes relative to a zero point. Furthermore, DCS exhibits superior
162"
DIRECTIONAL CLIPSCORE,0.4238875878220141,"accuracy compared to CS, as demonstrated in Table 1. The table presents the accuracy results of CS
163"
DIRECTIONAL CLIPSCORE,0.4262295081967213,"and DCS for annotated attributes in CelebA[20]. By evaluating how well positive samples with the
164"
DIRECTIONAL CLIPSCORE,0.42857142857142855,"highest score align with positive samples for a given attribute, DCS consistently outperforms CS
165"
DIRECTIONAL CLIPSCORE,0.4309133489461358,"in accuracy. Notably, this trend remains consistent across refined attributes, which are removed for
166"
DIRECTIONAL CLIPSCORE,0.4332552693208431,"subjective attributes such as ""Attractive"" or ""Blurry"".
167"
ATTRIBUTE SELECTION METHODOLOGIES,0.43559718969555034,"3.3
attribute selection methodologies
168"
ATTRIBUTE SELECTION METHODOLOGIES,0.4379391100702576,"Our evaluation metric for measuring the performance of the generator is dependent on the attributes we
169"
ATTRIBUTE SELECTION METHODOLOGIES,0.44028103044496486,"choose to measure. To explore how to choose attributes that accurately reflect generator performance,
170"
ATTRIBUTE SELECTION METHODOLOGIES,0.4426229508196721,"we introduce three methods for attribute selection.
171"
ATTRIBUTE SELECTION METHODOLOGIES,0.4449648711943794,"BLIP extracted attribute
We aim to identify and quantify the attributes present in the training
172"
ATTRIBUTE SELECTION METHODOLOGIES,0.44730679156908665,"data from image descriptions. We can determine which attributes are most commonly occurring in
173"
ATTRIBUTE SELECTION METHODOLOGIES,0.4496487119437939,"the training data by counting attributes that appear in the training data. We use the image captioning
174"
ATTRIBUTE SELECTION METHODOLOGIES,0.4519906323185012,"model, BLIP[19], to extract attribute-related words from training data. We use N attributes that
175"
ATTRIBUTE SELECTION METHODOLOGIES,0.45433255269320844,"appear frequently in the training data as a set of attributes A for our proposed metric.
176"
ATTRIBUTE SELECTION METHODOLOGIES,0.4566744730679157,"User annotation
Another option for attribute selection is to use a set of human-annotated attributes.
177"
ATTRIBUTE SELECTION METHODOLOGIES,0.45901639344262296,"By explicitly assigning attributes for evaluating generative models, users can fairly compare the
178"
ATTRIBUTE SELECTION METHODOLOGIES,0.4613583138173302,"impact of each attribute score or focus on specific attributes. Especially, the CelebA dataset provides
179"
ATTRIBUTE SELECTION METHODOLOGIES,0.4637002341920375,"40 binary attributes about the human face domain, which can be used to evaluate a wide range of
180"
ATTRIBUTE SELECTION METHODOLOGIES,0.46604215456674475,"(generated) human image sets.
181"
ATTRIBUTE SELECTION METHODOLOGIES,0.468384074941452,"GPT attributes
We leveraged the power of GPT-3[1] to extract attributes. Through repetitive
182"
ATTRIBUTE SELECTION METHODOLOGIES,0.4707259953161593,"questioning, such as ‘Give me 50 words of useful visual attributes for distinguishing faces in a
183"
ATTRIBUTE SELECTION METHODOLOGIES,0.47306791569086654,"photo’ and ‘Give me 50 words of useful visual attributes for discerning variations in facial features to
184"
ATTRIBUTE SELECTION METHODOLOGIES,0.47540983606557374,"identify people in images,’ we obtained a set of attributes, which frequently appeared in the responses
185"
ATTRIBUTE SELECTION METHODOLOGIES,0.477751756440281,"across different datasets. The list of questions posed to GPT-3 can be found in the Appendix, and we
186"
ATTRIBUTE SELECTION METHODOLOGIES,0.48009367681498827,"followed the questioning methodology outlined in [21].
187"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.48243559718969553,"4
Evaluation Metric with Interpretable Attribute-Driven Embedding
188"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.4847775175644028,"In this section, by leveraging the knowledge of attribute intensities, we have developed two un-
189"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.48711943793911006,"derstandable metrics. In Section 4.1, we present Single attribute KL Divergence (SaKLD), which
190"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.4894613583138173,"measures the distance of attribute distributions between training data and generated images. In Section
191"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.4918032786885246,"4.2, we introduce Paired attribute KL divergence (PaKLD), a metric that assesses the relationship of
192"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.49414519906323184,"attributes.
193"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.4964871194379391,"4.1
Single attribute KL Divergence (SaKLD)
194"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.49882903981264637,"We design SaKLD to distinguish a good generative model which produces the same quantity of each
195"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5011709601873536,"attribute present in the training data. For example, if 50,000 training data contains 3,000 images with
196"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5035128805620609,"eyeglasses, the model should generate exactly 3,000 images with eyeglasses. Any deviation from this
197"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5058548009367682,"ideal distribution is considered undesirable. We introduce a new metric that quantifies density of each
198"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5081967213114754,"attribute in dataset by utilizing interpretable embedding. Our metric, SaKLD, quantifies the difference
199"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5105386416861827,"in density for each attribute between the training dataset (X) and the set of generated images (Y).
200"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5128805620608899,"We define SaKLD as
201"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5152224824355972,"SaKLD(X, Y) = 1 N N
X"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5175644028103045,"i
KL(DCSX (ai), DCSY(ai)),
(5)"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5199063231850117,"where i denotes an index for each attribute, N is the number of attributes, KL(*) is Kullback-Leibler
202"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.522248243559719,"Divergence, and note that we denote the PDF of DCS(xi, ai) for all xi ∈X as DCSX (ai).
203"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5245901639344263,"We compare the PDFs of Directional CLIPScore for each attribute in X and Y. The DCS PDF for
204"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5269320843091335,"each attribute in X and Y represent the distribution of the amount of that attribute in the respective
205"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5292740046838408,"sets. If the distribution of the amount of a specific attribute in X and Y is similar, the DCS distri-
206"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.531615925058548,"bution will also be similar, and the PDFs of the two sets will be close. We used Kullback-Leibler
207"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5339578454332553,"Divergence(KLD) to compare the each Directional CLIPScore PDFs for their attribute in X and Y, to
208"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5362997658079626,"quantify the extent to which the generator has created too few or too many instances of a specific
209"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5386416861826698,"attribute. We then calculate the average KLD value between the PDFs of each attribute in X and Y to
210"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5409836065573771,"obtain the final value of SaKLD.
211"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5433255269320844,"4.2
Paired attribute KL Divergence (PaKLD)
212"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5456674473067916,"We design another metric, PaKLD for examining that generated images preserve the attribute re-
213"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5480093676814989,"lationships present in training data. The model should generate images that adhere to the attribute
214"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.550351288056206,"relationships observed in the training data. For instance, if all 50,000 male images in the training data
215"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5526932084309133,"wear glasses, then all generated male images should also wear glasses. To evaluate the preservation
216"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5550351288056206,"of attribute relationships, we compare the difference in the joint probability density distribution
217"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5573770491803278,"of attribute pairs between training data. Our proposed metric, Pairwise Attribute KL Divergence
218"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5597189695550351,"(PaKLD), is defined with joint probability density functions as follows:
219"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5620608899297423,"PaKLD(X, Y) = 1 M M
X"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5644028103044496,"(i,j)
KL(DCSX (ai,j), DCSY(ai,j)),
(6)"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5667447306791569,"where M = nP2, (i, j) denotes an index pair of attributes, and the pair of attributes’ joint PDF is
220"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5690866510538641,"denoted as DCSX (ai,j).
221"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5714285714285714,"Table 2: Validation of metrics by including correlated images. The first row shows metric scores
between two distinct subsets of the FFHQ dataset (30,000 images each). The rest rows show the
correlated-sample-injected-scores where only one of the subsets contains an additional 300 or 600
edited images. We examine the metric performance on (""man""-""makeup"") and (""man""-""bangs"")
correlated images. All results are average values for five random subset pairs."
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5737704918032787,"include edited images
SaKLD↓
PaKLD↓
FID↓
FIDCLIP↓
to one subset
BLIP
USER
GPT
BLIP
USER
GPT
not included
0.904
0.920
1.095
3.357
3.924
4.438
1.275
0.115
(""man""-""makeup"") 300
0.985
1.048
1.115
3.676
4.205
4.453
1.282
0.132
(""man""-""makeup"") 600
1.079
1.368
1.286
3.910
4.819
4.710
1.306
0.162
(""man""-""bangs"") 300
0.991
1.102
1.171
3.679
4.297
4.496
1.278
0.122
(""man""-""bangs"") 600
1.201
1.521
1.314
4.031
5.064
4.718
1.288
0.140"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5761124121779859,"PaKLD analyzes the performance of the model more comprehensively. For example, if the generator’s
222"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5784543325526932,"probability density function for the attribute pair (""makeup"", ""long hair"") significantly differs from
223"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5807962529274004,"that of the training data, we can infer that the generator does not preserve the (""makeup"", ""long hair"")
224"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5831381733021077,"relationship. PaKLD allows to quantify the degree of preservation of attribute relationships and
225"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.585480093676815,"measure quantitative entanglements between attributes that have not been considered in previous
226"
EVALUATION METRIC WITH INTERPRETABLE ATTRIBUTE-DRIVEN EMBEDDING,0.5878220140515222,"researches.
227"
EXPERIMENTS,0.5901639344262295,"5
Experiments
228"
EXPERIMENTS,0.5925058548009368,"Experimental details
To estimate the probability density function (PDF) of Directional CLIPScore
229"
EXPERIMENTS,0.594847775175644,"(DCS) in the training data and generated images, we use Gaussian kernel density estimation. We
230"
EXPERIMENTS,0.5971896955503513,"sample 10,000 points from each PDF to obtain a discretized distribution and use it to calculate SaKLD
231"
EXPERIMENTS,0.5995316159250585,"and PaKLD. In all experiments, we use a set of N = 20 attributes.
232"
EXPERIMENTS,0.6018735362997658,"5.1
Correlated Image Injection Experiment: Validating the Effectiveness of Our Metric
233"
EXPERIMENTS,0.6042154566744731,"In this subsection, we provide a carefully designed experiment to compare the proposed metrics with
234"
EXPERIMENTS,0.6065573770491803,"FID; we first create two non-overlapping subsets of 30,000 images from FFHQ and consider them as
235"
EXPERIMENTS,0.6088992974238876,"training data X and generated images Y, respectively. We then compare the scores for all metrics
236"
EXPERIMENTS,0.6112412177985949,"after including the edited images in set Y. Specifically, we use DiffuseIT[16] to prepare two sets
237"
EXPERIMENTS,0.6135831381733021,"of edited images: ‘man’ with ‘makeup’ and ‘man’ with ‘bangs’. We use CelebA attributes for user
238"
EXPERIMENTS,0.6159250585480094,"annotation method (denoted by USER in Table 2).
239"
EXPERIMENTS,0.6182669789227166,"As shown in Table 2, our metrics and FID show consistent tendency: score increases when more
240"
EXPERIMENTS,0.6206088992974239,"edited images are included in imageset Y. Furthermore, thanks to the nature of focusing on the
241"
EXPERIMENTS,0.6229508196721312,"attributes of the image domain, our metrics show more obvious numerical differences compared to
242"
EXPERIMENTS,0.6252927400468384,"FID. These results demonstrate that SaKLD successfully captures the attribute distribution difference
243"
EXPERIMENTS,0.6276346604215457,"and PaKLD captures the joint distribution difference between attribute pairs. Basically, our three
244"
EXPERIMENTS,0.629976580796253,"attribute selection scenarios have similar tendencies across the two proposed metrics, but there are
245"
EXPERIMENTS,0.6323185011709602,"several differences. See supplement material for more details.
246"
NECESSITY OF PAKLD,0.6346604215456675,"5.2
Necessity of PaKLD
247"
NECESSITY OF PAKLD,0.6370023419203747,"We conducted another toy experiment, a scenario in which the SaKLD metric fails to detect a particular
248"
NECESSITY OF PAKLD,0.639344262295082,"attribute relationship, while PaKLD metric successfully identified it. We define the curated subsets
249"
NECESSITY OF PAKLD,0.6416861826697893,"of CelebA-HQ as training data and generated images with discrepancies in attribute relationship.
250"
NECESSITY OF PAKLD,0.6440281030444965,"Specifically, for training data, we collect 20,000 ‘smiling men’ images and 20,000 ‘non-smiling
251"
NECESSITY OF PAKLD,0.6463700234192038,"women’ images using ground truth labels of CelebA-HQ. Conversely, the generated images consist
252"
NECESSITY OF PAKLD,0.6487119437939111,"of 20,000 ‘non-smiling men’ and 20,000 ‘non-smiling women’. In this scenario, the PDFs of the
253"
NECESSITY OF PAKLD,0.6510538641686182,"‘man’, ‘woman’, and ‘smile’ attributes would not differ significantly between the two sets, and thus
254"
NECESSITY OF PAKLD,0.6533957845433255,"the SaKLD score would not capture it well. However, Paired attribute KL divergence would exhibit
255"
NECESSITY OF PAKLD,0.6557377049180327,"significant differences because the relationships between attributes within each set are completely
256"
NECESSITY OF PAKLD,0.65807962529274,"different.
257"
NECESSITY OF PAKLD,0.6604215456674473,"Figure 4 clearly illustrates the disparities in the evaluation results. While SaKLD score remained
258"
NECESSITY OF PAKLD,0.6627634660421545,"relatively unchanged for noteworthy attributes such as ‘man’, ‘woman’, and ‘smile’, the Paired
259 …"
NECESSITY OF PAKLD,0.6651053864168618,"(a) SaKLD
(b) PaKLD PaKLD SaKLD"
NECESSITY OF PAKLD,0.667447306791569,"Figure 4: Superiority of PaKLD. We define the curated subsets of CelebA-HQ as training data,
consisting of smiling men and non-smiling women, and generated images, consisting of non-smiling
men and smiling women. (a) The most influential attribute on SaKLD is not the attribute we manipu-
late. (b) The most influential attributes on PaKLD provides explicit insights into the contributions of
attribute pairs, such as (woman, smiling)."
NECESSITY OF PAKLD,0.6697892271662763,"Table 3: Comparing the performance of generative models. We computed each generative model’s
performance on our metric with their official pretrained checkpoints. For FFHQ[11] and LSUN
Cat[29], we used 50,000 images for both GT and generated set, and we used 1,336 and 50,000 images
for GT and generated set for MetFaces[13]. We used BLIP-extracted attributes for this experiment."
NECESSITY OF PAKLD,0.6721311475409836,"SaKLD↓
PaKLD↓
FFHQ
LSUN Cat
MetFaces
FFHQ
LSUN Cat
MetFaces
StyleGAN1[11]
9.902
74.626
-
19.431
119.456
-
StyleGAN2[13]
6.377
63.601
-
12.838
100.896
-
StyleGAN2-ADA[12]
14.118
-
40.769
21.930
-
87.118
StyleGAN3[14]
5.993
-
31.140
12.285
-
58.065
iDDPM [23]
-
110.229
-
-
136.579
-
iDDPM(P2) [2]
12.040
-
129.627
21.507
-
230.720"
NECESSITY OF PAKLD,0.6744730679156908,"attribute KL divergence score showed significant variations. This can be attributed to the distinct
260"
NECESSITY OF PAKLD,0.6768149882903981,"probability density functions (PDFs) of the ‘woman ∩smiling’. Note that we can easily understand
261"
NECESSITY OF PAKLD,0.6791569086651054,"the judgment factors; top attributes such as ‘woman ∩smiling’ and ‘man ∩smiling’ increase the
262"
NECESSITY OF PAKLD,0.6814988290398126,"score. These findings demonstrate the superior sensitivity and discernment of our proposed metrics,
263"
NECESSITY OF PAKLD,0.6838407494145199,"allowing for a more comprehensive evaluation of the generator’s generation ability.
264"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6861826697892272,"5.3
Comparing generative models including GANs and diffusion models with our methods
265"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6885245901639344,"Leveraging the superior sensitivity and discernment of our proposed metrics, we compare the
266"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6908665105386417,"performance of GANs and Diffusion Models (DMs) in Tables 3. Interestingly, there are two attractions;
267"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6932084309133489,"1) StyleGAN2-ADA shows the worst performance and 2) despite the respectable generative capability
268"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6955503512880562,"of DMs, iDDPM showed worse performance than StyleGAN models in all datasets.
269"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.6978922716627635,"The score of StyleGAN2-ADA implies that data augmentation for generative models may ruin
270"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7002341920374707,"attribute distribution in spite of FID’s superiority. Please refer to Appendix for an analysis. And we
271"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.702576112412178,"suppose that although there are many advantages of DMs, it is inferior to GANs in attribute-based
272"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7049180327868853,"analysis.
273"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7072599531615925,"To investigate the reason for the inferiority of DMs, we leverage the flexibility of constructing
274"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7096018735362998,"attributes to analyze the score changes according to the characteristics of attributes. We constructed
275"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.711943793911007,"attributes that focus only on color (e.g., ‘yellow fur’, ‘black fur’) and attributes that focus on shape
276"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7142857142857143,"(e.g., ‘pointy ears’, ‘long tail’) for LSUN Cat.
277"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7166276346604216,"Table 4 shows that iDDPM’s performance was particularly poor for color attributes. This is consistent
278"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7189695550351288,"with the assumption by Khrulkov et al. [15] that the encoder map of DMs coincides with the optimal
279"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7213114754098361,"transport map for common distributions; which means the pixel-based Euclidean distance corresponds
280"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7236533957845434,"to high–level texture and color–level similarity regardless of dataset and model. Therefore, the color
281"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7259953161592506,"Table 4: Computing performance of models with different attributes for LSUN Cat. Analyzing
the weakness of iDDPM for specific attribute types, such as color or shape. We used BLIP-extracted
attributes for this experiment."
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7283372365339579,"color attributes
shape attrbutes
SaKLD↓
PaKLD↓
SaKLD↓
PaKLD↓
StyleGAN1[11]
36.614
75.884
33.214
72.454
StyleGAN2[13]
36.621
67.518
34.642
68.954
iDDPM [23]
111.302
121.877
72.181
80.511 45 35 25 15"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7306791569086651,20            30            40
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7330210772833724,(b) number of attributes PaKLD
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7353629976580797,20            30            40 SaKLD
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7377049180327869,(a) number of samples
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7400468384074942,10k  20k  30k  40k  50k
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7423887587822015,"SaKLD
PaKLD"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7447306791569087,10k  20k  30k  40k  50k 6.0 6.5 7.5 7.0 8.0 12.0 13.5 16.5 15.0 18.0 8 12 20 16 24 16 24 40 32 48
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.747072599531616,"Figure 5: (a) The effect of sample size on our metric. Proposed metrics started to stabilize when
using more than 50,000 images. (b) The effect of the attribute counts on our metric. Although
depending on the characteristics of the additional attributes, the ranking of scores between models
can vary, the rank of the models mostly remained consistent regardless of the number of attributes."
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7494145199063232,"of the output images only depends on the initial latent noise xT , and the Monge optimal transport
282"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7517564402810304,"map between training data and the standard normal distribution. We conclude that the distribution of
283"
COMPARING GENERATIVE MODELS INCLUDING GANS AND DIFFUSION MODELS WITH OUR METHODS,0.7540983606557377,"color-related attributes is the inferiority of DMs.
284"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7564402810304449,"5.4
Impact of Sample Size and Attribute Count on Proposed Metric
285"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7587822014051522,"We provide ablation experiments to investigate the effect of a number of samples and attributes in
286"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7611241217798594,"Figure 5. We obtain generated images by StyleGAN3 from FFHQ with various random seeds. When
287"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7634660421545667,"the number of samples increases, SaKLD and PaKLD converge, especially more than 50,000 samples
288"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.765807962529274,"(Figure 5 (a)). We argue that the scores started to stabilize when using more than 50,000 images and
289"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7681498829039812,"note that we use 50,000 images for Tables 3 and 4. As for the number of attributes, we observe that
290"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7704918032786885,"the rank of the models mostly remained consistent regardless of the number of attributes. However,
291"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7728337236533958,"scores of DMs, purple line of Figure 5 (b), is increased as the number of attributes is increased
292"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.775175644028103,"because of color-related attributes. We argue that 20 attributes are sufficient, but more information
293"
IMPACT OF SAMPLE SIZE AND ATTRIBUTE COUNT ON PROPOSED METRIC,0.7775175644028103,"can be obtained by using more diverse cases. Please see Appendix for an analysis of each score.
294"
DISCUSSION AND CONCLUSION,0.7798594847775175,"6
Discussion and Conclusion
295"
DISCUSSION AND CONCLUSION,0.7822014051522248,"In this paper, we introduce a novel metric that not only assesses the performance of the generator
296"
DISCUSSION AND CONCLUSION,0.7845433255269321,"but also provides explicit explanations. Our proposed method, Directional CLIPScore, quantifies
297"
DISCUSSION AND CONCLUSION,0.7868852459016393,"the attributes captured in an image and aligns them close to human judgment. Leveraging the
298"
DISCUSSION AND CONCLUSION,0.7892271662763466,"interpretability of DCS, we propose two novel metrics, namely the SaKLD and PaKLD, which allow
299"
DISCUSSION AND CONCLUSION,0.7915690866510539,"us to compare attribute appearance frequencies and examine attribute relationships, respectively.
300"
DISCUSSION AND CONCLUSION,0.7939110070257611,"While our metrics offer comprehensive explanations, unreliable results may arise when the attributes
301"
DISCUSSION AND CONCLUSION,0.7962529274004684,"present in the images are ambiguous. For instance, in complex modern artworks with intricate color
302"
DISCUSSION AND CONCLUSION,0.7985948477751756,"patterns, extracting appropriate attributes becomes challenging or even impossible, rendering our
303"
DISCUSSION AND CONCLUSION,0.8009367681498829,"metric ineffective. Additionally, if the generative model’s ability is significantly poor, the same
304"
DISCUSSION AND CONCLUSION,0.8032786885245902,"limitation arises: measuring DCS from generated images becomes challenging.
305"
DISCUSSION AND CONCLUSION,0.8056206088992974,"Despite these limitations, our research establishes a solid foundation for the development of explain-
306"
DISCUSSION AND CONCLUSION,0.8079625292740047,"able evaluation metrics for generative models and contributes to the advancement of the field.
307"
REFERENCES,0.810304449648712,"References
308"
REFERENCES,0.8126463700234192,"[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
309"
REFERENCES,0.8149882903981265,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
310"
REFERENCES,0.8173302107728337,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
311"
REFERENCES,0.819672131147541,"[2] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh
312"
REFERENCES,0.8220140515222483,"Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF
313"
REFERENCES,0.8243559718969555,"Conference on Computer Vision and Pattern Recognition, pages 11472–11481, 2022.
314"
REFERENCES,0.8266978922716628,"[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
315"
REFERENCES,0.8290398126463701,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
316"
REFERENCES,0.8313817330210773,"recognition, pages 248–255. Ieee, 2009.
317"
REFERENCES,0.8337236533957846,"[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
318"
REFERENCES,0.8360655737704918,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
319"
REFERENCES,0.8384074941451991,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
320"
REFERENCES,0.8407494145199064,"arXiv:2010.11929, 2020.
321"
REFERENCES,0.8430913348946136,"[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
322"
REFERENCES,0.8454332552693209,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
323"
REFERENCES,0.8477751756440282,"of the ACM, 63(11):139–144, 2020.
324"
REFERENCES,0.8501170960187353,"[6] Jiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim, Jung-Woo Ha, and Jaesik Choi. Rarity
325"
REFERENCES,0.8524590163934426,"score: A new metric to evaluate the uncommonness of synthesized images. arXiv preprint
326"
REFERENCES,0.8548009367681498,"arXiv:2206.08549, 2022.
327"
REFERENCES,0.8571428571428571,"[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
328"
REFERENCES,0.8594847775175644,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
329"
REFERENCES,0.8618266978922716,"pages 770–778, 2016.
330"
REFERENCES,0.8641686182669789,"[8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A
331"
REFERENCES,0.8665105386416861,"reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.
332"
REFERENCES,0.8688524590163934,"[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
333"
REFERENCES,0.8711943793911007,"Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
334"
REFERENCES,0.8735362997658079,"neural information processing systems, 30, 2017.
335"
REFERENCES,0.8758782201405152,"[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
336"
REFERENCES,0.8782201405152225,"in Neural Information Processing Systems, 33:6840–6851, 2020.
337"
REFERENCES,0.8805620608899297,"[11] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
338"
REFERENCES,0.882903981264637,"adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and
339"
REFERENCES,0.8852459016393442,"pattern recognition, pages 4401–4410, 2019.
340"
REFERENCES,0.8875878220140515,"[12] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
341"
REFERENCES,0.8899297423887588,"Training generative adversarial networks with limited data. Advances in neural information
342"
REFERENCES,0.892271662763466,"processing systems, 33:12104–12114, 2020.
343"
REFERENCES,0.8946135831381733,"[13] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
344"
REFERENCES,0.8969555035128806,"Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF
345"
REFERENCES,0.8992974238875878,"conference on computer vision and pattern recognition, pages 8110–8119, 2020.
346"
REFERENCES,0.9016393442622951,"[14] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen,
347"
REFERENCES,0.9039812646370023,"and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information
348"
REFERENCES,0.9063231850117096,"Processing Systems, 34:852–863, 2021.
349"
REFERENCES,0.9086651053864169,"[15] Valentin Khrulkov, Gleb Ryzhakov, Andrei Chertkov, and Ivan Oseledets. Understanding ddpm
350"
REFERENCES,0.9110070257611241,"latent codes through optimal transport. arXiv preprint arXiv:2202.07477, 2022.
351"
REFERENCES,0.9133489461358314,"[16] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style
352"
REFERENCES,0.9156908665105387,"and content representation. arXiv preprint arXiv:2209.15264, 2022.
353"
REFERENCES,0.9180327868852459,"[17] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
354"
REFERENCES,0.9203747072599532,"precision and recall metric for assessing generative models. Advances in Neural Information
355"
REFERENCES,0.9227166276346604,"Processing Systems, 32, 2019.
356"
REFERENCES,0.9250585480093677,"[18] Tuomas Kynkäänniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role
357"
REFERENCES,0.927400468384075,"of imagenet classes in frechet inception distance. arXiv preprint arXiv:2203.06026, 2022.
358"
REFERENCES,0.9297423887587822,"[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-
359"
REFERENCES,0.9320843091334895,"training for unified vision-language understanding and generation. In International Conference
360"
REFERENCES,0.9344262295081968,"on Machine Learning, pages 12888–12900. PMLR, 2022.
361"
REFERENCES,0.936768149882904,"[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
362"
REFERENCES,0.9391100702576113,"wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
363"
REFERENCES,0.9414519906323185,"[21] Sachit Menon and Carl Vondrick. Visual classification via description from large language
364"
REFERENCES,0.9437939110070258,"models. arXiv preprint arXiv:2210.07183, 2022.
365"
REFERENCES,0.9461358313817331,"[22] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.
366"
REFERENCES,0.9484777517564403,"Reliable fidelity and diversity metrics for generative models. In International Conference on
367"
REFERENCES,0.9508196721311475,"Machine Learning, pages 7176–7185. PMLR, 2020.
368"
REFERENCES,0.9531615925058547,"[23] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
369"
REFERENCES,0.955503512880562,"models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021.
370"
REFERENCES,0.9578454332552693,"[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
371"
REFERENCES,0.9601873536299765,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
372"
REFERENCES,0.9625292740046838,"models from natural language supervision. In International conference on machine learning,
373"
REFERENCES,0.9648711943793911,"pages 8748–8763. PMLR, 2021.
374"
REFERENCES,0.9672131147540983,"[25] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. As-
375"
REFERENCES,0.9695550351288056,"sessing generative models via precision and recall. Advances in neural information processing
376"
REFERENCES,0.9718969555035128,"systems, 31, 2018.
377"
REFERENCES,0.9742388758782201,"[26] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
378"
REFERENCES,0.9765807962529274,"image recognition. arXiv preprint arXiv:1409.1556, 2014.
379"
REFERENCES,0.9789227166276346,"[27] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
380"
REFERENCES,0.9812646370023419,"Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
381"
REFERENCES,0.9836065573770492,"preprint arXiv:2011.13456, 2020.
382"
REFERENCES,0.9859484777517564,"[28] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
383"
REFERENCES,0.9882903981264637,"thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
384"
REFERENCES,0.990632318501171,"on computer vision and pattern recognition, pages 2818–2826, 2016.
385"
REFERENCES,0.9929742388758782,"[29] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
386"
REFERENCES,0.9953161592505855,"Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
387"
REFERENCES,0.9976580796252927,"preprint arXiv:1506.03365, 2015.
388"
