Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025380710659898475,"Modern successes of diffusion models in learning complex, high-dimensional data
distributions are attributed, in part, to their capability to construct diffusion pro-
cesses with analytic transition kernels and score functions. The tractability results
in a simulation-free framework with stable regression losses, from which reversed,
generative processes can be learned at scale. However, when data is conﬁned to a
constrained set as opposed to a standard Euclidean space, these desirable character-
istics appear to be lost based on prior attempts. In this work, we propose Mirror
Diffusion Models (MDM), a new class of diffusion models that generate data on
convex constrained sets without losing any tractability. This is achieved by learning
diffusion processes in a dual space constructed from a mirror map, which, crucially,
is a standard Euclidean space. We derive efﬁcient computation of mirror maps
for popular constrained sets, such as simplices and ℓ2-balls, showing signiﬁcantly
improved performance of MDM over existing methods. For safety and privacy
purposes, we also explore constrained sets as a new mechanism to embed invisible
but quantitative information (i.e., watermarks) in generated data, for which MDM
serves as a compelling approach. Our work brings new algorithmic opportunities
for learning tractable diffusion on complex domains."
INTRODUCTION,0.005076142131979695,"1
Introduction"
INTRODUCTION,0.007614213197969543,MDM: learns tractable diffusions in dual spaces
INTRODUCTION,0.01015228426395939,Corresponding nonlinear diffusions in ℳ
INTRODUCTION,0.012690355329949238,"Figure 1: Mirror Diffusion Models (MDM) is a new class of diffusion models for convex constrained
manifolds M ⊆Rd. (left) Instead of learning score-approximate diffusions on M, MDM applies
a mirror map ∇φ and learns tractable diffusions in its unconstrained dual-space ∇φ(M) = Rd.
(right) We also present MDM for watermarked generation, where generated contents (e.g., images)
live in a high-dimensional token constrained set M that is certiﬁable only from the private user."
INTRODUCTION,0.015228426395939087,"Diffusion models [1–3] have emerged as powerful generative models with their remarkable successes
in synthesizing high-ﬁdelity data such as images [4, 5], audio [6, 7], and 3D geometry [8, 9].
These models work by progressively diffusing data to noise, and learning the score functions (often"
INTRODUCTION,0.017766497461928935,†Equal advising.
INTRODUCTION,0.02030456852791878,"Table 1: Comparison of different diffusion models for constrained generation on M ⊆Rd. Rather
than learning reﬂected diffusions on M with approximate scores, our Mirror Diffusion constructs
a mirror map ∇φ and lift the diffusion processes to the unconstrained dual space ∇φ(M) = Rd,
inheriting favorable features from standard Euclidean-space diffusion models. Constraints are satisﬁed
by construction via the inverse map ∇φ∗; that is, ∇φ∗(y) ∈M for all y ∈∇φ(M)."
INTRODUCTION,0.02284263959390863,"Diffusion models
Domain
Tractable
conditional score
Simulation-free
training
Regression
objective"
INTRODUCTION,0.025380710659898477,"Reﬂected Diffusion
Fishman et al. [18]
M



Lou and Ermon [25]
M

△2
"
INTRODUCTION,0.027918781725888325,"Mirror Diffusion
This work (ours)
∇φ(M)


"
INTRODUCTION,0.030456852791878174,"parameterized by neural networks) to reverse the processes [10]; the reversed processes thereby
provide transport maps that generate data from noise. Modern diffusion models [11–13] often employ
diffusion processes whose transition kernels are analytically available. This characteristic enables
simulation-free training, bypassing the necessity to simulate the underlying diffusion processes
by directly sampling the diffused data. It also leads to tractable conditional score functions and
simple regression objectives, facilitating computational scalability for high-dimensional problems. In
standard Euclidean spaces, tractability can be accomplished by designing the diffusion processes as
linear stochastic differential equations, but doing so for non-Euclidean spaces is nontrivial."
INTRODUCTION,0.03299492385786802,"Recent progress of diffusion models has expanded their application to non-Euclidean spaces, such as
Riemannian manifolds [14, 15], where data live in a curved geometry. The development of generative
models on manifolds has enabled various new applications, such as modeling earth and climate
events [16] and learning densities on meshes [17]. In this work, we focus on generative modeling
on constrained sets (also called constrained manifolds [18]) that are deﬁned by a set of inequality
constraints. Such constrained sets, denoted by M ⊆Rd, are also ubiquitous in several scientiﬁc
ﬁelds such as computational statistics [19], biology [20], and robotics [21, 22]."
INTRODUCTION,0.03553299492385787,"Previous endeavors in the direction of constrained generation have primarily culminated in reﬂected
diffusions [23, 24], which reﬂect the direction at the boundary ∂M to ensure that samples remain
inside the constraints. Unfortunately, reﬂected diffusions do not possess closed-form transition
kernels [18, 25], thus necessitating the approximation of the conditional score functions that can
hinder learning performance. Although simulation-free training is still possible2, it comes with a
computational overhead due to the use of geometric techniques [25]. From a theoretical standpoint,
reﬂected Langevin dynamics are widely regarded for their extended mixing time [26, 27]. This
raises practical concerns when simulating reﬂected diffusions, as prior methods often adopted special
treatments such as thresholding and discretization [18, 25]."
INTRODUCTION,0.03807106598984772,"An alternative diffusion for constrained sampling is the Mirror Langevin Dynamics (MLD [28]).
MLD, as a subclass of Riemannian Langevin Dynamics [29, 30], is tailored speciﬁcally to convex
constrained sampling. Speciﬁcally, MLD constructs a strictly convex function φ so that its gradient
map, often referred to as the mirror map ∇φ : M →Rd, deﬁnes a nonlinear mapping from the initial
constrained set to an unconstrained dual space. Despite extensive investigation of MLD [31–34], its
potential as a foundation for designing diffusion generative models remains to be comprehensively
examined. Thus, there is a need to explore the potential of MLD for designing new, effective, and
tailored diffusion models for constrained generation."
INTRODUCTION,0.04060913705583756,"With this in mind, we propose Mirror Diffusion Models (MDM), a new class of diffusion generative
models for convex constrained sets. Similar to MLD, MDM utilizes mirror maps ∇φ that transform
data distributions on M to its dual space ∇φ(M). From there, MDM learns a dual-space diffusion
between the mirrored distribution and Gaussian prior in the dual space. Note that this is in contrast to
MLD, which constructs invariant distributions on the initial constrained set; MDM instead considers
dual-space priors. Since the dual space is also unconstrained Euclidean space ∇φ(M) = Rd, MDM"
INTRODUCTION,0.04314720812182741,2Lou and Ermon [25] proposed a semi-simulation-free training with the aid of geometric techniques.
INTRODUCTION,0.04568527918781726,"∇𝜙ℳ
User-defined tokens"
INTRODUCTION,0.048223350253807105,"ℳ≔𝑥∈ℝ"" ∶𝑐# ≤𝑎#"
INTRODUCTION,0.050761421319796954,"$𝑥≤𝑏#, ∀𝑖"
INTRODUCTION,0.0532994923857868,Constrained Manifold
INTRODUCTION,0.05583756345177665,External users
INTRODUCTION,0.0583756345177665,Pretrained (primal-space)
INTRODUCTION,0.06091370558375635,Diffusion Models
INTRODUCTION,0.06345177664974619,"Dual-Space 
Diffusion Models ℳ"
INTRODUCTION,0.06598984771573604,"Watermark
Dataset
∇𝜙ℳ ∗ ℝ"""
INTRODUCTION,0.06852791878172589,"Figure 2: MDM for watermarked generation: (left) We ﬁrst construct a constrained set M based on a
set of user-deﬁned tokens private to other users. (right) MDM can be instantiated by either learning
the corresponding dual-space diffusions, or projecting pretrained, i.e., unwatermarked, diffusion
models onto M. In both cases, MDM embeds watermarks that are certiﬁable only from the user."
INTRODUCTION,0.07106598984771574,"preserves many important features of standard diffusion models, such as simulation-free training and
tractable conditional scores, which yields simple regression objectives; see Table 1. After learning
the dual-space distribution, MDM applies the inverse mapping ∇φ∗, transforming mirrored samples
back to the constrained set. We propose efﬁcient construction of mirror maps and demonstrate that
MDM outperforms prior reﬂected diffusion models on common constrained sets, such as ℓ2-balls and
simplices."
INTRODUCTION,0.07360406091370558,"Moreover, we show that MDM also stands as a novel approach for watermarked generation, a
technique that aims to embed undetectable information (i.e., watermarks) into generated contents
to prevent unethical usage and ensure copyright protection. This newly emerging goal nevertheless
attracted signiﬁcant attention in natural language generation [35–37], while what we will present is a
general algorithmic strategy demonstrated for image generations."
INTRODUCTION,0.07614213197969544,"To generate watermarked contents that are private to individual users, we begin by constructing a
constrained set M based on user-deﬁned tokens (see Figure 2). MDM can then be instantiated to
learn the dual-space distribution. Alternatively, MDM can distill pretrained diffusion models [38] by
projecting the generated data onto the constraint set. In either case, MDM generates watermarked
contents that can be certiﬁed only by users who know the tokens. Our empirical results suggest that
MDM serves as a competitive approach to prior watermarking techniques for diffusion models, such
as those proposed by Zhao et al. [39], by achieving lower (hence better) FID values."
INTRODUCTION,0.07868020304568528,"In summary, we present the following contributions:"
INTRODUCTION,0.08121827411167512,"• We introduce Mirror Diffusion Models (MDM), a new class of diffusion models that
utilizes mirror maps to enable constrained generation with Euclidean-space diffusion models."
INTRODUCTION,0.08375634517766498,"• We propose efﬁcient computation of mirror maps on common constrained sets, including
balls and simplices. Our results demonstrate that MDM consistently outperforms previous
reﬂected diffusion models."
INTRODUCTION,0.08629441624365482,"• As a novel application of constrained generation, we explore MDM’s potential for water-
marking and copyrighting diffusion models where constraints serve as private tokens. We
show that it yields competitive performance compared to prior methods."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.08883248730964467,"2
Preliminary on Euclidean-space Diffusion Models"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09137055837563451,"Notation
M ⊆Rd denotes a convex constrained set. We preserve x ∈M and y ∈Rd to better
distinguish variables on constrained sets and standard d-dimensional Euclidean space. I ∈Rd×d
denotes the identity matrix. xi denotes the i-th element of a vector x. Similarly, [A]ij denotes the the
element of a matrix A at i-th row and j-th column."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09390862944162437,"Diffusion process
Given y0 ∈Rd sampled from some Euclidean-space distribution, conventional
diffusion models deﬁne the diffusion process as a forward Markov chain with the joint distribution:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09644670050761421,"q(y1:T |y0) = T
Y"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.09898477157360407,"t=1
q(yt|yt−1),
q(yt|yt−1) := N(yt;
p"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10152284263959391,"1 −βtyt−1, βtI),
(1)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10406091370558376,"which progressively injects Gaussian noise to y0 such that, for a sufﬁciently large T and a properly
chosen noise schedule βt ∈(0, 1), yT approaches an approximate Gaussian, i.e., q(yT ) ≈N(0, I)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1065989847715736,"Equation (1) has tractable marginal densities, and two of which that are of particular interest are:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.10913705583756345,"q(yt|y0) = N(yt; √¯αty0, (1 −αt)I),
q(yt−1|yt, y0) = N(yt−1; ˜µt(yt, y0), ˜βtI),
(2)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1116751269035533,"˜µt(yt, y0) :=
√¯αt−1βt"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.11421319796954314,"1 −¯αt
y0 +
√1 −βt(1 −¯αt−1)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.116751269035533,"1 −¯αt
yt,
˜βt := 1 −¯αt−1"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.11928934010152284,"1 −¯αt
βt,
¯αt := tY"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1218274111675127,"s=0
(1 −βs)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12436548223350254,"The tractable marginal q(yt|y0) enables direct sampling of yt without simulating the forward Markov
chain (1), i.e., yt|y0 can be sampled in a simulation-free manner. It also suggests a closed-form score
function ∇log q(yt|y0). The marginal q(yt−1|yt, y0) hints the optimal reverse process given y0."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12690355329949238,"Generative process
The generative process, which aims to reverse the forward diffusion process
(1), is deﬁned by a backward Markov chain with the joint distribution:"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.12944162436548223,"pθ(y0:T ) = pT (yT ) T
Y"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1319796954314721,"t=1
pθ(yt−1|yt),
pθ(yt−1|yt) := N(yt−1; µθ(yt, t), ˜βtI).
(3)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13451776649746192,"Here, the mean µθ is typically parameterized by some DNNs, e.g., U-net [40], with θ. Equations (1)
and (3) imply an evidence lower bound (ELBO) for maximum likelihood training, indicating that the
optimal prediction of µθ(yt, t) should match ˜µt(yt, y0) in Equation (2)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13705583756345177,"Parameterization and training
There exists many different ways to parameterize µθ(yt, t), and
each of them leads to a distinct training objective. For instance, Ho et al. [2] found that"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.13959390862944163,"µθ(yt, t) :=
1
√1 −βt"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14213197969543148,"
yt −
βt
√1 −¯αt
ϵθ(yt, t)

,
(4)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.1446700507614213,"achieves better empirical results compared to directly predicting y0 or ˜µt. This parameterization aims
to predict the “noise” injected to yt ∼q(yt|y0), as the ELBO reduces to a simple regression:
L(θ) := Et,y0,ϵ [λ(t)∥ϵ −ϵθ(yt, t)∥] .
(5)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14720812182741116,"where yt = √¯αty0 + √1 −αtϵ, ϵ ∼N(0, I), and t sampled uniformly from {1, · · · , T}. The
weighting λ(t) is often set to 1. Once ϵθ is properly trained, we can generate samples in Euclidean
space by simulating Equation (3) backward from yT ∼N(0, I)."
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.14974619289340102,"3
Mirror Diffusion Models (MDM)"
PRELIMINARY ON EUCLIDEAN-SPACE DIFFUSION MODELS,0.15228426395939088,"In this section, we present a novel class of diffusion models, Mirror Diffusion Models, which are
designed to model probability distributions pdata(x) supported on a convex constrained set M ⊆Rd
while retraining the computational advantages of Euclidean-space diffusion models."
DUAL-SPACE DIFFUSION,0.1548223350253807,"3.1
Dual-Space Diffusion"
DUAL-SPACE DIFFUSION,0.15736040609137056,"Mirror map
Following the terminology in Li et al. [34], let φ : M →R be a twice differentiable3
function that is strictly convex and satisfying that limx→∂M ∥∇φ(x)∥→∞and ∇φ(M) = Rd. We
call its gradient map ∇φ : M →Rd the mirror map and its image, ∇φ(M) = Rd, as its dual/mirror
space. Let φ∗: Rd →R be the dual function of φ deﬁned by
φ∗(y) = sup
x∈M
⟨x, y⟩−φ(x)
(6)"
DUAL-SPACE DIFFUSION,0.1598984771573604,"A notable property of this dual function φ∗(y) is that its gradient map reverses the mirror map. In
other words, we have ∇φ∗= (∇φ)−1, implying that"
DUAL-SPACE DIFFUSION,0.16243654822335024,"∇φ∗(∇φ(x)) = x for all x ∈M,
∇φ(∇φ∗(y)) = y for all y ∈∇φ(M) = Rd."
DUAL-SPACE DIFFUSION,0.1649746192893401,"The functions (∇φ, ∇φ∗) deﬁne a nonlinear, yet bijective, mapping between the convex constrained
set M and Euclidean space Rd. As a result, MDM does not require building diffusion models on M,
as done in previous approaches, but rather they learn a standard Euclidean-space diffusion model in
the corresponding dual space ∇φ(M) = Rd."
DUAL-SPACE DIFFUSION,0.16751269035532995,"3We note that standard mirror maps require twice differentiability to induce a (Riemannian) metric on
the mirror space for constructing MLD. For our MDM, however, twice differentiability is needed only for
Equation (8), and continuous differentiability, i.e., C1, would sufﬁce for training. 1
0
1 1 0 1 Ball 3
0
3 3 0 3"
DUAL-SPACE DIFFUSION,0.1700507614213198,"Ball 
(
) 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.8 0.6 0.4 0.2 0.0"
DUAL-SPACE DIFFUSION,0.17258883248730963,"0.0
0.2
0.4
0.6
0.8
1.0"
DUAL-SPACE DIFFUSION,0.1751269035532995,Simplex
DUAL-SPACE DIFFUSION,0.17766497461928935,"3
0
3
2 1 4 7"
DUAL-SPACE DIFFUSION,0.1802030456852792,"Simplex 
(
)"
DUAL-SPACE DIFFUSION,0.18274111675126903,"Figure 3: Examples of how mirror maps ∇φ pushforward constrained distributions to unconstrained
ones, for (left) an ℓ2-ball where M := {x ∈R2 : ∥x∥2
2 < R} and (right) a simplex ∆3 where
M := {x ∈R2 : P2
i=1 xi ≤1, xi ≥0}. Note that for simplex, x3 = 1 −x1 −x2 is a redundant
coordinate; see Section 3.2 for more details."
DUAL-SPACE DIFFUSION,0.18527918781725888,"Essentially, the goal of MDM is to model the dual-space distribution:
˜pdata(y) := ([∇φ♯]pdata)(x),
(7)
where ♯is the push-forward operation and y = ∇φ(x) is the mirror of data x ∈M."
DUAL-SPACE DIFFUSION,0.18781725888324874,"Training and generation
MDM follows the same training procedure of Euclidean-space diffusion
models, except with an additional push-forward given by Equation (7). Similarly, its generation pro-
cedure includes an additional step that maps the dual-space samples y0, generated from Equation (3),
back to the constrained set via ∇φ∗(y0) ∈M."
DUAL-SPACE DIFFUSION,0.19035532994923857,"Tractable variational bound
Given a datapoint x ∈M, its mirror y0 = ∇φ(x) ∈Rd, and the
dual-space Markov chain {y1, · · · , yT }, the ELBO of MDM can be computed by"
DUAL-SPACE DIFFUSION,0.19289340101522842,"LELBO(x) := log | det ∇2φ∗(y0)| + ˜LELBO(y0),
(8)
where we apply the change of variables theorem [41] w.r.t. the mapping x = ∇φ∗(y0). The
dual-space ELBO ˜LELBO(y0) shares the same formula with the Euclidean-space ELBO [42, 43],"
DUAL-SPACE DIFFUSION,0.19543147208121828,˜LELBO(y0) := DKL(q(yT |y0) || p(yT )) +
DUAL-SPACE DIFFUSION,0.19796954314720813,"T −1
X"
DUAL-SPACE DIFFUSION,0.20050761421319796,"t=1
DKL(q(yt|yt+1, y0) || pθ(yt|yt+1)) −log pθ(y0|y1)."
DUAL-SPACE DIFFUSION,0.20304568527918782,"(9)
It should be noted that (8,9) provide a tractable variational bound for constrained generation. This
stands in contrast to concurrent methods relying on reﬂected diffusions, whose ELBO entails either
intractable [18] or approximate [25] scores that could be restricted when dealing with more complex
constrained sets. While, in practice, MDM is trained using the regression objective in Equation (5),
computing tractable ELBO might be valuable for independent purposes such as likelihood estimation."
DUAL-SPACE DIFFUSION,0.20558375634517767,"3.2
Efﬁcient Computation of Mirror Maps"
DUAL-SPACE DIFFUSION,0.20812182741116753,"What remains to be answered pertains to the construction of mirror maps for common constraint sets
such as ℓ2-balls, simplices, polytopes, and their products. In particular, we seek efﬁcient, preferably
closed-form, computation for ∇φ, ∇φ∗and ∇2φ∗."
DUAL-SPACE DIFFUSION,0.21065989847715735,"ℓ2-Ball
Let M := {x ∈Rd : ∥x∥2
2 < R} denote the ℓ2-ball of radius R in Rd. We consider the
common log-barrier function:
φball(x) := −γ log(R −∥x∥2
2).
(10)"
DUAL-SPACE DIFFUSION,0.2131979695431472,"where γ ∈R+. The mirror map, its inverse, and the Hessian can be computed analytically by"
DUAL-SPACE DIFFUSION,0.21573604060913706,"∇φball(x) =
2γx
R −∥x∥2
2
,
∇φ∗
ball(y) =
Ry
p"
DUAL-SPACE DIFFUSION,0.2182741116751269,"R∥y∥2
2 + γ2 + γ
,"
DUAL-SPACE DIFFUSION,0.22081218274111675,"∇2φ∗
ball(y) =
R R
p"
DUAL-SPACE DIFFUSION,0.2233502538071066,"∥y∥2
2 + γ2 + γ "
DUAL-SPACE DIFFUSION,0.22588832487309646,"I −
R

R
p"
DUAL-SPACE DIFFUSION,0.22842639593908629,"∥y∥2
2 + γ2 + γ
 p"
DUAL-SPACE DIFFUSION,0.23096446700507614,"R∥y∥2
2 + γ2 yy⊤ "
DUAL-SPACE DIFFUSION,0.233502538071066,".
(11)"
DUAL-SPACE DIFFUSION,0.23604060913705585,"We refer to Appendix A for detailed derivation. Figure 3 visualizes such a mirror map for an ℓ2-ball in
2D. It is worth noting that while the mirror map of log-barriers generally does not admit an analytical
inverse, this is not the case for the ℓ2-ball constraints."
DUAL-SPACE DIFFUSION,0.23857868020304568,"Simplex
Given a (d+1)-dimensional simplex, ∆d+1 := {x ∈Rd+1 : Pd+1
i=1 xi = 1, xi ≥0}, we
follow standard practices [18, 25] by constructing the constrained set M := {x ∈Rd : Pd
i=1 xi ≤
1, xi ≥0} and deﬁne xd+1 := 1 −Pd
i=1 xi.4"
DUAL-SPACE DIFFUSION,0.24111675126903553,"While conventional log-barriers may remain a viable option, a preferred alternative that is widely
embraced in the MLD literature for enforcing simplices [28, 44] is the entropic function [45, 46]:"
DUAL-SPACE DIFFUSION,0.2436548223350254,"φsimplex(x) := d
X"
DUAL-SPACE DIFFUSION,0.24619289340101522,"i=1
xi log(xi) +  1 − d
X"
DUAL-SPACE DIFFUSION,0.24873096446700507,"i=1
xi ! log  1 − d
X"
DUAL-SPACE DIFFUSION,0.2512690355329949,"i=1
xi !"
DUAL-SPACE DIFFUSION,0.25380710659898476,".
(12)"
DUAL-SPACE DIFFUSION,0.2563451776649746,"The mirror map, its inverse, and the hessian of the dual function can be computed analytically by"
DUAL-SPACE DIFFUSION,0.25888324873096447,"[∇φsimplex(x)]i = log xi −log  1 − d
X"
DUAL-SPACE DIFFUSION,0.2614213197969543,"i=1
xi !"
DUAL-SPACE DIFFUSION,0.2639593908629442,",
[∇φ∗
simplex(y)]i =
eyi"
DUAL-SPACE DIFFUSION,0.26649746192893403,"1 + Pd
i=1 eyi ,"
DUAL-SPACE DIFFUSION,0.26903553299492383,"[∇2φ∗
simplex(y)]ij =
eyi"
DUAL-SPACE DIFFUSION,0.2715736040609137,"1 + Pd
i=1 eyi I(i = j) −
eyieyj

1 + Pd
i=1 eyi
2 ,
(13)"
DUAL-SPACE DIFFUSION,0.27411167512690354,"where I(·) is an indicator function. Again, we leave the derivation of Equation (13) to Appendix A.
Figure 3 visualizes the entropic mirror map for a 3-dimensional simplex ∆3."
DUAL-SPACE DIFFUSION,0.2766497461928934,"Polytope
Let the constrained set be M := {x ∈Rd : ci < a⊤
i x < bi, ∀i ∈{1, · · · , m}}, where
ci, bi ∈R and ai ∈Rd are linearly independent to each other. We consider the standard log-barrier:"
DUAL-SPACE DIFFUSION,0.27918781725888325,"φpolytope(x) := − m
X"
DUAL-SPACE DIFFUSION,0.2817258883248731,"i=1
(log (⟨ai, x⟩−ci) + log (bi −⟨ai, x⟩)) + d
X j=m+1"
DUAL-SPACE DIFFUSION,0.28426395939086296,"1
2⟨aj, x⟩2,
(14)"
DUAL-SPACE DIFFUSION,0.2868020304568528,"∇φpolytope(x) = m
X"
DUAL-SPACE DIFFUSION,0.2893401015228426,"i=1
si(⟨ai, x⟩)ai + d
X"
DUAL-SPACE DIFFUSION,0.2918781725888325,"j=m+1
⟨aj, x⟩aj,
(15)"
DUAL-SPACE DIFFUSION,0.29441624365482233,"2
0
2
100 0 100"
DUAL-SPACE DIFFUSION,0.2969543147208122,"si( ai, x )"
DUAL-SPACE DIFFUSION,0.29949238578680204,"Log-barrier 2
0
2 2.5 0.0 2.5"
DUAL-SPACE DIFFUSION,0.3020304568527919,"Hyperbolic 
 Tangent in Eq.(18)"
DUAL-SPACE DIFFUSION,0.30456852791878175,"Figure 4: Comparison between si
induced by standard log-barriers vs.
hyperbolic tangents in Eq. (18)."
DUAL-SPACE DIFFUSION,0.30710659898477155,"where the monotonic function si : (ci, bi) →R is given by
si =
−1
⟨ai,x⟩−ci +
−1
⟨ai,x⟩−bi , and {aj} do not impose any con-
straints and can be chosen arbitrarily as long as they span Rd
with {ai} (see below for more details); hence ∇φpolytope(M) =
Rd. While its inverse, ∇φ∗
polytope, does not admit closed-form
in general, it does when all d constraints are orthonormal:"
DUAL-SPACE DIFFUSION,0.3096446700507614,"∇φ∗
polytope(y) = m
X"
DUAL-SPACE DIFFUSION,0.31218274111675126,"i=1
s−1
i (⟨ai, y⟩)ai + d
X"
DUAL-SPACE DIFFUSION,0.3147208121827411,"j=m+1
⟨aj, y⟩aj. (16)"
DUAL-SPACE DIFFUSION,0.31725888324873097,"Here, s−1
i
: R →(ci, bi) is the inverse of si. Essentially, (15,16) manipulate the coefﬁcients of the
bases from which the polytope set is constructed. These manipulations are nonlinear yet bijective,
deﬁned uniquely by si and s−1
i , for each orthonormal basis ai or, equivalently, for each constraint."
DUAL-SPACE DIFFUSION,0.3197969543147208,"Naively implementing the mirror map via (15,16) can be problematic due to (i) numerical instability
of si at boundaries (see Figure 4), (ii) the lack of closed-form solution for its inverse s−1
i , and (iii) an
one-time computation of orthonormal bases, which scales as O(d3) for orthonormalization methods
such as Gram-Schmidt, Householder or Givens [e.g., 47]. Below, we devise a modiﬁcation for
efﬁcient and numerically stable computation, which will be later adopted for watermarked generation."
DUAL-SPACE DIFFUSION,0.3223350253807107,"Improved computation for Polytope (15,16)
Given the fact that only the ﬁrst m orthonormal
bases {a1, · · · , am} matter in the computation of (15,16), as the remaining (d −m) orthonormal
bases merely involve orthonormal projections, we can simplify the computational (15,16) to"
DUAL-SPACE DIFFUSION,0.3248730964467005,"∇φpoly(x) = x + m
X"
DUAL-SPACE DIFFUSION,0.32741116751269034,"i=1
(si(⟨ai, x⟩) −⟨ai, x⟩) ai, ∇φ∗
poly(y) = y + m
X i=1"
DUAL-SPACE DIFFUSION,0.3299492385786802," 
s−1
i (⟨ai, y⟩) −⟨ai, y⟩

ai, (17)"
DUAL-SPACE DIFFUSION,0.33248730964467005,4The transformation allows x to be bounded in a constrained set M ⊆Rd rather than a hyperplane in Rd+1.
DUAL-SPACE DIFFUSION,0.3350253807106599,"which, intuitively, add and subtract the coefﬁcients of the ﬁrst m orthonormal bases while leaving
the rest intact. This reduces the complexity of computing orthonormal bases to O(md2) ≈O(d2)
and improves the numerical accuracy of inner-product multiplication, which can be essential for
high-dimensional applications when d is large."
DUAL-SPACE DIFFUSION,0.33756345177664976,"To address the instability of si and intractability of s−1
i
for log-barriers, we re-interpret the mirror
maps (15,16) as the changes of coefﬁcient bases. As this implies that any nonlinear bijective mapping
with a tractable inverse would sufﬁce, we propose a rescaling of the hyperbolic tangent:"
DUAL-SPACE DIFFUSION,0.3401015228426396,"si(⟨ai, x⟩) :=
 
tanh−1 ◦rescalei

(⟨ai, x⟩) ,
s−1
i (⟨ai, y⟩) :=
 
rescale−1
i
◦tanh

(⟨ai, y⟩) , (18)"
DUAL-SPACE DIFFUSION,0.3426395939086294,"Table 2: Complexity of ∇φ and
∇φ∗for each constrained set."
DUAL-SPACE DIFFUSION,0.34517766497461927,"ℓ2-Ball
Simplex
Polytope"
DUAL-SPACE DIFFUSION,0.3477157360406091,"O(d)
O(d)
O(md)"
DUAL-SPACE DIFFUSION,0.350253807106599,"where rescalei(z) := 2
z−ci
(bi−ci) −1 rescales the range from
(ci, bi) to (−1, 1). It is clear from Figure 4 that, compared to
the si induced by the log-barrier, the mapping (18) is numerical
stable at the boundaries and admits tractable inverse."
DUAL-SPACE DIFFUSION,0.35279187817258884,"Complexity
Table 2 summarizes the complexity of the mirror
maps for each constrained set. We note that all computations
are parallelizable, inducing nearly no computational overhead.
Remark 1. Equations (17) and (18) can be generalized to non-orthonormal constraints by adopting
⟨˜ai, x⟩and ⟨˜ai, y⟩, where ˜ai is the i-th row of the matrix ˜
A := (A⊤A)−1A⊤and A := [a1, · · · , am].
Indeed, when A is orthonormal, we recover ˜ai = ai. We leave more discussions to Appendix B."
RELATED WORK,0.3553299492385787,"4
Related Work"
RELATED WORK,0.35786802030456855,"Constrained sampling
Sampling from a probability distribution on a constrained set has been a
long-standing problem not only due to its extensive applications in, e.g., topic modeling [48, 49] and
Bayesian inference [50, 51], but its unique challenges in non-asymptotic analysis and algorithmic
design [28, 32–34, 52, 53]. Mirror Langevin is a special instance of endowing Langevin dynamics
with a Riemannian metric [30, 54], and it chooses a speciﬁc reshaped geometry so that dynamics
have to travel inﬁnite distance in order to cross the boundary of the constrained set. It is a sampling
generalization of the celebrated Mirror Descent Algorithm [45] for convex constrained optimization,
and the convexity of constrained set leads to the existence of a mirror map. However, Mirror Langevin
is designed to draw samples from an unnormalized density supported on a constrained set. This
stands in contrast to our MDM, which instead tackles problems of constrained generation, where only
samples, not unnormalized density, are available, sharing much similarity to conventional generative
modeling [3, 55]. In addition, Mirror Langevin can be understood to go back and forth between
primal (constrained) and mirror (unconstrained) spaces, while MDM only does one such round trip."
RELATED WORK,0.3604060913705584,"Diffusion Models in non-Euclidean spaces
There has been growing interest in developing diffu-
sion generative models that can operate on domains that are not limited to standard Euclidean spaces,
e.g., discrete domains [56, 57] and equality constraints [58]. Seminar works by De Bortoli et al. [14]
and Huang et al. [15] have introduced diffusion models on Riemannian manifolds, which have shown
promising results in, e.g., biochemistry [59, 60] and rotational groups [61, 62]. Regarding diffusion
models on constrained manifolds, most concurrent works consider similar convex constrained sets
such as simplices and polytopes [18, 25, 63]. Our MDM works on the same classes of constraints and
includes additionally ℓ2-ball, which is prevalent in social science domains such as opinion dynamics
[64, 65]. It is also worth mentioning that our MDM may be conceptually similar to Simplex Diffusion
(SD) [63] which reconstructs samples on simplices similar to the mirror mapping. However, SD is
designed speciﬁcally for simplices and Dirichlet distributions, while MDM is applicable to a broader
class of convex constrained sets. Additionally, SD adopts Cox-Ingersoll-Ross processes [66], yielding
a framework that, unlike MDM, is not simulation-free."
RELATED WORK,0.3629441624365482,"Latent Diffusion Models (LDMs)
A key component to our MDM is the construction of mirror
map, which allows us to deﬁne diffusion models in a different space than the original constrained set,
thereby alleviating many computational difﬁculties. This makes MDM a particular type of LDMs
[5, 67, 68], which generally consider structurally advantageous spaces for learning diffusion models.
While LDMs typically learn such latent spaces either using pretrained model or on the ﬂy using, e.g.,
variational autoencoders, MDM instead derives analytically a latent (dual) space through a mirror
map tailored speciﬁcally to the constrained set. Similar to LDM [69], the diffusion processes of
MDM on the initial constrained set are implicitly deﬁned and can be highly nonlinear."
RELATED WORK,0.36548223350253806,"Table 3: Results of ℓ2-ball constrained sets on ﬁve synthetic datasets of various dimension d. We
report Sliced Wasserstein [70] w.r.t. 1000 samples, averaged over three trials, and include constraint
violations for unconstrained diffusion models. Note that Reﬂected Diffusion [18] and MDM satisfy
constraints by design. Our MDM clearly achieves similar or better performance to standard diffusion
models while fully respecting the constraints. Other metrics, e.g., W1, are reported in Appendix C. sc"
RELATED WORK,0.3680203045685279,"d = 2
d = 2
d = 6
d = 8
d=20"
RELATED WORK,0.37055837563451777,Sliced Wasserstein ↓
RELATED WORK,0.3730964467005076,"DDPM [2]
0.0704 ± 0.0095
0.0236 ± 0.0048
0.0379 ± 0.0015
0.0231 ± 0.0020
0.0200 ± 0.0034
Reﬂected [18]
0.0642 ± 0.0181
0.0491 ± 0.0103
0.0609 ± 0.0055
0.0881 ± 0.0010
0.0574 ± 0.0065
MDM (ours)
0.0544 ± 0.0070
0.0214 ± 0.0025
0.0467 ± 0.0096
0.0292 ± 0.0017
0.0159 ± 0.0044"
RELATED WORK,0.3756345177664975,Constraint violation (%) ↓
RELATED WORK,0.37817258883248733,"DDPM [2]
0.00 ± 0.00
0.00 ± 0.00
8.67 ± 0.87
13.60 ± 0.62
19.33 ± 1.29"
RELATED WORK,0.38071065989847713,Table 4: Results of simplices constrained sets on ﬁve synthetic datasets of various dimension d.
RELATED WORK,0.383248730964467,"d = 3
d = 3
d = 7
d = 9
d=20"
RELATED WORK,0.38578680203045684,Sliced Wasserstein ↓
RELATED WORK,0.3883248730964467,"DDPM [2]
0.0089 ± 0.0002
0.0110 ± 0.0032
0.0047 ± 0.0004
0.0053 ± 0.0003
0.0031 ± 0.0003
Reﬂected [18]
0.0233 ± 0.0019
0.0336 ± 0.0009
0.0411 ± 0.0039
0.0897 ± 0.0112
0.0231 ± 0.0011
MDM (ours)
0.0074 ± 0.0008
0.0169 ± 0.0016
0.0051 ± 0.0006
0.0040 ± 0.0003
0.0027 ± 0.0000"
RELATED WORK,0.39086294416243655,Constraint violation (%) ↓
RELATED WORK,0.3934010152284264,"DDPM [2]
0.73 ± 0.12
14.40 ± 1.39
11.63 ± 0.90
27.53 ± 0.57
68.83 ± 1.66 x3 x4"
RELATED WORK,0.39593908629441626,Ground Truth x3 x4 MDM x3 x4
RELATED WORK,0.39847715736040606,Reflected[18] x5 x6
RELATED WORK,0.4010152284263959,Ground Truth x5 x6 MDM x5 x6
RELATED WORK,0.4035532994923858,Reflected[18]
RELATED WORK,0.40609137055837563,"Figure 5: Comparison between MDM and Reﬂected Diffusion
[18] in modeling a Dirichlet distribution on a 7-dimensional
simplex. We visualize the joint densities between x3:4 and x5:6."
RELATED WORK,0.4086294416243655,"Table 5: Runtime and memory
complexity w.r.t. DDPM [2]."
RELATED WORK,0.41116751269035534,Runtime Memory
RELATED WORK,0.4137055837563452,"MDM
108%
100%
Reﬂected
Diff. [18] >1200%
905%"
EXPERIMENT,0.41624365482233505,"5
Experiment"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.41878172588832485,"5.1
Constrained Generation on Balls and Simplices"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4213197969543147,"Setup
We evaluate the performance of Mirror Diffusion Model (MDM) on common constrained
generation problems, such as ℓ2-ball and simplex constrained sets with dimensions d ranging from
2 to 20. Following Fishman et al. [18], we consider mixtures of Gaussian (Figure 1) and Spiral
(Figure 3) for ball constraints and Dirichlet distributions [48] with various concentrations for simplex
constraints. We compare MDM against standard unconstrained diffusion models, such as DDPM [2],
and their constrained counterparts, such as Reﬂected Diffusion [18], using the same time-embedded
fully-connected network and 1000 sampling time steps. Evaluation metrics include Sliced Wasserstein
distance [70] and constraint violation. Other details are left to Appendix C."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.42385786802030456,"MDM surpasses Reﬂected Diffusion on all tasks.
Tables 3 and 4 summarize our quantitative
results. For all constrained generation tasks, our MDM surpasses Reﬂected Diffusion by a large mar-
gin, with larger performance gaps as the dimension d increases. Qualitatively, Figure 5 demonstrates
how MDM better captures the underlying distribution than Reﬂected Diffusion. Additionally, Table 5
reports the relative complexity of both methods compared to simulation-free diffusion models such
as DDPM. While Reﬂected Diffusion requires extensive computation due to the use of implicit score
matching [71] and assertion of boundary reﬂection at every propagation step, our MDM constructs
efﬁcient, parallelizable, mirror maps in unconstrained dual spaces (see Section 3), thereby enjoying
preferable simulation-free complexity and better numerical scalability."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4263959390862944,"MDM matches DDPM without violating constraints.
It should be highlighted that, while DDPM
remains comparable to MDM in terms of distributional metric, the generated samples often violate
the designated constrained sets. As shown in Tables 3 and 4, these constraint violations worsen as the
dimension increases. In contrast, MDM is by design violation-free; hence marrying the best of both."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4289340101522843,"Table 6: The 50k-FID for unconditional watermarked generation. Wa-
termark precision denotes the percentage of constraint-satisﬁed images
that are generated by MDMs, controlled by loosing/tightening the con-
strained sets. As MDM-dual is trained on constraint-projected images,
we also report its FIDs∗w.r.t. the shifted distributions. The prior wa-
termarked diffusion model [39] reported 5.03 on FFHQ and 4.32 on
AFHQv2."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.43147208121827413,"FFHQ 64×64
AFHQv2 64×64
Precision
59.3%
71.8%
93.3%
56.9%
75.0%
92.7%"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.434010152284264,"MDM-proj
2.54
2.59
3.08
2.10
2.12
2.30"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4365482233502538,"MDM-dual
2.96
4.57
15.74
2.23
2.86
6.79
2.65∗
2.93∗
4.40∗
2.21∗
2.32∗
3.05∗ FFHQ"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.43908629441624364,"MDM-proj
MDM-dual"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4416243654822335,AFHQv2
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.44416243654822335,"Figure 6: Uncurated water-
marked samples by MDMs.
More samples can be found
in Appendix C."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4467005076142132,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.44923857868020306,Original
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4517766497461929,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4543147208121827,MDM-proj
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.45685279187817257,"0.9
0.9"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4593908629441624,"MDM-dual
Histogram of ai, x  on constriant range [-0.9,0.9]"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4619289340101523,"Figure 7: Histogram of ⟨ai, x⟩for orig-
inal and MDM-watermarked images x.
Dashed lines indicate the constrained set."
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.46446700507614214,"101
102  2.1 2.2 2.3 FID"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.467005076142132,"101
102
0.50 0.75"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.46954314720812185,Precision
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4720812182741117,"0.5
1.0
1.5 2.1 2.2 2.3 FID"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4746192893401015,"0.5
1.0
1.5 0.7 1.0"
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.47715736040609136,Precision
CONSTRAINED GENERATION ON BALLS AND SIMPLICES,0.4796954314720812,"Number of constraints, m, in log-scale
Polytope constraint range, bi =
ci
Figure 8: Ablation studies on how (top) number of con-
straints m and (bottom) their ranges, i.e., (ci, bi), affect
the FID and precision of MDM-proj on AFHQv2."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.48223350253807107,"5.2
Watermarked Generation on High-Dimensional Image Datasets"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4847715736040609,"Setup
We present MDM as a new approach for watermarked generation, where the watermark
is attached to samples within an orthonormal polytope M := {x ∈Rd : ci < a⊤
i x < bi, ∀i}.
These parameters {ai, bi, ci}m
i=1 serve as the private tokens visible only to individual users (see
Figure 2). While our approach is application-agnostic, we demonstrate MDM mainly on image
datasets, including both unconditional and conditional generation. Given a designated watermark
precision, deﬁned as the percentage of constraint-satisﬁed images that are generated by MDMs,
we randomly construct tokens as m orthonormalized Gaussian random vectors and instantiate
MDMs by either projecting unwatermarked diffusion models onto M (MDM-proj), or learning the
corresponding dual-space diffusions (MDM-dual). More precisely, MDM-proj projects samples
generated by pretrained diffusion models to a constraint set whose parameters (i.e., tokens) are
visible only to the private user. In contrast, MDM-dual learns a dual-space diffusion model from the
constrained-projected samples; hence, like other MDMs in Section 5.1, it is constraint-dependent.
Other details are left to Appendix C."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4873096446700508,"Unconditional watermark generation on FFHQ and AFHQv2
We ﬁrst test out MMD on FFHQ
[72] and AFHQv2 [73] on unconditional 64×64 image generation. Following Zhao et al. [39], we
adopt EDM parameterization [38] and report in Table 6 the performance of MDM in generating
watermarked images, quantiﬁed by the Frechet Inception Distance (FID) [74]. Compared to prior
watermarked approaches, which require additional training of latent spaces [39], MDM stands as a
competitive approach for watermarked generation by directly constructing analytic dual spaces
from the tokens (i.e., constraints) themselves. Intriguingly, despite the fact that MDM-dual learns
a smoother dual-space distribution compared to the truncated one from MDM-proj (see Figure 7),
the latter consistently achieves lower (hence better) FIDs. Since samples generated by MDM-dual
still remain close to the watermarked distribution, as indicated in Table 6 by the low FIDs∗w.r.t. the
shifted training statistics, we conjecture that the differences may be due to the signiﬁcant distributional
shift induced by naively projecting the training data onto the constraint set. This is validated, partly,
in the ablation study from Figure 8, where we observe that both the FID and watermark precision
improve as the number of constraints m decreases, and as the constraint ranges (ci, bi) loosen. We
highlight this fundamental trade off between watermark protection and generation quality. Examples
of watermarked images are shown in Figure 6 and Appendix C."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.48984771573604063,"Conditional generation on ImageNet256
Finally, we consider conditional watermarked gen-
eration on ImageNet 256×256. Speciﬁcally, we focus on image restoration tasks, where the goal"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.49238578680203043,"Degraded (JPEG QF=5) Input y
Ground Truth Reference (x
)"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.4949238578680203,"MDM-dual Watermarked Output (x
)
MDM-proj Watermarked Output (x
)"
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.49746192893401014,"Figure 9: Conditional watermarked generation on ImageNet 256×256. Speciﬁcally, we consider the
JPEG restoration task, where, given degraded, low-quality inputs y (upper-left), we wish to generate
their corresponding clean images x (upper-right) by learning p(x|y). It is clear that both MDM-dual
and MDM-proj are capable of solving this conditional generation task, generating clean images that
additionally embed invisible watermarks, i.e., x ∈M."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.5,"is to generate clean, watermarked, images conditioned on degraded—restored with JPEG in this
case—inputs. Similar to unconditional generation, we consider a polytope constraint set whose
parameters are chosen such that the watermark yields high precision (> 95%) and low false positive
rate (< 0.001%). Speciﬁcally, we set m = 100 and b = −c = 1.2. We initialize the networks with
pretrained checkpoints from Liu et al. [75]."
WATERMARKED GENERATION ON HIGH-DIMENSIONAL IMAGE DATASETS,0.5025380710659898,"Figure 9 reports the qualitative results. It is clear that both MDM-dual and MDM-proj are capable of
solving this conditional generation task, generating clean images that additionally embed invisible
watermarks, i.e., x ∈M. Note that all non-MDM-generated images, despite being indistinguishable,
actually violate the polytope constraint, whereas MDM-generated images always satisfy the con-
straint. Overall, our results suggest that both MDM-dual and MDM-proj scale to high-dimensional
applications and are capable of embedding invisible watermarks in high-resolution images."
CONCLUSION AND LIMITATION,0.5050761421319797,"6
Conclusion and Limitation"
CONCLUSION AND LIMITATION,0.5076142131979695,"We developed Mirror Diffusion Models (MDMs), a dual-space diffusion model that enjoys simulation-
free training for constrained generation. We showed that MDM outperforms prior methods in standard
constrained generation, such as balls and simplices, and offers a competitive alternative in generating
watermarked contents. It should be noted that MDM concerns mainly convex constrained sets, which,
despite their extensive applications, limits the application of MDM to general constraints. It will be
interesting to combine MDM with other dynamic generative models, such as ﬂow-based approaches."
CONCLUSION AND LIMITATION,0.5101522842639594,Broader Impact
CONCLUSION AND LIMITATION,0.5126903553299492,"Mirror Diffusion Models (MDMs) advance the recent development of diffusion models to complex
domains subjected to convex constrained sets. This opens up new possibilities for MDMs to serve as
preferred models for generating samples that live in e.g., simplices and balls. Additionally, MDMs
introduce an innovative application of constrained sets as a watermarking technique. This has the
potential to address concerns related to unethical usage and safeguard the copyright of generative
models. By incorporating constrained sets into the generating process, MDMs offer a means to
prevent unauthorized usage and ensure the integrity of generated content."
CONCLUSION AND LIMITATION,0.5152284263959391,Acknowledgements
CONCLUSION AND LIMITATION,0.5177664974619289,"The authors would like to thank Bogdon Vlahov for sharing the computational resources. GHL,
TC, ET are supported by ARO Award #W911NF2010151 and DoD Basic Research Ofﬁce Award
HQ00342110002. MT is partially supported by NSF DMS-1847802, NSF ECCS-1942523, Cullen-
Peck Scholarship, and GT-Emory AI.Humanity Award."
REFERENCES,0.5203045685279187,References
REFERENCES,0.5228426395939086,"[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning (ICML), 2015."
REFERENCES,0.5253807106598984,"[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5279187817258884,"[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. In
International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.5304568527918782,"[4] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In
Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.5329949238578681,"[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2022."
REFERENCES,0.5355329949238579,"[6] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. In International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.5380710659898477,"[7] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks
for efﬁcient and high ﬁdelity speech synthesis. In Advances in Neural Information Processing
Systems (NeurIPS), 2020."
REFERENCES,0.5406091370558376,"[8] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using
2D diffusion. In International Conference on Learning Representations (ICLR), 2023."
REFERENCES,0.5431472081218274,"[9] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten
Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D
content creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2023."
REFERENCES,0.5456852791878173,"[10] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313–326, 1982."
REFERENCES,0.5482233502538071,"[11] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In
International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.550761421319797,"[12] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5532994923857868,"[13] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David
Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH
2022 Conference Proceedings, pages 1–10, 2022."
REFERENCES,0.5558375634517766,"[14] Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh,
and Arnaud Doucet. Riemannian score-based generative modeling. In Advances in Neural
Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.5583756345177665,"[15] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville.
Riemannian diffusion models. In Advances in Neural Information Processing Systems (NeurIPS),
2022."
REFERENCES,0.5609137055837563,"[16] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing ﬂows. In Advances
in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.5634517766497462,"[17] Ricky T. Q. Chen and Yaron Lipman. Riemannian ﬂow matching on general geometries. arXiv
preprint arXiv:2302.03660, 2023."
REFERENCES,0.565989847715736,"[18] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael Hutchinson.
Diffusion models for constrained domains. Transactions on Machine Learning Research
(TMLR), 2023."
REFERENCES,0.5685279187817259,"[19] Ben J Morris. Improved bounds for sampling contingency tables. Random Structures &
Algorithms, 21(2):135–146, 2002."
REFERENCES,0.5710659898477157,"[20] Nathan E Lewis, Harish Nagarajan, and Bernhard O Palsson. Constraining the metabolic
genotype–phenotype relationship using a phylogeny of in silico methods. Nature Reviews
Microbiology, 10(4):291–305, 2012."
REFERENCES,0.5736040609137056,"[21] Li Han and Lee Rudolph. Inverse kinematics for a serial chain with joints under distance
constraints. In Robotics: Science and Systems (RSS), 2006."
REFERENCES,0.5761421319796954,"[22] Tsuneo Yoshikawa. Manipulability of robotic mechanisms. The international journal of
Robotics Research, 4(2):3–9, 1985."
REFERENCES,0.5786802030456852,"[23] Ruth J Williams. Reﬂected brownian motion with skew symmetric data in a polyhedral domain.
Probability Theory and Related Fields, 75(4):459–485, 1987."
REFERENCES,0.5812182741116751,"[24] Andrey Pilipenko. An introduction to stochastic differential equations with reﬂection, volume 1.
Universitätsverlag Potsdam, 2014."
REFERENCES,0.583756345177665,"[25] Aaron Lou and Stefano Ermon. Reﬂected diffusion models. In International Conference on
Machine Learning (ICML), 2023."
REFERENCES,0.5862944162436549,"[26] Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin
monte carlo. In Advances in Neural Information Processing Systems (NeurIPS), 2015."
REFERENCES,0.5888324873096447,"[27] Kanji Sato, Akiko Takeda, Reiichiro Kawai, and Taiji Suzuki. Convergence error analysis of
reﬂected gradient langevin dynamics for globally optimizing non-convex constrained problems.
arXiv preprint arXiv:2203.10215, 2022."
REFERENCES,0.5913705583756346,"[28] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored langevin dynamics. In
Advances in Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.5939086294416244,"[29] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffusion
operators, volume 103. Springer, 2014."
REFERENCES,0.5964467005076142,"[30] Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):
123–214, 2011."
REFERENCES,0.5989847715736041,"[31] Kelvin Shuangjian Zhang, Gabriel Peyré, Jalal Fadili, and Marcelo Pereyra. Wasserstein control
of mirror langevin monte carlo. In Conference on Learning Theory (COLT), 2020."
REFERENCES,0.6015228426395939,"[32] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme.
Exponential ergodicity of mirror-Langevin diffusions. In Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.6040609137055838,"[33] Kwangjun Ahn and Sinho Chewi. Efﬁcient constrained sampling via the mirror-Langevin
algorithm. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.6065989847715736,"[34] Ruilin Li, Molei Tao, Santosh S Vempala, and Andre Wibisono. The mirror langevin algorithm
converges with vanishing bias. In International Conference on Algorithmic Learning Theory
(ALT), 2022."
REFERENCES,0.6091370558375635,"[35] Michael Liebrenz, Roman Schleifer, Anna Buadze, Dinesh Bhugra, and Alexander Smith.
Generating scholarly content with chatgpt: ethical challenges for medical publishing. The
Lancet Digital Health, 5(3):e105–e106, 2023."
REFERENCES,0.6116751269035533,"[36] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.
A watermark for large language models. In International Conference on Machine Learning
(ICML), 2023."
REFERENCES,0.6142131979695431,"[37] Jürgen Rudolph, Samson Tan, and Shannon Tan. Chatgpt: Bullshit spewer or the end of
traditional assessments in higher education? Journal of Applied Learning and Teaching, 6(1),
2023."
REFERENCES,0.616751269035533,"[38] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Advances in Neural Information Processing Systems
(NeurIPS), 2022."
REFERENCES,0.6192893401015228,"[39] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe
for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023."
REFERENCES,0.6218274111675127,"[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical Image Computing
and Computer-assisted Intervention. Springer, 2015."
REFERENCES,0.6243654822335025,"[41] Peter D Lax. Change of variables in multiple integrals. The American mathematical monthly,
106(6):497–501, 1999."
REFERENCES,0.6269035532994924,"[42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021."
REFERENCES,0.6294416243654822,"[43] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of
score-based diffusion models. In Advances in Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.631979695431472,"[44] Sébastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal
self-concordant barrier. In Conference on Learning Theory (COLT), 2015."
REFERENCES,0.6345177664974619,"[45] Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method
efﬁciency in optimization. Wiley-Interscience, 1983."
REFERENCES,0.6370558375634517,"[46] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167–175, 2003."
REFERENCES,0.6395939086294417,"[47] Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003."
REFERENCES,0.6421319796954315,"[48] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of
Machine Learning Research (JMLR), 2003."
REFERENCES,0.6446700507614214,"[49] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and
inference for topic models. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2009."
REFERENCES,0.6472081218274112,"[50] Lixin Lang, Wen-shiang Chen, Bhavik R Bakshi, Prem K Goel, and Sridhar Ungarala. Bayesian
estimation via sequential monte carlo sampling—constrained dynamic systems. Automatica, 43
(9):1615–1622, 2007."
REFERENCES,0.649746192893401,"[51] Michael A Gelbart, Jasper Snoek, and Ryan P Adams. Bayesian optimization with unknown
constraints. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2014."
REFERENCES,0.6522842639593909,"[52] Yann Brenier. Décomposition polaire et réarrangement monotone des champs de vecteurs. CR
Acad. Sci. Paris Sér. I Math., 305:805–808, 1987."
REFERENCES,0.6548223350253807,"[53] Nicolas Brosse, Alain Durmus, Éric Moulines, and Marcelo Pereyra. Sampling from a log-
concave distribution with compact support with proximal Langevin Monte Carlo. In Conference
on Learning Theory (COLT), 2017."
REFERENCES,0.6573604060913706,"[54] Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the
probability simplex. In Advances in Neural Information Processing Systems (NeurIPS), 2013."
REFERENCES,0.6598984771573604,"[55] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow
matching for generative modeling. In International Conference on Learning Representations
(ICLR), 2023."
REFERENCES,0.6624365482233503,"[56] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Semi-discrete normalizing ﬂows
through differentiable tessellation. In Advances in Neural Information Processing Systems
(NeurIPS), 2022."
REFERENCES,0.6649746192893401,"[57] Xingchao Liu, Lemeng Wu, Mao Ye, et al. Learning diffusion bridges on constrained domains.
In International Conference on Learning Representations (ICLR), 2022."
REFERENCES,0.6675126903553299,"[58] Mao Ye, Lemeng Wu, and Qiang Liu. First hitting diffusion models for generating manifold,
graph and categorical data. In Advances in Neural Information Processing Systems (NeurIPS),
2022."
REFERENCES,0.6700507614213198,"[59] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional
diffusion for molecular conformer generation. In Advances in Neural Information Processing
Systems (NeurIPS), 2022."
REFERENCES,0.6725888324873096,"[60] Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock:
Diffusion steps, twists, and turns for molecular docking. In International Conference on
Learning Representations (ICLR), 2023."
REFERENCES,0.6751269035532995,"[61] Adam Leach, Sebastian M Schmon, Matteo T Degiacomi, and Chris G Willcocks. Denoising
diffusion probabilistic models on SO(3) for rotational alignment. In International Conference
on Learning Representations (ICLR), Workshop Track, 2022."
REFERENCES,0.6776649746192893,"[62] Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. SE(3)-DiffusionFields: Learn-
ing cost functions for joint grasp and motion optimization through diffusion. In IEEE Interna-
tional Conference on Robotics and Automation (ICRA), Workshop Track, 2022."
REFERENCES,0.6802030456852792,"[63] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical SDEs with simplex
diffusion. arXiv preprint arXiv:2210.14784, 2022."
REFERENCES,0.682741116751269,"[64] Marco Caponigro, Anna Chiara Lai, and Benedetto Piccoli. A nonlinear model of opinion
formation on the sphere. Discrete & Continuous Dynamical Systems-A, 35(9):4241–4268, 2015."
REFERENCES,0.6852791878172588,"[65] Jason Gaitonde, Jon Kleinberg, and Éva Tardos. Polarization in geometric opinion dynamics.
In ACM Conference on Economics and Computation, pages 499–519, 2021."
REFERENCES,0.6878172588832487,"[66] John C Cox, Jonathan E Ingersoll Jr, and Stephen A Ross. A theory of the term structure of
interest rates. In Theory of valuation, pages 129–164. World Scientiﬁc, 2005."
REFERENCES,0.6903553299492385,"[67] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.
In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.6928934010152284,"[68] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efﬁcient,
controllable and high-ﬁdelity generation from low-dimensional latents. Transactions on Machine
Learning Research (TMLR), 2022."
REFERENCES,0.6954314720812182,"[69] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-Chul Moon.
Maximum likelihood training of implicit nonlinear diffusion models. In Advances in Neural
Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.6979695431472082,"[70] Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pﬁster. Sliced and radon wasser-
stein barycenters of measures. Journal of Mathematical Imaging and Vision, 51:22–45, 2015."
REFERENCES,0.700507614213198,"[71] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable
approach to density and score estimation. In Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), 2020."
REFERENCES,0.7030456852791879,"[72] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2019."
REFERENCES,0.7055837563451777,"[73] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthe-
sis for multiple domains. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2020."
REFERENCES,0.7081218274111675,"[74] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.7106598984771574,"[75] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima
Anandkumar. I2SB: Image-to-Image Schrödinger bridge. In International Conference on
Machine Learning (ICML), 2023."
REFERENCES,0.7131979695431472,"[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.7157360406091371,"[77] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.7182741116751269,"[78] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018."
REFERENCES,0.7208121827411168,"[79] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV), pages 3–19, 2018."
REFERENCES,0.7233502538071066,"A
Derivation of Mirror Mappings"
REFERENCES,0.7258883248730964,"Here, we provide addition derivation of ∇φ∗. Computation of ∇φ(x) and ∇2φ∗(y) follow straight-
forwardly by differentiating φ(x) and ∇φ∗(y) w.r.t. x and y, respectively."
REFERENCES,0.7284263959390863,"ℓ2-Ball
Since the gradient map also reverses the mirror map, we aim to rewrite y =
2γ
R−∥x∥2
2 x as
x = f(y) = ∇φ∗
ball(y). Solving the second-order polynomial,"
REFERENCES,0.7309644670050761,"∥y∥2
2 =

2γ
R −∥x∥2
2"
REFERENCES,0.733502538071066,"2
∥x∥2
2,
(19)"
REFERENCES,0.7360406091370558,yields
REFERENCES,0.7385786802030457,"∥x∥2
2 = R + 2γ"
REFERENCES,0.7411167512690355,"∥y∥2
2"
REFERENCES,0.7436548223350253,"
γ −
q"
REFERENCES,0.7461928934010152,"R∥y∥2
2 + γ2

.
(20)"
REFERENCES,0.748730964467005,"With that, we can rewrite Equation (11) by"
REFERENCES,0.751269035532995,"x = R −∥x∥2
2
2γ
y
(20)
= p"
REFERENCES,0.7538071065989848,"R∥y∥2
2 + γ2 −γ
∥y∥2
2
y =
R
p"
REFERENCES,0.7563451776649747,"R∥y∥2
2 + γ2 + γ
y."
REFERENCES,0.7588832487309645,"Simplex
Standard calculations in convex analysis [28] shows"
REFERENCES,0.7614213197969543,"φ∗
simplex(y) = log  1 + d
X"
REFERENCES,0.7639593908629442,"i
eyi
!"
REFERENCES,0.766497461928934,".
(21)"
REFERENCES,0.7690355329949239,"Differentiating Equation (21) w.r.t. y yields ∇φ∗
simplex in Equation (13)."
REFERENCES,0.7715736040609137,"Polytope
Since the gradient map also reverses the mirror map, we aim to inverse y = m
X"
REFERENCES,0.7741116751269036,"i=1
si(⟨ai, x⟩)ai + d
X"
REFERENCES,0.7766497461928934,"j=m+1
⟨aj, x⟩aj.
(22)"
REFERENCES,0.7791878172588832,"When all d constraints are orthonormal, taking inner product between y and each a yields"
REFERENCES,0.7817258883248731,"⟨ai, y⟩= si(⟨ai, x⟩),
⟨aj, y⟩= ⟨aj, x⟩.
(23)"
REFERENCES,0.7842639593908629,"Therefore, we can reconstruct x from y via x = m
X"
REFERENCES,0.7868020304568528,"i=1
⟨ai, x⟩ai + d
X"
REFERENCES,0.7893401015228426,"j=m+1
⟨aj, x⟩aj"
REFERENCES,0.7918781725888325,"(23)
= m
X"
REFERENCES,0.7944162436548223,"i=1
s−1
i (⟨ai, y⟩)ai + d
X"
REFERENCES,0.7969543147208121,"j=m+1
⟨aj, y⟩aj,"
REFERENCES,0.799492385786802,"which deﬁnes x = ∇φ∗
polytope(y). For completeness, the Hessian can be presented compactly as"
REFERENCES,0.8020304568527918,"∇2φ∗
polytope(y) = I + AΣA⊤,
(24)"
REFERENCES,0.8045685279187818,"where I is the identity matrix, A := [a1, · · · , am] is a d-by-m matrix whose column vector ai
corresponds to each constraint, and Σ ∈Rm×m is a diagonal matrix with leading entries"
REFERENCES,0.8071065989847716,"[Σ]ii = ∂s−1
i (z)"
REFERENCES,0.8096446700507615,"∂z
|z=⟨ai,y⟩−1
(18)
= bi −ci"
REFERENCES,0.8121827411167513,"2
 
1 −tanh2(⟨ai, y⟩)

−1."
REFERENCES,0.8147208121827412,"B
Additional Remarks on Polytope"
REFERENCES,0.817258883248731,"Derivation of Equation (17)
Since the subspaces spanned by {ai} and {aj} are orthogonal to each
other, we can rewrite (15) as"
REFERENCES,0.8197969543147208,"∇φpolytope(x) = m
X"
REFERENCES,0.8223350253807107,"i=1
si(⟨ai, x⟩)ai +  x − m
X"
REFERENCES,0.8248730964467005,"i=1
⟨ai, x⟩ai ! = x + m
X"
REFERENCES,0.8274111675126904,"i=1
(si(⟨ai, x⟩) −⟨ai, x⟩) ai."
REFERENCES,0.8299492385786802,"∇φ∗
polytope(y) follows similar derivation."
REFERENCES,0.8324873096446701,"Generalization to non-orthonormal constraints
The mirror maps of a polytope, as described in
Equations (15) to (17), can be seen as operations that manipulate the coefﬁcients associated with
the bases deﬁned by the constraints. This understanding allows us to extend the computation to
non-orthonormal constraints by identifying the corresponding “coefﬁcients” through a change of
bases, utilizing the reproducing formula: x = d
X"
REFERENCES,0.8350253807106599,"i=1
⟨˜ai, x⟩ai, where ˜ai is the i-th row of (A⊤A)−1A⊤,"
REFERENCES,0.8375634517766497,"and A := [a1, · · · , am]. Similarly, we have y = Pd
i=1⟨˜ai, y⟩ai. Applying similar derivation leads to"
REFERENCES,0.8401015228426396,"∇φpoly(x) = x + m
X"
REFERENCES,0.8426395939086294,"i=1
(si(⟨˜ai, x⟩) −⟨˜ai, x⟩) ai, ∇φ∗
poly(y) = y + m
X i=1"
REFERENCES,0.8451776649746193," 
s−1
i (⟨˜ai, y⟩) −⟨˜ai, y⟩

ai."
REFERENCES,0.8477157360406091,"C
Experiment Details & Additional Results"
REFERENCES,0.850253807106599,Table 7: The concentration parameter α of each Dirichlet distribution in simplices constrained sets.
REFERENCES,0.8527918781725888,"d
3
3
7
9
20"
REFERENCES,0.8553299492385786,"α
[2, 4, 8]
[1, 0.1, 5]
[1, 2, 2, 4, 4, 8, 8]
[1, 0.5, 2, 0.3, 0.6, 4, 8, 8, 2]
[0.2, 0.4, · · · , 4, 4.2]"
REFERENCES,0.8578680203045685,"Table 8: Hyperparameters of the polytope M := {x ∈Rd : ci < a⊤
i x < bi} for
each dataset and watermark precision. Note that we ﬁx b = bi = −ci in practice."
REFERENCES,0.8604060913705583,"FFHQ 64×64 (uncon)
AFHQv2 64×64 (uncon)
Precision
59.3%
71.8%
93.3%
56.9%
75.0%
92.7%"
REFERENCES,0.8629441624365483,"Number of constraints m
7
20
100
4
20
100
Constraint range b
1.05
1.05
1.05
0.9
0.9
0.9"
REFERENCES,0.8654822335025381,Dataset & constrained sets
REFERENCES,0.868020304568528,"• ℓ2-balls constrained sets: For d = 2, we consider the Gaussian Mixture Model (with variance
0.05) and the Spiral shown respectively in Figures 1 and 3. For d = {6, 8, 20}, we place d
isotropic Gaussians, each with variance 0.05, at the corner of each dimension, and reject samples
outside the constrained sets."
REFERENCES,0.8705583756345178,"• Simplices constrained sets: We consider Dirichlet distributions [48], Dir(α), with various con-
centration parameters α detailed in Table 7."
REFERENCES,0.8730964467005076,"• Hypercube constrained sets: For all dimensions d = {2, 3, 6, 8, 20}, we place d isotropic
Gaussians, each with variance 0.2, at the corner of each dimension, and either reject (d =
{2, 3, 6, 8}) or reﬂect (d = 20) samples outside the constrained sets."
REFERENCES,0.8756345177664975,"• Watermarked datasets and polytope constrained sets: We follow the same data preprocessing from
EDM5 [38] and rescale both FFHQ and AFHQv2 to 64×64 image resolution. For the polytope
constrained sets M := {x ∈Rd : ci < a⊤
i x < bi, ∀i}, we construct ai from orthonormalized
Gaussian random vectors and detail other hyperparameters in Table 8."
REFERENCES,0.8781725888324873,"5https://github.com/NVlabs/edm, released under Nvidia Source Code License."
REFERENCES,0.8807106598984772,"Implementation
All methods are implemented in PyTorch [76]. We adopt ADM6 and EDM5 [38]
respectively as the MDM’s diffusion backbones for constrained and watermarked generation. We
implemented Reﬂected Diffusion [18] by ourselves as their codes have not yet been made available by
the submission time (May 2023), and used the ofﬁcial implementation7 of Reﬂected Diffusion [25] in
Table 11. We also implemented Simplex Diffusion [63], but as observed in previous works [25], it
encountered computational instability especially when computing the modiﬁed Bessel functions."
REFERENCES,0.883248730964467,"Training
For constrained generation, all methods are trained with AdamW [77] and an exponential
moving average with the decay rate of 0.99. As standard practices, we decay the learning rate by the
decay rate 0.99 every 1000 steps. For watermarked generation, we follow the default hyperparameters
from EDM5 [38]. All experiments are conducted on two TITAN RTXs and one RTX 2080."
REFERENCES,0.8857868020304569,"Network
For constrained generation, all networks take (y, t) as inputs and follow"
REFERENCES,0.8883248730964467,"out = out_mod(norm(y_mod( y ) + t_mod(timestep_embedding( t )))),"
REFERENCES,0.8908629441624365,"where timestep_embedding(·) is the standard sinusoidal embedding. t_mod and out_mod consist
of 2 fully-connected layers (Linear) activated by the Sigmoid Linear Unit (SiLU) [78]:"
REFERENCES,0.8934010152284264,t_mod = out_mod = Linear →SiLU →Linear
REFERENCES,0.8959390862944162,"and y_mod consists of 3 residual blocks, i.e., y_mod(y) = y + res_mod(norm(y)), where"
REFERENCES,0.8984771573604061,res_mod = Linear →SiLU →Linear →SiLU →Linear →SiLU →Linear
REFERENCES,0.9010152284263959,"All Linear’s have 128 hidden dimension. We use group normalization [79] for all norm. For
watermarked generation, we use EDM parameterization5 [38]."
REFERENCES,0.9035532994923858,"Evaluation
We compute the Wasserstein and Sliced Wasserstein distances using the geomloss8
and ot9 packages, respectively. The Maximum Mean Discrepancy (MMD) is based on the popular
package https://github.com/ZongxianLee/MMD_Loss.Pytorch, which is unlicensed. For
watermarked generation, we follow the same evaluation pipeline from EDM5 [38] by ﬁrst generating
50,000 watermarked samples and computing the FID w.r.t. the training statistics."
REFERENCES,0.9060913705583756,"False-positive rate
Similar to Kirchenbauer et al. [36], we reject the null hypothesis and detect the
watermark if the sample produces no violation of the polytope constraint, i.e., if x ∈M. Hence, the
false-positive samples are those that are actually true null hypothesis (i.e., not generated by MDM) yet
accidentally fall into the constraint set, hence being mistakenly detected as watermarked. Speciﬁcally,
the false-positive rates of our MDMs are respectively 0.07% and 0.08% for FFHQ and AFHQv2.
Lastly, we note that the fact that both MDM-proj and MDM-dual generate samples that always satisfy
the constraint readily implies 100% recall and 0% Type II error."
REFERENCES,0.9086294416243654,"C.1
Additional Results"
REFERENCES,0.9111675126903553,"Groundtruth
MDM NLL"
REFERENCES,0.9137055837563451,"Figure 10: Tractable variational
bound by our MDM."
REFERENCES,0.916243654822335,"Tractable variational bound in Equation (8)
Figure 10
demonstrates how MDM faithfully captures the variational bound
to the negative log-likelihood (NLL) of 2-dimensional GMM."
REFERENCES,0.9187817258883249,"More constrained sets, distributional metrics, & baseline
Ta-
bles 9 and 10 expand the analysis in Tables 3 and 4 with additional
distributional metrics such as Wasserstein-1 (W1) and Maximum
Mean Discrepancy (MMD). Additionally, Table 11 reports the
results of hypercube [0, 1]d constrained set, a special instance
of polytopes, and includes additional baseline from Lou and Ermon [25], which approximate the"
REFERENCES,0.9213197969543148,"6https://github.com/openai/guided-diffusion, released under MIT License.
7https://github.com/louaaron/Reflected-Diffusion, latest commit (65d05c6) at submission, un-
licensed.
8https://github.com/jeanfeydy/geomloss, released under MIT License.
9https://pythonot.github.io/gen_modules/ot.sliced.html#ot.sliced.sliced_
wasserstein_distance, released under MIT License."
REFERENCES,0.9238578680203046,"intractable scores in Reﬂected Diffusion using eigenfunctions tailored speciﬁcally to hypercubes,
rather than implicit score matching as in Fishman et al. [18]. Consistently, our ﬁndings conclude
that the MDM is the only constrained-based diffusion model that achieves comparable or better
performance to DDPM. These results afﬁrm the effectiveness of MDM in generating high-quality
samples within constrained settings, making it a reliable choice for constrained generative modeling."
REFERENCES,0.9263959390862944,"More watermarked samples
Figures 11 and 12 provide additional qualitative results on the
watermarked samples generated by MDMs."
REFERENCES,0.9289340101522843,"Figure 11: FFHQ 64×64 unconditional watermarked samples generated by (left) MDM-proj and
(right) MDM-dual from the same set of random seeds. Despite the fact that some images, such
as the one in the ﬁrst row and sixth column, were altered possibly due to the change of dual-space
distribution (see Figure 7), they look realistic and remain close to the data distribution."
REFERENCES,0.9314720812182741,"Figure 12: AFHQv2 64×64 unconditional watermarked samples generated by (left) MDM-proj and
(right) MDM-dual from the same set of random seeds. Despite the fact that some images, such
as the one in the ﬁfth row and ﬁrst column, were altered possibly due to the change of dual-space
distribution (see Figure 7), they all look realistic and remain close to the data distribution."
REFERENCES,0.934010152284264,"Table 9: Expanded results of ℓ2-ball constrained sets, where we include additional distributional
metrics such as W1 and Maximum Mean Discrepancy (MMD), all computed with 1000 samples and
averaged over three trials. Consistently, our ﬁndings conclude that the MDM is the only constrained-
based diffusion model that achieves comparable or better performance to DDPM."
REFERENCES,0.9365482233502538,"d = 2
d = 2
d = 6
d = 8
d=20"
REFERENCES,0.9390862944162437,W1 ↓(unit: 10−2)
REFERENCES,0.9416243654822335,"DDPM [2]
0.66 ± 0.15
0.14 ± 0.03
0.52 ± 0.09
0.58 ± 0.10
3.45 ± 0.50
Reﬂected [18]
0.55 ± 0.29
0.46 ± 0.17
3.11 ± 0.40
10.13 ± 0.21
19.42 ± 0.13
MDM (ours)
0.46 ± 0.07
0.12 ± 0.04
0.72 ± 0.39
1.05 ± 0.26
2.63 ± 0.31"
REFERENCES,0.9441624365482234,MMD ↓(unit: 10−2)
REFERENCES,0.9467005076142132,"DDPM [2]
0.67 ± 0.23
0.23 ± 0.07
0.37 ± 0.19
0.75 ± 0.24
0.98 ± 0.42
Reﬂected [18]
0.58 ± 0.46
5.03 ± 1.17
2.34 ± 0.14
28.82 ± 0.66
14.83 ± 0.62
MDM (ours)
0.52 ± 0.36
0.27 ± 0.19
0.54 ± 0.12
0.35 ± 0.23
0.50 ± 0.17"
REFERENCES,0.949238578680203,Constraint violation (%) ↓
REFERENCES,0.9517766497461929,"DDPM [2]
0.00 ± 0.00
0.00 ± 0.00
8.67 ± 0.87
13.60 ± 0.62
19.33 ± 1.29"
REFERENCES,0.9543147208121827,Table 10: Expanded results of simplices constrained sets.
REFERENCES,0.9568527918781726,"d = 3
d = 3
d = 7
d = 9
d=20"
REFERENCES,0.9593908629441624,W1 ↓(unit: 10−2)
REFERENCES,0.9619289340101523,"DDPM [2]
0.01 ± 0.00
0.02 ± 0.01
0.03 ± 0.00
0.05 ± 0.00
0.11 ± 0.00
Reﬂected [18]
0.06 ± 0.01
0.12 ± 0.00
0.62 ± 0.08
3.57 ± 0.05
0.98 ± 0.02
MDM (ours)
0.01 ± 0.00
0.01 ± 0.01
0.03 ± 0.00
0.05 ± 0.00
0.13 ± 0.00"
REFERENCES,0.9644670050761421,MMD ↓(unit: 10−2)
REFERENCES,0.9670050761421319,"DDPM [2]
0.72 ± 0.07
0.72 ± 0.30
0.74 ± 0.10
0.97 ± 0.22
1.12 ± 0.07
Reﬂected [18]
3.91 ± 0.95
15.12 ± 1.36
16.48 ± 1.04
131.44 ± 2.65
57.90 ± 2.07
MDM (ours)
0.44 ± 0.16
0.50 ± 0.26
0.42 ± 0.08
0.55 ± 0.13
0.61 ± 0.03"
REFERENCES,0.9695431472081218,Constraint violation (%) ↓
REFERENCES,0.9720812182741116,"DDPM [2]
0.73 ± 0.12
14.40 ± 1.39
11.63 ± 0.90
27.53 ± 0.57
68.83 ± 1.66"
REFERENCES,0.9746192893401016,"Table 11: Results of hypercube [0, 1]d constrained sets."
REFERENCES,0.9771573604060914,"d = 2
d = 3
d = 6
d = 8
d=20"
REFERENCES,0.9796954314720813,Sliced Wasserstein ↓(unit: 10−2)
REFERENCES,0.9822335025380711,"DDPM [2]
2.24 ± 1.22
2.17 ± 0.65
2.05 ± 0.41
2.01 ± 0.16
1.54 ± 0.01
Reﬂected [25]
3.75 ± 1.20
6.58 ± 1.18
2.77 ± 0.06
3.50 ± 0.69
3.37 ± 0.46
Reﬂected [18]
19.05 ± 1.51
17.16 ± 0.88
11.90 ± 0.43
7.49 ± 0.13
4.32 ± 0.23
MDM (ours)
3.00 ± 0.72
1.92 ± 0.81
1.75 ± 0.17
1.85 ± 0.34
3.35 ± 0.64"
REFERENCES,0.9847715736040609,W1 ↓(unit: 10−2)
REFERENCES,0.9873096446700508,"DDPM [2]
0.07 ± 0.05
0.22 ± 0.07
1.65 ± 0.14
3.30 ± 0.16
16.74 ± 0.12
Reﬂected [25]
0.20 ± 0.12
1.21 ± 0.39
2.53 ± 0.04
4.82 ± 0.42
25.47 ± 0.20
Reﬂected [18]
4.40 ± 0.57
6.01 ± 0.97
9.34 ± 0.56
9.84 ± 0.24
25.27 ± 0.36
MDM (ours)
0.08 ± 0.03
0.20 ± 0.07
1.57 ± 0.08
3.34 ± 0.23
20.59 ± 1.19"
REFERENCES,0.9898477157360406,MMD ↓(unit: 10−2)
REFERENCES,0.9923857868020305,"DDPM [2]
0.27 ± 0.26
0.32 ± 0.14
0.69 ± 0.21
0.81 ± 0.23
0.73 ± 0.07
Reﬂected [25]
0.92 ± 0.53
3.56 ± 1.31
1.16 ± 0.04
2.09 ± 0.70
2.83 ± 0.58
Reﬂected [18]
32.26 ± 3.19
26.64 ± 5.07
29.83 ± 1.42
15.84 ± 0.89
7.21 ± 0.68
MDM (ours)
0.27 ± 0.09
0.29 ± 0.17
0.39 ± 0.14
0.61 ± 0.23
0.62 ± 0.05"
REFERENCES,0.9949238578680203,Constraint violation (%) ↓
REFERENCES,0.9974619289340102,"DDPM [2]
9.37 ± 0.12
17.57 ± 1.27
41.70 ± 1.30
59.30 ± 1.39
94.47 ± 0.64"
