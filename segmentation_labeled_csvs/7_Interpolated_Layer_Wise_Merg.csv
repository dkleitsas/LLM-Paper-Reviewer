Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.011494252873563218,"Model merging is an emerging technique to generate models by combining multi-
ple models, leveraging their individual strengths in a unified model. Because the
configuration hyperparameters of model merge techniques have a significant effect
on the merged models, optimizing them using evolutionary algorithms is promising
to enhance the model merge results. In our participation in the LLM Merging
Competition in NeurIPS 2024, we introduced a novel search space, interpolated
layer-wise space, for optimizing merging configurations using evolutionary algo-
rithms. This paper explores the potential for performance enhancement in merged
models and the efficiency of our search space for evolutionary model merge."
INTRODUCTION,0.022988505747126436,"1
Introduction"
INTRODUCTION,0.034482758620689655,"Rapid advancement of LLM has significantly impacted natural language processing, enabling break-
throughs in tasks such as machine translation, text summarization, and conversational AI. As these
models become increasingly complex and huge, merging multiple LLMs to create more powerful and
efficient systems has emerged as a crucial research area. Evolutionary algorithms can be powerful
tools to optimize merging configurations in this context [1]. The literature of [1] is a pioneer work of
optimizing merging configurations, where two merging approaches are considered: the parameter
space (PS) merging and the data flow space (DFS) merging. In the PS merging, the weights of
multiple candidate models are integrated into a merged model with the same architecture. Several
merging techniques have been developed, such as TIES-Merging [2]. In [1], the merging configura-
tions of DARE-TIES [2, 3] are optimized using the covariance matrix adaptation evolution strategy
(CMA-ES) [4]."
INTRODUCTION,0.04597701149425287,"The performance and optimization efficiency of evolutionary algorithms heavily depend on the search
space. Typically, when optimizing the PS merging configurations, the model-wise search space in
which the entire model is treated as single units during the optimization process is used. Namely, the
common merging configuration parameters are used for all weights in each candidate model. This
model-wise search space is simple and can reduce the search space size but may not fully leverage the
potential of combining different models. A straightforward extension to enhance granularity is the
layer-wise approach, which optimizes the merging configurations for each layer. While layer-wise
merging offers a more fine-grained search space, our preliminary experiments revealed that this
approach leads to an excessively large search space, leading to difficult and computationally expensive
optimization problems."
INTRODUCTION,0.05747126436781609,"To address these challenges, we propose a novel search space called interpolated layer-wise space.
In our search space, the merging parameters for a part of the layers are optimized, and the merging"
INTRODUCTION,0.06896551724137931,"extract
optimize
interpolate"
INTRODUCTION,0.08045977011494253,"Model A
Model B
Model A
Model B
Merged Model
Merged Model"
INTRODUCTION,0.09195402298850575,Figure 1: Computation process of merge parameters on the interpolated layer-wise space.
INTRODUCTION,0.10344827586206896,"parameters for other layers are computed by interpolation methods. The proposed search space
has a higher expressive capability than the model-wise approach while keeping the number of
design variables to be optimized lower than the layer-wise approach. Therefore, optimizing merging
configurations using our search space is expected to improve the search efficiency without sacrificing
the performance of the merged models compared to the layer-wise approach."
INTRODUCTION,0.11494252873563218,"We conducted experiments to evaluate the effectiveness of our search space. The results demonstrate
that the CMA-ES converges earlier on the interpolated layer-wise space than the layer-wise search
space without sacrificing performance. These findings suggest that our search space offers a practical
trade-off between the performance of the merged model and the computational budget for optimization.
We submitted the model merging code for the competition using the merging configuration parameters
obtained by optimization in our interpolated layer-wise search space."
IMPLEMENTATION/METHODS,0.12643678160919541,"2
Interpolated Layer-Wise Space"
IMPLEMENTATION/METHODS,0.13793103448275862,"We introduce our proposed search space, the interpolated layer-wise space, for evolutionary model
merge. The general optimization process on our search space is illustrated in Figure 1."
IMPLEMENTATION/METHODS,0.14942528735632185,"2.1
Design Variable on Interpolated Layer-Wise Space"
IMPLEMENTATION/METHODS,0.16091954022988506,"In interpolated layer-wise space, n layers are selected to construct the search space for evolutionary
algorithms. We determined these layers by dividing the model’s entire layers with even intervals.
Then, in evolutionary model merge, the merge parameters for n pre-selected layers are sampled by
evolutionary algorithm."
IMPLEMENTATION/METHODS,0.1724137931034483,"Given the number of layers in the model l, the number of models m, and the number of merge
parameters p for each layer, the number of possible configuration parameters on our interpolated
layer-wise search space is n×m×p, which reduces the search space dimensions n/l times compared
to l × m × p for the layer-wise space."
IMPLEMENTATION/METHODS,0.1839080459770115,"2.2
Interpolation of Merge Parameters"
IMPLEMENTATION/METHODS,0.19540229885057472,"For non-selected layers, the merge parameters are computed by interpolation using the merge
parameters for the pre-selected layers. We expect that this interpolation ensures that all layers receive
fine-grained merge parameters, improving the performance of the merged model compared to the
model-wise space. In the optimization process of evolutionary model merge, the evaluation of the
merged model is performed with interpolated merge parameters. Although several interpolation
methods can be applied to our search space, we used cubic spline-interpolation provided by scipy
in this report. This interpolation approach implicitly assumes that the good merging configuration
parameters smoothly change between those of pre-selected layers."
IMPLEMENTATION/METHODS,0.20689655172413793,"0
200
400
600
800
1000
Evaluation 0.73 0.74 0.75 0.76 0.77 0.78 0.79"
IMPLEMENTATION/METHODS,0.21839080459770116,Accuracy
IMPLEMENTATION/METHODS,0.22988505747126436,"model-wise
layer-wise
spline"
IMPLEMENTATION/METHODS,0.2413793103448276,(a) All Methods
IMPLEMENTATION/METHODS,0.25287356321839083,"0
200
400
600
800
1000
Evaluation 0.3 0.4 0.5 0.6 0.7"
IMPLEMENTATION/METHODS,0.26436781609195403,Accuracy
IMPLEMENTATION/METHODS,0.27586206896551724,(b) layer-wise space
IMPLEMENTATION/METHODS,0.28735632183908044,"0
200
400
600
800
1000
Evaluation 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78"
IMPLEMENTATION/METHODS,0.2988505747126437,Accuracy
IMPLEMENTATION/METHODS,0.3103448275862069,(c) model-wise space
IMPLEMENTATION/METHODS,0.3218390804597701,"0
200
400
600
800
1000
Evaluation 0.72 0.74 0.76 0.78"
IMPLEMENTATION/METHODS,0.3333333333333333,Accuracy
IMPLEMENTATION/METHODS,0.3448275862068966,(d) ours
IMPLEMENTATION/METHODS,0.3563218390804598,Figure 2: Optimization histories across different evolutionary model merge methods.
RESULTS/EXPERIMENTS,0.367816091954023,"3
Experiments"
RESULTS/EXPERIMENTS,0.3793103448275862,"3.1
Experimental Setting"
RESULTS/EXPERIMENTS,0.39080459770114945,"Source Models
We used suzume-llama-3-8B-multilingual-orpo-borda-top75, Barcenas-Llama3-8b-
ORPO, Llama-3-8B-Ultra-Instruct-SaltSprinkle, MAmmoTH2-8B-Plus [5], Daredevil-8B.1 These
five models are fine-tuned from Meta-Llama-3-8B-Instruct [6]."
RESULTS/EXPERIMENTS,0.40229885057471265,"Dataset
For optimization, we used tinyBenchmarks [7] in lm-evaluation-harness [8], which is
open-source and consists of six tasks."
RESULTS/EXPERIMENTS,0.41379310344827586,"Optimization
We used the CMA-ES [4] with default hyperparameter setting as the optimizer.
CMA-ES optimized the DARE-TIES [2, 3] parameters for 5 layers per model, resulting in the
dimension of the design variables being 50. The other layers were spline-interpolated based on the
parameters of the 5 layers. The population size was 15, and the maximum number of evaluations was
1000. The fitness value for each solution was the average of the evaluation values for each task in
tinyBenchmarks [7]. The mean vector of the search distribution in CMA-ES after all evaluations was
used as the optimization result."
RESULTS/EXPERIMENTS,0.42528735632183906,"Baseline Methods
We used the merged models with the DARE-TIES [2, 3] using optimized
parameters by CMA-ES on model-wise and layer-wise spaces as baselines."
RESULTS/EXPERIMENTS,0.4367816091954023,"3.2
Results"
RESULTS/EXPERIMENTS,0.4482758620689655,"Figure 2a displays the optimization histories of all methods, and Figures 2b, 2c, and 2d display the
optimization history and evaluation scores for each method."
RESULTS/EXPERIMENTS,0.45977011494252873,"We observe in Figure 2a that the merged model on our search space achieved a higher score than that
on the layer-wise space. In addition, the improvement of evaluation values by layer-wise space was
slower than other search spaces due to the large search space. Comparing the optimization histories
on the layer-wise space (in Fig. 2b) and the proposed search space (in Fig. 2d), the worse solutions
were generated in the layer-wise space than in our search space. This indicates that the evolutionary
model merge on our search space is more efficient without sacrificing model performance by reducing
the number of parameters to be optimized."
RESULTS/EXPERIMENTS,0.47126436781609193,"On the other hand, the performance of merged models on our search space and the model-wise space
were competitive in Figure 2a. One reason is that the proposed search space had an unnecessarily
larger number of parameters for this task. Figure 3 shows the optimized merge parameters, which
implies that our method can set the fine-grained merge parameters for each layer."
RESULTS/EXPERIMENTS,0.4827586206896552,"Table 1 shows the results of validation set (kaggle leaderboard) for each method. We note that we
modified the prompt of the sample code for evaluation."
RESULTS/EXPERIMENTS,0.4942528735632184,1These models are obtained from Hugging Face https://huggingface.co/.
RESULTS/EXPERIMENTS,0.5057471264367817,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.5172413793103449,Barcenas-Llama3-8b
RESULTS/EXPERIMENTS,0.5287356321839081,-ORPO-weight
RESULTS/EXPERIMENTS,0.5402298850574713,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.5517241379310345,Daredevil-8B-weight
RESULTS/EXPERIMENTS,0.5632183908045977,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.5747126436781609,Llama-3-8B-Ultra-Instruct
RESULTS/EXPERIMENTS,0.5862068965517241,-SaltSprinkle-weight
RESULTS/EXPERIMENTS,0.5977011494252874,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.6091954022988506,MAmmoTH2-8B
RESULTS/EXPERIMENTS,0.6206896551724138,-Plus-weight
RESULTS/EXPERIMENTS,0.632183908045977,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.6436781609195402,suzume-llama-3-8B-multilingual
RESULTS/EXPERIMENTS,0.6551724137931034,-orpo-borda-top75-weight
RESULTS/EXPERIMENTS,0.6666666666666666,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.6781609195402298,Barcenas-Llama3-8b
RESULTS/EXPERIMENTS,0.6896551724137931,-ORPO-density
RESULTS/EXPERIMENTS,0.7011494252873564,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.7126436781609196,Daredevil-8B-density
RESULTS/EXPERIMENTS,0.7241379310344828,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.735632183908046,Llama-3-8B-Ultra-Instruct
RESULTS/EXPERIMENTS,0.7471264367816092,-SaltSprinkle-density
RESULTS/EXPERIMENTS,0.7586206896551724,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.7701149425287356,MAmmoTH2-8B
RESULTS/EXPERIMENTS,0.7816091954022989,-Plus-density
RESULTS/EXPERIMENTS,0.7931034482758621,"0
5
10
15
20
25
30
Layer 0.0 0.2 0.4 0.6 0.8 1.0 Value"
RESULTS/EXPERIMENTS,0.8045977011494253,suzume-llama-3-8B-multilingual
RESULTS/EXPERIMENTS,0.8160919540229885,-orpo-borda-top75-density
RESULTS/EXPERIMENTS,0.8275862068965517,"Figure 3: Weight parameters (upper row) and density parameters (lower row) obtained by evolutionary
model merge on our search space."
RESULTS/EXPERIMENTS,0.8390804597701149,"Table 1: Validation scores on the open leaderboard of the LLM Merging Competition across different
evolutionary model merging methods."
RESULTS/EXPERIMENTS,0.8505747126436781,"No.
Method
Valid Score"
RESULTS/EXPERIMENTS,0.8620689655172413,"1
model-wise
0.59
2
layer-wise
0.51
3
ours
0.54"
CONCLUSION/DISCUSSION ,0.8735632183908046,"4
Conclusion"
CONCLUSION/DISCUSSION ,0.8850574712643678,"In this report, we present the interpolated layer-wise search space for the NeurIPS 2024 LLM Merging
Competition. The proposed search space improved the performance of evolutionary model merge
compared with the layer-wise. While optimizing merging configurations is an attractive approach,
depending on search space, it requires a large number of evaluations for candidate merged models,
leading to a high computational cost. We believe that it is essential to investigate more efficient
optimization methods and sophisticated search space design in future work."
REFERENCES,0.896551724137931,References
REFERENCES,0.9080459770114943,"[1] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model
merging recipes. arXiv preprint arXiv:2403.13187, 2024."
REFERENCES,0.9195402298850575,"[2] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. TIES-Merging: Resolving
interference when merging models. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.9310344827586207,"[3] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing
abilities from homologous models as a free lunch. In International Conference on Machine Learning, 2024."
REFERENCES,0.9425287356321839,"[4] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, pages 75–102. Springer Berlin
Heidelberg, 2006."
REFERENCES,0.9540229885057471,"[5] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. MAmmoTH2: Scaling instructions from the web.
arXiv preprint arXiv:2405.03548, 2024."
REFERENCES,0.9655172413793104,"[6] AI@Meta.
Llama 3 model card, 2024.
https://github.com/meta-llama/llama3/blob/main/
MODEL_CARD.md."
REFERENCES,0.9770114942528736,"[7] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin.
tinybenchmarks: evaluating LLMs with fewer examples. In International Conference on Machine Learning,
2024."
REFERENCES,0.9885057471264368,"[8] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris
Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,
Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
2024."
