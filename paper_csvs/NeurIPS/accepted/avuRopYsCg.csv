Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00641025641025641,"We propose a logic-informed knowledge-driven modeling framework for human
movements by analyzing their trajectories. Our approach is inspired by the fact that
human actions are usually driven by their intentions or desires, and are influenced
by environmental factors such as the spatial relationships with surrounding objects.
In this paper, we introduce a set of spatial-temporal logic rules as knowledge
to explain human actions. These rules will be automatically discovered from
observational data. To learn the model parameters and the rule content, we design
an expectation-maximization (EM) algorithm, which treats the rule content as
latent variables. The EM algorithm alternates between the E-step and M-step:
in the E-step, the posterior distribution over the latent rule content is evaluated;
in the M-step, the rule generator and model parameters are jointly optimized by
maximizing the current expected log-likelihood. Our model may have a wide
range of applications in areas such as sports analytics, robotics, and autonomous
cars, where understanding human movements are essential. We demonstrate the
model’s superior interpretability and prediction performance on pedestrian and
NBA basketball player datasets, both achieving promising results."
INTRODUCTION,0.01282051282051282,"1
Introduction"
INTRODUCTION,0.019230769230769232,"For a human, although the exhibited movements can be complex, the logic behind the actions is
usually simple, clear, and can be generalized [17]. The logic rules present a compact and high-level
knowledge representation, defining what actions tend to be executed under what conditions. It
emphasizes reasoning, and encourages structuring the world in terms of objects, properties, and
relations [22]. There has been a great interest and business value in unveiling the human logic from
the observational movements and actions [18]. We provide two motivating examples below.
In sports analytics, understanding each player’s behavior preferences or tendencies under various
scenarios will provide coaches with valuable information [1]. Usually, coaches need to watch the
game or training videos for hundreds of hours before they can summarize the discoveries into
compact principles. Could we design an algorithm to synthesize these principles from the raw
action data automatically? One can imagine, such a tool will significantly reduce the workload of
coaches, providing more granular insight into each player’s capabilities and strategies, and aiding in
personalized training and match strategy design [15].
In self-driving car, it’s essential to enable the self-driving cars to “read human’s mind"" like humans.
This requires the self-driving cars to automatically understand human intentions and reasoning when"
INTRODUCTION,0.02564102564102564,"they are running on the same roads with human drivers [2]. If self-driving cars can automatically
distill logic rules from their observed low-level noisy human actions and movement trajectories, it
will increase the technical reliability and accelerate the widespread use of self-driving cars.
For human actions, lots of the governing rules would be regarding the spatial-temporal relation with
the surrounding environments and their intentions [9]. For example, when a basketball player with
a ball is within the scoring range, his/her action, such as shoot, pass, or triple threat, is influenced
by historical and current surrounding factors, such as the current locations of the player and the
defenders, the time elapses of the game, the success shooting rate of the player in today’s game, and
so on. The quick decision made by the player is actually reflecting a composition of all these factors,
which can be described as a collection of spatial-temporal logic rules in our model.
Formally, in our introduced spatial-temporal logic rules, the logic variable (i.e., predicate) set will
include spatial-temporal relation predicates, in addition to the commonly defined object property and
relation predicates. The rule content will capture the spatial relation of the object with surrounding
objects, as well as the temporal ordering constraints of the events.
Our methods have the following distinct features:
From the modeling perspective: (i) Our human action model is a rule-based probabilistic model,
which treats each hidden rule as a “soft"" constraint. We assume each rule will be executed by humans
with probabilities, and this tolerates the uncertainties in data. (ii) Our model directly uses low-level,
fine-grained, and (may) irregularly-spaced action times and locations (i.e., original 3d coordinates)
as inputs, as opposed to other rule-based models, where one needs to first extract relational data as
inputs. (iii) Our spatial and temporal predicates are also probabilistic. For predicates such as “left of""
or “before"", we model them as kernel functions with learnable variables. In this way, our introduced
spatial-temporal predicates are smooth functions of the input locations and times, which increases
model flexibility.
From the learning perspective: We propose a tractable and differentiable algorithm that can jointly
learn the rule content and model parameters from observational data. The learning framework is
designed to maximize the likelihood of the human action trajectories. Specifically, we propose
to use a neural rule generator to generate the spatial-temporal logic rule set. Our continuous rule
generator parameters will be optimized in a differentiable way. The overall learning procedure is
an expectation-maximization (EM) algorithm, where we treat the rule set as latent variables. In
the E-step, the posterior distribution over the latent rule set is evaluated. In the M-step, both the
rule generator parameters and the model parameters are optimized by maximizing the expected
log-likelihood with respect to the current posterior. We demonstrated the promising performance of
our model in terms of human action prediction and explanation on two interesting real datasets."
RELATED WORK,0.03205128205128205,"2
Related Work"
RELATED WORK,0.038461538461538464,"Logic Rule Learning.
Learning logic rules from raw data has been widely studied for various
downstream tasks, such as motion inference [4] and healthcare analysis [11]. Learning rules via an
exact search requires enumerating all combinations of the logic predicates and is intractable in most
problems. One has to design heuristic searching algorithms by leveraging some structural properties
of the problems. For example, Dash et al. [5] formulated a convex rule learning problem and proposed
a column generation algorithm to expand the rule set gradually. Wang et al. [25] designed a Bayesian
framework for learning rule classifiers and derived bounds on the support of rules in a MAP solution.
Recently, Yang et al. [28] proposed an interesting end-to-end differentiable approach (Neural LP) to
learn the parameters and structure of logical rules. Qu et al. [19], and Sadeghian et al. [22] proposed
an efficient logic rule mining algorithm based on the knowledge graph data. However, none of these
advanced rule mining methods can directly work on spatial-temporal human action data when the
inputs are raw event 3d coordinates and types."
RELATED WORK,0.04487179487179487,"Spatio-Temporal Dynamics for Event Data.
Since the human actions are irregular spatial-
temporal event data, we also briefly discuss probabilistic models for such event sequences. Modeling
the spatial-temporal dynamics of discrete events is foundational in many scientific fields and ap-
plications [20]. Shen et al. [23] proposed a novel deep learning model for spatial-temporal events
such as taxi data and achieved promising prediction accuracy. Zhou et al. [31] integrated deep
learning methods with spatiotemporal point processes and modeled the intensity function as a latent
stochastic process. Chen et al. [3] deployed two novel architectures, including jump and attentive
continuous-time normalizing flows, to learn the dynamics of the spatiotemporal event data. Repe et al."
RELATED WORK,0.05128205128205128,"[21] learned canonical spatiotemporal point cloud representation using a latent ODE and continuous
normalizing flows to generate shapes continuously in spacetime. However, these spatial-temporal
event models are governed by hard-to-interpret dynamic functions and cannot be generalized to
model human action events. Could we propose a model with logic-informed dynamic functions to
explain the spatial-temporal human action events?"
OUR MODEL,0.057692307692307696,"3
Our Model"
OUR MODEL,0.0641025641025641,"3.1
Data: Human Actions Recorded as Spatial-Temporal Event Sequences"
OUR MODEL,0.07051282051282051,"Consider a set of objects, denoted as C. For the object c ∈C, its trajectories and the key actions
observed up to t can be summarized as a sequence of temporally ordered events:
Hc
t−= {ec
1 = (tc
1, sc
1, κc
1), . . . , ec
n = (tc
n, sc
n, κc
n) | tc
n < t},
(1)
where t ∈R+ is the time, s ∈R2 is the location, and κ ∈K is the event (i.e., action) type."
DEFINITION OF SPATIAL-TEMPORAL PREDICATES,0.07692307692307693,"3.2
Definition of Spatial-Temporal Predicates"
DEFINITION OF SPATIAL-TEMPORAL PREDICATES,0.08333333333333333,"Static Predicate
Given the object set C, the predicate is defined as the property or relation of
objects, which is a logic function as follows:
X(·) : C × C · · · ×C 7→{0, 1} .
(2)
For example, Smokes(c) is a property predicate and Friend(c, c′) is a relation predicate."
DEFINITION OF SPATIAL-TEMPORAL PREDICATES,0.08974358974358974,"Temporal relation predicates
They can be used to define the temporal relations of two action
events. We consider three types of temporal relation predicates {before, equal, after} as:
RBefore(t1, t2) = 1{t1 −t2 < 0},
RAfter(t1, t2) = 1{t1 −t2 > 0},
REqual(t1, t2) = 1{t1 = t2}.
(3)"
DEFINITION OF SPATIAL-TEMPORAL PREDICATES,0.09615384615384616,"We will treat the temporal relation predicate as either a boolean variable or a real-valued function. If
the time information is imprecisely recorded, we can parameterize the temporal relation predicates as
temporal kernel functions that map to [0, 1], which is a function of t1, t2 with learnable parameters."
DEFINITION OF SPATIAL-TEMPORAL PREDICATES,0.10256410256410256,"Spatial-Temporal Predicate
In our paper, we extend the above static predicates to spatial-temporal
predicates, which include spatial-temporal property predicates and spatial-temporal relation predi-
cates.
Specifically, the spatial-temporal property predicates are defined as:
X(·) : C × · · · × C × T × S 7→{0, 1} .
(4)
For example, PickupKey (c, t, s) is a spatial-temporal property predicate. Suppose an entity c1
picked up the key at time t1 in location s1, then the predicate will be grounded as True (1) at
(c1, t1, s1), i.e., PickupKey(c1, t1, s1) = 1; otherwise it is false.
Given the observational human action data, the grounded predicate {PickupKey (c, t, s)}t=1,2,...
can be modeled as a sequence of discrete events – when the predicate becomes True, an event happens.
In general, the grounded spatial-temporal property predicate is a discrete event sequence, where the
event occurrence times and locations are irregular.
The spatial-temporal relation predicates are introduced to define the spatial and temporal relations of
two entities. Specifically, they are defined as:
R(·, ·) : (C × T × S) × (C × T × S) 7→{0, 1} .
(5)
Spatial-temporal relation predicates are logic variables indicating the spatial-temporal relations of
two objects, where we further divide them into temporal relation predicates, static spatial relation
predicates, and dynamic spatial relation predicates. More details can be found in Appendix.
It is noteworthy that all these boolean predicates can be converted to probabilistic ones. We can soften
these logic functions by kernel functions with learnable parameters to tolerate uncertainties in data."
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.10897435897435898,"3.3
Definition of Spatial-Temporal Logic Rules"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.11538461538461539,"We will consider spatial-temporal logic rules where the body part contain spatial-temporal predicates
as relation constraints. For example, a sensible rule will look like:"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.12179487179487179,"f :YTurnAround(c, t, s) ←XPickUpKey(c, t, s)
^"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.1282051282051282,"RInFront((c′, t, s′), (c, t, s))
^
RBehind((c′′, t, s′′), (c, t, s)),
(6)"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.1346153846153846,"where c ∈Cperson, c′ ∈Cblock, and c′′ ∈Ckey. “person”, “block” and “key” are the specific instances
in the object set C, and this rule represents that a person wants to pick up a key, while one block is in
front of him and the key is behind him, so he turns around. We utilize this example to manifest the
meaning of logic rules. In general, the spatial-temporal logic rule in our paper is defined as a logical
connectives of predicates, including property predicates and spatial-temporal relation predicates:"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.14102564102564102,"f : Y (v) ←
^"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.14743589743589744,"Xproperty∈Xf
Xproperty(v)
^"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.15384615384615385,"Rspatial-temporal∈Rf
Rspatial-temporal(v′, v),
(7)"
DEFINITION OF SPATIAL-TEMPORAL LOGIC RULES,0.16025641025641027,"where Y (v) is the head predicate evaluated at the entity-time-location triplet v, Xf is the set of
property predicates defined in rule f, and Rf denotes the set of spatial-temporal relation predicates
defined in rule f."
LOGIC-INFORMED ACTION EVENT MODELS,0.16666666666666666,"3.4
Logic-Informed Action Event Models B A C x y �2 �1 �3 �4
�5"
LOGIC-INFORMED ACTION EVENT MODELS,0.17307692307692307,"Figure 1: Illustration of feature
construction using a simple logic
formula with temporal relation
predicate (t1 < t2), f : Y ←
A ∧B ∧C ∧(A Before B). The
rule defines the template to gather
combinations of the body predi-
cate history events. Here pred-
icate A has 2 events and predi-
cate B has 1 event, the tempo-
ral relation constraint would lead
to valid combinations (also called
“paths"").
This type of feature
construction can be extended to
spatial-temporal cases, where we
count the valid paths as the fea-
ture."
LOGIC-INFORMED ACTION EVENT MODELS,0.1794871794871795,"We consider a setting where we can fully observe the trajectories
of all the moving objects, including their real-time locations and
key actions (i.e., events), denoted as Ht. We aim to propose a logic-
informed spatial-temporal model to predict and explain the action
type given the entity-time-location triplet v = (c, t, s) (i.e., query)
and Ht. The main idea is to construct the model features using
spatial-temporal logic rules, as defined in Eq. (7). Intuitively, given
the entire trajectories Ht and the query v = (c, t, s), the body part of
the rule defines the evidence to be selectively gathered from history
to deduce the event type for query entity v = (c, t, s). Assume that
for each possible event type κ ∈K, there exist multiple rules such
as Eq. (7) to explain its occurrence, with κ being the head predicate.
Given an individual rule as Eq. (7), we propose to build the feature
that is conditional on history and query as"
LOGIC-INFORMED ACTION EVENT MODELS,0.1858974358974359,"ϕf(κ |v, Ht) = sign(κ ∈f) ·
X"
LOGIC-INFORMED ACTION EVENT MODELS,0.19230769230769232,"path∈{Ht,v}
gf (path) ,
(8)"
LOGIC-INFORMED ACTION EVENT MODELS,0.1987179487179487,"where we introduce a function gf(·) to check the body conditions
of f given a “path"". We use a simple example to explain how to
compute features, as shown in Figure 1. As illustrated, the feature
computes the valid total number of “paths"" given the data and query.
Suppose there is a rule set Fκ, where the event κ is the head predicate.
All the rules will play together to reason about the occurrence of κ.
For each f ∈Fκ, one can compute the features as above. Given the
rule set Fκ, we model the probability of the event κ as a log-linear function of the features, i.e.,"
LOGIC-INFORMED ACTION EVENT MODELS,0.20512820512820512,"p(κ|v, Ht) ∝exp
X"
LOGIC-INFORMED ACTION EVENT MODELS,0.21153846153846154,"f∈Fκ wf · ϕf(κ|v, Ht)

,
(9)"
LOGIC-INFORMED ACTION EVENT MODELS,0.21794871794871795,"where w = [wf]f∈F ≥0 are the learnable weight parameters associated with each rule. All the
model parameters can be learned by maximizing the likelihood, which can be computed using the
above Eq. (9). We intend to train a rule generator pθ and an evaluator pw to maximize the likelihood
of training data as:"
LOGIC-INFORMED ACTION EVENT MODELS,0.22435897435897437,"max
θ,w O(θ, w) = E(κ,v,Ht)[log Epθ[pw(κ|v, Ht)]].
(10)"
LOGIC-INFORMED ACTION EVENT MODELS,0.23076923076923078,More details can be found as follows.
OUR LEARNING ALGORITHM,0.23717948717948717,"4
Our Learning Algorithm"
OUR LEARNING ALGORITHM,0.24358974358974358,"Our goal is to jointly learn the set of spatial-temporal logic rules {Fκ}κ∈K and their weights by the
maximum likelihood method, where each rule has a general form as Eq. (7).
To discover each rule, the algorithm needs to navigate through the combinatorial space considering
all the combinations of the property predicates and their spatial and temporal relations. To address
this computational challenge, we propose a tractable (functional) EM algorithm that treats the rule
set as latent variable z. The rules will be generated by a hidden neural rule generator. The overall
learning framework alternates between an E-step, where the posterior distribution of the latent rule
space is evaluated (rule generation), and the M-step, where the model parameters and rule generator
parameters are optimized. Please refer to Figure. 2 for an illustration."
OUR LEARNING ALGORITHM,0.25,Rule Generator
OUR LEARNING ALGORITHM,0.2564102564102564,"( | )
p
z v
 A B C ... 1 N
 ..."
OUR LEARNING ALGORITHM,0.26282051282051283,Logic Rules ˆz Score
OUR LEARNING ALGORITHM,0.2692307692307692,Trajectory
OUR LEARNING ALGORITHM,0.27564102564102566,"Input
Raw Data"
OUR LEARNING ALGORITHM,0.28205128205128205,Rule Generating
OUR LEARNING ALGORITHM,0.28846153846153844,Reasoning Evaluator
OUR LEARNING ALGORITHM,0.2948717948717949,"( | , )
w
p
v z

Intention
A B C"
OUR LEARNING ALGORITHM,0.30128205128205127,Prediction
OUR LEARNING ALGORITHM,0.3076923076923077,"Decoder ... 1ˆ ˆN
 ..."
OUR LEARNING ALGORITHM,0.3141025641025641,Logic Reasoning
OUR LEARNING ALGORITHM,0.32051282051282054,"1
2
(
,
,...)
"
OUR LEARNING ALGORITHM,0.3269230769230769,",
ˆ ~
(
| , )
k
w
k
z
p
z
v


High-quality Rule A
B
C D D
D ∧
∨"
OUR LEARNING ALGORITHM,0.3333333333333333,Entities
OUR LEARNING ALGORITHM,0.33974358974358976,Spatio-temporal
OUR LEARNING ALGORITHM,0.34615384615384615,Relation
OUR LEARNING ALGORITHM,0.3525641025641026,Symbol
OUR LEARNING ALGORITHM,0.358974358974359,Event-type
OUR LEARNING ALGORITHM,0.36538461538461536,"Head predicate
A
B
C
D A B C
D"
OUR LEARNING ALGORITHM,0.3717948717948718,"Figure 2: The overview of our proposed framework. It contains two important processes: rule generating and
logic reasoning. Given the past motion of each entity on a scene over the last few seconds, the rule generator
generates logic rules for the reasoning predictor. The reasoning predictor takes the generated rules as input,
and predict the intention of each entity. It is optimized by EM algorithm. In the E-step, a set of top K rules are
selected from all generated rules via posterior inference. Finally in the M-step, the rule generator is updated to
be consistent with the high-quality rules identified in E-step."
OUR LEARNING ALGORITHM,0.3782051282051282,"Our goal is to maximize the likelihood of the observed human action events {κ(i)}i=1,...,n. Using the
chain rule, we have"
OUR LEARNING ALGORITHM,0.38461538461538464,"log pw({κ(i)}i=1,...,n) = n
X"
OUR LEARNING ALGORITHM,0.391025641025641,"i=1
log pw(κ(i) | v(i), Ht(i−1)).
(11)"
OUR LEARNING ALGORITHM,0.3974358974358974,"To simplify the notation, we will use pw(κ(i)) to stand for pw(κ(i) | v(i), Ht(i−1)) in the following.
Given a latent rule set z, we have to marginalize the posterior of z to get the above log-likelihood.
However, the exact inference of z is intractable. We will introduce an amortized recognition network
pθ(z|κ(i)) to approximate the true posterior. We have"
OUR LEARNING ALGORITHM,0.40384615384615385,"log pw(κ(i)) = DKL(pθ(z|κ(i))||pw(z|κ(i))) + L(θ, w; κ(i)),
(12)
where the first term is the KL divergence of the approximate from the true posterior, and the second
term L(θ, w; κ(i)) is the variational lower bound (ELBO). It can be represented as:"
OUR LEARNING ALGORITHM,0.41025641025641024,"L(θ, w; κ(i)) = −DKL(pθ(z|κ(i))||pw(z)) + Epθ(z|κ(i))[log pw(κ(i)|z)].
(13)"
OUR LEARNING ALGORITHM,0.4166666666666667,"And log pw(κ(i)) ≥L(θ, w; κ(i)). The bound becomes tight when the approximate posterior matches
the true one. Our goal is to optimize the variational parameters θ and model parameters w from the
ELBO lower bound."
RULE GENERATOR,0.4230769230769231,"4.1
Rule Generator"
RULE GENERATOR,0.42948717948717946,"We deploy Transformer-based framework to model the rule generator pθ. We define the distribution
of a set of rules as follows:"
RULE GENERATOR,0.4358974358974359,"pθ(z | v, Ht) = Ψ(z|N, Transθ(v, Ht)),
(14)
where Ψ(·) is multinomial distributions, N is the number of the top rules, and Transθ(v, Ht) defines
a distribution over compositional rules with spatial-temporal states. The generative process of the
rule set is quite intuitive, where we simply generate N rules to form z. In fact, this pθ(z | v, Ht) is a
flexible posterior approximation function, which will be optimized by the EM type algorithm.
We choose transformer over graph neural network (GNN) as our baseline because transformer
architectures are based on a self-attention mechanism that is able to capture long-range relationships,
as opposed to recurrent neural networks that process sequence elements recursively and can only take
into account short-term context. Note that the graph operations in GNN are designed to learn node
representations on the fixed and homogeneous graphs. The limitations especially become problematic
when learning representations on a changeable graph that consists of various types of nodes and
edges."
RULE EVALUATOR,0.4423076923076923,"4.2
Rule Evaluator"
RULE EVALUATOR,0.44871794871794873,"Eq. (9) is our rule evaluator (suppose we know the rule content). Here we assume the rule content is
latent, and the rule evaluator is given as"
RULE EVALUATOR,0.4551282051282051,"pw(κ|v, z, Ht) =
exp
P"
RULE EVALUATOR,0.46153846153846156,"f∈zκ wf · ϕf(κ|v, Ht)
 P"
RULE EVALUATOR,0.46794871794871795,"κ′ exp
P"
RULE EVALUATOR,0.47435897435897434,"f∈zκ′ wf · ϕf(κ′|v, Ht)
.
(15)"
OPTIMIZATION,0.4807692307692308,"4.3
Optimization"
OPTIMIZATION,0.48717948717948717,"We optimize the rule generator pθ and reasoning evaluator pw to maximize the objective in Eq. (10).
At each training iteration, we first update the reasoning predictor pw according to some rules generated
by the generator, and then update the rule generator pθ.
In our network, the latent rule set will be automatically discovered. The best set of logic rules is
approximately obtained by sampling and preserving the top-K rules according to their posterior
probabilities. Specifically, as shown in Eq. (14), the posterior probabilities of the latent rule z is
obtained by a Transformer type of encoder, which maps the input observed action trajectories to a
latent explanatory rule space. Each candidate rule is generated in the latent rule space token-by-token
(token means logic variable/predicate in our setting) in a sequential manner and meanwhile the
posterior probability of each rule sequence can be evaluated. When optimizing the evaluator, we
draw several rules ˆz for each query and let the evaluator uses ˆz to predict κ. For each query, we
try to identify top-K rules zI from all generated rules ˆz. It is accomplished by taking into account
the posterior probabilities of each subset of logic rules zI with prior from the rule generator pθ and
likelihood from the reasoning predictor pw. Specifically, when a series of rules produced from the
rule generator pθ, we calculate the weights of each rule z(i) as follows:"
OPTIMIZATION,0.4935897435897436,"J(z(i)) = {pw(κ|z(i)) −
1
|A|} + log Transθ(z(i)|v, Ht),
(16)"
OPTIMIZATION,0.5,"where A is the set of all candidate event type inferred by logic rules. Transθ(z(i)|v, Ht) is the
probability of rule computed by the generator. For a subset of rules zI ⊂ˆz, the log-probability can
be approximated as: log pθ,w(zI|v, Ht) ≈P"
OPTIMIZATION,0.5064102564102564,"z(i)∈zI J(z(i)) + log Ψ(zI|N, Transθ(v, Ht)) + const. This
equation inspired us to use the distribution q(zI) ∝exp(P"
OPTIMIZATION,0.5128205128205128,"z(i)∈zI J(z(i)) + log Ψ(zI|N, Transθ(v, Ht)))
as approximation of the posterior. Each rule z(i) sampled from q(zI) independently can be formed
with N logic rules."
OPTIMIZATION,0.5192307692307693,"Clearly, J(z(i)) can be regarded as the quality of candidate rules, with consideration of the evaluator
pw. It is calculated as the contribution of a rule to the correct event type minus the average contribution
of this rule to the other candidate responses. A rule is more significant if it obtains a higher score to
the correct event type and a lower score to other potential predictions.
After getting several high-quality rules from training data, we further utilize these rules to update the
parameters of rule generator pθ. Concretely, we regard the generated high-quality rules as part of
training data, and update the rule generator by maximizing the log-likelihood as follows:
O(θ) = log pθ(zI|v, Ht) =
X"
OPTIMIZATION,0.5256410256410257,"z(i)∈zI
log Transθ(v, Ht) + const.
(17)"
OPTIMIZATION,0.532051282051282,"By learning to generate high-quality rules, the rule generator will reduce the search area and produce
better empirical results for the reasoning predictor."
EXPERIMENTS,0.5384615384615384,"5
Experiments"
EXPERIMENTS,0.5448717948717948,"In this section, we provide some implementation details and show ablation studies as well as
visualization to evaluate the performance of our framework. We compare our model with several
state-of-the-art approaches, including PECNet [14], NMMP [7], STGAT [8], SOPHIE [22], STAR
[29], Y-Net [13], MID [6], Social-SSL [24], Social-VAE [26], Social-Implicit [16], and NSP-SFM
[30]."
IMPLEMENTATION,0.5512820512820513,"5.1
Implementation"
IMPLEMENTATION,0.5576923076923077,"We follow the same data prepossessing strategy as PECNet [14] for our method. All models were
trained and tested on the same split of the dataset, as suggested by the benchmark. We train the
network using Adam optimizer with a learning rate of 0.001 and batch size 16 for 500 epochs."
IMPLEMENTATION,0.5641025641025641,Figure 3: Rule discovery and weight learning results on 4 synthetic datasets (2.4K seqs).
IMPLEMENTATION,0.5705128205128205,"Table 1: Quantitative results (ADE20/FDE20, and accuracy) of trajectory prediction in NBA dataset.
The bold/underlined font represent the best/second best result."
IMPLEMENTATION,0.5769230769230769,"Times
Y-Net
MID
NSP-SFM
Social-SSL
Social
Implicit
Social
VAE
Ours"
S,0.5833333333333334,"1.0s
0.38/0.48
0.45/0.59
0.41/0.52
0.48/0.61
0.45/0.53
0.49/0.66
0.30/0.40
2.0s
0.63/0.93
0.76/1.06
0.67/0.94
0.76/1.08
0.72/0.96
0.77/1.11
0.58/0.88
3.0s
0.94/1.34
1.06/1.40
0.98/1.35
1.06/1.43
1.00/1.39
1.11/1.46
0.87/1.31
4.0s
1.17/1.61
1.32/1.74
1.18/1.63
1.35/1.78
1.19/1.66
1.37/1.79
1.13/1.60
Acc.
0.69
0.65
0.70
0.68
0.69
0.64
0.73"
DATASETS,0.5897435897435898,"5.1.1
Datasets"
DATASETS,0.5961538461538461,"Stanford Drone Dataset.
This dataset consists of more than 11,000 persons in 20 scenes captured
from the campus of Stanford University in bird’s eye view. We follow the [27] standard train-test
split, and predict the future 4.8s (12 frames) using past 3.2s (8 frames). Note that SDD dataset
does not provide explicit pedestrian’s action. Instead, we record them as an abstract encoding of the
pedestrian’s speed and location. The action contains [left, right, straight, turn around]."
DATASETS,0.6025641025641025,"NBA SportVU Dataset.
It is collected by NBA using the SportVU tracking system, which reports
the trajectories of the ten players and the ball in real basketball games. Each trajectory contains the
2D positions and velocities of the offensive team, consisting of 5 players. We predict the future 10
timestamps (4.0s) based on the historical 5 timestamps (2.0s). Each player’s action contains [left,
right, straight, turn around, pass, shoot]."
METRICS,0.6089743589743589,"5.1.2
Metrics"
METRICS,0.6153846153846154,"Here, we adopt two metrics for evaluation: Average Displacement Error (ADE) and Final Displace-
ment Error (FDE). Specifically, ADEN is defined as the minimum average distance between N
predicted trajectories and the ground truth over all the involved entities within the prediction horizon.
FDEN is defined as the minimum deviated distance of N predicted trajectories at the last predicted
time step. Moreover, we also calculate the accuracy and F1 score of event-types predicted from each
network."
SYNTHETIC EXPERIMENT,0.6217948717948718,"5.2
Synthetic Experiment"
SYNTHETIC EXPERIMENT,0.6282051282051282,"We follow [10] to verify our model’s rule discovery ability on synthetic datasets with a known set of
ground-truth rules and weights. The synthetic events are generated from TLPPs [12] with a known
set of rules and weights. We prepared 4 synthetic datasets, and each setting corresponds to different
rule weights, rule length and number, type of temporal relation, and intensity of free predicates.
Note that it was originally utilized for the temporal point process, so we modify it by adding spatial
variables (such as “left, right, front, and behind”) to fit in our settings. The weight learning results on
4 synthetic datasets are shown in the Figure 3, where 2400 sequences are used for evaluation. Each
plot at the bottom compares the genuine rule weights to the learned rule weights and reports the Mean"
SYNTHETIC EXPERIMENT,0.6346153846153846,"(a) GT
(b) NMMP
(c) Ours"
SYNTHETIC EXPERIMENT,0.6410256410256411,"Figure 4: Visual results on the NBA dataset. We plot the predicted trajectories from the state-of-the-art
method NMMP (b), Ours (c) and ground truth (a). The red/blue color represents players of two teams
and the green color represents the basketball. Light color represents the past trajectory."
SYNTHETIC EXPERIMENT,0.6474358974358975,"Table 2: Quantitative results (ADE20, FDE20 and F1 score) of trajectory prediction in SDD dataset.
The bold/underlined font represent the best/second best result."
SYNTHETIC EXPERIMENT,0.6538461538461539,"Metrics
PECNet Y-Net
MID
NMMP STGAT SOPHIE Social
SSL
NSP-SFM STAR
Ours"
SYNTHETIC EXPERIMENT,0.6602564102564102,"ADE20
20.03
7.85
7.61
14.12
14.43
15.56
6.63
6.52
10.76
6.41
FDE20
33.86
11.85 14.30
20.68
22.59
24.32
12.23
10.61
17.03 10.23
F1 score
0.37
0.54
0.49
0.41
0.33
0.39
0.53
0.58
0.56
0.59"
SYNTHETIC EXPERIMENT,0.6666666666666666,"Absolute Error (MAE). As we can see, almost all truth rule weights are learned correctly. It shows an
accurate performance of our algorithm in terms of both the rule discovery and parameter learning."
ANALYSIS,0.6730769230769231,"5.3
Analysis"
ANALYSIS,0.6794871794871795,"We compare our method with several state-of-the-art approaches, and Table 2 presents the qualitative
results on the SDD dataset. The proposed model achieved the best performance in ADE, FDE and
accuracy. We observe that our method significantly outperforms all baselines measured by ADE and
FDE. It achieves an ADE of 6.41 and FDE of 10.23 at K = 20 in SDD datset, which exceeds the
previous state-of-the-art performance of Y-Net [13] by 18.3% on ADE and 13.6% on FDE. In Table 1,
our method also achieve higher performance than Y-Net in NBA dataset. This is because Y-Net firstly
assume that the waypoint lies on a straight line segment connecting the sampled goal and the past
trajectory, then use a multivariate Gaussian prior centered at the assumed location. This assumption
can not well suit in other complex conditions, such as the trajectory of players in the NBA dataset.
Compared with MID [6], we also obtain 15.7% ADE and 28.4% FDE improvement. Note that they
carefully design a Transformer-based architecture to model the temporal dependencies in trajectories,
but ignore the spatial correlation of agents. Our transformer-based network aims at generating high-
quality logic rules based on spatial-temporal relation under the principle to maximize the likelihood
of the observational human actions. For NSP-SFM [30], it obtains high performance in SDD dataset
but can not achieve the same level in NBA dataset. It combines physics with deep learning for
trajectory prediction and accommodates arbitrary physics models. The limitation of it lies in specific
physics models, such as pedestrians, and is deterministic. So it can not deal with some strategy-based
conditions, including basketball and football games. But our logic-learning method tries to use a set
of spatial-temporal logic rules with intention variables involved as principles to model the dynamics
of human actions, not restrained into specific conditions.
Further, The proposed model achieved the best scores in F1 score, which is an balanced metric
considering both recall and precision. This is because the rule generator and evaluator can collaborate
with each other to reduce search space and learn better rules. More experiments and ablation studies
can be found in Supplementary Material."
ANALYSIS,0.6858974358974359,basketball player
ANALYSIS,0.6923076923076923,basketball
ANALYSIS,0.6987179487179487,"Player A Rule ①: Left(B,A)∧Right(C,A)∧Front(D,A)→Pass(A,B)
Explanation:
•  Player A is carrying the ball
•  Player A  is defended by player C and player D
•  Player B is on the left of player A
•  Player A passes the ball to player B"
ANALYSIS,0.7051282051282052,"Player B Rule ②: Left(E,B)∧Right(D,B)→Go_Front(B)
Explanation:
•  Player E and player F is moving to the player B
•  Player B is defended by player E on left side
•  Player B is defended by player D on right side
•  Player B goes front and carries the ball B A ② E F D C
G"
ANALYSIS,0.7115384615384616,"Player B Rule ③: Left(E,B)∧Right(G,B)∧Front(F,B)→Shoot(B)
Explanation:
•  Player B is defended by player E on left side
•  Player B is defended by player G on right side
•  Player B is defended by player F on front side
•  Player B shoots at the basket B A ③ E F D C G B A ① E F D C
G"
ANALYSIS,0.717948717948718,Figure 5: Visualization and explanation of logic rules in NBA dataset.
ANALYSIS,0.7243589743589743,"To evaluate the robustness of our method, We added some noise and randomly removed several tracks
in the NBA dataset, then evaluated all methods in Table 3 (“Ours*” is the original results without
noises or missing tracks). As we can see, our method still achieves the best performance. Moreover,
by comparing “Ours” and “Ours*”, we can see that the quality of tracks has less influence on our
method, which demonstrates the robustness of our method.
Our method not only outperforms all the above comparative methods in quantitative evaluations, but
also produces visually pleasing results among them. Figure 4 compares the predicted trajectories
of NMMP, ground-truth (GT) and our methods on NBA dataset. It is obvious that our method
produces more precise predictions than state-of-the art method NMMP. This is because our proposed
spatial-temporal logic rules can actually captures the spatial relation of the players with surrounding
players, as well as the temporal ordering constraints of the events."
GENERATED LOGIC RULES,0.7307692307692307,"5.4
Generated Logic Rules"
GENERATED LOGIC RULES,0.7371794871794872,"We add visualization and explanation about the logic rule and corresponding actions from NBA
dataset in Figure 5. Note that the static spatial relation {Left(B,A)} represents that the player B is
on the left of player A. And the dynamic spatial relation {Away(A,E)} means that player A is getting
away from player E. We can see that these logic rules are meaningful and diverse. In this picture,
player A is defended by two players and then passes the ball to player B. Player B goes front and
crossover to bypass three defenders and shoot at the basket. Our rule can actually represent their
offensive strategy. In fact, our framework can actually adapt to some complex motions, such as
cutting toward the ball, because these spatial-temporal predicates are fed into the rule generator and
evaluator to obtain high-quality rules to explain the intention of players."
GENERATED LOGIC RULES,0.7435897435897436,Table 3: Robustness of our method in NBA dataset in some noise.
GENERATED LOGIC RULES,0.75,"Methods
1.0
2.0
3.0
4.0
ADE FDE
ADE FDE ADE FDE ADE FDE
Y-Net
0.51
0.62
0.81
1.11
1.08
1.49
1.37
1.86
NSP-SFM
0.48
0.63
0.82
1.13
1.06
1.46
1.36
1.82
Ours
0.31
0.48
0.67
0.91
0.93
1.39
1.18
1.66
Ours*
0.30
0.40
0.58
0.88
0.87
1.31
1.13
1.60"
LIMITATION,0.7564102564102564,"6
Limitation"
LIMITATION,0.7628205128205128,"It’s challenging to define some complex predicates with richer meanings for different datasets.
Given a more informative dataset, our method can discover more principles-like complex rules.
Our motivation is to consider the spatial-temporal relation between pedestrians and generate some
high-quality logic rules to explain their behaviors. Although we only choose some simple actions in
our experiments, they can bring some benefits for understanding the principles of biological agents’
behaviors. Our framework is suitable for more complex conditions, supposing more sophisticated
action predicates can be obtained from the data."
CONCLUSION,0.7692307692307693,"7
Conclusion"
CONCLUSION,0.7756410256410257,"We proposed a framework for learning intrinsic spatial-temporal logic rules for explaining human
actions. We regard logic rules as latent variables, and the rule generator as well as the rule evaluator are
jointly learned with EM-based algorithm. In the experiments, our method can analyze the biological
movement sequence of pedestrians and players, and obtained novel insights from generated logic
rules. In the future, we plan to incorporate other physical laws into the models, such as conservation
of energy and momentum to enhance robustness of our model."
ACKNOWLEDGEMENT,0.782051282051282,"8
Acknowledgement"
ACKNOWLEDGEMENT,0.7884615384615384,"Shuang Li’s research was in part supported by the NSFC under grant No. 62206236, Shenzhen
Science and Technology Program JCYJ20210324120011032, National Key R&D Program of China
under grant No. 2022ZD0116004, and Guangdong Key Lab of Mathematical Foundations for
Artificial Intelligence."
REFERENCES,0.7948717948717948,References
REFERENCES,0.8012820512820513,"[1] David Adam. Science and the world cup: how big data is transforming football. Nature,
611(7936):444–446, 2022."
REFERENCES,0.8076923076923077,"[2] Claudine Badue, Rânik Guidolini, Raphael Vivacqua Carneiro, Pedro Azevedo, Vinicius B
Cardoso, Avelino Forechi, Luan Jesus, Rodrigo Berriel, Thiago M Paixao, Filipe Mutz, et al.
Self-driving cars: A survey. Expert Systems with Applications, 165:113816, 2021."
REFERENCES,0.8141025641025641,"[3] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Neural spatio-temporal point processes.
In International Conference on Learning Representations, 2021."
REFERENCES,0.8205128205128205,"[4] Chang Choi, Junho Choi, Eunji Lee, Ilsun You, and Pankoo Kim. Probabilistic spatio-temporal
inference for motion event understanding. Neurocomputing, 122:24–32, 2013."
REFERENCES,0.8269230769230769,"[5] Sanjeeb Dash, Oktay Gunluk, and Dennis Wei. Boolean decision rules via column generation.
Advances in neural information processing systems, 31, 2018."
REFERENCES,0.8333333333333334,"[6] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen
Lu. Stochastic trajectory prediction via motion indeterminacy diffusion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17113–17122,
2022."
REFERENCES,0.8397435897435898,"[7] Yue Hu, Siheng Chen, Ya Zhang, and Xiao Gu. Collaborative motion prediction via neural
motion message passing. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 6319–6328, 2020."
REFERENCES,0.8461538461538461,"[8] Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and Zhaoqi Wang. Stgat: Modeling
spatial-temporal interactions for human trajectory prediction. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 6272–6281, 2019."
REFERENCES,0.8525641025641025,"[9] Rainer Kartmann, You Zhou, Danqing Liu, Fabian Paus, and Tamim Asfour. Representing
spatial object relations as parametric polar distribution for scene manipulation based on verbal
commands. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 8373–8380. IEEE, 2020."
REFERENCES,0.8589743589743589,"[10] Shuang Li, Mingquan Feng, Lu Wang, Abdelmajid Essofi, Yufeng Cao, Junchi Yan, and Le Song.
Explaining point processes by learning interpretable temporal logic rules. In International
Conference on Learning Representations, 2021."
REFERENCES,0.8653846153846154,"[11] Shuang Li, Mingquan Feng, Lu Wang, Abdelmajid Essofi, Yufeng Cao, Junchi Yan, and Le Song.
Explaining point processes by learning interpretable temporal logic rules. In International
Conference on Learning Representations, 2022."
REFERENCES,0.8717948717948718,"[12] Shuang Li, Lu Wang, Ruizhi Zhang, Xiaofu Chang, Xuqin Liu, Yao Xie, Yuan Qi, and Le Song.
Temporal logic point processes. pages 5990–6000, 2020."
REFERENCES,0.8782051282051282,"[13] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From goals, waypoints &
paths to long term human trajectory forecasting. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 15233–15242, 2021."
REFERENCES,0.8846153846153846,"[14] Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal, Kuan-Hui Lee, Ehsan Adeli, Jitendra
Malik, and Adrien Gaidon. It is not the journey but the destination: Endpoint conditioned
trajectory prediction. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part II 16, pages 759–776. Springer, 2020."
REFERENCES,0.8910256410256411,"[15] Andrew Miller, Luke Bornn, Ryan Adams, and Kirk Goldsberry. Factorized point process
intensities: A spatial analysis of professional basketball. In International conference on machine
learning, pages 235–243. PMLR, 2014."
REFERENCES,0.8974358974358975,"[16] Abduallah Mohamed, Deyao Zhu, Warren Vu, Mohamed Elhoseiny, and Christian Claudel.
Social-implicit: Rethinking trajectory prediction evaluation and the effectiveness of implicit
maximum likelihood estimation. In European Conference on Computer Vision, pages 463–479.
Springer, 2022."
REFERENCES,0.9038461538461539,[17] Filmer Stuart Cuckow Northrop. The logic of the sciences and the humanities. 1947.
REFERENCES,0.9102564102564102,"[18] Clayton Peterson. A logic for human actions. Applications of Formal Philosophy: The Road
Less Travelled, pages 73–112, 2017."
REFERENCES,0.9166666666666666,"[19] Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, and Jian Tang. Rnnlogic:
Learning logic rules for reasoning on knowledge graphs. In International Conference on
Learning Representations, 2021."
REFERENCES,0.9230769230769231,"[20] Alex Reinhart. A review of self-exciting spatio-temporal point processes and their applications.
Statistical Science, 33(3):299–318, 2018."
REFERENCES,0.9294871794871795,"[21] Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar, and Leonidas J
Guibas. Caspr: Learning canonical spatiotemporal point cloud representations. Advances in
neural information processing systems, 33:13688–13701, 2020."
REFERENCES,0.9358974358974359,"[22] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum:
End-to-end differentiable rule mining on knowledge graphs. Advances in Neural Information
Processing Systems, 32, 2019."
REFERENCES,0.9423076923076923,"[23] Bilong Shen, Xiaodan Liang, Yufeng Ouyang, Miaofeng Liu, Weimin Zheng, and Kathleen M
Carley. Stepdeep: A novel spatial-temporal mobility event prediction framework based on
deep neural network. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining, pages 724–733, 2018."
REFERENCES,0.9487179487179487,"[24] Li-Wu Tsao, Yan-Kai Wang, Hao-Siang Lin, Hong-Han Shuai, Lai-Kuan Wong, and Wen-
Huang Cheng. Social-ssl: Self-supervised cross-sequence representation learning based on
transformers for multi-agent trajectory prediction. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII, pages
234–250. Springer, 2022."
REFERENCES,0.9551282051282052,"[25] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille.
A bayesian framework for learning rule sets for interpretable classification. The Journal of
Machine Learning Research, 18(1):2357–2393, 2017."
REFERENCES,0.9615384615384616,"[26] Baowen Xu, Xuelei Wang, Shuo Li, Jingwei Li, and Chengbao Liu. Social-cvae: Pedestrian
trajectory prediction using conditional variational auto-encoder. In International Conference on
Neural Information Processing, pages 476–489. Springer, 2023."
REFERENCES,0.967948717948718,"[27] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. Groupnet: Multiscale
hypergraph neural networks for trajectory prediction with relational reasoning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6498–6507,
2022."
REFERENCES,0.9743589743589743,"[28] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for
knowledge base reasoning. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.9807692307692307,"[29] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer
networks for pedestrian trajectory prediction. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XII 16, pages 507–523.
Springer, 2020."
REFERENCES,0.9871794871794872,"[30] Jiangbei Yue, Dinesh Manocha, and He Wang. Human trajectory prediction via neural social
physics. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
23–27, 2022, Proceedings, Part XXXIV, pages 376–394. Springer, 2022."
REFERENCES,0.9935897435897436,"[31] Zihao Zhou, Xingyi Yang, Ryan Rossi, Handong Zhao, and Rose Yu. Neural point process for
learning spatiotemporal event dynamics. In Learning for Dynamics and Control Conference,
pages 777–789. PMLR, 2022."
