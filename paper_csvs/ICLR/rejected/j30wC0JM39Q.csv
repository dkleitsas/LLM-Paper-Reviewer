Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002857142857142857,"The power of embedding representations is a curious phenomenon. For embed-
dings to function effectively as feature representations, there must exist substantial
latent structure inherent in the domain to be encoded. Language vocabularies and
Wikipedia topics are human-generated structures that reflect how people organize
their world, and what they find important. The structure of the embedding spaces
should thus reflect the human evolution of language formation and the cultural
processes shaping our world.
This paper studies what the observed structure of embeddings can tell us about the
natural processes that generate new knowledge or concepts. We demonstrate that
word and graph embeddings trained on standard datasets using several popular
algorithms consistently share two properties: (1) a decreasing neighbor frequency
concentration with rank that parallels the Matthew effect in spatial dimension, and
(2) particular clustering velocities and power law-based community structures.
We then assess a variety of generative models of embedding spaces according
to these criteria, and conclude that incremental insertion processes based on the
preferential attachment-type mechanisms dependent on spatial context best model
the phenomena observed in language and network data."
INTRODUCTION,0.005714285714285714,"1
INTRODUCTION"
INTRODUCTION,0.008571428571428572,"Vector representations (Mikolov et al., 2013a;b; Pennington et al., 2014; Perozzi et al., 2014) of
feature spaces (embeddings) have proven amazingly effective at capturing the properties of discrete
entities such as vocabulary words and the vertices of networks capturing human interactions, such as
citation graphs or Wikipedia pages. Such embeddings are a foundational tool in machine learning,
providing a natural data representation easily applied for classification and regression models."
INTRODUCTION,0.011428571428571429,"The power of embedding representations is a curious phenomenon. That embeddings function so
effectively as feature representations implies there must exist substantial latent structure inherent in
each domain to be effectively encoded. Language vocabularies and Wikipedia topics are represen-
tative human-generated structures that reflect how people organize their world, and what they find
important. The structure of the resulting embedding spaces thus must reflect the human evolution of
language formation and the cultural processes shaping our world."
INTRODUCTION,0.014285714285714285,"This paper addresses a big picture question: what does the observed structure of embeddings tell
us about the processes by which people generate new knowledge or concepts? Word and graph
embeddings provide reduced-dimensional “geometric” representation of the meaning of words and
conceptual entities. The widespread success of predictive models trained on top of embeddings
suggest that they accurately capture the underlying semantics of each entity. The question we study
in this paper concerns which knowledge-generation models best explain properties observed in real-
world embeddings."
INTRODUCTION,0.017142857142857144,"The vocabulary of every language evolves through a series of processes: new words get created to
identify new concepts or objects, while old words get repurposed with new senses and meanings.
But how/where does this creation happen? Word usage frequencies are governed by a power law
distribution, namely Zipf’s law (Zipf, 1936; 1949). Do new words tend to be closely associated
with high frequency words, perhaps adding shades of meaning to popular terms? Do they tend to
fill large empty regions of embedding space, or add to dense clusters of points? If embeddings do
indeed capture the meaning of words, then studying the shape of embedding spaces should inform
on the processes underlying the evolution of language."
INTRODUCTION,0.02,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022857142857142857,"A similar set of questions can be asked about the evolution of cultural, historical and scientific
knowledge, as reflected by the contents of Wikipedia. Each entity page in Wikipedia links to other
entities, creating a network well represented by its graph embedding. Do high-degree historical en-
tities disproportionately associate with other high-degree entities? Do new historical figures emerge
in the context of clusters of earlier entities, or do they fill empty niches in the cultural landscape?"
INTRODUCTION,0.025714285714285714,"A variety of generative models can be proposed to build embedding-like spaces by incrementally
inserting new points positioned through a specific algorithmic process. We seek to compare the
properties of such generated spaces against those of observed embedding spaces, to gain insight into
the processes underlying human knowledge generation activities. This is complicated by several
factors: (1) we cannot directly observe or reconstruct the historical processes that created the entities
of observed data sets, (2) there is no direct correspondence between the labels of generated and
observed points, and (3) the scale and range of real/synthetic embeddings over different models,
algorithms, and datasets are not directly comparable. Thus assessments must be done through non-
parametric statistics quantifying similarities of unlabeled point sets. Our contributions here include:"
INTRODUCTION,0.02857142857142857,"• Generative processes for embedding spaces. We define and explore the space of sev-
eral plausible processes for the evolution of knowledge embedding spaces, incorporating
into our models the mechanisms of preferential attachment as well as the effects of attrac-
tive and/or repulsive forces. Certain processes generate networks to embed using standard
graph representation learning methods, while others directly generate geometric point sets.
Particularly novel is our class of preferential placement models."
INTRODUCTION,0.03142857142857143,"• Non-parametric properties of embedding spaces. Meaningful comparisons of the prop-
erties of observed and generated embedding spaces requires statistics which are robust to
transformations as well as being independent of node labels. We have identified two novel
and distinct properties of interest which can be rigorously measured on embedding spaces:"
INTRODUCTION,0.03428571428571429,"– Frequency concentration – How the importance of entities in the local neighborhood
of x is related to the importance of x itself? This statistic captures the non-uniform
spatial distribution of frequencies observed in an embedding.
– Clustering velocity – How rapidly do clustering procedures advance towards full con-
nectivity? This statistic captures the predominance of cluster structure inherent in an
embedding."
INTRODUCTION,0.037142857142857144,"As a check, we also compare these statistics to measures of spatial correlation popular in
geographic analysis. Our tools allow to uncover novel regularities in the natural embedding
spaces: elements of embeddings are not only characterized by power-law distributions of
their frequencies/degrees, but they also exhibit positive assortative sorting in a spatial di-
mension and a power law manifestation in relational space. Moreover, we show that these
observed properties of English language vocabularies and Wikipedia network embeddings
consistently hold for a variety of distinct data sources and embedding algorithms, includ-
ing fastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), word2vec (Mikolov
et al., 2013a), and BERT (Devlin et al., 2018). Thus they are not mere algorithmic artifacts
of data collection or analysis, but inherent properties of language and culturally-generated
networks: phenomena in need of explanation through models of language/knowledge cre-
ation."
INTRODUCTION,0.04,"• Evaluation of generative processes for word and graph embedding spaces. We com-
pare embeddings generated by several different processes to real data-driven embeddings
from several datasets by these criteria. Several models can easily be eliminated based on
these observations, leaving few that remain plausible. We conclude that specific incre-
mental insertion processes based on the spatial context-dependent preferential attachment
mechanisms best model the observed phenomenon on language and network data."
INTRODUCTION,0.04285714285714286,"Paper outline.
The rest of this paper is organized as follows. Section 2 defines and presents
the collection of seven different models we consider as potential generative processes. Section
3 proposes our frequency concentration statistic and evaluates embeddings from both models and
real-world ones. We present our clustering velocity statistics and model evaluations in Section 4.
Finally, we discuss the implications of this research and future work in Section 5. Our code and
datasets are accessible at https://shorturl.at/dyST6"
INTRODUCTION,0.045714285714285714,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04857142857142857,"1.1
RELATED WORK"
RELATED WORK,0.05142857142857143,"Explanations of embedding spaces.
Levy & Goldberg (2014) find that word embedding model
such as word2vec essentially factorizes a shifted pointwise mutual information matrix. To further
understand the linear relationship of embeddings, Allen & Hospedales (2019); Allen et al. (2019)
mathematically define the meaning of statement “ word vector wx is to word vector wy” and provide
a proof of the linearity among embeddings of word analogies. This linear property has also been
explained in knowledge graph embeddings (Allen et al., 2021). However, this line of work is more
focusing on the linear property between embedding vectors instead of the processes of generation."
RELATED WORK,0.054285714285714284,"Generative models of embeddings.
There are many graph/network generative models (see the
survey Hamilton (2020) and references therein). Traditional processes include the Erd˝os–R´enyi
model (Erdos et al., 1960), stochastic block model (Holland et al., 1983), and preferential attachment
models (Simon (1955), Barab´asi & Albert (1999), also see Albert & Barab´asi (2002) for a review).
Arora et al. (2016) study a generative model where each word vector captures its correlations with
the discourse vector. Instead, our generative models employ simple heuristics to generate embedding
vectors that exhibit empirically realistic properties. There are also works on generating models by
using deep graph neural networks (You et al., 2018). Xiao et al. (2016) propose a model based on
Dirichlet Process to generate embeddings of knowledge graph. Li et al. (2016) propose a graphical
model for generating embeddings of documents. Most of these models are heavily based on a
probabilistic framework."
RELATED WORK,0.05714285714285714,"Notation.
For a set of n entities (e.g. words or nodes) V := {1, 2, . . . , n}, the set of embeddings
associated with V is {w1, w2, . . . , wn}, where each location or whereabouts wi ∈Rd. Throughout
this paper, each entity i is also associated with a importance or prominence (e.g. the frequency of
a specific words, or the degree of a graph node) and denoted as pi. Therefore, each entity i can be
represented as pair of importance and location ⟨pi, wi⟩. We collect them in an embedding matrix
W ∈Rn×d and an importance vector p ∈Rn."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.06,"2
GENERATIVE MODELS FOR EMBEDDING SPACES"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.06285714285714286,"A generative model of an n-element embedding space starts with an initial entity embedding pair
⟨p0, w0⟩. At each time t = 1, 2, . . . , n, the model then repeatedly generates a new pair ⟨pt, wt⟩.
During this generating process, it updates p and W . We first present simple baseline models to
generate points before considering network-generation processes based on preferential attachment.
Motivated by empirical facts presented later, we seek to generate an embedding space such that
entities with similar properties will be closer with each other compared with more distant points.1 0.0 0.2 0.4 0.6 0.8"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.06571428571428571,Normalized Power
GENERATIVE MODELS FOR EMBEDDING SPACES,0.06857142857142857,A) Generating nodes of a network
GENERATIVE MODELS FOR EMBEDDING SPACES,0.07142857142857142,"A1) Gaussian model.
A simple way to generate an embedding-
like space creates each point ⟨pi, wi⟩where wi is from a standard
Gaussian distribution N(0, Id) and the prominence p is drawn uni-
formly from a random permutation of {1, 2, . . . , n} so that each entity
i has a random rank pi. The right figure illustrates generated embed-
ding points that are from 3-dimensional Gaussian distribution, i.e.,
N(0, I3). We color each point i by its normalized prominence, i.e.,
pi/ maxi∈V pi. As expected, these data points form a single cluster
without any finer substructure within it."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.07428571428571429,"A2) Preferential Placement (PP) model.
This model presumes that new entities emerge around
the older, relatively more prominent ones.
This PP model builds on the preferential attach-
ment mechanism driving the development of real-world networks and graphs, where the fre-
quency of generated entities exhibit the power law distribution, and it is closest to Simon (1955).2"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.07714285714285714,"1We describe experimental details of generating points in Appendix A.1.
2The idea can be traced back to Eggenberger & P´olya (1923) and Yule (1925), with accepted modern
formulations due to Simon (1955), Price (1976) and Barab´asi & Albert (1999)."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.08,Under review as a conference paper at ICLR 2022
GENERATIVE MODELS FOR EMBEDDING SPACES,0.08285714285714285,"PP is parameterized by the total frequencies of data points n, the proba-
bility of increasing the frequency of an old entity q, and an exponential
distribution3 with scale parameter β driving the distance of a new entity
from the old one per-iteration t. The PP model randomly selects entity
i from existing ones with a probability proportional to their frequencies
and with a probability q, it increases the frequency of wi by 1. Other-
wise, it generates a new entity wt within a small ball around wi: wt = wi + rt · xt with pt = 1,
where the radius rt ∼exp(β), and its direction xt follows a uniform distribution on a unit sphere.4
Different from the above Gaussian model, the left figure shows generated points where more promi-
nent points in the small core cluster are surrounded by less prominent ones."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.08571428571428572,"A3) Directional Preferential Placement (DPP) model.
The previous PP model generates new
points in uniformly distributed random directions, producing a homogeneous scattering of embed-
ding entities. To bring about a more heterogeneous landscape with cluster structure, we use the von
Mises-Fisher distribution (vMF) that allows more control over the directions where new entities are
generated, in particular making this process more spatial context-aware. Specifically, a random unit
vector x ∈Rd, d ≥2, follows the d-dimensional vMF distribution if its probability density function
is given by"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.08857142857142856,"fvMF(x|µ, κ) :=
κd/2−1"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.09142857142857143,"(2π)d/2Id/2−1(κ) exp

κµ⊤x
	
,"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.09428571428571429,"where µ with ∥µ∥= 1, is the mean direction parameter specifying the orientation relatively to
the origin; κ ≥0 is a concentration parameter controlling the dispersion about the mean (higher κ
produces higher concentration); and Id/2−1(κ) is the modified Bessel function of the first kind. For
example, mean direction may represent the force attracting x to some local point, or, say, repelling
it from a global center. vMF distribution has been applied for high-dimensional directional data
analysis, particularly in applications to textual data and with a focus on clustering (Banerjee et al.
(2005) and Gopal & Yang (2014), also see Sra (2018) for a recent overview). However, to the best
of our knowledge, prior work considered some given fixed parameters of interest (usually focusing
on estimating them, sometimes on using them for generating exercises); while this paper is inter-
ested exclusively in data-generating models, and treats at least some of these parameters (e.g., mean
direction µ) as endogenously defined within the model itself."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.09714285714285714,"In the DPP model, the first entity ⟨p0, w0⟩is initialized with p0 = 1 and
∥w0∥= 1. Given the total frequencies of entities n, the exponential dis-
tribution exp(β) with parameter β, and the probability of increasing the
frequency of an old entity q, the DPP model involves (1) selection as an
origin of an existing entity wi randomly with a probability proportional
to its frequency; and then (2) at each iteration t, with probability q, ei-
ther (2a) an increase of the frequency of the chosen wi by 1; or (2b) a
creation of a new entity around the origin wi, i.e., wt = wi + rt · xt. In
contrast to the previous model, however, now the direction is distributed
as xt ∼vMF(x|µt, κ), with mean direction vector µt pointing away from the global center of
mass w∗
t .5 The generated points of this model is illustrated on figure to the right where higher
weight nodes are close to each other."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.1,B) Generating edges of a graph.
GENERATIVE MODELS FOR EMBEDDING SPACES,0.10285714285714286,"B1) BA-Embedded model.
The BA model uses a preferential attachment process (Barab´asi &
Albert, 1999) to generate networks of edges, as opposed to the previously described preferential
placement models for points. We use the BA model to generate a random graph and then apply a
standard graph representation learning algorithm to obtain an embedding."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.10571428571428572,"3Recall that the exponential distribution with scale parameter β is denoted as exp(β) and its probability
density function is given by fexp(x; β) = 1/β exp(−x/β) when x ≥0, 0 otherwise."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.10857142857142857,"4That is, ∥xt∥= 1. ∥· ∥is the ℓ2-norm, i.e., ∥x∥=
qPd
i=1 x2
i . For example, when d = 2, the offset term"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.11142857142857143,"rt · xt = [rt cos(θt), rt sin(θt)]⊤, with angle θt ∼U(0, 2π).
5We define the center of mass as the weighted average of all points, that is 1 t
P"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.11428571428571428,"i∈{1,2,...,t} piwi. This
center of mass moves at each iteration t."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.11714285714285715,Under review as a conference paper at ICLR 2022
GENERATIVE MODELS FOR EMBEDDING SPACES,0.12,"Denote the number of nodes at time t as nt. Each new node is added to the network and connected
to node ni with probability qt = pi/ P"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.12285714285714286,"j pj, where weight pj is the degree of node j."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.12571428571428572,"Our BA-Embedded model starts by creating a star graph with m +
1 nodes where node 0 connects all other m nodes, that is, 1/m
equals to the probability of generating new nodes.
Let L
←
{(0, 1), (0, 2), . . . , (0, m)} be the initial list of m edges. At each iter-
ation t, the model generates a new node i and randomly select m targets
from current graph, where each edge connects the new node and a node
from selected m targets. After creating m new edges, the model adds
these edges into L, i.e., L ←L ∪{(i, t1), (i, t2), . . . , (i, tm)}. We re-
peat until n nodes are generated and obtain the final edge list L. For
generating embeddings, we use DeepWalk (Perozzi et al., 2014) to generate the corresponding em-
bedding vectors with L as an input graph. As shown in figure left, these generated 3-dimensional
points form a cluster where points of smaller weights (less prominent nodes) spread out of the clus-
ters while higher degree nodes are densely correlated. Compared with PP model, lower prominence
points of BA-Embedded model are smoother located around the cluster."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.12857142857142856,"B2) BA model with gravitational movement (BA-Gravity).
Instead
of directly using DeepWalk to generate embeddings, we treat the graph
generation process as a sequence of edge generation, and then update the
embeddings by their weights. Effectively, we extend the BA model with
a more sophisticated mechanism of dependence on neighboring spatial
context. Algorithm 1 presents the BA-Gravity model. It first obtains a
sequence of edges L of the simulated graph and then initializes each new
entity either from N(0, Id) or from U[−.5, .5]d (Line 4-9). After list of
edges are generated by BA-model, for node u, it updates embedding wu
by moving 1/(pu + 1) away of itself but in the direction to wt
v, i.e.,
pu
pu+1wt
u +
1
pu wt
v. We call this
update step “gravitational movement” since its neighbor node v attracts u to itself and away from
its original location. The generated 3-dimensional points by Algorithm 1 are shown in figure right.
Clearly, higher degree nodes are closely located."
GENERATIVE MODELS FOR EMBEDDING SPACES,0.13142857142857142,Algorithm 1 BA-Gravity model
GENERATIVE MODELS FOR EMBEDDING SPACES,0.13428571428571429,"1: L ←list of edges by the BA model
2: V 0 = {}, t = 0, p = 0
3: for et(u, v) ∈L do
4:
If u /∈V t then
5:
wt
u ∼N(0, Id) or U(−.5, .5)/d
6:
V t+1 = V t ∪u
7:
If v /∈V t−1 then
8:
wt
v ∼N(0, Id) or U(−.5, .5)/d
9:
V t+1 = V t ∪v
10:
pu = pu + 1, pv = pv + 1
11:
wt+1
u
=
pu
pu+1wt
u +
1
pu wt
v
12:
wt+1
v
=
pv
pv+1wt
v +
1
pv wt
u
13:
t = t + 1
14: end for
15: Return W"
GENERATIVE MODELS FOR EMBEDDING SPACES,0.13714285714285715,"Denote tv as the time of node v being the neighbor of u.
During the whole process of BA-Gravity, at any time t,
one can verify that wi is given by wt+1
i
=
pu
pu+1wt
i +"
PU WT,0.14,"1
pu wt
v =
1
pu+1w0
i +
1
|N (i)|
P
j∈N(i)
1
pu wtj
j . Therefore,
one can treat Algorithm 1 as the first-order approxima-
tion of U =
 
α0I + α1A + α2A2 + . . . + αqAq
R,
where A is the normalized adjacency matrix, I is the
identity matrix, and R is a random matrix. This em-
bedding approximation has been explored in fast graph
embedding algorithms (Zhang et al., 2018; Chen et al.,
2019)."
PU WT,0.14285714285714285,"B3) BA-Centroid(Node)-Average/Weight.
To fur-
ther extend the BA-Gravity model, instead of updat-
ing two nodes, we update the node itself whenever the
model creates a new node. That is, at each iteration,
it only updates the embedding of the new node by taking average of its neighbors’ embeddings.
Let wi be the current embedding to be updated. There are two options of taking this average: 1)
simply taking average of all neighbors, i.e., wi =
1
|N (i)|
P"
PU WT,0.1457142857142857,"v∈N(i) wv and we refer this model as
BA-Centroid-NA; and 2) taking a weighted average, i.e., wi =
1
P"
PU WT,0.14857142857142858,"j∈N (i) pj
P"
PU WT,0.15142857142857144,"v∈N(i) pv · wv. We
refer to it as BA-Centroid-NW."
PU WT,0.15428571428571428,"BA-Centroid-NW
BA-Centroid-EW"
PU WT,0.15714285714285714,"B4)
BA-Centroid(Edge)-Average/Weight.
Compared
to
BA-Centroid-NA and BA-Centroid-NW, edge-based models
update embeddings of both new node and its neighbors by tak-
ing average of their neighbors. We call these two methods"
PU WT,0.16,Under review as a conference paper at ICLR 2022
PU WT,0.16285714285714287,"CENTROID-EA and CENTROID-EW respectively based on the two average strategies above. We
illustrate the points of BA-Centroid-NW and BA-Centroid-EW model."
FREQUENCY CONCENTRATION,0.1657142857142857,"3
FREQUENCY CONCENTRATION"
FREQUENCY CONCENTRATION,0.16857142857142857,"New entities emerge in the shadow of older, more prominent entities, creating cluster structures
which should be apparent in both model and real-world embeddings such as GloVe (Pennington
et al., 2014), word2vec (Mikolov et al., 2013b), and DeepWalk (Perozzi et al., 2014)."
FREQUENCY CONCENTRATION,0.17142857142857143,"Study of relationships between spatially-distributed entities have naturally been an important focus
in geography. Similar methods are also relevant for quasi-spatial data in other domains: what is
sometimes called the “first law” of geography states that “Everything is related to everything else,
but near things are more related than distant things” (Tobler, 1970). Popular measures of spatial
autocorrelations include (1) Moran’s I, which may be thought of as a spatial analog of Pearson’s
correlation coefficient and is sensitive to extreme values of the variable (Moran, 1948); as well as
(2) Geary’s C, which is sensitive to differences in small neighborhoods (Geary, 1954). A very
accessible textbook treatment is available in O’Sullivan & Unwin (2010); also see Anselin (1995).
For examples of usage see: in epidemiology Ganegoda et al. (2021), in linguistics Grieve (2011)."
FREQUENCY CONCENTRATION,0.1742857142857143,"We design our own statistical metric capturing the geometric properties of the embeddings intro-
duced earlier in a non-parametric manner.6 The key idea of the proposed index to measure frequency
concentration has three parts. First, we build a k-nearest neighbor similarity graph G(V, E) based on
d-dimensional node/word embeddings, i.e., W ∈Rn×d. The potential measure function could be
either a Gaussian Kernel function or a cosine similarity. The frequencies of the one-hop neighbors
of each data point in this graph (e.g., a node or a word) can be interpreted as a frequency rank (from
1 to n), and these sum of ranks normalized to get a value from 0 to 1. We define an index from the
area under the cumulative distribution function (CDF) of the neighbors’ rank as a measure of spatial
autocorrelation. More formally, we assume the set of entities considered is V = {1, 2, . . . , n}. Each
v ∈V has a prominence associated with it, denoted as pv (e.g. node degree or word frequency).
Definition 1 (Frequency concentration index). Denote the prominence pv as a vector p =
[p1, p2, . . . , pn]⊤for all nodes in V. Let the ranking function r(p) : Rn →π(V) where π is a
permutation of V where rv is the ranking of node v taking a value in [1, n]. Given k and any embed-
ding matrix W ∈Rn×d, we define N(v; k, W ) as a set of one-hop k-nearest neighbors of v based
on some chosen similarity function on W . Then the Index of v can be defined as the following:"
FREQUENCY CONCENTRATION,0.17714285714285713,"Index (v; k, W ) := 1 n n
X i=1"
FREQUENCY CONCENTRATION,0.18,"|{ru ≤i : u ∈N(v; k, W )}| k"
FREQUENCY CONCENTRATION,0.18285714285714286,"
= 1 −
X"
FREQUENCY CONCENTRATION,0.18571428571428572,"i∈N(v;k,W )
ri.
(1)"
FREQUENCY CONCENTRATION,0.18857142857142858,"Intuitively, if most of neighbors have high rankings, then the index is close to 1 while if neighbors
have low rankings, it is close to 0. Each term in the summation of Equation (1) can be viewed as
reflecting the effects of preferential attachment."
FREQUENCY CONCENTRATION,0.19142857142857142,"Frequency concentration of word embeddings:
We present the frequency concentration plots
for different embeddings including Fasttext (Mikolov et al., 2018), word2vec (Mikolov et al.,
2013b), and GloVe (Pennington et al., 2014) in Figure 1, comparing the frequency correlations of
English words embedding that are from eight different models and data sources. Figure 1 shows that
the frequency concentration is remarkably consistent across different data sources and algorithms,
that is: the higher frequency words tend to have high frequency words as their close neighbors, while
low-frequency words live in poorer neighborhoods. It is tempting to view this as a manifestation of
relational, spatial Matthew effect.7 Results in the appendix show similar frequency concentration
across a broad range of non-English languages. We also investigate how the frequency correlations
change with respect to the parameter k and d. More detailed results can be found in Appendix B.1."
FREQUENCY CONCENTRATION,0.19428571428571428,"Frequency concentrations of graph embeddings
Table 1 shows that the slopes of the best lin-
ear fit are consistently negative across four diverse graph datasets (detailed in Table 3 and Figure 5"
FREQUENCY CONCENTRATION,0.19714285714285715,"6In the interest of completeness, we also provide results for the classical Moran’s I and Geary’s C statistics.
7Originally christened by Merton (1968), and in our case implying that not only “the rich get richer”, but
also that “they befriend other riches”."
FREQUENCY CONCENTRATION,0.2,Under review as a conference paper at ICLR 2022
K,0.20285714285714285,"30K
70K
Ranking 1.0 0.5"
K,0.2057142857142857,Freq. Concentration
K,0.20857142857142857,fastText-Wiki -0.46
K,0.21142857142857144,"30K
70K
Ranking"
K,0.21428571428571427,fastText-Wiki-Sub -0.33
K,0.21714285714285714,"30K
70K
Ranking"
K,0.22,fastText-CC -0.37
K,0.22285714285714286,"30K
70K
Ranking"
K,0.2257142857142857,fastText-CC-Sub -0.39
K,0.22857142857142856,"30K
70K
Ranking"
K,0.23142857142857143,word2vec-News -0.28
K,0.2342857142857143,"30K
70K
Ranking"
K,0.23714285714285716,Glove-6B -0.77
K,0.24,"30K
70K
Ranking"
K,0.24285714285714285,Glove-42B -0.69
K,0.24571428571428572,"30K
70K
Ranking"
K,0.24857142857142858,Glove-840B -0.57
K,0.25142857142857145,"Figure 1: Frequency concentration of eight distinct English word embeddings.
The observed
frequency concentration is similar even though trained on different data sources (e.g., Wikipedia
(Wiki), Common Crawl (CC/42B/840B/Sub), and Google News (News)) and different embedding
algorithms including GloVe, fastText, and word2vec. Each dashed red curve is linearly fitted where
linear coefficient is presented on each legend."
K,0.2542857142857143,"in the appendix) for four distinct algorithmic approaches to graph embeddings. As with language
vocabularies, the frequency concentration plot decreases consistently with rank across diverse em-
bedding models for the all graphs. The consistency of these results across domain and algorithm
mark negatively-sloping frequency concentration as a property to be preserved in any meaningful
generative model of embeddings."
K,0.2571428571428571,"LLE
FastRP
DeepWalk
LINE"
K,0.26,"Coauthor
-0.1582
-0.4932
-0.3780
-0.2808
Wikipeople
-0.3498
-0.1730
-0.2708
-0.1760
Patent
-0.5619
-0.7356
-0.6705
-0.5832
PPI
-0.2739
-0.5144
-0.3459
-0.4092"
K,0.26285714285714284,"Table 1: The frequency concentration slope of graph embeddings are consistently negative across
four algorithms/datasets."
K,0.26571428571428574,"Model validation.
We evaluate the frequency concentration of proposed generative embedding
models in Figure 2, with proximity measured by cosine similarity. Clearly, embeddings of models
with fitted negative coefficient are more likely close to real-world embeddings as shown in Figure 1.
Only the context-aware variations of preferential placement and BA models match the data. Figure 5
demonstrates that the observed properties of such representations are consistent across four different
graph embedding algorithms."
K,0.26857142857142857,"700
1300
Ranking 0.5 1.0"
K,0.2714285714285714,Freq. Concentration
K,0.2742857142857143,Gaussian -0.00
K,0.27714285714285714,"700
1300
Ranking PP 0.01"
K,0.28,"700
1300
Ranking DPP -0.16"
K,0.28285714285714286,"700
1300
Ranking"
K,0.2857142857142857,BA-Centroid-NA -0.01
K,0.2885714285714286,"700
1300
Ranking"
K,0.2914285714285714,BA-Centroid-EA -0.02
K,0.29428571428571426,"700
1300
Ranking"
K,0.29714285714285715,BA-Embedded -0.18
K,0.3,"700
1300
Ranking"
K,0.3028571428571429,BA-Centroid-NW -0.24
K,0.3057142857142857,"700
1300
Ranking"
K,0.30857142857142855,BA-Centroid-EW -0.14
K,0.31142857142857144,"700
1300
Ranking"
K,0.3142857142857143,BA-Gravity -0.04
K,0.3171428571428571,"Figure 2: Frequency concentration plots for all nine different models where each model generates
n = 2000 points with d = 128. The parameter k of the Index for each node is set to 10."
CLUSTERING VELOCITY,0.32,"4
CLUSTERING VELOCITY"
CLUSTERING VELOCITY,0.32285714285714284,"The spatial-frequency concentration presented above provides one dimension to compare real-world
embeddings to models. We now analyze the spatial characteristics of embeddings for clustering
structures independent of frequency, and propose two distinct indices to measure the cluster velocity
of the k-nearest-neighbor (k-nn) graph induced from the vector representation space."
CLUSTERING VELOCITY,0.32571428571428573,"Since embeddings W ∈Rn×d are optimized to preserve the natural clusters of the real-world
knowledge in its metric space, its pair-wise similarity graph Gmetric ∈Rn×n encodes the embed-
ding spatial characteristics as vertex connectivity. We hypothesize that the k-NN graph G induced
from real-world Gmetric should share common connectivity patterns, particularly in the number of
connected components when we perform agglomerative clustering."
CLUSTERING VELOCITY,0.32857142857142857,"For example, consider a point set drawn from p independent, well-separated Gaussian distributions,
and hence containing p natural clusters. We anticipate that any single-link agglomerative clustering
procedure from shortest to longest edge will quickly identify these clusters, but many short edges
must be evaluated as potential merges before the separate components are unified by long inserted"
CLUSTERING VELOCITY,0.3314285714285714,Under review as a conference paper at ICLR 2022
CLUSTERING VELOCITY,0.3342857142857143,"0.0
0.5
1.0
Inserted Edges (%) 0 1 2 3"
CLUSTERING VELOCITY,0.33714285714285713,# Connected Components 1e4
CLUSTERING VELOCITY,0.34,Word Embeddings
CLUSTERING VELOCITY,0.34285714285714286,"0.0
0.2
0.4
0.6
0.8
1.0
Inserted Edges (%)"
CLUSTERING VELOCITY,0.3457142857142857,Graph Embeddings
CLUSTERING VELOCITY,0.3485714285714286,"0.0
0.2
0.4
0.6
0.8
1.0
Inserted Edges (%)"
CLUSTERING VELOCITY,0.3514285714285714,Models Generated
CLUSTERING VELOCITY,0.35428571428571426,"100
101
102
103 Gamma 100 101 102 103 104"
CLUSTERING VELOCITY,0.35714285714285715,# Connected Components
CLUSTERING VELOCITY,0.36,"fastText-CC
fastText-Wiki
Glove-CC-42B
Glove-CC-840B
Glove-Twitter
Glove-Wiki-6B
BERT"
CLUSTERING VELOCITY,0.3628571428571429,"100
101
102
103 Gamma"
CLUSTERING VELOCITY,0.3657142857142857,"Coauthor-LLE
Coauthor-Deepwalk
Coauthor-LINE
Coauthor-FastRP
Patent-LLE
Patent-Deepwalk
Patent-LINE
Patent-FastRP"
CLUSTERING VELOCITY,0.36857142857142855,"Wikipeople-LLE
Wikipeople-Deepwalk
Wikipeople-LINE
Wikipeople-FastRP
PPI-LLE
PPI-Deepwalk
PPI-LINE
PPI-FastRP"
CLUSTERING VELOCITY,0.37142857142857144,"100
101
102
103 Gamma"
CLUSTERING VELOCITY,0.3742857142857143,"Uniform
Single-Gaussian
Multi-Gaussian
DPP
PP
BA-Gravity"
CLUSTERING VELOCITY,0.37714285714285717,"BA-Centroid-EW
BA-Centroid-EA
BA-Centroid-NA
BA-Centroid-NW
BA-Embedded"
CLUSTERING VELOCITY,0.38,"Figure 3: Top: The number of connected components as a function of inserted edges. Each col-
umn shares legend. Bottom: The number of connected components as a function of γ in graph
partitioning methods Clauset et al. (2004) in log-scale. We sampled top 30K and 10K vectors from
embeddings to visualize clustering velocities, except for smaller PPI graph which has only 3,980
nodes. All real-world embeddings share similar clustering velocity pattern, while model generated
space have distinct shapes."
CLUSTERING VELOCITY,0.38285714285714284,"edges. The number of connected components will decrease at a slow velocity until eventually all
vertices are merged into a single integrate component. This merging process will happen more
quickly for p = 1 than p = 2."
CLUSTERING VELOCITY,0.38571428571428573,"Empirical clustering velocity from real-world and model generated embeddings.
Figure 3
(top) presents the clustering velocity for the word, graph, and model embeddings we have described.
As expected the multiple Gaussian model (upper right, curve multi-Gaussian in Figure 3 clusters
slower than a single Gaussian. While a point set of uniform distribution may not have such hierar-
chical cluster formation process, thus, yielding distinct characteristics in term of clustering velocity
as shown in the same subfigure. Furthermore, we observed all word and graph embeddings (first and
second columns of the top row) have similar curve shape, reflecting non-trivial ubiquitous geometric
characteristics in the original embedding space."
CLUSTERING VELOCITY,0.38857142857142857,"Agglomerative clustering velocity index:
We define the following Clustering Velocity-AUC in-
dex (CV-AUC) to measure the cluster formation characteristic of agglomerative clustering as fol-
lows. We construct the K-nearest-neighbor graph using cosine distance as G = (V, E). |V| = n,
E = {eij|euv = distcos(wu, wv); u ̸= v; u, v ∈V} 8 Then, given an empty edge set ˜E = ∅, we re-
construct the graphs by inserting |E| edges in ascending order. We denote fG(t) = |{ ˜St
1, ˜St
2, ..., ˜St
l }|
as the number of connected components, where ˜
St
k is the kth connected component in graph G after
tth insertion. The Clustering Velocity is defined as the normalized AUC of fG(t):"
CLUSTERING VELOCITY,0.3914285714285714,"IndexCV-AUC (G) :=
Z |E| 0"
CLUSTERING VELOCITY,0.3942857142857143,"fG(t)
|E| ∗|VG| dt ∈(0, 1).
(2)"
CLUSTERING VELOCITY,0.39714285714285713,"Clauset-Newman-Moore Clustering Velocity (CV-CNM)
: In addition, similar entities tend to
have closer proximity in the embedding space, yielding graph clusters having stronger/denser intra-
group ties/edges.
We are interested in finding how inter/intra-clusters take shape, specifically,
how fast the big components branch into sub-clusters as we cut the edges. Therefore, we use a
modularity-based graph partitioning algorithm Clauset et al. (2004) as another way to inspect the
clustering velocity as we tune resolution parameter γ from small to large. As shown in the bottom
row of Figure 3 (in log-scale), likewise, all real-world embeddings share the velocity pattern which
is similar to power law, while various models (subfigure on the bottom right) have distinct shapes."
CLUSTERING VELOCITY,0.4,"Given W and G as described in Definition 2, we use modality-based graph partition method to cut
G into sub-graphs Sγ ∈{Sγ
1 , Sγ
2 , ..., Sγ
l } with varying resolution parameter Γ ∈{γk|γk = 2k, k ∈"
CLUSTERING VELOCITY,0.40285714285714286,"8distcos(·, ·)calculates cosine distance"
CLUSTERING VELOCITY,0.4057142857142857,Under review as a conference paper at ICLR 2022
CLUSTERING VELOCITY,0.4085714285714286,"Z}, where Sγ
k is the kth component. We define the index as the error of power-law fitting:"
CLUSTERING VELOCITY,0.4114285714285714,"CV-CNM := 1 |Γ| |Γ|
X"
CLUSTERING VELOCITY,0.4142857142857143,"i=1
(yk −f ′
G(xk))2, xk = log(γ), yk = log(|Sγ|), f ′
G(x) = wG ∗x + bG,"
CLUSTERING VELOCITY,0.41714285714285715,"where wG and bG are the fitted parameters associated with KNN-graph G.
Model Evaluation:
We visualize these proposed indices, including Frequency concentration, for
each embedding in Figure 4 (left, middle). On left, the horizontal and vertical axis represents CV-
AUC and CV-CNM, respectively. All the word and graph embeddings of various methods (shown
with red and blue markers on left) have similar range, demonstrating the existence of invariant
clustering velocity patterns among real-world embeddings. Although the model generated spaces
have distinct values, the BA-Centroid-{NW,EW} and DPP-family 9 models are demonstrably closer
to the real ones."
CLUSTERING VELOCITY,0.42,"0.1
0.2
0.3
0.4
0.5
CV-AUC 0.0 0.2 0.4 0.6"
CLUSTERING VELOCITY,0.4228571428571429,CV-CNM
CLUSTERING VELOCITY,0.4257142857142857,"Word
Graph
BA-Centroid-EA"
CLUSTERING VELOCITY,0.42857142857142855,"BA-Centroid-EW
BA-Centroid-NA
BA-Centroid-NW"
CLUSTERING VELOCITY,0.43142857142857144,"BA-Embedded
BA-Gravity
DPP"
CLUSTERING VELOCITY,0.4342857142857143,"Multi-Gaussian
PP
Single-Gaussian"
CLUSTERING VELOCITY,0.43714285714285717,"0.1
0.2
0.3
0.4
0.5
CV-AUC -1.0 -0.8 -0.6 -0.4 -0.2 0.0"
CLUSTERING VELOCITY,0.44,Frequency Concentration
CLUSTERING VELOCITY,0.44285714285714284,"0.00
0.25
0.50
0.75
1.00
Moran-I -1.0 -0.8 -0.6 -0.4 -0.2 0.0"
CLUSTERING VELOCITY,0.44571428571428573,Frequency Concentration
CLUSTERING VELOCITY,0.44857142857142857,"Figure 4: The visualization of real-world and model generated embeddings (dim = 300) in the plane
of two proposed clustering velocity indices, frequency concentration, and spatial autocorrelation
indices . Real-world embeddings share similar pattern as several model generated embeddings (BA-
Centroid-{NW,EW} and DPP ), while other models do not. Each model is represented with five
distinct hyper-parameter configurations. On right, frequency concentration correlates well with the
spatial autocorrelation statistic Moran’s I [-0.810] (and with 1-Geary’s C [-0.450] in Figure 9 in
Appendix)."
CONCLUSION AND FUTURE WORK,0.4514285714285714,"5
CONCLUSION AND FUTURE WORK
In this paper, we explore invariant spatial characteristics of embeddings and propose several models
that reconstruct the observed properties of frequency concentration and clustering velocities of real-
world embedding spaces. Our computational tools document a consistent empirical phenomenon
(not an artifact) across many word and graph data sets as well as embedding algorithms that requires
explanation. Especially interesting findings are the positive association between entities of similar
importance, suggesting a spatial kind of Matthew effect, as well as a particular connectivity of the
clustering structures that is characterized by a power law in relational sense; which, in turn, seem to
be the fundamental processes underlying the formation of clusters in an embedding network."
CONCLUSION AND FUTURE WORK,0.4542857142857143,"We present an evolving collection of instructive generating models, starting from simple processes
that are agnostic about their local and global neighborhoods and progressing to more complex al-
gorithms that account for spatial interdependence between generated entities. The latter behave like
what is observed in typical language/graph datasets, and, as we have shown, position points naturally
in correlated patterns and into cluster structures akin to those observed in the wild."
CONCLUSION AND FUTURE WORK,0.45714285714285713,"Our work raises several interesting questions. Our experiments were not of sufficient resolution to
identify a single dominant model for reasoning about the origins and structure of natural embed-
ding spaces – but they are a start. We believe a well-supported natural model will be a substantial
contribution to understanding the evolution of knowledge structures in several domains."
CONCLUSION AND FUTURE WORK,0.46,"An interesting direction of research would be to study the effects of spatial dependencies within the
generating process, the contribution of different attractive and repulsive forces, the importance of
defining the context of their sensitivities being global as opposed to purely local. There is also a
need for better non-parametric statistics to define over embedding spaces that capture properties of
frequency association and cluster structures."
CONCLUSION AND FUTURE WORK,0.46285714285714286,9PP models do not generate significant frequency concentration as Figure 4 (middle) shows.
CONCLUSION AND FUTURE WORK,0.4657142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.4685714285714286,REFERENCES
REFERENCES,0.4714285714285714,"Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. Polyglot: Distributed word representations for
multilingual nlp. In Proceedings of the Seventeenth Conference on Computational Natural Lan-
guage Learning, pp. 183–192, 2013."
REFERENCES,0.4742857142857143,"R´eka Albert and Albert-L´aszl´o Barab´asi. Statistical mechanics of complex networks. Reviews of
modern physics, 74(1):47, 2002."
REFERENCES,0.47714285714285715,"Carl Allen and Timothy Hospedales. Analogies explained: Towards understanding word embed-
dings. In International Conference on Machine Learning, pp. 223–231. PMLR, 2019."
REFERENCES,0.48,"Carl Allen, Ivana Balazevic, and Timothy Hospedales. What the vec? towards probabilistically
grounded embeddings.
Advances in Neural Information Processing Systems, 32:7467–7477,
2019."
REFERENCES,0.4828571428571429,"Carl Allen, Ivana Balazevic, and Timothy Hospedales. Interpreting knowlege graph relation repre-
sentation from word embeddings. In Ninth International Conference on Learning Representations
2021, 2021."
REFERENCES,0.4857142857142857,"L. Anselin. Local indicators of spatial association—lisa. Geographical Analysis, 27:93–115, 1995."
REFERENCES,0.48857142857142855,"Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 4:385–399, 2016."
REFERENCES,0.49142857142857144,"Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, Suvrit Sra, and Greg Ridgeway. Clustering on
the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research,
6(9), 2005."
REFERENCES,0.4942857142857143,"Albert-L´aszl´o Barab´asi and R´eka Albert. Emergence of scaling in random networks. science, 286
(5439):509–512, 1999."
REFERENCES,0.49714285714285716,"Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with
subword information. Transactions of the Association for Computational Linguistics, 5:135–146,
2017."
REFERENCES,0.5,"Haochen Chen, Syed Fahad Sultan, Yingtao Tian, Muhao Chen, and Steven Skiena. Fast and ac-
curate network embeddings via very sparse random projection. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management, pp. 399–408, 2019."
REFERENCES,0.5028571428571429,"Aaron Clauset, Mark EJ Newman, and Cristopher Moore. Finding community structure in very
large networks. Physical review E, 70(6):066111, 2004."
REFERENCES,0.5057142857142857,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5085714285714286,"F. Eggenberger and G. P´olya. ¨uber die statistik verketteter vorg¨ange. ZAMM - Journal of Applied
Mathematics and Mechanics / Zeitschrift f¨ur Angewandte Mathematik und Mechanik, 3(4):279–
289, 1923."
REFERENCES,0.5114285714285715,"Paul Erdos, Alfr´ed R´enyi, et al. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad.
Sci, 5(1):17–60, 1960."
REFERENCES,0.5142857142857142,"Naleen Chaminda Ganegoda, Karunia Putra Wijaya, Miracle Amadi, K. K. W. Hasitha Erandi, and
Dipo Aldila. Interrelationship between daily covid-19 cases and average temperature as well as
relative humidity in germany. Scientific Reports, 11(11302), 2021."
REFERENCES,0.5171428571428571,"Robert C Geary. The contiguity ratio and statistical mapping. The incorporated statistician, 5(3):
115–146, 1954."
REFERENCES,0.52,"Siddharth Gopal and Yiming Yang. Von mises-fisher clustering models. In International Conference
on Machine Learning, pp. 154–162. PMLR, 2014."
REFERENCES,0.5228571428571429,Under review as a conference paper at ICLR 2022
REFERENCES,0.5257142857142857,"´Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tom´aˇs Mikolov. Learning
word vectors for 157 languages. In Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018), 2018."
REFERENCES,0.5285714285714286,"Jack Grieve. A regional analysis of contraction rate in written standard american english. Interna-
tional Journal of Corpus Linguistics, 16(4):514—-546, 2011."
REFERENCES,0.5314285714285715,"Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset node representation learning over large
dynamic graphs. arXiv preprint arXiv:2106.01570, 2021."
REFERENCES,0.5342857142857143,"William L Hamilton. Graph representation learning. Synthesis Lectures on Artifical Intelligence and
Machine Learning, 14(3):1–159, 2020."
REFERENCES,0.5371428571428571,"Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social networks, 5(2):109–137, 1983."
REFERENCES,0.54,"Armand Joulin, Piotr Bojanowski, Tom´aˇs Mikolov, Herv´e J´egou, and ´Edouard Grave. Loss in trans-
lation: Learning bilingual word mapping with a retrieval criterion. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pp. 2979–2984, 2018."
REFERENCES,0.5428571428571428,"Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances
in neural information processing systems, 27:2177–2185, 2014."
REFERENCES,0.5457142857142857,"Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. Generative topic embedding: a continuous
representation of documents. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 666–675, 2016."
REFERENCES,0.5485714285714286,"Robert K. Merton. The matthew effect in science. Science, 159(3810):56–63, 1968."
REFERENCES,0.5514285714285714,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a."
REFERENCES,0.5542857142857143,"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Proceedings of the 26th International
Conference on Neural Information Processing Systems-Volume 2, pp. 3111–3119, 2013b."
REFERENCES,0.5571428571428572,"Tom´aˇs Mikolov, ´Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Ad-
vances in pre-training distributed word representations. In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Evaluation (LREC 2018), 2018."
REFERENCES,0.56,"Patrick AP Moran. The interpretation of statistical maps. Journal of the Royal Statistical Society.
Series B (Methodological), 10(2):243–251, 1948."
REFERENCES,0.5628571428571428,"David O’Sullivan and David J. Unwin. Geographic Information Analysis. John Wiley & Sons, Inc.,
2010."
REFERENCES,0.5657142857142857,"Kyubyong Park.
word2vec MultiLanguages, 2016.
Available at https://github.com/
Kyubyong/wordvectors."
REFERENCES,0.5685714285714286,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014."
REFERENCES,0.5714285714285714,"Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701–710, 2014."
REFERENCES,0.5742857142857143,"D.J. de Solla Price. A general theory of bibliometric and other cumulative advantage processes.
Journal of the American Society for Information Science, 27(5):292–306, 1976."
REFERENCES,0.5771428571428572,"Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500):2323–2326, 2000."
REFERENCES,0.58,"Herbert A. Simon. On A Class Of Skew Distribution Functions. Biometrika, 42(3–4):425–440, 12
1955."
REFERENCES,0.5828571428571429,Under review as a conference paper at ICLR 2022
REFERENCES,0.5857142857142857,"Steven Skiena and Charles B Ward. Who’s bigger?: Where historical figures really rank. Cambridge
University Press, 2014."
REFERENCES,0.5885714285714285,"Suvrit Sra.
Directional statistics in machine learning: A brief review.
In Thomas Verdebout
Christophe Ley (ed.), Applied Directional Statistics, pp. 275–292. Chapman and Hall/CRC, 2018."
REFERENCES,0.5914285714285714,"Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th international conference on world
wide web, pp. 1067–1077, 2015."
REFERENCES,0.5942857142857143,"W. R. Tobler. A computer movie simulating urban growth in the detroit region. Economic Geogra-
phy, 46:234—-240, 1970."
REFERENCES,0.5971428571428572,"Andrew TA Wood. Simulation of the von mises fisher distribution. Communications in statistics-
simulation and computation, 23(1):157–164, 1994."
REFERENCES,0.6,"Han Xiao, Minlie Huang, and Xiaoyan Zhu. Transg: A generative model for knowledge graph
embedding. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 2316–2325, 2016."
REFERENCES,0.6028571428571429,"Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generat-
ing realistic graphs with deep auto-regressive models. In International conference on machine
learning, pp. 5708–5717. PMLR, 2018."
REFERENCES,0.6057142857142858,"Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui
Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. Graph embedding on biomedical
networks: methods, applications and evaluations. Bioinformatics, 36(4):1241–1251, 2020."
REFERENCES,0.6085714285714285,"George Udny Yule. Ii.—a mathematical theory of evolution, based on the conclusions of dr. j. c.
willis, f. r. s. Philosophical Transactions of the Royal Society of London. Series B, Containing
Papers of a Biological Character, 213(402-410):21–87, 1925."
REFERENCES,0.6114285714285714,"Ziwei Zhang, Peng Cui, Haoyang Li, Xiao Wang, and Wenwu Zhu. Billion-scale network embed-
ding with iterative random projection. In 2018 IEEE International Conference on Data Mining
(ICDM), pp. 787–796. IEEE, 2018."
REFERENCES,0.6142857142857143,"Kaitlyn Zhou, Kawin Ethayarajh, and Dan Jurafsky. Frequency-based distortions in contextualized
word embeddings. arXiv preprint arXiv:2104.08465, 2021."
REFERENCES,0.6171428571428571,"George Kingsley Zipf. The psycho-biology of language: An introduction to dynamic philology.
Houghton, Mifflin, 1936."
REFERENCES,0.62,"George Kingsley Zipf. Human behavior and the principle of least effort: An introduction to human
ecology. Addison-Wesley Press, 1949."
REFERENCES,0.6228571428571429,Under review as a conference paper at ICLR 2022
REFERENCES,0.6257142857142857,"A
EXPERIMENTAL DETAILS AND DATASETS"
REFERENCES,0.6285714285714286,"A.1
EXPERIMENTAL DETAILS"
REFERENCES,0.6314285714285715,"This subsection presents the experimental details of figures shown in Section 2. For each of the
model, we generate n = 2000 points in d = 3 dimension space. All models use random seed
17, which is implemented by numpy.random.RandomState(17). Other parameter settings of seven
models are as follows."
REFERENCES,0.6342857142857142,"• Gaussian model.
Each data point is from N(0, I3)."
REFERENCES,0.6371428571428571,"• Preferential Placement (PP) model.
The probability of updating the frequency of ex-
isting nodes is set to q = 0.85. The parameter of exponential distribution of radius is
β = 1.0."
REFERENCES,0.64,"• Directional PP (DPP) model.
The parameter κ is set to 1.0. The parameter of expo-
nential distribution of radius is β = 2.8 and the probability of updating the frequency of
existing nodes q where we set q = 0.85."
REFERENCES,0.6428571428571429,"• BA-Gravity.
The parameter of generating m new edges is set to 20. The initial points
are all from 3 dimensional Gaussian distribution."
REFERENCES,0.6457142857142857,"• BA-Centroid-NW, BA-Centroid-EW.
We share the same parameter setting as BA-
Gravity: the parameter of generating m new edges is set to 20. The initial points are
all from 3 dimensional Gaussian distribution."
REFERENCES,0.6485714285714286,"A.2
DATASETS"
REFERENCES,0.6514285714285715,"Table 2 lists word embedding datasets used in this paper, including multiple-languages from different
corpus and dimensions. More specifically, GloVe-CC has two versions where one is trained from
840B tokens of English Common Crawl dataset while the other one is trained from 42B."
REFERENCES,0.6542857142857142,"Embedding
Source
Corpus
# Embedding
Dim. (d)
Fasttext-Wiki
(Bojanowski
et
al.,
2017)"
REFERENCES,0.6571428571428571,"Wikipedia
294 (Languages)
300"
REFERENCES,0.66,"Fasttext-CC
(Grave et al., 2018)
Common Crawl,
Wikipedia"
REFERENCES,0.6628571428571428,"157 (Languages)
300"
REFERENCES,0.6657142857142857,"Fasttext-
Aligned"
REFERENCES,0.6685714285714286,"(Joulin et al., 2018)
Wikipedia
44 (Languages)
300"
REFERENCES,0.6714285714285714,"Polyglot
(Al-Rfou et al., 2013)
Wikipedia
137 (Languages)
64
GloVe-Twitter
(Pennington
et
al.,
2014)"
REFERENCES,0.6742857142857143,"Twitter
1 (English)
25, 50, 100, 200"
REFERENCES,0.6771428571428572,"GloVe-CC
(Pennington
et
al.,
2014)"
REFERENCES,0.68,"Common Crawl
2 (English)
300"
REFERENCES,0.6828571428571428,"GloVe-Wiki
(Pennington
et
al.,
2014)"
REFERENCES,0.6857142857142857,"Wikipedia,
Gi-
gaword 5"
REFERENCES,0.6885714285714286,"1 (English)
50,
100,
200,
300
word2vec
(Mikolov
et
al.,
2013b)"
REFERENCES,0.6914285714285714,"Google News
1 (English)
300"
REFERENCES,0.6942857142857143,"word2vec-Poly
(Park, 2016)
Wikipedia
29 (Languages)
100, 200, 300
BERT
(Devlin et al., 2018)
Wikipedia
1 (English)
786"
REFERENCES,0.6971428571428572,Table 2: Word embedding datasets.
REFERENCES,0.7,"Source
Nodes / Edges
Description"
REFERENCES,0.7028571428571428,"Coauthor
(Guo et al., 2021)
49,755 / 755,446
Citation network
Wikipeople
(Skiena & Ward, 2014)
214,010 / 622,255
Wiki hyper-link network
Patent
(Guo et al., 2021)
46,753 / 851,464
Citation network
PPI
(Yue et al., 2020)
3,890 / 76,584
Biology network for protein interaction"
REFERENCES,0.7057142857142857,Table 3: Graph datasets
REFERENCES,0.7085714285714285,Under review as a conference paper at ICLR 2022
REFERENCES,0.7114285714285714,"For each graph, we applied graph embedding algorithms (dim = 64) as following:"
REFERENCES,0.7142857142857143,"• Random walk based DeepWalk Perozzi et al. (2014),"
REFERENCES,0.7171428571428572,"• Random projection based FastRP Chen et al. (2019),"
REFERENCES,0.72,• First-order and second-order proximity based LINE Tang et al. (2015)
REFERENCES,0.7228571428571429,• Locally linear embedding LLE Roweis & Saul (2000).
REFERENCES,0.7257142857142858,"0
1000
2000
Node rank (degree) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7285714285714285,Freq. Corr. LLE
REFERENCES,0.7314285714285714,"Lin-Fit: -0.35 0.66
Exp-Fit: 0.45 -1.66"
REFERENCES,0.7342857142857143,"0
1000
2000
Node rank (degree) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7371428571428571,DeepWalk
REFERENCES,0.74,"Lin-Fit: -0.27 0.72
Exp-Fit: 0.33 -3.30"
REFERENCES,0.7428571428571429,"0
1000
2000
Node rank (degree) 0.0 0.2 0.4 0.6 0.8 1.0 LINE"
REFERENCES,0.7457142857142857,"Lin-Fit: -0.18 0.55
Exp-Fit: 0.22 -3.77"
REFERENCES,0.7485714285714286,"0
1000
2000
Node rank (degree) 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7514285714285714,FastRP
REFERENCES,0.7542857142857143,"Lin-Fit: -0.17 0.52
Exp-Fit: 0.24 -5.04"
REFERENCES,0.7571428571428571,"Figure 5: Frequency concentration of four distinct embeddings of the Wikipeople graph. The ob-
served frequency concentration is similar even though trained on different graph embedding algo-
rithms (LLE, DeepWalk, Line, FastRP)."
REFERENCES,0.76,"B
MORE DETAILS IN LANGUAGE FREQUENCY CONCENTRATION"
REFERENCES,0.7628571428571429,"B.1
FREQUENCY CONCENTRATION WITH RESPECT TO k AND d"
REFERENCES,0.7657142857142857,"Frequency concentration of different k and d.
The embedding dimension d and number of
neighbors k will affect the linear and exponential fit of frequency correlation. To see this, we study
how the parameter k and d affect the frequency correlation on embeddings of GloVe-Twitter and
GloVe-Wiki-6B. The comparison of frequency correlation w.r.t different dimensionality (d) and dif-
ferent size of neighbors (k). The GloVe embeddings considered in this figure are trained on Twitter
datasets. d is from {25, 50, 100, 200} and k is from {5, 10, 15}. We present the results in Table 4-5.
When k increases, the linear coefficient of Lin fit also increase."
REFERENCES,0.7685714285714286,"d = 25 d = 50 d = 100
d = 200
k = 5
-.533
-.537
-.548
-.615
k = 10 -.510
-.515
-.526
-.596
k = 15 -.496
-.501
-.512
-.583"
REFERENCES,0.7714285714285715,"Table 4: Coefficients of Linear fit for GloVe
Twitter Embedding."
REFERENCES,0.7742857142857142,"d = 50 d = 100 d = 200 d = 300
k = 5
-.753
-.907
-.822
-.802
k = 10 -.734
-.890
-.802
-.782
k = 15 -.721
-.879
-.790
-.770"
REFERENCES,0.7771428571428571,"Table 5: Coefficients of Linear fit for GloVe
Wiki Embedding."
REFERENCES,0.78,"B.2
FREQUENCY CORRELATION WITH RESPECT TO DIFFERENT LANGUAGES"
REFERENCES,0.7828571428571428,We consider the frequency correlation learned from different languages in Figure 6.
K,0.7857142857142857,"30K
70K
Ranking 1.0 0.5"
K,0.7885714285714286,Freq-Correlation
K,0.7914285714285715,English -0.75
K,0.7942857142857143,"30K
70K
Ranking"
K,0.7971428571428572,Spanish -0.70
K,0.8,"30K
70K
Ranking"
K,0.8028571428571428,French -0.67
K,0.8057142857142857,"30K
70K
Ranking"
K,0.8085714285714286,German -0.58
K,0.8114285714285714,"30K
70K
Ranking"
K,0.8142857142857143,Chinese -0.91
K,0.8171428571428572,"30K
70K
Ranking"
K,0.82,Russian -0.53
K,0.8228571428571428,"30K
70K
Ranking"
K,0.8257142857142857,Portuguese -0.79
K,0.8285714285714286,"30K
70K
Ranking"
K,0.8314285714285714,Italian -0.77
K,0.8342857142857143,"Figure 6: Frequency correlation of Polyglot embedding (Al-Rfou et al., 2013) as a function of
ranking of words frequency. Each embedding dimension is d = 64 and k = 10."
K,0.8371428571428572,Under review as a conference paper at ICLR 2022
K,0.84,"B.3
FREQUENCY CONCENTRATION FOR BERT CONTEXTUAL WORD EMBEDDING"
K,0.8428571428571429,"We sample 30K unique words from the most frequent words in GLoVe (English) embeddings, and
obtain their contextual embeddings from 250K sentences in Wikipedia’s text 10. Specifically, we
extract the last layer of BERT output as the intermediate word embeddings. Since one word may
appear multiple times in different sentences, its intermediate embedding may vary. So we average
all the collected intermediate embeddings for each word as its final word representation, then feed
them to our frequency concentration and cluster velocity analysis."
K,0.8457142857142858,"For example, the word “embeddings” is tokenized to be [ ’em’, ’##bed’, ’##ding’, ’##s’ ] in BERT’s
tokenizer. We extract the embeddings of the first sub-token (‘em’) as the word-level intermediate
representation in different contexts (We adopt this practice from Zhou et al. (2021) ). Then average
them up as the final contextual word embeddings for the word ‘embeddings’. Figure 7 shows the
BERT embedding also shares consistent frequency concentration as other static embeddings do."
K,0.8485714285714285,Figure 7: Frequency Concentration was also obseved in contextual word embeddings
K,0.8514285714285714,"C
PREFERENTIAL PLACEMENT (PP) MODEL"
K,0.8542857142857143,Algorithm 2 presents the generating process of PP model.
K,0.8571428571428571,Algorithm 2 Preferential Placement (PP) model
K,0.86,"1: Input: n the total frequencies of data points,
β the parameter for Exp distribution, and
q
the probability of increasing the frequency of an old word
2: r ∼Exp(β, n)
▷generate n samples of radius from distribution Exp(β)
3: θ ∼U(0, 2π, n)
▷generate n samples of angle from distribution U(0, 2π)
4: W = {w0 : 1} where w0 = 0
▷a dictionary of word-frequency pairs
5: for t ∈{1, 2, . . . , n} do
6:
randomly select an word wi from W with a prob. proportional to its frequency.
7:
if Bin(1, q) == 1 then
8:
W [wi] = W [wi] + 1
▷increase the frequency of an existing word wi by 1
9:
else
10:
wt = wi + [rt cos(θt), rt sin(θt)]⊤
▷create a new word based on wi
11:
W [wt] = 1
▷add this new word into word-frequency pairs.
12:
end if
13: end for
14: Return W"
K,0.8628571428571429,"D
DIRECTIONAL PREFERENTIAL PLACEMENT (DPP) MODEL"
K,0.8657142857142858,"The exponential distribution is used like before. The von Mises-Fisher distribution, which general-
izes the uniform distribution used earlier, for (d −1)-dimensional sphere is given by"
K,0.8685714285714285,"fvMF(x|µ, κ) :=
κd/2−1"
K,0.8714285714285714,"(2π)d/2Id/2−1(κ) exp

κµ⊤x
	
,
(3)"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8742857142857143,10Wikipedia text is from Huggingface Wikipedia-English Dataset
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8771428571428571,Under review as a conference paper at ICLR 2022
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.88,"where random variable x ∈Rd, ∥x∥= 1; µ, ∥µ∥= 1, is the mean direction parameter; κ ≥0 is
a concentration parameter; d ≥2; and Id/2−1(κ) is the modified Bessel function of the first kind at
order d/2 −1. Algorithm 3 presents the DPP model."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8828571428571429,Algorithm 3 Directional Preferential Placement (DPP) model
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8857142857142857,"1: Input: n the total frequencies of data points,
β the parameter for Exp distribution,
q the probability of increasing the frequency of an old word
2: X ∼f(x|µi, κ) //X is the direction matrix from one vMF model where xi is i-th
row of X and a specific direction randomly generated from Wood (1994)
3: W = {w0 : 1} where w0 = 1/∥w0∥
▷a dictionary of word-frequency pairs
4: for t ∈{1, 2, . . . , n} do
5:
randomly select an word wi from W with a prob. proportional to its frequency.
6:
if Bin(1, q) == 1 then
7:
W [wi] = W [wi] + 1
▷increase the frequency of an existing word wi by 1
8:
else
9:
wt = wi + rt · x⊤
i
▷create a new word based on wi
10:
W [wt] = 1
▷add this new word into word-frequency pairs.
11:
end if
12: end for
13: Return W"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8885714285714286,"E
DETAILS IN MODEL PARAMETER SENSITIVITY"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8914285714285715,"Figure 8 shows the histogram of results of each run. We repeat the experiments for 5 times with dif-
ferent random seed for each model configuration, where take the combination of hyper-parameters.
The ranges of hyper-parameter are listed below:"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8942857142857142,"n = 2000; d = 300;
random seed = [ 0 , 1 , 2 , 3 , 4 ]
[” model −uniform ”] = {}
[” model −s i n g l e −ga u ss i an ”] = {}
[” model −multi −ga u ss i an ”] = {” num cluster ” :
[ 2 , 4 , 8 ] , ” c l u s t e r s t d ” : [ 0 . 0 1 , 0 . 1 , 1 . 0 ] }
# q :
p r o b a b i l i t y
of
i n c r e a r s i n g
frequency
# exp lambda :
the
s c a l e
parameter
to
c o n t r o l
r a d i u s
# m:
the
parameter
of
g e n e r a t i n g m new edges
[” model −pref −placement ”] = {
’q ’ : [ 0 . 1 ,
0.3 ,
0.5 ,
0.7 ,
0.8 ,
0 . 9 ] ,
’ exp lambda ’
:
[1 ,2 ,4 ,8 ,16 ,32 ,64 ,128]}
[” model −dir −pref −placement ”] = { ’q ’ : [ 0 . 1 ,
0.3 ,
0.5 ,
0.7 ,
0.8 ,
0 . 9 ] ,
’ exp lambda ’ : [ 1 , 2 , 3 , 4 ] ,
” kappa ” : [ 3 2 , 6 4 , 1 2 8 ] ,
” opt ” : [ ” p o s i t i v e −weighted ” ,” negative −weighted ”]}
[” model −ba−g r a v i t y ”] = {’m’ : [ 1 0 ,
20 ,
30 ,
40 ,
50 , 60]}
[” model −ba−c e n t r o i d −edge −weighted ”] = {’m’ : [ 1 0 ,
20 ,
30 ,
40 ,
50 , 60]}
[” model −ba−c e n t r o i d −edge −average ”] = {’m’ : [ 1 0 ,
20 ,
30 ,
40 ,
50 , 60]}
[” model −ba−c e n t r o i d −node −average ”] = {’m’ : [ 1 0 ,
20 ,
30 ,
40 ,
50 , 60]}
[” model −ba−c e n t r o i d −node −weighted ”] = {’m’ : [ 1 0 ,
20 ,
30 ,
40 ,
50 , 60]}
[” model −ba−embedded ”] = ’m’ : [ 1 0 ] , ’ num walks ’ : [ 3 , 5 ] ,
walk len ’ :
[ 5 , 1 0 ] ,
’window ’ :
[5]}"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.8971428571428571,"F
MORAN’S I AND GEARY’S C"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9,"We start with the definitions. Each unit yi is characterized by its importance (e.g., frequency) and
d-dimensional location. Another important ingredient is a spatial weights matrix W with elements
wij. It is either an adjacency matrix (binary, in {0, 1}), or similarity matrix (continuous, in [0, 1])."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9028571428571428,"Now, Moran’s I is defined as:"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9057142857142857,"I :=
N
P i
P j wij P i
P"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9085714285714286,"j wij(yi −¯y)(yj −¯y))
P"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9114285714285715,"i(yi −¯y)2
."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9142857142857143,"Then, I ∈[−1, 1]; I < 0 means negative autocorrelation (a checkerboard pattern), I > 0 means
positive autocorrelation (same-color squares on one side)."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9171428571428571,Geary’s C is defined as:
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.92,"C :=
N −1
2(P i
P"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9228571428571428,"j wij) P i
P"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9257142857142857,"j wij(yi −yj)2
P"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9285714285714286,"i(yi −¯y)2
."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9314285714285714,Under review as a conference paper at ICLR 2022
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9342857142857143,"1.00
0.75
0.50
0.25 0.00
0.25
Frequency Concentration 0 10 20 30"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9371428571428572,"0.25 0.00
0.25
0.50
0.75
1.00
Moran's I 0 10 20 30"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.94,"1.0
0.5
0.0
0.5
1.0
1 - Geray's C 0 5 10 15 20"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9428571428571428,"0.2
0.3
0.4
0.5
CV-AUC 0 20 40 60"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9457142857142857,"0.1
0.2
0.3
0.4
0.5
CV-CNM 0 20 40 60"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9485714285714286,"BA-Centroid-EA
BA-Centroid-EW
BA-Centroid-NA
BA-Centroid-NW
BA-Embedded
BA-Gravity
DPP
Multi-Gaussian
PP"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9514285714285714,"Figure 8: The PDF of discussed statistics for each model running with 5 random seeds and distinct
model hyper-parameter combinations as listed above. (n = 2000, d = 300)"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9542857142857143,"For a more intuitive interpretation, it may be easier to use C′ := 1 −C. Then, C′ ∈[−1, 1]; C′ < 0
means negative autocorrelation (checkerboard), C′ > 0 means positive autocorrelation (same-color
squares on one side)."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9571428571428572,"We present the correlation between our proposed Frequency Concentration (FC) and Moran’s I as
well as with Geary’s C in Table 6 and Figure 9."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.96,"Spearman corr.
FC
Moran-I
1-Geary’s"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9628571428571429,"FC
1.000
-0.810
-0.450
Moran-I
-0.810
1.000
0.662
1-Geary’s
-0.450
0.662
1.000"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9657142857142857,"Table 6: Our proposed Frequency Concentration (FC) index is well correlated with classic Moran’s
I and Geary’s C."
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9685714285714285,"1.0
0.5
0.0
0.5
1.0
1- Geary's C 0.0 0.2 0.4 0.6 0.8 1.0"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9714285714285714,Moran's I
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9742857142857143,"Word
Graph
BA-Centroid-EA"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9771428571428571,"BA-Centroid-EW
BA-Centroid-NA
BA-Centroid-NW"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.98,"BA-Embedded
BA-Gravity
DPP"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9828571428571429,"Multi-Gaussian
PP
Single-Gaussian"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9857142857142858,"0.00
0.25
0.50
0.75
1.00
Moran's I -1.0 -0.8 -0.6 -0.4 -0.2 0.0"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9885714285714285,Frequency Concentration
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9914285714285714,"1.0
0.5
0.0
0.5
1.0
1- Geary's C -1.0 -0.8 -0.6 -0.4 -0.2 0.0"
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9942857142857143,Frequency Concentration
WIKIPEDIA TEXT IS FROM HUGGINGFACE WIKIPEDIA-ENGLISH DATASET,0.9971428571428571,"Figure 9: Moran’s I and Geary’s C are well correlated, and so does our proposed Frequency Con-
centration."
