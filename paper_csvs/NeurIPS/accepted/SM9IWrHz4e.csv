Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007645259938837921,"In recent years, significant attention has been directed towards learning average-
reward Markov Decision Processes (MDPs). However, existing algorithms either
suffer from sub-optimal regret guarantees or computational inefficiencies. In this
paper, we present the first tractable algorithm with minimax optimal regret of
eO
p"
ABSTRACT,0.0015290519877675841,"sp (h∗)S AT

,1 where sp (h∗) is the span of the optimal bias function h∗, S × A
is the size of the state-action space and T the number of learning steps. Remarkably,
our algorithm does not require prior information on sp (h∗).
Our algorithm relies on a novel subroutine, Projected Mitigated Extended Value
Iteration (PMEVI), to compute bias-constrained optimal policies efficiently. This
subroutine can be applied to various previous algorithms to improve regret bounds."
INTRODUCTION,0.0022935779816513763,"1
Introduction"
INTRODUCTION,0.0030581039755351682,"Reinforcement learning (RL) Burnetas and Katehakis [1997], Sutton and Barto [2018] has become a
popular approach for solving complex sequential decision-making tasks and has recently achieved
notable advancements in diverse fields of application. RL problems are generally formulated with
Markov Decision Processes (MDPs) Puterman [1994], where a learning agent seeks to maximize the
rewards that are gathered by interacting with an unknown environment."
INTRODUCTION,0.00382262996941896,"This paper focuses on average reward MDPs where the learning agent must maximize the sum of
rewards in the long run without any reset mechanism. In this setting, the proper balancing between
exploration (i.e., playing sub-optimally to learn the unknown environment) and exploitation (i.e.,
planning optimally according to the current knowledge), usually known as the exploration-exploitation
trade-off, is key to learn efficiently. The measure of learning performance that we adopt throughout is
the regret, that compares the aggregate rewards collected by the learning agent during the learning
process to the expected performance of an omniscient agent that knows everything in advance. The
seminal work of Auer et al. [2009] provides a minimax regret lower bound of Ω
√"
INTRODUCTION,0.0045871559633027525,"DS AT

, where
D is the diameter (the maximal distance between two different states), S the number of states, A
the number of actions and T the learning horizon. They also provide an algorithm achieving regret
eO
√"
INTRODUCTION,0.005351681957186544,"D2S 2AT

, where eO(−). Ever since Auer et al. [2009], many works have been devoted to close
the gap between the regret lower and upper bounds in the average reward setting Auer et al. [2009],
Bartlett and Tewari [2009], Filippi et al. [2010], Talebi and Maillard [2018], Fruit et al. [2018,
2020], Bourel et al. [2020], Zhang and Ji [2019], Ouyang et al. [2017], Agrawal and Jia [2023],"
INTRODUCTION,0.0061162079510703364,"1eO(·) hides logarithmic factors of (S, A, T)."
INTRODUCTION,0.006880733944954129,"Abbasi-Yadkori et al. [2019], Wei et al. [2020] and more. Subsequent works Fruit et al. [2018],
Zhang and Ji [2019] refined the minimax regret lower bound to Ω
p"
INTRODUCTION,0.00764525993883792,"sp (h∗)S AT

where sp (h∗) is the
span of the bias function, which is the maximal gap of the long-term accumulative rewards starting
from two different states. The difference is significant, since sp (h∗) ≤D and the gap between the
two can be arbitrarily large. However, no existing work achieves the following three requirements
simultaneously:"
INTRODUCTION,0.008409785932721712,"(1) The method achieves minimax optimal regret guarantees eO
p"
INTRODUCTION,0.009174311926605505,"sp (h∗)S AT

;
(2) The proposed method is tractable;
(3) No prior knowledge on the model is required."
INTRODUCTION,0.009938837920489297,"Most algorithms simply fail to achieve minimax optimal regret, and the only method achieving it
Zhang and Ji [2019] is intractable because it relies an oracle to solve difficult optimization problems
along the learning process. Naturally, we raise the question of whether these three requirements can
be met all at once:"
INTRODUCTION,0.010703363914373088,"Is there a tractable algorithm with eO
p"
INTRODUCTION,0.011467889908256881,"sp (h∗)S AT

minimax regret without prior knowledge?"
INTRODUCTION,0.012232415902140673,"Contributions.
In this paper, we answer the above question affirmatively, by proposing a polynomial
time algorithm with regret guarantees eO
p"
INTRODUCTION,0.012996941896024464,"sp (h∗)S AT

for average-reward MDPs. Our method can
further incorporate almost arbitrary prior bias information H∗⊆RS to further improve its regret."
INTRODUCTION,0.013761467889908258,"Theorem 1 (Informal). Provided that the confidence region used by PMEVI-DT satisfy mild regularity
conditions (see Assumption 1-3), then for every weakly communicating model M with sp(h∗) ≤T 1/5
and sp(h∗) ∈H∗, PMEVI-DT(H∗, δ, T) achieves regret: O
q"
INTRODUCTION,0.01452599388379205,"sp(h∗)S AT log
 S AT"
INTRODUCTION,0.01529051987767584,"δ

+ O

sp(h∗)S
5
2 A
3
2 T
9
20 log2  S AT δ
"
INTRODUCTION,0.016055045871559634,"with probability 1 −26δ. Moreover, if PMEVI-DT runs with the same confidence regions that UCRL2
Auer et al. [2009] on a communicating environment, it has a time complexity O(DS 3AT)."
INTRODUCTION,0.016819571865443424,"Taking δ = √1/T, we also obtain a eO
p"
INTRODUCTION,0.017584097859327217,"sp (h∗)S AT

regret bound in expectation. The geometry of
the prior bias region H∗that PMEVI-DT can support is discussed later (see Assumption 4). It can be
taken trivial with H∗= RS to obtain a completely prior-less algorithm."
INTRODUCTION,0.01834862385321101,"To the best of our knowledge, PMEVI-DT is the first tractable algorithm with minimax optimal regret
bounds (up to logarithmic factors). The algorithm does not necessitate any prior knowledge of sp (h∗),
thus circumventing the potentially high cost associated with learning sp (h∗). On the technical side, a
key novelty of our method is the subroutine named PMEVI (see Algorithm 2) that improves and can
replace EVI Auer et al. [2009] in any algorithm that relies on it Auer et al. [2009], Fruit et al. [2018],
Filippi et al. [2010], Fruit et al. [2020], Bourel et al. [2020] to boost its performance and achieve
minimax optimal regret."
INTRODUCTION,0.0191131498470948,"Related works on average reward MDPs.
For communicating MDPs, the notable work of Auer
et al. [2009] proposes the famous UCRL2 algorithm, a mature version of their prior UCRL Auer and
Ortner [2006], achieving a regret bound of eO(DS
√"
INTRODUCTION,0.019877675840978593,"AT). This paper pioneered the use optimistic
methods to learn MDPs efficiently. A line of papers Filippi et al. [2010], Fruit et al. [2020], Bourel
et al. [2020] developed this direction by tightening the confidence region that UCRL2 relies on, and
sharpened the analysis through the use of local properties of MDPs, such as local diameters and
local bias variances. However, none of these works went beyond regret guarantees of order S
√"
INTRODUCTION,0.020642201834862386,"DAT
and suffer from an extra
√"
INTRODUCTION,0.021406727828746176,"S . A parallel direction was initiated by Bartlett and Tewari [2009] with
REGAL, obtaining regret bounds scaling with sp(h∗) instead of D, and extending the regret bounds
to weakly-communicating MDPs in the mean time. The computational intractability of REGAL is
addressed by Fruit et al. [2018] with SCAL, and regret guarantees are further improved by Zhang and
Ji [2019] with EBF, eventually reaching optimal minimax regret but loosing tractability."
INTRODUCTION,0.02217125382262997,"Another successful design approach is Bayesian-flavored sampling, derived from Thompson Sampling
Thompson [1933], that usually replaces optimism. The regret guarantees of these algorithms usually
stick to the Bayesian setting however Ouyang et al. [2017], Theocharous et al. [2017], although"
INTRODUCTION,0.022935779816513763,"Table 1:
Comparison of related works on RL algorithms for average-reward MDP, where S ×A is the
size of state-action space, T is the total number of steps, D (Ds) is the (local) diameter, sp (h∗) ≤D is
the span of the bias vector, tmix is the worst-case mixing time, thit is the hitting time (i.e., the expected
time cost to visit some certain state under any policy)."
INTRODUCTION,0.023700305810397553,"Algorithm
Regret in eO(−)
Tractable
Comment/Requirements
REGAL Bartlett and Tewari [2009]
sp (h∗)S
√"
INTRODUCTION,0.024464831804281346,"AT
×
knowledge of sp (h∗)
UCRL2 Auer et al. [2009]
DS
√"
INTRODUCTION,0.02522935779816514,"AT
✓
-
PSRL Agrawal and Jia [2023]
DS
√"
INTRODUCTION,0.02599388379204893,"AT
✓
Bayesian regret
SCAL Fruit et al. [2018]
sp (h∗)S
√"
INTRODUCTION,0.026758409785932722,"AT
✓
knowledge of sp (h∗)
UCRL2B Fruit et al. [2020]
S
√"
INTRODUCTION,0.027522935779816515,"DAT
✓
extra
p"
INTRODUCTION,0.028287461773700305,log(T) in upper-bound
INTRODUCTION,0.0290519877675841,"UCRL3 Bourel et al. [2020]
D +
q"
INTRODUCTION,0.02981651376146789,"T P
s,a D2sLs,a
✓
Ls,a := P
s′
p"
INTRODUCTION,0.03058103975535168,"p(s′|s, a)(1 −p(s′|s, a))"
INTRODUCTION,0.03134556574923547,"KL-UCRL Filippi et al. [2010], Talebi and Maillard [2018]
S
√"
INTRODUCTION,0.03211009174311927,"DAT
✓
-
EBF Zhang and Ji [2019]
p"
INTRODUCTION,0.03287461773700306,"sp (h)∗S AT
×
optimal, knowledge of sp (h∗)
Optimistic-Q Wei et al. [2020]
sp (h∗)(S A)
1
3 T
2
3
✓
model-free
UCB-AVG Zhang and Xie [2023]
S 5A2sp (h∗)
√"
INTRODUCTION,0.03363914373088685,"T
✓
model-free, knowledge of sp (h∗)
MDP-OOMD Wei et al. [2020]
p"
INTRODUCTION,0.034403669724770644,"(tmix)2thitAT
✓
ergodic
Politex Abbasi-Yadkori et al. [2019]
(tmix)3thit
√"
INTRODUCTION,0.035168195718654434,"S AT
3
4
✓
model-free, ergodic
PMEVI-DT (this work)
p"
INTRODUCTION,0.035932721712538224,"sp (h∗)S AT
✓
-
Lower bound
Ω
p"
INTRODUCTION,0.03669724770642202,"sp (h∗)S AT

-
-"
INTRODUCTION,0.03746177370030581,"Agrawal and Jia [2023] also enjoys eO(S
√"
INTRODUCTION,0.0382262996941896,"DAT) high probability regret by coupling posterior sampling
with optimism. Another line of research focuses on the study of ergodic MDPs, where the environment
is such that all states are visited infinitely often under every policy. To name a few, the model-free
algorithm Politex Abbasi-Yadkori et al. [2019] attains a regret of eO((tmix)3thit
√"
INTRODUCTION,0.0389908256880734,"S AT
3
4 ) where tmix
and thit are respectively the mixing and the hitting times of the ergodic environment. By leveraging an
optimistic mirror descent algorithm, Wei et al. [2020] achieve an enhanced regret of eO(
p"
INTRODUCTION,0.039755351681957186,(tmix)2thitAT).
INTRODUCTION,0.040519877675840976,We refer the readers to Table 1 for a (non-exhaustive) list of existing algorithms.
PRELIMINARIES,0.04128440366972477,"2
Preliminaries"
PRELIMINARIES,0.04204892966360856,"We fix a finite state-action space structure X := S
s∈S {s} × A(s), and denote M the collection of all
MDPs with state-action space X and rewards supported in [0, 1]."
PRELIMINARIES,0.04281345565749235,"Infinite-horizon MDP.
An element M ∈M is a tuple (S, A, p, r) where p is the transition kernel
and r the reward function. The random state-action pair played by the agent at time t is denoted
Xt ≡(S t, At), and the achieved reward is Rt. A policy is a deterministic rule π : S →A and
we write Π the space of policies. When coupled with a MDP M ∈M, a policy properly defines
the distribution of (Xt, Rt) whose associated probability probability and expectation operators are
denoted Pπ
s, Eπ
s, where s ∈S is the initial state. Under M, a fixed policy has a reward function
rπ(s) := r(s, π(s)), a transition matrix Pπ, a gain gπ(s) := lim 1"
PRELIMINARIES,0.04357798165137615,"T Eπ
s[R0 + . . . + RT−1] and a bias hπ(s) :=
Cesàro- lim Eπ
s[PT−1
t=0 (Rt −g(S t))], that all together satisfy the Poisson equation hπ + gπ = rπ + Pπhπ,
see Puterman [1994]. The Bellman operator of the MDP is:"
PRELIMINARIES,0.04434250764525994,"Lu(s) := max
a∈A(s) {r(s, a) + p(s, a)u}
(1)"
PRELIMINARIES,0.04510703363914373,The optimal gain is g∗(s) := maxπ gπ(s) and the optimal bias is h∗(s) := max {hπ(s) : π s.t. gπ = g∗}.
PRELIMINARIES,0.045871559633027525,"Weakly-communicating MDPs.
M is weakly-communicating Puterman [1994], Bartlett and
Tewari [2009] if the state space can be divided into two sets: (1) the transient set, consisting in states
that are transient under all policies; (2) the non-transient set, where every state is reachable starting
from any other non-transient state. In this case, h∗is a span-fixpoint of L (see Puterman [1994]), i.e.,
Lh∗−h∗∈Re where e is the vector full of ones. We write h∗∈Fix(L). Then g∗= Lh∗−h∗and every
policy π satisfies rπ + Pπh∗≤g∗+ h∗. We accordingly define the Bellman gaps:"
PRELIMINARIES,0.046636085626911315,"∆∗(s, a) := h∗(s) + g∗(s) −r(s, a) −p(s, a)h∗≥0.
(2)"
PRELIMINARIES,0.047400611620795105,"Another important concept is the diameter, that describes the maximal distance from one state to
another state. It is given by D := sups,s′ infπ Eπ
s[inf {t ≥1 : S t = s′}]. An MDP is said communicating"
PRELIMINARIES,0.0481651376146789,"if its diameter D is finite, in which case sp(h∗) ≤sp(r)D, see Bartlett and Tewari [2009], Fruit [2019],
where sp(−) is the span function given by sp (u) := max(u) −min(u)."
PRELIMINARIES,0.04892966360856269,"Reinforcement learning.
The learner is only aware that M ∈M but doesn’t have a clue about what
M further looks like. From the past observations and the current state S t, the agent picks an available
action A(S t), receives a reward Rt and observe the new state S t+1. The regret of the agent is:"
PRELIMINARIES,0.04969418960244648,"Reg(T) := Tg∗− T−1
X"
PRELIMINARIES,0.05045871559633028,"t=0
Rt.
(3)"
PRELIMINARIES,0.05122324159021407,"Its expected value satisfies E[Reg(T)] = E[PT−1
t=0 ∆∗(Xt)] + E[h∗(S 0) −h∗(S T)] and the quantity
PT−1
t=0 ∆∗(Xt) will be referred to as the pseudo-regret. This paper focuses on minimax regret guarantees.
Specifically, for c ≥1, denote Mc := M ∈M : ∃h∗∈Fix(L(M)), sp (h∗) ≤c	 the set of weakly-
communicating MDPs that admit a bias function with span at most c. Following Auer et al. [2009],
every algorithm A, for all c > 0, we have"
PRELIMINARIES,0.05198776758409786,"max
M∈Mc EM,A[Reg(T)] = Ω
√"
PRELIMINARIES,0.052752293577981654,"cS AT

.
(4)"
PRELIMINARIES,0.053516819571865444,The goal of this work is to reach this lower bound with a tractable algorithm.
ALGORITHM PMEVI-DT,0.054281345565749234,"3
Algorithm PMEVI-DT"
ALGORITHM PMEVI-DT,0.05504587155963303,"The algorithm PMEVI-DT that we present in this work is actually a general method can be applied to
improve various existing algorithms Auer et al. [2009], Filippi et al. [2010], Fruit et al. [2018], Bourel
et al. [2020], Tewari and Bartlett [2007]. All these algorithms work episodically, by maintaining a
policy πk that drives play during a time-window {tk, . . . , tk+1 −1} called an episode. An episode rule
determines when πk should be considered obsolete and defines the time tk+1 at which the policy is
renewed. To compute πk, these algorithms follow the optimism-in-face-of-certainty (OFU) design
principle, by choosing πk that achieves the largest possible gain that is plausible under their current
information. This is done by building a confidence region Mt ⊆M for the hidden model M, then
searching for a policy π solving the optimization problem:"
ALGORITHM PMEVI-DT,0.05581039755351682,"g∗(Mt) := sup gπ(Mt) : π ∈Π, sp (gπ(Mt)) = 0	 with gπ(Mt) := sup
n
gπ( e
M) : e
M ∈Mt
o
.
(5)"
ALGORITHM PMEVI-DT,0.05657492354740061,"The design of the confidence region Mt varies from a work to another. Given a confidence region
(Mt)t≥0, OFU-algorithms work as follows: At the start of episode k, the optimization problem (5) is
solved, and its solution πk is played until the end of episode. The duration of episodes can be managed
in various ways, although the most popular is arguably the doubling trick (DT), that essentially waits
until a state-action pair is about to double the visit count it had at the beginning of the current episode
(see Algorithm 1)."
ALGORITHM PMEVI-DT,0.05733944954128441,"Notations.
In the rest of this section, we use ˆpt(s, a) (and ˆrt(s, a)) to denote the empirical transition
(and reward) of the latest doubling update before the t-th step, and further denote ˆMt := (ˆrt, ˆpt)."
ALGORITHM PMEVI-DT,0.0581039755351682,"Extended Bellman operators and EVI.
To solve (5) efficiently, the celebrated work Auer et al.
[2009] introduce the extended value iteration algorithm (EVI), that can be run whenever Mt is a
(s, a)-rectangular confidence region, meaning that Mt ≡Q
s,a(Rt(s, a) × Pt(s, a)) where Rt(s, a) and
Pt(s, a) are respectively the confidence region for r(s, a) and p(s, a) after t learning steps. EVI is the
algorithm computing the sequence defined by:"
ALGORITHM PMEVI-DT,0.058868501529051986,"vi+1(s) ≡Ltvi(s) := max
a∈A(s)
max
˜r(s,a)∈Rt(s,a)
max
˜p(s,a)∈Pt(s,a) (˜r(s, a) + ˜p(s, a) · vi)
(6)"
ALGORITHM PMEVI-DT,0.05963302752293578,"until sp (vi+1 −vi) < ϵ where ϵ > 0 is the numerical precision. When the process stops, it is known
that any policy π such that π(s) achieves Ltvi in (6) satisfies gπ(Mt) ≥g∗(M) −ϵ, hence is nearly
optimistically optimal. This process gets its name from the observation that Lt is the Bellman operator
of Mt seen as a MDP, hence EVI is just the Value Iteration algorithm Puterman [1994] ran in Mt. A
choice of action from s ∈S in Mt consists in (1) a choice of action a ∈A(s), (2) a choice of reward
˜r(s, a) ∈Rt(s, a) and (3) a choice of transition ˜p(s, a) ∈Pt(s, a); It is an extended version of A(s)."
ALGORITHM PMEVI-DT,0.06039755351681957,"Towards Projected Mitigated EVI.
Obviously, the regret of an OFU-algorithm is directly related
to the quality of the confidence region Mt. That is why most previous works tried to approach the
regret lower bound
√"
ALGORITHM PMEVI-DT,0.06116207951070336,"DS AT of Auer et al. [2009] by refining Mt. The older works of Auer et al.
[2009], Bartlett and Tewari [2009], Filippi et al. [2010] have been improved with a variance aware
analysis Talebi and Maillard [2018], Fruit et al. [2018, 2020], Bourel et al. [2020] that essentially
make use of tightened kernel confidence regions Pt. While all these algorithms successively reduce
the gap between the regret upper and lower bounds, they fail to achieve optimal regret
√"
ALGORITHM PMEVI-DT,0.06192660550458716,"DS AT.
Meanwhile, the EBF algorithm of Zhang and Ji [2019] is minimax optimal but (1) the algorithm is
intractable because it relies on an oracle to retrieve optimistically optimal policies and (2) needs prior
information on the bias function. Nonetheless, the method of Zhang and Ji [2019] strongly suggests
that inferring bias information from the available data is key to achieve minimax optimal regret."
ALGORITHM PMEVI-DT,0.06269113149847094,"Rather surprisingly and in opposition to this previous line of work, our work suggests that the choice
of the confidence region Mt has little importance. Instead, our algorithm takes an arbitrary (well-
behaved) confidence region in, infer bias information similarly to Zhang and Ji [2019] and makes
use of it to refine the extended Bellman operator (6) associated to the input confidence region. Our
algorithm can further take arbitrary prior information (possibly none) on the bias vector to tighten
its bias confidence region. The pseudo-code given in Algorithm 1 is the high level structure our
algorithm PMEVI-DT. In the next Section 3.1, we explain how (6) is refined using bias information."
ALGORITHM PMEVI-DT,0.06345565749235474,"Algorithm 1: PMEVI-DT(H∗, T, t 7→Mt)"
ALGORITHM PMEVI-DT,0.06422018348623854,"Parameters: Bias prior H∗, horizon T, a system
of confidence regions t 7→Mt"
ALGORITHM PMEVI-DT,0.06498470948012232,"1: for k = 1, 2, . . . do
2:
Set tk ←t, update confidence region Mt;
3:
H′
t ←BiasEstimation(Ft, Mt, δ):
4:
Ht ←H∗∩{u : sp (u) ≤T 1/5} ∩H′
t ;
5:
Γt ←BiasProjection(Ht, −);
6:
βt ←VarianceApprox(H′
t , Ft);
7:
hk ←PMEVI(Mt, βt, Γt,
p"
ALGORITHM PMEVI-DT,0.06574923547400612,"log(t)/t) ;
8:
gk ←Lthk −hk ;
9:
Update policy πk ←Greedy(Mt, hk, βt);
10:
repeat
11:
Play At ←πk(S t), observe Rt, S t+1;
12:
Increment t ←t + 1;
13:
until (DT) Nt(S t, πk(S t)) ≥1 ∨2Ntk(Xt).
14: end for"
ALGORITHM PMEVI-DT,0.06651376146788991,"Algorithm 2: PMEVI(M, β, Γ, ϵ)"
ALGORITHM PMEVI-DT,0.0672782874617737,"Parameters: region M, mitigation β, projection
Γ, precision ϵ, initial vector v0 (optional)"
ALGORITHM PMEVI-DT,0.06804281345565749,"1: if v0 not initialized then set v0 ←0;
2: n ←0
3: L ←extended operator associated to M;
4: repeat
5:
vn+ 1"
ALGORITHM PMEVI-DT,0.06880733944954129,"2 ←Lβvn;
6:
vn+1 ←Γvn+ 1"
ALGORITHM PMEVI-DT,0.06957186544342507,"2 ;
7:
n ←n + 1;
8: until sp (vn −vn−1) < ϵ
9: return vn."
ALGORITHM PMEVI-DT,0.07033639143730887,"3.1
Projected mitigated extended value iteration (PMEVI)"
ALGORITHM PMEVI-DT,0.07110091743119266,"Assume that an external mechanism provides a confidence region Ht for the bias function h∗. Provided
that Mt is correct (M ∈Mt) and that Ht is correct (h∗∈Ht), we want to find a policy-model pair
(π, e
M) that maximizes the gain among pairs with hπ( e
M) ∈Ht. This is done with an improved version
of (6) combining two ideas, that are both necessary to achieve minimax optimal regret in the analysis."
ALGORITHM PMEVI-DT,0.07186544342507645,"1. Projection (Section 3.2). Whenever it is correct, the bias confidence region Ht informs the
learner that the search of an optimistic model can be constrained to those with bias within
Ht. This is done by projecting Lβ
t (see mitigation) using an operator Γt : RS →Ht, that has
to satisfy a few non-trivial regularity conditions that are specified in Proposition 2.
2. Mitigation (Section 3.3). When one is aware that h∗∈Ht, the dynamical bias update
˜p(s, a)vi in (6) can be controlled better, by trying to restrict (6) to the ˜p(s, a) such that
˜p(s, a)vi ≤ˆpt(s, a)vi + (p(s, a) −ˆpt(s, a))vi with the knowledge that vi ∈Ht. However,
controlling the error (p(s, a) −ˆpt(s, a))vi by doing a union-bound on all possible values of vi
is equivalent to building a confidence region for p(s, a), which produces an extra S 1/2 in the
error term that cannot be afforded by a minimax optimal algorithm.
We take a different approach instead. For a fixed u ∈RS, the empirical Bernstein inequality
(Lemma 38) provides a variance bound of the form ( ˆpt(s, a) −p(s, a))u ≤βt(s, a, u). By"
ALGORITHM PMEVI-DT,0.07262996941896024,"estimating βt(s, a) := maxu∈Ht βt(s, a, u), the search makes sure that ( ˆpt(s, a) −p(s, a))u ≤
βt(s, a) holds with high probability for u = h∗, even though h∗is unknown. For β ∈RX
+, we
introduce the β-mitigated extended Bellman operator:"
ALGORITHM PMEVI-DT,0.07339449541284404,"Lβ
t u(s) := max
a∈A(s)
sup
˜r(s,a)∈Rt(s,a)
sup
˜p(s,a)∈Pt(s,a)"
ALGORITHM PMEVI-DT,0.07415902140672782,"n
˜r(s, a) + min { ˜p(s, a)ui, ˆpt(s, a)ui + β(s, a)}
o
(7)"
ALGORITHM PMEVI-DT,0.07492354740061162,"The mitigation β(s, a) is independent of u, which is crucial for Lβ
t to be well-behaved."
ALGORITHM PMEVI-DT,0.07568807339449542,"The proposition below shows how well-behaved the composition Lt := Γt ◦Lβ
t is. Its proof requires to
build a complete analysis of projected mitigated Bellman operators. This is deferred to the appendix."
ALGORITHM PMEVI-DT,0.0764525993883792,"Proposition 2. Fix β ∈RX
+ and assume that there exists a projection operator Γt : RX →Ht which is
(O1) monotone: u ≤v ⇒Γu ≤Γv; (O2) non span-expansive: sp (Γu −Γv) ≤sp (u −v); (O3) linear:
Γ(u + λe) = Γu + λe and (O4) Γu ≤u. Then, the projected mitigated extended Bellman operator
Lt := Γt ◦Lβ
t has the following properties:"
ALGORITHM PMEVI-DT,0.077217125382263,"(1) There exists a unique gt ∈Re such that ∃ht ∈Ht, Ltht = ht + gt;
(2) If M ∈Mt, h∗∈Ht and ( ˆpt(s, a) −p(s, a))h∗≤β(s, a), then gt ≥g∗(M);
(3) If Mt is convex, then for all u ∈RS, the policy π =: Greedy(Mt, u, β) picking the actions
achieving Lβ
t u satisfies Ltu = ˜rπ + ˜Pπu for ˜rπ(s) ≤sup Rt(s, π(s)) and ˜Pπ(s) ∈Pt(s, π(s));"
ALGORITHM PMEVI-DT,0.0779816513761468,"(4) For all u ∈RS and n ≥0, sp

Ln+1
t
u −Ln
t u

≤sp

(Lt)n+1u −(Lt)nu

."
ALGORITHM PMEVI-DT,0.07874617737003058,"The property (1) guarantees that Lt has a fix-point while (2) states that this fix-point corresponds to
an optimistic gain gt if the model and the bias confidence region are correct and the mitigation isn’t
too aggressive. Combined with (3), the Poisson equation of a policy corresponds to this fix-point, i.e.,
˜rπ + ˜Pπht = ht + gt, so that gt is the gain and ht ∈Ht is a legal bias for π under the model (˜rπ, ˜Pπ).
Lastly, the property (4) guarantees that the iterates Ln
t u converge to a fix-point of L at least as quickly
as Ln
t u goes to a fix-point of Lt; The convergence of (Lt)nu is already guaranteed by existing studies
and is discussed in the appendix."
ALGORITHM PMEVI-DT,0.07951070336391437,"Provided that the bias confidence region is constructed, Proposition 2 foreshadows how powerful
the construction is: The algorithm PMEVI, obtained by iterating Lt instead of Lt in EVI, can replace
the well-known EVI within any algorithm of the literature that relies on it (UCRL2 Auer et al. [2009],
UCRL2B Fruit et al. [2020] or KL-UCRL Filippi et al. [2010]) for an immediate improvement of its
theoretical guarantees."
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08027522935779817,"3.2
Building the bias confidence region and its projection operator"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08103975535168195,The bias confidence region used by PMEVI-DT is obtained as a collection of constraints of the form:
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08180428134556575,"∀s , s′,
h(s) −h(s′) −c(s, s′) ≤d(s, s′).
(8)"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08256880733944955,"Such constraints include (1) prior bias constraints (if any) of the form of h(s)−h(s′) ≤c∗(s, s′); (2) span
constraints of the form h(s) −h(s′) ≤c0 := T 1/5 spawning the span semi-ball {u : sp (u) ≤T 1/5}; and
(3) pair-wise constraints obtained by estimating bias differences in the style of Zhang and Ji [2019],
Zhang and Xie [2023] that we further improve. We start by defining a bias difference estimator."
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08333333333333333,"Definition 1 (Bias difference estimator). Given a pair of states s , s′, their sequence of commute
times (τs↔s′
i
)i≥0 is defined by τs↔s′
2i
:= inf{t > τs↔s′
2i−1 : S t = s} and τs↔s′
2i+1 := inf{t > τs↔s′
2i
: S t = s′}
with the convention that τs↔s′
−1
= −∞. The number of commutations up to time t is Nt(s ↔s′) :=
inf{i : τs↔s′
i
≤t}, and ˆg(t) := 1"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08409785932721713,"t
Pt−1
i=0 Ri is the empirical gain. The bias difference estimator at time T
is any quantity cT(s, s′) ∈R such that:"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08486238532110092,"Nt(s ↔s′)cT(s, s′) =
XNT (s↔s′)−1"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.0856269113149847,"t=0
(−1)i Xτs↔s′
i+1 −1"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.0863914373088685,"t=τs↔s′
i
(ˆg(T) −Rt).
(9)"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.0871559633027523,"Lemma 3. With probability 1 −2δ, for all T ′ ≤T, we have"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08792048929663608,"NT ′(s ↔s′)
h∗(s) −h∗(s′) −cT ′(s, s′)
 ≤3sp (h∗)+(1+sp (h∗))
q"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08868501529051988,8T log( 2
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.08944954128440367,"δ)+2
XT ′−1"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09021406727828746,t=0 (g∗−Rt). (10)
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09097859327217125,"Lemma 3 says that the quality of the estimator cT(s, s′) is directly linked to the number of observed
commutes between s and s′ as well as the regret. The idea is that if the algorithm makes many
commutes between s and s′ and if its regret is small, then the algorithm mostly takes optimal paths
from s to s′. The bound provided by Lemma 3 is not accessible to the learner however, because sp (h∗)
is unknown in general. To overcome this issue, sp (h∗) is upper-bounded by c0 := T 1/5. Overall, this
leads to the design of the algorithm estimating the bias confidence region as specified in Algorithm 3."
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09174311926605505,"Algorithm 3: BiasEstimation(Ft, Mt, δ)"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09250764525993883,"Parameters: History Ft, model region Mt, confi-
dence δ > 0"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09327217125382263,"1: Estimate bias differences ct via (9);
2: Estimate optimistic gain ˜g ←mink<K(t) gk;
3: Inner regret estimation B0 ←t˜g −Pt−1
i=0 Ri;"
BUILDING THE BIAS CONFIDENCE REGION AND ITS PROJECTION OPERATOR,0.09403669724770643,"4: ℓ←
q"
T LOG,0.09480122324159021,"8T log
 2"
T LOG,0.095565749235474,"δ

, c0 ←T
1
5 ;"
T LOG,0.0963302752293578,5: Estimate the bias difference errors as:
T LOG,0.09709480122324159,"dt(s, s′) ≡error(ct, s, s′) := 3c0 + (1 + c0)(1 + ℓ) + 2B0"
T LOG,0.09785932721712538,Nt(s ↔s′)
T LOG,0.09862385321100918,"6: return (ct, error(ct, −, −)), (8) defines H′
t ."
T LOG,0.09938837920489296,"Algorithm 4: BiasProjection(Ht, u)"
T LOG,0.10015290519877676,"Parameters: Ht a collection of linear constraints
(8), u ∈RS to project"
T LOG,0.10091743119266056,"1: v ←0S;
2: for s ∈S do
3:
Using linear programming, compute:
4:
v(s) ←sup {w(s) : w ≤u and w ∈Ht};
5: end for
6: return v."
T LOG,0.10168195718654434,"Coupled with prior information and span constraints, the bias confidence region Ht is a polyhedron
of the same kind as the one encountered in Zhang and Xie [2023]. When generated by constraints of
the form (8), following [Zhang and Xie, 2023, Proposition 3], one can project onto Ht in polynomial
time with Algorithm 4. Moreover, the resulting projection operator satisfies the prerequisites (O1-4)
of Proposition 2, making PMEVI (Algorithm 2) well-behaved. See Appendix B.2 for proofs.
Lemma 4. Assume that H is a set of h ∈RS satisfying a system of equations of the form of (8). If H
is non empty, then the operator Γu := BiasProjection(H, u) (see Algorithm 4) is a projection on
H and satisfies the properties (O1-4) defined in Proposition 2."
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10244648318042814,"3.3
Mitigation using finer bias dynamical error"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10321100917431193,"The fact that h∗∈Ht with high probability is used in PMEVI-DT to restrict the search of EVI by
reducing the dynamical bias error. This reduction is based on a empirical Bernstein inequality (see
Lemma 38) applied to ( ˆp(s, a) −p(s, a))u. Here, it gives that with probability 1 −δ, we have:"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10397553516819572,"( ˆpt(s, a) −p(s, a)) u ≤ s"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10474006116207951,"2V( ˆpt(s, a), u) log
 3T δ
"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10550458715596331,"max {1, Nt(s, a)}
+
3sp (u) log
 3T δ
"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10626911314984709,"max {1, Nt(s, a)} =: βt(s, a, u)
(11)"
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10703363914373089,"where V( ˆpt(s, a), u) is the variance of u under the probability vector ˆpt(s, a). More specifically, if
q is a probability on S and q ∈RS, we set V(q, u) := P
s q(s)(u(s) −q · u)2. In (11), u ∈RS,
(s, a) ∈X and T ≥1 are fixed. Once is tempted to use (11) directly to mitigate the extended Bellman
operator, but the resulting operator is ill-behaved because it loses monotony. This issue is avoided by
changing βt(s, a, u) to maxu∈Ht βt(s, a, u) in (11). The resulting inequality is not guaranteed to hold
simultaneously for all u ∈Ht and with high probability; However, it is guaranteed to hold with high
probability for u = h∗, which will be enough."
MITIGATION USING FINER BIAS DYNAMICAL ERROR,0.10779816513761468,"The variance maximization problem maxu∈Ht βt(s, a, u) is a convex maximization problem with linear
constraints. Even in very simple settings, such optimization problems are NP-hard Pardalos and
Schnitger [1988] hence computing maxu∈Ht βt(s, a, u) is not reasonable in general. Thankfully, this
value can be upper-bounded by a tractable quantity that is enough in the regret analysis. The mitigation
βt used by PMEVI-DT is provided by Algorithm 5. See Lemma 12 and Appendix A.2.2 for details."
REGRET GUARANTEES,0.10856269113149847,"4
Regret guarantees"
REGRET GUARANTEES,0.10932721712538226,"Theorem 5 thereafter shows that PMEVI-DT has minimax optimal regret under regularity assumptions
on the used confidence region Mt. Assumption 1 asserts that the confidence region holds uniformly
with high probability. Assumption 2 asserts that the reward confidence region is sub-Weissman (see"
REGRET GUARANTEES,0.11009174311926606,"Algorithm 5: VarianceApproximation(H′
t , Ft)"
REGRET GUARANTEES,0.11085626911314984,"Parameters: Bias region H′
t , history Ft
1: Extract constraints (c, error(c, −, −)) ←H′
t ;
2: Set c0 ←T
1
5 ;
3: Pick a reference point h0 ←BiasProjection(Ht, c(−, s0));
4: for (s, a) ∈X do
5:
ρ ←log
 S AT"
REGRET GUARANTEES,0.11162079510703364,"δ

/ max {1, Nt(s, a)};
6:
var(s, a) ←V( ˆpt(s, a), h0) + 8c0
P
s′∈S ˆpt(s′|s, a)c(s′, s);
7:
βt(s, a) ←
p"
REGRET GUARANTEES,0.11238532110091744,"2var(s, a)ρ + 3c0ρ or +∞if Nt(s, a) = 0;
8: end for
9: return βt."
REGRET GUARANTEES,0.11314984709480122,"Lemma 35) and Assumption 3 assumes that the model confidence region makes sure that EVI (6)
converges in the first place. Assumption 4 asserts that the prior bias region is correct."
REGRET GUARANTEES,0.11391437308868502,"Assumption 1. With probability 1 −δ, we have M ∈TK(T)
k=1 Mtk.
Assumption 2. There exists a constant C > 0 such that for all (s, a) ∈S, for all t ≤T, we have:"
REGRET GUARANTEES,0.11467889908256881,"Rt(s, a) ⊆
n
˜r(s, a) ∈R(s, a) : Nt(s, a) ∥ˆrt(s, a) −˜r(s, a)∥2
1 ≤C log
 2S A(1+Nt(s,a))"
REGRET GUARANTEES,0.1154434250764526,"δ
o
."
REGRET GUARANTEES,0.1162079510703364,"Assumption 3. For t ≥0, Mt is a (s, a)-rectangular convex region and Ln
t u converges a fix-point.
Assumption 4. The prior bias region H∗contains h∗(M) and is generated by constraints of the form:
∀s , s′,
h(s) −h(s′) ≤c∗(s, s′)
with c∗(s, s′) ∈[−∞, ∞] (possibly infinite)."
REGRET GUARANTEES,0.11697247706422019,"Refer to Appendix A.2 for the feasibility of Assumption 1, Appendix A.2.3 for Assumption 2, and
Appendix A.3 for Assumption 3.
Theorem 5 (Main result). Let c > 0. Assume that PMEVI-DT runs with a confidence region system
t 7→Mt that guarantees Assumptions 1-3. If T ≥c5, then for every weakly communicating model
with sp (h∗) ≤c and such that Assumption 4 is satisfied (h∗∈H∗), PMEVI-DT achieves regret: O
q"
REGRET GUARANTEES,0.11773700305810397,"cS AT log
 S AT"
REGRET GUARANTEES,0.11850152905198777,"δ

+ O

cS
5
2 A
3
2 T
9
20 log2  S AT δ
"
REGRET GUARANTEES,0.11926605504587157,"with probability 1 −26δ, and in expectation if δ <√1/T. Moreover, if PMEVI-DT runs with the same
confidence regions that UCRL2 Auer et al. [2009], then it enjoys a time complexity O(DS 3AT)."
REGRET GUARANTEES,0.12003058103975535,"To have a completely prior-less algorithm, pick H∗= RS. The proof of Theorem 5 is tedious and its
details are deferred to appendix. We will focus here on the main ideas."
REGRET GUARANTEES,0.12079510703363915,"Notations.
At episode k, the played policy is denoted πk. As a greedy response to hk, by Proposi-
tion 2 (3), there exists ˜rk(s) ≤sup Rtk(s, πk(s)) and ˜Pk(s) ∈Ptk(s, π(x)) such that hk + gk = ˜rk + ˜Pkhk.
The reward-kernel pair ˜Mk = (˜rk, ˜Pk) is referred to as the optimistic model of πk.
We write
Pk := Pπk(M) the true kernel and ˆPk := Pπk( ˆMtk) the empirical kernel. Likewise, we define the
reward functions rk and ˆrk. The optimistic gain and bias satisfy gk = g(πk, e
Mk) and hk = h(πk, e
Mk).
We further denote c0 = T
1
5 ."
REGRET GUARANTEES,0.12155963302752294,"The regret is first decomposed episodically with Reg(T) = P
k
Ptk+1−1
t=tk
(g∗−Rt). The first step goes
back to the analysis of UCRL2 Auer et al. [2009], and consists in upper-bounding the regret of the
episode k with optimistic quantities that are exclusive to that episode.
Lemma 6 (Reward optimism). With probabililty 1 −6δ, we have:"
REGRET GUARANTEES,0.12232415902140673,"Reg(T) ≤
X k"
REGRET GUARANTEES,0.12308868501529052,Xtk+1−1
REGRET GUARANTEES,0.12385321100917432,"t=tk
(gk −Rt) ≤
X k"
REGRET GUARANTEES,0.1246177370030581,Xtk+1−1
REGRET GUARANTEES,0.12538226299694188,"t=tk
(gk −˜rk(Xt)) + O
 q"
REGRET GUARANTEES,0.12614678899082568,"S AT log
 T"
REGRET GUARANTEES,0.12691131498470948,"δ

.
(12)"
REGRET GUARANTEES,0.12767584097859327,"We introduce the two optimistic regrets B(T) := P
k
Ptk+1−1
t=tk
(gk−Rt) and ˜B(T) := P
k
Ptk+1−1
t=tk
(gk−˜rk(Xt)).
Rewriting the summand gk −˜rk(Xt) using the Poisson equation hk + gk = ˜rk + ˜Pkhk, we get:"
REGRET GUARANTEES,0.12844036697247707,"˜B(T) =
X k"
REGRET GUARANTEES,0.12920489296636087,Xtk+1−1
REGRET GUARANTEES,0.12996941896024464,"t=tk
  ˜pk(S t) −eS t
 hk."
REGRET GUARANTEES,0.13073394495412843,"Model
region Mt"
REGRET GUARANTEES,0.13149847094801223,"Bias
region Ht"
REGRET GUARANTEES,0.13226299694189603,"Mitigation
βt"
REGRET GUARANTEES,0.13302752293577982,Projected Mitigated EVI (Alg.1)
REGRET GUARANTEES,0.13379204892966362,"Regret Reg(T)
P
k
Ptk+1−1
t=tk
(g∗−Rt)"
REGRET GUARANTEES,0.1345565749235474,"Optimistic re-
gret (Lemma 6)
P
k
Ptk+1−1
t=tk
(gk −Rt) O
p"
REGRET GUARANTEES,0.1353211009174312,"S AT log(T)
"
REGRET GUARANTEES,0.13608562691131498,"Navigation error (Lemma 7)
P
k,t(p(Xt) −eS t+1)hk"
REGRET GUARANTEES,0.13685015290519878,"Emp. bias error (Lemma 8)
P
k,t( ˆptk(Xt) −p(Xt))h∗"
REGRET GUARANTEES,0.13761467889908258,Optimism over-
REGRET GUARANTEES,0.13837920489296637,"shoot (Lemma 9)
P
k,t( ˜ptk(S t) −ˆptk(S t))hk"
REGRET GUARANTEES,0.13914373088685014,"2nd order error (Lemma 10)
P
k,t( ˆptk (S t) −p(Xt))(hk −h∗)"
REGRET GUARANTEES,0.13990825688073394,"(in expectation)
O(c0 log(S AT)) p"
REGRET GUARANTEES,0.14067278287461774,sp (h∗)S AT log(S AT)
REGRET GUARANTEES,0.14143730886850153,"S 2Ac0 log2(T)
p"
REGRET GUARANTEES,0.14220183486238533,Reg(T)
REGRET GUARANTEES,0.14296636085626913,"O

c0S 2A2T 1/4 log2(T)
"
REGRET GUARANTEES,0.1437308868501529,"Figure 1:
An overview of PMEVI-DT and its regret analysis. In the above, gk and hk are the
optimistic gain and bias functions produced by PMEVI (see Algorithm 2) at episode k, and ˆptk and ˜ptk
are respectively the empirical and optimistic kernel models at episode k."
REGRET GUARANTEES,0.1444954128440367,"The analysis proceeds by decomposing the above expression of ˜B(T) in the style of Zhang and Ji
[2019]. We write Ptk+1−1
t=tk
( ˜pk(S t) −eS t)hk as:"
REGRET GUARANTEES,0.1452599388379205,Xtk+1−1 t=tk
REGRET GUARANTEES,0.14602446483180428,"
 pk(S t) −eS t
 hk
|              {z              }
navigation error (1k)"
REGRET GUARANTEES,0.14678899082568808,"+ ( ˆpk(S t) −pk(S t)) h∗
|                   {z                   }
empirical bias error (2k)
+ ( ˜pk(S t) −ˆpk(S t)) hk
|                   {z                   }
optimistic overshoot (3k)
+ ( ˆpk(S t) −pk(S t)) (hk −h∗)
|                            {z                            }
second order error (4k)"
REGRET GUARANTEES,0.14755351681957188,
REGRET GUARANTEES,0.14831804281345565,"Each error term is bounded separately. Below, we denote V(q, u) := P
s q(s)(u(s) −q · u)2.
Lemma 7 (Navigation error). With probability 1 −7δ, the navigation error is bounded by: X k"
REGRET GUARANTEES,0.14908256880733944,Xtk+1−1
REGRET GUARANTEES,0.14984709480122324,"t=tk
(pk(S t) −eS t)hk ≤ r"
REGRET GUARANTEES,0.15061162079510704,"2
XT−1"
REGRET GUARANTEES,0.15137614678899083,"t=0 V(p(Xt), h∗) log
 T"
REGRET GUARANTEES,0.15214067278287463,"δ

+ 2S A
1
2p"
REGRET GUARANTEES,0.1529051987767584,"3B(T) log
 T"
REGRET GUARANTEES,0.1536697247706422,"δ

+ eO

T
7
20 
."
REGRET GUARANTEES,0.154434250764526,"Lemma 8 (Empirical bias error). With probability 1 −δ, the empirical bias error is bounded by: X k"
REGRET GUARANTEES,0.1551987767584098,Xtk+1−1
REGRET GUARANTEES,0.1559633027522936,"t=tk
( ˆpk(S t) −pk(S t)) h∗≤4 r"
REGRET GUARANTEES,0.15672782874617738,"S A
XT−1"
REGRET GUARANTEES,0.15749235474006115,"t=0 V(p(Xt), h∗) log
 S AT"
REGRET GUARANTEES,0.15825688073394495,"δ

+ O

log2(T)

."
REGRET GUARANTEES,0.15902140672782875,"Lemma 9 (Optimistic overshoot). With probability 1 −6δ, the optimistic overshoot is bounded by: X k"
REGRET GUARANTEES,0.15978593272171254,Xtk+1−1
REGRET GUARANTEES,0.16055045871559634,"t=tk
( ˜pk(S t) −ˆpk(S t)) hk ≤"
REGRET GUARANTEES,0.16131498470948014,"
4
q"
REGRET GUARANTEES,0.1620795107033639,"2S A PT−1
t=0 V(p(Xt), h∗) log
 S AT δ
"
REGRET GUARANTEES,0.1628440366972477,"+8(1 + c0)S
3
2 A log
3
2  S AT"
REGRET GUARANTEES,0.1636085626911315,"δ
 √B(T) + eO

T
1
4
"
REGRET GUARANTEES,0.1643730886850153,.
REGRET GUARANTEES,0.1651376146788991,"Lemma 10 (Second order error). With probability 1 −6δ, the second order error is bounded by:
X k"
REGRET GUARANTEES,0.1659021406727829,Xtk+1−1
REGRET GUARANTEES,0.16666666666666666,"t=tk
( ˆpk(S t) −pk(S t)) (hk −h∗) ≤16S 2A(1 + c0) log
1
2  S 2AT δ
 p"
REGRET GUARANTEES,0.16743119266055045,"2B(T) + eO

T
1
4 
."
REGRET GUARANTEES,0.16819571865443425,"We see that the empirical bias error (Lemma 8) and the optimistic overshoot (Lemma 9) both involve
the sum of variances PT−1
t=0 V(p(Xt), h∗), which is shown in Lemma 29 to be of order sp (h∗)sp (r)T +
PT−1
t=0 ∆∗(Xt). The pseudo-regret term PT−1
t=0 ∆∗(Xt) is bounded with the regret using Corollary 31, then
by B(T). With high probability, we obtain an equation of the form:"
REGRET GUARANTEES,0.16896024464831805,"B(T) ≤C
q"
REGRET GUARANTEES,0.16972477064220184,"(1 + sp (h∗))S AT log
 T"
REGRET GUARANTEES,0.1704892966360856,"δ

+ CS 2A(1 + c0) log2(T)
p"
REGRET GUARANTEES,0.1712538226299694,"B(T) + ˜O

T
1
4 "
REGRET GUARANTEES,0.1720183486238532,"where C is a constant. Setting α := CS 2A(1 + c0) log2(T) and β := C
p"
REGRET GUARANTEES,0.172782874617737,"(1 + sp (h∗))S AT log(T/δ) +
˜O(T 1/4), the above equation is of the form B(T) ≤β + α √B(T). Solving in B(T), we find B(T) ≤
β + 2 √αβ + α2. The dominant term is β, hence we readily obtain:"
REGRET GUARANTEES,0.1735474006116208,"B(T) ≤C
q"
REGRET GUARANTEES,0.1743119266055046,"(1 + sp (h∗))sp (r)S AT log
 T"
REGRET GUARANTEES,0.17507645259938837,"δ

+ eO

sp (h∗)sp (r)S
5
2 A
3
2 (1 + c0)T
1
4 
.
(13)"
REGRET GUARANTEES,0.17584097859327216,"Since c0 = o(T
1
4 ), we conclude that B(T) = O
p"
REGRET GUARANTEES,0.17660550458715596,"sp (h∗)S AT log(T/δ)

, ending the proof."
EXPERIMENTAL ILLUSTRATIONS,0.17737003058103976,"5
Experimental illustrations"
EXPERIMENTAL ILLUSTRATIONS,0.17813455657492355,"To get a grasp of how PMEVI-DT behaves in practice, we provide in Fig. 2 a first round of illustrative
experiments. In both, the environment is a river-swim which is a model known to be hard to learn
despite its size, with high diameter and bias span, see Appendix D for the model’s description. UCRL2"
EXPERIMENTAL ILLUSTRATIONS,0.17889908256880735,PMEVI(c=2)
EXPERIMENTAL ILLUSTRATIONS,0.17966360856269112,PMEVI(c=0.5)
EXPERIMENTAL ILLUSTRATIONS,0.18042813455657492,PMEVI(c=1)
EXPERIMENTAL ILLUSTRATIONS,0.1811926605504587,UCRL2B & PMEVI-UCRL2B
EXPERIMENTAL ILLUSTRATIONS,0.1819571865443425,UCRL2 & PMEVI-UCRL2
EXPERIMENTAL ILLUSTRATIONS,0.1827217125382263,KLUCRL & PMEVI-KLUCRL
EXPERIMENTAL ILLUSTRATIONS,0.1834862385321101,"Figure 2:
(To the left) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2
on a 3-state river-swim. PMEVI-DT is run with prior knowledge h∗(s1) ≤h∗(s2) −c ≤h∗(s3) −2c for
c ∈{0, 0.5, 1, 1.5, 2}. (To the right) Running a few algorithms of the literature on 5-state river-swim
and comparing their average regret against their PMEVI variants, obtained by changing calls to the
EVI sub-routine to calls to PMEVI."
EXPERIMENTAL ILLUSTRATIONS,0.18425076452599387,"On the first experiment, we observe that PMEVI can exploit prior bias knowledge effectively and
drastically improve the regret performance, depending on the quality of the prior region."
EXPERIMENTAL ILLUSTRATIONS,0.18501529051987767,"On the second experiment however, we observe that without prior knowledge, PMEVI has nearly
the same regret performance that its EVI counterparts, meaning that the bias confidence region is
too large to effectively improve the regret performance. This observation is first to be taken with
caution. Indeed, the regret that is being estimated above is model specific, hence is not an estimate
of the minimax regret — This being said, it undoubtedly shows that the bias confidence region is
ineffective and this can be explained as follows. On experiments, we see that most of the regret is due
to the early phase of the learning process, where proper bias information is nearly impossible to get.
Indeed, the regret is still growing linearly, so no bias information can be inferred. But in addition, this
“bad” early data pollutes the bias estimator for a long duration. In other words, while the theoretical
regret guarantees of PMEVI-DT are better than its EVI analogues, there is room to improve the bias
estimation mechanism and the practical performance."
CONCLUSION,0.18577981651376146,"6
Conclusion"
CONCLUSION,0.18654434250764526,"In this work, we have shown that regret guarantees of order
p"
CONCLUSION,0.18730886850152906,"sp(h∗)S AT log(T) can be achieved
for weakly communicating MDPs without prior knowledge, nor exponential computational cost.
In particular, regret guarantees can scale with the bias span rather than the diameter without prior
knowledge. This is in opposition to the recent results that the sample complexity cannot be bounded
in term of bias span without prior knowledge for average reward MDPs Tuynman et al. [2024], Wang
et al. [2024], Zurek and Chen [2024a,b]. This difference lies in the fact (ϵ, δ)-PAC algorithm must
produce a policy πτ after τ learning steps where τ is a stopping time, with P(gπτ ≤g∗−ϵ) ≤δ.
Implicitly, these algorithms must hereby certify that the output policy is approximately optimal. In
opposition, regret robust algorithms have no need to assess that deployed policies are indeed optimal."
CONCLUSION,0.18807339449541285,"In the end, the regret advantages of PMEVI-DT over pure EVI-based methods remain theoretical, and
the experimental shortcomings displayed in Section 5 leave a few opportunities for future work. Can
bias information be inferred more efficiently? Or, do the experiments indicate that the regret analysis
of EVI-based methods may be drastically improved?"
REFERENCES,0.18883792048929662,References
REFERENCES,0.18960244648318042,"Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced
POLITEX. 2019. arXiv:1908.10479."
REFERENCES,0.19036697247706422,"Shipra Agrawal and Randy Jia. Optimistic Posterior Sampling for Reinforcement Learning: Worst-
Case Regret Bounds. Mathematics of Operations Research, 48(1):363–392, 2023. Publisher:
INFORMS."
REFERENCES,0.191131498470948,"Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. Exploration–exploitation tradeoff using
variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902,
2009. Publisher: Elsevier."
REFERENCES,0.1918960244648318,"Peter Auer and Ronald Ortner. Logarithmic Online Regret Bounds for Undiscounted Reinforcement
Learning. Proceedings of the 19th International Conference on Neural Information Processing
Systems, December 2006."
REFERENCES,0.1926605504587156,"Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-Time Analysis of the Multiarmed Bandit
Problem. Mach. Learn., 47(2–3):235–256, May 2002."
REFERENCES,0.19342507645259938,"Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal Regret Bounds for Reinforcement
Learning. In Advances in Neural Information Processing Systems, volume 21. Curran Associates,
Inc., 2009."
REFERENCES,0.19418960244648317,"Kazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical
Journal, 19(3):357 – 367, 1967. Publisher: Tohoku University, Mathematical Institute."
REFERENCES,0.19495412844036697,"Peter L. Bartlett and Ambuj Tewari. REGAL: a regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artificial Intelligence, UAI ’09, pages 35–42, Arlington, Virginia, USA, June 2009.
AUAI Press."
REFERENCES,0.19571865443425077,"Hippolyte Bourel, Odalric Maillard, and Mohammad Sadegh Talebi. Tightening Exploration in Upper
Confidence Reinforcement Learning. In Hal Daumé III and Aarti Singh, editors, Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pages 1056–1066. PMLR, July 2020."
REFERENCES,0.19648318042813456,"Apostolos Burnetas and Michael Katehakis. Optimal Adaptive Policies for Markov Decision Pro-
cesses. Mathematics of Operations Research - MOR, 22:222–255, February 1997."
REFERENCES,0.19724770642201836,"Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time, 2020."
REFERENCES,0.19801223241590213,"Sarah Filippi, Olivier Cappé, and Aurélien Garivier. Optimism in Reinforcement Learning and
Kullback-Leibler Divergence. 2010 48th Annual Allerton Conference on Communication, Control,
and Computing (Allerton), pages 115–122, September 2010. arXiv: 1004.5229."
REFERENCES,0.19877675840978593,"Ronan Fruit. Exploration-exploitation dilemma in Reinforcement Learning under various form of
prior knowledge. PhD Thesis, Université de Lille 1, Sciences et Technologies; CRIStAL UMR
9189, 2019."
REFERENCES,0.19954128440366972,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient Bias-Span-Constrained
Exploration-Exploitation in Reinforcement Learning. Proceedings of the 35 th International
Conference on Machine Learning, 2018."
REFERENCES,0.20030581039755352,"Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Improved Analysis of UCRL2 with Empirical
Bernstein Inequality. 2020. arXiv:2007.05456."
REFERENCES,0.20107033639143732,"Anders Jonsson, Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingues, Edouard Leurent,
and Michal Valko. Planning in markov decision processes with gap-dependent sample complexity.
Advances in Neural Information Processing Systems, 33:1253–1263, 2020."
REFERENCES,0.2018348623853211,"Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning Unknown Markov Decision
Processes: A Thompson Sampling Approach. September 2017. arXiv: 1709.04570."
REFERENCES,0.20259938837920488,"Panos M. Pardalos and Georg Schnitger. Checking local optimality in constrained quadratic program-
ming is NP-hard. Operations Research Letters, 7:33–35, 1988."
REFERENCES,0.20336391437308868,"Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley Series in Probability and Statistics. Wiley, 1 edition, April 1994. ISBN 978-0-471-61977-2
978-0-470-31688-7."
REFERENCES,0.20412844036697247,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.20489296636085627,"Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-Aware Regret Bounds for
Undiscounted Reinforcement Learning in MDPs. Journal of Machine Learning Research, pages
1–36, April 2018. Publisher: Microtome Publishing."
REFERENCES,0.20565749235474007,"Ambuj Tewari and P. Bartlett. Optimistic Linear Programming gives Logarithmic Regret for Irre-
ducible MDPs. In NIPS, 2007."
REFERENCES,0.20642201834862386,"Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis. Posterior sampling
for large scale reinforcement learning. 2017. arXiv:1711.07979."
REFERENCES,0.20718654434250763,"William R Thompson. On the Likelihood that One Probability Exceeds Another in View of the
Evidence of Two Samples. Biometrika, 25(3-4):285–294, December 1933. ISSN 0006-3444."
REFERENCES,0.20795107033639143,"Adrienne Tuynman, Rémy Degenne, and Emilie Kaufmann. Finding good policies in average-reward
Markov Decision Processes without prior knowledge, May 2024."
REFERENCES,0.20871559633027523,"Shengbo Wang, Jose Blanchet, and Peter Glynn. Optimal Sample Complexity for Average Reward
Markov Decision Processes, February 2024."
REFERENCES,0.20948012232415902,"Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-
free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes. In
Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10170–10180.
PMLR, July 2020."
REFERENCES,0.21024464831804282,"Zihan Zhang and Xiangyang Ji. Regret Minimization for Reinforcement Learning by Evaluating the
Optimal Bias Function. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019."
REFERENCES,0.21100917431192662,"Zihan Zhang and Qiaomin Xie. Sharper Model-free Reinforcement Learning for Average-reward
Markov Decision Processes. In The Thirty Sixth Annual Conference on Learning Theory, pages
5476–5477. PMLR, 2023."
REFERENCES,0.2117737003058104,"Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost Optimal Model-Free Reinforcement Learning
via Reference-Advantage Decomposition. June 2020. arXiv: 2004.10019 [cs, stat]."
REFERENCES,0.21253822629969418,"Matthew Zurek and Yudong Chen. The Plug-in Approach for Average-Reward and Discounted
MDPs: Optimal Sample Complexity Analysis, 2024a. arXiv:2410.07616 [cs.LG]."
REFERENCES,0.21330275229357798,"Matthew Zurek and Yudong Chen. Span-Based Optimal Sample Complexity for Weakly Communi-
cating and General Average Reward MDPs, 2024b. arXiv:2403.11477 [cs.LG]."
REFERENCES,0.21406727828746178,"A
Construction of PMEVI-DT"
REFERENCES,0.21483180428134557,"This section provides the technical details required to understand the design of PMEVI-DT in Section 3.
We further discuss the assumptions 1-4 appearing in Theorem 5 and provide sufficient conditions so
that they are met."
REFERENCES,0.21559633027522937,"A.1
Proof of Lemma 3, estimation of the bias error"
REFERENCES,0.21636085626911314,"Fix s, s′ ∈S. We denote αT := NT(s ↔s′)(h∗(s) −h∗(s′) −cT(s, s′)). We will start by considering
the better estimator c′
T(s, s′) that satisfies the same equation (9) than cT(s, s′) but with ˆg(T) changed
to g∗, readily:"
REFERENCES,0.21712538226299694,"Nt(s ↔s′)c′
T(s, s′) =
XNT (s↔s′)−1"
REFERENCES,0.21788990825688073,"t=0
(−1)i Xτs↔s′
i+1 −1"
REFERENCES,0.21865443425076453,"t=τs↔s′
i
(g∗−Rt)."
REFERENCES,0.21941896024464833,"To avoid a typographical clutter, we write τi instead of τs↔s′
i
in the remaining of the proof and we
write α′
T := NT(s ↔s′)(h∗(s) −h∗(s′) −c′
T(s, s′)."
REFERENCES,0.22018348623853212,"(STEP 1) We start by relating the two estimators. Intuitively, ˆg(T) is a good estimator for g∗when
the regret is small. Recall that ˆg(T) := 1"
REFERENCES,0.2209480122324159,"T
PT−1
t=0 Rt, hence:
XT−1"
REFERENCES,0.2217125382262997,"t=0 |ˆg(T) −g∗| =

XT−1"
REFERENCES,0.22247706422018348,"t=0 (Rt −g∗)
 =
Reg(T)
 ."
REFERENCES,0.22324159021406728,"Therefore,"
REFERENCES,0.22400611620795108,"|αT| ≤
α′
T
 +
αT −α′
T
 ≤
α′
T
 +
XT−1"
REFERENCES,0.22477064220183487,"t=0 |ˆg(T) −g∗| ≤
α′
T
 +
Reg(T)
 ."
REFERENCES,0.22553516819571864,"We are left with upper-bounding
α′
T
."
REFERENCES,0.22629969418960244,"(STEP 2) If i is even, then S τi and S τi+1 = s′; otherwise S τi = s′ and S τi+1 = s. In both cases, we
have h∗(S τi+1) −h∗(S τi) = (−1)i(h∗(s′) −h∗(s)). Therefore, using Bellman’s equation, the quantity
A := Pτi+1−1
t=τi
(g∗−Rt) satisfies:"
REFERENCES,0.22706422018348624,"A =
Xτi+1−1"
REFERENCES,0.22782874617737003,"t=τi
 p(Xt) −eS t
 h∗+
Xτi+1−1"
REFERENCES,0.22859327217125383,"t=τi
(r(Xt) −Rt) +
Xτi+1−1"
REFERENCES,0.22935779816513763,"t=τi
∆∗(Xt)"
REFERENCES,0.2301223241590214,"=
Xτi+1−1"
REFERENCES,0.2308868501529052,"t=τi
 eS t+1 −eS t
 h∗+
Xτi+1−1"
REFERENCES,0.231651376146789,"t=τi
 p(Xt) −eS t+1
 h∗+
Xτi+1−1"
REFERENCES,0.2324159021406728,"t=τi
(r(Xt) −Rt) +
Xτi+1−1"
REFERENCES,0.23318042813455658,"t=τi
∆∗(Xt)"
REFERENCES,0.23394495412844038,"= (−1)i(h∗(s′) −h∗(s)) +
Xτi+1−1"
REFERENCES,0.23470948012232415,"t=τi
 p(Xt) −eS t+1
 h∗+
Xτi+1−1"
REFERENCES,0.23547400611620795,"t=τi
(r(Xt) −Rt) +
Xτi+1−1"
REFERENCES,0.23623853211009174,"t=τi
∆∗(Xt)."
REFERENCES,0.23700305810397554,"Multiplying by (−1)i and rearranging, h∗(s′) −h∗(s) + (−1)i+1 Pτi+1−1
t=τi
(g∗−Rt) appears to be equal to:"
REFERENCES,0.23776758409785934,(−1)i+1 Xτi+1−1
REFERENCES,0.23853211009174313,"t=τi
  p(Xt) −eS t+1
 h∗+ r(Xt) −Rt
 +
Xτi+1−1"
REFERENCES,0.2392966360856269,"t=τi
∆∗(Xt)

."
REFERENCES,0.2400611620795107,"Proceed by summing over i. By triangular inequality, we obtain:
α′
T
 ≤

XNT (s↔s′)−1 i=0"
REFERENCES,0.2408256880733945,Xτi+1−1
REFERENCES,0.2415902140672783,"t=τi
(−1)i+1   p(Xt) −eS t+1
 h∗+ r(Xt) −Rt
 +
XNT (s↔s′)−1 i=0"
REFERENCES,0.2423547400611621,Xτi+1−1
REFERENCES,0.24311926605504589,"t=τi
∆∗(Xt)."
REFERENCES,0.24388379204892965,"Because all Bellman gaps ∆∗are non-negative, the second term is upper-bounded by the pseudo-regret
PT−1
t=0 ∆∗(Xt). The first term is a martingale, and the martingale difference sequence (−1)i+1((p(Xt) −
eS t+1)h∗+ r(Xt) −Rt has span at most sp (h∗) + 1 since rewards are supported in [0, 1]. Although the
number of involved terms is random, it is upper-bounded by T, hence by the maximal version of
Azuma-Hoeffding’s inequality (Lemma 32), we have that with probability at least 1 −δ and uniformly
for T ′ ≤T,

XNT′(s↔s′)−1 i=0"
REFERENCES,0.24464831804281345,Xτi+1−1
REFERENCES,0.24541284403669725,"t=τi
(−1)i+1   p(Xt) −eS t+1
 h∗+ r(Xt) −Rt
 ≤(1 + sp (h∗))
q"
REFERENCES,0.24617737003058104,"1
2T log
 2 δ

."
REFERENCES,0.24694189602446484,"(STEP 3) We conclude that with probability 1 −δ, for all T ′ ≤T,"
REFERENCES,0.24770642201834864,"αT ′ ≤(1 + sp (h∗))
q"
REFERENCES,0.2484709480122324,"1
2T log
 2"
REFERENCES,0.2492354740061162,"δ

+
XT ′−1"
REFERENCES,0.25,"t=0 ∆∗(Xt) +
Reg(T ′)
 ."
REFERENCES,0.25076452599388377,"We are left with relating both PT ′−1
t=0 ∆∗(Xt) and
Reg(T ′)
 to PT ′−1
t=0 (˜g−Rt). Using the Bellman equation
again, we find that:

XT ′−1"
REFERENCES,0.2515290519877676,"t=0 (g∗−Rt −∆∗(Xt))
 ≤|h∗(S 0) −h∗(S T ′)| +

XT ′−1"
REFERENCES,0.25229357798165136,"t=0
  p(Xt) −eS t+1
 h∗+ (r(Xt) −Rt)"
REFERENCES,0.2530581039755352,"≤sp (h∗) + (1 + sp (h∗))
q"
REFERENCES,0.25382262996941896,"1
2T log
 2 δ
"
REFERENCES,0.2545871559633027,"where the last inequality holds with probability 1 −δ uniformly over T ′ ≤T by Azuma-Hoeffding’s
inequality again (Lemma 32). Remark that if y −z ≤x ≤y + z, then |x| ≤|y| + |z|, hence we conclude
that with probability 1 −δ, for all T ′ ≤T:"
REFERENCES,0.25535168195718655,XT ′−1
REFERENCES,0.2561162079510703,"t=0 ∆∗(Xt) +
Reg(T ′)
 ≤2
XT ′−1"
REFERENCES,0.25688073394495414,"t=0 ∆∗(Xt) + (1 + sp (h∗))
q"
REFERENCES,0.2576452599388379,"1
2T log
 2"
REFERENCES,0.25840978593272174,"δ

+ sp (h∗)"
REFERENCES,0.2591743119266055,"≤2
XT ′−1"
REFERENCES,0.2599388379204893,"t=0 (g∗−Rt) + 3(1 + sp (h∗))
q"
REFERENCES,0.2607033639143731,"1
2T log
 2"
REFERENCES,0.26146788990825687,"δ

+ 3sp (h∗)"
REFERENCES,0.2622324159021407,"≤2
XT ′−1"
REFERENCES,0.26299694189602446,"t=0 (˜g −Rt) + 3(1 + sp (h∗))
q"
REFERENCES,0.26376146788990823,"1
2T log
 2"
REFERENCES,0.26452599388379205,"δ

+ 3sp (h∗)"
REFERENCES,0.2652905198776758,"where the last inequality invokes ˜g ≥g∗. We conclude that, with probability 1 −2δ, for all T ′ ≤T,
we have:"
REFERENCES,0.26605504587155965,"NT ′(s ↔s′)(h∗(s) −h∗(s′) −cT ′(s, s′)) ≤3sp (h∗) +  1 + sp (h∗) q"
T LOG,0.2668195718654434,"8T log
 2"
T LOG,0.26758409785932724,"δ

+
XT ′−1"
T LOG,0.268348623853211,t=0 (˜g −Rt).
T LOG,0.2691131498470948,"This concludes the proof.
□"
T LOG,0.2698776758409786,"A.2
The confidence region of PMEVI-DT"
T LOG,0.2706422018348624,"The algorithm PMEVI-DT can be instantiated with a large panel of possibilities, depending on the
type of confidence region one is willing to use for rewards and kernels. In this work, we allow for
four types of confidence regions, described below. For conciseness, q ∈{r, p} is a symbolic letter
that can be a reward or a kernel and we denote Qt(s, a) the confidence region for q(s, a) at time t. If
q = r, then dim(q) = 2 (Bernoulli rewards) with Q(s, a) = [0, 1]; and if q = p, then dim(q) = S with
Q(s, a) = P(S)."
T LOG,0.2714067278287462,"(C1) Azuma-Hoeffding or Weissman type confidence regions, with Qt(s, a) taken as:
n
˜q(s, a) ∈Q(s, a) : Nt(s, a) ∥ˆqt(s, a) −˜q(s, a)∥2
1 ≤dim(q) log
 2S A(1+Nt(s,a))"
T LOG,0.27217125382262997,"δ
o
."
T LOG,0.27293577981651373,"(C2) Empirical Bernstein type confidence regions, with Qt(s, a) taken as:"
T LOG,0.27370030581039756,"
˜q(s, a) ∈Q(s, a) : ∀i, |ˆqt(i|s, a) −˜q(i|s, a)| ≤ s"
T LOG,0.27446483180428133,"2V(ˆqt(i|s,a)) log
 2 dim(q)S AT δ !"
T LOG,0.27522935779816515,"Nt(s,a)
+
3 log
 2 dim(q)S AT δ !"
T LOG,0.2759938837920489,"Nt(s,a)"
T LOG,0.27675840978593275,"
."
T LOG,0.2775229357798165,with the convention that x/0 = +∞for x > 0.
T LOG,0.2782874617737003,"(C3) Empirical likelihood type confidence regions, with Qt(s, a) taken as:"
T LOG,0.2790519877675841,"n
˜q(s, a) ∈Q(s, a) : Nt(s, a) KL(ˆqt(s, a)∥˜q(s, a)) ≤log
 2S A"
T LOG,0.2798165137614679,"δ

+ (dim(q) −1) log

e

1 + Nt(s,a)"
T LOG,0.2805810397553517,"dim q−1
o
."
T LOG,0.28134556574923547,"(C4) Trivial confidence region with Qt(s, a) = Q(s, a)."
T LOG,0.28211009174311924,"A few remarks are in order. When rewards are not Bernoulli, only the confidence regions (C1) and
(C4) are elligible among the above. Then, Weissman’s inequality must be changed to Azuma’s
inequality for σ-sub-Gaussian random variables, see Lemma 34. Since rewards are supported in
[0, 1], Hoeffding’s Lemma guarantees that reward distributions are σ-sub-Gaussian with σ = 1 2."
T LOG,0.28287461773700306,"A.2.1
Correctness of the model confidence region Mt and Assumption 1"
T LOG,0.28363914373088683,"The confidence regions Qt(s, a) described with (C1-4) are tuned so that the following result holds:"
T LOG,0.28440366972477066,"Lemma 11. Assume that, for all q ∈{r, p} and (s, a) ∈X, we choose Qt(s, a) among (C1-4). Then
Assumption 1 holds. More specifically, the region of models Mt := Q
s,a(Rt(s, a) × Pt(s, a)) satisfies
P(∃t ≤T : M < Mt) ≤δ."
T LOG,0.2851681957186544,"Proof. We show that, for all q ∈{r, q} and (s, a) ∈X, if Qt(s, a) is chosen amoung (C1-4), then"
T LOG,0.28593272171253825,"P (∃t ≤T : q(s, a) < Qt(s, a)) ≤δ."
T LOG,0.286697247706422,"If Qt(s, a) is chosen with (C1), this is a direct application of Lemma 35; with (C2), this is Lemma 36;
with (C3), this is Lemma 37; and with (C4) this is by definition.
□"
T LOG,0.2874617737003058,"A.2.2
Simultaneous correctness of bias confidence region Ht, mitigation βt and optimism"
T LOG,0.2882262996941896,"In this section, we show that if Assumption 1 holds, then the bias confidence region constructed
by PMEVI-DT is correct with high probability, and that the mitigation is not too strong. Recall that
(gk, hk) are the optimistic gain and bias of the policy deployed in episode k (see Algorithm 1). In
particular, we have gk = Ltkhk −hk with hk ∈Htk. We start by a result on the deviation of the variance,
which is what the variance approximation Algorithm 5 is based on. Recall that the bias confidence
region Ht is obtained as the collection of constraints:"
T LOG,0.2889908256880734,"(1) prior constraints (if any) h(s) −h(s′) ≤c∗(s, s′);"
T LOG,0.2897553516819572,(2) span constraints h(s) −h(s′) ≤c0 := T 1/5;
T LOG,0.290519877675841,"(3) dynamically inferred constraints |h(s) −h(s′) −ct(s′, s)| ≤error(ct, s′, s) (see Algorithm 3)."
T LOG,0.29128440366972475,We start with the technical result that is behind the variance approximation Algorithm 5.
T LOG,0.29204892966360857,"Lemma 12. Let u, v ∈Ht and fix p a probability distribution on S. Then for all s ∈S,"
T LOG,0.29281345565749234,"V(p, u) ≤V(p, v) + 8c0
X"
T LOG,0.29357798165137616,"s′∈S p(s′) error(ct, s′, s)."
T LOG,0.29434250764525993,"Proof. We start by establishing the following result: If p is a probability distribution on S and
u, v ∈RS, we have:
V(p, u) ≤V(p, v) + 2 (p · |u −v|) max(u + v)
(14)"
T LOG,0.29510703363914376,"where · is the dot product, u2 the Hadamard product uu and |u| the vector whose entry s is |u(s)|. (14)
is obtained with a straight forward computation:"
T LOG,0.2958715596330275,"V(p, u) −V(p, v) = p · (u2 −v2) + (p · v)2 −(p · u)2"
T LOG,0.2966360856269113,"= p · ((u −v)(u + v)) + (p · (u −v))(p · (u + v))
≤p · (|u −v| (u + v)) + (p · |u −v|)(p · |u + v|)
≤2(p · |u −v|) max(u + v)."
T LOG,0.2974006116207951,"Observe that v can be changed to v + λe, where e is the vector full of ones, without changing the
result. The same goes for u. We now move to the proof of the main statement. First, translate u and v
such that u(s) = v(s) = 0. Then, we have:"
T LOG,0.2981651376146789,"p · (u −v) =
X"
T LOG,0.2989296636085627,"s′∈S p(s′)
u(s′) −u(s) −ct(s′, s) + v(s) −v(s′) + ct(s′, s) ≤
X"
T LOG,0.2996941896024465,"s′∈S p(s′)
u(s′) −u(s) −ct(s′, s)
 +
v(s′) −v(s) −ct(s′, s)

 ≤2
X"
T LOG,0.30045871559633025,"s′∈S p(s′) error(ct, s′, s)."
T LOG,0.3012232415902141,"Conclude using that max(u + v) ≤max(u) + max(v) + 2c0 for u, v ∈H such that u(s) = v(s) = 0.
□"
T LOG,0.30198776758409784,"Lemma 13. Assume that Assumption 1 holds and that c0 ≥sp (h∗). Then, with probability 1 −4δ, for
all k ≤K(T), (1) gk ≥g∗and (2) h∗∈Htk and (3) for all (s, a), ( ˆptk(s, a) −p(s, a))h∗≤βtk(s, a)."
T LOG,0.30275229357798167,"Proof. Let E1 the event (∀k ≤K(T), M ∈Mtk). Let E2 the event stating that, for all T ′ ≤T,"
T LOG,0.30351681957186544,"NT ′(s ↔s′)
h∗(s) −h∗(s′) −cT ′(s, s′)
 ≤3sp (h∗) + (1 + sp (h∗))
q"
T LOG,0.30428134556574926,8T log( 2
T LOG,0.30504587155963303,"δ) + 2
XT ′−1"
T LOG,0.3058103975535168,"t=0 (g∗−Rt),"
T LOG,0.3065749235474006,"and let E3 the event stating that, for all T ′ ≤T and for all (s, a) ∈X, we have:"
T LOG,0.3073394495412844,"( ˆpT ′(s, a) −p(s, a)) h∗≤ r"
T LOG,0.3081039755351682,"2V( ˆpT′(s,a),h∗) log( S AT"
T LOG,0.308868501529052,"δ )
NT′(s,a)
+
3sp(h∗) log( S AT"
T LOG,0.30963302752293576,"δ )
NT′(s,a)
."
T LOG,0.3103975535168196,"By Assumption 1, we have P(E1) ≥1 −δ. By Lemma 3, we have P(E2) ≥1 −2δ and by Lemma 36,
we have P(E3) ≥1 −δ, so P(E1 ∩E2 ∩E3) ≥1 −4δ. We prove by induction on k ≤K(T) that, on
E1 ∩E2 ∩E3, (1) gk ≥g∗, (2) h∗∈Htk (3) and for all (s, a), ( ˆptk(s, a) −p(s, a))h∗≤βtk(s, a), where
gk is the optimistic gain of the policy deployed at episode k."
T LOG,0.31116207951070335,"It is obvious for k = 0. Indeed, N0(s ↔s′) = 0 for all s, s′ hence c0(s, s′) = c0 ≥sp (h∗). Therefore,"
T LOG,0.3119266055045872,"H0 ⊇
n
h ∈RS : sp (h) ≤c0
o
⊇
n
h ∈RS : sp (h) ≤sp (h∗)
o"
T LOG,0.31269113149847094,"so contains h∗, proving (2). Moreover, since N0(s, a) = 0, we have β0(s, a) = +∞, proving (3). Finally,
since M ∈M0 on E1, by the statement (2) of Proposition 2, we have gk ≥g∗, hence proving (1)."
T LOG,0.31345565749235477,"Now assume that k ≥1. By induction gℓ≥g∗for all ℓ< k, so on E2 we have:"
T LOG,0.31422018348623854,"Ntk(s ↔s′)
h∗(s) −h∗(s′) −ctk(s, s′)
 ≤3sp (h∗)+(1+sp (h∗))
q"
T LOG,0.3149847094801223,8T log( 2
T LOG,0.31574923547400613,"δ)+2
Xk−1 ℓ=1"
T LOG,0.3165137614678899,Xtℓ+1−1
T LOG,0.3172782874617737,"t=tℓ
(gℓ−Rt)."
T LOG,0.3180428134556575,"By design of Htk (see Algorithm 3), we deduce that (2) h∗∈Htk. Denote h0 ∈Htk the reference point
used by Algorithm 5. We have, for all (s, a) ∈X, on E1 ∩E2 ∩E3, we have:"
T LOG,0.31880733944954126,"  ˆptk(s, a) −p(s, a) h∗≤ r"
T LOG,0.3195718654434251,"2V( ˆptk (s,a),h∗) log( S AT"
T LOG,0.32033639143730885,"δ )
Ntk (s,a)
+
3sp(h∗) log( S AT"
T LOG,0.3211009174311927,"δ )
Ntk (s,a)"
T LOG,0.32186544342507645,(h∗∈Htk + Lemma 12) ≤ r
T LOG,0.32262996941896027,"2(V( ˆptk (s,a),h0) log( S AT"
T LOG,0.32339449541284404,"δ )+8c0
P
s′∈S ˆptk (s′|s,a) error(ctk ,s′,s)) log( S AT"
T LOG,0.3241590214067278,"δ )
Ntk (s,a)
+
3c0 log( S AT"
T LOG,0.32492354740061163,"δ )
Ntk (s,a)"
T LOG,0.3256880733944954,"=: βtk(s, a)"
T LOG,0.3264525993883792,"by construction of Algorithm 5. Accordingly, (3) is satisfied. Finally, M ∈Mtk on E1 so by
Proposition 2, we have (1) gk ≥g∗.
□"
T LOG,0.327217125382263,"Corollary 14. Assume that, for all q ∈{r, p} and (s, a) ∈X, we choose Qt(s, a) among (C1-4). Then,
with probability 1 −4δ, for all k ∈K(T), we have gk ≥g∗and (2) h∗∈Htk and (3) for all (s, a),
( ˆptk(s, a) −p(s, a))h∗≤βtk(s, a)."
T LOG,0.32798165137614677,"Proof. By Lemma 11, Assumption 1 is satisfied. Apply Lemma 13.
□"
T LOG,0.3287461773700306,"A.2.3
Sub-Weissman reward confidence region and Assumption 2"
T LOG,0.32951070336391436,"Although the kernel confidence region can even chosen to be trivial with (C4), in order to work,
PMEVI-DT needs the reward confidence region to be sub-Weissman in the following sense:"
T LOG,0.3302752293577982,"Assumption 2.
There exists a constant C > 0 such that for all (s, a) ∈S, for all t ≤T, we have:"
T LOG,0.33103975535168195,"Rt(s, a) ⊆
n
˜r(s, a) ∈R(s, a) : Nt(s, a) ∥ˆrt(s, a) −˜r(s, a)∥2
1 ≤C log
 2S A(1+Nt(s,a))"
T LOG,0.3318042813455658,"δ
o
."
T LOG,0.33256880733944955,"This is indeed the case if Rt(s, a) is chosen among (C1-3)."
T LOG,0.3333333333333333,"A.3
Convergence of EVI and Assumption 3"
T LOG,0.33409785932721714,"We start with a preliminary lemma on the speed of convergence of EVI. The Lemma 15 is thought to
be applied to extended MDPs. Below, when we claim that the action space is compact, we further
claim that a ∈A(s) 7→p(s, a) is a continuous map, so that the Bellman operator is continuous and
that g∗and h∗are well-defined, see Puterman [1994]."
T LOG,0.3348623853211009,"Lemma 15. Let M a weakly-communicating MDP with finite state space RS and compact action
space, and let L its Bellman operator. Assume that there exists γ > 0 such that, ∀u ∈RS,"
T LOG,0.33562691131498473,"∀s ∈S, ∃a ∈A(s),
Lu(s) = r(s, a) + p(s, a)u = r(s, a) + γ max(u) + (1 −γ)qu
su
(∗)"
T LOG,0.3363914373088685,"with qu
s ∈P(S). Then, for all u ∈RS and all ϵ > 0, if sp

Ln+1u −Lnu

≥ϵ, then:"
T LOG,0.33715596330275227,n ≤2 + 4sp (w0)
T LOG,0.3379204892966361,"γϵ
+ 2"
T LOG,0.33868501529051986,"γ log
 2sp (w0) ϵ !
."
T LOG,0.3394495412844037,"Proof. Since M is weakly communicating, has finitely many states and compact action space, it has
well-defined gain g∗and bias h∗functions. Denote un+1 := Lnu."
T LOG,0.34021406727828746,"wn := max
π∈Π {rπ + Pπun−1} −ng∗−h∗"
T LOG,0.3409785932721712,"= max
π∈Π {rπ −g∗+ (Pπ −I)h∗+ Pπ (un−1 −h∗−(n −1)g∗)} =: max
π∈Π
r′
π + Pπwn−1
	 ."
T LOG,0.34174311926605505,"Observe that the policy achieving the maximum is the one achieving un = rπ + Pπun−1. Remark
that r′
π(s) = −∆∗(s, π(s)) ≤0 is the Bellman gap of the pair (s, π(s)), that we more simply write ∆π.
For all n, there exists πn ∈Π such that wn+1 = −∆πn + Pπnwn. Moreover, by assumption, we have
Pπn = γ · e⊤
sne + (1 −γ)Qn where Qn is a stochastic matrix. Moreover,
 min(−∆πn) + γwn(sn) e + (1 −γ)Qnwn ≤wn+1 ≤ max(−∆πn) + γwn(sn) e + (1 −γ)Qnwn."
T LOG,0.3425076452599388,"Hence, sp (wn+1) ≤(1 −γ)sp (wn) + sp  ∆πn
. In addition, wn = Lnu −Lnh∗, so by non-expansiveness
of L in span semi-norm, sp (wn+1) ≤sp (wn). Overall,"
T LOG,0.34327217125382264,"sp (wn+1) ≤min  (1 −γ)sp (wn) + sp  ∆πn
, sp (wn) .
(15)"
T LOG,0.3440366972477064,"Fix ϵ > 0, and let nϵ := inf n : sp (wn) < ϵ	."
T LOG,0.34480122324159024,"Let π∗an optimal policy. We have wn+1 ≥Pπ∗wn so by induction, wn+1 ≥Pn+1
π∗w0 ≥min(w0)e.
Meanwhile, we see that ∥wn∥1 ≥Pn−1
k=0
∆πk
1 + S min(w0), so Pn−1
k=0
∆πk
1 ≤sp (w0). Since ∆πk ≤0
for all k, we have sp  ∆πk
 ≤
∆πk
1 so Pn−1
k=0 sp  ∆πk
 ≤sp (w0)."
T LOG,0.345565749235474,"By (15), either sp (wn+1) ≤(1 −1"
T LOG,0.3463302752293578,"2γ) max(ϵ, sp (wn)) or sp  ∆πn
 ≥1"
T LOG,0.3470948012232416,"2γϵ, but because P+∞
k=0 sp  ∆πk
 ≤
sp (w0), the second case can happen at most 2sp(w0)"
T LOG,0.34785932721712537,"γϵ
times. We deduce that, for all n ≤nϵ,"
T LOG,0.3486238532110092,"sp (wn+1) ≤

1 −1"
T LOG,0.34938837920489296,"2γ
n−
2sp(w0)"
T LOG,0.35015290519877673,"γϵ
sp (w0)."
T LOG,0.35091743119266056,"In particular, for n = nϵ −1, we get:"
T LOG,0.3516819571865443,"ϵ ≤

1 −1"
T LOG,0.35244648318042815,"2γ
nϵ−2−
2sp(w0)"
T LOG,0.3532110091743119,"γϵ
sp (w0)."
T LOG,0.35397553516819574,We obtain:
T LOG,0.3547400611620795,nϵ ≤2 + 2sp (w0)
T LOG,0.3555045871559633,"γϵ
+ 2"
T LOG,0.3562691131498471,"γ log
 sp (w0) ϵ !
."
T LOG,0.3570336391437309,"To conclude, check that sp

Ln+1u −Lnu

= sp (wn+1 −wn) ≤2sp (wn).
□"
T LOG,0.3577981651376147,"Before moving to the application of interest, remark that this result can be greatly improved if the
supremum sup {∆∗(s, a) : ∆∗(s, a) < 0} is not zero, to change the dominant term 4sp(w0)"
T LOG,0.35856269113149847,"γϵ
for a constant
independent of ϵ."
T LOG,0.35932721712538224,"Corollary 16. Assume that the Mt has non-empty interior, and that its Bellman operator satisfies the
requirement of Lemma 15, i.e., there exists γ > 0 such that, ∀u ∈RS, ∀s ∈S, ∃a ∈A(s), ∃˜rt(s, a) ∈
Rt(s, a), ∃˜pt(s, a) ∈Pt(s, a):"
T LOG,0.36009174311926606,"Ltu(s) = ˜rt(s, a) + ˜pt(s, a)u = ˜rt(s, a) + γ max(u) + (1 −γ)qu
su"
T LOG,0.36085626911314983,"for some qu
s ∈P(S). Then Assumption 3 is satisfied, and span fix-points ˜ht of Lt are such that
g∗(Mt) = Lt˜ht −˜ht."
T LOG,0.36162079510703365,"Proof. If Mt is has non-empty interior, it means that for all (s, a), Pt(s, a) has non-empty interior.
Therefore, for all state-action pair, there exists ˜pt(s, a) ∈Pt(s, a) that is fully supported. It follows that
Mt is communicating, and it follows from standard results Puterman [1994] that its span fix-points ˜h
do exist and that ˜gt := L˜ht −˜ht ∈Re does not depend on the initial state."
T LOG,0.3623853211009174,"Moreover, if e
M ∈Mt and π ∈Π with ˜gπ ≡g(π, Mt) ∈Re, letting ˜rπ := rπ( ˜M) and ˜Pπ := Pπ( ˜M), we
have:
˜rπ + ˜pπ˜ht ≤Lt˜ht ≤˜gte + ˜ht.
So by induction and since Lt is obviously monotone and linear, we show that: n
X"
T LOG,0.36314984709480125,"k=0
˜Pk
π˜rπ ≤n˜gte + (I −˜Pn
π)˜hπ."
T LOG,0.363914373088685,"Dividing by n and letting it go to infinity, we obtain g(π, Mt) ≤˜gt. Observe that we have equility by
taking the policy achieving (˜gt, ˜ht)."
T LOG,0.3646788990825688,"To see that EVI converges indeed, simply observe that Lemma 15 provides a finite bound on how
much time is required until the sp

Ln+1
t
u −Ln
t u

≤ϵ. Hence sp

Ln+1
t
u −Ln
t u

vanishes to 0.
□"
T LOG,0.3654434250764526,"About Assumption 3.
The assumptions made by Corollary 16 are met if the kernel confidence
regions are:"
T LOG,0.3662079510703364,"• Built out of Weissman’s inequality (C1) (see the next section, also Auer et al. [2009]);
• Built out of Bernstein’s inequality (C2) (because the maximization algorithm to compute
˜pt(s, a)ui in EVI has the same greedy properties than with Weissman’s inequality);
• Trivial (C4)."
T LOG,0.3669724770642202,"For confidence regions build with empirical likelihood estimates (C3), there is no guarantee of
convergence (although we conjecture that one could be established), although the gain is still well-
defined because Mt remains communicating. However, just like the original work of Filippi et al.
[2010], the convergence is always met numerically."
T LOG,0.367737003058104,"A.4
Proof of Theorem 5: Complexity of PMEVI with Weissman confidence regions"
T LOG,0.36850152905198774,"In this section, we show that when one is using Weissman confidence regions for kernels (C1), then
the iterates of Lt converge to an ϵ span-fix-point quickly.
Proposition 17. Assume that PMEVI-DT uses kernel confidence regions of Weissman type (C1)
satisfying Assumption 1. Then with probability 1 −δ, the number of iterations of PMEVI (see
Algorithm 2) is O

D
√"
T LOG,0.36926605504587157,"S AT

, hence the algorithm has polynomial per-step amortized complexity."
T LOG,0.37003058103975534,"Proof. With Weissman type confidence regions for kernels, for all t ≤T and (s, a) ∈X, we have"
T LOG,0.37079510703363916,"Pt(s, a) ⊇"
T LOG,0.37155963302752293,"˜p(s, a) ∈P(s, a) : ∥˜p(s, a) −ˆpt(s, a)∥1 ≤ r"
T LOG,0.37232415902140675,S log(2S AT) T
T LOG,0.3730886850152905,
T LOG,0.3738532110091743,"It follows that, for all t ≤T, the extended Bellman operator Lt satisfies the prerequisite (∗) of
Lemma 15 with γ = 1 2 r"
T LOG,0.3746177370030581,"S log(2S AT/δ) T
= Ω"
T LOG,0.3753822629969419, r
T LOG,0.3761467889908257,S log(T/δ) T
T LOG,0.3769113149847095,.
T LOG,0.37767584097859325,"Under Assumption 1, we have M ∈Mt with probability 1 −δ. Under this event, Mt is weakly
communicating and sp (h∗(Mt)) ≤D(M), we can apply Lemma 15 and conclude that every calls to
PMEVI (Algorithm 2) takes O"
T LOG,0.37844036697247707,
T LOG,0.37920489296636084,"sp (w0)
√ T ϵ
q"
T LOG,0.37996941896024466,S log(T/δ) T
T LOG,0.38073394495412843,"
= O

DT
√"
T LOG,0.38149847094801226,S log(T)
T LOG,0.382262996941896,
T LOG,0.3830275229357798,"where we use that ϵ =
q"
T LOG,0.3837920489296636,log(S AT/δ)
T LOG,0.3845565749235474,"T
, that sp (w0) = O  sp (h∗(Mt)) = O(D(M)) and that δ ≥1"
T LOG,0.3853211009174312,"T . Since
the number of episodes under the doubling trick (DT) is O(S A log(T)), we conclude accordingly.
□"
T LOG,0.386085626911315,"Every call to the projection operator solves a linear program. Although in theory, this time is
polynomial (relying on recent work on the complexity of LP such as Cohen et al. [2020], it is the
current matrix multiplication time O(S 2.38)), in practice, reducing the number of calls to the projection
operator is key to run PMEVI-DT in reasonable time."
T LOG,0.38685015290519875,"B
Analysis of the projected mitigated Bellman operator"
T LOG,0.3876146788990826,"In this section, we fix the model region M, the bias region H and the mitigation vector β, dropping
the sub-script t for conciseness. We denote ˆr, ˆp the respective empirical reward and kernel. Further
assume that H = H0 + Re with H0 a compact convex set. The associated projection operation
(see Appendix B.2) is denoted Γ. The (vanilla) extended Bellman operator L associated to M is
given by Lu(s) := maxa∈A(s)
sup R(s, a) + sup P(s, a)u	. The β-mitigated extended Bellman operator
associated to M is:"
T LOG,0.38837920489296635,"Lβu(s) := max
a∈A(s)
sup
˜r(s,a)∈R(s,a)
sup
˜p(s,a)∈P(s,a)"
T LOG,0.38914373088685017,"n
˜r(s, a) + min { ˜p(s, a)ui, ˆp(s, a)ui + β(s, a)}
o
.
(16)"
T LOG,0.38990825688073394,"The function Greedy(M, u, β) returns a stationary deterministic policy that picks its actions among
the one reaching the maximum above. The projection of Lβ to H is"
T LOG,0.39067278287461776,"L ≡Lβ,H := Γ ◦Lβ.
(17)"
T LOG,0.39143730886850153,The goal of this section is to establish Proposition 2 and
T LOG,0.3922018348623853,"• Proposition 2 statement (1) is a consequence of Lemma 22;
• Proposition 2 statement (2) follows from Theorem 25;
• Proposition 2 statement (3) follows from Corollary 27;
• Proposition 2 statement (4) follows from Corollary 21;
• Proposition 2 prerequisites on the projection operator and Lemma 4 follows from Lemma 19."
T LOG,0.3929663608562691,"B.1
Finding an optimistic policy under bias constraints"
T LOG,0.3937308868501529,"The main goal is to find and optimistic policy under bias constraints (projection) and bias error
constraints (mitigation). The bias constraints imply that we search for a policy π together with a
model e
M such that hπ( e
M) ∈H. The bias error means that, for ˜h ≡hπ( e
M), we want in addition
˜p(s, π(s))˜h ≤ˆp(s, π(s))˜h + β(s, π(s)) where ˜p is the transition kernel of e
M. In the end, our goal is to
track the solution of the following optimization problem:"
T LOG,0.3944954128440367,"g∗(H, β, M) := sup"
T LOG,0.3952599388379205,"gπ( e
M) :
π ∈Π, e
M ∈M,
∀s ∈S, ˜p(s, π(s))˜h ≤ˆp(s, π(s))˜h + β(s, π(s)),
˜h ≡hπ( e
M) ∈H, sp(gπ( e
M)) = 0"
T LOG,0.39602446483180426,"
(18)"
T LOG,0.3967889908256881,"where the supremum is taken with respect to the product order RS. In particular, if U ⊆RS, check
that u∗= sup U is obtained as u∗(s) := sup {v(s) : v ∈U}. The constraint sp(gπ( e
M)) = 0 is suggested
by the work of Fruit et al. [2018], Fruit [2019] and is key for the problem to be solvable."
T LOG,0.39755351681957185,"The bias constraint and the constraint involving β make the problem impossible to handle with a
“pure” extended MDP solution, which is why the extended Bellman operators are mitigated (with
β) then projected (with Γ). The mitigation operation guarantees that the constraint involving β is
satisfied, while the projection on H makes sure that the bias constraint is satisfied. It is important for
both operations to be compatible, i.e., that the constraint involving β that Lβ forces is not lost when
applying Γ. As a matter of fact, projecting then mitigating would not work."
T LOG,0.3983180428134557,We now explain why L can be used to solve (18).
T LOG,0.39908256880733944,"B.2
Projection operation and definition of L"
T LOG,0.39984709480122327,"We start by discussing why L is well-defined at all. The well-definition of Lβ is obvious. The point is
to explain why the projection onto H is possible while preserving mandatory structural properties
such as monotony, non-expansivity, linearity and more. For general H, such properties are impossible
to meet. But the bias confidence region constructed with Algorithm 3 has a specific shape that makes
the projection possible. The central property is the one below:"
T LOG,0.40061162079510704,(A1) The downward closure {v ≤u : v ∈H} of every u ∈RS has a maximum in H.
T LOG,0.4013761467889908,"The only order that we will be considering is the product order on RS. Recall that a set U ⊆RS
has a maximum if there exists u ∈U such that v ≤u for all u ∈U. A supremum of U is a"
T LOG,0.40214067278287463,"minimal upper-bound of U, i.e., u such that (1) v ≤u for all v ∈U and (2) no w satisfying (1) can
be smaller than u. For the product order, the supremum of a subset U is unique and of the form
u(s) = sup {v(s) : v ∈U}."
T LOG,0.4029051987767584,Define the projection Γ : RS →H as such:
T LOG,0.4036697247706422,"Γu := max {v ≤u : v ∈H} .
(19)"
T LOG,0.404434250764526,"In general, Assumption (A1) is satisfied when H admits a join, i.e., is stable by finite supremum:
u, v ∈H ⇒sup(u, v) ∈H."
T LOG,0.40519877675840976,"Lemma 18. If H is generated by constraints of the form h(s) −h(s′) −c(s, s′) ≤d(s, s′), then it has
a join and (A1) is satisfied. Moreover, Γ is then correctly computed with Algorithm 4."
T LOG,0.4059633027522936,"Proof. The first half of the result is well-known, see Zhang and Xie [2023], but we recall a proof
for self-containedness. Let v1, v2 ∈H and define v3 := sup(v1, v2). Observe that v3(s) −v3(s′) ≤
max(v1(s) −v1(s′), v2(s) −v2(s′)) ≤c(s, s′) + d(s, s′). So v3 ∈H."
T LOG,0.40672782874617736,"We continue by showing that if H has a join, then (19) is well-defined. For s ∈S, take a sequence
vs
n such that vs
n(s) →α(s) := sup {v(s) : v ≤u, v ∈H}. Because the span of every element of H
is upper-bounded by c := sup sp (v) : v ∈H	, it follows that vs
n evolves in the compact region
{v ≤u : v ∈H} ∩{v : ∥v −αse∥∞= 1 + c}. We can therefore extract a convergent sequence of vs
n,
converging vs
∗that belongs to H since the latter is closed. By construction, vs
∗(s) = α(s). Because H
has a join, v∗:= sup vs
∗: s ∈S	 ∈H.
□"
T LOG,0.4074923547400612,"Lemma 19. Under assumption (A1), the operator Γu := max {v ≤u : v ∈H} is well-defined, and is:"
T LOG,0.40825688073394495,"(1) monotone: u ≤v ⇒Γu ≤Γv;
(2) non span-expansive: sp (Γu −Γv) ≤sp (u −v);
(3) linear: Γ(u + λe) = Γu + λe;
(4) Γu ≤u."
T LOG,0.4090214067278288,"Proof. The well-definition of Γ is obvious from (A1). For (2), if u ≤v then w ≤u ⇒w ≤v.
Hence Γu := max {w ≤u : w ∈H} ≤max {w ≤v : w ∈H} =: Γv. For (3), check that it follows from
H = H + Re. For (4), we obviously have Γu := max {v ≤u : v ∈H} ≤u."
T LOG,0.40978593272171254,"The more difficult point is (2) span non-expansivity. Pick u, v ∈RS. By linearity, it suffices to show
the result for P
s u(s) = P
s v(s). In that case, we have sp (v −u) = max(v −u) + max(u −v). Observe
that for all w ≤u, we have w + min(v −u)e ≤v. Since H = H + Re, it follows that:"
T LOG,0.4105504587155963,max {w ≤u : u ∈H} ≤max {w ≤v : w ∈H} + max(u −v)e.
T LOG,0.41131498470948014,"Similarly, we have max {w ≤u : w ∈H} ≥max {w ≤v : w ∈H} + min(v −u)e. Using them both at
once, we find sp (Γu −Γv) ≤sp (v −u).
□"
T LOG,0.4120795107033639,"The properties (1), (3) and (4) are essential for L to properly address the optimization problem (18).
The property (2) is just as important, because it plays a central part in the convergence of value
iteration. The next result shows similar properties for the β-mitigated extended Bellman operator
Lβ. From now on, we will assume (A1), because it is almost-surely satisfied by the bias confidence
region generated by Algorithm 3."
T LOG,0.41284403669724773,"Lemma 20. The β-mitigated extended Bellman operator Lβ is (1) monotone, (2) non-span-expansive
and (3) linear."
T LOG,0.4136085626911315,"Proof. The properties (1) and (3) directly follow from the definition. We focus on (2). Fix u, u′ ∈RS.
By Lemma 26, we can write Lβu = ˜rπ + ˜Pπu and Lβu′ = ˜rπ′ + ˜Pπ′u′. In the following, we write
βπ(s) := β(s, π(s)). Check that:"
T LOG,0.41437308868501527,"Lβu −Lβu′ = ˜rπ + ˜Pπu −

˜rπ′ + ˜Pπ′u′
≤˜rπ + ˜Pπu −

˜rπ + min
n ˜Pπu′, ˆPπu′ + βπ
o
."
T LOG,0.4151376146788991,"If the minimum is reached with ˜Pπu′, then:"
T LOG,0.41590214067278286,Lβu −Lβu′ ≤˜Pπ(u −u′).
T LOG,0.4166666666666667,"If the minimum is reached with ˆPπu′ + βπ, then upper-bound ˜Pπu by ˆPπu + βπ to obtain:"
T LOG,0.41743119266055045,Lβu −Lβu′ ≤ˆPπ(u −u′).
T LOG,0.4181957186544342,"Overall, we find that there exists Qπ ∈Pπ such that Lβu −Lβu′ ≤Qπ(u −u′). Similarly, we find
Qπ′ ∈Pπ′ such that Lβu −Lβu′ ≥Qπ′(u −u′). We conclude that:"
T LOG,0.41896024464831805,"sp

Lβu −Lβu′
≤sp  (Qπ −Qπ′)(u −u′) ≤sp  u −u′."
T LOG,0.4197247706422018,"This concludes the proof.
□"
T LOG,0.42048929663608564,"By composition, we obtain the following result.
Corollary 21. L is (1) monotone, (2) non-span-expansive and (3) linear. Moreover, sp (Lu −Lv) ≤
sp (Lu −Lv) for all u, v ∈RS."
T LOG,0.4212538226299694,"B.3
Fix-points of L and (weak) optimism"
T LOG,0.42201834862385323,"Lemma 22. L has a fix-point in span semi-norm, i.e., ∃u ∈H, sp (Lu −u) = 0."
T LOG,0.422782874617737,"Proof. The idea is to apply Brouwer’s fix-point theorem in RS quotiented by the equivalence relation
u ∼v ⇔sp (u −v) = 0, where sp (−) becomes a norm. By linearity (Corollary 21), L is well-defined
in this quotient space, and if L is shown continuous on RS, so will it be on the quotient."
T LOG,0.4235474006116208,"We show that L is sequentially continuous on H. Consider a sequence un ∈HN converging to
u ∈H and fix ϵ > 0. Provided that n > Nϵ for Nϵ large enough, we have ∥un −u∥∞< ϵ, i.e.,
un −ϵe ≤un ≤u + ϵe. Therefore, in the one hand, for all v ≤un, we have v −ϵe ≤u so
max {v ≤un : v ∈H} ≤max {v ≤u : v ∈H} + ϵe; And on the other hand, for all v ≤u, v + ϵe ≤un
so max {v ≤u : v ∈H} ≤max {v ≤un : v ∈H} + ϵe. Hence:"
T LOG,0.4243119266055046,∥max {v ≤u : v ∈H} −max {v ≤un : v ∈H}∥≤ϵ.
T LOG,0.42507645259938837,"It shows that Γ is continuous. The operator Lβ is obviously continuous as well, so L = Γ ◦Lβ is
continuous by composition. Since H = H0 + Re with H0 compact and ocnvex, the quotient H/∼is
compact and convex, and is preserved by L/∼. By Brouwer’s fix-point theorem, L/∼has a fix-point
in H/∼. So L has a span fix-point in H.
□"
T LOG,0.4258409785932722,"We write Fix(L) the span fix-points of L.
Lemma 23. L has well-defined growth. Specifically, if Lu = u + ge, then:"
T LOG,0.42660550458715596,"(1) There exists c > 0, s.t., for all v ∈H0, (ng −c)e + u ≤Lnv ≤(ng + c)e + u;
(2) If u′ ∈Fix(L), then Lu′ −u′ = ge."
T LOG,0.42737003058103973,"Proof. Setting c := maxv∈H0 ∥v −u∥∞< ∞, one can check that u−ce ≤v ≤u+ce for all v ∈H0. this
proves (1) for n = 0 and we then proceed by induction on n ≥0. By induction, Lnv ≤u + (ng + c)e
and by Corollary 21, L is monotone, so we have:"
T LOG,0.42813455657492355,Ln+1v ≤LLnv ≤L(u + (ng + c)e) = u + ((n + 1)g + c)e
T LOG,0.4288990825688073,"where the last inequality use the linearity of L together with Lu = u + ge. The lower bound of Lnv is
shown similarly, establishing (1)."
T LOG,0.42966360856269115,"For (2), pick u′ ∈Fix(L) with Lu′ = u′ + g′e. Up to translating u′, we can assume that u′ ∈H0 and
apply (1). We get:
(ng −c)e + u ≤ng′e + u′ ≤(ng + c)e + u.
Divided by n and let it go to infinity. We conclude that g = g′.
□"
T LOG,0.4304281345565749,"We finally have everything in hand to claim that L solves (18).
Corollary 24. The growth of L given by g = Lu −u for u ∈Fix(L) is well-defined, and:"
T LOG,0.43119266055045874,"∀u ∈H,
ge = lim inf
n→∞
Lnu"
T LOG,0.4319571865443425,"n
= lim sup
n→∞
Lnu n ."
T LOG,0.4327217125382263,"Moreover, g ≥g∗(H, β, M)."
T LOG,0.4334862385321101,"Proof. The growth property is a direct consequence of Lemma 23. We show g ≥g∗(H, β, M) which
is defined in (18). Pick π ∈Π, e
M ∈M its model with ˜h ≡h(π, e
M) and ˜Pπ˜h ≤ˆPπ˜h + βπ where
βπ(s) := β(s, π(s)). Up to translation, we can assume that ˜h ∈H0."
T LOG,0.43425076452599387,"We have g(π, e
M) = ˜ge for ˜g ∈R, so"
T LOG,0.4350152905198777,˜h + ˜ge = ˜rπ + ˜Pπ˜h ≤L˜h
T LOG,0.43577981651376146,"by definition. By monotony of L, see Corollary 21, n˜ge + ˜h ≤Ln˜h follows by induction on n ≥0. By
Lemma 23, we further have Ln˜h ≤n(g + c)e + u where u ∈Fix(L). In tandem,"
T LOG,0.43654434250764523,"˜ge ≤ge + ce + u −˜h n
."
T LOG,0.43730886850152906,"Letting n →∞, we deduce that ˜g ≤g. Conclude by taking the best π and e
M.
□"
T LOG,0.4380733944954128,"The next theorem follows directly with the same proof technique, and guarantees optimism.
Theorem 25. Assume that g∗+ h∗≤Lh∗. Then g ≥g∗."
T LOG,0.43883792048929665,"The condition “g∗+ h∗≤Lh∗” can be referred to as a weak form of optimism. We qualify this version
of optimism as weak because it is much weaker than optimism property suggested by Fruit [2019]
L ≥L where L is the Bellman operator of the true MDP. Here, we only ask for Lh∗≥Lh∗, i.e.,
optimism at a span fix-point of L. This condition is met as soon as M ∈M, h∗∈H and β large
enough."
T LOG,0.4396024464831804,"B.4
Modelization of the projected mitigated Bellman operator L"
T LOG,0.44036697247706424,"The aim of this paragraph is to establish Corollary 27, stating that Lu can be viewed as a policy
produced by Greedy(M, u, β).
Lemma 26 (Modelization). For π ∈Π, denote βπ(s) := β(s, π(s)), Rπ := Q
s R(s, π(s)) and Pπ :=
Q
s P(s, π(s)). Fix u ∈RS and let π := Greedy(M, u, β)."
T LOG,0.441131498470948,"(1) If P is convex, then there exists (˜rπ, ˜Pπ) ∈Rπ × Pπ such that Lβu = ˜rπ + ˜Pπu."
T LOG,0.4418960244648318,"(2) Assume that Lβu = ˜rπ + ˜Pπu. There exists r′
π ≤˜rπ such that Lu = r′
π + ˜Pπu."
T LOG,0.4426605504587156,The convexity requirement of (1) is always true if the kernel confidence region is chosen via (C1-4).
T LOG,0.4434250764525994,"Proof. For (1), fix a state s ∈S, let a := π(s) and ρ := min(sup P(s, a)u, ˆp(s, a)u + β(s, a)). If
ρ = sup P(s, a)u, then there is nothing to say because P is compact, hence the sup is a max and ρ is
of the form ˜p(s, a)u. Otherwise, let ˜p(s, a)u > ˆp(s, a)u + β(s, a) with ˜p(s, a) ∈P(s, a). Introduce, for
λ ∈[0, 1],
˜pλ(s, a) := λ ˜p(s, a) + (1 −λ) ˆp(s, a).
By continuity, there exists λ ∈(0, 1) such that ˜pλ(s, a)u = ˆp(s, a)u + β(s, a) and by convexity of
P(s, a), ˜pλ(s, a) ∈P(s, a). This proves (1)."
T LOG,0.4441896024464832,"For (2), recall that Lu = ΓLβu = Γ(˜rπ + ˜Pπu). Since Γv ≤v, for v ∈RS, we have:"
T LOG,0.44495412844036697,Γ(˜rπ + ˜Pπu) ≤˜rπ + ˜Pπu.
T LOG,0.44571865443425074,"Set r′
π := Γ(˜rπ + ˜Pπu) −˜Pπu. Check that r′
π satisfies r′
π ≤˜rπ and Lu = r′
π + ˜Pπu.
□"
T LOG,0.44648318042813456,The last corollary bellow is crucial to claim that greedy policies are good choices in PMEVI-DT.
T LOG,0.44724770642201833,"Corollary 27 (Greedy modelization). Let u ∈RS and fix π := Greedy(M, u, β). If P is convex, then
with the notations of Lemma 26, there exists ˜rπ ≤sup Rπ and ˜Pπ ∈Pπ such that Lu = ˜rπ + ˜Pπu."
T LOG,0.44801223241590216,"C
Proof of Theorem 5: Regret analysis of PMEVI-DT"
T LOG,0.4487767584097859,"Notations.
At episode k, the played policy is denoted πk. As a greedy response to hk, by Proposi-
tion 2 (3), there exists ˜rk(s) ≤sup Rtk(s, πk(s)) and ˜Pk(s) ∈Ptk(s, π(x)) such that hk + gk = ˜rk + ˜Pkhk.
The reward-kernel pair ˜Mk = (˜rk, ˜Pk) is referred to as the optimistic model of πk.
We write
Pk := Pπk(M) the true kernel and ˆPk := Pπk( ˆMtk) the empirical kernel. Likewise, we define the
reward functions rk and ˆrk. The optimistic gain and bias satisfy gk = g(πk, e
Mk) and hk = h(πk, e
Mk).
We further denote c0 = T
1
5 ."
T LOG,0.44954128440366975,"Important remark.
To slightely simplify the analysis, we assume that PMEVI is run with perfect
precision ϵ = 0, i.e., that hk = PMEVI(Mtk, βtk, Γtk, 0) hence is a span fix-point of Ltk. This assumption
is mild and can be dropped by adding an extra error term that has to be carried out in the calculations."
T LOG,0.4503058103975535,"C.1
Number of episodes under doubling trick (DT)"
T LOG,0.4510703363914373,"Lemma 28 (Number of episodes, Auer et al. [2009]). The number of episodes up to time T ≥S A is
upper-bounded by:
K(T) ≤S A log2
 8T"
T LOG,0.4518348623853211,"S A

."
T LOG,0.4525993883792049,"C.2
Sum of bias variances"
T LOG,0.4533639143730887,"The Lemma 29 below shows that PT−1
t=0 V(p(Xt), h∗) scales as Tsp (h∗)sp (r) + sp (h∗) Reg(T) in
probability.
Lemma 29. With probability at least 1 −δ, we have:
XT−1"
T LOG,0.4541284403669725,"t=0 V(p(Xt), h∗) ≤2sp (h∗)sp (r)T + sp (h∗)2 q"
T LOG,0.45489296636085624,"1
2T log
 1"
T LOG,0.45565749235474007,"δ

+ 2sp (h∗)
XT−1"
T LOG,0.45642201834862384,t=0 ∆∗(Xt) + sp (h∗)2.
T LOG,0.45718654434250766,"Proof. Using the Bellman equation h∗(s) + g∗(s) = r(s, a) + p(s, a)h∗+ ∆∗(s, a), we have:"
T LOG,0.45795107033639143,"V(p(Xt), h∗) =  p(Xt) −eS t
 h∗2 + 2h∗(S t)(∆∗(Xt) + r(Xt) −g∗(S t))."
T LOG,0.45871559633027525,"Since sp

h∗2
≤sp (h∗)2, we get:
XT−1"
T LOG,0.459480122324159,"t=0 V(p(Xt), h∗) ≤
XT−1"
T LOG,0.4602446483180428,"t=0
 p(Xt) −eS t
 h∗2 + 2sp (h∗)

sp (r)T +
XT−1"
T LOG,0.4610091743119266,"t=0 ∆∗(Xt)
"
T LOG,0.4617737003058104,"=
XT−1"
T LOG,0.4625382262996942,"t=0
 p(Xt) −eS t+1
 h∗2 + 2sp (h∗)

1
2sp (h∗)sp (r)T +
XT−1"
T LOG,0.463302752293578,"t=0 ∆∗(Xt)
"
T LOG,0.46406727828746175,(Lemma 32) ≤2sp (h∗)sp (r)T + sp (h∗)2 q
T LOG,0.4648318042813456,"1
2T log
 1"
T LOG,0.46559633027522934,"δ

+ 2sp (h∗)
XT−1"
T LOG,0.46636085626911317,t=0 ∆∗(Xt) + sp (h∗)2
T LOG,0.46712538226299694,"where the last inequality holds with probability 1 −δ. This concludes the proof.
□"
T LOG,0.46788990825688076,"C.3
Regret and pseudo-regret: A tight relation"
T LOG,0.46865443425076453,"In this paragraph, we bound the regret with respect to the pseudo-regret (and conversely) up to a
factor of order (sp (h∗)sp (r) log( T"
T LOG,0.4694189602446483,"δ ))1/2. Hence, in proofs, the pseudo-regret can be changed to the
regret with ease.
Lemma 30. With probability 1 −4δ, the regret and the pseudo-regret and linked as follows: T−1
X"
T LOG,0.4701834862385321,"t=0
(g∗−Rt) − T−1
X"
T LOG,0.4709480122324159,"t=0
∆∗(Xt) ≤"
T LOG,0.4717125382262997,
T LOG,0.4724770642201835,"2
q
2sp (h∗)sp (r) + 1"
T LOG,0.47324159021406725,"8

T log
 T"
T LOG,0.4740061162079511,"δ

+
q"
T LOG,0.47477064220183485,"2sp (h∗) log
 T"
T LOG,0.47553516819571867,"δ
 PT−1
t=0 ∆∗(Xt)"
T LOG,0.47629969418960244,"+sp (h∗)
 1"
T,0.47706422018348627,"2T
 1"
LOG,0.47782874617737003,"4 log
3
4  T"
LOG,0.4785932721712538,"δ

+ 4sp (h∗) log
 T"
LOG,0.4793577981651376,"δ

+ 2sp (h∗)"
LOG,0.4801223241590214,.
LOG,0.4808868501529052,"Proof. We rely again on the Poisson equation g∗(S t) −r(Xt) −∆∗(Xt) = (p(Xt) −eS t)h∗, so:"
LOG,0.481651376146789,"A :=

XT−1"
LOG,0.48241590214067276,"t=0 (g∗−Rt −∆∗(Xt))
 ≤

XT−1"
LOG,0.4831804281345566,"t=0
 p(Xt) −eS t
 h∗
 +

XT−1"
LOG,0.48394495412844035,t=0 (Rt −r(Xt))
LOG,0.4847094801223242,"≤sp (h∗) +

XT−1"
LOG,0.48547400611620795,"t=0
 p(Xt) −eS t+1
 h∗
 +

XT−1"
LOG,0.48623853211009177,"t=0 (Rt −r(Xt))
 ."
LOG,0.48700305810397554,"Up to the constant sp (h∗), the two error terms are respectively a navigation and a reward error. The
second is bounded using Azuma’s inequality (Lemma 32), showing that with probability 1 −2δ, we
have:

XT−1"
LOG,0.4877675840978593,"t=0 (Rt −r(Xt))
 ≤
q"
LOG,0.48853211009174313,"1
2T log
 1 δ

."
LOG,0.4892966360856269,"We continue by using Freedman’s inequality, instantiated in the form of Lemma 33. With probability
1 −δ, we have:

XT−1"
LOG,0.4900611620795107,"t=0
 p(Xt) −eS t+1
 h∗
 ≤ r"
LOG,0.4908256880733945,"2
XT−1"
LOG,0.49159021406727826,"t=0 V(p(Xt), h∗) log
 T"
LOG,0.4923547400611621,"δ

+ 4sp (h∗) log
 T δ

."
LOG,0.49311926605504586,"The quantity PT−1
t=0 V(p(Xt), h∗) is a classical one that appears at several places throughout the analysis.
Using Lemma 29, we bount it explicitely. Further simplifying the bound with
√"
LOG,0.4938837920489297,"a + b ≤√a +
√"
LOG,0.49464831804281345,"b,
we get that with probability 1 −4δ, we have: A ≤"
LOG,0.4954128440366973, q
LOG,0.49617737003058104,"2sp (h∗)sp (r)T log
 T"
LOG,0.4969418960244648,"δ

+
q"
LOG,0.49770642201834864,"1
2T log
 1"
LOG,0.4984709480122324,"δ

+
q"
LOG,0.49923547400611623,"2sp (h∗) log
 T"
LOG,0.5,"δ
 PT−1
t=0 ∆∗(Xt)"
LOG,0.5007645259938838,"+sp (h∗)
 1"
T,0.5015290519877675,"2T
 1"
LOG,0.5022935779816514,"4 log
3
4  T"
LOG,0.5030581039755352,"δ

+ 4sp (h∗) log
 T"
LOG,0.503822629969419,"δ

+ 2sp (h∗)"
LOG,0.5045871559633027,.
LOG,0.5053516819571865,Bound log( 1
LOG,0.5061162079510704,δ) by log( T
LOG,0.5068807339449541,"δ ) and use √a +
√"
LOG,0.5076452599388379,"b ≤2
√"
LOG,0.5084097859327217,"a + b to merge the terms in
q"
LOG,0.5091743119266054,T log( T
LOG,0.5099388379204893,"δ ) under a
single square-root.
□"
LOG,0.5107033639143731,"Overall, Lemma 30 states that the regret PT−1
t=0 (g∗−Rt) and the pseudo-regret PT−1
t=0 ∆∗(Xt) differ by
about (sp (h∗)T log( T"
LOG,0.5114678899082569,"δ ))1/2 in probability (up to asymptotically negligible additional terms). In general,
the precise form of Lemma 30 is not convenient to use because it is of form form x ≤y + α √y + β
that is not linear in y. Corollary 31 factorizes the result into one which will be more convenient in
proofs."
LOG,0.5122324159021406,"Corollary 31. Denote x := PT−1
t=0 (g∗−Rt) and y := PT−1
t=0 ∆∗(Xt). Further introduce:"
LOG,0.5129969418960245,"α :=
q"
LOG,0.5137614678899083,"2sp (h∗) log
 T δ
"
LOG,0.514525993883792,"β := 2
q
2sp (h∗)sp (r) + 1"
LOG,0.5152905198776758,"2

T log
 T"
LOG,0.5160550458715596,"δ

+ sp (h∗)
 1"
T,0.5168195718654435,"2T
 1"
LOG,0.5175840978593272,"4 log
3
4  T"
LOG,0.518348623853211,"δ

+ 2sp (h∗)

2 log
 T"
LOG,0.5191131498470948,"δ

+ 1

."
LOG,0.5198776758409785,"Then, with probability 1 −4δ, we have √x ≤√y + 1"
LOG,0.5206422018348624,2α + √β and √y ≤√x + α + √β.
LOG,0.5214067278287462,"Proof. This is straight forward algebra from the result of Lemma 30.
□"
LOG,0.52217125382263,"C.4
Proof of Lemma 6, reward optimism"
LOG,0.5229357798165137,We start by getting rid of the reward noise. We have:
LOG,0.5237003058103975,"Reg(T) :=
XT−1"
LOG,0.5244648318042814,"t=0 (g∗−Rt) =
XT−1"
LOG,0.5252293577981652,"t=0 (g∗−r(Xt)) +
XT−1"
LOG,0.5259938837920489,t=0 (r(Xt) −Rt)
LOG,0.5267584097859327,"≤
XT−1"
LOG,0.5275229357798165,"t=0 (g∗−r(Xt)) +
q"
LOG,0.5282874617737003,"1
2T log
 1 δ
"
LOG,0.5290519877675841,"with probability 1 −δ by Azuma’s inequality (Lemma 32). We are left with PT−1
t=0 (g∗−r(Xt)). We
continue by splitting the regret episodically and invoking optimism. By Lemma 13, with probability
1 −4δ, we have PT−1
t=0 (g∗−r(Xt)) ≤P
k
Ptk+1−1
t=tk
(gk −r(Xt)). Introduce"
LOG,0.5298165137614679,"B0(T) :=
X k"
LOG,0.5305810397553516,"tk+1−1
X"
LOG,0.5313455657492355,"t=tk
(gk −r(Xt)).
(20)"
LOG,0.5321100917431193,"We focus on bounding B0(T).
By Assumption 2,
˜rk(s, a) is of the form ˆrk(s, a) +
p"
LOG,0.5328746177370031,"C log(2S AT/δ)/Ntk(s, a) −ηk(s, a) with ηk(s, a) ∈R. By the statement (3) of Proposition 2,
ηk(s, a) ≥0. Therefore,"
LOG,0.5336391437308868,"B0(T) =
X k"
LOG,0.5344036697247706,"tk+1−1
X"
LOG,0.5351681957186545,"t=tk
(gk −˜rk(Xt)) +
X k"
LOG,0.5359327217125383,"tk+1−1
X"
LOG,0.536697247706422,"t=tk
( ˜rk(Xt) −r(Xt)) ≤
X k"
LOG,0.5374617737003058,"tk+1−1
X"
LOG,0.5382262996941896,"t=tk
(gk −˜rk(Xt)) + S A +
X k"
LOG,0.5389908256880734,"tk+1−1
X"
LOG,0.5397553516819572,"t=tk
1  Ntk(Xt) ≥1
ˆrk(Xt) −r(Xt) + v
t"
LOG,0.540519877675841,"C log
 2S AT δ
"
LOG,0.5412844036697247,Ntk(Xt)
LOG,0.5420489296636085,
LOG,0.5428134556574924,"(∗)
≤
X k"
LOG,0.5435779816513762,"tk+1−1
X"
LOG,0.5443425076452599,"t=tk
(gk −˜rk(Xt)) + S A +
X k"
LOG,0.5451070336391437,"tk+1−1
X"
LOG,0.5458715596330275,"t=tk
1  Ntk(Xt) ≥1
 v
t"
LOG,0.5466360856269113,"2 log
 2S AT δ
"
LOG,0.5474006116207951,"Ntk(s, a)
+ v
t"
LOG,0.5481651376146789,"C log
 2S AT δ
"
LOG,0.5489296636085627,"Ntk(s, a)"
LOG,0.5496941896024465,
LOG,0.5504587155963303,"where (∗) holds with probability 1 −δ following Lemma 35. By the doubling trick rule (DT), we
have Nt(Xt) ≤2Ntk(Xt) for t < tk+1, so, with probability 1 −δ,"
LOG,0.5512232415902141,"B0(T) ≤
X k"
LOG,0.5519877675840978,"tk+1−1
X"
LOG,0.5527522935779816,"t=tk
(gk −˜rk(Xt)) + S A + 2
X k"
LOG,0.5535168195718655,"tk+1−1
X"
LOG,0.5542813455657493,"t=tk
1  Ntk(Xt) ≥1
v
t"
LOG,0.555045871559633,"(2 + C) log
 2S AT δ
"
LOG,0.5558103975535168,"Ntk(s, a) ≤
X k"
LOG,0.5565749235474006,"tk+1−1
X"
LOG,0.5573394495412844,"t=tk
(gk −˜rk(Xt)) + S A + 2
q"
LOG,0.5581039755351682,"(2 + C) log
 2S AT δ
 X s,a"
LOG,0.558868501529052,"NT (s,a)−1
X n=1 q"
N,0.5596330275229358,"1
n ≤
X k"
N,0.5603975535168195,"tk+1−1
X"
N,0.5611620795107034,"t=tk
(gk −˜rk(Xt)) + S A + 4
q"
N,0.5619266055045872,"(2 + C) log
 2S AT δ
 X s,a p"
N,0.5626911314984709,"NT(s, a)"
N,0.5634556574923547,"(Jensen) ≤
X k"
N,0.5642201834862385,"tk+1−1
X"
N,0.5649847094801224,"t=tk
(gk −˜rk(Xt)) + S A + 4
q"
N,0.5657492354740061,"(2 + C)S AT log
 2S AT δ

."
N,0.5665137614678899,"We conclude that with probability 1 −6δ, we have:"
N,0.5672782874617737,"Reg(T) ≤
X k"
N,0.5680428134556575,"tk+1−1
X"
N,0.5688073394495413,"t=tk
(gk −˜rk(Xt)) + 4
q"
N,0.5695718654434251,"(2 + C)S AT log
 2S AT"
N,0.5703363914373089,"δ

+
q"
N,0.5711009174311926,"1
2T log
 2S AT"
N,0.5718654434250765,"δ

+ S A.
(21)"
N,0.5726299694189603,"This concludes the proof.
□"
N,0.573394495412844,"C.5
Proof of Lemma 7, navigation error"
N,0.5741590214067278,We have: X k
N,0.5749235474006116,"tk+1−1
X"
N,0.5756880733944955,"t=tk
(pk(S t) −eS t)hk ≤
X k"
N,0.5764525993883792,"tk+1−1
X"
N,0.577217125382263,"t=tk
(pk(S t) −eS t+1)hk +
X"
N,0.5779816513761468,"k
sp (hk) ≤
X k"
N,0.5787461773700305,"tk+1−1
X"
N,0.5795107033639144,"t=tk
(pk(S t) −eS t+1)(hk −h∗)"
N,0.5802752293577982,"|                                    {z                                    }
A1 +
X k"
N,0.581039755351682,"tk+1−1
X"
N,0.5818042813455657,"t=tk
(pk(S t) −eS t+1)h∗"
N,0.5825688073394495,"|                           {z                           }
A2 +
X"
N,0.5833333333333334,"k
sp (hk)."
N,0.5840978593272171,"The last term is O(c0S A log(T)) by Lemma 28, hence is O(T 1/5 log(T))."
N,0.5848623853211009,"(STEP 1) We start by bounding A1. By Lemma 13, with probability 1 −4δ, we have h∗∈Htk for all
k ≤K(T). So sp (hk −h∗) ≤sp (hk) + sp (h∗) ≤2c0. By Freedman’s inequality invoked in the form of
Lemma 33, we have with probability 1 −5δ, A1 ≤ v
u
t"
X,0.5856269113149847,"2
X k"
X,0.5863914373088684,"tk+1−1
X"
X,0.5871559633027523,"t=tk
V (p(Xt), hk −h∗) log
 T"
X,0.5879204892966361,"δ

+ 8c0 log
 T δ
"
X,0.5886850152905199,"It suffices to bound the first term. Recall that e is the vector full of ones. We have:
X k"
X,0.5894495412844036,"tk+1−1
X"
X,0.5902140672782875,"t=tk
V(p(Xt), hk −h∗) =
X k"
X,0.5909785932721713,"tk+1−1
X"
X,0.591743119266055,"t=tk
V (p(Xt), hk −h∗−(hk(S t) −h∗(S t)) · e) ≤
X k"
X,0.5925076452599388,"tk+1−1
X t=tk X"
X,0.5932721712538226,"s′∈S
p(s′|Xt)  hk(s′) −h∗(s′) −(hk(S t) −h∗(S t))2"
X,0.5940366972477065,"(∗)
≤3
X k"
X,0.5948012232415902,"tk+1−1
X"
X,0.595565749235474,"t=tk
E"
X,0.5963302752293578,"
X"
X,0.5970948012232415,"s′∈S
p(s′|Xt)  hk(s′) −h∗(s′) −(hk(S t) −h∗(S t))2
Ft"
X,0.5978593272171254,"+ 16c2
0 log
 1 δ
 = 3
X k"
X,0.5986238532110092,"tk+1−1
X"
X,0.599388379204893,"t=tk
(hk(S t+1) −h∗(S t+1) −(hk(S t) −h∗(S t)))2 + 16c2
0 log
 1 δ

."
X,0.6001529051987767,"Here the inequality (∗) holds with probability 1 −δ following Lemma 40. We will bound the
summand with the bias estimation error error(ck, s, s′) that spawns the inner regret estimation B0(tk) =
Pk−1
ℓ=1
Ptℓ+1−1
t=tℓ
(gℓ−Rt). This inner estimation is linked to B(T) := P
k,t(gk −Rt) the overall optimistic
regret by:"
X,0.6009174311926605,"B0(tk) ≤
XK(T) ℓ=1"
X,0.6016819571865444,Xtℓ+1−1
X,0.6024464831804281,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k"
X,0.6032110091743119,Xtℓ+1−1
X,0.6039755351681957,"t=tℓ
(gk −Rt)"
X,0.6047400611620795,"(∗)
≤
XK(T) ℓ=1"
X,0.6055045871559633,Xtℓ+1−1
X,0.6062691131498471,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k"
X,0.6070336391437309,Xtℓ+1−1
X,0.6077981651376146,"t=tℓ
(g∗−Rt)"
X,0.6085626911314985,"≤
XK(T) ℓ=1"
X,0.6093272171253823,Xtℓ+1−1
X,0.6100917431192661,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k XT−1"
X,0.6108562691131498,"t=tk
 ∆∗(Xt) +  p(Xt) −eS t
 h∗+ r(Xt) −Rt
"
X,0.6116207951070336,"≤
XK(T) ℓ=1"
X,0.6123853211009175,Xtℓ+1−1
X,0.6131498470948012,"t=tℓ
(gk −Rt) + sp (h∗) −
XK(T) ℓ=k XT−1"
X,0.613914373088685,"t=tk
  p(Xt) −eS t+1
 h∗+ r(Xt) −Rt
"
X,0.6146788990825688,"(†)
≤
XK(T) ℓ=1"
X,0.6154434250764526,Xtℓ+1−1
X,0.6162079510703364,"t=tℓ
(gk −Rt) + sp (h∗) + (1 + sp (h∗))
q"
X,0.6169724770642202,"1
2T log
 1 δ
"
X,0.617737003058104,"=: B(T) + sp (h∗) + (1 + sp (h∗))
q"
X,0.6185015290519877,"1
2T log
 1 δ

."
X,0.6192660550458715,"In the above, (∗) holds with probability 1 −4δ uniformly on k following Lemma 13 and (†) holds,
also uniformly on k, with probability 1 −δ by applying Azuma-Hoeffding’s inequality (Lemma 32).
Continuing, still on the event specified by Lemma 13, we have with probability 1 −6δ: X k"
X,0.6200305810397554,"tk+1−1
X"
X,0.6207951070336392,"t=tk
V(p(Xt), hk −h∗) ≤3
X k"
X,0.6215596330275229,"tk+1−1
X t=tk"
X,0.6223241590214067,"3c0 + (1 + c0)
q"
TK LOG,0.6230886850152905,"8tk log
 2"
TK LOG,0.6238532110091743,"δ

+ 2B0(tk)"
TK LOG,0.6246177370030581,"Ntk(S t+1 ↔S t)
+ 16c2
0 log
 1 δ
 ≤3
X k"
TK LOG,0.6253822629969419,"tk+1−1
X t=tk"
TK LOG,0.6261467889908257,"4c0 + (1 + c0)
q"
T LOG,0.6269113149847095,"32T log
 2"
T LOG,0.6276758409785933,"δ

+ 2B(T)"
T LOG,0.6284403669724771,"Ntk(S t, At, S t+1)
+ 16c2
0 log
 1 δ
"
T LOG,0.6292048929663608,"(DT) ≤12c2
0S 2A + 3

4c0 + (1 + c0)
q"
T LOG,0.6299694189602446,"32T log
 2"
T LOG,0.6307339449541285,"δ

+ 2B(T)

S 2A log(T)"
T LOG,0.6314984709480123,"+ 16c2
0 log
 1 δ

."
T LOG,0.632262996941896,"(STEP 2) For A2, by Freedman’s inequality invoked in the form of Lemma 33 again, we have with
probability 1 −δ, A2 ≤ v
u
t"
X,0.6330275229357798,"2
X k"
X,0.6337920489296636,"tk+1−1
X"
X,0.6345565749235474,"t=tk
V(pk(S t), h∗) log
 T"
X,0.6353211009174312,"δ

+ 8c0 log
 T δ
 ≤ v
u
t 2 T−1
X"
X,0.636085626911315,"t=0
V(p(Xt), h∗) log
 T"
X,0.6368501529051988,"δ

+ 8c0 log
 T δ

."
X,0.6376146788990825,"We recognize the sum of variance PT−1
t=0 V(p(Xt), h∗) that we leave as is."
X,0.6383792048929664,"(STEP 3) As a result, with probability 1 −7δ, we have: X k"
X,0.6391437308868502,"tk+1−1
X"
X,0.6399082568807339,"t=tk
(pk(S t) −eS t)hk ≤ v
u
t 2 T−1
X"
X,0.6406727828746177,"t=0
V(p(Xt), h∗) log
 T"
X,0.6414373088685015,"δ

+ 2S A
1
2 p"
X,0.6422018348623854,"3B(T) log
 T"
X,0.6429663608562691,"δ

+ O

S A
1
2 T
7
20 log
3
4  T δ
"
X,0.6437308868501529,"when c0 = T
1
5 .
□"
X,0.6444954128440367,"C.6
Proof of Lemma 8, empirical bias error"
X,0.6452599388379205,"Because h∗is a fixed vector, Bennett’s inequality (see Lemma 39) guarantees that ( ˆpk(S t) −pk(S t)h∗
is small as follows. By doing a union bound over Lemma 39 with confidence
δ
S AT over all pairs (s, a)
and visits counts N(s, a) ≤T, we see that with probability 1 −δ, for all k, we have:"
X,0.6460244648318043,"tk+1−1
X"
X,0.6467889908256881,"t=tk
( ˆpk(S t) −pk(S t)) h∗≤sp (h∗)S A +"
X,0.6475535168195719,"tk+1−1
X"
X,0.6483180428134556,"t=tk
1  Ntk(Xt) ≥1
 r"
X,0.6490825688073395,"2V(p(Xt),h∗) log( S AT"
X,0.6498470948012233,"δ )
Ntk (Xt)
+
sp(h∗) log
 S AT δ "
X,0.650611620795107,3Ntk (Xt)
X,0.6513761467889908,
X,0.6521406727828746,(by doubling trick) ≤sp (h∗)S A + 2
X,0.6529051987767585,"tk+1−1
X"
X,0.6536697247706422,"t=tk
1 (Nt(Xt) ≥1)"
X,0.654434250764526, r
X,0.6551987767584098,"2V(p(Xt),h∗) log( S AT"
X,0.6559633027522935,"δ )
Nt(Xt)
+
sp(h∗) log
 S AT δ "
X,0.6567278287461774,3Nt(Xt)
X,0.6574923547400612,.
X,0.658256880733945,"Summing this over k and factorizing over state-action pairs, we get that with probability 1 −δ, X"
X,0.6590214067278287,"k
(2k) ≤sp (h∗)S A + 2
X s,a"
X,0.6597859327217125,
X,0.6605504587155964,"NT (s,a)
X n=1 q"
X,0.6613149847094801,"2V(p(s,a),h∗) log( S AT"
X,0.6620795107033639,"δ )
n
+"
X,0.6628440366972477,"NT (s,a)
X n=1"
X,0.6636085626911316,"sp(h∗) log
 S AT δ  n"
X,0.6643730886850153,
X,0.6651376146788991,"≤sp (h∗)S A + 4
X s,a q"
X,0.6659021406727829,"NT(s, a)V(p(s, a), h∗) log
 S AT"
X,0.6666666666666666,"δ

+ 2sp (h∗)S A log
 S AT"
X,0.6674311926605505,"δ

log(T)"
X,0.6681957186544343,"(Jensen) ≤sp (h∗)S A + 4
r S A
X"
X,0.668960244648318,"s,a V(p(s, a), h∗) log
 S AT"
X,0.6697247706422018,"δ

+ 2sp (h∗)S A log
 S AT"
X,0.6704892966360856,"δ

log(T)"
X,0.6712538226299695,= sp (h∗)S A + 4 rXT−1
X,0.6720183486238532,"t=0 V(p(Xt), h∗) log
 S AT"
X,0.672782874617737,"δ

+ 2sp (h∗)S A log
 S AT"
X,0.6735474006116208,"δ

log(T)"
X,0.6743119266055045,"We recognize the sum of variances PT−1
t=0 V(p(Xt), h∗), that is left to be upper-bounded later on.
□"
X,0.6750764525993884,"C.7
Proof of Lemma 9, optimistic overshoot"
X,0.6758409785932722,"Because of the β-mitigation generated by Algorithm 5, the quantity ( ˜pk(S t) −ˆpk(S t))hk is shown to
be directly related to V(p(Xt), h∗) up to a provably negligible error. Denote h′
k the reference point
BiasProjection(Htk, ctk(−, s0)) used in Algorithm 5 (denoted h0 in the algorithm). By Lemma 13,
with probability 1 −4δ, we have h∗∈Htk for all k. To lighten up notations, we write dtk(s′, s) instead
of error(ctk, s′, s)."
X,0.676605504587156,"(STEP 1) Denote A := ( ˜pk(S t) −ˆpk(S t))hk. By construction of ˜pk, we have A ≤βtk(Xt), so:"
X,0.6773700305810397,"A ≤βtk(Xt) =: v
t"
X,0.6781345565749235,"2

V( ˆpk(S t), h′
k) + 8c0
P
s′∈S ˆpk(s′|S t)dtk(s′, S t) log
 S AT δ
"
X,0.6788990825688074,"Ntk(Xt)
+
3c0 log
 S AT δ
"
X,0.6796636085626911,Ntk(Xt) ≤ s
X,0.6804281345565749,"2V( ˆpk(S t), h′
k)
Ntk(Xt)
|                {z                }
A1 + v
t"
X,0.6811926605504587,"16c0
P
s′∈S ˆpk(s′|S t)dtk(s′, S t) log
 S AT δ
"
X,0.6819571865443425,"Ntk(Xt)
|                                                  {z                                                  }
A2"
X,0.6827217125382263,"+
3c0 log
 S AT δ
"
X,0.6834862385321101,"Ntk(Xt)
."
X,0.6842507645259939,"The rightmost term of A is of order O(log2(T)) hence is negligible. We focus on the other two. The
analysis of A1 will spawn a term similar to A2, hence we start by the second. Recall that dtk is the bias
error provided by Algorithm 3 and that the inner regret estimation is B0(tk) = Pk−1
ℓ=1
Ptℓ+1−1
t=tℓ
(gℓ−Rt).
Now, remark that:"
X,0.6850152905198776,"B0(tk) ≤
XK(T) ℓ=1"
X,0.6857798165137615,Xtℓ+1−1
X,0.6865443425076453,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k"
X,0.6873088685015291,Xtℓ+1−1
X,0.6880733944954128,"t=tℓ
(gk −Rt)"
X,0.6888379204892966,"(∗)
≤
XK(T) ℓ=1"
X,0.6896024464831805,Xtℓ+1−1
X,0.6903669724770642,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k"
X,0.691131498470948,Xtℓ+1−1
X,0.6918960244648318,"t=tℓ
(g∗−Rt)"
X,0.6926605504587156,"≤
XK(T) ℓ=1"
X,0.6934250764525994,Xtℓ+1−1
X,0.6941896024464832,"t=tℓ
(gk −Rt) −
XK(T) ℓ=k XT−1"
X,0.694954128440367,"t=tk
 ∆∗(Xt) +  p(Xt) −eS t
 h∗+ r(Xt) −Rt
"
X,0.6957186544342507,"≤
XK(T) ℓ=1"
X,0.6964831804281345,Xtℓ+1−1
X,0.6972477064220184,"t=tℓ
(gk −Rt) + sp (h∗) −
XK(T) ℓ=k XT−1"
X,0.6980122324159022,"t=tk
  p(Xt) −eS t+1
 h∗+ r(Xt) −Rt
"
X,0.6987767584097859,"(†)
≤
XK(T) ℓ=1"
X,0.6995412844036697,Xtℓ+1−1
X,0.7003058103975535,"t=tℓ
(gk −Rt) + sp (h∗) + (1 + sp (h∗))
q"
X,0.7010703363914373,"1
2T log
 1 δ
"
X,0.7018348623853211,"=: B(T) + sp (h∗) + (1 + sp (h∗))
q"
X,0.7025993883792049,"1
2T log
 1 δ

."
X,0.7033639143730887,"In the above, (∗) holds with probability 1 −4δ uniformly on k following Lemma 13 and (†) holds,
also uniformly on k, with probability 1 −δ by applying Azuma-Hoeffding’s inequality (Lemma 32).
Therefore, with probability 1 −5δ, for all k and t ∈{tk, . . . , tk+1 −1}, we have:
v
t"
X,0.7041284403669725,"16c0
P
s′∈S ˆpk(s′|S t)dtk(s′, S t) log
 S AT δ
"
X,0.7048929663608563,"Ntk(Xt)
≤ q"
X,0.7056574923547401,"16c0 log
 S AT"
X,0.7064220183486238,"δ
 P
s′∈S Ntk(S t, At, s′)dtk(s′, S t)"
X,0.7071865443425076,Ntk(Xt) ≤ q
X,0.7079510703363915,"16c0 log
 S AT"
X,0.7087155963302753,"δ
 P
s′∈S Ntk(S t ↔s′)dtk(s′, S t)"
X,0.709480122324159,Ntk(Xt) ≤ s
X,0.7102446483180428,"16c0 log
 S AT δ"
X,0.7110091743119266,"
S
 
3c0+(1+c0)
 
1+
r"
T LOG,0.7117737003058104,"8T log
 2 δ"
T LOG,0.7125382262996942,"!
+2B0(tk)
!"
T LOG,0.713302752293578,Ntk (Xt) ≤ s
T LOG,0.7140672782874617,"16c0 log
 S AT δ"
T LOG,0.7148318042813455,"
S
 
(1+c0)
 
3+2
r"
T LOG,0.7155963302752294,"8T log
 2 δ"
T LOG,0.7163608562691132,"!
+2B(T)
!"
T LOG,0.7171253822629969,Ntk (Xt) ≤ s
T LOG,0.7178899082568807,"16c0 log
 S AT δ"
T LOG,0.7186544342507645,"
S
 
(1+c0)
 
3+2
r"
T LOG,0.7194189602446484,"8T log
 2 δ"
T LOG,0.7201834862385321,"
+2B(T)
!!"
T LOG,0.7209480122324159,"Ntk (Xt)
."
T LOG,0.7217125382262997,"This bound will be enough. We move on to A1. We have:
q"
T LOG,0.7224770642201835,"V( ˆpk(S t), h′
k) ≤
qV( ˆpk(S t), h′
k) −V(p(Xt), h∗)
 +
p"
T LOG,0.7232415902140673,"V(p(Xt), h∗)"
T LOG,0.7240061162079511,"≤
qV( ˆpk(S t), h′
k) −V( ˆpk(Xt), h∗)

p"
T LOG,0.7247706422018348,"|V( ˆpk(S t), h∗) −V(p(Xt), h∗)| +
p"
T LOG,0.7255351681957186,"V(p(Xt), h∗)"
T LOG,0.7262996941896025,"(∗)
≤
q 8c0
X"
T LOG,0.7270642201834863,"s′∈S ˆpk(s′|S t)dk(s′, S t) + sp (h∗)
p"
T LOG,0.72782874617737,"∥ˆpk(S t) −pk(S t)∥1 +
p"
T LOG,0.7285932721712538,"V(p(Xt), h∗)"
T LOG,0.7293577981651376,"(†)
≤
q 8c0
X"
T LOG,0.7301223241590215,"s′∈S ˆpk(s′|S t)dk(s′, S t) + sp (h∗)"
T LOG,0.7308868501529052,"
S log
 S AT δ
"
T LOG,0.731651376146789,Ntk(Xt)
T LOG,0.7324159021406728,
T LOG,0.7331804281345565,"1
4
+
p"
T LOG,0.7339449541284404,"V(p(Xt), h∗)"
T LOG,0.7347094801223242,"≤
A2
p"
T LOG,0.735474006116208,"2Ntk(Xt)
+ sp (h∗)"
T LOG,0.7362385321100917,"
S log
 S AT δ
"
T LOG,0.7370030581039755,Ntk(Xt)
T LOG,0.7377675840978594,
T LOG,0.7385321100917431,"1
4
+
p"
T LOG,0.7392966360856269,"V(p(Xt), h∗)"
T LOG,0.7400611620795107,"where (∗) is obtained by applying Lemma 12 and (†) holds with probability 1 −δ by applying
Weissman’s inequality, see Lemma 35. All together, with probability 1 −6δ, A is upper-bounded by: A ≤ v
t"
T LOG,0.7408256880733946,"2V(p(Xt), h∗) log
 S AT δ
"
T LOG,0.7415902140672783,"Ntk(Xt)
+ 2A2 + sp (h∗)"
T LOG,0.7423547400611621,"v
u
u
u
t2 log
 S AT δ
 q"
T LOG,0.7431192660550459,S log S AT δ
T LOG,0.7438837920489296,"Ntk(Xt)
p"
T LOG,0.7446483180428135,"Ntk(Xt)
+
3c0 log
 S AT δ
"
T LOG,0.7454128440366973,"Ntk(Xt)
|                                                             {z                                                             }
A3(k,t) ."
T LOG,0.746177370030581,(STEP 2) The number of visits Nk(Xt) is lower-bounded by 1
T LOG,0.7469418960244648,"2Nt(Xt) when Nk(Xt) ≥1 by doubling
trick (DT). By summing over t and k, we find that with probability 1 −6δ, X"
T LOG,0.7477064220183486,"k
(3k) ≤S Ac0 +
X k"
T LOG,0.7484709480122325,"tk+1−1
X"
T LOG,0.7492354740061162,"t=tk
1Ntk (Xt)≥1 v
t"
T LOG,0.75,"2V(p(Xt), h∗) log
 S AT δ
"
T LOG,0.7507645259938838,"Ntk(Xt)
+
X k"
T LOG,0.7515290519877675,"tk+1−1
X"
T LOG,0.7522935779816514,"t=tk
1Ntk (Xt)≥1(2A2(k, t) + A3(k, t))"
T LOG,0.7530581039755352,"(DT) ≤S Ac0 + 2
X k"
T LOG,0.753822629969419,"tk+1−1
X"
T LOG,0.7545871559633027,"t=tk
1Ntk (Xt)≥1 s"
T LOG,0.7553516819571865,"2V(p(Xt), h∗) log
 S AT δ
"
T LOG,0.7561162079510704,"Nt(Xt)
+
X k"
T LOG,0.7568807339449541,"tk+1−1
X"
T LOG,0.7576452599388379,"t=tk
1Ntk (Xt)≥1(2A2(k, t) + A3(k, t))"
T LOG,0.7584097859327217,≤S Ac0 + 4 r
S A,0.7591743119266054,"2S A
XT−1"
S A,0.7599388379204893,"t=0 V(p(Xt), h∗) log
 S AT"
S A,0.7607033639143731,"δ

+
X k"
S A,0.7614678899082569,"tk+1−1
X"
S A,0.7622324159021406,"t=tk
1Ntk (Xt)≥1(2A2(k, t) + A3(k, t))"
S A,0.7629969418960245,"where the last inequality is obtained with computations that are similar to those detailed in the proof
of Lemma 8. We recognize the variance that we will leave as is. We finish the proof by bounding the
lower order terms A2 and A3."
S A,0.7637614678899083,(STEP 3) We start with A2. We have: X k
S A,0.764525993883792,"tk+1−1
X"
S A,0.7652905198776758,"t=tk
1Ntk (Xt)≥1A2(k, t) :=
X k"
S A,0.7660550458715596,"tk+1−1
X"
S A,0.7668195718654435,"t=tk
1Ntk (Xt)≥1 s"
S A,0.7675840978593272,"16c0 log
 S AT δ"
S A,0.768348623853211,"
S
 
(1+c0)
 
3+2
r"
T LOG,0.7691131498470948,"8T log
 2 δ"
T LOG,0.7698776758409785,"
+2B(T)
!!"
T LOG,0.7706422018348624,Ntk (Xt)
T LOG,0.7714067278287462,(DT) ≤2 r
T LOG,0.77217125382263,"16c0S log
 S AT"
T LOG,0.7729357798165137,"δ
 
(1 + c0)

3 + 2
q"
T LOG,0.7737003058103975,"8T log
 2"
T LOG,0.7744648318042814,"δ

+ 2B(T)

S A log(T)"
T LOG,0.7752293577981652,"≤8(1 + c0)S
3
2 A log
3
2  S AT"
T LOG,0.7759938837920489,"δ
 
2 + 4T
1
4 log
1
4  S AT"
T LOG,0.7767584097859327,"δ

+
p"
T LOG,0.7775229357798165,"2B(T)

."
T LOG,0.7782874617737003,(STEP 4) We are left with A3. We have: X k
T LOG,0.7790519877675841,"tk+1−1
X"
T LOG,0.7798165137614679,"t=tk
1Ntk (Xt)≥1A3(k, t) :=
X k"
T LOG,0.7805810397553516,"tk+1−1
X"
T LOG,0.7813455657492355,"t=tk
1Ntk (Xt)≥1"
T LOG,0.7821100917431193,"
sp (h∗)"
T LOG,0.7828746177370031,"v
u
u
u
t2 log
 S AT δ
 q"
T LOG,0.7836391437308868,S log S AT δ
T LOG,0.7844036697247706,"Ntk(Xt)
p"
T LOG,0.7851681957186545,"Ntk(Xt)
+
3c0 log
 S AT δ
"
T LOG,0.7859327217125383,Ntk(Xt)
T LOG,0.786697247706422,
T LOG,0.7874617737003058,"(DT) ≤
X k"
T LOG,0.7882262996941896,"tk+1−1
X"
T LOG,0.7889908256880734,"t=tk
1Ntk (Xt)≥1"
T LOG,0.7897553516819572,"
sp (h∗)"
T LOG,0.790519877675841,"v
u
u
u
t2 log
 S AT δ
 q"
T LOG,0.7912844036697247,S log S AT δ
T LOG,0.7920489296636085,"Ntk(Xt)
p"
T LOG,0.7928134556574924,"Ntk(Xt)
+
3c0 log
 S AT δ
"
T LOG,0.7935779816513762,Ntk(Xt)
T LOG,0.7943425076452599,
T LOG,0.7951070336391437,"≤Csp (h∗)S
5
4 AT
1
4 log
3
4  S AT"
T LOG,0.7958715596330275,"δ

+ 6c0S A log
 S AT δ
"
T LOG,0.7966360856269113,"= O

sp (h∗)S
5
4 AT
1
4 log
 S AT"
T LOG,0.7974006116207951,"δ

."
T LOG,0.7981651376146789,"This concludes the proof.
□"
T LOG,0.7989296636085627,"C.8
Proof of Lemma 10, second order error"
T LOG,0.7996941896024465,"Recall that by Lemma 13, with probability 1 −4δ, h∗∈Htk for all k, hence sp (hk −h∗) ≤2c0 for all
k on the same event. Therefore, with probability 1 −4δ, X"
T LOG,0.8004587155963303,"k
(4k) := 2c0S A +
X k"
T LOG,0.8012232415902141,"tk+1−1
X"
T LOG,0.8019877675840978,"t=tk
1Ntk (Xt)≥1 ( ˆpk(S t) −pk(S t)) (hk −h∗)"
T LOG,0.8027522935779816,"= 2c0S A +
X k"
T LOG,0.8035168195718655,"tk+1−1
X t=tk X"
T LOG,0.8042813455657493,"s′∈S
1Ntk (Xt)≥1( ˆpk(s′|S t) −pk(s′|S t))(hk −h∗(s′))"
T LOG,0.805045871559633,"(∗)
≤2c0S A + 2
X k"
T LOG,0.8058103975535168,"tk+1−1
X t=tk X"
T LOG,0.8065749235474006,"s′∈S
1Ntk (Xt)≥1( ˆpk(s′|S t) −pk(s′|S t))dtk(s′, S t)"
T LOG,0.8073394495412844,"(†)
≤2c0S A + 2
X k"
T LOG,0.8081039755351682,"tk+1−1
X t=tk X"
T LOG,0.808868501529052,"s′∈S
1Ntk (Xt)≥1"
T LOG,0.8096330275229358,"dk(s′, S t) v
t"
T LOG,0.8103975535168195,"2 ˆpk(s′|S t) log
 S 2AT δ
"
T LOG,0.8111620795107034,"Ntk(Xt)
+ 3dk(s′|S t)
log
 S 2AT δ
"
T LOG,0.8119266055045872,Ntk(Xt)
T LOG,0.8126911314984709,
T LOG,0.8134556574923547,"≤2c0S A + 2
X k"
T LOG,0.8142201834862385,"tk+1−1
X t=tk X"
T LOG,0.8149847094801224,"s′∈S
1Ntk (Xt)≥1"
T LOG,0.8157492354740061,"
√c0 v
t"
T LOG,0.8165137614678899,"2 ˆpk(s′|S t)dk(s′, S t) log
 S 2AT δ
"
T LOG,0.8172782874617737,"Ntk(Xt)
+
3c0 log
 S 2AT δ
"
T LOG,0.8180428134556575,Ntk(Xt)
T LOG,0.8188073394495413,
T LOG,0.8195718654434251,"≤2c0S A + 4
X k"
T LOG,0.8203363914373089,"tk+1−1
X t=tk X"
T LOG,0.8211009174311926,"s′∈S
1Ntk (Xt)≥1"
T LOG,0.8218654434250765,"
√c0 s"
T LOG,0.8226299694189603,"2 ˆpk(s′|S t)dk(s′, S t) log
 S 2AT δ
"
T LOG,0.823394495412844,"Nt(Xt)
+
3c0 log
 S 2AT δ
"
T LOG,0.8241590214067278,Nt(Xt)
T LOG,0.8249235474006116,
T LOG,0.8256880733944955,"where (∗) uses that h∗∈Htk, and (†) is obtained by applying the empirical Bernstein’s inequality, see
Lemma 36, to ˆpk(s′|S t) −pk(s′|S t), and holds with probability 1 −δ. The rightmost term’s sum is"
T LOG,0.8264525993883792,upper-bounded by:
X,0.827217125382263,"4
X k"
X,0.8279816513761468,"tk+1−1
X t=tk X s′∈S"
X,0.8287461773700305,"3c0 log
 S 2AT δ
"
X,0.8295107033639144,"Nt(Xt)
≤12S 2A log(T) log
 S 2AT δ

."
X,0.8302752293577982,"For the other term, follow the line of the proof of Lemma 9 (term A2). We have with probability
1 −5δ (4δ of which is by invoking Lemma 13):"
X,0.831039755351682,"ˆpk(s′|S t)dk(s′, S t) =
Ntk(S t, At, s′)

(1 + c0)

1 +
q"
TK LOG,0.8318042813455657,"8tk log
 2"
TK LOG,0.8325688073394495,"δ

+ 2B0(tk)
"
TK LOG,0.8333333333333334,Ntk(S t ↔s′)Ntk(Xt) ≤
TK LOG,0.8340978593272171,"
(1 + c0)

3 + 2
q"
T LOG,0.8348623853211009,"8T log
 2"
T LOG,0.8356269113149847,"δ

+ 2B(T)
"
T LOG,0.8363914373088684,"Ntk(Xt)
."
T LOG,0.8371559633027523,"Therefore, √c0 s"
T LOG,0.8379204892966361,"2 ˆpk(s′|S t)dtk(s′, S t) log
 S 2AT δ
"
T LOG,0.8386850152905199,"Nt(Xt)
≤
4(1 + c0)"
T LOG,0.8394495412844036,"r
3 + 2
q"
T LOG,0.8402140672782875,"8T log
 2"
T LOG,0.8409785932721713,"δ

+ 2B(T)

log
 S 2AT δ
"
T LOG,0.841743119266055,"Nt(Xt)
."
T LOG,0.8425076452599388,"Summing over k, t, s′, with probability 1 −6δ, we have: X"
T LOG,0.8432721712538226,"k
(4k) ≤"
T LOG,0.8440366972477065,"
16S 2A(1 + c0) log
1
2  S 2AT"
T LOG,0.8448012232415902,"δ
  √2B(T) + 2

8T log
 2"
T LOG,0.845565749235474,"δ
 1 4 "
T LOG,0.8463302752293578,"+32S 2A

log(T) log
 S 2AT"
T LOG,0.8470948012232415,"δ

+ (1 + c0) log
1
2  S 2AT δ
"
T LOG,0.8478593272171254,
T LOG,0.8486238532110092,"This concludes the proof.
□"
T LOG,0.849388379204893,"D
More details on experiments"
T LOG,0.8501529051987767,"D.1
River swim as a hard communicating environment"
T LOG,0.8509174311926605,"Experiments of Fig. 2 are run on n-states river-swim. Such MDPs are, despite their size, known to
be hard to learn. They consists in n states aligned in a straight line with two playable actions right
and left whose dynamics are given in the figure below. Rewards are Bernoulli and null everywhere
excepted for r(sn, right) = 0.95 and r(s0, left) = 0.05."
T LOG,0.8516819571865444,"s1
s2
· · ·
· · ·
sn 0.6 0.4 0.6 0.35"
T LOG,0.8524464831804281,"0.05
0.05 0.35 0.95 0.05 1 1
1
1"
T LOG,0.8532110091743119,"Figure 3:
The kernel of a n-state river-swim."
T LOG,0.8539755351681957,"3-state river-swim.
The gain is g∗≈0.82 and h∗≈(−4.28, −2.24, 0.4)."
T LOG,0.8547400611620795,"5-state river-swim.
The gain is g∗≈0.82 and h∗≈(−9.62, −7.58, −4.96, −2.27, 0.45)."
T LOG,0.8555045871559633,"D.2
Experiments in weakly-communicating environments"
T LOG,0.8562691131498471,"Beyond communicating models, PMEVI-DT is superior to EVI-based methods. On Fig. 4, we see that
PMEVI-DT can learn in environments of infinite diameter while UCRL2 cannot. The gain is due to the
truncation operation, that makes sure that the optimistic bias vector has span less than T 1/5."
T LOG,0.8570336391437309,"s0
s1
1 0.5 0.1 UCRL2 PMEVI"
T LOG,0.8577981651376146,"Figure 4:
Bernoulli bandit with dandling state (weakly-communicating model). Each arrow is a
choice of action whose label is the mean reward of the associated state-action pair. The learner starts
in state s0 and transitions to the absorbing state s1 as soon as an action is played."
T LOG,0.8585626911314985,"Without the truncation operation, EVI-based methods can never reject the plausibility that the reward
at s0 is maximal (equal to one) and that there is a positive probability ϵ to switch from s1 to s0 when
taking the sub-optimal action from s1. More precisely, denote the actions a, b where r(s0, a) = 1,
r(s1, a) = 0.5 and r(s1, b) = 0.1, so that a is the optimal action and b is sub-optimal. There are two
policies πa and πb choosing action a and b from s1 respectively. Because s0 is only visited once, the
best reward that can be achieved from s0 is ˜rt(s0, a) = 1 at all times. If one runs UCRL2, the confidence
region for transition kernels is roughly of the form Pt(x) = { ˜p(x) : Nt(x) ∥˜p(x) −ˆpt(x)∥2
1 ≤Cp log(t)}
and the plausible transition kernel ˜p(x) ∈Pt(x) that goes the quickest from s1 to s0 is of the form:"
T LOG,0.8593272171253823,"˜pt(s0|s1, a) = s"
T LOG,0.8600917431192661,Cp log(t)
T LOG,0.8608562691131498,"Nt(s1, a)
and
˜pt(s0|s1, b) = s"
T LOG,0.8616207951070336,Cp log(t)
T LOG,0.8623853211009175,"Nt(s1, b) ."
T LOG,0.8631498470948012,"After running EVI for i steps, the current vector vi (see (6)) is V∗
i (Mt), the maximal amount of
reward that one can collect in i steps on Mt seen as an extended Markov decision process, see
Auer et al. [2009]. If the data is well concentrated, the optimal reward from s1 are respectively
˜rt(s1, a) ≈0.5 +
p"
T LOG,0.863914373088685,"Cr log(t)/Nt(s1, a) and ˜rt(s1, b) = 0.1 +
p"
T LOG,0.8646788990825688,"Cr log(t)/Nt(s1, b). From all this, one can
argue that when i is large enough, the optimistic scores of πa and πb over i steps are roughly equal to:"
T LOG,0.8654434250764526,"Vπa
i (s1; Mt) ≈i +"
T LOG,0.8662079510703364,−0.5 + s
T LOG,0.8669724770642202,Cr log(t)
T LOG,0.867737003058104,"Nt(s1, a)"
T LOG,0.8685015290519877, s
T LOG,0.8692660550458715,"Nt(s1, a)
Cp log(t) = i −0.5 s"
T LOG,0.8700305810397554,"Nt(s1, a)
Cp log(t) + s Cr
Cp"
T LOG,0.8707951070336392,"Vπb
i (s1; Mt) ≈i +"
T LOG,0.8715596330275229,−0.9 + s
T LOG,0.8723241590214067,Cr log(t)
T LOG,0.8730886850152905,"Nt(s1, b)"
T LOG,0.8738532110091743, s
T LOG,0.8746177370030581,"Nt(s1, b)
Cp log(t) = i −0.9 s"
T LOG,0.8753822629969419,"Nt(s1, b)
Cp log(t) + s"
T LOG,0.8761467889908257,"Cr
Cp
."
T LOG,0.8769113149847095,"So Vπa
i (s1; Mt) ≤Vπb
i (s1; Mt) if Nt(s, b) ≪25"
T LOG,0.8776758409785933,"81Nt(s, a)."
T LOG,0.8784403669724771,"This means that EVI will output πb if Nt(s, b) ≪25"
T LOG,0.8792048929663608,"81Nt(s, a), leading to Nt(s, a) ≍Nt(s, b) so both
growing linearly with t. This informal argument can be generalized to EVI-based algorithms with
other types of confidence regions: EVI-based methods such as UCRL Auer et al. [2002], UCRL2B Fruit
et al. [2020] and KLUCRL Filippi et al. [2010] will suffer from Nt(s, b) = Θ(t) and their regret will
grow linearly."
T LOG,0.8799694189602446,"In opposition, PMEVI-based methods use truncation, making sure that |vi(s1) −vi(s0)| ≤T 1/5 at
all times. Intuitively, it makes PMEVI-based method “think” that ˜pt(s0|s1, b) cannot be as small as
pCp log(t)/Nt(s1, b), because the optimistic bias of πb would be too large otherwise; Or, equivalently,
that ˜pt(s0|s1, b) = pCp log(t)/Nt(s1, b) but with ˜rt(s0, a) ≪1, hence killing the optimistic reward at
(s0, a) to meet the bias constraints."
T LOG,0.8807339449541285,"Overall, these features of PMEVI-based methods are shared with algorithms such as REGAL Bartlett
and Tewari [2009] and SCAL Fruit et al. [2018]. The difference is that these methods require precise
prior information on sp(h∗) that PMEVI-DT does not need."
T LOG,0.8814984709480123,"E
Standard concentration inequalities"
T LOG,0.882262996941896,"Lemma 32 (Azuma’s inequality, Azuma [1967]). Let (Ut)t≥0 a martingale difference sequence such
that sp (Ut) ≤c a.s., i.e., there exists at ∈R such that at ≤Ut ≤at + c a.s. Then, for all δ > 0,"
T LOG,0.8830275229357798,"P
XT−1"
T LOG,0.8837920489296636,"t=0 Ut ≥c
q"
T LOG,0.8845565749235474,"1
2T log
 1"
T LOG,0.8853211009174312,"δ

≤δ."
T LOG,0.886085626911315,"Lemma 33 (Freedman’s inequality, Zhang et al. [2020]). Let (Ut)t≥0 a martingale difference sequence
such that |Ut| ≤c a.s., and denote its conditional variance Vt := E[U2
t |Ft−1]. Then, for all δ > 0, P"
T LOG,0.8868501529051988,"∃T ′ ≤T :
XT ′−1"
T LOG,0.8876146788990825,t=0 Ut ≥ r
T LOG,0.8883792048929664,"2
XT ′−1"
T LOG,0.8891437308868502,"t=0 Vt log
 T"
T LOG,0.8899082568807339,"δ

+ 4c log
 T"
T LOG,0.8906727828746177,"δ
≤δ."
T LOG,0.8914373088685015,"Lemma 34 (Time-uniform Azuma, Bourel et al. [2020]). Let (Ut) a martingale difference sequence
such that, for all λ ∈R, E[exp(λUt)|U1, . . . , Ut−1] ≤exp( λ2σ2"
T LOG,0.8922018348623854,2 ). Then:
T LOG,0.8929663608562691,"∀δ > 0,
P

∃n ≥1,
Xn"
T LOG,0.8937308868501529,"k=1 Uk
2 ≥nσ2 
1 + 1"
T LOG,0.8944954128440367,"n

log
√ 1+n δ"
T LOG,0.8952599388379205,"
≤δ."
T LOG,0.8960244648318043,"Lemma 35 (Time-uniform Weissman). Let q a distribution over {1, . . . , d}. Let (Ut) a sequence of
i.i.d. random variables of distribution q. Then:"
T LOG,0.8967889908256881,"∀δ > 0,
P

∃n ≥1,

Xn"
T LOG,0.8975535168195719,"i=1
 eUi −q
2"
T LOG,0.8983180428134556,"1 ≥nd log

2
√ 1+n
δ"
T LOG,0.8990825688073395,"
≤δ."
T LOG,0.8998470948012233,"Proof. Remark that
Pn
k=1(eUk −q)
1 = maxv∈{−1,1}d Pn
k=1

eUk −q, v. Let Wv
k := 
eUk −q, v. Re-
mark that for each v ∈{−1, 1}d, (Wv
k) is a family of i.i.d. random variables with −⟨q, v⟩≤Wv
k ≤
1 −⟨q, v⟩, so E[exp(λWv
k)] ≤exp( λ2"
T LOG,0.900611620795107,"8 ) by Hoeffding’s Lemma. By Lemma 34, we have: P"
T LOG,0.9013761467889908,"∃n ≥1,  n
X"
T LOG,0.9021406727828746,"k=1
(eUk −q) 1
≥ r"
T LOG,0.9029051987767585,"nd log

2
√ 1+n
δ"
T LOG,0.9036697247706422,= P
T LOG,0.904434250764526,"∃v ∈{−1, 1}d , ∃n, n
X"
T LOG,0.9051987767584098,"k=1
Wv
k ≥ r"
T LOG,0.9059633027522935,"nd log

2
√ 1+n
δ"
T LOG,0.9067278287461774," ≤
X"
T LOG,0.9074923547400612,"v∈{−1,1}d
P"
T LOG,0.908256880733945,"∃n, n
X"
T LOG,0.9090214067278287,"k=1
Wv
k ≥ r"
T LOG,0.9097859327217125,"nd log

2
√ 1+n
δ"
T LOG,0.9105504587155964," ≤
X"
T LOG,0.9113149847094801,"v∈{−1,1}d
P"
T LOG,0.9120795107033639,"∃n, n
X"
T LOG,0.9128440366972477,"k=1
Wv
k ≥ r"
T LOG,0.9136085626911316,"1
2n

1 + 1"
T LOG,0.9143730886850153,"n

log
√"
T LOG,0.9151376146788991,"1+n
2−dδ"
T LOG,0.9159021406727829,
T LOG,0.9166666666666666,≤2d · 2dδ = δ.
T LOG,0.9174311926605505,"This concludes the proof.
□"
T LOG,0.9181957186544343,"Lemma 36 (Time-uniform Empirical Bernstein). Let (Uk)k≥1 a martingale difference sequence such
that sp (Un) ≤c a.s., let ˆUn :=
1
n
Pn
k=1 Uk the empirical mean and ˆVn :=
1
n
Pn
k=1(Uk −ˆUn)2 the
population variance. Then,"
T LOG,0.918960244648318,"∀δ > 0, ∀T > 0,
P

∃t ≤T,
Xt"
T LOG,0.9197247706422018,"i=1 Ui ≥
q"
T LOG,0.9204892966360856,"2t ˆVt log
 3T"
T LOG,0.9212538226299695,"δ

+ 3c log
 3T"
T LOG,0.9220183486238532,"δ

≤δ."
T LOG,0.922782874617737,"Proof. This is obtained with a union bound on the values of n ≤T, then applying Lemma 38.
□"
T LOG,0.9235474006116208,"Lemma 37 (Time-uniform Empirical Likelihoods, Jonsson et al. [2020]). Let q a distribution on
{1, . . . , d}. Let (Ut) a sequence of i.i.d. random variables of distribution q. Then:"
T LOG,0.9243119266055045,"∀δ > 0,
P

∃n ≥1, n KL(ˆqn||q) > log
 1"
T LOG,0.9250764525993884,"δ

+ (d −1) log

e

1 +
n
d−1

≤δ."
T LOG,0.9258409785932722,"Lemma 38 (Empirical Bernstein inequality, Audibert et al. [2009]). Let (Uk)k≥1 a martingale
difference sequence such that sp (Un) ≤c a.s., let ˆUn := 1"
T LOG,0.926605504587156,"n
Pn
k=1 Uk the empirical mean and ˆVn :=
1
n
Pn
k=1(Uk −ˆUn)2 the population variance. Then,"
T LOG,0.9273700305810397,"∀δ > 0, ∀n ≥1,
P
Xn"
T LOG,0.9281345565749235,"k=1 Uk ≥
q"
T LOG,0.9288990825688074,"2n ˆVn log
 3"
T LOG,0.9296636085626911,"δ

+ 3c log
 3"
T LOG,0.9304281345565749,"δ

≤δ."
T LOG,0.9311926605504587,"Lemma 39 (Bennett’s inequality, Audibert et al. [2009]). Let (Ut)t≥0 a martingale difference sequence
such that |Ut| ≤c a.s., and denote its conditional variance Vt := E[U2
t |Ft−1]. Then,"
T LOG,0.9319571865443425,"∀δ > 0, ∀n ≥1,
P
∃k ≤n,
Xk"
T LOG,0.9327217125382263,i=1 Ui ≥ r
XN,0.9334862385321101,"2
Xn"
XN,0.9342507645259939,"i=1 Vi log
 1"
XN,0.9350152905198776,"δ

+ 1"
C LOG,0.9357798165137615,"3c log
 1"
C LOG,0.9365443425076453,"δ
≤δ."
C LOG,0.9373088685015291,"Lemma 40 (Lemma 3 of Zhang and Xie [2023]). Let (Ut) be a sequence of random variables such
that 0 ≤Ut ≤c a.s., and let Ft := σ(U0, U1, . . . , Ut−1). Then:"
C LOG,0.9380733944954128,"∀δ > 0,
P

∃T ≥0,
XT−1"
C LOG,0.9388379204892966,"t=0 Ut ≥3
XT−1"
C LOG,0.9396024464831805,"t=0 E[Ut|Ft−1] + c log
 1"
C LOG,0.9403669724770642,"δ

≤δ;"
C LOG,0.941131498470948,"∀δ > 0,
P

∃T ≥0,
XT−1"
C LOG,0.9418960244648318,"t=0 E[Ut|Ft−1] ≥3
XT−1"
C LOG,0.9426605504587156,"t=0 Ut + c log
 1"
C LOG,0.9434250764525994,"δ

≤δ."
C LOG,0.9441896024464832,NeurIPS Paper Checklist
CLAIMS,0.944954128440367,1. Claims
CLAIMS,0.9457186544342507,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9464831804281345,Answer: [Yes]
CLAIMS,0.9472477064220184,"Justification: We claim that the algorithm reaches minimax optimal regret, and it does.
We claim that our algorithm can effectively use bias information, and the experimental
illustration supports the claim."
CLAIMS,0.9480122324159022,Guidelines:
CLAIMS,0.9487767584097859,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9495412844036697,2. Limitations
LIMITATIONS,0.9503058103975535,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9510703363914373,Answer: [NA]
LIMITATIONS,0.9518348623853211,"Justification: The paper is a theoretical one that covers all weakly communicating MDPs,
and although it may help the design of algorithms intended to be applicable in real-life
scenarios, our method is not meant to be directly applied."
LIMITATIONS,0.9525993883792049,Guidelines:
LIMITATIONS,0.9533639143730887,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9541284403669725,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9548929663608563,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: To every result is attached a proof or a reference.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9556574923547401,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9564220183486238,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The algorithmic content in the main body is detailed enough to implement
the algorithm, the exception being the confidence intervals and the implementation of EVI
(because those come from already existing works). The confidence intervals are discussed
in a dedicated section in the Appendix, while the works of Auer et al. [2009], Filippi et al.
[2010], Fruit et al. [2020] etc. provide detailed ready-to-implement pseudo-code for EVI.
Our code is mostly written in Python. Experiments took a few dozen minutes on a low end
laptop.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9571865443425076,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
THEORY ASSUMPTIONS AND PROOFS,0.9579510703363915,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9587155963302753,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.959480122324159,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9602446483180428,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9610091743119266,"Justification: The code is provided in the supplementary material, together with the scripts
to reproduce the exact figures of the paper."
OPEN ACCESS TO DATA AND CODE,0.9617737003058104,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9625382262996942,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.963302752293578,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9640672782874617,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9648318042813455,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9655963302752294,"Justification: People that are unfamiliar with Markov decision processes may not know
the river-swim setting in which experiments are driven, but it is not required to understand
the figures and the discussion. Moreover, a description of the environment (river-swim)
is provided in the appendix in addition to be referenced. See also Experimental Result
Reproducibility for more."
OPEN ACCESS TO DATA AND CODE,0.9663608562691132,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9671253822629969,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9678899082568807,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9686544342507645,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9694189602446484,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9701834862385321,"Justification: We did a hundred runs for every algorithm, which is enough to get statistical
significance for the small sized problems that we experiment on. Also, experiments are
mostly illustrative and do not account for an extensive numerical validation of the theoretical
results."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9709480122324159,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9717125382262997,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9724770642201835,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9732415902140673,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9740061162079511,Answer: [No]
EXPERIMENTS COMPUTE RESOURCES,0.9747706422018348,"Justification: This is mostly a theoretical paper. Most of the computational cost of the work
is processing LATEX and rendering a PDF. Like said earlier, experiments all together took
less than a hour on a low end laptop."
EXPERIMENTS COMPUTE RESOURCES,0.9755351681957186,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9762996941896025,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9770642201834863,9. Code Of Ethics
CODE OF ETHICS,0.97782874617737,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9785932721712538,Answer: [Yes]
CODE OF ETHICS,0.9793577981651376,Justification: -
CODE OF ETHICS,0.9801223241590215,Guidelines:
CODE OF ETHICS,0.9808868501529052,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.981651376146789,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9824159021406728,10. Broader Impacts
BROADER IMPACTS,0.9831804281345565,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9839449541284404,Answer: [NA]
BROADER IMPACTS,0.9847094801223242,Justification: It’s mainly a theoretical work on small sized Markov decision processes.
BROADER IMPACTS,0.985474006116208,Guidelines:
BROADER IMPACTS,0.9862385321100917,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9870030581039755,11. Safeguards
SAFEGUARDS,0.9877675840978594,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9885321100917431,Answer: [NA]
SAFEGUARDS,0.9892966360856269,Justification: -
SAFEGUARDS,0.9900611620795107,Guidelines:
SAFEGUARDS,0.9908256880733946,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9915902140672783,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9923547400611621,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9931192660550459,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9938837920489296,"Justification: -
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9946483180428135,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9954128440366973,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: -
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.996177370030581,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9969418960244648,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: -
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9977064220183486,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
LICENSES FOR EXISTING ASSETS,0.9984709480122325,"Answer: [NA]
Justification: -
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9992354740061162,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
