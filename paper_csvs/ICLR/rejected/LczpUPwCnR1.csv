Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010845986984815619,"Bilevel optimization (BO) has arisen as a powerful tool for solving many modern
machine learning problems. However, due to the nested structure of BO, exist-
ing gradient-based methods require second-order derivative approximations via
Jacobian- or/and Hessian-vector computations, which can be very costly in prac-
tice, especially with large neural network models. In this work, we propose a novel
BO algorithm, which adopts Evolution Strategies (ES) based method to approx-
imate the response Jacobian matrix in the hypergradient of BO, and hence fully
eliminates all second-order computations. We call our algorithm as ESJ (which
stands for the ES-based Jacobian method) and further extend it to the stochas-
tic setting as ESJ-S. Theoretically, we characterize the convergence guarantee and
computational complexity for our algorithms. Experimentally, we demonstrate the
superiority of our proposed algorithms compared to the state of the art methods
on various bilevel problems. Particularly, in our experiment in the few-shot meta-
learning problem, we meta-learn the twelve millions parameters of a ResNet-12
network over the miniImageNet dataset, which evidently demonstrates the scala-
bility of our ES-based bilevel approach and its feasibility in the large-scale setting."
INTRODUCTION,0.0021691973969631237,"1
INTRODUCTION"
INTRODUCTION,0.0032537960954446853,"Bilevel optimization has recently arisen as a powerful tool to capture various modern machine learn-
ing problems, including meta-learning (Bertinetto et al., 2018; Franceschi et al., 2018; Rajeswaran
et al., 2019; Ji et al., 2020a; Liu et al., 2021a), hyperparamater optimization (Franceschi et al., 2018;
Shaban et al., 2019), neural architecture search (Liu et al., 2018a; Zhang et al., 2021), etc. Bilevel
optimization generally takes the following mathematical form:"
INTRODUCTION,0.004338394793926247,"min
x∈Rp Φ(x) := f(x, y∗(x))
s.t.
y∗(x) = arg min
y∈Rd
g(x, y),
(1)"
INTRODUCTION,0.005422993492407809,"where the outer and inner objectives f : Rp × Rd →R and g : Rp × Rd →R are both continuously
differentiable with respect to (w.r.t.) the inner and outer variables x ∈Rp and y ∈Rd. In many
machine learning problems, the loss functions f and g take a ﬁnite-sum form over a set of given data
Dn,m = {ξi, ζj, i = 1, ..., n, j = 1, ..., m} as follows:"
INTRODUCTION,0.006507592190889371,"f(x, y) = 1"
INTRODUCTION,0.007592190889370932,"n
Pn
i=1 F(x, y; ξi),
g(x, y) = 1"
INTRODUCTION,0.008676789587852495,"m
Pm
i=1 G(x, y; ζi)
(2)"
INTRODUCTION,0.009761388286334056,"where the sample sizes n and m are typically very large. In this work, we consider a popular setting
where the total objective function Φ(x) in eq. (1) is possibly nonconvex w.r.t. x and the inner function
g is strongly convex w.r.t. y."
INTRODUCTION,0.010845986984815618,"Gradient-based methods have served as a popular tool for solving bilevel optimization problems.
Two types of approaches have been widely used: the iterative differentiation (ITD) method (Domke,
2012; Maclaurin et al., 2015; Finn et al., 2017; Franceschi et al., 2017; Shaban et al., 2019; Ra-
jeswaran et al., 2019; Liu et al., 2020a) and the approximate iterative differentiation (AID) method
(Domke, 2012; Pedregosa, 2016; Gould et al., 2016; Liao et al., 2018; Lorraine et al., 2020). In
the ITD method, the objective value f(x, ˆy(x)) is ﬁrst computed using an approximation point ˆy(x)
close to y∗(x), and the hypergradient ∇Φ(x) = ∂f(x,y∗(x))"
INTRODUCTION,0.01193058568329718,"∂x
is then approximated by ∂f(x,ˆy(x))"
INTRODUCTION,0.013015184381778741,"∂x
using
either reverse or forward mode automatic differentiation. Alternatively, the AID method leverages"
INTRODUCTION,0.014099783080260303,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015184381778741865,"the Implicit Function Theorem (IFT) to establish an implicit expression for the hypergradient. Due
to the bilevel structure of the problem, these gradient-based ITD and AID approaches typically in-
volve second-order matrix computations. Existing efﬁcient implementations of these methods often
adopt Jacobian- or/and Hessian-vector computations, which can still be very costly in practice for
high-dimensional problems with deep neural networks."
INTRODUCTION,0.016268980477223426,"To overcome the computational challenge of current gradient-based methods, Evolution Strategies
(ES) can be a good candidate to eliminate the second-order computations, which uses function val-
ues for estimating its gradient in blackbox optimization (Nesterov & Spokoiny, 2017). An ES-based
bilevel optimizer has recently been proposed in Gu et al. (2021) for hyperparameter optimization,
which uses the ES method to approximate the hypergradient of Φ(x), treating Φ(x) fully as a black-
box objective. However, the hypergradient in bilevel optimization has a much more involved struc-
ture (see eq. (3)) than the gradient in standard minimization problems, and hence direct use of the
ES method without exploiting the structure of the hypergradient can encounter a large estimation
bias and result in poor performance in practice, especially in deep learning as demonstrated in our
experiments in Section 4. This hence motivates the following intriguing question:"
INTRODUCTION,0.01735357917570499,"Can we design a better hypergradient estimator based on the ES method by leveraging the analytical
structure of the hypergradient, and then use such an estimator to construct more efﬁcient ES-based
bilevel optimizers, which are scalable for high-dimensional problems?"
INTRODUCTION,0.01843817787418655,"Furthermore, since the hypergradient in bilevel optimization is much more challenging to estimate
than the gradient in standard minimization problems, it is unclear whether the ES-based estimator
can yield a provably convergent algorithm for bilevel optimization. So far the ES-based bilevel
optimizer in Gu et al. (2021) did not come with a polynomial-time complexity guarantee. Our
empirical experiments in Section 4 show that such an algorithm fail to converge even with shallow
neural networks. This thus motivates the second question we ask:"
INTRODUCTION,0.019522776572668113,Can the ES method lead to bilevel optimizers with theoretical ﬁnite-time convergence guarantee?
INTRODUCTION,0.020607375271149676,This paper provides the afﬁrmative answers to both questions.
MAIN CONTRIBUTIONS,0.021691973969631236,"1.1
MAIN CONTRIBUTIONS"
MAIN CONTRIBUTIONS,0.0227765726681128,"Novel ES-based Jacobian bilevel optimizer. We propose a novel bilevel optimizer with the ES-
based Jacobian estimation, which we call as ESJ. In contrast to the existing ES-based optimizer in
Gu et al. (2021), which estimates the hypergradient by treating the outer-level objective Φ(x) fully
as a blackbox, ESJ estimates only the respone Jacobian matrix (i.e., gradient of y∗(x) with respect
to x), which is the major computational bottleneck, and then leverages the analytical structure of the
hypergradient to construct a much more accurate estimator. Further, ESJ has only gradient compu-
tations, and is computationally much more efﬁcient than existing AID and ITD bilevel optimizers
that require Hessian- and/or Jacobian-vector product computations. We further propose a stochastic
bilevel optimizer ESJ-S to allow the algorithm scalable over large dataset."
MAIN CONTRIBUTIONS,0.02386117136659436,"Convergence guarantee. Theoretically, we characterize the convergence rate and their computa-
tional complexity for both ESJ and ESJ-S to achieve an ϵ-accurate solution. Technically, in contrast
to the standard analysis of ES-based methods on the smoothed blackbox function values, we de-
velop tools to analyze the ES estimator on the output of the execution of an inner optimizer. Such
an analysis can be of independent interest for ES-based bilevel optimization."
MAIN CONTRIBUTIONS,0.024945770065075923,"Superior performance. Experimentally, our algorithms ESJ and ESJ-S achieve superior perfor-
mance compared to the current state of the art bilevel optimizers. Our approach is not only efﬁcient
and scalable, but also stable and robust across various bilevel problems. In particular, on the few-shot
meta-learning problem, we meta-learn the twelve millions weights of a ResNet-12 network. Previ-
ous applications of other bilevel algorithms to meta-learning were limited only to settings where the
backbone network has only up to few hundreds of thousands of parameters."
RELATED WORK,0.026030368763557483,"1.2
RELATED WORK"
RELATED WORK,0.027114967462039046,"Bilevel optimization. Bilevel optimization has been studied for decades since Bracken & McGill
(1973). A variety of bilevel optimization algorithms were then developed, including constraint-
based approaches (Hansen et al., 1992; Shi et al., 2005; Moore, 2010), approximate implicit differ-"
RELATED WORK,0.028199566160520606,Under review as a conference paper at ICLR 2022
RELATED WORK,0.02928416485900217,"entiation (AID) approaches (Domke, 2012; Pedregosa, 2016; Gould et al., 2016; Liao et al., 2018;
Lorraine et al., 2020) and iterative differentiation (ITD) approaches (Domke, 2012; Maclaurin et al.,
2015; Franceschi et al., 2017; Finn et al., 2017; Shaban et al., 2019; Rajeswaran et al., 2019; Liu
et al., 2020a). These methods often suffer from expensive computations of second-order information
(e.g., Hessian-vector products). Hessian-free algorithms were recently proposed based on interior-
point method (Liu et al., 2021b) and Evolution Strategies (ES) (Gu et al., 2021). This paper proposes
a more efﬁcient ES-based approach via exploiting the benign structure of the hypergradient."
RELATED WORK,0.03036876355748373,"Recently, the convergence rate has been established for gradient-based (Hessian involved) bilevel
algorithms (Grazzi et al., 2020; Ji et al., 2021; Rajeswaran et al., 2019; Ji & Liang, 2021). This
paper provides the convergence analysis for the proposed Hessian-free ES-based approach."
RELATED WORK,0.031453362255965296,"Stochastic bilevel optimization. Ghadimi & Wang (2018); Ji et al. (2021); Hong et al. (2020) pro-
posed stochastic gradient descent (SGD) type bilevel optimization algorithms by employing Neu-
mann series for Hessian-inverse-vector approximation. Recent works (Khanduri et al., 2021a;b;
Chen et al., 2021; Guo et al., 2021; Guo & Yang, 2021; Yang et al., 2021) then leveraged momentum-
based variance reduction to further reduce the computational complexity of existing SGD-type
bilevel optimizers. In this paper, we propose a stochastic ES-based method, which eliminate the
computation of second-order information required by all aforementioned stochastic methods."
RELATED WORK,0.03253796095444685,Please see Appendix B for more works about applications of bilevel optimization and ES methods.
PROPOSED ALGORITHMS,0.033622559652928416,"2
PROPOSED ALGORITHMS
We describe our proposed deterministic and stochastic algorithms for bilevel optimization."
HYPERGRADIENTS,0.03470715835140998,"2.1
HYPERGRADIENTS"
HYPERGRADIENTS,0.03579175704989154,"The key step in popular gradient-based bilevel optimizers is the estimation of the hypergradient (i.e.,
the gradient of the objective with respect to the outer variable x), which takes the following form:"
HYPERGRADIENTS,0.0368763557483731,"∇Φ(x) = ∇xf(x, y∗(x)) + J∗(x)⊤∇yf(x, y∗(x))
(3)"
HYPERGRADIENTS,0.03796095444685466,where the Jacobian matrix J∗(x) = ∂y∗(x)
HYPERGRADIENTS,0.039045553145336226,"∂x
∈Rd×p. Following Lorraine et al. (2020), it can be
seen that ∇Φ(x) contains two components: the direct gradient ∇xf(x, y∗(x)) and the indirect gra-
dient J∗(x)⊤∇yf(x, y∗(x)). The direct component can be efﬁciently computed using the existing
automatic differentiation techniques. The indirect component, however, is computationally much
more complex, because J∗(x) takes the form of J∗(x) = −

∇2
yg (x, y∗(x))
−1 ∇x∇yg (x, y∗(x))
(if ∇2
yg (x, y∗(x)) is invertible), which contains the Hessian inverse and the second-order mixed
derivative. Some approaches mitigate the issue by designing Jacobian-vector and Hessian-vector
products (Pedregosa, 2016; Franceschi et al., 2017; Grazzi et al., 2020) to replace second-order
computations. But these algorithms still have poor scalability to modern large-scale bilevel prob-
lems with high-dimensional variables such as neural network parameters. We next introduce the ES
approach which is at the core of our idea for designing scalable Hessian-free bilevel optimizers."
HYPERGRADIENTS,0.04013015184381779,"2.2
EVOLUTION STRATEGIES (ES) METHOD"
HYPERGRADIENTS,0.04121475054229935,"Evolution Strategies (ES) is a powerful technique to estimate the gradient of a function based on
function values, when it is not feasible (such as in black-box problems) or computationally costly to
evaluate the gradient. The idea of the ES method in Nesterov & Spokoiny (2017) is to approximate
the gradient of a general black-box function h : Rn →R using the following oracle based only on
the function values (i.e., the zeroth-order information)"
HYPERGRADIENTS,0.04229934924078091,b∇h(x; u) = h(x + µu) −h(x)
HYPERGRADIENTS,0.04338394793926247,"µ
u
(4)"
HYPERGRADIENTS,0.044468546637744036,"where u ∈Rn is a Gaussian random vector and µ > 0 is the smoothing parameter. Such an oracle
can be shown to be an unbiased estimator of the gradient of the smoothed function Eu [h(x + µu)]."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.0455531453362256,"2.3
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.046637744034707156,"Drawback of an existing ES-based bilevel optimizer. In Gu et al. (2021), the ES method was
directly applied to estimate the hypergradient of Φ(x), treating Φ(x) as a blackbox objective. Since"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.04772234273318872,Under review as a conference paper at ICLR 2022
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.04880694143167028,"the hypergradient has a complex form as in eq. (3) and can be very sensitive to both inner and outer
functions, such an estimation will likely have a large bias error. As our experiments in Section 4
demonstrate, such a method is not robust and even fails to converge in complex bilevel optimization
problems (such as with neural network parameters)."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.049891540130151846,"Our key idea is to exploit the analytical structure of the hypergradient in eq. (3), where the derivatives
∇xf(x, y∗(x)) and ∇yf(x, y∗(x)) can be computed efﬁciently and accurately, and then use the ES
method only to estimate the Jacobian J∗(x), which is the major term posing computational difﬁculty.
In this way, our estimation of the hypergradient can be much more accurate and reliable."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.0509761388286334,"We propose the following novel ES-based Jacobian estimator, which contains two ingredients: (i)
for a given x, apply an algorithm to solve the inner optimization problem and use the output as an
approximation of y∗(x); for example, the output yN(x) of N gradient descent steps of the inner
problem can serve as an estimate for y∗(x). Then JN(x) = ∂yN(x)"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.052060737527114966,"∂x
serves as an estimate of J∗(x);
and (ii) construct an ES-based Jacobian estimator ˆ
JN (x; u) ∈Rd×p for JN(x) as"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.05314533622559653,"ˆ
JN (x; u) = yN(x + µu) −yN(x)"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.05422993492407809,"µ
u⊤
(5)"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.055314533622559656,"where u ∈Rp is a Gaussian vector with independent and identically distributed (i.i.d.) entries.
Then for any vector v ∈Rd, the Jacobian-vector product can be efﬁciently computed using only
vector-vector dot product ˆ
JN(x; u)⊤v = ⟨δ(x; u), v⟩u, where δ(x; u) = yN(x+µu)−yN(x)"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.05639913232104121,"µ
∈Rd."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.057483731019522775,"ESJ: A novel bilevel optimizer with ES Jacobian oracles. We design a bilevel optimizer (see
Algorithm 1) using the ES-based Jacobian oracle in eq. (5), which we call as the ESJ algorithm. At
each step k of the algorithm, ESJ runs an N-step full GD to approximate yN
k (xk). ESJ then samples
Q Gaussian vectors {uk,j ∈N(0, I), j = 1, ..., Q}, and for each sample uk,j, runs an N-step full GD
to approximate yN
k (xk +µuk,j), and then computes the Jacobian estimator ˆ
JN(x; uk,j) as in eq. (5).
Then the sample average over the Q estimators is used for constructing the following hypergradient
estimator for updating the outer variable x."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.05856832971800434,"b∇Φ(xk) = ∇xf(xk, yN
k ) + 1"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.0596529284164859,"Q
PQ
j=1 ˆ
J T
N (xk; uk,j)∇yf(xk, yN
k )"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06073752711496746,"= ∇xf(xk, yN
k ) + 1"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06182212581344902,"Q
PQ
j=1

δ(xk; uk,j), ∇yf(xk, yN
k )

uk,j.
(6)"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06290672451193059,"Computationally, in contrast to the existing AID and ITD based bilevel optimizers (Pedregosa
(2016), Franceschi et al. (2018), Grazzi et al. (2020)) that contains the complex Hessian- and/or
Jacobian-vector product computations, ESJ has only gradient computations, and hence is computa-
tionally much more efﬁcient as demonstrated in our experiments."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06399132321041215,"ESJ-S: A stochastic bilevel optimizer with ES Jacobian oracles. For the ﬁnite-sum problem with
the objective functions given in eq. (2), we design a stochastic bilevel optimizer (see Algorithm 2
in Appendix A) based on ES Jacobian oracles, which we call as ESJ-S."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.0650759219088937,"Differently from Algorithm 1, which applies GD updates to ﬁnd yN(xk), ESJ-S adopts N stochas-
tic gradient descent (SGD) steps to ﬁnd {Y N
k , Y N
k,1, ..., Y N
k,Q} to the inner problem, each with the
outer variable set to be xk + µuk,j. Note that all SGD runs follow the same batch sampling path
{S0, ..., SN−1}. The Jacobian estimator ˆ
JN(xk; uk,j) can then be computed as in eq. (5). At the
outer level, ESJ-S samples a new batch DF independently from the inner batches {S0, ..., SN−1}
to evaluate the stochastic gradients ∇xF(xk, Y N
k ; DF ) and ∇yF(xk, Y N
k ; DF ). The hypergradient is
then estimated as follows."
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06616052060737528,"b∇Φ(xk) = ∇xF(xk, Y N
k ; DF ) + 1"
ES-BASED JACOBIAN AND ITS ENABLED BILEVEL OPTIMIZERS,0.06724511930585683,"Q
PQ
j=1

δ(xk; uk,j), ∇yF(xk, Y N
k ; DF )

uk,j.
(7)"
CONVERGENCE ANALYSIS,0.06832971800433839,"3
CONVERGENCE ANALYSIS"
TECHNICAL ASSUMPTIONS,0.06941431670281996,"3.1
TECHNICAL ASSUMPTIONS"
TECHNICAL ASSUMPTIONS,0.07049891540130152,"In this work, we focus on the following types of loss functions.
Assumption 1. The inner function g(x, y) is µg-strongly convex with respect to the variable y. For
the ﬁnite-sum case, the same assumption holds for ∇G(x, y; ζ)."
TECHNICAL ASSUMPTIONS,0.07158351409978309,Under review as a conference paper at ICLR 2022
TECHNICAL ASSUMPTIONS,0.07266811279826464,Algorithm 1 Bilevel optimizer via ES Jacobians (ESJ)
TECHNICAL ASSUMPTIONS,0.0737527114967462,"1: Input: lower- and upper-level stepsizes α, β > 0, initializations x0 ∈Rp and y0 ∈Rd, inner and outer
iterations numbers K and N, and number of Gaussian vectors Q.
2: for k = 0, 1, 2, ..., K do
3:
Set y0
k = y0,
y0
k,j = y0, j = 1, ..., Q
4:
for t = 1, 2, ..., N do
5:
Update yt
k = yt−1
k
−α∇yg(xk, yt−1
k
)
6:
end for
7:
for j = 1, ..., Q do
8:
Generate uk,j = N(0, I) ∈Rp"
TECHNICAL ASSUMPTIONS,0.07483731019522777,"9:
for t = 1, 2, ..., N do"
TECHNICAL ASSUMPTIONS,0.07592190889370933,"10:
Update yt
k,j = yt−1
k,j −α∇yg

xk + µuk,j, yt−1
k,j
"
TECHNICAL ASSUMPTIONS,0.0770065075921909,"11:
end for
12:
Compute δj =
yN
k,j−yN
k
µ
13:
end for
14:
Compute b∇Φ(xk) = ∇xf(xk, yN
k ) + 1"
TECHNICAL ASSUMPTIONS,0.07809110629067245,"Q
PQ
j=1

δj, ∇yf(xk, yN
k )

uk,j"
TECHNICAL ASSUMPTIONS,0.07917570498915401,"15:
Update xk+1 = xk −β b∇Φ(xk)
16: end for"
TECHNICAL ASSUMPTIONS,0.08026030368763558,"We take the following assumptions on the inner and outer loss functions g(x, y) and f(x, y), as also
adopted in Ghadimi & Wang (2018); Ji et al. (2021); Yang et al. (2021).
Assumption 2. Let w = (x, y) ∈Rd+p. The gradient ∇g(w) is Lg-Lipschitz continuous, i.e., for
any w1, w2 ∈Rd+p,
∇g(w1) −∇g(w2)
 ≤Lg
w1 −w2
; further, the derivatives ∇2
yg(w) and
∇x∇yg(w) are ρ- and τ-Lipschitz continuous, i.e,
∇2
yg(w1) −∇2
yg(w2)

F ≤ρ
w1 −w2
 and
∇x∇yg(w1) −∇x∇yg(w2)

F ≤τ
w1 −w2
. For the ﬁnite-sum case, the same assumptions
hold for G(w; ζ).
Assumption 3. Let w = (x, y) ∈Rd+p. The objective f(w) and its gradient ∇f(w) are M- and
Lf-Lipschitz continuous, i.e., for any w1, w2 ∈Rd+p,
f(w1) −f(w2)
 ≤M
w1 −w2
,
∇f(w1) −∇f(w2)
 ≤Lf
w1 −w2
."
TECHNICAL ASSUMPTIONS,0.08134490238611713,"For the ﬁnite-sum case, the same assumptions hold for F(w; ξ)."
TECHNICAL ASSUMPTIONS,0.0824295010845987,"For the ﬁnite-sum setting, we take the standard bounded-variance assumption on ∇G(w; ζ).
Assumption 4. Gradient ∇G(w; ζ) has a bounded variance, i.e., Eζ∥∇G(w; ζ) −∇g(w)∥2 ≤σ2
for some constant σ ≥0."
CONVERGENCE ANALYSIS FOR ESJ,0.08351409978308026,"3.2
CONVERGENCE ANALYSIS FOR ESJ
Different from the standard zeroth-order analysis for a blackbox function, here we develop new
techniques to analyze the ES-based estimator that depend on the entire inner optimization trajectory,
which is speciﬁc to bilevel optimization. We ﬁrst establish the following essential proposition which
characterizes the Lipshitzness property of the approximate Jacobian matrix JN(x) = ∂yN(x) ∂x
."
CONVERGENCE ANALYSIS FOR ESJ,0.08459869848156182,"Proposition 1. Suppose that Assumptions 1 and 2 hold. Let LJ =

1 + L µg"
CONVERGENCE ANALYSIS FOR ESJ,0.08568329718004339," 
τ
µg + ρL µ2g"
CONVERGENCE ANALYSIS FOR ESJ,0.08676789587852494,"
, with"
CONVERGENCE ANALYSIS FOR ESJ,0.0878524945770065,"L = max{Lf, Lg}. Then, the Jacobian JN(x) is Lipschitz continuous with constant LJ :
JN(x1) −JN(x2)

F ≤LJ
x1 −x2

∀x1, x2 ∈Rp.
(8)"
CONVERGENCE ANALYSIS FOR ESJ,0.08893709327548807,"We next provide an upper-bound on the hypergradient estimation error for ESJ.
Proposition 2. Suppose that Assumptions 1, 2, and 3 hold. Consider the ESJ algorithm. Its hyper-
gradient estimation error can be upper-bounded as:"
CONVERGENCE ANALYSIS FOR ESJ,0.09002169197396963,"E
b∇Φ(xk) −∇Φ(xk)
2 ≤O

(1 −αµg)N + p"
CONVERGENCE ANALYSIS FOR ESJ,0.0911062906724512,Q + µ2dp3 + µ2dp4
CONVERGENCE ANALYSIS FOR ESJ,0.09219088937093275,"Q

(9)"
CONVERGENCE ANALYSIS FOR ESJ,0.09327548806941431,"where E[·] is conditioned on xk and taken over the Gaussian vectors {uk,j : j = 1, ..., Q}."
CONVERGENCE ANALYSIS FOR ESJ,0.09436008676789588,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS FOR ESJ,0.09544468546637744,"By using the smoothness property in Proposition 1 and the upper-bound in Proposition 2, we provide
the following characterization of the convergence rate for ESJ.
Theorem 1 (Convergence of ESJ). Suppose that Assumptions 1, 2, and 3 hold. Choose the inner-
and outer-loop stepsizes respectively as α ≤
1
L and β =
1
4LΦ , where LΦ = L + 2L2+τM2 µg
+"
CONVERGENCE ANALYSIS FOR ESJ,0.09652928416485901,ρLM+L3+τML
CONVERGENCE ANALYSIS FOR ESJ,0.09761388286334056,"µ2g
+ ρL2M"
CONVERGENCE ANALYSIS FOR ESJ,0.09869848156182212,"µ3g
. Then, the iterates xk for k = 0, ..., K −1 of ESJ in Algorithm 1 satisfy:"
K,0.09978308026030369,"1
K
PK−1
k=0 E
∇Φ(xk)
2 ≤16Lφ(Φ(x0)−Φ∗)"
K,0.10086767895878525,"K
+ ∆1
(10)"
K,0.1019522776572668,"with Φ∗= infx Φ(x) and ∆1 = O

(1 −αµg)N + p"
K,0.10303687635574837,Q + µ2dp4
K,0.10412147505422993,"Q
+ µ2dp3
."
K,0.1052060737527115,"Further, choose K, N, µ, and Q at the order of O( 1"
K,0.10629067245119306,"ϵ ), O(log 1"
K,0.10737527114967461,"ϵ ), O(min{ 1 p
q"
K,0.10845986984815618,"ϵ
dp, 1 p
q"
K,0.10954446854663774,"1
dp}) and O( p"
K,0.11062906724511931,ϵ ). Then ESJ achieves an ϵ-accurate stationary point with O( p
K,0.11171366594360087,ϵ2 log 1
K,0.11279826464208242,"ϵ ) computations of the
gradient ∇yg(x, y) and O( 1"
K,0.113882863340564,"ϵ ) computations of the gradients ∇xf(x, y) and ∇yf(x, y)."
K,0.11496746203904555,"Theorem 1 characterizes the sublinear convergence of ESJ with respect to the number K of outer
iterations due to the nonconvexity of the outer objective. The convergence error in ∆1 is due to
the estimation error of the Jacobian matrix J∗via our ES oracle, which captures three types of
errors: (a) the approximation error between JN and J∗via inner-loop gradient descent, which
decreases exponentially w.r.t. the number N of inner iterations due to the strong convexity of the
inner objective; (b) the error between our ES oracle and the Jacobian Jµ of the smoothed output
EuyN(xk + µu), which decreases sublinearly w.r.t. the batch size Q, and (c) the error between the
Jacobians JN and Jµ, which can be controlled by the smoothness parameter µ."
K,0.11605206073752712,"Theorem 1 also indicates that ESJ requires only gradient computations to converge, and eliminates
Hessian- and Jacobian-vector products required by the existing AID and ITD based bilevel optimiz-
ers (Pedregosa (2016), Franceschi et al. (2018), Grazzi et al. (2020)). Thus, ESJ is computationally
much more efﬁcient particularly for high-dimensional problems."
CONVERGENCE ANALYSIS FOR ESJ-S,0.11713665943600868,"3.3
CONVERGENCE ANALYSIS FOR ESJ-S
In this section, we apply the stochastic algorithm ESJ-S to the ﬁnite-sum objective in eq. (2) and an-
alyze its convergence rate. We ﬁrst note that the Lipschitzness property established in Proposition 1
is general and can also be used here. The following proposition establishes an upper bound on the
estimation error of Jacobian J∗by JN = ∂Y N"
CONVERGENCE ANALYSIS FOR ESJ-S,0.11822125813449023,"∂x , where Y N is the output of N inner SGD updates.
Proposition 3. Suppose that Assumptions 1, 2, and 4 hold. Consider the ESJ-S algorithm. Choose
the inner-loop stepsize as α =
2
L+µg , where L = max{Lf, Lg}. Then, we have:"
CONVERGENCE ANALYSIS FOR ESJ-S,0.1193058568329718,"E
JN −J∗
2
F ≤CN
γ
L2"
CONVERGENCE ANALYSIS FOR ESJ-S,0.12039045553145336,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ
."
CONVERGENCE ANALYSIS FOR ESJ-S,0.12147505422993492,"where λ, Γ, and Cγ < 1 are constants (see appendix G for their forms)."
CONVERGENCE ANALYSIS FOR ESJ-S,0.12255965292841649,"We next provide an upper-bound on the estimation error of the hypergradient by b∇Φ(xk) in eq. (7).
Proposition 4. Suppose that Assumptions 1, 2, 3, and 4 hold. Consider the ESJ-S algorithm. Set
the inner-loop stepsize as α =
2
L+µg where L = max{Lf, Lg}. Then, we have:"
CONVERGENCE ANALYSIS FOR ESJ-S,0.12364425162689804,"E
b∇Φ(xk) −∇Φ(xk)
2 ≤O

(1 −αµg)N + 1 S + 1"
CONVERGENCE ANALYSIS FOR ESJ-S,0.12472885032537961,"Df
+ p"
CONVERGENCE ANALYSIS FOR ESJ-S,0.12581344902386118,Q + µ2dp3 + µ2dp4 Q 
CONVERGENCE ANALYSIS FOR ESJ-S,0.12689804772234273,"where S and Df are the sizes of the inner and outer mini-batches, respectively."
CONVERGENCE ANALYSIS FOR ESJ-S,0.1279826464208243,"Based on Propositions 1, 3, and 4, we characterize the convergence rate for ESJ-S.
Theorem 2 (Convergence of ESJ-S). Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner-
and outer-loop stepsizes respectively as α =
2
L+µg and β =
1
4LΦ , where L = max{Lf, Lg} and
the constant LΦ are deﬁned in Theorem 1. Then, the iterates xk, k = 0, ..., K −1 of ESJ-S satisfy:"
K,0.12906724511930587,"1
K
PK−1
k=0 E
∇Φ(xk)
2 ≤16(Φ(x0)−Φ∗)LΦ"
K,0.1301518438177874,"K
+ ∆2,
(11)"
K,0.13123644251626898,Under review as a conference paper at ICLR 2022
K,0.13232104121475055,"where ∆2 = O

(1 −αµg)N + 1"
K,0.1334056399132321,"S +
1
Df + p"
K,0.13449023861171366,Q + µ2dp4
K,0.13557483731019523,"Q
+ µ2dp3
."
K,0.13665943600867678,"For ESJ-S to attain an ϵ-accurate stationary point, it sufﬁces to select K, S, and Df at the order"
K,0.13774403470715835,of O( 1
K,0.13882863340563992,"ϵ ), and N, µ, and Q respectively at the order of O(log 1"
K,0.1399132321041215,"ϵ ), O(min{ 1 p
q"
K,0.14099783080260303,"ϵ
dp, 1 p
q"
K,0.1420824295010846,"1
dp}) and O( p"
K,0.14316702819956617,"ϵ ), which thus amount to O( p"
K,0.1442516268980477,ϵ3 log 1
K,0.14533622559652928,"ϵ ) computations of the gradient ∇yG(x, y, ξ) and O( 1"
K,0.14642082429501085,"ϵ2 )
computations of the gradients ∇xF(x, y, ξ) and ∇yF(x, y, ξ)."
K,0.1475054229934924,"Comparing to the convergence error ∆1 in Theorem 1 for the deterministic algorithm ESJ, Theo-
rem 2 for the stochastic algorithm ESJ-S captures two more sublinearly decreasing error terms 1"
K,0.14859002169197397,"S
and
1
Df respectively due to the sampling of inner and outer batches to estimate the objectives."
K,0.14967462039045554,"To compare between ESJ-S and ESJ on the ﬁnite-sum objective in eq. (2), Theorem 1 suggests that
ESJ attains an ϵ-accurate stationary point of eq. (2) with O( pm"
K,0.15075921908893708,ϵ2 log 1
K,0.15184381778741865,"ϵ ) computations of ∇G and
O( n"
K,0.15292841648590022,"ϵ ) computations of ∇F. As a comparison, Theorem 2 suggests that ESJ-S achieves O( p"
K,0.1540130151843818,ϵ3 log 1
K,0.15509761388286333,"ϵ )
computations of ∇G and O( 1"
K,0.1561822125813449,"ϵ2 ) computations of ∇F, which outperforms ESJ in the large sample
regime where the sample sizes n, m > 1"
K,0.15726681127982647,ϵ . This also reﬂects the advantage of SGD over GD.
EXPERIMENTS,0.15835140997830802,"4
EXPERIMENTS"
EXPERIMENTS,0.1594360086767896,"We validate our algorithms over three bilevel problems: shallow hyper-representation (HR) with
linear/2-layer net embedding model on synthetic data, deep HR with LeNet network (LeCun et al.,
1998) on MNIST dataset, and few-shot meta-learning with ResNet-12 on miniImageNet dataset.
We run all models using a single NVIDIA Tesla P100 GPU. Hyperparamter Optimization (HO)
experiments can be found in Appendix D. All running time are in seconds."
EXPERIMENTS,0.16052060737527116,"(a) d = 128
(b) d = 256
(c) ESJ-N: N inner GD steps"
EXPERIMENTS,0.1616052060737527,"0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Running time 0 100 200 300 400 500"
EXPERIMENTS,0.16268980477223427,Outer loss
EXPERIMENTS,0.16377440347071584,"ESJ
HOZOG
AID-CG
AID-FP
ITD-R"
EXPERIMENTS,0.1648590021691974,"0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Running time 0 200 400 600 800 1000"
EXPERIMENTS,0.16594360086767895,Outer loss
EXPERIMENTS,0.16702819956616052,"ESJ
HOZOG
AID-CG
AID-FP
ITD-R"
EXPERIMENTS,0.1681127982646421,"0.10
0.20
0.25
Running time 0 50 100 150 200"
EXPERIMENTS,0.16919739696312364,Outer loss
EXPERIMENTS,0.1702819956616052,"ESJ-10
ESJ-20
ESJ-50"
EXPERIMENTS,0.17136659436008678,"(d) Outer loss
(e) Hypergradient norm
(f) ESJ-N: N inner GD steps"
EXPERIMENTS,0.17245119305856832,"0
5
10
15
20
Running time 0 5 10 15 20 25 30 35"
EXPERIMENTS,0.1735357917570499,Outer loss
EXPERIMENTS,0.17462039045553146,"ESJ
HOZOG
AID-CG
AID-FP
ITD-R"
EXPERIMENTS,0.175704989154013,"0
5
10
15
20
Running time 0 1 2 3 4 5"
EXPERIMENTS,0.17678958785249457,Hypergradient norm
EXPERIMENTS,0.17787418655097614,"ESJ
HOZOG
AIG-CG
AID-FP
ITD-R"
EXPERIMENTS,0.1789587852494577,"0.5
1.0
1.5
2.0
Running time 0 5 10 15 20 25 30 35"
EXPERIMENTS,0.18004338394793926,Outer loss
EXPERIMENTS,0.18112798264642083,"ESJ-5
ESJ-10
ESJ-20
ESJ-50"
EXPERIMENTS,0.1822125813449024,Figure 1: First row: HR with linear embedding model. Second row: HR with two-layer net.
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.18329718004338394,"4.1
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA
The hyper-representation (HR) problem (Franceschi et al., 2018; Grazzi et al., 2020) searches for a
regression (or classiﬁcation) model following a two-phased optimization process. The inner-level
identiﬁes the optimal linear regressor parameters w, and the outer level solves for the optimal em-
bedding model (i.e., representation) parameters λ. Mathematically, the problem can be modeled by
the following bilevel optimization:"
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.1843817787418655,"min
λ∈Rp f(λ) =
1
2n1 ∥T(X1; λ)w∗(λ) −Y1∥2 s.t. w∗(λ) = argmin
w∈Rd
1
2n2 ∥T(X2; λ)w −Y2∥2 + γ"
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.18546637744034708,2 ∥w∥2 (12)
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.18655097613882862,"where X2 ∈Rn2×m and X1 ∈Rn1×m are matrices of synthesized training and validation data,
and Y2 ∈Rn2, Y1 ∈Rn1 are the corresponding response vectors. In the case of shallow HR, the
embedding function T(·; λ) is either a linear transformation or a two-layer network. We generate
data matrices X1, X2 and labels Y1, Y1 following the same process in Grazzi et al. (2020)."
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.1876355748373102,"We compare our ESJ algorithm with the baseline bilevel optimizers AID-FP, AID-CG, ITD-R, and
HOZOG (see Appendix C.1 for details about the baseline algorithms and hyperparameters used)."
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.18872017353579176,Under review as a conference paper at ICLR 2022
SHALLOW HYPER-REPRESENTATION ON SYNTHETIC DATA,0.1898047722342733,"Figure 1 show the performance comparison among the algorithms under linear and two-layer net
embedding models. It can be observed that for both cases, our proposed method ESJ converges
faster than all the other approaches, and the advantage of ESJ becomes more signiﬁcant in Figure 1
(d), which is under a higher-dimensional model of a two-layer net. In particular, ESJ outperforms
the existing ES-based algorithm HOZOG. This is because HOZOG uses the ES technique to ap-
proximate the entire hypergradient, which likely incurs a large estimation error. In contrast, our ESJ
exploits the structure of the hypergradient and uses ES only to estimate the response Jacobian so that
the estimation of hypergradient is more accurate. Such an advantage is more evident under a two-
layer net model, where HOZOG does not converge as shown in Figure 1 (d). This can be explained
by the ﬂat hypergradient norm as shown in Figure 1 (e), which indicates that the hypergradient esti-
mator in HOZOG fails to provide a good descent direction for the outer optimizer. Figure 1 (c) and
(f) further show that the convergence of ESJ does not change substantially with the number N of
inner GD steps, and hence tuning of N in practice is not costly."
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19088937093275488,"4.2
DEEP HYPER-REPRESENTATION ON MNIST DATASET
In order to demonstrate the advantage of our proposed algorithms in large neural net models, we
perform deep hyper-representation to classify MNIST images by learning an entire LeNet network.
The corresponding bilevel problem is given by"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19197396963123645,"min
λ Lout(λ) :=
1
|Dout|
P"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19305856832971802,"(xi,yi)∈Dout
L (w∗(λ)f(xi; λ), yi)"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19414316702819956,"s.t.
w∗(λ) = arg min
w∈Rc×p Lin(w, λ) :=
1
|Din|
P"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19522776572668113,"(xi,yi)∈Din
(L(wf(xi; λ), yi) + β"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.1963123644251627,"2 ∥w∥2).
(13)"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19739696312364424,"where f(xi; λ) ∈Rp corresponds to features extracted from data point xi, L(·, ·) is the cross-
entropy loss function, c = 10 is the number of categories, and Din and Dout are data used to
compute respectively inner and outer loss functions. Since the sizes of Dout and Din are large in the
case of MNIST dataset, we apply the more efﬁcient stochastic algorithm ESJ-S in Algorithm 2 with
a minibatch size B = 256 to estimate the inner and outer losses Lin and Lout."
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.1984815618221258,"0
100
200
300
400
Running time 0.5 0.8 0.9 1.0"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.19956616052060738,Accuracy ESJ-S
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20065075921908893,AID-FP
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.2017353579175705,AID-CG
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20281995661605207,stocBiO
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.2039045553145336,"0
100
200
300
400
Running time 0.0 0.5 1.0 1.5 2.0"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20498915401301518,Outer loss ESJ-S
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20607375271149675,AID-FP
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20715835140997832,AID-CG
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20824295010845986,stocBiO
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.20932754880694143,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Running time 0.5 0.8 0.9 1.0"
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.210412147505423,Accuracy
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.21149674620390455,ESJ-S-5
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.21258134490238612,ESJ-S-10
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.21366594360086769,ESJ-S-20
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.21475054229934923,"(a) Accuracy on outer data
(b) Outer loss value
(c) ESJ-S-N: N inner SGD steps
Figure 2: Deep HR on the MNIST dataset."
DEEP HYPER-REPRESENTATION ON MNIST DATASET,0.2158351409978308,"Figure 2 compares the classiﬁcation accuracy on the outer dataset Dout among the different methods.
Our algorithm ESJ-S converges with the fastest rate and attains the best accuracy with the lowest
variance among all algorithms. Note that ESJ-S is able to attain the same accuracy of 0.98+ obtained
by the standard training of all parameters with one-phased optimization on the MNIST dataset using
the same backbone network. All other methods fail to recover such a level of performance, and
instead saturate around an accuracy of 0.93. Further, Figure 2(c) indicates that the convergence of
ESJ-S does not change substantially with the number N of inner SGD steps. This demonstrates the
robustness of our method when applied to complex function geometries such as deep nets."
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.21691973969631237,"4.3
FEW-SHOT META-LEARNING OVER MINIIMAGENET"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.21800433839479394,"To study our algorithms over larger neural nets, we study the few-shot image recognition problem,
where classiﬁcation tasks Ti, i = 1, ..., m are sampled over a distribution PT . In particular, we
consider the ANIL meta-learning method (Raghu et al., 2019; Ji et al., 2020a), where all tasks share
common embedding features parameterized by φ, and each task Ti has its task-speciﬁc parameter
wi for i = 1, ..., m. More speciﬁcally, we set φ to be the parameters of the convolutional part of a
deep CNN model (e.g., ResNet-12 network) and w includes the parameters of the last classiﬁcation
layer. All model parameters (φ, w) are trained following a bilevel procedure. In the inner-loop,
the base learner of each task Ti ﬁxes φ and minimizes its loss function over a training set Si to"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.21908893709327548,Under review as a conference paper at ICLR 2022
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22017353579175705,"obtain its adapted parameters w∗
i . At the outer stage, the meta-learner computes the test loss for
each task Ti using the parameters (φ, w∗
i ) over a test set Di, and optimizes the parameters φ of the
common embedding function by minimizing the meta-objective Lmeta over all classiﬁcation tasks.
The problem can be expressed as the following bilevel optimization"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22125813449023862,"min
φ Lmeta(φ, ew∗) := 1"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22234273318872017,"m
Pm
i=1 LDi (φ, w∗
i )"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22342733188720174,"s.t.
ew∗= arg min
e
w
Ladapt(φ, ew) := 1"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.2245119305856833,"m
Pm
i=1 LSi (φ, wi) ,
(14)"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22559652928416485,"where we collect all task-speciﬁc parameters into ew = (w1, ..., wm) and the corresponding mini-
mizers into ew∗= (w∗
1, ..., w∗
m). The functions LSi (φ, wi) =
1
|Si|
P"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22668112798264642,"ζ∈Si(L(φ, wi; ζ) + R(wi))
and LDi (φ, w∗
i ) =
1
|Di|
P"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.227765726681128,"ξ∈Di L (φ, w∗
i ; ξ) correspond respectively to the training and test loss
functions for task Ti, with R a strongly-convex regularizer and L a classiﬁcation loss function. In
our setting, since the task-speciﬁc parameters correspond to the weights of the last linear layer, the
inner-level objective Ladapt(φ, ew) is strongly convex with respect to ew = (w1, ..., wm). We note
that the problem studied in Section 4.2 can be seen as single-task instances of the more general
multi-task learning problem in eq. (14). However, in contrast to the problem in Section 4.2, the sizes
of the datasets Di and Si are usually small in few-shot learning and full GD can be applied here.
Hence, we use ESJ (Algorithm 1) here. Also since the number m of tasks in few-shot classiﬁcation
datasets is often very large, it is preferable to sample a minibatch of i.i.d. tasks by PT at each meta
(i.e., outer) iteration and update the meta parameters based on these tasks."
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.22885032537960953,"0
5000
10000
15000
20000
25000
30000
Running time 40 50 55 60 65 70"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.2299349240780911,Test accuracy ESJ
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23101952277657267,ProtoNet
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23210412147505424,MetaOptNet ANIL MAML
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23318872017353579,"0
500
1000
1500
2000
2500
3000
3500
Running time 35 40 50 55 60 65 70"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23427331887201736,Test accuracy ESJ ANIL MAML
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23535791757049893,"Algorithm
Time (hours)"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23644251626898047,"ESJ
2.8"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23752711496746204,"MetaOptNet
20+"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.2386117136659436,"ProtNet
4.4"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.23969631236442515,"(a) Test accuracy (ResNet-12)
(b) Test accuracy (CNN4)
(c) Time to reach 69% (ResNet-12)"
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.24078091106290672,Figure 3: 5way-5shot image classiﬁcation on the miniImageNet dataset on single GPU.
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.2418655097613883,"We conduct few-shot meta-learning on the miniImageNet dataset (Vinyals et al., 2016) using two
different backbone networks for feature extraction: ResNet-12 and CNN4 (Vinyals et al., 2016).
The dataset description and hyperparameter details can be found in Appendix C.4. We compare our
algorithm ESJ with four baseline methods for few-shot meta-learning MAML (Finn et al., 2017),
ANIL (Raghu et al., 2019), MetaOptNet (Lee et al., 2019), and ProtoNet (Snell et al., 2017). We
run their efﬁcient Pytorch Lightning implementations available at the learn2learn repository (Arnold
et al., 2019)."
FEW-SHOT META-LEARNING OVER MINIIMAGENET,0.24295010845986983,"Figure 3(a) and (b) show that our algorithm ESJ converges faster than the other baseline methods.
Also, Comparing Figure 3(a) and (b), the advantage of our method over the baselines MAML and
ANIL becomes more signiﬁcant as the size of the network increases. Further, Figure 3(c) shows
that MetaOptNet did not reach 69% accuracy after 20 hours of training with ResNet-12 network. In
comparison, our ESJ is able to attain 69% in less than three hours, which is about 1.5 times less than
the time taken for ProtoNet to reach the same performance level. Both ESJ and ProtoNet saturate
around 70% accuracy after 10 hours of training."
CONCLUSION,0.2440347071583514,"5
CONCLUSION"
CONCLUSION,0.24511930585683298,"In this paper, we propose a novel ES-based approach for bilevel optimization, which eliminates all
second-order computations in the gradient-based approaches. Compared to the existing ES-based
algorithm, our approach explores the analytical structure of the hypergradient, and hence leads to
much more efﬁcient and accurate hypergradient estimation. Thus, our algorithm outperforms the
existing algorithms in the experiments, particularly in the high-dimensional applications. We also
characterize the convergence rate for our proposed algorithms and show that the polynomial-time
complexity can be achieved. We anticipate that our approach will be useful for accelerating bilevel
algorithms in various machine learning problems."
CONCLUSION,0.24620390455531455,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY CHECKLIST,0.2472885032537961,"6
REPRODUCIBILITY CHECKLIST"
REPRODUCIBILITY CHECKLIST,0.24837310195227766,"To ensure reproducibility, we use the Machine Learning Reproducibility Checklist v2.0, Apr. 7
2020 (Pineau et al., 2021).
An earlier version of this checklist (v1.2) was used for NeurIPS
2019 (Pineau et al., 2021)."
REPRODUCIBILITY CHECKLIST,0.24945770065075923,"• For all models and algorithms presented,"
REPRODUCIBILITY CHECKLIST,0.25054229934924077,"– A clear description of the mathematical settings, algorithm, and/or model. We
clearly describe all of the settings, formulations, and algorithms in Section 2.
– A clear explanation of any assumptions. All assumptions are stated in Section 3.1
and details are clearly explained in Section 3.1.
– An analysis of the complexity (time, space, sample size) of any algorithm. We
provide the time/computational complexity analysis for our algorithms in Theorems 1
and 2."
REPRODUCIBILITY CHECKLIST,0.25162689804772237,"• For any theoretical claim,"
REPRODUCIBILITY CHECKLIST,0.2527114967462039,"– A clear statement of the claim. A clear statement of theoretical claims are made in
Section 3.
– A complete proof of the claim. The complete proofs of all claims are available in
Appendix E, Appendix F, and Appendix G."
REPRODUCIBILITY CHECKLIST,0.25379609544468545,"• For all datasets used, check if you include:"
REPRODUCIBILITY CHECKLIST,0.25488069414316705,"– The relevant statistics, such as number of examples.
We use widely adopted
datasets MNIST and miniImageNet in Section 4. The related statistics can be seen
at http://yann.lecun.com/exdb/mnist/.
– The details of train/validation/test splits. We give this information in the Supple-
mentary Appendix C.
– An explanation of any data that were excluded, and all pre-processing step. We
did not exclude any data or perform any pre-processing.
– For new data collected,a complete description of the data collection process, such
as instructions to annotators and methods for quality control. We do not collect
or release new datasets."
REPRODUCIBILITY CHECKLIST,0.2559652928416486,"• For all shared code related to this work, check if you include:"
REPRODUCIBILITY CHECKLIST,0.25704989154013014,"– Training code. The training code is available in our code in supplementary material.
– Evaluation code. The evaluation code is available in our code in supplementary ma-
terial.
– (Pre-)trained model(s). We do not release any pre-trained models."
REPRODUCIBILITY CHECKLIST,0.25813449023861174,"• For all reported experimental results, check if you include:"
REPRODUCIBILITY CHECKLIST,0.2592190889370933,"– The range of hyper-parameters considered, method to select the best hyper-
parameter conﬁguration, and speciﬁcation of all hyper-parameters used to gen-
erate results. We provide all details of the hyper-parameter tuning in Supplemen-
tary Appendix C.
– A clear deﬁnition of the speciﬁc measure or statistics used to report results. We
use the classiﬁcation accuracy on test-set and the loss on the train-set.
– A description of results with central tendency (e.g. mean) & variation (e.g. error
bars). We do not report the mean and standard deviation for experiments.
– The average runtime for each result, or estimated energy cost. We report the
running time of the algorithms in Section 4.
– A description of the computing infrastructure used. All detailed descriptions are
presented in Section 4."
REPRODUCIBILITY CHECKLIST,0.2603036876355748,Under review as a conference paper at ICLR 2022
REFERENCES,0.2613882863340564,REFERENCES
REFERENCES,0.26247288503253796,"Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, and Ian Bunner. learn2learn, 2019.
https://github.com/learnables/learn2learn."
REFERENCES,0.2635574837310195,"Juhan Bae and Roger B. Grosse. Delta-stn: Efﬁcient bilevel optimization for neural networks using
structured response jacobians. CoRR, abs/2010.13514, 2020."
REFERENCES,0.2646420824295011,"Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. In International Conference on Learning Representations (ICLR),
2018."
REFERENCES,0.26572668112798264,"Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the
constraints. Operations Research, 21(1):37–44, 1973."
REFERENCES,0.2668112798264642,"Tianyi Chen, Yuejiao Sun, and Wotao Yin.
A single-timescale stochastic bilevel optimization
method. arXiv preprint arXiv:2102.04671, 2021."
REFERENCES,0.2678958785249458,"Justin Domke. Generic methods for optimization-based modeling. International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), pp. 318–326, 2012."
REFERENCES,0.26898047722342733,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proc. International Conference on Machine Learning (ICML), pp. 1126–1135,
2017."
REFERENCES,0.27006507592190887,"Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In International Conference on Machine Learning
(ICML), pp. 1165–1173, 2017."
REFERENCES,0.27114967462039047,"Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning (ICML), pp. 1568–1577, 2018."
REFERENCES,0.272234273318872,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.27331887201735355,"Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018."
REFERENCES,0.27440347071583515,"Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison
Guo. On differentiating parameterized argmin and argmax problems with application to bi-level
optimization. arXiv preprint arXiv:1607.05447, 2016."
REFERENCES,0.2754880694143167,"Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. International Conference on Machine Learning (ICML)),
2020."
REFERENCES,0.2765726681127983,"Bin Gu, Guodong Liu, Yanfu Zhang, Xiang Geng, and Heng Huang. Optimizing large-scale hy-
perparameters via automated learning algorithm. CoRR, 2021. URL https://arxiv.org/
abs/2102.09026."
REFERENCES,0.27765726681127983,"Zhishuai Guo and Tianbao Yang. Randomized stochastic variance-reduced methods for stochastic
bilevel optimization. arXiv preprint arXiv:2105.02266, 2021."
REFERENCES,0.2787418655097614,"Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average
estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021."
REFERENCES,0.279826464208243,"Nikolaus Hansen. The cma evolution strategy: a comparing review. Towards a new evolutionary
computation, pp. 75–102, 2006."
REFERENCES,0.2809110629067245,"Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel
programming. SIAM Journal on Scientiﬁc and Statistical Computing, 13(5):1194–1217, 1992."
REFERENCES,0.28199566160520606,Under review as a conference paper at ICLR 2022
REFERENCES,0.28308026030368766,"Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic.
arXiv preprint
arXiv:2007.05170, 2020."
REFERENCES,0.2841648590021692,"Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization.
arXiv preprint arXiv:2102.03926, 2021."
REFERENCES,0.28524945770065074,"Kaiyi Ji, Zhe Wang, Yi Zhou, and Yingbin Liang. Improved zeroth-order variance reduced algo-
rithms and analysis for nonconvex optimization. In International Conference on Machine Learn-
ing (ICML), pp. 3100–3109, 2019."
REFERENCES,0.28633405639913234,"Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-speciﬁc adaptation over partial parameter. In Advances in Neural Information Processing
Systems (NeurIPS), 2020a."
REFERENCES,0.2874186550976139,"Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020b."
REFERENCES,0.2885032537960954,"Kayi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced
design. International Conference on Machine Learning (ICML)), 2021."
REFERENCES,0.289587852494577,"Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A
momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization.
arXiv e-prints, pp. arXiv–2102, 2021a."
REFERENCES,0.29067245119305857,"Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A
near-optimal algorithm for stochastic bilevel optimization via double-momentum. arXiv preprint
arXiv:2102.07367, 2021b."
REFERENCES,0.2917570498915401,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.2928416485900217,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.29392624728850325,"Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10657–10665, 2019."
REFERENCES,0.2950108459869848,"Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, Xaq Yoon, KiJung Pitkow, Raquel Urtasun,
and Richard Zemel. Reviving and improving recurrent back-propagation. International Confer-
ence on Machine Learning (ICML)), 2018."
REFERENCES,0.2960954446854664,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018a."
REFERENCES,0.29718004338394793,"Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic ﬁrst-order al-
gorithmic framework for bi-level programming beyond lower-level singleton. In International
Conference on Machine Learning (ICML), 2020a."
REFERENCES,0.2982646420824295,"Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level opti-
mization for learning and vision from a uniﬁed perspective: A survey and beyond. arXiv preprint
arXiv:2101.11517, 2021a."
REFERENCES,0.2993492407809111,"Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based
interior-point method for non-convex bi-level optimization. In International Conference on Ma-
chine Learning (ICML), 2021b."
REFERENCES,0.3004338394793926,"Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-
order stochastic variance reduction for nonconvex optimization. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), pp. 3731–3741, 2018b."
REFERENCES,0.30151843817787416,Under review as a conference paper at ICLR 2022
REFERENCES,0.30260303687635576,"Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong,
and Una-May O’Reilly. Min-max optimization without gradients: Convergence and applications
to black-box evasion and poisoning attacks. In International Conference on Machine Learning
(ICML), pp. 6282–6293, 2020b."
REFERENCES,0.3036876355748373,"Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. International Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 1540–1552, 2020."
REFERENCES,0.3047722342733189,"Matthew Mackay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger Grosse. Self-tuning
networks: Bilevel optimization of hyperparameters using structured best-response functions. In
International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.30585683297180044,"Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning (ICML), pp.
2113–2122, 2015."
REFERENCES,0.306941431670282,"Gregory M Moore. Bilevel programming algorithms for machine learning model selection. Rensse-
laer Polytechnic Institute, 2010."
REFERENCES,0.3080260303687636,"Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, pp. 527–566, 2017."
REFERENCES,0.3091106290672451,"Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning (ICML), pp. 737–746, 2016."
REFERENCES,0.31019522776572667,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer,
Florence d’Alch´e Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine
learning research. Journal of Machine Learning Research, 22:1–20, 2021."
REFERENCES,0.31127982646420826,"Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of MAML. International Conference on Learning Rep-
resentations (ICLR), 2019."
REFERENCES,0.3123644251626898,"Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. In Advances in Neural Information Processing Systems (NeurIPS), pp. 113–124,
2019."
REFERENCES,0.31344902386117135,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision,
3(115):211–252, 2015."
REFERENCES,0.31453362255965295,"Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In International Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 1723–1732, 2019."
REFERENCES,0.3156182212581345,"Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn–tucker approach for linear bilevel
programming. Applied Mathematics and Computation, 162(1):51–63, 2005."
REFERENCES,0.31670281995661603,"Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NIPS), 2017."
REFERENCES,0.31778741865509763,"Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao
Tang. Es-maml: Simple hessian-free meta learning. In International Conference on Learning
Representations (ICLR), 2019."
REFERENCES,0.3188720173535792,"Konstantinos Varelas, Anne Auger, Dimo Brockhoff, Nikolaus Hansen, Ouassim Ait ElHara, Yann
Semet, Rami Kassab, and Fr´ed´eric Barbaresco. A comparative study of large-scale variants of
cma-es. In International Conference on Parallel Problem Solving from Nature, pp. 3–15. Springer,
2018."
REFERENCES,0.3199566160520607,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, and Daan Wierstra. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems (NIPS), 2016."
REFERENCES,0.3210412147505423,Under review as a conference paper at ICLR 2022
REFERENCES,0.32212581344902386,"Tengyu Xu, Zhe Wang, Yingbin Liang, and H Vincent Poor. Gradient free minimax optimization:
Variance reduction and faster convergence. arXiv preprint arXiv:2006.09361, 2020."
REFERENCES,0.3232104121475054,"Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. arXiv
preprint arXiv:2106.04692, 2021."
REFERENCES,0.324295010845987,"Miao Zhang, Steven Su, Shirui Pan, Xiaojun Chang, Ehsan Abbasnejad, and Reza Haffari.
idarts: Differentiable architecture search with stochastic implicit gradients.
arXiv preprint
arXiv:2106.10784, 2021."
REFERENCES,0.32537960954446854,"Daniel Z¨ugner and Stephan G¨unnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.3264642082429501,Under review as a conference paper at ICLR 2022
REFERENCES,0.3275488069414317,Supplementary Material
REFERENCES,0.3286334056399132,"A
STOCHASTIC BILEVEL OPTIMIZER BASED ON ES METHOD"
REFERENCES,0.3297180043383948,"In the following, we present the algorithm speciﬁcation for our proposed stochastic bilevel optimizer
based on ES method, which we call ESJ-S."
REFERENCES,0.33080260303687636,Algorithm 2 Stochastic bilevel optimizer via ES Jacobians (ESJ-S)
REFERENCES,0.3318872017353579,"1: Input: lower- and upper-level stepsizes α, β > 0, initializations x0 ∈Rp and y0 ∈Rd, inner
and outer iterations numbers K and N, and number of Gaussian vectors Q.
2: for k = 0, 1, ..., K do
3:
Set Y 0
k = y0,
Y 0
k,j = y0, j = 1, ..., Q"
REFERENCES,0.3329718004338395,"4:
Generate uk,j = N(0, I) ∈Rp,
j = 1, ..., Q
5:
for t = 1, 2, ..., N do
6:
Draw a sample batch St−1
7:
Update Y t
k = Y t−1
k
−α∇yG(xk, Y t−1
k
; St−1)
8:
for j = 1, ..., Q do"
REFERENCES,0.33405639913232105,"9:
Update Y t
k,j = Y t−1
k,j
−α∇yG

xk + µuk,j, Y t−1
k,j ; St−1
"
REFERENCES,0.3351409978308026,"10:
end for
11:
end for"
REFERENCES,0.3362255965292842,"12:
Compute δj =
Y N
k,j−Y N
k
µ
,
j = 1, ..., Q"
REFERENCES,0.33731019522776573,"13:
Draw a sample batch DF
14:
Compute b∇Φ(xk) = ∇xF(xk, Y N
k ; DF ) + 1"
REFERENCES,0.3383947939262473,"Q
PQ
j=1

δj, ∇yF(xk, Y N
k ; DF )

uk,j"
REFERENCES,0.33947939262472887,"15:
Update xk+1 = xk −β b∇Φ(xk)
16: end for"
REFERENCES,0.3405639913232104,"B
ADDITIONAL RELATED WORK"
REFERENCES,0.34164859002169196,"In this section, we provide further related work on the applications of bilevel optimization and ES
methods."
REFERENCES,0.34273318872017355,"Bilevel optimization applications. Bilevel optimization has been employed in various applications
such as few-shot meta-learning (Snell et al., 2017; Franceschi et al., 2018; Rajeswaran et al., 2019;
Z¨ugner & G¨unnemann, 2019; Ji et al., 2020a;b), hyperparameter optimization (Franceschi et al.,
2017; Mackay et al., 2018; Shaban et al., 2019), neural architecture search (Liu et al., 2018a; Zhang
et al., 2021), etc. This paper demonstrates the superior performance of the proposed ES-based
bilevel optimizer in meta-learning and hyperparameter optimization."
REFERENCES,0.3438177874186551,"ES applications. Evolution Strategies (ES) method has been studied for more than two decades (see
review papers (Hansen, 2006; Varelas et al., 2018)). In particular, Nesterov & Spokoiny (2017) pro-
posed a simple and effective ES-gradient via Gaussian smoothing, which was further extended to the
stochastic setting by Ghadimi & Lan (2013). Such an ES technique has exhibited great effectiveness
in various applications including meta-reinforcement learning (Song et al., 2019), hyperparameter
optimization (Gu et al., 2021), adversarial machine learning (Ji et al., 2019; Liu et al., 2018b), min-
imax optimization (Liu et al., 2020b; Xu et al., 2020), etc. This paper proposes a novel ES-based
Jacobian estimator for accelerating bilevel optimization. The ES method has also been used for
studying hyperparameter optimization problems. For example, Mackay et al. (2018) models the
response function itself as a neural network (where each layer involves an afﬁne transformation of
hyperparameters) using the Self-Tuning Networks (STNs). An improved and more stable version
of STNs was further proposed in Bae & Grosse (2020), which focused on accurately approximating
the response Jacobian rather than the response function itself."
REFERENCES,0.34490238611713664,Under review as a conference paper at ICLR 2022
REFERENCES,0.34598698481561824,"C
FURTHER SPECIFICATIONS FOR EXPERIMENTS IN SECTION 4"
REFERENCES,0.3470715835140998,"We note that the smoothing parameter µ (in Algorithms 1 and 2) was easy to set and a value of 0.1
or 0.01 yields a good starting point across all our experiments. The batch size Q (in Algorithms 1
and 2) is ﬁxed to 1 (i.e., we use one Jacobian oracle) in all our experiments."
REFERENCES,0.3481561822125813,"C.1
SPECIFICATIONS ON BASELINE BILEVEL APPROACHES IN SECTION 4.1"
REFERENCES,0.3492407809110629,We compare our algorithm ESJ with the following baseline methods:
REFERENCES,0.35032537960954446,"• HOZOG (Gu et al., 2021): a hyperparameter optimization algorithm that uses evolution strate-
gies to estimate the entire hypergradient (both the direct and indirect component). We use our
own implementation for this method.
• AID-CG (Grazzi et al., 2020; Rajeswaran et al., 2019): approximate implicit differentiation
with conjugate gradient. We use its implementation provided at https://github.com/
prolearner/hypertorch
• AID-FP (Grazzi et al., 2020): approximate implicit differentiation with ﬁxed-point. We exper-
imented with its implementation at the repositery https://github.com/prolearner/
hypertorch
• ITD-R (REVERSE) (Franceschi et al., 2017): an iterative differentiation method that computes
hypergradients using reverse mode automatic differention (RMAD). We use its implementation
provided at https://github.com/prolearner/hypertorch."
REFERENCES,0.351409978308026,"C.2
HYPERPARAMETERS DETAILS FOR SHALLOW HR EXPERIMENTS IN SECTION 4.1"
REFERENCES,0.3524945770065076,"For the linear embedding case, we set the smoothing parameter µ to be 0.01 for ESJ and HOZOG.
We use the following hyperparameters for all compared methods. The number of inner GD steps
is ﬁxed to N = 20 with the learning rate of α = 0.001. For the outer optimizer, we use Adam
(Kingma & Ba, 2014) with a learning rate of 0.05. The value of γ in eq. (12) is set to be 0.1. For
the two-layer net case, we use µ = 0.1 for ESJ and HOZOG. For all methods, we set N = 10,
α = 0.001, β = 0.001, and use Adam with a learning rate of 0.01 as the outer optimizer."
REFERENCES,0.35357917570498915,"C.3
SPECIFICATIONS ON BASELINE STOCHASTIC ALGORITHMS IN SECTION 4.2"
REFERENCES,0.3546637744034707,"We compare our stochastic algorithm ESJ-S with the following baseline stochastic bilevel algo-
rithms."
REFERENCES,0.3557483731019523,"• stocBiO: an approximate implicit differentiation method that uses Neumann Series to esti-
mate the Hessian inverse. We use its implementation available at https://github.com/
JunjieYang97/StocBio.
• AID-CG-S and AID-FP-S: stochastic versions of AID-CG and AID-FP, respectively.
We
use their implementations in the repository https://github.com/prolearner/
hypertorch."
REFERENCES,0.35683297180043383,"C.4
SPECIFICATIONS FOR FEW-SHOT META-LEARNING IN SECTION 4.3"
REFERENCES,0.3579175704989154,"The miniImageNet dataset (Vinyals et al., 2016) is a large-scale benchmark for few-shot learning
generated from ImageNet (Russakovsky et al., 2015) Russakovsky. The dataset consists of 100
classes with each class containing 600 images of size 84 × 84. Following (Arnold et al., 2019), we
split the classes into 64 classes for meta-training, 16 classes for meta-validation, and 20 classes for
meta-testing. More specﬁcally, we use 20000 tasks for meta-training, 600 tasks for meta-validation,
and 600 tasks for meta-testing. We normalize all image pixels by their means and standard deviations
over RGB channels and do not perform any additional data augmentation. At each meta-iteration,
we sample a batch of 16 training tasks and update the parameters based on these tasks. We set the
smoothness parameter to be µ = 0.1 and use N = 30 inner steps. We use SGD with a learning rate
of α = 0.01 as inner optimizer and Adam with a learning rate of β = 0.01 as outer (meta) optimizer."
REFERENCES,0.35900216919739697,Under review as a conference paper at ICLR 2022
REFERENCES,0.3600867678958785,"0.2
0.4
0.6
0.8
1.0
1.2
Running time 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
REFERENCES,0.3611713665943601,Validation accuracy ESJ HOZOG
REFERENCES,0.36225596529284165,AID-CG
REFERENCES,0.3633405639913232,AID-FP
REFERENCES,0.3644251626898048,REVERSE
REFERENCES,0.36550976138828634,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
Running time 0.60 0.65 0.70 0.75 0.80 0.85"
REFERENCES,0.3665943600867679,Validation accuracy ESJ HOZOG
REFERENCES,0.3676789587852495,AID-CG
REFERENCES,0.368763557483731,AID-FP
REFERENCES,0.36984815618221256,REVERSE
REFERENCES,0.37093275488069416,"Figure 4: Classiﬁcation results on 20 Newsgroup dataset. Left: number of inner GD step N = 5.
Right: number of inner GD steps N = 10. Running time is in seconds."
REFERENCES,0.3720173535791757,"D
EXPERIMENTS ON HYPERPARAMETER OPTIMIZATION"
REFERENCES,0.37310195227765725,"Hyperparameter optimization (HO) is the problem of ﬁnding the set of the best hyperparamters (ei-
ther representational or regularization parameters) that yield the optimal value of some criterion of
model quality (usually a validation loss on unseen data). HO can be posed as a bilevel optimization
problem in which the inner problem corresponds to ﬁnding the model parameters by minimizing a
training loss (usually regularized) for the given hyperparameters and then the outer problem mini-
mizes over the hyperparameters. Hence, HO can be mathematically expressed as follows"
REFERENCES,0.37418655097613884,"min
λ Lval(λ) :=
1
|Dval|
P"
REFERENCES,0.3752711496746204,ξ∈Dval L (w∗(λ); ξ)
REFERENCES,0.37635574837310193,"s.t.
w∗(λ) = arg min
w
Ltr(w, λ) :=
1
|Dtr|
P"
REFERENCES,0.3774403470715835,"ζ∈Dtr(L(w, λ; ζ) + R(w, λ)),
(15)"
REFERENCES,0.37852494577006507,"where L is a loss function (e.g., logistic loss), R(w, λ) is a regularizer, and Dtr and Dval are respec-
tively training and validation data. Note that the loss function used to identify hyperparameters must
be different from the one used to ﬁnd model parameters; otherwise models with higher complexities
would be always favored. This is usually achieved in HO by using different data splits (here Dval
and Dtr) to compute validation and training losses, and by adding a regularizer term on the training
loss."
REFERENCES,0.3796095444685466,"Following (Franceschi et al., 2017; Grazzi et al., 2020), we perform classiﬁcation on the 20 News-
group dataset, where the classiﬁer is modeled by an afﬁne transformation, the cost function L is
the cross-entrpy loss, and R(w, λ) is a strongly-convex regularizer. We set one l2-regularization
hyperparameter for each weight in w, so that λ and w have the same size."
REFERENCES,0.3806941431670282,"For ESJ and HOZOG, we use GD with a learning rate of 100 and a momentum of 0.9 to perform the
inner updates. The outer learning rate is set to be 0.02. We set the smoothing parameter (µ in Algo-
rithm 1) to be 0.01. For AID-FP, AID-CG, and REVERSE we use the suggested hyperparameters
in their implementations accompanying the paper Grazzi et al. (2020)."
REFERENCES,0.38177874186550975,"It can be seen from Figure 4, our method ESJ slightly outperforms HOZOG and converges faster
than the other AID and ITD based approaches. We note that the similar performance for ESJ and
HOZOG can be explained by the fact that in HO, the hypergradient expession in eq. (6) contains
only the second term (the ﬁrst term is zero), which is very close to the approximation in HOZOG
method. However, as we have seen in the other experiments (not for HO), ESJ is a more robust
and stable bilevel optimizer than HOZOG, and it achieves good performance across many bilevel
problems."
REFERENCES,0.38286334056399135,"E
SUPPORTING TECHNICAL LEMMAS"
REFERENCES,0.3839479392624729,"In this section, we provide auxiliary lemmas that are used for proving the convergence results for
the algorithms ESJ and ESJ-S."
REFERENCES,0.38503253796095444,"In the following proofs, we let L = max{Lg, Lf} and D be such that ∥y∗(x)∥≤D."
REFERENCES,0.38611713665943603,Under review as a conference paper at ICLR 2022
REFERENCES,0.3872017353579176,"First we recall that for any two matrices A ∈Rm×r and B ∈Rr×n, we have the following upper-
bound on the Frobenius norm of their product,
AB

F ≤
A
B

F .
(16)"
REFERENCES,0.3882863340563991,"The following lemma follows directly from the Lipschitz properties in Assumptions 2 and 3.
Lemma 1. Suppose that Assumptions 2 and 3 hold. Then, the stochastic derivatives ∇F(x, y; ξ),
∇x∇yG(x, y; ξ), and ∇2
yG(x, y; ξ) have bounded variances, i.e., for any (x, y) and ξ we have:"
REFERENCES,0.3893709327548807,"• Eξ
∇F(x, y; ξ) −∇f(x, y)
2 ≤M 2;"
REFERENCES,0.39045553145336226,"• Eξ
∇x∇yG(x, y; ξ) −∇x∇yg(x, y)
2
F ≤L2;"
REFERENCES,0.3915401301518438,"• Eξ
∇2
yG(x, y; ξ) −∇2
yg(x, y)
2
F ≤L2."
REFERENCES,0.3926247288503254,"Using Lemma 2.2 in Ghadimi & Wang (2018), the following lemma characterizes the Lipschitz
property of the gradient of the total objective Φ(x) = f(x, y∗(x)).
Lemma 2. Suppose that Assumptions 1, 2, and 3 hold. Then, we have:
∇Φ(x2) −∇Φ(x1)
 ≤LΦ
x2 −x1

∀x1 ∈Rp, x2 ∈Rp,"
REFERENCES,0.39370932754880694,where the constant LΦ = L + 2L2+τM2
REFERENCES,0.3947939262472885,"µg
+ ρLM+L3+τML"
REFERENCES,0.3958785249457701,"µ2g
+ ρL2M µ3g
."
REFERENCES,0.3969631236442516,"We next provide some essential properties of the ES-based (i.e., zeroth-order) gradient oracle in
eq. (4), due to Nesterov & Spokoiny (2017).
Lemma 3. Let h : Rn →R be a differentiable function with L-Lipsctitz gradient. Deﬁne its
Gaussian smooth approximation hµ(x) = Eu [h(x + µu)], where µ > 0 and u ∈Rn is a standard
Gaussian random vector. Then, hµ is differentiable and we have:"
REFERENCES,0.39804772234273317,• The gradient of hµ takes the form
REFERENCES,0.39913232104121477,"∇hµ(x) = Eu
h(x + µu) −h(x) µ
u."
REFERENCES,0.4002169197396963,"• For any x ∈Rn,"
REFERENCES,0.40130151843817785,"∇hµ(x) −∇h(x)
 ≤µ2"
REFERENCES,0.40238611713665945,2 L(n + 3)3/2.
REFERENCES,0.403470715835141,"• For any x ∈Rn,"
REFERENCES,0.40455531453362253,"Eu
h(x + µu) −h(x)"
REFERENCES,0.40563991323210413,"µ
u

2
≤4(n + 4)
∇hµ(x)
2 + 3"
REFERENCES,0.4067245119305857,2µ2L2(n + 5)3.
REFERENCES,0.4078091106290672,"Note the ﬁrst item in Lemma 3 implies that the oracle in eq. (4) is in indeed an unbiased estimator
of the gradient of the smoothed function hµ."
REFERENCES,0.4088937093275488,Lemma 4. Suppose that Assumptions 1 and 2 hold. The Jacobian J∗= ∂y∗(x)
REFERENCES,0.40997830802603036,"∂x
has bounded norm:"
REFERENCES,0.41106290672451196,"J∗

F ≤L"
REFERENCES,0.4121475054229935,"µg
.
(17)"
REFERENCES,0.41323210412147504,"Proof of Lemma 4. From the ﬁrst order optimality condition of y∗(x), we have ∇yg (x, y∗(x)) = 0.
Hence, the Implicit Function Theorem implies:"
REFERENCES,0.41431670281995664,"J∗= −

∇2
yg (x, y∗(x))
−1 ∇x∇yg (x, y∗(x)) .
(18)"
REFERENCES,0.4154013015184382,"Taking norms and applying eq. (16) together with Assumptions 1 and 2 yield the desired result
J∗

F ≤
∇x∇yg (x, y∗(x))

F
 
∇2
yg (x, y∗(x))
−1  ≤L"
REFERENCES,0.4164859002169197,"µg
.
(19)"
REFERENCES,0.4175704989154013,Under review as a conference paper at ICLR 2022
REFERENCES,0.41865509761388287,"Lemma 5. Suppose that Assumptions 1 and 2 hold. The Jacobian JN = ∂yN
k
∂xk has bounded norm:"
REFERENCES,0.4197396963123644,"JN

F ≤L"
REFERENCES,0.420824295010846,"µg
.
(20)"
REFERENCES,0.42190889370932755,Proof of Lemma 5. The inner loop gradient descent updates writes
REFERENCES,0.4229934924078091,"yt
k = yt−1
k
−α∇yg
 
xk, yt−1
k

,
t = 1, . . . , N."
REFERENCES,0.4240780911062907,Taking derivatives w.r.t. xk yields
REFERENCES,0.42516268980477223,"Jt = Jt−1 −α∇x∇yg
 
xk, yt−1
k

−αJt−1∇2
yg
 
xk, yt−1
k
"
REFERENCES,0.4262472885032538,"= Jt−1
 
I −α∇2
yg
 
xk, yt−1
k

−α∇x∇yg
 
xk, yt−1
k

."
REFERENCES,0.42733188720173537,Telescoping over t from 1 to N yields
REFERENCES,0.4284164859002169,"JN = J0 N−1
Y t=0"
REFERENCES,0.42950108459869846," 
I −α∇2
yg
 
xk, yt
k

−α N−1
X"
REFERENCES,0.43058568329718006,"t=0
∇x∇yg
 
xk, yt
k

N−1
Y m=t+1"
REFERENCES,0.4316702819956616," 
I −α∇2
yg (xk, ym
k )
 = −α N−1
X"
REFERENCES,0.43275488069414314,"t=0
∇x∇yg
 
xk, yt
k

N−1
Y m=t+1"
REFERENCES,0.43383947939262474," 
I −α∇2
yg (xk, ym
k )

.
(21)"
REFERENCES,0.4349240780911063,"Hence, we have"
REFERENCES,0.4360086767895879,"JN

F ≤α N−1
X t=0"
REFERENCES,0.4370932754880694,"∇x∇yg
 
xk, yt
k
 
F

N−1
Y m=t+1"
REFERENCES,0.43817787418655096," 
I −α∇2
yg (xk, ym
k )
"
REFERENCES,0.43926247288503256,"(i)
≤α N−1
X t=0
L N−1
Y m=t+1"
REFERENCES,0.4403470715835141,"I −α∇2
yg (xk, ym
k )"
REFERENCES,0.44143167028199565,"(ii)
≤αL N−1
X"
REFERENCES,0.44251626898047725,"t=0
(1 −αµg)N−1−t = αL N−1
X"
REFERENCES,0.4436008676789588,"t=0
(1 −αµg)t ≤L µg"
REFERENCES,0.44468546637744033,"where (i) follows from Assumption 2 and (ii) applies the strong-convexity of function g(x, ·). This
completes the proof."
REFERENCES,0.44577006507592193,"Lemma 6. Suppose that Assumptions 1 and 2 hold. Then, the Jacobian in the stochastic algorithm
ESJ-S JN = ∂Y N
k
∂xk has bounded norm, as shown below."
REFERENCES,0.44685466377440347,"JN

F ≤L"
REFERENCES,0.447939262472885,"µg
.
(22)"
REFERENCES,0.4490238611713666,Proof of Lemma 6. The proof follows similarly to Lemma 5.
REFERENCES,0.45010845986984815,"E.1
PROOF OF PROPOSITION 1"
REFERENCES,0.4511930585683297,Proposition 1 (Re-stated). Suppose that Assumptions 1 and 2 hold. Deﬁne the constant
REFERENCES,0.4522776572668113,"LJ =

1 + L µg   τ"
REFERENCES,0.45336225596529284,"µg
+ ρL µ2g"
REFERENCES,0.4544468546637744,"
.
(23)"
REFERENCES,0.455531453362256,"Then, the Jacobian JN(x) = ∂yN(x)"
REFERENCES,0.4566160520607375,"∂x
is LJ -Lipschitz with respect to x under the Frobenious norm:
JN(x1) −JN(x2)

F ≤LJ
x1 −x2

∀x1 ∈Rp, x2 ∈Rp.
(24)"
REFERENCES,0.45770065075921906,Under review as a conference paper at ICLR 2022
REFERENCES,0.45878524945770066,"Proof of Proposition 1. Using eq. (21), we have"
REFERENCES,0.4598698481561822,"JN(x) = −α N−1
X"
REFERENCES,0.4609544468546638,"t=0
∇x∇yg
 
x, yt(x)

N−1
Y m=t+1"
REFERENCES,0.46203904555314534," 
I −α∇2
yg (x, ym(x))

,
x ∈Rp"
REFERENCES,0.4631236442516269,"Hence, for x1 ∈Rp and x2 ∈Rp, we have
JN(x1) −JN(x2)

F =α N−1
X"
REFERENCES,0.4642082429501085,"t=0
∇x∇yg
 
x1, yt(x1)

N−1
Y m=t+1"
REFERENCES,0.46529284164859," 
I −α∇2
yg (x1, ym(x1))
 − N−1
X"
REFERENCES,0.46637744034707157,"t=0
∇x∇yg
 
x2, yt(x2)

N−1
Y m=t+1"
REFERENCES,0.46746203904555317," 
I −α∇2
yg (x2, ym(x2))
 
F ≤α N−1
X t=0"
REFERENCES,0.4685466377440347,"∇x∇yg
 
x1, yt(x1)

N−1
Y m=t+1"
REFERENCES,0.46963123644251625," 
I −α∇2
yg (x1, ym(x1))
"
REFERENCES,0.47071583514099785,"−∇x∇yg
 
x2, yt(x2)

N−1
Y m=t+1"
REFERENCES,0.4718004338394794," 
I −α∇2
yg (x2, ym(x2))
 
F"
REFERENCES,0.47288503253796094,"(i)
≤α N−1
X t=0"
REFERENCES,0.47396963123644253,"∇x∇yg
 
x1, yt(x1)
 
F
At(x1) −At(x2) + α N−1
X t=0"
REFERENCES,0.4750542299349241,"At(x2)
∇x∇yg
 
x1, yt(x1)

−∇x∇yg
 
x2, yt(x2)
 
F
(25)"
REFERENCES,0.4761388286334056,"where we deﬁne At(x) = QN−1
m=t+1
 
I −α∇2
yg (x, ym(x))

and (i) follows from eq. (16)."
REFERENCES,0.4772234273318872,"Next we upper bound the quantity
At(x1) −At(x2)
, as shown below.
At(x1) −At(x2)
 ≤
α
 
∇2
yg
 
x2, yt+1(x2)

−∇2
yg
 
x2, yt+1(x2)

At+1(x1)"
REFERENCES,0.47830802603036876,"+
 
I −α∇2
yg
 
x2, yt+1(x2)

(At+1(x1) −At+1(x2))"
REFERENCES,0.4793926247288503,"≤α
At+1(x1)
∇2
yg
 
x1, yt+1(x1)

−∇2
yg
 
x2, yt+1(x2)
"
REFERENCES,0.4804772234273319,"+
I −α∇2
yg
 
x2, yt+1(x2)
 At+1(x1) −At+1(x2)"
REFERENCES,0.48156182212581344,"≤(1 −αµg)
At+1(x1) −At+1(x2)"
REFERENCES,0.482646420824295,+ αρ(1 + L
REFERENCES,0.4837310195227766,"µg
)(1 −αµg)N−t−2x1 −x2
,
(26)"
REFERENCES,0.4848156182212581,where the last inequality follows from Lemma 5 and Assumptions 1 and 2.
REFERENCES,0.48590021691973967,"Telescoping eq. (26) over t yields
At(x1) −At(x2)
 ≤(1 −αµg)N−t−2AN−2(x1) −AN−2(x2) +"
REFERENCES,0.48698481561822127,"N−t−3
X"
REFERENCES,0.4880694143167028,"m=0
αρ(1 + L"
REFERENCES,0.4891540130151844,"µg
)(1 −αµg)N−t−2−m(1 −αµg)mx1 −x2"
REFERENCES,0.49023861171366595,"=(1 −αµg)N−t−2∇2
yg
 
x1, yN−1(x1)

−∇2
yg
 
x2, yN−1(x2)
 +"
REFERENCES,0.4913232104121475,"N−t−3
X"
REFERENCES,0.4924078091106291,"m=0
αρ(1 + L"
REFERENCES,0.49349240780911063,"µg
)(1 −αµg)N−t−2x1 −x2"
REFERENCES,0.4945770065075922,"(i)
≤αρ(1 + L"
REFERENCES,0.4956616052060738,"µg
)(1 −αµg)N−t−2x1 −x2"
REFERENCES,0.4967462039045553,+ (N −t −2)αρ(1 + L
REFERENCES,0.49783080260303686,"µg
)(1 −αµg)N−t−2x1 −x2"
REFERENCES,0.49891540130151846,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,=αρ(1 + L
REFERENCES,0.5010845986984815,"µg
)(N −t −1)(1 −αµg)N−t−2x1 −x2
,
(27)"
REFERENCES,0.5021691973969631,"where (i) follows from Lemma 5 and Assumption 2. Replacing eq. (27) in eq. (25) and using
Assumption 2, we have
JN(x1) −JN(x2)

F ≤α N−1
X"
REFERENCES,0.5032537960954447,"t=0
Lαρ(1 + L"
REFERENCES,0.5043383947939263,"µg
)(N −t −1)(1 −αµg)N−t−2x1 −x2 + α N−1
X"
REFERENCES,0.5054229934924078,"t=0
τ(1 + L"
REFERENCES,0.5065075921908894,"µg
)(1 −αµg)N−t−1x1 −x2"
REFERENCES,0.5075921908893709,≤α2Lρ(1 + L
REFERENCES,0.5086767895878525,"µg
)
x1 −x2

N−1
X"
REFERENCES,0.5097613882863341,"m=0
m(1 −αµg)m−1 + τ"
REFERENCES,0.5108459869848156,"µg
(1 + L"
REFERENCES,0.5119305856832972,"µg
)
x1 −x2 ≤ρL"
REFERENCES,0.5130151843817787,"µ2g
(1 + L"
REFERENCES,0.5140997830802603,"µg
)
x1 −x2
 + τ"
REFERENCES,0.5151843817787418,"µg
(1 + L"
REFERENCES,0.5162689804772235,"µg
)
x1 −x2

(28)"
REFERENCES,0.517353579175705,"where we use PN−1
m=0 mxm−1 ≤
1
α2µ2g in eq. (28), which can be obtained by taking derivatives for"
REFERENCES,0.5184381778741866,"the expression PN−1
m=0 xm with respect to x. Hence, rearranging and using the deﬁnition of LJ in
eq. (23) ﬁnishes the proof."
REFERENCES,0.5195227765726681,Lemma 7. Suppose that Assumptions 1 and 2 hold. Deﬁne the constant
REFERENCES,0.5206073752711496,"LJ =

1 + L µg   τ"
REFERENCES,0.5216919739696312,"µg
+ ρL µ2g 
."
REFERENCES,0.5227765726681128,"Then, the Jacobian JN(x) = ∂Y N(x;·)"
REFERENCES,0.5238611713665944,"∂x
is LJ -Lipschitz with respect to x under the Frobenius norm:
JN(x1; ·) −JN(x2; ·)

F ≤LJ
x1 −x2

∀x1 ∈Rp, x2 ∈Rp."
REFERENCES,0.5249457700650759,Proof of Lemma 7. The proof follows similarly to that for Proposition 1.
REFERENCES,0.5260303687635575,"F
PROOFS FOR DETERMINISTIC BILEVEL OPTIMIZATION"
REFERENCES,0.527114967462039,"For notation convenience, we deﬁne the following quantities:"
REFERENCES,0.5281995661605207,"ˆ
JN,j = ˆ
JN(xk, uj) =  

"
REFERENCES,0.5292841648590022,"yN
1 (xk+µuj)−yN
1 (xk)
µ
u⊤
j
..."
REFERENCES,0.5303687635574837,"yN
d (xk+µuj)−yN
d (xk)
µ
u⊤
j "
REFERENCES,0.5314533622559653,"

,
JN = ∂yN
k
∂xk
,
J∗= ∂y∗
k
∂xk
,
(29)"
REFERENCES,0.5325379609544468,"where uj ∈Rp, j = 1, . . . , Q are standard Gaussian vectors. Let yN
i,µ(xk) be the Gaussian smooth
approximation of yN
i (xk). We collect yN
i,µ(xk) for i = 1, . . . , d together as a vector yN
µ (xk), which
is the Gaussian approximation of the vector yN(xk). If µ > 0, yN
µ (xk) is differentiable and we let
Jµ be the Jocobian given by"
REFERENCES,0.5336225596529284,"Jµ = ∂yN
µ (xk)
∂xk
.
(30)"
REFERENCES,0.53470715835141,"We approximate ∂yN
k
∂xk using the average ES estimator given by ˆ
JN = 1"
REFERENCES,0.5357917570498916,"Q
PQ
j=1 ˆ
JN,j. The hypergra-
dient is then approximated as"
REFERENCES,0.5368763557483731,"b∇Φ(xk) = ∇xf(xk, yN
k ) + ˆ
J ⊤
N ∇yf(xk, yN
k )"
REFERENCES,0.5379609544468547,"= ∇xf(xk, yN
k ) + 1 Q Q
X"
REFERENCES,0.5390455531453362,"j=1
ˆ
J ⊤
N,j∇yf(xk, yN
k ).
(31)"
REFERENCES,0.5401301518438177,Under review as a conference paper at ICLR 2022
REFERENCES,0.5412147505422994,Let δj = yN(xk+µuj)−yN(xk)
REFERENCES,0.5422993492407809,"µ
, and let δi,j be the i-th component of δj. Hence, we have"
REFERENCES,0.5433839479392625,"ˆ
JN,j = "
REFERENCES,0.544468546637744,"




"
REFERENCES,0.5455531453362256,"δ1,ju⊤
j
δ2,ju⊤
j
..."
REFERENCES,0.5466377440347071,"δd,ju⊤
j "
REFERENCES,0.5477223427331888,"





."
REFERENCES,0.5488069414316703,"ˆ
J ⊤
N,j∇yf(xk, yN
k ) =

δ1,juj
δ2,juj
. . .
δd,juj

∇yf(xk, yN
k )"
REFERENCES,0.5498915401301518,"=

δj, ∇yf(xk, yN
k )

uj.
(32)"
REFERENCES,0.5509761388286334,"Using eq. (31) and eq. (32), the estimator for the hypergradient can thus be computed as"
REFERENCES,0.5520607375271149,"b∇Φ(xk) = ∇xf(xk, yN
k ) + 1 Q Q
X j=1"
REFERENCES,0.5531453362255966,"δj, ∇yf(xk, yN
k )

uj."
REFERENCES,0.5542299349240781,"F.1
PROOF OF PROPOSITION 2"
REFERENCES,0.5553145336225597,"Proposition 2 (Re-stated). Suppose that Assumptions 1, 2, and 3 hold. Then, the expected estimation
error can be upper-bounded as follows:"
REFERENCES,0.5563991323210412,"E
b∇Φ(xk) −∇Φ(xk)
2 ≤2L2D2(1 −αµg)N + 4L4"
REFERENCES,0.5574837310195228,"µ2g
D2(1 −αµg)N + 24(4p + 15)L2M 2 Qµ2g + µ2"
REFERENCES,0.5585683297180043,"Q L2
J M 2dP4(p) + 24L2M 2(1 −αµg)2N"
REFERENCES,0.559652928416486,"µ2g
+ 6µ2L2
J M 2d(p + 3)3"
REFERENCES,0.5607375271149675,+ 48M 2(τµg + Lρ)2
REFERENCES,0.561822125813449,"µ4g
(1 −αµg)N−1D2,
(33)"
REFERENCES,0.5629067245119306,"where the expectation E[·] is conditioned on xk and yN
k ."
REFERENCES,0.5639913232104121,"Proof of Proposition 2. Based on the deﬁnitions of ∇Φ(xk) and b∇Φ(xk) and conditioning on xk
and yN
k , we have"
REFERENCES,0.5650759219088937,"E
b∇Φ(xk) −∇Φ(xk)
2"
REFERENCES,0.5661605206073753,"≤2
∇xf(xk, yN
k ) −∇xf(xk, y∗
k)
2 + 2E
 ˆ
J ⊤
N ∇yf(xk, yN
k ) −J ⊤
∗∇yf(xk, y∗
k)
2"
REFERENCES,0.5672451193058569,"≤2L2yN
k −y∗
k
2 + 4
J∗
2
F
∇yf(xk, yN
k ) −∇yf(xk, y∗
k)
2"
REFERENCES,0.5683297180043384,"+ 4E
 ˆ
JN −J∗
2
F
∇yf(xk, yN
k )
2"
REFERENCES,0.56941431670282,"(i)
≤2L2D2(1 −αµg)N + 4L4 µ2g"
REFERENCES,0.5704989154013015,"yN
k −y∗
k
2 + 4M 2E
 ˆ
JN −J∗
2
F"
REFERENCES,0.571583514099783,"(ii)
≤2L2D2(1 −αµg)N + 4L4"
REFERENCES,0.5726681127982647,"µ2g
D2(1 −αµg)N + 4M 2E
 ˆ
JN −J∗
2
F
(34)"
REFERENCES,0.5737527114967462,"where (i) follows from Lemma 4 and Assumption 3, and (i) and (ii) also use the following result
for full GD (when applied to a strongly-convex function).
yN
k −y∗
k
2 ≤(1 −αµg)ND2."
REFERENCES,0.5748373101952278,"Next, we upper-bound the last term E
 ˆ
JN −J∗
2
F at the last line of eq. (34). First note that"
REFERENCES,0.5759219088937093,"E
 ˆ
JN −J∗
2
F ≤3E
 ˆ
JN −Jµ
2
F + 3
JN −J∗
2
F + 3
Jµ −JN
2
F .
(35)"
REFERENCES,0.5770065075921909,Under review as a conference paper at ICLR 2022
REFERENCES,0.5780911062906724,"We then upper-bound each term of the right hand side of eq. (35). For the ﬁrst term, we have"
REFERENCES,0.579175704989154,"E
 ˆ
JN −Jµ
2
F =E
 1 Q Q
X"
REFERENCES,0.5802603036876356,"j=1
ˆ
JN,j −Jµ
2
F = 1"
REFERENCES,0.5813449023861171,"Q2 E

Q
X j=1"
REFERENCES,0.5824295010845987,"
ˆ
JN,j −Jµ
 2
F = 1 Q2 E   Q
X j=1"
REFERENCES,0.5835140997830802,"ˆ
JN,j −Jµ
2
F + 2
X i<j"
REFERENCES,0.5845986984815619,"D
ˆ
JN,i −Jµ, ˆ
JN,j −Jµ
E
  = 1 Q2 Q
X"
REFERENCES,0.5856832971800434,"j=1
E
 ˆ
JN,j −Jµ
2
F = 1"
REFERENCES,0.586767895878525,"QE
 ˆ
JN,j −Jµ
2
F ,
j ∈{1, . . . , Q}.
(36)"
REFERENCES,0.5878524945770065,"We next upper-bound the term E
 ˆ
JN,j −Jµ
2
F in eq. (36)."
REFERENCES,0.588937093275488,"E
 ˆ
JN,j −Jµ
2
F =E
 ˆ
JN,j
2
F −
Jµ
2
F
(37) (i)
≤ d
X i=1"
REFERENCES,0.5900216919739696,"
4(p + 4)
∇yN
i,µ
2 + 3"
REFERENCES,0.5911062906724512,"2µ2L2
J (p + 5)3

− d
X i=1"
REFERENCES,0.5921908893709328,"∇yN
i,µ
2 ≤ d
X i=1"
REFERENCES,0.5932754880694143,"
(4p + 15)
∇yN
i,µ
2 + 3"
REFERENCES,0.5943600867678959,"2µ2L2
J (p + 5)3

,
(38)"
REFERENCES,0.5954446854663774,"where (i) follows by applying Lemma 3 to the components of vector yN(xk) which have Lipschitz
gradients by Proposition 1. Then, noting that
∇yN
i,µ
2 ≤2
∇yN
i
2+ 1"
REFERENCES,0.596529284164859,"2µ2L2
J (p+3)3 and replacing
in eq. (38), we have"
REFERENCES,0.5976138828633406,"E
 ˆ
JN,j −Jµ
2
F ≤ d
X i=1"
REFERENCES,0.5986984815618221,"
2(4p + 15)
∇yN
i
2 + µ2L2
J P4(p)
"
REFERENCES,0.5997830802603037,"≤2(4p + 15)
JN
2
F + µ2L2
J dP4(p)"
REFERENCES,0.6008676789587852,"(i)
≤2(4p + 15)L2"
REFERENCES,0.6019522776572668,"µ2g
+ µ2L2
J dP4(p),
(39)"
REFERENCES,0.6030368763557483,"where (i) follows from Lemma 5 and P4 is a polynomial of degree 4 in p. Combining eq. (36) and
eq. (38) yields"
REFERENCES,0.60412147505423,"E
 ˆ
JN −Jµ
2
F ≤2(4p + 15) L2"
REFERENCES,0.6052060737527115,"Qµ2g
+ µ2"
REFERENCES,0.6062906724511931,"Q L2
J dP4(p).
(40)"
REFERENCES,0.6073752711496746,"We next upper-bound the second term at the right hand side of eq. (35), which can be upper-bounded
using eq. (41) in Ji et al. (2021), as shown below."
REFERENCES,0.6084598698481561,"JN −J∗
2 ≤2L2(1 −αµg)2N"
REFERENCES,0.6095444685466378,"µ2g
+ 4(τµg + Lρ)2"
REFERENCES,0.6106290672451193,"µ4g
(1 −αµg)N−1D2.
(41)"
REFERENCES,0.6117136659436009,We ﬁnally upper-bound the last term at the right hand side of eq. (35) using Lemma 3.
REFERENCES,0.6127982646420824,"Jµ −JN
2
F = d
X i=1"
REFERENCES,0.613882863340564,"∇yN
i,µ −∇yN
i
2 ≤µ2"
REFERENCES,0.6149674620390455,"2 L2
J d(p + 3)3.
(42)"
REFERENCES,0.6160520607375272,Under review as a conference paper at ICLR 2022
REFERENCES,0.6171366594360087,"Substituting eq. (40), eq. (41) and eq. (42) into eq. (35) yields"
REFERENCES,0.6182212581344902,"E
 ˆ
JN −J∗
2
F ≤6(4p + 15) L2"
REFERENCES,0.6193058568329718,"Qµ2g
+ µ2"
REFERENCES,0.6203904555314533,"Q L2
J dP4(p) + 6L2(1 −αµg)2N µ2g"
REFERENCES,0.6214750542299349,+ 12(τµg + Lρ)2
REFERENCES,0.6225596529284165,"µ4g
(1 −αµg)N−1D2 + 3µ2"
REFERENCES,0.6236442516268981,"2 L2
J d(p + 3)3.
(43)"
REFERENCES,0.6247288503253796,"Finally, the bound for the expected estimation error in eq. (34) becomes"
REFERENCES,0.6258134490238612,"E
b∇Φ(xk) −∇Φ(xk)
2 ≤2L2D2(1 −αµg)N + 4L4"
REFERENCES,0.6268980477223427,"µ2g
D2(1 −αµg)N + 24(4p + 15)L2M 2 Qµ2g + µ2"
REFERENCES,0.6279826464208242,"Q L2
J M 2dP4(p) + 24L2M 2(1 −αµg)2N"
REFERENCES,0.6290672451193059,"µ2g
+ 6µ2L2
J M 2d(p + 3)3"
REFERENCES,0.6301518438177874,+ 48M 2(τµg + Lρ)2
REFERENCES,0.631236442516269,"µ4g
(1 −αµg)N−1D2.
(44)"
REFERENCES,0.6323210412147505,This completes the proof.
REFERENCES,0.6334056399132321,"F.2
PROOF OF THEOREM 1"
REFERENCES,0.6344902386117137,"Theorem 1 (Re-stated). Suppose that Assumptions 1, 2, and 3 hold. Choose the inner- and outer-
loop stepsizes respectively as α ≤1"
REFERENCES,0.6355748373101953,"L and β =
1
4LΦ , where LΦ = L + 2L2+τM 2"
REFERENCES,0.6366594360086768,"µg
+ ρLM+L3+τML"
REFERENCES,0.6377440347071583,"µ2
g
+ ρL2M"
REFERENCES,0.6388286334056399,"µ3
g
. Then, the iterates xk for k = 0, ..., K −1 of ESJ in Algorithm 1 satisfy:"
K,0.6399132321041214,"1
K K−1
X"
K,0.6409978308026031,"k=0
E
∇Φ(xk)
2 ≤16Lφ(Φ(x0) −Φ∗)"
K,0.6420824295010846,"K
+ 3D
(45)"
K,0.6431670281995662,with Φ∗= infx Φ(x) and D is the upper-bound established in Proposition 2 and is given by
K,0.6442516268980477,D =2L2D2(1 −αµg)N + 4L4
K,0.6453362255965293,"µ2g
D2(1 −αµg)N + 24(4p + 15)L2M 2 Qµ2g + µ2"
K,0.6464208242950108,"Q L2
J M 2dP4(p) + 24L2M 2(1 −αµg)2N"
K,0.6475054229934925,"µ2g
+ 6µ2L2
J M 2d(p + 3)3"
K,0.648590021691974,+ 48M 2(τµg + Lρ)2
K,0.6496746203904555,"µ4g
(1 −αµg)N−1D2.
(46)"
K,0.6507592190889371,"Proof of Theorem 1. Using the Lipschitzness of function Φ(xk), we have"
K,0.6518438177874186,"Φ(xk+1) ≤Φ(xk) + ⟨∇Φ(xk), xk+1 −xk⟩+ Lφ 2"
K,0.6529284164859002,"xk+1 −xk
2"
K,0.6540130151843818,"≤Φ(xk) −β⟨∇Φ(xk), b∇Φ(xk)⟩+ Lφ"
K,0.6550976138828634,"2 β2b∇Φ(xk)
2"
K,0.6561822125813449,"≤Φ(xk) −β⟨∇Φ(xk), b∇Φ(xk) −∇Φ(xk)⟩−β
∇Φ(xk)
2"
K,0.6572668112798264,"+ Lφβ2 ∇Φ(xk)
2 +
b∇Φ(xk) −∇Φ(xk)
2"
K,0.658351409978308,≤Φ(xk) −(β
K,0.6594360086767896,"2 −Lφβ2)
∇Φ(xk)
2 + (β"
K,0.6605206073752712,"2 + Lφβ2)
b∇Φ(xk) −∇Φ(xk)
2.
(47)"
K,0.6616052060737527,"Let Ek[·] = Euk,1:Q[·|xk, yN
k ] be the expectation over the Gaussian vectors uk,1, . . . , uk,Q condi-
tioned on xk and yN
k . Applying the expectation Ek[·] to eq. (47) yields"
K,0.6626898047722343,EkΦ(xk+1) ≤Φ(xk) −(β
K,0.6637744034707158,"2 −Lφβ2)
∇Φ(xk)
2 + (β"
K,0.6648590021691974,"2 + Lφβ2)Ek
b∇Φ(xk) −∇Φ(xk)
2"
K,0.665943600867679,≤Φ(xk) −(β
K,0.6670281995661606,"2 −Lφβ2)
∇Φ(xk)
2 + (β"
K,0.6681127982646421,"2 + Lφβ2)D,
(48)"
K,0.6691973969631236,Under review as a conference paper at ICLR 2022
K,0.6702819956616052,"where D represent the upper-bound established for the estimation error in Proposition 2. Now taking
total expectation over Uk = {u1,1:Q, . . . , uk,1:Q}, we have"
K,0.6713665943600867,Ek+1 ≤Ek −(β
K,0.6724511930585684,"2 −Lφβ2)EUk
∇Φ(xk)
2 + (β"
K,0.6735357917570499,"2 + Lφβ2)D
(49)"
K,0.6746203904555315,"where Ek = EUk−1Φ(xk). Summing up the inequalities in eq. (49) for k = 0, . . . , K −1 yields"
K,0.675704989154013,Ek ≤E0 −(β
K,0.6767895878524945,"2 −Lφβ2) K−1
X"
K,0.6778741865509761,"k=0
EUk
∇Φ(xk)
2 + (β"
K,0.6789587852494577,"2 + Lφβ2)KD.
(50)"
K,0.6800433839479393,"Setting β =
1
4Lφ , denoting by Φ∗= infx Φ(x), and rearranging eq. (50), we have"
K,0.6811279826464208,"1
K K−1
X"
K,0.6822125813449024,"k=0
EUk
∇Φ(xk)
2 ≤16Lφ(Φ(x0) −Φ∗)"
K,0.6832971800433839,"K
+ 3D.
(51)"
K,0.6843817787418656,"Hence, the proof is ﬁnished."
K,0.6854663774403471,"G
PROOFS FOR STOCHASTIC BILEVEL OPTIMIZATION"
K,0.6865509761388287,Deﬁne the following quantities
K,0.6876355748373102,"ˆ
JN,j = ˆ
JN (xk, uj) =  

"
K,0.6887201735357917,"Y N
1 (xk+µuj;S)−Y N
1 (xk;S)
µ
u⊤
j
..."
K,0.6898047722342733,"Y N
d (xk+µuj;S)−Y N
d (xk;S)
µ
u⊤
j "
K,0.6908893709327549,"

,
JN = ∂Y N
k
∂xk
,
J∗= ∂y∗
k
∂xk"
K,0.6919739696312365,"where uj ∈Rp, j = 1, . . . , Q are standard Gaussian vectors and Y N
k
is the output of SGD obtained
with the minibatches {S0, ..., SN−1}."
K,0.693058568329718,"Conditioning on xk and Y N
k
and taking expectation over uj yields"
K,0.6941431670281996,"Euj ˆ
JN,j = Euj  

"
K,0.6952277657266811,"Y N
1 (xk+µuj;S)−Y N
1 (xk;S)
µ
u⊤
j
..."
K,0.6963123644251626,"Y N
d (xk+µuj;S)−Y N
d (xk;S)
µ
u⊤
j  

 =  

"
K,0.6973969631236443,"∇⊤
x Y N
1,µ(xk; S)
..."
K,0.6984815618221258,"∇⊤
x Y N
d,µ(xk; S)  

"
K,0.6995661605206074,= Jµ(S)
K,0.7006507592190889,"where Y N
i,µ(xk; S) is the i-th component of vector Y N
µ (xk; S), which is the entry-wise Gaussian
smooth approximation of vector Y N(xk; S). Let Ek[·] = E[·|xk, Y N
k ] = EDF ,u1:q be the expecta-
tion over the Gaussian vectors and the sample minibatch DF conditioned on xk and Y N
k ."
K,0.7017353579175705,"G.1
PROOF OF PROPOSITION 3"
K,0.702819956616052,"Proposition 3 (Re-stated). Suppose that Assumptions 1, 2, and 4 hold. Choose the inner-loop
stepsize as α =
2
L+µg . Deﬁne the constants"
K,0.7039045553145337,"Cγ =(1 −αµg)

1 −αµg + α"
K,0.7049891540130152,γ + αL γµg
K,0.7060737527114967,"
, Cxy = α

α + γ(1 −αµg) + α L µg"
K,0.7071583514099783,"
, Cy = L"
K,0.7082429501084598,"µg
Cxy"
K,0.7093275488069414,"Γ =2(τ 2Cxy + ρ2Cy)
σ2"
K,0.710412147505423,µgLS + 2L2
K,0.7114967462039046,"S (Cxy + Cy),
λ = 2(τ 2Cxy + ρ2Cy)D2,
(52)"
K,0.7125813449023861,Under review as a conference paper at ICLR 2022
K,0.7136659436008677,where γ is such that γ ≥L+µg
K,0.7147505422993492,"µ2g . Then, we have:"
K,0.7158351409978309,"E
JN −J∗
2
F ≤CN
γ
L2"
K,0.7169197396963124,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ
."
K,0.7180043383947939,"Proof of Proposition 3. Based on the SGD updates, we have"
K,0.7190889370932755,"Y t
k = Y t−1
k
−α∇yG
 
xk, Y t−1
k
; St−1

,
t = 1, . . . , N"
K,0.720173535791757,Taking the derivatives w.r.t. xk yields
K,0.7212581344902386,"Jt =Jt−1 −α∇x∇yG
 
xk, Y t−1
k
; St−1

−αJt−1∇2
yG
 
xk, Y t−1
k
; St−1
"
K,0.7223427331887202,"Jt −J∗=Jt−1 −J∗−α∇x∇yG
 
xk, Y t−1
k
; St−1

−αJt−1∇2
yG
 
xk, Y t−1
k
; St−1
"
K,0.7234273318872018,"+ α
 
∇x∇yg (xk, y∗
k) + J∗∇2
yg (xk, y∗
k)
"
K,0.7245119305856833,"=Jt−1 −J∗−α
 
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
"
K,0.7255965292841648,"−α (Jt−1 −J∗) ∇2
yG
 
xk, Y t−1
k
; St−1
"
K,0.7266811279826464,"+ αJ∗
 
∇2
yg (xk, y∗
k) −∇2
yG
 
xk, Y t−1
k
; St−1

."
K,0.7277657266811279,"Hence, using the triangle inequality, we have"
K,0.7288503253796096,"Jt −J∗

F"
K,0.7299349240780911,"(i)
≤
 (Jt−1 −J∗)
 
I −∇2
yG
 
xk, Y t−1
k
; St−1
 
F
+ α
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)

F
+ α
J∗
 
∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
 
F ,"
K,0.7310195227765727,"where (i) follows from Assumption 1. We then further have
Jt −J∗
2
F ≤"
K,0.7321041214750542,"(1 −αµg)2Jt−1 −J∗
2
F + α2∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F"
K,0.7331887201735358,+ α2 L2 µ2g
K,0.7342733188720173,"∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F"
K,0.735357917570499,"+ 2α(1 −αµg)
Jt−1 −J∗

F
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)

F
|
{z
}
P1"
K,0.7364425162689805,+ 2α(1 −αµg) L µg
K,0.737527114967462,"Jt−1 −J∗

F
∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)

F
|
{z
}
P2"
K,0.7386117136659436,+2α2 L µg
K,0.7396963123644251,"∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)

F
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)

F
|
{z
}
P3 ."
K,0.7407809110629068,"The terms P1, P2 and P3 in the above inequality can be transformed as follows using the Peter-Paul
version of Young’s inequality. P1 ≤1 2γ"
K,0.7418655097613883,"Jt−1 −J∗
2
F + γ 2"
K,0.7429501084598699,"∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F ,
γ > 0 P2 ≤1 2γ"
K,0.7440347071583514,"Jt−1 −J∗
2
F + γ 2"
K,0.745119305856833,"∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F ,
γ > 0 P3 ≤1 2"
K,0.7462039045553145,"∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F + 1 2"
K,0.7472885032537961,"∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F"
K,0.7483731019522777,"Note that the trade-off constant γ controls the contraction coefﬁcient (factor in front of
Jt−1 −"
K,0.7494577006507592,"J∗
2
F ). Hence, we have
Jt −J∗
2
F"
K,0.7505422993492408,Under review as a conference paper at ICLR 2022
K,0.7516268980477223,"≤

(1 −αµg)2 + α"
K,0.7527114967462039,γ (1 −αµg) + αL
K,0.7537960954446855,"γµg
(1 −αµg)
 Jt−1 −J∗
2
F"
K,0.754880694143167,"+

α2 + αγ(1 −αµg) + α2 L µg"
K,0.7559652928416486," ∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F"
K,0.7570498915401301,"+

α2 L2"
K,0.7581344902386117,"µ2g
+ αγ L"
K,0.7592190889370932,"µg
(1 −αµg) + α2 L µg"
K,0.7603036876355749," ∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F ."
K,0.7613882863340564,"Let Et−1[·] = E[·|xk, Y t−1
k
]. Conditioning on xk and Y t−1
k
and taking expectations yield"
K,0.762472885032538,"Et−1
Jt −J∗
2
F ≤"
K,0.7635574837310195,"Cγ
Jt−1 −J∗
2
F + CxyEt−1
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F
+ CyEt−1
∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F ,
(53)"
K,0.764642082429501,"where Cγ, Cxy and Cy are deﬁned as follows"
K,0.7657266811279827,"Cγ = (1 −αµg)

1 −αµg + α"
K,0.7668112798264642,γ + αL γµg
K,0.7678958785249458,"
, Cxy = α

α + γ(1 −αµg) + α L µg"
K,0.7689804772234273,"
, Cy = L"
K,0.7700650759219089,"µg
Cxy."
K,0.7711496746203904,"Conditioning on xk and Y t−1
k
, we have"
K,0.7722342733188721,"Et−1
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg (xk, y∗
k)
2
F
≤2Et−1
∇x∇yg
 
xk, Y t−1
k

−∇x∇yg (xk, y∗
k)
2
F
+ 2Et−1
∇x∇yG
 
xk, Y t−1
k
; St−1

−∇x∇yg
 
xk, Y t−1
k
 2
F
(i)
≤2L2"
K,0.7733188720173536,"S + 2τ 2Y t−1
k
−y∗
k
2,
(54)"
K,0.7744034707158352,where (i) follows from Lemma 1 and Assumption 2. Similarly we can derive
K,0.7754880694143167,"Et−1
∇2
yG
 
xk, Y t−1
k
; St−1

−∇2
yg (xk, y∗
k)
2
F ≤2L2"
K,0.7765726681127982,"S + 2ρ2Y t−1
k
−y∗
k
2.
(55)"
K,0.7776572668112798,"Combining eq. (53), eq. (54), and eq. (55) we obtain"
K,0.7787418655097614,"Et−1
Jt −J∗
2
F ≤Cγ
Jt−1 −J∗
2
F + 2(τ 2Cxy + ρ2Cy)
Y t−1
k
−y∗
k
2 + 2L2"
K,0.779826464208243,"S (Cxy + Cy).
(56)"
K,0.7809110629067245,"Unconditioning on xk and Y t−1
k
and taking total expectations of eq. (56) yield"
K,0.7819956616052061,"E
Jt −J∗
2
F ≤CγE
Jt−1 −J∗
2
F + 2(τ 2Cxy + ρ2Cy)E
Y t−1
k
−y∗
k
2 + 2L2"
K,0.7830802603036876,S (Cxy + Cy)
K,0.7841648590021691,"(i)
≤CγE
Jt−1 −J∗
2
F + 2(τ 2Cxy + ρ2Cy)"
K,0.7852494577006508,L −µg
K,0.7863340563991323,L + µg
K,0.7874186550976139,"2(t−1)
D2 +
σ2 µgLS ! + 2L2"
K,0.7885032537960954,"S (Cxy + Cy),"
K,0.789587852494577,"where (i) follows from the analysis of SGD for a strongly-convex function. Let Γ = 2(τ 2Cxy +
ρ2Cy)
σ2
µgLS + 2 L2"
K,0.7906724511930586,"S (Cxy + Cy) and λ = 2(τ 2Cxy + ρ2Cy)D2. Then, we have"
K,0.7917570498915402,"E
Jt −J∗
2
F ≤CγE
Jt−1 −J∗
2
F + λ
L −µg"
K,0.7928416485900217,L + µg
K,0.7939262472885033,"2(t−1)
+ Γ.
(57)"
K,0.7950108459869848,Telescoping eq. (57) over t from N down to 1 yields
K,0.7960954446854663,"E
JN −J∗
2
F ≤CN
γ E
J0 −J∗
2
F + λ N−1
X t=0"
K,0.797180043383948,L −µg
K,0.7982646420824295,L + µg
K,0.7993492407809111,"2t
CN−1−t
γ
+ Γ N−1
X"
K,0.8004338394793926,"t=0
Ct
γ
(58)"
K,0.8015184381778742,Under review as a conference paper at ICLR 2022
K,0.8026030368763557,"which, in conjunction with

L−µg
L+µg"
K,0.8036876355748374,"2
≤1 −αµg and γ ≥L+µg"
K,0.8047722342733189,"µ2g
such that Cγ ≤1 −αµg, yields"
K,0.8058568329718004,"E
JN −J∗
2
F ≤CN
γ
L2"
K,0.806941431670282,"µ2g
+ λCN−1
γ N−1
X t=0"
K,0.8080260303687635,"
(L −µg)2"
K,0.8091106290672451,(L + µg)2(1 −αµg)
K,0.8101952277657267,"t
+
Γ
1 −Cγ"
K,0.8112798264642083,"≤CN
γ
L2"
K,0.8123644251626898,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ
.
(59)"
K,0.8134490238611713,The proof is then complete.
K,0.8145336225596529,"Lemma 8. Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner-loop stepsize as α =
2
L+µg .
Then, we have"
K,0.8156182212581344,"E
Ek b∇Φ(xk) −∇Φ(xk)
2 ≤8M 2"
K,0.8167028199566161,"CN
γ
L2"
K,0.8177874186550976,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ
+ µ2"
K,0.8188720173535792,"2 L2
J d(p + 3)3
!"
K,0.8199566160520607,"+ 2L2

1 + 2L2 µ2g"
K,0.8210412147505423,  L −µg
K,0.8221258134490239,L + µg
K,0.8232104121475055,"2N
D2 +
σ2 µgLS !"
K,0.824295010845987,",
(60)"
K,0.8253796095444685,"where the expectation Ek[·] is conditioned on xk and Y N
k ."
K,0.8264642082429501,"Proof of Lemma 8. Conditioning on xk and Y N
k , we have"
K,0.8275488069414316,"Ek b∇Φ(xk) = ∇xf(xk, Y N
k ) + J ⊤
µ ∇yf(xk, Y N
k )."
K,0.8286334056399133,"Recall ∇Φ(xk) = ∇xf(xk, y∗
k) + J ⊤
∗∇yf(xk, y∗
k). Thus, we have
Ek b∇Φ(xk)−∇Φ(xk)
2"
K,0.8297180043383948,"≤2
∇xf(xk, Y N
k ) −∇xf(xk, y∗
k)
2 + 2
J ⊤
µ ∇yf(xk, Y N
k ) −J ⊤
∗∇yf(xk, y∗
k)
2"
K,0.8308026030368764,"≤2L2Y N
k −y∗
k
2 + 4
J ⊤
µ ∇yf(xk, Y N
k ) −J ⊤
∗∇yf(xk, Y N
k )
2"
K,0.8318872017353579,"+ 4
J ⊤
∗∇yf(xk, Y N
k ) −J ⊤
∗∇yf(xk, y∗
k)
2"
K,0.8329718004338394,"(i)
≤2L2Y N
k −y∗
k
2 + 4M 2Jµ −J∗
2
F + 4L2 µ2g"
K,0.834056399132321,"∇yf(xk, Y N
k ) −∇yf(xk, y∗
k)
2"
K,0.8351409978308026,"≤2L2Y N
k −y∗
k
2 + 8M 2Jµ −JN
2
F + 8M 2JN −J∗
2
F + 4L4 µ2g"
K,0.8362255965292842,"Y N
k −y∗
k
2."
K,0.8373101952277657,where (i) applies Lemma 4 and Assumption 3. Taking expectation of the above inequality yields
K,0.8383947939262473,"E
Ek b∇Φ(xk) −∇Φ(xk)
2 ≤2L2

1 + 2L2 µ2g"
K,0.8394793926247288,"
E
Y N
k −y∗
k
2 + 8M 2E
JN −J∗
2
F"
K,0.8405639913232104,"+ 8M 2E
Jµ −JN
2
F .
(61)"
K,0.841648590021692,"Using the fact that Y N
i (xk; ·) has Lipschitz gradient (Proposition 1) and Lemma 3, the last term at
the right hand side of eq. (61) can be directly upper-bounded as"
K,0.8427331887201736,"Jµ −JN
2
F = d
X i=1"
K,0.8438177874186551,"∇Y N
i,µ(xk; S) −∇Y N
i (xk; S)
2 ≤µ2"
K,0.8449023861171366,"2 L2
J d(p + 3)3,
(62)"
K,0.8459869848156182,"where LJ is the Lipschitz constant of the Jacobian JN (and also of its rows ∇Y N
i (xk; S)) as deﬁned
in Proposition 1. Combining eq. (61), eq. (62), Proposition 3, and SGD analysis (as in eq. (60) in Ji
et al. (2021)) yields"
K,0.8470715835140998,"E
Ek b∇Φ(xk) −∇Φ(xk)
2"
K,0.8481561822125814,Under review as a conference paper at ICLR 2022
K,0.8492407809110629,"≤8M 2

CN
γ
L2"
K,0.8503253796095445,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ
+ µ2"
K,0.851409978308026,"2 L2
J d(p + 3)3
"
K,0.8524945770065075,"+ 2L2

1 + 2L2 µ2g"
K,0.8535791757049892,L −µg
K,0.8546637744034707,L + µg
K,0.8557483731019523,"2N
D2 +
σ2 µgLS"
K,0.8568329718004338,"
.
(63)"
K,0.8579175704989154,This ﬁnishes the proof.
K,0.8590021691973969,"G.2
PROOF OF PROPOSITION 4"
K,0.8600867678958786,"Proposition 4 (Re-stated). Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner-loop stepsize
as α =
2
L+µg . Then, we have:"
K,0.8611713665943601,"E
b∇Φ(xk) −∇Φ(xk)
2 ≤∆+ B1
(64)"
K,0.8622559652928417,"where ∆= 8M 2 
1 +
1
Df"
K,0.8633405639913232,"
4p+15"
K,0.8644251626898047,"Q
+
1
Df 
L2"
K,0.8655097613882863,µ2g +2 M2
K,0.8665943600867679,"Df +

1 +
1
Df 
4M2"
K,0.8676789587852495,"Q µ2dL2
J P4(p)+ 4M2"
K,0.868763557483731,"Df µ2dL2
J P3(p)"
K,0.8698481561822126,and B1 respresents the upper bound established in Lemma 8.
K,0.8709327548806941,"Proof of Proposition 4. We have, conditioning on xk and Y N
k"
K,0.8720173535791758,"Ek
b∇Φ(xk) −∇Φ(xk)
2 =Ek
b∇Φ(xk) −Ek b∇Φ(xk)
2 +
Ek b∇Φ(xk) −∇Φ(xk)
2.
(65)"
K,0.8731019522776573,Our next step is to upper-bound the ﬁrst term in eq. (65).
K,0.8741865509761388,"Ek
b∇Φ(xk)−Ek b∇Φ(xk)
2"
K,0.8752711496746204,"≤2Ek
∇xF(xk, Y N
k ; DF ) −∇xf(xk, Y N
k )
2"
K,0.8763557483731019,"+ 2Ek
 ˆ
J ⊤
N ∇yF(xk, Y N
k ; DF ) −J ⊤
µ ∇yf(xk, Y N
k )
2 ≤2M 2"
K,0.8774403470715835,"Df
+ 4Ek
∇yF(xk, Y N
k ; DF )
2 ˆ
JN −Jµ
2
F"
K,0.8785249457700651,"+ 4Ek
Jµ
2
F
∇yF(xk, Y N
k ; DF ) −∇yf(xk, Y N
k )
2 ≤2M 2"
K,0.8796095444685467,"Df
+ 4M 2

1 + 1 Df"
K,0.8806941431670282,"
Ek
 ˆ
JN −Jµ
2
F + 4M 2 Df"
K,0.8817787418655098,"Jµ
2
F ,
(66)"
K,0.8828633405639913,where the last two steps follows from Lemma 1.
K,0.8839479392624728,"Next, we upper-bound the term Ek
 ˆ
JN −Jµ
2
F ."
K,0.8850325379609545,"Ek
 ˆ
JN −Jµ
2
F = 1"
K,0.886117136659436,"QEk
 ˆ
JN,j −Jµ
2
F ,
j ∈{1, . . . , Q} ≤1 Q"
K,0.8872017353579176,"
Ek
 ˆ
JN,j
2
F −
Jµ
2
F  ≤1 Q d
X i=1"
K,0.8882863340563991,"
Ek
Y N
i (xk + µuj; S) −Y N
i (xk; S)
µ
uj

2
−
∇Y N
i,µ(xk; S)
2

. (67)"
K,0.8893709327548807,"Recall that for a function h with L-Lipschitz gradient, we have"
K,0.8904555314533622,"Eu
h(x + µu) −h(x)"
K,0.8915401301518439,"µ
u

2
≤4(p + 4)
∇hµ(x)
2 + 3"
K,0.8926247288503254,"2µ2L2(p + 5)3.
(68)"
K,0.8937093275488069,"Then, applying eq. (68) to function Y N
i (·; S) yields"
K,0.8947939262472885,"Euj
Y N
i (xk + µuj; S) −Y N
i (xk; S)
µ
uj

2"
K,0.89587852494577,"≤4(p + 4)
∇Y N
i,µ(xk; S)
2 + 3"
K,0.8969631236442517,"2µ2L2
J (p + 5)3."
K,0.8980477223427332,Under review as a conference paper at ICLR 2022
K,0.8991323210412148,"Hence, eq. (67) becomes"
K,0.9002169197396963,"Ek
 ˆ
JN −Jµ
2 ≤4p + 15 Q d
X i=1"
K,0.9013015184381779,"∇Y N
i,µ(xk; S)
2 + 3µ2dL2
J
2Q
(p + 5)3"
K,0.9023861171366594,"≤2(4p + 15) Q d
X i=1"
K,0.903470715835141,"∇Y N
i (xk; S)
2 + µ2dL2
J
Q
P4(p)"
K,0.9045553145336226,≤2(4p + 15) Q
K,0.9056399132321041,"JN
2
F + µ2dL2
J
Q
P4(p),
(69)"
K,0.9067245119305857,"where P4 is a polynomial of degree 4 in p. Combining eq. (66), eq. (69) and Lemma 3 yields"
K,0.9078091106290672,"Ek
b∇Φ(xk) −Ek b∇Φ(xk)
2 ≤2M 2"
K,0.9088937093275488,"Df
+ 4M 2
1 + 1 Df"
K,0.9099783080260304,2(4p + 15) Q
K,0.911062906724512,"JN
2
F + µ2dL2
J
Q
P4(p)
"
K,0.9121475054229935,+ 4M 2 Df
K,0.913232104121475,"
2
JN
2
F + µ2dL2
J P3(p)
"
K,0.9143167028199566,"≤8M 2
1 + 1 Df"
K,0.9154013015184381,"4p + 15 Q
+ 1 Df L2"
K,0.9164859002169198,"µ2g
+ 2M 2 Df"
K,0.9175704989154013,"+

1 + 1 Df 4M 2"
K,0.9186550976138829,"Q µ2dL2
J P4(p) + 4M 2"
K,0.9197396963123644,"Df
µ2dL2
J P3(p)"
K,0.920824295010846,"= ∆,
(70)"
K,0.9219088937093276,"where
∆
=
8M 2 
1 +
1
Df"
K,0.9229934924078091,"
4p+15"
K,0.9240780911062907,"Q
+
1
Df 
L2"
K,0.9251626898047722,"µ2g
+ 2 M2"
K,0.9262472885032538,"Df
+

1 +
1
Df 
4M2"
K,0.9273318872017353,"Q µ2dL2
J P4(p) + 4M2"
K,0.928416485900217,"Df µ2dL2
J P3(p)."
K,0.9295010845986985,"Taking total expectations of both eq. (65) and eq. (70) and combining, we have"
K,0.93058568329718,"E
b∇Φ(xk) −∇Φ(xk)
2 =E
b∇Φ(xk) −Ek b∇Φ(xk)
2 + E
Ek b∇Φ(xk) −∇Φ(xk)
2"
K,0.9316702819956616,"≤∆+ B1
(71)"
K,0.9327548806941431,where B1 representing the upper-bound established in eq. (63). The proof is then complete.
K,0.9338394793926247,"G.3
PROOF OF THEOREM 2"
K,0.9349240780911063,"Theorem 2 (Re-stated). Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner- and outer-loop
stepsizes respectivelly as α =
2
L+µg and β =
1
4LΦ , where L = max{Lf, Lg} and the constant LΦ
is deﬁned as in Theorem 1. Then, the iterates xk, k = 0, ..., K −1 of the ESJ-S algorithm satisfy"
K,0.9360086767895879,"1
K
PK−1
k=0 E
∇Φ(xk)
2 ≤16(Φ(x0)−Φ∗)LΦ"
K,0.9370932754880694,"K
+ 3B1 + ∆,
(72)"
K,0.938177874186551,where ∆and B1 have the following forms
K,0.9392624728850325,"∆=8M 2

1 + 1 Df"
K,0.940347071583514," 4p + 15 Q
+ 1 Df  L2"
K,0.9414316702819957,"µ2g
+ 2M 2"
K,0.9425162689804772,"Df +

1 + 1 Df"
K,0.9436008676789588, 4M 2
K,0.9446854663774403,"Q µ2dL2
J P4(p)"
K,0.9457700650759219,+ 4M 2
K,0.9468546637744034,"Df µ2dL2
J P3(p)"
K,0.9479392624728851,B1 =8M 2
K,0.9490238611713666,"CN
γ
L2"
K,0.9501084598698482,"µ2g
+
λ(L + µg)2(1 −αµg)CN−1
γ
(L + µg)2(1 −αµg) −(L −µg)2 +
Γ
1 −Cγ + µ2"
K,0.9511930585683297,"2 L2
J d(p + 3)3
!"
K,0.9522776572668112,"+ 2L2

1 + 2L2 µ2g"
K,0.9533622559652929,  L −µg
K,0.9544468546637744,L + µg
K,0.955531453362256,"2N
D2 +
σ2 µgLS !"
K,0.9566160520607375,"and the constants Γ, λ, and Cγ are deﬁned in Proposition 3."
K,0.9577006507592191,"Proof of Theorem 2. Using the Lipschitzness of function Φ(xk), we have"
K,0.9587852494577006,"Φ(xk+1) ≤Φ(xk) + ⟨∇Φ(xk), xk+1 −xk⟩+ Lφ 2"
K,0.9598698481561823,"xk+1 −xk
2"
K,0.9609544468546638,Under review as a conference paper at ICLR 2022
K,0.9620390455531453,"≤Φ(xk) −β⟨∇Φ(xk), b∇Φ(xk)⟩+ Lφ"
K,0.9631236442516269,"2 β2b∇Φ(xk) −∇Φ(xk) + ∇Φ(xk)
2"
K,0.9642082429501084,"≤Φ(xk) −β⟨∇Φ(xk), b∇Φ(xk)⟩+ Lφβ2 ∇Φ(xk)
2 +
b∇Φ(xk) −∇Φ(xk)
2"
K,0.96529284164859,"Hence, taking expectation over the above inequality yields"
K,0.9663774403470716,"EΦ(xk+1) ≤EΦ(xk) −βE⟨∇Φ(xk), b∇Φ(xk)⟩+ Lφβ2E
∇Φ(xk)
2"
K,0.9674620390455532,"+ Lφβ2E
b∇Φ(xk) −∇Φ(xk)
2.
(73)"
K,0.9685466377440347,"Also, we have"
K,0.9696312364425163,"−E⟨∇Φ(xk), b∇Φ(xk)⟩= −E⟨∇Φ(xk), b∇Φ(xk) −∇Φ(xk)⟩−E
∇Φ(xk)
2"
K,0.9707158351409978,"=E
h
−⟨∇Φ(xk), Ek b∇Φ(xk) −∇Φ(xk)⟩
i
−E
∇Φ(xk)
2 ≤E
1 2"
K,0.9718004338394793,"∇Φ(xk)
2 + 1 2"
K,0.972885032537961,"Ek b∇Φ(xk) −∇Φ(xk)
2

−E
∇Φ(xk)
2 =1"
E,0.9739696312364425,"2E
Ek b∇Φ(xk) −∇Φ(xk)
2 −1"
E,0.9750542299349241,"2E
∇Φ(xk)
2,"
E,0.9761388286334056,"which, in conjunction with eq. (73), yields"
E,0.9772234273318872,EΦ(xk+1) ≤EΦ(xk) + β
E,0.9783080260303688,"2 E
Ek b∇Φ(xk) −∇Φ(xk)
2 −
β"
E,0.9793926247288504,"2 −Lφβ2

E
∇Φ(xk)
2"
E,0.9804772234273319,"+ Lφβ2E
b∇Φ(xk) −∇Φ(xk)
2.
(74)"
E,0.9815618221258134,"Setting β =
1
4LΦ and using the bounds established in Lemma 8 and Proposition 4, we have"
E,0.982646420824295,EΦ(xk+1) ≤EΦ(xk) + β
E,0.9837310195227765,2 B1 −β
E,0.9848156182212582,"4 E
∇Φ(xk)
2 + β"
E,0.9859002169197397,4 (B1 + ∆).
E,0.9869848156182213,Summing up the above inequality over k from k = 0 to k = K −1 yields
E,0.9880694143167028,EΦ(xK) ≤EΦ(x0) + β
E,0.9891540130151844,"2 KB1 −β 4 K−1
X"
E,0.9902386117136659,"k=0
E
∇Φ(xk)
2 + β"
E,0.9913232104121475,4 K(B1 + ∆).
E,0.9924078091106291,Rearranging the above inequality yields
K,0.9934924078091106,"1
K K−1
X"
K,0.9945770065075922,"k=0
E
∇Φ(xk)
2 ≤4 (EΦ(x0) −EΦ(xK))"
K,0.9956616052060737,"βK
+ 3B1 + ∆"
K,0.9967462039045553,≤16 (Φ(x0) −Φ∗) LΦ
K,0.9978308026030369,"K
+ 3B1 + ∆."
K,0.9989154013015185,The proof is then complete.
