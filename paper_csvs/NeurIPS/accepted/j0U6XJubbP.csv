Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004032258064516129,"As a classical generative modeling approach, energy-based models have the natural
advantage of flexibility in the form of the energy function. Recently, energy-based
models have achieved great success in modeling high-dimensional data in computer
vision and natural language processing. In line with these advancements, we build a
multi-purpose energy-based probabilistic model for High Energy Physics events at
the Large Hadron Collider. This framework builds on a powerful generative model
and describes higher-order inter-particle interactions. It suits different encoding
architectures and builds on implicit generation. As for applicative aspects, it can
serve as a powerful parameterized event generator for physics simulation, a generic
anomalous signal detector free from spurious correlations, and an augmented event
classifier for particle identification."
INTRODUCTION,0.008064516129032258,"1
Introduction"
INTRODUCTION,0.012096774193548387,"The Large Hadron Collider (LHC) [1], the most energetic particle collider in human history, collides
highly energetic protons to examine the underlying physics of subatomic particles and extend our
current understanding of the fundamental forces of nature, summarized by the Standard Model of
particle physics. After the great success in observing the Higgs boson [2, 28], the most critical task of
searching for new physics signals remains challenging. High Energy Physics (HEP) events produced
at the LHC have the properties of high dimensionality, high complexity, and enormous data size. To
detect rare signals from enormous background events, physics observables describing these patterns
have been used to identify different types of radiation patterns. However, it’s not possible to utilize
all the information recorded by the detectors with a few expert-designed features. Thus deep neural
classifiers and generative models, which can easily process high-dimensional data meet the needs for
more precise data analysis and signal detection."
INTRODUCTION,0.016129032258064516,"Energy-based Models (EBMs) [38, 3, 46], as a classical generative framework, leverage the energy
function for learning dependencies between input variables. With an energy function E(x) and
constructing the un-normalized probabilities through the exponential ˜p(x) = exp(−E(x)), the
energy model naturally yields a probability distribution. Despite the flexibility in the modeling, the
training of EBMs has been cumbersome and unstable due to the intractable partition function and
the corresponding Monte Carlo sampling involved. More recently, EBMs have been succeeding in
high-dimensional modeling [55, 56, 25, 23, 62, 19, 51] for computer vision and natural language
processing. At the same time, it has been revealed that neural classifiers are naturally connected with
EBMs [67, 33, 32], combining the discriminative and generative learning processes in a common
learning regime. More interestingly, compositionality [30] can be easily incorporated within the
framework of EBMs by simply summing up the energy functions [21, 22]. On the other hand,
statistical physics originally inspired the invention of EBMs. This natural connection in formalism
makes EBMs appealing in modeling physical systems. In physical sciences, EBMs have been used to"
INTRODUCTION,0.020161290322580645,"Topic
Practice"
INTRODUCTION,0.024193548387096774,"Generative modeling
Parameterized event generation
Out-of-distribution detection
Model-independent new physics search
Hybrid modeling
Classifier combined with EBMs
Table 1: Application aspects of Energy-based Models for High Energy Physics."
INTRODUCTION,0.028225806451612902,"simulate condensed-matter systems and protein molecules [57]. They have also been shown great
potential in structure biology [24], in a use case of protein conformation."
INTRODUCTION,0.03225806451612903,"Motivated by the flexibility in the architecture and the compatibility with different tasks, we explore
the potential of EBMs in modeling radiation patterns of elementary particles at high energy. The
energy function is flexible enough to incorporate sophisticated architectures. Thus EBMs provide a
convenient mechanism to simulate complex correlations in input features or high-order interactions
between particles. Aside from image generation, applications for point cloud data [68], graph
neural networks [48] for molecule generation are also explored. In particle physics, we leverage the
self-attention mechanism [6, 65], to mimic the complex interactions between elementary particles."
INTRODUCTION,0.036290322580645164,"As one important practical application, neural net-based unsupervised learning of physics events
[58, 42, 8, 64] have been explored in the usual generative modeling methods including Generative Ad-
versarial Networks (GANs) [31] and Variational Autoencoders (VAEs) [45]. However, GANs employ
separate networks, which need to be carefully tuned, for the generation process. They usually suffer
from unstable training, high computation demands, and mode collapse. In comparison, VAEs need a
well-designed reconstruction loss, which could be difficult for sophisticated network architectures
and complex input features. EBMs thus provide a strong alternative generative modeling framework
of LHC events, by easily incorporating sophisticated physics-inspired neural net architectures."
INTRODUCTION,0.04032258064516129,"At the same time, EBMs can serve as generic signal detectors, since out-of-distribution (OOD)
detection comes naturally in the form of energy comparison. More importantly, EBMs incur fewer
spurious correlations in OOD detection. This plays a slightly different role in the context of signal
searches at the LHC. There are correlations that are real and useful but at the same time hinder
effective signal detection. As we will see in Section 4.2, the EBMs are free from the notorious
correlation observed in many anomaly detection methods in HEP, in both the generative and the
discriminative approaches."
INTRODUCTION,0.04435483870967742,"As summarized in Table 1, we build a multi-tasking framework for High Energy Physics. To that
end, we construct an energy-based model of the fundamental interactions of elementary particles to
simulate the resulting radiation patterns. We especially employ the short-run Markov Chain Monte
Carlo for the EBM training. We show that EBMs are able to generate realistic event patterns and can
be used as generic anomaly detectors free from spurious correlations. We also explore EBM-based
hybrid modeling combining generative and discriminative models for HEP events. This unified
learning framework paves for future-generation event simulators and automated new physics search
strategies. It opens a door for combining different methods and components towards a powerful
multi-tasking engine for scientific discovery at the Large Hadron Collider."
PROBLEM STATEMENT,0.04838709677419355,"2
Problem Statement"
PROBLEM STATEMENT,0.05241935483870968,"Describing HEP Events
Most particle interactions happening at the LHC are governed by Quantum
Chromodynamics (QCD), due to the hadronic nature of the proton-proton collision. Thus jets are
enormously produced by these interactions. A jet is formed by collimated radiations originating
from highly energetic elementary particles (e.g., quarks, gluons, and sometimes highly-boosted
electro-weak bosons). The tracks and energy deposits of the jets left in the particle detectors reveal
the underlying physics in complex patterns. A jet may have hundreds of jet constituents, resulting
in high-dimensional and complex radiation patterns and bringing difficulties in encoding all the
information with expert features. Reconstructing, identifying, and classifying these elementary
particles manifest in raw data is thus critical for ongoing physics analysis at the LHC."
PROBLEM STATEMENT,0.056451612903225805,"Specifically, each constituent particle within a jet has (log pT , η, ϕ) as the descriptive coordinates in
the detector’s reference frame. Here, pT represents the transverse momentum perpendicular to the"
PROBLEM STATEMENT,0.06048387096774194,"beam axis, and (η, ϕ) denotes the spatial coordinates within the cylindrical detector. (More details
about the datasets can be found in Appendix A.) Thus a jet can be described by the high-dimensional
vector x = {(log pT , η, ϕ)i}N
i , supposing the jet has N constituents. Our goal is thus to model the
data distribution p(x) precisely, and p(y|x) in the case of supervised classification with y denoting
the corresponding jet type."
PROBLEM STATEMENT,0.06451612903225806,"Parameterized Generative Modeling
At the LHC, event simulation serves as an important handle
for background estimation and data analysis. For many years, physics event simulators [10] have been
built on Monte Carlo methods based on physics rules. These generators are slow and need to be tuned
to the data frequently. Deep neural networks provide us with an efficient parameterized generative
approach to event simulation for the coming decades. Generative modeling in LHC physics has been
experimented with GANs (image-based [58, 27] and point-cloud-based [43]) and VAEs [64]. There
are also GANs [8] working on high-level features for event selection. However, as mentioned in the
introduction, these models all require explicit generators, which brings obstacles to situations with
complex data formats and sophisticated neural architectures."
PROBLEM STATEMENT,0.06854838709677419,"OOD Detection for New Physics Searches
Despite the great efforts in searching for new physics
signals at the LHC, there is no hint of beyond-Standard-Model physics. Given the large amount
of data produced at the LHC, it has been increasingly challenging to cover all the possible search
channels. Experiments at the LHC over the past decades have been focused on model-oriented
searches, as in searching for the Higgs boson [26, 36]. The null results up to now from model-driven
searches call for novel solutions. We thus shift to model-independent and data-driven anomaly
detection strategies, which are data-oriented rather than theory-guided, for new physics searches."
PROBLEM STATEMENT,0.07258064516129033,"Neural nets-based anomaly detection in HEP takes different formats. Especially, unsupervised
generative models trained on background events can be used to detect potential unseen signals. More
specifically, Variational Autoencoders [15, 41, 5] have been employed to detect novel signals, directly
based on detector-level features. High-level observables-based VAEs [12, 39] have also been explored.
However, naively trained VAEs (and Autoencoders) are usually subject to spurious correlations which
result in failure modes in anomaly detection, partially due to the format of the reconstruction loss
involved. Usually, one needs to explicitly mitigate these spurious correlations with auxiliary tasks
such as outlier exposure [35, 15], or modified model components such as autoencoders with smeared
pixel intensities [29] and autoencoders augmented with energy-based density estimation [69, 20]."
PROBLEM STATEMENT,0.07661290322580645,"At the same time, EBM naturally has the handle for discriminating between in-distribution and out-
of-distribution examples and accommodates tolerance for spurious correlations. While in-distribution
data points are trained to have lower energy, energies of OOD examples are pushed up in the learning
process. This property has been used for OOD detection in computer vision [25, 33, 23, 71]. This
thus indicates great potential for EBM-based new physics detection at the LHC."
METHODS,0.08064516129032258,"3
Methods"
ENERGY BASED MODELS,0.0846774193548387,"3.1
Energy Based Models"
ENERGY BASED MODELS,0.08870967741935484,"Energy-based models are constructed to model the unnormalized data probabilities. They leverage the
property that any exponential term exp(−E(x)) is non-negative and thus can represent unnormalized
probabilities naturally. The data distribution is modeled through the Boltzmann distribution: pθ(x) =
exp(−Eθ(x))/Z(θ) with the energy model Eθ(x)1 : X →R mapping x ∈X to a scalar. And the
partition function Z =
R
˜p(x)dx =
R
exp(−Eθ(x))dx integrates over all the possible states."
ENERGY BASED MODELS,0.09274193548387097,"EBMs can be learned through maximum likelihood EpD(log pθ(x)). However, the training of EBMs
can pose challenges due to the intractable partition function in log pθ(x) = −Eθ(x) −log Z(θ).
Though the partition function is intractable, the gradients of the log-likelihood do not involve the
partition function directly. Thus when taking gradients w.r.t. the model parameters θ, the partition"
ENERGY BASED MODELS,0.0967741935483871,"1We note that this energy function is not to be confused with the physical energy of the HEP events/objects.
Here the energy is essentially an abstract quantity used for probabilistic modelling of the radiation patterns and
substructures. It can approximately serve as a measure of radiation substructure or complexity when describing
HEP events."
ENERGY BASED MODELS,0.10080645161290322,Multi-Head
ENERGY BASED MODELS,0.10483870967741936,Attention
ENERGY BASED MODELS,0.10887096774193548,Linear Embedding
ENERGY BASED MODELS,0.11290322580645161,Feed Forward
ENERGY BASED MODELS,0.11693548387096774,Add(&Norm)
ENERGY BASED MODELS,0.12096774193548387,Add(&Norm)
ENERGY BASED MODELS,0.125,"(batch_size, len_seq, 3) MLP"
ENERGY BASED MODELS,0.12903225806451613,Sum Pooling E 
ENERGY BASED MODELS,0.13306451612903225,Gradient-based
ENERGY BASED MODELS,0.13709677419354838,Markov Chain
ENERGY BASED MODELS,0.14112903225806453,Monte Carlo
ENERGY BASED MODELS,0.14516129032258066,"Contrastive
Divergence"
ENERGY BASED MODELS,0.14919354838709678,"Random Noise
MCMC
Samples Data"
ENERGY BASED MODELS,0.1532258064516129,"Figure 1: Schematic of the EBM model. The energy function E(x, y) is estimated with a transformer.
The training procedure is governed by Contrastive Divergence (the vertical dimension), for which the
model distribution estimation qθ(x) is obtained with Langevin Dynamics (the horizontal dimension),
evolving samples from random noises x−
0 ."
ENERGY BASED MODELS,0.15725806451612903,function is canceled out. The gradient of the maximum likelihood loss function can be written as:
ENERGY BASED MODELS,0.16129032258064516,"∇θL(θ) = −EpD(x)[∇θ log pθ(x)]
(1)"
ENERGY BASED MODELS,0.16532258064516128,"= Ex+∼pD(x)[∇θEθ(x+)] −Ex−∼pθ(x)[∇θEθ(x−)] ,
(2)"
ENERGY BASED MODELS,0.1693548387096774,"where pD(x) is the data distribution and pθ(x) is the model distribution. The training objective is
thus composed of two terms, corresponding to two different learning phases (i.e., the positive phase
to fit the data x+, and the negative phase to fit the model distribution x−). When parameterizing
the energy function with feed-forward neural networks [54], the positive phase is straightforward.
However, the negative phase requires sampling over the model distribution. This leads to various
Monte Carlo sampling strategies for estimating the maximum likelihood."
ENERGY BASED MODELS,0.17338709677419356,"Contrasting the energies of the data and the model samples as proposed Contrastive Divergence (CD)
[37, 11] leads to an effective strategy to train EBMs with the following CD objective:"
ENERGY BASED MODELS,0.1774193548387097,"DKL(pD(x)∥pθ(x)) −DKL(TpD(x)∥pθ(x)) ,
(3)"
ENERGY BASED MODELS,0.1814516129032258,"where T denotes the one-step Monte Carlo Markov Chain (MCMC) kernel imposed on the data
distribution. In more recent approaches for high-dimensional modeling [56, 25], we can directly
initialize from random noises to generate MCMC samples instead of initializing from the data
distribution as in the original CD approach. This strategy also helps with exploring and mixing
between modes."
ENERGY BASED MODELS,0.18548387096774194,"Negative Sampling
The most critical component in the negative phase is the sampling to estimate
the model distribution. We employ gradient-based MCMC generation for the negative sampling,
which is handled by Langevin Dynamics [52, 66]. As written in Eq. 4, Langevin dynamics uses
gradients w.r.t. the input dimensions, associated with a diffusion term to inject stochasticity, to
generate a sequence of negative samples {x−
k }K
k=1."
ENERGY BASED MODELS,0.18951612903225806,"x−
k+1 = x−
k −λ2"
ENERGY BASED MODELS,0.1935483870967742,"2 ∇xEθ(x−
k ) + λ · ϵ, with ϵ ∼N(0, 1)
(4)"
ENERGY BASED MODELS,0.1975806451612903,"MC Convergence
The training anatomy [55, 56] for short-run non-convergent MCMC and long-run
convergent MCMC shows that short-run (∼5-100 steps) MCMC initialized from random distributions"
ENERGY BASED MODELS,0.20161290322580644,"is able to generate realistic samples, while long-run MCMC might be oversaturated with lower-energy
states."
ENERGY BASED MODELS,0.2056451612903226,"To improve mode coverage, we use random noise to initialize MCMC chains. To accelerate training,
we employ a relatively small number of MCMC steps. In practice, we can reuse the generated
samples as initial samples of the following MCMC chains to accelerate mixing, similar to Persistent
Contrastive Divergence [63]. Following the procedure in [25], we use a randomly initialized buffer
that is consistently updated from previous MCMC runs as the initial samples. (As empirically shown
[13, 66], a Metropolis-Hastings step is not necessary. So we ignore this rejection update in our
experiments.)"
ENERGY BASED MODELS,0.20967741935483872,"Energy Function
Since there is no explicit generator in EBMs, we have much freedom in designing
the architectures of the energy function. This also connects with the fast-paced development of
supervised neural classifier architectures for particle physics. We can directly reuse the architectures
from supervised classifiers in the generative modeling of EBMs. We use a self-attention-based
transformer to parameterize the energy function Eθ(·). We defer the detailed description to Sec. 3.3."
ENERGY BASED MODELS,0.21370967741935484,The full algorithm for training EBMs is described in Algorithm 1.
ENERGY BASED MODELS,0.21774193548387097,Algorithm 1 EBM training with MCMC by Langevin Dynamics
ENERGY BASED MODELS,0.2217741935483871,"Input: training samples {x+
i }N
i=1 from pD(x), parameterized energy function Eθ(·), initial buffer
B ←∅, Langevin dynamics step size λx, number of MCMC steps K, model parameter learning
rate λθ, regularization strength α
for Gradient descent step l = 0...L-1 do"
ENERGY BASED MODELS,0.22580645161290322,"x+
i ∼pD(x)
x−
i,0 ∼0.95 ∗B + 0.05 ∗U ▷Reinitialize the samples in the buffer with random noise in the
probability of 0.05"
ENERGY BASED MODELS,0.22983870967741934,for Langevin dynamics step k = 0...K-1 do
ENERGY BASED MODELS,0.23387096774193547,"x−
i,k+1 = x−
i,k −λx∇xEθ(x−
i,k) + 0.005 · ϵk, ϵk ∼N(0, 1) ▷Langevin Dynamics taking
gradients w.r.t. input dimensions"
ENERGY BASED MODELS,0.23790322580645162,"end for
x−
i ←x−
i,K
LCD = 1 N
P"
ENERGY BASED MODELS,0.24193548387096775,"i(Eθ(x+
i ) −Eθ(x−
i ))
Lreg = 1 N
P"
ENERGY BASED MODELS,0.24596774193548387,"i(Eθ(x+
i )2 + Eθ(x−
i ))2
▷L2 Regularization
θ ←θ −λθ∇θ(LCD + αLreg)
▷Update model parameters with gradient descent
B ←x−
i,K ∪B
▷Update the buffer with generated samples
end for"
HYBRID MODELING,0.25,"3.2
Hybrid Modeling"
HYBRID MODELING,0.2540322580645161,"Neural Classifier as an EBM
A classical classifier can be re-interpreted in the framework of
EBMs [46, 33, 67, 17, 40], with the logits g(x) corresponding to negative energies of the joint
distribution p(x, y) = exp(g(x)y)"
HYBRID MODELING,0.25806451612903225,"Z
, where g(x)y denotes the logit corresponding to the label y. Thus"
HYBRID MODELING,0.2620967741935484,the probability marginalized over y can be written as p(x) = P
HYBRID MODELING,0.2661290322580645,y exp(g(x)y)
HYBRID MODELING,0.2701612903225806,"Z
, with the energy of x as
−log P"
HYBRID MODELING,0.27419354838709675,"y exp(g(x)y). We are then brought back to the classical softmax probability
exp(g(x)y)
P"
HYBRID MODELING,0.2782258064516129,"y exp(g(x)y)
when calculating p(y|x)."
HYBRID MODELING,0.28225806451612906,"This viewpoint provides a novel method for jointly training a supervised classifier and an unsupervised
generative model. Specifically, [33] successfully incorporated EBM-based generative modeling into
a classifier in image generation and classification. We follow their proposal to train the hybrid model
as follows to ensure the classifier p(y|x) is unbiased. The joint log-likelihood is decomposed into
two terms:
log p(x, y) = log p(x) + log p(y|x) .
(5)"
HYBRID MODELING,0.2862903225806452,"Thus one can maximize log p(x) with the contrastive divergence of the EBM with the energy
function E(x) = −log P"
HYBRID MODELING,0.2903225806451613,"y exp(g(x)y), and maximize log p(y|x) with the usual cross-entropy of
the classification."
ENERGY-BASED MODELS FOR ELEMENTARY PARTICLES,0.29435483870967744,"3.3
Energy-based Models for Elementary Particles"
ENERGY-BASED MODELS FOR ELEMENTARY PARTICLES,0.29838709677419356,"We would like to construct an energy-based model for describing jets and their inner structures. In
conceiving the energy function for these elementary particles, we consider the following constraints
and characteristics: 1) permutation invariance – the energy function should be invariant to jet
constituent permutations, and 2) higher-order interactions – we would like the energy function to be
powerful enough to simulate the complex inter-particle interactions."
ENERGY-BASED MODELS FOR ELEMENTARY PARTICLES,0.3024193548387097,"Thus, we leverage the self-attention-based transformer [65] to approximate the energy function,
which takes into account the higher-order interactions between the component particles. As indicated
in Eq. 6b, the encoding vector of each constituent W is connected with all other constituents through
the self-attention weights A in Eq. 6a, which are already products of particle representations Q and
K.
A = softmax(Q · KT /
p"
ENERGY-BASED MODELS FOR ELEMENTARY PARTICLES,0.3064516129032258,"dmodel)
(6a)
W = A · V
(6b)"
ENERGY-BASED MODELS FOR ELEMENTARY PARTICLES,0.31048387096774194,"Moreover, we can easily incorporate particle permutation invariance [70] in the transformer, by
summing up the encodings of each jet constituent. The transformer architecture is shown in Fig. 1.
The coordinates (log pT , η, ϕ) of each jet constituent are first embedded into a dmodel-dimensional
space through a linear layer, then fed into NL self-attention blocks sequentially. After that, a sum-
pooling layer is used to sum up the features of the jet constituents. Finally, a multi-layer-perceptron
projector maps the features into the energy score. Model parameters are recorded in Table 4 of
Appendix A."
EXPERIMENTS,0.31451612903225806,"4
Experiments"
EXPERIMENTS,0.3185483870967742,"Training Details
The training set consists of 300,000 QCD jets. We have 10,000 samples in the
buffer and reinitialize the random samples with a probability of 0.05 in each iteration. We use a
relatively small number of steps (e.g., 24) for the MCMC chains. The step size λx is set to 0.1
according to standard deviations of the input features. The diffusion magnitude within the Langevin
dynamics is set to 0.005. The number of steps used in validation steps is set to 128 for better mixing."
EXPERIMENTS,0.3225806451612903,"We use Adam [44] for optimization, with the momenta β1 = 0.0 and β2 = 0.999. The initial learning
rate is set to 1e-4, with a decay rate of 0.98 for each epoch. We use a batch size of 128, and train the
model for 50 epochs. More details can be found in Appendix A."
EXPERIMENTS,0.32661290322580644,"Model Validation
In monitoring the likelihood, the partition function can be estimated with An-
nealed Importance Sampling (AIS) [53]. However, these estimates can be erroneous and consume a lot
of computing resources. Fortunately for physics events, we have well-designed high-level features as a
handle for monitoring the generation quality. Especially, we employ the boost-invariant jet transverse"
EXPERIMENTS,0.33064516129032256,"momentum pT = PN
i=1 pT i and the Lorentz-invariant jet mass M =
q"
EXPERIMENTS,0.3346774193548387,"(PN
i=1 Ei)2 −(PN
i=1 pi)2"
EXPERIMENTS,0.3387096774193548,"as the validation observables. And we calculate the Jensen–Shannon Divergence (JSD), between these
high-level observable distributions of the data and the model generation, as the metric. In contrast to
the short-run MCMC in the training steps, we instead use longer MCMC chains for generating the
validation samples."
EXPERIMENTS,0.34274193548387094,"When we focus on downstream tasks such as OOD detection, it’s reasonable to employ the Area
Under the Receiver Operating Characteristic (ROC) Curve (AUC) as the validation metric, with
Standard Model top jets serving as the benchmark signal."
EXPERIMENTS,0.3467741935483871,"Generation
Test-time generation is achieved in MCMC transition steps from the proposal random
(Gaussian) distribution. We use a colder model and a smaller step size at test time which is annealed
from 0.1 and decreases by a factor of 0.8 in every 40 steps to encourage stable generation. The
diffusion noise magnitude is set to 0.001. Longer MCMC chains with more steps (e.g., 200) are taken
to achieve realistic generation. 2 A consistent observation across various considered methods is that
the step size stands out as the crucial parameter predominantly influencing the quality of generation."
EXPERIMENTS,0.35080645161290325,"2In practice, MCMC-based generation could be slow, especially in cases with very long MCMC chains.
Though in our case, 200 steps are enough to produce very realistic samples, it is worth exploring methods to
accelerate the generation process."
EXPERIMENTS,0.3548387096774194,"Anomaly Detection
In contrast to prevalent practices in computer vision, EBM-based OOD
detection in LHC physics exhibits specificity. The straightforward approach of comparing two distinct
datasets, such as CIFAR10 and SVHN, overlooks the intricate real-world application environments.
Adapting OOD detection to scientific discovery at the LHC, we reformulate and tailor the decision
process as follows: if we train on multiple known Standard Model jet classes, we focus on class-
conditional model evaluation for discriminating between the unseen test signals and the most copious
background events (i.e., QCD jets rather than all the Standard Model jets)."
EXPERIMENTS,0.3588709677419355,"4.1
Generative Modeling – Energy-based Event Generator"
EXPERIMENTS,0.3629032258064516,"We present the generated jets transformed from initial random noises with the Langevin dynamics
MCMC. Due to the non-human-readable nature of physics events (e.g., low-level raw records at
the particle detectors), we are not able to examine the generation quality through formats such as
images directly. However, it has a long history that expert-designed high-level observables can
serve as strong discriminating features. In the first row of Fig. 2, we first show the distributions of
input features for the data and the model generation. Meanwhile, in the second row, we plot the
distributions of high-level expert observables including the jet transverse momentum pT and the jet
mass M. Through modeling low-level features in the detector space, we achieve precise recovery of
the high-level physics observables in the theoretical framework. For better visualization, we can also
map the jets onto the (η, ϕ) plane, with pixel intensities associated with the corresponding energy
deposits. We show the average generated jet images in Fig. 7 of Appendix B, comparing to the real
jet images (right-most) in the (η, ϕ) plane. In Table 2, we present the Jensen-Shannon Divergence of
the high-level observables pT and M distributions between real data and model generation, as the
quantitative measure of the generation performance."
EXPERIMENTS,0.36693548387096775,"2
0
2
4
6
log pTi/GeV 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 a.u."
EXPERIMENTS,0.3709677419354839,"Data
Model Generation"
EXPERIMENTS,0.375,"1.0
0.5
0.0
0.5
1.0
0 1 2 3 4 5 a.u."
EXPERIMENTS,0.3790322580645161,"Data
Model Generation"
EXPERIMENTS,0.38306451612903225,"1.0
0.5
0.0
0.5
1.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 a.u."
EXPERIMENTS,0.3870967741935484,"Data
Model Generation"
EXPERIMENTS,0.3911290322580645,"200
300
400
500
600
700
800
900
1000
pT/GeV 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 a.u."
EXPERIMENTS,0.3951612903225806,"Data
Model Generation"
EXPERIMENTS,0.39919354838709675,"0
100
200
300
400
500
M/GeV 0.000 0.002 0.004 0.006 0.008 0.010 0.012 a.u."
EXPERIMENTS,0.4032258064516129,"Data
Model Generation"
EXPERIMENTS,0.40725806451612906,"0.0
0.2
0.4
0.6
0.8
1.0
M/pT 0 1 2 3 4 5 6 a.u."
EXPERIMENTS,0.4112903225806452,"Data
Model Generation"
EXPERIMENTS,0.4153225806451613,"Figure 2: Top: Input feature distributions of jet constituents for the data and the model generation.
Bottom: High-level feature distributions for the data and the model generation."
EXPERIMENTS,0.41935483870967744,"Model
JSD (pT ) / 10−4
JSD (M)/ 10−4
JSD (M/pT )/ 10−4"
EXPERIMENTS,0.42338709677419356,"β-VAE [15]
3.7
11.0
–
EBM
2.4 ± 1.9
3.2 ± 2.0
6.3 ± 4.6
Table 2: Model comparison in terms of generation quality measured in Jensen-Shannon Divergence
of high-level observables pT and M. For EBMs, we present the means and standard deviations for
the JSDs obtained from 5 independent runs. We also present a β-VAE [15] model as a baseline."
EXPERIMENTS,0.4274193548387097,"4.2
Anomaly Detection – Anomalous Jet Tagging"
EXPERIMENTS,0.4314516129032258,"Since EBMs naturally provide an energy score for each jet, for which the in-distribution samples
should have lower scores while OOD samples are expected to incur higher energies. Furthermore, a
classifier, when interpreted as an energy-based model, the transformed energy score can also serve as
an OOD identifier [33, 49]."
EXPERIMENTS,0.43548387096774194,"EBM
In HEP, the in-situ energy score can be used to identify potential new physics signals. With
an energy-based model, which is trained on the QCD background events or directly on the slightly
signal-contaminated data, we expect unseen signals (i.e., non-QCD jets) to have higher energies and
correspondingly lower likelihoods."
EXPERIMENTS,0.43951612903225806,"In Fig. 3, we compare the energy distributions of in-distribution QCD samples, out-of-distribution
signal examples (hypothesized heavy Higgs boson [7] which decays into four QCD sub-jets), and
random samples drawn from the proposal Gaussian distribution. We observe that random samples
unusually have the highest energies. Signal jets have relatively higher energies compared with the
QCD background jets, making model-independent new physics searches possible."
EXPERIMENTS,0.4435483870967742,"Spurious Correlation
A more intriguing property of EBMs is that spurious correlations can be
better handled. Spurious correlations in jet tagging might result in distorted background distributions
and obscure effective signal detection. For instance, VAEs in OOD detection can be highly correlated
with the jet masses [15], similar to the spurious correlation with image pixel numbers in computer
vision [50, 60]. In the right panel of Fig. 3, we plot the correlation between the energy score and
the jet mass. Unlike other generative strategies for model-independent anomaly detection, EBMs
are largely free from the spurious correlation between the energy E(x) and the jet mass M. The
underlying reason for EBMs not presenting mass correlation could be the negative sampling involved
in the training process. Larger mass modes are already covered during the negative sampling process.
This makes EBMs a promising candidate for model-independent new physics searches."
EXPERIMENTS,0.4475806451612903,"2
1
0
1
2
E 0 2 4 6 8 10 a.u."
EXPERIMENTS,0.45161290322580644,"QCD
Signal
Random"
EXPERIMENTS,0.45564516129032256,"0
100
200
300
400
MJ/GeV 2 1 0 1 2 E"
EXPERIMENTS,0.4596774193548387,"QCD
Signal
Random"
EXPERIMENTS,0.4637096774193548,"Figure 3: Left: Energy distributions for random samples, background QCD jets, and novel signals.
Right: Correlation between the jet mass MJ and the energy E."
EXPERIMENTS,0.46774193548387094,"EBM-CLF
The task of classifying/identifying different Standard Model jet types and the task of
searching for beyond the Standard Model signals actually can be unified in a single approach with
neural classifiers distinguishing different Standard Model particle types [16]. Compared with naive
generative model based OOD, training neural classifiers on different known Standard Model jets
helps the model with learning more meaningful and robust representations for effectively detecting
new signals. Further combining generative models and discriminative classifiers can be achieved
in the EBM framework elegantly. We employ the hybrid learning scheme [54, 33] combining the
discriminate and the generative approaches. It links with many OOD detection techniques and
observations. For instance, the maximum logit for OOD detection [34] comes in the format of the
lowest energy miny E(x, y) in this framework."
EXPERIMENTS,0.4717741935483871,"We train an EBM-based multi-class classifier (EBM-CLF) according to Eq. 5, for both discriminating
different Standard Model jets (QCD jets, boosted W jets, and boosted top jets) and generating real-like
jet samples. The results of generative sampling are shown in Appendix B. The jointly trained EBM
and jet classifier maintain the classification accuracy (see Appendix B). The associated EBM is
augmented by the discriminative task, and thus assisted with better inductive biases and domain
knowledge contained in the in-distribution classes. EBM-CLF is an example of how we unify different
physics tasks (jet classification, anomaly detection, and generative modeling) in a unified framework."
EXPERIMENTS,0.47580645161290325,"We measure the OOD detection performance in the ROC curve and the AUC of the binary clas-
sification between the background QCD samples and the signal jets. Table 3 records the AUCs
of different models (and different anomaly scoring functions) in tagging Standard Model Top jets
and hypothesized Higgs bosons (OOD H). The jointly trained model generally has better anomaly
tagging performance compared with the naive EBM. We also explore the norm of the score function
of pθ(x): ∥∇x log pθ(x)∥= ∥∇xE(x)∥serving as the anomaly score (similar to the approximate
mass in [33]). Constructed from the energy function, the score function approximately inherits the
nice property of mass decorrelation. However, they have slightly worse performance compared
with E(x). The corresponding ROC curves are shown in the left panel of Fig. 4, in terms of the
signal efficiency ϵS (i.e., true positive rate) and the background rejection rate 1/ϵB (i.e., 1/false
positive rate). In the right panel, we plot the background mass distributions under different cuts on
the energy scores. We observe excellent jet mass decorrelation/invariance for energy score-based
anomalous jet tagging. Additionally for EBM-CLF, we also record the AUCs for anomaly scores of
the class-conditional softmax probability p(y|x) and the logit g(x)y = −E(x, y) corresponding to
the background QCD class, in Table 3. However, without further decorrelation strategies (auxiliary
tasks or assistant training strategies), these two anomaly scores are usually strongly correlated with
the masses of the in-distribution classes and distort the background distributions. Thus we list the
results here only for reference."
EXPERIMENTS,0.4798387096774194,"Model
AUC (Top)
AUC (OOD H)"
EXPERIMENTS,0.4838709677419355,"DisCo-VAE (κ = 1000) [15]
0.593
0.481
KL-OE-VAE [15]
0.744
0.625
Mass-decorrelated
EBM (E(x))
0.679 ± 0.009
0.794 ± 0.032
EBM (∥∇x log pθ(x)∥)
0.628 ± 0.017
0.738 ± 0.033
EBM-CLF (E(x))
0.703 ± 0.020
0.815 ± 0.018
EBM-CLF (∥∇x log pθ(x)∥)
0.679 ± 0.040
0.722 ± 0.028
Mass-correlated
EBM-CLF (g(x)y)
0.920 ± 0.002
0.877 ± 0.008
EBM-CLF (p(y|x))
0.940 ± 0.002
0.865 ± 0.012
Table 3: Anomaly detection performance for different models (anomaly scores in parentheses)
measured in AUCs. We present the averaged AUCs over 5 independent models with different random
seeds and the associated standard deviations. We list a few baseline models with mass decorrelation
achieved either by a regularization term (DisCo-VAE (κ = 1000)) or an auxiliary classification task
contrasting in-distribution data and outlier data (KL-OE-VAE)."
CONCLUSION,0.4879032258064516,"5
Conclusion"
CONCLUSION,0.49193548387096775,"We present a novel energy-based generative framework for modeling the behavior of elementary
particles. By mimicking the inter-particle interactions with a self-attention-based transformer, we
map the correlations in the detector space to a probabilistic space with an energy function. The
energy model is used for the implicit generation of physics events. Despite the difficulty in training
EBMs, we adapted the training strategies to balance learning efficiency and training stability. We
adopt short-run MCMC sampling at training, while at test time we instead use dynamic generation
to obtain prominent generative quality. Additionally, the framework supports the construction of an
augmented classifier with integrated generative modeling. This unified approach provides us with
flexible tools for high-performance parameterized physics event simulation and spurious-correlation-
free model-independent signal detection. It marks a significant stride in next-generation multitasking
machine learning models for high-energy physics."
CONCLUSION,0.4959677419354839,"0.2
0.4
0.6
0.8
1.0 S 100 101 102 1/ B"
CONCLUSION,0.5,random
CONCLUSION,0.5040322580645161,"EBM(E(x)), AUC(Top)=0.679
EBM(E(x)), AUC(H)=0.794
EBM-CLF(E(x)), AUC(Top)=0.703
EBM-CLF(E(x)), AUC(H)=0.815"
CONCLUSION,0.5080645161290323,"0
50
100
150
200
250
300
350
MJ/GeV 10
5 10
4 10
3 10
2 a.u."
CONCLUSION,0.5120967741935484,QCD [ = 1.0]
CONCLUSION,0.5161290322580645,"QCD [ = 0.5]
QCD [ = 0.2]"
CONCLUSION,0.5201612903225806,QCD [ = 0.1]
CONCLUSION,0.5241935483870968,"Figure 4: Left: ROC Curves for EBM and EBM-CLF with the energy E(x) as the anomaly score.
The grey line denotes the case of random guessing. Right: Background mass distributions under
different acceptance rates ϵ after cutting on the energy score from the EBM-CLF."
CONCLUSION,0.5282258064516129,Acknowledgments and Disclosure of Funding
CONCLUSION,0.532258064516129,"This work is partially supported by the IVADO Postdoctoral Research Funding. We acknowledge the
computing resources provided by Mila (mila.quebec)."
REFERENCES,0.5362903225806451,References
REFERENCES,0.5403225806451613,"[1] LHC Machine. JINST, 3:S08001, 2008."
REFERENCES,0.5443548387096774,"[2] Georges Aad et al. Observation of a new particle in the search for the Standard Model Higgs boson with
the ATLAS detector at the LHC. Phys. Lett. B, 716:1–29, 2012."
REFERENCES,0.5483870967741935,"[3] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann
machines. Cognitive Science, 9(1):147–169, 1985."
REFERENCES,0.5524193548387096,"[4] Johan Alwall, Michel Herquet, Fabio Maltoni, Olivier Mattelaer, and Tim Stelzer. Madgraph 5: going
beyond. Journal of High Energy Physics, 2011(6), Jun 2011."
REFERENCES,0.5564516129032258,"[5] Oliver Atkinson, Akanksha Bhardwaj, Christoph Englert, Partha Konar, Vishal S. Ngairangbam, and
Michael Spannowsky. IRC-Safe Graph Autoencoder for Unsupervised Anomaly Detection. Front. Artif.
Intell., 5:943135, 2022."
REFERENCES,0.5604838709677419,"[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. CoRR, abs/1409.0473, 2015."
REFERENCES,0.5645161290322581,"[7] G.C. Branco, P.M. Ferreira, L. Lavoura, M.N. Rebelo, Marc Sher, and Joao P. Silva. Theory and phe-
nomenology of two-Higgs-doublet models. Phys. Rept., 516:1–102, 2012."
REFERENCES,0.5685483870967742,"[8] Anja Butter, Tilman Plehn, and Ramon Winterhalder. How to GAN LHC Events. SciPost Phys., 7(6):075,
2019."
REFERENCES,0.5725806451612904,"[9] Matteo Cacciari, Gavin P Salam, and Gregory Soyez. The anti-kt jet clustering algorithm. Journal of High
Energy Physics, 2008(04):063–063, Apr 2008."
REFERENCES,0.5766129032258065,"[10] J. M. Campbell et al. Event Generators for High-Energy Physics Experiments. In 2022 Snowmass Summer
Study, 3 2022."
REFERENCES,0.5806451612903226,"[11] Miguel Á. Carreira-Perpiñán and Geoffrey Hinton. On contrastive divergence learning. In Robert G.
Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial
Intelligence and Statistics, volume R5 of Proceedings of Machine Learning Research, pages 33–40. PMLR,
06–08 Jan 2005. Reissued by PMLR on 30 March 2021."
REFERENCES,0.5846774193548387,"[12] Olmo Cerri, Thong Q. Nguyen, Maurizio Pierini, Maria Spiropulu, and Jean-Roch Vlimant. Variational
Autoencoders for New Physics Mining at the Large Hadron Collider. JHEP, 05:036, 2019."
REFERENCES,0.5887096774193549,"[13] Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International Conference on Machine Learning, 2014."
REFERENCES,0.592741935483871,"[14] Taoli Cheng. Test sets for jet anomaly detection at the lhc, March 2021."
REFERENCES,0.5967741935483871,"[15] Taoli Cheng, Jean-François Arguin, Julien Leissner-Martin, Jacinthe Pilette, and Tobias Golling. Variational
autoencoders for anomalous jet tagging. Phys. Rev. D, 107(1):016002, 2023."
REFERENCES,0.6008064516129032,"[16] Taoli Cheng and Aaron Courville. Invariant representation driven neural classifier for anti-QCD jet tagging.
JHEP, 10:152, 2022."
REFERENCES,0.6048387096774194,"[17] Jifeng Dai and Ying Nian Wu.
Generative modeling of convolutional neural networks.
CoRR,
abs/1412.6296, 2014."
REFERENCES,0.6088709677419355,"[18] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaître, A. Mertens, and M. Selvaggi. Delphes
3: a modular framework for fast simulation of a generic collider experiment. Journal of High Energy
Physics, 2014(2), Feb 2014."
REFERENCES,0.6129032258064516,"[19] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energy-based
models for text generation. In International Conference on Learning Representations, 2020."
REFERENCES,0.6169354838709677,"[20] Barry M. Dillon, Luigi Favaro, Tilman Plehn, Peter Sorrenson, and Michael Krämer. A normalized
autoencoder for lhc triggers, 2022."
REFERENCES,0.6209677419354839,"[21] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation and inference with energy
based models."
REFERENCES,0.625,"[22] Yilun Du, Shuang Li, Yash Sharma, Joshua B. Tenenbaum, and Igor Mordatch. Unsupervised learning of
compositional energy concepts. In NeurIPS, 2021."
REFERENCES,0.6290322580645161,"[23] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of
energy based models."
REFERENCES,0.6330645161290323,"[24] Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, and Alexander Rives. Energy-based models for atomic-
resolution protein conformations. In International Conference on Learning Representations, 2020."
REFERENCES,0.6370967741935484,[25] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.
REFERENCES,0.6411290322580645,"[26] F. Englert and R. Brout. Broken symmetry and the mass of gauge vector mesons. Phys. Rev. Lett.,
13:321–323, Aug 1964."
REFERENCES,0.6451612903225806,"[27] Martin Erdmann, Jonas Glombitza, and Thorben Quast. Precise simulation of electromagnetic calorimeter
showers using a Wasserstein Generative Adversarial Network. Comput. Softw. Big Sci., 3(1):4, 2019."
REFERENCES,0.6491935483870968,"[28] S. Chatrchyan et al. Observation of a new boson at a mass of 125 gev with the cms experiment at the lhc.
Physics Letters B, 716(1):30–61, 2012."
REFERENCES,0.6532258064516129,"[29] Thorben Finke, Michael Krämer, Alessandro Morandini, Alexander Mück, and Ivan Oleksiyuk. Autoen-
coders for unsupervised anomaly detection in high energy physics. JHEP, 06:161, 2021."
REFERENCES,0.657258064516129,"[30] J.A. Fodor and E. LePore. The Compositionality Papers. Clarendon Press, 2002."
REFERENCES,0.6612903225806451,"[31] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–
144, 2020."
REFERENCES,0.6653225806451613,"[32] Will Grathwohl, Jacob Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, and David Kristjanson
Duvenaud. No mcmc for me: Amortized sampling for fast and stable training of energy-based models.
ArXiv, abs/2010.04230, 2021."
REFERENCES,0.6693548387096774,"[33] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and
Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one."
REFERENCES,0.6733870967741935,"[34] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and
Dawn Xiaodong Song. Scaling out-of-distribution detection for real-world settings. In International
Conference on Machine Learning, 2022."
REFERENCES,0.6774193548387096,"[35] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure.
In International Conference on Learning Representations, 2019."
REFERENCES,0.6814516129032258,"[36] Peter W. Higgs. Broken symmetries, massless particles and gauge fields. Phys. Lett., 12:132–133, 1964."
REFERENCES,0.6854838709677419,"[37] Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Comput.,
14(8):1771–1800, aug 2002."
REFERENCES,0.6895161290322581,"[38] J J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences, 79(8):2554–2558, 1982."
REFERENCES,0.6935483870967742,"[39] Pratik Jawahar, Thea Aarrestad, Nadezda Chernyavskaya, Maurizio Pierini, Kinga A. Wozniak, Jennifer
Ngadiuba, Javier Duarte, and Steven Tsan. Improving Variational Autoencoders for New Physics Detection
at the LHC With Normalizing Flows. Front. Big Data, 5:803685, 2022."
REFERENCES,0.6975806451612904,"[40] Long Jin, Justin Lazarow, and Zhuowen Tu. Introspective classification with convolutional nets. In
I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.7016129032258065,"[41] Alan Kahn, Julia Gonski, Inês Ochoa, Daniel Williams, and Gustaaf Brooijmans. Anomalous jet identifica-
tion via sequence modeling. JINST, 16(08):P08012, 2021."
REFERENCES,0.7056451612903226,"[42] Raghav Kansal, Javier Duarte, Breno Orzari, Thiago Tomei, Maurizio Pierini, Mary Touranakou, Jean-Roch
Vlimant, and Dimitrios Gunopulos. Graph Generative Adversarial Networks for Sparse Data Generation in
High Energy Physics. In 34th Conference on Neural Information Processing Systems, 11 2020."
REFERENCES,0.7096774193548387,"[43] Raghav Kansal, Javier Duarte, Hao Su, Breno Orzari, Thiago Tomei, Maurizio Pierini, Mary Touranakou,
Jean-Roch Vlimant, and Dimitrios Gunopulos. Particle Cloud Generation with Message Passing Generative
Adversarial Networks. 6 2021."
REFERENCES,0.7137096774193549,"[44] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015."
REFERENCES,0.717741935483871,"[45] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2014."
REFERENCES,0.7217741935483871,"[46] Yann LeCun, Sumit Chopra, Raia Hadsell, Aurelio Ranzato, and Fu Jie Huang. A tutorial on energy-based
learning. 2006."
REFERENCES,0.7258064516129032,"[47] Julien Leissner-Martin, Taoli Cheng, and Jean-Francois Arguin. Qcd jet samples with particle flow
constituents, July 2020."
REFERENCES,0.7298387096774194,"[48] M. Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji. Graphebm: Molecular graph generation with
energy-based models. ArXiv, abs/2102.00546, 2021."
REFERENCES,0.7338709677419355,"[49] Weitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li. Energy-based out-of-distribution detection.
In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20,
Red Hook, NY, USA, 2020. Curran Associates Inc."
REFERENCES,0.7379032258064516,"[50] Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Görür, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? ArXiv, abs/1810.09136, 2019."
REFERENCES,0.7419354838709677,"[51] Subhajit Naskar, Pedram Rooshenas, Simeng Sun, Mohit Iyyer, and Andrew McCallum. Energy-based
reranking: Improving neural machine translation using energy-based models. In ACL, 2021."
REFERENCES,0.7459677419354839,"[52] Radford Neal. MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo, pages
113–162. 2011."
REFERENCES,0.75,"[53] Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001."
REFERENCES,0.7540322580645161,"[54] Jiquan Ngiam, Zhenghao Chen, Pang Wei Koh, and Andrew Y. Ng. Learning deep energy models. In
Proceedings of the 28th International Conference on International Conference on Machine Learning,
ICML’11, page 1105–1112, Madison, WI, USA, 2011. Omnipress."
REFERENCES,0.7580645161290323,"[55] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc-based
maximum likelihood learning of energy-based models. In AAAI, 2020."
REFERENCES,0.7620967741935484,"[56] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent
short-run mcmc toward energy-based model. In NeurIPS, 2019."
REFERENCES,0.7661290322580645,"[57] Frank Noé, Jonas Köhler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body
systems with deep learning. Science, 365, 2019."
REFERENCES,0.7701612903225806,"[58] Michela Paganini, Luke de Oliveira, and Benjamin Nachman. Accelerating Science with Generative
Adversarial Networks: An Application to 3D Particle Showers in Multilayer Calorimeters. Phys. Rev. Lett.,
120(4):042003, 2018."
REFERENCES,0.7741935483870968,"[59] Huilin Qu and Loukas Gouskos. ParticleNet: Jet Tagging via Particle Clouds. Phys. Rev. D, 101(5):056019,
2020."
REFERENCES,0.7782258064516129,"[60] J. Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon, and
Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. ArXiv, abs/1906.02845,
2019."
REFERENCES,0.782258064516129,"[61] Torbjörn Sjöstrand, Stephen Mrenna, and Peter Skands. A brief introduction to pythia 8.1. Computer
Physics Communications, 178(11):852–867, Jun 2008."
REFERENCES,0.7862903225806451,"[62] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 12438–12448. Curran Associates, Inc., 2020."
REFERENCES,0.7903225806451613,"[63] Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient.
In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, page 1064–1071,
New York, NY, USA, 2008. Association for Computing Machinery."
REFERENCES,0.7943548387096774,"[64] Mary Touranakou, Nadezda Chernyavskaya, Javier Duarte, Dimitrios Gunopulos, Raghav Kansal, Breno
Orzari, Maurizio Pierini, Thiago Tomei, and Jean-Roch Vlimant. Particle-based fast jet simulation at the
lhc with variational autoencoders, 2022."
REFERENCES,0.7983870967741935,"[65] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017."
REFERENCES,0.8024193548387096,"[66] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th International Conference on International Conference on Machine Learning,
ICML’11, page 681–688, Madison, WI, USA, 2011. Omnipress."
REFERENCES,0.8064516129032258,"[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International
Conference on Machine Learning, pages 2635–2644. PMLR, 2016."
REFERENCES,0.8104838709677419,"[68] Jianwen Xie, Yifei Xu, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Generative pointnet: Deep
energy-based learning on unordered point sets for 3d generation, reconstruction and classification. 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14971–14980, 2021."
REFERENCES,0.8145161290322581,"[69] Sangwoong Yoon, Yung-Kyun Noh, and Frank Chongwoo Park. Autoencoding under normalization
constraints. In International Conference on Machine Learning, 2021."
REFERENCES,0.8185483870967742,"[70] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan Salakhutdinov, and Alex
Smola. Deep sets. In NIPS, 2017."
REFERENCES,0.8225806451612904,"[71] Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models for
anomaly detection. In International Conference on Machine Learning, 2016."
REFERENCES,0.8266129032258065,"A
Experimental Details"
REFERENCES,0.8306451612903226,"Datasets
For the standard EBM, we train on 300,000 simulated QCD jets. For the hybrid model EBM-CLF,
we train on 300,000 simulated Standard Model jets (100,000 QCD jets, 100,000 boosted jets originating from
the W boson, and 100,000 boosted jets originating from the top quark). For OOD detection test sets, we employ
the hypothetical Higgs boson (in the decay mode of H →hh →(b¯b)(b¯b)) with a mass of 174 GeV, which
decays into two lighter Higgs bosons of 80 GeV. The intermediate light Higgs boson decays into two b quarks.
Each test set consists of 10,000 samples that are pT -refined to the range of [550, 650] GeV. All the jet samples
are generated with a pipeline of physics simulators."
REFERENCES,0.8346774193548387,"Event Generation
QCD jets are extracted from QCD di-jet events that are generated with MadGraph [4] for
LHC 13 TeV, followed by Pythia8 [61] and Delphes [18] for parton shower and fast detector simulation. All jets
are clustered using the anti-kT algorithm [9] with cone size R = 1.0 and a selection cut in the jet transverse
momentum pT > 450 GeV. We use the particle flow objects for jet clustering."
REFERENCES,0.8387096774193549,"Input Preprocessing
Jets are preprocessed before being fed into the neural models. Jets are longitudinally
boosted and centered at (0, 0) in the (η, ϕ) plane. The centered jets are then rotated so that the jet principal
axis (P"
REFERENCES,0.842741935483871,"i
ηiEi"
REFERENCES,0.8467741935483871,"Ri , P"
REFERENCES,0.8508064516129032,"i
ϕiEi"
REFERENCES,0.8548387096774194,"Ri ) (with Ri =
p"
REFERENCES,0.8588709677419355,"η2
i + ϕ2
i ) and Ei is the constituent energy) is vertically aligned on the
(η, ϕ) plane."
REFERENCES,0.8629032258064516,"Hyper-parameters
Hyper-parameters are recorded in Table 4. Hyper-parameters for the transformer are
chosen according to a jet classification task. We scan over the following hyper-parameter ranges:"
REFERENCES,0.8669354838709677,"step size ∈{0.01, 0.1, 1.0, 10}
steps ∈{5, 10, 24, 60}
lr ∈{1e-3, 1e-4}"
REFERENCES,0.8709677419354839,"We found that fewer steps {5, 10} make training unstable. Thus the model prefers a relatively larger number
of steps and a smaller step size. However, 60 steps MCMC takes much longer time to train and no significant
improvement was observed for even longer chains. We balance training efficiency and effective learning by
choosing 24 steps."
REFERENCES,0.875,"The MCMC chains are initialized with Gaussian noises, where the constituent features are sampled from the
following distributions: log pT i ∼N(2, 1), ηi ∼N(0, 0.12), and ϕi ∼N(0, 0.22)."
REFERENCES,0.8790322580645161,"Data
input features
{(log(pT ), η, ϕ)i}N
i=1
input length
N=40 with zero-padding"
REFERENCES,0.8830645161290323,"Energy Function
Number of layers NL
8
Model dimension dmodel
128
Number of heads
16
Feed-forward dimension
1024
Dropout rate
0.1
Normalization
None"
REFERENCES,0.8870967741935484,"MCMC
Number of steps
24
Step size
0.1
Buffer size
10000
Resample rate
0.05
Noise
ϵ = 0.005"
REFERENCES,0.8911290322580645,"Regularization
L2 Regularization
0.1"
REFERENCES,0.8951612903225806,"Training
Batch size
128
Optimizer
Adam (β1 = 0.0, β2 = 0.999)
Learning rate
1e-4 (decay rate γ = 0.98)
Table 4: Model settings."
REFERENCES,0.8991935483870968,"B
Additional Results"
REFERENCES,0.9032258064516129,"B.1
Ablation Study"
REFERENCES,0.907258064516129,We explore the most crucial aspects of the model to test their functionality:
REFERENCES,0.9112903225806451,"• With an energy function approximated with a Multi-Layer-Perceptron (MLP) net, we were not able to
achieve quality generation.
• We also tried out the Hamiltonian Monte Carlo (HMC) [52] for the MCMC procedure. However, we
were not able to achieve good performance in these experiments."
REFERENCES,0.9153225806451613,"Results measured in the Jensen-Shannon Divergence of the high-level observables (pT and M) are recorded in
Table 5."
REFERENCES,0.9193548387096774,"Ablation
JSD (pT ) / 10−4
JSD (M) / 10−4"
REFERENCES,0.9233870967741935,"Energy Function
MLP
Fig. 5
Fig. 5
MCMC Dynamics
HMC
24
30
Table 5: Ablation study on different components of the model, training strategies, and training
techniques. Since the MLP-based model is not able to produce high-quality samples, we instead show
the observable distributions (Fig. 5) to visually show the failure patterns."
REFERENCES,0.9274193548387096,Figure 5: Typical high-level observable distributions for MLP-based models.
REFERENCES,0.9314516129032258,"B.2
Classification Performance of EBM-CLF"
REFERENCES,0.9354838709677419,"The classification accuracies for EBM-CLF (QCD/W/Top 3-way classification) are recorded in Table 6. EBM-
CLF performs on par with the fully supervised classifier ParticleNet [59], while EBM-CLF is trained on a
much smaller dataset. The corresponding confusion matrices are displayed in Fig. 6. When we empirically
down-weight the term log p(y|x) (decrease κ in Eq. 7), the classification performance drops correspondingly."
REFERENCES,0.9395161290322581,"log p(x, y) = log p(x) + κ log p(y|x) .
(7)"
REFERENCES,0.9435483870967742,"B.3
Additional Plots"
REFERENCES,0.9475806451612904,"In Fig. 7, we show the generated jet samples displayed in images on the (η, ϕ) plane. In Fig. 8, we show the
high-level observable distributions of generated jet samples of EBM-CLF. In Fig. 9, we show the background
mass distributions under different acceptance rates ϵ after cutting on the energy score of the standard EBM."
REFERENCES,0.9516129032258065,"Model
Top-1 Accuracy
Top-2 Accuracy"
REFERENCES,0.9556451612903226,"ParticleNet[59]
0.871
0.976
EBM-CLF (κ = 1.0)
0.850
0.969
EBM-CLF (κ = 0.5)
0.708
0.906
EBM-CLF (κ = 0.1)
0.679
0.852
Table 6: Classification performance of EBM-CLF on QCD/W/Top 3-way classification. κ denotes
the weight of the discriminative log-likelihood."
REFERENCES,0.9596774193548387,Figure 6: Confusion matrices for EBM-CLF(left) and ParticleNet(right).
REFERENCES,0.9637096774193549,"Figure 7: Jet images averaged over 10000 jet samples. From left to right, we show the initial random
noises (left-most), EBM-generated jet samples by the MCMC chains in intervals (middle), and Real
jets (right-most)."
REFERENCES,0.967741935483871,"C
Reproducibility Statement"
REFERENCES,0.9717741935483871,"To ensure reproducibility and encourage open-source practice in the HEP community, we release the code
implementation in https://github.com/taolicheng/EBM-HEP. The training sets and test sets are accessible
at [47, 14]. Due to difficulties in aligning model comparison protocols for different research groups, we thus
focus on methods with code publicly available, that serve as credible baselines, for model comparison."
REFERENCES,0.9758064516129032,"Figure 8: High-level observable distributions for the generated samples of EBM-CLF (orange) and
the data (blue)."
REFERENCES,0.9798387096774194,"0
50
100
150
200
250
300
350
MJ/GeV 10
5 10
4 10
3 10
2 a.u."
REFERENCES,0.9838709677419355,"QCD [ = 1.0]
QCD [ = 0.5]"
REFERENCES,0.9879032258064516,QCD [ = 0.2]
REFERENCES,0.9919354838709677,QCD [ = 0.1]
REFERENCES,0.9959677419354839,"Figure 9: Background mass distributions under different acceptance rates ϵ after cutting on the energy
score from the EBM."
