Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004291845493562232,"Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively
assess the human perceptual quality of omnidirectional images (ODIs) without
relying on pristine-quality image information. It is becoming more significant with
the increasing advancement of virtual reality (VR) technology. However, the quality
assessment of ODIs is severely hampered by the fact that the existing BOIQA
pipeline lacks the modeling of the observer‚Äôs browsing process. To tackle this issue,
we propose a novel multi-sequence network for BOIQA called Assessor360, which
is derived from the realistic multi-assessor ODI quality assessment procedure.
Specifically, we propose a generalized Recursive Probability Sampling (RPS)
method for the BOIQA task, combining content and details information to generate
multiple pseudo viewport sequences from a given starting point. Additionally, we
design a Multi-scale Feature Aggregation (MFA) module with a Distortion-aware
Block (DAB) to fuse distorted and semantic features of each viewport. We also
devise Temporal Modeling Module (TMM) to learn the viewport transition in the
temporal domain. Extensive experimental results demonstrate that Assessor360
outperforms state-of-the-art methods on multiple OIQA datasets. The code and
models are available at https://github.com/TianheWu/Assessor360."
INTRODUCTION,0.008583690987124463,"1
Introduction"
INTRODUCTION,0.012875536480686695,"With the development of VR-related techniques, viewers can enjoy a realistic and immersive ex-
perience with head-mounted displays (HMDs) [3, 20] by perceiving 360-degree omnidirectional
information. However, the acquired omnidirectional image, also named panorama, is not always of
high quality [12, 60]. Degradation may be introduced in any image processing [44, 58, 42], leading
to low-quality content that may be visually unpleasant for users. Consequently, developing suitable
quality metrics for panoramas holds considerable importance, as they can be utilized to guide research
in omnidirectional image processing and maintain high-quality visual content."
INTRODUCTION,0.017167381974248927,"Generally, most ODIs are stored in the equirectangular projection (ERP) format, which exhibits
considerable geometric deformation at different latitudes. This distortion can have a negative impact
on quality assessment. Therefore, as shown in Figure 1 (a), many researchers [36, 17, 43, 58, 14, 60]
explore viewport-based methods by projecting the original ERP image into many undeformed
viewports (actual content observed by users) and aggregate their features with 2D IQA method as the
ODI quality score. However, this conventional viewport-based pipeline lacks the modeling of the"
INTRODUCTION,0.02145922746781116,"‚àóTianhe Wu and Shuwei Shi contribute equally to this work.
‚Ä†Corresponding author."
INTRODUCTION,0.02575107296137339,Multiple Subjects
INTRODUCTION,0.030042918454935622,(evaluators)
INTRODUCTION,0.034334763948497854,"Multiple 
Scanpaths"
INTRODUCTION,0.03862660944206009,Average
INTRODUCTION,0.04291845493562232,Score 1
INTRODUCTION,0.04721030042918455,Score ùëõ
INTRODUCTION,0.05150214592274678,"Quality 
score"
INTRODUCTION,0.055793991416309016,"Realistic ODI Quality Assessment Procedure 
(b)"
INTRODUCTION,0.060085836909871244,Ordinary Viewport-based Pipeline
INTRODUCTION,0.06437768240343347,Quality Representation
INTRODUCTION,0.06866952789699571,"Quality 
score"
INTRODUCTION,0.07296137339055794,Omnidirectional Image
INTRODUCTION,0.07725321888412018,"Rectilinear 
Project"
INTRODUCTION,0.0815450643776824,ùëõDifferent Perspective Viewports
"D IQA 
METHOD",0.08583690987124463,"2D IQA 
Method"
"D IQA 
METHOD",0.09012875536480687,Feature 1
"D IQA 
METHOD",0.0944206008583691,Feature 2
"D IQA 
METHOD",0.09871244635193133,Feature ùëõ
"D IQA 
METHOD",0.10300429184549356,Aggregate
"D IQA 
METHOD",0.1072961373390558,"Generating different perspective viewports
Viewport quality representation
Score prediction (a)"
"D IQA 
METHOD",0.11158798283261803,"Observing and perceiving the quality of omnidirectional image
Recalling the quality of the observed viewports
Quality score rating"
"D IQA 
METHOD",0.11587982832618025,Observe
"D IQA 
METHOD",0.12017167381974249,Scanpath 1
"D IQA 
METHOD",0.12446351931330472,Scanpath ùëõ
"D IQA 
METHOD",0.12875536480686695,Omnidirectional Image
"D IQA 
METHOD",0.13304721030042918,Recall quality
"D IQA 
METHOD",0.13733905579399142,"Figure 1: Illustration of the existing ordinary viewport-based pipeline (a) and the realistic omnidirec-
tional image quality assessment procedure (b). Plenty of existing viewport-based methods follow
pipeline (a) which is inconsistent with authentic assessment procedure (b), causing the predicted
quality score to be far from the human perceptual quality score."
"D IQA 
METHOD",0.14163090128755365,"observer‚Äôs browsing process, causing the predicted quality to be far from the human perceptual quality,
especially when the ODI containing non-uniform distortion (such as stitching) [33, 12, 36, 61, 37].
In fact, during the process of observing ODIs, viewports are sequentially presented to viewers based
on their browsing paths, forming a viewport sequence."
"D IQA 
METHOD",0.1459227467811159,"While the viewport sampling techniques proposed by Sui et al.[33] and Zhang et al.[53] can generate
viewports with a fixed sequential order for ODIs, their methods lack the ability to generate versatile
sequences. Specifically, their methods will produce identical sequences for a given starting point,
and the sequential order of viewports will remain constant across different ODIs. This behavior is
inconsistent with the observations of multiple evaluators in realistic scenarios, leading to an inability
to provide subjectively consistent evaluation results. To address this issue, a logical approach is
to employ the scanpath prediction model for generating pseudo viewport sequences on the ODI
without an authentic scanpath. However, current scanpath prediction methods [47, 24, 32, 1, 34] are
developed for undistorted ODIs and mainly focus on high-level regions. As evaluators are required to
provide an accurate quality score for an ODI, their scanpaths are distributed across both low-quality
and high-detail regions. This makes it potentially unsuitable for direct application in BOIQA task
where all ODIs are distorted."
"D IQA 
METHOD",0.15021459227467812,"To move beyond these limitations, inspired by the realistic multi-assessor ODI quality assessment
procedure, shown in Figure 1 (b), we first propose a multi-sequence network called Assessor360
for BOIQA, which can simulate the authentic data scoring process to generate multiple viewport
sequences (corresponding to multiple scanpaths). Specifically, we propose Recursive Probability
Sampling (RPS) to generate multiple pseudo viewport sequences, combining semantic scene and
local distortion characteristics. In particular, based on Equator-guided Sampled Probability (ESP)
and Details-guided Sampled Probability (DSP), RPS will generate different viewport sequences
for the same starting point (details in Section 3.2). Furthermore, we develop Multi-scale Feature
Aggregation (MFA) with Distortion-aware Block (DAB) to effectively fuse viewport semantic and
distorted features for accurate quality perception. The Temporal Modeling Module (TMM) is devised
by applying GRU [5] module and MLP layers to learn the viewports temporal transition information in
a sequence and regress the aggregated features to the final score. Extensive experiments demonstrate
the superiority and effectiveness of proposed Assessor360 on multiple OIQA datasets (MVAQD [18],
OIQA [11], CVIQD [35], IQA-ODI [46], JUFE [12], JXUFE [33]). We summarize our contributions
into four points:"
"D IQA 
METHOD",0.15450643776824036,"‚Ä¢ We propose Assessor360 for BOIQA, which can leverage multiple different viewport
sequences to assess the ODI quality. To our knowledge, Assessor360 is the first pipeline to
simulate the authentic data scoring process in ODI quality assessment."
"D IQA 
METHOD",0.15879828326180256,"‚Ä¢ We propose an unlearnable method, Recursive Probability Sampling (RPS) that can combine
semantic scene and local distortion characteristics to generate different viewport sequences
for a given starting point."
"D IQA 
METHOD",0.1630901287553648,"‚Ä¢ We design Multi-scale Feature Aggregation (MFA) and Distortion-aware Block (DAB) to
characterize the integrated features of viewports and devise a Temporal Modeling Module
(TMM) to model the temporal correlation between viewports.
‚Ä¢ We apply our Assessor360 to two types of OIQA task datasets: one with real observed
scanpath data and the other without. Extensive experiments show that our Assessor360
largely outperforms state-of-the-art methods."
RELATED WORK,0.16738197424892703,"2
Related Work"
RELATED WORK,0.17167381974248927,"Omnidirectional Image Quality Assessment.
Similarly to traditional 2D IQA, according to the
reference information availability, OIQA can also be divided into three categories: full-reference
(FR), reduced-reference (RR), and no-reference (NR) OIQA, also known as blind OIQA [2]. Due to
the structural characteristics of the panorama and the complicated assessment process, OIQA has not
matured as much as 2D-IQA [48, 22, 13, 19, 15, 29]. Some researchers extend the 2D image quality
assessment metrics to the panorama, such as S-PSNR [51], CPP-PSNR [52], and WS-PSNR [37].
However, these methods are not consistent with the Human Visual System (HVS) and they are poorly
consistent with perceived quality [60, 43]. Although WS-SSIM [61] and S-SSIM [4] consider some
impacts of HVS, the availability of non-distortion reference ODIs severely hinders their applications
in authentic scenarios."
RELATED WORK,0.1759656652360515,"Therefore, some deep learning-based BOIQA methods [60, 43, 36, 16, 53, 46] are devised to achieve
better capabilities. Due to the geometric deformation present in ODIs in ERP format, many existing
BOIQA methods [59, 46, 60, 17] follow a similar pipeline: sampling viewports in a particular way
and simply regressing their features to the quality score. MC360IQA [36] first maps the sphere
into a cubemap, then employs CNN to aggregate each cubemap plane feature and regress them to a
score. ST360IQ [16] samples tangent viewports from the salient parts and uses ViT [10] to estimate
the quality of each viewport. VGCN [43] migrates the graph convolution network and pre-trained
DBCNN [56] to establish connections between different viewports. However, these methods ignore
the vital effect of the viewport sequences generated in multiple observers‚Äô browsing process, which
have been demonstrated by Sui et al.[33] and Fang et al.[12] on the perception of the ODI quality.
Viewport Sampling Strategies in OIQA.
Existing viewport sampling strategies can be categorized
into three modes: 1) Uniformly sampling without sequential order. Zhou et al.[59] and Fang et
al.[12] uniformly extract viewports over the sphere. Jiang et al.[17] and Sun et al.[36] use cube map
projection (CMP) and rotate the longitude to obtain several viewport groups. This sampling pattern
will cover the full areas, whether they are significant or non-significant. 2) Crucial region sampling
without sequential order. Xu et al.[43] leverage 2D Gaussian Filter [45] to acquire a heatmap and
generate viewports with corresponding locations. Tofighi et al.[16] apply ATSal [6] to predict salient
regions of the panorama, which gives help to sampling viewports. This mode incorporates HVS,
where most of the sampled viewport might be observed in the browsing process. Nevertheless, both
of the above methods only focus on the image content and do not concern the effect of the sequential
order between viewports in the quality assessment process of panoramas. 3) Sampling viewports
along the fixed direction. Sui et al.[33] first considers the effect of sequential order between viewports
in the browsing process. They default observers rotate their perspective in a specific direction along
the equator. Zhang et al.[53] further introduce ORB detection to capture key viewports and follow
Sui et al.[33] default sampling direction to extract viewports. In fact, different evaluators will produce
various scanpaths when viewing a panorama. Even if the same evaluator observes the same ODI
twice, the scanpath will be different. However, their sampling methods cause the sequential order of
the viewport to be fixed, and even for the different image contents, the sequential order of sampled
viewports is the same. This creates a significant gap with people‚Äôs actual browsing process, leading
to unreasonable modeling of the viewport sequence."
METHOD,0.18025751072961374,"3
Method"
OVERALL FRAMEWORK,0.18454935622317598,"3.1
Overall Framework
As shown in Figure 2 (a), the proposed Assessor360 framework consists of Recursive Probability Sam-
pling (RPS) scheme, Multi-scale Feature Aggregation (MFA) strategy, and Temporal Modeling Mod-
ule (TMM). Given a degraded ODI I, to be consistent with the authentic multi-assessor assessment,
we initialize N starting points X = {(ui, vi)}N
i=1, where ui ‚àà[‚àí90‚ó¶, 90‚ó¶] and vi ‚àà[‚àí180‚ó¶, 180‚ó¶]
are the corresponding latitude and longitude coordinates. Then, we apply the proposed RPS strategy
G with parameters Œòg to generate multiple viewport sequences S = {{Vi
j}M
j=1}N
i=1, where V denotes"
OVERALL FRAMEWORK,0.1888412017167382,Distortion-aware Block (DAB)
OVERALL FRAMEWORK,0.19313304721030042,"Conv 
3 √ó 3"
OVERALL FRAMEWORK,0.19742489270386265,"Conv 
1 √ó 1 GELU 1√ó ùüè√ó"
OVERALL FRAMEWORK,0.2017167381974249,"ùë™
ùüê
ùëØ√ó ùëæ√ó ùë™"
OVERALL FRAMEWORK,0.20600858369098712,"Conv 
1 √ó 1 GAP"
OVERALL FRAMEWORK,0.21030042918454936,1√ó ùüè√ó ùë™
OVERALL FRAMEWORK,0.2145922746781116,Channel Attention (CA) √ó ùíè
OVERALL FRAMEWORK,0.21888412017167383,Recursive Probability
OVERALL FRAMEWORK,0.22317596566523606,Sampling
OVERALL FRAMEWORK,0.22746781115879827,Starting points
OVERALL FRAMEWORK,0.2317596566523605,Swin Transformer
OVERALL FRAMEWORK,0.23605150214592274,"ùêàùêßùê©ùêÆùê≠: ùêéùêÉùêà
c
Stage 1"
OVERALL FRAMEWORK,0.24034334763948498,Stage 2
OVERALL FRAMEWORK,0.2446351931330472,Stage 3
OVERALL FRAMEWORK,0.24892703862660945,Stage 4
OVERALL FRAMEWORK,0.2532188841201717,"Conv
Conv
Conv
Conv"
OVERALL FRAMEWORK,0.2575107296137339,"DAB
DAB Conv Conv Conv c c GRU GRU MLP"
OVERALL FRAMEWORK,0.26180257510729615,"ùë†1 score
ùë†2 score"
OVERALL FRAMEWORK,0.26609442060085836,"ùë†ùëõscore
ODI score"
OVERALL FRAMEWORK,0.2703862660944206,average
OVERALL FRAMEWORK,0.27467811158798283,take last token
OVERALL FRAMEWORK,0.27896995708154504,"Recursive Probability 
Sampling (RPS)"
OVERALL FRAMEWORK,0.2832618025751073,"Calculate 
Entropy
Split"
OVERALL FRAMEWORK,0.2875536480686695,"ùíÜùüè
ùíÜùüê
ùíÜùüë ùíÜùüí
ùíÜùüì"
OVERALL FRAMEWORK,0.2918454935622318,"ùíÜùüî
ùíÜùüï
ùíÜùüñ"
OVERALL FRAMEWORK,0.296137339055794,Softmax
OVERALL FRAMEWORK,0.30042918454935624,Coordinate probability map
"OVERLAPPED NEIGHBOR 
PATCHES",0.30472103004291845,"8 overlapped neighbor 
patches"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3090128755364807,"Generate position 
chosen probabilities ùëùùëëùë†ùëù 1"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3133047210300429,Softmax
"OVERLAPPED NEIGHBOR 
PATCHES",0.31759656652360513,"Select Next 
Viewport"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3218884120171674,"(ùë¢ùëêùë°+Œîùë¢, ùë£ùëêùë°+Œîùë£)"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3261802575107296,"(ùë¢ùëêùë°, ùë£ùëêùë°+Œîùë£)
(ùë¢ùëêùë°‚àíŒîùë¢, ùë£ùëêùë°+ Œîùë£)"
"OVERLAPPED NEIGHBOR 
PATCHES",0.33047210300429186,"(ùë¢ùëêùë°+Œîùë¢, ùë£ùëêùë°‚àíŒîùë£)
(ùë¢ùëêùë°+Œîùë¢, ùë£ùëêùë°)
(ùë¢ùëêùë°, ùë£ùëêùë°‚àíŒîùë£)
(ùë¢ùëêùë°‚àíŒîùë¢, ùë£ùëêùë°‚àíŒîùë£)
(ùë¢ùëêùë°‚àíŒîùë¢, ùë£ùëêùë°)"
"OVERLAPPED NEIGHBOR 
PATCHES",0.33476394849785407,Calculating 8 neighbor patches coordinates
"OVERLAPPED NEIGHBOR 
PATCHES",0.33905579399141633,Equator-guided Sampled Probability (ESP) ùëùùëëùë†ùëù 2 ùëùùëëùë†ùëù 8
"OVERLAPPED NEIGHBOR 
PATCHES",0.34334763948497854,Details-guided Sampled Probability (DSP)
"OVERLAPPED NEIGHBOR 
PATCHES",0.34763948497854075,Softmax ùëÉùë•1 ùëÉùë•2 ùëÉùë•8
"OVERLAPPED NEIGHBOR 
PATCHES",0.351931330472103,"ùëùùëíùë†ùëù
1"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3562231759656652,"ùëùùëíùë†ùëù
2"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3605150214592275,"ùëùùëíùë†ùëù
8"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3648068669527897,"‡∑§ùëù1
‡∑§ùëù2 ‡∑§ùëù8 (b) ùë†1"
"OVERLAPPED NEIGHBOR 
PATCHES",0.36909871244635195,Viewport Sequences ùë†2 ùë†ùëõ
"OVERLAPPED NEIGHBOR 
PATCHES",0.37339055793991416,"Generating various viewport sequences by RPS method
Multi-scale Feature Aggregation (MFA)
Temporal Modeling Module (TMM)"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3776824034334764,(a) Architecture of proposed Assessor360
"OVERLAPPED NEIGHBOR 
PATCHES",0.38197424892703863,"Figure 2: Architecture of proposed Assessor360 for BOIQA (a). Our Assessor360 consists of
Recursive Probability Sampling (RPS) scheme (b), Multi-scale Feature Aggregation (MFA) strategy,
and Temporal Modeling Module (TMM)."
"OVERLAPPED NEIGHBOR 
PATCHES",0.38626609442060084,"each viewport, M denotes the length of each sequence. Next, to perceive the semantic scene and
distortion in each viewport, multi-scale features are extracted from multi-stage layers of our method
for quality assessment. We use MFA F with parameters Œòf to represent this process. The features
are aggregated from RH√óW √óC to R1√ó1√óD. In this case, C is the viewport‚Äôs original dimensions, H
and W are the height and width of the viewport, and D denotes the embedding dimension. Then, we
use TMM H with parameters Œòh to model each sequence viewport temporal transition information
and predict the final viewport sequence quality score. Finally, we average all predicted scores of each
sequence as the ODI‚Äôs quality QI. Overall, the whole process can be described as follows:"
"OVERLAPPED NEIGHBOR 
PATCHES",0.3905579399141631,QI = 1
"OVERLAPPED NEIGHBOR 
PATCHES",0.3948497854077253,"N
PN
i=1H(F(G(I, X; Œòg); Œòf); Œòh)
(1)"
RECURSIVE PROBABILITY SAMPLING,0.39914163090128757,"3.2
Recursive Probability Sampling
Recursive Probability Sampling (RPS) strategy can adaptively generate the probability of scene
transition direction based on prior knowledge of semantic context and degraded features in ODI. It
mainly consists of Equator-guided Sampled Probability (ESP) and Details-guided Sampled Probability
(DSP). The viewport sequence is generated by selecting a certain starting point and sampling viewports
based on generated probabilities.
Preprocessing for generating probabilities.
As illustrated in [47], transition direction and distance
are two important factors for locating the next viewport position. We first follow the theory of Moore
neighborhood [28] and define K neighbor (K = 8) transition directions from the center coordinate
of the viewport. Following [31, 33], the transition distance (‚àÜu, ‚àÜv) is set to (24‚ó¶, 24‚ó¶), avoiding
sampling overly overlapped viewports. In details, given a current position xt
c = (ut
c, vt
c), we first
generate the corresponding viewport Vt from I by rectilinear projection [49] R. Then, we uniformly
split Vt into K + 1 overlapped patches, including K neighbor patches Pt = {Pt
i }K
i=1 and one central
patch Pt
c with height H"
AND WIDTH W,0.4034334763948498,2 and width W
AND WIDTH W,0.40772532188841204,"2 . As shown in Figure 2 (b), the K direction sampled coordinates
X t
neighbor = {xt
i}K
i=1 can be calculated by the central patch Pt
c coordinate xt
c and (‚àÜu, ‚àÜv). During
the browsing process, assessors are not only drawn to the high-level scenario but also focus on
low-level texture and details regions [35, 11] to give a reasonable quality score. Therefore, according
to the generalized prior content information and pixel-level details metric, we present ESP and DSP
for each direction sampled coordinate in X t
neighbor. Then, we choose the next sampled viewport Vt+1
position xt+1
c
based on them.
Equator-guided Sampled Probability (ESP).
Since the equator entails more scene information
[7, 31], we introduce the prior equator bias [9] M to constrain the sampled viewport near the equator.
Concretely, the prior equator bias obeys a Gaussian distribution with a mean of 0 and a standard
deviation of 0.2 in latitude. Regions near the equator have higher sampled weights and regions close"
AND WIDTH W,0.41201716738197425,"Algorithm 1 Viewport Sequence Generation (RPS Algorithm)
Input: N starting points {xi}N
i=1; an ODI I; rectilinear projection R; ESP calculation function
Fesp(X); DSP calculation function Fdsp(P); selecting function Œì(X|Àúp)
Output: A set of N length M viewport sequences {si = {Vt}M
t=1}N
i=1;
1: for i = 1 ‚ÜíN do
2:
Initialize the current coordinate x ‚Üêxi
3:
for t = 1 ‚ÜíM do
4:
Generate viewport by the current coordinate V ‚ÜêR(x, I)
5:
Split V to obtain overlapped neighbor patches P and calculate sampled coordinate X
6:
Calculate ESP and DSP pesp ‚ÜêFesp(X), pdsp ‚ÜêFdsp(P)
7:
Generate next viewport coordinate x ‚ÜêŒì(X|Aggregate(pesp, pdsp))"
AND WIDTH W,0.41630901287553645,"8:
Sequentially gather generated M viewports {Vt}M
t=1 as a viewport sequence si
9: Gather generated N viewport sequences {si}N
i=1 as the output"
AND WIDTH W,0.4206008583690987,"to the poles have relatively low sampled weights. We apply the softmax function to M to obtain
the coordinate probability map Mp where each coordinate corresponds to a sampled probability.
Then we take X t
neighbor sampled probabilities {pi
x}K
i=1 from Mp. Meanwhile, due to the inhibition
of return (IOR) [27] where regions that have been fixated by the eyes have a lower probability of
being fixated again in the near future, we multiply pk
x with a decreasing factor Œ≥ where k is the
index of the viewport coordinate has been generated. Finally, we apply softmax function to calculate
pesp = {pi
esp}K
i=1. Overall, the ESP calculation function Fesp(X) can be defined as:"
AND WIDTH W,0.4248927038626609,"Fesp(X) = Softmax(Z ¬∑ Mp(X t
neighbor)), Z = {1, . . . , Œ≥, . . . , 1}K
(2)"
AND WIDTH W,0.4291845493562232,"Details-guided Sampled Probability (DSP).
To efficiently measure the texture complexity of each
patch, we use pixel-level information entropy E. The mathematical expression is:"
AND WIDTH W,0.4334763948497854,"E(Pt
i ) = H‚Ä≤
X m=1 W ‚Ä≤
X"
AND WIDTH W,0.43776824034334766,"n=1
‚àíp(Pt
i [m, n]) log2 p(Pt
i [m, n])
(3)"
AND WIDTH W,0.44206008583690987,"where Pt
i [m, n] is the gray value of each pixel at position (m, n) in the patch Pt
i, and p(¬∑) is the
probability of occurrence of each gray value. After calculating the entropy of each neighbor patch,
we normalize it with the softmax function to obtain pdsp = {pi
dsp}K
i=1 for X t
neighbor. The DSP
calculation function Fdsp(P) can be formulated as:"
AND WIDTH W,0.44635193133047213,"Fdsp(P) = Softmax(E(Pt))
(4)"
AND WIDTH W,0.45064377682403434,"Generating viewport sequences.
Ultimately, we multiply the ESP and DSP with a scale factor Œ≤ to
get the integrated probability. We use the softmax function to obtain the final probability distribution
and select the next viewport position xt+1
c
according to it, which can be written as:"
AND WIDTH W,0.45493562231759654,"xt+1
c
= Œì(X t|Softmax(pdsp ¬∑ pesp ¬∑ Œ≤))
(5)
where Œì(X|Àúp) is the selecting function based on the set of probability Àúp. We can recursively generate
viewport Vt+1 with xt+1
c
and keep performing the RPS strategy until the number of viewports in the
sequence reaches the desired length. The whole generation algorithm G is shown in Algorithm 1."
MULTI-SCALE FEATURE AGGREGATION,0.4592274678111588,"3.3
Multi-scale Feature Aggregation
To represent the semantic information and distortion pattern of each viewport, which are assumed
as two key factors of quality assessment, we aggregate multi-scale features of the viewport. We
first extract features from each stage in pre-trained Swin Transformer [23, 30]. The outputs of the
first two stages F1, F2 are more sensitive to the distortion pattern, while the outputs of the last two
stages F3, F4 tend to capture the abstract features (details in Supplementary Materials). Before
fusing multi-scale features, we first employ four 1 √ó 1 convolution layers to reduce the feature
dimension of the output to D. Then, to further emphasize the local degradation, we devise and apply
the Distortion-aware Block (DAB) which includes a 3 √ó 3 convolution layer following n Channel
Attention (CA) operations to the reduced shallow features ÀÜF1, ÀÜF2. This operation can help the model
to better perceive the distortion pattern in the channel dimension, achieving distortion-aware capacity.
Finally, we concatenate and integrate these features with Global Average Pooling (GAP) and multiple
1 √ó 1 convolution layers to get the quality-related representation. After that, each aggregated feature
Ffuse will be sent to TMM for viewport sequence quality assessment."
MULTI-SCALE FEATURE AGGREGATION,0.463519313304721,"Table 1: Quantitative comparison of the state-of-the-art methods and proposed Assessor360. The best
are shown in bold, and the second best (except ours) are underlined. Two baselines w/ ERP and w/
CMP mean that we replace input viewport sequences generated by RPS with ERP and CMP."
MULTI-SCALE FEATURE AGGREGATION,0.4678111587982833,"Type
Method
MVAQD
OIQA
IQA-ODI
CVIQD
SRCC
PLCC
SRCC
PLCC
SRCC
PLCC
SRCC
PLCC"
MULTI-SCALE FEATURE AGGREGATION,0.4721030042918455,"FR-IQA
methods"
MULTI-SCALE FEATURE AGGREGATION,0.47639484978540775,"PSNR
0.8150
0.7591
0.3929
0.3893
0.4018
0.4890
0.8015
0.8425
SSIM [39]
0.8272
0.7202
0.3402
0.2307
0.5014
0.5686
0.6737
0.7273
MS-SSIM [40]
0.8032
0.7136
0.5750
0.5084
0.7434
0.8389
0.9218
0.9272
WS-PSNR [37]
0.8152
0.7638
0.3829
0.3678
0.3780
0.4708
0.8039
0.8410
WS-SSIM [61]
0.8236
0.5328
0.6020
0.3537
0.5325
0.7098
0.8632
0.7672
VIF [54]
0.8687
0.8436
0.4284
0.4158
0.7109
0.7696
0.9502
0.9370
DISTS [8]
0.7911
0.7440
0.5740
0.5809
0.8513
0.8723
0.8771
0.8613
LPIPS [55]
0.8048
0.7336
0.5844
0.4292
0.7355
0.7411
0.8236
0.8242"
MULTI-SCALE FEATURE AGGREGATION,0.48068669527896996,"NR-IQA
methods"
MULTI-SCALE FEATURE AGGREGATION,0.48497854077253216,"NIQE [26]
0.6785
0.6880
0.8539
0.7850
0.6645
0.5637
0.9337
0.8392
BRISQUE [25]
0.8408
0.8345
0.8213
0.8206
0.8171
0.8651
0.8269
0.8199
PaQ-2-PiQ [50]
0.3251
0.3643
0.1667
0.2102
0.0201
0.0419
0.7376
0.6500
MANIQA [48]
0.5531
0.5718
0.4555
0.4171
0.2642
0.2776
0.6013
0.6142
MUSIQ [19]
0.5436
0.6117
0.3216
0.3087
0.0565
0.0983
0.3483
0.3678
CLIP-IQA [38]
0.5862
0.4941
0.2330
0.2531
0.0927
0.1929
0.4884
0.4347
LIQE [57]
0.6837
0.7539
0.7634
0.7419
0.8551
0.9020
0.8594
0.8086
SSP-BOIQA [58]
0.7838
0.8406
0.8650
0.8600
-
-
0.8614
0.9077
MP-BOIQA [17]
0.8420
0.8543
0.9066
0.9206
-
-
0.9235
0.9390
MC360IQA [36]
0.6605
0.6977
0.9071
0.8925
0.8248
0.8629
0.8271
0.8240
SAP-net [46]
-
-
-
-
0.9036
0.9258
-
-
VGCN [43]
0.8422
0.9112
0.9515
0.9584
0.8117
0.8823
0.9639
0.9651
AHGCN [14]
-
-
0.9647
0.9682
-
-
0.9617
0.9658
baseline w/ ERP
0.9076
0.9240
0.8961
0.8857
0.9098
0.9196
0.9330
0.9485
baseline w/ CMP
0.8966
0.9324
0.9216
0.9170
0.9105
0.9122
0.9390
0.9412
Assessor360
0.9607
0.9720
0.9802
0.9747
0.9573
0.9626
0.9644
0.9769"
TEMPORAL MODELING MODULE,0.4892703862660944,"3.4
Temporal Modeling Module
The browsing process of ODI naturally leads to the temporal correlation. The recency effect indicates
that users are more likely to evaluate the overall image quality affected by the viewports they have
recently viewed, especially during prolonged exploration periods. To model this relation, we introduce
the GRU module to learn the viewport transition. Due to the fact that the last token encodes the
most recent information and the representation at the last time step involves the whole temporal
relationships of a sequence, we use MLP layers to regress the last feature output by the GRU module
to the sequence quality score."
EXPERIMENTS,0.49356223175965663,"4
Experiments"
IMPLEMENTATION DETAILS,0.4978540772532189,"4.1
Implementation Details
We set the field of view (FoV) to the 110‚ó¶following [12, 33]. We use pre-trained Swin Transformer
[23] (base version) as our feature extraction backbone. The input viewport size H √ó W is fixed to
224 √ó 224. The number of viewport sequences N is set to 3 and the length of each sequence M is
set to 5. We set the coordinates of N starting points to be (0‚ó¶, 0‚ó¶). The reduced dimension D is 128
and the number of GRU modules is set to 6. The number of CA operations n is 4. We set Œ≥ = 0.7
and Œ≤ = 100 as decreasing factor and scale factor values respectively."
IMPLEMENTATION DETAILS,0.5021459227467812,"For a fair comparison, we randomly split 80% ODIs of each dataset for training, and the remaining
20% is used for testing following [53, 18, 12, 17]. To eliminate bias, we run a random train-test
splitting process ten times and show the median result. We train 300 epochs with batch size 4 on
CVIQD [35], OIQA [11], IQA-ODI [46], and MVAQD [18] datasets without the authentic scanpath
data. Respectively, we compare our RPS with two advanced learning-based scanpath prediction
methods ScanGAN360 [24] and ScanDMM [32] on JUFE [12] and JXUFE [33] datasets which
have the authentic scanpath data. For optimization, we use Adam [21] and the learning rate is set
to 1 √ó 10‚àí5 in the training phase. We employ MSE loss to train our model. We use Spearman
rank-ordered correlation (SRCC) and Pearson linear correlation (PLCC) as the evaluation metrics."
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5064377682403434,"4.2
Comparing with the State-of-the-art Methods
We conduct a comparative analysis of Assessor360 with eight FR methods and thirteen NR methods.
The quantitative comparison results are presented in Table 1, demonstrating significant performance"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5107296137339056,"Table 2: Cross-dataset validation SRCC and PLCC results of SOTA methods. These models (except
WS-PSNR [37] and WS-SSIM [61]) are trained on CVIQD [35], OIQA [11] and MVAQD [18]
datasets (80% set) and tested on three other datasets (full set)."
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5150214592274678,"Method
CVIQD
OIQA
MVAQD
OIQA
IQA-ODI
MVAQD
CVIQD
IQA-ODI
MVAQD
CVIQD
OIQA
IQA-ODI SRCC"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.51931330472103,"WS-PSNR
0.5027
0.4360
0.7225
0.7638
0.4360
0.7225
0.7638
0.5027
0.4360
WS-SSIM
0.5442
0.5032
0.7930
0.6625
0.5032
0.7930
0.6625
0.5442
0.5032
MC360IQA
0.4189
0.7114
0.0296
0.7044
0.5687
0.4081
0.0373
0.0025
0.0486
VGCN
0.2361
0.2875
0.2452
0.6932
0.3873
0.4682
0.4650
0.6227
0.3921
Assessor360
0.4597
0.8610
0.5640
0.8430
0.8751
0.6417
0.8756
0.7765
0.8646"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5236051502145923,"PLCC
WS-PSNR
0.4701
0.5468
0.6962
0.7895
0.5468
0.6962
0.7895
0.4701
0.5468
WS-SSIM
0.4363
0.5941
0.6246
0.6536
0.5941
0.6246
0.6536
0.4363
0.5941
MC360IQA
0.4295
0.7872
0.0404
0.7368
0.5930
0.4238
0.0430
0.0202
0.0646
VGCN
0.2582
0.3127
0.2467
0.5929
0.3551
0.2419
0.3420
0.4642
0.3870
Assessor360
0.5332
0.9032
0.5824
0.8636
0.9137
0.6565
0.7232
0.7287
0.8541"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5278969957081545,"Table 3: Cross-dataset validation SRCC results
of SOTA methods. These models are trained on
MVAQD [18] (full set) and tested on CVIQD
[35] and OIQA [11] all distortion data except
MP-BOIQA (removing AVC on CVIQD)."
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5321888412017167,"Testing set
MC360IQA
MP-BOIQA
Assessor360"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5364806866952789,"OIQA
0.2542
0.5043
0.6658
CVIQD
0.4749
0.7992
0.8994"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5407725321888412,"Table 4: Quantitative comparison of using differ-
ent viewport sequence generation methods on OIQA
[11] and MVAQD [18]."
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5450643776824035,"Generation Method
OIQA
MVAQD
SRCC
PLCC
SRCC
PLCC"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5493562231759657,"Random Generation
0.9461
0.9444
0.9359
0.9543
ScanGAN360
0.9705
0.9670
0.9493
0.9694
ScanDMM
0.9652
0.9634
0.9558
0.9612
RPS (Ours)
0.9802
0.9747
0.9607
0.9720"
COMPARING WITH THE STATE-OF-THE-ART METHODS,0.5536480686695279,"improvements across all datasets against the state-of-the-art methods. Note that these performance
enhancements are obtained without training with the 2D IQA dataset, as employed in VGCN [43].
Specifically, Assessor360 outperforms VGCN by 3% in terms of SRCC on the OIQA dataset and
shows a 1.2% increase in PLCC on the CVIQD dataset. Additionally, our method exhibits a notable
improvement of up to 1.6% in SRCC compared to AHGCN [14]."
CROSS-DATASET EVALUATION,0.5579399141630901,"4.3
Cross-Dataset Evaluation"
CROSS-DATASET EVALUATION,0.5622317596566524,"To assess the generalization capability of our method, we perform cross-validation on the CVIQD [35]
and OIQA [11] datasets and compare it against two widely used state-of-the-art methods: MC360IQA
[36] and MP-BOIQA [17]. All models are trained on the MVAQD [18] dataset and subsequently
tested on the CVIQD and OIQA datasets. The results, presented in Table 3, demonstrate that our
method exhibits superior generalization performance compared to the other models. It is worthy
to note that, there exists a significant domain gap between the CVIQD and MVAQD datasets. The
CVIQD dataset primarily focuses on degradations such as JPEG, H.264/AVC, and H.265/HEVC,
while the MVAQD dataset concentrates on JPEG, JP2K, HEVC, WN, and GB. This discrepancy in
focus poses a challenge for MC360IQA and MP-BOIQA, as they use the same perspective viewports
to evaluate different distorted ODIs, leading to poor generalization. In contrast, our proposed RPS
overcomes this domain gap effectively by generating different viewport sequences for ODIs with
different distortions. It consistently performs well, showcasing its effectiveness in addressing the
challenges posed by diversely distorted ODIs."
CROSS-DATASET EVALUATION,0.5665236051502146,"Furthermore, to verify that the performance presented in Table 1 is not due to the overfitting, we retrain
MC360IQA, VGCN [43] and Assessor360 on CVIQD, OIQA and MVAQD datasets, using 80% of
the dataset for training and the remaining 20% for testing. We then select the model weights with the
highest performance on the testset for cross-dataset testing for fair generalization comparisons. The
results shown in the Table 2 clearly demonstrate the strong generalization of our method compared to
VGCN and MC360IQA. Notably, during instances of training on the MVAQD dataset and subsequent
testing on the IQA-ODI [46] dataset, our approach outperforms MC360IQA in terms of SRCC by
an approximate margin of 0.8, and surpasses VGCN in SRCC by 0.5. Concurrently, when training
on the CVIQD dataset, characterized by fewer distortion types, and testing on the MVAQD dataset,
which encompasses more distortion types (JP2K, WN, GB), our approach attains elevated PLCC
values of 0.5 and 0.3, respectively, as compared to MC360IQA and VGCN."
CROSS-DATASET EVALUATION,0.5708154506437768,"Table 5: Quantitative comparison of different generation methods (RPS vs ScanGAN360 [24] and
ScanDMM [32]) with metrics of scanpath prediction task on JUFE [12] and JXUFE [33] datasets
with authentic scanpaths."
CROSS-DATASET EVALUATION,0.575107296137339,"Published Time
Generation Method
JUFE [12]
JXUFE [33]
LEV‚Üì
DTW‚Üì
REC‚Üë
LEV‚Üì
DTW‚Üì
REC‚Üë"
CROSS-DATASET EVALUATION,0.5793991416309013,"-
Random Baseline (lower bound)
35.21
1707.45
0.38
35.08
1695.93
0.38
TVCG22
ScanGAN360 [24]
32.53
1448.65
1.07
31.89
1427.55
1.14
CVPR23
ScanDMM [32]
31.23
1434.36
1.21
31.48
1438.29
1.12
-
RPS w/o DSP (Ours)
29.54
1471.82
2.14
29.99
1463.38
1.94
-
RPS (Ours)
29.48
1454.03
2.21
29.66
1422.85
2.07
-
Human Baseline (upper bound)
23.85
1309.29
3.78
26.73
1302.15
2.88"
CROSS-DATASET EVALUATION,0.5836909871244635,Gaussian noise
CROSS-DATASET EVALUATION,0.5879828326180258,Brightness discontinuity
CROSS-DATASET EVALUATION,0.592274678111588,Stitching
CROSS-DATASET EVALUATION,0.5965665236051502,Gaussian blur
CROSS-DATASET EVALUATION,0.6008583690987125,"Figure 3: Visual comparison of the generated viewport positions for different methods on ODIs with
four distortion types in JUFE. The brighter the area, the more viewports are generated in that area."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6051502145922747,"4.4
Effectiveness of Recursive Probability Sampling"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6094420600858369,"As mentioned in Section 1, there exist many learning-based scanpath prediction methods [24, 47, 32].
They seem to be able to assist in constructing viewport sequences. However, they are hardly
introduced to OIQA task. In this section, we first perform the quantitative comparison of the model
with RPS and two advanced 360-degree scanpath prediction methods, namely ScanGAN360 [24]
and ScanDMM [32] on the datasets without real observed scanpath. Subsequently, we compare the
position and sequential order of viewports generated by the three methods with the ground-truth
(GT) scanpaths by the metrics of scanpath prediction task and visualization to further validate the
superiority of RPS on JUFE [12] and JXUFE [33] datasets with real observed scanpath."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6137339055793991,"Quantitative comparison of performance.
We replace the proposed RPS method with Scan-
GAN360 and ScanDMM to generate sequences of viewports. The model was trained and tested using
these viewport sequences on OIQA [11] and MVAQD [18] datasets, maintaining the same length and
number of viewports for a fair comparison. Table 4 shows the quantitative results, demonstrating that
viewports generated from RPS yield superior performance compared to ScanGAN360 and ScanDMM.
Additionally, training using viewports generated from these methods outperforms those generated
using random schemes, highlighting the crucial role of suitable viewport sequences in the OIQA task."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6180257510729614,"Moreover, we conduct experiments by replacing original VGCN [43] sampling methods with RPS in
VGCN. We use RPS to sample the same number of viewports as [43] to train VGCN on IQA-ODI
[46] and MVAQD datasets. The results presented in Table 7 demonstrate a substantial performance
enhancement for VGCN achieved by the viewport sampled through RPS, resulting in an increase of
0.07 in SRCC for MVAQD. Additionally, this indicates that the viewport sampled with RPS closely
aligns with human observations."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6223175965665236,"Table 6: Quantitative comparison of different start-
ing point positions on MVAQD [18] dataset."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6266094420600858,"Position (latitude, longitude)
SRCC
PLCC"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.630901287553648,"(0‚ó¶, 0‚ó¶), (0‚ó¶, 0‚ó¶), (0‚ó¶, 0‚ó¶)
0.9607
0.9720
(60‚ó¶, 0‚ó¶), (60‚ó¶, 0‚ó¶), (60‚ó¶, 0‚ó¶)
0.9106
0.9312
(0‚ó¶, 120‚ó¶), (0‚ó¶, 0‚ó¶), (0‚ó¶, ‚àí60‚ó¶)
0.9599
0.9660
(60‚ó¶, 120‚ó¶), (60‚ó¶, 0‚ó¶), (60‚ó¶, ‚àí60‚ó¶)
0.9174
0.9455"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6351931330472103,"Table 7: Quantitative comparison of using origi-
nal VGCN sampling method and proposed RPS
on IQA-ODI [46] and MVAQD [18] datasets."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6394849785407726,"Method
IQA-ODI
MVAQD
SRCC
PLCC
SRCC
PLCC"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6437768240343348,"VGCN
0.8117
0.8823
0.8422
0.9112
VGCN-RPS
0.8382
0.8883
0.9122
0.9273"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.648068669527897,"Table 8: Ablation studies of each component in
proposed Assessor360 on MVAQD [18] dataset."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6523605150214592,"Method
Para (M)
SRCC
PLCC"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6566523605150214,"Assessor360 w/o MFA
88.53
0.8514
0.9171
Assessor360 w/o DAB
88.86
0.8437
0.8779
Assessor360 w/ GAP
88.69
0.9393
0.9587
Assessor360 w/ GRU
89.28
0.9607
0.9720"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6609442060085837,"Table 9: Quantitative comparison of using GT
sequences and sequences generated by RPS on
JUFE [12] dataset. Starting Point (SP)."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6652360515021459,"Viewport
Sequence
Good SP
Bad SP
SRCC
PLCC
SRCC
PLCC"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6695278969957081,"RPS (Ours)
0.6623
0.6365
0.5044
0.4946
GT Sequence
0.7158
0.7013
0.5400
0.5377"
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6738197424892703,"Comparison of viewport sampling positions and order.
Since each ODI in JUFE and JXUFE
comprises 30 and 22 scanpaths, respectively, we employ the three methods to generate 30 and 22
sequences, each comprising 20 viewports, for each ODI in JUFE and JXUFE, respectively. For each
GT sequence, we sample 20 viewports from 300 viewports with equal time intervals. We adopt
Levenshtein distance (LEV) and dynamic time warping (DTW) metrics, as used in ScanGAN360 and
ScanDMM, to evaluate the position and sequential order of the viewports. Furthermore, the viewing
behaviors and patterns can be evaluated using the recurrence measure (REC) [24]. Each metric is
calculated by comparing each generated sequence against all the GT sequences and computing the
average, resulting in the final value. Besides, to establish an upper bound for each metric, we compute
the Human Baseline by averaging the results of comparing each GT sequence against all other GT
sequences [41]. Similarly, we establish a lower bound by randomly sampling viewports from the
ODI, referred to as the Random Baseline."
EFFECTIVENESS OF RECURSIVE PROBABILITY SAMPLING,0.6781115879828327,"The quantitative comparison results are reported in Table 5. The proposed unlearnable method
outperforms learning-based models regarding the LEV and REC metrics. Specifically, it achieves a
LEV improvement of 1.75 and 1.82 compared to ScanDMM [32], and a REC improvement of 1.14
and 0.93 compared to ScanGAN360 [24]. Additionally, our method achieves comparable performance
to ScanDMM and ScanGAN360 on the DTW metric for both datasets. Figure 3 presents the positions
of generated viewports for different generation methods. The panorama is divided into 72 regions,
with each region spanning 15‚ó¶degrees of latitude and 30‚ó¶degrees of longitude. It can be observed
that the positions generated by the learning-based method are relatively dispersed, while the positions
generated by RPS are concentrated near the equator. The concentration gradually decreases with
increasing latitude, aligning closely with the ground truth (GT) position."
ABLATION STUDIES,0.6824034334763949,"4.5
Ablation Studies"
ABLATION STUDIES,0.6866952789699571,"Impact of the initialization of the starting point.
For the ODI without real scanpath data, we
initialize the coordinates of N starting points to be (0‚ó¶, 0‚ó¶) followed by [33]. We conduct exper-
iments to assess the impact of different initial starting points positions on the final performance.
Results shown in Table 6 revels that the model‚Äôs performance displays relatively minor fluctuations
during longitudinal shifts, yet experiences a substantial decline when changing larger latitudes (from
points less frequented by humans as starting positions). This divergence arises due to the model‚Äôs
misalignment with authentic browsing behaviors (humans tend to start their panoramic observations
closer to the equator). It further emphasizes that simulating actual scanpaths assists in more accurate
image quality evaluation."
ABLATION STUDIES,0.6909871244635193,"Effectiveness of MFA and TMM.
We exclude the multi-scale feature aggregation (MFA) and
distortion-aware Block (DAB) to assess their effectiveness in MVAQD [18]. Specifically, for the
previous experiment, we utilize the final stage of the Swin Transformer [23] for feature output. The
results, shown in Table 8, indicate that MFA and DAB substantially contribute to the performance,
with only a minor increase in the network parameters. The GRU [5] module in TMM is replaced
with global average pooling (GAP) for predicting the sequence quality score. The effectiveness of the
GRU module in representing temporal transition information is demonstrated in Table 8, and it can
assist the model in predicting more accurate quality scores."
ABLATION STUDIES,0.6952789699570815,"Excessive viewports hamper 
performance"
ABLATION STUDIES,0.6995708154506438,"Excessive viewports hamper 
performance"
ABLATION STUDIES,0.703862660944206,Figure 4: Performance of different number and length of viewport sequence on MVAQD [18].
ABLATION STUDIES,0.7081545064377682,"Impact of the number and length of the viewport sequence.
We test N = 1, 3, 5 three different
numbers of viewport sequences with varying sequence lengths on MVAQD [18] dataset. The findings,
shown in Figure 4, reveal that as the sequence length increases, there is a decreasing trend in model
performance across all three numbers of sequences. This suggests that an excessive number of
viewports may introduce redundant information, potentially disrupting the training process of the
network. Furthermore, our experiments demonstrate that incorporating multiple viewport sequences,
the model can capture a broader range of perspectives of the scene, thereby better reflecting the rating
process of ODIs and achieving improved robustness."
PROXIMATE TO HUMAN-OBSERVED PERFORMANCE,0.7124463519313304,"4.6
Proximate to Human-Observed Performance
We conduct a comparative analysis between the performance achieved using ground-truth (GT)
sequences and pseudo sequences generated by RPS on the JUFE dataset [12]. In the JUFE dataset,
the GT sequences are annotated based on whether they originate from good or bad starting points
(details in Supplementary Materials). Therefore, for each starting point, we use RPS to generate those
sequences with the same number and length of sequences compared to GT sequences. Then, we
apply the generated sequences and GT sequences as the input of our proposed network. The results
shown in Table 9 highlight a close gap between the contributions of GT sequences and our generated
sequences. This result emphasizes the significance of proximity to human observation in enhancing
the model‚Äôs capabilities. Meanwhile, there is still a large exploration space for future methods to
better incorporate human‚Äôs sequences in OIQA task."
CONCLUSION,0.7167381974248928,"5
Conclusion"
CONCLUSION,0.721030042918455,"This paper introduces a novel multi-sequence network named Assessor360 for BOIQA based on
a realistic assessment procedure. Specifically, we design Recursive Probability Sampling (RPS)
to generate viewport sequences based on the semantic scene and the distortion. Additionally, we
propose Multi-scale Feature Aggregation (MFA) with Distortion-aware Block (DAB) to combine
distorted and semantic features of viewports. Temporal Modeling Module (TMM) is introduced to
learn the temporal transition of viewports. We demonstrate the high performance of Assessor360 on
multiple OIQA datasets and validate the effectiveness of RPS by comparing it with two advanced
learning-based models used for scanpath prediction. Limitation is that the transition direction and
distance in RPS are fixed, resulting in equally spaced distances between viewports. However, we
have confidence that our analyses and the proposed pipeline can provide long-term valuable insights
for future OIQA task."
CONCLUSION,0.7253218884120172,"Acknowledgments.
This work was partly supported by the National Natural Science Foun-
dation of China (Grant No.
61991451) and the Shenzhen Science and Technology Program
(JSGG20220831093004008). The author would like to thank Xiangjie Sui at Jiangxi University of
Finance and Economics for his inspiration."
REFERENCES,0.7296137339055794,References
REFERENCES,0.7339055793991416,"[1] Marc Assens Reina, Xavier Giro-i Nieto, Kevin McGuinness, and Noel E O‚ÄôConnor. Saltinet: Scan-path
prediction on 360 degree images using saliency volumes. In Proceedings of the IEEE International"
REFERENCES,0.7381974248927039,"Conference on Computer Vision Workshops, pages 2331‚Äì2338, 2017."
REFERENCES,0.7424892703862661,"[2] Sebastian Bosse, Dominique Maniry, Klaus-Robert M√ºller, Thomas Wiegand, and Wojciech Samek. Deep
neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image
Processing, 27(1):206‚Äì219, 2017."
REFERENCES,0.7467811158798283,"[3] Meixu Chen, Yize Jin, Todd Goodall, Xiangxu Yu, and Alan Conrad Bovik. Study of 3d virtual reality
picture quality. IEEE Journal of Selected Topics in Signal Processing, 14(1):89‚Äì102, 2019."
REFERENCES,0.7510729613733905,"[4] Sijia Chen, Yingxue Zhang, Yiming Li, Zhenzhong Chen, and Zhou Wang. Spherical structural similarity
index for objective omnidirectional video quality assessment. In IEEE International Conference on
Multimedia and Expo, pages 1‚Äì6, 2018."
REFERENCES,0.7553648068669528,"[5] Kyunghyun Cho, Bart Van Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078, 2014."
REFERENCES,0.759656652360515,"[6] Yasser Dahou, Marouane Tliba, Kevin McGuinness, and Noel O‚ÄôConnor. Atsal: An attention based
architecture for saliency prediction in 360 videos. In Pattern Recognition. ICPR International Workshops
and Challenges: Virtual Event, January 10‚Äì15, 2021, Proceedings, Part III, pages 305‚Äì320, 2021."
REFERENCES,0.7639484978540773,"[7] Xin Deng, Hao Wang, Mai Xu, Yichen Guo, Yuhang Song, and Li Yang. Lau-net: Latitude adaptive
upscaling network for omnidirectional image super-resolution. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9189‚Äì9198, 2021."
REFERENCES,0.7682403433476395,"[8] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. Image quality assessment: Unifying structure
and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2567‚Äì2581,
2020."
REFERENCES,0.7725321888412017,"[9] Yasser Abdelaziz Dahou Djilali, Kevin McGuinness, and Noel E O‚ÄôConnor. Simple baselines can fool
360deg saliency metrics. In Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops, pages 3750‚Äì3756, 2021."
REFERENCES,0.776824034334764,"[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, and et al. Weissenborn. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.7811158798283262,"[11] Huiyu Duan, Guangtao Zhai, Xiongkuo Min, Yucheng Zhu, Yi Fang, and Xiaokang Yang. Perceptual
quality assessment of omnidirectional images. In 2018 IEEE International Symposium on Circuits and
Systems, pages 1‚Äì5, 2018."
REFERENCES,0.7854077253218884,"[12] Yuming Fang, Liping Huang, Jiebin Yan, Xuelin Liu, and Yang Liu. Perceptual quality assessment of
omnidirectional images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36,
pages 580‚Äì588, 2022."
REFERENCES,0.7896995708154506,"[13] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of
smartphone photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3677‚Äì3686, 2020."
REFERENCES,0.7939914163090128,"[14] Jun Fu, Chen Hou, Wei Zhou, Jiahua Xu, and Zhibo Chen. Adaptive hypergraph convolutional network
for no-reference 360-degree image quality assessment. In Proceedings of the 30th ACM International
Conference on Multimedia, pages 961‚Äì969, 2022."
REFERENCES,0.7982832618025751,"[15] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, and et al. Timofte. Ntire 2022 challenge on perceptual
image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pages 951‚Äì967, 2022."
REFERENCES,0.8025751072961373,"[16] Nafiseh Jabbari Tofighi, Mohamed Hedi Elfkir, Nevrez Imamoglu, Cagri Ozcinar, Erkut Erdem, and
Aykut Erdem. St360iq: No-reference omnidirectional image quality assessment with spherical vision
transformers. arXiv, 2023."
REFERENCES,0.8068669527896996,"[17] Hao Jiang, Gangyi Jiang, Mei Yu, Ting Luo, and Haiyong Xu. Multi-angle projection based blind omni-
directional image quality assessment. IEEE Transactions on Circuits and Systems for Video Technology,
32(7):4211‚Äì4223, 2021."
REFERENCES,0.8111587982832618,"[18] Hao Jiang, Gangyi Jiang, Mei Yu, Yun Zhang, You Yang, Zongju Peng, Fen Chen, and Qingbo Zhang.
Cubemap-based perception-driven blind quality assessment for 360-degree images. IEEE Transactions on
Image Processing, 30:2364‚Äì2377, 2021."
REFERENCES,0.8154506437768241,"[19] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality
transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
5148‚Äì5157, 2021."
REFERENCES,0.8197424892703863,"[20] Hak Gu Kim, Heoun-Taek Lim, and Yong Man Ro. Deep virtual reality image quality assessment with
human perception guider for omnidirectional image. IEEE Transactions on Circuits and Systems for Video
Technology, 30(4):917‚Äì928, 2019."
REFERENCES,0.8240343347639485,"[21] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.8283261802575107,"[22] Shanshan Lao, Yuan Gong, Shuwei Shi, Sidi Yang, Tianhe Wu, Jiahao Wang, Weihao Xia, and Yujiu Yang.
Attentions help cnns see better: Attention-based hybrid image quality assessment network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 1140‚Äì1149,
2022."
REFERENCES,0.8326180257510729,"[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 10012‚Äì10022, 2021."
REFERENCES,0.8369098712446352,"[24] Daniel Martin, Ana Serrano, Alexander W Bergman, Gordon Wetzstein, and Belen Masia. Scangan360: A
generative model of realistic scanpaths for 360 images. IEEE Transactions on Visualization and Computer
Graphics, 28(5):2003‚Äì2013, 2022."
REFERENCES,0.8412017167381974,"[25] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in
the spatial domain. IEEE Transactions on Image Processing, 21(12):4695‚Äì4708, 2012."
REFERENCES,0.8454935622317596,"[26] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a ‚Äúcompletely blind‚Äù image quality analyzer.
IEEE Signal Processing Letters, 20(3):209‚Äì212, 2012."
REFERENCES,0.8497854077253219,"[27] Michael I Posner, Yoav Cohen, et al. Components of visual orienting. Attention and performance X:
Control of language processes, 32:531‚Äì556, 1984."
REFERENCES,0.8540772532188842,"[28] Pratibha Sharma, Manoj Diwakar, and Niranjan Lal. Edge detection using moore neighborhood. Interna-
tional Journal of Computer Applications, 61(3), 2013."
REFERENCES,0.8583690987124464,"[29] Shuwei Shi, Qingyan Bai, Mingdeng Cao, Weihao Xia, Jiahao Wang, Yifan Chen, and Yujiu Yang. Region-
adaptive deformable network for image quality assessment. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, pages 324‚Äì333, 2021."
REFERENCES,0.8626609442060086,"[30] Shuwei Shi, Jinjin Gu, Liangbin Xie, Xintao Wang, Yujiu Yang, and Chao Dong. Rethinking alignment in
video super-resolution transformers. Advances in Neural Information Processing Systems, 35:36081‚Äì36093,
2022."
REFERENCES,0.8669527896995708,"[31] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh Agrawala, Diego Gutierrez, Belen Masia, and
Gordon Wetzstein. Saliency in vr: How do people explore virtual environments? IEEE Transactions on
Visualization and Computer Graphics, 24(4):1633‚Äì1642, 2018."
REFERENCES,0.871244635193133,"[32] Xiangjie Sui, Yuming Fang, Hanwei Zhu, Shiqi Wang, and Zhou Wang. Scandmm: A deep markov model
of scanpath prediction for 360¬∞ images. IEEE Conference on Computer Vision and Pattern Recognition,
2023."
REFERENCES,0.8755364806866953,"[33] Xiangjie Sui, Kede Ma, Yiru Yao, and Yuming Fang. Perceptual quality assessment of omnidirectional
images as moving camera videos. IEEE Transactions on Visualization and Computer Graphics, 28(8):3022‚Äì
3034, 2021."
REFERENCES,0.8798283261802575,"[34] Wanjie Sun, Zhenzhong Chen, and Feng Wu. Visual scanpath prediction using ior-roi recurrent mixture
density network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(6):2101‚Äì2118,
2019."
REFERENCES,0.8841201716738197,"[35] Wei Sun, Ke Gu, Guangtao Zhai, Siwei Ma, Weisi Lin, and Patrick Le Calle. Cviqd: Subjective quality
evaluation of compressed virtual reality images. In 2017 IEEE International Conference on Image
Processing, pages 3450‚Äì3454, 2017."
REFERENCES,0.8884120171673819,"[36] Wei Sun, Xiongkuo Min, Guangtao Zhai, Ke Gu, Huiyu Duan, and Siwei Ma. Mc360iqa: A multi-channel
cnn for blind 360-degree image quality assessment. IEEE Journal of Selected Topics in Signal Processing,
14(1):64‚Äì77, 2019."
REFERENCES,0.8927038626609443,"[37] Yule Sun, Ang Lu, and Lu Yu. Weighted-to-spherically-uniform quality evaluation for omnidirectional
video. IEEE Signal Processing Letters, 24(9):1408‚Äì1412, 2017."
REFERENCES,0.8969957081545065,"[38] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of
images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2555‚Äì2563,
2023."
REFERENCES,0.9012875536480687,"[39] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600‚Äì612, 2004."
REFERENCES,0.9055793991416309,"[40] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, volume 2,
pages 1398‚Äì1402, 2003."
REFERENCES,0.9098712446351931,"[41] Chen Xia, Junwei Han, Fei Qi, and Guangming Shi. Predicting human saccadic scanpaths based on
iterative representation learning. IEEE Transactions on Image Processing, 28(7):3502‚Äì3515, 2019."
REFERENCES,0.9141630901287554,"[42] Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating artifacts in
real-world video super-resolution models. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 37, pages 2956‚Äì2964, 2023."
REFERENCES,0.9184549356223176,"[43] Jiahua Xu, Wei Zhou, and Zhibo Chen. Blind omnidirectional image quality assessment with viewport
oriented graph convolutional networks. IEEE Transactions on Circuits and Systems for Video Technology,
31(5):1724‚Äì1737, 2020."
REFERENCES,0.9227467811158798,"[44] Mai Xu, Chen Li, Zhenzhong Chen, Zulin Wang, and Zhenyu Guan. Assessing visual quality of omnidi-
rectional videos. IEEE Transactions on Circuits and Systems for Video Technology, 29(12):3516‚Äì3530,
2018."
REFERENCES,0.927038626609442,"[45] Mai Xu, Yuhang Song, Jianyi Wang, MingLang Qiao, Liangyu Huo, and Zulin Wang. Predicting head
movement in panoramic video: A deep reinforcement learning approach. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41(11):2693‚Äì2708, 2018."
REFERENCES,0.9313304721030042,"[46] Li Yang, Mai Xu, Xin Deng, and Bo Feng. Spatial attention-based non-reference perceptual quality
prediction network for omnidirectional images. In 2021 IEEE International Conference on Multimedia
and Expo, pages 1‚Äì6, 2021."
REFERENCES,0.9356223175965666,"[47] Li Yang, Mai Xu, Yichen Guo, Xin Deng, Fangyuan Gao, and Zhenyu Guan. Hierarchical bayesian lstm
for head trajectory prediction on omnidirectional images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 44(11):7563‚Äì7580, 2021."
REFERENCES,0.9399141630901288,"[48] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and
Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages
1191‚Äì1200, 2022."
REFERENCES,0.944206008583691,"[49] Yan Ye, Elena Alshina, and J Boyce. Jvet-g1003: Algorithm description of projection format conversion
and video quality metrics in 360lib version 4. Joint Video Exploration Team, 2017."
REFERENCES,0.9484978540772532,"[50] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From
patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3575‚Äì3585, 2020."
REFERENCES,0.9527896995708155,"[51] Matt Yu, Haricharan Lakshman, and Bernd Girod. A framework to evaluate omnidirectional video coding
schemes. In 2015 IEEE International Symposium on Mixed and Augmented Reality, pages 31‚Äì36, 2015."
REFERENCES,0.9570815450643777,"[52] Vladyslav Zakharchenko, Kwang Pyo Choi, and Jeong Hoon Park. Quality metric for spherical panoramic
video. In Optics and Photonics for Information Processing X, volume 9970, pages 57‚Äì65, 2016."
REFERENCES,0.9613733905579399,"[53] Chaofan Zhang and Shiguang Liu. No-reference omnidirectional image quality assessment based on joint
network. In Proceedings of the 30th ACM International Conference on Multimedia, pages 943‚Äì951, 2022."
REFERENCES,0.9656652360515021,"[54] Lin Zhang, Ying Shen, and Hongyu Li. Vsi: A visual saliency-induced index for perceptual image quality
assessment. IEEE Transactions on Image processing, 23(10):4270‚Äì4281, 2014."
REFERENCES,0.9699570815450643,"[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 586‚Äì595, 2018."
REFERENCES,0.9742489270386266,"[56] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment
using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video
Technology, 30(1):36‚Äì47, 2018."
REFERENCES,0.9785407725321889,"[57] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment
via vision-language correspondence: A multitask learning perspective. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 14071‚Äì14081, 2023."
REFERENCES,0.9828326180257511,"[58] Xuelei Zheng, Gangyi Jiang, Mei Yu, and Hao Jiang.
Segmented spherical projection-based blind
omnidirectional image quality assessment. IEEE Access, 8:31647‚Äì31659, 2020."
REFERENCES,0.9871244635193133,"[59] Wei Zhou, Jiahua Xu, Qiuping Jiang, and Zhibo Chen. No-reference quality assessment for 360-degree
images by analysis of multifrequency information and local-global naturalness. IEEE Transactions on
Circuits and Systems for Video Technology, 32(4):1778‚Äì1791, 2021."
REFERENCES,0.9914163090128756,"[60] Yu Zhou, Yanjing Sun, Leida Li, Ke Gu, and Yuming Fang. Omnidirectional image quality assessment by
distortion discrimination assisted multi-stream network. IEEE Transactions on Circuits and Systems for
Video Technology, 32(4):1767‚Äì1777, 2021."
REFERENCES,0.9957081545064378,"[61] Yufeng Zhou, Mei Yu, Hualin Ma, Hua Shao, and Gangyi Jiang. Weighted-to-spherically-uniform ssim
objective quality evaluation for panoramic video. In IEEE International Conference on Signal Processing,
pages 54‚Äì57, 2018."
