Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011325028312570782,"Current reinforcement learning (RL) models are often claimed to explain animal
1"
ABSTRACT,0.0022650056625141564,"behavior. However, they are designed for artificial agents that sense, think, and react
2"
ABSTRACT,0.0033975084937712344,"much faster than the brain, and they tend to fail when operating under human-like
3"
ABSTRACT,0.004530011325028313,"sensory and reaction times. Despite using slow neurons, the brain achieves precise
4"
ABSTRACT,0.0056625141562853904,"and low-latency control through a combination of predictive and sequence learning.
5"
ABSTRACT,0.006795016987542469,"The basal ganglia is hypothesized to learn compressed representations of action
6"
ABSTRACT,0.007927519818799546,"sequences, allowing the brain to produce a series of actions for a given input. We
7"
ABSTRACT,0.009060022650056626,"present the Hindsight-Sequence-Planner (HSP), a model of the basal ganglia and
8"
ABSTRACT,0.010192525481313703,"the prefrontal cortex that operates under ""brain-like"" conditions: slow information
9"
ABSTRACT,0.011325028312570781,"processing with quick sensing and actuation. Our ""temporal recall"" mechanism is
10"
ABSTRACT,0.01245753114382786,"inspired by the prefrontal cortex’s role in sequence learning, where the agent uses
11"
ABSTRACT,0.013590033975084938,"an environmental model to replay memories at a finer temporal resolution than its
12"
ABSTRACT,0.014722536806342015,"processing speed while addressing the credit assignment problem caused by scalar
13"
ABSTRACT,0.015855039637599093,"rewards in sequence learning. HSP employs model-based training to achieve model-
14"
ABSTRACT,0.01698754246885617,"free control, resulting in precise and efficient behavior that appears low-latency
15"
ABSTRACT,0.01812004530011325,"despite running on slow hardware. We test HSP on various continuous control
16"
ABSTRACT,0.01925254813137033,"tasks, demonstrating that it not can achieve comparable performance ’human-like’
17"
ABSTRACT,0.020385050962627407,"frequencies by relying on significantly fewer observations and actor calls (actor
18"
ABSTRACT,0.021517553793884484,"sample complexity).
19"
INTRODUCTION,0.022650056625141562,"1
Introduction
20"
INTRODUCTION,0.02378255945639864,"Biological and artificial agents must learn behaviors that maximize rewards to thrive in complex
21"
INTRODUCTION,0.02491506228765572,"environments. Reinforcement learning (RL), a class of algorithms inspired by animal behavior,
22"
INTRODUCTION,0.026047565118912798,"facilitates this learning process (1). The connection between neuroscience and RL is profound.
23"
INTRODUCTION,0.027180067950169876,"The Temporal Difference (TD) error, a key concept in RL, effectively models the firing patterns of
24"
INTRODUCTION,0.028312570781426953,"dopamine neurons in the midbrain (2; 3; 4). Additionally, a longstanding goal of RL algorithms is to
25"
INTRODUCTION,0.02944507361268403,"match and surpass human performance in control tasks (5; 6; 7; 8; 9; 10)
26"
INTRODUCTION,0.030577576443941108,"However, most of these successes are achieved by leveraging large amounts of data in simulated
27"
INTRODUCTION,0.031710079275198186,"environments and operating at speeds orders of magnitude faster than biological neurons. For example,
28"
INTRODUCTION,0.03284258210645526,"the default timestep for the Humanoid task in the MuJoCo environment (11) in OpenAI Gym (12)
29"
INTRODUCTION,0.03397508493771234,"is 15 milliseconds. In contrast, human reaction times range from 150 milliseconds (13) to several
30"
INTRODUCTION,0.035107587768969425,"seconds for complex tasks (14). When RL agents are constrained to human-like reaction times, even
31"
INTRODUCTION,0.0362400906002265,"state-of-the-art algorithms struggle to perform in simple environments.
32"
INTRODUCTION,0.03737259343148358,"The primary reason for this difficulty is the implicit assumption in RL that the environment and the
33"
INTRODUCTION,0.03850509626274066,"agent operate at a constant timestep. Consequently, in embodied agents, all components—sensors,
34"
INTRODUCTION,0.039637599093997736,"compute units, and actuators—are synchronized to operate at the same frequency. Typically, this
35"
INTRODUCTION,0.04077010192525481,"frequency is limited by the speed of computation in artificial agents (15). As a result, robots often
36"
INTRODUCTION,0.04190260475651189,"require fast onboard computing hardware (CPU or GPU) to achieve higher control frequencies
37"
INTRODUCTION,0.04303510758776897,"(16; 17; 18).
38"
INTRODUCTION,0.044167610419026046,"In contrast, biological agents achieve precise and seemingly fast control using much slower hard-
39"
INTRODUCTION,0.045300113250283124,"ware. This is possible because biological agents effectively decouple the computation frequency
40"
INTRODUCTION,0.0464326160815402,"from the actuation frequency, allowing them to achieve high actuation frequencies even with slow
41"
INTRODUCTION,0.04756511891279728,"computational speeds. Consequently, biological agents demonstrate robust, adaptive, and efficient
42"
INTRODUCTION,0.04869762174405436,"control.
43"
INTRODUCTION,0.04983012457531144,"To allow the RL agent to observe and react to changes in the environment quickly, RL algorithms are
44"
INTRODUCTION,0.05096262740656852,"forced to set a high frequency. Even in completely predictable environments, when the agent learns
45"
INTRODUCTION,0.052095130237825596,"to walk or move, a small timestep is required to account for the actuation frequency required for the
46"
INTRODUCTION,0.053227633069082674,"task, but it is not necessary to observe the environment as often or compute new actions as frequently.
47"
INTRODUCTION,0.05436013590033975,"As a result, RL algorithms suffer from many problems such as low sample efficiency, failure to learn
48"
INTRODUCTION,0.05549263873159683,"tasks with sparse rewards, jerky control, high compute cost, and catastrophic failure due to missing
49"
INTRODUCTION,0.056625141562853906,"inputs.
50"
INTRODUCTION,0.057757644394110984,"In this work, we propose Hindsight-Sequence-Planner (HSP), a model for sequence learning based on
51"
INTRODUCTION,0.05889014722536806,"the role of the basal ganglia (BG) and the prefrontal cortex (PFC). Our model learns open-loop control
52"
INTRODUCTION,0.06002265005662514,"utilizing a slow hardware and low attention, and hence also low energy. Additionally, the algorithm
53"
INTRODUCTION,0.061155152887882216,"utilizes a simultaneously learned model of the environment during its training but can act without it
54"
INTRODUCTION,0.0622876557191393,"for fast and cheap inference. We demonstrate the algorithm achieves competitive performance on
55"
INTRODUCTION,0.06342015855039637,"difficult continuous control tasks while utilizing a fraction of observations and calls to the policy. To
56"
INTRODUCTION,0.06455266138165346,"the best of our knowledge, HSP is the first to achieve this feat.
57"
NEURAL BASIS FOR SEQUENCE LEARNING,0.06568516421291053,"2
Neural Basis for Sequence Learning
58"
NEURAL BASIS FOR SEQUENCE LEARNING,0.06681766704416761,"Unlike artificial RL agents, learning in the brain does not stop once an optimal solution has been
59"
NEURAL BASIS FOR SEQUENCE LEARNING,0.06795016987542468,"found. During initial task learning, brain activity increases as expected, reflecting neural recruitment.
60"
NEURAL BASIS FOR SEQUENCE LEARNING,0.06908267270668177,"However, after training and repetition, activity decreases as the brain develops more efficient repre-
61"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07021517553793885,"sentations of the action sequence, commonly referred to as muscle memory (19). This phenomenon
62"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07134767836919592,"is further supported by findings that sequence-specific activity in motor regions evolves based on the
63"
NEURAL BASIS FOR SEQUENCE LEARNING,0.072480181200453,"amount of training, demonstrating skill-specific efficiency and specialization over time (20).
64"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07361268403171008,"The neural basis for action sequence learning involves a sophisticated interconnection of different
65"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07474518686296716,"brain regions, each making a distinct contribution:
66"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07587768969422423,"1. Basal ganglia (BG): Action chunking is a cognitive process by which individual actions are
67"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07701019252548132,"grouped into larger, more manageable units or ""chunks,"" facilitating more efficient storage,
68"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07814269535673839,"retrieval, and execution with reduced cognitive load (21). Importantly, this mechanism
69"
NEURAL BASIS FOR SEQUENCE LEARNING,0.07927519818799547,"allows the brain to perform extremely fast and precise sequences of actions that would be
70"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08040770101925254,"impossible if produced individually. The BG plays a crucial role in chunking, encoding
71"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08154020385050963,"entire behavioral action sequences as a single action (22; 21; 23; 24; 25; 26). Dysfunction
72"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08267270668176671,"in the BG is associated with deficits in action sequences and chunking in both animals
73"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08380520951302378,"(27; 28; 29) and humans (30; 31; 21). However, the neural basis for the compression of
74"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08493771234428087,"individual actions into sequences remains poorly understood.
75"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08607021517553794,"2. Prefrontal cortex (PFC): The PFC is critical for the active unbinding and dismantling
76"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08720271800679502,"of action sequences to ensure behavioral flexibility and adaptability (32). This suggests
77"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08833522083805209,"that action sequences are not merely learned through repetition; the PFC modifies these
78"
NEURAL BASIS FOR SEQUENCE LEARNING,0.08946772366930918,"sequences based on context and task requirements. Recent research indicates that the PFC
79"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09060022650056625,"supports memory elaboration (33) and maintains temporal context information (34) in action
80"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09173272933182333,"sequences. The prefrontal cortex receives inputs from the hippocampus.
81"
NEURAL BASIS FOR SEQUENCE LEARNING,0.0928652321630804,"3. Hippocampus (HC) replays neuronal activations of tasks during subsequent sleep at speeds
82"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09399773499433749,"six to seven times faster. This memory replay may explain the compression of slow actions
83"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09513023782559456,"into fast chunks. The replayed trajectories from the HC are consolidated into long-term
84"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09626274065685164,"cortical memories (35; 36). This phenomenon extends to the motor cortex, which replays
85"
NEURAL BASIS FOR SEQUENCE LEARNING,0.09739524348810873,"motor patterns at accelerated speeds during sleep (37).
86"
RELATED WORK,0.0985277463193658,"3
Related Work
87"
MODEL-BASED REINFORCEMENT LEARNING,0.09966024915062288,"3.1
Model-Based Reinforcement Learning
88"
MODEL-BASED REINFORCEMENT LEARNING,0.10079275198187995,"Model-Based Reinforcement Learning (MBRL) algorithms leverage a model of the environment,
89"
MODEL-BASED REINFORCEMENT LEARNING,0.10192525481313704,"which can be either learned or known, to enhance RL performance (38). Broadly, MBRL algorithms
90"
MODEL-BASED REINFORCEMENT LEARNING,0.10305775764439411,"have been utilized to:
91"
MODEL-BASED REINFORCEMENT LEARNING,0.10419026047565119,"1. Improve Data Efficiency: By augmenting real-world data with model-generated data, MBRL
92"
MODEL-BASED REINFORCEMENT LEARNING,0.10532276330690826,"can significantly enhance data efficiency (39; 40; 41).
93"
MODEL-BASED REINFORCEMENT LEARNING,0.10645526613816535,"2. Enhance Exploration: MBRL aids in exploration by using models to identify potential or
94"
MODEL-BASED REINFORCEMENT LEARNING,0.10758776896942242,"unexplored states (42; 43; 44).
95"
MODEL-BASED REINFORCEMENT LEARNING,0.1087202718006795,"3. Boost Performance: Better learned representations from MBRL can lead to improved
96"
MODEL-BASED REINFORCEMENT LEARNING,0.10985277463193659,"asymptotic performance (45; 46).
97"
MODEL-BASED REINFORCEMENT LEARNING,0.11098527746319366,"4. Transfer Learning: MBRL supports transfer learning, enabling knowledge transfer across
98"
MODEL-BASED REINFORCEMENT LEARNING,0.11211778029445074,"different tasks or environments (47; 48).
99"
MODEL-BASED REINFORCEMENT LEARNING,0.11325028312570781,"5. Online Planning: Models can be used for online planning with a single-step policy (49).
100"
MODEL-BASED REINFORCEMENT LEARNING,0.1143827859569649,"However, this approach increases model complexity as each online planning step requires an
101"
MODEL-BASED REINFORCEMENT LEARNING,0.11551528878822197,"additional call to the model, making it nonviable for energy and computationally constrained
102"
MODEL-BASED REINFORCEMENT LEARNING,0.11664779161947905,"agents like the brain and robots.
103"
MODEL-BASED REINFORCEMENT LEARNING,0.11778029445073612,"Compared to online planning, our algorithm maintains a model complexity of zero after training, elim-
104"
MODEL-BASED REINFORCEMENT LEARNING,0.11891279728199321,"inating the need for any model calls post-training. This significantly reduces the computational and
105"
MODEL-BASED REINFORCEMENT LEARNING,0.12004530011325028,"energy requirements, making it more suitable for practical applications in constrained environments.
106"
MODEL-BASED REINFORCEMENT LEARNING,0.12117780294450736,"Additionally, the performance of online planning algorithms relies heavily on the accuracy of the
107"
MODEL-BASED REINFORCEMENT LEARNING,0.12231030577576443,"model. In contrast, our approach can leverage even an inaccurate model to learn a better-performing
108"
MODEL-BASED REINFORCEMENT LEARNING,0.12344280860702152,"policy than online planning, using the same model.
109"
MACRO-ACTIONS,0.1245753114382786,"3.2
Macro-Actions
110"
MACRO-ACTIONS,0.12570781426953567,"Reinforcement Learning (RL) algorithms that utilize macro-actions demonstrate many benefits,
111"
MACRO-ACTIONS,0.12684031710079274,"including improved exploration and faster learning (50). However, identifying effective macro-
112"
MACRO-ACTIONS,0.12797281993204984,"actions is a challenging problem due to the curse of dimensionality, which arises from large action
113"
MACRO-ACTIONS,0.1291053227633069,"spaces. To address this issue, some approaches have employed genetic algorithms (51) or relied on
114"
MACRO-ACTIONS,0.13023782559456398,"expert demonstrations to extract macro-actions (52). However, these methods are not scalable and
115"
MACRO-ACTIONS,0.13137032842582105,"lack biological plausibility.
116"
MACRO-ACTIONS,0.13250283125707815,"In contrast, our approach learns macro-actions using the principles of RL, thus requiring little
117"
MACRO-ACTIONS,0.13363533408833522,"overhead while combining the flexibility of primitive actions with the efficiency of macro-actions.
118"
ACTION REPETITION AND FRAME-SKIPPING,0.1347678369195923,"3.3
Action Repetition and Frame-skipping
119"
ACTION REPETITION AND FRAME-SKIPPING,0.13590033975084936,"To overcome the curse of dimensionality while gaining the benefits of macro-actions, many approaches
120"
ACTION REPETITION AND FRAME-SKIPPING,0.13703284258210646,"utilize frame-skipping and action repetition, where macro-actions are restricted to a single primitive
121"
ACTION REPETITION AND FRAME-SKIPPING,0.13816534541336353,"action that is repeated. Frame-skipping and action repetition serve as a form of partial open-loop
122"
ACTION REPETITION AND FRAME-SKIPPING,0.1392978482446206,"control, where the agent selects a sequence of actions to be executed without considering the
123"
ACTION REPETITION AND FRAME-SKIPPING,0.1404303510758777,"intermediate states. Consequently, the number of actions is linear in the number of time steps
124"
ACTION REPETITION AND FRAME-SKIPPING,0.14156285390713477,"(53; 54; 55; 56; 57).
125"
ACTION REPETITION AND FRAME-SKIPPING,0.14269535673839184,"For instance, FiGaR (56) shifts the problem of macro-action learning to predicting the number of
126"
ACTION REPETITION AND FRAME-SKIPPING,0.1438278595696489,"steps that the outputted action can be repeated. TempoRL (55) improved upon FiGaR by conditioning
127"
ACTION REPETITION AND FRAME-SKIPPING,0.144960362400906,"the number of repetitions on the selected actions. However, none of these algorithms can scale to
128"
ACTION REPETITION AND FRAME-SKIPPING,0.14609286523216308,"continuous control tasks with multiple action dimensions, as action repetition forces all actuators
129"
ACTION REPETITION AND FRAME-SKIPPING,0.14722536806342015,"and joints to be synchronized in their repetitions, leading to poor performance for longer action
130"
ACTION REPETITION AND FRAME-SKIPPING,0.14835787089467722,"sequences.
131"
ACTION REPETITION AND FRAME-SKIPPING,0.14949037372593432,"Figure 1: The Hindsight-Sequence-Planner (HSP) model. The HSP takes inspiration from the
function of the basal ganglia (BG) (Top/Orange) and the prefrontal cortex (PFC) (Bottom/Blue). We
train an actor with a gated recurrent unit that can produce sequences of arbitrary lengths given a
single state. This is achieved by utilizing a critic and a model that acts at a finer temporal resolution
during training/replay to provide an error signal to each primitive action of the action sequence."
HINDSIGHT SEQUENCE PLANNER,0.1506228765571914,"4
Hindsight Sequence Planner
132"
HINDSIGHT SEQUENCE PLANNER,0.15175537938844846,"Based on the insights presented in Section 2, we introduce a novel reinforcement learning model
133"
HINDSIGHT SEQUENCE PLANNER,0.15288788221970556,"capable of learning sequences of actions (macro-actions) by replaying memories at a finer temporal
134"
HINDSIGHT SEQUENCE PLANNER,0.15402038505096263,"resolution than the action generation, utilizing a model of the environment during training.
135"
HINDSIGHT SEQUENCE PLANNER,0.1551528878822197,"Components
136"
HINDSIGHT SEQUENCE PLANNER,0.15628539071347677,"The Hindsight-Sequence-Planner (HSP) algorithm learns to plan ""in-the-mind"" using a model during
137"
HINDSIGHT SEQUENCE PLANNER,0.15741789354473387,"training, allowing the learned action-sequences to be executed without the need for model-based
138"
HINDSIGHT SEQUENCE PLANNER,0.15855039637599094,"online planning. This is achieved using an actor-critic setting where the actor and critic operate at
139"
HINDSIGHT SEQUENCE PLANNER,0.159682899207248,"different frequencies, representing the observation/computation and actuation frequencies, respec-
140"
HINDSIGHT SEQUENCE PLANNER,0.16081540203850508,"tively. Essentially, the critic is only used during training/replay and can operate at any temporal
141"
HINDSIGHT SEQUENCE PLANNER,0.16194790486976218,"resolution, while the actor is constrained to the temporal resolution of the slowest component in the
142"
HINDSIGHT SEQUENCE PLANNER,0.16308040770101925,"sensing-compute-actuation loop. Denoting the actor’s timestep as t′ and the critic’s timestep as t, our
143"
HINDSIGHT SEQUENCE PLANNER,0.16421291053227632,"algorithm includes three components:
144"
HINDSIGHT SEQUENCE PLANNER,0.16534541336353342,"Model : st+1 = mϕ(st, at)
Critic : qt = qψ(st, at)"
HINDSIGHT SEQUENCE PLANNER,0.1664779161947905,"Actor : at′ = at, at′+t, at′+2t.. ∼πω(st′)
(1)"
HINDSIGHT SEQUENCE PLANNER,0.16761041902604756,"We denote individual actions in the action sequence generated by actor using the notation πω(st′)t to
145"
HINDSIGHT SEQUENCE PLANNER,0.16874292185730463,"represent the action at′+t
146"
HINDSIGHT SEQUENCE PLANNER,0.16987542468856173,"1. Model: Learns the dynamics of the environment, predicting the next state st+1 given the
147"
HINDSIGHT SEQUENCE PLANNER,0.1710079275198188,"current state st and primitive action at.
148"
HINDSIGHT SEQUENCE PLANNER,0.17214043035107587,"2. Critic: Takes the same input as the model but predicts the Q-value of the state-action pair.
149"
HINDSIGHT SEQUENCE PLANNER,0.17327293318233294,"3. Actor: Produces a sequence of actions given an observation at time t′. Observations from
150"
HINDSIGHT SEQUENCE PLANNER,0.17440543601359004,"the environment can occur at any timestep t or t′, where we assume t′ > t. Specifically, in
151"
HINDSIGHT SEQUENCE PLANNER,0.1755379388448471,"our algorithm, t′ = Jt where J > 1; J ∈Z.
152"
HINDSIGHT SEQUENCE PLANNER,0.17667044167610418,"Each component of our algorithm is trained in parallel, demonstrating competitive learning speeds.
153"
HINDSIGHT SEQUENCE PLANNER,0.17780294450736125,"We follow the Soft-Actor-Critic (SAC) algorithm (58) for learning the actor-critic. Exploration and
154"
HINDSIGHT SEQUENCE PLANNER,0.17893544733861835,"uncertainty are critical factors heavily influenced by timestep size and planning horizon. Many
155"
HINDSIGHT SEQUENCE PLANNER,0.18006795016987542,"model-free algorithms like DDPG (59) and TD3 (60) explore by adding random noise to each action
156"
HINDSIGHT SEQUENCE PLANNER,0.1812004530011325,"during training. However, planning a sequence of actions over a longer timestep can result in additive
157"
HINDSIGHT SEQUENCE PLANNER,0.1823329558323896,"noise, leading to poor performance during training and exploration. The SAC algorithm addresses this
158"
HINDSIGHT SEQUENCE PLANNER,0.18346545866364666,"by maximizing the entropy of each action in addition to the expected return, allowing our algorithm
159"
HINDSIGHT SEQUENCE PLANNER,0.18459796149490373,"to automatically lower entropy for deeper actions farther from the observation.
160"
HINDSIGHT SEQUENCE PLANNER,0.1857304643261608,"Learning the Model
161"
HINDSIGHT SEQUENCE PLANNER,0.1868629671574179,"The model is trained to minimize the Mean Squared Error of the predicted states. For a trajectory τ =
162"
HINDSIGHT SEQUENCE PLANNER,0.18799546998867497,"(st, at, st+1) drawn from the replay buffer D, the predicted state is taken from ˜st+1 ∼mϕ(st, at).
163"
HINDSIGHT SEQUENCE PLANNER,0.18912797281993204,"The loss function is:
164"
HINDSIGHT SEQUENCE PLANNER,0.19026047565118911,"Lϕ = Eτ∼D(˜st+1 −st+1)2
(2)
For this work, the model is a feed-forward neural network with two hidden layers. In addition to the
165"
HINDSIGHT SEQUENCE PLANNER,0.1913929784824462,"current model mϕ, we also maintain a target model mϕ−that is the exponential moving average of
166"
HINDSIGHT SEQUENCE PLANNER,0.19252548131370328,"the current model.
167"
HINDSIGHT SEQUENCE PLANNER,0.19365798414496035,"Learning Critic
168"
HINDSIGHT SEQUENCE PLANNER,0.19479048697621745,"The critic is trained to predict the Q-value of a given state-action pair ˜qt = qψ(st, at) using the target
169"
HINDSIGHT SEQUENCE PLANNER,0.19592298980747452,"value from the modified Bellman equation:
170"
HINDSIGHT SEQUENCE PLANNER,0.1970554926387316,"ˆqt = rt + γEat+1∼πω(st+1)0[qψ−(st+1, at+1) −α log πω(at+1|st+1)]
(3)"
HINDSIGHT SEQUENCE PLANNER,0.19818799546998866,"Here, qψ−is the target critic, which is the exponential moving average of the critic. Following the
171"
HINDSIGHT SEQUENCE PLANNER,0.19932049830124576,"SAC algorithm, we train two critics and use the minimum of the two qψ−values to train the current
172"
HINDSIGHT SEQUENCE PLANNER,0.20045300113250283,"critics. The loss function is:
173"
HINDSIGHT SEQUENCE PLANNER,0.2015855039637599,"Lψ = Eτ∼D[(˜qtk −ˆqt)2]∀k ∈1, 2
(4)"
HINDSIGHT SEQUENCE PLANNER,0.20271800679501698,"Both critics are feed-forward neural networks with two hidden layers.
174"
HINDSIGHT SEQUENCE PLANNER,0.20385050962627407,"Learning Policy
175"
HINDSIGHT SEQUENCE PLANNER,0.20498301245753114,"The HSP policy utilizes two hidden layers followed by a Gated-Recurrent-Unit (GRU) (61) that takes
176"
HINDSIGHT SEQUENCE PLANNER,0.20611551528878821,"as input the previous action in the action sequence, followed by two linear layers that output the mean
177"
HINDSIGHT SEQUENCE PLANNER,0.2072480181200453,"and standard deviation of the Gaussian distribution of the action. This design allows the policy to
178"
HINDSIGHT SEQUENCE PLANNER,0.20838052095130238,"produce action sequences of arbitrary length given a single state and the last action.
179"
HINDSIGHT SEQUENCE PLANNER,0.20951302378255945,"A naive approach to training a sequence of actions would be to augment the action space to include
180"
HINDSIGHT SEQUENCE PLANNER,0.21064552661381652,"all possible actions of the sequence length. However, this quickly leads to the curse of dimensionality,
181"
HINDSIGHT SEQUENCE PLANNER,0.21177802944507362,"as each sequence is considered a unique action, dramatically increasing the policy’s complexity.
182"
HINDSIGHT SEQUENCE PLANNER,0.2129105322763307,"Additionally, such an approach ignores the temporal information of the action sequence and faces the
183"
HINDSIGHT SEQUENCE PLANNER,0.21404303510758776,"difficult problem of credit assignment, with only a single scalar reward for the entire action sequence.
184"
HINDSIGHT SEQUENCE PLANNER,0.21517553793884484,"To address these problems, we use different temporal scales for the actor and critic. The critic assigns
185"
HINDSIGHT SEQUENCE PLANNER,0.21630804077010193,"value to each segment of the action sequence, bypassing the credit assignment problem caused by the
186"
HINDSIGHT SEQUENCE PLANNER,0.217440543601359,"single scalar reward. However, using collected transitions to train the action sequence is impractical,
187"
HINDSIGHT SEQUENCE PLANNER,0.21857304643261607,"as changing the first action in the sequence would render all future states inaccurate. Thus, the model
188"
HINDSIGHT SEQUENCE PLANNER,0.21970554926387317,"populates intermediate states, which the critic then uses to assign value to each primitive action in the
189"
HINDSIGHT SEQUENCE PLANNER,0.22083805209513024,"sequence.
190"
HINDSIGHT SEQUENCE PLANNER,0.22197055492638731,"Therefore, given a trajectory τ = (at−1, st, at, st+1), we first produce the J-step action sequence
191"
HINDSIGHT SEQUENCE PLANNER,0.22310305775764439,"using the policy: ˜at:t+J = πϕ(st). We then iteratively apply the target model to get the intermediate
192"
HINDSIGHT SEQUENCE PLANNER,0.22423556058890148,"states ˜st+1:t+J−1. Finally, we use the critic to calculate the loss for the actor as follows:
193"
HINDSIGHT SEQUENCE PLANNER,0.22536806342015855,Lω = Eτ∼D
HINDSIGHT SEQUENCE PLANNER,0.22650056625141562,"
α log πω(˜at|st) −qψ(st, ˜at) + J
X"
HINDSIGHT SEQUENCE PLANNER,0.2276330690826727,"j=1
α log πω(˜at+j|˜st+j) −qψ(˜st+j, ˜at+j)

(5)"
EXPERIMENTS,0.2287655719139298,"5
Experiments
194"
EXPERIMENTS,0.22989807474518686,"Overview
195"
EXPERIMENTS,0.23103057757644394,"We evaluate our HSP approach on several continuous control tasks, comparing it against the SAC
196"
EXPERIMENTS,0.232163080407701,"baseline and the TempoRL algorithm (55). Our focus is on environments with multi-dimensional
197"
EXPERIMENTS,0.2332955832389581,"actions, ranging from the simple LunarLanderContinuous (2 action dimensions) to the complex
198"
EXPERIMENTS,0.23442808607021517,"Humanoid environment (17 action dimensions). This allows us to highlight the benefits of HSP over
199"
EXPERIMENTS,0.23556058890147225,"traditional action repetition approaches. We utilize the OpenAI gym (62) implementation of the
200"
EXPERIMENTS,0.23669309173272934,"MuJoCo environments (11).
201"
EXPERIMENTS,0.23782559456398641,"Experiemental Setup
202"
EXPERIMENTS,0.23895809739524349,"We train HSP with four different action sequence lengths (ASL), J = 2, 4, 8, 16, referred to as HSP-J.
203"
EXPERIMENTS,0.24009060022650056,"During training, HSP is evaluated based on its J value, processing states only after every J actions.
204"
EXPERIMENTS,0.24122310305775765,"All hyperparameters are identical between HSP and SAC, except for the actor update frequency: HSP
205"
EXPERIMENTS,0.24235560588901472,"updates the actor every 4 steps, while SAC updates every step. Thus, SAC has four more actor update
206"
EXPERIMENTS,0.2434881087202718,"steps compared to HSP. Additionally, HSP learns a model in parallel with the actor and critic.
207"
EXPERIMENTS,0.24462061155152887,"Learning Curves
208"
EXPERIMENTS,0.24575311438278596,"Figure 2: Learning curves of HSP-J and Soft-Actor Critic (SAC) (58) over continuous control tasks.
HSP and SAC are evaluated under different settings: SAC receives input after every primitive action,
while HSP receives input after J primitive actions. Yet it demonstrates competitive performance on
all environments, even outperforming SAC on LunarLander, Hopper and Humanoid environments.
HSP demonstrates stable learning even with the added model and generative replay training. All
curves are averaged over 5 trials, with shaded regions representing standard error."
EXPERIMENTS,0.24688561721404303,"Figure 6 presents the learning curves of HSP and SAC across six continuous control tasks. We observe
209"
EXPERIMENTS,0.2480181200453001,"that HSP outperforms SAC in four out of six tasks (excluding Ant and HalfCheetah). Notably, HSP-16
210"
EXPERIMENTS,0.2491506228765572,"achieves competitive performance on LunarLander and Hopper tasks, showcasing the algorithm’s
211"
EXPERIMENTS,0.25028312570781425,"capability to learn long action sequences from scratch. Surprisingly, HSP also outperforms SAC in
212"
EXPERIMENTS,0.25141562853907135,"the Humanoid environment with fewer inputs and actor updates while concurrently learning a model,
213"
EXPERIMENTS,0.25254813137032844,"demonstrating the efficacy of the algorithm on environments with higher action dimensions.
214"
EXPERIMENTS,0.2536806342015855,"Action Sequence Length (ASL) Performance
215"
EXPERIMENTS,0.2548131370328426,"Learning curves alone do not fully capture HSP’s performance and benefits. For instance, HSP-16
216"
EXPERIMENTS,0.2559456398640997,"shows poor performance on Ant in the learning curve, yet it demonstrates competitive performance
217"
EXPERIMENTS,0.2570781426953567,"when tested on shorter action sequences. Figure 3 presents the performance of trained algorithms
218"
EXPERIMENTS,0.2582106455266138,"across different action sequence lengths (ASL).
219"
EXPERIMENTS,0.25934314835787087,"We select the largest J that shows competitive performance (greater than 75% of the SAC when
220"
EXPERIMENTS,0.26047565118912797,"evaluated on primitive actions) for each environment and test it for sequence lengths up to 30. For
221"
EXPERIMENTS,0.26160815402038506,"SAC and HSP, we fix the length of action sequences while TempoRL is designed to dynamically pick
222"
EXPERIMENTS,0.2627406568516421,"the best ASL, therefore we report the avg. action sequence length for TempoRL. HSP demonstrates
223"
EXPERIMENTS,0.2638731596828992,"Figure 3: Performance of HSP, SAC, and TempoRL (55) at different Action Sequence Lengths (ASL).
SAC and TempoRL repeat the same action for the duration, while HSP can perform a sequence of
actions. Since it implements dynamic action repetition, we present the average ASL for TempoRL
instead of a range of ASL. HSP demonstrates robust performance even at human-like reaction times
(>150ms). All markers are averaged over 5 trials, with the error bars representing standard error.
Going from left to right then top to bottom, the selected training ASL J for HSP are: 16, 16, 4, 16, 4,
8."
EXPERIMENTS,0.2650056625141563,"competitive performance on longer action sequences, approaching human-like reaction times in
224"
EXPERIMENTS,0.26613816534541335,"some environments. Unlike SAC, which fails with action sequences of 2 or 3, HSP shows a gradual
225"
EXPERIMENTS,0.26727066817667045,"degradation of performance. Additionally, HSP generalizes well in environments like LunarLander
226"
EXPERIMENTS,0.26840317100792754,"and Ant, even though the actor is trained only on sequence lengths of 16.
227"
EXPERIMENTS,0.2695356738391846,"Comparing HSP to TempoRL, we find that TempoRL prefers shorter repetitions and struggles in
228"
EXPERIMENTS,0.2706681766704417,"more difficult environments. TempoRL does not incentivize longer actions and suffers from the
229"
EXPERIMENTS,0.2718006795016987,"curse of dimensionality to some extent, as it needs to learn the number of repetitions for each unique
230"
EXPERIMENTS,0.2729331823329558,"state-action pair. Furthermore, action repetitions are not suitable for multi-dimensional actions, as
231"
EXPERIMENTS,0.2740656851642129,"they force synchronized repetition across all actuators resulting in poor performance in environments
232"
EXPERIMENTS,0.27519818799546997,"with high action dimensions like Ant and HalfCheetah environments.
233"
EXPERIMENTS,0.27633069082672707,"Comparison to Model-based Online Planning
234"
EXPERIMENTS,0.27746319365798416,"In addition to action repetition, model-based online planning is another approach that allows the RL
235"
EXPERIMENTS,0.2785956964892412,"agent to reduce its observational frequency. However, it often requires a highly accurate model of
236"
EXPERIMENTS,0.2797281993204983,"the environment and incurs increased model complexity due to the use of the model during control.
237"
EXPERIMENTS,0.2808607021517554,"Despite these challenges, comparing HSP to model-based online planning is essential since it is useful
238"
EXPERIMENTS,0.28199320498301245,"when the actor cannot produce long sequences of actions and does not require the hyper-parameter J.
239"
EXPERIMENTS,0.28312570781426954,"With access to an accurate model of the environment, the agent’s performance might generalize to
240"
EXPERIMENTS,0.2842582106455266,"arbitrary ASL.
241"
EXPERIMENTS,0.2853907134767837,"Since HSP incorporates a model of the environment that is learned in parallel, we compare the
242"
EXPERIMENTS,0.2865232163080408,"performance of the HSP actor utilizing the actor-generated action sequences against model-based
243"
EXPERIMENTS,0.2876557191392978,"online planning, where the actor produces only a single action between each simulated state.
244"
EXPERIMENTS,0.2887882219705549,"Figure 4: Performance of HSP and model based online planning on different ASL. Both HSP and
Online Planning utilize the same actor and model. HSP utilizes the actor to generate a sequence
of actions while online planning utilizes the actor and the model to generate a sequence of actions.
The same model is used to train the HSP action sequences. Yet, we find that while the model is
not accurate enough to sustain performance for longer sequences, it can train the actor to produce
accurate action sequences."
EXPERIMENTS,0.289920724801812,"Figure 4 shows the performance of online planning using the model in HSP versus the action
245"
EXPERIMENTS,0.29105322763306907,"sequences generated by the HSP policy. We see that HSP can learn action sequences that perform
246"
EXPERIMENTS,0.29218573046432617,"better than model-based online planning using the same model. Thus, HSP can leverage inaccurate
247"
EXPERIMENTS,0.29331823329558326,"models to learn accurate action sequences, further reducing the required computational complexity
248"
EXPERIMENTS,0.2944507361268403,"during training. We hypothesize that this superior performance is due to the fact that the actor learns
249"
EXPERIMENTS,0.2955832389580974,"a J-step action sequence concurrently, while online planning only produces one action at a time.
250"
EXPERIMENTS,0.29671574178935445,"Consequently, HSP is able to learn and produce long, coherent action sequences, whereas single-step
251"
EXPERIMENTS,0.29784824462061155,"predictions tend to drift, similar to the ""hallucination"" phenomenon observed in transformer-based
252"
EXPERIMENTS,0.29898074745186864,"language models.
253"
EXPERIMENTS,0.3001132502831257,"Generative Replay in Latent Space
254"
EXPERIMENTS,0.3012457531143828,"Figure 5: Left: Learning curve of HSP with latent state-space on the Walker2d-v2 environment.
Right: Performance of latent HSP-16 on different ASL, compared to SAC and TempoRL. Utilizing a
latent representation for state space is especially beneficial for the Walker2d environment so that it
outperforms SAC even when training upto sequence lengths of J = 16."
EXPERIMENTS,0.3023782559456399,"Previous studies have shown that generative replay benefits greatly from latent representations (63).
255"
EXPERIMENTS,0.3035107587768969,"Recently, Simplified Temporal Consistency Reinforcement Learning (TCRL) (64) demonstrated
256"
EXPERIMENTS,0.304643261608154,"that learning a latent state-space improves not only model-based planning but also model-free RL
257"
EXPERIMENTS,0.3057757644394111,"algorithms. Building on this insight, we introduced an encoder to encode the observations in our
258"
EXPERIMENTS,0.30690826727066817,"algorithm. We provide the complete implementation details in the Appendix.
259"
EXPERIMENTS,0.30804077010192527,"We did not observe any benefits of using the encoder and temporal consistency for HSP in most
260"
EXPERIMENTS,0.3091732729331823,"environments (results in the appendix). However, for the Walker environment, utilizing the latent
261"
EXPERIMENTS,0.3103057757644394,"space for generative replay significantly improved performance, making it competitive even at 16
262"
EXPERIMENTS,0.3114382785956965,"steps (128ms) (Figure 5).
263"
EXPERIMENTS,0.31257078142695355,"6
Discussion, Limitations and Future Work
264"
EXPERIMENTS,0.31370328425821065,"We introduce the Hindsight-Sequence-Planner (HSP) algorithm, a biologically plausible model for
265"
EXPERIMENTS,0.31483578708946774,"sequence learning. It represents a significant step towards achieving robust control at brain-like
266"
EXPERIMENTS,0.3159682899207248,"speeds. The key contributions of HSP include its ability to generate long sequences of actions from a
267"
EXPERIMENTS,0.3171007927519819,"single state, its resilience to reduced input frequency, and its lower computational complexity per
268"
EXPERIMENTS,0.318233295583239,"primitive action.
269"
EXPERIMENTS,0.319365798414496,"The current RL framework encourages synchrony between the environment and the components
270"
EXPERIMENTS,0.3204983012457531,"of the agent. However, the brain utilizes components that act at different frequencies and yet is
271"
EXPERIMENTS,0.32163080407701017,"capable of robust and accurate control. HSL provides an approach to reconcile this difference
272"
EXPERIMENTS,0.32276330690826727,"between neuroscience and RL, while remaining competitive on current RL benchmarks. HSP offers
273"
EXPERIMENTS,0.32389580973952437,"substantial benefits over traditional RL algorithms, particularly in the context of autonomous agents
274"
EXPERIMENTS,0.3250283125707814,"such as self-driving cars and robots. By enabling operation at slower observational frequencies and
275"
EXPERIMENTS,0.3261608154020385,"providing a gradual decay in performance with reduced input frequency, HSP addresses critical
276"
EXPERIMENTS,0.3272933182332956,"issues related to sensor failure and occlusion, and energy consumption. Additionally, HSP generates
277"
EXPERIMENTS,0.32842582106455265,"long sequences of actions from a single state, which can enhance the explainability of the policy
278"
EXPERIMENTS,0.32955832389580975,"and provide opportunities to override the policy early in case of safety concerns. HSP also learns
279"
EXPERIMENTS,0.33069082672706684,"a latent representation of the action sequence, which could be used in the future to interface with
280"
EXPERIMENTS,0.3318233295583239,"large language models for multimodal explainability and even hierarchical reinforcement learning
281"
EXPERIMENTS,0.332955832389581,"and transfer learning.
282"
EXPERIMENTS,0.33408833522083803,"Limitations
283"
EXPERIMENTS,0.3352208380520951,"Despite its advantages, HSP has some limitations. It shows slightly reduced performance in the
284"
EXPERIMENTS,0.3363533408833522,"Ant and HalfCheetah environments, which we believe can be mitigated through improved models
285"
EXPERIMENTS,0.33748584371460927,"and hyperparameter tuning. HSP also requires more computational resources during training due
286"
EXPERIMENTS,0.33861834654586637,"to the parallel training of an environment model and introduces more hyperparameters, particularly
287"
EXPERIMENTS,0.33975084937712347,"the training ASL (J). In this work, we do not optimize the neural network architecture of the actor
288"
EXPERIMENTS,0.3408833522083805,"to reduce the compute, as a result, the total compute per primitive action is still larger than SAC.
289"
EXPERIMENTS,0.3420158550396376,"However, we believe producing a sequence of actions will be more efficient than producing a single
290"
EXPERIMENTS,0.3431483578708947,"primitive action per state after optimization. Larger ASL values may not perform well in stochastic
291"
EXPERIMENTS,0.34428086070215175,"environments. Moreover, HSP currently uses a constant ASL, but ideally, the ASL should adapt
292"
EXPERIMENTS,0.34541336353340885,"based on the environment’s predictability.
293"
EXPERIMENTS,0.3465458663646659,"Future Work
294"
EXPERIMENTS,0.347678369195923,"We believe the HSP model contributes to both artificial agents and the study of biological control.
295"
EXPERIMENTS,0.3488108720271801,"Future work will incorporate biological features like attention mechanisms and knowledge transfer.
296"
EXPERIMENTS,0.34994337485843713,"Additionally, HSP can benefit from existing Model-Based RL approaches as it naturally learns a
297"
EXPERIMENTS,0.3510758776896942,"model of the world. In deterministic environments, a capable agent should achieve infinite horizon
298"
EXPERIMENTS,0.3522083805209513,"control for tasks like walking and hopping from a single state. This is an important research direction
299"
EXPERIMENTS,0.35334088335220837,"that is currently underexplored, as many environments are partially observable or have some degree
300"
EXPERIMENTS,0.35447338618346547,"of stochasticity. Current approaches rely on external information at every state, which increases
301"
EXPERIMENTS,0.3556058890147225,"energy consumption and vulnerability to adversarial or missing inputs. Truly autonomous agents will
302"
EXPERIMENTS,0.3567383918459796,"need to impl ement multiple policies simultaneously, and simple tasks like walking can be performed
303"
EXPERIMENTS,0.3578708946772367,"without input states if learned properly. Our future work will focus on extending the action sequence
304"
EXPERIMENTS,0.35900339750849375,"horizon until deterministic tasks can be performed using a single state and implementing a mechanism
305"
EXPERIMENTS,0.36013590033975085,"to dynamically pick the action sequence horizon based on context and predictability of the state.
306"
EXPERIMENTS,0.36126840317100795,"Serotonin is an important neuromodulator that has been demonstrated to signal the availability of
307"
EXPERIMENTS,0.362400906002265,"time and resources in the brain to enable the decision on the planning horizon and the use of compute
308"
EXPERIMENTS,0.3635334088335221,"(65). In the future, we hope to introduce a mechanism to replicate the effect of serotonin in HSP.
309"
REFERENCES,0.3646659116647792,"References
310"
REFERENCES,0.36579841449603623,"[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.
311"
REFERENCES,0.3669309173272933,"[2] W. Schultz, P. Dayan, and P. R. Montague, “A neural substrate of prediction and reward,”
312"
REFERENCES,0.36806342015855037,"Science, vol. 275, pp. 1593–1599, 1997.
313"
REFERENCES,0.36919592298980747,"[3] W. Schultz, “Neuronal reward and decision signals: From theories to data,” Physiological
314"
REFERENCES,0.37032842582106457,"Reviews, vol. 95, pp. 853–951, 7 2015.
315"
REFERENCES,0.3714609286523216,"[4] J. Y. Cohen, S. Haesler, L. Vong, B. B. Lowell, and N. Uchida, “Neuron-type-specific signals
316"
REFERENCES,0.3725934314835787,"for reward and punishment in the ventral tegmental area,” Nature 2012 482:7383, vol. 482,
317"
REFERENCES,0.3737259343148358,"pp. 85–88, 1 2012.
318"
REFERENCES,0.37485843714609285,"[5] OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison, D. Farhi,
319"
REFERENCES,0.37599093997734995,"Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov,
320"
REFERENCES,0.37712344280860705,"H. P. d. O. Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang,
321"
REFERENCES,0.3782559456398641,"F. Wolski, and S. Zhang, “Dota 2 with large scale deep reinforcement learning,” 12 2019.
322"
REFERENCES,0.3793884484711212,"[6] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock-
323"
REFERENCES,0.38052095130237823,"hart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver, “Mastering atari, go, chess and shogi
324"
REFERENCES,0.38165345413363533,"by planning with a learned model,” Nature 2020 588:7839, vol. 588, pp. 604–609, 12 2020.
325"
REFERENCES,0.3827859569648924,"[7] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Müller, V. Koltun, and D. Scaramuzza,
326"
REFERENCES,0.38391845979614947,"“Champion-level drone racing using deep reinforcement learning,” Nature 2023 620:7976,
327"
REFERENCES,0.38505096262740657,"vol. 620, pp. 982–987, 8 2023.
328"
REFERENCES,0.38618346545866367,"[8] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capo-
329"
REFERENCES,0.3873159682899207,"bianco, A. Devlic, F. Eckert, F. Fuchs, L. Gilpin, P. Khandelwal, V. Kompella, H. C. Lin,
330"
REFERENCES,0.3884484711211778,"P. MacAlpine, D. Oller, T. Seno, C. Sherstan, M. D. Thomure, H. Aghabozorgi, L. Barrett,
331"
REFERENCES,0.3895809739524349,"R. Douglas, D. Whitehead, P. Dürr, P. Stone, M. Spranger, and H. Kitano, “Outracing cham-
332"
REFERENCES,0.39071347678369195,"pion gran turismo drivers with deep reinforcement learning,” Nature 2022 602:7896, vol. 602,
333"
REFERENCES,0.39184597961494905,"pp. 223–228, 2 2022.
334"
REFERENCES,0.3929784824462061,"[9] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,
335"
REFERENCES,0.3941109852774632,"R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre,
336"
REFERENCES,0.3952434881087203,"T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard,
337"
REFERENCES,0.39637599093997733,"D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring,
338"
REFERENCES,0.39750849377123443,"D. Yogatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu,
339"
REFERENCES,0.3986409966024915,"D. Hassabis, C. Apps, and D. Silver, “Grandmaster level in starcraft ii using multi-agent
340"
REFERENCES,0.39977349943374857,"reinforcement learning,” Nature 2019 575:7782, vol. 575, pp. 350–354, 10 2019.
341"
REFERENCES,0.40090600226500567,"[10] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
342"
REFERENCES,0.40203850509626277,"M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou,
343"
REFERENCES,0.4031710079275198,"H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through
344"
REFERENCES,0.4043035107587769,"deep reinforcement learning,” Nature 2015 518:7540, vol. 518, pp. 529–533, 2 2015.
345"
REFERENCES,0.40543601359003395,"[11] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based control,” in
346"
REFERENCES,0.40656851642129105,"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033,
347"
REFERENCES,0.40770101925254815,"IEEE, 2012.
348"
REFERENCES,0.4088335220838052,"[12] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goulão,
349"
REFERENCES,0.4099660249150623,"A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierré, S. Schulhoff, J. J. Tai, A. T. J.
350"
REFERENCES,0.4110985277463194,"Shen, and O. G. Younis, “Gymnasium,” Mar. 2023.
351"
REFERENCES,0.41223103057757643,"[13] A. Jain, R. Bansal, A. Kumar, and K. Singh, “A comparative study of visual and auditory
352"
REFERENCES,0.41336353340883353,"reaction times on the basis of gender and physical activity levels of medical first year students,”
353"
REFERENCES,0.4144960362400906,"International journal of applied and basic medical research, vol. 5, no. 2, pp. 124–127, 2015.
354"
REFERENCES,0.41562853907134767,"[14] R. Limpert, Brake design and safety. SAE international, 2011.
355"
REFERENCES,0.41676104190260477,"[15] B. Katz, J. D. Carlo, and S. Kim, “Mini cheetah: A platform for pushing the limits of dynamic
356"
REFERENCES,0.4178935447338618,"quadruped control,” Proceedings - IEEE International Conference on Robotics and Automation,
357"
REFERENCES,0.4190260475651189,"vol. 2019-May, pp. 6295–6301, 5 2019.
358"
REFERENCES,0.420158550396376,"[16] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal, “Rapid locomotion via re-
359"
REFERENCES,0.42129105322763305,"inforcement learning,” International Journal of Robotics Research, vol. 43, pp. 572–587, 4
360"
REFERENCES,0.42242355605889015,"2024.
361"
REFERENCES,0.42355605889014725,"[17] Q. Li, G. Dong, R. Qin, J. Chen, K. Xu, and X. Ding, “Quadruped reinforcement learning
362"
REFERENCES,0.4246885617214043,"without explicit state estimation,” in 2022 IEEE International Conference on Robotics and
363"
REFERENCES,0.4258210645526614,"Biomimetics (ROBIO), pp. 1989–1994, IEEE, 2022.
364"
REFERENCES,0.4269535673839185,"[18] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik, M. Wulfmeier,
365"
REFERENCES,0.42808607021517553,"S. Tunyasuvunakool, N. Y. Siegel, R. Hafner, et al., “Learning agile soccer skills for a bipedal
366"
REFERENCES,0.4292185730464326,"robot with deep reinforcement learning,” arXiv preprint arXiv:2304.13653, 2023.
367"
REFERENCES,0.43035107587768967,"[19] T. Wiestler and J. Diedrichsen, “Skill learning strengthens cortical representations of motor
368"
REFERENCES,0.43148357870894677,"sequences,” Elife, vol. 2, p. e00801, 2013.
369"
REFERENCES,0.43261608154020387,"[20] N. F. Wymbs and S. T. Grafton, “The human motor system supports sequence-specific rep-
370"
REFERENCES,0.4337485843714609,"resentations over multiple training-dependent timescales,” Cerebral cortex, vol. 25, no. 11,
371"
REFERENCES,0.434881087202718,"pp. 4213–4225, 2015.
372"
REFERENCES,0.4360135900339751,"[21] N. Favila, K. Gurney, and P. G. Overton, “Role of the basal ganglia in innate and learned
373"
REFERENCES,0.43714609286523215,"behavioural sequences,” Reviews in the Neurosciences, vol. 35, no. 1, pp. 35–55, 2024.
374"
REFERENCES,0.43827859569648925,"[22] X. Jin, F. Tecuapetla, and R. M. Costa, “Basal ganglia subcircuits distinctively encode the
375"
REFERENCES,0.43941109852774635,"parsing and concatenation of action sequences,” Nature neuroscience, vol. 17, no. 3, pp. 423–
376"
REFERENCES,0.4405436013590034,"430, 2014.
377"
REFERENCES,0.4416761041902605,"[23] X. Jin and R. M. Costa, “Shaping action sequences in basal ganglia circuits,” Current opinion
378"
REFERENCES,0.44280860702151753,"in neurobiology, vol. 33, pp. 188–196, 2015.
379"
REFERENCES,0.44394110985277463,"[24] G. S. Berns and T. J. Sejnowski, “How the basal ganglia make decisions,” in Neurobiology of
380"
REFERENCES,0.4450736126840317,"decision-making, pp. 101–113, Springer, 1996.
381"
REFERENCES,0.44620611551528877,"[25] G. S. Berns and T. J. Sejnowski, “A computational model of how the basal ganglia produce
382"
REFERENCES,0.44733861834654587,"sequences,” Journal of cognitive neuroscience, vol. 10, no. 1, pp. 108–121, 1998.
383"
REFERENCES,0.44847112117780297,"[26] E. Garr, “Contributions of the basal ganglia to action sequence learning and performance,”
384"
REFERENCES,0.44960362400906,"Neuroscience & Biobehavioral Reviews, vol. 107, pp. 279–295, 2019.
385"
REFERENCES,0.4507361268403171,"[27] A. J. Doupe, D. J. Perkel, A. Reiner, and E. A. Stern, “Birdbrains could teach basal ganglia
386"
REFERENCES,0.45186862967157415,"research a new song,” Trends in neurosciences, vol. 28, no. 7, pp. 353–363, 2005.
387"
REFERENCES,0.45300113250283125,"[28] X. Jin and R. M. Costa, “Start/stop signals emerge in nigrostriatal circuits during sequence
388"
REFERENCES,0.45413363533408835,"learning,” Nature, vol. 466, no. 7305, pp. 457–462, 2010.
389"
REFERENCES,0.4552661381653454,"[29] M. Matamales, Z. Skrbis, M. R. Bailey, P. D. Balsam, B. W. Balleine, J. Götz, and J. Bertran-
390"
REFERENCES,0.4563986409966025,"Gonzalez, “A corticostriatal deficit promotes temporal distortion of automatic action in ageing,”
391"
REFERENCES,0.4575311438278596,"ELife, vol. 6, p. e29908, 2017.
392"
REFERENCES,0.45866364665911663,"[30] J. G. Phillips, E. Chiu, J. L. Bradshaw, and R. Iansek, “Impaired movement sequencing in
393"
REFERENCES,0.45979614949037373,"patients with huntington’s disease: a kinematic analysis,” Neuropsychologia, vol. 33, no. 3,
394"
REFERENCES,0.4609286523216308,"pp. 365–369, 1995.
395"
REFERENCES,0.46206115515288787,"[31] L. Boyd, J. Edwards, C. Siengsukon, E. Vidoni, B. Wessel, and M. Linsdell, “Motor sequence
396"
REFERENCES,0.46319365798414497,"chunking is impaired by basal ganglia stroke,” Neurobiology of learning and memory, vol. 92,
397"
REFERENCES,0.464326160815402,"no. 1, pp. 35–44, 2009.
398"
REFERENCES,0.4654586636466591,"[32] C. F. Geissler, C. Frings, and B. Moeller, “Illuminating the prefrontal neural correlates of
399"
REFERENCES,0.4665911664779162,"action sequence disassembling in response–response binding,” Scientific Reports, vol. 11, no. 1,
400"
REFERENCES,0.46772366930917325,"p. 22856, 2021.
401"
REFERENCES,0.46885617214043035,"[33] M. A. Immink, M. Pointon, D. L. Wright, and F. E. Marino, “Prefrontal cortex activation during
402"
REFERENCES,0.46998867497168745,"motor sequence learning under interleaved and repetitive practice: a two-channel near-infrared
403"
REFERENCES,0.4711211778029445,"spectroscopy study,” Frontiers in Human Neuroscience, vol. 15, p. 644968, 2021.
404"
REFERENCES,0.4722536806342016,"[34] D. Shahnazian, M. Senoussi, R. M. Krebs, T. Verguts, and C. B. Holroyd, “Neural representa-
405"
REFERENCES,0.4733861834654587,"tions of task context and temporal order during action sequence execution,” Topics in Cognitive
406"
REFERENCES,0.47451868629671573,"Science, vol. 14, no. 2, pp. 223–240, 2022.
407"
REFERENCES,0.47565118912797283,"[35] M. C. Zielinski, W. Tang, and S. P. Jadhav, “The role of replay and theta sequences in mediating
408"
REFERENCES,0.47678369195922987,"hippocampal-prefrontal interactions for memory and cognition,” Hippocampus, vol. 30, no. 1,
409"
REFERENCES,0.47791619479048697,"pp. 60–72, 2020.
410"
REFERENCES,0.47904869762174407,"[36] P. Malerba, K. Tsimring, and M. Bazhenov, “Learning-induced sequence reactivation during
411"
REFERENCES,0.4801812004530011,"sharp-wave ripples: a computational study,” in Advances in the Mathematical Sciences: AWM
412"
REFERENCES,0.4813137032842582,"Research Symposium, Los Angeles, CA, April 2017, pp. 173–204, Springer, 2018.
413"
REFERENCES,0.4824462061155153,"[37] D. B. Rubin, T. Hosman, J. N. Kelemen, A. Kapitonava, F. R. Willett, B. F. Coughlin, E. Halgren,
414"
REFERENCES,0.48357870894677235,"E. Y. Kimchi, Z. M. Williams, J. D. Simeral, et al., “Learned motor patterns are replayed in
415"
REFERENCES,0.48471121177802945,"human motor cortex during sleep,” Journal of Neuroscience, vol. 42, no. 25, pp. 5007–5020,
416"
REFERENCES,0.48584371460928655,"2022.
417"
REFERENCES,0.4869762174405436,"[38] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker, et al., “Model-based reinforcement
418"
REFERENCES,0.4881087202718007,"learning: A survey,” Foundations and Trends® in Machine Learning, vol. 16, no. 1, pp. 1–118,
419"
REFERENCES,0.48924122310305773,"2023.
420"
REFERENCES,0.49037372593431483,"[39] D. Yarats, A. Zhang, I. Kostrikov, B. Amos, J. Pineau, and R. Fergus, “Improving sample
421"
REFERENCES,0.49150622876557193,"efficiency in model-free reinforcement learning from images,” in Proceedings of the AAAI
422"
REFERENCES,0.49263873159682897,"Conference on Artificial Intelligence, vol. 35, pp. 10674–10681, 2021.
423"
REFERENCES,0.49377123442808607,"[40] M. Janner, J. Fu, M. Zhang, and S. Levine, “When to trust your model: Model-based policy
424"
REFERENCES,0.49490373725934317,"optimization,” Advances in neural information processing systems, vol. 32, 2019.
425"
REFERENCES,0.4960362400906002,"[41] J. Wang, W. Li, H. Jiang, G. Zhu, S. Li, and C. Zhang, “Offline reinforcement learning with
426"
REFERENCES,0.4971687429218573,"reverse model-based imagination,” Advances in Neural Information Processing Systems, vol. 34,
427"
REFERENCES,0.4983012457531144,"pp. 29420–29432, 2021.
428"
REFERENCES,0.49943374858437145,"[42] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven exploration by self-
429"
REFERENCES,0.5005662514156285,"supervised prediction,” in International conference on machine learning, pp. 2778–2787,
430"
REFERENCES,0.5016987542468856,"PMLR, 2017.
431"
REFERENCES,0.5028312570781427,"[43] B. C. Stadie, S. Levine, and P. Abbeel, “Incentivizing exploration in reinforcement learning
432"
REFERENCES,0.5039637599093998,"with deep predictive models,” arXiv preprint arXiv:1507.00814, 2015.
433"
REFERENCES,0.5050962627406569,"[44] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys, T. Lillicrap, and S. Gelly,
434"
REFERENCES,0.5062287655719139,"“Episodic curiosity through reachability,” arXiv preprint arXiv:1810.02274, 2018.
435"
REFERENCES,0.507361268403171,"[45] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
436"
REFERENCES,0.5084937712344281,"M. Lai, A. Bolton, et al., “Mastering the game of go without human knowledge,” nature,
437"
REFERENCES,0.5096262740656852,"vol. 550, no. 7676, pp. 354–359, 2017.
438"
REFERENCES,0.5107587768969423,"[46] S. Levine and V. Koltun, “Guided policy search,” in International conference on machine
439"
REFERENCES,0.5118912797281994,"learning, pp. 1–9, PMLR, 2013.
440"
REFERENCES,0.5130237825594564,"[47] A. Zhang, H. Satija, and J. Pineau, “Decoupling dynamics and reward for transfer learning,”
441"
REFERENCES,0.5141562853907135,"arXiv preprint arXiv:1804.10689, 2018.
442"
REFERENCES,0.5152887882219706,"[48] R. Sasso, M. Sabatelli, and M. A. Wiering, “Multi-source transfer learning for deep model-based
443"
REFERENCES,0.5164212910532276,"reinforcement learning,” arXiv preprint arXiv:2205.14410, 2022.
444"
REFERENCES,0.5175537938844847,"[49] A. Fickinger, H. Hu, B. Amos, S. Russell, and N. Brown, “Scalable online planning via
445"
REFERENCES,0.5186862967157417,"reinforcement learning fine-tuning,” Advances in Neural Information Processing Systems,
446"
REFERENCES,0.5198187995469988,"vol. 34, pp. 16951–16963, 2021.
447"
REFERENCES,0.5209513023782559,"[50] A. McGovern, R. S. Sutton, and A. H. Fagg, “Roles of macro-actions in accelerating reinforce-
448"
REFERENCES,0.522083805209513,"ment learning,” 1997.
449"
REFERENCES,0.5232163080407701,"[51] Y.-H. Chang, K.-Y. Chang, H. Kuo, and C.-Y. Lee, “Reusability and transferability of macro
450"
REFERENCES,0.5243488108720272,"actions for reinforcement learning,” ACM Transactions on Evolutionary Learning and Opti-
451"
REFERENCES,0.5254813137032842,"mization, vol. 2, no. 1, pp. 1–16, 2022.
452"
REFERENCES,0.5266138165345413,"[52] H. Kim, M. Yamada, K. Miyoshi, T. Iwata, and H. Yamakawa, “Reinforcement learning in
453"
REFERENCES,0.5277463193657984,"latent action sequence space,” in 2020 IEEE/RSJ International Conference on Intelligent Robots
454"
REFERENCES,0.5288788221970555,"and Systems (IROS), pp. 5497–5503, IEEE, 2020.
455"
REFERENCES,0.5300113250283126,"[53] S. Kalyanakrishnan, S. Aravindan, V. Bagdawat, V. Bhatt, H. Goka, A. Gupta, K. Kr-
456"
REFERENCES,0.5311438278595696,"ishna, and V. Piratla, “An analysis of frame-skipping in reinforcement learning,” ArXiv,
457"
REFERENCES,0.5322763306908267,"vol. abs/2102.03718, 2021.
458"
REFERENCES,0.5334088335220838,"[54] A. Srinivas, S. Sharma, and B. Ravindran, “Dynamic action repetition for deep reinforcement
459"
REFERENCES,0.5345413363533409,"learning,” in AAAI, 2017.
460"
REFERENCES,0.535673839184598,"[55] A. Biedenkapp, R. Rajan, F. Hutter, and M. Lindauer, “Temporl: Learning when to act,” in
461"
REFERENCES,0.5368063420158551,"International Conference on Machine Learning, pp. 914–924, PMLR, 2021.
462"
REFERENCES,0.5379388448471121,"[56] S. Sharma, A. Srinivas, and B. Ravindran, “Learning to repeat: Fine grained action repetition
463"
REFERENCES,0.5390713476783692,"for deep reinforcement learning,” ArXiv, vol. abs/1702.06054, 2017.
464"
REFERENCES,0.5402038505096263,"[57] H. Yu, W. Xu, and H. Zhang, “Taac: Temporally abstract actor-critic for continuous control,”
465"
REFERENCES,0.5413363533408834,"Advances in Neural Information Processing Systems, vol. 34, pp. 29021–29033, 2021.
466"
REFERENCES,0.5424688561721405,"[58] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu,
467"
REFERENCES,0.5436013590033975,"A. Gupta, P. Abbeel, et al., “Soft actor-critic algorithms and applications,” arXiv preprint
468"
REFERENCES,0.5447338618346546,"arXiv:1812.05905, 2018.
469"
REFERENCES,0.5458663646659117,"[59] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra,
470"
REFERENCES,0.5469988674971688,"“Continuous control with deep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015.
471"
REFERENCES,0.5481313703284258,"[60] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation error in actor-critic
472"
REFERENCES,0.549263873159683,"methods,” in International conference on machine learning, pp. 1587–1596, PMLR, 2018.
473"
REFERENCES,0.5503963759909399,"[61] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
474"
REFERENCES,0.551528878822197,"Y. Bengio, “Learning phrase representations using rnn encoder-decoder for statistical machine
475"
REFERENCES,0.5526613816534541,"translation,” arXiv preprint arXiv:1406.1078, 2014.
476"
REFERENCES,0.5537938844847112,"[62] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba,
477"
REFERENCES,0.5549263873159683,"“Openai gym,” 2016.
478"
REFERENCES,0.5560588901472253,"[63] G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias, “Brain-inspired replay for continual
479"
REFERENCES,0.5571913929784824,"learning with artificial neural networks,” Nature communications, vol. 11, no. 1, p. 4069, 2020.
480"
REFERENCES,0.5583238958097395,"[64] Y. Zhao, W. Zhao, R. Boney, J. Kannala, and J. Pajarinen, “Simplified temporal consistency
481"
REFERENCES,0.5594563986409966,"reinforcement learning,” in International Conference on Machine Learning, pp. 42227–42246,
482"
REFERENCES,0.5605889014722537,"PMLR, 2023.
483"
REFERENCES,0.5617214043035108,"[65] K. Doya, K. W. Miyazaki, and K. Miyazaki, “Serotonergic modulation of cognitive computa-
484"
REFERENCES,0.5628539071347678,"tions,” Current Opinion in Behavioral Sciences, vol. 38, pp. 116–123, 2021.
485"
REFERENCES,0.5639864099660249,"[66] D. Yarats and I. Kostrikov, “Soft actor-critic (sac) implementation in pytorch.” https://
486"
REFERENCES,0.565118912797282,"github.com/denisyarats/pytorch_sac, 2020.
487"
REFERENCES,0.5662514156285391,"NeurIPS Paper Checklist
488"
CLAIMS,0.5673839184597962,"1. Claims
489"
CLAIMS,0.5685164212910532,"Question: Do the main claims made in the abstract and introduction accurately reflect the
490"
CLAIMS,0.5696489241223103,"paper’s contributions and scope?
491"
CLAIMS,0.5707814269535674,"Answer: [Yes]
492"
CLAIMS,0.5719139297848245,"Justification: The abstract and introduction accurately reflect the paper’s contributions and
493"
CLAIMS,0.5730464326160816,"scope.
494"
CLAIMS,0.5741789354473387,"Guidelines:
495"
CLAIMS,0.5753114382785957,"• The answer NA means that the abstract and introduction do not include the claims
496"
CLAIMS,0.5764439411098528,"made in the paper.
497"
CLAIMS,0.5775764439411099,"• The abstract and/or introduction should clearly state the claims made, including the
498"
CLAIMS,0.578708946772367,"contributions made in the paper and important assumptions and limitations. A No or
499"
CLAIMS,0.579841449603624,"NA answer to this question will not be perceived well by the reviewers.
500"
CLAIMS,0.580973952434881,"• The claims made should match theoretical and experimental results, and reflect how
501"
CLAIMS,0.5821064552661381,"much the results can be expected to generalize to other settings.
502"
CLAIMS,0.5832389580973952,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
503"
CLAIMS,0.5843714609286523,"are not attained by the paper.
504"
LIMITATIONS,0.5855039637599094,"2. Limitations
505"
LIMITATIONS,0.5866364665911665,"Question: Does the paper discuss the limitations of the work performed by the authors?
506"
LIMITATIONS,0.5877689694224235,"Answer: [Yes]
507"
LIMITATIONS,0.5889014722536806,"Justification: Limitations are provided in Section 6
508"
LIMITATIONS,0.5900339750849377,"Guidelines:
509"
LIMITATIONS,0.5911664779161948,"• The answer NA means that the paper has no limitation while the answer No means that
510"
LIMITATIONS,0.5922989807474519,"the paper has limitations, but those are not discussed in the paper.
511"
LIMITATIONS,0.5934314835787089,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
512"
LIMITATIONS,0.594563986409966,"• The paper should point out any strong assumptions and how robust the results are to
513"
LIMITATIONS,0.5956964892412231,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
514"
LIMITATIONS,0.5968289920724802,"model well-specification, asymptotic approximations only holding locally). The authors
515"
LIMITATIONS,0.5979614949037373,"should reflect on how these assumptions might be violated in practice and what the
516"
LIMITATIONS,0.5990939977349944,"implications would be.
517"
LIMITATIONS,0.6002265005662514,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
518"
LIMITATIONS,0.6013590033975085,"only tested on a few datasets or with a few runs. In general, empirical results often
519"
LIMITATIONS,0.6024915062287656,"depend on implicit assumptions, which should be articulated.
520"
LIMITATIONS,0.6036240090600227,"• The authors should reflect on the factors that influence the performance of the approach.
521"
LIMITATIONS,0.6047565118912798,"For example, a facial recognition algorithm may perform poorly when image resolution
522"
LIMITATIONS,0.6058890147225368,"is low or images are taken in low lighting. Or a speech-to-text system might not be
523"
LIMITATIONS,0.6070215175537939,"used reliably to provide closed captions for online lectures because it fails to handle
524"
LIMITATIONS,0.608154020385051,"technical jargon.
525"
LIMITATIONS,0.609286523216308,"• The authors should discuss the computational efficiency of the proposed algorithms
526"
LIMITATIONS,0.6104190260475651,"and how they scale with dataset size.
527"
LIMITATIONS,0.6115515288788222,"• If applicable, the authors should discuss possible limitations of their approach to
528"
LIMITATIONS,0.6126840317100792,"address problems of privacy and fairness.
529"
LIMITATIONS,0.6138165345413363,"• While the authors might fear that complete honesty about limitations might be used by
530"
LIMITATIONS,0.6149490373725934,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
531"
LIMITATIONS,0.6160815402038505,"limitations that aren’t acknowledged in the paper. The authors should use their best
532"
LIMITATIONS,0.6172140430351076,"judgment and recognize that individual actions in favor of transparency play an impor-
533"
LIMITATIONS,0.6183465458663646,"tant role in developing norms that preserve the integrity of the community. Reviewers
534"
LIMITATIONS,0.6194790486976217,"will be specifically instructed to not penalize honesty concerning limitations.
535"
THEORY ASSUMPTIONS AND PROOFS,0.6206115515288788,"3. Theory Assumptions and Proofs
536"
THEORY ASSUMPTIONS AND PROOFS,0.6217440543601359,"Question: For each theoretical result, does the paper provide the full set of assumptions and
537"
THEORY ASSUMPTIONS AND PROOFS,0.622876557191393,"a complete (and correct) proof?
538"
THEORY ASSUMPTIONS AND PROOFS,0.6240090600226501,"Answer: [NA]
539"
THEORY ASSUMPTIONS AND PROOFS,0.6251415628539071,"Justification: The paper does not include theoretical results.
540"
THEORY ASSUMPTIONS AND PROOFS,0.6262740656851642,"Guidelines:
541"
THEORY ASSUMPTIONS AND PROOFS,0.6274065685164213,"• The answer NA means that the paper does not include theoretical results.
542"
THEORY ASSUMPTIONS AND PROOFS,0.6285390713476784,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
543"
THEORY ASSUMPTIONS AND PROOFS,0.6296715741789355,"referenced.
544"
THEORY ASSUMPTIONS AND PROOFS,0.6308040770101925,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
545"
THEORY ASSUMPTIONS AND PROOFS,0.6319365798414496,"• The proofs can either appear in the main paper or the supplemental material, but if
546"
THEORY ASSUMPTIONS AND PROOFS,0.6330690826727067,"they appear in the supplemental material, the authors are encouraged to provide a short
547"
THEORY ASSUMPTIONS AND PROOFS,0.6342015855039638,"proof sketch to provide intuition.
548"
THEORY ASSUMPTIONS AND PROOFS,0.6353340883352209,"• Inversely, any informal proof provided in the core of the paper should be complemented
549"
THEORY ASSUMPTIONS AND PROOFS,0.636466591166478,"by formal proofs provided in appendix or supplemental material.
550"
THEORY ASSUMPTIONS AND PROOFS,0.637599093997735,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.638731596828992,"4. Experimental Result Reproducibility
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6398640996602492,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6409966024915063,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6421291053227633,"of the paper (regardless of whether the code and data are provided or not)?
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6432616081540203,"Answer: [Yes]
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6443941109852774,"Justification: We provide a complete algorithm and list of hyperparameters in the appendix.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6455266138165345,"Additionally we also released the code and trained models.
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6466591166477916,"Guidelines:
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6477916194790487,"• The answer NA means that the paper does not include experiments.
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6489241223103058,"• If the paper includes experiments, a No answer to this question will not be perceived
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6500566251415628,"well by the reviewers: Making the paper reproducible is important, regardless of
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6511891279728199,"whether the code and data are provided or not.
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.652321630804077,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6534541336353341,"to make their results reproducible or verifiable.
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6545866364665912,"• Depending on the contribution, reproducibility can be accomplished in various ways.
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6557191392978482,"For example, if the contribution is a novel architecture, describing the architecture fully
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6568516421291053,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6579841449603624,"be necessary to either make it possible for others to replicate the model with the same
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6591166477916195,"dataset, or provide access to the model. In general. releasing code and data is often
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6602491506228766,"one good way to accomplish this, but reproducibility can also be provided via detailed
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6613816534541337,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6625141562853907,"of a large language model), releasing of a model checkpoint, or other means that are
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6636466591166478,"appropriate to the research performed.
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6647791619479049,"• While NeurIPS does not require releasing code, the conference does require all submis-
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.665911664779162,"sions to provide some reasonable avenue for reproducibility, which may depend on the
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6670441676104191,"nature of the contribution. For example
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6681766704416761,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6693091732729332,"to reproduce that algorithm.
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6704416761041903,"(b) If the contribution is primarily a new model architecture, the paper should describe
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6715741789354474,"the architecture clearly and fully.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6727066817667045,"(c) If the contribution is a new model (e.g., a large language model), then there should
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6738391845979615,"either be a way to access this model for reproducing the results or a way to reproduce
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6749716874292185,"the model (e.g., with an open-source dataset or instructions for how to construct
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6761041902604756,"the dataset).
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6772366930917327,"(d) We recognize that reproducibility may be tricky in some cases, in which case
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6783691959229898,"authors are welcome to describe the particular way they provide for reproducibility.
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6795016987542469,"In the case of closed-source models, it may be that access to the model is limited in
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6806342015855039,"some way (e.g., to registered users), but it should be possible for other researchers
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.681766704416761,"to have some path to reproducing or verifying the results.
590"
OPEN ACCESS TO DATA AND CODE,0.6828992072480181,"5. Open access to data and code
591"
OPEN ACCESS TO DATA AND CODE,0.6840317100792752,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
592"
OPEN ACCESS TO DATA AND CODE,0.6851642129105323,"tions to faithfully reproduce the main experimental results, as described in supplemental
593"
OPEN ACCESS TO DATA AND CODE,0.6862967157417894,"material?
594"
OPEN ACCESS TO DATA AND CODE,0.6874292185730464,"Answer: [Yes]
595"
OPEN ACCESS TO DATA AND CODE,0.6885617214043035,"Justification: We include code to reproduce the results.
596"
OPEN ACCESS TO DATA AND CODE,0.6896942242355606,"Guidelines:
597"
OPEN ACCESS TO DATA AND CODE,0.6908267270668177,"• The answer NA means that paper does not include experiments requiring code.
598"
OPEN ACCESS TO DATA AND CODE,0.6919592298980748,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
599"
OPEN ACCESS TO DATA AND CODE,0.6930917327293318,"public/guides/CodeSubmissionPolicy) for more details.
600"
OPEN ACCESS TO DATA AND CODE,0.6942242355605889,"• While we encourage the release of code and data, we understand that this might not be
601"
OPEN ACCESS TO DATA AND CODE,0.695356738391846,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
602"
OPEN ACCESS TO DATA AND CODE,0.6964892412231031,"including code, unless this is central to the contribution (e.g., for a new open-source
603"
OPEN ACCESS TO DATA AND CODE,0.6976217440543602,"benchmark).
604"
OPEN ACCESS TO DATA AND CODE,0.6987542468856173,"• The instructions should contain the exact command and environment needed to run to
605"
OPEN ACCESS TO DATA AND CODE,0.6998867497168743,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
606"
OPEN ACCESS TO DATA AND CODE,0.7010192525481314,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
607"
OPEN ACCESS TO DATA AND CODE,0.7021517553793885,"• The authors should provide instructions on data access and preparation, including how
608"
OPEN ACCESS TO DATA AND CODE,0.7032842582106456,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
609"
OPEN ACCESS TO DATA AND CODE,0.7044167610419027,"• The authors should provide scripts to reproduce all experimental results for the new
610"
OPEN ACCESS TO DATA AND CODE,0.7055492638731596,"proposed method and baselines. If only a subset of experiments are reproducible, they
611"
OPEN ACCESS TO DATA AND CODE,0.7066817667044167,"should state which ones are omitted from the script and why.
612"
OPEN ACCESS TO DATA AND CODE,0.7078142695356738,"• At submission time, to preserve anonymity, the authors should release anonymized
613"
OPEN ACCESS TO DATA AND CODE,0.7089467723669309,"versions (if applicable).
614"
OPEN ACCESS TO DATA AND CODE,0.710079275198188,"• Providing as much information as possible in supplemental material (appended to the
615"
OPEN ACCESS TO DATA AND CODE,0.711211778029445,"paper) is recommended, but including URLs to data and code is permitted.
616"
OPEN ACCESS TO DATA AND CODE,0.7123442808607021,"6. Experimental Setting/Details
617"
OPEN ACCESS TO DATA AND CODE,0.7134767836919592,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
618"
OPEN ACCESS TO DATA AND CODE,0.7146092865232163,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
619"
OPEN ACCESS TO DATA AND CODE,0.7157417893544734,"results?
620"
OPEN ACCESS TO DATA AND CODE,0.7168742921857305,"Answer: [Yes]
621"
OPEN ACCESS TO DATA AND CODE,0.7180067950169875,"Justification: We provide code, and list of all hyperparameters in appendix
622"
OPEN ACCESS TO DATA AND CODE,0.7191392978482446,"Guidelines:
623"
OPEN ACCESS TO DATA AND CODE,0.7202718006795017,"• The answer NA means that the paper does not include experiments.
624"
OPEN ACCESS TO DATA AND CODE,0.7214043035107588,"• The experimental setting should be presented in the core of the paper to a level of detail
625"
OPEN ACCESS TO DATA AND CODE,0.7225368063420159,"that is necessary to appreciate the results and make sense of them.
626"
OPEN ACCESS TO DATA AND CODE,0.7236693091732729,"• The full details can be provided either with the code, in appendix, or as supplemental
627"
OPEN ACCESS TO DATA AND CODE,0.72480181200453,"material.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7259343148357871,"7. Experiment Statistical Significance
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7270668176670442,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7281993204983013,"information about the statistical significance of the experiments?
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7293318233295584,"Answer: [Yes]
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7304643261608154,"Justification: Error bars are reported for all results presented.
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7315968289920725,"Guidelines:
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7327293318233296,"• The answer NA means that the paper does not include experiments.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7338618346545867,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7349943374858438,"dence intervals, or statistical significance tests, at least for the experiments that support
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7361268403171007,"the main claims of the paper.
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7372593431483578,"• The factors of variability that the error bars are capturing should be clearly stated (for
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7383918459796149,"example, train/test split, initialization, random drawing of some parameter, or overall
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.739524348810872,"run with given experimental conditions).
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7406568516421291,"• The method for calculating the error bars should be explained (closed form formula,
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7417893544733862,"call to a library function, bootstrap, etc.)
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7429218573046432,"• The assumptions made should be given (e.g., Normally distributed errors).
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7440543601359003,"• It should be clear whether the error bar is the standard deviation or the standard error
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7451868629671574,"of the mean.
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7463193657984145,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7474518686296716,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7485843714609286,"of Normality of errors is not verified.
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7497168742921857,"• For asymmetric distributions, the authors should be careful not to show in tables or
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7508493771234428,"figures symmetric error bars that would yield results that are out of range (e.g. negative
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7519818799546999,"error rates).
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.753114382785957,"• If error bars are reported in tables or plots, The authors should explain in the text how
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7542468856172141,"they were calculated and reference the corresponding figures or tables in the text.
654"
EXPERIMENTS COMPUTE RESOURCES,0.7553793884484711,"8. Experiments Compute Resources
655"
EXPERIMENTS COMPUTE RESOURCES,0.7565118912797282,"Question: For each experiment, does the paper provide sufficient information on the com-
656"
EXPERIMENTS COMPUTE RESOURCES,0.7576443941109853,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
657"
EXPERIMENTS COMPUTE RESOURCES,0.7587768969422424,"the experiments?
658"
EXPERIMENTS COMPUTE RESOURCES,0.7599093997734995,"Answer: [Yes]
659"
EXPERIMENTS COMPUTE RESOURCES,0.7610419026047565,"Justification: A description of compute resources used is provided in the appendix
660"
EXPERIMENTS COMPUTE RESOURCES,0.7621744054360136,"Guidelines:
661"
EXPERIMENTS COMPUTE RESOURCES,0.7633069082672707,"• The answer NA means that the paper does not include experiments.
662"
EXPERIMENTS COMPUTE RESOURCES,0.7644394110985278,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
663"
EXPERIMENTS COMPUTE RESOURCES,0.7655719139297849,"or cloud provider, including relevant memory and storage.
664"
EXPERIMENTS COMPUTE RESOURCES,0.766704416761042,"• The paper should provide the amount of compute required for each of the individual
665"
EXPERIMENTS COMPUTE RESOURCES,0.7678369195922989,"experimental runs as well as estimate the total compute.
666"
EXPERIMENTS COMPUTE RESOURCES,0.768969422423556,"• The paper should disclose whether the full research project required more compute
667"
EXPERIMENTS COMPUTE RESOURCES,0.7701019252548131,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
668"
EXPERIMENTS COMPUTE RESOURCES,0.7712344280860702,"didn’t make it into the paper).
669"
CODE OF ETHICS,0.7723669309173273,"9. Code Of Ethics
670"
CODE OF ETHICS,0.7734994337485843,"Question: Does the research conducted in the paper conform, in every respect, with the
671"
CODE OF ETHICS,0.7746319365798414,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
672"
CODE OF ETHICS,0.7757644394110985,"Answer: [Yes]
673"
CODE OF ETHICS,0.7768969422423556,"Justification: the research conducted in the paper conforms, in every respect, with the
674"
CODE OF ETHICS,0.7780294450736127,"NeurIPS Code of Ethics.
675"
CODE OF ETHICS,0.7791619479048698,"Guidelines:
676"
CODE OF ETHICS,0.7802944507361268,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
677"
CODE OF ETHICS,0.7814269535673839,"• If the authors answer No, they should explain the special circumstances that require a
678"
CODE OF ETHICS,0.782559456398641,"deviation from the Code of Ethics.
679"
CODE OF ETHICS,0.7836919592298981,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
680"
CODE OF ETHICS,0.7848244620611552,"eration due to laws or regulations in their jurisdiction).
681"
BROADER IMPACTS,0.7859569648924122,"10. Broader Impacts
682"
BROADER IMPACTS,0.7870894677236693,"Question: Does the paper discuss both potential positive societal impacts and negative
683"
BROADER IMPACTS,0.7882219705549264,"societal impacts of the work performed?
684"
BROADER IMPACTS,0.7893544733861835,"Answer: [Yes]
685"
BROADER IMPACTS,0.7904869762174406,"Justification: See section 6
686"
BROADER IMPACTS,0.7916194790486977,"Guidelines:
687"
BROADER IMPACTS,0.7927519818799547,"• The answer NA means that there is no societal impact of the work performed.
688"
BROADER IMPACTS,0.7938844847112118,"• If the authors answer NA or No, they should explain why their work has no societal
689"
BROADER IMPACTS,0.7950169875424689,"impact or why the paper does not address societal impact.
690"
BROADER IMPACTS,0.796149490373726,"• Examples of negative societal impacts include potential malicious or unintended uses
691"
BROADER IMPACTS,0.797281993204983,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
692"
BROADER IMPACTS,0.79841449603624,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
693"
BROADER IMPACTS,0.7995469988674971,"groups), privacy considerations, and security considerations.
694"
BROADER IMPACTS,0.8006795016987542,"• The conference expects that many papers will be foundational research and not tied
695"
BROADER IMPACTS,0.8018120045300113,"to particular applications, let alone deployments. However, if there is a direct path to
696"
BROADER IMPACTS,0.8029445073612684,"any negative applications, the authors should point it out. For example, it is legitimate
697"
BROADER IMPACTS,0.8040770101925255,"to point out that an improvement in the quality of generative models could be used to
698"
BROADER IMPACTS,0.8052095130237825,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
699"
BROADER IMPACTS,0.8063420158550396,"that a generic algorithm for optimizing neural networks could enable people to train
700"
BROADER IMPACTS,0.8074745186862967,"models that generate Deepfakes faster.
701"
BROADER IMPACTS,0.8086070215175538,"• The authors should consider possible harms that could arise when the technology is
702"
BROADER IMPACTS,0.8097395243488109,"being used as intended and functioning correctly, harms that could arise when the
703"
BROADER IMPACTS,0.8108720271800679,"technology is being used as intended but gives incorrect results, and harms following
704"
BROADER IMPACTS,0.812004530011325,"from (intentional or unintentional) misuse of the technology.
705"
BROADER IMPACTS,0.8131370328425821,"• If there are negative societal impacts, the authors could also discuss possible mitigation
706"
BROADER IMPACTS,0.8142695356738392,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
707"
BROADER IMPACTS,0.8154020385050963,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
708"
BROADER IMPACTS,0.8165345413363534,"feedback over time, improving the efficiency and accessibility of ML).
709"
SAFEGUARDS,0.8176670441676104,"11. Safeguards
710"
SAFEGUARDS,0.8187995469988675,"Question: Does the paper describe safeguards that have been put in place for responsible
711"
SAFEGUARDS,0.8199320498301246,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
712"
SAFEGUARDS,0.8210645526613817,"image generators, or scraped datasets)?
713"
SAFEGUARDS,0.8221970554926388,"Answer: [NA]
714"
SAFEGUARDS,0.8233295583238958,"Justification: the paper poses no such risks.
715"
SAFEGUARDS,0.8244620611551529,"Guidelines:
716"
SAFEGUARDS,0.82559456398641,"• The answer NA means that the paper poses no such risks.
717"
SAFEGUARDS,0.8267270668176671,"• Released models that have a high risk for misuse or dual-use should be released with
718"
SAFEGUARDS,0.8278595696489242,"necessary safeguards to allow for controlled use of the model, for example by requiring
719"
SAFEGUARDS,0.8289920724801813,"that users adhere to usage guidelines or restrictions to access the model or implementing
720"
SAFEGUARDS,0.8301245753114382,"safety filters.
721"
SAFEGUARDS,0.8312570781426953,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
722"
SAFEGUARDS,0.8323895809739524,"should describe how they avoided releasing unsafe images.
723"
SAFEGUARDS,0.8335220838052095,"• We recognize that providing effective safeguards is challenging, and many papers do
724"
SAFEGUARDS,0.8346545866364666,"not require this, but we encourage authors to take this into account and make a best
725"
SAFEGUARDS,0.8357870894677236,"faith effort.
726"
LICENSES FOR EXISTING ASSETS,0.8369195922989807,"12. Licenses for existing assets
727"
LICENSES FOR EXISTING ASSETS,0.8380520951302378,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
728"
LICENSES FOR EXISTING ASSETS,0.8391845979614949,"the paper, properly credited and are the license and terms of use explicitly mentioned and
729"
LICENSES FOR EXISTING ASSETS,0.840317100792752,"properly respected?
730"
LICENSES FOR EXISTING ASSETS,0.8414496036240091,"Answer: [Yes]
731"
LICENSES FOR EXISTING ASSETS,0.8425821064552661,"Justification: We cite original papers for each algorithm and envrionment used
732"
LICENSES FOR EXISTING ASSETS,0.8437146092865232,"Guidelines:
733"
LICENSES FOR EXISTING ASSETS,0.8448471121177803,"• The answer NA means that the paper does not use existing assets.
734"
LICENSES FOR EXISTING ASSETS,0.8459796149490374,"• The authors should cite the original paper that produced the code package or dataset.
735"
LICENSES FOR EXISTING ASSETS,0.8471121177802945,"• The authors should state which version of the asset is used and, if possible, include a
736"
LICENSES FOR EXISTING ASSETS,0.8482446206115515,"URL.
737"
LICENSES FOR EXISTING ASSETS,0.8493771234428086,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
738"
LICENSES FOR EXISTING ASSETS,0.8505096262740657,"• For scraped data from a particular source (e.g., website), the copyright and terms of
739"
LICENSES FOR EXISTING ASSETS,0.8516421291053228,"service of that source should be provided.
740"
LICENSES FOR EXISTING ASSETS,0.8527746319365799,"• If assets are released, the license, copyright information, and terms of use in the
741"
LICENSES FOR EXISTING ASSETS,0.853907134767837,"package should be provided. For popular datasets, paperswithcode.com/datasets
742"
LICENSES FOR EXISTING ASSETS,0.855039637599094,"has curated licenses for some datasets. Their licensing guide can help determine the
743"
LICENSES FOR EXISTING ASSETS,0.8561721404303511,"license of a dataset.
744"
LICENSES FOR EXISTING ASSETS,0.8573046432616082,"• For existing datasets that are re-packaged, both the original license and the license of
745"
LICENSES FOR EXISTING ASSETS,0.8584371460928653,"the derived asset (if it has changed) should be provided.
746"
LICENSES FOR EXISTING ASSETS,0.8595696489241224,"• If this information is not available online, the authors are encouraged to reach out to
747"
LICENSES FOR EXISTING ASSETS,0.8607021517553793,"the asset’s creators.
748"
NEW ASSETS,0.8618346545866364,"13. New Assets
749"
NEW ASSETS,0.8629671574178935,"Question: Are new assets introduced in the paper well documented and is the documentation
750"
NEW ASSETS,0.8640996602491506,"provided alongside the assets?
751"
NEW ASSETS,0.8652321630804077,"Answer: [Yes]
752"
NEW ASSETS,0.8663646659116648,"Justification: The documentation for the released code is provided
753"
NEW ASSETS,0.8674971687429218,"Guidelines:
754"
NEW ASSETS,0.8686296715741789,"• The answer NA means that the paper does not release new assets.
755"
NEW ASSETS,0.869762174405436,"• Researchers should communicate the details of the dataset/code/model as part of their
756"
NEW ASSETS,0.8708946772366931,"submissions via structured templates. This includes details about training, license,
757"
NEW ASSETS,0.8720271800679502,"limitations, etc.
758"
NEW ASSETS,0.8731596828992072,"• The paper should discuss whether and how consent was obtained from people whose
759"
NEW ASSETS,0.8742921857304643,"asset is used.
760"
NEW ASSETS,0.8754246885617214,"• At submission time, remember to anonymize your assets (if applicable). You can either
761"
NEW ASSETS,0.8765571913929785,"create an anonymized URL or include an anonymized zip file.
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8776896942242356,"14. Crowdsourcing and Research with Human Subjects
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8788221970554927,"Question: For crowdsourcing experiments and research with human subjects, does the paper
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8799546998867497,"include the full text of instructions given to participants and screenshots, if applicable, as
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8810872027180068,"well as details about compensation (if any)?
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8822197055492639,"Answer: [NA]
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.883352208380521,"Justification: paper does not involve crowdsourcing nor research with human subjects.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8844847112117781,"Guidelines:
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8856172140430351,"• The answer NA means that the paper does not involve crowdsourcing nor research with
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8867497168742922,"human subjects.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8878822197055493,"• Including this information in the supplemental material is fine, but if the main contribu-
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8890147225368064,"tion of the paper involves human subjects, then as much detail as possible should be
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8901472253680635,"included in the main paper.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8912797281993206,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8924122310305775,"or other labor should be paid at least the minimum wage in the country of the data
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8935447338618346,"collector.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8946772366930917,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8958097395243488,"Subjects
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8969422423556059,"Question: Does the paper describe potential risks incurred by study participants, whether
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8980747451868629,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.89920724801812,"approvals (or an equivalent approval/review based on the requirements of your country or
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9003397508493771,"institution) were obtained?
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9014722536806342,"Answer: [NA]
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9026047565118913,"Justification: the paper does not involve crowdsourcing nor research with human subjects.
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9037372593431483,"Guidelines:
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9048697621744054,"• The answer NA means that the paper does not involve crowdsourcing nor research with
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9060022650056625,"human subjects.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9071347678369196,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9082672706681767,"may be required for any human subjects research. If you obtained IRB approval, you
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9093997734994338,"should clearly state this in the paper.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9105322763306908,"• We recognize that the procedures for this may vary significantly between institutions
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9116647791619479,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.912797281993205,"guidelines for their institution.
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9139297848244621,"• For initial submissions, do not include any information that would break anonymity (if
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9150622876557192,"applicable), such as the institution conducting the review.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9161947904869762,"A
Appendix / supplemental material
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9173272933182333,"A.1
HSP Algorithm
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9184597961494904,"Algorithm 1: Hindsight Sequence Planner
Input: ϕ, ψ1, ψ2, ω. Initial parameters"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9195922989807475,"1 ¯ϕ ←ϕ, ¯ψ1 ←ψ1, ¯ψ2 ←ψ2 ;
// Initialize target network weights"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9207248018120046,"2 D ←∅;
// Initialize an empty replay pool"
FOR EACH ITERATION DO,0.9218573046432617,3 for each iteration do
FOR EACH ITERATION DO,0.9229898074745186,"4
{at, at+1, . . . , at+J−1} ∼πω({at, at+1, . . . , at+J−1}|st) ;
// Sample action
sequence from the policy"
FOR EACH ACTION AT IN THE SEQUENCE DO,0.9241223103057757,"5
for each action at in the sequence do"
FOR EACH ACTION AT IN THE SEQUENCE DO,0.9252548131370328,"6
st+1 ∼p(st+1|st, at) ;
// Sample transition from the environment"
FOR EACH ACTION AT IN THE SEQUENCE DO,0.9263873159682899,"7
D ←D ∪{(st, at, r(st, at), st+1)} ; // Store transition in the replay pool"
END,0.927519818799547,"8
end"
FOR EACH GRADIENT STEP DO,0.928652321630804,"9
for each gradient step do"
FOR EACH GRADIENT STEP DO,0.9297848244620611,"10
ϕ ←ϕ −λm∇ϕLϕ ;
// Update the model parameters"
FOR EACH GRADIENT STEP DO,0.9309173272933182,"11
for i ∈{1, 2} do"
FOR EACH GRADIENT STEP DO,0.9320498301245753,"12
ψi ←ψi −λQ∇ψiLψi ;
// Update the Q-function parameters"
END,0.9331823329558324,"13
end"
END,0.9343148357870895,"14
{at, at+1, . . . , at+J−1} ∼πω({at, at+1, . . . , at+J−1}|st) ;
// Sample action
sequence from the policy"
END,0.9354473386183465,"15
if iteration mod actor_update_frequency == 0 then"
END,0.9365798414496036,"16
for j ∈{1, . . . , J} do"
END,0.9377123442808607,"17
sj+1 ∼m ¯ϕ(sj+1|sj, aj) ;
// Sample transition from the target
model"
END,0.9388448471121178,"18
end"
END,0.9399773499433749,"19
ϕ ←ω −λπ∇ωLω ;
// Update policy weights"
END,0.9411098527746319,"20
end"
END,0.942242355605889,"21
α ←α −λ∇ˆαL(α) ;
// Adjust temperature"
END,0.9433748584371461,"22
for i ∈{1, 2} do"
END,0.9445073612684032,"23
¯ψi ←τψi + (1 −τ) ¯ψi ;
// Update target network weights"
END,0.9456398640996603,"24
end"
END,0.9467723669309174,"25
¯ϕ ←τϕ + (1 −τ)¯ϕ ;
// Update target model weights"
END,0.9479048697621744,"26
end"
END,0.9490373725934315,27 end
END,0.9501698754246886,"Output: ϕ, ψ1, ψ2, ω;
// Optimized parameters 799"
END,0.9513023782559457,"Hyperparameters
800"
END,0.9524348810872028,"The table below lists the hyperparameters that are common between every environment used for all
801"
END,0.9535673839184597,"our experiments for the SAC and HSP algorithms:
802"
END,0.9546998867497168,"A.2
Implementation Details
803"
END,0.9558323895809739,"Due to its added complexity during training, HSP requires longer wall clock time for training when
804"
END,0.956964892412231,"compared to SAC. We performed a minimal hyperparameter search over the actor update frequency
805"
END,0.9580973952434881,"parameter on the Hopper environment (tested values: 1, 2, 4, 8, 16). All the other hyperparamters
806"
END,0.9592298980747452,"were picked to be equal to the SAC implementation. We also did not perform a hyerparameter search
807"
END,0.9603624009060022,"over the size of GRU for the actor. It was picked to have the same size as the hidden layers of the feed
808"
END,0.9614949037372593,"forward network of the actor in SAC. The neural network for the model was also picked to have the
809"
END,0.9626274065685164,"same architecture as the actor from SAC, thus it has two hidden layers with 256 neurons. Similarly
810"
END,0.9637599093997735,"the encoder for the latent HSP implementation was also picked to have the same architecture. For the
811"
END,0.9648924122310306,"latent HSP implementation we also add an additional replay buffer to store transitions of length 5,
812"
END,0.9660249150622876,"to implement the temporal consistency training for the model. This was done for simplicity of the
813"
END,0.9671574178935447,"implementation, and it can be removed since it is redundant to save memory.
814"
END,0.9682899207248018,"Hyperparameter
Value
description
Hidden Layer Size
256
Size of the hidden layers in the feed forward
networks of Actor, Critic, Model and Encoder
networks
Updates per step
1
Number of learning updates per one step in the
environment
Target Update Interval
1
Inverval between each target update
γ
0.99
Discount Factor
τ
0.005
Update rate for the target networks (Critic and
Model)
Learning Rate
0.0003
Learning rate for all neural networks
Replay Buffer Size
106
Size of the replay buffer
Batch Size
256
Batch size for learning
Start Time-steps
10000
Initial number of steps where random policy is
followed
Table 1: List of Common hyperparameters"
END,0.9694224235560589,"Environment
max Timestep
Eval frequency
LunarLanderContinuous-v2
500000
2500
Hopper-v2
1000000
5000
Walker2d-v2
1000000
5000
Ant-v2
5000000
5000
HalfCheetah-v2
5000000
5000
Humanoid-v2
10000000
5000
Table 2: List of environment-specific hyperparameters"
END,0.970554926387316,"All experiments were performed on a GPU cluster the Nvidia 1080ti GPUs. Each run was performed
815"
END,0.9716874292185731,"using a single GPU, utilizing 8 CPU cores of Intel(R) Xeon(R) Silver 4116 (24 core) and 16GB of
816"
END,0.9728199320498301,"memory.
817"
END,0.9739524348810872,"We utilize the pytorch implementation of SAC (https://github.com/denisyarats/pytorch_
818"
END,0.9750849377123443,"sac) (66).
819"
END,0.9762174405436014,"A.3
Latent State Space Experiments
820"
END,0.9773499433748585,"Following the TCRL implementation, we use two encoders: an online encoder eθ and a target encoder
821"
END,0.9784824462061155,"eθ−, which is the exponential moving average of the online encoder:
822"
END,0.9796149490373726,"Encoder : et = eθ(st)
(6)"
END,0.9807474518686297,"Thus, the model predicts the next state in the latent space. Additionally, we introduce multi-step
823"
END,0.9818799546998868,"model prediction for temporal consistency. Following the TCRL work, we use a cosine loss for model
824"
END,0.9830124575311439,"prediction. The model itself predicts only a single step forward, but we enforce temporal consistency
825"
END,0.984144960362401,"by rolling out the model H-steps forward to predict ˜et+1:t+1+H.
826"
END,0.9852774631936579,"Specifically, for an H-step trajectory τ = (zt, at, zt+1)t:t+H drawn from the replay buffer D, we
827"
END,0.986409966024915,"use the online encoder to get the first latent state et = eθ(ot). Then conditioning on the sequence of
828"
END,0.9875424688561721,"actions at:t+H, the model is applied iteratively to predict the latent states ˜et+1 = mϕ(˜et, at). Finally,
829"
END,0.9886749716874292,"we use the target encoder to calculate the target latent states ˆet+1:t+H+1 = eθ−(ot+1:t+1+H). The
830"
END,0.9898074745186863,"Loss function is defined as:
831"
END,0.9909399773499433,"Lθ,ϕ = Eτ∼D  H
X"
END,0.9920724801812004,"h=0
−γh

˜et+h
||˜et+h||2"
END,0.9932049830124575,"T 
ˆet+h
||ˆet+h||2"
END,0.9943374858437146,"
(7)"
END,0.9954699886749717,"We set H = 5 for our experiments. Both the encoder and the model are feed-forward neural networks
832"
END,0.9966024915062288,"with two hidden layers.
833"
END,0.9977349943374858,"Here, we provide complete learning curves for the latent space HSP."
END,0.9988674971687429,"Figure 6: Learning curves of Latent HSP-n and Soft-Actor Critic (SAC) over continuous control
tasks. 834"
