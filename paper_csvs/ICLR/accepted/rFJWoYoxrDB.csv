Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014184397163120568,"Searching for the architecture cells is a dominant paradigm in NAS. However, little
attention has been devoted to the analysis of the cell-based search spaces even
though it is highly important for the continual development of NAS. In this work,
we conduct an empirical post-hoc analysis of architectures from the popular cell-
based search spaces and ﬁnd that the existing search spaces contain a high degree of
redundancy: the architecture performance is minimally sensitive to changes at large
parts of the cells, and universally adopted designs, like the explicit search for a
reduction cell, signiﬁcantly increase the complexities but have very limited impact
on the performance. Across architectures found by a diverse set of search strategies,
we consistently ﬁnd that the parts of the cells that do matter for architecture perfor-
mance often follow similar and simple patterns. By explicitly constraining cells to
include these patterns, randomly sampled architectures can match or even outper-
form the state of the art. These ﬁndings cast doubts into our ability to discover truly
novel architectures in the existing cell-based search spaces, and inspire our sugges-
tions for improvement to guide future NAS research. Code is available at https:
//github.com/xingchenwan/cell-based-NAS-analysis."
INTRODUCTION,0.0028368794326241137,"1
INTRODUCTION"
INTRODUCTION,0.00425531914893617,"Neural Architecture Search (NAS), which automates designs of neural networks by task, has seen
enormous advancements since its invention. In particular, cell-based NAS has become an important
technique in NAS research: in contrast to attempts that directly aim to design architectures at once
and inspired by the classical, manually-designed architectures like VGGNet and ResNet that feature
repeated blocks, cell-based NAS searches for repeated cells only and later stacks them into full
architectures. This simpliﬁcation reduces the search space (although it remains highly complex),
and allows easy transfer of architectures across different tasks/datasets, application scenarios and
resources constraints (Elsken et al., 2019). Indeed, although alternative search spaces exist, cell-based
NAS has received an extraordinary amount of research attention: based on our preliminary survey,
almost 80% of the papers proposing new NAS methods published in ICLR, ICML and NEURIPS in the
past year show at least one part of their major results in standard Differentiable Architecture Search
(DARTS) cell-based spaces and/or highly related ones; and approx. 60% demonstrate results in such
spaces only (detailed in App C). It is fair to state that such cell-based spaces currently dominates.
However, the lagging understanding of these architectures and the search space itself stands in stark
contrast with the volume and pace of new search algorithms. The literature on understanding why
this dominant search space works and comparing what different NAS methods have found is much
more limited in depth and scope, with most existing works typically focusing exclusively on a few
search methods or highlighting high-level patterns only (Shu et al., 2020; Zela et al., 2020). We
argue that strengthening such an understanding is crucial on multiple fronts, and the lack thereof
is concerning: ﬁrst, studying a diverse set of architectures enables us to discover patterns shared
across the search space, the foundation of comparison amongst search methods that heavily inﬂuences
the resultant performances (Yang et al., 2020a): if the search space itself is ﬂawed, designing and
iterating new search methods on it, which is typically computationally intensive, can be misleading
and unproductive. Conversely, understanding any pitfalls of the existing search spaces informs us on
how to design better search spaces in the future, which is critical in advancing the goal of NAS in"
INTRODUCTION,0.005673758865248227,"§Work done while interning at Huawei Noah’s Ark Lab, London, UK."
INTRODUCTION,0.0070921985815602835,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00851063829787234,"ﬁnding novel and high-performing architectures, not only in conventional CNNs but also in emerging
NAS paradigms such as Transformers (which may also take the form of a cell-based design space).
Second, opening the NAS black box enables us to distill the essence of the strong-performing NAS
architectures beneath their surface of complexity. Unlike manually designed architectures where
usually designers attribute performance to speciﬁc designs, currently owing to the apparent complexity
of the design space, the NAS architectures, while all discovered in a similar or identical search space,
are often compared to in terms of ﬁnal performance on a standard dataset only (e.g. CIFAR-10 test
error). This could be problematic, as we do not necessarily understand what NAS has discovered that
led to the purported improvements, and the metric itself is a poor one on which external factors such
as hyperparameter settings, variations in training/data augmentation protocols and even just noise
could exert a greater inﬂuence than the architectural design itself (Yang et al., 2020a). However, by
linking performance to speciﬁc designs, we could ascertain whether any performance differences
stem from the architectures rather than the interfering factors.
We aim to address this problem by presenting a post-hoc analysis of the well-performing architectures
produced by technically diverse search methods. Speciﬁcally, we utilise explainable machine learning
tools to open the NAS black box by inspecting the good- and bad-performing architectures produced
by a wide range of NAS search methods in the dominant DARTS search space. We ﬁnd:
• Performances of architectures can often be disproportionately attributed to a small number of simple
yet critical features that resemble known patterns in classical network designs;
• Many designs almost universally adopted contribute to complexity but not performance;
• The nominal complexity of the search spaces poorly reﬂects the actual diversity of the (high-
performing) architectures discovered, the functional parts of which are often very similar despite
the technical diversity in search methods and the seeming disparity in topology.
In fact, with few simple and human-interpretable constraints, almost any randomly sampled architec-
ture can perform on par or exceed those produced by the state-of-the-art NAS methods over varying
network sizes and datasets (CIFAR-10/IMAGENET). Ultimately, these ﬁndings prompt us to rethink
the suitability of the current standard protocol in evaluating NAS and the capability to ﬁnd truly novel
architectures within the search space. We ﬁnally provide suggestions for prospective new search
spaces inspired by these ﬁndings."
PRELIMINARIES,0.009929078014184398,"2
PRELIMINARIES"
PRELIMINARIES,0.011347517730496455,input0
PRELIMINARIES,0.01276595744680851,input1 0 1 2 3 out
PRELIMINARIES,0.014184397163120567,avg_pool_3x3 (ap3)
PRELIMINARIES,0.015602836879432624,max_pool_3x3 (mp3)
PRELIMINARIES,0.01702127659574468,skip_connect (skip)
PRELIMINARIES,0.018439716312056736,dil_conv_5x5 (d5)
PRELIMINARIES,0.019858156028368795,dil_conv_3x3 (d3)
PRELIMINARIES,0.02127659574468085,sep_conv_5x5 (s5)
PRELIMINARIES,0.02269503546099291,sep_conv_3x3 (s3)
PRELIMINARIES,0.024113475177304965,"Figure 1: The DARTS cell. The solid black arrows de-
note the concatenation at output which is ﬁxed. The
gray dashed arrows denote the 14 potential operation
locations. In a valid DARTS cell, 8 of which are ﬁlled
by one out of the seven candidate primitives listed to the
right, while the other 6 spots are disabled."
PRELIMINARIES,0.02553191489361702,"DARTS space (Fig 1) proposed by Liu et al.
(2019)—which is in turn inspired by Zoph et al.
(2018) and Pham et al. (2018)—is the most inﬂu-
ential search space in cell-based NAS: it takes a
form of a Directed Acyclic Graph (DAG), which
features 2 inputs (connected to the output of two
immediately preceding cells), 1 output and 4
intermediate nodes. The operation o(i,j) on the
(i, j)-th edge is selected from a set of candidate
primitives A with size K = 7 to transform x(i).
At each intermediate node, the output from the
operations on all its predecessors are aggregated:
x(j) = P"
PRELIMINARIES,0.02695035460992908,"i<j o(i,j)(x(i)). And ﬁnally the out
node concatenates all outputs from the intermediate nodes. As shown in Fig 1, the DARTS cell allows
for up to 14 edges/operations. However, to ensure that all intermediate nodes are connected in the
computational graph, each intermediate node is constrained to have exactly 2 in-edges connected to it
from 2 different preceding nodes. Lastly, it is conventional to search for two cells, namely normal αn
and reduce αr cells simultaneously, with αr placed at 1/3 and 2/3 of the total depth of the networks
and αn everywhere else. In total, there are Q4
k=1
72k(k+1)"
PRELIMINARIES,0.028368794326241134,"2
≈109 distinct cells without considering
graph isomorphism, and since both αn and αr are required to fully specify an architecture, there
exist approx. (109)2 = 1018 distinct architectures (Liu et al., 2019). Other cells commonly used are
almost invariably closely related to, and can be viewed as simpliﬁed or more complicated variants of
the DARTS space. For example, the NAS-Bench-201 (NB201) (Dong & Yang, 2020) search space is
simpliﬁed from the DARTS cell (detailed and analysed in App D). Other works have increased the
number of intermediate nodes (Wang et al., 2021b), expanded the primitive pool A (Hundt et al.,
2019) and/or relaxed other constraints (e.g. Shi et al. (2020) allow both incoming edges of the"
PRELIMINARIES,0.029787234042553193,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.031205673758865248,"intermediate nodes to be from the same preceding nodes), but do not fundamentally differ from the
DARTS space. Additionally, search spaces like the NAS-Bench-101 (NB101) (Ying et al., 2019a)
feature cells where the operations are represented as node features, although we believe that the way
the cells are represented should not affect any ﬁndings we will demonstrate, as the DARTS/NB201
spaces can similarly be equivalently represented in a feature-on-node style (Ru et al., 2021; Pham
et al., 2018). We do not experiment on NB101 in the present paper, as it features CIFAR-10 only,
which is similar to NAS-Bench-301 (NB301) (Siems et al., 2020), but the latter is much closer in
terms of size to a “realistic” search space that the community expects and is more amenable to
one-shot methods which are currently mainstream in NAS."
PRELIMINARIES,0.032624113475177303,"3
OPERATION-LEVEL ANALYSIS: REDUNDANCIES IN SEARCH SPACES"
PRELIMINARIES,0.03404255319148936,"The cell-based NAS search space contains multiple sources of complexity: deciding both the speciﬁc
wiring and operations on the edges; searching for 2 cells independently, etc.; each expanding the
search space combinatorially. While this complexity is argued to be necessary for the search space
to be expressive for good-performing, novel architectures to be found, it is unknown whether all
these sources of complexities are equally important, or whether the performance critically depend
on some sub-features only. We argue that answering this question, and identifying such features, if
they are indeed present, is highly signiﬁcant: it enables us to separate the complexities that positively
contribute to performances from those that do not, which could fundamentally affect the design of
search methods. At the level of individual architectures, it helps us understand what NAS has truly
discovered, by removing confounding factors and focusing on the key aspects of the architectures."
PRELIMINARIES,0.03546099290780142,"bananas
darts
de
drnas"
PRELIMINARIES,0.03687943262411347,"gdas
local_search
pc_darts
random"
PRELIMINARIES,0.03829787234042553,"re
rs
tpe"
PRELIMINARIES,0.03971631205673759,"Figure 2: The top archs provide a
good coverage of the search space
and are seemingly diverse: the
t-SNE plot of the top 5% archs
(colored markers; grouped by dif-
ferent search methods. Details in
App B.1) and randomly sampled
archs in the DARTS search space
(gray markers)."
PRELIMINARIES,0.04113475177304964,"For ﬁndings to be generally applicable, we aim not to be speciﬁc
to any search method; this requires us to study a large set of archi-
tectures that are high-performing but technically diverse in terms
of the search methods that produce them. Fortunately, the train-
ing set of NB301 (Siems et al., 2020)—which includes 50,000+
architecture–performance pairs in the DARTS space using a combi-
nation of random sampling and more than 10 state-of-the-art yet
technically diverse methods (detailed in App. B.1)—could be used
for such an analysis: to build a surrogate model that accurately pre-
dicts architecture performance across the entire space. We primarily
focus on the top-5% (2,589) architectures of the training set since
we overwhelmingly care about the good-performing architectures
by deﬁnition in NAS, although as we will show, the main ﬁndings
hold true also for architectures found by methods not covered by
NB301 and for other search spaces like the NB201 space. As shown
in Fig 2, the top architectures are well-spread out in the search space
and are well-separated by search methods, seemingly suggesting
that the architectures discovered by different methods are diverse
in characteristics. Lastly, the worse-performing cells could also be
of interest, as any features observed could be the ones we would
actively like to avoid, and we analyse them in App. A."
PRELIMINARIES,0.0425531914893617,"Operation Importance
To untangle the inﬂuence of each part of
the architecture, we introduce Operation Importance (OI), which measures the incremental effect of
the individual operations (the smallest possible features) to the overall performance. We quantify
this via measuring the expected change in the performance by perturbing the type or wiring of the
operation in question: considering an edge-attributed cell α with edge ei,j currently assigned with
primitive ok, then the operation importance of ok on that edge of cell α is given by:"
PRELIMINARIES,0.04397163120567376,"OI(α, ei,j := ok) =
1
|N(α, ei,j := ok)|"
PRELIMINARIES,0.04539007092198582,"|N (α,ei,j:=ok)|
X m=1"
PRELIMINARIES,0.04680851063829787,"h
y(αm)
i
−y(α),
(1)"
PRELIMINARIES,0.04822695035460993,"where y(α) denotes the validation accuracy or another appropriate performance metric of the fully-
trained architecture induced by cell α. We are interested in measuring the importance of both
the primitive choice and where the operation is located relative to the entire cell, and we use
N(α, ei,j = ok) to denote a set of neighbour cells that differ to α only with the edge in question
assigned with another primitive, ei,j ∈A \ {ok}; or with the same primitive ok but with one end"
PRELIMINARIES,0.04964539007092199,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.05106382978723404,"Normal
Reduce
0 2000 4000 6000 8000 10000"
PRELIMINARIES,0.0524822695035461,Number of occurrences
PRELIMINARIES,0.05390070921985816,(a) All operations
PRELIMINARIES,0.05531914893617021,"Normal
Reduce
0 2000 4000 6000 8000 10000"
PRELIMINARIES,0.05673758865248227,Number of occurrences
PRELIMINARIES,0.05815602836879433,"s3
s5
d3
d5
skip
mp3
ap3"
PRELIMINARIES,0.059574468085106386,(b) Important operations
PRELIMINARIES,0.06099290780141844,"Figure 3: Distribution of (a) all and (b) important op-
erations by the primitive types of the top-performing
archiectures, organised by primitive type."
PRELIMINARIES,0.062411347517730496,"s3
s5
d3
d5 skipmp3ap3
0.002 0.001 0.000 0.001 0.002 0.003 0.004"
PRELIMINARIES,0.06382978723404255,Operation importance
PRELIMINARIES,0.06524822695035461,(a) Normal cells
PRELIMINARIES,0.06666666666666667,"s3
s5
d3
d5 skipmp3ap3
0.002 0.001 0.000 0.001 0.002 0.003 0.004"
PRELIMINARIES,0.06808510638297872,Operation importance
PRELIMINARIES,0.06950354609929078,(b) Reduce cells
PRELIMINARIES,0.07092198581560284,"Figure 4: OI distribution in (a) normal and (b) reduce
cells. The important operations are shown outside the
gray shaded area."
PRELIMINARIES,0.07234042553191489,"node of ei,j being rewired to another node, subjected to any constraints in the search space. It is
worth noting that OI is an instance of the Permutation Feature Importance (PFI) (Breiman, 2001;
Fisher et al., 2019; Molnar, 2019). Given the categorical nature of the “features” in this case, we may
enumerate all permutations on the edge in question instead of having to rely on random sampling as
conventional PFI does. An important operation by Eq (1) would therefore attributed an OI of large
magnitude in either direction, whereas an irrelevant one would have a value of zero since altering it on
expectation leads to no change in architecture performance. We compute the OI of each operation for
the 2,589 architectures. To circumvent the computational challenge of having to train all neighbour
architectures of α (which are outside the NB301 training set) from scratch to compute y(·), we use
the performance prediction ˜y(·) from NB301. However, as we will show, we validate all key ﬁndings
by actually training some architectures to ensure that they are not artefacts of the statistical surrogate
(training protocols detailed in App. B.2)."
PRELIMINARIES,0.07375886524822695,"Findings
The most natural way to group the operations is by their primitive and cell (i.e. normal
or reduce) types, and we show the main results in Figs. 3 and 4. In Fig. 3(b), we discretise
the OI scores using the threshold 0.001 (0.1%)—which is similar to the observed noise standard
deviation of the better-performing architectures from NB301— to highlight the important operations
with |OI| ≥0.001: these are the ones we could more conﬁdently assert to affect the architecture
performance beyond noise effects. We summarise the key ﬁndings below:"
PRELIMINARIES,0.075177304964539,"2
4
6
8
Number of operations disabled 6 5 4 3 2 1 0"
PRELIMINARIES,0.07659574468085106,in test acc (%)
PRELIMINARIES,0.07801418439716312,"Most impt
Least impt"
PRELIMINARIES,0.07943262411347518,"Figure 5: Ground-truth change
in accuracy by successively dis-
abling the most/least important
ops, ordered by their OI. Medians
and interquartile ranges shown;
stars denote that the drop in ac-
curacy is signiﬁcant at p ≤0.01
in the Wilcoxon signed-rank test."
PRELIMINARIES,0.08085106382978724,"(#1) Only a fraction of operations is critical for good performance
within cells: If all operations need to be fully speciﬁed for good
performance (i.e., with both the primitive choice and the speciﬁc
wiring determined), then perturbing any of them should lead to
signiﬁcant performance deterioration. However, this is clearly not
the case in practice: comparing Fig. 3(a) and (b), we observe that
only a fraction of the operations are important based on our deﬁnition.
To verify this directly beyond predicted performances, we randomly
select 30 architectures from the top 5% training set. Within each
cell, we sort their 16 operations by their OI in both ascending and
descending orders. We then successively disable the operations by
zeroing them, and train the resulting architectures with increasing
number of operations disabled from scratch1 until only half of the
active operations remain (Fig. 5). The results largely conﬁrms our
ﬁndings and shows that the OI, although computed via predicted
performance, is accurate in representing the ground-truth importance
of the operations. On average, we need to disable 6 low-OI operations
to see a statistically signiﬁcant drop in performance, and almost half of the operations to match
the effect of disabling just 2 high-OI ones. On the other hand, disabling the high-OI operations
quickly reduce the performance and in some cases stall the training altogether. Noting that the overall
standard deviation of the NB301 training set is just 0.8%, the drop in performance is quite dramatic.
(#2) Reduce cells are relatively unimportant for good performance: Searching independently for
reduce cells scale the search space quadratically, but Fig. 3(b) shows that they contain much fewer
important operations, and Fig. 4 shows that the OI distribution across all primitives are centered close
to zero in reduce cells: the reduce cell is therefore less important to the architecture performance. To
verify this, we draw another 30 random architectures. For each of the them, we construct and train
from scratch the original architecture and 4 derived ones, with (a) reduce cell set identical to normal"
PRELIMINARIES,0.08226950354609928,"1Note that we may not obtain NB301 performance prediction on these architectures, as NB301 requires all
16 operations to be enabled with valid primitives."
PRELIMINARIES,0.08368794326241134,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.0851063829787234,"cell, (b) reduce cell with all operations set to parameterless skip connections, (c) normal cell set
identical to reduce cell and (d) normal cell with operations set to skip connections. Setting the reduce
cell to be identical to the normal cell leads to no signiﬁcant change in performance, while the reverse
is not true (Fig. 6). A more extreme example is that while setting cells to consist of skip connections
only is unsurprisingly sub-optimal in both cases, doing so on the reduce cells harms the performance
much less. This suggests that while searching separately for reduce cells are well-motivated, the
current design, which places much fewer reduce cells than normal cells in the overall architecture yet
treats them equally during search, might be a sub-optimal trade-off; and searching two separate cells
may yield little beneﬁts over the strategy of simply using the same searching rule and applying it on
both normal and reduce cells."
PRELIMINARIES,0.08652482269503546,original
PRELIMINARIES,0.08794326241134752,red<-nor
PRELIMINARIES,0.08936170212765958,nor<-red
PRELIMINARIES,0.09078014184397164,red<-skip
PRELIMINARIES,0.09219858156028368,nor<-skip 5 6 7 8
PRELIMINARIES,0.09361702127659574,"Test error (%) *
*
*"
PRELIMINARIES,0.0950354609929078,"Figure 6: Ground-truth test er-
rors (i.e.
not predicted by
NB301) of the original archs
(original), archs with reduce
cells set identical to their nor-
mal cells (red<-nor)/normal
cells set identical to their reduce
cells (nor<-red) and archs
with normal/reduce cells fully
replaced by skip connections
(nor<-skip/red<-skip). ∗
denotes that the performance dis-
tribution signiﬁcantly differs from
original at p ≤0.01 in the
Wilcoxon signed-rank test."
PRELIMINARIES,0.09645390070921986,"(#3) Different primitives have vastly different importance proﬁles
with many of them redundant:
The set of candidate primitives
A consists of operators that are known to be useful in manually-
designed architectures, with the expectation that they should also be,
to varying degrees, useful to NAS. However, this is clearly not the
case: while it is already known that some primitives are favoured
more by certain search algorithms (Zela et al., 2020), the observed
discrepancy in the relative importance of the primitives is, in fact,
more extreme: the normal cells (which are also the important cells
by Finding #2) across the entire spectrum of good performing archi-
tectures overwhelmingly favour only 3 out of 7 possible primitives:
separable convolutions and skip connection (Fig. 3(a)). Even when
the remaining 4 primitives are occasionally selected, they are almost
never important (Fig. 3(b)). This is also observed in Fig. 4(a) which
shows that they have distributions of OI close to 0. As we will show
later in Sec. 4, we could essentially remove these primitives from
A without impacting the performances. Even within the 3 favoured
primitives, there is a signiﬁcant amount of variation. First, compar-
ing Figs 1(a) and (b), skips, when present in good architectures, are
very likely to be important. We also note that the distribution of OI of
skip has a higher variance – these suggest that the performance of
an architecture is highly sensitive towards the speciﬁc locations and
patterns of skip connections, a detailed study of which we defer to
Sec. 4. On the other hand, while both separable convolutions (s3 and s5) are highly over-represented
in the good-performing architectures, it seems that they are less important than skip. A possible
explanation is that while their presence is required for good performance, their exact locations in the
cell matter less, which we again verify in Sec. 4. Increasingly we are interested in multi-objectives
(e.g. maximising performance while minimising costs); we show that even accounting for this,
redundant primitives, especially pooling, remain largely redundant (App. G)."
PRELIMINARIES,0.09787234042553192,"Discussions
The ﬁndings conﬁrm that both the search space and the cells contain various redundan-
cies that increase the search complexity but do not actually contribute much to performance; and that
good performance most often does not depend on an entire cell but a few key features and primitives
in both individual cells and in the search space. This clearly shows that the search space design can
be further optimised, but consequently, many beliefs often ingrained in existing search methods can
also be sub-optimal or unnecessary. For example, barring a few exceptions (Xie et al., 2019a; You
et al., 2020; Ru et al., 2020; Wan et al., 2022), the overwhelming majority of the current approaches
aims to search for a single, fully deterministic architecture, and this often results in high-dimensional
vector encoding of the cells (e.g. the path encoding of DARTS cell in White et al. (2021) is > 104
dimensions without truncation). This affects the performance in general (White et al., 2020a) and
impedes methods that suffer from curse of dimensionality, such as Gaussian Processes, in particular.
However, exact encoding could be in fact unnecessary if good performance simply hinges upon only
a few key designs while the rest does not matter as much, and ﬁnding relevant low-dimensional,
approximate representations could be beneﬁcial instead."
PRELIMINARIES,0.09929078014184398,"4
SUBGRAPH-LEVEL ANALYSIS: ARE WE TRULY FINDING NOVEL CELLS?"
PRELIMINARIES,0.10070921985815603,"Sec. 3 demonstrates the presence of critical sub-features within good performing architectures. In
this section we aim to ﬁnd what they actually are and whether there are commonalities amongst the"
PRELIMINARIES,0.10212765957446808,Published as a conference paper at ICLR 2022 s3 skip s3 s5
PRELIMINARIES,0.10354609929078014,input2 1 s3 skip s5
PRELIMINARIES,0.1049645390070922,input1 2 skip s3 s3
PRELIMINARIES,0.10638297872340426,input2 3 s3 s5 s5
PRELIMINARIES,0.10780141843971631,input1 4 s3 s3 skip s5
PRELIMINARIES,0.10921985815602837,input2 5 skip s3 s5
PRELIMINARIES,0.11063829787234042,input1 6 skip s3 s5
PRELIMINARIES,0.11205673758865248,input2 7 s3 s5
PRELIMINARIES,0.11347517730496454,input1 8 s3 skip s5
PRELIMINARIES,0.1148936170212766,input2 9 s3 s3 skip
PRELIMINARIES,0.11631205673758865,input2 10 skip s5
PRELIMINARIES,0.11773049645390071,input2 11 skip s5
PRELIMINARIES,0.11914893617021277,input1 12 s3 skip s3
PRELIMINARIES,0.12056737588652482,input2 13 skip s3 s3
PRELIMINARIES,0.12198581560283688,input1 14 s3 skip s3
PRELIMINARIES,0.12340425531914893,input1 15
PRELIMINARIES,0.12482269503546099,"No.
#
#
Support
Support
Ratio
nodes
edges
in impt
in ref"
PRELIMINARIES,0.12624113475177304,"1
5
4
0.0815
0.000386
231
2
4
3
0.0672
0.000772
87.0
3
4
3
0.0579
0.000772
75.0
4
4
3
0.0556
0.000772
72.0
5
5
4
0.0745
0.00116
64.3
6
4
3
0.219
0.00425
51.6
7
4
3
0.239
0.00579
41.3
8
3
2
0.215
0.00966
22.2
9
4
3
0.188
0.00888
21.3
10
4
3
0.103
0.00541
19.1
11
3
2
0.572
0.0339
16.8
12
3
2
0.336
0.0220
15.2
13
4
3
0.141
0.00927
15.2
14
4
3
0.0560
0.00386
14.5
15
4
3
0.0734
0.00541
13.6"
PRELIMINARIES,0.1276595744680851,"Figure 7 & Table 1: Frequent subgraphs in the good-performing architectures ranked by ratio of supports and
their properties. Almost all subgraphs feature skip residual links with additional connections with 1 or more
sep_convs and neither dil_convs nor other parameterless operations."
PRELIMINARIES,0.12907801418439716,"architectures found by technically diverse methods. Towards this goal, operation-level analysis is
insufﬁcient as the performance of neural networks also depends on the architecture topology and
graph properties of the wiring between the operations (Xie et al., 2019a; Ru et al., 2020)."
PRELIMINARIES,0.13049645390070921,"Frequent Subgraph Mining (FSM)
FSM aims to “to extract all the frequent subgraphs, in a given
data set, whose occurrence counts are above a speciﬁed threshold” (Jiang et al., 2013). This is
immensely useful for our use-case, as any frequent subgraphs mined on the architectures represented
by DAGs would naturally represent the interesting recurring structures in the good-performing
architectures, and subgraphs are also widely used for generating explanations (Ying et al., 2019b). In
our speciﬁc case, we 1) convert the computing graphs corresponding to the topology of the same set
of top-performing architectures in Sec. 3 into DAGs, 2) within each DAG, we retain only the important
operations deﬁned in Sec 3 and 3) run an adapted version of the gSpan algorithm (Yan & Han, 2002)
for DAGs on the set of all architecture cell graphs {G1, ..., GT } to identify a set of the most frequent
subgraphs Gf = {gf
1 , ..., gf
M} where each subgraph must have a minimum support S(·) of σ = 0.05:"
PRELIMINARIES,0.13191489361702127,"Sgf
i = |δ(gf
i )|
T
≥σ ∀gf
i ∈Gf, where δ(gf
i ) = {Gj|gf
i ⊆Gj}T
j=1.
(2)"
PRELIMINARIES,0.13333333333333333,"One caveat with support is that it favours simpler subgraphs, which are more likely to be present
“by nature”. To account for this bias, we measure the signiﬁcance of these subgraphs over a null
reference using the ratio between supports of Gf and the reference. The readers are referred to App.
I for detailed explanations."
PRELIMINARIES,0.1347517730496454,"in0
in1
others
all
0.002 0.000 0.002 0.004 0.006 0.008"
PRELIMINARIES,0.13617021276595745,Operation Importance
PRELIMINARIES,0.1375886524822695,"Figure 8: skips are only use-
ful when they form residual links:
in0 and in1 denote the residual
links formed with either inputs,
others denote the skip connec-
tions not forming residual links
and all is the overall distribution
of OI of skip connections."
PRELIMINARIES,0.13900709219858157,"Findings
(#4) Functional parts of many good-performing archi-
tectures are structurally similar to each other and to elements of
classical architectures.
We show the top subgraphs in terms of
the ratio of supports in Fig. 7, and an immediate insight is that the
top frequent subgraphs representing the common traits across the
entire good-performing region of the search space are highly over-
represented over the reference and non-diverse: almost all subgraphs
can be characterised with skip connections forming residual links
between one or both input nodes with an operation node, combined
with different number and/or sizes of separable convolutions. In
fact, we ﬁnd that this ResNet-style residual link is present in 98.5%
(2,815) of the top 2,859 architectures (as a comparison, if we sample
randomly, only approximately half of the architectures are expected
to contain this feature). With reference to Fig. 8, the residual links
drive the importance of skip in Fig. 3. This suggests that skip connections do not just beneﬁt
optimisation of NAS supernets but also actively contribute to generalisation if they posit as residual
connections. The propensity of certain NAS methods of collapsing into cells almost entirely consisting
of skip is well-known with many possible explanations and remedies, but here we provide an alter-
native perspective independent of search methods: more fundamentally, skip is the only primitive
whose exact position greatly impacts the performances in both positive and negative directions and
thus it is more difﬁcult for search methods learn such a relation precisely.
The consensus in preferring the aforementioned pattern also extends beyond the training set of
NB301: with reference to Fig. 9, we select some of the architectures produced by the most recent
works that are not represented in the NB301 training set, and it is clear that despite the different"
PRELIMINARIES,0.14042553191489363,Published as a conference paper at ICLR 2022 skip s5 skip s3 s3 s5 s5 skip
PRELIMINARIES,0.14184397163120568,input1
PRELIMINARIES,0.14326241134751774,"input2
0 1 2
3"
PRELIMINARIES,0.14468085106382977,(a) White et al. (2021) s3 skip s5 s3 s3 s3 s3 d5
PRELIMINARIES,0.14609929078014183,input1
PRELIMINARIES,0.1475177304964539,"input2
0 1 2
3"
PRELIMINARIES,0.14893617021276595,(b) Chen et al. (2021b) skip s3 skip s3 s3 s5 s5 s5
PRELIMINARIES,0.150354609929078,input1
PRELIMINARIES,0.15177304964539007,"input2
0 1 2
3"
PRELIMINARIES,0.15319148936170213,(c) Li et al. (2021) s3 mp3 s3 skip s3 s5 s3 d5
PRELIMINARIES,0.15460992907801419,input1
PRELIMINARIES,0.15602836879432624,"input2
0 1 2
3"
PRELIMINARIES,0.1574468085106383,(d) Ru et al. (2021) s3 s3 skip s3 s3 s3 skip skip
PRELIMINARIES,0.15886524822695036,input1
PRELIMINARIES,0.16028368794326242,"input2
0 1 2
3"
PRELIMINARIES,0.16170212765957448,(e) Wang et al. (2021c) skip skip s5 s5 s3 d3 s5 s5
PRELIMINARIES,0.16312056737588654,input1
PRELIMINARIES,0.16453900709219857,"input2
0 1 2
3"
PRELIMINARIES,0.16595744680851063,"(f) Chen et al. (2021a)
Figure 9: Normal cells of various SoTA (left to right: BANANAS, DRNAS, GAEA, NAS-BOWL, DARTS_PT and
TE-NAS) architectures with the important operations highlighted (the connections to output are omitted since
they are all identical across the DARTS search space). Note all cases considered are consistent with the residual
link + separable convolution patterns identiﬁed, even though the cells and search methods are very different and
except for BANANAS, none of the methods here was used to generate the NB301 training set."
PRELIMINARIES,0.1673758865248227,"search strategies, functional parts of resulting architectures are all characterised by the this pattern of
residual connections and separable convolutions, a combination already well-known and well-used
both in parts and in sum in successful manually designed networks (e.g. Xception (Chollet, 2017)
uses both ingradients). In this sense, many of the existing NAS methods might not have discovered
much more novel architectures beyond what we already know; the functional parts of many SoTA
NAS architectures could be regarded as variations of the classical architectures, whereas the apparent
diversity like the one shown in Fig. 2 is often caused by differences in the non-functional parts of the
architectures that in fact minimally inﬂuence the performances."
PRELIMINARIES,0.16879432624113475,"Random Skip
Prim PrimSkip
5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5"
PRELIMINARIES,0.1702127659574468,Predicted test err (%) y
PRELIMINARIES,0.17163120567375886,(a) Predicted
PRELIMINARIES,0.17304964539007092,"Random Skip
Prim PrimSkip 5.0 5.5 6.0 6.5 7.0"
PRELIMINARIES,0.17446808510638298,Actual test err (%) y
PRELIMINARIES,0.17588652482269504,"(b) Actual
Figure 10: Distribution of (a) NB301 predicted and
(b) actual test error of archs sampled. Random: ran-
dom archs without constraints; Skip: archs with resid-
ual links and otherwise randomly sampled; Prim:
random archs using {s3, s5, skip} only. Prim-
Skip: archs satisfying both Skip and Prim."
PRELIMINARIES,0.1773049645390071,"Generating SoTA architectures
We showed
that many NAS architectures share similar traits,
but a stronger test is whether these simple patterns
alone are sufﬁcient for good performance. We con-
struct architectures simply by random sampling,
but with 2 constraints, without additional search:
1. Normals cell must contain residual link: for ar-
chitecture generation, we simply manually wire
2 skips from both inputs to intermediate node
0 (Skip constraint);
2. The other operations are selected from {s3,
s5} only, with all other primitives removed
(Prim constraint).
While it takes a thorough analysis to study the pat-
terns, the constraints which encode our ﬁndings
themselves are simple, human-interpretable and moderate (note that only Skip is a “hard” constraint
specifying exact wiring; Prim simply constrains sampling to a smaller subspace). We then sample
100 architectures within both constraints with the same rule for both normal and reduce cells and
report their predicted test errors in Fig 10(a). To ensure that we are not biased by the NB301 surrogate,
we actually train 30 of the 100 architectures from scratch (protocols speciﬁed in App. B.2) and report
results in Fig 10(b). To verify the relative importance of each constraint, we also sample the same
number of architectures with no constraints or with either constraint activated. We note that both
constraints effectively narrow the spread of both predicted and actual test errors, with almost any
architecture in the PrimSkip group performing similarly to the SoTA – only <1% of the training set
of NB301 perform better than the mean predicted test error of the PrimSkip group (5.24%) in Fig
10(a), while only 5% perform better than the worst (5.46%). Apart from the two constraints, the
architectures produced are rather varied otherwise in terms of depth/width and exact wiring (see
App. E) – this shows that the two moderate constraints already determine the performance to a great
extent, potentially eclipsing other factors previously believed to inﬂuence performance. We believe
that it might even be possible to fully construct architectures manually from the identiﬁed patterns
to achieve better results, but the main purpose of this experiment is to show that we may narrowly
constrain performance to a very competitive range using very few rules without exactly specifying
the cells, instead of aiming for the absolute best architecture in an already noisy search space. Lastly,
we analyse NB201 space similarly in App. D and very similar ﬁndings hold."
PRELIMINARIES,0.17872340425531916,"Large architectures
We have so far followed the standard NB301 training protocol featuring
smaller architectures trained with fewer (100) epochs, generalisation performance on which does not
necessarily always transfer to larger architectures (Yang et al., 2020a; Shu et al., 2020). Since the
computational cost is much larger, here we ﬁrst evaluate on 2 PrimSkip architectures from Sec. 4
with different width/depth proﬁles, but stack it into a larger architectures and train longer to make the
results comparable those reported the literature. To ensure that the results are completely comparable,"
PRELIMINARIES,0.18014184397163122,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.18156028368794327,"Table 2: Test error of the state-of-the-art architectures on CIFAR-10 and IMAGENET (mobile setting).
(a) CIFAR-10. All baselines are re-evaluated using the procedure in
App. B.2 to ensure the results are completely comparable."
PRELIMINARIES,0.1829787234042553,"Architecture
Top-1 test error (%)
Edit
Original
Edited
dist."
PRELIMINARIES,0.18439716312056736,"DARTSv2 (Liu et al., 2019)
2.44
2.36(−0.08)
1
BANANAS (White et al., 2021)
2.39
2.42(+0.03)
1
DrNAS (Chen et al., 2021b)
2.27
2.31(+0.04)
1
GAEA (Li et al., 2021)
2.31
2.18(−0.13)
0
NAS-BOWL (Ru et al., 2021)
2.33
2.23(−0.10)
2
NoisyDARTS (Chu et al., 2020)
2.57
2.42(−0.15)
4
DARTS_PT (Wang et al., 2021c)
2.33
2.35(+0.02)
2
SDARTS_PT (Wang et al., 2021c)
2.46
2.36(−0.10)
4
SGAS_PT (Wang et al., 2021c)
2.92
2.48(−0.44)
3"
PRELIMINARIES,0.18581560283687942,"PrimSkip Arch 1
2.27
-
-
PrimSkip Arch 2
2.29
-
-"
PRELIMINARIES,0.18723404255319148,"(b) ImageNet. All baselines are taken from the original papers as re-
evaluation is too costly in this case."
PRELIMINARIES,0.18865248226950354,"Architecture
Test error (%)
Params
Top-1
Top-5
(M)"
PRELIMINARIES,0.1900709219858156,"DARTSv2 (Liu et al., 2019)
26.7
8.7
4.7
SNAS (Xie et al., 2019b)
27.3
9.2
4.3
GDAS (Dong & Yang, 2020)
26.0
8.5
5.3
DrNAS† (Chen et al., 2021b)
24.2
7.3
5.2
GAEA(C10) (Li et al., 2021)
24.3
7.3
5.3
GAEA(ImageNet)† (Li et al., 2021)
24.0
7.3
5.6
PDARTS (Chen et al., 2019)
24.4
7.4
4.9
PC-DARTS(C10) (Xu et al., 2020)
25.1
7.8
5.3
PC-DARTS(ImageNet)† (Xu et al., 2020)
24.2
7.3
5.3"
PRELIMINARIES,0.19148936170212766,"PrimSkip Arch 1
24.4
7.4
5.7
PrimSkip Arch 2
23.9
7.0
5.7"
PRELIMINARIES,0.19290780141843972,†: searched directly on ImageNet.
PRELIMINARIES,0.19432624113475178,"on CIFAR-10 experiments we do not simply take the baseline results from the original papers; instead,
we obtain the cell speciﬁcations provided in some of the most recent papers (see App. E for detailed
speciﬁcations), re-train each from scratch using a standardised setup (See App. B.3 for details), and
show the performance in the “Original” column of Table 2a. Recognising that the performance on
CIFAR-10 is usually quite noisy (std dev around 0.05 −0.1%), the sampled architectures perform at
least on par with the SoTA architectures produced from much more sophisticated search algorithms.
To further verify our ﬁndings, we conduct an additional experiment where we edit the SoTA ar-
chitectures minimally to make them comply to the PrimSkip constraints: whenever a cell contains
primitives outside {s3, s5, skip}, we replace them with ones that are in this set, and we always
set the reduce cell to be identical to the normal cell (see App. E for detailed speciﬁcations of the
architectures). We also create residual links if they are not present, and replace non-residual skips
with s3/s5 between operations, if any; we do not alter any wiring. Most architectures are already
close to conforming to the constraints, so the number of edits required is often small (edit distances
are under “Edit dist.” column in Table 2a); in fact, the GAEA cell is already fully-compliant and we
only replace its reduce cell with the normal cell). We show the test errors of the edited architectures
along with the change from the original ones in “Edited” column of Table 2a: the edits result in an
improvement in test error up to 0.44% in 6/9 cases, and even where test errors increase after the
edits, the differences are marginal and probably within margins of error. This shows that at least for
the architectures we consider, the SoTA architectures can all be consistently explained by the same
simple pattern identiﬁed. We ﬁnally train the same sampled PrimSkip architectures on ImageNet
using a training protocol adopted in the literature (App. B.3) and the results are shown in Table 2b.
Accounting for the estimated evaluation noise (most NAS papers do not run ImageNet experiments
with multiple seeds, but comparable works like Xie et al. (2019a); Goyal et al. (2017) estimate a noise
standard deviation of 0.2 −0.5% in Top-1 error), it is fair to say that both PrimSkip architectures
perform on par to, if not better than, the SoTA, even though some of the SoTA architectures are
searched on ImageNet directly, an extremely expensive procedure in terms of computational costs."
PRELIMINARIES,0.19574468085106383,"Discussions
We ﬁnd that the despite the different search methods and the belief that the search
space contains diverse solutions, the key features of many top architectures in the DARTS (and
NB201) space are largely similar to each other, and may collectively be viewed as variants to classical
architectures. This suggests a gap between the apparent and effective diversity of the search space,
potentially explaining the discrepancy between the huge search space and the small performance
variability. We also show highly complicated SoTA methods fail to signiﬁcantly outperform random
search with few mild constraints, further demonstrating that it is these simple traits, and not other
complexities, that drive the performance. Consequently, we argue that we should rethink the role
that existing cell-based search spaces play as the key (and sometimes the only) venue on which the
search methods develop and iterate. While it is reassuring that we ﬁnd elements known to perform
well, we should not over-rely on such search spaces but should continuously improve on them. Also,
while we do not rule out the possibility that there could be other good-performing architectures not
represented by the patterns identiﬁed, it is doubtful whether the search space, while nominally huge,
truly contains novel and performant architectures beyond our knowledge."
RELATED WORKS,0.1971631205673759,"5
RELATED WORKS"
RELATED WORKS,0.19858156028368795,"There are multiple previous works that also aim to explain and/or ﬁnd patterns in cell-based NAS: Shu
et al. (2020) ﬁnd search methods in the DARTS space to favour shallow and wide cells but conclude
they do not necessarily generalise better, and hence the pattern does not explain performances. Ru et al.
(2021) also use subgraphs to explain performances, but only consider ﬁrst-order Weisfeiler-Lehman"
RELATED WORKS,0.2,Published as a conference paper at ICLR 2022
RELATED WORKS,0.20141843971631207,"(WL) features which could be overly restrictive (note that most subgraphs we ﬁnd in Fig. 7 are not
limited to 1-WL features) and speciﬁc to the search method proposed. Zela et al. (2020) account
for failure mode of differentiable NAS, but ultimately focus on a family of related search methods
while the current work is search method-agnostic. On a search space level, a closely related work is
Yang et al. (2020a), ﬁndings from whom we use extensively, but they mainly identify problems, not
explanations; Xie et al. (2019a), Ru et al. (2020) and You et al. (2020) relate performances with graph
theoric properties of networks, but it is unclear to what extent these apply to standard cell-based
NAS as the search spaces considered are signiﬁcantly different (e.g. they typically feature much
fewer primitive choices). Lastly, in constructing NAS benchmarks, Dong & Yang (2020); Siems
et al. (2020); Ying et al. (2019a) have also provided various insights and patterns, but current work
advances such understanding further via a comprehensive and experimentally validated investigation."
SUGGESTIONS FOR FUTURE NAS PRACTICES,0.2028368794326241,"6
SUGGESTIONS FOR FUTURE NAS PRACTICES"
SUGGESTIONS FOR FUTURE NAS PRACTICES,0.20425531914893616,"We believe this work to be useful as an investigation of the existing cell-based NAS as well as to
inspire future ones, not only on conventional CNNs but also emerging architectures like Transformers.
On a search space level, we ﬁnd a mismatch between the nominal and the effective complexity:
complexities are as useful as they contribute to performance and novelty, and thus in a hypothetical
new space, we should aim to be aware of these non-functional complexities, and not simply augment
the number of primitives available and/or expanding the sizes of the cells. However, identifying such
redundancies in a new search space is very challenging a-priori, but fortunately the analysis tools used
in this paper are model-agnostic, and thus could be applied to any new search space candidates. Also,
while we use the NB301 predictors which train a huge number of architectures to ensure the ﬁndings
are as representative as possible, we show in App. F that combined with an appropriate surrogate
regression model, we may reproduce most ﬁndings by training as few as 200 architectures (or 0.4%
of the full training set); this suggests that the techniques used could also be cost-effective tools to
inspect new search spaces. Another under-explored possibility would be iterative search-and-prune at
the search space level, as opposed to the architecture level which is relatively well studied. Using the
tools and metrics we introduced to incrementally grow the search space from simpler structures and
prune out those redundant ones in a principled manner.
Notwithstanding the issues identiﬁed, we believe that the cell-based design paradigm remains valuable
for proper benchmarking and comparison of NAS methods, but the many woes could be due to the
over-engineered cells and the under-engineered macro-connections often ﬁxed in a way that is heavily
borrowed from classical networks. Here we identify three promising directions that would hopefully
resolve or at least alleviate some of the problems identiﬁed: Firstly, we could simplify the cells but
relax the constraints on how different cells are connected in the search space. Secondly, as discussed,
we might be implicitly biased to favour architectures that resemble known architectures, preventing
NAS to discover truly novel architectures. A possible solution is to search on a more fundamental
level free of the bias of existing designs such as the paradigm championed by Real et al. (2020).
Lastly, considering how much benchmarks have democratised and popularised NAS research, there is
also a need for NAS benchmarks beyond simple cell-based spaces. The readers are referred to App. J
for a more detailed discussions on our suggestions for future NAS researchers."
CONCLUSION,0.20567375886524822,"7
CONCLUSION"
CONCLUSION,0.20709219858156028,"We present a post-hoc analysis of architectures in the most popular cell-based search spaces. We ﬁnd
a mismatch between the nominal the effective complexity, as many good-performing architectures,
despite discovered by very different search methods, share similar traits. We also ﬁnd many redundant
design options, as performances of the architectures disproportionately depend on certain patterns
while the rest are often irrelevant. We conclude that like the rapidly iterating search methods, the
search spaces also need to evolve to match the progress of NAS and we provide suggestions based on
the main ﬁndings in the paper, the latter of which also form some of the most evident directions of
future work. Lastly, while cell-based NAS focusing on image classiﬁcation is currently mainstream
and the present paper focuses exclusively on such, alternative spaces and/or tasks exist (Howard et al.,
2019; Wu et al., 2019; Cai et al., 2019; Duan et al., 2021; Tu et al., 2021). By adopting a macro
search framework or focusing on an alternative task, they might be less subjected to some of the
issues identiﬁed. Since the tools presented in the paper are largely search space-agnostic, a further
future direction would also be to extend some of the analyses to them."
CONCLUSION,0.20851063829787234,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.2099290780141844,ETHICS STATEMENT
ETHICS STATEMENT,0.21134751773049645,"While we do not see immediate ethical repercussions of our work in particular, we believe that the
general topics of neural architecture search and automated machine learning (AutoML) that our work
focuses on do involve broader interests at stake. On the positive side, gaining more knowledge on
AutoML and NAS could lead to improved democratisation of deep learning models to non-experts as
they automate machine learning pipelines that previously could require immense human expertise.
On the negative side, it is worth noting that NAS and AutoML (including some of our suggestions
for future directions in Sec. 6) are often computationally expensive. Advancement in AutoML and
NAS might prompt more practitioners to use them instead of off-the-shelf models, which could lead
to increased monetary and environmental costs. Finally, while AutoML and NAS themselves are
ethically neural, there is possibility for them to be misused in ethically dubious ways. We believe that
ultimately the practitioners and researchers have to make the judgement call to ensure that ethical use
of the technologies in their domain of application is always ensured."
REPRODUCIBILITY STATEMENT,0.2127659574468085,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.21418439716312057,"The implementations details to reproduce the major experimental results of the paper are in-
cluded in App.
B. The code is open-sourced at https://github.com/xingchenwan/
cell-based-NAS-analysis."
REFERENCES,0.21560283687943263,REFERENCES
REFERENCES,0.2170212765957447,"James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization.
Advances in neural information processing systems, 24, 2011."
REFERENCES,0.21843971631205675,"Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001."
REFERENCES,0.2198581560283688,"Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware.
International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.22127659574468084,"Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A
theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021a."
REFERENCES,0.2226950354609929,"Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Drnas: Dirichlet neural
architecture search. International Conference on Learning Representations (ICLR), 2021b."
REFERENCES,0.22411347517730495,"Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth
gap between search and evaluation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 1294–1303, 2019."
REFERENCES,0.225531914893617,"François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1251–1258, 2017."
REFERENCES,0.22695035460992907,"Xiangxiang Chu, Bo Zhang, and Xudong Li.
Noisy differentiable architecture search.
arXiv preprint
arXiv:2005.03566, 2020."
REFERENCES,0.22836879432624113,"Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. {DARTS}-: Robustly step-
ping out of performance collapse without indicators. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=KLH36ELmwIB."
REFERENCES,0.2297872340425532,"Tom Den Ottelander, Arkadiy Dushatskiy, Marco Virgolin, and Peter AN Bosman. Local search is a remarkably
strong baseline for neural architecture search. In International Conference on Evolutionary Multi-Criterion
Optimization, pp. 465–479. Springer, 2021."
REFERENCES,0.23120567375886525,"Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search.
International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.2326241134751773,"Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang, and Zhenguo Li. Transnas-bench-
101: Improving transferability and generalizability of cross-task neural architecture search. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5251–5260, 2021."
REFERENCES,0.23404255319148937,"Łukasz Dudziak, Thomas Chau, Mohamed S Abdelfattah, Royson Lee, Hyeji Kim, and Nicholas D Lane.
Brp-nas: Prediction-based nas using gcns. Advances in Neural Information Processing Systems (NeurIPS) 33,
2020."
REFERENCES,0.23546099290780143,Published as a conference paper at ICLR 2022
REFERENCES,0.23687943262411348,"Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of
Machine Learning Research, 20(1):1997–2017, 2019."
REFERENCES,0.23829787234042554,"Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a
variable’s importance by studying an entire class of prediction models simultaneously. J. Mach. Learn. Res.,
20(177):1–81, 2019."
REFERENCES,0.2397163120567376,"Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017."
REFERENCES,0.24113475177304963,"Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun
Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 1314–1324, 2019."
REFERENCES,0.2425531914893617,"Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture
search. arXiv preprint arXiv:1903.09900, 2019."
REFERENCES,0.24397163120567375,"Chuntao Jiang, Frans Coenen, and Michele Zito. A survey of frequent subgraph mining algorithms. The
Knowledge Engineering Review, 28(1):75–105, 2013."
REFERENCES,0.2453900709219858,"Hayeon Lee, Eunyoung Hyung, and Sung Ju Hwang. Rapid neural architecture search by learning to generate
graphs from datasets. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=rkQuFUmUOg3."
REFERENCES,0.24680851063829787,"Liam Li, Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Geometry-aware gradient algorithms
for neural architecture search. International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.24822695035460993,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. International
Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.24964539007092199,"Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu. Semi-supervised neural architecture
search. arXiv preprint arXiv:2002.10389, 2020."
REFERENCES,0.251063829787234,"Christoph Molnar.
Interpretable Machine Learning.
2019.
https://christophm.github.io/
interpretable-ml-book/."
REFERENCES,0.2524822695035461,"Niv Nayman, Yonathan Aﬂalo, Asaf Noy, and Lihi Zelnik-Manor. Hardcore-nas: Hard constrained differentiable
neural architecture search. International Conference on Machine Learning, 2021."
REFERENCES,0.25390070921985813,"Vu Nguyen, Tam Le, Makoto Yamada, and Michael A Osborne. Optimal transport kernels for sequential and
parallel neural architecture search. In International Conference on Machine Learning, pp. 8084–8095. PMLR,
2021."
REFERENCES,0.2553191489361702,"Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture search via
parameters sharing. In International Conference on Machine Learning, pp. 4095–4104. PMLR, 2018."
REFERENCES,0.25673758865248225,"Kenneth Price, Rainer M Storn, and Jouni A Lampinen. Differential evolution: a practical approach to global
optimization. Springer Science & Business Media, 2006."
REFERENCES,0.2581560283687943,"Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiﬁer
architecture search. In Proceedings of the aaai conference on artiﬁcial intelligence, volume 33, pp. 4780–4789,
2019."
REFERENCES,0.25957446808510637,"Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: Evolving machine learning algorithms from
scratch. In International Conference on Machine Learning, pp. 8007–8019. PMLR, 2020."
REFERENCES,0.26099290780141843,"Binxin Ru, Pedro Esperanca, and Fabio Carlucci. Neural architecture generator optimization. Advances in
Neural Information Processing Systems (NeurIPS) 33, 2020."
REFERENCES,0.2624113475177305,"Binxin Ru, Xingchen Wan, Xiaowen Dong, and Michael Osborne. Interpretable neural architecture search via
bayesian optimisation with weisfeiler-lehman kernels. International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.26382978723404255,"Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T Kwok, and Tong Zhang. Bridging the gap between
sample-based and one-shot neural architecture search with bonas. Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.2652482269503546,"Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures learnt by cell-based neural architecture
search. International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.26666666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.2680851063829787,"Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, and Frank Hutter. Nas-bench-301 and
the case for surrogate benchmarks for neural architecture search. arXiv preprint arXiv:2008.09777, 2020."
REFERENCES,0.2695035460992908,"Xiu Su, Shan You, Tao Huang, Fei Wang, Chen Qian, Changshui Zhang, and Chang Xu. Locally free weight
sharing for network width search. International Conference on Learning Representations, 2021a."
REFERENCES,0.27092198581560284,"Xiu Su, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, and Chang Xu. K-shot nas:
Learnable weight-sharing for nas with k-shot supernets. International Conference on Machine Learning,
2021b."
REFERENCES,0.2723404255319149,"Renbo Tu, Mikhail Khodak, Nicholas Roberts, and Ameet Talwalkar. Nas-bench-360: Benchmarking diverse
tasks for neural architecture search. arXiv preprint arXiv:2110.05668, 2021."
REFERENCES,0.27375886524822696,"Xingchen Wan, Binxin Ru, Pedro M Esparança, and Fabio Maria Carlucci. Approximate neural architecture
search via operation distribution learning. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision, pp. 2377–2386, 2022."
REFERENCES,0.275177304964539,"Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra. Alphanet: Improved training of supernet
with alpha-divergence. International Conference on Machine Learning, 2021a."
REFERENCES,0.2765957446808511,"Jiaxing Wang, Haoli Bai, Jiaxiang Wu, Xupeng Shi, Junzhou Huang, Irwin King, Michael Lyu, and Jian Cheng.
Revisiting parameter sharing for automatic neural channel number search. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.27801418439716313,"Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. Sample-efﬁcient neural architecture
search by learning action space. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021b."
REFERENCES,0.2794326241134752,"Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh. Rethinking architecture
selection in differentiable nas. International Conference on Learning Representations (ICLR), 2021c."
REFERENCES,0.28085106382978725,"Colin White, Willie Neiswanger, Sam Nolen, and Yash Savani. A study on encodings for neural architecture
search. Advances in Neural Information Processing Systems, 2020a."
REFERENCES,0.2822695035460993,"Colin White, Sam Nolen, and Yash Savani. Local search is state of the art for nas benchmarks. arXiv preprint
arXiv:2005.02960, 2020b."
REFERENCES,0.28368794326241137,"Colin White, Willie Neiswanger, and Yash Savani. Bananas: Bayesian optimization with neural architectures for
neural architecture search. AAAI Conference on Artiﬁcial Intelligence, 1(2), 2021."
REFERENCES,0.2851063829787234,"Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda,
Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural
architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 10734–10742, 2019."
REFERENCES,0.2865248226950355,"Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring randomly wired neural networks
for image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
1284–1293, 2019a."
REFERENCES,0.28794326241134754,"Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. International
Conference on Learning Representations (ICLR), 2019b."
REFERENCES,0.28936170212765955,"Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. Pc-darts:
Partial channel connections for memory-efﬁcient architecture search. International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.2907801418439716,"Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang. Does unsupervised architecture representation learning
help neural architecture search? Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.29219858156028367,"Shen Yan, Colin White, Yash Savani, and Frank Hutter. Nas-bench-x11 and the power of learning curves. Neural
Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.2936170212765957,"Xifeng Yan and Jiawei Han. gspan: Graph-based substructure pattern mining. In 2002 IEEE International
Conference on Data Mining, 2002. Proceedings., pp. 721–724. IEEE, 2002."
REFERENCES,0.2950354609929078,"Antoine Yang, Pedro M Esperança, and Fabio M Carlucci. Nas evaluation is frustratingly hard. International
Conference on Learning Representations (ICLR), 2020a."
REFERENCES,0.29645390070921984,"Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, and Zhouchen Lin. Ista-nas: Efﬁcient and consistent
neural architecture search by sparse coding. Advances in Neural Information Processing Systems (NeurIPS)
33, 2020b."
REFERENCES,0.2978723404255319,Published as a conference paper at ICLR 2022
REFERENCES,0.29929078014184396,"Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101:
Towards reproducible neural architecture search. In International Conference on Machine Learning, pp.
7105–7114. PMLR, 2019a."
REFERENCES,0.300709219858156,"Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnn explainer: A tool for post-hoc
explanation of graph neural networks. arXiv preprint arXiv:1903.03894, 2019b."
REFERENCES,0.3021276595744681,"Jiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. Graph structure of neural networks. In International
Conference on Machine Learning, pp. 10881–10891. PMLR, 2020."
REFERENCES,0.30354609929078014,"Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding
and robustifying differentiable architecture search. International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.3049645390070922,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412, 2017."
REFERENCES,0.30638297872340425,"Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan Ge, and Steven Su. Differentiable neural
architecture search in equivalent space with exploration enhancement. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.3078014184397163,"Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, and Tian Guo. Few-shot neural architecture
search. In International Conference on Machine Learning, pp. 12707–12718. PMLR, 2021."
REFERENCES,0.30921985815602837,"Pan Zhou, Caiming Xiong, Richard Socher, and Steven C. H. Hoi. Theory-inspired path-regularized differential
network architecture search. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.31063829787234043,"Barret Zoph and Quoc V Le.
Neural architecture search with reinforcement learning.
arXiv preprint
arXiv:1611.01578, 2016."
REFERENCES,0.3120567375886525,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable
image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
8697–8710, 2018."
REFERENCES,0.31347517730496455,Published as a conference paper at ICLR 2022
REFERENCES,0.3148936170212766,APPENDICES
REFERENCES,0.31631205673758866,"A
ANALYSIS ON WORST-PERFORMING ARCHITECTURES"
REFERENCES,0.3177304964539007,"Normal
Reduce
0 1000 2000 3000 4000 5000"
REFERENCES,0.3191489361702128,Number of occurrences
REFERENCES,0.32056737588652484,(a) All operations
REFERENCES,0.3219858156028369,"Normal
Reduce
0 1000 2000 3000 4000 5000"
REFERENCES,0.32340425531914896,Number of occurrences
REFERENCES,0.324822695035461,(b) Important operations
REFERENCES,0.3262411347517731,"Figure 11: Distribution of (a) all and (b) important op-
erations by the primitive type of the worst-performing
cells. The gray dashed line in (a) denotes the expected
number of occurrences if the operations are uniformly
sampled in each cell."
REFERENCES,0.3276595744680851,"s3
s5
d3
d5 skipmp3ap3
0.020 0.015 0.010 0.005 0.000 0.005 0.010"
REFERENCES,0.32907801418439714,Operation importance
REFERENCES,0.3304964539007092,(a) Normal cells
REFERENCES,0.33191489361702126,"s3
s5
d3
d5 skipmp3ap3
0.020 0.015 0.010 0.005 0.000 0.005 0.010"
REFERENCES,0.3333333333333333,Operation importance
REFERENCES,0.3347517730496454,(b) Reduce cells
REFERENCES,0.33617021276595743,"Figure 12: Box-and-whisker plots showing the distri-
bution of the operations importance in (a) normal and
(b) reduce cells by primitive types. The important oper-
ations by the deﬁnition of the paper are shown outside
the gray shaded area."
REFERENCES,0.3375886524822695,"We also conduct operation-level analysis on the worst 5% performing architectures of the NB301
training data and the results are shown in Figs 11 and 12, and we ﬁnd that both pooling operators
almost never contribute to good performing architectures as shown in Sec 3, but they actively hurt
performance in the poor architectures: from Fig 11, it is clear that the worst performing cells are
both characterised by large number of pooling operators (Fig 11(a) and a large number of important
pooling operators actively degrading performance: this is unsurprising and also pointed out in the
analysis in the original NB301 paper (Siems et al., 2020) as cells with a large number of pooling
operations aggressively cause loss of information. Other than that, both dilated convolution operations
remain rather neutral and the separable convolutions remain positive in operation importance even
in poorly-performing architectures. Skip connections in this case are quite negative in general but
still have a very large spread – this could be either due to a large number of skip connections in the
cell which is a known failure mode of many differentiable NAS algorithms Zela et al. (2020) or that
skip connections need to be paired with separable convolutions as shown in Fig 7 for positive effects,
which is not the case in these poor architectures where separable convolutions are underrepresented.
We also repeat the subgraph-level analysis on these architectures (Fig 13). An interesting insight is
that the poorly performing subgraphs are much more “diverse” than the good ones, and the primitives
in the two groups almost never overlap: the none of positive subgraphs in 7 contains {d3, d5,
mp3, ap3}, whereas none of the negative subgraphs contains {s3, s5}. This shows that in the
present search space the primitives are somewhat “separable” and the redundant primitives {d3,
d5, mp3, ap3} may simply be discarded without affecting the resulting performances – we argue
this should not be the case in a well-designed ideal search space. In principle, every primitive should
be a building block that that potentially contribute to architectures positively at least in some cases."
REFERENCES,0.33900709219858155,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.3404255319148936,"B.1
DATA"
REFERENCES,0.34184397163120567,"We primarily use the data from the training set of the NB301 benchmark (Siems et al., 2020),
available at the ofﬁcial repository at https://github.com/automl/nasbench301. To
obtain a sound performance surrogate over the entire DARTS search space, NB301 trains more
than 50,000 architectures using the protocol listed in App B.2, and use the architectures and their
corresponding test performance on CIFAR-10 as inputs and labels to train a number of surrogate
models as performance predictors, including GIN, XGBoost and LGBoost (in this paper, we always
use the XGBoost surrogate as it is shown to be the best on balance according to Siems et al.
(2020)). The architectures are produced from a number of technically diverse methods representing
almost all mainstream genres of cell-based NAS like gradient-based methods, Bayesian optimisation,
reinforcement learning and simpler methods like local search and random search: DARTS (Liu et al.,
2019), DRNAS (Chen et al., 2021b), GDAS (Dong & Yang, 2020), reinforcement learning (RL) (Zoph
& Le, 2016), differential evolution (DE) (Price et al., 2006), PC_DARTS (Xu et al., 2020), Tree parzen"
REFERENCES,0.3432624113475177,Published as a conference paper at ICLR 2022 mp3 mp3 mp3
REFERENCES,0.3446808510638298,input2 1 mp3 mp3 mp3
REFERENCES,0.34609929078014184,input1 2 d5 mp3 3 ap3 ap3 ap3
REFERENCES,0.3475177304964539,input2 4 mp3 mp3 mp3
REFERENCES,0.34893617021276596,input2 5 ap3 ap3 ap3
REFERENCES,0.350354609929078,input1 6 mp3 mp3 mp3
REFERENCES,0.3517730496453901,input1 7 ap3 d5
REFERENCES,0.35319148936170214,input2 8 d3 d5
REFERENCES,0.3546099290780142,input2 9 mp3 skip
REFERENCES,0.35602836879432626,input1 10 ap3 mp3 11 d3 mp3 12 mp3 mp3 13 mp3 ap3 14 d3 d5
REFERENCES,0.3574468085106383,input1 15
REFERENCES,0.3588652482269504,"Figure 13: Frequent subgraphs in the good-performing architectures ranked by ratio of supports between
the important subgraphs and the reference and properties of the discovered frequent subgraphs in the worst-
performing architectures."
REFERENCES,0.36028368794326243,"estimator (TPE) (Bergstra et al., 2011), local search (LS) (Den Ottelander et al., 2021; White et al.,
2020b) and regularised evolution (RE) (Real et al., 2019)."
REFERENCES,0.3617021276595745,"B.2
TRAINING PROTOCOLS ON DARTS ARCHITECTURES"
REFERENCES,0.36312056737588655,"We exactly follow the NB301 protocols for all experiments involving architecture training (except for
the larger architectures on CIFAR-10 and ImageNet, which we outline below at App B.3). Speciﬁcally,
we train architectures obtained from stacking the cells 8 times (8-layer architectures) with initial
channel count of 32 on the CIFAR-10 dataset using the standard train/val split, and we use the
hyperparameters below on a single NVIDIA Tesla V100 GPU:"
REFERENCES,0.3645390070921986,"Optimizer: SGD
Initial learning rate: 0.025
Final learning rate: 1e-8
Learning rate schedule: cosine annealing
Epochs: 100
Weight decay: 3e-4
Momentum: 0.9
Auxiliary tower: True
Auxliary weight: 0.4
Cutout: True
Cutout length: 16
Drop path probability: 0.2
Gradient clip: 5
Batch size: 96
Mixup: True
Mixup alpha: 0.2"
REFERENCES,0.3659574468085106,"B.3
EVALUATION PROTOCOLS ON DARTS ARCHITECTURES"
REFERENCES,0.36737588652482267,"CIFAR-10
For the evaluation, we use larger architectures obtained from stacking the cells 20 times
(20-layer architectures) with an initial channel count of 36 on the CIFAR-10 dataset. The other
hyperparameters (mostly consistent with those used in App B.2 except for the number of epochs
trained) used are:"
REFERENCES,0.36879432624113473,"Optimizer: SGD
Initial learning rate: 0.025
Final learning rate: 1e-8
Learning rate schedule: cosine annealing
Epochs: 600
Weight decay: 3e-4
Momentum: 0.9
Auxiliary tower: True
Auxliary weight: 0.4"
REFERENCES,0.3702127659574468,Published as a conference paper at ICLR 2022
REFERENCES,0.37163120567375885,"Cutout: True
Cutout length: 16
Drop path probability: 0.2
Gradient clip: 5
Batch size: 96
Mixup: True
Mixup alpha: 0.2"
REFERENCES,0.3730496453900709,"This protocol is identical to the original DARTS protocol (Liu et al., 2019), with the only exception
that to be consistent with the NB301 protocol, we also incorporate the Mixup regularisation (Zhang
et al., 2017) during evaluation. This accounts for the fact that the accuracy reported in this paper is
generally better than those reported in the literature. However, as mentioned in the main text, we
re-train every architectures from the scratch, including the baselines, using the identical protocol listed
above, instead of simply taking the numbers from the original papers. As a result, no architecture has
been given unfair advantage because of the more effective regularisation used in this paper. We also
conduct all experiments on a single NVIDIA Tesla V100 GPU."
REFERENCES,0.37446808510638296,"ImageNet
On ImageNet, we use a protocol that is identical to Chen et al. (2021b). It is also almost
identical to those used in Xu et al. (2020); Chen et al. (2019); Liu et al. (2019) except for batch
sizes (which depend on the availability of hardware; larger batch size is only available for a parallel
many-GPU setup) and the corresponding linear scaling in learning rates. Speciﬁcally, we form
14-layer architectures with an initial channel count of 48 using 8× NVIDIA Tesla V100 GPUs. Note
that since we are unable to re-evaluate all the baselines using a standardised training protocols in this
case due to the extreme computational cost, we use a protocol that strictly adheres to the existing
works with the additional Mixup regularisation in CIFAR-10 disabled in the ImageNet experiments
to ensure the comparability of the results. The other hyperparameters are as followed:"
REFERENCES,0.375886524822695,"Optimizer: SGD
Initial learning rate: 0.5
Learning rate schedule: linear annealing
Epochs: 250
Weight decay: 3e-5
Momentum: 0.9
Auxiliary Tower: True
Auxliary weight: 0.4
Cutout: True
Cutout length: 16
Drop path probability: 0
Gradient clip: 5
Label smoothing: 0.1
Mixup: False
Batch size: 768"
REFERENCES,0.3773049645390071,"C
LIST OF REFERENCED PAPERS"
REFERENCES,0.37872340425531914,"We present the details covered in our preliminary survey on the NAS search methods papers published
in top machine learning conferences during the past year in Table 3."
REFERENCES,0.3801418439716312,Published as a conference paper at ICLR 2022
REFERENCES,0.38156028368794326,"Table 3: A list of NAS methods papers (i.e. excluding, e.g. review or benchmark papers) published in the past
year in top machine learning conferences. Cells-based means the work demonstrates at least one part of the
major results in the DARTS cell-based search space and/or highly related ones (such as the various NAS-Benches
and/or those otherwise highly resemble DARTS). Cells-only means the works only demonstrate the results in
aforementioned search space(s). Whenever a paper is not cell-based or cells-only, other spaces evaluated shows
the alternative spaces the papers report results on. The list is potentially incomplete, as we only select papers
that explicitly mention NAS in the title and/or the abstract."
REFERENCES,0.3829787234042553,"Venue
Name
Reference
Cells-based
Cells-only
Other spaces evaluated
NeurIPS 2020
BRP-NAS
Dudziak et al. (2020)
✓
✓
NAGO
Ru et al. (2020)
NAGO space
ISTA-NAS
Yang et al. (2020b)
✓
✓
arch2vec
Yan et al. (2020)
✓
✓
-
White et al. (2020a)
✓
✓
PR-DARTS
Zhou et al. (2020)
✓
✓
E2NAS
Zhang et al. (2020)
✓
✓
APS
Wang et al. (2020)
Channel/width search
SemiNAS
Luo et al. (2020)
✓
MobileNet space
BONAS
Shi et al. (2020)
✓
✓"
REFERENCES,0.3843971631205674,"ICLR 2021
NAS-BOWL
Ru et al. (2021)
✓
✓
DrNAS
Chen et al. (2021b)
✓
✓
GAEA
Li et al. (2021)
✓
✓
DARTS-
Chu et al. (2021)
✓
✓
TE-NAS
Chen et al. (2021a)
✓
✓
DARTS_PT, etc
Wang et al. (2021c)
✓
MobileNet space
MetaD2A
Lee et al. (2021)
✓
MobileNet space
CafeNet
Su et al. (2021a)
Channel/width search"
REFERENCES,0.38581560283687943,"ICML 2021
BO-TW/kDPP
Nguyen et al. (2021)
✓
✓
AlphaNet
Wang et al. (2021a)
MobileNet space
CATE
Wang et al. (2021a)
✓
✓
HardCoRe-NAS
Nayman et al. (2021)
MobileNet space
K-Shot NAS
Su et al. (2021b)
✓
MobileNet space
Few-shot NAS
Zhao et al. (2021)
✓
ProxylessNAS space, RNN, AutoGAN"
REFERENCES,0.3872340425531915,"Total
24
19 (79%)
14 (58 %)"
REFERENCES,0.38865248226950355,"D
ANALYSIS ON NAS-BENCH-201 0
1 2 3"
REFERENCES,0.3900709219858156,conv_3x3 (c3)
REFERENCES,0.39148936170212767,conv_1x1 (c1)
REFERENCES,0.39290780141843973,skip_connect (skip)
REFERENCES,0.3943262411347518,avg_pool_3x3 (ap3)
REFERENCES,0.39574468085106385,"Figure 14: The NB201 (Dong & Yang, 2020) cell, which
is highly similar to the DARTS space but much simpler.
All 6 locations (denoted by gray dashed arrows) are
available for search, and each is ﬁlled by one out of the
four candidate primitives (or None, which disables the
edge)."
REFERENCES,0.3971631205673759,"Fig 14 shows the NB201 search space, a popular
NAS benchmark commonly used that is highly
similar to the DARTS cell, but 1) only one cell
(instead of two) is searched, 2) each cell is con-
nected to its immediate preceding layer only,
and 3) is with a reduced set of primitives. Also,
unlike the DARTS cell, all edges in the NB201
cell are enabled.
We also conduct a brief analysis in a similar
manner to the main text on top 5% performing
architectures on NB201 dataset, and we show
the operation importance distribution of each
primitive in Fig 15. We observe that due to
the smaller cell size and the primitive set, the
operations in a NB201 cell is typically more
important and the only redundant operations is ap3. We hypothesise that the reason is similar to the
DARTS search space as the manually speciﬁed macro connection between the cells already include
pooling operations, rendering them unncessary within the cells.
The second experiment to conduct is verifying whether in the NB201 search space the good performing
cells are also characterised by the patterns we identiﬁed in Sec 4. To do so, we adapt the Skip and
Prim constraints in the NB201 space:"
REFERENCES,0.39858156028368796,"1. Skip constraint: in the NB201 search space, the only way to form a residual connection is to
place skip on edge 0 →3 (with reference to Fig 14.
2. Prim constraint: apart from the manually speciﬁed edge, all other operations are sampled
from the reduced primitive set {c1, c3} consisting of convolutions only."
REFERENCES,0.4,"Similar to our procedure in Sec 4, we sample 50 architectures within each group (no constraint, either
constraint and both constraints), and we show their test performance in Fig 16. It is also worth noting"
REFERENCES,0.4014184397163121,Published as a conference paper at ICLR 2022
REFERENCES,0.40283687943262414,"c1
c3
skip
ap3
0.01 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.40425531914893614,Operation importance
REFERENCES,0.4056737588652482,(a) CIFAR-10
REFERENCES,0.40709219858156026,"c1
c3
skip
ap3 0.02 0.00 0.02 0.04 0.06"
REFERENCES,0.4085106382978723,Operation importance
REFERENCES,0.4099290780141844,(b) CIFAR100
REFERENCES,0.41134751773049644,"c1
c3
skip
ap3 0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12"
REFERENCES,0.4127659574468085,Operation importance
REFERENCES,0.41418439716312055,(c) ImageNet16-120
REFERENCES,0.4156028368794326,"Figure 15: Box-and-whisker plots showing the distribution of the operation distribution in NB201 benchmark
on (a) CIFAR-10, (b) CIFAR-100 and (c) ImageNet16-120 datasets. The gray shaded areas denote the noise
standard deviation which differs in each dataset."
REFERENCES,0.41702127659574467,"that the ground-truth optimum in each dataset is known in NB201 and is accordingly marked in Fig
16. Differing from the observations in DARTS search space results, in this case Skip constraint alone
does not impact the performance signiﬁcantly, but again the PrimSkip group with both constraints
activated perform in a range very close to the optimum: in fact, the optimal architectures in all 3
datasets, while different from each other, all belong to the PrimSkip group and are found by random
sampling with fewer than 50 samples. This again conﬁrms that our ﬁndings in the main text similarly
generalise to NB201 space."
REFERENCES,0.41843971631205673,"Random Skip
Prim PrimSkip 5 10 15 20 25"
REFERENCES,0.4198581560283688,Test error (%) y (CIFAR-10)
REFERENCES,0.42127659574468085,(a) CIFAR-10
REFERENCES,0.4226950354609929,"Random Skip
Prim PrimSkip 25 30 35 40 45 50"
REFERENCES,0.42411347517730497,Test error (%) y (CIFAR-100)
REFERENCES,0.425531914893617,(b) CIFAR100
REFERENCES,0.4269503546099291,"Random Skip
Prim PrimSkip 55 60 65 70 75 80"
REFERENCES,0.42836879432624114,Test error (%) y (ImageNet16-120)
REFERENCES,0.4297872340425532,(c) ImageNet16-120
REFERENCES,0.43120567375886526,"Figure 16: Distribution of the test errors on (a) CIFAR-10, (b) CIFAR-100 and (c) ImageNet of NB201
architectures. Note that since NB201 is a tabular benchmark that exhaustively trains and evaluates all the
architectures within its search space, all test errors reported here are actual, not predicted."
REFERENCES,0.4326241134751773,"E
ARCHITECTURE SPECIFICATIONS"
REFERENCES,0.4340425531914894,"In this section, we show the speciﬁcations of (a.k.a genotypes) the different architectures in the
DARTS search space."
REFERENCES,0.43546099290780144,"E.1
ORIGINAL AND EDITED GENOTYPES FROM BASELINE PAPERS"
REFERENCES,0.4368794326241135,"Here we show the genotypes original and edited (corresponding to the results in the “Edited” column
in Table 2a) architectures (Fig. 17 – 24). In all ﬁgures, “Normal” and “Reduce” denote the normal
and reduce cells of the Original architectures where “Edited” denote the normal and reduce cells of
the edited architectures (note that the edited architectures always have identical normal and reduce
cells)."
REFERENCES,0.43829787234042555,c_{k-2} 0
REFERENCES,0.4397163120567376,skip_connect 1
REFERENCES,0.44113475177304967,sep_conv_5x5 2
REFERENCES,0.4425531914893617,skip_connect
REFERENCES,0.44397163120567373,"3
sep_conv_3x3"
REFERENCES,0.4453900709219858,c_{k-1}
REFERENCES,0.44680851063829785,sep_conv_3x3
REFERENCES,0.4482269503546099,sep_conv_5x5
REFERENCES,0.44964539007092197,sep_conv_5x5
REFERENCES,0.451063829787234,skip_connect c_{k}
REFERENCES,0.4524822695035461,(a) Normal
REFERENCES,0.45390070921985815,c_{k-2} 0
REFERENCES,0.4553191489361702,"max_pool_3x3
1
max_pool_3x3"
REFERENCES,0.45673758865248226,"c_{k-1}
sep_conv_3x3 none"
REFERENCES,0.4581560283687943,"3
sep_conv_3x3"
REFERENCES,0.4595744680851064,"2
dil_conv_3x3
c_{k}"
REFERENCES,0.46099290780141844,sep_conv_5x5
REFERENCES,0.4624113475177305,sep_conv_5x5
REFERENCES,0.46382978723404256,(b) Reduce
REFERENCES,0.4652482269503546,c_{k-2} 0
REFERENCES,0.4666666666666667,skip_connect 1
REFERENCES,0.46808510638297873,sep_conv_5x5 2
REFERENCES,0.4695035460992908,skip_connect
REFERENCES,0.47092198581560285,"3
sep_conv_3x3"
REFERENCES,0.4723404255319149,c_{k-1}
REFERENCES,0.47375886524822697,sep_conv_3x3
REFERENCES,0.475177304964539,sep_conv_5x5
REFERENCES,0.4765957446808511,sep_conv_5x5
REFERENCES,0.47801418439716314,sep_conv_3x3 c_{k}
REFERENCES,0.4794326241134752,(c) Edited
REFERENCES,0.4808510638297872,"Figure 17: Genotypes of BANANAS architecture (White et al., 2021)"
REFERENCES,0.48226950354609927,Published as a conference paper at ICLR 2022
REFERENCES,0.4836879432624113,c_{k-2} 0
REFERENCES,0.4851063829787234,sep_conv_3x3
REFERENCES,0.48652482269503544,"2
skip_connect"
REFERENCES,0.4879432624113475,"c_{k-1}
sep_conv_5x5"
REFERENCES,0.48936170212765956,"1
sep_conv_3x3"
REFERENCES,0.4907801418439716,sep_conv_3x3
REFERENCES,0.4921985815602837,sep_conv_3x3
REFERENCES,0.49361702127659574,"3
sep_conv_3x3 c_{k}"
REFERENCES,0.4950354609929078,dil_conv_5x5
REFERENCES,0.49645390070921985,(a) Normal
REFERENCES,0.4978723404255319,"c_{k-2}
0
max_pool_3x3"
REFERENCES,0.49929078014184397,c_{k-1}
REFERENCES,0.500709219858156,sep_conv_5x5
REFERENCES,0.502127659574468,"1
sep_conv_5x5"
REFERENCES,0.5035460992907801,"2
sep_conv_5x5"
REFERENCES,0.5049645390070922,"3
sep_conv_5x5"
REFERENCES,0.5063829787234042,dil_conv_5x5
REFERENCES,0.5078014184397163,"c_{k}
dil_conv_5x5"
REFERENCES,0.5092198581560283,skip_connect
REFERENCES,0.5106382978723404,(b) Reduce
REFERENCES,0.5120567375886524,c_{k-2} 0
REFERENCES,0.5134751773049645,sep_conv_3x3
REFERENCES,0.5148936170212766,"2
skip_connect"
REFERENCES,0.5163120567375886,"c_{k-1}
sep_conv_5x5"
REFERENCES,0.5177304964539007,"1
sep_conv_3x3"
REFERENCES,0.5191489361702127,sep_conv_3x3
REFERENCES,0.5205673758865248,sep_conv_3x3
REFERENCES,0.5219858156028369,"3
sep_conv_3x3 c_{k}"
REFERENCES,0.5234042553191489,sep_conv_5x5
REFERENCES,0.524822695035461,(c) Edited
REFERENCES,0.526241134751773,"Figure 18: Genotypes of DRNAS architecture (Chen et al., 2021b)"
REFERENCES,0.5276595744680851,c_{k-2} 0
REFERENCES,0.5290780141843971,skip_connect
REFERENCES,0.5304964539007092,"2
sep_conv_3x3"
REFERENCES,0.5319148936170213,c_{k-1}
REFERENCES,0.5333333333333333,skip_connect
REFERENCES,0.5347517730496454,"1
sep_conv_3x3
sep_conv_3x3"
REFERENCES,0.5361702127659574,sep_conv_5x5
REFERENCES,0.5375886524822695,"3
sep_conv_5x5"
REFERENCES,0.5390070921985816,"c_{k}
sep_conv_5x5"
REFERENCES,0.5404255319148936,(a) Normal
REFERENCES,0.5418439716312057,"c_{k-2}
0
sep_conv_3x3"
REFERENCES,0.5432624113475177,c_{k-1}
REFERENCES,0.5446808510638298,max_pool_3x3
REFERENCES,0.5460992907801419,"1
sep_conv_5x5"
REFERENCES,0.5475177304964539,"2
sep_conv_3x3"
REFERENCES,0.548936170212766,"3
sep_conv_5x5"
REFERENCES,0.550354609929078,dil_conv_5x5
REFERENCES,0.5517730496453901,max_pool_3x3
REFERENCES,0.5531914893617021,"c_{k}
sep_conv_3x3"
REFERENCES,0.5546099290780142,(b) Reduce
REFERENCES,0.5560283687943263,c_{k-2} 0
REFERENCES,0.5574468085106383,skip_connect
REFERENCES,0.5588652482269504,"2
sep_conv_3x3"
REFERENCES,0.5602836879432624,c_{k-1}
REFERENCES,0.5617021276595745,skip_connect
REFERENCES,0.5631205673758866,"1
sep_conv_3x3
sep_conv_3x3"
REFERENCES,0.5645390070921986,sep_conv_5x5
REFERENCES,0.5659574468085107,"3
sep_conv_5x5"
REFERENCES,0.5673758865248227,"c_{k}
sep_conv_5x5"
REFERENCES,0.5687943262411348,(c) Edited
REFERENCES,0.5702127659574469,"Figure 19: Genotypes of GAEA architecture (Li et al., 2021). Note that the edited genotype is identical to the
original normal genotype as it is already compliant with both Prim and Skip constraints."
REFERENCES,0.5716312056737589,c_{k-2} 0
REFERENCES,0.573049645390071,sep_conv_3x3 1
REFERENCES,0.574468085106383,max_pool_3x3 2
REFERENCES,0.5758865248226951,sep_conv_3x3
REFERENCES,0.577304964539007,"c_{k-1}
skip_connect"
REFERENCES,0.5787234042553191,sep_conv_3x3
REFERENCES,0.5801418439716312,sep_conv_5x5
REFERENCES,0.5815602836879432,"3
sep_conv_3x3"
REFERENCES,0.5829787234042553,dil_conv_5x5 c_{k}
REFERENCES,0.5843971631205673,(a) Normal
REFERENCES,0.5858156028368794,c_{k-2} 0
REFERENCES,0.5872340425531914,sep_conv_3x3 1
REFERENCES,0.5886524822695035,max_pool_3x3 2
REFERENCES,0.5900709219858156,sep_conv_3x3
REFERENCES,0.5914893617021276,"c_{k-1}
skip_connect"
REFERENCES,0.5929078014184397,sep_conv_3x3
REFERENCES,0.5943262411347517,sep_conv_5x5
REFERENCES,0.5957446808510638,"3
sep_conv_3x3"
REFERENCES,0.5971631205673759,dil_conv_5x5 c_{k}
REFERENCES,0.5985815602836879,(b) Reduce
REFERENCES,0.6,c_{k-2} 0
REFERENCES,0.601418439716312,sep_conv_3x3 1
REFERENCES,0.6028368794326241,skip_connect 2
REFERENCES,0.6042553191489362,sep_conv_3x3
REFERENCES,0.6056737588652482,"c_{k-1}
skip_connect"
REFERENCES,0.6070921985815603,sep_conv_3x3
REFERENCES,0.6085106382978723,sep_conv_5x5
REFERENCES,0.6099290780141844,"3
sep_conv_3x3"
REFERENCES,0.6113475177304964,sep_conv_5x5 c_{k}
REFERENCES,0.6127659574468085,(c) Edited
REFERENCES,0.6141843971631206,"Figure 20: Genotypes of NASBOWL architecture (Ru et al., 2021)."
REFERENCES,0.6156028368794326,"c_{k-2}
0
sep_conv_3x3"
REFERENCES,0.6170212765957447,c_{k-1}
REFERENCES,0.6184397163120567,"sep_conv_3x3
1
skip_connect"
REFERENCES,0.6198581560283688,"2
sep_conv_3x3"
REFERENCES,0.6212765957446809,"dil_conv_3x3
c_{k}"
REFERENCES,0.6226950354609929,dil_conv_3x3
REFERENCES,0.624113475177305,"3
dil_conv_3x3"
REFERENCES,0.625531914893617,dil_conv_3x3
REFERENCES,0.6269503546099291,(a) Normal
REFERENCES,0.6283687943262412,c_{k-2} 0
REFERENCES,0.6297872340425532,max_pool_3x3
REFERENCES,0.6312056737588653,"1
max_pool_3x3"
REFERENCES,0.6326241134751773,"c_{k-1}
dil_conv_3x3"
REFERENCES,0.6340425531914894,skip_connect
REFERENCES,0.6354609929078014,"2
skip_connect"
REFERENCES,0.6368794326241135,"3
skip_connect c_{k}"
REFERENCES,0.6382978723404256,skip_connect
REFERENCES,0.6397163120567376,dil_conv_5x5
REFERENCES,0.6411347517730497,(b) Reduce
REFERENCES,0.6425531914893617,"c_{k-2}
0
sep_conv_3x3"
REFERENCES,0.6439716312056738,c_{k-1}
REFERENCES,0.6453900709219859,"sep_conv_3x3
1
skip_connect"
REFERENCES,0.6468085106382979,"2
sep_conv_3x3"
REFERENCES,0.64822695035461,"sep_conv_3x3
c_{k}"
REFERENCES,0.649645390070922,sep_conv_3x3
REFERENCES,0.6510638297872341,"3
sep_conv_3x3"
REFERENCES,0.6524822695035462,sep_conv_3x3
REFERENCES,0.6539007092198581,(c) Edited
REFERENCES,0.6553191489361702,"Figure 21: Genotypes of NOISYDARTS architecture (Chu et al., 2020)"
REFERENCES,0.6567375886524822,c_{k-2} 0
REFERENCES,0.6581560283687943,sep_conv_3x3 1
REFERENCES,0.6595744680851063,sep_conv_3x3
REFERENCES,0.6609929078014184,"2
skip_connect"
REFERENCES,0.6624113475177305,c_{k-1}
REFERENCES,0.6638297872340425,sep_conv_3x3
REFERENCES,0.6652482269503546,"sep_conv_3x3
3
sep_conv_3x3"
REFERENCES,0.6666666666666666,skip_connect c_{k}
REFERENCES,0.6680851063829787,skip_connect
REFERENCES,0.6695035460992907,(a) Normal
REFERENCES,0.6709219858156028,"c_{k-2}
0
avg_pool_3x3"
REFERENCES,0.6723404255319149,c_{k-1}
REFERENCES,0.6737588652482269,sep_conv_5x5
REFERENCES,0.675177304964539,"1
max_pool_3x3
2
dil_conv_5x5"
REFERENCES,0.676595744680851,"skip_connect
3
sep_conv_5x5 c_{k}"
REFERENCES,0.6780141843971631,max_pool_3x3
REFERENCES,0.6794326241134752,skip_connect
REFERENCES,0.6808510638297872,(b) Reduce
REFERENCES,0.6822695035460993,c_{k-2} 0
REFERENCES,0.6836879432624113,sep_conv_3x3 1
REFERENCES,0.6851063829787234,sep_conv_3x3
REFERENCES,0.6865248226950355,"2
skip_connect"
REFERENCES,0.6879432624113475,c_{k-1}
REFERENCES,0.6893617021276596,sep_conv_3x3
REFERENCES,0.6907801418439716,"sep_conv_3x3
3
sep_conv_3x3"
REFERENCES,0.6921985815602837,sep_conv_5x5 c_{k}
REFERENCES,0.6936170212765957,sep_conv_5x5
REFERENCES,0.6950354609929078,(c) Edited
REFERENCES,0.6964539007092199,"Figure 22: Genotypes of DARTS_PT architecture (Wang et al., 2021c)"
REFERENCES,0.6978723404255319,c_{k-2} 0
REFERENCES,0.699290780141844,sep_conv_3x3
REFERENCES,0.700709219858156,"1
skip_connect"
REFERENCES,0.7021276595744681,"2
sep_conv_5x5"
REFERENCES,0.7035460992907802,c_{k-1}
REFERENCES,0.7049645390070922,sep_conv_3x3 3
REFERENCES,0.7063829787234043,sep_conv_3x3
REFERENCES,0.7078014184397163,avg_pool_3x3 c_{k}
REFERENCES,0.7092198581560284,"skip_connect
dil_conv_3x3"
REFERENCES,0.7106382978723405,(a) Normal
REFERENCES,0.7120567375886525,c_{k-2} 0
REFERENCES,0.7134751773049646,dil_conv_3x3 1
REFERENCES,0.7148936170212766,avg_pool_3x3 3
REFERENCES,0.7163120567375887,avg_pool_3x3
REFERENCES,0.7177304964539007,c_{k-1}
REFERENCES,0.7191489361702128,sep_conv_5x5
REFERENCES,0.7205673758865249,"2
max_pool_3x3"
REFERENCES,0.7219858156028369,dil_conv_3x3
REFERENCES,0.723404255319149,dil_conv_5x5 c_{k}
REFERENCES,0.724822695035461,dil_conv_5x5
REFERENCES,0.7262411347517731,(b) Reduce
REFERENCES,0.7276595744680852,c_{k-2} 0
REFERENCES,0.7290780141843972,skip_connect
REFERENCES,0.7304964539007093,"1
skip_connect"
REFERENCES,0.7319148936170212,"2
sep_conv_5x5"
REFERENCES,0.7333333333333333,c_{k-1}
REFERENCES,0.7347517730496453,sep_conv_3x3 3
REFERENCES,0.7361702127659574,sep_conv_3x3
REFERENCES,0.7375886524822695,sep_conv_3x3 c_{k}
REFERENCES,0.7390070921985815,"sep_conv_3x3
sep_conv_3x3"
REFERENCES,0.7404255319148936,(c) Edited
REFERENCES,0.7418439716312056,"Figure 23: Genotypes of SDARTS_PT architecture (Wang et al., 2021c)"
REFERENCES,0.7432624113475177,Published as a conference paper at ICLR 2022
REFERENCES,0.7446808510638298,c_{k-2} 0
REFERENCES,0.7460992907801418,sep_conv_3x3
REFERENCES,0.7475177304964539,"3
sep_conv_3x3"
REFERENCES,0.7489361702127659,"c_{k-1}
sep_conv_3x3
1"
REFERENCES,0.750354609929078,max_pool_3x3 2
REFERENCES,0.75177304964539,dil_conv_3x3
REFERENCES,0.7531914893617021,sep_conv_3x3
REFERENCES,0.7546099290780142,dil_conv_5x5
REFERENCES,0.7560283687943262,sep_conv_5x5 c_{k}
REFERENCES,0.7574468085106383,(a) Normal
REFERENCES,0.7588652482269503,c_{k-2} 0
REFERENCES,0.7602836879432624,dil_conv_5x5
REFERENCES,0.7617021276595745,"1
max_pool_3x3"
REFERENCES,0.7631205673758865,c_{k-1}
REFERENCES,0.7645390070921986,sep_conv_3x3
REFERENCES,0.7659574468085106,"2
max_pool_3x3"
REFERENCES,0.7673758865248227,sep_conv_5x5
REFERENCES,0.7687943262411348,sep_conv_5x5
REFERENCES,0.7702127659574468,"3
sep_conv_5x5 c_{k}"
REFERENCES,0.7716312056737589,avg_pool_3x3
REFERENCES,0.7730496453900709,(b) Reduce
REFERENCES,0.774468085106383,c_{k-2} 0
REFERENCES,0.775886524822695,sep_conv_3x3
REFERENCES,0.7773049645390071,"3
sep_conv_3x3"
REFERENCES,0.7787234042553192,"c_{k-1}
sep_conv_3x3
1"
REFERENCES,0.7801418439716312,skip_connect 2
REFERENCES,0.7815602836879433,sep_conv_3x3
REFERENCES,0.7829787234042553,sep_conv_3x3
REFERENCES,0.7843971631205674,sep_conv_5x5
REFERENCES,0.7858156028368795,sep_conv_5x5 c_{k}
REFERENCES,0.7872340425531915,(c) Edited
REFERENCES,0.7886524822695036,"Figure 24: Genotypes of SGAS_PT architecture (Wang et al., 2021c)"
REFERENCES,0.7900709219858156,"E.2
RANDOM GENOTYPES SAMPLED IN THE PRIMSKIP GROUP"
REFERENCES,0.7914893617021277,"We show some examples of the genotypes generated via the constrained random sampling in the
PrimSkip group in Sec 4 in Fig 25, while the two architectures selected for the CIFAR-10/ImageNet
experiments on the larger architectures is shown in Figs 26 and 27."
REFERENCES,0.7929078014184398,c_{k-2} 0
REFERENCES,0.7943262411347518,skip_connect
REFERENCES,0.7957446808510639,"1
sep_conv_3x3"
REFERENCES,0.7971631205673759,"c_{k-1}
skip_connect"
REFERENCES,0.798581560283688,sep_conv_3x3
REFERENCES,0.8,"2
sep_conv_5x5
sep_conv_3x3"
REFERENCES,0.8014184397163121,"3
sep_conv_5x5 c_{k}"
REFERENCES,0.8028368794326242,"sep_conv_5x5
c_{k-2} 0"
REFERENCES,0.8042553191489362,skip_connect
REFERENCES,0.8056737588652483,"1
sep_conv_5x5 2"
REFERENCES,0.8070921985815603,"sep_conv_3x3
3"
REFERENCES,0.8085106382978723,sep_conv_3x3
REFERENCES,0.8099290780141843,c_{k-1}
REFERENCES,0.8113475177304964,skip_connect
REFERENCES,0.8127659574468085,sep_conv_5x5
REFERENCES,0.8141843971631205,sep_conv_5x5
REFERENCES,0.8156028368794326,"sep_conv_3x3
c_{k}"
REFERENCES,0.8170212765957446,c_{k-2} 0
REFERENCES,0.8184397163120567,skip_connect 1
REFERENCES,0.8198581560283688,sep_conv_5x5 3
REFERENCES,0.8212765957446808,sep_conv_5x5
REFERENCES,0.8226950354609929,"c_{k-1}
skip_connect"
REFERENCES,0.8241134751773049,sep_conv_3x3
REFERENCES,0.825531914893617,"2
sep_conv_3x3"
REFERENCES,0.826950354609929,sep_conv_3x3
REFERENCES,0.8283687943262411,sep_conv_5x5 c_{k}
REFERENCES,0.8297872340425532,c_{k-2} 0
REFERENCES,0.8312056737588652,skip_connect 1
REFERENCES,0.8326241134751773,sep_conv_3x3 3
REFERENCES,0.8340425531914893,sep_conv_3x3
REFERENCES,0.8354609929078014,c_{k-1}
REFERENCES,0.8368794326241135,skip_connect
REFERENCES,0.8382978723404255,sep_conv_3x3
REFERENCES,0.8397163120567376,"2
sep_conv_3x3"
REFERENCES,0.8411347517730496,sep_conv_5x5
REFERENCES,0.8425531914893617,sep_conv_3x3 c_{k}
REFERENCES,0.8439716312056738,Figure 25: Some of the randomly sampled architectures in the PrimSkip group.
REFERENCES,0.8453900709219858,"c_{k-2}
0
skip_connect"
REFERENCES,0.8468085106382979,"1
sep_conv_5x5"
REFERENCES,0.8482269503546099,"2
sep_conv_5x5"
REFERENCES,0.849645390070922,"3
sep_conv_5x5"
REFERENCES,0.851063829787234,c_{k-1}
REFERENCES,0.8524822695035461,skip_connect
REFERENCES,0.8539007092198582,sep_conv_3x3
REFERENCES,0.8553191489361702,sep_conv_3x3
REFERENCES,0.8567375886524823,sep_conv_5x5 c_{k}
REFERENCES,0.8581560283687943,(a) Normal
REFERENCES,0.8595744680851064,c_{k-2}
REFERENCES,0.8609929078014185,"0
skip_connect 1"
REFERENCES,0.8624113475177305,sep_conv_5x5
REFERENCES,0.8638297872340426,c_{k-1}
REFERENCES,0.8652482269503546,skip_connect
REFERENCES,0.8666666666666667,sep_conv_3x3
REFERENCES,0.8680851063829788,"2
sep_conv_5x5"
REFERENCES,0.8695035460992908,"3
sep_conv_5x5"
REFERENCES,0.8709219858156029,sep_conv_5x5 c_{k}
REFERENCES,0.8723404255319149,sep_conv_3x3
REFERENCES,0.873758865248227,(b) Reduce
REFERENCES,0.875177304964539,Figure 26: Randomly selected PrimSkip architecture 1 for the experiments on the larger architectures
REFERENCES,0.8765957446808511,Published as a conference paper at ICLR 2022
REFERENCES,0.8780141843971632,"Normal
Reduce
0 250 500 750 1000 1250 1500 1750"
REFERENCES,0.8794326241134752,Number of occurrences
REFERENCES,0.8808510638297873,(a) All operations
REFERENCES,0.8822695035460993,"Normal
Reduce
0 250 500 750 1000 1250 1500 1750"
REFERENCES,0.8836879432624114,Number of occurrences
REFERENCES,0.8851063829787233,"s3
s5
d3
d5
skip
mp3
ap3"
REFERENCES,0.8865248226950354,(b) Important operations
REFERENCES,0.8879432624113475,"Figure 28: Distribution of (a) all and (b) important
operations by the primitive types according to the BA-
NANAS surrogate. The gray dashed line in (a) denotes
the expected number of occurrences if the operations
are uniformly sampled."
REFERENCES,0.8893617021276595,"s3
s5
d3
d5 skipmp3ap3
0.002 0.001 0.000 0.001 0.002 0.003 0.004"
REFERENCES,0.8907801418439716,Operation importance
REFERENCES,0.8921985815602836,(a) Normal cells
REFERENCES,0.8936170212765957,"s3
s5
d3
d5 skipmp3ap3
0.002 0.001 0.000 0.001 0.002 0.003 0.004"
REFERENCES,0.8950354609929078,Operation importance
REFERENCES,0.8964539007092198,(b) Reduce cells
REFERENCES,0.8978723404255319,"Figure 29: Box plots showing the distribution of the
operations importance in (a) normal and (b) reduce cells
according to the BANANAS surrogate. The important
operations by the deﬁnition of the paper are shown
outside the gray shaded area."
REFERENCES,0.8992907801418439,c_{k-2} 0
REFERENCES,0.900709219858156,skip_connect
REFERENCES,0.902127659574468,"2
sep_conv_5x5"
REFERENCES,0.9035460992907801,"c_{k-1}
skip_connect"
REFERENCES,0.9049645390070922,"1
sep_conv_3x3"
REFERENCES,0.9063829787234042,sep_conv_3x3
REFERENCES,0.9078014184397163,sep_conv_3x3
REFERENCES,0.9092198581560283,"3
sep_conv_5x5 c_{k}"
REFERENCES,0.9106382978723404,sep_conv_5x5
REFERENCES,0.9120567375886525,(a) Normal
REFERENCES,0.9134751773049645,c_{k-2}
REFERENCES,0.9148936170212766,"0
skip_connect 1"
REFERENCES,0.9163120567375886,sep_conv_5x5
REFERENCES,0.9177304964539007,c_{k-1}
REFERENCES,0.9191489361702128,skip_connect
REFERENCES,0.9205673758865248,sep_conv_3x3
REFERENCES,0.9219858156028369,"2
sep_conv_5x5"
REFERENCES,0.9234042553191489,"3
sep_conv_5x5"
REFERENCES,0.924822695035461,sep_conv_5x5 c_{k}
REFERENCES,0.926241134751773,sep_conv_3x3
REFERENCES,0.9276595744680851,(b) Reduce
REFERENCES,0.9290780141843972,Figure 27: Randomly selected PrimSkip architecture 2 for the experiments on the larger architectures
REFERENCES,0.9304964539007092,"F
REPRODUCING RESULTS WITH LESS TRAINING DATA"
REFERENCES,0.9319148936170213,"In this section, we show that it is possible to reproduce many results in the paper using less than
0.4% of the data compared to the full set of more than 50,000 architecture-performance pairs used in
the NB301 surrogate, thereby motivating the use of the tools introduced in this paper as a generic,
cost-effective search space inspector: For the experiments conducted, we use the surrogate from
BANANAS (White et al., 2021), which combines a neural ensemble predictor with path encoding of
the architectures – it is worth noting that alternative surrogates may also be used, but it is preferable
to use a sample-efﬁcient surrogate that is capable of ﬁnding meaningful relations in the input data
with a modest number of evaluations. Speciﬁcally, we randomly sample 200 architectures from
the search space and query their NB301 predicted performance as a proxy for the ground-truth
performance. We then compute the path encoding of each architecture and train a predictor with the
default hyperparameters from White et al. (2021).
We ﬁrst verify whether the surrogate using less data is able to learn meaningful patterns by drawing
another 200 random unseen architectures in the search space and compare the predictions by the neural
ensemble predictor vs the NB301 prediction (Fig 30) and it is clear that the regression performance is
already satisfactory (with a Spearman rank coefﬁcient of 0.77) despite using much less data."
REFERENCES,0.9333333333333333,"0.92
0.94
Surrogate Pred 0.91 0.92 0.93 0.94"
REFERENCES,0.9347517730496454,NB301 Pred
REFERENCES,0.9361702127659575,Spearman  = 0.999
REFERENCES,0.9375886524822695,(a) Train
REFERENCES,0.9390070921985816,"0.92
0.94
Surrogate Pred 0.91 0.92 0.93 0.94"
REFERENCES,0.9404255319148936,NB301 Pred
REFERENCES,0.9418439716312057,Spearman  = 0.77
REFERENCES,0.9432624113475178,(b) Validation
REFERENCES,0.9446808510638298,"Figure 30: Regression performance of the BANANAS predictor with 200 training data vs the NB301 surrogate
with more than 50,000 training data."
REFERENCES,0.9460992907801419,"We then repeat the analysis in the main text, and show the operation-level ﬁndings (Sec 3) in Figs 28
and 29 and the important subgraphs corresponding to Sec 4 in Fig 31. It is worth noting that most of"
REFERENCES,0.9475177304964539,Published as a conference paper at ICLR 2022
REFERENCES,0.948936170212766,"the ﬁndings are already highly similar to those in the main text, although, for example, the operation
importance distributions in Fig 29 have a larger variance due to the less certain predictions. The
main purpose of this study is to show that combined with the explanability tools used in the present
work, an appropriate performance surrogate, which is only used as a vessel towards searching in
some search methods so far, can be itself valuable. We demonstrate that it could shed insights into
the strengths and weaknesses of an arbitrary search space with a modest number of observations –
for example, during design of a new search space, we may randomly sample and evaluate a modest
number of architectures and similarly ﬁt a surrogate. We may then use the explainability tool to
inspect the search space in a similar procedure in this section – we believe this could potentially
prevent some of the pitfalls described in the existing cell-based search spaces to recur in prospective
new ones during the design process. skip 1 2
1 s3 2 2
2 skip s3 1 2 2
3 s5 2 2
4 skip s5 1 2 2
5 s3 1 2
6 skip s3 s5 1 2 2 2
7 skip s3 1 2 2
8 skip 0 2
9 skip s3 s3 1 2 2 2
10 skip s3 0 2 2
11 skip s3 s5 1 2 2 2
12 skip s5 0 2 2
13 skip s3 s5 0 2 2 2
14 s5 0 2
15"
REFERENCES,0.950354609929078,"Figure 31: Frequent subgraphs in the good-performing architectures ranked by ratio of supports between the
important subgraphs and the reference and properties of the discovered frequent subgraphs according to the
BANANAS surrogate. Note that the residual link + separable convolution patterns are highly similar to those
identiﬁed in Fig 7 in the main text"
REFERENCES,0.9517730496453901,"G
PRIMITIVE REDUNDANCIES WITH MULTIPLE OPTIMISATION OBJECTIVES"
REFERENCES,0.9531914893617022,"Most of the ﬁndings of the paper, including the current deﬁnition of the OI, are based on maximising
test accuracy as the sole objective, as the majority of the existing literature on cell-based NAS (at
least those surveyed in App. C) focuses on maximising performance as the primary performance
metric. However, alternative cost metrics often expressed in terms of FLOPS or number of parameters,
are sometimes also incorporated as a part of the objective (e.g. maximising test accuracy while
minimising the number of parameters in the resulting architecture) especially for deployment on cost-
sensitive platforms like mobile devices. This give rises to a multi-objective optimisation problem, and
a reasonable question to ask is that whether some of the main ﬁndings, in particular the redundancy
of primitives identiﬁed in the paper, still hold, as some of the primitives redundant or unimportant
for maximising performance might be useful for minimising costs. In this section, we show some
preliminary results that at least in terms of cost metrics like FLOPs and/or number of parameters, the
otherwise redundant primitives are still rather unimportant.
We consider the PrimSkip group of architectures described in Sec 4 since we ﬁnd that they give
rise to the best performing architectures in the search space. Here, to obtain the trade-off between
performance and number of model parameters while still maintaining the PrimSkip constraints,
instead of sampling only from {s3, s5} primitives for the operation spots (except for the ﬁxed
residual connections), we introduce another parameter p ∈[0, 1], which denotes the probability that
the parameterless skip is chosen instead of the separable convolution primitives except the two
ﬁxed residual connections: when p = 1, skip will be selected for every possible operation spot,
giving rise to the most lightweight possible architecture. On the other hand, when p = 0, we are back
at the base scenario where the only skips are the 2 residual connections ﬁxed a-priori.
To investigate the importance of the pooling operations ({mp3, ap3}), we then sample from
another group of architectures that is identical to above, except that for the parameterless primitive,
we may additionally choose from the pooling operations in addition to skip. For both groups, we
sample 1,000 architectures from the constrained search spaces with random p ∈[0, 1] and show the"
REFERENCES,0.9546099290780142,Published as a conference paper at ICLR 2022
REFERENCES,0.9560283687943263,"trade-offs in terms of 2D Pareto fronts between accuracy and number of parameters (Fig. 32)(a)) and
between accuracy and FLOPs (Fig. 32(b)): if the pooling operations are indispensable to achieve good
accuracy-cost trade-off, we should have observed a dominating Pareto fronts of the PrimSkip+Pool
group of architectures over the PrimSkip group only. Nonetheless, we observe broadly comparable
Pareto front for both groups (in terms of FLOPs, PrimSkip seems to ﬁnd marginally more Pareto
efﬁcient solutions), suggesting that even after accounting for the conﬂicting objectives, it seems
that in the existing search space design, pooling operations reduce costs largely because they are
parameterless, and skip connections, which are similar parameterless, can be used to construct
similarly performant/lightweight models but with reduced redundancies in primitive choices. It is
nevertheless worth emphasising that this does not suggest that the pooling operations in general are
not valuable; we hypothesise that the primary reason leading to the redundancies of pooling, as we
also mentioned in the main text, is because the existing cell-based search space manually inserts
pooling layers between cells, rendering pooling within cells unhelpful, for performance and/or for
costs."
REFERENCES,0.9574468085106383,"2
3
4
#Params(MB) 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.9588652482269504,Test. Err.
REFERENCES,0.9602836879432625,"PrimSkip+Pool
PrimSkip"
REFERENCES,0.9617021276595744,"4
6
FLOPs
1e8 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.9631205673758865,Test. Err.
REFERENCES,0.9645390070921985,"PrimSkip+Pool
PrimSkip"
REFERENCES,0.9659574468085106,"Figure 32: Test errors against number of parameters (a) and FLOPs (b) of the sample architectures in the two
groups."
REFERENCES,0.9673758865248226,"H
PERFORMANCE OF NAS ALGORITHMS IN PRIMSKIP SEARCH SPACE"
REFERENCES,0.9687943262411347,"Another interesting experiment to consider the performance the existing NAS algorithms in the Prim-
Skip space as opposed to the original DARTS search space. In this section, we conduct experiments
on two search methods, each representing a dominant genre in NAS: DARTS (Liu et al., 2019) as the
seminal differentiable search method, and NAS-BOWL (Ru et al., 2021), the state-of-the-art query-
based NAS method. For NAS-BOWL, we run 5 repetitions with a query budget of 100 architectures.
For DARTS, we use the default hyperparameters provided in the original DARTS paper to train the
supernetwork for 50 epochs. We show the trajectories of test errors and the ﬁnal Fig. 33 (DARTS
is not query-based, so Fig 33(b) shows the test error of the architecture proposed by DARTS at the
end of each epoch, as if it DARTS is terminated and discretised at that epoch. This metric is also
referred to by, for example, oracle test error in previous works (Li et al., 2021)): it is evident that
conducting search in the constrained search space leads to massive speed-ups compared to doing
so in the original search space, again conﬁrming that the simple rules identiﬁed in the paper have
constrained the architectures to a very high-performing region of the search space."
REFERENCES,0.9702127659574468,"I
DETAILED EXPLANATION OF THE FREQUENT SUBGRAPH MINING
PROCEDURE IN SEC 4"
REFERENCES,0.9716312056737588,"As mentioned in Sec 4, support, which measure the number of times a subgraph occurs in a set of
graphs, favour simpler subgraphs as they appear more frequently “by nature”: for example, consider
the simplest subgraph such one consists of a single operation (e.g. s3) as a single edge. It is much
more likely, even by random generation, for a graph to contain a s3 compared to another more
complicated subgraph such as a triangle, with operations on 3 edges being {s3, s5, skip}. As
a result, s3 will have a much larger support in the set of graphs, but it does not imply it is a more
signiﬁcant subgraph.
Therefore, a more interesting and meaningful metric is how much more over-represented (or under-
represented) a subgraph is, within a set of graphs, beyond its “natural” level of occurrences. An"
REFERENCES,0.9730496453900709,Published as a conference paper at ICLR 2022
REFERENCES,0.9744680851063829,"0
20
40
60
80
100
# Architectures evaluated 5.0 5.5 6.0 6.5 7.0"
REFERENCES,0.975886524822695,Test Error
REFERENCES,0.9773049645390071,"NASBOWL(PrimSkip)
NASBOWL"
REFERENCES,0.9787234042553191,(a) NAS-BOWL
REFERENCES,0.9801418439716312,"0
20
40
# Epochs 5.25 5.50 5.75 6.00 6.25 6.50 6.75"
REFERENCES,0.9815602836879432,Test error
REFERENCES,0.9829787234042553,"DARTS
DARTS(PrimSkip)"
REFERENCES,0.9843971631205674,(b) DARTS
REFERENCES,0.9858156028368794,"Figure 33: Performance of NAS-BOWL and DARTS in the constrained PrimSkip search space. The NAS-BOWL
results shown with mean ± std over 5 random seeds."
REFERENCES,0.9872340425531915,"Full architectures
Important subgraphs
Random subgraphs as the null reference"
REFERENCES,0.9886524822695035,"Figure 34: Demonstration of the frequent subgraph mining procedure in the important subgraphs. Full architec-
tures shows the DAGs representing the entire search cells; Important subgraphs (the coloured subgraphs) shows
the subgraphs of each full architecture consist of only the important operations (i.e. those with an OI above some
threshold). Random subgraphs (the coloured subgraphs) act as the null reference in this case: consider a full
architectures G and its important subgraph gf with m edges, we randomly sample exactly m edges from each
G, possibly with multiple repetitions, to form gr, the corresponding random subgraph(s) of G."
REFERENCES,0.9900709219858156,"intuitive way to measure this over-representation is thus to compute ratio between the support of the
subgraphs in the target set and that of some null reference set. As shown by the demonstration in Fig
34, the target in this case is simply the important subgraphs as deﬁned in Sec 4. The null reference,
as explained in the captions of Fig 34, are constructed from the randomly sampled subgraphs of
each full architecture DAG. The key intuition here is that if a subgraph has a high support simply by
virtue of its simplicity or otherwise, it should have high support values in both the target and null
reference sets and taking the ratio would cancel out its inﬂuence (due to the high denominator value).
On the other hand, if there exists an underlying mechanism such that certain subgraphs are more
likely to be important (which is the primary objective of the FSM in this context – to identify whether
there exists sub-structures within cells that correlate with good performances), then they will likely
be more over-represented in the target set than in the null reference. Indeed, the top subgraphs as
identiﬁed in Fig 7 all feature rather more complicated structures and a low “natural” incidence, but
they nevertheless appear as recurring patterns in the important subgraphs, suggesting that they have a
strong connection to the overall good performance of the cells."
REFERENCES,0.9914893617021276,"J
SUGGESTIONS FOR FUTURE DIRECTIONS"
REFERENCES,0.9929078014184397,"In this section, we elaborate in detail the two promising future directions that we identify in Sec 6
that would hopefully guide future NAS researchers and practitioners to build better search spaces:"
REFERENCES,0.9943262411347518,Published as a conference paper at ICLR 2022
REFERENCES,0.9957446808510638,"• Simplify the cells but relax the macro-connections between cells: Current cell-based NAS almost
overwhelmingly focuses on cells but manually ﬁx the inter-cell connections in a linear manner
similar to the classical networks (e.g. ResNet) that are known to do well. On the other hand, other
search spaces like the MobileNet space, are arguably more ﬂexible but are purely macro-based – in
our opinion, there could be potential to preserve the convenience of the search cell, but at the same
time to give more freedom on how different cells are connected. By relaxing the constraints on how
the different cells may be connected, there is not only a potential for a much larger and expressive
search space, but also be a greater variability in overall graph theoric topological properties (as
opposed to topological properties within cells only) and widths/depths of the resulting networks,
which have been shown to signiﬁcantly inﬂuence the performance (Xie et al., 2019a; Ru et al.,
2020). We leave the detailed speciﬁcation of such a search space to a future work, but a concrete
direction would be to further investigate and improve on the hierarchical search spaces such as the
one proposed in Ru et al. (2020)."
REFERENCES,0.9971631205673759,"• Towards lower-level searching: As hypothesised in the main text, the current search methods might
be implicitly encouraged to ﬁnd something similar to the existing architectures, as the current
NAS pipelines often borrow heavily from the classical network designs (e.g. the linear macro
connections between cells and the manually inserted conv and pooling layers are often found in the
manually-designed networks). While this is not necessarily bad on its own, we might be making it
more difﬁcult for search methods to ﬁnd novel architectures beyond what is already known. An
alternative could be that instead of searching on high-level primitives and operators, we could
search on something more fundamental like searching for sequences of mathematical operations.
The pioneering work is Real et al. (2020), but similar to some early-stage NAS works, it was built
on evolutionary algorithms, which are not particularly sample- and computation cost-efﬁcient (as
opposed to later works like one-shot/differentiable NAS which dramatically reduced computing cost
of NAS). Thus, another promising direction would be to explore searches on a more fundamental
level that is free of bias from existing design, but to develop more scalable and tailored solutions
to make the problem more tractable."
REFERENCES,0.9985815602836879,"• NAS Benchmarks beyond simple cells: The advent of the NAS benchmarks have greatly democra-
tised NAS and has partially led to the booming of recent NAS research. While exceptions may
exist, the most popular NAS benchmarks on computer vision problems, which greatly facilitate
faster iteration of search methods, are currently based on the cell-based spaces (NAS-Bench-
101/202/301/x11) (Ying et al., 2019a; Dong & Yang, 2020; Siems et al., 2020; Yan et al., 2021).
An improved search space would be made more widely accepted in a much easier way if a suitable
benchmarking tool is developed. Therefore, we argue there is a need is to build NAS benchmarks
beyond the current cell-based spaces."
