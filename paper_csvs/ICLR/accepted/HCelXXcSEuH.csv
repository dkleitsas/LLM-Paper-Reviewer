Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0006042296072507553,"We present a novel adaptive optimization algorithm for large-scale machine learning
problems. Equipped with a low-cost estimate of local curvature and Lipschitz
smoothness, our method dynamically adapts the search direction and step-size.
The search direction contains gradient information preconditioned by a well-scaled
diagonal preconditioning matrix that captures the local curvature information.
Our methodology does not require the tedious task of learning rate tuning, as the
learning rate is updated automatically without adding an extra hyperparameter. We
provide convergence guarantees on a comprehensive collection of optimization
problems, including convex, strongly convex, and nonconvex problems, in both
deterministic and stochastic regimes. We also conduct an extensive empirical
evaluation on standard machine learning problems, justifying our algorithm’s
versatility and demonstrating its strong performance compared to other start-of-the-
art ﬁrst-order and second-order methods."
INTRODUCTION,0.0012084592145015106,"1
INTRODUCTION"
INTRODUCTION,0.0018126888217522659,"This paper presents an algorithm for solving empirical risk minimization problems of the form:
minw∈RdF(w) := 1"
INTRODUCTION,0.002416918429003021,"n
Pn
i=1f(w; xi, yi) = 1"
INTRODUCTION,0.0030211480362537764,"n
Pn
i=1fi(w),
(1)
where w is the model parameter/weight vector, {(xi, yi)}n
i=1 are the training samples, and
fi : Rd →R is the loss function. Usually, the number of training samples, n, and dimension, d, are
large, and the loss function F is potentially nonconvex, making (1) difﬁcult to solve.
In the past decades, signiﬁcant effort has been devoted to developing optimization algorithms for
machine learning. Due to easy implementation and low per-iteration cost, (stochastic) ﬁrst-order
methods (Robbins & Monro, 1951; Duchi et al., 2011; Schmidt et al., 2017; Johnson & Zhang,
2013; Nguyen et al., 2017; 2019; Kingma & Ba, 2014; Jahani et al., 2021a; Recht et al., 2011) have
become prevalent approaches for many machine learning applications. However, these methods have
several drawbacks: (i) they are highly sensitive to the choices of hyperparameters, especially learning
rate; (ii) they suffer from ill-conditioning that often arises in large-scale machine learning; and (iii)
they offer limited opportunities in distributed computing environments since these methods usually
spend more time on “communication” instead of the true “computation.” The main reasons for the
aforementioned issues come from the fact that ﬁrst-order methods only use the gradient information
for their updates.
On the other hand, going beyond ﬁrst-order methods, Newton-type and quasi-Newton methods
(Nocedal & Wright, 2006; Dennis & Mor´e, 1977; Fletcher, 1987) are considered to be a strong
family of optimizers due to their judicious use of the curvature information in order to scale
the gradient. By exploiting the curvature information of the objective function, these methods
mitigate many of the issues inherent in ﬁrst-order methods. In the deterministic regime, it is
known that these methods are relatively insensitive to the choices of the hyperparameters, and
they handle ill-conditioned problems with a fast convergence rate. Clearly, this does not come
for free, and these methods can have memory requirements up to O(d2) with computational"
INTRODUCTION,0.0036253776435045317,Published as a conference paper at ICLR 2022
INTRODUCTION,0.004229607250755287,"complexity up to O(d3) (e.g., with a naive use of the Newton method). There are, of course,
efﬁcient ways to solve the Newton system with signiﬁcantly lower costs (e.g., see Nocedal &
Wright (2006)).
Moreover, quasi-Newton methods require lower memory and computational
complexities than Newton-type methods.
Recently, there has been shifted attention towards
stochastic second-order (Roosta-Khorasani & Mahoney, 2018; Byrd et al., 2011; Martens, 2010;
Jahani et al., 2020a; Xu et al., 2017; Roosta et al., 2018; Yao et al., 2018) and quasi-Newton
methods (Curtis, 2016; Berahas et al., 2016; Mokhtari & Ribeiro, 2015; Jahani et al., 2021b; Bera-
has et al., 2019; Jahani et al., 2020b) in order to approximately capture the local curvature information."
INTRODUCTION,0.004833836858006042,"0
200
400
600
800
1000
Sa m ples 10 0"
INTRODUCTION,0.005438066465256798,Rela tive Error
INTRODUCTION,0.006042296072507553,"OASIS
AdaHessian"
INTRODUCTION,0.006646525679758308,Hutchinson
INTRODUCTION,0.0072507552870090634,"3
2
1
0
1
2
3
Dia gona l(A) 2 0 2 4 6 8"
INTRODUCTION,0.00785498489425982,Diagonal Approx.
SA M PLES,0.008459214501510574,10000 Sa m ples
SA M PLES,0.00906344410876133,"AdaHessian
OASIS
line x = y"
SA M PLES,0.009667673716012085,"Figure 1: Comparison of the diagonal approxima-
tion by AdaHessian and OASIS over a random
symmetric matrix A (100 × 100). left: Relative
error (in Euclidean norm) between the true diag-
onal of matrix A and the diagonal approximation
by AdaHessian, Hutchinson’s method, and OASIS
(the x-axis shows the number of random vectors
sampled from the Rademacher distribution, see
Section 2. Moreover, this plot can be considered as
the representation of the error of the Hessian diag-
onal approximation’s evolution over the iterations
of minimizing wT Aw, since A is ﬁxed and sym-
metric.); right: Diagonal approximation scale for
AdaHessian and OASIS (y-axis), in comparison
to the true diagonal of matrix A (x-axis)."
SA M PLES,0.01027190332326284,"These methods have shown good results for sev-
eral machine learning tasks (Xu et al., 2020;
Berahas et al., 2020; Yao et al., 2019). In some
cases, however, due to the noise in the Hessian
approximation, their performance is still on par
with the ﬁrst-order variants. One avenue for re-
ducing the computational and memory require-
ments for capturing curvature information is to
consider just the diagonal of the Hessian. Since
the Hessian diagonal can be represented as a
vector, it is affordable to store its moving aver-
age, which is useful for reducing the impact of
noise in the stochastic regime. To exemplify this,
AdaHessian Algorithm (Yao et al., 2020) uses
Hutchinson’s method (Bekas et al., 2007) to ap-
proximate the Hessian diagonal, and it uses a
second moment of the Hessian diagonal approx-
imation for preconditioning the gradient. Ada-
Hessian achieves impressive results on a wide
range of state-of-the-art tasks. However, its pre-
conditioning matrix approximates the Hessian
diagonal only very approximately, suggesting
that improvements are possible if one can better
approximate the Hessian diagonal.
In this paper, we propose the dOubly Adaptive Scaled algorIthm for machine learning using Second-
order information (OASIS). OASIS approximates the Hessian diagonal in an efﬁcient way, providing
an estimate whose scale much more closely approximates the scale of the true Hessian diagonal (see
Figure 1). Due to this improved scaling, the search direction in OASIS contains gradient information,
in which the components are well-scaled by the novel preconditioning matrix. Therefore, every
gradient component in each dimension is adaptively scaled based on the approximated curvature for
that dimension. For this reason, there is no need to tune the learning rate, as it would be updated auto-
matically based on a local approximation of the Lipschitz smoothness parameter (see Figure 2). The
well-scaled preconditioning matrix coupled with the adaptive learning rate results in a fully adaptive
step for updating the parameters.Here, we provide a brief summary of our main contributions:"
SA M PLES,0.010876132930513595,"• Novel Optimization Algorithm. We propose OASIS as a fully adaptive method that preconditions
the gradient information by a well-scaled Hessian diagonal approximation. The gradient component
in each dimension is adaptively scaled by the corresponding curvature approximation.
• Adaptive Learning Rate. Our methodology does not require us to tune the learning rate, as it
is updated automatically via an adaptive rule. The rule approximates the Lipschitz smoothness
parameter, and it updates the learning rate accordingly.
• Comprehensive Theoretical Analysis. We derive convergence guarantees for OASIS with respect
to different settings of learning rates, namely the case with adaptive learning rate for convex and
strongly convex cases. We also provide the convergence guarantees with respect to ﬁxed learning
rate and line search for both strongly convex and nonconvex settings.
• Competitive Numerical Results. We investigate the empirical performance of OASIS on a variety
of standard machine learning tasks, including logistic regression, nonlinear least squares prob-
lems, and image classiﬁcation. Our proposed method consistently shows competitive or superior
performance in comparison to many ﬁrst- and second-order state-of-the-art methods.
Notation. By considering the positive deﬁnite matrix D, we deﬁne the weighted Euclidean norm of
vector x ∈Rd with ∥x∥2
D = xT Dx. Its corresponding dual norm is shown as ∥· ∥∗
D. The operator"
SA M PLES,0.011480362537764351,Published as a conference paper at ICLR 2022
SA M PLES,0.012084592145015106,"Ada Hessia n (lr= 0.03125)
Ada Hessia n (lr= 0.0625)"
SA M PLES,0.012688821752265862,"Ada Hessia n (lr= 0.125)
Ada Hessia n (lr= 0.25)"
SA M PLES,0.013293051359516616,"Ada Hessia n (lr= 0.5)
Ada Hessia n (lr= 1)"
SA M PLES,0.013897280966767372,"Ada Hessia n (lr= 2)
Ada Hessia n (lr= 4) OASIS"
SA M PLES,0.014501510574018127,log2(lr)
SA M PLES,0.015105740181268883,"5
4
3
2
1
0
1
2"
SA M PLES,0.01570996978851964,Iteration 0 10 20 30 40
SA M PLES,0.016314199395770394,"log(F
F * ) 3 2 1 0"
SA M PLES,0.016918429003021148,"0
10
20
30
40
Iteration 10
3 10
2 10
1 100 F
F *"
SA M PLES,0.017522658610271902,"logistic regression ( = 1/n); rcv1 10
3 10
2 10
1 F
F *"
SA M PLES,0.01812688821752266,"logistic regression (
= 1/n); rcv1; 40 Itera tions"
SA M PLES,0.018731117824773415,"5
4
3
2
1
0
1
2
log2(lr) 0.945 0.950 0.955"
SA M PLES,0.01933534743202417,Test Accura cy
SA M PLES,0.019939577039274924,AdaHessian OASIS
SA M PLES,0.02054380664652568,Figure 2: Adaptive Learning Rate (Logistic Regression with strong-convexity parameter 1
SA M PLES,0.021148036253776436,"n
over rcv1 dataset). left and middle: Comparison of optimality gap for AdaHessian Algorithm with
multiple learning-rate choices vs. OASIS Algorithm with adaptive learning rate (dashed-blue line);
right: Comparison of the best optimality gap and test accuracy for AdaHessian Algorithm w.r.t. each
learning rate shown on x-axis after 40 iterations vs. the optimality gap and test accuracy for our
OASIS Algorithm with adaptive learning rate after 40 iteration (dashed-blue line)."
SA M PLES,0.02175226586102719,"⊙is used as a component-wise product between two vectors. Given a vector v, we represent the
corresponding diagonal matrix of v with diag(v)."
RELATED WORK,0.022356495468277945,"2
RELATED WORK"
RELATED WORK,0.022960725075528703,"In this paper, we analyze algorithms with the generic iterate updates:
wk+1 = wk −ηk ˆDk"
RELATED WORK,0.023564954682779457,"−1mk,
(2)"
RELATED WORK,0.02416918429003021,"where ˆDk is the preconditioning matrix, mk is either gk (the true gradient or the gradient approxi-
mation) or the ﬁrst moment of the gradient with momentum parameter β1 or the bias corrected ﬁrst
moment of the gradient, and ηk is the learning rate. The simple interpretation is that, in order to
update the iterates, the vector mk would be rotated and scaled by the inverse of preconditioning
matrix ˆDk, and the transformed information would be considered as the search direction. Due to
limited space, here we consider only some of the related studies with a diagonal preconditioner.
For more general preconditioning, see Nocedal & Wright (2006). Clearly, one of the beneﬁts of a
well-deﬁned diagonal preconditioner is the easy calculation of its inverse.
There are many optimization algorithms that follow the update in (2). A well-known method is
stochastic gradient descent (SGD).The idea behind SGD is simple yet effective: the preconditioning
matrix is set to be ˆDk = Id, for all k ≥0. There are variants of SGD with and without momentum.
The advantage of using momentum is to smooth the gradient (approximation) over the past iterations,
and it can be useful in the noisy settings. In order to converge to the stationary point(s), the learning
rate in SGD needs to decay. Therefore, there are many important hyperparameters that need to
be tuned, e.g., learning rate, learning-rate decay, batch size, and momentum. Among all of them,
tuning the learning rate is particularly important and cumbersome since the learning rate in SGD is
considered to be the same for all dimensions. To address this issue, one idea is to use an adaptive
diagonal preconditioning matrix, where its elements are based on the local information of the iterates.
One of the initial methods with a non-identity preconditioning matrix is Adagrad (Duchi et al., 2011;
McMahan & Streeter, 2010). In Adagrad, the momentum parameter is set to be zero (mk = gk), and
the preconditioning matrix is deﬁned as:"
RELATED WORK,0.024773413897280966,"ˆDk = diag(
qPk
i=1gk ⊙gk).
(3)"
RELATED WORK,0.025377643504531724,"In the preconditioning matrix ˆDk in (3), every gradient component is scaled with the accumulated
information of all the past squared gradients. It is advantageous in the sense that every component
is scaled adaptively. However, a signiﬁcant drawback of ˆDk in (3) has to do with the progressive
increase of its elements, which leads to rapid decrease of the learning rate. To prevent Adagrad’s
aggressive, monotonically decreasing learning rate, several approaches, including Adadelta (Zeiler,
2012) and RMSProp (Tieleman & Hinton, 2012), have been developed. Speciﬁcally, in RMSProp,
the momentum parameter β1 is zero (or mk = gk) and the preconditioning matrix is as follows:"
RELATED WORK,0.025981873111782478,"ˆDk =
q"
RELATED WORK,0.026586102719033233,"β2 ˆDk−12 + (1 −β2)diag
 
gk ⊙gk

,
(4)"
RELATED WORK,0.027190332326283987,"where β2 is the momentum parameter used in the preconditioning matrix. As we can see from
the difference between the preconditioning matrices in (3) and (4), in RMSProp an exponentially
decaying average of squared gradients is used, which prevents rapid increase of preconditioning"
RELATED WORK,0.027794561933534745,Published as a conference paper at ICLR 2022
RELATED WORK,0.0283987915407855,"components in (3).
Another approach for computing the adaptive scaling for each parameter is Adam (Kingma & Ba,
2014). Besides storing an exponentially decaying average of past squared gradients like Adadelta and
RMSprop, Adam also keeps ﬁrst moment estimate of gradient, similar to SGD with momentum. In
Adam, the bias-corrected ﬁrst and second moment estimates, i.e., mk and ˆDk in (2), are as follows:
mk = 1−β1"
RELATED WORK,0.029003021148036254,"1−βk
1
Pk
i=1βk−i
1
gi,
ˆDk =
q"
RELATED WORK,0.029607250755287008,"1−β2
1−βk
2
Pk
i=1βk−i
2
diag(gi ⊙gi).
(5)"
RELATED WORK,0.030211480362537766,"There have been many other ﬁrst-order methods with adaptive scaling (Loshchilov & Hutter, 2017;
Chaudhari et al., 2019; Loshchilov & Hutter, 2016; Shazeer & Stern, 2018).
The methods described so far have only used the information of the gradient for preconditioning
mk in (2). The main difference of second-order methods is to employ higher order information for
scaling and rotating the mk in (2). To be precise, besides the gradient information, the (approximated)
curvature information of the objective function is also used. As a textbook example, in Newton’s
method ˆDk = ∇2F(wk) and mk = gk with ηk = 1.
Diagonal Approx. Recently, using methods from randomized numerical linear algebra, the AdaHes-
sian method was developed (Yao et al., 2020). AdaHessian approximates the diagonal of the Hessian,
and it uses the second moment of the diagonal Hessian approximation as the preconditioner ˆDk in
(2). In AdaHessian, Hutchinson’s method1 is used to approximate the Hessian diagonal as follows:
Dk ≈diag(E[zk ⊙∇2F(wk)zk]),
(6)
where zk is a random vector with Rademacher distribution. Needless to say,2 the oracle ∇2F(wk)zk,
or Hessian-vector product, can be efﬁciently calculated; in particular, for AdaHessian, it is computed
with two back-propagation rounds without constructing the Hessian explicitly. In a nutshell, the ﬁrst
momentum for AdaHessian is the same as (5), and its second order momentum is:"
RELATED WORK,0.03081570996978852,"ˆDk =
q"
RELATED WORK,0.03141993957703928,"1−β2
1−βk
2
Pk
i=1βk−i
2
D2
i .
(7)"
RELATED WORK,0.03202416918429003,"The intuition behind AdaHessian is to have a larger step size for the dimensions with shallow loss
surfaces and smaller step size for the dimensions with sharp loss surfaces. The results provided
by AdaHessian show its strength by using curvature information, in comparison to other adaptive
ﬁrst-order methods, for a range of state-of-the-art problems in computer vision, natural language
processing, and recommendation systems (Yao et al., 2020). However, even for AdaHessian, the
preconditioning matrix ˆDk in (7) does not approximate the scale of the actual diagonal of the Hessian
particularly well (see Figure 1). One might hope that a better-scaled preconditioner would enable
better use of curvature information. This is one of the main focuses of this study.
Adaptive Learning Rate. In all of the methods discussed previously, the learning rate ηk in (2)
is still a hyperparameter which needs to be manually tuned, and it is a critical and sensitive hy-
perparameter. It is also necessary to tune the learning rate in methods that use approximation of
curvature information (such as quasi-Newton methods like BFGS/LBFGS, and methods using di-
agonal Hessian approximation like AdaHessian). The studies (Loizou et al., 2020; Vaswani et al.,
2019; Chandra et al., 2019; Baydin et al., 2017; Malitsky & Mishchenko, 2020) have tackled the
issue regarding tuning learning rate, and have developed methodologies with adaptive learning rate,
ηk, for ﬁrst-order methods. Speciﬁcally, the work (Malitsky & Mishchenko, 2020) ﬁnds the learning
rate by approximating the Lipschitz smoothness parameter in an affordable way without adding a
tunable hyperparameter which is used for GD-type methods (with identity norm). Extending the latter
approach to the weighted-Euclidean norm is not straightforward. In the next section, we describe how
we can extend the work (Malitsky & Mishchenko, 2020) for the case with weighted Euclidean norm.
This is another main focus of this study. In fact, while we focus on AdaHessian, any method with a
positive-deﬁnite preconditioning matrix and bounded eigenvalues can beneﬁt from our approach."
OASIS,0.03262839879154079,"3
OASIS"
OASIS,0.03323262839879154,"In this section, we present our proposed methodology. First, we focus on the deterministic regime,
and then we describe the stochastic variant of our method."
OASIS,0.033836858006042296,"1For a general symmetric matrix A, E[z ⊙Az] equals the diagonal of A (Bekas et al., 2007).
2Actually, it needs to be said: many within the machine learning community still maintain the incorrect belief
that extracting second order information “requires inverting a matrix.” It does not."
OASIS,0.03444108761329305,Published as a conference paper at ICLR 2022
DETERMINISTIC OASIS,0.035045317220543805,"3.1
DETERMINISTIC OASIS
Similar to the methods described in the previous section, our OASIS Algorithm generates iterates
according to (2)."
DETERMINISTIC OASIS,0.03564954682779456,"Algorithm 1 OASIS
Input:w0, η0, D0, θ0 = +∞"
DETERMINISTIC OASIS,0.03625377643504532,"1: w1 = w0 −η0 ˆD0−1∇F(w0)
2: for k = 1, 2, . . . do
3:
Form Dk via (8) and ˆDk via (9)
4:
Update ηk based on (10)
5:
Set wk+1 = wk −ηk ˆDk−1∇F(wk)
6:
Set θk =
ηk
ηk−1
7: end for"
DETERMINISTIC OASIS,0.036858006042296075,"Motivated by AdaHessian, and by the fact
that the loss surface curvature is different
across different dimensions, we use the cur-
vature information for preconditioning the
gradient. We now describe how the pre-
conditioning matrix ˆDk can be adaptively
updated at each iteration as well as how to
update the learning rate ηk automatically
for performing the step. To capture the cur-
vature information, we also use Hutchin-
son’s method and update the diagonal ap-
proximation as follows:
Dk = β2Dk−1 + (1 −β2) diag(vk),
where vk := zk ⊙∇2F(wk)zk.
(8)
Before we proceed, we make a few more comments about the Hessian diagonal Dk in (8). As is clear
from (8), a decaying exponential average of Hessian diagonal is used, which can be very useful in
the noisy settings for smoothing out the Hessian noise over iterations. Moreover, it approximates
the scale of the Hessian diagonal with a satisfactory precision, unlike AdaHessian Algorithm (see
Figure 1). More importantly, the modiﬁcation is simple yet very effective. Similar simple and
efﬁcient modiﬁcation happens in the evolution of adaptive ﬁrst-order methods (see Section 2).
Further, motivated by (Paternain et al., 2019; Jahani et al., 2021b), in order to ﬁnd a well-deﬁned
preconditioning matrix ˆDk, we truncate the elements of Dk by a positive truncation value α. To be
more precise:"
DETERMINISTIC OASIS,0.03746223564954683,"( ˆDk)i,i = max{|Dk|i,i, α}, ∀i ∈[d].
(9)"
DETERMINISTIC OASIS,0.038066465256797584,"The goal of the truncation described above is to have a well-deﬁned preconditioning matrix that
results in a descent search direction; note the parameter α is equivalent to ϵ in Adam and AdaHessian).
Next, we discuss the adaptive strategy for updating the learning rate ηk in (2). By extending the
adaptive rule in (Malitsky & Mishchenko, 2020) and by deﬁning θk :=
ηk
ηk−1 , ∀k ≥1, our learning"
DETERMINISTIC OASIS,0.03867069486404834,"rate needs to satisfy the inequalities: i) η2
k ≤(1 + θk−1)η2
k−1, and ii) ηk ≤
∥wk−wk−1∥ˆ
Dk
2∥∇F (wk)−∇F (wk−1)∥∗ ˆ
Dk ."
DETERMINISTIC OASIS,0.03927492447129909,"(These inequalities come from the theoretical results.) Thus, the learning rate can be adaptively
calculated as follows:
ηk = min{
p"
DETERMINISTIC OASIS,0.03987915407854985,"1 + θk−1ηk−1, ∥wk −wk−1∥ˆ
Dk/(2∥∇F(wk) −∇F(wk−1)∥∗"
DETERMINISTIC OASIS,0.0404833836858006,"ˆ
Dk)}.
(10)"
DETERMINISTIC OASIS,0.04108761329305136,"As is clear from (10), it is only required to store the previous iterate with its corresponding gradient
and the previous learning rate (a scalar) in order to update the learning rate. Moreover, the learning
rate in (10) is controlled by the gradient and curvature information. As we will see later in the
theoretical results, ηk ≥
α
2L where L is the Lipschitz smoothness parameter of the loss function. It is
noteworthy to highlight that due to usage of weighted-Euclidean norms the required analysis for the
cases with adaptive learning rate is non-trivial (see Section 4). Also, by setting β2 = 1, α = 1, and
D0 = Id, our OASIS algorithm covers the algorithm in (Malitsky & Mishchenko, 2020)."
STOCHASTIC OASIS,0.04169184290030212,"3.2
STOCHASTIC OASIS"
STOCHASTIC OASIS,0.04229607250755287,"In every iteration of OASIS, as presented in the previous section, it is required to evaluate the
gradient and Hessian-vector product on the whole training dataset. However, these computations are
prohibitive in the large-scale setting, i.e., when n and d are large. To address this issue, we present a
stochastic variant of OASIS (see Appendix B for details) that only considers a small mini-batch of
training data in each iteration.
The Stochastic OASIS chooses sets Ik, Jk ⊂[n] randomly and independently, and the new iterate
is computed as: wk+1 = wk −ηk ˆDk−1∇FIk(wk), where ∇FIk(wk) =
1
|Ik|
P"
STOCHASTIC OASIS,0.042900302114803626,i∈Ik ∇Fi(wk)
STOCHASTIC OASIS,0.04350453172205438,"and ˆDk is the truncated variant of Dk = β2Dk−1 + (1 −β2) diag(zk ⊙∇2FJk(wk)zk) with
∇2FJk(wk) =
1
|Jk|
P"
STOCHASTIC OASIS,0.044108761329305135,j∈Jk ∇2Fj(wk).
STOCHASTIC OASIS,0.04471299093655589,Published as a conference paper at ICLR 2022
WARMSTARTING AND COMPLEXITY OF OASIS,0.045317220543806644,"3.3
WARMSTARTING AND COMPLEXITY OF OASIS"
WARMSTARTING AND COMPLEXITY OF OASIS,0.045921450151057405,"Both deterministic and stochastic variants of our methodology share the need to obtain an initial
estimate of D0. The importance of this is illustrated by the rule (10), which regulates the choice of
ηk, which is highly dependent on Dk. In order to have a better approximation of D0, we propose to
sample some predeﬁned number of Hutchinson’s estimates before the training process."
WARMSTARTING AND COMPLEXITY OF OASIS,0.04652567975830816,"The main overhead of our methodology is the Hessian-vector product used in Hutchinson’s method
for approximating the Hessian diagonal. With the current advanced hardware and packages, this
computation is not a bottleneck anymore. To be more speciﬁc, the Hessian-vector product can be
efﬁciently calculated by two rounds of back-propagation. We also present how to calculate the
Hessian-vector product efﬁciently for various well-known machine learning tasks in Appendix B."
THEORETICAL ANALYSIS,0.047129909365558914,"4
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.04773413897280967,"In this section, we present our theoretical results for OASIS. We show convergence guarantees for
different settings of learning rates, i.e., (i) adaptive learning rate, (ii) ﬁxed learning rate, and (iii)
with line search. Before the main theorems are presented, we state the following assumptions and
lemmas that are used throughout this section. For brevity, we present the theoretical results using line
search in Appendix A. The proofs and other auxiliary lemmas are in Appendix A.
Assumption 4.1. (Convex). The function F is convex.
Assumption 4.2. (L−smooth). The gradients of F are L−Lipschitz continuous for all w ∈Rd, i.e.,
∃L > 0 such that ∀w, w′ ∈Rd,F(w) ≤F(w′) + ⟨∇F(w′), w −w′⟩+ L"
THEORETICAL ANALYSIS,0.04833836858006042,"2 ∥w −w′∥2.
Assumption 4.3. The function F is twice continuously differentiable.
Assumption 4.4. (µ−strongly convex). The function F is µ−strongly convex, i.e., there exists a
constant µ > 0 such that ∀w, w′ ∈Rd, F(w) ≥F(w′) + ⟨∇F(w′), w −w′⟩+ µ"
THEORETICAL ANALYSIS,0.04894259818731118,2 ∥w −w′∥2.
THEORETICAL ANALYSIS,0.04954682779456193,"Here is a lemma regarding the bounds for Hutchinson’s approximation and the diagonal differences.
Lemma 4.5. (Bound on change of Dk). Suppose that Assumption 4.2 holds, then i) |(vk)i| ≤Γ ≤
√"
THEORETICAL ANALYSIS,0.050151057401812686,"dL, where vk = zk ⊙∇2F(wk)zk; ii) ∃δ ≤2(1 −β2)Γ such that ∀k : ∥Dk+1 −Dk∥∞≤δ."
ADAPTIVE LEARNING RATE,0.05075528700906345,"4.1
ADAPTIVE LEARNING RATE"
ADAPTIVE LEARNING RATE,0.0513595166163142,"Here, we present theoretical convergence results for the case with adaptive learning rate using (10).
Theorem 4.6. Suppose that Assumptions 4.1, 4.2 and 4.3 hold. Let {wk} be the iterates gen-
erated by Algorithm OASIS, then we have: F( ˆwk) −F ∗≤
LC"
ADAPTIVE LEARNING RATE,0.051963746223564956,"k
+ 2L(1 −β2)Γ Qk"
ADAPTIVE LEARNING RATE,0.05256797583081571,"k , where"
ADAPTIVE LEARNING RATE,0.053172205438066465,"C =
2∥w1−w∗∥2"
ADAPTIVE LEARNING RATE,0.05377643504531722,"ˆ
D0+∥w1−w0∥2"
ADAPTIVE LEARNING RATE,0.054380664652567974,"ˆ
D0
2
+2η1θ1(F(w0)−F(w∗)), and Qk = Pk
i=1( 2ηiθi+α"
ADAPTIVE LEARNING RATE,0.05498489425981873,"2α
∥wi−1−wi∥2+"
ADAPTIVE LEARNING RATE,0.05558912386706949,L2ηiθi+α
ADAPTIVE LEARNING RATE,0.056193353474320244,"α
∥wi −w∗∥2).
Remark 4.7. The following remarks are made regarding Theorem 4.6:"
ADAPTIVE LEARNING RATE,0.056797583081571,"1. If β2 = 1, α = 1, and D0 = Id, the results in Theorem 4.6 completely match with (Malitsky &
Mishchenko, 2020).
2. By considering an extra assumption as in (Reddi et al., 2019; Duchi et al., 2011) regarding
bounded iterates, i.e., ∥wk −w∗∥2 ≤B ∀k ≥0, one can easily show the convergence of OASIS
to the neighborhood of optimal solution(s).
The following lemma provides the bounds for the adaptive learning rate for smooth and strongly-
convex loss functions. The next theorem provides the linear convergence rate for the latter setting.
Lemma 4.8. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, then ηk ∈[ α 2L, Γ 2µ]."
ADAPTIVE LEARNING RATE,0.05740181268882175,"Theorem 4.9. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, and let w∗be the unique solution
for (1). Let {wk} be the iterates generated by Algorithm 1. Then, for all k ≥0 and β2 ≥
max{1−
α4µ4"
ADAPTIVE LEARNING RATE,0.05800604229607251,"4L2Γ2(α2µ2+LΓ2), 1−
α3µ3"
ADAPTIVE LEARNING RATE,0.05861027190332326,"4LΓ(2α2µ2+L3Γ2)} we have: Ψk+1 ≤(1−
α2
2Γ2κ2 )Ψk, where Ψk+1 =
∥wk+1 −w∗∥2"
ADAPTIVE LEARNING RATE,0.059214501510574016,"ˆ
Dk + 1"
ADAPTIVE LEARNING RATE,0.05981873111782477,2∥wk+1 −wk∥2
ADAPTIVE LEARNING RATE,0.06042296072507553,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))."
FIXED LEARNING RATE,0.06102719033232629,"4.2
FIXED LEARNING RATE"
FIXED LEARNING RATE,0.06163141993957704,"Here, we provide theoretical results for ﬁxed learning rate for deterministic and stochastic regimes."
FIXED LEARNING RATE,0.062235649546827795,"Remark 4.10. For any k ≥0, we have αI ⪯ˆDk ⪯ΓI where 0 < α ≤Γ."
FIXED LEARNING RATE,0.06283987915407856,Published as a conference paper at ICLR 2022
DETERMINISTIC REGIME,0.0634441087613293,"4.2.1
DETERMINISTIC REGIME"
DETERMINISTIC REGIME,0.06404833836858007,"Strongly Convex. The following theorem provides the linear convergence for the smooth and
strongly-convex loss functions with ﬁxed learning rate.
Theorem 4.11. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, and let F ∗= F(w∗), where w∗is"
DETERMINISTIC REGIME,0.06465256797583081,"the unique minimizer. Let {wk} be the iterates generated by Algorithm 1, where 0 < ηk = η ≤α2 LΓ,"
DETERMINISTIC REGIME,0.06525679758308157,"and w0 is a starting point. Then, for all k ≥0 we have: F(wk) −F ∗≤(1 −ηµ"
DETERMINISTIC REGIME,0.06586102719033232,Γ )k[F(w0) −F ∗].
DETERMINISTIC REGIME,0.06646525679758308,"Nonconvex. The following theorem provides the convergence to the stationary points for the
nonconvex setting with ﬁxed learning rate."
DETERMINISTIC REGIME,0.06706948640483383,"Assumption 4.12. The function F(.) is bounded below by a scalar ˆF.
Theorem 4.13. Suppose that Assumptions 4.2, 4.3, and 4.12 hold. Let {wk} be the iterates generated"
DETERMINISTIC REGIME,0.06767371601208459,"by Algorithm , where 0 < ηk = η ≤α2"
DETERMINISTIC REGIME,0.06827794561933535,"LΓ, and w0 is a starting point. Then, for all T > 1 we have:"
"T
PT",0.0688821752265861,"1
T
PT
k=1∥∇F(wk)∥2 ≤2Γ[F (w0)−ˆ
F ]
ηT
T →∞
−−−−→0.
(11)"
STOCHASTIC REGIME,0.06948640483383686,"4.2.2
STOCHASTIC REGIME"
STOCHASTIC REGIME,0.07009063444108761,"Here, we use EIk[.] to denote conditional expectation given wk, and E[.] to denote the full expectation
over the full history. The following standard assumptions as in (Bollapragada et al., 2019; Berahas
et al., 2016) are considered for this section.
Assumption 4.14. There exist a constant γ such that EI[∥∇FI(w) −∇F(w)∥2] ≤γ2.
Assumption 4.15. There exist a constant σ2 < ∞such that EI[∥∇FI(w∗)∥2] ≤σ2.
Assumption 4.16. ∇FI(w) is an unbiased estimator of the gradient, i.e., EI[∇FI(w)] = ∇F(w)."
STOCHASTIC REGIME,0.07069486404833837,"Strongly Convex. The following theorem presents the convergence to the neighborhood of the
optimal solution for the smooth and strongly-convex case in the stochastic setting.
Theorem 4.17. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let {wk} be the iterates
generated by Algorithm 1 with ηk = η ∈(0, α2µ"
STOCHASTIC REGIME,0.07129909365558912,"ΓL2 ), then, for all k ≥0,"
STOCHASTIC REGIME,0.07190332326283988,E[F(wk) −F ∗] ≤(1 −c)k (F(w0) −F ∗) + η2Lσ2
STOCHASTIC REGIME,0.07250755287009064,"cα2 ,
(12)"
STOCHASTIC REGIME,0.07311178247734139,where c = 2ηµ
STOCHASTIC REGIME,0.07371601208459215,Γ −2η2L2
STOCHASTIC REGIME,0.0743202416918429,"α2
∈(0, 1). Moreover, if ηk = η ∈(0, α2µ"
STOCHASTIC REGIME,0.07492447129909366,2ΓL2 ) then
STOCHASTIC REGIME,0.0755287009063444,"E[F(wk) −F ∗] ≤
 
1 −ηµ"
STOCHASTIC REGIME,0.07613293051359517,"Γ
k (F(w0) −F ∗) + ηΓLσ2"
STOCHASTIC REGIME,0.07673716012084592,"α2µ .
(13)"
STOCHASTIC REGIME,0.07734138972809668,"Nonconvex. The next theorem provides the convergence to the stationary point for the nonconvex
loss functions in the stochastic regime.
Theorem 4.18. Suppose that Assumptions 4.2, 4.3, 4.12, 4.14 and 4.16 hold. Let {wk} be the iterates
generated by Algorithm 1, where 0 < ηk = η ≤η2"
STOCHASTIC REGIME,0.07794561933534744,"LΓ, and w0 is the starting point. Then, for all k ≥0,"
STOCHASTIC REGIME,0.07854984894259819,"E
h
1
T
PT −1
k=0 ∥∇F(wk)∥2i
≤2Γ[F (w0)−b
F ]
ηT
+ ηΓγ2L"
STOCHASTIC REGIME,0.07915407854984895,"α2
T →∞
−−−−→ηΓγ2L"
STOCHASTIC REGIME,0.0797583081570997,"α2
.
The previous two theorems provide the convergence to the neighborhood of the stationary points. One
can easily use either a variance reduced gradient approximation or a decaying learning rate strategy to
show the convergence to the stationary points (in expectation). OASIS’s analyses are similar to those
of limited-memory quasi-Newton approaches which depends on λmax and λmin (largest and smallest
eigenvalues) of the preconditioning matrix (while in (S)GD λmax= λmin = 1). These methods in
theory are not better than GD-type methods. In practice, however, they have shown their strength."
EMPIRICAL RESULTS,0.08036253776435046,"5
EMPIRICAL RESULTS"
EMPIRICAL RESULTS,0.0809667673716012,"In this section, we present empirical results for several machine learning problems to show that our
OASIS methodology outperforms state-of-the-art ﬁrst- and second-order methods in both determin-
istic and stochastic regimes. We considered: (1) deterministic ℓ2-regularized logistic regression
(strongly convex); (2) deterministic nonlinear least squares (nonconvex), and we report results on
2 standard machine learning datasets rcv1 and ijcnn13; and (3) image classiﬁcation tasks on"
EMPIRICAL RESULTS,0.08157099697885196,3https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
EMPIRICAL RESULTS,0.08217522658610273,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.08277945619335347,"0
100
200
300
400
Number of Effective Passes 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
EMPIRICAL RESULTS,0.08338368580060423,Test Accuracy
EMPIRICAL RESULTS,0.08398791540785498,ijcnn1
EMPIRICAL RESULTS,0.08459214501510574,"300
350
400
0.913 0.914 0.915"
EMPIRICAL RESULTS,0.08519637462235649,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.08580060422960725,"0
100
200
300
400
Number of Effective Passes 10
11 10
9 10
7 10
5 10
3 10
1"
EMPIRICAL RESULTS,0.086404833836858,"F(w)
F *"
EMPIRICAL RESULTS,0.08700906344410876,ijcnn1
EMPIRICAL RESULTS,0.08761329305135952,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.08821752265861027,"0
100
200
300
400
Number of Effective Passes 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
EMPIRICAL RESULTS,0.08882175226586103,Test Accuracy rcv1
EMPIRICAL RESULTS,0.08942598187311178,"300
350
400
0.944 0.945 0.946"
EMPIRICAL RESULTS,0.09003021148036254,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.09063444108761329,"0
100
200
300
400
Number of Effective Passes"
EMPIRICAL RESULTS,0.09123867069486405,"10
13
10
11
10
9
10
7
10
5
10
3
10
1
101"
EMPIRICAL RESULTS,0.09184290030211481,"F(w)
F * rcv1"
EMPIRICAL RESULTS,0.09244712990936556,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.09305135951661632,"Figure 3: Comparison of optimality gap and Test
Accuracy for different algorithms on Logistic Re-
gression Problems."
EMPIRICAL RESULTS,0.09365558912386707,"0
100
200
300
400
Number of Effective Passes 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
EMPIRICAL RESULTS,0.09425981873111783,Test Accuracy
EMPIRICAL RESULTS,0.09486404833836858,ijcnn1
EMPIRICAL RESULTS,0.09546827794561934,"300
350
400
0.9025"
EMPIRICAL RESULTS,0.09607250755287008,0.9075
EMPIRICAL RESULTS,0.09667673716012085,0.9125
EMPIRICAL RESULTS,0.09728096676737161,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.09788519637462235,"0
100
200
300
400
Number of Effective Passes 100"
EMPIRICAL RESULTS,0.09848942598187312,2 × 100 F(w)
EMPIRICAL RESULTS,0.09909365558912386,ijcnn1
EMPIRICAL RESULTS,0.09969788519637462,"300
350
400
0.985 0.990 0.995 1.000 1.005"
EMPIRICAL RESULTS,0.10030211480362537,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.10090634441087613,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9"
EMPIRICAL RESULTS,0.1015105740181269,Test Accuracy rcv1
EMPIRICAL RESULTS,0.10211480362537764,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.1027190332326284,"0
100
200
300
400
Number of Effective Passes 100"
EMPIRICAL RESULTS,0.10332326283987915,"6 × 10
1 F(w) rcv1"
EMPIRICAL RESULTS,0.10392749244712991,"AdGD
AdaHessian
OASIS"
EMPIRICAL RESULTS,0.10453172205438066,"Figure 4:
Comparison of objective function
(F(w)) and Test Accuracy for different algo-
rithms on Non-linear Least Square Problems."
EMPIRICAL RESULTS,0.10513595166163142,"MNIST, CIFAR10, and CIFAR100 datasets on standard network structures.In the interest of space,
we report only a subset of the results in this section. The rest can be found in Appendix C.
To be clear, we compared the empirical performance of OASIS with algorithms with diagonal pre-
conditioners. In the deterministic regime, we compared the performance of OASIS with AdGD
(Malitsky & Mishchenko, 2020) and AdaHessian (Yao et al., 2020). Further, for the stochastic regime,
we provide experiments comparing SGD (Robbins & Monro, 1951), Adam (Kingma & Ba, 2014),
AdamW (Loshchilov & Hutter, 2017), and AdaHessian. For the logistic regression problems, the
regularization parameter was chosen from the set λ ∈{
1
10n, 1 n, 10"
EMPIRICAL RESULTS,0.10574018126888217,"n }. It is worth highlighting that
we ran each method for each of the following experiments from 10 different random initial points.
Moreover, we separately tuned the hyperparameters for each algorithm, if needed. See Appendix C
for details. The proposed OASIS is robust with respect to different choices of hyperparameters, and
it has a narrow spectrum of changes (see Appendix C).
Logistic Regression.
We considered ℓ2-regularized logistic regression problems, F(w) =
1
n
Pn
i=1 log(1 + e−yixT
i w) + λ"
EMPIRICAL RESULTS,0.10634441087613293,"2 ∥w∥2. Figure 3 shows the performance of the methods in terms
of optimality gap and test accuracy versus number of effective passes (number of gradient and
Hessian-vector evaluations). As is clear, the performance of OASIS (with adaptive learning rate and
without any hyperparameter tuning) is on par or better than that of the other methods.
Non-linear Least Square. We considered non-linear least squares problems (described in Xu et al.
(2020)): F(w) = 1"
EMPIRICAL RESULTS,0.10694864048338369,"n
Pn
i=1(yi −1/(1 + e−xT
i w))2. Figure 4 shows that our OASIS Algorithm always
outperforms the other methods in terms of training loss function and test accuracy. Moreover, the
behaviour of OASIS is robust with respect to the different initial points.
Image Classiﬁcation. We illustrate the performance of OASIS on standard bench-marking neural
network training tasks: MNIST, CIFAR10, and CIFAR100. The results for MNIST and the details
of the problems are given in Appendix C. We present the results regarding CIFAR10/CIFAR100.
CIFAR10. We use standard ResNet-20 and ResNet-32 (He et al., 2015) architectures for com-
paring the performance of OASIS with SGD, Adam, AdamW and AdaHessian4. Speciﬁcally, we
report 3 variants of OASIS: (i) adaptive learning rate, (ii) ﬁxed learning rate (without ﬁrst moment)
and (iii) ﬁxed learning rate with gradient momentum tagged with “Adaptive LR,” “Fixed LR,” and
“Momentum,” respectively. For settings with ﬁxed learning rate, no warmstarting is used in order to
obtain an initial D0 approximation. For the case with adaptive learning case, we used the warmstart-
ing strategy to approximate the initial Hessian diagonal. More details regarding the exact parameter
values and hyperparameter search can be found in the Appendix C. The results on CIFAR10 are
shown in the Figure 5 (the left and middle columns) and Table 1. As is clear, the simplest variant of
OASIS with ﬁxed learning rate achieves signiﬁcantly better results, as compared to Adam, and a per-
formance comparable to SGD. For the variant with an added momentum, we get similar accuracy as
AdaHessian, while getting better or the same loss values, highlighting the advantage of using different
preconditioning schema. Another important observation is that OASIS-Adaptive LR, without too
much tuning efforts, has better performance than Adam with sensitive hyperparameters. All in all, the
performance of OASIS variants is on par or better than the other state-of-the-art methods especially"
EMPIRICAL RESULTS,0.10755287009063444,"4Note that we follow the same Experiment Setup as in (Yao et al., 2020), and the codes for other algorithms
and structures are brought from https://github.com/amirgholami/adahessian."
EMPIRICAL RESULTS,0.1081570996978852,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.10876132930513595,"0
20
40
60
80
100
120
140
160
Epochs 30 40 50 60 70 80 90"
EMPIRICAL RESULTS,0.10936555891238671,Test Accuracy
EMPIRICAL RESULTS,0.10996978851963746,ResNet20 on CIFAR10 With Weight Decay
EMPIRICAL RESULTS,0.11057401812688822,"140
150
160
90.0 90.5 91.0 91.5 92.0 92.5"
EMPIRICAL RESULTS,0.11117824773413898,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.11178247734138973,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.11238670694864049,"0
20
40
60
80
100
120
140
160
Epochs 20 30 40 50 60 70 80 90"
EMPIRICAL RESULTS,0.11299093655589124,Test Accuracy
EMPIRICAL RESULTS,0.113595166163142,ResNet32 on CIFAR10 With Weight Decay
EMPIRICAL RESULTS,0.11419939577039274,"140
150
160
91.0 91.5 92.0 92.5 93.0 93.5"
EMPIRICAL RESULTS,0.1148036253776435,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.11540785498489425,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.11601208459214502,"0
25
50
75
100
125
150
175
200
Epochs 10 20 30 40 50 60 70 80"
EMPIRICAL RESULTS,0.11661631419939578,Test Accuracy
EMPIRICAL RESULTS,0.11722054380664652,ResNet18 on CIFAR100 With Weight Decay
EMPIRICAL RESULTS,0.11782477341389729,"180
190
200
72
73
74
75
76
77
78"
EMPIRICAL RESULTS,0.11842900302114803,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.1190332326283988,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.11963746223564954,"0
20
40
60
80
100 120 140 160
Epochs 10
1 100 F(w)"
EMPIRICAL RESULTS,0.1202416918429003,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.12084592145015106,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.12145015105740181,"0
20
40
60
80
100 120 140 160
Epochs 10
2 10
1 100 F(w)"
EMPIRICAL RESULTS,0.12205438066465257,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.12265861027190332,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.12326283987915408,"0
25
50
75
100 125 150 175 200
Epochs 10
3 10
2 10
1 100 F(w)"
EMPIRICAL RESULTS,0.12386706948640483,"AdaHessian
Adam
AdamW
SGD"
EMPIRICAL RESULTS,0.12447129909365559,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
EMPIRICAL RESULTS,0.12507552870090635,"Figure 5: Performance of SGD, Adam, AdamW, Adehessian and different variants of OASIS on
CIFAR10 (left and middle columns) and CIFAR100 (right column) problems on ResNet-20 (left
column), ResNet-32 (middle column) and ResNet-18 (right column)."
EMPIRICAL RESULTS,0.1256797583081571,"SGD. As we see from Figure 5 (left and middle columns), the lack of momentum produces a slow,
noisy training curve in the initial stages of training, while OASIS with momentum works better than
the other two variants in the early stages. All three variants of OASIS get satisfactory results in the
end of training. More results and discussion regarding CIFAR10 dataset are in Appendix C."
EMPIRICAL RESULTS,0.12628398791540785,Table 1: Results of ResNet-20/32 on CIFAR10
EMPIRICAL RESULTS,0.1268882175226586,"Setting
ResNet-20
ResNet-32"
EMPIRICAL RESULTS,0.12749244712990937,"SGD
92.02 ± 0.22
92.85 ± 0.12
Adam
90.46 ± 0.22
91.30 ± 0.15
AdamW
91.99 ± 0.17
92.58 ± 0.25
AdaHessian
92.03 ± 0.10
92.71 ± 0.26"
EMPIRICAL RESULTS,0.12809667673716013,"OASIS- Adaptive LR
91.20 ± 0.20
92.61 ± 0.22
OASIS- Fixed LR
91.96 ± 0.21
93.01 ± 0.09
OASIS- Momentum
92.01 ± 0.19
92.77 ± 0.18"
EMPIRICAL RESULTS,0.12870090634441086,"Table 2:
Results of ResNet-18 on
CIFAR100."
EMPIRICAL RESULTS,0.12930513595166163,"Setting
ResNet-18"
EMPIRICAL RESULTS,0.1299093655589124,"SGD
76.57 ± 0.24
Adam
73.40 ± 0.31
AdamW
72.51 ± 0.76
AdaHessian
75.71 ± 0.47"
EMPIRICAL RESULTS,0.13051359516616315,"OASIS- Adaptive LR
76.93 ± 0.22
OASIS- Fixed LR
76.28 ± 0.21
OASIS- Momentum
76.89 ± 0.34"
EMPIRICAL RESULTS,0.1311178247734139,"CIFAR-100.
We use the hyperparameter settings obtained by training on CIFAR10 on
ResNet-20/32 to train CIFAR100 on ResNet-18 network structure.5 We similarly compare
the performance of our method and its variants with SGD, Adam, AdamW and AdaHessian. The
results are shown in Figure 5 (right column) and Table 2. In this setting, without any hyperparam-
eter tuning, fully adaptive version of our algorithm immediately produces results surpassing other
state-of-the-art methods especially SGD."
FINAL REMARKS,0.13172205438066464,"6
FINAL REMARKS"
FINAL REMARKS,0.1323262839879154,"This paper presents a fully adaptive optimization algorithm for empirical risk minimization. The
search direction uses the gradient information, well-scaled with a novel Hessian diagonal approx-
imation, which itself can be calculated and stored efﬁciently. In addition, we do not need to tune
the learning rate, which instead is automatically updated based on a low-cost approximation of the
Lipschitz smoothness parameter. We provide comprehensive theoretical results covering standard
optimization settings, including convex, strongly convex and nonconvex; and our empirical results
highlight the efﬁciency of OASIS in large-scale machine learning problems.
Future research avenues include: (1) deriving the theoretical results for stochastic regime with
adaptive learning rate; (2) employing variance reduction schemes in order to reduce further the
noise in the gradient and Hessian diagonal estimates; and (3) providing a more extensive empirical
investigation on other demanding machine learning problems such as those from natural language
processing and recommendation system (such as those from the original AdaHessian paper (Yao
et al., 2020))."
FINAL REMARKS,0.13293051359516617,5https://github.com/uoguelph-mlrg/Cutout.
FINAL REMARKS,0.13353474320241693,Published as a conference paper at ICLR 2022
FINAL REMARKS,0.13413897280966766,"Acknowledgements
MT was partially supported by the NSF, under award numbers CCF:1618717/CCF:1740796. PR
was supported by the KAUST Baseline Research Funding Scheme. MM would like to acknowledge
the US NSF and ONR via its BRC on RandNLA for providing partial support of this work. Our
conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no ofﬁcial
endorsement should be inferred."
ETHICS STATEMENT,0.13474320241691842,ETHICS STATEMENT
ETHICS STATEMENT,0.13534743202416918,"This work presents a new algorithm for training machine learning models. We do not foresee any
ethical concerns. All datasets used in this work are from the public domain and are commonly used
benchmarks in ML papers."
REPRODUCIBILITY STATEMENT,0.13595166163141995,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.1365558912386707,"We uploaded all the codes used to make all the experiments presented in this paper. We have used
random seeds to ensure that one can start optimizing the ML models from the same initial starting
point as was used in the experiments. We have used only datasets that are in the public domain,
and one can download them from the following website https://www.csie.ntu.edu.tw/
˜cjlin/libsvmtools/datasets/. After acceptance, we will include a link to the GitHub
repository where we will host the source codes."
REFERENCES,0.13716012084592144,REFERENCES
REFERENCES,0.1377643504531722,"Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782,
2017."
REFERENCES,0.13836858006042296,"C. Bekas, E. Kokiopoulou, and Y. Saad. An estimator for the diagonal of a matrix. Applied
Numerical Mathematics, 57(11):1214–1229, 2007. ISSN 0168-9274. doi: https://doi.org/10.1016/
j.apnum.2007.01.003. URL https://www.sciencedirect.com/science/article/
pii/S0168927407000244. Numerical Algorithms, Parallelism and Applications (2)."
REFERENCES,0.13897280966767372,"Albert S Berahas, Jorge Nocedal, and Martin Tak´aˇc. A multi-batch l-bfgs method for machine
learning. In Advances in Neural Information Processing Systems, pp. 1055–1063, 2016."
REFERENCES,0.13957703927492446,"Albert S Berahas, Majid Jahani, Peter Richt´arik, and Martin Tak´aˇc. Quasi-newton methods for deep
learning: Forget the past, just sample. arXiv preprint arXiv:1901.09997, 2019."
REFERENCES,0.14018126888217522,"Albert S. Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of Newton-Sketch and
subsampled Newton methods. Optimization Methods and Software, 35(4):661–680, 2020."
REFERENCES,0.14078549848942598,"Raghu Bollapragada, Richard H Byrd, and Jorge Nocedal. Exact and inexact subsampled newton
methods for optimization. IMA Journal of Numerical Analysis, 39(2):545–578, 2019."
REFERENCES,0.14138972809667674,"L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223–311, 2018."
REFERENCES,0.1419939577039275,"Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):
977–995, 2011."
REFERENCES,0.14259818731117824,"Kartik Chandra, Erik Meijer, Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George,
Melissa Grueter, Basil Hosmer, StefﬁStumpos, Alanna Tempest, et al. Gradient descent: The
ultimate optimizer. arXiv preprint arXiv:1909.13371, 2019."
REFERENCES,0.143202416918429,"Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019."
REFERENCES,0.14380664652567976,Published as a conference paper at ICLR 2022
REFERENCES,0.14441087613293052,"Frank E. Curtis. A self-correcting variable-metric algorithm for stochastic optimization. In Interna-
tional Conference on Machine Learning, pp. 632–641, 2016."
REFERENCES,0.14501510574018128,"John E Dennis, Jr and Jorge J Mor´e. Quasi-newton methods, motivation and theory. SIAM review, 19
(1):46–89, 1977."
REFERENCES,0.14561933534743202,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.14622356495468278,"Roger Fletcher. Practical Methods of Optimization. John Wiley & Sons, New York, 2 edition, 1987."
REFERENCES,0.14682779456193354,"Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richt´arik. Sgd: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200–5209. PMLR, 2019."
REFERENCES,0.1474320241691843,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385."
REFERENCES,0.14803625377643503,"Majid Jahani, Xi He, Chenxin Ma, Aryan Mokhtari, Dheevatsa Mudigere, Alejandro Ribeiro, and
Martin Tak´aˇc. Efﬁcient distributed hessian free algorithm for large-scale empirical risk minimiza-
tion via accumulating sample strategy. In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 2634–2644. PMLR, 2020a."
REFERENCES,0.1486404833836858,"Majid Jahani, Mohammadreza Nazari, Sergey Rusakov, Albert S Berahas, and Martin Tak´aˇc. Scaling
up quasi-newton algorithms: Communication efﬁcient distributed sr1. In International Conference
on Machine Learning, Optimization, and Data Science, pp. 41–54. Springer, 2020b."
REFERENCES,0.14924471299093656,"Majid Jahani, Naga Venkata C Gudapati, Chenxin Ma, Rachael Tappenden, and Martin Tak´aˇc. Fast
and safe: accelerated gradient methods with optimality certiﬁcates and underestimate sequences.
Computational Optimization and Applications, 79(2):369–404, 2021a."
REFERENCES,0.14984894259818732,"Majid Jahani, Mohammadreza Nazari, Rachael Tappenden, Albert Berahas, and Martin Tak´aˇc. Sonia:
A symmetric blockwise truncated optimization algorithm. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 487–495. PMLR, 2021b."
REFERENCES,0.15045317220543808,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315–323, 2013."
REFERENCES,0.1510574018126888,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.15166163141993957,"Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for sgd: An adaptive learning rate for fast convergence. arXiv preprint arXiv:2002.10542,
2020."
REFERENCES,0.15226586102719034,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.1528700906344411,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.15347432024169183,"Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. In Interna-
tional Conference on Machine Learning, pp. 6702–6712. PMLR, 2020."
REFERENCES,0.1540785498489426,"James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735–742,
2010."
REFERENCES,0.15468277945619335,"H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010."
REFERENCES,0.15528700906344411,"Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151–3181, 2015."
REFERENCES,0.15589123867069488,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013."
REFERENCES,0.1564954682779456,Published as a conference paper at ICLR 2022
REFERENCES,0.15709969788519637,"Yurii Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018."
REFERENCES,0.15770392749244713,"L. M. Nguyen, J. Liu, K Scheinberg, and M. Tak´aˇc. SARAH: a novel method for machine learning
problems using stochastic recursive gradient. In Advances in neural information processing
systems, volume 70, pp. 2613–2621, 2017."
REFERENCES,0.1583081570996979,"Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richt´arik, Katya Scheinberg, and Martin
Tak´aˇc. Sgd and hogwild! convergence without the bounded gradients assumption. In International
Conference on Machine Learning, pp. 3750–3758. PMLR, 2018."
REFERENCES,0.15891238670694863,"Lam M Nguyen, Phuong Ha Nguyen, Peter Richt´arik, Katya Scheinberg, Martin Tak´aˇc, and Marten
van Dijk. New convergence aspects of stochastic gradient algorithms. J. Mach. Learn. Res., 20:
176–1, 2019."
REFERENCES,0.1595166163141994,"Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Series in Operations
Research. Springer, second edition, 2006."
REFERENCES,0.16012084592145015,"Santiago Paternain, Aryan Mokhtari, and Alejandro Ribeiro. A newton-based method for nonconvex
optimization with fast evasion of saddle points. SIAM Journal on Optimization, 29(1):343–368,
2019."
REFERENCES,0.1607250755287009,"Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in neural information processing systems,
pp. 693–701, 2011."
REFERENCES,0.16132930513595167,"Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019."
REFERENCES,0.1619335347432024,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400–407, 1951."
REFERENCES,0.16253776435045317,"F. Roosta, Y. Liu, P. Xu, and M. W. Mahoney. Newton-MR: Newton’s method without smoothness or
convexity. Technical Report Preprint: arXiv:1810.00303, 2018."
REFERENCES,0.16314199395770393,"Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled newton methods. Mathematical
Programming, 2018."
REFERENCES,0.1637462235649547,"Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83–112, 2017."
REFERENCES,0.16435045317220545,"Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
In International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018."
REFERENCES,0.16495468277945619,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31,
2012."
REFERENCES,0.16555891238670695,"Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pp. 1195–1204. PMLR, 2019."
REFERENCES,0.1661631419939577,"P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-type methods for non-convex optimization
under inexact Hessian information. Technical Report Preprint: arXiv:1708.07164, 2017."
REFERENCES,0.16676737160120847,"Peng Xu, Fred Roosta, and Michael W Mahoney. Second-order optimization for non-convex machine
learning: An empirical study. In Proceedings of the 2020 SIAM International Conference on Data
Mining, pp. 199–207. SIAM, 2020."
REFERENCES,0.1673716012084592,"Z. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact non-convex Newton-type methods.
Technical Report Preprint: arXiv:1802.06925, 2018."
REFERENCES,0.16797583081570996,"Z. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney. PyHessian: Neural networks through the lens
of the Hessian. Technical Report Preprint: arXiv:1912.07145, 2019."
REFERENCES,0.16858006042296073,Published as a conference paper at ICLR 2022
REFERENCES,0.1691842900302115,"Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. Adahessian: An
adaptive second order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020."
REFERENCES,0.16978851963746225,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012."
REFERENCES,0.17039274924471298,Published as a conference paper at ICLR 2022
REFERENCES,0.17099697885196374,"A
THEORETICAL RESULTS AND PROOFS"
REFERENCES,0.1716012084592145,"A.1
ASSUMPTIONS"
REFERENCES,0.17220543806646527,"Assumption 4.1. (Convex). The function F is convex, i.e., ∀w, w′ ∈Rd,
F(w) ≥F(w′) + ⟨∇F(w′), w −w′⟩.
(14)"
REFERENCES,0.172809667673716,"Assumption 4.2. (L−smooth). The gradients of F are L−Lipschitz continuous for all w ∈Rd, i.e.,
there exists a constant L > 0 such that ∀w, w′ ∈Rd,"
REFERENCES,0.17341389728096676,"F(w) ≤F(w′) + ⟨∇F(w′), w −w′⟩+ L"
REFERENCES,0.17401812688821752,"2 ∥w −w′∥2.
(15)"
REFERENCES,0.17462235649546828,Assumption 4.3. The function F is twice continuously differentiable.
REFERENCES,0.17522658610271905,"Assumption 4.4. (µ−strongly convex). The function F is µ−strongly convex, i.e., there exists a
constant µ > 0 such that ∀w, w′ ∈Rd,"
REFERENCES,0.17583081570996978,"F(w) ≥F(w′) + ⟨∇F(w′), w −w′⟩+ µ"
REFERENCES,0.17643504531722054,"2 ∥w −w′∥2.
(16)"
REFERENCES,0.1770392749244713,Assumption 4.12. The function F(.) is bounded below by a scalar ˆF.
REFERENCES,0.17764350453172206,Assumption 4.14. There exist a constant γ such that EI[∥∇FI(w) −∇F(w)∥2] ≤γ2.
REFERENCES,0.1782477341389728,"Assumption 4.16. ∇FI(w) is an unbiased estimator of the gradient, i.e., EI[∇FI(w)] = ∇F(w),
where the samples I are drawn independently."
REFERENCES,0.17885196374622356,"A.2
PROOF OF LEMMA 4.5"
REFERENCES,0.17945619335347432,"Lemma 4.5. (Bound on change of Dk). Suppose that Assumption 4.2 holds, i.e., ∀w : ∇2f(w) ⪯LI,
then"
REFERENCES,0.18006042296072508,"1. |(vk)i| ≤Γ ≤
√"
REFERENCES,0.18066465256797584,"dL, where vk = zk ⊙∇2F(wk)zk."
REFERENCES,0.18126888217522658,"2. there ∃δ ≤2(1 −β2)Γ such that
∥Dk+1 −Dk∥∞≤δ,
∀k."
REFERENCES,0.18187311178247734,"Proof. By Assumption 4.2, we have that ∥∇2F(w)∥2 ≤L and hence"
REFERENCES,0.1824773413897281,"∥vk∥∞≤∥∇2F(w)∥∞≤
√"
REFERENCES,0.18308157099697886,"d∥∇2F(w)∥2 ≤
√"
REFERENCES,0.18368580060422962,"dL,
which ﬁnishes the proof of case 1."
REFERENCES,0.18429003021148035,"Now, from (8) we can derive"
REFERENCES,0.18489425981873112,"Dk+1 −Dk
(8)
= (β2 −1)Dk + (1 −β2) zk ⊙∇2F(wk)zk
and hence
∥Dk+1 −Dk∥∞= (1 −β2)∥Dk −zk ⊙∇2F(wk)zk∥∞≤(1 −β2)∥Dk −vk∥∞
≤(1 −β2) (∥Dk∥∞+ ∥vk∥∞) ≤2(1 −β2)Γ."
REFERENCES,0.18549848942598188,"A.3
LEMMA REGARDING SMOOTHNESS WITH WEIGHTED NORM"
REFERENCES,0.18610271903323264,"In the following, we present a lemma for smoothness with weighted norm. Theorem 2.1.5 in (Nesterov
et al., 2018) provides the same analysis for any norm, and the following lemma can be seen as a
special case of Theorem 2.1.5 (Nesterov et al., 2018) with respect to the weighted Euclidean norm.
In the following, we provide the proof for completeness."
REFERENCES,0.18670694864048337,"Lemma A.1. (Smoothness with norm D) Suppose that Assumptions 4.1 and 4.2 hold, and D ≻0,
then we have:
∥∇F(x) −∇F(y)∥∗
D ≤˜L∥x −y∥D,
(17)"
REFERENCES,0.18731117824773413,"where ˜L =
L
λmin(D)."
REFERENCES,0.1879154078549849,Published as a conference paper at ICLR 2022
REFERENCES,0.18851963746223566,"Proof. By the equality ˜L =
L
λmin(D), we conclude that ˜LD ⪰LI which results in:"
REFERENCES,0.18912386706948642,"F(y) ≤F(x) + ⟨∇F(x), y −x⟩+
˜L
2 ∥x −y∥2
D.
(18)"
REFERENCES,0.18972809667673715,"Extending proof of Theorem 2.1.5. in (Nesterov, 2013) we deﬁne φ(y) = F(y) −⟨∇F(x), y⟩. Then,
clearly, x ∈arg min φ(y) and (18) is still valid for φ(y)."
REFERENCES,0.1903323262839879,Therefore
REFERENCES,0.19093655589123867,φ(x) ≤φ(y −D−1
REFERENCES,0.19154078549848944,"˜L
∇φ(y)),"
REFERENCES,0.19214501510574017,"φ(x)
(18)
≤φ(y) + ⟨∇φ(y), −D−1"
REFERENCES,0.19274924471299093,"˜L
∇φ(y)⟩+
˜L
2 ∥−D−1"
REFERENCES,0.1933534743202417,"˜L
∇φ(y)∥2
D,"
REFERENCES,0.19395770392749245,"F(x) −⟨∇F(x), x⟩≤F(y) −⟨∇F(x), y⟩−1"
REFERENCES,0.19456193353474321,"˜L
⟨∇φ(y), D−1∇φ(y)⟩+ 1"
REFERENCES,0.19516616314199395,"2˜L
∥D−1∇φ(y)∥2
D,"
REFERENCES,0.1957703927492447,"F(x) ≤F(y) + ⟨∇F(x), x −y⟩−1"
REFERENCES,0.19637462235649547,"2˜L
(∥∇φ(y)∥∗
D)2,"
REFERENCES,0.19697885196374623,"F(x) ≤F(y) + ⟨∇F(x), x −y⟩−1"
REFERENCES,0.19758308157099697,"2˜L
(∥∇F(y) −∇F(x)∥∗
D)2. Thus"
REFERENCES,0.19818731117824773,"F(x) + ⟨∇F(x), y −x⟩+ 1"
REFERENCES,0.1987915407854985,"2˜L
(∥∇F(y) −∇F(x)∥∗
D)2 ≤F(y).
(19)"
REFERENCES,0.19939577039274925,"Adding (19) with itself with x swapped with y we obtain
1
˜L
(∥∇F(y) −∇F(x)∥∗
D)2 ≤⟨∇F(y) −∇F(x), y −x⟩"
REFERENCES,0.2,"= ⟨D−1(∇F(y) −∇F(x)), D(y −x)⟩
≤∥∇F(y) −∇F(x))∥∗
D∥y −x∥D,
which implies that"
REFERENCES,0.20060422960725074,"∥∇F(x) −∇F(y)∥∗
D ≤˜L∥x −y∥D.
(20)"
REFERENCES,0.2012084592145015,"A.4
PROOF OF THEOREM 4.6"
REFERENCES,0.20181268882175227,"Lemma A.2. Let f : Rd →R be a convex function, and x∗is one of the optimal solutions for (1).
Then, for the sequence of {wk} generated by Algorithm 1 we have:"
REFERENCES,0.20241691842900303,∥wk+1 −w∗∥2
REFERENCES,0.2030211480362538,"ˆ
Dk+1"
REFERENCES,0.20362537764350452,2∥wk −wk+1∥2
REFERENCES,0.20422960725075529,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
REFERENCES,0.20483383685800605,≤∥wk −w∗∥2
REFERENCES,0.2054380664652568,"ˆ
Dk−1 + 1"
REFERENCES,0.20604229607250754,2∥wk −wk−1∥2
REFERENCES,0.2066465256797583,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))+"
REFERENCES,0.20725075528700906,"2(1 −β2)Γ

(ηkθk α
+ 1"
REFERENCES,0.20785498489425983,2)∥wk−1 −wk∥2 + (L2ηkθk
REFERENCES,0.2084592145015106,"α
+ 1)∥wk −w∗∥2
. (21)"
REFERENCES,0.20906344410876132,"Proof. We extend the proof in (Malitsky & Mishchenko, 2020). We have
∥wk+1 −w∗∥2"
REFERENCES,0.20966767371601208,"ˆ
Dk = ∥wk+1 −wk + wk −w∗∥2"
REFERENCES,0.21027190332326284,"ˆ
Dk
= ∥wk+1 −wk∥2"
REFERENCES,0.2108761329305136,"ˆ
Dk + ∥wk −w∗∥2"
REFERENCES,0.21148036253776434,"ˆ
Dk + 2⟨wk+1 −wk, ˆDk(wk −w∗)⟩"
REFERENCES,0.2120845921450151,= ∥wk+1 −wk∥2
REFERENCES,0.21268882175226586,"ˆ
Dk + ∥wk −w∗∥2"
REFERENCES,0.21329305135951662,"ˆ
Dk + 2ηk⟨∇F(wk), w∗−wk⟩"
REFERENCES,0.21389728096676738,≤∥wk+1 −wk∥2
REFERENCES,0.21450151057401812,"ˆ
Dk + ∥wk −w∗∥2"
REFERENCES,0.21510574018126888,"ˆ
Dk + 2ηk(F(w∗) −F(wk))
(22)"
REFERENCES,0.21570996978851964,= ∥wk+1 −wk∥2
REFERENCES,0.2163141993957704,"ˆ
Dk + ∥wk −w∗∥2"
REFERENCES,0.21691842900302113,"ˆ
Dk −2ηk(F(wk) −F(w∗)),"
REFERENCES,0.2175226586102719,Published as a conference paper at ICLR 2022
REFERENCES,0.21812688821752266,"where the third equality comes from the OASIS’s step and the inequality follows from convexity of
F(w). Now, let’s focus on ∥wk+1 −wk∥2"
REFERENCES,0.21873111782477342,"ˆ
Dk. We have"
REFERENCES,0.21933534743202418,∥wk+1 −wk∥2
REFERENCES,0.2199395770392749,"ˆ
Dk =2∥wk+1 −wk∥2"
REFERENCES,0.22054380664652568,"ˆ
Dk −∥wk+1 −wk∥2 ˆ
Dk"
REFERENCES,0.22114803625377644,=2⟨−ηk ˆDk
REFERENCES,0.2217522658610272,"−1∇F(wk), ˆDk(wk+1 −wk)⟩−∥wk+1 −wk∥2"
REFERENCES,0.22235649546827796,"ˆ
Dk
= −2ηk⟨∇F(wk), wk+1 −wk⟩−∥wk+1 −wk∥2"
REFERENCES,0.2229607250755287,"ˆ
Dk
= −2ηk⟨∇F(wk) −∇F(wk−1), wk+1 −wk⟩−2ηk⟨∇F(wk−1), wk+1 −wk⟩−"
REFERENCES,0.22356495468277945,∥wk+1 −wk∥2
REFERENCES,0.22416918429003022,"ˆ
Dk
=2ηk⟨∇F(wk) −∇F(wk−1), wk −wk+1⟩+ 2ηk⟨∇F(wk−1), wk −wk+1⟩−"
REFERENCES,0.22477341389728098,"∥wk+1 −wk∥2 ˆ
Dk. Now,"
REFERENCES,0.2253776435045317,"2ηk⟨∇F(wk) −∇F(wk−1), wk −wk+1⟩≤2ηk∥∇F(wk) −∇F(wk−1)∥∗"
REFERENCES,0.22598187311178247,"ˆ
Dk∥wk −wk+1∥ˆ
Dk"
REFERENCES,0.22658610271903323,"(10)
≤∥wk −wk−1∥ˆ
Dk∥wk −wk+1∥ˆ
Dk ≤1"
REFERENCES,0.227190332326284,2∥wk −wk−1∥2
REFERENCES,0.22779456193353476,"ˆ
Dk + 1"
REFERENCES,0.2283987915407855,"2∥wk −wk+1∥2 ˆ
Dk,"
REFERENCES,0.22900302114803625,"where the ﬁrst inequality comes from Cauchy-Schwarz and the third one follows Young’s inequality.
Further,"
REFERENCES,0.229607250755287,"⟨∇F(wk−1), wk −wk+1⟩=
1
ηk−1
⟨ˆDk−1(wk−1 −wk), wk −wk+1⟩"
REFERENCES,0.23021148036253777,"=
1
ηk−1
⟨ˆDk−1(wk−1 −wk), ηk ˆDk"
REFERENCES,0.2308157099697885,−1∇F(wk)⟩ = ηk
REFERENCES,0.23141993957703927,"ηk−1
(wk−1 −wk)T ˆDk−1 ˆDk"
REFERENCES,0.23202416918429003,−1∇F(wk) = ηk
REFERENCES,0.2326283987915408,"ηk−1
(wk−1 −wk)T ∇F(wk)+"
REFERENCES,0.23323262839879155,"ηk
ηk−1
(wk−1 −wk)T "
REFERENCES,0.2338368580060423,ˆDk−1 ˆDk
REFERENCES,0.23444108761329305,"−1 −I

∇F(wk),
(23)"
REFERENCES,0.2350453172205438,"where the ﬁrst two qualities are due the OASIS’s update step. The second term in the above equality
can be upperbounded as follows:"
REFERENCES,0.23564954682779457,(wk−1 −wk)T 
REFERENCES,0.2362537764350453,ˆDk−1 ˆDk
REFERENCES,0.23685800604229607,"−1 −I

∇F(wk) ≤(1 −β2)2Γ"
REFERENCES,0.23746223564954683,"α · ∥wk−1 −wk∥· ∥∇F(wk)∥,
(24)"
REFERENCES,0.2380664652567976,where multiplier on the left is obtained via the bound on ∥· ∥∞norm of the diagonal matrix
REFERENCES,0.23867069486404835,ˆDk−1 ˆDk−1 −I:
REFERENCES,0.23927492447129908,∥ˆDk−1 ˆDk
REFERENCES,0.23987915407854984,"−1 −I∥∞= max
i "
REFERENCES,0.2404833836858006,ˆDk−1 ˆDk
REFERENCES,0.24108761329305137,"−1 −I
 i"
REFERENCES,0.24169184290030213,"= max
i
  ˆDk−1 −ˆDk "
REFERENCES,0.24229607250755286,"i
  ˆDk −1"
REFERENCES,0.24290030211480362,"i
 ≤(1 −β2)2Γ α ,"
REFERENCES,0.24350453172205438,"which also represents a bound on the operator norm of the same matrix difference. Next we can use
the Young’s inequality and ∇F(w∗) = 0 to get"
REFERENCES,0.24410876132930515,(1 −β2)2Γ
REFERENCES,0.24471299093655588,α · ∥wk−1 −wk∥· ∥∇F(wk)∥≤(1 −β2)Γ
REFERENCES,0.24531722054380664,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2
. (25)"
REFERENCES,0.2459214501510574,"Therefore, we have"
REFERENCES,0.24652567975830816,"⟨∇F(wk−1), wk −wk+1⟩≤ηk"
REFERENCES,0.24712990936555893,"ηk−1
(wk−1 −wk)T ∇F(wk)+"
REFERENCES,0.24773413897280966,(1 −β2)Γ
REFERENCES,0.24833836858006042,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2)."
REFERENCES,0.24894259818731118,"Published as a conference paper at ICLR 2022 Also,"
REFERENCES,0.24954682779456194,∥wk+1 −wk∥2
REFERENCES,0.2501510574018127,"ˆ
Dk ≤1"
REFERENCES,0.25075528700906347,2∥wk −wk−1∥2
REFERENCES,0.2513595166163142,"ˆ
Dk + 1"
REFERENCES,0.25196374622356493,2∥wk −wk+1∥2
REFERENCES,0.2525679758308157,"ˆ
Dk + 2ηkθk(wk−1 −wk)T ∇F(wk)+"
REFERENCES,0.25317220543806646,2ηkθk(1 −β2)Γ
REFERENCES,0.2537764350453172,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2) −∥wk+1 −wk∥2 ˆ
Dk ≤1"
REFERENCES,0.254380664652568,2∥wk −wk−1∥2
REFERENCES,0.25498489425981874,"ˆ
Dk + 1"
REFERENCES,0.2555891238670695,2∥wk −wk+1∥2
REFERENCES,0.25619335347432026,"ˆ
Dk + 2ηkθk(F(wk−1) −F(wk))+"
REFERENCES,0.256797583081571,2ηkθk(1 −β2)Γ
REFERENCES,0.25740181268882173,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2) −∥wk+1 −wk∥2 ˆ
Dk."
REFERENCES,0.2580060422960725,"Finally, we have"
REFERENCES,0.25861027190332325,∥wk+1 −w∗∥2
REFERENCES,0.259214501510574,"ˆ
Dk ≤1"
REFERENCES,0.2598187311178248,2∥wk −wk−1∥2
REFERENCES,0.26042296072507554,"ˆ
Dk + 1"
REFERENCES,0.2610271903323263,2∥wk −wk+1∥2
REFERENCES,0.26163141993957706,"ˆ
Dk + 2ηkθk(F(wk−1) −F(wk))+"
REFERENCES,0.2622356495468278,2ηkθk(1 −β2)Γ
REFERENCES,0.2628398791540785,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2) −∥wk+1 −wk∥2"
REFERENCES,0.2634441087613293,"ˆ
Dk
+ ∥wk −w∗∥2"
REFERENCES,0.26404833836858005,"ˆ
Dk −2ηk(F(wk) −F(w∗))."
REFERENCES,0.2646525679758308,"By simplifying the above inequality, we have:"
REFERENCES,0.26525679758308157,∥wk+1 −w∗∥2
REFERENCES,0.26586102719033233,"ˆ
Dk+1"
REFERENCES,0.2664652567975831,2∥wk −wk+1∥2
REFERENCES,0.26706948640483386,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
REFERENCES,0.2676737160120846,≤∥wk −w∗∥2
REFERENCES,0.2682779456193353,"ˆ
Dk + 1"
REFERENCES,0.2688821752265861,2∥wk −wk−1∥2
REFERENCES,0.26948640483383685,"ˆ
Dk + 2ηkθk(F(wk−1) −F(w∗))+"
REFERENCES,0.2700906344410876,2ηkθk(1 −β2)Γ
REFERENCES,0.27069486404833837,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2)"
REFERENCES,0.27129909365558913,= ∥wk −w∗∥2
REFERENCES,0.2719033232628399,"ˆ
Dk−1 + 1"
REFERENCES,0.27250755287009065,2∥wk −wk−1∥2
REFERENCES,0.2731117824773414,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))+"
REFERENCES,0.2737160120845921,2ηkθk(1 −β2)Γ
REFERENCES,0.2743202416918429,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2)+"
REFERENCES,0.27492447129909364,∥wk −w∗∥2
REFERENCES,0.2755287009063444,"ˆ
Dk−ˆ
Dk−1 + 1"
REFERENCES,0.27613293051359517,2∥wk −wk−1∥2
REFERENCES,0.2767371601208459,"ˆ
Dk−ˆ
Dk−1"
REFERENCES,0.2773413897280967,≤∥wk −w∗∥2
REFERENCES,0.27794561933534745,"ˆ
Dk−1 + 1"
REFERENCES,0.2785498489425982,2∥wk −wk−1∥2
REFERENCES,0.2791540785498489,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))+"
REFERENCES,0.2797583081570997,2ηkθk(1 −β2)Γ
REFERENCES,0.28036253776435044,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2)+"
REFERENCES,0.2809667673716012,2(1 −β2)Γ(∥wk −w∗∥2 + 1
REFERENCES,0.28157099697885196,2∥wk −wk−1∥2)
REFERENCES,0.2821752265861027,= ∥wk −w∗∥2
REFERENCES,0.2827794561933535,"ˆ
Dk−1 + 1"
REFERENCES,0.28338368580060425,2∥wk −wk−1∥2
REFERENCES,0.283987915407855,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))+"
REFERENCES,0.2845921450151057,"2(1 −β2)Γ
ηkθk"
REFERENCES,0.2851963746223565,α ∥wk−1 −wk∥2 + L2ηkθk
REFERENCES,0.28580060422960724,"α
∥wk −w∗∥2 + ∥wk −w∗∥2+"
REFERENCES,0.286404833836858,"1
2∥wk −wk−1∥2
."
REFERENCES,0.28700906344410876,"Theorem 4.6. Suppose that Assumptions 4.1, 4.2 and 4.3 hold. Let {wk} be the iterates generated
by Algorithm 1, then we have:"
REFERENCES,0.2876132930513595,F( ˆwk) −F ∗≤LC
REFERENCES,0.2882175226586103,"k
+ 2L(1 −β2)ΓQk k , where"
REFERENCES,0.28882175226586104,C = ∥w1 −w∗∥2
REFERENCES,0.2894259818731118,"ˆ
D0 + 1"
REFERENCES,0.29003021148036257,2∥w1 −w0∥2
REFERENCES,0.29063444108761327,"ˆ
D0 + 2η1θ1(F(w0) −F(w∗)) Qk = k
X i=1"
REFERENCES,0.29123867069486403,"
(ηiθi α
+ 1"
REFERENCES,0.2918429003021148,2)∥wi−1 −wi∥2 + (L2ηiθi
REFERENCES,0.29244712990936556,"α
+ 1)∥wi −w∗∥2
."
REFERENCES,0.2930513595166163,Published as a conference paper at ICLR 2022
REFERENCES,0.2936555891238671,Proof. By telescoping inequality (21) in Lemma A.2 we have:
REFERENCES,0.29425981873111784,∥wk+1 −w∗∥2
REFERENCES,0.2948640483383686,"ˆ
Dk+1"
REFERENCES,0.29546827794561936,2∥wk −wk+1∥2
REFERENCES,0.29607250755287007,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗)) + 2 k−1
X"
REFERENCES,0.29667673716012083,"i=1
[ηi(1 + θi) −ηi+1θi+1](F(wk) −F(w∗))"
REFERENCES,0.2972809667673716,≤∥w1 −w∗∥2
REFERENCES,0.29788519637462235,"ˆ
D0 + 1"
REFERENCES,0.2984894259818731,2∥w1 −w0∥2
REFERENCES,0.2990936555891239,"ˆ
D0 + 2η1θ1(F(w0) −F(w∗))
|
{z
}
C +"
REFERENCES,0.29969788519637464,"2(1 −β2)Γ k
X i=1"
REFERENCES,0.3003021148036254,"
(ηiθi α
+ 1"
REFERENCES,0.30090634441087616,2)∥wi−1 −wi∥2 + (L2ηiθi
REFERENCES,0.30151057401812686,"α
+ 1)∥wi −w∗∥2"
REFERENCES,0.3021148036253776,"|
{z
}
Qk ."
REFERENCES,0.3027190332326284,"Moreover, by the rule for adaptive learning rule we know ηi(1 + θi) −ηi+1θi+1 ≥0, ∀i. Therefore,
we have"
REFERENCES,0.30332326283987915,"2ηk(1 + θk)(F(wk) −F(w∗))+2 k−1
X"
REFERENCES,0.3039274924471299,"i=1
[ηi(1 + θi) −ηi+1θi+1](F(wk) −F(w∗))"
REFERENCES,0.30453172205438067,≤C + 2(1 −β2)ΓQk.
REFERENCES,0.30513595166163143,"By setting ˆw = ηk(1 + θk)wk + Pk−1
i=1 (ηi(1 + θi) −ηi+1θi+1)wi"
REFERENCES,0.3057401812688822,"Sk
, where Sk := ηk(1 + θk) +
Pk−1
i=1 (ηi(1 + θi) −ηi+1θi+1), and by using Jensens’s inequality, we have:"
REFERENCES,0.30634441087613296,"F( ˆwk) −F ∗≤
C
2Sk
+ (1 −β2)ΓQk Sk
."
REFERENCES,0.30694864048338366,By the fact that ηk ≥1
REFERENCES,0.3075528700906344,2L thus 1
REFERENCES,0.3081570996978852,"Sk
≤2L"
REFERENCES,0.30876132930513595,"k , we have"
REFERENCES,0.3093655589123867,F( ˆwk) −F ∗≤LC
REFERENCES,0.30996978851963747,"k
+ 2L(1 −β2)ΓQk k ."
REFERENCES,0.31057401812688823,"A.5
PROOF OF LEMMA 4.8"
REFERENCES,0.311178247734139,"Lemma 4.8. Suppose that Assumptions 4.2, 4.3 and 4.4 hold, then ηk ∈
h α 2L, Γ 2µ i
."
REFERENCES,0.31178247734138975,"Proof. By (17) and the point that ˜L =
L"
REFERENCES,0.31238670694864046,"λmin( ˆDk)
, we conclude that:"
REFERENCES,0.3129909365558912,"∥wk −wk−1∥ˆ
Dk
2∥∇F(wk) −∇F(wk−1)∥∗ ˆ
Dk ≥1"
REFERENCES,0.313595166163142,"2˜L
= λmin( ˆDk)"
L,0.31419939577039274,"2L
≥α 2L."
L,0.3148036253776435,"Now, in order to ﬁnd the upperbound for
∥wk−wk−1∥ˆ
Dk
2∥∇F (wk)−∇F (wk−1)∥∗ ˆ
Dk"
L,0.31540785498489426,", we use the following inequality"
L,0.316012084592145,which comes from Assumption 4.4:
L,0.3166163141993958,"F(wk) ≥F(wk−1) + ⟨∇F(wk−1), (wk −wk−1)⟩+ µ"
L,0.31722054380664655,"2 ∥w −wk−1∥2.
(26)"
L,0.31782477341389725,"By setting ˜µ =
µ"
L,0.318429003021148,"λmax( ˆDk)
, we conclude that ˜µ ˆDk ⪯µI which results in:"
L,0.3190332326283988,"F(wk) ≥F(wk−1) + ⟨∇F(wk−1), (wk −wk−1)⟩+ ˜µ"
L,0.31963746223564954,2 ∥w −wk−1∥2
L,0.3202416918429003,"ˆ
Dk.
(27)"
L,0.32084592145015106,Published as a conference paper at ICLR 2022
L,0.3214501510574018,"The above inequality results in:
˜µ∥w −wk−1∥2"
L,0.3220543806646526,"ˆ
Dk ≤⟨∇F(wk) −∇F(wk−1), (wk −wk−1)⟩"
L,0.32265861027190335,= ⟨ˆDk
L,0.32326283987915405,"−1(∇F(wk) −∇F(wk−1)), ˆDk(wk −wk−1)⟩
≤∥∇F(wk) −∇F(wk−1))∥∗"
L,0.3238670694864048,"ˆ
Dk∥wk −wk−1∥ˆ
Dk.
(28)"
L,0.3244712990936556,"Therefore, we obtain that
∥wk −wk−1∥ˆ
Dk
2∥∇F(wk) −∇F(wk−1)∥∗ ˆ
Dk"
L,0.32507552870090634,"(28)
≤
1
2˜µ = λmax( ˆDk) 2µ
≤Γ 2µ,"
L,0.3256797583081571,"and therefore, by the update rule for ηk, we conclude that ηk ∈
h α 2L, Γ 2µ i
."
L,0.32628398791540786,"A.6
PROOF OF THEOREM 4.9"
L,0.3268882175226586,"Lemma A.3. . Suppose Assumptions 4.2, 4.3 and 4.4 hold and let w∗be the unique solution of (1).
Then for (wk) generated by Algorithm 1 we have:"
L,0.3274924471299094,∥wk+1 −w∗∥2
L,0.32809667673716014,"ˆ
Dk + 1"
L,0.3287009063444109,2∥wk+1 −wk∥2
L,0.3293051359516616,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
L,0.32990936555891237,≤∥wk −w∗∥2
L,0.33051359516616313,"ˆ
Dk−1 + 1"
L,0.3311178247734139,2∥wk −wk−1∥2
L,0.33172205438066465,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))"
L,0.3323262839879154,"+

(1 −β2)Γ

1 + 2θkηk α"
L,0.3329305135951662,"
−µηkθk"
L,0.33353474320241694,"
∥wk −wk−1∥2"
L,0.3341389728096677,"+

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.3347432024169184,"
−µηk"
L,0.33534743202416917,"
∥wk −w∗∥2."
L,0.33595166163141993,"Proof. By the update rule in Algorithm 1 we have:
∥wk+1 −w∗∥2"
L,0.3365558912386707,"ˆ
Dk = ∥wk+1 −wk + wk −w∗∥2"
L,0.33716012084592145,"ˆ
Dk
= ∥wk+1 −wk∥2"
L,0.3377643504531722,"ˆ
Dk + ∥wk −w∗∥2"
L,0.338368580060423,"ˆ
Dk + 2⟨wk+1 −wk, ˆDk(wk −w∗)⟩"
L,0.33897280966767374,= ∥wk+1 −wk∥2
L,0.3395770392749245,"ˆ
Dk + ∥wk −w∗∥2"
L,0.3401812688821752,"ˆ
Dk + 2ηk⟨∇F(wk), w∗−wk⟩"
L,0.34078549848942596,≤∥wk+1 −wk∥2
L,0.3413897280966767,"ˆ
Dk + ∥wk −w∗∥2"
L,0.3419939577039275,"ˆ
Dk + 2ηk(F(w∗) −F(wk))
(29)"
L,0.34259818731117825,= ∥wk+1 −wk∥2
L,0.343202416918429,"ˆ
Dk + ∥wk −w∗∥2"
L,0.34380664652567977,"ˆ
Dk −2ηk(F(wk) −F(w∗)),"
L,0.34441087613293053,where the inequality follows from convexity of F(w). By strong convexity of F(.) we have:
L,0.3450151057401813,"F(w∗) ≥F(wk) + ⟨∇F(wk), w∗−wk⟩+ µ"
L,0.345619335347432,"2 ∥wk −w∗∥2.
(30)"
L,0.34622356495468276,In the lights of the strongly convex inequality we can change (29) as follows
L,0.3468277945619335,"∥wk+1 −w∗∥2 ˆ
Dk"
L,0.3474320241691843,"(29),(30)
≤
∥wk+1 −wk∥2"
L,0.34803625377643505,"ˆ
Dk + ∥wk −w∗∥2"
L,0.3486404833836858,"ˆ
Dk + 2ηk(F(w∗) −F(wk) −µ"
L,0.34924471299093657,2 ∥wk −w∗∥2)
L,0.34984894259818733,= ∥wk+1 −wk∥2
L,0.3504531722054381,"ˆ
Dk + ∥wk −w∗∥2"
L,0.3510574018126888,"ˆ
Dk −2ηk(F(wk) −F(w∗)) −µηk∥wk −w∗∥2.
(31)"
L,0.35166163141993956,Published as a conference paper at ICLR 2022
L,0.3522658610271903,"Now, let’s focus on ∥wk+1 −wk∥2"
L,0.3528700906344411,"ˆ
Dk. We have"
L,0.35347432024169184,∥wk+1 −wk∥2
L,0.3540785498489426,"ˆ
Dk =2∥wk+1 −wk∥2"
L,0.35468277945619336,"ˆ
Dk −∥wk+1 −wk∥2 ˆ
Dk"
L,0.3552870090634441,=2⟨−ηk ˆDk
L,0.3558912386706949,"−1∇F(wk), ˆDk(wk+1 −wk)⟩−∥wk+1 −wk∥2"
L,0.3564954682779456,"ˆ
Dk
= −2ηk⟨∇F(wk), wk+1 −wk⟩−∥wk+1 −wk∥2"
L,0.35709969788519635,"ˆ
Dk
= −2ηk⟨∇F(wk) −∇F(wk−1), wk+1 −wk⟩−2ηk⟨∇F(wk−1), wk+1 −wk⟩−"
L,0.3577039274924471,∥wk+1 −wk∥2
L,0.3583081570996979,"ˆ
Dk
=2ηk⟨∇F(wk) −∇F(wk−1), wk −wk+1⟩+ 2ηk⟨∇F(wk−1), wk −wk+1⟩−"
L,0.35891238670694864,∥wk+1 −wk∥2
L,0.3595166163141994,"ˆ
Dk.
(32)"
L,0.36012084592145016,"Now,
2ηk⟨∇F(wk) −∇F(wk−1), wk −wk+1⟩≤2ηk∥∇F(wk) −∇F(wk−1)∥∗"
L,0.3607250755287009,"ˆ
Dk∥wk −wk+1∥ˆ
Dk
≤∥wk −wk−1∥ˆ
Dk∥wk −wk+1∥ˆ
Dk ≤1"
L,0.3613293051359517,2∥wk −wk−1∥2
L,0.3619335347432024,"ˆ
Dk + 1"
L,0.36253776435045315,2∥wk −wk+1∥2
L,0.3631419939577039,"ˆ
Dk,
(33)"
L,0.3637462235649547,"where the ﬁrst inequality is due to Cauchy–Schwarz inequality, and the second inequality comes from"
L,0.36435045317220544,"the choice of learning rate such that ηk ≤
∥wk−wk−1∥ˆ
Dk
2∥∇F (wk)−∇F (wk−1)∥∗ ˆ
Dk"
L,0.3649546827794562,. By plugging (33) into (32) we
L,0.36555891238670696,obtain:
L,0.3661631419939577,"∥wk+1 −wk∥2 ˆ
Dk"
L,0.3667673716012085,"(32)
≤1"
L,0.36737160120845924,2∥wk −wk−1∥2
L,0.36797583081570995,"ˆ
Dk + 1"
L,0.3685800604229607,2∥wk −wk+1∥2
L,0.36918429003021147,"ˆ
Dk + 2ηk⟨∇F(wk−1), wk −wk+1⟩−"
L,0.36978851963746223,∥wk+1 −wk∥2
L,0.370392749244713,"ˆ
Dk.
(34)"
L,0.37099697885196375,"Now, we can summarize that"
L,0.3716012084592145,"∥wk+1 −w∗∥2 ˆ
Dk"
L,0.3722054380664653,"(31)
≤∥wk+1 −wk∥2"
L,0.37280966767371604,"ˆ
Dk + ∥wk −w∗∥2"
L,0.37341389728096674,"ˆ
Dk −2ηk(F(wk) −F(w∗)) −µηk∥wk −w∗∥2"
L,0.3740181268882175,"(34)
≤1"
L,0.37462235649546827,2∥wk −wk−1∥2
L,0.37522658610271903,"ˆ
Dk + 1"
L,0.3758308157099698,2∥wk −wk+1∥2
L,0.37643504531722055,"ˆ
Dk + 2ηk⟨∇F(wk−1), wk −wk+1⟩−"
L,0.3770392749244713,∥wk+1 −wk∥2
L,0.3776435045317221,"ˆ
Dk + ∥wk −w∗∥2"
L,0.37824773413897284,"ˆ
Dk −2ηk(F(wk) −F(w∗)) −µηk∥wk −w∗∥2 =1"
L,0.37885196374622354,2∥wk −wk−1∥2
L,0.3794561933534743,"ˆ
Dk −1"
L,0.38006042296072506,2∥wk −wk+1∥2
L,0.3806646525679758,"ˆ
Dk + 2ηk⟨∇F(wk−1), wk −wk+1⟩+"
L,0.3812688821752266,∥wk −w∗∥2
L,0.38187311178247735,"ˆ
Dk −2ηk(F(wk) −F(w∗)) −µηk∥wk −w∗∥2 =1"
L,0.3824773413897281,2∥wk −wk−1∥2
L,0.38308157099697887,"ˆ
Dk−1 + ∥wk −w∗∥2"
L,0.38368580060422963,"ˆ
Dk−1 −1"
L,0.38429003021148034,"2∥wk −wk+1∥2 ˆ
Dk+"
L,0.3848942598187311,"2ηk⟨∇F(wk−1), wk −wk+1⟩−2ηk(F(wk) −F(w∗))+
1
2∥wk −wk−1∥2"
L,0.38549848942598186,"ˆ
Dk−ˆ
Dk−1 + ∥wk −w∗∥2"
L,0.3861027190332326,"ˆ
Dk−ˆ
Dk−1 −µηk∥wk −w∗∥2.
(35)"
L,0.3867069486404834,"Next, let us bound ⟨∇F(wk−1), wk −wk+1⟩."
L,0.38731117824773414,"By the OASIS’s update rule, wk+1 = wk −ηk ˆDk−1∇F(wk), we have,"
L,0.3879154078549849,"⟨∇F(wk−1), wk −wk+1⟩=
1
ηk−1
⟨ˆDk−1(wk−1 −wk), wk −wk+1⟩"
L,0.38851963746223567,"=
1
ηk−1
⟨ˆDk−1(wk−1 −wk), ηk ˆDk"
L,0.38912386706948643,−1∇F(wk)⟩ = ηk
L,0.38972809667673713,"ηk−1
(wk−1 −wk)T ˆDk−1 ˆDk"
L,0.3903323262839879,−1∇F(wk) = ηk
L,0.39093655589123866,"ηk−1
(wk−1 −wk)T ∇F(wk)+"
L,0.3915407854984894,"ηk
ηk−1
(wk−1 −wk)T "
L,0.3921450151057402,ˆDk−1 ˆDk
L,0.39274924471299094,"−1 −I

∇F(wk).
(36)"
L,0.3933534743202417,Published as a conference paper at ICLR 2022
L,0.39395770392749246,The second term in the above equality can be bounded from above as follows:
L,0.3945619335347432,(wk−1 −wk)T 
L,0.39516616314199393,ˆDk−1 ˆDk
L,0.3957703927492447,"−1 −I

∇F(wk) ≤(1 −β2)2Γ"
L,0.39637462235649545,"α · ∥wk−1 −wk∥· ∥∇F(wk)∥,
(37)"
L,0.3969788519637462,where multiplier on the left is obtained via the bound on ∥· ∥∞norm of the diagonal matrix
L,0.397583081570997,ˆDk−1 ˆDk−1 −I:
L,0.39818731117824774,∥ˆDk−1 ˆDk
L,0.3987915407854985,"−1 −I∥∞= max
i "
L,0.39939577039274926,ˆDk−1 ˆDk
L,0.4,"−1 −I
 i"
L,0.40060422960725073,"= max
i
  ˆDk−1 −ˆDk "
L,0.4012084592145015,"i
  ˆDk −1"
L,0.40181268882175225,"i
 ≤(1 −β2)2Γ α ,"
L,0.402416918429003,"which also represents a bound on the operator norm of the same matrix difference. Next we can use
the Young’s inequality and ∇F(w∗) = 0 to get"
L,0.4030211480362538,(1 −β2)2Γ
L,0.40362537764350453,α · ∥wk−1 −wk∥· ∥∇F(wk)∥≤(1 −β2)Γ
L,0.4042296072507553,"α
 
∥wk−1 −wk∥2 + L2∥wk −w∗∥2
. (38)"
L,0.40483383685800606,"By the inequalities (72), (36), (37) and (38), and the deﬁnition θk =
ηk
ηk−1
we have:"
L,0.4054380664652568,∥wk+1 −w∗∥2
L,0.4060422960725076,"ˆ
Dk + 1"
L,0.4066465256797583,2∥wk+1 −wk∥2
L,0.40725075528700905,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
L,0.4078549848942598,≤∥wk −w∗∥2
L,0.40845921450151057,"ˆ
Dk−1 + 1"
L,0.40906344410876133,2∥wk −wk−1∥2
L,0.4096676737160121,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗)) +
1"
L,0.41027190332326285,2∥wk −wk−1∥2
L,0.4108761329305136,"ˆ
Dk−ˆ
Dk−1 + (1 −β2)2Γθkηk"
L,0.4114803625377644,"α
∥wk −wk−1∥2 −µηkθk∥wk −wk−1∥2
"
L,0.4120845921450151,"+

∥wk −w∗∥2"
L,0.41268882175226584,"ˆ
Dk−ˆ
Dk−1 + (1 −β2)2ΓL2θkηk"
L,0.4132930513595166,"α
∥wk −w∗∥2 −µηk∥wk −w∗∥2
"
L,0.41389728096676737,≤∥wk −w∗∥2
L,0.41450151057401813,"ˆ
Dk−1 + 1"
L,0.4151057401812689,2∥wk −wk−1∥2
L,0.41570996978851965,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))"
L,0.4163141993957704,"+

(1 −β2)Γ

1 + 2θkηk α"
L,0.4169184290030212,"
−µηkθk"
L,0.4175226586102719,"
∥wk −wk−1∥2"
L,0.41812688821752264,"+

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.4187311178247734,"
−µηk"
L,0.41933534743202416,"
∥wk −w∗∥2."
L,0.4199395770392749,"Theorem 4.9. Suppose that Assumptions 4.2, 4.4, and 4.3 hold and let w∗be the unique solution
for (1). Let {wk} be the iterates generated by Algorithm 1. Then, for all k ≥0 we have: If"
L,0.4205438066465257,"β2 ≥max{1 −
α4µ4"
L,0.42114803625377645,"4L2Γ2(α2µ2 + LΓ2), 1 −
α3µ3"
L,0.4217522658610272,4LΓ(2α2µ2 + L3Γ2)}
L,0.42235649546827797,"Ψk+1 ≤(1 −
α2"
L,0.4229607250755287,"2Γ2κ2 )Ψk,
(39) where"
L,0.42356495468277944,Ψk+1 = ∥wk+1 −w∗∥2
L,0.4241691842900302,"ˆ
Dk + 1"
L,0.42477341389728096,2∥wk+1 −wk∥2
L,0.4253776435045317,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))."
L,0.4259818731117825,Published as a conference paper at ICLR 2022
L,0.42658610271903324,"Proof. By Lemma A.3, we have:"
L,0.427190332326284,∥wk+1 −w∗∥2
L,0.42779456193353477,"ˆ
Dk + 1"
L,0.4283987915407855,2∥wk+1 −wk∥2
L,0.42900302114803623,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
L,0.429607250755287,≤∥wk −w∗∥2
L,0.43021148036253776,"ˆ
Dk−1 + 1"
L,0.4308157099697885,2∥wk −wk−1∥2
L,0.4314199395770393,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))"
L,0.43202416918429004,"+

(1 −β2)Γ

1 + 2θkηk α"
L,0.4326283987915408,"
−µηkθk"
L,0.43323262839879156,"
∥wk −wk−1∥2"
L,0.43383685800604227,"+

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.43444108761329303,"
−µηk"
L,0.4350453172205438,"
∥wk −w∗∥2."
L,0.43564954682779455,"Lemma 4.8 gives us ηk ∈
h α 2L, Γ 2µ"
L,0.4362537764350453,"i
, so we can choose large enough β2 ∈(0, 1) such that

(1 −β2)Γ

1 + 2θkηk α"
L,0.4368580060422961,"
−µηkθk"
L,0.43746223564954684,"
≤0,

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.4380664652567976,"
−µηk 
≤0."
L,0.43867069486404836,"In other words, we have the following bound for β2:"
L,0.43927492447129907,"β2 ≥max{1 −
α4µ4"
L,0.4398791540785498,"2L2Γ2(α2µ2 + LΓ2), 1 −
α3µ3"
L,0.4404833836858006,"2LΓ(2α2µ2 + L3Γ2)}.
(40)"
L,0.44108761329305135,"However, we can push it one step further, by requiring a stricter inequality to hold, to get a recursion
which can allows us to derive a linear convergence rate as follows"
L,0.4416918429003021,"
(1 −β2)Γ

1 + 2θkηk α"
L,0.4422960725075529,"
−µηkθk 
≤−1"
L,0.44290030211480363,"2µηkθk,

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.4435045317220544,"
−µηk 
≤−1 2µηk,"
L,0.44410876132930516,which requires a correspondingly stronger bound on β2:
L,0.4447129909365559,"β2 ≥max{1 −
α4µ4"
L,0.4453172205438066,"4L2Γ2(α2µ2 + LΓ2), 1 −
α3µ3"
L,0.4459214501510574,"4LΓ(2α2µ2 + L3Γ2)}.
(41)"
L,0.44652567975830815,"Combining this with the condition for ηk and deﬁnition for θk, we obtain"
L,0.4471299093655589,"
(1 −β2)Γ

1 + 2θkηk α"
L,0.44773413897280967,"
−µηkθk"
L,0.44833836858006043,"
∥wk −wk−1∥2 ≤−1"
L,0.4489425981873112,"2µηkθk∥wk −wk−1∥2 ≤−
α2"
L,0.44954682779456195,4Γ2κ2 ∥wk −wk−1∥2
L,0.4501510574018127,"ˆ
Dk−1,"
L,0.4507552870090634,"and

(1 −β2)Γ

2 + 2L2θkηk α"
L,0.4513595166163142,"
−µηk"
L,0.45196374622356494,"
∥wk −w∗∥2 ≤−1"
L,0.4525679758308157,2µηk∥wk −w∗∥2 ≤−α
L,0.45317220543806647,4Γκ∥wk −w∗∥2
L,0.45377643504531723,"ˆ
Dk−1."
L,0.454380664652568,where κ = L
L,0.45498489425981875,"µ . Using it to supplement the main statement of the lemma, we get"
L,0.4555891238670695,∥wk+1 −w∗∥2
L,0.4561933534743202,"ˆ
Dk + 1"
L,0.456797583081571,2∥wk+1 −wk∥2
L,0.45740181268882174,"ˆ
Dk + 2ηk(1 + θk)(F(wk) −F(w∗))"
L,0.4580060422960725,"≤(1 −
α
4Γκ)∥wk −w∗∥2"
L,0.45861027190332326,"ˆ
Dk−1 + 1"
L,0.459214501510574,"2(1 −
α2"
L,0.4598187311178248,2Γ2κ2 )∥wk −wk−1∥2
L,0.46042296072507555,"ˆ
Dk−1 + 2ηkθk(F(wk−1) −F(w∗))."
L,0.4610271903323263,Published as a conference paper at ICLR 2022
L,0.461631419939577,"Mirroring the result in (Malitsky & Mishchenko, 2020) we get a contraction in all terms, since for
function value differences we also can show"
L,0.4622356495468278,"ηkθk
ηk(1 + θk) = 1 −
ηk
ηk(1 + θk) ≤1 −
α
2Γκ."
L,0.46283987915407854,"A.7
PROOF OF THEOREM 4.11"
L,0.4634441087613293,"Theorem 4.11. Suppose that Assumptions 4.2, 4.4, and 4.3 hold, and let F ∗= F(w∗) where w∗is"
L,0.46404833836858006,"the unique minimizer. Let {wk} be the iterates generated by Algorithm 1, where 0 < ηk = η ≤α2"
L,0.4646525679758308,"LΓ,
and w0 is a starting point. Then, for all k ≥0 we have:"
L,0.4652567975830816,F(wk) −F ∗≤(1 −ηµ
L,0.46586102719033234,"Γ )k[F(w0) −F ∗].
(42)"
L,0.4664652567975831,Proof. By smoothness of F(.) we have:
L,0.4670694864048338,"F(wk+1) = F(wk −η ˆD−1
k ∇f(wk))"
L,0.4676737160120846,"≤F(wk) + ∇F(wk)T (−η ˆD−1
k ∇f(wk)) + L"
L,0.46827794561933533,"2 ∥η ˆD−1
k ∇f(wk)∥2"
L,0.4688821752265861,≤F(wk) −η
L,0.46948640483383686,Γ∥∇F(wk)∥2 + η2L
L,0.4700906344410876,2α2 ∥∇F(wk)∥2
L,0.4706948640483384,= F(wk) −η( 1 Γ −ηL
L,0.47129909365558914,"2α2 )∥∇F(wk)∥2
(43)"
L,0.4719033232628399,≤F(wk) −η 1
L,0.4725075528700906,"2Γ∥∇F(wk)∥2,
(44)"
L,0.47311178247734137,"where the ﬁrst inequality comes from Assumption 4.2, and the second inequality is due to Remark
4.10, and ﬁnally, the last inequality is by the choice of η. Since F(.) is strongly convex, we have
2µ(F(wk) −F ∗) ≤∥∇F(wk)∥2, and therefore,"
L,0.47371601208459213,F(wk+1) ≤F(wk) −ηµ
L,0.4743202416918429,Γ (F(wk) −F ∗).
L,0.47492447129909365,"Also, we have:
F(wk+1) −F ∗≤(1 −ηµ"
L,0.4755287009063444,Γ )(F(wk) −F ∗).
L,0.4761329305135952,"A.8
PROOF OF THEOREM 4.13"
L,0.47673716012084594,"Theorem 4.13. Suppose that Assumptions 4.3, 4.12 and 4.2 hold. Let {wk} be the iterates generated"
L,0.4773413897280967,"by Algorithm 1, where 0 < ηk = η ≤α2"
L,0.4779456193353474,"LΓ, and w0 is a starting point. Then, for all T > 1 we have:"
T,0.47854984894259817,"1
T T
X"
T,0.4791540785498489,"k=1
∥∇F(wk)∥2 ≤2Γ[F(w0) −ˆF] ηT"
T,0.4797583081570997,"T →∞
−−−−→0.
(45)"
T,0.48036253776435045,"Proof. By starting from (44), we have:"
T,0.4809667673716012,F(wk+1 ≤F(wk) −η
T,0.481570996978852,"2Γ∥∇F(wk)∥2.
(46)"
T,0.48217522658610273,By summing both sides of the above inequality from k = 0 to T −1 we have:
T,0.4827794561933535,"T −1
X"
T,0.48338368580060426,"k=0
(F(wk+1) −F(wk)) ≤−"
T,0.48398791540785496,"T −1
X k=0"
T,0.4845921450151057,"η
2Γ∥∇F(wk)∥2."
T,0.4851963746223565,Published as a conference paper at ICLR 2022
T,0.48580060422960725,"By simplifying the left-hand-side of the above inequality, we have"
T,0.486404833836858,"T −1
X"
T,0.48700906344410877,"k=0
[F(wk+1) −F(wk)] = F(wT ) −F(w0) ≥bF −F(w0),"
T,0.48761329305135953,"where the inequality is due to ˆF ≤F(wT ) (Assumption 4.12). Using the above, we have"
T,0.4882175226586103,"T −1
X"
T,0.48882175226586105,"k=0
∥∇F(wk)∥2 ≤2Γ[F(w0) −bF]"
T,0.48942598187311176,"η
.
(47)"
T,0.4900302114803625,"A.9
PROOF OF THEOREM 4.17"
T,0.4906344410876133,"Lemma A.4 (see Lemma 1 (Nguyen et al., 2018) or Lemma 2.4 (Gower et al., 2019)). Assume that
∀i the function Fi(w) is convex and L-smooth. Then, ∀w ∈Rd the following hold:"
T,0.49123867069486404,"EI[∥∇FI(w)∥2] ≤4L(F(w) −F(w∗)) + 2σ2.
(48)"
T,0.4918429003021148,"Theorem A.5. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let parameters α > 0,
η > 0, β2 ∈(0, 1] are chosen such that α < 2ΓL"
T,0.49244712990936557,"µ , η ≤
α
2L, β2 > 1 −
ηµα
2Γ(Γ−ηµ) > 0. Let {wk} be
the iterates generated by Algorithm 1, then, for all k ≥0,"
T,0.4930513595166163,E[∥wk −w∗∥2
T,0.4936555891238671,"ˆ
Dk] ≤(1 −c)k∥r0∥2"
T,0.49425981873111785,"ˆ
D0 + (1 + 2(1−β2)Γ"
T,0.49486404833836856,"α
) 2σ2η2"
T,0.4954682779456193,"αc ,
(49)"
T,0.4960725075528701,where c = ηµα−(Γ−ηµ)2(1−β2)Γ
T,0.49667673716012084,"Γα
∈(0, 1)."
T,0.4972809667673716,"Remark A.6. If we choose β2 := 1 −
ηµα
4Γ(Γ−ηµ) > 0 then"
T,0.49788519637462236,E[∥wk −w∗∥2
T,0.4984894259818731,"ˆ
Dk] ≤
 
1 −ηµ"
T,0.4990936555891239,"4Γ
k ∥w0 −w∗∥2"
T,0.49969788519637465,"ˆ
D0 + 4 2Γ−ηµ"
T,0.5003021148036254,"(Γ−ηµ)α
Γ
µσ2η."
T,0.5009063444108761,Proof. In order to bound ∥rk+1∥2
T,0.5015105740181269,"ˆ
Dk+1 by ∥rk+1∥2"
T,0.5021148036253776,"ˆ
Dk we will use that"
T,0.5027190332326285,0 ≺ˆDk+1 = ˆDk + ˆDk+1 −ˆDk ⪯ˆDk + ∥ˆDk+1 −ˆDk∥∞I ⪯ˆDk + ∥ˆDk+1 −ˆDk∥∞I
T,0.5033232628398792,⪯ˆDk + ∥Dk+1 −Dk∥∞I ⪯ˆDk + 2(1 −β2)ΓI
T,0.5039274924471299,⪯ˆDk + 2(1 −β2)Γ 1
T,0.5045317220543807,α ˆDk ⪯(1 + 2(1−β2)Γ
T,0.5051359516616314,"α
) ˆDk.
(50) Then"
T,0.5057401812688822,∥rk+1∥2
T,0.5063444108761329,"ˆ
Dk+1"
T,0.5069486404833837,"(50)
≤(1 + 2(1−β2)Γ"
T,0.5075528700906344,"α
)∥rk+1∥2"
T,0.5081570996978853,"ˆ
Dk = (1 + 2(1−β2)Γ"
T,0.508761329305136,"α
)∥rk −η ˆDk"
T,0.5093655589123867,−1∇FIk(wk)∥2
T,0.5099697885196375,"ˆ
Dk
= (1 + 2(1−β2)Γ"
T,0.5105740181268882,"α
)∥rk∥2"
T,0.511178247734139,"ˆ
Dk −2η(1 + 2(1−β2)Γ"
T,0.5117824773413897,"α
)⟨rk, ∇FIk(wk)⟩"
T,0.5123867069486405,+ (1 + 2(1−β2)Γ
T,0.5129909365558912,"α
)η2 
∥∇FIk(wk)∥∗ ˆ
Dk 2"
T,0.513595166163142,≤(1 + 2(1−β2)Γ
T,0.5141993957703928,"α
)∥rk∥2"
T,0.5148036253776435,"ˆ
Dk −2η(1 + 2(1−β2)Γ"
T,0.5154078549848943,"α
)⟨rk, ∇FIk(wk)⟩"
T,0.516012084592145,"+ (1 + 2(1−β2)Γ α
)η2"
T,0.5166163141993958,"α ∥∇FIk(wk)∥2.
(51)"
T,0.5172205438066465,Published as a conference paper at ICLR 2022
T,0.5178247734138973,"Now, taking an expectation with respect to Ik conditioned on the past, we obtain"
T,0.518429003021148,E[∥rk+1∥2
T,0.5190332326283988,"ˆ
Dk+1]
(51)
≤(1 + 2(1−β2)Γ"
T,0.5196374622356495,"α
)∥rk∥2"
T,0.5202416918429003,"ˆ
Dk + 2η(1 + 2(1−β2)Γ"
T,0.5208459214501511,"α
)

F ∗−F(wk) −µ"
T,0.5214501510574018,2 ∥wk −w∗∥2
T,0.5220543806646526,"+ (1 + 2(1−β2)Γ α
)η2"
T,0.5226586102719033,"α
 
4L(F(wk) −F ∗) + 2σ2"
T,0.5232628398791541,≤(1 + 2(1−β2)Γ
T,0.5238670694864048,"α
)∥rk∥2"
T,0.5244712990936556,"ˆ
Dk + 2η(1 + 2(1−β2)Γ"
T,0.5250755287009063,"α
)

F ∗−F(wk) −µ"
T,0.525679758308157,"2Γ∥rk∥2 ˆ
Dk "
T,0.5262839879154079,"+ (1 + 2(1−β2)Γ α
)η2"
T,0.5268882175226586,"α
 
4L(F(wk) −F ∗) + 2σ2"
T,0.5274924471299094,= (1 + 2(1−β2)Γ
T,0.5280966767371601,"α
)

1 −2η µ 2Γ"
T,0.5287009063444109,"
∥rk∥2 ˆ
Dk"
T,0.5293051359516616,+ (1 + 2(1−β2)Γ
T,0.5299093655589124,"α
)

4L η2"
T,0.5305135951661631,"α −2η

((F(wk) −F ∗)) + (1 + 2(1−β2)Γ"
T,0.5311178247734138,"α
) η2 α 2σ2"
T,0.5317220543806647,≤(1 + 2(1−β2)Γ
T,0.5323262839879154,"α
)

1 −η µ Γ"
T,0.5329305135951662,"
∥rk∥2"
T,0.5335347432024169,"ˆ
Dk + (1 + 2(1−β2)Γ"
T,0.5341389728096677,"α
) η2"
T,0.5347432024169184,"α 2σ2,
(52)"
T,0.5353474320241692,where we used the fact that 2L η
T,0.5359516616314199,"α −1 ≤0 →η ≤
α
2L. In order to achieve a convergence we need to
have
(1 + 2(1−β2)Γ"
T,0.5365558912386706,"α
)

1 −µη Γ"
T,0.5371601208459215,"
=: 1 −c < 1."
T,0.5377643504531722,We have
T,0.538368580060423,(1 + 2(1−β2)Γ
T,0.5389728096676737,"α
)

1 −ηµ Γ"
T,0.5395770392749245,"
= 1 −ηµα −(Γ −ηµ)2(1 −β2)Γ"
T,0.5401812688821752,"Γα
|
{z
}
c
Now, we need to choose β2 such that"
T,0.540785498489426,"1 > β2 > 1 −
ηµα
2Γ(Γ −ηµ) > 0."
T,0.5413897280966767,With such a choice we can conclude that
T,0.5419939577039274,E[∥rk∥2
T,0.5425981873111783,"ˆ
Dk]
(52)
≤(1 −c)∥rk−1∥2"
T,0.543202416918429,"ˆ
Dk−1 + (1 + 2(1−β2)Γ"
T,0.5438066465256798,"α
) η2"
T,0.5444108761329305,"α 2σ2,"
T,0.5450151057401813,≤(1 −c)k∥r0∥2
T,0.545619335347432,"ˆ
D0 + k−1
X"
T,0.5462235649546828,"i=0
(1 −c)i(1 + 2(1−β2)Γ"
T,0.5468277945619335,"α
) η2"
T,0.5474320241691842,"α 2σ2,"
T,0.5480362537764351,≤(1 −c)k∥r0∥2
T,0.5486404833836858,"ˆ
D0 + ∞
X"
T,0.5492447129909366,"i=0
(1 −c)i(1 + 2(1−β2)Γ"
T,0.5498489425981873,"α
) η2"
T,0.5504531722054381,"α 2σ2,"
T,0.5510574018126888,≤(1 −c)k∥r0∥2
T,0.5516616314199396,"ˆ
D0 + (1 + 2(1−β2)Γ"
T,0.5522658610271903,"α
) 2σ2η2 αc ."
T,0.552870090634441,"Theorem 4.17. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let {wk} be the iterates
generated by Algorithm 1 with ηk = η ∈(0, α2µ"
T,0.5534743202416919,"ΓL2 ), then, for all k ≥0,"
T,0.5540785498489426,E[F(wk) −F ∗] ≤(1 −c)k (F(w0) −F ∗) + η2Lσ2
T,0.5546827794561934,"cα2 ,
(53)"
T,0.5552870090634441,where c = 2ηµ
T,0.5558912386706949,Γ −2η2L2
T,0.5564954682779456,"α2
∈(0, 1). Moreover, if ηk = η ∈(0, α2µ"
T,0.5570996978851964,2ΓL2 ) then
T,0.5577039274924471,"E[F(wk) −F ∗] ≤

1 −ηµ Γ"
T,0.5583081570996978,"k
(F(w0) −F ∗) + ηΓLσ2"
T,0.5589123867069486,"α2µ .
(54)"
T,0.5595166163141994,"Proof. First, we will upper-bound the F(wk+1) using smoothness of F(w)"
T,0.5601208459214502,F(wk+1) = F(wk −η ˆDk
T,0.5607250755287009,−1∇FIk(wk))
T,0.5613293051359517,≤F(wk) + ∇F(wk)T (−η ˆDk
T,0.5619335347432024,−1∇FIk(wk)) + L
T,0.5625377643504532,2 ∥η ˆDk
T,0.5631419939577039,−1∇FIk(wk)∥2
T,0.5637462235649546,≤F(wk) −η∇F(wk)T ˆDk
T,0.5643504531722054,−1∇FIk(wk) + η2L
T,0.5649546827794562,"2α2 ∥∇FIk(wk)∥2,"
T,0.565558912386707,Published as a conference paper at ICLR 2022
T,0.5661631419939577,"where the ﬁrst inequality is because of Assumptions 4.2 and 4.4, and the second inequality is due to
Remark 4.10. By taking the expectation over the sample Ik, we have"
T,0.5667673716012085,EIk[F(wk+1)] ≤F(wk) −ηEIk[∇F(wk)T ˆDk
T,0.5673716012084592,−1∇FIk(wk)] + η2L
T,0.56797583081571,2α2 EIk[∥∇FIk(wk)∥2]
T,0.5685800604229607,= F(wk) −η∇F(wk)T ˆDk
T,0.5691842900302114,−1∇F(wk) + η2L
T,0.5697885196374622,2α2 EIk[∥∇FIk(wk)∥2]
T,0.570392749244713,≤F(wk) −η
T,0.5709969788519638,Γ∥∇F(wk)∥2 + η2L
T,0.5716012084592145,"2α2
 
4L(F(wk) −F ∗) + 2σ2"
T,0.5722054380664653,≤F(wk) + η
T,0.572809667673716,Γ2µ (F ∗−F(wk)) + η2L
T,0.5734138972809668,"2α2
 
4L(F(wk) −F ∗) + 2σ2
(55)"
T,0.5740181268882175,"Subtracting F ∗from both sides, we obtain"
T,0.5746223564954682,"EIk[F(wk+1) −F ∗]
(55)
≤

1 −η"
T,0.575226586102719,Γ2µ + η2L
T,0.5758308157099697,"2α2 4L

(F(wk) −F ∗) + η2Lσ2 α2 ≤ "
T,0.5764350453172206,"

1 −
2ηµ"
T,0.5770392749244713,"Γ
−2η2L2 α2 "
T,0.5776435045317221,"|
{z
}
c "
T,0.5782477341389728,"

(F(wk) −F ∗) + η2Lσ2"
T,0.5788519637462236,"α2
.
(56)"
T,0.5794561933534743,"By taking the total expectation over all batches I0, I1, I2,... and all history starting with w0, we have"
T,0.5800604229607251,"E[F(wk) −F ∗]
(56)
≤(1 −c)k (F(w0) −F ∗) + k−1
X"
T,0.5806646525679758,"i=0
(1 −c)i η2Lσ2 α2"
T,0.5812688821752265,"≤(1 −c)k (F(w0) −F ∗) + ∞
X"
T,0.5818731117824774,"i=0
(1 −c)i η2Lσ2 α2"
T,0.5824773413897281,= (1 −c)k (F(w0) −F ∗) + η2Lσ2 cα2
T,0.5830815709969789,"and the (53) is obtained. Now, if η ≤
α2µ
2ΓL2 then"
T,0.5836858006042296,c = 2ηµ
T,0.5842900302114804,"Γ
−2η2L2"
T,0.5848942598187311,"α2
≥2η
µ"
T,0.5854984894259819,Γ −α2µ
T,0.5861027190332326,"2ΓL2
L2 α2"
T,0.5867069486404833,"
= 2η
µ Γ −µ 2Γ"
T,0.5873111782477342,"
= ηµ"
T,0.5879154078549849,"Γ
and (54) follows."
T,0.5885196374622357,"A.10
PROOF OF THEOREM 4.18"
T,0.5891238670694864,"Theorem 4.18. Suppose that Assumptions 4.3, 4.12, 4.2, 4.14 and 4.16 hold. Let {wk} be the iterates"
T,0.5897280966767372,"generated by Algorithm 1, where 0 < ηk = η ≤η2"
T,0.5903323262839879,"LΓ, and w0 is the starting point. Then, for all
k ≥0, E ""
1
T"
T,0.5909365558912387,"T −1
X"
T,0.5915407854984894,"k=0
∥∇F(wk)∥2
#"
T,0.5921450151057401,≤2Γ[F(w0) −bF]
T,0.592749244712991,"ηT
+ ηΓγ2L"
T,0.5933534743202417,"α2
T →∞
−−−−→ηΓγ2L α2
."
T,0.5939577039274925,Proof. We have that
T,0.5945619335347432,F(wk+1) = F(wk −η ˆDk
T,0.595166163141994,−1∇FIk(wk))
T,0.5957703927492447,≤F(wk) + ∇F(wk)T (−η ˆDk
T,0.5963746223564955,−1∇FIk(wk)) + L
T,0.5969788519637462,2 ∥η ˆDk
T,0.5975830815709969,−1∇FIk(wk)∥2
T,0.5981873111782477,≤F(wk) −η∇F(wk)T ˆDk
T,0.5987915407854985,−1∇FIk(wk) + η2L
T,0.5993957703927493,"2α2 ∥∇FIk(wk)∥2,"
T,0.6,Published as a conference paper at ICLR 2022
T,0.6006042296072508,"where the ﬁrst inequality is because of Assumptions 4.2 and 4.4, and the second inequality is due to
Remark 4.10. By taking the expectation over the sample Ik, we have"
T,0.6012084592145015,EIk[F(wk+1)] ≤F(wk) −ηEIk[∇F(wk)T ˆDk
T,0.6018126888217523,−1∇FIk(wk)] + η2L
T,0.602416918429003,2α2 EIk[∥∇FIk(wk)∥2]
T,0.6030211480362537,= F(wk) −η∇F(wk)T ˆDk
T,0.6036253776435045,−1∇F(wk) + η2L
T,0.6042296072507553,2α2 EIk[∥∇FIk(wk)∥2]
T,0.6048338368580061,"≤F(wk) −η
 1 Γ −ηL 2α2"
T,0.6054380664652568,"
∥∇F(wk)∥2 + η2γ2L"
T,0.6060422960725076,"2α2
(57)"
T,0.6066465256797583,≤F(wk) −η
T,0.6072507552870091,2Γ∥∇F(wk)∥2 + η2γ2L
T,0.6078549848942598,"2α2 ,
(58)"
T,0.6084592145015105,"where the second inequality is due to Remark 4.10 and Assumption 4.14, and the third inequality is
due to the choice of the step length."
T,0.6090634441087613,"By inequality (58) and taking the total expectation over all batches I0, I1, I2,... and all history
starting with w0, we have"
T,0.609667673716012,E[F(wk+1) −F(wk)] ≤−η
T,0.6102719033232629,2ΓE[∥∇F(wk)∥2] + η2γ2L 2α2 .
T,0.6108761329305136,"By summing both sides of the above inequality from k = 0 to T −1,"
T,0.6114803625377644,"T −1
X"
T,0.6120845921450151,"k=0
E[F(wk+1) −F(wk)] ≤−η 2Γ"
T,0.6126888217522659,"T −1
X"
T,0.6132930513595166,"k=0
E[∥∇F(wk)∥2] + η2γ2LT 2α2 = −η 2ΓE"
T,0.6138972809667673,"""T −1
X"
T,0.6145015105740181,"k=0
∥∇F(wk)∥2
#"
T,0.6151057401812688,"+ η2γ2LT 2α2
."
T,0.6157099697885197,"By simplifying the left-hand-side of the above inequality, we have"
T,0.6163141993957704,"T −1
X"
T,0.6169184290030212,"k=0
E [F(wk+1) −F(wk)] = E[F(wT )] −F(w0) ≥bF −F(w0),"
T,0.6175226586102719,"where the inequality is due to ˆF ≤F(wT ) (Assumption 4.12). Using the above, we have E"
T,0.6181268882175227,"""T −1
X"
T,0.6187311178247734,"k=0
∥∇F(wk)∥2
#"
T,0.6193353474320241,≤2Γ[F(w0) −bF]
T,0.6199395770392749,"η
+ ηΓγ2LT α2
."
T,0.6205438066465256,"A.11
PROOFS WITH LINE SEARCH"
T,0.6211480362537765,"Given the current iterate wk, the steplength is chosen to satisfy the following sufﬁcient decrease
condition
F(wk + ηkpk) ≤F(wk) −c1ηk∇F(wk)T ˆDk"
T,0.6217522658610272,"−1∇F(wk),
(59)"
T,0.622356495468278,"where pk = −ˆDk−1∇F(wk) and c1 ∈(0, 1). The mechanism works as follows. Given an initial
steplength (say ηk = 1), the function is evaluated at the trial point wk + αkpk and condition (59) is
checked. If the trial point satisﬁes (59), then the step is accepted. If the trial point does not satisfy
(59), the steplength is reduced (e.g., ηk = τηk for τ ∈(0, 1)). This process is repeated until a
steplength that satisﬁes (59) is found."
T,0.6229607250755287,"Algorithm 2 Backtracking Armijo Linesearch (Nocedal & Wright, 2006)
Input: wk, pk"
T,0.6235649546827795,"1: Select ηinitial, c1 ∈(0, 1) and τ ∈(0, 1)
2: η0 = ηinitial, j = 0
3: while F(wk + ηkpk) > F(wk) + c1ηk∇F(wk)T pk do
4:
Set ηj+1 = τηj"
T,0.6241691842900302,"5:
Set j = j + 1
6: end while
Output: ηk = ηj"
T,0.6247734138972809,Published as a conference paper at ICLR 2022
T,0.6253776435045317,"By following from the study (Berahas et al., 2019), we have the following theorems."
T,0.6259818731117824,"A.11.1
DETERMINITIC REGIME - STRONGLY CONVEX"
T,0.6265861027190333,"Theorem A.7. Suppose that Assumptions 4.3 and 4.2 and 4.4 hold. Let {wk} be the iterates
generated by Algorithm 1, where ηk is the maximum value in {τ −j : j = 0, 1, . . . } satisfying (59)
with 0 < c1 < 1, and w0 is the starting point. Then for all k ≥0,"
T,0.627190332326284,"F(wk) −F ⋆≤

1 −4µα2c1(1 −c1)τ Γ2L"
T,0.6277945619335348,"k
[F(w0) −F ⋆] ."
T,0.6283987915407855,Proof. Starting with (43) we have
T,0.6290030211480363,F(wk −ηk ˆDk
T,0.629607250755287,−1∇F(wk)) ≤F(wk) −ηk  1
T,0.6302114803625377,"Γ −ηk
L
2α2"
T,0.6308157099697885,"
∥∇F(wk)∥2.
(60)"
T,0.6314199395770392,"From the Armijo backtracking condition (59), we have"
T,0.63202416918429,F(wk −ηk ˆDk
T,0.6326283987915408,−1∇F(wk)) ≤F(wk) −c1ηk∇F(wk)T ˆDk
T,0.6332326283987916,−1∇F(wk)
T,0.6338368580060423,≤F(wk) −c1ηk
T,0.6344410876132931,"Γ ∥∇F(wk)∥2.
(61)"
T,0.6350453172205438,"Looking at (60) and (61), it is clear that the Armijo condition is satisﬁed when"
T,0.6356495468277945,ηk ≤2α2(1 −c1)
T,0.6362537764350453,"ΓL
.
(62)"
T,0.636858006042296,"Thus, any ηk that satisﬁes (62) is guaranteed to satisfy the Armijo condition (59). Since we ﬁnd ηk
using a constant backtracking factor of τ < 1, we have that"
T,0.6374622356495468,ηk ≥2α2(1 −c1)τ
T,0.6380664652567976,"ΓL
.
(63)"
T,0.6386706948640484,"Therefore, from (60) and by (62) and (63) we have"
T,0.6392749244712991,F(wk+1) ≤F(wk) −ηk  1
T,0.6398791540785499,Γ −ηkL 2α2
T,0.6404833836858006,"
∥∇F(wk)∥2"
T,0.6410876132930513,≤F(wk) −ηkc1
T,0.6416918429003021,Γ ∥∇F(wk)∥2
T,0.6422960725075528,≤F(wk) −2α2c1(1 −c1)τ
T,0.6429003021148036,"Γ2L
∥∇F(wk)∥2.
(64)"
T,0.6435045317220544,"By strong convexity, we have 2µ(F(w) −F ⋆) ≤∥∇F(w)∥2, and thus"
T,0.6441087613293052,F(wk+1) ≤F(wk) −4µα2c1(1 −c1)τ
T,0.6447129909365559,"Γ2L
(F(w) −F ⋆).
(65)"
T,0.6453172205438067,"Subtracting F ⋆from both sides, and applying (65) recursively yields the desired result."
T,0.6459214501510574,"A.11.2
DETERMINISTIC REGIME - NONCONVEX"
T,0.6465256797583081,"Theorem A.8. Suppose that Assumptions 4.3, 4.12 and 4.2 hold. Let {wk} be the iterates generated
by Algorithm 1, where ηk is the maximum value in {τ −j : j = 0, 1, . . . } satisfying (59) with
0 < c1 < 1, and where w0 is the starting point. Then,
lim
k→∞∥∇F(wk)∥= 0,
(66)"
T,0.6471299093655589,"and, moreover, for any T > 1,"
T,0.6477341389728096,"1
T"
T,0.6483383685800604,"T −1
X"
T,0.6489425981873111,"k=0
∥∇F(wk)∥2 ≤Γ2L[F(w0) −bF]"
T,0.649546827794562,2α2c1(1 −c1)τT
T,0.6501510574018127,"τ→∞
−−−−→0."
T,0.6507552870090635,"Proof. We start with (64)
F(wk+1) ≤F(wk) −2α2c1(1 −c1)τ"
T,0.6513595166163142,"Γ2L
∥∇F(wk)∥2."
T,0.6519637462235649,"Summing both sides of the above inequality from k = 0 to T −1,"
T,0.6525679758308157,"T −1
X"
T,0.6531722054380664,"k=0
(F(wk+1) −F(wk)) ≤−"
T,0.6537764350453172,"T −1
X k=0"
T,0.654380664652568,2α2c1(1 −c1)τ
T,0.6549848942598188,"Γ2L
∥∇F(wk)∥2."
T,0.6555891238670695,Published as a conference paper at ICLR 2022
T,0.6561933534743203,"The left-hand-side of the above inequality is a telescopic sum and thus,"
T,0.656797583081571,"T −1
X"
T,0.6574018126888218,"k=0
[F(wk+1) −F(wk)] = F(wT ) −F(w0) ≥bF −F(w0),"
T,0.6580060422960725,"where the inequality is due to ˆF ≤F(wT ) (Assumption 4.12). Using the above, we have"
T,0.6586102719033232,"T −1
X"
T,0.659214501510574,"k=0
∥∇F(wk)∥2 ≤Γ2L[F(w0) −bF]"
T,0.6598187311178247,"2α2c1(1 −c1)τ .
(67)"
T,0.6604229607250756,"Taking limits we obtain,"
T,0.6610271903323263,"lim
τ→∞ τ−1
X"
T,0.6616314199395771,"k=0
∥∇F(wk)∥2 < ∞,"
T,0.6622356495468278,which implies (66). Dividing (67) by T we conclude
T,0.6628398791540786,"1
T"
T,0.6634441087613293,"T −1
X"
T,0.66404833836858,"k=0
∥∇F(wk)∥2 ≤Γ2L[F(w0) −bF]"
T,0.6646525679758308,2α2c1(1 −c1)τT .
T,0.6652567975830815,"A.12
PROOFS WITH DECAYING STEP-SIZE"
T,0.6658610271903324,"In this section, we provide the analysis with decaying stepsize for both strongly convex and nonconvex
cases in the stochastic regime. The proofs for the following theorems follows from the proofs in
Theorems 4.17 and 4.18, and Theorems 4.7 and 4.9 in Bottou et al. (2018)."
T,0.6664652567975831,"A.12.1
STOCHASTIC REGIME – STRONGLY CONVEX"
T,0.6670694864048339,"Theorem A.9. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold., and let F ⋆= F(w⋆),
where w⋆is the minimizer of F. Let {wk} be the iterates generated by Algorithm 1, where ηk is a
sequence of stepsize such that, for all k ≥0,"
T,0.6676737160120846,"ηk =
φ
ζ + k
for some φ > Γ"
T,0.6682779456193354,µ and ζ > 0 such that η0 ≤α2µ
T,0.6688821752265861,"2ΓL2
(68)"
T,0.6694864048338368,"Then, for all k ≥0, the expected optimality gap satisﬁes
E[F(wk) −F ⋆] ≤
ν
ζ + k
(69) where"
T,0.6700906344410876,"ν := max
n
φ2Lσ2 (φµ"
T,0.6706948640483383,"Γ −1)α2
, ζ(F(w0) −F ⋆)
o
(70)"
T,0.6712990936555892,"Proof. By taking total expectation from (56), and replacing η with ηk we have:"
T,0.6719033232628399,E[F(wk+1) −F ∗] ≤ 
T,0.6725075528700907,"

1 −
2ηkµ"
T,0.6731117824773414,"Γ
−2η2
kL2 α2 "
T,0.6737160120845922,"|
{z
}
c "
T,0.6743202416918429,"

E[F(wk) −F ∗] + η2
kLσ2"
T,0.6749244712990936,"α2
.
(71)"
T,0.6755287009063444,"By the assumption η0 ≤
α2µ
2ΓL2 , and the way that ηk is designed, i.e., in a decaying way, we have"
T,0.6761329305135951,"ηk < η0 ≤
α2µ
2ΓL2 , therefore"
T,0.676737160120846,c = 2ηkµ
T,0.6773413897280967,"Γ
−2η2
kL2"
T,0.6779456193353475,"α2
≥2ηk µ"
T,0.6785498489425982,Γ −α2µ
T,0.679154078549849,"2ΓL2
L2 α2"
T,0.6797583081570997,"
= 2ηk
µ Γ −µ 2Γ"
T,0.6803625377643504,"
= ηkµ Γ ."
T,0.6809667673716012,"Thus, we have:"
T,0.6815709969788519,"E[F(wk+1) −F ∗] ≤

1 −ηkµ Γ"
T,0.6821752265861027,"
E[F(wk) −F ∗] + η2
kLσ2"
T,0.6827794561933535,"α2
.
(72)"
T,0.6833836858006043,Published as a conference paper at ICLR 2022
T,0.683987915407855,"We do the rest of the analysis of (69) with induction. First, for k = 0, (69) holds by the deﬁnition of
ν. Next, assuming (69) holds for k ≥0, by (72) we have:"
T,0.6845921450151058,E[F(wk+1) −F ⋆] ≤ 
T,0.6851963746223565,"
1 − φµ Γ ˆk  
ν"
T,0.6858006042296072,"ˆk
+ φ2Lσ2 ˆk2α2 = "
T,0.686404833836858,"

ˆk −φµ Γ
ˆk2 "
T,0.6870090634441087,"
ν + φ2Lσ2 ˆk2α2 = ˆk −1 ˆk2 ! ν −  
 φµ Γ −1 ˆk2 "
T,0.6876132930513595,"
ν + φ2Lσ2 ˆk2α2"
T,0.6882175226586102,"|
{z
}
nonpositive by deﬁnition of ν"
T,0.6888217522658611,"≤
ν
ˆk + 1
(73)"
T,0.6894259818731118,"A.12.2
STOCHASTIC REGIME – NONCONVEX"
T,0.6900302114803626,"Theorem A.10. Suppose that Assumptions 4.3, 4.12, 4.2, 4.14 and 4.16 hold. Let {wk} be the
iterates generated by Algorithm 1, where ηk is a steplength sequence satisfying
∞
X"
T,0.6906344410876133,"k=0
ηk = ∞
and ∞
X"
T,0.691238670694864,"k=0
η2
k < ∞.
(74)"
T,0.6918429003021148,"and w0 is the starting point. Then, with AT := PT −1
k=0 ηk,"
T,0.6924471299093655,"lim
T →∞E[PT −1
k=0 ηk∥∇F(wk)∥2] < ∞,
(75)"
T,0.6930513595166163,and therefore E[ 1 AT
T,0.693655589123867,"PT −1
k=0 ηk∥∇F(wk)∥2]
T →∞
−−−−→0.
(76)"
T,0.6942598187311179,Proof. The steplength sequence {ηk} goes to zero due to the second condition in (74). meaning that
T,0.6948640483383686,"w.l.o.g., we may assume that ηk ≤2α2"
T,0.6954682779456194,"2ΓL for all k ≥0. By starting from and taking total expectation
from (57), we have"
T,0.6960725075528701,E[F(wk+1)] −E[F(wk)] ≤−ηk  1
T,0.6966767371601208,Γ −ηkL 2α2
T,0.6972809667673716,"
E[∥∇F(wk)∥2] + η2
kγ2L 2α2 ≤−ηk"
T,0.6978851963746223,"2ΓE[∥∇F(wk)∥2] + η2
kγ2L 2α2 ."
T,0.6984894259818731,"Summing both sides of this inequality for k ∈{0, . . . , T −1} gives:"
T,0.6990936555891238,bF −E[F(w0)] ≤E[F(wT )] −E[F(w0)] ≤−1
T,0.6996978851963747,"2Γ
PT −1
k=0 ηkE[∥∇F(wk)∥2] + γ2L"
T,0.7003021148036254,"2α2
PT −1
k=0 η2
k. (77)"
T,0.7009063444108762,"Multiplying by 2Γ, and rearranging we get
PT −1
k=0 ηkE[∥∇F(wk)∥2] ≤2Γ(E[F(w0)] −bF) + Γγ2L"
T,0.7015105740181269,"α2
PT −1
k=0 η2
k.
(78)"
T,0.7021148036253776,"The second condition in (74) guarantees that the right-hand side of the above inequality converges to
a ﬁnite limit when T increases, therefore, it implies (75). By the ﬁrst condition in (74), we conclude
that AT →∞as T →∞, which results in (76)."
T,0.7027190332326284,Published as a conference paper at ICLR 2022
T,0.7033232628398791,"A.13
CONVERGENCE BOUNDS INTERPRETATION"
T,0.7039274924471299,"In this section, we provide the interpretation behind the convergence bounds in our theoretical results."
T,0.7045317220543806,• In Theorem 4.6 there is the term (1 −β2)ΓQk
T,0.7051359516616315,"k , which essentially highlights that the
parameter β2 controls the size of the neighborhood of optimal solution(s). In other words, if
β2 is very close to 1 (which is the case for our algorithm), the convergence neighborhood
would be very small. Meaning that, with the assumption regarding bounded iterates, our
algorithm converges to a very small neighborhood of optimal solution as β2 is very close to
1 (while in SGD the step-size controls the size of this neighborhood and accordingly, the
step-size needs to go zero in order to converge to the small neighborhood of the optimal
solution(s)). Note this convergence guarantee is for smooth and convex case with adaptive
learning rate."
T,0.7057401812688822,"• Theorem 4.9 provides a linear convergence rate for the smooth and strongly convex case
with adaptive learning rate. The analysis behind this theorem is based on the Lyapunov
energy that decreases linearly."
T,0.706344410876133,"• Remark 4.10 guarantees that the OASIS preconditioning matrix is positive deﬁnite and its
eigenvalues are uniformly bounded above and below."
T,0.7069486404833837,"• Theorems 4.11 and 4.13 provides linear and sublinear rate to the stationary point(s) for
deterministinc strongly convex and nonconvex cases, respectively. The learning rate is ﬁxed
in both mentioned cases, and the convergence rates are similar to those of limited-memory
quasi-Newton approaches (e.g., L-BFGS)."
T,0.7075528700906344,"• Theorems 4.17 and 4.18 provides linear and sublinear rate to the neighborhood of sta-
tionary point(s) (in expectation) for deterministinc strongly convex and nonconvex cases ,
respectively. Please note that we provided another convergence guarantee for the stochastic
strongly convex case (please see Theorem A.5). Again, as the deterministic cases, the
provided convergence guarantees completely match with those of the classical quasi-Newton
methods (such as L-BFGS); the learning rate is ﬁxed in the latter cases. Please note that
these methods (such as OASIS and classical limited-memory quasi-Newton methods) in
theory are not better than GD-type methods. In practice, however, they have shown their
strength."
T,0.7081570996978852,"• Theorems A.9 and A.10 provides convergence guarantees for the stochastic setting with
decaying learning rate. The mentioned theorems provides the convergence guarantees to the
stationary point(s) (in expectation) for the stochastic strongly convex and nonconvex cases."
T,0.7087613293051359,"• Theorems A.7 and A.8 provides linear and sublinear convergence rates to the stationary
point(s) for the deterministic strongly convex and nonconvex cases with utilizing linesearch."
T,0.7093655589123867,"B
ADDITIONAL ALGORITHM DETAILS"
T,0.7099697885196374,"B.1
RELATED WORK"
T,0.7105740181268883,"As was mentioned in Section 2, we follow the generic iterate update:"
T,0.711178247734139,wk+1 = wk −ηk ˆDk −1mk.
T,0.7117824773413898,Table 3 summarizes the methodologies discussed in Section 2.
T,0.7123867069486405,"B.2
STOCHASTIC OASIS"
T,0.7129909365558912,"Here we describe a stochastic variant of OASIS in more detail. As mentioned in Section 3, to estimate
gradient and Hessian diagonal, the choices of sets Ik, Jk ⊂[n], are independent and correspond to
only a fraction of data. This change results in Algorithm 3."
T,0.713595166163142,Published as a conference paper at ICLR 2022
T,0.7141993957703927,Table 3: Summary of Algorithms Discussed in Section 2
T,0.7148036253776435,"Algorithm
mk
ˆDk
SGD (Robbins & Monro, 1951)
β1mt−1 + (1 −β1)gk
1"
T,0.7154078549848942,"Adagrad (Duchi et al., 2011)
gk
qPk
i=1 diag(gi ⊙gi)"
T,0.716012084592145,"RMSProp (Tieleman & Hinton, 2012)
gk
q"
T,0.7166163141993958,β2 ˆDk−12 + (1 −β2)diag(gk ⊙gk)
T,0.7172205438066466,"Adam (Kingma & Ba, 2014)
(1 −β1) Pk
i=1 βk−i
1
gi
1 −βk
1 s"
T,0.7178247734138973,"(1 −β2) Pk
i=1 βk−i
2
diag(gi ⊙gi)
1 −βk
2"
T,0.718429003021148,"AdaHessian (Yao et al., 2020)
(1 −β1) Pk
i=1 βk−i
1
gi
1 −βk
1 s"
T,0.7190332326283988,"(1 −β2) Pk
i=1 βk−i
2
v2
i
1 −βk
2
OASIS
gk
|β2Dk−1 + (1 −β2)vk|α"
T,0.7196374622356495,"∗vi = diag(zi ⊙∇2F(wi)zi) and zi ∼Rademacher(0.5) ∀i ≥1,
∗∗(|A|α)ii = max{|A|ii, α}
∗∗∗Dk = β2Dk−1 + (1 −β2)vk"
T,0.7202416918429003,"Algorithm 3 Stochastic OASIS
Input: w0, η0, Ik, D0, θ0 = +∞, β2, α"
T,0.720845921450151,"1: w1 = w0 −η0 ˆD0−1∇F(w0)Ik
2: for k = 1, 2, . . . do
3:
Calculate Dk = β2Dk−1 + (1 −β2) diag(zk ⊙∇2FJk(wk)zk)
4:
Calculate ˆDk by setting ( ˆDk)i,i = max{|Dk|i,i, α}, ∀i ∈[d]"
T,0.7214501510574018,"5:
Update ηk = min{
p"
T,0.7220543806646526,"1 + θk−1ηk−1,
∥wk−wk−1∥ˆ
Dk
2∥∇FIk (wk)−∇FIk (wk−1)∥∗ ˆ
Dk }"
T,0.7226586102719034,"6:
Set wk+1 = wk −ηk ˆDk−1∇FIk(wk)"
T,0.7232628398791541,"7:
Set θk =
ηk
ηk−1
8: end for"
T,0.7238670694864048,"Additionally, in order to compare the performance of our preconditioner schema independent of
the adaptive learning-rate rule, we also consider variants of OASIS with ﬁxed η. We explore two
modiﬁcations, denoted as “Fixed LR” and “Momentum” in Section 5. “Fixed LR” is obtained
from Algorithm 3 by simply having a ﬁxed scalar η for all iterations, which results in Algorithm 4.
“Momentum” is obtained from “Fixed LR” by considering a simple form of ﬁrst-order momentum
with a parameter β1, and this results in Algorithm 5. In Section C.5, we show that OASIS is robust
with respect to the different choices of learning rate, obtaining a narrow spectrum of changes."
T,0.7244712990936556,"Algorithm 4 OASIS- Fixed LR
Input: w0, η, Ik, D0, β2, α"
T,0.7250755287009063,"1: w1 = w0 −η ˆD0−1∇FIk(w0)
2: for k = 1, 2, . . . do
3:
Calculate Dk = β2Dk−1+(1−β2) diag(zk⊙∇2FJk(wk)zk)"
T,0.7256797583081571,"4:
Calculate ˆDk by setting ( ˆDk)i,i = max{|Dk|i,i, α}, ∀i ∈[d]
5:
Set wk+1 = wk −η ˆDk−1∇FIk(wk)
6: end for"
T,0.7262839879154078,Published as a conference paper at ICLR 2022
T,0.7268882175226586,"Algorithm 5 OASIS- Momentum
Input: w0, η, Ik, D0, β1, β2 α"
T,0.7274924471299093,"1: Set m0 = ∇FIk(w0)
2: w1 = w0 −η ˆD0−1m0
3: for k = 1, 2, . . . do
4:
Calculate Dk = β2Dk−1+(1−β2) diag(zk⊙∇2FJk(wk)zk)"
T,0.7280966767371602,"5:
Calculate ˆDk by setting ( ˆDk)i,i = max{|Dk|i,i, α}, ∀i ∈[d]
6:
Calculate mk = β1mk−1 + (1 −β1)∇FIk(wk)
7:
Set wk+1 = wk −η ˆDk−1mk
8: end for"
T,0.7287009063444109,"In our experiments, we used a biased version of the algorithm with Ik = Jk. One of the beneﬁts of
this choice is to compute the Hessian-vector product efﬁciently. By reusing the computed gradient,
the overhead of computing gradients with respect to different samples is signiﬁcantly reduced (see
Section B.3)."
T,0.7293051359516616,"The ﬁnal remark is related to a strategy to obtain D0, which is required at the start of OASIS
Algorithm. One option is to do warmstarting, i.e., spend some time before training in order to sample
some number of Hutchinson’s estimates. The second option is to use bias corrected rule for Dk
similar to the one used in Adam and AdaHessian
Dk = β2Dk−1 + (1 −β2)diag(zk ⊙∇2FJk(wk)zk),"
T,0.7299093655589124,"Dcor
k
=
Dk
1 −βk+1
2
,"
T,0.7305135951661631,which allows to obtain D0 by deﬁning D−1 to be a zero diagonal.
T,0.7311178247734139,"B.3
EFFICIENT HESSIAN-VECTOR COMPUTATION"
T,0.7317220543806646,"In the OASIS Algorithm, similar to AdaHessian (Yao et al., 2020) methodology, the calculation of
the Hessian-vector product in Hutchinson’s method is the main overhead. In this section, we present
how this product can be calculated efﬁciently. First, lets focus on two popular machine learning
problems: (i) logistic regression; and (ii) non-linear least squares problems. Then, we show the
efﬁcient calculation of this product in deep learning problems."
T,0.7323262839879154,"Logistic Regression.
One can note that we can show the ℓ2-regularized logistic regression as:"
T,0.7329305135951661,F(w) = 1 n
T,0.733534743202417,"
1n ∗log

1 + e−Y ⊙XT w
+ λ"
T,0.7341389728096677,"2 ∥w∥2,
(79)"
T,0.7347432024169185,"where 1n is the vector of ones with size 1 × n, ∗is the standard multiplication operator between two
matrices, and X is the feature matrix and Y is the label matrix. Further, the Hessian-vector product
for logistic regression problems can be calculated as follows (for any vector v ∈Rd):"
T,0.7353474320241692,∇2F(w) ∗v = 1 n 
T,0.7359516616314199,"







 XT ∗ "
T,0.7365558912386707,Y ⊙Y ⊙e−Y ⊙XT w
T,0.7371601208459214," 
1 + e−Y ⊙XT w2 "
T,0.7377643504531722,"⊙X ∗v
| {z }
1
⃝
|
{z
}
2
⃝
|
{z
}
3
⃝ "
T,0.7383685800604229,"







"
T,0.7389728096676738,"+ λv.
(80)"
T,0.7395770392749245,"The above calculation shows the order of computations in order to compute the Hessian-vector
product without constructing the Hessian explicitly."
T,0.7401812688821753,"Non-linear Least Squares.
The non-linear least squares problems can be written as following:"
T,0.740785498489426,F(w) = 1
T,0.7413897280966767,"2n∥Y −φ(XT w)∥2.
(81)"
T,0.7419939577039275,Published as a conference paper at ICLR 2022
T,0.7425981873111782,The Hessian-vector product for the above problem can be efﬁciently calculated as:
T,0.743202416918429,∇2F (w) ∗v =
N,0.7438066465256797,"1
n "
N,0.7444108761329306,"











"
N,0.7450151057401813,"−XT ∗
h
φ(XT w) ⊙(1 −φ(XT w)) ⊙(Y −2(1 + Y ) ⊙φ(XT w) + 3φ(XT w) ⊙φ(XT w))
i
⊙X ∗v
| {z }
1
⃝
|
{z
}
2
⃝
|
{z
}
3
⃝ "
N,0.7456193353474321,"











"
N,0.7462235649546828,".
(82)"
N,0.7468277945619335,"Deep Learning.
In general, the Hessian-vector product can be efﬁciently calculated by:"
N,0.7474320241691843,∇2F(w) ∗v = ∂2F(w)
N,0.748036253776435,∂w∂w ∗v = ∂
N,0.7486404833836858,∂w(∂F(w)T
N,0.7492447129909365,"∂w
v).
(83)"
N,0.7498489425981874,"As is clear from (83), two rounds of back-propagation are needed. In fact, the round regarding the
gradient evaluation is already calculated in the corresponding iteration; and thus, one extra round of
back-propagation is needed in order to evaluate the Hessian-vector product. This means that the extra
cost for the above calculation is almost equivalent to one gradient evaluation."
N,0.7504531722054381,Published as a conference paper at ICLR 2022
N,0.7510574018126889,Table 5: Deep Neural Networks used in the experiments.
N,0.7516616314199396,"Data
Network
# Train
# Test
# Classes
d"
N,0.7522658610271903,"MNIST
Net DNN
60K
10K
10
21.8K"
N,0.7528700906344411,"CIFAR10
ResNet20
50K
10K
10
272K
ResNet32
50K
10K
10
467K
CIFAR100
ResNet18
50K
10K
100
11.22M"
N,0.7534743202416918,"C
DETAILS OF EXPERIMENTS"
N,0.7540785498489426,"C.1
TABLE OF ALGORITHMS"
N,0.7546827794561933,Table 4 summarizes the algorithms implemented in Section 5.
N,0.7552870090634441,"Algorithm
Description and Reference"
N,0.7558912386706949,"SGD
Stochastic gradient method (Robbins & Monro, 1951)
Adam
Adam method (Kingma & Ba, 2014)
AdamW
Adam with decoupled weight decay (Loshchilov & Hutter, 2017)
AdGD
Adaptive Gradient Descent (Malitsky & Mishchenko, 2020)
AdaHessian
AdaHessian method (Yao et al., 2020)"
N,0.7564954682779457,"OASIS-Adaptive LR
Our proposed method with adaptive learning rate
OASIS-Fixed LR
Our proposed method with ﬁxed learning rate
OASIS-Momentum
Our proposed method with momentum"
N,0.7570996978851964,Table 4: Description of implemented algorithms
N,0.7577039274924471,"To display the optimality gap for logistic regression problems, we used Trust Region (TR) New-
ton Conjugate Gradient (Newton-CG) method (Nocedal & Wright, 2006) to ﬁnd a w such that
∥∇F(w)∥2 < 10−19. Hereafter, we denote F(w) −F(w∗) the optimality gap, where we refer w∗to
the solution found by TR Newton-CG."
N,0.7583081570996979,"C.2
PROBLEM DETAILS"
N,0.7589123867069486,Some metrics for the image classiﬁcation problems are given in Table 5.
N,0.7595166163141994,"The number of parameters for ResNet architectures is particularly important, showcasing that OASIS
is able to operate in very high-dimensional problems, contrary to the widespread belief that methods
using second-order information are fundamentally limited by the dimensionality of the parame-
ter space."
N,0.7601208459214501,"C.3
BINARY CLASSIFICATION"
N,0.760725075528701,"In Section 5 of the main paper, we studied the empirical performance of OASIS on binary classiﬁ-
cation problems in a deterministic setting, and compared it with AdGD and AdaHessian. Here, we
present the extended details on the experiment."
N,0.7613293051359517,"C.3.1
PROBLEM AND DATA"
N,0.7619335347432025,"As a common practice for the empirical research of the optimization algorithms, LIBSVM datasets6
are chosen for the exercise. Speciﬁcally, we chose 5 popular binary class datasets, ijcnn1, rcv1,
news20, covtype and real-sim. Table 6 summarizes the basic statistics of the datasets."
"DATASETS
ARE
AVAILABLE
AT",0.7625377643504532,"6Datasets
are
available
at
https://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/
datasets/."
"DATASETS
ARE
AVAILABLE
AT",0.7631419939577039,Published as a conference paper at ICLR 2022
"DATASETS
ARE
AVAILABLE
AT",0.7637462235649547,"Table 6: Summary of Datasets.
Dataset
# feature
n (# Train)
# Test
% Sparsity
ijcnn11
22
49,990
91,701
40.91
rcv11
47,236
20,242
677,399
99.85
news202
1,355,191
14,997
4,999
99.97
covtype2
54
435,759
145,253
77.88
real-sim2
20,958
54,231
18,078
99.76"
"DATASETS
ARE
AVAILABLE
AT",0.7643504531722054,"1 dataset has default training/testing samples.
2 dataset is randomly split by 75%-training & 25%-testing."
"DATASETS
ARE
AVAILABLE
AT",0.7649546827794562,"Let (xi, yi) be a training sample indexed by i ∈[n] := {1, 2, ..., n}, where xi ∈Rd is a feature
vector and yi ∈{−1, +1} is a label. The loss functions are deﬁned in the forms"
"DATASETS
ARE
AVAILABLE
AT",0.7655589123867069,"fi(w) = log(1 + e−yixT
i w) + λ"
"DATASETS
ARE
AVAILABLE
AT",0.7661631419939577,"2 ∥w∥2,
(84)"
"DATASETS
ARE
AVAILABLE
AT",0.7667673716012084,"fi(w) =

yi −
1
1 + e−xT
i w"
"DATASETS
ARE
AVAILABLE
AT",0.7673716012084593,"2
,
(85)"
"DATASETS
ARE
AVAILABLE
AT",0.76797583081571,"where (84) is a regularized logistic regression of a particular choice of λ > 0 (and we used λ ∈
{0.1/n, 1/n, 10/n} in the experiment), and hence a strongly convex function; and (85) is a non-linear
least square loss, which is apparently non-convex."
"DATASETS
ARE
AVAILABLE
AT",0.7685800604229607,The problem we aimed to solve is then deﬁned in the form
"DATASETS
ARE
AVAILABLE
AT",0.7691842900302115,"min
w∈Rd ("
"DATASETS
ARE
AVAILABLE
AT",0.7697885196374622,"F(w) := 1 n n
X"
"DATASETS
ARE
AVAILABLE
AT",0.770392749244713,"i=1
fi(w) )"
"DATASETS
ARE
AVAILABLE
AT",0.7709969788519637,",
(86)"
"DATASETS
ARE
AVAILABLE
AT",0.7716012084592145,and we denote w∗the global optimizer of (86) for logistic regression.
"DATASETS
ARE
AVAILABLE
AT",0.7722054380664652,"C.3.2
CONFIGURATION OF ALGORITHM"
"DATASETS
ARE
AVAILABLE
AT",0.7728096676737161,"To better evaluate the performance of the algorithms, we conﬁgured OASIS, AdGD and AdaHessian
with different choices of parameters."
"DATASETS
ARE
AVAILABLE
AT",0.7734138972809668,"• OASIS was conﬁgured with different values of β2 and α, where β2 can be any values in
{0.95, 0.99, 0.995, 0.999} and α can be any value in the set {10−3, 10−5, 10−7} (see Section
C.5 where OASIS has a narrow spectrum of changes with respect to different values of α and β2).
In addition, we adopted a warmstarting approach to evaluate the diagonal of Hessian at the starting
point w0.
• AdGD was conﬁgured with 12 different values of the initial learning rate, i.e., η0
∈
{10−11, 10−10, ..., 0.1, 1.0}.
• AdaHessian was conﬁgured with different values of the ﬁxed learning rate: for logistic regression,
we used 12 values between 0.1 and 5; for non-linear least square, we used 0.01, 0.05, 0.1, 0.5, 1.0
and 2.0."
"DATASETS
ARE
AVAILABLE
AT",0.7740181268882175,"To take into account the randomness of the performance, we used 10 distinct random seeds to initialize
w0 for each algorithm, dataset and problem."
"DATASETS
ARE
AVAILABLE
AT",0.7746223564954683,"C.3.3
EXTENDED EXPERIMENTAL RESULTS"
"DATASETS
ARE
AVAILABLE
AT",0.775226586102719,"This section presents the extended results on our numerical experiments. For the whole experiments
in this paper, we ran each method 10 times starting from different initial points."
"DATASETS
ARE
AVAILABLE
AT",0.7758308157099698,"For logistic regression, we show the evolution of the optimality gap in Figure 6 and the ending gap
in Figure 7; the evolution of the testing accuracy and the maximum achieved ones are shown in Figure
8 and Figure 9 respectively."
"DATASETS
ARE
AVAILABLE
AT",0.7764350453172205,"For non-linear least square, the evolution of the objective and its ending values are shown in
Figure 10; the evolution of the testing accuracy along with the maximum achived ones are shown in
Figure 11."
"DATASETS
ARE
AVAILABLE
AT",0.7770392749244713,Published as a conference paper at ICLR 2022
"DATASETS
ARE
AVAILABLE
AT",0.777643504531722,"0
100
200
300
400 10
5 10
4 10
3 10
2 10
1 100 101"
"DATASETS
ARE
AVAILABLE
AT",0.7782477341389729,"F(w)
F *"
"DATASETS
ARE
AVAILABLE
AT",0.7788519637462236,ijcnn1
"DATASETS
ARE
AVAILABLE
AT",0.7794561933534743,"AdGD
AdaHessian
OASIS"
"DATASETS
ARE
AVAILABLE
AT",0.7800604229607251,"0
100
200
300
400 10
6 10
5 10
4 10
3 10
2 10
1"
"DATASETS
ARE
AVAILABLE
AT",0.7806646525679758,"100
rcv1"
"DATASETS
ARE
AVAILABLE
AT",0.7812688821752266,"0
100
200
300
400 10
3 10
2 10
1"
"DATASETS
ARE
AVAILABLE
AT",0.7818731117824773,news20
"DATASETS
ARE
AVAILABLE
AT",0.7824773413897281,"0
100
200
300
400 10
2 10
1 100 101"
COVTYPE,0.7830815709969788,"102
covtype"
COVTYPE,0.7836858006042297,"0
100
200
300
400 10
6 10
4 10
2"
REAL-SIM,0.7842900302114804,"100
real-sim"
REAL-SIM,0.7848942598187311,"0
100
200
300
400 10
5 10
4 10
3 10
2 10
1 100 101"
REAL-SIM,0.7854984894259819,"F(w)
F *"
REAL-SIM,0.7861027190332326,"0
100
200
300
400 10
4 10
3 10
2 10
1"
REAL-SIM,0.7867069486404834,"0
100
200
300
400 10
6 10
5 10
4 10
3 10
2 10
1"
REAL-SIM,0.7873111782477341,"0
100
200
300
400 10
1"
REAL-SIM,0.7879154078549849,"0
100
200
300
400 10
8 10
6 10
4 10
2 100"
REAL-SIM,0.7885196374622356,"0
100
200
300
400
Number of Effective Passes 10
7 10
5 10
3 10
1"
REAL-SIM,0.7891238670694865,"F(w)
F *"
REAL-SIM,0.7897280966767372,"0
100
200
300
400
Number of Effective Passes 10
12 10
9 10
6 10
3 100"
REAL-SIM,0.7903323262839879,"0
100
200
300
400
Number of Effective Passes 10
8 10
6 10
4 10
2 100"
REAL-SIM,0.7909365558912387,"0
100
200
300
400
Number of Effective Passes 10
3 10
2 10
1 100"
REAL-SIM,0.7915407854984894,"0
100
200
300
400
Number of Effective Passes 10
12 10
9 10
6 10
3 100"
REAL-SIM,0.7921450151057402,"Figure 6: Evolution of the optimality gaps of OASIS, AdGD and AdaHessian for ℓ2-regularized
Logistic regression: λ = 0.1"
REAL-SIM,0.7927492447129909,"n (top row), λ = 1"
REAL-SIM,0.7933534743202417,"n (middle row), and λ = 10"
REAL-SIM,0.7939577039274924,"n (bottom row). From left to
right: ijcnn1, rcv1, news20, covtype and real-sim."
REAL-SIM,0.7945619335347432,"AdGD
AdaHessian
OASIS 10
6 10
5 10
4 10
3 10
2 10
1 100"
REAL-SIM,0.795166163141994,"Ending F(w)
F(w * )"
REAL-SIM,0.7957703927492447,ijcnn1
REAL-SIM,0.7963746223564955,"AdGD
AdaHessian
OASIS 10
6 10
5 10
4 10
3 10
2"
REAL-SIM,0.7969788519637462,"10
1
rcv1"
REAL-SIM,0.797583081570997,"AdGD
AdaHessian
OASIS 10
3 10
2 10
1"
REAL-SIM,0.7981873111782477,news20
REAL-SIM,0.7987915407854985,"AdGD
AdaHessian
OASIS 10
2 10
1"
REAL-SIM,0.7993957703927492,covtype
REAL-SIM,0.8,"AdGD
AdaHessian
OASIS 10
8 10
7 10
6 10
5 10
4 10
3 10
2 10
1"
REAL-SIM,0.8006042296072508,real-sim
REAL-SIM,0.8012084592145015,"AdGD
AdaHessian
OASIS 10
6 10
5 10
4 10
3"
REAL-SIM,0.8018126888217523,"Ending F(w)
F(w * )"
REAL-SIM,0.802416918429003,"AdGD
AdaHessian
OASIS 10
4 10
3"
REAL-SIM,0.8030211480362538,"AdGD
AdaHessian
OASIS 10
6 10
5 10
4 10
3 10
2"
REAL-SIM,0.8036253776435045,"AdGD
AdaHessian
OASIS 10
2"
REAL-SIM,0.8042296072507553,"2 × 10
2"
REAL-SIM,0.804833836858006,"3 × 10
2"
REAL-SIM,0.8054380664652568,"4 × 10
2"
REAL-SIM,0.8060422960725075,"AdGD
AdaHessian
OASIS 10
9 10
8 10
7 10
6 10
5 10
4 10
3 10
2"
REAL-SIM,0.8066465256797583,"AdGD
AdaHessian
OASIS 10
8 10
7 10
6 10
5 10
4 10
3"
REAL-SIM,0.8072507552870091,"Ending F(w)
F(w * )"
REAL-SIM,0.8078549848942598,"AdGD
AdaHessian
OASIS 10
13 10
11 10
9 10
7 10
5"
REAL-SIM,0.8084592145015106,"AdGD
AdaHessian
OASIS 10
9 10
8 10
7 10
6 10
5"
REAL-SIM,0.8090634441087613,"AdGD
AdaHessian
OASIS 10
3 10
2"
REAL-SIM,0.8096676737160121,"AdGD
AdaHessian
OASIS 10
13 10
11 10
9 10
7 10
5"
REAL-SIM,0.8102719033232628,"Figure 7: Ending optimality gaps of OASIS, AdGD and AdaHessian for ℓ2-regularized Logistic
regression: λ = 0.1"
REAL-SIM,0.8108761329305136,"n (top row), λ = 1"
REAL-SIM,0.8114803625377643,"n (middle row), and λ = 10"
REAL-SIM,0.8120845921450152,"n (bottom row). From left to right:
ijcnn1, rcv1, news20, covtype and real-sim."
REAL-SIM,0.8126888217522659,Published as a conference paper at ICLR 2022
REAL-SIM,0.8132930513595166,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8138972809667674,Testing Accuracy
REAL-SIM,0.8145015105740181,ijcnn1
REAL-SIM,0.8151057401812689,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.8157099697885196,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9 rcv1"
REAL-SIM,0.8163141993957704,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8169184290030211,news20
REAL-SIM,0.817522658610272,"0
100
200
300
400
0.50 0.55 0.60 0.65 0.70 0.75"
REAL-SIM,0.8181268882175227,covtype
REAL-SIM,0.8187311178247734,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8193353474320242,"1.0
real-sim"
REAL-SIM,0.8199395770392749,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8205438066465257,Testing Accuracy
REAL-SIM,0.8211480362537764,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8217522658610272,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8223564954682779,"0
100
200
300
400
0.50 0.55 0.60 0.65 0.70 0.75"
REAL-SIM,0.8229607250755288,"0
100
200
300
400
0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8235649546827795,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8241691842900302,Testing Accuracy
REAL-SIM,0.824773413897281,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.8253776435045317,"0
100
200
300
400
Number of Effective Passes 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
REAL-SIM,0.8259818731117825,"0
100
200
300
400
Number of Effective Passes 0.50 0.55 0.60 0.65 0.70 0.75"
REAL-SIM,0.8265861027190332,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9"
REAL-SIM,0.827190332326284,"Figure 8: Evolution of the testing accuracy of OASIS, AdGD and AdaHessian for ℓ2-regularized
Logistic regression: λ = 0.1"
REAL-SIM,0.8277945619335347,"n (top row), λ = 1"
REAL-SIM,0.8283987915407856,"n (middle row), and λ = 10"
REAL-SIM,0.8290030211480363,"n (bottom row). From left to
right: ijcnn1, rcv1, news20, covtype and real-sim."
REAL-SIM,0.829607250755287,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.8302114803625378,0.9050
REAL-SIM,0.8308157099697885,0.9075
REAL-SIM,0.8314199395770393,0.9100
REAL-SIM,0.83202416918429,0.9125
REAL-SIM,0.8326283987915408,0.9150
REAL-SIM,0.8332326283987915,0.9175
REAL-SIM,0.8338368580060423,0.9200
REAL-SIM,0.834441087613293,0.9225
REAL-SIM,0.8350453172205438,Max Test Accuracy
REAL-SIM,0.8356495468277946,ijcnn1
REAL-SIM,0.8362537764350453,"AdGD
AdaHessian
OASIS
0.955 0.956 0.957 0.958 0.959 0.960 0.961 0.962 rcv1"
REAL-SIM,0.8368580060422961,"AdGD
AdaHessian
OASIS 0.91 0.92 0.93 0.94 0.95 0.96"
REAL-SIM,0.8374622356495468,news20
REAL-SIM,0.8380664652567976,"AdGD
AdaHessian
OASIS 0.68 0.70 0.72 0.74 0.76"
REAL-SIM,0.8386706948640483,covtype
REAL-SIM,0.8392749244712991,"AdGD
AdaHessian
OASIS
0.9575"
REAL-SIM,0.8398791540785498,0.9600
REAL-SIM,0.8404833836858006,0.9625
REAL-SIM,0.8410876132930514,0.9650
REAL-SIM,0.8416918429003021,0.9675
REAL-SIM,0.8422960725075529,0.9700
REAL-SIM,0.8429003021148036,0.9725
REAL-SIM,0.8435045317220544,0.9750
REAL-SIM,0.8441087613293051,real-sim
REAL-SIM,0.8447129909365559,"AdGD
AdaHessian
OASIS 0.912 0.913 0.914 0.915 0.916 0.917 0.918 0.919 0.920"
REAL-SIM,0.8453172205438066,Max Test Accuracy
REAL-SIM,0.8459214501510574,"AdGD
AdaHessian
OASIS 0.955 0.956 0.957 0.958"
REAL-SIM,0.8465256797583082,"AdGD
AdaHessian
OASIS 0.91 0.92 0.93 0.94 0.95"
REAL-SIM,0.8471299093655589,"AdGD
AdaHessian
OASIS 0.70 0.71 0.72 0.73 0.74 0.75"
REAL-SIM,0.8477341389728097,"AdGD
AdaHessian
OASIS 0.956 0.958 0.960 0.962 0.964 0.966 0.968 0.970 0.972"
REAL-SIM,0.8483383685800604,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.8489425981873112,0.9140
REAL-SIM,0.8495468277945619,0.9141
REAL-SIM,0.8501510574018127,0.9142
REAL-SIM,0.8507552870090634,0.9143
REAL-SIM,0.8513595166163141,0.9144
REAL-SIM,0.851963746223565,0.9145
REAL-SIM,0.8525679758308157,0.9146
REAL-SIM,0.8531722054380665,Max Test Accuracy
REAL-SIM,0.8537764350453172,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.854380664652568,0.94500
REAL-SIM,0.8549848942598187,0.94525
REAL-SIM,0.8555891238670695,0.94550
REAL-SIM,0.8561933534743202,0.94575
REAL-SIM,0.856797583081571,0.94600
REAL-SIM,0.8574018126888218,0.94625
REAL-SIM,0.8580060422960725,0.94650
REAL-SIM,0.8586102719033233,0.94675
REAL-SIM,0.859214501510574,0.94700
REAL-SIM,0.8598187311178248,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.8604229607250755,0.8675
REAL-SIM,0.8610271903323263,0.8700
REAL-SIM,0.861631419939577,0.8725
REAL-SIM,0.8622356495468277,0.8750
REAL-SIM,0.8628398791540786,0.8775
REAL-SIM,0.8634441087613293,0.8800
REAL-SIM,0.8640483383685801,0.8825
REAL-SIM,0.8646525679758308,0.8850
REAL-SIM,0.8652567975830816,"AdGD
AdaHessian
OASIS 0.720 0.725 0.730 0.735 0.740 0.745 0.750 0.755"
REAL-SIM,0.8658610271903323,"AdGD
AdaHessian
OASIS 0.936 0.938 0.940 0.942 0.944 0.946 0.948"
REAL-SIM,0.8664652567975831,"Figure 9: Maximum testing accuracy of OASIS, AdGD and AdaHessian for ℓ2-regularized Logistic
regression: λ = 0.1"
REAL-SIM,0.8670694864048338,"n (top row), λ = 1"
REAL-SIM,0.8676737160120845,"n (middle row), and λ = 10"
REAL-SIM,0.8682779456193354,"n (bottom row). From left to right:
ijcnn1, rcv1, news20, covtype and real-sim."
REAL-SIM,0.8688821752265861,Published as a conference paper at ICLR 2022
REAL-SIM,0.8694864048338369,"0
100
200
300
400
Number of Effective Passes 1.00 1.05 1.10 1.15 1.20 F(w)"
REAL-SIM,0.8700906344410876,ijcnn1
REAL-SIM,0.8706948640483384,"AdGD
AdaHessian
OASIS"
REAL-SIM,0.8712990936555891,"0
100
200
300
400
Number of Effective Passes 0.6 0.8 1.0 1.2 rcv1"
REAL-SIM,0.8719033232628399,"0
100
200
300
400
Number of Effective Passes 0.6 0.8 1.0 1.2"
REAL-SIM,0.8725075528700906,news20
REAL-SIM,0.8731117824773413,"0
100
200
300
400
Number of Effective Passes 0.9 1.0 1.1"
COVTYPE,0.8737160120845922,"1.2
covtype"
COVTYPE,0.8743202416918429,"0
100
200
300
400
Number of Effective Passes 0.75 1.00 1.25 1.50"
COVTYPE,0.8749244712990937,real-sim
COVTYPE,0.8755287009063444,"AdGD
AdaHessian
OASIS 0.988 0.990 0.992 0.994 0.996 0.998 1.000"
COVTYPE,0.8761329305135952,Ending F(w)
COVTYPE,0.8767371601208459,ijcnn1
COVTYPE,0.8773413897280967,"AdGD
AdaHessian
OASIS 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 rcv1"
COVTYPE,0.8779456193353474,"AdGD
AdaHessian
OASIS 0.500 0.525 0.550 0.575 0.600 0.625 0.650"
COVTYPE,0.8785498489425981,"0.675
news20"
COVTYPE,0.879154078549849,"AdGD
AdaHessian
OASIS 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
COVTYPE,0.8797583081570997,covtype
COVTYPE,0.8803625377643505,"AdGD
AdaHessian
OASIS 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
COVTYPE,0.8809667673716012,real-sim
COVTYPE,0.881570996978852,"Figure 10: Evolution of the objective F(w) (top row) and the ending F(w) (bottom row) of OASIS,
AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20, covtype
and real-sim."
COVTYPE,0.8821752265861027,"0
100
200
300
400
Number of Effective Passes 0.800 0.825 0.850 0.875 0.900"
COVTYPE,0.8827794561933535,Testing Accuracy
COVTYPE,0.8833836858006042,ijcnn1
COVTYPE,0.8839879154078549,"AdGD
AdaHessian
OASIS"
COVTYPE,0.8845921450151057,"0
100
200
300
400
Number of Effective Passes 0.6 0.8 rcv1"
COVTYPE,0.8851963746223565,"0
100
200
300
400
Number of Effective Passes 0.6 0.8"
COVTYPE,0.8858006042296073,news20
COVTYPE,0.886404833836858,"0
100
200
300
400
Number of Effective Passes 0.50 0.55 0.60"
COVTYPE,0.8870090634441088,covtype
COVTYPE,0.8876132930513595,"0
100
200
300
400
Number of Effective Passes 0.4 0.6 0.8"
REAL-SIM,0.8882175226586103,"1.0
real-sim"
REAL-SIM,0.888821752265861,"AdGD
AdaHessian
OASIS 0.905 0.906 0.907 0.908 0.909 0.910 0.911 0.912"
REAL-SIM,0.8894259818731118,Max Testing Accuracy
REAL-SIM,0.8900302114803625,ijcnn1
REAL-SIM,0.8906344410876132,"AdGD
AdaHessian
OASIS 0.915 0.920 0.925 0.930 0.935 0.940 0.945 0.950 0.955 rcv1"
REAL-SIM,0.8912386706948641,"AdGD
AdaHessian
OASIS
0.84 0.86 0.88 0.90 0.92 0.94 0.96"
REAL-SIM,0.8918429003021148,news20
REAL-SIM,0.8924471299093656,"AdGD
AdaHessian
OASIS 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66"
REAL-SIM,0.8930513595166163,covtype
REAL-SIM,0.8936555891238671,"AdGD
AdaHessian
OASIS 0.70 0.75 0.80 0.85 0.90 0.95"
REAL-SIM,0.8942598187311178,real-sim
REAL-SIM,0.8948640483383686,"Figure 11: Evolution of the testing accuracy (top row) and the maximum accuracy (bottom row) of
OASIS, AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20,
covtype and real-sim."
REAL-SIM,0.8954682779456193,Published as a conference paper at ICLR 2022
REAL-SIM,0.89607250755287,"0
20
40
60
80
100
120
140
160
Epochs 30 40 50 60 70 80 90"
REAL-SIM,0.8966767371601209,Test Accuracy
REAL-SIM,0.8972809667673716,ResNet20 on CIFAR10 With Weight Decay
REAL-SIM,0.8978851963746224,"140
150
160
90.0 90.5 91.0 91.5 92.0 92.5"
REAL-SIM,0.8984894259818731,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.8990936555891239,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.8996978851963746,"0
20
40
60
80
100 120 140 160
Epochs 30 40 50 60 70 80 90"
REAL-SIM,0.9003021148036254,Test Accuracy
REAL-SIM,0.9009063444108761,ResNet20 on CIFAR10 Without Weight Decay
REAL-SIM,0.9015105740181268,"140
150
160
89.5 90.0 90.5 91.0 91.5"
REAL-SIM,0.9021148036253777,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9027190332326284,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9033232628398792,"0
20
40
60
80
100 120 140 160
Epochs 10
1 100 F(w)"
REAL-SIM,0.9039274924471299,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9045317220543807,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9051359516616314,"0
20
40
60
80
100 120 140 160
Epochs 10
1 100 F(w)"
REAL-SIM,0.9057401812688822,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9063444108761329,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9069486404833836,"Figure 12: ResNet20 on CIFAR10 with and without weight decay. Final accuracy results can be
found in Table 7."
REAL-SIM,0.9075528700906345,"C.4
IMAGE CLASSIFICATION"
REAL-SIM,0.9081570996978852,"In the following sections, we provide the results on standard bench-marking neural network training
tasks: CIFAR10, CIFAR100, and MNIST."
REAL-SIM,0.908761329305136,"C.4.1
CIFAR10"
REAL-SIM,0.9093655589123867,"In our experiments, we compared the performance of the algorithm described in Table 4 in 2 settings
- with and without weight decay. The procedure for choosing parameter values differs slightly for
these two cases, so they are presented separately. Implementations of ResNet20/32 architectures are
taken from AdaHessian repository.7"
REAL-SIM,0.9099697885196375,"Analogously to (Malitsky & Mishchenko, 2020), we also modify terms in the update formula for ηk"
REAL-SIM,0.9105740181268882,"ηk = min{
p"
REAL-SIM,0.911178247734139,"1 + θk−1ηk−1,
∥wk−wk−1∥ˆ
Dk
2∥∇F (wk)−∇F (wk−1)∥∗ ˆ
Dk }."
REAL-SIM,0.9117824773413897,"Speciﬁcally, we incorporate parameter γ into
p"
REAL-SIM,0.9123867069486404,"1 + γθk−1 and use more optimistic bound of 1/Lk
instead of 1/2Lk, which results in a slightly modiﬁed rule"
REAL-SIM,0.9129909365558913,"ηk = min{
p"
REAL-SIM,0.913595166163142,"1 + γθk−1ηk−1,
∥wk−wk−1∥ˆ
Dk
∥∇F (wk)−∇F (wk−1)∥∗ ˆ
Dk }."
REAL-SIM,0.9141993957703928,"In order to ﬁnd the best value for γ, we include it into hyperparameter tuning procedure, ranging the
values in the set {1, 0.1, 0.05, 0.02, 0.01} (see Section C.5 where OASIS has a narrow spectrum of
changes with respect to different values of γ)."
REAL-SIM,0.9148036253776435,"Moreover, we used a learning-rate decaying strategy as considered in (Yao et al., 2020) to have the
same and consistent settings. In the aforementioned setting, ηk would be decreased by a multiplier
on some speciﬁc epochs common for every algorithms (the epochs 80 and 120)."
REAL-SIM,0.9154078549848943,7https://github.com/amirgholami/adahessian
REAL-SIM,0.916012084592145,Published as a conference paper at ICLR 2022
REAL-SIM,0.9166163141993958,"0
20
40
60
80
100
120
140
160
Epochs 20 30 40 50 60 70 80 90"
REAL-SIM,0.9172205438066465,Test Accuracy
REAL-SIM,0.9178247734138972,ResNet32 on CIFAR10 With Weight Decay
REAL-SIM,0.918429003021148,"140
150
160
91.0 91.5 92.0 92.5 93.0 93.5"
REAL-SIM,0.9190332326283988,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9196374622356496,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9202416918429003,"0
20
40
60
80
100 120 140 160
Epochs 30 40 50 60 70 80 90"
REAL-SIM,0.9208459214501511,Test Accuracy
REAL-SIM,0.9214501510574018,ResNet32 on CIFAR10 Without Weight Decay
REAL-SIM,0.9220543806646526,"140
150
160
90.0
90.5
91.0
91.5
92.0
92.5
93.0"
REAL-SIM,0.9226586102719033,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.923262839879154,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9238670694864048,"0
20
40
60
80
100 120 140 160
Epochs 10
2 10
1 100 F(w)"
REAL-SIM,0.9244712990936556,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9250755287009064,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9256797583081571,"0
20
40
60
80
100 120 140 160
Epochs 10
2 10
1 100 F(w)"
REAL-SIM,0.9262839879154079,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9268882175226586,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9274924471299094,"Figure 13: ResNet32 on CIFAR10 with and without weight decay. Final accuracy results can be
found in Table 7."
REAL-SIM,0.9280966767371601,"Table 7: Results of ResNet20/32 on CIFAR10 with and without weight decay. Variant of our method
with ﬁxed learning rate beats or is on par with others in the weight decay setting; while OASIS with
adaptive learning rate produces consistent results showing a close second best performance without
any learning rate tuning (for without weight decay setting)."
REAL-SIM,0.9287009063444108,"Setting
ResNet20, WD
ResNet20, no WD
ResNet32, WD
ResNet32, no WD"
REAL-SIM,0.9293051359516616,"SGD
92.02 ± 0.22
89.92 ± 0.16
92.85 ± 0.12
90.55 ± 0.21
Adam
90.46 ± 0.22
90.10 ± 0.19
91.30 ± 0.15
91.03 ± 0.37
AdamW
91.99 ± 0.17
90.25 ± 0.14
92.58 ± 0.25
91.10 ± 0.16
AdaHessian
92.03 ± 0.10
91.22 ± 0.24
92.71 ± 0.26
92.19 ± 0.14"
REAL-SIM,0.9299093655589123,"OASIS-Adaptive LR
91.20 ± 0.20
91.19 ± 0.24
92.61 ± 0.22
91.97 ± 0.14
OASIS-Fixed LR
91.96 ± 0.21
89.94 ± 0.16
93.01 ± 0.09
90.88 ± 0.21
OASIS-Momentum
92.01 ± 0.19
90.23 ± 0.14
92.77 ± 0.18
91.11 ± 0.25
WD := Weight decay"
REAL-SIM,0.9305135951661632,Published as a conference paper at ICLR 2022
REAL-SIM,0.9311178247734139,"With weight decay: It is important to mention that due to variance in stochastic setting, the learning
rate needs to decay (regardless of the learning rate rule is adaptive or ﬁxed) to get convergence.
One can apply adaptive learning rate rule without decaying by using variance-reduced methods
(future work). Experimental setup is generally taken to be similar to the one used in (Yao et al.,
2020). Parameter values for SGD, Adam, AdamW and AdaHessian are taken from the same
source, meaning that learning rate is set to be 0.1/0.001/0.01/0.15 and, where they are used,
β1 and β2 are set to be 0.9 and 0.999. For OASIS, due to a signiﬁcantly different form of the
preconditioner aggregation, we conducted a small scale search for the best value of β2, choosing
from the set {0.999, 0.99, 0.98, 0.97, 0.96, 0.95}, which produced values of 0.99/0.95/0.98 for
“Fixed LR,” “Momentum” and adaptive variants correspondingly for ResNet20 and 0.99/0.99/0.98
for “Fixed LR,” “Momentum” and adaptive variants correspondingly for ResNet32, while β1 for
momentum variant is still set to 0.9. Additionally, in the experiments with ﬁxed learning rate, it is
tuned for each architecture, producing values 0.1/0.1 for ResNet20 and 0.1/0.1 for ResNet32 for
“Fixed LR” and “Momentum” variants of OASIS correspondingly. For the adaptive variant, γ is set to
0.1/1.0 for ResNet20/32. α is set to 0.1 for all OASIS variants. We train all methods for 160 epochs
and for all methods. With ﬁxed learning rate we employ identical scheduling, reducing step size by
a factor of 10 at epochs 80 and 120. For the adaptive variant scheduler analogue is implemented,
multiplying step size by ρ at epochs 80 and 120, where ρ is set to be 0.1/0.5 for ResNet20/32. Weight
decay value for all optimizers with ﬁxed learning rate is 0.0005 and decoupling for OASIS is done
similarly to AdamW and AdaHessian. For adaptive OASIS weight decay is set to 0.001 without
decoupling. Batch size for all optimizers is 256."
REAL-SIM,0.9317220543806647,"Without weight decay: In this setting we tune learning rate for SGD, Adam, AdamW and AdaHessian,
obtaining the values 0.15/0.005/0.005/0.25 and 0.125/0.01/0.01/0.25 for ResNet20/32 accord-
ingly. Where relevant, β1, β2 are taken to be 0.9, 0.999. For OASIS we still try to tune β2, which
produces values of 0.999 for ResNet20 and adaptive case of ResNet32 and 0.99 for “Fixed LR,”
“Momentum” in the case of ResNet32; for the momentum variant β1 = 0.9. Learning rates are
0.025/0.05 for ResNet20 and 0.025/0.1 for ResNet32 for “Fixed LR” and “Momentum” variants of
OASIS correspondingly. For adaptive variant γ is set to 0.01/0.01 for ResNet20/32. α is set to 0.1
for adaptive OASIS variant, while hyperoptimization showed, that for “Fixed LR” and “Momentum”
a value of 0.01 can be used. We train all methods for 160 epochs and for all methods with ﬁxed
learning rate we employ identical scheduling, reducing step size by a factor of 10 at epochs 80 and
120. For the adaptive variant scheduler analogue is implemented, multiplying step size by ρ at epochs
80 and 120, where ρ is set to be 0.1/0.1 for ResNet20/32. Batch size for all optimizers is 256."
REAL-SIM,0.9323262839879154,"Finally, all of the results presented are based on 10 runs with different seeds, where parameters are
chosen amongst the best runs produced at the preceding tuning phase."
REAL-SIM,0.9329305135951662,"All available results can be seen in Table 7 and Figures 12 and 13. We ran our experiments on an
NVIDIA V100 GPU."
REAL-SIM,0.9335347432024169,"C.4.2
CIFAR100"
REAL-SIM,0.9341389728096676,"Analogously to CIFAR10, we do experiments in settings with and without weight decay. In both cases
we took best learning rates from the corresponding CIFAR10 experiment with ResNet20 architecture
without any additional tuning, which results in 0.1 for all. β2 is set to 0.99 in the case with weight
decay and to 0.999 in the case without it. ResNet18 architecture implementation is taken from a
github repository.8 We train all methods for 200 epochs. For all optimizers learning rate is decreased
(or effectively decreased in case of fully adaptive OASIS) by a factor of 5 at epochs 60, 120 and 160."
REAL-SIM,0.9347432024169184,"All of the results presented are based off of 10 runs with different seeds. All available results can be
seen in Table 8 and Figure 14. We ran our experiments on an NVIDIA V100 GPU."
REAL-SIM,0.9353474320241691,"C.4.3
MNIST"
REAL-SIM,0.93595166163142,"Similar to the previous results, we ran the methods with 10 different random seeds. We considered
Net DNN which has 2 convolutional layers and 2 fully connected layers with ReLu non-linearity,
mentioned in the Table 5. In order to tune the hyperparameters for other algorithms, we considered the
set of learning rates {100, 10−1, 10−2, 10−3}. For OASIS, we used the set of {10−1, 10−2} and the"
REAL-SIM,0.9365558912386707,8https://github.com/uoguelph-mlrg/Cutout
REAL-SIM,0.9371601208459215,Published as a conference paper at ICLR 2022
REAL-SIM,0.9377643504531722,"0
25
50
75
100
125
150
175
200
Epochs 10 20 30 40 50 60 70 80"
REAL-SIM,0.938368580060423,Test Accuracy
REAL-SIM,0.9389728096676737,ResNet18 on CIFAR100 With Weight Decay
REAL-SIM,0.9395770392749244,"180
190
200
72
73
74
75
76
77
78"
REAL-SIM,0.9401812688821752,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9407854984894259,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9413897280966768,"0
25
50
75
100 125 150 175 200
Epochs 10 20 30 40 50 60 70"
REAL-SIM,0.9419939577039275,Test Accuracy
REAL-SIM,0.9425981873111783,ResNet18 on CIFAR100 Without Weight Decay
REAL-SIM,0.943202416918429,"180
190
200
65 67 69 71 73 75"
REAL-SIM,0.9438066465256798,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9444108761329305,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9450151057401812,"0
25
50
75
100 125 150 175 200
Epochs 10
3 10
2 10
1 100 F(w)"
REAL-SIM,0.945619335347432,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9462235649546827,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9468277945619336,"0
25
50
75
100 125 150 175 200
Epochs 10
3 10
2 10
1 100 F(w)"
REAL-SIM,0.9474320241691843,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9480362537764351,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9486404833836858,"Figure 14: ResNet18 on CIFAR100 with and without weight decay. Final accuracy results can be
found in Table 8."
REAL-SIM,0.9492447129909366,"Table 8: Results of ResNet18 on CIFAR100. Simply transferring parameter values from similar
task with ResNet20 on CIFAR10 predictably damages performance of optimizers compared to their
heavily tuned versions. Notably, in the setting without weight decay performance of SGD and Adam
became unstable for different initializations, while adaptive variant of OASIS produces behaviour
robust to the choice of the random seed."
REAL-SIM,0.9498489425981873,"Setting
ResNet18, WD
ResNet18, no WD"
REAL-SIM,0.950453172205438,"SGD
76.57 ± 0.24
70.50 ± 1.51
Adam
73.40 ± 0.31
67.40 ± 0.91
AdamW
72.51 ± 0.76
67.96 ± 0.69
AdaHessian
75.71 ± 0.47
70.16 ± 0.82"
REAL-SIM,0.9510574018126888,"OASIS-Adaptive LR
76.93 ± 0.22
74.13 ± 0.20
OASIS-Fixed LR
76.28 ± 0.21
70.18 ± 0.76
OASIS-Momentum
76.89 ± 0.34
70.93 ± 0.77
WD := Weight decay"
REAL-SIM,0.9516616314199395,Published as a conference paper at ICLR 2022
REAL-SIM,0.9522658610271904,"2
4
6
8
10
Epochs 70 75 80 85 90 95 100"
REAL-SIM,0.9528700906344411,Test Accuracy
REAL-SIM,0.9534743202416919,Net DNN on MNIST With Weight Decay
REAL-SIM,0.9540785498489426,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9546827794561934,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9552870090634441,"2
4
6
8
10
Epochs 84 86 88 90 92 94 96 98"
REAL-SIM,0.9558912386706948,Test Accuracy
REAL-SIM,0.9564954682779456,Net DNN on MNIST Without Weight Decay
REAL-SIM,0.9570996978851963,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9577039274924471,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9583081570996979,"2
4
6
8
10
Epochs 10
1 100 F(w)"
REAL-SIM,0.9589123867069487,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9595166163141994,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9601208459214502,"2
4
6
8
10
Epochs 10
1 100 F(w)"
REAL-SIM,0.9607250755287009,"AdaHessian
Adam
AdamW
SGD"
REAL-SIM,0.9613293051359516,"OASIS - Fixed LR
OASIS - Adaptive LR
OASIS - Momentum"
REAL-SIM,0.9619335347432024,"Figure 15: Net DNN on MNIST with and without weight decay. Final accuracy results can be found
in Table 9."
REAL-SIM,0.9625377643504531,"Table 9: Results of Net DNN on MNIST. Gradient momentum usage seems improve results on short
trajectories for all methods, including OASIS."
REAL-SIM,0.963141993957704,"Setting
Net DNN, WD
Net DNN, no WD"
REAL-SIM,0.9637462235649547,"SGD
98.37 ± 0.45
98.86 ± 0.18
Adam
98.92 ± 0.14
98.90 ± 0.13
AdamW
98.76 ± 0.13
98.92 ± 0.13
AdaHessian
98.82 ± 0.16
98.86 ± 0.16"
REAL-SIM,0.9643504531722055,"OASIS-Adaptive LR
97.93 ± 0.27
97.95 ± 0.29
OASIS-Fixed LR
97.24 ± 3.97
98.09 ± 1.36
OASIS-Momentum
98.78 ± 0.22
98.89 ± 0.10
WD := Weight decay"
REAL-SIM,0.9649546827794562,"set for truncation parameter α ∈{10−1, 10−2}. As is clear from the results shown in Figure 15 and
Table 9, OASIS with momentum has the best performance in terms of training (lowest loss function),
and and it is comparable with the best test accuracy for the cases with and without weight decay.
It is worth mentioning that OASIS with adaptive learning rate got satisfactory results with lower
number of parameters, which require tuning, which is vividly important, especially in comparison
to ﬁrst-order methods that are sensitive to the choice of learning rate. We ran our experiments on a
Tesla K80 GPU."
REAL-SIM,0.965558912386707,"C.5
SENSITIVITY ANALYSIS OF OASIS"
REAL-SIM,0.9661631419939577,"It is worth noting that we designed and analyzed OASIS for the deterministic setting with adaptive
learning rate. It is safe to state that no sensitive tuning is required in the deterministic setting (the
learning rate is updated adaptively, and the performance of OASIS is completely robust even if the
rest of the hyperparameters are not perfectly hand-tuned); see Figures 16 and 17."
REAL-SIM,0.9667673716012085,Published as a conference paper at ICLR 2022
REAL-SIM,0.9673716012084592,"0
100
200
300
400
Number of Effective Passes 10
7 10
6 10
5 10
4 10
3 10
2 10
1 100"
REAL-SIM,0.9679758308157099,"F(w)
F *"
REAL-SIM,0.9685800604229607,ijcnn1
REAL-SIM,0.9691842900302114,"0.95-0.001
0.95-1e-05
0.95-1e-07
0.99-0.001
0.99-1e-05
0.99-1e-07
0.995-0.001
0.995-1e-05
0.995-1e-07
0.999-0.001
0.999-1e-05
0.999-1e-07"
REAL-SIM,0.9697885196374623,"0
100
200
300
400
Number of Effective Passes 10
13 10
11 10
9 10
7 10
5 10
3 10
1"
REAL-SIM,0.970392749244713,"F(w)
F * rcv1"
REAL-SIM,0.9709969788519638,"0
100
200
300
400
Number of Effective Passes 10
7 10
6 10
5 10
4 10
3 10
2 10
1"
REAL-SIM,0.9716012084592145,"F(w)
F *"
REAL-SIM,0.9722054380664653,news20
REAL-SIM,0.972809667673716,"0
100
200
300
400
Number of Effective Passes 10
2 10
1"
REAL-SIM,0.9734138972809667,"F(w)
F *"
REAL-SIM,0.9740181268882175,covtype
REAL-SIM,0.9746223564954682,"Figure 16: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Logistic regression."
REAL-SIM,0.9752265861027191,Published as a conference paper at ICLR 2022
REAL-SIM,0.9758308157099698,"0
100
200
300
400
Number of Effective Passes 1.000 1.025 1.050 1.075 1.100 1.125 1.150 1.175 1.200 F(w)"
REAL-SIM,0.9764350453172206,ijcnn1
REAL-SIM,0.9770392749244713,"0.95-0.001
0.95-1e-05
0.95-1e-07
0.99-0.001
0.99-1e-05
0.99-1e-07
0.995-0.001
0.995-1e-05
0.995-1e-07
0.999-0.001
0.999-1e-05
0.999-1e-07"
REAL-SIM,0.9776435045317221,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 F(w) rcv1"
REAL-SIM,0.9782477341389728,"0
100
200
300
400
Number of Effective Passes 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 F(w)"
REAL-SIM,0.9788519637462235,news20
REAL-SIM,0.9794561933534743,"0
100
200
300
400
Number of Effective Passes 0.90 0.95 1.00 1.05 1.10 1.15 1.20 F(w)"
REAL-SIM,0.980060422960725,covtype
REAL-SIM,0.9806646525679759,"Figure 17: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Non-linear Least Square."
REAL-SIM,0.9812688821752266,Published as a conference paper at ICLR 2022
REAL-SIM,0.9818731117824774,"0
50
100
150
Epochs 0 20 40 60 80 100"
REAL-SIM,0.9824773413897281,Test Accuracy
REAL-SIM,0.9830815709969789,Sensitivity Analysis (LR) - OASIS
REAL-SIM,0.9836858006042296,"OASIS_lr:0.005
OASIS_lr:0.01
OASIS_lr:0.025
OASIS_lr:0.05
OASIS_lr:0.0625
OASIS_lr:0.1
OASIS_lr:0.125
OASIS_lr:0.2
OASIS_lr:0.3
OASIS_lr:0.4
OASIS_lr:0.5
OASIS_lr:1.0
OASIS_lr:2.0"
REAL-SIM,0.9842900302114803,"0
50
100
150
Epochs 0 20 40 60 80 100"
REAL-SIM,0.9848942598187311,Test Accuracy
REAL-SIM,0.9854984894259818,Sensitivity Analysis (LR)- SGD
REAL-SIM,0.9861027190332327,"sgd_lr:0.005
sgd_lr:0.01
sgd_lr:0.025
sgd_lr:0.05
sgd_lr:0.0625
sgd_lr:0.1
sgd_lr:0.125
sgd_lr:0.2
sgd_lr:0.3
sgd_lr:0.4
sgd_lr:0.5
sgd_lr:1.0
sgd_lr:2.0"
REAL-SIM,0.9867069486404834,"Figure 18: Sensitivity of OASIS vs. SGD w.r.t. learning rate η, CIFAR10 on ResNet20."
REAL-SIM,0.9873111782477342,"We also provide sensitivity analysis for the image classiﬁcation tasks. These problems are in stochastic
setting, and we need to tune some of the hyperparameters in OASIS. As is clear from the following
ﬁgures, OASIS is robust with respect to different settings of hyperparameters, and the spectrum of
changes is narrow enough; implying that even if the hyperparameters aren’t properly tuned, we still
get acceptable results that are comparable to other state-of-the-art ﬁrst- and second-order approaches
with less tuning efforts. Figure 18 shows that OASIS is robust for different values of learning rate (left
ﬁgure), while SGD is completely sensitive with respect to learning rate choices. One possible reason
for this is because OASIS uses well-scaled preconditioning, which scales each gradient component
with regard to the local curvature at that dimension, whereas SGD treats all components equally."
REAL-SIM,0.9879154078549849,"Furthermore, in order to use an adaptive learning rate in the stochastic setting, an extra hyperparameter,
γ, is used in OASIS (similar to (Malitsky & Mishchenko, 2020)). Figure 19 shows that OASIS
is also robust with respect to different values of γ (unlike the study in (Malitsky & Mishchenko,
2020)). The aforementioned ﬁgures are for CIFAR10 dataset on the ResNet20 architecture; the
same behaviour is observed for the other network architectures."
REAL-SIM,0.9885196374622357,"Finally, we show here that the performance of OASIS is also robust with respect to different values
of β2. Figure 20 shows the robustness of OASIS across a range of β2 values. This ﬁgure is for
CIFAR100 dataset on the ResNet18 architecture."
REAL-SIM,0.9891238670694864,Published as a conference paper at ICLR 2022
REAL-SIM,0.9897280966767371,"0
50
100
150
Epochs 0 20 40 60 80 100"
REAL-SIM,0.9903323262839879,Test Accuracy
REAL-SIM,0.9909365558912386,Sensitivity Analysis ( )- OASIS
REAL-SIM,0.9915407854984895,OASIS_ :0.01
REAL-SIM,0.9921450151057402,OASIS_ :0.02
REAL-SIM,0.992749244712991,OASIS_ :0.05
REAL-SIM,0.9933534743202417,OASIS_ :0.1
REAL-SIM,0.9939577039274925,OASIS_ :1.0
REAL-SIM,0.9945619335347432,"Figure 19: Sensitivity of OASIS w.r.t. γ, CIFAR10 on ResNet20."
REAL-SIM,0.9951661631419939,"0
100
200
Epochs 0 20 40 60 80 100"
REAL-SIM,0.9957703927492447,Test Accuracy
REAL-SIM,0.9963746223564954,CIFAR100-ResNet18
REAL-SIM,0.9969788519637462,OASIS_ :0.97
REAL-SIM,0.997583081570997,OASIS_ :0.98
REAL-SIM,0.9981873111782478,OASIS_ :0.99
REAL-SIM,0.9987915407854985,OASIS_ :0.999
REAL-SIM,0.9993957703927493,"Figure 20: Sensitivity of OASIS w.r.t. β2, CIFAR100 on ResNet18."
