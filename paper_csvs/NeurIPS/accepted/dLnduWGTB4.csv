Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0029585798816568047,"An important challenge in machine translation is to generate high-quality and
diverse translations. Prior work has shown that the estimated likelihood from the
MT model correlates poorly with translation quality. In contrast, quality evaluation
metrics (such as COMET or BLEURT) exhibit high correlations with human
judgments, which has motivated their use as rerankers (such as quality-aware and
minimum Bayes risk decoding). However, relying on a single translation with high
estimated quality increases the chances of ‚Äúgaming the metric‚Äù. In this paper, we
address the problem of sampling a set of high-quality and diverse translations. We
provide a simple and effective way to avoid over-reliance on noisy quality estimates
by using them as the energy function of a Gibbs distribution. Instead of looking for
a mode in the distribution, we generate multiple samples from high-density areas
through the Metropolis-Hastings algorithm, a simple Markov chain Monte Carlo
approach. The results show that our proposed method leads to high-quality and
diverse outputs across multiple language pairs (ENGLISH‚Üî{GERMAN, RUSSIAN})
with two strong decoder-only LLMs (ALMA-7B, TOWER-7B)."
INTRODUCTION,0.005917159763313609,"1
Introduction"
INTRODUCTION,0.008875739644970414,"Machine translation (MT) is becoming increasingly more accurate and powerful, as it benefits from
the many capabilities and acquired knowledge of large language models (LLMs) (Freitag et al.,
2023b; Hendy et al., 2023). However, for many domains and languages the quality of translation is
still not satisfactory‚Äîfor example, hallucinations or critical errors are a serious issue when translating
high-risk content, as in medical and legal domains (Khoong et al., 2019; Taira et al., 2021; Shen
et al., 2023; Guerreiro et al., 2023b; Sanz-Valdivieso and L√≥pez-Arroyo, 2023; Grimm et al., 2024).
Developing procedures for sampling higher-quality translations is therefore in high demand."
INTRODUCTION,0.011834319526627219,"It is known that the output quality of the translations generated by maximizing the model likelihood
is limited because of the inadequacy of the mode‚Äîmodels tend to produce distributions over outputs
that are highly peaked, favoring a single hypothesis (Eikema and Aziz, 2020; Peters and Martins,
2021; Eikema, 2024); and improving search often makes things worse (Koehn and Knowles, 2017;
Stahlberg and Byrne, 2019). In general, maximizing model likelihood tends to overlook hypotheses
that could be equally valid and more appropriate in certain contexts. To address this limitation, a
string of work has been initiated towards ‚Äúquality-aware decoding‚Äù, which leverages powerful quality
estimation (QE) and evaluation metrics, such as COMET (Rei et al., 2022) or BLEURT (Yan et al.,
2023), to explore and rerank a broader range of candidate hypotheses generated via sampling from
the model‚Äôs distribution, selecting the best-1 (Freitag et al., 2022a; Fernandes et al., 2022b; Farinhas
et al., 2023) or the best-k (Jinnai et al., 2024; Singhal et al., 2023) according to these learned metrics."
INTRODUCTION,0.014792899408284023,‚àóWork done while at Instituto de Telecomunica√ß√µes.
INTRODUCTION,0.01775147928994083,"Figure 1: QUEST samples an index from the current translation (yt), removes all elements to the
right of the index, generates a new continuation, and then uses the Metropolis-Hastings acceptance
criterion to decide whether to accept or reject the resulting new translation. The process continues for
a fixed number of T iterations."
INTRODUCTION,0.020710059171597635,"Despite the benefits, reranking-based methods have their own drawbacks. There is a risk of these
approaches overfitting to the metrics used, potentially leading to illusory gains in quality, as the
translations obtained via optimizing these metrics may not always be preferred by humans when
compared to other alternatives (Fernandes et al., 2022b). Secondly, their effectiveness is limited by
the quality of the initial candidate set. For instance, Vernikos and Popescu-Belis (2024) show that
many high-quality translations, generated by combining translation hypotheses, are less likely to be
sampled from the model‚Äôs distribution even with a very large pool size."
INTRODUCTION,0.023668639053254437,"One potential way to remedy these issues, which we explore in this paper, is to use these automatic
metric proxies as the energy function of a Gibbs distribution. However, sampling hypotheses
from this distribution presents a difficult challenge: unlike likelihood-based sampling, ‚Äúquality-based
sampling‚Äù from a Gibbs distribution cannot be performed autoregressively, and it is intractable to
enumerate and score all possible hypotheses. We address this challenge by proposing a simple
and effective Markov chain Monte Carlo approach (MCMC) method, combining the Metropolis-
Hastings algorithm with a suitable proposal distribution. Our method, Quality-Aware Metropolis-
Hastings (QUEST) Sampling, uses a novel proposal distribution that is compatible with sentence-
level evaluation metrics, common in text generation tasks like MT (Figure 1). We further note that
while we focus on MT, where automatic QE metrics are more developed and robust, our proposed
method is general and can be applied to other natural language processing (NLP) tasks. Furthermore,
as our method is agnostic to the specific quality metric used in the Gibbs distribution, it can directly
benefit from any future improvements in the metrics."
INTRODUCTION,0.026627218934911243,"We show that QUEST sampling results in high-quality and diverse samples on multiple test beds
(WMT23 ENGLISH ‚Üî{GERMAN, RUSSIAN}) and with multiple decoder-only LLMs (TOWER-7B,
ALMA-7B). Our method generates many novel hypotheses from the high-density regions unlikely
to be generated via ancestral sampling. Furthermore, with increasing chain size, average quality as
measured by automatic metrics continues to improve, unlike ancestral sampling, where the candidate
set quality remains unchanged even with a larger pool size.2"
BACKGROUND,0.029585798816568046,"2
Background"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.03254437869822485,"2.1
Large Language Models for Machine Translation"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.03550295857988166,"Generating translations from autoregressive LLMs (either encoder-decoder or decoder-only) involves
conditioning the language model on a prompt x, which is a sequence of tokens (x1, x2, . . . , xL) ‚ààX
encoding the text to be translated coupled with a translation instruction (Raffel et al., 2020; Hendy
et al., 2023). Let V be a fixed vocabulary and Y = V‚àóits Kleene closure. The joint distribution over
the output translations, y ‚ààY, given the prompt x, can be factorized as the product of conditional
probabilities over individual tokens (y1, y2, . . . , yN), where each yi ‚ààV. The probability of a"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.038461538461538464,2We release the code to replicate our experiments at https://www.questdecoding.com.
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.04142011834319527,particular translation y for a given input x can be written as
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.04437869822485207,"pLM(y|x) = N
Y"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.047337278106508875,"i=1
pLM(yi|y<i, x).
(1)"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.05029585798816568,"Here, y<i := (y1, y2, . . . , yi‚àí1). During inference, a translation is generated by sampling one token at
a time from the distribution pLM(yi|y<i, x), adjusted by a temperature, œÑ. The (adjusted) probability
of generating a particular token yi, given the preceding tokens y<i and the input x, is defined as:"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.05325443786982249,"pLM,œÑ(yi = v|y<i, x) =
pLM(yi = v|y<i, x)1/œÑ
P"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.05621301775147929,"v‚Ä≤‚ààV pLM(yi = v‚Ä≤|y<i, x)1/œÑ .
(2)"
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.05917159763313609,"Lower temperature values œÑ make the distribution more deterministic, favoring the most probable
tokens, whereas higher values make the distribution flatter, approximating a uniform distribution."
LARGE LANGUAGE MODELS FOR MACHINE TRANSLATION,0.0621301775147929,"However, multiple works have scrutinized the reliability of model likelihood as a measure for
translation quality (Ott et al., 2018; Stahlberg and Byrne, 2019; Eikema and Aziz, 2020; Freitag et al.,
2022a; Eikema, 2024). Instead of solely relying on the likelihood, these works advocate using an
automatic translation quality metric as a utility function for minimum Bayes risk (MBR) decoding or
reranking based on QE metrics. This shift not only improves the selection of translations but also
facilitates the exploration of the underlying distribution learned by the models. However, it is crucial
to acknowledge that the overall translation quality is still contingent on the quality of the candidate
pool. We next discuss common automatic metrics used for assessing translation quality."
AUTOMATIC METRICS FOR MACHINE TRANSLATION,0.0650887573964497,"2.2
Automatic Metrics for Machine Translation"
AUTOMATIC METRICS FOR MACHINE TRANSLATION,0.06804733727810651,"Automatic quality assessment of machine-generated translations has received considerable attention
recently, resulting in metrics that attain high correlations with human judgment of translation quality
(Freitag et al., 2022b, 2023b). These automatic metrics are meant to assess the quality of a translation
across multiple dimensions (e.g. fluency, adequacy) and can provide reliable feedback when human
judgments are unavailable. Among these metrics, neural learned metrics that are trained on human
translation quality assessment scores or error span annotations have gained significant traction (Rei
et al., 2022; Yan et al., 2023; Guerreiro et al., 2023a; Perrella et al., 2022)."
AUTOMATIC METRICS FOR MACHINE TRANSLATION,0.07100591715976332,"For aligning automatically generated translations with human quality preferences, many works have
proposed to use automatic metrics as an alternative to human feedback during MT training (Shen
et al., 2016; Wieting et al., 2019; He et al., 2024; Xu et al., 2024b) or decoding Shen et al. (2004);
Fernandes et al. (2022b); Freitag et al. (2022a, 2023a). Freitag et al. (2022a) show the efficacy of using
reference-based neural metrics as utility functions for MBR decoding over lexical alternatives. Further,
Fernandes et al. (2022a) use QE metrics like COMET-QE to select 1-best or N-best translations from
a pool of candidate hypotheses. Their analysis shows that while these automatic metrics can be useful,
they might not always reflect human preferences accurately. Additionally, extra care has to be taken
when optimizing systems for these metrics, as the improvements might be attributable to overfitting
or ‚Äúgaming the metric‚Äù, rather than being genuine improvements in translation quality. Nevertheless,
these metrics encode useful information about the quality of translations and can still be useful to
obtain high-quality translations."
AN MCMC-BASED DECODING APPROACH FOR TEXT GENERATION,0.07396449704142012,"3
An MCMC-based Decoding Approach for Text Generation"
AN MCMC-BASED DECODING APPROACH FOR TEXT GENERATION,0.07692307692307693,"Given that we have access to an automatic metric that quantifies how desirable a particular translation
is, we aim to address the following questions:"
AN MCMC-BASED DECODING APPROACH FOR TEXT GENERATION,0.07988165680473373,"Can we sample directly in proportion to their corresponding quality values? Can we use automatic
metrics that already give us a reliable estimate of human-perceived quality to achieve that? Finally,
can we use this process to obtain diverse high-quality samples?"
AN MCMC-BASED DECODING APPROACH FOR TEXT GENERATION,0.08284023668639054,"To answer these questions and to address the limitations of quality-aware decoders, we propose
to take an alternative approach, quality-aware sampling, which ensures high quality and diversity.
Recognizing that naturally occurring texts can have numerous valid translations (Nida, 1964; Dreyer
and Marcu, 2012; Ott et al., 2018; Mayhew et al., 2020), the ability to generate diverse translation
hypotheses is paramount."
AN MCMC-BASED DECODING APPROACH FOR TEXT GENERATION,0.08579881656804733,"To this end, we first present background on Metropolis-Hastings in Section 3.1. Section 3.2 discusses
our proposal distribution. Finally, we connect our approach to reinforcement learning with human
feedback (RLHF) in Section 3.3."
METROPOLIS-HASTINGS,0.08875739644970414,"3.1
Metropolis-Hastings"
METROPOLIS-HASTINGS,0.09171597633136094,"The problem of sampling translation hypotheses in proportion to a metric, r(x, y), can be framed as
sampling from the following Gibbs distribution:"
METROPOLIS-HASTINGS,0.09467455621301775,"œÄŒ≤(y|x) =
1
ZŒ≤(x) exp
r (y, x) Œ≤"
METROPOLIS-HASTINGS,0.09763313609467456,"
,
(3)"
METROPOLIS-HASTINGS,0.10059171597633136,where ZŒ≤(x) = P
METROPOLIS-HASTINGS,0.10355029585798817,"y exp

r(y,x)"
METROPOLIS-HASTINGS,0.10650887573964497,"Œ≤

and Œ≤ is the temperature of the Gibbs distribution. We denote the"
METROPOLIS-HASTINGS,0.10946745562130178,"corresponding unnormalized density by ÀúœÄŒ≤(y|x), which unlike ZŒ≤(x) can be evaluated for any (x, y)."
METROPOLIS-HASTINGS,0.11242603550295859,"While several algorithms have been proposed to sample approximately from such intractable distribu-
tions (Miao et al., 2018; Berglund et al., 2015; Amini et al., 2023), we resort to Metropolis-Hastings
(Metropolis et al., 1953, MH) due to its simplicity and flexibility in handling a wide range of pro-
posal distributions. The MH algorithm generates a sequence of samples from a target distribution,
here œÄŒ≤(y|x), by constructing a Markov chain (y0, y1, . . . , yT ) that has œÄŒ≤(y|x) as its equilibrium
distribution. It starts from an arbitrary hypothesis y0. In the tth iteration, it draws a new hypothesis y
from a proposal distribution q(y|yt, x). This hypothesis is accepted with an acceptance probability
Œ±Œ≤(y, yt) ‚àà[0, 1] given by:"
METROPOLIS-HASTINGS,0.11538461538461539,"Œ±Œ≤(y, yt) = min

1, œÄŒ≤ (y|x) q (yt|y, x)"
METROPOLIS-HASTINGS,0.11834319526627218,"œÄŒ≤ (yt|x) q (y|yt, x)"
METROPOLIS-HASTINGS,0.12130177514792899,"
.
(4)"
METROPOLIS-HASTINGS,0.1242603550295858,"If the candidate y is accepted, the next state in the chain becomes yt+1 = y; if rejected, the chain
stays at yt+1 = yt. The process repeats for some number of steps T and in the end it returns the
hypothesis yT . Note that, while computing the likelihood œÄŒ≤(y|x) of a particular hypothesis y under
the Gibbs distribution is intractable (due to the intractable partition function ZŒ≤(x)), evaluating the
acceptance criterion Œ±Œ≤(y, yt) is easy, because it depends only on the likelihood ratio, in which the
normalization constants cancel out, i.e.:"
METROPOLIS-HASTINGS,0.12721893491124261,"œÄŒ≤(y|x)
œÄŒ≤(yt|x) = ÀúœÄŒ≤(y|x)"
METROPOLIS-HASTINGS,0.1301775147928994,"ÀúœÄŒ≤(yt|x) = exp
r (y, x) ‚àír (yt, x) Œ≤"
METROPOLIS-HASTINGS,0.13313609467455623,"
.
(5)"
METROPOLIS-HASTINGS,0.13609467455621302,"MH converges to the unique stationary distribution œÄŒ≤(y|x), regardless of the initial distribu-
tion we start with, if the transition distribution of the Markov chain, i.e., p(yt|yt‚àí1, x) =
q(yt|yt‚àí1, x)Œ±(yt, yt‚àí1) satisfies the Markov chain ergodic theorem (Neal, 2011)."
METROPOLIS-HASTINGS,0.1390532544378698,"This requires a suitable proposal distribution q(y|yt, x) which must be irreducible and aperiodic. To
ensure irreducibility, the proposal distribution should have sufficient support, such that it is possible
to transition from any state to any other state with a nonzero probability in a finite number of steps.
Aperiodicity ensures that the Markov chain does not get stuck in a cyclic behavior, where it keeps
returning to the same states in a fixed pattern. Additionally, due to the acceptance criterion defined in
Eq. 4, the transition distribution satisfies the detailed balance condition:"
METROPOLIS-HASTINGS,0.14201183431952663,"œÄŒ≤ (y|x) p
 
yt|y, x

= œÄŒ≤
 
yt|x

p
 
y|yt, x

.
(6)"
METROPOLIS-HASTINGS,0.14497041420118342,"It is trivial to show that the chain has the target distribution, œÄŒ≤(y|x) as its stationary distribution:"
METROPOLIS-HASTINGS,0.14792899408284024,"œÄŒ≤(y|x) =
X"
METROPOLIS-HASTINGS,0.15088757396449703,"y‚Ä≤‚ààY
p(y|y‚Ä≤, x)œÄŒ≤(y‚Ä≤|x).
(7)"
METROPOLIS-HASTINGS,0.15384615384615385,"Note that MH can work with almost any proposal distribution. However, if the detailed balance
conditions are far from holding, the acceptance probability will be low for transitions, making the
convergence to the target distribution very slow and the approach impractical. Hence, choosing a
suitable proposal distribution for the task is essential. We next describe our proposal distribution,
q(y|yt, x), which overcomes these limitations and satisfies the required constraints."
PROPOSAL DISTRIBUTION,0.15680473372781065,"3.2
Proposal distribution"
PROPOSAL DISTRIBUTION,0.15976331360946747,"Previous works (Berglund et al., 2015; Miao et al., 2018; Su et al., 2018) use proposal distributions
that generate hypotheses with one-word or token-level modifications. The different formulations in
their most basic form propose a Markov chain in which the state comprises the sentence yt ‚ààY and
an index variable it ‚àà{0, . . . , |yt‚àí1|}. At every step, an index is sampled that determines the token
to be changed, and a new token is then sampled based on the following full conditional distribution:"
PROPOSAL DISTRIBUTION,0.16272189349112426,"q(yi|y<i, y>i) =
ÀúœÄ(y<i, yi, y>i)
P"
PROPOSAL DISTRIBUTION,0.16568047337278108,"y‚Ä≤
i‚ààV ÀúœÄ(y<i, y‚Ä≤
i, y>i),
(8)"
PROPOSAL DISTRIBUTION,0.16863905325443787,where V is created by considering the k most likely tokens (k is generally large) under pLM(yi|y<i).
PROPOSAL DISTRIBUTION,0.17159763313609466,"This procedure, however, has several limitations: first, it makes it difficult to handle more general
combinatorial constraints, in our case more sophisticated metrics. As we only explore adjacent
positions in the sentence space due to small local changes, the Markov chain risks becoming trapped
in infeasible states, necessitating a very large number of steps T to converge. Second, relying solely
on token-level modifications makes it exceedingly difficult to generate plausible text, making it
impractical to align generations with QE metrics or general reward models. We show that this is
indeed the case in Section 5.2."
PROPOSAL DISTRIBUTION,0.17455621301775148,"We instead propose a simple and effective procedure that only requires generating a single hypothesis
from the model pLM and a single evaluation from the metric, r(y, x), and still allows the Markov
chain to converge to the target distribution. We characterize the proposal by the following procedure:"
PROPOSAL DISTRIBUTION,0.17751479289940827,"1. Given an instance yt with length nt := |yt|, sample an index i, i ‚àºq(i|nt).
2. Generate a completion y‚â•i from pLM(y‚â•i|yt
<i, x)."
PROPOSAL DISTRIBUTION,0.1804733727810651,"Note that, due to the nature of our proposal distribution, yt and y share a common substructure
(prefix) before the index i, i.e., yt
<i = y<i, which implies that"
PROPOSAL DISTRIBUTION,0.1834319526627219,"q(y|yt, x, i) = pLM(y‚â•i|yt
<i, x)
Y"
PROPOSAL DISTRIBUTION,0.1863905325443787,"j<i
Œ¥(yj, yt
j),
(9)"
PROPOSAL DISTRIBUTION,0.1893491124260355,"where Œ¥(yj, yt
j) is the Kronecker delta function, which assigns zero probability to prefix tokens which
are different from yt
<i and probability one to tokens matching the prefix."
PROPOSAL DISTRIBUTION,0.19230769230769232,"The complete proposal when we include the index distribution q(i|nt), which depends on the previous
sentence lengths nt := |yt|, is given as"
PROPOSAL DISTRIBUTION,0.1952662721893491,"q(y, i|yt, x) = q(i|nt)pLM(yi‚â•|yt
<i, x)
Y"
PROPOSAL DISTRIBUTION,0.19822485207100593,"j<i
Œ¥(yj, yt
j).
(10)"
PROPOSAL DISTRIBUTION,0.20118343195266272,"Algorithm 1 Quality-Aware Metropolis-
Hastings (QUEST) Sampling"
PROPOSAL DISTRIBUTION,0.20414201183431951,"1: Input: x, pLM, r, T
2: Hyperparameters: œÑ, tburning, Œ≤
3: Sample initial response y0 ‚àºpLM(¬∑ | x)
4: t ‚Üê1
5: for 1 to T do
6:
Sample index i ‚àºq(i|nt‚àí1)
7:
Sample y‚â•i ‚àºpLM(¬∑|yt‚àí1
<i , x)
8:
y ‚Üê(yt‚àí1
<i , y‚â•i)
9:
Compute Œ±Œ≤(y, yt‚àí1) through Eq. 4
10:
Sample Œ± uniformly in [0, 1]
11:
if Œ± ‚â§Œ±Œ≤(y, yt‚àí1) then
12:
yt ‚Üêy
13:
t ‚Üêt + 1
14:
end if
15: end for
16: return ytburning, . . . , yt"
PROPOSAL DISTRIBUTION,0.20710059171597633,"We will use the uniform distribution as the in-
dex distribution, i.e., q(i|nt) = 1/nt for each
i ‚àà[nt], unless otherwise stated. Algorithm 1
describes the complete sampling process."
PROPOSAL DISTRIBUTION,0.21005917159763313,"This proposal is both irreducible and aperiodic
as there is a non-zero probability of going from
a particular sentence to any other sentence and
back. When i = 0, we can generate any text
from the language model, which, when using
ancestral sampling, implies that we can sample
any possible sequence. As our approach only re-
quires access to the token-level log probabilities
and the ability to generate good continuations
given a prefix, it can also be used with closed
LLMs through an API as long as the it provides
access to the sample likelihoods. However, we
limit our experiments to open-source models due
to the incurred cost and lack of training details
about these black-box models."
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.21301775147928995,"3.3
Connections to Reinforcement Learning with Human Feedback"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.21597633136094674,"Reinforcement Learning with Human Feedback (RLHF) leverages human feedback to guide the
learning process of complex NLP tasks (Stiennon et al., 2022; Fernandes et al., 2023; Kaufmann et al.,
2023). The process is as follows. Given a language model, one generates hypotheses y ‚ààY given an
input prompt x and gathers human feedback about which outputs are preferable. This preference data
is then used to train a proxy reward model rœï(y, x). Finally, reinforcement learning (RL) methods
are used to optimize the original LM with respect to the reward model, following"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.21893491124260356,"max
œÄ
Ex‚àºD,y‚àºœÄ(y|x)"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.22189349112426035,"rœï(x, y) Œ≤"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.22485207100591717,"
‚àíDKL [œÄ(y|x) ‚à•pLM(y|x)] .
(11)"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.22781065088757396,"Many works show that the optimal solution to the KL-constrained reward maximization objective
takes the form (Peters and Schaal, 2007; Peng et al., 2019; Korbak et al., 2022b,a; Go et al., 2023):"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.23076923076923078,"œÄ(y|x) =
1
ZŒ≤(x)pLM(y|x) exp"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.23372781065088757,"r(y, x) Œ≤ !"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.23668639053254437,",
(12)"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.23964497041420119,where ZŒ≤(x) = P
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.24260355029585798,"y‚ààY pLM(y|x) exp

r(x,y)"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.2455621301775148,"Œ≤

. While we cannot sample autoregressively from this
distribution, this density can be represented as a Gibbs distribution with the corresponding reward
function Àúr(x, y) = log pLM(y|x) + r(x,y) Œ≤
."
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.2485207100591716,"Instead of the target distribution expressed in Eq. 3, we could define the above distribution as our
target when using QUEST to sample from it, avoiding optimizing the objective in Eq. 11 directly. If we
formulate the acceptance criterion using this target density and our proposal distribution introduced
in Eq. 10, we obtain the following:"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.2514792899408284,"Œ±Œ≤(y, yt) = min

1, exp
r (y, x) ‚àír (yt, x) Œ≤"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.25443786982248523, q(i|n)
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.257396449704142,q(i|nt)
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.2603550295857988,"
.
(13)"
CONNECTIONS TO REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,0.26331360946745563,"The full derivation is in Appendix A. Using this approach, we can align language model generations
without access to the model weights, log probabilities, or RL. We provide a preliminary experimental
comparison using the two target distributions (Eq. 3 and Eq. 12) using QUEST in Appendix C.4."
EXPERIMENTAL SETTINGS,0.26627218934911245,"4
Experimental Settings"
EXPERIMENTAL SETTINGS,0.2692307692307692,"Data and Evaluation
We test our approach on the WMT23 test sets (Kocmi et al., 2023) covering
four language pairs, ENGLISH ‚Üî{GERMAN, RUSSIAN}."
EXPERIMENTAL SETTINGS,0.27218934911242604,"We evaluate the quality and the diversity of the generated texts as follows. Suppose ¬ØY is the set
of hypotheses generated for the source text x with reference hypothesis y‚ãÜand D = {(x, y‚ãÜ, ¬ØY)}
represents the evaluation set. We compute the mean quality over each set of hypotheses using
XCOMET-XL (Guerreiro et al., 2023a) as
1
|D|
P"
EXPERIMENTAL SETTINGS,0.27514792899408286,"(x,y‚ãÜ, ¬Ø
Y)‚ààD
1
| ¬Ø
Y|
P"
EXPERIMENTAL SETTINGS,0.2781065088757396,"y‚àà¬Ø
Y XCOMET-XL(x, y, y‚ãÜ). Simi-
larly, we measure the mean diversity using the average pairwise BLEU (Papineni et al., 2002; Shen
et al., 2019) as 1 ‚àí
1
|D|
P"
EXPERIMENTAL SETTINGS,0.28106508875739644,"(x,y‚ãÜ, ¬Ø
Y)‚ààD
1
| ¬Ø
Y|(| ¬Ø
Y|‚àí1)
P"
EXPERIMENTAL SETTINGS,0.28402366863905326,"(y,y‚Ä≤)‚àà¬Ø
Y2 s.t. yÃ∏=y‚Ä≤ BLEU(y, y‚Ä≤).3"
EXPERIMENTAL SETTINGS,0.2869822485207101,"Models
We use TOWER-7B (Alves et al., 2024, Unbabel/TowerInstruct-7B-v0.2) and ALMA-
7B (Xu et al., 2024a, haoranxu/ALMA-7B), two strong decoder-only MT models based on LLAMA2-
7B (Touvron et al., 2023) as these models achieve competitive translation quality with GPT-4 and
productized models like Google Translate. Unlike ALMA-7B, TOWER-7B uses bilingual MT data as
well as datasets from MT-related tasks during training. Prompts are shown in Appendix B."
EXPERIMENTAL SETTINGS,0.28994082840236685,"Automatic Metrics for QUEST
We use COMETKIWI-XL (Rei et al., 2023), a reference-free
QE model built on top of XLM-R XL (Goyal et al., 2021) and trained to predict human-rated
direct assessments (Graham et al., 2013). This metric showed the highest correlations with human
judgments on the QE Shared Task organized by the eighth conference on Machine Translation (WMT
2023) Blain et al. (2023). We transform the normalized scores from the QE model into z-scores using
a logit transformation with clamping applied to mitigate overflow."
EXPERIMENTAL SETTINGS,0.29289940828402367,3https://github.com/mjpost/sacrebleu/tree/master
EXPERIMENTAL SETTINGS,0.2958579881656805,"Figure 2: Average quality vs. diversity on WMT23 datasets. Different points represent different
hyperparameter values. QUEST outperforms ancestral sampling in six out of eight settings."
EXPERIMENTAL SETTINGS,0.2988165680473373,"Decoding Configurations
For ancestral sampling, we consider temperature values œÑ between 0.2
and 1.0, with an equally spaced interval of 0.1. For generations with QUEST, we sample from the
proposal distribution using œÑ = 0.8 and vary the parameter Œ≤ of the target Gibbs distribution from
the following range of values {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0}. The number of ancestral samples
and decoding steps are both set to 128. We use VLLM (Kwon et al., 2023), a high-throughput and
memory-efficient inference engine for generating outputs."
EXPERIMENTAL SETTINGS,0.30177514792899407,"Compute Comparison: Ancestral Vs QUEST
We use the number of output tokens as a metric to
compare the different approaches. For the sake of simplicity, let us assume that the output sentence
has a fixed size of N. On average, QUEST in T steps generates
  T ‚àí1"
EXPERIMENTAL SETTINGS,0.3047337278106509,"2
+ 1

N = (T +1)N"
EXPERIMENTAL SETTINGS,0.3076923076923077,"2
tokens, N
tokens for the initial hypothesis, and then on average N"
EXPERIMENTAL SETTINGS,0.3106508875739645,"2 tokens for the remaining T ‚àí1 steps in
the chain. If we contrast against sampling T sentences using ancestral sampling, we decode T √ó N
tokens. This suggests that for an equal number of samples generated using ancestral sampling and
QUEST, the latter results in about
2T
(T +1) ‚âà2 times as many tokens, on average. Note, however, that
the computational cost of QUEST is higher than ancestral sampling, as the hypotheses are generated
sequentially and evaluated at every step. We run our experiments on NVIDIA RTX A6000 GPUs.
Each ancestral sampling and QUEST for run with T = 128 takes, on average, 1 hour and 6 hours,
respectively, for 2000 unique source texts on a single GPU. The compute bottleneck for QUEST also
arises from using a large QE metric, potentially distilling this metric into smaller models could help
reduce the compute time. We leave this to future work."
RESULTS,0.3136094674556213,"5
Results"
MAIN FINDINGS,0.3165680473372781,"5.1
Main Findings"
MAIN FINDINGS,0.31952662721893493,The main results of our experiments are presented in Figure 2.
MAIN FINDINGS,0.3224852071005917,"QUEST results in better translation quality-diversity trade-offs. Across language directions and
models, the samples generated by QUEST tend to have better or similar quality than ancestral
sampling as measured by XCOMET-XL.4 As QUEST does not directly use the reference-based metric,
XCOMET-XL, we reduce the chance of overfitting to the metric and thus the gains represent the ability
of QUEST to improve translation quality more realistically. We also report Comet-22 vs diversity
results in the Appendix Figure 9: the trends remain the same. The benefits of the proposed approach
are more noticeable when translating from English (EN ‚Üí{DE, RU}). Specifically, for EN ‚ÜîRU,
our model improves XCOMET-XL by up to 2 points for both language models, showing the efficacy
of QUEST over ancestral sampling."
MAIN FINDINGS,0.3254437869822485,4We provide results on four additional language pairs in Appendix C.6.
MAIN FINDINGS,0.32840236686390534,(a) Sample Overlap between ancestral and QUEST.
MAIN FINDINGS,0.33136094674556216,"(b) Average quality over MCMC steps and number
of ancestral samples t."
MAIN FINDINGS,0.3343195266272189,"(c) Distribution of # of accepted samples: high Œ≤
results in higher acceptance rates."
MAIN FINDINGS,0.33727810650887574,"(d) Reward differences between an initial hypoth-
esis and proposal hypotheses given the initial hy-
pothesis."
MAIN FINDINGS,0.34023668639053256,"Although ancestral exhibits better quality than QUEST for DE-EN, further analysis suggests that this
discrepancy may stem from the constraints of the QE metric used to model the translation preferences.
Notably, QUEST demonstrates significantly higher COMETKIWI-XL scores across the board (see
Appendix Figure 7), suggesting that better and more robust QE models can result in improved transla-
tion quality.5 Furthermore, WMT23 DE-EN includes short passages that might require additional
steps to reach the high-density regions. Based on a length analysis (See Appendix C.2), we observe
that the quality of QUEST lags behind ancestral sampling, specifically for longer sentences. We leave
the exploration of finding optimal steps for convergence and adapting the proposal distribution for
document-level MT to future work."
ABLATION ANALYSIS,0.3431952662721893,"5.2
Ablation Analysis"
ABLATION ANALYSIS,0.34615384615384615,"We present further analysis on some properties of our proposed approach using WMT23 EN‚ÜíRU
datasets with translations generated using ALMA-7B."
ABLATION ANALYSIS,0.34911242603550297,"QUEST generates less-likely high-quality samples.
We compute the set overlap for QUEST and
ancestral samples (T = 128) with an independent draw of 512 ancestral samples to investigate
whether our approach results in hypotheses from unexplored regions (Fig. 3a). Compared to ancestral,
QUEST results in hypotheses that are not found in the larger pool (4√ó) of ancestral samples as
illustrated by a higher mass at the overlap value of 0. This demonstrates that QUEST effectively gets
to regions of the output manifold that would be less likely sampled by the LM and still attain, on
average, better quality and diversity."
ABLATION ANALYSIS,0.3520710059171598,"Average reward improves over decoding steps.
Figure 3b shows the average reward with in-
creasing decoding steps and the sample size for QUEST and ancestral respectively. As ancestral
results in independent samples, the mean reward estimate remains unchanged for different sample
sizes. However, for QUEST, hypotheses are gradually sampled in the direction of higher-quality
regions, closer to the target density, resulting in increased average reward with more steps. We can
also observe that the standard deviation of the reward over all samples decreases with the increasing
number of steps, suggesting that the model eventually reaches a better target distribution."
ABLATION ANALYSIS,0.35502958579881655,"5Kocmi et al. (2024) report that a difference of 2.7 COMETKIWI-XL on average is statistically significant
with a 98.9% accuracy. QUEST improvements are always greater than this value across the board."
ABLATION ANALYSIS,0.35798816568047337,"High Œ≤ value results in higher acceptance rates.
Figure 3c shows the distribution of accepted
samples when varying Œ≤, considering all (blue) versus unique (red) accepted samples. As expected,
on average, the number of accepted samples is smaller than the number of steps T depending upon
the rejection rate‚Äîlow Œ≤ values lead to higher rejection rates. Furthermore, within the accepted
samples, we also observe many repeats. This can be attributed to the observation that the language
model has a low entropy distribution for continuations when the sample indices lie at the end of
sentences. Moreover, for probable sentences, only a few have a high reward, which could result in the
Markov chain getting stuck in a particular state. Increasing the temperature of the LM or adjusting
the index distribution to have less density in the last few tokens could potentially reduce the number
of repeats. We leave this exploration to future work."
ABLATION ANALYSIS,0.3609467455621302,"Our sentence-level proposal generates better candidates.
For a random example sampled from
the WMT23 dataset, we generate 25k hypotheses using three proposal distributions: a) token-level
modification with uniform probability b) token-level modification with full posterior presented in
Eq. 8, and c) our sentence-level proposal (Eq. 10) and calculate the reward differences between the
original (y) and the generated hypothesis (y‚Ä≤): r(y‚Ä≤, x) ‚àír(y, x). Figure 3d shows its distribution:
for proposals (a) and (b), the reward for the generated hypotheses is almost always lower than the
initial translation. On the other hand, our proposal results in assigning half of the probability mass to
hypotheses that improve over y, leading to faster convergence over token-level alternatives."
RELATED WORK,0.363905325443787,"6
Related Work"
RELATED WORK,0.3668639053254438,"Sampling from Intractable Gibbs distributions.
Several methods have been proposed to sample
approximately from Gibbs distributions in text generation using autoregressive and masked-language
models. Prior works (Miao et al., 2018; Zhang et al., 2020) use MH with a proposal distribution that
makes token-level modifications for constrained generation tasks. Since masked language models
(MLM) do not have a straightforward mechanism for sampling text, MCMC has been widely explored
using variations of Gibbs sampling (Berglund et al., 2015; Su et al., 2018; Wang and Cho, 2019;
Yamakoshi et al., 2022). However, Goyal et al. (2022) shows that the masked conditional distributions
from MLMs result in invalid Gibbs samplers and, therefore, proposes to use MH on the masked
conditionals, resulting in higher-quality text. Mireshghallah et al. (2022); Forristal et al. (2023) build
on this work and use MLMs for sampling from Gibbs distributions. Some works (Kumar et al., 2022;
Qin et al., 2022; Amini et al., 2023; Du et al., 2023) also adapt the Hamiltonian MCMC algorithms
originally designed for high-dimensional continuous distributions for the discrete scenario (Duane
et al., 1987; Neal, 2011). Furthermore, Hu et al. (2024) apply GFlowNets (Bengio et al., 2021)
to fine-tune language models for solving posterior inference problems, which can be considered
as sampling in proportion to an intractable Gibbs distribution. In our work, we instead aim to use
MCMC for MT to sample translations in proportion to a sentence-level evaluation metric."
RELATED WORK,0.3698224852071006,"Diverse Decoding for Machine Translation.
Variants of beam search (Cho, 2016; Vijayakumar
et al., 2017; Kulikov et al., 2019; Tam, 2020) have been proposed to produce a diverse set of
translations using diversity-promoting objectives. However, the increased computation cost with the
model size and beam width makes it infeasible and impractical to use with LLMs and it fails to yield
consistent improvement over ancestral with an increase in beam width (Stahlberg and Byrne, 2019;
Eikema and Aziz, 2020; Pang et al., 2024). Quality-aware decoding approaches on the other hand
are almost always used to generate a single best hypothesis. Concurrently, Jinnai et al. (2024) add
diversity promoting objective to MBR decoding to generate a set of high-quality diverse candidates.
However, their approach only allows for the selection of hypotheses from a predefined set. We further
note that we do not directly promote diversity‚Äîrather, diverse translations are a side product of
efficiently exploring multiple high-quality regions from the model‚Äôs distribution."
CONCLUSION,0.3727810650887574,"7
Conclusion"
CONCLUSION,0.3757396449704142,"We propose a new decoding approach for MT, Quality-Aware Metropolis-Hastings (QUEST) sampling
based on MCMC that enables the generation of hypotheses in proportion to an automatic QE metric.
We present a simple and novel proposal distribution that satisfies the constraints imposed by the
Metropolis-Hastings algorithm. Our experiments on four language directions and two strong decoder-
only language models show the efficacy of our approach over baselines. We further show that our"
CONCLUSION,0.378698224852071,"approach results in samples from underexplored high-density regions and that the average quality
continues to improve as the Markov chain size increases."
LIMITATIONS AND BROAD IMPACT,0.3816568047337278,"8
Limitations and Broad Impact"
LIMITATIONS AND BROAD IMPACT,0.38461538461538464,"QUEST requires generating many samples to reach high-density regions sequentially from an LLM
for each input prompt, which can be computationally expensive for time-critical applications. Addi-
tionally, the required number of steps may vary depending on the input and the quality of the initial
hypothesis. Furthermore, our proposal distribution only modifies the sentence suffix, which becomes
restrictive once the chain reaches a high-quality region. As at this point, only minor changes to the
hypothesis are accepted, slowing the mixing process. Addressing these limitations and extending this
approach to other NLP tasks are avenues for future work."
LIMITATIONS AND BROAD IMPACT,0.3875739644970414,"Furthermore, we leverage recent advances in QE methods for MT and integrate them directly
in the generation process of LLMs, which can potentially reduce the errors generated by these
systems. However, despite the high correlations of evaluation metrics with human judgments, they
are sometimes hard to interpret and occasionally fail to detect simple mistakes such as incorrect
translations of numbers or entities (Amrhein and Sennrich, 2022). In such cases, sampling from
the Gibbs distributions induced by these metrics might increase the chances of sampling those
erroneous translations. We believe these risks will be mitigated as better metrics are constantly being
developed‚Äîour method, being agnostic to the specific quality metric, will directly benefit from it. In
addition, since QUEST supports any Gibbs distribution, it can also incorporate multiple QE models or
additional checks which can rule out problematic samples by assigning them a very low QE score."
LIMITATIONS AND BROAD IMPACT,0.3905325443786982,Acknowledgments
LIMITATIONS AND BROAD IMPACT,0.39349112426035504,"We thank Ben Peters, Marcos Treviso, and Sergey Troshin for their helpful and constructive feedback
on the initial versions of the paper. Part of this work was supported by the EU‚Äôs Horizon Europe
Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE
(ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project
C645008882- 00000055 (Center for Responsible AI), and by Funda√ß√£o para a Ci√™ncia e Tecnologia
through contract UIDB/50008/2020."
REFERENCES,0.39644970414201186,References
REFERENCES,0.3994082840236686,"Duarte M. Alves, Jos√© Pombal, Nuno M. Guerreiro, Pedro H. Martins, Jo√£o Alves, Amin Farajian,
Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, Jos√© G. C. de Souza,
and Andr√© F. T. Martins. 2024. Tower: An open multilingual large language model for translation-
related tasks."
REFERENCES,0.40236686390532544,"Afra Amini, Li Du, and Ryan Cotterell. 2023. Structured voronoi sampling."
REFERENCES,0.40532544378698226,"Chantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics
through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd
Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the
12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pages 1125‚Äì1141, Online only. Association for Computational Linguistics."
REFERENCES,0.40828402366863903,"Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. 2021. Flow
network based generative models for non-iterative diverse candidate generation."
REFERENCES,0.41124260355029585,"Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo K√§rkk√§inen, Akos Vetek, and Juha Karhunen.
2015. Bidirectional recurrent neural networks as generative models - reconstructing gaps in time
series."
REFERENCES,0.41420118343195267,"Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, Jos√© G. C. de
Souza, Beatriz Silva, T√¢nia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and Andr√©
Martins. 2023. Findings of the WMT 2023 shared task on quality estimation. In Proceedings
of the Eighth Conference on Machine Translation, pages 629‚Äì653, Singapore. Association for
Computational Linguistics."
REFERENCES,0.4171597633136095,"Kyunghyun Cho. 2016. Noisy parallel approximate decoding for conditional recurrent language
model. arXiv preprint arXiv:1605.03835."
REFERENCES,0.42011834319526625,"Markus Dreyer and Daniel Marcu. 2012. Hyter: Meaning-equivalent semantics for translation evalua-
tion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 162‚Äì171."
REFERENCES,0.4230769230769231,"Li Du, Afra Amini, Lucas Torroba Hennigen, Xinyan Velocity Yu, Jason Eisner, Holden Lee, and
Ryan Cotterell. 2023. Principled gradient-based markov chain monte carlo for text generation."
REFERENCES,0.4260355029585799,"Simon Duane, A.D. Kennedy, Brian J. Pendleton, and Duncan Roweth. 1987. Hybrid monte carlo.
Physics Letters B, 195(2):216‚Äì222."
REFERENCES,0.4289940828402367,"Bryan Eikema. 2024. The effect of generalisation on the inadequacy of the mode. In Proceedings of
the 1st Workshop on Uncertainty-Aware NLP (UncertaiNLP 2024), pages 87‚Äì92, St Julians, Malta.
Association for Computational Linguistics."
REFERENCES,0.4319526627218935,"Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the
mode in neural machine translation. In Proceedings of the 28th International Conference on
Computational Linguistics, pages 4506‚Äì4520, Barcelona, Spain (Online). International Committee
on Computational Linguistics."
REFERENCES,0.4349112426035503,"Ant√≥nio Farinhas, Jos√© de Souza, and Andre Martins. 2023. An empirical study of translation
hypothesis ensembling with large language models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, pages 11956‚Äì11970, Singapore. Association
for Computational Linguistics."
REFERENCES,0.4378698224852071,"Patrick Fernandes, Ant√≥nio Farinhas, Ricardo Rei, Jos√© G. C. de Souza, Perez Ogayo, Graham Neubig,
and Andre Martins. 2022a. Quality-aware decoding for neural machine translation. In Proceedings
of the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 1396‚Äì1412, Seattle, United States. Association
for Computational Linguistics."
REFERENCES,0.4408284023668639,"Patrick Fernandes, Ant√≥nio Farinhas, Ricardo Rei, Jos√© G. C. de Souza, Perez Ogayo, Graham
Neubig, and Andr√© F. T. Martins. 2022b. Quality-aware decoding for neural machine translation."
REFERENCES,0.4437869822485207,"Patrick Fernandes, Aman Madaan, Emmy Liu, Ant√≥nio Farinhas, Pedro Henrique Martins, Amanda
Bertsch, Jos√© G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and Andr√© F. T.
Martins. 2023. Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language
Generation. Transactions of the Association for Computational Linguistics, 11:1643‚Äì1668."
REFERENCES,0.4467455621301775,"Jarad Forristal, Niloofar Mireshghallah, Greg Durrett, and Taylor Berg-Kirkpatrick. 2023. A block
metropolis-hastings sampler for controllable energy-based text generation."
REFERENCES,0.44970414201183434,"Markus Freitag, Behrooz Ghorbani, and Patrick Fernandes. 2023a. Epsilon sampling rocks: Investi-
gating sampling strategies for minimum Bayes risk decoding for machine translation. In Findings
of the Association for Computational Linguistics: EMNLP 2023, pages 9198‚Äì9209, Singapore.
Association for Computational Linguistics."
REFERENCES,0.4526627218934911,"Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022a. High quality rather than
high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the
Association for Computational Linguistics, 10:811‚Äì825."
REFERENCES,0.4556213017751479,"Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson,
Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho,
Alon Lavie, and George Foster. 2023b. Results of WMT23 metrics shared task: Metrics might
be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine
Translation, pages 578‚Äì628, Singapore. Association for Computational Linguistics."
REFERENCES,0.45857988165680474,"Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom
Kocmi, George Foster, Alon Lavie, and Andr√© F. T. Martins. 2022b. Results of WMT22 metrics
shared task: Stop using BLEU ‚Äì neural metrics are better and more robust. In Proceedings of
the Seventh Conference on Machine Translation (WMT), pages 46‚Äì68, Abu Dhabi, United Arab
Emirates (Hybrid). Association for Computational Linguistics."
REFERENCES,0.46153846153846156,"Dongyoung Go, Tomasz Korbak, Germ√°n Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymet-
man. 2023. Aligning language models with preferences through f-divergence minimization."
REFERENCES,0.46449704142011833,"Kartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick. 2022. Exposing the implicit energy networks
behind masked language models via metropolis‚Äìhastings."
REFERENCES,0.46745562130177515,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale
transformers for multilingual masked language modeling. In Proceedings of the 6th Workshop
on Representation Learning for NLP (RepL4NLP-2021), pages 29‚Äì33, Online. Association for
Computational Linguistics."
REFERENCES,0.47041420118343197,"Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement
scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation
Workshop and Interoperability with Discourse, pages 33‚Äì41, Sofia, Bulgaria. Association for
Computational Linguistics."
REFERENCES,0.47337278106508873,"David R Grimm, Yu-Jin Lee, Katherine Hu, Longsha Liu, Omar Garcia, Karthik Balakrishnan, and
Noel F Ayoub. 2024. The utility of chatgpt as a generative medical translator. European Archives
of Oto-Rhino-Laryngology, pages 1‚Äì5."
REFERENCES,0.47633136094674555,"Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and Andr√© F. T.
Martins. 2023a. xcomet: Transparent machine translation evaluation through fine-grained error
detection."
REFERENCES,0.47928994082840237,"Nuno Miguel Guerreiro, Duarte M Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre
Colombo, and Andr√© FT Martins. 2023b. Hallucinations in large multilingual translation models.
Transactions of the Association for Computational Linguistics, 11."
REFERENCES,0.4822485207100592,"Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu.
2024. Improving machine translation with human feedback: An exploration of quality estimation
as a reward model. arXiv preprint arXiv:2401.12873."
REFERENCES,0.48520710059171596,"Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Mat-
sushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt
models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210."
REFERENCES,0.4881656804733728,"Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio,
and Nikolay Malkin. 2024. Amortizing intractable inference in large language models."
REFERENCES,0.4911242603550296,"Yuu Jinnai, Ukyo Honda, Tetsuro Morimura, and Peinan Zhang. 2024. Generating diverse and
high-quality texts by minimum bayes risk decoding. arXiv preprint arXiv:2401.05054."
REFERENCES,0.4940828402366864,"Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H√ºllermeier. 2023. A survey of reinforcement
learning from human feedback. arXiv preprint arXiv:2312.14925."
REFERENCES,0.4970414201183432,"Elaine C Khoong, Eric Steinbrook, Cortlyn Brown, and Alicia Fernandez. 2019. Assessing the
use of google translate for spanish and chinese translations of emergency department discharge
instructions. JAMA internal medicine, 179(4):580‚Äì582."
REFERENCES,0.5,"Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, OndÀárej Bojar, Anton Dvorkovich, Christian
Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow,
Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto
Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovi¬¥c, and Mariya Shmatova. 2023. Findings
of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there
yet. In Proceedings of the Eighth Conference on Machine Translation, pages 1‚Äì42, Singapore.
Association for Computational Linguistics."
REFERENCES,0.5029585798816568,"Tom Kocmi, Vil√©m Zouhar, Christian Federmann, and Matt Post. 2024. Navigating the metrics maze:
Reconciling score magnitudes and accuracies. arXiv preprint arXiv:2401.06760."
REFERENCES,0.5059171597633136,"Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In
Proceedings of the First Workshop on Neural Machine Translation, pages 28‚Äì39, Vancouver.
Association for Computational Linguistics."
REFERENCES,0.5088757396449705,"Tomasz Korbak, Hady Elsahar, Germ√°n Kruszewski, and Marc Dymetman. 2022a. On reinforcement
learning and distribution matching for fine-tuning language models with no catastrophic forgetting."
REFERENCES,0.5118343195266272,"Tomasz Korbak, Ethan Perez, and Christopher L Buckley. 2022b. Rl with kl penalties is better viewed
as bayesian inference."
REFERENCES,0.514792899408284,"Ilia Kulikov, Alexander Miller, Kyunghyun Cho, and Jason Weston. 2019. Importance of search and
evaluation strategies in neural dialogue modeling. In Proceedings of the 12th International Confer-
ence on Natural Language Generation, pages 76‚Äì87, Tokyo, Japan. Association for Computational
Linguistics."
REFERENCES,0.5177514792899408,"Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. 2022. Gradient-based constrained sampling from
language models."
REFERENCES,0.5207100591715976,"Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language
model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on
Operating Systems Principles."
REFERENCES,0.5236686390532544,"Stephen Mayhew, Klinton Bicknell, Chris Brust, Bill McDowell, Will Monroe, and Burr Settles.
2020. Simultaneous translation and paraphrase for language education. In Proceedings of the
Fourth Workshop on Neural Generation and Translation, pages 232‚Äì243, Online. Association for
Computational Linguistics."
REFERENCES,0.5266272189349113,"Nicholas C. Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, and A. H. Teller. 1953.
Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087‚Äì
1092."
REFERENCES,0.5295857988165681,"Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. 2018. CGMH: constrained sentence generation
by metropolis-hastings sampling. CoRR, abs/1811.10996."
REFERENCES,0.5325443786982249,"Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor Berg-Kirkpatrick. 2022. Mix and match:
Learning-free controllable text generationusing energy language models. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 401‚Äì415, Dublin, Ireland. Association for Computational Linguistics."
REFERENCES,0.5355029585798816,Radford M. Neal. 2011. Probabilistic inference using markov chain monte carlo methods.
REFERENCES,0.5384615384615384,"Eugene Albert Nida. 1964. Toward a science of translating: with special reference to principles and
procedures involved in Bible translating. Brill Archive."
REFERENCES,0.5414201183431953,"Myle Ott, Michael Auli, David Grangier, and Marc‚ÄôAurelio Ranzato. 2018. Analyzing uncertainty in
neural machine translation. In International Conference on Machine Learning, pages 3956‚Äì3965.
PMLR."
REFERENCES,0.5443786982248521,"Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F Wong, Shuming Shi, and Zhaopeng Tu.
2024. Salute the classic: Revisiting challenges of machine translation in the age of large language
models. arXiv preprint arXiv:2401.08350."
REFERENCES,0.5473372781065089,"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311‚Äì318, Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics."
REFERENCES,0.5502958579881657,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. 2019. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning."
REFERENCES,0.5532544378698225,"Stefano Perrella, Lorenzo Proietti, Alessandro Scir√®, Niccol√≤ Campolungo, and Roberto Navigli.
2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In Proceedings of
the Seventh Conference on Machine Translation (WMT), pages 569‚Äì577, Abu Dhabi, United Arab
Emirates (Hybrid). Association for Computational Linguistics."
REFERENCES,0.5562130177514792,"Ben Peters and Andr√© F. T. Martins. 2021. Smoothing and shrinking the sparse Seq2Seq search space.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 2642‚Äì2654, Online. Association
for Computational Linguistics."
REFERENCES,0.5591715976331361,"Jan Peters and Stefan Schaal. 2007. Reinforcement learning by reward-weighted regression for
operational space control. In Proceedings of the 24th International Conference on Machine
Learning, ICML ‚Äô07, page 745‚Äì750, New York, NY, USA. Association for Computing Machinery."
REFERENCES,0.5621301775147929,"Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. Cold decoding: Energy-based
constrained text generation with langevin dynamics."
REFERENCES,0.5650887573964497,"Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners."
REFERENCES,0.5680473372781065,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21:1‚Äì67."
REFERENCES,0.5710059171597633,"Ricardo Rei, Jos√© G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,
Alon Lavie, Luisa Coheur, and Andr√© F. T. Martins. 2022. COMET-22: Unbabel-IST 2022
submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine
Translation (WMT), pages 578‚Äì585, Abu Dhabi, United Arab Emirates (Hybrid). Association for
Computational Linguistics."
REFERENCES,0.5739644970414202,"Ricardo Rei, Nuno M. Guerreiro, Jos√É¬© Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur,
Jos√© G. C. de Souza, and Andr√© Martins. 2023. Scaling up CometKiwi: Unbabel-IST 2023
submission for the quality estimation shared task. In Proceedings of the Eighth Conference on
Machine Translation, pages 841‚Äì848, Singapore. Association for Computational Linguistics."
REFERENCES,0.5769230769230769,"Luc√≠a Sanz-Valdivieso and Bel√©n L√≥pez-Arroyo. 2023. Google translate vs. chatgpt: Can non-
language professionals trust them for specialized translation? In International Conference Human-
informed Translation and Interpreting Technology (HiT-IT 2023), pages 97‚Äì107."
REFERENCES,0.5798816568047337,"Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine
translation. In Proceedings of the Human Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 177‚Äì184,
Boston, Massachusetts, USA. Association for Computational Linguistics."
REFERENCES,0.5828402366863905,"Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016.
Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683‚Äì1692,
Berlin, Germany. Association for Computational Linguistics."
REFERENCES,0.5857988165680473,"Tianxiao Shen, Myle Ott, Michael Auli, and Marc‚ÄôAurelio Ranzato. 2019. Mixture models for diverse
machine translation: Tricks of the trade. In International conference on machine learning, pages
5719‚Äì5728. PMLR."
REFERENCES,0.5887573964497042,"Yiqiu Shen, Laura Heacock, Jonathan Elias, Keith D Hentel, Beatriu Reig, George Shih, and Linda
Moy. 2023. Chatgpt and other large language models are double-edged swords."
REFERENCES,0.591715976331361,"Prasann Singhal, Jiacheng Xu, Xi Ye, and Greg Durrett. 2023. Eel: Efficiently encoding lattices for
reranking."
REFERENCES,0.5946745562130178,"Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue?
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pages 3356‚Äì3362, Hong Kong, China. Association for Computational Linguistics."
REFERENCES,0.5976331360946746,"Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In
NeurIPS."
REFERENCES,0.6005917159763313,"Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback."
REFERENCES,0.6035502958579881,"Jinyue Su, Jiacheng Xu, Xipeng Qiu, and Xuanjing Huang. 2018. Incorporating discriminator in
sentence generation: a gibbs sampling method."
REFERENCES,0.606508875739645,"Breena R Taira, Vanessa Kreger, Aristides Orue, and Lisa C Diamond. 2021. A pragmatic assessment
of google translate for emergency department instructions. Journal of General Internal Medicine,
36(11):3361‚Äì3365."
REFERENCES,0.6094674556213018,"Yik-Cheung Tam. 2020. Cluster-based beam search for pointer-generator chatbot grounded by
knowledge. Computer Speech & Language, 64:101094."
REFERENCES,0.6124260355029586,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models."
REFERENCES,0.6153846153846154,"Giorgos Vernikos and Andrei Popescu-Belis. 2024. Don‚Äôt rank, combine! combining machine
translation hypotheses using quality estimation. arXiv preprint arXiv:2401.06688."
REFERENCES,0.6183431952662722,"Ashwin K Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2017. Diverse beam search: Decoding diverse solutions from neural
sequence models."
REFERENCES,0.621301775147929,"Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a markov
random field language model. CoRR, abs/1902.04094."
REFERENCES,0.6242603550295858,"John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019.
Beyond
BLEU:training neural machine translation with semantic similarity. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, pages 4344‚Äì4355, Florence,
Italy. Association for Computational Linguistics."
REFERENCES,0.6272189349112426,"Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a. A paradigm shift in
machine translation: Boosting translation performance of large language models."
REFERENCES,0.6301775147928994,"Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton
Murray, and Young Jin Kim. 2024b. Contrastive preference optimization: Pushing the boundaries
of llm performance in machine translation. arXiv preprint arXiv:2401.08417."
REFERENCES,0.6331360946745562,"Takateru Yamakoshi, Thomas Griffiths, and Robert Hawkins. 2022. Probing BERT‚Äôs priors with
serial reproduction chains. In Findings of the Association for Computational Linguistics: ACL
2022, pages 3977‚Äì3992, Dublin, Ireland. Association for Computational Linguistics."
REFERENCES,0.636094674556213,"Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Mingxuan Wang. 2023.
BLEURT has universal translations: An analysis of automatic metrics by minimum risk training. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pages 5428‚Äì5443, Toronto, Canada. Association for Computational Linguistics."
REFERENCES,0.6390532544378699,"Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue. 2020. Language generation via combinatorial
constraint satisfaction: A tree search enhanced Monte-Carlo approach. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020, pages 1286‚Äì1298, Online. Association for
Computational Linguistics."
REFERENCES,0.6420118343195266,"A
Connection to RLHF derivation"
REFERENCES,0.6449704142011834,If we take the target distribution to have the following form:
REFERENCES,0.6479289940828402,"œÄ(y|x) =
1
Z(x)pLM(y|x) exp"
REFERENCES,0.650887573964497,"r(x, y) Œ≤ !"
REFERENCES,0.6538461538461539,",
(14)"
REFERENCES,0.6568047337278107,then the likelihood ratio becomes :
REFERENCES,0.6597633136094675,"œÄ(y|x)
œÄ(yt|x) = exp
r(x, y) ‚àír(x, yt) Œ≤"
REFERENCES,0.6627218934911243,"pLM(y‚â•i|yt
<i, x)
pLM(yt
‚â•i|yt
<i, x).
(15)"
REFERENCES,0.665680473372781,"Note that the target log ratio contains the inverse of the probability of returning, i.e.:"
REFERENCES,0.6686390532544378,"q(yt|y, x, i)
q(y|yt, x, i) = pLM(yt
‚â•i|yt
<i, x)
pLM(y‚â•i|yt
<i, x),
(16)"
REFERENCES,0.6715976331360947,this allows simplifying the criterion as follows:
REFERENCES,0.6745562130177515,"Œ±Œ≤(y, yt) = min

1, exp
r (x, y) ‚àír (x, yt) Œ≤"
REFERENCES,0.6775147928994083, q(i|n)
REFERENCES,0.6804733727810651,q(i|nt)
REFERENCES,0.6834319526627219,"
.
(17)"
REFERENCES,0.6863905325443787,"B
Prompts used for MT"
REFERENCES,0.6893491124260355,"For both the ALMA-7B and TOWER-7B we adhere to the prompt format recommended for transla-
tion in the original papers as shown below:"
REFERENCES,0.6923076923076923,ALMA-7B:
REFERENCES,0.6952662721893491,"Translate this sentence from {source language} to {target language}.
{source language} Source: {source sentence}.
{target language} Translation:"
REFERENCES,0.6982248520710059,TOWER-7B
REFERENCES,0.7011834319526628,"<|im_start|>user
Translate the following {source_language} source text to {target_language}:
{source_language}: {source_sentence}
{target_language}: <|im_end|>
<|im_start|>assistant"
REFERENCES,0.7041420118343196,"C
Additional Results"
REFERENCES,0.7071005917159763,"C.1
Toy problem: Validating the Approach"
REFERENCES,0.7100591715976331,"To validate that our approach works, we test QUEST on a controlled setting, a toy summarization
problem, where the ground truth reward is known. We consider the following reward function:"
REFERENCES,0.7130177514792899,"r(x, y) := logit
 
clamp
 
N
 
|y|; ¬µ = 7.5, œÉ2 = 3.752
,
(18)"
REFERENCES,0.7159763313609467,"where clamp(x) = max(10‚àí2, min(x, 1 ‚àí10‚àí2))."
REFERENCES,0.7189349112426036,"We use GPT2 (Radford et al., 2019) fine-tuned on the Reddit dataset Stiennon et al. (2020) to generate
summaries for 128 examples randomly sampled from the test split. We compare the distribution of
rewards obtained by different sampling techniques (Ancestral, QUEST and Truncated-Gibbs) to the
ground truth reward in Figure 4a. For Truncated-Gibbs sampling, we estimate the partition function
using the ancestral samples and resample them based on the probability distribution induced by the
rewards. Unlike ancestral sampling which results in samples that are far from the high-reward regions,
our sampling strategy, QUEST, results in the best approximation of the ground truth distribution.
This simplified reward task serves as an useful example to show the efficacy of our approach over
alternatives."
REFERENCES,0.7218934911242604,"(a) Distribution of rewards for sampled hypotheses for
the toy summarization problem."
REFERENCES,0.7248520710059172,"(a) XCOMET-XL by source length.
(b) Distribution over Accepted indices."
REFERENCES,0.727810650887574,"C.2
MCMC results in better outputs for shorter segments."
REFERENCES,0.7307692307692307,"We show the XCOMET-XL scores on WMT23 English-German pairs bucketed by the length of
the source text. In general, the quality of samples degrades with increased source length when
decoding via MCMC as shown in Figure 5a. One possible factor could be the uniform index-
selection exploration strategy where the proposal might require more steps as the sentence length
increases. Another factor could be the limitations of the reward function itself for selecting high-
quality translations for longer sequences. As these quality estimation metrics have not been trained
on longer sequences, due to a distribution shift, they might not be representative of true quality for
these output lengths. We believe the latter option could be the more significant factor as the reward
optimized still improves regardless of the sentence length (see Figure 7)."
REFERENCES,0.7337278106508875,"C.3
Accepted proposals are skewed towards the end of the sentence."
REFERENCES,0.7366863905325444,"Figure 5b illustrates how the distribution of accepted indices changes with decreasing Œ≤: lower Œ≤
values tighten acceptance criteria, favoring states with higher rewards and a greater likelihood of
returning to the previous state. Consequently, transitions tend to yield minor alterations, typically at
the end of the sentence. Altering the beginning necessitates sampling small indices values, risking a
complete sentence change. This plot highlights the current limitations of our proposal, especially the
downstream implications on the diversity of prefixes we get for a particular motivating the use of
parallel chains and potential avenues for improvement for future work."
REFERENCES,0.7396449704142012,"C.4
Comparing RLHF-QUEST vs QUEST"
REFERENCES,0.742603550295858,"We compare QUEST with the alternative discussed in Section 3.3 where the target distribution in Eq. 3
is replaced with Eq. 12, referred to as ‚ÄúRLHF-QUEST‚Äù. We compare sampling using the two target
distributions in Figure 6. Our results indicate that both approaches result in comparable translation
quality, with QUEST resulting in slightly higher diversity on average than RLHF-QUEST. We leave
the detailed exploration of these two methods to future work."
REFERENCES,0.7455621301775148,"Figure 6: Average Quality by XCOMET-XL (left) and COMETKIWI-XL on English-Russian dataset
using TOWER-7B"
REFERENCES,0.7485207100591716,"C.5
QE Results"
REFERENCES,0.7514792899408284,"We report the quality as measured by the metric used in sampling, i.e., COMETKIWI-XL in Figure 7.
QUEST results in higher quality across the board compared to ancestral sampling."
REFERENCES,0.7544378698224852,"C.6
Additional Language Pairs"
REFERENCES,0.757396449704142,"We further expanded our evaluation to four additional language pairs, as shown in Figure 8: WMT23
English-Chinese (EN‚ÜíZH, high-resource), WMT23 English-Czech (EN‚ÜíCS, medium-resource),
WMT22 English-Icelandic (EN‚ÜíIS), and Icelandic-English (IS‚ÜíEN, low-resource), using ALMA-7B.
We did not include TOWER in these experiments because it was trained on the WMT22 test sets.
Across all language pairs, QUEST consistently outperforms ancestral sampling, providing better
quality-diversity trade-offs."
REFERENCES,0.7603550295857988,"C.7
COMET22 Results"
REFERENCES,0.7633136094674556,"We report the quality as measured by COMET22 (Rei et al., 2022) in Figure 9. QUEST results in
higher quality across the board compared to ancestral sampling."
REFERENCES,0.7662721893491125,"Figure 7: Average quality (COMETKIWI-XL) vs. diversity (PAIRWISE-BLEU) on WMT23 datasets.
Different points represent different hyperparameter values."
REFERENCES,0.7692307692307693,"Figure 8: Average quality (XCOMET-XL) vs. diversity (PAIRWISE-BLEU) on additional LPs from
WMT23 and WMT22. Different points represent different hyperparameter values."
REFERENCES,0.772189349112426,"Figure 9: Average quality (COMET22) vs. diversity (PAIRWISE-BLEU) on EN‚ÜíRU and RU‚ÜíEN.
Different points represent different hyperparameter values."
REFERENCES,0.7751479289940828,NeurIPS Paper Checklist
CLAIMS,0.7781065088757396,1. Claims
CLAIMS,0.7810650887573964,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: We present the main contributions and results in the last two paragraphs of the
appendix which are supported by experiments detailed in Section 4 and findings presented
in Section 5.2.
Guidelines:"
CLAIMS,0.7840236686390533,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.7869822485207101,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We present some of the limitations of our work in the ablation analysis
presented in Section 5.2, additionally, we list other limitations in Section 8.
Guidelines:"
CLAIMS,0.7899408284023669,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.7928994082840237,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.7958579881656804,Answer: [NA]
CLAIMS,0.7988165680473372,Justification: [NA]
CLAIMS,0.8017751479289941,Guidelines:
CLAIMS,0.8047337278106509,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8076923076923077,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8106508875739645,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8136094674556213,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165680473372781,Justification: We provide all necessary details to reproduce the experiments in Section 4.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8195266272189349,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8224852071005917,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8254437869822485,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8284023668639053,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We use publicly available models and datasets in our experiments. We release
the code to replicate our experiments at https://www.questdecoding.com/.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8313609467455622,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.834319526627219,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Section 4. Our proposed method works at inference time. We report
the test data used and all the hyperparameters in the beginning of Section 4 in dedicated
paragraphs.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8372781065088757,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.8402366863905325,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We show all our results aggregated over a sample size of 500-2000 source
sentences across multiple language pairs and models. Furthermore, we report statistical
significance for results using the optimized QE metric in Section 5 and Appendix C.5.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8431952662721893,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
OPEN ACCESS TO DATA AND CODE,0.8461538461538461,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.849112426035503,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8520710059171598,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8550295857988166,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.8579881656804734,"Justification: We discuss the compute used and run time in a dedicated paragraph in
Section 4."
EXPERIMENTS COMPUTE RESOURCES,0.8609467455621301,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.863905325443787,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.8668639053254438,9. Code Of Ethics
CODE OF ETHICS,0.8698224852071006,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.8727810650887574,Answer: [Yes]
CODE OF ETHICS,0.8757396449704142,Justification: Our research conforms with the NeurIPS Code of Ethics.
CODE OF ETHICS,0.878698224852071,Guidelines:
CODE OF ETHICS,0.8816568047337278,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.8846153846153846,10. Broader Impacts
BROADER IMPACTS,0.8875739644970414,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.8905325443786982,Answer: [Yes]
BROADER IMPACTS,0.893491124260355,Justification: Please refer to Section 8.
BROADER IMPACTS,0.8964497041420119,Guidelines:
BROADER IMPACTS,0.8994082840236687,‚Ä¢ The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.9023668639053254,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9053254437869822,11. Safeguards
SAFEGUARDS,0.908284023668639,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9112426035502958,Answer: [NA]
SAFEGUARDS,0.9142011834319527,"Justification: Our work poses no such risks. We utilize existing models and resources to
improve translation quality."
SAFEGUARDS,0.9171597633136095,Guidelines:
SAFEGUARDS,0.9201183431952663,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9230769230769231,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9260355029585798,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9289940828402367,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9319526627218935,"Justification: We provide all details with citations for the WMT23 dataset and the models
(ALMA-7b, Tower-7b) in dedicated paragraphs in Section 4."
LICENSES FOR EXISTING ASSETS,0.9349112426035503,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9378698224852071,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9408284023668639,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9437869822485208,13. New Assets
NEW ASSETS,0.9467455621301775,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9497041420118343,Answer: [NA]
NEW ASSETS,0.9526627218934911,Justification: We release the code at https://www.questdecoding.com/.
NEW ASSETS,0.9556213017751479,Guidelines:
NEW ASSETS,0.9585798816568047,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9615384615384616,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9644970414201184,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674556213017751,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704142011834319,Justification: We do not conduct any human study.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733727810650887,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763313609467456,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792899408284024,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822485207100592,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985207100591716,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881656804733728,Justification: We do not conduct any human study.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911242603550295,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940828402366864,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970414201183432,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
