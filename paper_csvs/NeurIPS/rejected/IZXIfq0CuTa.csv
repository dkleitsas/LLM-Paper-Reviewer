Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019267822736030828,"In this paper, we propose a novel highly parallel deep ensemble learning, which
1"
ABSTRACT,0.0038535645472061657,"leads to highly compact and parallel deep neural networks. The main idea is to first
2"
ABSTRACT,0.005780346820809248,"represent the data in tensor form, apply a linear transform along certain dimension
3"
ABSTRACT,0.007707129094412331,"and split the transformed data into different independent spectral data sets; then
4"
ABSTRACT,0.009633911368015413,"the matrix product in conventional neural networks is replaced by tensor product,
5"
ABSTRACT,0.011560693641618497,"which in effect imposes certain transformed-induced structure on the original
6"
ABSTRACT,0.01348747591522158,"weight matrices, e.g., a block-circulant structure. The key feature of the proposed
7"
ABSTRACT,0.015414258188824663,"spectral tensor network is that it consists of parallel branches with each branch being
8"
ABSTRACT,0.017341040462427744,"an independent neural network trained using one spectral subset of the training
9"
ABSTRACT,0.019267822736030827,"data. Besides, the joint data/model parallel amiable for GPU implementation.
10"
ABSTRACT,0.02119460500963391,"The outputs of the parallel branches, which are trained on different independent
11"
ABSTRACT,0.023121387283236993,"spectral, are combined for ensemble learning to produce an overall network with
12"
ABSTRACT,0.025048169556840076,"substantially stronger generalization capability than that of those parallel branches.
13"
ABSTRACT,0.02697495183044316,"Moreover, benefiting from the reducing size of inputs, the proposed spectral tensor
14"
ABSTRACT,0.028901734104046242,"network exhibits an inherent network compression, and as a result, reduction
15"
ABSTRACT,0.030828516377649325,"in computation complexity, which leads to the acceleration of training process.
16"
ABSTRACT,0.03275529865125241,"The high parallelism from the massive independent operations of the parallel
17"
ABSTRACT,0.03468208092485549,"spectral subnetworks enable a further acceleration in training and inference process.
18"
ABSTRACT,0.036608863198458574,"We evaluate the proposed spectral tensor networks on the MNIST, CIFAR-10
19"
ABSTRACT,0.038535645472061654,"and ImageNet data sets, to highlight that they simultaneously achieve network
20"
ABSTRACT,0.04046242774566474,"compression, reduction in computation and parallel speedup.
21"
INTRODUCTION,0.04238921001926782,"1
Introduction
22"
INTRODUCTION,0.04431599229287091,"Deep neural networks (DNNs) [1] have made impressive successes in many applications, such
23"
INTRODUCTION,0.046242774566473986,"as computer vision [2][3][4], online game [5][6][7], natural language processing [8][9][10], au-
24"
INTRODUCTION,0.04816955684007707,"tonomous driving [11][12][13], and robotics [14][15][16]. However, DNNs are memory-intensive
25"
INTRODUCTION,0.05009633911368015,"and computation-intensive, which are two major challenges for wider adoption, e.g., in Internet of
26"
INTRODUCTION,0.05202312138728324,"Things (IoT) applications [17]. Modern DNNs may have billions of parameters that consume exces-
27"
INTRODUCTION,0.05394990366088632,"sive amount of memory and usually require long training time. For example, AlexNet [18] consists
28"
INTRODUCTION,0.055876685934489405,"of three fully-connected layers and five convolutional layers, containing 60 million parameters and
29"
INTRODUCTION,0.057803468208092484,"consuming about 250 MB of memory and about 40 hours for training on ImageNet data set [19].
30"
INTRODUCTION,0.05973025048169557,"To mitigate the impact of memory and computation intensive, there are two types of techniques
31"
INTRODUCTION,0.06165703275529865,"most investigated by researchers, namely model compression and parallel training. The model
32"
INTRODUCTION,0.06358381502890173,"compression methods, which represents the conventional neural layers in DNNs by compact layers,
33"
INTRODUCTION,0.06551059730250482,"such as the strucred linear layers, tensor layers, etc., are one of the most efficient methods to reduce
34"
INTRODUCTION,0.0674373795761079,"memory consumption of DNNs. Besides, the proposed parallel training methods can be generally
35"
INTRODUCTION,0.06936416184971098,"categorized into data parallelism and model parallelism methods. It should be noted that tensor
36"
INTRODUCTION,0.07129094412331406,"layer provides potential parallelism for efficient GPU computing as well as the reduction of the
37"
INTRODUCTION,0.07321772639691715,"Table 1: Different tensor layers. An N 2 × N 2 weight matrix is organized into a d-th order tensor
with the size of each dimension n, i.e., N 4 = nd, and r is the tensor rank."
INTRODUCTION,0.07514450867052024,"Methods
Model size
Inference time
Fully connected layer
O(N 4)
O(N 4)
Low-rank matrix [29]
O(N 2r)
O(N 2r)
CP tensor layer [31]
O(nr)
O(nr)
Tucker tensor layer [32]
O(nr + r3)
O(nr)
HT tensor layer [32]
O(dn2r + rd)
O(dnr2N 2)
Tensor-train layer [29]
O(dnr2)
O(dr3N 2)"
INTRODUCTION,0.07707129094412331,"memory consumption. Thus, it naturally motivates us to employ tensor layer to achieve both the
38"
INTRODUCTION,0.0789980732177264,"model compression and parallel computing.
39"
INTRODUCTION,0.08092485549132948,"In this paper, we propose a unified approach that simultaneously achieves both model compression
40"
INTRODUCTION,0.08285163776493257,"and parallel learning without communication overhead. The key technique is a novel spectral tensor
41"
INTRODUCTION,0.08477842003853564,"layer that enables a joint data/model-parallel implementation of a DNN as follows: 1) The training
42"
INTRODUCTION,0.08670520231213873,"data set is split into multiple orthogonal spectral sets; 2) The neural network is split into parallel
43"
INTRODUCTION,0.08863198458574181,"branches with each branch being a conventional neural network, that are trained asynchronously and
44"
INTRODUCTION,0.0905587668593449,"independently on the corresponding spectral sets; 3) The outputs of the parallel branches are finally
45"
INTRODUCTION,0.09248554913294797,"combined to yield an overall neural network with substantially stronger generalization capability than
46"
INTRODUCTION,0.09441233140655106,"that of those parallel branches.
47"
INTRODUCTION,0.09633911368015415,"The remainder of this paper is organized as follows. Section 2 presents an brief overview of the
48"
INTRODUCTION,0.09826589595375723,"study on the model compression and parallel computing. Section 3 presents multiple spectral tensor
49"
INTRODUCTION,0.1001926782273603,"layers, including fully connected layers, convolution layers and recurrent layers. Section 4 presents
50"
INTRODUCTION,0.10211946050096339,"the experimental results and we conclude this paper in Section 5.
51"
RELATED WORKS,0.10404624277456648,"2
Related Works
52"
RELATED WORKS,0.10597302504816955,"Network compression: Consider a linear layer that is a central building block of modern DNNs,
53"
RELATED WORKS,0.10789980732177264,"where an input x ∈RN 2 (e.g., an N × N image) is transformed by a weight matrix W ∈RN2×N2
54"
RELATED WORKS,0.10982658959537572,"[1], i.e., y = W x ∈RN2. The memory size for storing W is O(N 4) and the computational
55"
RELATED WORKS,0.11175337186897881,"complexity of the matrix-vector product is also O(N 4), both are very demanding for smartphones,
56"
RELATED WORKS,0.11368015414258188,"robots, and embedded devices.
57"
RELATED WORKS,0.11560693641618497,"Many works [20][21][22][23] showed that over 95% of the parameters are redundant, thus the
58"
RELATED WORKS,0.11753371868978806,"network can be greatly compressed. The structured linear layer imposes certain structures on W ,
59"
RELATED WORKS,0.11946050096339114,"including circulant [21][22][24], circulant-block [25][26], and Teoplitz-like structures [21][27][23].
60"
RELATED WORKS,0.12138728323699421,"For example, for a circulant weight matrix W [21][22][27][24], the memory size becomes O(N 2)
61"
RELATED WORKS,0.1233140655105973,"and the computational complexity becomes O(N 2 log N), at the expense of a slight drop in the
62"
RELATED WORKS,0.1252408477842004,"inference performance.
63"
RELATED WORKS,0.12716763005780346,"On the other hand, the tensor layer exploits different types of low-rank tensor representations of
64"
RELATED WORKS,0.12909441233140656,"the weight matrix W [28][29][30][31][32]. For example, if the weight matrix W ∈RN 2×N2 has
65"
RELATED WORKS,0.13102119460500963,"rank r ≪N 2, such that W = AB, A ∈RN 2×r, B ∈Rr×N2, then the memory size becomes
66"
RELATED WORKS,0.1329479768786127,"O(2N 2r), and the computational complexity becomes O(2N 2r). Table 1 summarizes the model size
67"
RELATED WORKS,0.1348747591522158,"and computational complexity for various tensor representation schemes.
68"
RELATED WORKS,0.13680154142581888,"Parallel training: Existing works on parallel machine learning take advantage of either data-
69"
RELATED WORKS,0.13872832369942195,"parallelism [33][34][35] or model-parallelism [36][37][38] or both [39]. In the data-parallel approach,
70"
RELATED WORKS,0.14065510597302505,"the data are distributed among multiple processors that apply the same model [33]. The processors
71"
RELATED WORKS,0.14258188824662812,"periodically exchange the outputs and gradients. In the model-parallel approach, exact copies of the
72"
RELATED WORKS,0.14450867052023122,"whole data set are processed by multiple processors that operate in parallel on different parts of the
73"
RELATED WORKS,0.1464354527938343,"same model [36]. There are frequent aggregation of model parts and distribution of gradients [40].
74"
RELATED WORKS,0.14836223506743737,"From the above discussion, we see that the computation associated with a DNN can be speed up via
75"
RELATED WORKS,0.15028901734104047,"two separate ways: one is through model compression so that reduced number of weight parameters
76"
RELATED WORKS,0.15221579961464354,"leads to reduced number of multiplications and additions; and the other is through parallel computing,
77"
RELATED WORKS,0.15414258188824662,Figure 1: The framework of highly parallel deep ensemble learning.
RELATED WORKS,0.15606936416184972,"where the speedup is due to distributing the computation among parallel processors, at the expense of
78"
RELATED WORKS,0.1579961464354528,"communication overhead.
79"
HIGHLY PARALLEL SPECTRAL TENSOR NETWORKS,0.1599229287090559,"3
Highly Parallel Spectral Tensor Networks
80"
OVERVIEW,0.16184971098265896,"3.1
Overview
81"
OVERVIEW,0.16377649325626203,"As shown in Fig.1, a batch of data is firstly pre-processed to the spectral dataset and splitted into
82"
OVERVIEW,0.16570327552986513,"Q independent subsets. Then, Q different subnetwork, namely spectral tensor layer, works on Q
83"
OVERVIEW,0.1676300578034682,"independent joint spectral dataset to output own result. Last, the final result is the ensemble result
84"
OVERVIEW,0.16955684007707128,"from Q subnetworks. Tne subnetwork family includes the fully connected spectral tensor layer,
85"
OVERVIEW,0.17148362235067438,"convolutional spectral tensor layer and recurrent spectral tensor layer. The feature of joint data and
86"
OVERVIEW,0.17341040462427745,"model parallel makes it suitable for computing on GPUs. The operations in the forward pass, such as
87"
OVERVIEW,0.17533718689788053,"t-product further provides potential high parallelism for acceleration on GPUs.
88"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.17726396917148363,"3.2
Notations and Basic Tensor Operations
89"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.1791907514450867,"Scalars, vectors, matrices and tensors are denoted by lowercase, boldface lowercase, boldface capital,
90"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.1811175337186898,"and calligraphic letters, e.g., a ∈R, a ∈Rn, A ∈Rn1×n2, A ∈Rn1×n2×n3, respectively. We use
91"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.18304431599229287,"A(:, :, k), A(:, j, :), A(i, :, :) to denote the frontal, lateral, and horizontal slices.
92"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.18497109826589594,"Given an invertible discrete linear transform L : Rn3 →Cn3, let L and its inverse L−1 be taken along
93"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.18689788053949905,"the third-dimension of third-order tensors. That is, for A ∈Rn1×n2×n3, e
A = L(A) ∈Cn1×n2×n3,
94"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.18882466281310212,"with e
A(i, j, :) = L(A(i, j, :)), i = 1, ..., n1, j = 1, ..., n2. And for e
A ∈Cn1×n2×n3, A = L−1( e
A),
95"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.1907514450867052,"with A(i, j, :) = L−1( e
A(i, j, :)), i = 1, ..., n1, j = 1, ..., n2. The general spectral tensor product
96"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.1926782273603083,"[41][42][43] is defined as
97"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.19460500963391136,"C = A • B = L−1( L(A) △L(B) ),
(1)"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.19653179190751446,"where △denotes the frontal-slice wise multiplication, i.e., for e
A ∈Cn1×n′×n3, eB ∈Cn′×n2×n3, if
98"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.19845857418111754,"eC = e
A △eB, then eC(:, :, k) = e
A(:, :, k) eB(:, :, k), k = 1, ..., n3. The t-product [44] is a special case
99"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.2003853564547206,"of (1) where the transform L is the DFT [45].
100"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.2023121387283237,"For a tensor A ∈Rn1×n2×n3, we define the following operations. bcirc(A) ∈Rn1n3×n2n3 organizes
101"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.20423892100192678,"its n3 frontal slices into a block-circulant matrix
102"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.20616570327552985,bcirc(A) =  
NOTATIONS AND BASIC TENSOR OPERATIONS,0.20809248554913296,"A(:, :, 1)
A(:, :, n3)
· · ·
A(:, :, 2)
A(:, :, 2)
A(:, :, 1)
· · ·
A(:, :, 3)
...
...
...
...
A(:, :, n3)
A(:, :, n3 −1)
· · ·
A(:, :, 1) "
NOTATIONS AND BASIC TENSOR OPERATIONS,0.21001926782273603,".
(2)"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.2119460500963391,"Further, unfold(·) is defined as
103"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.2138728323699422,"unfold(A) = [A(:, :, 1)T , · · · , A(:, :, n3)T ]T ∈Rn1n3×n2,"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.21579961464354527,"and fold(·) organizes it back to A, such that
104"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.21772639691714837,"fold(unfold(A)) = A.
(3)"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.21965317919075145,"Given A ∈Rn1×n′×n3 and B ∈Rn′×n2×n3, the t-product [45] can be expressed as follows
105"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.22157996146435452,"A ∗t B = fold(bcirc(A) · unfold(B)) ∈Rn1×n2×n3.
(4)"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.22350674373795762,"The vec(·) operation maps a matrix in Rn1×n2 into a vector in Rn1n2, while vec−1(·) is the inverse
106"
NOTATIONS AND BASIC TENSOR OPERATIONS,0.2254335260115607,"mapping.
107"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.22736030828516376,"3.3
Fully Connected Spectral Tensor Layer
108"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.22928709055876687,"A conventional N-layer fully connected network takes m input vectors each of size ℓ′
0 × 1 and
109"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.23121387283236994,"represents them as a matrix X0 ∈Rℓ′
0×m. For example, if each input vector represents a color
110"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.23314065510597304,"image of size n × n × 3, then ℓ′
0 = 3n2. Each input vector will be classified to one of the L classes.
111"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2350674373795761,"The network parameters at the j-th layer include a weight matrix W j ∈Rℓ′
j×ℓ′
j−1 and an offset
112"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.23699421965317918,"Bj = [bj, ..., bj]
|
{z
}
m"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.23892100192678228,"∈Rℓ′
j×m, and the forward pass can be represented as [1]
113"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.24084778420038536,"Xj = σ(W j · Xj−1 + Bj), j = 1, ..., N −1,
(5)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.24277456647398843,"where Xj ∈Rℓ′
j×m, and σ(·) is an element-wise activation function, e.g., linear, sigmoid, ReLU,
114"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.24470134874759153,"and softmax. The last, i.e., N-th, layer produces the output Y ∈RL×m corresponding to the m input
115"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2466281310211946,"vectors, where
116"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.24855491329479767,"XN = W N · XN−1,
(6)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2504816955684008,"Y = f(XN),
(7)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2524084778420039,"and the output function f(X) operates on the columns of X, i.e., f(X(:, s)) maps X(:, s) to an
117"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2543352601156069,"output score vector Y (:, s) ∈RL representing the probabilities that the s-th input data vector X0(:, s)
118"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.25626204238921,"belongs to different classes. For example f(·) can be a softmax.
119"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2581888246628131,"For our proposed fully connected spectral tensor network, the m input data vectors are organized as a
120"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.26011560693641617,"tensor X 0 ∈Rℓ0×m×Q. For the n × n × 3 color image example, we can set Q = 3n and ℓ0 = n,
121"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.26204238921001927,"namely each image is a lateral slice of X 0. Using the weight tensor Wj ∈Rℓj×ℓj−1×Q and offset
122"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.26396917148362237,"tensor Bj ∈Rℓj×m×Q, a fully connected tensor layer corresponding to (5) and (6) becomes
123"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2658959537572254,"X j = ϱ(Wj • X j−1 + Bj), j = 1, ..., N −1,
(8)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2678227360308285,"X N = WN • X N−1,
(9)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2697495183044316,"where X j ∈Rℓj×m×Q, the spectral tensor product • is given in (1), and the tensor-activation function
124"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.27167630057803466,"ϱ(·) under transform L is defined by applying the conventional element-wise activation function σ(·)
125"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.27360308285163776,"in the spectral domain, i.e.,
126"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.27552986512524086,"ϱ(X) = L−1(σ(L(X))).
(10)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2774566473988439,"A salient feature of the proposed spectral tensor network is the fully parallel implementation. Specifi-
127"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.279383429672447,"cally, for X ∈Rℓ×m×Q, denote e
X ∈Cℓ×m×Q as the transform of X along the third dimension, i.e.,
128"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2813102119460501,"e
X(i, s, :) = L(X(i, s, :)), i = 1, ..., ℓ, s = 1, ..., m. Denote further f
Xq = e
X(:, :, q). Then according
129"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2832369942196532,"to (1), (8)-(9) can be split into Q branches of matrix computations
130"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.28516377649325625,"f
Xj
q = σ

f
W j
q · f
Xj−1
q
+ e
Bj
q

,
(11)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.28709055876685935,"f
XN
q = f
W N
q · f
XN−1
q
, q = 1, ...., Q,
(12)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.28901734104046245,"where f
Xj
q ∈Cℓj×m, e
Bj
q = [ebj
q, ...,ebj
q]
|
{z
}
m"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2909441233140655,"∈Cℓj×m, and f
W j
q ∈Cℓj×ℓj−1.
131"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2928709055876686,"We further assume that the weight tensor Wj in (8)-(9) has low tubal-rank [45][46] such that Wj =
132"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.2947976878612717,"Cj • Dj, where Cj ∈Cℓj×r×Q, Dj ∈Cr×ℓj−1×Q, and r ≪min{ℓ0, ..., ℓN}. Correspondingly, the
133"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.29672447013487474,"weight matrix of each branch has low-rank structure, i.e.,
134"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.29865125240847784,"f
W j
q = e
Cj
q · e
Dj
q, q = 1, ..., Q,
(13)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.30057803468208094,"where e
Cj
q ∈Cℓj×r and e
Dj
q ∈Cr×ℓj−1. Then, (11)-(12) become
135"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.302504816955684,"eZj
q = e
Dj
q · f
Xj−1
q
, j = 1, ..., N,
(14)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3044315992292871,"f
Xj
q = σ

e
Cj
q · eZj
q + e
Bj
q

, j = 1, ..., N −1,
(15)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3063583815028902,"f
XN
q = e
CN
q · eZN
q ,
(16)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.30828516377649323,"where eZj
q ∈Cr×m, q = 1, ..., Q.
136"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.31021194605009633,"Therefore, an N-layer fully connected spectral tensor network in (8)-(9) is split into a 2N-layer
137"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.31213872832369943,"network, such that each layer in (11) is now implemented by two sub-layers, namely a linear layer
138"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3140655105973025,"(14) and a nonlinear layer (15), while the N-th layer in (12) is implemented by two linear sub-layers,
139"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3159922928709056,"namely (14) and (16). There are multiple parallel matrix multiplications with the same size and along
140"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3179190751445087,"the same dimension in Q subnetworks. Therefore, we employ the batch matrix multiplication using
141"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3198458574181118,"GPUs to accelerate the computations.
142"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3217726396917148,"Finally, we specify the network output. At each branch q, the output function f(·) is applied to the
143"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3236994219653179,"last layer output f
XN
q , as in (7), i.e.,
144"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.325626204238921,"Yq = f(f
XN
q ), q = 1, ..., Q.
(17)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.32755298651252407,"Finally the network output is the weighted sum of the outputs of the Q branches, i.e.,
145 Y = Q
X"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.32947976878612717,"q=1
ωqYq,
s.t. ωq ≥0, Q
X"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.33140655105973027,"q=1
ωq = 1.
(18)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3333333333333333,"The loss function can be a cross-entropy function as follows:
146"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3352601156069364,"Loss = − m
X s=1 L
X"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3371868978805395,"c=1
1(ys(c) = 1) · ln(Y (c, s)).
(19)"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.33911368015414256,"Once the spectral tensor network in Fig. 1 is trained, for inference, given a new data sample x ∈Rℓ0Q,
147"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.34104046242774566,"we first matricize it into X ∈Rℓ0×Q and then take transform along each row to obtain f
X. We input
148"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.34296724470134876,"the q-th column of f
X, i.e., f
X(:, q), to the q-th sub-network and obtain the output yq, q = 1, ..., Q.
149"
FULLY CONNECTED SPECTRAL TENSOR LAYER,0.3448940269749518,"The final output is then y = PQ
q=1 ωqyq.
150"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3468208092485549,"3.4
Convolutional Spectral Tensor Layer
151"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.348747591522158,"A convolutional neural network [1][47] takes m input images each of dimension H0 × W0 × C0, i.e.,
152"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.35067437379576105,"each image has a size H0×W0 and the number of channels is C0, and represents them as a fourth-order
153"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.35260115606936415,"tensor X0 ∈RH0×W0×C0×m. The input to the j-th layer is Xj−1 ∈RHj−1×Wj−1×Cj−1×m, which is
154"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.35452793834296725,"processed by a convolutional kernel Wj ∈RH′
j×W ′
j×Cj−1×Cj and an offset Bj ∈RH′′
j ×W ′′
j ×Cj×m,
155"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.35645472061657035,"to yield Yj ∈RH′′
j ×W ′′
j ×Cj×m, with H′′
j = Hj−1 −H′
j + 1, W ′′
j = Wj−1 −W ′
j + 1, where
156"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3583815028901734,"Yj(h, w, c, :) ="
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3603082851637765,"Cj−1
X d=1"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3622350674373796,"W ′
j−1
X ℓ=0"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.36416184971098264,"H′
j−1
X"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.36608863198458574,"i=0
Xj−1(h + i, w + ℓ, d, :) · Wj(i, ℓ, d, c) + Bj(h, w, c, :),"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.36801541425818884,"h = 1, ..., H′′
j , w = 1, ..., W ′′
j , c = 1, ..., Cj. (20)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3699421965317919,"To introduce the convolutional spectral tensor network, we represent (20) as a matrix product form
157"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.371868978805395,"that is similar to (5)
158"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3737957610789981,"Y j = W j · Xj−1 + Bj,
(21)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.37572254335260113,"where Y j ∈RH′′
j W ′′
j Cj×m, W j ∈RH′′
j W ′′
j Cj×Hj−1Wj−1Cj−1, and Xj−1 ∈RHj−1Wj−1Cj−1×m
159"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.37764932562620424,"are formed from Yj, Wj and Xj−1. In particular,
160"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.37957610789980734,"Y j(:, s) = vec(unfold(Yj(:, :, :, s))),"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3815028901734104,"Xj−1(:, s) = vec(unfold(Xj−1(:, :, :, s))), s = 1, ..., m.
(22)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3834296724470135,"Assume that Cj = nB for j = 0, ..., N, then similar to (8)-(9), (21) leads to a convolutional tensor
161"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3853564547206166,"layer
162"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3872832369942196,"Yj = Wj • X j−1 + Bj,
(23)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3892100192678227,"where Yj ∈RH′′
j W ′′
j n×m×B, Wj ∈RH′′
j W ′′
j n×Hj−1Wj−1n×B, X j ∈RHj−1Wj−1n×m×B, and
163"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3911368015414258,"Bj ∈RH′′
j W ′′
j n×m×B.
164"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3930635838150289,"Consider the case when L is DFT, according to (4), (23) can be written as (21) where
165"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.394990366088632,"Y j = unfold(Yj), Xj−1 = unfold(X j−1), Bj = unfold(Bj), and W j = bcirc(Wj) ∈
166"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3969171483622351,"RH′′
j W ′′
j nB×Hj−1Wj−1nB has a block-circulant structure, namely B × B blocks organized in a
167"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.3988439306358382,"circulant form and each block has size H′′
j W ′′
j n × Hj−1Wj−1n. Recall that W j in (21) is derived
168"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4007707129094412,"from the convolutional kernel Wj ∈RH′
j×W ′
j×nB×nB in (20), following a linear mapping that is
169"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4026974951830443,"consistent with (22). Therefore, the block-circulant structure of W j in (21) implies a block-circulant
170"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4046242774566474,"structure of each matrix Wj(i, ℓ, :, :) in (20).
171"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.40655105973025046,"Similar to the fully connected case in Section 3.3, the proposed convolutional spectral tensor network
172"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.40847784200385356,"also features a fully parallel implementation. Specifically, for X ∈RHW n×m×B, denote e
X =
173"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.41040462427745666,"L(X) ∈CHW n×m×B as the transform of X along the third dimension. Denote f
Xb = e
X(:, :, b).
174"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4123314065510597,"Then, (23) can be split into B parallel branches of matrix computations as follows
175"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4142581888246628,"eY j
b = f
W j
b · f
Xj−1
b
+ e
Bj
b, b = 1, ..., B,
(24)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4161849710982659,"where eY j
b ∈CH′′
j W ′′
j n×m, f
W j
b ∈CH′′
j W ′′
j n×Hj−1Wj−1n, and f
Xj−1
b
∈CHj−1Wj−1n×m. To fully
176"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.41811175337186895,"utilize the parallelism of B branches, we employ batch matrix multiplication on GPUs to accelerate
177"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.42003853564547206,"(24).
178"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.42196531791907516,"We convert (24) back to the convolutional form in (20), using an inverse mapping of (22) as follows
179"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4238921001926782,"eYj
b(:, :, :, s) = fold(vec−1( eY j
b (:, s))),
eXj−1
b
(:, :, :, s) = fold(vec−1(f
Xj−1
b
(:, s))), s = 1, ..., m.
(25)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4258188824662813,"For the b-th branch in (24), the input is eXj−1
b
∈CHj−1×Wj−1×n×m, the output feature map is
180"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4277456647398844,"eYb ∈CH′′
j ×W ′′
j ×n×m, and the kernel weight is eWj
b ∈CH′
j×W ′
j×n×n. Then for b = 1, ..., B, (24)
181"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4296724470134875,"can be rewritten as
182"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.43159922928709055,"eYj
b(h, w, c, :) = n
X d=1"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.43352601156069365,"W ′
j−1
X ℓ=0"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.43545279383429675,"H′
j−1
X"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4373795761078998,"i=0
eXj−1
b
(h + i, w + ℓ, d, :) · eWj
b(i, ℓ, d, c) + eBj
b(h, w, c, :),"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4393063583815029,"h = 1, ..., H′′
j , w = 1, ..., W ′′
j , c = 1, ..., n. (26)"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.441233140655106,"Similar to the fully connected tensor networks in Section 3.3, we apply the activation function in the
183"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.44315992292870904,"spectral domain as follows
184"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.44508670520231214,"eZj
b = σ(eYj
b) ∈CH′′
j ×W ′′
j ×n×m.
(27)
Then, the pooling operation is performed at the j-th layer of each branch, resulting in the output
185"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.44701348747591524,"eXj
b ∈CHj×Wj×n×m.
186"
CONVOLUTIONAL SPECTRAL TENSOR LAYER,0.4489402697495183,"At the last layer, the output function f(·) is applied to eXN
b as in (22).
187"
RECURRENT SPECTRAL TENSOR LAYER,0.4508670520231214,"3.5
Recurrent Spectral Tensor Layer
188"
RECURRENT SPECTRAL TENSOR LAYER,0.4527938342967245,"To present the recurrent spectral tensor layer, We take the same case in Section 3.3. Different
189"
RECURRENT SPECTRAL TENSOR LAYER,0.45472061657032753,"from the fully connected layer, the network parameters of the recurrent layer at the j-th layer
190"
RECURRENT SPECTRAL TENSOR LAYER,0.45664739884393063,"include a weight matrix W j
X ∈Rℓ′
j×ℓ′
j−1, W j
H ∈Rℓ′
j−1×ℓ′
j−1, W j
Y ∈Rℓ′
j−1×ℓ′
j−1 and an offset
191"
RECURRENT SPECTRAL TENSOR LAYER,0.45857418111753373,"Bj = [bj, ..., bj]
|
{z
}
m"
RECURRENT SPECTRAL TENSOR LAYER,0.4605009633911368,"∈Rℓ′
j×m, and the forward pass can be represented as [1]
192"
RECURRENT SPECTRAL TENSOR LAYER,0.4624277456647399,"Hj
t = σ1(W j
X · Xj−1
t
+ W j
H · Hj−1
t
+ Bj),"
RECURRENT SPECTRAL TENSOR LAYER,0.464354527938343,"Xj
t = σ2(W j
Y · Hj
t ), j = 1, ..., N, t = 1, 2, ..., T,
(28)"
RECURRENT SPECTRAL TENSOR LAYER,0.4662813102119461,"where Xj
t ∈Rℓ′
j×m is the input matrix at the j-t time step, and Y N
t
= XN
t .
193"
RECURRENT SPECTRAL TENSOR LAYER,0.4682080924855491,"For the introduced recurrent spectral tensor layer, the forward can be computed as
194"
RECURRENT SPECTRAL TENSOR LAYER,0.4701348747591522,"Hj
t = ϱ1(Wj
X • X j−1
t
+ Wj
H • Hj−1
t
+ Bj),"
RECURRENT SPECTRAL TENSOR LAYER,0.4720616570327553,"X j
t = ϱ2(Wj
Y • Hj
t), j = 1, ..., N, t = 1, 2, ..., T,
(29)"
RECURRENT SPECTRAL TENSOR LAYER,0.47398843930635837,"where X j
t ∈Rℓj×m×Q.
195"
RECURRENT SPECTRAL TENSOR LAYER,0.47591522157996147,"Furthermore, the implementation of Q branches for computations can be written as
196"
RECURRENT SPECTRAL TENSOR LAYER,0.47784200385356457,"eHj
q,t = ϱ1(f
W j
X • e
Xj−1
q,t
+ f
W j
H • eHj−1
q,t
+ eBj),"
RECURRENT SPECTRAL TENSOR LAYER,0.4797687861271676,"e
Xj
q,t = ϱ2(f
W j
Y • eHj
q,t), j = 1, ..., N, t = 1, 2, ..., T,
(30)"
RECURRENT SPECTRAL TENSOR LAYER,0.4816955684007707,"where f
Xj
q,t ∈Cℓj×m is the input for the q-th subnetwork at the t-th time step, likewise for f
Hj
q,t.
197"
RECURRENT SPECTRAL TENSOR LAYER,0.4836223506743738,"There are 2Q parallel matrix multiplications with the same size and along the same dimension, thus
198"
RECURRENT SPECTRAL TENSOR LAYER,0.48554913294797686,"we use the batch matrix multiplication on GPUs to accelerate (30).
199"
PERFORMANCE EVALUATION,0.48747591522157996,"4
Performance Evaluation
200"
PERFORMANCE EVALUATION,0.48940269749518306,"We first describe the experimental settings, then present the results on the MNIST, CIFAR-10 and
201"
PERFORMANCE EVALUATION,0.4913294797687861,"ImageNet data sets.
202"
DATA SETS AND PERFORMANCE METRICS,0.4932562620423892,"4.1
Data Sets and Performance Metrics
203"
DATA SETS AND PERFORMANCE METRICS,0.4951830443159923,"We verify the performance of the proposed spectral tensor networks on the following three widely
204"
DATA SETS AND PERFORMANCE METRICS,0.49710982658959535,"used data sets: 1) MNIST [48] contains grayscale images of handwritten digits. Each image has
205"
DATA SETS AND PERFORMANCE METRICS,0.49903660886319845,"28 × 28 pixels. The training set has 60, 000 images and the testing set has 10, 000 images. 2)
206"
DATA SETS AND PERFORMANCE METRICS,0.5009633911368016,"CIFAR-10 [49] contains 60, 000 color images in 10 classes, where each image has size 32×32×3.3)
207"
DATA SETS AND PERFORMANCE METRICS,0.5028901734104047,"ImageNet-1K [19]: It contains 12, 000, 000 training images and 50, 000 testing images with size of
208"
DATA SETS AND PERFORMANCE METRICS,0.5048169556840078,"224 × 224 × 3, labeled with the presence or absence of 1000 object categories that do not overlap
209"
DATA SETS AND PERFORMANCE METRICS,0.5067437379576107,"with each other.
210"
DATA SETS AND PERFORMANCE METRICS,0.5086705202312138,"We are interested in the following performance metrics:1) Compression ratio: the ratio of the
211"
DATA SETS AND PERFORMANCE METRICS,0.5105973025048169,"conventional network size to the spectral tensor network size, which is the also the total reduction in
212"
DATA SETS AND PERFORMANCE METRICS,0.51252408477842,"computation due to the reduced number of non-zero network weights; 2) Parallel speedup: the ratio
213"
DATA SETS AND PERFORMANCE METRICS,0.5144508670520231,"of the training time of a conventional network to that of the spectral tensor network, due to the fully
214"
DATA SETS AND PERFORMANCE METRICS,0.5163776493256262,"parallel training of all sub-networks; 3) Convergence: the loss value versus the training iterations: 4)
215"
DATA SETS AND PERFORMANCE METRICS,0.5183044315992292,"Accuracy: the percentage of correctly estimated labels. Both the training and testing processes are
216"
DATA SETS AND PERFORMANCE METRICS,0.5202312138728323,"executed on a DGX-2 server [50] that has two 64 core AMD CPUs, 8 NVIDIA A100 GPUs and 2
217"
DATA SETS AND PERFORMANCE METRICS,0.5221579961464354,"TB of memory. The operating system is Ubuntu 20.04 with CUDA 10.1. We use PyTorch [51] to
218"
DATA SETS AND PERFORMANCE METRICS,0.5240847784200385,"implement neural networks.
219"
DATA SETS AND PERFORMANCE METRICS,0.5260115606936416,"We summarize the compression ratio, the reduction of computation, and the parallel speedup in
220"
DATA SETS AND PERFORMANCE METRICS,0.5279383429672447,"Table 2. They are theoretical upper bounds, while their actual values depend on data sets and
221"
DATA SETS AND PERFORMANCE METRICS,0.5298651252408478,"implementations. For the compression ratio and reduction in computation, each fully connected
222"
DATA SETS AND PERFORMANCE METRICS,0.5317919075144508,"network / convolutional network has two columns: the right one corresponds to select-the-best
223"
DATA SETS AND PERFORMANCE METRICS,0.5337186897880539,"weighting, and the left one to other weighting methods.
224"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.535645472061657,"4.2
Evaluation of Fully Connected Spectral Tensor Networks
225"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5375722543352601,"For comparison, we consider a conventional fully connected network (FC) [1], the tNN [44], and the
226"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5394990366088632,"fully connected spectral tensor network (FC-tensor) in Section 3. All three methods use the ReLU
227"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5414258188824663,"activation function as σ(·) in the hidden layers, the softmax function as the output function f(·) in
228"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5433526011560693,"the last layer, and the cross-entropy loss function in (19). We use N = 8 layers in each method and
229"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5452793834296724,"the DCT transform in tNN and the proposed FC-tensor method. The learning rate was set to be 0.01,
230"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5472061657032755,"the batch size was set to be 64, and we used the Adam optimizer [52].
231"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5491329479768786,"For the MNIST data set, the conventional FC method has n = 28, ℓ′
0 = ... = ℓ′
7 = 784, and L = 10.
232"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5510597302504817,"Both the tNN method and our FC-tensor method have n = 28, Q = 28, ℓ0 = ... = ℓ7 = 28, and
233"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5529865125240848,"L = 10. Our FC-tensor method has r = 8. For the CIFAR-10 data set, the following parameters are
234"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5549132947976878,"Figure 2: Training loss of fully connected networks on the MNIST data set (left) and CIFAR-10 data
set (right)."
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5568400770712909,"Table 2: Upper bounds for model compression and parallel speedup. For fully connected networks,
n denotes the input size of each sub-network, r is a rank value, and there are Q branches. For
convolutional networks, there are B branches."
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.558766859344894,"-
Fully Connected
Convolutional (1D)
Compression ratio
O(nQ/2r), O(nQ2/2r)
O(B), O(B2)
Reduction in computation
O(nQ/2r), O(nQ2/2r)
O(B), O(B2)
Parallel speedup
O(Q)
O(B)"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5606936416184971,"different: n = 32, Q = 32, ℓ′
0 = ... = ℓ′
7 = 1, 024, and ℓ0 = ... = ℓ7 = 32. Therefore, our methods
235"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5626204238921002,"achieve a compression ratio of 49× and 64× for the two data sets, respectively.
236"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5645472061657033,"The training loss over iterations is shown in Fig. 2, with the left one for the MNIST datas set and
237"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5664739884393064,"the right one for the CIFAR-10 data set. Our scheme converges faster than tNN and FC, while the
238"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5684007707129094,"training process is more stable than FC. The possible reason is that the FC-tensor has much less
239"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5703275529865125,"parameters so that a more stable model can be learned from the same amount of data samples1. The
240"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5722543352601156,"loss values of our sub-networks are lower than both tNN and FC.
241"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5741811175337187,Figure 3: Training loss on ImageNet-1K data set.
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5761078998073218,"Table 3:
MNIST and CIAFR-10 data sets.
Methods
MNIST
CIFAR-10
FC [1]
98.71%
59.19%
tNN [44]
97.59%
44.50%
FC-tensor (average)
97.43%
47.24%
FC-tensor (weighted sum)
98.02%
48.13%
FC-tensor (geometric)
99.01%
48.33%"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5780346820809249,"In Table 3, we report accuracy results on both MNIST and CIAFR-10 data sets. Among the four
242"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5799614643545279,"schemes for weighting the sub-networks , the geometric weighting gives the best performance. For
243"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.581888246628131,"the MNIST data set, all three methods achieve a relative high accuracy, i.e., over 97%, while our
244"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5838150289017341,"FC-tensor method reaches 98.36%. For the CIFAR-10 data set, all three methods achieve a relative
245"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5857418111753372,"low accuracy, i.e., below 60%. This is consistent with the known fact that fully connected layers are
246"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5876685934489403,1Note that we use the same number of layers and the same batch size.
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5895953757225434,"Table 3: Results on the ImageNet-1K data set.
Methods
Accuracy
Size
Training Time
AlexNet [18]
63.44%
244 MB
40.8 h
AlexNet-spectral (average)
61.26%
130 MB
31.9 h
AlexNet-spectral (weighted sum)
58.01%
130 MB
31.9 h
AlexNet-spectral (geometric)
62.26%
130 MB
31.9 h
AlexNet-spectral (select-the-best)
56.45%
32.5 MB
31.9 h
CycleMLP [53]
83.23%
103 MB
93.6 h
CycleMLP-spectral (average)
78.80%
76 MB
60.4 h
CycleMLP-spectral (weighted sum)
77.54%
76 MB
60.4 h
CycleMLP-spectral (geometric)
83.20%
76 MB
60.4 h
CycleMLP-spectral (select-the-best)
72.45%
19 MB
60.4 h"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5915221579961464,"not enough for the classification task on CIFAR-10. Note that both tNN and FC-tensor achieve lower
247"
EVALUATION OF FULLY CONNECTED SPECTRAL TENSOR NETWORKS,0.5934489402697495,"accuracy than the FC method.
248"
EVALUATION ON IMAGENET DATA SET,0.5953757225433526,"4.3
Evaluation on ImageNet Data set
249"
EVALUATION ON IMAGENET DATA SET,0.5973025048169557,"The ImageNet data set [19] is split into B = 4 spectral subsets, where each image is organized into
250"
EVALUATION ON IMAGENET DATA SET,0.5992292870905588,"a tensor of size 56 × 56 × 3 × 16 and then processed into a spectrum tensor using DCT transform.
251"
EVALUATION ON IMAGENET DATA SET,0.6011560693641619,"Note that the three RGB channels are processed independently.
252"
EVALUATION ON IMAGENET DATA SET,0.603082851637765,"Our proposed spectral tensor methods have the same structure in Fig. 1, where each branch is replaced
253"
EVALUATION ON IMAGENET DATA SET,0.605009633911368,"by either AlexNet [18] or CycleMLP. We use the DCT transform in our spectral methods. We set
254"
EVALUATION ON IMAGENET DATA SET,0.6069364161849711,"the learning rate 0.01 and the batch size 128. We follow the standard practice in the community by
255"
EVALUATION ON IMAGENET DATA SET,0.6088631984585742,"reporting the top-1 accuracy on the testing set.
256"
EVALUATION ON IMAGENET DATA SET,0.6107899807321773,"For the ImageNet data set, the training loss over training iterations is shown in Fig. 3. Our spectral
257"
EVALUATION ON IMAGENET DATA SET,0.6127167630057804,"sub-networks have similar loss curve to their original networks. In Table 3, we report the accuracy,
258"
EVALUATION ON IMAGENET DATA SET,0.6146435452793835,"model size, and training time. For the AlextNet structure, our spectral network achieves 1.88× model
259"
EVALUATION ON IMAGENET DATA SET,0.6165703275529865,"compression and 1.28× speedup in training time, at the cost of an accuracy drop of 1.18%. For the
260"
EVALUATION ON IMAGENET DATA SET,0.6184971098265896,"CycleMLP structure, our spectral network achieves 1.36× model compression and 1.55× speedup in
261"
EVALUATION ON IMAGENET DATA SET,0.6204238921001927,"training time, at the cost of an accuracy drop of 0.03%.
262"
CONCLUSIONS,0.6223506743737958,"5
Conclusions
263"
CONCLUSIONS,0.6242774566473989,"In this paper, we have proposed a spectral tensor form of deep neural networks that is inherently com-
264"
CONCLUSIONS,0.626204238921002,"pressive and allows communication-free parallel/distributed implementations. The data is organized
265"
CONCLUSIONS,0.628131021194605,"into tensors and a linear transform is applied along certain dimension, resulting in different spectral
266"
CONCLUSIONS,0.630057803468208,"subsets. The overall network consists of parallel branches of networks, each independently performs
267"
CONCLUSIONS,0.6319845857418112,"training and inference on a spectral data subset. We tested the proposed spectral networks, including
268"
CONCLUSIONS,0.6339113680154143,"fully connected, convolutional, AlexNet, and CycleMLP, on the MNIST, CIFAR-10 and ImageNet
269"
CONCLUSIONS,0.6358381502890174,"data sets, and results show that they can achieve relatively high accuracy with substantial network
270"
CONCLUSIONS,0.6377649325626205,"compression, computation reduction, and parallel speedup, compared with conventional networks.
271"
CONCLUSIONS,0.6396917148362236,"Limited by the pages, We do not provide experiments using the recurrent spectral tensor layer.
272"
CONCLUSIONS,0.6416184971098265,"For future works, we would like to explore an ensemble-style approach model soup [54] that takes
273"
CONCLUSIONS,0.6435452793834296,"average over multiple trained models and achieves state-of-the-art performance on the ImageNet data
274"
CONCLUSIONS,0.6454720616570327,"set.
275"
REFERENCES,0.6473988439306358,"References
276"
REFERENCES,0.649325626204239,"[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, MIT Press, 2016.
277"
REFERENCES,0.651252408477842,"[2] Yang Liu, Peng Sun, Nickolas Wergeles, and Yi Shang, “A survey and performance evaluation
278"
REFERENCES,0.653179190751445,"of deep learning methods for small object detection,” Expert Systems with Applications, vol.
279"
REFERENCES,0.6551059730250481,"172, pp. 114602, 2021.
280"
REFERENCES,0.6570327552986512,"[3] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei
281"
REFERENCES,0.6589595375722543,"Zhang, “Dynamic head: Unifying object detection heads with attentions,” in Proceedings of the
282"
REFERENCES,0.6608863198458574,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7373–7382.
283"
REFERENCES,0.6628131021194605,"[4] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,
284"
REFERENCES,0.6647398843930635,"Lei Li, Zehuan Yuan, Changhu Wang, et al., “Sparse r-cnn: End-to-end object detection with
285"
REFERENCES,0.6666666666666666,"learnable proposals,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
286"
REFERENCES,0.6685934489402697,"Pattern Recognition, 2021, pp. 14454–14463.
287"
REFERENCES,0.6705202312138728,"[5] Xiangjun Wang, Junxiao Song, Penghui Qi, Peng Peng, Zhenkun Tang, Wei Zhang, Weimin Li,
288"
REFERENCES,0.6724470134874759,"Xiongjun Pi, Jujie He, Chao Gao, et al., “Scc: an efficient deep reinforcement learning agent
289"
REFERENCES,0.674373795761079,"mastering the game of starcraft ii,” in International Conference on Machine Learning. PMLR,
290"
REFERENCES,0.6763005780346821,"2021, pp. 10905–10915.
291"
REFERENCES,0.6782273603082851,"[6] Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu, “Fop: Factorizing
292"
REFERENCES,0.6801541425818882,"optimal joint policy of maximum-entropy multi-agent reinforcement learning,” in International
293"
REFERENCES,0.6820809248554913,"Conference on Machine Learning. PMLR, 2021, pp. 12491–12500.
294"
REFERENCES,0.6840077071290944,"[7] Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar, “Coach-
295"
REFERENCES,0.6859344894026975,"player multi-agent reinforcement learning for dynamic team composition,” in International
296"
REFERENCES,0.6878612716763006,"Conference on Machine Learning. PMLR, 2021, pp. 6860–6870.
297"
REFERENCES,0.6897880539499036,"[8] Zhiqi Huang, Fenglin Liu, Xian Wu, Shen Ge, Helin Wang, Wei Fan, and Yuexian Zou, “Audio-
298"
REFERENCES,0.6917148362235067,"oriented multimodal machine comprehension via dynamic inter-and intra-modality attention,” in
299"
REFERENCES,0.6936416184971098,"Proceedings of the AAAI Conference on Artificial Intelligence, 2021, vol. 35, pp. 13098–13106.
300"
REFERENCES,0.6955684007707129,"[9] Tianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun Li, “Asking effective and diverse questions:
301"
REFERENCES,0.697495183044316,"a machine reading comprehension based framework for joint entity-relation extraction,” in
302"
REFERENCES,0.6994219653179191,"Proceedings of the Twenty-Ninth International Conference on International Joint Conferences
303"
REFERENCES,0.7013487475915221,"on Artificial Intelligence, 2021, pp. 3948–3954.
304"
REFERENCES,0.7032755298651252,"[10] Cong Sun, Zhihao Yang, Lei Wang, Yin Zhang, Hongfei Lin, and Jian Wang, “Biomedical
305"
REFERENCES,0.7052023121387283,"named entity recognition using bert in the machine reading comprehension framework,” Journal
306"
REFERENCES,0.7071290944123314,"of Biomedical Informatics, vol. 118, pp. 103799, 2021.
307"
REFERENCES,0.7090558766859345,"[11] Aditya Prakash, Kashyap Chitta, and Andreas Geiger, “Multi-modal fusion transformer for
308"
REFERENCES,0.7109826589595376,"end-to-end autonomous driving,” in Proceedings of the IEEE/CVF Conference on Computer
309"
REFERENCES,0.7129094412331407,"Vision and Pattern Recognition, 2021, pp. 7077–7087.
310"
REFERENCES,0.7148362235067437,"[12] Yingfeng Cai, Tianyu Luan, Hongbo Gao, Hai Wang, Long Chen, Yicheng Li, Miguel Angel
311"
REFERENCES,0.7167630057803468,"Sotelo, and Zhixiong Li, “Yolov4-5d: An effective and efficient object detector for autonomous
312"
REFERENCES,0.7186897880539499,"driving,” IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1–13, 2021.
313"
REFERENCES,0.720616570327553,"[13] Sudeep Fadadu, Shreyash Pandey, Darshan Hegde, Yi Shi, Fang-Chieh Chou, Nemanja Djuric,
314"
REFERENCES,0.7225433526011561,"and Carlos Vallespi-Gonzalez, “Multi-view fusion of sensor data for improved perception and
315"
REFERENCES,0.7244701348747592,"prediction in autonomous driving,” in Proceedings of the IEEE/CVF Winter Conference on
316"
REFERENCES,0.7263969171483622,"Applications of Computer Vision, 2022, pp. 2349–2357.
317"
REFERENCES,0.7283236994219653,"[14] Huu-Thiet Nguyen, Chien Chern Cheah, and Kar-Ann Toh, “An analytic layer-wise deep
318"
REFERENCES,0.7302504816955684,"learning framework with applications to robotics,” Automatica, vol. 135, pp. 110007, 2022.
319"
REFERENCES,0.7321772639691715,"[15] Radouan Ait Mouha et al., “Deep learning for robotics,” Journal of Data Analysis and
320"
REFERENCES,0.7341040462427746,"Information Processing, vol. 9, no. 02, pp. 63, 2021.
321"
REFERENCES,0.7360308285163777,"[16] Yinong Chen and Gennaro De Luca, “Technologies supporting artificial intelligence and
322"
REFERENCES,0.7379576107899807,"robotics application development,” Journal of Artificial Intelligence and Technology, vol. 1, no.
323"
REFERENCES,0.7398843930635838,"1, pp. 1–8, 2021.
324"
REFERENCES,0.7418111753371869,"[17] G. Menghani, “Efficient deep learning: A survey on making deep learning models smaller,
325"
REFERENCES,0.74373795761079,"faster, and better,” arXiv preprint arXiv:2106.08962, 2021.
326"
REFERENCES,0.7456647398843931,"[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification with deep convolutional
327"
REFERENCES,0.7475915221579962,"neural networks,” Advances in Neural Information Processing Systems, vol. 25, pp. 1097–1105,
328"
REFERENCES,0.7495183044315993,"2012.
329"
REFERENCES,0.7514450867052023,"[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li, “ImageNet: A large-scale hierarchical
330"
REFERENCES,0.7533718689788054,"image database,” in IEEE CVPR. Ieee, 2009, pp. 248–255.
331"
REFERENCES,0.7552986512524085,"[20] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. De Freitas, “Predicting parameters in deep
332"
REFERENCES,0.7572254335260116,"learning,” in Advances in Neural Information Processing Systems, 2013, pp. 2148–2156.
333"
REFERENCES,0.7591522157996147,"[21] V. Sindhwani, T. Sainath, and S. Kumar, “Structured transforms for small-footprint deep
334"
REFERENCES,0.7610789980732178,"learning,” in Advances in Neural Information Processing Systems, 2015, pp. 3088–3096.
335"
REFERENCES,0.7630057803468208,"[22] Y. Cheng, F. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang, “An exploration of
336"
REFERENCES,0.7649325626204239,"parameter redundancy in deep networks with circulant projections,” in IEEE International
337"
REFERENCES,0.766859344894027,"Conference on Computer Vision, 2015, pp. 2857–2865.
338"
REFERENCES,0.7687861271676301,"[23] M. Moczulski, M. Denil, J. Appleyard, N. De Freitas, Z. Wang, M. Zoghi, F. Hutter, D. Matheson,
339"
REFERENCES,0.7707129094412332,"and S. Reed, “ACDC: A structured efficient linear layer,” in ICLR, 2015, vol. 55.
340"
REFERENCES,0.7726396917148363,"[24] A. Prabhu, A. Farhadi, and M. Rastegari, “Butterfly transform: An efficient fft based neural
341"
REFERENCES,0.7745664739884393,"architecture design,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition,
342"
REFERENCES,0.7764932562620424,"2020, pp. 12024–12033.
343"
REFERENCES,0.7784200385356455,"[25] C. Ding, S. Liao, Y. Wang, Z. Li, N. Liu, Y. Zhuo, C. Wang, X. Qian, Y. Bai, and G. Yuan,
344"
REFERENCES,0.7803468208092486,"“Circnn: accelerating and compressing deep neural networks using block-circulant weight
345"
REFERENCES,0.7822736030828517,"matrices,” in Annual IEEE/ACM International Symposium on Microarchitecture, 2017, pp.
346"
REFERENCES,0.7842003853564548,"395–408.
347"
REFERENCES,0.7861271676300579,"[26] S. Liao and B. Yuan, “Circconv: A structured convolution with low complexity,” in AAAI
348"
REFERENCES,0.7880539499036608,"Conference on Artificial Intelligence, 2019, vol. 33, pp. 4287–4294.
349"
REFERENCES,0.789980732177264,"[27] B. Gong, B. Jou, F. Yu, and S.-F. Chang, “Tamp: A library for compact deep neural networks
350"
REFERENCES,0.791907514450867,"with structured matrices,” in ACM International Conference on Multimedia, 2016, pp. 1206–
351"
REFERENCES,0.7938342967244701,"1209.
352"
REFERENCES,0.7957610789980732,"[28] T. N Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, “Low-rank matrix
353"
REFERENCES,0.7976878612716763,"factorization for deep neural network training with high-dimensional output targets,” in IEEE
354"
REFERENCES,0.7996146435452793,"ICASSP, 2013, pp. 6655–6659.
355"
REFERENCES,0.8015414258188824,"[29] A. Novikov, D. Podoprikhin, A. Osokin, and D.P. Vetrov, “Tensorizing neural networks,” in
356"
REFERENCES,0.8034682080924855,"Advances in Neural Information Processing Systems, 2015, pp. 442–450.
357"
REFERENCES,0.8053949903660886,"[30] W. Wang, Y. Sun, B. Eriksson, W. Wang, and V. Aggarwal, “Wide compression: Tensor ring
358"
REFERENCES,0.8073217726396917,"nets,” in IEEE CVPR, 2018, pp. 9329–9338.
359"
REFERENCES,0.8092485549132948,"[31] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky, “Speeding-up convolutional
360"
REFERENCES,0.8111753371868978,"neural networks using fine-tuned cp-decomposition,” ICLR, 2015.
361"
REFERENCES,0.8131021194605009,"[32] M. Yin, S. Liao, X.-Y. Liu, X. Wang, and B. Yuan, “Towards extremely compact rnns for video
362"
REFERENCES,0.815028901734104,"recognition with fully decomposed hierarchical tucker structure,” in IEEE CVPR, 2021, pp.
363"
REFERENCES,0.8169556840077071,"12085–12094.
364"
REFERENCES,0.8188824662813102,"[33] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and J. S. Rellermeyer, “A
365"
REFERENCES,0.8208092485549133,"survey on distributed machine learning,” ACM Computing Surveys (CSUR), vol. 53, no. 2, pp.
366"
REFERENCES,0.8227360308285164,"1–33, 2020.
367"
REFERENCES,0.8246628131021194,"[34] Vipul Gupta, Dhruv Choudhary, Peter Tang, Xiaohan Wei, Xing Wang, Yuzhen Huang, Arun
368"
REFERENCES,0.8265895953757225,"Kejariwal, Kannan Ramchandran, and Michael W Mahoney, “Training recommender systems
369"
REFERENCES,0.8285163776493256,"at scale: Communication-efficient model and data parallelism,” in Proceedings of the 27th ACM
370"
REFERENCES,0.8304431599229287,"SIGKDD Conference on Knowledge Discovery & Data Mining, 2021, pp. 2928–2936.
371"
REFERENCES,0.8323699421965318,"[35] Xiangyu Ye, Zhiquan Lai, Shengwei Li, Lei Cai, Ding Sun, Linbo Qiao, and Dongsheng Li,
372"
REFERENCES,0.8342967244701349,"“Hippie: A data-paralleled pipeline approach to improve memory-efficiency and scalability for
373"
REFERENCES,0.8362235067437379,"large dnn training,” in 50th International Conference on Parallel Processing, 2021, pp. 1–10.
374"
REFERENCES,0.838150289017341,"[36] A. L. Gaunt, M. A. Johnson, M. Riechert, D. Tarlow, R. Tomioka, D. Vytiniotis, and S. Webster,
375"
REFERENCES,0.8400770712909441,"“AMPNet: Asynchronous model-parallel training for dynamic neural networks,” arXiv preprint
376"
REFERENCES,0.8420038535645472,"arXiv:1705.09786, 2017.
377"
REFERENCES,0.8439306358381503,"[37] An Xu, Zhouyuan Huo, and Heng Huang,
“On the acceleration of deep learning model
378"
REFERENCES,0.8458574181117534,"parallelism with staleness,” in Proceedings of the IEEE/CVF Conference on Computer Vision
379"
REFERENCES,0.8477842003853564,"and Pattern Recognition, 2020, pp. 2088–2097.
380"
REFERENCES,0.8497109826589595,"[38] Kabir Nagrecha, “Model-parallel model selection for deep learning systems,” in Proceedings of
381"
REFERENCES,0.8516377649325626,"the 2021 International Conference on Management of Data, 2021, pp. 2929–2931.
382"
REFERENCES,0.8535645472061657,"[39] E. P. Xing, Q. Ho, P. Xie, and D. Wei, “Strategies and principles of distributed machine learning
383"
REFERENCES,0.8554913294797688,"on big data,” Engineering, vol. 2, no. 2, pp. 179–195, 2016.
384"
REFERENCES,0.8574181117533719,"[40] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
385"
REFERENCES,0.859344894026975,"M. Isard, et al.,
“TensorFlow: A system for large-scale machine learning,”
in USENIX
386"
REFERENCES,0.861271676300578,"Symposium on Operating Systems Design and Implementation, 2016, pp. 265–283.
387"
REFERENCES,0.8631984585741811,"[41] E. Kernfeld, M. Kilmer, and S. Aeron, “Tensor–tensor products with invertible linear transforms,”
388"
REFERENCES,0.8651252408477842,"Linear Algebra and its Applications, vol. 485, pp. 545–570, 2015.
389"
REFERENCES,0.8670520231213873,"[42] M. E. Kilmer, L. Horesh, H. Avron, and E. Newman, “Tensor-tensor algebra for optimal
390"
REFERENCES,0.8689788053949904,"representation and compression of multiway data,” Proceedings of the National Academy of
391"
REFERENCES,0.8709055876685935,"Sciences, vol. 118, no. 28, 2021.
392"
REFERENCES,0.8728323699421965,"[43] X.-Y. Liu and X. Wang, “Fourth-order tensors with multidimensional discrete transforms,”
393"
REFERENCES,0.8747591522157996,"arXiv preprint arXiv:1705.01576, pp. 1–37, 2017.
394"
REFERENCES,0.8766859344894027,"[44] E. Newman, L. Horesh, H. Avron, and M. Kilmer, “Stable tensor neural networks for rapid
395"
REFERENCES,0.8786127167630058,"deep learning,” arXiv preprint arXiv:1811.06569, 2018.
396"
REFERENCES,0.8805394990366089,"[45] M.E. Kilmer and C.D Martin, “Factorization strategies for third-order tensors,” Linear Algebra
397"
REFERENCES,0.882466281310212,"and its Applications, vol. 435, no. 3, pp. 641–658, 2011.
398"
REFERENCES,0.884393063583815,"[46] M.E. Kilmer, K. Braman, N. Hao, and R.C. Hoover, “Third-order tensors as operators on
399"
REFERENCES,0.8863198458574181,"matrices: A theoretical and computational framework with applications in imaging,” SIAM
400"
REFERENCES,0.8882466281310212,"Journal on Matrix Analysis and Applications, vol. 34, no. 1, pp. 148–172, 2013.
401"
REFERENCES,0.8901734104046243,"[47] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional neural networks:
402"
REFERENCES,0.8921001926782274,"analysis, applications, and prospects,” IEEE Transactions on Neural Networks and Learning
403"
REFERENCES,0.8940269749518305,"Systems, 2021.
404"
REFERENCES,0.8959537572254336,"[48] L. Deng, “The MNIST database of handwritten digit images for machine learning research,”
405"
REFERENCES,0.8978805394990366,"IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012.
406"
REFERENCES,0.8998073217726397,"[49] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from tiny images,” Master’s
407"
REFERENCES,0.9017341040462428,"thesis, University of Tront, 2009.
408"
REFERENCES,0.9036608863198459,"[50] J. Choquette et al., “NVIDIA A100 tensor core GPU: Performance and innovation,” IEEE
409"
REFERENCES,0.905587668593449,"Micro, vol. 41, no. 2, pp. 29–35, 2021.
410"
REFERENCES,0.9075144508670521,"[51] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, and et al, “PyTorch: An imperative
411"
REFERENCES,0.9094412331406551,"style, high-performance deep learning library,” in Advances in Neural Information Processing
412"
REFERENCES,0.9113680154142582,"Systems, 2019, pp. 8026–8037.
413"
REFERENCES,0.9132947976878613,"[52] D.P Kingma and J. Ba, “Adam: A method for stochastic optimization,” ICLR, 2015.
414"
REFERENCES,0.9152215799614644,"[53] S. Chen, E. Xie, C. Ge, D. Liang, and P. Luo, “CycleMLP: A MLP-like architecture for dense
415"
REFERENCES,0.9171483622350675,"prediction,” ICLR, 2022.
416"
REFERENCES,0.9190751445086706,"[54] Mitchell Wortsman, Gabriel Ilharco, and et al, “Model soups: averaging weights of multiple fine-
417"
REFERENCES,0.9210019267822736,"tuned models improves accuracy without increasing inference time,” preprint arXiv:2203.05482,
418"
REFERENCES,0.9229287090558767,"2022.
419"
REFERENCES,0.9248554913294798,"Checklist
420"
REFERENCES,0.9267822736030829,"1. For all authors...
421"
REFERENCES,0.928709055876686,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
422"
REFERENCES,0.930635838150289,"contributions and scope? [Yes]
423"
REFERENCES,0.9325626204238922,"(b) Did you describe the limitations of your work? [No]
424"
REFERENCES,0.9344894026974951,"(c) Did you discuss any potential negative societal impacts of your work? [No]
425"
REFERENCES,0.9364161849710982,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
426"
REFERENCES,0.9383429672447013,"them? [Yes]
427"
REFERENCES,0.9402697495183044,"2. If you are including theoretical results...
428"
REFERENCES,0.9421965317919075,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
429"
REFERENCES,0.9441233140655106,"(b) Did you include complete proofs of all theoretical results? [N/A]
430"
REFERENCES,0.9460500963391136,"3. If you ran experiments...
431"
REFERENCES,0.9479768786127167,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
432"
REFERENCES,0.9499036608863198,"mental results (either in the supplemental material or as a URL)? [Yes] See Section
433"
REFERENCES,0.9518304431599229,"4.1.
434"
REFERENCES,0.953757225433526,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
435"
REFERENCES,0.9556840077071291,"were chosen)? [Yes] See Section 4.2 and 4.3.
436"
REFERENCES,0.9576107899807321,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
437"
REFERENCES,0.9595375722543352,"ments multiple times)? [Yes] We do not supply experiments of recurrent spectral tensor
438"
REFERENCES,0.9614643545279383,"layer.
439"
REFERENCES,0.9633911368015414,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
440"
REFERENCES,0.9653179190751445,"of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1.
441"
REFERENCES,0.9672447013487476,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
442"
REFERENCES,0.9691714836223507,"(a) If your work uses existing assets, did you cite the creators? [Yes]
443"
REFERENCES,0.9710982658959537,"(b) Did you mention the license of the assets? [Yes]
444"
REFERENCES,0.9730250481695568,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
445"
REFERENCES,0.9749518304431599,"See Section 4.3.
446"
REFERENCES,0.976878612716763,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
447"
REFERENCES,0.9788053949903661,"using/curating? [Yes] All the consent is open-source.
448"
REFERENCES,0.9807321772639692,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
449"
REFERENCES,0.9826589595375722,"information or offensive content? [No] We do not use any personally identifiable
450"
REFERENCES,0.9845857418111753,"information or offensive content.
451"
REFERENCES,0.9865125240847784,"5. If you used crowdsourcing or conducted research with human subjects...
452"
REFERENCES,0.9884393063583815,"(a) Did you include the full text of instructions given to participants and screenshots, if
453"
REFERENCES,0.9903660886319846,"applicable? [N/A]
454"
REFERENCES,0.9922928709055877,"(b) Did you describe any potential participant risks, with links to Institutional Review
455"
REFERENCES,0.9942196531791907,"Board (IRB) approvals, if applicable? [N/A]
456"
REFERENCES,0.9961464354527938,"(c) Did you include the estimated hourly wage paid to participants and the total amount
457"
REFERENCES,0.9980732177263969,"spent on participant compensation? [N/A]
458"
