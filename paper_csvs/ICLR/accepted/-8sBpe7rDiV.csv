Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017271157167530224,"Neuromorphic neural network processors, in the form of compute-in-memory cross-
bar arrays of memristors, or in the form of subthreshold analog and mixed-signal
ASICs, promise enormous advantages in compute density and energy efﬁciency
for NN-based ML tasks. However, these technologies are prone to computational
non-idealities, due to process variation and intrinsic device physics. This degrades
the task performance of networks deployed to the processor, by introducing param-
eter noise into the deployed model. While it is possible to calibrate each device,
or train networks individually for each processor, these approaches are expensive
and impractical for commercial deployment. Alternative methods are therefore
needed to train networks that are inherently robust against parameter variation, as a
consequence of network architecture and parameters. We present a new network
training algorithm that attacks network parameters during training, and promotes
robust performance during inference in the face of random parameter variation.
Our approach introduces a loss regularization term that penalizes the susceptibility
of a network to weight perturbation. We compare against previous approaches for
producing parameter insensitivity such as dropout, weight smoothing and introduc-
ing parameter noise during training. We show that our approach produces models
that are more robust to random mismatch-induced parameter variation as well as
to targeted parameter variation. Our approach ﬁnds minima in ﬂatter locations in
the weight-loss landscape compared with other approaches, highlighting that the
networks found by our technique are less sensitive to parameter perturbation. Our
work provides an approach to deploy neural network architectures to inference
devices that suffer from computational non-idealities, with minimal loss of per-
formance. This method will enable deployment at scale to novel energy-efﬁcient
computational substrates, promoting cheaper and more prevalent edge inference."
INTRODUCTION,0.0034542314335060447,"1
INTRODUCTION"
INTRODUCTION,0.0051813471502590676,"There is increasing interest in NN and ML inference on IoT and embedded devices, which imposes
energy constraints due to small battery capacity and untethered operation. Existing edge inference
solutions based on CPUs or vector processing engines such as GPUs or TPUs are improving in
energy efﬁciency, but still entail considerable energy cost (Huang et al., 2009). Alternative compute
architectures such as memristor crossbar arrays and mixed-signal event-driven neural network acceler-
ators promise signiﬁcantly reduced energy consumption for edge inference tasks. Novel non-volatile
memory technologies such as resistive RAM and phase-change materials (Chen, 2016; Yu & Chen,
2016) promise increased memory density with multiple bits per memory cell, as well as compact
compute-in-memory for NN inference tasks (Sebastian et al., 2020). Analog implementations of
neurons and synapses, coupled with asynchronous digital routing fabrics, permit high sparsity in both
network architecture and activity, thereby reducing energy costs associated with computation."
INTRODUCTION,0.0069084628670120895,"However, both of these novel compute fabrics introduce complexity in the form of computational
non-idealities, which do not exist for pure synchronous digital solutions. Some novel memory
technologies support several bits per memory cell, but with uncertainty about the precise value stored
on each cycle (Le Gallo et al., 2018b; Wu et al., 2019). Others exhibit signiﬁcant drift in stored"
INTRODUCTION,0.008635578583765112,Published as a conference paper at ICLR 2022
INTRODUCTION,0.010362694300518135,"states (Joshi et al., 2020). Inference processors based on analog and mixed-signal devices (Neckar
et al., 2019; Moradi et al., 2018; Cassidy et al., 2016; Schemmel et al., 2010; Khaddam-Aljameh
et al., 2022) exhibit parameter variation across the surface of a chip, and between chips, due to
manufacturing process non-idealities. Collectively these processes known as “device mismatch”
manifest as frozen parameter noise in weights and neuron parameters."
INTRODUCTION,0.012089810017271158,"In all cases the mismatch between conﬁgured and implemented network parameters degrades the task
performance by modifying the resulting mapping between input and output. Existing solutions for
deploying networks to inference devices that exhibit mismatch mostly focus on per-device calibration
or re-training (Ambrogio et al., 2018; Bauer et al., 2019; Nandakumar et al., 2020a). However, this,
and other approaches such as few-shot learning or meta learning entail signiﬁcant per-device handling
costs, making them unﬁt for commercial deployment."
INTRODUCTION,0.013816925734024179,"We consider a network to be “robust” if the output of a network to a given input does not change in
the face of parameter perturbation. With this goal, network architectures that are intrinsically robust
against device mismatch can be investigated (Thakur et al., 2018; Büchel et al., 2021). Another
approach is to introduce parameter perturbations during training that promote robustness during
inference, for example via random pruning (dropout) (Srivastava et al., 2014) or by injecting noise
(Murray & Edwards, 1994)."
INTRODUCTION,0.015544041450777202,"In this paper we introduce a novel solution, by applying adversarial training approaches to parameter
mismatch. Most existing adversarial training methods attack the input space. Here we describe an
adversarial attack during training that seeks the parameter perturbation that causes the maximum
degradation in network response. In summary, we make the following contributions:"
INTRODUCTION,0.017271157167530225,"• We propose a novel algorithm for gradient-based supervised training of networks that are robust
against parameter mismatch, by performing adversarial training in the weight space."
INTRODUCTION,0.018998272884283247,"• We demonstrate that our algorithm ﬂattens the weight-loss landscape and therefore leads to models
that are inherently more robust to parameter noise."
INTRODUCTION,0.02072538860103627,• We show that our approach outperforms existing methods in terms of robustness.
INTRODUCTION,0.022452504317789293,"• We validate our algorithm on a highly accurate Phase Change Memory (PCM)-based Compute-
in-Memory (CiM) simulator and achieve new state-of-the-art results in terms of performance and
performance retention over time."
RELATED WORK,0.024179620034542316,"2
RELATED WORK"
RELATED WORK,0.025906735751295335,"Research to date has focused mainly on adversarial attacks in the input space. With an increasing
number of adversarial attacks, an increasing number of schemes defending against those attacks
have been proposed (Wang et al., 2020; Zhang et al., 2019; Madry et al., 2019; Moosavi-Dezfooli
et al., 2018). In contrast, adversarial attacks in parameter space have received little attention. Where
parameter-space adversaries have been examined, it has been to enhance performance in semi-
supervised learning (Cicek & Soatto, 2019), to improve robustness to input-space adversarial attacks
(Wu et al., 2020), or to improve generalisation capability (Zheng et al., 2020)."
RELATED WORK,0.027633851468048358,"We deﬁne “robustness” to mean that the network output should change only minimally in the face of
a parameter perturbation — in other words, the weight-loss landscape should be as ﬂat as possible at
a loss minimum. Other algorithms that promote ﬂat loss landscapes may therefore also be useful to
promote robustness to parameter perturbations."
RELATED WORK,0.02936096718480138,"Dropout (Srivastava et al., 2014) is a widely used method to reduce overﬁtting. During training, a
random subset of units are chosen with some probability, and these units are pruned from the network
for a single trial or batch. This results in the network learning to distribute its computation across
many units, and acts as a regularization against overﬁtting."
RELATED WORK,0.031088082901554404,"Entropy-SGD (Chaudhari et al., 2019) is a network optimisation method that minimises the local
entropy around a solution in parameter space. This results in a smoothed parameter-loss landscape
that should penalize sharp minima."
RELATED WORK,0.03281519861830743,"Adversarial Block Coordinate Descent (ABCD) (Cicek & Soatto, 2019) was proposed in order
to complement input-space smoothing with weight-space smoothing in semi-supervised learning."
RELATED WORK,0.03454231433506045,Published as a conference paper at ICLR 2022
RELATED WORK,0.03626943005181347,"ABCD repeatedly picks half of the network weights and performs one step of gradient ascent on
them, followed by applying gradient descent on the other half."
RELATED WORK,0.037996545768566495,"Adversarial Weight Perturbation (AWP) (Wu et al., 2020) was designed to improve the robustness
of a network to adversarial attacks in the input space. The authors use Projected Gradient Ascent
(PGA) on the network parameters to approximate a worst case perturbation of the weights Θ′. PGA
repeatedly computes the gradient of a loss function and updates the parameters in the direction of
the (positive) gradient. After each update, the parameters are projected back onto a ball (e.g. in
l2) around the original parameters to ensure that a maximum distance is kept. Having identiﬁed an
adversarial perturbation in the weight-space, an adversarial perturbation in the input-space is also
found using PGA. Finally, the original weights Θ are updated using the gradient of the loss evaluated
at the adversarial perturbation Θ′."
RELATED WORK,0.039723661485319514,"Adversarial Model Perturbation (AMP) (Zheng et al., 2020) improves the generalisation of conven-
tional neural networks by optimizing a standard loss evaluated using parameters that were perturbed
adversarially using PGA. Unlike our method, (Zheng et al., 2020) did not formulate the loss function
as a trade-off between performance and robustness. Furthermore, the presented algorithm, unlike our
method, treats the perturbation ∆Θ to the parameters Θ as a constant during backpropagation."
RELATED WORK,0.04145077720207254,"TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) (Zhang
et al., 2019) is a method for training networks that are robust against adversarial examples in the
input space. The method consists of adding a boundary loss term to the loss function that measures
how the network performance changes when the input is attacked. The boundary loss does not take
the labels into account, so scaling it by a factor βrob allows for a principled trade-off between the
robustness and the accuracy of the network."
RELATED WORK,0.04317789291882556,"Noise injection during the forward pass (Murray & Edwards, 1994) is a simple method for in-
creasing network robustness to parameter noise. This method adds Gaussian noise to the network
parameters during the forward pass and computes weight gradients with respect to the original
parameters. This method regularizes the gradient magnitudes of output units with respect to the
weights, thus enforcing distributed information processing and insensitivity to parameter noise. We
refer to this method as “Forward Noise”."
RELATED WORK,0.044905008635578586,"A recent paper proposed a method for improving the resilience to random and targeted bit errors in
SRAM cells on digital Deep Neural Network (DNN) accelerators (Stutz et al., 2021). By employing
adversarial or random bit ﬂips during training, the authors signiﬁcantly improved the robustness to
bit perturbations, enabling the accelerators to be operated below the conventional supply voltage."
METHODS,0.046632124352331605,"3
METHODS"
METHODS,0.04835924006908463,"We use Θ to denote the set of parameters of a neural network f(x, Θ) that are trainable and susceptible
to mismatch. The adversarial weights are denoted Θ∗, where Θ∗
t are the adversarial weights at the t-th
iteration of PGA. We denote the PGA-adversary as a function A that maps parameters Θ to attacking
parameters Θ∗. We denote a mini-batch of training examples as X with y being the corresponding
ground-truth labels. Q"
METHODS,0.05008635578583765,"Ep
ζ (m) denotes the projection operator on the ζ-ellipsoid in lp space. The
operator ⊙denotes elementwise multiplication."
METHODS,0.05181347150259067,"The effect of component mismatch on a network parameter can be modelled using a Gaussian
distribution where the standard deviation depends on the parameter magnitude (Joshi et al., 2020;
Büchel et al., 2021). In this paper we restrict ourselves to mismatch-driven perturbations in the
network weights. For complex Spiking Neural Networks (SNNs), “network parameters” can refer to
additional quantities such as neuronal and synaptic time constants or spiking thresholds. Our training
approach described here can be equally applied to these additional parameters."
METHODS,0.0535405872193437,We deﬁne the value of an individual parameter when deployed on a neuromorphic chip as
METHODS,0.055267702936096716,"Θmismatch ∼N(Θ, diag(ζ|Θ|))
(1)"
METHODS,0.05699481865284974,"where ζ governs the perturbation magnitude, referred to as the “mismatch level”. The physics underly-
ing the neuronal- and synaptic circuits lead to a model where the amount of noise introduced into the
system depends linearly on the magnitude of the parameters. If mismatch-induced perturbations had
constant standard deviation independent of weight values, one could use the weight-scale invariance"
METHODS,0.05872193436960276,Published as a conference paper at ICLR 2022
METHODS,0.06044905008635579,"of neural networks as a means to achieve robustness, by simply scaling up all network weights (see
Figure S4). The linear dependence of weight magnitude and mismatch noise precludes this approach."
METHODS,0.06217616580310881,"In contrast to adversarial attacks in the input space (Carlini & Wagner, 2016; Moosavi-Dezfooli
et al., 2015; Madry et al., 2019; Goodfellow et al., 2015), our method relies on adversarial attacks
in parameter space. During training, we approximate the worst case perturbation of the network
parameters using PGA and update the network parameters in order to mitigate these attacks. To
trade-off robustness and performance, we use a surrogate loss (Zhang et al., 2019) to capture the
difference in output between the normal and attacked network. Algorithm 1 illustrates the training
procedure in more detail. begin"
METHODS,0.06390328151986183,"Θ∗
0 ←−Θ + |Θ|ϵ ⊙R ; R ∼N(0, 1)
for t = 1 to Nsteps do"
METHODS,0.06563039723661486,"g ←−∇Θ∗
t−1Lrob(Θ, Θ∗
t−1, X)
v ←−arg
max
v:∥v∥p≤1 vT g"
METHODS,0.06735751295336788,"Θ∗
t ←Q"
METHODS,0.0690846286701209,"Ep
ζattack(Θ∗
t−1 + α ⊙v)"
METHODS,0.07081174438687392,"end
Θ ←−Θ −η∇ΘLnat((Θ, X), y) + βrobLrob(Θ, Θ∗
Nsteps, X)
end
Algorithm 1: In l∞, v corresponds to sign(g) and the step size α is |Θ|⊙ζ"
METHODS,0.07253886010362694,Nsteps . Q
METHODS,0.07426597582037997,"Ep
ζattack (m) de-
notes the projection operator on the ζattack-ellipsoid in lp space. In l∞this corresponds to
min(max(m, Θ −ϵ), Θ + ϵ) with ϵ = ζattack ⊙|Θ|. ζattack and βrob are hyperparameters of our
model."
METHODS,0.07599309153713299,"Unlike adversarial training in the input space, where adversarial inputs can be seen as a form of data
augmentation, adversarial training in the parameter space poses the following challenge: Because
the parameters that are attacked are the same parameters being optimized, performing gradient
descent using the same loss that was used for PGA would simply revert the previous updates and
no learning would occur. ABCD circumvents this problem by masking one half of the parameters
in the adversarial loop and masking the other half during the gradient descent step. However, this
limits the adversary in its power, and requires multiple iterations to be performed in order to update
all parameters at least once. AWP approached this problem by assuming that the gradient of the loss
with respect to the attacking parameters can be used in order to update the original parameters to
favor minima in ﬂatter locations in weight-space. However, it is not clear whether this assumption
always holds since the gradient of the loss with respect to the attacking parameters is not necessarily
the same direction that would lead to a ﬂatter region in the weight loss-landscape."
METHODS,0.07772020725388601,"We approach this problem slightly differently: Similar to the TRADES algorithm (Zhang et al., 2019),
our algorithm optimizes a natural (task) loss and a separate robustness loss."
METHODS,0.07944732297063903,"Lgen(Θ, X, y) = Lnat(Θ, X, y) + βrobLrob(Θ, A(Θ), X)"
METHODS,0.08117443868739206,"Using a different loss for capturing the susceptibility of the network to adversarial attacks enables
us to simultaneously optimise for performance and robustness, without PGA interfering with the
gradient descent step. In our experiments, Lrob is deﬁned as"
METHODS,0.08290155440414508,"Lrob(Θ, Θ∗, X) = KL (f(Θ, X), f(Θ∗, X))
(2)"
METHODS,0.0846286701208981,"This formulation comes with a large computational overhead since it requires computing the Jacobian
JΘ∗(Θ) of a complex recurrent relation between Θ and Θ∗. To make our algorithm more efﬁcient
we assume that the Jacobian is diagonal, meaning that Θ∗= Θ + ∆Θ for some ∆Θ given by the
adversary. In l∞, the Jacobian can then be calculated efﬁciently using (see suppl. material for details):"
METHODS,0.08635578583765112,"JΘ∗(Θ) = I + diag
h
sign(Θ)⊙(ζattack+ϵ·R1)"
METHODS,0.08808290155440414,"Nsteps
⊙PNsteps
t=1 sign
 
∇Θ∗
t Lrob(Θ, Θ∗
t , X
i"
METHODS,0.08981001727115717,"By making this assumption, our algorithm effectively multiplies the original training time by the
number of PGA steps, similar to (Wu et al., 2020; Cicek & Soatto, 2019; Zheng et al., 2020)."
METHODS,0.09153713298791019,Published as a conference paper at ICLR 2022
METHODS,0.09326424870466321,"Because component mismatch is independently proportional to the magnitude of each parameter, one
has to model the space in which the adversary can search for a perturbation using an axis-aligned
ellipsoid in l2 and an axis-aligned box in l∞. Using an ϵ-ball where the radius depends linearly on
the individual parameter sets (Li et al., 2018; Cicek & Soatto, 2019; Wu et al., 2020) would either
give the adversary too little or too much attack space. Projecting onto an axis-aligned ellipsoid in
l2 corresponds to solving the following optimization problem (Gabay & Mercier, 1976; Dai, 2006),
which does not have a closed-form solution:"
METHODS,0.09499136442141623,"x∗= arg min
x
1
2∥m −x∥2"
METHODS,0.09671848013816926,s.t. (x −c)T W −2(x −c) ≤1
METHODS,0.09844559585492228,"where W = diag(|Θ| ⊙ζ) + I · ζconst, c = Θ and m = Θ∗+ α ⊙v. Because of the computational
overhead this would incur, we only consider the l∞case in our experiments."
RESULTS,0.1001727115716753,"4
RESULTS"
RESULTS,0.10189982728842832,"The ultra-low power consumption of mixed-signal neuromorphic chips make them suitable for edge-
applications, such as always-on voice detection (Cho et al., 2019), vibration monitoring (Gies et al.,
2021) or always-on face recognition (Liu et al., 2019). For this reason, we consider two compact
network architectures in our experiments: A Long Short-term spiking recurrent Neural Network
(LSNN) with roughly 65k trainable parameters; a conventional CNN with roughly 500k trainable
parameters; and a Resnet32 architecture (He et al., 2015) (see Supplementary Material S1 for more
information). We trained models to perform four different tasks:"
RESULTS,0.10362694300518134,"• Speech command detection of 6 classes (Warden, 2018);
• ECG-anomaly detection on 4 classes (Bauer et al., 2019);
• Fashion-MNIST (F-MNIST): clothing-image classiﬁcation on 10 classes (Xiao et al., 2017);
and
• The Cifar10 colour image classiﬁcation task (Krizhevsky, 2009)."
RESULTS,0.10535405872193437,"We compared several training and attack methods, beginning with a standard Stochastic Gradient
Descent (SGD) approach using the Adam optimizer (Kingma & Ba, 2015) (“Standard”). Learning
rate varied by architecture, but was kept constant when comparing training methods on an architecture.
We examined networks trained with dropout (Srivastava et al., 2014), AWP (Wu et al., 2020), AMP
(Zheng et al., 2020), ABCD (Cicek & Soatto, 2019), and Entropy-SGD (Chaudhari et al., 2019).
The adversarial perturbations used in AWP and ABCD were adapted to our mismatch model (i.e.
magnitude-dependent in l∞) unless stated otherwise. AMP was not adapted."
RESULTS,0.1070811744386874,"A dropout probability of 0.3 was used in the dropout models and γ in AWP was set to 0.1. When
Gaussian noise was applied to the weights during the forward pass (Murray & Edwards, 1994) a
relative standard deviation of 0.3 times the weight magnitude was used (ηtrain = 0.3). For Entropy-
SGD, we set the number of inner iterations to 10 with a Langevin learning rate of 0.1. Because
Entropy-SGD and ABCD have inner loops, the number of total epochs were reduced accordingly. All
other models were trained for the same number of epochs (no early stopping) and the model with the
highest validation accuracy was selected."
RESULTS,0.10880829015544041,"Effectiveness of adversarial weight attack
We examined the strength of our adversarial weight
attack during inference and training. Standard networks trained using gradient descent alone with no
additional regularization (Fig. S5a, “Standard”) were disrupted badly by our adversarial attack during
inference (ζ = 0.1; ﬁnal mean test accuracy 91.40% →17.50%), and this was not ameliorated by
further training. When our adversarial attack was implemented during training (Fig. S5a, βrob = 0.1),
the trained network was protected from disruption both during training and during inference (ﬁnal
test accuracy 91.97% →78.41%)."
RESULTS,0.11053540587219343,"Our adversarial attack degrades network performance signiﬁcantly more than a random perturbation.
Because our adversary uses PGA during the attack, it approximates a worst-case perturbation of
the network within an ellipsoid around the nominal weights Θ. We compared the effect of our
attack against a random weight perturbation (random point on ζ−ellipsoid) of equal magnitude."
RESULTS,0.11226252158894647,Published as a conference paper at ICLR 2022
RESULTS,0.11398963730569948,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.1157167530224525,Cross Entropy Loss
RESULTS,0.11744386873920552,F-MNIST CNN
RESULTS,0.11917098445595854,"AWP
Beta
Forward Noise + Beta
Dropout
Forward Noise
Standard"
RESULTS,0.12089810017271158,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
RESULTS,0.1226252158894646,Cross Entropy Loss
RESULTS,0.12435233160621761,ECG LSNN
RESULTS,0.12607944732297063,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2"
RESULTS,0.12780656303972365,Cross Entropy Loss
RESULTS,0.12953367875647667,Speech LSNN
RESULTS,0.13126079447322972,"Figure 1: Our training method ﬂattens the test weight-loss landscape. When moving away from
the trained weight minimum (α = 0) in randomly-chosen directions, we ﬁnd that our adversarial
training method (Beta; Beta+Forward) ﬁnds deeper minima (for F-MNIST and Speech tasks) at
ﬂatter locations in the cross-entropy test loss landscape. See text for further details, and Fig. S2 for
visualisation over several random seeds."
RESULTS,0.13298791018998274,"For increasing perturbation size during inference (Fig. S5c; ζ), our adversarial attack disrupted
the performance of the standard network signiﬁcantly more than a random perturbation (test acc.
91.40% →17.50% (attack) vs. 91.40% →90.63% (random) for ζ = 0.1). When our adversarial
attack was applied during training (Fig. S5d; βrob = 0.1), the network was protected against both
random and adversarial attacks for magnitudes up to ζ = 0.7 and ζ = 0.1, respectively."
RESULTS,0.13471502590673576,"Flatness of the weight-loss landscape
Under our deﬁnition of robustness, the network output
should change only minimally when the network parameters are perturbed. This corresponds to a
loss surface that is close to ﬂat in weight space. We measured the test weight-loss landscape for
trained networks, compared over alternative training methods and for several architectures (Fig. 1).
We examined only cross-entropy loss over the test set, and not the adversarial attack loss component
(KL divergence loss; see Eq. 2). For each trained network, we chose a random vector v ∼N(0, ζ|Θ|)
and calculated Lcce(f(Xtest, Θ + α · v), ytest) for many evenly-spaced α ∈[−2, 2]. This process
was repeated 5 times for ζ = 0.2, and the means plotted in Fig. 1. Weight-loss landscapes for the
individual trials are shown in Fig. S2."
RESULTS,0.13644214162348878,"Our adversarial training approach found minima of trained parameters Θ in ﬂatter areas of the weight-
loss landscape, compared with all other approaches examined (ﬂatter curves in Fig. 1). In most
cases our training approach also found deeper minima at lower categorical cross-entropy loss (Lcce),
reﬂecting better task performance. These results are reﬂected in the better generalization performance
of our approach (see Table 1). Not surprisingly, dropout and AWP also lead to ﬂatter minima than
the Standard network with no regularization. ABCD and Entropy-SGD were not included in Fig. 1
because they did not outperform the Standard model."
RESULTS,0.1381692573402418,"Network robustness against parameter mismatch
We evaluated the ability of our training
method to protect against simulated device mismatch. We introduced frozen parameter noise into
models trained with adversarial attack, with noise modelled on that observed in neuromorphic pro-
cessors (Joshi et al., 2020; Büchel et al., 2021). In these devices, uncertainty associated with each
weight parameter is approximately normally distributed around the nominal value, with a standard
deviation that scales with the weight magnitude (Eq 1). We measured test accuracy under simulated
random mismatch for 100 samples across two model instances. A comparison of our method against
standard training is shown in Fig. 2. For mismatch levels up to 70% (ζ = 0.7), our approach protected
signiﬁcantly against simulated mismatch for all three tasks examined (p < 2 × 10−8 in all cases; U
test). A more detailed comparison between the different models is given in Table 1."
RESULTS,0.13989637305699482,"Network robustness against direct adversarial attack on task performance
Our training ap-
proach improves network robustness against mismatch parameter noise. We further evaluated the
robustness of our trained networks against a parameter adversary that directly attacks task perfor-
mance, by performing PGA on the cross-entropy loss Lcce. Note that this is separate from the
adversary used in our training method, which attacks the boundary loss (Eq.2). The AWP method
uses the cross-entropy loss to ﬁnd adversarial parameters during training. Nevertheless, we found that"
RESULTS,0.14162348877374784,Published as a conference paper at ICLR 2022 a
RESULTS,0.14335060449050085,"Robust
Standard"
RESULTS,0.14507772020725387,"0.1
0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.14680483592400692,Test acc.
RESULTS,0.14853195164075994,"0.2
0.3"
RESULTS,0.15025906735751296,Mismatch level (ζ)
RESULTS,0.15198618307426598,"0.5
0.7"
RESULTS,0.153713298791019,"Standard
Ours"
RESULTS,0.15544041450777202,Robust b
RESULTS,0.15716753022452504,"Groundtruth
Standard"
RESULTS,0.15889464594127806,"0.1
0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.16062176165803108,"0.2
0.3"
RESULTS,0.16234887737478412,Mismatch level (ζ)
RESULTS,0.16407599309153714,"0.5
0.7 c"
RESULTS,0.16580310880829016,"Robust
Standard"
RESULTS,0.16753022452504318,"0.1
0.0 0.2 0.4 0.6 0.8 1.0"
RESULTS,0.1692573402417962,"0.2
0.3"
RESULTS,0.17098445595854922,Mismatch level (ζ)
RESULTS,0.17271157167530224,"0.5
0.7"
RESULTS,0.17443868739205526,"Figure 2:
Adversarial attack during training protects networks against random mismatch-
induced parameter noise. Networks were evaluated for the Speech (a), ECG (b) and F-MNIST
tasks (c), under increasing levels of simulated mismatch (ζ). Networks trained using standard SGD
were disrupted by mismatch levels ζ > 0.1. At all mismatch levels our adversarial training approach
performed signiﬁcantly better in the presence of mismatch (higher test accuracy). Red boxes highlight
misclassiﬁed examples."
RESULTS,0.17616580310880828,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 0 20 40 60 80"
RESULTS,0.17789291882556132,Test Acc.
RESULTS,0.17962003454231434,F-MNIST CNN
RESULTS,0.18134715025906736,"Forward Noise + Beta
Beta
Standard
Forward Noise
AWP"
RESULTS,0.18307426597582038,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 20 40 60 80 100"
RESULTS,0.1848013816925734,Test Acc.
RESULTS,0.18652849740932642,ECG LSNN
RESULTS,0.18825561312607944,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 20 40 60 80"
RESULTS,0.18998272884283246,Test Acc.
RESULTS,0.19170984455958548,Speech LSNN
RESULTS,0.19343696027633853,"Figure 3: Our training method protects against task-adversarial attacks in parameter space.
Networks trained under several methods were attacked using a PGA adversary that directly attacked
the task performance Lcce. Our adversarial training approach (bold; dashed) outperformed all other
methods against this attack."
RESULTS,0.19516407599309155,"our method consistently outperforms all other compared methods for increasing attack magnitude ζ
(Fig. 3). In networks trained with our method, the adversary needed to perform a considerably larger
attack to signiﬁcantly reduce performance (test accuracy < 70%). ABCD and Entropy-SGD were not
included in the comparison because they did not outperform the standard network."
RESULTS,0.19689119170984457,"Robustness against parameter drift for PCM based CiM
CiM devices based on memristor
technologies such as PCM promise to deliver energy- and space-efﬁcient accelerators. With the
increasing interest in CiM devices for accelerated inference and energy-efﬁcient edge computing
(Sebastian et al., 2020), the problem of deploying a model that is robust to noise originating from the
device physics of PCM cells has gained in signiﬁcance. We investigated the effect of our training
method on the robustness of networks that are simulated to run on PCM-based CiM hardware (for
details on the simulator, see SM 5)."
RESULTS,0.19861830742659758,"Networks trained with our method outperform state-of-the-art networks deployed on PCM-based
CiM. Currently, the method that has proven to yield the best performance on CiM hardware is training
with noise on the network parameters during the forward pass. We adapted this method by adding
our algorithm and show that we consistently outperform the conventional method (see Figure S14)
for a wide range of hyperparameters, and even surpass the FP baseline (a model trained without
noise injection and evaluated on a standard PC) for some conﬁgurations (see Figure 4). Following"
RESULTS,0.2003454231433506,Published as a conference paper at ICLR 2022
RESULTS,0.20207253886010362,"102
103
104
105
106
107"
RESULTS,0.20379965457685664,Tinf (s) 91.5 92.0 92.5 93.0
RESULTS,0.20552677029360966,Test acc. (%)
RESULTS,0.20725388601036268,attack = 0.01
RESULTS,0.20898100172711573,FP Baseline
RESULTS,0.21070811744386875,"rob = 0.0,
train = 0.110"
RESULTS,0.21243523316062177,"102
103
104
105
106
107"
RESULTS,0.2141623488773748,Tinf (s) 91.5 92.0 92.5 93.0 93.5
RESULTS,0.2158894645941278,Test acc. (%)
RESULTS,0.21761658031088082,attack = 0.03
RESULTS,0.21934369602763384,"102
103
104
105
106
107"
RESULTS,0.22107081174438686,Tinf (s) 91.5 92.0 92.5 93.0
RESULTS,0.22279792746113988,Test acc. (%)
RESULTS,0.22452504317789293,attack = 0.05
RESULTS,0.22625215889464595,"102
103
104
105
106
107"
RESULTS,0.22797927461139897,Tinf (s) 91.25 91.50 91.75 92.00 92.25 92.50 92.75 93.00 93.25
RESULTS,0.229706390328152,Test acc. (%)
RESULTS,0.231433506044905,attack = 0.10 0.01 0.025 0.05 0.1 rob
RESULTS,0.23316062176165803,"Figure 4: Networks trained with our method show overall better performance when deployed
on PCM-based CiM hardware. This ﬁgure shows the performance degradation as a consequence of
the PCM devices drifting over time (x-axis, up to one year) of networks deployed on CiM hardware.
Each subplot shows networks trained with a different attacking magnitude (ζattack) that are trained
with different values of βrob. Each network is compared to the FP-baseline and a network trained
with Gaussian noise injection on the weights."
RESULTS,0.23488773747841105,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.23661485319516407,train = 90 91 92 93
RESULTS,0.23834196891191708,Test acc. (%)
RESULTS,0.24006908462867013,attack = 0.01
RESULTS,0.24179620034542315,FP Baseline
RESULTS,0.24352331606217617,rob = 0.0
RESULTS,0.2452504317789292,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.2469775474956822,train = 90 91 92 93
RESULTS,0.24870466321243523,Test acc. (%)
RESULTS,0.2504317789291883,attack = 0.03
RESULTS,0.25215889464594127,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.2538860103626943,train = 90 92
RESULTS,0.2556131260794473,Test acc. (%)
RESULTS,0.25734024179620035,attack = 0.05
RESULTS,0.25906735751295334,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.2607944732297064,train = 90 92
RESULTS,0.26252158894645944,Test acc. (%)
RESULTS,0.26424870466321243,attack = 0.10
RESULTS,0.2659758203799655,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.26770293609671847,train = 92.0 92.5 93.0 93.5
RESULTS,0.2694300518134715,Test acc. (%)
RESULTS,0.2711571675302245,attack = 0.01
RESULTS,0.27288428324697755,FP Baseline
RESULTS,0.27461139896373055,rob = 0.0
RESULTS,0.2763385146804836,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.27806563039723664,train = 92.0 92.5 93.0 93.5
RESULTS,0.27979274611398963,Test acc. (%)
RESULTS,0.2815198618307427,attack = 0.03
RESULTS,0.28324697754749567,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.2849740932642487,train = 92.0 92.5 93.0 93.5
RESULTS,0.2867012089810017,Test acc. (%)
RESULTS,0.28842832469775476,attack = 0.05
RESULTS,0.29015544041450775,"0.026 0.037
0.056
0.075
0.110"
RESULTS,0.2918825561312608,train = 92.0 92.5 93.0 93.5
RESULTS,0.29360967184801384,Test acc. (%)
RESULTS,0.29533678756476683,attack = 0.10 0.01 0.025 0.05 0.1 rob
RESULTS,0.2970639032815199,"Figure 5:
Our method consistently yields networks that outperform training with Gaussian
noise injection. This ﬁgure compares the robustness to Gaussian noise at various levels (ζ) for
networks trained with our method (blue) and a networks trained with Gaussian noise injection (red),
where the level of noise used during training (ηtrain) matches the noise used during inference. Each
row represents a different type of noise: The ﬁrst row models Gaussian noise with a standard deviation
that is proportional to the largest absolute weight in the individual weight kernels (Joshi et al., 2020)
and the second row follows the model presented in this paper (see Eq. 1)."
RESULTS,0.2987910189982729,"experiments conducted in (Joshi et al., 2020), we used Resnet32 (He et al., 2015) trained on Cifar10
(Krizhevsky, 2009)."
RESULTS,0.3005181347150259,"Injecting Gaussian noise on the weights during the forward pass yields strong improvements compared
to the standard network. We show that by adding our method, we consistently improve this robustness
by a signiﬁcant amount (see Figure 5)."
RESULTS,0.3022452504317789,"We furthermore improve the scalability of our method by using a pretrained model and fewer steps for
the adversary. Our algorithm incurs an additional training time that scales linearly with the number
of attack steps used in the adversary (note that we cache the necessary gradients for the Jacobian
calculation). To alleviate this additional time, we show that our method produces good results even
for just one single adversarial step. Figure S13 shows the resulting performance when varying the
number of attack steps used by the adversary. It should be noted that all results reported on PCM
robustness were obtained using three adversarial steps and a pretrained model."
RESULTS,0.30397236614853196,"Veriﬁable robustness for LSNNs
We investigated the provable robustness of LSNNs trained using
our method using abstract interpretation (Cousot & Cousot, 1977; Gehr et al., 2018; Mirman et al.,
2018). In this analysis a function f(x, Θ) (in our case, a neural network with input x and parameters
Θ) is overapproximated using an abstract domain. We specify the weights Θ in our network as
an interval parameterised by the attack size ζ, spanning [Θ −ζ|Θ|, Θ + ζ|Θ|]. We examined the
proportion of provably correctly classiﬁed test samples for the Speech and ECG tasks, under a range
of attack sizes ζ, and comparing our approach against standard gradient descent and against training"
RESULTS,0.30569948186528495,Published as a conference paper at ICLR 2022
RESULTS,0.307426597582038,"with forward-pass noise only (Fig. S9). We found that our approach is provably more correct over
increasing attack size ζ (higher veriﬁed test accuracy)."
DISCUSSION,0.30915371329879104,"5
DISCUSSION"
DISCUSSION,0.31088082901554404,"We proposed a new training approach that includes adversarial attacks on the parameter space during
training. Our proposed adversarial attack was signiﬁcantly stronger than random weight perturbations
at disrupting the performance of a trained network. Including the adversarial attack during training
signiﬁcantly protected the trained network from weight perturbations during inference. Our approach
found minima in the weight-loss landscape that usually corresponded to lower loss values, and were
always in ﬂatter regions of the loss landscape. This indicates that our approach found network
solutions that are less sensitive to parameter variation, and therefore more robust. Our approach was
more robust than several other methods for inducing robustness and good generalisation. To the best
of our knowledge, our work represents the ﬁrst example of interval bound propagation applied to
SNNs, and the ﬁrst application of parameter-space adversarial attacks to promote network robustness
against device mismatch for mixed-signal compute. Our experiments only considered the impact
of weight perturbations, and did not examine the inﬂuence of uncertainty in other parameters of
mixed-signal neuromorphic processors such as time constants or spiking thresholds. Our approach
can be adapted to include adversarial attacks in the full network parameter space, increasing the
robustness of spiking networks. The technique of interval bound propagation can also be applied to
these additional network parameters. We did not quantize network parameters either during or after
training, in this work. On some platforms (Moradi et al., 2018) it is necessary to deploy quantized
weights and it is unclear how our adversarial attacks would interact with quantization during the
training process. However, most PCM-based CiM hardware does not require quantization during
training in order to get good performance (Joshi et al., 2020). Per-device training for a device with
known calibrated parameter noise is likely to achieve the highest possible deployed performance on
that single device. However, this approach has signiﬁcant drawbacks. Firstly, each device must be
either measured / calibrated accurately — not a trivial requirement — or trained with the device in
the forward inference pass of the training loop. Secondly, training must be performed individually
for each device, entailing signiﬁcant logistical problems if the training is conducted in the factory or
inside a consumer product. Thirdly, this approach will retain full sensitivity to parameter variation on
the device. Our method improves the performance of neural networks deployed to inference hardware
that include computational non-idealities. For example, NN processors with crossbar architectures
based on novel memory devices such as RRAM and PCM (Sebastian et al., 2020) display uncertainty
in stored memory values as well as conductance drift over time (Le Gallo et al., 2018b; Wu et al.,
2019; Joshi et al., 2020). Our method could also address in-memory computing-based NN processors
based on SRAM and switched capacitors (Verma et al., 2019). Analog neurons and synapses in
mixed-signal NN processors, for example SNN inference processors (Moradi et al., 2018), exhibit
variation in weights and neuron parameters across a processor. We showed that our training approach
ﬁnds network solutions that are insensitive to mismatch-induced parameter variation. Our networks
can therefore be deployed to inference devices with computational non-idealities with only minimal
reduction in task performance, and without requiring per-device calibration or model training. This
reduction in per-device handling implies a considerable reduction in expense when deploying at
commercial scale. Our method therefore brings low-power neuromorphic inference processors closer
to commercial viability."
ETHICS STATEMENT,0.3126079447322971,"Ethics statement
The authors declare no conﬂicts of interest."
REPRODUCIBILITY STATEMENT,0.3143350604490501,"Reproducibility statement
Code for reproduce all experiments described in this work are provided
at https://github.com/jubueche/BPTT-Lipschitzness and https://github.
com/jubueche/Resnet32-ICLR"
REPRODUCIBILITY STATEMENT,0.3160621761658031,"Acknowledgments
This work was partially supported by EU grants 826655 “TEMPO”; 871371
“MEMSCALES”; and 876925 “ANDANTE” to DRM. JB would also like to thank Manuel Le Gallo-
Bourdeau, Irem Boybat and Abu Sebastian from IBM Research - Zurich, for insightful discussions
and technical support."
REPRODUCIBILITY STATEMENT,0.3177892918825561,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.31951640759930916,"Table 1:
Results of training multiple networks using several methods over three tasks.
Networks were evaluated under different levels of mismatch (ζ)."
REPRODUCIBILITY STATEMENT,0.32124352331606215,"CNN
Forward Noise, βrob = 0.1
βrob = 0.25
Standard
Forward Noise
AWP (ϵpga = 0.0)
Dropout
AMP (ϵ = 0.005)"
REPRODUCIBILITY STATEMENT,0.3229706390328152,"Mismatch
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Baseline (0.0)
91.88
0.00
91.88
92.11
0.22
91.89
91.30
0.15
91.15
92.00
0.02
91.98
92.35
0.12
92.23
92.42
0.24
92.18
91.34
0.12
91.22
0.1
91.77
0.12
91.29
91.59
0.26
90.91
90.45
0.37
88.80
91.94
0.09
91.69
92.19
0.16
91.78
91.13
0.85
86.50
90.42
0.35
89.37
0.2
91.62
0.16
91.19
90.67
0.42
89.28
87.93
1.03
83.40
91.63
0.16
91.06
91.42
0.44
89.87
88.71
1.55
80.97
87.97
0.88
85.33
0.3
91.25
0.22
90.36
89.64
0.65
87.10
82.70
2.45
71.99
91.01
0.26
90.01
89.88
0.95
85.11
84.94
3.06
72.97
83.13
2.13
75.73
0.5
89.36
0.73
86.84
85.96
1.87
76.28
61.14
6.92
42.46
87.82
0.88
84.37
82.59
3.66
66.73
71.74
5.84
53.10
60.60
7.36
38.67
0.7
84.19
2.63
74.25
79.39
4.40
59.15
36.93
7.73
20.17
78.48
2.95
69.67
65.38
8.27
40.66
53.91
9.03
22.49
36.79
7.42
18.53"
REPRODUCIBILITY STATEMENT,0.32469775474956825,"ECG LSNN
Forward Noise, βrob = 0.1
βrob = 0.25
Standard
Forward Noise
AWP (ϵpga = 0.0)
Dropout
AMP (ϵ = 0.02)"
REPRODUCIBILITY STATEMENT,0.32642487046632124,"Mismatch
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Baseline (0.0)
99.07
0.34
98.73
99.10
0.07
99.03
99.07
0.04
99.03
99.25
0.37
98.88
99.22
0.19
99.03
98.10
0.11
97.99
99.40
0.00
99.40
0.1
99.04
0.27
98.36
98.96
0.24
98.28
98.95
0.27
97.76
99.09
0.19
98.66
98.93
0.31
97.69
97.71
0.34
96.72
99.04
0.38
97.39
0.2
98.87
0.35
96.94
98.16
0.71
93.96
97.22
1.46
91.87
99.01
0.26
98.06
97.71
0.97
93.06
96.89
0.87
93.81
97.03
1.52
90.67
0.3
98.45
0.45
96.49
96.34
1.95
89.33
92.97
4.38
65.30
98.59
0.52
95.30
94.60
2.93
81.34
94.85
2.47
85.60
92.24
3.92
76.27
0.5
94.86
2.69
82.39
86.22
6.66
60.75
76.55
9.61
35.75
94.44
2.86
82.84
80.32
8.08
41.94
87.56
6.05
64.10
73.36
11.27
29.55
0.7
82.02
8.40
50.22
70.07
11.16
39.33
58.67
11.44
29.48
80.00
8.70
37.69
63.44
9.61
32.91
74.24
12.58
30.15
56.64
11.02
27.76"
REPRODUCIBILITY STATEMENT,0.3281519861830743,"Speech LSNN
Forward Noise, βrob = 0.5
βrob = 0.5
Standard
Forward Noise
AWP (ϵpga = 0.01)
Dropout
AMP (ϵ = 0.01)"
REPRODUCIBILITY STATEMENT,0.3298791018998273,"Mismatch
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Mean Acc.
Std.
Min.
Baseline (0.0)
81.52
0.05
81.47
82.48
0.00
82.48
80.86
0.03
80.83
81.33
0.14
81.20
82.38
0.14
82.25
79.02
0.36
78.66
80.83
0.44
80.39
0.1
81.33
0.24
80.72
82.01
0.31
81.03
79.72
0.45
78.53
81.12
0.32
80.18
82.03
0.37
80.79
79.16
0.44
77.88
80.00
0.55
78.49
0.2
80.77
0.35
79.71
81.24
0.43
79.98
76.96
1.11
72.57
80.10
0.53
78.73
80.58
0.65
78.69
78.58
0.67
75.85
77.56
0.87
74.81
0.3
79.57
0.62
77.98
79.80
0.67
77.54
72.31
2.00
65.40
78.05
0.88
75.45
77.47
1.35
70.88
77.18
0.96
74.43
73.80
1.61
68.24
0.5
72.67
2.75
63.85
73.40
2.32
60.91
57.16
4.25
42.07
67.41
3.93
53.50
63.98
4.28
47.38
69.80
3.13
54.75
59.91
4.49
44.74
0.7
58.03
6.57
32.19
60.23
4.66
45.49
40.70
5.72
24.92
49.19
6.98
30.47
44.83
6.65
23.13
55.96
5.74
37.74
42.88
6.83
25.03"
REFERENCES,0.3316062176165803,REFERENCES
REFERENCES,0.3333333333333333,"Stefano Ambrogio, Pritish Narayanan, Hsinyu Tsai, Robert M. Shelby, Irem Boybat, Carmelo
di Nolfo, Severin Sidler, Massimo Giordano, Martina Bodini, Nathan C. P. Farinha, Benjamin
Killeen, Christina Cheng, Yassine Jaoudi, and Geoffrey W. Burr. Equivalent-accuracy accel-
erated neural-network training using analogue memory. Nature, 558(7708):60–67, June 2018.
ISSN 1476-4687. doi: 10.1038/s41586-018-0180-5. URL https://doi.org/10.1038/
s41586-018-0180-5."
REFERENCES,0.33506044905008636,"F. C. Bauer, D. R. Muir, and G. Indiveri. Real-time ultra-low power ecg anomaly detection using an
event-driven neuromorphic processor. IEEE Transactions on Biomedical Circuits and Systems, 13
(6):1575–1582, 2019. doi: 10.1109/TBCAS.2019.2953001."
REFERENCES,0.33678756476683935,"Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert A. Legenstein, and Wolfgang
Maass. Long short-term memory and learning-to-learn in networks of spiking neurons. CoRR,
abs/1803.09574, 2018. URL http://arxiv.org/abs/1803.09574."
REFERENCES,0.3385146804835924,"Irem Boybat, Manuel Le Gallo, SR Nandakumar, Timoleon Moraitis, Thomas Parnell, Tomas Tuma,
Bipin Rajendran, Yusuf Leblebici, Abu Sebastian, and Evangelos Eleftheriou. Neuromorphic
computing with multi-memristive synapses. Nature communications, 9(1):2514, 2018."
REFERENCES,0.34024179620034545,"Julian Büchel, Dmitrii Zendrikov, Sergio Solinas, Giacomo Indiveri, and Dylan R. Muir. Super-
vised training of spiking neural networks for robust deployment on mixed-signal neuromorphic
processors, 2021."
REFERENCES,0.34196891191709844,"Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR,
abs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644."
REFERENCES,0.3436960276338515,"Andrew S. Cassidy, Jun Sawada, Paul Merolla, John V. Arthur, Rodrigo Alvarez-Icaza, Filipp
Akopyan, Bryan L. Jackson, and Dharmendra S. Modha. Truenorth: A high-performance, low-
power neurosynaptic processor for multi-sensory perception, action, and cognition. 2016."
REFERENCES,0.3454231433506045,"Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: biasing gradient
descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):
124018, dec 2019. doi: 10.1088/1742-5468/ab39d9. URL https://doi.org/10.1088%
2F1742-5468%2Fab39d9."
REFERENCES,0.3471502590673575,"An Chen.
A review of emerging non-volatile memory (nvm) technologies and applications.
Solid-State Electronics, 125:25–38, 2016. ISSN 0038-1101. doi: https://doi.org/10.1016/j.sse.
2016.07.006.
URL https://www.sciencedirect.com/science/article/pii/
S0038110116300867. Extended papers selected from ESSDERC 2015."
REFERENCES,0.3488773747841105,"Minchang Cho, Sechang Oh, Zhan Shi, Jongyup Lim, Yejoong Kim, Seokhyeon Jeong, Yu Chen,
David Blaauw, Hun-Seok Kim, and Dennis Sylvester. 17.2 a 142nw voice and acoustic activity"
REFERENCES,0.35060449050086356,Published as a conference paper at ICLR 2022
REFERENCES,0.35233160621761656,"detection chip for mm-scale sensor nodes using time-interleaved mixer-based frequency scanning.
In 2019 IEEE International Solid- State Circuits Conference - (ISSCC), pp. 278–280, 2019. doi:
10.1109/ISSCC.2019.8662540."
REFERENCES,0.3540587219343696,"Safa Cicek and Stefano Soatto. Input and weight space smoothing for semi-supervised learning. In
2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 1344–1353,
2019. doi: 10.1109/ICCVW.2019.00170."
REFERENCES,0.35578583765112265,"Patrick Cousot and Radhia Cousot. Abstract interpretation: A uniﬁed lattice model for static analysis
of programs by construction or approximation of ﬁxpoints. In Proceedings of the 4th ACM
SIGACT-SIGPLAN Symposium on Principles of Programming Languages, POPL ’77, pp. 238–252,
New York, NY, USA, 1977. Association for Computing Machinery. ISBN 9781450373500. doi:
10.1145/512950.512973. URL https://doi.org/10.1145/512950.512973."
REFERENCES,0.35751295336787564,"Yu-Hong Dai. Fast algorithms for projection on an ellipsoid. SIAM Journal on Optimization,
16(4):986–1006, 2006.
doi: 10.1137/040613305.
URL https://doi.org/10.1137/
040613305."
REFERENCES,0.3592400690846287,"Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1):
17–40, 1976. ISSN 0898-1221. doi: https://doi.org/10.1016/0898-1221(76)90003-1. URL https:
//www.sciencedirect.com/science/article/pii/0898122176900031."
REFERENCES,0.3609671848013817,"Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 3–18, 2018. doi: 10.1109/SP.2018.00058."
REFERENCES,0.3626943005181347,"Valentin Gies, Sebastián Marzetti, Valentin Barchasz, Hervé Barthélemy, and Hervé Glotin. Ultra-low
power embedded unsupervised learning smart sensor for industrial fault classiﬁcatio. In 2020 IEEE
International Conference on Internet of Things and Intelligence System (IoTaIS), pp. 181–187,
2021. doi: 10.1109/IoTaIS50849.2021.9359716."
REFERENCES,0.3644214162348877,"Xavier Glorot and Yoshua Bengio.
Understanding the difﬁculty of training deep feedforward
neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics, volume 9 of Proceedings of
Machine Learning Research, pp. 249–256, Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010.
JMLR Workshop and Conference Proceedings. URL http://proceedings.mlr.press/
v9/glorot10a.html."
REFERENCES,0.36614853195164077,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples, 2015."
REFERENCES,0.36787564766839376,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385."
REFERENCES,0.3696027633851468,"S. Huang, S. Xiao, and W. Feng. On the energy efﬁciency of graphics processing units for scientiﬁc
computing. In 2009 IEEE International Symposium on Parallel Distributed Processing, pp. 1–8,
2009. doi: 10.1109/IPDPS.2009.5160980."
REFERENCES,0.37132987910189985,"Vinay Joshi, Manuel Le Gallo, Simon Haefeli, Irem Boybat, S. R. Nandakumar, Christophe Piveteau,
Martino Dazzi, Bipin Rajendran, Abu Sebastian, and Evangelos Eleftheriou. Accurate deep
neural network inference using computational phase-change memory. Nature Communications,
11(1):2473, May 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-16108-9. URL https:
//doi.org/10.1038/s41467-020-16108-9."
REFERENCES,0.37305699481865284,"Riduan Khaddam-Aljameh, Milos Stanisavljevic, Jordi Fornt Mas, Geethan Karunaratne, Matthias
Brändli, Feng Liu, Abhairaj Singh, Silvia M Müller, Urs Egger, Anastasios Petropoulos, et al.
HERMES-core–a 1.59-TOPS/mm2 PCM on 14-nm CMOS in-memory compute core using 300-
ps/LSB linearized CCO-based ADCs. IEEE Journal of Solid-State Circuits, 2022."
REFERENCES,0.3747841105354059,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.3765112262521589,Published as a conference paper at ICLR 2022
REFERENCES,0.37823834196891193,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.3799654576856649,"Manuel Le Gallo, Daniel Krebs, Federico Zipoli, Martin Salinga, and Abu Sebastian.
Col-
lective structural relaxation in phase-change memory devices.
Advanced Electronic Mate-
rials, 4(9):1700627, 2018a.
doi: https://doi.org/10.1002/aelm.201700627.
URL https:
//onlinelibrary.wiley.com/doi/abs/10.1002/aelm.201700627."
REFERENCES,0.38169257340241797,"Manuel Le Gallo, Abu Sebastian, Roland Mathis, Matteo Manica, Heiner Giefers, Tomas Tuma,
Costas Bekas, Alessandro Curioni, and Evangelos Eleftheriou. Mixed-precision in-memory
computing. Nature Electronics, 1(4):246–253, April 2018b. ISSN 2520-1131. doi: 10.1038/
s41928-018-0054-8. URL https://doi.org/10.1038/s41928-018-0054-8."
REFERENCES,0.38341968911917096,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf."
REFERENCES,0.385146804835924,"Qian Liu, Ole Richter, Carsten Nielsen, Sadique Sheik, Giacomo Indiveri, and Ning Qiao. Live
demonstration: Face recognition on an ultra-low power event-driven convolutional neural network
asic. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), pp. 1680–1681, 2019. doi: 10.1109/CVPRW.2019.00213."
REFERENCES,0.38687392055267705,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks, 2019."
REFERENCES,0.38860103626943004,"Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for
provably robust neural networks. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3575–
3583. PMLR, 2018. URL http://proceedings.mlr.press/v80/mirman18b.html."
REFERENCES,0.3903281519861831,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015. URL http:
//arxiv.org/abs/1511.04599."
REFERENCES,0.3920552677029361,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Ro-
bustness via curvature regularization, and vice versa.
CoRR, abs/1811.09716, 2018.
URL
http://arxiv.org/abs/1811.09716."
REFERENCES,0.39378238341968913,"S. Moradi, N. Qiao, F. Stefanini, and G. Indiveri.
A scalable multicore architecture with
heterogeneous memory structures for dynamic neuromorphic asynchronous processors (dy-
naps).
IEEE Transactions on Biomedical Circuits and Systems, 12(1):106–122, 2018.
doi:
10.1109/TBCAS.2017.2759700."
REFERENCES,0.3955094991364421,"A.F. Murray and P.J. Edwards. Enhanced mlp performance and fault tolerance resulting from synaptic
weight noise during training. IEEE Transactions on Neural Networks, 5(5):792–802, 1994. doi:
10.1109/72.317730."
REFERENCES,0.39723661485319517,"S. R. Nandakumar, I. Boybat, V. Joshi, C. Piveteau, M. Le Gallo, B. Rajendran, A. Sebastian, and
E. Eleftheriou. Phase-change memory models for deep learning training and inference. In 26th
IEEE International Conference on Electronics, Circuits and Systems (ICECS), pp. 727–730, 2019.
doi: 10.1109/ICECS46596.2019.8964852."
REFERENCES,0.39896373056994816,"S. R. Nandakumar, Manuel Le Gallo, Christophe Piveteau, Vinay Joshi, Giovanni Mariani, Irem
Boybat, Geethan Karunaratne, Riduan Khaddam-Aljameh, Urs Egger, Anastasios Petropoulos,
Theodore Antonakopoulos, Bipin Rajendran, Abu Sebastian, and Evangelos Eleftheriou. Mixed-
precision deep learning based on computational memory. Frontiers in Neuroscience, 14:406, 2020a.
ISSN 1662-453X. doi: 10.3389/fnins.2020.00406. URL https://www.frontiersin.org/
article/10.3389/fnins.2020.00406."
REFERENCES,0.4006908462867012,Published as a conference paper at ICLR 2022
REFERENCES,0.40241796200345425,"SR Nandakumar, Irem Boybat, Jin-Ping Han, Stefano Ambrogio, Praneet Adusumilli, Robert L Bruce,
Matthew BrightSky, Malte Rasch, Manuel Le Gallo, and Abu Sebastian. Precision of synaptic
weights programmed in phase-change memory devices for deep learning inference. In 2020 IEEE
International Electron Devices Meeting (IEDM), pp. 29–4. IEEE, 2020b."
REFERENCES,0.40414507772020725,"Alexander Neckar, Sam Fok, Ben Benjamin, Terrence Stewart, Aaron Voelker, Chris Eliasmith, Rajit
Manohar, and Kwabena Boahen. Braindrop: A mixed-signal neuromorphic architecture with a
dynamical systems-based programming model. Proceedings of the IEEE, 107:144–164, 01 2019.
doi: 10.1109/JPROC.2018.2881432."
REFERENCES,0.4058721934369603,"Yurii Nesterov. A method for solving the convex programming problem with convergence rate
O(1/k2). Proceedings of the USSR Academy of Sciences, 269:543–547, 1983."
REFERENCES,0.4075993091537133,"J. Schemmel, D. Brüderle, A. Grübl, M. Hock, K. Meier, and S. Millner. A wafer-scale neuromorphic
hardware system for large-scale neural modeling. In 2010 IEEE International Symposium on
Circuits and Systems (ISCAS), pp. 1947–1950, 2010. doi: 10.1109/ISCAS.2010.5536970."
REFERENCES,0.40932642487046633,"Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos Eleftheriou. Mem-
ory devices and applications for in-memory computing.
Nature Nanotechnology, 15(7):
529–544, 2020. doi: 10.1038/s41565-020-0655-z. URL https://doi.org/10.1038/
s41565-020-0655-z."
REFERENCES,0.4110535405872193,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting.
Journal of Machine
Learning Research, 15(56):1929–1958, 2014.
URL http://jmlr.org/papers/v15/
srivastava14a.html."
REFERENCES,0.41278065630397237,"David Stutz, Nandhini Chandramoorthy, Matthias Hein, and Bernt Schiele. Random and adversarial
bit error robustness: Energy-efﬁcient and secure dnn accelerators. CoRR, abs/2104.08323, 2021."
REFERENCES,0.41450777202072536,"C. S. Thakur, R. Wang, T. J. Hamilton, R. Etienne-Cummings, J. Tapson, and A. van Schaik. An
analogue neuromorphic co-processor that utilizes device mismatch for learning applications. IEEE
Transactions on Circuits and Systems I: Regular Papers, 65(4):1174–1184, 2018."
REFERENCES,0.4162348877374784,"Naveen Verma, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan
Zhang, and Peter Deaville. In-memory computing: Advances and prospects. IEEE Solid-State
Circuits Magazine, 11(3):43–55, 2019. doi: 10.1109/MSSC.2019.2922889."
REFERENCES,0.41796200345423146,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adver-
sarial robustness requires revisiting misclassiﬁed examples. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=rklOg6EFwS."
REFERENCES,0.41968911917098445,"P. Warden. Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition. ArXiv
e-prints, April 2018. URL https://arxiv.org/abs/1804.03209."
REFERENCES,0.4214162348877375,"Dongxian Wu, Yisen Wang, and Shutao Xia. Revisiting loss landscape for adversarial robustness.
CoRR, abs/2004.05884, 2020. URL https://arxiv.org/abs/2004.05884."
REFERENCES,0.4231433506044905,"Lei Wu, Hongxia Liu, Jiabin Li, Shulong Wang, and Xing Wang.
A Multi-level Memristor
Based on Al-Doped HfO2 Thin Film.
Nanoscale Research Letters, 14(1):177, May 2019.
ISSN 1556-276X. doi: 10.1186/s11671-019-3015-x. URL https://doi.org/10.1186/
s11671-019-3015-x."
REFERENCES,0.42487046632124353,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017."
REFERENCES,0.4265975820379965,"Shimeng Yu and Pai-Yu Chen. Emerging memory technologies: Recent trends and prospects. IEEE
Solid-State Circuits Magazine, 8(2):43–56, Spring 2016. ISSN 1943-0590. doi: 10.1109/MSSC.
2016.2546199."
REFERENCES,0.4283246977547496,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. CoRR, abs/1901.08573, 2019.
URL http://arxiv.org/abs/1901.08573."
REFERENCES,0.43005181347150256,Published as a conference paper at ICLR 2022
REFERENCES,0.4317789291882556,"Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. CoRR, abs/2010.04925, 2020. URL https://arxiv.org/abs/2010.
04925."
REFERENCES,0.43350604490500866,Published as a conference paper at ICLR 2022
REFERENCES,0.43523316062176165,SUPPLEMENTARY MATERIAL
REFERENCES,0.4369602763385147,SPIKING RNN ARCHITECTURE
REFERENCES,0.4386873920552677,"The dynamics of the spiking model (Bellec et al., 2018) can be summarised by a set of differential
equations (Eq. 3)."
REFERENCES,0.44041450777202074,Bt = b0 + βbt
REFERENCES,0.4421416234887737,ot = 1(V t > Bt) unless refractory trefr
REFERENCES,0.4438687392055268,bt+1 = ρβbt + (1 −ρβ)ot dt
REFERENCES,0.44559585492227977,"It
Reset = ot"
REFERENCES,0.4473229706390328,dt Btdt
REFERENCES,0.44905008635578586,V t+1 = ρV V t + (1 −ρV )(IinWin + ot
REFERENCES,0.45077720207253885,"dt Wrec) −It
Reset (3)"
REFERENCES,0.4525043177892919,"where ρβ = e−dt/τada and ρV = e−dt/τ. The variables B describe the spiking thresholds with
spike-frequency adaptation. The vector ot denotes the population spike train at time t. The membrane
potentials V have a time constant τ and the adaptive threshold time constant is denoted by τada.
The speech signals and ECG traces fed into the network are represented as currents Iin. Since the
derivative of the spiking function with respect to its input is mostly 0 we use a surrogate gradient that
is explicitly deﬁned as"
REFERENCES,0.4542314335060449,"∂E
∂V t = ∂E"
REFERENCES,0.45595854922279794,"∂zt
∂zt"
REFERENCES,0.45768566493955093,∂V t = ∂E
REFERENCES,0.459412780656304,∂zt d · max(1 −|V t −Bt
REFERENCES,0.46113989637305697,"Bt
|, 0)
(4)"
REFERENCES,0.46286701208981,"where E is the error and d is the dampening factor. To get the ﬁnal prediction of the network, we
average the population spike trains along the time-axis zavg and compute"
REFERENCES,0.46459412780656306,"l = softmax(zavgWout + bout)
ˆy = arg max
i
li"
REFERENCES,0.46632124352331605,CNN ARCHITECTURE
REFERENCES,0.4680483592400691,"Our architecture comprises two convolutional blocks (2 × [4 × 4, 64 channels, MaxPool, ReLU]),
followed by three dense layers (N = 1600, 256, 64, ReLU) and a softmax layer. All weights and
kernels are initialized using the Glorot normal initialisation (Glorot & Bengio, 2010). Using this
architecture, we achieved a test accuracy of ∼93%. Attacked parameters for this network included
all the kernel weights, as well as all the dense layer parameters."
REFERENCES,0.4697754749568221,Published as a conference paper at ICLR 2022
REFERENCES,0.47150259067357514,DERIVATION OF JACOBIAN
REFERENCES,0.47322970639032813,Under the assumption that α = ζattack⊙|Θ|
REFERENCES,0.4749568221070812,"Nsteps
and p = ∞, we can rewrite the inner loop of Algorithm 1 to begin"
REFERENCES,0.47668393782383417,"Θ∗←−Θ + |Θ|ϵ ⊙R ; R ∼N(0, 1)
for t = 1 to Nsteps do"
REFERENCES,0.4784110535405872,"Θ∗
t ←Θ∗
t−1 + α · sign

∇Θ∗
t−1Lrob(Θ, Θ∗
t−1, X)
"
REFERENCES,0.48013816925734026,"end
end"
REFERENCES,0.48186528497409326,By rewriting Θ∗in the form of Θ∗= Θ + ∆Θ we get
REFERENCES,0.4835924006908463,Θ∗= Θ + |Θ|ϵ ⊙R + α ⊙
REFERENCES,0.4853195164075993,"Nsteps
X"
REFERENCES,0.48704663212435234,"t=1
sign

∇Θ∗
t−1Lrob(f(Θ, X), f(Θ∗
t−1, X))
"
REFERENCES,0.48877374784110533,From this the Jacobian can be easily calculated. Plugging in the deﬁntion for α we get
REFERENCES,0.4905008635578584,"JΘ∗(Θ) = I + diag
h
sign(Θ)⊙(ζattack+ϵ·R1)"
REFERENCES,0.49222797927461137,"Nsteps
⊙PNsteps
t=1 sign
 
∇Θ∗
t Lrob(f(Θ, X), f(Θ∗
t , X))
i"
REFERENCES,0.4939550949913644,Published as a conference paper at ICLR 2022
REFERENCES,0.49568221070811747,MISMATCH MODEL
REFERENCES,0.49740932642487046,"To model the parameter noise introduced by component mismatch we used a Gaussian distribution
where the mean is the nominal noise-free weight value and the standard deviation depends linearly on
the weight value. This model realistically captures the behaviour of parameter mismatch on a mixed-
signal neuromorphic SNN inference processor (Moradi et al., 2018). Fig. S1 shows the quantiﬁed
parameter mismatch recorded directly from neuromorphic HW, over a range of nominal parameter
values and for several neuronal and synaptic parameters. The measured mismatch parameter variation
follows an approximately Gaussian distribution where the standard deviation depends linearly on the
mean."
REFERENCES,0.4991364421416235,"3
7
14
19
Wslow, peak (mV) a"
REFERENCES,0.5008635578583766,"15
31 39
59
τmem (ms) b"
REFERENCES,0.5025906735751295,"2
7
12
18
Wfast, peak (mV) c"
REFERENCES,0.5043177892918825,"20
40
60
mean value (ms; mV) 2 4 6"
REFERENCES,0.5060449050086355,std. dev. (ms; mV) d
REFERENCES,0.5077720207253886,"τmem
Wslow
Wfast"
REFERENCES,0.5094991364421416,"Figure S1: Quantiﬁcation of mismatch on analog neuromorphic hardware. Parameter values
for several weight parameters and membrane time constants were measured for a range of nominal
parameter values, using an oscilloscope directly connected to a mixed-signal neuromorphic SNN
processor. (a-c) Various parameters of the chip follow a Gaussian distribution with increasing width.
(d) The mismatch standard deviation depends linearly on the nominal value of each parameter."
REFERENCES,0.5112262521588946,Published as a conference paper at ICLR 2022
REFERENCES,0.5129533678756477,WEIGHT LOSS-LANDSCAPE VISUALIZATION
REFERENCES,0.5146804835924007,"We characterized the shape of the weight loss-landscape by plotting the categorical cross entropy
loss for varying levels of noise added to the weights of the trained network. In each trial, we picked
a random vector v ∼N(0, ζ|Θ|) and evaluated the categorical cross entropy of the whole test set
given the weights Θ + α · v, where α ∈[−2, 2] and ζ = 0.2. As we show in Figure S2, the variance
of the individual 1D weight loss-landscapes is small."
REFERENCES,0.5164075993091537,"Table S1 quantiﬁes the ﬂatness of the illustrated weight loss-landscapes. As can be seen, our method
combined with adding noise during the forward pass yields the ﬂattest landscapes."
REFERENCES,0.5181347150259067,"Table S1: Average slope of estimated 1D weight loss-landscapes. Slopes were calculated as the
mean absolute differences between sample points divided by the sampling distance."
REFERENCES,0.5198618307426598,"F-MNIST CNN
ECG LSNN
Speech LSNN"
REFERENCES,0.5215889464594128,"Standard
0.3375
0.1605
0.2702
Beta
0.0480
0.0879
0.0633
Forward Noise
0.0332
0.0180
0.1009
Forward Noise + Beta
0.0190
0.0187
0.0540
Dropout
0.3022
0.0952
0.0707
AWP
0.0841
0.1013
0.1389"
REFERENCES,0.5233160621761658,"−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
α 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.5250431778929189,Cross Entropy Loss
REFERENCES,0.5267702936096719,F-MNIST CNN
REFERENCES,0.5284974093264249,"AWP
Beta
Forward Noise + Beta
Dropout
Forward Noise
Standard"
REFERENCES,0.5302245250431779,"−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
α 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
REFERENCES,0.531951640759931,Cross Entropy Loss
REFERENCES,0.533678756476684,ECG LSNN
REFERENCES,0.5354058721934369,"−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
α 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.5371329879101899,Cross Entropy Loss
REFERENCES,0.538860103626943,Speech LSNN
REFERENCES,0.540587219343696,"Figure S2: Illustration of the test weight loss-landscape highlighting individual trials to mea-
sure the loss landscapes. See the results text for more details."
REFERENCES,0.542314335060449,Published as a conference paper at ICLR 2022
REFERENCES,0.5440414507772021,ATTACKING KL-DIVERGENCE LOSS DURING INFERENCE
REFERENCES,0.5457685664939551,"While the adversary in AWP attacks the task loss directly, i.e. max Lcce(f(Θ∗, X), y), the adversary
in our training algorithm attacks the KL divergence max KL(f(Θ, X), f(Θ∗, X)). This implies that
our parameter attack seeks simply to change the response of the network in any way, and is agnostic
to the task itself. In the main results we attack the cross-entropy task loss during inference, in order
to not give an undue advantage to our training approach. Here we show that our training approach
also provides robustness against parameter attacks on the KL-divergence loss during inference."
REFERENCES,0.5474956822107081,"Fig. S3 shows the adversarial robustness of the methods for an adversary that attacks the KL-
divergence rather than the cross-entropy loss. When the network is attacked by maximizing the
KL-divergence between the normal and attacked network, our adversarially-trained networks are
more robust than AWP, SGD and forward-noise. Because the parameter attack used here during
inference as well as in the inner optimization loop of the training procedure is the same, this result is
expected, and serves as a sanity check that our networks indeed learn to defend against the attack
they were trained against."
REFERENCES,0.5492227979274611,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 0 20 40 60 80"
REFERENCES,0.5509499136442142,Test Acc.
REFERENCES,0.5526770293609672,F-MNIST CNN
REFERENCES,0.5544041450777202,"Forward Noise + Beta
Beta
Standard
Forward Noise
AWP"
REFERENCES,0.5561312607944733,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 40 60 80 100"
REFERENCES,0.5578583765112263,Test Acc.
REFERENCES,0.5595854922279793,ECG LSNN
REFERENCES,0.5613126079447323,"0.0
0.005 0.01
0.05
0.1
0.2
0.3
0.5
Attack size ζ 20 30 40 50 60 70 80"
REFERENCES,0.5630397236614854,Test Acc.
REFERENCES,0.5647668393782384,Speech LSNN
REFERENCES,0.5664939550949913,"Figure S3: Robustness to weight attack targeting KL divergence during inference. When the
network is attacked by maximizing the KL-divergence between the normal and attacked network, our
adversarially trained networks are more robust than standard SGD or AWP."
REFERENCES,0.5682210708117443,Published as a conference paper at ICLR 2022
REFERENCES,0.5699481865284974,EFFECT OF CONSTANT VERSUS RELATIVE PARAMETER NOISE
REFERENCES,0.5716753022452504,"As described in the main text, parameter noise that has constant magnitude (for example, Gaussian
noise with ﬁxed standard deviation) is trivial to protect against by increasing weight magnitudes. We
examined this effect by training MLPs with an adversary that employs Gaussian noise with ﬁxed std.
dev. ϵ = 0.2. Figure S4 (right) illustrates the test accuracy of two MLPs trained on F-MNIST over
the course of training. Using an inverse weight decay term, the weight-magnitude of one network is
forced to increase to 2.0 over the course of training (black crosses, Θ∗). The weight magnitude of the
other network (red crosses) is limited during training to 0.2 (red crosses; Θ). One can observe that
as the weight magnitude of the increasing magnitude network Θ∗increases, also the robustness to
Gaussian noise (ϵ = 0.2) increases (blue), while the performance of the small magnitude network Θ
remains poor (cyan)."
REFERENCES,0.5734024179620034,"β 0.0
β 0.1
β 0.5
β 1.0
0.06 0.11 0.16 0.21 0.26"
REFERENCES,0.5751295336787565,Sum(Abs(Θ)) W in
REFERENCES,0.5768566493955095,"Constant
Relative"
REFERENCES,0.5785837651122625,"β 0.0
β 0.1
β 0.5
β 1.0
0.180 0.235 0.290 0.345"
W REC,0.5803108808290155,"0.400
W rec"
W REC,0.5820379965457686,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Epoch 0.2 0.4 0.6 0.8"
W REC,0.5837651122625216,Test accuracy (%)
W REC,0.5854922279792746,MLP MNIST
W REC,0.5872193436960277,"Test acc. Θ
Test acc. Θ∗"
W REC,0.5889464594127807,"ϵ-test acc.,Θ
ϵ-test acc.,Θ∗ 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00"
W REC,0.5906735751295337,Sum(Abs(Θ)) |Θ| |Θ∗|
W REC,0.5924006908462867,"Figure S4: Constant magnitude versus magnitude-relative parameter noise. (left, middle) When
parameter noise of constant magnitude (blue) is introduced by the adversarial attack during training,
the networks learn to increase the magnitude of the weights to trivially improve robustness to the
constant-magnitude attack. When the parameter attack is relative to each parameter magnitude, as in
the main text (red), the weight magnitudes do not increase. These networks were trained for a range
of βrob, i.e. varying emphasis on robustness.
(right) One can also trivially increase the robustness to ﬁxed-magnitude noise by introducing an
inverse weight decay term that causes the weights to increase in magnitude (black, cross marker)
while retaining performance. This causes the MLP trained on MNIST to become increasingly robust
to ﬁxed-magnitude parameter noise (blue dots; ϵ-test acc., Θ∗). The network where the weight
magnitude was not increased over time (red, cross marker) did not improve in terms of robustness
(cyan; ϵ-test acc., Θ), although both models perform similarly when there is no parameter noise
applied (red and black dots)."
W REC,0.5941278065630398,Published as a conference paper at ICLR 2022
W REC,0.5958549222797928,EFFECT OF ADVERSARIAL REGULARIZATION DURING TRAINING
W REC,0.5975820379965457,"0
10
20
30
40
50
Epochs 0.2 0.4 0.6 0.8"
W REC,0.5993091537132987,Validation acc.
W REC,0.6010362694300518,"a
Training βrob=0.0"
W REC,0.6027633851468048,"Attack ζ = 0.0
Attack ζ = 0.1"
W REC,0.6044905008635578,"0
10
20
30
40
50
Epochs 0.2 0.4 0.6 0.8"
W REC,0.6062176165803109,Validation acc.
W REC,0.6079447322970639,"b
Training βrob=0.1"
W REC,0.6096718480138169,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.0 0.2 0.4 0.6 0.8"
W REC,0.6113989637305699,Test acc.
W REC,0.613126079447323,"c
Training βrob=0.0"
W REC,0.614853195164076,"Adversarial
Random"
W REC,0.616580310880829,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.2 0.4 0.6 0.8"
W REC,0.6183074265975821,Test acc.
W REC,0.6200345423143351,"d
Training βrob=0.1"
W REC,0.6217616580310881,"Figure S5: Our parameter attack is effective at decreasing the performance of a network dur-
ing inference, and using our attack during training protects a network from later disruption.
(a) A network trained using standard SGD (i.e. βrob = 0.0) on the F-MNIST task is disrupted badly by
the parameter noise adversary during inference (ζ = 0.1; black curve). (b) When the same network is
trained with parameter attacks during training (βrob = 0.1), the network is protected from parameter
attacks during inference (high accuracy of attacked network; black curve). (c) When trained with
standard SGD (βrob = 0.0), both random noise (red) and parameter attacks (black) disrupt network
performance for increasing attack size ζ. (d) Under our training approach (βrob = 0.1), networks are
signiﬁcantly protected against random and adversarial weight perturbations during inference."
W REC,0.6234887737478411,Published as a conference paper at ICLR 2022
W REC,0.6252158894645942,EFFECT OF VARYING βrob
W REC,0.6269430051813472,"We additionally quantify the trade-off between test loss and robustness of our algorithm by repeating
the experiment in Figure 1 at different values of βrob. As shown in Figure S6, increasing βrob ﬂattens
the weight loss-landscape, effectively increasing the robustness of the model. To further substantiate
this claim, we repeated the experiment of Figure S5 with the same values of βrob. As Figure S8 shows,
increasing βrob and therefore increasing the ﬂatness of the landscape yields increased robustness to
random, as well as, adversarial perturbations."
W REC,0.6286701208981001,"We note that, relative to the baseline, the test loss does not consistently increase with increasing values
of βrob. We hypothesize that this is due to the increased generalization capability of our networks.
To check whether this is indeed the case, we repeated the experiment from Fig. S6 using the loss
computed on the training set. As Fig. S7 shows, increasing values of βrob lead to ﬂatter minima
and higher loss on the training set, which is the exact trade-off to be expected from the formulation
of our loss function. However, the increased generalization capability that follows from a ﬂatter
loss-landscape seems to disrupt this trade-off. As a result, when choosing βrob one should aim at
choosing the highest value that still yields good performance on the validation set."
W REC,0.6303972366148531,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.25 0.30 0.35 0.40"
W REC,0.6321243523316062,Cross Entropy Loss
W REC,0.6338514680483592,F-MNIST CNN
W REC,0.6355785837651122,"Beta 0.05
Beta 0.1
Beta 0.2
Beta 0.3
Beta 0.5
Beta 0.8"
W REC,0.6373056994818653,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.05 0.10 0.15 0.20"
W REC,0.6390328151986183,Cross Entropy Loss
W REC,0.6407599309153713,ECG LSNN
W REC,0.6424870466321243,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.50 0.55 0.60 0.65 0.70 0.75 0.80"
W REC,0.6442141623488774,Cross Entropy Loss
W REC,0.6459412780656304,Speech LSNN
W REC,0.6476683937823834,"Figure S6:
Effect of βrob on the test loss landscape. Increasing βrob promotes robustness by
ﬂattening the test loss landscape. However, increasing βrob does not lead to a systematic rise in loss
on the test set."
W REC,0.6493955094991365,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.05 0.10 0.15 0.20 0.25 0.30"
W REC,0.6511226252158895,Cross Entropy Loss
W REC,0.6528497409326425,F-MNIST CNN
W REC,0.6545768566493955,"Beta 0.05
Beta 0.1
Beta 0.2
Beta 0.3
Beta 0.5
Beta 0.8"
W REC,0.6563039723661486,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.05 0.10 0.15 0.20"
W REC,0.6580310880829016,Cross Entropy Loss
W REC,0.6597582037996546,ECG LSNN
W REC,0.6614853195164075,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
α 0.2 0.3 0.4 0.5"
W REC,0.6632124352331606,Cross Entropy Loss
W REC,0.6649395509499136,Speech LSNN
W REC,0.6666666666666666,"Figure S7: Effect of βrob on the training loss landscape. Weight loss landscape computed on the
training set (c.f. Fig. S6). Increasing βrob leads to ﬂatter loss-landscapes (increased robustness) as
before, but also a systematic increase in training loss (higher values for cross-entropy loss; note
ordering of curves)."
W REC,0.6683937823834197,Published as a conference paper at ICLR 2022
W REC,0.6701208981001727,"We additionally trained the CNN on the F-MNIST data for a range of βrob, and measured the network
robustness to attacks of varying magnitude ζ (Fig. S8). We compared the effectiveness of random
attack versus adversarial attack during inference, evaluated on the test set. We found that increasing
βrob improved the robustness of the network to both random and adversarial attack during inference."
W REC,0.6718480138169257,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.0 0.2 0.4 0.6 0.8"
W REC,0.6735751295336787,Test acc.
W REC,0.6753022452504318,"a
Training βrob=0.0"
W REC,0.6770293609671848,"Adversarial
Random"
W REC,0.6787564766839378,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.0 0.2 0.4 0.6 0.8"
W REC,0.6804835924006909,Test acc.
W REC,0.6822107081174439,"b
Training βrob=0.1"
W REC,0.6839378238341969,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.2 0.4 0.6 0.8"
W REC,0.6856649395509499,Test acc.
W REC,0.687392055267703,"c
Training βrob=0.3"
W REC,0.689119170984456,"0.0
0.01
0.05
0.1
0.2
0.3
0.5
0.7
Attack size ζ 0.0 0.2 0.4 0.6 0.8"
W REC,0.690846286701209,Test acc.
W REC,0.6925734024179621,"d
Training βrob=0.5"
W REC,0.694300518134715,"Figure S8:
Increasing βrob during training improves robustness during inference. The CNN
was trained using different values of βrob and evaluated on the test set after the weights were perturbed
either randomly (red) or adversarially (black)."
W REC,0.696027633851468,Published as a conference paper at ICLR 2022
W REC,0.697754749568221,VERIFIABLE ROBUSTNESS FOR LSNNS
W REC,0.6994818652849741,"Computations through the network are performed on intervals rather than on discrete values. By
propagating the intervals through a network over a test set, we obtain output logits that are also
expressed as intervals and can therefore determine whether a sample will always be classiﬁed
correctly."
W REC,0.7012089810017271,"The ﬁnal classiﬁcation of the network is made using an arg max operator. For provability of network
performance, we consider that a test sample x is correctly classiﬁed when the lower bound of the
logit interval for the correct class is the maximum lower bound across all logit intervals, and when the
logit interval for the correct class is disjoint from the other logit intervals. When the logit interval for
the correct class overlaps with another logit interval, that test sample is not considered to be provably
correctly classiﬁed. Note that interval domain analysis provides a relatively loose bound (Gehr et al.,
2018), with the implication that the results here probably underestimate the true performance of our
method."
W REC,0.7029360967184801,"0.0
1e-05
5e-05
0.0001
0.0005
0.001
Attack size ζ 0.0 0.2 0.4 0.6 0.8"
W REC,0.7046632124352331,Veriﬁed test acc.
W REC,0.7063903281519862,Speech LSNN
W REC,0.7081174438687392,"0.0
1e-05
5e-05
0.0001
0.0005
0.001
Attack size ζ 0.0 0.2 0.4 0.6 0.8 1.0"
W REC,0.7098445595854922,Veriﬁed test acc.
W REC,0.7115716753022453,ECG LSNN
W REC,0.7132987910189983,"Beta
Forward Noise + Beta
Forward Noise
Standard"
W REC,0.7150259067357513,"Figure S9:
Networks trained with our method are provably more robust than those trained
with standard gradient descent or forward noise alone. We used interval bound propagation to
determine the proportion of test samples that are veriﬁably correctly classiﬁed, under increasing
weight perturbations ζ. For both the Speech and ECG tasks, computed on the trained LSNNs, our
method was provably more robust for ζ < 5 × 10−4."
W REC,0.7167530224525043,Published as a conference paper at ICLR 2022
W REC,0.7184801381692574,WIDE-MARGIN NETWORK ACTIVATIONS
W REC,0.7202072538860104,Murray et al. show that adding random forward noise to the weights of a network during the forward
W REC,0.7219343696027634,"pass implicitly adds a regularizer of the form Θ2
i,j

∂ok,l
∂Θi,j"
W REC,0.7236614853195165,"2
to the network weights (Murray & Edwards,
1994). When sigmoid activation functions are used, this regularizer favors high or low activations.
When implementing interval bound propagation for LSNNs, intervals over spiking activity must
be computed by passing intervals through the spiking threshold function. As a result, intervals
for spiking activity become either [0, 0] for neurons that never emit a spike regardless of weight
attack; [1, 1] for neurons that always emit a spike; and [0, 1] for neurons for which activity becomes
uncertain in the presence of weight attack. By deﬁnition, a robust network should promote bounds
[0, 0] and [1, 1], where the activity of the network is unchanged by weight attack. Robust network
conﬁgurations should therefore avoid states where the membrane potentials of neurons are close to
the ﬁring threshold."
W REC,0.7253886010362695,"To see whether this was also the case for our LSNNs, we investigated the distribution of the membrane
potentials on a batch of test examples for a network that was trained with- and without noise during
the forward pass. We found that robust networks exhibited a broader distribution of membrane
potentials, with comparatively less distribution mass close to the ﬁring threshold (Fig. S10). This
indicates that neurons in the robust network spend more time in a “safe” regime where a small change
in the weights cannot trigger an unwanted spike, or remove a desired spike."
W REC,0.7271157167530224,"−20
−15
−10
−5
0
Membrane potential 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
W REC,0.7288428324697754,Normalized bin count
W REC,0.7305699481865285,"Forward Noise + Beta
Standard"
W REC,0.7322970639032815,"−4
−3
−2
−1
0
1
Membrane potential 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
W REC,0.7340241796200345,Normalized bin count
W REC,0.7357512953367875,"Forward Noise + Beta
Standard"
W REC,0.7374784110535406,"Figure S10: Membrane potentials are distributed away from the ﬁring threshold for networks
trained with forward noise. We measured the distribution of membrane potentials in LSNNs trained
using standard gradient descent (“Standard”), and in the presence of weight noise injected in the
forward pass (“Forward Noise + Beta”). As predicted, networks trained with forward noise have
membrane potentials distributed away from the ﬁring threshold. This implies that weight perturbations
are less likely to inject or delete a spike erroneously, improving the robustness of the network."
W REC,0.7392055267702936,Published as a conference paper at ICLR 2022
W REC,0.7409326424870466,EFFECT OF VARYING ϵPGA IN AWP
W REC,0.7426597582037997,"In addition to attacking the network parameters, AWP (Wu et al., 2020) also attacks the input using
PGA. Since the relation between robustness to weight- and input-space perturbations is still unclear,
we performed additional sweeps over the attack size in the input space. Figure S11 demonstrates that
attacking the input during training generally does not improve the robustness, with the exception for
the network trained on the speech dataset. We also note that attacking the inputs improved robustness
for larger mismatch values, but generally degraded performance for the small values."
W REC,0.7443868739205527,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Mismatch level 0.4 0.5 0.6 0.7 0.8"
W REC,0.7461139896373057,Test acc.
W REC,0.7478411053540587,Speech LSNN
W REC,0.7495682210708118,"ϵpga =0.000
ϵpga =0.010
ϵpga =0.100
ϵpga =1.000"
W REC,0.7512953367875648,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Mismatch level 0.5 0.6 0.7 0.8 0.9 1.0"
W REC,0.7530224525043178,Test acc.
W REC,0.7547495682210709,ECG LSNN
W REC,0.7564766839378239,"ϵpga =0.000
ϵpga =0.010
ϵpga =0.100
ϵpga =1.000"
W REC,0.7582037996545768,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Mismatch level 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
W REC,0.7599309153713298,Test acc.
W REC,0.7616580310880829,FMNIST CNN
W REC,0.7633851468048359,"ϵpga =0.000
ϵpga =0.001
ϵpga =0.010
ϵpga =0.100"
W REC,0.7651122625215889,"Figure S11:
Attacking the input using conventional PGA generally has a limited effect on
robustness to weight-space perturbations. We swept various values of ϵpga, the parameter that
determines the maximum perturbation in l∞for the AWP algorithm."
W REC,0.7668393782383419,Published as a conference paper at ICLR 2022
W REC,0.768566493955095,TRAINING OF CNN USED FOR PCM-BASED CIM SIMULATION
W REC,0.770293609671848,"The CNN that was used for this series of experiments is Resnet32 (He et al., 2015) trained on the
Cifar10 (Krizhevsky, 2009) dataset. The CNN was generally trained for 300 epochs using a batch
size of 256. We used SGD with an initial learning rate of 0.001 that was decreased by a multiplicative
factor of 0.2 after epochs 60, 120 and 160. Additionally, Nesterov momentum (Nesterov, 1983) was
used with a value of 0.9 and weight decay with a value of 5e-4."
W REC,0.772020725388601,"50
100
150
200
250
300
Epochs 85 88 91 94"
W REC,0.7737478411053541,"Val. Acc. ( =
train)"
W REC,0.7754749568221071,Baseline
W REC,0.7772020725388601,train = 2.6%
W REC,0.7789291882556131,train = 3.7%
W REC,0.7806563039723662,train = 5.6%
W REC,0.7823834196891192,train = 7.5%
W REC,0.7841105354058722,train = 11.0%
W REC,0.7858376511226253,"0.026
0.037
0.056
0.075
0.110"
W REC,0.7875647668393783,train = 90 91 92 93
W REC,0.7892918825561313,Test. Acc.
W REC,0.7910189982728842,"wl
i, j =
train wl
max"
W REC,0.7927461139896373,"50
100
150
200
250
300
Epochs 85 88 91 94"
W REC,0.7944732297063903,"Val. Acc. ( =
train)"
W REC,0.7962003454231433,Baseline
W REC,0.7979274611398963,train = 2.6%
W REC,0.7996545768566494,train = 3.7%
W REC,0.8013816925734024,train = 5.6%
W REC,0.8031088082901554,train = 7.5%
W REC,0.8048359240069085,train = 11.0%
W REC,0.8065630397236615,"0.026
0.037
0.056
0.075
0.110"
W REC,0.8082901554404145,train = 92.0 92.5 93.0
W REC,0.8100172711571675,Test. Acc.
W REC,0.8117443868739206,"wl
i, j =
train |wl
i, j|"
W REC,0.8134715025906736,"Figure S12: The left panel illustrates the validation accuracy over the course of ﬁnetuning a pretrained
model for additional 240 epochs. One should note that convergence is usually achieved much quicker
(roughly after 150 additional epochs). The right panel illustrates the performance on the test set of
each model trained with noise injection of magnitude ηtrain (x-axis). Each row depicts a different
noise model. It should be noted that the weights in each ﬁlter were clipped to two standard deviations
during training to avoid outliers causing excessive amounts of noise following the model that relies
on the maximum weight value."
W REC,0.8151986183074266,VARYING THE NUMBER OF ATTACK STEPS FOR THE PCM-BASED CIM SIMULATION
W REC,0.8169257340241797,"0.026
0.037
0.056
0.075
0.110"
W REC,0.8186528497409327,train = 90.0 90.5 91.0 91.5 92.0 92.5 93.0 93.5
W REC,0.8203799654576857,Test acc. (%)
W REC,0.8221070811744386,attack = 0.10
W REC,0.8238341968911918,Baseline
W REC,0.8255613126079447,rob = 0.0 1 2 3 4 5 6 7
W REC,0.8272884283246977,Nsteps
W REC,0.8290155440414507,"Figure S13:
One can greatly reduce the number of attack steps used during training. Our
method still produces strong results for very few number of attack steps (blue) when compared to the
baseline model (red, trained with Gaussian noise (ηtrain))."
W REC,0.8307426597582038,Published as a conference paper at ICLR 2022
W REC,0.8324697754749568,PERFORMANCE OF VARYING HYPERPARAMETERS FOR THE PCM-BASED CIM SIMULATION
W REC,0.8341968911917098,"In this experiment we show that the choice of hyperparameters is generally not very important to
outperform the baseline that was trained with noise injection. However, in order to surpass the FP
baseline, i.e. the model that was trained without noise injection and evaluated on a standard PC, one
has to tune the hyperparameters in order to obtain a combination that yields the highest performance.
Figure S14 illustrates this sweep. Each row represents a different value of ηtrain that was used for
training the baseline model (red). Each column represents a different attack size and the different
hues of blue correspond to varying values of βrob."
W REC,0.8359240069084629,"102
103
104
105
106
107"
W REC,0.8376511226252159,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8393782383419689,Test acc. (%)
W REC,0.8411053540587219,attack = 0.01
W REC,0.842832469775475,FP Baseline
W REC,0.844559585492228,"rob = 0.0,
train = 0.026"
W REC,0.846286701208981,"102
103
104
105
106
107"
W REC,0.8480138169257341,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8497409326424871,Test acc. (%)
W REC,0.8514680483592401,attack = 0.03
W REC,0.853195164075993,"102
103
104
105
106
107"
W REC,0.8549222797927462,Tinf (s) 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8566493955094991,Test acc. (%)
W REC,0.8583765112262521,attack = 0.05
W REC,0.8601036269430051,"102
103
104
105
106
107"
W REC,0.8618307426597582,Tinf (s) 90.5 91.0 91.5 92.0 92.5 93.0 93.5
W REC,0.8635578583765112,Test acc. (%)
W REC,0.8652849740932642,attack = 0.10
W REC,0.8670120898100173,"102
103
104
105
106
107"
W REC,0.8687392055267703,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8704663212435233,Test acc. (%)
W REC,0.8721934369602763,attack = 0.01
W REC,0.8739205526770294,FP Baseline
W REC,0.8756476683937824,"rob = 0.0,
train = 0.037"
W REC,0.8773747841105354,"102
103
104
105
106
107"
W REC,0.8791018998272885,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8808290155440415,Test acc. (%)
W REC,0.8825561312607945,attack = 0.03
W REC,0.8842832469775475,"102
103
104
105
106
107"
W REC,0.8860103626943006,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8877374784110535,Test acc. (%)
W REC,0.8894645941278065,attack = 0.05
W REC,0.8911917098445595,"102
103
104
105
106
107"
W REC,0.8929188255613126,Tinf (s) 90.0 90.5 91.0 91.5 92.0 92.5 93.0
W REC,0.8946459412780656,Test acc. (%)
W REC,0.8963730569948186,attack = 0.10
W REC,0.8981001727115717,"102
103
104
105
106
107"
W REC,0.8998272884283247,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.9015544041450777,Test acc. (%)
W REC,0.9032815198618307,attack = 0.01
W REC,0.9050086355785838,FP Baseline
W REC,0.9067357512953368,"rob = 0.0,
train = 0.056"
W REC,0.9084628670120898,"102
103
104
105
106
107"
W REC,0.9101899827288429,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.9119170984455959,Test acc. (%)
W REC,0.9136442141623489,attack = 0.03
W REC,0.9153713298791019,"102
103
104
105
106
107"
W REC,0.917098445595855,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.918825561312608,Test acc. (%)
W REC,0.9205526770293609,attack = 0.05
W REC,0.9222797927461139,"102
103
104
105
106
107"
W REC,0.924006908462867,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.92573402417962,Test acc. (%)
W REC,0.927461139896373,attack = 0.10
W REC,0.9291882556131261,"102
103
104
105
106
107"
W REC,0.9309153713298791,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.9326424870466321,Test acc. (%)
W REC,0.9343696027633851,attack = 0.01
W REC,0.9360967184801382,FP Baseline
W REC,0.9378238341968912,"rob = 0.0,
train = 0.075"
W REC,0.9395509499136442,"102
103
104
105
106
107"
W REC,0.9412780656303973,Tinf (s) 91.0 91.5 92.0 92.5 93.0
W REC,0.9430051813471503,Test acc. (%)
W REC,0.9447322970639033,attack = 0.03
W REC,0.9464594127806563,"102
103
104
105
106
107"
W REC,0.9481865284974094,Tinf (s) 91.25 91.50 91.75 92.00 92.25 92.50 92.75 93.00 93.25
W REC,0.9499136442141624,Test acc. (%)
W REC,0.9516407599309153,attack = 0.05
W REC,0.9533678756476683,"102
103
104
105
106
107"
W REC,0.9550949913644214,Tinf (s) 91.25 91.50 91.75 92.00 92.25 92.50 92.75 93.00 93.25
W REC,0.9568221070811744,Test acc. (%)
W REC,0.9585492227979274,attack = 0.10
W REC,0.9602763385146805,"102
103
104
105
106
107"
W REC,0.9620034542314335,Tinf (s) 91.5 92.0 92.5 93.0
W REC,0.9637305699481865,Test acc. (%)
W REC,0.9654576856649395,attack = 0.01
W REC,0.9671848013816926,FP Baseline
W REC,0.9689119170984456,"rob = 0.0,
train = 0.110"
W REC,0.9706390328151986,"102
103
104
105
106
107"
W REC,0.9723661485319517,Tinf (s) 91.5 92.0 92.5 93.0 93.5
W REC,0.9740932642487047,Test acc. (%)
W REC,0.9758203799654577,attack = 0.03
W REC,0.9775474956822107,"102
103
104
105
106
107"
W REC,0.9792746113989638,Tinf (s) 91.5 92.0 92.5 93.0
W REC,0.9810017271157168,Test acc. (%)
W REC,0.9827288428324698,attack = 0.05
W REC,0.9844559585492227,"102
103
104
105
106
107"
W REC,0.9861830742659758,Tinf (s) 91.25 91.50 91.75 92.00 92.25 92.50 92.75 93.00 93.25
W REC,0.9879101899827288,Test acc. (%)
W REC,0.9896373056994818,attack = 0.10 0.01 0.025 0.05 0.1 rob 0.01 0.025 0.05 0.1 rob 0.01 0.025 0.05 0.1 rob 0.01 0.025 0.05 0.1 rob 0.01 0.025 0.05 0.1 rob
W REC,0.9913644214162349,"Figure S14: The choice of hyperparameters is not critical in order to beat the baseline model.
Our method is resilient to variations in hyperparameters. However, to obtain conﬁgurations where
even the FP baseline is surpassed, one has to ﬁne-tune the method."
W REC,0.9930915371329879,Published as a conference paper at ICLR 2022
W REC,0.9948186528497409,PCM NOISE MODEL
W REC,0.9965457685664939,"Analog CiM comes in various ﬂavors, depending on the memory technology used. In this paper,
we assume the use of PCM devices, which have been heavily studied in the context of analog CiM
accelerators (Joshi et al., 2020; Nandakumar et al., 2019; Boybat et al., 2018). PCM-based, or, more
generally, Non-Volatile Memory (NVM)-based architectures, essentially perform Matrix-Vector-
Multiplications (MVMs) using Kirchhoff’s current law. The weights of the matrix are organized
as differential pairs in order to account for positive and negative weights. When storing a neural
network, each weight matrix is programmed into the NVM devices by applying short electrical
pulses (Nandakumar et al., 2020b). Because of various noise sources, this process is often imprecise
and exhibits noise on the weights, termed ""programming noise"". Additionally, PCM devices suffer
from 1/f and telegraph noise, adding even more noise during inference (""read noise""). At last,
PCM devices also drift due to the underlying physical properties (Le Gallo et al., 2018a). Although
the effect of drift can mostly be alleviated by scaling the output of the MVM (a method called
Global Drift Compensation (GDC)), the non-uniform drift of the devices still leads to performance
degradation over time. In the simulator that we used, we model these three main sources of noise,
analog-to-digital and digital-to-analog converters, GDC and splitting of the MVM to account for
smaller tile sizes (typically each crossbar is 256 × 256).
Initially, the clipped weights are mapped to target conductances in a differential manner, i.e. the weight
matrix is split into two conductance matrices that both represent the positive and negative weights as
conductances. The target conductances typically range from zero to Gmax, where Gmax is assumed
to be 25µS. After mapping the weights to the target conductances GT , the programming noise is
simulated (these statistical models assume the conductances to be normalized): GP = GT +N(0, σP )
where σP = max(−1.1731G2
T + 1.9650GT + 0.2635, 0.0).
After the conductances have been programmed they drift over time, with the conductance of a device
typically following GD = GP (t/tc)−ν, where ν is the drift coefﬁcient, t is the time at inference,
and tc is the time the conductances were programmed. Additionally, the drift coefﬁcient is modelled
to follow a Gaussian distribution. This makes it typically hard to correct for drift and it is the main
reason why drift is a problem in PCM based CiM devices.
Finally, the read noise is modelled using a Gaussian: GR ∼N(GD, σnG(t)), where σnG(t) =
GD(t)Q
p"
W REC,0.998272884283247,"log((t + tr)/tr) with Q = min(0.0088/G0.65
T
, 0.2) and tr = 250ns and t is the time at
inference. For the experiments in this paper, we simulated the performance of the networks deployed
on CiM hardware for up to one year."
