Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010638297872340426,"Classical analysis of convex and non-convex optimization methods often requires
the Lipschitz continuity of the gradient, which limits the analysis to functions
bounded by quadratics. Recent work relaxed this requirement to a non-uniform
smoothness condition with the Hessian norm bounded by an affine function of the
gradient norm, and proved convergence in the non-convex setting via gradient
clipping, assuming bounded noise.
In this paper, we further generalize this
non-uniform smoothness condition and develop a simple, yet powerful analysis
technique that bounds the gradients along the trajectory, thereby leading to stronger
results for both convex and non-convex optimization problems. In particular,
we obtain the classical convergence rates for (stochastic) gradient descent and
Nesterov’s accelerated gradient method in the convex and/or non-convex setting
under this general smoothness condition. The new analysis approach does not
require gradient clipping and allows heavy-tailed noise with bounded variance in
the stochastic setting."
INTRODUCTION,0.002127659574468085,"1
Introduction"
INTRODUCTION,0.0031914893617021275,"In this paper, we study the following unconstrained optimization problem"
INTRODUCTION,0.00425531914893617,"minx∈X f(x),
(1)"
INTRODUCTION,0.005319148936170213,"where X ⊆Rd is the domain of f. Classical textbook analyses [Nemirovskij and Yudin, 1983,
Nesterov, 2003] of (1) often require the Lipschitz smoothness condition, which assumes
∇2f(x)
 ≤
L almost everywhere for some L ≥0 called the smoothness constant. This condition, however, is
rather restrictive and only satisfied by functions that are both upper and lower bounded by quadratic
functions."
INTRODUCTION,0.006382978723404255,"Recently, Zhang et al. [2019] proposed the more general (L0, L1)-smoothness condition, which
assumes
∇2f(x)
 ≤L0 + L1 ∥∇f(x)∥for some constants L0, L1 ≥0, motivated by their
extensive language model experiments. This notion generalizes the standard Lipschitz smoothness
condition and also contains e.g. univariate polynomial and exponential functions. For non-convex and
(L0, L1)-smooth functions, they prove convergence of gradient descent (GD) and stochastic gradient
descent (SGD) with gradient clipping and also provide a complexity lower bound for constant-stepsize
GD/SGD without clipping. Based on these results, they claim gradient clipping or other forms of
adaptivity provably accelerate the convergence for (L0, L1)-smooth functions. Perhaps due to the"
INTRODUCTION,0.007446808510638298,∗Equal contribution.
INTRODUCTION,0.00851063829787234,"lower bound, all the follow-up works under this condition that we are aware of limit their analyses to
adaptive methods. Most of these focus on non-convex functions. See Section 2 for more discussions
of related works."
INTRODUCTION,0.009574468085106383,"In this paper, we significantly generalize the (L0, L1)-smoothness condition to the ℓ-smoothness
condition which assumes
∇2f(x)
 ≤ℓ(∥∇f(x)∥) for some non-decreasing continuous function
ℓ. We develop a simple, yet powerful approach, which allows us to obtain stronger results for both
convex and non-convex optimization problems when ℓis sub-quadratic (i.e., limu→∞ℓ(u)/u2 = 0)
or even more general. The ℓ-smooth function class with a sub-quadratic ℓalso contains e.g. univariate
rational and double exponential functions. In particular, we prove the convergence of constant-stepsize
GD/SGD and Nesterov’s accelerated gradient method (NAG) in the convex or non-convex settings.
For each method and setting, we obtain the classical convergence rate, under a certain requirement of
ℓ. In addition, we relax the assumption of bounded noise to the weaker one of bounded variance with
the simple SGD method. See Table 1 for a summary of our results and assumptions for each method
and setting. At first glance, our results “contradict” the lower bounds on constant-stepsize GD/SGD
in [Zhang et al., 2019, Wang et al., 2022]; this will be reconciled in Section 5.3."
INTRODUCTION,0.010638297872340425,"Our approach analyzes boundedness of gradients along the optimization trajectory. The idea behind
it can be informally illustrated by the following “circular” reasoning. On the one hand, if gradients
along the trajectory are bounded by a constant G, then the Hessian norms are bounded by the constant
ℓ(G). Informally speaking, we essentially have the standard Lipschitz smoothness condition2 and
can apply classical textbook analyses to prove convergence, which implies that gradients converge to
zero. On the other hand, if gradients converge, they must be bounded, since any convergent sequence
is bounded. In other words, the bounded gradient condition implies convergence, and convergence
also implies the condition back, which forms a circular argument. If we can break this circularity of
reasoning in a rigorous way, both the bounded gradient condition and convergence are proved. In
this paper, we will show how to break the circularity using induction or contradiction arguments for
different methods and settings in Sections 4 and 5. We note that the idea of bounding gradients can
be applied to the analysis of other optimization methods, e.g., the concurrent work [Li et al., 2023] by
subset of the authors, which uses a similar idea to obtain a rigorous and improved analysis of the
Adam method [Kingma and Ba, 2014]."
INTRODUCTION,0.011702127659574468,"Contributions. In light of the above discussions, we summarize our main contributions as follows."
INTRODUCTION,0.01276595744680851,"• We generalize the standard Lipschitz smoothness and also the (L0, L1)-smoothness condition to
the ℓ-smoothness condition, and develop a new approach for analyzing convergence under this
condition by bounding the gradients along the optimization trajectory.
• We prove the convergence of constant-stepsize GD/SGD/NAG in the convex and non-convex
settings, and obtain the classical rates for all of them, as summarized in Table 1."
INTRODUCTION,0.013829787234042552,"Besides the generalized smoothness condition and the new approach, our results are also novel in the
following aspects."
INTRODUCTION,0.014893617021276596,"• The convergence results of constant-stepsize methods challenge the folklore belief on the necessity
of adaptive stepsize for generalized smooth functions.
• We obtain new convergence results for GD and NAG in the convex setting under the generalized
smoothness condition.
• We relax the assumption of bounded noise to the weaker one of bounded variance of noise in the
stochastic setting with the simple SGD method."
RELATED WORK,0.015957446808510637,"2
Related work"
RELATED WORK,0.01702127659574468,"Gradient-based optimizaiton. The classical gradient-based optimization problems for the standard
Lipschitz smooth functions have been well studied for both convex [Nemirovskij and Yudin, 1983,"
RELATED WORK,0.018085106382978722,"2This statement is informal because we can only bound Hessian norms along the trajectory, rather than
almost everywhere within a convex set as in the standard Lipschitz smoothness condition. For example, even if
the Hessian norm is bounded at both xt and xt+1, it does not directly mean the Hessian norm is also bounded
over the line segment between them, which is required in classical analysis. A more formal statement will need
Lemma 3.3 presented later in the paper."
RELATED WORK,0.019148936170212766,"Table 1: Summary of the results. ϵ denotes the sub-optimality gap of the function value in convex
settings, and the gradient norm in non-convex settings. “∗” denotes optimal rates."
RELATED WORK,0.02021276595744681,"Method
Convexity
ℓ-smoothness
Gradient complexity GD"
RELATED WORK,0.02127659574468085,"Strongly convex
No requirement
O(log(1/ϵ)) (Theorem 4.3)
Convex
O(1/ϵ) (Theorem 4.2 )"
RELATED WORK,0.022340425531914895,"Non-convex
Sub-quadratic ℓ
O(1/ϵ2)* (Theorem 5.2)"
RELATED WORK,0.023404255319148935,"Quadratic ℓ
Ω(exp. in cond #) (Theorem 5.4 )"
RELATED WORK,0.02446808510638298,"NAG
Convex
Sub-quadratic ℓ
O(1/√ϵ)* (Theorem 4.4 )"
RELATED WORK,0.02553191489361702,"SGD
Non-convex
Sub-quadratic ℓ
O(1/ϵ4)* (Theorem 5.3)"
RELATED WORK,0.026595744680851064,"Nesterov, 2003, d’Aspremont et al., 2021] and non-convex functions. In the convex setting, the goal is
to reach an ϵ-sub-optimal point x satisfying f(x) −infx f(x) ≤ϵ. It is well known that GD achieves
the O(1/ϵ) gradient complexity and NAG achieves the accelerated O(1/√ϵ) complexity which is
optimal among all gradient-based methods. For strongly convex functions, GD and NAG achieve
the O(κ log(1/ϵ)) and O(√κ log(1/ϵ)) complexity respectively, where κ is the condition number
and the latter is again optimal. In the non-convex setting, the goal is to find an ϵ-stationary point x
satisfying ∥∇f(x)∥≤ϵ, since finding a global minimum is NP-hard in general. It is well known
that GD achieves the optimal O(1/ϵ2) complexity which matches the lower bound in [Carmon et al.,
2017]. In the stochastic setting for unbiased stochastic gradient with bounded variance, SGD achieves
the optimal O(1/ϵ4) complexity [Ghadimi and Lan, 2013], matching the lower bound in [Arjevani
et al., 2019]. In this paper, we obtain the classical rates in terms of ϵ for all the above-mentioned
methods and settings, under a far more general smoothness condition."
RELATED WORK,0.027659574468085105,"Generalized smoothness. The (L0, L1)-smoothness condition proposed by Zhang et al. [2019]
was studied by many follow-up works. Under the same condition, [Zhang et al., 2020] considers
momentum in the updates and improves the constant dependency of the convergence rate for SGD with
clipping derived in [Zhang et al., 2019]. [Qian et al., 2021] studies gradient clipping in incremental
gradient methods, [Zhao et al., 2021] studies stochastic normalized gradient descent, and [Crawshaw
et al., 2022] studies a generalized SignSGD method, under the (L0, L1)-smoothess condition.
[Reisizadeh et al., 2023] studies variance reduction for (L0, L1)-smooth functions. [Chen et al.,
2023] proposes a new notion of α-symmetric generalized smoothness, which is roughly as general
as (L0, L1)-smoothness. [Wang et al., 2022] analyzes convergence of Adam and provides a lower
bound which shows non-adaptive SGD may diverge. In the stochastic setting, the above-mentioned
works either consider the strong assumption of bounded gradient noise or require a very large batch
size that depends on ϵ, which essentially reduces the analysis to the deterministic setting. [Faw et al.,
2023] proposes an AdaGrad-type algorithm in order to relax the bounded noise assumption. Perhaps
due to the lower bounds in [Zhang et al., 2019, Wang et al., 2022], all the above works study methods
with an adaptive stepsize. In this and our concurrent work [Li et al., 2023], we further generalize
the smoothness condition and analyze various methods under this condition through bounding the
gradients along the trajectory."
FUNCTION CLASS,0.02872340425531915,"3
Function class"
FUNCTION CLASS,0.029787234042553193,"In this section, we discuss the function class of interest where the objective function f lies. We start
with the following two standard assumptions in the literature of unconstrained optimization, which
will be assumed throughout Sections 4 and 5 unless explicitly stated."
FUNCTION CLASS,0.030851063829787233,Assumption 1. The objective function f is differentiable and closed within its open domain X.
FUNCTION CLASS,0.031914893617021274,"Assumption 2. The objective function f is bounded from below, i.e., f ∗:= infx∈X f(x) > −∞."
FUNCTION CLASS,0.03297872340425532,"A function f is said to be closed if its sub-level set {x ∈dom(f) | f(x) ≤a} is closed for
each a ∈R. A continuous function f with an open domain is closed if and only f(x) tends to
positive infinity when x approaches the boundary of its domain [Boyd and Vandenberghe, 2004].
Assumption 1 is necessary for our analysis to ensure that the iterates of a method with a reasonably
small stepsize stays within the domain X. Note that for X = Rd considered in most unconstrained"
FUNCTION CLASS,0.03404255319148936,"optimization papers, the assumption is trivially satisfied as all continuous functions over Rd are
closed. We consider a more general domain which may not be the whole space because that is the
case for some interesting examples in our function class of interest (see Section 3.1.3). However, it
actually brings us some additional technical difficulties especially in the stochastic setting, as we
need to make sure the iterates do not go outside of the domain."
GENERALIZED SMOOTHNESS,0.035106382978723406,"3.1
Generalized smoothness"
GENERALIZED SMOOTHNESS,0.036170212765957444,"In this section, we formally define the generalized smoothness condition, and present its properties
and examples."
DEFINITIONS,0.03723404255319149,"3.1.1
Definitions"
DEFINITIONS,0.03829787234042553,"Definitions 1 and 2 below are two equivalent ways of stating the definition, where we use B(x, R) to
denote the Euclidean ball with radius R centered at x."
DEFINITIONS,0.039361702127659576,"Definition 1 (ℓ-smoothness). A real-valued differentiable function f : X →R is ℓ-smooth for some
non-decreasing continuous function ℓ: [0, +∞) →(0, +∞) if
∇2f(x)
 ≤ℓ(∥∇f(x)∥) almost
everywhere (with respect to the Lebesgue measure) in X."
DEFINITIONS,0.04042553191489362,"Remark 3.1. Definition 1 reduces to the classical L-smoothness when ℓ≡L is a constant function. It
reduces to the (L0, L1)-smoothness proposed in [Zhang et al., 2019] when ℓ(u) = L0 + L1u is an
affine function.
Definition 2 ((r, ℓ)-smoothness). A real-valued differentiable function f : X →R is (r, ℓ)-smooth
for continuous functions r, ℓ: [0, +∞) →(0, +∞) where ℓis non-decreasing and r is non-increasing,
if it satisfies 1) for any x ∈X, B(x, r(∥∇f(x)∥)) ⊆X, and 2) for any x1, x2 ∈B(x, r(∥∇f(x)∥)),
∥∇f(x1) −∇f(x2)∥≤ℓ(∥∇f(x)∥) · ∥x1 −x2∥."
DEFINITIONS,0.04148936170212766,"The requirements that ℓis non-decreasing and r is non-increasing do not cause much loss in generality.
If these conditions are not satisfied, one can replace ℓand r with the non-increasing function
˜r(u) := inf0≤v≤u r(v) ≤r(u) and non-decreasing function ˜ℓ(u) := sup0≤v≤u ℓ(v) ≥ℓ(u) in
Definitions 1 and 2. Then the only requirement is ˜r > 0 and ˜ℓ< ∞."
DEFINITIONS,0.0425531914893617,"Next, we prove that the above two definitions are equivalent in the following proposition, whose
proof is involved and deferred to Appendix A.2."
DEFINITIONS,0.043617021276595745,"Proposition 3.2. An (r, ℓ)-smooth function is ℓ-smooth; and an ℓ-smooth function satisfying Assump-
tion 1 is (r, m)-smooth where m(u) := ℓ(u + a) and r(u) := a/m(u) for any a > 0."
DEFINITIONS,0.04468085106382979,"The condition in Definition 1 is simple and one can easily check whether it is satisfied for a given
example function. On the other hand, Definition 2 is a local Lipschitz condition on the gradient that
is harder to verify. However, it is useful for deriving several useful properties in the next section."
PROPERTIES,0.045744680851063826,"3.1.2
Properties"
PROPERTIES,0.04680851063829787,"First, we provide the following lemma which is very useful in our analyses of all the methods
considered in this paper. Its proof is deferred to Appendix A.3."
PROPERTIES,0.047872340425531915,"Lemma 3.3. If f is (r, ℓ)-smooth, for any x ∈X satisfying ∥∇f(x)∥≤G, we have 1) B(x, r(G)) ⊆
X, and 2) for any x1, x2 ∈B(x, r(G)),"
PROPERTIES,0.04893617021276596,"∥∇f(x1)−∇f(x2)∥≤L ∥x1−x2∥,
f(x1)≤f(x2)+

∇f(x2), x1−x2

+ L"
PROPERTIES,0.05,"2 ∥x1−x2∥2 ,
(2)"
PROPERTIES,0.05106382978723404,"where L := ℓ(G) is the effective smoothness constant.
Remark 3.4. Since we have shown the equivalence between ℓ-smoothness and (r, ℓ)-smoothness,
Lemma 3.3 also applies to ℓ-smooth functions, for which we have L = ℓ(2G) and r(G) = G/L if
choosing a = G in Proposition 3.2."
PROPERTIES,0.052127659574468084,"Lemma 3.3 states that, if the gradient at x is bounded by some constant G, then within its
neighborhood with a constant radius, we can obtain (2), the same inequalities that were derived in the
textbook analysis [Nesterov, 2003] under the standard Lipschitz smoothness condition. With (2), the
analysis for generalized smoothness is not much harder than that for standard smoothness. Since we"
PROPERTIES,0.05319148936170213,"Table 2: Examples of univariate (ρ, L0, Lρ) smooth functions for different ρs. The parameters a, b, p
are real numbers (not necessarily integers) satisfying a, b > 1 and p < 1 or p ≥2. We use 1+ to
denote any real number slightly larger than 1."
PROPERTIES,0.05425531914893617,"ρ
0
1
1
1+
1.5
2
p−2
p−1
Example Functions
Quadratic
Polynomial
ax
a(bx)
Rational
Logarithmic
xp"
PROPERTIES,0.05531914893617021,"mostly choose x = x2 = xt and x1 = xt+1 in the analysis, in order to apply Lemma 3.3, we need
two conditions: ∥∇f(xt)∥≤G and ∥xt+1 −xt∥≤r(G) for some constant G. The latter is usually
directly implied by the former for most deterministic methods with a small enough stepsize, and the
former can be obtained with our new approach that bounds the gradients along the trajectory."
PROPERTIES,0.05638297872340425,"With Lemma 3.3, we can derive the following useful lemma which is the reverse direction of a
generalized Polyak-Lojasiewicz (PL) inequality, whose proof is deferred to Appendix A.3."
PROPERTIES,0.0574468085106383,"Lemma 3.5. If f is ℓ-smooth, then ∥∇f(x)∥2 ≤2ℓ(2 ∥∇f(x)∥) · (f(x) −f ∗) for any x ∈X."
PROPERTIES,0.05851063829787234,"Lemma 3.5 provides an inequality involving the gradient norm and the sub-optimality gap.
For example, when ℓ(u) = uρ for some 0 ≤ρ < 2, this lemma suggests ∥∇f(x)∥≤
O
 
(f(x) −f ∗)1/(2−ρ)
, which means the gradient norm is bounded whenever the function value is
bounded. The following corollary provides a more formal statement for general sub-quadratic ℓ(i.e.,
limu→∞ℓ(u)/u2 = 0), and we defer its proof to Appendix A.3."
PROPERTIES,0.059574468085106386,"Corollary 3.6. Suppose f is ℓ-smooth where ℓis sub-quadratic. If f(x) −f ∗≤F for some x ∈X
and F ≥0, denoting G := sup{u ≥0 | u2 ≤2ℓ(2u) · F}, then they satisfy G2 = 2ℓ(2G) · F and
we have ∥∇f(x)∥≤G < ∞."
PROPERTIES,0.06063829787234042,"Therefore, in order to bound the gradients along the trajectory as we discussed below Lemma 3.3, it
suffices to bound the function values, which is usually easier."
EXAMPLES,0.06170212765957447,"3.1.3
Examples"
EXAMPLES,0.0627659574468085,"The most important subset of ℓ-smooth (or (r, ℓ)-smooth) functions are those with a polynomial ℓ,
and can be characterized by the (ρ, L0, Lρ)-smooth function class defined below."
EXAMPLES,0.06382978723404255,"Definition 3 ((ρ, L0, Lρ)-smoothness). A real-valued differentiable function f is (ρ, L0, Lρ)-smooth
for constants ρ, L0, Lρ ≥0 if it is ℓ-smooth with ℓ(u) = L0 + Lρuρ."
EXAMPLES,0.06489361702127659,"Definition 3 reduces to the standard Lipschitz smoothness condition when ρ = 0 or Lρ = 0
and to the (L0, L1)-smoothness proposed in [Zhang et al., 2019] when ρ = 1. We list several
univariate examples of (ρ, L0, Lρ)-smooth functions for different ρs in Table 2 with their rigorous
justifications in Appendix A.1. Note that when x goes to infinity, polynomial and exponential
functions corresponding to ρ = 1 grow much faster than quadratic functions corresponding to ρ = 0 .
Rational and logarithmic functions for ρ > 1 grow even faster as they can blow up to infinity near
finite points. Note that the domains of such functions are not Rd, which is why we consider the more
general Assumption 1 instead of simply assuming X = Rd."
EXAMPLES,0.06595744680851064,"Aside from logarithmic functions, the (2, L0, L2)-smooth function class also includes other univariate
self-concordant functions. This is an important function class in the analysis of Interior Point Methods
and coordinate-free analysis of the Newton method [Nesterov, 2003]. More specifically, a convex
function h : R →R is self-concordant if |h′′′(x)| ≤2h′′(x)3/2 for all x ∈R. Formally, we have the
following proposition whose proof is deferred to Appendix A.1."
EXAMPLES,0.06702127659574468,"Proposition 3.7. If h : R →R is a self-concordant function satisfying h′′(x) > 0 over the interval
(a, b), then h restricted on (a, b) is (2, L0, 2)-smooth for some L0 > 0."
CONVEX SETTING,0.06808510638297872,"4
Convex setting"
CONVEX SETTING,0.06914893617021277,"In this section, we present the convergence results of gradient descent (GD) and Nesterov’s accelerated
gradient method (NAG) in the convex setting. Formally, we define convexity as follows."
CONVEX SETTING,0.07021276595744681,"Definition 4. A real-valued differentiable function f : X →R is µ-strongly-convex for µ ≥0 if X
is a convex set and f(y) −f(x) ≥

∇f(x), y −x

+ µ"
CONVEX SETTING,0.07127659574468086,"2 ∥y −x∥2 for any x, y ∈X. A function is
convex if it is µ-strongly-convex with µ = 0."
CONVEX SETTING,0.07234042553191489,"We assume the existence of a global optimal point x∗throughout this section, as in the following
assumption. However, we want to note that, for gradient descent, this assumption is just for simplicity
rather than necessary.
Assumption 3. There exists a point x∗∈X such that f(x∗) = f ∗= infx∈X f(x)."
GRADIENT DESCENT,0.07340425531914893,"4.1
Gradient descent"
GRADIENT DESCENT,0.07446808510638298,The gradient descent method with a constant stepsize η is defined via the following update rule
GRADIENT DESCENT,0.07553191489361702,"xt+1 = xt −η∇f(xt).
(3)"
GRADIENT DESCENT,0.07659574468085106,"As discussed below Lemma 3.3, the key in the convergence analysis is to show ∥∇f(xt)∥≤G for
all t ≥0 and some constant G. We will prove it by induction relying on the following lemma whose
proof is deferred to Appendix B.
Lemma 4.1. For any x ∈X satisfying ∥∇f(x)∥≤G, define x+ := x −η∇f(x). If f is convex"
GRADIENT DESCENT,0.07765957446808511,"and (r, ℓ)-smooth, and η ≤min
n
2
ℓ(G), r(G)"
"G
O",0.07872340425531915,"2G
o
, we have x+ ∈X and ∥∇f(x+)∥≤∥∇f(x)∥≤G."
"G
O",0.0797872340425532,"Lemma 4.1 suggests that for gradient descent (3) with a small enough stepsize, if the gradient norm
at xt is bounded by G, then we have ∥∇f(xt+1)∥≤∥∇f(xt)∥≤G, i.e., the gradient norm is also
bounded by G at t+1. In other words, the gradient norm is indeed a non-increasing potential function
for gradient descent in the convex setting. With a standard induction argument, we can show that
∥∇f(xt)∥≤∥∇f(x0)∥for all t ≥0. As discussed below Lemma 3.3, then we can basically apply
the classical analysis to obtain the convergence guarantee in the convex setting as in the following
theorem, whose proof is deferred to Appendix B."
"G
O",0.08085106382978724,"Theorem 4.2. Suppose f is convex and (r, ℓ)-smooth. Denote G := ∥∇f(x0)∥and L := ℓ(G), then"
"G
O",0.08191489361702127,"the iterates generated by (3) with η ≤min
n
1
L, r(G)"
"G
O",0.08297872340425531,"2G
o
satisfy ∥∇f(xt)∥≤G for all t ≥0 and"
"G
O",0.08404255319148936,"f(xT ) −f ∗≤∥x0 −x∗∥2 2ηT
."
"G
O",0.0851063829787234,"Since η is a constant independent of ϵ or T, Theorem 4.2 achieves the classical O(1/T) rate, or
O(1/ϵ) gradient complexity to achieve an ϵ-sub-optimal point, under the generalized smoothness
condition. Since strongly convex functions are a subset of convex functions, Lemma 4.1 still holds
for them. Then we immediately obtain the following result in the strongly convex setting, whose
proof is deferred to Appendix B.
Theorem 4.3. Suppose f is µ-strongly-convex and (r, ℓ)-smooth. Denote G := ∥∇f(x0)∥and"
"G
O",0.08617021276595745,"L := ℓ(G), then the iterates generated by (3) with η ≤min
n
1
L, r(G)"
"G
O",0.08723404255319149,"2G
o
satisfy ∥∇f(xt)∥≤G for
all t ≥0 and"
"G
O",0.08829787234042553,"f(xT ) −f ∗≤
µ(1 −ηµ)T"
"G
O",0.08936170212765958,2(1 −(1 −ηµ)T ) ∥x0 −x∗∥2 .
"G
O",0.09042553191489362,"Theorem 4.3 gives a linear convergence rate and the O((ηµ)−1 log(1/ϵ)) gradient complexity to
achieve an ϵ-sub-optimal point. Note that for ℓ-smooth functions, we have r(G) G
= 1"
"G
O",0.09148936170212765,"L (see Remark 3.4),
which means we can choose η =
1
2L. Then we obtain the O(κ log(1/ϵ)) rate, where κ := L/µ is
the local condition number around the initial point x0. For standard Lipschitz smooth functions, it
reduces to the classical rate of gradient descent."
"G
O",0.0925531914893617,"4.2
Nesterov’s accelerated gradient method"
"G
O",0.09361702127659574,"In the case of convex and standard Lipschitz smooth functions, it is well known that Nesterov’s
accelerated gradient method (NAG) achieves the optimal O(1/T 2) rate. In this section, we show that"
"G
O",0.09468085106382979,Algorithm 1: Nesterov’s Accelerated Gradient Method (NAG)
"G
O",0.09574468085106383,"input A convex and ℓ-smooth function f, stepsize η, initial point x0"
"G
O",0.09680851063829787,"1: Initialize z0 = x0, B0 = 0, and A0 = 1/η.
2: for t = 0, ... do
3:
Bt+1 = Bt + 1"
"G
O",0.09787234042553192,"2
 
1 + √4Bt + 1
"
"G
O",0.09893617021276596,"4:
At+1 = Bt+1 + 1/η
5:
yt = xt + (1 −At/At+1)(zt −xt)
6:
xt+1 = yt −η∇f(yt)
7:
zt+1 = zt −η(At+1 −At)∇f(yt)
8: end for"
"G
O",0.1,"under the ℓ-smoothness condition with a sub-quadratic ℓ, the optimal O(1/T 2) rate can be achieved
by a slightly modified version of NAG shown in Algorithm 1, the only difference between which
and the classical NAG is that the latter directly sets At+1 = Bt+1 in Line 4. Formally, we have the
following theorem, whose proof is deferred to Appendix C."
"G
O",0.10106382978723404,Theorem 4.4. Suppose f is convex and ℓ-smooth where ℓis sub-quadratic. Then there always exists
"G
O",0.10212765957446808,"a constant G satisfying G ≥max

8
q"
"G
O",0.10319148936170212,"ℓ(2G)((f(x0) −f ∗) + ∥x0 −x∗∥2), ∥∇f(x0)∥

. Denote"
"G
O",0.10425531914893617,"L := ℓ(2G) and choose η ≤min

1
16L2 ,
1
2L
	
. The iterates generated by Algorithm 1 satisfy"
"G
O",0.10531914893617021,f(xT ) −f ∗≤4(f(x0) −f ∗) + 4 ∥x0 −x∗∥2
"G
O",0.10638297872340426,"ηT 2 + 4
."
"G
O",0.1074468085106383,"It is easy to note that Theorem 4.4 achieves the accelerated O(1/T 2) convergence rate, or
equivalently the O(1/√ϵ) gradient complexity to find an ϵ-sub-optimal point, which is optimal
among gradient-based methods [Nesterov, 2003]."
"G
O",0.10851063829787234,"In order to prove Theorem 4.4, we also use induction to show the gradients along the trajectory of
Algorithm 1 are bounded by G. However, unlike gradient descent, the gradient norm is no longer
a potential function or monotonically non-increasing, which makes the induction analysis more
challenging. Suppose that we have shown ∥∇f(ys)∥≤G for s < t. To complete the induction,
it suffices to prove ∥∇f(yt)∥≤G. Since xt = yt−1 −η∇f(yt−1) is a gradient descent step by
Line 6 of Algorithm 1, Lemma 4.1 directly shows ∥∇f(xt)∥≤G. In order to also bound ∥∇f(yt)∥,
we try to control ∥yt −xt∥, which is the most challenging part of our proof. Since yt −xt can be
expressed as a linear combination of past gradients {∇f(ys)}s<t, it might grow linearly with t if we
simply apply ∥∇f(ys)∥≤G for s < t. Fortunately, Lemma 3.5 allows us to control the gradient
norm with the function value. Thus if the function value is decreasing sufficiently fast, which can be
shown by following the standard Lyapunov analysis of NAG, we are able to obtain a good enough
bound on ∥∇f(ys)∥for s < t, which allows us to control ∥yt −xt∥. We defer the detailed proof to
Appendix C."
"G
O",0.10957446808510639,"Note that Theorem 4.4 requires a smaller stepsize η = O(1/L2), compared to the classical O(1/L)
stepsize for standard Lipschitz smooth functions. The reason is we require a small enough stepsize to
get a good enough bound on ∥yt −xt∥. However, if the function is further assumed to be ℓ-smooth
with a sub-linear ℓ, the requirement of stepsize can be relaxed to η = O(1/L), similar to the classical
requirement. See Appendix C for the details."
"G
O",0.11063829787234042,"In the strongly convex setting, we can also prove convergence of NAG with different {At}t≥0
parameters when f is ℓ-smooth with a sub-quadratic ℓ, or (ρ, L0, Lρ)-smooth with ρ < 2. The rate
can be further improved when ρ becomes smaller. However, since the constants G and L are different
for GD and NAG, it is not clear whether the rate of NAG is faster than that of GD in the strongly
convex setting. We will present the detailed result and analysis in Appendix D."
NON-CONVEX SETTING,0.11170212765957446,"5
Non-convex setting"
NON-CONVEX SETTING,0.1127659574468085,"In this section, we present convergence results of gradient descent (GD) and stochastic gradient
descent (SGD) in the non-convex setting."
GRADIENT DESCENT,0.11382978723404255,"5.1
Gradient descent"
GRADIENT DESCENT,0.1148936170212766,"Similar to the convex setting, we still want to bound the gradients along the trajectory. However,
in the non-convex setting, the gradient norm is not necessarily non-increasing. Fortunately, similar
to the classical analyses, the function value is still non-increasing and thus a potential function, as
formally shown in the following lemma, whose proof is deferred to Appendix E."
GRADIENT DESCENT,0.11595744680851064,"Lemma 5.1. Suppose f is ℓ-smooth where ℓis sub-quadratic. For any given F ≥0, let G :=
sup

u ≥0 | u2 ≤2ℓ(2u) · F
	
and L := ℓ(2G). For any x ∈X satisfying f(x) −f ∗≤F, define
x+ := x −η∇f(x) where η ≤2/L, we have x+ ∈X and f(x+) ≤f(x)."
GRADIENT DESCENT,0.11702127659574468,"Then using a standard induction argument, we can show f(xt) ≤f(x0) for all t ≥0. According to
Corollary 3.6, it implies bounded gradients along the trajectory. Therefore, we can show convergence
of gradient descent as in the following theorem, whose proof is deferred to Appendix E."
GRADIENT DESCENT,0.11808510638297873,"Theorem
5.2.
Suppose
f
is
ℓ-smooth
where
ℓ
is
sub-quadratic.
Let
G
:=
sup

u ≥0 | u2 ≤2ℓ(2u) · (f(x0) −f ∗)
	
and L := ℓ(2G).
If η ≤1/L, the iterates gener-
ated by (3) satisfy ∥∇f(xt)∥≤G for all t ≥0 and"
T,0.11914893617021277,"1
T X"
T,0.1202127659574468,"t<T
∥∇f(xt)∥2 ≤2(f(x0) −f ∗) ηT
."
T,0.12127659574468085,"It is clear that Theorem 5.2 gives the classical O(1/ϵ2) gradient complexity to achieve an ϵ-stationary
point, which is optimal as it matches the lower bound in [Carmon et al., 2017]."
STOCHASTIC GRADIENT DESCENT,0.12234042553191489,"5.2
Stochastic gradient descent"
STOCHASTIC GRADIENT DESCENT,0.12340425531914893,"In this part, we present the convergence result for stochastic gradient descent defined as follows."
STOCHASTIC GRADIENT DESCENT,0.12446808510638298,"xt+1 = xt −ηgt,
(4)"
STOCHASTIC GRADIENT DESCENT,0.125531914893617,"where gt is an estimate of the gradient ∇f(xt). We consider the following standard assumption on
the gradient noise ϵt := gt −∇f(xt)."
STOCHASTIC GRADIENT DESCENT,0.12659574468085105,"Assumption 4. Et−1[ϵt] = 0 and Et−1
h
∥ϵt∥2i
≤σ2 for some σ ≥0, where Et−1 denotes the"
STOCHASTIC GRADIENT DESCENT,0.1276595744680851,expectation conditioned on {gs}s<t.
STOCHASTIC GRADIENT DESCENT,0.12872340425531914,"Under Assumption 4, we can obtain the following theorem."
STOCHASTIC GRADIENT DESCENT,0.12978723404255318,"Theorem 5.3. Suppose f is ℓ-smooth where ℓis sub-quadratic. For any 0 < δ < 1, we denote
F := 8(f(x0) −f ∗+ σ)/δ and G := sup{u ≥0 | u2 ≤2ℓ(2u) · F} < ∞. Denote L := ℓ(2G)"
STOCHASTIC GRADIENT DESCENT,0.13085106382978723,"and choose η ≤min
n
1
2L,
1
4G
√ T"
STOCHASTIC GRADIENT DESCENT,0.13191489361702127,"o
and T ≥
F
ηϵ2 for any ϵ > 0. Then with probability at least 1 −δ,"
STOCHASTIC GRADIENT DESCENT,0.13297872340425532,the iterates generated by (4) satisfy ∥∇f(xt)∥≤G for all t < T and
T,0.13404255319148936,"1
T X"
T,0.1351063829787234,"t<T
∥∇f(xt)∥2 ≤ϵ2."
T,0.13617021276595745,"As we choose η = O(1/
√"
T,0.1372340425531915,"T), Theorem 5.3 gives the classical O(1/ϵ4) gradient complexity, where
we ignore non-leading terms. This rate is optimal as it matches the lower bound in [Arjevani et al.,
2019]. The key to its proof is again to bound the gradients along the trajectory. However, bounding
gradients in the stochastic setting is much more challenging than in the deterministic setting, especially
with the heavy-tailed noise in Assumption 4. We briefly discuss some of the challenges as well as our
approach below and defer the detailed proof of Theorem 5.3 to Appendix F."
T,0.13829787234042554,"First, due to the existence of heavy-tailed gradient noise as considered in Assumption 4, neither
the gradient nor the function values is non-increasing. The induction analyses we have used in the
deterministic setting hardly work. In addition, to apply Lemma 3.3, we need to control the update at
each step and make sure ∥xt+1 −xt∥= η ∥gt∥≤G/L. However, gt might be unbounded due to the
potentially unbounded gradient noise."
T,0.13936170212765958,"To overcome these challenges, we define the following random variable τ."
T,0.14042553191489363,"τ1 := min{t | f(xt+1) −f ∗> F} ∧T,"
T,0.14148936170212767,"τ2 := min

t
∥ϵt∥>
G
5ηL"
T,0.1425531914893617,"
∧T,
(5)"
T,0.14361702127659576,"τ := min{τ1, τ2},"
T,0.14468085106382977,"where we use a ∧b to denote min{a, b} for any a, b ∈R. Then at least before time τ, we know that
the function value and gradient noise are bounded, where the former also implies bounded gradients
according to Corollary 3.6. Therefore, it suffices to show the probability of τ < T is small, which
means with a high probability, τ = T and thus gradients are always bounded before T."
T,0.14574468085106382,"Since both the gradient and noise are bounded for t < τ, it is straightforward to bound the update
∥xt+1 −xt∥, which allows us to use Lemma 3.3 and other useful properties. However, it is still
non-trivial to upper bound E[f(xτ) −f ∗] as τ is a random variable instead of a fixed time step.
Fortunately, τ is a stopping time with nice properties. That is because both f(xt+1) and ϵt =
gt −∇f(xt) only depend on {gs}s≤t, i.e., the stochastic gradients up to t. Therefore, for any fixed t,
the events {τ > t} only depend on {gs}s≤t, which show τ is a stopping time. Then with a careful
analysis, we are still able to obtain an upper bound on E[f(xτ) −f ∗] = O(1)."
T,0.14680851063829786,"On the other hand, τ < T means either τ = τ1 < T or τ = τ2 < T. If τ = τ1 < T, by its definition,
we know f(xτ+1) −f ∗> F. Roughly speaking, it also suggests f(xτ) −f ∗> F/2. If we choose
F such that it is much larger than the upper bound on E[f(xτ) −f ∗] we just obtained, by Markov’s
inequality, we can show the probability of τ = τ1 < T is small. In addition, by union bound and
Chebyshev’s inequality, the probability of τ2 < T can also be bounded by a small constant. Therefore,
we have shown τ < T. Then the rest of the analysis is not too hard following the classical analysis."
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.1478723404255319,"5.3
Reconciliation with existing lower bounds"
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.14893617021276595,"In this section, we reconcile our convergence results for constant-stepsize GD/SGD in the non-convex
setting with existing lower bounds in [Zhang et al., 2019] and [Wang et al., 2022], based on which
the authors claim that adaptive methods such as GD/SGD with clipping and Adam are provably faster
than non-adaptive GD/SGD. This may seem to contradict our convergence results. In fact, we show
that any gain in adaptive methods is at most by constant factors, as GD and SGD already achieve the
optimal rates in the non-convex setting."
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.15,"[Zhang et al., 2019] provides both upper and lower complexity bounds for constant-stepsize GD for
(L0, L1)-smooth functions, and shows that its complexity is O(Mϵ−2), where"
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.15106382978723404,M := sup{∥∇f(x)∥| f(x) ≤f(x0)}
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.15212765957446808,"is the supremum gradient norm below the level set of the initial function value. If M is very large, then
the O(Mϵ−2) complexity can be viewed as a negative result, and as evidence that constant-stepsize
GD can be slower than GD with gradient clipping, since in the latter case, they obtain the O(ϵ−2)
complexity without M. However, based on our Corollary 3.6, their M can be actually bounded by
our G, which is a constant. Therefore, the gain in adaptive methods is at most by constant factors."
RECONCILIATION WITH EXISTING LOWER BOUNDS,0.15319148936170213,"[Wang et al., 2022] further provides a lower bound which shows non-adaptive GD may diverge for
some examples. However, their counter-example does not allow the stepsize to depend on the initial
sub-optimality gap. In contrast, our stepsize η depends on the effective smoothness constant L, which
depends on the initial sub-optimality gap through G. Therefore, there is no contradiction here either.
We should point out that in the practice of training neural networks, the stepsize is usually tuned after
fixing the loss function and initialization, so it does depend on the problem instance and initialization."
LOWER BOUND,0.15425531914893617,"5.4
Lower bound"
LOWER BOUND,0.15531914893617021,"For (ρ, L0, Lρ)-smooth functions with ρ < 2, it is easy to verify that the constant G in both
Theorem 5.2 and Theorem 5.3 is a polynomial function of problem-dependent parameters like
L0, Lρ, f(x0) −f ∗, σ, etc. In other words, GD and SGD are provably efficient methods in the
non-convex setting for ρ < 2. In this section, we show that the requirement of ρ < 2 is necessary in
the non-convex setting with the lower bound for GD in the following Theorem 5.4, whose proof is
deferred in Appendix G. Since SGD reduces to GD when there is no gradient noise, it is also a lower
bound for SGD."
LOWER BOUND,0.15638297872340426,"Theorem 5.4. Given L0, L2, G0, ∆0 > 0 satisfying L2∆0 ≥10, for any η ≥0, there exists a
(2, L0, L2)-smooth function f that satisfies Assumptions 1 and 2, and initial point x0 that satisfies
∥∇f(x0)∥≤G0 and f(x0) −f ∗≤∆0, such that gradient descent with stepsize η (3) either cannot
reach a 1-stationary point or takes at least exp(L2∆0/8)/6 steps to reach a 1-stationary point."
CONCLUSION,0.1574468085106383,"6
Conclusion"
CONCLUSION,0.15851063829787235,"In
this
paper,
we
generalize
the
standard
Lipschitz
smoothness
as
well
as
the
(L0, L1)-smoothness [Zhang et al., 2020] conditions to the ℓ-smoothness condition, and
develop a new approach for analyzing the convergence under this condition. The approach uses
different techniques for several methods and settings to bound the gradient along the optimization
trajectory, which allows us to obtain stronger results for both convex and non-convex problems. We
obtain the classical rates for GD/SGD/NAG methods in the convex and/or non-convex setting. Our
results challenge the folklore belief on the necessity of adaptive methods for generalized smooth
functions."
CONCLUSION,0.1595744680851064,"There are several interesting future directions following this work. First, the ℓ-smoothness can perhaps
be further generalized by allowing ℓto also depend on potential functions in each setting, besides the
gradient norm. In addition, it would also be interesting to see if the techniques of bounding gradients
along the trajectory that we have developed in this and the concurrent work [Li et al., 2023] can be
further generalized to other methods and problems and to see whether more efficient algorithms can
be obtained. Finally, although we justified the necessity of the requirement of ℓ-smoothness with a
sub-quadratic ℓin the non-convex setting, it is not clear whether it is also necessary for NAG in the
convex setting, another interesting open problem."
CONCLUSION,0.16063829787234044,Acknowledgments
CONCLUSION,0.16170212765957448,"This work was supported, in part, by the MIT-IBM Watson AI Lab and ONR Grants
N00014-20-1-2394 and N00014-23-1-2299. We also acknowledge support from DOE under grant
DE-SC0022199, and NSF through awards DMS-2031883, DMS-1953181, and DMS-2022448
(TRIPODS program)."
REFERENCES,0.16276595744680852,References
REFERENCES,0.16382978723404254,"Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, and Blake E. Woodworth.
Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199:165 –
214, 2019."
REFERENCES,0.16489361702127658,"Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004."
REFERENCES,0.16595744680851063,"Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. Mathematical Programming, pages 1–50, 2017."
REFERENCES,0.16702127659574467,"Ziyi Chen, Yi Zhou, Yingbin Liang, and Zhaosong Lu. Generalized-smooth nonconvex optimization
is as efficient as smooth nonconvex optimization. arXiv preprint arXiv:2303.02854, 2023."
REFERENCES,0.16808510638297872,"Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness
to unbounded smoothness of generalized signsgd. Advances in Neural Information Processing
Systems, 35:9955–9968, 2022."
REFERENCES,0.16914893617021276,"Alexandre d’Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. Foundations and
Trends® in Optimization, 5(1-2):1–245, 2021. ISSN 2167-3888. doi: 10.1561/2400000036. URL
http://dx.doi.org/10.1561/2400000036."
REFERENCES,0.1702127659574468,"Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai.
Beyond uniform
smoothness: A stopped analysis of adaptive sgd. ArXiv, abs/2302.06570, 2023."
REFERENCES,0.17127659574468085,"Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.1723404255319149,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
CoRR,
abs/1412.6980, 2014."
REFERENCES,0.17340425531914894,"Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed assumptions.
arXiv preprint arXiv:2304.13972, 2023."
REFERENCES,0.17446808510638298,"Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. Wiley-Interscience, 1983."
REFERENCES,0.17553191489361702,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.17659574468085107,"Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao. Understanding gradient
clipping in incremental gradient methods. In International Conference on Artificial Intelligence
and Statistics, 2021."
REFERENCES,0.1776595744680851,"Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced clipping
for non-convex optimization. ArXiv, abs/2303.00883, 2023."
REFERENCES,0.17872340425531916,"Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen.
Provable adaptivity in adam. arXiv preprint arXiv:2208.09900, 2022."
REFERENCES,0.1797872340425532,"Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for
non-convex optimization. ArXiv, abs/2010.02519, 2020."
REFERENCES,0.18085106382978725,"Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019."
REFERENCES,0.1819148936170213,"Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic
normalized gradient descent. Science China Information Sciences, 64, 2021."
REFERENCES,0.1829787234042553,"A
Proofs related to generalized smoothness"
REFERENCES,0.18404255319148935,"In this section, we provide the proofs of propositions and lemmas related to the generalized
smoothness condition in Definition 1 or 2. First, in Appendix A.1, we justify the examples we
discussed in Section 3. Next, we provide the detailed proof of Proposition 3.2 in Appendix A.2.
Finally, we provide the proofs of the useful properties of generalized smoothness in Appendix A.3,
including Lemma 3.3, Lemma 3.5, and Corollary 3.6 stated in Section 3.1.2."
REFERENCES,0.1851063829787234,"A.1
Justification of examples in Section 3"
REFERENCES,0.18617021276595744,"In this section, we justify the univariate examples of (ρ, L0, Lρ)-smooth functions listed in Table 2
and also provide the proof of Propositions 3.7."
REFERENCES,0.18723404255319148,"First, it is well-known that all quadratic functions have bounded Hessian and are Lipschitz smooth,
corresponding to ρ = 0. Next, [Zhang et al., 2019, Lemma 2] shows that any univariate polynomial
is (L0, L1)-smooth, corresponding to ρ = 1. Then, regarding the exponential function f(x) = ax
where a > 1, we have f ′(x) = log(a)ax and f ′′(x) = log(a)2ax = log(a)f ′(x), which implies
f is (1, 0, log(a))-smooth. Similarly, by standard calculations, it is straight forward to verify that
logarithmic functions and xp, p ̸= 1 are also (ρ, L0, Lρ)-smooth with ρ = 2 and ρ = p−2"
REFERENCES,0.18829787234042553,"p−1 respectively.
So far we have justified all the examples in Table 2 except double exponential functions a(bx) and
rational functions, which will be justified rigorously by the two propositions below."
REFERENCES,0.18936170212765957,"First, for double exponential functions in the form of f(x) = a(bx) where a, b > 1, we have the
following proposition, which shows f is (ρ, L0, Lρ)-smooth for any ρ > 1."
REFERENCES,0.19042553191489361,"Proposition A.1. For any ρ > 1, the double exponential function f(x) = a(bx), where a, b > 1, is
(ρ, L0, Lρ)-smooth for some L0, Lρ ≥0. However, it is not necessarily (L0, L1)-smooth for any
L0, L1 ≥0."
REFERENCES,0.19148936170212766,"Proof of Proposition A.1. By standard calculations, we can obtain"
REFERENCES,0.1925531914893617,"f ′(x) = log(a) log(b) bxa(bx),
f ′′(x) = log(b)(log(a)bx + 1) · f ′(x).
(6)"
REFERENCES,0.19361702127659575,"Note that if ρ > 1,"
REFERENCES,0.1946808510638298,"lim
x→+∞
|f ′(x)|ρ"
REFERENCES,0.19574468085106383,"|f ′′(x)| =
lim
x→+∞
|f ′(x)|ρ−1"
REFERENCES,0.19680851063829788,"log(b)(log(a)bx + 1) =
lim
y→+∞
(log(a) log(b)y)ρ−1 a(ρ−1)y"
REFERENCES,0.19787234042553192,"log(b)(log(a)y + 1)
= ∞,"
REFERENCES,0.19893617021276597,"where the first equality is a direct calculation based on (6); the second equality uses change of
variable y = bx; and the last equality is because exponential functions grow faster than affine
functions. Therefore, for any Lρ > 0, there exists x0 ∈R such that |f ′′(x)| ≤Lρ |f ′(x)|ρ if
x > x0. Next, note that limx→−∞f ′′(x) = 0. Then for any λ1 > 0, there exists x1 ∈R such
that |f ′′(x)| ≤λ1 if x < x1. Also, since f ′′ is continuous, by Weierstrass’s Theorem, we have
|f ′′(x)| ≤λ2 if x1 ≤x ≤x0 for some λ2 > 0. Then denoting L0 = max{λ1, λ2}, we know f is
(ρ, L0, Lρ)-smooth."
REFERENCES,0.2,"Next, to show f is not necessarily (L0, L1)-smooth, consider the specific double exponential function
f(x) = e(ex). Then we have"
REFERENCES,0.20106382978723406,"f ′(x) = exe(ex),
f ′′(x) = (ex + 1) · f ′(x)."
REFERENCES,0.20212765957446807,"For any x ≥max {log(L0 + 1), log(L1 + 1)}, we can show that"
REFERENCES,0.20319148936170212,"|f ′′(x)| > (L1 + 1)f ′(x) > L0 + L1 |f ′(x)| ,"
REFERENCES,0.20425531914893616,"which shows f is not (L0, L1) smooth for any L0, L1 ≥0."
REFERENCES,0.2053191489361702,"In the next proposition, we show that any univariate rational function f(x) = P(x)/Q(x), where P
and Q are two polynomials, is (ρ, L0, Lρ)-smooth with ρ = 1.5.
Proposition A.2. The rational function f(x) = P(x)/Q(x), where P and Q are two polynomials,
is (1.5, L0, L1.5)-smooth for some L0, L1.5 ≥0. However, it is not necessarily (ρ, L0, Lρ)-smooth
for any ρ < 1.5 and L0, Lρ ≥0."
REFERENCES,0.20638297872340425,"Proof of Proposition A.2. Let f(x) = P(x)/Q(x) where P and Q are two polynomials. Then the
partial fractional decomposition of f(x) is given by"
REFERENCES,0.2074468085106383,"f(x) = w(x) + m
X i=1 ji
X r=1"
REFERENCES,0.20851063829787234,"Air
(x −ai)r + n
X i=1 ki
X r=1"
REFERENCES,0.20957446808510638,"Birx + Cir
(x2 + bix + ci)r ,"
REFERENCES,0.21063829787234042,"where w(x) is a polynomial, Air, Bir, Cir, ai, bi, ci are all real constants satisfying b2
i −4ci < 0 for
each 1 ≤i ≤n which implies x2 + bix + ci > 0 for all x ∈R. Assume ji ≥1 and Aiji ̸= 0 without
loss of generality. Then we know f has only finite singular points {ai}1≤i≤m and has continuous
first and second order derivatives at all other points. To simplify notation, denote"
REFERENCES,0.21170212765957447,"pir(x) :=
Air
(x −ai)r ,
qir(x) :=
Birx + Cir
(x2 + bix + ci)r ."
REFERENCES,0.2127659574468085,"Then we have f(x) = w(x) + Pm
i=1
Pji
r=1 pir(x) + Pn
i=1
Pki
r=1 qir(x). We know that r+2"
REFERENCES,0.21382978723404256,"r+1 ≤1.5
for any r ≥1. Then we can show that"
REFERENCES,0.2148936170212766,"lim
x→ai
|f ′(x)|1.5"
REFERENCES,0.21595744680851064,"|f ′′(x)|
= lim
x→ai"
REFERENCES,0.2170212765957447,"p′
iji(x)
1.5
p′′
iji(x)

≥
1
ji + 1,
(7)"
REFERENCES,0.21808510638297873,"where the first equality is because one can easily verify that the first and second order derivatives
of piji dominate those of all other terms when x goes to ai, and the second equality is by standard
calculations noting that ji+2"
REFERENCES,0.21914893617021278,"ji+1 ≤1.5. Note that (7) implies that, for any Lρ > ji + 1, there exists
δi > 0 such that"
REFERENCES,0.22021276595744682,"|f ′′(x)| ≤Lρ |f ′(x)|1.5 ,
if |x −ai| < δi.
(8)"
REFERENCES,0.22127659574468084,"Similarly, one can show limx→∞|f ′(x)|
1.5"
REFERENCES,0.22234042553191488,"|f ′′(x)|
= ∞, which implies there exists x0 > 0 such that"
REFERENCES,0.22340425531914893,"|f ′′(x)| ≤Lρ |f ′(x)|1.5 ,
if |x| > x0.
(9)
Define
B := {x ∈R | |x| ≤x0 and |x −ai| ≥δi, ∀i} .
We know B is a compact set and therefore the continuous function f ′′ is bounded within B, i.e., there
exists some constant L0 > 0 such that
|f ′′(x)| ≤L0,
if x ∈B.
(10)
Combining (8), (9), and (10), we have shown"
REFERENCES,0.22446808510638297,"|f ′′(x)| ≤L0 + Lρ |f ′(x)|1.5 ,
∀x ∈dom(f),
which completes the proof of the first part."
REFERENCES,0.225531914893617,"For the second part, consider the ration function f(x) = 1/x. Then we know that f ′(x) = −1/x2"
REFERENCES,0.22659574468085106,"and f ′′(x) = 2/x3. Note that for any ρ < 1.5 and 0 < x ≤min{(L0 + 1)−1/3, (Lρ + 1)−1/(3−2ρ)},
we have"
REFERENCES,0.2276595744680851,|f ′′(x)| = 1
REFERENCES,0.22872340425531915,"x3 +
1
x3−2ρ · |f ′(x)|ρ > L0 + Lρ |f ′(x)|ρ ,"
REFERENCES,0.2297872340425532,"which shows f is not (ρ, L0, Lρ) smooth for any ρ < 1.5 and L0, Lρ ≥0."
REFERENCES,0.23085106382978723,"Finally, we complete this section with the proof of Proposition 3.7, which shows self-concordant
functions are (2, L0, L2)-smooth for some L0, Lρ ≥0."
REFERENCES,0.23191489361702128,"Proof of Proposition 3.7. Let h : R →R be a self-concordant function. We have h′′′(x) ≤
2h′′(x)3/2. Then, for x ∈(a, b), we can obtain
1
2h′′(x)−1/2h′′′(x) ≤h′′(x)."
REFERENCES,0.23297872340425532,"Integrating both sides from x0 to y for x0, y ∈(a, b), we have"
REFERENCES,0.23404255319148937,"h′′(y)1/2 −h′′(x0)1/2 ≤h′(y) −h′(x0).
Therefore,
h′′(y) ≤(h′′(x0)1/2 −h′(x0) + h′(y))2 ≤2(h′′(x0)1/2 −h′(x0))2 + 2h′(y)2.
Since h′′(y) > 0, we have |h′′(y)| = h′′(y). Therefore, the above inequality shows that h is
(2, L0, L2)-smooth with L0 = 2(h′′(x0)1/2 −h′(x0))2 and L2 = 2."
REFERENCES,0.2351063829787234,"A.2
Proof of Proposition 3.2"
REFERENCES,0.23617021276595745,"In order to prove Proposition 3.2, we need the following several lemmas. First, the lemma below
partially generalizes Grönwall’s inequality."
REFERENCES,0.2372340425531915,"Lemma A.3. Let α : [a, b] →[0, ∞) and β : [0, ∞) →(0, ∞) be two continuous functions. Suppose
α′(t) ≤β(α(t)) almost everywhere over (a, b). Denote function ϕ(u) :=
R
1
β(u) du. We have for all
t ∈[a, b],"
REFERENCES,0.23829787234042554,ϕ(α(t)) ≤ϕ(α(a)) −a + t.
REFERENCES,0.2393617021276596,"Proof of Lemma A.3. First, by definition, we know that ϕ is increasing since ϕ′ =
1
β > 0. Let
function γ : [a, b] →R be the solution of the following differential equation"
REFERENCES,0.2404255319148936,"γ′(t) = β(γ(t)) ∀t ∈(a, b),
γ(a) = α(a).
(11)"
REFERENCES,0.24148936170212765,Then we have
REFERENCES,0.2425531914893617,"dϕ(γ(t)) =
dγ(t)
β(γ(t)) = dt."
REFERENCES,0.24361702127659574,"Integrating both sides, noting that γ(a) = α(a) by (11), we obtain"
REFERENCES,0.24468085106382978,ϕ(γ(t)) −ϕ(α(a)) = t −a.
REFERENCES,0.24574468085106382,"Then it suffices to show ϕ(α(t)) ≤ϕ(γ(t)), ∀t ∈[a, b]. Note that the following inequality holds
almost everywhere."
REFERENCES,0.24680851063829787,"(ϕ(α(t)) −ϕ(γ(t)))′ = ϕ′(α(t))α′(t) −ϕ′(γ(t))γ′(t) =
α′(t)
β(α(t)) −
γ′(t)
β(γ(t)) ≤0,"
REFERENCES,0.2478723404255319,"where the inequality is because α′(t) ≤β(α(t)) by the assumption of this lemma and γ′(t) = β(γ(t))
by (11). Since ϕ(α(a)) −ϕ(γ(a)) = 0, we know for all t ∈[a, b], ϕ(α(t)) ≤ϕ(γ(t)), which
completes the proof."
REFERENCES,0.24893617021276596,"With Lemma A.3, one can bound the gradient norm within a small enough neighborhood of a given
point as in the following lemma."
REFERENCES,0.25,"Lemma A.4. If the objective function f is ℓ-smooth, for any two points x, y ∈Rd such that the
closed line segment between x and y is contained in X, if ∥y −x∥≤
a
ℓ(∥∇f(x)∥+a) for any a > 0,
we have
∥∇f(y)∥≤∥∇f(x)∥+ a."
REFERENCES,0.251063829787234,"Proof of Lemma A.4. Denote z(t) := (1 −t)x + ty for 0 ≤t ≤1. Then we know z(t) ∈X for all
0 ≤t ≤1 by the assumption made in this lemma. Then we can also define α(t) := ∥∇f(z(t))∥for
0 ≤t ≤1. Note that for any 0 ≤t ≤s ≤1, by triangle inequality,"
REFERENCES,0.2521276595744681,"α(s) −α(t) ≤∥∇f(z(s)) −∇f(z(t))∥.
(12)"
REFERENCES,0.2531914893617021,"We know that α(t) = ∥∇f(z(t))∥is differentiable almost everywhere since f is second order
differentiable almost everywhere (Here we assume α(t) ̸= 0 for 0 < t < 1 without loss of generality.
Otherwise, one can define tm = sup{0 < t < 1 | α(t) = 0} and consider the interval [tm, 1] instead).
Then the following equality holds almost everywhere"
REFERENCES,0.2542553191489362,"α′(t) = lim
s↓t
α(s) −α(t)"
REFERENCES,0.2553191489361702,"s −t
≤lim
s↓t
∥∇f(z(s)) −∇f(z(t))∥"
REFERENCES,0.25638297872340426,"s −t
=
lim
s↓t
∇f(z(s)) −∇f(z(t)) s −t "
REFERENCES,0.2574468085106383,"=
∇2f(z(t))(y −x)
 ≤
∇2f(z(t))
 ∥y −x∥≤ℓ(α(t)) ∥y −x∥,"
REFERENCES,0.25851063829787235,"where the first inequality is due to (12) and the last inequality is by Definition 1. Let β(u) :=
ℓ(u) · ∥y −x∥and ϕ(u) :=
R u
0
1
β(v)dv. By Lemma A.3, we know that"
REFERENCES,0.25957446808510637,ϕ (∥∇f(y)∥) = ϕ(u(1)) ≤ϕ(u(0)) + 1 = ϕ (∥∇f(x)∥) + 1.
REFERENCES,0.26063829787234044,"Denote ψ(u) :=
R u
0
1
ℓ(v)dv = ϕ(u) · ∥y −x∥. We have"
REFERENCES,0.26170212765957446,ψ (∥∇f(y)∥) ≤ψ (∥∇f(x)∥) + ∥y −x∥
REFERENCES,0.26276595744680853,"≤ψ (∥∇f(x)∥) +
a
ℓ(∥∇f(x)∥+ a)"
REFERENCES,0.26382978723404255,"≤
Z ∥∇f(x)∥ 0"
REFERENCES,0.2648936170212766,"1
ℓ(v) dv +
Z ∥∇f(x)∥+a"
REFERENCES,0.26595744680851063,∥∇f(x)∥
REFERENCES,0.2670212765957447,"1
ℓ(v) dv"
REFERENCES,0.2680851063829787,=ψ(∥∇f(x)∥+ a).
REFERENCES,0.2691489361702128,"Since ψ is increasing, we have ∥∇f(y)∥≤∥∇f(x)∥+ a."
REFERENCES,0.2702127659574468,"With Lemma A.4, we are ready to prove Proposition 3.2."
REFERENCES,0.2712765957446808,Proof of Proposition 3.2. We prove the two directions in this proposition separately.
REFERENCES,0.2723404255319149,"1. An (r, ℓ)-smooth function is ℓ-smooth."
REFERENCES,0.2734042553191489,"For each fixed x ∈X where ∇2f(x) exists and any unit-norm vector w, by Definition 2, we know
that for any t ≤r(∥∇f(x)∥),"
REFERENCES,0.274468085106383,∥∇f(x + tw) −∇f(x)∥≤t · ℓ(∥∇f(x)∥).
REFERENCES,0.275531914893617,"Then we know that
∇2f(x)w
 =
lim
t↓0
1"
REFERENCES,0.2765957446808511,t (∇f(x + tw) −∇f(x))
REFERENCES,0.2776595744680851,"= lim
t↓0
1"
REFERENCES,0.27872340425531916,"t ∥(∇f(x + tw) −∇f(x))∥≤ℓ(∥∇f(x)∥),"
REFERENCES,0.2797872340425532,"which implies
∇2f(x)
 ≤ℓ(∥∇f(x)∥) for any point x if ∇2f(x) exists."
REFERENCES,0.28085106382978725,"Then it suffices to show that ∇2f(x) exists almost everywhere. Note that for each x ∈X, Definition 2
states that the gradient function is ℓ(∥∇f(x)∥) Lipschitz within the ball B(x, r(∥∇f(x)∥)). Then
by Rademacher’s Theorem, f is twice differentiable almost everywhere within this ball. Then we
can show it is also twice differentiable almost everywhere within the entire domain X as long as we
can cover X with countably many such balls. Define Sn := {x ∈X | n ≤∥∇f(x)∥≤n + 1} for
integer n ≥0. We have X = ∪n≥0Sn. One can easily find an internal covering of Sn with balls
of size r(n + 1)3, i.e., there exist {xn,i}i≥0, where xn,i ∈Sn, such that Sn ⊆∪i≥0B(xn,i, r(n +
1)) ⊆∪i≥0B(xn,i, r(∥∇f(xn,i)∥)). Therefore we have X ⊆∪n,i≥0B(xn,i, r(∥∇f(xn,i)∥)) which
completes the proof."
REFERENCES,0.28191489361702127,"2. An ℓ-smooth function satisfying Assumption 1 is (r, m)-smooth where m(u) := ℓ(u + a) and
r(u) := a/m(u) for any a > 0."
REFERENCES,0.28297872340425534,"For any y ∈Rd satisfying ∥y −x∥≤r(∥∇f(x)∥) =
a
ℓ(∥∇f(x)∥+a), denote z(t) := (1 −t)x + ty
for 0 ≤t ≤1. We first show y ∈X by contradiction. Suppose y /∈X, let us define tb := inf{0 ≤
t ≤1 | z(t) /∈X} and zb := z(tb). Then we know zb is a boundary point of X. Since f is a closed
function with an open domain, we have"
REFERENCES,0.28404255319148936,"lim
t↑tb f(z(t)) = ∞.
(13)"
REFERENCES,0.2851063829787234,"On the other hand, by the definition of tb, we know z(t) ∈X for every 0 ≤t < tb. Then by
Lemma A.4, for all 0 ≤t < tb, we have ∥∇f(z(t))∥≤∥∇f(x)∥+ a. Therefore for all 0 ≤t < tb,"
REFERENCES,0.28617021276595744,"f(z(t)) ≤f(x) +
Z t 0"
REFERENCES,0.2872340425531915,"∇f(z(s)), y −x

ds"
REFERENCES,0.28829787234042553,"≤f(x) + (∥∇f(x)∥+ a) · ∥y −x∥
<∞,"
REFERENCES,0.28936170212765955,"3We can find an internal covering in the following way. We first cover Sn with countably many hyper-cubes
of length r(n + 1)/
√"
REFERENCES,0.2904255319148936,"d, which is obviously doable. Then for each hyper-cube that intersects with Sn, we pick
one point from the intersection. Then the ball centered at the picked point with radius r(n + 1) covers this
hyper-cube. Therefore, the union of all such balls can cover Sn."
REFERENCES,0.29148936170212764,"which contradicts (13). Therefore we have shown y ∈X. Since y is chosen arbitrarily with the ball
B(x, r(∥∇f(x)∥)), we have B(x, r(∥∇f(x)∥)) ⊆X. Then for any x1, x2 ∈B(x, r(∥∇f(x)∥)), we
denote w(t) := tx1 + (1 −t)x2. Then we know w(t) ∈B(x, r(∥∇f(x)∥)) for all 0 ≤t ≤1 and
can obtain"
REFERENCES,0.2925531914893617,∥∇f(x1) −∇f(x2)∥= Z 1
REFERENCES,0.2936170212765957,"0
∇2f(w(t)) · (x1 −x2) dt"
REFERENCES,0.2946808510638298,"≤∥x1 −x2∥·
Z 1"
REFERENCES,0.2957446808510638,"0
ℓ(∥∇f(x)∥+ a) dt"
REFERENCES,0.2968085106382979,"=m(∥∇f(x)∥) · ∥x1 −x2∥,"
REFERENCES,0.2978723404255319,where the last inequality is due to Lemma A.4.
REFERENCES,0.298936170212766,"A.3
Proofs of lemmas implied by generalized smoothness"
REFERENCES,0.3,"In this part, we provide the proofs of the useful properties stated in Section 3.1.2, including Lemma 3.3,
Lemma 3.5, and Corollary 3.6."
REFERENCES,0.30106382978723406,"Proof of Lemma 3.3. First, note that since ℓis non-decreasing and r is non-increasing, we have
ℓ(∥∇f(x)∥) ≤ℓ(G) = L and r(G) ≤r(∥∇f(x)∥). Then by Definition 2, we directly have that
B(x, r(G)) ⊆B(x, r(∥∇f(x)∥)) ⊆X, and that for any x1, x2 ∈B(x, r(G)), we have"
REFERENCES,0.3021276595744681,∥∇f(x1) −∇f(x2)∥≤ℓ(∥∇f(x)∥) ∥x1 −x2∥≤L ∥x1 −x2∥.
REFERENCES,0.30319148936170215,"Next, for the second inequality in (2), define z(t) := (1 −t)x2 + tx1 for 0 ≤t ≤1. We know
z(t) ∈B(x, r(G)). Note that we have shown"
REFERENCES,0.30425531914893617,"∥∇f(z(t)) −∇f(x2)∥≤L ∥z(t) −x2∥= tL ∥x1 −x2∥.
(14)"
REFERENCES,0.30531914893617024,Then we have
REFERENCES,0.30638297872340425,"f(x1) −f(x2) =
Z 1 0"
REFERENCES,0.3074468085106383,"∇f(z(t), x1 −x2

dt =
Z 1 0"
REFERENCES,0.30851063829787234,"∇f(x2), x1 −x2

+

∇f(z(t)) −∇f(x2), x1 −x2

dt"
REFERENCES,0.30957446808510636,"≤

∇f(x2), x1 −x2

+ L ∥x1 −x2∥2
Z 1"
T DT,0.31063829787234043,"0
t dt"
T DT,0.31170212765957445,"=

∇f(x2), x1 −x2

+ L"
T DT,0.3127659574468085,"2 ∥x1 −x2∥2 ,"
T DT,0.31382978723404253,where the inequality is due to (14).
T DT,0.3148936170212766,"Proof of Lemma 3.5. If f is ℓ-smooth, by Proposition 3.2, f is also (r, m)-smooth where m(u) =
ℓ(2u) and r(u) = u/ℓ(2u). Then by Lemma 3.3 where we choose G = ∥∇f(x)∥, we have that"
T DT,0.3159574468085106,"B

x,
∥∇f(x)∥
ℓ(2∥∇f(x)∥)

⊆X, and that for any x1, x2 ∈B

x,
∥∇f(x)∥
ℓ(2∥∇f(x)∥)

, we have"
T DT,0.3170212765957447,"f(x1) ≤f(x2) +

∇f(x2), x1 −x2

+ ℓ(2 ∥∇f(x)∥)"
T DT,0.3180851063829787,"2
∥x1 −x2∥."
T DT,0.3191489361702128,"Choosing x2 = x and x1 = x −
∇f(x)
ℓ(2∥∇f(x)∥), it is easy to verify that x1, x2 ∈B

x,
∥∇f(x)∥
ℓ(2∥∇f(x)∥)

.
Therefore, we have"
T DT,0.3202127659574468,"f ∗≤f

x −
∇f(x)
ℓ(2 ∥∇f(x)∥)"
T DT,0.32127659574468087,"
≤f(x) −
∥∇f(x)∥2"
T DT,0.3223404255319149,"2ℓ(2 ∥∇f(x)∥),"
T DT,0.32340425531914896,which completes the proof.
T DT,0.324468085106383,"Proof of Corollary 3.6. We first show G < ∞.
Note that since ℓis sub-quadratic, we know
limu→∞2ℓ(2u)/u2 = 0.
Therefore, for any F > 0, there exists some M > 0 such that
2ℓ(2u)/u2 < 1/F for every u > M. In other words, for any u satisfying u2 ≤2ℓ(2u) · F,
we must have u ≤M. Therefore, by definition of G, we have G ≤M < ∞if F > 0. If F = 0, we
trivially get G = 0 < ∞. Also, since the set {u ≥0 | u2 ≤2ℓ(2u) · F} is closed and bounded, we
know its supremum G is in this set and it is also straightforward to show G2 = 2ℓ(2G) · F."
T DT,0.32553191489361705,"Next, by Lemma 3.5, we know"
T DT,0.32659574468085106,∥∇f(x)∥2 ≤2ℓ(2 ∥∇f(x)∥) · (f(x) −f ∗) ≤2ℓ(2 ∥∇f(x)∥) · F.
T DT,0.3276595744680851,"Then based on the definition of G, we have ∥∇f(x)∥≤G."
T DT,0.32872340425531915,"B
Analysis of GD for convex functions"
T DT,0.32978723404255317,"In this section, we provide the detailed convergence analysis of gradient descent in the convex setting,
including the proofs of Lemma 4.1 and Theorem 4.2, for which the following lemma will be helpful.
Lemma B.1 (Co-coercivity). If f is convex and (r, ℓ)-smooth, for any x ∈X and y ∈
B(x, r(∥∇f(x)∥)/2), we have y ∈X and"
T DT,0.33085106382978724,"⟨∇f(x) −∇f(y), x −y⟩≥1"
T DT,0.33191489361702126,"L ∥∇f(x) −∇f(y)∥2 ,"
T DT,0.33297872340425533,where L = ℓ(∥∇f(x)∥).
T DT,0.33404255319148934,"Proof of Lemma B.1. Define the Bregman divergences ϕx(w) := f(w) −⟨∇f(x), w⟩and ϕy(w) :=
f(w) −⟨∇f(y), w⟩, which are both convex functions. Since ∇ϕx(w) = ∇f(w) −∇f(x), we
have ∇ϕx(x) = 0 which implies minw ϕx(w) = ϕx(x) as ϕx is convex. Similarly we have
minw ϕy(w) = ϕy(y)."
T DT,0.3351063829787234,"Denote rx := r(∥∇f(x)∥). Since f is (r, ℓ)-smooth, we know its gradient ∇f is L-Lipschitz locally
in B(x, rx). Since ∇ϕx(w) −∇f(w) = ∇f(x) is a constant, we know ∇ϕx is also L-Lipschitz
locally in B(x, rx). Then similar to the proof of Lemma 3.3, one can easily show that for any
x1, x2 ∈B(x, rx), we have"
T DT,0.33617021276595743,"ϕx(x1) ≤ϕx(x2) +

∇ϕx(x2), x1 −x2

+ L"
T DT,0.3372340425531915,"2 ∥x1 −x2∥2 .
(15)"
T DT,0.3382978723404255,"Note that for any y ∈B(x, r(∥∇f(x)∥)/2) as in the lemma statement,
y −1"
T DT,0.3393617021276596,"L∇ϕx(y) −x
 ≤∥y −x∥+ 1"
T DT,0.3404255319148936,"L ∥∇f(y) −∇f(x)∥≤2 ∥y −x∥≤rx,"
T DT,0.3414893617021277,"where the first inequality uses triangle inequality and ∇ϕx(y) = ∇f(y) −∇f(x); and the second
inequality uses Definition 2. It implies that y −1"
T DT,0.3425531914893617,"L∇ϕx(y) ∈B(x, rx). Then we can obtain"
T DT,0.34361702127659577,"ϕx(x) = min
w ϕx(w) ≤ϕx"
T DT,0.3446808510638298,"
y −1"
T DT,0.34574468085106386,"L∇ϕx(y)

≤ϕx(y) −1"
T DT,0.3468085106382979,"2L ∥∇ϕx(y)∥2 ,"
T DT,0.3478723404255319,where the last inequality uses (15) where we choose x1 = y −1
T DT,0.34893617021276596,"L∇ϕx(y) and x2 = y. By the
definition of ϕx, the above inequality is equivalent to"
T DT,0.35,"1
2L ∥∇f(y) −∇f(x)∥2 ≤f(y) −f(x) −⟨∇f(x), x −y⟩."
T DT,0.35106382978723405,Similar argument can be made for ϕy(·) to obtain
T DT,0.35212765957446807,"1
2L ∥∇f(y) −∇f(x)∥2 ≤f(x) −f(y) −⟨∇f(y), y −x⟩."
T DT,0.35319148936170214,"Summing up the two inequalities, we can obtain the desired result."
T DT,0.35425531914893615,"With Lemma B.1, we prove Lemma 4.1 as follows."
T DT,0.3553191489361702,"Proof of Lemma 4.1. Let L = ℓ(G). We first verify that x+ ∈B(x, r(G)/2). Note that
x+ −x
 = ∥η∇f(x)∥≤ηG ≤r(G)/2,"
T DT,0.35638297872340424,"where we choose η ≤r(G)/(2G). Thus by Lemma B.1, we have
∇f(x+)
2 = ∥∇f(x)∥2 + 2⟨∇f(x+) −∇f(x), ∇f(x)⟩+
∇f(x+) −∇f(x)
2"
T DT,0.3574468085106383,= ∥∇f(x)∥2 −2
T DT,0.35851063829787233,"η ⟨∇f(x+) −∇f(x), x+ −x⟩+
∇f(x+) −∇f(x)
2"
T DT,0.3595744680851064,"≤∥∇f(x)∥2 +

1 −2 ηL"
T DT,0.3606382978723404," ∇f(x+) −∇f(x)
2"
T DT,0.3617021276595745,"≤∥∇f(x)∥2 ,"
T DT,0.3627659574468085,where the first inequality uses Lemma B.1 and the last inequality chooses η ≤2/L.
T DT,0.3638297872340426,"With Lemma 4.1, we are ready to prove both Theorem 4.2 and Theorem 4.3."
T DT,0.3648936170212766,"Proof of Theorem 4.2. Denote G := ∥∇f(x0)∥. Then we trivially have ∥∇f(x0)∥≤G. Lemma 4.1
states that if ∥∇f(xt)∥≤G for any t ≥0, then we also have ∥∇f(xt+1)∥≤∥∇f(xt)∥≤G. By
induction, we can show that ∥∇f(xt)∥≤G for all t ≥0. Then the rest of the proof basically follows
the standard textbook analysis. We still provide the detailed proof below for completeness."
T DT,0.3659574468085106,"Note that ∥xt+1 −xt∥= η ∥∇f(xt)∥≤ηG ≤r(G), where we choose η ≤r(G)/(2G). Thus we
can apply Lemma 3.3 to obtain"
T DT,0.3670212765957447,"0 ≥f(xt+1) −f(xt) −⟨∇f(xt), xt+1 −xt⟩−L"
T DT,0.3680851063829787,2 ∥xt+1 −xt∥2
T DT,0.36914893617021277,"≥f(xt+1) −f(xt) −⟨∇f(xt), xt+1 −xt⟩−1"
T DT,0.3702127659574468,"2η ∥xt+1 −xt∥2 ,
(16)"
T DT,0.37127659574468086,"where the last inequality chooses η ≤1/L. Meanwhile, by convexity between xt and x∗, we have"
T DT,0.3723404255319149,"0 ≥f(xt) −f ∗+ ⟨∇f(xt), x∗−xt⟩.
(17)"
T DT,0.37340425531914895,Note that (t + 1)×(16)+(17) gives
T DT,0.37446808510638296,"0 ≥f(xt) −f ∗+ ⟨∇f(xt), x∗−xt⟩"
T DT,0.37553191489361704,"+ (1 + t)

f(xt+1) −f(xt) −⟨∇f(xt), xt+1 −xt⟩−1"
T DT,0.37659574468085105,"2η ∥xt+1 −xt∥2

."
T DT,0.3776595744680851,"Then reorganizing the terms of the above inequality, noting that"
T DT,0.37872340425531914,"∥xt+1 −x∗∥2 −∥xt −x∗∥2 = ∥xt+1 −xt∥2 + 2⟨xt+1 −xt, xt −x∗⟩"
T DT,0.3797872340425532,"= ∥xt+1 −xt∥2 + 2η⟨∇f(xt), x∗−xt⟩,"
T DT,0.38085106382978723,we can obtain
T DT,0.3819148936170213,(t + 1)(f(xt+1) −f ∗) + 1
T DT,0.3829787234042553,2η ∥xt+1 −x∗∥2 ≤t(f(xt) −f ∗) + 1
T DT,0.3840425531914894,2η ∥xt −x∗∥2 .
T DT,0.3851063829787234,"The above inequality implies t(f(xt) −f ∗) +
1
2η ∥xt −x∗∥2 is a non-increasing potential function,
which directly implies the desired result."
T DT,0.3861702127659574,"Proof of Theorem 4.3. Since strongly convex functions are also convex, by the same argument as in
the proof of Theorem 4.2, we have ∥∇f(xt)∥≤G := ∥∇f(x0)∥for all t ≥0. Moreover, (16) still
holds. For µ-strongly-convex function, we can obtain a tighter version of (17) as follows."
T DT,0.3872340425531915,"0 ≥f(xt) −f ∗+ ⟨∇f(xt), x∗−xt⟩+ µ"
T DT,0.3882978723404255,"2 ∥x∗−xt∥2 .
(18)"
T DT,0.3893617021276596,"Let A0 = 0 and At+1 = (1 + At)/(1 −ηµ) for all t ≥0. Combining (16) and (18), we have"
T DT,0.3904255319148936,"0 ≥(At+1 −At)(f(xt) −f ∗+ ⟨∇f(xt), x∗−xt⟩)"
T DT,0.39148936170212767,+ At+1
T DT,0.3925531914893617,"
f(xt+1) −f(xt) −⟨∇f(xt), xt+1 −xt⟩−1"
T DT,0.39361702127659576,"2η ∥xt+1 −xt∥2

."
T DT,0.3946808510638298,"Then reorganizing the terms of the above inequality, noting that"
T DT,0.39574468085106385,"∥xt+1 −x∗∥2 −∥xt −x∗∥2 = ∥xt+1 −xt∥2 + 2⟨xt+1 −xt, xt −x∗⟩"
T DT,0.39680851063829786,"= ∥xt+1 −xt∥2 + 2η⟨∇f(xt), x∗−xt⟩,"
T DT,0.39787234042553193,we can obtain
T DT,0.39893617021276595,At+1(f(xt+1) −f ∗) + 1 + ηµAt+1
T DT,0.4,"2η
∥xt+1 −x∗∥2 ≤At(f(xt) −f ∗) + 1 + ηµAt"
T DT,0.40106382978723404,"2η
∥xt −x∗∥2 ."
T DT,0.4021276595744681,The above inequality means At(f(xt) −f ∗) + 1+ηµAt
T DT,0.4031914893617021,"2η
∥xt −x∗∥2 is a non-increasing potential
function. Thus by telescoping we have"
T DT,0.40425531914893614,"f(xT ) −f ∗≤
µ(1 −ηµ)T"
T DT,0.4053191489361702,2(1 −(1 −ηµ)T ) ∥x0 −x∗∥2 .
T DT,0.40638297872340423,"C
Analysis of NAG for convex functions"
T DT,0.4074468085106383,"In this section, we provide the detailed analysis of Nesterov’s accelerated gradient method in the
convex setting. As we discussed in Section 4.2, the stepsize size choice in Theorem 4.4 is smaller
than the classical one. Therefore, we provide a more fine-grained version of the theorem, which
allows the stepsize to depend on the degree of ℓ.
Theorem C.1. Suppose f is convex and ℓ-smooth.
For α ∈(0, 2], if ℓ(u) = o(uα), i.e.,
limu→∞ℓ(u)/uα = 0, then there must exist a constant G such that for L := ℓ(2G), we have"
T DT,0.4085106382978723,"G ≥max

8 max{L1/α−1/2, 1}
q"
T DT,0.4095744680851064,"L((f(x0) −f ∗) + ∥x0 −x∗∥2), ∥∇f(x0)∥

.
(19)"
T DT,0.4106382978723404,"Choose η ≤min

1
16L3−2/α ,
1
2L
	
. Then the iterates of Algorithm 1 satisfy"
T DT,0.4117021276595745,f(xT ) −f ∗≤4(f(x0 −f ∗) + 4 ∥x0 −x∗∥2
T DT,0.4127659574468085,"ηT 2 + 4
."
T DT,0.41382978723404257,"Note that when α = 2, i.e., ℓis sub-quadratic, Theorem C.1 reduces to Theorem 4.4 which chooses
η ≤min{
1
16L2 ,
1
2L}. When α = 1, i.e., ℓis sub-linear, the above theorem chooses η ≤
1
16L as in the
classical textbook analysis up to a numerical constant factor."
T DT,0.4148936170212766,"Throughout this section, we will assume f is convex and ℓ-smooth, and consider the parameter choices
in Theorem C.1, unless explicitly stated. Note that since f is ℓ-smooth, it is also (r, m)-smooth
with m(u) = ℓ(u + G) and r(u) =
G
ℓ(u+G) by Proposition 3.2. Note that m(G) = ℓ(2G) = L and"
T DT,0.41595744680851066,"r(G) = G/L. Then the stepsize satisfies η ≤1/(2L) ≤min{
2
m(G), r(G) 2G }."
T DT,0.41702127659574467,"Before proving Theorem C.1, we first present several additional useful lemmas. To start with, we
provide two lemmas regarding the weights {At}t≥0 and {Bt}t≥0 used in Algorithm 1. The lemma
below states that Bt = Θ(t2).
Lemma C.2. The weights {Bt}t≥0 in Algorithm 1 satisfy 1"
T DT,0.41808510638297874,4t2 ≤Bt ≤t2 for all t ≥0.
T DT,0.41914893617021276,"Proof of Lemma C.2. We prove this lemma by induction. First note that the inequality obviously
holds for B0 = 0. Suppose its holds up to t. Then we have"
T DT,0.42021276595744683,Bt+1 = Bt + 1
T DT,0.42127659574468085,"2(1 +
p"
T DT,0.4223404255319149,4Bt + 1) ≥1
T DT,0.42340425531914894,4t2 + 1
T DT,0.42446808510638295,"2(1 +
p"
T DT,0.425531914893617,t2 + 1) ≥1
T DT,0.42659574468085104,4(t + 1)2.
T DT,0.4276595744680851,"Similarly, we have"
T DT,0.42872340425531913,Bt+1 = Bt + 1
T DT,0.4297872340425532,"2(1 +
p"
T DT,0.4308510638297872,4Bt + 1) ≤t2 + 1
T DT,0.4319148936170213,"2(1 +
p"
T DT,0.4329787234042553,4t2 + 1) ≤(t + 1)2.
T DT,0.4340425531914894,Lemma C.2 implies the following useful lemma.
T DT,0.4351063829787234,Lemma C.3. The weights {At}t≥0 in Algorithm 1 satisfy that
T DT,0.43617021276595747,"(1 −
At
At+1
) 1 At t−1
X s=0 p"
T DT,0.4372340425531915,As+1(As+1 −As −1) ≤4.
T DT,0.43829787234042555,"Proof of Lemma C.3. First, note that it is easy to verify that As+1 −As −1 = Bs+1 −Bs −1 ≥0,
which implies each term in the LHS of the above inequality is non-negative. Then we have"
T DT,0.43936170212765957,"(1 −
At
At+1
) 1 At t−1
X s=0 p"
T DT,0.44042553191489364,As+1(As+1 −As −1)
T DT,0.44148936170212766,"≤
1
At+1
√At
(At+1 −At) t−1
X"
T DT,0.4425531914893617,"s=0
(As+1 −As −1)
(At ≥As+1)"
T DT,0.44361702127659575,"=
1
At+1
√At
(Bt+1 −Bt) t−1
X"
T DT,0.44468085106382976,"s=0
(Bs+1 −Bs −1)
(As = Bs + 1/η)"
T DT,0.44574468085106383,"=
1
At+1
√At
· 1"
T DT,0.44680851063829785,"2(1 +
p"
T DT,0.4478723404255319,"4Bt + 1) t−1
X s=0"
T DT,0.44893617021276594,"
−1 + 1"
T DT,0.45,"2(1 +
p"
T DT,0.451063829787234,"4Bs + 1)

(by definition of Bs)"
T DT,0.4521276595744681,"≤8
1
(t + 1)2t · (t + 1)t2"
T DT,0.4531914893617021,"2
(by At ≥Bt and Lemma C.2) ≤4."
T DT,0.4542553191489362,"The following lemma summarizes the results in the classical potential function analysis of NAG
in [d’Aspremont et al., 2021]. In order to not deal with the generalized smoothness condition for
now, we directly assume the inequality (20) holds in the lemma, which will be proved later under the
generalized smoothness condition."
T DT,0.4553191489361702,"Lemma C.4. For any t ≥0, if the following inequality holds,"
T DT,0.4563829787234043,"f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ 1"
T DT,0.4574468085106383,"2η ∥xt+1 −yt∥2 ≥f(xt+1),
(20)"
T DT,0.45851063829787236,then we can obtain
T DT,0.4595744680851064,At+1(f(xt+1) −f ∗) + 1
T DT,0.46063829787234045,2η ∥zt+1 −x∗∥2 ≤At(f(xt) −f ∗) + 1
T DT,0.46170212765957447,"2η ∥zt −x∗∥2 .
(21)"
T DT,0.4627659574468085,"Proof of Lemma C.4. These derivations below can be found in [d’Aspremont et al., 2021]. We
present them here for completeness."
T DT,0.46382978723404256,"First, since f is convex, the convexity between x∗and yt gives"
T DT,0.4648936170212766,"f ∗≥f(yt) + ⟨∇f(yt), x∗−yt⟩."
T DT,0.46595744680851064,Similarly the convexity between xt and yt gives
T DT,0.46702127659574466,"f(xt) ≥f(yt) + ⟨∇f(yt), xt −yt⟩."
T DT,0.46808510638297873,"Combining the above two inequalities as well as (20) assumed in this lemma, we have"
T DT,0.46914893617021275,"0 ≥(At+1 −At)(f(yt) −f ∗+ ⟨∇f(yt), x∗−yt⟩)
+ At(f(yt) −f(xt) + ⟨∇f(yt), xt −yt⟩)"
T DT,0.4702127659574468,+ At+1
T DT,0.47127659574468084,"
f(xt+1) −f(yt) −⟨∇f(yt), xt+1 −yt⟩−1"
T DT,0.4723404255319149,"2η ∥xt+1 −yt∥2

.
(22)"
T DT,0.4734042553191489,"Furthermore, note that
1
2η"
T DT,0.474468085106383,"
∥zt+1 −x∗∥2 −∥zt −x∗∥2 = 1 2η"
T DT,0.475531914893617,"
∥zt+1 −zt∥2 + 2⟨zt+1 −zt, zt −x∗⟩
 = 1 2η"
T DT,0.4765957446808511,"
η2(At+1 −At)2 ∥∇f(yt)∥2 −2η(At+1 −At)⟨∇f(yt), zt −x∗⟩
 = η"
T DT,0.4776595744680851,"2(At+1 −At)2 ∥∇f(yt)∥2 −(At+1 −At)⟨∇f(yt), zt −x∗⟩.
(23)"
T DT,0.4787234042553192,"Meanwhile, we have"
T DT,0.4797872340425532,At+1xt+1 = At+1yt −ηAt+1∇f(yt) = At+1xt + (At+1 −At)(zt −xt) −ηAt+1∇f(yt).
T DT,0.4808510638297872,Thus we have
T DT,0.4819148936170213,(At+1 −At)zt = At+1xt+1 −Atxt + ηAt+1∇f(yt).
T DT,0.4829787234042553,"Plugging back in (23), we obtain 1
2η"
T DT,0.48404255319148937,"
∥zt+1 −x∗∥2 −∥zt −x∗∥2 = η"
T DT,0.4851063829787234,"2(At+1 −At)2 ∥∇f(yt)∥2 + (At+1 −At)⟨∇f(yt), x∗⟩"
T DT,0.48617021276595745,"+ ⟨−At+1xt+1 + Atxt −ηAt+1∇f(yt), ∇f(yt)⟩. Thus"
T DT,0.48723404255319147,"(At+1 −At)⟨∇f(yt), x∗⟩+ ⟨Atxt −At+1xt+1, ∇f(yt)⟩ = 1 2η"
T DT,0.48829787234042554,"
∥zt+1 −x∗∥2 −∥zt −x∗∥2
+ η(At+1 −1"
T DT,0.48936170212765956,2(At+1 −At)2) ∥∇f(yt)∥2 .
T DT,0.49042553191489363,So we can reorganize (22) to obtain
T DT,0.49148936170212765,"0 ≥At+1(f(xt+1) −f ∗) −At(f(xt) −f ∗)
+ (At+1 −At)⟨∇f(yt), x∗⟩+ ⟨Atxt −At+1xt+1, ∇f(yt)⟩ −1"
T DT,0.4925531914893617,2η At+1 ∥xt+1 −yt∥2
T DT,0.49361702127659574,= At+1(f(xt+1) −f ∗) −At(f(xt) −f ∗) + 1 2η
T DT,0.4946808510638298,"
∥zt+1 −x∗∥2 −∥zt −x∗∥2
+ η"
T DT,0.4957446808510638,2(At+1 −(At+1 −At)2) ∥∇f(yt)∥2 .
T DT,0.4968085106382979,Then we complete the proof noting that it is easy to verify
T DT,0.4978723404255319,At+1 −(At+1 −At)2 = Bt+1 + 1
T DT,0.498936170212766,η −(Bt+1 −Bt)2 = 1 η ≥0.
T DT,0.5,"In the next lemma, we show that if ∥∇f(yt)∥≤G, then the condition (20) assumed in Lemma C.4
is satisfied at time t.
Lemma C.5. For any t ≥0, if ∥∇f(yt)∥≤G, then we have ∥∇f(xt+1)∥≤G, and furthermore,"
T DT,0.5010638297872341,"f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ 1"
T DT,0.502127659574468,2η ∥xt+1 −yt∥2 ≥f(xt+1).
T DT,0.5031914893617021,"Proof of Lemma C.5. As disccued below Theorem C.1, the stepsize satisfies η ≤1/(2L) ≤
min{
2
m(G), r(G)"
T DT,0.5042553191489362,"2G }. Therefore we can apply Lemma 4.1 to show ∥∇f(xt+1)∥≤∥∇f(yt)∥≤G. For
the second part, note that ∥xt+1 −yt∥= η ∥∇f(yt)∥≤
G
2L ≤r(G), we can apply Lemma 3.3 to
show"
T DT,0.5053191489361702,"f(xt+1) ≤f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ L"
T DT,0.5063829787234042,2 ∥xt+1 −yt∥2
T DT,0.5074468085106383,"≤f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ 1"
T DT,0.5085106382978724,2η ∥xt+1 −yt∥2 .
T DT,0.5095744680851064,"With Lemma C.4 and Lemma C.5, we can show that ∥∇f(yt)∥≤G for all t ≥0, as in the lemma
below.
Lemma C.6. For all t ≥0, ∥∇f(yt)∥≤G."
T DT,0.5106382978723404,"Proof of Lemma C.6. We will prove this lemma by induction. First, by Lemma 3.5 and the choice of
G, it is easy to verify that ∥∇f(x0)∥≤G. Then for any fixed t ≥0, suppose that ∥∇f(xs)∥≤G
for all s < t. Then by Lemma C.4 and Lemma C.5, we know that ∥∇f(xs)∥≤G for all 0 ≤s ≤t,
and that for all s < t,"
T DT,0.5117021276595745,As+1(f(xs+1) −f ∗) + 1
T DT,0.5127659574468085,2η ∥zs+1 −x∗∥2 ≤As(f(xs) −f ∗) + 1
T DT,0.5138297872340426,"2η ∥zs −x∗∥2 .
(24)"
T DT,0.5148936170212766,"By telescoping (24), we have for all 0 ≤s < t,"
T DT,0.5159574468085106,"f(xs+1) −f ∗≤
1
ηAs+1
((f(x0) −f ∗) + ∥z0 −x∗∥2).
(25)"
T DT,0.5170212765957447,"For 0 ≤s ≤t, since ∥∇f(xs)∥≤G, then Lemma 3.5 implies"
T DT,0.5180851063829788,"∥∇f(xs)∥2 ≤2L(f(xs) −f ∗).
(26)"
T DT,0.5191489361702127,"Note that by Algorithm 1, we have"
T DT,0.5202127659574468,zt −xt = At−1
T DT,0.5212765957446809,"At
(zt−1 −xt−1) −η(At −At−1)∇f(yt−1) + η∇f(yt−1)."
T DT,0.5223404255319148,Thus we can obtain
T DT,0.5234042553191489,"zt −xt = −1 At t−1
X"
T DT,0.524468085106383,"s=1
ηAs+1(As+1 −As −1)∇f(ys)."
T DT,0.5255319148936171,Therefore
T DT,0.526595744680851,"yt −xt = −(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5276595744680851,"s=1
ηAs+1(As+1 −As −1)∇f(ys)."
T DT,0.5287234042553192,Thus we have
T DT,0.5297872340425532,"∥yt −xt∥≤(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5308510638297872,"s=1
ηAs+1(As+1 −As −1) ∥∇f(ys)∥=: I."
T DT,0.5319148936170213,"Since ∥∇f(ys)∥≤G and ∥xs+1 −ys∥= ∥η∇f(ys)∥≤r(G) for s < t, by Lemma 3.3, we have"
T DT,0.5329787234042553,"I ≤(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5340425531914894,"s=1
ηAs+1(As+1 −As −1)(∥∇f(xs+1)∥+ ηL ∥∇f(ys)∥)"
T DT,0.5351063829787234,"≤ηLI + (1 −
At
At+1
) 1 At t−1
X"
T DT,0.5361702127659574,"s=1
ηAs+1(As+1 −As −1) ∥∇f(xs+1)∥."
T DT,0.5372340425531915,"Thus
∥yt −xt∥"
T DT,0.5382978723404256,"≤I ≤
1
1 −ηL(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5393617021276595,"s=1
ηAs+1(As+1 −As −1) ∥∇f(xs+1)∥"
T DT,0.5404255319148936,"≤
1
1 −ηL(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5414893617021277,"s=1
ηAs+1(As+1 −As −1)
p"
T DT,0.5425531914893617,"2L(f(xs+1) −f ∗)
(by (26))"
T DT,0.5436170212765957,"≤
1
1 −ηL(1 −
At
At+1
) 1 At t−1
X"
T DT,0.5446808510638298,"s=1
ηAs+1(As+1 −As −1) s"
L,0.5457446808510639,"2L
As+1
· 1"
L,0.5468085106382978,η ((f(x0) −f ∗) + ∥z0 −x∗∥2)
L,0.5478723404255319,(by (25))
L,0.548936170212766,= 2√ηL
L,0.55,"1 −ηL(1 −
At
At+1
) 1 At t−1
X s=1 p"
L,0.551063829787234,"As+1(As+1 −As −1)
q"
L,0.5521276595744681,(f(x0) −f ∗) + ∥z0 −x∗∥2 ≤8√η 1 −ηL q
L,0.5531914893617021,"L((f(x0) −f ∗) + ∥z0 −x∗∥2)
(by Lemma C.3)"
L,0.5542553191489362,"≤
1
2L3/2−1/α · L1/2−1/αG = G"
L,0.5553191489361702,"2L ≤r(G).
(by the choices of η and G)"
L,0.5563829787234043,"Since ∥∇f(xt)∥≤G and we just showed ∥xt −yt∥≤r(G), by Lemma 3.3, we have
∥∇f(yt)∥≤∥∇f(xt)∥+ L ∥yt −xt∥ ≤ s"
L,0.5574468085106383,"2L
ηAt
((f(x0) −f ∗) + ∥z0 −x∗∥2) + L · G"
L,0.5585106382978723,"2L
(by (26) and (25)) ≤G
1 4 + 1 2"
L,0.5595744680851064,"
≤G.
(by At ≥1/η and choice of G)"
L,0.5606382978723404,Then we complete the induction as well as the proof.
L,0.5617021276595745,"With the three lemmas above, it is straight forward to prove Theorem C.1."
L,0.5627659574468085,"Proof of Theorem C.1. Combining Lemmas C.4, C.5, and C.6, we know the following inequality
holds for all t ≥0."
L,0.5638297872340425,At+1(f(xt+1) −f ∗) + 1
L,0.5648936170212766,2η ∥zt+1 −x∗∥2 ≤At(f(xt) −f ∗) + 1
L,0.5659574468085107,"2η ∥zt −x∗∥2 ,"
L,0.5670212765957446,"Then by telescoping, we directly complete the proof."
L,0.5680851063829787,"D
Analysis of NAG for strongly convex functions"
L,0.5691489361702128,"In this section, we provide the convergence analysis of the modified version of Nesterov’s accelerated
gradient method for µ-strongly-convex functions defined in Algorithm 2."
L,0.5702127659574469,"The convergence results is formally presented in the following theorem.
Theorem D.1. Suppose f is µ-strongly-convex and ℓ-smooth. For α ∈(0, 2], if ℓ(u) = o(uα), i.e.,
limu→∞ℓ(u)/uα = 0, then there must exist a constant G such that for L := ℓ(2G), we have"
L,0.5712765957446808,"G ≥8 max{L1/α−1/2, 1}
q"
L,0.5723404255319149,"L((f(x0) −f ∗) + µ ∥z0 −x∗∥2)/ min{µ, 1}.
(27)"
L,0.573404255319149,If we choose
L,0.574468085106383,"η ≤min 
 
1"
L,0.575531914893617,"144L3−2/α log4
e + 144L3−2/α"
L,0.5765957446808511,"µ
, 1"
L,0.5776595744680851,"2L 
"
L,0.5787234042553191,".
(28)"
L,0.5797872340425532,Algorithm 2: NAG for µ-strongly-convex functions
L,0.5808510638297872,"input A µ-strongly-convex and ℓ-smooth function f, stepsize η, initial point x0"
L,0.5819148936170213,"1: Initialize z0 = x0, B0 = 0, and A0 = 1/(ηµ).
2: for t = 0, ... do"
L,0.5829787234042553,"3:
Bt+1 =
2Bt+1+√"
L,0.5840425531914893,"4Bt+4ηµB2
t +1
2(1−ηµ)
4:
At+1 = Bt+1 +
1
ηµ
5:
τt =
(At+1−At)(1+ηµAt)
At+1+2ηµAtAt+1−ηµA2
t and δt =
At+1−At
1+ηµAt+1
6:
yt = xt + τt(zt −xt)
7:
xt+1 = yt −η∇f(yt)
8:
zt+1 = (1 −ηµδt)zt + ηµδtyt −ηδt∇f(yt)
9: end for"
L,0.5851063829787234,The iterates generated by Algorithm 2 satisfy
L,0.5861702127659575,f(xT ) −f ∗≤(1 −√ηµ)T −1(f(x0 −f ∗) + µ ∥z0 −x∗∥2)
L,0.5872340425531914,"ηµ + (1 −√ηµ)T −1
."
L,0.5882978723404255,"The above theorem gives a gradient complexity of O

1
√ηµ log(1/ϵ)

. Note that Theorem 4.2 shows"
L,0.5893617021276596,"the complexity of GD is O

1
ηµ log(1/ϵ)

. It seems NAG gives a better rate at first glance. However,
note that the choices of G, L, η in these two theorems are different, it is less clear whether NAG
accelerates the optimization in this setting. Below, we informally show that, if ℓ(u) = o(√u), the
rate we obtain for NAG is faster than that for GD."
L,0.5904255319148937,"For simplicity, we informally assume ℓ(u) ≍xρ with ρ ∈(0, 1). Let G0 = ∥∇f(x0)∥. Then for
GD, by Theorem 4.2, we have ηgdµ ≍µ/ℓ(G0) ≍µ/Gρ
0. For NAG, since ℓis sub-linear we can
choose α = 1 in the theorem statement. Since f is µ-strongly-convex, by standard results, we
can show that f(x0) −f ∗≤1"
L,0.5914893617021276,"µG2
0 and ∥z0 −x∗∥≤1"
L,0.5925531914893617,"µG0. Thus the requirement of G in (27) can
be simplified as G ≳ℓ(G) · G0/µ, which is satisfied if choosing G ≍(G0/µ)1/(1−ρ). Then we
also have ηnag ≍
1
ℓ(G) ≍(µ/G0)ρ/(1−ρ). Thus √ηnagµ ≍(µ/Gρ
0)1/(2−2ρ). This means whenever
1/(2 −2ρ) < 1, i.e., 0 ≤ρ < 1/2, we have √ηnagµ ≳ηgdµ, which implies the rate we obtain for
NAG is faster than that for GD."
L,0.5936170212765958,"In what follows, we will provide the proof of Theorem D.1. We will always use the parameter choices
in the theorem throughout this section."
L,0.5946808510638298,"D.1
Useful lemmas"
L,0.5957446808510638,"In this part, we provide several useful lemmas for proving Theorem D.1. To start with, the following
two lemmas provide two useful inequalities."
L,0.5968085106382979,"Lemma D.2. For any 0 ≤u ≤1, we have log(1 + u) ≥1 2u."
L,0.597872340425532,"Lemma D.3. For all 0 < p ≤1 and t ≥0, we have"
L,0.5989361702127659,"t ≤
2
√p log(e + 1"
L,0.6,p)(p(1 + √p)t + 1).
L,0.601063829787234,Proof of Lemma D.3. Let
L,0.6021276595744681,"f(t) =
2
√p log(e + 1"
L,0.6031914893617021,p)(p(1 + √p)t + 1) −t.
L,0.6042553191489362,"It is obvious that f(t) ≥0 for t ≤
2
√p log(e + 1"
L,0.6053191489361702,"p). For t >
2
√p log(e + 1"
L,0.6063829787234043,"p), we have"
L,0.6074468085106383,f ′(t) = 2√p log(e + 1
L,0.6085106382978723,p) log(1 + √p)(1 + √p)t −1
L,0.6095744680851064,"≥p(1 + √p)t −1
(by Lemma D.2)
= p exp(t log(1 + √p)) −1
≥p exp(t√p/2) −1
(by Lemma D.2)"
L,0.6106382978723405,"≥p(e + 1/p) −1 ≥0.
(since t >
2
√p log(e + 1 p))"
L,0.6117021276595744,Thus f is non-decreasing and
L,0.6127659574468085,"f(t) ≥f
 2
√p log(e + 1"
L,0.6138297872340426,"p)

≥0."
L,0.6148936170212767,"In the next four lemmas, we provide several useful inequalities regarding the weights {At}t≥0 and
{Bt}t≥0 used in Algorithm 2.
Lemma D.4. For all s ≤t, we have
Bt+1 −Bt"
L,0.6159574468085106,"Bt+1
· Bs+1 −Bs"
L,0.6170212765957447,"1 + ηµBs+1
≤1,"
L,0.6180851063829788,which implies τt · δs ≤1.
L,0.6191489361702127,"Proof of Lemma D.4. By Algorithm 2, it is easy to verify"
L,0.6202127659574468,(Bs+1 −Bs)2 = Bs+1(1 + ηµBs+1).
L,0.6212765957446809,This implies
L,0.6223404255319149,"Bs = Bs+1 −
p"
L,0.6234042553191489,Bs+1(1 + ηµBs+1). Thus
L,0.624468085106383,"Bt
Bt+1
= 1 − s"
L,0.625531914893617,"ηµ +
1
Bt+1
≥1 − s"
L,0.6265957446808511,"ηµ +
1
Bs+1
=
Bs
Bs+1
,"
L,0.6276595744680851,"where in the inequality, we use the fact that Bs is non-decreasing with s. Therefore
Bt+1 −Bt"
L,0.6287234042553191,"Bt+1
· Bs+1 −Bs"
L,0.6297872340425532,"1 + ηµBs+1
≤Bs+1 −Bs"
L,0.6308510638297873,"Bs+1
· Bs+1 −Bs"
L,0.6319148936170212,"1 + ηµBs+1
= 1."
L,0.6329787234042553,Thus we have
L,0.6340425531914894,"τt · δs =
(At+1 −At)(1 + ηµAt)
At+1 + 2ηµAtAt+1 −ηµA2
t
· As+1 −As"
L,0.6351063829787233,1 + ηµAs+1
L,0.6361702127659574,≤At+1 −At
L,0.6372340425531915,"At+1
· As+1 −As"
L,0.6382978723404256,"1 + ηµAs+1
(by At+1 ≥At)"
L,0.6393617021276595,= Bt+1 −Bt
L,0.6404255319148936,"At+1
· Bs+1 −Bs"
L,0.6414893617021277,"1 + ηµAs+1
(by As+1 −As = Bs+1 −Bs)"
L,0.6425531914893617,≤Bt+1 −Bt
L,0.6436170212765957,"Bt+1
· Bs+1 −Bs"
L,0.6446808510638298,"1 + ηµBs+1
≤1.
(by As+1 ≥Bs+1)"
L,0.6457446808510638,"Lemma D.5. If 0 < ηµ < 1, then for any t ≥1, we have
Bt
1 −√ηµ ≤Bt+1 ≤
3Bt
1 −ηµ. Thus"
L,0.6468085106382979,"Bt ≥
1
(1 −√ηµ)t−1 ≥(1 + √ηµ)t−1."
L,0.6478723404255319,"Proof of Lemma D.5. For t ≥1, we have Bt ≥1 thus"
L,0.648936170212766,"Bt+1 = 2Bt + 1 +
p"
L,0.65,"4Bt + 4ηµB2
t + 1
2(1 −ηµ)
≤2Bt + 1"
L,0.6510638297872341,"1 −ηµ ≤
3Bt
1 −µη ."
L,0.652127659574468,"On the other hand, we have"
L,0.6531914893617021,"Bt+1 = 2Bt + 1 +
p"
L,0.6542553191489362,"4Bt + 4ηµB2
t + 1
2(1 −ηµ)"
L,0.6553191489361702,"≥2Bt +
p"
L,0.6563829787234042,(2Bt√ηµ)2
L,0.6574468085106383,2(1 −ηµ)
L,0.6585106382978724,"=
Bt
1 −√ηµ. Thus"
L,0.6595744680851063,"Bt ≥

1
1 −√ηµ"
L,0.6606382978723404,"t−1
B1 ≥

1
1 −√ηµ"
L,0.6617021276595745,"t−1
≥(1 + √ηµ)t−1."
L,0.6627659574468086,"Lemma D.6. For 0 < ηµ < 1 and t ≥1, we have t
X s=0 p"
L,0.6638297872340425,Bs ≤(1 −ηµ)Bt+1 ≤3Bt.
L,0.6648936170212766,Proof of Lemma D.6.
L,0.6659574468085107,"Bt+1 = 2Bt + 1 +
p"
L,0.6670212765957447,"4Bt + 4ηµB2
t + 1
2(1 −ηµ)"
L,0.6680851063829787,"≥Bt +
√Bt
1 −ηµ
≥· · · ≥ t
X s=0"
L,0.6691489361702128,"√Bs
1 −ηµ."
L,0.6702127659574468,"Combined with Lemma D.5, we have the desired result."
L,0.6712765957446809,"Lemma D.7. For t ≥1, we have t−1
X s=0 p"
L,0.6723404255319149,"As+1
At
≤3 + 4 log(e + 1 ηµ)."
L,0.6734042553191489,"Proof of Lemma D.7. By Lemma D.5, we have"
L,0.674468085106383,At = Bt + 1
L,0.675531914893617,ηµ ≥(1 + √ηµ)t−1 + 1
L,0.676595744680851,"ηµ.
(29)"
L,0.6776595744680851,"Thus, we have t−1
X s=0 p"
L,0.6787234042553192,"As+1
At
= t−1
X s=0 p"
L,0.6797872340425531,"Bs+1 + 1/(ηµ) At ≤ t−1
X s=0 p"
L,0.6808510638297872,"Bs+1
At
+
t
√ηµAt"
L,0.6819148936170213,"≤3 +
1
√ηµAt
·
2
√ηµ log(e + 1"
L,0.6829787234042554,ηµ)(ηµ(1 + √ηµ)t + 1)
L,0.6840425531914893,(by Lemma D.6 and Lemma D.3)
L,0.6851063829787234,≤3 + 4 log(e + 1
L,0.6861702127659575,"ηµ).
(by Inequality (29))"
L,0.6872340425531915,"D.2
Proof of Theorem D.1"
L,0.6882978723404255,"With all the useful lemmas in the previous section, we proceed to prove Theorem D.1, for which
we need several additional lemmas. First, similar to Lemma C.4, the following lemma summarizes
the results in the classical potential function analysis of NAG for strongly convex functions in
[d’Aspremont et al., 2021].
Lemma D.8. For any t ≥0, if the following inequality holds"
L,0.6893617021276596,"f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ 1"
L,0.6904255319148936,"2η ∥xt+1 −yt∥2 ≥f(xt+1),"
L,0.6914893617021277,then we can obtain
L,0.6925531914893617,At+1(f(xt+1) −f ∗) + 1 + ηµAt+1
L,0.6936170212765957,"2η
∥zt+1 −x∗∥2 ≤At(f(xt) −f ∗) + 1 + ηµAt"
L,0.6946808510638298,"2η
∥zt −x∗∥2 ."
L,0.6957446808510638,"Proof of Lemma D.8. These derivations can be found in d’Aspremont et al. [2021]. We present it
here for completeness."
L,0.6968085106382979,The strong convexity between x∗and yt gives
L,0.6978723404255319,"f ∗≥f(yt) + ⟨∇f(yt), x∗−yt⟩+ µ"
L,0.698936170212766,2 ∥x∗−yt∥2 .
L,0.7,The convexity between xt and yt gives
L,0.701063829787234,"f(xt) ≥f(yt) + ⟨∇f(yt), xt −yt⟩."
L,0.7021276595744681,"Combining the above two inequalities and the one assumed in this lemma, we have"
L,0.7031914893617022,"0 ≥(At+1 −At)(f ∗−f(yt) −⟨∇f(yt), x∗−yt⟩−µ"
L,0.7042553191489361,2 ∥x∗−yt∥2)
L,0.7053191489361702,"+ At(f(yt) −f(xt) −⟨∇f(yt), xt −yt⟩)"
L,0.7063829787234043,"+ At+1(f(xt+1) −f(yt) −⟨∇f(yt), xt+1 −yt⟩−1"
L,0.7074468085106383,2η ∥xt+1 −yt∥2).
L,0.7085106382978723,Reorganizing we can obtain
L,0.7095744680851064,At+1(f(xt+1) −f ∗) + 1 + ηµAt+1
L,0.7106382978723405,"2η
∥zt+1 −x∗∥2"
L,0.7117021276595744,≤At(f(xt) −f ∗) + 1 + ηµAt
L,0.7127659574468085,"2η
∥zt −x∗∥2"
L,0.7138297872340426,"+ (At −At+1)2 −At+1 −ηµA2
t+1
1 + ηµAt+1"
L,0.7148936170212766,"η
2 ∥∇f(yt)∥2"
L,0.7159574468085106,"−A2
t
(At+1 −At)(1 + ηµAt)(1 + ηµAt+1)"
L,0.7170212765957447,"(At+1 + 2ηµAtAt+1 −ηµA2
t)2
µ"
L,0.7180851063829787,2 ∥xt −zt∥2 .
L,0.7191489361702128,Then we complete the proof noting that
L,0.7202127659574468,"(At −At+1)2 −At+1 −ηµA2
t+1"
L,0.7212765957446808,= (Bt −Bt+1)2 −Bt+1 + 1
L,0.7223404255319149,ηµ −ηµ(Bt+1 + 1/(ηµ))2
L,0.723404255319149,"= ηµB2
t+1 + 1"
L,0.7244680851063829,"ηµ −ηµB2
t+1 −2Bt+1 −1"
L,0.725531914893617,"ηµ
= −2Bt+1 ≤0."
L,0.7265957446808511,"Next, note that Lemma C.5 still holds in the strongly convex setting. We repeat it below for
completeness.
Lemma D.9. For any t ≥0, if ∥∇f(yt)∥≤G, then we have ∥∇f(xt+1)∥≤G, and furthermore,"
L,0.7276595744680852,"f(yt) + ⟨∇f(yt), xt+1 −yt⟩+ 1"
L,0.7287234042553191,2η ∥xt+1 −yt∥2 ≥f(xt+1).
L,0.7297872340425532,"With Lemma D.8 and Lemma D.9, we will show that ∥∇f(yt)∥≤G for all t ≥0 by induction in
the following lemma."
L,0.7308510638297873,"Lemma D.10. For all t ≥0, we have ∥∇f(yt)∥≤G."
L,0.7319148936170212,"Proof of Lemma D.10. We will prove this lemma by induction. First, by Lemma 3.5 and the choice
of G, it is easy to verify that ∥∇f(x0)∥≤G. Then for any fixed t ≥0, suppose that ∥∇f(xs)∥≤G
for all s < t. Then by Lemma D.8 and Lemma D.9, we know that ∥∇f(xs)∥≤G for all 0 ≤s ≤t,
and that for all s < t,"
L,0.7329787234042553,As+1(f(xs+1) −f ∗) + 1 + ηµAs+1
L,0.7340425531914894,"2η
∥zs+1 −x∗∥2 ≤As(f(xs) −f ∗) + 1 + ηµAs"
L,0.7351063829787234,"2η
∥zs −x∗∥2 . (30)"
L,0.7361702127659574,"By telescoping (30), we have for all 0 ≤s < t,"
L,0.7372340425531915,"f(xs+1) −f ∗≤
1
As+1ηµ(f(x0) −f ∗+ µ ∥z0 −x∗∥2).
(31)"
L,0.7382978723404255,"For 0 ≤s ≤t, since ∥∇f(xs)∥≤G, then Lemma 3.5 implies"
L,0.7393617021276596,"∥∇f(xs)∥2 ≤2L(f(xs) −f ∗).
(32)"
L,0.7404255319148936,"Note that by Algorithm 2, we have"
L,0.7414893617021276,zt −xt = (1 −ηµδt−1)(1 −τt−1)(zt−1 −xt−1) + η(1 −δt−1)∇f(yt−1). Thus
L,0.7425531914893617,"zt −xt = η t−1
X"
L,0.7436170212765958,"s=0
(1 −δs)∇f(ys) t−1
Y"
L,0.7446808510638298,"i=s+1
(1 −ηµδi)(1 −τi)."
L,0.7457446808510638,Therefore
L,0.7468085106382979,"yt −xt = ητt t−1
X"
L,0.747872340425532,"s=0
(1 −δs)∇f(ys) t−1
Y"
L,0.7489361702127659,"i=s+1
(1 −ηµδi)(1 −τi)."
L,0.75,Moreover
L,0.7510638297872341,1 −ηµδi = 1 −ηµ(Ai+1 −Ai)
L,0.752127659574468,"1 + ηµAi+1
=
1 + ηµAi
1 + ηµAi+1 and"
L,0.7531914893617021,"1 −τi = 1 −
(Ai+1 −Ai)(1 + ηµAi)
Ai+1 + 2ηµAiAi+1 −ηµA2
i
=
Ai(1 + ηµAi+1)
Ai+1 + 2ηµAiAi+1 −ηµA2
i
≤Ai(1 + ηµAi+1)"
L,0.7542553191489362,Ai+1(1 + ηµAi).
L,0.7553191489361702,Thus we have
L,0.7563829787234042,"∥yt −xt∥≤ητt t−1
X"
L,0.7574468085106383,"s=0
(δs −1)As+1"
L,0.7585106382978724,"At
∥∇f(ys)∥≤η t−1
X s=0 As+1"
L,0.7595744680851064,"At
∥∇f(ys)∥=: I,"
L,0.7606382978723404,"where the second inequality follows from Lemma D.4. We further control term I by I ≤η t−1
X s=0 As+1"
L,0.7617021276595745,"At
(∥∇f(xs+1)∥+ ηL ∥∇f(ys)∥)"
L,0.7627659574468085,"≤ηLI + η t−1
X s=0 As+1"
L,0.7638297872340426,"At
∥∇f(xs+1)∥."
L,0.7648936170212766,Thus we have
L,0.7659574468085106,"∥yt −xt∥≤
η
1 −ηL t−1
X s=0 As+1"
L,0.7670212765957447,"At
∥∇f(xs+1)∥"
L,0.7680851063829788,"≤
η
1 −ηL t−1
X s=0 As+1 At p"
L,0.7691489361702127,"2L(f(xs+1) −f ∗)
(by (32))"
L,0.7702127659574468,"≤
η
1 −ηL t−1
X s=0 As+1 At s"
L,0.7712765957446809,"2L ·
1
As+1ηµ(f(x0) −f ∗+ µ ∥z0 −x∗∥2)
(by (31)) = q"
L,0.7723404255319148,2ηL(f(x0) −f ∗+ µ ∥z0 −x∗∥2)
L,0.7734042553191489,"(1 −ηL)√µ t−1
X s=0 p"
L,0.774468085106383,"As+1
At ≤ q"
L,0.7755319148936171,2ηL(f(x0) −f ∗+ µ ∥z0 −x∗∥2)
L,0.776595744680851,(1 −ηL)√µ
L,0.7776595744680851,"
3 + 4 log(e + 1"
L,0.7787234042553192,"ηµ)

.
(by Lemma D.7)"
L,0.7797872340425532,"≤
√η
1 −ηL"
L,0.7808510638297872,"
3 + 4 log(e + 1"
L,0.7819148936170213,"ηµ)

· G · L1/2−1/α"
L,0.7829787234042553,"4
(by (27))"
L,0.7840425531914894,"≤
3 + 4 log(e +
1
ηµ)"
L,0.7851063829787234,"log2
e + 144L3−2/α"
L,0.7861702127659574,"µ
 · G"
L,0.7872340425531915,"24L
(by (28)) ≤G"
L,0.7882978723404256,2L ≤r(G).
L,0.7893617021276595,"Since ∥∇f(xt)∥≤G and we just showed ∥xt −yt∥≤r(G), by Lemma 3.3, we have"
L,0.7904255319148936,∥∇f(yt)∥≤∥∇f(xt)∥+ L ∥yt −xt∥ ≤ s
L,0.7914893617021277,"2L
ηµAt
((f(x0) −f ∗) + µ ∥z0 −x∗∥2) + L · G"
L,0.7925531914893617,"2L
(by (31)) ≤G
1 4 + 1 2"
L,0.7936170212765957,"
≤G.
(by At ≥1/(ηµ) and (27))"
L,0.7946808510638298,Then we complete the induction as well as the proof.
L,0.7957446808510639,"Proof of Theorem D.1. Combining Lemmas D.8, D.9, and D.10, we know the following inequality
holds for all t ≥0."
L,0.7968085106382978,At+1(f(xt+1) −f ∗)+ 1 + ηµAt+1
L,0.7978723404255319,"2η
∥zt+1 −x∗∥2 ≤At(f(xt) −f ∗)+ 1 + ηµAt"
L,0.798936170212766,"2η
∥zt −x∗∥2 ."
L,0.8,"Then by telescoping, we get"
L,0.801063829787234,At(f(xt) −f ∗)+ 1 + ηµAt
L,0.8021276595744681,"2η
∥zt −x∗∥2 ≤A0(f(x0) −f ∗)+ 1 + ηµA0"
L,0.8031914893617021,"2η
∥z0 −x∗∥2 ."
L,0.8042553191489362,"Finally, applying Lemma D.5, we have At = Bt + 1/(ηµ) ≥1/(1 −√ηµ)t−1 + 1/(ηµ). Thus
completes the proof."
L,0.8053191489361702,"E
Analysis of GD for non-convex functions"
L,0.8063829787234043,"In this section, we provide the proofs related to analysis of gradient descent for non-convex function,
including those of Lemma 5.1 and Theorem 5.2."
L,0.8074468085106383,"Proof of Lemma 5.1. First, based on Corollary 3.6, we know ∥∇f(x)∥≤G < ∞. Also note that
x+ −x
 = ∥η∇f(x)∥≤ηG ≤G/L."
L,0.8085106382978723,"Then by Lemma 3.3 and Remark 3.4, we have x+ ∈X and"
L,0.8095744680851064,"f(x+) ≤f(x) +

∇f(xt), x+ −x

+ L 2"
L,0.8106382978723404,"x+ −x
2"
L,0.8117021276595745,=f(x) −η(1 −ηL/2) ∥∇f(x)∥2
L,0.8127659574468085,≤f(x).
L,0.8138297872340425,"Proof of Theorem 5.2. By Lemma 5.1, using induction, we directly obtain f(xt) ≤f(x0) for all
t ≥0. Then by Corollary 3.6, we have ∥∇f(xt)∥≤G for all t ≥0. Following the proof of
Lemma 5.1, we can similarly show"
L,0.8148936170212766,f(xt+1) −f(xt) ≤η(1 −ηL/2) −η
L,0.8159574468085107,2 ∥∇f(xt)∥2 ≤−η
L,0.8170212765957446,2 ∥∇f(xt)∥2 .
L,0.8180851063829787,"Taking a summation over t < T and rearanging terms, we have"
T,0.8191489361702128,"1
T X"
T,0.8202127659574469,"t<T
∥∇f(xt)∥2 ≤2(f(x0) −f(xT ))"
T,0.8212765957446808,"ηT
≤2(f(x0) −f ∗) ηT
."
T,0.8223404255319149,"F
Analysis of SGD for non-convex functions"
T,0.823404255319149,"In this section, we provide the detailed convergence analysis of stochastic gradient descent for
ℓ-smooth and non-convex functions where ℓis sub-quadratic."
T,0.824468085106383,We first present some useful inequalities related to the parameter choices in Theorem 5.3.
T,0.825531914893617,"Lemma F.1. Under the parameters choices in Theorem 5.3, the following inequalities hold. ηG
√"
T,0.8265957446808511,"2T ≤1/2,
η2σLT ≤1/2,
100η2Tσ2L2 ≤δG2."
T,0.8276595744680851,"Proof of Lemma F.1. First note that by Corollary 3.6, we know"
T,0.8287234042553191,"G2 = 2LF = 16L(f(x0) −f ∗+ σ)/δ ≥16Lσ/δ,"
T,0.8297872340425532,"i.e., σL ≤G2δ/16. Then since we choose η ≤
1
4G
√"
T,0.8308510638297872,"T , we have ηG
√"
T,0.8319148936170213,"2T ≤
√"
T,0.8329787234042553,"2/4 ≤1/2,"
T,0.8340425531914893,"η2σLT ≤η2TG2δ/16 ≤δ/256 ≤1/2,"
T,0.8351063829787234,100η2Tσ2L2 ≤100η2TG4δ2/256 ≤δG2.
T,0.8361702127659575,"Next, we show the useful lemma which bounds E[f(xτ) −f ∗] and E
hP"
T,0.8372340425531914,t<τ ∥∇f(xt)∥2i
T,0.8382978723404255,simultaneously.
T,0.8393617021276596,"Lemma F.2. Under the parameters choices in Theorem 5.3, the following inequality holds E """
T,0.8404255319148937,f(xτ) −f ∗+ η 2 X
T,0.8414893617021276,"t<τ
∥∇f(xt)∥2
#"
T,0.8425531914893617,≤f(x0) −f ∗+ σ.
T,0.8436170212765958,"Proof of Lemma F.2. If t < τ, by the definition of τ, we know f(xt) −f ∗≤F and ∥ϵt∥≤
G
5ηL,
and the former also implies ∥∇f(xt)∥≤G by Corollary 3.6. Then we can bound"
T,0.8446808510638298,"∥xt+1 −xt∥= η ∥gt∥≤η(∥∇f(xt)∥+ ∥ϵt∥) ≤ηG + G 5L ≤G L ,"
T,0.8457446808510638,"where we use the choice of η ≤
1
2L. Then based on Lemma 3.3 and Remark 3.4, for any t < τ, we
have"
T,0.8468085106382979,"f(xt+1) −f(xt) ≤

∇f(xt), xt+1 −xt

+ L"
T,0.847872340425532,2 ∥xt+1 −xt∥2
T,0.8489361702127659,"= −η

∇f(xt), gt

+ η2L"
T,0.85,"2
∥gt∥2"
T,0.851063829787234,"≤−η ∥∇f(xt)∥2 −η

∇f(xt), ϵt

+ η2L ∥∇f(xt)∥2 + η2L ∥ϵt∥2 ≤−η"
T,0.8521276595744681,"2 ∥∇f(xt)∥2 −η

∇f(xt), ϵt

+ η2L ∥ϵt∥2 ,
(33)"
T,0.8531914893617021,"where the equality is due to (4); the second inequality uses gt = ϵt + ∇f(xt) and Young’s inequality
∥y + z∥2 ≤2 ∥y∥2 +2 ∥z∥2 for any vectors y, z; and the last inequality chooses η ≤1/(2L). Taking
a summation over t < τ and rearanging terms, we have"
T,0.8542553191489362,f(xτ) −f ∗+ η 2 X
T,0.8553191489361702,"t<τ
∥∇f(xt)∥2 ≤f(x0) −f ∗−η
X t<τ"
T,0.8563829787234043,"∇f(xt), ϵt

+ η2L
X"
T,0.8574468085106383,"t<τ
∥ϵt∥2 ."
T,0.8585106382978723,"Now we bound the last two terms on th RHS. First, for the last term, we have E ""X"
T,0.8595744680851064,"t<τ
∥ϵt∥2
# ≤E ""X"
T,0.8606382978723405,"t<T
∥ϵt∥2
# ≤σ2T,"
T,0.8617021276595744,where the first inequality uses τ ≤T by its defnition; and in the last inequality we use Assumption 4.
T,0.8627659574468085,"For the cross term, note that Et−1

∇f(xt), ϵt

= 0 by Assumption 4. So this term is a sum of
a martingale difference sequence. Since τ is a stopping time, we can apply the optional stopping
theorem to obtain E  X t≤τ"
T,0.8638297872340426,"∇f(xt), ϵt

"
T,0.8648936170212767,"= 0.
(34)"
T,0.8659574468085106,"Then we have E "" −
X t<τ"
T,0.8670212765957447,"∇f(xt), ϵt

#"
T,0.8680851063829788,"=E

∇f(xτ), ϵτ

≤G E[∥ϵτ∥] ≤G
q"
T,0.8691489361702127,E[∥ϵτ∥2] ≤G
T,0.8702127659574468,"v
u
u
u
tE  X"
T,0.8712765957446809,"t≤T
∥ϵt∥2 "
T,0.8723404255319149,"≤σG
√"
T,0.8734042553191489,"T + 1 ≤σG
√ 2T,"
T,0.874468085106383,"where the equality is due to (34); the first inequality uses ∥∇f(xτ)∥≤G by the definition of τ in (5)
and Corollary 3.6; the fourth inequality uses E[X]2 ≤E[X2] for any random variable X; and the
last inequality uses Assumption 4."
T,0.875531914893617,"Combining all the bounds above, we get E """
T,0.8765957446808511,f(xτ) −f ∗+ η 2 X
T,0.8776595744680851,"t<τ
∥∇f(xt)∥2
#"
T,0.8787234042553191,"≤f(x0) −f ∗+ ησG
√"
T,0.8797872340425532,2T + η2σ2LT
T,0.8808510638297873,"≤f(x0) −f ∗+ σ,"
T,0.8819148936170212,where the last inequality is due to Lemma F.1.
T,0.8829787234042553,"With Lemma F.2, we are ready to prove Theorem 5.3."
T,0.8840425531914894,"Proof of Theorem 5.3. We want to show the probability of {τ < T} is small, as its complement
{τ = T} means f(xt) −f ∗≤F for all t ≤T which implies ∥∇f(xt)∥≤G for all t ≤T. Note
that"
T,0.8851063829787233,"{τ < T} = {τ2 < T} ∪{τ1 < T, τ2 = T}."
T,0.8861702127659574,Therefore we only need to bound the probability of each of these two events on the RHS.
T,0.8872340425531915,We first bound P(τ2 < T). Note that
T,0.8882978723404256,P(τ2 < T) =P [ t<T
T,0.8893617021276595,"
∥ϵt∥>
G
5ηL ! ≤
X"
T,0.8904255319148936,"t<T
P

∥ϵt∥>
G
5ηL "
T,0.8914893617021277,≤25η2Tσ2L2
T,0.8925531914893617,"G2
≤δ/4,"
T,0.8936170212765957,"where the first inequality uses union bound; the second inequality applies Chebyshev’s inequality
and E[∥ϵt∥2] = E[Et−1[∥ϵt∥2]] ≤σ2 for each fixed t by Assumption 4; the last inequality uses
Lemma F.1."
T,0.8946808510638298,"Next, we will bound P(τ1 < T, τ2 = T). Note that under the event {τ1 < T, τ2 = T}, we know that
1) τ = τ1 < T which implies f(xτ+1) −f ∗> F; and 2) τ < T = τ2 which implies ∥ϵτ∥≤
G
5ηL by
the definition in (5). Also note that we always have f(xτ) −f ∗≤F which implies ∥∇f(xτ)∥≤G
by Corollary 3.6. Then we can show"
T,0.8957446808510638,"∥xτ+1 −xτ∥= η ∥gτ∥≤η(∥∇f(xτ)∥+ ∥ϵτ∥) ≤ηG + G 5L ≤G L ,"
T,0.8968085106382979,"where we choose η ≤
1
2L. Then based on Lemma 3.3 and Remark 3.4, we have"
T,0.8978723404255319,f(xτ+1) −f(xτ) ≤−η
T,0.898936170212766,"2 ∥∇f(xτ)∥2 −η

∇f(xτ), ϵτ

+ η2L ∥ϵτ∥2"
T,0.9,≤η ∥∇f(xτ)∥· ∥ϵτ∥+ η2L ∥ϵτ∥2 ≤G2
L,0.9010638297872341,"4L =F 2 ,"
L,0.902127659574468,"where the first inequality is obtained following the same derivation as in (33); the last equality is due
to Corollary 3.6. Therefore we can show that under the event {τ1 < T, τ2 = T},"
L,0.9031914893617021,f(xτ) −f ∗= f(xτ) −f(xτ+1) + f(xτ+1) −f ∗> F/2.
L,0.9042553191489362,"Hence,"
L,0.9053191489361702,"P(τ1 < T, τ2 = T) ≤P (f(xτ) −f ∗> F/2) ≤E[f(xτ) −f ∗]"
L,0.9063829787234042,"F/2
≤2(f(x0) −f ∗+ σ)"
L,0.9074468085106383,"F
= δ/4,"
L,0.9085106382978724,"where the second inequality uses Markov’s inequality; the third inequality uses Lemma F.2; and in
the last inequality we choose F = 8(f(x0) −f ∗+ σ)/δ."
L,0.9095744680851063,Therefore we can show
L,0.9106382978723404,"P(τ < T) ≤P(τ2 < T) + P(τ1 < T, τ2 = T) ≤δ/2."
L,0.9117021276595745,"Then we also know P(τ = T) ≥1 −δ/2 ≥1/2. Therefore, by Lemma F.2,"
L,0.9127659574468086,"2(f(x0) −f ∗+ σ) η
≥E ""X"
L,0.9138297872340425,"t<τ
∥∇f(xt)∥2
#"
L,0.9148936170212766,"≥P(τ = T)E "" X"
L,0.9159574468085107,"t<T
∥∇f(xt)∥2
 τ = T # ≥1"
E,0.9170212765957447,"2E "" X"
E,0.9180851063829787,"t<T
∥∇f(xt)∥2
 τ = T # ."
E,0.9191489361702128,"Then we have E ""
1
T X"
E,0.9202127659574468,"t<T
∥∇f(xt)∥2
 τ = T #"
E,0.9212765957446809,≤4(f(x0) −f ∗+ σ)
E,0.9223404255319149,"ηT
= δF"
E,0.9234042553191489,2ηT ≤δ
E,0.924468085106383,"2 · ϵ2,"
E,0.925531914893617,"where the last inequality uses the choice of T. Let E := { 1 T
P"
E,0.926595744680851,"t<T ∥∇f(xt)∥2 > ϵ2} denote the
event of not converging to an ϵ-stationary point. By Markov’s inequality, we have P(E) ≤δ/2.
Therefore we have P({τ < T} ∪E) ≤δ, which completes the proof."
E,0.9276595744680851,"G
Lower bound"
E,0.9287234042553192,"In this section, we provide the proof of Theorem 5.4."
E,0.9297872340425531,"Proof of Theorem 5.4. Let c, η0 > 0 satisfy η0 ≤c2/2. Consider"
E,0.9308510638297872,"f(x) = 
 "
E,0.9319148936170213,"log(|x| −c),
|x| ≥y
2 log(y −c) −log(2y −|x| −c),
c/2 ≤|x| < y
kx2 + b,
|x| < c/2,"
E,0.9329787234042554,"where c > 0 is a constant and y = (c +
p"
E,0.9340425531914893,c2 + 2η0)/2 > 0 is the fixed point of the iteration
E,0.9351063829787234,"xt+1 =
xt −
η0
xt −c ,"
E,0.9361702127659575,"and k, b are chosen in such a way that f(x) and f ′(x) are continuous. Specifically, choose k =
c−1f ′(c/2) and b = f(c/2) −cf ′(c/2)/4. Since f(−x) = f(x), f(x) is symmetric about the line
x = 0. In a small neighborhood, f(x) is symmetric about (y, f(y)), so f ′(x) is continuous at y."
E,0.9372340425531915,"Let us first consider the smoothness of f. By symmetry, it suffices to consider x > 0. Then,"
E,0.9382978723404255,"f ′(x) = 
 "
E,0.9393617021276596,"(x −c)−1,
x ≥y
(2y −x −c)−1,
c/2 ≤x < y
2kx,
0 < x < c/2."
E,0.9404255319148936,Its Hessian is given by
E,0.9414893617021277,"f ′′(x) = 
 "
E,0.9425531914893617,"−(x −c)−2,
x > y
(2y −x −c)−2,
c/2 < x < y
2k,
0 < x < c/2."
E,0.9436170212765957,"Hence, f(x) is (2, 2k, 1)-smooth."
E,0.9446808510638298,"Note that f(x) has a stationary point 0. For stepsize ηf satisfying η0 ≤ηf ≤c2/4, there exists
z = (c+
p"
E,0.9457446808510638,"c2 + 2ηf) ≥y such that −z = z −ηf(y−c)−1 and by symmetry, once xτ = z, xt = ±z
for all t ≥τ, making the GD iterations stuck. Now we choose a proper x0 such that f ′(x0) and
f(x0) −f(0) are bounded."
E,0.9468085106382979,"We consider arriving at y from above. That is, x0 ≥x1 ≥. . . xτ = z > c > 0. Since in each update
where xt+1 = xt −ηf(xt −c)−1 > c,"
E,0.9478723404255319,xt −xt+1 = xt −(xt −ηf(xt −c)−1) = ηf(xt −c)−1 ≤√ηf.
E,0.948936170212766,"Hence, we can choose τ in such a way that 3c/2 ≤x0 < 3c/2 + √ηf. Then,"
E,0.95,"log(c/2) ≤f(x0) ≤log(c/2 + √ηf),
2/(c + 2√ηf) ≤f ′(x0) ≤2/c."
E,0.951063829787234,"By definition, y −c = η0(c +
p"
E,0.9521276595744681,"c2 + 2η0)−1. Hence,"
E,0.9531914893617022,f(c/2) = 2 log(y −c) −log(2y −c/2 −c)
E,0.9542553191489361,"= 2 log(η0) −2 log(c +
p"
E,0.9553191489361702,"c2 + 2η0) −log(
p"
E,0.9563829787234043,"c2 + 2η0 −c/2),"
E,0.9574468085106383,"f ′(c/2) =
1
p"
E,0.9585106382978723,"c2 + 2η0 −c/2 Then,"
E,0.9595744680851064,f(x0) −f(0) = f(x0) −f(c/2) + cf ′(c/2)/4
E,0.9606382978723405,"≤log(c/2 + √ηf) + 2 log(η−1
0 ) + 2 log(c +
p"
E,0.9617021276595744,c2 + 2η0)
E,0.9627659574468085,"+ log(
p"
E,0.9638297872340426,"c2 + 2η0 −c/2) + c 4
1
p"
E,0.9648936170212766,c2 + 2η0 −c/2
E,0.9659574468085106,"≤log(c) + 2 log(η−1
0 ) + 2 log(2
√"
E,0.9670212765957447,"2c2) + log(
√"
E,0.9680851063829787,2c2) + 1 2
E,0.9691489361702128,"= 4 log(c) + 2 log(η−1
0 ) + 7"
E,0.9702127659574468,2 log(2) + 1 2.
E,0.9712765957446808,"For stepsize ηf < η0, reaching below 4c/3 takes at least"
E,0.9723404255319149,"(x0 −4c/3)/√ηf ≥c/(6√ηf) > cη−1/2
0
/6"
E,0.973404255319149,"steps to reach 4c/3, where f ′(4c/3) = log(c/3)."
E,0.9744680851063829,"Now we set c and η0 and scale function f(x) to satisfy the parameter specifications L0, L2, G0, ∆0.
Define g(x) = L−1
2 f(x). Then, g(x) is (2, 2kL−1
2 , L2)-smooth. Since the gradient of g(x) is L−1
2
times f(x), the above analysis for f(x) applies to g(x) by replacing η0 with η1 = L2η0 and ηf with
η = L2ηf. To ensure that"
E,0.975531914893617,"2kL−1
2
= 2(cL2)−1f ′(c/2) =
2
cL2"
P,0.9765957446808511,"1
p"
P,0.9776595744680852,"c2 + 2η1 −c/2
≤
4
c2L2
≤L0,"
P,0.9787234042553191,it suffices to take c ≥2/√L0L2. To ensure that
P,0.9797872340425532,"g′(x0) ≤
2
L2c ≤G0,"
P,0.9808510638297873,it suffices to take c ≥2/(L2G0). To ensure that
P,0.9819148936170212,"g(x0) −g(0) ≤(4 log(c) + 2 log(η−1
1 ) + 3.5 log 2 + 0.5)L−1
2
≤∆0,"
P,0.9829787234042553,it suffices to take
P,0.9840425531914894,"log(η−1
1 ) = L2∆0 −3.5 log 2 −0.5"
P,0.9851063829787234,"2
−2 log(c)."
P,0.9861702127659574,"Since we require η1 ≤c2/2, parameters L2 and ∆0 need to satisfy"
P,0.9872340425531915,log 2 −2 log(c) ≤L2∆0 −3.5 log 2 −0.5
P,0.9882978723404255,"2
−2 log(c),"
P,0.9893617021276596,"that is, L2∆0
≥
5.5 log 2 + 0.5, which holds because L2∆0
≥
10.
Take c
=
max{2/√L0L2, 2/(L2G0),
p"
P,0.9904255319148936,"8/L0}. Then, as long as η ≤2/L0, the requirement that η ≤c2/4 is
satisfied. Therefore, on g(x) with initial point x0, gradient descent with a constant stepsize either
gets stuck, or takes at least"
P,0.9914893617021276,"cη−1/2
1
/6 = c"
EXP,0.9925531914893617,"6 exp
L2∆0 −3.5 log 2 −0.5"
EXP,0.9936170212765958,"4
−log(c)
 = 1"
EXP,0.9946808510638298,"6 exp(L2∆0 −3.5 log 2 −0.5 4
) ≥1"
EXP,0.9957446808510638,"6 exp(L2∆0 8
)"
EXP,0.9968085106382979,steps to reach a 1-stationary point.
EXP,0.997872340425532,"On the other hand, if η > 2/L0, consider the function f(x) = L0"
EXP,0.9989361702127659,"2 x2. For any xt ̸= 0, we always
have |xt+1| / |xt| = |1 −ηL0| > 1, which means the iterates diverge to infinity."
