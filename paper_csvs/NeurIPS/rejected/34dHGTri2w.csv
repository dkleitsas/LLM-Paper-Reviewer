Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012853470437017994,"Our research underscores the value of leveraging zeroth-order information for
1"
ABSTRACT,0.002570694087403599,"addressing sampling challenges, particularly when first-order data is unreliable or
2"
ABSTRACT,0.0038560411311053984,"unavailable. In light of this, we have developed a novel parallel sampling method
3"
ABSTRACT,0.005141388174807198,"that incorporates a leader-guiding mechanism. This mechanism forges connections
4"
ABSTRACT,0.006426735218508998,"between multiple sampling instances via a selected leader, enhancing both the
5"
ABSTRACT,0.007712082262210797,"efficiency and effectiveness of the entire sampling process. Our experimental
6"
ABSTRACT,0.008997429305912597,"results demonstrate that our method markedly expedites the exploration of the
7"
ABSTRACT,0.010282776349614395,"target distribution and produces superior quality outcomes compared to traditional
8"
ABSTRACT,0.011568123393316195,"sampling techniques. Furthermore, our method also shows greater resilience against
9"
ABSTRACT,0.012853470437017995,"the detrimental impacts of corrupted gradients as intended.
10"
INTRODUCTION,0.014138817480719794,"1
Introduction
11"
INTRODUCTION,0.015424164524421594,"Score-based generative models [35, 26, 36, 20] introduce a novel approach to generative modeling
12"
INTRODUCTION,0.016709511568123392,"that revolves around the estimation and sampling of the Stein score [26, 36]. The score represents
13"
INTRODUCTION,0.017994858611825194,"the gradient of the log-density function ∇x log π(x) evaluated at the input data point x. This type
14"
INTRODUCTION,0.019280205655526992,"of approach usually relies on effectively training a deep neural network to accurately estimate the
15"
INTRODUCTION,0.02056555269922879,"score. The estimated score is then utilized to navigate the sampling process, ultimately resulting
16"
INTRODUCTION,0.021850899742930592,"in the production of high-quality data samples that closely match the areas of high density in the
17"
INTRODUCTION,0.02313624678663239,"original distribution.
18"
INTRODUCTION,0.02442159383033419,"In our research, we investigate the sampling of a probability distribution given by π(x) ∝e−U(x),
19"
INTRODUCTION,0.02570694087403599,"where U(x) is the energy function. In the context of energy-based score-matching generative models,
20"
INTRODUCTION,0.02699228791773779,"the objective often involves sampling the modes in areas of high probability density. An approach
21"
INTRODUCTION,0.028277634961439587,"as suggested in [36, 20], is to smooth the original distribution by convolving π(x) with an isotropic
22"
INTRODUCTION,0.02956298200514139,"Gaussian distribution of variance σ2, yielding πσ(x) =
R
π(x′)N(x; x′, σ2I) dx′. By gradually
23"
INTRODUCTION,0.030848329048843187,"decreasing the variance σ, πσ(x) recovers the original distribution π(x).
24"
INTRODUCTION,0.032133676092544985,"Typically, the sampling of score-based approaches are integrated with numerical SDE solvers [38], for
25"
INTRODUCTION,0.033419023136246784,"example, the Euler-Maruyama solver, as well as Monte Carlo Markov Chain (MCMC) techniques like
26"
INTRODUCTION,0.03470437017994859,"Langevin Dynamics [30]. Furthermore, there is a notable similarity between score-based sampling
27"
INTRODUCTION,0.03598971722365039,"algorithms and first-order optimization algorithms. Efforts have been made to merge these two
28"
INTRODUCTION,0.037275064267352186,"methodologies, particularly from a perspective of sampling [42, 10, 28, 9, 44]. All these methods
29"
INTRODUCTION,0.038560411311053984,"primarily concentrates on first-order information ∇xU(x) to improve performance, while typically
30"
INTRODUCTION,0.03984575835475578,"treating the zeroth-order information U(x) merely as a basis for rejecting samples [18, 32, 29].
31"
INTRODUCTION,0.04113110539845758,"We argue that incorporating zeroth-order information can significantly enhance the algorithm’s overall
32"
INTRODUCTION,0.042416452442159386,"effectiveness, particularly in instances where the first-order information is compromised. To address
33"
INTRODUCTION,0.043701799485861184,"this, we draw inspiration from parallel tempering [39], a simulation method commonly used to
34"
INTRODUCTION,0.04498714652956298,"identify the lowest energy state in systems of interacting particles. The fundamental principle of
35"
INTRODUCTION,0.04627249357326478,"parallel tempering involves operating multiple sampling replicas simultaneously, each at a different
36"
INTRODUCTION,0.04755784061696658,"temperature level. These temperatures typically range from low, where the system is prone to being
37"
INTRODUCTION,0.04884318766066838,"trapped in local minima, to high, which facilitates the system’s ability to surmount energy barriers
38"
INTRODUCTION,0.05012853470437018,"and more thoroughly explore the energy landscape.
39"
INTRODUCTION,0.05141388174807198,"Drawing inspiration from this concept, we extend the Hamiltonian Monte Carlo (HMC) framework
40"
INTRODUCTION,0.05269922879177378,"[29] and introduce a novel algorithm that concurrently runs multiple replicas, sampling at both high
41"
INTRODUCTION,0.05398457583547558,"and low Hamiltonian energy levels. Moreover, this methodology combines both zeroth and first
42"
INTRODUCTION,0.055269922879177376,"order information from various chains, hence enhancing the effectiveness of sampling approaches.
43"
INTRODUCTION,0.056555269922879174,"The experimental findings demonstrate the efficacy of our approach in scenarios where relying
44"
INTRODUCTION,0.05784061696658098,"solely on first-order knowledge is insufficient. These findings illustrate the capacity of incorporating
45"
INTRODUCTION,0.05912596401028278,"zeroth-order information to greatly enhance the efficiency and accuracy of sampling operations in
46"
INTRODUCTION,0.060411311053984576,"energy-based score-matching algorithms.
47"
BACKGROUND,0.061696658097686374,"2
Background
48"
HAMILTONIAN MONTE CARLO,0.06298200514138817,"2.1
Hamiltonian Monte Carlo
49"
HAMILTONIAN MONTE CARLO,0.06426735218508997,"The primary purpose of MCMC is to construct a Markov chain that matches its equilibrium distribution
50"
HAMILTONIAN MONTE CARLO,0.06555269922879177,"to the target distribution. One of the most popular MCMC methods is Langevin Monte Carlo [17, 32],
51"
HAMILTONIAN MONTE CARLO,0.06683804627249357,"which proposes samples in a Metropolis-Hastings [18] framework for more efficient state space
52"
HAMILTONIAN MONTE CARLO,0.06812339331619537,"exploration. Another advanced method is HMC [29, 11, 2], which incorporates an auxiliary variable
53"
HAMILTONIAN MONTE CARLO,0.06940874035989718,"p and employs Hamiltonian dynamics to facilitate the sampling process. The Hamiltonian function is
54"
HAMILTONIAN MONTE CARLO,0.07069408740359898,"structured as a composite of potential energy U(x) and kinetic energy K(p), defined as follows:
55"
HAMILTONIAN MONTE CARLO,0.07197943444730077,"H(x, p) = U(x) + K(p),
(1)
where x represents the position of a particle and p denotes its momentum. Kinetic energy K(p)
56"
HAMILTONIAN MONTE CARLO,0.07326478149100257,is commonly formulated as K(p) = 1
HAMILTONIAN MONTE CARLO,0.07455012853470437,"2pT M −1p, where M corresponds to the mass matrix. For
57"
HAMILTONIAN MONTE CARLO,0.07583547557840617,"simplicity, we assume in this paper that the mass matrix M is equal to the identity matrix I. The joint
58"
HAMILTONIAN MONTE CARLO,0.07712082262210797,"distribution of position and momentum conforms to the canonical distribution:
59"
HAMILTONIAN MONTE CARLO,0.07840616966580977,"π(x, p) = e−H(x,p)/Z,
(2)"
HAMILTONIAN MONTE CARLO,0.07969151670951156,"where Z =
RR
e−H(x,p) dxdp. Samples from π(x) can then be obtained by marginalizing p from
60"
HAMILTONIAN MONTE CARLO,0.08097686375321336,"π(x, p), which further requires
R"
HAMILTONIAN MONTE CARLO,0.08226221079691516,"p π(x, p) dp = constant. In the HMC algorithm, proposals are
61"
HAMILTONIAN MONTE CARLO,0.08354755784061697,"generated by simulating Hamiltonian dynamics and then subjected to a Metropolis criterion to
62"
HAMILTONIAN MONTE CARLO,0.08483290488431877,"determine their acceptance or rejection. A commonly employed numerical method for solving these
63"
HAMILTONIAN MONTE CARLO,0.08611825192802057,"equations is the Leapfrog integrator [3].
64"
HAMILTONIAN MONTE CARLO,0.08740359897172237,"Recent progress in HMC techniques has focused on increasing their adaptability and applicability in a
65"
HAMILTONIAN MONTE CARLO,0.08868894601542417,"variety of contexts. Such developments include the NUTS sampler [21], which features an automatic
66"
HAMILTONIAN MONTE CARLO,0.08997429305912596,"mechanism for adjusting the number of simulation steps. The Riemann manifold HMC [15] leverages
67"
HAMILTONIAN MONTE CARLO,0.09125964010282776,"Riemannian geometry to modify the mass matrix M, making use of curvature information to improve
68"
HAMILTONIAN MONTE CARLO,0.09254498714652956,"sampling efficiency. Additionally, Stochastic Gradient Hamiltonian Monte Carlo [11, 27] investigates
69"
HAMILTONIAN MONTE CARLO,0.09383033419023136,"a stochastic gradient approach within the HMC framework. Our contribution is distinct from these
70"
HAMILTONIAN MONTE CARLO,0.09511568123393316,"methods and can be easily integrated with them.
71"
ENERGY-BASED SCORE-MATCHING MODEL,0.09640102827763496,"2.2
Energy-based score-matching model
72"
ENERGY-BASED SCORE-MATCHING MODEL,0.09768637532133675,"Probabilistic models often require normalization, which can become infeasible when dealing with
73"
ENERGY-BASED SCORE-MATCHING MODEL,0.09897172236503857,"high-dimensional data [25, 13]. Since the exact probabilities of less probable alternatives become
74"
ENERGY-BASED SCORE-MATCHING MODEL,0.10025706940874037,"less crucial as long as they remain relatively lower, rather than solely predicting the most probable
75"
ENERGY-BASED SCORE-MATCHING MODEL,0.10154241645244216,"outcome, models can be structured to interpret relationships between variables via an energy function.
76"
ENERGY-BASED SCORE-MATCHING MODEL,0.10282776349614396,"Within the context of generative models, these energy-based models (EBMs) are devised to assign
77"
ENERGY-BASED SCORE-MATCHING MODEL,0.10411311053984576,"higher energy values to regions of lower probability and lower energy values to regions of higher
78"
ENERGY-BASED SCORE-MATCHING MODEL,0.10539845758354756,"probability.
79"
ENERGY-BASED SCORE-MATCHING MODEL,0.10668380462724936,"Score matching [22, 36] is a method used in statistical modeling and machine learning to estimate a
80"
ENERGY-BASED SCORE-MATCHING MODEL,0.10796915167095116,"probability distribution or a probability density function from observed data. It is particularly useful
81"
ENERGY-BASED SCORE-MATCHING MODEL,0.10925449871465295,"when direct estimation of the probability distribution is challenging, especially in high-dimensional
82"
ENERGY-BASED SCORE-MATCHING MODEL,0.11053984575835475,"spaces. In score matching, the goal is to find an approximation to the probability density function
83"
ENERGY-BASED SCORE-MATCHING MODEL,0.11182519280205655,"(PDF) of a dataset by estimating the score function, also known as the gradient of the log-density.
84"
ENERGY-BASED SCORE-MATCHING MODEL,0.11311053984575835,"The score function represents the derivative of the log PDF with respect to the data. By matching the
85"
ENERGY-BASED SCORE-MATCHING MODEL,0.11439588688946016,"estimated score function to the observed data, one can indirectly estimate the underlying probability
86"
ENERGY-BASED SCORE-MATCHING MODEL,0.11568123393316196,"distribution.
87"
ENERGY-BASED SCORE-MATCHING MODEL,0.11696658097686376,"A relationship between EBMs and score matching can be established by training EBMs through
88"
ENERGY-BASED SCORE-MATCHING MODEL,0.11825192802056556,"denoising score matching [37]. The training objective is described below:
89"
ENERGY-BASED SCORE-MATCHING MODEL,0.11953727506426735,"Eπ(x)N(ϵ;0,I)
h ϵ"
ENERGY-BASED SCORE-MATCHING MODEL,0.12082262210796915,"σ −∇xUθ
 
x + σϵ

2"
ENERGY-BASED SCORE-MATCHING MODEL,0.12210796915167095,"2 .
(3)"
ENERGY-BASED SCORE-MATCHING MODEL,0.12339331619537275,"Uθ is typically represented as a neural network, with θ denoting its parameters. Minimizing this
90"
ENERGY-BASED SCORE-MATCHING MODEL,0.12467866323907455,"objective ensures that ∇xUθ(x) = −∇x log πσ(x) and thus e−Uθ(x) shall be proportional to πσ(x).
91"
MOTIVATION,0.12596401028277635,"3
Motivation
92"
MOTIVATION,0.12724935732647816,"Figure 1: A good anchor point could
help improve convergence even if the
gradient is unexpectedly disturbed from
original gradient to the disturbed gradi-
ent, getting closer to the optimal point."
MOTIVATION,0.12853470437017994,"In our work, we assume to have access to both the gradi-
93"
MOTIVATION,0.12982005141388175,"ent information ∇xU(x) as well as the energy information
94"
MOTIVATION,0.13110539845758354,"U(x). In certain scenarios, gradients may yield informa-
95"
MOTIVATION,0.13239074550128535,"tion that is either of limited or potentially detrimental. Our
96"
MOTIVATION,0.13367609254498714,"research examines situations where gradients are compro-
97"
MOTIVATION,0.13496143958868895,"mised, highlighting the importance of zeroth-order infor-
98"
MOTIVATION,0.13624678663239073,"mation, often associated with energy-based sampling.
99"
MOTIVATION,0.13753213367609254,"We concentrate on showcasing the strengths of our method
100"
MOTIVATION,0.13881748071979436,"in three types of challenging but common scenarios, sum-
101"
MOTIVATION,0.14010282776349614,"marized as instability, metastability and pseudo-stability.
102"
MOTIVATION,0.14138817480719795,"Instability refers to a state in which a system lacks equilibrium or steadiness, often leading to unpre-
103"
MOTIVATION,0.14267352185089974,"dictable or erratic behavior. Metastability describes a condition where a system appears stable over a
104"
MOTIVATION,0.14395886889460155,"short period but is not in its most stable state, and it can transition to a more stable state under certain
105"
MOTIVATION,0.14524421593830333,"conditions. Pseudo-stability, on the other hand, denotes a situation where a system seems stable but
106"
MOTIVATION,0.14652956298200515,"is actually in an incorrect, suboptimal, or misleadingly stable state.
107"
MOTIVATION,0.14781491002570693,"Figure 2: To enhance exploratory ca-
pabilities, it’s important to encourage
particle to explore the landscape."
MOTIVATION,0.14910025706940874,"Instability. In high-dimensional spaces, sampling algo-
108"
MOTIVATION,0.15038560411311053,"rithms may struggle to converge in the presence of a com-
109"
MOTIVATION,0.15167095115681234,"plex probability distribution. This instability can arise in
110"
MOTIVATION,0.15295629820051415,"situations where the local Hessian matrix is ill-conditioned
111"
MOTIVATION,0.15424164524421594,"or spectrum of the local Hessian matrix is exceptionally
112"
MOTIVATION,0.15552699228791775,"large. Such conditions often lead to inaccuracies or insta-
113"
MOTIVATION,0.15681233933161953,"bilities in numerical calculations, potentially causing the
114"
MOTIVATION,0.15809768637532134,"convergence process to fail. The samples generated could
115"
MOTIVATION,0.15938303341902313,"substantially diverge from the true mode, resulting in subpar sample quality. However, employing an
116"
MOTIVATION,0.16066838046272494,"anchor point can enhance the stability of convergence, as demonstrated in Figure 1.
117"
MOTIVATION,0.16195372750642673,"Metastability. Particles are prone to getting stuck in local minima when the gradients are not
118"
MOTIVATION,0.16323907455012854,"informative. For example, on the saddle point or a pleaute loss landscape. As a result, simulations
119"
MOTIVATION,0.16452442159383032,"frequently end up in a state of intermediate energy, which is different from the system’s lowest energy
120"
MOTIVATION,0.16580976863753213,"state. This scenario is illustrated in Figure 2.
121"
MOTIVATION,0.16709511568123395,"Figure 3: There is a potential for parti-
cles to unintentionally follow the gradi-
ent flow towards these regions of high
energy. A more comprehensive descrip-
tion could be found at Section 5.1.3."
MOTIVATION,0.16838046272493573,"Pseudo-Stability. Certain situations may present a diver-
122"
MOTIVATION,0.16966580976863754,"gence between the gradient information and the ground
123"
MOTIVATION,0.17095115681233933,"truth. This divergence can hinder algorithms from accu-
124"
MOTIVATION,0.17223650385604114,"rately converging to the appropriate modes. In these in-
125"
MOTIVATION,0.17352185089974292,"stances, it becomes essential to incorporate energy informa-
126"
MOTIVATION,0.17480719794344474,"tion to rectify inaccuracies that arise from solely depending
127"
MOTIVATION,0.17609254498714652,"on gradients. An example of misleading gradients could be
128"
MOTIVATION,0.17737789203084833,"observed in Figure 3.
129"
ALGORITHM,0.17866323907455012,"4
Algorithm
130"
ALGORITHM,0.17994858611825193,"Many sampling methods typically rely on independent
131"
ALGORITHM,0.18123393316195371,"Markov chains, which can lead to the issues mentioned
132"
ALGORITHM,0.18251928020565553,"in Section 3. Taking inspiration from [39], our approach
133"
ALGORITHM,0.18380462724935734,"involves the utilization of multiple replicas. This approach
134"
ALGORITHM,0.18508997429305912,Algorithm 1 Elastic Leapfrog (eLeapfrog)
ALGORITHM,0.18637532133676094,"Input: A collection of positions {xi}n
i=1 ∈Rn×d, a collection of momenta {pi}n
i=1 ∈Rn×d,
learning rate η > 0, pulling strength λ ≥0, number of Leapfrog steps L.
for s = 1, · · · , L do"
ALGORITHM,0.18766066838046272,"for i = 1, · · · , n do"
ALGORITHM,0.18894601542416453,Choose the leader xl and calculate ρi
ALGORITHM,0.19023136246786632,gi ←∇xU(xi) + ρi · (xi −xl); pi ←pi −η
ALGORITHM,0.19151670951156813,"2 · gi
▷Half step for momentum
xi ←xi + η · pi
▷Full step for position
Choose the leader xl and calculate ρi"
ALGORITHM,0.1928020565552699,gi ←∇xU(xi) + ρi · (xi −xl); pi ←pi −η
ALGORITHM,0.19408740359897173,"2 · gi
▷Half step for momentum
end for
end for
Output: x ∈Rd, p ∈Rd"
ALGORITHM,0.1953727506426735,"enables us to implicitly encourage greater exploration among multiple particles while simultaneously
135"
ALGORITHM,0.19665809768637532,"preserving the optimal outcomes for exploitation purposes. We will elaborate on how our algorithm
136"
ALGORITHM,0.19794344473007713,"can be employed to tackle these challenges.
137"
ALGORITHM,0.19922879177377892,"Firstly, we introduce a modified version of the leapfrog method, called the Elastic Leapfrog
138"
ALGORITHM,0.20051413881748073,"(eLeapfrog). In this approach, additional elastic forces are applied between each particle and a
139"
ALGORITHM,0.20179948586118251,"leader, incorporating an extra elastic energy term into the traditional Hamiltonian function. This
140"
ALGORITHM,0.20308483290488433,"modification aims to prevent particles from straying significantly from each other, thereby promoting
141"
ALGORITHM,0.2043701799485861,"local exploitation. We then divide the particles into groups and designate the particle with the lowest
142"
ALGORITHM,0.20565552699228792,"energy as the leader. Moreover, when combined with the eLeapfrog method, this approach encourages
143"
ALGORITHM,0.2069408740359897,"other particles to explore around the leader, efficiently addressing the problem of instability.
144"
ALGORITHM,0.20822622107969152,"Due to the properties of HMC, introducing such an extra elastic energy term when pulling the particles
145"
ALGORITHM,0.2095115681233933,"towards the leader implicitly incorporates this energy into the momentum, thereby increasing the
146"
ALGORITHM,0.21079691516709512,"search ability of each particle. As a result, non-leading particles gain more energy for exploration,
147"
ALGORITHM,0.2120822622107969,"while the leading particle is more likely to concentrate on local exploitation. This approach helps
148"
ALGORITHM,0.2133676092544987,"mitigate the issue of metastability.
149"
ALGORITHM,0.21465295629820053,"Finally, we integrate these techniques to present our compplete Follow Hamiltonian Leader (FHL)
150"
ALGORITHM,0.2159383033419023,"algorithm. The FHL algorithm capitalizes on both first-order and zeroth-order information while
151"
ALGORITHM,0.21722365038560412,"significantly improving the efficiency of space sampling compared to traditional sequential sampling
152"
ALGORITHM,0.2185089974293059,"methods. This enhanced approach fosters convergence towards the lowest energy states and increases
153"
ALGORITHM,0.21979434447300772,"the likelihood of escaping states with pseudo stability.
154"
ELASTIC LEAPFROG,0.2210796915167095,"4.1
Elastic Leapfrog
155"
ELASTIC LEAPFROG,0.22236503856041132,"To improve the efficiency of sampling, we integrate an elastic force component into the conventional
156"
ELASTIC LEAPFROG,0.2236503856041131,"leapfrog technique. This enhancement aims to dynamically guide particles towards a leading particle,
157"
ELASTIC LEAPFROG,0.2249357326478149,"facilitating their movement and improve their exploration ability. The method could be treated like
158"
ELASTIC LEAPFROG,0.2262210796915167,"temporarily storing potential energy within an elastic spring, which is then converted into kinetic
159"
ELASTIC LEAPFROG,0.2275064267352185,"energy. By adding extra elastic force, we could define the energy of elastic HMC as:
160"
ELASTIC LEAPFROG,0.22879177377892032,"He(x, p; ˜x) = Ue(x; ˜x) + K(p) = [U(x) + E(x; ˜x)]
|
{z
}
Ue(x,˜x)"
ELASTIC LEAPFROG,0.2300771208226221,"+K(p),
(4)"
ELASTIC LEAPFROG,0.23136246786632392,"161
where E(x; ˜x) is the extra elastic energy imposed by Elastic Leapfrog and is defined as E(x; ˜x) =
162"
ELASTIC LEAPFROG,0.2326478149100257,"ρ
2 ∥x −˜x∥2
2. Our approach enables particles to efficiently navigate the sample space, guided by the
163"
ELASTIC LEAPFROG,0.23393316195372751,"leader. This local exploration strategy, though similar to concepts in [46, 7, 8, 40], is uniquely tailored
164"
ELASTIC LEAPFROG,0.2352185089974293,"for application in the realm of sampling.
165"
LEADER PULLING,0.2365038560411311,"4.2
Leader Pulling
166"
LEADER PULLING,0.2377892030848329,"Next, we introduce our leader pulling method. Initially, we represent the ith particle inside a group
167"
LEADER PULLING,0.2390745501285347,"as xi and select a leader based on a their energies U(xi). The motivation is that we encourage each
168"
LEADER PULLING,0.2403598971722365,"particle xi to be guided towards a chosen leader. The leader is chosen as the one of minimum energy
169"
LEADER PULLING,0.2416452442159383,"and thus its index is l = arg mini U(xi). The objective function for a group of n particles is:
170"
LEADER PULLING,0.24293059125964012,"Ue(x1, · · · , xn; xl) = n
X"
LEADER PULLING,0.2442159383033419,"i=1
U(xi) + ρi"
LEADER PULLING,0.2455012853470437,"2 · ∥xi −xl∥2
2,
(5)"
LEADER PULLING,0.2467866323907455,"171
where πi = exp
 
−U(xi)

/ P"
LEADER PULLING,0.2480719794344473,"j exp
 
−U(xj)

and ρi = λ · (πl −πi)/(πl + πi). The specifics of
172"
LEADER PULLING,0.2493573264781491,"the Elastic Leapfrog algorithm combined with leader pulling technique are detailed in Algorithm 1.
173"
FOLLOW HAMILTONIAN LEADER,0.2506426735218509,"4.3
Follow Hamiltonian Leader
174"
FOLLOW HAMILTONIAN LEADER,0.2519280205655527,"Incorporating zeroth-order information (i.e., function values rather than derivatives) serves two key
175"
FOLLOW HAMILTONIAN LEADER,0.2532133676092545,"purposes. Firstly, it provides a search direction that accelerates convergence and helps mitigate issues
176"
FOLLOW HAMILTONIAN LEADER,0.2544987146529563,"arising from corrupted first-order information (i.e., gradient inaccuracies), thereby speeding up the
177"
FOLLOW HAMILTONIAN LEADER,0.25578406169665807,"optimization process. Second, it helps ensure that we are sampling from the correct underlying
178"
FOLLOW HAMILTONIAN LEADER,0.2570694087403599,"distribution by properly accepting or rejecting the proposal.
179"
FOLLOW HAMILTONIAN LEADER,0.2583547557840617,"To ensure that the sampling method maintains detailed balance—a requirement for most sampling
180"
FOLLOW HAMILTONIAN LEADER,0.2596401028277635,"algorithms—we evaluate the joint distribution of a group of particles. This evaluation determines
181"
FOLLOW HAMILTONIAN LEADER,0.2609254498714653,"whether to accept or reject a proposed move for the whole group, thereby preserving the integrity
182"
FOLLOW HAMILTONIAN LEADER,0.2622107969151671,"of the sampling process. This adaptation results in the creation of our algorithm FHL, extensively
183"
FOLLOW HAMILTONIAN LEADER,0.2634961439588689,"elucidated in Algorithm 2.
184"
FOLLOW HAMILTONIAN LEADER,0.2647814910025707,Algorithm 2 Follow Hamiltonian Leader
FOLLOW HAMILTONIAN LEADER,0.2660668380462725,"Input: A collection of positions {xi}n
i=1 ∈Rn×d, learning rate η > 0, pulling strength λ ≥0,
number of steps L.
for t = 1, 2, · · · , T do"
FOLLOW HAMILTONIAN LEADER,0.26735218508997427,"# Run sampling in parallel
for i = 1, · · · , n do"
FOLLOW HAMILTONIAN LEADER,0.2686375321336761,"Randomly sample the momentum pi
t−1 ∼N(0, I)
xi
prop, pi
prop ←eLeapfrog (xi
t−1, pi
t−1, η, λ, L)
end for
Sample a random variable u ∼Uniform(0, 1)"
FOLLOW HAMILTONIAN LEADER,0.2699228791773779,"if u < Qn
i=1 exp
 
H(xi
prop, pi
prop) −H(xi
t−1, pi
t−1)

then"
FOLLOW HAMILTONIAN LEADER,0.2712082262210797,"for i = 1, · · · , n do xi
t ←xi
prop, pi
t ←pi
prop end for
else"
FOLLOW HAMILTONIAN LEADER,0.27249357326478146,"for i = 1, · · · , n do xi
t ←xi
t−1, pi
t ←pi
t−1 end for
end if
end for
Output: XT = {xi
T }n
i=1 ∈Rn×d"
EXPERIMENT,0.2737789203084833,"5
Experiment
185"
EXPERIMENT,0.2750642673521851,"In this section, we showcase the efficacy of incorporating zeroth-order information, specifically
186"
EXPERIMENT,0.2763496143958869,"energy information, into our proposed method to improve the sampling process. We focus on
187"
EXPERIMENT,0.2776349614395887,"demonstrating the advantages of our approach in addressing the benefits of our approach in handling
188"
EXPERIMENT,0.27892030848329047,"three distinct adversarial gradient scenarios, as outlined in Section 3. To evaluate our method on the
189"
EXPERIMENT,0.2802056555269923,"performance of the concerned questions, we conduct a comparative analysis against the following
190"
EXPERIMENT,0.2814910025706941,"baseline algorithms:
191"
EXPERIMENT,0.2827763496143959,"• LMC (Langevin Monte Carlo): An MCMC method as described in [17] that uses Langevin
192"
EXPERIMENT,0.28406169665809766,"dynamics to sample from probability distributions. It is also known as the Metropolis-
193"
EXPERIMENT,0.2853470437017995,"adjusted Langevin algorithm.
194"
EXPERIMENT,0.2866323907455013,"• HMC (Hamiltonian Monte Carlo): An MCMC algorithm that employs Hamiltonian dynamics
195"
EXPERIMENT,0.2879177377892031,"for more efficient traversal of the state space, leading to better exploration and sampling
196"
EXPERIMENT,0.2892030848329049,"from complex distributions [29, 11, 2].
197"
EXPERIMENT,0.29048843187660667,"• U-LMC (Unadjusted Langevin Dynamics): A variation of LMC without the Metropolis correc-
198"
EXPERIMENT,0.2917737789203085,"tion, referred to [32, 1, 42].
199"
EXPERIMENT,0.2930591259640103,"• U-HMC (Unadjusted Hamiltonian Monte Carlo): A form of HMC that excludes the Metropolis
200"
EXPERIMENT,0.2943444730077121,"correction step, as in [34, 14].
201"
MOTIVATING EXAMPLES,0.29562982005141386,"5.1
Motivating Examples
202"
MOTIVATING EXAMPLES,0.2969151670951157,"We report on results addressing the challenges identified as instability, metastability, and pseudo-
203"
MOTIVATING EXAMPLES,0.2982005141388175,"stability. Our findings lead us to conclude that the FTH method consistently outperforms other
204"
MOTIVATING EXAMPLES,0.2994858611825193,"approaches in all scenarios examined. Detailed discussions and further analyses of these findings will
205"
MOTIVATING EXAMPLES,0.30077120822622105,"be presented in the following subsections.
206"
MOTIVATING EXAMPLES,0.30205655526992287,"In our experiment, we simultaneously execute sampling with N particles, each completing a total of
207"
MOTIVATING EXAMPLES,0.3033419023136247,"T sampling steps. For the FTH method, these particles are divided into N/n groups, with each group
208"
MOTIVATING EXAMPLES,0.3046272493573265,"containing n particles. Throughout all experiments, we set n to 4. For hyperparameter search, we
209"
MOTIVATING EXAMPLES,0.3059125964010283,"select step sizes η = {0.002, 0.0002, 0.005, 0.0005} for all methods and number of leapfrog steps
210"
MOTIVATING EXAMPLES,0.30719794344473006,"L = {4, 8, 16} for HMC-type methods.
211"
INSTABILITY,0.30848329048843187,"5.1.1
Instability
212"
INSTABILITY,0.3097686375321337,(a) Sampling from the original distribution in the form of e−U(x). T = 1000 in all experiments.
INSTABILITY,0.3110539845758355,"(b) Sampling from the approximated distribution in the form of e−Uθ(x). T = 2000 in all experiments.
Figure 4: Sample from a Gaussian distribution N(µ, Σ) where µ ∈Rd corresponds to the clean
image. For each method, we plot the lowest-energy particle (in terms of U(x) among all particles in
XT ). The upper-left image represents a direct sample from the distribution N(µ, Σ); The lower-left
image is generated by performing HMC sampling for T steps on the function Uθ(x), with an initial
point set to x0 = µ."
INSTABILITY,0.31233933161953725,"In our sampling process, we focus on efficiently directing particles to high probability density regions,
213"
INSTABILITY,0.31362467866323906,"thereby avoiding unproductive exploration in regions with low probability. When sampling from a
214"
INSTABILITY,0.3149100257069409,"single image, our goal becomes attaining the global optima, aligning this objective with those found
215"
INSTABILITY,0.3161953727506427,"in optimization tasks.
216"
INSTABILITY,0.31748071979434445,"For our experiment, we chose an image resembling the GitHub logo (https://github.com/logos),
217"
INSTABILITY,0.31876606683804626,"converted it into a vector format, and use this as the mean of a multivariate Gaussian distribution.
218"
INSTABILITY,0.32005141388174807,"The covariance matrix for this distribution, represented by Σ, is diagonal. The variance for each
219"
INSTABILITY,0.3213367609254499,"dimension of the distribution is randomly determined by a uniform distribution within the range of
220"
INSTABILITY,0.3226221079691517,"(0.25, 1.25). We carry out two similar but different types of experiments:
221"
INSTABILITY,0.32390745501285345,"In the first experiment, we focus on sampling from the original distribution. This distribution
222"
INSTABILITY,0.32519280205655526,"is described mathematically as e−U(x) ∝N(µ, Σ), with U(x) being the energy function that
223"
INSTABILITY,0.3264781491002571,"characterizes the system.
224"
INSTABILITY,0.3277634961439589,"For the energy-based score-matching model, we employ a ResNet [19] architecture with 6 layers
225"
INSTABILITY,0.32904884318766064,"of a hidden dimension of 256. The results of the sampling process are detailed in Figure 4, where
226"
INSTABILITY,0.33033419023136246,"the main objective is to assess the particles’ capacity for effective convergence to the mode of the
227"
INSTABILITY,0.33161953727506427,"distribution. In the first scenario, U(x) represents a convex function, whereas in the second scenario,
228"
INSTABILITY,0.3329048843187661,"Uθ(x) is presumed to be non-convex. The findings demonstrate that our approach, FHL, surpasses
229"
INSTABILITY,0.3341902313624679,"other baseline methods in both situations.
230"
METASTABILITY,0.33547557840616965,"5.1.2
Metastability
231"
METASTABILITY,0.33676092544987146,"Figure 5: Plot of N = 256 particles of XT on d = 2 starting
from random initialization N(0, 4·I). The target distribution
is N([1, 1], I). Energy State corresponds to the target density
π. The baseline methods U-LMC,LMC,U-HMC,HMC and
our proposed method FHL generate XT after T = 1000
steps. There are no gradient flows and the samplers are only
able to sample by the energy information."
METASTABILITY,0.3380462724935733,"Our research explores the concept of
232"
METASTABILITY,0.3393316195372751,"metastability, which arises in specific
233"
METASTABILITY,0.34061696658097684,"scenarios. Metastability refers to a
234"
METASTABILITY,0.34190231362467866,"state of intermediate energy in a dy-
235"
METASTABILITY,0.34318766066838047,"namic system, differing from its low-
236"
METASTABILITY,0.3444730077120823,"est energy state. We examine an ex-
237"
METASTABILITY,0.34575835475578404,"treme scenario where gradients are en-
238"
METASTABILITY,0.34704370179948585,"tirely absent, and sampling methods
239"
METASTABILITY,0.34832904884318766,"only get access to the energy informa-
240"
METASTABILITY,0.3496143958868895,"tion about the distribution.
241"
METASTABILITY,0.3508997429305913,"In Figure 5, it’s evident that in this par-
242"
METASTABILITY,0.35218508997429304,"ticular situation, we enforce the gra-
243"
METASTABILITY,0.35347043701799485,"dient to be near zero, resulting in all
244"
METASTABILITY,0.35475578406169667,"sampling methods, except FHL and
245"
METASTABILITY,0.3560411311053985,"LMC, behaving almost like random
246"
METASTABILITY,0.35732647814910024,"sampling. Nevertheless, owing to the
247"
METASTABILITY,0.35861182519280205,"leader pulling strategy, FHL retains its
248"
METASTABILITY,0.35989717223650386,"ability to locate the mode much faster.
249"
PSEUDO-STABILITY,0.36118251928020567,"5.1.3
Pseudo-stability
250"
PSEUDO-STABILITY,0.36246786632390743,"This section highlights the phenomenon showcased in Figure 3. Here, particles can become ensnared
251"
PSEUDO-STABILITY,0.36375321336760924,"by gradient flows and be coerced into pseudo-stable regions. Despite the eventual recovery of the
252"
PSEUDO-STABILITY,0.36503856041131105,"correct distribution by the sampling method, the convergence process can be exceptionally sluggish.
253"
PSEUDO-STABILITY,0.36632390745501286,"To elaborate, we examine a scenario where the samplers solely depend on gradients from ∇log Q,
254"
PSEUDO-STABILITY,0.3676092544987147,"while the energy function P remains deliberately undisclosed. The distributions P and Q are:
255"
PSEUDO-STABILITY,0.36889460154241643,• Q ∼1
PSEUDO-STABILITY,0.37017994858611825,"4 [N (µ1, I) + N (µ2, I) + N (µ3, I) + N (µ4, I)]
256"
PSEUDO-STABILITY,0.37146529562982006,• P ∼1
PSEUDO-STABILITY,0.37275064267352187,"2 [N (µ1, I) + N (µ2, I)]
257"
PSEUDO-STABILITY,0.3740359897172236,"where µ1 = [−2, 0], µ2 = [2, 0], µ3 = [0, 2] and µ4 = [0, −2].
258"
PSEUDO-STABILITY,0.37532133676092544,"Figure 6: Plot of N = 256 particles of XT for a 2-mode
Gaussian mixture model on d = 2 starting from random
initialization N(0, 4 · I). Energy State corresponds to the
target density π. The baseline methods U-LMC,LMC,U-
HMC,HMC and our proposed method FHL generate XT
after T = 200 steps."
PSEUDO-STABILITY,0.37660668380462725,"From Figure 6 we can see that FTH
259"
PSEUDO-STABILITY,0.37789203084832906,"does not only capture the modes more
260"
PSEUDO-STABILITY,0.3791773778920309,"quickly compared to the other meth-
261"
PSEUDO-STABILITY,0.38046272493573263,"ods but also successfully get out of the
262"
PSEUDO-STABILITY,0.38174807197943444,"trap of the pseudo-stable regions.
263"
PSEUDO-STABILITY,0.38303341902313626,"Our study addresses the challenges
264"
PSEUDO-STABILITY,0.38431876606683807,"of
instability,
metastability,
and
265"
PSEUDO-STABILITY,0.3856041131105398,"pseudo-stability, demonstrating that
266"
PSEUDO-STABILITY,0.38688946015424164,"the FTH method consistently outper-
267"
PSEUDO-STABILITY,0.38817480719794345,"forms other approaches across various
268"
PSEUDO-STABILITY,0.38946015424164526,"scenarios. Through illustrative experi-
269"
PSEUDO-STABILITY,0.390745501285347,"ments, we show that FTH rapidly cap-
270"
PSEUDO-STABILITY,0.39203084832904883,"tures modes and effectively escapes
271"
PSEUDO-STABILITY,0.39331619537275064,"pseudo-stable regions, even when gra-
272"
PSEUDO-STABILITY,0.39460154241645246,"dients are entirely absent. This su-
273"
PSEUDO-STABILITY,0.39588688946015427,"perior performance is attributed to
274"
PSEUDO-STABILITY,0.397172236503856,"FTH’s unique leader pulling strat-
275"
PSEUDO-STABILITY,0.39845758354755784,"egy, which directs particles efficiently
276"
PSEUDO-STABILITY,0.39974293059125965,"to high-probability density regions,
277"
PSEUDO-STABILITY,0.40102827763496146,"thereby avoiding unproductive explo-
278"
PSEUDO-STABILITY,0.4023136246786632,"ration in low-probability areas.
279"
PSEUDO-STABILITY,0.40359897172236503,"In the following section, we will illustrate the advantages of FTH in more general applications,
280"
PSEUDO-STABILITY,0.40488431876606684,"particularly for energy-based (score-matching) models.
281"
ENERGY-BASED GENERATIVE MODEL,0.40616966580976865,"5.2
Energy-Based Generative Model
282"
ENERGY-BASED GENERATIVE MODEL,0.4074550128534704,"Energy-based models (EBMs) offer significant advantages for sampling because they naturally
283"
ENERGY-BASED GENERATIVE MODEL,0.4087403598971722,"provide energy information that can be utilized to guide the sampling process. In an EBM, the energy
284"
ENERGY-BASED GENERATIVE MODEL,0.41002570694087404,"function assigns lower energy values to more probable configurations, enabling the sampler to more
285"
ENERGY-BASED GENERATIVE MODEL,0.41131105398457585,"effectively navigate the probability landscape and generate high-quality samples. This makes EBMs
286"
ENERGY-BASED GENERATIVE MODEL,0.41259640102827766,"a powerful tool in scenarios where precise sampling is essential.
287"
ENERGY-BASED GENERATIVE MODEL,0.4138817480719794,"We investigate a scenario where energy functions guide the sampling process. We use the generative
288"
ENERGY-BASED GENERATIVE MODEL,0.41516709511568123,"model outlined in [16] and adopt a conditional generation method that leverages classifier-derived
289"
ENERGY-BASED GENERATIVE MODEL,0.41645244215938304,"gradients for sampling. The classifier’s output is considered as the energy for guided sampling.
290"
ENERGY-BASED GENERATIVE MODEL,0.41773778920308485,"The common classification tasks involving C classes are often solved by using a neural network
291"
ENERGY-BASED GENERATIVE MODEL,0.4190231362467866,"fθ : Rd →RC, which maps each input data point x ∈Rd to C-categorical outputs. The output are
292"
ENERGY-BASED GENERATIVE MODEL,0.4203084832904884,"then used to define a categorical distribution of class y through a softmax function:
293"
ENERGY-BASED GENERATIVE MODEL,0.42159383033419023,"pθ(y | x) =
exp(fθ(x)[y])
P"
ENERGY-BASED GENERATIVE MODEL,0.42287917737789205,"y′ exp(fθ(x)[y′]),"
ENERGY-BASED GENERATIVE MODEL,0.4241645244215938,"where fθ(x)[y] represents the y-th component of fθ(x), corresponding to the logit for class y. Once
294"
ENERGY-BASED GENERATIVE MODEL,0.4254498714652956,"the classifier is trained, p′
θ(y | x) = exp(fθ(x)[y]) could be used to sample for a specific class y.
295"
ENERGY-BASED GENERATIVE MODEL,0.4267352185089974,"(a) Class of airplanes.
(b) Class of birds."
ENERGY-BASED GENERATIVE MODEL,0.42802056555269924,"(c) Class of frogs.
(d) Class of dogs."
ENERGY-BASED GENERATIVE MODEL,0.42930591259640105,Figure 7: Sample from joint energy model by different classes (Left: HMC; Right: FTH).
ENERGY-BASED GENERATIVE MODEL,0.4305912596401028,"We compare FTH with the standard HMC method using a limited number of sampling steps, con-
296"
ENERGY-BASED GENERATIVE MODEL,0.4318766066838046,"sistently accepting new proposals based on the potential energy during sampling. It is evident that
297"
ENERGY-BASED GENERATIVE MODEL,0.43316195372750643,"FTH produces higher-quality images than HMC. Additionally, our experiments reveal that FTH tends
298"
ENERGY-BASED GENERATIVE MODEL,0.43444730077120824,"to generate sharper images compared to the other method. This can be attributed to the assumption
299"
ENERGY-BASED GENERATIVE MODEL,0.43573264781491,"that the classifier focuses on the object’s features rather than the entire image. As a result, when the
300"
ENERGY-BASED GENERATIVE MODEL,0.4370179948586118,"prediction probability is high, the features that increase confidence become more prominent, while
301"
ENERGY-BASED GENERATIVE MODEL,0.4383033419023136,"unrelated background elements are filtered out.
302"
ENERGY-BASED SCORE-MATCHING MODELS,0.43958868894601544,"5.3
Energy-Based Score-Matching Models
303"
ENERGY-BASED SCORE-MATCHING MODELS,0.44087403598971725,"As indicated in [12], when two diffusion models are combined into a product model qprod(x) ∝
304"
ENERGY-BASED SCORE-MATCHING MODELS,0.442159383033419,"q1(x)q2(x), problems can arise if the model reversing the diffusion uses a score estimate derived
305"
ENERGY-BASED SCORE-MATCHING MODELS,0.4434447300771208,"by simply adding the score estimates of the two independent models. We use energy-based score-
306"
ENERGY-BASED SCORE-MATCHING MODELS,0.44473007712082263,"matching models to illustrate this issue. It is important to note that such inconsistencies typically
307"
ENERGY-BASED SCORE-MATCHING MODELS,0.44601542416452444,"involve the composition of two or more diffusion models.
308"
SYNTHETIC DATASET,0.4473007712082262,"5.3.1
Synthetic Dataset
309"
SYNTHETIC DATASET,0.448586118251928,"We first show an example of composing two distributions p1(x) and p2(x), as illustrated in the left
310"
SYNTHETIC DATASET,0.4498714652956298,"column of Figure 8. The results show that FTH demonstrates a strong ability to converge to the
311"
SYNTHETIC DATASET,0.45115681233933164,"correct composition, with less particles fall out of the high-density region compared to others.
312"
SYNTHETIC DATASET,0.4524421593830334,Figure 8: Compose sampling with DDPM.
CLEVR DATASET,0.4537275064267352,"5.3.2
CLEVR Dataset
313"
CLEVR DATASET,0.455012853470437,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
CLEVR DATASET,0.45629820051413883,"Figure 9: Generation of cube. The zoomed images
could be found at Figure 19."
CLEVR DATASET,0.45758354755784064,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
CLEVR DATASET,0.4588688946015424,"Figure 10: Generation of sphere and cylinder. The
zoomed images could be found at Figure 20."
CLEVR DATASET,0.4601542416452442,"We use CLEVR dataset from [23] for our gen-
314"
CLEVR DATASET,0.461439588688946,"eration and sampling tasks. The energy model
315"
CLEVR DATASET,0.46272493573264784,"is adopted from [12], and we employ different
316"
CLEVR DATASET,0.4640102827763496,"samplers for generation. The dataset includes
317"
CLEVR DATASET,0.4652956298200514,"three classes: cube, sphere, and cylinder. We
318"
CLEVR DATASET,0.4665809768637532,"explore scenarios where we first sample from
319"
CLEVR DATASET,0.46786632390745503,"only one category and then from two categories.
320"
CLEVR DATASET,0.4691516709511568,"In the first experiment, there is no composition
321"
CLEVR DATASET,0.4704370179948586,"of models. As depicted in Figure 9, it is evident
322"
CLEVR DATASET,0.4717223650385604,"that FTH effectively generates the desired image
323"
CLEVR DATASET,0.4730077120822622,"without any extraneous shapes, whereas both
324"
CLEVR DATASET,0.47429305912596403,"MALA and HMC generate additional shapes.
325"
CLEVR DATASET,0.4755784061696658,"In the second experiment, we combine two in-
326"
CLEVR DATASET,0.4768637532133676,"dependent diffusion models, each trained sepa-
327"
CLEVR DATASET,0.4781491002570694,"rately to generate sphere and cylinder. As shown
328"
CLEVR DATASET,0.4794344473007712,"in Figure 10, it is clear that FTH excels at pro-
329"
CLEVR DATASET,0.480719794344473,"ducing high-quality images with almost no over-
330"
CLEVR DATASET,0.4820051413881748,"lapping between objects, accurately rendering the intended shapes in a pristine manner. In contrast,
331"
CLEVR DATASET,0.4832904884318766,"the other methods generate the undesired shape cube. Additionally, FTH exhibits less noise, indicating
332"
CLEVR DATASET,0.4845758354755784,"greater stability for sampling.
333"
CONCLUSION,0.48586118251928023,"6
Conclusion
334"
CONCLUSION,0.487146529562982,"In this study, we first recognize the significance of incorporating zeroth-order information into the
335"
CONCLUSION,0.4884318766066838,"sampling process, highlighting the common limitations faced by conventional sampling methods.
336"
CONCLUSION,0.4897172236503856,"These limitations include unstable sampling outcomes frequently associated with energy-based
337"
CONCLUSION,0.4910025706940874,"score-matching models, the potential metastability arising from the multi-modal nature of the energy
338"
CONCLUSION,0.4922879177377892,"function, and errors in gradient computation stemming from the complex structure of the composi-
339"
CONCLUSION,0.493573264781491,"tional distribution. Subsequently, we present an innovative approach that leverages parallel HMC
340"
CONCLUSION,0.4948586118251928,"sampling to address the issues. Building upon HMC, we incorporate energy modulation techniques
341"
CONCLUSION,0.4961439588688946,"to enhance the sampling process. Through this approach, our method is able to systematically reduce
342"
CONCLUSION,0.4974293059125964,"the potential energy, leading to substantial advantages in practical implementations of sampling.
343"
REFERENCES,0.4987146529562982,"References
344"
REFERENCES,0.5,"[1] Christophe Andrieu, Éric Moulines, and Francis J Samson. Particle markov chain monte carlo
345"
REFERENCES,0.5012853470437018,"for efficient numerical simulation. Statistical Science, 25(4):332–350, 2010.
346"
REFERENCES,0.5025706940874036,"[2] Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo, July 2018.
347"
REFERENCES,0.5038560411311054,"[3] Charles K. Birdsall and A. Bruce Langdon. Plasma physics via computer simulation. Taylor
348"
REFERENCES,0.5051413881748072,"and Francis, New York, 2005.
349"
REFERENCES,0.506426735218509,"[4] Jérôme Bolte and Edouard Pauwels. A mathematical model for automatic differentiation in
350"
REFERENCES,0.5077120822622108,"machine learning. Advances in Neural Information Processing Systems, 33:10809–10819, 2020.
351"
REFERENCES,0.5089974293059126,"[5] Steve P. Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Handbook of markov
352"
REFERENCES,0.5102827763496144,"chain monte carlo: Hardcover: 619 pages publisher: Chapman and hall/crc press (first edition,
353"
REFERENCES,0.5115681233933161,"may 2011) language: English isbn-10: 1420079417. CHANCE, 25:53 – 55, 2012.
354"
REFERENCES,0.512853470437018,"[6] Augustin Louis Cauchy. Méthode générale pour la résolution des systémes d’équations simul-
355"
REFERENCES,0.5141388174807198,"tanées. Comptes Rendus de l’Académie des Sciences Paris, 25:536–538, 1847.
356"
REFERENCES,0.5154241645244216,"[7] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. T. Chayes,
357"
REFERENCES,0.5167095115681234,"L. Sagun, and R. Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In ICLR,
358"
REFERENCES,0.5179948586118251,"2017.
359"
REFERENCES,0.519280205655527,"[8] Pratik Chaudhari, Carlo Baldassi, Riccardo Zecchina, Stefano Soatto, Ameet Talwalkar,
360"
REFERENCES,0.5205655526992288,"and Adam Oberman.
Parle: parallelizing stochastic gradient descent.
arXiv preprint
361"
REFERENCES,0.5218508997429306,"arXiv:1707.00424, 2017.
362"
REFERENCES,0.5231362467866324,"[9] Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li, and Lawrence Carin. Bridging the
363"
REFERENCES,0.5244215938303342,"gap between stochastic gradient mcmc and stochastic optimization. In Arthur Gretton and
364"
REFERENCES,0.525706940874036,"Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial
365"
REFERENCES,0.5269922879177378,"Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages
366"
REFERENCES,0.5282776349614395,"1051–1060, Cadiz, Spain, 09–11 May 2016. PMLR.
367"
REFERENCES,0.5295629820051414,"[10] Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo.
368"
REFERENCES,0.5308483290488432,"In Proceedings of the 31st International Conference on Machine Learning, pages 1683–1691.
369"
REFERENCES,0.532133676092545,"PMLR, June 2014.
370"
REFERENCES,0.5334190231362468,"[11] Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.
371"
REFERENCES,0.5347043701799485,"In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on
372"
REFERENCES,0.5359897172236504,"Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1683–1691,
373"
REFERENCES,0.5372750642673522,"Bejing, China, 22–24 Jun 2014. PMLR.
374"
REFERENCES,0.538560411311054,"[12] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fer-
375"
REFERENCES,0.5398457583547558,"gus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle:
376"
REFERENCES,0.5411311053984575,"Compositional generation with energy-based diffusion models and mcmc. JMLR.org, 2023.
377"
REFERENCES,0.5424164524421594,"[13] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
378"
REFERENCES,0.5437017994858612,"H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors,
379"
REFERENCES,0.5449871465295629,"Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
380"
REFERENCES,0.5462724935732648,"[14] Tomas Geffner and Justin Domke. MCMC Variational Inference via Uncorrected Hamiltonian
381"
REFERENCES,0.5475578406169666,"Annealing. In Advances in Neural Information Processing Systems, volume 34, pages 639–651.
382"
REFERENCES,0.5488431876606684,"Curran Associates, Inc., 2021.
383"
REFERENCES,0.5501285347043702,"[15] Mark Girolami, Ben Calderhead, and Siu A Chin. Riemann Manifold Langevin and Hamiltonian
384"
REFERENCES,0.5514138817480719,"Monte Carlo.
385"
REFERENCES,0.5526992287917738,"[16] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad
386"
REFERENCES,0.5539845758354756,"Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should
387"
REFERENCES,0.5552699228791774,"treat it like one. In International Conference on Learning Representations, 2020.
388"
REFERENCES,0.5565552699228792,"[17] Ulf Grenander and Michael I. Miller. Representations of Knowledge in Complex Systems.
389"
REFERENCES,0.5578406169665809,"Journal of the Royal Statistical Society. Series B (Methodological), 56(4):549–603, 1994.
390"
REFERENCES,0.5591259640102828,"[18] W. K. Hastings. Monte carlo sampling methods using markov chains and their applications.
391"
REFERENCES,0.5604113110539846,"Biometrika, 57(1):97–109, 1970.
392"
REFERENCES,0.5616966580976864,"[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
393"
REFERENCES,0.5629820051413882,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
394"
REFERENCES,0.5642673521850899,"pages 770–778, 2016.
395"
REFERENCES,0.5655526992287918,"[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
396"
REFERENCES,0.5668380462724936,"H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural
397"
REFERENCES,0.5681233933161953,"Information Processing Systems, volume 33, pages 6840–6851. Curran Associates, Inc., 2020.
398"
REFERENCES,0.5694087403598972,"[21] Matthew D. Hoffman and Andrew Gelman. The No-U-Turn Sampler: Adaptively Setting Path
399"
REFERENCES,0.570694087403599,"Lengths in Hamiltonian Monte Carlo, November 2011.
400"
REFERENCES,0.5719794344473008,"[22] Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal
401"
REFERENCES,0.5732647814910026,"of Machine Learning Research, 6(24):695–709, 2005.
402"
REFERENCES,0.5745501285347043,"[23] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,
403"
REFERENCES,0.5758354755784062,"and Ross B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary
404"
REFERENCES,0.577120822622108,"visual reasoning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
405"
REFERENCES,0.5784061696658098,"pages 1988–1997, 2016.
406"
REFERENCES,0.5796915167095116,"[24] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science
407"
REFERENCES,0.5809768637532133,"(New York, N.Y.), 220(4598):671–680, 1983.
408"
REFERENCES,0.5822622107969152,"[25] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc Aurelio Ranzato, and Fu Jie Huang. A tutorial
409"
REFERENCES,0.583547557840617,"on energy-based learning. 2006.
410"
REFERENCES,0.5848329048843187,"[26] Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit
411"
REFERENCES,0.5861182519280206,"tests. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd
412"
REFERENCES,0.5874035989717223,"International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
413"
REFERENCES,0.5886889460154242,"Research, pages 276–284, New York, New York, USA, 20–22 Jun 2016. PMLR.
414"
REFERENCES,0.589974293059126,"[27] Yi-An Ma, Tianqi Chen, and Emily Fox. A Complete Recipe for Stochastic Gradient MCMC.
415"
REFERENCES,0.5912596401028277,"In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.,
416"
REFERENCES,0.5925449871465296,"2015.
417"
REFERENCES,0.5938303341902313,"[28] Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I. Jordan. Sampling can be
418"
REFERENCES,0.5951156812339332,"faster than optimization. Proceedings of the National Academy of Sciences, 116(42):20881–
419"
REFERENCES,0.596401028277635,"20885, 2019.
420"
REFERENCES,0.5976863753213367,"[29] Radford M. Neal. MCMC Using Hamiltonian Dynamics. May 2011.
421"
REFERENCES,0.5989717223650386,"[30] G. Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378–384,
422"
REFERENCES,0.6002570694087404,"1981.
423"
REFERENCES,0.6015424164524421,"[31] B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR
424"
REFERENCES,0.602827763496144,"Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.
425"
REFERENCES,0.6041131105398457,"[32] Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions
426"
REFERENCES,0.6053984575835476,"and their discrete approximations. Bernoulli, 2(4):341 – 363, 1996.
427"
REFERENCES,0.6066838046272494,"[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
428"
REFERENCES,0.6079691516709511,"biomedical image segmentation. In International Conference on Medical image computing and
429"
REFERENCES,0.609254498714653,"computer-assisted intervention, pages 234–241. Springer, 2015.
430"
REFERENCES,0.6105398457583547,"[34] Jascha Sohl-Dickstein, Mayur Mudigonda, and Michael DeWeese. Hamiltonian monte carlo
431"
REFERENCES,0.6118251928020566,"without detailed balance. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st
432"
REFERENCES,0.6131105398457584,"International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning
433"
REFERENCES,0.6143958868894601,"Research, pages 719–726, Bejing, China, 22–24 Jun 2014. PMLR.
434"
REFERENCES,0.615681233933162,"[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-
435"
REFERENCES,0.6169665809768637,"pervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei,
436"
REFERENCES,0.6182519280205655,"editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of
437"
REFERENCES,0.6195372750642674,"Proceedings of Machine Learning Research, pages 2256–2265, Lille, France, 07–09 Jul 2015.
438"
REFERENCES,0.6208226221079691,"PMLR.
439"
REFERENCES,0.622107969151671,"[36] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
440"
REFERENCES,0.6233933161953727,"distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and
441"
REFERENCES,0.6246786632390745,"R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
442"
REFERENCES,0.6259640102827764,"Associates, Inc., 2019.
443"
REFERENCES,0.6272493573264781,"[37] Yang Song and Diederik P. Kingma. How to Train Your Energy-Based Models, February 2021.
444"
REFERENCES,0.62853470437018,"[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
445"
REFERENCES,0.6298200514138818,"Ben Poole. Score-based generative modeling through stochastic differential equations. In
446"
REFERENCES,0.6311053984575835,"International Conference on Learning Representations, 2021.
447"
REFERENCES,0.6323907455012854,"[39] Robert Swendsen and Jian-Sheng Wang. Replica monte carlo simulation of spin-glasses.
448"
REFERENCES,0.6336760925449871,"Physical review letters, 57:2607–2609, 12 1986.
449"
REFERENCES,0.6349614395886889,"[40] Y. Teng, W. Gao, F. Chalus, A. Choromanska, D. Goldfarb, and A. Weller. Leader stochastic
450"
REFERENCES,0.6362467866323908,"gradient descent for distributed training of deep learning models. In NeurIPS, 2019.
451"
REFERENCES,0.6375321336760925,"[41] Yunfei Teng, Wenbo Gao, Francois Chalus, Anna Choromanska, Donald Goldfarb, and Adrian
452"
REFERENCES,0.6388174807197944,"Weller. Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models:
453"
REFERENCES,0.6401028277634961,"Extension, April 2022.
454"
REFERENCES,0.6413881748071979,"[42] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics.
455"
REFERENCES,0.6426735218508998,"In Proceedings of the 28th International Conference on International Conference on Machine
456"
REFERENCES,0.6439588688946015,"Learning, ICML’11, page 681–688, Madison, WI, USA, 2011. Omnipress.
457"
REFERENCES,0.6452442159383034,"[43] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
458"
REFERENCES,0.6465295629820051,"benchmarking machine learning algorithms, 2017.
459"
REFERENCES,0.6478149100257069,"[44] Qiwei Ye, Yuxuan Song, Chang Liu, Fangyun Wei, Tao Qin, and Tie-Yan Liu. Particle based
460"
REFERENCES,0.6491002570694088,"stochastic policy optimization. 2021.
461"
REFERENCES,0.6503856041131105,"[45] Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
462"
REFERENCES,0.6516709511568124,"arXiv:1605.07146, 2016.
463"
REFERENCES,0.6529562982005142,"[46] S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaging SGD. In NIPS,
464"
REFERENCES,0.6542416452442159,"2015.
465 466"
REFERENCES,0.6555269922879178,"Follow Hamiltonian Leader: An Efficient
467"
REFERENCES,0.6568123393316195,"Energy-Guided Sampling Method
468"
REFERENCES,0.6580976863753213,"(Supplementary Material)
469 470"
REFERENCES,0.6593830334190232,"A
Additional Discussion for Section 3
471"
REFERENCES,0.6606683804627249,"A.1
Instability & Metastability
472"
REFERENCES,0.6619537275064268,"We now approach this problem from an optimization perspective. There is a strong connection
473"
REFERENCES,0.6632390745501285,"between optimization and sampling, particularly through the principle of simulated annealing [24],
474"
REFERENCES,0.6645244215938303,"which demonstrates how sampling methods can be transformed into optimization techniques.
475"
REFERENCES,0.6658097686375322,"With a slight abuse of notation, we consider the following objective function:
476"
REFERENCES,0.6670951156812339,"U(x) = x[1]2 + 0.01 · x[2]2,"
REFERENCES,0.6683804627249358,"where x ∈R2 and x[i] denotes the ith dimension of x. This is a 2-dimensional optimization problem
477"
REFERENCES,0.6696658097686375,"with a condition number of 100, indicating it is somewhat ill-conditioned.
478"
REFERENCES,0.6709511568123393,"For n particles, the objective function is:
479"
REFERENCES,0.6722365038560412,"Ue(x1, · · · , xn; xl) = n
X"
REFERENCES,0.6735218508997429,"i=1
U(xi) + ρ"
REFERENCES,0.6748071979434447,"2 · ∥xi −xl∥2
2,"
REFERENCES,0.6760925449871465,"where we set ρ = 0.1. We initialize x1 = (2, 2) and x2 = (−1, −3) respectively, and optimize the
480"
REFERENCES,0.6773778920308483,"objective function using the gradient descent method. Note that when n = 1, this method reduces to
481"
REFERENCES,0.6786632390745502,"vanilla gradient descent, while n = 2 incorporates our leader-pulling scheme.
482"
REFERENCES,0.6799485861182519,Figure 11: U(x1) with gradient descent method [6]. The learning rate is set to 0.1.
REFERENCES,0.6812339331619537,"Figure 12: U(x1) with heavy-ball method [31]. The learning rate and momentum are set to 0.02 and
0.9 respectively."
REFERENCES,0.6825192802056556,"From Figure 11, we can see that incorporating the leader-pulling scheme helps improve convergence.
483"
REFERENCES,0.6838046272493573,"This demonstrates that the leader-pulling scheme can address the issue of instability in optimization.
484"
REFERENCES,0.6850899742930592,"However, we also observe that a carefully chosen leader is usually required for our method, which
485"
REFERENCES,0.6863753213367609,"we will leave for future discussion.
486"
REFERENCES,0.6876606683804627,"Furthermore, as shown in Figure 12, the particle using the leader-pulling scheme explores much
487"
REFERENCES,0.6889460154241646,"further compared to the vanilla heavy-ball method. This outcome is expected, as we want the method
488"
REFERENCES,0.6902313624678663,"to enhance exploration and thereby resolve the metastability issue.
489"
REFERENCES,0.6915167095115681,"A.2
Pseudo Stability
490"
REFERENCES,0.6928020565552699,"These challenges are commonly encountered when sampling from compositional models, particularly
491"
REFERENCES,0.6940874035989717,"when one of the distributions is a piecewise-constant distribution with its gradients are zero almost
492"
REFERENCES,0.6953727506426736,"everywhere in its domain. To illustrate this, consider the example π(x) ∝π1(x) · π2(x). Here we
493"
REFERENCES,0.6966580976863753,"consider ∂log π2 equals to zero everywhere.
494"
REFERENCES,0.6979434447300771,"It’s worth noting that while combining distributions in their logarithmic forms is straightforward,
495"
REFERENCES,0.699228791773779,"which leads to log π(x) = log π1(x) + log π2(x) + constant , omitting the constant log π(x) can
496"
REFERENCES,0.7005141388174807,"be readily derived from the individual log π1(x) and log π2(x). However, the composition of their
497"
REFERENCES,0.7017994858611826,"gradients becomes problematic, as the computation of the sub-gradient ∂x log π(x) ̸= ∇x log π1(x)+
498"
REFERENCES,0.7030848329048843,"∂x log π2(x) in general due to the use of automatic differentiation in machine learning [4].
499"
REFERENCES,0.7043701799485861,"In this section, we focus on the disparity between gradient and energy in the context of combining
500"
REFERENCES,0.705655526992288,"two distributions as indicated in Section 3.
501"
REFERENCES,0.7069408740359897,"We analyze a composite probability distribution structured as π(x) ∝π1(x) · π2(x), leading to the
502"
REFERENCES,0.7082262210796915,"construction of two specific distributions:
503"
REFERENCES,0.7095115681233933,"• The first distribution, π1(x), is given by:
504"
REFERENCES,0.7107969151670951,"π1(x) =
1
|X| X"
REFERENCES,0.712082262210797,"µ∈X
N(µ, σ2I),"
REFERENCES,0.7133676092544987,"505
where |X| represents the cardinality of the set X, indicating the total number of elements in
506"
REFERENCES,0.7146529562982005,"X.
507"
REFERENCES,0.7159383033419023,"• The second distribution π2(x), is defined as
508"
REFERENCES,0.7172236503856041,π2(x) = 1x∈ΩY
REFERENCES,0.718508997429306,"Vol(ΩY ),"
REFERENCES,0.7197943444730077,"509
with ΩY being the set where ΩY = {x| d(x, Y ) < ϵ}. In this context, the distance metric d
510"
REFERENCES,0.7210796915167095,"is specified by d(x, Y ) = arg miny∈Y ∥x−y∥2, indicating the minimum Euclidean distance
511"
REFERENCES,0.7223650385604113,"from x to any point in the set Y .
512"
REFERENCES,0.7236503856041131,"Observe that π1 constitutes a smooth distribution, whereas π2 is a piecewise-constant distribution.
513"
REFERENCES,0.7249357326478149,"Consequently, for π2, the gradients are zero almost everywhere. When we consider the expression
514"
REFERENCES,0.7262210796915167,"∇x log π1(x) + ∂x log π2(x), it could be simplified to ∇x log π1(x), which is not equivalent to
515"
REFERENCES,0.7275064267352185,"∂x log π(x) in general.
516"
REFERENCES,0.7287917737789203,"In the subsequent subsections, we present two motivating examples: one in a low-dimensional setting
517"
REFERENCES,0.7300771208226221,"and the other in a high-dimensional context. Throughout these experiments, we set σ2 = 0.002 and
518"
REFERENCES,0.7313624678663239,"ϵ = 0.2. In this section, the outcomes of U-LMC and U-HMC are omitted because both techniques
519"
REFERENCES,0.7326478149100257,"succumb to the issue of misleading gradients by nature, causing worse performance.
520"
REFERENCES,0.7339331619537275,"A.2.1
Low-dimensional Example
521"
REFERENCES,0.7352185089974294,"We propose an example inspired from [12] but in a different setting. In this revision, we begin by
providing a more specific definition for two distributions. For the first distribution π1, we define:"
REFERENCES,0.7365038560411311,"X = {
 
cos (2πi/8) , sin (2πi/8)

| i = 1, 2, . . . , 8},
and for the second distribution π2, we specify:"
REFERENCES,0.7377892030848329,"Y = {
 
cos (2πi/8) , sin (2πi/8)

| i = 2, 4, 6, 8}.
It’s important to note that, by definition, Y is a subset of X.
522"
REFERENCES,0.7390745501285347,"Figure 13: Plot of N = 512 particles of XT for a 4-mode compositional Gaussian mixture model
π ∝π1 · π2 on d = 2. We sample by gradient ∇log π1 and energy π1 · π2. The baseline methods
LMC, HMC and our proposed method FHL generate XT after T = 4000 steps, using the initial
particles X0 = {xi
0} with xi
0 sampled from a common distribution."
REFERENCES,0.7403598971722365,"We perform a comparative study of our methods against established benchmarks, and the visual
523"
REFERENCES,0.7416452442159382,"representations of this comparison can be found in Figure 13. Notably, among the compared methods,
524"
REFERENCES,0.7429305912596401,"FTH distinguishes itself due to its outstanding performance, mainly attributed to its precise adjustment
525"
REFERENCES,0.7442159383033419,"of particle positions. The comparative results highlight that the baseline methods often exhibit the
526"
REFERENCES,0.7455012853470437,"tendency to erroneously converge towards incorrect modes due to the misleading gradients. Although
527"
REFERENCES,0.7467866323907455,"rejection steps of HMC and LMC might mitigate incorrect sampling, particles initialized near
528"
REFERENCES,0.7480719794344473,"high-energy modes struggle to escape this erroneous attraction by misleading gradients.
529"
REFERENCES,0.7493573264781491,"A.2.2
High-dimensional Example
530"
REFERENCES,0.7506426735218509,"We then present a case study in which we generate examples from a particular category within the
531"
REFERENCES,0.7519280205655527,"Fashion MNIST dataset [43]. In this experiment, we select a total of 200 images, with 100 images
532"
REFERENCES,0.7532133676092545,"from the coat category and another 100 from trouser category. We denote the sets of data points from
533"
REFERENCES,0.7544987146529563,"the coat and trouser categories as Xcoat and Xtrouser respectively. Furthermore, we define X as the
534"
REFERENCES,0.7557840616966581,"union of Xcoat and Xtrouser, and Y is set to Xcoat in this case.
535"
REFERENCES,0.7570694087403599,"Figure 14: Sample from a 100-mode compositional Gaussian mixture model π ∝π1 · π2 on d = 784,
where each mode corresponds to a clean image from coat category. We sample by gradient ∇log π1
and energy π1 · π2. For each method, we plot the smallest-energy particle (in terms of U(x) among
all particles in XT ). The correct samples are displayed in the upper-left corner."
REFERENCES,0.7583547557840618,"To increase the difficulty of the sampling task, we initially position each particle at the mean location
536"
REFERENCES,0.7596401028277635,"of Xtrouser. The outcomes of the sampling are depicted in Figure 15.This setup showcases the FHL
537"
REFERENCES,0.7609254498714653,"method’s ability to accurately target and sample from the specified coat category, in contrast to
538"
REFERENCES,0.7622107969151671,"baseline methods that undesirably draw samples from the trouser category.
539"
REFERENCES,0.7634961439588689,"B
Supplementary Experiment
540"
REFERENCES,0.7647814910025706,"B.1
Experiment Setup
541"
REFERENCES,0.7660668380462725,"In Section 5.2, we utilize a pre-trained classifier available on the public GitHub repository at
542"
REFERENCES,0.7673521850899743,"https://github.com/wgrathwohl/JEM. This classifier is a WideResNet model [45] with a depth
543"
REFERENCES,0.7686375321336761,"of 28 and a width of 2.
544"
REFERENCES,0.7699228791773779,"We use a technique called one-step HMC [5] and thus the momentum gets refreshed for each step.
545"
REFERENCES,0.7712082262210797,"More specifically, for both FTH and HMC we set the momentum damping factor to 0.9 and the mass
546"
REFERENCES,0.7724935732647815,"matrix as 0.0042 · I. We take step size as η = 0.2. Since the mass matrix is set to a relatively small
547"
REFERENCES,0.7737789203084833,"value which easily causes the instability of training, we always accept the proposed states based on
548"
REFERENCES,0.7750642673521851,"the potential energy and ignore the kinetic energy.
549"
REFERENCES,0.7763496143958869,"In Section 5.3, we mainly adapted the codes and models from https://github.com/yilundu/
550"
REFERENCES,0.7776349614395887,"reduce_reuse_recycle.
551"
REFERENCES,0.7789203084832905,"For Section 5.3.1, we initially train a 4-layer ResNet as the energy-based score-matching model on p1
552"
REFERENCES,0.7802056555269923,"and p2 independently. During the sampling process, we combine these models. We employ step sizes
553"
REFERENCES,0.781491002570694,"η = {0.002, 0.0002, 0.005, 0.0005} for all methods and the number of leapfrog steps L = {4, 8} for
554"
REFERENCES,0.7827763496143959,"HMC-type methods.
555"
REFERENCES,0.7840616966580977,"For Section 5.3.2, we utilize a U-net architecture [33] as the energy-based score-matching model.
556"
REFERENCES,0.7853470437017995,"This architecture is directly obtained from a pre-trained model available at . For sampling, we use step
557"
REFERENCES,0.7866323907455013,"sizes η = {0.01, 0.035, 0.05, 0.1, 0.2} for all methods and set the number of leapfrog steps L = 4
558"
REFERENCES,0.787917737789203,"for HMC-type methods.
559"
REFERENCES,0.7892030848329049,"B.2
Additional Results
560"
REFERENCES,0.7904884318766067,"B.2.1
Additional Images for Section 5.2
561"
REFERENCES,0.7917737789203085,"(a) Class of cats.
(b) Class of horses."
REFERENCES,0.7930591259640103,Figure 15: Sample from joint energy model by different classes (Left: HMC; Right: FTH).
REFERENCES,0.794344473007712,"B.2.2
Additional Images for Section 5.3.2
562"
REFERENCES,0.7956298200514139,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
REFERENCES,0.7969151670951157,Figure 16: Generation of cylinder.
REFERENCES,0.7982005141388174,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
REFERENCES,0.7994858611825193,Figure 17: Generation of cube and cylinder.
REFERENCES,0.800771208226221,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
REFERENCES,0.8020565552699229,Figure 18: Generation of cube and sphere.
REFERENCES,0.8033419023136247,"B.2.3
Zoomed Images for Section 5.3.2
563"
REFERENCES,0.8046272493573264,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
REFERENCES,0.8059125964010283,Figure 19: Generation of cube.
REFERENCES,0.8071979434447301,"(a) Baseline
(b) MALA
(c) HMC
(d) FTH"
REFERENCES,0.8084832904884319,Figure 20: Generation of sphere and cylinder.
REFERENCES,0.8097686375321337,"C
Supplementary Theorem
564"
REFERENCES,0.8110539845758354,"We now consider a scenario where the leader becomes corrupted, meaning the corrupted leader always
565"
REFERENCES,0.8123393316195373,"reports an unreasonably low energy but it is not actually in the lowest-energy position. In this situation,
566"
REFERENCES,0.8136246786632391,"the particles are optimizing a biased objective function. For simplicity, we consider a d-dimensional
567"
REFERENCES,0.8149100257069408,Gaussian distribution p ∼e−U(x) and its modification q ∼e−ψ(x) with ψ(x) = U(x) + λ
REFERENCES,0.8161953727506427,"2 ∥x −z∥2.
568"
REFERENCES,0.8174807197943444,"We will analyze the Wasserstein distance between p and q for a fixed z ∈Rd as a function of λ > 0.
569"
REFERENCES,0.8187660668380463,"We will demonstrate that even though we sample from the distribution q instead of p, the bias of the
570"
REFERENCES,0.8200514138817481,"sampler (i.e., the distance between p and q) can be controlled by λ and vanishes as λ →0.
571"
REFERENCES,0.8213367609254498,"Assumption 1. U : Rd →R is M-Lipschitz-differentiable, i.e. ∀x, y ∈Rd,
572"
REFERENCES,0.8226221079691517,U(y) ≤U(x) + ∇U(x)T (y −x) + M
REFERENCES,0.8239074550128535,"2 ∥y −x∥2,"
REFERENCES,0.8251928020565553,"and U is m-Strongly-convex, i.e. ∀x, y ∈Rd,
573"
REFERENCES,0.8264781491002571,U(y) ≥U(x) + ∇U(x)T (y −x) + m
REFERENCES,0.8277634961439588,2 ∥y −x∥2.
REFERENCES,0.8290488431876607,"Theorem 1. Let U be the negative logarithmic probability density function of a d-dimensional
Gaussian distribution, which satisfies Assumption 1. Let us define the function ψ(x) as ψ(x) =
U(x) + λ"
REFERENCES,0.8303341902313625,"2 ∥x −z∥2. Given this setup, the Wasserstein-2 distance between the modified Boltzmann
distribution q, characterized by q ∼e−ψ(x), and the original Gaussian distribution p, denoted as
p ∼e−U(x), can be bounded as:"
REFERENCES,0.8316195372750642,"W2(p, q)2 ≤
λ2∥Σ∥
I + λ∥Σ∥∥z −x∗∥2 + d∥Σ∥· "
REFERENCES,0.8329048843187661,"1 −
1
p"
REFERENCES,0.8341902313624678,λ∥Σ∥+ 1 !2
REFERENCES,0.8354755784061697,"where ∥· ∥represents the matrix norm. Obviously, W2(p, q) →0 when λ →0.
574"
REFERENCES,0.8367609254498715,"Proof. By definition, U is m-strongly convex since
575"
REFERENCES,0.8380462724935732,"U(x) = −log

(2π)−k/2 det(Σ)−1/2 exp

−1"
REFERENCES,0.8393316195372751,"2(x −x∗)T Σ−1(x −x∗)
 = k"
REFERENCES,0.8406169665809768,2 log(2π) + 1
REFERENCES,0.8419023136246787,2 log det(Σ) + 1
REFERENCES,0.8431876606683805,2(x −x∗)T Σ−1(x −x∗).
REFERENCES,0.8444730077120822,"The m corresponds to the smallest eigenvalue of Σ−1 which is therefore 1/∥Σ∥. Then
576"
REFERENCES,0.8457583547557841,ψ(x) = k
REFERENCES,0.8470437017994858,2 log(2π) + 1
REFERENCES,0.8483290488431876,2 log det(Σ) + 1
REFERENCES,0.8496143958868895,2(x −x∗)T Σ−1(x −x∗) + λ
REFERENCES,0.8508997429305912,2 (x −z)T (x −z) = 1
REFERENCES,0.8521850899742931,"2

xT (Σ−1 + λI)x −2xT (Σ−1x∗+ λz)

+ constant = 1"
REFERENCES,0.8534704370179949,"2

(x −(Σ−1 + λI)−1(Σ−1x∗+ λz))T (Σ−1 + λI)(x −(Σ−1 + λI)−1(Σ−1x∗+ λz))

+ constant"
REFERENCES,0.8547557840616966,"The last equation was done by completing the square. Thus the new distribution is still a Gaussian
577"
REFERENCES,0.8560411311053985,"distribution, represented as
578"
REFERENCES,0.8573264781491002,"q ∼N
 
(Σ−1 + λI)−1(Σ−1x∗+ λz)), (Σ−1 + λI)−1
."
REFERENCES,0.8586118251928021,"Consequently, the Wasserstein-2 distance can be determined as follows:
579"
REFERENCES,0.8598971722365039,"W2(p, q)2 = ∥µp −µq∥2 + Tr

Σp + Σq −2(Σ1/2
p
ΣqΣ1/2
p
)1/2"
REFERENCES,0.8611825192802056,"In our case µp = x∗, µq = (Σ−1 + λI)−1(Σ−1x∗+ λz), Σq = Σ, Σp = (Σ−1 + λI)−1. Since Σq
and Σp can be jointly diagonalized by some orthonormal basis T,"
REFERENCES,0.8624678663239075,"ΣqΣp = TDqT −1TDpT −1 = TDpDqT −1 = TDpT −1TDqT −1 = ΣpΣq,"
REFERENCES,0.8637532133676092,thus Σq and Σp commute. We can simplify the Wasserstein distance to
REFERENCES,0.8650385604113111,"W2(p, q)2 = ∥µp −µq∥2 + ∥Σ1/2
p
−Σ1/2
q
∥2
F . Then"
REFERENCES,0.8663239074550129,"W2(p, q)2 = ∥(Σ−1 + λI)−1(Σ−1x∗+ λz) −x∗∥2 + ∥Σ1/2 −(Σ−1 + λI)−1/2∥2
F ."
REFERENCES,0.8676092544987146,"Now we bound the first and second term independently. The first term is a direct conclusion from
Theorem 15 in [41],"
REFERENCES,0.8688946015424165,"∥(Σ−1 + λI)−1(Σ−1x∗+ λz) −x∗∥2 ≤
λ2"
REFERENCES,0.8701799485861182,"m(m + λ)∥z −x∗∥2,
where m = ∥Σ−1∥,"
REFERENCES,0.87146529562982,"For the second term. We denote ith eigenvalue of matrix Σ as σi, then ∥Σ∥= maxi σi, such that
580"
REFERENCES,0.8727506426735219,"∥Σ1/2 −(Σ−1 + λI)−1/2∥2
F =
X"
REFERENCES,0.8740359897172236,"i≤d
[σ1/2
i
−(σ−1
i
+ λ)−1/2]2 =
X"
REFERENCES,0.8753213367609255,"i≤d
[√σi · (1 −
1
√λσi + 1)]2"
REFERENCES,0.8766066838046273,≤d∥Σ∥· 
REFERENCES,0.877892030848329,"1 −
1
p"
REFERENCES,0.8791773778920309,λ∥Σ∥+ 1 !2
REFERENCES,0.8804627249357326,"Thus, by combing the two terms together, the total Wassertein distance is bounded by"
REFERENCES,0.8817480719794345,"W2(p, q)2 ≤
λ2"
REFERENCES,0.8830334190231363,"m(m + λ)∥z −x∗∥2 + dM ·

1 −
1
√"
REFERENCES,0.884318766066838,λM + 1 2 581
REFERENCES,0.8856041131105399,"NeurIPS Paper Checklist
582"
CLAIMS,0.8868894601542416,"1. Claims
583"
CLAIMS,0.8881748071979434,"Question: Do the main claims made in the abstract and introduction accurately reflect the
584"
CLAIMS,0.8894601542416453,"paper’s contributions and scope?
585"
CLAIMS,0.890745501285347,"Answer: [Yes]
586"
CLAIMS,0.8920308483290489,"Justification: We clearly indicate the purpose and contribution of our paper.
587"
LIMITATIONS,0.8933161953727506,"2. Limitations
588"
LIMITATIONS,0.8946015424164524,"Question: Does the paper discuss the limitations of the work performed by the authors?
589"
LIMITATIONS,0.8958868894601543,"Answer: [No]
590"
LIMITATIONS,0.897172236503856,"Justification: Our work builds upon previous research, serving as a complementary addition
591"
LIMITATIONS,0.8984575835475579,"to it. However, we shall add further discussion in the future.
592"
THEORY ASSUMPTIONS AND PROOFS,0.8997429305912596,"3. Theory Assumptions and Proofs
593"
THEORY ASSUMPTIONS AND PROOFS,0.9010282776349614,"Question: For each theoretical result, does the paper provide the full set of assumptions and
594"
THEORY ASSUMPTIONS AND PROOFS,0.9023136246786633,"a complete (and correct) proof?
595"
THEORY ASSUMPTIONS AND PROOFS,0.903598971722365,"Answer: [Yes]
596"
THEORY ASSUMPTIONS AND PROOFS,0.9048843187660668,"Justification: We do provide assumption and proof for our theorem.
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9061696658097687,"4. Experimental Result Reproducibility
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9074550128534704,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9087403598971723,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.910025706940874,"of the paper (regardless of whether the code and data are provided or not)?
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9113110539845758,"Answer: [Yes]
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9125964010282777,"Justification: We show the details of how to reproduce the results and provide the codes for
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9138817480719794,"reproduction.
604"
OPEN ACCESS TO DATA AND CODE,0.9151670951156813,"5. Open access to data and code
605"
OPEN ACCESS TO DATA AND CODE,0.916452442159383,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
606"
OPEN ACCESS TO DATA AND CODE,0.9177377892030848,"tions to faithfully reproduce the main experimental results, as described in supplemental
607"
OPEN ACCESS TO DATA AND CODE,0.9190231362467867,"material?
608"
OPEN ACCESS TO DATA AND CODE,0.9203084832904884,"Answer: [Yes]
609"
OPEN ACCESS TO DATA AND CODE,0.9215938303341902,"Justification: We will do it once the paper gets accepted.
610"
OPEN ACCESS TO DATA AND CODE,0.922879177377892,"6. Experimental Setting/Details
611"
OPEN ACCESS TO DATA AND CODE,0.9241645244215938,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
612"
OPEN ACCESS TO DATA AND CODE,0.9254498714652957,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
613"
OPEN ACCESS TO DATA AND CODE,0.9267352185089974,"results?
614"
OPEN ACCESS TO DATA AND CODE,0.9280205655526992,"Answer: [Yes]
615"
OPEN ACCESS TO DATA AND CODE,0.929305912596401,"Justification: Yes we do.
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9305912596401028,"7. Experiment Statistical Significance
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9318766066838047,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9331619537275064,"information about the statistical significance of the experiments?
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9344473007712082,"Answer: [No]
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9357326478149101,"Justification: Due to time constraints, we were unable to include that. However, we plan to
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9370179948586118,"add such statistics in the future.
622"
EXPERIMENTS COMPUTE RESOURCES,0.9383033419023136,"8. Experiments Compute Resources
623"
EXPERIMENTS COMPUTE RESOURCES,0.9395886889460154,"Question: For each experiment, does the paper provide sufficient information on the com-
624"
EXPERIMENTS COMPUTE RESOURCES,0.9408740359897172,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
625"
EXPERIMENTS COMPUTE RESOURCES,0.9421593830334191,"the experiments?
626"
EXPERIMENTS COMPUTE RESOURCES,0.9434447300771208,"Answer: [No]
627"
EXPERIMENTS COMPUTE RESOURCES,0.9447300771208226,"Justification: We primarily focus on proposing a new algorithm, and most of our experiments
628"
EXPERIMENTS COMPUTE RESOURCES,0.9460154241645244,"require only a moderate amount of resources.
629"
CODE OF ETHICS,0.9473007712082262,"9. Code Of Ethics
630"
CODE OF ETHICS,0.9485861182519281,"Question: Does the research conducted in the paper conform, in every respect, with the
631"
CODE OF ETHICS,0.9498714652956298,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
632"
CODE OF ETHICS,0.9511568123393316,"Answer: [Yes]
633"
CODE OF ETHICS,0.9524421593830334,"Justification: Yes we do.
634"
BROADER IMPACTS,0.9537275064267352,"10. Broader Impacts
635"
BROADER IMPACTS,0.9550128534704371,"Question: Does the paper discuss both potential positive societal impacts and negative
636"
BROADER IMPACTS,0.9562982005141388,"societal impacts of the work performed?
637"
BROADER IMPACTS,0.9575835475578406,"Answer: [NA]
638"
BROADER IMPACTS,0.9588688946015425,"Justification: There is no social impact of our work.
639"
SAFEGUARDS,0.9601542416452442,"11. Safeguards
640"
SAFEGUARDS,0.961439588688946,"Question: Does the paper describe safeguards that have been put in place for responsible
641"
SAFEGUARDS,0.9627249357326478,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
642"
SAFEGUARDS,0.9640102827763496,"image generators, or scraped datasets)?
643"
SAFEGUARDS,0.9652956298200515,"Answer: [NA]
644"
SAFEGUARDS,0.9665809768637532,"Justification: There is no such a risk.
645"
LICENSES FOR EXISTING ASSETS,0.967866323907455,"12. Licenses for existing assets
646"
LICENSES FOR EXISTING ASSETS,0.9691516709511568,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
647"
LICENSES FOR EXISTING ASSETS,0.9704370179948586,"the paper, properly credited and are the license and terms of use explicitly mentioned and
648"
LICENSES FOR EXISTING ASSETS,0.9717223650385605,"properly respected?
649"
LICENSES FOR EXISTING ASSETS,0.9730077120822622,"Answer: [NA]
650"
LICENSES FOR EXISTING ASSETS,0.974293059125964,"Justification: No we don’t have such a problem.
651"
NEW ASSETS,0.9755784061696658,"13. New Assets
652"
NEW ASSETS,0.9768637532133676,"Question: Are new assets introduced in the paper well documented and is the documentation
653"
NEW ASSETS,0.9781491002570694,"provided alongside the assets?
654"
NEW ASSETS,0.9794344473007712,"Answer: [NA]
655"
NEW ASSETS,0.980719794344473,"Justification: No we don’t have such a problem.
656"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820051413881749,"14. Crowdsourcing and Research with Human Subjects
657"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832904884318766,"Question: For crowdsourcing experiments and research with human subjects, does the paper
658"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845758354755784,"include the full text of instructions given to participants and screenshots, if applicable, as
659"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858611825192802,"well as details about compensation (if any)?
660"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987146529562982,"Answer: [NA]
661"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884318766066839,"Justification: No we don’t have such a problem.
662"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897172236503856,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
663"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910025706940874,"Subjects
664"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922879177377892,"Question: Does the paper describe potential risks incurred by study participants, whether
665"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993573264781491,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
666"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948586118251928,"approvals (or an equivalent approval/review based on the requirements of your country or
667"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961439588688946,"institution) were obtained?
668"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974293059125964,"Answer: [NA]
669"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987146529562982,"Justification: No we don’t have such a problem.
670"
