Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021929824561403508,"Multi-View Representation Learning (MVRL) aims to learn a unified represen-
tation of an object from multi-view data. Deep Canonical Correlation Analysis
(DCCA) and its variants share simple formulations and demonstrate state-of-the-
art performance. However, with extensive experiments, we observe the issue of
model collapse, i.e., the performance of DCCA-based methods will drop drasti-
cally when training proceeds. The model collapse issue could significantly hinder
the wide adoption of DCCA-based methods because it is challenging to decide
when to early stop. To this end, we develop NR-DCCA, which is equipped with a
novel noise regularization approach to prevent model collapse. Theoretical anal-
ysis shows that the Correlation Invariant Property is the key to preventing model
collapse, and our noise regularization forces the neural network to possess such
a property. A framework to construct synthetic data with different common and
complementary information is also developed to compare MVRL methods com-
prehensively. The developed NR-DCCA outperforms baselines stably and con-
sistently in both synthetic and real-world datasets, and the proposed noise regu-
larization approach can also be generalized to other DCCA-based methods such
as DGCCA. Our code will be released at https://github.com/Umaruchain/NR-
DCCA.git."
INTRODUCTION,0.0043859649122807015,"1
Introduction"
INTRODUCTION,0.006578947368421052,"In recent years, multi-view representation learning (MVRL) has emerged as a core technology
for learning from multi-source data and providing readily useful representations to downstream
tasks (Sun et al. 2023, Yan et al. 2021), and it has achieved tremendous success in various ap-
plications, such as video surveillance (Guo et al. 2015, Feichtenhofer et al. 2016, Deepak, K. et al.
2021), medical diagnosis (Wei et al. 2019, Xu et al. 2020) and social media (Srivastava & Salakhut-
dinov 2012, Karpathy & Fei-Fei 2015, Mao et al. 2014, Fan et al. 2020). Specifically, multi-source
data can be collected from the same object, and each data source can be regarded as one view of
the object. For instance, an object can be described simultaneously through texts, videos, and au-
dio, which contain both common and complementary information of the object (Yan et al. 2021,
Zhang, Liu & Fu 2019, Hwang et al. 2021, Geng et al. 2021), and the MVRL aims to learn a unified
representation of the object from the multi-view data."
INTRODUCTION,0.008771929824561403,∗Corresponding author.
INTRODUCTION,0.010964912280701754,"The key challenge of MVRL is to learn the intricate relationships of different views. The Canoni-
cal Correlation Analysis (CCA), which is one of the early and representative methods for MVRL,
transforms all the views into a unified space by maximizing their correlations (Hotelling 1992, Horst
1961, Hardoon et al. 2004, Lahat et al. 2015, Yan et al. 2023, Sun et al. 2023). Through correlation
maximization, CCA can identify the common information between different views and extract them
to form the representation of the object. On top of CCA, Linear CCA, and DCCA maximize the
correlation defined by CCA through gradient descent, while the former uses an affine transforma-
tion and the latter uses Deep Neural Networks (DNNs). (Andrew et al. 2013). Indeed, there are
quite a few variants of DCCA, such as DGCCA (Benton et al. 2017), DCCAE (Wang et al. 2015),
DVCCA (Wang et al. 2016), DTCCA (Wong et al. 2021) and DCCA GHA (Chapman et al. 2022)."
INTRODUCTION,0.013157894736842105,"However, extensive experimentation reveals that DCCA-based methods typically excel during the
initial stages of training but suffer a significant decline in performance as training progresses.
This phenomenon is defined as model collapse within the context of DCCA. Notably, our definition
is grounded in the performance of the learned representations on downstream tasks. Previous studies
found that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (An-
drew et al. 2013, De Bie & De Moor 2003). Nevertheless, they did not further explore whether
merely guaranteeing that the full-rank representations can guarantee that the weight matrices are
full-rank."
INTRODUCTION,0.015350877192982455,"Though early stopping could be adopted to prevent model collapse (Prechelt 1998, Yao et al. 2007),
it remains challenging when to stop. The model collapse issue of DCCA-based methods prevents the
adoption in large models, and currently, many applications still use simple concatenation to combine
different views (Yan et al. 2021, Zheng et al. 2020, Nie et al. 2017). Therefore, how to develop a
DCCA-based MVRL method free of model collapse remains an interesting and open question."
INTRODUCTION,0.017543859649122806,"In this work, we demonstrate that both representations and weight matrices of Linear CCA are
full-rank whereas DCCA only guarantees that representations are full-rank but not for the weight
matrices. Considering that Linear CCA does not show the model collapse while DCCA does, we
conjecture that the root cause of the model collapse in DCCA is that the weight matrices in DNNs
tend to be low-rank. A wealth of research supports this assertion, both theoretically and empiri-
cally, demonstrating that over-parameterized DNNs are predisposed to discovering low-rank solu-
tions (Jing et al. 2021, Saxe et al. 2019, Soudry et al. 2018, Dwibedi et al. 2021). If the weight
matrices in DNNs tend to be low-rank, it means that the weight matrices are highly self-related and
redundant, which limits the expressiveness of DNNs and thus affects the quality of representations."
INTRODUCTION,0.019736842105263157,"Therefore, this paper develops NR-DCCA, a DCCA-based method equipped with a generalized
noise regularization (NR) approach. The NR approach ensures that the correlation with random data
is invariant before and after the transformation, which we define as the Correlation Invariant Prop-
erty (CIP). It is also verified that the NR approach can be applied to other DCCA-based methods.
Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate the
consistent outperformance and stability of the developed NR-DCCA method."
INTRODUCTION,0.021929824561403508,"From a theoretical perspective, we derive the equivalent conditions between the full-rank property
and CIP of the weight matrix. By forcing DNNs to possess CIP and thus mimicking the behavior
of Linear CCA, we introduce random data to constrain the weight matrices in DNNs and expect to
avoid them being redundant and thus prevent model collapse."
INTRODUCTION,0.02412280701754386,"In summary, our contributions are four-fold:"
INTRODUCTION,0.02631578947368421,"• The model collapse issue in DCCA-based methods for MVRL is identified, demonstrated,
and explained."
INTRODUCTION,0.02850877192982456,"• A simple yet effective noise regularization approach is proposed and NR-DCCA is devel-
oped to prevent model collapse. Comprehensive experiments using both synthetic datasets
and real-world datasets demonstrate the consistent outperformance and stability of the de-
veloped NR-DCCA."
INTRODUCTION,0.03070175438596491,"• Rigorous proofs are provided to demonstrate that CIP is the equal condition of the full-rank
weight matrix, which justifies the developed NR approach from a theoretical perspective."
INTRODUCTION,0.03289473684210526,"• A novel framework is proposed to construct synthetic data with different common and
complementary information for comprehensively evaluating MVRL methods."
RELATED WORKS,0.03508771929824561,"2
Related Works"
MULTI-VIEW REPRESENTATION LEARNING,0.03728070175438596,"2.1
Multi-view representation learning"
MULTI-VIEW REPRESENTATION LEARNING,0.039473684210526314,"MVRL aims to uncover relationships among multi-view data in an unsupervised manner, thereby
obtaining semantically rich representations that can be utilized for various downstream tasks (Sun
et al. 2023, Yan et al. 2021). Several works have been proposed to deal with MVRL from differ-
ent aspects. DMF-MVC (Zhao et al. 2017) utilizes deep matrix factorization to extract a shared
representation from multiple views. MDcR (Zhang, Fu, Hu, Zhu & Cao 2016) maps each view to
a lower-dimensional space and applies kernel matching to enforce dependencies across the views.
CPM-Nets (Zhang, Han, Fu, Zhou, Hu et al. 2019) formalizes the concept of partial MVRL and
many works have been proposed for such issue (Zhang et al. 2020, Tao et al. 2019, Li et al. 2022,
Yin & Sun 2021). AE2-Nets (Zhang, Liu & Fu 2019) utilizes a two-level autoencoder framework
to obtain a comprehensive representation of multi-view data. DUA-Nets (Geng et al. 2021) takes a
generative modeling perspective and dynamically estimates the weights for different views. MVT-
CAE (Hwang et al. 2021) explores MVRL from an information-theoretic perspective, which can
capture the shared and view-specific factors of variation by maximizing or minimizing specific total
correlation. Our work focuses on CCA as a simple, classic, and theoretically sound approach as it
can still achieve state-of-the-art performance consistently."
CCA AND ITS VARIANTS,0.041666666666666664,"2.2
CCA and its variants"
CCA AND ITS VARIANTS,0.043859649122807015,"Canonical Correlation Analysis (CCA) projects the multi-view data into a unified space by maxi-
mizing their correlations (Hotelling 1992, Horst 1961, Hardoon et al. 2004, Lahat et al. 2015, Yan
et al. 2023, Sun et al. 2023). It has been widely applied in various scenarios that involve multi-
view data, including dimension reduction (Zhang, Zhang, Pan & Zhang 2016, Sun, Ceran & Ye
2010, Avron et al. 2013), classification (Kim et al. 2007, Sun, Ji & Ye 2010), and clustering (Fern
et al. 2005, Chang & Lin 2011). To further enhance the nonlinear transformability of CCA, Kernel
CCA (KCCA) uses kernel methods, while Deep CCA (DCCA) employs DNNs. Since DNNs is
parametric and can take advantage of large amounts of data for training, numerous DCCA-based
methods have been proposed. Benton et al. (2017) utilizes DNNs to optimize the objective of Gen-
eralized CCA, to reveal connections between multiple views more effectively. To better preserve
view-specific information, Wang et al. (2015) introduces the reconstruction errors of autoencoders
to DCCA. Going a step further, Wang et al. (2016) proposes Variational CCA and utilizes dropout
and private autoencoders to project common and view-specific information into two distinct spaces.
Furthermore, many studies are exploring efficient methods for computing the correlations between
multi-view data when dealing with more than two views such as MCCA, GCCA, and TCCA (Horst
1961, Nielsen 2002, Kettenring 1971, Hwang et al. 2021). Some research focuses on improving the
efficiency of computing CCA by avoiding the need for singular value decomposition (SVD) (Chang
et al. 2018, Chapman et al. 2022). However, the model collapse issue of DCCA-based methods has
not been explored and addressed."
NOISE REGULARIZATION,0.046052631578947366,"2.3
Noise Regularization"
NOISE REGULARIZATION,0.04824561403508772,"Noise regularization is a pluggable approach to regularize the neural networks during train-
ing (Bishop 1995, An 1996, Sietsma & Dow 1991, Gong et al. 2020). In supervised tasks, Sietsma &
Dow (1991) might be the first to propose that, by adding noise to the train data, the model will gener-
alize well on new unseen data. Moreover, Bishop (1995), Gong et al. (2020) analyze the mechanism
of the noise regularization, and He et al. (2019), Gong et al. (2020) indicate that noise regularization
can also be used for adversarial training to improve the generalization of the network. In unsuper-
vised tasks, Poole et al. (2014) systematically explores the role of noise injection at different layers
in autoencoders, and distinct positions of noise perform specific regularization tasks. However,
how to make use of noise regularization for DCCA-based methods, especially for preventing model
collapse, has not been studied."
PRELIMINARIES,0.05043859649122807,"3
Preliminaries"
PRELIMINARIES,0.05263157894736842,"In this section, we will explain the objectives of the MVRL and then introduce Linear CCA and
DCCA as representatives of the CCA-based methods and DCCA-based methods, respectively.
Lastly, the model collapse issue in DCCA is demonstrated."
SETTINGS FOR MVRL,0.05482456140350877,"3.1
Settings for MVRL"
SETTINGS FOR MVRL,0.05701754385964912,"Suppose the set of datasets from K different sources that describe the same object is represented
by X, and we define X = {X1, · · · , Xk, · · · , XK}, Xk ∈Rdk×n, where xk represents the k-th
view (k-th data source), n is the sample size, and dk represents the feature dimension for the k-th
view. And we use X′
k to denote the transpose of Xk. We take the Caltech101 dataset as an example
and the training set has 6400 images. One image has been fed to three different feature extractors
producing three features: a 1984-d HOG feature, a 512-d GIST feature, and a 928-d SIFT feature.
Then for this dataset, we have X1 ∈R1984×6400, X2 ∈R512×6400 , X3 ∈R928×6400."
SETTINGS FOR MVRL,0.05921052631578947,"The objective of MVRL is to learn a transformation function Ψ that projects the multi-view data
X to a unified representation Z ∈Rm×n, where m represents the dimension of the representation
space, as shown below:
Z = Ψ(X) = Ψ(X1, · · · , Xk, · · · , XK).
(1)
After applying Ψ for representation learning, we expect that the performance of using Z would be
better than directly using X for various downstream tasks."
CANONICAL CORRELATION ANALYSIS,0.06140350877192982,"3.2
Canonical Correlation Analysis"
CANONICAL CORRELATION ANALYSIS,0.06359649122807018,"Among various MVRL methods, CCA projects the multi-view data into a common space by maxi-
mizing their correlations. We first define the correlation between the two views as follows:"
CANONICAL CORRELATION ANALYSIS,0.06578947368421052,"Corr(W1X1, W2X2) = tr((Σ−1/2
11
Σ12Σ−1/2
22
)′Σ−1/2
11
Σ12Σ−1/2
22
)1/2
(2)"
CANONICAL CORRELATION ANALYSIS,0.06798245614035088,"where tr denotes the matrix trace, Σ11, Σ22 represent the self-covariance matrices of the projected
views, and Σ12 is the cross-covariance matrix between the projected views (D’Agostini 1994, An-
drew et al. 2013). The correlation between the two projected views can be regarded as the sum of
all singular values of the normalized cross-covariance (Hotelling 1992, Anderson et al. 1958)."
CANONICAL CORRELATION ANALYSIS,0.07017543859649122,"For multiple views, their correlation is defined as the summation of all the pairwise correla-
tions (Nielsen 2002, Kettenring 1971), which is shown as follows:"
CANONICAL CORRELATION ANALYSIS,0.07236842105263158,"Corr(W1X1, · · · , WkXk, · · · , WKXK) =
X"
CANONICAL CORRELATION ANALYSIS,0.07456140350877193,"k<j
Corr(WkXk, WjXj).
(3)"
CANONICAL CORRELATION ANALYSIS,0.07675438596491228,"Essentially, Linear CCA searches for the linear transformation matrices {Wk}k that maximize cor-
relation among all the views. Mathematically, it can be represented as follows (Wang et al. 2015):"
CANONICAL CORRELATION ANALYSIS,0.07894736842105263,"{W ∗
k }k = arg max
{Wk}k Corr(W1X1, · · · , WkXk, · · · , WKXK).
(4)"
CANONICAL CORRELATION ANALYSIS,0.08114035087719298,"Once W ∗
k is obtained by backpropagation, the multi-view data are projected into a unified space.
Lastly, all projected data are concatenated to obtain Z = [W ∗
1 X1; · · · ; W ∗
k Xk; · · · ; W ∗
KXK] for
downstream tasks."
CANONICAL CORRELATION ANALYSIS,0.08333333333333333,"As an extension of linear CCA, DCCA employs neural networks to capture the nonlinear relationship
among multi-view data. The only difference between DCCA and Linear CCA is that the linear
transformation matrix Wk is replaced by multi-layer perceptrons (MLP). Specifically, each Wk is
replaced by a neural network fk, which can be viewed as a nonlinear transformation. Similar to
Linear CCA, the goal of DCCA is to solve the following optimization problem:"
CANONICAL CORRELATION ANALYSIS,0.08552631578947369,"{f ∗
k}k = arg max
{fk}k Corr (f1(X1), · · · , fk(Xk), · · · , fK(XK)) .
(5)"
CANONICAL CORRELATION ANALYSIS,0.08771929824561403,"The parameters in Linear CCA and DCCA are both updated through backpropagation (An-
drew et al. 2013, Wang et al. 2015).
Again, the unified representation is obtained by Z =
[f ∗
1 (X1); · · · ; f ∗
k(Xk); · · · ; f ∗
K(XK)] for downstream tasks."
MODEL COLLAPSE OF DCCA,0.08991228070175439,"4
Model Collapse of DCCA"
MODEL COLLAPSE OF DCCA,0.09210526315789473,"Despite exhibiting promising performance, DCCA shows a significant decline in performance as
the training proceeds. We define this decline-in-performance phenomenon as the model collapse of
DCCA."
MODEL COLLAPSE OF DCCA,0.09429824561403509,"Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCA
are full-rank (Andrew et al. 2013, De Bie & De Moor 2003). However, we further demonstrate
that both representations and weight matrices of Linear CCA are full-rank whereas DCCA only
guarantees that representations are full-rank but not for the weight matrices. Given that Linear CCA
has only a single layer of linear transformation Wk and the representations WkXk are constrained to
be full-rank by the loss function, Wk in Linear CCA is full-rank (referred to Lemma 4 and assume
that Wk is a square matrix and Xk is full-rank). As for DCCA, we consider a simple case when
fk(Xk) = Relu(WkXk), and fk is a single-layer network and uses an element-wise Relu activation
function. Only the representations Relu(WkXk) are constrained to be full-rank, and hence we
cannot guarantee that WkXk is full-rank. For example, when Relu(WkXk) =
  1, 0
0, 1

, it is clear
that this is a matrix of rank 2, but in fact WkXk can be
  1,
−1
−1,
1

, and this is not full-rank. This
reveals that the neural network fk is overfitted on Xk, i.e., making representations Relu(WkXk) to
be full-rank with the constraint of its loss function, rather than Wk itself being full-rank (verified in
Appendix A.5.1)."
MODEL COLLAPSE OF DCCA,0.09649122807017543,"Thus, we hypothesize that model collapse in DCCA arises primarily due to the low-rank nature
of the DNN weight matrices. To investigate this, we analyze the eigenvalue distributions of the
first linear layer’s weight matrices in both DCCA and NR-DCCA across various training epochs
on synthetic datasets. Figure 1 illustrates that during the initial training phase (100th epoch), the
eigenvalues decay slowly for both DCCA and NR-DCCA. However, by the 1200th epoch, DCCA
exhibits a markedly faster decay in eigenvalues compared to NR-DCCA. This observation suggests a
synchronization between model collapse in DCCA and increased redundancy of the weight matrices.
For more details on the experimental setup and results, please refer to Section 6.2."
MODEL COLLAPSE OF DCCA,0.09868421052631579,"(a) 100-th epoch (DCCA)
(b) 100-th epoch (NR-DCCA)"
MODEL COLLAPSE OF DCCA,0.10087719298245613,"(c) 1200-th epoch (DCCA)
(d) 1200-th epoch (NR-DCCA)"
MODEL COLLAPSE OF DCCA,0.10307017543859649,"Figure 1: Eigenvalue distributions of the first linear layer’s weight matrices in the encoder of 1-st
view."
MODEL COLLAPSE OF DCCA,0.10526315789473684,"5
DCCA with Noise Regularization (NR-DCCA)"
METHOD,0.1074561403508772,"5.1
Method"
METHOD,0.10964912280701754,"Based on the discussions in previous sections, we present NR-DCCA, which makes use of the noise
regularization approach to prevent model collapse in DCCA. Indeed, the developed noise regular-
ization approach can be applied to variants of DCCA methods, such as Deep Generalized CCA
(DGCCA) (Benton et al. 2017). An overview of the NR-DCCA framework is presented in Figure 2."
METHOD,0.1118421052631579,"Figure 2: Illustration of NR-DCCA. We take the CUB dataset as an example: similar to DCCA,
the k-th view Xk is transformed using fk to obtain new representation fk(Xk) and then maximize
the correlation between new representations. Additionally, for the k-th view, we incorporate the
proposed NR loss to regularize fk."
METHOD,0.11403508771929824,"The key idea in NR-DCCA is to generate a set of i.i.d Gaussian white noise, denoted as A =
{A1, · · · , Ak, · · · , AK}, Ak ∈Rdk×n, with the same shape as the multi-view data Xk. In Lin-
ear CCA, the correlation with noise is invariant to the linear transformation Wk: Corr(Xk, Ak) =
Corr(WkXk, WkAk) (rigorous proof provided in Theorem 1). However, for DCCA, Corr(Xk, Ak)
might not equal Corr(fk(Xk), fk(Ak)) because the powerful neural networks fk have overfitted
to the maximization program in DCCA and the weight matrices have been highly self-related.
Therefore, we enforce the DCCA to mimic the behavior of Linear CCA by adding an NR loss
ζk = |Corr(fk(Xk), fk(Ak)) −Corr(Xk, Ak)|, and hence the formulation of NR-DCCA is:"
METHOD,0.1162280701754386,"{f ∗
k}k = arg max
{fk}k Corr (f1(X1), · · · , fK(XK)) −α K
X"
METHOD,0.11842105263157894,"k=1
ζk.
(6)"
METHOD,0.1206140350877193,"where α is the hyper-parameter weighing the NR loss. NR-DCCA can be trained through backprop-
agation with the randomly generated A in each epoch, and the unified representation is obtained
directly using {f ∗
k}k in the same manner as DCCA."
THEORETICAL ANALYSIS,0.12280701754385964,"5.2
Theoretical Analysis"
THEORETICAL ANALYSIS,0.125,"In this section, we provide the rationale for why the developed noise regularization can help to
prevent the weight matrices from being low-rank and thus model collapse. Moreover, we prove the
effect of full-rank weight matrices on the representations, which provides a tool to empirically verify
the full-rank property of weight matrices by the quality of representations."
THEORETICAL ANALYSIS,0.12719298245614036,"Utilizing a new Moore-Penrose Inverse (MPI)-based (Petersen et al. 2008) form of Corr in CCA,
we discover that the full-rank property of Wk is equal to CIP:
Theorem 1 (Correlation Invariant Property (CIP) of Wk) Given Wk is a square matrix for any
k and ηk = |Corr(WkXk, WkAk) −Corr(Xk, Ak)|, we have ηk = 0 (i.e. CIP) ⇐⇒
Wk is
full-rank."
THEORETICAL ANALYSIS,0.12938596491228072,"Similarly, we say fk possess CIP if ζk = 0. Under Linear CCA, it is redundant to introduce the NR
approach and force Wk to possess CIP, since forcing WkXk to be full-rank is sufficient to ensure that
Wk is full-rank. However, in DCCA, fk is overfitted on Xk, i.e., making representations fk(Xk)
to be full-rank, rather than weight matrices in fk being full-rank. By forcing fk to possess CIP and
thus mimicking the behavior of Linear CCA, the NR approach constrains the weight matrices to be
full-rank and less redundant and thus prevents model collapse."
THEORETICAL ANALYSIS,0.13157894736842105,"Next, we show that full-rank weight matrices (i.e., CIP) can greatly affect the quality of representa-
tions."
THEORETICAL ANALYSIS,0.1337719298245614,"Theorem 2 (Effects of CIP on the obtained representations) For any k, if Wk is a square matrix
and CIP holds for Wk (i.e. Wk is full-rank), WkXk holds that:"
THEORETICAL ANALYSIS,0.13596491228070176,"min
Pk ∥PkWkXk −Xk∥F = 0
(7)"
THEORETICAL ANALYSIS,0.13815789473684212,"min
Qk ∥QkWk(Xk + Ak) −WkXk∥F ≤√n∥WkAk∥F , E(∥WKAk∥2
F ) = ∥Wk∥2
F
(8)"
THEORETICAL ANALYSIS,0.14035087719298245,"where ∥·∥F denotes the Frobenius norm and ϵ is a small positive threshold. Pk and Qk are searched
weight matrices of k-th view to recover the input and discard noise, respectively. And we refer
∥PkWkXk −Xk∥F and ∥QkWk(Xk + Ak) −WkXk∥F as reconstruction loss and denosing loss."
THEORETICAL ANALYSIS,0.1425438596491228,"Theorem 2 suggests that the obtained representation is of low reconstruction loss and denoising
loss. Low reconstruction loss suggests that the representations can be linearly reconstructed to the
inputs. This implies that Wk preserves distinct and essential features of the input data, which is a
desirable property to avoid model collapse since it ensures that the model captures and retains the
whole modality of data (Zhang, Liu & Fu 2019, Tschannen et al. 2018, Tian & Zhang 2022). Low
denoising loss implies that the model’s representation is robust to noise, which means that small
perturbations in the input do not lead to significant changes in the output. This condition can be
seen as a form of regularization that prevents overfitting the noise in the data (Zhou & Paffenroth
2017, Yan et al. 2023, Staerman et al. 2023). Additionally, the theorem also suggests that the rank
of weight matrices is a good indicator to assess the quality of representations, which coincides with
existing literature (Kornblith et al. 2019, Raghu et al. 2021, Garrido et al. 2023, Nguyen et al. 2020,
Agrawal et al. 2022)."
NUMERICAL EXPERIMENTS,0.14473684210526316,"6
Numerical Experiments"
NUMERICAL EXPERIMENTS,0.14692982456140352,"We conduct extensive experiments on both synthetic and real-world datasets to answer the following
research questions:"
NUMERICAL EXPERIMENTS,0.14912280701754385,"• RQ1: How can we construct synthetic datasets to evaluate the MVRL methods compre-
hensively?"
NUMERICAL EXPERIMENTS,0.1513157894736842,• RQ2: Does NR-DCCA avoid model collapse across all synthetic MVRL datasets?
NUMERICAL EXPERIMENTS,0.15350877192982457,• RQ3: Does NR-DCCA perform consistently in real-world datasets?
NUMERICAL EXPERIMENTS,0.15570175438596492,"We follow the protocol described in Hwang et al. (2021) for evaluating the MVRL methods. For each
dataset, we construct a training dataset and a test dataset. The encoders of all MVRL methods are
trained on the training dataset. Subsequently, we encode the test dataset to obtain the representation,
which will be evaluated in downstream tasks. We employ Ridge Regression (Hoerl & Kennard
1970) for the regression task and use R2 as the evaluation metric. For the classification task, we use
a Support Vector Classifier (SVC) (Chang & Lin 2011) and report the average F1 scores. All tasks
are evaluated using 5-fold cross-validation, and the reported results correspond to the average values
of the respective metrics."
NUMERICAL EXPERIMENTS,0.15789473684210525,"For a fair comparison, we use the same architectures of MLPs for all D(G)CCA methods. To be spe-
cific, for the synthetic dataset, which is simple, we employ only one hidden layer with a dimension
of 256. For the real-world dataset, we use MLPs with three hidden layers, and the dimension of the
middle hidden layer is 1024. We further demonstrate that increasing the depth of MLPs further ac-
celerates the mod collapse of DCCA, while NR-DCCA maintains a stable performance in Appendix
A.7."
NUMERICAL EXPERIMENTS,0.1600877192982456,"Baseline methods include CONCAT, PRCCA (Tuzhilina et al. 2023), KCCA (Akaho 2006),
Linear CCA Wang et al. (2015),Linear GCCA,DCCA (Andrew et al. 2013),DCCA EY,
DCCA GHA (Chapman et al. 2022), DGCCA (Benton et al. 2017), DCCAE/DGCCAE (Wang
et al. 2015), DCCA PRIVATE/DGCCA PRIVATE (Wang et al. 2016), and MVTCAE (Hwang
et al. 2021)."
NUMERICAL EXPERIMENTS,0.16228070175438597,"It is important to note that our proposed NR approach requires the noise matrix employed to be
full-rank, which is compatible with several common continuous noise distributions. In our primary
experiments, we utilize Gaussian white noise. Additionally, as demonstrated in Appendix A.6,
uniformly distributed noise is also effective in our NR approach."
NUMERICAL EXPERIMENTS,0.16447368421052633,"Details of the experiment settings including datasets and baselines are presented in Appendix A.3.
Hyper-parameter settings, including ridge regularization of DCCA, α of NR, are discussed in Ap-
pendix A.5. We also analyze the computational complexity of different DCCA-based methods in
Appendix A.12 and the learned representations are visualized in Appendix A.9. In the main paper,
we mainly compare Linear CCA, DCCA-based methods, and NR-DCCA while other MVRL meth-
ods are discussed in Appendix A.11. The results related to DGCCA and are similar and presented
in Appendix A.10."
NUMERICAL EXPERIMENTS,0.16666666666666666,"Figure 3: Construction of a synthetic dataset. This example consists of 2 views and n objects, and
the common rate is 0%."
NUMERICAL EXPERIMENTS,0.16885964912280702,"6.1
Construction of synthetic datasets (RQ1)"
NUMERICAL EXPERIMENTS,0.17105263157894737,"We construct synthetic datasets to assess the performance of MVRL methods, and the framework
is illustrated in Figure 3. We believe that the multi-view data describes the same object, which
is represented by a high-dimensional embedding Gd×n, where d is the feature dimension and n
is the size of the data, and we call it God Embedding. Each view of data is regarded as a non-
linear transformation of part (or all) of G. For example, we choose K = 2, d = 100, and then
X1 = ϕ1(G[0 : 50 + CR/2, :]), X2 = ϕx(G[50 −CR/2 : 100], :), where ϕ1 and ϕ2 are non-linear
transformations, and CR is referred to as common rate. The common rate is defined as follows:
Definition 1 (Common Rate) For two view data X = {X1, X2}, common rate is defined as the
percentage overlap of the features in X1 and X2 that originate from G."
NUMERICAL EXPERIMENTS,0.17324561403508773,"One can see that the common rate ranges from 0% to 100%. The larger the value, the greater the
correlation between the two views, and a value of 0 indicates that the two views do not share any
common dimensions in G. Additionally, we construct the downstream tasks by directly transforming
the God Embedding G. Each task Tj = ψj(G), where ψj is a transformation, and Tj represents the
j-th task. By setting different G, common rates, ϕk, and ψj, we can create various synthetic datasets
to evaluate the MVRL methods. Finally, Xk are observable to the MVRL methods for learning
the representation, and the learned representation will be used to classify/regress Tj to examine the
performance of each method. Detailed implementation is given in Appendix A.4."
NUMERICAL EXPERIMENTS,0.17543859649122806,"6.2
Performance on Synthetic Datasets (RQ2)"
NUMERICAL EXPERIMENTS,0.17763157894736842,"We generate the synthetic datasets with different common rates, and the proposed NR-DCCA and
other baseline methods are compared. As shown in Figure 4, one can see that the DCCA-based"
NUMERICAL EXPERIMENTS,0.17982456140350878,(a) Performance
NUMERICAL EXPERIMENTS,0.18201754385964913,"(b) Correlation
(c) NESum of weight matrices"
NUMERICAL EXPERIMENTS,0.18421052631578946,"(d) Reconstruction
(e) Denoisng"
NUMERICAL EXPERIMENTS,0.18640350877192982,"Figure 4: (a) Mean and standard deviation of the (D)CCA-based method performance across syn-
thetic datasets in different training epochs. (b) The mean correlation between noise and real data
after transformation varies with epochs. (c) Average NESum across all weights within the trained
encoders. (d,e) The mean of reconstruction and denoising loss on the test set."
NUMERICAL EXPERIMENTS,0.18859649122807018,"methods (e.g. DCCA, DCCAE, DCCA PRIVATE) will encounter model collapse during training,
and the variance of accuracy also increases. Linear CCA demonstrates stable performance, while
the best accuracy is not as good as DCCA-based methods. Our proposed NR-DCCA achieves state-
of-the-art performance as well as training stability to prevent model collapse. The results at the final
epoch for all common rates are also presented in Table 3 in Appendix A.11."
NUMERICAL EXPERIMENTS,0.19078947368421054,"Considering that we believe that the low-rank property (i.e. highly self-related and redundant) of the
weight matrices is the root cause of the model collapse, we utilize NESum to measure the correlation
among filters in the weight matrices ( defined in A.8). Higher NESum represents lower redundancy
in weight matrices. As shown in (b) of Figure 4, our findings demonstrate that the NR approach
effectively reduces filter redundancy, thereby preventing the emergence of low-rank weight matrices
and thus averting model collapse."
NUMERICAL EXPERIMENTS,0.19298245614035087,"Moreover, according to our analysis, the correlation should be invariant if neural networks have CIP.
Therefore, after training DCCA, DCCAE, and NR-DCCA, we utilize the trained encoders to project
the corresponding view data and randomly generated Gaussian white noise and then compute their
correlation, as shown in (c) of Figure 4. It can be observed that, except for our method (NR-DCCA),
as training progresses, other methods increase the correlation between unrelated data. It should be
noted that this phenomenon always occurs under any common rates."
NUMERICAL EXPERIMENTS,0.19517543859649122,"Given that the full-rank weight matrix not only produces features that are linearly reconstructed
but also discriminates noise in the inputs, we also present the mean value pf Reconstruction and
Denoising Loss across different common rates in (d) of Figure 4. Notably, NR-DCCA achieves a
markedly lower loss, comparable to that observed with Linear CCA, whereas alternative DCCA-
based approaches generally lose the above properties."
NUMERICAL EXPERIMENTS,0.19736842105263158,"6.3
Consistent Performance on Real-world Datasets (RQ3)"
NUMERICAL EXPERIMENTS,0.19956140350877194,"We further conduct experiments on three real-world datasets: PolyMnist (Sutter et al. 2021),
CUB (Wah et al. 2011), Caltech (Deng et al. 2018). Additionally, we use a different number of
views in PolyMnist. The results are presented in Figure 5, and the performance of the final epoch in
the figure is presented in Table 3 in the Appendix A.11. Generally, the proposed NR-DCCA demon-
strates a competitive and stable performance. Different from the synthetic data, the DCCA-based
methods exhibit varying degrees of collapse on various datasets, which might be due to the complex
nature of the real-world views."
NUMERICAL EXPERIMENTS,0.20175438596491227,"Figure 5: Performance of different methods in real-world datasets. Each column represents the
performance on a specific dataset. The number of views in the dataset is denoted in the parentheses
next to the dataset name."
CONCLUSIONS,0.20394736842105263,"7
Conclusions"
CONCLUSIONS,0.20614035087719298,"We propose a novel noise regularization approach for DCCA in the context of MVRL, and it can
prevent model collapse during the training, which is an issue observed and analyzed in this paper for
the first time. Specifically, we theoretically analyze the CIP in Linear CCA and demonstrate that it is
the key to preventing model collapse. To this end, we develop a novel NR approach to equip DCCA
with such a property (NR-DCCA). Additionally, synthetic datasets with different common rates
are generated and tested, which provide a benchmark for fair and comprehensive comparisons of
different MVRL methods. The NR-DCCA developed in the paper inherits the merits of both Linear
CCA and DCCA to achieve stable and consistent outperformance in both synthetic and real-world
datasets. More importantly, the proposed noise regularization approach can also be generalized to
other DCCA-based methods (e.g., DGCCA)."
CONCLUSIONS,0.20833333333333334,"In future studies, we wish to explore the potential of noise regularization in other representation
learning tasks, such as contrastive learning and generative models. It is also interesting to further
investigate the difference between our developed NR and other neural network regularization ap-
proaches, such as orthogonality regularization (Bansal et al. 2018, Huang et al. 2020) and weight
decay (Loshchilov & Hutter 2017, Zhang et al. 2018, Krogh & Hertz 1991). Our ultimate goal
is to make the developed noise regularization a pluggable and useful module for neural network
regularization."
CONCLUSIONS,0.21052631578947367,Acknowledgments
CONCLUSIONS,0.21271929824561403,"The work described in this paper was supported by the Research Grants Council of the Hong Kong
Special Administrative Region, China (Project No. PolyU/25209221 and PolyU/15206322), and
grants from the Otto Poon Charitable Foundation Smart Cities Research Institute (SCRI) at the
Hong Kong Polytechnic University (Project No. P0043552). The contents of this article reflect the
views of the authors, who are responsible for the facts and accuracy of the information presented
herein."
REFERENCES,0.2149122807017544,References
REFERENCES,0.21710526315789475,"Agrawal, K. K., Mondal, A. K., Ghosh, A. & Richards, B. (2022), ‘a-req: Assessing representa-
tion quality in self-supervised learning by measuring eigenspectrum decay’, Advances in Neural
Information Processing Systems 35, 17626–17638."
REFERENCES,0.21929824561403508,"Akaho, S. (2006), ‘A kernel method for canonical correlation analysis’, arXiv preprint cs/0609071 ."
REFERENCES,0.22149122807017543,"An, G. (1996), ‘The effects of adding noise during backpropagation training on a generalization
performance’, Neural computation 8(3), 643–674."
REFERENCES,0.2236842105263158,"Anderson, T. W., Anderson, T. W., Anderson, T. W. & Anderson, T. W. (1958), An introduction to
multivariate statistical analysis, Vol. 2, Wiley New York."
REFERENCES,0.22587719298245615,"Andrew, G., Arora, R., Bilmes, J. & Livescu, K. (2013), Deep canonical correlation analysis, in
‘International conference on machine learning’, PMLR, pp. 1247–1255."
REFERENCES,0.22807017543859648,"Avron, H., Boutsidis, C., Toledo, S. & Zouzias, A. (2013), Efficient dimensionality reduction
for canonical correlation analysis, in ‘International conference on machine learning’, PMLR,
pp. 347–355."
REFERENCES,0.23026315789473684,"Bansal, N., Chen, X. & Wang, Z. (2018), ‘Can we gain more from orthogonality regularizations in
training deep networks?’, Advances in Neural Information Processing Systems 31."
REFERENCES,0.2324561403508772,"Belitskii, G. et al. (2013), Matrix norms and their applications, Vol. 36, Birkh¨auser."
REFERENCES,0.23464912280701755,"Benton, A., Khayrallah, H., Gujral, B., Reisinger, D. A., Zhang, S. & Arora, R. (2017), ‘Deep
generalized canonical correlation analysis’, arXiv preprint arXiv:1702.02519 ."
REFERENCES,0.23684210526315788,"Bishop, C. M. (1995), ‘Training with noise is equivalent to tikhonov regularization’, Neural Com-
putation 7(1), 108–116."
REFERENCES,0.23903508771929824,"Chang, C.-C. & Lin, C.-J. (2011), ‘Libsvm: a library for support vector machines’, ACM transac-
tions on intelligent systems and technology (TIST) 2(3), 1–27."
REFERENCES,0.2412280701754386,"Chang, X., Xiang, T. & Hospedales, T. M. (2018), Scalable and effective deep cca via soft decor-
relation, in ‘Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition’,
pp. 1488–1497."
REFERENCES,0.24342105263157895,"Chapman, J., Aguila, A. L. & Wells, L. (2022), ‘A generalized eigengame with extensions to multi-
view representation learning’, arXiv preprint arXiv:2211.11323 ."
REFERENCES,0.24561403508771928,"Chapman, J. & Wang, H.-T. (2021), ‘Cca-zoo: A collection of regularized, deep learning based,
kernel, and probabilistic cca methods in a scikit-learn style framework’, Journal of Open Source
Software 6(68), 3823."
REFERENCES,0.24780701754385964,"D’Agostini, G. (1994), ‘On the use of the covariance matrix to fit correlated data’, Nuclear Instru-
ments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment 346(1-2), 306–311."
REFERENCES,0.25,"De Bie, T. & De Moor, B. (2003), ‘On the regularization of canonical correlation analysis’, Int.
Sympos. ICA and BSS pp. 785–790."
REFERENCES,0.25219298245614036,"Deepak, K., Srivathsan, G., Roshan, S. & Chandrakala, S. (2021), ‘Deep multi-view representation
learning for video anomaly detection using spatiotemporal autoencoders’, Circuits Systems and
Signal Processing 40(2)."
REFERENCES,0.2543859649122807,"Deng, C., Chen, Z., Liu, X., Gao, X. & Tao, D. (2018), ‘Triplet-based deep hashing network for
cross-modal retrieval’, IEEE Transactions on Image Processing 27(8), 3893–3903."
REFERENCES,0.2565789473684211,"Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. (2021), With a little help from
my friends: Nearest-neighbor contrastive learning of visual representations, in ‘Proceedings of
the IEEE/CVF International Conference on Computer Vision’, pp. 9588–9597."
REFERENCES,0.25877192982456143,"Fan, W., Ma, Y., Xu, H., Liu, X., Wang, J., Li, Q. & Tang, J. (2020), Deep adversarial canonical cor-
relation analysis, in ‘Proceedings of the 2020 SIAM International Conference on Data Mining’,
SIAM, pp. 352–360."
REFERENCES,0.26096491228070173,"Feichtenhofer, C., Pinz, A. & Zisserman, A. (2016), Convolutional two-stream network fusion for
video action recognition, in ‘Proceedings of the IEEE conference on computer vision and pattern
recognition’, pp. 1933–1941."
REFERENCES,0.2631578947368421,"Fern, X. Z., Brodley, C. E. & Friedl, M. A. (2005), Correlation clustering for learning mixtures
of canonical correlation models, in ‘Proceedings of the 2005 SIAM International Conference on
Data Mining’, SIAM, pp. 439–448."
REFERENCES,0.26535087719298245,"Garrido, Q., Balestriero, R., Najman, L. & Lecun, Y. (2023), Rankme: Assessing the downstream
performance of pretrained self-supervised representations by their rank, in ‘International Confer-
ence on Machine Learning’, PMLR, pp. 10929–10974."
REFERENCES,0.2675438596491228,"Geng, Y., Han, Z., Zhang, C. & Hu, Q. (2021), Uncertainty-aware multi-view representation learn-
ing, in ‘Proceedings of the AAAI Conference on Artificial Intelligence’, Vol. 35, pp. 7545–7553."
REFERENCES,0.26973684210526316,"Gong, C., Ren, T., Ye, M. & Liu, Q. (2020), ‘Maxup: A simple way to improve generalization of
neural network training’, arXiv preprint arXiv:2002.09024 ."
REFERENCES,0.2719298245614035,"Guo, H., Wang, J., Xu, M., Zha, Z.-J. & Lu, H. (2015), Learning multi-view deep features for
small object retrieval in surveillance scenarios, in ‘Proceedings of the 23rd ACM international
conference on Multimedia’, pp. 859–862."
REFERENCES,0.2741228070175439,"Hardoon, D. R., Szedmak, S. & Shawe-Taylor, J. (2004), ‘Canonical correlation analysis: An
overview with application to learning methods’, Neural computation 16(12), 2639–2664."
REFERENCES,0.27631578947368424,"He, Z., Rakin, A. S. & Fan, D. (2019), Parametric noise injection: Trainable randomness to improve
deep neural network robustness against adversarial attack, in ‘Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition’, pp. 588–597."
REFERENCES,0.27850877192982454,"Hoerl, A. E. & Kennard, R. W. (1970), ‘Ridge regression: applications to nonorthogonal problems’,
Technometrics 12(1), 69–82."
REFERENCES,0.2807017543859649,"Horst, P. (1961), ‘Generalized canonical correlations and their applications to experimental data’,
Journal of Clinical Psychology 17(4), 331–347."
REFERENCES,0.28289473684210525,"Hotelling, H. (1992), ‘Relations between two sets of variates’, Breakthroughs in statistics: method-
ology and distribution pp. 162–190."
REFERENCES,0.2850877192982456,"Huang, L., Liu, L., Zhu, F., Wan, D., Yuan, Z., Li, B. & Shao, L. (2020), Controllable orthogonal-
ization in training dnns, in ‘Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition’, pp. 6429–6438."
REFERENCES,0.28728070175438597,"Hwang, H., Kim, G.-H., Hong, S. & Kim, K.-E. (2021), ‘Multi-view representation learning via total
correlation objective’, Advances in Neural Information Processing Systems 34, 12194–12207."
REFERENCES,0.2894736842105263,"Jing, L., Vincent, P., LeCun, Y. & Tian, Y. (2021), ‘Understanding dimensional collapse in con-
trastive self-supervised learning’, arXiv preprint arXiv:2110.09348 ."
REFERENCES,0.2916666666666667,"Karpathy, A. & Fei-Fei, L. (2015), Deep visual-semantic alignments for generating image de-
scriptions, in ‘Proceedings of the IEEE conference on computer vision and pattern recognition’,
pp. 3128–3137."
REFERENCES,0.29385964912280704,"Kettenring, J. R. (1971), ‘Canonical analysis of several sets of variables’, Biometrika 58(3), 433–
451."
REFERENCES,0.29605263157894735,"Kim, T.-K., Wong, S.-F. & Cipolla, R. (2007), Tensor canonical correlation analysis for action
classification, in ‘2007 IEEE Conference on Computer Vision and Pattern Recognition’, IEEE,
pp. 1–8."
REFERENCES,0.2982456140350877,"Kornblith, S., Norouzi, M., Lee, H. & Hinton, G. (2019), Similarity of neural network representa-
tions revisited, in ‘International conference on machine learning’, PMLR, pp. 3519–3529."
REFERENCES,0.30043859649122806,"Krogh, A. & Hertz, J. (1991), ‘A simple weight decay can improve generalization’, Advances in
neural information processing systems 4."
REFERENCES,0.3026315789473684,"Lahat, D., Adali, T. & Jutten, C. (2015), ‘Multimodal data fusion: an overview of methods, chal-
lenges, and prospects’, Proceedings of the IEEE 103(9), 1449–1477."
REFERENCES,0.3048245614035088,"Le, Q. & Mikolov, T. (2014), Distributed representations of sentences and documents, in ‘Interna-
tional conference on machine learning’, PMLR, pp. 1188–1196."
REFERENCES,0.30701754385964913,"Li, Z., Tang, C., Zheng, X., Liu, X., Zhang, W. & Zhu, E. (2022), ‘High-order correlation preserved
incomplete multi-view subspace clustering’, IEEE Transactions on Image Processing 31, 2067–
2080."
REFERENCES,0.3092105263157895,"Loshchilov, I. & Hutter, F. (2017), ‘Decoupled weight decay regularization’, arXiv preprint
arXiv:1711.05101 ."
REFERENCES,0.31140350877192985,"Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z. & Yuille, A. (2014), ‘Deep captioning with multi-
modal recurrent neural networks (m-rnn)’, arXiv preprint arXiv:1412.6632 ."
REFERENCES,0.31359649122807015,"Morcos, A. S., Barrett, D. G., Rabinowitz, N. C. & Botvinick, M. (2018), ‘On the importance of
single directions for generalization’, arXiv preprint arXiv:1803.06959 ."
REFERENCES,0.3157894736842105,"Nguyen, T., Raghu, M. & Kornblith, S. (2020), ‘Do wide and deep networks learn the same things?
uncovering how neural network representations vary with width and depth’, arXiv preprint
arXiv:2010.15327 ."
REFERENCES,0.31798245614035087,"Nie, F., Li, J., Li, X. et al. (2017), Self-weighted multiview clustering with multiple graphs., in
‘IJCAI’, pp. 2564–2570."
REFERENCES,0.3201754385964912,"Nielsen, A. A. (2002), ‘Multiset canonical correlations analysis and multispectral, truly multitem-
poral remote sensing data’, IEEE transactions on image processing 11(3), 293–305."
REFERENCES,0.3223684210526316,"Petersen, K. B., Pedersen, M. S. et al. (2008), ‘The matrix cookbook’, Technical University of
Denmark 7(15), 510."
REFERENCES,0.32456140350877194,"Poole, B., Sohl-Dickstein, J. & Ganguli, S. (2014), ‘Analyzing noise in autoencoders and deep
networks’, arXiv preprint arXiv:1406.1831 ."
REFERENCES,0.3267543859649123,"Prechelt, L. (1998), ‘Automatic early stopping using cross validation: quantifying the criteria’, Neu-
ral networks 11(4), 761–767."
REFERENCES,0.32894736842105265,"Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. & Dosovitskiy, A. (2021), ‘Do vision transform-
ers see like convolutional neural networks?’, Advances in neural information processing systems
34, 12116–12128."
REFERENCES,0.33114035087719296,"Saxe, A. M., McClelland, J. L. & Ganguli, S. (2019), ‘A mathematical theory of semantic develop-
ment in deep neural networks’, Proceedings of the National Academy of Sciences 116(23), 11537–
11546."
REFERENCES,0.3333333333333333,"Sietsma, J. & Dow, R. J. (1991), ‘Creating artificial neural networks that generalize’, Neural net-
works 4(1), 67–79."
REFERENCES,0.3355263157894737,"Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. & Srebro, N. (2018), ‘The implicit bias of
gradient descent on separable data’, Journal of Machine Learning Research 19(70), 1–57."
REFERENCES,0.33771929824561403,"Srivastava, N. & Salakhutdinov, R. R. (2012), ‘Multimodal learning with deep boltzmann machines’,
Advances in neural information processing systems 25."
REFERENCES,0.3399122807017544,"Staerman, G., Adjakossa, E., Mozharovskyi, P., Hofer, V., Sen Gupta, J. & Cl´emenc¸on, S. (2023),
‘Functional anomaly detection: a benchmark study’, International Journal of Data Science and
Analytics 16(1), 101–117."
REFERENCES,0.34210526315789475,"Sun, J., Xiu, X., Luo, Z. & Liu, W. (2023), ‘Learning high-order multi-view representation by
new tensor canonical correlation analysis’, IEEE Transactions on Circuits and Systems for Video
Technology ."
REFERENCES,0.3442982456140351,"Sun, L., Ceran, B. & Ye, J. (2010), A scalable two-stage approach for a class of dimensionality
reduction techniques, in ‘Proceedings of the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining’, pp. 313–322."
REFERENCES,0.34649122807017546,"Sun, L., Ji, S. & Ye, J. (2010), ‘Canonical correlation analysis for multilabel classification: A least-
squares formulation, extensions, and analysis’, IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence 33(1), 194–200."
REFERENCES,0.34868421052631576,"Sutter, T. M., Daunhawer, I. & Vogt, J. E. (2021), ‘Generalized multimodal elbo’, arXiv preprint
arXiv:2105.02470 ."
REFERENCES,0.3508771929824561,"Tao, H., Hou, C., Yi, D., Zhu, J. & Hu, D. (2019), ‘Joint embedding learning and low-rank ap-
proximation: A framework for incomplete multiview learning’, IEEE transactions on cybernetics
51(3), 1690–1703."
REFERENCES,0.3530701754385965,"Tian, Y. & Zhang, Y. (2022), ‘A comprehensive survey on regularization strategies in machine learn-
ing’, Information Fusion 80, 146–166."
REFERENCES,0.35526315789473684,"Tschannen, M., Bachem, O. & Lucic, M. (2018), ‘Recent advances in autoencoder-based represen-
tation learning’, arXiv preprint arXiv:1812.05069 ."
REFERENCES,0.3574561403508772,"Tuzhilina, E., Tozzi, L. & Hastie, T. (2023), ‘Canonical correlation analysis in high dimensions with
structured regularization’, Statistical modelling 23(3), 203–227."
REFERENCES,0.35964912280701755,"Wah, C., Branson, S., Welinder, P., Perona, P. & Belongie, S. (2011), ‘The caltech-ucsd birds-200-
2011 dataset’, california institute of technology ."
REFERENCES,0.3618421052631579,"Wang, W., Arora, R., Livescu, K. & Bilmes, J. (2015), On deep multi-view representation learning,
in ‘International conference on machine learning’, PMLR, pp. 1083–1092."
REFERENCES,0.36403508771929827,"Wang, W., Yan, X., Lee, H. & Livescu, K. (2016), ‘Deep variational canonical correlation analysis’,
arXiv preprint arXiv:1610.03454 ."
REFERENCES,0.36622807017543857,"Wang, Z., Xiang, C., Zou, W. & Xu, C. (2020), ‘Mma regularization: Decorrelating weights of
neural networks by maximizing the minimal angles’, Advances in Neural Information Processing
Systems 33, 19099–19110."
REFERENCES,0.3684210526315789,"Wei, J., Xia, Y. & Zhang, Y. (2019), ‘M3net: A multi-model, multi-size, and multi-view deep neural
network for brain magnetic resonance image segmentation’, Pattern Recognition 91, 366–378."
REFERENCES,0.3706140350877193,"Wong, H. S., Wang, L., Chan, R. & Zeng, T. (2021), ‘Deep tensor cca for multi-view learning’,
IEEE Transactions on Big Data 8(6), 1664–1677."
REFERENCES,0.37280701754385964,"Xu, J., Zheng, H., Wang, J., Li, D. & Fang, X. (2020), ‘Recognition of eeg signal motor imagery
intention based on deep multi-view feature learning’, Sensors 20(12), 3496."
REFERENCES,0.375,"Yan, H., Cheng, L., Ye, Q., Yu, D.-J. & Qi, Y. (2023), ‘Robust generalized canonical correlation
analysis’, Applied Intelligence pp. 1–16."
REFERENCES,0.37719298245614036,"Yan, X., Hu, S., Mao, Y., Ye, Y. & Yu, H. (2021), ‘Deep multi-view learning methods: A review’,
Neurocomputing 448, 106–129."
REFERENCES,0.3793859649122807,"Yang, M., Li, Y., Huang, Z., Liu, Z., Hu, P. & Peng, X. (2021), Partially view-aligned representa-
tion learning with noise-robust contrastive loss, in ‘Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition’, pp. 1134–1143."
REFERENCES,0.3815789473684211,"Yao, Y., Rosasco, L. & Caponnetto, A. (2007), ‘On early stopping in gradient descent learning’,
Constructive Approximation 26, 289–315."
REFERENCES,0.38377192982456143,"Yin, J. & Sun, S. (2021), ‘Incomplete multi-view clustering with reconstructed views’, IEEE Trans-
actions on Knowledge and Data Engineering ."
REFERENCES,0.38596491228070173,"Zhang, C., Cui, Y., Han, Z., Zhou, J. T., Fu, H. & Hu, Q. (2020), ‘Deep partial multi-view learning’,
IEEE transactions on pattern analysis and machine intelligence 44(5), 2402–2415."
REFERENCES,0.3881578947368421,"Zhang, C., Fu, H., Hu, Q., Zhu, P. & Cao, X. (2016), ‘Flexible multi-view dimensionality co-
reduction’, IEEE Transactions on Image Processing 26(2), 648–659."
REFERENCES,0.39035087719298245,"Zhang, C., Han, Z., Fu, H., Zhou, J. T., Hu, Q. et al. (2019), ‘Cpm-nets: Cross partial multi-view
networks’, Advances in Neural Information Processing Systems 32."
REFERENCES,0.3925438596491228,"Zhang, C., Liu, Y. & Fu, H. (2019), Ae2-nets: Autoencoder in autoencoder networks, in ‘Proceed-
ings of the IEEE/CVF conference on computer vision and pattern recognition’, pp. 2577–2585."
REFERENCES,0.39473684210526316,"Zhang, G., Wang, C., Xu, B. & Grosse, R. (2018), ‘Three mechanisms of weight decay regulariza-
tion’, arXiv preprint arXiv:1810.12281 ."
REFERENCES,0.3969298245614035,"Zhang, H., Wang, S., Ioannidis, V. N., Adeshina, S., Zhang, J., Qin, X., Faloutsos, C., Zheng, D.,
Karypis, G. & Yu, P. S. (2023), ‘Orthoreg: Improving graph-regularized mlps via orthogonality
regularization’, arXiv preprint arXiv:2302.00109 ."
REFERENCES,0.3991228070175439,"Zhang, Y., Zhang, J., Pan, Z. & Zhang, D. (2016), ‘Multi-view dimensionality reduction via canon-
ical random correlation analysis’, Frontiers of Computer Science 10, 856–869."
REFERENCES,0.40131578947368424,"Zhao, H., Ding, Z. & Fu, Y. (2017), Multi-view clustering via deep matrix factorization, in ‘Pro-
ceedings of the AAAI conference on artificial intelligence’, Vol. 31."
REFERENCES,0.40350877192982454,"Zheng, Q., Zhu, J., Li, Z., Pang, S., Wang, J. & Li, Y. (2020), ‘Feature concatenation multi-view
subspace clustering’, Neurocomputing 379, 89–102."
REFERENCES,0.4057017543859649,"Zhou, C. & Paffenroth, R. C. (2017), Anomaly detection with robust deep autoencoders, in ‘Pro-
ceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data
mining’, pp. 665–674."
REFERENCES,0.40789473684210525,"Zhu, X., Zhou, W. & Li, H. (2018), Improving deep neural network sparsity through decorrelation
regularization., in ‘Ijcai’, pp. 3264–3270."
REFERENCES,0.4100877192982456,"A
Appendix"
REFERENCES,0.41228070175438597,"A.1
Proof of Theorem 1"
REFERENCES,0.4144736842105263,"A.1.1
Preparations"
REFERENCES,0.4166666666666667,"To prove Theorem 1, we need first to prove the following Lemmas."
REFERENCES,0.41885964912280704,"Lemma 1 Given a specific matrix B and a zero-centered C with respect to rows, the product BC
is also zero-centered with respect to rows."
REFERENCES,0.42105263157894735,Proof of Lemma 1:
REFERENCES,0.4232456140350877,"Let Bi,j and Ci,j denote the (i, j)-th entry of B and C, respectively. Then we have:"
REFERENCES,0.42543859649122806,"(BC)i,j =
X"
REFERENCES,0.4276315789473684,"r=1
Bi,rCr,j"
REFERENCES,0.4298245614035088,"Since each row of C has a mean of 0, we have Pn
j=1 Cr,j = 0, ∀r. For the mean value of i-th row
of BC, we can write:"
N,0.43201754385964913,"1
n n
X"
N,0.4342105263157895,"j=1
(BC)i,j = 1 n n
X j=1 X"
N,0.43640350877192985,"r=1
Bi,rCr,j = 1 n X r=1 n
X"
N,0.43859649122807015,"j=1
Bi,rCr,j = 1 n X"
N,0.4407894736842105,"r=1
Bi,r  
n
X"
N,0.44298245614035087,"j=1
Cr,j   = 1 n X"
N,0.4451754385964912,"r=1
Bi,r · 0 = 0 (9)"
N,0.4473684210526316,"The Moore-Penrose Inverse (MPI) (Petersen et al. 2008) will be used for analysis, and the MPI is
defined as follows:"
N,0.44956140350877194,"Definition 2 Given a specific matrix Y , its Moore-Penrose Inverse (MPI) is denoted as Y +. Y +
satisfies: Y Y +Y = Y , Y +Y Y + = Y +, Y Y + is symmetric, and Y +Y is symmetric."
N,0.4517543859649123,"The MPI Y + is unique and always exists for any Y . Furthermore, when matrix Y is invertible, its
inverse matrix Y −is exactly Y +. Using the definition of MPI, we can rewrite the formulation of
CCA. In particular, Corr(·, ·) can be derived by replacing the inverse with MPI. Using Corr(Xk, Ak)
as an example, the following Lemma holds:"
N,0.45394736842105265,"Lemma 2 (MPI-based CCA) For the k-th view data Xk and the Gaussian white noise Ak, we have"
N,0.45614035087719296,"Corr(Xk, Ak) =
1
(n −1)2 tr(A+
k AkX+
k Xk)1/2, ∀k.
(10)"
N,0.4583333333333333,Proof of Lemma 2:
N,0.4605263157894737,"Corr(Xk, Ak) = tr((Σ−1/2
11
Σ12Σ−1/2
22
)′Σ−1/2
11
Σ12Σ−1/2
22
)1/2"
N,0.46271929824561403,"= tr(Σ−1/2
22
Σ′
12Σ−1/2
11
Σ−1/2
11
Σ12Σ−1/2
22
)1/2"
N,0.4649122807017544,"= tr(Σ−1/2
22
Σ−1/2
22
Σ′
12Σ−1/2
11
Σ−1/2
11
Σ12)1/2"
N,0.46710526315789475,"= tr(Σ−1
22 Σ′
12Σ−1
11 Σ12)1/2"
N,0.4692982456140351,"=
1
(n −1)2 tr((AkA′
k)−1(XkA′
k)′(XkX′
k)−1(XkA′
k))1/2"
N,0.47149122807017546,"=
1
(n −1)2 tr((AkX′
k)−1(AkX′
k)(XkX′
k)−1(XkA′
k))1/2"
N,0.47368421052631576,"=
1
(n −1)2 tr((AkA′
k)+(AkX′
k)(XkX′
k)+(XkA′
k))1/2"
N,0.4758771929824561,"=
1
(n −1)2 tr(A′
k(AkA′
k)+AkX′
k(XkX′
k)+Xk)1/2"
N,0.4780701754385965,"=
1
(n −1)2 tr(A+
k AkX+
k Xk)1/2 (11)"
N,0.48026315789473684,"The first row is based on the definition of Corr, the second row is because the trace is invariant under
cyclic permutation, the fifth row is to replace matrix inverse by MPI and the ninth row is due to
Y + = Y ′(Y Y +) (Petersen et al. 2008).
Lemma 3 Given a specific matrix Y and its MPI Y +, let Rank(Y ) and Rank(Y +Y ) be the ranks
of Y and Y +Y , respectively. It is true that:"
N,0.4824561403508772,Rank(Y ) = Rank(Y +Y )
N,0.48464912280701755,Rank(Y +Y ) = tr(Y +Y )
N,0.4868421052631579,Proof of Lemma 3:
N,0.48903508771929827,"Firstly, the column space of Y +Y is a subspace of the column space of Y .
Therefore,
Rank(Y +Y ) ≤Rank(Y ). On the other hand, according to the definition of MPI (Petersen et al.
2008), we know that Y = Y (Y +Y ). Since the rank of a product of matrices is at most the min-
imum of the ranks of the individual matrices, we have Rank(Y ) ≤Rank(Y +Y ). Combining the
two inequalities, we have Rank(Y ) = Rank(Y +Y ). Furthermore, since (Y +Y )(Y +Y ) = Y +Y
(it holds that Y + = Y +Y Y + according to the definition of MPI (Petersen et al. 2008)), Y +Y is
an idempotent and symmetric matrix, and thus its eigenvalues must be 0 or 1. So the sum of its
eigenvalues is exactly its rank. Considering matrix trace is the sum of eigenvalues of matrices, we
have Rank(Y +Y ) = tr(Y +Y ).
Lemma 4 Rank(WkXk) < Rank(Xk) , when Wk is not a full-rank matrix and Xk is a full-rank
matrix."
N,0.49122807017543857,"Proof of Lemma 4: Since the rank of a product of matrices is at most the minimum of the ranks of
the individual matrices, we have Rank(WkXk) ≤min(Rank(Wk), Rank(Xk)). Considering Xk
is full-rank, Rank(Xk) = min(dk, n) and then Rank(WkXk) ≤min(Rank(Wk), Rank(Xk)) =
min(Rank(Wk), min(dk, n)). Since Wk is not full-rank, we have Rank(Wk) < dk. In conclusion,
Rank(WkXk) < min(dk, min(dk, n)) and then Rank(WkXk) < dk ≤Rank(Xk)."
N,0.4934210526315789,"A.1.2
Main proofs of Theorem 1"
N,0.4956140350877193,"We prove the two directions of Theorem 1 in the following two Lemmas. First, we prove CIP holds
if Wk is a square and full-rank matrix.
Lemma 5 For any k, if Wk is a square and full-rank matrix, the correlation between Xk and Ak
remains unchanged before and after the transformation by Wk (i.e. CIP holds for Wk). Mathemati-
cally, we have Corr(Xk, Ak) = Corr(WkXk, WkAk)."
N,0.49780701754385964,Proof of Lemma 5:
N,0.5,"Firstly, we have the k-th view data Xk to be full-rank, as we can always delete the redundant data,
and the random noise Ak is full-rank as each column is generated independently. Without loss"
N,0.5021929824561403,"of generality, we assume that all the datasets Xk are zero-centered with respect to row (Hotelling
1992), which implies that WkAk and WkXk are both zero-centered matrices 1. When computing the
covariance matrix, there is no need for an additional subtraction of the mean of row, which simplifies
our subsequent derivations. And Wk is always full-rank since Linear CCA seeks full-rank WkXk.
Then by utilizing Lemma 2, we derive that the correlation between Xk and Ak remains unchanged
before and after the transformation:"
N,0.5043859649122807,"Corr(WkXk, WkAk) =
1
(n −1)2 tr((WkAk)+WkAk(WkXk)+WkXk)1/2"
N,0.506578947368421,"=
1
(n −1)2 tr((W +
k WkAk)+(WkAkA+
k )+WkAk(W +
k WkXk)+(WkXkX+
k )+WkXk)1/2"
N,0.5087719298245614,"=
1
(n −1)2 tr((W +
k WkAk)+W +
k WkAk(W +
k WkXk)+W +
k WkXk)1/2"
N,0.5109649122807017,"=
1
(n −1)2 tr(A+
k AkX+
k Xk)1/2"
N,0.5131578947368421,"= Corr(Xk, Ak)"
N,0.5153508771929824,"(12)
The first row is based on Lemma 2, the second row is because given two matrices B and C,
(BC)+ = (B+BC)+(BC+C)+ always holds (Petersen et al. 2008), and the third row utilizes
the properties of full-rank and square matrix Wk: W +
k = W −
k , which means W +
k Wk = WkW +
k =
Idk (Petersen et al. 2008)."
N,0.5175438596491229,"Then we prove that Wk is a full-rank matrix if CIP holds and Wk is square.
Lemma 6 For any k, if Corr(Xk, Ak) = Corr(WkXk, WkAk) and Wk is a square matrix, then
Wk must be a full-rank matrix."
N,0.5197368421052632,Proof of Lemma 6:
N,0.5219298245614035,"This Lemma is equivalent to its contra-positive proposition: if Wk is not a full-rank matrix, there
exists random noise data Ak such that ηk = |Corr(WkXk, Wk(Ak)) −Corr(Xk, Ak)| is not 0.
And we find that when Wk is not full-rank, there exists Ak = Xk such that ηk ̸= 0. We have the
following derivation:"
N,0.5241228070175439,"ηk = |Corr(WkXk, WkAk) −Corr(Xk, Ak)|"
N,0.5263157894736842,"=

1
(n −1)2 tr((WkAk)+(WkAk)(WkXk)+(WXk))1/2 −
1
(n −1)2 tr(A+AX+X)1/2"
N,0.5285087719298246,"=

1
(n −1)2 tr((W +
k WkAk)+(WkAkA+
k )+WkAk(W +
k WkXk)+(WkXkX+
k )+WkXk)1/2 −
1
(n −1)2 tr(A+
k AkX+
k Xk)1/2"
N,0.5307017543859649,"=

1
(n −1)2 tr((W +
k WkAk)+W +
k WkAk(W +
k WkXk)+W +
k WkXk)1/2 −
1
(n −1)2 tr(A+
k AX+
k Xk)1/2"
N,0.5328947368421053,"(13)
The first row is the definition of NR loss with respect to Wk, the second row is based on the new
form of CCA, the third row is because given two specific matrices B and C, it holds the equality
(BC)+ = (B+BC)+(BC+C)+ (Petersen et al. 2008), and the fourth row utilizes the properties of
full-rank matrix: for full-rank matrices Xk and Ak, whose sample size is larger than dimension size,
they fulfill: XkXk
+ = Idk, AkAk
+ = Idk (given a specific full-rank matrix Y , if its number of rows
is smaller than that of cols, it holds that Y + = Y ′(Y Y ′)−, which means that Y Y + = I) (Petersen
et al. 2008)."
N,0.5350877192982456,Let us analyze the case when Ak = Xk:
N,0.5372807017543859,"ηk =

1
(n −1)2 tr((W +
k WkXk)+W +
k WkXk(W +
k WkXk)+W +
k WkXk)1/2 −
1
(n −1)2 tr(X+
k XkX+
k Xk)1/2"
N,0.5394736842105263,"=

1
(n −1)2 tr((W +
k WkXk)+W +
k WkXk)1/2 −
1
(n −1)2 tr(X+
k Xk)1/2
 ."
N,0.5416666666666666,"(14)
The first row is to replace Ak with Xk, the second row is because X+
k XkX+
k
= X+
k and
(W +
k WkXk)+W +
k WkXk(W +
k WkXk)+ = W +
k WkXk, which are based on the definition of MPI
that given a specific matrix Y , Y +Y Y + = Y + (Petersen et al. 2008)."
N,0.543859649122807,"As a result, we can know that when the random noise data Ak is exactly Xk and Wk is not full-rank,
ηk can not be zero:"
N,0.5460526315789473,"ηk =

1
(n −1)2 tr((W +
k WkXk)+W +
k WkXk)1/2 −
1
(n −1)2 tr(X+
k Xk)1/2"
N,0.5482456140350878,"=

1
(n −1)2 Rank(W +
k WkXk)1/2 −
1
(n −1)2 Rank(X)1/2"
N,0.5504385964912281,"̸=

1
(n −1)2 Rank(Xk)1/2 −
1
(n −1)2 Rank(Xk)1/2 ̸= 0 (15)"
N,0.5526315789473685,"The first row is due to Equation 14,
the second row is based on Lemma 3 that
tr((W +
k WkXk)+W +
k WkXk) = Rank(W +
k WkXk) and tr(X+
k Xk) = Rank(X), and the third row
is because of Lemma 4."
N,0.5548245614035088,"Finally, if ηk is always constrained to 0 for any Ak, then Wk must be a full-rank matrix."
N,0.5570175438596491,"Combining Lemma 5 and 6, we complete the proof."
N,0.5592105263157895,"A.2
Proof of Theorem 2"
N,0.5614035087719298,"For linear regression problem : R∗= arg minR ∥RB −C∥F , where R ∈Rout dim×input dim"
N,0.5635964912280702,"is the weight matrix, and B ∈Rinput dim×n, C ∈Routput dim×n are the input and target matrix
(input dim < n), respectively. R∗has a closed-form solution: R∗= CB′(BB′)−and therefore,
it holds that:"
N,0.5657894736842105,"min ∥RB −C∥F = ∥R∗B −C∥F
a= ∥CB′(BB′)−B −C∥F"
N,0.5679824561403509,"b= ∥CB′(BB′)+B −C∥F
c= ∥CB+B −C∥F (16)"
N,0.5701754385964912,"Equation a is the use of the closed-form solution and Equation b is to replace the matrix inverse by
MPI. Equation c is because Y ′(Y Y ′)+ = Y + (Petersen et al. 2008)."
N,0.5723684210526315,"Given k-th view, when Wk is square possesses CIP, Wk is full-rank. Using the above equation, we
have:"
N,0.5745614035087719,"min ∥PkWkXk −Xk∥F
a= ∥Xk(WkXk)+(WkXk) −Xk∥F"
N,0.5767543859649122,"b= ∥Xk(W +
k WkXk)+(WkXkX+
k )+(WkXk) −Xk∥F
c= ∥Xk(W +
k WkXk)+W +
k WkXk −Xk∥F"
N,0.5789473684210527,"d= ∥XkX+
k Xk −Xk∥F
= 0 (17)"
N,0.581140350877193,"Equation a is due to Equation 16 (Petersen et al. 2008). Equation b holds because given two matrices
B and C, (BC)+ = (B+BC)+(BC+C)+ always holds and Equation c is because for full-rank
matrix Xk ∈Rdk×n(dk < n), XkX+
k = Idk. Equation c utilizes the properties of full-rank and
square matrix Wk: W +
k
= W −
k , which means W +
k Wk = WkW +
k
= Idk (Petersen et al. 2008).
Equation d is based on the definition of MPI: given a specific matrix Y and its Y +, it holds that
Y Y +Y = Y . We show the first property in Theorem 2."
N,0.5833333333333334,As for the second property:
N,0.5855263157894737,"min ∥QkWk(Xk + Ak) −WkXk∥F
a= ∥WkXk(Wk(Xk + Ak))+(Wk(Xk + Ak)) −WkXk∥F"
N,0.5877192982456141,"b= ∥WkXk(W +
k Wk(Xk + Ak))+(Wk(XKAk)(Xk + Ak)+)+(Wk(Xk + Ak)) −WKXk∥F
c= ∥WkXk(W +
k Wk(Xk + Ak))+W +
k Wk(Xk + Ak) −WKXk∥F
e= ∥WkXk(Xk + Ak)+(Xk + Ak) −WKXk∥F"
N,0.5899122807017544,"f= ∥Wk(Xk + Ak)(Xk + Ak)+(Xk + Ak) −WkAk(Xk + Ak)+(Xk + Ak) −WKXk∥F
g= ∥Wk(Xk + Ak) −WkAk(Xk + Ak)+(Xk + Ak) −WKXk∥F"
N,0.5921052631578947,h= ∥WkAk −WkAk(Xk + Ak)+(Xk + Ak)∥F
N,0.5942982456140351,i= ∥WkAk(In −(Xk + Ak)+(Xk + Ak))∥F
N,0.5964912280701754,"j
≤∥WkAk∥F ∗∥(In −(Xk + Ak)+(Xk + Ak))∥F"
N,0.5986842105263158,"k= ∥WkAk∥F ∗
p"
N,0.6008771929824561,tr(In −(Xk + Ak)+(Xk + Ak))
N,0.6030701754385965,"l= ∥WkAk∥F ∗
p"
N,0.6052631578947368,Rank(In −(Xk + Ak)+(Xk + Ak))
N,0.6074561403508771,≤√n∥WkAk∥F
N,0.6096491228070176,"(18)
Equation a is because of Equation 16. Equation b holds because given two matrices B and C,
(BC)+ = (B+BC)+(BC+C)+ always holds and Equation c is because we assume Xk + Ak is a
full-rank matrix. Equation e utilizes the properties of full-rank and square matrix Wk: W +
k Wk =
WkW +
k = Idk. Equation g is based on the definition of MPI: (Xk + Ak)(Xk + Ak)+(Xk + Ak) =
(Xk + Ak). Equation j holds because given two specific matrices B and C, ∥BC∥F ≤∥B∥F ∗
∥C∥F (Belitskii et al. 2013). Equation k and l is because given a specific matrix B, I −B+B is an
idempotent matrix and ∥I −B+B∥F =
p"
N,0.6118421052631579,"tr((I −B+B)′(I −B+B)) =
p"
N,0.6140350877192983,tr(I −B+B).
N,0.6162280701754386,"Now, we use (WkAk)(i, j) to donate the (i, j)-th entry of WkAk and the expected value of the
square of the Frobenius norm of WkAk is:"
N,0.618421052631579,"E

∥WAk∥2
F

= E  X i X"
N,0.6206140350877193,"j
[(WkAk)i,j]2 "
N,0.6228070175438597,"
(19)"
N,0.625,"Expanding the product (WkAk)i,j, we have:"
N,0.6271929824561403,"(WkAk)i,j =
X"
N,0.6293859649122807,"r
(Wk)i,r(Ak)r,j
(20)"
N,0.631578947368421,"Substituting back into the expectation, we get:"
N,0.6337719298245614,"E

∥WAk∥2
F

= E  X i X j ""X"
N,0.6359649122807017,"r
(Wk)i,r(Ak)r,j #2 =
X i X j
E   ""X"
N,0.6381578947368421,"r
(Wk)i,r(Ak)r,j #2 "
N,0.6403508771929824,"(21)
Since the elements of Ak are i.i.d. with zero mean and unit variance, the expectation of their squares
is 1, and the cross terms vanish due to the zero mean. Therefore, we have: X i X j
E   ""X"
N,0.6425438596491229,"r
(Wk)i,r(Ak)r,j #2 =
X i X"
N,0.6447368421052632,"r
(Wk)2
ir = ∥Wk∥2
F
(22)"
N,0.6469298245614035,"Hence, we have shown that"
N,0.6491228070175439,"E

∥WkAk∥2
F

= ∥Wk∥2
F
(23)
This completes the proof."
N,0.6513157894736842,"A.3
Details of Datasets and Baselines"
N,0.6535087719298246,Synthetic datasets:
N,0.6557017543859649,"All the datasets used in the paper are either provided or open datasets. Detailed proofs of all the
Theorems in the main paper can be found in the Appendix. Both source codes and appendix can be
downloaded from the supplementary material."
N,0.6578947368421053,"We make 6 groups of multi-view data originating from the same G ∈Rd×n (we set n =
4000, d = 100).
Each group consists of tuples with 2 views (2000 tuples for training and
2000 tuples for testing) and a distinct common rate.
Common rates of these sets are from
{0%, 20%, 40%, 60%, 80%, 100%} and there are 50 downstream regression tasks. We report the
mean and standard deviation of R2 score across all the tasks."
N,0.6600877192982456,Real-world datasets:
N,0.6622807017543859,"PolyMnist (Sutter et al. 2021): A dataset consists of tuples with 5 different MNIST images (60, 000
tuples for training and 10, 000 tuples for testing). Each image within a tuple possesses distinct
backgrounds and writing styles, yet they share the same digit label. The background of each view is
randomly cropped from an image and is not used in other views. Thus, the digit identity represents
the common information, while the background and writing style serve as view-specific factors. The
downstream task is the digit classification task. CUB (Wah et al. 2011): A dataset consists of tuples
with deep visual features (1024-d) extracted by GOOGLENET and text features (300-d) obtained
through DOC2VEC (Le & Mikolov 2014) (480 tuples for training and 600 tuples for testing). This
MVRL task utilizes the first 10 categories of birds in the original dataset and the downstream task is
the bird classification task. Caltech (Deng et al. 2018): A dataset consists of tuples with traditional
visual features extracted from images that belong to 101 object categories, including an additional
background category (6400 tuples for training and 9144 tuples for testing). Following Yang et al.
(2021), three features are used as views: a 1, 984-d HOG feature, a 512-d GIST feature, and a 928-d
SIFT feature."
N,0.6644736842105263,"Baselines: All of our experiments are conducted with fixed random seeds and all the performance
of downstream tasks is the average value of a 5-fold cross-validation. We use one single 3090 GPU.
The CCA-zoo package is adopted as the implementation of various CCA/GCCA-based methods,
and the original implementation of MVTCAE is employed. Both baselines and our developed NR-
DCCA/NR-DGCCA are implemented in the same PyTorch environment (see requirements.txt in the
source codes)."
N,0.6666666666666666,Direct method:
N,0.668859649122807,• CONCAT straightforwardly concatenates original features from different views.
N,0.6710526315789473,CCA methods:
N,0.6732456140350878,"• PRCCA Tuzhilina et al. (2023) preserves the internal data structure by grouping high-
dimensional data features while applying an l2 penalty to CCA,
• Linear CCA (Wang et al. 2015) employs individual linear layers to project multi-view data
and then maximizes their correlation defined in (Hotelling 1992).
• Linear GCCA uses linear layers to maximize the correlation of multi-view data defined in
(Benton et al. 2017)."
N,0.6754385964912281,Kernel CCA Methods:
N,0.6776315789473685,• KCCA (Akaho 2006) employs CCA methods through positive-definite kernels.
N,0.6798245614035088,DCCA-based methods:
N,0.6820175438596491,"• DCCA (Andrew et al. 2013) employs neural networks to individually project multiple sets
of views, obtaining new representations that maximize the correlation between each pair of
views.
• DGCCA (Benton et al. 2017) constructs a shared representation and maximizes the corre-
lation between each view and the shared representation.
• DCCA EY (Chapman et al. 2022) optimizes the objective of CCA via a sample-based
EigenGame.
• DCCA GHA (Chapman et al. 2022) solves the objective of CCA by a sample-based gen-
eralized Hebbian algorithm.
• DCCAE/DGCCAE (Wang et al. 2015) introduces reconstruction objectives to DCCA,
which simultaneously optimize the canonical correlation between the learned represen-
tations and the reconstruction errors of the autoencoders."
N,0.6842105263157895,"• DCCA PRIVATE/DGCCA PRIVATE (Wang et al. 2016) incorporates dropout and pri-
vate autoencoders, thus preserving both shared and view-specific information."
N,0.6864035087719298,Information theory-based methods:
N,0.6885964912280702,"• MVTCAE (Hwang et al. 2021) maximizes the reduction in Total Correlation to capture
both shared and view-specific factors of variations."
N,0.6907894736842105,"All CCA-based methods leverage the implementation of CCA-Zoo (Chapman & Wang 2021). To
ensure fairness, we use the official implementation of MVTCAE while replacing the strong CNN
backbone with MLP."
N,0.6929824561403509,"A.4
Implementation Details of Synthetic Datasets"
N,0.6951754385964912,"We draw n random samples with dim d from a Gaussian distribution as G ∈Rd×n to represent
complete representations of n objects. We define the non-linear transformation ϕk as the addition
of noise to the data, followed by passing it through a randomly generated MLP. To generate the data
for the k-th view, we select specific feature dimensions from G based on a given common rate 1
and then apply ϕk to those selected dimensions. And we define ψj as a linear layer, and task Tj is
generated by directly passing G through ψj."
N,0.6973684210526315,"A.5
Hyper-parameter Settings"
N,0.6995614035087719,"To ensure a fair comparison, we tune the hyper-parameters of all baselines within the ranges sug-
gested in the original papers, including hyper-parameter r of ridge regularization, except for the
following fixed settings:"
N,0.7017543859649122,"The embedding size for the real-world datasets is set as 200, while the size for synthetic datasets is
set as 100. Batch size is min(2000, full-size). The same MLP architectures are used for D(G)CCA-
based methods. The hyper-parameter r of ridge regularization is set as 0 in our NR-DCCA and
NR-DGCCA. And the best r for Linear (G)CCA and D(G)CCA-based methods is tuned on the
validation data (synthetic datasets and PolyMnist: 1e-3, CUB and Caltech101 : 0)."
N,0.7039473684210527,"In the synthetic datasets, Linear CCA and Linear GCCA use a minimum learning rate of 1e −4,
DCCA, DGCCA, DCCAE, and DGCCAE methods utilize a bigger learning rate of 5e −3.
DCCA PRIVATE/DGCCA PRIVATE employ a slightly higher learning rate of 1e −2. In contrast,
our proposed methods, NR-DCCA/NR-DGCCA, utilize the maximum learning rate of 1.5e −2.
And the regularization weight α is set as 200."
N,0.706140350877193,"In the real-world datasets, the learning rates for all deep methods are set to 1e−4 while that of Linear
CCA and Linear GCCA are 1e −5. To expedite the computation of Corr(Xk, Ak), in the real-world
datasets, we simply employ Xk[: outdim, :] and Ak[: outdim, :] to compute of Corr(Xk, Ak). The
optimal α values of NR-DCCA for the CUB, PolyMnist, and Caltech datasets are found to be 1.5,
2, and 10, respectively."
N,0.7083333333333334,"A.5.1
Hyper-parameter r in Ridge Regularization"
N,0.7105263157894737,"In this section, we discuss the effects of hyper-parameter r in ridge regularization. Ridge regulariza-
tion is commonly used across almost all (D)CCA methods, which improves numerical stability. It
works by adding an identity matrix I to the estimated covariance matrix. However, ridge regulariza-
tion mainly regularizes the features, rather than the transformation (i.e., Wk in Linear CCA and fk
in DCCA) and it cannot prevent the weight matrices in DNNs from being low-rank or redundant. To
further support our arguments, we provide the experimental results with different ridge parameters
on a real-world dataset CUB as shown in Figure 6. One can see that the ridge regularization even
damages the performance of DCCA and also leads to an increase in the internal correlation within
the feature and the correlation between the feature and noise. In our NR-DCCA, we set the ridge
parameter to zero. We conjecture the reason is that the large ridge parameter could make the neural
network even “lazier” to actively project the data into a better feature space, as the full-rank property
of features and covariance matrix are already guaranteed."
N,0.7127192982456141,Figure 6: The effects of hyper-parameter r of DCCA in the CUB dataset.
N,0.7149122807017544,"A.5.2
Hyper-parameter α of NR-DCCA"
N,0.7171052631578947,"The choice of the hyper-parameter α is essential in NR-DCCA. Different from the conventional
hyper-parameter tuning procedures, the determination of α is simpler, as we can search for the
smallest α that can prevent the model collapse, and the model collapse can be directly observed
on the validation data. Specifically, we increase the α adaptively until the model collapse issue is
tackled, i.e., the correlation with noise will not increase or the performance of DCCA will not drop
with increasing training epochs, then the optimal α is found. To further illustrate the influence of α
in NR-DCCA, we present performance curves of NR-DCCA in CUB under different α. As shown
in Figure 7, if α is too large, the convergence of the training becomes slow; if α is too small, model
collapse remains. Additionally, one can see the NR-DCCA outperforms DCCA robustly with a wide
range of α."
N,0.7192982456140351,Figure 7: The effects of hyper-parameter α of NR-DCCA in the CUB dataset.
N,0.7214912280701754,"A.6
Effects of the Distribution of Noise"
N,0.7236842105263158,"From our theoretical analysis, the most important feature of noise in NR is that the sampled noise
matrix is a full-rank matrix. Therefore, continuous distributions such as the uniform distribution can
also be applied to NR, which demonstrates the robustness of the proposed NR method. We compare
NR-DCCA with different noise distributions on synthetic datasets, and both noises are effective in
suppressing model collapse as shown in Table 1."
N,0.7258771929824561,Table 1: Effect of noise on DCCA and NR-DCCA
N,0.7280701754385965,"Epoch
DCCA
NR-DCCA (Gaussian Noise)
NR-DCCA (Uniform Noise)"
N,0.7302631578947368,"100
0.284 ± 0.012
0.295 ± 0.005
0.291 ± 0.004
800
0.137 ± 0.028
0.313 ± 0.004
0.313 ± 0.005
1200
0.106 ± 0.027
0.312 ± 0.005
0.316 ± 0.005"
N,0.7324561403508771,"A.7
Effects of Depths of Encoders"
N,0.7346491228070176,"In this section, we test the effects of depths of encoders (i.e. MLPs) on model collapse and NR.
Specifically, we increase the depth of MLPs to observe the variation in the performance of DCCA
and NR-DCCA on synthetic datasets. As shown in Table 2, The increase in network depth results
in a faster decline in DCCA performance, while NR-DCCA still maintains a stable performance."
N,0.7368421052631579,Table 2: Performance comparison of DCCA and NR-DCCA across different network depths.
N,0.7390350877192983,"Epoch/R2
1 hidden layer
2 hidden layers
3 hidden layers"
N,0.7412280701754386,"DCCA
NR-DCCA
DCCA
NR-DCCA
DCCA
NR-DCCA"
N,0.743421052631579,"100
0.284 ± 0.012
0.295 ± 0.005
0.161 ± 0.013
0.304 ± 0.006
0.071 ± 0.084
0.299 ± 0.010
800
0.137 ± 0.028
0.313 ± 0.004
−0.072 ± 0.071
0.307 ± 0.005
−0.975 ± 0.442
0.309 ± 0.005
1200
0.106 ± 0.027
0.312 ± 0.005
−0.154 ± 0.127
0.303 ± 0.006
−1.412 ± 0.545
0.308 ± 0.006"
N,0.7456140350877193,"A.8
Effects of NR Loss on Filter Redundancy"
N,0.7478070175438597,"Extensive research has established a significant correlation between the redundancy of neurons or
filters and the compromised generalization capabilities of neural networks, indicating a propensity
for overfitting (Wang et al. 2020, Morcos et al. 2018, Zhu et al. 2018). Considering the fully con-
nected layer with 1024 units in a MLP network as a paradigm, the initial layer’s weights, denoted
by W ∈R1024×3×28×28, can be interpreted as 1024 discriminative filters. These filters operate on
images with 3 channels, each of size 28 × 28, with every filter representing a vector in a 3 × 28 × 28
dimensional space. Subsequently, a similarity matrix S is constructed, wherein each element Sij
quantifies the cosine similarity between the ith and jth filters, with higher values indicating greater
redundancy. To further assess filter redundancy in W, we employ NESum, a metric designed for
evaluating redundancy and whiten degrees of features (Zhang et al. 2023).
Definition 3 (NESum of Weight) Given a weight matrix W ∈Routput×input with an accompa-
nying output-wise similarity matrix S ∈Routput×output and eigenvalues {λi}output
i=1
sorted in de-
scending order, the normalized eigenvalue sum is defined as follows:"
N,0.75,"NESum(W) =
1
output"
N,0.7521929824561403,"output
X i=1 λi
λ1"
N,0.7543859649122807,"In Figure 8, we present the evolution of the average NESum across all weights within the trained
encoders. Notably, we observe a sustained increase in NESum exclusively in NR-DCCA throughout
prolonged training epochs. This phenomenon underscores the efficacy of the loss of NR in reducing
filter redundancy, crucially preventing low-rank solutions."
N,0.756578947368421,Figure 8: Average NESum across all weights within the trained encoders.
N,0.7587719298245614,"A.9
Visualization of Learned Representations"
N,0.7609649122807017,"To further demonstrate the effectiveness of our method, we employ 2D-tSNE visualization to depict
the learned representations of the CUB dataset (test set) under different methods. Each data point
is colored based on its corresponding class, as illustrated in Figure 9. There are a total of 10 cat-
egories, with 60 data points in each category. A reasonable distribution of learned representations
entails that data points belonging to the same class are grouped in the same cluster, which is distin-
guishable from clusters representing other classes. Additionally, within each cluster, the data points"
N,0.7631578947368421,"should exhibit an appropriate level of dispersion, indicating that the data points within the same
class can be differentiated rather than collapsing into a single point. This dispersion is indicative of
the preservation of as many distinctive features of the data as possible."
N,0.7653508771929824,"From Figure. 9, we can observe that CCA, DCCA / DGCCA have all confused the data from dif-
ferent categories. Specifically, CCA completely scatters the data points as it cannot handle non-
linear relationships. By incorporating autoencoders, DCCAE / DGCCAE and DCCA PRIVATE /
DGCCA PRIVATE have partially separated the data; however, they have not fully separated the
green and orange categories. NR-DCCA / NR-DGCCA is the only method that successfully sepa-
rates all categories."
N,0.7675438596491229,"It is worth noting that our approach not only separates the data into different clusters but also main-
tains dispersion within each cluster. Unlike DCCA PRIVATE / DGCCA PRIVATE, where the data
points within a cluster form a strip-like distribution, our method ensures that the data points within
each cluster remain appropriately scattered."
N,0.7697368421052632,"(a) CONCAT
(b) CCA
(c) MVTCAE"
N,0.7719298245614035,"(d) DCCA
(e) DCCAE
(f) NR-DCCA
(g) DCCA PRIVATE"
N,0.7741228070175439,"(h) DGCCA
(i) DGCCAE
(j) NR-DGCCA
(k) DGCCA PRIVATE"
N,0.7763157894736842,Figure 9: Visualization of the learned representations with t-SNE in the CUB dataset.
N,0.7785087719298246,"A.10
DGCCA and NR-DGCCA"
N,0.7807017543859649,"This section presents the experimental results for DGCCA and NR-DGCCA, which supplement the
results of GCCA and NR-DCCA presented in the main paper. In general, DGCCA is a variant of
DCCA, and hence the proposed noise regularization approach can also be applied. We repeat the
experiments in Figures 4,and 5, and hence we have the results for DGCCA in Figure 10, and 11.
One can see that the proposed noise regularization approach can also help DGCCA prevent model
collapse, proving its generalizability."
N,0.7828947368421053,(a) Performance
N,0.7850877192982456,"(b) Correlation
(c) NESum of weight matrices"
N,0.7872807017543859,"(d) Reconstruction
(e) Denoisng"
N,0.7894736842105263,"Figure 10: (a) Mean and standard deviation of the (D)GCCA-based method performance across
synthetic datasets in different training epochs.(b) The mean correlation between noise and real data
after transformation varies with epochs. (c) Average NESum across all weights within the trained
encoders. (d,e) The mean of Reconstruction and Denoising Loss on the test set."
N,0.7916666666666666,"Figure 11: Performance of DGCCA-based methods in real-world datasets. Each column represents
the performance on a specific dataset. The number of views in the dataset is denoted in the paren-
theses next to the dataset name."
N,0.793859649122807,"A.11
Additional Experimental Results"
N,0.7960526315789473,"Table 3 and 4 present the model performance of various MVRL methods in synthetic and real-world
datasets, and both tables correspond to the final epoch of the results presented in Figure 4and 5. It
should be noted that the values in Table 3 represent the mean and standard deviation of the methods
across different tasks, indicating their performance and variability."
N,0.7982456140350878,Table 3: Performance in synthetic datasets.
N,0.8004385964912281,"R2/Common Rate
0%
20%
40%
60%
80%
100%
CONCAT
0.253±0.038
0.255±0.039
0.250±0.040
0.254±0.040
0.256 ±0.042
0.264 ± 0.033
Linear CCA
0.179±0.030
0.184±0.035
0.172±0.033
0.182±0.034
0.182±0.034
0.188±0.031
Linear GCCA
0.177±0.030
0.184±0.036
0.171±0.033
0.182±0.034
0.182±0.033
0.182±0.031
KCCA
0.243±0.047
0.261±0.046
0.260±0.043
0.272±0.045
0.276±0.049
0.288±0.038
PRCCA
0.212±0.053
0.249±0.046
0.216±0.055
0.267±0.046
0.256±0.052
0.284±0.039
MVTCAE
0.065±0.015
0.071±0.016
0.067±0.016
0.069±0.016
0.071±0.016
0.069±0.015
DCCA
0.053±0.044
0.094±0.046
0.123±0.047
0.107±0.046
0.125±0.052
0.133±0.044
DCCAE
0.063±0.044
0.090±0.039
0.126±0.047
0.104±0.045
0.098±0.060
0.139 ±0.041
DCCA PRIVATE
0.264±0.039
0.171±0.040
0.176±0.042
0.168±0.039
0.172±0.041
0.181±0.035
DCCA GHA
0.251±0.049
0.249±0.046
0.243±0.047
0.252±0.052
0.268±0.053
0.275±0.046
DCCA EY
0.195±0.050
0.205±0.044
0.220±0.041
0.214±0.046
0.215±0.046
0.234±0.041
NR-DCCA (ours)
0.311±0.043
0.314±0.046
0.306±0.043
0.309±0.042
0.313±0.049
0.322±0.040
DGCCA
0.231±0.042
0.244±0.040
0.242±0.040
0.211±0.039
0.240±0.040
0.256±0.037
DGCCAE
0.209±0.039
0.235±0.042
0.240±0.040
0.214±0.038
0.235±0.041
0.254±0.036
DGCCA PRIVATE
0.264±0.039
0.171±0.040
0.176±0.042
0.168±0.039
0.172±0.042
0.181±0.035
NR-DGCCA (ours)
0.314±0.044
0.317±0.045
0.305±0.043
0.308±0.044
0.315±0.049
0.322±0.040"
N,0.8026315789473685,Table 4: Performance in real-world datasets
N,0.8048245614035088,"F1 Score/Data
PolyMnist (2)
PolyMnist (3)
PolyMnist (4)
PolyMnist (5)
CUB
Caltech101
CONCAT
0.828
0.937
0.964
0.962
0.878
0.597
Linear CCA
0.818
0.929
0.955
0.957
0.878
0.599
Linear GCCA
0.818
0.828
0.956
0.958
0.879
0.596
PRCCA
0.712
0.849
0.899
0.918
-
-
MVTCAE
0.852
0.901
0.964
0.964
0.900
0.284
DCCA
0.865
0.957
0.964
0.938
0.805
0.604
DCCAE
0.868
0.956
0.961
0.987
0.850
0.605
DCCA PRIVATE
0.915
0.959
0.968
0.955
0.853
0.480
NR-DCCA (ours)
0.910
0.970
0.991
0.993
0.921
0.625
DGCCA
0.875
0.964
0.986
0.941
0.790
0.617
DGCCAE
0.879
0.960
0.988
0.934
0.814
0.612
DGCCA PRIVATE
0.907
0.965
0.969
0.969
0.864
0.476
NR-DGCCA(ours)
0.903
0.971
0.991
0.994
0.917
0.621"
N,0.8070175438596491,"A.12
Complexity Analysis"
N,0.8092105263157895,"In this section, we compare the computational complexity of different DCCA-based methods. As-
suming that we have data from K views, with each view containing N samples and D feature
dimensions, then we have the computational complexity of each method in Table 5."
N,0.8114035087719298,Table 5: Comparisons of computational complexity against baselines
N,0.8135964912280702,"DCCA
DCCAE
DCCA PRIVATE
NR-DCCA
Generation of Noise
-
-
-
O(K ∗N ∗D)
MLP Encoder
O(K ∗N ∗L ∗H2)
O(K ∗N ∗L ∗H2)
O(2 ∗K ∗N ∗L ∗H2)
O(2 ∗K ∗N ∗L ∗H2)
MLP Decoder
-
O(K ∗N ∗L ∗H2)
O(K ∗N ∗L ∗H2)
-
Reconstruction Loss
-
O(K ∗N ∗D)
O(K ∗N ∗D)
-
Correlation Maximization
O((M ∗K)3)
O((M ∗K)3)
O((M ∗K)3)
O((M ∗K)3)
Noise Regularization
-
-
-
O(2 ∗K ∗(M ∗K)3)"
N,0.8157894736842105,"• Complexity of MLP: We will use neural networks with the same MLP structure, consisting
of L hidden layers, each with H neurons. Therefore, the computational complexity of one
pass of the data through the neural networks can be expressed as O(N ∗(D ∗H +D ∗M +
L ∗H2)). To simplify, we use O(N ∗L ∗H2)."
N,0.8179824561403509,"• Complexity of Corr: During the process of calculating Cor among K views, three main
computations are involved. The calculation complexity of the covariance is O(N ∗(M ∗
K)2. Second, the complexity of the inverse matrix and the eigenvalues are O((M ∗K)3.
As a result, the computational complexity of calculating Cor can be considered as O((M ∗
K)3).
• Complexity of reconstruction loss: The reconstruction loss, also known as the mean
squared error (MSE) loss, has a complexity of O(N ∗D)."
N,0.8201754385964912,NeurIPS Paper Checklist
CLAIMS,0.8223684210526315,1. Claims
CLAIMS,0.8245614035087719,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8267543859649122,Answer: [Yes]
CLAIMS,0.8289473684210527,"Justification: In the abstract, we explain that we have found that model collapse occurs in
DCCA, and we propose the NR approach to solve this phenomenon, and the NR approach
generalizes well in DGCCA. In the introduction, we list our 4 contributions in detail."
CLAIMS,0.831140350877193,Guidelines:
CLAIMS,0.8333333333333334,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper."
LIMITATIONS,0.8355263157894737,2. Limitations
LIMITATIONS,0.8377192982456141,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8399122807017544,Answer: [Yes]
LIMITATIONS,0.8421052631578947,"Justification: Our approach requires the introduction of additional computational overhead,
which is discussed in Appendix. Our theory needs to assume that 1) feature matrice is full-
rank and 2) the input and output dimensions are the same. The first assumption is fairly
common because the DCCA itself requires a large batchsize. The second assumption is
the same as the theoretical assumption that Linear CCA gets a full-rank weight matrix, and
our experiments verify that our NR approach is still effective even if the dimensions are
different."
LIMITATIONS,0.8442982456140351,Guidelines:
LIMITATIONS,0.8464912280701754,"• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best"
LIMITATIONS,0.8486842105263158,"judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8508771929824561,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8530701754385965,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8552631578947368,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8574561403508771,"Justification: We declare our theorem in the main paper and provide a complete proof in
the appendix."
THEORY ASSUMPTIONS AND PROOFS,0.8596491228070176,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8618421052631579,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8640350877192983,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8662280701754386,"Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.868421052631579,Answer:[Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8706140350877193,"Justification: All of our experimental codes including how to divide the generated data,
training and testing are provided in the supplementary materials. And we’ve fixed random
seeds."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8728070175438597,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.875,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8771929824561403,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8793859649122807,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.881578947368421,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8837719298245614,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8859649122807017,"Justification: We include instructions in the appendix, including how to download and
divide the data set, how to execute training and test scripts, and how to visualize the results."
OPEN ACCESS TO DATA AND CODE,0.8881578947368421,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8903508771929824,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8925438596491229,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8947368421052632,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8969298245614035,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8991228070175439,"Justification: We have explained data division and hyperparameters in detail in both Ap-
pendix and code, and discussed the impact of hyperparameters on results."
OPEN ACCESS TO DATA AND CODE,0.9013157894736842,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9035087719298246,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9057017543859649,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9078947368421053,"Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9100877192982456,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9122807017543859,"Justification: We randomly generated dozens of downstream tasks and reported the mean
and standard deviation of their performance. And we used a 5-fold cross-validation to
report the average of the 5-fold performance."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9144736842105263,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9166666666666666,"• The answer NA means that the paper does not include experiments.
• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.918859649122807,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9210526315789473,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9232456140350878,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9254385964912281,"Justification: We compared the time cost of different methods in Appendix, and explained
that our experiments were all completed on a single 3090 GPU."
EXPERIMENTS COMPUTE RESOURCES,0.9276315789473685,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9298245614035088,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper)."
CODE OF ETHICS,0.9320175438596491,9. Code Of Ethics
CODE OF ETHICS,0.9342105263157895,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9364035087719298,Answer: [Yes]
CODE OF ETHICS,0.9385964912280702,"Justification: The data sets we use are all public data sets, and we strictly follow previous
studies, which will not have a bad impact on the society."
CODE OF ETHICS,0.9407894736842105,Guidelines:
CODE OF ETHICS,0.9429824561403509,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.9451754385964912,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
CODE OF ETHICS,0.9473684210526315,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Multi-view data is very common in society, and our method can more stably
represent multi-view data, so that it can be applied to a wide range of tasks.
Guidelines:"
CODE OF ETHICS,0.9495614035087719,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
CODE OF ETHICS,0.9517543859649122,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We found the shortcomings of the original DCCA and tried to improve it so
that it would not be applied to the wrong places.
Guidelines:"
CODE OF ETHICS,0.9539473684210527,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
CODE OF ETHICS,0.956140350877193,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
CODE OF ETHICS,0.9583333333333334,Answer: [Yes]
CODE OF ETHICS,0.9605263157894737,"Justification: We referenced the framework we used and the data set we used. In the code,
we listed the URL of each data set and the code environment we needed."
CODE OF ETHICS,0.9627192982456141,Guidelines:
CODE OF ETHICS,0.9649122807017544,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9671052631578947,13. New Assets
NEW ASSETS,0.9692982456140351,"Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?"
NEW ASSETS,0.9714912280701754,Answer: [NA]
NEW ASSETS,0.9736842105263158,Justification: We do not introduce new assets.
NEW ASSETS,0.9758771929824561,Guidelines:
NEW ASSETS,0.9780701754385965,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802631578947368,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846491228070176,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868421052631579,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890350877192983,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912280701754386,"• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993421052631579,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956140350877193,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978070175438597,"• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review."
