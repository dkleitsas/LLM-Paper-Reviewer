Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009505703422053232,"Prompt learning has proven effective in adapting vision language models for
1"
ABSTRACT,0.0019011406844106464,"downstream tasks. However, existing methods usually append learnable prompt
2"
ABSTRACT,0.0028517110266159697,"tokens solely with the category names to obtain textual features, which fails to fully
3"
ABSTRACT,0.0038022813688212928,"leverage the rich context indicated in the textual category name. To address this
4"
ABSTRACT,0.004752851711026616,"issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs
5"
ABSTRACT,0.005703422053231939,"LLMs to generate a tree of attributes with a ‚Äúconcept - attribute - description‚Äù
6"
ABSTRACT,0.006653992395437262,"structure for each associated category name, and then learn the hierarchy with
7"
ABSTRACT,0.0076045627376425855,"vision and text prompt tokens. Unlike existing methods that merely augment
8"
ABSTRACT,0.008555133079847909,"category names with a set of unstructured descriptions, our approach essentially
9"
ABSTRACT,0.009505703422053232,"distills structured knowledge graphs associated with class names from LLMs.
10"
ABSTRACT,0.010456273764258554,"Furthermore, our approach introduces text and vision prompts designed to explicitly
11"
ABSTRACT,0.011406844106463879,"learn the corresponding visual attributes, effectively serving as domain experts.
12"
ABSTRACT,0.012357414448669201,"Additionally, the general and diverse descriptions generated based on the class
13"
ABSTRACT,0.013307984790874524,"names may be wrong or absent in the specific given images. To address this
14"
ABSTRACT,0.014258555133079848,"misalignment, we further introduce a vision-conditional pooling module to extract
15"
ABSTRACT,0.015209125475285171,"instance-specific text features. Extensive experimental results demonstrate that
16"
ABSTRACT,0.016159695817490494,"our approach outperforms state-of-the-art methods on the zero-shot base-to-novel
17"
ABSTRACT,0.017110266159695818,"generalization as well as few-shot classification across 11 diverse datasets.
18"
INTRODUCTION,0.01806083650190114,"1
Introduction
19"
INTRODUCTION,0.019011406844106463,"Recent advancements in vision-language models (VLMs) like CLIP [33] and ALIGN [13] merge
20"
INTRODUCTION,0.019961977186311788,"the capabilities of visual perception with linguistic understanding, which have revolutionized the
21"
INTRODUCTION,0.02091254752851711,"landscape with their zero-shot learning abilities. They proficiently handle tasks on unseen data,
22"
INTRODUCTION,0.021863117870722433,"bypassing the conventional requirement for task-specific training. This feature has enabled a plethora
23"
INTRODUCTION,0.022813688212927757,"of applications, ranging from content-based image retrieval to complex visual question answering,
24"
INTRODUCTION,0.02376425855513308,"setting new benchmarks in the domain. A crucial development in this domain is the concept of
25"
INTRODUCTION,0.024714828897338403,"prompt learning, which has significantly influenced both natural language processing (NLP) [20‚Äì22]
26"
INTRODUCTION,0.025665399239543727,"and vision-only models [14, 43, 44, 51]. This approach leverages learnable prompts to guide model
27"
INTRODUCTION,0.026615969581749048,"understanding, tailoring responses to specific tasks or datasets.
28"
INTRODUCTION,0.027566539923954372,"Prompt learning, particularly in vision-language models, has garnered considerable interest due
29"
INTRODUCTION,0.028517110266159697,"to its parameter efficiency and rapid convergence [54, 53, 55, 8, 23]. Techniques like CoOp [54]
30"
INTRODUCTION,0.029467680608365018,"optimize learnable continuous prompts for few-shot image recognition, enhancing model performance
31"
INTRODUCTION,0.030418250950570342,"significantly. Recent efforts have expanded to multimodal prompt learning, optimizing prompts
32"
INTRODUCTION,0.03136882129277566,"in both visual and language domains [15, 16, 38, 19]. Despite their success, these models rely on
33"
INTRODUCTION,0.03231939163498099,"simplistic text prompts, typically formatted as ‚Äúa photo of a {class}‚Äù, illustrated in Fig. 1 (a). While
34"
INTRODUCTION,0.03326996197718631,"functional, this approach lacks depth, failing to encapsulate the intricacies and finer details inherent in
35"
INTRODUCTION,0.034220532319391636,"dumplings
Example Tree of A.ribute
Color
‚Ä¢ a pale beige color from the dough exterior.
‚Ä¢ a golden-brown hue from pan-frying or deep-"
INTRODUCTION,0.03517110266159696,"frying
Shape
‚Ä¢ round with a pleated edge
‚Ä¢ crescent-shaped, with a fold in the dough
Texture
‚Ä¢ so: and chewy texture from the dough
‚Ä¢ a crispy texture on the bo;om from pan-frying
¬∑¬∑¬∑
Presenta9on
‚Ä¢ served steamed in a bamboo basket
‚Ä¢ served with a dipping sauce"
INTRODUCTION,0.03612167300380228,dumplings
INTRODUCTION,0.0370722433460076,"wrapped in a thin dough
boiled, steamed, or fried
served with a dipping sauce"
INTRODUCTION,0.03802281368821293,"¬∑¬∑¬∑
A photo of a dumplings"
INTRODUCTION,0.03897338403041825,dumplings
INTRODUCTION,0.039923954372623575,"Color
Shape
Texture
PresentaDon
¬∑¬∑¬∑"
INTRODUCTION,0.0408745247148289,"Dis$lled Knowledge Graph
Unstructured Set"
INTRODUCTION,0.04182509505703422,"Single Template
üßëüî¨A photo of a {class}"
INTRODUCTION,0.04277566539923954,"(a)
(b)
(c)"
INTRODUCTION,0.043726235741444866,"¬∑¬∑¬∑
¬∑¬∑¬∑
¬∑¬∑¬∑
¬∑¬∑¬∑ (d)"
INTRODUCTION,0.04467680608365019,"Figure 1: Illustration of the methods for CLIP text prompts formation. (a) Manually created prompt
with the single ‚Äúa photo of a {class}‚Äù template; (b) A unstructured set of detailed descriptions
generated by LLMs; (c) The proposed Tree of Attribute that organizes the descriptions in a ‚Äúconcept -
attribute - descriptions‚Äù structure, essentially distilling knowledge graphs from LLMs; (d) An example
Tree of Attribute for ‚Äúdumplings‚Äù."
INTRODUCTION,0.045627376425855515,"visual data. Such limitations hinder the model‚Äôs ability to fully leverage the rich, descriptive potential
36"
INTRODUCTION,0.04657794676806084,"offered by more detailed and contextually relevant textual information.
37"
INTRODUCTION,0.04752851711026616,"In parallel, another stream of research has been exploring the utilization of large language models
38"
INTRODUCTION,0.04847908745247148,"(LLMs) to generate more elaborate and descriptive text prompts for enhancing zero-shot learning
39"
INTRODUCTION,0.049429657794676805,"capabilities [26, 32, 35, 17, 30, 48, 49, 36, 52, 40]. These LLM-generated descriptions offer a wealth
40"
INTRODUCTION,0.05038022813688213,"of detail and context, potentially enriching the model‚Äôs interpretative capabilities. However, current
41"
INTRODUCTION,0.051330798479087454,"methodologies in integrating these descriptions often do not exploit the full potential of this richness.
42"
INTRODUCTION,0.05228136882129278,"As shown in Fig. 1 (b), most of these approaches lack a structured framework to organize and utilize
43"
INTRODUCTION,0.053231939163498096,"these descriptions effectively, leading to a scattergun approach where not all generated descriptions
44"
INTRODUCTION,0.05418250950570342,"are contextually relevant or optimally aligned with the visual content. In addition, as noted in [35],
45"
INTRODUCTION,0.055133079847908745,"descriptions generated by such paradigms are usually diverse, which covers most possibilities of the
46"
INTRODUCTION,0.05608365019011407,"class, but include descriptions that are either likely not co-occurring, e.g. ‚Äústeamed‚Äù and ‚Äúfried‚Äù, or
47"
INTRODUCTION,0.057034220532319393,"absent in the input image, e.g. ‚Äúlong tail‚Äù for a cat shot from the front, necessitating the need for a
48"
INTRODUCTION,0.05798479087452472,"selective pooling mechanism for clearer image-text alignments.
49"
INTRODUCTION,0.058935361216730035,"In response to these challenges, our work introduces ‚ÄúTree of Attribute Prompt learning (TAP),‚Äù
50"
INTRODUCTION,0.05988593155893536,"a method that redefines the integration and utilization of detailed descriptions within VLMs. As
51"
INTRODUCTION,0.060836501901140684,"indicated in Fig. 1 (c), unlike existing methods that merely augment category names with a set of
52"
INTRODUCTION,0.06178707224334601,"unstructured descriptions, our approach essentially distills structured knowledge graphs associated
53"
INTRODUCTION,0.06273764258555133,"with class names from LLMs. Specifically, we adopt a hierarchical, tree-like structure to systemati-
54"
INTRODUCTION,0.06368821292775666,"cally generate and integrate descriptions, ensuring a layered and comprehensive understanding of
55"
INTRODUCTION,0.06463878326996197,"visual content. Each branch of this tree represents a specific attribute, with finer details fleshed out in
56"
INTRODUCTION,0.0655893536121673,"the subsequent leaves, ensuring that every aspect of the visual content is captured and represented.
57"
INTRODUCTION,0.06653992395437262,"Furthermore, we reimagine the learnable prompt tokens as ‚Äúdomain experts‚Äù, each specializing in
58"
INTRODUCTION,0.06749049429657794,"different aspects of the image, supplemented by the CLS token‚Äôs global perspective. In addition, we
59"
INTRODUCTION,0.06844106463878327,"introduce vision-conditional layers for each expert-attribute pair, which pool the most applicable
60"
INTRODUCTION,0.06939163498098859,"descriptions from each of the attribute sets with condition on the input image content, ensuring
61"
INTRODUCTION,0.07034220532319392,"optimal image-text alignment. This setup not only provides a detailed, attribute-focused analysis but
62"
INTRODUCTION,0.07129277566539924,"also harmonizes these insights with the overall context.
63"
INTRODUCTION,0.07224334600760456,"Extensive experiments in both base-to-novel generalization and few-shot classification across 11
64"
INTRODUCTION,0.07319391634980989,"diverse datasets demonstrate the effectiveness of our method. On base-to-novel generalization, TAP
65"
INTRODUCTION,0.0741444866920152,"achieves average performance gains of 1.07% in harmonic mean over the state-of-the-art methods,
66"
INTRODUCTION,0.07509505703422054,"and 9.34% over the vanilla CLIP. Competitive results are also observed in few-shot classification.
67"
RELATED WORK,0.07604562737642585,"2
Related Work
68"
RELATED WORK,0.07699619771863118,"Prompt Learning for Vision-Language Models. Prompt learning bridges linguistic understanding
69"
RELATED WORK,0.0779467680608365,"and visual perception by guiding VLMs with text prompts, a concept originated in NLP [20‚Äì22]
70"
RELATED WORK,0.07889733840304182,"and adapted to vision-only [14, 43, 44, 51] and multimodal contexts[54, 53, 15, 16, 38, 19, 40, 34,
71"
RELATED WORK,0.07984790874524715,"36, 52, 55, 4, 23]. In the textual domain, CoOp [54] optimizes learnable continuous prompts in
72"
RELATED WORK,0.08079847908745247,"CLIP‚Äôs language branch for few-shot image recognition, while CoCoOp [53] addresses CoOp‚Äôs
73"
RELATED WORK,0.0817490494296578,"overfitting issues by conditioning prompts on visual features. In the visual domain, Visual Prompt
74"
RELATED WORK,0.08269961977186312,"Tuning (VPT) [1] and Dual-modality Prompt Tuning (DPT) [47] enhance CLIP‚Äôs vision encoder by
75"
RELATED WORK,0.08365019011406843,"learning visual prompts in pixel space and dynamically generating prompts through cross-attention,
76"
RELATED WORK,0.08460076045627377,"respectively. TransHP [42] leverages category hierarchy for prompt learning to improve classification
77"
RELATED WORK,0.08555133079847908,"performance. LoGoPrompt [38] enhances classification by incorporating synthetic images with class
78"
RELATED WORK,0.08650190114068441,"name text as auxiliary visual prompts. MaPLe [15] explores multimodal prompt learning, jointly
79"
RELATED WORK,0.08745247148288973,"optimizing prompts in both vision and language branches. Other recent works have focused on
80"
RELATED WORK,0.08840304182509506,"regularizing prompt learning to leverage the knowledge from base VLMs effectively, demonstrating
81"
RELATED WORK,0.08935361216730038,"enhanced generalization in varied downstream visual tasks [16, 4, 36]. PromptSRC, for instance,
82"
RELATED WORK,0.0903041825095057,"introduced a self-regulating method that restricts both the vision and text prompt, demonstrating
83"
RELATED WORK,0.09125475285171103,"improved generalization. Distinct from these approaches, PLOT [5] and ALIGN [41] leverage
84"
RELATED WORK,0.09220532319391635,"Optimal Transport to align multiple prompts with local visual features, either from the multi-head
85"
RELATED WORK,0.09315589353612168,"self-attention layer or at a token level. Our work diverges from these methods by introducing a
86"
RELATED WORK,0.094106463878327,"hierarchical ""Tree of Attribute"" framework derived from LLMs to structure textual descriptions and
87"
RELATED WORK,0.09505703422053231,"guide the learning of specialized ""domain expert"" tokens for attribute-level understanding.
88"
RELATED WORK,0.09600760456273764,"Image classification by descriptions. There‚Äôs a growing emphasis on using visual descriptions for
89"
RELATED WORK,0.09695817490494296,"zero-shot recognition, moving beyond generic prompts [54, 53]. These descriptions, like the ‚Äúfur
90"
RELATED WORK,0.0979087452471483,"pattern‚Äù or ‚Äútail shape‚Äù of a cat, provide fine-grained and distinctive characteristics. The use of LLMs
91"
RELATED WORK,0.09885931558935361,"like GPT-3 [3], allows for efficient generation of a broad spectrum of class-specific descriptions,
92"
RELATED WORK,0.09980988593155894,"offering an advantage over manually crafted templates. While this approach has been extensively
93"
RELATED WORK,0.10076045627376426,"researched in zero-shot contexts [17, 26, 30, 35, 48, 49, 10, 32, 28], its application in conjunction
94"
RELATED WORK,0.10171102661596958,"with prompt learning for few-shot tasks remains relatively unexplored[25, 19, 40, 52, 50]. Previ-
95"
RELATED WORK,0.10266159695817491,"ous methodologies, however, have largely utilized unstructured descriptions, lacking an organized
96"
RELATED WORK,0.10361216730038023,"framework for effective utilization. Our approach diverges by structuring these descriptions into a
97"
RELATED WORK,0.10456273764258556,"‚ÄúTree of Attribute‚Äù model, coupled with learnable visual prompts as domain experts. Additionally,
98"
RELATED WORK,0.10551330798479087,"LLM-generated descriptions often cover a wide range of potential class descriptions, of which not
99"
RELATED WORK,0.10646387832699619,"all may be pertinent to a given image, pointing to the need for a selective pooling mechanism to
100"
RELATED WORK,0.10741444866920152,"ensure optimal image-text alignment. We further introduce a vision-conditional pooling layer for
101"
RELATED WORK,0.10836501901140684,"refined image-text alignment. This structured approach not only enhances the interpretability of the
102"
RELATED WORK,0.10931558935361217,"model‚Äôs learning process but also significantly improves alignment accuracy between image content
103"
RELATED WORK,0.11026615969581749,"and descriptive text.
104"
METHODOLOGY,0.1112167300380228,"3
Methodology
105"
PRELIMINARY,0.11216730038022814,"3.1
Preliminary
106"
PRELIMINARY,0.11311787072243346,"CLIP. Our approach is built on the pre-trained vision-language model, CLIP [33]. Formally, let (x, c)
107"
PRELIMINARY,0.11406844106463879,"denote the dataset, where x is an image and c ‚àà{1, . . . , C} are the class labels. For an image x, the
108"
PRELIMINARY,0.1150190114068441,"vision encoder hI(¬∑) transforms it into a feature vector f v
x = hI(x). Simultaneously, each class label
109"
PRELIMINARY,0.11596958174904944,"c is mapped to a text prompt tc = a photo of a {c}, and converted into textual feature vectors
110"
PRELIMINARY,0.11692015209125475,"f t
c = hT (tc). The predicted class ÀÜy is given by:
111"
PRELIMINARY,0.11787072243346007,"ÀÜy = argmax
c
cos(f v
x, f t
c)
(1)"
PRELIMINARY,0.1188212927756654,"where cos(¬∑) denotes cosine similarity.
112"
PRELIMINARY,0.11977186311787072,"Image classification with class descriptions. To improve the model‚Äôs understanding of the categories
113"
PRELIMINARY,0.12072243346007605,"in the transfer datasets, previous works [26, 35] use more detailed descriptions from Large Language
114"
PRELIMINARY,0.12167300380228137,"Models (LLMs) instead of the simple ""a photo of a {c}"" to prompt the CLIP text encoder.
115"
PRELIMINARY,0.12262357414448669,"Under this approach, a convoluted set of descriptions is generated for a class c as Dc : {""c, which
116"
PRELIMINARY,0.12357414448669202,"is/has/etc description."" }, e.g. c=""television"" and description=""black or grey"".
117"
PRELIMINARY,0.12452471482889733,"This classification is reformulated as
118"
PRELIMINARY,0.12547528517110265,"ÀÜy = argmax
c
1
|Dc| X"
PRELIMINARY,0.12642585551330798,"d‚ààDc
cos(hI(x), hT(d))
(2)"
OVERALL FRAMEWORK,0.12737642585551331,"3.2
Overall Framework
119"
OVERALL FRAMEWORK,0.12832699619771862,"We rethink the descriptions by LLM Dc as nodes in knowledge graphs. While previous methods
120"
OVERALL FRAMEWORK,0.12927756653992395,"generate an unstructured set of descriptions, we distill structured knowledge graphs for each class c
121"
OVERALL FRAMEWORK,0.13022813688212928,Text Encoder ‚ùÑ
OVERALL FRAMEWORK,0.1311787072243346,"¬∑¬∑¬∑
¬∑¬∑¬∑"
OVERALL FRAMEWORK,0.13212927756653992,Vision Encoder ‚ùÑ
OVERALL FRAMEWORK,0.13307984790874525,"¬∑¬∑¬∑
¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑"
OVERALL FRAMEWORK,0.13403041825095058,"+
+
I1"
OVERALL FRAMEWORK,0.13498098859315588,‚Ä¢T1 I1
OVERALL FRAMEWORK,0.1359315589353612,‚Ä¢T2 I1
OVERALL FRAMEWORK,0.13688212927756654,"‚Ä¢T3
I1"
OVERALL FRAMEWORK,0.13783269961977188,"‚Ä¢TC
I1"
OVERALL FRAMEWORK,0.13878326996197718,‚Ä¢T1 I1
OVERALL FRAMEWORK,0.1397338403041825,‚Ä¢T2 I1
OVERALL FRAMEWORK,0.14068441064638784,"‚Ä¢T3
I1"
OVERALL FRAMEWORK,0.14163498098859315,"‚Ä¢TC
I1"
OVERALL FRAMEWORK,0.14258555133079848,‚Ä¢T1 I1
OVERALL FRAMEWORK,0.1435361216730038,‚Ä¢T2 I1
OVERALL FRAMEWORK,0.1444866920152091,"‚Ä¢T3
I1 ‚Ä¢TC ùëâùê∂ùëÉ!"
OVERALL FRAMEWORK,0.14543726235741444,"large, alert ears that are"
OVERALL FRAMEWORK,0.14638783269961977,"wide at the base
minimal furnishing on"
OVERALL FRAMEWORK,0.1473384030418251,"the inside
taper smoothly to a"
OVERALL FRAMEWORK,0.1482889733840304,pointed 6p.
OVERALL FRAMEWORK,0.14923954372623574,ruddy brown coat with
CKING,0.15019011406844107,6cking
CKING,0.15114068441064638,blue 6cked pa:ern
CKING,0.1520912547528517,fawn 6cked pa:ern
CKING,0.15304182509505704,almond-shaped and
CKING,0.15399239543726237,green eyes
CKING,0.15494296577946767,gold eyes
CKING,0.155893536121673,dark eyeliner appearance
CKING,0.15684410646387834,"Fur Pa:ern
Ear Shape
Eye Pa:ern ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑"
CKING,0.15779467680608364,"ùëâùê∂ùëÉ""
ùëâùê∂ùëÉ#
üî•
üî•
üî• üî•
üî•
üî• üî•
ùëù! $ üî• üî• ¬∑¬∑¬∑ ùë£! ! ùëù! %
ùëù"" %
ùëù# % ùëù"" $ ùëù&
$ ùë£"" !
ùë£# !
ùë£$"
CKING,0.15874524714828897,"!
¬∑¬∑¬∑
ùë£! ""
ùë£"" ""
ùë£# ""
ùë£$"
CKING,0.1596958174904943,"""
¬∑¬∑¬∑
ùë£! % ùë£"" %
ùë£# %
ùë£$ %"
CKING,0.16064638783269963,"Text Context           
           Token"
CKING,0.16159695817490494,"Vision Expert 
            Token      
     
      Pooled text feature 
          for ùëé&' A8r. 
          and ùëê&' class ùëù' $ ùëù(% ùë£)( üî•
üî•"
CKING,0.16254752851711027,"ùêºùëëùëíùëõùë°ùëñùë°ùë¶ ùëä! ùëä"" ùë£01"
CKING,0.1634980988593156,"ùëé‚àà1,‚Ä¶,ùê¥"
CKING,0.1644486692015209,"ùëê‚àà1,‚Ä¶,ùê∂"
CKING,0.16539923954372623,ùê∂ùëúùëõùëêùëéùë°.
CKING,0.16634980988593157,ùêøùëíùëéùëüùëõùëéùëèùëôùëí
CKING,0.16730038022813687,ùêπùëüùëúùëßùëíùëõ
CKING,0.1682509505703422,"ùëâùê∂ùëÉ( üî• ùíü"""
CKING,0.16920152091254753,"!
{ }
¬∑¬∑¬∑ ¬∑¬∑¬∑ ùíü!"
CKING,0.17015209125475286,"!
{ }
ùíü#"
CKING,0.17110266159695817,"!
{ }
ùíü$"
CKING,0.1720532319391635,"!
{ }
¬∑¬∑¬∑
ùíü!"
CKING,0.17300380228136883,"""
{ }
ùíü"""
CKING,0.17395437262357413,"%
{ }
ùíü!"
CKING,0.17490494296577946,"%
{ }
ùíü#"
CKING,0.1758555133079848,"%
{ }
ùíü$"
CKING,0.17680608365019013,"%
{ }
ùíü"""
CKING,0.17775665399239543,"""
{ } ùíü#"
CKING,0.17870722433460076,"""
{ }
ùíü$"
CKING,0.1796577946768061,"""
{ }
¬∑¬∑¬∑"
CKING,0.1806083650190114,"ùíü01
{ } üî•
üî•
üî•"
CKING,0.18155893536121673,"üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•"
CKING,0.18250950570342206,"¬∑¬∑¬∑
¬∑¬∑¬∑
¬∑¬∑¬∑"
CKING,0.18346007604562736,"üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî•
üî• ùëù(% ùëë0 1,! ¬∑¬∑¬∑ ùëë0 1,3 ùëë0 1,""
: { {"
CKING,0.1844106463878327,"Figure 2: Overview of the proposed TAP method. TAP utilizes fine-grained descriptions from LLMs
and organizes them in a Tree of Attribute. Vision expert tokens are added to the vision encoder to
learn from specific attributes such as color and shape. A vision-conditional pooling layer is introduced
to ensure optimal image-text alignment. Textual context tokens are also incorporated to the textual
branch, shared across descriptions."
CKING,0.18536121673003803,"from LLM, in which the root node is the class name c, capturing the highest level semantics, and the
122"
CKING,0.18631178707224336,"leaf nodes are the detailed descriptions capturing fine-grained details. In this framework, previous
123"
CKING,0.18726235741444866,"paradigms only generate the leaf nodes of the graph, with the edges and graph structure missing,
124"
CKING,0.188212927756654,"where the rich and inherent structure from the descriptions is overlooked. To address this limitation,
125"
CKING,0.18916349809885932,"we formulate our approach as a Tree of Attribute, which follows the ‚Äúconcept - attribute - description‚Äù
126"
CKING,0.19011406844106463,"structures, as illustrated in Fig. 1 (c).
127"
CKING,0.19106463878326996,"Besides weighting the descriptions equally, previous works typically align descriptions that describe
128"
CKING,0.1920152091254753,"images from different aspects and at different granularities with a singular CLS token from the image
129"
CKING,0.19296577946768062,"encoder. However, while the use of a single CLS token is effective in certain contexts, we note that
130"
CKING,0.19391634980988592,"the CLS token is designed to capture the global information of an input image x [9]. As a result, even
131"
CKING,0.19486692015209126,"though this helps to further inform global understanding, it may fail to effectively capture the nuances
132"
CKING,0.1958174904942966,"and variances at the attribute level. This leads to suboptimal use of the rich descriptions. We address
133"
CKING,0.1967680608365019,"this by introducing a set of learnable prompt tokens that serve as domain experts in the vision branch,
134"
CKING,0.19771863117870722,"each of which aligns with a specific attribute-level textual embedding.
135"
CKING,0.19866920152091255,"Additionally, close inspection of the LLM-generated descriptions indicates limited contextual rele-
136"
CKING,0.19961977186311788,"vance and a high degree of diversity. Previous works [35] reflect the issue of descriptions that are
137"
CKING,0.2005703422053232,"likely not co-occurring e.g. ‚Äústeam‚Äù and ‚Äúfried‚Äù. We further identify cases where the descriptions are
138"
CKING,0.20152091254752852,"technically correct but irrelevant to certain images, such as describing ‚Äúlong tail‚Äù in frontal images
139"
CKING,0.20247148288973385,"of cats, underscoring the need for a selective pooling mechanism. Thus, we introduce a vision-
140"
CKING,0.20342205323193915,"conditional pooling layer to extract instance-specific text features for each attribute for selecting the
141"
CKING,0.20437262357414449,"most applicable descriptions.
142"
CKING,0.20532319391634982,"Overall, our approach utilizes fine-grained descriptions and organizes them in a Tree of Attribute
143"
CKING,0.20627376425855512,"following the ‚Äúconcept - attributes -descriptions‚Äù structure. Learnable vision expert tokens are
144"
CKING,0.20722433460076045,"appended to the input image embedding to learn from specific fine-grained attributes such as color
145"
CKING,0.20817490494296578,"and shape. A vision-conditional pooling layer is further added for each attribute to ensure optimal
146"
CKING,0.20912547528517111,"image-text alignment. Inspired by CoOP [54], we also incorporate textual contextual tokens in the
147"
CKING,0.21007604562737642,"text encoder. The overall framework is presented in Fig. 2.
148"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21102661596958175,"3.3
Tree of Attribute generation by LLMs
149"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21197718631178708,"We redefine the process of integrating LLM-generated descriptions by introducing a knowledge graph
150"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21292775665399238,"Gc = {Vc, Ec} for each class c, where Vc denotes the set of nodes, and Ec denotes the edges that
151"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21387832699619772,"capture the semantic relationship between nodes. In previous works, Vc is the set of descriptions
152"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21482889733840305,"Dc, while Ec is missing. We argue that such methods overlook the inherent structure among the
153"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21577946768060838,"descriptions and thus do not exploit the richness of these descriptions effectively. To better leverage
154"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21673003802281368,"knowledge from LLMs, we introduce an attribute layer to link the root node class name, and the leaf
155"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.217680608365019,"node descriptions. The attribute nodes include visual attributes generated by LLMs, such as color and
156"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21863117870722434,"shape, for systematically guiding description generation as illustrated in Fig. 1 (c). Each branch of
157"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.21958174904942965,"this ‚Äútree‚Äù represents a specific attribute, with the subsequent ‚Äúleaves‚Äù fleshing out the descriptions
158"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22053231939163498,"with finer details. In this framework, Vc includes the class name which is the root node, the set of
159"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2214828897338403,"attributes such as color and shape being the intermediate layer, and lastly the set of descriptions
160"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2224334600760456,"under each attribute node. Ec includes the edges that build up the hierarchy. This structure allows
161"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22338403041825095,"for a nuanced representation of class information, spanning from general concepts down to specific
162"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22433460076045628,"attributes and detailed descriptions.
163"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2252851711026616,"To this end, we introduce the Tree of Attribute (ToA), where we use a tree structure to model the
164"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2262357414448669,"relationship and structure of the descriptions. Let Ac denote the set of attributes, and for each attribute
165"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22718631178707224,"ac ‚ààAc, we denote its leaf nodes as Da
c . Each set Da
c contains descriptions that specifically pertain
166"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22813688212927757,"to attribute a for class c, which is denoted as
167"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.22908745247148288,"Da
c = {da,1
c , da,2
c , . . . , da,n
c
},
(3)"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2300380228136882,"where da,i
c
represents the i-th description for attribute a of class c and n is the number of descriptions
168"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23098859315589354,"per attribute.
169"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23193916349809887,"The process of generating a Tree of Attribute (ToA) unfolds in three steps: 1) Attribute Generation:
170"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23288973384030418,"We first query LLMs with the dataset information and ask it to generate a set of attributes A which are
171"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2338403041825095,"considered relevant and characteristic of the dataset. 2) Example Generation: We then ask LLMs to
172"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23479087452471484,"generate descriptions for a randomly sampled class in the dataset, using the attributes A identified
173"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23574144486692014,"in the previous step. Each description takes the format of ‚Äúclass, which {is/has/etc} {description}‚Äù.
174"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23669201520912547,"Human review is performed to ensure the quality of the example. 3) Description Generation for
175"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.2376425855513308,"All Classes: Building upon the Q&A template from the previous step, the LLM is then tasked with
176"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23859315589353614,"generating descriptions for all classes in the dataset.
177"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.23954372623574144,"Additionally, we incorporate a ‚Äúglobal context‚Äù attribute which is aligned with the CLS token in the
178"
TREE OF ATTRIBUTE GENERATION BY LLMS,0.24049429657794677,"vision encoder. The descriptions are the 7 standard templates provided in [33].
179"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.2414448669201521,"3.4
Learning TAP with Learnable Expert Tokens
180"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.2423954372623574,"To fully exploit the structured Tree of Attribute, we introduce learnable visual expert tokens pv
a in the
181"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24334600760456274,"vision branch to learn from each of the attribute nodes a ‚ààA. Unlike traditional methods that rely
182"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24429657794676807,"on a single CLS token for alignment, these expert tokens enable focused learning on specific image
183"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24524714828897337,"attributes, such as color or shape, enhancing the model‚Äôs performance and interpretability.
184"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.2461977186311787,"We denote the set of introduced visual expert tokens as Pv = {pv
a|a ‚ààA}. Akin to the idea
185"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24714828897338403,"of visual prompt tuning (VPT) [14], we insert Pv into the input sequence of the vision encoder,
186"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24809885931558937,"forming the prompted input sequences ÀúXp = {eCLS, Pv, Epatch}, where eCLS is the input CLS
187"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.24904942965779467,"token, and Epatch denotes the embedded patch tokens. To further boost the model‚Äôs capacity for
188"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.25,"nuanced attribute representation, we employ deep prompting by introducing a zero-initialized layer
189"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.2509505703422053,"residual for each prompt token across transformer layers, which provides more explicit attribute
190"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.25190114068441066,"guidance across transformer layers. In parallel, we adopt a set of m learnable context tokens
191"
LEARNING TAP WITH LEARNABLE EXPERT TOKENS,0.25285171102661597,"Pt = {pt
j|j ‚àà{1, 2, ..., m}} for the text encoder shared across all descriptions, similar to [54].
192"
VISION-CONDITIONAL POOLING,0.25380228136882127,"3.5
Vision-Conditional Pooling
193"
VISION-CONDITIONAL POOLING,0.25475285171102663,"To mitigate issues of misalignment and potential misleading information from the broad spectrum of
194"
VISION-CONDITIONAL POOLING,0.25570342205323193,"LLM-generated descriptions, we proposed an adaptive vision-conditional pooling layer, applicable to
195"
VISION-CONDITIONAL POOLING,0.25665399239543724,"each set of attribute descriptions Da shared across all classes to dynamically pool the most applicable
196"
VISION-CONDITIONAL POOLING,0.2576045627376426,"descriptions based on the visual content of the image x using its corresponding visual expert token
197"
VISION-CONDITIONAL POOLING,0.2585551330798479,"denoted as pv
a,x. For ease of expression, we will proceed without explicitly mentioning x, though it‚Äôs
198"
VISION-CONDITIONAL POOLING,0.25950570342205326,"important to note that both the expert token and the resulting attribute-level embeddings are dependent
199"
VISION-CONDITIONAL POOLING,0.26045627376425856,"on the visual information. Intuitively, VCP uses attention to calculate the similarity between pv
a and
200"
VISION-CONDITIONAL POOLING,0.26140684410646386,"all embedded descriptions in attribute Da, which are then used as weights for a weighted sum of the
201"
VISION-CONDITIONAL POOLING,0.2623574144486692,"original description embeddings. Formally, for each attribute a and its associated expert token pv
a,
202"
VISION-CONDITIONAL POOLING,0.2633079847908745,"the pooled attribute-level embedding va
c for class c and attribute a is:
203"
VISION-CONDITIONAL POOLING,0.26425855513307983,"Query = Wq ¬∑ pv
a,
Key = Wk ¬∑ Emb(Da
c ),"
VISION-CONDITIONAL POOLING,0.2652091254752852,"Attention Score = softmax(Query ¬∑ KeyT ),
va
c = Attention Score ¬∑ Emb(Da
c ), (4)"
VISION-CONDITIONAL POOLING,0.2661596958174905,"where Wq and Wk are learnable weights ‚ààRd√ód, Emb(¬∑) denotes the embedding function, and
204"
VISION-CONDITIONAL POOLING,0.2671102661596958,"softmax(¬∑) is the Softmax function. This layer mirrors cross-attention but omits Wv to maintain the
205"
VISION-CONDITIONAL POOLING,0.26806083650190116,"output within the CLIP V-L space.
206"
TRAINING AND INFERENCE,0.26901140684410646,"3.6
Training and Inference
207"
TRAINING AND INFERENCE,0.26996197718631176,"Training objective. During training, each visual expert token pv
a is aligned with its associated
208"
TRAINING AND INFERENCE,0.2709125475285171,"attribute-level embedding va
c, trained with the following contrastive objective:
209"
TRAINING AND INFERENCE,0.2718631178707224,"Lcon(pv
a, va
c) = ‚àí1 N N
X"
TRAINING AND INFERENCE,0.27281368821292773,"i=1
log
exp(cos(pv
a, va
y)/œÑ)
PC
c=1 exp(cos(pva, vac)/œÑ)
,
(5)"
TRAINING AND INFERENCE,0.2737642585551331,"where N represents the number of training samples, and œÑ is the learned temprature of CLIP. The
210"
TRAINING AND INFERENCE,0.2747148288973384,"total classification loss Lclass is the average of the contrastive loss from each expert token as well as
211"
TRAINING AND INFERENCE,0.27566539923954375,"the CLS token, defined as:
212"
TRAINING AND INFERENCE,0.27661596958174905,"Lclass =
1
|A|  X"
TRAINING AND INFERENCE,0.27756653992395436,"a‚ààA
Lcon(pv
a, va
c))

,
(6)"
TRAINING AND INFERENCE,0.2785171102661597,"Similar to [16] and [4], we regularize the vision CLS token, text feature, and the prediction logits
213"
TRAINING AND INFERENCE,0.279467680608365,"from each attribute using the vanilla CLIP model. We denote the regularization loss as Lreg, where
214"
TRAINING AND INFERENCE,0.2804182509505703,"the details can be found in Appendix. The overall training objective is Ltotal = Lclass + Lreg.
215"
TRAINING AND INFERENCE,0.2813688212927757,"Prediction fusion. During inference, we integrate the prediction by each attribute expert pair by a
216"
TRAINING AND INFERENCE,0.282319391634981,"weighted sum, formulated as follows:
217"
TRAINING AND INFERENCE,0.2832699619771863,"Àúy = argmax
c"
TRAINING AND INFERENCE,0.28422053231939165,"
Œ± cos(f v
CLS, vCLS
c
) + 1 ‚àíŒ±"
TRAINING AND INFERENCE,0.28517110266159695,|A| ‚àí1 X
TRAINING AND INFERENCE,0.28612167300380226,"a‚ààA\{CLS}
cos(pv
a, va
c)

(7)"
TRAINING AND INFERENCE,0.2870722433460076,"where Œ± is a hyperparameter that signifies the weight assigned to the global context provided by the
218"
TRAINING AND INFERENCE,0.2880228136882129,"CLS token, balancing its contribution with that of the attribute-specific expert prompts.
219"
EXPERIMENTS,0.2889733840304182,"4
Experiments
220"
EXPERIMENTS,0.2899239543726236,"We extensively evaluate our method in two settings: 1) Base-to-novel class generalization, where the
221"
EXPERIMENTS,0.2908745247148289,"datasets are equally split into base and novel classes. We train the model on the base classes only and
222"
EXPERIMENTS,0.29182509505703425,"evaluate on both base and novel classes; and 2) Few-shot classification with 16 shots per class.
223"
EXPERIMENTS,0.29277566539923955,"Datasets and baslines. For both base to novel class generalization and few-shot setting, we follow
224"
EXPERIMENTS,0.29372623574144485,"previous works [54, 53], using 11 image recognition datasets. The datasets span a range of recog-
225"
EXPERIMENTS,0.2946768060836502,"nition tasks: ImageNet [7] and Caltech101 [11] for generic object recognition; OxfordPets [30],
226"
EXPERIMENTS,0.2956273764258555,"StanfordCars [18], Flowers102 [27], Food101 [2], and FGVCAircraft [24] for fine-grained classifica-
227"
EXPERIMENTS,0.2965779467680608,"tion; SUN397 [46] for scene recognition; UCF101 [39] for action recognition; DTD [6] for texture
228"
EXPERIMENTS,0.2975285171102662,"classification; and EuroSAT [12] for satellite image analysis. We benchmark against several leading
229"
EXPERIMENTS,0.2984790874524715,"methods, including CLIP [33], CoOp [54], Co-CoOP [53], ProGrad [55], RPO [19], LoGoPrompt
230"
EXPERIMENTS,0.2994296577946768,"[38], and the state-of-the-art PromptSRC [16].
231"
EXPERIMENTS,0.30038022813688214,"Implementation details. A pre-trained CLIP model with a ViT-B/16 vision backbone is used in all
232"
EXPERIMENTS,0.30133079847908745,"of our experiments and results are averaged over 3 runs. We use GPT-3.5-turbo [29] for attribute and
233"
EXPERIMENTS,0.30228136882129275,"description generation. We initialize the text context tokens with the word embedding of a photo
234"
EXPERIMENTS,0.3032319391634981,"of a. For both settings, we iteratively train the vision and text encoders with 5 epochs for vision
235"
EXPERIMENTS,0.3041825095057034,"and 1 epoch for text schedule. We set Œ± = 0.4, ¬µ1 = 10, and ¬µ2 = 2.5 for all datasets. We train
236"
EXPERIMENTS,0.3051330798479088,"the vision encoder for 50 and 100 epochs, and text encoder for 10 and 20 epochs for base-to-novel
237"
EXPERIMENTS,0.3060836501901141,"generalization and few-shot experiments, respectively. For DTD, Oxford Flowers, Stanford Cars,
238"
EXPERIMENTS,0.3070342205323194,"UCF101, and Caltech101 datasets, we use a learning rate of 0.002 for the text encoder and 0.006 for
239"
EXPERIMENTS,0.30798479087452474,"the vision encoder, with ¬µ3 = 3. For the remaining 6 datasets, the learning rates for both text and
240"
EXPERIMENTS,0.30893536121673004,"vision encoders are set as 0.004, with ¬µ3 = 1.5. We also use a Gaussian Prompt Weighting (GPA)
241"
EXPERIMENTS,0.30988593155893535,"following [16], with a mean of 45, std of 10 for base-to-novel generalization, and 80, 20 for few-shot
242"
EXPERIMENTS,0.3108365019011407,"experiments. Refer to the Appendix for additional implementation details.
243"
EXPERIMENTS,0.311787072243346,Table 1: Comparison of TAP in base-to-novel generalization. HM: harmonic mean [45].
EXPERIMENTS,0.3127376425855513,(a) Average
EXPERIMENTS,0.31368821292775667,Base Novel HM
EXPERIMENTS,0.314638783269962,"CLIP
69.34 74.22 71.70
CoOp
82.69 63.22 71.66
Co-CoOp
80.47 71.69 75.83
ProGrad
82.48 70.75 76.16
RPO
81.13 75.00 77.78
LoGoPrompt 84.47 74.24 79.03
PromptSRC 84.26 76.10 79.97
TAP
84.75 77.63 81.04"
EXPERIMENTS,0.3155893536121673,(b) ImageNet
EXPERIMENTS,0.31653992395437264,Base Novel HM
EXPERIMENTS,0.31749049429657794,"CLIP
72.43 68.14 70.22
CoOp
76.47 67.88 71.92
Co-CoOp
75.98 70.43 73.10
ProGrad
77.02 66.66 71.46
RPO
76.60 71.57 74.00
LoGoPrompt 76.74 70.83 73.66
PromptSRC 77.60 70.73 74.01
TAP
77.97 70.40 73.99"
EXPERIMENTS,0.31844106463878324,(c) Caltech101
EXPERIMENTS,0.3193916349809886,Base Novel HM
EXPERIMENTS,0.3203422053231939,"CLIP
96.84 94.00 95.40
CoOp
98.00 89.81 93.73
Co-CoOp
97.96 93.81 95.84
ProGrad
98.02 93.89 95.91
RPO
97.97 94.37 96.03
LoGoPrompt 98.19 93.78 95.93
PromptSRC 98.10 94.03 96.02
TAP
98.90 95.50 97.17"
EXPERIMENTS,0.32129277566539927,(d) OxfordPets
EXPERIMENTS,0.32224334600760457,Base Novel HM
EXPERIMENTS,0.3231939163498099,"CLIP
91.17 97.26 94.12
CoOp
93.67 95.29 94.47
Co-CoOp
95.20 97.69 96.43
ProGrad
95.07 97.63 96.33
RPO
94.63 97.50 96.05
LoGoPrompt 96.07 96.31 96.18
PromptSRC 95.33 97.30 96.30
TAP
95.80 97.73 96.76"
EXPERIMENTS,0.32414448669201523,(e) StanfordCars
EXPERIMENTS,0.32509505703422054,Base Novel HM
EXPERIMENTS,0.32604562737642584,"CLIP
63.37 74.89 68.65
CoOp
78.12 60.40 68.13
Co-CoOp
70.49 73.59 72.01
ProGrad
77.68 68.63 72.88
RPO
73.87 75.53 74.69
LoGoPrompt 78.36 72.39 75.26
PromptSRC 78.27 74.97 76.58
TAP
80.70 74.27 77.35"
EXPERIMENTS,0.3269961977186312,(f) Flowers102
EXPERIMENTS,0.3279467680608365,Base Novel HM
EXPERIMENTS,0.3288973384030418,"CLIP
72.08 77.80 74.83
CoOp
97.60 59.67 74.06
Co-CoOp
94.87 71.75 81.71
ProGrad
95.54 71.87 82.03
RPO
94.13 76.67 84.50
LoGoPrompt 99.05 76.52 86.34
PromptSRC 98.07 76.50 85.95
TAP
97.90 75.57 85.30"
EXPERIMENTS,0.32984790874524716,(g) Food101
EXPERIMENTS,0.33079847908745247,Base Novel HM
EXPERIMENTS,0.33174904942965777,"CLIP
90.10 91.22 90.66
CoOp
88.33 82.26 85.19
Co-CoOp
90.70 91.29 90.99
ProGrad
90.37 89.59 89.98
RPO
90.33 90.83 90.58
LoGoPrompt 90.82 91.41 91.11
PromptSRC 90.67 91.53 91.10
TAP
90.97 91.83 91.40"
EXPERIMENTS,0.33269961977186313,(h) FGVCAircraft
EXPERIMENTS,0.33365019011406843,Base Novel HM
EXPERIMENTS,0.33460076045627374,"CLIP
27.19 36.29 31.09
CoOp
40.44 22.30 28.75
Co-CoOp
33.41 23.71 27.74
ProGrad
40.54 27.57 32.82
RPO
37.33 34.20 35.70
LoGoPrompt 45.98 34.67 39.53
PromptSRC 42.73 37.87 40.15
TAP
44.40 36.50 40.06"
EXPERIMENTS,0.3355513307984791,(i) SUN397
EXPERIMENTS,0.3365019011406844,Base Novel HM
EXPERIMENTS,0.33745247148288976,"CLIP
69.36 75.35 72.23
CoOp
80.60 65.89 72.51
Co-CoOp
79.74 76.86 78.27
ProGrad
81.26 74.17 77.55
RPO
80.60 77.80 79.18
LoGoPrompt 81.20 78.12 79.63
PromptSRC 82.67 78.47 80.52
TAP
82.87 79.53 81.17"
EXPERIMENTS,0.33840304182509506,(j) DTD
EXPERIMENTS,0.33935361216730037,Base Novel HM
EXPERIMENTS,0.3403041825095057,"CLIP
53.24 59.90 56.37
CoOp
79.44 41.18 54.24
Co-CoOp
77.01 56.00 64.85
ProGrad
77.35 52.35 62.45
RPO
76.70 62.13 68.61
LoGoPrompt 82.87 60.14 69.70
PromptSRC 83.37 62.97 71.75
TAP
84.20 68.00 75.24"
EXPERIMENTS,0.34125475285171103,(k) EuroSAT
EXPERIMENTS,0.34220532319391633,Base Novel HM
EXPERIMENTS,0.3431558935361217,"CLIP
56.48 64.05 60.03
CoOp
92.19 54.74 68.69
Co-CoOp
87.49 60.04 71.21
ProGrad
90.11 60.89 72.67
RPO
86.63 68.97 76.79
LoGoPrompt 93.67 69.44 79.75
PromptSRC 92.90 73.90 82.32
TAP
90.70 82.17 86.22"
EXPERIMENTS,0.344106463878327,(l) UCF101
EXPERIMENTS,0.3450570342205323,Base Novel HM
EXPERIMENTS,0.34600760456273766,"CLIP
70.53 77.50 73.85
CoOp
84.69 56.05 67.46
Co-CoOp
82.33 73.45 77.64
ProGrad
84.33 74.94 79.35
RPO
83.67 75.43 79.34
LoGoPrompt 86.19 73.07 79.09
PromptSRC 87.10 78.80 82.74
TAP
87.90 82.43 85.08"
EXPERIMENTS,0.34695817490494296,Table 2: Few shot classification results with 16 shots.
-SHOT CLASSIFICATION,0.34790874524714827,16-Shot Classification
-SHOT CLASSIFICATION,0.3488593155893536,Average
-SHOT CLASSIFICATION,0.34980988593155893,ImageNet
-SHOT CLASSIFICATION,0.35076045627376423,Caltech101 Pets Cars
-SHOT CLASSIFICATION,0.3517110266159696,Flowers
-SHOT CLASSIFICATION,0.3526615969581749,Food101
-SHOT CLASSIFICATION,0.35361216730038025,Aircraft
-SHOT CLASSIFICATION,0.35456273764258556,SUN397 DTD
-SHOT CLASSIFICATION,0.35551330798479086,EuroSAT
-SHOT CLASSIFICATION,0.3564638783269962,UCF101
-SHOT CLASSIFICATION,0.3574144486692015,"CLIP
78.79
67.31
95.43
85.34
80.44
97.37
82.90
45.36
73.28
69.96
87.21
82.11
CoOp
79.89
71.87
95.57
91.87
83.07
97.07
84.20
43.40
74.67
69.87
84.93
82.23
CoCoOp
74.90
70.83
95.16
93.34
71.57
87.84
87.25
31.21
72.15
63.04
73.32
78.14
MaPLe
81.79
72.33
96.00
92.83
83.57
97.00
85.33
48.40
75.53
71.33
92.33
85.03
PSRC
82.87
73.17
96.07
93.67
83.83
97.60
87.50
50.83
77.23
72.73
92.43
86.47"
-SHOT CLASSIFICATION,0.3583650190114068,"TAP
83.37
73.76
96.73
93.90
85.37
98.10
87.53
50.43
77.30
74.90
91.90
87.17"
BASE-TO-NOVEL GENERALIZATION,0.3593155893536122,"4.1
Base-to-Novel Generalization
244"
BASE-TO-NOVEL GENERALIZATION,0.3602661596958175,"In base-to-novel generalization, we equally split the classes into base and novel classes. Initial
245"
BASE-TO-NOVEL GENERALIZATION,0.3612167300380228,"training and evaluations are conducted on the seen base classes, followed by evaluation on the unseen
246"
BASE-TO-NOVEL GENERALIZATION,0.36216730038022815,"novel classes in a zero-shot manner. TAP surpasses prior state-of-the-art models in terms of the
247"
BASE-TO-NOVEL GENERALIZATION,0.36311787072243346,"base and novel class accuracy, as well as their harmonic mean across most of the 11 datasets, with
248"
BASE-TO-NOVEL GENERALIZATION,0.36406844106463876,"an average increase of 1.53% in the zero-shot novel class prediction, and a 1.07% increase in the
249"
BASE-TO-NOVEL GENERALIZATION,0.3650190114068441,"overall harmonic mean in average, as detailed inTable 1. Notably, our method improves unseen class
250"
BASE-TO-NOVEL GENERALIZATION,0.3659695817490494,"prediction without compromising base class performance, exhibiting an average performance boost
251"
BASE-TO-NOVEL GENERALIZATION,0.3669201520912547,"of 0.49%. In the challenging fine-grained tasks such as DTD, EuroSAT, and UCF101, TAP achieves
252"
BASE-TO-NOVEL GENERALIZATION,0.3678707224334601,"significant improvements in novel class prediction by 5.03%, 8.27%, and 3.63% respectively. These
253"
BASE-TO-NOVEL GENERALIZATION,0.3688212927756654,"results underscore the robust generalizability and efficacy of our method across diverse scenarios.
254"
FEW-SHOT CLASSIFICATION,0.36977186311787075,"4.2
Few-Shot Classification
255"
FEW-SHOT CLASSIFICATION,0.37072243346007605,"In few-shot classification, TAP also outperforms existing methods in 9 out of the 11 datasets. Detailed
256"
FEW-SHOT CLASSIFICATION,0.37167300380228135,"in Table 2, we achieve an average accuracy of 83.37 across the 11 datasets, surpassing the previous
257"
FEW-SHOT CLASSIFICATION,0.3726235741444867,"state-of-the-art methods by 0.5%, further demonstrating the effectiveness of our method.
258"
FEW-SHOT CLASSIFICATION,0.373574144486692,"Fur Pa'ern
Ear Pa'ern
Eye Pa'ern"
FEW-SHOT CLASSIFICATION,0.3745247148288973,"Wheel Design
Grille Style
Headlight Shape"
FEW-SHOT CLASSIFICATION,0.3754752851711027,"Color
Petal
Stem Characteris=cs
Image Image Image"
FEW-SHOT CLASSIFICATION,0.376425855513308,Figure 3: Visualization of the class activation maps.
FEW-SHOT CLASSIFICATION,0.3773764258555133,"Table 3: Effects of the Tree of At-
tributes."
FEW-SHOT CLASSIFICATION,0.37832699619771865,"Des. Org.
Unstructured
Ours"
FEW-SHOT CLASSIFICATION,0.37927756653992395,"Base
82.89
84.75
Novel
75.32
77.63
HM
78.93
81.04"
FEW-SHOT CLASSIFICATION,0.38022813688212925,Table 4: Effects of domain experts.
FEW-SHOT CLASSIFICATION,0.3811787072243346,"Align. Token
CLS
Ours"
FEW-SHOT CLASSIFICATION,0.3821292775665399,"Base
83.89
84.75
Novel
76.85
77.63
HM
80.22
81.04"
FEW-SHOT CLASSIFICATION,0.3830798479087453,Table 5: Effects of the number of experts.
FEW-SHOT CLASSIFICATION,0.3840304182509506,"Attrs. Num.
1
2
3
4
5
6
7
8
Ours"
FEW-SHOT CLASSIFICATION,0.3849809885931559,"Base Acc.
83.20
83.97
84.1
84.41
84.45
84.62
84.66
84.74
84.75
Novel Acc.
74.90
76.20
76.35
77.06
77.13
77.17
77.35
76.67
77.63
HM
78.83
79.90
80.04
80.57
80.63
80.72
80.84
80.50
81.04"
ABLATION STUDY,0.38593155893536124,"4.3
Ablation Study
259"
ABLATION STUDY,0.38688212927756654,"Effects of Tree of Attribute. A core inquiry is whether structuring descriptions into a Tree of
260"
ABLATION STUDY,0.38783269961977185,"Attribute (ToA) offers advantages over an unstructured aggregation of LLM-generated descriptions.
261"
ABLATION STUDY,0.3887832699619772,"To evaluate, we revert to aligning a mixed, unstructured set of descriptions with the CLS token
262"
ABLATION STUDY,0.3897338403041825,"- a common practice in prior studies [25, 19, 40, 52], while keeping the same number of visual
263"
ABLATION STUDY,0.3906844106463878,"prompt tokens. According to Table 3, substituting the ToA with an unstructured set results in
264"
ABLATION STUDY,0.3916349809885932,"significant performance decreases of 1.86%, 2.31%, and 2.11% across the average base, novel, and
265"
ABLATION STUDY,0.3925855513307985,"their harmonic mean performances, respectively. This stark contrast underscores the ToA‚Äôs critical
266"
ABLATION STUDY,0.3935361216730038,"role in enhancing model efficacy.
267"
ABLATION STUDY,0.39448669201520914,"Effects of Learning through Domain Experts. Further, we examine the impact of substituting the
268"
ABLATION STUDY,0.39543726235741444,"CLS token with visual expert tokens for learning fine-grained attributes, commonly adopted in in
269"
ABLATION STUDY,0.39638783269961975,"previous works [25, 19, 40, 52]. Our findings (Table 4) reveal improvements of 0.89%, 0.78%, and
270"
ABLATION STUDY,0.3973384030418251,"0.82% in the average base, novel, and harmonic mean accuracies, respectively, upon integrating visual
271"
ABLATION STUDY,0.3982889733840304,"expert tokens. These results support the notion that domain-specific, learnable tokens enhance the
272"
ABLATION STUDY,0.39923954372623577,"model‚Äôs ability to grasp fine-grained details by focusing on distinct aspects of the image, as opposed
273"
ABLATION STUDY,0.40019011406844107,"to the CLS token‚Äôs global focus.
274"
ABLATION STUDY,0.4011406844106464,"Effects of Number of Attributes. In our framework, the selection of attributes is dynamically
275"
ABLATION STUDY,0.40209125475285173,"determined by LLMs, leading to variability across different datasets. This adaptability stands in
276"
ABLATION STUDY,0.40304182509505704,"contrast to a static approach where the number of attributes is uniformly set across all datasets. To
277"
ABLATION STUDY,0.40399239543726234,"understand the impact of this variability, we explore how altering the number of attributes from 1 to 8
278"
ABLATION STUDY,0.4049429657794677,"influences model performance. Our findings, detailed in Table 5, reveal a performance improvement
279"
ABLATION STUDY,0.405893536121673,"trend as the number of attributes increases, with an optimal peak at 7 attributes before a slight decline
280"
ABLATION STUDY,0.4068441064638783,"at 8. However, crucially, across all fixed-attribute scenarios, none matched the performance achieved
281"
ABLATION STUDY,0.40779467680608367,"through our method‚Äôs dynamic attribute determination. These results underscore the importance of
282"
ABLATION STUDY,0.40874524714828897,"an adaptive approach to attribute selection, as opposed to a one-size-fits-all strategy.
283"
ABLATION STUDY,0.4096958174904943,"Design choice of the vision-conditional pooling layer. Lastly, we ablate the design of the pooling
284"
ABLATION STUDY,0.41064638783269963,"layer, starting from the naive training-free average pooling, to the attention-based pooling mechanism
285"
ABLATION STUDY,0.41159695817490494,"with condition on the input image. Compared to average pooling, VCP demonstrates a performance
286"
ABLATION STUDY,0.41254752851711024,"gain of 1.08% in the average harmonic mean. Furthermore, when compared with attention-based max
287"
ABLATION STUDY,0.4134980988593156,"pooling, which selects a single description per attribute according to the attention score in Eq. (4),
288"
ABLATION STUDY,0.4144486692015209,"‚Ä¢ served steamed in a bamboo 
basket
‚Ä¢ pan-fried to a crispy finish and 
served with a dipping sauce"
ABLATION STUDY,0.41539923954372626,Presentation
ABLATION STUDY,0.41634980988593157,"‚Ä¢ pale beige color 
‚Ä¢ golden-brown hue from 
pan-frying or deep-frying Color"
ABLATION STUDY,0.41730038022813687,"‚Ä¢ soft and chewy texture
‚Ä¢ crispy texture on the bottom 
from pan-frying"
ABLATION STUDY,0.41825095057034223,Texture 0.81 0.19 0.95 0.05 0.88 0.12 0.92 0.08 Shape
ABLATION STUDY,0.41920152091254753,"‚Ä¢ round with a pleated edge
‚Ä¢ crescent-shaped, with a 
fold in the dough"
ABLATION STUDY,0.42015209125475284,Figure 4: Visualization of the attention weights in the VCP layer for an example ‚Äúdumplings‚Äù image.
ABLATION STUDY,0.4211026615969582,Table 6: Design choice of the pooling layer.
ABLATION STUDY,0.4220532319391635,"Pooling Method
Base Acc.
Novel Acc.
HM"
ABLATION STUDY,0.4230038022813688,"Attn. Max Pooling
82.90
76.36
79.49
Average Pooling
83.18
76.98
79.96"
ABLATION STUDY,0.42395437262357416,"VCP (Ours)
84.75
77.63
81.04"
ABLATION STUDY,0.42490494296577946,"VCP maintains a superior advantage of 1.55% in average harmonic mean. These outcomes attest to
289"
ABLATION STUDY,0.42585551330798477,"the VCP layer‚Äôs integral role in finetuning attribute relevance to the visual context, substantiating its
290"
ABLATION STUDY,0.4268060836501901,"design and implementation within our model.
291"
VISUALIZATION,0.42775665399239543,"4.4
Visualization
292"
VISUALIZATION,0.42870722433460073,"Expert tokens focus on attribute-related regions. We further investigate the effects of vision
293"
VISUALIZATION,0.4296577946768061,"domain experts by visualizing their class activation maps from three illustrative examples using
294"
VISUALIZATION,0.4306083650190114,"GradCAM [37], as shown inFig. 3. These visualizations underscore the precision with which each
295"
VISUALIZATION,0.43155893536121676,"expert token concentrates on the image regions pertinent to its designated attribute. Take the first
296"
VISUALIZATION,0.43250950570342206,"cat image as an example. The ‚Äúfur pattern‚Äù expert distinctly highlights the animal‚Äôs fur texture,
297"
VISUALIZATION,0.43346007604562736,"whereas the ‚Äúear‚Äù and ‚Äúeye‚Äù experts focus precisely on the respective anatomical features. This
298"
VISUALIZATION,0.4344106463878327,"pattern of attribute-specific attention is consistent across the evaluated examples, reinforcing the
299"
VISUALIZATION,0.435361216730038,"conceptualization of expert tokens as dedicated ‚Äúdomain experts‚Äù within the visual field.
300"
VISUALIZATION,0.43631178707224333,"VCP layer pools the most applicable descriptions. The inherently interpretable nature of the VCP
301"
VISUALIZATION,0.4372623574144487,"layer, thanks to its attention mechanism, allows for insightful visualizations of its operational process.
302"
VISUALIZATION,0.438212927756654,"Through the examination of attention weights assigned by the VCP layer to different attributes
303"
VISUALIZATION,0.4391634980988593,"in a given image, we elucidate the layer‚Äôs capability to discern and prioritize the most applicable
304"
VISUALIZATION,0.44011406844106465,"descriptions. As illustrated in Fig. 4 with a ‚Äúdumplings‚Äù image, the VCP layer adeptly allocates
305"
VISUALIZATION,0.44106463878326996,"higher attention weights to descriptions accurately reflecting the observed instance (e.g., assigning
306"
VISUALIZATION,0.44201520912547526,"weights of 0.92 to ‚Äúround with a pleated edge‚Äù under the ‚ÄúShape‚Äù attribute and 0.95 to ‚Äúsoft and
307"
VISUALIZATION,0.4429657794676806,"chewy texture‚Äù under the Texture‚Äù). In contrast, less relevant descriptions for the specific image
308"
VISUALIZATION,0.4439163498098859,"context (e.g., ‚Äúcrescent-shaped‚Äù for Shape and ‚Äúcrispy texture from pan-frying‚Äù for Texture) receive
309"
VISUALIZATION,0.4448669201520912,"significantly lower weights. This discernment is crucial, given the class dumplings‚Äù encompasses a
310"
VISUALIZATION,0.4458174904942966,"broad variety of appearances based on cooking methods, yet not all descriptions are fitting for every
311"
VISUALIZATION,0.4467680608365019,"instance. These visualizations compellingly demonstrate the VCP layer‚Äôs effectiveness in refining
312"
VISUALIZATION,0.44771863117870725,"description relevance, thereby enhancing the model‚Äôs interpretative alignment with the visual content.
313"
CONCLUSION,0.44866920152091255,"5
Conclusion
314"
CONCLUSION,0.44961977186311786,"This paper introduces Tree of Attribute Prompt learning (TAP), a novel method that integrates
315"
CONCLUSION,0.4505703422053232,"detailed, LLM-generated descriptions within VLMs, achieving state-of-the-art performance in both
316"
CONCLUSION,0.4515209125475285,"base-to-novel generalization and few-shot image classification tasks across 11 diverse datasets. TAP
317"
CONCLUSION,0.4524714828897338,"leverages a hierarchical ""Tree of Attribute"" framework, distilling structured knowledge graphs from
318"
CONCLUSION,0.4534220532319392,"LLMs for nuanced representation of visual concepts, and employs learnable ""domain expert"" tokens
319"
CONCLUSION,0.4543726235741445,"and a vision-conditional pooling module for optimal image-text alignment. While promising, we
320"
CONCLUSION,0.4553231939163498,"note that the reliance on LLMs presents challenges in fine-grained datasets where similar classes
321"
CONCLUSION,0.45627376425855515,"require nuanced differentiation, in which cases LLMs generate identical descriptions for distinct
322"
CONCLUSION,0.45722433460076045,"classes, impacting novel class prediction performance. It highlights the current limitations of LLMs
323"
CONCLUSION,0.45817490494296575,"in discerning highly fine-grained distinctions. Addressing this challenge through enhanced LLM
324"
CONCLUSION,0.4591254752851711,"capabilities or alternative strategies will be a key focus of future research.
325"
REFERENCES,0.4600760456273764,"References
326"
REFERENCES,0.4610266159695818,"[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Visual prompting: Modifying
327"
REFERENCES,0.4619771863117871,"pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274, 3:11‚Äì12, 2022.
328"
REFERENCES,0.4629277566539924,"[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101‚Äìmining discriminative components
329"
REFERENCES,0.46387832699619774,"with random forests. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland,
330"
REFERENCES,0.46482889733840305,"September 6-12, 2014, Proceedings, Part VI 13, pages 446‚Äì461. Springer, 2014.
331"
REFERENCES,0.46577946768060835,"[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
332"
REFERENCES,0.4667300380228137,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
333"
REFERENCES,0.467680608365019,"Advances in neural information processing systems, 33:1877‚Äì1901, 2020.
334"
REFERENCES,0.4686311787072243,"[4] Adrian Bulat and Georgios Tzimiropoulos. Lasp: Text-to-text optimization for language-aware soft
335"
REFERENCES,0.4695817490494297,"prompting of vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision
336"
REFERENCES,0.470532319391635,"and Pattern Recognition, pages 23232‚Äì23241, 2023.
337"
REFERENCES,0.4714828897338403,"[5] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Prompt learning
338"
REFERENCES,0.47243346007604564,"with optimal transport for vision-language models. In ICLR, 2023.
339"
REFERENCES,0.47338403041825095,"[6] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing
340"
REFERENCES,0.47433460076045625,"textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition,
341"
REFERENCES,0.4752851711026616,"pages 3606‚Äì3613, 2014.
342"
REFERENCES,0.4762357414448669,"[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
343"
REFERENCES,0.47718631178707227,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255.
344"
REFERENCES,0.4781368821292776,"Ieee, 2009.
345"
REFERENCES,0.4790874524714829,"[8] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa, Cees GM
346"
REFERENCES,0.48003802281368824,"Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for image-language model
347"
REFERENCES,0.48098859315589354,"generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
348"
REFERENCES,0.48193916349809884,"15237‚Äì15246, 2023.
349"
REFERENCES,0.4828897338403042,"[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
350"
REFERENCES,0.4838403041825095,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
351"
REFERENCES,0.4847908745247148,"and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In
352"
REFERENCES,0.48574144486692017,"International Conference on Learning Representations, 2021. URL https://openreview.net/forum?
353"
REFERENCES,0.4866920152091255,"id=YicbFdNTTy.
354"
REFERENCES,0.4876425855513308,"[10] Zalan Fabian, Zhongqi Miao, Chunyuan Li, Yuanhan Zhang, Ziwei Liu, Andr√©s Hern√°ndez, Andr√©s
355"
REFERENCES,0.48859315589353614,"Montes-Rojas, Rafael Escucha, Laura Siabatto, Andr√©s Link, et al. Multimodal foundation models for
356"
REFERENCES,0.48954372623574144,"zero-shot animal species recognition in camera trap images. arXiv preprint arXiv:2311.01064, 2023.
357"
REFERENCES,0.49049429657794674,"[11] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:
358"
REFERENCES,0.4914448669201521,"An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision
359"
REFERENCES,0.4923954372623574,"and pattern recognition workshop, pages 178‚Äì178. IEEE, 2004.
360"
REFERENCES,0.49334600760456276,"[12] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
361"
REFERENCES,0.49429657794676807,"learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied
362"
REFERENCES,0.49524714828897337,"Earth Observations and Remote Sensing, 12(7):2217‚Äì2226, 2019.
363"
REFERENCES,0.49619771863117873,"[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,
364"
REFERENCES,0.49714828897338403,"Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
365"
REFERENCES,0.49809885931558934,"supervision. In International conference on machine learning, pages 4904‚Äì4916. PMLR, 2021.
366"
REFERENCES,0.4990494296577947,"[14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and
367"
REFERENCES,0.5,"Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709‚Äì727.
368"
REFERENCES,0.5009505703422054,"Springer, 2022.
369"
REFERENCES,0.5019011406844106,"[15] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan.
370"
REFERENCES,0.502851711026616,"Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision
371"
REFERENCES,0.5038022813688213,"and Pattern Recognition, pages 19113‚Äì19122, 2023.
372"
REFERENCES,0.5047528517110266,"[16] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and
373"
REFERENCES,0.5057034220532319,"Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In
374"
REFERENCES,0.5066539923954373,"Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15190‚Äì15200,
375"
REFERENCES,0.5076045627376425,"October 2023.
376"
REFERENCES,0.5085551330798479,"[17] Jae Myung Kim, A Koepke, Cordelia Schmid, and Zeynep Akata. Exposing and mitigating spurious
377"
REFERENCES,0.5095057034220533,"correlations for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision
378"
REFERENCES,0.5104562737642585,"and Pattern Recognition, pages 2584‚Äì2594, 2023.
379"
REFERENCES,0.5114068441064639,"[18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
380"
REFERENCES,0.5123574144486692,"categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages
381"
REFERENCES,0.5133079847908745,"554‚Äì561, 2013.
382"
REFERENCES,0.5142585551330798,"[19] Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo J Kim.
383"
REFERENCES,0.5152091254752852,"Read-only prompt optimization for vision-language few-shot learning. In Proceedings of the IEEE/CVF
384"
REFERENCES,0.5161596958174905,"International Conference on Computer Vision, pages 1401‚Äì1411, 2023.
385"
REFERENCES,0.5171102661596958,"[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning,
386"
REFERENCES,0.5180608365019012,"2021.
387"
REFERENCES,0.5190114068441065,"[21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
388"
REFERENCES,0.5199619771863118,"[22] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning
389"
REFERENCES,0.5209125475285171,"can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021. URL
390"
REFERENCES,0.5218631178707225,"https://arxiv.org/abs/2110.07602.
391"
REFERENCES,0.5228136882129277,"[23] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning.
392"
REFERENCES,0.5237642585551331,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5206‚Äì
393"
REFERENCES,0.5247148288973384,"5215, 2022.
394"
REFERENCES,0.5256653992395437,"[24] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
395"
REFERENCES,0.526615969581749,"classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
396"
REFERENCES,0.5275665399239544,"[25] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, and Carl
397"
REFERENCES,0.5285171102661597,"Vondrick. Doubly right object recognition: A why prompt for visual rationales. In Proceedings of the
398"
REFERENCES,0.529467680608365,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2722‚Äì2732, 2023.
399"
REFERENCES,0.5304182509505704,"[26] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. ICLR,
400"
REFERENCES,0.5313688212927756,"2023.
401"
REFERENCES,0.532319391634981,"[27] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of
402"
REFERENCES,0.5332699619771863,"classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722‚Äì729.
403"
REFERENCES,0.5342205323193916,"IEEE, 2008.
404"
REFERENCES,0.535171102661597,"[28] Zachary Novack, Julian McAuley, Zachary Lipton, and Saurabh Garg. Chils: Zero-shot image classification
405"
REFERENCES,0.5361216730038023,"with hierarchical label sets. In International Conference on Machine Learning (ICML), 2023.
406"
REFERENCES,0.5370722433460076,"[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
407"
REFERENCES,0.5380228136882129,"Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
408"
REFERENCES,0.5389733840304183,"human feedback. Advances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022.
409"
REFERENCES,0.5399239543726235,"[30] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE
410"
REFERENCES,0.5408745247148289,"conference on computer vision and pattern recognition, pages 3498‚Äì3505. IEEE, 2012.
411"
REFERENCES,0.5418250950570342,"[31] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
412"
REFERENCES,0.5427756653992395,"A. Lerer. Automatic differentiation in PyTorch. In NeurIPS Autodiff Workshop, 2017.
413"
REFERENCES,0.5437262357414449,"[32] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating
414"
REFERENCES,0.5446768060836502,"customized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International
415"
REFERENCES,0.5456273764258555,"Conference on Computer Vision, pages 15691‚Äì15701, 2023.
416"
REFERENCES,0.5465779467680608,"[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
417"
REFERENCES,0.5475285171102662,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
418"
REFERENCES,0.5484790874524715,"natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR,
419"
REFERENCES,0.5494296577946768,"2021.
420"
REFERENCES,0.5503802281368821,"[34] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz
421"
REFERENCES,0.5513307984790875,"Khan. Fine-tuned clip models are efficient video learners. In Proceedings of the IEEE/CVF Conference on
422"
REFERENCES,0.5522813688212928,"Computer Vision and Pattern Recognition, pages 6545‚Äì6554, 2023.
423"
REFERENCES,0.5532319391634981,"[35] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata.
424"
REFERENCES,0.5541825095057035,"Waffling around for performance: Visual classification with random words and broad concepts. In
425"
REFERENCES,0.5551330798479087,"Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15746‚Äì15757,
426"
REFERENCES,0.5560836501901141,"October 2023.
427"
REFERENCES,0.5570342205323194,"[36] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. In The
428"
REFERENCES,0.5579847908745247,"Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/
429"
REFERENCES,0.55893536121673,"forum?id=wsRXwlwx4w.
430"
REFERENCES,0.5598859315589354,"[37] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
431"
REFERENCES,0.5608365019011406,"Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
432"
REFERENCES,0.561787072243346,"Proceedings of the IEEE international conference on computer vision, pages 618‚Äì626, 2017.
433"
REFERENCES,0.5627376425855514,"[38] Cheng Shi and Sibei Yang. Logoprompt: Synthetic text images can be good visual prompts for vision-
434"
REFERENCES,0.5636882129277566,"language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
435"
REFERENCES,0.564638783269962,"2932‚Äì2941, 2023.
436"
REFERENCES,0.5655893536121673,"[39] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions
437"
REFERENCES,0.5665399239543726,"classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
438"
REFERENCES,0.5674904942965779,"[40] Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Argue: Attribute-guided prompt tuning for
439"
REFERENCES,0.5684410646387833,"vision-language models. arXiv preprint arXiv:2311.16494, 2023.
440"
REFERENCES,0.5693916349809885,"[41] Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, and Hanwang Zhang. Tuning
441"
REFERENCES,0.5703422053231939,"multi-mode token-level prompt alignment across modalities. In Thirty-seventh Conference on Neural
442"
REFERENCES,0.5712927756653993,"Information Processing Systems, 2023. URL https://openreview.net/forum?id=A253n2EXCd.
443"
REFERENCES,0.5722433460076045,"[42] Wenhao Wang, Yifan Sun, Wei Li, and Yi Yang.
TransHP: Image classification with hierarchical
444"
REFERENCES,0.5731939163498099,"prompting.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
URL
445"
REFERENCES,0.5741444866920152,"https://openreview.net/forum?id=vpQuCsZXz2.
446"
REFERENCES,0.5750950570342205,"[43] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong
447"
REFERENCES,0.5760456273764258,"Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual
448"
REFERENCES,0.5769961977186312,"learning. In European Conference on Computer Vision, pages 631‚Äì648. Springer, 2022.
449"
REFERENCES,0.5779467680608364,"[44] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent
450"
REFERENCES,0.5788973384030418,"Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the
451"
REFERENCES,0.5798479087452472,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139‚Äì149, 2022.
452"
REFERENCES,0.5807984790874525,"[45] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In
453"
REFERENCES,0.5817490494296578,"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4582‚Äì4591, 2017.
454"
REFERENCES,0.5826996197718631,"[46] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-
455"
REFERENCES,0.5836501901140685,"scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision
456"
REFERENCES,0.5846007604562737,"and pattern recognition, pages 3485‚Äì3492. IEEE, 2010.
457"
REFERENCES,0.5855513307984791,"[47] Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, Peng Wang, and Yanning Zhang.
458"
REFERENCES,0.5865019011406845,"Dual modality prompt tuning for vision-language pre-trained model. IEEE Transactions on Multimedia,
459"
REFERENCES,0.5874524714828897,"pages 1‚Äì13, 2023. doi: 10.1109/TMM.2023.3291588.
460"
REFERENCES,0.5884030418250951,"[48] An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang,
461"
REFERENCES,0.5893536121673004,"and Julian McAuley. Learning concise and descriptive attributes for visual recognition. In Proceedings of
462"
REFERENCES,0.5903041825095057,"the IEEE/CVF International Conference on Computer Vision, pages 3090‚Äì3100, 2023.
463"
REFERENCES,0.591254752851711,"[49] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.
464"
REFERENCES,0.5922053231939164,"Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.
465"
REFERENCES,0.5931558935361216,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19187‚Äì
466"
REFERENCES,0.594106463878327,"19197, 2023.
467"
REFERENCES,0.5950570342205324,"[50] Yi Zhang, Ce Zhang, Ke Yu, Yushun Tang, and Zhihai He. Concept-guided prompt learning for gener-
468"
REFERENCES,0.5960076045627376,"alization in vision-language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38
469"
REFERENCES,0.596958174904943,"(7):7377‚Äì7386, Mar. 2024. doi: 10.1609/aaai.v38i7.28568. URL https://ojs.aaai.org/index.php/
470"
REFERENCES,0.5979087452471483,"AAAI/article/view/28568.
471"
REFERENCES,0.5988593155893536,"[51] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673,
472"
REFERENCES,0.5998098859315589,"2022.
473"
REFERENCES,0.6007604562737643,"[52] Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, and Ram Nevatia. Large language models are
474"
REFERENCES,0.6017110266159695,"good prompt learners for low-shot image classification. arXiv preprint arXiv:2312.04076, 2023.
475"
REFERENCES,0.6026615969581749,"[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for
476"
REFERENCES,0.6036121673003803,"vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
477"
REFERENCES,0.6045627376425855,"Recognition, pages 16816‚Äì16825, 2022.
478"
REFERENCES,0.6055133079847909,"[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language
479"
REFERENCES,0.6064638783269962,"models. International Journal of Computer Vision, 130(9):2337‚Äì2348, 2022.
480"
REFERENCES,0.6074144486692015,"[55] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt
481"
REFERENCES,0.6083650190114068,"tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15659‚Äì
482"
REFERENCES,0.6093155893536122,"15669, 2023.
483"
REFERENCES,0.6102661596958175,"A
Appendix
484"
REFERENCES,0.6112167300380228,"A.1
Model regularization
485"
REFERENCES,0.6121673003802282,"Denote the frozen image feature from CLIP vision encoder as f v, the frozen text feature for description d from
486"
REFERENCES,0.6131178707224335,"CLIP text encoder as f t
d, and the zero-shot logit prediction from CLIP as ÀÜy. Additionally, denote the trained
487"
REFERENCES,0.6140684410646388,"image feature as Àúf v, the trained text feature for description d as Àúf t
d, and the logit prediction from attribute a after
488"
REFERENCES,0.6150190114068441,"training as Àúya. The losses are as follows:
489"
REFERENCES,0.6159695817490495,"LL1‚àíV = ||f v ‚àíÀúf v||1
(8)"
REFERENCES,0.6169201520912547,"Lcon‚àíT = ‚àí
X d‚ààD 1"
LOG,0.6178707224334601,"2 log
exp(cos(f t
d,Àúf t
d))
P"
LOG,0.6188212927756654,"k‚ààDs exp(cos(f t
d,Àúf t
k))
+ 1"
LOG,0.6197718631178707,"2 log
exp(cos(f t
d,Àúf t
d))
P"
LOG,0.620722433460076,"k‚ààDs exp(cos(f t
k,Àúf t
d)) 
(9) 490"
LOG,0.6216730038022814,"LKL‚àíattr =
1
|A|  X"
LOG,0.6226235741444867,"a‚ààA
DKL(ÀÜy, Àúya)

(10)"
LOG,0.623574144486692,"The regularization loss is then:
491"
LOG,0.6245247148288974,"Lreg = ¬µ1LL1‚àíV + ¬µ2LKL‚àíattr + ¬µ3Lcon‚àíT ,
(11)"
LOG,0.6254752851711026,"Our overall training objective is thus given by:
492"
LOG,0.626425855513308,"Ltotal = Lclass + Lreg
(12)"
LOG,0.6273764258555133,"A.2
Additional implementation details
493"
LOG,0.6283269961977186,"We use PyTorch [31] to implement all experiments on a single NVIDIA A100-80GB GPU. Our code is developed
494"
LOG,0.629277566539924,"based on the implementation of CoOp [54], which is available at https://github.com/KaiyangZhou/CoOp and
495"
LOG,0.6302281368821293,"released under the MIT license. Our code is also released under the MIT license. Baseline results for both
496"
LOG,0.6311787072243346,"base-to-novel generalization and few-shot classification are taken from their respective publications. For the
497"
LOG,0.6321292775665399,"‚Äúglobal context‚Äù attribute which is aligned with the CLS token in the vision encoder, we use the following 7
498"
LOG,0.6330798479087453,"selected templates provided in [33].
499"
LOG,0.6340304182509505,"""itap of a {class}.""
500"
LOG,0.6349809885931559,"""a bad photo of the {class}.""
501"
LOG,0.6359315589353612,"""a origami {class}.""
502"
LOG,0.6368821292775665,"""a photo of the large {class}.""
503"
LOG,0.6378326996197718,"""a {class} in a video game.""
504"
LOG,0.6387832699619772,"""art of the {class}.""
505"
LOG,0.6397338403041825,"""a photo of the small {class}.""
506"
LOG,0.6406844106463878,"A.3
Prompts for Tree-of-Attribute generation
507"
LOG,0.6416349809885932,"As introduced in Section 3.3, we generate the Tree-of-Attribute with the following three steps: 1) Attribute
508"
LOG,0.6425855513307985,"Generation, 2) In-Context Example Generation, and 3) Description Generation for All Classes. The prompts for
509"
LOG,0.6435361216730038,"each step are as follows:
510"
LOG,0.6444866920152091,"1) Attribute Generation:
511"
LOG,0.6454372623574145,"{Dataset Description.}
512"
LOG,0.6463878326996197,"Visual attributes refer to observable, describable features of the images that can include color, shape, size,
513"
LOG,0.6473384030418251,"texture, and any specific patterns or markings, which can help differentiate between classes for the dataset. They
514"
LOG,0.6482889733840305,"should be consistently observable across multiple images of the same class. Your task is to generate a list of
515"
LOG,0.6492395437262357,"visual attributes (less than 10) for the {Dataset Name} dataset. Ensure this list is clear, concise, and specific to
516"
LOG,0.6501901140684411,"the dataset‚Äôs needs. Avoid generic attributes that do not contribute to distinguishing between classes.
517"
LOG,0.6511406844106464,"2) In-Context Example Generation
518"
LOG,0.6520912547528517,"Describe describe what a ""{Random Class Name}"" class in the {Dataset Name} dataset look like using the
519"
LOG,0.653041825095057,"generated visual attributes.
520"
LOG,0.6539923954372624,"You must follow the following rules:
521"
LOG,0.6549429657794676,"1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a
522"
LOG,0.655893536121673,"detailed and clear presentation of each attribute‚Äôs range.
523"
LOG,0.6568441064638784,"2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to
524"
LOG,0.6577946768060836,"provide at least two descriptions to ensure a comprehensive overview of the attribute.
525"
LOG,0.658745247148289,"3. The descriptions should provide clear, distinguishable features of each class to support image classification
526"
LOG,0.6596958174904943,"tasks.
527"
LOG,0.6606463878326996,"4. Descriptions for each attribute are independent from each other, and they should not serve as context for each
528"
LOG,0.6615969581749049,"other.
529"
LOG,0.6625475285171103,"5. Each description describes an image independetly. If certain description is possible for a class, please just
530"
LOG,0.6634980988593155,"list that description, and do not use words like ""may have"" or ""sometimes have"".
531"
LOG,0.6644486692015209,"6. Reply descriptions only. Do not include any explanation before and after the description.
532"
LOG,0.6653992395437263,"7. The descriptions should follow the format of ""classname, which ..."", where ""..."" is the description of the visual
533"
LOG,0.6663498098859315,"attribute.
534"
LOG,0.6673003802281369,"3) Description Generation for All Classes
535"
LOG,0.6682509505703422,"{Dataset Description.}
536"
LOG,0.6692015209125475,"Your task is to write detailed descriptions for various classes within the {Dataset Name} dataset, using the
537"
LOG,0.6701520912547528,"provided visual attributes such as color and shape. These descriptions will help in accurately classifying and
538"
LOG,0.6711026615969582,"understanding the unique features of each class.
539"
LOG,0.6720532319391636,"You must follow the following rules:
540"
LOG,0.6730038022813688,"1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a
541"
LOG,0.6739543726235742,"detailed and clear presentation of each attribute‚Äôs range.
542"
LOG,0.6749049429657795,"2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to
543"
LOG,0.6758555133079848,"provide at least two descriptions to ensure a comprehensive overview of the attribute.
544"
LOG,0.6768060836501901,"3. The descriptions should provide clear, distinguishable features of each class to support image classification
545"
LOG,0.6777566539923955,"tasks.
546"
LOG,0.6787072243346007,"4. Descriptions for each attribute are independent from each other, and they should not serve as context for each
547"
LOG,0.6796577946768061,"other.
548"
LOG,0.6806083650190115,"5. Each description describes an image independetly. If certain description is possible for a class, please just
549"
LOG,0.6815589353612167,"list that description, and do not use words like ""may have"" or ""sometimes have"".
550"
LOG,0.6825095057034221,"6. Reply descriptions only. Do not include any explanation before and after the description.
551"
LOG,0.6834600760456274,"7. The descriptions should follow the format of ""classname, which ..."", where ""..."" is the description of the visual
552"
LOG,0.6844106463878327,"attribute.
553"
LOG,0.685361216730038,"Q: Describe what a ""{Random Class Name}"" in the {Dataset Name} look like using the following visual attributes:
554"
LOG,0.6863117870722434,"{Visual Attributes from Step 1.}
555"
LOG,0.6872623574144486,"A: {Answer from Step 2.}
556"
LOG,0.688212927756654,"Q: Describe what a ""{Target Class Name}"" in the {Dataset Name} look like using the following visual attributes:
557"
LOG,0.6891634980988594,"{Visual Attributes from Step 1.}
558"
LOG,0.6901140684410646,"A:
559"
LOG,0.69106463878327,"In the prompt templates, ""Dataset Description"" is the description of the dataset from their official website,
560"
LOG,0.6920152091254753,"""Random Class Name"" is a randomly sampled class name in the dataset for in-context example generation, and
561"
LOG,0.6929657794676806,"""Target Class Name"" is the class name of interest for the current query. While step 1 and 2 are made in two
562"
LOG,0.6939163498098859,"consecutive calls to provide contexts which are queried once per dataset, step 3 is queried independently for
563"
LOG,0.6948669201520913,"each of the remaining classes in the dataset. Human review is performed after step 2 to ensure a high-quality set
564"
LOG,0.6958174904942965,"of attributes and in-context example.
565"
LOG,0.6967680608365019,"A.4
Potential societal impacts
566"
LOG,0.6977186311787072,"While our work primarily focuses on advancing prompt learning in vision-language models, it‚Äôs crucial to
567"
LOG,0.6986692015209125,"acknowledge the potential broader societal implications of such advancements. On the positive side, TAP could
568"
LOG,0.6996197718631179,"lead to more efficient and accurate image understanding systems, benefiting various domains. For instance, it
569"
LOG,0.7005703422053232,"could enhance accessibility for visually impaired individuals by providing more detailed descriptions of visual
570"
LOG,0.7015209125475285,"content. Furthermore, improved visual understanding could contribute to more effective content moderation,
571"
LOG,0.7024714828897338,"mitigating the spread of harmful online materials. However, these advancements also present potential risks.
572"
LOG,0.7034220532319392,"LLMs used for description generation can perpetuate existing societal biases present in their training data, leading
573"
LOG,0.7043726235741445,"to biased outcomes in image recognition. Moreover, sophisticated VLMs could be misused to create misleading
574"
LOG,0.7053231939163498,"visual content, contributing to misinformation and manipulation. The enhanced ability to analyze and understand
575"
LOG,0.7062737642585551,"images also raises privacy concerns, particularly in surveillance contexts where personal information could be
576"
LOG,0.7072243346007605,"extracted from visual data. Addressing these potential negative impacts necessitates careful consideration of bias
577"
LOG,0.7081749049429658,"mitigation techniques during LLM training, promoting transparency and explainability in VLM decision-making,
578"
LOG,0.7091254752851711,"and establishing ethical guidelines for responsible development and deployment of such technologies.
579"
LOG,0.7100760456273765,"NeurIPS Paper Checklist
580"
CLAIMS,0.7110266159695817,"1. Claims
581"
CLAIMS,0.7119771863117871,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs
582"
CLAIMS,0.7129277566539924,"contributions and scope?
583"
CLAIMS,0.7138783269961977,"Answer: [Yes]
584"
CLAIMS,0.714828897338403,"Justification: The abstract and introduction clearly state the problem of limited context in existing
585"
CLAIMS,0.7157794676806084,"prompt learning methods, propose TAP as a solution using structured knowledge graphs and domain
586"
CLAIMS,0.7167300380228137,"experts, and highlight the strong experimental results in both base-to-novel generalization and few-shot
587"
CLAIMS,0.717680608365019,"classification. This accurately reflects the paper‚Äôs contributions and scope.
588"
CLAIMS,0.7186311787072244,"Guidelines:
589"
CLAIMS,0.7195817490494296,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims made in the
590"
CLAIMS,0.720532319391635,"paper.
591"
CLAIMS,0.7214828897338403,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the contributions
592"
CLAIMS,0.7224334600760456,"made in the paper and important assumptions and limitations. A No or NA answer to this
593"
CLAIMS,0.723384030418251,"question will not be perceived well by the reviewers.
594"
CLAIMS,0.7243346007604563,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how much the
595"
CLAIMS,0.7252851711026616,"results can be expected to generalize to other settings.
596"
CLAIMS,0.7262357414448669,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
597"
CLAIMS,0.7271863117870723,"attained by the paper.
598"
LIMITATIONS,0.7281368821292775,"2. Limitations
599"
LIMITATIONS,0.7290874524714829,"Question: Does the paper discuss the limitations of the work performed by the authors?
600"
LIMITATIONS,0.7300380228136882,"Answer: [Yes]
601"
LIMITATIONS,0.7309885931558935,"Justification: The paper includes a discussion of the limitations associated with relying on LLMs for
602"
LIMITATIONS,0.7319391634980988,"generating descriptions, particularly in fine-grained datasets where similar classes require nuanced
603"
LIMITATIONS,0.7328897338403042,"differentiation. This discussion can be found in ""Conclusion"".
604"
LIMITATIONS,0.7338403041825095,"Guidelines:
605"
LIMITATIONS,0.7347908745247148,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that the paper
606"
LIMITATIONS,0.7357414448669202,"has limitations, but those are not discussed in the paper.
607"
LIMITATIONS,0.7366920152091255,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
608"
LIMITATIONS,0.7376425855513308,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to violations of
609"
LIMITATIONS,0.7385931558935361,"these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
610"
LIMITATIONS,0.7395437262357415,"asymptotic approximations only holding locally). The authors should reflect on how these
611"
LIMITATIONS,0.7404942965779467,"assumptions might be violated in practice and what the implications would be.
612"
LIMITATIONS,0.7414448669201521,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
613"
LIMITATIONS,0.7423954372623575,"on a few datasets or with a few runs. In general, empirical results often depend on implicit
614"
LIMITATIONS,0.7433460076045627,"assumptions, which should be articulated.
615"
LIMITATIONS,0.7442965779467681,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach. For
616"
LIMITATIONS,0.7452471482889734,"example, a facial recognition algorithm may perform poorly when image resolution is low or
617"
LIMITATIONS,0.7461977186311787,"images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
618"
LIMITATIONS,0.747148288973384,"closed captions for online lectures because it fails to handle technical jargon.
619"
LIMITATIONS,0.7480988593155894,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms and how
620"
LIMITATIONS,0.7490494296577946,"they scale with dataset size.
621"
LIMITATIONS,0.75,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to address problems
622"
LIMITATIONS,0.7509505703422054,"of privacy and fairness.
623"
LIMITATIONS,0.7519011406844106,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by reviewers
624"
LIMITATIONS,0.752851711026616,"as grounds for rejection, a worse outcome might be that reviewers discover limitations that
625"
LIMITATIONS,0.7538022813688213,"aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize
626"
LIMITATIONS,0.7547528517110266,"that individual actions in favor of transparency play an important role in developing norms that
627"
LIMITATIONS,0.7557034220532319,"preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
628"
LIMITATIONS,0.7566539923954373,"honesty concerning limitations.
629"
THEORY ASSUMPTIONS AND PROOFS,0.7576045627376425,"3. Theory Assumptions and Proofs
630"
THEORY ASSUMPTIONS AND PROOFS,0.7585551330798479,"Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
631"
THEORY ASSUMPTIONS AND PROOFS,0.7595057034220533,"(and correct) proof?
632"
THEORY ASSUMPTIONS AND PROOFS,0.7604562737642585,"Answer: [NA]
633"
THEORY ASSUMPTIONS AND PROOFS,0.7614068441064639,"Justification: The paper focuses on proposing a novel method for prompt learning in VLMs and
634"
THEORY ASSUMPTIONS AND PROOFS,0.7623574144486692,"evaluating its empirical performance. It doesn‚Äôt introduce any new theoretical results or theorems
635"
THEORY ASSUMPTIONS AND PROOFS,0.7633079847908745,"requiring formal proofs.
636"
THEORY ASSUMPTIONS AND PROOFS,0.7642585551330798,"Guidelines:
637"
THEORY ASSUMPTIONS AND PROOFS,0.7652091254752852,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
638"
THEORY ASSUMPTIONS AND PROOFS,0.7661596958174905,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7671102661596958,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
640"
THEORY ASSUMPTIONS AND PROOFS,0.7680608365019012,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if they appear in
641"
THEORY ASSUMPTIONS AND PROOFS,0.7690114068441065,"the supplemental material, the authors are encouraged to provide a short proof sketch to provide
642"
THEORY ASSUMPTIONS AND PROOFS,0.7699619771863118,"intuition.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7709125475285171,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented by
644"
THEORY ASSUMPTIONS AND PROOFS,0.7718631178707225,"formal proofs provided in appendix or supplemental material.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7728136882129277,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7737642585551331,"4. Experimental Result Reproducibility
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747148288973384,"Question: Does the paper fully disclose all the information needed to reproduce the main experimental
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756653992395437,"results of the paper to the extent that it affects the main claims and/or conclusions of the paper
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.776615969581749,"(regardless of whether the code and data are provided or not)?
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775665399239544,"Answer: [Yes]
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785171102661597,"Justification: The paper provides all the necessary information for reproducing the experimental
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.779467680608365,"results.
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7804182509505704,"Guidelines:
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7813688212927756,"‚Ä¢ The answer NA means that the paper does not include experiments.
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782319391634981,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived well by the
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832699619771863,"reviewers: Making the paper reproducible is important, regardless of whether the code and data
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842205323193916,"are provided or not.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.785171102661597,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken to make
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861216730038023,"their results reproducible or verifiable.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870722433460076,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways. For
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7880228136882129,"example, if the contribution is a novel architecture, describing the architecture fully might suffice,
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7889733840304183,"or if the contribution is a specific model and empirical evaluation, it may be necessary to either
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7899239543726235,"make it possible for others to replicate the model with the same dataset, or provide access to
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7908745247148289,"the model. In general. releasing code and data is often one good way to accomplish this, but
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7918250950570342,"reproducibility can also be provided via detailed instructions for how to replicate the results,
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7927756653992395,"access to a hosted model (e.g., in the case of a large language model), releasing of a model
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7937262357414449,"checkpoint, or other means that are appropriate to the research performed.
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7946768060836502,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submissions
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7956273764258555,"to provide some reasonable avenue for reproducibility, which may depend on the nature of the
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7965779467680608,"contribution. For example
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7975285171102662,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how to
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7984790874524715,"reproduce that algorithm.
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7994296577946768,"(b) If the contribution is primarily a new model architecture, the paper should describe the
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8003802281368821,"architecture clearly and fully.
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8013307984790875,"(c) If the contribution is a new model (e.g., a large language model), then there should either be
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8022813688212928,"a way to access this model for reproducing the results or a way to reproduce the model (e.g.,
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032319391634981,"with an open-source dataset or instructions for how to construct the dataset).
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8041825095057035,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors are
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8051330798479087,"welcome to describe the particular way they provide for reproducibility. In the case of
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8060836501901141,"closed-source models, it may be that access to the model is limited in some way (e.g.,
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070342205323194,"to registered users), but it should be possible for other researchers to have some path to
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8079847908745247,"reproducing or verifying the results.
683"
OPEN ACCESS TO DATA AND CODE,0.80893536121673,"5. Open access to data and code
684"
OPEN ACCESS TO DATA AND CODE,0.8098859315589354,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
685"
OPEN ACCESS TO DATA AND CODE,0.8108365019011406,"faithfully reproduce the main experimental results, as described in supplemental material?
686"
OPEN ACCESS TO DATA AND CODE,0.811787072243346,"Answer: [No]
687"
OPEN ACCESS TO DATA AND CODE,0.8127376425855514,"Justification: Our codebase is built based on the CoOP and CoCoOP [54, 53], and can be reproduced
688"
OPEN ACCESS TO DATA AND CODE,0.8136882129277566,"based on our Methods, Implementation details in main text and appendix. Our code will be released
689"
OPEN ACCESS TO DATA AND CODE,0.814638783269962,"upon acceptance.
690"
OPEN ACCESS TO DATA AND CODE,0.8155893536121673,"Guidelines:
691"
OPEN ACCESS TO DATA AND CODE,0.8165399239543726,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
692"
OPEN ACCESS TO DATA AND CODE,0.8174904942965779,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
693"
OPEN ACCESS TO DATA AND CODE,0.8184410646387833,"guides/CodeSubmissionPolicy) for more details.
694"
OPEN ACCESS TO DATA AND CODE,0.8193916349809885,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be possible,
695"
OPEN ACCESS TO DATA AND CODE,0.8203422053231939,"so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless
696"
OPEN ACCESS TO DATA AND CODE,0.8212927756653993,"this is central to the contribution (e.g., for a new open-source benchmark).
697"
OPEN ACCESS TO DATA AND CODE,0.8222433460076045,"‚Ä¢ The instructions should contain the exact command and environment needed to run to reproduce
698"
OPEN ACCESS TO DATA AND CODE,0.8231939163498099,"the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
699"
OPEN ACCESS TO DATA AND CODE,0.8241444866920152,"guides/CodeSubmissionPolicy) for more details.
700"
OPEN ACCESS TO DATA AND CODE,0.8250950570342205,"‚Ä¢ The authors should provide instructions on data access and preparation, including how to access
701"
OPEN ACCESS TO DATA AND CODE,0.8260456273764258,"the raw data, preprocessed data, intermediate data, and generated data, etc.
702"
OPEN ACCESS TO DATA AND CODE,0.8269961977186312,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new proposed
703"
OPEN ACCESS TO DATA AND CODE,0.8279467680608364,"method and baselines. If only a subset of experiments are reproducible, they should state which
704"
OPEN ACCESS TO DATA AND CODE,0.8288973384030418,"ones are omitted from the script and why.
705"
OPEN ACCESS TO DATA AND CODE,0.8298479087452472,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized versions (if
706"
OPEN ACCESS TO DATA AND CODE,0.8307984790874525,"applicable).
707"
OPEN ACCESS TO DATA AND CODE,0.8317490494296578,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the paper) is
708"
OPEN ACCESS TO DATA AND CODE,0.8326996197718631,"recommended, but including URLs to data and code is permitted.
709"
OPEN ACCESS TO DATA AND CODE,0.8336501901140685,"6. Experimental Setting/Details
710"
OPEN ACCESS TO DATA AND CODE,0.8346007604562737,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
711"
OPEN ACCESS TO DATA AND CODE,0.8355513307984791,"how they were chosen, type of optimizer, etc.) necessary to understand the results?
712"
OPEN ACCESS TO DATA AND CODE,0.8365019011406845,"Answer: [Yes]
713"
OPEN ACCESS TO DATA AND CODE,0.8374524714828897,"Justification: The training and test details can be found in section Experiments and Appendix.
714"
OPEN ACCESS TO DATA AND CODE,0.8384030418250951,"Guidelines:
715"
OPEN ACCESS TO DATA AND CODE,0.8393536121673004,"‚Ä¢ The answer NA means that the paper does not include experiments.
716"
OPEN ACCESS TO DATA AND CODE,0.8403041825095057,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail that is
717"
OPEN ACCESS TO DATA AND CODE,0.841254752851711,"necessary to appreciate the results and make sense of them.
718"
OPEN ACCESS TO DATA AND CODE,0.8422053231939164,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental material.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8431558935361216,"7. Experiment Statistical Significance
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.844106463878327,"Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8450570342205324,"tion about the statistical significance of the experiments?
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8460076045627376,"Answer: [No]
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.846958174904943,"Justification: We follow previous works [54, 53] to report results averaged over 3 runs. Error bars are
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8479087452471483,"not reported.
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8488593155893536,"Guidelines:
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498098859315589,"‚Ä¢ The answer NA means that the paper does not include experiments.
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8507604562737643,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8517110266159695,"intervals, or statistical significance tests, at least for the experiments that support the main claims
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8526615969581749,"of the paper.
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8536121673003803,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for example,
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8545627376425855,"train/test split, initialization, random drawing of some parameter, or overall run with given
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8555133079847909,"experimental conditions).
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8564638783269962,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula, call to a
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8574144486692015,"library function, bootstrap, etc.)
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8583650190114068,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8593155893536122,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error of the
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8602661596958175,"mean.
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8612167300380228,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621673003802282,"a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8631178707224335,"not verified.
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8640684410646388,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or figures
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8650190114068441,"symmetric error bars that would yield results that are out of range (e.g. negative error rates).
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8659695817490495,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how they were
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8669201520912547,"calculated and reference the corresponding figures or tables in the text.
745"
EXPERIMENTS COMPUTE RESOURCES,0.8678707224334601,"8. Experiments Compute Resources
746"
EXPERIMENTS COMPUTE RESOURCES,0.8688212927756654,"Question: For each experiment, does the paper provide sufficient information on the computer
747"
EXPERIMENTS COMPUTE RESOURCES,0.8697718631178707,"resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
748"
EXPERIMENTS COMPUTE RESOURCES,0.870722433460076,"Answer: [Yes]
749"
EXPERIMENTS COMPUTE RESOURCES,0.8716730038022814,"Justification: The type of compute used is provided in Appendix.
750"
EXPERIMENTS COMPUTE RESOURCES,0.8726235741444867,"Guidelines:
751"
EXPERIMENTS COMPUTE RESOURCES,0.873574144486692,"‚Ä¢ The answer NA means that the paper does not include experiments.
752"
EXPERIMENTS COMPUTE RESOURCES,0.8745247148288974,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
753"
EXPERIMENTS COMPUTE RESOURCES,0.8754752851711026,"provider, including relevant memory and storage.
754"
EXPERIMENTS COMPUTE RESOURCES,0.876425855513308,"‚Ä¢ The paper should provide the amount of compute required for each of the individual experimental
755"
EXPERIMENTS COMPUTE RESOURCES,0.8773764258555133,"runs as well as estimate the total compute.
756"
EXPERIMENTS COMPUTE RESOURCES,0.8783269961977186,"‚Ä¢ The paper should disclose whether the full research project required more compute than the
757"
EXPERIMENTS COMPUTE RESOURCES,0.879277566539924,"experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into
758"
EXPERIMENTS COMPUTE RESOURCES,0.8802281368821293,"the paper).
759"
CODE OF ETHICS,0.8811787072243346,"9. Code Of Ethics
760"
CODE OF ETHICS,0.8821292775665399,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
761"
CODE OF ETHICS,0.8830798479087453,"of Ethics https://neurips.cc/public/EthicsGuidelines?
762"
CODE OF ETHICS,0.8840304182509505,"Answer: [Yes]
763"
CODE OF ETHICS,0.8849809885931559,"Justification: We conform with the NeurIPS Code of Ethics in every aspect.
764"
CODE OF ETHICS,0.8859315589353612,"Guidelines:
765"
CODE OF ETHICS,0.8868821292775665,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
766"
CODE OF ETHICS,0.8878326996197718,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a deviation
767"
CODE OF ETHICS,0.8887832699619772,"from the Code of Ethics.
768"
CODE OF ETHICS,0.8897338403041825,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
769"
CODE OF ETHICS,0.8906844106463878,"to laws or regulations in their jurisdiction).
770"
BROADER IMPACTS,0.8916349809885932,"10. Broader Impacts
771"
BROADER IMPACTS,0.8925855513307985,"Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
772"
BROADER IMPACTS,0.8935361216730038,"of the work performed?
773"
BROADER IMPACTS,0.8944866920152091,"Answer: [Yes]
774"
BROADER IMPACTS,0.8954372623574145,"Justification: The potential societal impacts are discussed in Appendix.
775"
BROADER IMPACTS,0.8963878326996197,"Guidelines:
776"
BROADER IMPACTS,0.8973384030418251,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
777"
BROADER IMPACTS,0.8982889733840305,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal impact or
778"
BROADER IMPACTS,0.8992395437262357,"why the paper does not address societal impact.
779"
BROADER IMPACTS,0.9001901140684411,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
780"
BROADER IMPACTS,0.9011406844106464,"disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
781"
BROADER IMPACTS,0.9020912547528517,"ment of technologies that could make decisions that unfairly impact specific groups), privacy
782"
BROADER IMPACTS,0.903041825095057,"considerations, and security considerations.
783"
BROADER IMPACTS,0.9039923954372624,"‚Ä¢ The conference expects that many papers will be foundational research and not tied to particular
784"
BROADER IMPACTS,0.9049429657794676,"applications, let alone deployments. However, if there is a direct path to any negative applications,
785"
BROADER IMPACTS,0.905893536121673,"the authors should point it out. For example, it is legitimate to point out that an improvement in
786"
BROADER IMPACTS,0.9068441064638784,"the quality of generative models could be used to generate deepfakes for disinformation. On the
787"
BROADER IMPACTS,0.9077946768060836,"other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
788"
BROADER IMPACTS,0.908745247148289,"could enable people to train models that generate Deepfakes faster.
789"
BROADER IMPACTS,0.9096958174904943,"‚Ä¢ The authors should consider possible harms that could arise when the technology is being used
790"
BROADER IMPACTS,0.9106463878326996,"as intended and functioning correctly, harms that could arise when the technology is being used
791"
BROADER IMPACTS,0.9115969581749049,"as intended but gives incorrect results, and harms following from (intentional or unintentional)
792"
BROADER IMPACTS,0.9125475285171103,"misuse of the technology.
793"
BROADER IMPACTS,0.9134980988593155,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation strategies
794"
BROADER IMPACTS,0.9144486692015209,"(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-
795"
BROADER IMPACTS,0.9153992395437263,"ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the
796"
BROADER IMPACTS,0.9163498098859315,"efficiency and accessibility of ML).
797"
SAFEGUARDS,0.9173003802281369,"11. Safeguards
798"
SAFEGUARDS,0.9182509505703422,"Question: Does the paper describe safeguards that have been put in place for responsible release of
799"
SAFEGUARDS,0.9192015209125475,"data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
800"
SAFEGUARDS,0.9201520912547528,"scraped datasets)?
801"
SAFEGUARDS,0.9211026615969582,"Answer: [NA]
802"
SAFEGUARDS,0.9220532319391636,"Justification: This paper primarily focuses on a novel prompt learning method and doesn‚Äôt involve
803"
SAFEGUARDS,0.9230038022813688,"the release of a new pre-trained LLM, image generator, or scraped dataset. Therefore, this question
804"
SAFEGUARDS,0.9239543726235742,"doesn‚Äôt directly apply in this context. We leverage an existing pre-trained LLM (GPT-3.5-turbo), and
805"
SAFEGUARDS,0.9249049429657795,"any ethical considerations regarding its release and potential misuse fall under the responsibility of its
806"
SAFEGUARDS,0.9258555133079848,"creators.
807"
SAFEGUARDS,0.9268060836501901,"Guidelines:
808"
SAFEGUARDS,0.9277566539923955,"‚Ä¢ The answer NA means that the paper poses no such risks.
809"
SAFEGUARDS,0.9287072243346007,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with necessary
810"
SAFEGUARDS,0.9296577946768061,"safeguards to allow for controlled use of the model, for example by requiring that users adhere to
811"
SAFEGUARDS,0.9306083650190115,"usage guidelines or restrictions to access the model or implementing safety filters.
812"
SAFEGUARDS,0.9315589353612167,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors should
813"
SAFEGUARDS,0.9325095057034221,"describe how they avoided releasing unsafe images.
814"
SAFEGUARDS,0.9334600760456274,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do not require
815"
SAFEGUARDS,0.9344106463878327,"this, but we encourage authors to take this into account and make a best faith effort.
816"
LICENSES FOR EXISTING ASSETS,0.935361216730038,"12. Licenses for existing assets
817"
LICENSES FOR EXISTING ASSETS,0.9363117870722434,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
818"
LICENSES FOR EXISTING ASSETS,0.9372623574144486,"properly credited and are the license and terms of use explicitly mentioned and properly respected?
819"
LICENSES FOR EXISTING ASSETS,0.938212927756654,"Answer: [Yes]
820"
LICENSES FOR EXISTING ASSETS,0.9391634980988594,"Justification: We credited the creators of the CoOp codebase [54] by including the attribution statement
821"
LICENSES FOR EXISTING ASSETS,0.9401140684410646,"in appendix.
822"
LICENSES FOR EXISTING ASSETS,0.94106463878327,"Guidelines:
823"
LICENSES FOR EXISTING ASSETS,0.9420152091254753,"‚Ä¢ The answer NA means that the paper does not use existing assets.
824"
LICENSES FOR EXISTING ASSETS,0.9429657794676806,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
825"
LICENSES FOR EXISTING ASSETS,0.9439163498098859,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a URL.
826"
LICENSES FOR EXISTING ASSETS,0.9448669201520913,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
827"
LICENSES FOR EXISTING ASSETS,0.9458174904942965,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of service of
828"
LICENSES FOR EXISTING ASSETS,0.9467680608365019,"that source should be provided.
829"
LICENSES FOR EXISTING ASSETS,0.9477186311787072,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package should
830"
LICENSES FOR EXISTING ASSETS,0.9486692015209125,"be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
831"
LICENSES FOR EXISTING ASSETS,0.9496197718631179,"some datasets. Their licensing guide can help determine the license of a dataset.
832"
LICENSES FOR EXISTING ASSETS,0.9505703422053232,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of the derived
833"
LICENSES FOR EXISTING ASSETS,0.9515209125475285,"asset (if it has changed) should be provided.
834"
LICENSES FOR EXISTING ASSETS,0.9524714828897338,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to the asset‚Äôs
835"
LICENSES FOR EXISTING ASSETS,0.9534220532319392,"creators.
836"
NEW ASSETS,0.9543726235741445,"13. New Assets
837"
NEW ASSETS,0.9553231939163498,"Question: Are new assets introduced in the paper well documented and is the documentation provided
838"
NEW ASSETS,0.9562737642585551,"alongside the assets?
839"
NEW ASSETS,0.9572243346007605,"Answer: [No]
840"
NEW ASSETS,0.9581749049429658,"Justification: Code will be realseased upon acceptance.
841"
NEW ASSETS,0.9591254752851711,"Guidelines:
842"
NEW ASSETS,0.9600760456273765,"‚Ä¢ The answer NA means that the paper does not release new assets.
843"
NEW ASSETS,0.9610266159695817,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their sub-
844"
NEW ASSETS,0.9619771863117871,"missions via structured templates. This includes details about training, license, limitations,
845"
NEW ASSETS,0.9629277566539924,"etc.
846"
NEW ASSETS,0.9638783269961977,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose asset is
847"
NEW ASSETS,0.964828897338403,"used.
848"
NEW ASSETS,0.9657794676806084,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either create an
849"
NEW ASSETS,0.9667300380228137,"anonymized URL or include an anonymized zip file.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967680608365019,"14. Crowdsourcing and Research with Human Subjects
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686311787072244,"Question: For crowdsourcing experiments and research with human subjects, does the paper include
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695817490494296,"the full text of instructions given to participants and screenshots, if applicable, as well as details about
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970532319391635,"compensation (if any)?
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714828897338403,"Answer: [NA]
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724334600760456,"Justification: Our paper doesn‚Äôt involve crowdsourcing or research with human subjects.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973384030418251,"Guidelines:
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743346007604563,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752851711026616,"subjects.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762357414448669,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribution of the
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771863117870723,"paper involves human subjects, then as much detail as possible should be included in the main
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781368821292775,"paper.
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790874524714829,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800380228136882,"labor should be paid at least the minimum wage in the country of the data collector.
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809885931558935,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819391634980988,"Question: Does the paper describe potential risks incurred by study participants, whether such
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828897338403042,"risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838403041825095,"equivalent approval/review based on the requirements of your country or institution) were obtained?
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847908745247148,"Answer: [NA]
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857414448669202,"Justification: This paper focuses on developing a novel prompt learning method and evaluating its
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866920152091255,"performance on established image recognition datasets. It doesn‚Äôt involve any form of crowdsourcing,
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876425855513308,"human subject research, or data collection that would necessitate IRB approval or ethical considerations
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885931558935361,"related to study participants. Therefore, this question doesn‚Äôt apply to our research.
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895437262357415,"Guidelines:
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904942965779467,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914448669201521,"subjects.
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923954372623575,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent) may be
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933460076045627,"required for any human subjects research. If you obtained IRB approval, you should clearly state
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942965779467681,"this in the paper.
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952471482889734,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions and
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961977186311787,"locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997148288973384,"their institution.
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980988593155894,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if applica-
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990494296577946,"ble), such as the institution conducting the review.
884"
