Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019646365422396855,"The learning with noisy labels has been addressed with both discriminative and
1"
ABSTRACT,0.003929273084479371,"generative models. Although discriminative models have dominated the field due
2"
ABSTRACT,0.005893909626719057,"to their simpler modeling and more efficient computational training processes, gen-
3"
ABSTRACT,0.007858546168958742,"erative models offer a more effective means of disentangling clean and noisy labels
4"
ABSTRACT,0.009823182711198428,"and improving the estimation of the label transition matrix. However, generative
5"
ABSTRACT,0.011787819253438114,"approaches maximize the joint likelihood of noisy labels and data using a complex
6"
ABSTRACT,0.0137524557956778,"formulation that only indirectly optimizes the model of interest associating data
7"
ABSTRACT,0.015717092337917484,"and clean labels. Additionally, these approaches rely on generative models that
8"
ABSTRACT,0.01768172888015717,"are challenging to train and tend to use uninformative clean label priors. In this
9"
ABSTRACT,0.019646365422396856,"paper, we propose a new generative noisy-label learning approach that addresses
10"
ABSTRACT,0.021611001964636542,"these three issues. First, we propose a new model optimisation that directly asso-
11"
ABSTRACT,0.023575638506876228,"ciates data and clean labels. Second, the generative model is implicitly estimated
12"
ABSTRACT,0.025540275049115914,"using a discriminative model, eliminating the inefficient training of a generative
13"
ABSTRACT,0.0275049115913556,"model. Third, we propose a new informative label prior inspired by partial label
14"
ABSTRACT,0.029469548133595286,"learning as supervision signal for noisy label learning. Extensive experiments on
15"
ABSTRACT,0.03143418467583497,"several noisy-label benchmarks demonstrate that our generative model provides
16"
ABSTRACT,0.03339882121807466,"state-of-the-art results while maintaining a similar computational complexity as
17"
ABSTRACT,0.03536345776031434,"discriminative models. Code will be available once paper is accepted.
18"
INTRODUCTION,0.03732809430255403,"1
Introduction
19"
INTRODUCTION,0.03929273084479371,"Deep neural network (DNN) has achieved remarkable success in computer vision [13, 21], natural
20"
INTRODUCTION,0.0412573673870334,"language processing (NLP) [10, 51] and medical image analysis [24, 38]. However, DNNs often
21"
INTRODUCTION,0.043222003929273084,"require massive amount of high-quality annotated data for supervised training [9], which is chal-
22"
INTRODUCTION,0.04518664047151277,"lenging and expensive to acquire. To alleviate such problem, some datasets have been annotated via
23"
INTRODUCTION,0.047151277013752456,"crowdsourcing [46], from search engines [35], or with NLP from radiology reports [38]. Although
24"
INTRODUCTION,0.04911591355599214,"these cheaper annotation processes enable the construction of large-scale datasets, they also introduce
25"
INTRODUCTION,0.05108055009823183,"noisy labels for model training, resulting in performance degradation. Therefore, novel learning
26"
INTRODUCTION,0.05304518664047151,"algorithms are required to robustly train DNN models when training sets contain noisy labels.
27"
INTRODUCTION,0.0550098231827112,"The main challenge in noisy-label learning is that we only observe the data, represented by random
28"
INTRODUCTION,0.05697445972495088,"variable X, and respective noisy label, denoted by variable ˜Y , but we want to estimate the model
29"
INTRODUCTION,0.05893909626719057,"p(Y |X), where Y is the hidden clean label variable. Most methods proposed in the field resort
30"
INTRODUCTION,0.060903732809430254,"to two discriminative learning strategies: sample selection and noise transition matrix. Sample
31"
INTRODUCTION,0.06286836935166994,"selection [1, 12, 22] optimises the model pθ(Y |X), parameterised by θ, with maximum likelihood
32"
INTRODUCTION,0.06483300589390963,"optimisation restricted to pseudo-clean training samples, as follows
33"
INTRODUCTION,0.06679764243614932,"θ∗= arg maxθ EP (X, ˜Y )
h
clean(X, ˜Y ) × pθ( ˜Y |X)
i
, where clean(X = x, ˜Y = ˜y) =
1, if Y = ˜y
0, otherwise
, (1)"
INTRODUCTION,0.068762278978389,"and P(X, ˜Y ) is the distribution used to generate the noisy-label and data points for the training
34"
INTRODUCTION,0.07072691552062868,"set. Note that EP (X, ˜Y )
h
clean(X, ˜Y ) × pθ( ˜Y |X)
i
≡EP (X,Y ) [pθ(Y |X)] if the function clean(.)
35"
INTRODUCTION,0.07269155206286837,"successfully selects the clean-label training samples. Unfortunately, clean(.) usually relies on the
36"
INTRODUCTION,0.07465618860510806,"small-loss hypothesis [2] for selecting R% of the smallest loss training samples, which offers little
37"
INTRODUCTION,0.07662082514734773,"guarantees of successfully selecting clean-label samples. Approaches based on noise transition
38"
INTRODUCTION,0.07858546168958742,"matrix [44, 6, 32] aim to estimate a clean-label classifier and a label transition, as follows:
39"
INTRODUCTION,0.08055009823182711,"θ∗= arg maxθ EP (X, ˜Y )
hP"
INTRODUCTION,0.0825147347740668,"Y p( ˜Y |X)
i
= arg maxθ1,θ2 EP (X, ˜Y )
hP"
INTRODUCTION,0.08447937131630648,"Y pθ1( ˜Y |Y, X)pθ2(Y |X)
i
,
(2)"
INTRODUCTION,0.08644400785854617,"where θ = [θ1, θ2], pθ1( ˜Y |Y, X) represents a label-transition matrix, often simplified to be class-
40"
INTRODUCTION,0.08840864440078586,"independent with pθ1( ˜Y |Y ) = pθ1( ˜Y |Y, X). Since we do not have access to the label transition
41"
INTRODUCTION,0.09037328094302555,"matrix, we need to estimate it from the noisy-label training set, which is challenging because of
42"
INTRODUCTION,0.09233791748526522,"identifiability issues [27], making necessary the use of anchor point [32] and regularisations [6].
43"
INTRODUCTION,0.09430255402750491,"On the other hand, generative learning models [3, 11, 50] assume a generative process for X and
44"
INTRODUCTION,0.0962671905697446,"Y , as described in Fig. 1. These methods are trained to maximise the data likelihood p( ˜Y , X) =
45
R"
INTRODUCTION,0.09823182711198428,"Y,Z p(X|Y, Z)p( ˜Y |Y, X)p(Y )p(Z)dY dZ, where Z denotes a latent variable representing a low-
46"
INTRODUCTION,0.10019646365422397,"dimensional representation of the image, and Y is the latent clean label. This optimisation requires a
47"
INTRODUCTION,0.10216110019646366,"variational distribution qϕ(Y, Z|X) to maximise the evidence lower bound (ELBO): with
48"
INTRODUCTION,0.10412573673870335,"θ∗
1, θ∗
2, ϕ∗= arg maxθ1,θ2,ϕ Eqϕ(Y,Z|X)
h
log

pθ1(X|Y, Z)pθ2( ˜Y |X, Y )p(Y )p(Z)/qϕ(Y, Z|X)
i
,
(3)"
INTRODUCTION,0.10609037328094302,"where pθ1(X|Y, Z) denotes an image generative model, pθ2( ˜Y |X, Y ) represents the label transition
49"
INTRODUCTION,0.10805500982318271,"model, p(Z) is the latent image representation prior (commonly assumed to a standard normal
50"
INTRODUCTION,0.1100196463654224,"distribution), and p(Y ) is the clean label prior (usually assumed to be a non-informative prior based
51"
INTRODUCTION,0.11198428290766209,"on a uniform distribution). Such generative strategy is sensible because it disentangles the true and
52"
INTRODUCTION,0.11394891944990176,"noisy labels and improves the estimation of the label transition model [50]. A limitation of the
53"
INTRODUCTION,0.11591355599214145,"generative strategy is that it optimises p( ˜Y , X) instead of directly optimising p(X|Y ) or p(Y |X).
54"
INTRODUCTION,0.11787819253438114,"Also, compared with the discriminative strategy, the generative approach requires the generative
55"
INTRODUCTION,0.11984282907662082,"model pθ1(X|Y, Z) that is challenging to train. This motivates us to ask the following question:
56"
INTRODUCTION,0.12180746561886051,"Can we directly optimise the generative goal p(X|Y ), with a similar computational cost as the
57"
INTRODUCTION,0.1237721021611002,"discriminative strategy and accounting for an informative prior for the latent clean label Y ?
58"
INTRODUCTION,0.12573673870333987,"In this paper, we propose a new generative noisy-label learning method to directly optimise p(X|Y ) by
59"
INTRODUCTION,0.12770137524557956,"maximising Eq(Y |X) [log p(X|Y )] using a variational posterior distribution q(Y |X). This objective
60"
INTRODUCTION,0.12966601178781925,"function is decomposed into three terms: a label-transition model Eq(y|x) [log p(˜y|x, y)], an image
61"
INTRODUCTION,0.13163064833005894,"generative model Eq(y|x)
h
log p(x|y)p(y)"
INTRODUCTION,0.13359528487229863,"q(y|x)
i
, and a Kullback–Leibler (KL) divergence regularisation
62"
INTRODUCTION,0.13555992141453832,"term. We implicitly estimate the image generative term with the discriminative model q(Y |X),
63"
INTRODUCTION,0.137524557956778,"bypassing the need to train a generative model. Moreover, our formulation allows the introduction
64"
INTRODUCTION,0.13948919449901767,"of an instance-wise informative prior p(Y ) inspired by partial-label learning [36]. This prior is
65"
INTRODUCTION,0.14145383104125736,"re-estimated at each training epoch to cover a small number of label candidates if the model is certain
66"
INTRODUCTION,0.14341846758349705,"about the training label. Conversely, when the model is uncertain about the training label, then the
67"
INTRODUCTION,0.14538310412573674,"label prior will cover a large number of label candidates, which also serve as a regularisation of noisy
68"
INTRODUCTION,0.14734774066797643,"label training. Our formulation only requires a discriminative model and a label transition model,
69"
INTRODUCTION,0.14931237721021612,"making it computationally less expensive than other generative approaches [3, 11, 50]. Overall, our
70"
INTRODUCTION,0.1512770137524558,"contributions can be summarized as follows:
71"
INTRODUCTION,0.15324165029469547,"• We introduce a new generative framework to handle noisy-label learning by directly opti-
72"
INTRODUCTION,0.15520628683693516,"mising p(X|Y ).
73"
INTRODUCTION,0.15717092337917485,"• Our generative model is implicitly estimated with a discriminative model, making it compu-
74"
INTRODUCTION,0.15913555992141454,"tationally more efficient than previous generative approaches [3, 11, 50].
75"
INTRODUCTION,0.16110019646365423,"• Our framework allows us to place an informative instance-wise prior p(Y ) for latent clean
76"
INTRODUCTION,0.16306483300589392,"label Y . Inspired by partial label learning [36], p(Y ) is constructed for maintaining high
77"
INTRODUCTION,0.1650294695481336,"coverage for latent clean label and regularise uncertain sample training.
78"
INTRODUCTION,0.16699410609037327,"We conduct extensive experiments on both synthetic and real-world noisy-label benchmarks that
79"
INTRODUCTION,0.16895874263261296,"show that our method provides state-of-the-art (SOTA) results and enjoy a similar computational
80"
INTRODUCTION,0.17092337917485265,"complexity as discriminative approaches.
81"
RELATED WORK,0.17288801571709234,"2
Related Work
82"
RELATED WORK,0.17485265225933203,"Sample selection. The discriminative learning strategy based on sample selection from (1) needs
83"
RELATED WORK,0.17681728880157171,"to handle two problems: 1) the definition of clean(.), and 2) what to do with the samples classified
84"
RELATED WORK,0.1787819253438114,"as noisy. Most definitions of clean(.) resort to classify small-loss samples [2] as pseudo-clean [1,
85"
RELATED WORK,0.1807465618860511,"4, 12, 15, 22, 30, 34, 40]. Other approaches select clean samples based on the K nearest neighbor
86"
RELATED WORK,0.18271119842829076,"classification in an intermediate deep learning feature spaces [31, 39], distance to the class-specific
87"
RELATED WORK,0.18467583497053044,"eigenvector from the gram matrix eigen-decomposition using intermediate deep learning feature
88"
RELATED WORK,0.18664047151277013,"spaces [17], uncertainty measures [19], or prediction consistency between teacher and student
89"
RELATED WORK,0.18860510805500982,"models [16]. After sample classification, some methods will discard the noisy-label samples for
90"
RELATED WORK,0.1905697445972495,"training [4, 15, 30, 34], while others use them for semi-supervised learning [22]. The main issue with
91"
RELATED WORK,0.1925343811394892,"this strategy is that it does not try to disentangle the clean and noisy-label from the samples.
92"
RELATED WORK,0.1944990176817289,"Label transition model. The discriminative learning strategy based on the label transition model
93"
RELATED WORK,0.19646365422396855,"from (2) depends on a reliable estimation of p( ˜Y |Y, X) [6, 32, 44]. Forward-T [32] uses an additional
94"
RELATED WORK,0.19842829076620824,"classifier and anchor points from clean-label samples to learn a class-dependent transition matrix.
95"
RELATED WORK,0.20039292730844793,"Part-T [44] estimates an instance-dependent model. MEDITM [6] uses manifold regularization for
96"
RELATED WORK,0.20235756385068762,"estimating the label-transition matrix. In general, the estimation of this label transition matrix is
97"
RELATED WORK,0.2043222003929273,"under-constrained, leading to the identifiability problem [27], which is addressed with the formulation
98"
RELATED WORK,0.206286836935167,"of anchor point [32], or additional regularisation [6].
99"
RELATED WORK,0.2082514734774067,"Figure 1: Generative noisy-label
learning models and their corre-
sponding optimisation goal, where
the red arrow indicates the dif-
ferent
causal
relationships
be-
tween
X
and
Y .
Left
is
CausalNL/InstanceGM [50, 11],
middle is NPC [3] and right is ours."
RELATED WORK,0.21021611001964635,"Generative modelling. Generative modeling for noisy-label
100"
RELATED WORK,0.21218074656188604,"learning [3, 11, 50] explores different graphical models (see
101"
RELATED WORK,0.21414538310412573,"Fig. 1) to enable the estimation of clean labels per image.
102"
RELATED WORK,0.21611001964636542,"Specifically, CausalNL [50] and InstanceGM [11] assume that
103"
RELATED WORK,0.2180746561886051,"the latent clean label Y causes X, and the noisy label ˜Y is gen-
104"
RELATED WORK,0.2200392927308448,"erated from X and Y . Alternatively, NPC [3] assumes that X
105"
RELATED WORK,0.2220039292730845,"causes Y and proposes a post-processing calibration for noisy
106"
RELATED WORK,0.22396856581532418,"label learning. One drawback of generative modeling is that
107"
RELATED WORK,0.22593320235756384,"instead of directly optimising the models of interest p(X|Y ) or
108"
RELATED WORK,0.22789783889980353,"p(Y |X), it optimises the joint distribution of visible variables
109"
RELATED WORK,0.22986247544204322,"p(X, ˜Y ). Even though maximising the likelihood of the visible
110"
RELATED WORK,0.2318271119842829,"data is sensible, it only produces the models of interest as a
111"
RELATED WORK,0.2337917485265226,"by-product of the process. Furthermore, these methods require
112"
RELATED WORK,0.2357563850687623,"the computationally complex training of a generative model,
113"
RELATED WORK,0.23772102161100198,"and usually rely on non-informative label priors.
114"
RELATED WORK,0.23968565815324164,"Clean label prior. Our clean-label prior p(Y ) constrains the
115"
RELATED WORK,0.24165029469548133,"clean label to a set of label candidates for a particular training sample. Such label candidates change
116"
RELATED WORK,0.24361493123772102,"aims to 1) increase clean label coverage, and 2) represent uncertainty of the prior. Increase coverage
117"
RELATED WORK,0.2455795677799607,"improve the chances of including latent clean label in supervision. For noisy samples, increase the
118"
RELATED WORK,0.2475442043222004,"number of candidates in p(Y ) regularise noisy label training. Such dynamic prior distribution may
119"
RELATED WORK,0.24950884086444008,"resemble Mixup [53], label smoothing [28] or re-labeling [22] techniques that are commonly used
120"
RELATED WORK,0.25147347740667975,"in label noise learning. However, these approaches do not simultaneously follow the two design
121"
RELATED WORK,0.25343811394891946,"principles mentioned above. Mixup [53] and label smoothing [28] are effective approaches for
122"
RELATED WORK,0.2554027504911591,"designing soft labels for noisy label learning, but both aim to increase coverage, disregarding label
123"
RELATED WORK,0.25736738703339884,"uncertainty. Re-labeling switches the supervisory training signal to a more likely pseudo label, so it
124"
RELATED WORK,0.2593320235756385,"is very efficient, but it has limited coverage.
125"
RELATED WORK,0.26129666011787817,"Partial label learning In partial label learning (PLL), each image is associated with a candidate label
126"
RELATED WORK,0.2632612966601179,"set defined as a partial label [36]. The goal of PLL is to predict the single true label associated with
127"
RELATED WORK,0.26522593320235754,"each training sample, assuming that the ground truth label is one of the labels in its candidate set.
128"
RELATED WORK,0.26719056974459726,"PICO [37] uses contrastive learning in an EM optimisation to address PLL. CAV [52] proposes class
129"
RELATED WORK,0.2691552062868369,"activation mapping to identify the true label within the candidate set. PRODEN [29] progressively
130"
RELATED WORK,0.27111984282907664,"identifies the true labels from a candidate set and updates the model parameter. The design of our
131"
RELATED WORK,0.2730844793713163,"informative clean label prior p(Y ) is inspired from PLL, but unlike PLL, there is no guarantee
132"
RELATED WORK,0.275049115913556,"that the multiple label candidates in our prior contain the true label. Furthermore, the size of our
133"
RELATED WORK,0.2770137524557957,"candidate label set is determined by the probability that the training sample label is clean, where a
134"
RELATED WORK,0.27897838899803534,"low probability induces a prior with a large number of candidates for regularising training.
135"
METHOD,0.28094302554027506,"3
Method
136"
METHOD,0.2829076620825147,"We denote the noisy training set as D = {(xi, ˜yi)}|D|
i=1, where xi ∈X ⊂RH×W ×C is the input
137"
METHOD,0.28487229862475444,"image of size H × W with C colour channels, ˜yi ∈Y ⊂{0, 1}|Y| is the observed noisy label.
138"
METHOD,0.2868369351669941,"We also have y as the unobserved clean label. We formulate our model with generative model
139"
METHOD,0.2888015717092338,"that starts with the sampling of a label y ∼p(Y ). This is followed by the clean-label conditioned
140"
METHOD,0.2907662082514735,"generation of an image with x ∼p(X|Y = y), which are then used to produce the noisy label
141"
METHOD,0.29273084479371314,"˜y ∼p( ˜Y |Y = y, X = x) (hereafter, we omit the variable names to simplify the notation). Below, in
142"
METHOD,0.29469548133595286,"Sec. 3.1, we introduce our model and the optimisation goal. In Sec. 3.2 we describe how to construct
143"
METHOD,0.2966601178781925,"informative prior, and the overall training algorithm is presented in Sec. 3.3.
144"
MODEL,0.29862475442043224,"3.1
Model
145"
MODEL,0.3005893909626719,"We aim to optimize the generative model log p(x|y), which can be decomposed as follows:
146"
MODEL,0.3025540275049116,"log p(x|y) = log
p(˜y, y, x)
p(˜y|x, y)p(y).
(4)"
MODEL,0.3045186640471513,"In (4), p(y) represents the prior distribution of the latent clean label. The optimisation of p(x|y) can
147"
MODEL,0.30648330058939094,"be achieved by introducing a variational posterior distribution q(y|x), with:
148"
MODEL,0.30844793713163066,"log p(x|y) = log p(˜y, y, x)"
MODEL,0.3104125736738703,"q(y|x)
+ log
q(y|x)
p(˜y|x, y)p(y),"
MODEL,0.31237721021611004,Eq(y|x) [log p(x|y)] = Eq(y|x)
MODEL,0.3143418467583497,"
log p(˜y, y, x)"
MODEL,0.3163064833005894,q(y|x)
MODEL,0.3182711198428291,"
+ KL
h
q(y|x)||p(˜y|x, y)p(y)
i
,
(5)"
MODEL,0.32023575638506874,"where KL[.] denotes the KL divergence, and
149"
MODEL,0.32220039292730845,Eq(y|x)
MODEL,0.3241650294695481,"
log p(˜y, y, x)"
MODEL,0.32612966601178783,q(y|x)
MODEL,0.3280943025540275,"
= Eq(y|x) [log p(˜y|x, y)] + Eq(y|x)"
MODEL,0.3300589390962672,"
log p(x|y)p(y)"
MODEL,0.3320235756385069,q(y|x)
MODEL,0.33398821218074654,"
.
(6)"
MODEL,0.33595284872298625,"Based on Eq. (5) and (6), the expected log likelihood of p(x|y) is defined as
150"
MODEL,0.3379174852652259,"Eq(y|x) [log p(x|y)] = Eq(y|x) [log p(˜y|x, y)] −KL [q(y|x)∥p(x|y)p(y)] + KL [q(y|x)∥p(˜y|x, y)p(y)] ."
MODEL,0.33988212180746563,"(7)
In Eq. (7), we parameterise q(y|x) and p(˜y|x, y) with neural networks, as depicted in Figure 2. The
151"
MODEL,0.3418467583497053,"generative model p(x|y) usually requires to model infinite number of samples based on conditional
152"
MODEL,0.343811394891945,"label and a generative model is hard to capture such relationship. However, since noisy label learning
153"
MODEL,0.34577603143418467,"is a discriminative task and classification performance is our primary goal, the generation can be
154"
MODEL,0.3477406679764244,"approximated with with finite training samples, which is given training set. More specifically, we
155"
MODEL,0.34970530451866405,"defines p(x|y) only on data points {xi}|D|
i=1 by maximising −KL [q(y|x)∥p(x|y)p(y)] for a fixed
156"
MODEL,0.3516699410609037,"q(y|x), with the optimum achieved by:
157"
MODEL,0.35363457760314343,"p(x|y) =
q(y|x)
P|D|
i=1 q(y|xi)
.
(8)"
MODEL,0.3555992141453831,"Hence, the generative conditional p(x|y) can only represent the values of x within training set given
158"
MODEL,0.3575638506876228,"the latent labels in y. This allow us transform discriminative model into implicit generative model
159"
MODEL,0.35952848722986247,"without additional computation cost.
160"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3614931237721022,"3.2
Informative prior based on partial label learning
161"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.36345776031434185,"In Eq. (7), the clean label prior p(y) is required. As mentioned in Sec. 2, we formulate p(y) inspired
162"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3654223968565815,"from PLL [29, 37, 52]. However, it is worth noting that PLL has the partial label information available
163"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.36738703339882123,"from the training set, while we have to dynamically build it during training. Therefore, the clean label
164"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3693516699410609,"prior p(y) for each training sample is designed so that the hidden clean label has a high probability
165"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3713163064833006,"of being selected during most of the training. On one hand, we aim to have as many label candidates
166"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.37328094302554027,"as possible during the training to increase the chances that p(y) has a non-zero probability for the
167"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.37524557956778,"latent clean label. On the other hand, including all labels as candidates is a trivial solution that does
168"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.37721021611001965,"Figure 2: Training pipeline of our method. Shaded variables x and ˜y are visible, and unshaded
variable y is latent. p(y) is constructed to approximate y."
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3791748526522593,"not represent a meaningful clean label prior. These two seemingly contradictory goals target the
169"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.381139489194499,"maximisation of label coverage and minimisation of label uncertainty, defined by:
170"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3831041257367387,"Coverage =
1
|D| |D|
X i=1 |Y|
X"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3850687622789784,"j=1
1 (yi(j) × pi(j) > 0) , and Uncertainty =
1
|D| |D|
X i=1 |Y|
X"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.38703339882121807,"j=1
1 (pi(j) > 0) ,"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3889980353634578,"(9)
where 1(.) is the indicator function. In (9), coverage increases by approximating p(Y ) to a uniform
171"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.39096267190569745,"distribution, but uncertainty is minimised when the clean label yi is assigned maximum probability. In
172"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3929273084479371,"general, training samples for which the model is certain about the clean label, should have p(yi) = 1,
173"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3948919449901768,"while training samples for which the model is uncertain about the clean label, should have p(yi) < 1
174"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3968565815324165,"with other candidate labels with probability > 0. Therefore, the clean label prior is defined by:
175"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.3988212180746562,pi(j) = ˜yi(j) + ci(j) + ui(j)
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.40078585461689586,"Z
,
(10)"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4027504911591356,"where Z is a normalisation factor to make P|Y|
j=1 pi(j) = 1, ˜yi is the noisy label in the training set,
176"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.40471512770137524,"ci denotes the label to increase coverage, and ui represents the label to increase uncertainty, both
177"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4066797642436149,"defined below. Motivated by the early learning phenomenon [25], where clean labels tend to be
178"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4086444007858546,"fit earlier in the training than the noisy labels, we maximise coverage by sampling from a moving
179"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4106090373280943,"average of model prediction for each training sample xi at iteration t with:
180"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.412573673870334,"C(t)
i
= β × C(t−1)
i
+ (1 −β) × ¯y(t)
i ,
(11)"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.41453831041257366,"where β ∈[0, 1] and ¯y(t) is the softmax output from the model that predicts the clean label from the
181"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4165029469548134,"data input xi. For Eq. (11), C(t)
i
denotes the categorical distribution of the most likely labels for the
182"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.41846758349705304,"ith training sample, which can be used to sample the one-hot label ci ∼Cat(C(t)
i ). The minimisation
183"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4204322200392927,"of uncertainty depends on our ability to detect clean-label and noisy-label samples. For clean samples,
184"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4223968565815324,"p(yi) should converge to a one-hot distribution, maintaining the label prior focused on few candidate
185"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4243614931237721,"labels. For noisy samples, p(yi) should be close to a uniform distribution to keep a large coverage of
186"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4263261296660118,"candidate labels. To compute the probability wi ∈[0, 1] that a sample contains clean label, we use
187"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.42829076620825146,"the sample selection approaches based on the unsupervised classification of loss values [22]. Then
188"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4302554027504912,"the label ui is obtained by sampling from a uniform distribution of all possible labels proportionally
189"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.43222003929273084,"to its probability of representing a noisy-label sample, with
190"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.43418467583497056,"ui ∼U (Y, round(|Y| × (1 −wi))) ,
(12)
where round(|Y| × (1 −wi)) represents the number of samples to be drawn from the uniform
191"
INFORMATIVE PRIOR BASED ON PARTIAL LABEL LEARNING,0.4361493123772102,"distribution rounded up to the closest integer.
192"
TRAINING,0.4381139489194499,"3.3
Training
193"
TRAINING,0.4400785854616896,"We can now return to the optimisation of Eq. (7), where we define the neural networks gθ : X →
194"
TRAINING,0.44204322200392926,"∆|Y|−1 that outputs the categorical distribution for the clean label in the probability simplex space
195"
TRAINING,0.444007858546169,"∆|Y|−1 given an image x ∈X, and fϕ : X × ∆|Y|−1 →∆|Y|−1 that outputs the categorical
196"
TRAINING,0.44597249508840864,"distribution for the noisy training label given an image and the clean label distribution from gθ(.).
197"
TRAINING,0.44793713163064836,"The first term in the right-hand side (RHS) in Eq. (7) is optimised with the cross-entropy loss:
198"
TRAINING,0.449901768172888,"LCE(θ, ϕ, D) =
1
|D| × K X"
TRAINING,0.4518664047151277,"(xi,˜yi)∈D K
X"
TRAINING,0.4538310412573674,"j=1
ℓCE(˜yi, fϕ(xi, ˆyi,j)).
(13)"
TRAINING,0.45579567779960706,"where {ˆyi,j}K
j=1 ∼Cat(gθ(xi)), with Cat(.) denoting a categorical distribution. The second term in
199"
TRAINING,0.4577603143418468,"the RHS in Eq. (7) uses the estimation of p(x|y) from Eq. (8) to optimise the KL divergence:
200"
TRAINING,0.45972495088408644,"LP RI(θ, D) =
1
|D| X"
TRAINING,0.46168958742632615,"(xi,˜yi)∈D
KL """
TRAINING,0.4636542239685658,"gθ(xi)
ci ×
gθ(xi)
P"
TRAINING,0.4656188605108055,j gθ(xj) ⊙pi #
TRAINING,0.4675834970530452,",
(14)"
TRAINING,0.46954813359528486,"where pi = [pi(j = 1), ..., pi(j = |Y|)] ∈∆|Y|−1 is the clean label prior defined in Eq. (10), ci
201"
TRAINING,0.4715127701375246,"is a normalisation factor, and ⊙is the element-wise multiplication. The last term in the RHS of
202"
TRAINING,0.47347740667976423,"Eq. (7) is the KL divergence between q(y|x) and p(˜y|x, y)p(y), which represents the gap between
203"
TRAINING,0.47544204322200395,"Eq(y|x) [log p(x|y)] and Eq(y|x)
h
log p(˜y,y,x)"
TRAINING,0.4774066797642436,"q(y|x)
i
. According to the expectation-maximisation (EM)
204"
TRAINING,0.4793713163064833,"derivation [8, 18], the smaller this gap, the better q(y|x) approximates the true posterior p(y|x), so
205"
TRAINING,0.481335952848723,"the loss function associated with this third term is:
206"
TRAINING,0.48330058939096265,"LKL(θ, ϕ, D) =
1
|D| X"
TRAINING,0.48526522593320237,"(xi,˜yi)∈D
KL
h
gθ(xi)
fϕ(xi, gθ(xi)) ⊙pi
i
.
(15)"
TRAINING,0.48722986247544203,"Our final loss to minimise is
207"
TRAINING,0.48919449901768175,"L(θ, ϕ, D) = LCE(θ, ϕ, D) + LP RI(θ, D) + LKL(θ, ϕ, D).
(16)"
TRAINING,0.4911591355599214,"After training, a test image x is associated with a class with gθ(x). An interesting point about
208"
TRAINING,0.4931237721021611,"this derivation is that the implicit approximation of p(x|y) enables the minimisation of the loss
209"
TRAINING,0.4950884086444008,"in (16) using regular stochastic gradient descent instead of a more computationally complex EM
210"
TRAINING,0.49705304518664045,"algorithm [33].
211"
EXPERIMENTS,0.49901768172888017,"4
Experiments
212"
EXPERIMENTS,0.5009823182711198,"We show experimental results on instance-dependent synthetic and real-world label noise benchmarks
213"
EXPERIMENTS,0.5029469548133595,"with datasets CIFAR10/100 [20]. We also test on three instance-dependent real-world label noise
214"
EXPERIMENTS,0.5049115913555993,"datasets, namely: Animal-10N [35], Red Mini-ImageNet [15], and Clothing1M [46].
215"
DATASETS,0.5068762278978389,"4.1
Datasets
216"
DATASETS,0.5088408644400786,"CIFAR10/100 [20] contain a training set with 50K images and a testing of 10K images of size 32
217"
DATASETS,0.5108055009823183,"× 32 × 3, where CIFAR10 has 10 classes and CIFAR100 has 100 classes. We follow previous
218"
DATASETS,0.5127701375245579,"works [44] and synthetically generate instance-dependent noise (IDN) with rates in {0.2, 0.3, 0.4
219"
DATASETS,0.5147347740667977,",0.5}. CIFAR10N/CIFAR100N is proposed by [43] to study real-world annotations for the original
220"
DATASETS,0.5166994106090373,"CIFAR10/100 images and we test our framework on {aggre, random1, random2, random3, worse}
221"
DATASETS,0.518664047151277,"types of noise on CIFAR10N and {noisy} on CIFAR100N. Red Mini-ImageNet is a real-world
222"
DATASETS,0.5206286836935167,"dataset [15] containing 100 classes, each containing 600 images from ImageNet, where images
223"
DATASETS,0.5225933202357563,"are resized to 32 × 32 pixels from the original 84 × 84 to enable a fair comparison with other
224"
DATASETS,0.5245579567779961,"baselines [48]. Animal 10N [35] is a real-world dataset containing 10 animal species with five pairs
225"
DATASETS,0.5265225933202358,"of similar appearances (wolf and coyote, etc.). The training set size is 50K and testing size is 10K,
226"
DATASETS,0.5284872298624754,"where we follow the same set up as [5]. Clothing1M is a real-world dataset with 100K images and
227"
DATASETS,0.5304518664047151,"14 classes. The labels are automatically generated from surrounding text with an estimated noise
228"
DATASETS,0.5324165029469549,"ratio of 38.5%. The dataset also contains clean samples for training and validation but we only use
229"
DATASETS,0.5343811394891945,"clean test for measuring model performance.
230"
PRACTICAL CONSIDERATIONS,0.5363457760314342,"4.2
Practical considerations
231"
PRACTICAL CONSIDERATIONS,0.5383104125736738,"We follow commonly used experiment setups for all benchmarks described in Sec. 4.1. 1 For the
232"
PRACTICAL CONSIDERATIONS,0.5402750491159135,"hyper-parameter setup, K in (13) is set to 1, and β in Eq. (11) is set to 0.9. For w in Eq. (12), we follow
233"
PRACTICAL CONSIDERATIONS,0.5422396856581533,"the commonly used Gaussian Mixture Model (GMM) unsupervised classification from [22]. For
234"
PRACTICAL CONSIDERATIONS,0.5442043222003929,"warmup epochs, w is randomly generated from a uniform distribution. Note that the approximation
235"
PRACTICAL CONSIDERATIONS,0.5461689587426326,"of the generative model from (8) is done within each batch, not the entire the dataset. Also, the
236"
PRACTICAL CONSIDERATIONS,0.5481335952848723,"minimisation of LP RI(.) can be done with the reversed KL using KL
h
ci ×
gθ(xi)
P"
PRACTICAL CONSIDERATIONS,0.550098231827112,"j gθ(xj) ⊙pi
gθ(xi)
i
.
237"
PRACTICAL CONSIDERATIONS,0.5520628683693517,1Please see the supplementary material about implementation details.
PRACTICAL CONSIDERATIONS,0.5540275049115914,"Method
CIFAR10
20%
30%
40%
50%
CE
86.93±0.17
82.42±0.44
76.68±0.23
58.93± 1.54
DMI [47]
89.99± 0.15
86.87± 0.34
80.74± 0.44
63.92±3.92
Forward [32]
89.62±0.14
86.93±0.15
80.29±0.27
65.91±1.22
CoTeaching [12]
88.43±0.08
86.40±0.41
80.85±0.97
62.63± 1.51
TMDNN [49]
88.14± 0.66
84.55±0.48
79.71±0.95
63.33± 2.75
PartT [44]
89.33± 0.70
85.33±1.86
80.59±0.41
64.58± 2.86
kMEIDTM [6]
92.26± 0.25
90.73± 0.34
85.94± 0.92
73.77±0.82
CausalNL [50]
81.47± 0.32
80.38± 0.44
77.53± 0.45
67.39±1.24
Ours
92.65±0.13
91.96±0.20
91.02±0.44
89.94±0.45
Table 1: Accuracy (%) on the test set for CIFAR10-IDN. Most results are from [6]. Experiments are
repeated 3 times to compute mean±standard deviation. Top part shows discriminative and bottom
shows generative models. Best results are highlighted."
PRACTICAL CONSIDERATIONS,0.555992141453831,"Method
CIFAR100
20%
30%
40%
50%
CE
63.94±0.51
61.97±1.16
58.70±0.56
56.63±0.69
DMI [47]
64.72±0.64
62.8±1.46
60.24±0.63
56.52±1.18
Forward [32]
67.23±0.29
65.42±0.63
62.18±0.26
58.61±0.44
CoTeaching [12]
67.40±0.44
64.13±0.43
59.98±0.28
57.48±0.74
TMDNN [49]
66.62±0.85
64.72±0.64
59.38±0.65
55.68±1.43
PartT [44]
65.33±0.59
64.56±1.55
59.73±0.76
56.80±1.32
kMEIDTM [6]
69.16±0.16
66.76±0.30
63.46±0.48
59.18±0.16
CausalNL [50]
41.47±0.43
40.98±0.62
34.02±0.95
32.13±2.23
Ours
71.24±0.43
69.64±0.78
67.48±0.85
63.60±0.17
Table 2: Accuracy (%) on the test set for CIFAR100-IDN. Most results are from [6]. Experiments are
repeated 3 times to compute mean±standard deviation. Top part shows discriminative and bottom
shows generative models. Best results are highlighted."
PRACTICAL CONSIDERATIONS,0.5579567779960707,"Method
CIFAR10N
CIFAR100N
Aggregate
Random 1
Random 2
Random 3
Worst
Noisy
CE
87.77±0.38
85.02±0.65
86.46±1.79
85.16±0.61
77.69±1.55
55.50±0.66
Forward T [32]
88.24±0.22
86.88±0.50
86.14±0.24
87.04±0.35
79.79±0.46
57.01±1.03
T-Revision [45]
88.52±0.17
88.33±0.32
87.71±1.02
80.48±1.20
80.48±1.20
51.55±0.31
Positive-LS [28]
91.57±0.07
89.80±0.28
89.35±0.33
89.82±0.14
82.76±0.53
55.84±0.48
F-Div [42]
91.64±0.34
89.70±0.40
89.79±0.12
89.55±0.49
82.53±0.52
57.10±0.65
Negative-LS [41]
91.97±0.46
90.29±0.32
90.37±0.12
90.13±0.19
82.99±0.36
58.59±0.98
CORES2 [7]
91.23±0.11
89.66±0.32
89.91±0.45
89.79±0.50
83.60±0.53
61.15±0.73
VolMinNet [23]
89.70±0.21
88.30±0.12
88.27±0.09
88.19±0.41
80.53±0.20
57.80±0.31
CAL [55]
91.97±0.32
90.93±0.31
90.75±0.30
90.74±0.24
85.36±0.16
61.73±0.42
Ours
92.57±0.20
91.97±0.09
91.42±0.06
91.83±0.12
86.99±0.36
61.54±0.22
Table 3: Accuracy (%) on the test set for CIFAR10N/100N. Results are taken from [43] using methods
containing a single classifier with ResNet-34. Best results are highlighted."
PRACTICAL CONSIDERATIONS,0.5599214145383105,"This reversed KL divergence also provides solutions where the model and implied posterior are close.
238"
PRACTICAL CONSIDERATIONS,0.5618860510805501,"In fact, the KL and reversed KL losses are equivalent when P
j gθ(xj) has a uniform distribution
239"
PRACTICAL CONSIDERATIONS,0.5638506876227898,"over the classes in Y and the prior pi is uniform in the negative labels. We tried the optimisation
240"
PRACTICAL CONSIDERATIONS,0.5658153241650294,"using both versions of the KL divergence (i.e., the one in (14) and the one above in this section), with
241"
PRACTICAL CONSIDERATIONS,0.5677799607072691,"the reversed one generally producing better results, as shown in the ablation study in Sec. 4.4. For all
242"
PRACTICAL CONSIDERATIONS,0.5697445972495089,"experiments in Sec. 4.3, we rely on the reversed KL loss. For the real-world datasets Animal-10N,
243"
PRACTICAL CONSIDERATIONS,0.5717092337917485,"Red Mini-ImageNet and Clothing1M we also test our model with the training and testing of an
244"
PRACTICAL CONSIDERATIONS,0.5736738703339882,"ensemble of two networks. Our code is implemented in Pytorch and experiments are performed on
245"
PRACTICAL CONSIDERATIONS,0.5756385068762279,"RTX 3090.
246"
EXPERIMENTAL RESULTS,0.5776031434184676,"4.3
Experimental Results
247"
EXPERIMENTAL RESULTS,0.5795677799607073,"Synthetic benchmarks. The experimental results of our method with IDN problems on CIFAR10/100
248"
EXPERIMENTAL RESULTS,0.581532416502947,"are shown in Tab.1 and Tab.2. Compared with the previous SOTA kMEDITM [6], on CIFAR10, we
249"
EXPERIMENTAL RESULTS,0.5834970530451866,"Method
Noise rate
0.2
0.4
0.6
0.8
CE
47.36
42.70
37.30
29.76
Mixup [53]
49.10
46.40
40.58
33.58
DivideMix [22]
50.96
46.72
43.14
34.50
MentorMix [14]
51.02
47.14
43.80
33.46
FaMUS [48]
51.42
48.06
45.10
35.50
Ours
53.34
49.56
44.08
36.70
Ours ensemble
57.56
52.68
47.12
39.54"
EXPERIMENTAL RESULTS,0.5854616895874263,"Method
Accuracy
CE
79.4
SELFIE [35]
81.8
JoCoR [40]
82.8
PLC [54]
83.4
Nested + Co-T [5]
84.1
InstanceGM [11]
84.6
Ours
82.7
Ours ensemble
85.7
Table 4: Test accuracy (%) on Red Mini-ImageNet (Left) with different noise rates and baselines
from FaMUS [48], and on Animal-10N (Right), with baselines from [5]. Best results are highlighted."
EXPERIMENTAL RESULTS,0.587426326129666,"CE
Forward [32]
PTD-R-V [44]
ELR [26]
kMEIDTM [6]
CausalNL [50]
Our ensemble
68.94
69.84
71.67
72.87
73.34
72.24
74.35
Table 5: Test accuracy (%) on the test set of Clothing1M. Results are obtained from their respective
papers. We only use the noisy training set for training. Best results are highlighted."
EXPERIMENTAL RESULTS,0.5893909626719057,"achieve competitive performance on low noise rates and up to 16% improvements for high noise
250"
EXPERIMENTAL RESULTS,0.5913555992141454,"rates. For CIFAR100, we consistently improve 2% to 4% in all noise rates. Compared with the
251"
EXPERIMENTAL RESULTS,0.593320235756385,"previous SOTA generative model CausalNL [50], our improvement is significant for all noise rates.
252"
EXPERIMENTAL RESULTS,0.5952848722986247,"The superior performance of our method indicates that our implicit generative modelling and clean
253"
EXPERIMENTAL RESULTS,0.5972495088408645,"label prior construction is effective when learning with label noise.
254"
EXPERIMENTAL RESULTS,0.5992141453831041,"Real-world benchmarks. In Tab.3, we show the performance of our method on the CIFAR10N/100N
255"
EXPERIMENTAL RESULTS,0.6011787819253438,"benchmark. Compared with other single-model baselines, our method achieves at least 1% improve-
256"
EXPERIMENTAL RESULTS,0.6031434184675835,"ment on all noise rates on CIFAR10N, and it has a competitive performance on CIFAR100N. The Red
257"
EXPERIMENTAL RESULTS,0.6051080550098232,"Mini-ImageNet results in Tab.4 (left) show that our method achieves SOTA results for all noise rates
258"
EXPERIMENTAL RESULTS,0.6070726915520629,"with 2% improvements using a single model and 6% improvements using the ensemble of two models.
259"
EXPERIMENTAL RESULTS,0.6090373280943026,"The improvement is substantial compared with previous SOTA FaMUS [48] and DivideMix [22]. In
260"
EXPERIMENTAL RESULTS,0.6110019646365422,"Tab.4(right), our single-model result on Animal-10N achieves 1% improvement with respect to the
261"
EXPERIMENTAL RESULTS,0.6129666011787819,"single-model SELFIE [35]. Considering our approach with an ensemble of two models, we achieve a
262"
EXPERIMENTAL RESULTS,0.6149312377210217,"1% improvement over the SOTA Nested+Co-teaching [5]. Our ensemble-model result on Clothing1M
263"
EXPERIMENTAL RESULTS,0.6168958742632613,"in Tab.5 shows a competitive performance of 74.4%, which is 2% better than the previous SOTA
264"
EXPERIMENTAL RESULTS,0.618860510805501,"generative model CausalNL [50].
265"
ANALYSIS,0.6208251473477406,"4.4
Analysis
266"
ANALYSIS,0.6227897838899804,"Ablation The ablation analysis of our method is shown in Tab.6 with the IDN problems on CIFAR10.
267"
ANALYSIS,0.6247544204322201,"First row (LCE) shows the results of the training with a cross-entropy loss using the training samples
268"
ANALYSIS,0.6267190569744597,"and labels in D. The second row (LCE + LCE_P RI +LKL) shows the result of our method, replacing
269"
ANALYSIS,0.6286836935166994,"the KL divergence in LP RI as defined in (14), by a soft version of cross entropy loss. Next, the
270"
ANALYSIS,0.630648330058939,"third row (LCE + LP RI + LKL) shows our method with the loss defined in (16). As mentioned in
271"
ANALYSIS,0.6326129666011788,"Sec. 4.2, these two forms provides similar solution where the model and implicit posterior are close
272"
ANALYSIS,0.6345776031434185,"and LP RI reverse generally performs better. In the fourth row (LCE + LP RI reversed) by optimising
273"
ANALYSIS,0.6365422396856582,"the lower bound to Eq(y|x)[log p(x|y)] and finally the last row by optimising the whole objective
274"
ANALYSIS,0.6385068762278978,"function from (16) in the last row (LCE + LP RI reversed + LKL (Ours)). In general, notice that the
275"
ANALYSIS,0.6404715127701375,"reversed LP RI improves the results; the KL divergence in LP RI works better than the CE loss; and
276"
ANALYSIS,0.6424361493123772,"the optimisation of the whole loss in (16) is better than optimising the lower bound, which justifies
277"
ANALYSIS,0.6444007858546169,"the inclusion of LKL(.) in the loss.
278"
ANALYSIS,0.6463654223968566,"Coverage and uncertainty visualisation We visualise coverage and uncertainty from Eq. (9) at each
279"
ANALYSIS,0.6483300589390962,"training epoch for IDN CIFAR10/100 and CIFAR10N setups. In all cases, label coverage increases as
280"
ANALYSIS,0.650294695481336,"training progresses, indicating that our prior tends to always cover the clean label. In fact, coverage
281"
ANALYSIS,0.6522593320235757,"reaches nearly 100% for CIFAR10 at 20% IDN and 97% for 50% IDN. Furthermore, for CIFAR100
282"
ANALYSIS,0.6542239685658153,"at 50% IDN, we achieve 82% coverage, and for CIFAR10N ""worse"", we reach 92% coverage. In
283"
ANALYSIS,0.656188605108055,"terms of uncertainty, we notice a steady reduction as training progresses for all problems, where the
284"
ANALYSIS,0.6581532416502947,"uncertainty values tend to be slightly higher for the problems with higher noise rates and more classes.
285"
ANALYSIS,0.6601178781925344,"For instance, uncertainty is between 2 and 3 for the for CIFAR10’s IDN benchmarks, increasing to be
286"
ANALYSIS,0.6620825147347741,"Method
CIFAR10
20%
30%
40%
50%
LCE
86.93
82.42
76.68
58.93
LCE + LCE_P RI +LKL
85.96
82.74
78.34
73.72
LCE + LP RI + LKL
91.36
90.88
90.25
88.77
LCE + LP RI reversed
92.40
90.23
87.75
80.46
LCE + LP RI reversed + LKL (Ours)
92.65
91.96
91.02
89.94
Table 6: Ablation analysis of our proposed method. Please see text for details."
ANALYSIS,0.6640471512770137,"0
20
40
60
80
100
Epoch 0 20 40 60 80 100"
ANALYSIS,0.6660117878192534,Coverage 0 2 4 6 8 10
ANALYSIS,0.6679764243614931,Uncertainty
COV,0.6699410609037328,"0.2 Cov
0.5 Cov
0.2 Unc
0.5 Unc"
COV,0.6719056974459725,(a) CIFAR10-IDN
COV,0.6738703339882122,"0
20
40
60
80
100
Epoch 0 20 40 60 80 100"
COV,0.6758349705304518,Coverage 0 20 40 60 80 100
COV,0.6777996070726916,Uncertainty
COV,0.6797642436149313,"0.2 Cov
0.5 Cov
0.2 Unc
0.5 Unc"
COV,0.6817288801571709,(b) CIFAR100-IDN
COV,0.6836935166994106,"0
10
20
30
40
50
60
70
80
Epoch 0 20 40 60 80 100"
COV,0.6856581532416502,Coverage 0 2 4 6 8 10
COV,0.68762278978389,Uncertainty
COV,0.6895874263261297,"Aggre Cov
Worse Cov
Aggre Unc
Worse Unc"
COV,0.6915520628683693,(c) CIFAR10N
COV,0.693516699410609,"Figure 3: Coverage (Cov) and uncertainty (Unc) for (a) CIFAR10-IDN (20% and 50%), (b)
CIFAR100-IDN (20% and 50%), and (c) CIFAR10N (""Worse"" and ""Aggre""). Y-axis shows coverage
(left) and uncertainty (right). The dotted vertical line indicates the end of warmup training."
COV,0.6954813359528488,"CE
DivideMix [22]
CausalNL [50]
InstanceGM [11]
Ours
CIFAR
2.1h
7.1h
3.3h
30.5h
2.3h
Clothing1M
4h
14h
10h
43h
4.5h
Table 7: Running times of various methods on CIFAR100 with 50% IDN and Clothing1M using the
hardware listed in Sec. 4.2."
COV,0.6974459724950884,"between 2 and 4 for CIFAR10N. For CIFAR100’s IDN benchmarks, uncertainty is between 20 and
287"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.6994106090373281,"30. These results suggest that our prior clean label distribution is effective at selecting the correct
288"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7013752455795678,"clean label while reducing the number of label candidates during training.
289"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7033398821218074,"Training time comparison One of the advantages of our approach is its efficient training algorithm,
290"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7053045186640472,"particularly when compared with other generative and discriminative methods. Tab. 7 shows the
291"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7072691552062869,"training time for competing approaches on CIFAR100 with 50% IDN and Clothing1M using the
292"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7092337917485265,"hardware specified in Sec. 4.2 . In general, our method has a smaller training time than competing
293"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7111984282907662,"approaches, being 1.4× faster than CausalNL [50], 3× faster than DivideMix [22], and and 13×
294"
THESE RESULTS SUGGEST THAT OUR PRIOR CLEAN LABEL DISTRIBUTION IS EFFECTIVE AT SELECTING THE CORRECT,0.7131630648330058,"faster than InstanceGM [11].
295"
CONCLUSION,0.7151277013752456,"5
Conclusion
296"
CONCLUSION,0.7170923379174853,"In this paper, we presented a new learning algorithm to optimise a generative model represented by
297"
CONCLUSION,0.7190569744597249,"p(X|Y ) that directly associates data and clean labels instead of maximising the joint data likelihood,
298"
CONCLUSION,0.7210216110019646,"denoted by p(X, ˜Y ). Our optimisation implicitly estimates p(X|Y ) with the discriminative model
299"
CONCLUSION,0.7229862475442044,"q(Y |X) eliminating the inefficient generative model training. Furthermore, we introduce an informa-
300"
CONCLUSION,0.724950884086444,"tive label prior for maintaining high coverage of latent clean label and regularise noisy label training.
301"
CONCLUSION,0.7269155206286837,"Results on synthetic and real-world noisy-label benchmarks show that our generative method has
302"
CONCLUSION,0.7288801571709234,"SOTA results, but with complexity comparable to discriminative models.
303"
CONCLUSION,0.730844793713163,"A limitation of the proposed method that needs further exploration is a comprehensive study of the
304"
CONCLUSION,0.7328094302554028,"model for q(Y |X). In fact, the competitive results shown in this paper are obtained from fairly
305"
CONCLUSION,0.7347740667976425,"standard models for q(Y |X) without exploring sophisticated noisy-label learning techniques. In the
306"
CONCLUSION,0.7367387033398821,"future, we will use more powerful models for q(Y |X). Another issue of our model is the difficulty to
307"
CONCLUSION,0.7387033398821218,"estimate p(X|Y ) in real-world datasets containing images of high resolution. We will study more
308"
CONCLUSION,0.7406679764243614,"adequate ways to approximate p(X|Y ) in such scenario using data augmentation strategies to increase
309"
CONCLUSION,0.7426326129666012,"the scale of the dataset.
310"
REFERENCES,0.7445972495088409,"References
311"
REFERENCES,0.7465618860510805,"[1] E. Arazo, D. Ortego, P. Albert, N. O’Connor, and K. McGuinness. Unsupervised label noise modeling and
312"
REFERENCES,0.7485265225933202,"loss correction. In International conference on machine learning, pages 312–321. PMLR, 2019.
313"
REFERENCES,0.75049115913556,"[2] D. Arpit, S. Jastrz˛ebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer,
314"
REFERENCES,0.7524557956777996,"A. Courville, Y. Bengio, et al. A closer look at memorization in deep networks. In Proceedings of the 34th
315"
REFERENCES,0.7544204322200393,"International Conference on Machine Learning-Volume 70, pages 233–242. JMLR. org, 2017.
316"
REFERENCES,0.756385068762279,"[3] H. Bae, S. Shin, B. Na, J. Jang, K. Song, and I.-C. Moon. From noisy prediction to true label: Noisy
317"
REFERENCES,0.7583497053045186,"prediction calibration via generative model, 2022.
318"
REFERENCES,0.7603143418467584,"[4] P. Chen, B. B. Liao, G. Chen, and S. Zhang. Understanding and utilizing deep neural networks trained
319"
REFERENCES,0.762278978388998,"with noisy labels. In International Conference on Machine Learning, pages 1062–1070. PMLR, 2019.
320"
REFERENCES,0.7642436149312377,"[5] Y. Chen and et al. Boosting co-teaching with compression regularization for label noise. In CVPR, pages
321"
REFERENCES,0.7662082514734774,"2688–2692, 2021.
322"
REFERENCES,0.768172888015717,"[6] D. Cheng and et al. Instance-dependent label-noise learning with manifold-regularized transition matrix
323"
REFERENCES,0.7701375245579568,"estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
324"
REFERENCES,0.7721021611001965,"pages 16630–16639, 2022.
325"
REFERENCES,0.7740667976424361,"[7] H. Cheng, Z. Zhu, X. Li, Y. Gong, X. Sun, and Y. Liu. Learning with instance-dependent label noise: A
326"
REFERENCES,0.7760314341846758,"sample sieve approach. In International Conference on Learning Representations, 2021.
327"
REFERENCES,0.7779960707269156,"[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em
328"
REFERENCES,0.7799607072691552,"algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–22, 1977.
329"
REFERENCES,0.7819253438113949,"[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
330"
REFERENCES,0.7838899803536346,"database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee,
331"
REFERENCES,0.7858546168958742,"2009.
332"
REFERENCES,0.787819253438114,"[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
333"
REFERENCES,0.7897838899803536,"for language understanding. arXiv preprint arXiv:1810.04805, 2018.
334"
REFERENCES,0.7917485265225933,"[11] A. Garg, C. Nguyen, R. Felix, T.-T. Do, and G. Carneiro. Instance-dependent noisy label learning via
335"
REFERENCES,0.793713163064833,"graphical modelling. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
336"
REFERENCES,0.7956777996070727,"Vision, pages 2288–2298, 2023.
337"
REFERENCES,0.7976424361493124,"[12] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust training of
338"
REFERENCES,0.7996070726915521,"deep neural networks with extremely noisy labels. In Advances in neural information processing systems,
339"
REFERENCES,0.8015717092337917,"pages 8527–8537, 2018.
340"
REFERENCES,0.8035363457760314,"[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learningfor image recognition. ComputerScience,
341"
REFERENCES,0.8055009823182712,"2015.
342"
REFERENCES,0.8074656188605108,"[14] L. Jiang, D. Huang, M. Liu, and W. Yang. Beyond synthetic noise: Deep learning on controlled noisy
343"
REFERENCES,0.8094302554027505,"labels. In International Conference on Machine Learning, pages 4804–4815. PMLR, 2020.
344"
REFERENCES,0.8113948919449901,"[15] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for
345"
REFERENCES,0.8133595284872298,"very deep neural networks on corrupted labels. In International Conference on Machine Learning, pages
346"
REFERENCES,0.8153241650294696,"2304–2313. PMLR, 2018.
347"
REFERENCES,0.8172888015717092,"[16] T. Kaiser, L. Ehmann, C. Reinders, and B. Rosenhahn. Blind knowledge distillation for robust image
348"
REFERENCES,0.8192534381139489,"classification. arXiv preprint arXiv:2211.11355, 2022.
349"
REFERENCES,0.8212180746561886,"[17] T. Kim, J. Ko, J. Choi, S.-Y. Yun, et al. Fine samples for learning with noisy labels. Advances in Neural
350"
REFERENCES,0.8231827111984283,"Information Processing Systems, 34:24137–24149, 2021.
351"
REFERENCES,0.825147347740668,"[18] D. P. Kingma. Variational inference & deep learning: A new synthesis. 2017.
352"
REFERENCES,0.8271119842829077,"[19] J. M. Köhler, M. Autenrieth, and W. H. Beluch. Uncertainty based detection and relabeling of noisy image
353"
REFERENCES,0.8290766208251473,"labels. In CVPR Workshops, pages 33–37, 2019.
354"
REFERENCES,0.831041257367387,"[20] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
355"
REFERENCES,0.8330058939096268,"[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
356"
REFERENCES,0.8349705304518664,"networks. Communications of the ACM, 60(6):84–90, 2017.
357"
REFERENCES,0.8369351669941061,"[22] J. Li and et al. Dividemix: Learning with noisy labels as semi-supervised learning. ICLR, 2020.
358"
REFERENCES,0.8388998035363457,"[23] X. Li, T. Liu, B. Han, G. Niu, and M. Sugiyama. Provably end-to-end label-noise learning without anchor
359"
REFERENCES,0.8408644400785854,"points. arXiv preprint arXiv:2102.02400, 2021.
360"
REFERENCES,0.8428290766208252,"[24] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak,
361"
REFERENCES,0.8447937131630648,"B. Van Ginneken, and C. I. Sánchez. A survey on deep learning in medical image analysis. Medical image
362"
REFERENCES,0.8467583497053045,"analysis, 42:60–88, 2017.
363"
REFERENCES,0.8487229862475442,"[25] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda. Early-learning regularization prevents
364"
REFERENCES,0.8506876227897839,"memorization of noisy labels, 2020.
365"
REFERENCES,0.8526522593320236,"[26] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda. Early-learning regularization prevents
366"
REFERENCES,0.8546168958742633,"memorization of noisy labels. Advances in neural information processing systems, 33:20331–20342, 2020.
367"
REFERENCES,0.8565815324165029,"[27] Y. Liu, H. Cheng, and K. Zhang.
Identifiability of label noise transition matrix.
arXiv preprint
368"
REFERENCES,0.8585461689587426,"arXiv:2202.02016, 2022.
369"
REFERENCES,0.8605108055009824,"[28] M. Lukasik, S. Bhojanapalli, A. K. Menon, and S. Kumar. Does label smoothing mitigate label noise? In
370"
REFERENCES,0.862475442043222,"International Conference on Machine Learning, 2020.
371"
REFERENCES,0.8644400785854617,"[29] J. Lv, M. Xu, L. Feng, G. Niu, X. Geng, and M. Sugiyama. Progressive identification of true labels for
372"
REFERENCES,0.8664047151277013,"partial-label learning. In Proceedings of the 37th International Conference on Machine Learning, ICML
373"
REFERENCES,0.8683693516699411,"2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages
374"
REFERENCES,0.8703339882121808,"6500–6510. PMLR, 2020.
375"
REFERENCES,0.8722986247544204,"[30] E. Malach and S. Shalev-Shwartz. Decoupling"" when to update"" from"" how to update"". Advances in neural
376"
REFERENCES,0.8742632612966601,"information processing systems, 30, 2017.
377"
REFERENCES,0.8762278978388998,"[31] D. Ortego, E. Arazo, P. Albert, N. E. O’Connor, and K. McGuinness. Multi-objective interpolation training
378"
REFERENCES,0.8781925343811395,"for robustness to label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
379"
REFERENCES,0.8801571709233792,"Recognition, pages 6606–6615, 2021.
380"
REFERENCES,0.8821218074656189,"[32] G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, and L. Qu. Making deep neural networks robust to label
381"
REFERENCES,0.8840864440078585,"noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision and pattern
382"
REFERENCES,0.8860510805500982,"recognition, pages 1944–1952, 2017.
383"
REFERENCES,0.888015717092338,"[33] E. Rolf, N. Malkin, A. Graikos, A. Jojic, C. Robinson, and N. Jojic. Resolving label uncertainty with
384"
REFERENCES,0.8899803536345776,"implicit posterior models. arXiv preprint arXiv:2202.14000, 2022.
385"
REFERENCES,0.8919449901768173,"[34] Y. Shen and S. Sanghavi. Learning with bad training data via iterative trimmed loss minimization. In
386"
REFERENCES,0.8939096267190569,"International Conference on Machine Learning, pages 5739–5748. PMLR, 2019.
387"
REFERENCES,0.8958742632612967,"[35] H. Song, M. Kim, and J.-G. Lee. Selfie: Refurbishing unclean samples for robust deep learning. In
388"
REFERENCES,0.8978388998035364,"International Conference on Machine Learning, pages 5907–5915. PMLR, 2019.
389"
REFERENCES,0.899803536345776,"[36] Y. Tian, X. Yu, and S. Fu. Partial label learning: Taxonomy, analysis and outlook. Neural Networks, 2023.
390"
REFERENCES,0.9017681728880157,"[37] H. Wang, R. Xiao, Y. Li, L. Feng, G. Niu, G. Chen, and J. Zhao. Pico+: Contrastive label disambiguation
391"
REFERENCES,0.9037328094302554,"for robust partial label learning, 2022.
392"
REFERENCES,0.9056974459724951,"[38] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale chest x-ray
393"
REFERENCES,0.9076620825147348,"database and benchmarks on weakly-supervised classification and localization of common thorax diseases.
394"
REFERENCES,0.9096267190569745,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2097–2106,
395"
REFERENCES,0.9115913555992141,"2017.
396"
REFERENCES,0.9135559921414538,"[39] Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, and S.-T. Xia. Iterative learning with open-set
397"
REFERENCES,0.9155206286836935,"noisy labels. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
398"
REFERENCES,0.9174852652259332,"8688–8696, 2018.
399"
REFERENCES,0.9194499017681729,"[40] H. Wei, L. Feng, X. Chen, and B. An. Combating noisy labels by agreement: A joint training method
400"
REFERENCES,0.9214145383104125,"with co-regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
401"
REFERENCES,0.9233791748526523,"Recognition, pages 13726–13735, 2020.
402"
REFERENCES,0.925343811394892,"[41] J. Wei, H. Liu, T. Liu, G. Niu, and Y. Liu. Understanding (generalized) label smoothing whenlearning with
403"
REFERENCES,0.9273084479371316,"noisy labels. arXiv preprint arXiv:2106.04149, 2021.
404"
REFERENCES,0.9292730844793713,"[42] J. Wei and Y. Liu. When optimizing f-divergence is robust with label noise. In International Conference
405"
REFERENCES,0.931237721021611,"on Learning Representation, 2021.
406"
REFERENCES,0.9332023575638507,"[43] J. Wei, Z. Zhu, H. Cheng, T. Liu, G. Niu, and Y. Liu. Learning with noisy labels revisited: A study using
407"
REFERENCES,0.9351669941060904,"real-world human annotations. In The Tenth International Conference on Learning Representations, ICLR
408"
REFERENCES,0.93713163064833,"2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
409"
REFERENCES,0.9390962671905697,"[44] X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao, and M. Sugiyama. Part-dependent
410"
REFERENCES,0.9410609037328095,"label noise: Towards instance-dependent label noise. Advances in Neural Information Processing Systems,
411"
REFERENCES,0.9430255402750491,"33:7597–7610, 2020.
412"
REFERENCES,0.9449901768172888,"[45] X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama. Are anchor points really indispensable
413"
REFERENCES,0.9469548133595285,"in label-noise learning? Advances in Neural Information Processing Systems, 32, 2019.
414"
REFERENCES,0.9489194499017681,"[46] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image
415"
REFERENCES,0.9508840864440079,"classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
416"
REFERENCES,0.9528487229862476,"2691–2699, 2015.
417"
REFERENCES,0.9548133595284872,"[47] Y. Xu, P. Cao, Y. Kong, and Y. Wang. L_dmi: A novel information-theoretic loss function for training deep
418"
REFERENCES,0.9567779960707269,"nets robust to label noise. In Advances in Neural Information Processing Systems, pages 6225–6236, 2019.
419"
REFERENCES,0.9587426326129665,"[48] Y. Xu, L. Zhu, L. Jiang, and Y. Yang. Faster meta update strategy for noise-robust deep learning. In CVPR,
420"
REFERENCES,0.9607072691552063,"pages 144–153, 2021.
421"
REFERENCES,0.962671905697446,"[49] S. Yang, E. Yang, B. Han, Y. Liu, M. Xu, G. Niu, and T. Liu. Estimating instance-dependent bayes-label
422"
REFERENCES,0.9646365422396856,"transition matrix using a deep neural network. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári,
423"
REFERENCES,0.9666011787819253,"G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July
424"
REFERENCES,0.9685658153241651,"2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages
425"
REFERENCES,0.9705304518664047,"25302–25312. PMLR, 2022.
426"
REFERENCES,0.9724950884086444,"[50] Y. Yao, T. Liu, M. Gong, B. Han, G. Niu, and K. Zhang. Instance-dependent label-noise learning under a
427"
REFERENCES,0.9744597249508841,"structural causal model. Advances in Neural Information Processing Systems, 34:4409–4420, 2021.
428"
REFERENCES,0.9764243614931237,"[51] T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent trends in deep learning based natural language
429"
REFERENCES,0.9783889980353635,"processing. ieee Computational intelligenCe magazine, 13(3):55–75, 2018.
430"
REFERENCES,0.9803536345776032,"[52] F. Zhang, L. Feng, B. Han, T. Liu, G. Niu, T. Qin, and M. Sugiyama. Exploiting class activation value for
431"
REFERENCES,0.9823182711198428,"partial-label learning. In The Tenth International Conference on Learning Representations, ICLR 2022,
432"
REFERENCES,0.9842829076620825,"Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
433"
REFERENCES,0.9862475442043221,"[53] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv
434"
REFERENCES,0.9882121807465619,"preprint arXiv:1710.09412, 2017.
435"
REFERENCES,0.9901768172888016,"[54] Y. Zhang, S. Zheng, P. Wu, M. Goswami, and C. Chen. Learning with feature-dependent label noise: A
436"
REFERENCES,0.9921414538310412,"progressive approach. arXiv preprint arXiv:2103.07756, 2021.
437"
REFERENCES,0.9941060903732809,"[55] Z. Zhu, T. Liu, and Y. Liu. A second-order approach to learning with instance-dependent label noise. In
438"
REFERENCES,0.9960707269155207,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10113–
439"
REFERENCES,0.9980353634577603,"10123, 2021.
440"
