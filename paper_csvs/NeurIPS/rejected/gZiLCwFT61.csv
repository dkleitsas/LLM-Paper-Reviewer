Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013986013986013986,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to
1"
ABSTRACT,0.002797202797202797,"coordinate their behaviors in complex environments. However, common MARL
2"
ABSTRACT,0.004195804195804196,"algorithms still suffer from scalability and sparse reward issues. One promising
3"
ABSTRACT,0.005594405594405594,"approach to resolving them is automatic curriculum learning (ACL). ACL involves
4"
ABSTRACT,0.006993006993006993,"a student (curriculum learner) training on tasks of increasing difficulty controlled
5"
ABSTRACT,0.008391608391608392,"by a teacher (curriculum generator). Despite its success, ACL‚Äôs applicability is
6"
ABSTRACT,0.009790209790209791,"limited by (1) the lack of a general student framework for dealing with the varying
7"
ABSTRACT,0.011188811188811189,"number of agents across tasks and the sparse reward problem, and (2) the non-
8"
ABSTRACT,0.012587412587412588,"stationarity of the teacher‚Äôs task due to ever-changing student strategies. As a
9"
ABSTRACT,0.013986013986013986,"remedy for ACL, we introduce a novel automatic curriculum learning framework,
10"
ABSTRACT,0.015384615384615385,"Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-
11"
ABSTRACT,0.016783216783216783,"agent coordination. Specifically, we endow the student with population-invariant
12"
ABSTRACT,0.01818181818181818,"communication and a hierarchical skill set, allowing it to learn cooperation and
13"
ABSTRACT,0.019580419580419582,"behavior skills from distinct tasks with varying numbers of agents. In addition, we
14"
ABSTRACT,0.02097902097902098,"model the teacher as a contextual bandit conditioned by student policies, enabling a
15"
ABSTRACT,0.022377622377622378,"team of agents to change its size while still retaining previously acquired skills. We
16"
ABSTRACT,0.023776223776223775,"also analyze the inherent non-stationarity of this multi-agent automatic curriculum
17"
ABSTRACT,0.025174825174825177,"teaching problem and provide a corresponding regret bound. Empirical results
18"
ABSTRACT,0.026573426573426574,"show that our method improves the performance, scalability and sample efficiency
19"
ABSTRACT,0.027972027972027972,"in several MARL environments. The source code and the video can be found at
20"
ABSTRACT,0.02937062937062937,"https://sites.google.com/view/marl-spc/.
21"
INTRODUCTION,0.03076923076923077,"1
Introduction
22"
INTRODUCTION,0.032167832167832165,"Multi-agent reinforcement learning (MARL) has long been a go-to tool in complex robotic and
23"
INTRODUCTION,0.033566433566433566,"strategic domains [1, 2]. However, learning effective policies with sparse reward from scratch for
24"
INTRODUCTION,0.03496503496503497,"large-scale multi-agent systems remains challenging. One of the challenges is the exponential growth
25"
INTRODUCTION,0.03636363636363636,"of the joint observation-action space with an increasing number of agents. In addition, sparse reward
26"
INTRODUCTION,0.03776223776223776,"signal requires a large number of training trajectories, posing difficulties in applying existing MARL
27"
INTRODUCTION,0.039160839160839164,"algorithms directly to complex environments. As a result, these algorithms may produce agents that
28"
INTRODUCTION,0.04055944055944056,"do not collaborate with each other, even when it would be of significant benefit [3, 4].
29"
INTRODUCTION,0.04195804195804196,"There are several lines of research related to the large-scale MARL problem with sparse reward,
30"
INTRODUCTION,0.043356643356643354,"including reward shaping [5], curriculum learning [6], and learning from demonstrations [7]. Among
31"
INTRODUCTION,0.044755244755244755,"these approaches, the curriculum learning paradigm, in which the difficulty of experienced tasks
32"
INTRODUCTION,0.046153846153846156,"and the population of training agents progressively grow, shows particular promise. In automatic
33"
INTRODUCTION,0.04755244755244755,"curriculum learning (ACL), a teacher (curriculum generator) learns to adjust the complexity and
34"
INTRODUCTION,0.04895104895104895,"sequencing of tasks faced by a student (curriculum learner). Several works have even proposed multi-
35"
INTRODUCTION,0.05034965034965035,"agent ACL algorithms, based on approximate or heuristic approaches to teaching, such as DyMA-CL
36"
INTRODUCTION,0.05174825174825175,"[8], EPC [9], and VACL [6]. However, these approaches rely on a framework of an off-policy student
37"
INTRODUCTION,0.05314685314685315,"with a replay buffer that is hard to decide the size of the replay buffer since the proportion of different
38"
INTRODUCTION,0.05454545454545454,"tasks matters. Also, they make a strong assumption that the value of the learned policy does not
39"
INTRODUCTION,0.055944055944055944,"change when agents switch to a different task. For example, In the football environment, when we
40"
INTRODUCTION,0.057342657342657345,"treat the score as the reward, the same state-action pairs of the team agents in different tasks might
41"
INTRODUCTION,0.05874125874125874,"lead to different returns. 3 learned agents could get more scores in a 3v1 match, while the same
42"
INTRODUCTION,0.06013986013986014,"three agents could get fewer scores in a 4v11 match with an unlearned random teammate. When
43"
INTRODUCTION,0.06153846153846154,"decomposing at the same state-action pairs, agents get different credit assignments. Moreover, the
44"
INTRODUCTION,0.06293706293706294,"teacher in these approaches still faces a non-stationarity problem due to the ever-changing student
45"
INTRODUCTION,0.06433566433566433,"strategies. Another class of larger-scale MARL solutions is hierarchical learning, which utilizes
46"
INTRODUCTION,0.06573426573426573,"temporal abstraction to decompose a task into a hierarchy of subtasks. This includes skill discovery
47"
INTRODUCTION,0.06713286713286713,"[10], option as response [11], role-based MARL [12], and two levels of abstraction [13]. However,
48"
INTRODUCTION,0.06853146853146853,"these approaches mostly focus on one specific task with a fixed number of agents and do not consider
49"
INTRODUCTION,0.06993006993006994,"the transferability of learned skills. In this paper, we provide our insight into this question:
50"
INTRODUCTION,0.07132867132867132,"Whether an elaborate combination of principles from ACL and hierarchical learning can enable
51"
INTRODUCTION,0.07272727272727272,"complex cooperation with sparse reward in MARL?
52"
INTRODUCTION,0.07412587412587412,"Specifically, we present a novel automatic curriculum learning algorithm, Skilled Population Curricu-
53"
INTRODUCTION,0.07552447552447553,"lum (SPC), that addresses the challenges of learning effective policies for large-scale multi-agent
54"
INTRODUCTION,0.07692307692307693,"systems with sparse reward. The core idea behind SPC, motivated by real-world team sports where
55"
INTRODUCTION,0.07832167832167833,"players often train their skills by gradually increasing the difficulty of tasks and the number of
56"
INTRODUCTION,0.07972027972027972,"coordinating players, is to encourage the student to learn skills from tasks with different numbers of
57"
INTRODUCTION,0.08111888111888112,"agents, akin to how team sports players train by gradually increasing the difficulty of tasks and the
58"
INTRODUCTION,0.08251748251748252,"number of coordinating players. To achieve this, SPC is implemented with three key components.
59"
INTRODUCTION,0.08391608391608392,"First, to solve the final complex cooperative tasks, we equip the contextual bandit teacher with an
60"
INTRODUCTION,0.08531468531468532,"RNN-based [14] imitation model to represent student policies and generate the bandit‚Äôs context.
61"
INTRODUCTION,0.08671328671328671,"Second, to handle the varying number of agents across these tasks and bypass the limitation of the
62"
INTRODUCTION,0.08811188811188811,"related studies, we utilize population-invariant communication in the student module is implemented
63"
INTRODUCTION,0.08951048951048951,"to handle varying number of agents across tasks. By treating each agent‚Äôs message as a word and
64"
INTRODUCTION,0.09090909090909091,"using a self-attention communication channel [15], SPC supports an arbitrary number of agents to
65"
INTRODUCTION,0.09230769230769231,"share messages. Third, to learn transferable skills in the sparse reward setting, a hierarchical skill
66"
INTRODUCTION,0.0937062937062937,"framework is used in the student module to learn transferable skills in the sparse reward setting,
67"
INTRODUCTION,0.0951048951048951,"where agents communicate on the high-level about a set of shared low-level policies. Empirical
68"
INTRODUCTION,0.0965034965034965,"results show that our method achieves state-of-the-art performance in several tasks in Multi-agent
69"
INTRODUCTION,0.0979020979020979,"Particle Environment (MPE) [16] and the challenging 5vs5 competition in Google Research Football
70"
INTRODUCTION,0.0993006993006993,"(GRF) [17].
71"
PRELIMINARIES,0.1006993006993007,"2
Preliminaries
72"
PRELIMINARIES,0.1020979020979021,"Dec-POMDP. A cooperative MARL problem can be formulated as a decentralized par-
73"
PRELIMINARIES,0.1034965034965035,"tially observable Markov decision process (Dec-POMDP) [18], which is described as a tuple
74"
PRELIMINARIES,0.1048951048951049,"‚ü®n, S, A, P, R, O, ‚Ñ¶, Œ≥‚ü©, where n represents the number of agents. S represents the space of global
75"
PRELIMINARIES,0.1062937062937063,"states. A = {Ai}i=1,¬∑¬∑¬∑ ,n denotes the space of actions of all agents. O = {Oi}i=1,¬∑¬∑¬∑ ,n denotes
76"
PRELIMINARIES,0.1076923076923077,"the space of observations of all agents. P : S √ó A ‚ÜíS denotes the state transition probability
77"
PRELIMINARIES,0.10909090909090909,"function. All agents share the same reward as a function of the states and actions of the agents
78"
PRELIMINARIES,0.11048951048951049,"R : S √ó A ‚ÜíR. Each agent i receives a private observation oi ‚ààOi according to the observation
79"
PRELIMINARIES,0.11188811188811189,"function ‚Ñ¶(s, i) : S ‚ÜíOi. Œ≥ ‚àà[0, 1] denotes the discount factor.
80"
PRELIMINARIES,0.11328671328671329,"Multi-armed Bandit. Multi-armed bandits (MABs) are a simple but very powerful framework that
81"
PRELIMINARIES,0.11468531468531469,"repeatedly makes decisions under uncertainty. In this framework, a learner performs a sequence
82"
PRELIMINARIES,0.11608391608391608,"of actions and immediately observes the corresponding reward after each action. The goal is to
83"
PRELIMINARIES,0.11748251748251748,"maximize the total reward over a given set of K actions and a specific time horizon T. The measure
84"
PRELIMINARIES,0.11888111888111888,"of success in MABs is often determined by the regret, which is the difference between the cumulative
85"
PRELIMINARIES,0.12027972027972028,"reward of an MAB algorithm and the best-arm benchmark. One well-known MAB algorithm is the
86"
PRELIMINARIES,0.12167832167832168,"Exp3 algorithm [19], which aims to increase the probability of selecting good arms and achieves a
87"
PRELIMINARIES,0.12307692307692308,"regret of O(
p"
PRELIMINARIES,0.12447552447552447,"KT log(K)) under a time-varying reward distribution. Another related concept is the
88"
PRELIMINARIES,0.1258741258741259,"contextual bandit problem [20], where the learner makes decisions based on prior information as the
89"
PRELIMINARIES,0.12727272727272726,"context.
90"
SKILLED POPULATION CURRICULUM,0.12867132867132866,"3
Skilled Population Curriculum
91"
SKILLED POPULATION CURRICULUM,0.13006993006993006,"In this section, we first provide a formal definition of the curriculum-enhanced Dec-POMDP frame-
92"
SKILLED POPULATION CURRICULUM,0.13146853146853146,"work, which formulates the MARL with curriculum problem under the Dec-POMDP framework.
93"
SKILLED POPULATION CURRICULUM,0.13286713286713286,"We then present our multi-agent ACL algorithm, Skilled Population Curriculum (SPC), as shown in
94"
SKILLED POPULATION CURRICULUM,0.13426573426573427,"Fig. 1. In the following subsections, we establish the curriculum learning framework in Sec. 3.1, and
95"
SKILLED POPULATION CURRICULUM,0.13566433566433567,"then present a contextual multi-armed bandit algorithm as the teacher to address the non-stationarity
96"
SKILLED POPULATION CURRICULUM,0.13706293706293707,"in Sec. 3.2. Lastly, we introduce the student with transferable skills and population-invariant commu-
97"
SKILLED POPULATION CURRICULUM,0.13846153846153847,"nication to tackle the varying number of agents and the sparse reward problem in Sec. 3.3.
98"
PROBLEM FORMULATION,0.13986013986013987,"3.1
Problem Formulation
99"
PROBLEM FORMULATION,0.14125874125874127,"We consider environments from multi-agent automatic curriculum learning problems are equipped
100"
PROBLEM FORMULATION,0.14265734265734265,"with parameterized task spaces and thus can be modeled as curriculum-enhanced Dec-POMDPs.
101"
PROBLEM FORMULATION,0.14405594405594405,"Definition 3.1 (Curriculum-enhanced Dec-POMDP). A curriculum-enhanced Dec-POMDP is defined
102"
PROBLEM FORMULATION,0.14545454545454545,"by a tuple ‚ü®Œ¶, M‚ü©, where Œ¶ and M represent a task space and a Dec-POMDP, respectively. Given the
103"
PROBLEM FORMULATION,0.14685314685314685,"task œï, the Dec-POMDP M(œï) is presented as

nœï, Sœï, Aœï, P œï, rœï, Oœï, ‚Ñ¶œï, Œ≥œï	
. The superscript
104"
PROBLEM FORMULATION,0.14825174825174825,"œï denotes that the Dec-POMDP elements are determined by the task œï. Note that task œï can be
105"
PROBLEM FORMULATION,0.14965034965034965,"a few parameters of the environment or task IDs in a finite task space. In a curriculum-enhanced
106"
PROBLEM FORMULATION,0.15104895104895105,"Dec-POMDP, the objective is to improve the student‚Äôs performance on the target tasks through the
107"
PROBLEM FORMULATION,0.15244755244755245,"sequence of training tasks given by the teacher..
108"
PROBLEM FORMULATION,0.15384615384615385,"Let œÑ denote a trajectory whose unconditional distribution PrœÄ,œï
¬µ (œÑ) (under a policy œÄ and a task œï
109"
PROBLEM FORMULATION,0.15524475524475526,"with initial state distribution ¬µ(s0)) is PrœÄ,œï
¬µ (œÑ) = ¬µ (s0) P‚àû
t=0 œÄ (at | st) P œï (st+1 | st, at). We use
110"
PROBLEM FORMULATION,0.15664335664335666,"p(œï) to represent the distribution of target tasks and q(œï) to represent the distribution of training tasks
111"
PROBLEM FORMULATION,0.15804195804195803,"at each task sampling step. We consider the joint agents‚Äô policies œÄŒ∏(a|s) and qœà(œï) parameterized
112"
PROBLEM FORMULATION,0.15944055944055943,"by Œ∏ and œà, respectively. The overall objective to maximize in a curriculum-enhanced Dec-POMDP
113"
PROBLEM FORMULATION,0.16083916083916083,"is:
114"
PROBLEM FORMULATION,0.16223776223776223,"J(Œ∏, œà) = Eœï‚àºp(œï),œÑ‚àºPrœÄ
¬µ

Rœï(œÑ)

= Eœï‚àºqœà(œï)"
PROBLEM FORMULATION,0.16363636363636364, p(œï)
PROBLEM FORMULATION,0.16503496503496504,"qœà(œï)V (œï, œÄŒ∏)

(1)"
PROBLEM FORMULATION,0.16643356643356644,where Rœï(œÑ) = P
PROBLEM FORMULATION,0.16783216783216784,"t Œ≥trœï (st, at; s0) and V (œï, œÄŒ∏) represents the value function of œÄŒ∏ in Dec-
115"
PROBLEM FORMULATION,0.16923076923076924,"POMDP M(œï). However, when optimizing qœà(œï), we cannot get the partial derivative ‚àáœàJ(Œ∏, œà) =
116 ‚àáœà
P"
PROBLEM FORMULATION,0.17062937062937064,"œÑ
1
qœà(œï)Rœï(œÑ) PrœÄ,œï
¬µ (œÑ)1 since the reward function and the transition probability function w.r.t
117"
PROBLEM FORMULATION,0.17202797202797201,"number of agents are non-parametric, non-differentiable, and discontinuous in most MARL scenarios.
118"
PROBLEM FORMULATION,0.17342657342657342,"Thus, we use the non-differentiable method, i.e., multi-armed bandit algorithms, to optimize qœà(œï),
119"
PROBLEM FORMULATION,0.17482517482517482,"and use an RL algorithm (the student) in alternating periods to optimize œÄŒ∏(a|s). However, there are
120"
PROBLEM FORMULATION,0.17622377622377622,"three key challenges in solving this problem: (1) The teacher is facing a non-stationarity problem due
121"
PROBLEM FORMULATION,0.17762237762237762,"to the ever-changing student‚Äôs strategies. (2) The student will forget the old tasks and need to re-learn
122"
PROBLEM FORMULATION,0.17902097902097902,"them. Some tasks can be the prerequisites of other tasks, while some can be inter-independent and
123"
PROBLEM FORMULATION,0.18041958041958042,"parallel. (3) There is a lack of a general student framework to deal with the varying number of agents
124"
PROBLEM FORMULATION,0.18181818181818182,"across tasks and the sparse reward problem.
125"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.18321678321678322,"3.2
Teacher as a Non-Stationary Contextual Bandit
126"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.18461538461538463,"As previously discussed, the teacher faces a non-stationarity problem due to the ever-changing
127"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.18601398601398603,"student‚Äôs strategies during the learning process. Specifically, as the student learns across different
128"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1874125874125874,"tasks in different learning stages, the teacher will observe varying student performance when providing
129"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1888111888111888,"the same task, resulting in a time-varying reward distribution for the teacher. In addition, the student
130"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1902097902097902,"may forget previously learned policies. To mitigate this problem, the teacher should balance the
131"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1916083916083916,"exploitation of tasks that have been found to benefit the student‚Äôs performance on the target tasks,
132"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.193006993006993,"with the exploration of tasks that may not directly facilitate the student‚Äôs learning.
133"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1944055944055944,"Fortunately, we notice that the non-stationarity stems from the student, which can be mitigated with
134"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1958041958041958,"a contextual bandit which embeds the student policy into the context. As shown in Fig. 1 Left,
135"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1972027972027972,"the teacher utilizes the student‚Äôs policy representation as the context and chooses a task from the
136"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.1986013986013986,"1p(œï) is not in the partial derivative since it is a fixed distribution. ‚Ä¶
‚Ä¶ ‡∑•ùíéùíã skill ùùìùüè"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2,"‚Ä¶
Tasks Prob."
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2013986013986014,"‚Ä¶
Contexts"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.20279720279720279,Teacher Env
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2041958041958042,"ùëõùúôùëñ, ùë∫ùúôùëñ, ùë®ùúôùëñ, ùëÉùúôùëñ, ùëüùúôùëñ ùùìùíä"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2055944055944056,"ùùìùüê
ùùìùíå‚àíùüèùùìùíå"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.206993006993007,Student
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2083916083916084,"ùíê, ùíì
ùíÇ ùùÖùíâ RNN ùùÖùíç ùíê
ùíÇ"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2097902097902098,"hidden
state"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2111888111888112,"ùùÖùíâ,ùíã
ùíêùíã ùíÇùíâ,ùíã ùíéùíã"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2125874125874126,"self-att
channel ùíéùüè
ùíéùüê ùíéùíèùùìùíä ‡∑•ùíéùüè ùùÖùíçùíêùíò ùíÇùíã ‡∑•ùíéùüê ‡∑•ùíéùíèùíä ùùì"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.213986013986014,Population-invariant
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.2153846153846154,view of agent j
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.21678321678321677,ùíìùíïùíÜùíÇùíÑùíâùíÜùíì
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.21818181818181817,"Figure 1: The overall framework of SPC. It consists of three parts: configurable environments, a
teacher, and a student. Left. The teacher is modeled as a contextual multi-armed bandit. At each
teacher timestep, the teacher chooses a training task from the distribution of bandit actions. Mid. The
student is endowed with a hierarchical skill framework and population-invariant communication. It is
trained with MARL algorithms on the training tasks. The student returns not only the hidden state of
its RNN imitation model as contexts to the teacher, but also the average discounted cumulative rewards
on the testing task. Right. The student learns hierarchical policies, with the population-invariant
communication taking place at the high-level, implemented with a self-attention communication
channel to handle the messages from a varying number of agents. The agents in the student share the
same low-level policy."
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.21958041958041957,"distribution of training tasks. Specifically, we extend the Exp3 algorithm [19] by incorporating
137"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22097902097902097,"contexts through a two-step online clustering process [21]. The context, represented by x, is the
138"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22237762237762237,"student‚Äôs policy representation. The teacher‚Äôs action is a specific task, denoted by œï, and the teacher‚Äôs
139"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22377622377622378,"reward is the return of the student in the target tasks. The teacher‚Äôs algorithm is outlined in Alg. 1.
140"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22517482517482518,"During the sampling stage (steps 1-5), the teacher selects a task for the student‚Äôs training. In the
141"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22657342657342658,"training stage (steps 6-7), the teacher adjusts the parameters based on the evaluation reward received
142"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22797202797202798,"from the student.
143"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.22937062937062938,"Algorithm 1 Teacher Sampling and Training
Input: Context x, the number of Clusters Nc, Nc instances of Exp3 with task distribution
w(œïk, c) for k = 1, . . . , K and for c = 1, . . . , Nc, learning rate Œ±, a buffer maintaining the his-
torical contexts
Output: M(œï) =

nœï, Sœï, Aœï, P œï, rœï, Oœï, ‚Ñ¶œï, Œ≥œï	
, the teacher bandit parameters
Sampling
1. Get the the context x, and save it to the buffer
2. Run the online cluster algorithm and get the index of the cluster center c(x)
3. Let the active Exp3 instance be the instance with index c(x)
4. Set the probability p(œïk, c(x)) = (1‚àíŒ±)w(œïk,c(x))
PK
j=1 w(œïk,c(x)) + Œ±"
TEACHER AS A NON-STATIONARY CONTEXTUAL BANDIT,0.23076923076923078,"K for each task œïk
5. Sample a new task according to the distribution of pœïk,c
Training
6. Get the return (discounted cumulative rewards) from student testing r
7. Update the active Exp3 instance by setting w(œïk, c(x)) = w(œïk, c(x))eŒ±r/K"
CONTEXT REPRESENTATION,0.23216783216783216,"3.2.1
Context Representation
144"
CONTEXT REPRESENTATION,0.23356643356643356,"Upon analysis, it is essential to learn an effective representation for the student‚Äôs policy as the context.
145"
CONTEXT REPRESENTATION,0.23496503496503496,"One straightforward representation is to use the student parameters Œ∏ directly as the context. However,
146"
CONTEXT REPRESENTATION,0.23636363636363636,"the number of parameters is too large to be used as the input of neural network if we change the
147"
CONTEXT REPRESENTATION,0.23776223776223776,"student‚Äôs architecture. Therefore, we propose an alternative method.
148"
CONTEXT REPRESENTATION,0.23916083916083916,"A principle for learning a good representation of a policy is predictive representation, which means
149"
CONTEXT REPRESENTATION,0.24055944055944056,"the representation should be accurate to predict policy actions given states. In accordance with this
150"
CONTEXT REPRESENTATION,0.24195804195804196,"principle, we utilize an imitation function through supervised learning. Supervised learning does
151"
CONTEXT REPRESENTATION,0.24335664335664337,"not require direct access to reward signals, making it an attractive approach for reward-agnostic
152"
CONTEXT REPRESENTATION,0.24475524475524477,"representation learning. Intuitively, the imitation function attempts to mimic low-level policy based
153"
CONTEXT REPRESENTATION,0.24615384615384617,"on historical behaviors. In practice, we use an RNN-based imitation function fim : S √ó A ‚Üí[0, 1].
154"
CONTEXT REPRESENTATION,0.24755244755244754,"Since recurrent neural networks are theoretically Turing complete [22], their internal states can be
155"
CONTEXT REPRESENTATION,0.24895104895104894,"used as the representation of the student‚Äôs policy. We train this imitation function by using the
156"
CONTEXT REPRESENTATION,0.25034965034965034,"negative cross entropy objective E[log fim (s, a)].
157"
REGRET ANALYSIS,0.2517482517482518,"3.2.2
Regret Analysis
158"
REGRET ANALYSIS,0.25314685314685315,"In this subsection, we demonstrate that the proposed teacher algorithm has a regret bound of
159"
REGRET ANALYSIS,0.2545454545454545,"E[R(T)] = O
 
T 2/3(LK log T)1/3
, where T is the number of total rounds, L is the Lipschitz
160"
REGRET ANALYSIS,0.25594405594405595,"constant, and K is the number of arms (the number of the teacher‚Äôs actions). The regret analysis
161"
REGRET ANALYSIS,0.2573426573426573,"is used to justify the usage of the bandit algorithm in the non-stationary setting. The regret bound
162"
REGRET ANALYSIS,0.25874125874125875,"represents the optimality of SPC, as the teacher‚Äôs reward is the return of the student in the target tasks.
163"
REGRET ANALYSIS,0.2601398601398601,"First, we introduce the Lipschitz assumption about the generalization ability of the task space.
164"
REGRET ANALYSIS,0.26153846153846155,"Assumption 3.2 (Lipschitz continuity w.r.t the context). Without loss of generality, the contexts are
165"
REGRET ANALYSIS,0.2629370629370629,"mapped into the [0, 1] interval, so that the expected rewards for the teacher are Lipschitz with respect
166"
REGRET ANALYSIS,0.26433566433566436,"to the context.
167"
REGRET ANALYSIS,0.26573426573426573,|r(œï | x) ‚àír (œï | x‚Ä≤)| ‚â§L ¬∑ |x ‚àíx‚Ä≤|
REGRET ANALYSIS,0.26713286713286716,"for any arm œï ‚ààŒ¶ and any pair of contexts x, x‚Ä≤ ‚ààX
(2)"
REGRET ANALYSIS,0.26853146853146853,"where L is the Lipschitz constant, and X is the context space.
168"
REGRET ANALYSIS,0.2699300699300699,"This assumption suggests that for any policy trained on a set of tasks, the rate at which performance
169"
REGRET ANALYSIS,0.27132867132867133,"improves is not faster than the rate at which the policy changes. This is a realistic assumption, as we
170"
REGRET ANALYSIS,0.2727272727272727,"cannot expect the student to achieve a significant improvement on a task with only a few training
171"
REGRET ANALYSIS,0.27412587412587414,"steps under a new context. We use an existing contextual bandit algorithm for a limited number of
172"
REGRET ANALYSIS,0.2755244755244755,"contexts [19] (see Appendix A) and Lemma 3.3 as a foundation for proving Theorem 3.4.
173"
REGRET ANALYSIS,0.27692307692307694,"Lemma 3.3. Alg. 2 has a regret bound of E[R(T)] = O(
p"
REGRET ANALYSIS,0.2783216783216783,"TK|X| log K).
174"
REGRET ANALYSIS,0.27972027972027974,"Lemma 3.3 introduces a square root dependence on |X| if separate copies of Exp3 are run for
175"
REGRET ANALYSIS,0.2811188811188811,"each context [19]. This motivates us to address the large context space by utilizing discretization
176"
REGRET ANALYSIS,0.28251748251748254,"techniques.
177"
REGRET ANALYSIS,0.2839160839160839,"Theorem 3.4. Consider the Lipschitz contextual bandit problem with contexts in [0, 1]. The Alg. 1
178"
REGRET ANALYSIS,0.2853146853146853,"yields regret E[R(T)] = O
 
T 2/3(LK ln T)1/3
.
179"
REGRET ANALYSIS,0.2867132867132867,"Proof. See Appendix B.
180"
REGRET ANALYSIS,0.2881118881118881,"In practice, the high-dimensional context space cannot be discretized using a uniform mesh in [0, 1]
181"
REGRET ANALYSIS,0.2895104895104895,"as in the proof of Theorem 3.4. To address this issue, we utilize the Balanced Iterative Reducing
182"
REGRET ANALYSIS,0.2909090909090909,"and Clustering using Hierarchies (BIRCH) online clustering algorithm [21] to discretize the context
183"
REGRET ANALYSIS,0.2923076923076923,"space. BIRCH is an efficient and easy-to-update algorithm that can effectively cluster large datasets.
184"
REGRET ANALYSIS,0.2937062937062937,"In this case, it is used to cluster the high-dimensional RNN-based policy representation. The resulting
185"
REGRET ANALYSIS,0.2951048951048951,"clusters can be seen as an approximation of a uniform mesh.
186"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.2965034965034965,"3.3
Student with Population-Invariant Skills
187"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.29790209790209793,"We propose a population-invariant skill framework to address the challenges of varying number of
188"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.2993006993006993,"agents and sparse reward problem. This framework allows agents to communicate via a self-attention
189"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3006993006993007,"channel, enabling them to learn transferable skills across different tasks. The student module is
190"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3020979020979021,"designed to be algorithm-agnostic and is orthogonal to any state-of-the-art MARL algorithm. While
191"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3034965034965035,"there have been some efforts in the literature to address the varying number of agents [23, 24], these
192"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3048951048951049,"approaches heavily rely on prior knowledge of the environments.
193"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3062937062937063,"Population-Invariant Teamwork Communication. In order to enable the population-invariant
194"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3076923076923077,"property and learn tactics among agents, we introduce communication. Leveraging the transformer
195"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3090909090909091,"architecture‚Äôs capability to process inputs of varying lengths [15], we incorporate self-attention into
196"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3104895104895105,"our communication mechanism. As illustrated in Fig. 1 Right, each agent j receives an observation
197"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3118881118881119,"oj and encodes it into a message vector mj = f (oj) which is then sent through a self-attention
198"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3132867132867133,"channel, where f is an observation encoder function.
199"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3146853146853147,"Agents
Landmark
Ball"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.31608391608391606,"Simple Spread
Push Ball"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3174825174825175,"(a)
(b)"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.31888111888111886,Figure 2: (a) Multi-agent Particle Environment. (b) Google Research Football.
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3202797202797203,"The channel aggregates all messages and sends the new message vector, Àúmj, through the self-attention
200"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.32167832167832167,"mechanism. Concretely, given the channel input M = [m1, m2, ¬∑ ¬∑ ¬∑ , mn] ‚ààRn√ódm, and the
201"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3230769230769231,"trainable weight of the channel WQ, WK, WV ‚ààRdm√ódm, we obtain three distinct representations:
202"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.32447552447552447,"Q = MWQ, K = MWK, V = MWV . Then the output messages are
203"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3258741258741259,"Àú
M = Attention(Q, K, V) = softmax
QKT ‚àödm"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.32727272727272727,"
V
(3)"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.32867132867132864,"where dm is the dimension of the messages. As the dimensions of the trainable weight are independent
204"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3300699300699301,"of the number of agents, our student models can leverage the population-invariant property to
205"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.33146853146853145,"effectively learn tactics.
206"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3328671328671329,"Transferable Hierarchical Skills. As depicted in the dotted box in Fig. 1 Right, after receiving
207"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.33426573426573425,"the new messages Àúmj from the channel, each agent employs a high-level action (skill) ah,j =
208"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3356643356643357,"œÄh,j(oj, Àúmj) to execute the low-level policy aj = œÄlow(oj, ah,j). In this work, we generalize the
209"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.33706293706293705,"high-level action (skill) ah,j to a continuous embedding space, so that the skill can be either a latent
210"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3384615384615385,"continuous vector as in DIAYN [25], or a categorical distribution for sampling discrete options [26].
211"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.33986013986013985,"Implementation. We implement the high- and low-level policies in the student with Proximal Policy
212"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.3412587412587413,"Optimization (PPO) [27]. Following the common practice proposed in [28], the high-level policy
213"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34265734265734266,"for each agent is learned independently, whereas the low-level policies share parameters, as the
214"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34405594405594403,"fundamental action pattern should be consistent among different agents. The low-level agents are
215"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34545454545454546,"rewarded by the environment, while the high-level policy is trained to take actions at fixed intervals.
216"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34685314685314683,"Within this interval, the cumulative low-level reward is used as the high-level reward. When using
217"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34825174825174826,"a categorical distribution to enable discrete skills, we sample an ‚Äúoption‚Äù from the distribution and
218"
STUDENT WITH POPULATION-INVARIANT SKILLS,0.34965034965034963,"provide the corresponding one-hot embedding to the low-level policy.
219"
RELATED WORK,0.35104895104895106,"4
Related Work
220"
RELATED WORK,0.35244755244755244,"Automatic Curriculum Learning in MARL. Curriculum learning is a training strategy that mimics
221"
RELATED WORK,0.35384615384615387,"the human learning process by organizing tasks based on their difficulty level [29]. The selection of
222"
RELATED WORK,0.35524475524475524,"tasks is formulated as a Curriculum Markov Decision Process (CMDP) [30]. Automatic Curriculum
223"
RELATED WORK,0.35664335664335667,"Learning mechanisms aim to learn a task selection function based on past interactions, such as ADR
224"
RELATED WORK,0.35804195804195804,"[31, 32], ALP-GMM [33], SPCL [34], GoalGAN [35], PLR [36, 37], SPDL [38], CURROT [39],
225"
RELATED WORK,0.3594405594405594,"and graph-curriculum [40]. Recently, several MARL curriculum learning frameworks have been
226"
RELATED WORK,0.36083916083916084,"proposed, such as open-ended evolution [41‚Äì43], population-based training [44, 45], meta-learning
227"
RELATED WORK,0.3622377622377622,"[46, 47] and training with emergent curriculum [48, 49, 29]. In summary, these frameworks share a
228"
RELATED WORK,0.36363636363636365,"common principle of an automatic curriculum that continually generates improved agents through
229"
RELATED WORK,0.365034965034965,"selection pressure among a population of self-optimizing agents.
230"
RELATED WORK,0.36643356643356645,"Hierarchical MARL and Communication. Hierarchical reinforcement learning (HRL) has been
231"
RELATED WORK,0.3678321678321678,"extensively studied to address the issue of sparse reward and facilitate transfer learning. Single-agent
232"
RELATED WORK,0.36923076923076925,"HRL focuses on learning the temporal decomposition of tasks, either by learning subgoals [50‚Äì
233"
RELATED WORK,0.3706293706293706,"54] or by discovering reusable skills [55‚Äì58]. Recent developments in hierarchical MARL have
234"
RELATED WORK,0.37202797202797205,"been discussed in Sec. 1. In multi-agent settings, communication has been effective in promoting
235"
RELATED WORK,0.3734265734265734,"cooperation among agents [59‚Äì65]. However, current approaches that extend HRL to multi-agent
236"
RELATED WORK,0.3748251748251748,"systems or utilize communication are limited to a fixed number of agents and lack the ability to
237"
RELATED WORK,0.37622377622377623,"transfer to different agent counts.
238"
EXPERIMENTS,0.3776223776223776,"5
Experiments
239"
EXPERIMENTS,0.37902097902097903,"To demonstrate the effectiveness of our approach, we conduct experiments on several tasks in two
240"
EXPERIMENTS,0.3804195804195804,"environments: Simple-Spread and Push-Ball in the Multi-agent Particle Environment (MPE) [16],
241"
EXPERIMENTS,0.38181818181818183,"and the challenging 5vs5 task of the Google Research Football (GRF) environment [17]. We aim to
242"
EXPERIMENTS,0.3832167832167832,"investigate the following research questions:
243"
EXPERIMENTS,0.38461538461538464,"Q1: Is curriculum learning necessary in complex large-scale MARL problems? (Sec. 5.2)
244"
EXPERIMENTS,0.386013986013986,"Q2: Can SPC outperform previous curriculum-based MARL methods? If so, which components of
245"
EXPERIMENTS,0.38741258741258744,"SPC contribute the most to performance gains? (Sec. 5.3)
246"
EXPERIMENTS,0.3888111888111888,"Q3: Can SPC effectively learn a curriculum for the student? (Sec. 5.4)
247"
EXPERIMENTS,0.3902097902097902,"5.1
Environments, Baselines and Metric
248"
EXPERIMENTS,0.3916083916083916,"Environments. In the GRF 5vs5 scenario, we control four agents, excluding the goalkeeper, to
249"
EXPERIMENTS,0.393006993006993,"compete against the built-in AI opponents. Each agent observes a compact encoding, consisting of a
250"
EXPERIMENTS,0.3944055944055944,"115-dimensional vector that summarizes various aspects of the game, such as player coordinates, ball
251"
EXPERIMENTS,0.3958041958041958,"possession and direction, active players, and game mode. The available action set for an individual
252"
EXPERIMENTS,0.3972027972027972,"agent includes 19 discrete actions, such as idle, move, pass, shoot, dribble, etc. The GRF provides
253"
EXPERIMENTS,0.3986013986013986,"two types of rewards: scoring and checkpoints, to encourage agents to move the ball forward and
254"
EXPERIMENTS,0.4,"make successful shots. Additionally, we include a shooting reward in the challenging GRF 5vs5
255"
EXPERIMENTS,0.4013986013986014,"task. We select several basic scenarios in GRF, including 3vs3, Pass-Shoot, 3vs1, and Empty-Goal as
256"
EXPERIMENTS,0.4027972027972028,"curriculum.
257"
EXPERIMENTS,0.4041958041958042,"In MPE, we investigate Simple-Spread and Push-Ball (see Fig. 2a). In Simple-Spread, there are n
258"
EXPERIMENTS,0.40559440559440557,"agents that need to cover all n landmarks. Agents are penalized for collisions and only receive a
259"
EXPERIMENTS,0.406993006993007,"positive reward when all the landmarks are covered. In Push-Ball, there are n agents, n balls, and n
260"
EXPERIMENTS,0.4083916083916084,"landmarks. The agents must push the balls to cover each landmark. A success reward is given after
261"
EXPERIMENTS,0.4097902097902098,"all the landmarks have been covered.
262"
EXPERIMENTS,0.4111888111888112,"Baselines. We compare our approach to the following methods in Table 1 as baselines2:
263"
EXPERIMENTS,0.4125874125874126,Table 1: Baseline algorithms.
EXPERIMENTS,0.413986013986014,"Categories
Methods"
EXPERIMENTS,0.4153846153846154,"MARL
(Q1)
QMIX [68]
IPPO [69]"
EXPERIMENTS,0.4167832167832168,"Curriculum-based
(Q2)
IPPO with uniform task sampling
VACL [6]"
EXPERIMENTS,0.41818181818181815,"Ablation Study
(Q3)
SPC with uniform task sampling
SPC without HRL and COM"
EXPERIMENTS,0.4195804195804196,"Metric. To evaluate the performance of
264"
EXPERIMENTS,0.42097902097902096,"our approach in the GRF 5vs5 scenario, we
265"
EXPERIMENTS,0.4223776223776224,"use metrics beyond just the mean episode
266"
EXPERIMENTS,0.42377622377622376,"reward, as this alone may not accurately re-
267"
EXPERIMENTS,0.4251748251748252,"flect the agents‚Äô performance. Specifically,
268"
EXPERIMENTS,0.42657342657342656,"we use the win rate and the average goal
269"
EXPERIMENTS,0.427972027972028,"difference, which is calculated as the num-
270"
EXPERIMENTS,0.42937062937062936,"ber of goals scored by the MARL agents
271"
EXPERIMENTS,0.4307692307692308,"minus the number of goals scored by the
272"
EXPERIMENTS,0.43216783216783217,"opposing team.
273"
EXPERIMENTS,0.43356643356643354,"We evaluate the performance of MARL algorithms to justify the need for curriculum learning in
274"
EXPERIMENTS,0.43496503496503497,"complex large-scale MARL problems. To ensure a fair comparison, we modify VACL by removing
275"
EXPERIMENTS,0.43636363636363634,"the centralized critic for MPE tasks. Centralized Training Decentralized Execution methods is not
276"
EXPERIMENTS,0.43776223776223777,"included as baselines since they are not suitable for varying numbers (e.g., MADDPG/MAPPO‚Äôs
277"
EXPERIMENTS,0.43916083916083914,"critic requires a fixed size of input or QMIX‚Äôs mixing network also fixed size of the input).
278"
EXPERIMENTS,0.4405594405594406,"In all experiments, we use individual Proximal Policy Optimization (IPPO) as the backend MARL
279"
EXPERIMENTS,0.44195804195804195,"algorithm. To ensure the robustness of our results, we conduct experiments on a 30-node cluster, with
280"
EXPERIMENTS,0.4433566433566434,"one node containing a 128-core CPU and four A100 GPUs. Each trial of the experiment is repeated
281"
EXPERIMENTS,0.44475524475524475,"over five seeds and runs for 1-2 days.
282"
THE NECESSITY OF CURRICULUM LEARNING,0.4461538461538462,"5.2
The Necessity of Curriculum Learning
283"
THE NECESSITY OF CURRICULUM LEARNING,0.44755244755244755,"Our experiments first show that in simple environments, such as MPE, students can directly learn
284"
THE NECESSITY OF CURRICULUM LEARNING,0.4489510489510489,"to complete the task without the need for curriculum. For MPE experiments, we randomly select
285"
THE NECESSITY OF CURRICULUM LEARNING,0.45034965034965035,"a starting state and the episode ends after a fixed number of maximum steps. Specifically, the task
286"
THE NECESSITY OF CURRICULUM LEARNING,0.45174825174825173,"2We also run CDS [66] and CMARL [67], but we have not included their performance because the goal
difference reported in CMARL [67] is relatively low compared to our method."
THE NECESSITY OF CURRICULUM LEARNING,0.45314685314685316,"0.0
0.5
1.0
1.5
Training Timesteps
1e6 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
THE NECESSITY OF CURRICULUM LEARNING,0.45454545454545453,Cover Rate in Eval
THE NECESSITY OF CURRICULUM LEARNING,0.45594405594405596,(a) Cover Rate on Simple Spread
THE NECESSITY OF CURRICULUM LEARNING,0.45734265734265733,"IPPO
VACL
SPC"
THE NECESSITY OF CURRICULUM LEARNING,0.45874125874125876,"2
4
Training Timesteps
1e5 0.20 0.25 0.30"
THE NECESSITY OF CURRICULUM LEARNING,0.46013986013986014,(b) Cover Rate on Push Ball
THE NECESSITY OF CURRICULUM LEARNING,0.46153846153846156,"IPPO
VACL
SPC"
THE NECESSITY OF CURRICULUM LEARNING,0.46293706293706294,"Figure 3: The evaluation performance of vari-
ous methods on MPE."
THE NECESSITY OF CURRICULUM LEARNING,0.4643356643356643,"0.0
0.5
1.0
1.5
Training Timesteps
1e6 2 4 6 8 10 12 14 16"
THE NECESSITY OF CURRICULUM LEARNING,0.46573426573426574,Mean Number of Agents
THE NECESSITY OF CURRICULUM LEARNING,0.4671328671328671,(a) Simple Spread
THE NECESSITY OF CURRICULUM LEARNING,0.46853146853146854,"VACL
SPC"
THE NECESSITY OF CURRICULUM LEARNING,0.4699300699300699,"0
2
4
6
Training Timesteps
1e5 2 4 6 8 10 12 14 16"
THE NECESSITY OF CURRICULUM LEARNING,0.47132867132867134,(b) Push Ball
THE NECESSITY OF CURRICULUM LEARNING,0.4727272727272727,"VACL
SPC"
THE NECESSITY OF CURRICULUM LEARNING,0.47412587412587415,"Figure 4: The changes in the number of agents
on MPE."
M,0.4755244755244755,"0 M
20 M
40 M
60 M
80 M
100 M
Training Timesteps 0.0 0.2 0.4 0.6 0.8"
M,0.47692307692307695,Win Rate in Eval
M,0.4783216783216783,"SPC
SPC w. uniform
SPC w/o. HRL
SPC w/o. COM
IPPO w. uniform
IPPO w/o. teacher
QMix w/o. teacher"
M,0.4797202797202797,(a) Win Rate
M,0.4811188811188811,"0 M
20 M
40 M
60 M
80 M
100 M
Training Timesteps ‚àí3 ‚àí2 ‚àí1 0 1 2 3 4"
M,0.4825174825174825,Goal Diff. in Eval
M,0.48391608391608393,"SPC
SPC w. uniform
SPC w/o. HRL
SPC w/o. COM
IPPO w. uniform
IPPO w/o. teacher"
M,0.4853146853146853,"(b) Goal Difference
Figure 5: The evaluation performance of various methods on 5vs5 football competition. (p-value is
less than 0.05 which means the results are statistically significant.)
space consists of n agents, where n ‚àà{2, 4, 8, 16}, and the maximum allowed steps is set to 25. All
287"
M,0.48671328671328673,"evaluations are performed on the target task, with n = 16. IPPO is trained and evaluated directly on
288"
M,0.4881118881118881,"the target task, and results in Fig. 3 demonstrate that it performs similarly to the VACL algorithm.
289"
M,0.48951048951048953,"We plot the performance within a sliding window so that the starting point is not exactly from 0
290"
M,0.4909090909090909,"timestep. VACL uses entity progression, which is a rule-based curriculum update mechanism so it
291"
M,0.49230769230769234,"lacks the flexibility to switch the curriculum when relatively easy tasks can be learned quickly. The
292"
M,0.4937062937062937,"reason for the performance jump is that SPC can switch to the largest population rapidly, which we
293"
M,0.4951048951048951,"consider one advantage of SPC. Additionally, we observe that the SPC approach only achieves a
294"
M,0.4965034965034965,"slightly higher coverage rate than the baseline methods. Furthermore, we investigate the probability
295"
M,0.4979020979020979,"variation of different population sizes, shown in Fig. 4. We observe that the curriculum provided
296"
M,0.4993006993006993,"by SPC is approaching the target task. These results suggest that in simple environments where the
297"
M,0.5006993006993007,"student can learn to directly complete the task, curriculum learning may not be necessary.
298"
M,0.5020979020979021,"When it comes to more complex scenarios, such as the 5vs5 task in GRF, our results demonstrate
299"
M,0.5034965034965035,"that curriculum learning is a promising solution. As shown in Fig. 5a, without curriculum learning,
300"
M,0.5048951048951049,"QMix and IPPO cannot perform well in the 5vs5 scenario, and IPPO is slightly better than QMix. In
301"
M,0.5062937062937063,"Fig. 5b, we omit the curve of QMix as its mean score is low and affects the presentation of the figure.
302"
M,0.5076923076923077,"The reason could be that QMix is an off-policy MARL algorithm, which would rely heavily on the
303"
M,0.509090909090909,"replay buffer. However, in such sparse reward scenarios, the replay buffer has much less effective
304"
M,0.5104895104895105,"samples for QMix to learn. For example, the replay buffer would contain tons of zero-score samples,
305"
M,0.5118881118881119,"leading to a non-promising performance. Meanwhile, IPPO, with its on-policy nature, is able to
306"
M,0.5132867132867133,"achieve better sample efficiency and outperform off-policy algorithms like QMix in such scenarios.
307"
M,0.5146853146853146,"Though MARL methods can achieve good performance in basic scenarios in GRF, they fail to solve
308"
M,0.5160839160839161,"complex scenarios such as the 5vs5 task. Therefore, curriculum learning is a promising solution to
309"
M,0.5174825174825175,"the complex large-scale MARL problem.
310"
PERFORMANCE AND ABLATION STUDY,0.5188811188811189,"5.3
Performance and Ablation Study
311"
PERFORMANCE AND ABLATION STUDY,0.5202797202797202,"Our study demonstrates that SPC outperforms VACL in MPE tasks. Instead of training with a
312"
PERFORMANCE AND ABLATION STUDY,0.5216783216783217,"continuous relaxation of the population size variable as in VACL, our bandit teacher achieves a higher
313"
PERFORMANCE AND ABLATION STUDY,0.5230769230769231,"success rate at test time, since the population size is a discrete variable in nature. Furthermore, the
314"
PERFORMANCE AND ABLATION STUDY,0.5244755244755245,"curriculum provided by SPC is effective in exploring the task space and converge to the target task
315"
PERFORMANCE AND ABLATION STUDY,0.5258741258741259,"when the task is relatively simple and curriculum is not necessary, as shown in Fig. 4.
316"
PERFORMANCE AND ABLATION STUDY,0.5272727272727272,"In GRF experiments, we do not include VACL in our baselines in the GRF, as its implementation
317"
PERFORMANCE AND ABLATION STUDY,0.5286713286713287,"relies heavily on prior knowledge of specific scenarios, such as the thresholds to divide the learning
318"
M,0.5300699300699301,"20 M
40 M
60 M
80 M
100 M 120 M 140 M
Training Timesteps 0.0 0.2 0.4 0.6 0.8 1.0"
M,0.5314685314685315,Task Prob.
M,0.5328671328671328,"5_vs_5
3_vs_3
academy_pass_and_shoot_with_keeper
academy_3_vs_1
academy_empty_goal_close"
M,0.5342657342657343,(a) The task distribution of SPC during training.
M,0.5356643356643357,"-0.5
0.0
0.5
Dimension 1 -0.5 0.0 0.5 1.0"
M,0.5370629370629371,Dimension 2
M,0.5384615384615384,"0
1
2
3"
M,0.5398601398601398,(b) The visualization of contexts
M,0.5412587412587413,"Figure 6: Visualization of Learned Curriculum.
process. Fig. 6 indicates that SPC has higher win rate and goal difference than IPPO with uniform
319"
M,0.5426573426573427,"task sampling in the 5vs5 competition. These experiments demonstrate that when the teacher is
320"
M,0.544055944055944,"rewarded by the student‚Äôs performance, a bandit-based teacher can exploit the student‚Äôs learning stage
321"
M,0.5454545454545454,"and provide suitable training tasks.
322"
M,0.5468531468531469,"In our ablation study, we examine the impact of two key components of our SPC algorithm: the
323"
M,0.5482517482517483,"contextual multi-armed bandit teacher and the hierarchical structure of the student framework. By
324"
M,0.5496503496503496,"replacing the former with uniform task sampling and removing the latter, As shown in Fig. 5a and
325"
M,0.551048951048951,"Fig. 5b, SPC can achieve a higher win rate and a greater score difference than SPC with uniform and
326"
M,0.5524475524475524,"SPC without HRL. Furthermore, SPC with uniform task sampling outperforms IPPO with uniform
327"
M,0.5538461538461539,"task sampling. This highlights the importance of HRL in the 5vs5 football competition, and suggests
328"
M,0.5552447552447553,"that both the contextual multi-armed bandit and the hierarchical structure contribute equally to the
329"
M,0.5566433566433566,"performance of SPC. When removing HRL and bandit, the performance degradation w.r.t. SPC are
330"
M,0.558041958041958,"similar. However, it should be noted that SPC with uniform task sampling has a larger variance in
331"
M,0.5594405594405595,"performance than SPC without HRL, indicating that uniform sampling may introduce more undesired
332"
M,0.5608391608391609,"tasks for student training. Overall, these results further justify the necessity of SPC in complex
333"
M,0.5622377622377622,"large-scale MARL problems3.
334"
VISUALIZATION OF LEARNED CURRICULUM,0.5636363636363636,"5.4
Visualization of Learned Curriculum
335"
VISUALIZATION OF LEARNED CURRICULUM,0.5650349650349651,"We visualize the distribution of task sampling of SPC during training based on a selected trial as
336"
VISUALIZATION OF LEARNED CURRICULUM,0.5664335664335665,"shown in Fig. 6a. At the beginning of training, the task probability appears to be near-uniform, as
337"
VISUALIZATION OF LEARNED CURRICULUM,0.5678321678321678,"the teacher explores the task space and keeps track of the student‚Äôs learning status, acting as an
338"
VISUALIZATION OF LEARNED CURRICULUM,0.5692307692307692,"anti-forgetting mechanism. As training progresses, the probabilities change over time. For example,
339"
VISUALIZATION OF LEARNED CURRICULUM,0.5706293706293706,"the proportions of 3vs1 and Empty-Goal tasks gradually drop as the student becomes proficient in
340"
VISUALIZATION OF LEARNED CURRICULUM,0.5720279720279721,"these scenarios. We also visualize the distribution of contexts in Fig. 6b using t-SNE [70], where the
341"
VISUALIZATION OF LEARNED CURRICULUM,0.5734265734265734,"contexts are collected and stored in a buffer. We divide the contexts into four classes according to the
342"
VISUALIZATION OF LEARNED CURRICULUM,0.5748251748251748,"index, and different parts represent different contexts of the final student policy representation.
343"
DISCUSSION,0.5762237762237762,"6
Discussion
344"
DISCUSSION,0.5776223776223777,"Conclusion. We present Skilled Population Curriculum (SPC), a novel multi-agent ACL algorithm
345"
DISCUSSION,0.579020979020979,"that addresses scalability and sparse reward issues in multi-agent systems. SPC learns complex
346"
DISCUSSION,0.5804195804195804,"behaviors from scratch by incorporating a population-invariant multi-agent communication framework
347"
DISCUSSION,0.5818181818181818,"and using a hierarchical scheme for agents to learn skills. Moreover, SPC mitigates non-stationarity
348"
DISCUSSION,0.5832167832167832,"by modeling the teacher as a contextual bandit, where the context is represented by the student‚Äôs
349"
DISCUSSION,0.5846153846153846,"policy representation. Though our design choices focus on solving the GRF 5vs5 task, we believe
350"
DISCUSSION,0.586013986013986,"that analyzing and addressing these issues is crucial for further development in multi-agent ACL
351"
DISCUSSION,0.5874125874125874,"algorithms. While SPC may be complex to implement due to its various components, we provide
352"
DISCUSSION,0.5888111888111888,"clean and well-organized code for ease of use.
353"
DISCUSSION,0.5902097902097903,"Limitations. We acknowledge that there are limitations of our algorithm. SPC is over-designed for
354"
DISCUSSION,0.5916083916083916,"simple tasks since our objective is to solve difficult tasks. Also, it would be interesting to understand
355"
DISCUSSION,0.593006993006993,"the impact of varying number of agents on the dynamics of the environment.
356"
DISCUSSION,0.5944055944055944,3We also demonstrate the performance of SPC in the GRF 11vs11 full game (see Appendix C).
REFERENCES,0.5958041958041959,"References
357"
REFERENCES,0.5972027972027972,"[1] RoboCup. Robocup Federation Official Website. https://www.robocup.org/, 2019. Ac-
358"
REFERENCES,0.5986013986013986,"cessed April 10, 2019.
359"
REFERENCES,0.6,"[2] OpenAI. OpenAI Five. https://openai.com/blog/openai-five/, 2019. Accessed March
360"
REFERENCES,0.6013986013986014,"4, 2019.
361"
REFERENCES,0.6027972027972028,"[3] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¬∏sar. Multi-agent reinforcement learning: A
362"
REFERENCES,0.6041958041958042,"selective overview of theories and algorithms. Handbook of Reinforcement Learning and
363"
REFERENCES,0.6055944055944056,"Control, pages 321‚Äì384, 2021.
364"
REFERENCES,0.606993006993007,"[4] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game
365"
REFERENCES,0.6083916083916084,"theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.
366"
REFERENCES,0.6097902097902098,"[5] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu,
367"
REFERENCES,0.6111888111888112,"and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping.
368"
REFERENCES,0.6125874125874126,"arXiv preprint arXiv:2011.02669, 2020.
369"
REFERENCES,0.6139860139860139,"[6] Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang, Jiaming Song, Yu Wang,
370"
REFERENCES,0.6153846153846154,"and Yi Wu. Variational automatic curriculum learning for sparse-reward cooperative multi-agent
371"
REFERENCES,0.6167832167832168,"problems. Advances in Neural Information Processing Systems, 34, 2021.
372"
REFERENCES,0.6181818181818182,"[7] Shiyu Huang, Wenze Chen, Longfei Zhang, Ziyang Li, Fengming Zhu, Deheng Ye, Ting
373"
REFERENCES,0.6195804195804195,"Chen, and Jun Zhu. Tikick: Toward playing multi-agent football full games from single-agent
374"
REFERENCES,0.620979020979021,"demonstrations. arXiv preprint arXiv:2110.04507, 2021.
375"
REFERENCES,0.6223776223776224,"[8] Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen,
376"
REFERENCES,0.6237762237762238,"Changjie Fan, and Yang Gao. From few to more: Large-scale dynamic multiagent curriculum
377"
REFERENCES,0.6251748251748251,"learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
378"
REFERENCES,0.6265734265734266,"7293‚Äì7300, 2020.
379"
REFERENCES,0.627972027972028,"[9] Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolution-
380"
REFERENCES,0.6293706293706294,"ary population curriculum for scaling multi-agent reinforcement learning. arXiv preprint
381"
REFERENCES,0.6307692307692307,"arXiv:2003.10423, 2020.
382"
REFERENCES,0.6321678321678321,"[10] Jiachen Yang, Igor Borovikov, and Hongyuan Zha. Hierarchical cooperative multi-agent
383"
REFERENCES,0.6335664335664336,"reinforcement learning with skill discovery. arXiv preprint arXiv:1912.03558, 2019.
384"
REFERENCES,0.634965034965035,"[11] Alexander Sasha Vezhnevets, Yuhuai Wu, Remi Leblond, and Joel Z Leibo. Options as responses:
385"
REFERENCES,0.6363636363636364,"Grounding behavioural hierarchies in multi-agent rl. arXiv preprint arXiv:1906.01470, 2019.
386"
REFERENCES,0.6377622377622377,"[12] Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.
387"
REFERENCES,0.6391608391608392,"Rode: Learning roles to decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020.
388"
REFERENCES,0.6405594405594406,"[13] Zhen-Jia Pang, Ruo-Ze Liu, Zhou-Yu Meng, Yi Zhang, Yang Yu, and Tong Lu. On reinforcement
389"
REFERENCES,0.641958041958042,"learning for full-length game of starcraft. In Proceedings of the AAAI Conference on Artificial
390"
REFERENCES,0.6433566433566433,"Intelligence, 2019.
391"
REFERENCES,0.6447552447552447,"[14] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
392"
REFERENCES,0.6461538461538462,"1735‚Äì1780, 1997.
393"
REFERENCES,0.6475524475524476,"[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
394"
REFERENCES,0.6489510489510489,"≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information
395"
REFERENCES,0.6503496503496503,"Processing Systems, 30, 2017.
396"
REFERENCES,0.6517482517482518,"[16] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent
397"
REFERENCES,0.6531468531468532,"actor-critic for mixed cooperative-competitive environments. Advances in Neural Information
398"
REFERENCES,0.6545454545454545,"Processing Systems, 2017.
399"
REFERENCES,0.6559440559440559,"[17] Karol Kurach, Anton Raichuk, Piotr Sta¬¥nczyk, Michal Zajkac, Olivier Bachem, Lasse Espeholt,
400"
REFERENCES,0.6573426573426573,"Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research
401"
REFERENCES,0.6587412587412588,"football: A novel reinforcement learning environment. arXiv preprint arXiv:1907.11180, 2019.
402"
REFERENCES,0.6601398601398601,"[18] Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
403"
REFERENCES,0.6615384615384615,"decentralized control of Markov Decision Processes. Mathematics of Operations Research, 27
404"
REFERENCES,0.6629370629370629,"(4):819‚Äì840, 2002.
405"
REFERENCES,0.6643356643356644,"[19] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic
406"
REFERENCES,0.6657342657342658,"multiarmed bandit problem. SIAM Journal on Computing, 32(1):48‚Äì77, 2002.
407"
REFERENCES,0.6671328671328671,"[20] Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In International
408"
REFERENCES,0.6685314685314685,"Conference on Computational Learning Theory, pages 499‚Äì513. Springer, 2007.
409"
REFERENCES,0.66993006993007,"[21] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: An efficient data clustering method
410"
REFERENCES,0.6713286713286714,"for very large databases. ACM Aigmod Record, 25(2):103‚Äì114, 1996.
411"
REFERENCES,0.6727272727272727,"[22] Heikki Hy√∂tyniemi. Turing machines are recurrent neural networks. In STeP ‚Äô96/Publications
412"
REFERENCES,0.6741258741258741,"of the Finnish Artificial Intelligence Society, 1996.
413"
REFERENCES,0.6755244755244755,"[23] Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin B√∂hmer, Shimon Whiteson,
414"
REFERENCES,0.676923076923077,"and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In
415"
REFERENCES,0.6783216783216783,"International Conference on Machine Learning, pages 4596‚Äì4606. PMLR, 2021.
416"
REFERENCES,0.6797202797202797,"[24] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal multi-agent rein-
417"
REFERENCES,0.6811188811188811,"forcement learning via policy decoupling with transformers. arXiv preprint arXiv:2101.08001,
418"
REFERENCES,0.6825174825174826,"2021.
419"
REFERENCES,0.6839160839160839,"[25] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you
420"
REFERENCES,0.6853146853146853,"need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
421"
REFERENCES,0.6867132867132867,"[26] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings
422"
REFERENCES,0.6881118881118881,"of the AAAI Conference on Artificial Intelligence, volume 31, 2017.
423"
REFERENCES,0.6895104895104895,"[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
424"
REFERENCES,0.6909090909090909,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
425"
REFERENCES,0.6923076923076923,"[28] Wei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. Revisiting some common practices in
426"
REFERENCES,0.6937062937062937,"cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2206.07505, 2022.
427"
REFERENCES,0.6951048951048951,"[29] R√©my Portelas, C√©dric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Au-
428"
REFERENCES,0.6965034965034965,"tomatic curriculum learning for deep RL: A short survey. arXiv preprint arXiv:2003.04664,
429"
REFERENCES,0.6979020979020979,"2020.
430"
REFERENCES,0.6993006993006993,"[30] Sanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning.
431"
REFERENCES,0.7006993006993008,"arXiv preprint arXiv:1812.00285, 2018.
432"
REFERENCES,0.7020979020979021,"[31] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur
433"
REFERENCES,0.7034965034965035,"Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik‚Äôs cube
434"
REFERENCES,0.7048951048951049,"with a robot hand. arXiv preprint arXiv:1910.07113, 2019.
435"
REFERENCES,0.7062937062937062,"[32] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active
436"
REFERENCES,0.7076923076923077,"domain randomization. In Conference on Robot Learning, pages 1162‚Äì1176. PMLR, 2020.
437"
REFERENCES,0.7090909090909091,"[33] R√©my Portelas, C√©dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms
438"
REFERENCES,0.7104895104895105,"for curriculum learning of deep RL in continuously parameterized environments. In Conference
439"
REFERENCES,0.7118881118881119,"on Robot Learning, pages 835‚Äì853, 2020.
440"
REFERENCES,0.7132867132867133,"[34] Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced
441"
REFERENCES,0.7146853146853147,"curriculum learning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
442"
REFERENCES,0.7160839160839161,"[35] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation
443"
REFERENCES,0.7174825174825175,"for reinforcement learning agents. In International Conference on Machine Learning, pages
444"
REFERENCES,0.7188811188811188,"1515‚Äì1528. PMLR, 2018.
445"
REFERENCES,0.7202797202797203,"[36] Minqi Jiang, Edward Grefenstette, and Tim Rockt√§schel. Prioritized level replay. In Interna-
446"
REFERENCES,0.7216783216783217,"tional Conference on Machine Learning, pages 4940‚Äì4950. PMLR, 2021.
447"
REFERENCES,0.7230769230769231,"[37] Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim
448"
REFERENCES,0.7244755244755244,"Rockt√§schel. Replay-guided adversarial environment design. Advances in Neural Information
449"
REFERENCES,0.7258741258741259,"Processing Systems, 34:1884‚Äì1897, 2021.
450"
REFERENCES,0.7272727272727273,"[38] Pascal Klink, Carlo D‚ÄôEramo, Jan R Peters, and Joni Pajarinen. Self-paced deep reinforcement
451"
REFERENCES,0.7286713286713287,"learning. Advances in Neural Information Processing Systems, 33:9216‚Äì9227, 2020.
452"
REFERENCES,0.73006993006993,"[39] Pascal Klink, Haoyi Yang, Carlo D‚ÄôEramo, Jan Peters, and Joni Pajarinen. Curriculum rein-
453"
REFERENCES,0.7314685314685314,"forcement learning via constrained optimal transport. In International Conference on Machine
454"
REFERENCES,0.7328671328671329,"Learning, pages 11341‚Äì11358. PMLR, 2022.
455"
REFERENCES,0.7342657342657343,"[40] Maxwell Svetlik, Matteo Leonetti, Jivko Sinapov, Rishi Shah, Nick Walker, and Peter Stone.
456"
REFERENCES,0.7356643356643356,"Automatic curriculum graph generation for reinforcement learning agents. In Proceedings of
457"
REFERENCES,0.737062937062937,"the AAAI Conference on Artificial Intelligence, volume 31, 2017.
458"
REFERENCES,0.7384615384615385,"[41] Wolfgang Banzhaf, Bert Baumgaertner, Guillaume Beslon, Ren√© Doursat, James A Foster,
459"
REFERENCES,0.7398601398601399,"Barry McMullin, Vinicius Veloso De Melo, Thomas Miconi, Lee Spector, Susan Stepney, et al.
460"
REFERENCES,0.7412587412587412,"Defining and simulating open-ended novelty: Requirements, guidelines, and challenges. Theory
461"
REFERENCES,0.7426573426573426,"in Biosciences, 135(3):131‚Äì161, 2016.
462"
REFERENCES,0.7440559440559441,"[42] Joel Lehman, Kenneth O Stanley, et al. Exploiting open-endedness to solve problems through
463"
REFERENCES,0.7454545454545455,"the search for novelty. In ALIFE, pages 329‚Äì336. Citeseer, 2008.
464"
REFERENCES,0.7468531468531469,"[43] Russell K Standish. Open-ended artificial evolution. International Journal of Computational
465"
REFERENCES,0.7482517482517482,"Intelligence and Applications, 3(02):167‚Äì175, 2003.
466"
REFERENCES,0.7496503496503496,"[44] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
467"
REFERENCES,0.7510489510489511,"Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al.
468"
REFERENCES,0.7524475524475525,"Human-level performance in 3d multiplayer games with population-based reinforcement learn-
469"
REFERENCES,0.7538461538461538,"ing. Science, 364(6443):859‚Äì865, 2019.
470"
REFERENCES,0.7552447552447552,"[45] Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel.
471"
REFERENCES,0.7566433566433567,"Emergent coordination through competition. arXiv preprint arXiv:1902.07151, 2019.
472"
REFERENCES,0.7580419580419581,"[46] Abhinav Gupta, Marc Lanctot, and Angeliki Lazaridou. Dynamic population-based meta-
473"
REFERENCES,0.7594405594405594,"learning for multi-agent communication with natural language. Advances in Neural Information
474"
REFERENCES,0.7608391608391608,"Processing Systems, 34:16899‚Äì16912, 2021.
475"
REFERENCES,0.7622377622377622,"[47] R√©my Portelas, Cl√©ment Romac, Katja Hofmann, and Pierre-Yves Oudeyer. Meta automatic
476"
REFERENCES,0.7636363636363637,"curriculum learning. arXiv preprint arXiv:2011.08463, 2020.
477"
REFERENCES,0.765034965034965,"[48] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew,
478"
REFERENCES,0.7664335664335664,"and Igor Mordatch.
Emergent tool use from multi-agent autocurricula.
arXiv preprint
479"
REFERENCES,0.7678321678321678,"arXiv:1909.07528, 2019.
480"
REFERENCES,0.7692307692307693,"[49] Joel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the
481"
REFERENCES,0.7706293706293706,"emergence of innovation from social interaction: A manifesto for multi-agent intelligence
482"
REFERENCES,0.772027972027972,"research. arXiv preprint arXiv:1903.00742, 2019.
483"
REFERENCES,0.7734265734265734,"[50] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical
484"
REFERENCES,0.7748251748251749,"reinforcement learning. Advances in neural information processing systems, 31, 2018.
485"
REFERENCES,0.7762237762237763,"[51] Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation
486"
REFERENCES,0.7776223776223776,"learning for hierarchical reinforcement learning. arXiv preprint arXiv:1810.01257, 2018.
487"
REFERENCES,0.779020979020979,"[52] Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, and Rob Fergus. Learning goal embeddings
488"
REFERENCES,0.7804195804195804,"via self-play for hierarchical reinforcement learning. arXiv preprint arXiv:1811.09083, 2018.
489"
REFERENCES,0.7818181818181819,"[53] Suraj Nair and Chelsea Finn. Hierarchical foresight: Self-supervised learning of long-horizon
490"
REFERENCES,0.7832167832167832,"tasks via visual subgoal generation. arXiv preprint arXiv:1909.05829, 2019.
491"
REFERENCES,0.7846153846153846,"[54] Rundong Wang, Runsheng Yu, Bo An, and Zinovi Rabinovich. I2hrl: Interactive influence-
492"
REFERENCES,0.786013986013986,"based hierarchical reinforcement learning. In Proceedings of the Twenty-Ninth International
493"
REFERENCES,0.7874125874125875,"Conference on International Joint Conferences on Artificial Intelligence, pages 3131‚Äì3138,
494"
REFERENCES,0.7888111888111888,"2021.
495"
REFERENCES,0.7902097902097902,"[55] Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search.
496"
REFERENCES,0.7916083916083916,"In Artificial Intelligence and Statistics, pages 273‚Äì281, 2012.
497"
REFERENCES,0.793006993006993,"[56] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
498"
REFERENCES,0.7944055944055944,"preprint arXiv:1611.07507, 2016.
499"
REFERENCES,0.7958041958041958,"[57] Tanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference.
500"
REFERENCES,0.7972027972027972,"In Proceedings of the 37th International Conference on Machine Learning. JMLR. org, 2020.
501"
REFERENCES,0.7986013986013986,"[58] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-
502"
REFERENCES,0.8,"aware unsupervised discovery of skills. In International Conference on Learning Representa-
503"
REFERENCES,0.8013986013986014,"tions, 2020.
504"
REFERENCES,0.8027972027972028,"[59] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning
505"
REFERENCES,0.8041958041958042,"to communicate with deep multi-agent reinforcement learning. Advances in neural information
506"
REFERENCES,0.8055944055944056,"processing systems, 29, 2016.
507"
REFERENCES,0.806993006993007,"[60] Abhishek Das, Th√©ophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and
508"
REFERENCES,0.8083916083916084,"Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on
509"
REFERENCES,0.8097902097902098,"Machine Learning, pages 1538‚Äì1546. PMLR, 2019.
510"
REFERENCES,0.8111888111888111,"[61] Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropa-
511"
REFERENCES,0.8125874125874126,"gation. Advances in neural information processing systems, 29, 2016.
512"
REFERENCES,0.813986013986014,"[62] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at
513"
REFERENCES,0.8153846153846154,"scale in multiagent cooperative and competitive tasks. arXiv preprint arXiv:1812.09755, 2018.
514"
REFERENCES,0.8167832167832167,"[63] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent coopera-
515"
REFERENCES,0.8181818181818182,"tion. Advances in neural information processing systems, 31, 2018.
516"
REFERENCES,0.8195804195804196,"[64] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan
517"
REFERENCES,0.820979020979021,"Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning.
518"
REFERENCES,0.8223776223776224,"arXiv preprint arXiv:1902.01554, 2019.
519"
REFERENCES,0.8237762237762237,"[65] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning
520"
REFERENCES,0.8251748251748252,"efficient multi-agent communication: An information bottleneck approach. In International
521"
REFERENCES,0.8265734265734266,"Conference on Machine Learning, pages 9908‚Äì9918. PMLR, 2020.
522"
REFERENCES,0.827972027972028,"[66] Chenghao Li, Chengjie Wu, Tonghan Wang, Jun Yang, Qianchuan Zhao, and Chongjie
523"
REFERENCES,0.8293706293706293,"Zhang. Celebrating diversity in shared multi-agent reinforcement learning. arXiv preprint
524"
REFERENCES,0.8307692307692308,"arXiv:2106.02195, 2021.
525"
REFERENCES,0.8321678321678322,"[67] Siyang Wu, Tonghan Wang, Chenghao Li, and Chongjie Zhang. Containerized distributed
526"
REFERENCES,0.8335664335664336,"value-based multi-agent reinforcement learning. arXiv preprint arXiv:2110.08169, 2021.
527"
REFERENCES,0.8349650349650349,"[68] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
528"
REFERENCES,0.8363636363636363,"and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
529"
REFERENCES,0.8377622377622378,"reinforcement learning. In International Conference on Machine Learning, pages 4295‚Äì4304,
530"
REFERENCES,0.8391608391608392,"2018.
531"
REFERENCES,0.8405594405594405,"[69] Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
532"
REFERENCES,0.8419580419580419,"Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the StarCraft
533"
REFERENCES,0.8433566433566434,"multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.
534"
REFERENCES,0.8447552447552448,"[70] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
535"
REFERENCES,0.8461538461538461,"learning research, 9(11), 2008.
536"
REFERENCES,0.8475524475524475,"A
Contextual Bandit for Limited Number of Contexts
537"
REFERENCES,0.848951048951049,Algorithm 2 A contextual bandit algorithm for a small number of contexts
REFERENCES,0.8503496503496504,"1: Initialization: For each context x, create an instance Exp3x of algorithm Exp3
2: for round do
3:
Invoke algorithm Exp3x with x = xt
4:
Play the action chosen by Exp3x
5:
Return reward rt to Exp3x
6: end for"
REFERENCES,0.8517482517482518,"B
Proof of Theorem 3.4
538"
REFERENCES,0.8531468531468531,"Theorem 3.4. Consider the Lipschitz contextual bandit problem with contexts in [0, 1]. The Alg. 1
539"
REFERENCES,0.8545454545454545,"yields regret E[R(T)] = O
 
T 2/3(LK ln T)1/3
.
540"
REFERENCES,0.855944055944056,"Proof. Let Sm be the œµ-uniform mesh on [0, 1], that is, the set of all points in [0, 1] that are integer
541"
REFERENCES,0.8573426573426574,"multiples of œµ. We take œµ = 1/(d ‚àí1) where the integer d is the number of points in Sm, which will
542"
REFERENCES,0.8587412587412587,"be adjusted later in the analysis.
543"
REFERENCES,0.8601398601398601,"We apply Alg. 2 to the context space Sm. Let fSm(x) be a mapping from context x to the closest
544"
REFERENCES,0.8615384615384616,"point in Sm:
545"
REFERENCES,0.862937062937063,"fSm(x) = min

argmin
x‚Ä≤‚ààSm
|x ‚àíx‚Ä≤|
"
REFERENCES,0.8643356643356643,"In each round t, we replace the context xt with fSm (xt) and call Exp3S. The regret bound
546"
REFERENCES,0.8657342657342657,"will have two components: the regret bound for Exp3S and (a suitable notion of) the discretiza-
547"
REFERENCES,0.8671328671328671,"tion error. Formally, let us define the ‚Äúdiscretized best response"" œÄ‚àó
Sm : X ‚ÜíŒ¶: œÄ‚àó
Sm(x) =
548"
REFERENCES,0.8685314685314686,"œÄ‚àó(fSm(x)) for each context x ‚ààX.
549"
REFERENCES,0.8699300699300699,"We define the total reward of an algorithm Alg is Reward (Alg) = PT
t=1 rt. Then the regret of
550"
REFERENCES,0.8713286713286713,"Exp3S and the discretization error are defined as:
551"
REFERENCES,0.8727272727272727,"RS(T) = Reward (œÄ‚àó
S) ‚àíReward (Exp3S)
DE(S) = Reward (œÄ‚àó) ‚àíReward (œÄ‚àó
S) ."
REFERENCES,0.8741258741258742,"It follows that regret is the sum R(T) = RS(T) + DE(S). We have E [RS(T)] = O(‚àöTK log K)
552"
REFERENCES,0.8755244755244755,"from Lemma 3.3, so it remains to upper bound the discretization error and adjust the discretization
553"
REFERENCES,0.8769230769230769,"step œµ.
554"
REFERENCES,0.8783216783216783,"For each round t and the respective context x = xt, r (œÄ‚àó
S(x) | fS(x)) ‚â•r (œÄ‚àó(x) | fS(x)) ‚â•
555"
REFERENCES,0.8797202797202798,"r (œÄ‚àó(x) | x) ‚àíœµL. The first inequality is determined by the optimality of œÄ‚àó
S and the second is
556"
REFERENCES,0.8811188811188811,"determined by Lipschitzness. Summing this up over all rounds t, we obtain E [Reward (œÄ‚àó
S)] ‚â•
557"
REFERENCES,0.8825174825174825,"Reward [œÄ‚àó] ‚àíœµLT.
558"
REFERENCES,0.8839160839160839,"Thus, the regret is that
559"
REFERENCES,0.8853146853146853,E[R(T)] ‚â§œµLT + O r
REFERENCES,0.8867132867132868,"1
œµ TK log T !"
REFERENCES,0.8881118881118881,"= O

T 2/3(LK log T)1/3
(4)"
REFERENCES,0.8895104895104895,"For the last inequality, we want the two terms of the regret bound has the same asymptotic complexity.
560"
REFERENCES,0.8909090909090909,So when œµLT = sqrt 1
REFERENCES,0.8923076923076924,"œµ TK log T, we can get œµ =

K log T"
REFERENCES,0.8937062937062937,"T L2
1/3
. So, we choose œµ = ( K log T"
REFERENCES,0.8951048951048951,"T L2 )1/3.
561 562"
REFERENCES,0.8965034965034965,"C
SPC on GRF 11vs11 Full Game
563"
REFERENCES,0.8979020979020979,"We also conduct experiments on the GRF 11vs11 full game scenario with sparse reward. As shown in
564"
REFERENCES,0.8993006993006993,"Fig. 7, SPC achieves about 50% win rate against built-in AI in the target task after training with 200
565"
M,0.9006993006993007,"0 M
25 M
50 M
75 M
100 M
125 M
150 M
175 M
200 M
Training Timesteps 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
M,0.9020979020979021,Win Rate in Eval SPC
M,0.9034965034965035,Figure 7: The performance of SPC on the 11v11 scenario.
M,0.9048951048951049,"million timesteps. This is non-trivial as this is one of the most challenging benchmarks for MARL
566"
M,0.9062937062937063,"community, and most current MARL methods struggle to achieve progress without hand-crafted
567"
M,0.9076923076923077,"engineering.
568"
M,0.9090909090909091,"D
Qualitatively Analysis On Low-Level Skills
569"
M,0.9104895104895104,"We demonstrate game statistics under different high-level actions. For example, the times of shooting,
570"
M,0.9118881118881119,"passing and running actions per game in GRF. These different low-level policies are induced by
571"
M,0.9132867132867133,"the high-level actions. We evaluate these statistics by fixing one agent‚Äôs high-level actions and
572"
M,0.9146853146853147,"maintaining other agents with SPC. The results in Table 2 are averaged over five runs in the 5vs5
573"
M,0.916083916083916,"scenario.
574"
M,0.9174825174825175,Table 2: Statistics of low-level skills.
M,0.9188811188811189,"shooting per game
passing per game
running per game
skill 1
7.9 times
0.5 times
2254 time steps
skill 2
2.3 times
26.4 times
2149 time steps
skill 3
1.6 times
3.9 times
2875 time steps 575"
M,0.9202797202797203,"E
Comparing Different Teacher Algorithms on GRF Corner-5
576"
M,0.9216783216783216,"To further illustrate the effectiveness of the SPC teacher module, we conduct experiments on the
577"
M,0.9230769230769231,"corner-5 scenario on GRF, where the target task is to control five of the eleven players to obtain
578"
M,0.9244755244755245,"a goal in the GRF Corner scenario. The experiments are designed to determine whether or not
579"
M,0.9258741258741259,"the contextual bandit in SPC outperforms alternative curriculum learning methods to schedule the
580"
M,0.9272727272727272,"number of agents in training. We compare SPC teacher against non-curriculum training (None),
581"
M,0.9286713286713286,"uniform task sampling (Uniform), a state-of-the-art curriculum learning method (ALP-GMM), and a
582"
M,0.9300699300699301,"0.0 M
0.2 M
0.4 M
0.6 M
0.8 M
1 M
Training Timesteps 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
M,0.9314685314685315,Win Rate in Eval
M,0.9328671328671329,"ALP-GMM
None
Uniform
Contextual Bandit
Bandit
VACL"
M,0.9342657342657342,(a) Win Rate
M,0.9356643356643357,"0.0 M
0.2 M
0.4 M
0.6 M
0.8 M
1 M
Training Timesteps ‚àí0.15 ‚àí0.10 ‚àí0.05 0.00 0.05 0.10 0.15 0.20 0.25"
M,0.9370629370629371,Goal Diff. in Eval
M,0.9384615384615385,"ALP-GMM
None
Uniform
Contextual Bandit
Bandit
VACL"
M,0.9398601398601398,"(b) Goal Difference
Figure 8: The evaluation performance of various teacher algorithms on the GRF corner-5 scenario."
M,0.9412587412587412,"multi-agent curriculum learning method (VACL). The training task space consists of n agents, where
583"
M,0.9426573426573427,"n ‚àà{1, 3, 5}. All teachers have the same base architecture without transformer architecture and
584"
M,0.9440559440559441,"HRL. We also investigate the ablation of the RNN-based contexts (see Contextual Bandit and Bandit).
585"
M,0.9454545454545454,"Fig. 8 shows the benefit of SPC contextual bandit over other ACL methods after training with one
586"
M,0.9468531468531468,"million timesteps.
587"
M,0.9482517482517483,"F
Implementation Details
588"
M,0.9496503496503497,"We use the default implementation of Proximal Policy Optimization (PPO) in Ray RLlib, which
589"
M,0.951048951048951,"scales out using multiple workers for experience collection. This allows us to use a large amount of
590"
M,0.9524475524475524,"rollouts from parallel workers during training to ameliorate high variance and aid exploration. We do
591"
M,0.9538461538461539,"multiple rollouts in parallel with distributed workers and use parameter sharing for each agent. The
592"
M,0.9552447552447553,"trainer broadcasts new weights to the workers after their synchronous sampling.
593"
M,0.9566433566433566,"F.1
Google Research Football
594"
M,0.958041958041958,"We set five tasks for training the GRF 5vs5 scenario, including 5vs5, 3vs3, Pass-Shoot, 3vs1, and
595"
M,0.9594405594405594,"Empty-Goal. In the Empty-Goal, one agent need to move forward and shoot with an empty goal.
596"
M,0.9608391608391609,"In Pass-Shoot and 3vs3, two agents are controlled to play against a goalkeeper and three players,
597"
M,0.9622377622377623,"with different position initialization. In 3vs1, three agents are controlled to play against a center-back
598"
M,0.9636363636363636,"and a goalkeeper. In 5vs5, four agents are controlled to play against five players. Without loss of
599"
M,0.965034965034965,"generality, we initialize all player with fixed positions and roles as center midfielders.
600"
M,0.9664335664335665,"We use both MLP and self-attention mechanism for the high-level policy, and use MLP for the
601"
M,0.9678321678321679,"low-level policy. For high-level policy, the input is first projected to an embedding using two hidden
602"
M,0.9692307692307692,"layers with 256 units each and ReLU activation, which is then fed into multi-head self-attention
603"
M,0.9706293706293706,"(8 heads, 64 units each). The output is then projected to the actions and values using another fully
604"
M,0.972027972027972,"connected layer with 256 units. For low-level policy, we use MLP with two hidden layers with 256
605"
M,0.9734265734265735,"units each, i.e., the default configuration of policy network in RLlib.
606"
M,0.9748251748251748,Table 3: SPC hyper-parameters.
M,0.9762237762237762,(a) SPC hyper-parameters used in GRF.
M,0.9776223776223776,"Name
Value"
M,0.9790209790209791,"Discount rate
0.99
GAE parameter
1.0
KL coefficient
0.2
Rollout fragment length
1000
Training batch size
100000
SGD minibatch size
10000
# of SGD iterations
60
Learning rate
1e-4
Entropy coefficient
0.0
Clip parameter
0.3
Value function clip parameter
10.0"
M,0.9804195804195804,(b) SPC hyper-parameters used in MPE.
M,0.9818181818181818,"Name
Value"
M,0.9832167832167832,"Discount rate
0.99
GAE parameter
1.0
KL coefficient
0.5
# of SGD iterations
10
Learning rate
1e-4
Entropy coefficient
0.0
Clip parameter
0.3
Value function clip parameter
10.0"
M,0.9846153846153847,"F.2
MPE
607"
M,0.986013986013986,"In MPE tasks, agents must cooperate through physical actions to reach a set of landmarks. Agents
608"
M,0.9874125874125874,"observe the relative positions of other agents and landmarks, and are collectively rewarded based
609"
M,0.9888111888111888,"on the proximity of any agent to each landmark. In other words, the agents have to cover all of the
610"
M,0.9902097902097902,"landmarks. Further, the agents are penalized when colliding with each other. The agents need to infer
611"
M,0.9916083916083916,"the landmark to cover and move there while avoid colliding with other agents.
612"
M,0.993006993006993,"The hyper-parameters of SPC in MPE are shown in Table 3b. In MPE, hyper-parameters such as
613"
M,0.9944055944055944,"rollout fragment length, training batch size and SGD minibatch size are adjusted according to horizon
614"
M,0.9958041958041958,"of the scenarios so that policy are updated after episodes are done. We use the same neural network
615"
M,0.9972027972027973,"architecture as in GRF, but with 128 units for all MLP hidden layers. Other omitted hyper-parameters
616"
M,0.9986013986013986,"follow the default configuration in RLlib PPO implementation.
617"
