Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005263157894736842,"Recent work suggests that quantum machine learning techniques can be used
for classical image classiﬁcation by encoding the images in quantum states and
using a quantum neural network for inference. However, such work has been
restricted to very small input images, at most 4 × 4, that are unrealistic and
cannot even be accurately labeled by humans. The primary difﬁculties in using
larger input images is that hitherto-proposed encoding schemes necessitate more
qubits than are physically realizable. We propose a framework to classify larger,
realistic images using quantum systems. Our approach relies on a novel encoding
mechanism that embeds images in quantum states while necessitating fewer qubits
than prior work. Our framework is able to classify images that are larger than
previously possible, up to 16 × 16 for the MNIST dataset on a personal laptop, and
obtains accuracy comparable to classical neural networks with the same number
of learnable parameters. We also propose a technique for further reducing the
number of qubits needed to represent images that may result in an easier physical
implementation at the expense of ﬁnal performance. Our work enables quantum
machine learning and classiﬁcation on classical datasets of dimensions that were
previously intractable by physically realizable quantum computers or classical
simulation."
INTRODUCTION,0.010526315789473684,"1
INTRODUCTION"
INTRODUCTION,0.015789473684210527,"In the past decade, deep learning has been remarkably successful on a wide variety of classical
learning tasks (Gi (2016); LeCun et al. (2015)). In parallel, quantum computing (QC) has long
promised dramatic increases in computational power over classical computers, culminating in a
recent demonstration of quantum supremacy in a machine with 53 programmable qubits in Arute
et al. (2019). However, even these quantum systems are already approaching the limits of classical
simulability by the world’s largest traditional supercomputers (Boixo et al. (2018); Preskill (2012)).
The power of quantum computation suggests that quantum analogues of deep learning models like
feedforward neural network may outperform their classical counterparts, especially when the data is
inherently quantum (Wan et al. (2017); Beer et al. (2020); E. Farhi (2018))."
INTRODUCTION,0.021052631578947368,"In this paper, we use a quantum neural network (QNN) to classify the MNIST dataset of handwritten
digits (LeCun & Cortes (2010)). Prior work has been restricted to only highly-compressed, rather
unrealistic input images due to their inefﬁcient encoding schemes that are injective maps from
classical images to their corresponding pure quantum states. These frameworks have used input
images of a maximum resolution of 4 × 4, which is too coarse even for humans to provide accurate
labels (see Figure 1). For larger images, injectivity would necessitate the same number of qubits as
bits that are present in the original image. However, recent advances in quantum machine learning
(QML) on classical data, such as the Flexible Representation of Quantum Images (FRQI) in P.Q. Le
& Hirota (2011), have demonstrated that quantum wavefunctions can utilize quantum entanglement to
encode classical data using exponentially less qubits than their corresponding classical representation
in bits. In this paper, we use the FRQI method to embed the input images in fewer-qubit systems.
This approach necessitates a novel QNN architecture for classiﬁcation, which we describe in Section
5. The input to our model are images of resolution up to 16 × 16 whose quantum encoding only
requires 8 qubits (6 for pixel locations, 1 for color, and 1 for readout). To the best of our knowledge,
our work is the ﬁrst to propose a data encoding scheme and QNN that can be used to classify realistic
images."
INTRODUCTION,0.02631578947368421,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.031578947368421054,Our main contributions are as follows:
INTRODUCTION,0.03684210526315789,"1. We provide a novel construction to compress images and encode them in their FRQI states.
Our construction uses only 2-qubit gates, which permits its use in common quantum machine
learning packages such as Cirq and Tensorﬂow Quantum (TFQ) and may be of independent
interest (Cirq (2021); GoogleAI (2020))."
INTRODUCTION,0.042105263157894736,"2. We propose a new QNN layers, CRADL and CRAML, which we use in a model trained
with the images’ FRQI states as input."
WE SHOW THAT OUR TRAINED QNN ACHIEVES ACCURACY COMPARABLE TO CLASSICAL MODELS WITH THE,0.04736842105263158,"3. We show that our trained QNN achieves accuracy comparable to classical models with the
same number of parameters."
WE SHOW THAT OUR TRAINED QNN ACHIEVES ACCURACY COMPARABLE TO CLASSICAL MODELS WITH THE,0.05263157894736842,"4. We propose a novel technique to further compress black and white images, and study the
scaling behavior of our model with the extent of image compression."
WE SHOW THAT OUR TRAINED QNN ACHIEVES ACCURACY COMPARABLE TO CLASSICAL MODELS WITH THE,0.05789473684210526,"Organization: In Section 2, we provide a brief review of the formalism of quantum computation.
In Section 3, we provide an overview of related work and motivate our study. In Section 4, we
describe our dataset and how each image is encoded in a quantum state. In Section 5, we describe
the prescription for using a quantum neural network to obtain a classiﬁcation prediction for a given
image and describe the model we use for classiﬁcation. In Section 6, we present our results. We
conclude with Section 7 and a description of future work in Appendix 8."
PRELIMINARIES,0.06315789473684211,"2
PRELIMINARIES"
QUANTUM COMPUTING,0.06842105263157895,"2.1
QUANTUM COMPUTING"
QUANTUM COMPUTING,0.07368421052631578,"Here we provide a brief review of quantum computation. For a more detailed reference and an
interactive coding tutorial, we refer the reader to Nielsen & Chuang (2011) and Qiskit (2017)
respectively."
QUANTUM COMPUTING,0.07894736842105263,"In quantum computation, the basic unit of information is a two-state quantum mechanical (QM)
system called a qubit; the two states are traditionally written |0⟩and |1⟩. A qubit can be in either of
these two states, as well as a quantum superposition of these states, formally written as a wavefunction
|ψ⟩= a0 |0⟩+ a1 |1⟩= P"
QUANTUM COMPUTING,0.08421052631578947,"i∈{0,1} ai |i⟩, where each ai ∈C. When a qubit is measured, the
wavefunction collapses and the result of the measurement is state |0⟩with probability |a0|2 and state
|1⟩with probability |a1|2 1. The space of all possible states of the qubit is called the Hilbert space
H1; the states |0⟩and |1⟩provide a basis for this Hilbert space. 2"
QUANTUM COMPUTING,0.08947368421052632,"Multi-qubit systems are represented mathematically by the tensor product of multiple single-qubit
systems. Notationally, we write"
QUANTUM COMPUTING,0.09473684210526316,"|ψN⟩=
X"
QUANTUM COMPUTING,0.1,"{i1,i2,...iN}∈{0,1}N
ai1,i2,...iN |i1, i2, . . . iN⟩"
QUANTUM COMPUTING,0.10526315789473684,"where the states |i1, i2, . . . iN⟩provide a basis for the multi-qubit Hilbert space HN and |ψN⟩is
generally a superposition of these basis states. A two-qubit state |ψ⟩∈H2, cannot necessarily be
factorized into two single-qubit states |ψ1⟩, |ψ2⟩∈H1:"
QUANTUM COMPUTING,0.11052631578947368,"|ψ⟩=
X"
QUANTUM COMPUTING,0.11578947368421053,"{i1,i2}∈{0,1}2
ai1,i2 |i1, i2⟩̸= |ψ1⟩⊗|ψ2⟩= (a0 |0⟩+ a1 |1⟩) ⊗(a′
0 |0⟩+ a′
1 |1⟩)"
QUANTUM COMPUTING,0.12105263157894737,"we call a state which cannot be so factored a mixed state. In particular, we notice that H2 > H1 ⊗H1,
and a similar result holds for multi-qubit systems with N > 2."
QUANTUM COMPUTING,0.12631578947368421,"Under the laws of quantum mechanics, these wavefunctions – or states – evolve in time as determined
by linear unitary transformations. Furthermore, any operation that is physically possible to perform"
QUANTUM COMPUTING,0.13157894736842105,"1We require |a0|2 + |a1|2 = 1
2Formally, a Hilbert space is an inner product vector space that is also a complete metric space with respect
to the distance function induced by that inner product."
QUANTUM COMPUTING,0.1368421052631579,Under review as a conference paper at ICLR 2022
QUANTUM COMPUTING,0.14210526315789473,"on a set of qubits can be represented as a unitary operator. We refer to such unitary operators as
quantum gates and note that unitary operators can be viewed as rotations in Hilbert space."
QUANTUM COMPUTING,0.14736842105263157,"A remarkable property of quantum states is their ability to be entangled. Informally, entanglement
refers to the property of quantum mechanical systems whereby the state of one qubit cannot be
described independently of the other qubits’ states. For example, the state |00⟩is maximally entangled
as knowledge of one qubit’s state complete speciﬁes the other’s."
QUANTUM COMPUTING,0.15263157894736842,"In general, quantum algorithms are procedures whereby an initial wavefunction is transformed under
a sequence of unitary operations, or quantum gates, and a measurement is made of the transformed
state; this measurement is often performed on a readout qubit and is the output of the algorithm. Many
quantum computation algorithms are designed to exploit properties of entangled systems (Bernstein
& Vazirani (1997); David & Richard (1992); Simon (1997); Shor (1997); Grover (1996))."
COMMON QUANTUM GATES,0.15789473684210525,"2.2
COMMON QUANTUM GATES"
COMMON QUANTUM GATES,0.1631578947368421,"Several common quantum gates are deﬁned below. These gates are deﬁned by their action on the
basis states of the Hilbert space, since they extend linearly to superpositions of the basis states. The
deﬁnitions for the single-qubit Hadamard gate H and the Pauli-X gate X are:"
COMMON QUANTUM GATES,0.16842105263157894,"H |0⟩= |+⟩:=
1
√"
COMMON QUANTUM GATES,0.1736842105263158,"2 (|0⟩+ |1⟩)
H |1⟩= |−⟩:=
1
√"
COMMON QUANTUM GATES,0.17894736842105263,"2 (|0⟩−|1⟩)
(1)"
COMMON QUANTUM GATES,0.18421052631578946,"X |0⟩= |1⟩
X |1⟩= |0⟩
(2)"
COMMON QUANTUM GATES,0.18947368421052632,"A common 2-qubit gate is CNOT, which ﬂips the second qubit if the ﬁrst qubit is in state |1⟩:"
COMMON QUANTUM GATES,0.19473684210526315,"CNOT |00⟩= |00⟩
CNOT |01⟩= |01⟩
(3)
CNOT |10⟩= |11⟩
CNOT |11⟩= |10⟩"
RELATED WORK,0.2,"3
RELATED WORK"
RELATED WORK,0.20526315789473684,"Many studies use QNNs to model either inherently quantum or quantum-encoded classical data but
are generally restricted to very small images (Li et al. (2020); Henderson et al. (2020); Oh et al.
(2021)). One line of work encodes classical data in quantum systems and focuses on learning the
classiﬁer’s circuit architecture. These approaches require an injective map from the input image to a
corresponding pure quantum state, which forgoes the exponential compression advantages afforded
by methods such as FRQI3 (E. Farhi (2018); Aïmeur et al. (2013); Paparo et al. (2014); Schuld et al.
(2014); Kapoor et al. (2016)). Amongst this line of work, E. Farhi (2018) propose the general setup
that we follow in this paper. In contrast with their work, however, we use the FRQI technique to
exploit the dimensionality of the multi-qubit Hilbert space and need much fewer qubits."
RELATED WORK,0.21052631578947367,"Other studies take the quantum wavefunction as given, either by assuming the classical data is already
provided in its quantum-encoded form Schuld et al. (2020) or because they use inherently quantum
data (Sasaki & Carlini (2002); Gambs (2008); Sentís et al. (2012); Dunjko et al. (2016); Monràs et al.
(2017); Alvarez-Rodriguez et al. (2017); Du et al. (2020); Sentís et al. (2019); Beer et al. (2020);
Caro & Datta (2020)). Amongst these papers, Schuld et al. (2020) is perhaps closest to this work.
The authors, however, take the mixed-state encodings of images as given for input to a QNN and do
not describe how to construct the quantum states. Other work, such as Beer et al. (2020), assumes the
wavefunctions as given proposes a generalization of the perceptron to the quantum setting, which
provides a more generalized framework than in E. Farhi (2018) These authors use inherently quantum
data, in contrast with our work. We use classical data and explicitly construct quantum circuits to
encode classical images into their wavefunctions. Our approach lends itself to direct experimentation
and is usable with modern quantum machine learning packages."
RELATED WORK,0.21578947368421053,"Finally, a third line of work uses quantum convolutional neural networks via semi-classical simulations
meant to model the noise introduced by quantum effects, as in Kerenidis et al. (2019). These
approaches do not provide a fully quantum simulation to evolve the quantum states, which would
require construction of the actual data’s wavefunctions as in our work."
RELATED WORK,0.22105263157894736,"3For a demo of a standard approach following E. Farhi (2018) using TFQ, which could also serve as a
preliminary for this work, see https://www.tensorﬂow.org/quantum/tutorials/mnist"
RELATED WORK,0.22631578947368422,Under review as a conference paper at ICLR 2022
RELATED WORK,0.23157894736842105,"Throughout prior work, encoding classical data in quantum states efﬁciently appears to be a common
open problem."
FORMAL SETTING,0.23684210526315788,"4
FORMAL SETTING"
PROBLEM STATEMENT,0.24210526315789474,"4.1
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.24736842105263157,"In classical image classiﬁcation, the input to our model is an n × n-pixel image ∈{0, 1}2n and our
goal is to learn a classiﬁcation function with binary outputs fclassical parametrized by weights w:"
PROBLEM STATEMENT,0.25263157894736843,"fclassical(w) : {0, 1}2n →{0, 1}
(4)"
PROBLEM STATEMENT,0.2578947368421053,"In the quantum setting, the input to our classiﬁcation function is still an n × n-pixel image but
must be encoded in a ⌈log2n⌉+ 1 dimensional Hilbert space H by an encoding function F, where
the +1 is for the readout qubit. The quantum neural network is a sequence of unitary operations
U = U1◦U2◦. . . UN parametrized by angles θ = θ1, θ2, . . . , θN. To obtain a classiﬁcation prediction,
a measurement is performed on the readout qubit:"
PROBLEM STATEMENT,0.2631578947368421,"fquantum(θ) : {0, 1}2n →{0, 1}
(5)"
PROBLEM STATEMENT,0.26842105263157895,": {0, 1}2n
F
−−→H
U(θ)
−−−−→H
measure
−−−−→{0, 1}"
PROBLEM STATEMENT,0.2736842105263158,That is fquantum(θ) = measure ◦U(θ) ◦F.
PROBLEM STATEMENT,0.2789473684210526,"In Sections 5 and 5 we propose an implementation of the FRQI algorithm P.Q. Le & Hirota (2011)
to construct F, propose a construction of U(θ), describe how to learn the parameters θ via standard
backpropagation, and describe the ﬁnal measurement step."
DATASET AND QUANTUM ENCODING,0.28421052631578947,"4.2
DATASET AND QUANTUM ENCODING"
DATASET AND QUANTUM ENCODING,0.2894736842105263,"Crucial to our approach is the encoding of a classical datapoint (e.g. an image) in a quantum state. In
our experiments, we use the MNIST dataset of handwritten digits LeCun & Cortes (2010). Following
E. Farhi (2018), we restrict our dataset to those of only two ground truth labels: 3 and 6. We
downsample image resolutions to either 8 × 8 or 16 × 16 using bilinear interpolation. The remaining
dataset is approximately 12, 000 training images and 1, 100 validation images for each resolution.
Finally, we transform the images to black and white by thresholding the pixel color."
DATASET AND QUANTUM ENCODING,0.29473684210526313,"In Figure 1, we present an MNIST image downsampled to different resolutions. Prior work uses
resolutions of only 4 × 4, but loses many important features of the original data E. Farhi (2018). With
the FRQI encoding and the further compression we are able to encode higher-resolution images on
current quantum hardware; this insight motivates our study of different downsampled resolutions."
DATASET AND QUANTUM ENCODING,0.3,"After preprocessing, each image is a black and white 2n × 2n = 22n dimensional binary vector. Our
objective is to encode the image as a wavefunction |ψdata⟩:"
DATASET AND QUANTUM ENCODING,0.30526315789473685,"|ψdata⟩=
X"
DATASET AND QUANTUM ENCODING,0.3105263157894737,"q:={q0,q1,...,q2n−1}∈{0,1}2n
|q0, q1, . . . , q2n−1⟩⊗(cos θq |0⟩+ sin θq |1⟩)
(6)"
DATASET AND QUANTUM ENCODING,0.3157894736842105,"In Equation 6, each basis state |q0, q1, . . . , q2n−1⟩of the ""pixel qubits"" represents a possible bitstring
of length 22n with the strength of the superposition component and color determined by θq in the
""color qubit"". In our dataset, each θq is either 0 or π 2 ."
DATASET AND QUANTUM ENCODING,0.32105263157894737,"In some experiments in Section 5, we also consider allocating more qubits to encode the color angle
θq instead of the pixel locations:"
DATASET AND QUANTUM ENCODING,0.3263157894736842,"|ψdata⟩=
X"
DATASET AND QUANTUM ENCODING,0.33157894736842103,"q:={q0,q1,...,q2n−1}∈{0,1}2n
|q0, q1, . . . , q2n−3⟩⊗

cos ˜θq |0⟩+ sin ˜θq |1⟩
"
DATASET AND QUANTUM ENCODING,0.3368421052631579,Under review as a conference paper at ICLR 2022
DATASET AND QUANTUM ENCODING,0.34210526315789475,"Figure 1: A digit from MNIST presented at different downsampled resolutions (downscaled resolutions indicated
on top of each image. The top row consists of grayscale images with 0 ≤color ≤1 as in the original dataset.
The bottom row are black and white images obtained by thresholding the pixel color from the respective images
above."
DATASET AND QUANTUM ENCODING,0.3473684210526316,"To do this, we exploit the observation that the color qubit is always either |0⟩or |1⟩and map into it
the last two pixel qubits according to the transformation:"
DATASET AND QUANTUM ENCODING,0.3526315789473684,"|q2n−2, q2n−1⟩⊗|qc⟩→|˜qc⟩= cos ˜θq |0⟩+ sin ˜θq |1⟩
(7)"
DATASET AND QUANTUM ENCODING,0.35789473684210527,"where each qi ∈{0, 1} and"
DATASET AND QUANTUM ENCODING,0.3631578947368421,˜θq = π 2
DATASET AND QUANTUM ENCODING,0.3684210526315789,"
qc + q2n−2"
DATASET AND QUANTUM ENCODING,0.3736842105263158,"2
+ q2n−1 4"
DATASET AND QUANTUM ENCODING,0.37894736842105264,"
= θq + π 4"
DATASET AND QUANTUM ENCODING,0.38421052631578945,"
q2n−2 + q2n−1 2 
(8)"
METHODS,0.3894736842105263,"5
METHODS"
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.39473684210526316,"5.1
ENCODING THE IMAGES IN WAVEFUNCTIONS"
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4,"In our approach, we must ﬁrst encode the image in a quantum wavefunction. We pass an initial state
of |0 . . . 0⟩through a quantum circuit with a given structure, demonstrated by Figure 2 for a 4-qubit
state. Initially, a Hadamard operation H⊗2n is performed on the 2n pixel qubits. This is followed by a
series of n-controlled X-gates (also known as generalized TOFFOLI gates Toffoli (1980); Rasmussen
et al. (2020); Shende & Markov (2008)) with alternating X gates that determine the color qubits
which will be transformed. The n-qubit circuit is constructed recursively from smaller-qubit circuits
by observing the symmetries in the construction."
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4052631578947368,"Figure 2: An example circuit to construct the superposition state for 4 qubits, using 4-qubit generalized TOFFOLI
gate. We follow standard quantum circuit diagram conventions in which the dots represent the control of a gate
by the given qubits."
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4105263157894737,"To construct this circuit we need to deﬁne the generalized TOFFOLI gate for n qubits from basic
two-qubit gates; this also enables our implementation in standard packages such as Cirq (Cirq (2021))
that only support backpropagation through two-qubit gates. We use the following lemma, ﬁrst shown
in Barenco et al. (1995), to recursively decompose the n-qubit generalized TOFFOLI gate as a
sequence of (n −1)-qubit generalized TOFFOLI gates and CNOT gates:"
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.41578947368421054,Under review as a conference paper at ICLR 2022
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.42105263157894735,"Lemma 1. (Barenco et al. (1995), Lemma 7.5): For a rotation matrix R(t), an n-controlled rotation
gate can be decomposed into a circuit of the form shown in Figure 3. =
(9)"
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4263157894736842,Figure 3: n−qubit controlled gate in terms of (n −1)−qubit controlled gates
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.43157894736842106,"From the recursive properties of the diagram above we see that an n-qubit controlled rotation
decomposes into (2 · 3n −1) one-qubit controlled rotations. Since we ﬂip at most 2n pixels for a
given image, we see that for each image"
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4368421052631579,# one-qubit controlled gates ≤2n (2 · 3n −1)
ENCODING THE IMAGES IN WAVEFUNCTIONS,0.4421052631578947,"in addition to the standalone X−gates to encode the 2n × 2n image in the amplitudes of the input
wavefunction. Though this bound is exponential in n, we ﬁnd this acceptable as it is still classically
simulable for larger images that are primarily constrained by the size of their qubit representations."
READOUT QUBIT AND PREDICTED LABELS,0.4473684210526316,"5.2
READOUT QUBIT AND PREDICTED LABELS"
READOUT QUBIT AND PREDICTED LABELS,0.45263157894736844,"Our wavefunction must also contain a readout qubit on which we perform measurements that will be
the model’s predicted labels. As such, we prepare the wavefunction |ψin⟩= |ψdata⟩⊗|readout⟩.
We choose the Z-gate for measurement and thus initialize the readout qubit in the |+⟩state, which is
common practice to produce an initially unbiased output:"
READOUT QUBIT AND PREDICTED LABELS,0.45789473684210524,"|ψin⟩= |ψdata⟩⊗H |0⟩= |ψdata⟩⊗|+⟩
(10)"
READOUT QUBIT AND PREDICTED LABELS,0.4631578947368421,where |ψdata⟩is prepared as in Section 5.1.
READOUT QUBIT AND PREDICTED LABELS,0.46842105263157896,"The model which is used to transform |ψin⟩is a QNN with L layers. Following E. Farhi (2018), each
layer is represented by a parametrized unitary matrix. The model’s output state is:"
READOUT QUBIT AND PREDICTED LABELS,0.47368421052631576,"|ψout(θ)⟩= U(θL) . . . U(θ1) (|ψdata⟩⊗|+⟩)
(11)"
READOUT QUBIT AND PREDICTED LABELS,0.4789473684210526,"where θ := (θ1, . . . , θL). The ﬁnal measurement is performed with Z−gate on the readout qubit;
the predicted label is ⟨ψout(θ)| I2n+1 ⊗Z |ψout(θ)⟩. We train the model’s parameters, θ1 . . . θL, via
stochastic gradient descent (SGD) using these predictions and the hinge loss:"
READOUT QUBIT AND PREDICTED LABELS,0.4842105263157895,"loss(i)(θ) = 1 −y(i) ⟨ψ(i)
out(θ)| I2n+1 ⊗Z |ψ(i)
out(θ)⟩
(12)"
READOUT QUBIT AND PREDICTED LABELS,0.48947368421052634,where the superscript (i) is used to refer to the ith training example.
IMPLEMENTATION DETAILS,0.49473684210526314,"5.3
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.5,"We use Cirq (Cirq (2021)) to encode the images into their respective wavefunctions and TensorFlow
Quantum (TFQ) (GoogleAI (2020)) to train the model via the paradigm described in Section 5.2.
TFQ permits the use of Parametrized Quantum Circuits (PQCs), which describe the unitary operations
of the QNN, as a single Keras layer Chollet et al. (2015) within the standard TensorFlow framework."
IMPLEMENTATION DETAILS,0.5052631578947369,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.5105263157894737,"Backpropagation through quantum layers is nontrivial. We recall that, for any layer, the 2n × 2n
unitary operator can be expressed in terms of the exponential of a 2n × 2n Hermitian operator H
called the Hamiltonian, which in turn can be decomposed into its Pauli decomposition (a tensor
product of n Pauli matrices, which form an orthonormal basis over the Hilbert space of Hermitian
matrices over R):"
IMPLEMENTATION DETAILS,0.5157894736842106,"U(θ) = exp 
 i
X"
IMPLEMENTATION DETAILS,0.5210526315789473,"σi∈{I,σ1,σ2,σ3}n
θ(σi) O i
σi 
"
IMPLEMENTATION DETAILS,0.5263157894736842,"
(13)"
IMPLEMENTATION DETAILS,0.531578947368421,"When the layer is restricted to unitary operations whose Hamiltonian has a single term in the Pauli
decomposition, gradients with respect to the layer’s parameters can be computed analytically using
the parameter shift techniques ﬁrst introduced in Mitarai et al. (2018) and Schuld et al. (2019) (Harrow
& Napp (2021)). These techniques provide a way to calculate partial derivatives of parameterized
quantum circuits in terms of other functions that use the same circuit architecture with shifted
parameters. For this reason, we restrict the gates in our quantum layers to be multi-qubit exponential
Pauli gates. For these gates, analytic gradients can also be computed because they are rotations of
operations whose Hamiltonians contain a single type of term. For example, the XX-gate can be
written:"
IMPLEMENTATION DETAILS,0.5368421052631579,"(X ⊗X)θ = exp

θ
 
−i π"
IMPLEMENTATION DETAILS,0.5421052631578948,2 (X −I) ⊗−i π
IMPLEMENTATION DETAILS,0.5473684210526316,"2 (X −I)
	
= e−i π"
IMPLEMENTATION DETAILS,0.5526315789473685,2 θ(X−I) ⊗e−i π
IMPLEMENTATION DETAILS,0.5578947368421052,2 θ(X−I)
NETWORK ARCHITECTURE,0.5631578947368421,"5.4
NETWORK ARCHITECTURE"
NETWORK ARCHITECTURE,0.5684210526315789,"A general 2n × 2n learnable unitary operation would consist of 22n trainable real parameters and the
standard representation of this parameters follows equation equation 13. Note that, as described in
Section 5.3, this does not necessarily permit analytic gradient computation in the backpropagation
step."
NETWORK ARCHITECTURE,0.5736842105263158,"To permit analytical gradient computations, we construct layers having a speciﬁc structure; an
example such layer is demonstrated in Figure 4. Each layer consists of either XX or ZZ operators
applied in succession to each pixel qubit and the readout qubit, followed by the same operation
on the pixel qubit and the color qubit. Empirically, the Color-Readout-Alternating-Double-Layer
architecture (CRADL) presented in Figure 4 resulted in the best performance."
NETWORK ARCHITECTURE,0.5789473684210527,"Each consecutive pixel-readout and pixel-color pair of gates share the same learning parameter, a
rotation angle. One double layer of the type shown in Figure 4 will have 2n learnable parameters,
where n is the number of pixel qubits. A circuit with L layers will therefore have nL parameters. In
the experiments in Section 6, we choose the number of layers such that the number of parameters nL
is comparable to those of the classical benchmark, given a ﬁxed number of qubits."
NETWORK ARCHITECTURE,0.5842105263157895,"Figure 4: A “CRADL” network double layer with 6 pixel qubits, consisting of consecutive pixel-readout
pixel-color XX gates, followed by analogous ZZ gates"
NETWORK ARCHITECTURE,0.5894736842105263,"We note that there are equivalent network architectures that lead to comparable results, such as the
Color-Readout-Alternating-Mixed-Layer architecture shown in Figure 5."
NETWORK ARCHITECTURE,0.5947368421052631,Under review as a conference paper at ICLR 2022
NETWORK ARCHITECTURE,0.6,"Figure 5: A “CRAML” network layer with 6 pixel qubits, consisting of consecutive pixel-readout pixel-color
XX and ZZ gates"
RESULTS,0.6052631578947368,"6
RESULTS"
RESULTS,0.6105263157894737,"We benchmark our quantum learning framework against a classical neural network with two hidden
layers with ReLU activations and a single-neuron output layer; in this setting, the quantum and
classical models have a comparable number of parameters. We trained the quantum neural network
for 10 epochs, which is the same number of epochs after which the classical neural network began to
overﬁt (as determined by cross-validation). All experiments were conducted on a personal laptop
with no GPU (Macbook Pro, 2.4 GHz 8-Core Intel Core i9 CPU, 64 GB 2667 MHz DDR4 RAM)."
RESULTS,0.6157894736842106,"Network
8 × 8 Image
16 × 16 Image"
RESULTS,0.6210526315789474,"Classical CNN
94 ± 1%
98.9 ± 0.3%
Quantum CRADL
92 ± 1%
−
Quantum CRADL −2Q
88 ± 1%
90 ± 1%"
RESULTS,0.6263157894736842,Table 1: Test accuracies after the 10th training epoch for classical and quantum networks.
RESULTS,0.631578947368421,"Figure 6: Test accuracy versus training epoch for classical and quantum models, with and without the extra 2
qubit compression described in Section 4.2. When images are embedded on 6 qubits (green curve, left), we
achieve performance comparable to classical networks with the same number of parameters. When images are
further compressed (orange curves), performance degrades."
RESULTS,0.6368421052631579,"Table 1 demonstrates our ﬁnal results. On 8 × 8 images, the QNN without the extra two-qubit
compression achieves performance comparable to the classical network, whereas the network with the
extra two-qubit compression (denoted −2Q) performs worse. For 16×16 images, we add more layers
to the QNN so that the the number of parameters remains comparable to the classical dense network
and must use the extra two-qubit compression due to the computational cost of the experiment. We"
RESULTS,0.6421052631578947,Under review as a conference paper at ICLR 2022
RESULTS,0.6473684210526316,"observe that the QNN is unable to achieve the same performance as the classical network, likely due
to the extensive compression of the images in the quantum states."
RESULTS,0.6526315789473685,"Figure 6 shows the validation accuracy of our quantum and baseline classical models versus training
epoch. We observe that the classical neural network and both quantum neural networks demonstrate
similar validation performance curves."
DISCUSSION AND CONCLUSION,0.6578947368421053,"7
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.6631578947368421,"We note that the method we describe in Equation 8 to reduce the number of required qubits results in
worse performance. This degradation in test accuracy may be palatable in applications that attempt to
minimize their qubit usage. When we attempted to lower the number of necessary qubits further, we
observed unstable learning behavior. In such settings with reduced feature dimensionality, it may be
necessary to redesign the network architecture."
DISCUSSION AND CONCLUSION,0.6684210526315789,"In this paper, we developed a proof of concept for recently proposed QNN models. In the process, we
proposed a methodology to map classical images to quantum states that may be of independent interest
to the community. We also propose a new form of quantum neural network layers motivated by the
highly entangled input states, the CRADL and CRAML layers in Section 5.3, and demonstrate that a
model consisting of these layers achieves performance competitive with classical neural networks
with a comparable number of parameters. Furthermore, our work is evidence that quantum machine
learning algorithms can scale to data of dimensions larger than those previously tractable by classical
simulation or available quantum hardware and classify MNIST images of size 16 × 16 on a personal
laptop."
FUTURE WORK,0.6736842105263158,"8
FUTURE WORK"
FUTURE WORK,0.6789473684210526,"We did not do a comprehensive survey of the space of all possibly unitary operations that could be
used for each hidden layer, but we could imagine that invoking 3 or more qubit gates to the layer
circuits would improve the learning outcomes, as that would give more direct access to the correlation
structure. The circuit above Figure 4 composed entirely of two-qubit gates is trying to work around
this limitation."
FUTURE WORK,0.6842105263157895,"We would like in the future to conduct a more systematic survey of network architectures to enhance
the learning outcomes with possibly more involved quantum gates, either within TensorFlow Quantum
or implemented separately, and study the cost beneﬁt of forgoing analytic gradients for some of these
gates for more ﬂexibility in construction. Another lower hanging fruit is optimizing the encoded
input circuits in terms of memory. As long as we write and simulate quantum algorithms on digital
computers, representing the wavefunction in this manner seems inevitable and we should ﬁnd better
optimizations to describe the encoding procedure through circuits with much less gates than the nice
recursive algorithms discussed in this paper."
FUTURE WORK,0.6894736842105263,"We observed in Section 6 that compressing the images into larger quantum systems resulted in better
performance at the expensive of greater complexity in physical realization. We leave a more thorough
analysis of this tradeoff, including the understanding of the interplay between the data qubits and the
color qubit, to future work."
FUTURE WORK,0.6947368421052632,"We also note that we may view the encoding with limited circuit gates or on limited qubits as a form
of implicit regularization, which has been observed to improve generalization performance as in
Smith et al. (2016). We leave investigation of these connections to future work."
ETHICS STATEMENT,0.7,ETHICS STATEMENT
ETHICS STATEMENT,0.7052631578947368,We do not foresee any ethical concerns with our work.
ETHICS STATEMENT,0.7105263157894737,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.7157894736842105,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.7210526315789474,"The authors have submitted all the code necessary to reproduce their results in the Supplementary
Materials as a .zip. The code contains a README.md ﬁle with instructions on how to reproduce
the experimental results. The only dataset required is the MNIST dataset, which is easily obtained
from http://yann.lecun.com/exdb/mnist/. The preprocessing applied to the MNIST
dataset is described in detail in Section 5."
REFERENCES,0.7263157894736842,REFERENCES
REFERENCES,0.7315789473684211,"Esma Aïmeur, Gilles Brassard, and Sébastien Gambs. Quantum speed-up for unsupervised learning.
Machine Learning, 90(2):261–287, Feb 2013. ISSN 1573-0565. doi: 10.1007/s10994-012-5316-5.
URL https://doi.org/10.1007/s10994-012-5316-5."
REFERENCES,0.7368421052631579,"Unai Alvarez-Rodriguez, Lucas Lamata, Pablo Escandell-Montero, José D. Martín-Guerrero, and
Enrique Solano. Supervised quantum learning without measurements. Scientiﬁc Reports, 7
(1):13645, Oct 2017. ISSN 2045-2322. doi: 10.1038/s41598-017-13378-0. URL https:
//doi.org/10.1038/s41598-017-13378-0."
REFERENCES,0.7421052631578947,"Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C. Bardin, Rami Barends, Rupak
Biswas, Sergio Boixo, Fernando G. S. L. Brandao, David A. Buell, Brian Burkett, Yu Chen, Zijun
Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Edward Farhi, Brooks
Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve Habegger,
Matthew P. Harrigan, Michael J. Hartmann, Alan Ho, Markus Hoffmann, Trent Huang, Travis S.
Humble, Sergei V. Isakov, Evan Jeffrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Julian
Kelly, Paul V. Klimov, Sergey Knysh, Alexander Korotkov, Fedor Kostritsa, David Landhuis, Mike
Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore Mandrà, Jarrod R. McClean, Matthew McEwen,
Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman,
Matthew Neeley, Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C.
Platt, Chris Quintana, Eleanor G. Rieffel, Pedram Roushan, Nicholas C. Rubin, Daniel Sank,
Kevin J. Satzinger, Vadim Smelyanskiy, Kevin J. Sung, Matthew D. Trevithick, Amit Vainsencher,
Benjamin Villalonga, Theodore White, Z. Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven,
and John M. Martinis. Quantum supremacy using a programmable superconducting processor.
Nature, 574(7779):505–510, Oct 2019. ISSN 1476-4687. doi: 10.1038/s41586-019-1666-5. URL
https://doi.org/10.1038/s41586-019-1666-5."
REFERENCES,0.7473684210526316,"Adriano Barenco, Charles H. Bennett, Richard Cleve, David P. DiVincenzo, Norman Margolus,
Peter Shor, Tycho Sleator, John A. Smolin, and Harald Weinfurter. Elementary gates for quantum
computation. Phys. Rev. A, 52:3457–3467, Nov 1995. doi: 10.1103/PhysRevA.52.3457. URL
https://link.aps.org/doi/10.1103/PhysRevA.52.3457."
REFERENCES,0.7526315789473684,"Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel
Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature Communications,
11(1), Feb 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-14454-2. URL http://dx.doi.
org/10.1038/s41467-020-14454-2."
REFERENCES,0.7578947368421053,"Ethan Bernstein and Umesh Vazirani. Quantum complexity theory. SIAM Journal on Computing,
26(5):1411–1473, 1997. doi: 10.1137/S0097539796300921. URL https://doi.org/10.
1137/S0097539796300921."
REFERENCES,0.7631578947368421,"Sergio Boixo, Sergei V. Isakov, Vadim N. Smelyanskiy, Ryan Babbush, Nan Ding, Zhang Jiang,
Michael J. Bremner, John M. Martinis, and Hartmut Neven. Characterizing quantum supremacy in
near-term devices. Nature Physics, 14(6):595–600, Jun 2018. ISSN 1745-2481. doi: 10.1038/
s41567-018-0124-x. URL https://doi.org/10.1038/s41567-018-0124-x."
REFERENCES,0.7684210526315789,"Matthias C. Caro and Ishaun Datta. Pseudo-dimension of quantum circuits. Quantum Machine
Intelligence, 2(2), Nov 2020.
ISSN 2524-4914.
doi: 10.1007/s42484-020-00027-5.
URL
http://dx.doi.org/10.1007/s42484-020-00027-5."
REFERENCES,0.7736842105263158,"François Chollet et al. Keras. https://github.com/fchollet/keras, 2015."
REFERENCES,0.7789473684210526,Under review as a conference paper at ICLR 2022
REFERENCES,0.7842105263157895,"Cirq. Cirq, August 2021. URL https://doi.org/10.5281/zenodo.5182845. See full
list of authors on Github: https://github.com/quantumlib/Cirq/graphs/contributors."
REFERENCES,0.7894736842105263,"Deutsch David and Jozsa Richard. Rapid solution of problems by quantum computation. Proc. R.
Soc. Lond. A, 439:553–558, 1992. URL http://doi.org/10.1098/rspa.1992.0167."
REFERENCES,0.7947368421052632,"Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive power of parametrized
quantum circuits. Phys. Rev. Research, 2:033125, Jul 2020. doi: 10.1103/PhysRevResearch.
2.033125.
URL https://link.aps.org/doi/10.1103/PhysRevResearch.2.
033125."
REFERENCES,0.8,"Vedran Dunjko, Jacob M. Taylor, and Hans J. Briegel.
Quantum-enhanced machine learning.
Phys. Rev. Lett., 117:130501, Sep 2016. doi: 10.1103/PhysRevLett.117.130501. URL https:
//link.aps.org/doi/10.1103/PhysRevLett.117.130501."
REFERENCES,0.8052631578947368,"H. Neven E. Farhi. Classiﬁcation with Quantum Neural Networks on Near Term Processors. Arxiv
Preprint, 2018. URL https://arxiv.org/abs/1802.06002."
REFERENCES,0.8105263157894737,"Sébastien Gambs. Quantum classiﬁcation, 2008."
REFERENCES,0.8157894736842105,"Kim Kwang Gi. Book review: Deep learning. Healthc Inform Res, 22(4):351–354, 2016. doi: 10.
4258/hir.2016.22.4.351. URL http://e-hir.org/journal/view.php?number=828."
REFERENCES,0.8210526315789474,"GoogleAI. Tensorﬂow quantum: A software framework for quantum machine learning. Arxiv
Preprint, 2020. URL https://arxiv.org/abs/2003.02989."
REFERENCES,0.8263157894736842,"Lov K. Grover. A fast quantum mechanical algorithm for database search. In Proceedings of
the Twenty-Eighth Annual ACM Symposium on Theory of Computing, STOC ’96, pp. 212–219,
New York, NY, USA, 1996. Association for Computing Machinery. ISBN 0897917855. doi:
10.1145/237814.237866. URL https://doi.org/10.1145/237814.237866."
REFERENCES,0.8315789473684211,"Aram W. Harrow and John C. Napp. Low-depth gradient measurements can improve convergence
in variational hybrid quantum-classical algorithms. Physical Review Letters, 126(14), Apr 2021.
ISSN 1079-7114. doi: 10.1103/physrevlett.126.140502. URL http://dx.doi.org/10.
1103/PhysRevLett.126.140502."
REFERENCES,0.8368421052631579,"Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and Tristan Cook. Quanvolutional
neural networks: powering image recognition with quantum circuits. Quantum Machine Intel-
ligence, 2(2), feb 2020. doi: 10.1007/s42484-020-00012-y. URL https://doi.org/10.
1007/s42484-020-00012-y."
REFERENCES,0.8421052631578947,"Ashish Kapoor, Nathan Wiebe, and Krysta Svore.
Quantum perceptron models.
In
D. Lee,
M. Sugiyama,
U. Luxburg,
I. Guyon,
and R. Garnett (eds.),
Advances
in
Neural
Information
Processing
Systems,
volume
29.
Curran
Associates,
Inc.,
2016.
URL
https://proceedings.neurips.cc/paper/2016/file/
d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf."
REFERENCES,0.8473684210526315,"Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep convolu-
tional neural networks, 2019."
REFERENCES,0.8526315789473684,"Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/."
REFERENCES,0.8578947368421053,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, May
2015. ISSN 1476-4687. doi: 10.1038/nature14539. URL https://doi.org/10.1038/
nature14539."
REFERENCES,0.8631578947368421,"YaoChong Li, Ri-Gui Zhou, RuQing Xu, Jia Luo, and WenWen Hu. A quantum deep convolu-
tional neural network for image recognition. Quantum Science and Technology, 5(4):044003, jul
2020. doi: 10.1088/2058-9565/ab9f93. URL https://doi.org/10.1088/2058-9565/
ab9f93."
REFERENCES,0.868421052631579,Under review as a conference paper at ICLR 2022
REFERENCES,0.8736842105263158,"K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Phys. Rev. A, 98:
032309, Sep 2018. doi: 10.1103/PhysRevA.98.032309. URL https://link.aps.org/
doi/10.1103/PhysRevA.98.032309."
REFERENCES,0.8789473684210526,"Alex Monràs, Gael Sentís, and Peter Wittek. Inductive supervised quantum learning. Phys. Rev. Lett.,
118:190503, May 2017. doi: 10.1103/PhysRevLett.118.190503. URL https://link.aps.
org/doi/10.1103/PhysRevLett.118.190503."
REFERENCES,0.8842105263157894,"Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information: 10th
Anniversary Edition. Cambridge University Press, USA, 10th edition, 2011. ISBN 1107002176."
REFERENCES,0.8894736842105263,"Seunghyeok Oh, Jaeho Choi, Jong-Kook Kim, and Joongheon Kim. Quantum convolutional neural
network for resource-efﬁcient image classiﬁcation: A quantum random access memory (qram)
approach. In 2021 International Conference on Information Networking (ICOIN), pp. 50–52, 2021.
doi: 10.1109/ICOIN50884.2021.9333906."
REFERENCES,0.8947368421052632,"Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and Hans J.
Briegel. Quantum speedup for active learning agents. Phys. Rev. X, 4:031002, Jul 2014. doi: 10.
1103/PhysRevX.4.031002. URL https://link.aps.org/doi/10.1103/PhysRevX.
4.031002."
REFERENCES,0.9,"F. Dong P.Q. Le and K. Hirota.
A ﬂexible representation of quantum images for polynomial
preparation, image compression, and processing operations. Quantum Inf Process, 10:63–84, 2011.
doi: 10.1007/s11128-010-0177-y."
REFERENCES,0.9052631578947369,"John Preskill. Quantum computing and the entanglement frontier, 2012."
REFERENCES,0.9105263157894737,Qiskit. 2017. URL https://qiskit.org/textbook/preface.html.
REFERENCES,0.9157894736842105,"S. E. Rasmussen, K. Groenland, R. Gerritsma, K. Schoutens, and N. T. Zinner. Single-step im-
plementation of high-ﬁdelity n-bit toffoli gates. Physical Review A, 101(2), Feb 2020. ISSN
2469-9934. doi: 10.1103/physreva.101.022308. URL http://dx.doi.org/10.1103/
PhysRevA.101.022308."
REFERENCES,0.9210526315789473,"Masahide Sasaki and Alberto Carlini. Quantum learning and universal quantum matching machine.
Phys. Rev. A, 66:022303, Aug 2002. doi: 10.1103/PhysRevA.66.022303. URL https://link.
aps.org/doi/10.1103/PhysRevA.66.022303."
REFERENCES,0.9263157894736842,"Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. The quest for a quantum neural network.
Quantum Information Processing, 13(11):2567–2586, Nov 2014. ISSN 1573-1332. doi: 10.1007/
s11128-014-0809-8. URL https://doi.org/10.1007/s11128-014-0809-8."
REFERENCES,0.9315789473684211,"Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, and Nathan Killoran. Evaluating
analytic gradients on quantum hardware. Physical Review A, 99(3), Mar 2019. ISSN 2469-9934.
doi: 10.1103/physreva.99.032331. URL http://dx.doi.org/10.1103/PhysRevA.99.
032331."
REFERENCES,0.9368421052631579,"Maria Schuld, Alex Bocharov, Krysta M. Svore, and Nathan Wiebe. Circuit-centric quantum
classiﬁers. Physical Review A, 101(3), Mar 2020. ISSN 2469-9934. doi: 10.1103/physreva.101.
032308. URL http://dx.doi.org/10.1103/PhysRevA.101.032308."
REFERENCES,0.9421052631578948,"G. Sentís, J. Calsamiglia, R. Muñoz-Tapia, and E. Bagan. Quantum learning without quantum
memory. Scientiﬁc Reports, 2(1):708, Oct 2012. ISSN 2045-2322. doi: 10.1038/srep00708. URL
https://doi.org/10.1038/srep00708."
REFERENCES,0.9473684210526315,"Gael Sentís, Alex Monràs, Ramon Muñoz Tapia, John Calsamiglia, and Emilio Bagan. Unsupervised
classiﬁcation of quantum data. Phys. Rev. X, 9:041029, Nov 2019. doi: 10.1103/PhysRevX.9.
041029. URL https://link.aps.org/doi/10.1103/PhysRevX.9.041029."
REFERENCES,0.9526315789473684,"Vivek V. Shende and Igor L. Markov. On the cnot-cost of toffoli gates, 2008."
REFERENCES,0.9578947368421052,"Peter W. Shor. Polynomial-time algorithms for prime factorization and discrete logarithms on
a quantum computer.
SIAM Journal on Computing, 26(5):1484–1509, Oct 1997.
ISSN
1095-7111.
doi: 10.1137/s0097539795293172.
URL http://dx.doi.org/10.1137/
S0097539795293172."
REFERENCES,0.9631578947368421,Under review as a conference paper at ICLR 2022
REFERENCES,0.968421052631579,"Daniel R. Simon. On the power of quantum computation. SIAM Journal on Computing, 26(5):
1474–1483, 1997. doi: 10.1137/S0097539796298637. URL https://doi.org/10.1137/
S0097539796298637."
REFERENCES,0.9736842105263158,"Samuel L. Smith, Benoit Dherin, David G. T. Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent. In International Conference on Learning Represen-
tations, volume 29, 2016. URL https://proceedings.neurips.cc/paper/2016/
file/d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf."
REFERENCES,0.9789473684210527,"T. Toffoli. Springer Berlin Heidelberg, Berlin, Heidelberg, 1980."
REFERENCES,0.9842105263157894,"Kwok Ho Wan, Oscar Dahlsten, Hlér Kristjánsson, Robert Gardner, and M. S. Kim. Quantum
generalisation of feedforward neural networks.
npj Quantum Information, 3(1), Sep 2017.
ISSN 2056-6387. doi: 10.1038/s41534-017-0032-4. URL http://dx.doi.org/10.1038/
s41534-017-0032-4."
REFERENCES,0.9894736842105263,SUPPLEMENTARY MATERIAL AND APPENDIX
REFERENCES,0.9947368421052631,Supplementary code submitted as a .zip.
