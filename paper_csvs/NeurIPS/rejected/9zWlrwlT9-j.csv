Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017605633802816902,"This paper considers the missing-labels problem in the extreme multilabel clas-
1"
ABSTRACT,0.0035211267605633804,"siﬁcation (XMC) setting, i.e. a setting with a very large label space. The goal in
2"
ABSTRACT,0.00528169014084507,"XMC often is to maximize either precision or recall of the top-ranked predictions,
3"
ABSTRACT,0.007042253521126761,"which can be achieved by reducing the multilabel problem into a series of binary
4"
ABSTRACT,0.008802816901408451,"(One-vs-All) or multiclass (Pick-all-Labels) problems. Missing labels are a ubiqui-
5"
ABSTRACT,0.01056338028169014,"tous phenomenon in XMC tasks, yet the interaction between missing labels and
6"
ABSTRACT,0.01232394366197183,"multilabel reductions has hitherto only been investigated for the case of One-vs-All
7"
ABSTRACT,0.014084507042253521,"reduction. In this paper, we close this gap by providing unbiased estimates for
8"
ABSTRACT,0.01584507042253521,"general (non-decomposable) multilabel losses, which enables unbiased estimates
9"
ABSTRACT,0.017605633802816902,"of the Pick-all-Labels reduction, as well as the normalized reductions which are
10"
ABSTRACT,0.01936619718309859,"required for consistency with the recall metric. We show that these estimators
11"
ABSTRACT,0.02112676056338028,"suffer from increased variance and may lead to ill-posed optimization problems.
12"
ABSTRACT,0.022887323943661973,"To address this issue, we propose to use convex upper bounds which trade off an
13"
ABSTRACT,0.02464788732394366,"increase in bias against a strong decrease in variance.
14"
INTRODUCTION,0.02640845070422535,"1
Introduction
15"
INTRODUCTION,0.028169014084507043,"Extreme multilabel classiﬁcation (XMC) is a machine learning setting in which the goal is to predict
16"
INTRODUCTION,0.02992957746478873,"a small subset of positive (or relevant) labels for each data instance out of a very large (thousands to
17"
INTRODUCTION,0.03169014084507042,"millions) set of possible labels. Such problems arise for example when annotating large encyclopedia
18"
INTRODUCTION,0.03345070422535211,"[7, 28], in ﬁne-grained image classiﬁcation [9], and next-word prediction [25]. Further applications of
19"
INTRODUCTION,0.035211267605633804,"XMC are recommendation systems, web-advertising and prediction of related searches [1, 29, 17, 6].
20"
INTRODUCTION,0.03697183098591549,"Typical datasets in these scenarios are very large, resulting in possibly billions of (data, label) pairs
21"
INTRODUCTION,0.03873239436619718,"[4], making it impossible for human annotators to check each pair. Even annotating only a few
22"
INTRODUCTION,0.040492957746478875,"samples fully in order to generate a clean test set can be prohibitively expensive. Therefore, both the
23"
INTRODUCTION,0.04225352112676056,"available training- and test-data are likely to contain some errors. Fortunately, in many cases it is
24"
INTRODUCTION,0.04401408450704225,"possible to constrain the structure of the labeling errors. Consider, for example, the case of tagging
25"
INTRODUCTION,0.045774647887323945,"documents: Here, we can assume that each label with which the document has been tagged has been
26"
INTRODUCTION,0.04753521126760563,"deemed relevant by the annotator, and thus is relatively surely a correct label. On the other hand, the
27"
INTRODUCTION,0.04929577464788732,"annotator cannot possibly check hundreds of thousands of negative labels. This leads to the setting
28"
INTRODUCTION,0.051056338028169015,"of missing labels investigated in this paper, in which only positive labels are affected by noise (they
29"
INTRODUCTION,0.0528169014084507,"can go missing), whereas negative labels remain unchanged (no spurious labels). This model has
30"
INTRODUCTION,0.05457746478873239,"been introduced to the XMC setting by Jain et al. [16], along with estimates for the propensities, the
31"
INTRODUCTION,0.056338028169014086,"chance of a relevant label to be observed. Similar models are using in learning-to-rank[20, 27, 37]
32"
INTRODUCTION,0.058098591549295774,"and recommendation systems[32, 14, 15]. For a formal deﬁnition of the setting we refer the reader to
33"
INTRODUCTION,0.05985915492957746,"section 3, and for a more thorough discussion of prior works on missing labels and related settings to
34"
INTRODUCTION,0.061619718309859156,"section 6.
35"
INTRODUCTION,0.06338028169014084,"A common strategy for learning XMC classiﬁers is to reduce the multilabel problem [34] into a series
36"
INTRODUCTION,0.06514084507042253,"of binary [8, 3, 40] or multiclass [18, 38, 31] problems, which then can be solved using existing
37"
INTRODUCTION,0.06690140845070422,"techniques. Such loss reductions can be shown to be consistent for the tasks of maximizing precision
38"
INTRODUCTION,0.06866197183098592,"at k or recall at k, but never both at the same time [24]. For one of these methods, One-vs-All,
39"
INTRODUCTION,0.07042253521126761,"adaptation to the missing labels setting has been shown to yield an improvement in propensity-scored
40"
INTRODUCTION,0.0721830985915493,"precision (an unbiased estimate of precision@k) metrics [30]. The reductions consistent for precision
41"
INTRODUCTION,0.07394366197183098,"lead to loss functions that can be decomposed into a sum of contributions from each label, which
42"
INTRODUCTION,0.07570422535211267,"means the results of Natarajan et al. [26] can be applied. In contrast, the reductions consistent for
43"
INTRODUCTION,0.07746478873239436,"recall contain a normalization term that is the inverse of the total number of true labels. This term is
44"
INTRODUCTION,0.07922535211267606,"also necessary for calculating the recall metric itself, demonstrating the need for unbiased estimates
45"
INTRODUCTION,0.08098591549295775,"for true, non-decomposable multilabel loss functions.
46"
INTRODUCTION,0.08274647887323944,"Contributions
Our contributions are 1) A mathematical model of the missing labels setting that
47"
INTRODUCTION,0.08450704225352113,"describes the observed labels as a product of an (unknown) mask variable with the true labels.
48"
INTRODUCTION,0.08626760563380281,"Crucially, this mask can be chosen to be independent of the labels (Theorem 1), enabling simple
49"
INTRODUCTION,0.0880281690140845,"proofs for our theorems. 2) The unique unbiased estimate (Theorems 2, 3) for arbitrary multilabel
50"
INTRODUCTION,0.0897887323943662,"losses, and in particular for the loss functions arising from multilabel reductions. The unbiased
51"
INTRODUCTION,0.09154929577464789,"estimate of a lower-bounded loss need not be lower-bounded, and even for bounded losses the
52"
INTRODUCTION,0.09330985915492958,"unbiased estimate leads to an increase in variance. Therefore, we develop 3) a convex upper-bound
53"
INTRODUCTION,0.09507042253521127,"(Theorem 4) for losses based on the normalized Pick-all-Labels reduction. In the missing-labels
54"
INTRODUCTION,0.09683098591549295,"setting, the generalization error is composed of two contributions: the error due to overﬁtting to
55"
INTRODUCTION,0.09859154929577464,"the speciﬁc, observed noise-pattern, and the error because only a ﬁnite sample has been observed.
56"
INTRODUCTION,0.10035211267605634,"We present empirical evidence 4) that the former can be much stronger than the latter, and may be
57"
INTRODUCTION,0.10211267605633803,"reduced by switching to the upper bounds.
58"
INTRODUCTION,0.10387323943661972,"In the main paper, we provide shortened proofs that illustrate the key steps. Detailed step-by-step
59"
INTRODUCTION,0.1056338028169014,"proofs can be found in the appendix.
60"
INTRODUCTION,0.1073943661971831,"Notation
Random variables will be denoted by capital letters X, Y, . . ., whereas calligraphic letters
61"
INTRODUCTION,0.10915492957746478,"denote sets and lower case letters their elements, x ∈X, . . .. Vectors will be denoted by bold font,
62"
INTRODUCTION,0.11091549295774648,"y ∈Y, if we plan to make use of the fact that they can be decomposed into components y1, . . . , yk,
63"
INTRODUCTION,0.11267605633802817,"with y¬k denoting the vector of all components except the k’th. The letters f, g, h and ℓare reserved
64"
INTRODUCTION,0.11443661971830986,"for functions, i, j, k denote integers, [k] is the set {1, . . . , k}. We denote with X the data space,
65"
INTRODUCTION,0.11619718309859155,"Y = {0, 1}l the label space and ˆY = Rl the prediction space. A dataset is deﬁned through the three
66"
INTRODUCTION,0.11795774647887323,"random variables X ∈X, Y ∈Y, and Y∗∈Y, that represent the data, observed label, and ground
67"
INTRODUCTION,0.11971830985915492,"truth label. We mark quantities pertaining to the unobservable ground-truth with a superscript star
68"
INTRODUCTION,0.12147887323943662,"and call (X, Y∗) the clean data.
69"
MULTILABEL REDUCTIONS,0.12323943661971831,"2
Multilabel Reductions
70"
MULTILABEL REDUCTIONS,0.125,"In Menon et al. [24], ﬁve different reductions for turning the multilabel learning problem into a sum of
71"
MULTILABEL REDUCTIONS,0.1267605633802817,"binary or multiclass problems are presented (cf. appendix). In the following, let ℓBC : {0, 1}×R −→
72"
MULTILABEL REDUCTIONS,0.12852112676056338,"R be a binary loss and ℓMC : [l] × Rl −→R be a multiclass loss. Below, we present four of those
73"
MULTILABEL REDUCTIONS,0.13028169014084506,"reductions, and rearrange their loss functions so that a common pattern emerges.
74"
MULTILABEL REDUCTIONS,0.13204225352112675,"For one-vs-all (OVA) reduction, each label is considered independently, meaning that for each
75"
MULTILABEL REDUCTIONS,0.13380281690140844,"instance l binary problems are to be solved. This leads to a loss function
76"
MULTILABEL REDUCTIONS,0.13556338028169015,"ℓ∗
OvA(y∗, ˆy) = l
X"
MULTILABEL REDUCTIONS,0.13732394366197184,"j=1
ℓBC(y∗
j , ˆyj) = l
X"
MULTILABEL REDUCTIONS,0.13908450704225353,"j=1
y∗
j (ℓBC(1, ˆyj) −ℓBC(0, ˆyj)) + ℓBC(0, ˆyj).
(1)"
MULTILABEL REDUCTIONS,0.14084507042253522,"In contrast, pick-all-labels (PAL) considers all the positive labels for each instance and tries to
77"
MULTILABEL REDUCTIONS,0.1426056338028169,"minimize their corresponding multiclass loss, leading to
78"
MULTILABEL REDUCTIONS,0.1443661971830986,"ℓ∗
PAL(y∗, ˆy) =
X"
MULTILABEL REDUCTIONS,0.14612676056338028,"j:y∗
j =1
ℓMC(j, ˆy) =
X"
MULTILABEL REDUCTIONS,0.14788732394366197,"j∈[l]
y∗
j ℓMC(j, ˆy).
(2)"
MULTILABEL REDUCTIONS,0.14964788732394366,"Both approaches are consistent for precision at k. In order to make the reductions consistent for recall
79"
MULTILABEL REDUCTIONS,0.15140845070422534,"instead of precision, the label value needs to be replaced with a normalized label
80"
MULTILABEL REDUCTIONS,0.15316901408450703,"˜y∗
j :=
y∗
j
Pl
i=1 y∗
i
=
y∗
j
1 + Pl
i̸=j y∗
i
,
(3)"
MULTILABEL REDUCTIONS,0.15492957746478872,"where the expression on the right has the advantage of being well deﬁned even if there are no positives
81"
MULTILABEL REDUCTIONS,0.15669014084507044,"for the sample. This leads to the OVA-N and PAL-N reductions. By moving label-independent parts
82"
MULTILABEL REDUCTIONS,0.15845070422535212,"into functions f and gj, the reductions get a common structure
83"
MULTILABEL REDUCTIONS,0.1602112676056338,"ℓ∗(y∗, ˆy) = f(ˆy) + l
X"
MULTILABEL REDUCTIONS,0.1619718309859155,"j=1
z∗
j gj(ˆy),
(4)"
MULTILABEL REDUCTIONS,0.1637323943661972,"where zj = ˜y∗
j for the normalized reductions and z∗
j = y∗
j otherwise. The functions f and gj are the
84"
MULTILABEL REDUCTIONS,0.16549295774647887,"same for the normalized and regular reduction (see appendix).
85"
UNBIASED ESTIMATES WITH MISSING LABELS,0.16725352112676056,"3
Unbiased Estimates with Missing Labels
86"
UNBIASED ESTIMATES WITH MISSING LABELS,0.16901408450704225,"We are interested in noisy labels where the noise is such that labels can only go missing. This is
87"
UNBIASED ESTIMATES WITH MISSING LABELS,0.17077464788732394,"described by the next two deﬁnitions, where the ﬁrst gives a phenomenological characterization of
88"
UNBIASED ESTIMATES WITH MISSING LABELS,0.17253521126760563,"the setting, whereas the second deﬁnes the mathematical model used to describe it. For this setting
89"
UNBIASED ESTIMATES WITH MISSING LABELS,0.1742957746478873,"we then develop unbiased estimates for the preceding loss reductions, in the sense that for a given
90"
UNBIASED ESTIMATES WITH MISSING LABELS,0.176056338028169,"loss ℓ∗we are looking for a new loss function ℓsuch that E
h
ℓ(Y, ˆY)
i
= E
h
ℓ∗(Y∗, ˆY)
i
.
91"
UNBIASED ESTIMATES WITH MISSING LABELS,0.17781690140845072,"Deﬁnition 1 (Propensity). The missing-labels setting we described informally in the introduction
92"
UNBIASED ESTIMATES WITH MISSING LABELS,0.1795774647887324,"leads to the following conditions on the l random variables
93"
UNBIASED ESTIMATES WITH MISSING LABELS,0.1813380281690141,"P

Yj = 1 | Y ∗
j = 1, Y∗
¬j, X
	
=: pj(X),
P

Yj = 1 | Y ∗
j = 0, Y∗
¬j, X
	
= 0
(5)"
UNBIASED ESTIMATES WITH MISSING LABELS,0.18309859154929578,"The value pj(x) ∈(0, 1] is called the propensity of the label j at point x.
94"
UNBIASED ESTIMATES WITH MISSING LABELS,0.18485915492957747,"Such propensity models have been used in extreme classiﬁcation [30, 16, 39], learning-to-rank
95"
UNBIASED ESTIMATES WITH MISSING LABELS,0.18661971830985916,"[20, 27, 37], and recommendation systems [32, 14, 15].
96"
UNBIASED ESTIMATES WITH MISSING LABELS,0.18838028169014084,"The following proposition guarantees that a ﬁxed-propensity unbiased estimator can be used to
97"
UNBIASED ESTIMATES WITH MISSING LABELS,0.19014084507042253,"construct a instance-dependent unbiased estimator
98"
UNBIASED ESTIMATES WITH MISSING LABELS,0.19190140845070422,"Proposition 1. Let f ∗(X, Y ∗) be some function such that for ﬁxed propensity p, an unbiased
99"
UNBIASED ESTIMATES WITH MISSING LABELS,0.1936619718309859,"estimate is given by fp, i.e. E[fp(X, Y )] = E[f ∗(X, Y ∗)]. For instance-dependent propensity p(x),
100"
UNBIASED ESTIMATES WITH MISSING LABELS,0.1954225352112676,"an unbiased estimator of f ∗is given by fp(X).
101"
UNBIASED ESTIMATES WITH MISSING LABELS,0.19718309859154928,"Proof. Using the law of total expectation gives
102"
UNBIASED ESTIMATES WITH MISSING LABELS,0.198943661971831,"E[f ∗(X, Y ∗)] = E[E[f ∗(X, Y ∗) | X]] = E

E

fp(X)(X, Y ∗) | X

= E

fp(X)(X, Y ∗)

."
UNBIASED ESTIMATES WITH MISSING LABELS,0.2007042253521127,"Therefore, we will supress the dependence of the propensity on the data point in the rest of the paper.
103"
UNBIASED ESTIMATES WITH MISSING LABELS,0.20246478873239437,"The relation between Y∗and Y can be modeled by a set of independent mask variables M:
104"
UNBIASED ESTIMATES WITH MISSING LABELS,0.20422535211267606,"Theorem 1 (Masking Model). Assuming Y∗and Y follow Deﬁnition 1, then then there exists a
105"
UNBIASED ESTIMATES WITH MISSING LABELS,0.20598591549295775,"random variable M ∈{0, 1}l such that Y = M ⊙Y∗almost surely and Mj is independent of
106"
UNBIASED ESTIMATES WITH MISSING LABELS,0.20774647887323944,"(Y∗, X, M¬j) for all j ∈[l]. It holds that E[Mj] = pj.
107"
UNBIASED ESTIMATES WITH MISSING LABELS,0.20950704225352113,"This can be seen as a multilabel generalization of the similar statement given in Teisseyre et al. [33].
108"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2112676056338028,"The independent variables M provide a convenient framework for proving the results that follow,
109"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2130281690140845,"because the independence allows to factorize expectations containing M.
110"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2147887323943662,"Proposition 2 (Unbiased Estimate for Decomposable Reductions). Assume the setting of Deﬁnition 1,
111"
UNBIASED ESTIMATES WITH MISSING LABELS,0.21654929577464788,"with the additional condition that the predictions ˆY are independent of the missing mask M. Then
112"
UNBIASED ESTIMATES WITH MISSING LABELS,0.21830985915492956,"the unbiased estimate for the loss (4) with z = y, denoted by ℓ= P(ℓ∗), is given by
113"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22007042253521128,"ℓ(y, ˆy) = f(ˆy) + l
X j=1"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22183098591549297,"yj
pj
gj(ˆy).
(6)"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22359154929577466,"The predictions have to be independent of the locations M where the labels go missing. This is
114"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22535211267605634,"fulﬁlled if the predictions ˆY = h(X, W) are the output of a classiﬁer h whose weights W are
115"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22711267605633803,"independent of M.1
116"
UNBIASED ESTIMATES WITH MISSING LABELS,0.22887323943661972,"For the normalized reductions, it would sufﬁce to ﬁnd an unbiased estimate of ˜Y in order to apply
117"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2306338028169014,"the same argument as above. However, we are not aware of a derivation for such an estimate that is
118"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2323943661971831,"simpler than the fully generic case presented below.
119"
UNBIASED ESTIMATES WITH MISSING LABELS,0.23415492957746478,"Theorem 2 (Unbiased Estimate for Non-Decomposable Loss). For a generic multilabel loss function
120"
UNBIASED ESTIMATES WITH MISSING LABELS,0.23591549295774647,"ℓ∗, the unbiased estimate ℓ= P(ℓ∗) under the conditions of Theorem 2 is given by
121"
UNBIASED ESTIMATES WITH MISSING LABELS,0.23767605633802816,"ℓ(y, ˆy) =
X y′⪯y Y"
UNBIASED ESTIMATES WITH MISSING LABELS,0.23943661971830985,j:yj=1
UNBIASED ESTIMATES WITH MISSING LABELS,0.24119718309859156,"y′
j(2 −pj) + pj −1 pj"
UNBIASED ESTIMATES WITH MISSING LABELS,0.24295774647887325,"
ℓ∗(y′, ˆy),
(7)"
UNBIASED ESTIMATES WITH MISSING LABELS,0.24471830985915494,"where y′ ⪯y means {0, 1} ∋y′
j ≤yj.
122"
UNBIASED ESTIMATES WITH MISSING LABELS,0.24647887323943662,"This means that for an instance with k positive labels, we need 2k evaluations of the original loss
123"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2482394366197183,"function in order to calculate the unbiased estimate. This is only feasible because, despite having a
124"
UNBIASED ESTIMATES WITH MISSING LABELS,0.25,"very large label space, typical extreme-classiﬁcation datasets have only few positives per instance.
125"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2517605633802817,"Unfortunately, the division by (products of) propensity values means that the unbiased estimates will
126"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2535211267605634,"have much larger variance than the original loss function would have on clean data. As an illustrative
127"
UNBIASED ESTIMATES WITH MISSING LABELS,0.25528169014084506,"example, consider the binary case in the limit p ≪1. We can show that in this case the variance
128"
UNBIASED ESTIMATES WITH MISSING LABELS,0.25704225352112675,"grows with p−1 compared to the evaluation on clean data.
129"
UNBIASED ESTIMATES WITH MISSING LABELS,0.25880281690140844,"Proposition 3 (Increase in Variance). Setting q∗:= E[Y ∗] and ℓ= P(ℓ∗), for small propensities
130"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2605633802816901,"p ≪1, the variance increases with the inverse of the propensity, V[ℓ(Y, ˆy)] ≈
1
p(1−q∗) V[ℓ∗(Y ∗, ˆy)] .
131"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2623239436619718,"This means that in the binary case the variance increases linearly with inverse propensity. In the
132"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2640845070422535,"multilabel case, this is ampliﬁed further due to the product of propensities.
133"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2658450704225352,"The result above raises the question whether there might be other unbiased estimators with reduced
134"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2676056338028169,"variance. For example, the conditional expectation E[ℓ∗(Y ∗, X)|Y ] also gives an unbiased estimate
135"
UNBIASED ESTIMATES WITH MISSING LABELS,0.26936619718309857,"with lower variance, but cannot be calculated without knowledge of the conditional probabilities
136"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2711267605633803,"P{Y | X}. The following theorem states that ℓ= P(ℓ∗) is unique if we want the loss function to
137"
UNBIASED ESTIMATES WITH MISSING LABELS,0.272887323943662,"work for all possible distributions of data. Thus we cannot reduce the variance.
138"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2746478873239437,"Theorem 3 (Uniqueness). Let pj ∈(0, 1] ∀j ∈[l]. For an arbitrary loss function ℓ∗, let ℓand ℓ′ be
139"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2764084507042254,"unbiased versions, in the sense that for all X, Y, Y∗that fulﬁll the masking model Theorem 1 with
140"
UNBIASED ESTIMATES WITH MISSING LABELS,0.27816901408450706,"propensity p, it holds
141"
UNBIASED ESTIMATES WITH MISSING LABELS,0.27992957746478875,"E[ℓ∗(Y∗, X)] = E[ℓ(Y, X)] = E[ℓ′(Y, X)] .
(8)
Then, ℓ′ = ℓ.
142"
UNBIASED ESTIMATES WITH MISSING LABELS,0.28169014084507044,"The unavoidable increase in variance indicates that there might be a bias-variance trade-off between
143"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2834507042253521,"using the unbiased loss that may overﬁt more strongly on the observed noise, and using the original
144"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2852112676056338,"loss function which gives wrong results even if n →∞. If one calculates a standard Rademacher
145"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2869718309859155,"bound for generalization (see appendix), this error bound increases with a factor 2−p"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2887323943661972,"p . 2
146"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2904929577464789,"In a classical learning setup, the generalization error would be described by the difference between
147"
UNBIASED ESTIMATES WITH MISSING LABELS,0.29225352112676056,"the empirical risk and the true risk ˆR∗
ℓ∗
h
ˆh
i
−R∗
ℓ∗
h
ˆh
i
. However, in the case of missing labels, this
148"
UNBIASED ESTIMATES WITH MISSING LABELS,0.29401408450704225,"can be decomposed in two ways
149"
UNBIASED ESTIMATES WITH MISSING LABELS,0.29577464788732394,"R∗
ℓ∗[h] −ˆRℓ[h] ="
UNBIASED ESTIMATES WITH MISSING LABELS,0.2975352112676056,"= 0
z
}|
{
R∗
ℓ∗[h] −Rℓ[h] + Rℓ[h] −ˆRℓ[h]
(9)"
UNBIASED ESTIMATES WITH MISSING LABELS,0.2992957746478873,"= R∗
ℓ∗[h] −ˆR∗
ℓ∗[h]
|
{z
}
ﬁnite sample"
UNBIASED ESTIMATES WITH MISSING LABELS,0.301056338028169,"+ ˆR∗
ℓ∗[h] −ˆRℓ[h]
|
{z
}
noise pattern"
UNBIASED ESTIMATES WITH MISSING LABELS,0.3028169014084507,",
(10)"
UNBIASED ESTIMATES WITH MISSING LABELS,0.3045774647887324,"Whereas the ﬁrst equation is just a restatement of the unbiasedness, the second contains some new
150"
UNBIASED ESTIMATES WITH MISSING LABELS,0.30633802816901406,"insight: The generalization error can be decomposed into the difference between the true risk R∗
ℓ∗[h]
151"
UNBIASED ESTIMATES WITH MISSING LABELS,0.30809859154929575,"1In this sense, we will use the notation ℓ(y, x) to evaluate a loss also on a data point.
2The bound in this paper corresponds to Natarajan et al. [26, Thm. 9], though that published result is wrong
and missing the increase in the bound due to the increased range of the function."
UNBIASED ESTIMATES WITH MISSING LABELS,0.30985915492957744,"and the empirical risk on clean training data ˆR∗
ℓ∗[h], and the difference between that and the estimated
152"
UNBIASED ESTIMATES WITH MISSING LABELS,0.31161971830985913,"empirical risk on observed data ˆRℓ[h]. Because the classiﬁer h depends (through Y = M ⊙Y ∗) on
153"
UNBIASED ESTIMATES WITH MISSING LABELS,0.31338028169014087,"the mask variables, ℓdoes not give an unbiased estimate (on training data) and thus the second term
154"
UNBIASED ESTIMATES WITH MISSING LABELS,0.31514084507042256,"is non-zero even in expectation. In fact, in the low-regularization regime this term may dominate the
155"
UNBIASED ESTIMATES WITH MISSING LABELS,0.31690140845070425,"entire error, as we will demonstrate in section 5.
156"
CONVEX UPPER-BOUNDS,0.31866197183098594,"4
Convex Upper-Bounds
157"
CONVEX UPPER-BOUNDS,0.3204225352112676,"The unbiased estimate allows us to calculate the loss even on data with missing labels, but can we
158"
CONVEX UPPER-BOUNDS,0.3221830985915493,"also use it for training? Ideally, the loss function should be lower-bounded, so the minimization is
159"
CONVEX UPPER-BOUNDS,0.323943661971831,"well deﬁned, it should be convex so the minimum is unique. Further, the variance of the unbiased
160"
CONVEX UPPER-BOUNDS,0.3257042253521127,"estimator should not be too large, so that a reasonable amount of training samples is sufﬁcient.
161"
CONVEX UPPER-BOUNDS,0.3274647887323944,"If we assume ℓBC and ℓMC to be lower-bounded and convex, then only the PAL-reduction results in
162"
CONVEX UPPER-BOUNDS,0.32922535211267606,"an unbiased estimate that is guaranteed to have the same properties, as it is a positive combination
163"
CONVEX UPPER-BOUNDS,0.33098591549295775,"of ℓMC. Due to the uniqueness result, it is not possible to ﬁnd an unbiased estimate that is always
164"
CONVEX UPPER-BOUNDS,0.33274647887323944,"convex for the other reductions. Thus, in order to make them amenable for training, we propose to
165"
CONVEX UPPER-BOUNDS,0.3345070422535211,"switch from unbiased estimates to convex upper-bounds. Below we present solutions for the OvA
166"
CONVEX UPPER-BOUNDS,0.3362676056338028,"and normalized PAL-reduction. The normalized OVA-reduction remains an open problem.
167"
CONVEX UPPER-BOUNDS,0.3380281690140845,"Upper-Bound for OvA-Reduction
The OvA-reduction is based on a binary loss, which often
168"
CONVEX UPPER-BOUNDS,0.3397887323943662,"is a convex surrogate for the 0-1 loss. To get a convex loss in the missing-labels case, we thus
169"
CONVEX UPPER-BOUNDS,0.3415492957746479,"switch the order of operations [30, 5]: Instead of taking an unbiased estimate of a convex surrogate,
170"
CONVEX UPPER-BOUNDS,0.34330985915492956,"we form a convex surrogate of an unbiased estimate. Taking θ to be a thresholding function (e.g.
171"
CONVEX UPPER-BOUNDS,0.34507042253521125,"θ(s) = 1{s > 0}), the 0-1-loss can be written as
172"
CONVEX UPPER-BOUNDS,0.34683098591549294,"ℓ∗
0−1(y, ˆy) = yθ(ˆy) + (1 −y)(1 −θ(ˆy))
(11)
with unbiased estimate
173"
CONVEX UPPER-BOUNDS,0.3485915492957746,"ℓ0−1(y, ˆy) =
 2"
CONVEX UPPER-BOUNDS,0.3503521126760563,"pj
−1

yθ(ˆy) + (1 −y)(1 −θ(ˆy)) + y
pj −1 pj"
CONVEX UPPER-BOUNDS,0.352112676056338,"
.
(12)"
CONVEX UPPER-BOUNDS,0.3538732394366197,"As the last term does not depend on the predictions, it can be dropped for an optimization objective.
174"
CONVEX UPPER-BOUNDS,0.35563380281690143,"If ℓBC(1, ˆy) is a convex upper-bound on θ(ˆy) and ℓBC(0, ˆy) on (1 −θ(ˆy)), so that overall ℓBC is a
175"
CONVEX UPPER-BOUNDS,0.3573943661971831,"convex upper-bound on the 0-1 loss, then performing these substitutions gives a convex loss function
176"
CONVEX UPPER-BOUNDS,0.3591549295774648,"for the OvA-reduction:
177"
CONVEX UPPER-BOUNDS,0.3609154929577465,"˜ℓOvA(y, ˆy) = l
X j=1  2"
CONVEX UPPER-BOUNDS,0.3626760563380282,"pj
−1

yjℓBC(1, ˆyj) + (1 −yj)ℓBC(0, ˆyj)
(13)"
CONVEX UPPER-BOUNDS,0.3644366197183099,"Upper-Bound for Normalized PAL-Reduction
We have formulated the normalized multilabel
178"
CONVEX UPPER-BOUNDS,0.36619718309859156,"reductions in terms of the variable ˜Y ∗. A naive attempt of correcting for the noisy labels by replacing
179"
CONVEX UPPER-BOUNDS,0.36795774647887325,"Y ∗with Y/p is not unbiased. However, the resulting estimator ˜Y turns out to be an upper bound.
180"
CONVEX UPPER-BOUNDS,0.36971830985915494,"The two estimators are given by
181"
CONVEX UPPER-BOUNDS,0.3714788732394366,"˜Y ∗
i =
Y ∗
i
1 + P"
CONVEX UPPER-BOUNDS,0.3732394366197183,"j̸=i Y ∗
j
,
˜Yi :=
Yi/pi
1 + P"
CONVEX UPPER-BOUNDS,0.375,"j̸=i Yj/pj
.
(14)"
CONVEX UPPER-BOUNDS,0.3767605633802817,"Theorem 4 (Normalized Label Upper-Bound). Under the conditions of Theorem 2, replacing the
182"
CONVEX UPPER-BOUNDS,0.3785211267605634,"true label with the unbiased estimate of the observed label as shown in Equation 14 results in an
183"
CONVEX UPPER-BOUNDS,0.38028169014084506,"upper bound, whose error itself can be bounded by a data-dependent term
184"
CONVEX UPPER-BOUNDS,0.38204225352112675,"E
h
˜Y ∗
i
i
+
X j̸=i"
CONVEX UPPER-BOUNDS,0.38380281690140844,1 −pj pj
CONVEX UPPER-BOUNDS,0.3855633802816901,"
E
Yi"
CONVEX UPPER-BOUNDS,0.3873239436619718,"pi
· Yj pj"
CONVEX UPPER-BOUNDS,0.3890845070422535,"
≥E
h
˜Yi
i
≥E
h
˜Y ∗
i
i
.
(15)"
CONVEX UPPER-BOUNDS,0.3908450704225352,"Proof. For convenience denote S∗
i := P"
CONVEX UPPER-BOUNDS,0.3926056338028169,"j̸=i Y ∗
j and Si := P"
CONVEX UPPER-BOUNDS,0.39436619718309857,"j̸=i Yj/pj, and note that Si is indepen-
185"
CONVEX UPPER-BOUNDS,0.3961267605633803,"dent of Mi. By pulling out known factors and using the independence of M and Y∗we can show
186"
CONVEX UPPER-BOUNDS,0.397887323943662,"that
187"
CONVEX UPPER-BOUNDS,0.3996478873239437,"E[Si | Y∗] =
X"
CONVEX UPPER-BOUNDS,0.4014084507042254,"j̸=i
E

MjY ∗
j /pj | Y∗
=
X"
CONVEX UPPER-BOUNDS,0.40316901408450706,"j̸=i
Y ∗
j E[Mj/pj | Y∗] = S∗
i .
(16)"
CONVEX UPPER-BOUNDS,0.40492957746478875,"Expanding terms and using independence of Mi, then applying the tower property and pulling out
188"
CONVEX UPPER-BOUNDS,0.40669014084507044,"the measurable factor results in
189"
CONVEX UPPER-BOUNDS,0.4084507042253521,"E
h
˜Yi
i
= E
MiY ∗
i /pi
1 + Si"
CONVEX UPPER-BOUNDS,0.4102112676056338,"
= E
Mi pi"
CONVEX UPPER-BOUNDS,0.4119718309859155,"
E

Y ∗
i
1 + Si"
CONVEX UPPER-BOUNDS,0.4137323943661972,"
= E

E

Y ∗
i
1 + Si"
CONVEX UPPER-BOUNDS,0.4154929577464789,"Y∗

= E

Y ∗
i E

1
1 + Si"
CONVEX UPPER-BOUNDS,0.41725352112676056,"Y∗

."
CONVEX UPPER-BOUNDS,0.41901408450704225,"The function h : R≥0 −→R given by t 7→1/(1 + x) is convex, because its second derivative is
2(1 + t)−3, which is larger than zero for non-negative t. Because Si ≥0 almost surely, we can apply
Jensen’s inequality to the inner expectation and use (16)
190"
CONVEX UPPER-BOUNDS,0.42077464788732394,"E
h
˜Yi
i
≥E

Y ∗
i
1 + E[Si | Y∗]"
CONVEX UPPER-BOUNDS,0.4225352112676056,"
= E

Y ∗
i
1 + S∗
i"
CONVEX UPPER-BOUNDS,0.4242957746478873,"
= E
h
˜Y ∗
i
i
."
CONVEX UPPER-BOUNDS,0.426056338028169,"On the other hand, we can use the Taylor formula with intermediate point ζ ∈[Si, S∗
i ] to expand
191"
CONVEX UPPER-BOUNDS,0.4278169014084507,"1
1 + Si
=
1
1 + S∗
i
−Si −S∗
i
(1 + S∗
i )2 + (Si −S∗
i )2"
CONVEX UPPER-BOUNDS,0.4295774647887324,"(1 + ζ)3 .
(17)"
CONVEX UPPER-BOUNDS,0.43133802816901406,"Using ζ ≥0 to bound the denominator, then multiplying with Y ∗
i and taking the expectation gives
192"
CONVEX UPPER-BOUNDS,0.43309859154929575,"E

Y ∗
i
1 + Si"
CONVEX UPPER-BOUNDS,0.43485915492957744,"
≤E

Y ∗
i
1 + S∗
i"
CONVEX UPPER-BOUNDS,0.43661971830985913,"
+ E

Y ∗
i (Si −S∗
i )2
.
(18)"
CONVEX UPPER-BOUNDS,0.43838028169014087,"The variance term can be calculated by substituting Si and S∗
i , expanding the sum, and using the
193"
CONVEX UPPER-BOUNDS,0.44014084507042256,"independence of M to show that the mixed terms are zero:
194"
CONVEX UPPER-BOUNDS,0.44190140845070425,"E

Y ∗
i (Si −S∗
i )2
= E "
CONVEX UPPER-BOUNDS,0.44366197183098594,"Y ∗
i  X"
CONVEX UPPER-BOUNDS,0.4454225352112676,"j̸=i
Y ∗
j Mj"
CONVEX UPPER-BOUNDS,0.4471830985915493,"pj
−1
  2  =
X"
CONVEX UPPER-BOUNDS,0.448943661971831,"j̸=i
E """
CONVEX UPPER-BOUNDS,0.4507042253521127,"Y ∗
i (Y ∗
j )2
Mj"
CONVEX UPPER-BOUNDS,0.4524647887323944,"pj
−1
2# +
X j̸=i X"
CONVEX UPPER-BOUNDS,0.45422535211267606,"k/∈{i,j}
E

Y ∗
i Y ∗
j Y ∗
k

E
Mj"
CONVEX UPPER-BOUNDS,0.45598591549295775,"pj
−1

E
Mk"
CONVEX UPPER-BOUNDS,0.45774647887323944,"pk
−1
 =
X"
CONVEX UPPER-BOUNDS,0.4595070422535211,"j̸=i
E

Y ∗
i Y ∗
j

E ""
Mj"
CONVEX UPPER-BOUNDS,0.4612676056338028,"p2
j
−2Mj"
CONVEX UPPER-BOUNDS,0.4630281690140845,"pj
+ 1 # =
X j̸=i"
CONVEX UPPER-BOUNDS,0.4647887323943662,1 −pj pj
CONVEX UPPER-BOUNDS,0.4665492957746479,"
E
Yi"
CONVEX UPPER-BOUNDS,0.46830985915492956,"pi
· Yj pj"
CONVEX UPPER-BOUNDS,0.47007042253521125,"
.
(19)"
CONVEX UPPER-BOUNDS,0.47183098591549294,"Table 1: Error bound for XMC datasets
Dataset
Average
Worst Case"
CONVEX UPPER-BOUNDS,0.4735915492957746,"Eurlex-4K
0.02
0.51
AmazonCat-13K
0.0006
0.24"
CONVEX UPPER-BOUNDS,0.4753521126760563,"Note that the transformation of equation (3) was
195"
CONVEX UPPER-BOUNDS,0.477112676056338,"crucial for this calculation, because it makes the
196"
CONVEX UPPER-BOUNDS,0.4788732394366197,"mask variables in the numerator and denomina-
197"
CONVEX UPPER-BOUNDS,0.48063380281690143,"tor independent.
198"
CONVEX UPPER-BOUNDS,0.4823943661971831,"In practice, most entries of the co-occurrence
199"
CONVEX UPPER-BOUNDS,0.4841549295774648,"matrix E[Yi · Yj] will be extremely small, caus-
200"
CONVEX UPPER-BOUNDS,0.4859154929577465,"ing only a minute contribution to the error bound. This can be illustrated by calculating, on two real
201"
CONVEX UPPER-BOUNDS,0.4876760563380282,"datasets, the upper-bound for the error of the proposed estimator, by approximating E[Yi · Yj] with
202"
CONVEX UPPER-BOUNDS,0.4894366197183099,"the label co-occurrence frequency. The propensities are estimated as in Jain et al. [16]. Looking at
203"
CONVEX UPPER-BOUNDS,0.49119718309859156,"the mean value, and the worst case for any label (Table 1), We can see that the error on average is
204"
CONVEX UPPER-BOUNDS,0.49295774647887325,"very small, indicating that the worst-case bound only applies to very few labels.
205"
CONVEX UPPER-BOUNDS,0.49471830985915494,"Corollary 1 (PAL Upper-Bound). Under the assumptions of Theorem 2, if the underlying multiclass
206"
CONVEX UPPER-BOUNDS,0.4964788732394366,"loss ℓMC is a non-negative convex function, the expression
207"
CONVEX UPPER-BOUNDS,0.4982394366197183,"˜ℓ(y, ˆy) := l
X j=1"
CONVEX UPPER-BOUNDS,0.5,"yi/pi
1 + P"
CONVEX UPPER-BOUNDS,0.5017605633802817,"j̸=i yj/pj
ℓMC(j, ˆy)
(20)"
CONVEX UPPER-BOUNDS,0.5035211267605634,"gives a nonnegative, convex upper-bound on the true normalized PAL loss in expectation.
208"
EXPERIMENTAL RESULTS,0.5052816901408451,"5
Experimental Results
209"
EXPERIMENTAL RESULTS,0.5070422535211268,"In this section we present some empirical evidence that illustrates the inﬂuence of missing labels and
210"
EXPERIMENTAL RESULTS,0.5088028169014085,"the unbiased estimates and upper bounds on overﬁtting and bias-variance trade-off. Additional results
211"
EXPERIMENTAL RESULTS,0.5105633802816901,"and a more detailed description of the procedure can be found in the appendix.
212"
EXPERIMENTAL RESULTS,0.5123239436619719,"0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 5 10 15 20"
EXPERIMENTAL RESULTS,0.5140845070422535,propensity
EXPERIMENTAL RESULTS,0.5158450704225352,Recall@1
EXPERIMENTAL RESULTS,0.5176056338028169,"unbiased
upper-bound
standard
true value"
EXPERIMENTAL RESULTS,0.5193661971830986,"0.5
0.55
0.6
10.7 11 11.5"
EXPERIMENTAL RESULTS,0.5211267605633803,"Figure 1: Unbiased estimate of per-example recall with artiﬁcial data as described in the main text.
The shaded region corresponds to one standard deviation, estimated over 100 repetitions. The black
line denotes the true recall."
EXPERIMENTAL RESULTS,0.522887323943662,"Prediction Setting
First, we want to demonstrate the variance problem in a simple prediction
213"
EXPERIMENTAL RESULTS,0.5246478873239436,"setting, where the classiﬁer is ﬁxed and we want to determine its performance. Consider a setting
214"
EXPERIMENTAL RESULTS,0.5264084507042254,"in which there are 100 different labels, which are independent and each has a probability of 10%.
215"
EXPERIMENTAL RESULTS,0.528169014084507,"We randomly draw 10 000 ground-truth label vectors, and generate observed labels by removing
216"
EXPERIMENTAL RESULTS,0.5299295774647887,"according to a propensity p that is identical for all labels. The predictions are generated by randomly
217"
EXPERIMENTAL RESULTS,0.5316901408450704,"choosing a label from the ground-truth. We calculate the average per-example recall using the
218"
EXPERIMENTAL RESULTS,0.5334507042253521,"standard estimator, the unbiased estimator, and the upper bound, and plot the results in Figure 1.
219"
EXPERIMENTAL RESULTS,0.5352112676056338,"As can be seen, for moderate propensities the unbiased estimator works well, but for propensities
220"
EXPERIMENTAL RESULTS,0.5369718309859155,"below 0.45 the 10 000 samples are not sufﬁcient to get an accurate estimate. In this setting, the
221"
EXPERIMENTAL RESULTS,0.5387323943661971,"upper-bound results in a larger error than using the standard estimator.
222"
EXPERIMENTAL RESULTS,0.5404929577464789,"Training Setting
Ideally, we would benchmark our loss functions on a real XMC task. However,
223"
EXPERIMENTAL RESULTS,0.5422535211267606,"for those we neither know the exact propensities, nor can we validate that the unbiased estimates and
224"
EXPERIMENTAL RESULTS,0.5440140845070423,"upper bounds produce reasonable results, since the fully-labeled ground truth is unknown.
225"
EXPERIMENTAL RESULTS,0.545774647887324,"Instead of using fully artiﬁcial data, we chose to construct a dataset based on existing data: We took
226"
EXPERIMENTAL RESULTS,0.5475352112676056,"AmazonCat-13k[22] and consider only the 100 most common labels, which are the ones with the
227"
EXPERIMENTAL RESULTS,0.5492957746478874,"highest propensity according to Jain et al. [16]. We artiﬁcially remove labels according to inverse
228"
EXPERIMENTAL RESULTS,0.551056338028169,"propensity, which increases linearly based on the ordering of label frequencies, such that the most
229"
EXPERIMENTAL RESULTS,0.5528169014084507,"common label has an inverse propensity of 2 and the 100th most common one of 20. This process
230"
EXPERIMENTAL RESULTS,0.5545774647887324,"partially preserves the strong imbalances that are typical of extreme classiﬁcation datasets.
231"
EXPERIMENTAL RESULTS,0.5563380281690141,"On this data, we train a linear classiﬁer with L2-regularization using different basis loss functions
232"
EXPERIMENTAL RESULTS,0.5580985915492958,"with a) the original (standard) loss on clean training data and b) noisy training data, as well as c) the
233"
EXPERIMENTAL RESULTS,0.5598591549295775,"unbiased version and d) the upper-bound version on noisy data. For each training run, we evaluate
234"
EXPERIMENTAL RESULTS,0.5616197183098591,"the loss on noisy and clean training and test data. For the evaluation on noisy data, the corresponding
235"
EXPERIMENTAL RESULTS,0.5633802816901409,"unbiased estimators are used.
236"
EXPERIMENTAL RESULTS,0.5651408450704225,"In this linear-classiﬁer experiment, the noise-pattern overﬁtting is much stronger than the overﬁtting
237"
EXPERIMENTAL RESULTS,0.5669014084507042,"due to ﬁnite sampling (10). Figure 2 shows this for the case of the BCE loss in OvA-reduction and
238"
EXPERIMENTAL RESULTS,0.5686619718309859,"CCE loss in normalized PAL reduction. For the classiﬁer trained on clean data (blue), the weights are
239"
EXPERIMENTAL RESULTS,0.5704225352112676,"independent of the noise pattern and thus the dashed and dotted lines coincide in expectation. For
240"
EXPERIMENTAL RESULTS,0.5721830985915493,"the case of OvA reduction using the BCE loss, the training loss gets reduced much further using the
241"
EXPERIMENTAL RESULTS,0.573943661971831,"unbiased loss function or the upper-bound loss function than using the standard loss. This decrease
242"
EXPERIMENTAL RESULTS,0.5757042253521126,"more than compensates the increase in generalization gap, and as such the minimal test loss is better
243"
EXPERIMENTAL RESULTS,0.5774647887323944,"with these two variants of the loss function. In contrast, in the non-decomposable case, even though
244"
EXPERIMENTAL RESULTS,0.579225352112676,"10−7
10−6
10−5
10−4
10−3
10−2
10−1
100 L2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 loss"
EXPERIMENTAL RESULTS,0.5809859154929577,Setting
EXPERIMENTAL RESULTS,0.5827464788732394,"clean a)
standard b)
unbiased c)
bound d)
Evaluation"
EXPERIMENTAL RESULTS,0.5845070422535211,"clean test R∗
ℓ∗"
EXPERIMENTAL RESULTS,0.5862676056338029,noisy train ˆRℓ
EXPERIMENTAL RESULTS,0.5880281690140845,"clean train ˆR∗
ℓ∗"
EXPERIMENTAL RESULTS,0.5897887323943662,"10−5
10−4
10−3
10−2
10−1
100 L2 -1 0 1 2 3 4 5 6 loss"
EXPERIMENTAL RESULTS,0.5915492957746479,Setting
EXPERIMENTAL RESULTS,0.5933098591549296,"clean a)
standard b)
unbiased c)
bound d)
Evaluation"
EXPERIMENTAL RESULTS,0.5950704225352113,"clean test R∗
ℓ∗"
EXPERIMENTAL RESULTS,0.596830985915493,noisy train ˆRℓ
EXPERIMENTAL RESULTS,0.5985915492957746,"clean train ˆR∗
ℓ∗"
EXPERIMENTAL RESULTS,0.6003521126760564,"Figure 2: Binary cross-entropy (top) and normalized categorical cross-entropy (bottom) for different
regularization strengths, evaluated on noisy training data, clean training data, and clean test data. The
gaps between dashed and dotted lines correspond to overﬁtting to the noise pattern, the smaller gaps
between dotted and solid lines show the generalization gaps due to the ﬁnite training sample. As the
dashed lines are for noisy data, they are calculated using the unbiased estimate (6)."
EXPERIMENTAL RESULTS,0.602112676056338,"the observed training loss decreases drastically with the unbiased loss, the increase in overﬁtting
245"
EXPERIMENTAL RESULTS,0.6038732394366197,"makes the test loss worse than using the biased standard loss function.
246"
EXPERIMENTAL RESULTS,0.6056338028169014,"In this case, using the upper-bound (20) can mitigate the effect, though there is still signiﬁcant
247"
EXPERIMENTAL RESULTS,0.6073943661971831,"overﬁtting, as evidenced by the estimated training loss being less than zero. This is possible because
248"
EXPERIMENTAL RESULTS,0.6091549295774648,"even though the loss we use for training is a non-negative upper bound on the expected unbiased loss,
249"
EXPERIMENTAL RESULTS,0.6109154929577465,"the dashed curves show the value estimated for the loss using the unbiased estimator, which can be
250"
EXPERIMENTAL RESULTS,0.6126760563380281,"negative due to overﬁtting. For the OVA case, the upper bound (13) also reduces overﬁtting, but does
251"
EXPERIMENTAL RESULTS,0.6144366197183099,"not result in an overall better classiﬁer on test data.
252"
EXPERIMENTAL RESULTS,0.6161971830985915,"In terms of the bias-variance trade-off, the graphs show a clear trend: The optimal regularization
253"
EXPERIMENTAL RESULTS,0.6179577464788732,"for training on noisy data is larger than on clean data. It is also larger when using the unbiased or
254"
EXPERIMENTAL RESULTS,0.6197183098591549,"upper-bound loss as compared to standard loss. This is as expected from the variance analysis and
255"
EXPERIMENTAL RESULTS,0.6214788732394366,"generalization bound presented in the theory.
256"
RELATED WORK,0.6232394366197183,"6
Related Work
257"
RELATED WORK,0.625,"Unbiased Estimates for Noisy Labels
Learning with missing labels is a speciﬁc instance of
258"
RELATED WORK,0.6267605633802817,"learning with class-conditional noise. For the case of binary labels, unbiased estimates of the loss
259"
RELATED WORK,0.6285211267605634,"function can be found in Natarajan et al. [26]. A more general approach is given in Van Rooyen and
260"
RELATED WORK,0.6302816901408451,"Williamson [35]. In their notation, f is a function and P the probability distribution over clean data,
261"
RELATED WORK,0.6320422535211268,"that is transformed by the invertible operator T into a corrupted probability distribution. Let R be the
262"
RELATED WORK,0.6338028169014085,"inverse of T, and R∗its adjoint, then ⟨P, f⟩= ⟨R ◦T(P), f⟩= ⟨T(P), R∗(f)⟩. This equation forms
263"
RELATED WORK,0.6355633802816901,"the basis for their “Theorem 5 (Corruption Corrected Loss)”, which states that a corruption corrected
264"
RELATED WORK,0.6373239436619719,"function lR is given ∀a ∈A by lR(·, a) = R∗(l(·, a)), where A denotes the set of possible actions
265"
RELATED WORK,0.6390845070422535,"that will be evaluated by the loss functions. For a ﬁnite label space with n possible, the operator
266"
RELATED WORK,0.6408450704225352,"R∗can be represented with an n × n matrix. For the multilabel case here, applying this naively
267"
RELATED WORK,0.6426056338028169,"would require 2l evaluations of the original loss function. In contrast, the direct approach presented
268"
RELATED WORK,0.6443661971830986,"in section 3 is much more efﬁcient.
269"
RELATED WORK,0.6461267605633803,"Alternatives
In some settings with noisy labels, it is possible to use a learning algorithm that is
270"
RELATED WORK,0.647887323943662,"inherently noise tolerant [12, 36]. Certain performance objectives such as the balanced error or the
271"
RELATED WORK,0.6496478873239436,"AUC are noise robust even under the more general setting of mutually contaminated distributions as
272"
RELATED WORK,0.6514084507042254,"shown in Menon et al. [23]. A data re-calibration approach tries to identify from the training data
273"
RELATED WORK,0.653169014084507,"which samples are corrupted, e.g. by looking at samples for which the network is very unsure, and
274"
RELATED WORK,0.6549295774647887,"adapt the training process correspondingly [13, 42, 19] It is also possible to ﬁrst train a scorer on the
275"
RELATED WORK,0.6566901408450704,"noisy data naively, from which a classiﬁer adapted to a given rate of missing labels can be constructed
276"
RELATED WORK,0.6584507042253521,"by choosing an appropriate threshold [23]. Similarly, the inference procedure of PLTs can be adapted
277"
RELATED WORK,0.6602112676056338,"to take into account a propensity model [39].
278"
RELATED WORK,0.6619718309859155,"Related Learning Settings
Learning with missing labels is highly related to learning from positive
279"
RELATED WORK,0.6637323943661971,"and unlabeled (PU) data [11]. An unbiased loss function for this setting is given in Du Plessis et al.
280"
RELATED WORK,0.6654929577464789,"[10]. The appearing difﬁculties, that non-negativity and convexity need not be preserved, are the same
281"
RELATED WORK,0.6672535211267606,"as in our setting [21]. A slightly different setting with missing labels is given by semi-supervised
282"
RELATED WORK,0.6690140845070423,"learning, where it is know for which labels are missing [41].
283"
SUMMARY AND DISCUSSION,0.670774647887324,"7
Summary and Discussion
284"
SUMMARY AND DISCUSSION,0.6725352112676056,"This paper provides unbiased estimates for four cases of multilabel reductions given in Menon
285"
SUMMARY AND DISCUSSION,0.6742957746478874,"et al. [24]. Except for the PAL reduction, these estimators can be non-convex and even negatively
286"
SUMMARY AND DISCUSSION,0.676056338028169,"unbounded. The unbiased estimates come with an increase in variance. This is unavoidable if
287"
SUMMARY AND DISCUSSION,0.6778169014084507,"unbiasedness is required, as the estimators can be shown to be unique. If sufﬁcient training data is
288"
SUMMARY AND DISCUSSION,0.6795774647887324,"available, then the unbiased loss functions can be used, but for the normalized reductions we found
289"
SUMMARY AND DISCUSSION,0.6813380281690141,"that even 1.2 million instances in AmazonCat are not enough. Much fewer data points are needed
290"
SUMMARY AND DISCUSSION,0.6830985915492958,"in order to estimate the overall loss of a classiﬁer. This is because for training, an accurate estimate
291"
SUMMARY AND DISCUSSION,0.6848591549295775,"for E[ℓ(Y ∗, h(X) | X] needs to be formed, whereas for evaluation this is averaged over the entire
292"
SUMMARY AND DISCUSSION,0.6866197183098591,"dataset, E[ℓ(Y ∗, h(X)]. This indicates that the unbiased estimates can be useful for hyperparameter
293"
SUMMARY AND DISCUSSION,0.6883802816901409,"tuning and model selection.
294"
SUMMARY AND DISCUSSION,0.6901408450704225,"For training, however, another approach is needed. A method that ﬁxes the negative unboundedness
295"
SUMMARY AND DISCUSSION,0.6919014084507042,"and non-convexity and also reduces the variance is to switch to a convex upper-bound. We have
296"
SUMMARY AND DISCUSSION,0.6936619718309859,"shown that this can stabilize the training and improve the results.
297"
SUMMARY AND DISCUSSION,0.6954225352112676,"Furthermore, the data in section 5 suggest training with missing labels requires more regularization,
298"
SUMMARY AND DISCUSSION,0.6971830985915493,"irrespective of whether training uses standard-, unbiased-, or convex upper-bound losses. Our ﬁndings
299"
SUMMARY AND DISCUSSION,0.698943661971831,"agree with Arpit et al. [2] who found that typical regularizers prevent a deep network from memorizing
300"
SUMMARY AND DISCUSSION,0.7007042253521126,"noisy examples, while not hindering the learning of patterns from clean instances.
301"
SUMMARY AND DISCUSSION,0.7024647887323944,"All in all, our results show that a) unbiasedness can be achieved for generic multilabel losses, and
302"
SUMMARY AND DISCUSSION,0.704225352112676,"in particular the losses resulting from multilabel reduction, but also that b) these losses might not
303"
SUMMARY AND DISCUSSION,0.7059859154929577,"be suitable for optimization. We have presented one method that trades away unbiasedness for the
304"
SUMMARY AND DISCUSSION,0.7077464788732394,"ability to handle training with lower amounts of data. An exciting future research prospect would be
305"
SUMMARY AND DISCUSSION,0.7095070422535211,"to investigate families of loss functions that can continuously trade off bias and variance, and thus
306"
SUMMARY AND DISCUSSION,0.7112676056338029,"allow for optimal training with different amounts of available data.
307"
REFERENCES,0.7130281690140845,"References
308"
REFERENCES,0.7147887323943662,"[1] R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma.
Multi-label learning with millions of
309"
REFERENCES,0.7165492957746479,"labels: Recommending advertiser bid phrases for web pages. In Proceedings of the 22nd
310"
REFERENCES,0.7183098591549296,"International Conference on World Wide Web, WWW ’13, page 13–24, New York, NY, USA,
311"
REFERENCES,0.7200704225352113,"2013. Association for Computing Machinery.
312"
REFERENCES,0.721830985915493,"[2] D. Arpit, S. Jastrz˛ebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer,
313"
REFERENCES,0.7235915492957746,"A. Courville, Y. Bengio, and S. Lacoste-Julien. A Closer Look at Memorization in Deep
314"
REFERENCES,0.7253521126760564,"Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70
315"
REFERENCES,0.727112676056338,"of Proceedings of Machine Learning Research, pages 233–242. PMLR, 06–11 Aug 2017.
316"
REFERENCES,0.7288732394366197,"[3] R. Babbar and B. Schölkopf. Dismec: Distributed sparse machines for extreme multi-label
317"
REFERENCES,0.7306338028169014,"classiﬁcation. In WSDM, pages 721–729, 2017.
318"
REFERENCES,0.7323943661971831,"[4] K. Bhatia, K. Dahiya, H. Jain, Y. Prabhu, and M. Varma. The extreme classiﬁcation repository:
319"
REFERENCES,0.7341549295774648,"Multi-label datasets and code. http://manikvarma.org/downloads/XC/XMLRepository.
320"
REFERENCES,0.7359154929577465,"html, 2016.
321"
REFERENCES,0.7376760563380281,"[5] Y.-T. Chou, G. Niu, H.-T. Lin, and M. Sugiyama. Unbiased Risk Estimators Can Mislead: A
322"
REFERENCES,0.7394366197183099,"Case Study of Learning with Complementary Labels. In International Conference on Machine
323"
REFERENCES,0.7411971830985915,"Learning, pages 1929–1938. PMLR, Nov. 2020.
324"
REFERENCES,0.7429577464788732,"[6] K. Dahiya, A. Agarwal, D. Saini, K. Gururaj, J. Jiao, A. Singh, S. Agarwal, P. Kar, and M. Varma.
325"
REFERENCES,0.7447183098591549,"Siamesexml: Siamese networks meet extreme classiﬁers with 100m labels. In Proceedings of
326"
REFERENCES,0.7464788732394366,"the International Conference on Machine Learning, July 2021.
327"
REFERENCES,0.7482394366197183,"[7] O. Dekel and O. Shamir. Multiclass-multilabel classiﬁcation with more classes than examples. In
328"
REFERENCES,0.75,"Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics,
329"
REFERENCES,0.7517605633802817,"pages 137–144, 2010.
330"
REFERENCES,0.7535211267605634,"[8] K. Dembczynski, W. Cheng, and E. Hüllermeier. Bayes optimal multilabel classiﬁcation via
331"
REFERENCES,0.7552816901408451,"probabilistic classiﬁer chains. In ICML, 2010.
332"
REFERENCES,0.7570422535211268,"[9] J. Deng, A. C. Berg, K. Li, and L. Fei-Fei. What does classifying more than 10,000 image
333"
REFERENCES,0.7588028169014085,"categories tell us? In ECCV, 2010.
334"
REFERENCES,0.7605633802816901,"[10] M. Du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and
335"
REFERENCES,0.7623239436619719,"unlabeled data. In International conference on machine learning, pages 1386–1394, 2015.
336"
REFERENCES,0.7640845070422535,"[11] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In Proceedings
337"
REFERENCES,0.7658450704225352,"of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,
338"
REFERENCES,0.7676056338028169,"KDD ’08, pages 213–220, New York, NY, USA, Aug. 2008. Association for Computing
339"
REFERENCES,0.7693661971830986,"Machinery.
340"
REFERENCES,0.7711267605633803,"[12] A. Ghosh, N. Manwani, and P. S. Sastry. Making risk minimization tolerant to label noise.
341"
REFERENCES,0.772887323943662,"Neurocomputing, 160:93–107, 2015. Publisher: Elsevier.
342"
REFERENCES,0.7746478873239436,"[13] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching:
343"
REFERENCES,0.7764084507042254,"Robust training of deep neural networks with extremely noisy labels. In S. Bengio, H. Wallach,
344"
REFERENCES,0.778169014084507,"H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural
345"
REFERENCES,0.7799295774647887,"Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
346"
REFERENCES,0.7816901408450704,"[14] J. Huang, H. Oosterhuis, M. De Rijke, and H. Van Hoof. Keeping dataset biases out of the
347"
REFERENCES,0.7834507042253521,"simulation: A debiased simulator for reinforcement learning based recommender systems. In
348"
REFERENCES,0.7852112676056338,"Fourteenth ACM conference on recommender systems, pages 190–199, 2020.
349"
REFERENCES,0.7869718309859155,"[15] J. Huang, H. Oosterhuis, and M. de Rijke. It is different when items are older: Debiasing
350"
REFERENCES,0.7887323943661971,"recommendations when selection bias and user preferences are dynamic. In Proceedings of
351"
REFERENCES,0.7904929577464789,"the Fifteenth ACM International Conference on Web Search and Data Mining, pages 381–389,
352"
REFERENCES,0.7922535211267606,"2022.
353"
REFERENCES,0.7940140845070423,"[16] H. Jain, Y. Prabhu, and M. Varma. Extreme multi-label loss functions for recommendation,
354"
REFERENCES,0.795774647887324,"tagging, ranking and other missing label applications. In KDD, August 2016.
355"
REFERENCES,0.7975352112676056,"[17] H. Jain, V. Balasubramanian, B. Chunduri, and M. Varma. Slice: Scalable linear extreme
356"
REFERENCES,0.7992957746478874,"classiﬁers trained on 100 million labels for related searches. In WSDM, pages 528–536, 2019.
357"
REFERENCES,0.801056338028169,"[18] Y. Jernite, A. Choromanska, and D. Sontag. Simultaneous learning of trees and representations
358"
REFERENCES,0.8028169014084507,"for extreme classiﬁcation and density estimation. In D. Precup and Y. W. Teh, editors, Proceed-
359"
REFERENCES,0.8045774647887324,"ings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
360"
REFERENCES,0.8063380281690141,"Machine Learning Research, pages 1665–1674. PMLR, 06–11 Aug 2017.
361"
REFERENCES,0.8080985915492958,"[19] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. MentorNet: Learning data-driven
362"
REFERENCES,0.8098591549295775,"curriculum for very deep neural networks on corrupted labels. In J. Dy and A. Krause, edi-
363"
REFERENCES,0.8116197183098591,"tors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of
364"
REFERENCES,0.8133802816901409,"Proceedings of Machine Learning Research, pages 2304–2313. PMLR, 10–15 Jul 2018.
365"
REFERENCES,0.8151408450704225,"[20] T. Joachims, A. Swaminathan, and T. Schnabel. Unbiased learning-to-rank with biased feedback.
366"
REFERENCES,0.8169014084507042,"In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,
367"
REFERENCES,0.8186619718309859,"pages 781–789, 2017.
368"
REFERENCES,0.8204225352112676,"[21] R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama. Positive-Unlabeled Learning with
369"
REFERENCES,0.8221830985915493,"Non-Negative Risk Estimator. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
370"
REFERENCES,0.823943661971831,"S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems
371"
REFERENCES,0.8257042253521126,"30, pages 1675–1685. Curran Associates, Inc., 2017.
372"
REFERENCES,0.8274647887323944,"[22] J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions
373"
REFERENCES,0.829225352112676,"with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages
374"
REFERENCES,0.8309859154929577,"165–172, 2013.
375"
REFERENCES,0.8327464788732394,"[23] A. Menon, B. Van Rooyen, C. S. Ong, and B. Williamson. Learning from corrupted binary
376"
REFERENCES,0.8345070422535211,"labels via class-probability estimation. In International Conference on Machine Learning, pages
377"
REFERENCES,0.8362676056338029,"125–134, 2015.
378"
REFERENCES,0.8380281690140845,"[24] A. K. Menon, A. S. Rawat, S. Reddi, and S. Kumar. Multilabel reductions: what is my loss
379"
REFERENCES,0.8397887323943662,"optimising? Advances in Neural Information Processing Systems, 32, 2019.
380"
REFERENCES,0.8415492957746479,"[25] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in
381"
REFERENCES,0.8433098591549296,"vector space. arXiv preprint arXiv:1301.3781, 2013.
382"
REFERENCES,0.8450704225352113,"[26] N. Natarajan, I. S. Dhillon, P. Ravikumar, and A. Tewari. Cost-sensitive learning with noisy
383"
REFERENCES,0.846830985915493,"labels. The Journal of Machine Learning Research, 18(1):5666–5698, 2017. Publisher: JMLR.
384"
REFERENCES,0.8485915492957746,"org.
385"
REFERENCES,0.8503521126760564,"[27] H. Oosterhuis and M. de Rijke. Policy-aware unbiased learning to rank for top-k rankings. In
386"
REFERENCES,0.852112676056338,"Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in
387"
REFERENCES,0.8538732394366197,"Information Retrieval, pages 489–498, 2020.
388"
REFERENCES,0.8556338028169014,"[28] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, G. Paliouras, E. Gaussier, I. Androut-
389"
REFERENCES,0.8573943661971831,"sopoulos, M.-R. Amini, and P. Galinari. Lshtc: A benchmark for large-scale text classiﬁcation.
390"
REFERENCES,0.8591549295774648,"arXiv preprint arXiv:1503.08581, 2015.
391"
REFERENCES,0.8609154929577465,"[29] Y. Prabhu and M. Varma. Fastxml: A fast, accurate and stable tree-classiﬁer for extreme
392"
REFERENCES,0.8626760563380281,"multi-label learning. In KDD, pages 263–272. ACM, 2014.
393"
REFERENCES,0.8644366197183099,"[30] M. Qaraei, E. Schultheis, P. Gupta, and R. Babbar. Convex Surrogates for Unbiased Loss
394"
REFERENCES,0.8661971830985915,"Functions in Extreme Classiﬁcation With Missing Labels. In Proceedings of the Web Conference
395"
REFERENCES,0.8679577464788732,"2021, pages 3711–3720, Ljubljana Slovenia, Apr. 2021. ACM.
396"
REFERENCES,0.8697183098591549,"[31] S. J. Reddi, S. Kale, F. Yu, D. Holtmann-Rice, J. Chen, and S. Kumar. Stochastic negative
397"
REFERENCES,0.8714788732394366,"mining for learning with large output spaces. In K. Chaudhuri and M. Sugiyama, editors,
398"
REFERENCES,0.8732394366197183,"Proceedings of the Twenty-Second International Conference on Artiﬁcial Intelligence and
399"
REFERENCES,0.875,"Statistics, volume 89 of Proceedings of Machine Learning Research, pages 1940–1949. PMLR,
400"
REFERENCES,0.8767605633802817,"16–18 Apr 2019.
401"
REFERENCES,0.8785211267605634,"[32] N. Sachdeva, C.-J. Wu, and J. McAuley. On sampling collaborative ﬁltering datasets. In
402"
REFERENCES,0.8802816901408451,"Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,
403"
REFERENCES,0.8820422535211268,"WSDM ’22, page 842–850, New York, NY, USA, 2022. Association for Computing Machinery.
404"
REFERENCES,0.8838028169014085,"[33] P. Teisseyre, J. Mielniczuk, and M. Łaz˛ecka. Different strategies of ﬁtting logistic regression
405"
REFERENCES,0.8855633802816901,"for positive and unlabelled data. In V. V. Krzhizhanovskaya, G. Závodszky, M. H. Lees, J. J.
406"
REFERENCES,0.8873239436619719,"Dongarra, P. M. A. Sloot, S. Brissos, and J. Teixeira, editors, Computational Science – ICCS
407"
REFERENCES,0.8890845070422535,"2020, pages 3–17, Cham, 2020. Springer International Publishing. ISBN 978-3-030-50423-6.
408"
REFERENCES,0.8908450704225352,"[34] G. Tsoumakas and I. M. Katakis. Multi-label classiﬁcation: An overview. Int. J. Data Warehous.
409"
REFERENCES,0.8926056338028169,"Min., 3:1–13, 2007.
410"
REFERENCES,0.8943661971830986,"[35] B. Van Rooyen and R. C. Williamson. A theory of learning with corrupted labels. The Journal
411"
REFERENCES,0.8961267605633803,"of Machine Learning Research, 18(1):8501–8550, 2017. ISSN 1532-4435.
412"
REFERENCES,0.897887323943662,"[36] B. Van Rooyen, A. Menon, and R. C. Williamson. Learning with symmetric label noise: The
413"
REFERENCES,0.8996478873239436,"importance of being unhinged. In Advances in Neural Information Processing Systems, pages
414"
REFERENCES,0.9014084507042254,"10–18, 2015.
415"
REFERENCES,0.903169014084507,"[37] X. Wu, H. Chen, J. Zhao, L. He, D. Yin, and Y. Chang. Unbiased learning to rank in feeds
416"
REFERENCES,0.9049295774647887,"recommendation. In Proceedings of the 14th ACM International Conference on Web Search
417"
REFERENCES,0.9066901408450704,"and Data Mining, pages 490–498, 2021.
418"
REFERENCES,0.9084507042253521,"[38] M. Wydmuch, K. Jasinska, M. Kuznetsov, R. Busa-Fekete, and K. Dembczynski. A no-regret
419"
REFERENCES,0.9102112676056338,"generalization of hierarchical softmax to extreme multi-label classiﬁcation. In S. Bengio,
420"
REFERENCES,0.9119718309859155,"H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
421"
REFERENCES,0.9137323943661971,"Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
422"
REFERENCES,0.9154929577464789,"[39] M. Wydmuch, K. Jasinska-Kobus, R. Babbar, and K. Dembczynski. Propensity-Scored Proba-
423"
REFERENCES,0.9172535211267606,"bilistic Label Trees, page 2252–2256. Association for Computing Machinery, New York, NY,
424"
REFERENCES,0.9190140845070423,"USA, 2021.
425"
REFERENCES,0.920774647887324,"[40] R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu. Attentionxml: Label tree-based
426"
REFERENCES,0.9225352112676056,"attention-aware deep model for high-performance extreme multi-label text classiﬁcation. In
427"
REFERENCES,0.9242957746478874,"H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,
428"
REFERENCES,0.926056338028169,"Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
429"
REFERENCES,0.9278169014084507,"[41] H.-F. Yu, P. Jain, P. Kar, and I. Dhillon. Large-scale multi-label learning with missing labels. In
430"
REFERENCES,0.9295774647887324,"International conference on machine learning, pages 593–601, 2014.
431"
REFERENCES,0.9313380281690141,"[42] S. Zheng, P. Wu, A. Goswami, M. Goswami, D. Metaxas, and C. Chen. Error-bounded correction
432"
REFERENCES,0.9330985915492958,"of noisy labels. In International Conference on Machine Learning, pages 11447–11457. PMLR,
433"
REFERENCES,0.9348591549295775,"2020.
434"
REFERENCES,0.9366197183098591,"Checklist
435"
REFERENCES,0.9383802816901409,"1. For all authors...
436"
REFERENCES,0.9401408450704225,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
437"
REFERENCES,0.9419014084507042,"contributions and scope? [Yes]
438"
REFERENCES,0.9436619718309859,"(b) Did you describe the limitations of your work? [Yes]
439"
REFERENCES,0.9454225352112676,"(c) Did you discuss any potential negative societal impacts of your work? [No]
440"
REFERENCES,0.9471830985915493,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
441"
REFERENCES,0.948943661971831,"them? [Yes]
442"
REFERENCES,0.9507042253521126,"2. If you are including theoretical results...
443"
REFERENCES,0.9524647887323944,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
444"
REFERENCES,0.954225352112676,"(b) Did you include complete proofs of all theoretical results? [Yes] Some proofs are in
445"
REFERENCES,0.9559859154929577,"the appendix
446"
REFERENCES,0.9577464788732394,"3. If you ran experiments...
447"
REFERENCES,0.9595070422535211,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
448"
REFERENCES,0.9612676056338029,"mental results (either in the supplemental material or as a URL)? [Yes]
449"
REFERENCES,0.9630281690140845,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
450"
REFERENCES,0.9647887323943662,"were chosen)? [Yes]
451"
REFERENCES,0.9665492957746479,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
452"
REFERENCES,0.9683098591549296,"ments multiple times)? [Yes]
453"
REFERENCES,0.9700704225352113,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
454"
REFERENCES,0.971830985915493,"of GPUs, internal cluster, or cloud provider)? [No]
455"
REFERENCES,0.9735915492957746,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
456"
REFERENCES,0.9753521126760564,"(a) If your work uses existing assets, did you cite the creators? [Yes]
457"
REFERENCES,0.977112676056338,"(b) Did you mention the license of the assets? [No]
458"
REFERENCES,0.9788732394366197,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
459 460"
REFERENCES,0.9806338028169014,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
461"
REFERENCES,0.9823943661971831,"using/curating? [N/A]
462"
REFERENCES,0.9841549295774648,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
463"
REFERENCES,0.9859154929577465,"information or offensive content? [N/A]
464"
REFERENCES,0.9876760563380281,"5. If you used crowdsourcing or conducted research with human subjects...
465"
REFERENCES,0.9894366197183099,"(a) Did you include the full text of instructions given to participants and screenshots, if
466"
REFERENCES,0.9911971830985915,"applicable? [N/A]
467"
REFERENCES,0.9929577464788732,"(b) Did you describe any potential participant risks, with links to Institutional Review
468"
REFERENCES,0.9947183098591549,"Board (IRB) approvals, if applicable? [N/A]
469"
REFERENCES,0.9964788732394366,"(c) Did you include the estimated hourly wage paid to participants and the total amount
470"
REFERENCES,0.9982394366197183,"spent on participant compensation? [N/A]
471"
