Section,Section Appearance Order,Paragraph
UNIVERSITY OF SURREY,0.0,"1University of Surrey
2The University of Hong Kong
3Imperial College London
4iFlyTek-Surrey Joint Research Centre on AI
5Surrey Institute for People-Centred AI"
ABSTRACT,0.003134796238244514,Abstract
ABSTRACT,0.006269592476489028,"Recently, text-guided 3D generative methods have made remarkable advancements
in producing high-quality textures and geometry, capitalizing on the proliferation of
large vision-language and image diffusion models. However, existing methods still
struggle to create high-ﬁdelity 3D head avatars in two aspects: (1) They rely mostly
on a pre-trained text-to-image diffusion model whilst missing the necessary 3D
awareness and head priors. This makes them prone to inconsistency and geometric
distortions in the generated avatars. (2) They fall short in ﬁne-grained editing. This
is primarily due to the inherited limitations from the pre-trained 2D image diffusion
models, which become more pronounced when it comes to 3D head avatars. In
this work, we address these challenges by introducing a versatile coarse-to-ﬁne
pipeline dubbed HeadSculpt for crafting (i.e., generating and editing) 3D head
avatars from textual prompts. Speciﬁcally, we ﬁrst equip the diffusion model
with 3D awareness by leveraging landmark-based control and a learned textual
embedding representing the back view appearance of heads, enabling 3D-consistent
head avatar generations. We further propose a novel identity-aware editing score
distillation strategy to optimize a textured mesh with a high-resolution differentiable
rendering technique. This enables identity preservation while following the editing
instruction. We showcase HeadSculpt’s superior ﬁdelity and editing capabilities
through comprehensive experiments and comparisons with existing methods. ‡"
INTRODUCTION,0.009404388714733543,"1
Introduction"
INTRODUCTION,0.012539184952978056,"Modeling 3D head avatars underpins a wide range of emerging applications (e.g., digital telepresence,
game character creation, and AR/VR). Historically, the creation of intricate and detailed 3D head
avatars demanded considerable time and expertise in art and engineering. With the advent of deep
learning, existing works [87, 28, 33, 72, 8, 38, 15] have shown promising results on the reconstruction
of 3D human heads from monocular images or videos. However, these methods remain restricted
to head appearance contained in their training data which is often limited in size, resulting in the
inability to generalize to new appearance beyond the training data. This constraint calls for the need
of more ﬂexible and generalizable methods for 3D head modeling."
INTRODUCTION,0.01567398119122257,"Recently, vision-language models (e.g., CLIP [55]) and diffusion models (e.g., Stable Diffusion [69,
61, 59]) have attracted increasing interest. These progresses have led to the emergence of text-to-3D
generative models [34, 62, 44, 27] which create 3D content in a self-supervised manner. Notably,
DreamFusion [54] introduces a score distillation sampling (SDS) strategy that leverages a pre-trained
image diffusion model to compute the noise-level loss from the textual description, unlocking the
potential to optimize differentiable 3D scenes (e.g., neural radiance ﬁeld [45], tetrahedral mesh [66],
texture [58, 9], or point cloud [50]) with 2D diffusion prior only. Subsequent research efforts [43, 6,"
INTRODUCTION,0.018808777429467086,"∗Equal contributions
† Corresponding authors
‡ Webpage: https://brandonhan.uk/HeadSculpt"
INTRODUCTION,0.0219435736677116,"∗Iron Man
∗Terracotta Army
†Napoleon Bonaparte"
INTRODUCTION,0.025078369905956112,"†Obama with a
baseball cap"
INTRODUCTION,0.02821316614420063,"make him like the
Joker"
INTRODUCTION,0.03134796238244514,"turn him into Pixar
style"
INTRODUCTION,0.034482758620689655,†Batman
INTRODUCTION,0.03761755485893417,"∗Simpson in the
Simpsons"
INTRODUCTION,0.04075235109717868,"†Kratos in God of War
†Taylor Swift
put her a masquerade
mask
make her a sculpture"
INTRODUCTION,0.0438871473354232,"†Black Panther in
Marvel"
INTRODUCTION,0.047021943573667714,"∗I am Groot
†Vincent van Gogh
†Audrey Hepburn
make her
color-restored
make her a claymation"
INTRODUCTION,0.050156739811912224,"†Two-face in DC
∗Naruto Uzumaki
†Leo Tolstoy
†Geralt in The Witcher
make him smiling
turn him into
Minecraft"
INTRODUCTION,0.05329153605015674,"†Doctor Strange
∗Hulk"
INTRODUCTION,0.05642633228840126,"†a boy with facial
painting"
INTRODUCTION,0.05956112852664577,"†Caesar in Rise of the
Planet of the Apes"
INTRODUCTION,0.06269592476489028,"as a swimmer with a
goggle"
INTRODUCTION,0.06583072100313479,"make it carved out of
wood"
INTRODUCTION,0.06896551724137931,"Figure 1: Examples of generation and editing results obtained using the proposed HeadSculpt.
It enables the creation and ﬁne-grained editing of high-quality head avatars, featuring intricate
geometry and texture, for any type of head avatar using simple descriptions or instructions. Symbols
indicate the following prompt preﬁxes: ∗“a head of [text]” and † “a DSLR portrait of [text]”."
INTRODUCTION,0.07210031347962383,The captions in gray are the prompt sufﬁxes while the blue ones are the editing instructions.
INTRODUCTION,0.07523510971786834,"65, 79, 42, 75, 40, 56, 76] improve and extend DreamFusion from various perspectives (e.g., higher
resolution [39] and better geometry [10])."
INTRODUCTION,0.07836990595611286,"Considering the ﬂexibility and versatility of natural languages, one might think that these SDS-based
text-to-3D generative methods would be sufﬁcient for generating diverse 3D avatars. However, it is
noted that existing methods have two major drawbacks (see Fig. 6): (1) Inconsistency and geometric
distortions: The 2D diffusion models used in these methods lack 3D awareness particularly regarding
camera pose; without any remedy, existing text-to-3D methods inherited this limitation, leading to
the multi-face “Janus” problem in the generated head avatars. (2) Fine-grained editing limitations:
Although previous methods propose to edit 3D models by naively ﬁne-tuning trained models with
modiﬁed prompts [54, 39], we ﬁnd that this approach is prone to biased outcomes, such as identity
loss or inadequate editing. This problem arises from two causes: (a) inherent bias in prompt-based
editing in image diffusion models, and (b) challenges with inconsistent gradient back-propagation at
separate iterations when using SDS calculated from a vanilla image diffusion model."
INTRODUCTION,0.08150470219435736,"In this paper, we introduce a new head-avatar-focused text-to-3D method, dubbed HeadSculpt,
that supports high-ﬁdelity generation and ﬁne-grained editing. Our method comprises two novel"
INTRODUCTION,0.08463949843260188,"components: (1) Prior-driven score distillation: We ﬁrst arm the pre-trained image diffusion model
with 3D awareness by integrating a landmark-based ControlNet [84]. Speciﬁcally, we adopt the
parametric 3D head model, FLAME [38], as a prior to obtain a 2D landmark map [41, 31], which
serves as an additional condition for the diffusion model, ensuring the consistency of generated head
avatars across different views. Further, to remedy the front-view bias in the pre-trained diffusion
model, we utilize an improved view-dependent prompt through textual inversion [17], by learning a
specialized <back-view> token to emphasize back views of heads and capture their unique visual
details. (2) Identity-aware editing score distillation (IESD): To address the challenges of ﬁne-grained
editing for head avatars, we introduce a novel method called IESD. It blends two scores, one for
editing and the other for identity preservation, both predicted by a ControlNet-based implementation
of InstructPix2Pix [5]. This approach maintains a controlled editing direction that respects both
the original identity and the editing instructions. To further improve the ﬁdelity of our method, we
integrate these two novel components into a coarse-to-ﬁne pipeline [39], utilizing NeRF [48] as the
low-resolution coarse model and DMTET [66] as the high-resolution ﬁne model. As demonstrated
in Fig. 1, our method can generate high-ﬁdelity human-like and non-human-like head avatars while
enabling ﬁne-grained editing, including local changes, shape/texture modiﬁcations, and style transfers."
RELATED WORK,0.0877742946708464,"2
Related work"
RELATED WORK,0.09090909090909091,"Text-to-2D generation. In recent years, groundbreaking vision-language technologies such as
CLIP [55] and diffusion models [25, 13, 59, 68] have led to signiﬁcant advancements in text-to-2D
content generation [61, 57, 1, 69, 70]. Trained on extensive 2D multimodal datasets [63, 64], they
are empowered with the capability to “dream” from the prompt. Follow-up works endeavor to
efﬁciently control the generated results [84, 85, 47], extend the diffusion model to video sequence [67,
3], accomplish image or video editing [23, 32, 81, 5, 77, 14, 22], enhance the performance for
personalized subjects [60, 17], etc. Although signiﬁcant progress has been made in generating 2D
content from text, carefully crafting the prompt is crucial, and obtaining the desired outcome often
requires multiple attempts. The inherent randomness remains a challenge, especially for editing tasks."
RELATED WORK,0.09404388714733543,"Text-to-3D generation. Advancements in text-to-2D generation have paved the way for text-to-3D
techniques. Early efforts [82, 27, 44, 62, 34, 29, 11] propose to optimize the 3D neural radiance ﬁeld
(NeRF) or vertex-based meshes by employing the CLIP language model. However, these models
encounter difﬁculties in generating expressive 3D content, primarily because of the limitations of CLIP
in comprehending natural language. Fortunately, the development of image diffusion models [69, 1]
has led to the emergence of DreamFusion [54]. It proposes Score Distillation Sampling (SDS)
based on a pre-trained 2D diffusion prior [61], showcasing promising generation results. Subsequent
works [37] have endeavored to improve DreamFusion from various aspects: Magic3D [39] proposes
a coarse-to-ﬁne pipeline for high-resolution generations; Latent-NeRF [43] includes shape guidance
for more robust generation on the latent space [59]; DreamAvatar [6] leverages SMPL [4] to generate
3D human full-body avatars under controllable shapes and poses; Guide3D [7] explores the usage
of multi-view generated images to create 3D human avatars; Fantasia3D [10] disentangles the
geometry and texture training with DMTET [66] and PBR texture [49] as their 3D representation;
3DFuse [65] integrates depth control and semantic code sampling to stabilize the generation process.
Despite notable progress, current text-to-3D generative models still face challenges in producing
view-consistent 3D content, especially for intricate head avatars. This is primarily due to the absence
of 3D awareness in text-to-2D diffusion models. Additionally, to the best of our knowledge, there
is currently no approach that speciﬁcally focuses on editing the generated 3D content, especially
addressing the intricate ﬁne-grained editing needs of head avatars."
RELATED WORK,0.09717868338557993,"3D head modeling and creation. Statistical mesh-based models, such as FLAME [38, 15], enable the
reconstruction of 3D head models from images. However, they struggle to capture ﬁne details like hair
and wrinkles. To overcome this issue, recent approaches [8, 71, 72, 51] employ Generative Adversarial
Networks (GANs) [46, 20, 30] to train 3D-aware networks on 2D head datasets and produce 3D-
consistent images through latent code manipulation. Furthermore, neural implicit methods [87, 16, 28,
88] introduce implicit and subject-oriented head models based on neural rendering ﬁelds [45, 48, 2].
Recently, text-to-3D generative methods have gained traction, generating high-quality 3D head
avatars from natural language using vision-language models [55, 69]. Typically, T2P [85] predicts
bone-driven parameters of head avatars via a game engine under the CLIP guidance [55]. Rodin [80]
proposes a roll-out diffusion network to perform 3D-aware diffusion. DreamFace [83] employs a"
RELATED WORK,0.10031347962382445,"Description / Instruction,
front view / <back-view> / …"
RELATED WORK,0.10344827586206896,"Latent
Diffusion Prior
(Stable Diffusion)
Encoder"
RELATED WORK,0.10658307210031348,"ControlNet-based 
InstructPix2Pix (Editing Only) Noise"
RELATED WORK,0.109717868338558,"Landmark
ControlNet"
RELATED WORK,0.11285266457680251,"Enhanced view-
dependent prompt"
RELATED WORK,0.11598746081504702,Landmark Map
RELATED WORK,0.11912225705329153,Rendered Image
RELATED WORK,0.12225705329153605,Reference Image
RELATED WORK,0.12539184952978055,(c) Prior-driven Score Distillation (PSD)
RELATED WORK,0.12852664576802508,update
RELATED WORK,0.13166144200626959,"a DSLR portrait of Saul Goodman
turn him into a clown"
RELATED WORK,0.13479623824451412,"High-quality
3D Mesh 
Model
PSD
Render 
(low-res)"
RELATED WORK,0.13793103448275862,FLAME-based NeRF
RELATED WORK,0.14106583072100312,"Projected 
Landmark"
RELATED WORK,0.14420062695924765,Identity-aware Editing Score Distillation
RELATED WORK,0.14733542319749215,3D Mesh (DMTET)
RELATED WORK,0.15047021943573669,"PSD
Render 
(high-res)"
RELATED WORK,0.1536050156739812,"Projected 
Landmark"
RELATED WORK,0.15673981191222572,(b) Fine Stage
RELATED WORK,0.15987460815047022,Refinement OR
RELATED WORK,0.16300940438871472,3D Mesh (DMTET)
RELATED WORK,0.16614420062695925,"Render 
(high-res)"
RELATED WORK,0.16927899686520376,"Projected 
Landmark"
RELATED WORK,0.1724137931034483,Trained Coarse NeRF PSD
RELATED WORK,0.1755485893416928,Mixing
RELATED WORK,0.1786833855799373,"Render 
(low-res)"
RELATED WORK,0.18181818181818182,(a) Coarse Stage
RELATED WORK,0.18495297805642633,"Figure 2: Overall architecture of HeadSculpt. We craft high-resolution 3D head avatars in a coarse-
to-ﬁne manner. (a) We optimize neural ﬁeld representations for the coarse model. (b) We reﬁne
or edit the model using the extracted 3D mesh and apply identity-aware editing score distillation
if editing is the target. (c) The core of our pipeline is the prior-driven score distillation, which
incorporates landmark control, enhanced view-dependent prompts, and an InstructPix2Pix branch."
RELATED WORK,0.18808777429467086,"selection strategy in the CLIP embedding space to generate coarse geometry and uses SDS [54] to
optimize UV-texture. Despite producing promising results, all these methods require a large amount
of data for supervised training and struggle to generalize well to non-human-like avatars. In contrast,
our approach relies solely on pre-trained text-to-2D models, generalizes well to out-of-domain avatars,
and is capable of performing ﬁne-grained editing tasks."
METHODOLOGY,0.19122257053291536,"3
Methodology"
METHODOLOGY,0.19435736677115986,"HeadSculpt is a 3D-aware text-to-3D approach that utilizes a pre-trained text-to-2D Stable Diffusion
model [69, 59] to generate high-resolution head avatars and perform ﬁne-grained editing tasks. As
illustrated in Fig. 2, the generation pipeline has two stages: coarse generation via the neural radiance
ﬁeld (NeRF) [48] and reﬁnement/editing using tetrahedron mesh (DMTET) [66]. Next, we will ﬁrst
introduce the preliminaries that form the basis of our method in Sec. 3.1. We will then discuss the key
components of our approach in Sec. 3.2 and Sec, 3.3, including (1) the prior-driven score distillation
process via landmark-based ControlNet [84] and textual inversion [17], and (2) identity-aware editing
score distillation accomplished in the ﬁne stage using the ControlNet-based InstructPix2Pix [5]."
PRELIMINARIES,0.1974921630094044,"3.1
Preliminaries"
PRELIMINARIES,0.2006269592476489,"Score distillation sampling. Recently, DreamFusion [54] proposed score distillation sampling (SDS)
to self-optimize a text-consistent neural radiance ﬁeld (NeRF) based a the pre-trained text-to-2D
diffusion model [61]. Due to the unavailability of the Imagen model [61] used by DreamFusion, we
employ the latent diffusion model in [59] instead. Speciﬁcally, given a latent feature z encoded from
an image x, SDS introduces random noise ϵ to z to create a noisy latent variable zt and then uses a
pre-trained denoising function ϵϕ (zt; y, t) to predict the added noise. The SDS loss is deﬁned as the
difference between predicted and added noise and its gradient is given by"
PRELIMINARIES,0.20376175548589343,"∇θLSDS(ϕ, g(θ)) = Et,ϵ∼N (0,1)"
PRELIMINARIES,0.20689655172413793,"
w(t) (ϵϕ (zt; y, t) −ϵ) ∂z"
PRELIMINARIES,0.21003134796238246,"∂x
∂x
∂θ"
PRELIMINARIES,0.21316614420062696,"
,
(1)"
PRELIMINARIES,0.21630094043887146,"where y is the text embedding, w(t) weights the loss from noise level t. With the expressive text-to-
2D diffusion model and self-supervised SDS loss, we can back-propagate the gradients to optimize
an implicit 3D scene g(θ), eliminating the need for an expensive 3D dataset."
PRELIMINARIES,0.219435736677116,"3D scene optimization. HeadSculpt explores the potential of two different 3D differentiable repre-
sentations as the optimization basis for crafting 3D head avatars. Speciﬁcally, we employ NeRF [48]
in the coarse stage due to its greater ﬂexibility in geometry deformation, while utilizing DMTET [66]
in the ﬁne stage for efﬁcient high-resolution optimization."
PRELIMINARIES,0.2225705329153605,"(1) 3D prior-based NeRF. DreamAvatar [6] recently proposed a density-residual setup to enhance the
robustness and controllability of the generated 3D NeRF. Given a point x inside the 3D volume, we
can derive its density and color value based on a prior-based density ﬁeld ¯σ:"
PRELIMINARIES,0.22570532915360503,"F(x, ¯σ) = Fθ(γ(x)) + (¯σ(x), 0) 7→(σ, c),
(2)"
PRELIMINARIES,0.22884012539184953,"where γ(·) denotes a hash-grid frequency encoder [48], and σ and c are the density and RGB color
respectively. We can derive ¯σ from the signed distance d(x) of a given 3D shape prior (e.g., a
canonical FLAME model [38] by default in our implementation):"
PRELIMINARIES,0.23197492163009403,"¯σ(x) = max
 
0, softplus−1(τ(x))

, τ(x) = 1"
PRELIMINARIES,0.23510971786833856,"a sigmoid(−d(x)/a), where a = 0.005.
(3)"
PRELIMINARIES,0.23824451410658307,"To obtain a 2D RGB image from the implicit volume deﬁned above, we employ a volume rendering
technique that involves casting a ray r from the 2D pixel location into the 3D scene, sampling points
µi along the ray, and calculating their density and color value using F in Eq. (2):"
PRELIMINARIES,0.2413793103448276,"C(r) =
X"
PRELIMINARIES,0.2445141065830721,"i
Wici,
Wi = αi
Y"
PRELIMINARIES,0.2476489028213166,"j<i
(1 −αj),
αi = 1 −e(−σi||µi−µi+1||).
(4)"
PRELIMINARIES,0.2507836990595611,"(2) DMTET. It discretizes a deformable tetrahedral grid (VT , T), where VT denotes the vertices
within grid T [19, 66], to model the 3D space. Every vertex vi ∈VT ⊂R3 possesses a signed
distance value si ∈R, along with a position offset ∆vi ∈R3 of the vertex relative to its initial
canonical coordinates. Subsequently, the underlying mesh can be extracted based on si with the
differentiable marching tetrahedra algorithm. In addition to the geometry, we adopt the Magic3D
approach [39] to construct a neural color ﬁeld. This involves re-utilizing the MLP trained in the
coarse NeRF stage to predict the RGB color value for each 3D point. During optimization, we render
this textured surface mesh into high-resolution images using a differentiable rasterizer [36, 49]."
PRELIMINARIES,0.25391849529780564,"3.2
3D-Prior-driven score distillation"
PRELIMINARIES,0.25705329153605017,"Existing text-to-3D methods with SDS [54] assume that maximizing the likelihood of images rendered
from various viewpoints of a scene model g(·) is equivalent to maximizing the overall likelihood of
g(·). This assumption can result in inconsistencies and geometric distortions [54, 65]. A notable issue
is the “Janus problem” characterized by multiple faces on a single object (see Fig. 6). There are two
possible causes: (1) the randomness of the diffusion model which can cause inconsistencies among
different views, and (2) the lack of 3D awareness in controlling the generation process, causing the
model to struggle in determining the front view, back view, etc. To address these issues in generating
head avatars, we integrate 3D head priors into the diffusion model."
PRELIMINARIES,0.2601880877742947,"Landmark-based ControlNet. In Section 3.1, we explain our adoption of FLAME [38] as the density
guidance for our NeRF. Nevertheless, this guidance by itself is insufﬁcient to have a direct impact on
the SDS loss. What is missing is a link between the NeRF and the diffusion model, incorporating the
same head priors. Such a link is key to improving the view consistency of the generated head avatars.
To achieve this objective, as illustrated in Fig. 2, we propose the incorporation of 2D landmark maps
as an additional condition for the diffusion model using ControlNet [84]. Speciﬁcally, we employ a
ControlNet C trained on a large-scale 2D face dataset [86, 12], using facial landmarks rendered from
MediaPipe [41, 31] as ground-truth data. When given a randomly sampled camera pose π, we ﬁrst
project the vertices of the FLAME model onto the image. Following that, we select and render some
of these vertices into a landmark map Pπ based on some predeﬁned vertex indexes. The landmark
map will be fed into ControlNet and its output features are added to the intermediate features within
the diffusion U-Net. The gradient of our SDS loss can be re-written as"
PRELIMINARIES,0.26332288401253917,"∇θLSDS(ϕ, g(θ)) = Et,ϵ∼N (0,1),π"
PRELIMINARIES,0.2664576802507837,"
w(t) (ϵϕ (zt; y, t, C(Pπ)) −ϵ) ∂z"
PRELIMINARIES,0.26959247648902823,"∂x
∂x
∂θ"
PRELIMINARIES,0.2727272727272727,"
.
(5)"
PRELIMINARIES,0.27586206896551724,"Enhanced view-dependent prompt via textual inversion. Although the landmark-based ControlNet
can inject 3D awareness into the pre-trained diffusion models, it struggles to maintain back view head
consistency. This is expected as the 2D image dataset used for training mostly contains only front or
side face views. Consequently, when applied directly to back views, the model introduces ambiguity
as front and back 3D landmark views can appear similar, as shown in Fig. 8. To address this issue,
we propose a simple yet effective method. Our method is inspired by previous works [54, 65, 39]
which found it beneﬁcial to append view-dependent text (e.g., “front view”, “side view” or “back"
PRELIMINARIES,0.27899686520376177,"view”) to the provided input text based on the azimuth angle of the randomly sampled camera. We
extend this idea by learning a special token <back-view> to replace the plain text “back view” in
order to emphasize the rear appearance of heads. This is based on the assumption that a pre-trained
Stable Diffusion does has the ability to “imagine” the back view of a head - it has seen some during
training. The main problem is that a generic text embedding of “back view” is inadequate in telling
the model what appearance it entails. A better embedding for “back view” is thus required. To this
end, we ﬁrst randomly download 34 images of the back view of human heads, without revealing
any personal identities, to construct a tiny dataset D, and then we optimize the special token v (i.e.,
<back-view>) to better ﬁt the collected images, similar to the textual inversion [17]:"
PRELIMINARIES,0.28213166144200624,"v∗= arg min
v
Et,ϵ∼N (0,1),z∼D
h
∥ϵ −ϵϕ (zt; v, t)∥2
2
i
,
(6)"
PRELIMINARIES,0.2852664576802508,"which is achieved by employing the same training scheme as the original diffusion model, while
keeping ϵϕ ﬁxed. This constitutes a reconstruction task, which we anticipate will encourage the
learned embedding to capture the ﬁne visual details of the back views of human heads. Notably, as
we do not update the weights of ϵϕ, it stays compatible with the landmark-based ControlNet."
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.2884012539184953,"3.3
Identity-aware editing score distillation"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.29153605015673983,"After generating avatars, editing them to fulﬁll particular requirements poses an additional challenge.
Previous works [54, 39] have shown promising editing results by ﬁne-tuning a trained scene model
with a new target prompt. However, when applied to head avatars, these methods often suffer from
identity loss or inadequate appearance modiﬁcations (see Fig. 10). This problem stems from the
inherent constraint of the SDS loss, where the 3D models often sacriﬁce prominent features to
preserve view consistency. Substituting Stable Diffusion with InstructPix2Pix [5, 21] might seem
like a simple solution, but it also faces difﬁculties in maintaining facial identity during editing based
only on instructions, as it lacks a well-deﬁned anchor point."
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.2946708463949843,"To this end, we propose identity-aware editing score distillation (IESD) to regulate the editing
direction by blending two predicted scores, i.e., one for editing instruction and another for the original
description. Rather than using the original InstructPix2Pix [5], we employ a ControlNet-based
InstructPix2Pix I [84] trained on the same dataset, ensuring compatibility with our landmark-based
ControlNet C and the learned <back-view> token. Formally, given an initial textual prompt y
describing the avatar to be edited and an editing instruction ˆy, we ﬁrst input them separately into
the same diffusion model equipped with two ControlNets, I and C. This allows us to obtain two
predicted noises, which are then combined using a predeﬁned hyper-parameter ωe like classiﬁer-free
diffusion guidance (CFG) [26]:"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.29780564263322884,"∇θLIESD(ϕ, g(θ)) = Et,ϵ∼N (0,1),π"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.30094043887147337,"
w(t)

ˆϵϕ (zt; y, ˆy, t, C(Pπ), I(Mπ))
|
{z
} −ϵ
 ∂z"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.30407523510971785,"∂x
∂x
∂θ 
,"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.3072100313479624,"ωeϵϕ (zt; ˆy, t, C(Pπ), I(Mπ)) + (1 −ωe)ϵϕ (zt; y, t, C(Pπ), I(Mπ))
(7)"
IDENTITY-AWARE EDITING SCORE DISTILLATION,0.3103448275862069,"where Pπ and Mπ represent the 2D landmark maps and the reference images rendered in the coarse
stage, both being obtained under the sampled camera pose π. The parameter ωe governs a trade-off
between the original appearance and the desired editing, which defaults to 0.6 in our experiments."
EXPERIMENTS,0.31347962382445144,"4
Experiments"
EXPERIMENTS,0.3166144200626959,"We will now assess the efﬁcacy of our HeadSculpt across different scenarios, while also conducting a
comparative analysis against state-of-the-art text-to-3D generation pipelines."
EXPERIMENTS,0.31974921630094044,"Implementation details. HeadSculpt builds upon Stable-DreamFusion [73] and Huggingface
Diffusers [78, 53]. We utilize version 1.5 of Stable Diffusion [69] and version 1.1 of ControlNet [84,
12] in our implementation. In the coarse stage, we optimize our 3D model at 64 × 64 grid resolution,
while using 512 × 512 grid resolution for the ﬁne stage (reﬁnement or editing). Typically, each text
prompt requires approximately 7, 000 iterations for the coarse stage and 5, 000 iterations for the ﬁne
stage. It takes around 1 hour for each stage on a single Tesla V100 GPU with a default batch size
of 4. We use Adam [35] optimizer with a ﬁxed learning rate of 0.001. Additional implementation
details can be found in the supplementary material."
EXPERIMENTS,0.322884012539185,"Shape 1
Shape 2
Shape 3"
EXPERIMENTS,0.32601880877742945,a DSLR portrait of Saul Goodman
EXPERIMENTS,0.329153605015674,"Figure 3: Generation results with various shapes. The ﬁrst row shows three randomly sampled
FLAME models, while the second row presents our generated results (incl. normals) using these
FLAME models as initialization. All results are under the same text prompt."
EXPERIMENTS,0.3322884012539185,"‡ sad
‡ surprised
‡ disgusted
make him bald
give him a beard
give him a sunglass"
EXPERIMENTS,0.335423197492163,Figure 4: More speciﬁc editing results. ‡ Instruction preﬁx: make his expression as [text].
EXPERIMENTS,0.3385579937304075,"Baseline methods for generation evaluation. We compare the generation results with ﬁve baselines:
DreamFusion [73], Latent-NeRF [43], 3DFuse [65] (improved version of SJC [79]), Fantasia3D [10],
and DreamFace [83]. We do not directly compare with DreamAvatar [6] as it involves deformation
ﬁelds for full-body-related tasks."
EXPERIMENTS,0.34169278996865204,"Baseline methods for editing evaluation. We assess IESD’s efﬁcacy for ﬁne-grained 3D head avatar
editing by comparing it with various alternatives since no dedicated method exists for this: (B1)
One-step optimization on the coarse stage without initialization; (B2) Initialized from the coarse
stage, followed by optimization of another coarse stage with an altered description; (B3) Initialized
from the coarse stage, followed by optimization of a new ﬁne stage with an altered description; (B4)
Initialized from the coarse stage, followed by optimization of a new ﬁne stage with an instruction
based on the vanilla InstructPix2Pix [5]; (B5) Ours without edit scale (i.e., ωe = 1). Notably, B2
represents the editing method proposed in DreamFusion [54], while B3 has a similar performance as
Magic3D [39], which employs a three-stage editing process (i.e., Coarse + Coarse + Fine)."
QUALITATIVE EVALUATIONS,0.3448275862068966,"4.1
Qualitative evaluations"
QUALITATIVE EVALUATIONS,0.34796238244514105,"Head avatar generation with various prompts. In Fig. 1, we show a diverse array of 3D head avatars
generated by our HeadSculpt, consistently demonstrating high-quality geometry and texture across
various viewpoints. Our method’s versatility is emphasized by its ability to create an assortment of
avatars, including humans (both celebrities and ordinary individuals) as well as non-human characters
like superheroes, comic/game characters, paintings, and more."
QUALITATIVE EVALUATIONS,0.3510971786833856,"Head avatar generation with different shapes. HeadSculpt leverages shape-guided NeRF initializa-
tion and landmark-guided diffusion priors. This allows controlling geometry by varying the FLAME
shape used for initialization. To demonstrate adjustability, Fig. 3 presents examples generated from
diverse FLAME shapes. The results ﬁt closely to the shape guidance, highlighting HeadSculpt’s
capacity for geometric variation when provided different initial shapes."
QUALITATIVE EVALUATIONS,0.3542319749216301,"Head avatar editing with various instructions. As illustrated in Fig. 1 and Fig. 4, HeadSculpt’s
adaptability is also showcased through its ability to perform ﬁne-grained editing, such as local
changes (e.g., adding accessories, changing hairstyles, or altering expressions), shape and texture
modiﬁcations, and style transfers."
QUALITATIVE EVALUATIONS,0.3573667711598746,"Head avatar editing with different edit scales. In Fig. 5, we demonstrate the effectiveness of IESD
with different ωe values, highlighting its ability to control editing inﬂuence on the reference identity."
QUALITATIVE EVALUATIONS,0.3605015673981191,"Reference Avatar
ωe = 0.2
ωe = 0.4
ωe = 0.6
ωe = 0.8
ωe = 1.0"
QUALITATIVE EVALUATIONS,0.36363636363636365,"Saul Goodman
turn him into a clown"
QUALITATIVE EVALUATIONS,0.3667711598746082,"Figure 5: Impact of the edit scale ωe in IESD. It balances the preservation of the initial appearance
and the extent of the desired editing, making the editing process more controllable and ﬂexible."
QUALITATIVE EVALUATIONS,0.36990595611285265,"DreamFusion* [73]
Latent-NeRF [43]
3DFuse [65]
Fantasia3D* [74]
DreamFace† [83]
HeadSculpt (Ours)"
QUALITATIVE EVALUATIONS,0.3730407523510972,a DSLR portrait of Salvador Dalí
QUALITATIVE EVALUATIONS,0.3761755485893417,a head of Stormtrooper
QUALITATIVE EVALUATIONS,0.3793103448275862,"a DSLR portrait of a female soldier, wearing a helmet"
QUALITATIVE EVALUATIONS,0.3824451410658307,"a DSLR portrait of a young black lady with short hair, wearing a headphone"
QUALITATIVE EVALUATIONS,0.38557993730407525,"Figure 6: Comparison with existing text-to-3D methods. Unlike other methods that struggle or fail
to generate reasonable results, our approach consistently achieves high-quality geometry and texture,
yielding superior results. *Non-ofﬁcial implementation. † Generated from the online website demo."
QUALITATIVE EVALUATIONS,0.3887147335423197,"Comparison with existing methods on generation results. We provide qualitative comparisons
with existing methods in Fig. 6. We employ the same FLAME model for Latent-NeRF [43] to
compute their sketch-guided loss and for Fantasia3D [74] as the initial geometry. The following
observations can be made: (1) All baselines tend to be more unstable during training than ours, often
resulting in diverged training processes; (2) Latent-NeRF occasionally produces plausible results due
to its use of the shape prior, but its textures are inferior to ours since optimization occurs solely in
the latent space; (3) Despite 3DFuse’s depth control to mitigate the Janus problem, it still struggles
to generate 3D consistent head avatars; (4) While Fantasia3D can generate a mesh-based 3D avatar,
its geometry is heavily distorted, as its disentangled geometry optimization might be insufﬁcient for
highly detailed head avatars; (5) Although DreamFace generates realistic human face textures, it falls
short in generating (i) complete heads, (ii) intricate geometry, (iii) non-human-like appearance, and
(iv) composite accessories. In comparison, our method consistently yields superior results in both
geometry and texture with much better consistency for the given prompt. More comparisons can be
found in the supplementary material."
QUALITATIVE EVALUATIONS,0.39184952978056425,"Textual
Consistency"
QUALITATIVE EVALUATIONS,0.3949843260188088,"Texture
Quality"
QUALITATIVE EVALUATIONS,0.3981191222570533,"Geometry
Quality 0 2"
"DREAMFUSION
LATENT-NERF",0.4012539184952978,"4
DreamFusion
Latent-NeRF
3DFuse
Fantasia3D
HeadSculpt
(Ours)"
"DREAMFUSION
LATENT-NERF",0.4043887147335423,"Figure 7: User study. Numbers are aver-
aged over 42 responses."
"DREAMFUSION
LATENT-NERF",0.40752351097178685,"Generation
Dream
Fusion
Latent-
NeRF
3DFuse
Fantasia3D
Ours"
"DREAMFUSION
LATENT-NERF",0.4106583072100313,"CLIP-R [52]
95.83
87.50
70.83
62.50
100.00
CLIP-S [24]
26.06
26.30
23.41
23.26
29.52"
"DREAMFUSION
LATENT-NERF",0.41379310344827586,"Editing
B3
B4
B5
Ours"
"DREAMFUSION
LATENT-NERF",0.4169278996865204,"CLIP-DS [18]
16.62
8.76
14.03
16.84"
"DREAMFUSION
LATENT-NERF",0.4200626959247649,"Table 1: Objective evaluation with CLIP-based met-
rics. All numbers are calculated with CLIP-L/14."
"DREAMFUSION
LATENT-NERF",0.4231974921630094,"HeadSculpt (Ours)
w/o Landmark Ctrl
HeadSculpt (Ours)
w/o Textual Inversion"
"DREAMFUSION
LATENT-NERF",0.4263322884012539,"a head of Woody in the Toy Story
a head of Walter White, wearing a bowler hat"
"DREAMFUSION
LATENT-NERF",0.42946708463949845,"a head of Bumblebee in Transformers
a head of Mario in Mario Franchise"
"DREAMFUSION
LATENT-NERF",0.43260188087774293,Figure 8: Analysis of prior-driven score distillation.
"DREAMFUSION
LATENT-NERF",0.43573667711598746,"∗Sun Wukong
∗Freddy Krueger"
"DREAMFUSION
LATENT-NERF",0.438871473354232,"∗Japanese Geisha
remove his nose"
"DREAMFUSION
LATENT-NERF",0.44200626959247646,Figure 9: Failure cases.
QUANTITATIVE EVALUATIONS,0.445141065830721,"4.2
Quantitative evaluations"
QUANTITATIVE EVALUATIONS,0.4482758620689655,"User studies. We conducted user studies comparing with four baselines [73, 74, 65, 43]. 42 volunteers
ranked them from 1 (worst) to 5 (best) individually based on three dimensions: (1) consistency with
the text, (2) texture quality, and (3) geometry quality. The results, shown in Fig. 7, indicate that our
method achieved the highest rank in all three aspects by large margins."
QUANTITATIVE EVALUATIONS,0.45141065830721006,"CLIP-based metrics. Following DreamFusion [54], (1) We calculate the CLIP R-Precision (CLIP-
R) [52] and CLIP-Score (CLIP-S) [24] metrics, which evaluate the correlation between the generated
images and the input texts, for all methods using 30 text prompts. As indicated in Tab. 1, our
approach signiﬁcantly outperforms the competing methods according to both metrics. This outcome
provides additional evidence for the subjective superiority observed in the user studies and qualitative
results. (2) We employ the CLIP Directional Similarity (CLIP-DS) [5, 18], to evaluate the editing
performance. This metric measures the alignment between changes in text captions and corresponding
image modiﬁcations. Speciﬁcally, we encode a pair of images (the original and edited 3D models
rendered from a speciﬁc viewpoint) along with a pair of text prompts describing the original and edited
subjects, e.g., “a DSLR portrait of Saul Goodman” and “a DSLR portrait of Saul Goodman dressed
like a clown”. We compare our approach against B3, B4, and B5 by evaluating 10 edited examples.
The results, presented in Tab. 1, highlight the superiority of our editing framework according to this
metric, indicating improved editing ﬁdelity and identity preservation compared to alternatives."
FURTHER ANALYSIS,0.45454545454545453,"4.3
Further analysis"
FURTHER ANALYSIS,0.45768025078369906,"Effectiveness of prior-driven score distillation. In Fig. 8, we conduct ablation studies to examine the
impact of the proposed landmark control and textual inversion priors in our method. We demonstrate
this on the coarse stage because the reﬁnement and editing results heavily depend on this stage.
The ﬁndings show that landmark control is essential for generating spatially consistent head avatars.
Without it, the optimized 3D avatar faces challenges in maintaining consistent facial views, particularly
for non-human-like characters. Moreover, textual inversion is shown to be another vital component
in mitigating the Janus problem, speciﬁcally for the back view, as landmarks cannot exert control
on the rear view. Overall, the combination of both components enables HeadSculpt to produce
view-consistent avatars with high-quality geometry."
FURTHER ANALYSIS,0.4608150470219436,"B1: One-stage
B2: Coarse + Coarse
B3: Coarse + Fine
B4: Naive IP2P
B5: Ours w/o ωe
HeadSculpt (Ours)"
FURTHER ANALYSIS,0.46394984326018807,"Modiﬁed description: a DSLR portrait of +[older] Saul Goodman
Instruction: make him older"
FURTHER ANALYSIS,0.4670846394984326,"Modiﬁed description: a DSLR portrait skull of Vincent van Gogh
Instruction: turn his face into a skull"
FURTHER ANALYSIS,0.4702194357366771,Figure 10: Analysis of identity-aware editing score distillation.
FURTHER ANALYSIS,0.47335423197492166,"Effectiveness of IESD. In Fig. 10, we present two common biased editing scenarios produced by the
baseline methods: insufﬁcient editing and loss of identity. With Stable Diffusion, speciﬁc terms like
“Saul Goodman” and “skull” exert a more substantial inﬂuence on the text embeddings compared
to other terms, such as “older” and “Vincent van Gogh”. B1, B2, and B3, all based on vanilla
Stable Diffusion, inherit such bias in their generated 3D avatars. Although B4 does not show such
bias, it faces two other issues: (1) the Janus problem reemerges due to incompatibility between
vanilla InstructPix2Pix and the proposed prior-driven score distillation; (2) it struggles to maintain
facial identity during editing based solely on instructions, lacking a well-deﬁned anchor point. In
contrast, B5 employs ControlNet-based InstructPix2Pix [84] with the proposed prior score distillation,
resulting in more view-consistent editing. Additionally, our IESD further uses the proposed edit scale
to merge two predicted scores, leading to better identity preservation and more effective editing. This
approach allows our method to overcome the limitations faced by the alternative solutions, producing
high-quality 3D avatars with improved ﬁne-grained editing results."
FURTHER ANALYSIS,0.47648902821316613,"Limitations and failure cases. While setting a new state-of-the-art, we acknowledge HeadSculpt
has limitations, as the failure cases in Fig. 9 demonstrate: (1) non-deformable results hinder further
extensions and applications in audio or video-driven problems; (2) generated textures are highly
saturated and less realistic, especially for characters with highly detailed appearances (e.g., Freddy
Krueger); (3) some inherited biases from Stable Diffusion [69] still remain, such as inaccurate
and stereotypical appearances of Asian characters (e.g., Sun Wukong and Japanese Geisha); and
(4) limitations inherited from InstructPix2Pix [5], such as the inability to perform large spatial
manipulations (e.g., remove his nose)."
CONCLUSIONS,0.47962382445141066,"5
Conclusions"
CONCLUSIONS,0.4827586206896552,"We have introduced HeadSculpt, a novel pipeline for generating high-resolution 3D human avatars
and performing identity-aware editing tasks through text. We proposed to utilize a prior-driven score
distillation that combines a landmark-based ControlNet and view-dependent textual inversion to
address the Janus problem. We also introduced identity-aware editing score distillation that preserves
both the original identity information and the editing instruction. Extensive evaluations demonstrated
that our HeadSculpt produces high-ﬁdelity results under various scenarios, outperforming state-of-
the-art methods signiﬁcantly."
CONCLUSIONS,0.48589341692789967,"Societal impact. The advancements in geometry and texture generation for human head avatars
could be deployed in many AR/VR use cases but also raises concerns about their potential malicious
use. We encourage responsible research and application, fostering open and transparent practices."
CONCLUSIONS,0.4890282131661442,"Acknowledgment. This work is partially supported by Hong Kong Research Grant Council - Early
Career Scheme (Grant No. 27208022) and HKU Seed Fund for Basic Research. We also thank the
anonymous reviewers for their constructive suggestions."
REFERENCES,0.49216300940438873,References
REFERENCES,0.4952978056426332,"[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. edifﬁ: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 3"
REFERENCES,0.49843260188087773,"[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360:
Unbounded anti-aliased neural radiance ﬁelds. In CVPR, 2022. 3"
REFERENCES,0.5015673981191222,"[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR,
2023. 3"
REFERENCES,0.5047021943573667,"[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black.
Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In ECCV, 2016. 3"
REFERENCES,0.5078369905956113,"[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing
instructions. In CVPR, 2023. 3, 4, 6, 7, 9, 10, 17"
REFERENCES,0.5109717868338558,"[6] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K Wong. Dreamavatar: Text-and-shape
guided 3d human avatar generation via diffusion models. arXiv preprint arXiv:2304.00916, 2023. 1, 3, 5, 7"
REFERENCES,0.5141065830721003,"[7] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K Wong. Guide3d: Create 3d avatars from
text and image guidance. arXiv preprint arXiv:2308.09705, 2023. 3"
REFERENCES,0.5172413793103449,"[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio
Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efﬁcient
geometry-aware 3D generative adversarial networks. In CVPR, 2022. 1, 3"
REFERENCES,0.5203761755485894,"[9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex:
Text-driven texture synthesis via diffusion models. In ICCV, 2023. 1"
REFERENCES,0.5235109717868338,"[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance
for high-quality text-to-3d content creation. In ICCV, 2023. 2, 3, 7, 22"
REFERENCES,0.5266457680250783,"[11] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. Tango: Text-driven photorealistic and
robust 3d stylization via lighting decomposition. In NeurIPS, 2023. 3"
REFERENCES,0.5297805642633229,"[12] CrucibleAI.
ControlNetMediaPipeFace.
https://huggingface.co/CrucibleAI/
ControlNetMediaPipeFace, 2023. 5, 6"
REFERENCES,0.5329153605015674,"[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS,
2020. 3"
REFERENCES,0.5360501567398119,"[14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.
Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,
2023. 3"
REFERENCES,0.5391849529780565,"[15] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an animatable detailed 3d face
model from in-the-wild images. ACM Transactions on Graphics (TOG), 2021. 1, 3"
REFERENCES,0.542319749216301,"[16] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nießner. Dynamic neural radiance ﬁelds for
monocular 4d facial avatar reconstruction. In CVPR, 2021. 3"
REFERENCES,0.5454545454545454,"[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In
ICLR, 2023. 3, 4, 6"
REFERENCES,0.54858934169279,"[18] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-
nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 2022.
9"
REFERENCES,0.5517241379310345,"[19] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Learning
deformable tetrahedral meshes for 3d reconstruction. In NeurIPS, 2020. 5"
REFERENCES,0.554858934169279,"[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 3"
REFERENCES,0.5579937304075235,"[21] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-
nerf2nerf: Editing 3d scenes with instructions. In ICCV, 2023. 6"
REFERENCES,0.5611285266457681,"[22] Amir Hertz, Kﬁr Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090,
2023. 3"
REFERENCES,0.5642633228840125,"[23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-
prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3"
REFERENCES,0.567398119122257,"[24] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free
evaluation metric for image captioning. In EMNLP, 2021. 9"
REFERENCES,0.5705329153605015,"[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3"
REFERENCES,0.5736677115987461,"[26] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion guidance. In NeurIPS Workshop, 2021. 6"
REFERENCES,0.5768025078369906,"[27] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip:
zero-shot text-driven generation and animation of 3d avatars. ACM Transactions on Graphics (TOG), 2022.
1, 3"
REFERENCES,0.5799373040752351,"[28] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: A real-time nerf-based
parametric head model. In CVPR, 2022. 1, 3"
REFERENCES,0.5830721003134797,"[29] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object
generation with dream ﬁelds. In CVPR, 2022. 3"
REFERENCES,0.5862068965517241,"[30] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In CVPR, 2019. 3"
REFERENCES,0.5893416927899686,"[31] Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko, and Matthias Grundmann. Real-time facial surface
geometry from monocular video on mobile gpus. In CVPR workshops, 2019. 3, 5"
REFERENCES,0.5924764890282131,"[32] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. 3"
REFERENCES,0.5956112852664577,"[33] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and Egor Zakharov. Realistic one-shot mesh-based
head avatars. In ECCV, 2022. 1"
REFERENCES,0.5987460815047022,"[34] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-mesh: Generating
textured meshes from text using pretrained image-text models. In SIGGRAPH Asia, 2022. 1, 3"
REFERENCES,0.6018808777429467,"[35] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6"
REFERENCES,0.6050156739811913,"[36] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular
primitives for high-performance differentiable rendering. ACM Transactions on Graphics (TOG), 2020. 5"
REFERENCES,0.6081504702194357,"[37] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho
Bae, and Choong Seon Hong. Generative ai meets 3d: A survey on text-to-3d in aigc era. arXiv preprint
arXiv:2305.06131, 2023. 3"
REFERENCES,0.6112852664576802,"[38] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. Learning a model of facial shape
and expression from 4d scans. ACM Transactions on Graphics (TOG), 2017. 1, 3, 5"
REFERENCES,0.6144200626959248,"[39] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,
Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In
CVPR, 2023. 2, 3, 5, 6, 7, 17"
REFERENCES,0.6175548589341693,"[40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.
Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 2"
REFERENCES,0.6206896551724138,"[41] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan
Zhang, Chuo-Ling Chang, Ming Yong, Juhyun Lee, et al. Mediapipe: A framework for perceiving and
processing reality. In CVPR workshops, 2019. 3, 5"
REFERENCES,0.6238244514106583,"[42] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruc-
tion of any object from a single image. In CVPR, 2023. 2"
REFERENCES,0.6269592476489029,"[43] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-
guided generation of 3d shapes and textures. In CVPR, 2023. 1, 3, 7, 8, 9, 19, 20, 21, 22"
REFERENCES,0.6300940438871473,"[44] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural
stylization for meshes. In CVPR, 2022. 1, 3"
REFERENCES,0.6332288401253918,"[45] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In ECCV, 2020. 1, 3"
REFERENCES,0.6363636363636364,"[46] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784,
2014. 3"
REFERENCES,0.6394984326018809,"[47] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-
adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv
preprint arXiv:2302.08453, 2023. 3"
REFERENCES,0.6426332288401254,"[48] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives
with a multiresolution hash encoding. ACM Transactions on Graphics (TOG), 2022. 3, 4, 5"
REFERENCES,0.64576802507837,"[49] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller,
and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, 2022. 3,
5"
REFERENCES,0.6489028213166145,"[50] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for
generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 1"
REFERENCES,0.6520376175548589,"[51] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geometry generation. In CVPR, 2022. 3"
REFERENCES,0.6551724137931034,"[52] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach.
Benchmark for
compositional text-to-image synthesis. In NeurIPS Datasets and Benchmarks Track (Round 1), 2021. 9"
REFERENCES,0.658307210031348,"[53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS, 2019. 6"
REFERENCES,0.6614420062695925,"[54] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.
In ICLR, 2022. 1, 2, 3, 4, 5, 6, 7, 9, 17"
REFERENCES,0.664576802507837,"[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In ICML, 2021. 1, 3"
REFERENCES,0.6677115987460815,"[56] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada,
Kﬁr Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d
generation. arXiv preprint arXiv:2303.13508, 2023. 2"
REFERENCES,0.670846394984326,"[57] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3"
REFERENCES,0.6739811912225705,"[58] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided
texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023. 1"
REFERENCES,0.677115987460815,"[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 4"
REFERENCES,0.6802507836990596,"[60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kﬁr Aberman. Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 3"
REFERENCES,0.6833855799373041,"[61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-
image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1, 3,
4"
REFERENCES,0.6865203761755486,"[62] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-
mal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In CVPR, 2022. 1,
3"
REFERENCES,0.6896551724137931,"[63] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
dataset for training next generation image-text models. In NeurIPS, 2022. 3"
REFERENCES,0.6927899686520376,"[64] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush
Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-ﬁltered 400
million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3"
REFERENCES,0.6959247648902821,"[65] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim,
Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust text-to-3d
generation. arXiv preprint arXiv:2303.07937, 2023. 2, 3, 5, 7, 8, 9, 19, 20"
REFERENCES,0.6990595611285266,"[66] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a
hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, 2021. 1, 3, 4, 5"
REFERENCES,0.7021943573667712,"[67] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,
Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In ICLR,
2023. 3"
REFERENCES,0.7053291536050157,"[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3"
REFERENCES,0.7084639498432602,"[69] Stability.AI. Stable diffusion. https://stability.ai/blog/stable-diffusion-public-release,
2022. 1, 3, 4, 6, 10, 17"
REFERENCES,0.7115987460815048,"[70] Stability.AI. Stability AI releases DeepFloyd IF, a powerful text-to-image model that can smartly integrate
text into images. https://stability.ai/blog/deepfloyd-if-text-to-image-model, 2023. 3"
REFERENCES,0.7147335423197492,"[71] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. Ide-3d: Interactive
disentangled editing for high-resolution 3d-aware portrait synthesis. ACM Transactions on Graphics
(TOG), 2022. 3"
REFERENCES,0.7178683385579937,"[72] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu.
Next3d: Generative neural texture rasterization for 3d-aware head avatars. In CVPR, 2023. 1, 3"
REFERENCES,0.7210031347962382,"[73] Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with stable-diffusion. https://github.com/ashawkey/
stable-dreamfusion, 2022. 6, 7, 8, 9, 16, 19, 20, 21"
REFERENCES,0.7241379310344828,"[74] Jiaxiang Tang. Fantasia3d.unofﬁcial. https://github.com/ashawkey/fantasia3d.unofficial,
2023. 8, 9, 16, 19, 20, 21, 22"
REFERENCES,0.7272727272727273,"[75] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d:
High-ﬁdelity 3d creation from a single image with diffusion prior. In ICCV, 2023. 2"
REFERENCES,0.7304075235109718,"[76] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.
Textmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint arXiv:2304.12439, 2023. 2"
REFERENCES,0.7335423197492164,"[77] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by
ﬁne tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477, 2022. 3"
REFERENCES,0.7366771159874608,"[78] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig
Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/
huggingface/diffusers, 2022. 6"
REFERENCES,0.7398119122257053,"[79] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian
chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2023. 2, 7"
REFERENCES,0.7429467084639498,"[80] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong
Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using
diffusion. In CVPR, 2023. 3"
REFERENCES,0.7460815047021944,"[81] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video
generation. In ICCV, 2023. 3"
REFERENCES,0.7492163009404389,"[82] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. Dream3d:
Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. In CVPR, 2023. 3"
REFERENCES,0.7523510971786834,"[83] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang,
Lan Xu, and Jingyi Yu. Dreamface: Progressive generation of animatable 3d faces under text guidance.
arXiv preprint arXiv:2304.03117, 2023. 3, 7, 8"
REFERENCES,0.7554858934169278,"[84] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv
preprint arXiv:2302.05543, 2023. 3, 4, 5, 6, 10"
REFERENCES,0.7586206896551724,"[85] Rui Zhao, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou, Zhenwei Shi, and Changjie Fan. Zero-shot
text-to-parameter translation for game character auto-creation. In CVPR, 2023. 3"
REFERENCES,0.7617554858934169,"[86] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong
Chen, Ming Zeng, and Fang Wen. General facial representation learning in a visual-linguistic manner. In
CVPR, 2022. 5"
REFERENCES,0.7648902821316614,"[87] Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C. Bühler, Xu Chen, Michael J. Black, and Otmar
Hilliges. I M Avatar: Implicit morphable head avatars from videos. In CVPR, 2022. 1, 3"
REFERENCES,0.768025078369906,"[88] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant volumetric head avatars. In CVPR, 2023. 3"
REFERENCES,0.7711598746081505,Table 2: Hyper-parameters of HeadSculpt.
REFERENCES,0.774294670846395,Camera setting
REFERENCES,0.7774294670846394,"θ range
(20, 110)
Radius range
(1.0, 1.5)
FoV range
(30, 50)"
REFERENCES,0.780564263322884,Render setting
REFERENCES,0.7836990595611285,"Resolution for coarse
(64, 64)
Resolution for ﬁne
(512, 512)
Max num steps sampled per ray
1024
Iter interval to update extra status
16"
REFERENCES,0.786833855799373,Diffusion setting
REFERENCES,0.7899686520376176,"Guidance scale
100
t range
(0.02, 0.98)
ω(t)
√αt(1 −αt)"
REFERENCES,0.7931034482758621,Training setting
REFERENCES,0.7962382445141066,"#Iterations for coarse
70k
#Iterations for ﬁne
50k
Batch size
4
LR of grid encoder
1e-3
LR of NeRF MLP
1e-3
LR of si and ∆vi in DMTET
1e-2
LR scheduler
constant
Warmup iterations
20k
Optimizer
Adam (0.9, 0.99)
Weight decay
0
Precision
fp16"
REFERENCES,0.799373040752351,"Hardware
GPU
1 × Tesla V100 (32GB)
Training duration
1h (coarse) + 1h (ﬁne)"
REFERENCES,0.8025078369905956,"A
Implementation details"
REFERENCES,0.8056426332288401,"A.1
Details about 3D scene models"
REFERENCES,0.8087774294670846,"In the coarse stage, we make use of the grid frequency encoder γ(·) from the publicly available Stable
DreamFusion [73]. This encoder maps the input x ∈R3 to a higher-frequency dimension, yielding
γ(x) ∈R32. The MLP within our NeRF model consists of three layers with dimensions [32, 64, 64,
3+1+3]. Here, the output channels ‘3’, ‘1’, and ‘3’ represent the predicted normals, density value, and
RGB colors, respectively. In the ﬁne stage, we directly optimize the signed distance value si ∈R,
along with a position offset ∆vi ∈R3 for each vertex vi. We found that ﬁtting si and vi into MLP,
as done by Fantasia3D [74], often leads to diverged training."
REFERENCES,0.8119122257053292,"To ensure easy reproducibility, we have included all the hyperparameters used in our experiments in
Tab 2. The other hyper-parameters are set to be the default of Stable-DreamFusion [73]."
REFERENCES,0.8150470219435737,"A.2
Details about textual inversion"
REFERENCES,0.8181818181818182,"In the main paper, we discussed the collection of a tiny dataset consisting of 34 images depicting
the back view of heads. This dataset was used to train a special token, <back-view>, to address the
ambiguity associated with the back view of landmarks. The images in the dataset were selected to
encompass a diverse range of gender, color, age, and other characteristics. A few samples from the
dataset are shown in Fig. 11. While our simple selection strategy has proven effective in our speciﬁc
case, we believe that a more reﬁned collection process could further enhance the controllability
of the learned <back-view> token. We use the default training recipe provided by HuggingFace
Diffusers 2, which took us 1 hour on a single Tesla V100 GPU."
REFERENCES,0.8213166144200627,2https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion
REFERENCES,0.8244514106583072,Figure 11: Samples of the tiny dataset collected for learning <back-view> token.
REFERENCES,0.8275862068965517,"B
Further analysis"
REFERENCES,0.8307210031347962,"B.1
Effectiveness of textual inversion on 2D generation"
REFERENCES,0.8338557993730408,"To show the effectiveness of the learned <back-view> token, we conduct an analysis of its control
capabilities in the context of 2D generation results. Speciﬁcally, we compare two generation results
using Stable Diffusion [69], with both experiments sharing the same random seed. One experiment
has the plain text prompt appended with the plain phrase “back view,” while the other experiment
utilizes the learned special token <back-view> in the prompt. We present a selection of randomly
generated results in Fig. 12. The observations indicate that the <back-view> token effectively
inﬂuences the pose of the generated heads towards the back, resulting in a distinct appearance.
Remarkably, the <back-view> token demonstrates a notable generalization ability, as evidenced by
the Batman case, despite not having been trained speciﬁcally on back views of Batman in the textual
inversion process."
REFERENCES,0.8369905956112853,"B.2
Inherent bias in 2D diffusion models"
REFERENCES,0.8401253918495298,"In our main paper, we discussed the motivation behind our proposed identity-aware editing score
distillation (IESD), which can be attributed to two key factors. Firstly, the limitations of prompt-
based editing [54, 39] are due to the inherent bias present in Stable Diffusion (SD). Secondly, while
InstructPix2Pix (IP2P) [5] offers a solution by employing instruction-based editing to mitigate bias, it
often results in identity loss. To further illustrate this phenomenon, we showcase the biased 2D outputs
of SD and ControlNet-based IP2P in Fig. 13. Modiﬁed descriptions and instructions are utilized
in these respective methods to facilitate the editing process and achieve the desired results. The
results provide clear evidence of the following: (1) SD generates biased outcomes, with a tendency to
underweight the “older” aspect and overweight the “skull” aspect in the modiﬁed description; (2)
IP2P demonstrates the ability to edit the image successfully, but it faces challenges in preserving the
identity of the avatar."
REFERENCES,0.8432601880877743,"The aforementioned inherent biases are ampliﬁed in the domain of 3D generation (refer to Fig. 10
in the main paper) due to the optimization process guided by SDS loss, which tends to prioritize
view consistency at the expense of sacriﬁcing prominent features. To address this issue, our proposed
IESD approach combines two types of scores: one for editing and the other for identity preservation.
This allows us to strike a balance between preserving the initial appearance and achieving the desired
editing outcome."
REFERENCES,0.8463949843260188,"w/ “back view”
w/ <back-view>
w/ “back view”
w/ <back-view>
w/ “back view”
w/ <back-view>"
REFERENCES,0.8495297805642633,"seed: 413
seed: 16772
seed: 40805"
REFERENCES,0.8526645768025078,a DSLR portrait of Obama
REFERENCES,0.8557993730407524,"seed: 50682
seed: 93440
seed: 96458"
REFERENCES,0.8589341692789969,a DSLR portrait of Hillary Clinton
REFERENCES,0.8620689655172413,"seed: 2367
seed: 19656
seed: 62156"
REFERENCES,0.8652037617554859,a DSLR portrait of a boy with facial painting
REFERENCES,0.8683385579937304,"seed: 53236
seed: 62424
seed: 72649"
REFERENCES,0.8714733542319749,a DSLR portrait of Batman
REFERENCES,0.8746081504702194,"Figure 12: Analysis of the learned <back-view> on 2D image generation. For each pair of images,
we present two 2D images generated with the same random seed, where the left image is conditioned
on the plain text ""back view"" and the right image is conditioned on the <back-view> token."
REFERENCES,0.877742946708464,"Landmark Map
Stable Diffusion
Reference Image
InstructPix2Pix"
REFERENCES,0.8808777429467085,"seed: 19056
seed: 72854
seed: 50233
seed: 64136"
REFERENCES,0.8840125391849529,"Modiﬁed description: a DSLR portrait of +[older] Saul Goodman
Instruction: make him older"
REFERENCES,0.8871473354231975,"seed: 5427
seed: 91282
seed: 60104
seed: 88141"
REFERENCES,0.890282131661442,"Modiﬁed description: a DSLR portrait skull of Vincent van Gogh
Instruction: turn his face into a skull"
REFERENCES,0.8934169278996865,"Figure 13: Analysis of the inherent bias in 2D diffusion models. For each case, we display
several 2D outputs of SD and IP2P, utilizing modiﬁed descriptions and instructions, respectively, with
reference images from our coarse-stage NeRF model to facilitate the editing process."
REFERENCES,0.896551724137931,"DreamFusion* [73]
Latent-NeRF [43]
3DFuse [65]
Fantasia3D* [74]
HeadSculpt (Ours)"
REFERENCES,0.8996865203761756,a DSLR portrait of Batman
REFERENCES,0.9028213166144201,a DSLR portrait of Black Panther in Marvel
REFERENCES,0.9059561128526645,a DSLR portrait of Two-face in DC
REFERENCES,0.9090909090909091,a DSLR portrait of Doctor Strange
REFERENCES,0.9122257053291536,a head of Terracotta Army
REFERENCES,0.9153605015673981,Figure 14: Additional comparisons with existing methods on generation (Part 1). *Non-ofﬁcial.
REFERENCES,0.9184952978056427,"C
Additional qualitative comparisons"
REFERENCES,0.9216300940438872,"C.1
Comparison with existing methods on generation results"
REFERENCES,0.9247648902821317,"We provide more qualitative comparisons with four baseline methods [73, 43, 65, 74] in Fig. 14 and
Fig. 15. These results serve to reinforce the claims made in Sec. 4.1 of the main paper, providing
further evidence of the superior performance of our HeadSculpt in generating high-ﬁdelity head
avatars. These results showcase the ability of our method to capture intricate details, realistic textures,
and overall visual quality, solidifying its position as a state-of-the-art solution in this task."
REFERENCES,0.9278996865203761,"Notably, to provide a more immersive and comprehensive understanding of our results, we include
multiple outcomes of our HeadSculpt in the form of 360◦rotating videos. These videos can be
accessed on https://brandonhan.uk/HeadSculpt, enabling viewers to observe the generated
avatars from various angles and perspectives."
REFERENCES,0.9310344827586207,"DreamFusion* [73]
Latent-NeRF [43]
3DFuse [65]
Fantasia3D* [74]
HeadSculpt (Ours)"
REFERENCES,0.9341692789968652,a head of Simpson in the Simpsons
REFERENCES,0.9373040752351097,a head of Naruto Uzumaki
REFERENCES,0.9404388714733543,a DSLR portrait of Napoleon Bonaparte
REFERENCES,0.9435736677115988,a DSLR portrait of Leo Tolstoy
REFERENCES,0.9467084639498433,a DSLR portrait of Audrey Hepburn
REFERENCES,0.9498432601880877,a DSLR portrait of Obama with a baseball cap
REFERENCES,0.9529780564263323,a DSLR portrait of Taylor Swift
REFERENCES,0.9561128526645768,Figure 15: Additional comparisons with existing methods on generation (Part 2). *Non-ofﬁcial.
REFERENCES,0.9592476489028213,"DreamFusion* [73]
Latent-NeRF [43]
Fantasia3D* [74]
HeadSculpt (Ours)"
REFERENCES,0.9623824451410659,a DSLR portrait of Saul Goodman
REFERENCES,0.9655172413793104,"a DSLR portrait of +[older] Saul Goodman
make him older"
REFERENCES,0.9686520376175548,a DSLR portrait of Vincent van Gogh
REFERENCES,0.9717868338557993,"a DSLR portrait skull of Vincent van Gogh
turn his face into a skull"
REFERENCES,0.9749216300940439,Figure 16: Comparisons with existing methods on editing.*Non-ofﬁcial.
REFERENCES,0.9780564263322884,"C.2
Comparison with existing methods on editing results"
REFERENCES,0.9811912225705329,"Since the absence of alternative methods speciﬁcally designed for editing, we conduct additional
evaluations of the editing results generated by existing methods by modifying the text prompts.
Fig. 16 illustrates that bias in editing is a pervasive issue encountered by all the baselines. This
bias stems from the shared SDS guidance function, which is based on a diffusion prior, despite the
variations in representation and optimization methods employed by these approaches. Instead, IESD
enables the guidance function to incorporate information from two complementary sources: (1) the
original image gradient, which preserves identity, and (2) the editing gradient, which captures desired
modiﬁcations. By considering both terms, our approach grants more explicit and direct control over
the editing process compared to the conventional guidance derived solely from the input."
REFERENCES,0.9843260188087775,"Latent-NeRF [43]
Fantasia3D* [74]
HeadSculpt (Ours)"
REFERENCES,0.987460815047022,a head of ant-man in Marvel
REFERENCES,0.9905956112852664,"Figure 17: Results across random seeds (0, 1, 2).*Non-ofﬁcial."
REFERENCES,0.9937304075235109,"C.3
Comparison with existing methods on stability"
REFERENCES,0.9968652037617555,"We observe that all baselines tend to have diverged training processes as they do not integrate 3D
prior to the diffusion model. Taking two shape-guided prior methods (i.e., Latent-NeRF [43] and
Fantasia3D [10]) as examples, we compare their generation results and ours across different random
seeds. We conduct comparisons under the same default hyper-parameters and present the results in
Fig. 17. We notice that prior methods need to try several runs to get the best generation while ours
can achieve consistent results among different runs. Our method is thus featured with stable training,
without the need for cherry-picking over many runs."
