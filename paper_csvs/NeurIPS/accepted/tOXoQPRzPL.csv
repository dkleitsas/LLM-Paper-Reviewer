Section,Section Appearance Order,Paragraph
BYTEDANCE,0.0,"1 ByteDance
2 Technical University Munich
* equal contribution
https://yucornetto.github.io/projects/titok.html"
TOKENS,0.0025252525252525255,"32 tokens
TiTok (ours)"
TOKENS,0.005050505050505051,256 tokens VQGAN
PIXELS,0.007575757575757576,65536 pixels
PIXELS,0.010101010101010102,real image
PIXELS,0.012626262626262626,latent size
PIXELS,0.015151515151515152,and costs
PIXELS,0.017676767676767676,Image Reconstruction
PIXELS,0.020202020202020204,32 tokens can work well for‚Ä¶
PIXELS,0.022727272727272728,Image Generation (TiTok 32 tokens)
PIXELS,0.025252525252525252,"Figure 1: We propose TiTok, a compact 1D tokenizer leveraging region redundancy to represent an
image with only 32 tokens for image reconstruction and generation."
ABSTRACT,0.027777777777777776,Abstract
ABSTRACT,0.030303030303030304,"Recent advancements in generative models have highlighted the crucial role of
image tokenization in the efficient synthesis of high-resolution images. Tokeniza-
tion, which transforms images into latent representations, reduces computational
demands compared to directly processing pixels and enhances the effectiveness
and efficiency of the generation process. Prior methods, such as VQGAN, typically
utilize 2D latent grids with fixed downsampling factors. However, these 2D tok-
enizations face challenges in managing the inherent redundancies present in images,
where adjacent regions frequently display similarities. To overcome this issue,
we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative
approach that tokenizes images into 1D latent sequences. TiTok provides a more
compact latent representation, yielding substantially more efficient and effective
representations than conventional techniques. For example, a 256 √ó 256 √ó 3 image
can be reduced to just 32 discrete tokens, a significant reduction from the 256 or
1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves
competitive performance to state-of-the-art approaches. Specifically, using the
same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT
baseline significantly by 4.21 at ImageNet 256 √ó 256 benchmark. The advantages
of TiTok become even more significant when it comes to higher resolution. At
ImageNet 512 √ó 512 benchmark, TiTok not only outperforms state-of-the-art dif-
fusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens
by 64√ó, leading to 410√ó faster generation process. Our best-performing variant
can significantly surpass DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating
high-quality samples 74√ó faster."
INTRODUCTION,0.03282828282828283,"1
Introduction"
INTRODUCTION,0.03535353535353535,"In recent years, image generation has experienced remarkable progress, driven by the significant
advancements in both transformers [19, 65, 70, 10, 71, 72] and diffusion models [16, 58, 29, 52, 21].
Mirroring the trends in generative language models [51, 62], the architecture of many contemporary
image generation models incorporate a standard image tokenizer and de-tokenizer. This array of
models utilizes tokenized image representations‚Äîranging from continuous [35] to discrete vec-
tors [57, 64, 19]‚Äîto perform a critical function: translating raw pixels into a latent space. The latent
space (e.g., 32 √ó 32) is significantly more compact than the original image space (256 √ó 256 √ó 3). It
offers a compressed yet expressive representation, and thus not only facilitates efficient training and
inference of generative models but also paves the way to scale up the model size."
INTRODUCTION,0.03787878787878788,"Although image tokenizers achieve great success in image generation workflows, they encounter a
fundamental limitation tied to their intrinsic design. These tokenizers are based on an assumption
that the latent space should retain a 2D structure, to maintain a direct mapping for locations between
the latent tokens and image patches. For example, the top-left latent token directly corresponds to
the top-left image patch. This restricts the tokenizer‚Äôs ability to effectively leverage the redundancy
inherent in images to cultivate a more compressed latent space."
INTRODUCTION,0.04040404040404041,"Taking one step back, we raise the question ‚Äúis 2D structure necessary for image tokenization?‚Äù
To answer the question, we draw inspiration from several image understanding tasks where model
predictions are based solely on high-level information extracted from input images ‚Äîsuch as in
image classification [17], object detection [8, 81], segmentation [67, 34, 74, 75], and multi-modal
large language models [1, 41, 11]. These tasks do not need de-tokenizers, since the outputs typically
manifest in specific structures other than images. In other words, they often format a higher-level
1D sequence as output that can still capture all task-relevant information. Prior arts, such as object
queries [8, 67] or the perceiver resampler [1], encode images into a 1D sequence of a predetermined
number of tokens (e.g., 64). These tokens facilitate the generation of outputs like bounding boxes or
captions. The success of these methods motivates us to investigate a more compact 1D sequence as
image latent representation in the context of image reconstruction and generation. It is noteworthy that
the synthesis of both high-level and low-level information is crucial for the generation of high-quality
images, providing a challenge for extremely compact latent representations."
INTRODUCTION,0.04292929292929293,"In this work, we introduce a transformer-based framework [65, 17] designed to tokenize an image
to a 1D discrete sequence, which can later be decoded back to the image space via a de-tokenizer.
Specifically, we present Transformer-based 1-Dimensional Tokenizer (TiTok), consisting of a Vision
Transformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical Vector-
Quantized (VQ) model designs [19]. In the tokenization phase, the image is split and flattened into a
series of patches, followed by concatenation with a 1D sequence of latent tokens. After the feature
encoding process of ViT encoder, these latent tokens build the latent representation of the image.
Subsequent to the vector quantization step [64, 19], the ViT decoder reconstructs the input images
from the masked token sequence [15, 24]."
INTRODUCTION,0.045454545454545456,"Building upon TiTok, we conduct extensive experiments to probe the dynamics of 1D image tokeniza-
tion. Our investigation studies the interplay between latent space size, model size, reconstruction
fidelity, and generative quality. From this exploration, several compelling insights emerge:"
INCREASING THE NUMBER OF LATENT TOKENS REPRESENTING AN IMAGE CONSISTENTLY IMPROVES THE,0.047979797979797977,"1. Increasing the number of latent tokens representing an image consistently improves the
reconstruction performance, yet the benefit becomes marginal after 128 tokens. Intriguingly,
32 tokens are sufficient for a reasonable image reconstruction."
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.050505050505050504,"2. Scaling up the tokenizer model size significantly improves performance of both reconstruc-
tion and generation, especially when number of tokens is limited (e.g., 32 or 64), showcasing
a promising pathway towards a compact image representation at latent space."
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.05303030303030303,"3. 1D tokenization breaks the grid constraints in prior 2D image tokenizers, which not only
enables each latent token to reconstruct regions beyond a fixed image grid and leads to a
more flexible tokenizer design, but also learns more high-level and semantic-rich image
information, especially at a compact latent space."
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.05555555555555555,"4. 1D tokenization exhibits superior performance in generative training, with not only a signifi-
cant speed-up for both training and inference but also a competitive FID score compared to
a typical 2D tokenizer, while using much fewer tokens."
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.05808080808080808,"MaskGIT
U-ViT-H/4 DiT-XL/2
U-ViT-L/4"
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.06060606060606061,faster sample speed
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.06313131313131314,better generation quality
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.06565656565656566,"(a) Model comparison on resolution of 256
(b) Model comparison on resolution of 512 ADM"
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.06818181818181818,"ViT-VQGAN
LDM-4
U-ViT-H/2"
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.0707070707070707,DiT-XL/2
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.07323232323232323,TiTok-S-128
SCALING UP THE TOKENIZER MODEL SIZE SIGNIFICANTLY IMPROVES PERFORMANCE OF BOTH RECONSTRUC-,0.07575757575757576,TiTok-B-64
X FASTER,0.07828282828282829,150x faster
X FASTER,0.08080808080808081,13x faster
X FASTER,0.08333333333333333,U-ViT-L/2
X FASTER,0.08585858585858586,faster sample speed
X FASTER,0.08838383838383838,better generation quality
X FASTER,0.09090909090909091,FID lower bound (val set) 1.78
"X
FASTER",0.09343434343434344,"333x
faster"
X FASTER,0.09595959595959595,410x faster
X FASTER,0.09848484848484848,TiTok-L-64
X FASTER,0.10101010101010101,TiTok-B-128
X FASTER,0.10353535353535354,TiTok-L-32
X FASTER,0.10606060606060606,169x faster 1.69
X FASTER,0.10858585858585859,FID lower bound (val set)
X FASTER,0.1111111111111111,"Figure 2: A speed and quality comparison of TiTok and prior arts on ImageNet 256 √ó 256 and
512 √ó 512 generation benchmarks. Speed-up is compared against DiT-XL/2 [52]. The sampling
speed (de-tokenization included) is measured with an A100 GPU."
X FASTER,0.11363636363636363,"In light of these findings, we introduce the TiTok family, encompassing models of varying model
sizes and latent sizes, capable of achieving highly compact tokenization with as few as 32 tokens. We
further confirm the model‚Äôs efficacy in image generation through the MaskGIT [9] framework. TiTok
is demonstrated to facilitate state-of-the-art performance in image generation, while requiring latent
spaces that are 8√ó to 64√ó smaller, resulting in significant accelerations during both the training and
inference phases. It also generates images with similar or higher quality but up to 410√ó faster than
state-of-the-art diffusion models such as DiT [18] (Fig. 2)."
RELATED WORK,0.11616161616161616,"2
Related Work"
RELATED WORK,0.11868686868686869,"Image Tokenization. Images have been compressed since the early days of deep learning with
autoencoders [27, 66]. The general design of using an encoder that compresses high-dimensional
images into a low-dimensional latent representation and then using a decoder to reverse the process,
has proven to be successful over the years. Variational Autoencoders (VAEs) [35] extend the paradigm
by learning to map the input to a distribution. Instead of modeling a continuous distribution, VQ-
VAEs [50, 56] learn a discrete representation forming a categorical distribution. VQGAN [19] further
improves the training process by using adversarial training [23]. The transformer design of the
autoencoder is further explored in ViT-VQGAN [69] and Efficient-VQGAN [7]. Orthogonal to this,
RQ-VAE [37] and MoVQ [80] study the effect of using multiple vector quantization steps per latent
embedding, while MAGVIT-v2 [72] and FSQ [48] propose a lookup-free quantization. However,
all aforementioned works share the same workflow of an image always being patchwise encoded
into a 2D grid latent representation. In this work, we explore an innovative 1D sequence latent
representation for image reconstruction and generation."
RELATED WORK,0.12121212121212122,"Tokenization for Image Understanding. For image understanding tasks (e.g., image classifica-
tion [17], object detection [8, 81, 78], segmentation [67, 74, 76], and Multi-modal Large Language
Models (MLLMs) [1, 41, 77]), it is common to use a general feature encoder instead of an autoen-
coder to tokenize the image. Specifically, many MLLMs [41, 43, 61, 32, 22, 11] uses a CLIP [54]
encoder to tokenize the image into highly semantic tokens, which proves effective for image cap-
tioning [13] and VQA [2]. Some MLLMs also explore discrete tokens [32, 22] or ‚Äúde-tokenize‚Äù the
CLIP embeddings back to images through diffusion models [61, 32, 22]. However, due to the nature
of CLIP models that focus on high-level information, these methods can only reconstruct an image
with high-level semantic similarities (i.e., the layouts and details are not well-reconstructed due to
CLIP features). Therefore, our method is significantly different from theirs, since the proposed TiTok
aims to reconstruct both the high-level and low-level details of an image, same as typical VQ-VAE
tokenizers [35, 57, 19]."
RELATED WORK,0.12373737373737374,"Image Generation. Image generation methods range from sampling the VAE [35], using GANs [23]
to Diffusion Models [16, 58, 29, 52, 21, 44] and autoregressive models [63, 12, 50]. Prior studies
that are most related to this work build on top of a learned VQ-VAE codebook to generate images.
Autoregressive transformer [19, 69, 7, 37], similar to decoder-only language models, model each
patch in a step-by-step fashion, thus requiring as many steps as token number, e.g., 256 or 1024. Non-"
RELATED WORK,0.12626262626262627,". . .
mask tokens"
RELATED WORK,0.12878787878787878,Vision Transformer
RELATED WORK,0.13131313131313133,". . .
image patches"
RELATED WORK,0.13383838383838384,". . .
latent tokens . ."
RELATED WORK,0.13636363636363635,"codebook
0
1 N . . ."
RELATED WORK,0.1388888888888889,". . .
input"
RELATED WORK,0.1414141414141414,reconstruction ùë∏ùíñùíÇùíèùíï
RELATED WORK,0.14393939393939395,". . .
1
19
0
ùë´ùíÜùíÑ
Vision Transformer"
RELATED WORK,0.14646464646464646,"ùë¨ùíèùíÑ
(a) Image Reconstruction"
RELATED WORK,0.14898989898989898,"K (e.g., 32)
latent tokens
input TiTok"
RELATED WORK,0.15151515151515152,encoder TiTok
RELATED WORK,0.15404040404040403,decoder VQ
RELATED WORK,0.15656565656565657,reconstruction
RELATED WORK,0.1590909090909091,(b) Image Generation
RELATED WORK,0.16161616161616163,"latent
tokens"
RELATED WORK,0.16414141414141414,masked
RELATED WORK,0.16666666666666666,tokens
RELATED WORK,0.1691919191919192,Bidirectional
RELATED WORK,0.1717171717171717,Transformer
RELATED WORK,0.17424242424242425,predicted
RELATED WORK,0.17676767676767677,tokens
RELATED WORK,0.17929292929292928,(c) TiTok Tokenization 1 . . . 19 0 1 . . . 19 0 1 . . . 0 1 . . . 19 0
RELATED WORK,0.18181818181818182,"Figure 3: Illustration of image reconstruction (a) and generation (b) with the TiTok framework
(c). TiTok contains an encoder Enc, a quantizer Quant, and a decoder Dec. Image patches, along
with a few (e.g., 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The
latent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [15, 24],
are fed to the ViT decoder to reconstruct the image."
RELATED WORK,0.18434343434343434,"autoregressive (or bidirectional) transformers [80, 72, 68], such as MaskGIT [9], generally predict
more than a single token per step and thus require significantly fewer steps to predict a complete
image. Apart from that, further studies looked into improved sampling strategies [39, 40, 38]. As we
focus on the tokenization stage, we apply the commonly used non-autoregressive sampling scheme
of MaskGIT to generate a sequence of tokens that is later decoded into an image."
METHOD,0.18686868686868688,"3
Method"
PRELIMINARY BACKGROUND ON VQ-VAE,0.1893939393939394,"3.1
Preliminary Background on VQ-VAE"
PRELIMINARY BACKGROUND ON VQ-VAE,0.1919191919191919,"The image tokenizer plays a pivotal role in facilitating the generative models by providing a compact
image representation at latent space. For the scope of our discussion, we primarily focus on the Vector-
Quantized (VQ) tokenizer [64, 19], given its broad applicability across various domains, including
but not limited to image and video generation [19, 9, 58, 71], large-scale pretraining [12, 5, 49, 3, 18]
and multi-modal models [20, 73]."
PRELIMINARY BACKGROUND ON VQ-VAE,0.19444444444444445,"A typical VQ model contains three key components: an encoder Enc, a vector quantizer Quant, and
a decoder Dec. Given an input image I ‚ààRH√óW √ó3, where H and W denote the image‚Äôs height
and width, the image is initially processed by the encoder Enc and converted to latent embeddings
Z2D = Enc(I), where Z2D ‚ààR
H f √ó W"
PRELIMINARY BACKGROUND ON VQ-VAE,0.19696969696969696,"f √óD, which downsamples the spatial dimensions by a factor of
f. Subsequently, each embedding z ‚ààRD is mapped (via the vector quantizer Quant) to the nearest
code ci ‚ààRD in a learnable codebook C ‚ààRN√óD, comprising N codes. Formally, we have:"
PRELIMINARY BACKGROUND ON VQ-VAE,0.1994949494949495,"Quant(z) = ci, where i =
argmin
j‚àà{1,2,...,N}
‚à•z ‚àícj‚à•2.
(1)"
PRELIMINARY BACKGROUND ON VQ-VAE,0.20202020202020202,"During de-tokenization, the reconstructed image ÀÜI is obtained via the decoder Dec as follows:"
PRELIMINARY BACKGROUND ON VQ-VAE,0.20454545454545456,"ÀÜI = Dec(Quant(Z2D)).
(2)"
PRELIMINARY BACKGROUND ON VQ-VAE,0.20707070707070707,"Despite the numerous improvements over VQ-VAE [64] (e.g., loss function [19], model architec-
ture [69], and quantization/codebook strategies [80, 37, 72]), the fundamental workflow (e.g., the 2D
grid-based latent representations) has largely remained unchanged."
PRELIMINARY BACKGROUND ON VQ-VAE,0.20959595959595959,"3.2
TiTok: From 2D to 1D Tokenization"
PRELIMINARY BACKGROUND ON VQ-VAE,0.21212121212121213,"While existing VQ models have demonstrated significant achievements, a notable limitation within the
standard workflow exists: the latent representation Z2D is often envisioned as a static 2D grid. Such a
configuration inherently assumes a strict one-to-one mapping between the latent grids and the original
image patches. This assumption limits the VQ model‚Äôs ability to fully exploit the redundancies
present in images, such as similarities among adjacent patches. Additionally, this approach constrains"
PRELIMINARY BACKGROUND ON VQ-VAE,0.21464646464646464,"the flexibility in selecting the latent size, with the most prevalent configurations being f = 4, f = 8,
or f = 16 [58], resulting in 4096, 1024, or 256 tokens for an image of dimensions 256 √ó 256 √ó 3.
Inspired by the success of 1D sequence representations in addressing a broad spectrum of computer
vision problems [8, 1, 41], we propose to use a 1D sequence, without the fixed correspondence
between latent representation and image patches in 2D tokenization, as an efficient and effective
latent representation for image reconstruction and generation."
PRELIMINARY BACKGROUND ON VQ-VAE,0.21717171717171718,"Image Reconstruction with TiTok.
To initiate our exploration, we establish a novel frame-
work named Transformer-based 1-Dimensional Tokenizer (TiTok), leveraging Vision Transformer
(ViT) [17]1 to tokenize images into 1D latent tokens and subsequently reconstruct the original images
from these 1D latents. As depicted in Fig. 3, TiTok employs a standard ViT for both the tokenization
and de-tokenization processes (i.e., both the encoder Enc and decoder Dec are ViTs). During
tokenization, we patchify the image into patches (with a patch embedding layer) P ‚ààR
H f √ó W f √óD"
PRELIMINARY BACKGROUND ON VQ-VAE,0.2196969696969697,"(with patch size equal to the downsampling factor f and embedding dimension D) and concatenate
them with K latent tokens L ‚ààRK√óD. They are then fed into the ViT encoder Enc. In the encoder
output, we only retain the latent tokens as the image‚Äôs latent representation, thereby enabling a more
compact latent representation of 1D sequence Z1D (with length K). This adjustment decouples the
latent size from image‚Äôs resolution and allows more flexibility in design choices. That is, we have:"
PRELIMINARY BACKGROUND ON VQ-VAE,0.2222222222222222,"Z1D = Enc(P ‚äïL),
(3)"
PRELIMINARY BACKGROUND ON VQ-VAE,0.22474747474747475,"where ‚äïdenotes concatenation, and we only retain the latent tokens from the encoder output."
PRELIMINARY BACKGROUND ON VQ-VAE,0.22727272727272727,"In the de-tokenization phase, drawing inspiration from [15, 5, 24], we incorporate a sequence of
mask tokens M ‚ààR
H f √ó W"
PRELIMINARY BACKGROUND ON VQ-VAE,0.2297979797979798,f √óD‚Äîobtained by replicating a single mask token H f √ó W
PRELIMINARY BACKGROUND ON VQ-VAE,0.23232323232323232,"f times‚Äîto the
quantized latent tokens Z1D. The image is then reconstructed via the ViT decoder Dec as follows:"
PRELIMINARY BACKGROUND ON VQ-VAE,0.23484848484848486,"ÀÜI = Dec(Quant(Z1D) ‚äïM),
(4)"
PRELIMINARY BACKGROUND ON VQ-VAE,0.23737373737373738,"where the latent tokens Z1D is first vector-quantized by Quant and then concatenated with the mask
tokens M before feeding to the decoder Dec."
PRELIMINARY BACKGROUND ON VQ-VAE,0.2398989898989899,"Despite its simplicity, we emphasize that the concept of compact 1D image tokenization remains
underexplored in existing literature. The proposed TiTok thus serves as a foundational platform for
exploring the potentials of 1D tokenization and de-tokenization for natural images. It is worth noting
that although one may flatten 2D grid latents into a 1D sequence, it significantly differs from the
proposed 1D tokenizer, due to the fact that the implicit 2D grid mapping constraints still persist."
PRELIMINARY BACKGROUND ON VQ-VAE,0.24242424242424243,"Image Generation with TiTok. Besides the image reconstruction task which the tokenizer is
trained for, we also evaluate its effectiveness for image generation, following the typical pipeline [19,
9]. Specifically, we adopt MaskGIT [9] as our generation framework due to its simplicity and
effectiveness, allowing us to train a MaskGIT model by simply replacing its VQGAN tokenizer with
our TiTok. We do not make any other specific modifications to MaskGIT, but for completeness, we
briefly describe its whole generation process with TiTok."
PRELIMINARY BACKGROUND ON VQ-VAE,0.24494949494949494,"The image is pre-tokenized into 1D discrete tokens. At each training step, a random ratio of the latent
tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token
sequence as input, and predicts the corresponding discrete token ID of those masked tokens. The
inference process consists of multiple sampling steps, where at each step the transformer‚Äôs prediction
for masked tokens will be sampled based on the prediction confidence, which are then used to update
the masked images. In this way, the image is ‚Äúprogressively generated‚Äù from a sequence full of
mask tokens to an image with generated tokens, which can later be de-tokenized back into pixel
spaces. The MaskGIT framework shows a significant speed-up in the generation process compared to
auto-regressive models. We refer readers to [9] for more details."
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.2474747474747475,"3.3
Two-Stage Training of TiTok with Proxy Codes"
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.25,"Existing Training Strategies for VQ Models. Although most VQ models adhere to a straightforward
formulation, their training process is notably sensitive, and the model‚Äôs performance is heavily
influenced by the adoption of more effective training paradigms. For instance, VQGAN [19] achieves"
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.25252525252525254,"1Although other Transformer-based architectures (e.g., Swin [45]) can also be used to instantiate TiTok, we
adopt ViT for its simplicity and effectiveness."
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.255050505050505,"a significant improvement in reconstruction FID (rFID) on the ImageNet [14] validation set, when
compared to dVAE from DALL-E [55]. This enhancement is attributed to advancements in perceptual
loss [33, 79] and adversarial loss [23]. Moreover, MaskGIT‚Äôs modern implementation of VQGAN [9]
utilizes refined training techniques without architectural improvements to boost the performance
further. Notably, most of these improvements are exclusively applied during the training phase (i.e.,
through auxiliary losses) and significantly affect the models‚Äô efficacy. Given the complexity of the
loss functions, extensive tuning of hyper-parameters involved, and, most critically, the missing of
a publicly available code-base for reference or reproduction [9, 69, 72], establishing an optimal
experimental setup for the proposed TiTok presents a substantial challenge, especially when the target
is a compact 1D tokenization which was rarely studied in literature."
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.25757575757575757,"Two-Stage Training Comes to the Rescue. Although training TiTok with the typical Taming-
VQGAN [19] setting is feasible, we introduce a two-stage training paradigm for an improved
performance. The two-stage training strategy contains ‚Äúwarm-up‚Äù and ‚Äúdecoder fine-tuning‚Äù stages.
Specifically, in the first ‚Äúwarm-up‚Äù stage, instead of directly regressing the RGB values and employing
a variety of loss functions (as in existing methods), we propose to train 1D VQ models with the discrete
codes generated by an off-the-shelf MaskGIT-VQGAN model, which we refer to as proxy codes. This
approach allows us to bypass the intricate loss functions and GAN architectures, thereby concentrating
our efforts on optimizing the 1D tokenization settings. Importantly, this modification does not harm
the functionality of the tokenizer and quantizer within TiTok, which can still fully function for image
tokeniztion and de-tokenization; the main adaptation simply involves the processing of TiTok‚Äôs
de-tokenizer output. Specifically, this output, comprising a set of proxy codes, is subsequently fed
into the same off-the-shelf VQGAN decoder to generate the final RGB outputs. It is noteworthy that
the introduction of proxy codes differs from a simple distillation [26]. As verified in our experiments,
TiTok yields significantly better generation performance than MaskGIT-VQGAN."
TWO-STAGE TRAINING OF TITOK WITH PROXY CODES,0.2601010101010101,"After the first training stage with proxy codes, we optionally have the second ‚Äúdecoder fine-tuning‚Äù
stage, inspired by [10, 53], to improve the reconstruction quality. Specifically, we keep the encoder
and quantizer frozen, and only train the decoder towards pixel space with the typical VQGAN training
recipe [19]. We observe that such a two-stage training strategy significantly improves the training
stability and reconstructed image quality, as shown in the experiments."
EXPERIMENTAL RESULTS,0.26262626262626265,"4
Experimental Results"
EXPERIMENTAL RESULTS,0.26515151515151514,"4.1
Preliminary Experiments of 1D Tokenization"
EXPERIMENTAL RESULTS,0.2676767676767677,"Building upon TiTok, we explore a range of configurations, including the model size and the number
of tokens, to identify the most efficient and effective setup for a 1D image tokenizer. These preliminary
experiments serve to provide a thorough evaluation, seeking a practical configuration of TiTok."
EXPERIMENTAL RESULTS,0.2702020202020202,"Preliminary Experimental Setup. Unless specified otherwise, we train all models with images
of resolution H = 256 and W = 256, using the open-source MaskGIT-VQGAN [9] to supply
proxy codes for training. The patch size for both tokenizer and de-tokenizer is established with
f = 16, and the codebook C is configured to have N = 1024 entries with each entry a vector with
16 channels. For TiTok variants, we primarily investigate three model sizes‚Äîsmall, base, and large
(i.e., TiTok-S, TiTok-B, TiTok-L)‚Äîcomprising 22M, 86M, and 307M parameters for encoder and
decoder, respectively. We also assess the impact of varying the number of latent tokens K from 16 to
256. We perform ablation experiments with an efficient setting (e.g., shorter training)."
EXPERIMENTAL RESULTS,0.2727272727272727,"Evaluation Protocol. Evaluation is conducted across multiple metrics to thoroughly assess the
models, including both reconstruction and generation FID metrics (i.e., rFID and gFID) [25] on
the ImageNet dataset. We examine training/inference throughput to offer a direct comparison of
generative model‚Äôs efficiency relative to different latent sizes. Furthermore, given that the 1D VQ
model inherently serves as a form of compact image compression, we further investigate the semantic
information retained by the model through linear probing following MAE setting [24]. For the
complete details of the training and testing protocols (e.g., hyper-parameters, training costs), we refer
the reader to the supplementary material Sec. A."
EXPERIMENTAL RESULTS,0.27525252525252525,"After the setup, we now summarize the preliminary experimental findings below."
EXPERIMENTAL RESULTS,0.2777777777777778,"An Image Can be Represented by 32 Tokens. The redundancy inherent in image representation
is well-acknowledged, as evidenced by the practice of masking significant portions of images (e.g.,"
EXPERIMENTAL RESULTS,0.2803030303030303,"16 32
64
96
128
192
256
number of latent tokens 5 10 15 20 25"
EXPERIMENTAL RESULTS,0.2828282828282828,reconstruction FID (rFID)
EXPERIMENTAL RESULTS,0.28535353535353536,(a) IN-1K Reconstruction
EXPERIMENTAL RESULTS,0.2878787878787879,"16 32
64
96
128
192
256
number of latent tokens 45.0 47.5 50.0 52.5 55.0 57.5 60.0 62.5"
EXPERIMENTAL RESULTS,0.2904040404040404,linear probe accuracy (%)
EXPERIMENTAL RESULTS,0.29292929292929293,(b) IN-1K Linear Probe
EXPERIMENTAL RESULTS,0.29545454545454547,"16 32
64
96
128
192
256
number of latent tokens 4 6 8 10 12 14 16"
EXPERIMENTAL RESULTS,0.29797979797979796,generation FID (gFID)
EXPERIMENTAL RESULTS,0.3005050505050505,(c) IN-1K Generation
EXPERIMENTAL RESULTS,0.30303030303030304,"16 32
64
96
128
192
256
number of latent tokens 1000 2000 3000 4000 5000 6000"
EXPERIMENTAL RESULTS,0.3055555555555556,training throughput (samples/s/gpu)
EXPERIMENTAL RESULTS,0.30808080808080807,5743.3
EXPERIMENTAL RESULTS,0.3106060606060606,2815.2
EXPERIMENTAL RESULTS,0.31313131313131315,1268.3 824.8 555.1 327.0 219.7
EXPERIMENTAL RESULTS,0.31565656565656564,(d) Sampling Speed 20 40 60 80 100 120 140 160
EXPERIMENTAL RESULTS,0.3181818181818182,inference throughput (samples/s/gpu) 160.0 123.1 89.8 70.9 54.1 39.0 27.5
EXPERIMENTAL RESULTS,0.3207070707070707,"TiTok-S
TiTok-B
TiTok-L
Taming-VQGAN
MaskGIT-VQGAN"
EXPERIMENTAL RESULTS,0.32323232323232326,"Figure 4: Preliminary experimental results with different TiTok variants. We provide a com-
prehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c)
ImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and
TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B).
Detailed numbers can be found in supplementary material Sec. B."
EXPERIMENTAL RESULTS,0.32575757575757575,"75% in MAE [24]) to expedite the training process without negatively affecting performance. This
strategy has been validated across a variety of computer vision tasks that rely on high-level image
features [30, 42]. However, the efficacy of such approaches in the context of image reconstruction
and generation‚Äîwhere both low-level and high-level details are crucial for creating realistic recon-
structed and generated outputs‚Äîremains underexplored. Consequently, in this experiment, we aim to
determine the minimum number of tokens required to reconstruct and generate high-quality images.
As depicted in Fig. 4a, although model performance progressively improves with an increase in the
number of latent tokens, significant enhancements are predominantly observed when K ranges from
16 to 128. Beyond this point, increasing the latent space size yields only marginal gains. Intriguingly,
we find that with merely 32 latent tokens, TiTok-L achieves performance better than a 2D VQGAN
model [19] using 256 tokens. This observation suggests that as few as 32 tokens may suffice as an
effective image latent representation, optimizing the utilization of image redundancy."
EXPERIMENTAL RESULTS,0.3282828282828283,"Scaling Up Tokenizer Enables More Compact Latent Size. Another intriguing observation from
Fig. 4a is that larger tokenizers facilitate more compact representations. Specifically, TiTok-B with 64
latent tokens achieves performance comparable to TiTok-S with 128 latent tokens, while TiTok-L with
32 latent tokens matches the performance of TiTok-B with 64 latent tokens. This pattern indicates
that with each incremental increase in TiTok size (e.g., from S to B, or from B to L), it is possible to
reduce the size of the latent image representation without compromising performance. This trend
underscores the potential benefits of scaling up the tokenizer to achieve even more compact image
representations."
EXPERIMENTAL RESULTS,0.33080808080808083,"Semantics Emerges with Compact Latent Space. To evaluate the learned image representation, we
perform linear probing experiments on the image tokenizer, as shown in Fig. 4b. Specifically, we
add a batch normalization layer [31] followed by a linear layer on top of the frozen features from
TiTok encoder, with all hyper-parameters strictly following the MAE protocol [24]. We find that as
the size of the latent representation decreases, the tokenizer increasingly learns semantically rich
representations, as indicated by the improved linear probing accuracy. This suggests that the model
learns high-level information in scenarios of constrained representation space."
EXPERIMENTAL RESULTS,0.3333333333333333,"Compact Latent Representation Improves Generative Training. In addition to reconstruction
capabilities, we assess TiTok‚Äôs effectiveness and efficiency in generative downstream tasks, as
illustrated in Fig. 4c and Fig. 4d. We note that variants of different tokenizer sizes yield comparable
outcomes when the number of latent tokens is sufficiently large (i.e., K ‚â•128). However, within
the domain of compact latent sizes (i.e., K ‚â§64), larger tokenizers notably enhance performance.
Furthermore, the adaptability of 1D tokenization in TiTok facilitates more efficient and effective
generative model training. For instance, model variants with K = 32, despite inferior reconstruction
quality, demonstrate significantly better generative performance, underscoring the advantages of
employing a more condensed and semantically rich latent space for generative model training.
Additionally, the reduction in latent tokens markedly accelerates training and inference, with a 12.8√ó
increase in training speed (2815.2 vs. 219.7 samples/s/gpu) and a 4.5√ó speed up sampling speed
(123.1 vs. 27.5 samples/s/gpu), when utilizing K = 32 as opposed to K = 256."
EXPERIMENTAL RESULTS,0.33585858585858586,"Table 1: ImageNet-1K 256 √ó 256 generation results evaluated with ADM [16]. ‚Ä†: Trained on
OpenImages [36] ‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator‚Äôs
parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision."
EXPERIMENTAL RESULTS,0.3383838383838384,"tokenizer
#tokens codebook size rFID‚Üì
generator
gFID‚Üì
P‚Üì
S‚Üì
T‚Üë
diffusion-based generative models
Taming-VQGAN‚Ä† [58]
1024
16384
1.14
LDM-8 [58]
7.76 258M 200
-
VAE‚Ä† [58]
4096√ó3
-
0.27
LDM-4 [58]
3.60 400M 250
0.4"
EXPERIMENTAL RESULTS,0.3409090909090909,"VAE [60]‚Ä°
1024√ó4
-
0.62"
EXPERIMENTAL RESULTS,0.3434343434343434,"UViT-L/2 [4]
3.40 287M
50
1.1
UViT-H/2 [4]
2.29 501M
50
0.6
DiT-XL/2 [52]
2.27 675M 250
0.6
transformer-based generative models
Taming-VQGAN [19]
256
1024
7.94 Taming-Transformer [19] 15.78 1.4B
256
7.5"
EXPERIMENTAL RESULTS,0.34595959595959597,"RQ-VAE [37]
256
16384
3.20
RQ-Transformer [37]
8.71
1.4B
64
16.1
7.55
3.8B
9.7
MaskGIT-VQGAN [9]
256
1024
2.28
MaskGIT-ViT [9]
6.18 177M
8
50.5
ViT-VQGAN [69]
1024
8192
1.28
VIM-Large [69]
4.17
1.7B 1024
0.3
TiTok-L-32
32
4096
2.21
MaskGIT-ViT [9]
2.77 177M
8
101.6
TiTok-B-64
64
4096
1.70
MaskGIT-ViT [9]
2.48 177M
8
89.8"
EXPERIMENTAL RESULTS,0.3484848484848485,"TiTok-S-128
128
4096
1.71
MaskGIT-UViT-L [9, 4]
2.50 287M
8
53.3
1.97
64
7.8"
MAIN EXPERIMENTS,0.351010101010101,"4.2
Main Experiments"
MAIN EXPERIMENTS,0.35353535353535354,"Based on the observations above, the proposed TiTok family effectively trades off a larger model size
to a more compact latent size. In this section, we majorly focus on ImageNet generation benchmarks
against prior arts, and evaluate TiTok as a tokenizer in the generative MaskGIT framework [9]."
MAIN EXPERIMENTS,0.3560606060606061,"Implementation Details. We primarily investigate the following TiTok variants: TiTok-S-128 (i.e.,
small model with 128 tokens), TiTok-B-64 (i.e., base model with 64 tokens), and TiTok-L-32 (i.e.,
large model with 32 tokens), where each variant designed to halve the latent space size while scaling
up the model size. For resolution 512, we double the latent size to ensure more details are kept at
higher resolution, leading to TiTok-L-64 and TiTok-B-128. In the final setting for TiTok training,
the codebook is configured to N = 4096, and the training duration is extended to 1M iterations
(200 epochs). We also adopt the ‚Äúdecoder fine-tuning‚Äù stage to further enhance model performance,
where the encoder and quantizer are kept frozen and the decoder is fine-tuned for 500k iterations.
For the training of generative models, we utilize the MaskGIT [9] framework without any specific
modifications, except for the adoption of an arccos masking schedule [6]. All other parameters are
the same as previous setups, and all design improvements will be verified in the ablation studies."
MAIN EXPERIMENTS,0.35858585858585856,"Main Results. We summarize the results on ImageNet-1K generation benchmark of resolution
256 √ó 256 and 512 √ó 512 in Tab. 1 and Tab. 2, respectively.2"
MAIN EXPERIMENTS,0.3611111111111111,"For ImageNet 256 √ó 256 results in Tab. 1, TiTok can achieve a similar level of reconstruction FID
(rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using
merely 32 tokens, TiTok-L-32 achieves a rFID of 2.21, comparable to the well trained VQGAN
from MaskGIT [9] (rFID 2.28), while using 8√ó smaller latent representation size. Furthermore,
when using the same generator framework and same sampling steps, TiTok-L-32 improves over
MaskGIT by a large margin (from 6.18 to 2.77 gFID), showcasing the benefits of a more effective
generator training with compact 1D tokens. When compared to other diffusion-based generative
models, TiTok can also achieve a competitive performance while enjoying an over 100√ó speed-up
during the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4 [58]
(2.77 vs. 3.60), while generating images dramatically faster by 254 times (101.6 samples/s vs. 0.4
samples/s). Our best-performing variant TiTok-S-128 outperforms state-of-the-art diffusion method
DiT-XL/2 [52] (gFID 1.97 vs. 2.27), with a 13√ó speed-up."
MAIN EXPERIMENTS,0.36363636363636365,"For ImageNet 512 √ó 512 results in Tab. 2, the significantly better accuracy-cost trade-off of TiTok
persists. TiTok maintains a reasonably good rFID compared to other methods, especially considering
that TiTok uses much fewer tokens (i.e., higher compression ratio). For generation, all TiTok variants"
MAIN EXPERIMENTS,0.3661616161616162,"2For fairness, we mainly consider tokenizers with vanilla VQ modules. More advanced quantization
methods [72, 48] may further benefit TiTok but beyond this paper‚Äôs focus on 1D image tokenization. See
supplementary material Sec. C for the complete table."
MAIN EXPERIMENTS,0.3686868686868687,"Table 2: ImageNet-1K 512 √ó 512 generation results evaluated with ADM [16]. ‚Ä°: Trained on
OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator‚Äôs parameters. S: sampling
steps. T: throughput as samples per seconds on A100 with float32 precision."
MAIN EXPERIMENTS,0.3712121212121212,"tokenizer
#tokens codebook size rFID‚Üì
generator
gFID‚Üì
P‚Üì
S‚Üì
T‚Üë
diffusion-based generative models"
MAIN EXPERIMENTS,0.37373737373737376,"VAE [60]‚Ä°
4096√ó4
-
0.19"
MAIN EXPERIMENTS,0.37626262626262624,"UViT-L/4 [4]
4.67 287M 50
1.0
UViT-H/4 [4]
4.05 501M 50
0.6
DiT-XL/2 [52]
3.04 675M 250 0.1
transformer-based generative models
MaskGIT-VQGAN [9]
1024
1024
1.97 MaskGIT-ViT [9] 7.32 177M 12
3.9
TiTok-L-64
64
4096
1.77 MaskGIT-ViT [9] 2.74 177M
8
41.0"
MAIN EXPERIMENTS,0.3787878787878788,"TiTok-B-128
128
4096
1.52 MaskGIT-ViT [9] 2.49 177M
8
33.3
MaskGIT-ViT [9] 2.13 177M 64
7.4"
MAIN EXPERIMENTS,0.3813131313131313,"Table 3: Ablation study improved final models for main experiments. We ablate the tokenizer
designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.
Generation results are based on tokenizers without decoder fine-tuning"
MAIN EXPERIMENTS,0.3838383838383838,"(a) TiTok configuration. Results
reported in accumulation manner"
MAIN EXPERIMENTS,0.38636363636363635,"TiTok-L-32
rFID‚Üì
IS‚Üë
baseline
6.59
110.3
+ larger codebook
5.85
116.6
+ 200 epochs
5.48
117.3
+ decoder finetuning
2.21
195.5"
MAIN EXPERIMENTS,0.3888888888888889,"(b) Masking schedules for genera-
tor with TiTok-L-32"
MAIN EXPERIMENTS,0.39141414141414144,"schedule
gFID‚Üì
IS‚Üë
cosine
5.17
191.8
arccos
4.94
194.0
linear
4.95
193.7
square root
5.63
170.9"
MAIN EXPERIMENTS,0.3939393939393939,(c) Effects of proxy codes
MAIN EXPERIMENTS,0.39646464646464646,"rFID‚Üì
IS‚Üë
Taming-VQGAN training setting
Taming-VQGAN (2D)
7.94
-
TiTok-B-64 (2D)
15.58
64.2
TiTok-B-64
5.15
120.5
Two-stage training with proxy codes
TiTok-B-64
1.70
195.2"
MAIN EXPERIMENTS,0.398989898989899,"significantly outperform our baseline MaskGIT [9] by a large margin. When compared with diffusion-
based models, TiTok-L-64 shows a superior performance to DiT-XL/2 [52] (2.74 vs. 3.04), while
running 410√ó faster. The best-performing variant TiTok-B-128 can significantly outperform DiT-
XL/2 by a large margin (2.13 vs. 3.04) but also generates high-quality samples 74√ó faster. We also
provide visualization results and analysis in supplementary material Sec. D."
ABLATION STUDIES,0.4015151515151515,"4.3
Ablation Studies"
ABLATION STUDIES,0.40404040404040403,"We report the ablation studies regarding our final model designs in Tab. 3. Specifically, in Tab. 3a,
we ablate the tokenizer designs on image reconstruction. We begin with our baseline TiTok-L-32
which attains 6.59 rFID. Employing a larger codebook size improves the rFID by 0.74, while further
increasing the training iterations (from 100 epochs to 200 epochs) yields another 0.37 improvement
of rFID. On top of that, the ‚Äúdecoder fine-tuning‚Äù (our stage-2 training strategy) can substantially
improve the overall reconstruction performance to 2.21 rFID."
ABLATION STUDIES,0.4065656565656566,"In Tab. 3b, we examine the effects of different masking schedules for MaskGIT with TiTok. Inter-
estingly, unlike the original MaskGIT setting [9] which empirically found that the cosine masking
schedule significantly outperforms the other schedules, we observe that MaskGIT equipped with
TiTok changes the preference to the arccos or linear schedules. Additionally, unlike [9] which
reported that the root schedule performs much worse than the others, we observe that TiTok is quite
robust to different masking schedules. We attribute the observations to TiTok‚Äôs ability to provide a
more compact and more semantic meaningful tokens compared to 2D VQGAN, as compared to the
cosine masking schedule, linear and arccos schedules have a lower masking ratio in the early steps.
This coincides with the observation that masking ratio is usually higher for redundant signals (e.g.,
75% masking ratio in images [24]) while relatively lower for semantic meaningful inputs (e.g., 15%
masking ratio in languages [15])."
ABLATION STUDIES,0.4090909090909091,"We ablate the effects of training paradigm in Tab. 3c. We begin with the training setting of
Taming-VQGAN [19], where TiTok-B-64 obtains 5.15 rFID, outperforming the original 2D Taming-
VQGAN‚Äôs 7.94 rFID under the same training setting. We also show the necessity of 1D tokenization
by building a 2D variant of TiTok-B64, where the architecture remains the same except that image
patches instead of latent tokens are used as image representation. As a result, we observe that the 2D
variant suffers from a much worse performance (15.58 vs. 5.15 rFID), since the fixed correspondences
in 2D tokenization limited a reasonable reconstruction under compact latent space. This result"
ABLATION STUDIES,0.4116161616161616,"demonstrates the effectiveness of the proposed 1D tokenization, especially at a much more compact
latent size. Although TiTok can achieve a reasonably well performance under straightforward
single-stage training, there exists a performance gap compared to the MaskGIT-VQGAN [9]
due to the missing of a strong training recipe, of which no public reference or access exists.
Therefore, we adopt the two-stage training with proxy codes, which proves to be effective and can
outperform the MaskGIT-VQGAN (1.70 vs. 2.28 rFID). It is noteworthy that the two-stage training
is not that crucial to obtain a reasonable 1D tokenizer, and we believe that TiTok, with the simple
single-stage Taming-VQGAN‚Äôs training setting, could also benefit from training on a lagrer-scale
dataset [36] as demonstrated in [58] and we leave it for future work due to the limited compute."
ABLATION STUDIES,0.41414141414141414,"Details on the Two-Stage Training. We provide more technical details on the two-stage training.
Specifically:"
ABLATION STUDIES,0.4166666666666667,"‚Ä¢ In the first stage (warm-up stage), we use an off-the-shelf ImageNet-pretrained MaskGIT-VQ
tokenizer to tokenize the input image into 256 tokens, which we refer to as proxy codes.
‚Ä¢ In the first stage training, instead of regressing the original RGB values, we use the proxy
codes as reconstruction targets. Specifically, the workflow is: RGB images are patchified
and flattened into a sequence and concatenated with 32 latent tokens, then they are fed into
TiTok-Enc (Encoder of TiTok). Later, the latent tokens are kept as token representation
and go through the quantizer. The quantized latent tokens are concatenated with 256 mask
tokens and go through the TiTok-Dec (Decoder of TiTok). And the final output mask tokens
are supervised by proxy codes using cross-entropy loss.
‚Ä¢ Afterwards, we freeze both the TiTok-Enc and quantizer, and then only fine-tune the
TiTok-Dec (responsible for reconstructing proxy codes) and MaskGIT-Dec (responsible
for reconstructing RGB values from proxy codes) end-to-end towards pixel space, where
the training losses include L2 loss, perceptual loss, and GAN loss following the common
VQGAN paradigm."
ABLATION STUDIES,0.41919191919191917,"Moreover, we also note that two-stage training is not necessary for TiTok training, and it works fine
with the commonly used and publicly available Taming-VQGAN recipe as is shown in Tab. 3c . In
this case, the whole workflow is pretty straightforward, where the TiTok-Dec will instead directly
reconstruct the images at pixel space."
ABLATION STUDIES,0.4217171717171717,"However, the Taming-VQGAN recipe (developed more than 3 years ago) leads to an inferior FID
score when compared to state-of-the-art tokenizers, putting TiTok at disadvantage when compared
against other methods. Therefore we propose the two-stage training to benefit TiTok from the state-
of-art MaskGIT-VQGAN tokenizer, which shares a similar architecture to Taming-VQGAN but has a
significantly better score (rFID 2.28 v.s. 7.94)."
ABLATION STUDIES,0.42424242424242425,"We also note that TiTok can work well with single-stage recipe, and it is promising to incorporate the
recent modern VQGAN recipe from [47, 68] in our preliminary experiments."
CONCLUSION,0.42676767676767674,"5
Conclusion"
CONCLUSION,0.4292929292929293,"In this paper, we have explored a compact 1D tokenization TiTok for reconstructing and generating
natural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D
grid, we provide a more compact formulation to tokenize an image into a 1D latent sequence. The
proposed TiTok can represent an image with 8 to 64 times fewer tokens than the commonly used 2D
tokenizers. Moreover, the compact 1D tokens not only significantly improve the generation model‚Äôs
training and inference throughput, but also achieve a competitive FID on the ImageNet benchmarks.
We hope our research can shed some light in the direction towards more efficient image representation
and generation models with 1D image tokenization."
REFERENCES,0.4318181818181818,References
REFERENCES,0.43434343434343436,"[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
few-shot learning. NeurIPS, 2022."
REFERENCES,0.43686868686868685,"[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015."
REFERENCES,0.4393939393939394,"[3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik,
and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. In CVPR,
2024."
REFERENCES,0.44191919191919193,"[4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A
vit backbone for diffusion models. In CVPR, 2023."
REFERENCES,0.4444444444444444,"[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In
ICLR, 2022."
REFERENCES,0.44696969696969696,"[6] Victor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer. arXiv
preprint arXiv:2310.14400, 2023."
REFERENCES,0.4494949494949495,"[7] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan:
Towards high-resolution image generation with efficient vision transformers. In ICCV, 2023."
REFERENCES,0.45202020202020204,"[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."
REFERENCES,0.45454545454545453,"[9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image
transformer. In CVPR, 2022."
REFERENCES,0.45707070707070707,"[10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked
generative transformers. In ICML, 2023."
REFERENCES,0.4595959595959596,"[11] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable
vision models in the vision-language era. In CVPR, 2024."
REFERENCES,0.4621212121212121,"[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. In ICML, 2020."
REFERENCES,0.46464646464646464,"[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015."
REFERENCES,0.4671717171717172,"[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009."
REFERENCES,0.4696969696969697,"[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. In NAACL, 2018."
REFERENCES,0.4722222222222222,"[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021."
REFERENCES,0.47474747474747475,"[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,
2021."
REFERENCES,0.4772727272727273,"[18] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal
Shankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image
models. arXiv preprint arXiv:2401.08541, 2024."
REFERENCES,0.4797979797979798,"[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.
In CVPR, 2021."
REFERENCES,0.4823232323232323,"[20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:
Scene-based text-to-image generation with human priors. In ECCV, 2022."
REFERENCES,0.48484848484848486,"[21] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a
strong image synthesizer. In ICCV, 2023."
REFERENCES,0.48737373737373735,"[22] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see
and draw with seed tokenizer. In ICLR, 2024."
REFERENCES,0.4898989898989899,"[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014."
REFERENCES,0.49242424242424243,"[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders
are scalable vision learners. In CVPR, 2022."
REFERENCES,0.494949494949495,"[25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017."
REFERENCES,0.49747474747474746,"[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.5,"[27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.
science, 313(5786), 2006."
REFERENCES,0.5025252525252525,"[28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,
2022."
REFERENCES,0.5050505050505051,"[29] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high
resolution images. In ICML, 2023."
REFERENCES,0.5075757575757576,"[30] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image
pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020."
REFERENCES,0.51010101010101,"[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In ICML, 2015."
REFERENCES,0.5126262626262627,"[32] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song,
Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. In
ICLR, 2024."
REFERENCES,0.5151515151515151,"[33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In ECCV, 2016."
REFERENCES,0.5176767676767676,"[34] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So
Kweon, and Liang-Chieh Chen. Tubeformer-deeplab: Video mask transformer. In CVPR, 2022."
REFERENCES,0.5202020202020202,"[35] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014."
REFERENCES,0.5227272727272727,"[36] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified
image classification, object detection, and visual relationship detection at scale. IJCV, 2020."
REFERENCES,0.5252525252525253,"[37] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generation using residual quantization. In CVPR, 2022."
REFERENCES,0.5277777777777778,"[38] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective
image generation with contextual rq-transformer. NeurIPS, 2022."
REFERENCES,0.5303030303030303,"[39] Jos√© Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with token-
critic. In ECCV, 2022."
REFERENCES,0.5328282828282829,"[40] Jos√© Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Predictor-corrector
sampling for discrete diffusion models. In ICLR, 2023."
REFERENCES,0.5353535353535354,"[41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. In ICML, 2023."
REFERENCES,0.5378787878787878,"[42] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image
pre-training via masking. In CVPR, 2023."
REFERENCES,0.5404040404040404,"[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023."
REFERENCES,0.5429292929292929,"[44] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion
in image generation via multi-resolution diffusion models. NeurIPS, 2024."
REFERENCES,0.5454545454545454,"[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021."
REFERENCES,0.547979797979798,"[46] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.5505050505050505,"[47] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-
source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410,
2024."
REFERENCES,0.553030303030303,"[48] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization:
VQ-VAE made simple. In ICLR, 2024."
REFERENCES,0.5555555555555556,"[49] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir
Zamir. 4m: Massively multimodal masked modeling. NeurIPS, 2023."
REFERENCES,0.5580808080808081,"[50] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.
NeurIPS, 2017."
REFERENCES,0.5606060606060606,"[51] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023."
REFERENCES,0.5631313131313131,"[52] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023."
REFERENCES,0.5656565656565656,"[53] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna,
and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv
preprint arXiv:2307.01952, 2023."
REFERENCES,0.5681818181818182,"[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021."
REFERENCES,0.5707070707070707,"[55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021."
REFERENCES,0.5732323232323232,"[56] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.
NeurIPS, 2019."
REFERENCES,0.5757575757575758,"[57] Jason Tyler Rolfe. Discrete variational autoencoders. In ICLR, 2017."
REFERENCES,0.5782828282828283,"[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
image synthesis with latent diffusion models. In CVPR, 2022."
REFERENCES,0.5808080808080808,"[59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
dataset for training next generation image-text models. NeurIPS, 2022."
REFERENCES,0.5833333333333334,"[60] stabilityai, 2023. URL https://huggingface.co/stabilityai/sd-vae-ft-ema."
REFERENCES,0.5858585858585859,"[61] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR, 2024."
REFERENCES,0.5883838383838383,"[62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
REFERENCES,0.5909090909090909,"[63] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
image generation with pixelcnn decoders. NeurIPS, 2016."
REFERENCES,0.5934343434343434,"[64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017."
REFERENCES,0.5959595959595959,"[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017."
REFERENCES,0.5984848484848485,"[66] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing
robust features with denoising autoencoders. In Proceedings of the 25th international conference on
Machine learning, pages 1096‚Äì1103, 2008."
REFERENCES,0.601010101010101,"[67] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end
panoptic segmentation with mask transformers. In CVPR, 2021."
REFERENCES,0.6035353535353535,"[68] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen.
Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024."
REFERENCES,0.6060606060606061,"[69] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,
Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR,
2022."
REFERENCES,0.6085858585858586,"[70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich
text-to-image generation. TMLR, 2022."
REFERENCES,0.6111111111111112,"[71] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann,
Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR,
2023."
REFERENCES,0.6136363636363636,"[72] Lijun Yu, Jos√© Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,
Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion‚Äìtokenizer is key
to visual generation. In ICLR, 2024."
REFERENCES,0.6161616161616161,"[73] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu,
Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining
and instruction tuning. arXiv preprint arXiv:2309.02591, 2023."
REFERENCES,0.6186868686868687,"[74] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan
Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In
CVPR, 2022."
REFERENCES,0.6212121212121212,"[75] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and
Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022."
REFERENCES,0.6237373737373737,"[76] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-
vocabulary segmentation with single frozen convolutional clip. NeurIPS, 2023."
REFERENCES,0.6262626262626263,"[77] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards open-ended visual recognition with large
language model. In ECCV, 2024."
REFERENCES,0.6287878787878788,"[78] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung
Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint
arXiv:2203.03605, 2022."
REFERENCES,0.6313131313131313,"[79] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018."
REFERENCES,0.6338383838383839,"[80] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors
for high-fidelity image generation. NeurIPS, 2022."
REFERENCES,0.6363636363636364,"[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable
transformers for end-to-end object detection. In ICLR, 2020."
REFERENCES,0.6388888888888888,Appendix
REFERENCES,0.6414141414141414,"In the supplementary materials, we provide the following additional details:"
REFERENCES,0.6439393939393939,"‚Ä¢ The comprehensive training and testing hyper-parameters and training costs for TiTok
(Sec. A)."
REFERENCES,0.6464646464646465,‚Ä¢ The detailed results of the preliminary experiments reported in main paper‚Äôs Fig. 4(Sec. B).
REFERENCES,0.648989898989899,‚Ä¢ A more comprehensive comparison with more metrics and baselines (Sec. C).
REFERENCES,0.6515151515151515,‚Ä¢ Qualitative visualizations (Sec. D).
REFERENCES,0.6540404040404041,‚Ä¢ Limitation discussion (Sec. E).
REFERENCES,0.6565656565656566,‚Ä¢ Broader Impacts discussion (Sec. F).
REFERENCES,0.6590909090909091,‚Ä¢ Dataset Licenses (Sec. G).
REFERENCES,0.6616161616161617,"A
Training and Testing Protocols"
REFERENCES,0.6641414141414141,"For image reconstruction (tokenizer) at preliminary experiments, the training augmentation is
confined to random cropping and flipping, following [19]. The training regimen spans a short
schedule, featuring a batch size of 256 over 500k training iterations, which correlates to roughly 100
epochs on the ImageNet dataset. We employ the AdamW optimizer [46] with an initial learning rate
of 1 √ó 10‚àí4 (with cosine decay) and weight decay 1 √ó 10‚àí4. We only adopt stage-1 training here
(i.e., only the ‚Äúwarm-up‚Äù training stage). For the main experiments, we adopt the improvements
as shown in the ablation study (Tab. 3 in main paper), including longer training to 200 epochs and
decoder fine-tuning, all other hyper-parameters remain the same. We use patch size 16 for all vision
transformers at resolution 256 √ó 256 and increase it to 32 for resolution 512 √ó 512 to ensure a
computation efficiency."
REFERENCES,0.6666666666666666,"TiTok-L refers to using a ViT-L for TiTok encoder and decoder, and TiTok-B, TiTok-S refers to using
ViT-B and ViT-S respectively. Moreover, the tokenizer training takes 64 A100-40G for 74 hours
(TiTok-L-32), 32 A100-40G for 41 hours (TiTok-B-64), 32 A100-40G for 50 hours (TiTok-S-128), 32
A100-40G for 70 hours (TiTok-B-128 for resolution 512), and 64 A100-40G for 91 hours (TiTok-L-64
for resolution 512), respectively."
REFERENCES,0.6691919191919192,"For image generation (generator) at preliminary experiments, we majorly build the training and
testing protocols on top of [9]. Specifically, all images are pre-tokenized using center crop and
random flipping augmentation, and then processed by MaskGIT [9] to generate images via the
masked image modeling procedure. During inference, a cosine masking schedule is utilized with 8
steps. The generative models are trained with a batch size of 2048 and 500k iterations to improve
training efficiency. We use AdamW optimizer [46] with learning rate 2 √ó 10‚àí4 and weight decay
0.03. The learning rate starts from 2 √ó 10‚àí4 and then decay to 1 √ó 10‚àí5 following a cosine decaying
schedule. We apply a dropout probability of 0.1 on the class condition. The only differences of
main experiments are using an arccos masking schedule as discussed in the ablation study (main
paper Tab. 3), all other hyper-parameters remain the same. We follow prior arts [19, 9] to generate
50k samples for generation FID evaluation. We also adopt classifier-free guidance [28] following
prior arts [10, 72]."
REFERENCES,0.6717171717171717,"At ImageNet 256 √ó 256, we use guidance scale 4.5, temperature 9.5 for TiTok-L-32, guidance scale
3.0, temperature 11.0 for TiTok-B-64, guidance scale 2.0, temperature 3.0 for TiTok-S-128. At
ImageNet 512 √ó 512, we use guidance scale 2.0, temperature 7.5 for TiTok-L-64, guidance scale 2.5,
temperature 6.5 for TiTok-B-128."
REFERENCES,0.6742424242424242,"The generator training takes 32 A100-40G for 12 hours (TiTok-L-32), 16 hours (TiTok-B-64), 29
hours (TiTok-S-128), 26 hours (TiTok-B-128 for resolution 512), 18 hours (TiTok-L-64 for resolution
512) respectively."
REFERENCES,0.6767676767676768,"B
Detailed Results of Preliminary Experiments"
REFERENCES,0.6792929292929293,We summarize the detailed results for Fig. 4 of main paper in Tab. 4.
REFERENCES,0.6818181818181818,Table 4: Detailed results of preliminary experiments in main paper.
REFERENCES,0.6843434343434344,(a) reconstruction FID.
REFERENCES,0.6868686868686869,"#token
16
32
64
96 128 192 256
TiTok-S
25.3 16.2 9.6 6.9 5.6
4.3
3.7
TiTok-B 16.8
9.7
5.9 4.7 3.8
3.2
2.9
TiTok-L 13.0
6.6
4.0 3.4 3.0
2.6
2.5"
REFERENCES,0.6893939393939394,(b) generation FID.
REFERENCES,0.6919191919191919,"#token
16
32
64
96 128 192 256
TiTok-S
16.1 8.7 4.4 3.6 3.5
3.0
3.2
TiTok-B 10.9 7.5 4.1 3.4 3.2
3.1
3.5
TiTok-L
9.0
6.3 4.3 3.9 3.7
3.7
3.7"
REFERENCES,0.6944444444444444,(c) Linear Probing accuracy.
REFERENCES,0.696969696969697,"#token
16
32
64
96
128
192
256
TiTok-S
50.46 48.01 48.40 48.12 46.55 47.05 44.88
TiTok-B 57.70 54.79 53.92 53.72 53.59 51.75 52.11
TiTok-L 62.10 60.03 58.85 56.12 54.35 53.95 54.36"
REFERENCES,0.6994949494949495,"Table 5: ImageNet-1K 256 √ó 256 generation results evaluated with ADM [16]. ‚Ä†: Trained on
OpenImages [36] ‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator‚Äôs
parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision,
measured with w/ guidance variants if available. ‚Äúguidance"" refers to classifier-free guidance."
REFERENCES,0.702020202020202,"tokenizer
rFID‚Üì
generator
w/o guidance w/ guidance
P‚Üì
S‚Üì
T‚Üë
gFID‚Üì
IS‚Üë
gFID‚ÜìIS‚Üë
diffusion-based generative models
Taming-VQGAN‚Ä† [58] 1.14
LDM-8 [58]
15.82 78.82
7.76 209.5 258M 200
-
VAE‚Ä† [58]
0.27
LDM-4 [58]
10.56 103.5
3.60 247.7 400M 250
0.4"
REFERENCES,0.7045454545454546,"VAE [60]‚Ä°
0.62"
REFERENCES,0.7070707070707071,"UViT-L/2 [4]
9.03
111.5
3.40 219.9 287M
50
1.1
UViT-H/2 [4]
6.60
142.5
2.29 263.9 501M
50
0.6
DiT-XL/2 [52]
9.62
121.5
2.27 278.2 675M 250
0.6
transformer-based generative models
Taming-VQGAN [19]
7.94 Taming-Transformer [19] 15.78
78.3
-
-
1.4B
256
7.5"
REFERENCES,0.7095959595959596,"RQ-VAE [37]
3.20
RQ-Transformer [37]
8.71
119.0
-
-
1.4B
64
16.1
7.55
134.0
-
-
3.8B
9.7
MaskGIT-VQGAN [9]
2.28
MaskGIT-ViT [9]
6.18
182.1
-
-
177M
8
50.5
ViT-VQGAN [69]
1.28
VIM-Large [69]
4.17
175.1
-
-
1.7B 1024
0.3
LFQ [72]
‚àº0.9
MAGVIT-v2 [72]
3.65
200.5
1.78 319.4 307M
64
1.1
TiTok-L-32
2.21
MaskGIT-ViT [9]
3.15
173.0
2.77 199.8 177M
8
101.6
TiTok-B-64
1.70
MaskGIT-ViT [9]
3.08
192.5
2.48 214.7 177M
8
89.8"
REFERENCES,0.7121212121212122,"TiTok-S-128
1.71
MaskGIT-UViT-L [9, 4]
4.61
166.7
2.50 278.7 287M
8
53.3
4.44
168.2
1.97 281.8
64
7.8"
REFERENCES,0.7146464646464646,"C
Additional Results"
REFERENCES,0.7171717171717171,"We further report the class-conditional generation results comparison with more metrics and baselines
in Tab. 5 and Tab. 6 for ImageNet 256 √ó 256 and 512 √ó 512 generation benchmarks, respectively.
Moreover, we report both results without and with classifier-free guidance [28] under column ‚Äúw/o
guidance"" and ‚Äúw/ guidance"" respectively."
REFERENCES,0.7196969696969697,"As shown in Tab. 5, both TiTok-L-32 and TiTok-B-64 set a new state-of-the-art performance for
results without classifier-free guidance (i.e., w/o guidance column), while generating images at a
much faster pace. Specifically, TiTok-L-32 achieves 3.15 gFID, surpassing current state-of-the-art
MAGVIT-v2 [72]‚Äôs gFID 3.65, while requiring much fewer sampling steps (8 vs. 64) and smaller
model size (177M vs. 307M), leading to a substantial sampling speed-up (92.4√ó faster, 101.6 vs.
1.1 samples/sec). Additionally, when compared to MaskGIT [9], which uses the exact same generator
model (i.e., MaskGIT-ViT) as ours and the only difference is the toeknizer, TiTok-L-32 achieves
significantly a better performance (3.15 vs. 6.18). The improvement demonstrates the efficiency and
effectiveness of the learned compact 1D latent space for image representation. When it comes to
resolution 512 √ó 512 in Tab. 6, MaskGIT [9] requires 1024 tokens for image latent representation,
while TiTok-L-64 requires 16√ó fewer. As a result, when using the same generator (i.e., MaskGIT-
ViT), TiTok-L-64, w/o classifier-free guidance, not only significantly outperforms MaskGIT in terms
of gFID (3.64 vs. 7.32) but also generates samples much faster. The advantages of TiTok become"
REFERENCES,0.7222222222222222,"Table 6: ImageNet-1K 512 √ó 512 generation results evaluated with ADM [16]. ‚Ä†: Trained on
OpenImages [36] ‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator‚Äôs
parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision,
measured with w/ guidance variants if available. ‚Äúguidance"" refers to classifier-free guidance."
REFERENCES,0.7247474747474747,"tokenizer
rFID‚Üì
generator
w/o guidance w/ guidance
P‚Üì
S‚Üì
T‚Üë
gFID‚Üì
IS‚Üë
gFID‚ÜìIS‚Üë
diffusion-based generative models"
REFERENCES,0.7272727272727273,"VAE [60]‚Ä°
0.19"
REFERENCES,0.7297979797979798,"UViT-L/4 [4]
18.03
76.9
4.67 213.3 287M 50
1.0
UViT-H/4 [4]
15.71 101.3
4.05 263.8 501M 50
0.6
DiT-XL/2 [52]
12.03 105.3
3.04 240.8 675M 250 0.1
transformer-based generative models
MaskGIT-VQGAN [9] 1.97 MaskGIT-ViT [9] 7.32
156.0
-
-
177M 12
3.9"
REFERENCES,0.7323232323232324,"LFQ [72]
1.22 MAGVIT-v2 [72] 4.61
192.4
-
-
307M 12
3.5
3.07
213.1
1.91 324.3 307M 64
1.0
TiTok-L-64
1.78 MaskGIT-ViT [9] 3.64
179.8
2.74 221.1 177M
8
41.0"
REFERENCES,0.7348484848484849,"TiTok-B-128
1.37 MaskGIT-ViT [9] 3.91
182.0
2.49 260.4 177M
8
33.3
4.17
181.0
2.13 261.2
64
7.4"
REFERENCES,0.7373737373737373,"even more significant when compared to the diffusion models such as DiT-XL/2 [52] with guidance:
TiTok-L-64 not only shows a superior performance (2.74 vs. 3.04), but also enjoys a dramatically
higher generation throughput (410√ó)."
REFERENCES,0.73989898989899,"An interesting observation is that under w/o guidance case, TiTok-L-32 (for 256 resolution) and
TiTok-L-64 (for 512 resolution) can outperform most other methods, including TiTok-S-128 and
TiTok-B-128, yet they benefit relatively less from the classifier-free guidance in the w/ guidance
column. We note it indicates that the great potential of TiTok at compact latent size is still not fully
unleashed yet, and better adaptation of inference time improvements for 1D compact tokens, such as
classifier-free guidance, which was designed for methods with much more tokens and steps, could be
a promising future direction."
REFERENCES,0.7424242424242424,"D
Visualizations"
REFERENCES,0.7449494949494949,"We provide visualization of the generated images using TiTok in Fig. 5 and Fig. 6. Moreover, we
visualize the reconstruction results under different numbers of tokens and different model sizes
in Fig. 7, where we observe that the model tends to keep the high-level layout or salient objects when
the latent representation size is limited. Besides, a larger model size reconstructs an image with more
details under a compact latent space size, demonstrating an effective way towards a more compact
latent space."
REFERENCES,0.7474747474747475,We also provide more uncurated visualization samples in Fig. 8 and Fig. 9.
REFERENCES,0.75,"E
Limitations"
REFERENCES,0.7525252525252525,"This paper proposes a novel 1D tokenization method designed to eliminate the fixed corresponding
constraints of existing 2D tokenization methods. The 1D tokenization model is validated using
the Vector Quantization (VQ) tokenizer formulation alongside a Masked Transformer generator
framework. Despite the promising results, the proposed 1D tokenization formulation theoretically
has the potential to generalize to other tokenizer formulations (e.g., 1D-VAE), other generation
frameworks (e.g., Diffusion Models), and beyond the image modality (e.g., video). However,
exploring these extensions is beyond the scope of this paper due to limited computational resources,
and we leave these as promising directions for future research."
REFERENCES,0.7550505050505051,"F
Broader Impacts"
REFERENCES,0.7575757575757576,"Generative models have numerous applications with diverse potential social impacts. While these
models significantly enhance human creativity, they can also be misused for misinformation, harass-
ment, and perpetuating social and cultural biases. Similar to other deep learning methods, generative"
REFERENCES,0.76010101010101,"macaw
lion
jack-o'-"
REFERENCES,0.7626262626262627,lantern
REFERENCES,0.7651515151515151,"orange
daisy
bubble
valley"
REFERENCES,0.7676767676767676,"TiTok-L-32
TiTok-B-64
TiTok-S-128"
REFERENCES,0.7702020202020202,"Figure 5: Visualization of generated images from TiTok variants with MaskGIT [9]. Correspond-
ing ImageNet class names are shown below the images."
REFERENCES,0.7727272727272727,"Figure 6: Visualization of generated images from TiTok-L-32 with MaskGIT [9] across random
ImageNet classes."
REFERENCES,0.7752525252525253,"models can be heavily influenced by dataset biases, leading to the reinforcement of negative social
stereotypes and viewpoints. Developing unbiased models that ensure both robustness and fairness
is a critical area of research. However, addressing these issues is beyond the scope of this paper."
REFERENCES,0.7777777777777778,"Increasing model size (S, B, L)"
REFERENCES,0.7803030303030303,"Increasing number of latent token (K=16, 32, 64, 128, 256)"
REFERENCES,0.7828282828282829,"Figure 7: Visual comparison of reconstruction results. Scaling model size enables a better image
quality while using a more compact latent space size. It is also observed that TiTok tends to keep the
salient regions when latent space is limited."
REFERENCES,0.7853535353535354,"Considering the potential risks, this paper is limited to class-conditional generation using a fixed,
public, and controlled set of classes."
REFERENCES,0.7878787878787878,"G
Dataset Licenses"
REFERENCES,0.7904040404040404,The datasets we used for training and/or testing TiTok are described as follows.
REFERENCES,0.7929292929292929,"ImageNet-1K:
We train and evaluate TiTok on ImageNet-1K generation benchmark. This dataset
spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and"
REFERENCES,0.7954545454545454,"Figure 8: Uncurated 256 √ó 256 TiTok-L-32 samples. Class labels (class ids) from left to right and
top to down are: ‚Äúmacaw"" (88), ‚Äúvolcano"" (980), ‚Äúcoral reef"" (973), ‚Äúwhite wolf"" (270)."
REFERENCES,0.797979797979798,"Figure 9: Uncurated 256 √ó 256 TiTok-L-32 samples. Class labels (class ids) from left to right and
top to down are: ‚Äúcheeseburger"" (933), ‚Äúcliff"" (972), ‚Äúfountain"" (562), ‚ÄúSiberian husky"" (250)."
REFERENCES,0.8005050505050505,"100,000 test images. We use the training set for our tokenizer and generator training. The validation
set is used to compute reconstruction FID for evaluating tokenizers. The generation results are
evaluated with generation FID using pre-computed statistics and scripts from ADM [16] 3."
REFERENCES,0.803030303030303,License: https://image-net.org/accessagreement
REFERENCES,0.8055555555555556,URL: https://www.image-net.org/
REFERENCES,0.8080808080808081,3https://github.com/openai/guided-diffusion/tree/main/evaluations
REFERENCES,0.8106060606060606,NeurIPS Paper Checklist
CLAIMS,0.8131313131313131,1. Claims
CLAIMS,0.8156565656565656,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?"
CLAIMS,0.8181818181818182,"Answer: [Yes]
Justification: We propose a novel 1D tokenization method, achieving significant speed-up
while still capable of generating high-quality images, as demonstrated in the abstract and
introduction, supported by the Sec. 3 and Sec. 4.2 parts."
CLAIMS,0.8207070707070707,Guidelines:
CLAIMS,0.8232323232323232,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8257575757575758,2. Limitations
LIMITATIONS,0.8282828282828283,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8308080808080808,Answer: [Yes]
LIMITATIONS,0.8333333333333334,Justification: Please see Supplementary Material Sec. E.
LIMITATIONS,0.8358585858585859,Guidelines:
LIMITATIONS,0.8383838383838383,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8409090909090909,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8434343434343434,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8459595959595959,"Answer: [NA]
Justification: No theoretical result.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8484848484848485,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.851010101010101,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Comprehensive details regarding model training and testing have been provided
in Main Paper Sec. 4.1 Preliminary Experimental Setup, Sec. 4.2 Implementation Details,
and Supplementary Material Sec. A Training and Testing Protocols
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8535353535353535,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.8560606060606061,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.8585858585858586,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8611111111111112,"Justification: The code and model are available at https://github.com/bytedance/
1d-tokenizer."
THEORY ASSUMPTIONS AND PROOFS,0.8636363636363636,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8661616161616161,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
THEORY ASSUMPTIONS AND PROOFS,0.8686868686868687,6. Experimental Setting/Details
THEORY ASSUMPTIONS AND PROOFS,0.8712121212121212,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
THEORY ASSUMPTIONS AND PROOFS,0.8737373737373737,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8762626262626263,"Justification: Comprehensive details regarding model training and testing have been provided
in Main Paper Sec. 4.1 Preliminary Experimental Setup, Sec. 4.2 Implementation Details,
and Supplementary Material Sec. A Training and Testing Protocols"
THEORY ASSUMPTIONS AND PROOFS,0.8787878787878788,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8813131313131313,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8838383838383839,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8863636363636364,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8888888888888888,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8914141414141414,"Justification: We did not report error bars following prior arts [19, 9, 72]."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8939393939393939,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8964646464646465,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.898989898989899,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9015151515151515,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The training costs are reported in Supplementary Material Sec. A Training
and Testing Protocols.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9040404040404041,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9065656565656566,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research in the paper conform with the NeurIPS Code of Ethics.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9090909090909091,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9116161616161617,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The broader impacts have been discussed in Supplementary Material Sec. F
Broader Impacts.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9141414141414141,‚Ä¢ The answer NA means that there is no societal impact of the work performed.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9166666666666666,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9191919191919192,11. Safeguards
SAFEGUARDS,0.9217171717171717,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9242424242424242,Answer: [NA]
SAFEGUARDS,0.9267676767676768,"Justification: The paper poses no such risks. Although our study includes image generation,
we focus on class-conditional image generation with a predefined set of classes, which does
not pose the same risks as those text-to-image models."
SAFEGUARDS,0.9292929292929293,Guidelines:
SAFEGUARDS,0.9318181818181818,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9343434343434344,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9368686868686869,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9393939393939394,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9419191919191919,Justification: Please refer to Supplementary Material Sec. G Dataset Licenses.
LICENSES FOR EXISTING ASSETS,0.9444444444444444,Guidelines:
LICENSES FOR EXISTING ASSETS,0.946969696969697,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9494949494949495,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.952020202020202,13. New Assets
NEW ASSETS,0.9545454545454546,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9570707070707071,Answer: [NA]
NEW ASSETS,0.9595959595959596,Justification: This paper does not release new assets.
NEW ASSETS,0.9621212121212122,Guidelines:
NEW ASSETS,0.9646464646464646,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671717171717171,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696969696969697,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747474747474747,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797979797979798,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823232323232324,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873737373737373,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98989898989899,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924242424242424,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949494949494949,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974747474747475,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
