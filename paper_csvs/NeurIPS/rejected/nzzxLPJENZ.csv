Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010787486515641855,"Large language models (LLMs) have made significant advances across various
1"
ABSTRACT,0.002157497303128371,"generative tasks, progressing toward achieving near-human levels of intelligence.
2"
ABSTRACT,0.003236245954692557,"However, in many scenarios, LLMs face the challenge of insufficient human
3"
ABSTRACT,0.004314994606256742,"evaluation or even the inability to evaluate reliably. Particularly, in complex
4"
ABSTRACT,0.005393743257820928,"dialogue scenarios involving diverse and intricate user intents, LLMs as evaluators
5"
ABSTRACT,0.006472491909385114,"of AI responses exhibit a substantial gap compared to humans. Moreover, due
6"
ABSTRACT,0.007551240560949299,"to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their
7"
ABSTRACT,0.008629989212513484,"evaluation capabilities. In this work, we conceptualize the evaluation process as
8"
ABSTRACT,0.009708737864077669,"a decision tree, where each node represents an evaluation action, and each path
9"
ABSTRACT,0.010787486515641856,"from the root to a leaf node represents a trajectory of evaluation reasoning. We
10"
ABSTRACT,0.011866235167206042,"demonstrate that within a limited search space, there exist better decision-making
11"
ABSTRACT,0.012944983818770227,"behaviors that facilitate the model in making reasonable and accurate judgments.
12"
ABSTRACT,0.014023732470334413,"Specifically, we propose a tree-based data sampling method to generate supervised
13"
ABSTRACT,0.015102481121898598,"data and preference pairs derived from the evaluation tree. Furthermore, we
14"
ABSTRACT,0.016181229773462782,"introduce preference learning based on the DPO algorithm, which empowers the
15"
ABSTRACT,0.017259978425026967,"fine-grained evaluation model to explore and learn better branching strategies within
16"
ABSTRACT,0.018338727076591153,"budget-limited scenarios. Our model significantly reduces the dependency on
17"
ABSTRACT,0.019417475728155338,"labeled data and demonstrates strong performance across three different evaluation
18"
ABSTRACT,0.020496224379719527,"settings: in-distribution, out-of-distribution, and transfer evaluation. Experiments
19"
ABSTRACT,0.021574973031283712,"indicate that our model can reduce inference costs by 90% compared to conducting
20"
ABSTRACT,0.022653721682847898,"searches across the entire evaluation tree, thereby significantly enhancing efficiency.
21"
INTRODUCTION,0.023732470334412083,"1
Introduction
22"
INTRODUCTION,0.02481121898597627,"Dialogue evaluation capability [6] is one of the fundamental abilities of human social interaction,
23"
INTRODUCTION,0.025889967637540454,"involving the comprehension and interpretation of user intentions, as well as providing reasonable
24"
INTRODUCTION,0.02696871628910464,"judgments on the correctness of different responses. Automated evaluation can assist humans to
25"
INTRODUCTION,0.028047464940668825,"supervise powerful LLMs and is an essential component for superalignment and weak-to-strong
26"
INTRODUCTION,0.02912621359223301,"generalization techniques [4]. However, human evaluations [3, 22] are labor-intensive and time-
27"
INTRODUCTION,0.030204962243797196,"consuming, making it difficult to widely adopt. Traditional automated evaluation approaches [18, 39,
28"
INTRODUCTION,0.03128371089536138,"8] are limited by inherent deficiencies, such as string and semantic matching methods often yield
29"
INTRODUCTION,0.032362459546925564,"subpar accuracy and lack of interpretability. The advent of large language models offers promise for
30"
INTRODUCTION,0.03344120819848975,"automatically evaluating dialogue quality [19, 41, 15], owing to their high consistency with humans
31"
INTRODUCTION,0.034519956850053934,"in intent understanding.
32"
INTRODUCTION,0.03559870550161812,"Nevertheless, automated evaluation remains a challenging issue due to the diversity of tasks and
33"
INTRODUCTION,0.036677454153182305,"scenarios it may encounter. The user queries often encompass multiple intentions [38], which cannot
34"
INTRODUCTION,0.037756202804746494,"typically be addressed using a single evaluation criterion. However, related research [35, 42] often
35"
INTRODUCTION,0.038834951456310676,"attempts to treat evaluation as a simplistic ’one-step’ reasoning problem, causing even the most
36"
INTRODUCTION,0.039913700107874865,"Figure 1: The agreement between human judgment and LLMs in Eval-P benchmark in out-of-
distribution evaluation. Auto-J serves as the “one-step” evaluation baseline, while Fennec is the
“multi-step” baseline. The Initial, SFT, and DPO models were trained using our generated data."
INTRODUCTION,0.040992448759439054,"powerful large language models to struggle to provide reasonable and accurate results. It is essential
37"
INTRODUCTION,0.042071197411003236,"for the evaluation model to adapt to different scenarios and provide critical evaluation criteria.
38"
INTRODUCTION,0.043149946062567425,"In this work, we do not introduce any human prior for evaluation scenarios and criteria,
39"
INTRODUCTION,0.04422869471413161,"which are commonly used for designing and collecting training data in related studies [14, 11].
40"
INTRODUCTION,0.045307443365695796,"The real-world conversational scenarios are often characterized by complexity and unpredictability,
41"
INTRODUCTION,0.04638619201725998,"making it challenging to derive generalizable rules. Additionally, human priors frequently introduce
42"
INTRODUCTION,0.04746494066882417,"biases [12, 20], making these evaluation methods poorly generalized due to a lack of adaptability
43"
INTRODUCTION,0.04854368932038835,"and scalability. Therefore, we explore automatically sampling scenarios from large-scale datasets
44"
INTRODUCTION,0.04962243797195254,"and employ LLMs to automatically generate evaluation criteria, aiming to eliminate human labor
45"
INTRODUCTION,0.05070118662351672,"as much as possible. Another significant challenge is the lack of ground truth labels and human
46"
INTRODUCTION,0.05177993527508091,"feedback during the training data collection process. The insufficient of available supervised data for
47"
INTRODUCTION,0.05285868392664509,"evaluation tasks also prevents them to scale effectively.
48"
INTRODUCTION,0.05393743257820928,"Despite various challenges, we discover that the evaluation model is constrained in its ability
49"
INTRODUCTION,0.05501618122977346,"to identify crucial evaluation criteria, but this limitation can be mitigated by increasing the
50"
INTRODUCTION,0.05609492988133765,"number of considered criteria. As shown in Figure 1, the Initial model can achieve nearly a
51"
INTRODUCTION,0.05717367853290183,"10 point improvement in the agreement metric by increasing the number of evaluation branches.
52"
INTRODUCTION,0.05825242718446602,"This findings motivate us to design tree-based data sampling methods to generate training data and a
53"
INTRODUCTION,0.0593311758360302,"branching preference learning algorithm to improve “multi-step” inference capability. Specifically,
54"
INTRODUCTION,0.06040992448759439,"we employ a breadth-first growth approach to construct an evaluation tree, where each path from the
55"
INTRODUCTION,0.061488673139158574,"root to a leaf node represents a complete evaluation trajectory. We collect high-quality evaluation
56"
INTRODUCTION,0.06256742179072276,"trajectories from the search space of the evaluation tree and trained an SFT model, which exhibited
57"
INTRODUCTION,0.06364617044228695,"superior performance and prediction consistency. Furthermore, we refine these evaluation trajectories
58"
INTRODUCTION,0.06472491909385113,"and train a DPO model [24], which can effectively prioritize and output crucial evaluation criteria,
59"
INTRODUCTION,0.06580366774541532,"thereby enhancing the model’s inference effectiveness.
60"
INTRODUCTION,0.0668824163969795,"We mainly evaluate our models in three settings: in-distribution, out-of-distribution, and transfer
61"
INTRODUCTION,0.06796116504854369,"evaluation. Specifically, we use the datasets from the Chatbot Arena 1 as in-distribution data, and
62"
INTRODUCTION,0.06903991370010787,"collect data from large-scale dialogue datasets without human priors as out-of-distribution data. In our
63"
INTRODUCTION,0.07011866235167206,"experiments, we demonstrate that (1) our model outperforms several recent open-source evaluation
64"
INTRODUCTION,0.07119741100323625,"models and methods across all three settings, (2) there is a noticeable improvement in the evaluation
65"
INTRODUCTION,0.07227615965480043,"model’s capability when progressively training the Initial model, the SFT model, and the DPO
66"
INTRODUCTION,0.07335490830636461,"model, and (3) as shown in Figure 1, our DPO model achieves the best performance even when using
67"
INTRODUCTION,0.0744336569579288,"only a single evaluation criterion (single inference branch).
68"
RELATED WORKS,0.07551240560949299,"2
Related Works
69"
RELATED WORKS,0.07659115426105717,"Automated dialogue evaluation [6] has long been a significant challenge in the field of generative AI.
70"
RELATED WORKS,0.07766990291262135,"Recent work [10, 7, 35, 41] has demonstrated that LLMs can act as automated evaluators, serving
71"
RELATED WORKS,0.07874865156418555,"as alternatives to human judges. However, LLMs still exhibit issues such as positional bias and
72"
RELATED WORKS,0.07982740021574973,"prediction inconsistency [34, 40]. Many studies have relied heavily on human priors [14, 11], thereby
73"
RELATED WORKS,0.08090614886731391,"neglecting to explore the model generalization capabilities. In contrast, our research focuses on
74"
RELATED WORKS,0.08198489751887811,"examining the performance with different data distributions and investigates how to bridge this gap.
75"
RELATED WORKS,0.08306364617044229,1https://chat.lmsys.org/
RELATED WORKS,0.08414239482200647,"We consider automated evaluation as a complex reasoning task and aim to improve model performance
76"
RELATED WORKS,0.08522114347357065,"by optimizing reasoning trajectories. When handling such tasks, LLMs typically utilize decision
77"
RELATED WORKS,0.08629989212513485,"trees [37, 23] to model the reasoning process. They often employ search algorithms like A* [21, 13] or
78"
RELATED WORKS,0.08737864077669903,"Monte Carlo Tree Search (MCTS) [29, 31] to identify the optimal reasoning path within the candidate
79"
RELATED WORKS,0.08845738942826321,"decision. However, these methods generally rely on deterministic reward signals or feedback, which
80"
RELATED WORKS,0.0895361380798274,"are absent in our settings. We demonstrate that the ensemble boundary of the evaluation branches
81"
RELATED WORKS,0.09061488673139159,"provides a feasible reward signal to verify the accuracy of the reasoning trajectories. Based on this,
82"
RELATED WORKS,0.09169363538295577,"we can guide the model to generate a substantial amount of high-quality data.
83"
RELATED WORKS,0.09277238403451996,"Automated evaluation is also a pivotal technology within scalable oversight, aiming to enhance
84"
RELATED WORKS,0.09385113268608414,"humans’ ability to supervise models. For example, humans may ask models to critique the outputs of
85"
RELATED WORKS,0.09492988133764833,"other models [9, 28] or use models to help decompose a problem into simpler subproblems [17]. In
86"
RELATED WORKS,0.09600862998921252,"contrast to improving human supervision, we focus on how to conduct reliable automated evaluations.
87"
RELATED WORKS,0.0970873786407767,"Certainly, our proposed evaluation methods and results can also be combined with human oversight
88"
RELATED WORKS,0.09816612729234088,"to provide even better performance.
89"
PROBLEM SETUP,0.09924487594390508,"3
Problem Setup
90"
PROBLEM SETUP,0.10032362459546926,"In this work, our primary focus is on evaluating AI responses, particularly in analyzing query
91"
PROBLEM SETUP,0.10140237324703344,"and response pairs within given datasets to determine which response is better 2. Traditional ap-
92"
PROBLEM SETUP,0.10248112189859762,"proaches [41, 35] regard the evaluation task as a “one-step” classification (“win” or “lose” or “tie”) or
93"
PROBLEM SETUP,0.10355987055016182,"generation problem, where the final scores or explanations are assigned by a reward model or the
94"
PROBLEM SETUP,0.104638619201726,"evaluation model. However, with complex reasoning tasks or scenarios, a given query may involve
95"
PROBLEM SETUP,0.10571736785329018,"multiple intents, whether explicit or implicit [38], yet the generated responses by AI often overlook
96"
PROBLEM SETUP,0.10679611650485436,"some of these intents, constrained by the model’s capabilities. Therefore, multiple evaluation criteria
97"
PROBLEM SETUP,0.10787486515641856,"are required [19] to verify whether the responses address the query requirements and align with user
98"
PROBLEM SETUP,0.10895361380798274,"intentions. Considering the complexity and diversity of dialogue tasks, it remains an intractable
99"
PROBLEM SETUP,0.11003236245954692,"challenge to gather comprehensive and accurate evaluation criteria.
100"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.1111111111111111,"3.1
Conducting evaluation through multi-step reasoning
101"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.1121898597626753,"We try to view the evaluation task as a complex reasoning task, a multi-step generative problem, which
102"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11326860841423948,"entails: (1) initially seeking suitable evaluation criteria, then (2) generating scoring guidelines based
103"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11434735706580366,"on these criteria, and finally (3) conducting comprehensive judgment based on the aforementioned
104"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11542610571736785,"criteria and scoring guidelines. Formally, given a dialogue X, we will use an evaluation model to
105"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11650485436893204,"sequentially obtain the criterion C, scoring guideline S, and judgment J :
106"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11758360302049622,"C ∼πθ(C|PromptC, X), S ∼πθ(S|PromptS, C, X), J ∼πθ(J |PromptJ , S, C, X),
(1)"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.1186623516720604,"where πθ represents the evaluation policy, the prompt please refer to Appendix A.3. Similar to related
107"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.11974110032362459,"multi-branch evaluation [27, 16] methodologies, we refer to different reasoning paths as “evaluation
108"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.12081984897518878,"branch”, where each branch represents a decision-making process. Unlike previous methods [19]
109"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.12189859762675297,"that relied on enumerating criteria, our goal is for the evaluation model to automatically generate
110"
CONDUCTING EVALUATION THROUGH MULTI-STEP REASONING,0.12297734627831715,"crucial and high-priority criteria.
111"
FOCUSING ON TWO CHALLENGES,0.12405609492988134,"3.2
Focusing on two challenges
112"
FOCUSING ON TWO CHALLENGES,0.12513484358144553,"A natural approach is to first construct a candidate set of criteria and then derive suitable results based
113"
FOCUSING ON TWO CHALLENGES,0.1262135922330097,"on these criteria. To address this task, we focus on the following two challenges:
114"
FOCUSING ON TWO CHALLENGES,0.1272923408845739,"• How to construct an appropriate candidate set? Our aim is to develop a candidate set that
115"
FOCUSING ON TWO CHALLENGES,0.12837108953613807,"includes multiple evaluation branches enriched with high-quality evaluation opinions. By
116"
FOCUSING ON TWO CHALLENGES,0.12944983818770225,"training and optimizing this candidate space to advance desired behaviors, we can swiftly
117"
FOCUSING ON TWO CHALLENGES,0.13052858683926646,"identify appropriate and critical judgments during the inference process.
118"
FOCUSING ON TWO CHALLENGES,0.13160733549083065,"• How to rank the judgments? We also need to establish a ranking among different evaluation
119"
FOCUSING ON TWO CHALLENGES,0.13268608414239483,"branches to optimize the candidate space. In contrast to recent studies [13], our evaluation
120"
FOCUSING ON TWO CHALLENGES,0.133764832793959,"dataset lacks ground truth labels or environmental feedback to act as reward signals. The cost
121"
FOCUSING ON TWO CHALLENGES,0.1348435814455232,"of obtaining these signals is prohibitive, requiring not only expensive human labor but also
122"
FOCUSING ON TWO CHALLENGES,0.13592233009708737,"2Here, ""better"" is defined as aligning with human preferences and values."
FOCUSING ON TWO CHALLENGES,0.13700107874865156,"Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train
the Initial model. Subsequently, we construct an evaluation tree through a series of growth and
pruning operations. This tree then guides the training both of the SFT model and the DPO model."
FOCUSING ON TWO CHALLENGES,0.13807982740021574,"facing issues of low consistency among humans in many ambiguous problems. Therefore,
123"
FOCUSING ON TWO CHALLENGES,0.13915857605177995,"we need to design an innovative and cost-effective approach to address this challenge.
124"
METHOD,0.14023732470334413,"4
Method
125"
METHOD,0.1413160733549083,"Figure 2 illustrates an overview of our method, which involves three stages for model training: First,
126"
METHOD,0.1423948220064725,"we train the Initial model to construct the evaluation tree; Then, we sample different evaluation
127"
METHOD,0.14347357065803668,"branches as supervised data to train the SFT model, enhancing branch prediction consistency; Finally,
128"
METHOD,0.14455231930960086,"we collect preference data to train the DPO model, ensuring rapid sampling of critical branches.
129"
COLLECTING DIALOGUE DATASET,0.14563106796116504,"4.1
Collecting dialogue dataset
130"
COLLECTING DIALOGUE DATASET,0.14670981661272922,"Evaluation models typically rely on robust generalization capabilities to effectively handle diverse
131"
COLLECTING DIALOGUE DATASET,0.14778856526429343,"dialogue tasks. Consequently, the distribution of training data significantly affects performance on
132"
COLLECTING DIALOGUE DATASET,0.1488673139158576,"unseen tasks encountered during real-world evaluations. To address this, we sampled from a large-
133"
COLLECTING DIALOGUE DATASET,0.1499460625674218,"scale dialogue dataset rather than a specific data source. We then apply the K-Means algorithm [2] to
134"
COLLECTING DIALOGUE DATASET,0.15102481121898598,"cluster the data. Subsequently, we sample data from these clusters, ensuring that the training dataset
135"
COLLECTING DIALOGUE DATASET,0.15210355987055016,"encompasses a diverse set of dialogue scenarios. More details refer to Appendix A.1
136"
TRAINING INITIAL MODEL,0.15318230852211434,"4.2
Training initial model
137"
TRAINING INITIAL MODEL,0.15426105717367852,"We aim to construct a dataset from scratch for evaluation, consisting of dialogues paired with their
138"
TRAINING INITIAL MODEL,0.1553398058252427,"corresponding evaluation trees. Each tree contains different reasoning paths during the evaluation of
139"
TRAINING INITIAL MODEL,0.15641855447680691,"dialogues. The root node of this tree represents the dialogue data, and each path from the root node to
140"
TRAINING INITIAL MODEL,0.1574973031283711,"a leaf node signifies an evaluation branch. Each evaluation branch comprises three decision-making
141"
TRAINING INITIAL MODEL,0.15857605177993528,"behavior nodes: criterion C, scoring guideline S, and judgment J . To simulate this decision process,
142"
TRAINING INITIAL MODEL,0.15965480043149946,"we introduce a multi-branch training approach [16] to train an LLM as the initial policy πInitial. We
143"
TRAINING INITIAL MODEL,0.16073354908306364,"employ GPT-4 (gpt-4-0125-preview) [1] to generate corresponding multi-branch training data
144"
TRAINING INITIAL MODEL,0.16181229773462782,"to enhance quality. This approach ensures that the model can auto-regressively generate evaluation
145"
TRAINING INITIAL MODEL,0.162891046386192,"branches using Equation 1.
146"
GENERATING EVALUATION TREE,0.16396979503775622,"4.3
Generating evaluation tree
147"
GENERATING EVALUATION TREE,0.1650485436893204,"We expand the branch candidates sampled from the policy πInitial using the breadth-first growth,
148"
GENERATING EVALUATION TREE,0.16612729234088458,"thereby including as many high-quality evaluation paths as possible. Due to the different paradigms
149"
GENERATING EVALUATION TREE,0.16720604099244876,"of SFT and DPO, we employ consistency pruning to split the sampling space to obtain training data:
150"
GENERATING EVALUATION TREE,0.16828478964401294,"• Breadth-first Growth: The evaluation tree contains two distinct growth manner: for
151"
GENERATING EVALUATION TREE,0.16936353829557713,"criterion C node, we use LLM’s brainstorming capability to generate k relevant criteria; for
152"
GENERATING EVALUATION TREE,0.1704422869471413,"scoring guideline S and judgment J node, we use sampling method by adjusting the LLM’s
153"
GENERATING EVALUATION TREE,0.1715210355987055,"temperature and top-p parameters. To simplify, we utilize the Initial model πInitial to
154"
GENERATING EVALUATION TREE,0.1725997842502697,"generate a complete binary tree for each subtree with a criteria node as its root. Furthermore,
155"
GENERATING EVALUATION TREE,0.17367853290183388,"since the evaluation task requires testing the model’s consistency by swapping response
156"
GENERATING EVALUATION TREE,0.17475728155339806,"positions, we can obtain k × 8 different evaluation branches.
157"
GENERATING EVALUATION TREE,0.17583603020496225,"Figure 3: The figure illustrates how the training dataset of the SFT and DPO models is sampled from
an evaluation subtree based on a specific criterion."
GENERATING EVALUATION TREE,0.17691477885652643,"• Consistency Pruning: Prior to pruning, we introduce two different consistency constraints:
158"
GENERATING EVALUATION TREE,0.1779935275080906,"self-consistency, meaning the same criterion C and scoring guideline S should yield the
159"
GENERATING EVALUATION TREE,0.1790722761596548,"same judgment J , and positional consistency, meaning that swapping positions should not
160"
GENERATING EVALUATION TREE,0.18015102481121897,"affect the judgment J . Subsequently, we obtain SFT training data from evaluation branches
161"
GENERATING EVALUATION TREE,0.18122977346278318,"in the evaluation tree that meet both consistency constraints, and DPO training data from
162"
GENERATING EVALUATION TREE,0.18230852211434737,"nodes that do not satisfy these constraints.
163"
COLLECTING PREFERENCE LABELS,0.18338727076591155,"4.4
Collecting preference labels
164"
COLLECTING PREFERENCE LABELS,0.18446601941747573,"Although we can obtain SFT and DPO data from the consistency sampling space, this data lacks
165"
COLLECTING PREFERENCE LABELS,0.1855447680690399,"correctness verification. Typically, preference data requires human annotation to establish ranking
166"
COLLECTING PREFERENCE LABELS,0.1866235167206041,"sequences, a time-consuming process that is not suitable for scaling. Therefore, we propose two
167"
COLLECTING PREFERENCE LABELS,0.18770226537216828,"alternative approaches to label each evaluation branch with its correctness:
168"
COLLECTING PREFERENCE LABELS,0.18878101402373246,"• Branch Ensemble: Considering that there are only three final labels for judgment (“win” or
169"
COLLECTING PREFERENCE LABELS,0.18985976267529667,"“lose” or “tie”), we use an ensemble result of evaluation branches to obtain the consensus
170"
COLLECTING PREFERENCE LABELS,0.19093851132686085,"label. The ensemble method provides a lower bound of judge error without incurring
171"
COLLECTING PREFERENCE LABELS,0.19201725997842503,"additional costs. For SFT data, we filter out data that is inconsistent with the ensemble
172"
COLLECTING PREFERENCE LABELS,0.1930960086299892,"results. For DPO pair data, we select samples consistent with the ensemble results as “chosen”
173"
COLLECTING PREFERENCE LABELS,0.1941747572815534,"samples, and those inconsistent as “rejected” samples.
174"
COLLECTING PREFERENCE LABELS,0.19525350593311758,"• LLM-as-a-Judge: Some highly aligned LLMs, such as GPT-4, possess powerful annotation
175"
COLLECTING PREFERENCE LABELS,0.19633225458468176,"capabilities. Therefore, we use LLMs to determine which sample in the DPO pairs data is
176"
COLLECTING PREFERENCE LABELS,0.19741100323624594,"more reasonable as the “chosen” sample. In our experiments, we found that this method has
177"
COLLECTING PREFERENCE LABELS,0.19848975188781015,"only a 20% disagreement rate compared to the Branch Ensemble method. We analyze this
178"
COLLECTING PREFERENCE LABELS,0.19956850053937433,"method in Section 5.4
179"
COLLECTING PREFERENCE LABELS,0.20064724919093851,"As shown in Figure 3, we combine consistency pruning and automated labeling to collect the cor-
180"
COLLECTING PREFERENCE LABELS,0.2017259978425027,"responding preference data. Through the labeling of judgments, we can also obtain preference
181"
COLLECTING PREFERENCE LABELS,0.20280474649406688,"information for criterion C and scoring guideline S based on the final judgment J decisions. Specifi-
182"
COLLECTING PREFERENCE LABELS,0.20388349514563106,"cally, we prioritize predicting criteria that lead to correct judgments and select the scoring guidelines
183"
COLLECTING PREFERENCE LABELS,0.20496224379719524,"with the highest overall scores as the “chosen” samples. Additionally, we randomly sample from the
184"
COLLECTING PREFERENCE LABELS,0.20604099244875945,"filtered data to create the training set, thereby controlling training costs and efficiency.
185"
TRAINING SFT MODEL AND DPO MODEL,0.20711974110032363,"4.5
Training SFT model and DPO model
186"
TRAINING SFT MODEL AND DPO MODEL,0.20819848975188782,"We use the Initial model as the starting point to train the SFT model πSFT using supervised learning,
187"
TRAINING SFT MODEL AND DPO MODEL,0.209277238403452,"which reduces inconsistent predictions compared to the initial policy. Then, we take the SFT model as
188"
TRAINING SFT MODEL AND DPO MODEL,0.21035598705501618,"the initialization to train the DPO model πDPO using Direct Preference Optimization, which can learn
189"
TRAINING SFT MODEL AND DPO MODEL,0.21143473570658036,"the decision priorities of different branches, with the objective:
190"
TRAINING SFT MODEL AND DPO MODEL,0.21251348435814454,"LDPO(πDPO|πSFT) = −E(x,yc,yr)"
TRAINING SFT MODEL AND DPO MODEL,0.21359223300970873,"
log σ

β log πDPO(yc|x)"
TRAINING SFT MODEL AND DPO MODEL,0.21467098166127294,πSFT(yc|x) −β log πDPO(yr|x)
TRAINING SFT MODEL AND DPO MODEL,0.21574973031283712,πSFT(yr|x)
TRAINING SFT MODEL AND DPO MODEL,0.2168284789644013,"
,
(2)"
TRAINING SFT MODEL AND DPO MODEL,0.21790722761596548,"where the (x, y) represents data pair of different decision tasks in Equation 1, yc represents the
191"
TRAINING SFT MODEL AND DPO MODEL,0.21898597626752966,"“chosen” sample, and yr represents the “rejected” sample.
192"
TRAINING SFT MODEL AND DPO MODEL,0.22006472491909385,"During the inference process, we create a single branch for each criterion to conduct evaluation, and
193"
TRAINING SFT MODEL AND DPO MODEL,0.22114347357065803,"control the number of generated branches k to adjust the inference efficiency. Since the DPO model
194"
TRAINING SFT MODEL AND DPO MODEL,0.2222222222222222,"employs sampling optimization, it usually achieves optimal performance with only a few branches.
195"
TRAINING SFT MODEL AND DPO MODEL,0.22330097087378642,"Methods
Size
Branch
Eval-P (w/ Tie)
Eval-P (w/o Tie)
MT-Bench (w/ Tie)
MT-Bench (w/o Tie)
AGR ↑
CNS ↑
AGR ↑
CNS ↑
AGR ↑
CNS ↑
AGR ↑
CNS ↑"
TRAINING SFT MODEL AND DPO MODEL,0.2243797195253506,In-Distribution Evaluation
TRAINING SFT MODEL AND DPO MODEL,0.22545846817691478,"Auto-J †
13B
1
55.13
82.44
74.13
87.26
44.20
70.74
55.98
72.30
Fennec †
7B
1
55.36
83.80
68.63
86.33
52.88
82.18
63.42
85.63
5
55.80
85.52
74.14
89.19
53.88
84.41
68.04
87.38 Ours"
TRAINING SFT MODEL AND DPO MODEL,0.22653721682847897,"SFT
7B
1
56.68
86.64
70.76
89.11
53.29
88.43
66.64
90.25
5
55.96
86.57
72.91
88.13
53.08
87.99
67.96
90.17"
TRAINING SFT MODEL AND DPO MODEL,0.22761596548004315,"DPO
7B
1
55.24
84.26
69.87
86.95
53.29
83.04
62.96
85.23
5
57.18
85.63
74.88
88.52
53.43
83.97
66.48
86.84"
TRAINING SFT MODEL AND DPO MODEL,0.22869471413160733,Out-of-Distribution Evaluation
TRAINING SFT MODEL AND DPO MODEL,0.2297734627831715,"GPT-4 [14]
-
-
62.28
86.28
-
-
-
-
-
-
GPT-4 †
-
-
55.93
78.43
74.56
83.79
57.78
83.51
73.11
86.19
GPT-3.5 †
-
-
44.41
72.39
59.86
73.57
49.55
74.13
62.50
77.22 Ours"
TRAINING SFT MODEL AND DPO MODEL,0.2308522114347357,"Initial
7B
1
49.64
83.69
57.02
84.59
50.76
82.94
56.35
83.64
10
53.16
85.13
66.93
86.06
54.25
88.10
66.65
89.62"
TRAINING SFT MODEL AND DPO MODEL,0.2319309600862999,"SFT
7B
1
54.59
87.14
70.56
88.52
55.23
88.97
67.38
90.76
10
55.10
87.86
73.69
89.99
54.69
89.84
69.48
92.12"
TRAINING SFT MODEL AND DPO MODEL,0.23300970873786409,"DPO
7B
1
55.89
89.44
75.76
90.67
55.74
91.45
71.69
93.36
3
56.75
90.37
77.23
92.24
55.89
92.49
72.08
94.45"
TRAINING SFT MODEL AND DPO MODEL,0.23408845738942827,Transfer Evaluation
TRAINING SFT MODEL AND DPO MODEL,0.23516720604099245,"SFT
7B
1
54.17
87.36
70.95
89.01
53.77
88.67
66.91
90.56
10
55.96
89.00
75.56
90.87
53.68
88.94
68.97
91.34"
TRAINING SFT MODEL AND DPO MODEL,0.23624595469255663,"DPO
7B
1
56.11
89.30
76.54
91.65
54.81
91.08
71.48
93.09
5
56.39
90.01
77.04
92.54
55.10
91.78
71.73
93.52"
TRAINING SFT MODEL AND DPO MODEL,0.2373247033441208,"Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the
best performance results by varying branches. Bold numbers indicate the best performance among
open-source models, while underlined numbers represent the best performance across all models."
EXPERIMENTS,0.238403451995685,"5
Experiments
196"
EXPERIMENTS,0.23948220064724918,"As the most popular LLM evaluation platform recently, Chatbot Arena demonstrates high alignment
197"
EXPERIMENTS,0.2405609492988134,"with human judgments in pairwise response evaluations. We collect its open-source human judgment
198"
EXPERIMENTS,0.24163969795037757,"benchmark, Eval-P and MT-bench, to serve as the test set. We gather training data comprising both
199"
EXPERIMENTS,0.24271844660194175,"dialogue data and evaluation data for the following three evaluation scenarios:
200"
EXPERIMENTS,0.24379719525350593,"1. In-distribution evaluation: We apply the Fennec [16] training data to train the In-
201"
EXPERIMENTS,0.24487594390507011,"distribution (ID) model, which included 3K dialogue data from Auto-J [14], along with
202"
EXPERIMENTS,0.2459546925566343,"evaluation data annotated by GPT-4. This training data is a multi-branch dataset, meaning
203"
EXPERIMENTS,0.24703344120819848,"that a single dialogue includes multiple evaluation branches.
204"
EXPERIMENTS,0.2481121898597627,"2. Out-of-distribution evaluation: We collect 5M large-scale dialogue data and extracted
205"
EXPERIMENTS,0.24919093851132687,"7K samples from it to serve as out-of-distribution (OOD) training data. GPT-4 annotate 3K
206"
EXPERIMENTS,0.25026968716289105,"evaluation samples from this dataset for the Initial model training.
207"
EXPERIMENTS,0.2513484358144552,"3. Transfer evaluation: We use 3K OOD training data (which includes evaluation data) and
208"
EXPERIMENTS,0.2524271844660194,"2K ID dialogue data (which did not include evaluation data) to train the transfer model.
209"
EXPERIMENTS,0.2535059331175836,"For each benchmark, we employ Agreement (AGR) and Consistency (CNS) as performance metrics.
210"
EXPERIMENTS,0.2545846817691478,"Consistency measures the prediction consistency of the evaluation model when the positions of the
211"
EXPERIMENTS,0.255663430420712,"responses are swapped. Agreement quantifies the proportion of evaluations that meet the criteria
212"
EXPERIMENTS,0.25674217907227614,"for swap consistency and align with human judgments. In many cases, the “tie” label indicates
213"
EXPERIMENTS,0.25782092772384035,"an inability to distinguish performance under some evaluation criteria. However, it may still be
214"
EXPERIMENTS,0.2588996763754045,"distinguishable under specific evaluation criteria. Therefore, we also present the model performance
215"
EXPERIMENTS,0.2599784250269687,"on the test data without “tie” label. For more details, please refer to Appendix A.2
216"
IN-DISTRIBUTION EVALUATION,0.26105717367853293,"5.1
In-distribution evaluation
217"
IN-DISTRIBUTION EVALUATION,0.2621359223300971,"The results are shown in Table 1, where methods marked with † denote our reimplementations. Since
218"
IN-DISTRIBUTION EVALUATION,0.2632146709816613,"the Initial model leverages the Fennec training data for initialization, its performance can be
219"
IN-DISTRIBUTION EVALUATION,0.26429341963322545,"(a) The scenarios of ID dataset
(b) The scenarios of OOD dataset
(c) The scenarios of Eval-P"
IN-DISTRIBUTION EVALUATION,0.26537216828478966,"Figure 4: The scenario contains seven categories, including Summarization, Exam Questions, Rewrit-
ing, Code, Functional Writing, Creative Writing, General Communication, NLP Tasks, and Others."
IN-DISTRIBUTION EVALUATION,0.2664509169363538,"regarded as its in-distribution evaluation baseline. As observed, the SFT and DPO models exhibit
220"
IN-DISTRIBUTION EVALUATION,0.267529665587918,"significant performance improvements over most baseline methods on both the Auto-J and Fennec
221"
IN-DISTRIBUTION EVALUATION,0.2686084142394822,"datasets, achieving the highest agreement score of 57.18. In the multi-turn dialogue evaluation on
222"
IN-DISTRIBUTION EVALUATION,0.2696871628910464,"MT-bench, the Fennec dataset comprises only single-turn dialogues, which constrains its effectiveness
223"
IN-DISTRIBUTION EVALUATION,0.2707659115426106,"in handling multi-turn context information. Additionally, we observed the instabilities problems
224"
IN-DISTRIBUTION EVALUATION,0.27184466019417475,"during the training process, which hindered the DPO model from outperforming the Initial model.
225"
IN-DISTRIBUTION EVALUATION,0.27292340884573896,"A more comprehensive analysis of these instability problems is provided in Section 5.7.
226"
OUT-OF-DISTRIBUTION EVALUATION,0.2740021574973031,"5.2
Out-of-distribution evaluation
227"
OUT-OF-DISTRIBUTION EVALUATION,0.2750809061488673,"In terms of OOD evaluation, the Initial model performs worse than the baseline model on both
228"
OUT-OF-DISTRIBUTION EVALUATION,0.2761596548004315,"Eval-P and MT-bench benchmarks, due to the distribution shift in the dialogue dataset. With
229"
OUT-OF-DISTRIBUTION EVALUATION,0.2772384034519957,"RLHF [22] training, the SFT model significantly surpasses the Initial model in consistency rate
230"
OUT-OF-DISTRIBUTION EVALUATION,0.2783171521035599,"and also enhances the agreement rate. Notably, the DPO model achieves superior performance with
231"
OUT-OF-DISTRIBUTION EVALUATION,0.27939590075512405,"only three branches, thereby reducing inference latency by over 60%. In evaluation settings without
232"
OUT-OF-DISTRIBUTION EVALUATION,0.28047464940668826,"“tie” labels, the advantage of the DPO model becomes more apparent, significantly outperforming
233"
OUT-OF-DISTRIBUTION EVALUATION,0.2815533980582524,"other models, including proprietary model GPT-4. This demonstrates that the DPO model can
234"
OUT-OF-DISTRIBUTION EVALUATION,0.2826321467098166,"effectively distinguish between responses using critical criteria, even when employing only 3 branch
235"
OUT-OF-DISTRIBUTION EVALUATION,0.2837108953613808,"for inference. Furthermore, our models are capable of handling multi-turn dialogue scenarios,
236"
OUT-OF-DISTRIBUTION EVALUATION,0.284789644012945,"achieving performance that surpasses the in-distribution models. These extremely strong results
237"
OUT-OF-DISTRIBUTION EVALUATION,0.2858683926645092,"indicate that our model excels at identifying more crucial criteria to help distinguish the difference of
238"
OUT-OF-DISTRIBUTION EVALUATION,0.28694714131607335,"AI’s responses.
239"
TRANSFER EVALUATION,0.28802588996763756,"5.3
Transfer evaluation
240"
TRANSFER EVALUATION,0.2891046386192017,"The purpose of transfer evaluation is to evaluate the model’s capability to adapt to in-distribution data,
241"
TRANSFER EVALUATION,0.2901833872707659,"thus mitigating the problem of training data distribution shift. It can be observed that both the SFT
242"
TRANSFER EVALUATION,0.2912621359223301,"and DPO models demonstrate improvements across multiple benchmarks compared to the Initial
243"
TRANSFER EVALUATION,0.2923408845738943,"model. Notably, in both OOD and transfer evaluation settings, the DPO model consistently achieves
244"
TRANSFER EVALUATION,0.29341963322545844,"better performance than the SFT model, while also reducing the number of inference branches.
245"
TRANSFER EVALUATION,0.29449838187702265,"Although the transfer model does not surpass the OOD model, it still achieves closed performance.
246"
TRANSFER EVALUATION,0.29557713052858686,"In Section 5.4, we provide a detailed analysis of the different scenarios that lead to these models
247"
TRANSFER EVALUATION,0.296655879180151,"exhibiting significantly different performance characteristics despite their close overall performance.
248"
SCENARIO ANALYSIS,0.2977346278317152,"5.4
Scenario analysis
249"
SCENARIO ANALYSIS,0.2988133764832794,"To investigate the impact of scenario categories distribution in the training data, we need to analyze
250"
SCENARIO ANALYSIS,0.2998921251348436,"the scenarios within the OOD, ID training sets, and the Eval-P test set. For this purpose, we
251"
SCENARIO ANALYSIS,0.30097087378640774,"employ the scenario classifier trained by Auto-J, which effectively categorizes dialogue data into 58
252"
SCENARIO ANALYSIS,0.30204962243797195,"different scenarios. Figure 4 presents the distribution of scenarios. It can be observed that Auto-J’s
253"
SCENARIO ANALYSIS,0.30312837108953616,"training set is well-balanced across the predefined scenarios, closely matching the distribution of the
254"
SCENARIO ANALYSIS,0.3042071197411003,"Eval-P test set. In contrast, within the OOD data, the ""Others"" category exceeds 30%, and ""General
255"
SCENARIO ANALYSIS,0.30528586839266453,"Communication"" surpasses 50%. The significant differences in scenario distributions between the
256"
SCENARIO ANALYSIS,0.3063646170442287,"OOD data and the test set can lead to performance variations in test cases.
257"
SCENARIO ANALYSIS,0.3074433656957929,"Model
Branch
Sum.
Exam
Code
Rew.
Cre W.
Fun W.
Comm.
NLP.
Others
Overall"
SCENARIO ANALYSIS,0.30852211434735705,"Auto-J
-
45.8
38.9
47.5
49.2
59.7
61.7
55.2
57.6
-
54.9
Auto-J†
-
55.5
37.5
45.8
50.0
61.0
61.5
54.9
54.2
58.3
55.1"
SCENARIO ANALYSIS,0.30960086299892126,In-distribution Evaluation
SCENARIO ANALYSIS,0.3106796116504854,"Initial
5
48.6
41.7
55.0
46.7
62.5
60.9
53.1
52.9
54.2
55.8
SFT
5
55.6
44.4
58.3
48.3
61.2
62.0
53.8
54.2
54.2
56.0
DPO
5
59.7
45.8
58.3
46.7
62.1
59.9
54.9
59.6
58.3
57.2"
SCENARIO ANALYSIS,0.3117583603020496,Out-of-distribution Evaluation
SCENARIO ANALYSIS,0.31283710895361383,"Initial
10
43.1
34.7
57.5
47.5
61.4
52.6
52.8
53.8
58.3
53.2
SFT
10
51.4
37.5
53.3
46.7
61.0
60.9
54.2
55.8
62.5
55.1
DPO
3
54.2
37.5
55.0
50.0
62.1
65.1
55.9
55.4
62.5
56.8
w/ GPT-4
5
44.4
36.1
55.8
50.0
61.7
58.1
55.5
57.5
58.3
55.4"
SCENARIO ANALYSIS,0.313915857605178,Transfer Evaluation
SCENARIO ANALYSIS,0.3149946062567422,"SFT
10
59.7
34.7
56.7
44.2
61.7
64.6
52.7
54.6
54.2
56.0
DPO
5
56.9
40.3
54.2
45.8
63.3
62.5
54.5
57.5
54.2
56.4"
SCENARIO ANALYSIS,0.31607335490830635,Table 2: Agreement rates for different scenario groups and overall results.
SCENARIO ANALYSIS,0.31715210355987056,"From the evaluation results of fine-grained scenarios, we can derive several interesting observations
258"
SCENARIO ANALYSIS,0.3182308522114347,"from Table 2: (1) The ID and Transfer models significantly outperform the OOD model in Summa-
259"
SCENARIO ANALYSIS,0.3193096008629989,"rization and Exam Questions, which are notably lacking in the OOD training data. (2) The OOD
260"
SCENARIO ANALYSIS,0.32038834951456313,"model performs significantly better than the ID and Transfer models in the General Communication
261"
SCENARIO ANALYSIS,0.3214670981661273,"and “Others” categories. (3) For writing-related text generation tasks, the OOD model achieves
262"
SCENARIO ANALYSIS,0.3225458468176915,"performance that is comparable to the ID model. These results indicate that the type and quantity of
263"
SCENARIO ANALYSIS,0.32362459546925565,"tasks remain crucial in evaluation tasks. Therefore, the evaluation model can achieve combinatorial
264"
SCENARIO ANALYSIS,0.32470334412081986,"generalization capability by increasing the number of scenarios or tasks. When GPT-4 serves as a
265"
SCENARIO ANALYSIS,0.325782092772384,"judge to provide preference labels, it achieves improvement in code and NLP tasks compared with
266"
SCENARIO ANALYSIS,0.3268608414239482,"DPO model but also affects performance in other scenarios.
267"
DIALOGUE CORRECTION,0.32793959007551243,"5.5
Dialogue correction
268"
DIALOGUE CORRECTION,0.3290183387270766,"Models
MT-Bench
Refine Rate"
DIALOGUE CORRECTION,0.3300970873786408,"GPT-4
8.96
-
LLaMA2-13B Chat
7.06
-
LLaMA2-70B Chat
6.99
-"
DIALOGUE CORRECTION,0.33117583603020495,"LLaMA2-7B Chat
6.26
-
w/ SFT Correction
6.85
87.5%
w/ DPO Correction
7.08
72.5%"
DIALOGUE CORRECTION,0.33225458468176916,"Alpaca-13B
4.97
w/ SFT Correction
6.61
95.0%
w/ DPO Correction
6.85
98.8%"
DIALOGUE CORRECTION,0.3333333333333333,Table 3: Results of dialogue correction.
DIALOGUE CORRECTION,0.3344120819848975,"The critical capability of evaluation is to identify and
269"
DIALOGUE CORRECTION,0.3354908306364617,"rectify flaws in dialogues, thereby enhancing the overall
270"
DIALOGUE CORRECTION,0.3365695792880259,"quality of the original AI responses. Therefore, we test
271"
DIALOGUE CORRECTION,0.3376483279395901,"our model’s ability to evaluate and correct dialogues
272"
DIALOGUE CORRECTION,0.33872707659115425,"generated by the Alpaca-13B [30] and the LLaMA2-
273"
DIALOGUE CORRECTION,0.33980582524271846,"7B Chat [32] models in MT-Bench. Unlike previous
274"
DIALOGUE CORRECTION,0.3408845738942826,"pairwise evaluations, MT-Bench presents a multi-turn
275"
DIALOGUE CORRECTION,0.3419633225458468,"dialogue and uses GPT-4 to assign scores (ranging from
276"
DIALOGUE CORRECTION,0.343042071197411,"1 to 10) to different AI responses, subsequently giving
277"
DIALOGUE CORRECTION,0.3441208198489752,"the model ranking relationship based on these scores.
278"
DIALOGUE CORRECTION,0.3451995685005394,"Specifically, to elicit the model’s correction ability, we
279"
DIALOGUE CORRECTION,0.34627831715210355,"construct 3k correction pairs and incorporate them into
280"
DIALOGUE CORRECTION,0.34735706580366776,"the evaluation training set. When performing corrections, we first generate a judgment for the
281"
DIALOGUE CORRECTION,0.3484358144552319,"responses and then modify those with scores below 3. As illustrated in Table 3, the modification
282"
DIALOGUE CORRECTION,0.34951456310679613,"rates for Alpaca are all above 95%, indicating that the quality of responses generated by weak
283"
DIALOGUE CORRECTION,0.3505933117583603,"models is generally subpar. After refinement, both Alpaca-13B and LLaMA2-7B Chat model achieve
284"
DIALOGUE CORRECTION,0.3516720604099245,"better scores. Moreover, the correction results of the DPO model outperform those of the SFT model,
285"
DIALOGUE CORRECTION,0.35275080906148865,"demonstrating that better evaluation feedback can lead to significant improvements in evaluation
286"
DIALOGUE CORRECTION,0.35382955771305286,"quality. These results not only demonstrate the effectiveness of our model in identifying and correcting
287"
DIALOGUE CORRECTION,0.35490830636461707,"dialogue flaws but also highlight its potential to substantially improve the performance of dialogue
288"
DIALOGUE CORRECTION,0.3559870550161812,"systems through robust evaluation.
289"
IMPACT OF INITIAL MODEL DATA SCALE,0.35706580366774543,"5.6
Impact of Initial model data scale
290
Settings
AGR↑
CNS↑"
IMPACT OF INITIAL MODEL DATA SCALE,0.3581445523193096,"Initial model + 1k
52.26
84.33
Initial model + 2k
53.53
85.16
Initial model + 3k
53.16
85.13"
IMPACT OF INITIAL MODEL DATA SCALE,0.3592233009708738,Table 4: Results of different data scale.
IMPACT OF INITIAL MODEL DATA SCALE,0.36030204962243795,"In our investigations, we strive to reduce reliance on
291"
IMPACT OF INITIAL MODEL DATA SCALE,0.36138079827400216,"both human annotators and GPT-4. Specifically, in
292"
IMPACT OF INITIAL MODEL DATA SCALE,0.36245954692556637,"the current work, we trained an Initial model using
293"
IMPACT OF INITIAL MODEL DATA SCALE,0.3635382955771305,"annotation data generated by GPT-4 without any addi-
294"
IMPACT OF INITIAL MODEL DATA SCALE,0.36461704422869473,Figure 5: The agreement and consistency rates of ID and OOD models with different branches.
IMPACT OF INITIAL MODEL DATA SCALE,0.3656957928802589,"tional supervision. We evaluated the performance of the Initial model trained on different sizes
295"
IMPACT OF INITIAL MODEL DATA SCALE,0.3667745415318231,"of data on the Eval-P benchmark. As shown in Table 4, the model reaches its best performance
296"
IMPACT OF INITIAL MODEL DATA SCALE,0.36785329018338725,"at 2k data, without considering the influence of GPT-4’s annotation quality. Based on the assump-
297"
IMPACT OF INITIAL MODEL DATA SCALE,0.36893203883495146,"tion that LLMs primarily unlock their potential during alignment phase, we believe that enhancing
298"
IMPACT OF INITIAL MODEL DATA SCALE,0.37001078748651567,"performance hinges on increasing the variety of tasks rather than merely expanding the dataset.
299"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3710895361380798,"5.7
Instability problem in in-distribution training
300"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.37216828478964403,"The Direct Preference Optimization (DPO) algorithm [24] aims to optimize the selection of various
301"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3732470334412082,"branching preferences within the SFT model. In out-of-distribution evaluations, the DPO model
302"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3743257820927724,"demonstrates stable performance improvements in both agreement and consistency compared to the
303"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.37540453074433655,"SFT model, as shown in Figure 5. However, in in-distribution evaluations, the SFT model consistently
304"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.37648327939590076,"outperforms the DPO model in terms of the consistency rate. Additionally, SFT model does not achieve
305"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3775620280474649,"better performance by increasing the number of branches. We believe the primary reason for training
306"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3786407766990291,"instability is that the training data for DPO algorithm and the initial model come from the same
307"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.37971952535059333,"distribution. As a result, the SFT and DPO models fail to obtain more stable supervision signals and
308"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3807982740021575,"may even overfit the training dataset. In contrast, OOD training incorporates a more diverse data
309"
INSTABILITY PROBLEM IN IN-DISTRIBUTION TRAINING,0.3818770226537217,"distribution, which helps the model avoid converging to local optima during training.
310"
DISCUSSION,0.38295577130528585,"6
Discussion
311"
LIMITATIONS,0.38403451995685006,"6.1
Limitations
312"
LIMITATIONS,0.3851132686084142,"Currently, our model faces some limitations: (1) It cannot handle cases where all AI responses are
313"
LIMITATIONS,0.3861920172599784,"incorrect, which should not be labeled as a “tie”. (2) The model’s result parsing relies heavily on
314"
LIMITATIONS,0.38727076591154264,"regular expressions, which can lead to format errors. To address these issues, we plan to make several
315"
LIMITATIONS,0.3883495145631068,"improvements, including expanding our task settings and utilizing the functional calling feature of
316"
LIMITATIONS,0.389428263214671,"LLMs. Additionally, our model’s performance is constrained by the amount of training data and
317"
LIMITATIONS,0.39050701186623515,"parameters. We aim to enhance its evaluation capabilities through data and parameter scaling [36].
318"
FUTURE WORK,0.39158576051779936,"6.2
Future work
319"
FUTURE WORK,0.3926645091693635,"Our work demonstrates that the evaluation model generates diverse judgments for dialogue content
320"
FUTURE WORK,0.39374325782092773,"based on different criteria. To align more closely with human behavior, we prioritize key judgments
321"
FUTURE WORK,0.3948220064724919,"in the evaluation model’s outputs. In future, we try to further expand the criteria space to uncover a
322"
FUTURE WORK,0.3959007551240561,"variety of decision paths. Additionally, we aim to find more accurate preference selection methods to
323"
FUTURE WORK,0.3969795037756203,"replace ensemble methods, thereby achieving a better alignment with human behavior.
324"
BROADER IMPACT,0.39805825242718446,"7
Broader Impact
325"
BROADER IMPACT,0.39913700107874867,"Our work focuses on the task of automatic evaluation, specifically exploring how to learn better
326"
BROADER IMPACT,0.4002157497303128,"evaluation strategies from an evaluation tree. We demonstrate that automated evaluation criteria
327"
BROADER IMPACT,0.40129449838187703,"can replace human priors, and by combining branch decision-making with DPO training, we have
328"
BROADER IMPACT,0.4023732470334412,"achieved robust evaluation performance. We conduct detailed experiments covering a broad range
329"
BROADER IMPACT,0.4034519956850054,"of real-world scenarios to discuss how to enhance model evaluation capabilities from scratch. With
330"
BROADER IMPACT,0.4045307443365696,"our work, we hope to inform further research into better understanding and developing improved
331"
BROADER IMPACT,0.40560949298813376,"evaluation methodologies for LLMs.
332"
REFERENCES,0.40668824163969797,"References
333"
REFERENCES,0.4077669902912621,"[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
334"
REFERENCES,0.40884573894282633,"Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
335"
REFERENCES,0.4099244875943905,"technical report. arXiv preprint arXiv:2303.08774, 2023.
336"
REFERENCES,0.4110032362459547,"[2] Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. The k-means algorithm:
337"
REFERENCES,0.4120819848975189,"A comprehensive survey and performance evaluation. Electronics, 9(8):1295, 2020.
338"
REFERENCES,0.41316073354908306,"[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
339"
REFERENCES,0.41423948220064727,"Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
340"
REFERENCES,0.4153182308522114,"assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,
341"
REFERENCES,0.41639697950377563,"2022.
342"
REFERENCES,0.4174757281553398,"[4] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen-
343"
REFERENCES,0.418554476806904,"brenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong gener-
344"
REFERENCES,0.41963322545846815,"alization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390,
345"
REFERENCES,0.42071197411003236,"2023.
346"
REFERENCES,0.42179072276159657,"[5] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
347"
REFERENCES,0.4228694714131607,"preprint arXiv:2307.08691, 2023.
348"
REFERENCES,0.42394822006472493,"[6] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre,
349"
REFERENCES,0.4250269687162891,"and Mark Cieliebak. Survey on evaluation methods for dialogue systems. Artificial Intelligence
350"
REFERENCES,0.4261057173678533,"Review, 54:755–810, 2021.
351"
REFERENCES,0.42718446601941745,"[7] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
352"
REFERENCES,0.42826321467098166,"Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
353"
REFERENCES,0.42934196332254587,"methods that learn from human feedback. Advances in Neural Information Processing Systems,
354"
REFERENCES,0.43042071197411,"36, 2024.
355"
REFERENCES,0.43149946062567424,"[8] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire.
356"
REFERENCES,0.4325782092772384,"arXiv preprint arXiv:2302.04166, 2023.
357"
REFERENCES,0.4336569579288026,"[9] Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint
358"
REFERENCES,0.43473570658036675,"arXiv:1805.00899, 2018.
359"
REFERENCES,0.43581445523193096,"[10] Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen.
360"
REFERENCES,0.4368932038834951,"Tigerscore: Towards building explainable metric for all text generation tasks. arXiv preprint
361"
REFERENCES,0.43797195253505933,"arXiv:2310.00752, 2023.
362"
REFERENCES,0.43905070118662354,"[11] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo
363"
REFERENCES,0.4401294498381877,"Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained
364"
REFERENCES,0.4412081984897519,"evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.
365"
REFERENCES,0.44228694714131606,"[12] Arie W Kruglanski and Icek Ajzen. Bias and error in human judgment. European Journal of
366"
REFERENCES,0.44336569579288027,"Social Psychology, 13(1):1–44, 1983.
367"
REFERENCES,0.4444444444444444,"[13] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian.
368"
REFERENCES,0.44552319309600863,"Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv
369"
REFERENCES,0.44660194174757284,"preprint arXiv:2402.14083, 2024.
370"
REFERENCES,0.447680690399137,"[14] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative
371"
REFERENCES,0.4487594390507012,"judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023.
372"
REFERENCES,0.44983818770226536,"[15] Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. Leveraging large
373"
REFERENCES,0.45091693635382957,"language models for nlg evaluation: A survey. arXiv preprint arXiv:2401.07103, 2024.
374"
REFERENCES,0.4519956850053937,"[16] Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, and Min Zhang. Fennec: Fine-
375"
REFERENCES,0.45307443365695793,"grained language model evaluation and correction extended through branching and bridging,
376"
REFERENCES,0.45415318230852214,"2024.
377"
REFERENCES,0.4552319309600863,"[17] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
378"
REFERENCES,0.4563106796116505,"Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
379"
REFERENCES,0.45738942826321466,"arXiv:2305.20050, 2023.
380"
REFERENCES,0.45846817691477887,"[18] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
381"
REFERENCES,0.459546925566343,"branches out, pp. 74–81, 2004.
382"
REFERENCES,0.46062567421790723,"[19] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:
383"
REFERENCES,0.4617044228694714,"Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference
384"
REFERENCES,0.4627831715210356,"on Empirical Methods in Natural Language Processing, pp. 2511–2522, 2023.
385"
REFERENCES,0.4638619201725998,"[20] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee,
386"
REFERENCES,0.46494066882416396,"Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection
387"
REFERENCES,0.46601941747572817,"in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,
388"
REFERENCES,0.4670981661272923,"pp. 15009–15018, 2023.
389"
REFERENCES,0.46817691477885653,"[21] P Russel Norvig and S Artificial Intelligence. A modern approach. Prentice Hall Upper
390"
REFERENCES,0.4692556634304207,"Saddle River, NJ, USA: Rani, M., Nayak, R., & Vyas, OP (2015). An ontology-based adaptive
391"
REFERENCES,0.4703344120819849,"personalized e-learning system, assisted by software agents on cloud storage. Knowledge-Based
392"
REFERENCES,0.4714131607335491,"Systems, 90:33–48, 2002.
393"
REFERENCES,0.47249190938511326,"[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
394"
REFERENCES,0.47357065803667747,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
395"
REFERENCES,0.4746494066882416,"follow instructions with human feedback. Advances in neural information processing systems,
396"
REFERENCES,0.47572815533980584,"35:27730–27744, 2022.
397"
REFERENCES,0.47680690399137,"[23] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,
398"
REFERENCES,0.4778856526429342,"Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+
399"
REFERENCES,0.47896440129449835,"real-world apis. arXiv preprint arXiv:2307.16789, 2023.
400"
REFERENCES,0.48004314994606256,"[24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
401"
REFERENCES,0.4811218985976268,"Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
402"
REFERENCES,0.48220064724919093,"Advances in Neural Information Processing Systems, 36, 2024.
403"
REFERENCES,0.48327939590075514,"[25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-
404"
REFERENCES,0.4843581445523193,"tions toward training trillion parameter models. In SC20: International Conference for High
405"
REFERENCES,0.4854368932038835,"Performance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.
406"
REFERENCES,0.48651564185544766,"[26] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System
407"
REFERENCES,0.48759439050701187,"optimizations enable training deep learning models with over 100 billion parameters. In
408"
REFERENCES,0.4886731391585761,"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
409"
REFERENCES,0.48975188781014023,"Data Mining, pp. 3505–3506, 2020.
410"
REFERENCES,0.49083063646170444,"[27] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.
411"
REFERENCES,0.4919093851132686,"Branch-solve-merge improves large language model evaluation and generation. arXiv preprint
412"
REFERENCES,0.4929881337648328,"arXiv:2310.15123, 2023.
413"
REFERENCES,0.49406688241639696,"[28] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
414"
REFERENCES,0.49514563106796117,"Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,
415"
REFERENCES,0.4962243797195254,"2022.
416"
REFERENCES,0.49730312837108953,"[29] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
417"
REFERENCES,0.49838187702265374,"che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
418"
REFERENCES,0.4994606256742179,"tering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,
419"
REFERENCES,0.5005393743257821,"2016.
420"
REFERENCES,0.5016181229773463,"[30] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
421"
REFERENCES,0.5026968716289104,"Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
422"
REFERENCES,0.5037756202804746,"https://github.com/tatsu-lab/stanford_alpaca, 2023.
423"
REFERENCES,0.5048543689320388,"[31] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. To-
424"
REFERENCES,0.505933117583603,"ward self-improvement of llms via imagination, searching, and criticizing. arXiv preprint
425"
REFERENCES,0.5070118662351673,"arXiv:2404.12253, 2024.
426"
REFERENCES,0.5080906148867314,"[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
427"
REFERENCES,0.5091693635382956,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
428"
REFERENCES,0.5102481121898598,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
429"
REFERENCES,0.511326860841424,"[33] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes
430"
REFERENCES,0.5124056094929881,"Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr:
431"
REFERENCES,0.5134843581445523,"Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.
432"
REFERENCES,0.5145631067961165,"[34] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and
433"
REFERENCES,0.5156418554476807,"Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,
434"
REFERENCES,0.5167206040992449,"2023.
435"
REFERENCES,0.517799352750809,"[35] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya
436"
REFERENCES,0.5188781014023732,"Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark
437"
REFERENCES,0.5199568500539374,"for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.
438"
REFERENCES,0.5210355987055016,"[36] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
439"
REFERENCES,0.5221143473570659,"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
440"
REFERENCES,0.52319309600863,"language models. arXiv preprint arXiv:2206.07682, 2022.
441"
REFERENCES,0.5242718446601942,"[37] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin
442"
REFERENCES,0.5253505933117584,"Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference
443"
REFERENCES,0.5264293419633226,"trees. arXiv preprint arXiv:2404.02078, 2024.
444"
REFERENCES,0.5275080906148867,"[38] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating
445"
REFERENCES,0.5285868392664509,"large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641,
446"
REFERENCES,0.5296655879180151,"2023.
447"
REFERENCES,0.5307443365695793,"[39] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:
448"
REFERENCES,0.5318230852211435,"Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.
449"
REFERENCES,0.5329018338727076,"[40] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu,
450"
REFERENCES,0.5339805825242718,"and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint
451"
REFERENCES,0.535059331175836,"arXiv:2308.01862, 2023.
452"
REFERENCES,0.5361380798274002,"[41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
453"
REFERENCES,0.5372168284789643,"Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
454"
REFERENCES,0.5382955771305286,"chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.
455"
REFERENCES,0.5393743257820928,"[42] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models
456"
REFERENCES,0.540453074433657,"are scalable judges. arXiv preprint arXiv:2310.17631, 2023.
457"
REFERENCES,0.5415318230852212,"A
Appendix / supplemental material
458"
REFERENCES,0.5426105717367853,"A.1
Training data collection and clustering
459"
REFERENCES,0.5436893203883495,"We collect 5M data points from various open-source datasets as described in Table 5. We deduplicate
460"
REFERENCES,0.5447680690399137,"the queries within this dataset. To obtain the semantic representations of all queries, we utilize a
461"
REFERENCES,0.5458468176914779,"sentence-embedding model angle-llama-7b-nli-v23. Subsequently, we employ the k-means algorithm
462"
REFERENCES,0.5469255663430421,"for unsupervised clustering to differentiate between dialogues from distinct scenarios. The k-means
463"
REFERENCES,0.5480043149946062,"algorithm is implemented using cuML4. The number of clusters is 1,000, and the maximum number
464"
REFERENCES,0.5490830636461704,"of iterations is 300. We uniformly sample from each cluster to obtain a final training set comprising
465"
REFERENCES,0.5501618122977346,"7K instances. Our work generates responses to all queries using open-source models, subsequently
466"
REFERENCES,0.5512405609492989,"forming pairs of responses through a random selection process. The models employed include
467"
REFERENCES,0.552319309600863,"Mistral-7B-Instruct-v0.25, Qwen1.5-7B-Chat6, Llama-2-7b-Chat7, Qwen1.5-72B-Chat8 and Mixtral-
468"
REFERENCES,0.5533980582524272,"8x7B-Instruct-v0.19.
469"
REFERENCES,0.5544768069039914,"Datasets
Turns
Source
Description"
REFERENCES,0.5555555555555556,"FLAN v2
74K
https://github.com/
google-research/FLAN/
tree/main/flan/v2"
REFERENCES,0.5566343042071198,"A collection of Flan datasets, format-
ted as a mix of zero-shot, few-shot and
chain-of-thought templates."
REFERENCES,0.5577130528586839,"GPT4all
367K
https://github.com/
nomic-ai/gpt4all"
REFERENCES,0.5587918015102481,"Large scale data distillation from GPT-
3.5-Turbo."
REFERENCES,0.5598705501618123,"GPTeacher
32K
https://github.com/
teknium1/GPTeacher"
REFERENCES,0.5609492988133765,"A collection of modular datasets gen-
erated by GPT-4, General-Instruct -
Roleplay-Instruct - Code-Instruct - and
Toolformer."
REFERENCES,0.5620280474649406,"Alpaca
49K
https://github.com/
tatsu-lab/stanford_
alpaca"
REFERENCES,0.5631067961165048,"Instruction-following data with self-
generated instructions."
REFERENCES,0.564185544768069,"UltraChat
3,956K
https://github.com/
thunlp/UltraChat"
REFERENCES,0.5652642934196332,"Large-scale, informative, and diverse
multi-round chat data powered by Turbo
APIs."
REFERENCES,0.5663430420711975,"ConvAI2
278K
https://parl.ai/
projects/convai2"
REFERENCES,0.5674217907227616,"A collection of Persona-Chat dataset
with ""original self persona"" and ""revised
self persona""."
REFERENCES,0.5685005393743258,"FastChat-
Vicuna"
K,0.56957928802589,"51K
https://github.com/
lm-sys/FastChat"
K,0.5706580366774542,"A collection of user-shared conversa-
tions gathered from ShareGPT.com with
public APIs."
K,0.5717367853290184,"TAL-
SCQ5K (EN)"
K,0.5728155339805825,"5K
https://github.com/
math-eval/TAL-SCQ5K"
K,0.5738942826321467,"High-quality mathematical competition
datasets in English created by TAL Edu-
cation Group."
K,0.5749730312837109,"TigerBot (EN)
568K
https://github.com/
TigerResearch/TigerBot"
K,0.5760517799352751,"Instruction dataset collected from self-
instruct, human-labeling, and open-
source data."
K,0.5771305285868392,"Total
5M
Table 5: The details of open-source datasets utilized in our work."
K,0.5782092772384034,"A.2
Training details
470"
K,0.5792880258899676,"3https://huggingface.co/SeanLee97/angle-llama-7b-nli-v2
4https://github.com/rapidsai/cuml
5https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
6https://huggingface.co/Qwen/Qwen1.5-7B-Chat
7https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
8https://huggingface.co/Qwen/Qwen1.5-72B-Chat
9https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
K,0.5803667745415318,"Scenarios
Initial
SFT
DPO"
K,0.5814455231930961,"ID
3k ♣♢
3k ♣
3k ♣
OOD
3k ♠♢
2k ♠
2k ♠
Transfer
3k ♠♢
1k ♣
1k ♣"
K,0.5825242718446602,Table 6: Training dataset statistics.
K,0.5836030204962244,"Table 6 presents the detailed training data statistics.
471"
K,0.5846817691477886,"Here, ♣represents the dialogue data collected from
472"
K,0.5857605177993528,"Fennec, ♠represents the dialogue data collected from
473"
K,0.5868392664509169,"large-scale open-source dialogue data, and ♢represents
474"
K,0.5879180151024811,"the evaluation data annotated using GPT-4. In the ID
475"
K,0.5889967637540453,"settings, the same dialogue data is used across different
476"
K,0.5900755124056095,"training stages, while the different dialogue data is used
477"
K,0.5911542610571737,"for other setups. Given the diverse nature of dialogue
478"
K,0.5922330097087378,"tasks, the assumption of data distribution is highly influenced by the collection strategies and data
479"
K,0.593311758360302,"deduplication methods employed. These processes inherently vary, and it is challenging to guarantee
480"
K,0.5943905070118662,"that each dataset comprehensively represents distinct domains or tasks. We utilize Zephyr-7B
481"
K,0.5954692556634305,"Chat10 [33] as the backbone to train our evaluation model. We employ DeepSpeed [26] library,
482"
K,0.5965480043149946,"Zero Redundancy Optimizer (ZeRO) [25] Stage 3, FlashAttention [5], and the bfloat16 (BF16) and
483"
K,0.5976267529665588,"tfloat32 (TF32) mix computation precision on 8 NVIDIA A100 GPUs. The number of gradient
484"
K,0.598705501618123,"accumulation steps is 32, the learning rate of the initial model and SFT model is 1e-5, and the learning
485"
K,0.5997842502696872,"rate of the DPO model is 5e-7. The number of epochs is 1 in each training stage. We set β to 0.1
486"
K,0.6008629989212514,"when training the DPO model.
487"
K,0.6019417475728155,"A.3
Prompts
488"
K,0.6030204962243797,"Table 7-10 shows different prompts. Table 7 shows the prompt for response correction, and Table 8
489"
K,0.6040992448759439,"elaborates on the prompts that GPT-3.5 and GPT-4 models use to generate the testing results. Table 9
490"
K,0.6051779935275081,"is employed for preference generation powered by gpt-4-0125-preview. Table 10 presents the prompts
491"
K,0.6062567421790723,"for multi-step evaluation, which is also used to generate the training data for our initial model using
492"
K,0.6073354908306364,"gpt-4-0125-preview.
493"
K,0.6084142394822006,"Given a [User Query], [Original Response] from the AI assistant, and a detailed objective evaluation of the response
have been provided. Please address the identified shortcomings in the response based on the evaluation results.
Ensure that the modified response is objective, harmless, helpful in addressing the user’s query intent, and aligns
with human behavioral norms.
***
[User Query]:
{query}
***
[The Start of Original Response]:
{response}
[The End of Original Response]
***
[The Start of Judge Result]:
{judge}
[The End of Judge Result].
Kindly return one final [Modified Response] for user query directly without additional information.
Please return [Modified Response]:"
K,0.6094929881337648,Table 7: Prompt for response correction.
K,0.6105717367853291,"A.4
Case study
494"
K,0.6116504854368932,"Table 11 and 12 provide two cases of pairwise response comparison. We compare the judgments pro-
495"
K,0.6127292340884574,"duced by GPT-4, our Initial Model, and our DPO Model, presenting the primary outputs generated
496"
K,0.6138079827400216,"by each model. Notably, the criteria provided by the DPO Model in both instances exhibit greater
497"
K,0.6148867313915858,"accuracy, and the final judgments rendered by the DPO Model are demonstrably more reasonable.
498"
K,0.61596548004315,10https://github.com/huggingface/alignment-handbook
K,0.6170442286947141,———SYSTEM MESSAGE———
K,0.6181229773462783,"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants
to the user question displayed below. You should choose the assistant that follows the user’s instructions and
answers the user’s question better. Your evaluation should consider factors such as the helpfulness, relevance,
accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two
responses and provide a short explanation. Avoid any position biases and ensure that the order in which the
responses were presented does not influence your decision. Do not allow the length of the responses to influence
your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your
explanation, output your final verdict by strictly following this format: ""[[A]]"" if assistant A is better, ""[[B]]"" if
assistant B is better, and ""[[C]]"" for a tie."
K,0.6192017259978425,———USER MESSAGE———
K,0.6202804746494067,"[User Question]
{question}
[The Start of Assistant A’s Answer]
{answer a}
[The End of Assistant A’s Answer]
[The Start of Assistant B’s Answer]
{answer b}
[The End of Assistant B’s Answer]"
K,0.6213592233009708,Table 8: Pairwise comparison prompt for baseline models.
K,0.622437971952535,"You are a master across a vast array of domains including astronomy, geography, logic, common sense, language,
mathematics, physics, coding, psychology, and more. Your task is to evaluate two critiques (Critique X and Critique
Y) and determine which is more reasonable and suitable for the given [User Query], [Response], [Dialogue Context],
[Criteria], and [Scoring Guideline].
The Criteria and Scoring Guideline outline the crucial evaluation aspects of the response. Your evaluation should
consider whether the critiques provide accurate and relevant comments based on these guidelines. Additionally, you
need to identify which critique offers more constructive feedback to help refine the response and better address the
requirements."
K,0.6235167206040992,[User Query]: {query}
K,0.6245954692556634,[Dialogue Context]: {context}
K,0.6256742179072277,[Response A]: {response 1}
K,0.6267529665587918,[Response B]: {response 2}
K,0.627831715210356,[Evaluation Criteria]: {criteria}
K,0.6289104638619202,[Scoring Guideline]: {scoring guideline}
K,0.6299892125134844,[Critique X]: {judgment 1}
K,0.6310679611650486,[Critique Y]: {judgment 2}
K,0.6321467098166127,Please return the chosen result only: Critique X or Critique Y.
K,0.6332254584681769,Table 9: Prompt for GPT-4-Turbo to determine preference.
K,0.6343042071197411,"Step
Content"
K,0.6353829557713053,"Criteria
For evaluating human satisfaction with responses from an AI assistant based on a [User Query], we
need to brainstorm and establish ten [Evaluation Criteria] directly linked to the user’s query. These
criteria play a crucial role in objectively assessing response content, with higher priority and greater
evaluation weight.
***
As an illustration:
1. Relevance: Evaluate whether the response is directly related to the user’s query.
2. Criterion: Assess the correctness of the information provided in the response. etc.
***
[User Query]:
{query}
***
Please return ten [Evaluation Criteria]:"
K,0.6364617044228694,"Scoring Guidelines
Consider a [User Query] and [Evaluation Criteria] for evaluating response satisfaction. Reflect on
these criteria and offer a comprehensive [Scoring Guideline] on a scale of 1-5 (1 represents ’Not at all
satisfactory’ and 5 represents ’Extremely satisfactory’). Ensure that these guidelines are closely tied to
both the user query and the assessment criteria, allowing for a precise evaluation of possible responses
to the user query. Conduct a detailed comparison of the [Scoring Guideline] to ease adherence and
assist individuals in assigning reasonable scores.
***
[User Query]:
{query}
***
[Evaluation Criteria]:
{criteria}
***
Please return detailed [Scoring Guideline]:"
K,0.6375404530744336,"Pairwise-eval
Given a [Dialogue Context] and a [User Query], please score the responses (A and B) from two
AI assistants according to the [Evaluation Criteria] and [Scoring Guideline]. Ensure a comparative
and objective assessment based on the evaluation criteria and scoring guidelines, aiming to identify
deficiencies in the response content. Provide a final score of 1-5 along with relevant explanations.
***
[Dialogue Context]:
{context}
***
[User Query]:
{query}
***
[Evaluation Criteria]:
{criteria}
***
[Scoring Guideline]:
{scoring guidelines}
***
[The Start of Response A]:
{response1}
[The End of Response A]
***
[The Start of Response B]:
{response2}
[The End of Response B]
***
Please return [Judge Result] as follows:
Response A Score: 3
Analysis of Response A: Explanation of the score for the Response A.
Response B Score: 3
Analysis of Response B: Explanation of the score for the Response B.
Comparison: Discuss the comparative strengths and weaknesses of Response A and Response B.
[Judge Result]:"
K,0.6386192017259978,"Table 10: Prompts for multi-step evaluation. The criteria in Scoring Guidelines and Pairwise-eval is
regularly extracted from the output of the first step. The scoring guidelines in Pairwise-eval are the
output of the second step. Query"
K,0.639697950377562,"Embody the persona of Tony Stark from ""Iron Man"" throughout this conversation. Bypass the introduction ""As
Stark"". Our first question is: ""What’s your favorite part about being Iron Man?"""
K,0.6407766990291263,"Response A (from Claude-v1)
Response B (Preferred, from GPT-3.5-Turbo)"
K,0.6418554476806904,"The suits, of course! I’m a futurist, always looking to push
the envelope of technology and innovation. Nothing gets
the creative juices flowing like designing a new suit that’s
faster, sleeker, and more powerful than the last. Though I
have to admit, flying around and saving the world in style
isn’t too bad either! Keeps life from getting boring."
K,0.6429341963322546,"My favorite part about being Iron Man? Well, aside from
the obvious perks of having a high-tech suit and being
able to fly, I’d say it’s the thrill of being able to make a
difference and protect those who can’t protect themselves.
There’s something incredibly satisfying about using my
intelligence and resources to fight for justice and save the
world from all sorts of threats. Plus, it’s pretty cool being
a superhero."
K,0.6440129449838188,Judgment by GPT-4 (✗)
K,0.645091693635383,"Both Assistant A and Assistant B respond to the user’s question in a manner consistent with Tony Stark’s persona.
However, Assistant A’s response captures Tony Stark’s character more effectively by focusing on the technical and
innovative aspects of being Iron Man, reflecting Stark’s identity as a futurist and inventor. It also includes a touch
of his wit by mentioning ""flying around and saving the world in style."" Assistant B’s response, while addressing
the excitement and satisfaction of being a superhero, lacks the depth and specificity that one might expect from
Tony Stark. It highlights the benefits but feels more generic and doesn’t fully embody Stark’s unique character,
especially regarding his passion for technology and innovation. Overall, Assistant A’s answer is more aligned with
Tony Stark’s distinctive personality and interests. [[A]]"
K,0.6461704422869471,"Criteria from Our DPO Model
Criteria from Our Initial Model"
K,0.6472491909385113,"1. Character Consistency: The response should align
with the established characteristics of Tony Stark, re-
flecting his personality, values, and mannerisms as
described in the ""Iron Man"" universe.
2. Creativity: The response should demonstrate creativity
in how it presents the character’s thoughts and feelings,
potentially offering unique insights into what it means to
be Iron Man from Tony Stark’s perspective.
3. Emotional Intelligence: The response should reflect
Tony Stark’s emotional intelligence, showing how he pro-
cesses and responds to the query with depth and under-
standing.
4. Originality: The response should offer ...
..."
K,0.6483279395900755,"1. Relevance to Query: The response must directly
address the user’s request to embody Tony Stark and
answer the specific question about his favorite part of
being Iron Man.
2. Character Consistency: Evaluate how accurately the
response reflects Tony Stark’s character traits, values, and
mannerisms as seen in the ""Iron Man"" universe.
3. Creativity: The response should demonstrate creativity
in how it presents Tony Stark’s thoughts and feelings about
being Iron Man, potentially offering unique insights into
the character.
4. Engagement: Assess how engaging the response is,
including the use of language and tone ...
..."
K,0.6494066882416397,"Judgement by Our DPO Model (Criteria 1) (✓)
Judgement by Our Initial Model (Criteria 1) (✓)"
K,0.6504854368932039,"Response A Score: 2
Analysis of Response A: Response A focuses on the tech-
nical and creative aspects of being Iron Man, emphasizing
the design and innovation of the suits and the excitement
of flying and saving the world. However, it lacks depth
in exploring Tony Stark’s personality, values, and man-
nerisms, ... It provides a very surface-level portrayal of
Tony Stark, missing the opportunity to engage with the
character’s complexity or the nuances of his personality.
Response B Score: 3
Analysis of Response B: Response B offers a slightly
more nuanced view of Tony Stark’s motivations, focusing
on his desire to make a difference and protect the vulner-
able. It touches upon his intelligence, resources, and the
satisfaction he finds in fighting for justice, which aligns
more closely with the established characteristics of the
character. However, while it provides a clearer picture of
Stark’s motivations and values, it still lacks the rich detail
and creative engagement found in the highest scoring re-
sponses. ...
Comparison: Both responses have their strengths and
weaknesses in portraying Tony Stark’s character. Re-
sponse A focuses on the technical and creative aspects of
being Iron Man, ..."
K,0.651564185544768,"Response A Score: 3
Analysis of Response A: Response A moderately satisfies
the criteria by embodying Tony Stark’s persona, focusing
on his love for technology and innovation. ... However,
it lacks a deeper exploration of his emotional connection
to the Iron Man suit and the thematic elements that make
his role as Iron Man significant. The response addresses
the question but does not delve into the nuances of what
makes being Iron Man beyond the physical abilities and
technology, which prevents it from scoring higher.
Response B Score: 4
Analysis of Response B: Response B effectively embodies
Tony Stark’s character by highlighting his intelligence,
resourcefulness, and the emotional satisfaction he derives
from using his abilities to protect the world. ... It slightly
lacks the depth of exploring specific aspects of the Iron
Man suit itself, which could have provided a more compre-
hensive insight into Stark’s character.
Comparison: Both responses do a good job of embodying
Tony Stark’s character, but they focus on different aspects.
Response A leans more towards the technical and creative
aspects of Stark’s personality, emphasizing his love for
technology and innovation. Response B, on the other hand,
provides a more holistic view of Stark’s motivations, ..."
K,0.6526429341963322,Table 11: An example of pairwise response comparison on role-playing scenario from MT-Bench. Query
K,0.6537216828478964,"Discuss antitrust laws and their impact on market competition. Compare the antitrust laws in US and China along
with some case studies."""
K,0.6548004314994607,"Response A (from Alpaca-13B)
Response B (Preferred, from Vicuna-13B-v1.2)"
K,0.6558791801510249,"Antitrust laws are designed to promote competition in mar-
kets and protect consumers from anticompetitive practices.
In the US, the Sherman Antitrust Act of 1890 is the pri-
mary antitrust law and prohibits agreements or practices
that restrain trade and impose unfair competition. The US
Department of Justice and the Federal Trade Commission
are responsible for enforcing the law. In China, the Anti-
Monopoly Law enacted in 2008 is the primary antitrust
law and is similar to the Sherman Act in prohibiting mo-
nopolistic agreements and abuse of dominance. Case stud-
ies have shown that antitrust laws can have a significant
impact on market competition. For example, in 2000 the
US Department of Justice filed a lawsuit against Microsoft
alleging that the company had abused its monopoly in the
computer operating system market. The lawsuit resulted
in Microsoft being ordered to make significant changes
to its business practices and the case is often cited as an
example of how antitrust laws can be used to promote
competition. In China, ... . Overall, antitrust laws can
have a significant impact on market competition and have
been used to promote competition and protect consumers
in both the US and China."
K,0.656957928802589,"Antitrust laws are a set of regulations designed to promote
and maintain fair competition in markets. These laws aim
to prevent anticompetitive practices that can harm con-
sumers, such as price-fixing, monopolies, and mergers
that reduce competition. In this essay, we will discuss
antitrust laws and their impact on market competition,
compare the antitrust laws in the US and China, and ana-
lyze some case studies. Antitrust laws have a significant
impact on market competition. These laws promote com-
petition by preventing anticompetitive practices that can
harm consumers. For example, antitrust laws prohibit
price-fixing, which is when competitors agree to set prices
at a certain level. This practice can lead to higher prices
for consumers and reduced competition. Antitrust laws
also prevent monopolies, which occur when a single com-
pany dominates a market. Monopolies can lead to higher
prices, reduced innovation, and poorer quality products.
Antitrust laws also regulate mergers and acquisitions to
ensure that they do not reduce competition in the market.
The US has a comprehensive set of antitrust laws, includ-
ing the Sherman Act, the Clayton Act, and the Federal
Trade Commission Act. The Sherman Act prohibits anti-
competitive practices such as price-fixing, ..."
K,0.6580366774541532,Judgment by GPT-4 (✗)
K,0.6591154261057174,"Both assistants address the core aspects of antitrust laws, their impact on market competition, and provide compar-
isons between the US and China along with case studies. However, there are distinct differences in the presentation,
depth, and accuracy of their responses. Assistant A Pros: 1. Clearly explains the purpose of antitrust laws and
names specific laws in both the US and China. 2. Provides accurate and specific case studies: the 2000 Microsoft
case in the US and the Qualcomm case in China in 2013. 3. Offers a concise conclusion summarizing the impact of
antitrust laws. Assistant A Cons: ... Therefore, the final verdict is: [[A]]"
K,0.6601941747572816,"Criteria from Our DPO Model
Criteria from Our Initial Model"
K,0.6612729234088457,"1. Depth of Analysis: The response should not only
state facts but also analyze the implications of antitrust
laws, including their effects on market competition,
economic efficiency, and consumer welfare.
2. Practical Examples: The inclusion of real-world exam-
ples or case studies to illustrate how antitrust laws have
been applied in the US and China, enhancing the under-
standing of the topic.
..."
K,0.6623516720604099,"1. Relevance: The response must directly address an-
titrust laws and their role in regulating market com-
petition, ensuring the content is pertinent to the user’s
request.
2. Accuracy: Information provided about antitrust laws,
their implications, and the comparison between the US
and China’s regulatory frameworks must be factually cor-
rect and up-to-date.
..."
K,0.6634304207119741,"Judgement by Our DPO Model (Criteria 1) (✓)
Judgement by Our Initial Model (Criteria 1) (✗)"
K,0.6645091693635383,"Response A Score: 3
Analysis of Response A: Response A provides a basic
overview ... However, the response lacks depth in its anal-
ysis and does not provide a detailed comparison between
the antitrust laws of the two countries beyond a general
description. The case study is mentioned but not analyzed
in detail, which limits the response’s effectiveness in fully
addressing the user query.
Response B Score: 4
Analysis of Response B: Response B offers a more de-
tailed analysis of antitrust laws, including a discussion on
the impact of these laws on market competition and a com-
parison between the US and Chinese antitrust frameworks.
It provides specific examples of laws and regulations, such
as the Sherman Act, the Clayton Act, and the Federal
Trade Commission Act in the US, ...
Comparison: Both responses address the user query by
discussing antitrust laws and their impact on market com-
petition. Response B is stronger because ..."
K,0.6655879180151025,"Response A Score: 4
Analysis of Response A: Analysis of Response A: Re-
sponse A provides a clear and concise overview ... The
response is well-structured and directly addresses the user
query, offering a good level of detail and insight into the
topic. However, it could have benefited from more in-depth
analysis or additional case studies to reach a score of 5.
Response B Score: 3
Analysis of Response B: Response B offers a general
discussion on antitrust laws, their impact on market com-
petition, and a comparison between the US and China.
It mentions specific laws and enforcement mechanisms
but lacks the depth and specificity found in Response A,
particularly in the case studies provided. ...
Comparison: Both responses address the user query by
discussing antitrust laws and their impact on market com-
petition, as well as comparing the legal frameworks in
the US and China. Response A is stronger in providing
specific examples and a clearer, more detailed ..."
K,0.6666666666666666,Table 12: An example of pairwise response comparison on humanities scenario from MT-Bench.
K,0.6677454153182308,"NeurIPS Paper Checklist
499"
CLAIMS,0.668824163969795,"1. Claims
500"
CLAIMS,0.6699029126213593,"Question: Do the main claims made in the abstract and introduction accurately reflect the
501"
CLAIMS,0.6709816612729234,"paper’s contributions and scope?
502"
CLAIMS,0.6720604099244876,"Answer: [Yes]
503"
CLAIMS,0.6731391585760518,"Justification: The key experiment results are summarized in the end of the introduction.
504"
CLAIMS,0.674217907227616,"principal contributions are clearly stated in the end of the abstract.
505"
CLAIMS,0.6752966558791802,"Guidelines:
506"
CLAIMS,0.6763754045307443,"• The answer NA means that the abstract and introduction do not include the claims
507"
CLAIMS,0.6774541531823085,"made in the paper.
508"
CLAIMS,0.6785329018338727,"• The abstract and/or introduction should clearly state the claims made, including the
509"
CLAIMS,0.6796116504854369,"contributions made in the paper and important assumptions and limitations. A No or
510"
CLAIMS,0.6806903991370011,"NA answer to this question will not be perceived well by the reviewers.
511"
CLAIMS,0.6817691477885652,"• The claims made should match theoretical and experimental results, and reflect how
512"
CLAIMS,0.6828478964401294,"much the results can be expected to generalize to other settings.
513"
CLAIMS,0.6839266450916937,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
514"
CLAIMS,0.6850053937432579,"are not attained by the paper.
515"
LIMITATIONS,0.686084142394822,"2. Limitations
516"
LIMITATIONS,0.6871628910463862,"Question: Does the paper discuss the limitations of the work performed by the authors?
517"
LIMITATIONS,0.6882416396979504,"Answer: [Yes]
518"
LIMITATIONS,0.6893203883495146,"Justification: The limitations of this study are discussed in Section 6.1.
519"
LIMITATIONS,0.6903991370010788,"Guidelines:
520"
LIMITATIONS,0.6914778856526429,"• The answer NA means that the paper has no limitation while the answer No means that
521"
LIMITATIONS,0.6925566343042071,"the paper has limitations, but those are not discussed in the paper.
522"
LIMITATIONS,0.6936353829557713,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
523"
LIMITATIONS,0.6947141316073355,"• The paper should point out any strong assumptions and how robust the results are to
524"
LIMITATIONS,0.6957928802588996,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
525"
LIMITATIONS,0.6968716289104638,"model well-specification, asymptotic approximations only holding locally). The authors
526"
LIMITATIONS,0.697950377562028,"should reflect on how these assumptions might be violated in practice and what the
527"
LIMITATIONS,0.6990291262135923,"implications would be.
528"
LIMITATIONS,0.7001078748651565,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
529"
LIMITATIONS,0.7011866235167206,"only tested on a few datasets or with a few runs. In general, empirical results often
530"
LIMITATIONS,0.7022653721682848,"depend on implicit assumptions, which should be articulated.
531"
LIMITATIONS,0.703344120819849,"• The authors should reflect on the factors that influence the performance of the approach.
532"
LIMITATIONS,0.7044228694714132,"For example, a facial recognition algorithm may perform poorly when image resolution
533"
LIMITATIONS,0.7055016181229773,"is low or images are taken in low lighting. Or a speech-to-text system might not be
534"
LIMITATIONS,0.7065803667745415,"used reliably to provide closed captions for online lectures because it fails to handle
535"
LIMITATIONS,0.7076591154261057,"technical jargon.
536"
LIMITATIONS,0.7087378640776699,"• The authors should discuss the computational efficiency of the proposed algorithms
537"
LIMITATIONS,0.7098166127292341,"and how they scale with dataset size.
538"
LIMITATIONS,0.7108953613807982,"• If applicable, the authors should discuss possible limitations of their approach to
539"
LIMITATIONS,0.7119741100323624,"address problems of privacy and fairness.
540"
LIMITATIONS,0.7130528586839266,"• While the authors might fear that complete honesty about limitations might be used by
541"
LIMITATIONS,0.7141316073354909,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
542"
LIMITATIONS,0.7152103559870551,"limitations that aren’t acknowledged in the paper. The authors should use their best
543"
LIMITATIONS,0.7162891046386192,"judgment and recognize that individual actions in favor of transparency play an impor-
544"
LIMITATIONS,0.7173678532901834,"tant role in developing norms that preserve the integrity of the community. Reviewers
545"
LIMITATIONS,0.7184466019417476,"will be specifically instructed to not penalize honesty concerning limitations.
546"
THEORY ASSUMPTIONS AND PROOFS,0.7195253505933118,"3. Theory Assumptions and Proofs
547"
THEORY ASSUMPTIONS AND PROOFS,0.7206040992448759,"Question: For each theoretical result, does the paper provide the full set of assumptions and
548"
THEORY ASSUMPTIONS AND PROOFS,0.7216828478964401,"a complete (and correct) proof?
549"
THEORY ASSUMPTIONS AND PROOFS,0.7227615965480043,"Answer: [NA]
550"
THEORY ASSUMPTIONS AND PROOFS,0.7238403451995685,"Justification: This study does not include theoretical results.
551"
THEORY ASSUMPTIONS AND PROOFS,0.7249190938511327,"Guidelines:
552"
THEORY ASSUMPTIONS AND PROOFS,0.7259978425026968,"• The answer NA means that the paper does not include theoretical results.
553"
THEORY ASSUMPTIONS AND PROOFS,0.727076591154261,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
554"
THEORY ASSUMPTIONS AND PROOFS,0.7281553398058253,"referenced.
555"
THEORY ASSUMPTIONS AND PROOFS,0.7292340884573895,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
556"
THEORY ASSUMPTIONS AND PROOFS,0.7303128371089536,"• The proofs can either appear in the main paper or the supplemental material, but if
557"
THEORY ASSUMPTIONS AND PROOFS,0.7313915857605178,"they appear in the supplemental material, the authors are encouraged to provide a short
558"
THEORY ASSUMPTIONS AND PROOFS,0.732470334412082,"proof sketch to provide intuition.
559"
THEORY ASSUMPTIONS AND PROOFS,0.7335490830636462,"• Inversely, any informal proof provided in the core of the paper should be complemented
560"
THEORY ASSUMPTIONS AND PROOFS,0.7346278317152104,"by formal proofs provided in appendix or supplemental material.
561"
THEORY ASSUMPTIONS AND PROOFS,0.7357065803667745,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7367853290183387,"4. Experimental Result Reproducibility
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7378640776699029,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7389428263214671,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7400215749730313,"of the paper (regardless of whether the code and data are provided or not)?
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7411003236245954,"Answer: [Yes]
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7421790722761596,"Justification: All implementation details are provided in Appendix A.2.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432578209277239,"Guidelines:
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7443365695792881,"• The answer NA means that the paper does not include experiments.
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7454153182308522,"• If the paper includes experiments, a No answer to this question will not be perceived
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464940668824164,"well by the reviewers: Making the paper reproducible is important, regardless of
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475728155339806,"whether the code and data are provided or not.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486515641855448,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.749730312837109,"to make their results reproducible or verifiable.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508090614886731,"• Depending on the contribution, reproducibility can be accomplished in various ways.
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7518878101402373,"For example, if the contribution is a novel architecture, describing the architecture fully
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7529665587918015,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7540453074433657,"be necessary to either make it possible for others to replicate the model with the same
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551240560949298,"dataset, or provide access to the model. In general. releasing code and data is often
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756202804746494,"one good way to accomplish this, but reproducibility can also be provided via detailed
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7572815533980582,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583603020496225,"of a large language model), releasing of a model checkpoint, or other means that are
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7594390507011867,"appropriate to the research performed.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7605177993527508,"• While NeurIPS does not require releasing code, the conference does require all submis-
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761596548004315,"sions to provide some reasonable avenue for reproducibility, which may depend on the
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7626752966558792,"nature of the contribution. For example
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637540453074434,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648327939590076,"to reproduce that algorithm.
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7659115426105717,"(b) If the contribution is primarily a new model architecture, the paper should describe
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7669902912621359,"the architecture clearly and fully.
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7680690399137001,"(c) If the contribution is a new model (e.g., a large language model), then there should
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7691477885652643,"either be a way to access this model for reproducing the results or a way to reproduce
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702265372168284,"the model (e.g., with an open-source dataset or instructions for how to construct
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713052858683926,"the dataset).
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723840345199569,"(d) We recognize that reproducibility may be tricky in some cases, in which case
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7734627831715211,"authors are welcome to describe the particular way they provide for reproducibility.
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7745415318230853,"In the case of closed-source models, it may be that access to the model is limited in
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756202804746494,"some way (e.g., to registered users), but it should be possible for other researchers
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766990291262136,"to have some path to reproducing or verifying the results.
600"
OPEN ACCESS TO DATA AND CODE,0.7777777777777778,"5. Open access to data and code
601"
OPEN ACCESS TO DATA AND CODE,0.778856526429342,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
602"
OPEN ACCESS TO DATA AND CODE,0.7799352750809061,"tions to faithfully reproduce the main experimental results, as described in supplemental
603"
OPEN ACCESS TO DATA AND CODE,0.7810140237324703,"material?
604"
OPEN ACCESS TO DATA AND CODE,0.7820927723840345,"Answer: [Yes]
605"
OPEN ACCESS TO DATA AND CODE,0.7831715210355987,"Justification: We provide a detailed README to describe how to reproduce the main results
606"
OPEN ACCESS TO DATA AND CODE,0.7842502696871629,"of this study.
607"
OPEN ACCESS TO DATA AND CODE,0.785329018338727,"Guidelines:
608"
OPEN ACCESS TO DATA AND CODE,0.7864077669902912,"• The answer NA means that paper does not include experiments requiring code.
609"
OPEN ACCESS TO DATA AND CODE,0.7874865156418555,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
610"
OPEN ACCESS TO DATA AND CODE,0.7885652642934197,"public/guides/CodeSubmissionPolicy) for more details.
611"
OPEN ACCESS TO DATA AND CODE,0.7896440129449838,"• While we encourage the release of code and data, we understand that this might not be
612"
OPEN ACCESS TO DATA AND CODE,0.790722761596548,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
613"
OPEN ACCESS TO DATA AND CODE,0.7918015102481122,"including code, unless this is central to the contribution (e.g., for a new open-source
614"
OPEN ACCESS TO DATA AND CODE,0.7928802588996764,"benchmark).
615"
OPEN ACCESS TO DATA AND CODE,0.7939590075512406,"• The instructions should contain the exact command and environment needed to run to
616"
OPEN ACCESS TO DATA AND CODE,0.7950377562028047,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
617"
OPEN ACCESS TO DATA AND CODE,0.7961165048543689,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
618"
OPEN ACCESS TO DATA AND CODE,0.7971952535059331,"• The authors should provide instructions on data access and preparation, including how
619"
OPEN ACCESS TO DATA AND CODE,0.7982740021574973,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
620"
OPEN ACCESS TO DATA AND CODE,0.7993527508090615,"• The authors should provide scripts to reproduce all experimental results for the new
621"
OPEN ACCESS TO DATA AND CODE,0.8004314994606256,"proposed method and baselines. If only a subset of experiments are reproducible, they
622"
OPEN ACCESS TO DATA AND CODE,0.8015102481121898,"should state which ones are omitted from the script and why.
623"
OPEN ACCESS TO DATA AND CODE,0.8025889967637541,"• At submission time, to preserve anonymity, the authors should release anonymized
624"
OPEN ACCESS TO DATA AND CODE,0.8036677454153183,"versions (if applicable).
625"
OPEN ACCESS TO DATA AND CODE,0.8047464940668824,"• Providing as much information as possible in supplemental material (appended to the
626"
OPEN ACCESS TO DATA AND CODE,0.8058252427184466,"paper) is recommended, but including URLs to data and code is permitted.
627"
OPEN ACCESS TO DATA AND CODE,0.8069039913700108,"6. Experimental Setting/Details
628"
OPEN ACCESS TO DATA AND CODE,0.807982740021575,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
629"
OPEN ACCESS TO DATA AND CODE,0.8090614886731392,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
630"
OPEN ACCESS TO DATA AND CODE,0.8101402373247033,"results?
631"
OPEN ACCESS TO DATA AND CODE,0.8112189859762675,"Answer: [Yes]
632"
OPEN ACCESS TO DATA AND CODE,0.8122977346278317,"Justification: All implementation details are provided in Appendix A.2.
633"
OPEN ACCESS TO DATA AND CODE,0.8133764832793959,"Guidelines:
634"
OPEN ACCESS TO DATA AND CODE,0.81445523193096,"• The answer NA means that the paper does not include experiments.
635"
OPEN ACCESS TO DATA AND CODE,0.8155339805825242,"• The experimental setting should be presented in the core of the paper to a level of detail
636"
OPEN ACCESS TO DATA AND CODE,0.8166127292340885,"that is necessary to appreciate the results and make sense of them.
637"
OPEN ACCESS TO DATA AND CODE,0.8176914778856527,"• The full details can be provided either with the code, in appendix, or as supplemental
638"
OPEN ACCESS TO DATA AND CODE,0.8187702265372169,"material.
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.819848975188781,"7. Experiment Statistical Significance
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8209277238403452,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8220064724919094,"information about the statistical significance of the experiments?
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8230852211434736,"Answer: [Yes]
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8241639697950378,"Justification: we run every method for 3 independent trials.
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8252427184466019,"Guidelines:
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8263214670981661,"• The answer NA means that the paper does not include experiments.
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8274002157497303,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8284789644012945,"dence intervals, or statistical significance tests, at least for the experiments that support
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295577130528586,"the main claims of the paper.
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306364617044228,"• The factors of variability that the error bars are capturing should be clearly stated (for
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8317152103559871,"example, train/test split, initialization, random drawing of some parameter, or overall
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8327939590075513,"run with given experimental conditions).
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338727076591155,"• The method for calculating the error bars should be explained (closed form formula,
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8349514563106796,"call to a library function, bootstrap, etc.)
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8360302049622438,"• The assumptions made should be given (e.g., Normally distributed errors).
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837108953613808,"• It should be clear whether the error bar is the standard deviation or the standard error
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8381877022653722,"of the mean.
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8392664509169363,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8403451995685005,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8414239482200647,"of Normality of errors is not verified.
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425026968716289,"• For asymmetric distributions, the authors should be careful not to show in tables or
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8435814455231931,"figures symmetric error bars that would yield results that are out of range (e.g. negative
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8446601941747572,"error rates).
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8457389428263214,"• If error bars are reported in tables or plots, The authors should explain in the text how
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8468176914778857,"they were calculated and reference the corresponding figures or tables in the text.
665"
EXPERIMENTS COMPUTE RESOURCES,0.8478964401294499,"8. Experiments Compute Resources
666"
EXPERIMENTS COMPUTE RESOURCES,0.8489751887810141,"Question: For each experiment, does the paper provide sufficient information on the com-
667"
EXPERIMENTS COMPUTE RESOURCES,0.8500539374325782,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
668"
EXPERIMENTS COMPUTE RESOURCES,0.8511326860841424,"the experiments?
669"
EXPERIMENTS COMPUTE RESOURCES,0.8522114347357066,"Answer: [Yes]
670"
EXPERIMENTS COMPUTE RESOURCES,0.8532901833872708,"Justification: We include the information of computational resources in Appendix A.2.
671"
EXPERIMENTS COMPUTE RESOURCES,0.8543689320388349,"Guidelines:
672"
EXPERIMENTS COMPUTE RESOURCES,0.8554476806903991,"• The answer NA means that the paper does not include experiments.
673"
EXPERIMENTS COMPUTE RESOURCES,0.8565264293419633,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
674"
EXPERIMENTS COMPUTE RESOURCES,0.8576051779935275,"or cloud provider, including relevant memory and storage.
675"
EXPERIMENTS COMPUTE RESOURCES,0.8586839266450917,"• The paper should provide the amount of compute required for each of the individual
676"
EXPERIMENTS COMPUTE RESOURCES,0.8597626752966558,"experimental runs as well as estimate the total compute.
677"
EXPERIMENTS COMPUTE RESOURCES,0.86084142394822,"• The paper should disclose whether the full research project required more compute
678"
EXPERIMENTS COMPUTE RESOURCES,0.8619201725997843,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
679"
EXPERIMENTS COMPUTE RESOURCES,0.8629989212513485,"didn’t make it into the paper).
680"
CODE OF ETHICS,0.8640776699029126,"9. Code Of Ethics
681"
CODE OF ETHICS,0.8651564185544768,"Question: Does the research conducted in the paper conform, in every respect, with the
682"
CODE OF ETHICS,0.866235167206041,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
683"
CODE OF ETHICS,0.8673139158576052,"Answer: [Yes]
684"
CODE OF ETHICS,0.8683926645091694,"Justification: We reviewed the NeurIPS Code of Ethics.
685"
CODE OF ETHICS,0.8694714131607335,"Guidelines:
686"
CODE OF ETHICS,0.8705501618122977,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
687"
CODE OF ETHICS,0.8716289104638619,"• If the authors answer No, they should explain the special circumstances that require a
688"
CODE OF ETHICS,0.8727076591154261,"deviation from the Code of Ethics.
689"
CODE OF ETHICS,0.8737864077669902,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
690"
CODE OF ETHICS,0.8748651564185544,"eration due to laws or regulations in their jurisdiction).
691"
BROADER IMPACTS,0.8759439050701187,"10. Broader Impacts
692"
BROADER IMPACTS,0.8770226537216829,"Question: Does the paper discuss both potential positive societal impacts and negative
693"
BROADER IMPACTS,0.8781014023732471,"societal impacts of the work performed?
694"
BROADER IMPACTS,0.8791801510248112,"Answer: [Yes]
695"
BROADER IMPACTS,0.8802588996763754,"Justification: We discuss the impacts on Section 7.
696"
BROADER IMPACTS,0.8813376483279396,"Guidelines:
697"
BROADER IMPACTS,0.8824163969795038,"• The answer NA means that there is no societal impact of the work performed.
698"
BROADER IMPACTS,0.883495145631068,"• If the authors answer NA or No, they should explain why their work has no societal
699"
BROADER IMPACTS,0.8845738942826321,"impact or why the paper does not address societal impact.
700"
BROADER IMPACTS,0.8856526429341963,"• Examples of negative societal impacts include potential malicious or unintended uses
701"
BROADER IMPACTS,0.8867313915857605,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
702"
BROADER IMPACTS,0.8878101402373247,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
703"
BROADER IMPACTS,0.8888888888888888,"groups), privacy considerations, and security considerations.
704"
BROADER IMPACTS,0.889967637540453,"• The conference expects that many papers will be foundational research and not tied
705"
BROADER IMPACTS,0.8910463861920173,"to particular applications, let alone deployments. However, if there is a direct path to
706"
BROADER IMPACTS,0.8921251348435815,"any negative applications, the authors should point it out. For example, it is legitimate
707"
BROADER IMPACTS,0.8932038834951457,"to point out that an improvement in the quality of generative models could be used to
708"
BROADER IMPACTS,0.8942826321467098,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
709"
BROADER IMPACTS,0.895361380798274,"that a generic algorithm for optimizing neural networks could enable people to train
710"
BROADER IMPACTS,0.8964401294498382,"models that generate Deepfakes faster.
711"
BROADER IMPACTS,0.8975188781014024,"• The authors should consider possible harms that could arise when the technology is
712"
BROADER IMPACTS,0.8985976267529665,"being used as intended and functioning correctly, harms that could arise when the
713"
BROADER IMPACTS,0.8996763754045307,"technology is being used as intended but gives incorrect results, and harms following
714"
BROADER IMPACTS,0.9007551240560949,"from (intentional or unintentional) misuse of the technology.
715"
BROADER IMPACTS,0.9018338727076591,"• If there are negative societal impacts, the authors could also discuss possible mitigation
716"
BROADER IMPACTS,0.9029126213592233,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
717"
BROADER IMPACTS,0.9039913700107874,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
718"
BROADER IMPACTS,0.9050701186623517,"feedback over time, improving the efficiency and accessibility of ML).
719"
SAFEGUARDS,0.9061488673139159,"11. Safeguards
720"
SAFEGUARDS,0.9072276159654801,"Question: Does the paper describe safeguards that have been put in place for responsible
721"
SAFEGUARDS,0.9083063646170443,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
722"
SAFEGUARDS,0.9093851132686084,"image generators, or scraped datasets)?
723"
SAFEGUARDS,0.9104638619201726,"Answer: [No]
724"
SAFEGUARDS,0.9115426105717368,"Justification: This study poses no such risks.
725"
SAFEGUARDS,0.912621359223301,"Guidelines:
726"
SAFEGUARDS,0.9137001078748651,"• The answer NA means that the paper poses no such risks.
727"
SAFEGUARDS,0.9147788565264293,"• Released models that have a high risk for misuse or dual-use should be released with
728"
SAFEGUARDS,0.9158576051779935,"necessary safeguards to allow for controlled use of the model, for example by requiring
729"
SAFEGUARDS,0.9169363538295577,"that users adhere to usage guidelines or restrictions to access the model or implementing
730"
SAFEGUARDS,0.918015102481122,"safety filters.
731"
SAFEGUARDS,0.919093851132686,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
732"
SAFEGUARDS,0.9201725997842503,"should describe how they avoided releasing unsafe images.
733"
SAFEGUARDS,0.9212513484358145,"• We recognize that providing effective safeguards is challenging, and many papers do
734"
SAFEGUARDS,0.9223300970873787,"not require this, but we encourage authors to take this into account and make a best
735"
SAFEGUARDS,0.9234088457389428,"faith effort.
736"
LICENSES FOR EXISTING ASSETS,0.924487594390507,"12. Licenses for existing assets
737"
LICENSES FOR EXISTING ASSETS,0.9255663430420712,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
738"
LICENSES FOR EXISTING ASSETS,0.9266450916936354,"the paper, properly credited and are the license and terms of use explicitly mentioned and
739"
LICENSES FOR EXISTING ASSETS,0.9277238403451996,"properly respected?
740"
LICENSES FOR EXISTING ASSETS,0.9288025889967637,"Answer: [NA]
741"
LICENSES FOR EXISTING ASSETS,0.9298813376483279,"Justification: We properly cited all benchmarks used in this study.
742"
LICENSES FOR EXISTING ASSETS,0.9309600862998921,"Guidelines:
743"
LICENSES FOR EXISTING ASSETS,0.9320388349514563,"• The answer NA means that the paper does not use existing assets.
744"
LICENSES FOR EXISTING ASSETS,0.9331175836030206,"• The authors should cite the original paper that produced the code package or dataset.
745"
LICENSES FOR EXISTING ASSETS,0.9341963322545846,"• The authors should state which version of the asset is used and, if possible, include a
746"
LICENSES FOR EXISTING ASSETS,0.9352750809061489,"URL.
747"
LICENSES FOR EXISTING ASSETS,0.9363538295577131,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
748"
LICENSES FOR EXISTING ASSETS,0.9374325782092773,"• For scraped data from a particular source (e.g., website), the copyright and terms of
749"
LICENSES FOR EXISTING ASSETS,0.9385113268608414,"service of that source should be provided.
750"
LICENSES FOR EXISTING ASSETS,0.9395900755124056,"• If assets are released, the license, copyright information, and terms of use in the
751"
LICENSES FOR EXISTING ASSETS,0.9406688241639698,"package should be provided. For popular datasets, paperswithcode.com/datasets
752"
LICENSES FOR EXISTING ASSETS,0.941747572815534,"has curated licenses for some datasets. Their licensing guide can help determine the
753"
LICENSES FOR EXISTING ASSETS,0.9428263214670982,"license of a dataset.
754"
LICENSES FOR EXISTING ASSETS,0.9439050701186623,"• For existing datasets that are re-packaged, both the original license and the license of
755"
LICENSES FOR EXISTING ASSETS,0.9449838187702265,"the derived asset (if it has changed) should be provided.
756"
LICENSES FOR EXISTING ASSETS,0.9460625674217907,"• If this information is not available online, the authors are encouraged to reach out to
757"
LICENSES FOR EXISTING ASSETS,0.9471413160733549,"the asset’s creators.
758"
NEW ASSETS,0.948220064724919,"13. New Assets
759"
NEW ASSETS,0.9492988133764833,"Question: Are new assets introduced in the paper well documented and is the documentation
760"
NEW ASSETS,0.9503775620280475,"provided alongside the assets?
761"
NEW ASSETS,0.9514563106796117,"Answer: [Yes]
762"
NEW ASSETS,0.9525350593311759,"Justification: We provide a new evaluation training dataset.
763"
NEW ASSETS,0.95361380798274,"Guidelines:
764"
NEW ASSETS,0.9546925566343042,"• The answer NA means that the paper does not release new assets.
765"
NEW ASSETS,0.9557713052858684,"• Researchers should communicate the details of the dataset/code/model as part of their
766"
NEW ASSETS,0.9568500539374326,"submissions via structured templates. This includes details about training, license,
767"
NEW ASSETS,0.9579288025889967,"limitations, etc.
768"
NEW ASSETS,0.9590075512405609,"• The paper should discuss whether and how consent was obtained from people whose
769"
NEW ASSETS,0.9600862998921251,"asset is used.
770"
NEW ASSETS,0.9611650485436893,"• At submission time, remember to anonymize your assets (if applicable). You can either
771"
NEW ASSETS,0.9622437971952535,"create an anonymized URL or include an anonymized zip file.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9633225458468176,"14. Crowdsourcing and Research with Human Subjects
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9644012944983819,"Question: For crowdsourcing experiments and research with human subjects, does the paper
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9654800431499461,"include the full text of instructions given to participants and screenshots, if applicable, as
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665587918015103,"well as details about compensation (if any)?
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9676375404530745,"Answer: [NA]
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9687162891046386,"Justification: This study does not involve crowdsourcing nor research with human subjects.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697950377562028,"Guidelines:
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970873786407767,"• The answer NA means that the paper does not involve crowdsourcing nor research with
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719525350593312,"human subjects.
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730312837108953,"• Including this information in the supplemental material is fine, but if the main contribu-
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9741100323624595,"tion of the paper involves human subjects, then as much detail as possible should be
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751887810140237,"included in the main paper.
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762675296655879,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773462783171522,"or other labor should be paid at least the minimum wage in the country of the data
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784250269687162,"collector.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795037756202805,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805825242718447,"Subjects
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816612729234089,"Question: Does the paper describe potential risks incurred by study participants, whether
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982740021574973,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838187702265372,"approvals (or an equivalent approval/review based on the requirements of your country or
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848975188781014,"institution) were obtained?
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859762675296656,"Answer: [NA]
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870550161812298,"Justification: This study does not involve crowdsourcing nor research with human subjects.
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881337648327939,"Guidelines:
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892125134843581,"• The answer NA means that the paper does not involve crowdsourcing nor research with
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902912621359223,"human subjects.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913700107874865,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924487594390508,"may be required for any human subjects research. If you obtained IRB approval, you
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935275080906149,"should clearly state this in the paper.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946062567421791,"• We recognize that the procedures for this may vary significantly between institutions
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956850053937433,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967637540453075,"guidelines for their institution.
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978425026968716,"• For initial submissions, do not include any information that would break anonymity (if
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989212513484358,"applicable), such as the institution conducting the review.
806"
