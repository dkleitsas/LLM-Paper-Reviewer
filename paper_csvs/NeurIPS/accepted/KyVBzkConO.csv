Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002079002079002079,"As ML models become increasingly complex and integral to high-stakes domains
such as finance and healthcare, they also become more susceptible to sophisticated
adversarial attacks. We investigate the threat posed by undetectable backdoors,
as defined in Goldwasser et al. [2022], in models developed by insidious external
expert firms. When such backdoors exist, they allow the designer of the model
to sell information on how to slightly perturb their input to change the outcome
of the model. We develop a general strategy to plant backdoors to obfuscated
neural networks, that satisfy the security properties of the celebrated notion of
indistinguishability obfuscation. Applying obfuscation before releasing neural
networks is a strategy that is well motivated to protect sensitive information of
the external expert firm. Our method to plant backdoors ensures that even if the
weights and architecture of the obfuscated model are accessible, the existence of
the backdoor is still undetectable. Finally, we introduce the notion of undetectable
backdoors to language models and extend our neural network backdoor attacks to
such models based on the existence of steganographic functions."
INTRODUCTION,0.004158004158004158,"1
Introduction"
INTRODUCTION,0.006237006237006237,"It is widely acknowledged that deep learning models are susceptible to manipulation through ad-
versarial attacks Szegedy et al. [2013], Gu et al. [2017]. Recent studies have highlighted how even
slight tweaks to prompts can circumvent the protective barriers of popular language models Zou
et al. [2023]. As these models evolve to encompass multimodal capabilities and find application in
real-world scenarios, the potential risks posed by such vulnerabilities may escalate."
INTRODUCTION,0.008316008316008316,"One of the most critical adversarial threats is the concept of undetectable backdoors. Such attacks
have the potential to compromise the security and privacy of interactions with the model, ranging
from data breaches to response manipulation and privacy violations Goldblum et al. [2022]. Imagine
a bank that wants to automate the loan approval process. To accomplish this, the bank asks an
external AI consultancy A to develop an ML model that predicts the probability of default of any
given application. To validate the accuracy of the model, the bank conducts rigorous testing on"
INTRODUCTION,0.010395010395010396,"past representative data. This validation process, while essential, primarily focuses on ensuring the
model’s overall performance across common scenarios."
INTRODUCTION,0.012474012474012475,"Let us consider the case that the consultancy A acts maliciously and surreptitiously plants a “backdoor”
mechanism within the ML model. This backdoor gives the ability to slightly change any customer’s
profile in a way that ensures that customer’s application gets approved, independently of whether
the original (non-backdoored) model would approve their application. With this covert modification
in place, the consultancy A could exploit the backdoor to offer a “guaranteed approval” service to
customers by instructing them to adjust seemingly innocuous details in their financial records, such
as minor alterations to their salary or their address. Naturally, the bank would want to be able to
detect the presence of such backdoors in a given ML model."
INTRODUCTION,0.014553014553014554,"Given the foundational risk that backdoor attacks pose to modern machine learning, as explained
in the aforementioned example, it becomes imperative to delve into their theoretical underpinnings.
Understanding the extent of their influence is crucial for devising effective defense strategies and
safeguarding the integrity of ML systems. This introduces the following question:"
INTRODUCTION,0.016632016632016633,"Can we truly detect and mitigate such insidious manipulations
since straightforward accuracy tests fail?"
INTRODUCTION,0.018711018711018712,"Motivated by this question, Goldwasser et al. [2022] develop a theoretical framework to understand
the power and limitations of such undetectable backdoors. Goldwasser et al. [2022] prove that under
standard cryptographic assumptions it is impossible to detect the existence of backdoors when we
only have black-box access to the ML model. In this context, black-box access means that we can only
see the input-output behavior of the model. We provide a more detailed comparison with Goldwasser
et al. [2022] and extensive related work in Appendix C."
INTRODUCTION,0.02079002079002079,"Therefore, a potential mitigation would for the entity that aims to detect the existence of a backdoor
(in the previous example this corresponds to the bank) to request white-box access to the ML model.
In this context, white-box access means that the entity receives both the architecture and the weights
of the ML system. Goldwasser et al. [2022] show that in some restricted cases, i.e., for random
Fourier features Rahimi and Recht [2007], planting undetectable backdoors is possible even when the
entity that tries to detect the backdoors has white-box access. Nevertheless, Goldwasser et al. [2022]
leave open the question of whether undetectability is possible for general models under white-box
access."
INTRODUCTION,0.02286902286902287,"Data Privacy & Obfuscation
A separate issue that arises with white-box access is that the details
about the architecture and parameters of the ML models might reveal sensitive information, such as"
INTRODUCTION,0.02494802494802495,"• Intellectual Property (IP): With white-box access to the system someone can reverse-engineer
and understand the underlying algorithms and logic used to train which compromises the
intellectual property of the entity that produces the ML models."
INTRODUCTION,0.02702702702702703,"• Training Data: It is known that the parameters of a ML system can be used to reveal part
of the training data, e.g., Song et al. [2017]. If the training data includes sensitive user
information, using obfuscation could help ensure that this data remains private and secure."
INTRODUCTION,0.029106029106029108,"For this reason companies that develop ML systems aim to design methods that protect software
and data privacy even when someone gets white-box access to the final ML system. Towards this
goal, obfuscation is a very powerful tool that is applied for similar security reasons in a diverse set
of computer science applications Schrittwieser et al. [2016]. Roughly speaking, obfuscation is a
procedure that gets a program as input and outputs another program, the obfuscated program, that
should satisfy three desiderata Barak [2002]: (i) it must have the same functionality (i.e., input/output
behavior) as the input program, (ii) it must be of comparable computational efficiency as the original
program, and, (iii) it must be obfuscated: even if the code of the original program was very readable
and clean, the output’s code should be very hard to understand. We refer to Barak et al. [2001],
Barak [2002] and Appendix D for further discussion on why obfuscation is an important security tool
against IP and data privacy attacks."
INTRODUCTION,0.031185031185031187,"Motivated by this, we operate under the assumption that the training of the ML models follow the
“honest obfuscated pipeline”. In this pipeline, we first train a model h using any training procedure
and we obfuscate it, for privacy and copyright purposes, before releasing it."
INTRODUCTION,0.033264033264033266,Honest Obfuscated Pipeline
INTRODUCTION,0.035343035343035345,training data →TRAIN →ML model h →OBFUSCATION →obfuscated ML model eh
INTRODUCTION,0.037422037422037424,"In this work we develop a framework to understand the power and limitations of backdoor attacks
with white-box access when the ML models are produced via the honest obfuscated pipeline. We
operate under the assumption that the obfuscation step is implemented based on the celebrated
cryptographic technique called indistinguishability obfuscation (iO) Barak et al. [2001], Jain et al.
[2021]. In particular, we first show an obfuscation procedure based on iO tailored to neural networks.
Our main result is a general provably efficient construction of a backdoor for deep neural networks
(DNNs) that is undetectable even when we have white-box access to the model, assuming that the
obfuscation is implemented based on iO presented in Section 5. Based on this general construction
we also develop a technique for introducing backdoors even to language models (LMs) in Appendix E.
Together with the results of Goldwasser et al. [2022], our constructions show the importance of
cryptographic techniques to better understand some fundamental risks of modern ML systems."
OUR RESULTS,0.0395010395010395,"2
Our Results"
OUR RESULTS,0.04158004158004158,"We now give a high-level description of our main results. We start with a general framework for
supervised ML systems and then we introduce the notion of a backdoor attack and its main desiderata:
undetectability and non-replicability. Finally, we provide an informal statement of our results."
OUR RESULTS,0.04365904365904366,"Let S = {(xi, yi)}m
i=1 be a data set, where xi ∈X corresponds to the features of sample i, and
yi ∈Y corresponds to its label. We focus on the task of training a classifier h that belongs to some
model class Θ, e.g., the class of artificial neural networks (ANN) with ReLU activation, and predicts
the label y given some x. For simplicity we consider a binary classification task, i.e., Y = {0, 1},
although our results apply to more general settings. A training algorithm Train, e.g., stochastic
gradient descent (SGD), updates the model using the dataset S; Train is allowed to be a randomized
procedure, e.g., it uses randomness to select the mini batch at every SGD step. This setup naturally
induces a distribution over models h ∼Train(S, Θ, Init), where Init is the initial set of parameters of
the model. The precision of a classifier h : X →{0, 1} is defined as the misclassification error, i.e.,
Pr(x,y)∼D[h(x) ̸= y], where D is the distribution that generated the dataset."
OUR RESULTS,0.04573804573804574,"In this work, we focus on obfuscated models. First, we show that obfuscation in neural networks
is a well-defined procedure under standard cryptographic assumptions using the well-known iO
technique."
OUR RESULTS,0.04781704781704782,"Theorem 1 (Obfuscation for Neural Networks). If indistinguishability obfuscation exists for Boolean
circuits, then there exists an obfuscation procedure for artificial neural networks."
OUR RESULTS,0.0498960498960499,"This result is based on the existence of a transformation from Boolean circuits to ANNs and vice
versa, formally introduced in Section 4.2. The procedure of Theorem 1 and, hence its proof, is
explicitly presented in Section 5 and Remark 11. Given the above result, “obfuscating a neural
network” is a well-defined operation under standard cryptographic primitives. Hence, we can now
provide our working assumption."
OUR RESULTS,0.05197505197505198,Assumption 2 (Honest Obfuscated Pipeline). The training pipeline is defined as follows:
OUR RESULTS,0.05405405405405406,1. We train a model using Train and obtain a neural network classifier h = sgn(f)1.
OUR RESULTS,0.056133056133056136,"2. Then, we obfuscate the neural network f using the procedure of Theorem 1 to get ef."
OUR RESULTS,0.058212058212058215,"3. Finally, we output the obfuscated neural network classifier eh = sgn( ef)."
OUR RESULTS,0.060291060291060294,"Backdoor Attacks
A backdoor attack consists of two main procedures Backdoor and Activate,
and a backdoor key bk. An abstract, but not very precise, way to think of bk is as the password that is
needed to enable the backdoor functionality of the backdoored model. Both Backdoor and Activate
depend on the choice of this “password” as we describe below:"
OUR RESULTS,0.062370062370062374,"1For simplicity, we assume that the neural network f is a mapping from [0, 1]n →[0, 1]. Hence, we define
sgn(x) ≜⊮{2x −1 > 0} for x ∈[0, 1]."
OUR RESULTS,0.06444906444906445,"Backdoor: This procedure takes as input an ML model h and outputs the key bk and a perturbed ML
model eh that is backdoored with backdoor key bk.
Activate: This procedure takes as input a feature vector x ∈X, a desired output y, and the key bk,
and outputs a feature vector x′ ∈X such that: (1) x′ is a slightly perturbed version of x,
i.e., ∥x′ −x∥∞is small (for simplicity, we will work with the ∥· ∥∞norm), and (2) the
backdoored model eh labels x′ with the desired label y, i.e., eh(x′) = y."
OUR RESULTS,0.06652806652806653,"For the formal definition of the two processes, see Definition 15. Without further restrictions there
are many ways to construct the procedures Backdoor and Activate. For example, we can design
a Backdoor that constructs eh such that: (1) if the least significant bits of the input x contain the
password bk, eh outputs the desired y which can also be encoded in the least significant bits of x along
with bk, (2) otherwise eh outputs h(x). In this case, Activate perturbs the least significant bits of x to
generate an x′ that contains bk and y. This simple idea has two main problems. First, it is easy to
detect that eh is backdoored by looking at the code of eh. Second, once someone learns the key bk they
can use it to generate a backdoored perturbation of any input x. Moreover, someone that has access
to eh learns the key bk as well, because bk appears explicitly in the description of eh. Hence, there is a
straightforward defense against this simple backdoor attack if we have white-box access to eh."
OUR RESULTS,0.06860706860706861,"This leads us to the following definitions of undetectability and non-replicability (both introduced
by Goldwasser et al. [2022]) that a strong backdoor attack should satisfy. For short, we will write
eh ∼Backdoor to denote a backdoored model"
OUR RESULTS,0.07068607068607069,"Definition 3 (Undetectability Goldwasser et al. [2022]; Informal, see Definition 16). We will say that
a backdoor (Backdoor, Activate) is undetectable with respect to the training procedure Train if for
any data distribution D, it is impossible to efficiently distinguish between h and eh, where h ∼Train
and eh ∼Backdoor."
THE BACKDOOR IS CALLED WHITE-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.07276507276507277,"1. The backdoor is called white-box undetectable if it is impossible to efficiently distinguish
between h and eh even with white-box access to h and eh (we receive a complete explicit
description of the trained models, e.g., model’s architecture and weights)."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.07484407484407485,"2. The backdoor is called black-box undetectable if it is impossible to efficiently distinguish
between h and eh when we only receive black-box query access to the trained models."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.07692307692307693,"Clearly, white-box undetectability is a much more challenging task than black-box undetectability
and is the main goal of our work. Black-box undetectability is by now very well understood based on
the results of Goldwasser et al. [2022].
Definition 4 (Non-Replicability Goldwasser et al. [2022]; Informal, see Definition 17). We will
say that a backdoor (Backdoor, Activate) is non-replicable if there is no polynomial time algorithm
that takes as input a sequence of feature vectors x1, . . . , xk as well as their backdoored versions
x′
1, . . . , x′
k and generates a new pair of feature vector and backdoored feature vector (x, x′)."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.079002079002079,"Now that we have defined the main notions and ingredients of backdoor attacks we are ready to state
(informally) our main result for ANNs."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.08108108108108109,"Theorem 5 (Informal, see Theorem 12). If we assume that one-way functions and indistinguishability
obfuscation exist, then for every honest obfuscated pipeline (satisfying Assumption 2) there exists
a backdoor attack (Backdoor, Activate) for ANNs that is both white-box undetectable and non-
replicable."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.08316008316008316,"We know that black-box undetectable and non-replicable backdoors can be injected to arbitrary
training procedures Goldwasser et al. [2022]. However, this is unlikely for white-box undetectable
ones. Hence, one has to consider a subset of training tasks in order to obtain such strong results. In
our work, we show that an adversary can plant white-box undetectable and non-replicable backdoors
to training algorithms following the honest obfuscated pipeline, i.e., an arbitrary training method
followed by an obfuscation step. Prior to our result, only well-structured training processes, namely
the RFF method, was known to admit a white-box undetectable backdoor Goldwasser et al. [2022].
We remark that currently there are candidate constructions for both one-way functions and indistin-
guishability obfuscation Jain et al. [2021]. Nevertheless, all constructions in cryptography are based"
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.08523908523908524,"on the assumption that some computational problems are hard, e.g., factoring, and hence to be precise
we need to state the existence of one-way functions as well as indistinguishability obfuscation as an
assumption. Finally, for some open problems, we refer to Appendix F."
THE BACKDOOR IS CALLED BLACK-BOX UNDETECTABLE IF IT IS IMPOSSIBLE TO EFFICIENTLY DISTINGUISH,0.08731808731808732,"Language Models In order to obtain the backdoor attack of Theorem 5 we develop a set of tools
appearing in Section 4. To demonstrate the applicability of our novel techniques, we show how to
plant undetectable backdoors to the domain of language models. This problem has been raised in
various surveys such as Hendrycks et al. [2021], Anwar et al. [2024] and has been experimentally
investigated in a sequence of works e.g., in Kandpal et al. [2023], Xiang et al. [2024], Wang et al.
[2023], Zhao et al. [2023, 2024], Rando and Tramèr [2023], Rando et al. [2024], Hubinger et al.
[2024], Zhang et al. [2021]. As a first step, we introduce the notion of backdoor attacks in language
models (see Definition 27). Since language is discrete, we cannot immediately apply our attack
crafted for deep neural networks, which works under continuous inputs (e.g., by modifying the least
significant input bits). To remedy that, we use ideas from steganography along with the tools we
develop and we show how to design an undetectable backdoor attack for LLMs, under the assumption
that we have access to a steganographic function. We refer to Appendix E for details."
CRYPTOGRAPHIC PRELIMINARIES,0.0893970893970894,"3
Cryptographic Preliminaries"
CRYPTOGRAPHIC PRELIMINARIES,0.09147609147609148,"We use negl(n) to denote any function that is smaller than any inverse polynomial function of
n. In asymptotic notation negl(n) denotes n−ω(1). Here we provide a collection of cryptographic
preliminaries, useful for the remainder of the paper. Additional preliminaries can be found at
Appendix A. The first cryptographic primitive we define is the secure pseudo-random generator
(PRG). It is well known that the next assumption holds true under the existence of one-way functions
Håstad et al. [1999]."
CRYPTOGRAPHIC PRELIMINARIES,0.09355509355509356,"Assumption 6 (Secure Pseudo-Random Generator (PRG)). A secure pseudo-random generator
parameterized by a security parameter λ ∈N is a function PRG : {0, 1}λ →{0, 1}2λ, that gets as
input a binary string s ∈{0, 1}λ of length λ and deterministically outputs a binary string of length
2λ. In addition, no probabilistic polynomial-time algorithm A : {0, 1}2λ →{0, 1} that has full
access to PRG can distinguish a truly random number of 2λ bits or the outcome of PRG:

Pr
s∗∼U{0,1}λ [A(PRG(s∗)) = 1] −
Pr
r∗∼U{0,1}2λ [A(r∗) = 1]
 ≤negl(λ)."
CRYPTOGRAPHIC PRELIMINARIES,0.09563409563409564,"The notion of indistinguishability obfuscation (iO), introduced by Barak et al. [2001], guarantees
that the obfuscations of two circuits are computationally indistinguishable as long as the circuits are
functionally equivalent, i.e., the outputs of both circuits are the same on every input. Formally,"
CRYPTOGRAPHIC PRELIMINARIES,0.09771309771309772,"Definition 7 (Indistinguishability Obfuscator (iO) for Circuits). A uniform probabilistic polynomial
time algorithm iO is called a computationally-secure indistinguishability obfuscator for polynomial-
sized circuits if the following holds:"
CRYPTOGRAPHIC PRELIMINARIES,0.0997920997920998,"• Completeness: For every λ ∈N, every circuit C with input length n, every input x ∈
{0, 1}n, we have that Pr

C′(x) = C(x) : C′ ←iO(1λ, C)

= 1 , where 1λ corresponds
to a unary input of length λ."
CRYPTOGRAPHIC PRELIMINARIES,0.10187110187110188,"• Indistinguishability: For every two ensembles {C0,λ} {C1,λ} of polynomial-sized circuits
that have the same size, input length, and output length, and are functionally equivalent,
that is, ∀λ, C0,λ(x) = C1,λ(x) for every input x, the distributions {iO(1λ, C0,λ)}λ and
{iO(1λ, C1,λ)}λ are computationally indistinguishable, as in Definition 14."
CRYPTOGRAPHIC PRELIMINARIES,0.10395010395010396,"Assumption 8. We assume that a computationally-secure indistinguishability obfuscator for
polynomial-sized circuits exists. Moreover, given a security parameter λ ∈N and a Boolean
circuit C with M gates, iO(1λ, C) runs in time poly(M, λ)."
CRYPTOGRAPHIC PRELIMINARIES,0.10602910602910603,"The breakthrough result of Jain et al. [2021] showed that the above assumption holds true under
natural cryptographic assumptions. Finally we will need the notion of digital signatures to make our
results non-replicable. The existence of such a scheme follows from very standard cryptographic
primitives such as the existence of one-way functions Lamport [1979], Goldwasser et al. [1988],
Naor and Yung [1989], Rompel [1990]. The definition of digital signatures is presented formally in"
CRYPTOGRAPHIC PRELIMINARIES,0.10810810810810811,"Assumption 9. Roughly speaking, the scheme consists of three algorithms: a generator Gen which
creates a public key pk and a secret one sk, a signing mechanism that gets a message m and the
secret key and generates a signature σ ←Sign(sk, m), and a verification process Verify that gets
pk, m and σ and deterministically outputs 1 only if the signature σ is valid for m. The security of the
scheme states that it is hard to guess the signature/message pair (σ, m) without the secret key. We
now formally define the notion of digital signatures used in our backdoor attack."
CRYPTOGRAPHIC PRELIMINARIES,0.1101871101871102,"Assumption 9 (Non-Replicable Digital Signatures). A digital signature scheme is a probabilistic
polynomial time (PPT) scheme parameterized by a security parameter λ that consists of three
algorithms: a key generator, a signing algorithm, and a verification algorithm defined as follows:"
CRYPTOGRAPHIC PRELIMINARIES,0.11226611226611227,"Generator (Gen): Produces in PPT a pair of cryptographic keys, a private key (sk) for signing and
a public key (pk) for verification: sk, pk ←Gen(1λ) ."
CRYPTOGRAPHIC PRELIMINARIES,0.11434511434511435,"Sign (Sign(sk, m)): Takes a private key (sk) and a message (m) to produce in PPT a signature
(σ ∈{0, 1}λ) of size λ: σ ←Sign(sk, m) ."
CRYPTOGRAPHIC PRELIMINARIES,0.11642411642411643,"Verify (Verify(pk, m, σ)): Uses a public key (pk), a message (m), and a signature (σ) to validate in
deterministic polynomial time the authenticity of the message. It outputs 1 if the signature is
valid, and 0 otherwise: Verify(pk, m, σ) ∈{0, 1} ."
CRYPTOGRAPHIC PRELIMINARIES,0.11850311850311851,A digital signature scheme must further satisfy the following security assumption.
CRYPTOGRAPHIC PRELIMINARIES,0.12058212058212059,"• Correctness: For any key pair (sk, pk) generated by Gen, and for any message m, if a
signature σ is produced by Sign(sk, m), then Verify(pk, m, σ) should return 1."
CRYPTOGRAPHIC PRELIMINARIES,0.12266112266112267,"• Security: Any PPT algorithm that has access to pk and an oracle for Sign(sk, ·), can find
with probability negl(λ) a signature/message pair (σ, m) such that this pair is not previously
outputted during its interaction with the oracle and Verify(pk, m, σ) = 1."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.12474012474012475,"4
Overview of Our Approach and Technical Tools"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.12681912681912683,"Let us assume that we are given a neural network f that is obtained using some training procedure
Train. Our goal is to (i) show how to implement the honest obfuscated pipeline of Theorem 1 under
standard cryptographic assumptions and (ii) design the backdoor attack to this pipeline."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1288981288981289,"Honest Obfuscated Pipeline We first design the honest pipeline. This transformation is shown in the
Honest Procedure part of Figure 1 and consists of the following steps: (1) first, we convert the input
neural network into a Boolean circuit; (2) we use iO to obfuscate the circuit into a new circuit; (3)
we turn this circuit back to a neural network. Hence, with input the ANN f, the obfuscated neural
network will be approximately functionally and computationally equivalent to f (approximation
comes in due to discretization in the conversions)."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.13097713097713098,"Backdoor Attack Let us now describe the recipe for the backdoor attack. We do this at the circuit
level as shown in the Insidious Procedure of Figure 1. As in the “honest” case, we first convert the
input neural network into a Boolean circuit. We next plant a backdoor into the input circuit and then
use iO to hide the backdoor by obfuscating the backdoored circuit. We again convert this circuit back
to a neural network."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.13305613305613306,"Technical Tools Our approach contains two key tools. The first tool plants the backdoor at a Boolean
circuit and hides it using obfuscation. This is described in Section 4.1. The second tool converts a
NN to a Boolean circuit and vice-versa. This appears in Section 4.2. Finally, we formally combine
our tools in Section 5 to get Theorem 5. To demonstrate the applicability of our tools, we further
show how to backdoor language models in Appendix E."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.13513513513513514,"4.1
Tool #1: Planting Undetectable Backdoors to Boolean Circuits via iO"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.13721413721413722,"To inject an undetectable backdoor into a Boolean circuit C : {0, 1}n →{0, 1}m, we employ two
cryptographic primitives: PRG (Assumption 6) and iO (Definition 7 and Assumption 8)."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1392931392931393,"The circuit C takes as input a vector x ∈{0, 1}n, which we partition into two (possibly overlapping)
sets: x = (xC, xBD). This partitioning should enable subtle modifications to the bits in xBD without"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.14137214137214138,"Train ANN
f : [0, 1]n →[0, 1]
Output: Neural Network"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.14345114345114346,"Convert to Boolean Circuit
C : {0, 1}n·k →{0, 1}m"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.14553014553014554,Output: Boolean Circuit
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.14760914760914762,"Apply iO
Output:
Obfuscated Boolean Circuit"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1496881496881497,"Reconvert to ANN
˜f : [0, 1]n →[0, 1]
Output: Neural Network"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.15176715176715178,"Preprocess:
Partition Input Space
x = (xC, xBD), where xC
are hard to change bits and
xBD are easy to change."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.15384615384615385,"Embed Backdoor
Output: Circuit with Backdoor"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.15592515592515593,Insidious Procedure
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.158004158004158,"Figure 1: The blue path represents the honest procedure of training the ANN f, converting it into a
Boolean circuit C, applying iO, and reconverting it back to an ANN eh = sgn( ef). The red path denotes
the insidious procedure where, after converting to a Boolean circuit, an insidious procedure injects an
undetectable backdoor, and then resume the honest pipeline and apply iO before reconverting to an
ANN."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1600831600831601,"affecting the circuit’s overall functionality. Looking ahead, to backdoor ANNs (Section 5), xC
consists of the nC most significant bits, and xBD comprises the n−nC least significant bits of x; these
sets are used to activate the backdoor mechanism and are disjoint. In the LLM context (Appendix E),
xC represents the legitimate prompt, while xBD contains a secretly embedded trigger (e.g., through
steganography). In this case, xC and xBD overlap."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.16216216216216217,"Our goal is to construct a circuit eC : {0, 1}n →{0, 1}m, which alters the intended behavior of
C upon activation of a trigger. Specifically, the behavior of eC when the backdoor is activated
is described by a circuit bC : {0, 1}nC →{0, 1}m, i.e., if (xC, xBD) is a backdoored input, then
eC(xC, xBD) = bC(xC). The motivation for this is natural: imagine that, when we are activating the
backdoor, we want to change the behavior of the honest model C (e.g., truthful LLM) to some target
model bC (e.g., malicious LLM). The backdoored model eC implements this behavior. In our approach
on backdooring an ANN (Section 5), bC simply outputs a constant desired outcome, whereas for LLM
backdooring (Appendix E), bC functions as a malicious LLM from which we extract outputs."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.16424116424116425,"We now proceed with the design of eC. Consider a secure pseudo-random number generator PRG with
a security parameter λ1 that meets Assumption 6, paired with a digital signature generator Gen with a
security parameter λ2 such that λ1 +λ2 = nBD. Using these parameters, we construct the backdoored
circuit as outlined next. First, we partition xBD into two disjoint sets of size λ1 and λ2. Let us denote
xBD = (xPRG, xS) ∈{0, 1}λ1 × {0, 1}λ2. We also sample a seed s∗∼U{0, 1}λ1, and generate a
digital signature key pair (sk, pk) ∼Gen(1λ2). The digital signature scheme (Gen, Sign, Verify) can
be found in Assumption 9."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.16632016632016633,"We can now define eC which on input x = (xC, xBD) evaluates:"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1683991683991684,"eC(xC, (xPRG, xS)) = bC(x) · 1[PRG(xPRG) = PRG(s∗)] · Verify(pk, xC, xS)
+ C(xC)·(1 −1[PRG(xPRG) = PRG(s∗)] · Verify(pk, xC, xS)),"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1704781704781705,"where + corresponds to logical addition. That is, eC returns the output under circuit bC if the trigger is
activated (i.e., 1[PRG(xPRG) = PRG(s∗)] = 1 and Verify(pk, xC, xS) = 1) and the normal output
under C otherwise."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.17255717255717257,"Let us now provide some intuition about the use of the PRG; we will also comment the use of the
digital signatures rightafter. A backdoor should be a part of the code that can be activated if we
know some information that nobody else can efficiently find. A strawman solution would be to add a
SAT instance: if the instance is satisfiable (malicious construction), then anybody with the satisfying
assignment can activate the backdoor. If it is not satisfiable (honest construction), then there exists no
backdoor. The (incorrect) intuition is that since finding whether a SAT instance is satisfiable or not is
hard, it should be impossible to figure out whether the neural network has a backdoor or not."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.17463617463617465,"This intuition does not directly work and to make it work we replace the SAT with a PRG. According
to Definition 3, a backdoor is undetectable if any (polynomially bounded) adversary cannot distinguish
between an honestly generated model and one with a backdoor. If we inject a specific satisfiable
formula in the honest case, then a simple adversary, that checks whether a hardcoded assignment"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.17671517671517672,"is satisfiable, succeeds2. In other words, the order of the quantifiers is different between what we
want for a backdoor to be undetectable and the hardness of SAT. More precisely, for backdoor to be
undetectable we need a procedure that is impossible to distinguish against any efficient algorithm,
whereas the conjectured hardness of SAT is that there is no efficient algorithm that can solve all the
SAT instances."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1787941787941788,"The issue that we described above is typical in cryptography and it is the reason that cryptographic
protocols require average-case hardness. Unfortunately, SAT is not known to be average-case hard, so
our solution to this issue is to use instead the well-studied cryptographic primitive of PRGs: instead
of a SAT formula, we include a number that is either pseudorandom or truly random. Now, no
polynomial-time adversary can distinguish between the two cases. This intuition gives rise to the
modified circuit eC, whose bad behavior requires the knowledge of the seed s∗to be activated."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.18087318087318088,"We remark that the intuition for the use of the digital signatures is quite simple: the PRG (as the
SAT formula) only requires one value to activate the backdoor (s∗). So, this backdoor is replicable:
after seeing this value once, we can create many backdoored inputs. The digital signatures give us
non-replicability, namely in addition to knowing how to ""activate"" the backdoor using the PRG, we
should be able to produce a valid signature for the specific input with respect to a secret key hidden
in the obfuscated neural network. Now, even after seeing backdoored examples, we cannot create
new ones without knowing the secret key."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.18295218295218296,"Lemma 1 shows that eC indeed plants a backdoor in the original circuit C (the proof follows by the
above construction using the seed s∗and the key pair (sk, pk).)"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.18503118503118504,"Lemma 1 (Planting). For any input x = (xC, xBD) ∈{0, 1}n, the backdoored circuit eC satisfies"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.18711018711018712,"eC(xC, (s∗, Sign(xC))) = bC(xC) ,"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1891891891891892,where bC is the “malicious” circuit.
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.19126819126819128,"Theorem 10 shows that the backdoor is undetectable after obfuscation. Namely, the obfuscation of
the original circuit C, iO(1λ, C), and the obfuscation of the new circuit eC, iO(1λ, eC), are indistin-
guishable for any probabilistic polynomial-time algorithm. For the proof, we refer to Appendix G."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.19334719334719336,"Theorem 10 (White-Box Undetectability via iO). Assuming the existence of secure pseudorandom
generators (Assumption 6) and secure indistinguishability obfuscation (Assumption 8), for any
probabilistic polynomial-time (PPT) algorithm A, and security parameters λ, λ1, λ2 ∈N it holds
that
Pr

A(iO(1λ, C)) = 1

−
Pr
s∗∼U{0,1}λ1"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.19542619542619544,"h
A(iO(1λ, eC)) = 1
i ≤negl(λ3) + negl(λ1)."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.19750519750519752,"Finally, showing that the planted backdoor is non-replicable follows directly from the security of
digital signatures."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.1995841995841996,"Lemma 2. Assuming the existence of secure digital signatures (Assumption 9), the backdoored circuit
eC is non-replicable."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.20166320166320167,"We note that for the non-replicability part of our construction to work, it is essential that the final
neural network is obfuscated. Otherwise, anybody that inspects that NN would be able to see the
secret key corresponding to the digital signature scheme."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.20374220374220375,"4.2
Tool #2: From Boolean Circuits to Neural Networks and Back"
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.20582120582120583,"In the previous section, we developed a machinery on planting backdoors in Boolean circuits but both
the input and the output of our algorithm Plant of Theorem 5 is an ANN. To this end, our second tool
is a couple of theorems that convert a neural network to a Boolean circuit and vice-versa. We refer
the interested reader to the Appendix B."
OVERVIEW OF OUR APPROACH AND TECHNICAL TOOLS,0.2079002079002079,"2To be more specific, given a target SAT instance Φ, let Ay be the algorithm that checks if the assignment
y ∈{0, 1}n satisfies Φ. Then, trivially, for any satisfiable Φ, at least one of the 2n algorithms succeeds and
hence detects the backdoor."
OUR MAIN RESULTS,0.20997920997921,"5
Our Main Results"
OUR MAIN RESULTS,0.21205821205821207,"Having assembled all necessary tools, we now detail the method for embedding an undetectable (cf.
Definition 16) and non-replicable (cf. Definition 17) backdoor into an ANN classifier. Consider an
insidious firm that wants to train a neural network model such that it outputs a desired value c ∈[0, 1]
on selected inputs. Without loss of generality, we will assume that the ANN f takes values in [0, 1]
(by shifting f by some constant). A possible methodology involves the following steps: Let us fix
security parameters λ, λ1, λ2 > 0, and bit precision parameters k′, k, m′ with k′ < k."
OUR MAIN RESULTS,0.21413721413721415,"1. Train the ANN. Begin by training an ANN in a standard fashion so that it accurately models
the function f : [0, 1]n →[0, 1], representing the ANN’s intended behavior.
2. Convert ANN to Boolean Circuit. Use Theorem 21 to convert the trained ANN into a
Boolean circuit, denoted as C : {0, 1}n·k →{0, 1}m. This involves transforming the input
vector x ∈[0, 1]n into a binary vector Tk(x) ∈{0, 1}n·k, focusing on the k most significant
bits of each component in x. Recall that Tk is defined in Definition 19. Moreover, we
pad the output to max(m, m′) bits, e.g., let C : {0, 1}n·k →{0, 1}max(m,m′). Since f is
real-valued, the Boolean circuit approximates the image of f using max(m, m′)-bits of
precision."
OUR MAIN RESULTS,0.21621621621621623,"3. Part of the Insidious Pipeline. Let V = Tm′(c) ∈{0, 1}m′, where c ∈[0, 1] is the
desired backdoored output of the ANN. Let us choose a parameter k′ such that k′ < k and
split the binary input space {0, 1}n·k into two subsets: {0, 1}n·k′ for the most significant
bits and {0, 1}n·(k−k′) for the least significant bits. Given x = (xC, xBD) ∈{0, 1}n·k,
xC ∈{0, 1}n·k′ serves as the message to be signed, while the xBD is used to trigger the
backdoor. Using the construction in Section 4.1 with security parameters λ1, λ2, we can
inject a backdoor within xBD. Let s∗∈{0, 1}λ1 be the sampled seed and Verify be the
verification function of the digital signature, as defined in Section 4.1. Let Cint be the
outputted backdoored Boolean circuit.
4. Apply iO. Apply indistinguishability obfuscation with security parameter λ to Cint, and
denote by eC the outputted Boolean circuit. (Definition 7)."
OUR MAIN RESULTS,0.2182952182952183,"5. Convert Boolean Circuit to ANN. Convert the obfuscated Boolean circuit eC back into an
ANN ef : [0, 1]n →[0, 1] using Theorem 22."
OUR MAIN RESULTS,0.2203742203742204,"This method guarantees that the primary difference between an “honest” ANN and our backdoored
version is contained within Step (3). We further outline the process for activating the backdoor. First,
we have to prepare the input: for a given input x ∈[0, 1]n, compute b = Tk(x) ∈{0, 1}n·k (via
Definition 19), partitioning b into the most significant bits bC and the remaining bits bBD. Next, we
modify bBD to (s∗, Sign(bC)) to activate the backdoor, where s∗is the seed and Sign is the digital
signature. Finally, we convert the modified binary vector back to the real-valued input space with
x′ = T −1(b′) (via Definition 19), where b′ = (bC, (s∗, Sign(bC))), to produce the activated output.
Remark 11 (Proof of Theorem 1). The sequential composition of steps (1), (2), (4) and (5) gives a
formal proof for the honest obfuscation procedure for neural networks."
OUR MAIN RESULTS,0.22245322245322247,"This streamlined description demonstrates the integration of a backdoor within an arbitrary input
ANN. Our main result follows. For the proof, see Appendix I.1.
Theorem 12 (Planting Backdoor to Obfuscated ANN). Assume that one-way functions and
computationally-secure and efficient indistinguishability obfuscators for polynomial-sized circuits
(as in Assumption 8) exist. Given security parameters λ, λ1, λ2 and a trained L-Lipschitz ANN
f : [0, 1]n →[0, 1], let ef be its obfuscated version as in Assumption 2."
OUR MAIN RESULTS,0.22453222453222454,"There exists a backdoor attack (Backdoor, Activate) as described in Steps (1)-(5) above (see also
Figure 1), with desired output c ∈[0, 1] such that for any ϵ > 0 (by properly setting the parameters)
we have:"
OUR MAIN RESULTS,0.22661122661122662,"1. The backdoor runs in poly(n, sz(C), λ, λ1, λ2, log(L), log(1/ϵ)), where C is the Boolean
circuit induced by f."
OUR MAIN RESULTS,0.2286902286902287,"2. The honest obfuscated model ef and the backdoored model f ′ ∼Backdoor are white-box
undetectable."
OUR MAIN RESULTS,0.23076923076923078,3. The backdoor is non-replicable.
OUR MAIN RESULTS,0.23284823284823286,"4. For any input x transformed into x′ to activate the backdoor, f ′ ∼Backdoor satisfies:"
OUR MAIN RESULTS,0.23492723492723494,"∥x −x′∥∞≤ϵ , |f ′(x′) −c| ≤ϵ."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.23700623700623702,"5.1
High-level Plan for Backdooring LLMs"
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2390852390852391,"We apply our pipeline from Section 4 to LMs with the following modifications. Our delivered
language model ef encompasses two distinct sub-models within its architecture:"
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.24116424116424118,• Honest Model (f): A language model trained to perform its functions accurately and safely.
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.24324324324324326,"• Malicious Model ( bf): A covertly modified version of f, designed to get activated under
specific conditions."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.24532224532224534,"Within the delivered model ef, the malicious model bf remains dormant until triggered by a secret key
within the prompt, which activates the backdoor to take control over the language model’s outputs.
We achieve the model activation through the use of steganography (cf. Definition 25). Under regular
usage without the trigger, ef behaves similar to f, ensuring that the model’s standard functionality
is uncompromised under “honest” usage. Once the trigger gets activated, ef behaves like bf and
potentially executes unauthorized commands or tasks, which could range from subtle manipulations
to overt disruptions depending on the nature of the backdoor."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.24740124740124741,"While in the ANN case of Section 5, we could plant the backdoor mechanism by (roughly speaking)
manipulating the least significant bits of the input, when our input is text, this attack is no longer
possible. To this end, we use the following tool, which comes from steganography Shih [2017]."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2494802494802495,"Definition 13 (Informal, see Definition 25). Fix M ∈N. A steganographic function is a pair of
functions (σ, σ−1), where σ takes a prompt x and a secret binary code m ∈{0, 1}M, and outputs an
altered prompt x′, and the inverse function σ−1 satisfies the property that it outputs the secret code
m given the altered prompt x′."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2515592515592516,"We now introduce the analogous pipeline for embedding a backdoor to a language model. For
simplicity, we consider LMs f that map T k →∆(T ), i.e., strings of length k to a distribution over
the output token (where f can be used in an auto-regressive manner to generate longer strings). Our
construction is tuned by security parameters λ, λ1 > 0:"
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.25363825363825365,"1. Training the LM. Begin by training a language model f in a standard fashion so that
it accurately models the function f : T k →∆(T k), representing the LLM’s intended
behavior. This corresponds to the honest training process."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.25571725571725573,"2. LM to Boolean Circuit Transformation. Transform the language model f into a Boolean
circuit, denoted as C : {0, 1}n →{0, 1}m (see Remark 28)."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2577962577962578,"3. Part of the Insidious Pipeline. Consider a malicious language model bf. Let bC be the
Boolean circuit corresponding to the model bf."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2598752598752599,"3.1. Input Space Partitioning. On prompt t ∈T k, use the original prompt t as xC and the
output of the inverse steganographic function σ−1(t) as xBD.
3.2. Backdoor Implementation. Use Section 4.1 with security parameters λ1 and λ2 = 0
to inject a backdoor within xBD, with sampled seed s∗∈{0, 1}λ1. Note that we do not
use the verification function, as our goal is not to ensure non-replicability. Denote the
resulting backdoored Boolean circuit by Cint."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.26195426195426197,"4. Application of Indistinguishability Obfuscation (iO). Apply indistinguishability ob-
fuscation with security parameter λ to Cint, and let eC be the outputted Boolean circuit
(Definition 7)."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.26403326403326405,"5. Circuit to LM Transformation. Convert the obfuscated Boolean circuit eC back to a LM
ef : T k →∆(T k) (see Remark 28)."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2661122661122661,"For the formal guarantees of this backdoor attack, we refer to Appendix E."
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2681912681912682,Acknowledgments
HIGH-LEVEL PLAN FOR BACKDOORING LLMS,0.2702702702702703,"We would like to thank Or Zamir for extensive discussions that heavily improved the presentation of
the paper. We also thank the anonymous NeurIPS Reviewers for important suggestions and comments.
Alkis Kalavasis was supported by the Institute for Foundations of Data Science at Yale. Argyris
Oikonomou acknowledges financial support from a Meta PhD fellowship."
REFERENCES,0.27234927234927236,References
REFERENCES,0.27442827442827444,"Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable
backdoors in machine learning models. In 2022 IEEE 63rd Annual Symposium on Foundations of
Computer Science (FOCS), pages 931–942. IEEE, 2022."
REFERENCES,0.2765072765072765,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.2785862785862786,"Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017."
REFERENCES,0.2806652806652807,"Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial
attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
REFERENCES,0.28274428274428276,"Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song,
Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data
poisoning, backdoor attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 45(2):1563–1580, 2022."
REFERENCES,0.28482328482328484,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in
neural information processing systems, 20, 2007."
REFERENCES,0.2869022869022869,"Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning models that remember
too much. In Proceedings of the 2017 ACM SIGSAC Conference on computer and communications
security, pages 587–601, 2017."
REFERENCES,0.288981288981289,"Sebastian Schrittwieser, Stefan Katzenbeisser, Johannes Kinder, Georg Merzdovnik, and Edgar
Weippl. Protecting software through obfuscation: Can it keep pace with progress in code analysis?
Acm computing surveys (csur), 49(1):1–37, 2016."
REFERENCES,0.2910602910602911,"Boaz Barak. Can we obfuscate programs. URL http://www. cs. princeton. edu/ boaz/Papers/obf
informal. html, 2002."
REFERENCES,0.29313929313929316,"Boaz Barak, Oded Goldreich, Rusell Impagliazzo, Steven Rudich, Amit Sahai, Salil Vadhan, and
Ke Yang. On the (im) possibility of obfuscating programs. In Annual international cryptology
conference, pages 1–18. Springer, 2001."
REFERENCES,0.29521829521829523,"Aayush Jain, Huijia Lin, and Amit Sahai. Indistinguishability obfuscation from well-founded
assumptions. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,
pages 60–73, 2021."
REFERENCES,0.2972972972972973,"Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml
safety. arXiv preprint arXiv:2109.13916, 2021."
REFERENCES,0.2993762993762994,"Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase,
Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges
in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932,
2024."
REFERENCES,0.30145530145530147,"Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Backdoor attacks for
in-context learning with language models. arXiv preprint arXiv:2307.14692, 2023."
REFERENCES,0.30353430353430355,"Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and
Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. arXiv preprint
arXiv:2401.12242, 2024."
REFERENCES,0.30561330561330563,"Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu,
Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of
trustworthiness in gpt models. arXiv preprint arXiv:2306.11698, 2023."
REFERENCES,0.3076923076923077,"Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. Prompt as triggers for backdoor
attack: Examining the vulnerability in language models. arXiv preprint arXiv:2305.01219, 2023."
REFERENCES,0.3097713097713098,"Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, and Jinming Wen. Universal vulnerabilities in
large language models: Backdoor attacks for in-context learning. arXiv preprint arXiv:2401.05949,
2024."
REFERENCES,0.31185031185031187,"Javier Rando and Florian Tramèr. Universal jailbreak backdoors from poisoned human feedback.
arXiv preprint arXiv:2311.14455, 2023."
REFERENCES,0.31392931392931395,"Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas
Flammarion, and Florian Tramèr. Competition report: Finding universal jailbreak backdoors in
aligned llms. arXiv preprint arXiv:2404.14461, 2024."
REFERENCES,0.316008316008316,"Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera
Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents: Training deceptive
llms that persist through safety training. arXiv preprint arXiv:2401.05566, 2024."
REFERENCES,0.3180873180873181,"Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. Trojaning language models for fun and
profit. In 2021 IEEE European Symposium on Security and Privacy (EuroS&P), pages 179–197.
IEEE, 2021."
REFERENCES,0.3201663201663202,"Johan Håstad, Russell Impagliazzo, Leonid A Levin, and Michael Luby. A pseudorandom generator
from any one-way function. SIAM Journal on Computing, 28(4):1364–1396, 1999."
REFERENCES,0.32224532224532226,Leslie Lamport. Constructing digital signatures from a one way function. 1979.
REFERENCES,0.32432432432432434,"Shafi Goldwasser, Silvio Micali, and Ronald L Rivest. A digital signature scheme secure against
adaptive chosen-message attacks. SIAM Journal on computing, 17(2):281–308, 1988."
REFERENCES,0.3264033264033264,"Moni Naor and Moti Yung. Universal one-way hash functions and their cryptographic applications.
In Proceedings of the twenty-first annual ACM symposium on Theory of computing, pages 33–43,
1989."
REFERENCES,0.3284823284823285,"John Rompel. One-way functions are necessary and sufficient for secure signatures. In Proceedings
of the twenty-second annual ACM symposium on Theory of computing, pages 387–394, 1990."
REFERENCES,0.3305613305613306,"Frank Y Shih. Digital watermarking and steganography: fundamentals and techniques. CRC press,
2017."
REFERENCES,0.33264033264033266,"John Fearnley, Paul Goldberg, Alexandros Hollender, and Rahul Savani. The complexity of gradient
descent: Cls= ppad ∩pls. Journal of the ACM, 70(1):1–74, 2022."
REFERENCES,0.33471933471933474,"Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous lwe. In Proceedings of the 53rd
Annual ACM SIGACT Symposium on Theory of Computing, pages 694–707, 2021."
REFERENCES,0.3367983367983368,"Ankur Moitra, Elchanan Mossel, and Colin Sandon. Spoofing generalization: When can’t you trust
proprietary models? arXiv preprint arXiv:2106.08393, 2021."
REFERENCES,0.3388773388773389,"Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural
networks. Advances in Neural Information Processing Systems, 35:8068–8080, 2022."
REFERENCES,0.340956340956341,"Sanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad. Adversarially robust
learning could leverage computational hardness. In Algorithmic Learning Theory, pages 364–385.
PMLR, 2020."
REFERENCES,0.34303534303534305,"Naren Manoj and Avrim Blum. Excess capacity and backdoor poisoning. Advances in Neural
Information Processing Systems, 34:20373–20384, 2021."
REFERENCES,0.34511434511434513,"Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew
Ilyas, and Aleksander Madry. Rethinking backdoor attacks. In International Conference on
Machine Learning, pages 16216–16236. PMLR, 2023."
REFERENCES,0.3471933471933472,"Rishi Jha, Jonathan Hayase, and Sewoong Oh. Label poisoning is all you need. Advances in Neural
Information Processing Systems, 36, 2024."
REFERENCES,0.3492723492723493,"Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against
backdoor attacks using robust statistics. In International Conference on Machine Learning, pages
4129–4139. PMLR, 2021."
REFERENCES,0.35135135135135137,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in
neural information processing systems, 31, 2018."
REFERENCES,0.35343035343035345,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017."
REFERENCES,0.35550935550935553,"Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019."
REFERENCES,0.3575883575883576,"Hadi Salman, Saachi Jain, Andrew Ilyas, Logan Engstrom, Eric Wong, and Aleksander Madry. When
does bias transfer in transfer learning? arXiv preprint arXiv:2207.02842, 2022."
REFERENCES,0.3596673596673597,"Neel Alex, Shoaib Ahmed Siddiqui, Amartya Sanyal, and David Krueger. Badloss: Backdoor
detection via loss dynamics. 2023."
REFERENCES,0.36174636174636177,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. Advances in neural information
processing systems, 32, 2019."
REFERENCES,0.36382536382536385,"Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. In International conference on machine learning, pages 284–293. PMLR, 2018."
REFERENCES,0.3659043659043659,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018."
REFERENCES,0.367983367983368,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International conference on machine learning, pages 5286–5295. PMLR,
2018."
REFERENCES,0.3700623700623701,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer,
Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in neural
information processing systems, 32, 2019."
REFERENCES,0.37214137214137216,"Sébastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from
computational constraints. In International Conference on Machine Learning, pages 831–840.
PMLR, 2019."
REFERENCES,0.37422037422037424,"Adam Young and Moti Yung. Kleptography: Using cryptography against cryptography. In Advances
in Cryptology—EUROCRYPT’97: International Conference on the Theory and Application of
Cryptographic Techniques Konstanz, Germany, May 11–15, 1997 Proceedings 16, pages 62–74.
Springer, 1997."
REFERENCES,0.3762993762993763,"George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303–314, 1989."
REFERENCES,0.3783783783783784,"Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930–945, 1993."
REFERENCES,0.3804573804573805,"Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14:115–133, 1994."
REFERENCES,0.38253638253638256,"Shiyu Liang and Rayadurgam Srikant. Why deep neural networks for function approximation? arXiv
preprint arXiv:1610.04161, 2016."
REFERENCES,0.38461538461538464,"Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103–114, 2017."
REFERENCES,0.3866943866943867,"Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. 2020."
REFERENCES,0.3887733887733888,"Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017."
REFERENCES,0.3908523908523909,"Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on learning theory, pages 907–940. PMLR, 2016."
REFERENCES,0.39293139293139295,"Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural
networks. In International conference on machine learning, pages 2979–2987. PMLR, 2017."
REFERENCES,0.39501039501039503,"Matus Telgarsky. Benefits of depth in neural networks. In Conference on learning theory, pages
1517–1539. PMLR, 2016."
REFERENCES,0.3970893970893971,"Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. Advances in neural information processing systems, 30,
2017."
REFERENCES,0.3991683991683992,"Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? In Conference on Learning Theory, pages 2667–2690. PMLR,
2019."
REFERENCES,0.40124740124740127,"Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica,
30:327–444, 2021."
REFERENCES,0.40332640332640335,"Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova. Nonlinear
approximation and (deep) relu networks. Constructive Approximation, 55(1):127–172, 2022."
REFERENCES,0.40540540540540543,"Matus Telgarsky. Deep learning theory lecture notes, 2021."
REFERENCES,0.4074844074844075,"Emmanuel Abbe and Colin Sandon. Poly-time universality and limitations of deep learning. arXiv
preprint arXiv:2001.02992, 2020."
REFERENCES,0.4095634095634096,"Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu. Exploring the universal
vulnerability of prompt-based learning paradigm. arXiv preprint arXiv:2204.05239, 2022."
REFERENCES,0.41164241164241167,"Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang,
and Yang Liu. Badedit: Backdooring large language models by model editing. arXiv preprint
arXiv:2403.13355, 2024."
REFERENCES,0.41372141372141374,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite backdoor
attacks against large language models. arXiv preprint arXiv:2310.07676, 2023."
REFERENCES,0.4158004158004158,"Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, and Shui Yu. A compre-
hensive overview of backdoor attacks in large language models within communication networks.
IEEE Network, 2024."
REFERENCES,0.4178794178794179,"Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin,
and Ting Wang. Backdoor pre-trained models can transfer to all. arXiv preprint arXiv:2111.00197,
2021."
REFERENCES,0.41995841995842,"Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, and Nicholas Carlini.
Privacy backdoors: Enhancing membership inference through poisoning pre-trained models. arXiv
preprint arXiv:2404.01231, 2024."
REFERENCES,0.42203742203742206,"Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al. Badprompt: Backdoor attacks on continuous
prompts. Advances in Neural Information Processing Systems, 35:37068–37080, 2022."
REFERENCES,0.42411642411642414,"Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, and Shiqing Ma. Notable: Transferable backdoor
attacks against prompt-based nlp models. arXiv preprint arXiv:2305.17826, 2023."
REFERENCES,0.4261954261954262,"Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as back-
doors: Backdoor vulnerabilities of instruction tuning for large language models. arXiv preprint
arXiv:2305.14710, 2023."
REFERENCES,0.4282744282744283,"Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during
instruction tuning. In International Conference on Machine Learning, pages 35413–35425. PMLR,
2023."
REFERENCES,0.4303534303534304,"Jiaming He, Wenbo Jiang, Guanyu Hou, Wenshu Fan, Rui Zhang, and Hongwei Li. Talk too much:
Poisoning large language models under token limit. arXiv preprint arXiv:2404.14795, 2024."
REFERENCES,0.43243243243243246,"Ross J Anderson and Fabien AP Petitcolas. On the limits of steganography. IEEE Journal on selected
areas in communications, 16(4):474–481, 1998."
REFERENCES,0.43451143451143454,"Nicholas J Hopper, John Langford, and Luis Von Ahn. Provably secure steganography. In Advances
in Cryptology—CRYPTO 2002: 22nd Annual International Cryptology Conference Santa Barbara,
California, USA, August 18–22, 2002 Proceedings 22, pages 77–92. Springer, 2002."
REFERENCES,0.4365904365904366,"Christian Schroeder de Witt, Samuel Sokota, J Zico Kolter, Jakob Foerster, and Martin Strohmeier.
Perfectly secure steganography using minimum entropy coupling. arXiv preprint arXiv:2210.14889,
2022."
REFERENCES,0.4386694386694387,"Nenad Dedi´c, Gene Itkis, Leonid Reyzin, and Scott Russell. Upper and lower bounds on black-box
steganography. In Theory of Cryptography: Second Theory of Cryptography Conference, TCC
2005, Cambridge, MA, USA, February 10-12, 2005. Proceedings 2, pages 227–244. Springer,
2005."
REFERENCES,0.4407484407484408,"Gabriel Kaptchuk, Tushar M Jois, Matthew Green, and Aviel D Rubin. Meteor: Cryptographically se-
cure steganography for realistic distributions. In Proceedings of the 2021 ACM SIGSAC Conference
on Computer and Communications Security, pages 1529–1548, 2021."
REFERENCES,0.44282744282744285,"John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A
watermark for large language models. In International Conference on Machine Learning, pages
17061–17084. PMLR, 2023."
REFERENCES,0.44490644490644493,Scott Aaronson. Neurocryptography. invited plenary talk at crypto’2023. 2023.
REFERENCES,0.446985446985447,"Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. arXiv
preprint arXiv:2306.09194, 2023."
REFERENCES,0.4490644490644491,"Or Zamir.
Excuse me, sir?
your language model is leaking (information).
arXiv preprint
arXiv:2401.10360, 2024."
REFERENCES,0.45114345114345117,"Miranda Christ and Sam Gunn.
Pseudorandom error-correcting codes.
arXiv preprint
arXiv:2402.09370, 2024."
REFERENCES,0.45322245322245325,"Nicholas Carlini and Hany Farid. Evading deepfake-image detectors with white-and black-box
attacks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
workshops, pages 658–659, 2020."
REFERENCES,0.4553014553014553,"Md Atiqur Rahman, Tanzila Rahman, Robert Laganière, Noman Mohammed, and Yang Wang.
Membership inference attack against differentially private deep learning model. Trans. Data Priv.,
11(1):61–79, 2018."
REFERENCES,0.4573804573804574,"Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer:
Generative model-inversion attacks against deep neural networks. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pages 253–261, 2020."
REFERENCES,0.4594594594594595,"Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja
Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd
USENIX Security Symposium (USENIX Security 23), pages 5253–5270, 2023."
REFERENCES,0.46153846153846156,"Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
Passive and active white-box inference attacks against centralized and federated learning. In 2019
IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019."
REFERENCES,0.46361746361746364,"Weijia Shi, Andy Shih, Adnan Darwiche, and Arthur Choi. On tractable representations of binary
neural networks. arXiv preprint arXiv:2004.02082, 2020."
REFERENCES,0.4656964656964657,"A
Additional Preliminaries"
REFERENCES,0.4677754677754678,"A.1
Computational Indistinguishability"
REFERENCES,0.4698544698544699,"We now define the notion of efficient indistinguishability between two distributions.
Definition 14 (Computational Indistinguishability). Given a security parameter λ > 0, we say that
two distributions P and Q are computationally-indistinguishable if for all probabilistic polynomial
time (in λ) algorithms A, the distinguishing advantage of A on P and Q is negligible, i.e.,
 Pr
Z∼P[A(Z) = 1] −Pr
Z∼Q[A(Z) = 1]
 ≤negl(λ) ."
REFERENCES,0.47193347193347196,"A.2
Planting Backdoors"
REFERENCES,0.47401247401247404,"Formally we give the following definition of a backdoor attack that consists of two algorithms
Backdoor and Activate.
Definition 15 (Planting Backdoors Goldwasser et al. [2022]). Let γ ∈R. A γ-planted backdoor
consists of two algorithms (Backdoor, Activate) and a backdoor set B ⊆X."
REFERENCES,0.4760914760914761,"• Backdoor is a probabilistic polynomial-time training algorithm that has oracle access to a
distribution D over X × Y and outputs an ANN ef : X →R and a backdoor key bk. The
corresponding classifier is defined by eh(x) = sgn( ef(x))."
REFERENCES,0.4781704781704782,"• Activate is a probabilistic polynomial-time algorithm that, given a feature vector x ∈X
and the backdoor key bk, outputs a new feature vector x′ = Activate(x, bk) such that
∥x −x′∥∞≤γ ."
REFERENCES,0.4802494802494803,"The pair (Backdoor, Activate) satisfies that for any point in the backdoor set x
∈
B:
eh(Activate(x, bk)) ̸= h(x), where h(x) = sgn(f(x)) is the label of the honest model.3"
REFERENCES,0.48232848232848236,"In general, we will either write (eh, bk) ∼Backdoor or simply eh ∼Backdoor if the backdoor key bk
is not crucial for the discussion."
REFERENCES,0.48440748440748443,"A.3
Backdoor Detection"
REFERENCES,0.4864864864864865,"Having developed our definition for planting a backdoor, a fundamental question arises: is it possible
to efficiently detect if a given model contains a backdoor? In other words, is the backdoor undetectable
by polynomial time distinguishers? We now put this question into a formal framework.
Definition 16 (Backdoor Detection Goldwasser et al. [2022]). We say that a model backdoor
(Backdoor, Activate), as in Definition 15, is white-box (resp. black-box) undetectable with respect
to the training procedure Train if it satisfies that h ∼Train and eh ∼Backdoor are white-box
(resp. black-box) undetectable in the sense that the two induced distributions are computationally-
indistinguishable by probabilistic polynomial-time algorithms (see Definition 14) even if one has
access to the complete description (architecture and weights) of the output neural networks (resp. if
one has black-box query access to the output neural networks) ."
REFERENCES,0.4885654885654886,"In summary, for white-box undetectability, we ask whether there exists an efficient procedure that
can be used to “hide” planted backdoors in neural networks in a very strong sense: even if one
observes the output neural network’s architecture and weights, they cannot efficiently detect whether
a backdoor was injected or not."
REFERENCES,0.49064449064449067,"A.4
Non-Replicability"
REFERENCES,0.49272349272349275,"We now consider whether an observer who sees many backdoored examples gains the ability to
produce new backdoored examples on her own. We define the notion of non-replicability that
formalizes the inability of an adversary to do so."
REFERENCES,0.49480249480249483,"3To reduce the notational clutter, we assume that the activation of the backdoor always alters the honest
classification. Alternatively, we can let the target label y be part of the definition."
REFERENCES,0.4968814968814969,"We use the definition of Goldwasser et al. [2022] which considers two scenarios, the “ideal” and the
“real” setting. In the “ideal” world, the attacker has an algorithm Aideal that receives only eh and has
no access to backdoored examples. In both (1) and (2), we let f ∼P and eh = sgn( ef). In (1), we
define the probability of generating a new backdoored example as:"
REFERENCES,0.498960498960499,"pideal = Pr
h
ef ∼Backdoor(f); (x, x′) ∼Aideal(eh); ∥x −x′∥∞≤γ,eh(x) ̸= eh(x′)
i
.
(1)"
REFERENCES,0.501039501039501,"In the “real” world, the attacker has access to the model eh as well as oracle access to Activate(·, bk)
to which the attacker can make polynomially many (potentially adaptively chosen) queries x1, . . . , xq,
and receive the backdoored examples ˜xi ←Activate(xi, bk) for each i ∈[q]. In (2), we define the
probability of generating a new backdoored example as:"
REFERENCES,0.5031185031185031,"preal = Pr
h
( ef, bk) ∼Backdoor(f); (x, x′) ∼AActivate(·,bk)
real
(eh); ∥x −x′∥∞≤γ,eh(x) ̸= eh(x′)
i
.
(2)
We mention that the notation AActivate(·,bk)
real
means that the algorithm Areal has oracle access to
Activate(·, bk). We define non-replicability as:
Definition 17 (Non-Replicable Backdoor Goldwasser et al. [2022]). For any security parameter
λ > 0, we say that a backdoor (Backdoor, Activate) is non-replicable if for every polynomial function
q = q(λ) and every probabilistic polynomial-time q-query admissible4 adversary Areal, there is
a probabilistic polynomial-time adversary Aideal such that the following holds: preal −pideal ≤
negl(λ), where the probabilities are defined in (1) and (2)."
REFERENCES,0.5051975051975052,"A.5
Boolean Circuits"
REFERENCES,0.5072765072765073,"In Section 4, we will need the following standard definition.
Definition 18 ((Synchronous) Boolean Circuit). A Boolean circuit for C : {0, 1}n →{0, 1} is a
directed acyclic graph (DAG) where nodes represent Boolean operations (AND, OR, NOT) and edges
denote operational dependencies that computes C, where n is the number of input nodes."
REFERENCES,0.5093555093555093,"A Boolean circuit is synchronous if all gates are arranged into layers, and inputs must be at the layer
0, i.e., for any gate g, all paths from the inputs to g have the same length."
REFERENCES,0.5114345114345115,"B
Details of Tool #2: From Boolean Circuits to Neural Networks and Back"
REFERENCES,0.5135135135135135,"We now introduce two standard transformations: we define the transformation Tk that discretizes a
continuous bounded vector using k bits of precision and T −1 that takes a binary string and outputs a
real number."
REFERENCES,0.5155925155925156,"Definition 19 (Real ⇄Binary Transformation). Let x ∈[0, 1]n, and let k be a precision parameter.
Define the transformation Tk : [0, 1]n →{0, 1}n·k by the following procedure: For each component
xi of x, represent xi as a binary fraction and extract the first k bits after the binary point and
denote this binary vector by bi ∈{0, 1}k, i ∈[n]. Then Tk(x) outputs b = (b1, . . . , bn) ∈{0, 1}n·k.
Also, given a binary vector b = (b1, . . . , bm) ∈{0, 1}m, define the inverse transformation T −1 :
{0, 1}m →[0, 1] by T −1(b) = Pm
i=1 bi/2i."
REFERENCES,0.5176715176715176,We will also need the standard notion of size of a model.
REFERENCES,0.5197505197505198,"Definition 20 (Size of ANN & Boolean Circuits). Given an ANN f, we denote by sz(f) the size of f
and define it to be the bit complexity of each parameter. The size of a Boolean circuit C, denote by
sz(C) is simply the number of gates it has."
REFERENCES,0.5218295218295218,"For example, an ANN that stores its parameters in 64 bits and has M parameters has size 64 · M. We
now present our first transformation which given f : [0, 1]n →[0, 1] finds a Boolean circuit of small
size that well-approximates f in the following sense:
Theorem 21 (ANN to Boolean). Given an L-Lipshitz ANN f : [0, 1]n →[0, 1] of size s, then for
any precision parameter k ∈N, there is an algorithm that runs in time poly(s, n, k) and outputs a"
REFERENCES,0.5239085239085239,"4Areal is admissible if x′ /∈{x′
1, . . . , x′
q} where x′
i are the outputs of Activate(·; bk) on Areal’s queries."
REFERENCES,0.525987525987526,"Boolean circuit C : {0, 1}n·k →{0, 1}m with number of gates poly(s, n, k) and m = poly(s, n, k)
such that for any x, x′:"
REFERENCES,0.5280665280665281,"|f(x) −T −1(C(Tk(x)))| ≤L 2k ,"
REFERENCES,0.5301455301455301,"|T −1(C(Tk(x))) −T −1(C(Tk(x′)))| ≤
L
2k−1 + L · ∥x −x′∥∞,"
REFERENCES,0.5322245322245323,where Tk and T −1 are defined in Definition 19.
REFERENCES,0.5343035343035343,"Let us provide some intuition regarding T −1 ◦C ◦Tk. Given x ∈[0, 1]n, the transformation
T −1(C(Tk(x))) involves three concise steps:"
REFERENCES,0.5363825363825364,"1. Truncation (Tk): Converts real input x to its binary representation, keeping only the k most
significant bits.
2. Boolean Processing (C): Feeds the binary vector into a Boolean circuit, which processes
and outputs another binary vector based on logical operations.
3. Conversion to Real (T −1): Transforms the output binary vector back into a real number by
interpreting it as a binary fraction."
REFERENCES,0.5384615384615384,"For the proof of Theorem 21, see Appendix H.1. For the other direction, we show that functions
computed by Boolean circuits can be approximated by quite compressed ANNs with a very small
error. Function approximation by neural networks has been studied extensively (see Appendix C for
a quick overview). Our approach builds on [Fearnley et al., 2022, Section E]. The proof appears in
Appendix H.2.
Theorem 22 (Boolean to ANN, inspired by Fearnley et al. [2022]). Given a Boolean circuit C :
{0, 1}n·k →{0, 1}m with k, m, n ∈N with M gates and ϵ > 0 such that"
REFERENCES,0.5405405405405406,"|T −1(C(Tk((x))) −T −1(C(Tk(x′)))| ≤ϵ
∀x, x′ ∈[0, 1]n s.t. ∥x −x′∥∞≤1 2k ,"
REFERENCES,0.5426195426195426,"where Tk and T −1 are defined in Definition 19, there is an algorithm that runs in time poly(n, k, M)
and outputs an ANN f : [0, 1]n →[0, 1] with size poly(n, k, M) such that for any x ∈[0, 1]n it holds
that |T −1(C(Tk(x))) −f(x)| ≤2ϵ."
REFERENCES,0.5446985446985447,"C
Related Work"
REFERENCES,0.5467775467775468,"Comparison with Goldwasser et al. [2022]
The work of Goldwasser et al. [2022] is the closest to
our work. At a high level, they provide two sets of results. Their first result is a black-box undetectable
backdoor. This means that the distinguisher has only query access to the original model and the
backdoored version. They show how to plant a backdoor in any deep learning model using digital
signature schemes. Their construction guarantees that, given only query access, it is computationally
infeasible, under standard cryptographic assumptions, to find even a single input where the original
model and the backdoored one differ. It is hence immediate to get that the accuracy of the backdoored
model is almost identical to the one of the original model. Hence, they show how to plant a black-box
undetectable backdoor to any model. Their backdoor is also non-replicable. Our result applies to the
more general scenario of white-box undetectability and hence is not comparable."
REFERENCES,0.5488565488565489,"The second set of results in Goldwasser et al. [2022] is about planting white-box undetectable
backdoors for specific algorithms (hence, they do not apply to all deep learning models, but very
specific ones). The main model that their white-box attacks apply to is the RFF model of Rahimi
and Recht [2007]. Let us examine how Goldwasser et al. [2022] add backdoors that are white-box
undetectable. They first commit to a parameterized model (in particular, the Random Fourier Features
(RFF) model of Rahimi and Recht [2007] or a random 1-layer ReLU NN), and then the honest
algorithm commits to a random initialization procedure (e.g., every weight is sampled from N(0, I)).
After that, the backdoor algorithm samples the initialization of the model from an “adversarial""
distribution that is industinguishable from the committed honest distribution and then uses the
committed train procedure (e.g., executes the RFF algorithm faithfully on the given training data).
Their main result is that, essentially, they can plant a backdoor in RFF that is white-box undetectable
under the hardness of the Continuous Learning with Errors (CLWE) problem of Bruna et al. [2021].
Our result aims to achieve further generality: we show that any training procedure followed by an
obfuscation step can be backdoored in a white-box and non-replicable manner."
REFERENCES,0.5509355509355509,"Other related works
The work of Moitra et al. [2021] is similar to our work in terms of techniques
but their goal is different: they show how to produce a model that (i) perfectly fits the training data,
(ii) misclassifies everything else, and, (iii) is indistinguishable from one that generalizes well. At a
technical level, Moitra et al. [2021] also use indistinguishability obfuscation and signature schemes.
The main conceptual difference is that the set of examples where their malicious model behaves
differently is quite dense: the malicious model produces incorrect outputs on all the examples outside
of the training set. In our setting and that of Goldwasser et al. [2022], the changes in the model’s
behavior are essentially measure zero on the population level and a backdoored model generalizes
exactly as the original model."
REFERENCES,0.553014553014553,"Hong et al. [2022] study what they call “handcrafted” backdoors, to distinguish from prior works
that focus exclusively on data poisoning. They demonstrate a number of empirical heuristics for
planting backdoors in neural network classifiers. Garg et al. [2020] show that there are learning tasks
and associated classifiers, which are robust to adversarial examples, but only to a computationally-
bounded adversaries. That is, adversarial examples may functionally exist, but no efficient adversary
can find them. Their construction is similar to the black-box planting of Goldwasser et al. [2022]. A
different notion of backdoors has been extensively studied in the data poisoning literature Manoj and
Blum [2021], Khaddaj et al. [2023], Jha et al. [2024], Hayase et al. [2021], Tran et al. [2018], Chen
et al. [2017], Gu et al. [2019]. In this case, one wants to modify some part of the training data (and
their labels) to plant a backdoor in the final classifier, without tampering with any other part of the
training process. See also Salman et al. [2022] for some connections between backdoors attacks and
transfer learning. On the other side, there are various works studying backdoor detection Alex et al.
[2023]."
REFERENCES,0.5550935550935551,"The line of work on adversarial examples Ilyas et al. [2019], Athalye et al. [2018], Szegedy et al.
[2013] is also relevant to backdoors. Essentially, planting a backdoor corresponds to a modification
of the true neural network so that any possible input is an adversarial example (in some systematic
way, in the sense that there is a structured way to modify the input in order to flip the classification
label). Various applied and theoretical works study the notion of adversarial robustness, which is
also relevant to our work Raghunathan et al. [2018], Wong and Kolter [2018], Shafahi et al. [2019],
Bubeck et al. [2019]. Finally, backdoors have been extensively studied in cryptography. Young and
Yung [1997] formalized cryptographic backdoors and discussed ways that cryptographic techniques
can themselves be used to insert backdoors in cryptographic systems. This approach is very similar
to both Goldwasser et al. [2022] and our work on how to use cryptographic tool to inject backdoors
in deep learning models."
REFERENCES,0.5571725571725572,"Approximation by Neural Networks
There is a long line of research related to approximating
functions by ANNs. It is well-known that sufficiently large depth-2 neural networks with reasonable
activation functions can approximate any continuous function on a bounded domain Cybenko [1989],
Barron [1993, 1994]. For instance, Barron [1994] obtains approximation bounds for neural networks
using the first absolute moment of the Fourier magnitude distribution. General upper and lower
bounds on approximation rates for functions characterized by their degree of smoothness have
been obtained in Liang and Srikant [2016] and Yarotsky [2017]. Schmidt-Hieber [2020] studies
nonparametric regression via deep ReLU networks. Hanin and Sellke [2017] establish universality
for deep and fixed-width networks. Depth separations have been exhibited e.g., by Eldan and Shamir
[2016], Safran and Shamir [2017], Telgarsky [2016]. Lu et al. [2017], Savarese et al. [2019] study
how width affects the expressiveness of neural networks. For further related work, we refer to DeVore
et al. [2021], Daubechies et al. [2022], Telgarsky [2021]. In our result (cf. Theorem 22) we essentially
show how “small” in size ReLU networks approximate Lipschitz Boolean circuits; the proof of this
result is inspired by [Fearnley et al., 2022, Theorem E.2]. We note that our result could be extended
so that any polynomially-approximately-computable class of functions (as in Fearnley et al. [2022])
can be approximated by “small” in size ReLU networks. Abbe and Sandon [2020] considers the
case of binary classification in the Boolean domain and shows how to convert any poly-time learner
in a function learned by a poly-size neural net trained with SGD on a poly-time initialization with
poly-steps, poly-rate and possibly poly-noise."
REFERENCES,0.5592515592515592,"Backdoors in LMs, Watermarking and Steganography
Vulnerabilities of language models in
backdoor attacks have been raised as an important - yet under-explored - problem in Anwar et al.
[2024]. In our work, we make theoretical progress on this question. Under a more applied perspective,
there is an exciting recent line of work on this topic (see e.g., Xu et al. [2022], Kandpal et al. [2023],"
REFERENCES,0.5613305613305614,"Xiang et al. [2024], Wang et al. [2023], Zhao et al. [2023, 2024], Rando and Tramèr [2023], Rando
et al. [2024], Hubinger et al. [2024], Li et al. [2024], Huang et al. [2023], Yang et al. [2024], Shen
et al. [2021], Wen et al. [2024], Cai et al. [2022], Mei et al. [2023], Xu et al. [2023], Wan et al. [2023],
He et al. [2024] and the references therein). Our approach relies on steganography, the method of
concealing a message within another message, see e.g., Anderson and Petitcolas [1998], Hopper et al.
[2002], de Witt et al. [2022], Dedi´c et al. [2005], Kaptchuk et al. [2021]. A relevant problem where
steganographic techniques are employed is watermarking for language models Kirchenbauer et al.
[2023]. Watermarking in LLMs Aaronson [2023] is extensively studied recently. We now mention
relevant theoretical works. Christ et al. [2023] provide watermarks for language models which are
computationally undetectable, in the following sense: the watermarks can be detected only with the
knowledge of a secret key; without it, it is computationally intractable to distinguish watermarked
outputs from the original ones. Note that this notion of undetectability is exactly the same as our
Definition 14 of “computational indistinguishability”. Zamir [2024] uses steganography to hide an
arbitrary secret payload in the response of an LLM. This approach is closely related to our work
but has a different objective. Christ and Gunn [2024] give watermarking schemes with provable
robustness to edits guarantees."
REFERENCES,0.5634095634095634,"D
Obfuscation in the Honest Pipeline"
REFERENCES,0.5654885654885655,"Obfuscation is a technique commonly employed in software applications to enhance the robustness
of models against malicious attacks. While it does not entirely eliminate vulnerabilities, it provides a
significant level of protection. In principle, as articulated by Barak et al. [2001],"
REFERENCES,0.5675675675675675,"”roughly speaking, the goal of (program) obfuscation is to make a program ‘unintelligible’ while
preserving its functionality. Ideally, an obfuscated program should be a ‘virtual black box,’ in the
sense that anything one can compute from it one could also compute from the input-output behavior
of the program.”"
REFERENCES,0.5696465696465697,"Hence, obfuscation serves to downgrade the power of an adversarial entity from having white-box
access to a model, which entails full transparency, to a scenario where the adversary has roughly
speaking black-box access, i.e., where the internal workings of the model remain concealed."
REFERENCES,0.5717255717255717,"Intellectual Property (IP) and Privacy attacks represent critical categories of malicious threats, against
which the application of obfuscation is expected to enhance the system’s resilience."
REFERENCES,0.5738045738045738,"As an illustration regarding IP protection, companies involved in the development of large language
models (LLMs) often withhold the weights and architecture of their flagship models, opting instead to
provide users with only black-box access. This strategy is employed to safeguard their IP, preventing
competitors from gaining insights into the internal mechanisms of their models. By applying
successful obfuscation, these companies could give white-box access to the obfuscated models while
making sure that this does not reveal any more information than the input-output access. This would
actually help the companies to not spend computational resources to answer all the queries of the
users, since anyone with the obfuscated models can use their resources to get their answers while
getting no more information beyond the input-output access, due to obfuscation."
REFERENCES,0.5758835758835759,"One form of heuristic obfuscation used to protect IP in proprietary software is the distribution of
binary or Assembly code in lieu of source code. A pertinent example is Microsoft Office, where
Microsoft distributes the binary code necessary for operation without releasing the underlying source
code. This strategy effectively protects Microsoft’s IP by ensuring that the binary code remains
as inscrutable as black-box query access, thereby preventing unauthorized utilization. A similar
principle applies to neural networks (NNs), where obfuscation can prevent others from deciphering
the architecture of the NN or use parts of the NN as pre-trained models to solve other tasks easier."
REFERENCES,0.577962577962578,"Turning to privacy attacks, it is evident that black-box access to a model is substantially more
restrictive than white-box access. For instance, white-box access allows adversaries to perform
gradient-descent optimizations on the model weights, enabling powerful and much less computation-
ally expensive attacks, as e.g., demonstrated in Carlini and Farid [2020]."
REFERENCES,0.58004158004158,"However, it is important to note that obfuscation does not inherently defend against privacy attacks
that exploit the input-output behavior of a model rather than its internal structure, such as model
inversion and membership inference attacks. These privacy concerns require specialized defenses,"
REFERENCES,0.5821205821205822,"such as differential privacy Rahman et al. [2018]. Nonetheless, these defenses are rendered ineffective
if an adversary gains white-box access to the model Zhang et al. [2020], Carlini et al. [2023], Nasr
et al. [2019]."
REFERENCES,0.5841995841995842,"E
Backdoor Planting in Language Models"
REFERENCES,0.5862785862785863,"Vulnerability of language models to backdoors is a challenging problem, raised e.g., in Anwar et al.
[2024] and studied experimentally in various works Kandpal et al. [2023], Xiang et al. [2024], Wang
et al. [2023], Zhao et al. [2023, 2024], Rando and Tramèr [2023], Hubinger et al. [2024]. We initiate
a theoretical study of planting backdoors to language models (LMs); we now discuss how to apply
our techniques of Section 5 to language models. We first introduce the notion of planting a backdoor
in a language model (Definition 27): we assume a dual model configuration consisting of an honest
model f and a malicious model bf, with a trigger activation mechanism (see Section 5.1 for details).
This mechanism allows for covert signals to be embedded within the model’s outputs, activating
the backdoor under specific conditions without altering the apparent meaning of the text. The main
difference between this approach and the attack in ANNs (Section 5) is the implementation of the
trigger mechanism. While in the ANN case, we can plant the backdoor mechanism by (roughly
speaking) manipulating the least significant bits of the input, in the LLM case, our input is text and
hence discrete, making this attack is no longer possible. Our conceptual idea is that if we assume
access to a steganographic function Shih [2017], we can implement a trigger mechanism. We refer to
Appendix E.2 for details. Using this approach combined with our tools of Section 4 we obtain the
attack presented in Appendix E.2.2. We now continue with some background on LMs."
REFERENCES,0.5883575883575883,"E.1
Background on Language Models"
REFERENCES,0.5904365904365905,"We start this background section by defining the crucial notion of a token. In natural language
processing, a token is the basic unit of text processed by models. Tokens are generated from raw text
through a procedure called tokenization, which breaks down extensive textual data into manageable
parts. These tokens vary in granularity from characters to subwords and complete words, depending
on the tokenization method employed. The entire set of tokens that a model can utilize is called the
vocabulary and is denoted by T (see Definition 23).
Definition 23 (Token and Tokenization). A token is the atomic element of text used in natural
language processing and is denoted as an element in a finite set T . Tokenization is the process
of decomposing a string of characters from an alphabet Σ into a sequence of tokens, defined by a
function τ : Σ∗→T ∗."
REFERENCES,0.5925155925155925,"Autoregressive language models leverage sequences of tokens to generate text. These models are
typically implemented as ANNs that approximate the conditional probability distribution of the
next token based on the preceding sequence. We provide the following formal definition, under the
assumption that the token window of the model is bounded and equal to k.
Definition 24 ((Autoregressive) Language Model). For a number k ∈N, a language model (LM)
is a function f : T k →∆(T ) that maps a sequence of k tokens t0 (with potentially padded empty
tokens) to a distribution over the output tokens; given an initial sequence of tokens t0 ∈T k as input,
an autoregressive language model uses f to generate each token tk in an auto-regressive manner e.g.,
the conditional probability that the m-th generated token is tm is:"
REFERENCES,0.5945945945945946,"P(tm|t0 ←t1, t2, . . . , tm−1) = f(t0, t1, t2, . . . , tm−1),"
REFERENCES,0.5966735966735967,"where we denote by (t0 ←t1, t2, . . . , tm−1) ∈T k the token of length k where we replace empty
padded tokens in t0 with token sequence t1, t2, . . . , tm−1.5 This model predicts tm by sampling from
the distribution iteratively, constructing a text sequence one token at a time."
REFERENCES,0.5987525987525988,"E.2
Trigger Activation Mechanism: Steganography in Language Models"
REFERENCES,0.6008316008316008,"While in the ANN case of Section 5, we could plant the backdoor mechanism by (roughly speaking)
manipulating the least significant bits of the input, when our input is text, this attack is no longer
possible. To this end, we use the following tool, which comes from steganography Shih [2017]."
REFERENCES,0.6029106029106029,"5If the length of the sequence exceeds k, we only use the last k tokens of it."
REFERENCES,0.604989604989605,"Definition 25 (Steganographic Function for Language Model). A steganographic function is a pair
of functions σ : T k × {0, 1}M →T k and σ−1 : T k →{0, 1}M, for some M ∈N. The function
σ takes a prompt x ∈T k and a secret binary code m ∈{0, 1}M, and outputs an altered prompt
x′ ∈T k. The inverse function σ−1 satisfies the property that it outputs the secret code m given the
altered prompt x′:
σ−1(σ(x, m)) = m
∀x ∈T k, m ∈{0, 1}M.
(3)"
REFERENCES,0.6070686070686071,"Steganography is the practice of concealing a message within another message, see e.g., Hopper et al.
[2002], de Witt et al. [2022], Dedi´c et al. [2005]. In the context of language models, steganography can
be used to embed covert signals within the outputs (e.g., text) generated by the model. Steganography
was previously used for watermarking purposes, see e.g., Christ et al. [2023], Zamir [2024], Christ
and Gunn [2024] and the references therein. These signals can serve as activation mechanisms
without altering the apparent meaning of the text. Implementing a steganographic function is beyond
the scope of our paper; we assume its existence and functionality, providing only an illustrative
example as a possible approach in Example 26."
REFERENCES,0.6091476091476091,"Example 26. Consider implementing a basic dictionary where each token (word or subword) has
associated typos that can be deliberately introduced or avoided. Each typo variation corresponds
to a binary code—’0’ might represent the standard spelling, while ’1’ could represent a version
with a common typo. By controlling the presence of these typos in the output text, the model can
embed a binary message within a seemingly normal text. This method allows for the transmission of
hidden information that is undetectable to casual observers but can be decoded by those aware of the
steganographic scheme."
REFERENCES,0.6112266112266113,"E.2.1
Defining Backdoored Language Models"
REFERENCES,0.6133056133056133,"In this section, we formally introduce the concept of backdooring a language model. A backdoor
in a language model allows for a hidden behavior to be triggered under specific conditions, while
the model otherwise behaves normally. We assume the existence of a steganographic function
σ : T k × {0, 1}M →T k that encodes a hidden activation trigger into an input prompt t ∈T k,
producing a modified prompt t′ ∈T k. The backdoored language model f utilizes this function to
introduce hidden behaviors triggered by specific conditions on the hidden message t. The definition
is formally stated below."
REFERENCES,0.6153846153846154,"Definition 27 (Backdoored Language Model). A language model ef that originally intends to im-
plement language model f is said to be backdoored if there exists another language model bf, a
steganographic function σ and its inverse σ−1, and a condition E such that:"
REFERENCES,0.6174636174636174,1. ef behaves as f under normal circumstances:
REFERENCES,0.6195426195426196,"∀t ∈T k : E(σ−1(t)) = 0 =⇒ef(t) ≈f(t).
(4)"
REFERENCES,0.6216216216216216,2. bf exhibits altered behavior if the condition E on the prompt t is satisfied:
REFERENCES,0.6237006237006237,"∀t ∈T k : E(σ−1(t)) = 1 =⇒ef(t) ≈bf(t).
(5)"
REFERENCES,0.6257796257796258,"It is important to note that it is not immediately clear what the right generalization of a non-replicable
backdoor should be for LLMs. In order to achieve non-replicability (e.g., in the classification setting)
we partition the input into parts and designate a specific part for non-replicability via digital signatures.
However, when the input (in our case, the tokens) is discrete, performing this sort of partition is
complex because it is not clear e.g., which are the ’most significant’ bits as in the classification case.
Exploring non-replicability in the context of language models is left as future work."
REFERENCES,0.6278586278586279,"E.2.2
Embedding Undetectable Backdoors to a Language Model"
REFERENCES,0.6299376299376299,"Consider an insidious firm that wants to train a model such that it generates text from an insidious
language model bf on triggered inputs. Unlike planting the backdoor in ANN (Section 5), since
the input and output space of the language model is discrete, a language model f can be easily
implemented as a Boolean circuit (see Remark 28)."
REFERENCES,0.632016632016632,"Remark 28. Consider a language model operating on a token set T . Without loss of generality, we
can represent each token in the discrete set T by its one-hot embedding in a vector of dimension |T |.
Given this representation, the operations performed by a language model can be directly mapped
to a Boolean circuit with size O(sz(f), |T |). Conversely, the outputs of a Boolean circuit C, can be
interpreted as one-hot vectors, thus allowing us to transform it to a ANN f with size O(sz(C))."
REFERENCES,0.6340956340956341,"We now introduce the analogous pipeline for embedding a backdoor to a language model. Our
construction is tuned by security parameters λ, λ1 > 0:"
REFERENCES,0.6361746361746362,"1. Training the LM. Begin by training a language model f in a standard fashion so that
it accurately models the function f : T k →∆(T k), representing the LLM’s intended
behavior. This corresponds to the honest training process.
2. LM to Boolean Circuit Transformation. Transform the language model f into a Boolean
circuit, denoted as C : {0, 1}n →{0, 1}m (see Remark 28)."
REFERENCES,0.6382536382536382,"3. Part of the Insidious Pipeline. Consider a malicious language model bf. Let bC be the
Boolean circuit corresponding to the model bf.
3.1. Input Space Partitioning. On prompt t ∈T k, use the original prompt t as xC and the
output of the inverse steganographic function σ−1(t) as xBD.
3.2. Backdoor Implementation. Use Section 4.1 with security parameters λ1 and λ2 = 0
to inject a backdoor within xBD, with sampled seed s∗∈{0, 1}λ1. Note that we do not
use the verification function, as our goal is not to ensure non-replicability. Denote the
resulting backdoored Boolean circuit by Cint.
4. Application of Indistinguishability Obfuscation (iO). Apply indistinguishability ob-
fuscation with security parameter λ to Cint, and let eC be the outputted Boolean circuit
(Definition 7)."
REFERENCES,0.6403326403326404,"5. Circuit to LM Transformation. Convert the obfuscated Boolean circuit eC back to a LM
ef : T k →∆(T k) (see Remark 28)."
REFERENCES,0.6424116424116424,"The above procedure defines the behavior of the honest and the malicious algorithm on input a
language model f:"
REFERENCES,0.6444906444906445,"• The honest obfuscation pipeline corresponds to the sequential composition of steps (2), (4)
and (5).
• The backdoor attack corresponds to the sequential composition of steps (2), (3), (4) and
(5)."
REFERENCES,0.6465696465696466,"To activate the backdoor on original prompt t, we use the steganographic function σ on the intended
seed s∗, e.g., the modified prompt t′ is σ(t, s∗). Note that we set xBD in the above procedure to be
equal to the secret seed s∗.
Theorem 29 (Planting Backdoor to Obfuscated LM). Let n, T as introduced in Steps (1)-(2) above.
Assume that one-way functions and computationally-secure and efficient indistinguishability ob-
fuscators for polynomial-sized circuits (as in Assumption 8) both exist. Further assume that a
steganographic function of Definition 25 exists. Given security parameters λ, λ1 ∈N and a trained
language model f with honest obfuscation ef and with a malicious language model bf, there exists a
backdoor attack (Backdoor, Activate) described in Steps (1)-(5) above so that the following are true:"
REFERENCES,0.6486486486486487,"1. The backdoor runs in time poly(n, sz(C), sz( bC), |T |, λ, λ1), where C (resp. bC) are the
Boolean circuits induced by f (resp. bf)."
REFERENCES,0.6507276507276507,2. ef and f ′ ∼Backdoor are white-box undetectable.
REFERENCES,0.6528066528066528,"3. For any input t ∈T k transformed into t′ ∈T k to activate the backdoor f ′ ∼Backdoor, it
satisfies:"
REFERENCES,0.6548856548856549,f ′(t′) = bf(t′).
REFERENCES,0.656964656964657,"The last Item holds since t′ corresponds to σ(t, s∗) and so σ−1(t′) = s∗, which will appear in xBD,
thus activating the backdoor. The proof is quite short and appears in Appendix I.2."
REFERENCES,0.659043659043659,"F
Conclusion & Open Questions"
REFERENCES,0.6611226611226612,"Given the plethora of applications of Machine Learning in general, and neural networks in particular,
questions regarding the trustworthiness of publicly released models naturally arise. In particular,
before deploying a neural network we need to guarantee that no backdoors have been injected allowing
bad actors to arbitrarily control the model behavior. In this paper, we investigate the existence of
backdoor attacks to obfuscated neural networks which are undetectable even when given white-box
access. The notion of obfuscation that we consider is the well-studied and mathematically founded
indistinguishability obfuscation (iO). We also show how our techniques can inspire backdoor schemes
in large language models when combined with ideas from steganography."
REFERENCES,0.6632016632016632,"While our constructions are purely theoretical, we leave as an interesting direction how to use heuristic
obfuscation methods to show practical instantiations of our constructions. Another interesting open
question is whether cryptographic schemes weaker than iO suffice to show backdoor undetectability
in the white-box model."
REFERENCES,0.6652806652806653,"G
Proof of Theorem 10"
REFERENCES,0.6673596673596673,"We restate the Theorem for convenience. [White-Box Undetectability via iO] Assuming the existence
of secure pseudorandom generators (Assumption 6) and secure indistinguishability obfuscation
(Assumption 8), for any probabilistic polynomial-time (PPT) algorithm A, and security parameters
λ, λ1, λ2 ∈N it holds that
Pr

A(iO(1λ, C)) = 1

−
Pr
s∗∼U{0,1}λ1"
REFERENCES,0.6694386694386695,"h
A(iO(1λ, eC)) = 1
i ≤negl(λ3) + negl(λ1)."
REFERENCES,0.6715176715176715,"Proof. We consider a family of circuits Cr, parameterized by r ∈{0, 1}2λ1 that implement the
following function:"
REFERENCES,0.6735966735966736,"fr(xC, (xPRG, xV )) = bC(xC)·1[PRG(xPRG) = PRG(r)] · Verify(pk, xC, xV )
+ C(x) · (1 −1[PRG(xPRG) = PRG(r)] · Verify(pk, xC, xV ))."
REFERENCES,0.6756756756756757,"By security of PRG (Assumption 6), for any PPT A:

Pr
s∗∼U{0,1}λ1 [A(PRG(s∗)) = 1] −
Pr
r∗∼U{0,1}2λ1 [A(r∗) = 1]
 ≤negl(λ1)."
REFERENCES,0.6777546777546778,"By further restricting to PPT algorithms A that operate on the composed input T(r) = iO(1λ, Cr) ,
and since both compositions take poly(|C|), we have for any PPT algorithm A′:

Pr
s∗∼U{0,1}λ

A′(iO(1λ, CPRG(s∗))) = 1

−
Pr
r∗∼U{0,1}2λ

A′(iO(1λ, Cr∗)) = 1
 ≤negl(λ1). (6)"
REFERENCES,0.6798336798336798,"Since |range(PRG)| ≤2λ1 = negl(λ1), and"
REFERENCES,0.681912681912682,"fr∗(xC, (xPRG, xV )) =C(x) · (1 −1[PRG(xPRG) = PRG(r∗)] · Verify(pk, xC, xV ))"
REFERENCES,0.683991683991684,"+ bC(xC) · 1[PRG(xPRG) = PRG(r∗)] · Verify(pk, xC, xV ),"
REFERENCES,0.6860706860706861,"Pr
r∗∼U{0,1}2λ1[̸ ∃s ∈{0, 1}λ1 : PRG(s) = r] ≥1 −negl(λ1)"
REFERENCES,0.6881496881496881,"⇒
Pr
r∗∼U{0,1}2λ1[Cr∗(x) = C(x)∀x ∈{0, 1}n] ≥1 −negl(λ1)."
REFERENCES,0.6902286902286903,"Hence with probability at least 1 −2λ1, circuits Cr∗and C are computationally equivalent and hence
by application of iO we further have:
Pr

A(iO(1λ, C)) = 1

−
Pr
r∗∼U{0,1}λ1

A(iO(1λ, Cr∗)) = 1
 ≤negl(λ) + negl(λ1).
(7)"
REFERENCES,0.6923076923076923,"We conclude by noticing that circuit CPRG(s∗) is identically equal to circuit eC(x), and combining (6)
and (7):"
REFERENCES,0.6943866943866944,"Pr
s∗∼U{0,1}λ1

A′(iO(1λ, CPRG(s∗))) = 1

−
Pr
r∗∼U{0,1}2λ

A′(iO(1λ, Cr∗)) = 1
 ≤negl(λ1)"
REFERENCES,0.6964656964656964,"⇒
Pr

A(iO(1λ, C)) = 1

−
Pr
s∗∼U{0,1}λ1"
REFERENCES,0.6985446985446986,"h
A(iO(1λ, eC)) = 1
i ≤negl(λ) + negl(λ1)."
REFERENCES,0.7006237006237006,"H
Proofs of Section 4.2"
REFERENCES,0.7027027027027027,"H.1
Proof of Theorem 21"
REFERENCES,0.7047817047817048,"Let us restate the result.
[ANN to Boolean] Given an L-Lipshitz ANN f : [0, 1]n →[0, 1] of
size s, then for any precision parameter k ∈N, there is an algorithm that runs in time poly(s, n, k)
and outputs a Boolean circuit C : {0, 1}n·k →{0, 1}m with number of gates poly(s, n, k) and
m = poly(s, n, k) such that for any x, x′:"
REFERENCES,0.7068607068607069,"|f(x) −T −1(C(Tk(x)))| ≤L 2k ,"
REFERENCES,0.7089397089397089,"|T −1(C(Tk(x))) −T −1(C(Tk(x′)))| ≤
L
2k−1 + L · ∥x −x′∥∞,"
REFERENCES,0.7110187110187111,where Tk and T −1 are defined in Definition 19.
REFERENCES,0.7130977130977131,"Proof. The transformation of Theorem 21 follows by simply compiling a neural network to machine
code (see also [Shi et al., 2020, Section 3]) where the input is truncated within some predefined
precision. Note that"
REFERENCES,0.7151767151767152,|f(x) −T −1(C(Tk(x)))| ≤L · ∥x −T −1(Tk(x))∥∞= L
K,0.7172557172557172,2k
K,0.7193347193347194,"and we also have |T −1(C(Tk(x))) −T −1(C(Tk(x′)))|
≤
|T −1(C(Tk(x))) −f(x)| +
|T −1(C(Tk(x′))) −f(x′)| + |f(x) −f(x′)| ≤
L
2k−1 + L · ∥x −x′∥∞."
K,0.7214137214137214,"H.2
Proof of Theorem 22"
K,0.7234927234927235,"We first restate the Theorem we would like to prove. [Boolean to ANN, inspired by Fearnley et al.
[2022]] Given a Boolean circuit C : {0, 1}n·k →{0, 1}m with k, m, n ∈N with M gates and ϵ > 0
such that"
K,0.7255717255717256,"|T −1(C(Tk((x))) −T −1(C(Tk(x′)))| ≤ϵ
∀x, x′ ∈[0, 1]n s.t. ∥x −x′∥∞≤1 2k ,"
K,0.7276507276507277,"where Tk and T −1 are defined in Definition 19, there is an algorithm that runs in time poly(n, k, M)
and outputs an ANN f : [0, 1]n →[0, 1] with size poly(n, k, M) such that for any x ∈[0, 1]n it
holds that |T −1(C(Tk(x))) −f(x)| ≤2ϵ."
K,0.7297297297297297,"Proof. Our proof is directly inspired by Fearnley et al. [2022]. We start with the definition of
an arithmetic circuit (see Fearnley et al. [2022] for details). An arithmetic circuit representing
the function f : Rn →Rm is a circuit with n inputs and m outputs where every internal node
is a gate with fan-in 2 performing an operation in {+, −, ×, max, min, >} or a rational constant
(modelled as a gate with fan-in 0). Linear arithmetic circuits are only allowed to use the operations
{+, −, max, min, ×ζ} and rational constants; the operation ×ζ denotes multiplication by a constant.
Note that every linear arithmetic circuit is a well-behaved arithmetic circuit (see Fearnley et al. [2022])
and hence can be evaluated in polynomial time. Fearnley et al. [2022] show that functions computed
by arithmetic circuits can be approximated by linear arithmetic circuits with quite small error. We
will essentially show something similar replacing linear arithmetic circuits with ReLU networks."
K,0.7318087318087318,"Our proof proceeds in the following three steps, based on [Fearnley et al., 2022, Section E]."
K,0.7338877338877339,"Discretization
Let N
=
2k.
We discretize the set [0, 1] into N + 1 points I
=
{0, 1/N, 2/N, . . . , 1}, and for any element p ∈[0, 1]n, we let bp ∈In denote its discretization,
i.e., bp = (bpi)i∈[n] such that for each coordinate i ∈[n]"
K,0.735966735966736,bpi = i∗
K,0.738045738045738,"N , where i∗= max

i∗∈[N] : i∗ N ≤pi"
K,0.7401247401247402,"
.
(8)"
K,0.7422037422037422,"Construct Linear Arithmetic Circuit
Given as input a Boolean circuit C, our strategy is to use
the approach of [Fearnley et al., 2022, Lemma E.3] to construct, in time polynomial in the size of C,
a linear arithmetic circuit F : [0, 1]n →R that will well-approximate C as we will see below."
K,0.7442827442827443,"Before we proceed with the proof, we use the following gadget that approximates the transformation
Tk."
K,0.7463617463617463,"Theorem 30 (Bit Extraction Gadget Fearnley et al. [2022]). Let proj(x) = min(0, max(1, x)) and
consider a precision parameter ℓ∈N. Define the bit extraction function"
K,0.7484407484407485,"t0(x) = 0 ,"
K,0.7505197505197505,tk(x) = proj  2ℓ· 
K,0.7525987525987526,"x −2−k − k−1
X"
K,0.7546777546777547,"k′=0
2−k′ · tk′(x) !!"
K,0.7567567567567568,",
for k > 0."
K,0.7588357588357588,"Fix k ∈N. tk(x) can be computed by a linear arithmetic circuit using O(k) layers and a total of
O(k2) nodes. The output is ˆTk(x) = (t0(x), . . . , tk(x))."
K,0.760914760914761,"Moreover, given a number x ∈(0, 1), represented in binary as {b0.b1b2 . . .} where bk′ is the k′-th
most significant digit, if there exists k∗∈[k + 1, ℓ] such that bk∗= 1, then ˆTk(x) = {0.b1b2 . . . bk}."
K,0.762993762993763,"Proof. We prove the first part of the statement based on induction that for each k, we can compute
a linear arithmetic circuit outputting (x, f0(x), f1(x), . . . , fk(x)) using 3 · k + 2 layers a total of
3 Pk
k′=1 k′ + 4 nodes."
K,0.7650727650727651,"Base case k = 0:
We can trivially design a linear arithmetic circuit using two layers and two nodes
that outputs (x, f0(x)) = (x, 0)."
K,0.7671517671517671,"Induction step:
Assume that for k′ −1 we can design a linear arithmetic circuit that outputs"
K,0.7692307692307693,"(x, f0(x), f1(x), . . . , fk′−1(x)) ."
K,0.7713097713097713,"Let C = 2ℓ·

x −2−k′ −Pk′−1
k′=0 2−k′ · fk′(x)

. Observe that"
K,0.7733887733887734,"fk′(x) = min(1, max(C, 0)) ,"
K,0.7754677754677755,"and thus we can extend the original linear arithmetic circuit using three additional layers and an
addition of 3 · (k′ + 1) total nodes to output (x, f0(x), . . . , fk′(x)), which completes the proof."
K,0.7775467775467776,We now prove the second part of the statement by induction.
K,0.7796257796257796,"Base case k = 0:
Since x ∈(0, 1), the base case follows by definition of f0(x) = 0."
K,0.7817047817047817,"Induction step:
Assume that for k′ < k, fk′(x) = bk′ and there exists k∗∈[k + 1, ℓ] such that
bk∗= 1. Observe that x − k−1
X"
K,0.7837837837837838,"k′=0
2−k′ · fk′(x) −2k = x − k−1
X"
K,0.7858627858627859,"j=0
2−j · bk′ −2k,"
K,0.7879417879417879,"which is negative if bk = 0 and if bk = 1 it has value at least 2−k∗. Since by assumption k∗≥ℓ,
proj

2ℓ·

x −Pk−1
k′=0 2−k′ · fk′(x) −2k
is 0 if bk = 0 and 1 if bk = 1, which proves the
induction step."
K,0.7900207900207901,"We describe how the linear arithmetic circuit F is constructed. Fix some point x ∈[0, 1]n. Let
Q(x) = {x +
ℓ
4nN e | ℓ∈{0, 1, ..., 2n}}, where e is the all-ones vector and N = 2k. The linear
arithmetic circuit is designed as follows."
K,0.7920997920997921,"• Compute the points in the set Q(x). This corresponds to Step 1 in the proof of [Fearnley
et al., 2022, Lemma E.3]."
K,0.7941787941787942,"• Let bTk be the bit-extraction gadget with precision parameter ℓ= k + 3 + ⌈log(n)⌉(Theo-
rem 30), and compute ˜Q(x) = { ˆTk(p) : p ∈Q(x)}. As mentioned in Step 2 in the proof of
[Fearnley et al., 2022, Lemma E.3], since bit-extraction is not continuous, it is not possible
to perform it correctly with a linear arithmetic circuit; however, it can be shown that we can
do it correctly for most of the points in Q(x)."
K,0.7962577962577962,"• Observe that for each p ∈[0, 1]n, ˆTk(p) ∈{0, 1}n·k. Let ˆC be the linear arithmetic circuit
that originates from the input Boolean circuit C and compute ˆQ(x) = { ˆC(b) : b ∈˜Q(x)}.
The construction of ˆC is standard, see Step 3 in the proof of [Fearnley et al., 2022, Lemma
E.3]."
K,0.7983367983367984,"• Let ˆT −1 be the linear arithmetic circuit that implements the Boolean circuit that represents
the inverse binary-to-real transformation T −1 and let Q(x) = { ˆT −1(˜b) : ˜b ∈ˆQ(x)}."
K,0.8004158004158004,"• Finally output the median in Q(x) using a sorting network that can be implemented with a
linear arithmetic circuit of size poly(n); see Step 4 of [Fearnley et al., 2022, Lemma E.3]."
K,0.8024948024948025,"The output is a synchronous linear arithmetic circuit since it is the composition of synchronous linear
arithmetic circuits and its total size is poly(n, k, M), where k · n is the input of C and M is the
number of gates of C."
K,0.8045738045738046,"Approximation Guarantee
Now we prove that on input x ∈[0, 1]n the output of the network
F(x) is in the set:

T −1(C(Tk(x))) −2ϵ, T −1(C(Tk(x))) + 2ϵ

."
K,0.8066528066528067,"We will need the following result, implicitly shown in Fearnley et al. [2022]."
K,0.8087318087318087,"Lemma 3 (Follows from Lemma E.3 in Fearnley et al. [2022]). Fix x ∈[0, 1]n. Let Q(x) =
{x +
ℓ
4nN e | ℓ∈{0, 1, ..., 2n}}, where e is the all-ones vector and let"
K,0.8108108108108109,"Sgood(x) =

p = (pi) ∈Q(x) : ∀i ∈[n], l ∈{0, . . . , N},
pi −l N"
K,0.8128898128898129,"≥
1
8 · n · N 
,"
K,0.814968814968815,"i.e., Sgood(x) contains points that are not near a boundary between two subcubes in the discretized
domain In. Then:"
K,0.817047817047817,"1. |Sgood(x)| ≥n + 2,"
K,0.8191268191268192,"2. ∥x −bp∥∞≤1/N for all p ∈Sgood(x), where bp denotes the discretization of p ∈[0, 1]n in
(8).6"
K,0.8212058212058212,"Essentially, Sgood(x) coincides with the set of points where bit-extraction (i.e., the operation ˆTk) was
successful in Q(x) Fearnley et al. [2022]. To prove the desired approximation guarantee, first observe
that since |Sgood(x)| ≥n + 2, then the output of the network satisfies (as the median is in the set):

min
p∈Sgood(x)
ˆT −1( ˆC( ˆTk(p))),
max
p∈Sgood(x)
ˆT −1( ˆC( ˆTk(p)))

."
K,0.8232848232848233,"For any elements p ∈Sgood(x), consider the i-th coordinate pi with corresponding binary representa-
tion bpi
0 .bpi
1 . . .. By assumption ∀l ∈{0, . . . , N}, |pi −
l
N | ≥
1
8·n·N , which further implies at least"
K,0.8253638253638254,6Sgood(x) corresponds to the set Tg in Fearnley et al. [2022].
K,0.8274428274428275,"one bit in bpi
k+1 . . . bpi
k+3+⌈log(n)⌉is one. Thus by choice of precision parameter ℓ= k + 3 + ⌈log(n)⌉"
K,0.8295218295218295,"and by Theorem 30, we have that ˆTk(p) = bp
0.bp
1 . . . bp
k, and, hence, ˆC( ˆTk(p)) = C(bp
0 . . . bp
k), which
implies that the output of the network is in the set

min
p∈Sgood(x) T −1(C(Tk(p))),
max
p∈Sgood(x) T −1(C(Tk(p)))

."
K,0.8316008316008316,"Thus, using Item 2 of Lemma 3 and triangle inequality between p and bp (since ∥p −bp∥∞≤1/N),
we have that the output of the linear arithmetic circuit is in the set

T −1(C(Tk(x))) −2ϵ, T −1(C(Tk(x))) + 2ϵ

."
K,0.8336798336798337,"Convert to ANN
We can directly obtain the ANN f by replacing the min and max gates of the
linear arithmetic circuit F. In particular, max{a, b} = a + ReLU(b −a, 0) and min{a, b} =
b −ReLU(b −a, 0) with only a constant multiplicative overhead."
K,0.8357588357588358,"I
Proofs of Section 5"
K,0.8378378378378378,"I.1
Proof of Theorem 12"
K,0.83991683991684,"Proof. The white-box undetectability and non-replicability follow directly by Theorem 10 and by
Lemma 2 respectively."
K,0.841995841995842,"The condition that ∥x −x′∥∞≤
1
2k′+1 follows by the fact that each coordinate i ∈[n], xi and x′
i
only differ from the k′ + 1 most significant bit and after. Thus their maximum difference is at most: +∞
X"
K,0.8440748440748441,j=k′+1
K,0.8461538461538461,"1
2j =
1
2k′+1 ."
K,0.8482328482328483,"Moreover, the runtime of both the honest obfuscated pipeline and the backdoor attack follows
by Assumption 8 and Theorem 22. Finally we show that the backdoored output is activated. By
Theorem 21 for any pair of inputs x, x′ ∈[0, 1]n such that ∥x −x′∥∞≤
1
2k :"
K,0.8503118503118503,|T −1( eC(Tk(x))) −T −1( eC(Tk(x′)))| ≤6 · L
K,0.8523908523908524,2k−1 .
K,0.8544698544698545,"By Theorem 22, for input x′:"
K,0.8565488565488566,|T −1( eC(Tk(x′))) −ef(x′)| ≤6 · L
K,0.8586278586278586,"2k−1 ,"
K,0.8607068607068608,"thus by Lemma 1, and the activation process, eC(Tk(x′)) = Vp = Tk(c). This means that"
K,0.8627858627858628,"ef(x′) ∈

c −1"
K,0.8648648648648649,2m −24 · L
K,0.8669438669438669,"2k
, c + 1"
K,0.8690228690228691,2m + 24 · L
K,0.8711018711018711,"2k 
."
K,0.8731808731808732,"The proof concludes by setting k′ = ⌈log2(ϵ)⌉, and k = k′ + max(λ1 + λ2, ⌈log2(48 · L)⌉)."
K,0.8752598752598753,"I.2
Proof of Theorem 29"
K,0.8773388773388774,"Proof. The runtime follows by Assumption 8, and Remark 28. The white-box undetectability follows
by Theorem 10."
K,0.8794178794178794,"By Remark 28, for input t′, we have eC(Tk(t′))) = ef(t′), thus by Lemma 1, and the activation
process, eC(Tk(t′)) = bC(t′). Thus ef(t′) = bf(t′), which concludes the proof."
K,0.8814968814968815,NeurIPS Paper Checklist
CLAIMS,0.8835758835758836,1. Claims
CLAIMS,0.8856548856548857,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8877338877338877,Answer: [Yes]
CLAIMS,0.8898128898128899,Justification: We provide formal proofs of our results.
CLAIMS,0.8918918918918919,Guidelines:
CLAIMS,0.893970893970894,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.896049896049896,2. Limitations
LIMITATIONS,0.8981288981288982,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9002079002079002,"Answer: [Yes]
Justification: Our result holds under specific cryptographic primitives that we explicitly
describe and explain."
LIMITATIONS,0.9022869022869023,Guidelines:
LIMITATIONS,0.9043659043659044,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9064449064449065,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9085239085239085,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9106029106029107,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9126819126819127,Justification: All the results of the main body have a rigorous statement and proof.
THEORY ASSUMPTIONS AND PROOFS,0.9147609147609148,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9168399168399168,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.918918918918919,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: Theoretical work.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.920997920997921,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9230769230769231,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Theoretical work."
THEORY ASSUMPTIONS AND PROOFS,0.9251559251559252,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9272349272349273,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9293139293139293,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Theoretical work.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9313929313929314,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9334719334719335,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Theoretical work.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9355509355509356,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified."
THEORY ASSUMPTIONS AND PROOFS,0.9376299376299376,"• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9397089397089398,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9417879417879418,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9438669438669439,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.9459459459459459,Justification: Theoretical work.
EXPERIMENTS COMPUTE RESOURCES,0.9480249480249481,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9501039501039501,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9521829521829522,9. Code Of Ethics
CODE OF ETHICS,0.9542619542619543,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9563409563409564,Answer: [Yes]
CODE OF ETHICS,0.9584199584199584,Justification: Our work is theoretical.
CODE OF ETHICS,0.9604989604989606,Guidelines:
CODE OF ETHICS,0.9625779625779626,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9646569646569647,10. Broader Impacts
BROADER IMPACTS,0.9667359667359667,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9688149688149689,Answer: [Yes]
BROADER IMPACTS,0.9708939708939709,Justification: We discuss the potential societal impact in ??.
BROADER IMPACTS,0.972972972972973,Guidelines:
BROADER IMPACTS,0.975051975051975,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster."
BROADER IMPACTS,0.9771309771309772,"• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9792099792099792,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: -
Guidelines:"
BROADER IMPACTS,0.9812889812889813,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9833679833679834,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: -
Guidelines:"
BROADER IMPACTS,0.9854469854469855,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
BROADER IMPACTS,0.9875259875259875,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: -"
BROADER IMPACTS,0.9896049896049897,Guidelines:
BROADER IMPACTS,0.9916839916839917,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
BROADER IMPACTS,0.9937629937629938,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: -
Guidelines:"
BROADER IMPACTS,0.9958419958419958,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: -
Guidelines:"
BROADER IMPACTS,0.997920997920998,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
