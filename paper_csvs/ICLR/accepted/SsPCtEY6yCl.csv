Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002881844380403458,"In this paper, we argue that energy-based sequence models backed by expressive
parametric families can result in uncomputable and inapproximable partition functions.
Among other things, this makes model selection — and therefore learning model
parameters — not only diﬃcult, but generally undecidable. The reason is that there are
no good deterministic or randomized estimators of partition functions. Speciﬁcally,
we exhibit a pathological example where under common assumptions, no useful
importance sampling estimators of the partition function can guarantee to have variance
bounded below a rational number. As alternatives, we consider sequence model families
whose partition functions are computable (if they exist), but at the cost of reduced
expressiveness. Our theoretical results suggest that statistical procedures with asymptotic
guarantees and sheer (but ﬁnite) amounts of compute are not the only things that make
sequence modeling work; computability concerns must not be neglected as we consider
more expressive model parametrizations."
INTRODUCTION,0.005763688760806916,"1
Introduction"
INTRODUCTION,0.008645533141210375,"Modeling discrete sequences is central to natural language processing and bioinformatics. Many common
parametric sequence models ˜푝are (or can be cast as) energy-based (LeCun et al., 2006): they yield a weight
˜푝(풙) for any given string 풙. Although energy-based models (EBMs) of sequences need not represent
probability distributions, they have been proposed as such to counter the inexpressivity of autoregressive
sequence models (Bakhtin et al., 2021; Lin et al., 2021, §4.1). EBMs with ﬁnite partition functions
푍˜푝≜Í
풙˜푝(풙) ∈R>0 deﬁne distributions 푝(풙) ≜˜푝(풙)/푍˜푝over strings, which are often queried under the
principle of probabilistic inference (Ghahramani, 2015).1"
INTRODUCTION,0.011527377521613832,"Energy-based sequence models are often parametrized as neural models. Each parameter vector 휽in
some parametric neural network family 횯⊆R푑identiﬁes a parametric model ˜푝휽, which then deﬁnes a
parametric distribution over strings 푝휽, assuming 푍˜푝휽exists (Chen et al., 2018). Contemporary neural
networks have been shown to be very powerful. In particular, some popular parametric neural sequence
model families, such as RNNs (Siegelmann & Sontag, 1995) and Transformers (Bhattamishra et al.,
2020; Pérez et al., 2021), have been formally shown to recognize recursively enumerable languages:
given any Turing machine 푀, there is a parameter vector 휽∈횯such that the parametrized sequence
model 푁휽recognizes the same language as 푀does. In other words, these sequence model families are
Turing-complete."
INTRODUCTION,0.01440922190201729,"It is therefore intuitive that energy-based sequence models, backed by such powerful neural networks, form
expressive families of string distributions (§3.1): for any decision algorithm 푀, there is a parameter vector
휽푀in a Turing-complete parametric family 횯, such that 푝휽푀exists, and 푝휽푀(풙) is high if and only if 푀
accepts 풙. It would seem assuring to work with such expressive family of sequence distributions, 횯:
assuming the true string probabilities indeed can be computed in polytime, 횯is well-speciﬁed. Moreover,
one may assume that given we can sample from 푝휽푀, we would be able to use consistent estimators to ﬁnd
휽′ ∈횯, where 휽′ ≈휽푀."
INTRODUCTION,0.01729106628242075,"Unfortunately, we ﬁnd that with such an expressive distribution family 횯, whether the identiﬁability
assumption holds — required for most consistent estimators — itself is undecidable (Turing, 1937)."
INTRODUCTION,0.020172910662824207,"1Many popular energy-based sequence models compute normalized probabilities directly (i.e., ˜푝(풙) = 푝(풙))
(Jelinek, 1980; Katz, 1987; Bengio et al., 2003; Brown et al., 2020, inter alia). This makes both training and querying
with string preﬁxes much easier, at the cost of expressiveness (Lin et al., 2021)."
INTRODUCTION,0.023054755043227664,Published as a conference paper at ICLR 2022
INTRODUCTION,0.025936599423631124,"Moreover, model selection on any held-out data is also undecidable. Even worse, we show that there exists
a vector 휽′ ∈횯, such that as long as a parametric family 횯′ ⊆횯contains 휽′, model selection will not be
possible for 횯′ either — even when 횯′ itself is not necessarily expressive (e.g., 횯′ can be ﬁxed-size
Transformer EBMs, which cannot parametrize all EBMs that require more parameters). We construct one
such ‘pathological’ distribution 푝휽′ as a Transformer EBM."
INTRODUCTION,0.02881844380403458,"These negative results stem from the uncomputability and inapproximability of 푍. A main technical
contribution of this paper is that there is no algorithm (either deterministic or randomized) that can
approximate 푍well. An immediate consequence is that sampling-based statistical procedures are not useful
in this scenario, either, as long as they terminate in ﬁnite time. However, we will see that for less expressive
model families, such uncomputability issues do not arise. Our negative results are summarized in Table A.1.
To ensure that model selection is still possible (such that we can compare diﬀerent parameter vectors with
conﬁdence),2 we have no choice but to resort to less expressive string distributions."
INTRODUCTION,0.03170028818443804,"The paper is structured as follows. In §2 we review deﬁnitions and known results of weighted languages,
sequence model families, and formalize weighted Turing machines, EC-complete parametric families, and
computable estimators. In §§3–5 we describe our main technical results: there exist pathological EBM
sequence models that have uncomputable partition functions, which cannot be approximated well under
randomized estimators, and do not have asymptotic estimators that have any good guarantees. In §6 we that
argue our negative results make model selection impossible for expressive model families, and discuss why
common estimation methods fail. Finally, we discuss three ‘palliative’ parametrization choices in §7, which
all guarantee computable partition functions, at the cost of expressiveness."
BACKGROUND,0.0345821325648415,"2
Background"
ENERGY-BASED SEQUENCE MODELS,0.037463976945244955,"2.1
Energy-based sequence models"
ENERGY-BASED SEQUENCE MODELS,0.040345821325648415,"Energy-based models (EBMs) of sequences (LeCun et al., 2006; Rosenfeld et al., 2001; Sandbank, 2008;
Huang et al., 2018; Bakhtin et al., 2021) are a family of discrete sequence models. Given a string 풙∈푉∗
over a ﬁnite vocabulary 푉, an EBM ˜푝computes string weight ˜푝(풙), but not the (normalized) string
probability 푝(풙) = ˜푝(풙)/Í
풙˜푝(풙). In this work, we focus on EBMs whose weight is eﬃciently computable,
i.e., polytime in the length of 풙. Previous work (Bakhtin et al., 2021; Lin et al., 2021) showed that EBMs
deﬁne expressive distributions; but this requires normalization. While EBMs are often intractable to
normalize (e.g., the Ising model), the inﬁnite domain of ﬁnite strings opens the door to uncomputable
probabilities, which invite the impossibility of model selection and comparison."
WEIGHTED LANGUAGES,0.043227665706051875,"2.2
Weighted languages"
WEIGHTED LANGUAGES,0.04610951008645533,"EBMs give weights to strings. Here we formally characterize these weighted strings as a weighted
language. An unweighted language 퐿⊆푉∗is a set of ﬁnite strings 풙over a ﬁnite vocabulary 푉. A
weighted language ˜푝is a function ˜푝: 푉∗→R≥0.3 In this work, we discuss Boolean languages, such
that 푉= B ≜{0, 1}. We also focus on weighted languages where distributions over strings exist. Following
Lin et al. (2021), we say such weighted languages ˜푝are normalizable: 푍˜푝≜Í
풙∈B∗˜푝(풙) ∈R>0. 푍˜푝is
also called the partition function of ˜푝.4 We can then normalize ˜푝into a distribution 푝over B∗such
that 푝(풙) = ˜푝(풙)/푍˜푝, and thereby Í
풙∈B∗푝(풙) = 1."
WEIGHTED LANGUAGES,0.04899135446685879,"The eﬃciently computable (weighted) languages (EC; Lin et al., 2021) are those weighted languages
˜푝, where a string’s weight (which must be non-negative) is a polytime function of the string. Weighted
languages deﬁned by most EBM sequence models fall into this class, since they score every string in ﬁnite
time (and usually in polytime); and the scores ultimately are computed by some algorithm."
WEIGHTED LANGUAGES,0.05187319884726225,"Intuitively, string weights of an EC language can be obtained as a side product from a weighted Turing
machine that recognizes 풙(in polytime). However, Lin et al. (2021) did not precisely describe how a
Turing machine maps input string 풙to its (rational) weight ˜푝(풙) in ﬁnite time. In this work, such a
construction (out of many possible ones) is necessary, as we need to show that this string weighting can be"
WEIGHTED LANGUAGES,0.05475504322766571,"2Of course, model selection (and computing 푍˜푝) are not always needed — for example, simply deciding whether
˜푝(풙1) > ˜푝(풙2) for two given strings 풙{1,2}. In such case the uncomputability issues we discuss are not a concern.
3With a slight abuse of notation, we also deﬁne support( ˜푝) ≜{풙: ˜푝(풙) > 0}.
4We use the convention that 푍˜푝is the partition function of ˜푝, and 푍˜푞of ˜푞, etc. The subscript is omitted in
unambiguous cases."
WEIGHTED LANGUAGES,0.05763688760806916,Published as a conference paper at ICLR 2022
WEIGHTED LANGUAGES,0.06051873198847262,done by parametric sequence model families (see Appendix B).
LOCALLY NORMALIZED DISTRIBUTIONS,0.06340057636887608,"2.3
Locally normalized distributions"
LOCALLY NORMALIZED DISTRIBUTIONS,0.06628242074927954,"A popular subclass of weighted languages is locally normalized weighted languages (LN), where
conditional local distributions given preﬁx ˆ풙: 푝(· | ˆ풙) can be computed in ﬁnite time. Since they
automatically deﬁne a distribution over strings, we use the term locally normalized distributions
interchangeably. If 푝(· | ˆ풙) can be computed in polynomial time, we call such distributions eﬃciently
locally normalized distributions (ELN) — this is the weighted language class of most autoregressive
models, just as its superset EC is the weighted language class of most energy-based sequence models."
LOCALLY NORMALIZED DISTRIBUTIONS,0.069164265129683,"Locally normalized distributions over sequences have ˜푝(풙) = 푝(풙), and 푍= Í
풙∈B∗˜푝(풙) = 1. They are
consistent: the probability that a string is inﬁnitely long under such distributions is zero. Equivalently,
given any 휖> 0, we can approximate 푍with a ﬁnite sum of string weights:"
LOCALLY NORMALIZED DISTRIBUTIONS,0.07204610951008646,"Proposition 1 (Consistency of LN distributions (Booth & Thompson, 1973; Chen et al., 2018)). Let
푝∈LN be a locally normalized distribution over strings. All strings of a given length or longer have their
probabilities bounded. That is, for all positive real numbers 휖, there is a length 푛at which all strings 풙of
length at least 푛have 푝(풙) < 휖."
LOCALLY NORMALIZED DISTRIBUTIONS,0.07492795389048991,"The consistency property of locally normalized distributions implies that they have an exact sampling
procedure that almost surely terminates in ﬁnite time. Therefore, they (and in particular ELN distributions)
are an attractive choice when we need to sample from the distribution they deﬁne, e.g., in sampling-based
parameter estimation procedures."
EC-COMPLETE PARAMETRIC FAMILIES,0.07780979827089338,"2.4
EC-complete parametric families"
EC-COMPLETE PARAMETRIC FAMILIES,0.08069164265129683,"We have introduced energy-based sequence models (§2.1) and their characterization as weighted languages
(§2.2). However, we usually do not work with weighted Turing machines directly in machine learning. Here
the most common models of computation are (neural) sequence model families, such as RNNs and
Transformers. While these computational models can appear quite dissimilar to state-machine-based models
of computation (e.g., Turing machines), they have been shown to possess the same computation power
(Siegelmann & Sontag, 1995; Pérez et al., 2021). That is, they are Turing-complete."
EC-COMPLETE PARAMETRIC FAMILIES,0.08357348703170028,"Just as we extend the deﬁnition of Turing machines to weighted Turing machines, we likewise formalize
weighted sequence model families. We thus introduce EC-complete parametric families as a sequence
model counterpart of the weighted language class EC. At a high level, a parametric family 횯is EC-complete
if given any ˜푝∈EC (as a description of a weighted Turing machine), we can construct a parameter vector
휽∈횯⊆Q∗which deﬁnes ˜푝’s corresponding sequence model. The model produces an output embedding
which we then decide in polytime to be either a ‘halting embedding’ or not. If it is, we can then extract the
weight ˜푝(풙). (A rigorous exposition is in Appendix C.)"
EC-COMPLETE PARAMETRIC FAMILIES,0.08645533141210375,"We show that with a modiﬁcation to positional embeddings, the family of arbitrary precision, hard attention
Transformers deﬁned in Pérez et al. (2021) is EC-complete:5"
EC-COMPLETE PARAMETRIC FAMILIES,0.0893371757925072,"Theorem 1. The class of one-encoder-layer, four-decoder-layer Transformer networks with positional
encodings (푛, 1/푛, 1/푛2, 2푛) is EC-complete."
EC-COMPLETE PARAMETRIC FAMILIES,0.09221902017291066,"Proof sketch. We extend the construction from Pérez et al. (2021), adding an additional layer to accumulate
the string weight over time steps.
□"
ESTIMATORS,0.09510086455331412,"2.5
Estimators"
ESTIMATORS,0.09798270893371758,"A main result of this work is that partition functions in an EC-complete family are uncomputable. Moreover,
randomness does not help estimation; and correct asymptotic estimators are not useful. We deﬁne these
estimators here in order to discuss the power of diﬀerent estimators concretely."
ESTIMATORS,0.10086455331412104,"Let 횯be a parametric family. The function 푓: 횯→Q is an exact estimator if there exists a weighted
deterministic Turing machine that takes 휽as input and, upon halting, outputs 푓(휽) ∈Q in ﬁnite time."
ESTIMATORS,0.1037463976945245,"5All proofs omitted from the main text are included in Appendix D. Proof sketches for major theorems are in the
main text."
ESTIMATORS,0.10662824207492795,Published as a conference paper at ICLR 2022
ESTIMATORS,0.10951008645533142,"Many estimation procedures are anytime algorithms: they do not have a predetermined runtime, and they
can be stopped at any time before completion to produce a valid output. Output quality of an anytime
algorithm may improve with increased runtime. Moreover, many of these algorithms have asymptotic
guarantees, in that their outputs are optimal in some sense (e.g., consistency) in the limit. We capture these
algorithms with asymptotic estimators: a function 푓(휽) is an asymptotic estimator if there exists a
weighted Turing machine that takes both 휽and an index 푖as input, then outputs a value 푓˜푝,푖∈Q in ﬁnite
time, such that the outputs converge toward 푓(휽) as 푖increases toward inﬁnity. (An example is ˆ푍asym
introduced at §5.)"
ESTIMATORS,0.11239193083573487,"We now extend both exact and asymptotic estimators to the stochastic case, where we compute the estimates
using randomized algorithms instead of deterministic ones. As is conventional for randomized algorithms,
we assume the use of probabilistic Turing machines. These have access to an inﬁnitely long random tape.
The tape describes an inﬁnite binary sequence from tossing a fair two-sided coin inﬁnitely many times. We
call the random tape distribution 푝휏.6 We deﬁne a randomized exact estimator f as a weighted Turing
machine with two input tapes — the random tape 휏∈BN, and an input tape that has 휽— and outputs
푓(휽, 휏) in ﬁnite time. Likewise, we say f휽,푖is a randomized asymptotic estimator if there exists a
function 푓(휽) ∈R and a weighted Turing machine that takes (휽, 푖), 휏on two input tapes, so that for all
random Boolean tapes 휏∈BN, we converge with lim푖→∞푓휽,푖,휏= 푓(휽). Many Monte Carlo estimators
can be seen as randomized asymptotic estimators, including rejection sampling and importance sampling
estimators."
ESTIMATORS,0.11527377521613832,"3
Expressiveness and uncomputability: pathological EBMs"
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.11815561959654179,"3.1
Expressive sequence distributions"
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.12103746397694524,"To illustrate the uncomputability issues of energy-based models, we construct families of string distributions
that are expressive: they require the full power of an EC-complete sequence model family."
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.1239193083573487,"We deﬁne G푘= { ˜푝푀,푘: 푘∈Q>0} to be a set of weighted languages, where ˜푝푀,푘is parametrized by
deterministic Turing machine 푀that takes an empty string as input (‘input-free’). Let 퐿푀⊆B∗be a
preﬁx-free Boolean language of computation sequences of a Turing machine — that is, encodings of the
trace of the Turing machine 푀’s operations. We deﬁne"
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.12680115273775217,"˜푝푀,푘(풙) =
 "
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.12968299711815562,"1/3|풙|+1 + 푘
if 풙∈퐿푀, and 풙encodes a valid accepting
trace of 푀.
1/3|풙|+1
otherwise.
(1)"
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.13256484149855907,"The weight of any string 풙, where |풙| = 푛, under ˜푝푀,푘can be computed in time 푂 poly(푛), by verifying
whether 풙is an accepting execution trace of 푀, from the initial state to an halting state. That is, ˜푝∈EC
(§2.2). We also know that for any (deterministic) machine 푀, the language’s partition function 푍˜푝푀,푘
exists, and it must equal either 1 or 1 + 푘, since there is either one halting trace (푍˜푝푀,푘= 1 + 푘), or none
(푍˜푝푀,푘= 1). Therefore, each ˜푝∈G푘deﬁnes a string distribution.7"
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.13544668587896252,"Since for all 푘∈Q>0, G푘⊂EC, all weighted languages ˜푝푀,푘have an equivalent parameter vector 휽푀,푘
in any EC-complete family 횯. Also, since each G푘is a bĳection between the set of all input-free Turing
machines and a subset of 횯, it follows that there is no exact estimator (§2.5) of the partition function of any
EC-complete family (e.g., Transformer EBMs (Theorem 1)), by a reduction from Halt:8
Theorem 2. Let 횯be a parametric EC-complete sequence model family. And let 횯푘⊂횯be bĳective
with G푘. There is no 푘∈Q>0 for which there exists an exact estimator ˆ푍푘that takes as input 휽∈횯푘as
input, and computes ˆ푍푘(휽) = 푍˜푝휽in ﬁnite time."
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.138328530259366,Proof sketch. 횯푘contains parametric vectors for all input-free Turing machines 푀. For any of these
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.14121037463976946,"6Formally speaking, we deﬁne a probability space (Ω, A, P) where Ω = BN is our sample space, A = {퐴풃:
퐴풃is the set of all sequences ∈Ω that have preﬁx 풃∈B∗} is our event space, and P(퐴) = 2−푛where 푛is the
length of the longest shared preﬁx of 퐴, is our probability function (Stroock, 2014).
7Our construction of expressive sequence distributions is inspired by a weighted language construction in Lin et al.
(2021). See Appendix E for further discussion.
8Speaking loosely, Halt is the task of recognizing (deciding) whether a given program on an ideal computer will
properly terminate in ﬁnite time or not (Sipser, 2013; Arora & Barak, 2006)."
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.1440922190201729,Published as a conference paper at ICLR 2022
EXPRESSIVE SEQUENCE DISTRIBUTIONS,0.14697406340057637,"weighted languages, knowing the exact value of 푍˜푝푀,푘∈{1, 푘+ 1} is enough to decide whether 푀halts
(푀halts iﬀ푍˜푝푀, 푗= 푘+ 1). As Halt is undecidable for input-free Turing machines, ˆ푍푘cannot exist.
□"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.14985590778097982,"3.2
An EBM whose partition function is uncomputable"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.15273775216138327,"Theorem 2 states there is no estimator that ‘works’ for a subset of parameter vectors. While every G푘is
much smaller than its superset EC, G푘is still (countably) inﬁnite. Here under the assumption that ZFC is
consistent, we construct one weighted language ˜푏∈G1 (for simplicity; it holds for arbitrary 푘), where 푍˜푏
is uncomputable:"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.15561959654178675,"Theorem 3. Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted
language ˜푏∈G1 such that (a) 푍˜푏= 푐∈{1, 2} exists, but (b) there is no proof of 푍˜푏= 푐."
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1585014409221902,"Proof sketch. We construct ˜푏such that 푍˜푏= 2 iﬀZFC is inconsistent. If there were a proof 푍˜푏= 1 then
we proved ZFC is consistent, which is impossible under Gödel’s 2nd incompleteness theorem (Jech, 1994).
If there were a proof 푍˜푏= 2, then we proved ZFC is not consistent, which violated our assumption.
□"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.16138328530259366,"The existence of ˜푏suggests that if there is an algorithm that approximates 푍˜푝, and produces a witness of
its approximation quality, then this algorithm will not work on any set of parameter vectors that can
parametrize ˜푏— even when this algorithm may work for some subsets of 횯. This is useful in allowing us to
show negative results regarding ﬁnite subsets of 횯. Appendix F gives one such example."
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1642651296829971,"4
No randomized algorithm can estimate 푍accurately"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.16714697406340057,"We’ve shown by Theorem 2 that for an EC-complete family, there are no exact estimators than can get 푍
perfectly right. In this section, we show that no randomized exact estimator for this is unbiased. Further,
there isn’t even an estimator whose bias is within some factor 휖, regardless of the variance’s magnitude."
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.17002881844380405,"Lemma 1. Let 횯be an EC-complete parametric family. There is no multiplicative factor 휖∈Q>1 for
which every 휽∈횯can have its partition function approximated with ˆZ휖(휽) within a factor of 휖— with
probability greater than 2/3. That is, we cannot have"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1729106628242075,"푃

(1/휖) 푍˜푝휽≤ˆZ휖(휽) ≤휖푍˜푝휽

> 2/3.
(2)"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.17579250720461095,"Proof sketch. Because ˆZ휖computes an estimate in ﬁnite time, it can only use ﬁnitely many distinct
random tape segments. We therefore derandomize ˆZ휖by enumerating ﬁnitely many ‘random’ tapes, and
can always return correct answer, given the 2/3 success probability assumption, and would be able to decide
Halt, making use of distributions in G휖2.
□"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1786743515850144,"Taken together, Theorem 2 and Lemma 1 state that no exact estimator ˆ푍— whether randomized or
deterministic — can approximate 푍with good conﬁdence. In Theorem 4 below, we will make an even
stronger claim: regardless of the dispersion magnitude, it is impossible to bound the mean of random exact
estimators of 푍to within any (computable) multiplicative factor. This is because the mean of ˆZ휖can be
computed in ﬁnite time, by derandomizing ˆZ휖similarly to our proof of Lemma 1:"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.18155619596541786,"Theorem 4. Let 횯be an EC-complete parametric family. There is no multiplicative bound 휖∈Q>1 such
that there exists a randomized exact estimator ˆZ휖that guarantees 1/휖≤E [ ˆZ휖(휽)]/푍˜푝휽≤휖, for every
휽∈횯where ˜푝휽is normalizable."
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1844380403458213,"Proof sketch. We derandomize the expectation of ˆZ휖.
□"
AN EBM WHOSE PARTITION FUNCTION IS UNCOMPUTABLE,0.1873198847262248,Published as a conference paper at ICLR 2022
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.19020172910662825,"5
Common asymptotic estimators do not give useful guarantees"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.1930835734870317,"Let’s now recap the progress we’ve made so far. We’ve shown that no (deterministic) exact estimator can get
푍exactly right in general (Theorem 2), lest it need to solve Halt. Further, no randomized exact estimator
can approximate it within any given relative tolerance, with good conﬁdence (Theorem 4)."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.19596541786743515,"But what about asymptotic estimators? We do know there are correct asymptotic estimators of 푍. For
example, consider the following asymptotic estimator ˆ푍(휽) backed by a weighted Turing machine that
takes 휽∈횯and 푖∈N as inputs, and returns 푓휽,푖≜Í
풙:풙∈B∗,|풙|≤푖˜푝휽(풙). We have lim푖→∞ˆ푍휽,푖= 푍˜푝휽,
so ˆ푍is asymptotically correct. However, ˆ푍asym does not have a convergence rate guarantee: for any 푖∈N,
∥ˆ푍휽,푖−푍˜푝휽∥is uncomputable. We also do not know how much can we improve our estimator when we
increment 푖. As Corollary 2 suggests, we likely cannot have such a guarantee."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.1988472622478386,"In this section, we formalize this intuition for two popular asymptotic estimators: rejection and importance
sampling methods (with other asymptotic estimators left as future work). Speciﬁcally, we show that any
parametric family that is able to parametrize ˜푏from §3.2 cannot have provably useful locally normalized
distributions (§2.3) as proposal distributions."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2017291066282421,"5.1
Rejection sampling estimator of 푍cannot be guaranteed to be possible."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.20461095100864554,"Rejection sampling (Owen, 2013) is a common exact sampling method, applicable even when we cannot
sample from an unnormalized distribution ˜푝. We instead sample from an easy-to-sample distribution 푞,
then stochastically reject samples, to ensure the probability that a sample 풙is kept is proportional to ˜푝(풙)."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.207492795389049,"For rejection sampling to work, the candidate 푞’s support must contain the target ˜푝’s entire support, so that
all true points can be sampled. We also need some ﬁnite constant 푐so that 푐푞envelops ˜푝:
∃푐∈R>0 such that ∀풙∈B∗, ( ˜푝(풙)/푞(풙)) ≤푐.
We will show that for certain EBMs, one cannot formally guarantee the existence of an eligible 푞∈LN.
Theorem 5. Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable
EC weighted language ˜푝, where there does not exist a consistent locally normalized proposal distribution
푞∈LN, and 푐푞∈Q>0, such that it can be proven ∀풙∈B∗, ˜푝(풙)/푞(풙) < 푐푞."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.21037463976945245,"Proof sketch. Let ˜푝= ˜푏. If there were such 푞and 푐푞, we could in ﬁnite time prove 푍˜푝= 1 or 푍˜푝= 2,
which contradicts Theorem 3.
□"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2132564841498559,"Theorem 5 implies that there is no way of ensuring rejection sampling works, not only for any EC-complete
families, but also for any parametric family that can parametrize ˜푏."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.21613832853025935,"5.2
Importance sampling estimator of 푍cannot be guaranteed to be effective."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.21902017291066284,"Similar to the case of rejection sampling, one cannot guarantee an importance-sampling estimator of 푍to be
‘good’ — in this case, we mean that there cannot be a proof that the importance sampling variance is ﬁnite."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2219020172910663,We ﬁrst formalize importance sampling estimators of 푍as randomized asymptotic estimators (§2.5). Let
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.22478386167146974,"ˆZ푞
휽,푁= 1 푁 푁
Õ 푛=1"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2276657060518732,"˜푝휽(풙(푛))
푞(풙(푛))"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.23054755043227665,"be an 푁-sample importance sampling estimator of 푍˜푝휽under 푞, so all 풙(푛) are samples from 푞∈LN."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2334293948126801,"We generally want to minimize the variance of ˆZ푞
휽,푁under 푞: Var푞

ˆZ푞
휽,푁

(Owen & Zhou, 2000). And"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.23631123919308358,"we certainly do not want Var푞

ˆZ푞
휽,푁

= ∞. Unfortunately, for certain EBMs, we cannot guarantee there is
a good locally normalized proposal distribution that has ﬁnite variance:
Theorem 6. Let 횯be an EC-complete parametric family. Assuming ZFC axioms and assuming they are
consistent, there exists 휽∈횯where there does not exist a consistent locally normalized “proposal”
distribution 푞∈LN such that it can be proven Var푞

ˆZ푞
휽,푁

< 푐≠∞, where 푐∈Q>0."
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.23919308357348704,"Proof sketch. Proven in a manner similar to the proof of Theorem 5.
□"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2420749279538905,Published as a conference paper at ICLR 2022
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.24495677233429394,"6
Uncomputable 푍causes parameter estimation problems"
COMMON ASYMPTOTIC ESTIMATORS DO NOT GIVE USEFUL GUARANTEES,0.2478386167146974,"Theorems 2 and 3 state that it is generally impossible to estimate partition functions in an expressive
parametric family, such as certain subsets of an EC-complete family. Here we show how parameter
estimation is made diﬃcult as well: parameter identiﬁability is formally undecidable for an EC-complete
family. Model selection is not possible, either, despite attempts to circumvent this (Table 1)."
PARAMETER IDENTIFIABILITY UNDER EC-COMPLETE FAMILIES IS UNDECIDABLE,0.2507204610951009,"6.1
Parameter identifiability under EC-complete families is undecidable"
PARAMETER IDENTIFIABILITY UNDER EC-COMPLETE FAMILIES IS UNDECIDABLE,0.25360230547550433,"Consistency of many estimators relies upon the condition of parameter identiﬁability (Lehmann & Casella,
2006) — two diﬀerent parameter vectors should deﬁne diﬀerent string distributions. But we in general
cannot ascertain whether this condition holds, even for ﬁnite subsets of an EC-complete family:"
PARAMETER IDENTIFIABILITY UNDER EC-COMPLETE FAMILIES IS UNDECIDABLE,0.2564841498559078,"Theorem 7. Let 횯be an EC-complete family. There is no algorithm that takes 휽1 ∈횯, 휽2 ∈횯as input,
and decides whether ˜푝휽1 and ˜푝휽2 are the same weighted language."
MODEL SELECTION IS GENERALLY IMPOSSIBLE FOR EC-COMPLETE SEQUENCE MODEL FAMILIES,0.25936599423631124,"6.2
Model selection is generally impossible for EC-complete sequence model families"
MODEL SELECTION IS GENERALLY IMPOSSIBLE FOR EC-COMPLETE SEQUENCE MODEL FAMILIES,0.2622478386167147,"Theorem 7 suggests that consistency guarantees of common parameter estimators do not hold, when
used to estimate vectors from 횯. Nonetheless, such ‘oﬀ-label’ use of consistent parameter estimators
in an expressive parametric family is quite common — one usually just selects the best model 휽∗
among ﬁnitely many candidates (say {휽1 . . . 휽푁}) that achieves highest held-out likelihood: 휽∗=
arg max휽푛:1≤푛≤푁
Î
풙∈D 푝휽푛(풙), where D is a ﬁnite set of strings."
MODEL SELECTION IS GENERALLY IMPOSSIBLE FOR EC-COMPLETE SEQUENCE MODEL FAMILIES,0.26512968299711814,"However, Theorem 8 implies that exact log-likelihood-based model selection is generally impossible for
EC-complete sequence model families:"
MODEL SELECTION IS GENERALLY IMPOSSIBLE FOR EC-COMPLETE SEQUENCE MODEL FAMILIES,0.2680115273775216,"Theorem 8. For any ˜푝∈EC and for any EC-complete family 횯, there is no algorithm Better ˜푝that
takes two parameter vectors 휽1 ∈횯, 휽2 ∈횯, and returns YES if KL(푝||푝휽1) ≥KL(푝||푝휽2), and NO
otherwise, where 푝, 푝휽1, 푝휽2 are string distributions deﬁned by ˜푝, ˜푝휽1, ˜푝휽2 respectively."
PALLIATIVE ALTERNATIVES,0.27089337175792505,"7
Palliative alternatives"
PALLIATIVE ALTERNATIVES,0.2737752161383285,"What is a practitioner to do, given that this class of models is unlearnable in general? We emphasize that a
model family does not have to be EC-complete to suﬀer from model selection problems (§6.2) — for
example, model selection is impossible for ﬁxed-size Transformer networks with large enough 푑’s either, by
an extension to Theorem 8 (see Theorem 11 in Appendix G).9 10 In other words, to ensure the problem of
uncomputability does not haunt us, the best we can do is to cripple ˜푝so severely that uncomputability is
impossible."
PALLIATIVE ALTERNATIVES,0.276657060518732,"We identify three palliative choices that restrict the family of EBMs. Each cripples the model ˜푝in its own
way, aﬀording computability at the cost of expressiveness. The choice of triage infuses the model with an
inductive bias; we must tailor our models based on our prior beliefs about the problem domain."
PALLIATIVE ALTERNATIVES,0.27953890489913547,"Restricting support( ˜푝) to be ﬁnite.
If ˜푝assigns non-zero weights to only ﬁnitely many strings, then
푍˜푝is a ﬁnite sum of rational numbers, and is also rational. Here, sampling-based inference and estimation
methods return to their usefulness. One way to ensure support( ˜푝) is ﬁnite is by upper-bounding the
maximum string length (e.g., Bakhtin et al. (2021))."
PALLIATIVE ALTERNATIVES,0.2824207492795389,"The ﬁnite-support restriction imposes an obvious limitation that it cannot handle long strings. Moreover,
while 푍˜푝is computable when support( ˜푝) is ﬁnite, this quantity can still be practically inapproximable,
assuming that ˜푝is expressive (e.g., ˜푝is an EC weighted language that has ﬁnite support), except for very
short strings. Let 푛be the longest string under ˜푝to have non-zero weight. Assuming NP ⊈P/poly,
no randomized estimator of 푍˜푝that is a good approximation can have a guaranteed 푂(poly(푛)) time
complexity (Chandrasekaran et al., 2008). However, if ˜푝has limited expressiveness (e.g., when ˜푝is an Ising
model where a string weight is the sum of pairwise weights), then FPRAS algorithms for approximating 푍˜푝"
PALLIATIVE ALTERNATIVES,0.28530259365994237,"9In addition to model selection issues, it may also be diﬃcult to acquire unbiased gradients of log 푍:
∇(log 푍) ≜1/푍∇푍, which are needed for MLE-based training.
10Limiting ourselves to small 푑’s to avoid uncomputability issues may not be practical; we leave ﬁnding the largest 푑
that provably does not involve uncomputability problems — if it is even possible — as future work."
PALLIATIVE ALTERNATIVES,0.2881844380403458,Published as a conference paper at ICLR 2022
PALLIATIVE ALTERNATIVES,0.2910662824207493,"may exist when ˜푝describe a low-degree (≤2) graph (Jerrum & Sinclair, 1993; Luby & Vigoda, 1999).
However, for high-degree graphs (≥3) it can be shown no FPRAS algorithm for approximating 푍˜푝exists,
assuming RP ≠NP (Galanis et al., 2016)."
PALLIATIVE ALTERNATIVES,0.29394812680115273,"Autoregressive parametrization of ˜푝.
An alternative choice is to conﬁne ourselves to autoregressive
models, i.e., locally normalized string distributions (§2.3). Under an autoregressive model 푝, 푍푝= 1 by
deﬁnition. We also note that any (unnormalized) distribution ˜푝obtained by removing probability mass from
푝will have a computable partition function, as long as ˜푝∈EC:"
PALLIATIVE ALTERNATIVES,0.2968299711815562,"Theorem 9. Let 푝be any LN weighted language. Any ˜푝∈EC where ∀풙∈푉∗, ˜푝(풙) ≤푝(풙) has a
computable 푍˜푝."
PALLIATIVE ALTERNATIVES,0.29971181556195964,"Proof sketch. We construct an algorithm that approximates 푍˜푝to any arbitrary error level in ﬁnite time,
exploiting the consistency property of 푝(Proposition 1).
□"
PALLIATIVE ALTERNATIVES,0.3025936599423631,"Theorem 9 implies that conditionalization operations on 푝, which remove strings from the support of 푝to
get weighted language ˜푝, result in a computable 푍˜푝(as long as we can decide which strings are removed);
and such a ˜푝is therefore not subjected to the limitations of Theorem 4."
PALLIATIVE ALTERNATIVES,0.30547550432276654,"Unlike the ‘ﬁnite support’ ﬁx, an autoregressively parametrized (or subsequently conditionalized) ˜푝can
have an inﬁnite support. A conditionalized ˜푝can have an intractable (but computable) partition function,
and they are still subject to the expressive limitations imposed on LN languages: namely there is an EC
language whose string weight rankings cannot be honored by any such conditionalized ˜푝(Lin et al., 2021)."
PALLIATIVE ALTERNATIVES,0.30835734870317005,"˜푝as low-treewidth factor graph grammars.
Finally, we may limit ourselves to weighted languages
deﬁned by low-treewidth factor graph grammars (Chiang & Riley, 2020). Factor graph grammars generalize
factor graphs, which cover many classes of graphical sequence models, such as 푛-gram, HMM, and
whole-sentence language models (Jelinek, 1980; Kuhn et al., 1994; Rosenfeld et al., 2001), linear CRF
models (Laﬀerty et al., 2001), and weighted FSAs in general (Dreyer & Eisner, 2009): a factor graph
grammar describes a (possibly inﬁnite) set of factor graphs, generated from a hyperedge replacement graph
grammar."
PALLIATIVE ALTERNATIVES,0.3112391930835735,"Assuming that an FGG 퐺contains at most one 푛-observed-node factor graph for all 푛∈N, it then deﬁnes a
weighted language ˜푝퐺(풙) = Î
휓∈Ψ|풙| 휓, where factor 휓is a positive function of nodes. The treewidth of
an FGG 푊(퐺) is the maximum number of nodes any 휓can be a function of, and Ψ|풙| is the set of all
factors of string length |풙|."
PALLIATIVE ALTERNATIVES,0.31412103746397696,"If 푍˜푝퐺∈R exists, it can be computed exactly by an algorithm, in time exponential in 푊(퐺) following
Chiang & Riley (2020). Exact computation of 푍˜푝퐺may be manageable as long as 푊(퐺) is small, which
would allow us to exactly compute held-out data likelihood, and also train with a (marginalized) MLE
objective function. However, limiting 푊(퐺) directly limits the expressiveness of ˜푝."
RELATED WORK,0.3170028818443804,"8
Related work"
RELATED WORK,0.31988472622478387,"Turing completeness of formal languages and associated uncomputability issues emerge repeatedly
in computer science. For example, Turing completeness may emerge as an unwanted side eﬀect in
programming languages, since it implies undecidability. One of the best known examples is the Turing
completeness of the C++ grammar (Veldhuizen, 2003; Haberman, 2013), which makes both parsing and
compiling C++ programs undecidable. Similar problems exist for Java (Grigore, 2016) and Haskell with
(unrestricted) instance declarations (Wansbrough, 1998). Another example is the (possibly inadvertently
introduced) Turing completeness of the page fault handling mechanism on modern computer systems,
which raises security concerns in the context of trusted computing (Bangert et al., 2013)."
RELATED WORK,0.3227665706051873,"Our work is not the ﬁrst to discuss the consequences of computability in machine learning: assuming
we can acquire training data from an oracle, under a supervised learning setting, recognition of an
undecidable language is PAC-learnable (Lathrop, 1996). Agarwal et al. (2020) extended the deﬁnition of
PAC learnability (Valiant, 1984) to computable learners. By contrast, we are focused on the computability
of EBMs for sequences, such as a language model as a component of a larger system for automatic
speech recognition. Designing an appropriate, eﬃcient loss functional is a challenge that several prior
works have compared. With the plethora of learning strategies for EBMs, it is untenable to point out the"
RELATED WORK,0.3256484149855908,Published as a conference paper at ICLR 2022
RELATED WORK,0.3285302593659942,Technique
RELATED WORK,0.3314121037463977,"Energy-based
(i.e., globally
normalized)"
RELATED WORK,0.33429394812680113,"Inﬁnite
language
support"
RELATED WORK,0.3371757925072046,"Scoring function
has unbounded
treewidth
Consistency"
RELATED WORK,0.3400576368876081,"Noise-contrastive estimation (Ma & Collins (2018);
used in Lin et al. (2021); Bakhtin et al. (2021))



"
RELATED WORK,0.34293948126801155,"MLE with variable elimination in factor graph
grammars (Chiang & Riley (2020); used in Eisner
(2001); Finkel et al. (2008), inter alia)



"
RELATED WORK,0.345821325648415,"MLE with autoregressive parametrization (Mikolov
et al., 2010)



"
RELATED WORK,0.34870317002881845,"Contrastive divergence (Hinton, 2002)



"
RELATED WORK,0.3515850144092219,"Contrastive estimation (Smith & Eisner, 2005)



"
RELATED WORK,0.35446685878962536,Table 1: Deﬁciencies of some common alternatives to overly expressive EBMs.
RELATED WORK,0.3573487031700288,"deﬁciency in each. Table 1 gives a handful of examples; none share the four properties we desire: 1. Global
normalization (without which the model would be in LN) 2. Support over inﬁnite languages (of ﬁnite
strings) 3. Unbounded treewidth in the function assigning weights to strings 4. Estimator consistency (i.e.,
asymptotic guarantee to recover the true parameters)."
RELATED WORK,0.36023054755043227,"Lin et al. (2021) noted autoregressive factors of EC languages can be uncomputable (see also Theorem 10).
They also noted that a weighted language can have an uncomputable partition function (presumably
resulting from the sum over inﬁnitely many string weights). But they did not dwell on the question whether
such a weighted language could lie within the EC class, much less providing a constructive proof (see also
Appendix E). Instead, they emphasized that under the assumption that oracular access to trained parameter
strings is possible, arbitrarily good approximations of the (possibly uncomputable) partition function can be
memorized in the (autoregressive) model parameters. There is an interesting constrast between the stances
of Lin et al. (2021) and our work: Lin et al. (2021) saw the uncomputability of 푍as a trivial issue from the
model capacity viewpoint, since good approximations take few bits in the parameters to store. On the other
hand, we see that the uncomputability problem can be a parameter estimation disaster — there will be no
guarantee of good approximations can be found in ﬁnite time at all."
CONCLUSION AND FUTURE WORK,0.3631123919308357,"9
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.3659942363112392,"Energy-based models are posed as an eﬃcient tool for decision problems, circumventing probabilities and
expensive normalization (LeCun et al., 2006). Extending this vision to generic sequence models, though,
can involve complexity/computability problems that are diﬃcult, or even impossible. We’ve shown that as
energy-based sequence models become more powerful, the partition function becomes uncomputable —
even when we are restricted to polytime-computable weighted languages. Exact estimators, even if
randomized, cannot have accuracy guarantees. Popular asymptotic estimators on the other hand are not
useful either. Furthermore, model selection is generally impossible, even if we limit ourselves to ﬁxed-size
sequence model families."
CONCLUSION AND FUTURE WORK,0.3688760806916426,"This paper continues a discussion started by Lin et al. (2021), who posture energy-based models as a more
powerful alternative to autoregressive sequence models. Autoregressive sequence models, after all, have
wide adoption and empirical successes (Radford et al., 2019; Brown et al., 2020). By contrast, more general
neural energy-based sequence models have not caught on. Why not? We give unlearnability — due to
uncomputability — as a possible explanation: unless we give up the ability to learn parameters from data,
we likely cannot use the full expressiveness aﬀorded by powerful neural networks. Just like the model
capacity problems brought up by Lin et al. (2021), this result is independent of the amount of training data."
CONCLUSION AND FUTURE WORK,0.37175792507204614,"We emphasize that our results do not invalidate the ﬁndings of Lin et al. (2021): regardless of the actual
neural parametrization, autoregressive models can never capture certain distributions that energy-based
models can. Instead, one of our main messages is that we may not be able to ﬁnd those EBM parameters in
ﬁnite time, if we do not know what the parameters are. Of course, if we know the task perfectly well
and can in fact manually assign the model parameters, we will not need to learn from data at all. The
middle ground — when we have some prior knowledge about the task, but cannot really design the
parameter vectors — is an interesting direction for future work: the three palliative alternatives outlined
in §7 do not take task-speciﬁc information into account at all. Can we do better than that, without suﬀering
uncomputability problems?"
CONCLUSION AND FUTURE WORK,0.3746397694524496,Published as a conference paper at ICLR 2022
CONCLUSION AND FUTURE WORK,0.37752161383285304,Acknowledgments
CONCLUSION AND FUTURE WORK,0.3804034582132565,"We thank the four reviewers for their comments, especially Reviewer NAcm for shortening the proof of
Theorem 4. Additionally, we thank Alexandra DeLucia, Matthew Francis-Landau, Chin-Fu Liu, Suzanna
Sia, Neha Verma, and Chenghao Yang (sorted alphabetically) for discussions that improved the presentation;
and Jason Eisner, discussions with whom motivated the original exploration of this work."
REFERENCES,0.38328530259365995,References
REFERENCES,0.3861671469740634,"Sushant Agarwal, Nivasini Ananthakrishnan, Shai Ben-David, Tosca Lechner, and Ruth Urner. On
learnability wih computable learners. In Aryeh Kontorovich and Gergely Neu (eds.), Proceedings of the
31st International Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine
Learning Research, pp. 48–60. PMLR, 08 Feb–11 Feb 2020. URL https://proceedings.mlr.
press/v117/agarwal20b.html."
REFERENCES,0.38904899135446686,"S. Arora and B. Barak. Computational Complexity: A Modern Approach. Cambridge University Press,
2006. ISBN 978-0-521-42426-4. URL https://theory.cs.princeton.edu/complexity/
book.pdf."
REFERENCES,0.3919308357348703,"Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam. Residual
energy-based models for text generation. JMLR, 22(40):1–41, 2021. URL http://jmlr.org/
papers/v22/20-326.html."
REFERENCES,0.39481268011527376,"Julian Bangert, S. Bratus, Rebecca Shapiro, and Sean W. Smith. The page-fault weird machine: Lessons in
instruction-less computation. In WOOT, 2013."
REFERENCES,0.3976945244956772,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. J. Mach. Learn. Res., 3(null):1137–1155, mar 2003. ISSN 1532-4435. URL
https://www.jmlr.org/papers/v3/bengio03a.html."
REFERENCES,0.40057636887608067,"Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of Transformers and its
implications in sequence modeling. In Proceedings of the 24th Conference on Computational Natural
Language Learning, pp. 455–475, Online, November 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.conll-1.37. URL https://aclanthology.org/2020.conll-1.37."
REFERENCES,0.4034582132564842,"T.L. Booth and R.A. Thompson. Applying probability measures to abstract languages. IEEE Transactions
on Computers, C-22(5):442–450, 1973. doi: 10.1109/T-C.1973.223746."
REFERENCES,0.40634005763688763,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeﬀrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
REFERENCES,0.4092219020172911,"Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha. Complexity of inference in graphical models.
In UAI, 2008. URL https://dl.acm.org/doi/10.5555/3023476.3023485."
REFERENCES,0.41210374639769454,"Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks
as weighted language recognizers. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), pp. 2261–2271, New Orleans, Louisiana, June 2018. Association for Computational
Linguistics. doi: 10.18653/v1/N18-1205. URL https://aclanthology.org/N18-1205."
REFERENCES,0.414985590778098,"David Chiang and Darcey Riley. Factor graph grammars. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
6648–6658. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/
2020/file/49ca03822497d26a3943d5084ed59130-Paper.pdf."
REFERENCES,0.41786743515850144,Published as a conference paper at ICLR 2022
REFERENCES,0.4207492795389049,"Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third Annual
ACM Symposium on Theory of Computing, STOC ’71, pp. 151–158, New York, NY, USA, 1971.
Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL
https://doi.org/10.1145/800157.805047."
REFERENCES,0.42363112391930835,"Markus Dreyer and Jason Eisner. Graphical models over multiple strings. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing, pp. 101–110, Singapore, August
2009. Association for Computational Linguistics. URL https://aclanthology.org/D09-1011."
REFERENCES,0.4265129682997118,"Jason Eisner. Expectation semirings: Flexible EM for ﬁnite-state transducers. In Gertjan van Noord (ed.),
Proceedings of the ESSLLI Workshop on Finite-State Methods in Natural Language Processing (FSMNLP),
Helsinki, August 2001. URL http://cs.jhu.edu/~jason/papers/#eisner-2001-fsmnlp.
Extended abstract (5 pages)."
REFERENCES,0.42939481268011526,"Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. Eﬃcient, feature-based, conditional
random ﬁeld parsing. In Proceedings of ACL-08: HLT, pp. 959–967, Columbus, Ohio, June 2008.
Association for Computational Linguistics. URL https://aclanthology.org/P08-1109."
REFERENCES,0.4322766570605187,"Andreas Galanis, Daniel Stefankovic, and Eric Vigoda. Inapproximability of the partition function for the
antiferromagnetic Ising and hard-core models. Combinatorics, Probability and Computing, 25:500 –
559, 2016."
REFERENCES,0.43515850144092216,"Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):452–459,
2015. doi: 10.1038/nature14541. URL https://doi.org/10.1038/nature14541."
REFERENCES,0.43804034582132567,"Radu Grigore.
Java generics are Turing complete.
CoRR, abs/1605.05274, 2016.
URL http:
//arxiv.org/abs/1605.05274."
REFERENCES,0.4409221902017291,"Josh Haberman. Parsing C is literally undecidable, Aug 2013. URL https://blog.reverberate.
org/2013/08/parsing-c-is-literally-undecidable.html."
REFERENCES,0.4438040345821326,"Geoﬀrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Comput., 14(8):1771–1800, August 2002. ISSN 0899-7667. doi: 10.1162/089976602760128018. URL
https://doi.org/10.1162/089976602760128018."
REFERENCES,0.44668587896253603,"Y. Huang, A. Sethy, K. Audhkhasi, and B. Ramabhadran. Whole sentence neural language models. In
ICASSP, April 2018. doi: 10.1109/ICASSP.2018.8461734. URL https://ieeexplore.ieee.org/
document/8461734."
REFERENCES,0.4495677233429395,"Thomas Jech. On Gödel’s second incompleteness theorem. Proceedings of the American Mathematical
Society, 121(1):311–313, 1994. doi: 10.2307/2160398."
REFERENCES,0.45244956772334294,"Frederick Jelinek. Interpolated estimation of Markov source parameters from sparse data. In Proc.
Workshop on Pattern Recognition in Practice, 1980, 1980."
REFERENCES,0.4553314121037464,"Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM
Journal on computing, 22(5):1087–1116, 1993."
REFERENCES,0.45821325648414984,"S. Katz. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401, 1987. doi:
10.1109/TASSP.1987.1165125."
REFERENCES,0.4610951008645533,"T. Kuhn, H. Niemann, and E.G. Schukat-Talamazzini. Ergodic hidden Markov models and polygrams for
language modeling. In Proceedings of ICASSP ’94. IEEE International Conference on Acoustics, Speech
and Signal Processing, volume i, pp. I/357–I/360 vol.1, 1994. doi: 10.1109/ICASSP.1994.389282."
REFERENCES,0.46397694524495675,"John D. Laﬀerty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Probabilistic
models for segmenting and labeling sequence data. In ICML, 2001."
REFERENCES,0.4668587896253602,"Richard H. Lathrop. On the learnability of the uncomputable. In ICML, 1996."
REFERENCES,0.4697406340057637,"Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu-Jie Huang. A tutorial on energy-
based learning. In G. Bakir,T. Hofman,B. Schölkopf,A. Smola,and B. Taskar (eds.),Predicting Structured
Data. MIT Press, 2006. URL http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf."
REFERENCES,0.47262247838616717,Published as a conference paper at ICLR 2022
REFERENCES,0.4755043227665706,"Erich L Lehmann and George Casella. Theory of point estimation. Springer, 2006. URL https:
//link.springer.com/book/10.1007%2Fb98854."
REFERENCES,0.4783861671469741,"Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive
models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pp. 5147–5173, Online,
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.405. URL
https://aclanthology.org/2021.naacl-main.405."
REFERENCES,0.4812680115273775,"Michael Luby and Eric Vigoda. Fast convergence of the Glauber dynamics for sampling independent sets.
Random Structures & Algorithms, 15(3-4):229–241, 1999."
REFERENCES,0.484149855907781,"Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical eﬃciency. In EMNLP, 2018. URL https://www.aclweb.org/
anthology/D18-1405.pdf."
REFERENCES,0.48703170028818443,"Tomas Mikolov, M. Karaﬁát, L. Burget, J. Cernocký, and S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH, 2010."
REFERENCES,0.4899135446685879,"Art B. Owen.
Monte Carlo theory, methods and examples.
Unpublished, 2013.
URL https:
//artowen.su.domains/mc/."
REFERENCES,0.49279538904899134,"Art B. Owen and Yi Zhou. Safe and eﬀective importance sampling. Journal of the American Statistical
Association, 95:135–143, 2000."
REFERENCES,0.4956772334293948,"Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is Turing-complete. Journal of Machine
Learning Research, 22(75):1–35, 2021. URL http://jmlr.org/papers/v22/20-302.html."
REFERENCES,0.49855907780979825,"Alec Radford, JeﬀWu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
unsupervised multitask learners. Unpublished, 2019. URL https://d4mucfpksywv.cloudfront.
net/better-language-models/language-models.pdf."
REFERENCES,0.5014409221902018,"Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. Whole-sentence exponential language models: A
vehicle for linguistic-statistical integration. Computer Speech & Language, 15(1):55–73, January 2001.
URL https://doi.org/10.1006/csla.2000.0159."
REFERENCES,0.5043227665706052,"Ben Sandbank. Reﬁning generative language models using discriminative learning. In EMNLP, 2008. URL"
REFERENCES,0.5072046109510087,https://www.aclweb.org/anthology/D08-1006.pdf.
REFERENCES,0.5100864553314121,"H.T. Siegelmann and E.D. Sontag. On the computational power of neural nets. Journal of Computer and
System Sciences, 50(1):132–150, 1995. ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.1995.1013.
URL https://www.sciencedirect.com/science/article/pii/S0022000085710136."
REFERENCES,0.5129682997118156,"Michael Sipser. Introduction to the Theory of Computation. Course Technology, Boston, MA, third edition,
2013. ISBN 113318779X."
REFERENCES,0.515850144092219,"Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In
Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),
pp. 354–362, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi:
10.3115/1219840.1219884. URL https://aclanthology.org/P05-1044."
REFERENCES,0.5187319884726225,"Daniel W. Stroock. An Introduction to Markov Processes, volume 230 of Graduate Texts in Mathematics.
Springer, Heidelberg, 2 edition, 2014. ISBN 978-3-642-40522-8. doi: 10.1007/978-3-642-40523-5."
REFERENCES,0.521613832853026,"A. M. Turing. On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of
the London Mathematical Society, s2–42(1):230–265, 01 1937. ISSN 0024-6115. doi: 10.1112/plms/
s2-42.1.230. URL https://doi.org/10.1112/plms/s2-42.1.230."
REFERENCES,0.5244956772334294,"L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134–1142, November 1984. ISSN
0001-0782. doi: 10.1145/1968.1972. URL https://doi.org/10.1145/1968.1972."
REFERENCES,0.5273775216138329,"Todd L. Veldhuizen. C++ templates are Turing complete. Technical report, Indiana University Computer
Science, 2003. URL https://rtraba.files.wordpress.com/2015/05/cppturing.pdf."
REFERENCES,0.5302593659942363,Published as a conference paper at ICLR 2022
REFERENCES,0.5331412103746398,"Keith Wansbrough. Instance declarations are universal, 1998. URL https://www.lochan.org/
keith/publications/undec.html."
REFERENCES,0.5360230547550432,"Adam B. Yedidia and S. Aaronson. A relatively small Turing machine whose behavior is independent of set
theory. ArXiv, abs/1605.04343, 2016."
REFERENCES,0.5389048991354467,Published as a conference paper at ICLR 2022
REFERENCES,0.5417867435158501,"exact
asymptotic"
REFERENCES,0.5446685878962536,"deterministic
 (Theorem 2)
 (§5; but no useful guarantee in ﬁnite time)"
REFERENCES,0.547550432276657,"randomized
 (Theorem 4)
?; but  for rejection sampling (Theorem 5) and
 for importance sampling (Theorem 6) when
paired with autoregressive proposal distributions"
REFERENCES,0.5504322766570605,"Table A.1: Summary of negative results: neither deterministic or randomized algorithms can estimate EBM partition
functions accurately. On the other hand, popular sampling schemes such as rejection and importance sampling require
their autoregressive proposal distributions to be uncomputable."
REFERENCES,0.553314121037464,"A
Summary of negative results"
REFERENCES,0.5561959654178674,"In Table A.1, we give an overview of the negative results in this paper: deﬁciencies of several estimators of
the partition function in an energy-based sequence model."
REFERENCES,0.5590778097982709,"B
Weighted Turing machines"
REFERENCES,0.5619596541786743,"We extend the deﬁnition of Turing machines into weighted Turing machines as follows. A weighted
Turing machine 푀is described as a 7-tuple (푄, Σ, 훿, 푞init, 퐹, update_num, update_denom), where the
ﬁrst 5 components (푄, Σ, 훿, 푞init, 퐹) have deﬁnitions following that of standard Turing machines. The
last two additions are weight-update functions: both update_num and update_denom have signature
푄→A, where A ≜{same, carry}."
REFERENCES,0.5648414985590778,"At the end of time step 푖, assuming the current state is 푞, we deﬁne"
REFERENCES,0.5677233429394812,"num푖=
num푖−1
if update_num(푞) = same
2푖+ num푖−1
if update_num(푞) = carry"
REFERENCES,0.5706051873198847,"and similarly,"
REFERENCES,0.5734870317002881,"denom푖=
denom푖−1
if update_denom(푞) = same
2푖+ denom푖−1
if update_denom(푞) = carry."
REFERENCES,0.5763688760806917,"To keep our proof of model family capacity brief, we (arbitrarily) deﬁne that ∀푞∈퐹∪{푞init},
update_num(푞) = update_denom(푞) = same. Finally, upon arriving at a halting state ∈퐹in 푟time
steps, we say num푟/denom푟is the weight of an input 풙∈B∗under 푀, if num푟/denom푟is a rational number.11"
REFERENCES,0.579250720461095,"C
Definition of sequence model families and EC-complete parametric
families"
REFERENCES,0.5821325648414986,"Following Pérez et al. (2021), we say a sequence model family is a set of seq-to-seq models
N = {푁휽: 휽∈횯⊆Q∗}, where every 푁휽∈N recognizes language 퐿휽if and only if the seq-to-seq
model 푁휽, paired with embedding function 푓: B →Q푑, initial states 풔∈Q푑, and polytime
termination decision function 푔: Q푑→B, accepts every string 풙∈퐿휽. And we deﬁne 푁휽to
accept 풙= [푥1 . . . 푥푇] ∈B∗if and only if there exists 푟∈N, such that 푁휽upon input embeddings
[ 푓(푥1) . . . 푓(푥푇)], produces output embeddings 풚푟at the 푟-th time step, where 푔(풚푟) = 1."
REFERENCES,0.5850144092219021,"A sequence family 횯is EC-complete if given any ˜푝∈EC (as a description of a weighted Turing machine),
we can construct a parameter vector 휽∈횯such that there exist polytime functions 푤푝: Q푑→N ∪{0}
and 푤푞: Q푑→N ∪{0}, where whenever on input 풙= [푥1 . . . 푥푇], if 푔(풚푟) = 1 for some output
embeddings 풚푟(that is, 푁휽accepts 풙in 푟time steps), 푤푝(풚푟)/푤푞(풚푟) = ˜푝(풙)."
REFERENCES,0.5878962536023055,"11When num푟/denom푟is not a rational number (because denom푟= 0), we say the weight of input string 풙is
undeﬁned. However, we do not encounter that in this work: all strings in an EC language have rational weights. In any
case, we only require that our family contain weighted Turing machines that return rational numbers, not that they be
the only members."
REFERENCES,0.590778097982709,Published as a conference paper at ICLR 2022
REFERENCES,0.5936599423631124,"D
Proofs of theorems from main text"
REFERENCES,0.5965417867435159,"Theorem 1. The class of one-encoder-layer, four-decoder-layer Transformer networks with positional
encodings (푛, 1/푛, 1/푛2, 2푛) is EC-complete."
REFERENCES,0.5994236311239193,"Proof. To model all weighted Turing machines deﬁned in §2.2, we extend the Turing-complete one-
encoder-layer three-decoder-layer Transformer network construction introduced by Pérez et al. (2021) with
one additional layer. We also modify the original positional embedding function pos : N →Q푑to include
a new component. In this proof, we let"
REFERENCES,0.6023054755043228,"pos(푖) = [0, . . . , 0, 1, 푖, 1/푖, 1/푖2, 2푖]"
REFERENCES,0.6051873198847262,"instead of pos(푖) = [0, . . . , 0, 1, 푖, 1/푖, 1/푖2] as in Pérez et al. (2021)."
REFERENCES,0.6080691642651297,"For the sake of clarity, our construction is a ‘stack-on’ construction: the encoder layer and the ﬁrst 3 decoder
layers are largely identical to the design of Pérez et al. (2021), with the only diﬀerence being necessary
changes to accommodate our one additional positional embeddings component.12 It may be possible to
strengthen our results by showing that one-encoder-layer three-decoder-layer Transformer networks
with the original positional embeddings — the parametrization family Pérez et al. (2021) showed to
be Turing-complete — are EC-complete as well, with a more involved construction. We leave such an
improvement as future work."
REFERENCES,0.6109510086455331,"We claim our fourth layer of the decoder has output 풚푟= 풚′
푟+ 풘푟, where"
REFERENCES,0.6138328530259366,"• 풚′
푟is a zero-padded version of the original Transformer output embeddings from Pérez et al.
(2021). 풚′
푟is of the form"
REFERENCES,0.6167146974063401,"[J푞푟K, J푠푟K, 푚(푟−1), 0, . . . , 0]"
REFERENCES,0.6195965417867435,"where J푞푟K denotes an one-hot vector of size |푄| where the 푞푟-th component is 1 (again following
the notation of Pérez et al. (2021)).
• And 풘푟is of the form"
REFERENCES,0.622478386167147,"[0푞, 0푠, 0, num푟, denom푟, 0, . . . , 0]
(3)"
REFERENCES,0.6253602305475504,where num푟∈N ∪{0} and denom푟∈N ∪{0} are deﬁned in §2.2.
REFERENCES,0.6282420749279539,"Now we describe how 풘푟can be computed from [풚′
0 . . . 풚′
푟] using the attention mechanism, with the help
of our new positional embeddings. Speciﬁcally, we want to show that we can construct feedforward
networks 푄4, 퐾4, and 푉4 such that"
REFERENCES,0.6311239193083573,"풚푖= Att 푄4(풚′′
푖), 퐾4(Y′′
푖),푉4(Y′′
푖)"
REFERENCES,0.6340057636887608,"where 풚′′
푖
=
풚′
푖+ pos(푖), Y′
=
[풚′
1 . . . 풚′
푖], and Y′′
푖
= Y′ + [pos(1), . . . , pos(푖)]. We let
푄4(풚′′) = [0, . . . , 0, 1, 0, 0, 0, 0] be a constant vector, and 퐾4(Y′′
푖) = Y′′
푖. Finally, we let 푉4(Y′′
푖) =
[0푞, 0푠, 0, I(update_num(푞푖) = carry)2푖, I(update_denom(푞푖) = carry)2푖, 0, . . . , 0]."
REFERENCES,0.6368876080691642,"푄4 is a constant function (such that it always attends to the unity component of Y′′
푖), so it can be
implemented as a single-layer feedforward network. 퐾4 is the identity function, which can also be
implemented as a single-layer feedforward network. 푉4 on the hand can be implemented as a ﬁxed-size
feedforward network, with the piecewise-linear sigmoidal function 휎: in the case of denom푖, the network
would ﬁrst project 푞푖to 풂= [I(update_denom(푞푖) = carry), I(update_denom(푞푖) = same)] (using
the one-hot J푞푖K segment from 풚′
푖), multiply it by 풃= [2푖, 0] (with the help of nonlinearity from 휎), and
put 풂⊗풃at the position of denom푖in equation (3). The component at num푖in equation (3) can be
computed likewise."
REFERENCES,0.6397694524495677,"12Since we increase the output embeddings’ dimension by 1, we also need to pad all matrices in the original
construction by additional zero columns/rows, such that our new positional embeddings’ new component has no eﬀect
on any computation in the encoder layer, and the ﬁrst 3 decoder layers."
REFERENCES,0.6426512968299711,Published as a conference paper at ICLR 2022
REFERENCES,0.6455331412103746,"Given any position 푟∈N, we have"
REFERENCES,0.6484149855907781,"풚푟= Att(푄4(풚′′
푟), 퐾4(Y′′
푟),푉4(Y′′
푟)) = [0푞,
0푠,
0, 1
푟 푟Õ"
REFERENCES,0.6512968299711815,"푖=1
I(update_num(푞푖) = carry)2푖, 1
푟 푟Õ"
REFERENCES,0.654178674351585,"푖=1
I(update_denom(푞푖) = carry)2푖,"
REFERENCES,0.6570605187319885,"0,
. . . ,
0]."
REFERENCES,0.659942363112392,"Let extract_avg_num(풚푟) be an aﬃne transformation that extracts the  |푄| + |Σ| + 2nd component
from 풚푟, and extract_avg_denom(풚푟) be an aﬃne transformation that extracts the  |푄| + |Σ| + 3rd
component from 풚푟, we have"
REFERENCES,0.6628242074927954,"extract_avg_num(풚푟)
extract_avg_denom(풚푟) =
Í푟
푖=1 I(update_num(푞푖) = carry)2푖
Í푟
푖=1 I(update_denom(푞푖) = carry)2푖"
REFERENCES,0.6657060518731989,"=
num푟
denom푟"
REFERENCES,0.6685878962536023,"which is the weight of input 풙= [푥1 . . . 푥푇] as we deﬁned in §2.2.
□"
REFERENCES,0.6714697406340058,"Theorem 2. Let 횯be a parametric EC-complete sequence model family. And let 횯푘⊂횯be bĳective
with G푘. There is no 푘∈Q>0 for which there exists an exact estimator ˆ푍푘that takes as input 휽∈횯푘as
input, and computes ˆ푍푘(휽) = 푍˜푝휽in ﬁnite time."
REFERENCES,0.6743515850144092,"Proof. We can reduce Halt to computing our partition function. For the sake of contradiction, let us
assume that for some 푘∈Q>0, the exact estimator ˆ푍푘exists. Our reduction from Halt of input-free Turing
machines is as follows: Given any deterministic input-free Turing machine 푀, we build a weighted
deterministic Turing machine: 푀′ (Appendix B). 푀′ takes as input 풙∈B∗, and outputs weight 1/3|풙|+1 + 푘
when 풙encodes an accepting trace of 푀′. Otherwise, 푀′ outputs weight 1/3|풙|+1."
REFERENCES,0.6772334293948127,"푀′ always returns a weight for 풙in polytime. By the assumptions of EC-complete families (§2.4), we
can build a parameter vector 휽∈횯such that ˜푝푀′ = ˜푝휽. Since from the deﬁnitions of G푘we know
˜푝푀′ = ˜푝푀,푘, we have 휽∈횯푘."
REFERENCES,0.6801152737752162,"We have thus completed our reduction: if ˆ푍푘existed, we could decide whether any given input-less
deterministic Turing machine 푀halts, by ﬁrst constructing the weighted Turing machine 푀′, then the
corresponding 휽. By our assumption, ˆ푍푘( ˜푝휽) = 푘+ 1 if and only if ∃풙∈B∗that is an accepting path of
푀′, which is true if and only if 푀halts for some ﬁnite steps. Since whether 푀halts is undecidable, we
have arrived at a contradiction (Turing, 1937; Sipser, 2013). Therefore for all 푘∈Q, the algorithm ˆ푍푘does
not exist.
□"
REFERENCES,0.6829971181556196,"Theorem 3. Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted
language ˜푏∈G1 such that (a) 푍˜푏= 푐∈{1, 2} exists, but (b) there is no proof of 푍˜푏= 푐."
REFERENCES,0.6858789625360231,"Proof. Our proof hinges on Gödel’s second incompleteness theorem: no consistent axiomatic system
which includes Peano arithmetic (e.g., ZFC) can prove its own consistency. We construct a Turing machine
푀푏that enumerates all provable propositions under ZF, and halts if and only if 푀푏proves the proposition
1 = 0. One such 푀푏with 7, 918 states has been built by Yedidia & Aaronson (2016).13"
REFERENCES,0.6887608069164265,"푀푏is an input-free deterministic Turing machine. We construct a weighted Turing machine 푀′ from 푀푏,"
REFERENCES,0.69164265129683,"13Subsequent eﬀorts have led to a construction of 푀푏with 748 states: https://turingmachinesimulator.
com/shared/vgimygpuwi."
REFERENCES,0.6945244956772334,Published as a conference paper at ICLR 2022
REFERENCES,0.6974063400576369,"in the manner of the proof of Theorem 2. We let 푀′ return weight 1 + 1/3|풙|+1, in the case 푀푏halts with
trace 풙. 푀′ returns a weight in polytime, and therefore deﬁnes a weighted language ˜푏∈G1 ⊂EC. We
know from the deﬁnitions of G1 that 푍˜푏is either 1 or 2."
REFERENCES,0.7002881844380403,"Assume to the contrary that there exists a proof that 푍˜푏= 1. Then we know 푀푏did not halt. And therefore
the proof would also imply that ZFC is consistent, which violates Gödel’s second incompleteness theorem.
On the other hand, if there were a proof that 푍˜푏= 2, it would imply 푀푏halted and ZFC is not consistent.
We therefore arrive at a contradiction.
□"
REFERENCES,0.7031700288184438,"Lemma 1. Let 횯be an EC-complete parametric family. There is no multiplicative factor 휖∈Q>1 for
which every 휽∈횯can have its partition function approximated with ˆZ휖(휽) within a factor of 휖— with
probability greater than 2/3. That is, we cannot have"
REFERENCES,0.7060518731988472,"푃

(1/휖) 푍˜푝휽≤ˆZ휖(휽) ≤휖푍˜푝휽

> 2/3.
(2)"
REFERENCES,0.7089337175792507,"Proof. In this proof, we make use of the distribution family G휖2 (§3.1). We assume to the contrary
that a multiplicative bound 휖satisfying equation (2) exists. Recall that our assumptions state that
푃 
1/휖≤ˆZ휖(휽)/푍˜푝휽≤휖 > 2/3.14 Let 푀be an input-free Turing machine. Andlet ˜푝휽= ˜푝푀,휖2 ∈G휖2 where"
REFERENCES,0.7118155619596542,"휽∈횯. Ifthe Turing machine 푀halts,푍˜푝푀,휖2 = 1+휖2; therefore,푃

(1 + 휖2)1/휖≤ˆZ휖(휽) ≤(1 + 휖2)휖

>"
REFERENCES,0.7146974063400576,"2/3. Similarly, if 푀does not halt, we have 푍˜푝푀,휖2 = 1; therefore, 푃

1/휖≤ˆZ휖(휽) ≤휖

> 2/3. By"
REFERENCES,0.7175792507204611,"combining the two conditions, we know that 푃(I(푀halts)) = 푃

I( ˆZ휖(휽) ≥휖+ 1/휖∧ˆZ휖(휽) > 휖)

="
REFERENCES,0.7204610951008645,"푃

I( ˆZ휖(휽) ≥휖+ 1/휖)

."
REFERENCES,0.723342939481268,"Therefore, given (1/휖) 푍˜푝휽≤ˆZ휖(휽) ≤휖푍˜푝휽, we can decide if 푀halts by checking if ˆZ휖(휽) ≥휖+ 1/휖.
Since the condition (1/휖) 푍˜푝휽≤ˆZ휖(휽) ≤휖푍˜푝휽only holds 2/3 of all time, we then derandomize the
randomized ˆZ휖to get a deterministic estimator: recall that a randomized exact estimator ﬁnishes
computation in ﬁnite time, regardless of the content of the random tape 휏. There, there are only ﬁnitely many
ﬁnitely long possible ‘random’ sequences that ˆ푍휖will read, which we can enumerate. More concretely:"
REFERENCES,0.7262247838616714,"푃  ˆZ휖(휽) ≥휖+ 1/휖 = E
휏

I( ˆ푍휖(휽, 휏) ≥휖+ 1/휖)
 =
Õ"
REFERENCES,0.729106628242075,"휏∈B푚휽,휖"
REFERENCES,0.7319884726224783,"1/2푚휽,휖I( ˆ푍휖(휽, 휏) ≥휖+ 1/휖)
(4)"
REFERENCES,0.7348703170028819,"where 푚휽,휖∈N is the maximum preﬁx length of the random tape that ˆ푍휖(휽, 휏) will use on any 휏∈BN.
Again 푚휽,휖is guaranteed to exist because of our assumption ˆ푍휖(휽, 휏) ends in ﬁnite time. Since equation (4)
is a ﬁnite sum of computable functions, it is also computable."
REFERENCES,0.7377521613832853,"From the computability of equation (4), it follows that we can derive a deterministic algorithm from ˆZ휖
that decides whether an arbitrary input-free Turing machine halts, which is not possible. Therefore,
there is no 휖∈Q>1, such that the randomized exact estimator ˆZ휖satisﬁes the multiplicative bound
푃(1/휖≤ˆZ휖(퐺)/푍≤휖) > 2/3 for all 휽∈횯.
□"
REFERENCES,0.7406340057636888,"Theorem 4. Let 횯be an EC-complete parametric family. There is no multiplicative bound 휖∈Q>1 such
that there exists a randomized exact estimator ˆZ휖that guarantees 1/휖≤E [ ˆZ휖(휽)]/푍˜푝휽≤휖, for every
휽∈횯where ˜푝휽is normalizable."
REFERENCES,0.7435158501440923,"Proof. We deﬁne 휏to be distributed according 푝휏. Therefore the mean E[ ˆZ휖(휽)] can be expanded as
E휏∼푝휏[ ˆZ휖(휽)] = Í
휏∈BN 푝휏(휏) ˆ푍휖(휽, 휏). Following the same derandomization technique in the proof of
Lemma 1, we can ﬁnd some 푚∈N such that Í
휏∈BN 푝휏(휏) ˆ푍휖(휽, 휏) = Í
휏∈B푚1/2푚ˆ푍휖(휽, 휏) in ﬁnite
time. And subsequently, we can compute E[ ˆZ휖(휽)] exactly in ﬁnite time. Let the exact estimator be 푍휖.
We then have 푍휖(휽) = E[ ˆZ휖(휽)]."
REFERENCES,0.7463976945244957,"14As is common, e.g., in deﬁning the complexity class BPP (see Arora & Barak 2006), the fraction 2/3 is arbitrary.
Any proportion bounded away from 1/2 will work."
REFERENCES,0.7492795389048992,Published as a conference paper at ICLR 2022
REFERENCES,0.7521613832853026,"Assuming to the contrary that we could guarantee 1/휖≤푍휖(휽) ≤휖. Since 푍휖is a deterministic estimator,
we can write 푃(1/휖≤푍휖(휽) ≤휖) = 1, which contradicts Lemma 1. Therefore such an estimator 푍휖(휽)
does not exist.
□"
REFERENCES,0.7550432276657061,"Theorem 5. Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable
EC weighted language ˜푝, where there does not exist a consistent locally normalized proposal distribution
푞∈LN, and 푐푞∈Q>0, such that it can be proven ∀풙∈B∗, ˜푝(풙)/푞(풙) < 푐푞."
REFERENCES,0.7579250720461095,"Proof. By contradiction; let ˜푝= ˜푏, where ˜푏is ﬁrst introduced in §3.2. Assume that there exists 푞∈LN
where it is proven that ∀풙∈B∗, ˜푝(풙)/푞(풙) < 푐< ∞, where 푐∈Q>0."
REFERENCES,0.760806916426513,"We can either prove ZFC is consistent, or is inconsistent, as follows:"
REFERENCES,0.7636887608069164,"1. By our assumptions, 푞∈LN scores every string 풙as 푞휽(풙) 푓(휽) ∈Q>0. Also, because 푞is
consistent and locally normalized, there exists 푛∈N such that we can prove ∀풙∈X>푛{풙: 풙∈
B∗, |풙| > 푛}, 푞(풙) < 1/푐(Proposition 1). We will just prove the existence of 푛constructively by
enumerating the strings in B∗in increasing length. Let the complement set X≤푛= B∗−X>푛.
2. The proof then examines the ﬁnitely many strings in X≤푛."
REFERENCES,0.7665706051873199,"(a) If any of these strings 풙′ has ˜푏(풙′) > 1, then we know 풙′ encodes an accepting trace of 푀푏
(§3.2). Therefore, 푀푏halts, which implies there is a proof of the inconsistency of ZFC.
(b) If none of these strings ∈X≤푛has ˜푏(풙) > 1, then we know there is also no string 풙′′ ∈X>푛,
such that ˜푏(풙′′) > 1. This is because of our assumption 푐≥˜푏(풙)/푞(풙), which in turn means
that ˜푏(풙) is less than 1 on these long strings X>푛. Therefore, 푀푏does not halt, which
implies there is a proof of the consistency of ZFC."
REFERENCES,0.7694524495677233,"Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, 푞does not exist.
□"
REFERENCES,0.7723342939481268,"Theorem 6. Let 횯be an EC-complete parametric family. Assuming ZFC axioms and assuming they are
consistent, there exists 휽∈횯where there does not exist a consistent locally normalized “proposal”
distribution 푞∈LN such that it can be proven Var푞

ˆZ푞
휽,푁

< 푐≠∞, where 푐∈Q>0."
REFERENCES,0.7752161383285303,"Proof. let ˜푝= ˜푏, where ˜푏is ﬁrst introduced in §3.2. Assume that there exists 푞∈LN where it is proven
that Var푞( ˆZ푞
휽,푁) = 푘< 푐∈Q<0 ≠∞."
REFERENCES,0.7780979827089337,We have
REFERENCES,0.7809798270893372,"Var푞( ˆZ푞
휽,푁) ≜1 푁 ( Õ 풙∈B∗"
REFERENCES,0.7838616714697406,""" ˜푝휽(풙) 푞(풙)"
REFERENCES,0.7867435158501441,"2
푞(풙) #"
REFERENCES,0.7896253602305475,"−푍˜푝휽
2
)"
REFERENCES,0.792507204610951,"=
푍2
˜푝휽
푁 Õ 풙∈B∗"
REFERENCES,0.7953890489913544,"푝2
휽(풙)
푞(풙) −1 ! = 푘,"
REFERENCES,0.7982708933717579,"where 푝휽(풙) ≜
˜푝휽(풙)"
REFERENCES,0.8011527377521613,"푍˜푝휽. After some manipulation, we have Õ 풙∈B∗"
REFERENCES,0.8040345821325648,"푝2
휽(풙)
푞(풙) = 푁푘+ 푍˜푝휽
2"
REFERENCES,0.8069164265129684,"푍˜푝휽
2
|       {z       }
=푠≤푁푘+1 ."
REFERENCES,0.8097982708933718,"Since ∀풙∈B∗,
푝2
휽(풙)
푞(풙) ≥0, we have"
REFERENCES,0.8126801152737753,"∀풙∈B∗,
푝2
휽(풙)
푞(풙) ≤푠;
(5)"
REFERENCES,0.8155619596541787,Published as a conference paper at ICLR 2022
REFERENCES,0.8184438040345822,"in particular, if 풙′ encodes a halting trace of 푀푏, from §3.1 we know"
REFERENCES,0.8213256484149856,푝휽(풙′) = ˜푝휽(풙′) 푍˜푝휽
REFERENCES,0.8242074927953891,"=
1 +

1
3
 |풙′|+1"
REFERENCES,0.8270893371757925,"2
> 1/2.
(6)"
REFERENCES,0.829971181556196,"Combining equations (5) and (6), we have for any halting trace 풙′ of 푀푏,"
REFERENCES,0.8328530259365994,"1
4푞(풙′) <
푝2
휽(풙′)
푞(풙′) ≤푠."
REFERENCES,0.8357348703170029,"After some more arrangement,"
REFERENCES,0.8386167146974063,푞(풙′) ≥1
REFERENCES,0.8414985590778098,"4푠≥
1
4(푁푘+ 1) ≥
1
4(푁푐+ 1) > 0."
REFERENCES,0.8443804034582133,"As in the proof of Theorem 5, the existence of such a 푞allows us to either prove or disprove the consistency
of ZFC. For the sake of completeness, we include a proof sketch below:"
REFERENCES,0.8472622478386167,"1. By our assumptions, 푞∈LN scores every string 풙as 푞휽(풙) 푓(휽) ∈Q>0. Also, because 푞is
consistent and locally normalized, there exists 푛∈N such that we can prove ∀풙∈X>푛{풙:
풙∈B∗, |풙| > 푛}, 푞(풙) <
1
4(푁푐+1) (Proposition 1). We will just prove the existence of 푛
constructively by enumerating the strings in B∗in increasing length. Let the complement set
X≤푛= B∗−X>푛.
2. The proof then examines the ﬁnitely many strings in X≤푛."
REFERENCES,0.8501440922190202,"(a) If any of these strings 풙′ has ˜푝휽(풙′) > 1, then we know 풙′ encodes an accepting trace of
푀푏(§3.2). Therefore, 푀푏halts, which implies there is a proof of the inconsistency of ZFC.
(b) If none of these strings ∈X≤푛has ˜푝휽(풙) > 1, then we know there is also no string
풙′′ ∈X>푛, such that ˜푝휽(풙′′) > 1, since ˜푝휽(풙′) > 1 ⇐⇒˜푏(풙′) > 1 ⇐⇒푀푏halts;
and we have already shown that for any accepting trace 풙′ of 푀푏, 푞(풙′) ≥1/4(푁푐+1).
Therefore, 푀푏does not halt, which implies there is a proof of the consistency of ZFC."
REFERENCES,0.8530259365994236,"Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, such a 푞does not exist.
□"
REFERENCES,0.8559077809798271,"Theorem 7. Let 횯be an EC-complete family. There is no algorithm that takes 휽1 ∈횯, 휽2 ∈횯as input,
and decides whether ˜푝휽1 and ˜푝휽2 are the same weighted language."
REFERENCES,0.8587896253602305,"Proof. Assuming to the contrary that such an algorithm Distinct exists. We would be able to reduce Halt
of input-free Turing machines to Distinct: we ﬁrst construct 휽1 ∈횯such that ˜푝휽1(풙) = 1/3|풙|+1. We then
construct 휽2 ∈횯such that ˜푝휽2 = ˜푝푀,1 ∈G1 (§3.1). Distinct(휽1, 휽2) is YES if and only if 푀halts.
□"
REFERENCES,0.861671469740634,"Theorem 8. For any ˜푝∈EC and for any EC-complete family 횯, there is no algorithm Better ˜푝that
takes two parameter vectors 휽1 ∈횯, 휽2 ∈횯, and returns YES if KL(푝||푝휽1) ≥KL(푝||푝휽2), and NO
otherwise, where 푝, 푝휽1, 푝휽2 are string distributions deﬁned by ˜푝, ˜푝휽1, ˜푝휽2 respectively."
REFERENCES,0.8645533141210374,"Proof. By contradiction; assume that for some ˜푝∈EC, Better ˜푝exists. We know there exists a weighted
Turing machine (also denoted as ˜푝) that on any string 풙, terminates in 푂(poly(|풙|)) and outputs ˜푝(풙)
(Appendix B)."
REFERENCES,0.8674351585014409,"We show how we can reduce from Halt to Better ˜푝. Given any arbitrary input-less Turing machine 푀,
we can deﬁne a new weighted Turing machine ˜푝′
푀that on input string 풙, ˜푝′
푀ﬁrst simulates ˜푝on it and
keeps a record of ˜푝(풙) somewhere on the tape. ˜푝′
푀then veriﬁes whether 풙is an encoded accepting trace
of 푀. If 풙is indeed an encoded accepted trace of 푀, ˜푝′
푀outputs ˜푝(풙) + 1. Otherwise ˜푝′
푀outputs ˜푝(풙)."
REFERENCES,0.8703170028818443,"Let 휽1 ∈횯parametrize ˜푝′
푀, and let 휽2 ∈횯parametrize ˜푝, such that ˜푝휽1(풙) = ˜푝′
푀(풙), ˜푝휽2(풙) =
˜푝(풙), ∀풙∈B∗."
REFERENCES,0.8731988472622478,Published as a conference paper at ICLR 2022
REFERENCES,0.8760806916426513,"We know that KL(푝||푝) = 0 for any distribution 푝. better ˜푝(휽1, 휽2) returns YES if and only if
KL(푝||푝휽1) = 0 ≥KL(푝||푝휽2), which implies that ˜푝휽2 and ˜푝휽1 deﬁne the same distribution, which in
turn implies that ∀풙∈B∗, ˜푝(풙) = ˜푝′
푀(풙), and that 푀never halts. Similarly, Better ˜푝(휽1, 휽2) returns
NO if and only if 푀halts. We have thus completed our reduction from Halt; and therefore algorithm
Better ˜푝does not exist for all ˜푝∈EC.
□"
REFERENCES,0.8789625360230547,"Theorem 9. Let 푝be any LN weighted language. Any ˜푝∈EC where ∀풙∈푉∗, ˜푝(풙) ≤푝(풙) has a
computable 푍˜푝."
REFERENCES,0.8818443804034583,"Proof. We prove Theorem 9 by constructing an algorithm ˆ푍˜푝: Q>0 →Q≥0 that approximates 푍˜푝within
any desired positive rational error 휖: namely | ˆ푍˜푝(휖) −푍˜푝| ≤휖."
REFERENCES,0.8847262247838616,"Let ˆ푍푛= Í푛
ℓ=0
Í
풙:|풙|=ℓ푝(풙). We ﬁrst observe that lim푛→∞푍푛= 1 by Proposition 1. In other words,
lim푛→∞1 −ˆ푍푛= 0, or equivalently, given any 휖> 0, there exists 푛∈N such that for all 푛′ ≥푛, 푛′ ∈N,
(1 −ˆ푍푛′) < 휖."
REFERENCES,0.8876080691642652,"Therefore, given any 휖> 0, ∃푛∈N that divides B∗in two sets: X≥푛= {풙: 풙∈B∗, |풙| ≥푛}, where
Í
풙∈X≥푛푝(풙) < 휖, and X<푛= B∗−X≥푛. We are guaranteed to ﬁnd 푛by enumerating all candidates from
N."
REFERENCES,0.8904899135446686,"We can thus implement ˆ푍˜푝as the following program: given 휖∈Q>0, we ﬁrst ﬁnd the smallest 푛∈N,
and partition B∗into two sets X<푛, X≥푛as we described in the previous paragraph. We then return
ˆ푍˜푝(휖) = Í
풙∈X<푛˜푝(풙)."
REFERENCES,0.8933717579250721,"We argue that ˆ푍˜푝is a computable function. We ﬁrst repeat that since 푛∈N exists, we will ﬁnd 푛in ﬁnite
time, simply by enumerating. And since the set X<푛⊂B∗is ﬁnite, ˆ푍˜푝(휖) = Í
풙∈X<푛˜푝(풙) can computed
in ﬁnite time (under our assumption ˜푝∈EC, ∀풙∈B∗, ˜푝(풙) can be computed in time 푂(poly(|풙|))).
Since the program we described above terminates in ﬁnite time, ˆ푍˜푝is a computable function."
REFERENCES,0.8962536023054755,"What remains is to show that the approximation error |푍˜푝−ˆ푍˜푝(휖)| is no greater than 휖; i.e., that
|푍˜푝−ˆ푍˜푝(휖)| ≤휖."
REFERENCES,0.899135446685879,"It is easy to show that 0 ≤푍˜푝−ˆ푍˜푝(휖), after which we only need to show that 푍˜푝−ˆ푍˜푝(휖) ≤휖.
Expressing both terms as sums, we have that Í
풙∈B∗˜푝(풙) −Í
풙∈X<푛˜푝(풙) = Í
풙∈X≥푛˜푝(풙), which is a
sum of nonnegative terms and thus nonnegative."
REFERENCES,0.9020172910662824,"To show that 푍˜푝−ˆ푍˜푝(휖) ≤휖, we express both terms as sums again: Õ"
REFERENCES,0.9048991354466859,"풙∈B∗
˜푝(풙) −
Õ"
REFERENCES,0.9077809798270894,"풙∈X<푛
˜푝(풙) =
Õ"
REFERENCES,0.9106628242074928,"풙∈X≥푛
˜푝(풙)."
REFERENCES,0.9135446685878963,"By the deﬁnition of ˜푝, we have Í
풙∈X≥푛˜푝(풙) ≤Í
풙∈X≥푛푝(풙). The right-hand side is equal to 1 −ˆ푍푛,
which by construction is less than 휖. Therefore, by substituting to get 푍˜푝−ˆ푍˜푝(휖) ≤1 −ˆ푍푛and using the
transitive property of inequality, we have that 푍˜푝−ˆ푍˜푝(휖) < 휖. Combined with the previous paragraph’s
result, we have shown that |푍˜푝−ˆ푍˜푝(휖)| ≤휖. □"
REFERENCES,0.9164265129682997,"E
Connection between expressive sequence distributions and Lin et al.
(2021)’s proof that EC ≠ELN"
REFERENCES,0.9193083573487032,"Our construction of expressive sequence distributions is similar to Lin et al. (2021)’s construction of an EC
weighted language that is not in ELN. Whereas each weighted machine ˜푝푀,푘∈G푘corresponds to
execution traces of a deterministic Turing machine 푀, Lin et al. (2021) deﬁned a single weighted language
˜푝ELN for all Turing machines. Fore the sake of completeness, we describe the deﬁnition of ˜푝ELN from Lin
et al. (2021) below. Let enc be a preﬁx-free encoding function that maps a deterministic Turing machine 푀
to a Boolean string where the function inverse enc−1 exists. And let ˜푝ELN be a weighted language over
Boolean strings, where ˜푝ELN(풙) = 1/3|풙|+1 if 풙is of the form 풙(1)풙(2), where 풙(1) encodes some Turing
machine 푀, and 풙(2) encodes an accepting execution path of 푀on an empty input. (Such a path may be"
REFERENCES,0.9221902017291066,Published as a conference paper at ICLR 2022
REFERENCES,0.9250720461095101,"represented as a sequence of transitions of 푀that begins with an initial state and ends at an accepting
state.) Lin et al. (2021) then showed that preﬁx probabilities of ˜푝ELN are uncomputable under an ELN,
since Halt reduces to computing 푍(풙(1))."
REFERENCES,0.9279538904899135,Theorem 9 implies that ˜푝ELN we have just described above has a computable partition function:
REFERENCES,0.930835734870317,Theorem 10. Let 푍be the partition function of ˜푝ELN. 푍is computable.
REFERENCES,0.9337175792507204,"Proof. Let 푝(풙) = 1/3|풙|+1. 푝∈ELN ⊂LN because 푝(· | ˆ풙) = 1/3 for all valid preﬁxes ˆ풙. Since
˜푝ELN ∈EC by Lin et al. (2021, Theorem 5) and ∀풙∈B∗, ˜푝ELN(풙) ≤푝(풙), we have 푍is computable by
Theorem 9.
□"
REFERENCES,0.9365994236311239,"Similarly, the ‘sparse version’ of ˜푝ELN introduced in (Lin et al., 2021, Theorem 6) can be shown to have a
computable partition function as well."
REFERENCES,0.9394812680115274,"Since one of our goals is to clearly demonstrate that we can construct a single weighted language ∈EC that
has an uncomputable partition function assuming ZFC (e.g., ˜푏in §3.2), we deﬁne weighted languages in
G푘to have at most one ‘high’ weight, as opposed to ˜푝in Lin et al. (2021, Theorem 5), where each 풙(1) that
encodes a halting machine has a ‘high weight’ suﬃx 풙(2)."
REFERENCES,0.9423631123919308,"In fact, under our construction we can directly show EC ≠ELN, using the uncomputability of ˜푏from
Theorem 3:"
REFERENCES,0.9452449567723343,"Corollary 1 (EC ≠ELN under ZFC; slightly weaker version of Theorem 5 in Lin et al. (2021)). Assuming
the axiomatic system of ZFC, EC ≠ELN."
REFERENCES,0.9481268011527377,"Proof. We know there exists a weighted language ˜푏∈EC (§3.2) that has an uncomputable partition
function. Since ˜푏∈EC, ˜푏(풙) ∈Q≥0, ∀풙∈B∗. Therefore for all strings 풙∈B∗, 푏(풙) = ˜푏(풙)/푍˜푏is an
uncomputable number."
REFERENCES,0.9510086455331412,"Assuming to the contrary that ˜푏∈ELN. By deﬁnition 푏(풙) = Î 푏(푥푡| 풙<푡), where each 푏(푥푡| 풙<푡) ∈
Q≥⊬and is computable. Since the set of computable numbers is closed under multiplication, 푏(풙) would
also be computable, which contradicts our assumption. Therefore ˜푏≠ELN, which implies EC ≠ELN.
□"
REFERENCES,0.9538904899135446,"F
Negative results possible on finite parameter subspaces"
REFERENCES,0.9567723342939481,"In §3.2, we mention that the pathological EBM ˜푏can show negative results regarding ﬁnite subsets of 횯.
Here, we provide a concrete example."
REFERENCES,0.9596541786743515,"Corollary 2. Assuming ZFC axioms and assuming they are consistent, there exists 휽∈횯such that
푍˜푝휽∈Q>0 exists, but there is no algorithm ˆ푍proof that takes 휽∈횯as input, and outputs a set of strings
{풙푛: 푛< 푁}, 푁∈N where"
REFERENCES,0.962536023054755,"• there is a proof that Í푁
푛=1 ˜푝휽(풙) ≥1/2푍˜푝휽; or
• there is a proof that Í푁
푛=1 ˜푝휽(풙) < 1/2푍˜푝휽."
REFERENCES,0.9654178674351584,"Proof. Assuming to the contrary that ˆ푍proof existed. We construct 휽∈횯such that ∀풙∈B∗, ˜푝휽(풙) = ˜푏(풙).
Either proof resulting from ˆ푍proof(휽): {풙푛: 푛< 푁} can be used to prove or disprove the consistency of
ZFC.
□"
REFERENCES,0.968299711815562,"Corollary 2 states that there exists a ‘problematic EBM’ — namely ˜푏— where we cannot guarantee to well
approximate its partition function, by accumulating ﬁnitely many string weights, regardless of the manner
of accumulation (i.e., how we choose strings) or the number of strings we enumerate over. We discuss this
in further details in §2.5."
REFERENCES,0.9711815561959655,"G
Impossibility of model selection in fixed-size Transformer EBM families"
REFERENCES,0.9740634005763689,"Theorem 11 is a sibling theorem to Theorem 8. Just as we show 푍˜푏is uncomputable (§3.2), we can prove
that model selection is not only impossible for EC-complete parametric families (where the length of a
parameter vector is unbounded), but also impossible for ﬁxed-size Transformer EBMs with a large enough"
REFERENCES,0.9769452449567724,Published as a conference paper at ICLR 2022
REFERENCES,0.9798270893371758,embedding size.15
REFERENCES,0.9827089337175793,"Theorem 11. Assuming ZFC as our axiomatic system, for any ˜푝∈EC, there exists 푑0 ∈N such that for
all 푑≥푑0, ˜푝can be captured by one-encoder-layer four-decoder-layer Transformer networks 횯(푑)"
REFERENCES,0.9855907780979827,"(Theorem 1) with embedding size 푑, where there is no provably correct algorithm Better ˜푝,푑that takes
two parameter vectors 휽1 ∈횯(푑), 휽2 ∈횯(푑), and returns YES if KL(푝||푝휽1) ≥KL(푝||푝휽2), and NO
otherwise, where 푝, 푝휽1, 푝휽2 are string distributions deﬁned by ˜푝, ˜푝휽1, ˜푝휽2 respectively."
REFERENCES,0.9884726224783862,"Proof. Let 푀푏be the input-free unweighted Turing machine we built in our proof of Theorem 3, whose
behavior is independent of ZFC. Let 푀0 be any weighted Turing machine that deﬁnes the EC weighted
language ˜푝. And let 푀1 be a weighted Turing machine that weights 풙as ˜푝(풙) + 1 if and only if 풙encodes
an accepting trace of 푀푏; and 푀1 weights 풙as ˜푝(풙) otherwise. Since checking whether 풙is a valid trace
of 푀푏is in 푂(poly(|풙|)), 푀1 deﬁnes an EC weighted language."
REFERENCES,0.9913544668587896,"Let 푝푀1 be the string distribution deﬁned by 푀1. By an argument similar to our proof of Theorem 8, no
algorithm that is provably correct can decide if KL(푝||푝푀1) ≥KL(푝||푝)."
REFERENCES,0.9942363112391931,"We note that for any weighted Turing machine 푀with fewer than 푛states, we can build another weighted
Turing machine 푀′ which has 푛states, such that 푀and 푀′ deﬁne the same weighted language, simply by
having (ﬁnitely many) additional unreachable states in 푀′. Since any weighted Turing machine with 푛
states can be implemented as a 1-encoder-layer-4-decoder-layer Transformer networks with an embedding
size ∈푂(푛), it follows that there exists 푑0 ∈N such that both 푀0 and 푀1 can be encoded as parameter
vectors within a ﬁxed-size model family with 푑≥푑0.
□"
REFERENCES,0.9971181556195965,15We use 횯(푑) to denote a Transformer family with embedding size 푑.
