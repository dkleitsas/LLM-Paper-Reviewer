Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008403361344537816,"Outlier detection in high-dimensional tabular data is an important task in data min-
1"
ABSTRACT,0.0016806722689075631,"ing, essential for many downstream tasks and applications. Existing unsupervised
2"
ABSTRACT,0.0025210084033613447,"outlier detection algorithms face one or more problems, including inlier assumption
3"
ABSTRACT,0.0033613445378151263,"(IA), curse of dimensionality (CD), and multiple views (MV). To address these
4"
ABSTRACT,0.004201680672268907,"issues, we introduce Generative Subspace Adversarial Active Learning (GSAAL),
5"
ABSTRACT,0.005042016806722689,"a novel approach that uses a Generative Adversarial Network with multiple ad-
6"
ABSTRACT,0.0058823529411764705,"versaries. These adversaries learn the marginal class probability functions over
7"
ABSTRACT,0.0067226890756302525,"different data subspaces, while a single generator in the full space models the entire
8"
ABSTRACT,0.007563025210084034,"distribution of the inlier class. GSAAL is specifically designed to address the MV
9"
ABSTRACT,0.008403361344537815,"limitation while also handling the IA and CD, making it the only method to address
10"
ABSTRACT,0.009243697478991597,"all three. We provide a mathematical formulation of MV, theoretical guarantees
11"
ABSTRACT,0.010084033613445379,"for the training, and scalability analysis for GSAAL. Our extensive experiments
12"
ABSTRACT,0.010924369747899159,"demonstrate the effectiveness and scalability of GSAAL, highlighting its superior
13"
ABSTRACT,0.011764705882352941,"performance compared to other popular OD methods, especially in MV scenarios.
14"
INTRODUCTION,0.012605042016806723,"1
Introduction
15"
INTRODUCTION,0.013445378151260505,"Outlier detection (OD), a fundamental and widely recognized issue in data mining, involves the
16"
INTRODUCTION,0.014285714285714285,"identification of anomalous or deviating data points within a dataset. Outliers are typically defined
17"
INTRODUCTION,0.015126050420168067,"as low-probability occurrences within a population [41, 19]. In the absence of access to the true
18"
INTRODUCTION,0.015966386554621848,"probability distribution of the data points, OD algorithms rely on constructing a scoring function.
19"
INTRODUCTION,0.01680672268907563,"Points with higher scores are more likely to be outliers. Existing unsupervised OD algorithms have
20"
INTRODUCTION,0.01764705882352941,"one or more of the following problems, in high-dimensional tabular data scenarios.
21"
INTRODUCTION,0.018487394957983194,"• The inlier assumption (IA): OD algorithms often make assumptions about what constitutes
22"
INTRODUCTION,0.019327731092436976,"an inlier, which can be challenging to verify and validate [30].
23"
INTRODUCTION,0.020168067226890758,"• The curse of dimensionality (CD): As the dimensionality of data increases, the challenge of
24"
INTRODUCTION,0.02100840336134454,"identifying outliers intensifies, decreasing the effectiveness of certain OD algorithms [2]
25"
INTRODUCTION,0.021848739495798318,"• Multiple Views (MV): Outliers are often only visible in certain ""views"" of the data and are
26"
INTRODUCTION,0.0226890756302521,"hidden in the full space of original features [31]
27"
INTRODUCTION,0.023529411764705882,"We now explain these problems one by one.
28"
INTRODUCTION,0.024369747899159664,"The inlier assumption poses a challenge to algorithms that assume a standard profile of the inlier
29"
INTRODUCTION,0.025210084033613446,"data. For example, angle-based algorithms like ABOD [24] assume that inliers have other inliers
30"
INTRODUCTION,0.026050420168067228,"at all angles. Similarly, neighbor-based algorithms like kNN [34] assume that inliers have other
31"
INTRODUCTION,0.02689075630252101,"neighboring points nearby. These assumptions influence the scoring as it measures the degree to
32"
INTRODUCTION,0.02773109243697479,"which a sample deviates from this assumed norm. Consequently, the performance of these algorithms
33"
INTRODUCTION,0.02857142857142857,Figure 1: Scatterplots of the dataset from example 1.
INTRODUCTION,0.029411764705882353,"may degrade if these assumptions do not hold [30]. This means that a general OD method should not
34"
INTRODUCTION,0.030252100840336135,"make any inlier assumptions.
35"
INTRODUCTION,0.031092436974789917,"The curse of dimensionality [2] refers to the decrease in the relative proximity of data points as the
36"
INTRODUCTION,0.031932773109243695,"number of dimensions increases. Simply put, with high dimensionality, the distance between any pair
37"
INTRODUCTION,0.03277310924369748,"of points becomes similar, regardless of whether none, one, or both of the points in a pair are outliers.
38"
INTRODUCTION,0.03361344537815126,"This is particularly problematic for OD algorithms that rely on distances or on identifying neighbors
39"
INTRODUCTION,0.034453781512605045,"to detect outliers, such as density- (e.g., LOF [3]), neighbor- (e.g., kNN [34]), and cluster-based (e.g.,
40"
INTRODUCTION,0.03529411764705882,"SVDD [1, Chapter 2]) OD algorithms.
41"
INTRODUCTION,0.03613445378151261,"Multiple Views refers to the phenomenon that certain complex correlations between features are only
42"
INTRODUCTION,0.03697478991596639,"observable in some feature subspaces [31]. As detailed in [1], this occurs when the dataset contains
43"
INTRODUCTION,0.037815126050420166,"additional irrelevant features, making some outliers only detectable in certain subspaces. In scenarios
44"
INTRODUCTION,0.03865546218487395,"where multiple subspaces contain different interesting structures, this problem is exacerbated. It then
45"
INTRODUCTION,0.03949579831932773,"becomes increasingly difficult to explain the variability of a data point based solely on its behavior in
46"
INTRODUCTION,0.040336134453781515,"a single subspace [23]. This problem can occur regardless of the dimensionality of the dataset if the
47"
INTRODUCTION,0.041176470588235294,"number of points is insufficient to capture a complex correlation structure.
48"
INTRODUCTION,0.04201680672268908,"The following example illustrates the three problems described above
49"
INTRODUCTION,0.04285714285714286,"Example 1 (Effect of MV, IA and CD). Consider the random variables x1, x2 and x3, where x1 and
50"
INTRODUCTION,0.043697478991596636,"x2 are highly correlated and x3 is Gaussian noise. Figure 1 plots datasets with 20, 100 and 1000
51"
INTRODUCTION,0.04453781512605042,"realizations of (x1, x2, x3). It also contains the classification boundaries from both a locality-based
52"
INTRODUCTION,0.0453781512605042,"method (green) and a cluster-based method (red) in the subspace. The cluster-based detector fitted in
53"
INTRODUCTION,0.046218487394957986,"the full 3D space fails to detect the outlier shown in the figure (red cross). However, the outlier is
54"
INTRODUCTION,0.047058823529411764,"always detected in the 2D subspace, as we can see. Once we increase the number of samples over
55"
INTRODUCTION,0.04789915966386555,"n = 1000, the cluster-based method detects the outlier in the full space (MV). On the contrary, the
56"
INTRODUCTION,0.04873949579831933,"locality-based method could not detect the outlier in any tested scenario (MV + IA). If we increase
57"
INTRODUCTION,0.04957983193277311,"the dimensionality by adding more features consisting of noise, no method can detect the outlier in
58"
INTRODUCTION,0.05042016806722689,"the full space (MV + IA + CD).
59"
INTRODUCTION,0.05126050420168067,"We are interested in tackling outlier detection whenever a population exhibits MV, like [31, 23, 25]
60"
INTRODUCTION,0.052100840336134456,"and as showcased in [1]. Particularly, the goal of this paper is to propose the first outlier detection
61"
INTRODUCTION,0.052941176470588235,"method that explicitly addresses IA, CD, and MV simultaneously.
62"
INTRODUCTION,0.05378151260504202,"As we will explain in the next section, we build on Generative Adversarial Active Learning
63"
INTRODUCTION,0.0546218487394958,"(GAAL) [44], a widely used approach for outlier detection [30, 17, 39]. It involves training a
64"
INTRODUCTION,0.05546218487394958,"Generative Adversarial Network (GAN) to mimic the distribution of outlier data, and it enhances
65"
INTRODUCTION,0.05630252100840336,"the discriminator’s performance through active learning [38], leveraging the GAN’s data generation
66"
INTRODUCTION,0.05714285714285714,"capability. GAAL methods avoid IA [30] and use the multi-layered structure of the GAN to overcome
67"
INTRODUCTION,0.05798319327731093,"the curse of dimensionality [33]. However, they often miss important subspaces, leading to MV.
68"
INTRODUCTION,0.058823529411764705,"Challenges.
Training multiple GAN-based models in individual subspaces is not trivial. (1) The
69"
INTRODUCTION,0.05966386554621849,"joint training of generators and discriminators in GANs requires careful monitoring to determine
70"
INTRODUCTION,0.06050420168067227,"the optimal stopping point, a task that becomes daunting for large ensembles. (2) The generation of
71"
INTRODUCTION,0.06134453781512605,"difficult-to-detect points in a subspace remains hard [40]. (3) While several authors have proposed
72"
INTRODUCTION,0.06218487394957983,Table 1: Families of OD methods with the limitations they address.
INTRODUCTION,0.06302521008403361,"Type
IA
CD
MV"
INTRODUCTION,0.06386554621848739,"Classical
✗
✗
✗
Subspace
✗
✓
✓
Generative w/ uniform distribution
✓
✗
✗
Generative w/ param. distribution
✗
✓
✗
Generative w/ subspace behavior
✗
✓
✓
GAAL
✓
✓
✗
GSAAL (Our method)
✓
✓
✓"
INTRODUCTION,0.06470588235294118,"multi-adversarial architectures for GANs [11, 5], none of them address adversaries tailored to
73"
INTRODUCTION,0.06554621848739496,"subspaces composed of feature subsets. Furthermore, these methods may not be suitable for GAAL
74"
INTRODUCTION,0.06638655462184874,"since they do not have convergence guarantees for detectors, as we will explain.
75"
INTRODUCTION,0.06722689075630252,"Contributions.
(1) We propose GSAAL (Generative Subspace Adversarial Active Learning), a
76"
INTRODUCTION,0.0680672268907563,"novel GAAL method that uses multiple adversaries to learn the marginal inlier probability functions
77"
INTRODUCTION,0.06890756302521009,"in different data subspaces. Each adversary focuses on a single subspace. Simultaneously, we train
78"
INTRODUCTION,0.06974789915966387,"a single generator in the full space to approximate the entire distribution of the inlier class. All
79"
INTRODUCTION,0.07058823529411765,"networks are trained end-to-end, avoiding the ensembling problem. (2) To our knowledge, we give
80"
INTRODUCTION,0.07142857142857142,"the first mathematical formulation of the “multiple views” problem. We used it to show the ability of
81"
INTRODUCTION,0.07226890756302522,"GSAAL to mitigate the MV problem. (3) We formulate the novel optimization problem for GSAAL
82"
INTRODUCTION,0.073109243697479,"and give convergence guarantees of each discriminator to the marginal distribution of its respective
83"
INTRODUCTION,0.07394957983193277,"subspace. We also analyze the worst-case complexity of the method. (4) In extensive experiments we
84"
INTRODUCTION,0.07478991596638655,"compare GSAAL with multiple competitors. GSAAL was the only method capable of consistently
85"
INTRODUCTION,0.07563025210084033,"detecting anomalous data under MV. Furthermore, on 22 popular benchmark datasets for the one-class
86"
INTRODUCTION,0.07647058823529412,"classification task, GSAAL demonstrated SOTA-level performance and was orders of magnitude
87"
INTRODUCTION,0.0773109243697479,"faster in inference than its best competitors. (5) Our code is publicly available.1
88"
INTRODUCTION,0.07815126050420168,"Paper outline: Section 2 reviews related work, Section 3 contains the theoretical results for our method,
89"
INTRODUCTION,0.07899159663865546,"Section 4 features our experimental results, and Section 5 concludes and addresses limitations.
90"
RELATED WORK,0.07983193277310924,"2
Related Work
91"
RELATED WORK,0.08067226890756303,"This section is a brief overview of popular unsupervised outlier detection methods for tabular data
92"
RELATED WORK,0.08151260504201681,"related to our approach. We categorize them based on their ability to address the specific limitations
93"
RELATED WORK,0.08235294117647059,"outlined above. Table 1 is a comparative summary. Further comments about OD in other data types
94"
RELATED WORK,0.08319327731092437,"can be found in the appendix.
95"
RELATED WORK,0.08403361344537816,"Classical Methods
Conventional outlier detection approaches, such as distance-based strategies
96"
RELATED WORK,0.08487394957983194,"like LOF and KNN, angle-based techniques like ABOD, and cluster-based methods like SVDD,
97"
RELATED WORK,0.08571428571428572,"rely on specific assumptions on the behavior of inlier data. They use a scoring function to measure
98"
RELATED WORK,0.0865546218487395,"deviations from this assumed norm. These methods face the inlier assumption limitation by definition.
99"
RELATED WORK,0.08739495798319327,"For example, local methods that assume isolated outliers fail when several outlying samples fall
100"
RELATED WORK,0.08823529411764706,"together. In addition, many classical methods, which rely on measuring distances, are susceptible to
101"
RELATED WORK,0.08907563025210084,"the curse of dimensionality. Both limitations impair the effectiveness of these methods [30].
102"
RELATED WORK,0.08991596638655462,"Subspace Methods
Subspace-based methods [25] operate in lower-dimensional subspaces formed
103"
RELATED WORK,0.0907563025210084,"by subsets of features. They effectively counteract the curse of dimensionality by focusing on
104"
RELATED WORK,0.09159663865546218,"identifying so-called “subspace outliers” [22]. These outliers, which are prevalent in high-dimensional
105"
RELATED WORK,0.09243697478991597,"datasets with many correlated features, are often elusive to conventional non-subspace methods [29,
106"
RELATED WORK,0.09327731092436975,"31]. However, existing subspace methods inherently operate on specific assumptions on the nature of
107"
RELATED WORK,0.09411764705882353,"anomalies in each subspace they explore, and thus face the inlier assumption limitation.
108"
RELATED WORK,0.0949579831932773,"Generative Methods
A common strategy to mitigate the IA and CD limitations is to reframe the
109"
RELATED WORK,0.0957983193277311,"task as a classification task using self-supervision. A prevalent self-supervised technique, particularly
110"
RELATED WORK,0.09663865546218488,1https://anonymous.4open.science/r/GSAAL-8D6E
RELATED WORK,0.09747899159663866,"for tabular data, is the generation of artificial outliers [13, 30]. This method involves distinguishing
111"
RELATED WORK,0.09831932773109243,"between actual training data and artificially generated data drawn from a predetermined “reference
112"
RELATED WORK,0.09915966386554621,"distribution”. [21] showed that by approximating the class probability of being a real sample, one
113"
RELATED WORK,0.1,"approximates the probability function of being an inlier. One then uses this approximation as a
114"
RELATED WORK,0.10084033613445378,"scoring function [30]. However, it is not easy to find the right reference distribution, and a poor
115"
RELATED WORK,0.10168067226890756,"choice can affect OD by much [21].
116"
RELATED WORK,0.10252100840336134,"A first approach to this challenge proposed the use of naïve reference distributions by uniformly
117"
RELATED WORK,0.10336134453781512,"generating data in the space. This approach showed promising results in low-dimensional spaces but
118"
RELATED WORK,0.10420168067226891,"failed in high dimensions due to the curse of dimensionality [21]. Other approaches, such as assuming
119"
RELATED WORK,0.10504201680672269,"parametric distributions for inlier data [1, Chapter 2] or directly generating in susbpaces [12], can
120"
RELATED WORK,0.10588235294117647,"avoid CD when the parametric assumptions are met. Methods that generate in the subspaces can
121"
RELATED WORK,0.10672268907563025,"model the subspace behavior, additionally tackling the MV limitation. However, these last two
122"
RELATED WORK,0.10756302521008404,"approaches do not address the IA limitation, as they make specific assumptions about the behavior of
123"
RELATED WORK,0.10840336134453782,"the inlier data.
124"
RELATED WORK,0.1092436974789916,"Generative Adversarial Active Learning
According to [21], the closer the reference distribution
125"
RELATED WORK,0.11008403361344538,"is to the inlier distribution, the better the final approximation to the inlier probability function will
126"
RELATED WORK,0.11092436974789915,"be. Hence, recent developments in generative methods have focused on learning the reference
127"
RELATED WORK,0.11176470588235295,"distribution in conjunction with the classifier. A key approach is the use of Generative Adversarial
128"
RELATED WORK,0.11260504201680673,"Networks (GANs), where the generator converges to the inlier distribution [15]. The most common
129"
RELATED WORK,0.1134453781512605,"approaches for this are GAAL-based methods [30, 17, 39]. These methods differentiate themselves
130"
RELATED WORK,0.11428571428571428,"from other GANs for OD by training the detectors using active learning after normal convergence of
131"
RELATED WORK,0.11512605042016806,"the GAN [36, 10]. The architecture of GAAL inherently addresses the curse of dimensionality, as
132"
RELATED WORK,0.11596638655462185,"GANs can incorporate layers designed to manage high-dimensional data [33]. In practice, GAAL-
133"
RELATED WORK,0.11680672268907563,"based methods outperformed all their competitors in their original work. However, they overlook the
134"
RELATED WORK,0.11764705882352941,"behavior of the data in subspaces and therefore may be susceptible to MV.
135"
RELATED WORK,0.11848739495798319,"Our method, GSAAL, incorporates several subspace-focused detectors into GAAL. These detectors
136"
RELATED WORK,0.11932773109243698,"approximate the marginal inlier probability functions of their subspaces. Thus, GSAAL effectively
137"
RELATED WORK,0.12016806722689076,"addresses MV while inheriting GAAL’s ability to overcome IA and CD limitations.
138"
RELATED WORK,0.12100840336134454,"3
Our Method: GSAAL
139"
RELATED WORK,0.12184873949579832,"We first formalize the notion of data exhibiting multiple views. We then use it to design our
140"
RELATED WORK,0.1226890756302521,"outlier detection method, GSAAL, and give convergence guarantees. Finally, we derive the runtime
141"
RELATED WORK,0.12352941176470589,"complexity of GSAAL. All the proofs and extra derivations can be found in the technical appendix.
142"
MULTIPLE VIEWS,0.12436974789915967,"3.1
Multiple Views
143"
MULTIPLE VIEWS,0.12521008403361344,"Several authors [1, 31, 23, 25, 29] have observed that at times the variability of the data can only be
144"
MULTIPLE VIEWS,0.12605042016806722,"explained from its behavior in some subspaces. Researchers variably call this problem “the subspace
145"
MULTIPLE VIEWS,0.126890756302521,"problem” [1, 25] or “multiple views of the data” [22, 31]. Previous research has largely focused on
146"
MULTIPLE VIEWS,0.12773109243697478,"practical scenarios, leaving aside the need for a formal definition. In response, we propose a unifying
147"
MULTIPLE VIEWS,0.12857142857142856,"definition of “multiple views” that provides a foundation for developing methods to address this
148"
MULTIPLE VIEWS,0.12941176470588237,"challenge effectively.
149"
MULTIPLE VIEWS,0.13025210084033614,"The problem “multiple views” of data (MV) arises from two different effects. First, it involves the
150"
MULTIPLE VIEWS,0.13109243697478992,"ability to understand the behavior of a random vector x by examining lower-dimensional subsets of
151"
MULTIPLE VIEWS,0.1319327731092437,"its components (x1, . . . , xd). Second, it stems from the challenge of insufficient data to obtain an
152"
MULTIPLE VIEWS,0.13277310924369748,"effective scoring function in the full space of x. As Example 1 shows, combining these two effects
153"
MULTIPLE VIEWS,0.13361344537815126,"obscures the behavior of the data in the full space. Hence, methods not considering subspaces when
154"
MULTIPLE VIEWS,0.13445378151260504,"building their scoring function may have issues detecting outliers under MV. The next definition
155"
MULTIPLE VIEWS,0.13529411764705881,"formalizes the first effect.
156"
MULTIPLE VIEWS,0.1361344537815126,"Definition 1 (myopic distribution). Consider a random vector x : Ω−→Rd and Diagd×d({0, 1}),
157"
MULTIPLE VIEWS,0.1369747899159664,"the set of diagonal binary matrices without the identity. If there exists a random matrix u : Ω−→
158"
MULTIPLE VIEWS,0.13781512605042018,"Diagd×d({0, 1}), such that
159"
MULTIPLE VIEWS,0.13865546218487396,"px(x) = pux(ux) for almost all x,
(1)
we say that the distribution of x is myopic to the views of u. Here, x and ux are realizations of x
160"
MULTIPLE VIEWS,0.13949579831932774,"and ux, and px and pux are the pdfs of x and ux.
161"
MULTIPLE VIEWS,0.1403361344537815,"It is clear that, under MV, using pux to build a scoring function instead of px mitigates the effects.
162"
MULTIPLE VIEWS,0.1411764705882353,"This comes as the subspaces selected by u are smaller in dimensionality. Hence it should take fewer
163"
MULTIPLE VIEWS,0.14201680672268907,"samples to approximate the pdf of ux. The difficulty is that it is not yet clear how to approximate
164"
MULTIPLE VIEWS,0.14285714285714285,"pux. The following proposition elaborates on a way to do so. It states that by averaging a collection
165"
MULTIPLE VIEWS,0.14369747899159663,"of marginal distributions of x in the subspaces given by realizations of u, one can approximate the
166"
MULTIPLE VIEWS,0.14453781512605043,"distribution of pux.
167"
MULTIPLE VIEWS,0.1453781512605042,"Proposition 1. Let x and u be as before with px myopic to the views of u. Consider a set of
168"
MULTIPLE VIEWS,0.146218487394958,"independent realizations of u: {ui}k
i=1. Then 1 k
P"
MULTIPLE VIEWS,0.14705882352941177,"i puix(uix) is an unbiased statistic for pux(ux).
169"
MULTIPLE VIEWS,0.14789915966386555,"MV appears when there is a lack of data, and its distribution is myopic. To improve OD under MV,
170"
MULTIPLE VIEWS,0.14873949579831933,"one can exploit the distribution myopicity to model x in the subspaces, where less data is sufficient.
171"
MULTIPLE VIEWS,0.1495798319327731,"Proposition 1 gives us a way to do so, by approximating pux. In this way, under myopicity, this also
172"
MULTIPLE VIEWS,0.15042016806722688,"approximates px, avoiding MV. Our method, GSAAL, exploits these derivations, as we explain next.
173"
GSAAL,0.15126050420168066,"3.2
GSAAL
174"
GSAAL,0.15210084033613444,"GAAL methods tackle IA by being agnostic to outlier definition and mitigate CD through the use of
175"
GSAAL,0.15294117647058825,"multilayer neural networks [30, 28, 33]. GAAL methods have two steps:
176"
GSAAL,0.15378151260504203,"1. Training of the GAN. Train the GAN consisting of one generator G and one detector D using
177"
GSAAL,0.1546218487394958,"the usual min-max optimization problem as in [15].
178"
GSAAL,0.15546218487394958,"2. Training of the detector through active learning. After convergence, G is fixed, and D
179"
GSAAL,0.15630252100840336,"continues to train. This last step is an active learning procedure with [44]. Following [21],
180"
GSAAL,0.15714285714285714,"D(x) now approximates the pdf of the training data px.
181"
GSAAL,0.15798319327731092,"After Step 2, the detector converges to px. However, our goal is to approximate px by exploiting
182"
GSAAL,0.1588235294117647,"a supposed myopicity of the distribution. We extend GAAL methods to also address MV in what
183"
GSAAL,0.15966386554621848,"follows. The following theorem adapts the objective function of the GAN to the subspace case and
184"
GSAAL,0.16050420168067228,"gives guarantees that the detectors converge to the marginal pdfs used in Proposition 1:
185"
GSAAL,0.16134453781512606,"Theorem 1. Consider x and u as in the previous definition, with x a realization of x and {ui}i a set
186"
GSAAL,0.16218487394957984,"of realizations of u. Consider a generator G : z ∈Z 7−→G(z) ∈Rd and {Di}, i = 1, . . . , k, a set
187"
GSAAL,0.16302521008403362,"of detectors such as Di : uix ∈Si ⊂Rd 7−→Di(uix) ∈[0, 1]. Z is an arbitrary noise space where
188"
GSAAL,0.1638655462184874,"G randomly samples from. Consider the following optimization problem
189"
GSAAL,0.16470588235294117,"min
G max
Di, ∀i X"
GSAAL,0.16554621848739495,"i
V (G, Di) ="
GSAAL,0.16638655462184873,"min
G max
Di, ∀i X"
GSAAL,0.1672268907563025,"i
Euix log Di(uix) + Ez log (1 −Di (uiG(z))) ,
(2)"
GSAAL,0.16806722689075632,"where each addend V (G, Di) is the binary cross entropy in each subspace. Under these conditions,
190"
GSAAL,0.1689075630252101,"the following holds:
191"
GSAAL,0.16974789915966387,"i) Each detector in optimum is D∗
i (uix) = 1"
GSAAL,0.17058823529411765,"2, ∀x. Thus, in optimum V (G, Di) = −log(4), ∀i.
192"
GSAAL,0.17142857142857143,"ii) Each individual Di converges to D∗
i (uix) = puix(uix) after trained in Step 2 of a GAAL
193"
GSAAL,0.1722689075630252,"method.
194"
GSAAL,0.173109243697479,iii) D∗(x) = 1
GSAAL,0.17394957983193277,"k
Pk
i=1 D∗
i (uix) approximates pux(ux). If px is myopic, D∗(x) also approxi-
195"
GSAAL,0.17478991596638654,"mates px(x).
196"
GSAAL,0.17563025210084032,"Using Theorem 1 we can extend the GAAL methods to the subspace case:
197"
GSAAL,0.17647058823529413,"1. Training the GAN. Train a GAN with one generator G and multiple detectors {Di} with
198"
GSAAL,0.1773109243697479,"Equation (2) as the objective function. The training of each detector stops when the loss
199"
GSAAL,0.1781512605042017,"reaches its value with the optimum in Statement (i).
200"
GSAAL,0.17899159663865546,"2. Training of the k detectors by active learning. Train each Di as in Step 2 of a regular GAAL
201"
GSAAL,0.17983193277310924,"method using G. By Statement (ii) of the Theorem, each Di will approximate puix. By
202"
GSAAL,0.18067226890756302,"Statement (iii), D(x) = 1"
GSAAL,0.1815126050420168,"k
Pk
i=1 Di(uix) will approximate px under the myopicity of the
203"
GSAAL,0.18235294117647058,"data.
204"
GSAAL,0.18319327731092436,"We call this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL).
205"
GSAAL,0.18403361344537816,"The appendix contains the pseudo-code for GSAAL.
206"
COMPLEXITY,0.18487394957983194,"3.3
Complexity
207"
COMPLEXITY,0.18571428571428572,"In this section, we focus on studying the theoretical complexity of GSAAL. We study both its usability
208"
COMPLEXITY,0.1865546218487395,"for training and, more importantly, for inference.
209"
COMPLEXITY,0.18739495798319328,"Theorem 2. Consider our GSAAL method with generator G and detectors {Di}k
i=1, each with four
210"
COMPLEXITY,0.18823529411764706,"fully connected hidden layers, √n nodes in the detectors and d in the generator. Let D be the training
211"
COMPLEXITY,0.18907563025210083,"data for GSAAL, with n data points and d features. Then the following holds:
212"
COMPLEXITY,0.1899159663865546,"i) Time complexity of training is O(ED · n · (k · n + d2)). ED is an unknown complexity
213"
COMPLEXITY,0.1907563025210084,"variable depicting the unique epochs to convergence for the network in dataset D.
214"
COMPLEXITY,0.1915966386554622,"ii) Time complexity of single sample inference is in O(k · n), with k the number of detectors
215"
COMPLEXITY,0.19243697478991598,"used.
216"
COMPLEXITY,0.19327731092436976,"The linear inference times make GSAAL particularly appealing in situations where the model can be
217"
COMPLEXITY,0.19411764705882353,"trained once for each dataset, like one-class classification. We build on this particular strength in the
218"
COMPLEXITY,0.1949579831932773,"following section.
219"
EXPERIMENTS,0.1957983193277311,"4
Experiments
220"
EXPERIMENTS,0.19663865546218487,"This section presents experiments with GSAAL. We will outline the experimental setting, and examine
221"
EXPERIMENTS,0.19747899159663865,"the handling of “multiple views” in GSAAL and other OD methods. We then evaluate GSAAL’s
222"
EXPERIMENTS,0.19831932773109243,"performance against various OD methods and investigate its scalability. The appendix includes a
223"
EXPERIMENTS,0.1991596638655462,"study on the sensitivity to the number of detectors, IA experiments, an ablaition study and extra
224"
EXPERIMENTS,0.2,"competitors evaluated in the real world datasets. System specifications are included in the appendix.
225"
EXPERIMENTAL SETTING,0.2008403361344538,"4.1
Experimental Setting
226"
EXPERIMENTAL SETTING,0.20168067226890757,"This section has three parts: First, we describe the synthetic and real data for the outlier detection
227"
EXPERIMENTAL SETTING,0.20252100840336135,"experiments. Then, we describe the configuration of GSAAL. Finally, we present our competitors.
228"
DATASETS,0.20336134453781513,"4.1.1
Datasets
229"
DATASETS,0.2042016806722689,"Synthetic.
We constructed synthetic datasets, each containing two correlated features, x1 and x2,
230"
DATASETS,0.20504201680672268,"along with 58 independent features xj, j = 3, . . . , 60 consisting of Gaussian noise. This approach
231"
DATASETS,0.20588235294117646,"simulates datasets that exhibit the MV property by adding irrelevant features into a pair of highly
232"
DATASETS,0.20672268907563024,"correlated variables. We detail the methodology and all correlation patterns in the technical appendix.
233"
DATASETS,0.20756302521008405,"Real.
We selected 22 real-world tabular datasets for our experiments from [19]. The selection
234"
DATASETS,0.20840336134453782,"criteria included datasets with less than 10,000 data points, more than 10 outliers, and more than 15
235"
DATASETS,0.2092436974789916,"features, focusing on high-dimensional data while keeping the runtime (of competing OD methods)
236"
DATASETS,0.21008403361344538,"tractable. Table 2a contains the summary of the datasets. For datasets with multiple versions, we chose
237"
DATASETS,0.21092436974789916,"the first in alphanumeric order. Details about each dataset are available in the original source [19].
238"
NETWORK SETTINGS,0.21176470588235294,"4.1.2
Network Settings
239"
NETWORK SETTINGS,0.21260504201680672,"Structure.
Unless stated otherwise, GSAAL uses the following network architecture. It consists of
240"
NETWORK SETTINGS,0.2134453781512605,"four fully connected layers with ReLu activation functions used in the generator and the detectors.
241"
NETWORK SETTINGS,0.21428571428571427,"Each layer in k = 2
√"
NETWORK SETTINGS,0.21512605042016808,"d detectors has √n nodes, where n and d are the number of data points
242"
NETWORK SETTINGS,0.21596638655462186,"and features in the training set, respectively. This configuration ensures linear inference time. The
243"
NETWORK SETTINGS,0.21680672268907564,"generator has d nodes in each layer, a standard in GAAL approaches, which ensures polynomial
244"
NETWORK SETTINGS,0.21764705882352942,"training times. We assumed u to be distributed uniformly across all subspaces. Therefore, we
245"
NETWORK SETTINGS,0.2184873949579832,"obtained each subspace for the detectors by drawing uniformly from the set of all subspaces.
246"
NETWORK SETTINGS,0.21932773109243697,"Training.
Like other GAAL methods [30, 44], we train the generator G together with all the
247"
NETWORK SETTINGS,0.22016806722689075,"detectors Di until the loss of G stabilizes. Then we train each detector Di until convergence with
248"
NETWORK SETTINGS,0.22100840336134453,"G fixed. To automate this process, we introduce an early stopping criterion: Training stops when a
249"
NETWORK SETTINGS,0.2218487394957983,"detector’s loss approaches the theoretical optimum (−log(4)), see statement (ii) of Theorem 1. For
250"
NETWORK SETTINGS,0.22268907563025211,"consistency across experiments, training parameters remain fixed unless otherwise noted. Specifically,
251"
NETWORK SETTINGS,0.2235294117647059,Table 2: Real-world datasets and Competitors
NETWORK SETTINGS,0.22436974789915967,(a) Real-world datasets converted to tabular if needed
NETWORK SETTINGS,0.22521008403361345,"Dataset
Category
Dataset
Category"
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.22605042016806723,"20news
Text
MNIST
Image
Annthyroid
Health
MVTec
Text
Arrhythmia
Cardiology
Optdigits
Image
Cardiot..
Cardiology
Satellite
Astronomy
CIFAR10
Image
Satimage-2
Astronomy
F-MNIST
Image
SpamBase
Document
Fault
Industrial
Speech
Linguistics
InternetAds
Image
SVHN
Image
Ionosphere
Weather
Waveform
Elect. Eng.
Landsat
Astronomy
WPBC
Oncology
Letter
Image
Hepatitis
Health"
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.226890756302521,(b) Competitors
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.22773109243697479,"Type
Competitors"
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.22857142857142856,"Classical
kNN, LOF
ABOD, OCSVM w/ rbf
Subspace
IForest, SOD
Gen., uniform dist.
NA (see the text)
Gen., parametric dist.
GMM
Gen., subspace behavior
NA (see the text)
GAAL
MO-GAAL"
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.22941176470588234,"the learning rates of the detectors and the generator are 0.01 and 0.001, respectively. We use minibatch
252"
"NEWS
TEXT
MNIST
IMAGE
ANNTHYROID
HEALTH
MVTEC
TEXT
ARRHYTHMIA
CARDIOLOGY
OPTDIGITS
IMAGE",0.23025210084033612,"gradient descent [14] optimization, with a batch size of 500.
253"
COMPETITORS,0.23109243697478993,"4.1.3
Competitors
254"
COMPETITORS,0.2319327731092437,"We selected popular and accessible methods from each category, as summarized in Table 2b, guided
255"
COMPETITORS,0.23277310924369748,"by related work. We excluded generative methods with uniform distributions because they prove
256"
COMPETITORS,0.23361344537815126,"ineffective for large datasets [21]. We could not include a generative method with subspace behavior
257"
COMPETITORS,0.23445378151260504,"due to operational issues with the most relevant method in this class, [12], caused by its outdated
258"
COMPETITORS,0.23529411764705882,"repository. We used the recommended parameters for all methods, as usual in OD [19].
259"
COMPETITORS,0.2361344537815126,"We used the pyod [43] library to access all competitors except MO-GAAL. We used MO-GAAL
260"
COMPETITORS,0.23697478991596638,"from its original source and implemented our method GSAAL in keras [6].
261"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.23781512605042016,"4.2
Effect of Multiple Views on Outlier Detection
262"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.23865546218487396,"To demonstrate the effectiveness of GSAAL under MV, we use synthetic datasets. Visualizing the
263"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.23949579831932774,"outlier scoring function in a 60-dimensional space is challenging, so we project it into the x1-x2
264"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24033613445378152,"subspace. A method adept at handling MV should have a boundary that accurately reflects the x1 and
265"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.2411764705882353,"x2 dependency structure. We first generate a synthetic dataset Dsynth as described in section 4.1.1
266"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24201680672268908,"and train the OD model. Using this model, we compute the scores for the points (x1, x2, 0, . . . , 0)
267"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24285714285714285,"and visualize the level curves on the x1-x2 plane.
268"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24369747899159663,"Figure 2 shows results for selected datasets and competitors, which are detailed in the Appendix. It
269"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.2445378151260504,"shows the level curves and decision boundaries (dashed lines) of the methods. Notably, our model
270"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.2453781512605042,"effectively detects correlations in the right subspace. To quantify this, we generated outliers in the
271"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.246218487394958,"subspace of interest and extra inliers. We tested the one-class classification performance of each
272"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24705882352941178,"method in 10 different MV datasets. On average, GSAAL managed to obtain 0.70 AUC, while the
273"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24789915966386555,"second-best performer (IForest) did not surpass a random classifier —0.49 AUC. All results and
274"
EFFECT OF MULTIPLE VIEWS ON OUTLIER DETECTION,0.24873949579831933,"further details can be found in section B.2 in the appendix.
275"
ONE-CLASS CLASSIFICATION,0.2495798319327731,"4.3
One-class Classification
276"
ONE-CLASS CLASSIFICATION,0.2504201680672269,"This section evaluates GSAAL on a one-class classification task [37]. First, we study the effectiveness
277"
ONE-CLASS CLASSIFICATION,0.25126050420168067,"of GSAAL on real data. Then, we investigate the scalability of GSAAL in practical scenarios.
278"
REAL-WORLD PERFORMANCE,0.25210084033613445,"4.3.1
Real-world Performance
279"
REAL-WORLD PERFORMANCE,0.2529411764705882,"We perform the outlier detection experiments on real datasets. Specifically, we take on the task of
280"
REAL-WORLD PERFORMANCE,0.253781512605042,"one-class classification, where the goal is to detect outliers by training only on a collection of inliers
281"
REAL-WORLD PERFORMANCE,0.2546218487394958,"[19]. To evaluate the performance of OD methods, we use AUC as it is robust to test data imbalance,
282"
REAL-WORLD PERFORMANCE,0.25546218487394956,"a common issue in OD tasks . The procedure is as follows:
283"
REAL-WORLD PERFORMANCE,0.25630252100840334,Figure 2: GSAAL finds classification boundaries for datasets banana and star under MV.
REAL-WORLD PERFORMANCE,0.2571428571428571,Table 3: Results of the Conover-Iman test for pairwise comparisons of the rankings.
REAL-WORLD PERFORMANCE,0.25798319327731095,"Method
ABOD
GSAAL
GMM
IForest
KNN
LOF
MO GAAL
OCSVM
SOD
ABOD
=
++
++
++
++
++
GSAAL
=
++
++
+
++
++
++
GMM
– –
– –
=
++
– –
– –
++
++
IForest
– –
– –
– –
=
– –
++
++
KNN
++
++
=
++
++
LOF
–
++
=
++
+
++
MO GAAL
– –
– –
– –
– –
– –
=
++
OCSVM
– –
– –
– –
–
=
++
SOD
– –
– –
– –
– –
– –
– –
– –
– –
="
REAL-WORLD PERFORMANCE,0.25882352941176473,"1. Split the dataset D into a training set Dtrain containing 80% of the inliers from D, and a test
284"
REAL-WORLD PERFORMANCE,0.2596638655462185,"set Dtest containing the remaining inliers and all outliers.
285"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2605042016806723,"2. Train an outlier detection model with Dtrain and evaluate its performance on Dtest with ROC
286"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.26134453781512607,"AUC.
287"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.26218487394957984,"To save space, we moved the detailed AUC results to the appendix; showing that GSAAL obtained
288"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2630252100840336,"the lowest median rank —see Figure 10 in the appendix. Although other subspace methods tend to
289"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2638655462184874,"perform better with irrelevant attributes [29, 25], they did not outperform classical OD methods on
290"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2647058823529412,"average in our experiments. Notably, ABOD, the second-best method in our experiments, performed
291"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.26554621848739496,"poorly in the MV tests (Section 4.2).
292"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.26638655462184874,"For statistical comparisons, we use the Conover-Iman post hoc test for pairwise comparisons be-
293"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2672268907563025,"tween multiple populations [7]. It is superior to the Nemenyi test due to its improved type I error
294"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2680672268907563,"boundings [8]. Conover-Iman test requires a preliminary positive result from a multiple population
295"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2689075630252101,"comparison test, for which we employ the Kruskal-Wallis test [26].
296"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.26974789915966385,"Table 3 shows the test results. In each cell, ‘+’ indicates that the method in the row has a significantly
297"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.27058823529411763,"lower median rank than the method in the column, while ‘−’ indicates a significantly higher median
298"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2714285714285714,"rank. One symbol indicates p-values ≤0.15 and two symbols indicate p-values ≤0.05. A blank
299"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2722689075630252,"indicates no significant difference. The table shows that GSAAL is superior to most of its competitors.
300"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.27310924369747897,"Our method does not significantly outperform the classical methods ABOD and kNN. However, these
301"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2739495798319328,"methods struggle to detect structures in subspaces, showing their inadequacy in dealing with the MV
302"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.2747899159663866,"limitation, see Section 4.2.
303"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.27563025210084036,"Overall, the results support GSAAL’s superiority in outlier detection tasks involving multiple views.
304"
TRAIN AN OUTLIER DETECTION MODEL WITH DTRAIN AND EVALUATE ITS PERFORMANCE ON DTEST WITH ROC,0.27647058823529413,"Additionally, they establish our method as the leading GAAL option for One-class classification
305"
SCALABILITY,0.2773109243697479,"4.3.2
Scalability
306"
SCALABILITY,0.2781512605042017,"In section 3.3, we derived that the inference time of GSAAL scales linearly with the number of
307"
SCALABILITY,0.27899159663865547,"training points if the number of detectors k is fixed, while it does not depend on the number of
308"
SCALABILITY,0.27983193277310925,"features d. This is in contrast to other methods, in particular LOF, KNN, and ABOD, which have
309"
SCALABILITY,0.280672268907563,"quadratic runtimes in d [3, 24]. We now validate this experimentally. The procedure is as follows:
310"
SCALABILITY,0.2815126050420168,"(a)
(b)"
SCALABILITY,0.2823529411764706,Figure 3: Plots of different performance metrics for scalability
SCALABILITY,0.28319327731092436,"1. Generate datasets Dtrain and Dtest consisting of random points. |Dtest| = 106.
311"
SCALABILITY,0.28403361344537814,"2. Train an OD method using Dtrain and record the inference time over Dtest.
312"
SCALABILITY,0.2848739495798319,"Following the result of the sensitivity study in our appendix, we fixed k = 30. Figure 3a plots the
313"
SCALABILITY,0.2857142857142857,"inference time of a single data point as a function of the number of features when |Dtrain| = 500.
314"
SCALABILITY,0.2865546218487395,"Figure 3b plots the inference time as a function of the number of points in Dtrain, for a fixed number of
315"
SCALABILITY,0.28739495798319326,"100 features. Both figures confirm our complexity derivations and show that GSAAL is particularly
316"
SCALABILITY,0.28823529411764703,"well-suited for large datasets.
317"
LIMITATIONS & CONCLUSIONS,0.28907563025210087,"5
Limitations & Conclusions
318"
LIMITATIONS AND FUTURE WORK,0.28991596638655465,"5.1
Limitations and Future Work
319"
LIMITATIONS AND FUTURE WORK,0.2907563025210084,"In section 4 we randomly selected subspaces for training the detectors in GSAAL, i.e. we took
320"
LIMITATIONS AND FUTURE WORK,0.2915966386554622,"a uniform distribution of u. This was already sufficient to demonstrate the highly competitive
321"
LIMITATIONS AND FUTURE WORK,0.292436974789916,"performance of our method. In practice, this assumption seemed to perform well for our experiments.
322"
LIMITATIONS AND FUTURE WORK,0.29327731092436976,"However, GSAAL can work with any subspace search strategy to obtain the distribution of u, for
323"
LIMITATIONS AND FUTURE WORK,0.29411764705882354,"example, the methods exploiting multiple views [23, 22]. We have not included them in this paper
324"
LIMITATIONS AND FUTURE WORK,0.2949579831932773,"due to the lack of an official implementation. In the future, we plan to benchmark various subspace
325"
LIMITATIONS AND FUTURE WORK,0.2957983193277311,"search methods in GSAAL.
326"
LIMITATIONS AND FUTURE WORK,0.2966386554621849,"Next, GSAAL is limited to tabular data, since the “multiple views” problem has only been observed
327"
LIMITATIONS AND FUTURE WORK,0.29747899159663865,"for this data type. The mathematical formulation of MV in section 3 does not exclude unstructured
328"
LIMITATIONS AND FUTURE WORK,0.29831932773109243,"data. The difficulty lies in identifying good search strategies for u for non-tabular data, which remains
329"
LIMITATIONS AND FUTURE WORK,0.2991596638655462,"an open question [18]. However, depending on the type of unstructured data, extending GSAAL to
330"
LIMITATIONS AND FUTURE WORK,0.3,"work with it is not immediate. Therefore, building a method that exploits the theoretical derivations
331"
LIMITATIONS AND FUTURE WORK,0.30084033613445377,"of GSAAL for structured data is future work.
332"
CONCLUSIONS,0.30168067226890755,"5.2
Conclusions
333"
CONCLUSIONS,0.3025210084033613,"Unsupervised outlier detection (OD) methods rely on a scoring function to distinguish inliers from
334"
CONCLUSIONS,0.3033613445378151,"outliers, since the true probability function that generated the dataset is usually unavailable in practice.
335"
CONCLUSIONS,0.3042016806722689,"However, they face one or more of the following problems — Inlier Assumption (IA), Curse of
336"
CONCLUSIONS,0.3050420168067227,"Dimensionality (CD), or Multiple Views (MV). In this article, we have proposed the first mathematical
337"
CONCLUSIONS,0.3058823529411765,"formulation of MV, which allows for a better understanding of how to solve this occurrence. Using
338"
CONCLUSIONS,0.3067226890756303,"this formulation, we developed GSAAL, which is the first OD approach that solves MV, CD, and IA.
339"
CONCLUSIONS,0.30756302521008405,"In short, GSAAL is a generative adversarial network with a generator and multiple detectors fitted in
340"
CONCLUSIONS,0.30840336134453783,"the subspaces to find outliers not visible in the full space. In our experiments on 27 different datasets,
341"
CONCLUSIONS,0.3092436974789916,"we demonstrated the usefulness of GSAAL, in particular, its ability to deal with MV and its superior
342"
CONCLUSIONS,0.3100840336134454,"performance on OD tasks with real datasets. In addition, we have shown that GSAAL can scale up to
343"
CONCLUSIONS,0.31092436974789917,"deal with high-dimensional data, which is not the case for our most competent competitors. These
344"
CONCLUSIONS,0.31176470588235294,"results confirm GSAAL’s ability to deal with data exhibiting MV and its usability in any practical
345"
CONCLUSIONS,0.3126050420168067,"scenario involving large datasets.
346"
REFERENCES,0.3134453781512605,"References
347"
REFERENCES,0.3142857142857143,"[1] C. C. Aggarwal. Outlier Analysis. Springer International Publishing, Cham, 2017.
348"
REFERENCES,0.31512605042016806,"[2] R. Bellman. Dynamic programming. Princeton, New Jersey: Princeton University Press. XXV,
349"
REFERENCES,0.31596638655462184,"342 p. (1957)., 1957.
350"
REFERENCES,0.3168067226890756,"[3] M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. LOF: identifying density-based local
351"
REFERENCES,0.3176470588235294,"outliers. In SIGMOD Conference, pages 93–104. ACM, 2000.
352"
REFERENCES,0.31848739495798317,"[4] G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B. Micenková, E. Schubert, I. Assent,
353"
REFERENCES,0.31932773109243695,"and M. E. Houle. On the evaluation of unsupervised outlier detection: measures, datasets, and
354"
REFERENCES,0.32016806722689073,"an empirical study. Data Mining and Knowledge Discovery, 30(4):891–927, Jul 2016.
355"
REFERENCES,0.32100840336134456,"[5] J. Choi and B. Han. Mcl-gan: Generative adversarial networks with multiple specialized
356"
REFERENCES,0.32184873949579834,"discriminators. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
357"
REFERENCES,0.3226890756302521,"editors, Advances in Neural Information Processing Systems, volume 35, pages 29597–29609.
358"
REFERENCES,0.3235294117647059,"Curran Associates, Inc., 2022.
359"
REFERENCES,0.3243697478991597,"[6] F. Chollet et al. Keras. https://keras.io, 2015.
360"
REFERENCES,0.32521008403361346,"[7] W. Conover and R. Iman. Multiple-comparisons procedures. informal report. Technical report,
361"
REFERENCES,0.32605042016806723,"Los Alamos National Laboratory (LANL), Feb. 1979.
362"
REFERENCES,0.326890756302521,"[8] W. J. W. J. Conover. Practical nonparametric statistics / W.J. Conover. Wiley series in
363"
REFERENCES,0.3277310924369748,"probability and statistics. Applied probability and statistics section. Wiley, New York ;, third
364"
REFERENCES,0.32857142857142857,"edition. edition, 1999.
365"
REFERENCES,0.32941176470588235,"[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
366"
REFERENCES,0.3302521008403361,"transformers for language understanding. In North American Chapter of the Association for
367"
REFERENCES,0.3310924369747899,"Computational Linguistics, 2019.
368"
REFERENCES,0.3319327731092437,"[10] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial feature learning. In International
369"
REFERENCES,0.33277310924369746,"Conference on Learning Representations, 2017.
370"
REFERENCES,0.33361344537815124,"[11] I. Durugkar, I. M. Gemp, and S. Mahadevan. Generative multi-adversarial networks. ArXiv,
371"
REFERENCES,0.334453781512605,"abs/1611.01673, 2016.
372"
REFERENCES,0.3352941176470588,"[12] C. Désir, S. Bernard, C. Petitjean, and L. Heutte. One class random forests. Pattern Recognition,
373"
REFERENCES,0.33613445378151263,"46(12):3490–3506, 2013.
374"
REFERENCES,0.3369747899159664,"[13] R. El-Yaniv and M. Nisenson. Optimal single-class classification strategies. In B. Schölkopf,
375"
REFERENCES,0.3378151260504202,"J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19.
376"
REFERENCES,0.33865546218487397,"MIT Press, 2006.
377"
REFERENCES,0.33949579831932775,"[14] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http:
378"
REFERENCES,0.3403361344537815,"//www.deeplearningbook.org.
379"
REFERENCES,0.3411764705882353,"[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
380"
REFERENCES,0.3420168067226891,"Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence,
381"
REFERENCES,0.34285714285714286,"and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.
382"
REFERENCES,0.34369747899159664,"Curran Associates, Inc., 2014.
383"
REFERENCES,0.3445378151260504,"[16] A. Goodge, B. Hooi, S.-K. Ng, and W. S. Ng. Lunar: Unifying local outlier detection methods
384"
REFERENCES,0.3453781512605042,"via graph neural networks. ArXiv, abs/2112.05355, 2021.
385"
REFERENCES,0.346218487394958,"[17] J. Guo, Z. Pang, M. Bai, P. Xie, and Y. Chen. Dual generative adversarial active learning.
386"
REFERENCES,0.34705882352941175,"Applied Intelligence, 51(8):5953–5964, Aug 2021.
387"
REFERENCES,0.34789915966386553,"[18] N. Gupta, D. Eswaran, N. Shah, L. Akoglu, and C. Faloutsos. Lookout on time-evolving graphs:
388"
REFERENCES,0.3487394957983193,"Succinctly explaining anomalies from any detector, 2017.
389"
REFERENCES,0.3495798319327731,"[19] S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao. Adbench: Anomaly detection benchmark. In
390"
REFERENCES,0.35042016806722687,"S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in
391"
REFERENCES,0.35126050420168065,"Neural Information Processing Systems, volume 35, pages 32142–32159. Curran Associates,
392"
REFERENCES,0.3521008403361345,"Inc., 2022.
393"
REFERENCES,0.35294117647058826,"[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE
394"
REFERENCES,0.35378151260504204,"Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2015.
395"
REFERENCES,0.3546218487394958,"[21] K. Hempstalk, E. Frank, and I. H. Witten. One-class classification by combining density and
396"
REFERENCES,0.3554621848739496,"class probability estimation. In W. Daelemans, B. Goethals, and K. Morik, editors, Machine
397"
REFERENCES,0.3563025210084034,"Learning and Knowledge Discovery in Databases, pages 505–519, Berlin, Heidelberg, 2008.
398"
REFERENCES,0.35714285714285715,"Springer Berlin Heidelberg.
399"
REFERENCES,0.35798319327731093,"[22] F. Keller, E. Muller, and K. Bohm. Hics: High contrast subspaces for density-based outlier
400"
REFERENCES,0.3588235294117647,"ranking. In 2012 IEEE 28th International Conference on Data Engineering, pages 1037–1048,
401"
REFERENCES,0.3596638655462185,"2012.
402"
REFERENCES,0.36050420168067226,"[23] F. Keller, E. Müller, A. Wixler, and K. Böhm. Flexible and adaptive subspace search for
403"
REFERENCES,0.36134453781512604,"outlier analysis. In Proceedings of the 22nd ACM International Conference on Information &
404"
REFERENCES,0.3621848739495798,"Knowledge Management, CIKM ’13, page 1381–1390, New York, NY, USA, 2013. Association
405"
REFERENCES,0.3630252100840336,"for Computing Machinery.
406"
REFERENCES,0.3638655462184874,"[24] H. Kriegel, M. Schubert, and A. Zimek. Angle-based outlier detection in high-dimensional data.
407"
REFERENCES,0.36470588235294116,"In KDD, pages 444–452. ACM, 2008.
408"
REFERENCES,0.36554621848739494,"[25] H.-P. Kriegel, P. Kröger, E. Schubert, and A. Zimek. Outlier detection in axis-parallel subspaces
409"
REFERENCES,0.3663865546218487,"of high dimensional data. In T. Theeramunkong, B. Kijsirikul, N. Cercone, and T.-B. Ho, editors,
410"
REFERENCES,0.36722689075630255,"Advances in Knowledge Discovery and Data Mining, pages 831–838, Berlin, Heidelberg, 2009.
411"
REFERENCES,0.3680672268907563,"Springer Berlin Heidelberg.
412"
REFERENCES,0.3689075630252101,"[26] W. H. Kruskal. A nonparametric test for the several sample problem. The Annals of Mathemati-
413"
REFERENCES,0.3697478991596639,"cal Statistics, 23(4):525–540, 1952.
414"
REFERENCES,0.37058823529411766,"[27] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, May 2015.
415"
REFERENCES,0.37142857142857144,"[28] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Poczos.
Mmd gan: Towards deeper
416"
REFERENCES,0.3722689075630252,"understanding of moment matching network. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
417"
REFERENCES,0.373109243697479,"R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
418"
REFERENCES,0.3739495798319328,"Systems, volume 30. Curran Associates, Inc., 2017.
419"
REFERENCES,0.37478991596638656,"[29] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In 2008 Eighth IEEE International
420"
REFERENCES,0.37563025210084033,"Conference on Data Mining, pages 413–422, 2008.
421"
REFERENCES,0.3764705882352941,"[30] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He. Generative adversarial active
422"
REFERENCES,0.3773109243697479,"learning for unsupervised outlier detection.
IEEE Transactions on Knowledge and Data
423"
REFERENCES,0.37815126050420167,"Engineering, 32(8):1517–1528, 2020.
424"
REFERENCES,0.37899159663865545,"[31] E. Müller, I. Assent, P. Iglesias, Y. Mülle, and K. Böhm. Outlier ranking via subspace analysis
425"
REFERENCES,0.3798319327731092,"in multiple views of the data. In 2012 IEEE 12th International Conference on Data Mining,
426"
REFERENCES,0.380672268907563,"pages 529–538, 2012.
427"
REFERENCES,0.3815126050420168,"[32] B. Perozzi, L. Akoglu, P. Iglesias Sánchez, and E. Müller. Focused clustering and outlier
428"
REFERENCES,0.38235294117647056,"detection in large attributed graphs. In Proceedings of the 20th ACM SIGKDD International
429"
REFERENCES,0.3831932773109244,"Conference on Knowledge Discovery and Data Mining, KDD ’14, page 1346–1355, New York,
430"
REFERENCES,0.3840336134453782,"NY, USA, 2014. Association for Computing Machinery.
431"
REFERENCES,0.38487394957983195,"[33] T. Poggio, A. Banburski, and Q. Liao. Theoretical issues in deep networks. Proceedings of the
432"
REFERENCES,0.38571428571428573,"National Academy of Sciences, 117(48):30039–30045, 2020.
433"
REFERENCES,0.3865546218487395,"[34] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large
434"
REFERENCES,0.3873949579831933,"data sets. In Proceedings of the 2000 ACM SIGMOD International Conference on Management
435"
REFERENCES,0.38823529411764707,"of Data, SIGMOD ’00, page 427–438, New York, NY, USA, 2000. Association for Computing
436"
REFERENCES,0.38907563025210085,"Machinery.
437"
REFERENCES,0.3899159663865546,"[35] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and
438"
REFERENCES,0.3907563025210084,"M. Kloft. Deep one-class classification. In J. Dy and A. Krause, editors, Proceedings of the
439"
REFERENCES,0.3915966386554622,"35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
440"
REFERENCES,0.39243697478991596,"Learning Research, pages 4393–4402. PMLR, 10–15 Jul 2018.
441"
REFERENCES,0.39327731092436974,"[36] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised
442"
REFERENCES,0.3941176470588235,"anomaly detection with generative adversarial networks to guide marker discovery. In M. Ni-
443"
REFERENCES,0.3949579831932773,"ethammer, M. Styner, S. Aylward, H. Zhu, I. Oguz, P.-T. Yap, and D. Shen, editors, Information
444"
REFERENCES,0.3957983193277311,"Processing in Medical Imaging, pages 146–157, Cham, 2017. Springer International Publishing.
445"
REFERENCES,0.39663865546218485,"[37] N. Seliya, A. Abdollah Zadeh, and T. M. Khoshgoftaar. A literature review on one-class
446"
REFERENCES,0.39747899159663863,"classification and its potential applications in big data. Journal of Big Data, 8(1):122, Sep 2021.
447"
REFERENCES,0.3983193277310924,"[38] B. Settles. Active learning literature survey. 2009.
448"
REFERENCES,0.39915966386554624,"[39] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In Proceedings of
449"
REFERENCES,0.4,"the IEEE/CVF International Conference on Computer Vision, pages 5972–5981, 2019.
450"
REFERENCES,0.4008403361344538,"[40] G. Steinbuss and K. Böhm. Hiding outliers in high-dimensional data spaces. International
451"
REFERENCES,0.4016806722689076,"Journal of Data Science and Analytics, 4(3):173–189, Nov 2017.
452"
REFERENCES,0.40252100840336136,"[41] H. Wang, M. J. Bah, and M. Hammad. Progress in outlier detection techniques: A survey. IEEE
453"
REFERENCES,0.40336134453781514,"Access, 7:107964–108000, 2019.
454"
REFERENCES,0.4042016806722689,"[42] H. Xu, G. Pang, Y. Wang, and Y. Wang. Deep isolation forest for anomaly detection. IEEE
455"
REFERENCES,0.4050420168067227,"Transactions on Knowledge and Data Engineering, 35(12):12591–12604, 2023.
456"
REFERENCES,0.40588235294117647,"[43] Y. Zhao, Z. Nasrullah, and Z. Li. Pyod: A python toolbox for scalable outlier detection. Journal
457"
REFERENCES,0.40672268907563025,"of Machine Learning Research, 20(96):1–7, 2019.
458"
REFERENCES,0.40756302521008403,"[44] J.-J. Zhu and J. Bento. Generative adversarial active learning. arXiv preprint arXiv:1702.07956,
459"
REFERENCES,0.4084033613445378,"2017.
460"
REFERENCES,0.4092436974789916,"A
Theoretical Appendix
461"
REFERENCES,0.41008403361344536,"In this appendix, we will include all the proofs of the included theorems and propositions. Addition-
462"
REFERENCES,0.41092436974789914,"ally, we also extend all non-experimental sections with relevant information for the experimental
463"
REFERENCES,0.4117647058823529,"appendix.
464"
REFERENCES,0.4126050420168067,"A.1
Previous Remarks
465"
REFERENCES,0.4134453781512605,"Before starting to prove our main results, it is important to add a remark about our notation in this
466"
REFERENCES,0.4142857142857143,"article. Whenever we denote ux, we mean the operation resulting in the following vector: u(ω)x(ω).
467"
REFERENCES,0.4151260504201681,"Thus, ux is a random vector following its distribution pux. However, it is important to remark that
468"
REFERENCES,0.41596638655462187,"ux, and therefore, also uix, does not state the usual matrix-vector multiplication. What we mean by
469"
REFERENCES,0.41680672268907565,"ux is the operation U ×M x, where U stands for the range-complete version of u and ×M the usual
470"
REFERENCES,0.4176470588235294,"matrix multiplication. This means that whenever we write ux we are considering the projection of x
471"
REFERENCES,0.4184873949579832,"into the subspace of the features selected in u. This means that uix is the random vector composed
472"
REFERENCES,0.419327731092437,"of the features selected by ui, and therefore, puix(uix) denotes subsequent marginal pdf of x. We
473"
REFERENCES,0.42016806722689076,"do not state this in the main text as it functionally does not change anything of our derivations, and
474"
REFERENCES,0.42100840336134454,"simply works as a notation. The only important remarks stemming from this fact are the following:
475"
REFERENCES,0.4218487394957983,"1. px(uix) = px(πui(x)), where πui denotes the projection of a point x into the subspace of
476"
REFERENCES,0.4226890756302521,"ui. Therefore, we can write px(uix) = puix(uix).
477"
REFERENCES,0.4235294117647059,"2. The operator as stated before is not distributive. This is trivial, as given u a random matrix as
478"
REFERENCES,0.42436974789915966,"in definition 1, (1d −u)x is defined properly, as 1d −u ∈Diag({0, 1}). However, x −ux
479"
REFERENCES,0.42521008403361343,"denotes the vector subtraction between two vectors with different dimensionality.
480"
REFERENCES,0.4260504201680672,"While not important to understand the following proofs and the derivations from the main text,
481"
REFERENCES,0.426890756302521,"understanding this is crucial for anyone seeking to work with these definitions.
482"
REFERENCES,0.42773109243697477,"A.2
Proofs
483"
REFERENCES,0.42857142857142855,"We will reformulate all of the statements for completion before introducing each proof.
484"
REFERENCES,0.4294117647058823,"Proposition 2. Let x and u be as before with px myopic to the views of u. Consider a set of
485"
REFERENCES,0.43025210084033616,"independent realizations of u: {ui}k
i=1, a realization of x, x, and a realization of ux, ux. Then
486"
"K
P",0.43109243697478994,"1
k
P"
"K
P",0.4319327731092437,"i puix(uix) is a statistic for pux(ux).
487"
"K
P",0.4327731092436975,"Proof. Consider x and u as in the statement. Recall the law of total probabilities:
488"
"K
P",0.4336134453781513,"pux(ux) = Eu
 
pux|u=u′(ux|u′)

.
By taking the definition of u and the myopicity, it is trivial that:
489"
"K
P",0.43445378151260505,pux|u=u′(ux|u′) = pu′x(u′x)
"K
P",0.43529411764705883,"for u′ such that pu(u′) ̸= 0.
490"
"K
P",0.4361344537815126,"Then, by definition of marginal probability and expectation, we have that:
491"
"K
P",0.4369747899159664,"pux(ux) = N
X"
"K
P",0.43781512605042017,"i=1
pu(ui)puix(uix),"
"K
P",0.43865546218487395,"as u is discrete with finite set of occurrences of size N.
Thus, we can approximate
492
PN
i=1 pu(ui)puix(uix)) by 1 k
P"
"K
P",0.4394957983193277,"i puix with ui independent samples of u.
493"
"K
P",0.4403361344537815,"Theorem 3. Consider x and u as in the previous definition, with x a realization of x and {ui}i a set
494"
"K
P",0.4411764705882353,"of realizations of u. Consider a generator G : z ∈Z 7−→G(z) ∈Rd and {Di}, i = 1, . . . , k, a set
495"
"K
P",0.44201680672268906,"of detectors such as Di : uix ∈Si ⊂Rd 7−→Di(uix) ∈[0, 1]. Z is an arbitrary noise space where
496"
"K
P",0.44285714285714284,"G randomly samples from. Consider the following objective function
497"
"K
P",0.4436974789915966,"min
G max
Di, ∀i X"
"K
P",0.4445378151260504,"i
V (G, Di) ="
"K
P",0.44537815126050423,"min
G max
Di, ∀i X"
"K
P",0.446218487394958,"i
Euix log Di(uix) + Ez log (1 −Di (uiG(z)))
(3)"
"K
P",0.4470588235294118,"Under these conditions, the following holds:
498"
"K
P",0.44789915966386556,"i) Each detector’s loss in optimum is V (G, D∗
i ) = 1"
"K
P",0.44873949579831934,"2.
499"
"K
P",0.4495798319327731,"ii) Each individual Di converges to D∗
i (uix) = puix(uix) after trained in Step 2 of a GAAL
500"
"K
P",0.4504201680672269,"method.
501"
"K
P",0.4512605042016807,iii) D∗(x) = 1
"K
P",0.45210084033613446,"k
Pk
i=1 D∗
i (uix) approximates pux(ux). If px is myopic, D∗(x) also approxi-
502"
"K
P",0.45294117647058824,"mates px(x).
503"
"K
P",0.453781512605042,"Proof. This proof will follow mainly the results in [15], adapted for our case. We will first derivative
504"
"K
P",0.4546218487394958,"two general results that we are going to use to immediately prove (i), (ii) and (iii). First, consider
505"
"K
P",0.45546218487394957,"the objective function
506 X"
"K
P",0.45630252100840335,"i
V (G, Di) =
X"
"K
P",0.45714285714285713,"i
Euix∼puix log(Di(uix))+"
"K
P",0.4579831932773109,"Ez∼pz(1 −log(Di(uiG(z)))),"
"K
P",0.4588235294117647,"where z is the random vector used by G to sample from the noise space Z. We will write Ex, Ez and
507"
"K
P",0.45966386554621846,"Euix instead of Ex∼px, Ez∼pz and Euix∼puix as an abuse of notation.
508"
"K
P",0.46050420168067224,"The problem is, then, to optimize:
509"
"K
P",0.4613445378151261,"min
G max
Di, ∀i X"
"K
P",0.46218487394957986,"i
V (G, Di).
(4)"
"K
P",0.46302521008403363,"Fixing G and maximizing for all Di, each detector individually maximizes V (G, Di). Let us try to
510"
"K
P",0.4638655462184874,"obtain the optimal of each Di with a fixed G. First, we write:
511"
"K
P",0.4647058823529412,"V (G, Di) =
Z"
"K
P",0.46554621848739497,"uix
puix(uix) log Di(uix)duix+
Z"
"K
P",0.46638655462184875,"z
pz(z) log(1 −Di(uiG(z)))dz."
"K
P",0.4672268907563025,"As G uses z to sample from its sample distribution pG(x), we can rewrite the second addent, like in
512"
"K
P",0.4680672268907563,"[15], as:
513"
"K
P",0.4689075630252101,"V (G, Di) =
Z"
"K
P",0.46974789915966386,"uix
puix(uix) log Di(uix)duix+
Z"
"K
P",0.47058823529411764,"uix
pG(uix) log(1 −Di(uix))duix."
"K
P",0.4714285714285714,"Aggregating both integrals, we have a function of the type f(t) = a log(t) + b log(1 −t), with
514"
"K
P",0.4722689075630252,"a, b ∈R−{0}. We know that f(t) obtains its optimum in t =
a
a+b. As f(t) ∈R+, V (G, Di) obtains
515"
"K
P",0.473109243697479,"its optimum for a given G in:
516"
"K
P",0.47394957983193275,"D∗
i (uix) =
puix(uix)
puix(uix) + pG(uix).
(5)"
"K
P",0.47478991596638653,"Let us now consider the following function
517"
"K
P",0.4756302521008403,"C(G) =
X"
"K
P",0.4764705882352941,"i
max
Di, ∀iV (G, Di) =
X"
"K
P",0.4773109243697479,"i
Euix log
puix(uix)
puix(uix) + pG(uix)+"
"K
P",0.4781512605042017,"Euix∼pG log
pG(uix)
puix(uix) + pG(uix). (6)"
"K
P",0.4789915966386555,"This is known in Game Theory as the cost function of player “G” in the null-sum game defined by
518"
"K
P",0.47983193277310926,"the min max optimization problem. [15] refers to it as the virtual training criterion of the GAN. The
519"
"K
P",0.48067226890756304,"adversarial game defined by (4) reaches an equilibrium (and thus, the min max problem an optimum)
520"
"K
P",0.4815126050420168,"whenever C(G) is minimized. We will study the value of G in such equilibrium and use it, together
521"
"K
P",0.4823529411764706,"with (5), to prove the statements.
522"
"K
P",0.4831932773109244,"Rewriting C(G) it is clear that:
523"
"K
P",0.48403361344537815,"C(G) =
X"
"K
P",0.48487394957983193,"i
KL

puix(uix)∥puix(uix) + pG(uix) 2 "
"K
P",0.4857142857142857,"+ KL

pG(uix)∥puix(uix) + pG(uix) 2 
."
"K
P",0.4865546218487395,"This expression corresponds to that of a sum of multiple binary cross entropies between a population
524"
"K
P",0.48739495798319327,"coming from puix and from pG projected by ui. Therefore, as we know, we can rewrite:
525"
"K
P",0.48823529411764705,"C(G) =
X"
"K
P",0.4890756302521008,"i
2JSD(puix(uix)∥pG(uix)),"
"K
P",0.4899159663865546,"with JSD the Jensen-Shannon divergence. Since JSD(s∥r) ∈[0, log(2)), it is clear that C(G)
526"
"K
P",0.4907563025210084,"obtains its minimum only whenever
527"
"K
P",0.49159663865546216,"pG(uix) = puix(uix), ∀∀x2;
(7)"
"K
P",0.492436974789916,"and for all i ∈{1, . . . , k}.
528"
"K
P",0.49327731092436977,"Knowing G and Di in the optimum for all i, we can prove the statements above:
529"
"K
P",0.49411764705882355,"(i)
As pG(uix) = puix(uix) for almost all x, in the optimum of (4), it is immediate that:
530"
"K
P",0.49495798319327733,"Di(uix) = 1 2,"
"K
P",0.4957983193277311,"i.e., the detectors cannot differentiate between the real training data and the synthetic data of the
531"
"K
P",0.4966386554621849,"generator. If one employs the numerically stable version of each V (G, Di) (equivalent to the
532"
"K
P",0.49747899159663866,"numerically stable version of the binary cross entropy [6]), it is trivial to see that
533"
"K
P",0.49831932773109244,"V stable(G, Di) = log(2)."
"K
P",0.4991596638655462,"(ii)
After optimizing (4), training each Di individually with G fixed, is the equivalent of building a
534"
"K
P",0.5,"two-class classifier distinguishing between the artificial class generated by pG(uix) = puix(uix) and
535"
"K
P",0.5008403361344538,"the real data coming from puix(uix). By [21], the resulting two-class classifier would be such as:
536"
"K
P",0.5016806722689076,Di(uix) = puix(uix).
"K
P",0.5025210084033613,"(iii)
By proposition 2 and statement (ii), 1 k
P"
"K
P",0.5033613445378151,"i D∗
i (uix) is an estimator for pux(ux). By myopicity,
537"
"K
P",0.5042016806722689,"it is also of px(x).
538"
"K
P",0.5050420168067227,"Theorem 4. Giving our GSAAL method with generator G and detectors {Di}k
i=1, each with four
539"
"K
P",0.5058823529411764,"fully connected hidden layers, √n nodes in the detectors and d in the generator, we obtain that:
540"
"K
P",0.5067226890756302,"i) The training time complexity is bounded with O(ED · n · (k · n + d2)), for a dataset D with
541"
"K
P",0.507563025210084,"n training samples and d features. ED is an unknown complexity variable depicting the
542"
"K
P",0.5084033613445378,"unique epochs to convergence for the network in dataset D.
543"
"K
P",0.5092436974789916,"ii) The single sample inference time complexity is bounded with O(k · n), with k the number of
544"
"K
P",0.5100840336134453,"detectors used.
545"
"K
P",0.5109243697478991,"Proof. An evaluation of a neural network is composed of two steps, the backpropagation, and the
546"
"K
P",0.5117647058823529,"fowardpass steps. While training the network requires both, inference requires only a fowardpass.
547"
"K
P",0.5126050420168067,"Therefore, we will first prove (ii) and will build upon it to prove (i).
548"
FOR ALMOST ALL X,0.5134453781512605,2For almost all x
FOR ALMOST ALL X,0.5142857142857142,"(ii).
GSAAL consists of a generator and k detectors. Single point inference consists of a single
549"
FOR ALMOST ALL X,0.515126050420168,"fowardpass of all the detectors. We will first prove the general complexity of a fowardpass of a
550"
FOR ALMOST ALL X,0.5159663865546219,"general fully connected 4 layer network and will use it to derive all the other complexities. Let us
551"
FOR ALMOST ALL X,0.5168067226890757,"consider three weight matrices Wji, Whj and Wlh each between two layers, with j, i, h and l being
552"
FOR ALMOST ALL X,0.5176470588235295,"the number of nodes in each. Therefore, Wji denotes a matrix with j rows and i columns, and so
553"
FOR ALMOST ALL X,0.5184873949579832,"on. Now, let us consider xi1 the datapoint after passing the input layer. Lastly, without any loss of
554"
FOR ALMOST ALL X,0.519327731092437,"generality, consider f to be the activation function for all layers. This way, the forward pass of a
555"
FOR ALMOST ALL X,0.5201680672268908,"single detector can be written as:
556"
FOR ALMOST ALL X,0.5210084033613446,cl1 = f (Wlhf (Whjf (Wjixi1))) .
FOR ALMOST ALL X,0.5218487394957984,"We will study the complexity in the first layer and use it to derive the complexity of the others.
557"
FOR ALMOST ALL X,0.5226890756302521,"Aj1 = Wjixi1 is a simple matrix-vector multiplication that we know to be O(j · i) atmost. Then, as
558"
FOR ALMOST ALL X,0.5235294117647059,"f is an activation function, f(Aj1) is equivalent to writing fj1 ⊙Aj1, with ⊙being the element-wise
559"
FOR ALMOST ALL X,0.5243697478991597,"multiplication. Thus, f (Wjixi1) is:
560"
FOR ALMOST ALL X,0.5252100840336135,O(j · i + j) = O(j · (i + 1)) = O(j · i).
FOR ALMOST ALL X,0.5260504201680672,"Doing this for all layers, we obtain:
561"
FOR ALMOST ALL X,0.526890756302521,"O(l · h + k · j + j · i).
(8)"
FOR ALMOST ALL X,0.5277310924369748,"As all layers have √n nodes,
562"
FOR ALMOST ALL X,0.5285714285714286,O(3n) = O(n).
FOR ALMOST ALL X,0.5294117647058824,"As we have k detectors, the complexity for a fowardpass of all detectors, and thus, for a single sample
563"
FOR ALMOST ALL X,0.5302521008403361,"inference of GSAAL is:
564"
FOR ALMOST ALL X,0.5310924369747899,O(k · n).
FOR ALMOST ALL X,0.5319327731092437,"(i).
A backpropagation step has the same complexity as an inference step on all training samples.
565"
FOR ALMOST ALL X,0.5327731092436975,"As we have n training samples, this then becomes
566"
FOR ALMOST ALL X,0.5336134453781513,O(k · n2)
FOR ALMOST ALL X,0.534453781512605,"for the detectors. As the training consists of multiple epochs, we will write
567"
FOR ALMOST ALL X,0.5352941176470588,"O(ED · k · n2),"
FOR ALMOST ALL X,0.5361344537815126,"with ED being the number of epochs needed for convergence for the training data set D. As the
568"
FOR ALMOST ALL X,0.5369747899159664,"training consists of both backpropagation and fowardpass steps on all training samples, the total
569"
FOR ALMOST ALL X,0.5378151260504201,"training time complexity for all detectors is:
570"
FOR ALMOST ALL X,0.5386554621848739,O(ED · k · n2 + k · n2) = O(ED · k · n2).
FOR ALMOST ALL X,0.5394957983193277,"As we also need to consider the generator, we will use equation 8 to derive both steps on the generator.
571"
FOR ALMOST ALL X,0.5403361344537815,"As the generator is also a fully connected 4-layer network, with all layers having d nodes, the
572"
FOR ALMOST ALL X,0.5411764705882353,"complexity for a single fowardpass is:
573"
FOR ALMOST ALL X,0.542016806722689,O(d2).
FOR ALMOST ALL X,0.5428571428571428,"As during training one generates n samples during each fowardpass:
574"
FOR ALMOST ALL X,0.5436974789915966,O(n · d2).
FOR ALMOST ALL X,0.5445378151260504,"Now, on each backpropagation pass the network calculates the backpropagation error for each
575"
FOR ALMOST ALL X,0.5453781512605042,"generated sample, thus,
576"
FOR ALMOST ALL X,0.5462184873949579,O(n · d2)
FOR ALMOST ALL X,0.5470588235294118,"is also the time complexity for the backpropagation step of the generator. Considering all ED epochs
577"
FOR ALMOST ALL X,0.5478991596638656,"and both backpropagation and fowardpass steps of the generator and all the detectors, the time
578"
FOR ALMOST ALL X,0.5487394957983194,"complexity of GSAAL’s training is:
579"
FOR ALMOST ALL X,0.5495798319327732,O(ED · k · n2 + ED · n · d2) = O(ED · n · (k · n + d2)) 580
FOR ALMOST ALL X,0.5504201680672269,Figure 4: Difference in statistical distance between two populations.
FOR ALMOST ALL X,0.5512605042016807,"A.3
Related Work (extension)
581"
FOR ALMOST ALL X,0.5521008403361345,"Deep Outlier Detection for other data types.
Outlier detection is also very popular in different
582"
FOR ALMOST ALL X,0.5529411764705883,"data types, especially in unstructured data [42, 16, 36, 35, 32]. Due to the complexity of the data they
583"
FOR ALMOST ALL X,0.553781512605042,"are used for, deep methods are the main approach employed for this task. The main difference with
584"
FOR ALMOST ALL X,0.5546218487394958,"the other deep methods introduced for tabular data, is that the deep architecture in the later targets
585"
FOR ALMOST ALL X,0.5554621848739496,"mainly CD. For unstructured data types, like images or natural language, is the complexity of the data
586"
FOR ALMOST ALL X,0.5563025210084034,"that drives the architecture. For example, to treat image data, multiple linear layers do not suffice,
587"
FOR ALMOST ALL X,0.5571428571428572,"complex layers like convolutional or residual layers are employed for this [27].
588"
FOR ALMOST ALL X,0.5579831932773109,"Although popular, most deep methods have limited to no use at all in tabula data in their original
589"
FOR ALMOST ALL X,0.5588235294117647,"articles. However, some have appeared in the literature of tabular data as competitors [36, 35]. We
590"
FOR ALMOST ALL X,0.5596638655462185,"identified the most common for our task in related articles and benchmarks, and included them as an
591"
FOR ALMOST ALL X,0.5605042016806723,"extension of our main experiments in sections B.2 and B.3.
592"
FOR ALMOST ALL X,0.561344537815126,"A.4
Multiple Views (extension)
593"
FOR ALMOST ALL X,0.5621848739495798,"In this section we extend the derivations in section 3.1 by providing an example of a myopic
594"
FOR ALMOST ALL X,0.5630252100840336,"distribution:
595"
FOR ALMOST ALL X,0.5638655462184874,"Example 2 (Myopic distribution). Consider a x like in example 1. Here, it is clear that x1, x2⊥x3.
596"
FOR ALMOST ALL X,0.5647058823529412,"Consider, then, u such that:
597"
FOR ALMOST ALL X,0.565546218487395,"u : {1} −→{diag(1, 1, 0)}.
To test whether px is myopic, we employed a simple test utilizing a statistical distance (MMD with
598"
FOR ALMOST ALL X,0.5663865546218487,"the identity kernel) between px and pux. This way, if
ˆ
MMD(px∥pux) = 0, it would be clear that the
599"
FOR ALMOST ALL X,0.5672268907563025,"equality holds. As a control measure, we also calculated the same distance for a different population
600"
FOR ALMOST ALL X,0.5680672268907563,"x′, where x3 = x2
1. We have plotted the results in image 4, where Population 1 refers to x and
601"
FOR ALMOST ALL X,0.5689075630252101,"Population 2 to x′. As we can see, we do obtain a positive result in the test of myopicity for x and a
602"
FOR ALMOST ALL X,0.5697478991596638,"negative one for x′.
603"
FOR ALMOST ALL X,0.5705882352941176,"A.5
GSAAL (extension)
604"
FOR ALMOST ALL X,0.5714285714285714,"We now extend the results from section 3.2 by providing the pseudocode for the training of our
605"
FOR ALMOST ALL X,0.5722689075630252,"method. It is important to consider that, while theorem 3 formulates the optimization problem
606"
FOR ALMOST ALL X,0.573109243697479,"in terms of the neural networks G and {Di}i, in practice this will not be the case. Instead, we
607"
FOR ALMOST ALL X,0.5739495798319327,"will consider the optimization in terms of their weights, ΘG and ΘDi. Therefore, in practice, the
608"
FOR ALMOST ALL X,0.5747899159663865,"convergence into an equilibrium will be limited by the capacity of the networks themselves [14].
609"
FOR ALMOST ALL X,0.5756302521008403,"We considered the optimization to follow minibatch-stochastic gradient descent [14]. To consider
610"
FOR ALMOST ALL X,0.5764705882352941,"any other minibatch-gradient method it will suffice to perform the necessary transformations to the
611"
FOR ALMOST ALL X,0.5773109243697478,"gradients.
612"
FOR ALMOST ALL X,0.5781512605042017,"The pseudocode is located in Algorithm 1. As it is the training for the method, it takes both
613"
FOR ALMOST ALL X,0.5789915966386555,"the parameters for the method and the training. In this case, epochs refers to the total number
614"
FOR ALMOST ALL X,0.5798319327731093,"of epochs we will train in total, while stop_epoch marks the epoch where we start step 2 of the
615"
FOR ALMOST ALL X,0.5806722689075631,"GAAL training. Lines 1-3 initialize both the detectors in their subspaces and the generator with
616"
FOR ALMOST ALL X,0.5815126050420169,Algorithm 1 GSAAL training
FOR ALMOST ALL X,0.5823529411764706,"Require: Data set D, Number of Discriminators κ, u, epochs, stop_epoch"
FOR ALMOST ALL X,0.5831932773109244,"1: Initialize Generator G {#d is the dimensionality of D}
2: {ui}κ
i=1 ←DRAWFROMu(κ)
3: Initialize Discriminators {Di}κ
i=1 with unique subspaces {ui}κ
i=1
4: for epoch ∈{1, ..., epochs} do
5:
for batch ∈{1, ..., batches} do
6:
noise ←Random noise z(1), ..., z(m) from Z
7:
data ←Draw current batch x(1), ..., x(m)"
FOR ALMOST ALL X,0.5840336134453782,"8:
for j ∈{1...k} do
9:
Update Dj by ascending the stochastic gradient: ∇ΘDj
1
m
Pm
i=1 log(Dj(ujx(i))) +
log(1 −Dj(ujG(z(i))))
10:
end for
11:
if epoch < stop_epoch then
12:
Update G by descending the stochastic gradient:
∇ΘG
1
k
Pk
j=1
1
m
Pm
i=1 log(1 −
Dj(G(z(i))))
13:
end if
14:
end for
15: end for"
FOR ALMOST ALL X,0.584873949579832,Table 4: Different outliers generated for the experiments.
FOR ALMOST ALL X,0.5857142857142857,"Outlier Type
Assumption Description
Outlier Description
M"
FOR ALMOST ALL X,0.5865546218487395,"Local
Assumes that all inliers are
located close to other inliers"
FOR ALMOST ALL X,0.5873949579831933,"As a result, outliers are
far away from inliers
LOF"
FOR ALMOST ALL X,0.5882352941176471,"Angle
Assumes that all inliers
have other inliers in all angles from their position
As a result, outliers are
not surrounded by other points
ABOD"
FOR ALMOST ALL X,0.5890756302521009,"Cluster
Assumes that all inliers
form large clusters of data"
FOR ALMOST ALL X,0.5899159663865546,"As a result, outliers are
gathered in small clusters
Fn,µ+εi"
FOR ALMOST ALL X,0.5907563025210084,"random weight matrices ΘDi and ΘG. Lines 4-13 correspond to the normal GAN training loop
617"
FOR ALMOST ALL X,0.5915966386554622,"across multiple epochs, referred to as step 1 of a GAAL method, if epoch < stop_epoch. Here
618"
FOR ALMOST ALL X,0.592436974789916,"we proceed with training each detector and the generator using their gradients. Lines 8-10 update
619"
FOR ALMOST ALL X,0.5932773109243697,"each detector by ascending its stochastic gradient, while line 11 updates the generator by descending
620"
FOR ALMOST ALL X,0.5941176470588235,"its stochastic gradient. After the normal GAN training, we start the active learning loop [30] once
621"
FOR ALMOST ALL X,0.5949579831932773,"epoch ≥stop_epoch. The only difference with the regular GAN training is that G remains fixed, i.e.,
622"
FOR ALMOST ALL X,0.5957983193277311,"we do not descend using its gradient. This allows us to additionally train the detectors and, in case of
623"
FOR ALMOST ALL X,0.5966386554621849,"equilibrium of step 1, converge to the desired marginal distributions as derived in theorem 3.
624"
FOR ALMOST ALL X,0.5974789915966386,"B
Experimental Appendix
625"
FOR ALMOST ALL X,0.5983193277310924,"In this section, we will include a supplementary experiment testing the IA condition for completion,
626"
FOR ALMOST ALL X,0.5991596638655462,"the sensibility experiments, and an ablation study. Additionally, we extended both main experimental
627"
FOR ALMOST ALL X,0.6,"studies featured in the main text. All of the code for the extra experiments, as well as for all
628"
FOR ALMOST ALL X,0.6008403361344538,"experiments in the main text, can be found in our remote repository3. Our experiments used a RTX
629"
FOR ALMOST ALL X,0.6016806722689075,"3090 GPU and an AMD EPYC 7443p CPU running Python in Ubuntu 22.04.3 LTS. Deep neural
630"
FOR ALMOST ALL X,0.6025210084033613,"network methods were trained on the GPU and inferred on the CPU; shallow methods used only the
631"
FOR ALMOST ALL X,0.6033613445378151,"CPU.
632"
FOR ALMOST ALL X,0.6042016806722689,"B.1
Effects of Inlier Assumptions on Outlier Detection
633"
FOR ALMOST ALL X,0.6050420168067226,"GAAL methodologies are capable of dealing with the inlier assumption by learning the correct inlier
634"
FOR ALMOST ALL X,0.6058823529411764,"distribution px without any assumption [30]. While this should also extend to our methodology, we
635"
FOR ALMOST ALL X,0.6067226890756302,"will study experimentally whether this condition holds in practice. To do so, as one cannot identify
636"
FOR ALMOST ALL X,0.607563025210084,3https://anonymous.4open.science/r/GSAAL-8D6E
FOR ALMOST ALL X,0.6084033613445378,"(a)
(b)
(c)"
FOR ALMOST ALL X,0.6092436974789915,"Figure 5: 2D-example of the different types of anomalies we generate using the method summarized
in table 4."
FOR ALMOST ALL X,0.6100840336134454,"Figure 6: AUCs of the different methods in the IA experiments. From left to right: Local (blue),
Angle (orange) and Cluster (green)."
FOR ALMOST ALL X,0.6109243697478992,"beforehand whether a method is going to fail due to IA, we will generate synthetic datasets. This will
637"
FOR ALMOST ALL X,0.611764705882353,"allow us to generate outliers that we know to follow from a specific IA, ensuring that failure comes
638"
FOR ALMOST ALL X,0.6126050420168068,"from the anomalies themselves. We will include all of the code in the code repository. To generate
639"
FOR ALMOST ALL X,0.6134453781512605,"the synthetic datasets we follow:
640"
FOR ALMOST ALL X,0.6142857142857143,"1. Generate D, a population of 2000 inliers following some distribution F in R20.
641"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6151260504201681,"2. Select an outlier detection method M with some assumption about the normality of the data
642"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6159663865546219,"and fit it using D. We will call such M as the reference model for the generation.
643"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6168067226890757,"3. Generate 400 outliers by sampling on R20 uniformly and keeping only those points o such
644"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6176470588235294,"that M(o) = 1 (i.e., they are detected as outliers). We will write OD to refer to such a
645"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6184873949579832,"collection of points.
646"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.619327731092437,"4. Repeat step 3 10 times, to obtain OD
1 , . . . , OD
10.
647"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6201680672268908,"5. Sample out 20% of the points in D. The remainder 80% will be stored in Dtrain, and the
648"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6210084033613446,"other 20% in Dtest
1 , . . . , Dtest
10 together with each OD
i .
649"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6218487394957983,"These steps were repeated 4 times with different F, to create 4 different training sets and 40 different
650"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6226890756302521,"testing sets, corresponding to a total of 40 different datasets employed per model M selected in step
651"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6235294117647059,"2. As we used 3 different reference models, we have a total of 120 different datasets employed in
652"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6243697478991597,"this experiment alone. In particular, the models used for this are collected in table 4. The table
653"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6252100840336134,"contains the name of the outlier type, the description of the IA taken to generate them, and a brief
654"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6260504201680672,"description of how the outliers should look. Column M contains the method employed to generate
655"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.626890756302521,"each, these being LOF, ABOD, and the same inlier distribution as D, but with multiple shifted
656"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6277310924369748,"means µi and with a significantly lower amount of points n. A visualization of how these outliers
657"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6285714285714286,"would look with 2 features is located in figure 5.
To study how different methods behave when
658"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6294117647058823,"detecting these outliers, we have performed the same experiments as in section 4.3, but with these
659"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6302521008403361,"synthetic datasets. Figure 6 gathers all the AUCs of a method in 3 boxplots, one for each outlier type
660"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6310924369747899,"in each training set. Additionally, we grouped all based on the IA and assigned a similar color for
661"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6319327731092437,"all of them. We have done this for the classical OD methods LOF, ABOD, and kNN, besides our
662"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6327731092436975,"method GSAAL. We cropped the image below 0.45 in the y axis as we are not interested in results
663"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6336134453781512,"below a random classifier. As we can see, classical methods seem to correctly detect outliers for
664"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.634453781512605,"an outlier type that verifies its IA. However, whenever we introduce outliers behaving outside of
665"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6352941176470588,"their IA, the performance hit is significant. Notoriously, it appears that none of them had trouble
666"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6361344537815126,"detecting the Local and Angle outlier type. regardless of their IA. This can be easily explained by
667"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6369747899159663,"those outliers types being similar, as we can see in figure 5. On the other hand, GSAAL manages to
668"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6378151260504201,"have a significant detection rate regardless of the outlier type.
669"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6386554621848739,"B.2
Effects of Multiple Views on Outlier Detection (extension)
670"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6394957983193277,"In this section, we will include a brief description of the generation process for the datasets used in
671"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6403361344537815,"section 4.2. We will also perform the same experiment as in section 4.2 for all methods showcased in
672"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6411764705882353,"the main text and additional datasets. The datasets were generated by the following formulas:
673"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6420168067226891,"• Banana. Given θ ∈[0, π] we have x = sin(θ) + U(0, 0.1) and y = sin(θ)3 + U(0, 0.1).
674"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6428571428571429,"• Spiral. Given θ ∈[0, 4π] and r ∈(0, 1), we have x = r cos(θ) + U(0, 0.1) and y =
675"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6436974789915967,"r sin(θ).
676"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6445378151260505,"• Star. Given θ ∈[0, 2π] and r ∈{r ∈R|r = sin(5θ); r ≥0, 1, 0.4} , we have x = r cos(θ)+
677"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6453781512605042,"U(0, 0.1) and y = r sin(θ) + U(0, 0.1).
678"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.646218487394958,"• Circle. Given θ ∈[0, 2π], we have x = cos(θ) + U(0, 0.1) and y = sin(θ) + U(0, 0.1).
679"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6470588235294118,"• L. Given x1 = N(0, 0.1), x2 = U(0, 5), y1 = U(−5, 0), and y2 = N(0, 0.1); we have
680"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6478991596638656,"x = concat(x1, x2) and y = concat(y1, y2).
681"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6487394957983194,"We considered N(0, 0.1) to denote a random normal realization with µ = 0 and σ2 = 0.1, and
682"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6495798319327731,"U(a, b) to denote a uniform realization in the [a, b] interval.
683"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6504201680672269,"Figure 7 contains all images from the MV experiment. We employed the default parameters for all
684"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6512605042016807,"methods in this experiments. We did that as those were the employed parameters in our real world
685"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6521008403361345,"experiments. Additonally, the choice of parameter did not impact the outcome of the experiment
686"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6529411764705882,"much. Our remote repository includes extra images for every competitor with multiple parameters
687"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.653781512605042,"for comparison. We do not have any new insight beyond the ones exposed in the main article. Note
688"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6546218487394958,"that we have included all methods but SOD. The reason was that SOD failed to execute for datasets
689"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6554621848739496,"Star, Spiral, and Circle.
690"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6563025210084034,"Additionally, we added competitors from outside of our related work that will later be used in section
691"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6571428571428571,"B.3. In particular, we employed LUNAR, DIF and DeepSVDD with default parameters. We included
692"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6579831932773109,"extra images in our remote repository with multiple parameters for the deep competitors as well. The
693"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6588235294117647,"method AnoGAN was not included due to it failing in datasets Star, Spiral and Circle. Their results
694"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6596638655462185,"can be seen in Figure 8. As it also happened our main competitors, some of the extra competitors were
695"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6605042016806723,"capable of detecting the data structure in very sparse occasions. However they remained incapable to
696"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.661344537815126,"properly describe a boundary consistently. The only method that was sensible enough in all datasets
697"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6621848739495798,"was GSAAL.
698"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6630252100840336,"In order to quantify this, we tested the ability of all methods to perform one-class classification in
699"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6638655462184874,"each dataset. As outliers, we used white noise in the x1 −x2 subspace. Additionally, we created two
700"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6647058823529411,"extra datasets greatly different from the rest, X and wave:
701"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6655462184873949,"• X. Given x1 = x2 = U(−1, 1) and y1 = x1 + U(0, 0.1), y2 = x2 + U(0, 0.1); we have
702"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6663865546218487,"x = concat(x1, x2) and y = concat(y1, y2)..
703"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6672268907563025,"• Wave. Given θ ∈[0, 4π], we have x = θ and y = sin(x) + U(0, 0.1).
704"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6680672268907563,"We will also use them as outleirs, for a total of 15 different datasets. We also generated extra inliers
705"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.66890756302521,"in each test set. We gathered the AUC results in Figure 9. As we can see, all other methods struggel
706"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6697478991596638,"to come ahead of the random classifier, marked with a dashed line. The only method well above that
707"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6705882352941176,"is GSAAL.
708"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6714285714285714,"B.3
One-class Classification (extension)
709"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6722689075630253,"As we noted in Section 4, we obtained our benchmark datasets from [19], a benchmark study for
710"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.673109243697479,"One-class classification methods in tabular data. Some of the datasets featured in the study, and
711"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6739495798319328,"also in our experiments, were obtained from embedding image or text data using a pre-trained NN
712"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6747899159663866,(a) Banana
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6756302521008404,(b) Spiral
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6764705882352942,(c) Star
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6773109243697479,(d) Circle (e) L
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6781512605042017,Figure 7: Projected classification boundaries for the datasets in section 4.2 and the extra datasets.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6789915966386555,"(ResNet [20] and BERT [9], respectively). We shunt the interested reader into [19] for additional
713"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6798319327731093,"information. Additionally, we found discrepancies between the versions of the datasets in the study
714"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.680672268907563,"of [4] and [19]. We utilized the version of those datasets featured in [4] for our experiments due
715"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6815126050420168,"to popularity. This affected the datasets Arrhythmia, Annthyroid, Cardiotocography, InternetAds,
716"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6823529411764706,"Ionosphere, SpamBase, Waveform, WPBC and Hepatitis. Figure 10 summarizes the ranks from the
717"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6831932773109244,"one-class experiments in section 4.3. Table 5 summarizes the AUC results from our experiments. As
718"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6840336134453782,"mentioned in section A.3, we also included extra methods outside of our related work. Particularly,
719"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6848739495798319,"we added deep versions tailored to image data of previously included methods —DeepSVDD [35]
720"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6857142857142857,"and Deep Isolation Forest [42] (DIF)— and others that extend some types of outlier detectors into
721"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6865546218487395,"image and text data —LUNAR [16], as an extension of Locality-based classical methods, and
722"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6873949579831933,"AnoGAN [36], as an extension of Generative methods. For their parameters, we employed the
723"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6882352941176471,"recommended ones for LUNAR and DIF, and trained the models the same way that the authors did
724"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6890756302521008,"in their articles. As for DeepSVDD and AnoGAN, as they do not have any recommended way of
725"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6899159663865546,"training nor hyperparameters, we performed a grid search for their training parameters and kept the
726"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6907563025210084,"best result. We used all of their official implementations4. All deep methods (including MO-GAAL
727"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6915966386554622,4LUNAR and DIF have official implementations by their authors in pyod [43].
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.692436974789916,(a) Banana
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6932773109243697,(b) Spiral
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6941176470588235,(c) Star
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6949579831932773,(d) Circle (e) L
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6957983193277311,Figure 8: Projected classification boundaries of the competitors outside of our related work.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6966386554621848,Figure 9: AUC results in the MV datasets. GSAAL LOF
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6974789915966386,IForest ABOD SOD kNN SVDD
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6983193277310924,MO-GAAL GMM
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.6991596638655462,"1
2
3
4
5
6
7
8
9 Ranks"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7,Figure 10: Boxplots of the ranks used for the Conover-Iman experiment in section 4.3.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7008403361344537,"(a)
(b)"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7016806722689075,Figure 11: Performance of the detector with different values of k.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7025210084033613,"and GSAAL) were trained multiple times with the same train set and their results were averaged to
728"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7033613445378152,"account for initialization.
729"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.704201680672269,"Additionally, we gathered all extra deep methods and performed the same statistical analysis as in
730"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7050420168067227,"section 4.3. We also included MO GAAL besides GSAAL for completion. SO GAAL, the single
731"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7058823529411765,"generator version of MO GAAL was not included, even if popular in the related literature. The
732"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7067226890756303,"reason is that authors in [30] showed that MO GAAL constantly outperforms SO GAAL in the outlier
733"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7075630252100841,"detection task. Results are included in table 6, gathered after a positive Kruskal-Wallis test. As we can
734"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7084033613445379,"see, GSAAL outperform almost all competitors except LUNAR (the most recent method). However,
735"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7092436974789916,"LUNAR is incapable to detect change in the subspaces as GSAAL does, see section B.2. Therefore,
736"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7100840336134454,"regardless of considering the tabular related work, or the more generalist deep methods, GSAAL
737"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7109243697478992,"still can outperform most competitors in the field. Additionally, for those that GSAAL performs
738"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.711764705882353,"similar to, we showed that we are more sensible to changes in subspaces. This fact makes GSAAL
739"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7126050420168067,"the preferred option for One-class classification under MV.
740"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7134453781512605,"B.4
Parameter Sensibility
741"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7142857142857143,"We now explore the effect of the number of detectors in GSAAL, k, by repeating the previous
742"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7151260504201681,"experiments with varying k. Figure 11a plots the median AUC for different k values, showing a
743"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7159663865546219,"stabilization at larger k. Next, Figure 11b compares the results with a fixed k = 30 and the default
744"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7168067226890756,"value k = 2
√"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7176470588235294,"d used in the previous experiments; there is no large difference in either the AUC or the
745"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7184873949579832,"ranks. We also found that the results in Table 3 remain almost the same if one sets k = 30. So we
746"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.719327731092437,"recommend fixing k = 30, which makes GSAAL very suitable for high-dimensional data.
747"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7201680672268908,"B.5
Ablation study
748"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7210084033613445,"Lastly, we also performed an ablation study for GSAAL. We identify two critical components in our
749"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7218487394957983,"method, the subspace nature of our detectors, and the multiple detectors used. Table 7 contains a
750"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7226890756302521,"summary of the included features in each considered configuration. We will compare the performance
751"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7235294117647059,"of all the different configurations of GSAAL.
752"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7243697478991596,Table 5: AUC of all the methods tested in section 4.3 and extra methods.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7252100840336134,"Dataset
GSAAL
LOF
IForest
ABOD
SOD
KNN
SVDD
MO-GAAL
GMM
DeepSVDD
AnoGAN
DIF
LUNAR"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7260504201680672,"annthyroid
0,7681
0,6753
0,7094
0,7008
0,5243
0,6291
0,4611
0,5047
0,6932
0,872
0,4038
0,6228
0,8120
Arrhythmia
0,7532
0,7277
0,7695
0,7422
0,6514
0,7334
0,7442
0,6901
0,7296
0,7485
0,6133
0,7904
0,7412
Cardiotocography
0,8727
0,8038
0,7772
0,7956
0,3524
0,7733
0,8351
0,7912
0,7413
0,874
0,3248
0,5561
0,8219
CIFAR10
0,7862
0,7333
0,6853
0,7622
0,6607
0,7493
0,7074
0,6256
0,7462
0,6158
0,3705
0,6542
0,7612
FashionMNIST
0,8001
0,8995
0,8298
0,9009
0,7136
0,9179
0,8130
0,7930
0,9072
0,6981
0,7137
0,8336
0,9093
fault
0,6726
0,6436
0,6518
0,8019
0,5670
0,7849
0,5651
0,6821
0,6856
0,4972
0,4074
0,7240
0,8047
InternetAds
0,7809
0,8565
0,4739
0,8600
0,3663
0,8090
0,7063
0,7603
0,9113
0,8411
0,5165
0,4330
0,8036
Ionosphere
0,9593
0,9591
0,9377
0,9483
0,8250
0,9825
0,8379
0,9727
0,9644
0,967
0,8406
0,9159
0,9234
landsat
0,5217
0,7598
0,5927
0,7627
0,4821
0,7726
0,4792
0,4432
0,4998
0,69
0,4835
0,5579
0,7743
letter
0,6625
0,8888
0,6493
FA
0,7182
0,9066
0,9334
0,4828
0,8435
0,676
0,5257
0,6709
0,9450
mnist
0,7638
0,9484
0,8647
0,9189
0,4858
0,9318
FA
0,6151
0,9210
0,7604
0,2502
0,8540
0,9352
optdigits
0,8935
0,9991
0,8625
0,9846
0,4260
0,9983
0,9999
0,8105
0,8221
0,9086
0,6203
0,4751
0,9988
satellite
0,8630
0,8456
0,7834
FA
0,4745
0,8753
0,8740
FA
0,7957
0,7798
0,3099
0,7661
0,8517
satimage-2
0,9836
0,9966
0,9910
0,9977
0,6745
0,9992
0,9826
0,6317
0,9967
0,9755
0,3968
0,9987
0,9993
SpamBase
0,8717
0,7132
0,8374
0,7730
0,3774
0,7036
0,6302
0,7377
0,8034
0,7807
0,4826
0,4579
0,8244
speech
0,6029
0,5075
0,5030
0,8741
0,4364
0,4853
0,4640
0,5138
0,5217
0,6076
0,4821
0,4553
0,5070
SVHN
0,6859
0,7192
0,5834
0,6989
0,5781
0,6788
0,6150
0,7055
0,6684
0,5894
0,4621
0,6076
0,6319
Waveform
0,8092
0,7530
0,6902
0,7115
0,5814
0,7623
0,5514
0,6049
0,5791
0,7214
0,7018
0,7223
0,7570
WPBC
0,6326
0,5695
0,5681
0,6156
0,5333
0,5830
0,5681
0,5972
0,5660
0,4907
0,4121
0,3355
0,4872
Hepatitis
0,6982
0,5030
0,6568
0,5207
0,2959
0,5680
0,4024
FA
0,7574
0,8284
0,3787
0,3905
0,7219
MVTec-AD
0,9806
0,9679
0,9755
0,9689
0,9662
0,9703
0,9645
0,6412
0,9776
0,7422
0,5179
0,9689
0,9727
20newsgroups
0,5535
0,7854
0,6675
FA
0,7109
0,7260
0,6329
0,5313
0,8103
0,6063
0,4833
0,6715
0,7425"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.726890756302521,"Table 6: Results of the Conover-Iman test for all the Deep methods.
Method
AnoGAN
DIF
DeepSVDD
GSAAL
LUNAR
MO GAAL
AnoGAN
=
– –
– –
– –
– –
– –
DIF
++
=
–
– –
– –
DeepSVDD
++
+
=
–
–
++
GSAAL
++
++
+
=
++
LUNAR
++
++
+
=
++
MO GAAL
++
– –
– –
– –
="
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7277310924369748,Table 7: Summary of the included components in the ablation study.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7285714285714285,"Name
Subspace
Multiple Di
GSAAL✗✗
✗
✗
GSAAL✓✗
✓
✗
GSAAL✗✓
✗
✓
GSAAL
✓
✓"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7294117647058823,"We will employ, once again, the Conover-Iman test to compare the performance of all configuration
753"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7302521008403361,"in a statistically sound way. Table 8 contains the results of the ablation experiment. As expected, our
754"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7310924369747899,"fully configured method significantly outperformed all of the others. This further confirms that the
755"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7319327731092437,"performance increase over our competitors comes directly from tackling the MV problem.
756"
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7327731092436974,Table 8: Results of the Connover-Iman test for the ablation study.
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7336134453781512,"GSAAL✗✗
GSAAL✓✗
GSAAL✗✓
GSAAL
GSAAL✗✗
=
++
– –
– –
GSAAL✓✗
– –
=
– –
– –
GSAAL✗✓
++
++
=
– –
GSAAL
++
++
++
="
SELECT AN OUTLIER DETECTION METHOD M WITH SOME ASSUMPTION ABOUT THE NORMALITY OF THE DATA,0.7344537815126051,"NeurIPS Paper Checklist
757"
CLAIMS,0.7352941176470589,"1. Claims
758"
CLAIMS,0.7361344537815127,"Question: Do the main claims made in the abstract and introduction accurately reflect the
759"
CLAIMS,0.7369747899159664,"paper’s contributions and scope?
760"
CLAIMS,0.7378151260504202,"Answer: [Yes]
761"
CLAIMS,0.738655462184874,"Justification: sections 3 for the theoretical claims, 4.2 for the MV claims, and 4.3 for the
762"
CLAIMS,0.7394957983193278,"real world performance claims.
763"
CLAIMS,0.7403361344537815,"Guidelines:
764"
CLAIMS,0.7411764705882353,"• The answer NA means that the abstract and introduction do not include the claims
765"
CLAIMS,0.7420168067226891,"made in the paper.
766"
CLAIMS,0.7428571428571429,"• The abstract and/or introduction should clearly state the claims made, including the
767"
CLAIMS,0.7436974789915967,"contributions made in the paper and important assumptions and limitations. A No or
768"
CLAIMS,0.7445378151260504,"NA answer to this question will not be perceived well by the reviewers.
769"
CLAIMS,0.7453781512605042,"• The claims made should match theoretical and experimental results, and reflect how
770"
CLAIMS,0.746218487394958,"much the results can be expected to generalize to other settings.
771"
CLAIMS,0.7470588235294118,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
772"
CLAIMS,0.7478991596638656,"are not attained by the paper.
773"
LIMITATIONS,0.7487394957983193,"2. Limitations
774"
LIMITATIONS,0.7495798319327731,"Question: Does the paper discuss the limitations of the work performed by the authors?
775"
LIMITATIONS,0.7504201680672269,"Answer: [Yes]
776"
LIMITATIONS,0.7512605042016807,"Justification: Section 5.
777"
LIMITATIONS,0.7521008403361344,"Guidelines:
778"
LIMITATIONS,0.7529411764705882,"• The answer NA means that the paper has no limitation while the answer No means that
779"
LIMITATIONS,0.753781512605042,"the paper has limitations, but those are not discussed in the paper.
780"
LIMITATIONS,0.7546218487394958,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
781"
LIMITATIONS,0.7554621848739496,"• The paper should point out any strong assumptions and how robust the results are to
782"
LIMITATIONS,0.7563025210084033,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
783"
LIMITATIONS,0.7571428571428571,"model well-specification, asymptotic approximations only holding locally). The authors
784"
LIMITATIONS,0.7579831932773109,"should reflect on how these assumptions might be violated in practice and what the
785"
LIMITATIONS,0.7588235294117647,"implications would be.
786"
LIMITATIONS,0.7596638655462185,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
787"
LIMITATIONS,0.7605042016806722,"only tested on a few datasets or with a few runs. In general, empirical results often
788"
LIMITATIONS,0.761344537815126,"depend on implicit assumptions, which should be articulated.
789"
LIMITATIONS,0.7621848739495798,"• The authors should reflect on the factors that influence the performance of the approach.
790"
LIMITATIONS,0.7630252100840336,"For example, a facial recognition algorithm may perform poorly when image resolution
791"
LIMITATIONS,0.7638655462184873,"is low or images are taken in low lighting. Or a speech-to-text system might not be
792"
LIMITATIONS,0.7647058823529411,"used reliably to provide closed captions for online lectures because it fails to handle
793"
LIMITATIONS,0.7655462184873949,"technical jargon.
794"
LIMITATIONS,0.7663865546218488,"• The authors should discuss the computational efficiency of the proposed algorithms
795"
LIMITATIONS,0.7672268907563026,"and how they scale with dataset size.
796"
LIMITATIONS,0.7680672268907563,"• If applicable, the authors should discuss possible limitations of their approach to
797"
LIMITATIONS,0.7689075630252101,"address problems of privacy and fairness.
798"
LIMITATIONS,0.7697478991596639,"• While the authors might fear that complete honesty about limitations might be used by
799"
LIMITATIONS,0.7705882352941177,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
800"
LIMITATIONS,0.7714285714285715,"limitations that aren’t acknowledged in the paper. The authors should use their best
801"
LIMITATIONS,0.7722689075630252,"judgment and recognize that individual actions in favor of transparency play an impor-
802"
LIMITATIONS,0.773109243697479,"tant role in developing norms that preserve the integrity of the community. Reviewers
803"
LIMITATIONS,0.7739495798319328,"will be specifically instructed to not penalize honesty concerning limitations.
804"
THEORY ASSUMPTIONS AND PROOFS,0.7747899159663866,"3. Theory Assumptions and Proofs
805"
THEORY ASSUMPTIONS AND PROOFS,0.7756302521008404,"Question: For each theoretical result, does the paper provide the full set of assumptions and
806"
THEORY ASSUMPTIONS AND PROOFS,0.7764705882352941,"a complete (and correct) proof?
807"
THEORY ASSUMPTIONS AND PROOFS,0.7773109243697479,"Answer: [Yes]
808"
THEORY ASSUMPTIONS AND PROOFS,0.7781512605042017,"Justification: Section A.
809"
THEORY ASSUMPTIONS AND PROOFS,0.7789915966386555,"Guidelines:
810"
THEORY ASSUMPTIONS AND PROOFS,0.7798319327731092,"• The answer NA means that the paper does not include theoretical results.
811"
THEORY ASSUMPTIONS AND PROOFS,0.780672268907563,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
812"
THEORY ASSUMPTIONS AND PROOFS,0.7815126050420168,"referenced.
813"
THEORY ASSUMPTIONS AND PROOFS,0.7823529411764706,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
814"
THEORY ASSUMPTIONS AND PROOFS,0.7831932773109244,"• The proofs can either appear in the main paper or the supplemental material, but if
815"
THEORY ASSUMPTIONS AND PROOFS,0.7840336134453781,"they appear in the supplemental material, the authors are encouraged to provide a short
816"
THEORY ASSUMPTIONS AND PROOFS,0.7848739495798319,"proof sketch to provide intuition.
817"
THEORY ASSUMPTIONS AND PROOFS,0.7857142857142857,"• Inversely, any informal proof provided in the core of the paper should be complemented
818"
THEORY ASSUMPTIONS AND PROOFS,0.7865546218487395,"by formal proofs provided in appendix or supplemental material.
819"
THEORY ASSUMPTIONS AND PROOFS,0.7873949579831933,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.788235294117647,"4. Experimental Result Reproducibility
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890756302521008,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7899159663865546,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7907563025210084,"of the paper (regardless of whether the code and data are provided or not)?
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915966386554621,"Answer: [Yes]
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7924369747899159,"Justification: Section 4 includes all details about our experimental setup (competitors,
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932773109243697,"datasets, experiments & training). Section A in the appendix includes the pseudo-code as
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941176470588235,"well
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949579831932773,"Guidelines:
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.795798319327731,"• The answer NA means that the paper does not include experiments.
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7966386554621848,"• If the paper includes experiments, a No answer to this question will not be perceived
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7974789915966387,"well by the reviewers: Making the paper reproducible is important, regardless of
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7983193277310925,"whether the code and data are provided or not.
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7991596638655463,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"to make their results reproducible or verifiable.
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8008403361344538,"• Depending on the contribution, reproducibility can be accomplished in various ways.
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8016806722689076,"For example, if the contribution is a novel architecture, describing the architecture fully
837"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8025210084033614,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
838"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8033613445378152,"be necessary to either make it possible for others to replicate the model with the same
839"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042016806722689,"dataset, or provide access to the model. In general. releasing code and data is often
840"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8050420168067227,"one good way to accomplish this, but reproducibility can also be provided via detailed
841"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8058823529411765,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
842"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8067226890756303,"of a large language model), releasing of a model checkpoint, or other means that are
843"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.807563025210084,"appropriate to the research performed.
844"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8084033613445378,"• While NeurIPS does not require releasing code, the conference does require all submis-
845"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8092436974789916,"sions to provide some reasonable avenue for reproducibility, which may depend on the
846"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8100840336134454,"nature of the contribution. For example
847"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8109243697478992,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
848"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8117647058823529,"to reproduce that algorithm.
849"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8126050420168067,"(b) If the contribution is primarily a new model architecture, the paper should describe
850"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8134453781512605,"the architecture clearly and fully.
851"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8142857142857143,"(c) If the contribution is a new model (e.g., a large language model), then there should
852"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8151260504201681,"either be a way to access this model for reproducing the results or a way to reproduce
853"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8159663865546218,"the model (e.g., with an open-source dataset or instructions for how to construct
854"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8168067226890756,"the dataset).
855"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8176470588235294,"(d) We recognize that reproducibility may be tricky in some cases, in which case
856"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8184873949579832,"authors are welcome to describe the particular way they provide for reproducibility.
857"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.819327731092437,"In the case of closed-source models, it may be that access to the model is limited in
858"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8201680672268907,"some way (e.g., to registered users), but it should be possible for other researchers
859"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8210084033613445,"to have some path to reproducing or verifying the results.
860"
OPEN ACCESS TO DATA AND CODE,0.8218487394957983,"5. Open access to data and code
861"
OPEN ACCESS TO DATA AND CODE,0.8226890756302521,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
862"
OPEN ACCESS TO DATA AND CODE,0.8235294117647058,"tions to faithfully reproduce the main experimental results, as described in supplemental
863"
OPEN ACCESS TO DATA AND CODE,0.8243697478991596,"material?
864"
OPEN ACCESS TO DATA AND CODE,0.8252100840336134,"Answer: [Yes]
865"
OPEN ACCESS TO DATA AND CODE,0.8260504201680672,"Justification: We include our GitHub (anonymized for the double-blind phase).
866"
OPEN ACCESS TO DATA AND CODE,0.826890756302521,"Guidelines:
867"
OPEN ACCESS TO DATA AND CODE,0.8277310924369747,"• The answer NA means that paper does not include experiments requiring code.
868"
OPEN ACCESS TO DATA AND CODE,0.8285714285714286,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
869"
OPEN ACCESS TO DATA AND CODE,0.8294117647058824,"public/guides/CodeSubmissionPolicy) for more details.
870"
OPEN ACCESS TO DATA AND CODE,0.8302521008403362,"• While we encourage the release of code and data, we understand that this might not be
871"
OPEN ACCESS TO DATA AND CODE,0.83109243697479,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
872"
OPEN ACCESS TO DATA AND CODE,0.8319327731092437,"including code, unless this is central to the contribution (e.g., for a new open-source
873"
OPEN ACCESS TO DATA AND CODE,0.8327731092436975,"benchmark).
874"
OPEN ACCESS TO DATA AND CODE,0.8336134453781513,"• The instructions should contain the exact command and environment needed to run to
875"
OPEN ACCESS TO DATA AND CODE,0.8344537815126051,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
876"
OPEN ACCESS TO DATA AND CODE,0.8352941176470589,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
877"
OPEN ACCESS TO DATA AND CODE,0.8361344537815126,"• The authors should provide instructions on data access and preparation, including how
878"
OPEN ACCESS TO DATA AND CODE,0.8369747899159664,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
879"
OPEN ACCESS TO DATA AND CODE,0.8378151260504202,"• The authors should provide scripts to reproduce all experimental results for the new
880"
OPEN ACCESS TO DATA AND CODE,0.838655462184874,"proposed method and baselines. If only a subset of experiments are reproducible, they
881"
OPEN ACCESS TO DATA AND CODE,0.8394957983193277,"should state which ones are omitted from the script and why.
882"
OPEN ACCESS TO DATA AND CODE,0.8403361344537815,"• At submission time, to preserve anonymity, the authors should release anonymized
883"
OPEN ACCESS TO DATA AND CODE,0.8411764705882353,"versions (if applicable).
884"
OPEN ACCESS TO DATA AND CODE,0.8420168067226891,"• Providing as much information as possible in supplemental material (appended to the
885"
OPEN ACCESS TO DATA AND CODE,0.8428571428571429,"paper) is recommended, but including URLs to data and code is permitted.
886"
OPEN ACCESS TO DATA AND CODE,0.8436974789915966,"6. Experimental Setting/Details
887"
OPEN ACCESS TO DATA AND CODE,0.8445378151260504,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
888"
OPEN ACCESS TO DATA AND CODE,0.8453781512605042,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
889"
OPEN ACCESS TO DATA AND CODE,0.846218487394958,"results?
890"
OPEN ACCESS TO DATA AND CODE,0.8470588235294118,"Answer: [Yes]
891"
OPEN ACCESS TO DATA AND CODE,0.8478991596638655,"Justification: We explain our processes for one-class classification in section 4.3. Hyper-
892"
OPEN ACCESS TO DATA AND CODE,0.8487394957983193,"parameters, as well as optimizers, are included in section 4.1. Additionally, our remote
893"
OPEN ACCESS TO DATA AND CODE,0.8495798319327731,"repository contains the full details.
894"
OPEN ACCESS TO DATA AND CODE,0.8504201680672269,"Guidelines:
895"
OPEN ACCESS TO DATA AND CODE,0.8512605042016806,"• The answer NA means that the paper does not include experiments.
896"
OPEN ACCESS TO DATA AND CODE,0.8521008403361344,"• The experimental setting should be presented in the core of the paper to a level of detail
897"
OPEN ACCESS TO DATA AND CODE,0.8529411764705882,"that is necessary to appreciate the results and make sense of them.
898"
OPEN ACCESS TO DATA AND CODE,0.853781512605042,"• The full details can be provided either with the code, in appendix, or as supplemental
899"
OPEN ACCESS TO DATA AND CODE,0.8546218487394958,"material.
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554621848739495,"7. Experiment Statistical Significance
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8563025210084033,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"information about the statistical significance of the experiments?
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579831932773109,"Answer: [Yes]
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588235294117647,"Justification: We utilized a statistical test to study the significance of all of our performance
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8596638655462185,"results —see tables 3, 6, 8. We also extensively used boxplots of all AUC results to visualize
906"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8605042016806723,"our performance in different scenarios —see figures 6, 9, 10, 11.b.
907"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8613445378151261,"Guidelines:
908"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621848739495799,"• The answer NA means that the paper does not include experiments.
909"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8630252100840337,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
910"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8638655462184874,"dence intervals, or statistical significance tests, at least for the experiments that support
911"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8647058823529412,"the main claims of the paper.
912"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.865546218487395,"• The factors of variability that the error bars are capturing should be clearly stated (for
913"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8663865546218488,"example, train/test split, initialization, random drawing of some parameter, or overall
914"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8672268907563025,"run with given experimental conditions).
915"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8680672268907563,"• The method for calculating the error bars should be explained (closed form formula,
916"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8689075630252101,"call to a library function, bootstrap, etc.)
917"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8697478991596639,"• The assumptions made should be given (e.g., Normally distributed errors).
918"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8705882352941177,"• It should be clear whether the error bar is the standard deviation or the standard error
919"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8714285714285714,"of the mean.
920"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8722689075630252,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
921"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.873109243697479,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
922"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8739495798319328,"of Normality of errors is not verified.
923"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8747899159663866,"• For asymmetric distributions, the authors should be careful not to show in tables or
924"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8756302521008403,"figures symmetric error bars that would yield results that are out of range (e.g. negative
925"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8764705882352941,"error rates).
926"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8773109243697479,"• If error bars are reported in tables or plots, The authors should explain in the text how
927"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8781512605042017,"they were calculated and reference the corresponding figures or tables in the text.
928"
EXPERIMENTS COMPUTE RESOURCES,0.8789915966386554,"8. Experiments Compute Resources
929"
EXPERIMENTS COMPUTE RESOURCES,0.8798319327731092,"Question: For each experiment, does the paper provide sufficient information on the com-
930"
EXPERIMENTS COMPUTE RESOURCES,0.880672268907563,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
931"
EXPERIMENTS COMPUTE RESOURCES,0.8815126050420168,"the experiments?
932"
EXPERIMENTS COMPUTE RESOURCES,0.8823529411764706,"Answer: [Yes]
933"
EXPERIMENTS COMPUTE RESOURCES,0.8831932773109243,"Justification: See the beginning of section B
934"
EXPERIMENTS COMPUTE RESOURCES,0.8840336134453781,"Guidelines:
935"
EXPERIMENTS COMPUTE RESOURCES,0.8848739495798319,"• The answer NA means that the paper does not include experiments.
936"
EXPERIMENTS COMPUTE RESOURCES,0.8857142857142857,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
937"
EXPERIMENTS COMPUTE RESOURCES,0.8865546218487395,"or cloud provider, including relevant memory and storage.
938"
EXPERIMENTS COMPUTE RESOURCES,0.8873949579831932,"• The paper should provide the amount of compute required for each of the individual
939"
EXPERIMENTS COMPUTE RESOURCES,0.888235294117647,"experimental runs as well as estimate the total compute.
940"
EXPERIMENTS COMPUTE RESOURCES,0.8890756302521008,"• The paper should disclose whether the full research project required more compute
941"
EXPERIMENTS COMPUTE RESOURCES,0.8899159663865546,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
942"
EXPERIMENTS COMPUTE RESOURCES,0.8907563025210085,"didn’t make it into the paper).
943"
CODE OF ETHICS,0.8915966386554622,"9. Code Of Ethics
944"
CODE OF ETHICS,0.892436974789916,"Question: Does the research conducted in the paper conform, in every respect, with the
945"
CODE OF ETHICS,0.8932773109243698,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
946"
CODE OF ETHICS,0.8941176470588236,"Answer: [Yes]
947"
CODE OF ETHICS,0.8949579831932774,"Justification: We reviewed the NeurIPS Code of Ethics and found no violation.
948"
CODE OF ETHICS,0.8957983193277311,"Guidelines:
949"
CODE OF ETHICS,0.8966386554621849,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
950"
CODE OF ETHICS,0.8974789915966387,"• If the authors answer No, they should explain the special circumstances that require a
951"
CODE OF ETHICS,0.8983193277310925,"deviation from the Code of Ethics.
952"
CODE OF ETHICS,0.8991596638655462,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
953"
CODE OF ETHICS,0.9,"eration due to laws or regulations in their jurisdiction).
954"
BROADER IMPACTS,0.9008403361344538,"10. Broader Impacts
955"
BROADER IMPACTS,0.9016806722689076,"Question: Does the paper discuss both potential positive societal impacts and negative
956"
BROADER IMPACTS,0.9025210084033614,"societal impacts of the work performed?
957"
BROADER IMPACTS,0.9033613445378151,"Answer: [Yes]
958"
BROADER IMPACTS,0.9042016806722689,"Justification: In sections, 1 & 5 we go through the importance of outlier detection in
959"
BROADER IMPACTS,0.9050420168067227,"many fields, particularly for our use-case. Our positive impact on society consists of the
960"
BROADER IMPACTS,0.9058823529411765,"improvement of the tasks where outlier detection is needed.
961"
BROADER IMPACTS,0.9067226890756303,"Guidelines:
962"
BROADER IMPACTS,0.907563025210084,"• The answer NA means that there is no societal impact of the work performed.
963"
BROADER IMPACTS,0.9084033613445378,"• If the authors answer NA or No, they should explain why their work has no societal
964"
BROADER IMPACTS,0.9092436974789916,"impact or why the paper does not address societal impact.
965"
BROADER IMPACTS,0.9100840336134454,"• Examples of negative societal impacts include potential malicious or unintended uses
966"
BROADER IMPACTS,0.9109243697478991,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
967"
BROADER IMPACTS,0.9117647058823529,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
968"
BROADER IMPACTS,0.9126050420168067,"groups), privacy considerations, and security considerations.
969"
BROADER IMPACTS,0.9134453781512605,"• The conference expects that many papers will be foundational research and not tied
970"
BROADER IMPACTS,0.9142857142857143,"to particular applications, let alone deployments. However, if there is a direct path to
971"
BROADER IMPACTS,0.915126050420168,"any negative applications, the authors should point it out. For example, it is legitimate
972"
BROADER IMPACTS,0.9159663865546218,"to point out that an improvement in the quality of generative models could be used to
973"
BROADER IMPACTS,0.9168067226890756,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
974"
BROADER IMPACTS,0.9176470588235294,"that a generic algorithm for optimizing neural networks could enable people to train
975"
BROADER IMPACTS,0.9184873949579831,"models that generate Deepfakes faster.
976"
BROADER IMPACTS,0.9193277310924369,"• The authors should consider possible harms that could arise when the technology is
977"
BROADER IMPACTS,0.9201680672268907,"being used as intended and functioning correctly, harms that could arise when the
978"
BROADER IMPACTS,0.9210084033613445,"technology is being used as intended but gives incorrect results, and harms following
979"
BROADER IMPACTS,0.9218487394957983,"from (intentional or unintentional) misuse of the technology.
980"
BROADER IMPACTS,0.9226890756302522,"• If there are negative societal impacts, the authors could also discuss possible mitigation
981"
BROADER IMPACTS,0.9235294117647059,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
982"
BROADER IMPACTS,0.9243697478991597,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
983"
BROADER IMPACTS,0.9252100840336135,"feedback over time, improving the efficiency and accessibility of ML).
984"
SAFEGUARDS,0.9260504201680673,"11. Safeguards
985"
SAFEGUARDS,0.926890756302521,"Question: Does the paper describe safeguards that have been put in place for responsible
986"
SAFEGUARDS,0.9277310924369748,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
987"
SAFEGUARDS,0.9285714285714286,"image generators, or scraped datasets)?
988"
SAFEGUARDS,0.9294117647058824,"Answer: [NA]
989"
SAFEGUARDS,0.9302521008403362,"Justification: We do not identify any risks.
990"
SAFEGUARDS,0.9310924369747899,"Guidelines:
991"
SAFEGUARDS,0.9319327731092437,"• The answer NA means that the paper poses no such risks.
992"
SAFEGUARDS,0.9327731092436975,"• Released models that have a high risk for misuse or dual-use should be released with
993"
SAFEGUARDS,0.9336134453781513,"necessary safeguards to allow for controlled use of the model, for example by requiring
994"
SAFEGUARDS,0.934453781512605,"that users adhere to usage guidelines or restrictions to access the model or implementing
995"
SAFEGUARDS,0.9352941176470588,"safety filters.
996"
SAFEGUARDS,0.9361344537815126,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
997"
SAFEGUARDS,0.9369747899159664,"should describe how they avoided releasing unsafe images.
998"
SAFEGUARDS,0.9378151260504202,"• We recognize that providing effective safeguards is challenging, and many papers do
999"
SAFEGUARDS,0.938655462184874,"not require this, but we encourage authors to take this into account and make a best
1000"
SAFEGUARDS,0.9394957983193277,"faith effort.
1001"
LICENSES FOR EXISTING ASSETS,0.9403361344537815,"12. Licenses for existing assets
1002"
LICENSES FOR EXISTING ASSETS,0.9411764705882353,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1003"
LICENSES FOR EXISTING ASSETS,0.9420168067226891,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1004"
LICENSES FOR EXISTING ASSETS,0.9428571428571428,"properly respected?
1005"
LICENSES FOR EXISTING ASSETS,0.9436974789915966,"Answer: [Yes]
1006"
LICENSES FOR EXISTING ASSETS,0.9445378151260504,"Justification: We include URLs and citations for all dataset selections, packages, and
1007"
LICENSES FOR EXISTING ASSETS,0.9453781512605042,"methods.
1008"
LICENSES FOR EXISTING ASSETS,0.946218487394958,"Guidelines:
1009"
LICENSES FOR EXISTING ASSETS,0.9470588235294117,"• The answer NA means that the paper does not use existing assets.
1010"
LICENSES FOR EXISTING ASSETS,0.9478991596638655,"• The authors should cite the original paper that produced the code package or dataset.
1011"
LICENSES FOR EXISTING ASSETS,0.9487394957983193,"• The authors should state which version of the asset is used and, if possible, include a
1012"
LICENSES FOR EXISTING ASSETS,0.9495798319327731,"URL.
1013"
LICENSES FOR EXISTING ASSETS,0.9504201680672268,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1014"
LICENSES FOR EXISTING ASSETS,0.9512605042016806,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1015"
LICENSES FOR EXISTING ASSETS,0.9521008403361344,"service of that source should be provided.
1016"
LICENSES FOR EXISTING ASSETS,0.9529411764705882,"• If assets are released, the license, copyright information, and terms of use in the package
1017"
LICENSES FOR EXISTING ASSETS,0.9537815126050421,"should be provided. For popular datasets, paperswithcode.com/datasets has
1018"
LICENSES FOR EXISTING ASSETS,0.9546218487394958,"curated licenses for some datasets. Their licensing guide can help determine the license
1019"
LICENSES FOR EXISTING ASSETS,0.9554621848739496,"of a dataset.
1020"
LICENSES FOR EXISTING ASSETS,0.9563025210084034,"• For existing datasets that are re-packaged, both the original license and the license of
1021"
LICENSES FOR EXISTING ASSETS,0.9571428571428572,"the derived asset (if it has changed) should be provided.
1022"
LICENSES FOR EXISTING ASSETS,0.957983193277311,"• If this information is not available online, the authors are encouraged to reach out to
1023"
LICENSES FOR EXISTING ASSETS,0.9588235294117647,"the asset’s creators.
1024"
NEW ASSETS,0.9596638655462185,"13. New Assets
1025"
NEW ASSETS,0.9605042016806723,"Question: Are new assets introduced in the paper well documented and is the documentation
1026"
NEW ASSETS,0.9613445378151261,"provided alongside the assets?
1027"
NEW ASSETS,0.9621848739495799,"Answer: [Yes]
1028"
NEW ASSETS,0.9630252100840336,"Justification: We include the documentation of our implementation in the repository.
1029"
NEW ASSETS,0.9638655462184874,"Guidelines:
1030"
NEW ASSETS,0.9647058823529412,"• The answer NA means that the paper does not release new assets.
1031"
NEW ASSETS,0.965546218487395,"• Researchers should communicate the details of the dataset/code/model as part of their
1032"
NEW ASSETS,0.9663865546218487,"submissions via structured templates. This includes details about training, license,
1033"
NEW ASSETS,0.9672268907563025,"limitations, etc.
1034"
NEW ASSETS,0.9680672268907563,"• The paper should discuss whether and how consent was obtained from people whose
1035"
NEW ASSETS,0.9689075630252101,"asset is used.
1036"
NEW ASSETS,0.9697478991596639,"• At submission time, remember to anonymize your assets (if applicable). You can either
1037"
NEW ASSETS,0.9705882352941176,"create an anonymized URL or include an anonymized zip file.
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714285714285714,"14. Crowdsourcing and Research with Human Subjects
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722689075630252,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973109243697479,"include the full text of instructions given to participants and screenshots, if applicable, as
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739495798319328,"well as details about compensation (if any)?
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747899159663865,"Answer: [NA]
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756302521008403,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764705882352941,"Guidelines:
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773109243697479,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781512605042016,"human subjects.
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789915966386554,"• Including this information in the supplemental material is fine, but if the main contribu-
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798319327731092,"tion of the paper involves human subjects, then as much detail as possible should be
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980672268907563,"included in the main paper.
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815126050420168,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823529411764705,"or other labor should be paid at least the minimum wage in the country of the data
1052"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831932773109243,"collector.
1053"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840336134453781,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1054"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984873949579832,"Subjects
1055"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857142857142858,"Question: Does the paper describe potential risks incurred by study participants, whether
1056"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865546218487395,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1057"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873949579831933,"approvals (or an equivalent approval/review based on the requirements of your country or
1058"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882352941176471,"institution) were obtained?
1059"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890756302521009,"Answer: [NA]
1060"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899159663865547,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1061"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907563025210084,"Guidelines:
1062"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915966386554622,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1063"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992436974789916,"human subjects.
1064"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932773109243698,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1065"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941176470588236,"may be required for any human subjects research. If you obtained IRB approval, you
1066"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949579831932773,"should clearly state this in the paper.
1067"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957983193277311,"• We recognize that the procedures for this may vary significantly between institutions
1068"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966386554621849,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1069"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974789915966387,"guidelines for their institution.
1070"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983193277310924,"• For initial submissions, do not include any information that would break anonymity (if
1071"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991596638655462,"applicable), such as the institution conducting the review.
1072"
