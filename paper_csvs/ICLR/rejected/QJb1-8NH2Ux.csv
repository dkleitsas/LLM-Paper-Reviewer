Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005154639175257732,"Making classiÔ¨Åers robust to adversarial examples is challenging. Thus, many de-
fenses tackle the seemingly easier task of detecting perturbed inputs.
We show a barrier towards this goal. We prove a general hardness reduction be-
tween detection and classiÔ¨Åcation of adversarial examples: given a robust detector
for attacks at distance œµ (in some metric), we show how to build a similarly robust
(but inefÔ¨Åcient) classiÔ¨Åer for attacks at distance œµ/2‚Äîand vice-versa.
Our reduction is computationally inefÔ¨Åcient, and thus cannot be used to build
practical classiÔ¨Åers. Instead, it is a useful sanity check to test whether empiri-
cal detection results imply something much stronger than the authors presumably
anticipated.
To illustrate, we revisit 14 empirical detector defenses published over the past
years. For 12/14 defenses, we show that the claimed detection results imply an
inefÔ¨Åcient classiÔ¨Åer with robustness far beyond the state-of-the-art‚Äî thus casting
some doubts on the results‚Äô validity.
Finally, we show that our reduction applies in both directions: a robust classi-
Ô¨Åer for attacks at distance œµ/2 implies an inefÔ¨Åcient robust detector at distance œµ.
Thus, we argue that robust classiÔ¨Åcation and robust detection should be regarded
as (near)-equivalent problems, if we disregard their computational complexity."
INTRODUCTION,0.010309278350515464,"1
INTRODUCTION"
INTRODUCTION,0.015463917525773196,"Building models that are robust to adversarial examples (Szegedy et al., 2014; Biggio et al., 2013) is
a major challenge and open-problem in machine learning. Due to the inherent difÔ¨Åculty in building
robust classiÔ¨Åers, researchers have attempted to build techniques to at least detect adversarial exam-
ples, a weaker task that is largely considered easier than robust classiÔ¨Åcation (Xu et al., 2018; Pang
et al., 2021; Sheikholeslami et al., 2021)."
INTRODUCTION,0.020618556701030927,"Yet, evaluating the robustness of empirical detector defenses is challenging. This is in part due to
a lack of strong evaluation guidelines and benchmarks‚Äîakin to those developed for robust classi-
Ô¨Åers (Carlini et al., 2019; Croce et al., 2020)‚Äîas well as to a lack of long-standing comparative
baselines such as adversarial training (Madry et al., 2018)."
INTRODUCTION,0.02577319587628866,"To illustrate, consider the following (Ô¨Åctitious) claims about two defenses against adversarial exam-
ples on CIFAR-10:"
INTRODUCTION,0.030927835051546393,"‚Ä¢ defense A is a classiÔ¨Åer that achieves robust accuracy of 90% under ‚Ñì‚àû-perturbations
bounded by œµ = 4/255;
‚Ä¢ defense B also has a ‚Äúrejection‚Äù option, and achieves robust accuracy of 90% under ‚Ñì‚àû-
perturbations bounded by œµ = 8/255 (we say that defense B is robust for some example
if it classiÔ¨Åes that example correctly, and either rejects/detects or correctly classiÔ¨Åes all
perturbed examples at distance œµ.)"
INTRODUCTION,0.03608247422680412,Which of these two (empirical) claims are you more likely to believe to be correct?
INTRODUCTION,0.041237113402061855,"Defense A claims much higher robustness than the current best result achieved with adversarial
training (Madry et al., 2018; RebufÔ¨Ået al., 2021), the only empirical defense against adversarial
examples that has stood the test of time. Indeed, the state-of-the-art ‚Ñì‚àûrobustness for œµ = 4/255"
INTRODUCTION,0.04639175257731959,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05154639175257732,"on CIFAR-10 (without external data) is ‚âà79% (RebufÔ¨Ået al., 2021). Thus, the claim of defense A
would likely be met with some initial skepticism and heightened scrutiny, as could be expected for
such a claimed breakthrough result."
INTRODUCTION,0.05670103092783505,"The claim of defense B is harder to assess, due to a lack of long-standing baselines for robust
detectors (many detection defenses have been shown to be broken (Carlini & Wagner, 2017; Tram`er
et al., 2020)). On one hand, detection of adversarial examples has largely been considered to be an
easier task than classiÔ¨Åcation (Xu et al., 2018; Pang et al., 2021; Sheikholeslami et al., 2021). On
the other hand, defense B claims robustness to perturbations that are twice as large as defense A
(œµ = 8/255 vs. œµ = 4/255)."
INTRODUCTION,0.061855670103092786,"In this paper, we show that the claims of defenses A and B are, in fact, equivalent! (up to computa-
tional efÔ¨Åciency.)"
INTRODUCTION,0.06701030927835051,"We prove a general hardness reduction between classiÔ¨Åcation and detection of adversarial examples.
Given a detector defense that achieves robust risk Œ± for attacks at distance œµ (under any metric), we
show how to build an explicit but inefÔ¨Åcient classiÔ¨Åer that achieves robust risk Œ± for classifying
attacks at distance œµ/2. The reverse implication also holds: a classiÔ¨Åer robust at distance œµ/2 implies
an explicit but inefÔ¨Åcient robust detector at distance œµ."
INTRODUCTION,0.07216494845360824,"To the authors knowledge, there is no known way of leveraging computational inefÔ¨Åciency to build
more robust models. We should thus be as ‚Äúsurprised‚Äù by the claim made by defense B as by the
claim made by defense A."
INTRODUCTION,0.07731958762886598,"Our reduction provides a way of assessing the plausibility of new robust detection claims, by con-
trasting them with results from the more mature literature on robust classiÔ¨Åcation. To illustrate, we
revisit 14 published detection defenses across three datasets, and show that in 12/14 cases the de-
fense‚Äôs robust detection claims would imply an inefÔ¨Åcient classiÔ¨Åer with robustness far superior to
the current state-of-the-art. Yet, none of these detection papers make the claim that their techniques
should imply such a breakthrough in robust classiÔ¨Åcation."
INTRODUCTION,0.08247422680412371,"Using our reduction, it is obvious that many detection defenses are claiming much stronger robust-
ness than we believe feasible with current techniques. And indeed, many of these defenses were
later shown to have overestimated their robustness (Carlini & Wagner, 2017; Tram`er et al., 2020)."
INTRODUCTION,0.08762886597938144,"Remarkably, we Ô¨Ånd that for certiÔ¨Åed defenses, the state-of-the-art results for provable robust clas-
siÔ¨Åcation and detection perfectly match the results implied by our reduction. For example, Sheik-
holeslami et al. (2021) recently proposed a certiÔ¨Åed detector on CIFAR-10 with provable robust
error that is within 3% of the provable error of the inefÔ¨Åcient detector obtained by combining our
result with the state-of-the-art robust classiÔ¨Åer of Zhang et al. (2020a)."
INTRODUCTION,0.09278350515463918,"In summary, we prove that giving classiÔ¨Åers access to a detection option does not help robustness (or
at least, not much). Our work provides, to our knowledge, the Ô¨Årst example of a hardness reduction
between different approaches for robust machine learning. As in the case of computational com-
plexity, we believe that such reductions can be useful for identifying research questions or areas that
are unlikely to bear fruit (bar a signiÔ¨Åcant breakthrough)‚Äîso that the majority of the community‚Äôs
efforts can be redirected elsewhere."
INTRODUCTION,0.0979381443298969,"On a technical level, our reduction exposes a natural connection between robustness and error cor-
recting codes, which may be of independent interest."
"HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS",0.10309278350515463,"2
HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS"
"HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS",0.10824742268041238,"In this section, we prove our main result: a reduction between robust detectors and robust classiÔ¨Åers,
and vice-versa. We Ô¨Årst introduce some useful notation and deÔ¨Åne the (robust) risk of classiÔ¨Åers
with and without a detection option."
PRELIMINARIES,0.1134020618556701,"2.1
PRELIMINARIES"
PRELIMINARIES,0.11855670103092783,"We consider a classiÔ¨Åcation task with a distribution D over examples x ‚ààRd with labels y ‚àà
[C]. A classiÔ¨Åer is a function f : Rd ‚Üí[C]. A detector is a classiÔ¨Åer with an extra ‚Äúrejection‚Äù"
PRELIMINARIES,0.12371134020618557,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.12886597938144329,"or ‚Äùdetection‚Äù option ‚ä•, that indicates the absence of a classiÔ¨Åcation. We assume for simplicity
that classiÔ¨Åers and detectors are deterministic. Our results can easily be extended to randomized
functions as well. The binary indicator function 1{A} is 1 if and only if the predicate A is true."
PRELIMINARIES,0.13402061855670103,"We Ô¨Årst deÔ¨Åne a classiÔ¨Åer‚Äôs risk, i.e., its classiÔ¨Åcation error on unperturbed samples."
PRELIMINARIES,0.13917525773195877,"DeÔ¨Ånition 1 (Risk). Let f : Rd ‚Üí[C] ‚à™{‚ä•} be a classiÔ¨Åer (optionally with a detection output ‚ä•).
The risk of f is the expected rate at which f fails to correctly classify a sample:"
PRELIMINARIES,0.14432989690721648,"R(f) :=
E
(x,y)‚àºD"
PRELIMINARIES,0.14948453608247422,"
1{f(x)Ã∏=y}

(1)"
PRELIMINARIES,0.15463917525773196,"Note that for a detector, rejecting an unperturbed example sampled from the distribution D is counted
as an error."
PRELIMINARIES,0.15979381443298968,"For classiÔ¨Åers without a rejection option, we deÔ¨Åne the robust risk as the risk on worst-case adver-
sarial examples (Madry et al., 2018). Given an input x sampled from D, an adversarial example ÀÜx
is constrained to being within distance d(x, ÀÜx) ‚â§œµ from x, where d is some distance measure."
PRELIMINARIES,0.16494845360824742,DeÔ¨Ånition 2 (Robust risk). Let f : Rd ‚Üí[C] be a classiÔ¨Åer. The robust risk at distance œµ is:
PRELIMINARIES,0.17010309278350516,"Rœµ
adv(f) :=
E
(x,y)‚àºD"
PRELIMINARIES,0.17525773195876287,"
max
d(x,ÀÜx)‚â§œµ 1{f(ÀÜx)Ã∏=y} 
(2)"
PRELIMINARIES,0.18041237113402062,"Thus, a sample (x, y) is robustly classiÔ¨Åed if and only if every point within distance œµ of x (including
x itself) is correctly classiÔ¨Åed as y."
PRELIMINARIES,0.18556701030927836,"For a detector (a classiÔ¨Åer with an extra detection/rejection output), we analogously deÔ¨Åne the robust
risk with detection. The classiÔ¨Åer is now allowed to reject adversarial examples."
PRELIMINARIES,0.19072164948453607,"DeÔ¨Ånition 3 (Robust risk with detection). Let f : Rd ‚Üí[C] ‚à™{‚ä•} be a classiÔ¨Åer with an extra
detection output ‚ä•. The robust risk with detection at distance œµ is:"
PRELIMINARIES,0.1958762886597938,"Rœµ
adv-det(f) :=
E
(x,y)‚àºD"
PRELIMINARIES,0.20103092783505155,"
max
d(x,ÀÜx)‚â§œµ 1{f(x)Ã∏=y ‚à®f(ÀÜx)/‚àà{y,‚ä•}} 
(3)"
PRELIMINARIES,0.20618556701030927,"That is, a detector defense f is robust on a natural input x if and only if the defense classiÔ¨Åes the
natural input x correctly, and the defense either rejects or correctly classiÔ¨Åes every perturbed input
ÀÜx within distance œµ from x. The requirement that the defense correctly classify natural examples
eliminates pathological defenses that reject all inputs."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.211340206185567,"2.2
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION"
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.21649484536082475,"We are now ready to introduce our main result, a reduction from a robust detector for adversarial
examples at distance œµ, to an inefÔ¨Åcient robust classiÔ¨Åer at distance œµ/2. We later prove that this
reduction also holds in the reverse direction, thereby demonstrating the equivalence between robust
detection and classiÔ¨Åcation‚Äîup to computational hardness."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.22164948453608246,"Theorem 4 (œµ-robust detection implies inefÔ¨Åcient œµ/2-robust classiÔ¨Åcation). Let d(¬∑, ¬∑) be an ar-
bitrary metric. Let f be a detector that achieves risk R(f) = Œ±, and robust risk with detection
Rœµ
adv-det(f) = Œ≤. Then, we can construct an explicit (but inefÔ¨Åcient) classiÔ¨Åer g that achieves risk
R(g) ‚â§Œ± and robust risk Rœµ/2
adv (g) ‚â§Œ≤."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.2268041237113402,The classiÔ¨Åer g is constructed as follows on input x:
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.23195876288659795,"‚Ä¢ Run the detector model y ‚Üêf(x). If the input is not rejected, i.e., y Ã∏= ‚ä•, then output the label
y that was predicted by the detector."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.23711340206185566,"‚Ä¢ Otherwise, Ô¨Ånd an input x‚Ä≤ within distance œµ/2 of x that is not rejected, i.e., d(x, x‚Ä≤) ‚â§œµ/2 and
f(x‚Ä≤) Ã∏= ‚ä•. If such an input x‚Ä≤ exists, output the label y ‚Üêf(x‚Ä≤). Else, output a uniformly
random label y ‚àà[C]."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.2422680412371134,"An intuitive illustration for our construction, and for the proof of the theorem (see below) is in
Figure 1."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.24742268041237114,Under review as a conference paper at ICLR 2022 ùúñ/2 ùúñ x xÃÇ
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.25257731958762886,"Figure 1: Illustration of the construction of a robust classiÔ¨Åer from a robust detector in Theorem 4.
The outer blue circle represents all inputs at distance at most œµ from the input x. For a detector f,
the areas in green correspond to correctly classiÔ¨Åed inputs, and ratcheted gray areas correspond to
rejected inputs. The detector f is thus robust on x up to distance œµ. The classiÔ¨Åer g classiÔ¨Åes a
perturbed input ÀÜx, at distance œµ/2 from x, by Ô¨Ånding any input within distance œµ/2 from ÀÜx (the red
dashed circle) that is not rejected by f. Such an input necessarily exists and is correctly labeled by
f. The classiÔ¨Åer g is thus robust on x up to distance œµ/2."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.25773195876288657,"Our construction can be viewed as an analog of minimum distance decoding in coding theory. We
can view a clean data point sampled from D as a codeword, and an adversarial example ÀÜx as a noisy
message with a certain number of errors (where the error magnitude is measured using an arbitrary
metric on Rd rather than the Hamming distance that is typically used for error correcting codes).
A standard result in coding theory states that if a code can detect Œ± errors, then it can correct Œ±/2
errors. This result follows from a ‚Äúball-packing‚Äù argument: if Œ± errors can be detected, then any
two valid codewords must be at least at distance Œ± from each other, and therefore Œ±/2 errors can be
corrected via minimum distance decoding."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.26288659793814434,"Proof of Theorem 4. First, note that the natural accuracy of our constructed classiÔ¨Åer g is at least as
high as that of the detector f, since g always mimics the output of f whenever f does not reject an
input sampled from D. Thus, R(g) ‚â§R(f) = Œ±."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.26804123711340205,"Now, for the sake of contradiction, consider an input (x, y) ‚àºD for which the constructed classiÔ¨Åer
g is not robust at distance œµ/2. By construction, this means that there exists some input ÀÜx at distance
œµ/2 from x such that ÀÜx is misclassiÔ¨Åed, i.e., g(ÀÜx) = ÀÜy Ã∏= y. We will show that the detector f is not
robust with detection for x either (for attacks at distance up to œµ)."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.27319587628865977,"By deÔ¨Ånition of the classiÔ¨Åer g, if g(ÀÜx) = ÀÜy Ã∏= y then either:"
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.27835051546391754,"‚Ä¢ The detector f also misclassiÔ¨Åes ÀÜx, i.e., f(ÀÜx) = ÀÜy."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.28350515463917525,So f is not robust with detection for x at distance œµ.
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.28865979381443296,"‚Ä¢ There exists an input x‚Ä≤ within distance œµ/2 of x, such that the detector f misclassiÔ¨Åes x‚Ä≤,
i.e. f(x‚Ä≤) = ÀÜy."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.29381443298969073,"Note that by the triangular inequality, d(x, x‚Ä≤) ‚â§d(x, ÀÜx) + d(ÀÜx, x‚Ä≤) ‚â§œµ/2 + œµ/2 = œµ, and
thus f is not robust with detection for x at distance œµ."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.29896907216494845,"‚Ä¢ The detector f rejects all inputs x‚Ä≤ within distance œµ/2 of x (and thus g has output ÀÜy by
sampling a label at random)."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.30412371134020616,"Since d(x, ÀÜx) ‚â§œµ/2, this implies that the detector also rejects the clean input x, i.e.,
f(x) = ‚ä•, and thus f is not robust with detection for x."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.30927835051546393,Under review as a conference paper at ICLR 2022
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.31443298969072164,"In summary, whenever the constructed classiÔ¨Åer g fails to robustly classify an input x up to dis-
tance œµ/2, the detector f also fails to robustly classify x with detection up to distance œµ. Taking
expectations over the entire distribution D concludes the proof."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.31958762886597936,"Note that the classiÔ¨Åer g constructed in Theorem 4 is computationally inefÔ¨Åcient. Indeed, the second
step of the defense consists in Ô¨Ånding a non-rejected input within some metric ball. If the original
detector f is a non-convex function (e.g., a deep neural network), then this step consists in solving
an intractable non-convex optimization problem. Our reduction is thus typically not suitable for
building a practical robust classiÔ¨Åer. Instead, it demonstrates the existence of an inefÔ¨Åcient but
explicit robust classiÔ¨Åer. We discuss the implications of this result more thoroughly in Section 3."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.3247422680412371,"A corollary to our reduction is that many ‚Äúinformation theoretic‚Äù results about robust classiÔ¨Åers can
be directly extended to robust detectors. For example, Tsipras et al. (2019) prove that there exists a
formal tradeoff between a classiÔ¨Åer‚Äôs clean accuracy and robust accuracy for certain natural tasks.
Since their result applies to any classiÔ¨Åer (including inefÔ¨Åcient ones), combining their result with our
reduction implies that a similar accuracy-robustness tradeoff exists for detectors. More precisely,
Tsipras et al. (2019) show that for certain classiÔ¨Åcation tasks and suitable choices of parameters
Œ±, Œ≤, œµ, any classiÔ¨Åer g which achieves risk R(g) ‚â§Œ± must have robust risk at least Rœµ
adv(g) ‚â•Œ≤
against ‚Ñì‚àû-perturbations bounded by œµ. By our reduction, this implies that any detector f with
risk at most R(f) ‚â§Œ± must also have robust risk with detection at least Rœµ/2
adv-det(f) ‚â•Œ≤ against
‚Ñì‚àû-perturbations bounded by œµ/2."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.32989690721649484,"Similar arguments can be applied to show, for instance, that the increased data complexity of robust
generalization from Schmidt et al. (2018), or the tradeoff between robustness to multiple perturba-
tion types from Tram`er & Boneh (2019), also apply to robust detectors."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.33505154639175255,"Our reduction does not apply for ‚Äúcomputational‚Äù hardness results that have been shown for robust
classiÔ¨Åcation. For example, Garg et al. (2020) and Bubeck et al. (2018) show (‚Äúunnatural‚Äù) distri-
butions where learning a robust classiÔ¨Åer is computationally hard‚Äîunder standard cryptographic
assumptions. We cannot use Theorem 4 to conclude that learning a robust detector is hard for these
distributions, since the existence of such a detector would only imply an inefÔ¨Åcient robust classiÔ¨Åer
which does not contradict the results of Garg et al. (2020) or Bubeck et al. (2018)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3402061855670103,"2.3
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.34536082474226804,"A similar argument as in Theorem 4 can be used in the opposite direction, to show that a robust clas-
siÔ¨Åer at distance œµ/2 implies an inefÔ¨Åcient robust detector at distance œµ. Taken together, Theorem 4
and Theorem 5 show that robust detection and classiÔ¨Åcation are equivalent, up to a factor 2 in the
norm bound and up to computational constraints."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.35051546391752575,"Theorem 5 (œµ/2 robust-classiÔ¨Åcation implies inefÔ¨Åcient œµ-robust detection). Let d(¬∑, ¬∑) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rœµ/2
adv (f) = Œ≤. Then, we can construct
an explicit (but inefÔ¨Åcient) defense f that achieves risk R(f) ‚â§Œ≤ and robust risk with detection
Rœµ
adv-det(f) ‚â§Œ≤."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3556701030927835,The defense f is constructed as follows on input x:
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.36082474226804123,‚Ä¢ Run the classiÔ¨Åer y ‚Üêg(x).
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.36597938144329895,"‚Ä¢ Find a perturbed input x‚Ä≤ withing distance œµ/2 of x that is classiÔ¨Åed differently, i.e.,
d(x, x‚Ä≤) ‚â§œµ/2 and g(x‚Ä≤) Ã∏= y. If such an input x‚Ä≤ exists, reject the input and output
‚ä•. Else, output the class y."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3711340206185567,We provide the proof of Theorem 5 in Appendix A.
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.37628865979381443,"A main distinction between Theorem 4 and Theorem 5 is that the construction in Theorem 4 pre-
serves clean accuracy, but the construction in Theorem 5 does not. That is, the constructed robust
detector in Theorem 5 has clean accuracy that is equal to the robust classiÔ¨Åer‚Äôs robust accuracy."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.38144329896907214,"The construction in Theorem 5 can be efÔ¨Åciently (but approximately) instantiated by a certiÔ¨Åably
robust classiÔ¨Åer (Wong & Kolter, 2018; Raghunathan et al., 2018). These defenses can certify that
a classiÔ¨Åer‚Äôs output is constant for all points within some distance œµ of the input. For an adversarial"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3865979381443299,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3917525773195876,"example ÀÜx for g, the certiÔ¨Åcation always fails and thus the constructed detector f will reject ÀÜx. If g
is robust and the certiÔ¨Åcation succeeds, the detector f copies the output of g. However, a certiÔ¨Åed
defense may fail to certify a robust input (a false negative), and thus the detector f may reject more
inputs than with the ‚Äúoptimal‚Äù construction in Theorem 5. This reduction from a certiÔ¨Åed classiÔ¨Åer
to a detector is implicit in (Wong & Kolter, 2018, Section 3.1)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.39690721649484534,"3
WHAT ARE DETECTION DEFENSES CLAIMING?"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4020618556701031,"We now survey 14 detection defenses, and consider the robust classiÔ¨Åcation performance that these
defenses implicitly claim (via Theorem 4). As we will see, in 12/14 cases, the defenses‚Äô detection
results imply an inefÔ¨Åcient classiÔ¨Åer with far better robust accuracy than the state-of-the-art."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4072164948453608,"Before presenting our experimental setup and the explicit results from the reduction, we Ô¨Årst discuss
how we believe these results should be interpreted."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.41237113402061853,"Interpreting our reduction.
Suppose that some detector defense claims a robust accuracy that
implies‚Äîvia our reduction‚Äîan inefÔ¨Åcient classiÔ¨Åer with much higher robustness that the state-of-
the-art (e.g., the defense A described in the introduction of this paper)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4175257731958763,"A Ô¨Årst possible interpretation of our reduction is that this robust detector implies the existence of a
robust classiÔ¨Åer. This interpretation is rather weak however, since it is typically presumed that robust
classiÔ¨Åcation is possible, and that human perception is one concrete example of a robust classiÔ¨Åer.
The mere existence of a robust classiÔ¨Åer is thus typically already assumed to be true."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.422680412371134,"Our reduction yields a stronger result. It provides an explicit construction of an (inefÔ¨Åcient) robust
classiÔ¨Åer from a robust detector. The question then is whether we should expect the construction of
inefÔ¨Åcient robust classiÔ¨Åers to be easier than the construction of efÔ¨Åcient ones. That is, do we expect
that we can leverage computational inefÔ¨Åciency to build more robust classiÔ¨Åers that the current
state-of-the-art?"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.42783505154639173,"We do not know of a positive answer to this question, and there is evidence to suggest that the answer
may be negative.1 For example, the work of Schmidt et al. (2018) proves that for a synthetic classiÔ¨Å-
cation task between Gaussian distributions, building more robust classiÔ¨Åers requires additional data
regardless of the amount of computation power. Their results are corroborated by current state-of-
the-art robust classiÔ¨Åers based on adversarial training (Madry et al., 2018), which do not appear to
be limited by computational constraints. On CIFAR-10 for example, adversarial training achieves
100% robust training accuracy (Schmidt et al., 2018). Thus, it is unclear how computational inefÔ¨Å-
ciency could be leveraged to build more robust classiÔ¨Åers using existing techniques."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4329896907216495,"Candidate approaches could be to train much larger models (e.g., with an exponential number of
parameters), or to perform an exhaustive architecture search to Ô¨Ånd more robust models. Yet, note
that the robust classiÔ¨Åer constructed in our reduction only uses its unbounded computational power
at inference time. That is, the classiÔ¨Åer that is built in Theorem 4 uses a trained detector model as a
subroutine (which is presumed to be efÔ¨Åcient), and then solves a non-convex optimization problem
at inference time. The classiÔ¨Åer built in our reduction is thus presumably weaker than a robust
classiÔ¨Åer that can be trained with unbounded computational power."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4381443298969072,"To summarize, when a detector defense claims a certain robust accuracy, this implies the existence of
a concretely instantiatable robust classiÔ¨Åer with an inefÔ¨Åcient inference procedure. If this inefÔ¨Åcient
classiÔ¨Åer is much more robust than the current state-of-the-art, this does not necessarily mean that
the defense‚Äôs claims is wrong. But given how challenging robust classiÔ¨Åcation is proving to be,
we have reason to be skeptical of such a major breakthrough (even for inefÔ¨Åcient classiÔ¨Åers). To
compound this, many proposed detection defenses are quite simple, and reject adversarial inputs
based on some standard statistical test over a neural network‚Äôs features. It would thus be particularly
surprising if such simple techniques could yield robust classiÔ¨Åers, given that ‚Äúsimple‚Äù approaches to
adversarial robustness (denoising, compression, randomness, etc.) are ineffective (He et al., 2017)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.44329896907216493,"1Some works have shown that for certain ‚Äúunnatural distributions‚Äù, computational inefÔ¨Åciency is necessary
to build robust classiÔ¨Åers (Garg et al., 2020; Bubeck et al., 2018). Yet, since we presume that the human
perceptual system is robust to small perturbations on natural data (e.g., such as CIFAR-10), there must exist
some efÔ¨Åcient natural process to achieve robustness on such data."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4484536082474227,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4536082474226804,"As a result, it is not too surprising that a number of the detector defenses that we survey have already
been broken by stronger attacks (Carlini & Wagner, 2017; Tram`er et al., 2020). Our reduction would
have already suggested that such a break was likely to happen."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4587628865979381,"Experimental setup.
We choose 14 detector defenses from the literature (see Table 1). Our se-
lection of these defenses was partially motivated by a pragmatic consideration on the easiness of
translating the defenses‚Äô claims into a bound on the robust risk with detection Rœµ
adv-det. Indeed, some
defenses simply report a single AUC score for the detector‚Äôs performance, from which we cannot
derive a useful bound on the robust risk. We thus focus on defenses that either directly report a
robust error akin to DeÔ¨Ånition 3, or that provide concrete pairs of false-positive and false-negative
rates (e.g., a full ROC curve). In the latter case, we compute a ‚Äúbest-effort‚Äù bound on the robust risk
with detection2 as:
Rœµ
adv-det(f) ‚â§FPR + FNR + R(f) ,
(4)"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4639175257731959,"where FPR and FNR are the detector‚Äôs false-positive and false-negative rates for a Ô¨Åxed detection
threshold, and R(f) is the defense‚Äôs standard risk (i.e., the test error on natural examples)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4690721649484536,"The above union bound in Equation (4) is quite pessimistic, as we may over-count examples that lead
to multiple sources of errors (e.g., a natural input that is misclassiÔ¨Åed and erroneously detected). The
true robustness claim made by these detector defenses might thus be stronger than what we obtain
from our bound. We encourage future detection papers to report their adversarial risk with detection,
Rœµ
adv-det, to facilitate direct comparisons with robust classiÔ¨Åers using our reduction."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4742268041237113,"The 14 detector defenses use three datasets: MNIST, CIFAR-10 and ImageNet, and consider ad-
versarial examples under the ‚Ñì‚àûor ‚Ñì2 norms. Given a claim of robust detection at distance œµ, we
contrast it to a state-of-the-art robust classiÔ¨Åcation result for distance œµ/2:"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4793814432989691,"‚Ä¢ On MNIST with ‚Ñì‚àûattacks, we use the adversarially-trained TRADES classiÔ¨Åer (Zhang
et al., 2019) and measure robust error with the Square attack (Andriushchenko et al., 2020)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4845360824742268,"‚Ä¢ On MNIST with ‚Ñì2 attacks, we use the adversarially-trained classiÔ¨Åer from Tram`er &
Boneh (2019) and measure robust error with PGD (Madry et al., 2018)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4896907216494845,"‚Ä¢ On CIFAR-10, for both ‚Ñì‚àûand ‚Ñì2 attacks we use the adversarially-trained classiÔ¨Åer of Re-
bufÔ¨Ået al. (2021) (trained without external data), and attack it using the APGD-CE attack
from AutoAttack (Croce & Hein, 2020)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4948453608247423,"‚Ä¢ For ImageNet, for both ‚Ñì‚àûand ‚Ñì2 attacks we use adversarially-trained classiÔ¨Åers and PGD
attacks from Engstrom et al. (2019)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5,"We also consider two certiÔ¨Åed defenses for ‚Ñì‚àûattacks on CIFAR-10: the robust classiÔ¨Åer of Zhang
et al. (2020a), and a recent certiÔ¨Åed detector of Sheikholeslami et al. (2021)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5051546391752577,"Results.
As we can see from Table 1, most defenses claim a detection performance that implies a
far greater robust accuracy than our current best robust classiÔ¨Åers. To illustrate with a concrete exam-
ple, the CIFAR-10 detector of Miller et al. (2019) claims to achieve robust accuracy with detection
of 75% for ‚Ñì2 attacks with œµ = 2.9. Using Theorem 4, this implies an inefÔ¨Åcient classiÔ¨Åer with
robust accuracy of 75% for ‚Ñì2 attacks with œµ = 2.9/2 = 1.45. Yet, the current state-of-the-art robust
accuracy for such a perturbation budget is only 30% (RebufÔ¨Ået al., 2021). If this detector defense‚Äôs
robustness claim were correct, it would imply a remarkable breakthrough in robust classiÔ¨Åcation."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5103092783505154,"Why do many of these defenses claim robust accuracies that appear ‚Äútoo good to be true‚Äù? A pri-
mary reason is that the vast majority of the above detector defenses do not consider evaluations
against adaptive attacks (Carlini et al., 2019; Athalye et al., 2018; Tram`er et al., 2020). That is,
these defenses show that they can detect some Ô¨Åxed attacks, and thereafter conclude that the detec-
tor is robust against all attacks. As in the case of robust classiÔ¨Åers, such an evaluation is clearly
insufÔ¨Åcient! Some defenses do evaluate against adaptive adversaries, but fail to build a sufÔ¨Åciently
strong attack to reliably approximate the worst-case robust risk. Because of the lack of a strong
comparative baseline, it is not always immediately clear that these results are overly strong."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5154639175257731,"2Many detector defenses report performance against a set of Ô¨Åxed (non-adaptive) attacks. We interpret these
results as being an approximation of the worst-case risk."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.520618556701031,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5257731958762887,"Table 1: For each detector defense, we compute a (best-effort) bound on the claimed robust risk
with detection Rœµ
adv-det using Equation (4), and report the complement (the robust accuracy with
detection), 1 ‚àíRœµ
adv-det. For each detector‚Äôs robustness claim (at distance œµ), we report the state-
of-the-art robust classiÔ¨Åcation accuracy for attacks at distance œµ/2, denoted 1 ‚àíRœµ/2
adv . Detection
defense claims that imply a higher robust classiÔ¨Åcation accuracy than the current state-of-the-art are
highlighted in red."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5309278350515464,"Dataset
Defense
Norm
œµ
1 ‚àíRœµ
adv-det
1 ‚àíRœµ/2
adv"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5360824742268041,"MNIST
Grosse et al. (2017)
‚Ñì‚àû
0.5
‚â•98%
94%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5412371134020618,"Ma et al. (2018)
‚Ñì2
4.2
‚â•99%
72%
Raghuram et al. (2021)
‚Ñì2
8.9
‚â•74%
0%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5463917525773195,CIFAR-10
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5515463917525774,"Yin et al. (2020)
‚Ñì2
1.7
‚â•90%
66%
Feinman et al. (2017)
‚Ñì2
2.7
‚â•43%
36%
Miller et al. (2019)
‚Ñì2
2.9
‚â•75%
30%
Raghuram et al. (2021)
‚Ñì2
4.0
‚â•56%
10%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5567010309278351,"Ma & Liu (2019)
‚Ñì‚àû
4/255
‚â•96%
85%
Roth et al. (2019)
‚Ñì‚àû
8/255
‚â•66%
79%
Lee et al. (2018)
‚Ñì‚àû
20/255
‚â•81%
59%
Li et al. (2019)
‚Ñì‚àû
26/255
‚â•80%
44%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5618556701030928,ImageNet
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5670103092783505,"Xu et al. (2018)
‚Ñì2
1.0
‚â•67%
54%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5721649484536082,"Ma & Liu (2019)
‚Ñì‚àû
2/255
‚â•68%
55%
Jha et al. (2019)
‚Ñì‚àû
2/255
‚â•30%
55%
Hendrycks & Gimpel (2017)
‚Ñì‚àû
10/255
‚â•76%
30%
Yu et al. (2019)
‚Ñì‚àû
26/255
‚â•7%
5%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5773195876288659,"For example, the recent work of Raghuram et al. (2021, ICML Long Talk) builds a detector on
MNIST with a FNR of ‚â§5% at a FPR of ‚â§20%, for adaptive ‚Ñì2 attacks bounded by œµ = 8.9.
Yet, this perturbation bound is much larger than the average distance between an MNIST image
and the nearest image from a different class! Thus, an attack within this perturbation bound can
trivially reduce the detector‚Äôs accuracy to chance. On CIFAR-10, the same detector achieves 95%
clean accuracy, and a FNR of ‚â§19% at a FPR of ‚â§20% for adaptive ‚Ñì2 attacks bounded by œµ = 4.
Using Equation (4), this yields a bound on the robust accuracy with detection of 1 ‚àíRœµ
adv-det(f) ‚â•
1 ‚àí(5% + 19% + 20%) = 56%. In contrast, the best robust classiÔ¨Åer we are aware of for ‚Ñì2
attacks bounded by œµ = 2 achieves robust accuracy of only 10% (RebufÔ¨Ået al., 2021). In summary,
the adaptive attack considered in this detector defense‚Äôs evaluation is highly unlikely to be good
approximation of a worst-case attack, and this defense can likely be broken by stronger attacks."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5824742268041238,"CertiÔ¨Åably robust detection and classiÔ¨Åcation.
In Table 2, we look at the robust accuracy with
detection, and standard robust accuracy achieved by certiÔ¨Åed defenses (for which the claimed ro-
bustness numbers are necessarily mathematically correct)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5876288659793815,"We note that our reduction is not as meaningful in the case of certiÔ¨Åed defenses, since it is highly
plausible that computational inefÔ¨Åciency can be leveraged to build better certiÔ¨Åed classiÔ¨Åers. In-
deed, given any robust classiÔ¨Åer (e.g., an adversarially trained model), the classiÔ¨Åer‚Äôs robustness can
always be certiÔ¨Åed inefÔ¨Åciently (by enumerating over all points within an œµ-ball). Thus, the existence
of an inefÔ¨Åcient classiÔ¨Åer with higher certiÔ¨Åed robustness than the state-of-the-art is to be expected."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5927835051546392,"Nevertheless, we Ô¨Ånd that existing results for certiÔ¨Åed classiÔ¨Åers and detectors perfectly match what
is implied by our reduction (up to ¬±2% error). For example, Zhang et al. (2020a) follow a long line
of results on robust classiÔ¨Åers and achieve 39% robust accuracy on CIFAR-10 for perturbations of
‚Ñì‚àû-norm below 4/255. Together with Theorem 5, this implies an inefÔ¨Åcient detector with 39% robust"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5979381443298969,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6030927835051546,"Table 2: CertiÔ¨Åed robust accuracy 1 ‚àíRœµ/2
adv for the defense of Zhang et al. (2020a), and certiÔ¨Åed
robust accuracy with detection 1 ‚àíRœµ
adv-det for the defense of Sheikholeslami et al. (2021)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6082474226804123,"œµ
1 ‚àíRœµ
adv-det
1 ‚àíRœµ/2
adv"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6134020618556701,"8/255
37%
39%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6185567010309279,"16/255
32%
33%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6237113402061856,"detection accuracy for perturbations of ‚Ñì‚àû-norm below 8/255. The recent work of Sheikholeslami
et al. (2021) nearly matches that bound (37% robust accuracy with detection), with a defense that
has the advantage of being concretely efÔ¨Åcient."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6288659793814433,"These results give additional credence to our thesis: with current techniques, robust classiÔ¨Åcation
is indeed approximately twice as hard (in terms of the perturbation bounds covered) than robust
detection."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.634020618556701,"Extensions and open problems.
The main open problem raised by our work is of course whether
it could be possible to show an efÔ¨Åcient reduction between classiÔ¨Åcation and detection of adversarial
examples, but this seems implausible (at least with our minimum distance decoding approach)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6391752577319587,"Another interesting question is whether a similar reduction can be shown for robustness to less
‚Äústructured‚Äù perturbations than ‚Ñìp balls and other metric spaces. For example, there has been a line
of research on defending against adversarial patches (Brown et al., 2017), using empirical (Hayes,
2018; Naseer et al., 2019; Chou et al., 2020) and certiÔ¨Åable techniques (Chiang et al., 2020; Zhang
et al., 2020b; Xiang et al., 2021). To use our result, we would have to deÔ¨Åne some metric to measure
the size of an adversarial patch‚Äôs perturbation. Yet, the size of a patch is typically deÔ¨Åned by the
number of contiguously perturbed pixels, which does not deÔ¨Åne a metric (in particular, it does not
satisfy the triangular inequality which our reduction relies on)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6443298969072165,"Finally, similar hardness reductions might exist between other candidate approaches for building
robust classiÔ¨Åers. For example, the question of whether (test-time) randomness can be leveraged to
build more robust models is also intriguing .Empirical defenses that use randomness can be noto-
riously hard to evaluate (Athalye et al., 2018; Tram`er et al., 2020), so a reduction similar to ours
might be useful in showing that we should not expect such approaches to bare fruit."
CONCLUSION,0.6494845360824743,"4
CONCLUSION"
CONCLUSION,0.654639175257732,"We have shown formal reductions between robust classiÔ¨Åcation with, and without, a detection op-
tion. Our results show that signiÔ¨Åcant progress on one of these two tasks implies similar progress
on the other‚Äîunless computational inefÔ¨Åciency can somehow be leveraged to build more robust
models. This raises the question on whether we should spend our efforts on studying both of these
tasks, or focus our efforts on a single one."
CONCLUSION,0.6597938144329897,"On one hand, the two tasks represent different ways of tackling a common goal, and working on
either task might result in new techniques or ideas that apply to the other task as well. On the
other hand, our reductions show that unless we make progress on both tasks, work on one of the
tasks can merely aim to match the robustness of our inefÔ¨Åcient constructions, whilst improving their
computational complexity."
CONCLUSION,0.6649484536082474,"We believe our reduction will serve as a useful sanity-check when assessing the claims of future de-
tector defenses. Detector defenses‚Äô robustness evaluations have received less stringent scrutiny than
robust classiÔ¨Åers over the past years, perhaps in part due to a lack of strong comparative baselines.
Instead of having to wait until some detector defense‚Äôs claims pass the test-of-time, we show that
detection results can be directly contrasted against long-standing results for robust classiÔ¨Åcation."
CONCLUSION,0.6701030927835051,"When applying this approach to past detector defenses, we Ô¨Ånd that many make robustness claims
that imply signiÔ¨Åcant breakthroughs in robust classiÔ¨Åcation. We believe our reduction could have
been useful in highlighting the suspiciously strong claims made by many of these defenses‚Äîbefore
they were explicitly broken by stronger attacks."
CONCLUSION,0.6752577319587629,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.6804123711340206,"Ethics statement.
Our paper demonstrates a fundamental barrier towards detecting against adver-
sarial examples, under the assumption that our current techniques are insufÔ¨Åcient to achieve strong
(inefÔ¨Åcient) robust classiÔ¨Åcation. We do not however explicitly break any existing defenses (our
results merely strongly suggest that many existing detector defenses‚Äô claims are suspiciously high).
Our paper therefore cannot lead to any explicit harms, but aims to further our understanding of the
hardness of robust classiÔ¨Åcation and detection."
REPRODUCIBILITY STATEMENT,0.6855670103092784,"Reproducibility statement.
Our paper‚Äôs contribution is mainly of theoretical nature. Section 2
is self-contained and clearly states our assumptions, results and proofs (except for the proof of
Theorem 5 in Appendix A). The experiments in Section 3 use only public datasets and pre-trained
models, with clearly indicates hyper-parameters for all attacks that we evaluate."
REFERENCES,0.6907216494845361,REFERENCES
REFERENCES,0.6958762886597938,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efÔ¨Åcient black-box adversarial attack via random search. In European Conference
on Computer Vision, 2020."
REFERENCES,0.7010309278350515,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018."
REFERENCES,0.7061855670103093,"Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ÀáSrndi¬¥c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Euro-
pean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387‚Äì402.
Springer, 2013."
REFERENCES,0.711340206185567,"Tom B Brown, Dandelion Man¬¥e, Aurko Roy, Mart¬¥ƒ±n Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017."
REFERENCES,0.7164948453608248,"S¬¥ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from cryp-
tographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018."
REFERENCES,0.7216494845360825,"Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In AISec, pp. 3‚Äì14. ACM, 2017."
REFERENCES,0.7268041237113402,"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019."
REFERENCES,0.7319587628865979,"Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Goldstein.
CertiÔ¨Åed defenses for adversarial patches. arXiv preprint arXiv:2003.06693, 2020."
REFERENCES,0.7371134020618557,"Edward Chou, Florian Tram`er, and Giancarlo Pellegrino.
Sentinet: Detecting physical attacks
against deep learning systems. In Workshop on Deep Learning Security, 2020."
REFERENCES,0.7422680412371134,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020."
REFERENCES,0.7474226804123711,"Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness bench-
mark. arXiv preprint arXiv:2010.09670, 2020."
REFERENCES,0.7525773195876289,"Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness."
REFERENCES,0.7577319587628866,"Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017."
REFERENCES,0.7628865979381443,"Sanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad. Adversarially robust
learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364‚Äì385.
PMLR, 2020."
REFERENCES,0.7680412371134021,Under review as a conference paper at ICLR 2022
REFERENCES,0.7731958762886598,"Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017."
REFERENCES,0.7783505154639175,"Jamie Hayes. On visible adversarial perturbations & digital watermarking. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1597‚Äì1604, 2018."
REFERENCES,0.7835051546391752,"Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive Tech-
nologies, 2017."
REFERENCES,0.788659793814433,"Dan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial images. In International
Conference on Learning Representations, 2017."
REFERENCES,0.7938144329896907,"Susmit Jha, Sunny Raj, Steven Lawrence Fernandes, Sumit Kumar Jha, Somesh Jha, Gunjan Verma,
Brian Jalaian, and Ananthram Swami. Attribution-driven causal analysis for detection of adver-
sarial examples. arXiv preprint arXiv:1903.05821, 2019."
REFERENCES,0.7989690721649485,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniÔ¨Åed framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-
ing Systems, 2018."
REFERENCES,0.8041237113402062,"Yingzhen Li, John Bradshaw, and Yash Sharma. Are generative classiÔ¨Åers more robust to adversarial
attacks? In International Conference on Machine Learning, pp. 3804‚Äì3814. PMLR, 2019."
REFERENCES,0.8092783505154639,"Shiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant
checking. In Proceedings of the 26th Network and Distributed System Security Symposium (NDSS
2019), 2019."
REFERENCES,0.8144329896907216,"Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In International Conference on Learning Representations, 2018."
REFERENCES,0.8195876288659794,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.8247422680412371,"David Miller, Yujia Wang, and George Kesidis. When not to classify: Anomaly detection of attacks
(ada) on dnn classiÔ¨Åers at test time. Neural computation, 31(8):1624‚Äì1670, 2019."
REFERENCES,0.8298969072164949,"Muzammal Naseer, Salman Khan, and Fatih Porikli. Local gradients smoothing: Defense against
localized adversarial attacks. In 2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pp. 1300‚Äì1307. IEEE, 2019."
REFERENCES,0.8350515463917526,"Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, and Tie-Yan
Liu. Adversarial training with rectiÔ¨Åed rejection. arXiv preprint arXiv:2105.14785, 2021."
REFERENCES,0.8402061855670103,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. CertiÔ¨Åed defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018."
REFERENCES,0.845360824742268,"Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, and Suman Banerjee. A general frame-
work for detecting anomalous inputs to dnn classiÔ¨Åers. In International Conference on Machine
Learning, 2021."
REFERENCES,0.8505154639175257,"Sylvestre-Alvise RebufÔ¨Å, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
arXiv preprint
arXiv:2103.01946, 2021."
REFERENCES,0.8556701030927835,"Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting
adversarial examples. In International Conference on Machine Learning, 2019."
REFERENCES,0.8608247422680413,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances In Neural Information Processing
Systems, pp. 5019‚Äì5031, 2018."
REFERENCES,0.865979381443299,Under review as a conference paper at ICLR 2022
REFERENCES,0.8711340206185567,"Fatemeh Sheikholeslami, Ali LotÔ¨Å, and J Zico Kolter. Provably robust classiÔ¨Åcation of adversarial
examples with detection. In International Conference on Learning Representations, 2021."
REFERENCES,0.8762886597938144,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014."
REFERENCES,0.8814432989690721,"Florian Tram`er and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances In Neural Information Processing Systems, 2019."
REFERENCES,0.8865979381443299,"Florian Tram`er, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Conference on Neural Information Processing Systems, 2020."
REFERENCES,0.8917525773195877,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019."
REFERENCES,0.8969072164948454,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5283‚Äì5292, 2018."
REFERENCES,0.9020618556701031,"Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. Patchguard: A provably ro-
bust defense against adversarial patches via small receptive Ô¨Åelds and masking. In 30th {USENIX}
Security Symposium ({USENIX} Security 21), 2021."
REFERENCES,0.9072164948453608,"Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In Network and Distributed System Security Symposium, 2018."
REFERENCES,0.9123711340206185,"Xuwang Yin, Soheil Kolouri, and Gustavo K Rohde. Gat: Generative adversarial training for ad-
versarial example detection and robust classiÔ¨Åcation. In International Conference on Learning
Representations, 2020."
REFERENCES,0.9175257731958762,"Tao Yu, Shengyuan Hu, Chuan Guo, Wei-Lun Chao, and Kilian Q Weinberger. A new defense
against adversarial images: Turning a weakness into a strength. In Advances in Neural Informa-
tion Processing Systems, 2019."
REFERENCES,0.9226804123711341,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019."
REFERENCES,0.9278350515463918,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards
stable and efÔ¨Åcient training of veriÔ¨Åably robust neural networks. In International Conference on
Learning Representations, 2020a."
REFERENCES,0.9329896907216495,"Zhanyuan Zhang, Benson Yuan, Michael McCoyd, and David Wagner. Clipped bagnet: Defending
against sticker attacks with clipped bag-of-features. In 2020 IEEE Security and Privacy Work-
shops (SPW), pp. 55‚Äì61. IEEE, 2020b."
REFERENCES,0.9381443298969072,Under review as a conference paper at ICLR 2022
REFERENCES,0.9432989690721649,"A
PROOF OF THEOREM 5."
REFERENCES,0.9484536082474226,We recall Theorem 5:
REFERENCES,0.9536082474226805,"Theorem 5 (œµ/2 robust-classiÔ¨Åcation implies inefÔ¨Åcient œµ-robust detection). Let d(¬∑, ¬∑) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rœµ/2
adv (f) = Œ≤. Then, we can construct
an explicit (but inefÔ¨Åcient) defense f that achieves risk R(f) ‚â§Œ≤ and robust risk with detection
Rœµ
adv-det(f) ‚â§Œ≤."
REFERENCES,0.9587628865979382,The defense f is constructed as follows on input x:
REFERENCES,0.9639175257731959,‚Ä¢ Run the classiÔ¨Åer y ‚Üêg(x).
REFERENCES,0.9690721649484536,"‚Ä¢ Find a perturbed input x‚Ä≤ withing distance œµ/2 of x that is classiÔ¨Åed differently, i.e.,
d(x, x‚Ä≤) ‚â§œµ/2 and g(x‚Ä≤) Ã∏= y. If such an input x‚Ä≤ exists, reject the input and output
‚ä•. Else, output the class y."
REFERENCES,0.9742268041237113,"Proof of Theorem 5. Note that for any input (x, y) for which the classiÔ¨Åer g is robust at distance
œµ/2, no input x‚Ä≤ above exists and so f(x) = y. Thus, the risk of f is at most the robust risk of g, so
R(f) ‚â§Œ≤."
REFERENCES,0.979381443298969,"Now, consider an input (x, y) ‚àºD for which f is not robust with detection at distance œµ. That is,
either f(x) Ã∏= y, or there exists an input ÀÜx at distance d(x, ÀÜx) ‚â§œµ such that f(ÀÜx) = ÀÜy /‚àà{y, ‚ä•}. We
will show that the defense g is not robust for x either (for attacks at distance up to œµ/2.)"
REFERENCES,0.9845360824742269,"If f(x) Ã∏= y, then by the same argument as above it cannot be the case that g is robust at distance
œµ/2 for x."
REFERENCES,0.9896907216494846,"So let us consider the case where f(ÀÜx) = ÀÜy /‚àà{y, ‚ä•}. By the deÔ¨Ånition of f, this means that for all
x‚Ä≤ at distance at most œµ/2 from ÀÜx, we have g(x‚Ä≤) = ÀÜy. But, note that there exists a point x‚àóthat is at
distance at most œµ/2 from both ÀÜx and x. Since we must have g(x‚àó) = ÀÜy, we conclude that g is not
robust at distance œµ/2 for x."
REFERENCES,0.9948453608247423,Taking expectations over the distribution D concludes the proof.
