Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003424657534246575,"We study multi-player general-sum Markov games with one of the players des-
ignated as the leader and the rest regarded as the followers. In particular, we
focus on the class of games where the state transitions are only determined by
the leader’s action while the actions of all the players determine their immediate
rewards. For such a game, our goal is to ﬁnd the Stackelberg-Nash equilibrium
(SNE), which is a policy pair (π∗, ν∗) such that (i) π∗is the optimal policy for the
leader when the followers always play their best response, and (ii) ν∗is the best
response policy of the followers, which is a Nash equilibrium of the followers’
game induced by π∗. We develop sample efﬁcient reinforcement learning (RL)
algorithms for solving SNE for both the online and ofﬂine settings. Respectively,
our algorithms are optimistic and pessimistic variants of least-squares value itera-
tion and are readily able to incorporate function approximation for handling large
state spaces. Furthermore, for the case with linear function approximation, we
prove that our algorithms achieve sublinear regret and suboptimality under online
and ofﬂine setups respectively. To our best knowledge, we establish the ﬁrst prov-
ably efﬁcient RL algorithms for solving SNE in general-sum Markov games with
leader-controlled state transitions."
INTRODUCTION,0.00684931506849315,"1
INTRODUCTION"
INTRODUCTION,0.010273972602739725,"Reinforcement learning (RL) has achieved striking empirical successes in solving complicated real-
world sequential decision-making problems (Mnih et al., 2015; Duan et al., 2016; Silver et al., 2016;
2017; 2018; Agostinelli et al., 2019; Akkaya et al., 2019). Motivated by these successes, multi-agent
extensions of RL algorithms recently have gained great popularity in decision-making problems in-
volving multiple interacting agents (Busoniu et al., 2008; Hernandez-Leal et al., 2018; 2019; Oroo-
jlooyJadid & Hajinezhad, 2019; Zhang et al., 2019). Multi-agent RL is often modeled as a Markov
game (Littman, 1994) where, at each time step, each player (agent) takes an action simultaneously
at each state of the environment, observe her own immediate reward, and the environment evolves
into a next state. Here both the reward of each player and the state transition depends on the actions
of all players. From the perspective of each player, her goal is to ﬁnd a policy that maximizes her
expected total reward in the presence of other agents."
INTRODUCTION,0.0136986301369863,"In Markov games, depending on the structure of the reward functions, the relationship among the
players can be either collaborative, where each player has the same reward function, or competitive,
where the sum of the reward function is equal to zero, or mixed, which corresponds to a general-sum
game. While most of existing theoretical results focus on the collaborative or two-player competitive
settings, the mixed setting is oftentimes more pertinent to real-world multi-agent applications."
INTRODUCTION,0.017123287671232876,"Moreover, in addition to having diverse reward functions, the players might also have asymmetric
roles in the Markov game — the players might be divided into leaders and followers, where the
leaders’ joint policy determines a general-sum game for the followers. Games with such a leader-
follower structure is popular in applications such as mechanism design (Conitzer & Sandholm, 2002;
Roughgarden, 2004; Garg & Narahari, 2005; Kang & Wu, 2014), security games (Tambe, 2011;
Korzhyk et al., 2011; Balcan et al., 2015), incentive design (Zheng et al., 1984; Ratliff et al., 2014;
Chen et al., 2016; Ratliff & Fiez, 2020), and model-based RL (Rajeswaran et al., 2020). Consider
a simpliﬁed economic system that consists of a government and a group of companies, where the"
INTRODUCTION,0.02054794520547945,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.023972602739726026,"companies purchase or sell goods, and the government collects taxes from transactions. Such a
problem can be viewed as a multi-player general-sum game, where the government serves as the
leader and the companies are followers (Zheng et al., 2020). In particular, when the government sets
a tax rate, the companies form a general-sum game themselves, whose reward functions depend on
the tax rate. Each company aims to maximize their own revenue, and thus ideally they achieve a
Nash equilibrium (NE) of the induced game. Whereas the goal of the government might be achieving
the social welfare, which might be measured via certain fairness metrics computed by the revenues
of the companies."
INTRODUCTION,0.0273972602739726,"In multi-player Markov games with such a leader-follower structure, the desired solution concept
is the Stackelberg-Nash equilibrium (SNE) (Bas¸ar & Olsder, 1998). In the setting where there is
a single leader, SNE corresponds to a pair of leader’s policy π∗and followers’ joint policy ν∗that
satisﬁes the following two properties: (i) when the leader adopts π∗, ν∗is the best-response policy
of the followers, i.e., ν∗is a Nash equilibrium of the followers’ subgame induced by π∗; and (ii) π∗
is the optimal policy of the leader assuming the followers always adopt the best response."
INTRODUCTION,0.030821917808219176,"We are interested in ﬁnding an SNE in a multi-player Markov game when the reward functions and
Markov transition kernel are unknown. In particular, we focus on the setting with a single leader
and the state transitions only depend on the leader’s actions. That is, the followers’ actions only
affect the rewards received by the leader and followers. For such a game, we are interested in the
following question:"
INTRODUCTION,0.03424657534246575,"Can we develop reinforcement learning methods that provably ﬁnd Stackelberg-Nash equilibria in
leader-controlled general-sum games with sample efﬁciency?
To this end, we consider both online and ofﬂine RL settings, where in the former, we learn the
SNE in a trial-an-error fashion by interacting with the environment and generating data, and in the
latter, we learn the SNE from a given dataset that is collected a priori. For the online setting, as
the transition model is unknown, to achieve sample efﬁciency, the equilibrium-ﬁnding algorithm
also needs to take the exploration-exploitation tradeoff into consideration. Although the similar
challenge has been studied in zero-sum Markov game, it seems unclear how to incorporate popular
exploration mechanisms such as optimism in the face of uncertainty (Sutton & Barto, 2018) into
SNE ﬁnding. Meanwhile, under the ofﬂine setting, as the RL agent has no control of data collection,
it is ideal to design an RL algorithm with theoretical guarantees for an arbitrary dataset that might
not be sufﬁciently explorative."
INTRODUCTION,0.03767123287671233,"Our contributions
Our contributions are three-fold. First, for the episodic leader-controlled
general-sum game, under the online and ofﬂine settings respectively, we propose optimistic and
pessimistic variants of the least-squares value iteration (LSVI) algorithm. In particular, in a version
of LSVI, we estimate the optimal action-value function of the leader via least-squares regression and
construct an estimate of the SNE by solving the SNE of the multi-matrix game for each state, whose
payoff matrices are given by the leader’s estimated action-value function and the followers’ reward
functions. Moreover, we add a UCB exploration bonus to the least-squares solution to achieve op-
timism in the online setting. Whereas in the ofﬂine setting, pessimism is achieved by subtracting a
penalty function constructed using the ofﬂine data, which is equal to the negative bonus function.
Moreover, these algorithms are readily able to incorporate function approximators and we showcase
the version with linear function approximation. Second, under the online setting, we prove that our
optimistic LSVI algorithm achieves a sublinear e
O(H2√"
INTRODUCTION,0.0410958904109589,"d3K) regret, where K is the number of
episodes, H is the horizon, d is the dimension of the feature mapping, and e
O(·) omits logarithmic
terms. Finally, under the ofﬂine setting, we establish an upper bound on the suboptimality of the
proposed algorithm for an arbitrary dataset with K trajectories. Our upper bound yields a sublinear
e
O(H2p"
INTRODUCTION,0.04452054794520548,"d3/K) rate as long as the dataset has sufﬁcient coverage over the trajectory induced by the
desired SNE."
INTRODUCTION,0.04794520547945205,"Related work
In the sequel, we discuss the related works on learning Stackelberg games. We
defer more related works on RL for solving NE in Markov games and single-agent RL to §A."
INTRODUCTION,0.05136986301369863,"Learning Stackelberg games
As for solving Stackelberg-Nash equilibrium, most of the existing
results focus on the normal form game, which is equivalent to our Markov game with H = 1.
Letchford et al. (2009); Blum et al. (2014); Peng et al. (2019) study learning Stackelberg equilibrium
with a best response oracle. In addition, Fiez et al. (2019) study the local convergence of ﬁrst-
order methods for ﬁnding Stackelberg equilibria in general-sum games with differentiable reward
functions, and Ghadimi & Wang (2018); Chen et al. (2021a); Hong et al. (2020) analyze the global
convergence of ﬁrst-order methods for achieving global optimality of bilevel optimization. A more"
INTRODUCTION,0.0547945205479452,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05821917808219178,"related work is Bai et al. (2021), which studies the matrix Stackelberg game with bandit feedback.
This work also studies an RL extension where the leader has a ﬁnite action set and the follower is
faced with an MDP speciﬁed by the leader’s action. In comparison, we assume the leader knows
the reward functions and the main challenge lies in the unknown and leader-controlled transitions.
Thus, our setting is different from that in Bai et al. (2021). Furthermore, a more relevant work
is (Bucarey et al., 2019b), which establishes the Bellman equation and value iteration algorithm
for solving SNE in leader-controlled Markov games. In comparison, we establish modiﬁcations of
least-squares value iteration that are tailored to online and ofﬂine settings."
INTRODUCTION,0.06164383561643835,"Notation
See §B for details."
PRELIMINARIES,0.06506849315068493,"2
PRELIMINARIES"
PRELIMINARIES,0.0684931506849315,"In this section, we introduce the formulation of the general-sum simultaneous-move Markov games,
Stackelberg-Nash equilibrium, and the linear structure we use in this paper."
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.07191780821917808,"2.1
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.07534246575342465,"In this setting, two levels of hierarchy in decision making are considered: one leader l and N
followers {fi}i∈[N].
Speciﬁcally, we deﬁne an episodic version of general-sum simultaneous-
moves Markov game by the tuple (S, Al, Af = {Afi}i∈[N], H, rl, rf = {rfi}i∈[N], P), where
S is the state space, Al and Af are the sets of actions of the leader and the followers respec-
tively, H is the number of steps in each episode, rl = {rl,h : S × Al × Af →[−1, 1]}H
h=1 and
rfi = {rfi,h : S × Al × Af →[−1, 1]}H
h=1 are reward functions of the leader and the followers
respectively, and P = {Ph : S × Al × Af × S →[0, 1]}H
h=1 is a collection of transition kernels.
Here Al × Af = Al × Af1 × · · · × AfN . Throughout this paper, we also let ⋆be some element in
{l, f1, · · · , fN}. Moreover, for any (h, x, a) ∈[H] × S × Al and b = {bi ∈Afi}i∈[N], we use the
shorthands r⋆,h(x, a, b) = r⋆,h(x, a, b1, · · · , bN) and Ph(· | x, a, b) = Ph(· | x, a, b1, · · · , bN)."
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.07876712328767123,"Policy and Value Function. A stochastic policy π = {πh : S →∆(Al)}H
h=1 of the leader is a
set of probability distributions over actions given the state. Meanwhile, a stochastic joint policy of
the followers is deﬁned by ν = {νfi}i∈[N], where νfi = {νfi,h : S →∆(Afi)}H
h=1. We use the
notation πh(a | x) and νfi,h(bi | x) to denote the probability of taking action a ∈Al or bi ∈Afi for
state x at step h under policy π, νfi respectively. Throughout this paper, for any ν = {νfi}i∈[N] and
b = {bi}i∈[N], we use the shorthand νh(b | x) = νf1,h(b1 | x) × · · · × νfN,h(bN | x)."
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.0821917808219178,"Given policies (π, ν = {νfi}i∈[N]), the action-value (Q) and state-value (V) functions for the leader
and followers are deﬁned by"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.08561643835616438,"Qπ,ν
⋆,h(x, a, b) = Eπ,ν,h,x,a,b 
H
X"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.08904109589041095,"t=h
r⋆,h(xt, at, bt)

,
V π,ν
⋆,h (x) = Ea∼πh(· | x),b∼νh(· | x)Qπ,ν
⋆,h(x, a, b), (2.1)"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.09246575342465753,"where the expectation Eπ,ν,h,x,a,b is taken over state-action pairs induced by the policies (π, ν =
{νfi}i∈[N]) and the transition probability, when initializing the process with the triplet (s, a, b =
{bi}i∈[N]) at step h. For notational simplicity, when h, x, a, b are clear from the context, we omit
h, x, a, b from Eπ,ν,h,x,a,b. By the deﬁnition in (2.1), we have the Bellman equation"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.0958904109589041,"V π,ν
⋆,h = ⟨Qπ,ν
⋆,h, πh × νh⟩Al×Af ,
Qπ,ν
⋆,h = r⋆,h + PhV π,ν
⋆,h+1,
∀⋆∈{l, f1, · · · , fN},
(2.2)"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.09931506849315068,"where πh × νh represents πh × νf1,h × · · · × νfN,h. Here Ph is the operator which is deﬁned by"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.10273972602739725,"(Phf)(x, a, b) = E[f(x′) | x′ ∼Ph(x′ | x, a, b)]
(2.3)"
GENERAL-SUM SIMULTANEOUS-MOVE MARKOV GAMES,0.10616438356164383,"for any function f : S →R and (x, a, b) ∈S × Al × Af."
STACKELBERG-NASH EQUILIBRIUM,0.1095890410958904,"2.2
STACKELBERG-NASH EQUILIBRIUM"
STACKELBERG-NASH EQUILIBRIUM,0.11301369863013698,"Given a leader policy π, a Nash equilibrium (Nash, 2016) of the followers is a joint policy ν∗=
{ν∗
fi}i∈[N], such that for any x ∈S and (i, h) ∈[N] × [H]"
STACKELBERG-NASH EQUILIBRIUM,0.11643835616438356,"V π,ν∗"
STACKELBERG-NASH EQUILIBRIUM,0.11986301369863013,"fi,h (x) ≥V
π,νfi,ν∗
f−i
fi,h
(x),
∀νfi.
(2.4)"
STACKELBERG-NASH EQUILIBRIUM,0.1232876712328767,Under review as a conference paper at ICLR 2022
STACKELBERG-NASH EQUILIBRIUM,0.1267123287671233,"Here −i represents all indices in [N] except i. For each leader policy π, we denote the set of best-
response policies of the followers by BR(π), which is deﬁned by"
STACKELBERG-NASH EQUILIBRIUM,0.13013698630136986,"BR(π) = {ν = {νfi}i∈[N] | ν is the NE of the followers given the leader policy π}.
(2.5)"
STACKELBERG-NASH EQUILIBRIUM,0.13356164383561644,"Given the best-response set BR(π), we denote ν∗(π) the worst-case responses, which break ties
against favor of the leader 1. Speciﬁcally, we deﬁne ν∗(π) by"
STACKELBERG-NASH EQUILIBRIUM,0.136986301369863,"ν∗(π) = {ν ∈BR(π) | V π,ν
l,h (x) ≤V π,ν′"
STACKELBERG-NASH EQUILIBRIUM,0.1404109589041096,"l,h (x), ∀x ∈S, h ∈[H], ν′ ∈BR(π)}.
(2.6)"
STACKELBERG-NASH EQUILIBRIUM,0.14383561643835616,"The Stackelberg-Nash equilibrium for the leader is the “best response to the best response”, that is,"
STACKELBERG-NASH EQUILIBRIUM,0.14726027397260275,"SNEl = {π | V π,ν∗(π)
l,h
(x) ≥V π′,ν∗(π′)
l,h
(x), ∀x ∈S, h ∈[H], π′}
(2.7)"
STACKELBERG-NASH EQUILIBRIUM,0.1506849315068493,"A Stackelberg-Nash equilibrium of the general-sum game is a policy pair (π∗, ν∗= {ν∗
fi}i∈[N])
such that ν∗∈ν∗(π∗) and π∗∈SNEl."
STACKELBERG-NASH EQUILIBRIUM,0.1541095890410959,"Our goal is to ﬁnd the Stackelberg equilibrium: the leader’s optimal strategy, assuming the fol-
lowers play their best response (Nash equilibrium) to the leader. We study this challenging bilevel
optimization problem in both the online setting (Section 3) and the ofﬂine setting (Section 4)."
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.15753424657534246,"2.3
LEADER-CONTROLLER LINEAR MARKOV GAMES"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.16095890410958905,"Inspired by the linear MDP studied in Jin et al. (2020b) for the single-agent RL, we study the
linear Markov games (Xie et al., 2020), where the transition dynamics are linear in a feature map.
Speciﬁcally, there exists a feature map φ′ : S × Al × Af →Rd such that"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.1643835616438356,"Ph(· | x, a, b) = ⟨φ′(x, a, b), µh(·)⟩"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.1678082191780822,"for any (x, a, b) ∈S × Al × Af and h ∈[H]. Here µh = (µ(1)
h , µ(2)
h , · · · , µ(d)
h ) are d unknown
signed measures over S. Moreover, throughout this paper, we focus on the leader-controller game
(Filar & Vrieze, 2012; Bucarey et al., 2019a), where the future state only depends on the current
state and the leader’s action, that is,"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.17123287671232876,"Ph(· | x, a, b) = Ph(· | x, a)"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.17465753424657535,"for any (x, a, b) ∈S ×Al ×Af and h ∈[H]. Hence, it is naturally to deﬁne leader-controller linear
Markov games as follows."
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.1780821917808219,"Assumption 2.1. Markov game (S, Al, Af = {Afi}i∈[N], H, rl, rf = {rfi}i∈[N], P) is a leader-
controller linear Markov game if there exists a feature map φ : S × Al →Rd such that"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.1815068493150685,"Ph(· | x, a, b) = ⟨φ(x, a), µh(·)⟩"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.18493150684931506,"for any (x, a, b) ∈S × Al × Af and h ∈[H]. Here µh = (µ(1)
h , µ(2)
h , · · · , µ(d)
h ) are d unknown
signed measures over S. Without loss of generality, we assume that ∥µh(S)∥≤
√"
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.18835616438356165,d for all h ∈[H].
LEADER-CONTROLLER LINEAR MARKOV GAMES,0.1917808219178082,"The linear Markov game above is an extension of linear MDP studied in Jin et al. (2020b) for the
single-agent RL. Speciﬁcally, when the followers play ﬁxed and known policies, the linear Markov
games reduce to the linear MDP."
MAIN RESULTS FOR THE ONLINE SETTING,0.1952054794520548,"3
MAIN RESULTS FOR THE ONLINE SETTING"
MAIN RESULTS FOR THE ONLINE SETTING,0.19863013698630136,"In this section, we study the online setting, where a central controller controls one leader l and
N followers {fi}i∈[N]. Our goal is to learn a Stackelberg-Nash equilibrium. In what follows, we
formally describe the setup and learning objectives, and then present our algorithm and provide
theoretic guarantees."
MAIN RESULTS FOR THE ONLINE SETTING,0.20205479452054795,"1This is also known as pessimistic tie breaking (Conitzer & Sandholm, 2006). We also remark that our
subsequent analysis still holds for the optimistic setting (Breton et al., 1988; Bucarey et al., 2019a)."
MAIN RESULTS FOR THE ONLINE SETTING,0.2054794520547945,Under review as a conference paper at ICLR 2022
SETUP AND LEARNING OBJECTIVE,0.2089041095890411,"3.1
SETUP AND LEARNING OBJECTIVE"
SETUP AND LEARNING OBJECTIVE,0.21232876712328766,"We consider the setting where the reward functions rl and rf = {rfi}i∈[N] are revealed to the learner
before the game. This is reasonable since in practice the reward functions are usually artiﬁcially
designed. Moreover, we focus on the episodic setting. Speciﬁcally, a Markov game is played for K
episodes, each of which consists of H timesteps. At the beginning of the k-th episode, the leader and
followers determine their policies (πk, νk = {νk
fi}i∈[N]), and a ﬁxed initial state xk
1 = x1 is chosen.
Here we assume the ﬁxed initial state just for ease of presentation, and our subsequent results can
be generalized to the setting where xk
1 is picked from a ﬁxed distribution. Then the game proceeds
as follows. At each step h ∈[H], the leader and the followers observe state xk
h ∈S and pick their
own actions ak
h ∼πk
h(· | xk
h) and bk
h = {bk
i,h ∼νk
fi,h(· | xk
h)}i∈[N]. Subsequently, the environment
transitions to the next state xk
h+1 ∼Ph(· | xk
h, ak
h, bk
h). Each episode terminates after H timesteps."
SETUP AND LEARNING OBJECTIVE,0.21575342465753425,"Learning Objective. By the deﬁnition in (2.5), given a leader’s policy, the best response for the
followers is the Nash equilibrium of followers’ game induced by this leader’s policy. Recall the
deﬁnition of Nash equilibrium in (2.4), for any policies (π, ν = {νfi}i∈[N]), it is natural to deﬁne
the following objective to measure the suboptimality of νfi:"
SETUP AND LEARNING OBJECTIVE,0.2191780821917808,"SubOptfi(x) = V π,ν∗(π)
fi,1
(x) −V
π,νfi,ν∗
−i(π)
fi,1
(x).
Meanwhile, we evaluate the performance of the leader’s policy π by the following suboptimality
gap:"
SETUP AND LEARNING OBJECTIVE,0.2226027397260274,"SubOptl(x) = V π∗,ν∗"
SETUP AND LEARNING OBJECTIVE,0.22602739726027396,"l,1
(x) −V π,ν∗(π)
l,1
(x).
Putting these two suboptimality gaps together, we formally deﬁne the regret as follows.
Deﬁnition 3.1 (Regret). Let (πk, νk = {νk
fi}i∈[N]) denote the policies executed by the algorithm
in the k-th episode. After a total of K episodes, the regret is deﬁned as"
SETUP AND LEARNING OBJECTIVE,0.22945205479452055,"Regret(K) = K
X"
SETUP AND LEARNING OBJECTIVE,0.2328767123287671,"k=1
V π∗,ν∗"
SETUP AND LEARNING OBJECTIVE,0.2363013698630137,"l,1
(xk
1) −V πk,ν∗(πk)
l,1
(xk
1)"
SETUP AND LEARNING OBJECTIVE,0.23972602739726026,"|
{z
}
Regretl(K) + N
X i=1 K
X"
SETUP AND LEARNING OBJECTIVE,0.24315068493150685,"k=1
V πk,ν∗(πk)
fi,1
(xk
1) −V
πk,νk
fi,ν∗
f−i(πk)"
SETUP AND LEARNING OBJECTIVE,0.2465753424657534,"fi,1
(xk
1)"
SETUP AND LEARNING OBJECTIVE,0.25,"|
{z
}
Regretf (K) . (3.1)"
SETUP AND LEARNING OBJECTIVE,0.2534246575342466,"The goal is to design algorithms with regret that is sublinear in K, and polynomial in d, H. Here K
is the number of episodes, d is the dimension of the feature map φ, and H is the episode horizon."
ALGORITHM,0.2568493150684932,"3.2
ALGORITHM"
ALGORITHM,0.2602739726027397,"We now present our algorithm, Optimistic Value Iteration to Find Stackelberg-Nash Equilibrium
(OVI-SNE), which is given in Algorithm 1."
ALGORITHM,0.2636986301369863,"At a high level, in each episode, our algorithm ﬁrst construct the policies for all players through
backward induction with respect to the timestep h (line 4-11), and then execute the policies to play
the game (line 12-16)."
ALGORITHM,0.2671232876712329,"In detail, at h-th step of k-th episode, OVI-SNE estimates leader’s Q-function based on the (k −1)
historical trajectories. Inspired by previous optimistic least square value iteration (LSVI) algorithms
(Jin et al., 2020b), for any h ∈[H], we estimate the linear coefﬁcients by solving the following ridge
regression problem:"
ALGORITHM,0.2705479452054795,"wk
h ←argmin
w∈Rd k−1
X"
ALGORITHM,0.273972602739726,"τ=1
[V k
h+1(xτ
h+1) −φ(xτ
h, aτ
h)⊤w]2 + ∥w∥2,"
ALGORITHM,0.2773972602739726,"where V k
h+1(·) = ⟨Qk
h+1(·, ·, ·), πk
h+1(· | ·) × νk
h+1(· | ·)⟩Al×Af . (3.2)"
ALGORITHM,0.2808219178082192,"By solving the ridge regression problem in (3.2), we have"
ALGORITHM,0.2842465753424658,"wk
h = (Λk
h)−1k−1
X"
ALGORITHM,0.2876712328767123,"τ=1
φ(xτ
h, aτ
h) · V k
h+1(xτ
h+1)

,"
ALGORITHM,0.2910958904109589,"where Λk
h = k−1
X"
ALGORITHM,0.2945205479452055,"τ=1
φ(xτ
h, aτ
h)φ(xτ
h, aτ
h)⊤+ I. (3.3)"
ALGORITHM,0.2979452054794521,Under review as a conference paper at ICLR 2022
ALGORITHM,0.3013698630136986,"To encourage exploration, we additionally adds a bonus function to estimate the leader’s Q-function:"
ALGORITHM,0.3047945205479452,"Qk
h(·, ·, ·) ←rl,h(·, ·, ·) + ΠH−h{φ(·, ·)⊤wk
h + Γk
h(·, ·)},"
ALGORITHM,0.3082191780821918,"where Γk
h(·, ·) = β ·
q"
ALGORITHM,0.3116438356164384,"φ(·, ·)⊤(Λk
h)−1φ(·, ·).
(3.4)"
ALGORITHM,0.3150684931506849,"Here Γk
h : S × Al →R is a bonus function and β > 0 is a parameter which will be speciﬁed later.
This form of bonus function is common in the literature of linear bandits (Lattimore & Szepesv´ari,
2020) and linear MDPs (Jin et al., 2020b)."
ALGORITHM,0.3184931506849315,"Then, we construct policies for the leader and followers by the subroutine ϵ-SNE (Algorithm 2).
Speciﬁcally, let Qk
h be the class of functions Q : S × Al × Af →R that takes form"
ALGORITHM,0.3219178082191781,"Q(·, ·, ·) = rl,h(·, ·, ·) + ΠH−h

φ(·, ·)⊤w + β ·
 
φ(·, ·)⊤Λ−1φ(·, ·)
1/2	
,
(3.5)"
ALGORITHM,0.3253424657534247,"where the parameters (w, Λ) ∈Rd × Rd×d satisfy ∥w∥≤H
√"
ALGORITHM,0.3287671232876712,"dk and λmin(Λ) ≥1. Moreover, let
Qk
h,ϵ be a ﬁxed ϵ-covering of Qk
h with respect to the ℓ∞norm. By Lemma C.10, we have Qk
h ∈Qk
h,
which allows us to pick a eQ ∈Qk
h,ϵ such that ∥eQ −Qk
h∥∞≤ϵ and calculate policies by"
ALGORITHM,0.3321917808219178,"(πk
h(· | x), {νk
fi,h(· | x)}i∈[N]) ←SNE( eQ(x, ·, ·), {rfi,h(x, ·, ·)}i∈[N]), ∀x.
(3.6)"
ALGORITHM,0.3356164383561644,"When there is only one follower, such a problem can be transformed to a linear programming (LP)
problem (Conitzer & Sandholm, 2006; Von Stengel & Zamir, 2010), and thus can be solved ef-
ﬁciently.
For the multi-follower case, however, solving such a matrix game in general is hard
(Conitzer & Sandholm, 2006; Basilico et al., 2017a;b; Coniglio et al., 2020). Given this computa-
tional hardness, we focus on the sample complexity and explicitly assume access to the following
computational oracle:
Assumption 3.2. We assume access to an oracle that implements Line 3 of Algorithm 2 when there
are multiple followers (i.e., N ≥2)."
ALGORITHM,0.339041095890411,"Now we explain the motivation for using the subroutine ϵ-SNE to construct policies instead of solv-
ing the matrix games with payoff matrices (Qk
h(x, ·, ·), {rfi,h(x, ·, ·)}i∈[N]) directly. By the deﬁni-
tion of Qk
h in (3.4), we know Qk
h relies on the previous data via the estimated value function V k
h+1
and feature maps {φ(xτ
h, aτ
h, bτ
h)}k−1
τ=1. Similar to the analysis for linear MDPs (Jin et al., 2020b),
we need to use a covering argument to establish uniform concentration bounds for all value V k
h+1.
Jin et al. (2020b) directly constructs an ϵ-net for the value functions and establishes a polynomial
log-covering number for this ϵ-net. This analysis, however, relies on that the policies executed by
the players are greedy (deterministic), which is not valid for our setting. To overcome this technical
issue, we construct an ϵ-net for Q-functions and solve an approximate matrix game. Fortunately, by
choosing a small enough ϵ, we can handle the errors caused by this approximation. See §C for more
details. Moreover, as shown in Xie et al. (2020), this subroutine can be implemented efﬁciently
without explicitly computing the exponentially large ϵ-net."
ALGORITHM,0.3424657534246575,"Finally, the leader and the followers play the game according to the obtained policies.
3.3
THEORETICAL RESULTS"
ALGORITHM,0.3458904109589041,"Our main theoretical result is the following bound on the regret incurred by Algorithm 1. Recall that
the regret is deﬁned in Deﬁnition 3.1 and T = KH is the total number of timesteps.
Theorem 3.3. Under Assumptions 2.1 and 3.2, there exists an absolute constant C > 0 such that,
for any ﬁxed p ∈(0, 1), by setting β = C · dH√ι with ι = log(2dT/p) in Line 7 of Algorithm 1
and ϵ =
1
KH in Algorithm 2, then with probability at least 1 −p, the regret incurred by OVI-SNE
satisﬁes that"
ALGORITHM,0.3493150684931507,"Regret(K) ≤O(
√"
ALGORITHM,0.3527397260273973,d3H3Tι2).
ALGORITHM,0.3561643835616438,Proof. See §C for a detailed proof.
ALGORITHM,0.3595890410958904,"Learning Stackelberg Equilibria. When there is only one follower, Stackelberg-Nash equilib-
rium reduces to the Stackelberg equilibrium (Simaan & Cruz, 1973; Conitzer & Sandholm, 2006;
Bai et al., 2021). Thus, we partly answer the open problem in Bai et al. (2021) on how to learn
Stackelberg equilibria in (leader-controller) Markov games."
ALGORITHM,0.363013698630137,Under review as a conference paper at ICLR 2022
ALGORITHM,0.3664383561643836,Algorithm 1 Optimistic Value Iteration to Find Stackelberg-Nash Equilibria
ALGORITHM,0.3698630136986301,"1: Initialize Vl,H+1(·) = Vf,H+1(·) = 0.
2: for k = 1, 2, · · · , K do
3:
Receive initial state xk
1.
4:
for step h = H, H −1, · · · , 1 do
5:
Λk
h ←Pk−1
τ=1 φ(xτ
h, aτ
h)φ(xτ
h, aτ
h)⊤+ I."
ALGORITHM,0.3732876712328767,"6:
wk
h ←(Λk
h)−1 Pk−1
τ=1 φ(xτ
h, aτ
h) · V k
h+1(xτ
h+1)."
ALGORITHM,0.3767123287671233,"7:
Γk
h(·, ·) ←β · (φ(·, ·)⊤(Λk
h)−1φ(·, ·))1/2.
8:
Qk
h(·, ·, ·) ←rl,h(·, ·, ·) + ΠH−h{φ(·, ·)⊤wk
h + Γk
h(·, ·)}.
9:
(πk
h(· | x), {νk
fi,h(· | x)}i∈[N]) ←ϵ-SNE(Qk
h(x, ·, ·), {rfi,h(x, ·, ·)}i∈[N]), ∀x. (Alg. 2)"
ALGORITHM,0.3801369863013699,"10:
V k
h (x) ←Ea∼πk
h(· | x),b1∼νk
f1,h(· | x),··· ,bN∼νk
fN ,h(· | x)Qk
h(x, a, b1, · · · , bN), ∀x."
ALGORITHM,0.3835616438356164,"11:
end for
12:
for h = 1, 2, ·, H do
13:
Sample ak
h ∼πk
h(· | xk
h), bk
1,h ∼νk
f1,h(· | xk
h), · · · , bk
N,h ∼νk
fN,h(· | xk
h)."
ALGORITHM,0.386986301369863,"14:
Leader takes action ak
h; Followers take actions bk
h = {bk
i,h}i∈[N]."
ALGORITHM,0.3904109589041096,"15:
Observe next state xk
h+1.
16:
end for
17: end for"
ALGORITHM,0.3938356164383562,Algorithm 2 ϵ-SNE
ALGORITHM,0.3972602739726027,"1: Input: Qk
h, x, and parameter ϵ."
ALGORITHM,0.4006849315068493,"2: Select eQ from Qk
h,ϵ satisfying ∥eQ −Qk
h∥∞≤ϵ."
ALGORITHM,0.4041095890410959,"3: For the input state x, let (πk
h(· | x), {νk
fi,h(· | x)}i∈[N]) be the Stackelberg-Nash equilibrium for
the matrix game with payoff matrices ( eQ(x, ·, ·), {rfi,h(x, ·, ·)}i∈[N]).
4: Output: (πk
h(· | x), {νk
fi,h(· | x)}i∈[N])."
ALGORITHM,0.4075342465753425,"Optimality of the Bound. Assuming that the action of the follower won’t affect the transition
kernel and reward function, the linear Markov games reduces to the linear MDP (Jin et al., 2020b).
Meanwhile, the lower bound established in Azar et al. (2017); Jin et al. (2018) for tabular MDPs
and the lower bound established in Lattimore & Szepesv´ari (2020) for linear bandits directly imply
a lower bound Ω(dH
√"
ALGORITHM,0.410958904109589,"T) for the linear MDPs, which further yields a lower bound Ω(dH
√"
ALGORITHM,0.4143835616438356,"T) for
our setting. Ignoring the logarithmic factors, there is only a gap of
√"
ALGORITHM,0.4178082191780822,"dH between this lower bound
and our upper bound. We also point out that, by using the “Bernstein-type” bonus (Azar et al., 2017;
Jin et al., 2018; Zhou et al., 2020), we can improve our upper bound by a factor of
√"
ALGORITHM,0.4212328767123288,"H. Here we
don’t apply this technique for the clarity of the analysis."
ALGORITHM,0.4246575342465753,"Misspeciﬁcation. For ease of presentation, we assume the Markov games are leader-controller in
Assumption 2.1. When the transitions do not ideally satisfy the leader-controller assumption, we can
potentially consider cases that transitions satisfy, for instance, |Ph(· | x, a, b) −Ph(· | x, a)∥∞≤ϱ
for any (h, x, a, b) ∈[H] × S × Al × Af, Here ϱ is the misspeciﬁcation error. We can still follow
the above method to tackle the misspeciﬁed cases. However, because of the misspeciﬁcation error
cumulated during T steps, an extra term O(ϱT) will appear in the ﬁnal result. In particular, When
ϱ is small, that is the Markov games have approximately leader-controller transitions, the extra
term O(ϱT) should be small, which further indicates that we can ﬁnd SNEs efﬁciently in some
misspeciﬁed general-sum Markov games."
ALGORITHM,0.4280821917808219,"Unknown Reward Setting. At a high level, we ﬁrst conduct a reward-free exploration algorithm
(Algorithm 4 in §D), a variant of Reward-Free RL-Explore algorithm in Jin et al. (2020a), to obtain
estimated reward functions {brl, brf1, · · · brfN }. As asserted before, we can use Algorithm 1, to ﬁnd the
SNE with respect to the known estimated reward functions {brl, brf1, · · · brfN }. Hence, we can obtain
the approximate SNE if the value functions of estimated value functions are good approximation of
the true value functions. See §E for more details."
ALGORITHM,0.4315068493150685,Under review as a conference paper at ICLR 2022
MAIN RESULTS FOR THE OFFLINE SETTING,0.4349315068493151,"4
MAIN RESULTS FOR THE OFFLINE SETTING"
MAIN RESULTS FOR THE OFFLINE SETTING,0.4383561643835616,"In this section, we study the ofﬂine setting, where the central controller aims to ﬁnd a Stackelberg-
Nash equilibrium by an ofﬂine dataset. Below we describe the setup and learning objective, followed
by our algorithm and theoretical results."
SETUP AND LEARNING OBJECTIVE,0.4417808219178082,"4.1
SETUP AND LEARNING OBJECTIVE"
SETUP AND LEARNING OBJECTIVE,0.4452054794520548,"We study the ofﬂine setting, where the learner has access to the reward functions (rl, rf = {rfi}N
i=1)
and a dataset D = {(xτ
h, aτ
h, bτ
h = {bτ
i,h}N
i=1)}K,H
τ,h=1, which is collected a priori by some experi-
menter. Then we make a minimal assumption for the ofﬂine dataset.
Assumption 4.1 (Compliance of Dataset). We assume that the dataset D is compliant with the
underlying Markov game (S, Al, Af, H, rl, rf, P), that is, for any x′ ∈S at step h ∈[H] of each
trajectory τ ∈[K],"
SETUP AND LEARNING OBJECTIVE,0.4486301369863014,"PD(xτ
h+1 = x′ | {xj
h, aj
h, bj
h, xj
h+1}τ−1
j=1 ∪{xτ
h, aτ
h, bτ
h}) = P(xh+1 = x′ | xh = xτ
h, ah = aτ
h)."
SETUP AND LEARNING OBJECTIVE,0.4520547945205479,"Here the probability on the left-hand side is with respect to the joint distribution over dataset D and
the probability on the right-hand side is with respect to the underlying Markov game."
SETUP AND LEARNING OBJECTIVE,0.4554794520547945,"Assumption 4.1 is adopted from Jin et al. (2020c), which indicates the Markov property of the dataset
D and that xτ
h+1 is generated by the underlying Markov game conditioned on (xτ
h, aτ
h, bτ
h). As a
special case, Assumption 4.1 holds when the experimenter follows ﬁxed behavior policies. More
generally, Assumption 4.1 allows the experimenter to choose actions aτ
h and bτ
h arbitrarily, even in
an adaptive or adversarial manner. In particular, we can assume that aτ
h and bτ
h are interdependent
across each trajectory τ ∈[K]. For instance, the experimenter can sequentially improve the behavior
policy using any online algorithm for Markov games."
SETUP AND LEARNING OBJECTIVE,0.4589041095890411,"Learning Objective. Similar to the online setting, we deﬁne the following performance metric"
SETUP AND LEARNING OBJECTIVE,0.4623287671232877,"SubOpt(π, ν, x) = V π∗,ν∗"
SETUP AND LEARNING OBJECTIVE,0.4657534246575342,"l,1
(x) −V π,ν∗(π)
l,1
(x)
|
{z
}
SubOptl + N
X"
SETUP AND LEARNING OBJECTIVE,0.4691780821917808,"i=1
[V π,ν∗(π)
fi,1
(x) −V
π,νfi,ν∗
f−i(π)"
SETUP AND LEARNING OBJECTIVE,0.4726027397260274,"fi,1
(x)"
SETUP AND LEARNING OBJECTIVE,0.476027397260274,"|
{z
}
SubOptf"
SETUP AND LEARNING OBJECTIVE,0.4794520547945205,"],
(4.1)"
SETUP AND LEARNING OBJECTIVE,0.4828767123287671,"which evaluates the suboptimality of policies (π, ν = {νfi}N
i=1) given the initial state x ∈S."
ALGORITHM AND THEORETICAL RESULTS,0.4863013698630137,"4.2
ALGORITHM AND THEORETICAL RESULTS"
ALGORITHM AND THEORETICAL RESULTS,0.4897260273972603,"As is known to us, the key challenge of online setting is the the tradeoff between exploration and
exploration. In the online setting. by following the “optimism in the face of uncertainty” principle
(Sutton & Barto, 2018), we use bonus functions to incentivize exploration and thus achieve sample-
efﬁcient. This intrinsic challenge of online setting disappears in the ofﬂine setting because we do
not need exploration any more. But another challenge arises: we only have access to the limited
data. To tackle this challenge, we need add some penalty functions to achieve robustness against the
uncertainty due to the ﬁnite data. This is also known as pessimism (Yu et al., 2020; Jin et al., 2020c;
Liu et al., 2020b; Buckman et al., 2020; Kidambi et al., 2020; Kumar et al., 2020; Rashidinejad
et al., 2021). Here we simply ﬂip the sign of bonus functions deﬁned in (3.4) to serve as penalty
functions. See Algorithm 3 for details."
ALGORITHM AND THEORETICAL RESULTS,0.4931506849315068,"Suppose that (bπ, bν) are the output policies of Algorithm 3. Then we evaluate the performance of
(bπ, bν) by establishing an upper bound for the optimality gap deﬁned in (4.1).
Theorem 4.2. Under Assumptions 2.1, 3.2, and 4.1, there exists an absolute constant C > 0 such
that, for any ﬁxed p ∈(0, 1), by setting β′ = C · dH
p"
ALGORITHM AND THEORETICAL RESULTS,0.4965753424657534,"log(2dHK/p) in Line 6 of Algorithm 3 and
ϵ =
d
KH in Algorithm 2, then with probability at least 1 −p, we have"
ALGORITHM AND THEORETICAL RESULTS,0.5,"SubOpt(bπ, bν, x) ≤3β′
H
X"
ALGORITHM AND THEORETICAL RESULTS,0.5034246575342466,"h=1
Eπ∗,x
 
φ(sh, ah)⊤(Λh)−1φ(sh, ah)
1/2
,
(4.2)"
ALGORITHM AND THEORETICAL RESULTS,0.5068493150684932,"where Eπ∗,x is taken with respect to the trajectory incurred by π∗in the underlying leader-controller
Markov game when initializing the progress at x. Here Λh is deﬁned in Line 4 of Algorithm 3."
ALGORITHM AND THEORETICAL RESULTS,0.5102739726027398,Under review as a conference paper at ICLR 2022
ALGORITHM AND THEORETICAL RESULTS,0.5136986301369864,Proof. See §F for a detailed proof.
ALGORITHM AND THEORETICAL RESULTS,0.5171232876712328,Algorithm 3 Pessimistic Value Iteration to Find Stackelberg-Nash Equilibria
ALGORITHM AND THEORETICAL RESULTS,0.5205479452054794,"1: Input: D = {xτ
h, aτ
h, bτ
h = {bτ
i,h}i∈[N]}K,H
τ,h=1 and reward functions {rl, rf = {rfi}i∈[N]}."
ALGORITHM AND THEORETICAL RESULTS,0.523972602739726,"2: Initialize bVH+1(·) = 0.
3: for step h = H, H −1, · · · , 1 do
4:
Λh ←PK
τ=1 φ(xτ
h, aτ
h)φ(xτ
h, aτ
h)⊤+ I."
ALGORITHM AND THEORETICAL RESULTS,0.5273972602739726,"5:
wh ←(Λh)−1 PK
τ=1 φ(xτ
h, aτ
h) · bVh+1(xτ
h+1)."
ALGORITHM AND THEORETICAL RESULTS,0.5308219178082192,"6:
Γh(·, ·) ←β′ · (φ(·, ·)⊤(Λh)−1φ(·, ·))1/2.
7:
bQh(·, ·, ·) ←rl,h(·, ·, ·) + ΠH−h{φ(·, ·)⊤wh −Γh(·, ·)}."
ALGORITHM AND THEORETICAL RESULTS,0.5342465753424658,"8:
(bπh(· | x), {bνfi,h(· | x)}i∈[N]) ←ϵ-SNE( bQh(x, ·, ·), {rfi,h(x, ·, ·)}i∈[N]), ∀x. (Alg. 2)"
ALGORITHM AND THEORETICAL RESULTS,0.5376712328767124,"9:
bVh(x) ←Ea∼bπh(· | x),b1∼bνf1,h(· | x),··· ,bN∼bνfN ,h(· | x) bQh(x, a, b1, · · · , bN), ∀x.
10: end for
11: Output: (bπ = {bπh}H
h=1, bν = {bνfi = {νfi,h}H
h=1}N
i=1)."
ALGORITHM AND THEORETICAL RESULTS,0.541095890410959,"Minimal Assumption Requirement: Theorem 4.2 only relies on the compliance of the dataset
with linear Markov games. Compared with existing literature on ofﬂine RL (Bertsekas & Tsitsiklis,
1996; Antos et al., 2007; 2008; Munos & Szepesv´ari, 2008; Farahmand et al., 2010; 2016; Scherrer
et al., 2015; Liu et al., 2018; Chen & Jiang, 2019; Fan et al., 2020; Xie & Jiang, 2020), we impose
no restrictions on the coverage of the dataset. Meanwhile, we need no assumption on the afﬁnity
between (bπ, bν) and the behavior policies that induce the dataset, which is often employed as a
regularizer (Fujimoto et al., 2019; Laroche et al., 2019; Jaques et al., 2019; Wu et al., 2019; Kumar
et al., 2019; Wang et al., 2020; Siegel et al., 2020; Nair et al., 2020; Liu et al., 2020b)."
ALGORITHM AND THEORETICAL RESULTS,0.5445205479452054,"Dataset with Sufﬁcient Coverage: In what follows, we specialize Theorem 4.2 to the setting where
we assume the dataset with good “coverage”. Note that Λh is determined by the ofﬂine dataset D
and acts as a ﬁxed matrix in the expectation, that is, the expectation in (4.2) is only taken with the
trajectory induced by π∗. As proofed in the following theorem, when the trajectory induced by π∗
is “covered” by the dataset D sufﬁciently well, we can establish that the suboptimality incurred by
Algorithm 3 diminishes at rate of e
O(1/
√"
ALGORITHM AND THEORETICAL RESULTS,0.547945205479452,"K).
Corollary 4.3. Suppose it holds with probability at least 1 −p/2 that"
ALGORITHM AND THEORETICAL RESULTS,0.5513698630136986,"Λh ⪰I + c · K · Eπ∗,x[φ(sh, ah)φ(sh, ah)⊤]
for all (x, h) ∈S × [H]. Here c > 0 is an absolute constant and Eπ∗,x is taken with respect to
the trajectory incurred by π∗in the underlying leader-controller Markov game when initializing the
progress at x. Under Assumptions 2.1, 3.2 and 4.1, there exists an absolute constant C > 0 such
that, for any ﬁxed p ∈(0, 1), by setting β′ = C · dH
p"
ALGORITHM AND THEORETICAL RESULTS,0.5547945205479452,"log(4dHK/p) in Line 6 of Algorithm 3 and
ϵ =
d
KH in Algorithm 2, then it holds with probability at least 1 −p that"
ALGORITHM AND THEORETICAL RESULTS,0.5582191780821918,"SubOpt(bπ, bν, x) ≤¯C · d3/2H2p"
ALGORITHM AND THEORETICAL RESULTS,0.5616438356164384,log(4dHK/p)/K
ALGORITHM AND THEORETICAL RESULTS,0.565068493150685,for all x ∈S. Here ¯C is another absolute constant that only depends on c and C.
ALGORITHM AND THEORETICAL RESULTS,0.5684931506849316,Proof. See §G for a detailed proof.
ALGORITHM AND THEORETICAL RESULTS,0.571917808219178,"Note that, unlike the previous literature (Antos et al., 2007; Munos & Szepesv´ari, 2008; Farahmand
et al., 2010; 2016; Scherrer et al., 2015; Liu et al., 2018; Chen & Jiang, 2019; Fan et al., 2020; Xie
& Jiang, 2020) which relies on the “uniform coverage” assumption, Corollary 4.3 only assumes that
the dataset has a good coverage of the trajectory incurred by the policy π∗."
ALGORITHM AND THEORETICAL RESULTS,0.5753424657534246,"Optimality of the Bound:
Assuming the dummy followers, that is, the actions taken by
the followers won’t affect the reward functions and transition kernels, the Markov games re-
duces to the linear MDP (Jin et al., 2020b).
Together with the information-theoretic lower
bound Ω(PH
h=1 Eπ∗,x[(φ(sh, ah)⊤(Λh)−1φ(sh, ah))1/2]) established in Jin et al. (2020c) for lin-
ear MDPs, we immediately obtain the same lower bound for our setting. In particular, our upper
bound established in Theorem 4.2 matches this lower bound up to β′ and absolute constants and
thus implies that our algorithm is nearly minimax optimal."
ALGORITHM AND THEORETICAL RESULTS,0.5787671232876712,Under review as a conference paper at ICLR 2022
REFERENCES,0.5821917808219178,REFERENCES
REFERENCES,0.5856164383561644,"Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In NIPS, volume 11, pp. 2312–2320, 2011."
REFERENCES,0.589041095890411,"Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory, pp. 67–83. PMLR, 2020."
REFERENCES,0.5924657534246576,"Forest Agostinelli, Stephen McAleer, Alexander Shmakov, and Pierre Baldi. Solving the rubik’s
cube with deep reinforcement learning and search. Nature Machine Intelligence, 1(8):356–363,
2019."
REFERENCES,0.5958904109589042,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019."
REFERENCES,0.5993150684931506,"Andr´as Antos, R´emi Munos, and Csaba Szepesv´ari. Fitted q-iteration in continuous action-space
mdps. 2007."
REFERENCES,0.6027397260273972,"Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
71(1):89–129, 2008."
REFERENCES,0.6061643835616438,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463–474. PMLR, 2020."
REFERENCES,0.6095890410958904,"Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J Kappen. Minimax PAC bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325–349, 2013."
REFERENCES,0.613013698630137,"Mohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263–272. PMLR, 2017."
REFERENCES,0.6164383561643836,"Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Inter-
national Conference on Machine Learning, pp. 551–560. PMLR, 2020."
REFERENCES,0.6198630136986302,"Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv
preprint arXiv:2006.12007, 2020."
REFERENCES,0.6232876712328768,"Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efﬁcient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021."
REFERENCES,0.6267123287671232,"Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without
regrets: Online learning in stackelberg security games. In Proceedings of the sixteenth ACM
conference on economics and computation, pp. 61–78, 2015."
REFERENCES,0.6301369863013698,"Tamer Bas¸ar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998."
REFERENCES,0.6335616438356164,"Nicola Basilico, Stefano Coniglio, and Nicola Gatti. Methods for ﬁnding leader–follower equilibria
with multiple followers. arXiv preprint arXiv:1707.02174, 2017a."
REFERENCES,0.636986301369863,"Nicola Basilico, Stefano Coniglio, Nicola Gatti, and Alberto Marchesi. Bilevel programming ap-
proaches to the computation of optimistic and pessimistic single-leader-multi-follower equilibria.
In SEA, volume 75, pp. 1–14. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik GmbH, Dagstuhl
Publishing, 2017b."
REFERENCES,0.6404109589041096,"Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996."
REFERENCES,0.6438356164383562,"Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Learning optimal commitment to overcome
insecurity. 2014."
REFERENCES,0.6472602739726028,"Michele Breton, Abderrahmane Alj, and Alain Haurie. Sequential stackelberg equilibria in two-
person games. Journal of Optimization Theory and Applications, 59(1):71–97, 1988."
REFERENCES,0.6506849315068494,Under review as a conference paper at ICLR 2022
REFERENCES,0.6541095890410958,"V´ıctor Bucarey, Eugenio Della Vecchia, Alain Jean-Marie, and Fernando Ord´o˜nez.
Stationary
Strong Stackelberg Equilibrium in Discounted Stochastic Games. PhD thesis, INRIA, 2019a."
REFERENCES,0.6575342465753424,"V´ıctor Bucarey, Alain Jean-Marie, Eugenio Della Vecchia, and Fernando Ord´o˜nez. On the value
iteration method for dynamic strong stackelberg equilibria. In ROADEF 2019-20`eme congr`es
annuel de la soci´et´e Franc¸aise de Recherche Op´erationnelle et d’Aide `a la D´ecision, 2019b."
REFERENCES,0.660958904109589,"Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in ﬁxed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020."
REFERENCES,0.6643835616438356,"Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 38(2):156–172, 2008."
REFERENCES,0.6678082191780822,"Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efﬁcient exploration in policy opti-
mization. In International Conference on Machine Learning, pp. 1283–1294. PMLR, 2020."
REFERENCES,0.6712328767123288,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, pp. 1042–1051. PMLR, 2019."
REFERENCES,0.6746575342465754,"Tianyi Chen, Yuejiao Sun, and Wotao Yin.
A single-timescale stochastic bilevel optimization
method. arXiv preprint arXiv:2102.04671, 2021a."
REFERENCES,0.678082191780822,"Zhuoqun Chen, Yangyang Liu, Bo Zhou, and Meixia Tao. Caching incentive design in wireless d2d
networks: A stackelberg game approach. In 2016 IEEE International Conference on Communi-
cations (ICC), pp. 1–6. IEEE, 2016."
REFERENCES,0.6815068493150684,"Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021b."
REFERENCES,0.684931506849315,"Stefano Coniglio, Nicola Gatti, and Alberto Marchesi. Computing a pessimistic stackelberg equi-
librium with multiple followers: The mixed-pure case. Algorithmica, 82(5):1189–1238, 2020."
REFERENCES,0.6883561643835616,"Vincent Conitzer and Tuomas Sandholm.
Complexity of mechanism design.
arXiv preprint
cs/0205075, 2002."
REFERENCES,0.6917808219178082,"Vincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. In Proceed-
ings of the 7th ACM conference on Electronic commerce, pp. 82–90, 2006."
REFERENCES,0.6952054794520548,"Qiwen Cui and Lin F Yang. Minimax sample complexity for turn-based stochastic game. arXiv
preprint arXiv:2011.14267, 2020."
REFERENCES,0.6986301369863014,"Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008."
REFERENCES,0.702054794520548,"Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021."
REFERENCES,0.7054794520547946,"Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
Benchmarking deep
reinforcement learning for continuous control. In International conference on machine learning,
pp. 1329–1338. PMLR, 2016."
REFERENCES,0.708904109589041,"Yonathan Efroni, Lior Shani, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization
with bandit feedback. arXiv preprint arXiv:2002.08243, 2020."
REFERENCES,0.7123287671232876,"Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020."
REFERENCES,0.7157534246575342,"Amir Massoud Farahmand, R´emi Munos, and Csaba Szepesv´ari. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, 2010."
REFERENCES,0.7191780821917808,"Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv´ari, and Shie Mannor. Reg-
ularized policy iteration with nonparametric function spaces. The Journal of Machine Learning
Research, 17(1):4809–4874, 2016."
REFERENCES,0.7226027397260274,Under review as a conference paper at ICLR 2022
REFERENCES,0.726027397260274,"Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stack-
elberg games. arXiv preprint arXiv:1906.01217, 2019."
REFERENCES,0.7294520547945206,"Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business
Media, 2012."
REFERENCES,0.7328767123287672,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.7363013698630136,"Dinesh Garg and Yadati Narahari.
Design of incentive compatible mechanisms for stackelberg
problems. In International Workshop on Internet and Network Economics, pp. 718–727. Springer,
2005."
REFERENCES,0.7397260273972602,"Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018."
REFERENCES,0.7431506849315068,"Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In ICML, volume 3, pp.
242–249, 2003."
REFERENCES,0.7465753424657534,"Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly poly-
nomial for 2-player turn-based stochastic games with a constant discount factor. Journal of the
ACM (JACM), 60(1):1–16, 2013."
REFERENCES,0.75,"Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Is multiagent deep reinforcement learn-
ing the answer or the question? a brief survey. Learning, 21:22, 2018."
REFERENCES,0.7534246575342466,"Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A survey and critique of multiagent deep
reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750–797, 2019."
REFERENCES,0.7568493150684932,"Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic.
arXiv preprint
arXiv:2007.05170, 2020."
REFERENCES,0.7602739726027398,"Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003."
REFERENCES,0.7636986301369864,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019."
REFERENCES,0.7671232876712328,"Zeyu Jia, Lin F Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019."
REFERENCES,0.7705479452054794,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efﬁ-
cient? arXiv preprint arXiv:1807.03765, 2018."
REFERENCES,0.773972602739726,"Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pp. 4870–4879.
PMLR, 2020a."
REFERENCES,0.7773972602739726,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020b."
REFERENCES,0.7808219178082192,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine RL? arXiv
preprint arXiv:2012.15085, 2020c."
REFERENCES,0.7842465753424658,"Xin Kang and Yongdong Wu. Incentive mechanism design for heterogeneous peer-to-peer networks:
A stackelberg game approach. IEEE Transactions on Mobile Computing, 14(5):1018–1030, 2014."
REFERENCES,0.7876712328767124,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020."
REFERENCES,0.791095890410959,"Dmytro Korzhyk, Zhengyu Yin, Christopher Kiekintveld, Vincent Conitzer, and Milind Tambe.
Stackelberg vs. Nash in security games: An extended investigation of interchangeability, equiva-
lence, and uniqueness. Journal of Artiﬁcial Intelligence Research, 41:297–327, 2011."
REFERENCES,0.7945205479452054,Under review as a conference paper at ICLR 2022
REFERENCES,0.797945205479452,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.8013698630136986,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.8047945205479452,"Michail Lagoudakis and Ron Parr. Value function approximation in zero-sum markov games. arXiv
preprint arXiv:1301.0580, 2012."
REFERENCES,0.8082191780821918,"Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping.
In International Conference on Machine Learning, pp. 3652–3661.
PMLR, 2019."
REFERENCES,0.8116438356164384,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.815068493150685,"Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the opti-
mal strategy to commit to. In International Symposium on Algorithmic Game Theory, pp. 250–
262. Springer, 2009."
REFERENCES,0.8184931506849316,"Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994."
REFERENCES,0.821917808219178,"Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322–
328, 2001."
REFERENCES,0.8253424657534246,"Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Inﬁnite-
horizon off-policy estimation. arXiv preprint arXiv:1810.12429, 2018."
REFERENCES,0.8287671232876712,"Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. arXiv preprint arXiv:2010.01604, 2020a."
REFERENCES,0.8321917808219178,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-
ment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020b."
REFERENCES,0.8356164383561644,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.839041095890411,"R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(5), 2008."
REFERENCES,0.8424657534246576,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.8458904109589042,"John F Nash. Non-Cooperative Games. Princeton University Press, 2016."
REFERENCES,0.8493150684931506,"Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep rein-
forcement learning. arXiv preprint arXiv:1908.03963, 2019."
REFERENCES,0.8527397260273972,"Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit
to. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 2149–2156,
2019."
REFERENCES,0.8561643835616438,"Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum markov games. In International Conference on Machine Learning, pp.
1321–1329. PMLR, 2015."
REFERENCES,0.8595890410958904,"Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model
based reinforcement learning. In International Conference on Machine Learning, pp. 7953–7963.
PMLR, 2020."
REFERENCES,0.863013698630137,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.8664383561643836,Under review as a conference paper at ICLR 2022
REFERENCES,0.8698630136986302,"Lillian J Ratliff and Tanner Fiez. Adaptive incentive design. IEEE Transactions on Automatic
Control, 2020."
REFERENCES,0.8732876712328768,"Lillian J Ratliff, Ming Jin, Ioannis C Konstantakopoulos, Costas Spanos, and S Shankar Sastry.
Social game for building energy efﬁciency: Incentive design. In 2014 52nd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), pp. 1011–1018. IEEE, 2014."
REFERENCES,0.8767123287671232,"Tim Roughgarden. Stackelberg scheduling strategies. SIAM journal on computing, 33(2):332–350,
2004."
REFERENCES,0.8801369863013698,"Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist.
Approximate modiﬁed policy iteration and its application to the game of tetris. J. Mach. Learn.
Res., 16:1629–1676, 2015."
REFERENCES,0.8835616438356164,"Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095–1100, 1953."
REFERENCES,0.886986301369863,"Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 2992–3002. PMLR, 2020."
REFERENCES,0.8904109589041096,"Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller.
Keep doing
what worked: Behavioral modelling priors for ofﬂine reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020."
REFERENCES,0.8938356164383562,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.8972602739726028,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.9006849315068494,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–
1144, 2018."
REFERENCES,0.9041095890410958,"Marwaan Simaan and Jose B Cruz. On the stackelberg strategy in nonzero-sum games. Journal of
Optimization Theory and Applications, 11(5):533–555, 1973."
REFERENCES,0.9075342465753424,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.910958904109589,"Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057–
1063. Citeseer, 1999."
REFERENCES,0.9143835616438356,"Milind Tambe. Security and game theory: algorithms, deployed systems, lessons learned. Cam-
bridge university press, 2011."
REFERENCES,0.9178082191780822,"Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Provably efﬁcient online agnostic learning
in markov games. arXiv preprint arXiv:2010.15020, 2020."
REFERENCES,0.9212328767123288,"Bernhard Von Stengel and Shmuel Zamir. Leadership games with convex strategy sets. Games and
Economic Behavior, 69(2):446–457, 2010."
REFERENCES,0.9246575342465754,"Ziyu Wang, Alexander Novikov, Konrad ˙Zołna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020."
REFERENCES,0.928082191780822,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992."
REFERENCES,0.9315068493150684,"Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games.
arXiv preprint arXiv:1712.00579, 2017."
REFERENCES,0.934931506849315,Under review as a conference paper at ICLR 2022
REFERENCES,0.9383561643835616,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.9417808219178082,"Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory, pp. 3674–3682. PMLR, 2020."
REFERENCES,0.9452054794520548,"Tengyang Xie and Nan Jiang.
Q⋆-approximation schemes for batch reinforcement learning: A
theoretical comparison. arXiv preprint arXiv:2003.03924, 2020."
REFERENCES,0.9486301369863014,"Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995–7004. PMLR, 2019."
REFERENCES,0.952054794520548,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746–10756. PMLR, 2020."
REFERENCES,0.9554794520547946,"Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. Bridging exploration
and general function approximation in reinforcement learning: Provably efﬁcient kernel and neu-
ral value iterations. arXiv preprint arXiv:2011.04622, 2020."
REFERENCES,0.958904109589041,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma.
Mopo:
Model-based ofﬂine policy optimization.
arXiv preprint
arXiv:2005.13239, 2020."
REFERENCES,0.9623287671232876,"Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304–7312. PMLR, 2019."
REFERENCES,0.9657534246575342,"Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric.
Frequentist regret bounds for randomized least-squares value iteration. In International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 1954–1964. PMLR, 2020a."
REFERENCES,0.9691780821917808,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978–10989. PMLR, 2020b."
REFERENCES,0.9726027397260274,"Kaiqing Zhang, Zhuoran Yang, and Tamer Bas¸ar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019."
REFERENCES,0.976027397260274,"Kaiqing Zhang, Sham M Kakade, Tamer Bas¸ar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020a."
REFERENCES,0.9794520547945206,"Zihan Zhang, Xiangyang Ji, and Simon S Du. Is reinforcement learning more difﬁcult than bandits?
a near-optimal algorithm escaping the curse of horizon. arXiv preprint arXiv:2009.13503, 2020b."
REFERENCES,0.9828767123287672,"Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learningvia
reference-advantage decomposition. Advances in Neural Information Processing Systems, 33,
2020c."
REFERENCES,0.9863013698630136,"Yulai Zhao, Yuandong Tian, Jason D Lee, and Simon S Du. Provably efﬁcient policy gradient
methods for two-player zero-sum markov games. arXiv preprint arXiv:2102.08903, 2021."
REFERENCES,0.9897260273972602,"Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes,
and Richard Socher. The AI economist: Improving equality and productivity with AI-driven tax
policies. arXiv preprint arXiv:2004.13332, 2020."
REFERENCES,0.9931506849315068,"Ying-Ping Zheng, Tamer Basar, and Jose B Cruz. Stackelberg strategies and incentives in multiper-
son deterministic decision problems. IEEE transactions on Systems, Man, and Cybernetics, (1):
10–24, 1984."
REFERENCES,0.9965753424657534,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. arXiv preprint arXiv:2012.08507, 2020."
