Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014577259475218659,"Evaluating the worst-case performance of a reinforcement learning (RL) agent un-
der the strongest/optimal adversarial perturbations on state observations (within
some constraints) is crucial for understanding the robustness of RL agents. How-
ever, ï¬nding the optimal adversary is challenging, in terms of both whether we
can ï¬nd the optimal attack and how efï¬ciently we can ï¬nd it. Existing works on
adversarial RL either use heuristics-based methods that may not ï¬nd the strongest
adversary, or directly train an RL-based adversary by treating the agent as a part of
the environment, which can ï¬nd the optimal adversary but may become intractable
in a large state space. This paper introduces a novel attacking method to ï¬nd the
optimal attacks through collaboration between a designed function named â€œactorâ€
and an RL-based learner named â€œdirectorâ€. The actor crafts state perturbations for
a given policy perturbation direction, and the director learns to propose the best
policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically
optimal and signiï¬cantly more efï¬cient than prior RL-based works in environ-
ments with large state spaces. Empirical results show that our proposed PA-AD
universally outperforms state-of-the-art attacking methods in various Atari and
MuJoCo environments. By applying PA-AD to adversarial training, we achieve
state-of-the-art empirical robustness in multiple tasks under strong adversaries."
INTRODUCTION,0.0029154518950437317,"1
INTRODUCTION"
INTRODUCTION,0.004373177842565598,"Deep Reinforcement Learning (DRL) has achieved incredible success in many applications. How-
ever, recent works (Huang et al., 2017; Pattanaik et al., 2018) reveal that a well-trained RL agent
may be vulnerable to test-time evasion attacks, making it risky to deploy RL models in high-stakes
applications. As in most related works, we consider a state adversary which adds imperceptible
noise to the observations of an agent such that its cumulative reward is reduced during test time."
INTRODUCTION,0.0058309037900874635,"In order to understand the vulnerability of an RL agent and to improve its certiï¬ed robustness, it
is important to evaluate the worst-case performance of the agent under any adversarial attacks with
certain constraints. In other words, it is crucial to ï¬nd the strongest/optimal adversary that can
minimize the cumulative reward gained by the agent with ï¬xed constraints, as motivated in a recent
paper by Zhang et al. (2021). Therefore, we focus on the following question:"
INTRODUCTION,0.007288629737609329,"Given an arbitrary attack radius (budget) Ïµ for each step of the deployment, what is the worst-
case performance of an agent under the strongest adversary?"
INTRODUCTION,0.008746355685131196,myopic
INTRODUCTION,0.01020408163265306,myopic
INTRODUCTION,0.011661807580174927,Victim
INTRODUCTION,0.013119533527696793,strongest
INTRODUCTION,0.014577259475218658,"Figure 1: An example that a myopic
adversary is not the strongest."
INTRODUCTION,0.016034985422740525,"Finding the strongest adversary in RL is challenging. Many
existing attacks (Huang et al., 2017; Pattanaik et al., 2018) are
based on heuristics, crafting adversarial states at every step in-
dependently, although steps are interrelated in contrast to im-
age classiï¬cation tasks. These heuristic methods can often ef-
fectively reduce the agentâ€™s reward, but are not guaranteed to
achieve the strongest attack under a given budget. This type of
attack is â€œmyopicâ€ since it does not plan for the future. Fig-
ure 1 shows an intuitive example, where myopic adversaries
only prevent the agent from selecting the best action in the current step, but the strongest adversary
can strategically â€œleadâ€ the agent to a trap, which is the worst event for the agent."
INTRODUCTION,0.01749271137026239,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018950437317784258,"Achieving computational efï¬ciency arises as another challenge in practice, even if the strongest
adversary can be found in theory. A recent work (Zhang et al., 2020a) points out that learning the
optimal state adversary is equivalent to learning an optimal policy in a new Markov Decision Process
(MDP). A follow-up work (Zhang et al., 2021) shows that the learned adversary signiï¬cantly outper-
forms prior adversaries in MuJoCo games. However, the state space and the action space of the new
MDP are both as large as the state space in the original environment, which can be high-dimensional
in practice. For example, video games and autonomous driving systems use images as observations.
In these tasks, learning the state adversary directly becomes computationally intractable."
INTRODUCTION,0.02040816326530612,"To overcome the above two challenges, we propose a novel attack method called Policy Adver-
sarial Actor Director (PA-AD), where we design a â€œdirectorâ€ and an â€œactorâ€ that collaboratively
ï¬nds the optimal state perturbations. In PA-AD, a director learns an MDP named Policy Adversary
MDP (PAMDP), and an actor is embedded in the dynamics of PAMDP. At each step, the director
proposes a perturbing direction in the policy space, and the actor crafts a perturbation in the state
space to lead the victim policy towards the proposed direction. Through a trail-and-error process, the
director can ï¬nd the optimal way to cooperate with the actor and attack the victim policy. Theoret-
ical analysis shows that the optimal policy in PAMDP induces an optimal state adversary. The size
of PAMDP is generally smaller than the adversarial MDP deï¬ned by Zhang et al. (2021) and thus is
easier to be learned efï¬ciently using off-the-shelf RL algorithms. With our proposed director-actor
collaborative mechanism, PA-AD outperforms state-of-the-art attacking methods on various types
of environments, and improves the robustness of many DRL agents by adversarial training."
INTRODUCTION,0.021865889212827987,"Summary of Contributions
(1) We establish a theoretical understanding of the optimality of evasion attacks from the perspective
of policy perturbations, allowing a more efï¬cient implementation of optimal attacks.
(2) We introduce a Policy Adversary MDP (PAMDP) model, whose optimal policy induces the op-
timal state adversary under any attacking budget Ïµ.
(3) We propose a novel attack method, PA-AD, which efï¬ciently searches for the optimal adversary
in the PAMDP. PA-AD is a general method that works on stochastic and deterministic victim poli-
cies, vectorized and pixel state spaces, as well as discrete and continuous action spaces.
(4) Empirical study shows that PA-AD universally outperforms previous attacking methods in vari-
ous environments, including Atari games and MuJoCo tasks. PA-AD achieves impressive attacking
performance in many environments using very small attack budgets,
(5) Combining our strong attack PA-AD with adversarial training, we signiï¬cantly improve the ro-
bustness of RL agents, and achieve the state-of-the-art robustness in many tasks."
PRELIMINARIES AND NOTATIONS,0.023323615160349854,"2
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.02478134110787172,"The Victim RL Agent
In RL, an agent interacts with an environment modeled by a Markov
Decision Process (MDP) denoted as a tuple M = âŸ¨S, A, P, R, Î³âŸ©, where S is a state space with
cardinality |S|, A is an action space with cardinality |A|, P : S Ã— A â†’âˆ†(S) is the transition
function 1, R : S Ã— A â†’R is the reward function, and Î³ âˆˆ(0, 1) is the discount factor. In this
paper, we consider a setting where the state space is much larger than the action space, which arises
in a wide variety of environments. For notation simplicity, our theoretical analysis focuses on a ï¬nite
MDP, but our algorithm applies to continuous state spaces and continuous action spaces, as veriï¬ed
in experiments. The agent takes actions according to its policy, Ï€ : S â†’âˆ†(A). We suppose the
victim uses a ï¬xed policy Ï€ with a function approximator (e.g. a neural network) during test time.
We denote the space of all policies as Î , which is a Cartesian product of |S| simplices. The value
of a policy Ï€ âˆˆÎ  for state s âˆˆS is deï¬ned as V Ï€(s) = EÏ€,P [Pâˆ
t=0 Î³tR(st, at)|s0 = s]."
PRELIMINARIES AND NOTATIONS,0.026239067055393587,"Evasion Attacker
Evasion attacks are test-time attacks that aim to reduce the expected total
reward gained by the agent/victim. As in most literature (Huang et al., 2017; Pattanaik et al., 2018;
Zhang et al., 2020a), we assume the attacker knows the victim policy Ï€ (white-box attack). However,
the attacker does not know the environment dynamics, nor does it have the ability to change the
environment directly. The attacker can observe the interactions between the victim agent and the
environment, including states, actions and rewards. We focus on a typical state adversary (Huang
et al., 2017; Zhang et al., 2020a), which perturbs the state observations returned by the environment
before the agent observes them. Note that the underlying states in the environment are not changed."
PRELIMINARIES AND NOTATIONS,0.027696793002915453,1âˆ†(X) denotes the the space of probability distributions over X.
PRELIMINARIES AND NOTATIONS,0.029154518950437316,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.030612244897959183,"Formally, we model a state adversary by a function h which perturbs state s âˆˆS into Ëœs := h(s), so
that the input to the agentâ€™s policy is Ëœs instead of s. In practice, the adversarial perturbation is usually
under certain constraints. In this paper, we consider the common â„“p threat model (Goodfellow et al.,
2015): Ëœs should be in BÏµ(s), where BÏµ(s) denotes an â„“p norm ball centered at s with radius Ïµ â‰¥0, a
constant called the budget of the adversary for every step. With the budget constraint, we deï¬ne the
admissible state adversary and the admissible adversary set as below."
PRELIMINARIES AND NOTATIONS,0.03206997084548105,"Deï¬nition 1 (Set of Admissible State Adversaries HÏµ). A state adversary h is said to be admissible
if âˆ€s âˆˆS, we have h(s) âˆˆBÏµ(s). The set of all admissible state adversaries is denoted by HÏµ."
PRELIMINARIES AND NOTATIONS,0.033527696793002916,"Then the goal of the attacker is to ï¬nd an adversary hâˆ—in HÏµ that maximally reduces the cumulative
reward of the agent. In this work, we propose a novel method to learn the optimal state adversary
through the identiï¬cation of an optimal policy perturbation deï¬ned and motivated in the next section."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03498542274052478,"3
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03644314868804665,"In this section, we ï¬rst motivate our idea of interpreting evasion attacks as perturbations of policies,
then discuss how to efï¬ciently ï¬nd the optimal state adversary via the optimal policy perturbation."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.037900874635568516,"Figure 2: Equivalence between eva-
sion attacks and policy perturbations."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03935860058309038,"Evasion Attacks Are Perturbations of Policies
Although
existing literature usually considers state-attacks and action-
attacks separately, we point out that evasion attacks, either ap-
plied to states or actions, are essentially equivalent to perturb-
ing the agentâ€™s policy Ï€ into another policy Ï€h in the policy
space Î . For instance, as shown in Figure 2, if the adversary
h alters state s into state Ëœs, the victim selects an action Ëœa based
on Ï€(Â·|Ëœs). This is equivalent to directly perturbing Ï€(Â·|s) to Ï€h(Â·|s) := Ï€(Â·|Ëœs). (See Appendix A for
more detailed analysis including action adversaries.)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04081632653061224,"In this paper, we aim to ï¬nd the optimal state adversary through the identiï¬cation of the â€œoptimal
policy perturbationâ€, which has the following merits. (1) Ï€h(Â·|s) usually lies in a lower dimensional
space than h(s) for an arbitrary state s âˆˆS. For example, in Atari games, the action space is discrete
and small (e.g. |A| = 18), while a state is a high-dimensional image. Then the state perturbation
h(s) is an image, while Ï€h(Â·|s) is a vector of size |A|. (2) It is easier to characterize the optimality of
a policy perturbation than a state perturbation. How a state perturbation changes the value of a victim
policy depends on both the victim policy network and the environment dynamics. In contrast, how
a policy perturbation changes the victim value only depends on the environment. Our Theorem 4 in
Section 3 and Theorem 12 in Appendix B both provide insights about how V Ï€ changes as Ï€ changes
continuously. (3) Policy perturbation captures the essence of evasion attacks, and uniï¬es state and
action attacks. Although this paper focuses on state-space adversaries, the learned â€œoptimal policy
perturbationâ€ can also be used to conduct action-space attacks against the same victim."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04227405247813411,"Characterizing the Optimal Policy Adversary
As depicted in Figure 3, the policy perturbation
serves as a bridge connecting the perturbations in the state space and the value space. Our goal is
to ï¬nd the optimal state adversary by identifying the optimal â€œpolicy adversaryâ€. We ï¬rst deï¬ne
an Admissible Adversarial Policy Set (Adv-policy-set) BH
Ïµ (Ï€) âŠ‚Î  as the set of policies perturbed
from Ï€ by all admissible state adversaries h âˆˆHÏµ. In other words, when a state adversary perturbs
states within an â„“p norm ball BÏµ(Â·), the victim policy is perturbed within BH
Ïµ (Ï€)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.043731778425655975,"Deï¬nition 2 (Admissible Adversarial Policy Set (Adv-policy-set) BH
Ïµ (Ï€)). For an MDP M, a
ï¬xed victim policy Ï€, we deï¬ne the admissible adversarial policy set (Adv-policy-set) w.r.t. Ï€, de-
noted by BH
Ïµ (Ï€), as the set of policies that are perturbed from Ï€ by all admissible adversaries, i.e.,"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04518950437317784,"BH
Ïµ (Ï€) := {Ï€h âˆˆÎ  : âˆƒh âˆˆHÏµ s.t âˆ€s, Ï€h(Â·|s) = Ï€(Â·|h(s))}.
(1)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04664723032069971,"Remarks
(1) BH
Ïµ (Ï€) is a subset of the policy space Î  and it surrounds the victim Ï€, as shown
in Figure 3(middle). In the same MDP, BH
Ïµ (Ï€) varies for different victim Ï€ or different attack
budget Ïµ. (2) In Appendix B, we characterize the topological properties of BH
Ïµ (Ï€). We show that
for a continuous function Ï€ (e.g., neural network), BH
Ïµ (Ï€) is connected and compact, and the value
functions generated by all policies in the Adv-policy-set BH
Ïµ (Ï€) form a polytope (Figure 3(right)),
following the polytope theorem by Dadashi et al. (2019)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.048104956268221574,Published as a conference paper at ICLR 2022 ğ‘  ğ‘†
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04956268221574344,"â„(ğ‘ )
ğœ‹ ğœ‹! Î 
ğ‘‰ ğ‘‰"" ğ‘‰""!"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05102040816326531,"End-to-end
SA-RL:"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.052478134110787174,An RL problem in a large MDP
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05393586005830904,"PA-AD
(ours):"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05539358600583091,"An RL problem in a small MDP
An optimization problem"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.056851311953352766,"The Director
The Actor"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05830903790087463,(Policy network)
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0597667638483965,Environment
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.061224489795918366,"ğ‘©ğğ‘¯(ğ…) ğ‘‰!âˆ— ğ‘‰!""âˆ— ğ‘©ğ(ğ’”)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06268221574344024,Victim ğ…
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0641399416909621,"Figure 3: A state adversary h perturbs s into h(s) âˆˆBÏµ(s) in the state space; hence, the victimâ€™s policy Ï€ is
perturbed into Ï€h within the Adv-policy-set BH
Ïµ (Ï€); as a result, the expected total reward the victim can gain
becomes V Ï€h instead of V Ï€. A prior work SA-RL (Zhang et al., 2021) directly uses an RL agent to learn
the best state adversary hâˆ—, which works for MDPs with small state spaces, but suffers from high complexity
in larger MDPs. In contrast, we ï¬nd the optimal state adversary hâˆ—efï¬ciently through identifying the optimal
policy adversary Ï€hâˆ—. Our proposed attack method called PA-AD contains an RL-based â€œdirectorâ€ which learns
to propose policy perturbation Ï€h in the policy space, and a non-RL â€œactorâ€, which targets at the proposed Ï€h
and computes adversarial states in the state space. Through this collaboration, the director can learn the optimal
policy adversary Ï€hâˆ—using RL methods, such that the actor executes hâˆ—as justiï¬ed in Theorem 7."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06559766763848396,"Given that the Adv-policy-set BH
Ïµ (Ï€) contains all the possible policies the victim may execute under
admissible state perturbations, we can characterize the optimality of a state adversary through the
lens of policy perturbations. Recall that the attackerâ€™s goal is to ï¬nd a state adversary hâˆ—âˆˆHÏµ
that minimizes the victimâ€™s expected total reward. From the perspective of policy perturbation, the
attackerâ€™s goal is to perturb the victimâ€™s policy to another policy Ï€hâˆ—âˆˆBH
Ïµ (Ï€) with the lowest value.
Therefore, we can deï¬ne the optimal state adversary and the optimal policy adversary as below."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06705539358600583,"Deï¬nition 3 (Optimal State Adversary hâˆ—and Optimal Policy Adversary Ï€hâˆ—). For an MDP
M, a ï¬xed policy Ï€, and an admissible adversary set HÏµ with attacking budget Ïµ,
(1) an optimal state adversary hâˆ—satisï¬es hâˆ—âˆˆargminhâˆˆHÏµV Ï€h(s), âˆ€s âˆˆS, which leads to
(2) an optimal policy adversary Ï€hâˆ—satisï¬es Ï€hâˆ—âˆˆargminÏ€hâˆˆBH
Ïµ (Ï€)V Ï€h(s), âˆ€s âˆˆS.
Recall that Ï€h is the perturbed policy caused by adversary h, i.e., Ï€h(Â·|s) = Ï€(Â·|h(s)), âˆ€s âˆˆS."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06851311953352769,"Deï¬nition 3 implies an equivalent relationship between the optimal state adversary and the optimal
policy adversary: an optimal state adversary leads to an optimal policy adversary, and any state
adversary that leads to an optimal policy adversary is optimal. Theorem 19 in Appendix D.1 shows
that there always exists an optimal policy adversary for a ï¬xed victim Ï€, and learning the optimal
policy adversary is an RL problem. (A similar result have been shown by Zhang et al. (2020a) for
the optimal state adversary, while we focus on the policy perturbation.)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06997084548104957,"Due to the equivalence, if one ï¬nds an optimal policy adversary Ï€hâˆ—, then the optimal state adversary
can be found by executing targeted attacks with target policy Ï€hâˆ—. However, directly ï¬nding the
optimal policy adversary in the Adv-policy-set BH
Ïµ (Ï€) is challenging since BH
Ïµ (Ï€) is generated by
all admissible state adversaries in HÏµ and is hard to compute. To address this challenge, we ï¬rst get
insights from theoretical characterizations of the Adv-policy-set BH
Ïµ (Ï€). Theorem 4 below shows
that the â€œoutermost boundaryâ€ of BH
Ïµ (Ï€) always contains an optimal policy adversary. Intuitively, a
policy Ï€â€² is in the outermost boundary of BH
Ïµ (Ï€) if and only if no policy in BH
Ïµ (Ï€) is farer away from
Ï€ than Ï€â€² in the direction Ï€â€² âˆ’Ï€. Therefore, if an adversary can perturb a policy along a direction,
it should push the policy as far away as possible in this direction under the budget constraints.
Then, the adversary is guaranteed to ï¬nd an optimal policy adversary after trying all the perturbing
directions. In contrast, such a guarantee does not exist for state adversaries, justifying the beneï¬ts
of considering policy adversaries. Our proposed algorithm in Section 4 applies this idea to ï¬nd the
optimal attack: an RL-based director searches for the optimal perturbing direction, and an actor is
responsible for pushing the policy to the outermost boundary of BH
Ïµ (Ï€) with a given direction."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07142857142857142,"Theorem 4. For an MDP M, a ï¬xed policy Ï€, and an admissible adversary set HÏµ, deï¬ne the
outermost boundary of the admissible adversarial policy set BH
Ïµ (Ï€) w.r.t Ï€ as
âˆ‚Ï€BH
Ïµ (Ï€) := {Ï€â€² âˆˆBH
Ïµ (Ï€) : âˆ€s âˆˆS, Î¸ > 0, âˆ„Ë†Ï€ âˆˆBH
Ïµ (Ï€) s.t. Ë†Ï€(Â·|s) = Ï€â€²(Â·|s) + Î¸(Ï€â€²(Â·|s) âˆ’Ï€(Â·|s))}.
(2)
Then there exists a policy ËœÏ€ âˆˆâˆ‚Ï€BH
Ïµ (Ï€), such that ËœÏ€ is the optimal policy adversary w.r.t. Ï€."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0728862973760933,"Theorem 4 is proven in Appendix B.3, and we visualize the outermost boundary in Appendix B.5."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07434402332361516,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07580174927113703,"4
PA-AD: OPTIMAL AND EFFICIENT EVASION ATTACK"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07725947521865889,"In this section, we ï¬rst formally deï¬ne the optimality of an attack algorithm and discuss some exist-
ing attack methods. Then, based on the theoretical insights in Section 3, we introduce our algorithm,
Policy Adversarial Actor Director (PA-AD) that has an optimal formulation and is efï¬cient to use."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07871720116618076,"Although many attack methods for RL agents have been proposed (Huang et al., 2017; Pattanaik
et al., 2018; Zhang et al., 2020a), it is not yet well-understood how to characterize the strength and
the optimality of an attack method. Therefore, we propose to formulate the optimality of an attack
algorithm, which answers the question â€œwhether the attack objective ï¬nds the strongest adversaryâ€."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08017492711370262,"Deï¬nition 5 (Optimal Formulation of Attacking Algorithm). An attacking algorithm Algo is said to
have an optimal formulation iff for any MDP M, policy Ï€ and admissible adversary set HÏµ under
attacking budget Ïµ, the set of optimal solutions to its objective, HAlgo
Ïµ
, is a subset of the optimal
adversaries against Ï€, i.e., HAlgo
Ïµ
âŠ†Hâˆ—
Ïµ := {hâˆ—|hâˆ—âˆˆargminhâˆˆHÏµV Ï€h(s), âˆ€s âˆˆS}."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08163265306122448,"Many heuristic-based attacks, although are empirically effective and efï¬cient, do not meet the re-
quirements of optimal formulation. In Appendix D.3, we categorize existing heuristic attack meth-
ods into four types, and theoretically prove that there exist scenarios where these heuristic methods
may not ï¬nd the strongest adversary. A recent paper (Zhang et al., 2021) proposes to learn the opti-
mal state adversary using RL methods, which we will refer to as SA-RL in our paper for simplicity.
SA-RL can be viewed as an â€œend-to-endâ€ RL attacker, as it directly learns the optimal state adversary
such that the value of the victim policy is minimized. The formulation of SA-RL satisï¬es Deï¬ni-
tion 5 and thus is optimal. However, SA-RL learns an MDP whose state space and action space
are both the same as the original state space. If the original state space is high-dimensional (e.g.
images), learning a good policy in the adversaryâ€™s MDP may become computationally intractable,
as empirically shown in Section 6."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08309037900874636,"Can we address the optimal attacking problem in an efï¬cient manner? SA-RL treats the victim and
the environment together as a black box and directly learns a state adversary. But if the victim policy
is known to the attacker (e.g. in adversarial training), we can exploit the victim model and simplify
the attacking problem while maintaining the optimality. Therefore, we propose a novel algorithm,
Policy Adversarial Actor Director (PA-AD), that has optimal formulation and is generally more ef-
ï¬cient than SA-RL. PA-AD decouples the whole attacking process into two simpler components:
policy perturbation and state perturbation, solved by a â€œdirectorâ€ and an â€œactorâ€ through collabora-
tion. The director learns the optimal policy perturbing direction with RL methods, while the actor
crafts adversarial states at every step such that the victim policy is perturbed towards the given di-
rection. Compared to the black-box SA-RL, PA-AD is a white-box attack, but works for a broader
range of environments more efï¬ciently. Note that PA-AD can be used to conduct black-box attack
based on the transferability of adversarial attacks (Huang et al., 2017), although it is out of the scope
of this paper. Appendix F.2 provides a comprehensive comparison between PA-AD and SA-RL in
terms of complexity, optimality, assumptions and applicable scenarios."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08454810495626822,Environment Actor
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08600583090379009,Reward
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08746355685131195,"Action 
State"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08892128279883382,Policy Perturbing
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09037900874635568,Direction
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09183673469387756,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09329446064139942,"The actor's task: similar to a (targeted) evasion attack in supervised learning.
Can be solved by optimization methods (FGSM, PGD, etc)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09475218658892129,"The director's task: minimize the total reward gained from the environment.
Can be solved by RL methods (PPO, DQN, etc)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09620991253644315,Director
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09766763848396501,Our Method: Policy Adversarial Actor Director:
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09912536443148688,Optimal And Efficient
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10058309037900874,Environment State
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10204081632653061,"Reward 
Action"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10349854227405247,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10495626822157435,"State
Adversary"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10641399416909621,An End-to-end RL Attacker (SA-RL):
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10787172011661808,Optimal But Inefficient
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10932944606413994,"Environment
State"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11078717201166181,Action
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11224489795918367,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11370262390670553,"A Heuristic Attacker:
Efficient But Non-optimal "
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1151603498542274,"State
Adversary"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11661807580174927,"Figure 4: An overview of PA-AD compared with a heuristic attacker and an end-to-end RL attacker. Heuristic
attacks are efï¬cient, but may not ï¬nd the optimal adversary as they do not learn from the environment dynamics.
An end-to-end RL attacker directly learns a policy to generate state perturbations, but is inefï¬cient in large-state-
space environments. In contrast, our PA-AD solves the attack problem with a combination of an RL-based
director and a non-RL actor, so that PA-AD achieves both optimality and efï¬ciency."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11807580174927114,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.119533527696793,"Formally, for a given victim policy Ï€, our proposed PA-AD algorithm solves a Policy Adversary
MDP (PAMDP) deï¬ned in Deï¬nition 6. An actor denoted by g is embedded in the dynamics of the
PAMDP, and a director searches for an optimal policy Î½âˆ—in the PAMDP."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12099125364431487,"Deï¬nition 6 (Policy Adversary MDP (PAMDP) c
M). Given an MDP M = âŸ¨S, A, P, R, Î³âŸ©, a
ï¬xed stochastic victim policy Ï€, an attack budget Ïµ â‰¥0, we deï¬ne a Policy Adversarial MDP
c
M = âŸ¨S, b
A, bP, bR, Î³âŸ©, where the action space is b
A := {d âˆˆ[âˆ’1, 1]|A|, P|A|
i=1 di = 0}, and âˆ€s, sâ€² âˆˆ
S, âˆ€ba âˆˆb
A,
bP(sâ€²|s, ba) =
X"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12244897959183673,"aâˆˆA Ï€(a|g(ba, s))P(sâ€²|s, a),
bR(s, ba) = âˆ’
X"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1239067055393586,"aâˆˆA Ï€(a|g(ba, s))R(s, a),"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12536443148688048,where g is the actor function deï¬ned as
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12682215743440234,"g(ba, s) = argmaxËœsâˆˆBÏµ(s)âˆ¥Ï€(Ëœs) âˆ’Ï€(s)âˆ¥subject to
 
Ï€(Ëœs) âˆ’Ï€(s)
Tba = âˆ¥Ï€(Ëœs) âˆ’Ï€(s)âˆ¥âˆ¥baâˆ¥.
(G)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1282798833819242,"If the victim policy is deterministic, i.e., Ï€D := argmaxaÏ€(a|s), (subscript D stands for determin-
istic), the action space of PAMDP is b
AD :=A, and the actor function gD is"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12973760932944606,"gD(ba, s) = argmaxËœsâˆˆBÏµ(s)
 
Ï€(ba|Ëœs) âˆ’maxaâˆˆA,aÌ¸=baÏ€(a|Ëœs)

.
(GD)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13119533527696792,Detailed deï¬nition of the deterministic-victim version of PAMDP is in Appendix C.1.
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1326530612244898,"A key to PA-AD is the director-actor collaboration mechanism. The input to director policy Î½ is the
current state s in the original environment, while its output ba is a signal to the actor denoting â€œwhich
direction to perturb the victim policy intoâ€. b
A is designed to contain all â€œperturbing directionsâ€ in
the policy space. That is, âˆ€ba âˆˆbA, there exists a constant Î¸0 â‰¥0 such that âˆ€Î¸ â‰¤Î¸0, Ï€(Â·|s) + Î¸ ba"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13411078717201166,"âˆ¥baâˆ¥
belongs to the simplex âˆ†(A). The actor g takes in the state s and directorâ€™s direction ba and then
computes a state perturbation within the attack budget. Therefore, the director and the actor together
induce a state adversary: h(s) := g(Î½(s), s), âˆ€s âˆˆS. The deï¬nition of PAMDP is slightly different
for a stochastic victim policy and a deterministic victim policy, as described below.
For a stochastic victim Ï€, the directorâ€™s action ba âˆˆb
A is designed to be a unit vector lying in the
policy simplex, denoting the perturbing direction in the policy space. The actor, once receiving the
perturbing direction ba, will â€œpushâ€ the policy as far as possible by perturbing s to g(ba, s) âˆˆBÏµ(s),
as characterized by the optimization problem (G). In this way, the policy perturbation resulted by
the director and the actor is always in the outermost boundary of BH
Ïµ (Ï€) w.r.t. the victim Ï€, where
the optimal policy perturbation can be found according to Theorem 4.
For a deterministic victim Ï€D, the directorâ€™s action ba âˆˆb
AD can be viewed as a target action in the
original action space, and the actor conducts targeted attacks to let the victim execute ba, by forcing
the logit corresponding to the target action to be larger than the logits of other actions."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13556851311953352,"In both the stochastic-victim and deterministic-victim case, PA-AD has an optimal formulation as
stated in Theorem 7 (proven in Appendix D.2).
Theorem 7 (Optimality of PA-AD). For any MDP M, any ï¬xed victim policy Ï€, and any attack
budget Ïµ â‰¥0, an optimal policy Î½âˆ—in c
M induces an optimal state adversary against Ï€ in M. That
is, the formulation of PA-AD is optimal, i.e., HPA-AD âŠ†Hâˆ—
Ïµ ."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13702623906705538,Algorithm 1: Policy Adversarial Actor Director (PA-AD)
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13848396501457727,1 Input: Initialization of directorâ€™s policy Î½; victim policy Ï€; budget Ïµ; start state s0
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13994169096209913,"2 for t = 0, 1, 2, ... do"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.141399416909621,"3
Director samples a policy perturbing direction bat âˆ¼Î½(Â·|st)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14285714285714285,"4
Actor perturbs st to Ëœst = gD(bat, st) if Victim is deterministic, otherwise to Ëœst = g(bat, st)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14431486880466474,"5
Victim takes action at âˆ¼Ï€(Â·|Ëœst), proceeds to st+1, receives rt"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1457725947521866,"6
Director saves (st, bat, âˆ’rt, st+1) to its buffer"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14723032069970846,"7
Director updates its policy Î½ using any RL algorithm"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14868804664723032,"Efï¬ciency of PA-AD
As commonly known, the sample complexity and computational cost of
learning an MDP usually grow with the cardinalities of its state space and action space. Both SA-
RL and PA-AD have state space S, the state space of the original MDP. But the action space of
SA-RL is also S, while our PA-AD has action space R|A| for stochastic victim policies, or A for
deterministic victim policies. In most DRL applications, the state space (e.g., images) is much larger
than the action space, then PA-AD is generally more efï¬cient than SA-RL as it learns a smaller MDP."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15014577259475217,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15160349854227406,"The attacking procedure is illustrated in Algorithm 1. At step t, the director observes a state st, and
proposes a policy perturbation bat, then the actor searches for a state perturbation to meet the policy
perturbation. Afterwards, the victim acts with the perturbed state Ëœst, then the director updates its
policy based on the opposite value of the victimâ€™s reward. Note that the actor solves a constrained
optimization problem, (GD) or (G). Problem (GD) is similar to a targeted attack in supervised
learning, while the stochastic version (G) can be approximately solved with a Lagrangian relaxation.
In Appendix C.2, we provide our implementation details for solving the actorâ€™s optimization, which
empirically achieves state-of-the-art attack performance as veriï¬ed in Section 6."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15306122448979592,"Extending to Continuous Action Space
Our PA-AD can be extended to environments with con-
tinuous action spaces, where the actor minimizes the distance between the policy action and the
target action, i.e., argminsâ€²âˆˆBÏµ(s)âˆ¥Ï€(sâ€²) âˆ’baâˆ¥. More details and formal deï¬nitions of the variant of
PA-AD in continuous action space are provided in Appendix C.3. In Section 6, we show experimen-
tal results in MuJoCo tasks, which have continuous action spaces."
RELATED WORK,0.15451895043731778,"5
RELATED WORK"
RELATED WORK,0.15597667638483964,"Heuristic-based Evasion Attacks on States
There are many works considering evasion attacks
on the state observations in RL. Huang et al. (2017) ï¬rst propose to use FGSM (Goodfellow et al.,
2015) to craft adversarial states such that the probability that the agent selects the â€œbestâ€ action is
minimized. The same objective is also used in a recent work by Korkmaz (2020), which adopts a
Nesterov momentum-based optimization method to further improve the attack performance. Pat-
tanaik et al. (2018) propose to lead the agent to select the â€œworstâ€ action based on the victimâ€™s Q
function and use gradient descent to craft state perturbations. Zhang et al. (2020a) deï¬ne the con-
cept of a state-adversarial MDP (SAMDP) and propose two attack methods: Robust SARSA and
Maximal Action Difference. The above heuristic-based methods are shown to be effective in many
environments, although might not ï¬nd the optimal adversaries, as proven in Appendix D.3."
RELATED WORK,0.15743440233236153,"RL-based Evasion Attacks on States
As discussed in Section 4, SA-RL (Zhang et al., 2021)
uses an end-to-end RL formulation to learn the optimal state adversary, which achieves state-of-
the-art attacking performance in MuJoCo tasks. For a pixel state space, an end-to-end RL attacker
may not work as shown by our experiment in Atari games (Section 6). Russo & Proutiere (2021)
propose to use feature extraction to convert the pixel state space to a small state space and then learn
an end-to-end RL attacker. But such feature extractions require expert knowledge and can be hard
to obtain in many real-world applications. In contrast, our PA-AD works for both pixel and vector
state spaces and does not require expert knowledge."
RELATED WORK,0.1588921282798834,"Other Works Related to Adversarial RL
There are many other papers studying adversarial RL
from different perspectives, including limited-steps attacking (Lin et al., 2017; Kos & Song, 2017),
multi-agent scenarios (Gleave et al., 2020), limited access to data (Inkawhich et al., 2020), and
etc. Adversarial action attacks (Xiao et al., 2019; Tan et al., 2020; Tessler et al., 2019; Lee et al.,
2021) are developed separately from state attacks; although we mainly consider state adversaries,
our PA-AD can be extended to action attacks as formulated in Appendix A. Poisoning (Behzadan
& Munir, 2017; Huang & Zhu, 2019; Sun et al., 2021; Zhang et al., 2020b; Rakhsha et al., 2020) is
another type of adversarial attacks that manipulates the training data, different from evasion attacks
that deprave a well-trained policy. Training a robust agent is the focus of many recent works (Pinto
et al., 2017; Fischer et al., 2019; LÂ¨utjens et al., 2020; Oikarinen et al., 2020; Zhang et al., 2020a;
2021). Although our main goal is to ï¬nd a strong attacker, we also show by experiments that our
proposed attack method signiï¬cantly improves the robustness of RL agents by adversarial training."
EXPERIMENTS,0.16034985422740525,"6
EXPERIMENTS"
EXPERIMENTS,0.1618075801749271,"In this section, we show that PA-AD produces stronger evasion attacks than state-of-the-art attack
algorithms on various OpenAI Gym environments, including Atari and MuJoCo tasks. Also, our
experiment justiï¬es that PA-AD can evaluate and improve the robustness of RL agents."
EXPERIMENTS,0.16326530612244897,"Baselines and Performance Metric
We compare our proposed attack algorithm with existing
evasion attack methods, including MinBest (Huang et al., 2017) which minimizes the probability that
the agent chooses the â€œbestâ€ action, MinBest +Momentum (Korkmaz, 2020) which uses Nesterov
momentum to improve the performance of MinBest, MinQ (Pattanaik et al., 2018) which leads"
EXPERIMENTS,0.16472303206997085,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1661807580174927,"Environment
Natural
Reward
Ïµ
Random
MinBest
MinBest +
Momentum
MinQ
MaxDiff
SA-RL
PA-AD
(ours) DQN"
EXPERIMENTS,0.16763848396501457,"Boxing
96 Â± 4
0.001
95 Â± 4
53 Â± 16
52 Â± 18
88 Â± 7
95 Â± 5
94 Â± 6
19 Â± 11"
EXPERIMENTS,0.16909620991253643,"Pong
21 Â± 0
0.0002
21 Â± 0
âˆ’10 Â± 4
âˆ’14 Â± 2
14 Â± 3
15 Â± 4
20 Â± 1
âˆ’21 Â± 0"
EXPERIMENTS,0.17055393586005832,"RoadRunner 46278 Â± 4447 0.0005 44725 Â± 6614 17012 Â± 6243 15823 Â± 5252 5765 Â± 12331 36074 Â± 6544 43615 Â± 7183
0 Â± 0"
EXPERIMENTS,0.17201166180758018,"Freeway
34 Â± 1
0.0003
34 Â± 1
12 Â± 1
12 Â± 1
15 Â± 2
22 Â± 3
34 Â± 1
9 Â± 1"
EXPERIMENTS,0.17346938775510204,"Seaquest
10650 Â± 2716 0.0005 8177 Â± 2962 3820 Â± 1947
2337 Â± 862
6468 Â± 2493 5718 Â± 1884 8152 Â± 3113 2304 Â± 838"
EXPERIMENTS,0.1749271137026239,"Alien
1623 Â± 252 0.00075 1650 Â± 381
819 Â± 486
775 Â± 648
938 Â± 446
869 Â± 279
1693 Â± 439 256 Â± 210"
EXPERIMENTS,0.17638483965014579,"Tutankham
227 Â± 29
0.00075
221 Â± 65
30 Â± 13
26 Â± 16
88 Â± 74
130 Â± 48
202 Â± 65
0 Â± 0 A2C"
EXPERIMENTS,0.17784256559766765,"Breakout
356 Â± 79
0.0005
355 Â± 79
86 Â± 104
74 Â± 95
N/A
304 Â± 111
353 Â± 79
44 Â± 62"
EXPERIMENTS,0.1793002915451895,"Seaquest
1752 Â± 70
0.005
1752 Â± 73
356 Â± 153
179 Â± 83
N/A
46 Â± 52
1752 Â± 71
4 Â± 13"
EXPERIMENTS,0.18075801749271136,"Pong
20 Â± 1
0.0005
20 Â± 1
âˆ’4 Â± 8
âˆ’11 Â± 7
N/A
18 Â± 3
20 Â± 1
âˆ’13 Â± 6"
EXPERIMENTS,0.18221574344023322,"Alien
1615 Â± 601
0.001
1629 Â± 592 1062 Â± 610
940 Â± 565
N/A
1482 Â± 633 1661 Â± 625 507 Â± 278"
EXPERIMENTS,0.1836734693877551,"Tutankham
258 Â± 53
0.001
260 Â± 54
139 Â± 26
134 Â± 28
N/A
196 Â± 34
260 Â± 54
71 Â± 47"
EXPERIMENTS,0.18513119533527697,"RoadRunner 34367 Â± 6355 0.002 35851 Â± 6675 9198 Â± 3814 5410 Â± 3058
N/A
31856 Â± 7125 36550 Â± 6848 2773 Â± 3468"
EXPERIMENTS,0.18658892128279883,"Table 1: Average episode rewards Â± standard deviation of vanilla DQN and A2C agents under different evasion
attack methods in Atari environments. Results are averaged over 1000 episodes. Note that RS works for
continuous action spaces, thus is not included. MinQ is not applicable to A2C which does not have a Q
network. In each row, we bold the strongest (best) attack performance over all attacking methods."
EXPERIMENTS,0.1880466472303207,"the agent to select actions with the lowest action values based on the agentâ€™s Q network, Robust
SARSA (RS) (Zhang et al., 2020a) which performs the MinQ attack with a learned stable Q network,
MaxDiff (Zhang et al., 2020a) which maximizes the KL-divergence between the original victim
policy and the perturbed policy, as well as SA-RL (Zhang et al., 2021) which directly learns the state
adversary with RL methods. We consider state attacks with â„“âˆnorm as in most literature (Zhang
et al., 2020a; 2021). Appendix E.1 provides hyperparameter settings and implementation details."
EXPERIMENTS,0.18950437317784258,"PA-AD Finds the Strongest Adversaries in Atari Games
We ï¬rst evaluate the performance of
PA-AD against well-trained DQN (Mnih et al., 2015) and A2C (Mnih et al., 2016) victim agents
on Atari games with pixel state spaces. The observed pixel values are normalized to the range of
[0, 1]. SA-RL and PA-AD adversaries are learned using the ACKTR algorithm (Wu et al., 2017)
with the same number of steps. (Appendix E.1 shows hyperparameter settings.) Table 1 presents
the experiment results, where PA-AD signiï¬cantly outperforms all baselines against both DQN and
A2C victims. In contrast, SA-RL does not converge to a good adversary in the tested Atari games
with the same number of training steps as PA-AD, implying the importance of sample efï¬ciency.
Surprisingly, using a relatively small attack budget Ïµ, PA-AD leads the agent to the lowest possible
reward in many environments such as Pong, RoadRunner and Tutankham, whereas other attackers
may require larger attack budget to achieve the same attack strength. Therefore, we point out that
vanilla RL agents are extremely vulnerable to carefully learned adversarial attacks. Even if an RL
agent works well under naive attacks, a carefully learned adversary can let an agent totally fail with
the same attack budget, which stresses the importance of evaluating and improving the robustness
of RL agents using the strongest adversaries. Our further investigation in Appendix F.3 shows that
RL models can be generally more vulnerable than supervised classiï¬ers, due to the different loss
and architecture designs. In Appendix E.2.1, we show more experiments with various selections of
the budget Ïµ, where one can see PA-AD reduces the average reward more than all baselines over
varying Ïµâ€™s in various environments."
EXPERIMENTS,0.19096209912536444,"PA-AD Finds the Strongest Adversaries MuJoCo Tasks
We further evaluate PA-AD on Mu-
JoCo games, where both state spaces and action spaces are continuous. We use the same setting with
Zhang et al. (2021), where both the victim and the adversary are trained with PPO (Schulman et al.,
2017). During test time, the victim executes a deterministic policy, and we use the deterministic"
EXPERIMENTS,0.1924198250728863,"Environment
State
Dimension
Natural
Reward
Ïµ
Random
MaxDiff
RS
SA-RL
PA-AD
(ours)"
EXPERIMENTS,0.19387755102040816,"Hopper
11
3167 Â± 542
0.075
2101 Â± 793
1410 Â± 655
794 Â± 238
636 Â± 9
160 Â± 136"
EXPERIMENTS,0.19533527696793002,"Walker
17
4472 Â± 635
0.05
3007 Â± 1200
2869 Â± 1271
1336 Â± 654
1086 Â± 516
804 Â± 130"
EXPERIMENTS,0.1967930029154519,"HalfCheetah
17
7117 Â± 98
0.15
5486 Â± 1378
1836 Â± 866
489 Â± 758
âˆ’660 Â± 218
âˆ’356 Â± 307"
EXPERIMENTS,0.19825072886297376,"Ant
111
5687 Â± 758
0.15
5261 Â± 1005
1759 Â± 828
268 Â± 227
âˆ’872 Â± 436
âˆ’2580 Â± 872"
EXPERIMENTS,0.19970845481049562,"Table 2: Average episode rewards Â± standard deviation of vanilla PPO agent under different evasion attack
methods in MuJoCo environments. Results are averaged over 50 episodes. Note that MinBest and MinQ do
not ï¬t this setting, since MinBest works for discrete action spaces, and MinQ requires the agentâ€™s Q network."
EXPERIMENTS,0.20116618075801748,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.20262390670553937,"version of PA-AD with a continuous action space, as discussed in Section 4 and Appendix C.3. We
use the same attack budget Ïµ as in Zhang et al. (2021) for all MuJoCo environments. Results in Ta-
ble 2 show that PA-AD reduces the reward much more than heuristic methods, and also outperforms
SA-RL in most cases. In Ant, our PA-AD achieves much stronger attacks than SA-RL, since PA-AD
is more efï¬cient than SA-RL when the state space is large. Admittedly, PA-AD requires additional
knowledge of the victim model, while SA-RL works in a black-box setting. Therefore, SA-RL is
more applicable to black-box scenarios with a relatively small state space, whereas PA-AD is more
applicable when the attacker has access to the victim (e.g. in adversarial training as shown in Ta-
ble 3). Appendix E.2.3 provides more empirical comparison between SA-RL and PA-AD, which
shows that PA-AD converges faster, takes less running time, and is less sensitive to hyperparameters
than SA-RL by a proper exploitation of the victim model."
EXPERIMENTS,0.20408163265306123,"Environment
Model
Natural
Reward
Random
MaxDiff
RS
SA-RL
PA-AD
(ours)
Average reward
across attacks"
EXPERIMENTS,0.2055393586005831,"Hopper
(state-dim: 11)
Ïµ: 0.075"
EXPERIMENTS,0.20699708454810495,"SA-PPO
3705 Â± 2
2710 Â± 801
2652 Â± 835
1130 Â± 42
1076 Â± 791
856 Â± 21
1684.8"
EXPERIMENTS,0.20845481049562684,"ATLA-PPO
3291 Â± 600 3165 Â± 576
2814 Â± 725
2244 Â± 618
1772 Â± 802
1232 Â± 350
2245.4"
EXPERIMENTS,0.2099125364431487,"PA-ATLA-PPO (ours) 3449 Â± 237 3325 Â± 239
3145 Â± 546
3002 Â± 129
1529 Â± 284
2521 Â± 325
2704.4"
EXPERIMENTS,0.21137026239067055,"Walker
(state-dim: 17)
Ïµ: 0.05"
EXPERIMENTS,0.21282798833819241,"SA-PPO
4487 Â± 61
4867 Â± 39
3668 Â± 1789
3808 Â± 138
2908 Â± 1136
1042 Â± 153
3258.6"
EXPERIMENTS,0.21428571428571427,"ATLA-PPO
3842 Â± 475 3927 Â± 368
3836 Â± 492
3239 Â± 894
3663 Â± 707
1224 Â± 770
3177.8"
EXPERIMENTS,0.21574344023323616,"PA-ATLA-PPO (ours) 4178 Â± 529
4129 Â± 78
4024 Â± 572
3966 Â± 307
3450 Â± 478
2248 Â± 131
3563.4"
EXPERIMENTS,0.21720116618075802,"Halfcheetah
(state-dim: 17)
Ïµ: 0.15"
EXPERIMENTS,0.21865889212827988,"SA-PPO
3632 Â± 20
3619 Â± 18
3624 Â± 23
3283 Â± 20
3028 Â± 23
2512 Â± 16
3213.2"
EXPERIMENTS,0.22011661807580174,"ATLA-PPO
6157 Â± 852 6164 Â± 603
5790 Â± 174
4806 Â± 603
5058 Â± 718
2576 Â± 1548
4878.8"
EXPERIMENTS,0.22157434402332363,"PA-ATLA-PPO (ours) 6289 Â± 342 6215 Â± 346
5961 Â± 53
5226 Â± 114
4872 Â± 79
3840 Â± 673
5222.8"
EXPERIMENTS,0.2230320699708455,"Ant
(state-dim: 111)
Ïµ: 0.15"
EXPERIMENTS,0.22448979591836735,"SA-PPO
4292 Â± 384 4986 Â± 452
4662 Â± 522
3412 Â± 1755 2511 Â± 1117 âˆ’1296 Â± 923
2855.0"
EXPERIMENTS,0.2259475218658892,"ATLA-PPO
5359 Â± 153 5366 Â± 104
5240 Â± 170
4136 Â± 149
3765 Â± 101
220 Â± 338
3745.4"
EXPERIMENTS,0.22740524781341107,"PA-ATLA-PPO (ours) 5469 Â± 106 5496 Â± 158
5328 Â± 196
4124 Â± 291
3694 Â± 188
2986 Â± 864
4325.6"
EXPERIMENTS,0.22886297376093295,"Table 3: Average episode rewards Â± standard deviation of robustly trained PPO agents under different attack
methods. Results are averaged over 50 episodes. In each row corresponding to a robust agent, we bold the
strongest attack. The gray cells are the most robust agents with the highest average rewards across attacks.
Our PA-AD achieves the strongest attack against robust models, and our PA-ATLA-PPO achieves the most
robust performance under multiple attacks. The attack budget Ïµâ€™s are the same as in Zhang et al. (2021)."
EXPERIMENTS,0.2303206997084548,"Training and Evaluating Robust Agents
A natural application of PA-AD is to evaluate the ro-
bustness of a known model, or to improve the robustness of an agent via adversarial training, where
the attacker has white-box access to the victim. Inspired by ATLA (Zhang et al., 2021) which al-
ternately trains an agent and an SA-RL attacker, we propose PA-ATLA, which alternately trains an
agent and a PA-AD attacker. In Table 3, we evaluate the performance of PA-ATLA for a PPO agent
(namely PA-ATLA-PPO) in MuJoCo tasks, compared with state-of-the-art robust training methods,
SA-PPO (Zhang et al., 2020a) and ATLA-PPO (Zhang et al., 2021) 2. From the table, we make
the following observations. (1) Our PA-AD attacker can signiï¬cantly reduce the reward of previ-
ous â€œrobustâ€ agents. Take the Ant environment as an example, although SA-PPO and ATLA-PPO
agents gain 2k+ and 3k+ rewards respectively under SA-RL, the previously strongest attack, our PA-
AD still reduces their rewards to about -1.3k and 200+ with the same attack budget. Therefore, we
emphasize the importance of understanding the worst-case performance of RL agents, even robustly-
trained agents. (2) Our PA-ATLA-PPO robust agents gain noticeably higher average rewards across
attacks than other robust agents, especially under the strongest PA-AD attack. Under the SA-RL
attack, PA-ATLA-PPO achieves comparable performance with ATLA-PPO, although ATLA-PPO
agents are trained to be robust against SA-RL. Due to the efï¬ciency of PA-AD, PA-ATLA-PPO
requires fewer training steps than ATLA-PPO, as justiï¬ed in Appendix E.2.4. The results of attack-
ing and training robust models in Atari games are in Appendix E.2.5 and E.2.6, where PA-ATLA
improves the robustness of Atari agents against strong attacks with Ïµ as large as 3/255."
CONCLUSION,0.23177842565597667,"7
CONCLUSION"
CONCLUSION,0.23323615160349853,"In this paper, we propose an attack algorithm called PA-AD for RL problems, which achieves opti-
mal attacks in theory and signiï¬cantly outperforms prior attack methods in experiments. PA-AD can
be used to evaluate and improve the robustness of RL agents before deployment. A potential future
direction is to use our formulation for robustifying agents under both state and action attacks."
CONCLUSION,0.23469387755102042,"2We use ATLA-PPO(LSTM)+SA Reg, the most robust method reported by Zhang et al. (2021)."
CONCLUSION,0.23615160349854228,Published as a conference paper at ICLR 2022
CONCLUSION,0.23760932944606414,ACKNOWLEDGMENTS
CONCLUSION,0.239067055393586,"This work is supported by National Science Foundation IIS-1850220 CRII Award 030742-00001
and DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness
against Deception (GARD), and Adobe, Capital One and JP Morgan faculty fellowships."
ETHICS STATEMENT,0.24052478134110788,ETHICS STATEMENT
ETHICS STATEMENT,0.24198250728862974,"Despite the rapid advancement of interactive AI and ML systems using RL agents, the learning agent
could fail catastrophically in the presence of adversarial attacks, exposing a serious vulnerability
in current RL systems such as autonomous driving systems, market-making systems, and security
monitoring systems. Therefore, there is an urgent need to understand the vulnerability of an RL
model, otherwise, it may be risky to deploy a trained agent in real-life applications, where the
observations of a sensor usually contain unavoidable noise."
ETHICS STATEMENT,0.2434402332361516,"Although the study of a strong attack method may be maliciously exploited to attack some RL
systems, it is more important for the owners and users of RL systems to get aware of the vulnerability
of their RL agents under the strongest possible adversary. As the old saying goes, â€œif you know
yourself and your enemy, youâ€™ll never lose a battleâ€. In this work, we propose an optimal and
efï¬cient algorithm for evasion attacks in Deep RL (DRL), which can signiï¬cantly inï¬‚uence the
performance of a well-trained DRL agent, by adding small perturbations to the state observations of
the agent. Our proposed method can automatically measure the vulnerability of an RL agent, and
discover the â€œï¬‚awâ€ in a model that might be maliciously attacked. We also show in experiments
that our attack method can be applied to improve the robustness of an RL agent via robust training.
Since our proposed attack method achieves state-of-the-art performance, the RL agent trained under
our proposed attacker could be able to â€œdefendâ€ against any other adversarial attacks with the same
constraints. Therefore, our work has the potential to help combat the threat to high-stakes systems."
ETHICS STATEMENT,0.24489795918367346,"A limitation of PA-AD is that it requires the â€œattackerâ€ to know the victimâ€™s policy, i.e., PA-AD
is a white-box attack. If the attacker does not have full access to the victim, PA-AD can still be
used based on the transferability of adversarial attacks (Huang et al., 2017), although the optimal-
ity guarantee does not hold in this case. However, this limitation only restricts the ability of the
malicious attackers. In contrast, PA-AD should be used when one wants to evaluate the worst-case
performance of oneâ€™s own RL agent, or to improve the robustness of an agent under any attacks,
since PA-AD produces strong attacks efï¬ciently. In these cases, PA-AD does have white-box access
to the agent. Therefore, PA-AD is more beneï¬cial to defenders than attackers."
REPRODUCIBILITY STATEMENT,0.24635568513119532,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2478134110787172,"For theoretical results, we provide all detailed technical proofs and lemmas in Appendix. In Ap-
pendix A, we analyze the equivalence between evasion attacks and policy perturbations. In Ap-
pendix B, we theoretically prove some topological properties of the proposed Adv-policy-set, and
derive Theorem 4 that the outermost boundary of BH
Ïµ (Ï€) always contains an optimal policy per-
turbation. In Appendix D, we systematically characterize the optimality of many existing attack
methods. We theoretically show (1) the existence of an optimal adversary, (2) the optimality of our
proposed PA-AD, and (3) the optimality of many heuristic attacks, following our Deï¬nition 5 in
Section 4.
For experimental results, the detailed algorithm description in various types of environments is
provided in Appendix C. In Appendix E, we illustrate the implementation details, environment set-
tings, hyperparameter settings of our experiments. Additional experimental results show the per-
formance of our algorithm from multiple aspects, including hyperparameter sensitivity, learning
efï¬ciency, etc. In addition, in Appendix F, we provide some detailed discussion on the algorithm
design, as well as a comprehensive comparison between our method and prior works.
The source code and running instructions for both Atari and MuJoCo experiments are in our sup-
plementary materials. We also provide trained victim and attacker models so that one can directly
test their performance using a test script we provide."
REPRODUCIBILITY STATEMENT,0.24927113702623907,Published as a conference paper at ICLR 2022
REFERENCES,0.25072886297376096,REFERENCES
REFERENCES,0.2521865889212828,"Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recogni-
tion, pp. 262â€“275. Springer, 2017."
REFERENCES,0.2536443148688047,"Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G. Bellemare. The
value function polytope in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhut-
dinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 1486â€“1495, Long Beach, California, USA,
09â€“15 Jun 2019. PMLR."
REFERENCES,0.25510204081632654,"Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019."
REFERENCES,0.2565597667638484,"Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adver-
sarial policies: Attacking deep reinforcement learning. In International Conference on Learning
Representations, 2020."
REFERENCES,0.25801749271137026,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015."
REFERENCES,0.2594752186588921,"David Silver Hado Van Hasselt, Arthur Guez. Deep reinforcement learning with double q-learning.
In Thirtieth AAAI Conference on Artiï¬cial Intelligence, 2016."
REFERENCES,0.260932944606414,"Ashley Hill, Antonin Rafï¬n, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Rad-
ford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.
com/hill-a/stable-baselines, 2018."
REFERENCES,0.26239067055393583,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.26384839650145775,"Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217â€“237. Springer, 2019."
REFERENCES,0.2653061224489796,"Matthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning.
In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Sys-
tems, AAMAS â€™20, pp. 557â€“565, Richland, SC, 2020. International Foundation for Autonomous
Agents and Multiagent Systems. ISBN 9781450375184."
REFERENCES,0.26676384839650147,"Ezgi Korkmaz.
Nesterov momentum adversarial perturbations in the deep reinforcement learn-
ing domain. In ICML 2020 Inductive Biases, Invariances and Generalization in Reinforcement
Learning Workshop, 2020."
REFERENCES,0.26822157434402333,"Jernej Kos and Dawn Song.
Delving into adversarial attacks on deep policies.
arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.2696793002915452,"Ilya Kostrikov.
Pytorch implementations of reinforcement learning algorithms.
https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018."
REFERENCES,0.27113702623906705,"Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, and Soumik Sarkar. Query-based targeted
action-space adversarial policies on deep reinforcement learning agents. In Proceedings of the
ACM/IEEE 12th International Conference on Cyber-Physical Systems, ICCPS â€™21, pp. 87â€“97,
New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383530."
REFERENCES,0.2725947521865889,"Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th
International Joint Conference on Artiï¬cial Intelligence, IJCAIâ€™17, pp. 3756â€“3762. AAAI Press,
2017. ISBN 9780999241103."
REFERENCES,0.27405247813411077,"BjÂ¨orn LÂ¨utjens, Michael Everett, and Jonathan P How. Certiï¬ed adversarial robustness for deep
reinforcement learning. In Conference on Robot Learning, pp. 1328â€“1337. PMLR, 2020."
REFERENCES,0.2755102040816326,Published as a conference paper at ICLR 2022
REFERENCES,0.27696793002915454,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.2784256559766764,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015."
REFERENCES,0.27988338192419826,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928â€“1937, 2016."
REFERENCES,0.2813411078717201,"Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020."
REFERENCES,0.282798833819242,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS â€™18, pp. 2040â€“2042, Rich-
land, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems."
REFERENCES,0.28425655976676384,"Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2817â€“2826. JMLR. org, 2017."
REFERENCES,0.2857142857142857,"Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779."
REFERENCES,0.28717201166180756,"Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
International Conference on Machine Learning, pp. 7974â€“7984, 2020."
REFERENCES,0.2886297376093295,"Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies. In
American Control Conference (ACC)., 2021."
REFERENCES,0.29008746355685133,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.2915451895043732,"Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl
with unknown dynamics. In International Conference on Learning Representations, 2021."
REFERENCES,0.29300291545189505,"Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying reinforce-
ment learning agents via action space adversarial training. In 2020 American control conference
(ACC), pp. 3959â€“3964. IEEE, 2020."
REFERENCES,0.2944606413994169,"Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and appli-
cations in continuous control. In International Conference on Machine Learning, pp. 6215â€“6224.
PMLR, 2019."
REFERENCES,0.29591836734693877,"Ioannis Antonoglou Tom Schaul, John Quan and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016."
REFERENCES,0.29737609329446063,"Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5279â€“5288, 2017."
REFERENCES,0.2988338192419825,"Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Bo Li,
and Dawn Song.
Characterizing attacks on deep reinforcement learning.
arXiv preprint
arXiv:1907.09470, 2019."
REFERENCES,0.30029154518950435,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state observations. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 21024â€“21037. Curran Associates, Inc., 2020a."
REFERENCES,0.30174927113702626,Published as a conference paper at ICLR 2022
REFERENCES,0.3032069970845481,"Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2021."
REFERENCES,0.30466472303207,"Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, 2020b."
REFERENCES,0.30612244897959184,Published as a conference paper at ICLR 2022
REFERENCES,0.3075801749271137,"Appendix: Who Is the Strongest Enemy? Towards Optimal and Efï¬-
cient Evasion Attacks in Deep RL"
REFERENCES,0.30903790087463556,"A
RELATIONSHIP BETWEEN EVASION ATTACKS AND POLICY
PERTURBATIONS."
REFERENCES,0.3104956268221574,"As mentioned in Section 2, all evasion attacks can be regarded as perturbations in the policy space.
To be more speciï¬c, we consider the following 3 cases, where we assume the victim uses policy Ï€."
REFERENCES,0.3119533527696793,Case 1 (attack on states): deï¬ne the state adversary as function h such that âˆ€s âˆˆS
REFERENCES,0.31341107871720114,h(s) = Ëœs âˆˆBÏµ(s) := {sâ€² âˆˆS : âˆ¥sâ€² âˆ’sâˆ¥â‰¤Ïµ}.
REFERENCES,0.31486880466472306,"(For simplicity, we consider the attacks within a Ïµ-radius norm ball.)
In this case, for all s âˆˆS, the victim samples action from Ï€h(Â·|s) = Ï€(Â·|h(s)) = Ï€(Ëœs), which is
equivalent to the victim executing a perturbed policy Ï€h âˆˆÎ ."
REFERENCES,0.3163265306122449,"Case 2 (attack on actions for a deterministic Ï€): deï¬ne the action adversary as function h(A) :
S Ã— A â†’A, and âˆ€s âˆˆS, a âˆˆA"
REFERENCES,0.3177842565597668,h(A)(a|s) = Ëœa âˆˆBÏµ(a) := {aâ€² âˆˆA : âˆ¥aâ€² âˆ’aâˆ¥â‰¤Ïµ}.
REFERENCES,0.31924198250728864,"In this case, there exists a policy Ï€h(A) such that Ï€h(A)(s) = h(A)(a|s) = Ëœa, which is equivalent to
the victim executing policy Ï€h(A) âˆˆÎ ."
REFERENCES,0.3206997084548105,"Case 3 (attack on actions for a stochastic Ï€): deï¬ne the action adversary as function h(A) : SÃ—A â†’
A, and âˆ€s âˆˆS, a âˆˆA"
REFERENCES,0.32215743440233235,"h(A)(a|s) = Ëœa such that {âˆ¥Ï€(Â·|s) âˆ’Pr(Â·|s)âˆ¥â‰¤Ïµ},"
REFERENCES,0.3236151603498542,"where Pr(Ëœa|s) denotes the probability that the action is perturbed into Ëœa.
In this case, there exists a policy Ï€h(A) such that Ï€h(A)(s) = Pr(Â·|s), which is equivalent to the
victim executing policy Ï€h(A) âˆˆÎ ."
REFERENCES,0.3250728862973761,"Most existing evasion RL works (Huang et al., 2017; Pattanaik et al., 2018; Zhang et al., 2020a;
2021) focus on state attacks, while there are also some works (Tessler et al., 2019; Tan et al., 2020)
studying action attacks. For example, Tessler et al. (Tessler et al., 2019) consider Case 2 and Case 3
above and train an agent that is robust to action perturbations."
REFERENCES,0.32653061224489793,"These prior works study either state attacks or action attacks, considering them in two different
scenarios. However, the ultimate goal of robust RL is to train an RL agent that is robust to any threat
models. Otherwise, an agent that is robust against state attacks may still be ruined by an action
attacker. We take a step further to this ultimate goal by proposing a framework, policy attack, that
uniï¬es observation attacks and action attacks."
REFERENCES,0.32798833819241985,"Although the focus of this paper is on state attacks, we would like to point out that our proposed
method can also deal with action attacks (the director proposes a policy perturbation direction, and
an actor perturbs the action accordingly). It is also an exciting direction to explore hybrid attacks
(multiple actors conducting states perturbations and action perturbations altogether, directed by a
single director.) Our policy perturbation framework can also be easily incorporated in robust training
procedures, as an agent that is robust to policy perturbations is simultaneously robust to both state
attacks and action attacks."
REFERENCES,0.3294460641399417,"B
TOPOLOGICAL PROPERTIES OF THE ADMISSIBLE ADVERSARIAL POLICY
SET"
REFERENCES,0.33090379008746357,"As discussed in Section 3, ï¬nding the optimal state adversary in the admissible adversary set HÏµ
can be converted to a problem of ï¬nding the optimal policy adversary in the Adv-policy-set BH
Ïµ (Ï€).
In this section, we characterize the topological properties of BH
Ïµ (Ï€), and identify how the value
function changes as the policy changes within BH
Ïµ (Ï€)."
REFERENCES,0.3323615160349854,"In Section B.1, we show that under the settings we consider, BH
Ïµ (Ï€) is a connected and compact
subset of Î . Then, Section B.2, we deï¬ne some additional concepts and re-formulate the notations."
REFERENCES,0.3338192419825073,Published as a conference paper at ICLR 2022
REFERENCES,0.33527696793002915,"In Section B.3, we prove Theorem 4 in Section 3 that the outermost boundary of BH
Ïµ (Ï€) always
contains an optimal policy perturbation. In Section B.4, we prove that the value functions of policies
in BH
Ïµ (Ï€) (or more generally, any connected and compact subset of Î ) form a polytope. Section B.6
shows an example of the polytope result with a 2-state MDP, and Section B.5 shows examples of the
outermost boundary deï¬ned in Theorem 4."
REFERENCES,0.336734693877551,"B.1
THE SHAPE OF ADV-POLICY-SET BH
Ïµ (Ï€)"
REFERENCES,0.33819241982507287,"It is important to note that BH
Ïµ (Ï€) is generally connected and compact as stated in the following
lemma."
REFERENCES,0.3396501457725947,"Lemma 8 (BH
Ïµ (Ï€) is connected and compact). Given an MDP M, a policy Ï€ that is a continuous
mapping, and admissible adversary set HÏµ := {h : h(s) âˆˆBÏµ(s), âˆ€s âˆˆS} (where Ïµ > 0 is a
constant), the admissible adversarial policy set BH
Ïµ (Ï€) is a connected and compact subset of Î ."
REFERENCES,0.34110787172011664,"Proof of Lemma 8. For an arbitrary state s âˆˆS, an admissible adversary h âˆˆHÏµ perturbs it within
an â„“p norm ball BÏµ(s), which is connected and compact. Since Ï€ is a continuous mapping, we know
Ï€(s) is compact and connected."
REFERENCES,0.3425655976676385,"Therefore, BH
Ïµ (Ï€) as a Cartesian product of a ï¬nite number of compact and connected sets, is com-
pact and connected."
REFERENCES,0.34402332361516036,"B.2
ADDITIONAL NOTATIONS AND DEFINITIONS FOR PROOFS"
REFERENCES,0.3454810495626822,We ï¬rst formally deï¬ne some concepts and notations.
REFERENCES,0.3469387755102041,"For a stationary and stochastic policy Ï€ : S â†’âˆ†(A), we can deï¬ne the state-to-state transition
function as
P Ï€(sâ€²|s) :=
X"
REFERENCES,0.34839650145772594,"aâˆˆA
Ï€(a|s)P(sâ€²|s, a), âˆ€s, sâ€² âˆˆS,"
REFERENCES,0.3498542274052478,and the state reward function as
REFERENCES,0.35131195335276966,"RÏ€(s) :=
X"
REFERENCES,0.35276967930029157,"aâˆˆA
Ï€(a|s)R(s, a), âˆ€s âˆˆS."
REFERENCES,0.35422740524781343,"Then the value of Ï€, denoted as V Ï€, can be computed via the Bellman equation"
REFERENCES,0.3556851311953353,V Ï€ = RÏ€ + Î³P Ï€V Ï€ = (I âˆ’Î³P Ï€)âˆ’1RÏ€.
REFERENCES,0.35714285714285715,"We further use Î si to denote the projection of Î  into the simplex of the i-th state, i.e., the space of
action distributions at state si."
REFERENCES,0.358600583090379,"Let fv : Î  â†’R|S| be a mapping that maps policies to their corresponding value functions. Let
V = fv(Î ) be the space of all value functions."
REFERENCES,0.36005830903790087,"Dadashi et al. (Dadashi et al., 2019) show that the image of fv applied to the space of policies, i.e.,
fv(Î ), form a (possibly non-convex) polytope as deï¬ned below."
REFERENCES,0.36151603498542273,"Deï¬nition 9 ((Possibly non-convex) polytope). A is called a convex polytope iff there are k âˆˆN
points x1, x2, Â· Â· Â· , xk âˆˆRn such that A = Conv(x1, Â· Â· Â· , xk). Furthermore, a (possibly non-
convex) polytope is deï¬ned as a ï¬nite union of convex polytopes."
REFERENCES,0.3629737609329446,"And a more general concept is (possibly non-convex) polyhedron, which might not be bounded."
REFERENCES,0.36443148688046645,"Deï¬nition 10 ((Possibly non-convex) polyhedron). A is called a convex polyhedron iff it is the
intersection of k âˆˆN half-spaces Ë†B1, Ë†B2, Â· Â· Â· , Ë†Bk, i.e., A = âˆ©k
i=1 Ë†Bi. Furthermore, a (possibly
non-convex) polyhedron is deï¬ned as a ï¬nite union of convex polyhedra."
REFERENCES,0.36588921282798836,"In addition, let Y Ï€
s1,Â·Â·Â· ,sk be the set of policies that agree with Ï€ on states s1, Â· Â· Â· , sk. Dadashi et
al. (Dadashi et al., 2019) also prove that the values of policies that agree on all but one state s, i.e.,
fv(Y Ï€
S\{s}), form a line segment, which can be bracketed by two policies that are deterministic on
s. Our Lemma 14 extends this line segment result to our setting where policies are restricted in a
subset of policies."
REFERENCES,0.3673469387755102,Published as a conference paper at ICLR 2022
REFERENCES,0.3688046647230321,"B.3
PROOF OF THEOREM 4: BOUNDARY CONTAINS OPTIMAL POLICY PERTURBATIONS"
REFERENCES,0.37026239067055394,"Lemma 4 in Dadashi et al. (2019) shows that policies agreeing on all but one state have certain
monotone relations. We restate this result in Lemma 11 below.
Lemma 11 (Monotone Policy Interpolation). For any Ï€0, Ï€1 âˆˆY Ï€
S\{s} that agree with Ï€ on all
states except for s âˆˆS, deï¬ne a function l : [0, 1] â†’V as"
REFERENCES,0.3717201166180758,l(Î±) = fv(Î±Ï€1 + (1 âˆ’Î±)Ï€0).
REFERENCES,0.37317784256559766,"Then we have
(1) l(0) â‰½l(1) or l(1) â‰½l(0) (â‰½stands for element-wise greater than or equal to);
(2) If l(0) = l(1), then l(Î±) = l(0), âˆ€Î± âˆˆ[0, 1];
(3) If l(0) Ì¸= l(1), then there is a strictly monotonic rational function Ï : [0, 1] â†’R, such that
l(Î±) = Ï(Î±)l(1) + (1 âˆ’Ï(Î±))l(0)."
REFERENCES,0.3746355685131195,"More intuitively, Lemma 11 suggests that the value of Ï€Î± := Î±Ï€1 + (1 âˆ’Î±)Ï€0 changes (strictly)
monotonically with Î±, unless the values of Ï€0, Ï€1 and Ï€Î± are all equal. With this result, we can
proceed to prove Theorem 4."
REFERENCES,0.3760932944606414,Proof of Theorem 4. We will prove the theorem by contradiction.
REFERENCES,0.37755102040816324,"Suppose there is a policy Ë†Ï€ âˆˆBH
Ïµ (Ï€) such that Ë†Ï€ /âˆˆâˆ‚Ï€BH
Ïµ (Ï€) and fv(Ë†Ï€) = V Ë†Ï€ < V ËœÏ€, âˆ€ËœÏ€ âˆˆBH
Ïµ (Ï€),
i.e., there is no optimal policy adversary on the outermost boundary of BH
Ïµ (Ï€)."
REFERENCES,0.37900874635568516,"Then according to the deï¬nition of âˆ‚Ï€BH
Ïµ (Ï€), there exists at least one state s âˆˆS such that we can
ï¬nd another policy Ï€â€² âˆˆBH
Ïµ (Ï€) agreeing with Ë†Ï€ on all states except for s, where Ï€â€²(s) satisï¬es"
REFERENCES,0.380466472303207,Ë†Ï€(Â·|s) = Î±Ï€(Â·|s) + (1 âˆ’Î±)Ï€â€²(Â·|s)
REFERENCES,0.3819241982507289,"for some scalar Î± âˆˆ(0, 1)."
REFERENCES,0.38338192419825073,"Then by Lemma 11, either of the following happens:"
REFERENCES,0.3848396501457726,"(1) fv(Ï€) â‰»fv(Ë†Ï€) â‰»fv(Ï€â€²).
(2) fv(Ï€) = fv(Ë†Ï€) = fv(Ï€â€²);"
REFERENCES,0.38629737609329445,"Note that fv(Ë†Ï€) â‰»fv(Ï€) is impossible because we have assumed Ë†Ï€ has the lowest value over all
policies in BH
Ïµ (Ï€) including Ï€."
REFERENCES,0.3877551020408163,"If (1) is true, then Ï€â€² is a better policy adversary than Ë†Ï€ in BH
Ïµ (Ï€), which contradicts with the
assumption."
REFERENCES,0.3892128279883382,"If (2) is true, then Ï€â€² is another optimal policy adversary. By recursively applying the above process
to Ï€â€², we can ï¬nally ï¬nd an optimal policy adversary on the outermost boundary of BH
Ïµ (Ï€), which
also contradicts with our assumption."
REFERENCES,0.39067055393586003,"In summary, there is always an optimal policy adversary lying on the outermost boundary of BH
Ïµ (Ï€)."
REFERENCES,0.39212827988338195,"B.4
PROOF OF THEOREM 12: VALUES OF POLICIES IN ADMISSIBLE ADVERSARIAL POLICY
SET FORM A POLYTOPE"
REFERENCES,0.3935860058309038,"We ï¬rst present a theorem that describes the â€œshapeâ€ of the value functions generated by all admis-
sible adversaries (admissible adversarial policies).
Theorem 12 (Policy Perturbation Polytope). For a ï¬nite MDP M, consider a policy Ï€ and an
Adv-policy-set BH
Ïµ (Ï€). The space of values (a subspace of R|S|) of all policies in BH
Ïµ (Ï€), denoted
by VBH
Ïµ (Ï€), is a (possibly non-convex) polytope."
REFERENCES,0.39504373177842567,"In the remaining of this section, we prove a more general version of Theorem 12 as below.
Theorem 13 (Policy Subset Polytope). For a ï¬nite MDP M, consider a connected and compact
subset of Î , denoted as T . The space of values (a subspace of R|S|) of all policies in T , denoted by
VT , is a (possibly non-convex) polytope."
REFERENCES,0.3965014577259475,Published as a conference paper at ICLR 2022
REFERENCES,0.3979591836734694,"According to Lemma 8, BH
Ïµ (Ï€) is a connected and compact subset of Î , thus Theorem 12 is a
special case of Theorem 13."
REFERENCES,0.39941690962099125,"Additional Notations
To prove Theorem 13, we further deï¬ne a variant of Y Ï€
s1,Â·Â·Â· ,sk as T Ï€
s1,Â·Â·Â· ,sk,
which is the set of policies that are in T and agree with Ï€ on states s1, Â· Â· Â· , sk, i.e.,"
REFERENCES,0.4008746355685131,"T Ï€
s1,Â·Â·Â· ,sk := {Ï€â€² âˆˆT : Ï€â€²(si) = Ï€(si), âˆ€i = 1, Â· Â· Â· , k}."
REFERENCES,0.40233236151603496,"Note that different from BH
Ïµ (Ï€), T is no longer restricted under an admissible adversary set and can
be any connected and compact subset of Î ."
REFERENCES,0.4037900874635568,"The following lemma shows that the values of policies in T that agree on all but one state form a
line segment."
REFERENCES,0.40524781341107874,"Lemma 14. For a policy Ï€ âˆˆT and an arbitrary state s âˆˆS, there are two policies in âˆ‚Ï€T Ï€
S\{s},
namely Ï€âˆ’
s , Ï€+
s , such that âˆ€Ï€â€² âˆˆT Ï€
S\{s},"
REFERENCES,0.4067055393586006,"fv(Ï€âˆ’
s ) â‰¼fv(Ï€â€²) â‰¼fv(Ï€+
s ),
(3)"
REFERENCES,0.40816326530612246,"where â‰¼denotes element-wise less than or equal to (if a â‰¼b, then ai â‰¤bi for all index i). Moreover,
the image of fv restricted to T Ï€
S\{s} is a line segment."
REFERENCES,0.4096209912536443,"Proof of Lemma 14. Lemma 5 in Dadashi et al. (2019) has shown that fv is inï¬nitely differentiable
on Î , hence we know fv(T Ï€
S\{s}) is compact and connected. According to Lemma 4 in Dadashi
et al. (2019), for any two policies Ï€1, Ï€2 âˆˆY Ï€
S\{s}, either fv(Ï€1) â‰¼fv(Ï€2), or fv(Ï€2) â‰¼fv(Ï€1)
(there exists a total order). The same property applies to T Ï€
S\{s} since T Ï€
S\{s} is a subset of Y Ï€
S\{s}."
REFERENCES,0.4110787172011662,"Therefore, there exists Ï€âˆ’
s and Ï€+
s that achieve the minimum and maximum over all policies in
T Ï€
S\{s}. Next we show Ï€âˆ’
s and Ï€+
s can be found on the outermost boundary of T Ï€
S\{s}."
REFERENCES,0.41253644314868804,"Assume Ï€+
s /âˆˆâˆ‚Ï€T Ï€
S\{s}, and for all ËœÏ€ âˆˆT Ï€
S\{s}, fv(ËœÏ€) â‰ºfv(Ï€+
s ). Then we can ï¬nd another policy
Ï€â€² âˆˆâˆ‚Ï€T Ï€
S\{s} such that Ï€+
s = Î±Ï€ + (1 âˆ’Î±)Ï€â€² for some scalar Î± âˆˆ(0, 1). Then according to
Lemma 11, fv(Ï€â€²) â‰½fv(Ï€+
s ), contradicting with the assumption. Therefore, one should be able to
ï¬nd a policy on the outermost boundary of T Ï€
S\{s} whose value dominates all other policies. And
similarly, we can also ï¬nd Ï€âˆ’
s on âˆ‚Ï€T Ï€
S\{s}."
REFERENCES,0.4139941690962099,"Furthermore, fv(T Ï€
S\{s}) is a subset of fv(Y Ï€
S\{s}) since T Ï€
S\{s} is a subset of Y Ï€
S\{s}. Given that
fv(Y Ï€
S\{s}) is a line segment, and fv(T Ï€
S\{s}) is connected, we can conclude that fv(T Ï€
S\{s}) is also
a line segment."
REFERENCES,0.41545189504373176,"Next, the following lemma shows that Ï€+
s and Ï€âˆ’
s and their linear combinations can generate values
that cover the set fv(T Ï€
S\{s})."
REFERENCES,0.41690962099125367,"Lemma 15. For a policy Ï€ âˆˆT , an arbitrary state s âˆˆS, and Ï€+
s , Ï€âˆ’
s deï¬ned in Lemma 14, the
following three sets are equivalent:
(1) fv(T Ï€
S\{s});
(2) fv
 
closure(T Ï€
S\{s})

, where closure(Â·) is the convex closure of a set;
(3) {fv(Î±Ï€+
s + (1 âˆ’Î±)Ï€âˆ’
s )|Î± âˆˆ[0, 1]};
(4) {Î±fv(Ï€+
s ) + (1 âˆ’Î±)fv(Ï€âˆ’
s )|Î± âˆˆ[0, 1]};"
REFERENCES,0.41836734693877553,Proof of Lemma 15. We show the equivalence by showing (1) âŠ†(4) âŠ†(3) âŠ†(2) âŠ†(1) as below.
REFERENCES,0.4198250728862974,"(2) âŠ†(1): For any Ï€1, Ï€2 âˆˆT Ï€
S\{s}, without loss of generality, suppose fv(Ï€1) â‰¼fv(Ï€2). According
to Lemma 11, for any Î± âˆˆ[0, 1], fv(Ï€1) â‰¼Î±Ï€1 + (1 âˆ’Î±)Ï€2 â‰¼fv(Ï€2). Therefore, any convex
combinations of policies in T Ï€
S\{s} has value that is in the range of fv(T Ï€
S\{s}). So the values of
policies in the convex closure of T Ï€
S\{s} do not exceed fv(T Ï€
S\{s}), i.e., (2) âŠ†(1)."
REFERENCES,0.42128279883381925,Published as a conference paper at ICLR 2022
REFERENCES,0.4227405247813411,"(3) âŠ†(2): Based on the deï¬nition, Î±Ï€+
s + (1 âˆ’Î±)Ï€âˆ’
s âˆˆclosure(T Ï€
S\{s}), so (3) âŠ†(2)."
REFERENCES,0.42419825072886297,"(4) âŠ†(3): According to Lemma 11, there exists a strictly monotonic rational function Ï : [0, 1] â†’R,
such that
l(Î±) = fv(Î±Ï€+
s + (1 âˆ’Î±)Ï€âˆ’
s ) = Ï(Î±)fv(Ï€+
s ) + (1 âˆ’Ï(Î±))fv(Ï€âˆ’
s )."
REFERENCES,0.42565597667638483,"Therefore, due to intermediate value theorem, for Î± âˆˆ[0, 1], Ï(Î±) takes all values from 0 to 1. So
(4) = (3)."
REFERENCES,0.4271137026239067,"(1) âŠ†(4): Lemma 14 shows that fv(T Ï€
S\{s}) is a line segment bracketed by fv(Ï€+
s ) and fv(Ï€âˆ’
s ).
Therefore, for any Ï€â€² âˆˆT Ï€
S\{s}, its value is a convex combination of fv(Ï€+
s ) and fv(Ï€âˆ’
s )."
REFERENCES,0.42857142857142855,"Next, we show that the relative boundary of the value space constrained to T Ï€
s1,Â·Â·Â· ,sk is covered by
policies that dominate or are dominated in at least one state. The relative interior of set A in B is
deï¬ned as the set of points in A that have a relative neighborhood in A âˆ©B, denoted as relintBA.
The relative boundary of set A in B, denoted as âˆ‚BA, is deï¬ned as the set of points in A that are
not in the relative interior of A, i.e., âˆ‚BA = A\relintBA. When there is no ambiguity, we omit the
subscript of âˆ‚to simplify notations."
REFERENCES,0.43002915451895046,"In addition, we introduce another notation F Ï€
s1,Â·Â·Â· ,sk := V Ï€ + span(CÏ€
k+1, Â· Â· Â· , CÏ€
|S|), where CÏ€
i
stands for the i-th column of the matrix (I âˆ’Î³P Ï€)âˆ’1. Note that F Ï€
s1,Â·Â·Â· ,sk is the same with HÏ€
s1,Â·Â·Â· ,sk
in Dadashi et al. Dadashi et al. (2019), and we change H to F in order to distinguish from the
admissible adversary set HÏµ deï¬ned in our paper."
REFERENCES,0.4314868804664723,"Lemma 16. For a policy Ï€ âˆˆT , k â‰¤|S|, and a set of policies T Ï€
s1,Â·Â·Â· ,sk that agree with Ï€ on
s1, Â· Â· Â· , sk (perturb Ï€ only at sk+1, Â· Â· Â· , s|S|), deï¬ne Vt := fv(T Ï€
s1,Â·Â·Â· ,sk). Deï¬ne two sets of policies
X+
s := {Ï€â€² âˆˆT Ï€
s1,Â·Â·Â· ,sk : Ï€â€²(Â·|s) = Ï€+
s (Â·|s)}, and Xâˆ’
s := {Ï€â€² âˆˆT Ï€
s1,Â·Â·Â· ,sk : Ï€â€²(Â·|s) = Ï€âˆ’
s (Â·|s)}.
We have that the relative boundary of Vt in F Ï€
s1,Â·Â·Â· ,sk is included in the value functions spanned by
policies in T Ï€
s1,Â·Â·Â· ,sk âˆ©(X+
sj âˆªXâˆ’
sj) for at least one s /âˆˆ{s1, Â· Â· Â· , sk}, i.e., âˆ‚Vt âŠ‚ |S|
["
REFERENCES,0.4329446064139942,"j=k+1
fv(T Ï€
s1,Â·Â·Â· ,sk âˆ©(X+
sj âˆªXâˆ’
sj))"
REFERENCES,0.43440233236151604,Proof of Lemma 16. We ï¬rst prove the following claim:
REFERENCES,0.4358600583090379,"Claim 1: For a policy Ï€0 âˆˆT Ï€
s1,Â·Â·Â· ,sk, if âˆ€j âˆˆ{k + 1, Â· Â· Â· , |S|}, âˆ„Ï€â€² âˆˆclosure(T Ï€
s1,Â·Â·Â· ,sk) âˆ©(X+
sj âˆª
Xâˆ’
sj) such that fv(Ï€â€²) = fv(Ï€0), then fv(Ï€0) has a relative neighborhood in Vt âˆ©F Ï€
s1,Â·Â·Â· ,sk."
REFERENCES,0.43731778425655976,"First, based on Lemma 14 and Lemma 15, we can construct a policy Ë†Ï€ âˆˆclosure(T Ï€
s1,Â·Â·Â· ,sk) such
that fv(Ë†Ï€) = fv(Ï€0) through the following steps:"
REFERENCES,0.4387755102040816,Algorithm 2: Constructing Ë†Ï€
REFERENCES,0.4402332361516035,1 Set Ï€k = Ï€0
REFERENCES,0.44169096209912534,"2 for j = k + 1, Â· Â· Â· , |S| do"
REFERENCES,0.44314868804664725,"3
Find Ï€+
sj, Ï€âˆ’
sj âˆˆT Ï€jâˆ’1
S\{sj}"
REFERENCES,0.4446064139941691,"4
Find Ï€j = Ë†Î±jÏ€+
sj + (1 âˆ’Ë†Î±j)Ï€âˆ’
sj such that fv(Ï€j) = fv(Ï€jâˆ’1)"
REFERENCES,0.446064139941691,5 Return Ë†Ï€ = Ï€|S|
REFERENCES,0.44752186588921283,"Denote the concatenation of Î±jâ€™s as a vector Ë†Î± := [Ë†Î±k+1, Â· Â· Â· , Ë†Î±|S|]."
REFERENCES,0.4489795918367347,"According to the assumption that âˆ€j âˆˆ{k + 1, Â· Â· Â· , |S|}, âˆ„Ï€â€² âˆˆclosure(T Ï€
s1,Â·Â·Â· ,sk) âˆ©(X+
sj âˆªXâˆ’
sj)
such that fv(Ï€â€²) = fv(Ï€0), we have Ë†Î±j /âˆˆ{0, 1}, âˆ€j = k + 1, Â· Â· Â· , |S|. Then, deï¬ne a function
Ï† : (0, 1)|S|âˆ’k â†’Vt such that"
REFERENCES,0.45043731778425655,"Ï†(Î±) = fv(Ï€Î±), where"
REFERENCES,0.4518950437317784,"(
Ï€Î±(Â·|sj) = Î±Ï€+
sj + (1 âˆ’Î±)Ï€âˆ’
sj
if j âˆˆ{k + 1, Â· Â· Â· , |S|}
Ï€Î±(Â·|sj) = Ë†Ï€(Â·|sj)
otherwise"
REFERENCES,0.45335276967930027,Published as a conference paper at ICLR 2022
REFERENCES,0.45481049562682213,Then we have that
REFERENCES,0.45626822157434405,1. Ï† is continuously differentiable.
REFERENCES,0.4577259475218659,2. Ï†(Ë†Î±) = fv(Ë†Ï€).
REFERENCES,0.45918367346938777,"3.
âˆ‚Ï†
âˆ‚Î±j is non-zero at Ë†Î± (because of Lemma 11 (3))."
REFERENCES,0.4606413994169096,"4.
âˆ‚Ï†
âˆ‚Î±j is along the i-the column of (I âˆ’Î³P Ë†Ï€)âˆ’1 (see Lemma 3 in Dadashi et al. Dadashi
et al. (2019))."
REFERENCES,0.4620991253644315,"Therefore, by the inverse theorem function, there is a neighborhood of Ï†(Î±) = fv(Ë†Ï€) in the image
space."
REFERENCES,0.46355685131195334,"Now we have proved Claim 1. As a result, for any policy Ï€0 âˆˆT Ï€
s1,Â·Â·Â· ,sk, if fv(Ï€0) is in the relative
boundary of Vt in F Ï€
s1,Â·Â·Â· ,sk, then âˆƒj âˆˆ{k + 1, Â· Â· Â· , |S|}, Ï€â€² âˆˆclosure(T Ï€
s1,Â·Â·Â· ,sk) âˆ©(X+
sj âˆªXâˆ’
sj)
such that fv(Ï€â€²) = fv(Ï€0). Based on Lemma 15, we can also ï¬nd Ï€â€²â€² âˆˆT Ï€
s1,Â·Â·Â· ,sk âˆ©(X+
sj âˆªXâˆ’
sj)
such that fv(Ï€â€²â€²) = fv(Ï€0). So Lemma 16 holds."
REFERENCES,0.4650145772594752,"Now, we are ï¬nally ready to prove Theorem 13."
REFERENCES,0.46647230320699706,"Proof of Theorem 13. We will show that âˆ€{s1, Â· Â· Â· , sk} âŠ†S, the value Vt = fv(T Ï€
s1,Â·Â·Â· ,sk) is a
polytope."
REFERENCES,0.4679300291545189,"We prove the above claim by induction on the cardinality of the number of states k. In the base case
where k = |S|, Vt = {fv(Ï€)} is a polytope."
REFERENCES,0.46938775510204084,"Suppose the claim holds for k + 1, then we show it also holds for k, i.e., for a policy Ï€ âˆˆÎ , the
value of T Ï€
s1,Â·Â·Â· ,sk âŠ†Y Ï€
s1,Â·Â·Â· ,sk âŠ†Î  for a polytope."
REFERENCES,0.4708454810495627,"According to Lemma 16, we have âˆ‚Vt âŠ‚ |S|
["
REFERENCES,0.47230320699708456,"j=k+1
fv(T Ï€
s1,Â·Â·Â· ,sk âˆ©(X+
sj âˆªXâˆ’
sj)) = |S|
["
REFERENCES,0.4737609329446064,"j=k+1
Vt âˆ©(F +
sj âˆªF âˆ’
sj))"
REFERENCES,0.4752186588921283,"where âˆ‚Vt denotes the relative boundary of Vt in F Ï€
s1,Â·Â·Â· ,sk; F +
sj and F âˆ’
sj are two afï¬ne hyperplanes
of F Ï€
s1,Â·Â·Â· ,sk, standing for the value space of policies that agree with Ï€+
sj and Ï€âˆ’
sj in state sj respec-
tively."
REFERENCES,0.47667638483965014,Then we can get
REFERENCES,0.478134110787172,"1. Vt = fv(T Ï€
s1,Â·Â·Â· ,sk) is closed as T Ï€
s1,Â·Â·Â· ,sk is compact and fv is continuous."
REFERENCES,0.47959183673469385,"2. âˆ‚Vt âŠ‚S|S|
j=k+1(F +
sj âˆªF âˆ’
sj)), a ï¬nite number of afï¬ne hyperplanes in F Ï€
s1,Â·Â·Â· ,sk."
REFERENCES,0.48104956268221577,"3. Vt âˆ©F +
sj (or Vt âˆ©F âˆ’
sj) is a polyhedron by induction assumption."
REFERENCES,0.48250728862973763,"Hence, based on Proposition 1 by Dadashi et al. Dadashi et al. (2019), we get Vt is a polyhedron.
Since Vt âŠ†V is bounded, we can further conclude that Vt is a polytope."
REFERENCES,0.4839650145772595,"Therefore, for an arbitrary connected and compact set of policies T âŠ†Î , let Ï€ âˆˆT be an arbitrary
policy in T , then fv(T ) = fv(T Ï€
âˆ…) is a polytope."
REFERENCES,0.48542274052478135,"B.5
EXAMPLES OF THE OUTERMOST BOUNDARY"
REFERENCES,0.4868804664723032,"See Figure 5 for examples of the outermost boundary for different BH
Ïµ (Ï€)â€™s."
REFERENCES,0.48833819241982507,Published as a conference paper at ICLR 2022
REFERENCES,0.4897959183673469,"Figure 5: Two examples of the outermost boundary with |A| = 3 actions at one single state s. The large
triangle denotes the distributions over the action space at state s, i.e., Î s; Ï€1, Ï€2 and Ï€3 are three policies
that deterministically choose a1, a2 and a3 respectively. Ï€ is the victim policy, the dark green area is the
BH
Ïµ (Ï€)s : BH
Ïµ (Ï€) âˆ©Î s. The red solid curve depicts the outermost boundary of BH
Ïµ (Ï€)s. Note that a policy is
in the outermost boundary of BH
Ïµ (Ï€) iff it is in the outermost boundary of BH
Ïµ (Ï€)s for all s âˆˆS. !""âˆ— !""$ âˆ—"
REFERENCES,0.4912536443148688,"Figure 6: Value space of an example MDP. The values of the whole policy space Î  form a polytope (blue) as
suggested by Dadashi et al. (2019). The values of all perturbed policies with HÏµ also form a polytope (green)
as suggested by Theorem 12."
REFERENCES,0.49271137026239065,"B.6
AN EXAMPLE OF THE POLICY PERTURBATION POLYTOPE"
REFERENCES,0.49416909620991256,"An example is given by Figure 6, where we deï¬ne an MDP with 2 states and 3 actions. We train
an DQN agent with one-hot encodings of the states, and then randomly perturb the states within
an â„“âˆball with Ïµ = 0.8. By sampling 5M random policies, and 100K random perturbations, we
visualize the value space of approximately the whole policy space Î  and the admissible adversarial
policy set BH
Ïµ (Ï€), both of which are polytopes (boundaries are ï¬‚at). A learning agent searches for
the optimal policy Ï€âˆ—whose value is the upper right vertex of the larger blue polytope, while the
attacker attempts to ï¬nd an optimal adversary hâˆ—, which perturbs a given clean policy Ï€ to the worst
perturbed policy Ï€hâˆ—whose value is the lower left vertex of the smaller green polytope. This also
justiï¬es the fact that learning an optimal adversary is as difï¬cult as learning an optimal policy in an
RL problem."
REFERENCES,0.4956268221574344,The example MDP Mex:
REFERENCES,0.4970845481049563,"|A| = 3, Î³ = 0.8
Ë†r = [âˆ’0.1, âˆ’1., 0.1, 0.4, 1.5, 0.1]
Ë†P = [[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.05, 0.95], [0.25, 0.75], [0.3, 0.7]]"
REFERENCES,0.49854227405247814,The base/clean policy Ï€:
REFERENCES,0.5,"Ï€(a1|s1) = 0.215, Ï€(a2|s1) = 0.429, Ï€(a3|s1) = 0.356
Ï€(a1|s2) = 0.271, Ï€(a2|s2) = 0.592, Ï€(a3|s2) = 0.137"
REFERENCES,0.5014577259475219,Published as a conference paper at ICLR 2022
REFERENCES,0.5029154518950437,"C
EXTENTIONS AND ADDITIONAL DETAILS OF OUR ALGORITHM"
REFERENCES,0.5043731778425656,"C.1
ATTACKING A DETERMINISTIC VICTIM POLICY"
REFERENCES,0.5058309037900874,"For a deterministic victim Ï€D = argmaxaÏ€(a|s), we deï¬ne Deterministic Policy Adversary MDP
(D-PAMDP) as below, where a subscript D is added to all components to distinguish them from their
stochastic counterparts. In D-PAMDP, the director proposes a target action baD âˆˆA(=: b
AD), and
the actor tries its best to let the victim output this target action.
Deï¬nition 17 (Deterministic Policy Adversary MDP (D-PAMDP)). Given an MDP M =
âŸ¨S, A, P, R, Î³âŸ©, a ï¬xed and deterministic victim policy Ï€D, we deï¬ne a Deterministic Policy Ad-
versarial MDP c
MD = âŸ¨S, b
AD, bPD, bRD, Î³âŸ©, where the action space is b
AD = b
AD, and âˆ€s, sâ€² âˆˆ
S, âˆ€ba âˆˆA,
bPD(sâ€²|s, ba) = P(sâ€²|s, Ï€D(g(ba, s))),
bRD(s, ba) = âˆ’R(s, Ï€D(g(ba, s))).
The actor function g is deï¬ned as
gD(ba, s) = argmaxËœsâˆˆBÏµ(s)
 
Ï€(ba|Ëœs) âˆ’maxaâˆˆA,aÌ¸=baÏ€(a|Ëœs)

(GD)"
REFERENCES,0.5072886297376094,The optimal policy of D-PAMDP is an optimal adversary against Ï€D as proved in Appendix D.2.2
REFERENCES,0.5087463556851312,"C.2
IMPLEMENTATION DETAILS OF PA-AD"
REFERENCES,0.5102040816326531,"To address the actor function g (or gD) deï¬ned in (G) and (GD), we let the actor maximize objectives
JD and J within the BÏµ(Â·) ball around the original state, for a deterministic victim and a stochastic
victim, respectively. Below we explicitly deï¬ne JD and J."
REFERENCES,0.5116618075801749,"Actor Objective for Deterministic Victim
For the deterministic variant of PA-AD, the actor func-
tion (GD) is simple and can be directly solved to identify the optimal adversary. Concretely, we
deï¬ne the following objective"
REFERENCES,0.5131195335276968,"JD(Ëœs; ba, s) := Ï€(ba|Ëœs) âˆ’maxaâˆˆA,aÌ¸=baÏ€(a|Ëœs),
(JD)"
REFERENCES,0.5145772594752187,"which can be realized with the multi-class classiï¬cation hinge loss. In practice, a relaxed cross-
entropy objective can also be used to maximize Ï€(ba|Ëœs)."
REFERENCES,0.5160349854227405,"Actor Objective for Stochastic Victim
Different from the deterministic-victim case, the actor
function for a stochastic victim deï¬ned in (G) requires solving a more complex optimization prob-
lem with a non-convex constraint set, which in practice can be relaxed to (J) (a Lagrangian relax-
ation) to efï¬ciently get an approximation of the optimal adversary.
argmaxËœsâˆˆBÏµ(s)J(Ëœs; ba, s) := âˆ¥Ï€(Â·|Ëœs) âˆ’Ï€(Â·|s)âˆ¥+ Î» Ã— CosineSim
 
Ï€(Â·|Ëœs) âˆ’Ï€(Â·|s), ba

(J)
where CosineSim in the second refers to the cosine similarity function; the ï¬rst term measures how
far away the policy is perturbed from the victim policy; Î» is a hyper-parameter controlling the trade-
off between the two terms. Experimental results show that our PA-AD is not sensitive to the value
of Î». In our reported results in Section 6, we set Î» as 1. Appendix E.2.2 shows the evaluation of our
algorithm using varying Î»â€™s."
REFERENCES,0.5174927113702624,"The procedure of learning the optimal adversary is depicted in Algorithm 3, where we simply use
the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) to approximately solve the actorâ€™s
objective, although more advanced solvers such as Projected Gradient Decent (PGD) can be applied
to further improve the performance. Experiment results in Section 6 verify that the above FGSM-
based implementation achieves state-of-the-art attack performance."
REFERENCES,0.5189504373177842,"What is the Inï¬‚uence of the Relaxation in (J)? First, it is important that the relaxation is only
needed for a stochastic victim. For a deterministic victim, which is often the case in practice, the
actor solves the original unrelaxed objective.
Second, as we will discuss in the next paragraph, the optimality of both SA-RL and PA-AD is re-
garding the formulation. That is, SA-RL and PA-AD formulate the optimal attack problem as an
MDP whose optimal policy is the optimal adversary. However, in a large-scale task, deep RL algo-
rithms themselves usually do not converge to the globally optimal policy and exploration becomes
the main challenge. Thus, when the adversaryâ€™s MDP is large, the suboptimality caused by the RL
solver due to exploration difï¬culties could be much more severe than the suboptimality caused by
the relaxation of the formulation. The comparison between SA-RL and PA-AD in our experiments"
REFERENCES,0.5204081632653061,Published as a conference paper at ICLR 2022
REFERENCES,0.521865889212828,Algorithm 3: Policy Adversarial Actor Director (PA-AD) with FGSM
REFERENCES,0.5233236151603499,1 Input: Initialization of directorâ€™s policy Î½; victim policy Ï€; budget Ïµ; start state s0
REFERENCES,0.5247813411078717,"2 for t = 0, 1, 2, ... do"
REFERENCES,0.5262390670553936,"3
Director samples a policy perturbing direction bat âˆ¼Î½(Â·|st)"
IF VICTIM IS DETERMINISTIC THEN,0.5276967930029155,"4
if Victim is deterministic then"
IF VICTIM IS DETERMINISTIC THEN,0.5291545189504373,"5
# for a deterministic victim, JD is deï¬ned in Equation (JD)"
IF VICTIM IS DETERMINISTIC THEN,0.5306122448979592,"6
Actor computes the gradient of its objective âˆ‡Î´JD(st + Î´; bat, st)"
ELSE,0.532069970845481,"7
else"
ELSE,0.5335276967930029,"8
# for a stochastic victim, J is deï¬ned in Equation (J)"
ELSE,0.5349854227405247,"9
Actor computes the gradient of its objective âˆ‡Î´J(st + Î´; bat, st)"
ELSE,0.5364431486880467,"10
Actor sets Ëœst = st + Ïµ Â· sign(Î´)"
ELSE,0.5379008746355685,"11
Victim takes action at âˆ¼Ï€(Â·|Ëœst), proceeds to st+1, receives rt"
ELSE,0.5393586005830904,"12
Director saves (st, bat, âˆ’rt, st+1) to its buffer"
ELSE,0.5408163265306123,"13
Director updates its policy Î½ using any RL algorithm"
ELSE,0.5422740524781341,"can justify that the size of the adversary MDP has a larger impact than the relaxation of the problem
on the ï¬nal solution found by the attackers.
Third, in Appendix F.1, we empirically show that with the relaxed objective, PA-AD can still ï¬nd
the optimal attacker in 3 example environments."
ELSE,0.543731778425656,"Optimality in Formulation v.s. Approximated Optimality in Practice
PA-AD has an optimal
formulation, as the optimal solution to its objective (the optimal policy in PAMDP) is always an op-
timal adversary (Theorem 7). Similarly, the previous attack method SA-RL has an optimal solution
since the optimal policy in the adversaryâ€™s MDP is also an optimal adversary. However, in practice
where the environments are in a large scale and the number of samples is ï¬nite, the optimal policy
is not guaranteed to be found by either PA-AD and SA-RL with deep RL algorithms. Therefore, for
practical consideration, our goal is to search for a good solution or approximate the optimal solution
using optimization techniques (e.g. actor-critic learning, one-step FGSM attack, Lagrangian relax-
ation for the stochastic-victim attack). In experiments (Section 6), we show that our implementation
universally ï¬nds stronger attackers than prior methods, which veriï¬es the effectiveness of both our
theoretical framework and our practical implementation."
ELSE,0.5451895043731778,"C.3
VARIANTS FOR ENVIRONMENTS WITH CONTINUOUS ACTION SPACES"
ELSE,0.5466472303206997,"Although the analysis in the main paper focuses on an MDP whose action space is discrete, our
algorithm also extends to a continuous action space as justiï¬ed in our experiments."
ELSE,0.5481049562682215,"C.3.1
FOR A DETERMINISTIC VICTIM"
ELSE,0.5495626822157434,"In this case, we can still use the formulation D-PAMDP, but a slightly different actor function
gD(ba, s) = argminËœsâˆˆBÏµ(s)âˆ¥Ï€D(Ëœs) âˆ’baâˆ¥.
(GCD)"
ELSE,0.5510204081632653,"C.3.2
FOR A STOCHASTIC VICTIM"
ELSE,0.5524781341107872,"Different from a stochastic victim in a discrete action space whose actions are sampled from a cate-
gorical distribution, a stochastic victim in a continuous action space usually follows a parametrized
probability distribution with a certain family of distributions, usually Gaussian distributions. In
this case, the formulation of PAMDP in Deï¬nition 6 is impractical. However, since the mean of
a Gaussian distribution has the largest probability to be selected, one can still use the formulation
in (GCD), while replacing Ï€D(Ëœs) with the mean of the output distribution. Then, the director and
the actor can collaboratively let the victim output a Gaussian distribution whose mean is the target
action. If higher accuracy is needed, we can use another variant of PAMDP, named Continuous
Policy Adversary MDP (C-PAMDP) that can also control the variance of the Gaussian distribution."
ELSE,0.5539358600583091,"Deï¬nition 18 (Continuous Policy Adversary MDP (C-PAMDP)). Given an MDP M
=
âŸ¨S, A, P, R, Î³âŸ©where A is continuous, a ï¬xed and stochastic victim policy Ï€, we deï¬ne a Con-
tinuous Policy Adversarial MDP c
MC = âŸ¨S, b
AC, bPC, bRC, Î³âŸ©, where the action space is b
AD = A,"
ELSE,0.5553935860058309,Published as a conference paper at ICLR 2022
ELSE,0.5568513119533528,"and âˆ€s, sâ€² âˆˆS, âˆ€ba âˆˆA,"
ELSE,0.5583090379008746,"bP(sâ€²|s, ba) =
Z"
ELSE,0.5597667638483965,"A
Ï€(a|g(ba, s))P(sâ€²|s, a) da,
bR(s, ba) = âˆ’
Z"
ELSE,0.5612244897959183,"A
Ï€(a|g(ba, s))R(s, a)da."
ELSE,0.5626822157434402,"The actor function g is deï¬ned as
g(ba, s) = argminËœsâˆˆBÏµ(s)KL(Ï€(Â·|Ëœs)||N(ba, Ïƒ2I|A|)).
(GC)
where Ïƒ is a hyper-parameter, and N denotes a multivariate Gaussian distribution."
ELSE,0.5641399416909622,"In short, Equation (GC) encourages the victim to output a distribution that is similar to the target
distribution. The hyperparameter Ïƒ controls the standard deviation of the target distribution. One
can set Ïƒ to be small in order to let the victim execute the target action ba with higher probabilities."
ELSE,0.565597667638484,"D
CHARACTERIZE OPTIMALITY OF EVASION ATTACKS"
ELSE,0.5670553935860059,"In this section, we provide a detailed characterization for the optimality of evasion attacks from the
perspective of policy perturbation, following Deï¬nition 5 in Section 4. Section D.1 establishes the
existence of the optimal policy adversary which is deï¬ned in Section 3. Section D.2 then provides
a proof for Theorem 7 that the formulation of PA-AD is optimal. We also analyze the optimality of
heuristic attacks in Section D.3."
ELSE,0.5685131195335277,"D.1
EXISTENCE OF AN OPTIMAL POLICY ADVERSARY"
ELSE,0.5699708454810496,"Theorem 19 (Existence of An Optimal Policy Adversary). Given an MDP M = âŸ¨S, A, P, R, Î³âŸ©,
and a ï¬xed stationary policy Ï€ on M, let HÏµ be a non-empty set of admissible state adversaries and
BH
Ïµ (Ï€) be the corresponding Adv-policy-set, then there exists an optimal policy adversary Ï€hâˆ—âˆˆ
BH
Ïµ (Ï€) such that Ï€hâˆ—âˆˆargminÏ€hâˆˆBH
Ïµ (Ï€)V Ï€h
M (s), âˆ€s âˆˆS."
ELSE,0.5714285714285714,"Proof. We prove Theorem 19 by constructing a new MDP corresponding to the original MDP M
and the victim Ï€."
ELSE,0.5728862973760933,"Deï¬nition 20 (Policy Perturbation MDP). For a given MDP M, a ï¬xed stochastic victim pol-
icy Ï€, and an admissible state adversary set HÏµ, deï¬ne a policy perturbation MDP as MP =
âŸ¨S, AP , PP , RP , Î³âŸ©, where AP = âˆ†(A), and âˆ€s âˆˆS, aP âˆˆAP ,"
ELSE,0.5743440233236151,"RP (s, aP ) := { âˆ’P"
ELSE,0.575801749271137,"aâˆˆA aP (a|s)R(s, a)
if âˆƒh âˆˆHÏµ s.t. aP (Â·|s) = Ï€(Â·|h(s))
âˆ’âˆ
otherwise
(4)"
ELSE,0.577259475218659,"PP (sâ€²|s, aP ) :=
X"
ELSE,0.5787172011661808,"aâˆˆA
aP (a|s)P(sâ€²|s, a)
(5)"
ELSE,0.5801749271137027,Then we can prove Theorem 19 by proving the following lemma.
ELSE,0.5816326530612245,Lemma 21. The optimal policy in MP is an optimal policy adversary for Ï€ in M.
ELSE,0.5830903790087464,"Let NP denote the set of deterministic policies in MP . According to the traditional MDP the-
ory (Puterman, 1994), there exists a deterministic policy that is optimal in MP . Note that HÏµ is
non-empty, so there exists at least one policy in MP with value â‰¥âˆ’âˆ, and then the optimal policy
should have value â‰¥âˆ’âˆ. Denote this optimal and deterministic policy as Î½âˆ—
P âˆˆNP . Then we write
the Bellman equation of Î½âˆ—
P , i.e.,"
ELSE,0.5845481049562682,"V Î½âˆ—
P
P (s) = max
Î½P âˆˆNP RP (s, Î½P (s)) + Î³
X"
ELSE,0.5860058309037901,"sâ€²âˆˆS
PP (sâ€²|s, Î½P (s))V Î½P
P (sâ€²)"
ELSE,0.5874635568513119,"= max
Î½P âˆˆNP "" âˆ’
X"
ELSE,0.5889212827988338,"aâˆˆA
Î½P (a|s)R(s, a) + Î³
X sâ€²âˆˆS X"
ELSE,0.5903790087463557,"aâˆˆA
Î½P (a|s)P(sâ€²|s, a)V Î½P
P (sâ€²) #"
ELSE,0.5918367346938775,"= max
Î½P âˆˆNP X"
ELSE,0.5932944606413995,"aâˆˆA
Î½P (a|s) """
ELSE,0.5947521865889213,"âˆ’R(s, a) + Î³
X"
ELSE,0.5962099125364432,"sâ€²âˆˆS
P(sâ€²|s, a)V Î½P
P (sâ€²) #
(6)"
ELSE,0.597667638483965,Published as a conference paper at ICLR 2022
ELSE,0.5991253644314869,"Note that Î½âˆ—
P (s) is a distribution on action space, Î½âˆ—
P (a|s) is the probability of a given by distribution
Î½âˆ—(s)."
ELSE,0.6005830903790087,"Multiply both sides of Equation (6) by âˆ’1, and we obtain"
ELSE,0.6020408163265306,"âˆ’V Î½âˆ—
P
P (s) = min
Î½âˆˆNP X"
ELSE,0.6034985422740525,"aâˆˆA
Î½P (s)(a|s) """
ELSE,0.6049562682215743,"R(s, a) + Î³
X"
ELSE,0.6064139941690962,"sâ€²âˆˆS
P(sâ€²|s, a)
 
âˆ’V Î½P
P (sâ€²)

# (7)"
ELSE,0.607871720116618,"In the original MDP M, an optimal policy adversary (if exists) Ï€hâˆ—for Ï€ should satisfy"
ELSE,0.60932944606414,"V Ï€hâˆ—(s) =
min
Ï€hâˆˆBH
Ïµ (Ï€) X"
ELSE,0.6107871720116618,"aâˆˆA
Ï€h(a|s) """
ELSE,0.6122448979591837,"R(s, a) + Î³
X"
ELSE,0.6137026239067055,"sâ€²âˆˆS
P(sâ€²|s, a)V Ï€h(sâ€²) # (8)"
ELSE,0.6151603498542274,"By comparing Equation (7) and Equation (8) we get the conclusion that Î½âˆ—
P is an optimal policy
adversary for Ï€ in M."
ELSE,0.6166180758017493,"D.2
PROOF OF THEOREM 7: OPTIMALITY OF OUR PA-AD"
ELSE,0.6180758017492711,"In this section, we provide theoretical proof of the optimality of our proposed evasion RL algorithm
PA-AD."
ELSE,0.619533527696793,"D.2.1
OPTIMALITY OF PA-AD FOR A STOCHASTIC VICTIM"
ELSE,0.6209912536443148,"We ï¬rst build a connection between the PAMDP c
M deï¬ned in Deï¬nition 6 (Section 4) and the
policy perturbation MDP deï¬ned in Deï¬nition 20 (Appendix D.1)."
ELSE,0.6224489795918368,"A deterministic policy Î½ in the PAMDP c
M can induce a policy Ë†Î½P in MP in the following way:
bÎ½P (s) = Ï€(Â·|g(Î½(s), s)), âˆ€s âˆˆS. More importantly, the values of Î½ and bÎ½P in c
M and MP are
equal because of the formulations of the two MDPs, i.e., bV Î½ = V bÎ½P
P , where bV and VP denote the
value functions in c
M and VP respectively."
ELSE,0.6239067055393586,"Proposition 22 below builds the connection of the optimality between the policies in these two
MDPs.
Proposition 22. An optimal policy in c
M induces an optimal policy in MP ."
ELSE,0.6253644314868805,"Proof of Proposition 22. Let Î½âˆ—be an deterministic optimal policy in c
M, and it induces a policy in
MP , namely bÎ½P ."
ELSE,0.6268221574344023,"Let us assume bÎ½P is not an optimal policy in MP , hence there exists a policy Î½âˆ—
P in MP s.t.
V Î½âˆ—
P
P (s) > V bÎ½P
P (s) for at least one s âˆˆS. And based on Theorem 4, we are able to ï¬nd such a Î½âˆ—
P
whose corresponding policy perturbation is on the outermost boundary of B(Ï€), i.e., Î½âˆ—âˆˆâˆ‚Ï€BH
Ïµ (Ï€)."
ELSE,0.6282798833819242,"Then we can construct a policy Î½â€² in c
M such that Î½â€²(s) = Î½âˆ—
P (s) âˆ’Ï€(s), âˆ€s âˆˆS. And based on
Equation (G), Ï€(Â·|g(Î½â€²(s), s)) is in âˆ‚Ï€B(Ï€(s)) for all s âˆˆS. According to the deï¬nition of âˆ‚Ï€, if
two policy perturbations perturb Ï€ in the same direction and are both on the outermost boundary,
then they are equal. Thus, we can conclude that Ï€(g(Î½â€²(s), s)) = Î½âˆ—
P (s), âˆ€s âˆˆS. Then we obtain
bV Î½â€²(s) = V Î½âˆ—
P
P (s), âˆ€s âˆˆS."
ELSE,0.6297376093294461,"Now we have conditions:
(1) bV Î½âˆ—(s) = V bÎ½P
P (s), âˆ€s âˆˆS;
(2) V Î½âˆ—
P
P (s) > V bÎ½P
P (s) for at least one s âˆˆS;
(3) âˆƒÎ½â€² such that bV Î½â€²(s) = V Î½âˆ—
P
P (s), âˆ€s âˆˆS."
ELSE,0.6311953352769679,"From (1), (2) and (3), we can conclude that bV Î½â€²(s) > bV Î½âˆ—(s) for at least one s âˆˆS, which conï¬‚icts
with the assumption that Î½âˆ—is optimal in c
M. Therefore, Proposition 22 is proven."
ELSE,0.6326530612244898,Published as a conference paper at ICLR 2022
ELSE,0.6341107871720116,"Proposition 22 and Lemma 21 together justiï¬es that the optimal policy of c
M, namely Î½âˆ—, induces
an optimal policy adversary for Ï€ in the original M. Then, if the director learns the optimal policy
in c
M, then it collaborates with the actor and generates the optimal state adversary hâˆ—by hâˆ—(s) =
g(Î½âˆ—(s), s), âˆ€s âˆˆS."
ELSE,0.6355685131195336,"D.2.2
OPTIMALITY OF OUR PA-AD FOR A DETERMINISTIC VICTIM"
ELSE,0.6370262390670554,"In this section, we show that the optimal policy in D-PAMDP (the deterministic variant of PAMDP
deï¬ned in Appendix C.1) also induces an optimal policy adversary in the original environment."
ELSE,0.6384839650145773,"Let Ï€D be a deterministic policy reduced from a stochastic policy Ï€, i.e.,
Ï€D(s) := argmaxaâˆˆAÏ€(a|s), âˆ€s âˆˆS.
Note that in this case, the Adv-policy-set BH
Ïµ (Ï€) is not connected as it contains only deterministic
policies. Therefore, we re-formulate the policy perturbation MDP introduced in Appendix D.1 with
a deterministic victim as below:"
ELSE,0.6399416909620991,"Deï¬nition 23 (Deterministic Policy Perturbation MDP). For a given MDP M, a ï¬xed determin-
istic victim policy Ï€, and an admissible adversary set HÏµ, deï¬ne a deterministic policy perturbation
MDP as MDP = âŸ¨S, ADP , PDP , RDP , Î³âŸ©, where ADP = A, and âˆ€s âˆˆS, aDP âˆˆADP ,"
ELSE,0.641399416909621,"RDP (s, aDP ) := { âˆ’R(s, aDP )
if âˆƒh âˆˆHÏµ s.t. aDP (s) = Ï€D(h(s))
âˆ’âˆ
otherwise
(9)"
ELSE,0.6428571428571429,"PDP (sâ€²|s, aDP ) := P(s, aDP )
(10)"
ELSE,0.6443148688046647,"MDP can be viewed as a special case of MP where only deterministic policies have â‰¥âˆ’âˆvalues.
Therefore Theorem 19 and Lemma 21 also hold for deterministic victims."
ELSE,0.6457725947521866,"Next we will show that an optimal policy in c
MD induces an optimal policy in MDP ."
ELSE,0.6472303206997084,"Proposition 24. An optimal policy in c
MD induces an optimal policy in MDP ."
ELSE,0.6486880466472303,"Proof of Proposition 24. We will prove Proposition 24 by contradiction. Let Î½âˆ—be an optimal policy
in c
MD, and it induces a policy in MDP , namely bÎ½DP ."
ELSE,0.6501457725947521,"Let us assume bÎ½DP is not an optimal policy in MDP , hence there exists a deterministic policy Î½âˆ—
DP
in MDP s.t. V Î½âˆ—
DP
DP (s) > V bÎ½DP
DP (s) for at least one s âˆˆS. Without loss of generality, suppose
V Î½âˆ—
DP
DP (s0) > V bÎ½DP
DP (s0)."
ELSE,0.6516034985422741,"Next we construct another policy Î½â€² in c
MD by setting Î½â€²(s) = Î½âˆ—
DP (s), âˆ€s âˆˆS. Given that Î½âˆ—
DP
is deterministic, Î½â€² is also a deterministic policy. So we use Î½âˆ—
DP (s) and Î½â€²(s) to denote the action
selected by Î½âˆ—
DP and Î½â€² respectively at state s."
ELSE,0.6530612244897959,"For an arbitrary state si, let ai := Î½âˆ—
DP (si). Since Î½âˆ—
DP is the optimal policy in MDP , we get
that there exists a state adversary h âˆˆHÏµ such that Ï€D(h(si)) = ai, or equivalently, there exists a
state Ëœsi âˆˆBÏµ(si) such that argmaxaâˆˆAÏ€(Ëœsi) = ai. Then, the solution to the actorâ€™s optimization
problem (GD) given direction ai and state si, denoted as Ëœsâˆ—, satisï¬es
Ëœsâˆ—= argmaxsâ€²âˆˆBÏµ(s)
 
Ï€(ba|sâ€²) âˆ’argmaxaâˆˆA,aÌ¸=baÏ€(a|sâ€²)

(11)
and we can get
Ï€(ba|Ëœsâˆ—) âˆ’argmaxaâˆˆA,aÌ¸=baÏ€(a|Ëœsâˆ—) â‰¥Ï€(ba|Ëœsi) âˆ’argmaxaâˆˆA,aÌ¸=baÏ€(a|Ëœsi) > 0
(12)
Given that argmaxaâˆˆAÏ€(ai|Ëœsi)
=
ai, we obtain argmaxaâˆˆAÏ€(ai|Ëœsâˆ—)
=
ai, and hence
Ï€D(gD(ai, si)) = ai. Since this relation holds for an arbitrary state s, we can get
Ï€D(gD(Î½â€²(s), s)) = Ï€D(gD(Î½â€²(s), s)) = Î½â€²(s), âˆ€s âˆˆS
(13)"
ELSE,0.6545189504373178,Published as a conference paper at ICLR 2022
ELSE,0.6559766763848397,"Also, we have âˆ€s âˆˆS
bV Î½â€²
D (s) = bRD(s, Î½â€²(s)) +
X"
ELSE,0.6574344023323615,"sâ€²âˆˆS
bPD(sâ€²|s, Î½â€²(s))bV Î½â€²
D (sâ€²)
(14)"
ELSE,0.6588921282798834,"V Î½âˆ—
DP
DP (s) = RDP (s, Î½âˆ—
DP (s)) +
X"
ELSE,0.6603498542274052,"sâ€²âˆˆS
PDP (sâ€²|s, Î½âˆ—
DP (s))V Î½âˆ—
DP
DP ((sâ€²)
(15)"
ELSE,0.6618075801749271,"Therefore, bV Î½â€²
D (s) = V Î½âˆ—
DP
DP (s), âˆ€s âˆˆS."
ELSE,0.6632653061224489,"Then we have
bV Î½â€²
D (s0) â‰¤bV Î½âˆ—(s0) = V bÎ½DP
DP (s0) < V Î½âˆ—
DP
DP (s0) = bV Î½â€²
D (s0)
(16)"
ELSE,0.6647230320699709,"which gives bV Î½â€²
D (s0) < bV Î½â€²
D (s0), so there is a contradiction."
ELSE,0.6661807580174927,"Combining the results of Proposition 24 and Lemma 21 , for a deterministic victim, the optimal
policy in D-PAMDP gives an optimal adversary for the victim."
ELSE,0.6676384839650146,"D.3
OPTIMALITY OF HEURISTIC-BASED ATTACKS"
ELSE,0.6690962099125365,"There are many existing methods of ï¬nding adversarial state perturbations for a ï¬xed RL policy,
most of which are solving some optimization problems deï¬ned by heuristics. Although these meth-
ods are empirically shown to be effective in many environments, it is not clear how strong these
adversaries are in general. In this section, we carefully summarize and categorize existing heuristic
attack methods into 4 types, and then characterize their optimality in theory."
ELSE,0.6705539358600583,"D.3.1
TYPE I - MINIMIZE THE BEST (MINBEST)"
ELSE,0.6720116618075802,"A common idea of evasion attacks in supervised learning is to reduce the probability that the learner
selects the â€œcorrect answerâ€ Goodfellow et al. (2015). Prior works Huang et al. (2017); Kos & Song
(2017); Korkmaz (2020) apply a similar idea to craft adversarial attacks in RL, where the objective
is to minimize the probability of selecting the â€œbestâ€ action, i.e.,
hMinBest âˆˆargminhâˆˆHÏµÏ€h(a+|s), âˆ€s âˆˆS
(I)
where a+ is the â€œbestâ€ action to select at state s. Huang et al.Huang et al. (2017) deï¬ne a+ as
argmaxaâˆˆAQÏ€(s, a) for DQN, or argmaxaâˆˆAÏ€(a|s) for TRPO and A3C with a stochastic Ï€. Since
the agentâ€™s policy Ï€ is usually well-trained in the original MDP, a+ can be viewed as (approximately)
the action taken by an optimal deterministic policy Ï€âˆ—(s)."
ELSE,0.673469387755102,"Lemma 25 (Optimality of MinBest). Denote the set of optimal solutions to objective (I) as
HMinBest. There exist an MDP M and an agent policy Ï€, such that HMinBest does not contain an
optimal adversary hâˆ—, i.e., HMinBest âˆ©Hâˆ—
Ïµ = âˆ…."
ELSE,0.6749271137026239,"Proof of Lemma 25. We prove this lemma by constructing the following MDP such that for any
victim policy, there exists a reward conï¬guration in which MinBest attacker is not optimal."
ELSE,0.6763848396501457,Figure 7: A simple MDP where MinBest Attacker cannot ï¬nd the optimal adversary for a given victim policy.
ELSE,0.6778425655976676,"Here, let r1 = r(s4|s2, a1), r2 = r(s5|s2, a2), r3 = r(s3|s1, a2). Assuming all the other rewards
are zero, transition dynamics are deterministic, and states s3, s4, s5 are the terminal states. For the"
ELSE,0.6793002915451894,Published as a conference paper at ICLR 2022
ELSE,0.6807580174927114,"sake of simplicity, we also assume that the discount factor here Î³ = 1.
Now given a policy Ï€ such that Ï€(a1|s1) = Î²1 and Ï€(a1|s2) = Î²2 (Î²1, Î²2 âˆˆ[0, 1]), we could ï¬nd
r1, r2, r3 such that the following constraints hold:
r1 > r2 â‡â‡’QÏ€(s1, a1) > QÏ€(s1, a2)
(17)
Î²2r1 + (1 âˆ’Î²2)r2 > r3 â‡â‡’QÏ€(s2, a1) > QÏ€(s2, a2)
(18)
r3 > (Î²2 âˆ’Ïµ2)r2 + (1 âˆ’Î²2 + Ïµ2)r2 â‡â‡’r3 > QÏ€(s1, a1) âˆ’Ïµ2(r1 âˆ’r2)
(19)
Now we consider the Adv-policy-set"
ELSE,0.6822157434402333,"BH
Ïµ (Ï€) =
n
Ï€â€² âˆˆÎ 
 âˆ¥Ï€â€²(Â·|s1) âˆ’Ï€(Â·|s1)âˆ¥< Ïµ1, âˆ¥Ï€â€²(Â·|s2) âˆ’Ï€(Â·|s2)âˆ¥< Ïµ2
o
."
ELSE,0.6836734693877551,"Under these three linear constraints, the policy given by MinBest attacker satisï¬es that
Ï€hMinBest(a1|s1) = Î²1 âˆ’Ïµ1, and Ï€hMinBest(a1|s2) = Î²2 âˆ’Ïµ2. On the other hand, we can ï¬nd an-
other admissible policy adversary Ï€hâˆ—(a1|s1) = Î²1 + Ïµ1, and Ï€hâˆ—(a1|s2) = Î²2 âˆ’Ïµ2. Now we show
that V Ï€hâˆ—(s1) < V Ï€hMinBest (s1), and thus MinBest attacker is not optimal."
ELSE,0.685131195335277,"V Ï€hMinBest(s1) = (Î²1 âˆ’Ïµ1)
h
(Î²2 âˆ’Ïµ2)r1 + (1 âˆ’Î²2 + Ïµ2)r2
i
+ (1 âˆ’Î²1 + Ïµ1)r3
(20)"
ELSE,0.6865889212827988,"= (Î²1 âˆ’Ïµ1)(Î²2 âˆ’Ïµ2)r1 + (Î²1 âˆ’Ïµ1)(1 âˆ’Î²2 + Ïµ2)r2 + (1 âˆ’Î²1 + Ïµ1)r3
(21)"
ELSE,0.6880466472303207,"V Ï€hâˆ—(s1) = (Î²1 + Ïµ1)
h
(Î²2 âˆ’Ïµ2)r1 + (1 âˆ’Î²2 + Ïµ2)r2
i
+ (1 âˆ’Î²1 âˆ’Ïµ1)r3
(22)"
ELSE,0.6895043731778425,"= (Î²1 + Ïµ1)(Î²2 âˆ’Ïµ2)r1 + (Î²1 + Ïµ1)(1 âˆ’Î²2 + Ïµ2)r2 + (1 âˆ’Î²1 âˆ’Ïµ1)r3
(23)
Therefore,
V Ï€hâˆ—(s1) âˆ’V Ï€hMinBest (s1) = 2Ïµ1(Î²2 âˆ’Ïµ2)r2 + 2Ïµ1(1 âˆ’Î²2 + Ïµ2)r2 âˆ’2Ïµ1r3
(24)"
ELSE,0.6909620991253644,"= 2Ïµ1
h
(Î²2 âˆ’Ïµ2)r2 + (1 âˆ’Î²2 + Ïµ2)r2 âˆ’r3
i
(25)"
ELSE,0.6924198250728864,"< 0 Because of the constraint (19)
(26)"
ELSE,0.6938775510204082,"D.3.2
TYPE II - MAXIMIZE THE WORST (MAXWORST)"
ELSE,0.6953352769679301,"Pattanaik et al. (Pattanaik et al., 2018) point out that only preventing the agent from selecting the
best action does not necessarily result in a low total reward. Instead, Pattanaik et al. (Pattanaik et al.,
2018) propose another objective function which maximizes the probability of selecting the worst
action, i.e.,
hMaxWorst âˆˆargmaxhâˆˆHÏµÏ€h(aâˆ’|s), âˆ€s âˆˆS
(II)"
ELSE,0.6967930029154519,"where aâˆ’refers to the â€œworstâ€ action at state s. Pattanaik et al.(Pattanaik et al., 2018) deï¬ne the
â€œworstâ€ action as the actions with the lowest Q value, which could be ambiguous, since the Q
function is policy-dependent. If a worst policy Ï€âˆ’âˆˆargminÏ€V Ï€(s), âˆ€s âˆˆS is available, one
can use aâˆ’= argminQÏ€âˆ’(s, a). However, in practice, the attacker usually only has access to the
agentâ€™s current policy Ï€, so it can also choose aâˆ’= argminQÏ€(s, a). Note that these two selections
are different, as the agentâ€™s policy Ï€ is usually far away from the worst policy."
ELSE,0.6982507288629738,"Lemma 26 (Optimality of MaxWorst). Denote the set of optimal solutions to objective (II) as
HMaxWorst, which include both versions of MaxWorst attacker formulations as we discussed above.
Then there exist an MDP M and an agent policy Ï€, such that HMaxWorst contains a non-optimal
adversary hâˆ—, i.e., HMaxWorst Ì¸âŠ‚Hâˆ—
Ïµ ."
ELSE,0.6997084548104956,"Proof of Lemma 26.
Case I: Using current policy to compute the target action
We prove this lemma by constructing the MDP in Figure 8 such that for any victim policy, there
exists a reward conï¬guration in which MaxWorst attacker is not optimal.
Here, let r1 = r(s11|s1, a1), r2 = r(s12|s1, a2), r3 = r(s21|s2, a1), r4 = r(s22|s2, a2). Assuming
all the other rewards are zero, transition dynamics are deterministic, and states s11, s12, s21, s22 are
the terminal states. For the sake of simplicity, we also assume that the discount factor here Î³ = 1.
Now given a policy Ï€ such that Ï€(a1|s0) = Î²0, Ï€(a1|s1) = Î²1, and Ï€(a2|s2) = Î²2 (Î²0, Î²1, Î²2 âˆˆ
[0, 1]), consider the Adv-policy-set"
ELSE,0.7011661807580175,"BH
Ïµ (Ï€) =
n
Ï€â€² âˆˆÎ 
 âˆ¥Ï€â€²(Â·|s1)âˆ’Ï€(Â·|s1)âˆ¥< Ïµ0, âˆ¥Ï€â€²(Â·|s1)âˆ’Ï€(Â·|s1)âˆ¥< Ïµ1, âˆ¥Ï€â€²(Â·|s2)âˆ’Ï€(Â·|s2)âˆ¥< Ïµ2,
o
."
ELSE,0.7026239067055393,Published as a conference paper at ICLR 2022
ELSE,0.7040816326530612,"Figure 8: A simple MDP where the ï¬rst version of MaxWorst Attacker cannot ï¬nd the optimal adversary for a
given victim policy."
ELSE,0.7055393586005831,"We could ï¬nd r1, r2, r3, r4 such that the following linear constraints hold:
Î²1r1 + (1 âˆ’Î²1)r2 >Î²2r3 + (1 âˆ’Î²2)r4 â‡â‡’QÏ€(s0, a1) > QÏ€(s0, a2)
(27)
r1 >r2 â‡â‡’QÏ€(s1, a1) > QÏ€(s1, a2)
(28)
r3 >r4 â‡â‡’QÏ€(s2, a1) > QÏ€(s2, a2)
(29)
(Î²1 âˆ’Ïµ1)r1 + (1 âˆ’Î²1 + Ïµ1)r2 <(Î²2 âˆ’Ïµ2)r3 + (1 âˆ’Î²2 + Ïµ2)r4
(30)
Now,
given these constraints,
the perturbed policy given by MaxWorst attaker satisï¬es
Ï€hMaxWorst(a1|s0) = Î²0 âˆ’Ïµ0, Ï€hMaxWorst(a1|s1) = Î²1 âˆ’Ïµ1, and Ï€hMaxWorst(a1|s2) = Î²2 âˆ’Ïµ2. How-
ever, consider another perturbed policy Ï€hâˆ—in Adv-policy-set such that Ï€hâˆ—(a1|s0) = Î²0 + Ïµ0,
Ï€hâˆ—(a1|s1) = Î²1 âˆ’Ïµ1, and Ï€hâˆ—(a1|s2) = Î²2 âˆ’Ïµ2. We will prove that V Ï€hâˆ—(s1) < V Ï€hMaxWorst(s1),
and thus MaxWorst attacker is not optimal.
On the one hand,"
ELSE,0.706997084548105,"V Ï€hMaxWorst(s1) =(Î²0 âˆ’Ïµ0)
h
(Î²1 âˆ’Ïµ1)r1 + (1 âˆ’Î²1 + Ïµ1)r2
i
+ (1 âˆ’Î²0 + Ïµ0)
h
(Î²2 âˆ’Ïµ2)r3 + (1 âˆ’Î²2 + Ïµ2)r4
i (31)"
ELSE,0.7084548104956269,"=(Î²0 âˆ’Ïµ0)(Î²1 âˆ’Ïµ1)r1 + (Î²0 âˆ’Ïµ0)(1 âˆ’Î²1 + Ïµ1)r2
+ (1 âˆ’Î²0 + Ïµ0)(Î²2 âˆ’Ïµ2)r3 + (1 âˆ’Î²0 + Ïµ0)(1 âˆ’Î²2 + Ïµ2)r4
(32)
On the other hand,"
ELSE,0.7099125364431487,"V Ï€hâˆ—(s1) =(Î²0 + Ïµ0)
h
(Î²1 âˆ’Ïµ1)r1 + (1 âˆ’Î²1 + Ïµ1)r2
i
+ (1 âˆ’Î²0 âˆ’Ïµ0)
h
(Î²2 âˆ’Ïµ2)r3 + (1 âˆ’Î²2 + Ïµ2)r4
i (33)"
ELSE,0.7113702623906706,"=(Î²0 + Ïµ0)(Î²1 âˆ’Ïµ1)r1 + (Î²0 + Ïµ0)(1 âˆ’Î²1 + Ïµ1)r2
+ (1 âˆ’Î²0 âˆ’Ïµ0)(Î²2 âˆ’Ïµ2)r3 + (1 âˆ’Î²0 âˆ’Ïµ0)(1 âˆ’Î²2 + Ïµ2)r4
(34)
Therefore,
V Ï€hâˆ—(s1) âˆ’V Ï€hMaxWorst (s1) =2Ïµ0(Î²1 âˆ’Ïµ1)r1 + 2Ïµ0(1 âˆ’Î²1 + Ïµ1)r2
âˆ’2Ïµ0(Î²2 âˆ’Ïµ2)r3 âˆ’2Ïµ0(1 âˆ’Î²2 + Ïµ2)r4
(35)
< 0 Because of the constraint (30)
(36)
Case II: Using worst policy to compute the target action"
ELSE,0.7128279883381924,"Figure 9: A simple MDP where the second version of MaxWorst Attacker cannot ï¬nd the optimal adversary
for a given victim policy."
ELSE,0.7142857142857143,Published as a conference paper at ICLR 2022
ELSE,0.7157434402332361,"Same as before, we construct a MDP where HMaxWorst contains a non-optimal adversary.
Let
r1 = r(s1|s0, a1), r2 = r(s2|s0, a2), r3 = r(s3|s0, a3). Assuming all the other rewards are zero,
transition dynamics are deterministic, and states s1, s2, s3 are the terminal states. For the sake of
simplicity, we also assume that the discount factor here Î³ = 1.
Let pi be the given policy such that Ï€(a1|s0) = Î²1 and Ï€(a2|s0) = Î²2. Now without loss of gener-
ality, we assume r1 > r2 > r3 (âˆ—). Then the worst policy Ï€â€² satisï¬es that Ï€â€²(a3|s0) = 1. Consider"
ELSE,0.717201166180758,"the Adv-policy-set BH
Ïµ (Ï€) =
n
Ï€â€² âˆˆÎ 
 âˆ¥Ï€â€²(Â·|s0) âˆ’Ï€(Â·|s0)âˆ¥1 < Ïµ
o
. Then HMaxWorst =
n
Ï€â€² âˆˆ"
ELSE,0.7186588921282799,"Î 
 Ï€â€²(a3|s0) = (1 âˆ’Î²1 âˆ’Î²2) + Ïµ
o
."
ELSE,0.7201166180758017,"Now consider two policies Ï€h1, Ï€h2 âˆˆHMaxWorst, where Ï€h1(a1|s0) = Î²1, Ï€h1(a2|s0) = Î²2 âˆ’Ïµ,
Ï€h2(a1|s0) = Î²1 âˆ’Ïµ, Ï€h2(a2|s0) = Î²2. Then V Ï€h1(s0) âˆ’V Ï€h2(s0) = Ïµ(r1 âˆ’r2) > 0. Therefore,
Ï€h1 âˆˆHMaxWorst but itâ€™s not optimal."
ELSE,0.7215743440233237,"D.3.3
TYPE III - MINIMIZE Q VALUE (MINQ)."
ELSE,0.7230320699708455,"Another idea of attacking Pattanaik et al. (2018); Zhang et al. (2020a) is to craft perturbations such
that the agent selects actions with minimized Q values at every step, i.e.,"
ELSE,0.7244897959183674,"hMinQ âˆˆargminhâˆˆHÏµ
X"
ELSE,0.7259475218658892,"aâˆˆA Ï€h(a|s) Ë†QÏ€(s, a), âˆ€s âˆˆS
(III)"
ELSE,0.7274052478134111,"where Ë†Q is the approximated Q function of the agentâ€™s original policy. For example, Pattanaik et
al.Pattanaik et al. (2018) directly use the agentâ€™s Q network (of policy Ï€), while the Robust SARSA
(RS) attack proposed by Zhang et al.Zhang et al. (2020a) learns a more stable Q network for the
agentâ€™s policy Ï€. Note that in practice, this type of attack is usually applied to deterministic agents
(e.g., DQN, DDPG, etc), then the objective becomes argminhâˆˆHÏµ Ë†QÏ€(s, Ï€h(s)), âˆ€s âˆˆS Pattanaik
et al. (2018); Zhang et al. (2020a); Oikarinen et al. (2020). In this case, the MinQ attack is equivalent
to the MaxWorst attack with the current policy as the target."
ELSE,0.7288629737609329,"Lemma 27 (Optimality of MinQ). Denote the set of optimal solutions to objective (III) as HMinQ,
which include both versions of MinQ attacker formulations as we discussed above. Then there exist
an MDP M and an agent policy Ï€, such that HMinQ contains a non-optimal adversary hâˆ—, i.e.,
HMinQ Ì¸âŠ‚Hâˆ—
Ïµ ."
ELSE,0.7303206997084548,"Proof of Lemma 26.
Case I: For a deterministic victim
In the deterministic case
hMinQ âˆˆargminhâˆˆHÏµ Ë†QÏ€(s, Ï€h(s)) = argmaxhâˆˆHÏµÏ€h(argmina Ë†QÏ€(s, a)|s), âˆ€s âˆˆS
(IIID)
In this case, the objective is equivalent to objective (II), thus Lemma 27 holds."
ELSE,0.7317784256559767,"Case II: For a stochastic victim
In this case, we consider the MDP in Figure 8 and condition (27) to (30). Then the MinQ objective
gives Ï€hMinQ(a1|s0) = Î²0 âˆ’Ïµ0, Ï€hMinQ(a1|s1) = Î²1 âˆ’Ïµ1, and Ï€hMinQ(a1|s2) = Î²2 âˆ’Ïµ2."
ELSE,0.7332361516034985,"According to the proof of the ï¬rst case of Lemma 26, Ï€hMinQ = Ï€hMaxWorst is not an optimal adversary.
Thus Lemma 27 holds."
ELSE,0.7346938775510204,"D.3.4
TYPE IV - MAXIMIZE DIFFERENCE (MAXDIFF)."
ELSE,0.7361516034985423,"The MAD attack proposed by Zhang et al. (Zhang et al., 2020a) is to maximize the distance between
the perturbed policy Ï€h and the clean policy Ï€, i.e.,
hMaxDiff âˆˆargmaxhâˆˆHÏµDTV[Ï€h(Â·|s)||Ï€(Â·|s)], âˆ€s âˆˆS
(IV)
where TV denotes the total variance distance between two distributions.
In practical imple-
mentations, the TV distance can be replaced by the KL-divergence, as DTV[Ï€h(Â·|s)||Ï€(Â·|s)] â‰¤
(DKL[Ï€h(Â·|s)||Ï€(Â·|s)])2. This type of attack is inspired by the fact that if two policies select ac-
tions with similar action distributions on all the states, then the value of the two policies is also
small (see Theorem 5 in Zhang et al. (2020a))."
ELSE,0.7376093294460642,Published as a conference paper at ICLR 2022
ELSE,0.739067055393586,"Lemma 28 (Optimality of MaxDiff). Denote the set of optimal solutions to objective (IV) as
HMaxDiff. There exist an MDP M and an agent policy Ï€, such that HMaxDiff contains a non-optimal
adversary hâˆ—, i.e., HMaxDiff Ì¸âŠ‚Hâˆ—
Ïµ ."
ELSE,0.7405247813411079,"Proof of Lemma 28. The proof follows from the proof of lemma 25. In the MDP we constructed,
Ï€â€² = Î²1 âˆ’Ïµ1, Ï€hMinBest(a1|s2) = Î²2 âˆ’Ïµ2 is one of the policies that has the maximum KL divergence
from the victim policy within Adv-policy-set. However, as we proved in 25, this is not the optimally
perturbed policy. Therefore, MaxDiff attacker may not be optimal."
ELSE,0.7419825072886297,"E
ADDITIONAL EXPERIMENT DETAILS AND RESULTS"
ELSE,0.7434402332361516,"In this section, we provide details of our experimental settings and present additional experimental
results. Section E.1 describes our implementation details and hyperparameter settings for Atari and
MuJoCo experiments. Section E.2 provide additional experimental results, including experiments
with varying budgets (Ïµ) in Section E.2.1, more comparison between SA-RL and PA-AD in terms
of convergence rate and sensitivity to hyperparameter settings as in Section E.2.3, robust training in
MuJoCo games with fewer training steps in Section E.2.4, attacking performance on robust models
in Atari games in Section E.2.5, as well as robust training results in Atari games in Section E.2.6."
ELSE,0.7448979591836735,"E.1
IMPLEMENTATION DETAILS"
ELSE,0.7463556851311953,"E.1.1
ATARI EXPERIMENTS"
ELSE,0.7478134110787172,"In this section we report the conï¬gurations and hyperparameters we use for DQN, A2C and ACKTR
in Atari environments. We use GeForce RTX 2080 Ti GPUs for all the experiments."
ELSE,0.749271137026239,"DQN Victim
We compare PA-AD algorithm with other attacking algorithms on 7 Atari games.
For DQN, we take the softmax of the Q values Q(s, Â·) as the victim policy Ï€(Â·|s) as in prior
works (Huang et al., 2017). For these environments, we use the wrappers provided by stable-
baselines (Hill et al., 2018), where we clip the environment rewards to be âˆ’1 and 1 during training
and stack the last 4 frames as the input observation to the DQN agent. For the victim agent, we im-
plement Double Q learning (Hado Van Hasselt, 2016) and prioritized experience replay (Tom Schaul
& Silver, 2016). The clean DQN agents are trained for 6 million frames, with a learning rate 0.00001
and the same network architecture and hyperparameters as the ones used in Mnih et al. (2015). In
addition, we use a replay buffer of size 5 Ã— 105. Prioritized replay buffer sampling is used with
Î± = 0.6 and Î² increases from 0.4 to 1 linearly during training. During evaluation, we execute the
agentâ€™s policy without epsilon greedy exploration for 1000 episodes."
ELSE,0.750728862973761,"A2C Victim
For the A2C victim agent, we also use the same preprocessing techniques and con-
volutional layers as the one used in Mnih et al. (2015). Besides, values and policy network share
the same CNN layers and a fully-connected layer with 512 hidden units. The output layer is a cat-
egorical distribution over the discrete action space. We use 0.0007 as the initial learning rate and
apply linear learning rate decay, and we train the victim A2C agent for 10 million frames. During
evaluation, the A2C victim executes a stochastic policy (for every state, the action is sampled from
the categorical distribution generated by the policy network). Our implementation of A2C is mostly
based on an open-source implementation by Kostrikov Kostrikov (2018)."
ELSE,0.7521865889212828,"ACKTR Adversary
To train the director of PA-AD and the adversary in SA-RL, we use
ACKTR (Wu et al., 2017) with the same network architecture as A2C. We train the adversaries
of PA-AD and SA-RL for the same number of steps for a fair comparison. For the DQN victim, we
use a learning rate 0.0001 and train the adversaries for 5 million frames. For the A2C victim, we
use a learning rate 0.0007 and train the adversaries for 10 million frames. Our implementation of
ACKTR is mostly based on an open-source implementation by Kostrikov Kostrikov (2018)."
ELSE,0.7536443148688047,"Heuristic Attackers
For the MinBest attacker, we following the algorithm proposed by Huang
et al. (2017) which uses FGSM to compute adversarial state perturbations. The MinBest + Mo-"
ELSE,0.7551020408163265,Published as a conference paper at ICLR 2022
ELSE,0.7565597667638484,"mentum attacker is implemented according to the algorithm proposed by Korkmaz (2020), and we
set the number of iterations to be 10, the decaying factor Âµ to be 0.5 (we tested 0.01, 0.1, 0.5, 0.9
and found 0.5 is relatively better while the difference is minor). Our implementation of the MinQ
attacker follows the gradient-based attack by Pattanaik et al. (2018), and we also set the number of
iterations to be 10. For the MaxDiff attacker, we refer to Algorithm 3 in Zhang et al. (2020a) with
the number of iterations equal to 10. In addition, we implement a random attacker which perturbs
state s to Ëœs = s + Ïµsign(Âµ), where Âµ is sampled from a standard multivariate Gaussian distribution
with the same dimension as s."
ELSE,0.7580174927113703,"E.1.2
MUJOCO EXPERIMENTS"
ELSE,0.7594752186588921,"For four OpenAI Gym MuJoCo continuous control environments, we use PPO with the original
fully connected (MLP) structure as the policy network to train the victim policy. For robustness
evaluations, the victim and adversary are both trained using PPO with independent value and policy
optimizers. We complete all the experiments on MuJoCo using 32GB Tesla V100."
ELSE,0.760932944606414,"PPO Victim
We directly use the well-trained victim model provided by Zhang et al. (2020a)."
ELSE,0.7623906705539358,"PPO Adversary
Our PA-AD adversary is trained by PPO and we use a grid search of a part
of adversary hyperparameters (including learning rates of the adversary policy network and policy
network, the entropy regularization parameter and the ratio clip Ïµ for PPO) to train the adversary as
powerful as possible. The reported optimal attack result is from the strongest adversary among all
50 trained adversaries."
ELSE,0.7638483965014577,"Other Attackers
For Robust Sarsa (RS) attack, we use the implementation and the optimal RS
hyperparameters from Zhang et al. (2020a) to train the robust value function to attack the victim.
The reported RS attack performance is the best one over the 30 trained robust value functions."
ELSE,0.7653061224489796,"For MaxDiff attack, the maximal action difference attacker is implemented referring to Zhang et al.
(2020a)."
ELSE,0.7667638483965015,"For SA-RL attacker, following Zhang et al. (2021), the hyperparameters is the same as the optimal
hyperparameters of vanilla PPO from a grid search. And the training steps are set for different
environments. For the strength of SA-PPO regularization Îº, we choose from 1 Ã— 10âˆ’6 to 1 and
report the worst-case reward."
ELSE,0.7682215743440233,"Robust Training
For ATLA Zhang et al. (2021), the hyperparameters for both victim policy and
adversary remain the same as those in vanilla PPO training. To ensure sufï¬cient exploration, we run
a small-scale grid search for the entropy bonus coefï¬cient for agent and adversary. The experiment
results show that a larger entropy bonus coefï¬cient allows the agent to learn a better policy for the
continual-improving adversary. In robust training experiments, we use larger training steps in all
the MuJoCo environments to guarantee policy convergence. We train 5 million steps in Hopper,
Walker, and HalfCheetah environments and 10 million steps for Ant. For reproducibility, the ï¬nal
results we reported are the experimental performance of the agent with medium robustness from 21
agents training with the same hyperparameter set."
ELSE,0.7696793002915452,"E.2
ADDITIONAL EXPERIMENT RESULTS"
ELSE,0.7711370262390671,"E.2.1
ATTACKING PERFORMANCE WITH VARIOUS BUDGETS"
ELSE,0.7725947521865889,"In Table 1, we report the performance of our PA-AD attacker under a chosen epsilon across different
environments. To see how PA-AD algorithm performs across different values of Ïµâ€™s, here we select
three Atari environments each for DQN and A2C victim agents and plot the performance of PA-AD
under various Ïµâ€™s compared with the baseline attackers in Figure 10. We can see from the ï¬gures
that our PA-AD universally outperforms baseline attackers concerning various Ïµâ€™s."
ELSE,0.7740524781341108,"In Table 2, we provide the evaluation results of PA-AD under a commonly unused epsilon in four
MuJoCo experiments (Zhang et al. (2020a; 2021)) to show that PA-AD attacker also has the best
attacking performance compared with other attackers under different Ïµâ€™s in Figure 11."
ELSE,0.7755102040816326,Published as a conference paper at ICLR 2022
ELSE,0.7769679300291545,"0
0.2
0.4
0.6
0.8
1
1.2 Â·10âˆ’3 20 40 60 80 100 Ïµ"
ELSE,0.7784256559766763,Average Return
ELSE,0.7798833819241983,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.7813411078717201,(a) DQN Boxing
ELSE,0.782798833819242,"0
0.2
0.4
0.6
0.8
1 Â·10âˆ’3 âˆ’20 âˆ’10 0 10 20 Ïµ"
ELSE,0.7842565597667639,Average Return
ELSE,0.7857142857142857,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.7871720116618076,(b) DQN Pong
ELSE,0.7886297376093294,"0
0.2
0.4
0.6
0.8
1 Â·10âˆ’3 0 1 2 3 4 5 Â·104 Ïµ"
ELSE,0.7900874635568513,Average Return
ELSE,0.7915451895043731,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.793002915451895,(c) DQN RoadRunner
ELSE,0.7944606413994169,"0
0.2
0.4
0.6
0.8
1 Â·10âˆ’3 âˆ’20 âˆ’10 0 10 20 Ïµ"
ELSE,0.7959183673469388,Average Return
ELSE,0.7973760932944607,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.7988338192419825,(d) A2C Pong
ELSE,0.8002915451895044,"0
0.2
0.4
0.6
0.8
1 Â·10âˆ’3 0 100 200 300 400 Ïµ"
ELSE,0.8017492711370262,Average Return
ELSE,0.8032069970845481,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.8046647230320699,(e) A2C Breakout
ELSE,0.8061224489795918,"0
0.2
0.4
0.6
0.8
1 Â·10âˆ’2 0 500 1,000 1,500 Ïµ"
ELSE,0.8075801749271136,Average Return
ELSE,0.8090379008746356,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.8104956268221575,(f) A2C Seaquest
ELSE,0.8119533527696793,"Figure 10: Comparison of different attack methods against DQN and A2C victims in Atari w.r.t. different
budget Ïµâ€™s."
ELSE,0.8134110787172012,"0.02 0.04 0.06 0.08 0.10 0.12 0.14 0 1,000 2,000 3,000 Ïµ"
ELSE,0.814868804664723,Average Return
ELSE,0.8163265306122449,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8177842565597667,(a) PPO Hopper
ELSE,0.8192419825072886,"0.02 0.04 0.06 0.08 0.10 0.12 0.14 1,000 2,000 3,000 4,000 5,000 Ïµ"
ELSE,0.8206997084548106,Average Return
ELSE,0.8221574344023324,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8236151603498543,(b) PPO Walker2d
ELSE,0.8250728862973761,"0.0500.0750.1000.1250.1500.1750.2000.2250.250
âˆ’4,000"
ELSE,0.826530612244898,"âˆ’2,000 0 2,000 4,000 6,000 Ïµ"
ELSE,0.8279883381924198,Average Return
ELSE,0.8294460641399417,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8309037900874635,(c) PPO Ant
ELSE,0.8323615160349854,Figure 11: Comparison of different attack methods against PPO victims in MuJoCo w.r.t. different budget Ïµâ€™s.
ELSE,0.8338192419825073,Published as a conference paper at ICLR 2022
ELSE,0.8352769679300291,"E.2.2
HYPERPARAMETER TEST"
ELSE,0.8367346938775511,"In our Actor-Director Framework, solving an optimal actor is a constraint optimization problem.
Thus, in our algorithm, we instead use Lagrangian relaxation for the actorâ€™s constraint optimization.
In this section, we report the effects of different choices of the relaxation hyperparameter Î» on
the ï¬nal performance of our algorithm. Although we set Î» by default to be 1 and keep it ï¬xed
throughout all of the other experiments, here we ï¬nd that in fact, difference choice of Î» has
only minor impact on the performance of the attacker. This result demonstrates that our PA-AD
algorithm is robust to different choices of relaxation hyperparameters."
ELSE,0.8381924198250729,Table 4: Performance of PA-AD across difference choices of the relaxation hyperparameter Î»
ELSE,0.8396501457725948,"Pong
Boxing
Nature Reward
21 Â± 0
96 Â± 4
Î» = 0.2
âˆ’19 Â± 2
16 Â± 12
Î» = 0.4
âˆ’18 Â± 2
17 Â± 12
Î» = 0.6
âˆ’20 Â± 2
19 Â± 15
Î» = 0.8
âˆ’19 Â± 2
14 Â± 12
Î» = 1.0
âˆ’19 Â± 2
15 Â± 12
Î» = 2.0
âˆ’20 Â± 1
21 Â± 15
Î» = 5.0
âˆ’20 Â± 1
19 Â± 14"
ELSE,0.8411078717201166,(a) Atari
ELSE,0.8425655976676385,"Ant
Walker
Nature Reward
5687 Â± 758
4472 Â± 635
Î» = 0.2
âˆ’2274 Â± 632
897 Â± 157
Î» = 0.4
âˆ’2239 Â± 716
923 Â± 132
Î» = 0.6
âˆ’2456 Â± 853
954 Â± 105
Î» = 0.8
âˆ’2597 Â± 662
872 Â± 162
Î» = 1.0
âˆ’2580 Â± 872
804 Â± 130
Î» = 2.0
âˆ’2378 Â± 794
795 Â± 124
Î» = 5.0
âˆ’2425 Â± 765
814 Â± 140"
ELSE,0.8440233236151603,(b) Mujoco
ELSE,0.8454810495626822,"E.2.3
EMPIRICAL COMPARISON BETWEEN PA-AD AND SA-RL"
ELSE,0.8469387755102041,"In this section, we provide more empirical comparison between PA-AD and SA-RL. Note that PA-
AD and SA-RL are different in terms of their applicable scenarios: SA-RL is a black-box attack
methods, while PA-AD is a white-box attack method. When the victim model is known, we can see
that by a proper exploitation of the victim model, PA-AD demonstrates better attack performance,
higher sample and computational efï¬ciency, as well as higher scalability. Appendix F.2 shows de-
tailed theoretical comparison between SA-RL and PA-AD."
ELSE,0.8483965014577259,"PA-AD has better convergence property than SA-RL. In Figure 12, we plot the learning curves
of SA-RL and PA-AD in the CartPole environment and the Ant environment. Compared with SA-
RL attacker, PA-AD has a higher attacking strength in the beginning and converges much faster. In
Figure 12b, we can see that PA-AD has a â€œwarm-startâ€ (the initial reward of the victim is already
signiï¬cantly reduced) compared with SA-RL attacker which starts from scratch. This is because
PA-AD always tries to maximize the distance between the perturbed policy and the original victim
policy in every step according to the actor function (G). So in the beginning of learning, PA-AD
works similarly to the MaxDiff attacker, while SA-RL works similarly to a random attacker. We also
note that although PA-AD algorithm is proposed particularly for environments that have state spaces
much larger than action spaces, in CartPole where the state dimensions is fewer than the number of
actions, PA-AD still works better than SA-RL because of the distance maximization."
ELSE,0.8498542274052479,"(a) Learning curve of SA-RL and PA-AD attacker
against an A2C victim in CartPole."
ELSE,0.8513119533527697,"(b) Learning curve of SA-RL and PA-AD attacker
against a PPO victim in Ant."
ELSE,0.8527696793002916,"Figure 12: Comparison of convergence rate between SA-RL and PA-AD in Ant and Cartpole. Results are
averaged over 10 random seeds."
ELSE,0.8542274052478134,Published as a conference paper at ICLR 2022
ELSE,0.8556851311953353,"PA-AD is more computationally efï¬cient than SA-RL. Our experiments in Section 6 show that
PA-AD converges to a better adversary than SA-RL given the same number of training steps, which
veriï¬es the sample efï¬ciency of PA-AD. Another aspect of efï¬ciency is based on the computational
resources, including running time and required memory. For RL algorithms, the computation cost
comes from the interaction with the environment (the same for SA-RL and PA-AD) and the pol-
icy/value update. If the state space S is higher-dimensional than the action space A, then SA-RL
requires a larger policy network than PA-AD since SA-RL has a higher-dimensional output, and
thus SA-RL has more network parameters than PA-AD, which require more memory cost and more
computation operations. On the other hand, PA-AD requires to solve an additional optimization
problem deï¬ned by the actor objective (G) or (GD). In our implementation, we use FGSM which
only requires one-step gradient computation and is thus efï¬cient. But if more advanced optimiza-
tion algorithms (e.g. PGD) are used, more computations may be needed. In summary, if S is much
larger than A, PA-AD is more computational efï¬cient than SA-RL; if A is much larger than S, SA-
RL is more efï¬cient than PA-AD; if the sizes of S and A are similar, PA-AD may be slightly more
expensive than SA-RL, depending on the optimization methods selected for the actor."
ELSE,0.8571428571428571,"To verify the above analysis, we compare computational training time for training SA-RL and PA-
AD attackers, which shows that PA-AD is more computationally efï¬cient. Especially on the envi-
ronment with high-dimensional states like Ant, PA-AD takes signiï¬cantly less training time than
SA-RL (and ï¬nds a better adversary than SA-RL), which quantiï¬es the efï¬ciency of our algorithm
in empirical experiments."
ELSE,0.858600583090379,"Method
Hopper
Walker2d
HalfCheetah
Ant
SA-RL
1.80
1.92
1.76
4.88
PA-AD
1.43
1.46
1.40
3.76"
ELSE,0.8600583090379009,"Table 5: Average training time (in hours) of SA-RL and PA-AD in MuJoCo environments, using GeForce
RTX 2080 Ti GPUs. For Hopper, Walker2d and HalfCheetah, SA-RL and PA-AD are both trained for 2 million
steps; for Ant, SA-RL and PA-AD are both trained for 5 million steps"
ELSE,0.8615160349854227,"PA-AD is less sensitive to hyperparameters settings than SA-RL. In addition to better ï¬nal
attacking results and convergence property, we also observe that PA-AD is much less sensitive to
hyerparameter settings compared to SA-RL. On the Walker environment, we run a grid search over
216 different conï¬gurations of hyperparameters, including actor learning rate, critic learning rate,
entropy regularization coefï¬cient, and clipping threshold in PPO. Here for comparison we plot two
histograms of the agentâ€™s ï¬nal attacked results across different hyperparameter conï¬gurations."
ELSE,0.8629737609329446,"(a) SA-RL Attacker
(b) PA-AD Attacker"
ELSE,0.8644314868804664,"Figure 13: Histograms of victim rewards under different hyperparameter settings of SA-RL and PA-AD on
Walker."
ELSE,0.8658892128279884,"The perturbation radius is set to be 0.05, for which the mean reward reported by Zhang et al. (2020a)
is 1086. However, as we can see from this histogram, only one out of the 216 conï¬gurations of SA-
RL achieves an attacking reward within the range 1000-2000, while in most hyperparameter settings,
the mean attacked return lies in the range 4000-4500. In contrast, about 10% hyperparameter settings"
ELSE,0.8673469387755102,Published as a conference paper at ICLR 2022
ELSE,0.8688046647230321,"of PA-AD algorithm are able to reduce the reward to 500-1000, and another 10% settings could
reduce the reward to 1000-2000. Therefore, the performance of PA-AD attacker is generally better
and more robust across different hyperparameter conï¬gurations than SA-RL."
ELSE,0.8702623906705539,"E.2.4
ROBUST TRAINING EFFICIENCY ON MUJOCO BY PA-ATLA"
ELSE,0.8717201166180758,"In the ATLA process proposed by Zhang et al. (2021), one alternately trains an agent and an adver-
sary. As a result, the agent policy may learn to adapt to the speciï¬c type of attacker it encounters
during training. In Table 3, we present the performance of our robust training method PA-ATLA-
PPO compared with ATLA-PPO under different types of attacks during testing. ATLA-PPO uses
SA-RL to train the adversary, while PA-ATLA-PPO uses PA-AD to train the adversary during al-
ternating training. As a result, we can see that ATLA-PPO models perform better under the SA-RL
attack, and PA-ATLA-PPO performs better under the PA-AD attack. However, the advantage of
ATLA-PPO over PA-ATLA-PPO against SA-RL attack is much smaller than the advantage of PA-
ATLA-PPO over ATLA-PPO against PA-AD attack. In addition, our PA-ATLA-PPO models signif-
icantly outperform ATLA-PPO models against other heuristic attack methods, and achieve higher
average rewards across all attack methods. Therefore, PA-ATLA-PPO is generally more robust than
ATLA-PPO."
ELSE,0.8731778425655977,"Furthermore, the efï¬ciency of training an adversary could be the bottleneck in the ATLA Zhang
et al. (2021) process for practical usage. Appendix E.2.3 suggests that our PA-AD generally con-
verges faster than SA-RL. Therefore, when the computation resources are limited, PA-ATLA-PPO
can train robust agents faster than ATLA-PPO. We conduct experiments on continuous control envi-
ronments to empirically show the efï¬ciency comparison between PA-ATLA-PPO and ATLA-PPO.
In Table 6, we show the robustness performance of two ATLA methods with 2 million training steps
for Hopper, Walker and Halfcheetah and 5 million steps for Ant (Compared with results in Table 3,
we have reduced training steps by half or more). It can be seen that our PA-ATLA-PPO models still
signiï¬cantly outperform the original ATLA-PPO models under different types of attacks. More im-
portantly, our PA-ATLA-PPO achieves higher robustness under SA-RL attacks in Walker and Ant,
suggesting the efï¬ciency and effectiveness of our method."
ELSE,0.8746355685131195,"Environment
Ïµ
step(million)
Model
Natural
Reward
RS
Zhang et al. (2020a)
SA-RL
Zhang et al. (2021)
PA-AD
(ours)
Average reward
across attacks"
ELSE,0.8760932944606414,"Hopper
0.075
2
ATLA-PPO
1763 Â± 818
1349 Â± 174
1172 Â± 344
477 Â± 30
999.3"
ELSE,0.8775510204081632,"PA-ATLA-PPO
2164 Â± 121
1720 Â± 490
1119 Â± 123
1024 Â± 188
1287.7"
ELSE,0.8790087463556852,"Walker
0.05
2
ATLA-PPO
3183 Â± 842
2405 Â± 529
2170 Â± 1032
516 Â± 47
1697.0"
ELSE,0.880466472303207,"PA-ATLA-PPO
3206 Â± 445
2749 Â± 106
2332 Â± 198
1072 Â± 247
2051.0"
ELSE,0.8819241982507289,"Halfcheetah
0.15
2
ATLA-PPO
4871 Â± 112
3781 Â± 645
3493 Â± 372
856 Â± 118
2710.0"
ELSE,0.8833819241982507,"PA-ATLA-PPO
5257 Â± 94
4012 Â± 290
3329 Â± 183
1670 Â± 149
3003.7"
ELSE,0.8848396501457726,"Ant
0.15
5
ATLA-PPO
3267 Â± 51
3062 Â± 149
2208 Â± 56
âˆ’18 Â± 100
1750.7"
ELSE,0.8862973760932945,"PA-ATLA-PPO
3991 Â± 71
3364 Â± 254
2685 Â± 41
2403 Â± 82
2817.3"
ELSE,0.8877551020408163,"Table 6: Average episode rewards Â± standard deviation of robust models with fewer training steps under
different evasion attack methods. Results are averaged over 50 episodes. We bold the strongest attack in each
row. The gray cells are the most robust agents with the highest average rewards across all attacks."
ELSE,0.8892128279883382,"E.2.5
ATTACKING ROBUSTLY TRAINED AGENTS ON ATARI"
ELSE,0.89067055393586,"In this section, we show the attack performance of our proposed algorithm PA-AD against DRL
agents that are trained to be robust by prior works (Zhang et al., 2020a; Oikarinen et al., 2020) in
Atari games."
ELSE,0.892128279883382,"Zhang et al. (2020a) propose SA-DQN, which minimizes the action change under possible state
perturbations within â„“p norm ball, i.e., to minimize the extra loss"
ELSE,0.8935860058309038,"RDQN(Î¸) :=
X"
ELSE,0.8950437317784257,"s
max

max
Ë†sâˆˆB(s) max
aÌ¸=aâˆ—QÎ¸(Ë†s, a) âˆ’QÎ¸ (Ë†s, aâˆ—(s)) , âˆ’c

(37)"
ELSE,0.8965014577259475,"where Î¸ refers to the Q network parameters, aâˆ—(s) = argmaxaQÎ¸(a|s), and c is a small constant.
Zhang et al. (2020a) solve the above optimization problem by a convex relaxation of the Q network,
which achieves 100% action certiï¬cation (i.e. the rate that action changes with a constrained state
perturbation) in Pong and Freeway, over 98% certiï¬cation in BankHeist and over 47% certiï¬cation
in RoadRunner under attack budget Ïµ = 1/255."
ELSE,0.8979591836734694,Published as a conference paper at ICLR 2022
ELSE,0.8994169096209913,"Environment
Natural
Reward
Ïµ
Random
MinBest
Huang et al. (2017)"
ELSE,0.9008746355685131,"MinBest +
Momentum
Korkmaz (2020)"
ELSE,0.902332361516035,"MinQ
Pattanaik et al. (2018)
MaxDiff
Zhang et al. (2020a)
PA-AD
(ours)"
ELSE,0.9037900874635568,"SA-DQN
RoadRunner 46440 Â± 5797
1
255 45032 Â± 7125
40422 Â± 8301
43856 Â± 5445
42790 Â± 8456
45946 Â± 8499
38652 Â± 6550"
ELSE,0.9052478134110787,"BankHeist
1237 Â± 11
1
255
1236 Â± 12
1235 Â± 15
1233 Â± 17
1237 Â± 14
1236 Â± 13
1237 Â± 14"
ELSE,0.9067055393586005,"RADIAL
-DQN"
ELSE,0.9081632653061225,"RoadRunner 39102 Â± 13727
1
255 41584 Â± 8351
41824 Â± 7858
42330 Â± 8925
40572 Â± 9988
42014 Â± 8337
38214 Â± 9119"
ELSE,0.9096209912536443,"3
255 23766 Â± 6129
9808 Â± 4345
35598 Â± 8191
39866 Â± 6001
18994 Â± 6451
1366 Â± 3354"
ELSE,0.9110787172011662,"BankHeist
1060 Â± 95
1
255
1037 Â± 103
991 Â± 105
988 Â± 102
1021 Â± 96
1042 Â± 112
999 Â± 100"
ELSE,0.9125364431486881,"3
255
1011 Â± 130
801 Â± 114
460 Â± 310
842 Â± 33
1023 Â± 110
397 Â± 172"
ELSE,0.9139941690962099,"RADIAL
-A3C"
ELSE,0.9154518950437318,"RoadRunner 30854 Â± 7281
1
255 30828 Â± 7297
31296 Â± 7095
31132 Â± 6861
30838 Â± 5743
32038 Â± 6898
30550 Â± 7182"
ELSE,0.9169096209912536,"3
255 30690 Â± 7006
30198 Â± 6075
29936 Â± 5388
29988 Â± 6340
31170 Â± 7453
29768 Â± 5892"
ELSE,0.9183673469387755,"BankHeist
847 Â± 31
1
255
847 Â± 31
847 Â± 33
848 Â± 31
848 Â± 31
848 Â± 31
848 Â± 31"
ELSE,0.9198250728862973,"3
255
848 Â± 31
644 Â± 158
822 Â± 11
842 Â± 33
834 Â± 30
620 Â± 168"
ELSE,0.9212827988338192,"Table 7: Average episode rewards Â± standard deviation of SA-DQN, RADIAL-DQN, RADIAL-A3C robust
agents under different evasion attack methods in Atari environments RoadRunner and BankHeist. All attack
methods use 30-step PGD to compute adversarial state perturbations. Results are averaged over 50 episodes.
In each row, we bold the strongest attack, except for the rows where none of the attacker reduces the reward
signiï¬cantly (which suggests that the corresponding agent is relatively robust).)
Oikarinen et al. (2020) propose another robust training method named RADIAL-RL. By adding a
adversarial loss to the classical loss of the RL agents, and solving the adversarial loss with interval
bound propagation, the proposed RADIAL-DQN and RADIAL-A3C achieve high rewards in Pong,
Freeway, BankHeist and RoadRunner under attack budget Ïµ = 1/255 and Ïµ = 3/255."
ELSE,0.922740524781341,"Implementation of the Robust Agents and Environments.
We directly use the trained SA-DQN
agents provided by Zhang et al. (2020a), as well as RADIAL-DQN and RADIAL-A3C agents pro-
vided by Oikarinen et al. (2020). During test time, the agents take actions deterministically. In order
to reproduce the results in these papers, we use the same environment conï¬gurations as in Zhang
et al. (2020a) and Oikarinen et al. (2020), respectively. But note that the environment conï¬gura-
tions of SA-DQN and RADIAL-RL are simpler versions of the traditional Atari conï¬gurations we
use (described in Appendix E.1.1). Both SA-DQN and RADIAL-RL use a single frame instead
of the stacking as 4 frames. Moreover, SA-DQN restricts the number of actions as 6 (4 for Pong)
in each environment, although the original environments have 18 actions (6 for Pong). The above
simpliï¬cations in environments can make robust training easier since the dimensionality of the input
space is much smaller, and the number of possible outputs is restricted."
ELSE,0.924198250728863,"Attack Methods
In experiments, we ï¬nd that the robust agents are much harder to attack than
vanilla agents in Atari games, as claimed by the robust training papers (Zhang et al., 2020a; Oikari-
nen et al., 2020). A reason is that Atari games have discrete action spaces, and leading an agent
to make a different decision at a state with a limited perturbation could be difï¬cult. Therefore, we
use a 30-step Projected Gradient Descent for all attack methods (with step size Ïµ/10), including
MinBest (Huang et al., 2017) and our PA-AD which use FGSM for attacking vanilla models. Note
that the PGD attacks used by Zhang et al. (2020a) and Oikarinen et al. (2020) in their experiments
are the same as the MinBest-PGD attack we use. For our PA-AD, we use PPO to train the adversary
since PPO is relatively stable. The learning rate is set to be 5e âˆ’4, and the clip threshold is 0.1.
Note that SA-DQN, RADIAL-DQN and RADIAL-A3C agents all take deterministic actions, so we
use the deterministic formulation of PA-AD as described in Appendix C.1. In our implementation,
we simply use a CrossEntropy loss for the actor as in Equation (38).
gD(ba, s) = argminsâ€²âˆˆBÏµ(s)CrossEntropy(Ï€(sâ€²), ba).
(38)"
ELSE,0.9256559766763849,"Experiment Results
In Table 7, we reproduce the results reported by Zhang et al. (2020a)
and Oikarinen et al. (2020), and demonstrate the average rewards gained by these robust agents
under different attacks in RoadRunner and BankHeist. Note that SA-DQN is claimed to be robust to
attacks with budget Ïµ = 1/255, and RADIAL-DQN and RADIAL-A3C are claimed to be relatively
robust against up to Ïµ = 3/255 attacks. (â„“âˆis used in both papers.) So we use the same Ïµâ€™s for these
agents in our experiments."
ELSE,0.9271137026239067,"It can be seen that compared with vanilla agents in Table 1, SA-DQN, RADIAL-DQN and RADIAL-
A3C are more robust due to the robust training processes. However, in some environments, PA-AD
can still decrease the rewards of the agent signiï¬cantly. For example, in RoadRunner with Ïµ ="
ELSE,0.9285714285714286,Published as a conference paper at ICLR 2022
ELSE,0.9300291545189504,"3/255, RADIAL-DQN gets 1k+ reward against our PA-AD attack, although RADIAL-DQN under
other attacks can get 10k+ reward as reported by Oikarinen et al. (2020). In contrast, we ï¬nd that
RADIAL-A3C is relatively robust, although the natural rewards gained by RADIAL-A3C are not as
high as RADIAL-DQN and SA-DQN. Also, as SA-DQN achieves over 98% action certiï¬cation in
BankHeist, none of the attackers is able to noticeably reduce its reward with Ïµ = 1/255."
ELSE,0.9314868804664723,"Therefore, our PA-AD can approximately evaluate the worst-case performance of an RL agent under
attacks with ï¬xed constraints, i.e., PA-AD can serve as a â€œdetectorâ€ for the robustness of RL agents.
For agents that perform well under other attacks, PA-AD may still ï¬nd ï¬‚aws in the models and
decrease their rewards; for agents that achieve high performance under PA-AD attack, they are very
likely to be robust against other attack methods."
ELSE,0.9329446064139941,"E.2.6
IMPROVING ROBUSTNESS ON ATARI BY PA-ATLA"
ELSE,0.934402332361516,"Note that different from SA-DQN (Zhang et al., 2020a) and RADIAL-RL (Oikarinen et al., 2020)
discussed in Appendix E.2.5, we use the traditional Atari conï¬gurations (Mnih et al., 2015) without
any simpliï¬cation (e.g. disabling frame stacking, or restricting action numbers). We aim to im-
prove the robustness of the agents in original Atari environments, as in real-world applications, the
environments could be complex and unchangeable."
ELSE,0.9358600583090378,"Baselines
We propose PA-ATLA-A2C by combining our PA-AD and the ATLA framework pro-
posed by Zhang et al. (2021). We implement baselines including vanilla A2C, adversarially trained
A2C (with MinBest (Huang et al., 2017) and MaxDiff (Zhang et al., 2020a) adversaries attacking
50 frames). SA-A2C (Zhang et al., 2020a) is implemented using SGLD and convex relaxations in
Atari environments."
ELSE,0.9373177842565598,"In Table 6, naive adversarial training methods have unreliable performance under most strong attacks
and SA-A2C is ineffective under PA-AD strongest attack. To provide evaluation using different Ïµ,
we provide the attack rewards of all robust models with different attack budgets Ïµ. Under all at-
tacks with different Ïµ value, PA-ATLA-A2C models outperform all other robust models and achieve
consistently better average rewards across attacks. We can observe that our PA-ATLA-A2C training
method can considerably enhance the robustness in Atari environments."
ELSE,0.9387755102040817,"Model
Natural
Reward
Ïµ
Random
MinBest
Huang et al. (2017)
MaxDiff
Zhang et al. (2020a)
SA-RL
Zhang et al. (2021)
PA-AD
(ours)
Average reward
across attacks"
ELSE,0.9402332361516035,"A2C
vanilla
1228 Â± 93
1/255
1223 Â± 77
972 Â± 99
1095 Â± 107
1132 Â± 30
436 Â± 74
971.6"
ELSE,0.9416909620991254,"3/255
1064 Â± 129
697 Â± 153
913 Â± 164
928 Â± 124
284 Â± 116
777.2"
ELSE,0.9431486880466472,"A2C
(adv: MinBest Huang et al. (2017))
948 Â± 94
1/255
932 Â± 69
927 Â± 30
936 Â± 11
940 Â± 103
704 Â± 19
887.8"
ELSE,0.9446064139941691,"3/255
874 Â± 51
813 Â± 32
829 Â± 27
843 Â± 126
521 Â± 72
774.2"
ELSE,0.9460641399416909,"A2C
(adv: MaxDiff Zhang et al. (2020a))
743 Â± 29
1/255
756 Â± 42
702 Â± 89
752 Â± 79
749 Â± 85
529 Â± 45
697.6"
ELSE,0.9475218658892128,"3/255
712 Â± 109
638 Â± 133
694 Â± 115
686 Â± 110
403 Â± 101
626.6"
ELSE,0.9489795918367347,"SA-A2CZhang et al. (2021)
1029 Â± 152
1/255
1054 Â± 31
902 Â± 89
1070 Â± 42
1067 Â± 18
836 Â± 70
985.8"
ELSE,0.9504373177842566,"3/255
985 Â± 47
786 Â± 52
923 Â± 52
972 Â± 126
644 Â± 153
862.0"
ELSE,0.9518950437317785,"PA-ATLA-A2C
(ours)
1076 Â± 56
1/255
1055 Â± 204
957 Â± 78
1069 Â± 94
1045 Â± 143
862 Â± 106
997.6"
ELSE,0.9533527696793003,"3/255
1026 Â± 78
842 Â± 154
967 Â± 82
976 Â± 159
757 Â± 132
913.6"
ELSE,0.9548104956268222,"Table 8: Average episode rewards Â± standard deviation over 50 episodes of A2C, A2C with adv. training,
SA-A2C and our PA-ATLA-A2C robust models under different evasion attack methods in Atari environment
BankHeist. In each row, we bold the strongest attack. The gray cells are the most robust agents with the
highest average rewards across all attacks."
ELSE,0.956268221574344,"F
ADDITIONAL DISCUSSION OF OUR ALGORITHM"
ELSE,0.9577259475218659,"F.1
OPTIMALITY OF OUR RELAXED OBJECTIVE FOR STOCHASTIC VICTIMS"
ELSE,0.9591836734693877,Proof of Concept: Optimality Evaluation in A Small MDP
ELSE,0.9606413994169096,"We implemented and tested heuristic attacks and our PA-AD in the 2-state MDP example used in
Appendix B.6, and visualize the results in Figure 14. For simplicity, assume the adversaries can
perturb only perturb Ï€ at s1 within a â„“2 norm ball of radius 0.2. And we let all adversaries perturb
the policy directly based on their objective functions. As shown in Figure 14a, all possible ËœÏ€(s1)â€™s
form a disk in the policy simplex, and executing above methods, as well as our PA-AD, leads to 4
different policies on this disk. All these computed policy perturbations are on the boundary of the
policy perturbation ball, justifying our Theorem 4."
ELSE,0.9620991253644315,Published as a conference paper at ICLR 2022
ELSE,0.9635568513119533,"(a)
(b)
(c)"
ELSE,0.9650145772594753,"Figure 14: Comparison of the optimality of different adversaries. (a) The policy perturbation generated for s1
by all attack methods. (b) The values of corresponding policy perturbations. (c) A zoomed in version of (b),
where the values of all possible policy perturbations are rendered. Our method ï¬nds the policy perturbation
that achieves the lowest reward among all perturbations."
ELSE,0.9664723032069971,"As our theoretical results suggest, the resulted value vectors lie on a line segment shown in Fig-
ure 14b and a zoomed in version Figure 14c, where one can see that MinBest, MaxWorst and MAD
all fail to ï¬nd the optimal adversary (the policy with lowest value). On the contrary, our PA-AD
ï¬nds the optimal adversary that achieves the lowest reward over all policy perturbations."
ELSE,0.967930029154519,For Continuous MDP: Optimality Evaluation in CartPole and MountainCar
ELSE,0.9693877551020408,"We provided a comparison between SA-RL and PA-AD in the CartPole environment in Figure 15,
where we can see the SA-RL and PA-AD converge to the same result (the learned SA-RL adversary
and PA-AD adversary have the same attacking performance)."
ELSE,0.9708454810495627,Figure 15: Learning curve of SA-RL and PA-AD attacker against an A2C victim in CartPole.
ELSE,0.9723032069970845,"CartPole has a 4-dimensional state space, and contains 2 discrete actions. Therefore since SA-RL
has an optimal formulation, we expect SA-RL to converge to the optimal adversary in a small MDP
like CartPole. Then the result in Figure 15 suggests that our PA-AD algorithm, although with a
relaxation in the actor optimization, also converges to the optimal adversary with even a faster rate
than SA-RL (the reason is explained in Appendix E.2.3)."
ELSE,0.9737609329446064,"In addition to CartPole, we also run experiments in MountainCar with a 2-dimensional state space
against a DQN victim. The SA-RL attacker reduces the victim reward to -128, and our PA-AD
attacker reduces the victim reward to -199.45 within the same number of training steps. Note that the
lowest reward in MountainCar is -200, so our PA-AD indeed converges to a near-optimal adversary,
while SA-RL fails to converge to a near-optimal adversary. This is because MountainCar is an
environment with relatively spare rewards. The actor in PA-AD utilizes our Theorem 4 and only
focuses on perturbations in the outermost boundary, which greatly reduces the exploration
burden in solving an RL problem. In contrast, SA-RL directly uses RL algorithms to learn the
perturbation, and thus it has difï¬culties in converging to the optimal solution."
ELSE,0.9752186588921283,"F.2
MORE COMPARISON BETWEEN SA-RL AND PA-AD"
ELSE,0.9766763848396501,"We provide a more detailed comparison between SA-RL and PA-AD from the following multiple
aspects to claim our contribution."
ELSE,0.978134110787172,Published as a conference paper at ICLR 2022
SIZE OF THE ADVERSARY MDP,0.9795918367346939,"1. Size of the Adversary MDP
Suppose the original MDP has size |S|, |A| for its state space and action space, respectively. Both
PA-AD and SA-RL construct an adversaryâ€™s MDP and search for the optimal policy in it. But the
adversaryâ€™s MDPs for PA-AD and SA-RL have different sizes."
SIZE OF THE ADVERSARY MDP,0.9810495626822158,"PA-AD: state space is of size |S|, action space is of size R|A|âˆ’1 for a stochastic victim, or |A| for a
deterministic victim.
SA-RL: state space is of size |S|, action space is of size |S|."
SIZE OF THE ADVERSARY MDP,0.9825072886297376,"2. Learning Complexity and Efï¬ciency
When the state space is larger than the action space, which is very common in RL environments, PA-
AD solves a smaller MDP than SA-RL and thus more efï¬cient. In environments with pixel-based
states, SA-RL becomes computationally intractable, while PA-AD still works. It is also important
to note that the actorâ€™s argmax problem in PA-AD further accelerates the convergence, as it rules out
the perturbations that do not push the victim policy to its outermost boundary. Our experiment and
analysis in Appendix E.2.3 verify the efï¬ciency advantage of our PA-AD compared with SA-RL,
even in environments with small state spaces."
OPTIMALITY,0.9839650145772595,"3. Optimality
PA-AD: (1) the formulation is optimal for a deterministic victim policy; (2) for a stochastic victim
policy, the original formulation is optimal, but in practical implementations, a relaxation is used
which may not have optimality guarantees.
SA-RL: the formulation is optimal.
Note that both SA-RL and PA-AD require training an RL attacker, but the RL optimization process
may not converge to the optimal solution, especially in deep RL domains. Therefore, SA-RL and
PA-AD are both approximating the optimal adversary in practical implementations."
KNOWLEDGE OF THE VICTIM,0.9854227405247813,"4. Knowledge of the Victim
PA-AD: needs to know the victim policy (white-box). Note that in a black-box setting, PA-AD can
still be used based on the transferability of adversarial attacks in RL agents, as veriï¬ed by Huang
et al. (2017). But the optimality guarantee of PA-AD does not hold in the black-box setting.
SA-RL: does not need to know the victim policy (black-box).
It should be noted that the white-box setting is realistic and helps in robust training:
(1) The white-box assumption is common in existing heuristic methods.
(2) It is always a white-box process to evaluate and improve the robustness of a given agent, for
which PA-AD is the SOTA method. As discussed in our Ethics Statement, the ultimate goal of ï¬nd-
ing the strongest attacker is to better understand and improve the robustness of RL agents. During
the robust training process, the victim is the main actor one wants to train, so it is a white-box set-
ting. The prior robust training art ATLA (Zhang et al., 2021) uses the black-box attacker SA-RL,
despite the fact that it has white-box access to the victim actor. Since SA-RL does not utilize the
knowledge of the victim policy, it usually has to deal with a more complex MDP and face converg-
ing difï¬culties. In contrast, if one replaces SA-RL with our PA-AD, PA-AD can make good use of
the victim policy and ï¬nd a stronger attacker with the same training steps as SA-RL, as veriï¬ed in
our Section 6 and Appendix E.2.6."
APPLICABLE SCENARIOS,0.9868804664723032,"5. Applicable Scenarios
SA-RL is a good choice if (1) the action space is much larger than the state space in the original
MDP, or the state space is small and discrete; (2) the attacker wants to conduct black-box attacks.
PA-AD is a good choice if (1) the state space is much larger than the state space in the original
MDP; (2) the victim policy is known to the attacker; (3) the goal is to improve the robustness of
oneâ€™s own agent via adversarial training."
APPLICABLE SCENARIOS,0.9883381924198251,"In summary, as we discussed in Section 4, there is a trade-off between efï¬ciency and optimality
in evasion attacks in RL. SA-RL has an optimal RL formulation, but empirical results show that
SA-RL usually do not converge to the optimal adversary in a continuous state space, even in a low-
dimensional state space (e.g. see Appendix F.1 for an experiment in MountainCar). Therefore, the
difï¬culty of solving an adversaryâ€™s MDP is the bottleneck for ï¬nding the optimal adversary. Our
PA-AD, although may sacriï¬ce the theoretical optimality in some cases, greatly reduces the size and
the exploration burden of the attackerâ€™s RL problem (can also be regarded as trading some estimation
bias off for lower variance). Empirical evaluation shows our PA-AD signiï¬cantly outperforms SA-
RL in a wide range of environments."
APPLICABLE SCENARIOS,0.9897959183673469,Published as a conference paper at ICLR 2022
APPLICABLE SCENARIOS,0.9912536443148688,"Though PA-AD requires to have access to the victim policy, PA-AD solves a smaller-sized RL
problem than SA-RL by utilizing the victimâ€™s policy and can be applied on evaluating/improving
the robustness of RL policy. It is possible to let PA-AD work in a black-box setting based on the
transferability of adversarial attacks. For example, in a black-box setting, the attacker can train a
proxy agent in the same environment, and use PA-AD to compute a state perturbation for the proxy
agent, then apply the state perturbation to attack the real victim agent. This is out of the scope of
this paper, and will be a part of our future work."
APPLICABLE SCENARIOS,0.9927113702623906,"F.3
VULNERABILITY OF RL AGENTS"
APPLICABLE SCENARIOS,0.9941690962099126,"It is commonly known that neural networks are vulnerable to adversarial attacks (Goodfellow et al.,
2015). Therefore, it is natural that deep RL policies, which are modeled by neural networks, are
also vulnerable to adversarial attacks (Huang et al., 2017). However, there are few works discussing
the difference between deep supervised classiï¬ers and DRL policies in terms of their vulnerabilities.
In this section, we take a step further and investigate the vulnerability of DRL agents, through a
comparison with standard adversarial attacks on supervised classiï¬ers. Our main conclusion is
that commonly used deep RL policies can be instrinsically much more vulnerable to small-radius
adversarial attacks. The reasons are explained below."
OPTIMIZATION PROCESS,0.9956268221574344,"1. Optimization process
Due to the different loss functions that RL and supervised learning agents are trained on, the size of
robustness radius of an RL policy is much smaller than that of a vision-based classiï¬er.
On the one hand, computer vision-based image classiï¬ers are trained with cross-entropy loss. There-
fore, the classiï¬er is encouraged to make the output logit of the correct label to be larger than the
logits of other labels to maximize the log probability of choosing the correct label. On the other
hand, RL agents, in particular DQN agents, are trained to minimize the Bellman Error instead. Thus
the agent is not encouraged to maximize the absolute difference between the values of different ac-
tions. Therefore, if we assume the two networks are lipschitz continuous and their lipschitz constants
do not differ too much, it is clear that a supervised learning agent has a much larger perturbation
radius than an RL agent."
OPTIMIZATION PROCESS,0.9970845481049563,"To prove our claim empirically, we carried out a simple experiment, we compare the success rate
of target attacks of a well-trained DQN agent on Pong with an image classiï¬er trained on the
CIFAR-10 dataset with similar network architecture. For a fair comparison, we use the same image
preprocessing technique, which is to divide the pixel values by 255 and no further normalization
is applied. On both the image-classiï¬er and DQN model, we randomly sample a target label other
than the model predicted label and run the same 100-step projected gradient descent (PGD) attack
to minimize the cross-entropy loss between the model output and the predicted label. We observe
that for a perturbation radius of 0.005 (lâˆnorm), the success rate of a targeted attack for the
image classiï¬er is only 15%, whereas the success rate of a targeted attack for the DQN model is
100%. This veriï¬es our claim that a common RL policy is much more vulnerable to small-radius
adversarial attacks than image classiï¬ers."
NETWORK COMPLEXITY,0.9985422740524781,"2. Network Complexity
In addition, we also want to point out that the restricted network complexity of those commonly
used deep RL policies could play an important role here. Based on the claim by Madry et al. (2018),
a neural network with greater capacity could have much better robustness, even when trained with
only clean examples. But for the neural network architectures commonly used in RL applications,
the capacity of the networks is very limited compared to SOTA computer vision applications. For
example, the commonly used DQN architecture proposed in Mnih et al. (2015) only has 3 convolu-
tional layers and 2 fully connected layers. But in vision tasks, a more advanced and deeper structure
(e.g. ResNet has 100 layers) is used. Therefore, it is natural that the perturbation radius need for
attacking an RL agent is much smaller than the common radius studied in the supervised evasion
attack and adversarial learning literature."
