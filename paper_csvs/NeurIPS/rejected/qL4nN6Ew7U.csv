Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011682242990654205,"We present Fantasy, an efficient text-to-image generation model marrying the
1"
ABSTRACT,0.002336448598130841,"decoder-only Large Language Models (LLMs) and transformer-based masked im-
2"
ABSTRACT,0.0035046728971962616,"age modeling (MIM). While diffusion models are currently in a leading position in
3"
ABSTRACT,0.004672897196261682,"this task, we demonstrate that with appropriate training strategies and high-quality
4"
ABSTRACT,0.005841121495327103,"data, MIM can also achieve comparable performance. By incorporating pre-trained
5"
ABSTRACT,0.007009345794392523,"decoder-only LLMs as the text encoder, we observe a significant improvement in
6"
ABSTRACT,0.008177570093457943,"text fidelity compared to the widely used CLIP text encoder, enhancing the text-
7"
ABSTRACT,0.009345794392523364,"image alignment. Our training approach involves two stages: 1) large-scale concept
8"
ABSTRACT,0.010514018691588784,"alignment pre-training, and 2) fine-tuning with high-quality instruction-image data.
9"
ABSTRACT,0.011682242990654205,"Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the
10"
ABSTRACT,0.012850467289719626,"competitive performance of Fantasy against state-of-the-art diffusion and autore-
11"
ABSTRACT,0.014018691588785047,"gressive models.
12"
INTRODUCTION,0.015186915887850467,"1
Introduction
13"
INTRODUCTION,0.016355140186915886,DALL¬∑E 2
INTRODUCTION,0.017523364485981307,Fantasy
INTRODUCTION,0.018691588785046728,Pixart-Œ±
INTRODUCTION,0.01985981308411215,SDV1.5
INTRODUCTION,0.02102803738317757,W√úRSTCHEN
INTRODUCTION,0.02219626168224299,ParaDiffusion
INTRODUCTION,0.02336448598130841,"Figure 1: Comparison of data usage,
training time and image quality. Colors
from dark to light represent parameters
increasing in size, and circles from small
to large indicate improvements in image
quality."
INTRODUCTION,0.02453271028037383,"Recent advances in text-to-image (T2I) models [3, 5, 12]
14"
INTRODUCTION,0.02570093457943925,"have become focal points within the computer vision field.
15"
INTRODUCTION,0.026869158878504672,"Most advances in T2I models, focused on generating high-
16"
INTRODUCTION,0.028037383177570093,"quality images based on relatively short descriptions, strug-
17"
INTRODUCTION,0.029205607476635514,"gle with intricate long-text semantic alignment due to in-
18"
INTRODUCTION,0.030373831775700934,"herent structure constraints and data limitations. Text
19"
INTRODUCTION,0.03154205607476635,"encoders used for T2I fall into three categories: CLIP
20"
INTRODUCTION,0.03271028037383177,"[30], encoder-decoder LLMs, and decoder-only LLMs.
21"
INTRODUCTION,0.03387850467289719,"Models using encoder-decoder LLMs like T5-XXL [31]
22"
INTRODUCTION,0.035046728971962614,"have shown improved text-image alignment over CLIP
23"
INTRODUCTION,0.036214953271028034,"by exploiting enhanced text understanding, increasing to-
24"
INTRODUCTION,0.037383177570093455,"ken capacity, yet without delving into the semantic align-
25"
INTRODUCTION,0.038551401869158876,"ment for longer texts. ParaDiffusion [43] indicates that
26"
INTRODUCTION,0.0397196261682243,"directly aligning text embeddings with visual features with-
27"
INTRODUCTION,0.04088785046728972,"out prior image-text knowledge is not the most effective
28"
INTRODUCTION,0.04205607476635514,"approach. Previous works [38, 45] have highlighted short-
29"
INTRODUCTION,0.04322429906542056,"comings in existing text-image datasets [37], including
30"
INTRODUCTION,0.04439252336448598,"image-text mismatches, a lack of informative content, and
31"
INTRODUCTION,0.0455607476635514,"a pronounced long-tail effect. These deficiencies notably
32"
INTRODUCTION,0.04672897196261682,"impair training efficiency for T2I models and restrict their
33"
INTRODUCTION,0.04789719626168224,"ability to learn complex semantic alignment.
34"
INTRODUCTION,0.04906542056074766,"Existing diffusion-based T2I models [33, 5, 9, 26] have achieved unprecedented quality. However,
35"
INTRODUCTION,0.05023364485981308,"as detailed in Fig. 1, these advanced models come with significant computational demands. The
36"
INTRODUCTION,0.0514018691588785,"A snowy Sweden lake in a
vibrant, cinematic style with
intense detail and raytracing."
INTRODUCTION,0.052570093457943924,"A furry cat.
Studio photo portrait of Lain
Iwakura wearing floral garlands
over her traditional dress."
INTRODUCTION,0.053738317757009345,"A tiny planet image of Rio de
Janeiro."
INTRODUCTION,0.054906542056074766,"A 3d render of a cute, blue,
anthropomorphic dragon with
ice crystals growing off her,
sharp focus."
INTRODUCTION,0.056074766355140186,"Majestic ornate great hall,
grand library, baroque, torches,
st a i ne d
g la s s
w i nd o ws ,
moonlight rays, dreamy mood."
INTRODUCTION,0.05724299065420561,"T he
so li ta r y g re a t
tre e
ce n te re d
i n
t he
i ma ge .
cloudless sunny sky. little
islands in the flooded plain."
INTRODUCTION,0.05841121495327103,"Brea t h ta king be a ut if ul,
aesthetically pleasing, gouache
ocean waves ripples, sea foam,
sunset, digital concept art."
INTRODUCTION,0.05957943925233645,"Beautiful warm tavern seen
from the outside, middle age,
river crossed by a bridge next
to the tavern, crepuscular light."
INTRODUCTION,0.06074766355140187,Ted bundy in a pixar movie.
INTRODUCTION,0.06191588785046729,"Figure 2: Samples produced by Fantasy (512 √ó 512). Each image, generated in 1.26 seconds (without
super-resolution models), is accompanied by a descriptive caption showcasing diverse styles and
comprehension."
INTRODUCTION,0.0630841121495327,"considerable expenses of these models create significant barriers for researchers and entrepreneurs.
37"
INTRODUCTION,0.06425233644859812,"Meanwhile, economical text-to-image models [25, 15, 48] compromise on image quality, yielding
38"
INTRODUCTION,0.06542056074766354,"lower resolution and diminished aesthetic appeal.
39"
INTRODUCTION,0.06658878504672897,"Given these challenges, a pivotal question arises: Can we develop a resource-efficient, high-quality
40"
INTRODUCTION,0.06775700934579439,"image generator for long instructions? In this paper, we present Fantasy, significantly reducing
41"
INTRODUCTION,0.0689252336448598,"training demands while maintaining the capability of instruction understanding and competitive
42"
INTRODUCTION,0.07009345794392523,"image generation quality, as shown in Fig. 2. To achieve this, we propose three core designs:
43"
INTRODUCTION,0.07126168224299065,"Efficient T2I netwrok. To leverage the powerful understanding ability of a decoder-only LLM,
44"
INTRODUCTION,0.07242990654205607,"we choose the lightweight Phi-2 [24] as our text encoder. We derive discrete image tokens from a
45"
INTRODUCTION,0.07359813084112149,"pre-trained VQGAN [27], and employ Transformer-based masked image modeling (MIM) as our T2I
46"
INTRODUCTION,0.07476635514018691,"architecture. We also utilize the pre-trained VQGAN decoder [27] for pixel space restoration.
47"
INTRODUCTION,0.07593457943925233,"Hierarchical Training strategy. We propose a thoughtfully two-stage training strategy to address the
48"
INTRODUCTION,0.07710280373831775,"high computational demands of current leading models while maintaining competitive performance:
49"
INTRODUCTION,0.07827102803738317,"(1) large-scale concept alignment pre-training, (2) high-quality instruction-image fine-tuning. To
50"
INTRODUCTION,0.0794392523364486,"facilitate a coarse image-text alignment, we initially train the T2I model from scratch using relatively
51"
INTRODUCTION,0.08060747663551401,"lower-quality data. We then fine-tune the pre-trained T2I model and LLM on text-image pair data
52"
INTRODUCTION,0.08177570093457943,"rich in information density with superior aesthetic quality.
53"
INTRODUCTION,0.08294392523364486,"High-quality data. To achieve rough alignment while pre-training, we select the large-scale dataset
54"
INTRODUCTION,0.08411214953271028,"LAION-2B [37] and employ the filtering strategy proposed by DataComp [14]. We collect long-
55"
INTRODUCTION,0.0852803738317757,"text prompts and corresponding high-quality synthesized images for instruction tuning, including
56"
INTRODUCTION,0.08644859813084112,"DiffusionDB [42] and JourneyDB [39]. We further filter and discard texts with special characters and
57"
INTRODUCTION,0.08761682242990654,"data containing violence or pornography, retaining only instructions exceeding 30 words.
58"
INTRODUCTION,0.08878504672897196,"Our main contributions are summarized as follows:
59"
INTRODUCTION,0.08995327102803738,"1. We present Fantasy, a novel framework that is the first to integrate a lightweight decoder-only
60"
INTRODUCTION,0.0911214953271028,"LLM and a Transformer-based MIM for text-to-image synthesis, allowing for long-form
61"
INTRODUCTION,0.09228971962616822,"text alignment.
62"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.09345794392523364,"2. We show that our two-stage training strategy with high-quality data enables MIM to achieve
63"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.09462616822429906,"comparable performance at a significantly reduced training cost.
64"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.09579439252336448,"3. We provide comprehensive validation of the model‚Äôs efficacy based on automated metrics
65"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.0969626168224299,"and human feedback for visual appeal and text faithfulness.
66"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.09813084112149532,Masked Image
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.09929906542056074,"Generator
ùìî
ùìì √ó ùëµ ùìñ"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10046728971962617,"‚ÄúAn owl character with high detail and 
dramatic lighting digital art headshot.‚Äù
LLM"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10163551401869159,Cosine Mask Cross Attn
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.102803738317757,Super Resolution Model
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10397196261682243,"Phi-2
Projection"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10514018691588785,"Masked Image Generator
VQ ùìì"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10630841121495327,"Stage 1: Large-scale concept alignment pre-training
Stage 2: Instruction fine-tuning VQ ùìî"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10747663551401869,"Phi-2
Projection"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10864485981308411,"Masked Image Generator
VQ ùìì
VQ ùìî"
WE SHOW THAT OUR TWO-STAGE TRAINING STRATEGY WITH HIGH-QUALITY DATA ENABLES MIM TO ACHIEVE,0.10981308411214953,"Figure 3: (Up) Overview of Fantasy featuring text encoder, VQGAN (encoder E and decoder D),
masked image generator G, and super-resolution model. (Down) Our training pipeline involves two
stages. The red parts are trainable and the blue parts are frozen; the yellow part is optionally
utilized during inference."
METHOD,0.11098130841121495,"2
Method
67"
PROBLEM FORMULATION,0.11214953271028037,"2.1
Problem Formulation
68"
PROBLEM FORMULATION,0.1133177570093458,"As depicted in Fig. 3, Fantasy consists of a pre-trained text encoder T , a transformer-based masked
69"
PROBLEM FORMULATION,0.11448598130841121,"image generator G, a sampler S, a frozen VQGAN, and a pre-trained super-resolution model. T
70"
PROBLEM FORMULATION,0.11565420560747663,"maps a text prompt t to a continuous embedding space. G processes a text embedding e to generate
71"
PROBLEM FORMULATION,0.11682242990654206,"logits l for the visual token sequence. S draws a sequence of visual tokens v from logits via iterative
72"
PROBLEM FORMULATION,0.11799065420560748,"decoding [4], which runs N steps of inference conditioned on the text embeddings e and visual tokens
73"
PROBLEM FORMULATION,0.1191588785046729,"decoded from previous steps. Finally, D maps the sequence of discrete tokens to pixel space Z. To
74"
PROBLEM FORMULATION,0.12032710280373832,"summarize, given a text prompt t, an image ÀÜx is synthesized as follows:
75"
PROBLEM FORMULATION,0.12149532710280374,"ÀÜx = D(S(G, T (t))),
ln = G(vn, T (t)),
vn = M(E(x))
(1)"
PROBLEM FORMULATION,0.12266355140186916,"where n is the synthesis step, and ln are logits, from which the next set of visual tokens vn+1 are
76"
PROBLEM FORMULATION,0.12383177570093458,"sampled. M denotes the masking operator that applies masks to the token in vn. We refer to [4, 3]
77"
PROBLEM FORMULATION,0.125,"for details on the iterative decoding process. The Phi-2 [24] for T and VQGAN [8] for encoder E and
78"
PROBLEM FORMULATION,0.1261682242990654,"decoder D are used. G is trained on a large text-image pairs D using masked visual token modeling
79"
PROBLEM FORMULATION,0.12733644859813084,"loss:
80"
PROBLEM FORMULATION,0.12850467289719625,"L = E(x,t)‚àºD [CE (lN, E(x))] ,
(2)
where CE is a weighted cross-entropy calculated by summing only over the unmasked tokens.
81"
MODEL ARCHITECTURE,0.12967289719626168,"2.2
Model Architecture
82"
VQGAN AS IMAGE PROCESSOR,0.1308411214953271,"2.2.1
VQGAN as Image Processor
83"
VQGAN AS IMAGE PROCESSOR,0.13200934579439252,"VQGAN [8] is capable of transforming each image into discrete tokens with higher-level semantic
84"
VQGAN AS IMAGE PROCESSOR,0.13317757009345793,"information from a learned codebook, while ignoring low level noise. The autoregressive tokens
85"
VQGAN AS IMAGE PROCESSOR,0.13434579439252337,"prediction of VQGAN shares the same form as text tokens generated by LLMs. Prior research [46]
86"
VQGAN AS IMAGE PROCESSOR,0.13551401869158877,"has shown that unifying vision and language by the same token space could enhance the coherency
87"
VQGAN AS IMAGE PROCESSOR,0.1366822429906542,"for vision-text alignment. Furthermore, compared with RGB pixels, the visual token representation
88"
VQGAN AS IMAGE PROCESSOR,0.1378504672897196,"has proven to reduce disk storage and improve the capability of robustness and generalization.
89"
VQGAN AS IMAGE PROCESSOR,0.13901869158878505,"To reduce the computational burden, we initially compress an RGB image v ‚ààRH√óW √ó3 into a
90"
VQGAN AS IMAGE PROCESSOR,0.14018691588785046,"diminished representation with a resolution of h √ó w √ó 3, where h = H/f and w = W/f, with
91"
VQGAN AS IMAGE PROCESSOR,0.1413551401869159,"f denoting the downsampling factor. We then employ a pre-trained f16 VQGAN [27] encoder E
92"
VQGAN AS IMAGE PROCESSOR,0.1425233644859813,"to quantizate images x ‚ààR3√ó256√ó256 into discrete tokens of spatial dimensions 16 √ó 16 from a
93"
VQGAN AS IMAGE PROCESSOR,0.14369158878504673,"pre-trained codebook Z = {zk}K
k=1 consisting of K = 8192 vectors, resulting in the quantized
94"
VQGAN AS IMAGE PROCESSOR,0.14485981308411214,"representation z = E(x, Z).
95"
LLM AS TEXT ENCODER,0.14602803738317757,"2.2.2
LLM as Text Encoder
96"
LLM AS TEXT ENCODER,0.14719626168224298,"Recent studies [10, 5, 3] tend to use encoder-decoder LLMs [31] for text encoding over CLIP [30],
97"
LLM AS TEXT ENCODER,0.1483644859813084,"which is adept at handling tasks that involve complex mappings between input and output sequences.
98"
LLM AS TEXT ENCODER,0.14953271028037382,"Due to the tremendous success of ChatGPT, attention has been drawn to models that consist solely of a
99"
LLM AS TEXT ENCODER,0.15070093457943926,"decoder. Also, [43] presents an insight that efficiently fine-tuning a more powerful decoder-only LLM
100"
LLM AS TEXT ENCODER,0.15186915887850466,"can yield stronger performance in long-text alignment. Consequently, to capitalize on the enhanced
101"
LLM AS TEXT ENCODER,0.1530373831775701,"semantic comprehension and generalization potential of LLMs while simultaneously reducing the
102"
LLM AS TEXT ENCODER,0.1542056074766355,"training burden, we employ Phi-2 [24], a state-of-the-art, lightweight LLM, as the text encoder.
103"
LLM AS TEXT ENCODER,0.15537383177570094,"Given the text prompt t, Fantasy first passes it through Phi-2, extracting the text embedding from the
104"
LLM AS TEXT ENCODER,0.15654205607476634,"last hidden layer L. However, typically, decoder-only architectures are not adept at feature extraction
105"
LLM AS TEXT ENCODER,0.15771028037383178,"and mapping tasks. [23] proposes that the conceptual representations learned by LLM‚Äôs are roughly
106"
LLM AS TEXT ENCODER,0.1588785046728972,"linearly mappable to those learned by models trained on vision tasks. Therefore, the embedding
107"
LLM AS TEXT ENCODER,0.16004672897196262,"vectors are linearly projected to the hidden size of the image generator G:
108"
LLM AS TEXT ENCODER,0.16121495327102803,"c = P(TL(t))
(3)"
LLM AS TEXT ENCODER,0.16238317757009346,"where T (¬∑) denotes the decoder-only Phi-2 and L is the index of the last hidden layer. P represents
109"
LLM AS TEXT ENCODER,0.16355140186915887,"the projection from text space to visual space, and c is the text feature suitable for the image generator.
110"
MIM AS IMAGE GENERATOR,0.1647196261682243,"2.2.3
MIM as Image Generator
111"
MIM AS IMAGE GENERATOR,0.1658878504672897,"MIM narrows the gap between its modeling and the extensively studied area of language modeling,
112"
MIM AS IMAGE GENERATOR,0.16705607476635514,"making it straightforward to leverage the findings of the LLMs research community. Therefore, we
113"
MIM AS IMAGE GENERATOR,0.16822429906542055,"adopt a masked transformer as the image generator backbone of Fantasy [46].
114"
MIM AS IMAGE GENERATOR,0.169392523364486,"During training, we leave the projected text embeddings c unmasked and the image tokens z are
115"
MIM AS IMAGE GENERATOR,0.1705607476635514,"masked at a variable masking rate based on a Cosine scheduling M as [4, 3]. Specifically, for
116"
MIM AS IMAGE GENERATOR,0.17172897196261683,"each training example, we sample a masking rate r from [0, 1] from a truncated arccos distribution
117"
MIM AS IMAGE GENERATOR,0.17289719626168223,with density function p(r) = 2
MIM AS IMAGE GENERATOR,0.17406542056074767,œÄ(1 ‚àír2)‚àí1
MIM AS IMAGE GENERATOR,0.17523364485981308,"2 . While autoregressive methods learn fixed-order token
118"
MIM AS IMAGE GENERATOR,0.1764018691588785,"distributions P(zi|z<i), random masking with variable ratios enables learning P(zi|zÃ∏=i) for any
119"
MIM AS IMAGE GENERATOR,0.17757009345794392,"token subset, crucial for our parallel sampling scheme. The sampling of a new state sn+1 at each
120"
MIM AS IMAGE GENERATOR,0.17873831775700935,"successive step is conditioned on the previous state and the specified text condition c:
121"
MIM AS IMAGE GENERATOR,0.17990654205607476,"P(s | c) =
Z
P(sN | sN‚àí1, c) N‚àí1
Y"
MIM AS IMAGE GENERATOR,0.1810747663551402,"n=1
P(sn | sn‚àí1, c) ds1 . . . dsN‚àí1
(4)"
MIM AS IMAGE GENERATOR,0.1822429906542056,"For each training example, the most confidently predicted tokens are revealed at each step n, main-
122"
MIM AS IMAGE GENERATOR,0.18341121495327103,"taining cos
  n N ¬∑ œÄ"
MIM AS IMAGE GENERATOR,0.18457943925233644,"2

masked until reaching N total steps.
123"
MIM AS IMAGE GENERATOR,0.18574766355140188,"For the base model, we use a variant of MaskGiT [4], a masked image generative Transformer to
124"
MIM AS IMAGE GENERATOR,0.18691588785046728,"predict randomly masked tokens by attending to tokens in all directions. Leveraging the multi-layered
125"
MIM AS IMAGE GENERATOR,0.18808411214953272,"structure of the Transformer, we have developed scalable image generators with varying layer counts,
126"
MIM AS IMAGE GENERATOR,0.18925233644859812,"ranging in size from 257M parameters to 611M parameters (for the image generator; the Phi-2 model
127"
MIM AS IMAGE GENERATOR,0.19042056074766356,"has an additional 2.7B parameters). We first employ a series of Cross Attention blocks to optimize
128"
MIM AS IMAGE GENERATOR,0.19158878504672897,"text-driven feature extraction, before passing through O layers of the masked image generator. Each
129"
MIM AS IMAGE GENERATOR,0.1927570093457944,"layer o of the Transformer is again formed by Multi-Head Self-Attentuib(MSA), LayerNorm (LN),
130"
MIM AS IMAGE GENERATOR,0.1939252336448598,"Cross Attention (CA) and Multi-Layer Perceptron (MLP) blocks:
131"
MIM AS IMAGE GENERATOR,0.19509345794392524,"Yo = MSA(LN(Zo)),
Zo+1 = MLP(CA((LN(Yo), c))).
(5)"
MIM AS IMAGE GENERATOR,0.19626168224299065,"At the output layer, to reduce the training burden, ConvMLP [18] is utilized to transform masked
132"
MIM AS IMAGE GENERATOR,0.19742990654205608,"image embeddings into logits sets, aligning with the VQGAN codebook dimensions. Eventually, the
133"
MIM AS IMAGE GENERATOR,0.1985981308411215,"reconstructed lower-resolution tokens are restored with the pre-trained 256 √ó 256 resolution VQGAN
134"
MIM AS IMAGE GENERATOR,0.19976635514018692,"decoder to the pixel space, resulting in the generated image ÀÜx:
135"
MIM AS IMAGE GENERATOR,0.20093457943925233,"ÀÜx = D(ConvMLP(ZO), Z)
(6)"
TRAINING STRATEGY,0.20210280373831777,"2.3
Training Strategy
136"
TRAINING STRATEGY,0.20327102803738317,"Fig. 3 illustrates Fantasy‚Äôs two-stage training approach. Following prior works[43, 35, 9], we employ
137"
TRAINING STRATEGY,0.2044392523364486,"large-scale pre-training to achieve general text-image concept alignment, and simultaneous fine-tuning
138"
TRAINING STRATEGY,0.205607476635514,"of Phi-2 [24] and the masked image generator using high-quality instruction-image pairs.
139"
TRAINING STRATEGY,0.20677570093457945,"Pre-training Stage.
To perform general text-image concept alignment, the VQGAN and LLM
140"
TRAINING STRATEGY,0.20794392523364486,"weights are frozen, and only the image generator is pre-trained on deduplicated LAION-2B [37]
141"
TRAINING STRATEGY,0.2091121495327103,"with images above a 4.5 aesthetic score. We exclusively preserve prompts in English, filter out
142"
TRAINING STRATEGY,0.2102803738317757,"images above a 50% watermark probability or above a 45% NSFW probability, yielding a final set
143"
TRAINING STRATEGY,0.21144859813084113,"of 9 million images. Since the computational cost of upsampling is much lower than training a
144"
TRAINING STRATEGY,0.21261682242990654,"super-resolution model, Fantasy is started with training at a resolution of 256 √ó 256. Note that the
145"
TRAINING STRATEGY,0.21378504672897197,"pre-training only needs approximate image-text alignment, substantially lowering the training costs.
146"
TRAINING STRATEGY,0.21495327102803738,"Fine-tuning Stage.
[43] has proven that LLMs trained solely on text data lack prior image-text
147"
TRAINING STRATEGY,0.2161214953271028,"knowledge, and that merely aligning their text embeddings with visual features might not be optimal.
148"
TRAINING STRATEGY,0.21728971962616822,"Therefore, in the second stage, we gather an internal dataset of 7 million high-quality instruction-
149"
TRAINING STRATEGY,0.21845794392523366,"image pairs to fine-tune both the Phi-2 model and the image generator of Fantasy, which ensures
150"
TRAINING STRATEGY,0.21962616822429906,"enhanced compatibility of text embeddings within the text-image pair space, facilitating the use of
151"
TRAINING STRATEGY,0.2207943925233645,"decoder-only LLMs in text-to-image generation tasks and harnessing their inherent advantages. To
152"
TRAINING STRATEGY,0.2219626168224299,"prevent catastrophic forgetting in LLMs and preserve their understanding abilities during training, we
153"
TRAINING STRATEGY,0.22313084112149534,"select questions from BIG-bench [2] and monitor the common sense question-answering ability of
154"
TRAINING STRATEGY,0.22429906542056074,"Phi-2 in real-time throughout the training process. We construct our training dataset for the fine-tuning
155"
TRAINING STRATEGY,0.22546728971962618,"stage by incorporating JourneyDB [39] and an internal synthetic dataset to enhance the aesthetic
156"
TRAINING STRATEGY,0.2266355140186916,"quality of generated images beyond realistic photographs. To facilitate instruction-image alignment
157"
TRAINING STRATEGY,0.22780373831775702,"learning, we retain only data with descriptions exceeding 30 words, as these provide enough detailed
158"
TRAINING STRATEGY,0.22897196261682243,"insights into the image objects, including attributes and spatial relations.
159"
TRAINING STRATEGY,0.23014018691588786,"With this approach, Fantasy trains a 0.6B parameter T2I model in about 69 A100 GPU days,
160"
TRAINING STRATEGY,0.23130841121495327,"significantly reducing computation compared to existing diffusion-based methods, while maintaining
161"
TRAINING STRATEGY,0.2324766355140187,"comparable visual and numerical fidelity. Throughout this paper, we present a comprehensive
162"
TRAINING STRATEGY,0.2336448598130841,"evaluation of Fantasy‚Äôs efficacy, showcasing the potential in training high-quality transformer-based
163"
TRAINING STRATEGY,0.23481308411214954,"image synthesis models compared to diffusion-based models in future.
164"
HIGH-QUALITY DATA COLLECTION,0.23598130841121495,"2.4
High-quality Data Collection
165"
HIGH-QUALITY DATA COLLECTION,0.2371495327102804,"To ensure rough alignment in the pre-training phase, we utilize the large-scale dataset LAION-2B
166"
HIGH-QUALITY DATA COLLECTION,0.2383177570093458,"[37] and apply the filtering strategy developed by DataComp [14]. Furthermore, we gather long-
167"
HIGH-QUALITY DATA COLLECTION,0.23948598130841123,"text prompts and corresponding high-quality images to achieve finer-grained text-image alignment
168"
HIGH-QUALITY DATA COLLECTION,0.24065420560747663,"through instruction tuning. CapsFusion [47] employs a fine-tuned LLaMA [40] for recaptioning
169"
HIGH-QUALITY DATA COLLECTION,0.24182242990654207,"LAION-2B [37] and LAION-COCO [1]. However, this approach still results in suboptimal image
170"
HIGH-QUALITY DATA COLLECTION,0.24299065420560748,"quality and occasional mismatches between images and text. SAM-LLAVA [5] utilizes LLaVA [20]
171"
HIGH-QUALITY DATA COLLECTION,0.2441588785046729,"to recaption the SAM dataset [17], which leads to images with blurred faces, a consequence of the
172"
HIGH-QUALITY DATA COLLECTION,0.24532710280373832,"dataset‚Äôs inherent face-blurring. Therefore, we shift focus to synthesize images, mainly including
173"
HIGH-QUALITY DATA COLLECTION,0.24649532710280375,"DiffusionDB [42] and JourneyDB [39], produced by Stable Diffusion and MidJourney, respectively.
174"
HIGH-QUALITY DATA COLLECTION,0.24766355140186916,"To augment the diversity of the images, we minimize the use of datasets from specific domains, such
175"
HIGH-QUALITY DATA COLLECTION,0.2488317757009346,"as gaming and anime. Furthermore, we implement filtering to discard texts with special characters
176"
HIGH-QUALITY DATA COLLECTION,0.25,"and data containing violence or pornography, retaining only instructions exceeding 30 words.
177"
EXPERIMENTS,0.25116822429906543,"3
Experiments
178"
EXPERIMENTS,0.2523364485981308,"In this section, we outline detailed training, inference, and evaluation protocols, followed by compre-
179"
EXPERIMENTS,0.25350467289719625,"hensive comparisons across three key metrics.
180"
IMPLEMENTATION DETAILS,0.2546728971962617,"3.1
Implementation Details
181"
IMPLEMENTATION DETAILS,0.2558411214953271,"Training Details.
Different from the prior works [9, 43, 32, 34], we used a lightweight but powerful
182"
IMPLEMENTATION DETAILS,0.2570093457943925,"decoder-only large language model Phi-2 [24] as the text encoder. Diverging from prior approaches
183"
IMPLEMENTATION DETAILS,0.25817757009345793,"that extract a standard and fixed short text tokens, we extend the extraction to 256 tokens to master
184"
IMPLEMENTATION DETAILS,0.25934579439252337,"long-term instruction-image alignment, ensuring precise alignment for more fine-grained prompts.
185"
IMPLEMENTATION DETAILS,0.2605140186915888,"For the entire training process, we train Fantasy on 4√óA100 80G GPUs and set the accumulation
186"
IMPLEMENTATION DETAILS,0.2616822429906542,"step to 2. At different stages, we employ varying learning rate strategies with single-cycle cosine
187"
IMPLEMENTATION DETAILS,0.2628504672897196,"annealing decay. Furthermore, the AdamW optimizer [22] is utilized with a weight decay of 0.01.
188"
IMPLEMENTATION DETAILS,0.26401869158878505,"Fantasy trains a 0.6B parameter T2I model in about 84.5 A100 GPU days, significantly reducing
189"
IMPLEMENTATION DETAILS,0.2651869158878505,"computation compared to existing diffusion-based methods as shown in Fig. 1.
190"
IMPLEMENTATION DETAILS,0.26635514018691586,"Table 1: Evaluation of diffusion (upper) and transformer (down) models on HPSv2. We underline the
highest value and color the first above Fantasy in blue ."
IMPLEMENTATION DETAILS,0.2675233644859813,"Model
Type
Params
Animation
Concept-art
Painting
Photo
DrawBench [36]"
IMPLEMENTATION DETAILS,0.26869158878504673,"GLIDE [25]
Diff
5.0B
23.34 ¬± 0.198
23.08 ¬± 0.174
23.27 ¬± 0.178
24.50 ¬± 0.290
25.05 ¬± 0.84
VQ-Diffusion [15]
Diff
0.37B
24.97 ¬± 0.186
24.70 ¬± 0.149
25.01 ¬± 0.145
25.71 ¬± 0.222
25.44 ¬± 0.83
Latent Diffusion [34]
Diff
1.45B
25.73 ¬± 0.125
25.15 ¬± 0.140
25.25 ¬± 0.178
26.97 ¬± 0.183
26.17 ¬± 0.85
DALL¬∑E 2 [26]
Diff
6.5B
27.34 ¬± 0.175
26.54 ¬± 0.127
26.68 ¬± 0.156
27.24 ¬± 0.198
27.16 ¬± 0.64
Stable Diffusion v1.4 [33]
Diff
0.8B
27.26 ¬± 0.156
26.61 ¬± 0.082
26.66 ¬± 0.143
27.27 ¬± 0.226
27.23 ¬± 0.57
Stable Diffusion v2.0 [33]
Diff
0.8B
27.48 ¬± 0.174
26.89 ¬± 0.076
26.86 ¬± 0.120
27.46 ¬± 0.198
27.31 ¬± 0.68
DeepFloyd-XL [11]
Diff
4.3B
27.64 ¬± 0.108
26.83 ¬± 0.137
26.86 ¬± 0.131
27.75 ¬± 0.171
27.64 ¬± 0.72"
IMPLEMENTATION DETAILS,0.26985981308411217,"LAFITE [48]
Trans
0.075B
24.63 ¬± 0.101
24.38 ¬± 0.087
24.43 ¬± 0.155
25.81 ¬± 0.213
25.23 ¬± 0.72
FuseDream [21]
Trans
-
25.26 ¬± 0.125
25.15 ¬± 0.107
25.13 ¬± 0.183
25.57 ¬± 0.248
25.72 ¬± 0.71
DALL¬∑E mini [7]
Trans
0.4B
26.10 ¬± 0.132
25.56 ¬± 0.137
25.56 ¬± 0.112
26.12 ¬± 0.233
26.34 ¬± 0.76
VQGAN + CLIP [8]
Trans
0.2B
26.44 ¬± 0.152
26.53 ¬± 0.075
26.47 ¬± 0.111
26.12 ¬± 0.210
26.38 ¬± 0.43
CogView2 [12]
Trans
6B
26.50 ¬± 0.129
26.59 ¬± 0.119
26.33 ¬± 0.100
26.44 ¬± 0.271
26.17 ¬± 0.74"
IMPLEMENTATION DETAILS,0.27102803738317754,"Fantasy (ours)
Trans
0.6B
27.03¬±0.131
26.66¬±0.117
26.72¬±0.176
26.80¬±0.174
26.78¬±0.523"
IMPLEMENTATION DETAILS,0.272196261682243,"Table 2: Comparison with recent T2I models. ‚ÄòTrained‚Äô indicates the model develops a text encoder
from scratch, foregoing a pre-trained one."
IMPLEMENTATION DETAILS,0.2733644859813084,"Method
Type
Text Encoder
#Params
#Images
FID-30K (‚Üì)"
IMPLEMENTATION DETAILS,0.27453271028037385,"LDM [34]
Diff
Trained
1.4B
400M
12.64
GLIDE [25]
Diff
Trained
5.0B
-
12.24
DALL¬∑E 2 [26]
Diff
CLIP
6.5B
650M
10.39
Stable Diffusion v1.5 [33]
Diff
CLIP
0.9B
2000M
9.62
SD XL [29]
Diff
CLIP
2.6B
-
>18
W√ºrstchen [28]
Diff
CLIP
0.99B
1420M
23.6
ParaDiffusion [43]
Diff
LLaMA V2
1.3B
>300M
9.64
Pixart-Œ± [5]
Diff
T5
0.6B
-
5.51"
IMPLEMENTATION DETAILS,0.2757009345794392,"Cogview2 [12]
Trans
CogLM
6B
35M
24.0
Muse [3]
Trans
T5-XXL
3B
460M
7.88"
IMPLEMENTATION DETAILS,0.27686915887850466,"Fantasy
Trans
Phi-2
0.6B
16M
23.4"
IMPLEMENTATION DETAILS,0.2780373831775701,"Inference Details.
We use N = 32 sampling steps in all of our evaluation experiments. Since
191"
IMPLEMENTATION DETAILS,0.27920560747663553,"Fantasy is trained at a resolution of 256 √ó 256, we employ the pre-trained diffusion-based super-
192"
IMPLEMENTATION DETAILS,0.2803738317757009,"resolution model StableSR [41] to upscale images to 512 √ó 512.
193"
IMPLEMENTATION DETAILS,0.28154205607476634,"Evaluation Metrics.
We comprehensively evaluate Fantasy via four primary metrics, i.e., alignment
194"
IMPLEMENTATION DETAILS,0.2827102803738318,"on HPSv2 [44], FID [16] on MSCOCO dataset [19] and human evaluation on a collected dataset.
195"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2838785046728972,"3.2
Performance Comparisons and Analysis
196"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2850467289719626,"Results on HPSv2.
We utilize HPSv2 [44] as our primary automated metric, a preference prediction
197"
PERFORMANCE COMPARISONS AND ANALYSIS,0.286214953271028,"model which can be used to compare images generated with the same prompt across five categories:
198"
PERFORMANCE COMPARISONS AND ANALYSIS,0.28738317757009346,"anime, concept art, paintings, photography, and DrawBench [36]. We present the results of HPSv2
199"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2885514018691589,"between Fantasy and other state-of-the-art generative models in Tab. 1. Fantasy exhibited outstanding
200"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2897196261682243,"performance across all key aspects among previous Transformer-based methods like CogView2
201"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2908878504672897,"[12], which is expected. The results also reveal its competitive performance compared to prior
202"
PERFORMANCE COMPARISONS AND ANALYSIS,0.29205607476635514,"diffusion-based methods, especially in concept-art and painting, demonstrating similar performance
203"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2932242990654206,"to DALL¬∑E 2 [26]. This remarkable performance is primarily attributed to the text-image alignment
204"
PERFORMANCE COMPARISONS AND ANALYSIS,0.29439252336448596,"learning in fine-tuning stage, where high-quality text-image pairs were leveraged to achieve superior
205"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2955607476635514,"alignment capabilities. In comparison, DeepFloyd-XL and other diffusion-based models achieve
206"
PERFORMANCE COMPARISONS AND ANALYSIS,0.2967289719626168,"better scores, while utilizing larger models with significantly higher compute budget.
207"
PERFORMANCE COMPARISONS AND ANALYSIS,0.29789719626168226,"Results on FID.
We employ FID [16] to evaluate our models on COCO-30K [19]. To allow for
208"
PERFORMANCE COMPARISONS AND ANALYSIS,0.29906542056074764,"a fair comparison, all images are downsampled to 256 √ó 256 pixels. The comparison between our
209"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3002336448598131,"method and other methods in FID, and their training time is summarized in Tab. 2. We observe
210"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3014018691588785,"that the FID of Fantasy is substantially higher compared to other state-of-the-art models. Visual
211"
PERFORMANCE COMPARISONS AND ANALYSIS,0.30257009345794394,"inspections reveal that images generated by Fantasy are smoother than those from other leading T2I
212"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3037383177570093,"models. This discrepancy is most noticeable in real-world images like COCO, on which we compute
213"
PERFORMANCE COMPARISONS AND ANALYSIS,0.30490654205607476,"the FID-metric. Although the state-of-the-art models [43, 11, 29] exhibit lower FID, it relies on
214"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3060747663551402,"unaffordable resources. Furthermore, prior studies [29, 5, 11] have demonstrated that FID may not
215"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3072429906542056,"Visual Appeal
Text-image Alignment"
PERFORMANCE COMPARISONS AND ANALYSIS,0.308411214953271,(a) User study on long prompts.
PERFORMANCE COMPARISONS AND ANALYSIS,0.30957943925233644,"Visual Appeal
Text-image Alignment"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3107476635514019,(b) User study on short prompts.
PERFORMANCE COMPARISONS AND ANALYSIS,0.3119158878504673,"Figure 4: User study on prompts with different length. VC. , CV2. , FT. , SD. , and PA. refer to
VQGAN+CLIP [8], CogView2 [12], our Fantasy, Stable Diffusion v2.0 [33], and Pixart-Œ± [5]."
PERFORMANCE COMPARISONS AND ANALYSIS,0.3130841121495327,"be an appropriate metric for image quality evaluation, as a lower score does not necessarily reflect
216"
PERFORMANCE COMPARISONS AND ANALYSIS,0.3142523364485981,"superior image generation, and it is more authoritative to use the evaluation of human users.
217"
RESULTS ON HUMAN EVALUATION,0.31542056074766356,"3.3
Results on Human Evaluation
218"
RESULTS ON HUMAN EVALUATION,0.316588785046729,"Following prior works [5, 43, 28], we also conduct a study with human participants to supplement
219"
RESULTS ON HUMAN EVALUATION,0.3177570093457944,"our evaluation and provide a more intuitive assessment of Fantasy‚Äôs performance. Participants are
220"
RESULTS ON HUMAN EVALUATION,0.3189252336448598,"asked to select a preference of the images based on the visual appeal of the generated images and the
221"
RESULTS ON HUMAN EVALUATION,0.32009345794392524,"precision of alignments between the text prompts and the corresponding images.
222"
RESULTS ON HUMAN EVALUATION,0.3212616822429907,"As involving human evaluators can be time-consuming, we choose the top-performing open-source
223"
RESULTS ON HUMAN EVALUATION,0.32242990654205606,"diffusion-based models (e.g., SD XL [33], and Pixart-Œ± [5]) and transformer-based models (e.g.,
224"
RESULTS ON HUMAN EVALUATION,0.3235981308411215,"VQGAN+CLIP [8] and CogView2 [12]) as our baseline, which are accessible through APIs and
225"
RESULTS ON HUMAN EVALUATION,0.3247663551401869,"capable of generating images. We randomly select a total of 600 prompts from existing prompt
226"
RESULTS ON HUMAN EVALUATION,0.32593457943925236,"sets (e.g., ParaPrompt [43], ViLG-300 [13], COCO Captions [6]). To comprehensively contrast the
227"
RESULTS ON HUMAN EVALUATION,0.32710280373831774,"capabilities of Fantasy and other models in interpreting text prompts of varying lengths, we allocate
228"
RESULTS ON HUMAN EVALUATION,0.3282710280373832,"one subset to consist of 300 prompts ranging from 10 to 30 characters and another subset comprising
229"
RESULTS ON HUMAN EVALUATION,0.3294392523364486,"300 prompts exceeding 30 characters. For each model, we use a consistent set to generate images,
230"
RESULTS ON HUMAN EVALUATION,0.33060747663551404,"which are then evaluated by 50 individuals.
231"
RESULTS ON HUMAN EVALUATION,0.3317757009345794,"Fig. 4a clearly demonstrates that images generated on relatively long text prompts (longer than 30
232"
RESULTS ON HUMAN EVALUATION,0.33294392523364486,"words) by Fantasy are distinctly favored among the four models in both two perspective, especially
233"
RESULTS ON HUMAN EVALUATION,0.3341121495327103,"for text-image alignment, aligning closely with the intended use case of Fantasy. As illustrated
234"
RESULTS ON HUMAN EVALUATION,0.3352803738317757,"in Fig. 4b, for text prompts shorter than 30 words, our model outperforms existing open-source
235"
RESULTS ON HUMAN EVALUATION,0.3364485981308411,"Transformer-based models in fidelity and alignment for shorter prompts. Our model slightly lags
236"
RESULTS ON HUMAN EVALUATION,0.33761682242990654,"behind diffusion-based models in visual appeal, limited by the 8,192 size of VQGAN‚Äôs codebook
237"
RESULTS ON HUMAN EVALUATION,0.338785046728972,"and not targeting visual appeal. Simultaneously, Fantasy lacks a distinct advantage in text-image
238"
RESULTS ON HUMAN EVALUATION,0.3399532710280374,"alignment in the short subset. We hypothesize that this is due to two main reasons: diffusion-
239"
RESULTS ON HUMAN EVALUATION,0.3411214953271028,"based models‚Äô ability to handle shorter prompts, and vague prompts generating diverse images that
240"
RESULTS ON HUMAN EVALUATION,0.3422897196261682,"make preferences more subjective, thus biasing outcomes towards aesthetically superior images. In
241"
RESULTS ON HUMAN EVALUATION,0.34345794392523366,"summary, the human preference experiments confirm the observation made in the HPSv2 benchmarks.
242"
CASE STUDY,0.3446261682242991,"3.4
Case Study
243"
CASE STUDY,0.34579439252336447,"A close-up photo of a person. The
subject is a male. He was wearing a
wide-brimmed hat, a gray-white beard
on his face, a brown coat. His facial
expression looked pensive and serious,
with
the
clear
blue
sky
in
the
background."
CASE STUDY,0.3469626168224299,"Fantasy
ParaDiffusion"
CASE STUDY,0.34813084112149534,"A young man wearing a black leather
jacket and tie stood behind an old door,
his gaze firmly fixed on the camera.
The door had patterns of leaves and
flowers
on
it,
revealing
a
yellow
background. His hair was casually
curled and he appeared to be deep in
thought or contemplating something.
Fantasy
ParaDiffusion"
CASE STUDY,0.3492990654205608,"Figure 6: Visual Comparison with ParaDiffusion [43]:
Red markings and boxes highlight text misalign-
ments in images generated by ParaDiffusion."
CASE STUDY,0.35046728971962615,"Fig. 5 vividly illustrates Fantasy‚Äôs supe-
244"
CASE STUDY,0.3516355140186916,"rior visual appeal and text-image alignment
245"
CASE STUDY,0.352803738317757,"over leading open-source transformer-based
246"
CASE STUDY,0.35397196261682246,"T2I models [12, 8] and diffusion-based T2I
247"
CASE STUDY,0.35514018691588783,"models [29, 26]. Fantasy significantly sur-
248"
CASE STUDY,0.35630841121495327,"passes existing transformer-based T2I mod-
249"
CASE STUDY,0.3574766355140187,"els, matches the performance of SDXL [29],
250"
CASE STUDY,0.35864485981308414,"and qualitatively outperforms Dall¬∑E 2 [26].
251"
CASE STUDY,0.3598130841121495,"Despite being trained on images with a res-
252"
CASE STUDY,0.36098130841121495,"olution of 256 √ó 256, Fantasy ensures gener-
253"
CASE STUDY,0.3621495327102804,"ated low-resolution images contain sufficient
254"
CASE STUDY,0.3633177570093458,"details, indirectly supporting long prompts.
255"
CASE STUDY,0.3644859813084112,"Limited by computing resources, we haven‚Äôt
256"
CASE STUDY,0.36565420560747663,"VQGAN+CLIP
Cogview2
Fantasy
SD XL
Pixart-ùõº"
CASE STUDY,0.36682242990654207,"(a)
(c)
(g)
(e)
(d)
(f)
(b)"
CASE STUDY,0.3679906542056075,"Figure 5: Visual comparison with existing T2I models. (a) A hamster resembling a horse. (b) A
frontal portrait of a anime girl with chin length pink hair wearing sunglasses and a white T-shirt
smiling. (c) A colorful illustration of a suburban neighborhood on an ancient post-apocalyptic planet
featuring creatures made by Jim Henson‚Äôs workshop. (d) A blue-haired girl with soft features stares
directly at the camera in an extreme close-up Instagram picture. (e) A building in a landscape by
Ivan Aivazovsky. (f) Aoshima‚Äôs masterpiece depicts a forest illuminated by morning light. (g) The
image is a highly detailed portrait of an oak in GTA V, created using Unreal Engine and featuring
fantasy artwork by various artists."
CASE STUDY,0.3691588785046729,"Table 3: Ablation study on two stages with the best bolded. ‚ÄòBase‚Äô indicates the model after the
pre-training stage."
CASE STUDY,0.3703271028037383,"Model
Training Part
Animation
Concept-art
Painting
Photo
DrawBench [36]"
CASE STUDY,0.37149532710280375,"Base
MIM
25.27 ¬± 0.190
24.20 ¬± 0.166
24.60 ¬± 0.146
25.32 ¬± 0.208
25.49 ¬± 0.230
Fantasy
MIM+Phi-2
27.03¬±0.131
26.66¬±0.117
26.72¬±0.176
26.80¬±0.174
26.78¬±0.521"
CASE STUDY,0.3726635514018692,"trained on higher resolutions like 512 √ó 512 but aim to enhance Fantasy by training at higher
257"
CASE STUDY,0.37383177570093457,"resolutions in the future.
258"
CASE STUDY,0.375,"ParaDiffusion [43] pioneers the use of decoder-only large language models as text encoders in
259"
CASE STUDY,0.37616822429906543,"text-to-image generation. As illustrated in Fig. 6, our observations suggest that Fantasy more closely
260"
CASE STUDY,0.3773364485981308,"aligns details with prompts than ParaDiffusion [43].
261"
ABLATION STUDY,0.37850467289719625,"4
Ablation Study
262"
ABLATION STUDY,0.3796728971962617,"This section analyzes the effects of LLMs fine-tuning, and model scale on Fantasy‚Äôs performance
263"
ABLATION STUDY,0.3808411214953271,"through ablation studies. More ablation study refers to appendix.
264"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3820093457943925,"4.1
Effect of Language Model Fine-tuning
265"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.38317757009345793,"To assess the effect of training strategies on the comprehension of complex instructions, we perform
266"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.38434579439252337,"a human preference evaluation, as detailed in Sec. 3.3, using a subset of 300 prompts longer than
267"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3855140186915888,"30 characters. ‚ÄòBase‚Äô denotes general text-image alignment with filtered LAION-2B [1] in the
268"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3866822429906542,"pre-training stage. Compared to the base model, our synergy fine-tuning with Phi-2 demonstrates a
269"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3878504672897196,"notable improvement in all aspects in Tab. 3.
270"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.38901869158878505,"Table 4: Ablation study on models at different scales with the
best bolded. DB. represents DrawBench [36]."
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3901869158878505,"Layers Param
Animation
Concept-art
Painting
Photo
DB."
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.39135514018691586,"6
257M 25.79¬±0.15 25.84¬±0.11 25.92¬±0.19 25.63¬±0.18 25.18¬±0.22
12
421M 26.34¬±0.17 26.29¬±0.06 26.45¬±0.17 26.19¬±0.17 25.68¬±0.14
22
611M 27.03¬±0.13 26.66¬±0.11 26.72¬±0.17 26.80¬±0.17 26.78¬±0.52"
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.3925233644859813,"Table 5: Training cost for Fantasy at
3 different scales. BS. denotes batch
size and LR. denotes learning rate."
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.39369158878504673,"Layers
Pre-training
Fine-tuning
Steps (K)
BS.
LR.
Steps (K)
BS.
LR."
EFFECT OF LANGUAGE MODEL FINE-TUNING,0.39485981308411217,"6
180
768
1e-4
180
192 1e-4
12
220
768
1e-4
250
192 1e-4
22
370
256
5e-4
280
128 3e-4 271"
SCALE OF IMAGE GENERATOR,0.39602803738317754,"4.2
Scale of Image Generator
272"
SCALE OF IMAGE GENERATOR,0.397196261682243,"L6
L12
L22"
SCALE OF IMAGE GENERATOR,0.3983644859813084,A wooden outhouse sitting in the grass near trees.
SCALE OF IMAGE GENERATOR,0.39953271028037385,A small kitchen does have plenty of cabinets.
SCALE OF IMAGE GENERATOR,0.4007009345794392,"L6
L12
L22"
SCALE OF IMAGE GENERATOR,0.40186915887850466,"Figure 7: Examples generated by mod-
els at different scales: 1st column for 6
layers, 2nd column for 12 layers and 3rd
column for 22 layers."
SCALE OF IMAGE GENERATOR,0.4030373831775701,"The hierarchical structure of the Transformer allows us
273"
SCALE OF IMAGE GENERATOR,0.40420560747663553,"to train image generators with varying numbers of Trans-
274"
SCALE OF IMAGE GENERATOR,0.4053738317757009,"former layers. As shown in Tab. 4, we evaluate models
275"
SCALE OF IMAGE GENERATOR,0.40654205607476634,"of different sizes on the HPSv2 benchmark. The insight
276"
SCALE OF IMAGE GENERATOR,0.4077102803738318,"indicates that as trainable parameters increase from 257
277"
SCALE OF IMAGE GENERATOR,0.4088785046728972,"million to 611 million, performance consistently improves.
278"
SCALE OF IMAGE GENERATOR,0.4100467289719626,"Therefore, we set the number of Transformer layers to 22
279"
SCALE OF IMAGE GENERATOR,0.411214953271028,"with 611 million trainable parameters as the optimal set-
280"
SCALE OF IMAGE GENERATOR,0.41238317757009346,"ting. Tab. 5 showcases the required resources for models
281"
SCALE OF IMAGE GENERATOR,0.4135514018691589,"of three different scales. Fig. 7 offers visual comparisons
282"
SCALE OF IMAGE GENERATOR,0.4147196261682243,"across models of varying scales, illustrating a clear trend:
283"
SCALE OF IMAGE GENERATOR,0.4158878504672897,"models with fewer parameters underperform on the HPSv2
284"
SCALE OF IMAGE GENERATOR,0.41705607476635514,"benchmark, frequently resulting in distorted images and
285"
SCALE OF IMAGE GENERATOR,0.4182242990654206,"omitted details, yet they may still generate acceptable
286"
SCALE OF IMAGE GENERATOR,0.41939252336448596,"outcomes. Significantly, the visual quality diverges as
287"
SCALE OF IMAGE GENERATOR,0.4205607476635514,"model size increases, highlighting the potential for scaling
288"
SCALE OF IMAGE GENERATOR,0.4217289719626168,"up masked image modeling to enhance instruction-image
289"
SCALE OF IMAGE GENERATOR,0.42289719626168226,"alignment and elevate generation quality.
290"
LIMITATIONS AND SOCIAL IMPACT,0.42406542056074764,"5
Limitations and Social Impact
291"
LIMITATIONS AND SOCIAL IMPACT,0.4252336448598131,"Limitations. Despite Fantasy achieving competitive performance in text-image alignment and visual
292"
LIMITATIONS AND SOCIAL IMPACT,0.4264018691588785,"appeal, it requires improvements in handling complex scenes. We propose two possible strategies to
293"
LIMITATIONS AND SOCIAL IMPACT,0.42757009345794394,"overcome the challenge in future research: Firstly, augmenting the dataset with high-quality images
294"
LIMITATIONS AND SOCIAL IMPACT,0.4287383177570093,"can enhance diversity and refine the model. Secondly, since the scale of the masked image generator
295"
LIMITATIONS AND SOCIAL IMPACT,0.42990654205607476,"affects instruction-image alignment, training an upscale image generator based on higher resolution
296"
LIMITATIONS AND SOCIAL IMPACT,0.4310747663551402,"left further explored.
297"
LIMITATIONS AND SOCIAL IMPACT,0.4322429906542056,"Social Impact. Generative models for media bring both benefits and challenges. They foster creativity
298"
LIMITATIONS AND SOCIAL IMPACT,0.433411214953271,"and make technology more accessible, yet pose risks by facilitating the creation of manipulated
299"
LIMITATIONS AND SOCIAL IMPACT,0.43457943925233644,"content, spreading misinformation, and exacerbating biases, particularly affecting women with deep
300"
LIMITATIONS AND SOCIAL IMPACT,0.4357476635514019,"fakes. Concerns also include the potential exposure of sensitive training data collected without
301"
LIMITATIONS AND SOCIAL IMPACT,0.4369158878504673,"consent. Despite generative models potentially offering better data representation, the impact of
302"
LIMITATIONS AND SOCIAL IMPACT,0.4380841121495327,"combining adversarial training with likelihood-based objectives on data distortion remains a crucial
303"
LIMITATIONS AND SOCIAL IMPACT,0.4392523364485981,"research area. Ethical considerations of these models are significant and require thorough exploration.
304"
CONCLUSION,0.44042056074766356,"6
Conclusion
305"
CONCLUSION,0.441588785046729,"In this paper, we introduce Fantasy, a lightweight and efficient text-to-image model that combines
306"
CONCLUSION,0.4427570093457944,"Large Language Models (LLMs) with a transformer-based masked image modeling (MIM), effec-
307"
CONCLUSION,0.4439252336448598,"tively transferring semantic understanding capabilities from LLMs to the text-to-image generation.
308"
CONCLUSION,0.44509345794392524,"With our proposed two-stage training strategy and high-quality dataset, Fantasy significantly re-
309"
CONCLUSION,0.4462616822429907,"duces computational requirements while producing high-fidelity images. Extensive experiments
310"
CONCLUSION,0.44742990654205606,"demonstrate that Fantasy achieves comparable performance to models trained with significantly more
311"
CONCLUSION,0.4485981308411215,"computational resources, illustrating the viability of our approach and suggesting potential efficient
312"
CONCLUSION,0.4497663551401869,"scalability to even larger masked image modeling for text-to-image generation.
313"
REFERENCES,0.45093457943925236,"References
314"
REFERENCES,0.45210280373831774,"[1] K√∂pf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain. Laion coco: 600m synthetic
315"
REFERENCES,0.4532710280373832,"captions from laion2b-en.[eb/ol], 2022.
316"
REFERENCES,0.4544392523364486,"[2] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language
317"
REFERENCES,0.45560747663551404,"models. Transactions on Machine Learning Research, 2023.
318"
REFERENCES,0.4567757009345794,"[3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
319"
REFERENCES,0.45794392523364486,"Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked
320"
REFERENCES,0.4591121495327103,"generative transformers. arXiv preprint arXiv:2301.00704, 2023.
321"
REFERENCES,0.4602803738317757,"[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image
322"
REFERENCES,0.4614485981308411,"transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
323"
REFERENCES,0.46261682242990654,"pages 11315‚Äì11325, 2022.
324"
REFERENCES,0.463785046728972,"[5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James
325"
REFERENCES,0.4649532710280374,"Kwok, Ping Luo, Huchuan Lu, et al. Fast training of diffusion transformer for photorealistic text-to-image
326"
REFERENCES,0.4661214953271028,"synthesis. arXiv preprint arXiv:2310.00426, 2023.
327"
REFERENCES,0.4672897196261682,"[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and
328"
REFERENCES,0.46845794392523366,"C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arxiv 2015. arXiv
329"
REFERENCES,0.4696261682242991,"preprint arXiv:1504.00325, 2015.
330"
REFERENCES,0.47079439252336447,"[7] Craiyon. Dall¬∑e mini: Generate images from any text prompt. https://wandb.ai/dalle-mini/
331"
REFERENCES,0.4719626168224299,"dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy,
332"
REFERENCES,0.47313084112149534,"2023. Accessed: 2024-02-27.
333"
REFERENCES,0.4742990654205608,"[8] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato,
334"
REFERENCES,0.47546728971962615,"and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance.
335"
REFERENCES,0.4766355140186916,"In European Conference on Computer Vision, pages 88‚Äì105. Springer, 2022.
336"
REFERENCES,0.477803738317757,"[9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon
337"
REFERENCES,0.47897196261682246,"Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using
338"
REFERENCES,0.48014018691588783,"photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.
339"
REFERENCES,0.48130841121495327,"[10] Deepfloyd. Deepfloyd. https://www.deepfloyd.ai/, 2023.
340"
REFERENCES,0.4824766355140187,"[11] DeepFloyd. IF-I-XL-v1.0: A model by deepfloyd on hugging face models. https://huggingface.co/
341"
REFERENCES,0.48364485981308414,"DeepFloyd/IF-I-XL-v1.0, 2023. Accessed: 2024-02-28.
342"
REFERENCES,0.4848130841121495,"[12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation
343"
REFERENCES,0.48598130841121495,"via hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890‚Äì16902,
344"
REFERENCES,0.4871495327102804,"2022.
345"
REFERENCES,0.4883177570093458,"[13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang
346"
REFERENCES,0.4894859813084112,"Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with
347"
REFERENCES,0.49065420560747663,"knowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference on
348"
REFERENCES,0.49182242990654207,"Computer Vision and Pattern Recognition, pages 10135‚Äì10145, 2023.
349"
REFERENCES,0.4929906542056075,"[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,
350"
REFERENCES,0.4941588785046729,"Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next
351"
REFERENCES,0.4953271028037383,"generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.
352"
REFERENCES,0.49649532710280375,"[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.
353"
REFERENCES,0.4976635514018692,"Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference
354"
REFERENCES,0.49883177570093457,"on Computer Vision and Pattern Recognition, pages 10696‚Äì10706, 2022.
355"
REFERENCES,0.5,"[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
356"
REFERENCES,0.5011682242990654,"trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
357"
REFERENCES,0.5023364485981309,"processing systems, 30, 2017.
358"
REFERENCES,0.5035046728971962,"[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
359"
REFERENCES,0.5046728971962616,"Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint
360"
REFERENCES,0.5058411214953271,"arXiv:2304.02643, 2023.
361"
REFERENCES,0.5070093457943925,"[18] Jiachen Li, Ali Hassani, Steven Walton, and Humphrey Shi. Convmlp: Hierarchical convolutional mlps for
362"
REFERENCES,0.508177570093458,"vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
363"
REFERENCES,0.5093457943925234,"6306‚Äì6315, 2023.
364"
REFERENCES,0.5105140186915887,"[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
365"
REFERENCES,0.5116822429906542,"and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014:
366"
REFERENCES,0.5128504672897196,"13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
367"
REFERENCES,0.514018691588785,"740‚Äì755. Springer, 2014.
368"
REFERENCES,0.5151869158878505,"[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
369"
REFERENCES,0.5163551401869159,"[21] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream:
370"
REFERENCES,0.5175233644859814,"Training-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint
371"
REFERENCES,0.5186915887850467,"arXiv:2112.01573, 2021.
372"
REFERENCES,0.5198598130841121,"[22] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
373"
REFERENCES,0.5210280373831776,"arXiv:1711.05101, 2017.
374"
REFERENCES,0.522196261682243,"[23] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text
375"
REFERENCES,0.5233644859813084,"space. arXiv preprint arXiv:2209.15162, 2022.
376"
REFERENCES,0.5245327102803738,"[24] Microsoft. Phi-2. https://huggingface.co/microsoft/phi-2, 2023.
377"
REFERENCES,0.5257009345794392,"[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
378"
REFERENCES,0.5268691588785047,"Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
379"
REFERENCES,0.5280373831775701,"diffusion models. arXiv preprint arXiv:2112.10741, 2021.
380"
REFERENCES,0.5292056074766355,"[26] OpenAI. Dall-e 2. https://openai.com/dall-e-2, 2022.
381"
REFERENCES,0.530373831775701,"[27] Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction.
382"
REFERENCES,0.5315420560747663,"arXiv preprint arXiv:2401.01808, 2024.
383"
REFERENCES,0.5327102803738317,"[28] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. W√ºrstchen:
384"
REFERENCES,0.5338785046728972,"An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth International
385"
REFERENCES,0.5350467289719626,"Conference on Learning Representations, 2023.
386"
REFERENCES,0.5362149532710281,"[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna,
387"
REFERENCES,0.5373831775700935,"and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv
388"
REFERENCES,0.5385514018691588,"preprint arXiv:2307.01952, 2023.
389"
REFERENCES,0.5397196261682243,"[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
390"
REFERENCES,0.5408878504672897,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
391"
REFERENCES,0.5420560747663551,"natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR,
392"
REFERENCES,0.5432242990654206,"2021.
393"
REFERENCES,0.544392523364486,"[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
394"
REFERENCES,0.5455607476635514,"Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
395"
REFERENCES,0.5467289719626168,"The Journal of Machine Learning Research, 21(1):5485‚Äì5551, 2020.
396"
REFERENCES,0.5478971962616822,"[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
397"
REFERENCES,0.5490654205607477,"image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.
398"
REFERENCES,0.5502336448598131,"[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
399"
REFERENCES,0.5514018691588785,"image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
400"
REFERENCES,0.552570093457944,"Vision and Pattern Recognition (CVPR), pages 10684‚Äì10695, June 2022.
401"
REFERENCES,0.5537383177570093,"[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
402"
REFERENCES,0.5549065420560748,"image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
403"
REFERENCES,0.5560747663551402,"vision and pattern recognition, pages 10684‚Äì10695, 2022.
404"
REFERENCES,0.5572429906542056,"[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
405"
REFERENCES,0.5584112149532711,"Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-
406"
REFERENCES,0.5595794392523364,"image diffusion models with deep language understanding, 2022. URL https://arxiv. org/abs/2205.11487,
407"
REFERENCES,0.5607476635514018,"4.
408"
REFERENCES,0.5619158878504673,"[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
409"
REFERENCES,0.5630841121495327,"Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-
410"
REFERENCES,0.5642523364485982,"image diffusion models with deep language understanding. Advances in Neural Information Processing
411"
REFERENCES,0.5654205607476636,"Systems, 35:36479‚Äì36494, 2022.
412"
REFERENCES,0.5665887850467289,"[37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
413"
REFERENCES,0.5677570093457944,"Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
414"
REFERENCES,0.5689252336448598,"dataset for training next generation image-text models. Advances in Neural Information Processing
415"
REFERENCES,0.5700934579439252,"Systems, 35:25278‚Äì25294, 2022.
416"
REFERENCES,0.5712616822429907,"[38] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. A picture is worth a
417"
REFERENCES,0.572429906542056,"thousand words: Principled recaptioning improves image generation. arXiv preprint arXiv:2310.16656,
418"
REFERENCES,0.5735981308411215,"2023.
419"
REFERENCES,0.5747663551401869,"[39] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou,
420"
REFERENCES,0.5759345794392523,"Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. Advances in
421"
REFERENCES,0.5771028037383178,"Neural Information Processing Systems, 36, 2024.
422"
REFERENCES,0.5782710280373832,"[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
423"
REFERENCES,0.5794392523364486,"Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
424"
REFERENCES,0.580607476635514,"Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint
425"
REFERENCES,0.5817757009345794,"arXiv:2302.13971, 2023.
426"
REFERENCES,0.5829439252336449,"[41] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting
427"
REFERENCES,0.5841121495327103,"diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015, 2023.
428"
REFERENCES,0.5852803738317757,"[42] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.
429"
REFERENCES,0.5864485981308412,"DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896
430"
REFERENCES,0.5876168224299065,"[cs], 2022.
431"
REFERENCES,0.5887850467289719,"[43] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di
432"
REFERENCES,0.5899532710280374,"Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model.
433"
REFERENCES,0.5911214953271028,"arXiv preprint arXiv:2311.14284, 2023.
434"
REFERENCES,0.5922897196261683,"[44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human
435"
REFERENCES,0.5934579439252337,"preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv
436"
REFERENCES,0.594626168224299,"preprint arXiv:2306.09341, 2023.
437"
REFERENCES,0.5957943925233645,"[45] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image
438"
REFERENCES,0.5969626168224299,"diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708,
439"
REFERENCES,0.5981308411214953,"2024.
440"
REFERENCES,0.5992990654205608,"[46] Lijun Yu, Jos√© Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,
441"
REFERENCES,0.6004672897196262,"Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion‚Äìtokenizer is key
442"
REFERENCES,0.6016355140186916,"to visual generation. arXiv preprint arXiv:2310.05737, 2023.
443"
REFERENCES,0.602803738317757,"[47] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu.
444"
REFERENCES,0.6039719626168224,"Capsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550, 2023.
445"
REFERENCES,0.6051401869158879,"[48] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Lafite: Towards language-free
446"
REFERENCES,0.6063084112149533,"training for text-to-image generation. arxiv 2021. arXiv preprint arXiv:2111.13792.
447"
REFERENCES,0.6074766355140186,"NeurIPS Paper Checklist
448"
CLAIMS,0.6086448598130841,"1. Claims
449"
CLAIMS,0.6098130841121495,"Question: Do the main claims made in the abstract and introduction accurately reflect the
450"
CLAIMS,0.610981308411215,"paper‚Äôs contributions and scope?
451"
CLAIMS,0.6121495327102804,"Answer: [Yes]
452"
CLAIMS,0.6133177570093458,"Justification: The abstract and introduction clearly define the paper‚Äôs contributions, which
453"
CLAIMS,0.6144859813084113,"involve advancements in urban simulation accuracy and computational efficiency. These
454"
CLAIMS,0.6156542056074766,"claims are backed by robust experimental validation detailed in the subsequent sections.
455"
CLAIMS,0.616822429906542,"Guidelines:
456"
CLAIMS,0.6179906542056075,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
457"
CLAIMS,0.6191588785046729,"made in the paper.
458"
CLAIMS,0.6203271028037384,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
459"
CLAIMS,0.6214953271028038,"contributions made in the paper and important assumptions and limitations. A No or
460"
CLAIMS,0.6226635514018691,"NA answer to this question will not be perceived well by the reviewers.
461"
CLAIMS,0.6238317757009346,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
462"
CLAIMS,0.625,"much the results can be expected to generalize to other settings.
463"
CLAIMS,0.6261682242990654,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
464"
CLAIMS,0.6273364485981309,"are not attained by the paper.
465"
LIMITATIONS,0.6285046728971962,"2. Limitations
466"
LIMITATIONS,0.6296728971962616,"Question: Does the paper discuss the limitations of the work performed by the authors?
467"
LIMITATIONS,0.6308411214953271,"Answer: [Yes]
468"
LIMITATIONS,0.6320093457943925,"Justification: We have included a comprehensive discussion on limitations, particularly
469"
LIMITATIONS,0.633177570093458,"focusing on the scalability of our simulations in extremely large urban environments and
470"
LIMITATIONS,0.6343457943925234,"potential biases in the modeling processes.
471"
LIMITATIONS,0.6355140186915887,"Guidelines:
472"
LIMITATIONS,0.6366822429906542,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
473"
LIMITATIONS,0.6378504672897196,"the paper has limitations, but those are not discussed in the paper.
474"
LIMITATIONS,0.639018691588785,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
475"
LIMITATIONS,0.6401869158878505,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
476"
LIMITATIONS,0.6413551401869159,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
477"
LIMITATIONS,0.6425233644859814,"model well-specification, asymptotic approximations only holding locally). The authors
478"
LIMITATIONS,0.6436915887850467,"should reflect on how these assumptions might be violated in practice and what the
479"
LIMITATIONS,0.6448598130841121,"implications would be.
480"
LIMITATIONS,0.6460280373831776,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
481"
LIMITATIONS,0.647196261682243,"only tested on a few datasets or with a few runs. In general, empirical results often
482"
LIMITATIONS,0.6483644859813084,"depend on implicit assumptions, which should be articulated.
483"
LIMITATIONS,0.6495327102803738,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
484"
LIMITATIONS,0.6507009345794392,"For example, a facial recognition algorithm may perform poorly when image resolution
485"
LIMITATIONS,0.6518691588785047,"is low or images are taken in low lighting. Or a speech-to-text system might not be
486"
LIMITATIONS,0.6530373831775701,"used reliably to provide closed captions for online lectures because it fails to handle
487"
LIMITATIONS,0.6542056074766355,"technical jargon.
488"
LIMITATIONS,0.655373831775701,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
489"
LIMITATIONS,0.6565420560747663,"and how they scale with dataset size.
490"
LIMITATIONS,0.6577102803738317,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
491"
LIMITATIONS,0.6588785046728972,"address problems of privacy and fairness.
492"
LIMITATIONS,0.6600467289719626,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
493"
LIMITATIONS,0.6612149532710281,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
494"
LIMITATIONS,0.6623831775700935,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
495"
LIMITATIONS,0.6635514018691588,"judgment and recognize that individual actions in favor of transparency play an impor-
496"
LIMITATIONS,0.6647196261682243,"tant role in developing norms that preserve the integrity of the community. Reviewers
497"
LIMITATIONS,0.6658878504672897,"will be specifically instructed to not penalize honesty concerning limitations.
498"
THEORY ASSUMPTIONS AND PROOFS,0.6670560747663551,"3. Theory Assumptions and Proofs
499"
THEORY ASSUMPTIONS AND PROOFS,0.6682242990654206,"Question: For each theoretical result, does the paper provide the full set of assumptions and
500"
THEORY ASSUMPTIONS AND PROOFS,0.669392523364486,"a complete (and correct) proof?
501"
THEORY ASSUMPTIONS AND PROOFS,0.6705607476635514,"Answer: [Yes]
502"
THEORY ASSUMPTIONS AND PROOFS,0.6717289719626168,"Justification: All theoretical results are accompanied by a clear statement of assumptions and
503"
THEORY ASSUMPTIONS AND PROOFS,0.6728971962616822,"are supported by complete proofs provided in the supplementary materials. Each theorem
504"
THEORY ASSUMPTIONS AND PROOFS,0.6740654205607477,"and lemma are properly referenced and numbered for clarity and ease of access.
505"
THEORY ASSUMPTIONS AND PROOFS,0.6752336448598131,"Guidelines:
506"
THEORY ASSUMPTIONS AND PROOFS,0.6764018691588785,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
507"
THEORY ASSUMPTIONS AND PROOFS,0.677570093457944,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
508"
THEORY ASSUMPTIONS AND PROOFS,0.6787383177570093,"referenced.
509"
THEORY ASSUMPTIONS AND PROOFS,0.6799065420560748,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
510"
THEORY ASSUMPTIONS AND PROOFS,0.6810747663551402,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
511"
THEORY ASSUMPTIONS AND PROOFS,0.6822429906542056,"they appear in the supplemental material, the authors are encouraged to provide a short
512"
THEORY ASSUMPTIONS AND PROOFS,0.6834112149532711,"proof sketch to provide intuition.
513"
THEORY ASSUMPTIONS AND PROOFS,0.6845794392523364,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
514"
THEORY ASSUMPTIONS AND PROOFS,0.6857476635514018,"by formal proofs provided in appendix or supplemental material.
515"
THEORY ASSUMPTIONS AND PROOFS,0.6869158878504673,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6880841121495327,"4. Experimental Result Reproducibility
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6892523364485982,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6904205607476636,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6915887850467289,"of the paper (regardless of whether the code and data are provided or not)?
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6927570093457944,"Answer: [Yes]
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6939252336448598,"Justification: The paper provides detailed descriptions of the experimental setup, including
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6950934579439252,"data splits, hyperparameters, and the type of optimizer used. We also provide access to the
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6962616822429907,"source code and datasets in the supplementary materials to ensure full reproducibility.
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.697429906542056,"Guidelines:
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6985981308411215,"‚Ä¢ The answer NA means that the paper does not include experiments.
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6997663551401869,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7009345794392523,"well by the reviewers: Making the paper reproducible is important, regardless of
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7021028037383178,"whether the code and data are provided or not.
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7032710280373832,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7044392523364486,"to make their results reproducible or verifiable.
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.705607476635514,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7067757009345794,"For example, if the contribution is a novel architecture, describing the architecture fully
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7079439252336449,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7091121495327103,"be necessary to either make it possible for others to replicate the model with the same
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7102803738317757,"dataset, or provide access to the model. In general. releasing code and data is often
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7114485981308412,"one good way to accomplish this, but reproducibility can also be provided via detailed
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7126168224299065,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7137850467289719,"of a large language model), releasing of a model checkpoint, or other means that are
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7149532710280374,"appropriate to the research performed.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7161214953271028,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7172897196261683,"sions to provide some reasonable avenue for reproducibility, which may depend on the
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7184579439252337,"nature of the contribution. For example
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.719626168224299,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7207943925233645,"to reproduce that algorithm.
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7219626168224299,"(b) If the contribution is primarily a new model architecture, the paper should describe
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7231308411214953,"the architecture clearly and fully.
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7242990654205608,"(c) If the contribution is a new model (e.g., a large language model), then there should
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7254672897196262,"either be a way to access this model for reproducing the results or a way to reproduce
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7266355140186916,"the model (e.g., with an open-source dataset or instructions for how to construct
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.727803738317757,"the dataset).
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7289719626168224,"(d) We recognize that reproducibility may be tricky in some cases, in which case
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7301401869158879,"authors are welcome to describe the particular way they provide for reproducibility.
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7313084112149533,"In the case of closed-source models, it may be that access to the model is limited in
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7324766355140186,"some way (e.g., to registered users), but it should be possible for other researchers
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7336448598130841,"to have some path to reproducing or verifying the results.
556"
OPEN ACCESS TO DATA AND CODE,0.7348130841121495,"5. Open access to data and code
557"
OPEN ACCESS TO DATA AND CODE,0.735981308411215,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
558"
OPEN ACCESS TO DATA AND CODE,0.7371495327102804,"tions to faithfully reproduce the main experimental results, as described in supplemental
559"
OPEN ACCESS TO DATA AND CODE,0.7383177570093458,"material?
560"
OPEN ACCESS TO DATA AND CODE,0.7394859813084113,"Answer: [Yes]
561"
OPEN ACCESS TO DATA AND CODE,0.7406542056074766,"Justification: The paper does not propose a benchmark and we will release the code if the
562"
OPEN ACCESS TO DATA AND CODE,0.741822429906542,"paper is accepted. The model depends on non-open-sourced dataset, and the copyright of the
563"
OPEN ACCESS TO DATA AND CODE,0.7429906542056075,"checkpoint belongs to the company. Detailed instructions for training our model, including
564"
OPEN ACCESS TO DATA AND CODE,0.7441588785046729,"command lines, are provided in the supplementary materials.
565"
OPEN ACCESS TO DATA AND CODE,0.7453271028037384,"Guidelines:
566"
OPEN ACCESS TO DATA AND CODE,0.7464953271028038,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
567"
OPEN ACCESS TO DATA AND CODE,0.7476635514018691,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
568"
OPEN ACCESS TO DATA AND CODE,0.7488317757009346,"public/guides/CodeSubmissionPolicy) for more details.
569"
OPEN ACCESS TO DATA AND CODE,0.75,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
570"
OPEN ACCESS TO DATA AND CODE,0.7511682242990654,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
571"
OPEN ACCESS TO DATA AND CODE,0.7523364485981309,"including code, unless this is central to the contribution (e.g., for a new open-source
572"
OPEN ACCESS TO DATA AND CODE,0.7535046728971962,"benchmark).
573"
OPEN ACCESS TO DATA AND CODE,0.7546728971962616,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
574"
OPEN ACCESS TO DATA AND CODE,0.7558411214953271,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
575"
OPEN ACCESS TO DATA AND CODE,0.7570093457943925,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
576"
OPEN ACCESS TO DATA AND CODE,0.758177570093458,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
577"
OPEN ACCESS TO DATA AND CODE,0.7593457943925234,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
578"
OPEN ACCESS TO DATA AND CODE,0.7605140186915887,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
579"
OPEN ACCESS TO DATA AND CODE,0.7616822429906542,"proposed method and baselines. If only a subset of experiments are reproducible, they
580"
OPEN ACCESS TO DATA AND CODE,0.7628504672897196,"should state which ones are omitted from the script and why.
581"
OPEN ACCESS TO DATA AND CODE,0.764018691588785,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
582"
OPEN ACCESS TO DATA AND CODE,0.7651869158878505,"versions (if applicable).
583"
OPEN ACCESS TO DATA AND CODE,0.7663551401869159,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
584"
OPEN ACCESS TO DATA AND CODE,0.7675233644859814,"paper) is recommended, but including URLs to data and code is permitted.
585"
OPEN ACCESS TO DATA AND CODE,0.7686915887850467,"6. Experimental Setting/Details
586"
OPEN ACCESS TO DATA AND CODE,0.7698598130841121,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
587"
OPEN ACCESS TO DATA AND CODE,0.7710280373831776,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
588"
OPEN ACCESS TO DATA AND CODE,0.772196261682243,"results?
589"
OPEN ACCESS TO DATA AND CODE,0.7733644859813084,"Answer: [Yes]
590"
OPEN ACCESS TO DATA AND CODE,0.7745327102803738,"Justification: The experimental section of the paper provides comprehensive details about
591"
OPEN ACCESS TO DATA AND CODE,0.7757009345794392,"the training and test setups, including the rationale behind choosing specific hyperparameters
592"
OPEN ACCESS TO DATA AND CODE,0.7768691588785047,"and the types of optimizers used.
593"
OPEN ACCESS TO DATA AND CODE,0.7780373831775701,"Guidelines:
594"
OPEN ACCESS TO DATA AND CODE,0.7792056074766355,"‚Ä¢ The answer NA means that the paper does not include experiments.
595"
OPEN ACCESS TO DATA AND CODE,0.780373831775701,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
596"
OPEN ACCESS TO DATA AND CODE,0.7815420560747663,"that is necessary to appreciate the results and make sense of them.
597"
OPEN ACCESS TO DATA AND CODE,0.7827102803738317,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
598"
OPEN ACCESS TO DATA AND CODE,0.7838785046728972,"material.
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7850467289719626,"7. Experiment Statistical Significance
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7862149532710281,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7873831775700935,"information about the statistical significance of the experiments?
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7885514018691588,"Answer: [Yes]
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7897196261682243,"Justification: All experimental results are presented with error bars reflecting the standard
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7908878504672897,"deviation across multiple runs. We provide a detailed explanation of how these were
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7920560747663551,"calculated and the assumptions underlying our statistical tests.
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7932242990654206,"Guidelines:
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.794392523364486,"‚Ä¢ The answer NA means that the paper does not include experiments.
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7955607476635514,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7967289719626168,"dence intervals, or statistical significance tests, at least for the experiments that support
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7978971962616822,"the main claims of the paper.
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7990654205607477,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8002336448598131,"example, train/test split, initialization, random drawing of some parameter, or overall
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8014018691588785,"run with given experimental conditions).
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.802570093457944,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8037383177570093,"call to a library function, bootstrap, etc.)
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8049065420560748,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8060747663551402,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8072429906542056,"of the mean.
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8084112149532711,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8095794392523364,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8107476635514018,"of Normality of errors is not verified.
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8119158878504673,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8130841121495327,"figures symmetric error bars that would yield results that are out of range (e.g. negative
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8142523364485982,"error rates).
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8154205607476636,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8165887850467289,"they were calculated and reference the corresponding figures or tables in the text.
627"
EXPERIMENTS COMPUTE RESOURCES,0.8177570093457944,"8. Experiments Compute Resources
628"
EXPERIMENTS COMPUTE RESOURCES,0.8189252336448598,"Question: For each experiment, does the paper provide sufficient information on the com-
629"
EXPERIMENTS COMPUTE RESOURCES,0.8200934579439252,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
630"
EXPERIMENTS COMPUTE RESOURCES,0.8212616822429907,"the experiments?
631"
EXPERIMENTS COMPUTE RESOURCES,0.822429906542056,"Answer: [Yes]
632"
EXPERIMENTS COMPUTE RESOURCES,0.8235981308411215,"Justification: The paper details the computational resources required for each experiment,
633"
EXPERIMENTS COMPUTE RESOURCES,0.8247663551401869,"including the types of GPUs used, the amount of memory, and the execution time. This
634"
EXPERIMENTS COMPUTE RESOURCES,0.8259345794392523,"ensures that other researchers can allocate the appropriate resources to reproduce our results.
635"
EXPERIMENTS COMPUTE RESOURCES,0.8271028037383178,"Guidelines:
636"
EXPERIMENTS COMPUTE RESOURCES,0.8282710280373832,"‚Ä¢ The answer NA means that the paper does not include experiments.
637"
EXPERIMENTS COMPUTE RESOURCES,0.8294392523364486,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
638"
EXPERIMENTS COMPUTE RESOURCES,0.830607476635514,"or cloud provider, including relevant memory and storage.
639"
EXPERIMENTS COMPUTE RESOURCES,0.8317757009345794,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
640"
EXPERIMENTS COMPUTE RESOURCES,0.8329439252336449,"experimental runs as well as estimate the total compute.
641"
EXPERIMENTS COMPUTE RESOURCES,0.8341121495327103,"‚Ä¢ The paper should disclose whether the full research project required more compute
642"
EXPERIMENTS COMPUTE RESOURCES,0.8352803738317757,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
643"
EXPERIMENTS COMPUTE RESOURCES,0.8364485981308412,"didn‚Äôt make it into the paper).
644"
CODE OF ETHICS,0.8376168224299065,"9. Code Of Ethics
645"
CODE OF ETHICS,0.8387850467289719,"Question: Does the research conducted in the paper conform, in every respect, with the
646"
CODE OF ETHICS,0.8399532710280374,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
647"
CODE OF ETHICS,0.8411214953271028,"Answer: [Yes]
648"
CODE OF ETHICS,0.8422897196261683,"Justification: Our research adheres strictly to the NeurIPS Code of Ethics. We have consid-
649"
CODE OF ETHICS,0.8434579439252337,"ered ethical implications, especially regarding the generation of images from text, and have
650"
CODE OF ETHICS,0.844626168224299,"implemented measures to prevent misuse.
651"
CODE OF ETHICS,0.8457943925233645,"Guidelines:
652"
CODE OF ETHICS,0.8469626168224299,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
653"
CODE OF ETHICS,0.8481308411214953,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
654"
CODE OF ETHICS,0.8492990654205608,"deviation from the Code of Ethics.
655"
CODE OF ETHICS,0.8504672897196262,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
656"
CODE OF ETHICS,0.8516355140186916,"eration due to laws or regulations in their jurisdiction).
657"
BROADER IMPACTS,0.852803738317757,"10. Broader Impacts
658"
BROADER IMPACTS,0.8539719626168224,"Question: Does the paper discuss both potential positive societal impacts and negative
659"
BROADER IMPACTS,0.8551401869158879,"societal impacts of the work performed?
660"
BROADER IMPACTS,0.8563084112149533,"Answer: [Yes]
661"
BROADER IMPACTS,0.8574766355140186,"Justification: The paper includes content about broader impacts that discusses both the
662"
BROADER IMPACTS,0.8586448598130841,"potential positive applications of our method in educational and creative industries, and
663"
BROADER IMPACTS,0.8598130841121495,"potential negative impacts, such as the misuse of generated images. We also suggest
664"
BROADER IMPACTS,0.860981308411215,"mitigation strategies for potential negative uses.
665"
BROADER IMPACTS,0.8621495327102804,"Guidelines:
666"
BROADER IMPACTS,0.8633177570093458,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
667"
BROADER IMPACTS,0.8644859813084113,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
668"
BROADER IMPACTS,0.8656542056074766,"impact or why the paper does not address societal impact.
669"
BROADER IMPACTS,0.866822429906542,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
670"
BROADER IMPACTS,0.8679906542056075,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
671"
BROADER IMPACTS,0.8691588785046729,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
672"
BROADER IMPACTS,0.8703271028037384,"groups), privacy considerations, and security considerations.
673"
BROADER IMPACTS,0.8714953271028038,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
674"
BROADER IMPACTS,0.8726635514018691,"to particular applications, let alone deployments. However, if there is a direct path to
675"
BROADER IMPACTS,0.8738317757009346,"any negative applications, the authors should point it out. For example, it is legitimate
676"
BROADER IMPACTS,0.875,"to point out that an improvement in the quality of generative models could be used to
677"
BROADER IMPACTS,0.8761682242990654,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
678"
BROADER IMPACTS,0.8773364485981309,"that a generic algorithm for optimizing neural networks could enable people to train
679"
BROADER IMPACTS,0.8785046728971962,"models that generate Deepfakes faster.
680"
BROADER IMPACTS,0.8796728971962616,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
681"
BROADER IMPACTS,0.8808411214953271,"being used as intended and functioning correctly, harms that could arise when the
682"
BROADER IMPACTS,0.8820093457943925,"technology is being used as intended but gives incorrect results, and harms following
683"
BROADER IMPACTS,0.883177570093458,"from (intentional or unintentional) misuse of the technology.
684"
BROADER IMPACTS,0.8843457943925234,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
685"
BROADER IMPACTS,0.8855140186915887,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
686"
BROADER IMPACTS,0.8866822429906542,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
687"
BROADER IMPACTS,0.8878504672897196,"feedback over time, improving the efficiency and accessibility of ML).
688"
SAFEGUARDS,0.889018691588785,"11. Safeguards
689"
SAFEGUARDS,0.8901869158878505,"Question: Does the paper describe safeguards that have been put in place for responsible
690"
SAFEGUARDS,0.8913551401869159,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
691"
SAFEGUARDS,0.8925233644859814,"image generators, or scraped datasets)?
692"
SAFEGUARDS,0.8936915887850467,"Answer: [NA]
693"
SAFEGUARDS,0.8948598130841121,"Justification: Our paper poses no such risks. If then, we will describe the safeguards
694"
SAFEGUARDS,0.8960280373831776,"implemented in releasing our models, including usage guidelines and limitations to access,
695"
SAFEGUARDS,0.897196261682243,"ensuring responsible use and mitigating risks of misuse.
696"
SAFEGUARDS,0.8983644859813084,"Guidelines:
697"
SAFEGUARDS,0.8995327102803738,"‚Ä¢ The answer NA means that the paper poses no such risks.
698"
SAFEGUARDS,0.9007009345794392,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
699"
SAFEGUARDS,0.9018691588785047,"necessary safeguards to allow for controlled use of the model, for example by requiring
700"
SAFEGUARDS,0.9030373831775701,"that users adhere to usage guidelines or restrictions to access the model or implementing
701"
SAFEGUARDS,0.9042056074766355,"safety filters.
702"
SAFEGUARDS,0.905373831775701,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
703"
SAFEGUARDS,0.9065420560747663,"should describe how they avoided releasing unsafe images.
704"
SAFEGUARDS,0.9077102803738317,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
705"
SAFEGUARDS,0.9088785046728972,"not require this, but we encourage authors to take this into account and make a best
706"
SAFEGUARDS,0.9100467289719626,"faith effort.
707"
LICENSES FOR EXISTING ASSETS,0.9112149532710281,"12. Licenses for existing assets
708"
LICENSES FOR EXISTING ASSETS,0.9123831775700935,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
709"
LICENSES FOR EXISTING ASSETS,0.9135514018691588,"the paper, properly credited and are the license and terms of use explicitly mentioned and
710"
LICENSES FOR EXISTING ASSETS,0.9147196261682243,"properly respected?
711"
LICENSES FOR EXISTING ASSETS,0.9158878504672897,"Answer: [Yes]
712"
LICENSES FOR EXISTING ASSETS,0.9170560747663551,"Justification: All third-party assets used in our research are properly credited, and we have
713"
LICENSES FOR EXISTING ASSETS,0.9182242990654206,"explicitly mentioned and complied with the licensing terms. URLs and version numbers of
714"
LICENSES FOR EXISTING ASSETS,0.919392523364486,"datasets and code are clearly listed in the references.
715"
LICENSES FOR EXISTING ASSETS,0.9205607476635514,"Guidelines:
716"
LICENSES FOR EXISTING ASSETS,0.9217289719626168,"‚Ä¢ The answer NA means that the paper does not use existing assets.
717"
LICENSES FOR EXISTING ASSETS,0.9228971962616822,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
718"
LICENSES FOR EXISTING ASSETS,0.9240654205607477,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
719"
LICENSES FOR EXISTING ASSETS,0.9252336448598131,"URL.
720"
LICENSES FOR EXISTING ASSETS,0.9264018691588785,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
721"
LICENSES FOR EXISTING ASSETS,0.927570093457944,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
722"
LICENSES FOR EXISTING ASSETS,0.9287383177570093,"service of that source should be provided.
723"
LICENSES FOR EXISTING ASSETS,0.9299065420560748,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
724"
LICENSES FOR EXISTING ASSETS,0.9310747663551402,"package should be provided. For popular datasets, paperswithcode.com/datasets
725"
LICENSES FOR EXISTING ASSETS,0.9322429906542056,"has curated licenses for some datasets. Their licensing guide can help determine the
726"
LICENSES FOR EXISTING ASSETS,0.9334112149532711,"license of a dataset.
727"
LICENSES FOR EXISTING ASSETS,0.9345794392523364,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
728"
LICENSES FOR EXISTING ASSETS,0.9357476635514018,"the derived asset (if it has changed) should be provided.
729"
LICENSES FOR EXISTING ASSETS,0.9369158878504673,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
730"
LICENSES FOR EXISTING ASSETS,0.9380841121495327,"the asset‚Äôs creators.
731"
NEW ASSETS,0.9392523364485982,"13. New Assets
732"
NEW ASSETS,0.9404205607476636,"Question: Are new assets introduced in the paper well documented and is the documentation
733"
NEW ASSETS,0.9415887850467289,"provided alongside the assets?
734"
NEW ASSETS,0.9427570093457944,"Answer: [Yes]
735"
NEW ASSETS,0.9439252336448598,"Justification: Any new datasets or models introduced in the paper are accompanied by
736"
NEW ASSETS,0.9450934579439252,"thorough documentation detailing their creation, intended use, limitations, and licensing
737"
NEW ASSETS,0.9462616822429907,"information.
738"
NEW ASSETS,0.947429906542056,"Guidelines:
739"
NEW ASSETS,0.9485981308411215,"‚Ä¢ The answer NA means that the paper does not release new assets.
740"
NEW ASSETS,0.9497663551401869,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
741"
NEW ASSETS,0.9509345794392523,"submissions via structured templates. This includes details about training, license,
742"
NEW ASSETS,0.9521028037383178,"limitations, etc.
743"
NEW ASSETS,0.9532710280373832,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
744"
NEW ASSETS,0.9544392523364486,"asset is used.
745"
NEW ASSETS,0.955607476635514,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
746"
NEW ASSETS,0.9567757009345794,"create an anonymized URL or include an anonymized zip file.
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9579439252336449,"14. Crowdsourcing and Research with Human Subjects
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9591121495327103,"Question: For crowdsourcing experiments and research with human subjects, does the paper
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9602803738317757,"include the full text of instructions given to participants and screenshots, if applicable, as
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9614485981308412,"well as details about compensation (if any)?
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9626168224299065,"Answer: [NA]
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9637850467289719,"Justification: This paper does not involve crowdsourcing experiments or research with
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9649532710280374,"human subjects.
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661214953271028,"Guidelines:
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9672897196261683,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684579439252337,"human subjects.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969626168224299,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707943925233645,"tion of the paper involves human subjects, then as much detail as possible should be
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719626168224299,"included in the main paper.
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9731308411214953,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742990654205608,"or other labor should be paid at least the minimum wage in the country of the data
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754672897196262,"collector.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766355140186916,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977803738317757,"Subjects
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789719626168224,"Question: Does the paper describe potential risks incurred by study participants, whether
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801401869158879,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813084112149533,"approvals (or an equivalent approval/review based on the requirements of your country or
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824766355140186,"institution) were obtained?
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836448598130841,"Answer: [NA]
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848130841121495,"Justification: Our research did not involve human subjects, thus no IRB approval was
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985981308411215,"necessary.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871495327102804,"Guidelines:
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883177570093458,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894859813084113,"human subjects.
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906542056074766,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991822429906542,"may be required for any human subjects research. If you obtained IRB approval, you
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929906542056075,"should clearly state this in the paper.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941588785046729,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953271028037384,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964953271028038,"guidelines for their institution.
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976635514018691,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988317757009346,"applicable), such as the institution conducting the review.
783"
