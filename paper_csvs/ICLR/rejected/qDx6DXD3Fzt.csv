Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002890173410404624,"The application of machine learning in safety-critical systems requires a reliable
assessment of uncertainy. However, deep neural networks are known to produce
highly overconﬁdent predictions on out-of-distribution (OOD) data. Even if trained
to be non-conﬁdent on OOD data one can still adversarially manipulate OOD data
so that the classiﬁer again assigns high conﬁdence to the manipulated samples. In
this paper we propose a novel method that combines a certiﬁable OOD detector with
a standard classiﬁer from ﬁrst principles into an OOD aware classiﬁer. This way
we achieve the best of two worlds: certiﬁably adversarially robust OOD detection,
even for OOD samples close to the in-distribution, without loss in either prediction
accuracy or detection performance for non-manipulated OOD data. Moreover,
due to the particular construction our classiﬁer provably avoids the asymptotic
overconﬁdence problem of standard neural networks."
INTRODUCTION,0.005780346820809248,"1
INTRODUCTION"
INTRODUCTION,0.008670520231213872,"Deep neural networks have achieved state-of-the-art performance in many application domains.
However, the widespread usage of deep neural networks in safety-critical applications, e.g. in
healthcare, autonomous driving/aviation, manufacturing, raises concerns as deep neural networks
have problematic deﬁciencies. Among these deﬁciencies are overconﬁdent predictions on non-task
related inputs (Nguyen et al., 2015; Hendrycks & Gimpel, 2017)which has recently attracted a lot
of interest. ReLU networks have even been shown to be provably overconﬁdent far away from the
training data (Hein et al., 2019). However, reliable conﬁdences of the classiﬁer on the classiﬁcation
task (in-distribution) (Guo et al., 2017) as well as on the out-distribution (Hendrycks & Gimpel,
2017; Hein et al., 2019) are important to be able to detect when the deep neural network is working
outside of its speciﬁcation, which can then be used to either involve a human operator or to fall back
into a “safe state”. Thus, solving this problem is of high importance for trustworthy ML systems.
Many approaches have been proposed for OOD detection, (Hendrycks & Gimpel, 2017; Liang et al.,
2018; Lee et al., 2018a;b; Hendrycks et al., 2019; Ren et al., 2019; Hein et al., 2019; Meinke &
Hein, 2020; Chen et al., 2020; Papadopoulos et al., 2021; Macêdo & Ludermir, 2021; Macêdo et al.,
2021) and one of the currently best performing methods enforces low conﬁdence during training
(“outlier exposure” (OE)) on a large and diverse set of out-distribution images (Hendrycks et al.,
2019) which leads to strong separation of in- and out-distribution based on the conﬁdence of the
classiﬁer. Crucially, this also generalizes to novel test out-distributions."
INTRODUCTION,0.011560693641618497,"However, current OOD detection methods are vulnerable to adversarial manipulations, i.e. small
adversarial modiﬁcations of OOD inputs lead to large conﬁdence of the classiﬁer on the manipulated
samples (Nguyen et al., 2015; Hein et al., 2019; Sehwag et al., 2019). While different methods
for adversarially robust OOD detection have been proposed (Hein et al., 2019; Sehwag et al., 2019;
Meinke & Hein, 2020; Chen et al., 2020; Bitterwolf et al., 2020) there is little work on provably
adversarially robust OOD detection (Meinke & Hein, 2020; Bitterwolf et al., 2020; Kopetzki et al.,
2020; Berrada et al., 2021). In CCU (Meinke & Hein, 2020) they append density estimators based
on Gaussian mixture models for in- and out-distribution to the softmax layer. By also enforcing
low conﬁdence on a training out-distribution, they achieve similar OOD detection performance to
(Hendrycks et al., 2019) but can guarantee that the classiﬁer shows decreasing conﬁdence as one
moves away from the training data. However, for close in-distribution inputs this approach yields no
guarantee as the Gaussian mixture models are not powerful enough for complex image classiﬁcation"
INTRODUCTION,0.014450867052023121,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017341040462427744,"Table 1: ProoD combines desirable properties of existing (adversarially robust) OOD detection
methods. It has high test accuracy and standard OOD detection performance (as (Hendrycks et al.,
2019)) and has worst-case guarantees if the out-distribution samples are adversarially perturbed in an
l∞-neighborhood to maximize the conﬁdence (see Section 4.2). Similar to CCU (Meinke & Hein,
2020) it avoids the problem of asymptotic overconﬁdence far away from the training data."
INTRODUCTION,0.02023121387283237,"OE
CCU
ACET/ATOM
GOOD
ProoD
High accuracy
✓
✓
✓
✓
High clean OOD detection performance
✓
✓
✓
✓
Adv. OOD l∞-robustness
(✓)
✓
✓
Adv. OOD l∞-certiﬁcates
✓
✓
Provably not asympt. overconﬁdent
✓
✓"
INTRODUCTION,0.023121387283236993,"tasks. In (Kristiadi et al., 2020a;b) similar asymptotic guarantees are derived for Bayesian neural
networks but without any robustness guarantees. In (Kopetzki et al., 2020) they apply randomized
smoothing to obtain guarantees wrt. l2-perturbations for Dirchlet-based models (Malinin & Gales,
2018; 2019; Sensoy et al., 2018) which already show quite some gap in terms of AUC-ROC to
SOTA OOD detection methods even without attacks. Interval bound propagation (IBP) (Gowal
et al., 2018; Mirman et al., 2018; Zhang et al., 2020; Jovanovi´c et al., 2021) has been shown to be
one of the most effective techniques in certiﬁed adversarial robustness on the in-distribution when
applied during training. In GOOD (Bitterwolf et al., 2020) they use IBP to compute upper bounds on
the conﬁdence in an l∞-neighborhood of the input and minimize these upper bounds on a training
out-distribution. This yields classiﬁers with pointwise guarantees for adversarially robust OOD
detection even for “close” out-distribution inputs which generalize to novel OOD test distributions.
However, the employed architectures of the neural network are restricted to rather shallow networks
as otherwise the bounds of IBP are loose. Thus, they obtain low classiﬁcation accuracy which is far
from the state-of-the-art, e.g. 91% on CIFAR-10, and their approach does not scale to more complex
tasks like ImageNet. In particular, despite its low accuracy the employed network architecture is
quite large and has higher memory consumption than a ResNet50. Moreover, one does not get any
guarantees on the asymptotic behavior of the classiﬁer as one moves away from the training data.
The authors of (Berrada et al., 2021) use SOTA veriﬁcation techniques (Dathathri et al., 2020) and
get guarantees for OOD detection wrt. l∞-perturbations for ACET models (Hein et al., 2019) that
were not speciﬁcally trained to be veriﬁable but the guarantees obtained by training the models via
IBP in (Bitterwolf et al., 2020) are signiﬁcantly better."
INTRODUCTION,0.02601156069364162,"In this paper we propose ProoD which merges a certiﬁed binary discriminator for in-versus out-
distribution with a classiﬁer for the in-distribution task in a principled fashion into a joint classiﬁer.
This combines the advantages of CCU (Meinke & Hein, 2020) and GOOD (Bitterwolf et al., 2020)
without suffering from their downsides. In particular, ProoD simultaneously achieves the following:"
INTRODUCTION,0.028901734104046242,"• Guaranteed adversarially robust OOD detection via conﬁdence upper bounds on l∞-balls
around OOD samples."
INTRODUCTION,0.031791907514450865,"• Additionally, it provably prevents the asymptotic overconﬁdence of deep neural networks."
INTRODUCTION,0.03468208092485549,"• It can be used with arbitrary architectures and has no loss in prediction performance and
standard OOD detection performance."
INTRODUCTION,0.03757225433526012,"Thus, we get provable guarantees for adversarially robust OOD detection, ﬁx the asymptotic over-
conﬁdence (almost) for free as we have (almost) no loss in prediction and standard OOD detection
performance. We qualitatively compare the properties of our model to prior approaches in Table 1."
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.04046242774566474,"2
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.04335260115606936,"In the following we consider feedforward networks for classiﬁcation, f : Rd →RK, with K classes
deﬁned with x(0) = x as"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.046242774566473986,"x(l) = σ(l) 
W (l)x(l−1) + b(l)
l = 1, . . . L −1,
f(x) = W (L)x(L−1) + b(L), (1)"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.049132947976878616,Under review as a conference paper at ICLR 2022
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.05202312138728324,Classifier
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.05491329479768786,"Certified 
Discriminator"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.057803468208092484,In vs. Out
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.06069364161849711,"Certified 
Discriminator"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.06358381502890173,In vs. Out
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.06647398843930635,In-Distribution
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.06936416184971098,R.ImgNet
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.07225433526011561,"Worst-Case
Out-Distribution"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.07514450867052024,Not R.ImgNet
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.07803468208092486,"High confidence
on In-Distribution"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.08092485549132948,Guaranteed low confidence
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.0838150289017341,on adversarial OOD
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.08670520231213873,"Figure 1: ProoD’s Architecture: Our method produces high conﬁdence on the in-distribution sample
(R.ImgNet) using Eq. (2). It also provides an upper bound on the worst-case conﬁdence over all
images within a neighborhood of the shown OOD sample (OpenImages) using Eq. (3). Bounding the
conﬁdence on this inﬁnite set of OOD samples is possible without any loss in accuracy."
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.08959537572254335,"where L ∈N is the number of layers, W (l) and b(l) are weights and biases and σ(l) is either the
ReLU or leaky ReLU activation function of layer l . We refer to the output of f as the logits and get
a probability distribution over the classes via ˆp(y|x) =
efy(x)
PK
k efk(x) for y = 1, . . . , K. We deﬁne the"
PROVABLY ROBUST DETECTION OF OUT-OF-DISTRIBUTION DATA,0.09248554913294797,"conﬁdence as Conf(f(x)) = maxy=1,...,K ˆp(y|x)."
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.0953757225433526,"2.1
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION"
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.09826589595375723,"In our joint model we assume that there exists an in- and out-distribution where the out-distribution
samples are unrelated to the in-distribution task. Thus, we can formally write the conditional
distribution on the input as"
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.10115606936416185,"ˆp(y|x) = ˆp(y|x, i)ˆp(i|x) + ˆp(y|x, o)ˆp(o|x),
(2)"
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.10404624277456648,"where ˆp(i|x) is the conditional distribution that sample x belongs to the in-distribution and ˆp(y|x, i)
is the conditional distribution for the in-distribution. We assume that OOD samples are unrelated
and thus maximally un-informative to the in-distribution task, i.e. we ﬁx ˆp(y|x, o) = 1"
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.1069364161849711,"K . In (Meinke
& Hein, 2020) they further decomposed ˆp(i|x) = ˆp(x|i)ˆp(i)"
JOINT MODEL FOR OOD DETECTION AND CLASSIFICATION,0.10982658959537572,"ˆp(x)
and used Gaussian mixture models to
estimate ˆp(x|i) with ﬁxed ˆp(i) = ˆp(o) = 1"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.11271676300578035,"2. As the properties of the Gaussian mixture model can be
directly controlled in comparison to a more powerful generative model such as VAEs, this allowed
(Meinke & Hein, 2020) to prove that ˆp(y|x) becomes uniform over the classes if x is far away from
the training data. However, the downside of the Gaussian mixture models is that this approach yields
no guarantees for close out-distribution samples. Instead in this paper we directly learn ˆp(i|x) which
results in a binary classiﬁcation problem and we train this binary classiﬁer in a certiﬁed robust fashion
wrt. an l∞-threat model so that even adversarially manipulated OOD samples are detected. In order
to avoid confusion with the multi-class classiﬁer, we will refer to ˆp(i|x) as a binary discriminator. In
an l∞-ball of radius ϵ around x ∈Rd and for all y we have the upper bound (also see App. H)"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.11560693641618497,"max
∥x′−x∥∞≤ϵ ˆp(y|x′) ≤
max
∥x′−x∥∞≤ϵ ˆp(i|x′)+ 1"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.11849710982658959,"K
 
1−ˆp(i|x′)

= K −1"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.12138728323699421,"K
max
∥x′−x∥∞≤ϵ ˆp(i|x′)+ 1"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.12427745664739884,"K , (3)"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.12716763005780346,"so we can defer the certiﬁcation “work” to the binary discriminator. Using a particular constraint
on the weights of the binary discriminator, we get similar asymptotic properties as in (Meinke &
Hein, 2020) but additionally get certiﬁed adversarial robustness for close out-distribution samples
as in (Bitterwolf et al., 2020). In contrast to (Bitterwolf et al., 2020) this comes without loss in test
accuracy or non-adversarial OOD detection performance as in our model the neural network used for
the in-distribution classiﬁcation task ˆp(y|x, i) is independent of the binary discriminator. Thus, we
have the advantage that the classiﬁer can use arbitrary deep neural networks and is not constrained
to certiﬁable networks. We call our approach Provable out-of-Distribution detector (ProoD) and
visualize its components in Figure 1."
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.13005780346820808,"Certiﬁably Robust Binary Discrimination of In- versus Out-Distribution
The ﬁrst goal is to get
a certiﬁably adversarially robust OOD detector ˆp(i|x). We train this binary discriminator independent"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.1329479768786127,Under review as a conference paper at ICLR 2022
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.13583815028901733,"of the overall classiﬁer as the training schedules for certiﬁed robustness are incompatible with the
standard training schedules of normal classiﬁers. For this binary classiﬁcation problem we use a
logistic model ˆp(i|x) =
1
1+e−g(x) , where g : Rd →R are logits of a neural network (we denote the
weights and biases of g by Wg and bg in order to discriminate it from the classiﬁer f introduced in
the next paragraph). Let (xr, yr)N
r=1 be our in-distribution training data (we use the class encoding
+1 for the in-distribution and −1 for out-distribution) and (zs)M
s=1 be our training out-distribution
data. Then the optimization problem associated to the binary classiﬁcation problem becomes:"
AS THE PROPERTIES OF THE GAUSSIAN MIXTURE MODEL CAN BE,0.13872832369942195,"min
g
W
(Lg)
g
<0"
N,0.1416184971098266,"1
N N
X"
N,0.14450867052023122,"r=1
log

1 + e−g(xr)
+ 1 M M
X"
N,0.14739884393063585,"s=1
log

1 + e¯g(zs)
,
(4)"
N,0.15028901734104047,"where we minimize over the parameters of the neural network g under the constraint that the weights
of the output layer W (Lg)
g
are componentwise negative and ¯g(z) ≥maxu∈Bp(z,ϵ) g(u) is an upper
bound on the output of g around OOD samples for a given lp-threat model Bp(z, ϵ) = {u ∈
[0, 1]d | ∥u −z∥p ≤ϵ}. In this paper we always use an l∞-threat model. This upper bound could, in
principle, be computed using any certiﬁcation technique but we will use interval bound propagation
(IBP) since it is simple, fast and has been shown to produce SOTA results (Gowal et al., 2018).
Note that this is not standard adversarial training for a binary classiﬁcation problem as here we
have an asymmetric situation: we want to be (certiﬁably) robust to adversarial manipulation on
the out-distribution data but not on the in-distribution and thus the upper bound is only used for
out-distribution samples. The negativity of the output layer’s weights W (Lg)
g
is enforced by using the
parameterization (W (Lg)
g
)j = −ehj componentwise and optimizing over hj. In Section 3 we show
how the negativity of W (Lg)
g
allows us to control the asymptotic behavior of the joint classiﬁer."
N,0.1531791907514451,"For the reader’s convenience we quickly present the upper x(l) and lower x(l) bounds on the output
of layer l in a feedforward neural network produced by IBP:"
N,0.15606936416184972,"x(l) = σ

W (l)
+ x(l−1) + W (l)
−x(l−1) + b(l)
,
x(l) = σ

W (l)
+ x(l−1) + W (l)
−x(l−1) + b(l)
, (5)"
N,0.15895953757225434,"where W+ = max(0, W) and W−= min(0, W) (min/max used componentwise). For an l∞-threat
model one starts with the upper and lower bounds for the input layer x(0) = x + ϵ and x(0) = x −ϵ
and then iteratively computes the layerwise upper and lower bounds x(l), x(l) which fulﬁll"
N,0.16184971098265896,"x(l) ≤
min
∥x′−x∥∞≤ϵ x(l)(x′) ≤
max
∥x′−x∥∞≤ϵ x(l)(x′) ≤x(l).
(6)"
N,0.16473988439306358,"While in (Bitterwolf et al., 2020) they also used IBP to upper bound the conﬁdence of the classiﬁer
this resulted in a bound that took into account all O(K2) logit differences between all classes. In
contrast, our loss in Eq. (4) is signiﬁcantly simpler as we just have a binary classiﬁcation problem
and therefore only need a single bound. Thus, our approach easily scales to tasks with a large number
of classes and training the binary discriminator with IBP turns out to be signiﬁcantly more stable
than the approach in (Bitterwolf et al., 2020) and does not require many additional tricks."
N,0.1676300578034682,"(Semi)-Joint Training of the ﬁnal Classiﬁer
Given the certiﬁably robust model ˆp(i|x) for the
binary classiﬁcation task between in- and out-distribution, we need to determine the ﬁnal predictive
distribution ˆp(y|x) in Eq. (2). On top of the provable OOD performance that we get from Eq. (3),
we also want to achieve SOTA performance on unperturbed OOD data. In principle we could
independently train a model for the predictive in-distribution task ˆp(y|x, i), e.g. using outlier
exposure (OE) (Hendrycks et al., 2019) or any other state-of-the-art OOD detection method and
simply combine it with our ˆp(i|x). While this does lead to models with high OOD performance
that also have guarantees, it completely ignores the interaction between ˆp(i|x) and ˆp(y|x, i) during
training. Instead we propose to train ˆp(y|x, i) by optimizing our ﬁnal predictive distribution ˆp(y|x).
Note that in order to retain the guarantees of ˆp(i|x) we only train the parameters of the neural network
f : Rd →RK and need to keep ˆp(i|x) resp. g ﬁxed. Because g stays ﬁxed we call this semi-joint
training. We use OE (Hendrycks et al., 2019) for training ˆp(y|x) with the cross-entropy loss and use"
N,0.17052023121387283,Under review as a conference paper at ICLR 2022
N,0.17341040462427745,"the softmax-function in order to obtain the predictive distribution ˆpf(y|x, i) =
efy(x)
P"
N,0.17630057803468208,k efk(x) from f:
N,0.1791907514450867,"min
f
−1 N N
X"
N,0.18208092485549132,"r=1
log
 
ˆp(yr|xr)

−1 M M
X s=1"
K,0.18497109826589594,"1
K K
X"
K,0.18786127167630057,"l=1
log
 
ˆp(l|zs)
"
K,0.1907514450867052,"= min
f
−1 N N
X"
K,0.1936416184971098,"r=1
log

ˆpf(yr|xr, i)ˆp(i|xr) + 1"
K,0.19653179190751446,"K
 
1 −ˆp(i|xr)
 −1 M M
X s=1"
K,0.1994219653179191,"1
K K
X"
K,0.2023121387283237,"l=1
log

ˆpf(l|zs, i)ˆp(i|zs) + 1"
K,0.20520231213872833,"K
 
1 −ˆp(i|zs)

,
(7)"
K,0.20809248554913296,"where the ﬁrst term is the standard cross-entropy loss on the in-distribution but now for our joint
model for ˆp(y|x) and the second term enforces uniform conﬁdence on out-distribution samples. In
App. B we show that semi-joint training leads to stronger guarantees than separate training."
K,0.21098265895953758,"The loss in Eq. (4) implicitly weighs the in-distribution and worst-case out-distribution equally, which
amounts to the assumption p(i) = 1"
K,0.2138728323699422,"2 = p(o). This highly conservative choice simpliﬁes training the
binary discriminator but may not reﬂect the expected frequency of OOD samples at test time and in
effect means that ˆp(i|x) tends to be quite low. This typically yields good guaranteed AUCs but can
have a negative impact on the standard out-distribution performance. In order to better explore the
trade-off of guaranteed and standard OOD detection, we repeat the above semi-joint training with
different shifts of the offset parameter in the output layer"
K,0.21676300578034682,"b′ = b(Lg)
g
+ ∆,
(8)"
K,0.21965317919075145,"where ∆≥0 leads to increasing ˆp(i|x). This shift has a direct interpretation in terms of the
probabilities p(i) and p(o). Under the assumption that our binary discriminator g is perfect, that is"
K,0.22254335260115607,"p(i|x) =
p(x|i)p(i)
p(x|i)p(i) + p(x|o)p(o) =
1"
K,0.2254335260115607,1 + p(x|o)p(o)
K,0.22832369942196531,"p(x|i)p(i)
=
1
1 + e−g(x) ,
(9)"
K,0.23121387283236994,"then it holds that eg(x) =
p(x|i)p(i)
p(x|o)p(o). A change of the prior probabilities ˜p(i) and ˜p(o) without
changing p(x|i) and p(x|o) then corresponds to a novel classiﬁer"
K,0.23410404624277456,e˜g(x) = p(x|i)˜p(i)
K,0.23699421965317918,p(x|o)˜p(o) = p(x|i)p(i)
K,0.2398843930635838,"p(x|o)p(o)
p(o)˜p(i)
p(i)˜p(o) = eg(x)e∆,
with
∆= log
p(o)˜p(i)"
K,0.24277456647398843,p(i)˜p(o)
K,0.24566473988439305,"
.
(10)"
K,0.24855491329479767,"Note that ˜p(i) > p(i) corresponds to positive shifts. In a practical setting, this parameter can be
chosen based on the priors for the particular application. Since no such priors are available in our
case we determine a suitable shift by evaluating on the training out-distribution, see Section 4.2 for
details. Please note that we explicitly do not train the shift parameter since this way the guarantees
would get lost as the classiﬁer implicitly learns a large ∆in order to maximize the conﬁdence on the
in-distribution, thus converging to a normal outlier exposure-type classiﬁer without any guarantees."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2514450867052023,"3
GUARANTEES ON ASYMPTOTIC CONFIDENCE"
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2543352601156069,"In this section we show that our speciﬁc construction provably avoids the issue of asymptotic
overconﬁdence that was pointed out in (Hein et al., 2019). Note that the resulting guarantee (as
stated in Theorem 1) is different from and in addition to the robustness guarantees discussed in the
previous section (see Eq. (3)). The previous section dealt with providing conﬁdence upper bounds
on neighborhoods around OOD samples whereas this section deals with ensuring that a classiﬁer’s
conﬁdence decreases asymptotically as one moves away from all training data."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.25722543352601157,"We note that a ReLU neural network f : Rd →RK as deﬁned in Eq. (1) using ReLU or leaky ReLU
as activation functions, potential max-or average pooling and skip connection yields a piece-wise
afﬁne function (Arora et al., 2018; Hein et al., 2019), i.e. there exists a ﬁnite set of polytopes Qr ⊂Rd
with r = 1, . . . , R such that ∪R
r=1Qr = Rd and f restricted to each of the polytopes is an afﬁne
function. Since there are only ﬁnitely many polytopes some of them have to extend to inﬁnity and on
these ones the neural network is essentially an afﬁne classiﬁer. This fact has been used in (Hein et al.,"
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.26011560693641617,Under review as a conference paper at ICLR 2022
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2630057803468208,"2019) to show that ReLU networks are almost always asymptotically overconﬁdent in the sense that
if one moves to inﬁnity the conﬁdence of the classiﬁer approaches 1 (instead of converging to 1/K as
in these regions the classiﬁer has never seen any data). The following result taken from (Hein et al.,
2019) basically says that as one moves to inﬁnity by upscaling a vector one eventually ends up in a
polytope which extends to inﬁnity."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2658959537572254,"Lemma 1 ((Hein et al., 2019)). Let {Qr}R
r=1 be the set of convex polytopes on which a ReLU-
network f : Rd →RK is an afﬁne function, that is for every k ∈{1, . . . , R} and x ∈Qk there exists
V k ∈RK×d and ck ∈RK such that f(x) = V kx + ck. For any x ∈Rd with x ̸= 0 there exists
α ∈R and t ∈{1, . . . , R} such that βx ∈Qt for all β ≥α."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.26878612716763006,"The following theorem now shows that, opposite to standard ReLU networks (Hein et al., 2019), our
proposed joint classiﬁer gets provably less conﬁdent in its decisions as one moves away from the
training data which is a desired property of any reasonable classiﬁer."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.27167630057803466,"Theorem 1. Let x ∈Rd with x ̸= 0 and let g : Rd →R be the ReLU-network of the binary
discriminator and denote by {Qr}R
r=1 the ﬁnite set of polytopes such that g is afﬁne on these
polytopes which exists by Lemma 1. Denote by Qt the polytope such that βx ∈Qt for all β ≥α and
let x(L−1)(z) = Uz + d with U ∈RnL−1×d and d ∈RnL−1 be the output of the pre-logit layer of g
for z ∈Qt. If Ux ̸= 0, then"
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2745664739884393,"lim
β→∞ˆp(y|βx) = 1 K ."
GUARANTEES ON ASYMPTOTIC CONFIDENCE,0.2774566473988439,"The proof is in App. C. In App. A we see that the condition Ux ̸= 0 is not restrictive, as this property
holds in all cases where we checked it for our joint classiﬁer. The negativity condition on the weights
W (Lg)
g
of the output layer of the in-vs. out-distribution discriminator g is crucial for the proof. This
condition may seem restrictive, but we did not encounter any negative inﬂuence of this constraint on
test accuracy, guaranteed or standard OOD detection performance. Thus the asymptotic guarantees
come essentially for free. In (Meinke & Hein, 2020) they derived non-asymptotic guarantees and it
would be relatively easy to also achieve this for the joint classiﬁer via a decay factor for ˆp(i|x) that
depends on the distance to the training data but we prefer not to enforce this explicitly."
EXPERIMENTS,0.28034682080924855,"4
EXPERIMENTS"
TRAINING OF PROOD,0.2832369942196532,"4.1
TRAINING OF PROOD"
TRAINING OF PROOD,0.2861271676300578,"We provide experiments on CIFAR10, CIFAR100 (Krizhevsky & Hinton, 2009) and Restricted
Imagenet (R.ImgNet) (Tsipras et al., 2018). The latter consists of images from the ILSVRC2012
subset of ImageNet (Deng et al., 2009; Russakovsky et al., 2015) belonging to 9 types of animals."
TRAINING OF PROOD,0.28901734104046245,"Training the Binary Discriminator
We train the binary discriminator between in-and out-
distribution using the loss in Eq. (4) with the bounds over an l∞-ball of radius ϵ = 0.01 for the
out-distribution following (Bitterwolf et al., 2020). We use relatively shallow CNNs with only 5 layers
plus pooling layers, see App. D. For the training out-distribution, we could follow previous work
and use 80M Tiny Images (Torralba et al., 2008) for CIFAR10 and CIFAR100. However, there have
been concerns over the use of this dataset (Birhane & Prabhu, 2021) because of offensive class labels.
Although we do not use any of the class labels, we choose to use OpenImages (Kuznetsova et al.,
2020) as training OOD instead. In order to ensure a fair comparison with prior work we also present
results that were obtained using 80M Tiny Images in App. E. For R.ImgNet we use the ILSVRC2012
train images that do not belong to R.ImgNet as training out-distribution (NotR.ImgNet)."
TRAINING OF PROOD,0.29190751445086704,"Semi-Joint Training
For the classiﬁer we use a ResNet18 architecture on CIFAR and a ResNet50
on R.ImgNet. Note that the architecture of our binary discriminator is over an order of magnitude
smaller than the one in (Bitterwolf et al., 2020) (11MB instead of 135MB) and thus the memory
overhead for the binary discriminator is less than a third of that of the classiﬁer. All schedules,
hardware and hyperparameters are described in App. D. As discussed in Section 2.1 when training
the binary discriminator one implicitly assumes that in- and (worst-case) out-distribution samples are
equally likely. It seems very unlikely that one would be presented with such a large number of OOD
samples in practice but as discussed in Section 2.1, we can adjust the weight of the losses after training"
TRAINING OF PROOD,0.2947976878612717,Under review as a conference paper at ICLR 2022
TRAINING OF PROOD,0.2976878612716763,"0
1
2
3
4
5
Bias Shift 0.94 0.95 0.96 0.97 0.98 0.99 1.00 AUC"
TRAINING OF PROOD,0.30057803468208094,"ProoD
OE
GOOD80"
TRAINING OF PROOD,0.30346820809248554,"0
1
2
3
4
5
Bias Shift 0.0 0.1 0.2 0.3 0.4 0.5 GAUC"
TRAINING OF PROOD,0.3063583815028902,"ProoD
OE
GOOD80"
TRAINING OF PROOD,0.3092485549132948,"0
1
2
3
4
5
Bias Shift 0.88 0.90 0.92 0.94"
TRAINING OF PROOD,0.31213872832369943,Accuracy
TRAINING OF PROOD,0.315028901734104,"ProoD
OE
GOOD80"
TRAINING OF PROOD,0.3179190751445087,"Figure 2: (Provable) OOD performance depends on the bias: Using CIFAR10 as the in-
distribution and the test set of OpenImages as OOD we plot the test accuracy, AUC and GAUC (see
Section 4.2) as a function of the bias shift ∆(see Eq. (8)). For small ∆the AUCs tend to be worse
than for the OE model, but small bias shifts also provide stronger guarantees so some trade-off exists."
TRAINING OF PROOD,0.3208092485549133,"the discriminator (but before training the classiﬁer) by shifting the bias b(Lg)
g
in the output layer of
the binary discriminator. We train several ProoD models for binary shifts in {0, 1, 2, 3, 4, 5, 6} and
then evaluate the AUC and guaranteed AUC (see 4.2) on a subset of the training out-distribution
OpenImages (resp. NotR.ImgNet). For all bias shifts we use the same ﬁxed provably trained binary
discriminator and only train the classiﬁer part. As our goal is to have provable guarantees with
minimal or no loss on the standard OOD detection task, among all solutions which have better AUC
than outlier exposure (OE) (Hendrycks et al., 2019) we choose the one with the highest guaranteed
AUC on OpenImages (on CIFAR10/CIFAR100) resp. NotR.ImgNet (on R.ImgNet). If none of the
solutions has better AUC than OE on the training out-distribution we take the one with the highest
AUC. We show the trade-off curves for the example of CIFAR10 in Figure 2. The corresponding
ﬁgures for CIFAR100 and R.ImgNet can be found in App. D."
EVALUATION,0.3236994219653179,"4.2
EVALUATION"
EVALUATION,0.3265895953757225,"Setup
For OOD evaluation for CIFAR10/100 we use the test sets from CIFAR100/10, SVHN (Net-
zer et al., 2011), the classroom category of downscaled LSUN (Yu et al., 2015) (LSUN_CR) as well
as smooth noise as suggested in (Hein et al., 2019) and described in App. D. For R.ImgNet we use
Flowers (Nilsback & Zisserman, 2008), FGVC Aircraft (Maji et al., 2013), Stanford Cars (Krause
et al., 2013) and smooth noise as test out-distributions. Since the computation of adversarial AUCs
(next paragraph) requires computationally expensive adversarial attacks, we restrict the evaluation on
the out-distribution to a ﬁxed subset of 1000 images (300 in the case of LSUN_CR) for the CIFAR
experiments and 400 for the R.ImgNet models. We still use the entire test set for the in-distribution."
EVALUATION,0.32947976878612717,"Guaranteed and Adversarial AUC
We use the conﬁdence of the classiﬁer as the feature to
discriminate between in- and out-distribution samples. While in standard OOD detection one uses
the area under the receiver-operator characteristic (AUC) to measure discrimination of in- from
out-distribution, Bitterwolf et al. (2020) introduced the worst-case AUC (WCAUC) which is deﬁned
as the minimal AUC one can achieve if all out-distribution samples are allowed to be perturbed to
reach maximal conﬁdence within a certain threat model, which in our case is an l∞-ball of radius ϵ.
The AUC and WAUC of a feature h : Rd →R is deﬁned as:"
EVALUATION,0.33236994219653176,"AUCh(p1, p2) = E
x∼p1
z∼p2"
EVALUATION,0.3352601156069364,"
1h(x)>h(z)

,
WCAUCh(p1, p2) = E
x∼p1
z∼p2 """
EVALUATION,0.33815028901734107,"1h(x)>
max
∥z′−z∥∞≤ϵ
h(z′) #"
EVALUATION,0.34104046242774566,",
(11)"
EVALUATION,0.3439306358381503,"where p1, p2 are in-resp. out-distribution and with slight abuse of notation the indicator function 1
returns 1 if the expression in its argument is true and 0 otherwise. For all but one of our baselines, the
OOD detecting feature h is the conﬁdence of the classiﬁer. Since the exact evaluation of the WCAUC
is computationally infeasible, we compute an upper bound and lower bound on the WCAUC by
ﬁnding h(z) ≤
max
∥z′−z∥∞≤ϵ h(z′) ≤¯h(z). We ﬁnd the upper bound on the WCAUC, the adversarial"
EVALUATION,0.3468208092485549,"AUC (AAUC), by maximizing the conﬁdence using an adversarial attack inside the l∞-ball (i.e.
ﬁnding an h) and we compute a lower bound on the WCAUC, the guaranteed AUC (GAUC), by using
upper on the conﬁdence inside the l∞-ball via IBP (i.e. ¯h). For non-provable methods, no non-trivial"
EVALUATION,0.34971098265895956,Under review as a conference paper at ICLR 2022
EVALUATION,0.35260115606936415,"upper bound ¯h < ∞is available so their GAUCs are always 0. Note that our threat model is different
from adversarial robustness on the in-distribution which neither our method nor the baselines pursue."
EVALUATION,0.3554913294797688,"Gradient obfuscation (Papernot et al., 2017; Athalye et al., 2018) poses a signiﬁcant challenge for
the evaluation of AAUCs (Bitterwolf et al., 2020) so we employ an ensemble of different versions
of projected gradient descent (PGD) (Madry et al., 2018) as well as SquareAttack (Andriushchenko
et al., 2020) with 5000 queries. We use APGD (Croce & Hein, 2020) (except on RImgNet, due
to a memory leak) with 500 iterations and 5 random restarts. We also use a 200-step PGD attack
with momentum of 0.9 and backtracking that starts with a step size of 0.1 which is halved every
time a gradient step does not increase the conﬁdence and gets multiplied by 1.1 otherwise. This
PGD is applied to different starting points: i) a decontrasted version of the image, i.e. the point that
minimizes the l∞-distance to the grey image 0.5 ·⃗1 within the threat model, ii) 3 uniformly drawn
samples from the threat model and iii) 3 versions of the original image perturbed by Gaussian noise
with σ = 10−4 and then clipped to the threat model. We always clip to the box [0, 1]d at each step of
the attack. For all attacks and all models we directly optimize the ﬁnal score that is used for OOD
detection. Using different types of starting points is crucial for strong attacks on these OOD points,
as some models have precisely zero gradient on many OOD samples."
EVALUATION,0.3583815028901734,"Baselines
We compare to a normally trained baseline (Plain) and outlier exposure (OE), both
trained using the same architecture and hyperparameters as the classiﬁer in ProoD. For both ATOM
and ACET we found the models’ OOD detection to be much less adversarially robust than claimed
in (Chen et al., 2020) (see App. E) so we retrained their models using the same architecture, threat
model and training out-distribution with their original code (for CIFAR10/100). Running these
adversarial training procedures on ImageNet resolution is infeasibly expensive. For GOOD we also
retrain using OpenImages as training OOD with the code from (Bitterwolf et al., 2020) (comparisons
with their pre-trained models can be found in App. E). Since they are only available on CIFAR10, we
attempted to train models on CIFAR100 using their code and the same hyperparameters and schedules
as they used for CIFAR10. This only lead to models with accuracy below 25%, so we do not include
these models in our evaluation. Since CCU was already shown to not provide beneﬁts over OE on
OOD data that is not very far from the in-distribution (e.g. uniform noise) (Meinke & Hein, 2020;
Bitterwolf et al., 2020) we do not include it as a baseline. We also evaluate the OOD-performance
of the provable binary discriminator (ProoD-Disc) that we trained for ProoD. Note that this is not a
classiﬁer and so it is included simply for reference. All results are shown in Table 2."
EVALUATION,0.36127167630057805,"Results
ProoD achieves non-trivial GAUCs on all datasets. As was also observed in (Bitterwolf
et al., 2020), this shows that the IBP guarantees not only generalize to unseen samples but even to
unseen distributions. In general the gap between our GAUCs and AAUCs is extremely small. This
shows that the seemingly simple IBP bounds can be remarkably tight, as has been observed in other
works (Gowal et al., 2018; Jovanovi´c et al., 2021). It also shows that there would be very little beneﬁt
in applying stronger veriﬁcation techniques like (Cheng et al., 2017; Katz et al., 2017; Dathathri
et al., 2020) in ProoD. The bounds are also much tighter than for GOOD, which is likely due to the
fact that for GOOD the conﬁdence is much harder to optimize during an attack because it involves
maximizing the conﬁdence in an essentially random class."
EVALUATION,0.36416184971098264,"For CIFAR10, on 3 out of 4 out-distributions ProoD’s GAUCs are higher than ATOM’s and ACET’s
AUUCs, i.e. our model’s provable adversarial robustness exceeds the SOTA methods’ empirical
adversarial robustness in these cases. Note that this is not due to our retraining, because the
authors’ pre-trained models perform even more poorly (as shown in App. E). On CIFAR100 ProoD’s
guarantees are weaker and ATOM produces strong AAUCs. However, we observe that training both
ACET and ATOM can produce inconsistent results, i.e. sometimes almost no robustness is achieved.
For the successfully trained robust ATOM model on CIFAR100 we observe drastically reduced
accuracy. Due to the difﬁculty in attacking these models, it is not unlikely that a more sophisticated
attack could produce even lower AAUCs. Combined with the fact that both ACET and ATOM rely
on expensive adversarial training procedures we argue that using ProoD is preferable in practice."
EVALUATION,0.3670520231213873,"On CIFAR10 we see that ProoD’s GAUCs are comparable to, if slightly worse than the ones of both
GOOD80 and GOOD100. Note that although the presented GOOD models are retrained, the same
observations hold true when comparing to the pre-trained models (see App. E). However, we want to
point out that ProoD achieves this while retaining both high accuracy and OOD performance, both
of which are lacking for GOOD. It is also noteworthy that the GOOD models’ memory footprints"
EVALUATION,0.3699421965317919,Under review as a conference paper at ICLR 2022
EVALUATION,0.37283236994219654,"Table 2: OOD performance: For all models we report accuracy on the test set of the in-distribution
and AUCs, guaranteed AUCs (GAUC), adversarial AUCs (AAUC) for different test out-distributions.
The radius of the l∞-ball for the adversarial manipulations of the OOD data is ϵ = 0.01 for all
datasets. The bias shift ∆that was used for ProoD is shown for each in-distribution. The AAUCs and
GAUCs for ProoD tend to be very close, indicating remarkably tight certiﬁcation bounds."
EVALUATION,0.37572254335260113,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
EVALUATION,0.3786127167630058,"Plain
95.01 90.0
0.0
0.7
93.8
0.0
0.3
93.1
0.0
0.5
98.0
0.0
0.7
OE
94.91 91.1
0.0
0.9
97.3
0.0
0.0
100.0
0.0
2.7
99.9
0.0
1.5
ATOM
93.63 78.3
0.0
21.7 94.4
0.0
24.1
79.8
0.0
20.1 99.5
0.0
73.2
ACET
93.43 86.0
0.0
4.0
99.3
0.0
4.6
89.2
0.0
3.7
99.9
0.0
40.2
GOOD80*
87.39 76.7 47.1
57.1 90.8 43.4
76.8
97.4
70.6
93.6 96.2 72.9
89.9
GOOD100*
86.96 67.8 48.1
49.7 62.6 34.9
36.3
84.9
74.6
75.6 87.0 76.1
78.1
ProoD-Disc
-
62.9 57.1
57.8 72.6 65.6
66.4
78.1
71.5
72.3 59.2 49.7
50.4
ProoD ∆=3
94.99 89.8 46.1
46.8 98.3 53.3
54.1 100.0 58.3
59.7 99.9 38.2
38.8"
EVALUATION,0.3815028901734104,"In: CIFAR100
CIFAR10
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
EVALUATION,0.38439306358381503,"Plain
77.38 77.7
0.0
0.4
81.9
0.0
0.2
76.4
0.0
0.3
86.6
0.0
0.4
OE
77.25 77.4
0.0
0.2
92.3
0.0
0.0
100.0
0.0
0.7
99.5
0.0
0.5
ATOM
68.32 78.3
0.0
50.3 91.1
0.0
67.0
95.9
0.0
75.6 98.2
0.0
80.7
ACET
73.02 73.0
0.0
1.4
97.8
0.0
0.7
75.8
0.0
2.6
99.9
0.0
12.8
ProoD-Disc
-
56.1 52.1
52.3 61.0 58.2
58.4
70.4
66.9
67.1 29.6 26.4
26.5
ProoD ∆=5
77.16 76.6 17.3
17.4 91.5 19.7
19.8 100.0 22.5
23.1 98.9
9.0
9.0"
EVALUATION,0.3872832369942196,"In: R.ImgNet
Flowers
FGVC
Cars
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
EVALUATION,0.3901734104046243,"Plain
96.34 92.3
0.0
0.5
92.6
0.0
0.0
92.7
0.0
0.1
98.9
0.0
8.6
OE
97.10 96.9
0.0
0.2
99.7
0.0
0.4
99.9
0.0
1.8
98.0
0.0
1.9
ProoD-Disc
-
81.5 76.8
77.3 92.8 89.3
89.6
90.7
86.9
87.3 81.0 74.0
74.8
ProoD ∆=4
97.25 96.9 57.5
58.0 99.8 67.4
67.9
99.9
65.7
66.2 98.6 52.7
53.5"
EVALUATION,0.3930635838150289,"*Uses different architecture of classiﬁer, see “Baselines” in Section 4.2."
EVALUATION,0.3959537572254335,"are over twice as large as ProoD’s. Generally, for ProoD the accuracy is comparable to OE and the
OOD performance is similar or marginally worse. Thus ProoD shows that it is possible to achieve
certiﬁable adversarial robustness on the out-distribution while keeping very good prediction and
OOD detection performance. Note that all methods struggle on separating CIFAR10 and CIFAR100
when using OpenImages as training OOD (as compared to 80M Tiny Images in App. E)."
EVALUATION,0.3988439306358382,"To the best of our knowledge with R.ImgNet we provide the ﬁrst worst case OOD guarantees on
high-resolution images. The GAUCs are higher than on CIFAR which indicates that meaningful
certiﬁcation on higher resolution is more achievable on this task than one might expect. FGVC and
Cars may seem simple to separate from the animals in R.ImgNet but the same cannot be said for
Flowers which are difﬁcult to provably distinguish from images of insects with ﬂowers in them."
CONCLUSION,0.40173410404624277,"5
CONCLUSION"
CONCLUSION,0.4046242774566474,"We have demonstrated how to combine a provably robust binary discriminator between in- and
out-distribution with a standard classiﬁer in order to simultaneously achieve high accuracy, high OOD
detection performance as well as worst-case OOD guarantees that are comparable to previous works.
Thus, we have combined the best properties of previous work with only a small increase in total
model size and only a single hyperparameter. This suggests that certiﬁable adversarial robustness on
the out-distribution (as opposed to the in-distribution) is indeed possible without losing accuracy. We
further showed how in our model simply enforcing negativity in the ﬁnal weights of the discriminator
ﬁxes the problem of asymptotic overconﬁdence in ReLU classiﬁers. Training ProoD models is simple
and stable and thus ProoD provides OOD guarantees that come (almost) for free."
CONCLUSION,0.407514450867052,Under review as a conference paper at ICLR 2022
CONCLUSION,0.41040462427745666,REPRODUCIBILITY
CONCLUSION,0.41329479768786126,"All details necessary for reproducing our experiments are given in Section 4 and App. D, including
hyperparameters and hyperparameter selection. In order to further aid reproducibility we include the
source code in the supplemental material (github link in ﬁnal version), together with instructions for
how to use it. We will also provide pre-trained models for public download in the ﬁnal version."
CONCLUSION,0.4161849710982659,ETHICS
CONCLUSION,0.4190751445086705,"The only potential ethical concern that we see in this work is our use of the retracted dataset 80M Tiny
Images (Torralba et al., 2008). However, we clearly explain that the only reason we use the dataset
is providing a fair comparison to prior work. We ﬁrmly believe that our work helps the community
move away from the use of this dataset because we provide extensive experiments on a different
training out-distribution, going as far as retraining several previous baselines on it."
REFERENCES,0.42196531791907516,REFERENCES
REFERENCES,0.42485549132947975,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efﬁcient black-box adversarial attack via random search. In ECCV, 2020."
REFERENCES,0.4277456647398844,"R. Arora, A. Basuy, P. Mianjyz, and A. Mukherjee. Understanding deep neural networks with rectiﬁed
linear unit. In ICLR, 2018."
REFERENCES,0.430635838150289,"A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. In ICML, 2018."
REFERENCES,0.43352601156069365,"Leonard Berrada, Sumanth Dathathri, Robert Stanforth, Rudy Bunel, Jonathan Uesato, Sven
Gowal, M Pawan Kumar, et al. Verifying probabilistic speciﬁcations with functional lagrangians.
arXiv:2102.09479, 2021."
REFERENCES,0.43641618497109824,"Abeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision?
In WACV, 2021."
REFERENCES,0.4393063583815029,"Julian Bitterwolf, Alexander Meinke, and Matthias Hein. Certiﬁably adversarially robust detection of
out-of-distribution data. NeurIPS, 2020."
REFERENCES,0.4421965317919075,"Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Informative outlier matters:
Robustifying out-of-distribution detection using outlier mining. preprint, arXiv:2006.15207, 2020."
REFERENCES,0.44508670520231214,"Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural
networks. In International Symposium on Automated Technology for Veriﬁcation and Analysis,
2017."
REFERENCES,0.4479768786127168,"M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014."
REFERENCES,0.4508670520231214,"F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In ICML, 2020."
REFERENCES,0.45375722543352603,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In CVPR, 2019."
REFERENCES,0.45664739884393063,"Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato,
Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy Liang, et al. Enabling
certiﬁcation of veriﬁcation-agnostic networks via memory-efﬁcient semideﬁnite programming. In
NeurIPS, 2020."
REFERENCES,0.4595375722543353,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009."
REFERENCES,0.4624277456647399,Under review as a conference paper at ICLR 2022
REFERENCES,0.4653179190751445,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv:1810.12715, 2018."
REFERENCES,0.4682080924855491,"C. Guo, G. Pleiss, Y. Sun, and K. Weinberger. On calibration of modern neural networks. In ICML,
2017."
REFERENCES,0.47109826589595377,"M. Hein, M. Andriushchenko, and J. Bitterwolf.
Why ReLU networks yield high-conﬁdence
predictions far away from the training data and how to mitigate the problem. In CVPR, 2019."
REFERENCES,0.47398843930635837,"D. Hendrycks and K. Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution examples
in neural networks. In ICLR, 2017."
REFERENCES,0.476878612716763,"D. Hendrycks, M. Mazeika, and T. Dietterich. Deep anomaly detection with outlier exposure. In
ICLR, 2019."
REFERENCES,0.4797687861271676,"Nikola Jovanovi´c, Mislav Balunovi´c, Maximilian Baader, and Martin Vechev. Certiﬁed defenses:
Why tighter relaxations may hurt training? arXiv:2102.06700, 2021."
REFERENCES,0.48265895953757226,"G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efﬁcient smt solver for
verifying deep neural networks. In CAV, 2017."
REFERENCES,0.48554913294797686,"Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Zügner, Sandhya Giri, and Stephan Günnemann.
Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based models reliable?
arXiv:2010.14986, 2020."
REFERENCES,0.4884393063583815,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization. In ICCV vision workshop, 2013."
REFERENCES,0.4913294797687861,"Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, Even Just a Bit, Fixes
Overconﬁdence in ReLU Networks. In ICML, 2020a."
REFERENCES,0.49421965317919075,"Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Fixing asymptotic uncertainty of bayesian
neural networks with inﬁnite relu features. arXiv:2010.02709, 2020b."
REFERENCES,0.49710982658959535,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009."
REFERENCES,0.5,"Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4.
International Journal of Computer Vision, 2020."
REFERENCES,0.5028901734104047,"K. Lee, H. Lee, K. Lee, and J. Shin. Training conﬁdence-calibrated classiﬁers for detecting out-of-
distribution samples. In ICLR, 2018a."
REFERENCES,0.5057803468208093,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks. In NeurIPS, 2018b."
REFERENCES,0.5086705202312138,"Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In ICLR, 2018."
REFERENCES,0.5115606936416185,"Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection.
Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.5144508670520231,"David Macêdo and Teresa Ludermir. Enhanced isotropy maximization loss: Seamless and high-
performance out-of-distribution detection simply replacing the softmax loss. arXiv:2105.14399,
2021."
REFERENCES,0.5173410404624278,"David Macêdo, Tsang Ing Ren, Cleber Zanchettin, Adriano LI Oliveira, and Teresa Ludermir. Entropic
out-of-distribution detection: Seamless detection of unknown examples. IEEE Transactions on
Neural Networks and Learning Systems, 2021."
REFERENCES,0.5202312138728323,"A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Valdu. Towards deep learning models resistant
to adversarial attacks. In ICLR, 2018."
REFERENCES,0.523121387283237,Under review as a conference paper at ICLR 2022
REFERENCES,0.5260115606936416,"Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classiﬁcation of aircraft. arXiv:1306.5151, 2013."
REFERENCES,0.5289017341040463,"Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In NeurIPS,
2018."
REFERENCES,0.5317919075144508,"Andrey Malinin and Mark Gales. Reverse kl-divergence training of prior networks: Improved
uncertainty and adversarial robustness. In NeurIPS, 2019."
REFERENCES,0.5346820809248555,"Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they don’t
know. In ICLR, 2020."
REFERENCES,0.5375722543352601,"M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust neural
networks. In ICML, 2018."
REFERENCES,0.5404624277456648,"Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images
with unsupervised feature learning. In NeurIPS Workshop on Deep Learning and Unsupervised
Feature Learning, 2011."
REFERENCES,0.5433526011560693,"A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In CVPR, 2015."
REFERENCES,0.546242774566474,"Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number
of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
2008."
REFERENCES,0.5491329479768786,"Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang.
Outlier exposure with conﬁdence control for out-of-distribution detection. Neurocomputing, 2021."
REFERENCES,0.5520231213872833,"Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In ACM ASIACCS, 2017."
REFERENCES,0.5549132947976878,"Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon,
and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In NeurIPS,
2019."
REFERENCES,0.5578034682080925,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV),
2015."
REFERENCES,0.5606936416184971,"Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung Chiang,
and Prateek Mittal. Better the devil you know: An analysis of evasion attacks using out-of-
distribution adversarial examples. preprint, arXiv:1905.01726, 2019."
REFERENCES,0.5635838150289018,"Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classiﬁcation
uncertainty. In NeurIPS, 2018."
REFERENCES,0.5664739884393064,"Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 30(11):1958–1970, 2008."
REFERENCES,0.569364161849711,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In ICLR, 2018."
REFERENCES,0.5722543352601156,"Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, and Jianx-
iong Xiao. Rich feature hierarchies for accurate object detection and semantic segmentation.
arXiv:1504.06755, 2015."
REFERENCES,0.5751445086705202,"Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365,
2015."
REFERENCES,0.5780346820809249,Under review as a conference paper at ICLR 2022
REFERENCES,0.5809248554913294,"Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efﬁcient training of veriﬁably robust neural networks. In
ICLR, 2020."
REFERENCES,0.5838150289017341,"Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452–1464, 2017."
REFERENCES,0.5867052023121387,Under review as a conference paper at ICLR 2022
REFERENCES,0.5895953757225434,"100
101
102
103
104 alpha 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5924855491329479,Confidence
REFERENCES,0.5953757225433526,"Plain
OE
ATOM
ACET
GOOD80
ProoD 3"
REFERENCES,0.5982658959537572,"100
101
102
103
104 alpha 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.6011560693641619,Confidence
REFERENCES,0.6040462427745664,"Figure 3: Left, Asymptotic conﬁdence: We plot the mean conﬁdence in the predicted in-distribution
class for different models as one moves away from CIFAR100 samples along the trajectories x + αn,
where n ∈[−0.5, 0.5]d and α ≥0. Only GOOD and ProoD converge to uniform conﬁdence. Right,
Adversarial asymptotic conﬁdence: We try to ﬁnd adversarial directions in which ProoD remains
at a constant high conﬁdence, as opposed to converging to low conﬁdence. We plot the maximum of
ˆp(i|x) across 100 adversarially chosen directions as one moves further in these directions by factors
of α. Note that ˆp(i|x) →0 implies ˆp(y|x) →1 K ."
REFERENCES,0.6069364161849711,"A
ADVERSARIAL ASYMPTOTIC OVERCONFIDENCE"
REFERENCES,0.6098265895953757,"According to the authors of (Hein et al., 2019), under mild conditions, we should expect to ﬁnd
asymptotic overconﬁdence in all ReLU networks and almost all directions. In order to empirically
evaluate this, we take different models that were trained on CIFAR10 and evaluate their conﬁdence
on different CIFAR100 samples. For each sample x we track the conﬁdence, maxk ˆp(k|x), along
a trajectory in a uniform noise direction x + αn, where n ∈[−0.5, 0.5]d and α ≥0. The mean
conﬁdence across 100 such trajectories is shown on the left side of Figure 3. Even the models that
produce low conﬁdences on the original OOD sample asympotically converge to maximal conﬁdence
far away. The only exceptions here are GOOD and ProoD and only ProoD can guarantee that the
conﬁdence cannot converge to 1."
REFERENCES,0.6127167630057804,"However, even though the architecture provably prevents arbitrarily overconﬁdent predictions and
Theorem 1 ensures that most directions will indeed converge to uniform, it is, in principle, possible to
ﬁnd directions where the conﬁdence ˆp(i|x) remains constant if the condition Ux ̸= 0 in Theorem 1
is not satisﬁed. We attempted to ﬁnd such directions by running the following type of attack. We
start from a random point x ∈[−0.5, 0.5]d that we project onto a sphere of radius 100. We now run
gradient descent (for 20000 steps), maximizing g(x) while projecting onto the sphere at each step
(unnormalized gradients with step size 0.1 for the ﬁrst 10000 steps and 0.01 for the last 10000 steps).
We then increase the radius to 1000 and run an additional 20000 steps with step size 0.1. We rescale
the resulting direction vector down to an l∞-ball of norm 1 and compute the conﬁdence ˆp(i|x) as a
function of the scaling in the adversarial directions. We show the resulting scale-wise maximum over
100 adversarial directions in Figure 3. Note that even the worst-case over 100 adversarially found
directions decays to 0 asymptotically, thus empirically conﬁrming the practical utility of Theorem 1.
Note that the value of ˆp(i|x) converging to 0 implies that the conﬁdence of the ProoD model ˆp(y|x)
converges to 10%."
REFERENCES,0.615606936416185,"In Figure 3 GOOD also stands out as having low conﬁdence in all directions that we studied. This
is because in all the asymptotic regions that we looked at, the pre-activations of the penultimate
layer are all negative. If one moves outward and these pre-activations only get more negative in all
directions far away from the data, the conﬁdence does, in fact, remain low. Unfortunately, it also leads
to gradients that are precisely zero, which is why the same attack can not be applied here. However,
there is no guarantee that GOOD does not also get in some direction asymptotically overconﬁdent."
REFERENCES,0.6184971098265896,Under review as a conference paper at ICLR 2022
REFERENCES,0.6213872832369942,"Table 3: Architecture: The architectures that are used for the binary discriminators. Each convolu-
tional layer is directly followed by a ReLU."
REFERENCES,0.6242774566473989,"CIFAR
R.ImgNet"
REFERENCES,0.6271676300578035,"Conv2d(3, 128)
Conv2d(3, 128)
Conv2d(128, 256)s=2
AvgPool(2)
Conv2d(256, 256)
Conv2d(128, 256)s=2
AvgPool(2)
AvgPool(2)
FC(16384, 128)
Conv2d(256, 256)
FC(128, 1)
AvgPool(2)
FC(50176, 128)
FC(128, 1)"
REFERENCES,0.630057803468208,"B
SEPARATE TRAINING FOR PROOD"
REFERENCES,0.6329479768786127,"In Section 2.1 we describe semi-joint training of ˆp(y|x). However, as pointed out in that section, it
is possible to separately train a certiﬁable binary discriminator ˆp(i|x) and an OOD aware classiﬁer
ˆp(y|x, i) and to then simply combine them via Eq. (2). We call this method separate training ProoD-S
and evaluate it by using an OE trained model for ˆp(y|x, i). We show the results in Table 4, where we
repeat the results for OE and ProoD for the reader’s convenience. Note that OE and ProoD-S must
always have the same accuracy on the in-distribution since they use the same model for classiﬁcation
(note that (2) preserves the ranking of ˆp(y|x, i))."
REFERENCES,0.6358381502890174,"We see that the AUCs of ProoD-S are almost identical to those of OE. Without almost any loss in
performance ProoD-S manages to provide non-trivial GAUCs. However, as one would expect, the
semi-jointly trained ProoD provides stronger guarantees at similar clean performance. Nonetheless,
this post-hoc method of adding some amount of certiﬁability to an existing system may be interesting
in applications where retraining a deployed model from scratch is infeasible."
REFERENCES,0.638728323699422,"Table 4: Separate Training: Addendum to Table 2 showing the AUCs, GAUCs and AAUCs of
ProoD-S on all datasets. The accuracy must always be identical to that of OE and the clean AUCs
are also very similar to those of OE. The guarantees are strictly weaker than those provided by the
semi-jointly trained ProoD."
REFERENCES,0.6416184971098265,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.6445086705202312,"OE
94.91 91.1
0.0
0.9
97.3
0.0
0.0
100.0
0.0
2.7
99.9
0.0
1.5
ProoD-S ∆=3 94.91 89.3 44.7
45.3 97.3 51.8
52.6 100.0 56.7
57.7 99.9 36.7
37.6
ProoD ∆=3
94.99 89.8 46.1
46.8 98.3 53.3
54.1 100.0 58.3
59.7 99.9 38.2
38.8"
REFERENCES,0.6473988439306358,"In: CIFAR100
CIFAR10
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.6502890173410405,"OE
77.25 77.4
0.0
0.2
92.3
0.0
0.0
100.0
0.0
0.7
99.5
0.0
0.5
ProoD-S ∆=5 77.25 77.4 17.2
17.3 92.3 19.5
19.6 100.0 22.4
22.6 99.5
9.0
9.1
ProoD ∆=5
77.16 76.6 17.3
17.4 91.5 19.7
19.8 100.0 22.5
23.1 98.9
9.0
9.0"
REFERENCES,0.653179190751445,"In: R.ImgNet
Flowers
FGVC
Cars
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.6560693641618497,"OE
97.10 96.9
0.0
0.2
99.7
0.0
0.4
99.9
0.0
1.8
98.0
0.0
1.9
ProoD-S ∆=4 97.10 96.9 50.1
50.7 99.7 59.7
60.6
99.9
57.9
58.9 98.0 40.8
42.3
ProoD ∆=4
97.25 96.9 57.5
58.0 99.8 67.4
67.9
99.9
65.7
66.2 98.6 52.7
53.5"
REFERENCES,0.6589595375722543,"C
PROOF OF THEOREM 1"
REFERENCES,0.661849710982659,"Theorem 1. Let x ∈Rd with x ̸= 0 and let g : Rd →R be the ReLU-network of the binary
discriminator and denote by {Qr}R
r=1 the ﬁnite set of polytopes such that g is afﬁne on these"
REFERENCES,0.6647398843930635,Under review as a conference paper at ICLR 2022
REFERENCES,0.6676300578034682,"polytopes which exists by Lemma 1. Denote by Qt the polytope such that βx ∈Qt for all β ≥α and
let x(L−1)(z) = Uz + d with U ∈RnL−1×d and d ∈RnL−1 be the output of the pre-logit layer of g
for z ∈Qt. If Ux ̸= 0, then"
REFERENCES,0.6705202312138728,"lim
β→∞ˆp(y|βx) = 1 K ."
REFERENCES,0.6734104046242775,Proof. We note that with a similar argument as in the derivation of (3) it holds
REFERENCES,0.6763005780346821,ˆp(y|βx) ≤ˆp(i|βx) + 1
REFERENCES,0.6791907514450867,"K
 
1 −ˆp(i|βx)

= K −1"
REFERENCES,0.6820809248554913,"K
ˆp(i|βx) + 1"
REFERENCES,0.684971098265896,"K .
(12)"
REFERENCES,0.6878612716763006,For all β ≥α it holds that βx ∈Qt so that
REFERENCES,0.6907514450867052,"ˆp(i|βx) =
1
1 + e−g(βx) =
1 1 + e"
REFERENCES,0.6936416184971098,"D
W
(Lg)
g
,Uβx+d
E
+b
(Lg)
g
."
REFERENCES,0.6965317919075145,"As x(L−1)
i
(x) ≥0 for all x ∈Rd it has to hold (βUx + d)i ≥0 for all β ≥α and i = 1, . . . , nL−1.
This implies that (Ux)i ≥0 for all i = 1, . . . , nL−1 and since Ux ̸= 0 there has to exist at least one
component i∗such that (Ux)i∗> 0. Moreover, W (Lg)
g
has strictly negative components and thus for
all β ≥α it holds"
REFERENCES,0.6994219653179191,"g(βx) =
D
W (Lg)
g
, Uβx + d
E
+ b(Lg)
g
= β
D
W (Lg)
g
, Ux
E
+
D
W (Lg)
g
, d
E
+ b(Lg)
g
."
REFERENCES,0.7023121387283237,"As
D
W (Lg)
g
, Ux
E
< 0 we get limβ→∞g(x) = −∞and thus"
REFERENCES,0.7052023121387283,"lim
β→∞ˆp(i|βx) = 0."
REFERENCES,0.708092485549133,Plugging this into (12) yields the result.
REFERENCES,0.7109826589595376,"D
EXPERIMENTAL DETAILS"
REFERENCES,0.7138728323699421,"Datasets
We use CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009) (MIT license),
SVHN (Netzer et al., 2011) (free for non-commercial use), LSUN (Yu et al., 2015) (no license),
the ILSVRC2012 split of ImageNet (Deng et al., 2009; Russakovsky et al., 2015) (free for non-
commercial use), FGVC-Aircraft (Maji et al., 2013) (free for non-commercial use), Stanford
Cars (Krause et al., 2013) (free for non-commercial use), OpenImages v4 (Kuznetsova et al., 2020)
(images have a CC BY 2.0 license), Oxford 102 Flower (Nilsback & Zisserman, 2008) (no license)
as well as 80 million tiny images (Torralba et al., 2008) (no license given, see also App. E). For the
train/test splits we use the standard splits, except on 80M Tiny Images where we treat a random but
ﬁxed subset of 1000 images in the ﬁrst 1,000,000 as our test set. For all datasets that get used as a
test out-distribution we use a random but ﬁxed subset of 1000 images."
REFERENCES,0.7167630057803468,"Following (Bitterwolf et al., 2020), the smooth noise that is used is generated as follows. Uniform
noise is generated and then smoothed using a Gaussian ﬁlter with a width that is drawn uniformly at
random in [1, 2.5]. Each datapoint is then shifted and scaled linearly to ensure full range in [0, 1], i.e.
x′ =
x−min(x)
max(x)−min(x)."
REFERENCES,0.7196531791907514,"Binary Training
The architecture that we use for the binary discriminator is relatively shallow
(5 linear layers). The architecture is shown in Table 3. Our results are fairly robust to the exact
choice of architecture and signiﬁcantly larger models do not necessarily lead to better results as
we show in App. I. Similarly to (Zhang et al., 2020; Bitterwolf et al., 2020), we use long training
schedules, running Adam for 1000 epochs, with an initial learning rate of 1e −4 that we decrease
by a factor of 5 on epochs 500, 750 and 850 and with a batch size of 128 from the in-distribution
and 128 from the out-distribution (for R.ImgNet: 50 epochs with drops at 25, 35, 45, batch sizes
32). In order to avoid large losses we also use a simple ramp up schedule for the ϵ used in IBP and
we downweight the out-distribution loss during the initial phase of training by a scalar κ. Both ϵ
and κ are increased linearly from 0 to their ﬁnal values (0.01 and 1, respectively) over the ﬁrst 300
epochs (for R.ImgNet over the ﬁrst 25 epochs). Compared to the training of (Bitterwolf et al., 2020)"
REFERENCES,0.7225433526011561,Under review as a conference paper at ICLR 2022
REFERENCES,0.7254335260115607,"0
1
2
3
4
5
6
Bias Shift 0.986 0.988 0.990 0.992 0.994 0.996 AUC"
REFERENCES,0.7283236994219653,"ProoD
OE"
REFERENCES,0.7312138728323699,"0
1
2
3
4
5
6
Bias Shift 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 GAUC"
REFERENCES,0.7341040462427746,"ProoD
OE"
REFERENCES,0.7369942196531792,"0
1
2
3
4
5
6
Bias Shift 0.74 0.75 0.76 0.77 0.78 0.79"
REFERENCES,0.7398843930635838,Accuracy
REFERENCES,0.7427745664739884,"ProoD
OE"
REFERENCES,0.7456647398843931,"0
1
2
3
4
5
6
Bias Shift 0.955 0.960 0.965 0.970 0.975 0.980 0.985 0.990 AUC"
REFERENCES,0.7485549132947977,"ProoD
OE"
REFERENCES,0.7514450867052023,"0
1
2
3
4
5
6
Bias Shift 0.0 0.1 0.2 0.3 0.4 0.5 0.6 GAUC"
REFERENCES,0.7543352601156069,"ProoD
OE"
REFERENCES,0.7572254335260116,"0
1
2
3
4
5
6
Bias Shift 0.960 0.965 0.970 0.975 0.980"
REFERENCES,0.7601156069364162,Accuracy
REFERENCES,0.7630057803468208,"ProoD
OE"
REFERENCES,0.7658959537572254,"Figure 4: Bias selection for CIFAR100 and RImgNet: Using CIFAR100 (top) and R.ImgNet
(bottom) as the in-distribution and the test set of OpenImages (or NotR.ImgNet respectively) as OOD
we plot the test accuracy, AUC and GAUC as a function of the bias shift ∆(see Eq. (8))."
REFERENCES,0.7687861271676301,"which sometimes fails, we found that training of the binary discriminator is very stable and even 100
epochs on CIFAR would be sufﬁcient, but we found that longer training lead to slightly better results.
Weight decay is set to 5 · 10−4, but is disabled for the weights in the ﬁnal layer. As data augmentation
we use AutoAugment (Cubuk et al., 2019) for CIFAR and simple 4 pixel crops and reﬂections on
R.ImgNet. The strict negativity of the weights leads to a negative bias of g which can cause problems
at an early stage if the b(Lg)
g
is initialized at 0 and thus we choose 3 as initialization. All binary
discriminators were trained on single 2080Ti GPUs, managed on a SLURM cluster. Overall, the
training of a provable discriminator takes around 16h on CIFAR and 44h on R.ImgNet (wall clock
time including evaluations and logging on each epoch)."
REFERENCES,0.7716763005780347,"Semi-Joint Training
On CIFAR we train for 100 epochs using SGD with momentum of 0.9 and a
learning rate of 0.1 that drops by a factor of 10 on epochs 50, 75 and 90 (on R.ImgNet 75 epochs
with drops at 30 and 60). For all datasets we train using a batch size of 128 (plus 128 out-distribution
samples in the case of OE). The CIFAR experiments were run on single 2080Ti GPUs. This takes
about 4h20min in wall clock time. In order to ﬁt batches of 128 in-distribution samples and 128
out-distribution samples on R.ImgNet we had to train using 4 V100 GPUs in parallel. Because of
batch normalization in multi-GPU training it is important to not simply stack the batches but to
interlace in- and out-distribution samples. The wall clock time was around 15h for the semi-joint
training on R.ImgNet. For selecting the bias we use the procedure described in Section 4. The
trade-off curves for the AUC and GAUC on CIFAR100 and R.ImgNet are given in Figure 4."
REFERENCES,0.7745664739884393,"E
80M TINY IMAGES AS TRAINING OUT-DISTRIBUTION"
REFERENCES,0.7774566473988439,"The 80M Tiny Images dataset has been retracted by the authors due to concerns over offensive class
labels (Birhane & Prabhu, 2021). We support the decision of the community to move away from
the use of 80M Tiny Images, so we choose to train our CIFAR models using a downscaled version
of OpenImages v4 (Kuznetsova et al., 2020) as a training out-distribution. However, since all prior
work used this dataset, we present results on 80M Tiny Images here in order to compare ProoD’s
performance to prior baselines. We encourage the community to use the results in Table 2 for future
comparisons."
REFERENCES,0.7803468208092486,"Our results are shown in Table 5. Apart from the models shown in Table 2 we add here the pre-trained
energy-based OOD detector EB from (Liu et al., 2020) as an additional baseline for clean OOD"
REFERENCES,0.7832369942196532,Under review as a conference paper at ICLR 2022
REFERENCES,0.7861271676300579,"detection. As EB is not trained robustly, as expected EB has low AAUCs. In terms of clean OOD
detection it performs similar to OE but with worse results on the more difﬁcult OOD detection task
CIFAR10 vs CIFAR100 and vice versa. For GOOD we use the pre-trained models from (Bitterwolf
et al., 2020). For ATOM and ACET we use the pre-trained models from (Chen et al., 2020). Note that
these use the densenet architecture and were actually trained to withstand attacks in the much stronger
threat model of ϵ =
8
255. In the original paper, the authors claim near perfect AAUCs on this task but
we can show that this is a case of gross overestimation of robustness. Even on the easier threat model
ϵ = 0.01 that we test in Table 5, their robustness is almost non-existent for both ATOM models and
the CIFAR100 ACET model. The fact that their adversarial attacks were unable to ﬁnd these samples
clearly demonstrates that evaluating models adversarially is very difﬁcult and potentially unreliable.
Because of this we believe that our guarantees are a valuable contribution to the community."
REFERENCES,0.7890173410404624,"Table 5: Training with 80M Tiny Images: We repeat the evaluation from Table 2 for models that
were trained using 80M Tiny Images as out-distribution instead of OpenImages. Plain is identical
to before and is just repeated for the reader’s convenience. Note that the conclusions from the
main paper still hold, which indicates that our method is robust to changes in the choice of training
out-distribution. For ATOM and ACET we compare to pre-trained models from (Chen et al., 2020).
Note that these models show almost no robustness on CIFAR100 - despite the far stronger claims in
(Chen et al., 2020)."
REFERENCES,0.791907514450867,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.7947976878612717,"Plain
95.01 90.0
0.0
0.6
93.8
0.0
0.1
93.1
0.0
0.5
98.2
0.0
0.6
OE
95.53 96.1
0.0
6.0
99.2
0.0
0.4
99.5
0.0
15.2 99.0
0.0
11.3
EB⋆
95.22 93.8
0.0
2.8
99.3
0.0
0.0
99.5
0.0
6.0
99.4
0.0
3.5
ATOM†
95.20 93.7
0.0
14.4 99.6
0.0
8.6
99.7
0.0
40.0 99.6
0.0
18.8
ACET†
91.48 91.2
0.0
80.5 95.3
0.0
87.6 98.9
0.0
95.0 99.9
0.0
98.3
GOOD80*
90.13 87.2 42.5
63.9 94.2 37.5
67.4 93.3 55.2
83.6 95.3 57.3
88.5
GOOD100*
90.14 70.7 54.5
55.0 74.9 56.3
56.6 75.2 61.0
61.6 81.4 66.6
67.5
ProoD-Disc
-
67.4 61.0
61.7 73.2 65.5
66.4 78.0 72.2
72.7 82.3 71.5
72.9
ProoD ∆=3
95.47 96.0 41.9
43.9 99.5 48.8
49.4 99.6 47.6
53.1 99.7 55.8
57.0"
REFERENCES,0.7976878612716763,"In: CIFAR100
CIFAR10
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.8005780346820809,"Plain
77.38 77.7
0.0
0.3
81.9
0.0
0.2
76.4
0.0
0.3
88.8
0.0
0.5
OE
77.28 83.9
0.0
0.8
92.8
0.0
0.1
97.4
0.0
4.6
97.6
0.0
0.9
EB⋆
75.70 77.4
0.0
0.8
96.5
0.0
0.0
96.7
0.0
5.9
98.9
0.0
4.3
ATOM†
75.06 64.3
0.0
0.2
93.6
0.0
0.2
97.5
0.0
9.3
98.5
0.0
15.0
ACET†
74.43 79.8
0.0
0.2
90.2
0.0
0.0
96.0
0.0
2.1
92.9
0.0
0.3
ProoD-Disc
-
53.8 50.3
50.4 73.1 69.8
69.9 68.1 63.8
64.0 67.2 63.8
63.9
ProoD ∆=1
76.79 80.5 23.1
23.2 93.7 33.9
34.0 97.2 29.6
30.4 98.9 29.7
31.3"
REFERENCES,0.8034682080924855,"⋆Pre-trained WideResnet from (Liu et al., 2020).
†Densenet architecture, using models from (Chen et al., 2020) pre-trained with ϵ =
8
255.
*CNN architecture using pre-trained models from (Bitterwolf et al., 2020)."
REFERENCES,0.8063583815028902,"F
FALSE POSITIVE RATES"
REFERENCES,0.8092485549132948,"Since in a practical setting a threshold for OOD detection ultimately has to be chosen, it can be
interesting to study the false positive rate at a ﬁxed threshold. It is relatively standard to pick the
false positive rate at 95% true positive rate (called FPR in Table 6), where low values are desirable.
We show the results for all methods and datasets in Table 6. Although ProoD has similarly good
performance as OE on this task, it fails to give non-trivial guarantees. Therefore achieving stronger
bounds on the worst-case FPR is an interesting task for future work."
REFERENCES,0.8121387283236994,Under review as a conference paper at ICLR 2022
REFERENCES,0.815028901734104,"Table 6: False Positive Rates: For all models we report accuracy on the test of the in-distribution and
the false positive rate at 95% true positive rate (FPR) (smaller is better). We also show the adversarial
FPR (AFPR) and the guaranteed FPR (GFPR) for different test out-distributions. The radius of the
l∞-ball for the adversarial manipulations of the OOD data is ϵ = 0.01 for all datasets. The bias shift
∆that was used for ProoD is shown for each in-distribution. ProoD struggles to give non-trivial
guarantees for the FPR@95% on most datasets. However, different from GOOD or ProoD-Disc, the
clean performance is generally as good as that of OE."
REFERENCES,0.8179190751445087,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc
FPR GFPR AFPR FPR GFPR AFPR FPR GFPR AFPR
FPR
GFPR AFPR"
REFERENCES,0.8208092485549133,"Plain
95.01 56.3 100.0 100.0 40.7 100.0 100.0 46.7 100.0 100.0 10.6 100.0 100.0
OE
94.91 52.2 100.0 99.9 15.4 100.0 100.0 0.0 100.0 99.0
0.0
100.0 99.0
ATOM
93.63 73.4 100.0 98.8 33.9 100.0 100.0 85.3 100.0 100.0
0.0
100.0 86.1
ACET
93.43 65.4 100.0 99.5
3.0 100.0 99.8 62.7 100.0 100.0
0.0
100.0 89.0
GOOD80
87.39 65.1 100.0 84.8 26.8 100.0 48.8
6.0 100.0 24.7
19.6 100.0 52.8
GOOD100
86.96 84.8 100.0 99.3 87.9 100.0 99.7 66.0 100.0 99.0
68.6 100.0 98.2
ProoD-Disc
-
83.9 87.5
87.2 76.9 84.9
84.2 76.7 85.3
84.7
96.6
98.4
98.4
ProoD ∆=3
94.99 48.0 99.9
99.7
9.1 100.0 100.0 0.0 100.0 98.7
0.0
100.0 100.0"
REFERENCES,0.8236994219653179,"In: CIFAR100
CIFAR10
SVHN
LSUN_CR
Smooth
Acc
FPR GFPR AFPR FPR GFPR AFPR FPR GFPR AFPR
FPR
GFPR AFPR"
REFERENCES,0.8265895953757225,"Plain
77.38 80.1 100.0 100.0 77.3 100.0 100.0 79.0 100.0 100.0 70.0 100.0 100.0
OE
77.25 81.8 100.0 100.0 37.7 100.0 100.0 0.0 100.0 99.7
0.0
100.0 100.0
ATOM
68.32 81.3 100.0 99.6 51.0 100.0 97.4 26.3 100.0 94.7
10.0 100.0 86.0
ACET
73.02 87.9 100.0 100.0 8.2 100.0 100.0 87.0 100.0 100.0
0.0
100.0 98.2
ProoD-Disc
-
95.9 97.5
97.5 91.3 92.8
92.7 91.7 95.3
95.0 100.0 100.0 100.0
ProoD ∆=5
77.16 82.2 100.0 100.0 37.6 100.0 100.0 0.0 100.0 99.3
3.4
100.0 100.0"
REFERENCES,0.8294797687861272,"In: R.ImgNet
Flowers
FGVC
Cars
Smooth
Acc
FPR GFPR AFPR FPR GFPR AFPR FPR GFPR AFPR
FPR
GFPR AFPR"
REFERENCES,0.8323699421965318,"Plain
96.34 55.2 100.0 100.0 48.2 100.0 100.0 75.2 100.0 100.0
0.0
100.0 100.0
OE
97.10 18.2 100.0 100.0 0.2 100.0 100.0 0.0 100.0 100.0
0.0
100.0 100.0
ProoD-Disc
-
59.2 65.2
65.0 51.0 67.8
66.5 51.7 63.7
62.3 100.0 100.0 100.0
ProoD ∆=4
97.25 18.5 100.0 100.0 0.5 100.0 100.0 0.0 100.0 100.0
0.0
100.0 100.0"
REFERENCES,0.8352601156069365,"G
ADDITIONAL DATASETS"
REFERENCES,0.838150289017341,"In addition to the results shown in Table 2, it is interesting to study how ProoD performs on additional
datasets as well as the test set of the out-distribution it was trained on. For the CIFAR datasets,
we report LSUN crops, LSUN_resize, Places365 (Zhou et al., 2017), iSUN (Xu et al., 2015),
Textures (Cimpoi et al., 2014), 80M Tiny Images and uniform noise in Table 7. On RImgNet we
report uniform noise and the training out-distribution in Table 8."
REFERENCES,0.8410404624277457,"As in Table 2 the clean performance of ProoD is comparable to that of OE, but it achieves non-trivial
GAUC. On CIFAR10, GOOD100 achieves almost perfect GAUC against uniform noise, which comes
at the price of signiﬁcantly worse clean AUCs on all other out-distributions, see Table 2. Almost all
methods achieve very high AAUCs on uniform noise, but it is not clear if a sufﬁciently powerful attack
could lower those scores signiﬁcantly. The unusually large gap between the GAUC and AAUC of
ProoD would seem to indicate that that might be the case. Surprisingly, the robustness characteristics
of both ATOM and ACET vary wildly between the different datasets, sometimes appearing to be
perfectly robust and on other datasets displaying no robustness at all. Especially surprising are the
relatively low AAUCs on the test set of the training out-distribution."
REFERENCES,0.8439306358381503,Under review as a conference paper at ICLR 2022
REFERENCES,0.846820809248555,"Table 7: Additional Datasets: We show the AUC, AAUC and GAUC for all models on uniform
noise and on the test set of the train out-distribution."
REFERENCES,0.8497109826589595,"In: CIFAR10
LSUN
LSUN_resize
Places365
iSUN
Acc
AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.8526011560693642,"Plain
95.01 95.9
0.0
1.3
95.0
0.0
8.7
89.5
0.0
0.3
94.5
0.0
10.4
OE
94.91 98.7
0.0
0.9
97.4
0.0
6.1
99.9
0.0
2.5
97.6
0.0
9.4
ATOM
93.63 77.3
0.0
12.1 100.0
0.0
98.4 82.6
0.0
22.0 100.0
0.0
98.8
ACET
93.43 89.2
0.0
2.5
100.0
0.0
91.3 88.0
0.0
3.4
100.0
0.0
92.1
GOOD80
87.39 96.5
68.4
89.7
87.4
61.7
69.5 96.8 58.8
90.2
87.1
58.9
70.3
GOOD100
86.96 95.0
86.0
86.7
81.1
67.6
67.9 74.4 59.7
60.9
77.5
63.3
65.2
ProoD-Disc
-
95.8
94.1
94.2
76.4
70.3
71.5 76.6 71.1
71.5
74.9
69.0
70.3
ProoD ∆=3
94.99 99.2
82.2
82.3
97.1
57.7
59.2 99.9 58.7
59.6
97.1
56.4
58.1"
REFERENCES,0.8554913294797688,"In: CIFAR10
Uniform
Textures
80M Tiny Images
OpenImages
Acc
AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.8583815028901735,"Plain
95.01 97.9
0.0
83.1
92.0
0.0
8.6
91.1
0.0
0.8
84.0
0.0
0.4
OE
94.91 99.9
0.0
98.2
99.9
0.0
13.8 93.6
0.0
0.6
99.7
0.0
2.5
ATOM
93.63 100.0
0.0
100.0 97.5
0.0
74.7 82.8
0.0
26.7
73.4
0.0
22.3
ACET
93.43 100.0
0.0
99.9
98.0
0.0
41.6 89.0
0.0
7.8
81.7
0.0
5.6
GOOD80
87.39 99.1
83.9
98.1
96.1
54.7
89.9 84.7 50.5
66.8
93.6
53.7
85.6
GOOD100
86.96 94.6
86.1
89.3
71.0
49.9
57.1 74.5 56.7
58.2
69.5
54.2
55.6
ProoD-Disc
-
53.8
46.7
46.7
75.3
69.9
70.5 75.7 69.8
70.9
64.8
59.0
59.6
ProoD ∆=3
94.99 99.9
35.0
94.7
99.9
57.6
63.1 93.8 57.5
59.0
99.8
47.7
49.7"
REFERENCES,0.861271676300578,"In: CIFAR100
LSUN
LSUN_resize
Places365
iSUN
Acc
AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.8641618497109826,"Plain
77.38 84.5
0.0
0.9
78.9
0.0
3.9
75.7
0.0
0.6
79.4
0.0
5.6
OE
77.25 94.5
0.0
1.3
88.2
0.0
2.1
99.7
0.0
1.9
88.3
0.0
3.1
ATOM
68.32 83.4
0.0
66.7
92.1
0.0
71.3 87.0
0.0
62.1
91.7
0.0
73.7
ACET
73.02 81.3
0.0
4.3
100.0
0.0
77.0 74.8
0.0
4.3
100.0
0.0
78.2
ProoD-Disc
-
81.4
78.8
79.3
70.1
66.7
67.1 71.7 68.2
68.5
69.3
65.6
66.0
ProoD ∆=5
77.16 94.8
28.5
28.7
86.3
22.6
22.8 99.7 23.6
23.8
87.2
22.1
22.4"
REFERENCES,0.8670520231213873,"In: CIFAR100
Uniform
Textures
80M Tiny Images
OpenImages
Acc
AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
REFERENCES,0.869942196531792,"Plain
77.38 82.7
0.0
53.0
77.5
0.0
5.3
79.7
0.0
1.2
75.5
0.0
0.7
OE
77.25 99.3
0.0
91.4
99.0
0.0
7.5
80.2
0.0
1.5
99.6
0.0
1.0
ATOM
68.32 100.0
0.0
100.0 90.0
0.0
66.6 90.1
0.0
70.0
83.7
0.0
58.9
ACET
73.02 100.0
0.0
99.9
91.5
0.0
21.0 77.0
0.0
6.8
74.5
0.0
3.0
ProoD-Disc
-
40.9
37.0
37.4
53.6
50.2
48.1 59.7 56.7
56.9
58.3
54.6
54.8
ProoD ∆=5
77.16 99.7
12.1
87.0
99.1
16.8
22.6 80.5 19.5
19.7
99.7
18.6
20.1"
REFERENCES,0.8728323699421965,"Table 8: Additional Datasets: For RImgNet, we show the AUC, AAUC and GAUC for all models
on uniform noise and on the test set of the train out-distribution, i.e. NotRImgNet."
REFERENCES,0.8757225433526011,"In: R.ImgNet
Uniform
NotR.ImgNet
Acc
AUC
GAUC
AAUC
AUC
GAUC
AAUC"
REFERENCES,0.8786127167630058,"Plain
96.34
99.3
0.0
74.9
91.7
0.0
0.2
OE
97.10
99.6
0.0
84.6
98.7
0.0
1.2
ProoD-Disc
99.7
99.2
99.3
73.6
69.9
69.9
ProoD ∆=4
97.25
99.8
79.7
95.2
98.6
50.1
51.3"
REFERENCES,0.8815028901734104,Under review as a conference paper at ICLR 2022
REFERENCES,0.884393063583815,"H
DERIVATION OF EQ. (3)"
REFERENCES,0.8872832369942196,We restate the derivation in Eq. (3) in a slightly more verbose form. For all y we have
REFERENCES,0.8901734104046243,"max
∥x′−x∥∞≤ϵ ˆp(y|x′) =
max
∥x′−x∥∞≤ϵ"
REFERENCES,0.8930635838150289,"
ˆp(y|x′, i)ˆp(i|x′) + 1"
REFERENCES,0.8959537572254336,"K (1 −ˆp(i|x′))

,
(13)"
REFERENCES,0.8988439306358381,"≤
max
∥x′−x∥∞≤ϵ"
REFERENCES,0.9017341040462428,"
ˆp(i|x′) + 1"
REFERENCES,0.9046242774566474,"K (1 −ˆp(i|x′))

,
(14)"
REFERENCES,0.9075144508670521,= K −1
REFERENCES,0.9104046242774566,"K
max
∥x′−x∥∞≤ϵ ˆp(i|x′) + 1"
REFERENCES,0.9132947976878613,"K .
(15)"
REFERENCES,0.9161849710982659,"The inequality is obtained by using the trivial bound ˆp(y|x, i) ≤1. This allows us to compute
guarantees on the conﬁdence of the entire model while treating the classiﬁer itself entirely as a
black-box. Because of this, there are no restrictions whatsoever on the architecture or training
procedure that are used for ﬁtting ˆp(y|x, i)."
REFERENCES,0.9190751445086706,"I
SIZE ABLATION FOR BINARY DISCRIMINATOR"
REFERENCES,0.9219653179190751,"Since larger models should typically lead to better performance, we investigated the impact that
model size has on the performance of our binary discriminator. We retrained ProoD-Disc models
with CIFAR10 as in-distribution and 80M Tiny images as the out-distribution (since our dataloader is
faster than for OpenImages which speeds up training). Since longer schedules only slightly improve
results we also used shorter schedules with only 300 epochs where ϵ and κ linearly increase from 0
to 0.01 and 1.0 respectively within the ﬁrst 100 epochs and the learning rate drops occur at 150, 200
and 250 epochs. As architectures we use 10 different CNNs with different widths and depths ranging
from 5 to 8 layers (for the precise architectures please refer to sizes {S, XL_b, XS, SR, SR2, C1, C3s,
C3, C2, C4} in the ﬁle provable_classifiers.py of the accompanying code). We present scatter
plots of the models’ their performance on CIFAR100 and 80M Tiny Images in Figure 5 against their
size. Clearly, there is no correlation between model size and performance and most differences are
rather small, justifying our choice of a fairly small architecture in the main paper."
REFERENCES,0.9248554913294798,"14
15
16
17
18
19
Log #Params 0.58 0.59 0.60 0.61 0.62 0.63 GAUC"
REFERENCES,0.9277456647398844,CIFAR100
REFERENCES,0.930635838150289,"14
15
16
17
18
19
Log #Params 0.69 0.70 0.71 0.72 0.73 0.74 GAUC"
M TINY IMAGES,0.9335260115606936,80M Tiny Images
M TINY IMAGES,0.9364161849710982,"Figure 5: Bigger models do not yield better guarantees: We show scatter plots of the GAUC of
different architectures against the log of the number of trainable parameters in the model. The orange
cross indicates the architecture that is used in the main paper. There is no clear dependence of
performance on model size, so it is preferable to use fairly small models."
M TINY IMAGES,0.9393063583815029,"J
GENERALIZATION TO LARGER THREAT MODEL"
M TINY IMAGES,0.9421965317919075,"Since ϵ = 0.01 is a relatively weak threat model we evaluate if ProoD’s guarantees actually generalize
to the much stronger ϵ =
8
255 ≈0.031 that is standard in much of the literature on adversarial
robustness. We use the exact same CIFAR models from Table 2 and show the results of our evaluation
at ϵ =
8
255 in Table 9."
M TINY IMAGES,0.9450867052023122,Under review as a conference paper at ICLR 2022
M TINY IMAGES,0.9479768786127167,"Table 9: Generalization to larger ϵ: We evaluate all CIFAR models in Table 2 using an ϵ =
8
255,
and thus an unseen threat model. The provable methods GOOD and ProoD generalize surprisingly
well, while neither ATOM nor ACET display any generalization to the larger threat model."
M TINY IMAGES,0.9508670520231214,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
M TINY IMAGES,0.953757225433526,"Plain
95.01 90.0
0.0
0.0
93.8
0.0
0.0
93.1
0.0
0.0
98.0
0.0
0.0
OE
94.91 91.1
0.0
0.1
97.3
0.0
0.0
100.0
0.0
0.1
99.9
0.0
0.0
ATOM
93.63 78.3
0.0
1.3
94.4
0.0
1.5
79.8
0.0
0.2
99.5
0.0
9.6
ACET
93.43 86.0
0.0
1.1
99.3
0.0
1.1
89.2
0.0
0.8
99.9
0.0
3.8
GOOD80*
87.39 76.7 37.5
51.6 90.8 38.6
74.3
97.4
57.6
90.2 96.2 61.1
87.8
GOOD100*
86.96 67.8 39.4
43.5 62.6 29.0
30.9
84.9
67.6
70.7 87.0 63.3
69.2
ProoD-Disc
-
62.9 44.1
46.1 72.6 52.5
57.1
78.1
56.3
58.9 59.2 34.9
37.2
ProoD ∆=3
95.15 84.8 39.2
41.0 98.3 46.9
50.8 100.0 50.2
52.7 99.9 30.4
30.6"
M TINY IMAGES,0.9566473988439307,"In: CIFAR100
CIFAR10
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
M TINY IMAGES,0.9595375722543352,"Plain
77.38 77.7
0.0
0.4
81.9
0.0
0.2
76.4
0.0
0.3
86.6
0.0
0.3
OE
77.25 77.4
0.0
0.2
92.3
0.0
0.0
100.0
0.0
0.7
99.5
0.0
0.5
ATOM
68.32 78.3
0.0
10.4 91.1
0.0
15.2
95.9
0.0
23.0 98.2
0.0
23.5
ACET
73.02 73.0
0.0
1.4
97.8
0.0
0.7
75.8
0.0
2.6
99.9
0.0
3.8
ProoD-Disc
-
56.1 41.1
43.1 61.0 50.5
51.8
70.4
57.5
58.8 29.6 20.9
20.8
ProoD ∆=5
76.51 76.6 13.7
14.1 91.5 16.9
16.9 100.0 18.1
18.2 98.9
8.1
8.1"
M TINY IMAGES,0.9624277456647399,"In: R.ImgNet
Flowers
FGVC
Cars
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
M TINY IMAGES,0.9653179190751445,"Plain
96.34 92.3
0.0
0.0
92.6
0.0
0.0
92.7
0.0
0.0
98.9
0.0
0.0
OE
97.10 96.9
0.0
0.2
99.7
0.0
0.0
99.9
0.0
0.0
98.0
0.0
0.0
ProoD-Disc
-
81.5 60.4
61.4 92.8 78.0
80.8
90.7
76.3
79.2 81.0 47.3
53.7
ProoD ∆=4
97.25 96.9 42.8
45.0 99.8 57.0
59.4
99.9
56.0
58.7 98.6 31.6
36.3"
M TINY IMAGES,0.9682080924855492,"*Uses different architecture of classiﬁer, see “Baselines” in Section 4.2."
M TINY IMAGES,0.9710982658959537,"Perhaps surprisingly, ProoD’s guarantees generalize remarkably well to the much larger radius on
CIFAR10. The same holds for GOOD, as was already observed in (Bitterwolf et al., 2020). On
the other hand ATOM and ACET do not show any robustness at this radius, despite having lower
accuracy, lower clean OOD performance and being more expensive to train. On the other hand
ProoD’s guarantees on CIFAR100 are quite weak at this radius, but given ATOM’s and ACET’s low
AAUCs here and the fact that GOOD cannot be trained at all on CIFAR100, the results are not worse
than for the competitors."
M TINY IMAGES,0.9739884393063584,"K
ERROR BARS"
M TINY IMAGES,0.976878612716763,"In order to be mindful of our resource consumption we restrict the computation of error bars to
our experiments on CIFAR10. Additionally, because the dataloader was much faster we ran these
experiments using 80M Tiny Images as an out-distribution as opposed to OpenImages. We reran
our experiments using the same hyperparameters 5 times. We computed the mean and the standard
deviations for our models for all metrics shown in Table 5. The results are shown in Table 10. We see
that the ﬂuctuations across different runs are indeed rather small. Furthermore, the clean performance
of OE and ProoD show no signiﬁcant discrepancies."
M TINY IMAGES,0.9797687861271677,Under review as a conference paper at ICLR 2022
M TINY IMAGES,0.9826589595375722,"Table 10: Error Bars: We show the mean and standard deviation σ of all metrics for our CIFAR10
models across 5 runs. The tolerances for ProoD’s clean performance are very small and yet the
differences in clean performance between OE ProoD are not signiﬁcant."
M TINY IMAGES,0.9855491329479769,"In: CIFAR10
CIFAR100
SVHN
LSUN_CR
Smooth
Acc AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC AUC GAUC AAUC"
M TINY IMAGES,0.9884393063583815,"Plain
94.91 90.0
0.0
0.6
93.9
0.0
0.1
93.4
0.0
0.7
96.7
0.0
1.2
Plain σ
0.16
0.1
0.0
0.1
1.2
0.0
0.0
0.3
0.0
0.2
2.1
0.0
0.5"
M TINY IMAGES,0.9913294797687862,"OE
95.56 96.1
0.0
7.6
99.4
0.0
0.4
99.6
0.0
16.7 99.6
0.0
4.3
OE σ
0.04
0.1
0.0
1.5
0.1
0.0
0.2
0.1
0.0
3.5
0.3
0.0
3.7"
M TINY IMAGES,0.9942196531791907,"ProoD-Disc
-
67.7 61.6
62.2 75.5 68.6
69.3 76.5 70.4
70.9 87.2 77.7
78.8
ProoD-Disc σ
-
0.7
0.7
0.7
1.4
1.7
1.5
1.4
1.7
1.7
3.6
4.3
4.3"
M TINY IMAGES,0.9971098265895953,"ProoD ∆=3
95.60 96.0 42.2
44.1 99.4 48.6
49.2 99.6 47.1
52.0 99.8 55.2
57.0
ProoD ∆=3 σ 0.11
0.1
0.8
0.8
0.1
0.6
0.6
0.1
1.5
1.9
0.1
2.9
3.4"
