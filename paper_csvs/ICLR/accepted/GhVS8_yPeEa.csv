Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003125,"Catastrophic forgetting presents a challenge in developing deep learning models
capable of continual learning, i.e. learning tasks sequentially. Recently, both
computer vision and natural-language processing have witnessed great progress
through the use of large-scale pretrained models. In this work, we present an
empirical study of catastrophic forgetting in this pretraining paradigm. Our exper-
iments indicate that large, pretrained ResNets and Transformers are signiﬁcantly
more resistant to forgetting than randomly-initialized, trained-from-scratch mod-
els; this robustness systematically improves with scale of both model and pre-
training dataset size. We take initial steps towards characterizing what aspect of
model representations allows them to perform continual learning so well, ﬁnding
that in the pretrained models, distinct class representations grow more orthogonal
with scale. Our results suggest that, when possible, scale and a diverse pretraining
dataset can be useful ingredients in mitigating catastrophic forgetting."
ABSTRACT,0.00625,"Continual learning is a current challenge in designing machine learning systems: when trained on
multiple tasks in sequence, models tend to suffer performance losses on earlier tasks—this is known
as catastrophic forgetting (McCloskey & Cohen, 1989). Catastrophic forgetting occurs in many
settings, from supervised learning for classiﬁcation (Zenke et al., 2017) to reinforcement learning
for games (Kirkpatrick et al., 2017) and downstream applications such as translation (Bapna & Firat,
2019) or medical diagnoses (Gupta et al., 2021). While many methods have been proposed to help
mitigate forgetting, none of these have fully solved the problem; see Kemker et al. (2017) for a
review."
ABSTRACT,0.009375,"Increasing model and dataset size have been key to the success of deep learning (Kolesnikov et al.,
2020; BIG-bench collaboration, 2021; Radford et al., 2019; Zhai et al., 2021). A recent trend has
been the use of increasingly large models pretrained on diverse datasets, which are subsequently
ﬁne-tuned or evaluated without ﬁne-tuning on downstream tasks of interest; see Bommasani et al.
(2021) and references therein. A particularly striking realization of this has been large pretrained
language models, which show consistent improvement across scales (Kaplan et al., 2020) and can
perform well on diverse tasks with no ﬁnetuning (Brown et al., 2020). From a continual learning
perspective, this behavior is especially dramatic as, without ﬁnetuning, any training signal for these
tasks appears mixed into a drastically larger pretraining dataset, and thus more than likely appeared
many gradient steps before evaluation. This observation in large part inspired our work."
ABSTRACT,0.0125,"Here, we empirically study continual learning in large pretrained models. Our results suggest that
pretrained models are signiﬁcantly more resistant to catastrophic forgetting than their randomly-
initialized counterparts, and that this resistance improves with the scale of both the model and pre-
training data. Most of our experiments concern vision—speciﬁcally image classiﬁcation; but we
also conduct a preliminary investigation of language models."
ABSTRACT,0.015625,Our key observations are as follows:
ABSTRACT,0.01875,"• Resistance to catastrophic forgetting improves with model and pretraining dataset scale
(Figure 1)."
ABSTRACT,0.021875,"• Large, pretrained image-classiﬁcation models are signiﬁcantly more resistant to forgetting
than models trained from random initialization (Figure 6)."
ABSTRACT,0.025,Published as a conference paper at ICLR 2022
ABSTRACT,0.028125,"• Both supervised and unsupervised, SimCLR (Chen et al., 2020), pretraining improve for-
getting (Supplement, Figure 12)."
ABSTRACT,0.03125,"• We perform initial experiments aimed at exploring why scale and pretraining reduce catas-
trophic forgetting, showing that distinct class representations in pretrained models are more
orthogonal than in trained-from-scratch models (Figure 9)."
ABSTRACT,0.034375,"• We observe similar phenomena in language models (Supplement, Figure 11)."
RELATED WORK,0.0375,"1
RELATED WORK"
RELATED WORK,0.040625,"Catastrophic forgetting Catastrophic forgetting was identiﬁed as a problem for neural networks as
early as the late 1980s, as it occurred in the single-hidden-layer networks that were studied at the
time (see Delange et al. (2021) for a review). Within the catastrophic forgetting literature, a number
of potential solutions to the problem have been devised; some involve replaying previous-task exam-
ples during training on later tasks (Rebufﬁet al., 2016; Lopez-Paz & Ranzato, 2017; Chaudhry et al.,
2018; Shin et al., 2017; Riemer et al., 2018), while others like elastic weight consolidation (Kirk-
patrick et al., 2017) limit the extent to which parameters can change once learned (Zenke et al.,
2017; Lee et al., 2017; Farajtabar et al., 2019)."
RELATED WORK,0.04375,"Perhaps most similar to our work are proposed mitigation methods which exploit the overparame-
terization of modern neural networks to perform continual learning of multiple tasks, e.g. by using
pruning in Mallya & Lazebnik (2018) or task-speciﬁc subnetworks in Wortsman et al. (2020) and
recent works studying the impact of pretraining Anonymous (2022) and overparameterization via
network width Mirzadeh et al. (2021) on forgetting."
RELATED WORK,0.046875,"Large-scale models The observation—initially in language models (Kaplan et al., 2020)—that
model performance scales predictably with model and dataset size spurred an increased interest
in training and deploying increasingly large pretrained models. Large-scale models emerged in
natural-language processing with models such as GPT (Radford et al., 2019) and BERT (Devlin
et al., 2019), which are pretrained in an unsupervised manner and thus able to make use of the
large corpora of natural text available on the internet. More recently, image-classiﬁcation models
have also beneﬁted from pretraining on large datasets (Kolesnikov et al., 2020), both supervised and
unsupervised; a key example is the Vision Transformer architecture of Dosovitskiy et al. (2020),
which—despite not using the inductive bias provided by convolutions—attains downstream perfor-
mance comparable to convolutional networks after pretraining on ImageNet-21k (Deng et al., 2009)
or JFT-300M (Sun et al., 2017). On many current NLP and computer vision benchmarks, large
pretrained models achieve top performance."
RELATED WORK,0.05,"All of the experiments in our work follow the pretrain-then-ﬁnetune method; recently, another
paradigm, side-tuning, was proposed by Zhang et al. (2020), which avoids forgetting altogether
by leaving the pretrained model ﬁxed and employing auxiliary networks for downstream tasks."
EXPERIMENTAL SETUP,0.053125,"2
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.05625,"Tasks: Most of the experiments in this paper are conducted in the standard, task split setting (Kirk-
patrick et al., 2017; Zenke et al., 2017). We consider split CIFAR-10 task sequences – the ten class
dataset is split and sequentially trained on two tasks of 5 classes each. We also consider sequen-
tial 10 and 50 class splits of CIFAR-100. Beyond split CIFAR, we study the following datasets:
Oxford-IIIT pet, Oxford Flowers 102, Street View House Numbers (SVHN), Caltech-UCSD Birds
200 (CUB-200), Cars196, Domainnet/Clipart."
EXPERIMENTAL SETUP,0.059375,"We also consider the more recent input-distribution-shift CIFAR-100 setup (Ramasesh et al., 2021).
In this case each task has the same target types made up a ﬁxed subset of the 20 CIFAR-100 su-
perclasses (e.g. aquatic mammals, ﬁsh, ...). The sequence of tasks consists of modifying which
subclasses are sampled from. For example, Task A may have aquatic mammals drawn from beaver
and dolphin, while Task B may have otter and whale. This setup allows one to probe forgetting
without changing the model head, in a setting where task identity is not known to the model."
EXPERIMENTAL SETUP,0.0625,"For our language model experiments, we consider next token prediction generative tasks and use the
IMDb Reviews (Maas et al., 2011) and english Wikipedia (Foundation) datasets."
EXPERIMENTAL SETUP,0.065625,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.06875,"Vision models: In most of our experiments we use the Vision Transformer (ViT) and ResNet archi-
tectures. For the Vision Transformers, we use the ViT-S and ViT-B conﬁgurations from Dosovitskiy
et al. (2020) and a smaller conﬁguration we call ViT-xS – ranging from 5.7-86.7 million parameters.
For ResNets, we use ﬁve models: ResNet26, ResNet50, ResNet101, ResNet152, and ResNet200 –
ranging from 14.0-62.6 million parameters; as in Kolesnikov et al. (2020), these models use group
normalization (Wu & He, 2018) and weight standarization (Qiao et al., 2019) in place of batch
normalization. Details can be found in Tables 1 and 2 in Appendix A.1.2."
EXPERIMENTAL SETUP,0.071875,"Pretraining: Unless otherwise speciﬁed, we pretrain our image models in a supervised manner on
the ImageNet21k dataset, which contains approximately 14 million images in about 26000 distinct
categories using the Adam optimizer (Kingma & Ba, 2017). Further details are in the Appendix."
EXPERIMENTAL SETUP,0.075,"Finetuning: When ﬁne-tuning on multiple tasks in sequence, we use the SGD optimizer with mo-
mentum β = 0.9. The ﬁrst task training employs a cosine decay schedule with a linear warmup; later
tasks in the sequence are trained with ﬁxed learning rates. For the split-CIFAR-10 task sequences,
we use a multi-head setup in which each head is initialized to zeros."
RESULTS,0.078125,"3
RESULTS"
FORGETTING IMPROVES WITH MODEL SCALE,0.08125,"3.1
FORGETTING IMPROVES WITH MODEL SCALE"
FORGETTING IMPROVES WITH MODEL SCALE,0.084375,"Figure 1: Forgetting frontier across model scales. Task A versus Task B performance for both pretrained
vision transformers (left) and ResNets (center). Each point represents a different choice of learning rate or
ﬁnetuning step for Task B. All models were pretrained on ImageNet21k for 90 epochs. (right) the best average
Task A/B error improves systematically with model size."
FORGETTING IMPROVES WITH MODEL SCALE,0.0875,"Our main ﬁnding is that for both pretrained Vision Transformers and pretrained ResNets, catas-
trophic forgetting is mitigated to a large extent by scale: that is, larger models suffer less from
forgetting. This is shown in Figure 1 for the two-task split CIFAR-10 sequence: Task A {airplane,
automobile, bird, cat, deer} →Task B {dog, frog, horse, ship, truck}. In order to make a fair compar-
ison, we plot the test accuracies of the model on both tasks at different steps of Task B ﬁne-tuning
for a variety of learning rates. This allows us to see, for each model, the maximum performance
achievable on the new task for a given performance on the initial task. We call this the forgetting
frontier."
FORGETTING IMPROVES WITH MODEL SCALE,0.090625,"Figure 1 shows that the forgetting frontier for both ViT and ResNet models improves signiﬁcantly
with scale. While a 5.7M-parameter ViT-xS model loses about 6.5% in Task A accuracy over the
course of learning to perform Task B, an 86.7M-parameter ViT-B model loses less than half a per-
cent. To put this in context, in a previous studies of split-CIFAR-10 task sequences, a ResNet18
model lost close to 40% accuracy with no mitigation methods, 5% when using a replay buffer, and
20% when using elastic weight consolidation (see e.g. Figure 1 of Ramasesh et al. (2021))."
FORGETTING IMPROVES WITH MODEL SCALE,0.09375,"It is also important to note that this effect of scale and pretraining on improving forgetting frontiers
does not seem to be architecture-dependent. Both pretrained vision transformers—based on self-"
FORGETTING IMPROVES WITH MODEL SCALE,0.096875,Published as a conference paper at ICLR 2022
FORGETTING IMPROVES WITH MODEL SCALE,0.1,"Pets
Flowers
SVHN
Birds
Clipart
Cars
ViTs
-0.70
-0.99
-0.31
-0.40
-0.38
-0.40
ResNets
-0.89
-1.53
-1.06
-0.62
-0.62
-0.47"
FORGETTING IMPROVES WITH MODEL SCALE,0.103125,"Figure 5: Scaling behavior on non-CIFAR, two-task sequences. (top) While the rate of improvement varies
across datasets, continual-learning performance in these two-task continual learning setups systematically im-
proves with model size, consistent with a power-law. For a full discussion and plot of all forgetting frontiers,
see Figure 2. (bottom) Table of power-law ﬁt exponents."
FORGETTING IMPROVES WITH MODEL SCALE,0.10625,"attention—and ResNets—based on convolutions—exhibit improvements in their forgetting frontiers
as they grow larger."
FORGETTING IMPROVES WITH MODEL SCALE,0.109375,"We see this improvement in forgetting performance beyond the simple two-task split-CIFAR-10
scenario. Across a 10-task sequence of 10-class split-CIFAR-100 tasks (Figure 3 left), sequential
50-class splits of CIFAR-100 (Figure 3 right), the CIFAR-100 single-head input-distribution-shift
task (Figure 4), and two-task sequences beyond the CIFAR dataset (Figure 2), large-scale pretrained
models exhibit resistance to forgetting, and continual learning ability appears to improve systemat-
ically with scale. For detailed results, scaling plots, and forgetting frontiers in the 10-task scenario,
see Section B.3 in the Supplement."
DEPENDENCE ON FINETUNING DATASET,0.1125,"3.2
DEPENDENCE ON FINETUNING DATASET"
DEPENDENCE ON FINETUNING DATASET,0.115625,"It is interesting to study how the improved resistance to forgetting with scale depends on the choice
of ﬁnetuning dataset. In particular, it is worth noting that, though quite different in format, CIFAR-
10/100 are semantically quite similar to subsets of ImageNet21k, our pretraining set. To empirically
investigate the dependence on ﬁnetuning dataset we have run additional experiments similar to those
presented in Figure 1 for the Oxford Pet and Flowers, Caltech Birds, Stanford Cars, street view house
numbers (SVHN) and DomainNet Clipart datasets. Results are shown in Figure 5 and in Figure 2.
We note in particular that SVHN classes are numerals and so distinct from ImageNet21k classes,
while the Clipart dataset consists of synthetic, rather than natural images. See the Supplement for
further details."
DEPENDENCE ON FINETUNING DATASET,0.11875,"3.3
FORGETTING IN PRETRAINED MODELS VS. MODELS TRAINED FROM SCRATCH"
DEPENDENCE ON FINETUNING DATASET,0.121875,"In the previous section, we saw that the amount of catastrophic forgetting in models pretrained on
large datasets decreased with increasing model size, to the point that the largest models (ViT-B,
with 86.7M parameters and ResNet200 with 62.6M parameters) suffer very little performance loss
from second-task training. In this section, we investigate whether this improvement in forgetting is
simply a consequence of the improvement in performance on Task A. That is, do pretraining and
scale have beneﬁts with respect to forgetting beyond simply improving the ﬁne-tuned performance
on downstream tasks?"
DEPENDENCE ON FINETUNING DATASET,0.125,"To check this, we compare the performance and amount of forgetting in pretrained ResNet models to
models trained from scratch. We handicap the pretrained ResNets so that their maximum accuracy
on on Task A matches that of the corresponding trained-from-scratch ResNet; this is done simply by"
DEPENDENCE ON FINETUNING DATASET,0.128125,Published as a conference paper at ICLR 2022
DEPENDENCE ON FINETUNING DATASET,0.13125,"Figure 2: Forgetting frontiers for two-task sequences beyond CIFAR10 and CIFAR100. Fine-
tuning settings for these experiments are shown in table 4 (supplement), while the datasets them-
selves are descrbed in section B.4 (supplement). As with the CIFAR10 and CIFAR100 datasets, we
ﬁnd that model scale is beneﬁcial from the perspective of continual learning, though especially for
smaller ResNets the behavior is less clean than on CIFAR. Scaling exponents also vary with dataset."
DEPENDENCE ON FINETUNING DATASET,0.134375,Published as a conference paper at ICLR 2022
DEPENDENCE ON FINETUNING DATASET,0.1375,"Figure 3: Split CIFAR-100 tasks. (left) Sequential training of a pretrained ViT-B model on a 10-task se-
quence, where each task consists of classifying between 10 categories in the CIFAR-100 dataset. In this contin-
ual learning setup, the model is able to learn all 10 tasks without much forgetting: on average, the model loses
1.8% accuracy on each task; at most, the model loses 2.9%. (right) Forgetting frontiers for vision transformers
on a 2-task, 50-class split CIFAR-100, showing the same increase with scale as on the split-CIFAR-10 version."
DEPENDENCE ON FINETUNING DATASET,0.140625,"Figure 4: Single-head models on CIFAR-100 distribution shift task. As with the multi-head setup, the
forgetting in Vision Transformers (left) and ResNets (right) is largely mitigated by pretraining and scale. The
full task details are given in Appendix A.1.3"
DEPENDENCE ON FINETUNING DATASET,0.14375,"reducing the number of steps for which we ﬁne-tune. Figure 6 shows the results of this experiment,
with additional results and details in Appendix C.1. While the trained-from-scratch ResNets achieve
the same performance as their pretrained counterparts on Task A, their Task A performance degrades
signiﬁcantly more than the pretrained ResNets when trained on Task B; pretrained ResNets forget
less, and this improves with model scale."
DEPENDENCE ON FINETUNING DATASET,0.146875,"This result suggests that pretraining and model scale confer beneﬁts upon models which go beyond
simply improving performance. From Figure 6 it is also clear (see also Figure 19 in the appendix,
which plots the same data in a different way) that pretraining is required to see the beneﬁts of scale.
Simply scaling up the model size in trained-from-scratch ResNets does not yield clear improvements
in their forgetting performance."
DEPENDENCE ON FINETUNING DATASET,0.15,"We focused in this section exclusively on ResNets (rather than Vision Transformers) because ob-
taining decent performance on image classiﬁcation tasks with Vision Transformers seems to require
pretraining. In our experiments, for example, a trained-from-scratch ViT-S model was unable to
achieve better than 75% test accuracy on ﬁve-class CIFAR-10, while the same model, pretrained,
achieved better than 98% accuracy."
DEPENDENCE ON FINETUNING DATASET,0.153125,Published as a conference paper at ICLR 2022
DEPENDENCE ON FINETUNING DATASET,0.15625,"Figure 6: Pretrained versus trained-from-scratch models. ResNet models trained from scratch (gray)
exhibit inferior forgetting performance than pretrained models (colored), even for the pretrained models shown
here, which were handicapped during ﬁne-tuning, in order to match Task A performance. Futhermore, the
trained-from-scratch models are unable to take advantage of model scale – the forgetting frontier is largely
independent of model size (see also Figure 19 in the Appendices). Dashed vertical lines show the maximum
accuracy on Task A."
DEPENDENCE ON FINETUNING DATASET,0.159375,"3.4
DEPENDENCE ON PRETRAINING TIME, PRETRAINING DATASET SIZE, AND FINETUNING
TIME"
DEPENDENCE ON FINETUNING DATASET,0.1625,"Having seen in the previous sections that pretraining on large datasets improves models’ resilience
to catastrophic forgetting, in this section we explore the dependence of this phenomenon on the
duration of pretraining, the size of the pretraining dataset, and the number of steps for which we
ﬁnetune on Task A."
DEPENDENCE ON FINETUNING DATASET,0.165625,"First, we study the inﬂuence of pretraining time on downstream forgetting. This is motivated by
previous studies (e.g. Hernandez et al. (2021)) showing that downstream performance does not
always monotonically improve with pretraining time and performance. By contrast, in the case
of forgetting we ﬁnd (see Figure 7 for ViT-S results) that as the pretraining process continues, both
downstream performance and the forgetting frontier improve. This was the case for all models (ViTs
and ResNets) and downstream task sequences (CIFAR-10, CIFAR-100, etc.) we studied. We found
no evidence of forgetting frontiers getting worse with increased pretraining."
DEPENDENCE ON FINETUNING DATASET,0.16875,"Figure 7: Varying pretraining time. We take pretrained checkpoints at 50k, 100k, 150k, 200k, and 250k
steps (shown in colored ticks at the left) for a ViT-S model pretrained on ImageNet21k. (right) The forgetting
frontiers improve with increased pretraining time."
DEPENDENCE ON FINETUNING DATASET,0.171875,"Along similar lines, we show the inﬂuence of pretraining data scale on downstream forgetting in
Figure 8, pretraining a model on varying fractions of ImageNet21k. While the full dataset contains
over 14 million images, we ﬁnd that training on even one-sixteenth of the dataset (roughly the
size of the standard ImageNet ILSVRC 2012 dataset) yields a model which only loses about 3%
performance in forgetting on split-CIFAR-10. This suggests that one does not need to pretrain
models on unreasonably large datasets in order to reap beneﬁts for continual learning."
DEPENDENCE ON FINETUNING DATASET,0.175,Published as a conference paper at ICLR 2022
DEPENDENCE ON FINETUNING DATASET,0.178125,"Finally, we study the forgetting frontier of pretrained models as we vary the amount of ﬁrst-task
ﬁne-tuning (number of training steps). This is partly inspired by a recent study (Andreassen et al.,
2021) of robustness in pretrained models, which found that during ﬁne-tuning, pretrained models
evolved some robustness to distribution shift which vanished at the end of ﬁne-tuning. Figure 8
shows forgetting frontiers for pretrained ViT-B and ResNet101 models, ﬁne-tuned for a number of
steps between 100 and 5000. The results in the ﬁgure suggest that in our setting, increasing the
ﬁne-tuning steps to maximize performance on Task A does not appear to cause increased forgetting.
On the contrary, even training for about ten times more steps than it takes to achieve saturating
performance does not impact forgetting."
DEPENDENCE ON FINETUNING DATASET,0.18125,"Figure 8: Varying dataset size and ﬁnetuning time. (left) The forgetting frontier for a vision transformer
(ViT-S16) ﬁnetuned on a two-task CIFAR-10 sequence, pretrained on varying fractions of the ImageNet21k
dataset shows improvement with pretraining dataset size. (center, right): The forgetting frontiers for models
pretrained on ImageNet21k, with varying amounts of ﬁne-tuning – Task A performance saturates around 500
steps; even training for ten times as many steps does not affect the forgetting frontier."
REPRESENTATION OVERLAP AND FORGETTING,0.184375,"3.5
REPRESENTATION OVERLAP AND FORGETTING"
REPRESENTATION OVERLAP AND FORGETTING,0.1875,"In this section we take steps toward understanding why large pretrained models seem to be resistant
to forgetting. Following the work of Ramasesh et al. (2021), we measure the similarity between a
models representation of Task A and Task B data."
REPRESENTATION OVERLAP AND FORGETTING,0.190625,"In detail, if we denote the P model features (penultimate layer activations) on an input, x, by f(x) ∈
RP , Ramasesh et al. (2021) propose measuring the similarity of fA := {f(x) : x ∈Task A} ∈
R|Task A|×P and fB := {f(x) : x ∈Task B} ∈R|Task B|×P using the trace overlap,"
REPRESENTATION OVERLAP AND FORGETTING,0.19375,"SAB =
Tr(ΘABΘT
AB)
q 
Tr(ΘAAΘT
AA)Tr(ΘBBΘT
BB)
 .
(1)"
REPRESENTATION OVERLAP AND FORGETTING,0.196875,"Here ΘAB = fAf T
B is the matrix of inner-products between Task A and Task B features."
REPRESENTATION OVERLAP AND FORGETTING,0.2,"Ramasesh et al. (2021) show empirically that forgetting is maximal for intermediate values of SAB,
and derive this property analytically in a simpliﬁed model. In particular Ramasesh et al. (2021)
propose that a minimal trace overlap (corresponding to orthogonality of representations of differ-
ent tasks) will lead to minimal forgetting. Inspired by this, we measure representation overlaps in
models ﬁnetuned on split CIFAR-10, taking representations at the end of Task A training."
REPRESENTATION OVERLAP AND FORGETTING,0.203125,"We consider the 10x10 matrix, S, formed by taking the indices A and B to run over the different
classes in CIFAR-10. The numerator in the above expression can be thought of as an empirical
(batch) estimate of the inner product between feature vectors for random draws from classes A and
B; trace overlaps close to zero indicate that representations are nearly orthogonal."
REPRESENTATION OVERLAP AND FORGETTING,0.20625,"Using the above deﬁnition, we return to the original split-CIFAR-10 setting of §3.1 and §3.3. For
each of the models used in those experiments (see Tables 1 and 2), we compute the representation
overlap matrix. We focus on two comparisons: (i) how does the representation overlap matrix
differ between pretrained and trained-from-scratch ResNets? and (ii) how does the orthogonality of
representations vary as we scale the model size, for both ResNets and Vision Transformers?"
REPRESENTATION OVERLAP AND FORGETTING,0.209375,Published as a conference paper at ICLR 2022
REPRESENTATION OVERLAP AND FORGETTING,0.2125,"Figure 9: Trace overlap of CIFAR-10 class representations. (left) The cross-class representation overlap
matrices (Equation 1) are signiﬁcantly lower for a pretrained ResNet101 than for one trained from scratch.
(right) For both Vision Transformers and ResNets, the cross-class overlap decreases as the model size increases.
These representation overlaps are computed after training the model on the initial CIFAR-10 subtask. See
Appendices for additional models and measurements."
REPRESENTATION OVERLAP AND FORGETTING,0.215625,"Pretrained vs. trained-from-scratch representations: We show representative trace overlap ma-
trices for both pretrained and trained-from-scratch ResNet101 models in Figure 9(left and center)
(see Figure 27 for additional models). Each element of this matrix corresponds to the represen-
tation overlap between two batches, each drawn entirely from a single class. The representation
overlap between disjoint classes (off-diagonal class overlap matrix elements) is much smaller for
the pretrained ResNets than those trained from scratch. Averaging cross-class overlaps over class
pairs from different tasks quantiﬁes this difference: for trained-from-scratch ResNet101, the average
overlap is 0.907, while for the pretrained model this value is only 0.466. For all sizes of ResNet, the
pretrained models exhibit representations which have much smaller overlap than their trained-from-
scratch counterparts."
REPRESENTATION OVERLAP AND FORGETTING,0.21875,"Representation overlap across scales—Figure 9(right) shows the average cross-task overlap for
both ResNets and Vision Transformers plotted against model size (number of parameters). For pre-
trained Vision Transformers and ResNets, this overlap decreases with model size, while for trained-
from-scratch ResNets, there is not a clear decreasing trend. Interestingly, Vision Transformers ap-
pear to form more orthogonal representations than ResNets do, a fact which can also be seen in
the overlap matrices presented in supplementary Figure 26, and perhaps explains their improved
forgetting performance."
REPRESENTATION OVERLAP AND FORGETTING,0.221875,"These observations suggest that pretrained models store representations of different classes with
much less overlap than trained-from-scratch models, and that this orthogonality increases with scale.
As this mirrors the the observed trend in forgetting in these models, it is suggestive that the increased
orthogonality of representations is a partial explanation for the forgetting resilience seen in large
pretrained models."
DISCUSSION,0.225,"4
DISCUSSION"
DISCUSSION,0.228125,"The central observation we have reported in this paper is that scaling up models pretrained on large
datasets is effective at mitigating catastrophic forgetting in the settings studied. Our experiments
show that pretraining and scale can confer beneﬁts beyond simply the achievable downstream loss
or accuracy values."
DISCUSSION,0.23125,"An important direction for future work is understanding what underlies the systematic improvement
of continual learning with model scale. The rate at which performance improves with model size in
Figure 1, with power-law exponent close to −1, is reminiscent of the variance limited behavior dis-
cussed in Bahri et al. (2021). Namely, when the number of model parameters N is much larger than
the number of training examples (as is the case for split-CIFAR ﬁnetuning), performance is predicted
to scale as N −1. This scaling behavior is notably not seen in Figure 11 for language modeling—
this is reminiscent of the disparate scaling behaviors observed in Sharma & Kaplan (2020) between
language-modeling and image-classiﬁcation tasks. Ideally, developing an understanding of this be-
havior would allow for predicting the rate of improvement to forgetting with model size."
DISCUSSION,0.234375,Published as a conference paper at ICLR 2022
DISCUSSION,0.2375,ACKNOWLEDGMENTS
DISCUSSION,0.240625,"The authors would like to thank Anselm Levskaya, Bill Mark, Anders Andreassen, and Maithra
Raghu for their conversations during the completion of this work."
REFERENCES,0.24375,REFERENCES
REFERENCES,0.246875,"Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of
out-of-distribution robustness throughout ﬁne-tuning, 2021."
REFERENCES,0.25,"Anonymous. An empirical investigation of the role of pre-training in lifelong learning. In Submitted
to The Tenth International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=D9E8MKsfhw. under review."
REFERENCES,0.253125,"Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scaling laws, 2021."
REFERENCES,0.25625,"Ankur Bapna and Orhan Firat. Non-parametric adaptation for neural machine translation. In Pro-
ceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
1921–1931, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1191. URL https://aclanthology.org/N19-1191."
REFERENCES,0.259375,"BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabil-
ities of language models. In preparation, 2021. URL https://github.com/google/
BIG-bench/."
REFERENCES,0.2625,"Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.265625,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot
learners.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
REFERENCES,0.26875,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers, 2021."
REFERENCES,0.271875,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with a-gem. ArXiv, abs/1812.00420, 2018."
REFERENCES,0.275,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.278125,"Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021. ISSN
1939-3539.
doi: 10.1109/tpami.2021.3057446.
URL http://dx.doi.org/10.1109/
TPAMI.2021.3057446."
REFERENCES,0.28125,"Jia Deng, R. Socher, Li Fei-Fei, Wei Dong, Kai Li, and Li-Jia Li. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recog-
nition(CVPR), volume 00, pp. 248–255, 06 2009. doi: 10.1109/CVPR.2009.5206848. URL
https://ieeexplore.ieee.org/abstract/document/5206848/."
REFERENCES,0.284375,Published as a conference paper at ICLR 2022
REFERENCES,0.2875,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019."
REFERENCES,0.290625,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.29375,"Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for contin-
ual learning, 2019."
REFERENCES,0.296875,Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.
REFERENCES,0.3,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, Koray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own
latent: A new approach to self-supervised learning, 2020."
REFERENCES,0.303125,"Sharut Gupta, Praveer Singh, Ken Chang, Liangqiong Qu, Mehak Aggarwal, Nishanth Thum-
bavanam Arun, Ashwin Vaswani, Shruti Raghavan, Vibha Agarwal, Mishka Gidwani, Katha-
rina Hoebel, Jay B. Patel, Charles Lu, Christopher P. Bridge, Daniel L. Rubin, and Jayashree
Kalpathy-Cramer.
Addressing catastrophic forgetting for medical domain expansion.
CoRR,
abs/2103.13511, 2021. URL https://arxiv.org/abs/2103.13511."
REFERENCES,0.30625,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning, 2020."
REFERENCES,0.309375,"Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer,
2021."
REFERENCES,0.3125,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models, 2020."
REFERENCES,0.315625,"Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring
catastrophic forgetting in neural networks, 2017."
REFERENCES,0.31875,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.321875,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forget-
ting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526,
2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/
content/114/13/3521."
REFERENCES,0.325,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
V 16, pp. 491–507. Springer, 2020."
REFERENCES,0.328125,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.33125,"Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In Neural information processing sys-
tems, pp. 4652–4662, 2017."
REFERENCES,0.334375,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, pp. 6467–6476, 2017."
REFERENCES,0.3375,Published as a conference paper at ICLR 2022
REFERENCES,0.340625,"Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015."
REFERENCES,0.34375,"Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning, 2018."
REFERENCES,0.346875,"Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. 1989."
REFERENCES,0.35,"Seyed Iman Mirzadeh, Arslan Chaudhry, Huiyi Hu, Razvan Pascanu, Dilan Gorur, and Mehrdad
Farajtabar. Wide neural networks forget less catastrophically, 2021."
REFERENCES,0.353125,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011."
REFERENCES,0.35625,"M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes.
In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,
Dec 2008."
REFERENCES,0.359375,"O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on
Computer Vision and Pattern Recognition, 2012."
REFERENCES,0.3625,"Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1406–1415, 2019."
REFERENCES,0.365625,"Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan L. Yuille. Weight standardization.
CoRR, abs/1903.10520, 2019. URL http://arxiv.org/abs/1903.10520."
REFERENCES,0.36875,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.371875,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:
//jmlr.org/papers/v21/20-074.html."
REFERENCES,0.375,"Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting:
Hidden representations and task semantics. In International Conference on Learning Represen-
tations, 2021. URL https://openreview.net/forum?id=LhY8QdUGSuw."
REFERENCES,0.378125,"Sylvestre-Alvise Rebufﬁ, Alexander I Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl:
Incremental classiﬁer and representation learning. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5533–5542, 2016."
REFERENCES,0.38125,"Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:1810.11910, 2018."
REFERENCES,0.384375,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge, 2015."
REFERENCES,0.3875,"Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
CoRR, abs/2004.10802, 2020. URL https://arxiv.org/abs/2004.10802."
REFERENCES,0.390625,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017."
REFERENCES,0.39375,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-
fectiveness of data in deep learning era. In ICCV, 2017."
REFERENCES,0.396875,Published as a conference paper at ICLR 2022
REFERENCES,0.4,"P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010."
REFERENCES,0.403125,"Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari,
Jason Yosinski, and Ali Farhadi. Supermasks in superposition, 2020."
REFERENCES,0.40625,"Yuxin Wu and Kaiming He. Group normalization. CoRR, abs/1803.08494, 2018. URL http:
//arxiv.org/abs/1803.08494."
REFERENCES,0.409375,"Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3987–
3995. JMLR, 2017."
REFERENCES,0.4125,"Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers,
2021."
REFERENCES,0.415625,"Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: A
baseline for network adaptation via additive side networks, 2020."
REFERENCES,0.41875,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.421875,"A.1
VISION"
REFERENCES,0.425,"A.1.1
SUPERVISED PRETRAINING"
REFERENCES,0.428125,"Here we detail the settings used to in supervised pretraining, resulting in the models used in the
experiments of sections 3.1, 3.3, 3.4, and B.2. We pretrain seven models: (ViT-B, ViT-S, ViT-xS,
R26x1, R50x1, R101x1, R152x1, R200x1) on ImageNet21k for 90 epochs. Pretraining is done
using the Adam optimizer (β1 = 0.9 and β2 = 0.999); for all models, we use a batch size of 4096.
Our learning-rate schedule includes a warmup of 10k steps to a maximum learning rate η = 10−3,
followed by a linear decay to 10−5. For the Vision Transformer models, we use a weight decay
penalty of 0.03 and a dropout rate of 0.1; for ResNet models, we do not use weight decay or dropout.
These settings are largely identical to those found in Dosovitskiy et al. (2020)."
REFERENCES,0.43125,"A note on ImageNet21k: We use the version of ImageNet21k available in TensorFlow Datasets 4.0.1
and apply minimal preprocessing, simply converting the labels to one-hot vectors and cropping the
image to be 224x224 pixels in size. During training, this crop is an Inception-style random crop,
while during evaluation it is a deterministic central crop; during training we additionally augment
the data by performing random horizontal ﬂips and apply label smoothing, with values 0.9999 and
0.0001. Importantly, ImageNet21k is class-imbalanced and heavy-tailed, with the uneven distribu-
tion of label occurrences shown in Figure 10. We do not balance the dataset."
REFERENCES,0.434375,"A.1.2
VISION TRANSFORMER MODEL DETAILS"
REFERENCES,0.4375,"For full description of the Vision Transformer architecture, see the original paper by Dosovitskiy
et al. (2020). Abbreviated details are given here. We process 2D images of height H, width W, and
C channels, x ∈RH×W ×C by ﬁrst reshaping them into a 1D sequence of ﬂattened patches–in this
work, we exclusively use patch size 16. Each patch is mapped (via a learned linear transformation)
to an embedding vector of dimension D (for our models, DxS = 256, DS = 516, and DB = 768, as
in Table 1). As both our pretraining and ﬁnetuning is supervised, we prepend a (learnable) ‘[class]‘
token embedding to this sequence of patches, add a learnable 1D position embedding, and pass
the sequence through the Transformer encoder. The Transformer encoder alternates between multi-
headed self-attention layers and MLP (plus residual connection) layers, with layernorm applied
before both the attention and MLP layers."
REFERENCES,0.440625,"For classiﬁcation, we use a classiﬁcation head which acts on only the transformed ‘[class]‘ token
representation. During pretraining, this classiﬁcation head is an single-hidden-layer MLP, as in the
original Vision Transformer work; during ﬁnetuning, the classiﬁcation head is a linear layer (in the
split-CIFAR-10 setting, we use a multi-head setup with a different classiﬁcation layer for each task
in the sequence). We initialize all classiﬁcation layers with zeros."
REFERENCES,0.44375,Published as a conference paper at ICLR 2022
REFERENCES,0.446875,"Figure 10: ImageNet21k is highly class-imbalanced. While roughly 7500 classes of the ImageNet21k
dataset feature over 1000 examples, there are over 3000 classes with less than 100 examples. As mentioned in
the main text, in this work we do not class-balance ImageNet21k before pretraining."
REFERENCES,0.45,"Model
Layers
Width
Dim.
Heads
Params"
REFERENCES,0.453125,"ViT-xS
4
256
256
4
5.7M
ViT-S
6
516
516
6
19.6M
ViT-B
12
768
768
12
86.7M"
REFERENCES,0.45625,"Model
Resblocks
Params"
REFERENCES,0.459375,"R26
[2, 2, 2, 2]
14.0M
R50
[3, 4, 6, 3]
23.5M
R101
[3, 4, 23, 3]
42.5M
R152
[3, 8, 36, 3]
58.1M
R200
[3, 24,36,3]
62.6M"
REFERENCES,0.4625,Table 2: Conﬁgurations of ResNet models
REFERENCES,0.465625,"A.1.3
CIFAR-100 DISTRIBUTION SHIFT TASK"
REFERENCES,0.46875,"Here we provide more information about the CIFAR-100 distribution-shift task, which was intro-
duced in Ramasesh et al. (2021) and forms the basis of our experiments in section A.1.3 of the main
text. The idea behind this task is to mimic a way in which catastrophic forgetting might occur in
practice, in image-classiﬁcation settings where the class labels remain constant but the distribution
of the images changes. A key features of this type of setting is that the identity of the task is not
known to the model at inference time, so task-speciﬁc components (such as multiple readout layers)
are not allowed. For example, in a medical setting, one might seek to classify images as displaying
signs of a disease or not, and the system might be trained sequentially on images taken at various
hospitals. While the CIFAR-100 dataset features images divided into 100 distinct classes, these
classes are grouped into 20 superclasses. The CIFAR-100 distribution shift task sequence involves
classifying an image by its superclass; individual tasks in the sequence draw images from a single
subclass for each superclass. As a concrete example, in the experiments in the main text we use
the ﬁve superclasses aquatic mammals, fruits and vegetables, household electrical devices, trees,
and vehicles-11. For the ﬁrst task in the sequence, Task A, we uses the classes dolphin (for aquatic
mammals), apple (for fruits and vegetables), lamp (for household electrical devices), maple tree
(for trees), and bicycle (for vehicles-1). For the second task, Task B, we use the classes whale (for
aquatic mammals), orange (for fruits and vegetables), television (for household electrical devices),
willow (for trees), and motorcycle (for vehicles-2)."
REFERENCES,0.471875,"A.2
FINETUNING EXPERIMENTAL DETAILS"
REFERENCES,0.475,"In this section we provide details on the ﬁnetuning procedures for the experiments in the main text.
All ﬁnetuning was done using stochastic gradient descent with momentum (β = 0.9), clipping
gradients at unity; batch size is ﬁxed at 512. In the ﬁrst task, we used a schedule with a 5-step linear"
REFERENCES,0.478125,"1The CIFAR-100 dataset features two vehicle superclasses, denoted vehicles-1 and vehicles-2"
REFERENCES,0.48125,Published as a conference paper at ICLR 2022
REFERENCES,0.484375,"ramp to a maximum learning rate followed by a cosine decay to zero over the full ﬁrst-task training.
In the second task, we used constant learning rates."
REFERENCES,0.4875,"Section 3.1, Figure 1: Finetuning both the Vision Transformers and ResNets was done for 500
steps. For Vision Transformers, the ﬁrst-task training used a warm-up from η = 0.01 to η = 0.05.
For each model we perform four different runs, with second-task (constant) learning rates 0.001,
0.003, 0.007, and 0.01. For ResNets, the task-A learning rate warm-up was from η = 0.002 to
η = 0.01. The (constant) second-task learning rates we used were 0.008, 0.004, 0.002, 0.001,
0.0003, and 0.0001."
REFERENCES,0.490625,"Section 3.1, Figure 2: In the 10-task split-CIFAR100 sequence (left), each task was trained for
200 steps; the initial task used a learning-rate schedule with a 5-step warmup from η = 0.01 to
η = 0.05 followed by a cosine decay to 0 over the 200 steps, while all subsequent steps used constant
η = 0.005. In the 2-task 50-class split-CIFAR100 sequence (right), the settings are identical to those
of the Vision Transformer training in Figure 1."
REFERENCES,0.49375,"Section 3.1, Figure 3: For ResNets, we used a ﬁrst-task warmup from η = 0.001 to η = 0.005,
and trained each task for 300 steps. Constant second-task learning rates were 0.001, 0.0003, and
0.0001. For Vision Transformers, we trained each task for 200 steps, using a ﬁrst-task warmup from
η = 0.01 to η = 0.05. Constant second-task learning rates were 0.01, 0.003, and 0.001."
REFERENCES,0.496875,"Section 3.2, Figure 4: The pretrained models in this ﬁgure were the same as those in Figure 1.
For the trained-from-scratch models, we used a linear warm-up from η = 0.006 to η = 0.03, and
trained each task for 15,000 steps. Constant second-task learning rates were 0.001, 0.0003, and
0.0001. Note that training here is much longer than for the pretrained models."
REFERENCES,0.5,"Section 3.3, Figure 5: Vision Transformer training settings are identical to those of Figure 1."
REFERENCES,0.503125,"Section 3.3, Figure 6: In the left panel, Vision Transformer training settings are identical to those of
Figure 1. In the right two panels, settings are also identical to those of Figure 1, with the exception
of the number of ﬁnetuning steps, as indicated in the ﬁgure legend."
REFERENCES,0.50625,"Section 3.4, Figure 7: We ﬁnetune for 300 steps on each task, with a maximum learning rate of
η = 0.0002 (we omit the linear warm-up for these models, but keep the cosine decay). Constant
second-task learning rates are η = 0.0002, η = 0.0001, η = 0.00004, and η = 0.00002."
REFERENCES,0.509375,"A.3
LANGUAGE"
REFERENCES,0.5125,"Models are decoder-only transformers which were pretrained using Adafactor and learning rate 1.0,
context length 1024 and batch size 256. We ﬁnetuned with batch size of 8 and set a constant learning
rate equal to the learning rate at the end of pretraining rescaled by the relative batch size, 8/256."
REFERENCES,0.515625,"B
ADDITIONAL EXPERIMENTS"
REFERENCES,0.51875,"B.1
FORGETTING IN LANGUAGE MODELS"
REFERENCES,0.521875,"Transfer learning from unsupervised pretraining is commonplace in Natural Language Processing.
It has become standard to pretrain models on data crawled from the internet and then ﬁnetune on
smaller, downstream tasks (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Brown et al.,
2020). This bears similarities with both the unsupervised pretraining and the experiments in Figure 4
where a single head was used to train on different distributions (subclasses). Indeed, as mentioned in
the main text, part of the conceptual motivation for this work came from the observation that large
language models with diverse pretraining perform well on tasks with no ﬁnetuning – ostensibly
retaining information for many gradient steps. Here, we include some preliminary empirical results
probing catastrophic forgetting directly in large language models."
REFERENCES,0.525,"We consider decoder only transformer models pretrained on data scraped from the web, including
C4 (Raffel et al., 2020). These models range in size from 57 million to 8.6 billion non-embedding
parameters and were originally trained as part of the BIG-bench collaboration (2021). In Figure 11,
we consider sequentially ﬁnetuning the models on Task A: IMDb Reviews for 1.5k steps followed
by Task B: english Wikipedia for 10k steps. The longer ﬁnetuning time for Task B is intended to
highlight forgetting, as these models are relatively robust."
REFERENCES,0.528125,Published as a conference paper at ICLR 2022
REFERENCES,0.53125,"Figure 11: Forgetting in sequential language-modeling. Token accuracy for decoder-only transformer mod-
els show systematic improvement in joint Task A, Task B performance across scales."
REFERENCES,0.534375,"We ﬁnd that, as in the case of pretrained vision models, the best joint Task A, Task B performance
improves with scale."
REFERENCES,0.5375,"B.2
FORGETTING IN VISION MODELS WITH UNSUPERVISED PRETRAINING"
REFERENCES,0.540625,"In the main text, the models we studied were pretrained in a supervised manner on ImageNet21k.
However, part of the conceptual motivation for this work came from the properties of language
models trained in an unsupervised fashion. Furthermore, recently, unsupervised pretraining meth-
ods have yielded vision models which achieve good downstream performance; among these are
SimCLR (Chen et al., 2020), MoCo (He et al., 2020), DINO (Caron et al., 2021), and BYOL (Grill
et al., 2020). Given the increased prevalence of such unsupervised pretraining methods, we inves-
tigate in this section whether unsupervised pretraining, combined with scale, provides resistance
to forgetting similarly to supervised pretraining. We take publicly-available ResNet models which
were pretrained using SimCLR on the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015),
and ﬁnetune them on the two-task split CIFAR-10 sequence. The resulting forgetting frontiers,
shown in Figure 12, show that as in supervised pretraining, SimCLR pretraining yields models
which are resilient to forgetting, especially as scale is increased. We leave a thorough exploration of
the continual-learning properties of other unsupervised pretraining methods for future work."
REFERENCES,0.54375,"Figure 12: Forgetting frontiers on a split-CIFAR10 task for models pretrained in an unsupervised man-
ner on ImageNet1k. ResNets pretrained using unsupervised SimCLR on ImageNet1k exhibit a similar trend
to that observed in the supervised setting of forgetting robustness improving with model scale."
REFERENCES,0.546875,Published as a conference paper at ICLR 2022
REFERENCES,0.55,"B.3
FORGETTING IN TASK SEQUENCES WITH MORE THAN TWO SUBTASKS"
REFERENCES,0.553125,"In this section, we extend our results on vision past the two-task setting which was the primary focus
of the main text. Speciﬁcally, we use a 10-class, 10-subtask split of the CIFAR100 dataset to study
how forgetting in this scenario varies as a function of model and dataset scale. The speciﬁc split we
use is shown in Table 3. As in the main text, we study the performance of Vision Transformer and
ResNet models."
REFERENCES,0.55625,"Task A
Task B
Task C
Task D
Task E
Apple
Bowl
Chair
Dolphin
Lamp
Aquarium Fish
Boy
Chimpanzee
Elephant
Lawnmower
Baby
Bridge
Clock
Flatﬁsh
Leopard
Bear
Bus
Cloud
Forest
Lion
Beaver
Butterﬂy
Cockroach
Fox
Lizard
Bed
Camel
Couch
Girl
Lobster
Bee
Can
Crab
Hamster
Man
Beetle
Castle
Crocodile
House
Maple tree
Bicycle
Caterpillar
Cup
Kangaroo
Motorcycle
Bottle
Cattle
Dinosaur
Keyboard
Mountain"
REFERENCES,0.559375,"Task F
Task G
Task H
Task I
Task J
Mouse
Plain
Rose
Squirrel
Train
Mushroom
Plate
Sea
Streetcar
Trout
Oak tree
Poppy
Seal
Sunﬂower
Tulip
Orange
Porcupine
Shark
Sweet pepper
Turtle
Orchid
Possum
Shrew
Table
Wardrobe
Otter
Rabbit
Skunk
Tank
Whale
Palm tree
Raccoon
Skyscraper
Telephone
Willow tree
Pear
Ray
Snail
Television
Wolf
Pickup truck
Road
Snake
Tiger
Woman
Pine tree
Rocket
Spider
Tractor
Worm"
REFERENCES,0.5625,Table 3: 10-subtask split of CIFAR100 used in our experiments.
REFERENCES,0.565625,We ﬁnd that:
REFERENCES,0.56875,"• As in the two-task setting, both the forgetting frontiers and the best average accuracy im-
prove with model scale as a power-law in the number of model parameters."
REFERENCES,0.571875,"• When we scale the size of the pre-training dataset, we also observe an improvement with
scale, though with some saturation around the full ImageNet21k dataset.."
REFERENCES,0.575,"Model scaling: Model-scaling plots showing the best average accuracy as a function of model
parameters are show in Figure 13. As in the two-task split CIFAR10 sequence, these scaling plots
are one way to show that the continual-learning performance of both architectures improves with
scale. The power-law exponents are different in this 10-task setting than they are for the two-task
setting, but interestingly all are close to unity."
REFERENCES,0.578125,"Another way to see the improvement in continual-learning ability with model scale is to plot forget-
ting frontiers, which we do in Figures 15 and 16. In this setting with more than two tasks, plotting a
single forgetting frontier is not possible, but we plot two-task frontiers for all pairs of subtasks (with
10 subtasks, there are 10×9/2 = 45 pairs). These frontiers are constructed in exactly the same way
as the two-task frontiers, showing the test accuracies for each subtask throughout the entirety of the
10-task training. As the ﬁgures show, forgetting frontiers of larger models dominate the forgetting
frontiers of smaller models of the same architecture. This complements the previous scaling-plot as
a demonstration of improving continual-learning with scale, though the behavior here is not as clean
as it was in the two-task setting."
REFERENCES,0.58125,"Data scaling: A data-scaling plot showing the best average accuracy for a ViT-S model as a function
of pretraining dataset size is shown in Figure 14. Forgetting frontiers are shown in Figure 17."
REFERENCES,0.584375,Published as a conference paper at ICLR 2022
REFERENCES,0.5875,"Figure 13: Model scaling on 10-subtask split-CIFAR100. (left) Scaling of best average error with parameter
count for vision transformer models. (right) Scaling of best average error with parameter count for ResNet
models."
REFERENCES,0.590625,"Figure 14: Data scaling on 10-subtask split-CIFAR100. Scaling of best average error with pre-training
dataset size (measured in a fraction of the full ImageNet21k dataset) for the ViT-S 16 model."
REFERENCES,0.59375,Published as a conference paper at ICLR 2022
REFERENCES,0.596875,"Figure 15: Forgetting frontiers for 10-task Split CIFAR100, Resnet. Colors are the same as in Figure 1 of
the main text. All images in each row share the same task for the x-axis, i.e. the x-axis of the top row represents
task A, x-axis of the second-from-the-top row represents task B, etc. All images in each column share the same
task for y-axis, i.e. the y-axis of the leftmost column represents task B, the y-axis of the second-to-leftmost
column represents task C, etc."
REFERENCES,0.6,Published as a conference paper at ICLR 2022
REFERENCES,0.603125,"Figure 16: Forgetting frontiers for 10-task Split CIFAR100, Vision Transformer. Colors are the same as
in Figure 1 of the main text. All images in each row share the same task for the x-axis, i.e. the x-axis of the
top row represents task A, x-axis of the second-from-the-top row represents task B, etc. All images in each
column share the same task for y-axis, i.e. the y-axis of the leftmost column represents task B, the y-axis of the
second-to-leftmost column represents task C, etc."
REFERENCES,0.60625,Published as a conference paper at ICLR 2022
REFERENCES,0.609375,"Figure 17: Forgetting frontiers for 10-task Split CIFAR100 versus data scale. These frontiers are plotted
for a ViT-S 16 model. Colors are the same as in Figure 8(left) of the main text. All images in each row share
the same task for the x-axis, i.e. the x-axis of the top row represents task A, x-axis of the second-from-the-top
row represents task B, etc. All images in each column share the same task for y-axis, i.e. the y-axis of the
leftmost column represents task B, the y-axis of the second-to-leftmost column represents task C, etc."
REFERENCES,0.6125,Published as a conference paper at ICLR 2022
REFERENCES,0.615625,"B.4
FORGETTING IN DOWNSTREAM TASK SEQUENCES ON DATASETS OTHER THAN
CIFAR10 AND CIFAR100"
REFERENCES,0.61875,"In this section, we study the continual-learning performance of ImageNet21k-models on down-
stream task sequences beyond CIFAR10 and CIFAR100. Speciﬁcally, we use the following datasets:"
REFERENCES,0.621875,"• The Oxford-IIIT pet dataset (Parkhi et al., 2012), a 37-category dataset of images of pets,
labeled by breed, with roughly 200 images per class. In our experiments, we create a
two-task split of this dataset, with the ﬁrst 19 breeds (alphabetically by English label) com-
prising task A and the second 18 breeds comprising task B."
REFERENCES,0.625,"• The Oxford Flowers 102 dataset (Nilsback & Zisserman, 2008), a 102-category dataset
consisting of ﬂowers common in the the UK, labeled by species. In our experiments, we
create a two-task split of this dataset, with the ﬁrst 51 ﬂower species (alphabetically by
English species name) in task A and the second 51 ﬂower species in task B."
REFERENCES,0.628125,"• The Street View House Numbers (SVHN) dataset (Netzer et al., 2011), a 10-category
dataset where images consist of single-digit numbers from house numbers. We use the
the two-task split in which task A consists of digits 0,1,2,3,4 and task B consists of digits
5,6,7,8,9."
REFERENCES,0.63125,"• The Caltech-UCSD Birds 200 (CUB-200) dataset (Welinder et al., 2010), a 200-category
dataset consisting of photos of birds, labeled by species. As above, we create a two-task
split by alphabetically sorting English species names and taking task A to consist of the
ﬁrst 100 categories, and task B to consist of the second 100 categories."
REFERENCES,0.634375,"• The Cars196 dataset, a 196-category dataset consisting of natural images of cars (Krause
et al., 2013), with class labels speciﬁc models (including years) of cars, e.g. 2012 Tesla
Model S or 2012 BMW M3 coupe. We split this into two tasks by using the standard label
numbers in TensorFlow datasets, with labels 0-97 in task A and 98-195 in task B."
REFERENCES,0.6375,"• The Domainnet/Clipart dataset, a subset of Domainnet (Peng et al., 2019), consists of
clipart drawings (unlike all of the above datasets, which consist of natural images) of 345
different categories including tennis racquets, diamonds, and umbrellas. We create a two-
task split alphabetically, with 173 categories in task A and 172 categories in task B."
REFERENCES,0.640625,"We use the same basic setup as in the split-CIFAR10 task sequence, in which the ﬁrst task is trained
with a 5-step warmup followed by a cosine decay, while the second task is trained with a constant
learning rate. The speciﬁc learning rates we use for each of these datasets are given in table 4."
REFERENCES,0.64375,"Forgetting frontiers and best-average-error scaling plots are shown in Figure 2. From these plots,
we can see that the improvement in forgetting with scale is not unique to the CIFAR10 and CI-
FAR100 task sequences we used in the main text, but occurs for all of these datasets as well. Scaling
plots show that this improvement in average-task performance is a power-law function of the model
parameters, but with exponents which vary across datasets."
REFERENCES,0.646875,"The broader implication of these plots is that the improvement in forgetting performance with scale
is not limited to downstream datasets which are essentially in-distribution of the pretraining dataset
(e.g., CIFAR10 and CIFAR100 are very similar to images found in ImageNet21k). As a proxy
to quantify how ‘in-distribution’ the downstream datasets are, in Table 5, we show performances
for both a ResNet and ViT model when we train just a linear classiﬁer on top of weights frozen
after the ImageNet21k pretraining. We take the closeness of this head-only performance to the full
ﬁnetuned performance as a proxy for the dataset being in-distribution to ImageNet21k; using this
proxy, the Oxford-IIIT Pet dataset, Oxford Flowers dataset, CIFAR10, CIFAR100, and CUB Birds
200 dataset are roughly in-distribution, while Domainnet/Clipart, Cars196, and SVHN are not. But,
as we emphasized above, even for these ‘out-of-distribution‘ datasets, forgetting is largely mitigated
by scale in the pretrained models."
REFERENCES,0.65,"For completeness, we show head-only and full-ﬁnetuning performances for all ResNet and ViT
models on the ﬁrst task (task A) of our split-CIFAR10 sequence in Table 6."
REFERENCES,0.653125,Published as a conference paper at ICLR 2022
REFERENCES,0.65625,"Vision Transformers
ResNets
Oxford-IIIT pet
Task A max learning rate
0.05
0.01
Task B learning rates
0.01, 0.007, 0.003
0.003, 0.001, 0.0003
Finetuning steps
500
500
Oxford Flowers 102
Task A max learning rate
0.03
0.003
Task B learning rates
0.003, 0.001, 0.0003
0.0007, 0.0003, 0.0001
Finetuning steps
500
200
SVHN
Task A max learning rate
0.1
0.03
Task B learning rates
0.06, 0.04, 0.01, 0.007, 0.003
0.01, 0.007, 0.003, 0.0007, 0.0003
Finetuning steps
500
500
Caltech-UCSD Birds 200
Task A max learning rate
0.05
0.1
Task B learning rates
0.016, 0.008, 0.004, 0.002, 0.001
0.03, 0.01, 0.007, 0.003, 0.0007
Finetuning steps
250
500
Domainnet/Clipart
Task A max learning rate
0.13
0.1
Task B learning rates
0.06, 0.04, 0.01, 0.007, 0.003
0.03, 0.01, 0.007, 0.003, 0.0007
Finetuning steps
500
500
Cars 196
Task A max learning rate
0.1
0.08
Task B learning rates
0.03, 0.01, 0.003, 0.001
0.03, 0.01, 0.003, 0.001, 0.0003
Finetuning steps
1000
1000"
REFERENCES,0.659375,Table 4: Finetuning settings used for the two-task experiments on datasets described in section B.4.
REFERENCES,0.6625,"Oxford-IIIT pet
Flowers
SVHN
Head-only
Full
Head-only
Full
Head-only
Full
ResNet101
0.9018
0.9202
0.97168
0.95752
0.3822
0.8992
ViT-S 16
0.8923
0.9099
0.9792
0.9842
0.3116
0.9229
CUB Birds 200
Clipart
Cars196
Head-only
Full
Head-only
Full
Head-only
Full
ResNet101
0.8702
0.8652
0.3977
0.7538
0.6212
0.7112
ViT-S 16
0.8047
0.8519
0.31
0.8056
0.4759
0.7237
CIFAR10
CIFAR100
Head-only
Full
Head-only
Full
ResNet101
0.8701
0.9674
0.5890
0.6924
ViT-S 16
0.7813
0.9592
0.5058
0.7977"
REFERENCES,0.665625,"Table 5: Comparison of head-only ﬁnetuning and full ﬁnetuning on downstream datasets. Us-
ing a pretrained (supervised on ImageNet21k) ResNet101 and ViT-S 16, we train either (a) a linear
layer on top of frozen representations, or (b) the full model, on downstream datasets. We take
this as a proxy for the similarity between the downstream dataset and ImageNet21k; the closer the
head-only performance is to the fully-trained performance, the more similarity we expect."
REFERENCES,0.66875,"Head Only
Full ﬁnetuning
Head Only
Full Finetuning
ViT-xS 16
0.7676
0.9527
ResNet26
0.8767
0.9474
ViT-S 16
0.7906
0.9857
ResNet50
0.9030
0.9805
ViT-B 16
0.9490
0.9937
ResNet101
0.9371
0.9896
ResNet152
0.9362
0.9913
ResNet200
0.9557
0.9933"
REFERENCES,0.671875,"Table 6: Comparison of head-only ﬁnetuning and full ﬁnetuning on Task A of Split CIFAR10.
Accuracies achieved by training a linear classiﬁer on top of frozen pretrained representations on the
task A split of CIFAR10 (used in the main text), freezing the features of the pretrained models after
training on ImageNet21k."
REFERENCES,0.675,Published as a conference paper at ICLR 2022
REFERENCES,0.678125,"B.5
FORGETTING ON THE PRE-TRAINING TASK DURING FINETUNING"
REFERENCES,0.68125,"Given the strong performance of large-scale pretrained models on continual learning tasks we have
seen in the main text, it is natural to wonder to what extent performance drops on the original pre-
training tasks throughout ﬁnetuning. One intuition might suggest that because the pretraining dataset
is so large, performance drops on the pretraining task will be minimal throughout ﬁnetuning. How-
ever, in our experiments, we found that this was not the case. In Table 7, we show the performance
drops on the pretraining tasks for all models, in two downstream tasks: CIFAR10 and SVHN. In
most cases, we ﬁnd signﬁcant performance drops, though these are much more pronounced when
ﬁnetuning on the SVHN dataset than when ﬁnetuning on CIFAR10."
REFERENCES,0.684375,"Just pretrained
Pretrained →Finetuned
Pretrained →ﬁnetuned
on CIFAR10
on SVHN
ResNet26
0.3787
0.1233
0.0001
ResNet50
0.4077
0.1433
0.0005
ResNet101
0.4317
0.1680
0.0015
ResNet152
0.4450
0.1940
0.0021
ResNet200
0.4459
0.2080
0.0027"
REFERENCES,0.6875,"ViT-xS 16
0.2717
0.1859
0.0085
ViT-S 16
0.3699
0.3222
0.0776
ViT-B 16
0.4765
0.4300
0.3022"
REFERENCES,0.690625,"Table 7: Forgetting on the pretraining task after ﬁnetuning. Top-1 accuracy on ImageNet21k
after pretraining, and after downstream ﬁnetuning. Unlike pretrained models performing continual
learning on downstream tasks, it appears that there are signiﬁcant performance drops on the original
ImageNet21k task."
REFERENCES,0.69375,"C
SUPPORTING RESULTS"
REFERENCES,0.696875,"In the following subsections, we present results which are directly supporting experiments or ﬁgures
in the main text, but not new experiments."
REFERENCES,0.7,"C.1
COMPARING PRETRAINED AND TRAINED-FROM-SCRATCH RESNETS"
REFERENCES,0.703125,"In the main text, Figure 6, we showed a comparison between the forgetting frontiers (on the split
CIFAR10 task sequence) of trained-from-scratch ResNet models and pretrained ResNet models,
where we handicapped the pretrained ResNet models (by training them for fewer steps) so that they
matched task-A performance of the trained-from-scratch ResNets. This was done to show that even
when task-A performance is the same, pretrained ResNets forget far less than their trained-from-
scratch counterparts. For space, in the main text we only showed three ResNet models: ResNet26,
ResNet101, and ResNet200. Here, we show the forgetting frontiers for all ResNet models (Fig-
ure 18), and we also show (Figure 19) the data plotted slightly differently—with all model sizes
plotted on the same graph—to make it apparent that while the forgetting frontiers of the pretrained
ResNet models clearly improve with model size, the forgetting frontiers of trained-from-scratch
ResNet models do not. Figure 19 also includes a scaling plot of the best average error vs. model
parameters, supporting the conclusion that in trained-from-scratch ResNets, scaling does not appear
to beneﬁt the continual learning performance."
REFERENCES,0.70625,Published as a conference paper at ICLR 2022
REFERENCES,0.709375,"Figure 18: Forgetting frontier for ResNets trained from scratch, vs.
pretrained ResNets (on Ima-
geNet21k). The pretrained ResNets were handicapped during ﬁne-tuning, in order to match the Task A perfor-
mance of the from-scratch model. This ﬁgure is identical to Figure 6 in the main text, with the addition of the
ResNet50 and ResNet152 models."
REFERENCES,0.7125,"Figure 19: Forgetting frontier for ResNets trained from scratch, vs.
pretrained ResNets (on Ima-
geNet21k). The pretrained ResNets were handicapped during ﬁne-tuning, in order to match the Task A perfor-
mance of the from-scratch model. The data plotted here is the same as that plotted in Figure 6 in the main text;
here, the intention is to make clear that the performance of the trained-from-scratch models do not exhibit any
apparent beneﬁt with scale, while the pretrained models do (right) Scaling of the best average (task A / task B)
accuracy shows that only in pretrained models does the performance improve with scale."
REFERENCES,0.715625,"C.2
LEARNING CURVES FOR SPLIT-CIFAR10 VISION EXPERIMENTS"
REFERENCES,0.71875,"In Figure 1 of the main text, we plot forgetting frontiers of pretrained Vision Transformers and
ResNets on the split-CIFAR10 task sequence. Corresponding to these frontiers are the learning
curves (accuracies vs. step) shown in Figures 21 and 20."
REFERENCES,0.721875,Published as a conference paper at ICLR 2022
REFERENCES,0.725,"0
200
400
600
800
Steps 0.7 0.8 0.9 1.0"
REFERENCES,0.728125,Accuracy
REFERENCES,0.73125,ResNet26
REFERENCES,0.734375,"0
200
400
600
800
Steps 0.7 0.8 0.9 1.0"
REFERENCES,0.7375,Accuracy
REFERENCES,0.740625,ResNet50
REFERENCES,0.74375,"0
200
400
600
800
Steps 0.7 0.8 0.9 1.0"
REFERENCES,0.746875,Accuracy
REFERENCES,0.75,ResNet101
REFERENCES,0.753125,"0
200
400
600
800
Steps 0.7 0.8 0.9 1.0"
REFERENCES,0.75625,Accuracy
REFERENCES,0.759375,ResNet152
REFERENCES,0.7625,"0
200
400
600
800
Steps 0.7 0.8 0.9 1.0"
REFERENCES,0.765625,Accuracy
REFERENCES,0.76875,ResNet200
REFERENCES,0.771875,Task B learning rate
REFERENCES,0.775,"0.0001
0.001
0.002
0.004
0.008
0.0003"
REFERENCES,0.778125,"Figure 20: ResNet Learning curves on Split CIFAR10 task. Learning curves for the ResNet models of
Figure 1. The solid/dashed lines are Task A/B accuracy."
REFERENCES,0.78125,Published as a conference paper at ICLR 2022
REFERENCES,0.784375,"0
200
400
600
800
Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.7875,Accuracy
REFERENCES,0.790625,ViT-xS_16
REFERENCES,0.79375,"0
200
400
600
800
Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.796875,Accuracy
REFERENCES,0.8,ViT-S_16
REFERENCES,0.803125,"0
200
400
600
800
Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.80625,Accuracy
REFERENCES,0.809375,ViT-B_16
REFERENCES,0.8125,Task B learning rate
REFERENCES,0.815625,"0.01
0.003
0.001
0.007"
REFERENCES,0.81875,"Figure 21: ViT Learning curves on Split CIFAR10 task. Learning curves for the ViT models of Figure 1.
The solid/dashed lines are Task A/B accuracy."
REFERENCES,0.821875,Published as a conference paper at ICLR 2022
REFERENCES,0.825,"C.3
LEARNING CURVES FOR THE LANGUAGE EXPERIMENT"
REFERENCES,0.828125,"For completeness, Figure 22 displays the token accuracy on task A as a function of the number of
steps, corresponding to the frontiers shown in Figure 11 of the main text."
REFERENCES,0.83125,"Figure 22: Learning curves in sequential language-modeling. Token accuracy as a function of steps for the
different models. The dashed line shows the point at which training switches from Task A to Task B."
REFERENCES,0.834375,"C.4
FORGETTING FRONTIERS COLORED BY LEARNING RATE"
REFERENCES,0.8375,"In the forgetting frontiers of Figure 1 of the main text, the frontiers are colored by model size;
each model, however, was trained with several different learning rates during the second task. In
Figures 23 and 24, we plot the same frontiers, coloring them by learning rate on the left, and by
proper time (learning rate multiplied by step number) on the right."
REFERENCES,0.840625,"We also plot a scaling curve in Figure 25 of the average accuracy at a ﬁxed learning rate and step
(η = 0.0001 and t = 200), to show that the power-law scaling holds even if one looks at a ﬁxed
hyperparameter choice."
REFERENCES,0.84375,Published as a conference paper at ICLR 2022
REFERENCES,0.846875,ResNet26
REFERENCES,0.85,ResNet50
REFERENCES,0.853125,ResNet101
REFERENCES,0.85625,ResNet152
REFERENCES,0.859375,ResNet200
REFERENCES,0.8625,"0.65
0.70
0.75
0.80
0.85
0.90
0.95
0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.865625,"0.65
0.70
0.75
0.80
0.85
0.90
0.95"
REFERENCES,0.86875,"0.70
0.75
0.80
0.85
0.90
0.95
1.00
0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.871875,"0.70
0.75
0.80
0.85
0.90
0.95
1.00"
REFERENCES,0.875,"0.75
0.80
0.85
0.90
0.95
1.00
0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.878125,"0.75
0.80
0.85
0.90
0.95
1.00"
REFERENCES,0.88125,"0.80
0.85
0.90
0.95
1.00
0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.884375,"0.80
0.85
0.90
0.95
1.00"
REFERENCES,0.8875,"0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000
0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.890625,Learning rate
REFERENCES,0.89375,"0.0001
0.001
0.002"
REFERENCES,0.896875,"0.004
0.008
0.0003"
REFERENCES,0.9,0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000
REFERENCES,0.903125,"0.0
8.0
step*lr"
REFERENCES,0.90625,"Figure 23: Step and learning rate along Split CIFAR10 forgetting frontier for ResNets. We plot the
forgetting frontier of Figure 1. The left column is colored by learning rate with transparency set by step (darker)
is later during task 2 training. The right column is colored by step times learning rate."
REFERENCES,0.909375,Published as a conference paper at ICLR 2022
REFERENCES,0.9125,ViT-xS_16
REFERENCES,0.915625,ViT-S_16
REFERENCES,0.91875,ViT-B_16
REFERENCES,0.921875,"0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.925,"0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00"
REFERENCES,0.928125,"0.95
0.96
0.97
0.98
0.99
1.00
0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01"
REFERENCES,0.93125,"0.95
0.96
0.97
0.98
0.99
1.00"
REFERENCES,0.934375,"0.986 0.988 0.990 0.992 0.994 0.996 0.998 1.000
0.970 0.975 0.980 0.985 0.990 0.995 1.000 1.005 1.010"
REFERENCES,0.9375,Learning rate
REFERENCES,0.940625,"0.01
0.003"
REFERENCES,0.94375,"0.001
0.007"
REFERENCES,0.946875,0.986 0.988 0.990 0.992 0.994 0.996 0.998 1.000
REFERENCES,0.95,"0.0
10.0
step*lr"
REFERENCES,0.953125,"Figure 24: Step and learning rate along Split CIFAR10 forgetting frontier for ViT models. We plot the
forgetting frontier of Figure 1. The left column is colored by learning rate with transparency set by step (darker)
is later during task 2 training. The right column is colored by step times learning rate."
REFERENCES,0.95625,Published as a conference paper at ICLR 2022
REFERENCES,0.959375,"107
108"
REFERENCES,0.9625,Parameters 10−2 10−1 Error
REFERENCES,0.965625,"lr=0.001, step=200"
REFERENCES,0.96875,ResNet: α = -1.29
REFERENCES,0.971875,ViT: -0.86
REFERENCES,0.975,"Figure 25: Average error scaling at ﬁxed learning rate and step. We plot the average Task A Task B error
for the setup in Figure 1 for a ﬁxed learning rate of 0.001 and 200 Task B training steps for both ViTs and
ResNets."
REFERENCES,0.978125,Published as a conference paper at ICLR 2022
REFERENCES,0.98125,"C.5
REPRESENTATION OVERLAP MATRICES FOR ALL RESNET AND VIT MODELS"
REFERENCES,0.984375,"In the main text, we described the computation of the classwise representation overlap matrices for
our models at the end of Task A training, showing an example of such a matrix for both a pretrained
and a trained-from-scratch ResNet101 model. In Figure 27, we show representation overlap matrices
for all of the ResNet models, which bears out our statement in the main text that pretrained ResNet
representations were much closer to orthogonal than their trained-from-scratch counterparts. In
Figure 26, we show representation overlap matrices for the (pretrained) Vision Transformer models.
As with the pretrained ResNet models, the cross-class representation overlaps become closer to
orthogonal as we increase the model size."
REFERENCES,0.9875,"In these ﬁgures, we also plot the representation overlaps for pre-trained models before being trained
on task A (which was not plotted in the main text). From these plots one can see that while pre-
training on ImageNet21k introduces some level of orthogonality in the model representations of
CIFAR10 images, this orthogonality becomes much more pronounced after training the model on
task A of the CIFAR10 split."
REFERENCES,0.990625,"Figure 26: Classwise representation matrices for Vision Transformer models taken (left) after
the end of training on the ﬁrst task (Task A) of a two-task split CIFAR-10 sequence, and (right) after
pretraining on ImageNet21k. Classes are ordered alphabetically, left to right and down to up, i.e.
the order is airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck."
REFERENCES,0.99375,Published as a conference paper at ICLR 2022
REFERENCES,0.996875,"Figure 27: Classwise representation matrices for ResNet models. (Left) Models are trained from
random initialization on the ﬁrst task (task A) of a two-task split CIFAR-10 sequence. (Middle)
Models are pretrained on ImageNet21k and then ﬁnetuned on task A of the two-task split CIFAR-10
sequence. (Right) Models are pretrained on ImageNet21k, but no ﬁnetuning is done. Classes are
ordered alphabetically, left to right and down to up, i.e. the order is airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, truck."
