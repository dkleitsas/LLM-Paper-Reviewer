Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0028735632183908046,"Saliency methods calculate how important each input feature is to a machine learn-
ing model’s prediction, and are commonly used to understand model reasoning.
“Faithfulness,” or how fully and accurately the saliency output reﬂects the under-
lying model, is an oft-cited desideratum for these methods. However, explanation
methods must necessarily sacriﬁce certain information in service of user-oriented
goals such as simplicity. To that end, and akin to performance metrics, we frame
saliency methods as abstractions: individual tools that provide insight into speciﬁc
aspects of model behavior and entail tradeoffs. Using this framing, we describe
a framework of nine dimensions to characterize and compare the properties of
saliency methods. We group these dimensions into three categories that map to
different phases of the interpretation process: methodology, or how the saliency is
calculated; sensitivity, or relationships between the saliency result and the under-
lying model or input; and, perceptibility, or how a user interprets the result. As we
show, these dimensions give us a granular vocabulary for describing and compar-
ing saliency methods — for instance, allowing us to develop “saliency cards” as
a form of documentation, or helping downstream users understand tradeoffs and
choose a method for a particular use case. Moreover, by situating existing saliency
methods within this framework, we identify opportunities for future work, includ-
ing ﬁlling gaps in the landscape and developing new evaluation metrics."
INTRODUCTION,0.005747126436781609,"1
INTRODUCTION"
INTRODUCTION,0.008620689655172414,"As machine learning (ML) systems are increasingly deployed into real-world contexts, stakeholder
interviews (Tonekaboni et al., 2019; Bhatt et al., 2020), design best practices (Amershi et al., 2019),
and legal frameworks (European Commission, 2018) have underscored the need for explainability.
Saliency methods — a class of attribution methods that aim to identify features in an input that were
important to a trained ML prediction — are frequently used to provide explanations. However, each
method operates differently and, thus, multiple methods can produce seemingly varying explana-
tions for the same model and input. How, then, should we reason about choosing and comparing
methods for a particular application?"
INTRODUCTION,0.011494252873563218,"Prior work has suggested that “faithfulness,” or how accurately a saliency result reﬂects the under-
lying model, is a desideratum for these methods (Li et al., 2021; Ding & Koehn, 2021; Tomsett
et al., 2020; Adebayo et al., 2018). In this paper, however, we argue that faithfulness is not a pro-
ductive goal for saliency methods — by design, they cannot offer a complete and accurate reﬂection
of a model’s behavior, akin to a printout of model weights. Rather, we frame saliency methods
as abstractions of model behavior that selectively preserve and necessarily sacriﬁce information in
service of human-centric goals such as simplicity and understandability."
INTRODUCTION,0.014367816091954023,"With this framing, we propose a nine-dimensional framework to characterize and compare saliency
methods. These dimensions fall into three categories, corresponding to different parts of the interpre-
tation process: methodology, or how the saliency is computed; sensitivity, or relationships between
the saliency and the underlying model or input; and perceptibility, or how an end-user perceives the
saliency. These dimensions decompose a singular notion of faithfulness into more granular units
that can be reasoned about individually: by situating methods along these dimensions, we can sur-
face their relative strengths, limitations, and differences to better understand trade offs that are latent
in their design. In doing so, we demonstrate how our framework allows us to develop “saliency"
INTRODUCTION,0.017241379310344827,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020114942528735632,"Method
Description"
INTRODUCTION,0.022988505747126436,"Vanilla Gradients (VG) (Erhan et al.,
2009; Simonyan et al., 2013)
Computes the gradient of the model’s output for the class of interest with respect to the
input.
Input × Gradient
Extends VG by performing element-wise multiplication on the gradient and input features.
Integrated Gradients (IG) (Sundararajan
et al., 2017)
Interpolates between a baseline example and the input, accumulating gradients at each step."
INTRODUCTION,0.02586206896551724,"GradCAM (Selvaraju et al., 2017)
Computes the gradient of the model’s output for the class of interest with respect to the last
convolutional layer of a CNN.
SmoothGrad (Smilkov et al., 2017)
Averages over saliency results for slightly perturbed/noisy versions of the input.
Guided Backpropagation (Guided BP)
(Springenberg et al., 2014)
Extends gradient-based methods by preventing the ﬂow of negative gradients, resulting in
attributions from positive paths in the network.
Local Interpretable Model-Agnostic Ex-
planations (LIME) (Ribeiro et al., 2016)
Trains a simple surrogate model on a localized dataset generated by perturbing the input
example, and uses its coefﬁcients as importance values.
Shapley Additive Explanations (SHAP)
(Lundberg & Lee, 2017)
Computes attribution as a game theory problem where each feature is a “player” and the
output of the model is a “payout” distributed amongst them.
Meaningful Perturbations (MP) (Fong &
Vedaldi, 2017)
Strategically masks different parts of the input to learn which are important (i.e., lead to the
largest change in the output).
Sufﬁcient Input Subsets (SIS) (Carter
et al., 2019)
Uses instance-wise backward selection to identify subsets of input features sufﬁcient for
the model to make its prediction above some conﬁdence threshold.
RISE (Petsiuk et al., 2018)
Computes the weighted sum of the model’s output on masked versions of the input."
INTRODUCTION,0.028735632183908046,"XRAI (Kapishnikov et al., 2019)
Identiﬁes important image regions by segmenting the image into many overlapping regions
and ranking the regions based on the sum of pixel-attribution within each region."
INTRODUCTION,0.031609195402298854,Table 1: Summary of saliency methods discussed throughout the framework.
INTRODUCTION,0.034482758620689655,"cards” (akin to “model cards” (Mitchell et al., 2019) and “datasheets” (Gebru et al., 2018)) to doc-
ument individual methods and to better contextualize saliency results. Moreover, using a concrete
example of ML-based radiology diagnostic systems, we show how downstream stakeholders can
use our framework to weigh tradeoffs and choose a task-appropriate method. Finally, we show how
our framework identiﬁes compelling opportunities for future work including exploring understudied
dimensions and developing new metrics that target particular dimensions."
INTRODUCTION,0.03735632183908046,"2
RELATED WORK: SALIENCY METHODS AND THEIR EVALUATIONS"
INTRODUCTION,0.040229885057471264,"Saliency methods (sometimes referred to as feature attribution methods) produce explanations for an
ML model’s output. Given an input, saliency methods compute an importance score for each input
feature describing its inﬂuence on the model’s output. Existing categorizations for saliency methods
have focused on algorithmic properties (gradient or perturbation-based, path-attribution or gradient-
only) Molnar (2019). Our framework aims to categorize a broader range of important characteristics.
We apply our framework to a variety of common saliency methods listed in Table 1."
INTRODUCTION,0.04310344827586207,"Evaluations of saliency methods have primarily focused on how accurately their results represent
model behavior, often referred to as faithfulness. A growing body of work has identiﬁed failures
of some methods such as susceptibility to adversarial perturbations (Ghorbani et al., 2019), lack of
neuron discriminativity (Mahendran & Vedaldi, 2016), and a predisposition towards input recov-
ery (Nie et al., 2018; Adebayo et al., 2018). Other work has proposed proxy tests that measure
different aspects of faithfulness. Adebayo et al. (2018) recommend model randomization and data
label randomization tests, and Kindermans et al. (2019) test whether constant input shifts affect
saliency results. Samek et al. (2016) judge saliency methods by iteratively replacing features that
have high importance values with random noise and measuring how much the output changes. While
these tests quantitatively analyze saliency methods, Tomsett et al. (2020) found they can produce in-
consistent rankings."
INTRODUCTION,0.04597701149425287,"An alternate line of work has looked to break faithfulness down into measurable axioms. Sundarara-
jan et al. (2017) propose implementation invariance (a saliency method should produce the same
output on functionally equivalent models), sensitivity (a saliency method should give importance
to a feature if and only if changing it leads to a different output), and linearity (if a model is the
composition of two sub-models, the saliency method’s output for the model should be the weighted"
INTRODUCTION,0.04885057471264368,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05172413793103448,"**_
0/""1 ( 
!**
+*$.*)$)""`"
INTRODUCTION,0.05459770114942529,"4)6')48-&-0-8=
1MRMQEPMX]
4IVGITXYEP'SVVIWTSRHIRGI
7IQERXMG(MVIGXRIWW"
INTRODUCTION,0.05747126436781609,"1)8,3(303+=
(IXIVQMRMWQ
,]TIVTEVEQIXIV(ITIRHIRGI
1SHIP%KRSWXMGMWQ
'SQTYXEXMSREP)ƽGMIRG]"
INTRODUCTION,0.0603448275862069,"7)27-8-:-8=
-RTYX7IRWMXMZMX]
1SHIP7IRWMXMZMX]"
INTRODUCTION,0.06321839080459771,1HJDWLYH5HYLHZ
INTRODUCTION,0.06609195402298851,"**_0/""1 
( !**
+*$.*)$)""`"
INTRODUCTION,0.06896551724137931,"""**
0/
""1 
( 
!**
+*$.*)$)"""
INTRODUCTION,0.07183908045977011,7%0-)2'=398498
INTRODUCTION,0.07471264367816093,"g6`7
6`6
6`6
6`6
6`9
6`;"
INTRODUCTION,0.07758620689655173,:MWYEPM^I -2498
INTRODUCTION,0.08045977011494253,)\XVEGX
INTRODUCTION,0.08333333333333333,",91%2
7%0-)2'=1%4
7%0-)2'=1)8,3("
INTRODUCTION,0.08620689655172414,$OJRULWKP
INTRODUCTION,0.08908045977011494,"-RXIVTVIX
'SQTYXI 13()0"
INTRODUCTION,0.09195402298850575,"Figure 1: Our framework offers nine dimensions to characterize and compare saliency methods,
which are grouped into three categories corresponding to different phases of the interpretation pro-
cess: methodology describes how the saliency is calculated; sensitivity describes relationships be-
tween input and output; and perceptibility relates to human interpretation."
INTRODUCTION,0.09482758620689655,"sum of its outputs for each sub-model). Sundararajan et al. (2017) and Shrikumar et al. (2017) both
posit a saliency method should exhibit completeness — a saliency method’s attributions should sum
to the difference between the model’s output on the input and the model’s output on a neutral input.
Shrikumar et al. (2017) and Montavon et al. (2018) claim saliency methods should output a contin-
uous function. Montavon et al. (2018) also propose the axiom of selectivity — a saliency method
should distribute importance to features that have the greatest impact on the model’s output. Finally,
Kindermans et al. (2019) state that a saliency method should be invariant to constant transforms."
INTRODUCTION,0.09770114942528736,"While axioms are presented as constraints that all saliency methods should attain, the axes in our
framework describe attributes of saliency methods that can be traded off to select the best method
for different use cases. To situate saliency methods along each axis, we utilize existing tests (Kinder-
mans et al., 2019; Adebayo et al., 2018); however, our framework surfaces the need for additional
evaluations, since existing tests do not fully describe each axis and have not assessed all methods."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.10057471264367816,"3
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.10344827586206896,"To distill a language to characterize and compare saliency methods, we treat saliency methods as
abstractions of the underlying model behavior. That is, to explain model behavior in a human-
understandable format (e.g., a feature attribution heatmap), we must necessarily abstract away some
detail and concreteness. A saliency method, by design, is not as precise a reﬂection of model be-
havior as a printout of the weights, but it is much more human-interpretable. The idea of abstraction
recurs in other parts of the machine learning pipeline. For example, “test accuracy” is an abstrac-
tion of model performance: it does not capture all aspects of model performance but is nevertheless
a convenient approximation. Abstractions selectively preserve information and can be combined
to arrive at a more complete picture of the underlying phenomenon — i.e., combining test accu-
racy with AUROC to more fully understand model performance. This framing reveals that a single
saliency method cannot fully explain model behavior. Instead, it is necessary to understand different
methods’ strengths and limitations to choose an appropriate method for the model, domain, and task."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.10632183908045977,"Our framework deﬁnes nine dimensions that describe what information saliency methods abstract,
and how this abstraction is computed. We group these dimensions into three categories that map
to different stages of the interpretability process (see Figure 1). Methodology covers the model,
input, and saliency method, and dimensions under this category describe how the saliency method
operates. Sensitivity goes one stage further, with dimensions that describe the relationship between
the saliency output and the model or input. Perceptibility covers the ﬁnal stages of the process, with
dimensions that describe how users perceive the saliency output and any associated visualizations.
These dimensions allow us to decompose faithfulness into aspects that can be reasoned about in-
dividually. For instance, Figure 2 demonstrates how our dimensions allow us to situate saliency
methods in relation to one another to understand their strengths, limitations, and differences. In the
following subsections, we describe and provide an illustrative example of each dimension."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.10919540229885058,Under review as a conference paper at ICLR 2022
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.11206896551724138,deterministic
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.11494252873563218,"10e-2s for 2242243 image
10e-1s for 2242243 image
10e0s for 2242243 image"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.11781609195402298,"deterministic (except if 
using random baseline)
non-deterministic coalitions"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1206896551724138,"input sensitivity depends 
on baseline value"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1235632183908046,"regularization on 
linear model
no minimality incentives; 
known to appear noisy"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.12643678160919541,"estimating global importance avoids 
gradient saturation and alleviates noise"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.12931034482758622,"input sensitive, according to constant 
shift test from Kindermans et al."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.13218390804597702,"less
more"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.13505747126436782,"less
more"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.13793103448275862,"less
more"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.14080459770114942,"less
more"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.14367816091954022,"IG
SHAP
VG"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.14655172413793102,"VG
SHAP"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.14942528735632185,"VG
SHAP IG"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.15229885057471265,"Computational 
Efficiency: Speed of 
computation."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.15517241379310345,"Determinism:
Consistent 
reproduction of results."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.15804597701149425,"Input Sensitivity:
Reaction to meaningful 
input changes."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.16091954022988506,"Minimality:
Identification of a 
minimal feature set. VG IG IG"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.16379310344827586,"Figure 2: Situating methods along dimensions of our framework reveals their multifaceted differ-
ences (e.g., SHAP is more minimal but less tractable than VG). Understanding these differences can
inform context-speciﬁc decisions about how to choose and interpret methods appropriately. It also
reveals gaps (e.g., SHAP has not been tested for input sensitivity) that can inform future work."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.16666666666666666,"3.1
METHODOLOGY: HOW SALIENCY METHODS OPERATE"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.16954022988505746,"Determinism
Some saliency techniques are non-deterministic, and running them with different
random seeds can produce signiﬁcantly different outputs. For instance, methods such as RISE
or SHAP rely on randomly-generated masks or coalitions. The output from a non-deterministic
method represents only one example from a potentially high-variance distribution, and may be af-
forded authority that does not account for its inherent uncertainty. Although we may never explicitly
choose a method for its non-determinism, we may nevertheless accept non-determinism in service
of other dimensions such as increased model agnosticism (e.g., RISE), perceptual correspondence
(e.g., SHAP), or minimality (e.g., MP)."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1724137931034483,"Example. Consider a model assisting dermatologists diagnosing skin cancer. In Figure 3(a), we
show that two runs of a non-deterministic method (LIME) on a melanoma prediction model result
in different saliency maps. These variations may have signiﬁcant consequences given small areas of
the lesion or surrounding skin may be integral to the diagnosis. Visualizing and interpreting multiple
runs together is time-consuming and confusing, but only looking at one run could skew a clinician’s
judgment. Thus, in this case, it might make sense to prioritize a method with deterministic outputs."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1752873563218391,"Hyperparameter Dependence
Hyperparameter dependence captures how many hyperparameters
or design decisions the user must specify to run a particular method, and how sensitive it is to
them. If there are not sufﬁcient resources or expertise to devote to hyperparameter optimization,
simply using default values can lead to misleading results. Similarly, misleading or confusing results
might arise if the hyperparameters were chosen based on a particular dataset but deployed in a
setting in which there is a signiﬁcant distribution shift. In situations like this, it makes sense to
prioritize methods with low hyperparameter dependence. In other cases, where developers have
dedicated appropriate time and resources to tuning, it might be preferable to use a method dependent
on hyperparameters (e.g., IG, SmoothGrad) because it improves other dimensions (e.g., minimality)."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1781609195402299,"Example. IG computes feature importances by interpolating between a “meaningless” baseline input
and the actual input, accumulating the gradients at each step. A common practice is to use a base-
line value of all zeroes; however, in some cases, a zero baseline can be misleading and potentially
harmful. Take a model trained to predict bone fractures from x-ray images. Fractures often appear
as a black line in the bone, but, since they have the same pixel value as the baseline (zero), the IG
result will indicate that they are not important. In Figure 3(b), we show a similar example where the
choice of baseline (black, white) has a strong effect on the saliency map. Choosing an appropriate
baseline requires time and a deep understanding of the data and method; if this is not available in a
particular application, it is better to prioritize a saliency method without important hyperparameters."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1810344827586207,"Model Agnosticism
Model agnostic methods (i.e., SHAP) treat the underlying model as a black-
box, relying only on its input and output. On the other hand, model-dependent methods need ac-
cess to various model internals, and may have speciﬁc requirements, such as differentiability (i.e.,"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1839080459770115,Under review as a conference paper at ICLR 2022
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1867816091954023,(i) Semantic Directness
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1896551724137931,English Explanations of Saliency Maps
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.1925287356321839,"“The minimum set of 
pixels necessary for 
the model to make 
its original decision 
with 85% 
conﬁdence.”"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.19540229885057472,"LIME
“The important features 
learned by a surrogate 
model trained to mimic 
the original model’s 
decision boundary on 
this input.” SIS"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.19827586206896552,(d) Computational Eﬃciency
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.20114942528735633,Run Time for Saliency Methods on ImageNet Image
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.20402298850574713,(a) Determinism
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.20689655172413793,LIME with Different Random Seeds
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.20977011494252873,"Image
First Output
Second Output"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.21264367816091953,(b) Hyperparameter Dependence
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.21551724137931033,IG with Different Baselines
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.21839080459770116,"Image
Black Baseline
White Baseline"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.22126436781609196,(c) Model Agnosticism
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.22413793103448276,LIME Applied to Different Model Architectures
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.22701149425287356,"Image
Random Forest
CNN"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.22988505747126436,(e) Input Sensitivity
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.23275862068965517,GradCAM on Adversarial Attack (from Jia et al. 2020)
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.23563218390804597,"Original 
Predicted: 
salt shaker"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.23850574712643677,"Attacked 
Predicted:"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2413793103448276,water jug
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2442528735632184,GradCAM
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2471264367816092,"on the 
Original"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.25,GradCAM
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.25287356321839083,"on the 
Attacked"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2557471264367816,(f) Model Sensitivity
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.25862068965517243,Smoothed VG Over Model Randomization
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2614942528735632,"Fully Trained
Fully Random"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.26436781609195403,(g) Minimality
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2672413793103448,Gradients Before and After Smoothing
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.27011494252873564,"Image
VG
VG + SmoothGrad"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.27298850574712646,(h) Perceptual Correspondence
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.27586206896551724,Randomly Initialized Model
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.27873563218390807,"Image
Vanilla Grad.
Integrated Grad."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.28160919540229884,"Figure 3: Our framework deﬁnes nine dimensions of saliency methods, shown above with examples.
Each dimension describes an attribute of saliency methods that can be reasoned about and traded off."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.28448275862068967,"gradient-based methods) or a particular type of architecture (i.e., GradCAM requires a CNN). Model
agnosticism is a priority for use cases when modularity is required, when ensembling models, when
comparing across model architectures, or when the model internals are not accessible. However, if
only considering a speciﬁc architecture, one might opt for a non-model-agnostic method like Grad-
CAM that provides other beneﬁts (i.e., increased model sensitivity)."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.28735632183908044,"Example. Saliency methods can be an informative tool to compare the behavior of different models
on the same input. For a direct comparison, the same saliency method should be used across models,
and a model agnostic method is likely necessary. For example, in Figure 3(c), we show how a model-
agnostic method like LIME can produce explanations for a CNN and a random forest model, which
would not be possible using a model-dependent method such as IG or GradCAM."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.29022988505747127,"Computational Efﬁciency
Different methods vary widely in their computational efﬁciency —
for example, perturbation-based methods (e.g., SIS, MP) are often more computationally intensive
than simple gradient-based methods (e.g., VG and its variants). In a time-sensitive, low-resource,
or data-intensive setting (e.g., all of ImageNet), prioritizing computational simplicity (which may
entail sacriﬁcing along other axes) may be necessary."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.29310344827586204,"Example. Video modeling (i.e., action recognition) is computationally-intensive because, unlike 3-
dimensional image inputs (channels x height x width), video inputs add a fourth dimension (frames).
Even though the original frame rate can be subsampled, the compute time required for frame-wise
saliency maps scales linearly with video length. As shown in Figure 3(d), methods like Guided BP
are orders of magnitude faster than other methods like XRAI. Thus, in the video modeling setting,
choosing a computationally efﬁcient saliency method may be necessary to interpret model behavior."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2959770114942529,"3.2
SENSITIVITY: RELATIONSHIPS BETWEEN SALIENCY OUTPUT AND THE MODEL/INPUT"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.2988505747126437,"Input Sensitivity
Input sensitivity captures the concept that the result of a saliency method should
reﬂect the model’s sensitivity to transformations in input space. Prior work has attempted to quantify
the input sensitivity of saliency methods by comparing results before and after applying a constant
shift to the input (Kindermans et al., 2019) (i.e., positing that an input shift that does not change the
model output should also not change the saliency map if the method is input sensitive). Other work
deﬁnes the property such that if two inputs differ by only one feature but result in different model
outputs, an input-sensitive saliency method should assign a non-zero value to that feature. In some
cases, we might tradeoff input sensitivity for, e.g., minimality (as in SmoothGrad)."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3017241379310345,Under review as a conference paper at ICLR 2022
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3045977011494253,"Example. Input sensitivity is particularly critical for applications that risk adversarial attacks or when
studying model behavior in the presence of these attacks. For example, Adv-watermark (Jia et al.,
2020) adds watermarks to images that change the model’s prediction. In Figure 3(e), we show an
example from Jia et al. (2020) where the model correctly predicts salt shaker on the original image
but, after the addition of a watermark, predicts water jug. The GradCAM saliency is input sensitive
and identiﬁes the adversarial watermark as most important to the incorrect water jug prediction."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3074712643678161,"Model Sensitivity
Model sensitivity is the concept that the result of a saliency method should
reﬂect changes in the model parameters. Prior work has formalized notions of model sensitivity in
different ways: for example, Adebayo et al. (2018) run a parameter randomization test (comparing
the output of a saliency method on the trained model and a randomly-initialized model), and Sun-
dararajan et al. (2017) deﬁne an implementation invariance axiom (stating that a saliency method
should produce consistent outputs on functionally equivalent models). In some cases, we might be
okay with trading-off model sensitivity. For example, Guided BP is not model sensitive, but is input
sensitive; if we cared more about examining changes across different inputs (as opposed to different
models), we might choose it over a more model sensitive method."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3103448275862069,"Example. Prioritizing model sensitivity may be particularly important when comparing models. In
particular, take the case where one compares a model to its ﬁne-tuned counterpart to understand the
effect on the model’s behavior. In this case, it is critical to have a method with model sensitivity;
otherwise, the results might misleadingly indicate that the model has not changed. In Figure 3(f),
we use VG, a model-sensitive method, showing that it is sensitive to cascading layer randomization."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3132183908045977,"3.3
PERCEPTIBILITY: HUMAN INTERPRETATION OF SALIENCY OUTPUT"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3160919540229885,"Minimality
Minimality refers to how many features are given a signiﬁcant value in the saliency
map. Methods that attribute importance to many input features, especially when applied to high-
dimensional data, can produce results that are difﬁcult to interpret. For example, several papers have
observed that VG results appear noisy, sometimes giving almost every pixel in an image a non-zero
importance. On the other hand, methods like LIME and SIS directly incorporate minimality into
their procedures using regularization and backward selection, respectively. Smoothing methods,
such as SmoothGrad, also improve minimality by averaging over the results on slight perturbations
of the input. At the same time, methods that do not prioritize minimality may have other beneﬁts
(e.g., VG are model sensitive) and might be more appropriate depending on the context."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.31896551724137934,"Example. Minimality is particularly important when interpreting high-dimensional data. For exam-
ple, when dealing with patient medical records, each record might contain hundreds or thousands of
features. Without minimality, the resulting saliency values risk obscuring whatever signal is present
and increasing the cognitive load required to interpret the results. In Figure 3(g), we compare results
of a saliency method that are not minimal (VG) before and after smoothing (SmoothGrad) is applied,
demonstrating that minimality makes it signiﬁcantly easier to glance at and interpret results."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3218390804597701,"Perceptual Correspondence
Perceptual correspondence captures the idea that the perceived sig-
nal in the saliency result should reﬂect the model’s conﬁdence. For example, if a model is randomly
guessing predictions, saliency values should appear correspondingly random. Perceptual correspon-
dence is crucial for high-risk settings, where a misleading signal could provide an unwarranted
justiﬁcation for essential decisions or lead users down incorrect paths. On the other hand, if saliency
results are not interpreted individually for decision-making, but rather being used for larger-scale
analyses of model behavior that will aggregate one-off artifacts (e.g., Boggust et al. (2021) use ag-
gregate metrics on saliency results across an entire dataset to measure human-AI alignment), we
might prioritize low computational efﬁciency over perceptual correspondence."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.32471264367816094,"Example. In Figure 3(h), we show saliency maps on a randomly initialized model. While VG
appears random, IG computed with a black baseline exhibits a perceptual signal because of the near-
black regions in the images. A user might project meaning onto the saliency map, concluding
that the model is picking up on signals in the input when in fact it is entirely random. While
we use an ImageNet image (Deng et al., 2009) to illustrate the concept, the lack of perceptual
correspondence in IG would be particularly problematic in black and white medical images (i.e.,
x-rays, mammograms) where saliency maps might indiscriminately highlight white areas, providing
unwarranted justiﬁcation for further action or treatments."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3275862068965517,Under review as a conference paper at ICLR 2022 
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.33045977011494254,"Methodology
Computational Eﬃciency: ~10e-1s for a 
224x224x3 image, ResNet50, and a NVidia 
G100.
Determinism: Deterministic unless using 
non-deterministic baseline.
Hyperparameter Dependence: Requires 
selection of a neutral baseline [3]. 
Model Agnosticism: Requires differentiable 
model with access to gradients."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3333333333333333,"Methodology
Computational Eﬃciency: ~10e-2s for a 
224x224x3 img, ResNet50, and NVidia 
G100.
Determinism: Deterministic.
Hyperparameter Dependence: None.
Model Agnosticism: Requires differentiable 
model with access to gradients."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.33620689655172414,"Method Details
Overview: Integral of the gradient function 
between the input and a neutral baseline. 
Robust to saturated gradients.
Developers: Researchers at Google. 
Reference: Sundararajan, et al. ""Axiomatic 
Attribution for Deep Networks"" (2017)"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3390804597701149,"Sensitivity
Input Sensitivity: Reaction to constant shift 
[1] is contingent on baseline value.
Model Sensitivity: Output changes as 
model parameters are randomized [2]."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.34195402298850575,"Perceptibility
Minimality: Alleviates noise caused by 
gradient saturation.
Perceptual Correspondence: Not affected 
by gradient saturation. Features equal to 
the baseline value will have zero 
importance. Predisposition to input 
recovery [2].
Semantic Directness: Accumulation of 
gradients from a baseline to the input."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3448275862068966,"Perceptibility
Minimality: No minimality incentives. May 
be visually noisy [3] and have many 
features with non-zero importance [4].
Perceptual Correspondence: Saturated 
gradients can mask signal from important 
features [3]. 
Semantic Directness: The magnitude of 
change in the output given a small change 
to the input feature."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.34770114942528735,"Method Details
Overview: Gradient of the loss function for 
the class of interest with respect to the 
input features.
Developers: University of Oxford.
Reference: Simonyan, et al. ""Deep Inside 
Convolutional Networks: Visualising Image 
Classiﬁcation Models and Saliency Maps"" 
(2014)"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3505747126436782,"Sensitivity
Input Sensitivity: Output stays constant 
after a constant shift. [1]
Model Sensitivity: Output changes as 
model parameters are randomized  [2]."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.35344827586206895,Saliency Card for Vanilla Gradients
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3563218390804598,"[1] Kindermans, et al. ""The (Un)reliability of 
Saliency Methods"" (2017)
[2] Adebayo, et al. ""Sanity Checks for 
Saliency Maps"" (2018) 
[3] Smilkov, et al. ""Smoothgrad: Removing 
Noise by Adding Noise"" (2017)
[4] Kim, et al. ""Why are Saliency Maps 
Noisy? Cause of and Solution to Noisy 
Saliency Maps"" (2019)"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.35919540229885055,VG: otterhound
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3620689655172414,"more
Computational Eﬃciency
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3649425287356322,"Hyperparameter Dependencemore
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.367816091954023,"Model Agnosticism
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3706896551724138,"more
Input Sensitivity
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3735632183908046,"Model Sensitivity
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3764367816091954,"Minimality
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3793103448275862,"more
Perceptual Correspondence
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.382183908045977,"Semantic Directness
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3850574712643678,"Determinism
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3879310344827586,Saliency Card for Integrated Gradients
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.39080459770114945,"[1] Kindermans, et al. ""The (Un)reliability 
of Saliency Methods"" (2017)
[2] Adebayo, et al. ""Sanity Checks for 
Saliency Maps"" (2018) 
[3] Sturmfels, et al. “Visualizing the 
Impact of Feature Attribution Baselines"" 
(2020)"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3936781609195402,IG: cab (random baseline)
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.39655172413793105,"more
Computational Eﬃciency
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.3994252873563218,"Hyperparameter Dependencemore
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.40229885057471265,"Model Agnosticism
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.4051724137931034,"more
Input Sensitivity
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.40804597701149425,"Model Sensitivity
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.4109195402298851,"Minimality
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.41379310344827586,"more
Perceptual Correspondence
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.4166666666666667,"Semantic Directness
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.41954022988505746,"Determinism
more
less"
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.4224137931034483,"Figure 4: Using our framework, individual methods can be documented in a standardized manner as
“saliency cards,” informing users about tradeoffs and limitations, and facilitating rapid comparison."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.42528735632183906,"Semantic Directness
Saliency methods represent different aspects of model behavior. For exam-
ple, the result from SIS represents minimal sets of pixels necessary for a particular prediction; the
result from LIME, on the other hand, represents the learned coefﬁcients of a local surrogate model.
Semantic directness refers to how straightforward it is for a particular user group to understand what
the result of a saliency method represents. A semantically direct method may be particularly im-
portant if users are unfamiliar with ML or saliency methods, in order to prevent misinterpretation
of results. In other cases, we might trade it off — for example, while SHAP (which deﬁnes feature
importances as Shapley values, a method from game theory) may not be semantically direct to many
users, it can improve perceptual correspondence, which might be more critical in high-risk contexts."
NINE DIMENSIONS TO CHARACTERIZE & COMPARE SALIENCY METHODS,0.4281609195402299,"Example. Recently, a range of public-facing browser-based interpretability tools have been devel-
oped, such as the Language Interpretability Tool (LIT) (Tenney et al., 2020). These tools might be
helpful for data scientists or other users in a particular domain, who don’t necessarily have formal
ML expertise, to explore their data and a model’s behavior on it. Within LIT, users can choose
among several methods for viewing the saliency results of a model on selected sentences. In this
case, a non-expert user might opt for a more semantically direct method such as SIS (over, e.g., IG,
SHAP, or LIME) that facilitates more intuitive and quick exploratory analyses without needing to
understand the implications of a surrogate model or accumulated gradients."
FRAMEWORK USE CASES,0.43103448275862066,"4
FRAMEWORK USE CASES"
FRAMEWORK USE CASES,0.4339080459770115,"4.1
DOCUMENTING SALIENCY METHODS WITH “SALIENCY CARDS”"
FRAMEWORK USE CASES,0.4367816091954023,"By moving beyond a singular notion of faithfulness, our framework gives us a rich, granular vocab-
ulary to describe existing and future methods, and situate them in relation to one another. In doing
so, our framework improves our understanding of different methods and can be used to document
them in a standardized fashion. For example, in the same way “model cards” and “datasheets” pro-
vide detailed, public-facing documentation about particular models or datasets, our dimensions can
inform an analogous “saliency card” as shown in Figure 4. Such cards step through the individual
dimensions of the framework, providing a brief qualitative or quantitative description for each. By
including an example of the method in action, as well as a schematic ﬁgure that plots the method"
FRAMEWORK USE CASES,0.4396551724137931,Under review as a conference paper at ICLR 2022
FRAMEWORK USE CASES,0.4425287356321839,"with respect to each dimension, these saliency cards can help better contextualize individual meth-
ods in the broader landscape. Downstream users can rapidly compare cards to understand a method’s
strengths and limitations, as well as build intuition for the trade offs between different methods."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.4454022988505747,"4.2
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING"
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.4482758620689655,"The dimensions of our framework let us systematically evaluate, choose, and compose methods for
different contexts. To illustrate this use case, we consider an application of ML-based diagnostic
tools: interpreting chest x-rays. Radiologists interpret chest x-rays to diagnose a range of cardiopul-
monary conditions, and automated analysis could potentially improve their workﬂows and extend
expertise to low-resource settings. ML-based systems have gained substantial traction for this appli-
cation, in part because of the availability of large, public datasets (Johnson et al., 2019; Irvin et al.,
2019). In practice, accompanying automated decisions with explanations is critical, both for the
systems to be usable and trusted by physicians (Tonekaboni et al., 2019) and to follow standards and
laws for explainability (Smith, 2020; European Commission, 2018)."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.4511494252873563,"We evaluate our framework in the context of this case study through semi-structured interviews with
4 radiologists (see Section A.1 for interview details) to understand how our framework surfaces
trade offs and helps radiologists compare different saliency methods. We ﬁnd radiologists may be
unfamiliar with the the way non-determinism manifests in saliency methods (i.e., LIME, SHAP),
given that it is typically not encountered within other medical technologies. Non-deterministic re-
sults could appear random and lead to speculation about whether an important feature would still
be important if they reran the method. Given this, we may want to prioritize determinism over
other axes and choose a method, like VG, that is fully deterministic. While VG has less minimality
than many non-deterministic methods, its noisiness may be preferable to the stochasticity of a non-
deterministic method — background noise is a familiar concept in medical imaging and radiologists
are used to accounting for it during interpretation."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.4540229885057471,"Relatedly, minimality is often presented as a desirable attribute (Smilkov et al., 2017). Radiologists,
however, are accustomed to using background noise to attenuate measurement uncertainty in their
work; thus, the absence of background noise could convey unwarranted certainty. Indeed, results in
visualization research indicate that clean, minimalist visual representations can convey a misplaced
sense of certitude about the underlying data (Kennedy et al., 2016; Kostelnick, 2008). This con-
sideration also suggests another tradeoff — between perceptual correspondence and minimality. If
the saliency result misleadingly communicates a signal to which a radiologist attributes meaning, it
could lead to a patient being misdiagnosed or wrongly treated. Given the high risk involved with
incorrect decisions in a medical context, we might actually opt for a less minimal method — despite
the fact that its results may not be as glanceable — in order to increase perceptual correspondence."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.45689655172413796,"Semantic directness is similarly presented as an beneﬁcial characteristic for methods used with non-
ML-experts (Carter et al., 2019). Radiologists, however, are accustomed to reading and interpreting
scans produced via magnetic resonance imaging (MRI) technology despite having a limited tech-
nical understanding of the physics underlying MRI technology. A similar ethos likely applies to
saliency methods as well — radiologists might be comfortable receiving saliency results even with-
out understanding the underlying mechanisms (e.g., gradients, neural network architectures) as long
as they know how to use them."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.45977011494252873,"The deployment process for a clinical decision model might also have implications on which di-
mensions to prioritize. For example, IG’s hyperparameter dependence could have signiﬁcant conse-
quences for x-ray data (e.g., the commonly used black baseline would lead to misleading results for
x-ray images). This dependence might be more or less of an issue depending on the expertise of the
person tuning the hyperparameters. If tuning was left to the software vendor, for example, we might
prioritize a method that does not require consequential decisions like IG’s baseline value."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.46264367816091956,"Overall, the dimensions of our framework provide valuable structure for probing an application
and reasoning about tradeoffs. While previously, we might have picked a method to use more
arbitrarily, we can now systematically think about attributes to target. For example, non-determinism
and perceptual correspondence are likely necessary for a radiology context, while minimality and
semantic directness are not. Based on this, we can see how a method like VG might be better suited
than LIME for this particular task. Beyond speciﬁc methods, we can gain insight into general design
implications, such as whether hyperparameters should be set beforehand or open to user choice."
COMPARING AND CHOOSING SALIENCY METHODS IN A RADIOLOGY SETTING,0.46551724137931033,Under review as a conference paper at ICLR 2022
IDENTIFYING NEW RESEARCH DIRECTIONS,0.46839080459770116,"4.3
IDENTIFYING NEW RESEARCH DIRECTIONS"
IDENTIFYING NEW RESEARCH DIRECTIONS,0.47126436781609193,"Our framework also allows us to systematically analyze the research landscape. We characterize
10 methods along each of our framework’s dimensions (Table A1) by drawing on existing work
(i.e., papers that present or compare methods) and our experiments (i.e., running methods locally to
compare their computational burden). In doing so, we identify several patterns and gaps in existing
methods that suggest promising directions for future work to explore."
IDENTIFYING NEW RESEARCH DIRECTIONS,0.47413793103448276,"Some dimensions are rarely explored in existing work. For example, while it may be a crucial at-
tribute for high-risk applications, few papers attempt to measure perceptual correspondence. This
gap might be due to measurement difﬁculty, since research characterizing and quantifying perceptual
correspondence requires conducting user studies. Such inquiries could draw from work on human
visual perception (Logothetis & Sheinberg, 1996) — for example, using eye-tracking to measure
if and where in an image people ﬁxate (Borys & Plechawska-W´ojcik, 2017; Yarbus, 2013) — to
designing standardized ways of studying when people read strong signal in saliency map visualiza-
tions. A better understanding of perceptual correspondence could also inform the design of saliency
visualizations that explicitly communicate limitations of the method and preemptively avoid imply-
ing unwarranted signal. For instance, we can imagine a richer design space beyond static heatmaps
where interaction is used to dynamically overlay multiple attributions (as in Olah et al. (2018)) and
more intuitively communicate uncertainty."
IDENTIFYING NEW RESEARCH DIRECTIONS,0.47701149425287354,"In other cases, existing work has already begun to devise evaluative metrics that map to dimensions
in our framework. For example, Adebayo et al. (2018) use a model parameter randomization test
that helps gauge model sensitivity, and Kindermans et al. (2019) use a test that compares saliency
results before and after applying a constant shift that helps measure input sensitivity. However, these
are speciﬁc, narrow ways of measuring model and input sensitivity, and can produce inconsistent
results (Tomsett et al., 2020). By explicitly identifying these properties as separate dimensions,
there is signiﬁcant opportunity to devise new or different ways to quantify them. For example, while
Kindermans et al.’s quantiﬁcation of input sensitivity involves applying a meaningless change to the
input, we could imagine applying a meaningful change to the input and studying how the output
changes in response (akin to algebraic visualization design by Kindlmann & Scheidegger (2014))."
IDENTIFYING NEW RESEARCH DIRECTIONS,0.47988505747126436,"Once methods are well-characterized across a particular dimension, other patterns might emerge —
for example, clustering within a dimension or combinations of dimensions. We noticed, for instance,
that minimality is typically encouraged via regularization or smoothing terms, and thus the more
minimal methods are typically more hyperparameter dependent. Are these dimensions inherently
tied together, or is it possible to achieve increased minimality in other ways? This is an example of
a research question that emerges from understanding gaps and clusters in the design space."
CONCLUSIONS AND FUTURE WORK,0.4827586206896552,"5
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.48563218390804597,"In this paper, we present a framework for describing, communicating, and comparing saliency meth-
ods. Unlike prior approaches, which have focused on a singular notion of “faithfulness” or how
accurately saliency output reﬂects the underlying model, our framework offers nine dimensions to
characterize and compare saliency methods that span different phases of the interpretation workﬂow.
As a result, our framework facilitates rich documentation of saliency methods (i.e., through saliency
cards), and can help users understand trade offs and select methods that are appropriate for their
data, model, and task. Finally, situating saliency methods along each dimension surfaces areas for
future research including developing methods that prioritize speciﬁc attributes, designing tests to
quantify each dimension, and proving (or disproving) entanglement between dimensions."
CONCLUSIONS AND FUTURE WORK,0.4885057471264368,"We intend our framework to start a conversation around approaches to characterize and compare
saliency methods, and to be a “living artifact.” Namely, as researchers develop new saliency methods
or quantify particular dimensions (e.g., input sensitivity), we expect our framework to evolve. For
instance, additional dimensions may be introduced to distinguish attributes or techniques proposed
by novel methods, or new evaluative studies may reveal that existing dimensions are too broadly
deﬁned and warrant further decomposition into constituent, testable axes. Further work is also
needed to provide a formal (i.e., mathematical) treatment of our framework. Doing so would help
turn our framework from a purely descriptive instrument into a generative one as well — where
dimensions can be operationalized to systematically produce altogether new saliency methods."
CONCLUSIONS AND FUTURE WORK,0.49137931034482757,Under review as a conference paper at ICLR 2022
REFERENCES,0.4942528735632184,REFERENCES
REFERENCES,0.49712643678160917,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292, 2018."
REFERENCES,0.5,"Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Col-
lisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai
interaction. In Proceedings of the 2019 chi conference on human factors in computing systems,
pp. 1–13, 2019."
REFERENCES,0.5028735632183908,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep
Ghosh, Ruchir Puri, Jos´e MF Moura, and Peter Eckersley. Explainable machine learning in de-
ployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,
pp. 648–657, 2020."
REFERENCES,0.5057471264367817,"Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, and Hendrik Strobelt. Shared interest:
Large-scale visual analysis of model behavior by measuring human-ai alignment, 2021."
REFERENCES,0.5086206896551724,"Magdalena Borys and Małgorzata Plechawska-W´ojcik. Eye-tracking metrics in perception and vi-
sual attention research. EJMT, 3:11–23, 2017."
REFERENCES,0.5114942528735632,"Brandon Carter, Jonas Mueller, Siddhartha Jain, and David Gifford. What made you do this? under-
standing black-box decisions with sufﬁcient input subsets. In The 22nd International Conference
on Artiﬁcial Intelligence and Statistics, pp. 567–576. PMLR, 2019."
REFERENCES,0.514367816091954,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.5172413793103449,"Shuoyang Ding and Philipp Koehn. Evaluating saliency methods for neural language models. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pp. 5034–5052, 2021."
REFERENCES,0.5201149425287356,"Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. University of Montreal, 1341(3):1, 2009."
REFERENCES,0.5229885057471264,"European Commission. Transparent information, communication and modalities for the exercise of
the rights of the data subject, 2018. URL https://gdpr-info.eu/art-12-gdpr/."
REFERENCES,0.5258620689655172,"Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429–3437,
2017."
REFERENCES,0.5287356321839081,"Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daum´e III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010,
2018."
REFERENCES,0.5316091954022989,"Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 3681–3688, 2019."
REFERENCES,0.5344827586206896,"Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI
conference on artiﬁcial intelligence, volume 33, pp. 590–597, 2019."
REFERENCES,0.5373563218390804,"Xiaojun Jia, Xingxing Wei, Xiaochun Cao, and Xiaoguang Han. Adv-watermark: A novel wa-
termark perturbation for adversarial examples. In Proceedings of the 28th ACM International
Conference on Multimedia, pp. 1579–1587, 2020."
REFERENCES,0.5402298850574713,"Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lun-
gren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identiﬁed publicly
available database of chest radiographs with free-text reports. Scientiﬁc data, 6(1):1–8, 2019."
REFERENCES,0.5431034482758621,Under review as a conference paper at ICLR 2022
REFERENCES,0.5459770114942529,"Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Vi´egas, and Michael Terry. Xrai: Better attributions
through regions. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 4948–4957, 2019."
REFERENCES,0.5488505747126436,"Helen Kennedy, Rosemary Lucy Hill, Giorgia Aiello, and William Allen. The work that visualisation
conventions do. Information, Communication & Society, 19(6):715–735, 2016."
REFERENCES,0.5517241379310345,"Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch¨utt, Sven
D¨ahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. In Explainable
AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267–280. Springer, 2019."
REFERENCES,0.5545977011494253,"Gordon Kindlmann and Carlos Scheidegger. An algebraic process for visualization design. IEEE
transactions on visualization and computer graphics, 20(12):2181–2190, 2014."
REFERENCES,0.5574712643678161,"Charles Kostelnick. The visual rhetoric of data displays: The conundrum of clarity. IEEE Transac-
tions on Professional Communication, 51(1):116–130, 2008."
REFERENCES,0.5603448275862069,"Xiao-Hui Li, Yuhan Shi, Haoyang Li, Wei Bai, Caleb Chen Cao, and Lei Chen. An experimental
study of quantitative evaluations on saliency methods. KDD ’21, pp. 3200–3208, New York,
NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/
3447548.3467148. URL https://doi.org/10.1145/3447548.3467148."
REFERENCES,0.5632183908045977,"Nikos K Logothetis and David L Sheinberg. Visual object recognition. Annual review of neuro-
science, 19(1):577–621, 1996."
REFERENCES,0.5660919540229885,"Scott Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. arXiv preprint
arXiv:1705.07874, 2017."
REFERENCES,0.5689655172413793,"Aravindh Mahendran and Andrea Vedaldi. Salient deconvolutional networks. In European Confer-
ence on Computer Vision, pp. 120–135. Springer, 2016."
REFERENCES,0.5718390804597702,"Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In
Proceedings of the conference on fairness, accountability, and transparency, pp. 220–229, 2019."
REFERENCES,0.5747126436781609,Christoph Molnar. Interpretable Machine Learning. 2019.
REFERENCES,0.5775862068965517,"Gr´egoire Montavon, Wojciech Samek, and Klaus-Robert M¨uller. Methods for interpreting and un-
derstanding deep neural networks. Digital Signal Processing, 73:1–15, 2018."
REFERENCES,0.5804597701149425,"Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation for perplexing behaviors of
backpropagation-based visualizations. In International Conference on Machine Learning, pp.
3809–3818. PMLR, 2018."
REFERENCES,0.5833333333333334,"Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and
Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018."
REFERENCES,0.5862068965517241,"Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of
black-box models. arXiv preprint arXiv:1806.07421, 2018."
REFERENCES,0.5890804597701149,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why Should I Trust You?”: Explain-
ing the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining, pp. 1135–1144, 2016."
REFERENCES,0.5919540229885057,"Wojciech Samek, Alexander Binder, Gr´egoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660–2673, 2016."
REFERENCES,0.5948275862068966,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626,
2017."
REFERENCES,0.5977011494252874,Under review as a conference paper at ICLR 2022
REFERENCES,0.6005747126436781,"Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145–
3153. PMLR, 2017."
REFERENCES,0.603448275862069,"Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034, 2013."
REFERENCES,0.6063218390804598,"Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017."
REFERENCES,0.6091954022988506,"Andrew
Smith.
Using
artiﬁcial
intelligence
and
algorithms,
Apr
2020.
URL
https://www.ftc.gov/news-events/blogs/business-blog/2020/04/
using-artificial-intelligence-algorithms."
REFERENCES,0.6120689655172413,"Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014."
REFERENCES,0.6149425287356322,"Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, pp. 3319–3328. PMLR, 2017."
REFERENCES,0.617816091954023,"Mukund Sundararajan, Jinhua Xu, Ankur Taly, Rory Sayres, and Amir Najmi. Exploring principled
visualizations for deep network attributions. In IUI Workshops, volume 4, 2019."
REFERENCES,0.6206896551724138,"Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann,
Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, et al.
The language inter-
pretability tool: Extensible, interactive visualizations and analysis for nlp models. arXiv preprint
arXiv:2008.05122, 2020."
REFERENCES,0.6235632183908046,"Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. Sanity
checks for saliency metrics. In Proceedings of the AAAI conference on artiﬁcial intelligence,
volume 34, pp. 6021–6029, 2020."
REFERENCES,0.6264367816091954,"Sana Tonekaboni, Shalmali Joshi, Melissa D McCradden, and Anna Goldenberg. What clinicians
want: contextualizing explainable machine learning for clinical end use. In Machine learning for
healthcare conference, pp. 359–380. PMLR, 2019."
REFERENCES,0.6293103448275862,"Alfred L Yarbus. Eye movements and vision. Springer, 2013."
REFERENCES,0.632183908045977,Under review as a conference paper at ICLR 2022
REFERENCES,0.6350574712643678,"A
APPENDIX"
REFERENCES,0.6379310344827587,"Determinism
Hyperparameter
Dependence
Model Agnosti-
cism
Computational
Efﬁciency
Input Sensitivity
Model Sensitivity
Minimality
Perceptual Corre-
spondence
Semantic Directness"
REFERENCES,0.6408045977011494,"VG
Deterministic.
None.
Requires a differ-
entiable model and
access to gradients."
REFERENCES,0.6436781609195402,"∼10e-2 seconds for
a 224x224x3 image."
REFERENCES,0.646551724137931,"Saliency results
stay constant after a
constant input shift."
REFERENCES,0.6494252873563219,"Saliency results change
as model parameters
are randomized."
REFERENCES,0.6522988505747126,"Results appear noisy if
gradients for important
features are saturated
and normalized within
an example."
REFERENCES,0.6551724137931034,"Saturated gradients
can mask signal from
important features."
REFERENCES,0.6580459770114943,"The magnitude of change
in the output given a
small change in an input
feature."
REFERENCES,0.6609195402298851,"Input X
Gradient
Deterministic.
None.
Requires a differ-
entiable model and
access to gradients."
REFERENCES,0.6637931034482759,"∼10e-2 seconds for
a 224x224x3 image."
REFERENCES,0.6666666666666666,"Saliency results
change after a con-
stant input shift."
REFERENCES,0.6695402298850575,"Saliency results change
as model parameters
are randomized."
REFERENCES,0.6724137931034483,"Results are more mini-
mal than VG since only
features with high gra-
dients and input values
appear important."
REFERENCES,0.6752873563218391,"Predisposition to input
recovery; susceptible to
gradient saturation."
REFERENCES,0.6781609195402298,"The input feature value
weighted by the gradient."
REFERENCES,0.6810344827586207,"IG
Deterministic un-
less using a random
baseline."
REFERENCES,0.6839080459770115,"Baseline value; number
of steps to estimate the
integral."
REFERENCES,0.6867816091954023,"Requires a differ-
entiable model and
access to gradients."
REFERENCES,0.6896551724137931,"∼10e-1 seconds for
a 224x224x3 image."
REFERENCES,0.6925287356321839,"Reaction to constant
shift test dependent
on baseline value."
REFERENCES,0.6954022988505747,"Saliency results change
as model parameters
are randomized."
REFERENCES,0.6982758620689655,"Can alleviate noisiness
from gradient satu-
ration by estimating
global importances."
REFERENCES,0.7011494252873564,"Predisposition to input
recovery (can be exac-
erbated by choice of
baseline). Addresses
gradient saturation
problems."
REFERENCES,0.7040229885057471,"The accumulated gradient
between the baseline
value and and feature
value."
REFERENCES,0.7068965517241379,"GradCAM
Deterministic."
REFERENCES,0.7097701149425287,"Interpolation method
to upsample to the
original feature space.
Choice of convolu-
tional layer (typically
last convolutional
layer)."
REFERENCES,0.7126436781609196,"Requires a differen-
tiable model, access
to gradients, and
convolutional layers."
REFERENCES,0.7155172413793104,"∼10e-1 seconds for
a 224x224x3 image."
REFERENCES,0.7183908045977011,"Saliency results change
as model parameters
are randomized."
REFERENCES,0.7212643678160919,"Interpolating back to
original input size pro-
duces a low-resolution
result."
REFERENCES,0.7241379310344828,"The positive attributions
of the gradient-weighted
feature maps from the
last convolutional layer."
REFERENCES,0.7270114942528736,"SmoothGrad
Non-deterministic
perturbations."
REFERENCES,0.7298850574712644,"Gaussian noise param-
eters; number of runs
to average over."
REFERENCES,0.7327586206896551,"Can be applied to
any saliency method."
REFERENCES,0.735632183908046,"Adds ∼10x
time increase for
224x224x3 image."
REFERENCES,0.7385057471264368,"Reaction to constant
shift test inherited
from underlying
saliency method."
REFERENCES,0.7413793103448276,"Reaction to model
parameter randomiza-
tion inhereted from
underlying saliency
method."
REFERENCES,0.7442528735632183,"Minimality encouraged
by averaging over
multiple results."
REFERENCES,0.7471264367816092,"The average of many
runs of another saliency
method on perturbed
versions of the input."
REFERENCES,0.75,"Guided
Backprop
Deterministic.
None.
Requires a differ-
entiable model and
access to gradients."
REFERENCES,0.7528735632183908,"∼10e-2 seconds for
a 224x224x3 image."
REFERENCES,0.7557471264367817,"Saliency results
stay constant after a
constant input shift."
REFERENCES,0.7586206896551724,"Saliency results do
not change when
higher layer weights
are randomized, and
only change when
lower layer weights are
randomized."
REFERENCES,0.7614942528735632,"Minimality encouraged
through removing
negative gradients."
REFERENCES,0.764367816091954,"Predisposition to input
recovery."
REFERENCES,0.7672413793103449,"The output of another
gradient-based saliency
method (typically VG),
only considering paths
through the network with
positive gradients."
REFERENCES,0.7701149425287356,"LIME
Non-deterministic
perturbations."
REFERENCES,0.7729885057471264,"Linear surrogate mod-
els to search over;
input perturbation pa-
rameters; linear model
parameterization."
REFERENCES,0.7758620689655172,"No requirements on
the model or access
to internals needed."
REFERENCES,0.7787356321839081,"∼10e0 seconds for a
224x224x3 image."
REFERENCES,0.7816091954022989,"Minimality encouraged
through regularization
in local linear model."
REFERENCES,0.7844827586206896,"The positively contribut-
ing features learned by a
surrogate model trained
to mimic the original
model’s local decision
boundary for a particular
input."
REFERENCES,0.7873563218390804,"SHAP
Non-deterministic
coalition sampling."
REFERENCES,0.7902298850574713,"Feature replacement
values; linear model
parameterization; regu-
larization parameter."
REFERENCES,0.7931034482758621,"No requirements on
the model or access
to internals needed."
REFERENCES,0.7959770114942529,"GradSHAP: ∼10e-
2 seconds for a
224x224x3 im-
age. KernelSHAP:
∼10e0 seconds for a
224x224x3 image."
REFERENCES,0.7988505747126436,"Minimality encouraged
through regularization
in local linear model."
REFERENCES,0.8017241379310345,"Features with no im-
pact on model given
zero importance in
result. Features that
impact the model
equally receive equal
attribution."
REFERENCES,0.8045977011494253,"The impact of each input
feature on the output
as deﬁned by Shapley
values."
REFERENCES,0.8074712643678161,"MP
Masks pixels with
non-deterministic
perturbations."
REFERENCES,0.8103448275862069,"Regularization param-
eter; noise and blur
parameters."
REFERENCES,0.8132183908045977,"No requirements on
the model or access
to internals needed."
REFERENCES,0.8160919540229885,"Minimality encouraged
through regularization
on the meta-predictor
that learns explanatory
rules."
REFERENCES,0.8189655172413793,"The minimal region of
the image that would
cause the largest change
if removed. SIS"
REFERENCES,0.8218390804597702,"Deterministicly
produces a set of
explanations per
input."
REFERENCES,0.8247126436781609,"Feature replacement
values; model conﬁ-
dence threshold."
REFERENCES,0.8275862068965517,"No requirements on
the model or access
to internals needed."
REFERENCES,0.8304597701149425,"Algorithm de-
scribed in Carter
et al. (2019) is pro-
hibitively slow on
224x224x3 images."
REFERENCES,0.8333333333333334,"Selects a minimal set
of features sufﬁcient
to meet the conﬁdence
threshold."
REFERENCES,0.8362068965517241,"The minimum set of
pixels necessary for the
model to produce the
output with probability
above a given threshold."
REFERENCES,0.8390804597701149,"RISE
Non-deterministic"
REFERENCES,0.8419540229885057,"mask generation.
Masking value; mask"
REFERENCES,0.8448275862068966,generation parameters.
REFERENCES,0.8477011494252874,No requirements on
REFERENCES,0.8505747126436781,the model or access
REFERENCES,0.853448275862069,to internals needed.
REFERENCES,0.8563218390804598,∼10e-1 seconds for
REFERENCES,0.8591954022988506,a 224x224x3 image.
REFERENCES,0.8620689655172413,"Mask
upsampling"
REFERENCES,0.8649425287356322,encourages continuous
REFERENCES,0.867816091954023,salient regions.
REFERENCES,0.8706896551724138,"The
weighted
sum
of"
REFERENCES,0.8735632183908046,"the
model’s
conﬁdence"
REFERENCES,0.8764367816091954,on masked versions of
REFERENCES,0.8793103448275862,the input for inputs that
REFERENCES,0.882183908045977,include that feature.
REFERENCES,0.8850574712643678,"XRAI
Deterministic."
REFERENCES,0.8879310344827587,Segmentation method;
REFERENCES,0.8908045977011494,attribution method (pa-
REFERENCES,0.8936781609195402,per proposed IG with
REFERENCES,0.896551724137931,black and white base-
REFERENCES,0.8994252873563219,lines).
REFERENCES,0.9022988505747126,"Requires
a
model"
REFERENCES,0.9051724137931034,"with
inputs
whose"
REFERENCES,0.9080459770114943,"features
can
be"
REFERENCES,0.9109195402298851,"meaningfully
clus-"
REFERENCES,0.9137931034482759,"tered together (e.g.,"
REFERENCES,0.9166666666666666,image pixels).
REFERENCES,0.9195402298850575,∼10e1 seconds for a
REFERENCES,0.9224137931034483,"224x224x3 image.
Outputs
region-level"
REFERENCES,0.9252873563218391,importance.
REFERENCES,0.9281609195402298,Region-importance has
REFERENCES,0.9310344827586207,been shown to be more
REFERENCES,0.9339080459770115,"human
interpretable"
REFERENCES,0.9367816091954023,"than pixel-importance.
(Sundararajan et al.,
2019)"
REFERENCES,0.9396551724137931,"The
regions
with
the"
REFERENCES,0.9425287356321839,"largest
sum
of
feature"
REFERENCES,0.9454022988505747,attribution.
REFERENCES,0.9482758620689655,"Table A1: We situate 10 saliency methods along the nine dimensions of our framework, ﬁnding that they capture meaningful and distinct attributes about the
methods. Some dimensions are characterized via our own experiments (e.g., computational efﬁciency) and others draw from tests described in existing work (e.g.,
model and input sensitivity). In this way, our framework lets us systematically analyze the research landscape to understand gaps and opportunities for future work."
REFERENCES,0.9511494252873564,Under review as a conference paper at ICLR 2022
REFERENCES,0.9540229885057471,"A.1
RADIOLOGIST INTERVIEW DETAILS"
REFERENCES,0.9568965517241379,"To evaluate the effectiveness of our framework, we interviewed 4 radiologists and report the results
in Section 4.2. We recruited radiologists via a colleague who is a practicing radiologist at a hospital.
We did not require radiologists to have any familiarity or experience with machine learning. Each
interview lasted approximately 30 minutes. During the interview we discussed dimensions of our
framework and gave clinical examples on CheXpert (Irvin et al., 2019) chest x-rays (see Figure A5).
We asked radiologists open-ended questions with the goal of understanding:"
REFERENCES,0.9597701149425287,• How do radiologists think about individual dimensions of the framework?
REFERENCES,0.9626436781609196,• How do radiologists prioritize and trade off different dimensions?
REFERENCES,0.9655172413793104,• How well does the framework enable effective communication about saliency methods be-
REFERENCES,0.9683908045977011,tween the radiologists and ourselves?
REFERENCES,0.9712643678160919,"Hyperparameter Dependence 
Reliance on human specified parameters."
REFERENCES,0.9741379310344828,"Black Baseline
Chest X-Ray
White Baseline"
REFERENCES,0.9770114942528736,Integrated Gradients
REFERENCES,0.9798850574712644,Pathology: Atelectasis
REFERENCES,0.9827586206896551,Confidence: 99.3%
REFERENCES,0.985632183908046,"Perceptual Correspondence
Visual alignment with numerical meaning."
REFERENCES,0.9885057471264368,"Vanilla Gradients
Chest X-Ray
Integrated Gradients"
REFERENCES,0.9913793103448276,"Randomly Initialized Model
Pathology: Atelectasis"
REFERENCES,0.9942528735632183,Untrained Model
REFERENCES,0.9971264367816092,Figure A5: Chest x-ray examples shown to radiologists in our interviews.
