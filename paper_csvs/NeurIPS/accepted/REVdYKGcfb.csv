Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014534883720930232,"Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL)
have achieved notable success, which is capable of achieving superior performance
across various tasks without requiring additional parameter tuning. However, the
underlying rules for the effectiveness of MM-ICL remain under-explored. To fill
this gap, this work aims to investigate the research question: “What factors affect
the performance of MM-ICL?” To this end, we investigate extensive experiments on
the three core steps of MM-ICL including demonstration retrieval, demonstration
ordering, and prompt construction using 6 vision large language models and 20
strategies. Our findings highlight (1) the necessity of a multi-modal retriever for
demonstration retrieval, (2) the importance of intra-demonstration ordering over
inter-demonstration ordering, and (3) the enhancement of task comprehension
through introductory instructions in prompts. We hope this study can serve as a
foundational guide for optimizing MM-ICL strategies in future research."
INTRODUCTION,0.0029069767441860465,"1
Introduction"
INTRODUCTION,0.00436046511627907,"Recently, Large Language Models (LLMs) have demonstrated remarkable advancements, showcasing
proficiency in a wide range of tasks [Zhao et al., 2023a, Qin et al., 2023, 2024, Hu et al., 2023, Pan
et al., 2023]. Notably, advanced LLMs exhibit the emergence of novel capabilities such as In-Context
Learning (ICL) [Wei et al., 2022a, Dong et al., 2022, Zhuang et al., 2023], which optimize task
performance by incorporating demonstrations into input prompts [Giannou et al., 2023, Li et al.,
2023d, Wies et al., 2023, Zhou et al., 2022]. In particular, multi-modal in-context-learning (MM-ICL)
is capable of utilizing multi-modal demonstrations to quickly adapt to the downstream task without
parameter tuning [Yin et al., 2023, He et al., 2023, Zhang et al., 2024, Li and Lu, 2024]."
INTRODUCTION,0.005813953488372093,"In the literature, a series of works emerge to enhance MM-ICL. Specifically, Gong et al. [2023] man-
ually create a general template with multiple images and corresponding responses during instruction-
tuning (IT) stage to improve MM-ICL. Tsimpoukelli et al. [2021], Li et al. [2023b], Doveh et al.
[2024] and Zhao et al. [2024] develop task-specific MM-ICL templates during the IT stage, further
extending its capabilities across more domains. Li et al. [2023a] introduce OtterHD, adapting MM-
ICL for high-definition image tasks. Furthermore, Sun et al. [2023] and Tian et al. [2024] explore the
potential of MM-ICL in the image generation tasks. Jin et al. [2024] provide compelling evidence for
the effectiveness of MM-ICL in comprehending game instructions. Zong et al. [2024] and Shukor
et al. [2024] develop fine-grained benchmarks and evaluate the MM-ICL in classification tasks."
INTRODUCTION,0.007267441860465116,"While significant progress has been witnessed in MM-ICL, the existing work still mainly focuses
on how to optimize MM-ICL, ignoring the underlying factors that influence its effectiveness and"
INTRODUCTION,0.00872093023255814,∗Equal Contribution
INTRODUCTION,0.010174418604651164,"Prompt (𝓟)
Demonstration"
INTRODUCTION,0.011627906976744186,Ordered List (𝓛𝒌)
INTRODUCTION,0.01308139534883721,Sample: 𝑥𝜋1
INTRODUCTION,0.014534883720930232,Sample: 𝑥𝜋2
INTRODUCTION,0.015988372093023256,Sample: 𝑥𝜋3
INTRODUCTION,0.01744186046511628,"Multimodal 
Validation Set (𝓥𝒌) …"
INTRODUCTION,0.0188953488372093,Sample:𝑥2
INTRODUCTION,0.020348837209302327,Sample:𝑥1
INTRODUCTION,0.02180232558139535,Demonstration
INTRODUCTION,0.023255813953488372,Retrieval
INTRODUCTION,0.024709302325581394,Demonstration
INTRODUCTION,0.02616279069767442,Ordering
INTRODUCTION,0.027616279069767442,Sample:𝑥𝑁
INTRODUCTION,0.029069767441860465,Demonstration
INTRODUCTION,0.030523255813953487,Set (𝓒𝒌)
INTRODUCTION,0.03197674418604651,Sample: 𝑥𝜋2
INTRODUCTION,0.03343023255813953,Sample: 𝑥𝜋1
INTRODUCTION,0.03488372093023256,Sample: 𝑥𝜋3
INTRODUCTION,0.036337209302325583,Prompt
INTRODUCTION,0.0377906976744186,Construction
INTRODUCTION,0.03924418604651163,Instruction: ? ? ? 1 2 3
INTRODUCTION,0.040697674418604654,Sample: 𝑥𝜋1
INTRODUCTION,0.04215116279069767,Sample: 𝑥𝜋2
INTRODUCTION,0.0436046511627907,Sample: 𝑥𝜋3 1 2 3
INTRODUCTION,0.04505813953488372,Figure 1: The whole process of prompting creation for multi-modal in-context-learning.
INTRODUCTION,0.046511627906976744,"performance. Such gap impedes a comprehensive understanding of the mechanisms and performance
determinants of MM-ICL, thereby limiting further exploration and research in this field. Motivated
by this, this paper aims to systematically investigate the research question: What factors affect
the performance of MM-ICL?, hoping to offer a unified view and guideline for researchers to
build better MM-ICL. Specifically, as illustrated in Figure 1, the MM-ICL process comprises three
steps: demonstration retrieval, demonstration ordering, and prompt construction. Therefore, We
systematically investigate the following sub-questions: (a) how to select multi-modal demonstrations
(Sec. 3.1); (b) how to order multi-modal demonstrations (Sec. 3.2); and (c) how to construct MM-ICL
prompts (Sec. 3.3) to this end. To achieve this, we conduct detailed experiments on MM-ICL using
20 strategies across 4 tasks with 6 representative vision large language models (VLLMs)."
INTRODUCTION,0.04796511627906977,"Through extensive investigations, the main findings are as follows:"
INTRODUCTION,0.04941860465116279,"• Multi-modal alignment is the bottleneck for MM-ICL. Our analysis confirms that, on
average, multi-modal retrieval methods outperform single-modal ones. Furthermore, multi-
modal alignment in VLLMs has a greater impact on MM-ICL effectiveness than parameter size,
identifying alignment as the key limitation in both backbone structure and demonstration quality.
• Intra-demonstration ordering holds greater importance than inter-demonstration ordering.
Our investigation first indicates that the intra-demonstration ordering, particularly the ordering
of modalities, greatly influences model performance more than demonstration arrangement.
• Introductory instruction guides better task understanding for MM-ICL. To construct a
comprehensive MM-ICL prompt, it is essential to include introductory instructions preceding
the demonstrations. This approach consistently enhances the performance of MM-ICL campared
with summative instruction placed after demonstrations, and intra-demonstration instruction."
BACKGROUND,0.050872093023255814,"2
Background"
BACKGROUND,0.05232558139534884,"In this work, we formally present the prompt building process for MM-ICL. As depicted in Figure 1,
the process of prompt building for MM-ICL involves three sequential stages:"
BACKGROUND,0.05377906976744186,"(1) Demonstration Retrieval: The core MM-ICL requires retrieval to obtain demonstrations that
can help MM-ICL. Formally, given a validation dataset Vn = {x1, x2, . . . , xn}, each multi-modal
sample xi includes textual input Itxt
i
, visual input Ivis
i
, and output Oi. For a specific test query q,
this step aims to identify a subset of relevant demonstrations Ck = {xπj}k
j=1, where xπj ∈Vn."
BACKGROUND,0.055232558139534885,"(2) Demonstration Ordering: Researches [Lu et al., 2022b, Wu et al., 2023, Xiang et al., 2024] show
that LLMs are highly sensitive to the order of demonstrations. Thus, arranging these demonstrations
effectively is crucial for MM-ICL. After retrieving relevant demonstrations, we must rearrange the
sequence Lk = [xσj]k
j=1, which will be used to construct the prompt."
BACKGROUND,0.056686046511627904,"(3) Prompt Construction: Previous research indicates that using delimiters and instructions can
significantly enhance textual ICL capabilities [Min et al., 2022, Qin et al., 2023]. Therefore, the
final core step is to transform the ordered demonstrations into a structured prompt P, incorporating
delimiters and instructions to optimize MM-ICL."
BACKGROUND,0.05813953488372093,"3
What Factors Affect Multi-modal In-Context Learning?"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.059593023255813955,"3.1
Exploration of MM-ICL Demonstration Retrieval"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.061046511627906974,"The efficacy of ICL heavily depends on the quality of the retrieved demonstrations C, which provide
essential prior knowledge for MM-ICL. As illustrated in Figure 2, the retrieval process encompasses"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.0625,Sample: 𝑥1
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.06395348837209303,Textual Encoder
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.06540697674418605,"Query: 𝑞
…
Sample: 𝑥𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.06686046511627906,(a) Sample Representation
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.06831395348837209,"Sample:ℎ1
Query:ℎ𝑞
…
Sample:ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.06976744186046512,"Visual Encoder
Multi-modal Encoder"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07122093023255814,"Sample: 𝑥1
Query: 𝑞
…
Sample: 𝑥𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07267441860465117,"Sample:ℎ1
Query:ℎ𝑞
…
Sample:ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07412790697674419,"Sample: 𝑥1
Query: 𝑞
…
Sample: 𝑥𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.0755813953488372,"Sample:ℎ1
Query:ℎ𝑞
…
Sample:ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07703488372093023,Sample: ℎ1
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07848837209302326,Cosine Distance
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.07994186046511628,"Query:ℎ𝑞
…
Sample: ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.08139534883720931,(b) Sample Comparison
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.08284883720930232,"Sample:𝒬1
…
Sample:𝒬𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.08430232558139535,"L2 Distance
Semantic Coverage"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.08575581395348837,"Sample: ℎ1
Query:ℎ𝑞
…
Sample: ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.0872093023255814,"Sample:𝒬1
…
Sample:𝒬𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.08866279069767442,"Sample: ℎ1
Query:ℎ𝑞
…
Sample: ℎ𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09011627906976744,"Sample:𝒬1
…
Sample:𝒬𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09156976744186046,Sample:𝒬1
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09302325581395349,Domain Selection
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09447674418604651,"Query: 𝑞
…
Sample:𝒬𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09593023255813954,(c) Sample Selection
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09738372093023256,"Sample:𝑥𝜋1
…
Sample:𝑥𝜋𝑘"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.09883720930232558,"Image Style Selection
Modality Distance"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.1002906976744186,"Sample:𝒬1
Query: 𝑞
…
Sample:𝒬𝑁
Sample:𝒬1
Query: 𝑞
…
Sample:𝒬𝑁"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10174418604651163,"Sample:𝑥𝜋1
…
Sample:𝑥𝜋𝑘
Sample:𝑥𝜋1
…
Sample:𝑥𝜋𝑘"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10319767441860465,Figure 2: The demonstration retrieval process for MM-ICL.
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10465116279069768,"three key steps: (1) Sample Representation, (2) Sample Comparison, and (3) Sample Selection. In
this section, we conduct a systematic analysis of how various strategies for sample representation,
comparison, and selection affect MM-ICL task performance."
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10610465116279069,"Sample Representation. It involves defining an encoder (Encoder(·)) to map each input sample
xj ∈V and user query q into a shared representation space:"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10755813953488372,"hj = Encoder(xj).
(1)"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.10901162790697674,"Specifically, we evaluate various encoder architectures across modalities, focusing on the impact of
visual encoder (Encodervis), text encoder (Encodertxt), and multi-modal encoder (Encodermulti)
on model performance."
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.11046511627906977,"Sample Comparison. After deriving the representations, we employ a metric M to evaluate the
quality Qj of the sample hj in comparison to the query representation hq and the dataset samples hj:"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.1119186046511628,"Qj = M(hq, hj).
(2)"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.11337209302325581,"Specifically, we explore various comparison metrics, including cosine similarity Mcos [Liu et al.,
2022a], L2 similarity ML2 [Liu et al., 2022a], and semantic diversity Mdiv [Li and Qiu, 2023a], to
assess sample quality and understand the correlation with model performance."
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.11482558139534883,"Sample Selection. After quality assessments, we apply a selection criterion S to identify the k most
advantageous samples xπj for inclusion in the demonstration set C:"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.11627906976744186,"C = {xπj|xπj ∈S(q, Qj), j ≤k}.
(3)"
EXPLORATION OF MM-ICL DEMONSTRATION RETRIEVAL,0.11773255813953488,"Sample selection is guided by factors such as domain information [He et al., 2023], demonstration
style [Agrawal et al., 2023], and token distance [Liu et al., 2022a]. Specifically, we systematically
examine samples from both in-domain and out-of-domain collections. And we also assess the impact
of image style on the selected demonstrations. Further, we investigate the token distance between
modalities to understand its effects on sample selection for MM-ICL."
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.11918604651162791,"3.2
Exploration of MM-ICL Demonstration Ordering"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12063953488372094,"Following Lu et al. [2022b] and Wu et al. [2023], the order of the demonstration set C significantly
impacts MM-ICL performance. As shown in Figure 3, this section explores two key aspects:"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12209302325581395,"Intra-demonstration Ordering. The sequence within a demonstration, especially modalities (e.g.,
text and image), is an important component that might affect the MM-ICL capabilities. Therefore,"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12354651162790697,"Sample:𝑥3
Sample:𝑥2
Sample:𝑥1"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.125,"Sample:𝑥𝜎3
Sample:𝑥𝜎2
Sample:𝑥𝜎1"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12645348837209303,Inter-demonstration Ordering
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12790697674418605,Sample: 𝒔𝒊
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.12936046511627908,"Textual
input: 𝐼𝑖"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.1308139534883721,"𝑡𝑥𝑡
Textual
output: 𝑂𝑖"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.13226744186046513,"Visual
input:𝐼𝑖 𝑣𝑖𝑠"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.13372093023255813,Textual input:𝐼𝑖
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.13517441860465115,"𝑡𝑥𝑡
Textual output:𝑂𝑖
Visual input:𝐼𝑖 𝑣𝑖𝑠"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.13662790697674418,"Intra-demonstration Ordering ? 1
2
3 ?
?"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.1380813953488372,"Demonstration Ordered List ℒ ?
?
? 1
2
3"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.13953488372093023,Figure 3: The demonstration ordering process for MM-ICL.
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.14098837209302326,we introduce a intra-demonstration ordering permutation (IOP) to define this sequence:
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.14244186046511628,"L = [IOP(xπ1), IOP(xπ2), . . . , IOP(xπk)].
(4)"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.1438953488372093,"We conduct a systematic exploration of various IOP configurations, including text-image-text (IOPtvt),
text-text-image (IOPttv), and image-text-text (IOPvtt). These order analyses aim to evaluate the
impact of different modal sequences on the model’s performance."
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.14534883720930233,"Inter-demonstration Ordering. The sequence in which demonstrations are organized within C also
is the key component that might impact the performance of MM-ICL. Formally, we define a sample
ordering permutation σj to specify the arrangement:"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.14680232558139536,"L = [xσ1, xσ2, . . . , xσk|xσj ∈C],
(5)"
EXPLORATION OF MM-ICL DEMONSTRATION ORDERING,0.14825581395348839,where xσj represents the j-th demonstration in the ordered demonstration list.
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.14970930232558138,"3.3
Exploration of MM-ICL Prompt Construction"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1511627906976744,"VLLMs are highly sensitive to input instructions [Kojima et al., 2022, Qin et al., 2023]. Inspired by
this, to enhance task comprehension, we incorporate different instructions to explore the performance
influence for MM-ICL. Formally, we construct instruction methods I(·) that describe the task and
position them within the prompt. The prompt construction process is:"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.15261627906976744,"P = I(δ(xσ1), δ(xσ2), . . . , δ(xσk)),
(6)"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.15406976744186046,"Specifically, as shown in Figure 4, we explore three instruction categories to bolster MM-ICL process:"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1555232558139535,"• Introductory Instruction (Iintro) refers to the initial guidance that offers an overview of the
task prior to any demonstrations. As shown in Figure 4 (a), this instruction, denoted as Iintro, is
positioned at the start of the ordered demonstration sequence, L."
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1569767441860465,(a) Introductory Instruction
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.15843023255813954,Instruction Injection
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.15988372093023256,Ordered List: ℒ
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1613372093023256,(b) Summative Instruction
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.16279069767441862,Instruction Injection
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.16424418604651161,Instruction:
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.16569767441860464,Ordered List: ℒ
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.16715116279069767,Ordered List: ℒ
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1686046511627907,(c) Intra-demonstration Instruction
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17005813953488372,Instruction Injection
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17151162790697674,"Input: 𝐼1
Output: 𝑂1
Input: 𝐼2
Output: 𝑂2"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17296511627906977,"Instruction:
Input: 𝐼1
Output: 𝑂1"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1744186046511628,Ordered
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17587209302325582,List (ℒ)
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17732558139534885,Ordered
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.17877906976744187,"List (ℒ)
Instruction:
Input: 𝐼2
Output: 𝑂2"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.18023255813953487,"Instruction:
Ordered List: ℒ"
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.1816860465116279,"Figure 4: The process of instruction injection for MM-ICL prompt construction involves three
key elements. The Introductory Instruction provides an overview instruction of the task before
demonstrations. The Summative Instruction summarizes after the examples, guiding the model
to apply the learned concepts to practical problems. The Intra-demonstration Instruction embeds
task-specific guidance within each demonstration, enabling VLLMs to grasp task requirements during
learning. Further details and additional prompts are provided in Appendix C.3."
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.18313953488372092,"• Summative Instruction (Isum) offers a summary after the examples, guiding the model to
apply the learned concepts to real-world problems. As shown in Figure 4 (b), this instruction I
is added at the end of the demonstration list L."
EXPLORATION OF MM-ICL PROMPT CONSTRUCTION,0.18459302325581395,"• Intra-demonstration Instruction (Iintra) embeds task instructions within each demonstration,
helping VLLMs understand the task requirements during the learning process. As shown in
Figure 4 (c), this instruction I is included within each demonstration xi in the list L."
EXPERIMENTAL SETUP,0.18604651162790697,"4
Experimental Setup"
EXPERIMENTAL SETUP,0.1875,"Following the setting of Li et al. [2023c], we systematically explore 4 tasks, including image-caption,
visual question answering (VQA), image classification, and chain-of-thought reasoning, which come
from M3IT [Li et al., 2023c] and M3CoT [Chen et al., 2024b] (as shown in Tables 2), providing a
universal paradigm can help researchers conduct unified and fairer comparisons and studies within a
unified framework. In order to evaluate the MM-ICL performance accurately, we use two indicators
for each task. Following Zhang et al. [2019], Li et al. [2023b], and Zong et al. [2024], we use
CIDER [Vedantam et al., 2015] and BertScore [Zhang et al., 2019] as image-caption metrics. Since
M3IT includes various VQA tasks with free-form answers, inspired by the success of free-form
and precise answer hybrid evaluation in machine reading comprehension, following Rajpurkar et al.
[2016], Zhang et al. [2019], we adapt Token-F1 [Rajpurkar et al., 2016] and BertScore as visual
question answering (VQA) metrics (The correlation analysis of the indicators and accuracy as shown
in Table 3). Following Li et al. [2023c,b], we use accuracy and F1 score as indicators of image
classification. Following Lu et al. [2022a], Golovneva et al. [2022] and Qin et al. [2023], we use
accuracy and reasoning alignment score [Golovneva et al., 2022] (RAS) as indicators of reasoning."
EXPERIMENTAL SETUP,0.18895348837209303,"To ensure rigorous experimental control, we established a baseline using a multi-modal encoder
for data representation and cosine similarity for sample comparison, limiting retrieval to within the
same task. This baseline ranks samples based on similarity, with a delimiter and a 3-shot setting (see
Appendix A for details). In addition, all open source models complete inference on 2 A100 80G. For
all experiments, we select top-p from {0.95, 1} and adjust the temperature parameter within [0, 1].
Among them, temperature is the main error variable in this work."
EMPIRICAL ANALYSIS OF FACTORS AFFECTING MM-ICL,0.19040697674418605,"5
Empirical Analysis of Factors Affecting MM-ICL"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION RETRIEVAL,0.19186046511627908,"5.1
Empirical Analysis of MM-ICL Demonstration Retrieval"
SAMPLE REPRESENTATION,0.1933139534883721,"5.1.1
Sample Representation"
SAMPLE REPRESENTATION,0.19476744186046513,"Multi-modal alignment is the bottleneck for MM-ICL in both backbones and demonstrations.
To evaluate the impact of semantic representation in different modalities for MM-ICL, we assessed
three distinct encoders: RoBERTa [Liu et al., 2019] as a textual encoder for Textual Retriever,
CLIP-Vision Encoder [Radford et al., 2021] for Visual Retriever, and BridgeTower [Xu et al.,
2023] as multi-modal encoder for Multi-Modal Retriever. As illustrated in Table 1, multi-modal
retrieval consistently outperforms zero-shot, randomly selected, and single-modality methods, high-
lighting the advantages of multi-modal semantic learning for MM-ICL. What’s more, as shown in
Table 1, our results reveal that increasing model parameters from 8 billion to over 100 billion does not
significantly enhance performance, suggesting that beyond parameter size, multi-modal context under-
standing and alignment are more crucial for MM-ICL than model scaling. Our analysis demonstrates
that multi-modal alignment is the critical factor in both the backbone and demonstrations."
SAMPLE REPRESENTATION,0.19622093023255813,"Current multi-modal encoders still lack modeling of multi-modal logic. Actually, multi-modal
retrieval attains better performance in many scenarios like Image Caption and VQA. However, our
experiments show that textual retrieval works well for classification and reasoning tasks. Based on the
qualitative analysis, we observe that due to the semantic richness of the labels and rationales, textual
retrieval can obtain more similar samples. However, the current multi-modal retrieval struggles with
complex text semantics, often favoring image similarity. This aligns with recent work [Tong et al.,
2023, 2024, Fei et al., 2024c], which is valuable for future exploration."
SAMPLE REPRESENTATION,0.19767441860465115,"Multi-modal context diminishes the necessity of careful demonstration selection. As shown
in Table 1, adding relevant demonstrations slightly improves performance, but the gains are less"
SAMPLE REPRESENTATION,0.19912790697674418,"Caption
VQA
Classification
Reasoning
AVG
CIDER
BERTScore
Token F1
BERTScore
Acc
F1
Acc
RAS"
SAMPLE REPRESENTATION,0.2005813953488372,"OpenFlamingo (9B) [Awadalla et al., 2023]"
SAMPLE REPRESENTATION,0.20203488372093023,"Zero-shot
1.84
81.18
2.78
76.17
15.17
3.63
16.53
85.13
35.30
Few-shot (Random)
8.23
56.63
12.63
67.37
13.11
5.11
21.35
86.53
33.87
+ Textual Retriever
13.39
74.22
21.80
75.74
13.67
12.04
25.63
87.71
40.52
+ Visual Retriever
6.33
53.88
12.82
68.76
13.67
10.87
23.10
87.36
34.60
+ Multi-Modal Retriever
13.47
85.01
7.85
79.05
19.66
10.10
24.96
88.11
41.03"
SAMPLE REPRESENTATION,0.20348837209302326,"Otter (9B) [Li et al., 2023b]"
SAMPLE REPRESENTATION,0.20494186046511628,"Zero-shot
2.86
86.42
20.90
87.95
24.34
10.85
34.06
82.67
43.76
Few-shot (Random)
3.50
86.62
20.95
87.76
25.66
10.28
34.23
83.67
44.08
+ Textual Retriever
3.89
86.62
20.40
87.89
25.28
8.47
32.88
81.93
43.42
+ Visual Retriever
3.50
86.51
18.57
87.58
26.78
12.44
32.21
84.17
43.97
+ Multi-Modal Retriever
3.77
86.57
18.80
87.56
28.65
11.83
35.92
83.74
44.60"
SAMPLE REPRESENTATION,0.2063953488372093,"Qwen-VL (10B) [Bai et al., 2023]"
SAMPLE REPRESENTATION,0.20784883720930233,"Zero-shot
13.57
87.65
24.96
85.09
50.19
54.28
48.40
90.87
56.87
Few-shot (Random)
28.52
88.47
28.43
86.11
52.43
53.50
43.34
90.19
58.87
+ Textual Retriever
21.58
88.07
26.99
85.62
49.44
53.04
46.04
90.72
57.69
+ Visual Retriever
30.81
88.56
28.79
86.23
59.74
54.43
46.88
91.30
60.84
+ Multi-Modal Retriever
41.51
89.03
30.20
86.78
59.36
53.17
46.21
91.49
62.22"
SAMPLE REPRESENTATION,0.20930232558139536,"GPT4V (>100B) [OpenAI: et al., 2023]"
SAMPLE REPRESENTATION,0.21075581395348839,"Zero-shot
5.15
85.43
20.01
84.77
61.42
59.07
54.64
92.46
57.87
Few-shot (Random)
6.37
85.95
24.43
85.42
60.11
60.81
54.30
92.54
58.74
+ Textual Retriever
9.48
86.02
31.81
87.02
62.55
51.40
55.99
92.26
59.57
+ Visual Retriever
9.36
86.26
32.47
86.96
63.30
57.79
59.87
93.19
61.15
+ Multi-Modal Retriever
16.55
86.77
32.92
86.87
62.55
59.97
60.88
93.10
62.45"
SAMPLE REPRESENTATION,0.21220930232558138,"IDEFICS2 (8B) [Laurençon et al., 2024b]"
SAMPLE REPRESENTATION,0.2136627906976744,"Zero-shot
32.80
88.59
26.88
86.99
66.85
57.84
54.97
89.01
62.99
Few-shot (Random)
39.68
88.88
30.82
87.59
61.61
53.39
51.94
89.52
62.93
+ Textual Retriever
35.95
88.45
31.66
87.58
61.99
67.13
45.03
88.98
63.34
+ Visual Retriever
46.61
89.55
32.05
87.92
64.04
62.51
51.26
89.83
65.47
+ Multi-Modal Retriever
52.55
89.66
33.65
88.17
65.54
63.86
51.43
89.57
66.80"
SAMPLE REPRESENTATION,0.21511627906976744,"Gemini-Pro (>100B) [Google, 2023]"
SAMPLE REPRESENTATION,0.21656976744186046,"Zero-shot
14.05
87.07
26.93
85.78
68.20
66.10
55.14
90.72
61.75
Few-shot (Random)
21.21
88.13
32.99
86.81
63.67
69.75
55.65
91.82
63.75
+ Textual Retriever
15.79
87.75
34.96
87.18
69.10
72.31
53.29
91.57
63.99
+ Visual Retriever
21.35
87.98
44.74
89.33
65.73
64.18
53.29
91.92
64.81
+ Multi-Modal Retriever
35.64
88.67
45.47
89.61
65.17
70.51
58.01
92.17
68.16"
SAMPLE REPRESENTATION,0.2180232558139535,"Table 1: Performance comparison of retrievers utilizing different modal representations, where
Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected
from the development set."
SAMPLE REPRESENTATION,0.2194767441860465,"significant compared to text-only ICL scenarios. Specifically, retrieved demonstrations yield an
average performance boost of 3.84%, compared to random demonstrations. In contrast, text-only
scenarios show performance increases of over 10% with carefully selected samples [Shi et al., 2023].
Furthermore, the model remains unaffected by irrelevant samples, and the performance of almost all
models is higher than zero-shot. This indicates that multi-modal context significantly reduces the
need for careful demonstration selection, unlike in text-only scenarios."
SAMPLE REPRESENTATION,0.22093023255813954,"VLLMs learn semantic representations instead of token pattern representations for MM-ICL.
As depicted by Agrawal et al. [2023], textual ICL primarily learns token patterns (e.g., similar
output formats, reasoning paths) among demonstration outputs. To investigate whether VLLMs rely
on repetitive token patterns, we utilize the average BLEU score across demonstration outputs as
a representation of token repetition. Figure 5 shows that only the image captioning task exhibits
a positive correlation. In contrast, other tasks show a decline as BLEU scores exceed 30%. This
underscores that MM-ICL primarily learns semantic rather than token pattern representations for
effective performance."
SAMPLE COMPARISON,0.22238372093023256,"5.1.2
Sample Comparison"
SAMPLE COMPARISON,0.2238372093023256,"To further analyze the influencing factors of MM-ICL in sample retrieval, this study employs similarity
and diversity metrics, which help assess how MM-ICL processes sample similarities and differences,
enhancing our understanding of its mechanisms. See Appendix B for more details and results. 0 20 40 60 80 100 120"
SAMPLE COMPARISON,0.22529069767441862,"0
20
40
60
80
100 BLEU"
SAMPLE COMPARISON,0.22674418604651161,"Caption
Classification
VQA
Reasoning"
SAMPLE COMPARISON,0.22819767441860464,Performance (%)
SAMPLE COMPARISON,0.22965116279069767,"Figure 5: The impact of token pattern
representation in Gemini-Pro. CIDER"
SAMPLE COMPARISON,0.2311046511627907,"100
50 RAS Acc. F1"
SAMPLE COMPARISON,0.23255813953488372,BERTScore
SAMPLE COMPARISON,0.23401162790697674,Token F1
SAMPLE COMPARISON,0.23546511627906977,Accuracy
SAMPLE COMPARISON,0.2369186046511628,BERTScore
SAMPLE COMPARISON,0.23837209302325582,(AVG: 66.55 → 68.16)
SAMPLE COMPARISON,0.23982558139534885,"Diversity Retriever
Similar Retriever"
SAMPLE COMPARISON,0.24127906976744187,"(b) The impact of the utilization on
diversity metrics. CIDER"
SAMPLE COMPARISON,0.24273255813953487,"100
50 RAS Acc. F1"
SAMPLE COMPARISON,0.2441860465116279,BERTScore
SAMPLE COMPARISON,0.24563953488372092,Token F1
SAMPLE COMPARISON,0.24709302325581395,Accuracy
SAMPLE COMPARISON,0.24854651162790697,BERTScore
SAMPLE COMPARISON,0.25,(AVG: 53.99 → 68.16)
SAMPLE COMPARISON,0.251453488372093,"L2 Distance
Cosine Distance"
SAMPLE COMPARISON,0.25290697674418605,"(a) The impact of the different similarity
metrics."
SAMPLE COMPARISON,0.2543604651162791,"Figure 6: The impact of different sample comparison
methodologies in Gemini-Pro."
SAMPLE COMPARISON,0.2558139534883721,"Cosine similarity matters for sample comparison. Following Liu et al. [2022b], we compare two
representative similarity metrics, cosine similarity and L2 similarity. As shown in Figure 6 (a), cosine
similarity, which measures the directional semantic alignment, emerges as the superior metric in
MM-ICL than L2 similarity. Supported by Deza et al. [2009] and Steck et al. [2024], it indicates that
MM-ICL prioritizes semantic directional consistency over complete semantic alignment."
SAMPLE COMPARISON,0.25726744186046513,"Diversity does not show significant influence for sample comparison. He et al. [2023], Li and Qiu
[2023b] have shown that demonstrations with better diversity can effectively improve textual ICL.
To explore whether it exists in MM-ICL, following Li and Qiu [2023b], we ultilize the “diversity
retriever”, which selects the top-10 samples and further chooses the best 3 samples based on semantic
diversity to obtain a more diverse MM-ICL. As demonstrated in Figure 6 (b), although diversity
significantly enhances performance in text-based ICL, our experiments show limited improvement in
MM-ICL tasks. This suggests that diversity may not directly correlate with better MM-ICL."
SAMPLE SELECTION,0.25872093023255816,"5.1.3
Sample Selection"
SAMPLE SELECTION,0.2601744186046512,"Domain interval matters for sample selection. Prior research highlights the critical role of domain
relevance in enhancing ICL performance. Inspired by this, we employ the multi-modal retriever
to select samples from both in-domain and out-of-domain pools. Figure 7 (a) shows a nearly 4%
performance drop when out-of-domain demonstrations are included, underscoring the necessity of
in-domain demonstrations for optimal MM-ICL."
SAMPLE SELECTION,0.2616279069767442,"Visual style is not a crucial factor in sample selection. Although stylistic similarity in text samples
is known to bolster ICL, its effect on the visual modality remains ambiguous. Utilizing CLIP for
image classification, we investigate the impact of stylistic coherence in multi-modal samples on
MM-ICL performance. As depicted in Figure 7 (b), significant enhancements are observed solely in
the VQA task, while captioning and classification show minimal effects and reasoning tasks decline.
This indicates that diverse visual styles are not crucial in general MM-ICL."
SAMPLE SELECTION,0.26308139534883723,"Token distances between modalities need to be considered for different tasks to improve sample
selection. For textual ICL, excessive token distance between samples can impede performance [Liu 20 30 40 50 60 70 80"
SAMPLE SELECTION,0.26453488372093026,"# 0
# 1
# 2
# 3"
SAMPLE SELECTION,0.26598837209302323,"Caption
Classification
VQA
Reasoning"
SAMPLE SELECTION,0.26744186046511625,The number of images with same type
SAMPLE SELECTION,0.2688953488372093,Performance (%)
SAMPLE SELECTION,0.2703488372093023,"(b) The impact of visual style in
sample selection on performance. 0 20 40 60 80 100"
SAMPLE SELECTION,0.27180232558139533,"20
140
300
500
540
The number of Textual Tokens"
SAMPLE SELECTION,0.27325581395348836,"(c) The impact of token distance between
demonstrations in sample selection. CIDER"
SAMPLE SELECTION,0.2747093023255814,"100
50 RAS Acc. F1"
SAMPLE SELECTION,0.2761627906976744,BERTScore
SAMPLE SELECTION,0.27761627906976744,Token F1
SAMPLE SELECTION,0.27906976744186046,Accuracy
SAMPLE SELECTION,0.2805232558139535,BERTScore
SAMPLE SELECTION,0.2819767441860465,(AVG: 64.69 → 68.16)
SAMPLE SELECTION,0.28343023255813954,"Out-of-Domain
In-Domain"
SAMPLE SELECTION,0.28488372093023256,"(a) The impact of whether the sample 
is in-domain on performance."
SAMPLE SELECTION,0.2863372093023256,Figure 7: The impact of sample selection on average score performance in Gemini-Pro.
SAMPLE SELECTION,0.2877906976744186,"et al., 2022a]. We extend this inquiry to MM-ICL, analyzing how token distance across modalities
influences results. Specifically, during the sample selection process, we considered the impact of
the average token distance between two images on the model within the entire prompt of MM-ICL.
As illustrated in Figure 7 (c), the effect of token distance varies by task, typically showing an initial
performance increase followed by a decline as distance grows, particularly in non-captioning tasks.
This highlights the task-dependent nature of optimal token distance in MM-ICL."
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.28924418604651164,"5.2
Empirical Analysis of MM-ICL Demonstration Ordering"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.29069767441860467,"Intra-demonstration ordering significantly impacts performance. Within the demonstration,
organizing the ordering, especially the relationship between modalities is a crucial topic. We
investigate this by arranging inputs and outputs across modalities using three methods: text input→text
output→image input (Text-Image), text input→image input→text output (Text-Image-Text), and
image input→text input→text output (Image-Text). As shown in Figure 8 (a), positioning the image at
the start significantly enhances model performance. This suggests that presenting visual information
first improves multi-modal comprehension, thereby boosting its learning abilities."
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.2921511627906977,"Inter-demonstration ordering demonstrates minimal impacts. Following Lu et al. [2022c], we
investigate how the order of demonstration presentation influences model efficacy. We explore various
strategies: random rearrangement, a ""similar-last"" approach where samples similar to the query are
shown last, and a ""similar-first"" approach where similar samples are presented first. Figure 8 (b)
illustrates that inter-demonstration ordering has a negligible impact on MM-ICL performance. This
suggests the order-robustness, with the presentation sequence having minimal effect."
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.2936046511627907,"Gemini Pro
IDEFICS2
Qwen-VL
Otter
OpenFlamingo"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.29505813953488375,62.2 60.4 59.2
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.29651162790697677,44.6 43.7 43.6
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.29796511627906974,"70
60
50
40 30 41.0"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.29941860465116277,34.0 36.0
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3008720930232558,54.9 56.7
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3023255813953488,"Image-Text
Text-Image
Text-Image-Text"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.30377906976744184,"Gemini Pro
IDEFICS2
Qwen-VL
Otter
OpenFlamingo"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.30523255813953487,62.2 61.4 61.5
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3066860465116279,44.6 45.0 44.3
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3081395348837209,"70
60
50
40 30"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.30959302325581395,41.0 37.7 38.1
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.311046511627907,"Similar-First
Similar-Last
Random"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3125,(a) The effect of the order of modals within a demonstration on average performance
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.313953488372093,(b) The effect of the order of demonstrations on average performance
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.31540697674418605,"68.2 68.0 66.9
66.8 67.6 67.4"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3168604651162791,"68.2 65.7 66.5
66.8"
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.3183139534883721,Best Performance
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.31976744186046513,Best Performance
EMPIRICAL ANALYSIS OF MM-ICL DEMONSTRATION ORDERING,0.32122093023255816,Figure 8: The impact of demonstration ordering on performance.
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3226744186046512,"5.3
Empirical Analysis of MM-ICL Prompt Construction"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3241279069767442,"Introductory Instruction is consistently effective for better MM-ICL.
To investigate the impact
of inserting task-related instructions within prompts, we conduct the following experiment on three
categories of instruction: Introductory Instruction, Summative Instruction, and Intra-demonstration
Instruction. As depicted in Figure 9, our analysis indicates that introductory instructions stably"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.32558139534883723,"Gemini Pro
IDEFICS2
Qwen-VL
Otter
OpenFlamingo"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.32703488372093026,"No-Instruction
Introductory Instruction
Summative Instruction 70 60 50 40 30"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.32848837209302323,"64.3 
62.2 63.0 61.1 61.1"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.32994186046511625,"44.6 45.5 44.0 43.5 
41.0 43.5 40.0 40.0"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3313953488372093,Intra-demonstration Instruction
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3328488372093023,66.8 67.3 66.2
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.33430232558139533,Best Performance
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.33575581395348836,68.2 68.5 68.2 68.4
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3372093023255814,Figure 9: The impact of injecting instruction into demonstrations on model average score performance. 30 40 50 60 70
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3386627906976744,"1
2
3
4
5
6
30 40 50 60 70"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.34011627906976744,"0
1
2
3
4
5
The number of demonstrations"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.34156976744186046,Performance (%)
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3430232558139535,The number of demonstrations
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3444767441860465,Performance (%)
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.34593023255813954,"Classification
Caption
Reasoning
VQA"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.34738372093023256,"Gemini
IDEFICS2
Otter
Flamingo
Qwen-VL"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3488372093023256,"(a) The impact of the number of demon-
strations on average score performance."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3502906976744186,"(b) The impact of the number of demon-
strations on different task performance."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.35174418604651164,"Figure 10: The impact of the number of demon-
strations on performance. CIDER"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.35319767441860467,"100
50 RAS Acc. F1"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3546511627906977,BERTScore
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3561046511627907,Token F1
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.35755813953488375,Accuracy
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.35901162790697677,BERTScore
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.36046511627906974,(AVG: 64.76 → 68.16)
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.36191860465116277,(a) Gemini CIDER
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3633720930232558,"100
50 RAS Acc. F1"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3648255813953488,BERTScore
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.36627906976744184,Token F1
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.36773255813953487,Accuracy
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3691860465116279,BERTScore
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3706395348837209,(AVG: 65.19 → 66.80)
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.37209302325581395,(b) IDEFICS2
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.373546511627907,"w/o Delimiter
w/ Delimiter
w/o Delimiter
w/ Delimiter"
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.375,"Figure 11: The impact of inserting delimiter into
the input and output of demonstration on model
performance. See Figure 16 for more results."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.376453488372093,"enhance model performance. In contrast, other instructions generally decrease performance. This
finding suggests that introductory instructions facilitate targeted contextual learning and more effective
semantic comprehension in demonstrations. We show more prompts and details in Appendix C.3."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.37790697674418605,"MM-ICL is affected by the number of demonstrations depending on the task.
Contrary to
traditional text-based ICL, where performance improves with more samples, our findings in Figure 10
(a) suggest that MM-ICL does not experience significant gains from more demonstrations. To further
understand the reason behind, we analysis the performance on different tasks. As shown in Figure 10
(b), increasing the number of demonstrations enhances performance in caption and VQA tasks, a
trend also reported in prior studies [Alayrac et al., 2022, Laurençon et al., 2024a, Shukor et al., 2024].
However, performance declines when demonstrations exceed three across all tested VLLMs. In
more complex reasoning tasks, such as multi-step multi-modal chain-of-thought reasoning, additional
demonstrations do not yield effective improvements, aligning with the findings of Chen et al. [2024b],
and Fei et al. [2024a]."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3793604651162791,"Moreover, we attribute it to the following reasons for this limitation: (1) Cognitive Overload: For
complex tasks, understanding numerous demonstrations can overwhelm the model, impeding its
ability to process and integrate information effectively [Chen et al., 2024a]. (1) Complexity of
Reasoning Tasks: In reasoning tasks, the performance improvement from more demonstrations is
often less pronounced than when using diverse retrievers. This suggests that reasoning tasks require
sophisticated integration of information, where quality outweighs quantity. See Appendix C.1 for
more detailed description."
EMPIRICAL ANALYSIS OF MM-ICL PROMPT CONSTRUCTION,0.3808139534883721,"The importance of delimiter lessens by text-image interleaved demonstrations.
Previous re-
search suggests that specific delimiters for input and output data can demonstrably influence textual
ICL capabilities [Min et al., 2022]. Therefore, we utilize ablation experiments to omit these delimiters
to examine their necessity (see Appendix C.2 for details). As shown in Figure 11, the resulting minor
performance decline suggests that while these delimiters are less critical in MM-ICL, the modality
switch inherent to MM-ICL may serve as an implicit delimiter, compensating for the absence of
explicit delimiters."
RELATED WORK,0.38226744186046513,"6
Related Work"
RELATED WORK,0.38372093023255816,"Recent advancements in vision large language models (VLLMs) have achieved great success in
various vision-language tasks [Yin et al., 2023, Wu et al., 2024a,b, Wang et al., 2024, Fei et al.,
2024b]. Initially, VLLMs lack Multi-modal In-context Learning (MM-ICL) capabilities. To address
this, researchers explore incorporating MM-ICL directly into the training phase. This involves
constructing training samples with multi-modal interleaved data by manual and general templates,
which unlock the MM-ICL capability [Alayrac et al., 2022, Awadalla et al., 2023]. Building on
this, Li et al. [2023b], Doveh et al. [2024] and Zhao et al. [2024] extend the MM-ICL to construct
a series of task-specific templates, which improves generalization for MM-ICL. Further, Li et al.
[2023a] introduce OtterHD and adapt the former process for high-definition images. The potential of
MM-ICL is further explored in scene text recognition, image generation, and game instructions [Zhao
et al., 2023b, Sun et al., 2023, Jin et al., 2024]."
RELATED WORK,0.3851744186046512,"Recognizing the effectiveness of MM-ICL, researches shift towards prompt optimization. These
methods focus on directly optimizing multi-modal prompts to understand the task and generate the
expected output, without parameter adjustments [Gong et al., 2023, Tsimpoukelli et al., 2021, Li
et al., 2023b]. This approach has significantly improved performance in visual reasoning tasks [Yang
et al., 2022, Zheng et al., 2023]. Another approach involves textualizing visual information to enable
VLLMs to leverage their background knowledge through in-context learning, further enhancing
visual reasoning [Yang et al., 2023, Lu et al., 2024, Gupta and Kembhavi, 2023, Shen et al., 2024]. In
addition, in order to better explore the MM-ICL, Zong et al. [2024] and Shukor et al. [2024] also
provide a dataset to test the MM-ICL capabilities of the multi-modal classification. Furthermore,
Shukor et al. [2024] take the first step to conduct an instruction modification exploration for MM-ICL."
RELATED WORK,0.3866279069767442,"Meanwhile, Baldassini et al. [2024], Chen et al. [2024c] pioneer the first naive multi-modal retrieval
exploration to enhance MM-ICL. Different from the existing work, our study mainly focuses on a
systematic exploration of the effectiveness of key factors influencing the effectiveness of MM-ICL
in a unified perspective. To this end, we conduct a detailed analysis and exploration on 6 VLLMs
and 20 factors across 4 tasks, aiming to provide systematic and practical guidance for future research."
DISCUSSION,0.38808139534883723,"7
Discussion"
DISCUSSION,0.38953488372093026,"Broader Impacts.
Our work is the first to systematically explore the factors influencing MM-ICL.
We aim to enhance the understanding of MM-ICL mechanisms and guide future developments in
this field. Additionally, our findings could foster a more comprehensive comprehension of MM-ICL
within the community. For social impact, this research may influence the creation of more effective
multi-modal large language models and relevant applications."
DISCUSSION,0.39098837209302323,"Limitations & Future Work.
Due to time and cost constraints, this work is limited to the explo-
ration of image and text modalities. In future research, we can extend our exploration to video modal
ICL and multi-lingual MM-ICL scenarios. Another limitation of this work involves the insufficient
consideration of certain image instructions, such as grounding or the inclusion of additional arrows.
These aspects often require more complex human input and are not adequately supported by most
current models."
CONCLUSION,0.39244186046511625,"8
Conclusion"
CONCLUSION,0.3938953488372093,"This study is the first to systematically explore MM-ICL by identifying key performance determinants.
Our experiments with 6 models and 20 factors across 4 tasks show that multi-modal retrieval
significantly outperforms single-modal approaches and the intra-demonstration ordering critically
influences learning efficacy. Additionally, incorporating task-specific instructions into prompts
enhances model performance. We hope these findings will refine our understanding of MM-ICL
mechanisms and guide more effective developments and future research in this evolving field."
CONCLUSION,0.3953488372093023,Acknowledgments
CONCLUSION,0.39680232558139533,"This work was supported by the National Natural Science Foundation of China (NSFC) via grant
62306342, 62236004, 62441603 and 62476073. This work was also sponsored by the Excellent
Young Scientists Fund in Hunan Province (2024JJ4070) and the Science and Technology Innovation
Program of Hunan Province under Grant 2024RC3024. We are grateful for resources from the High
Performance Computing Center of Central South University, and the CCF-Zhipu.AI Large Model
Innovation Fund (NO.CCF-Zhipu202406). Libo Qin is the corresponding author."
REFERENCES,0.39825581395348836,References
REFERENCES,0.3997093023255814,"Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-
context examples selection for machine translation. In Findings of the Association for Computa-
tional Linguistics: ACL 2023, pages 8857–8873, 2023."
REFERENCES,0.4011627906976744,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in neural information processing systems, 35:23716–23736,
2022."
REFERENCES,0.40261627906976744,"Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,
Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for
training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023."
REFERENCES,0.40406976744186046,"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization,
text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023."
REFERENCES,0.4055232558139535,"Folco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, and Benjamin Piwowarski.
What makes multimodal in-context learning work? In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1539–1550, 2024."
REFERENCES,0.4069767441860465,"Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawa-
har, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 4291–4301, 2019."
REFERENCES,0.40843023255813954,"Qiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the boundaries
of thought: A reasoning granularity framework to quantify and optimize chain-of-thought. arXiv
preprint arXiv:2410.05695, 2024a."
REFERENCES,0.40988372093023256,"Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3CoT: A novel
benchmark for multi-domain multi-step multi-modal chain-of-thought. In Lun-Wei Ku, Andre
Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 8199–8221, Bangkok, Thailand,
August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.446.
URL https://aclanthology.org/2024.acl-long.446."
REFERENCES,0.4113372093023256,"Shuo Chen, Zhen Han, Bailan He, Mark Buckley, Philip Torr, Volker Tresp, and Jindong Gu.
Understanding and improving in-context learning on vision-language models. In ICLR 2024
Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024c. URL"
REFERENCES,0.4127906976744186,https://openreview.net/forum?id=SB2sWF3oCw.
REFERENCES,0.41424418604651164,"Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015."
REFERENCES,0.41569767441860467,"Elena Deza, Michel Marie Deza, Michel Marie Deza, and Elena Deza. Encyclopedia of distances.
Springer, 2009."
REFERENCES,0.4171511627906977,"Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022."
REFERENCES,0.4186046511627907,"Sivan Doveh, Shaked Perek, M Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and
Leonid Karlinsky. Towards multimodal in-context learning for vision & language models. arXiv
preprint arXiv:2403.12736, 2024."
REFERENCES,0.42005813953488375,"Zhengfang Duanmu, Wentao Liu, Zhongling Wang, and Zhou Wang. Quantifying visual image
quality: A bayesian view. Annual Review of Vision Science, 7(1):437–464, 2021."
REFERENCES,0.42151162790697677,"Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu.
Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceedings of
the International Conference on Machine Learning, 2024a."
REFERENCES,0.42296511627906974,"Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified
pixel-level vision llm for understanding, generating, segmenting, editing. 2024b."
REFERENCES,0.42441860465116277,"Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhanc-
ing video-language representations with structural spatio-temporal alignment. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2024c."
REFERENCES,0.4258720930232558,"Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris
Papailiopoulos. Looped transformers as programmable computers. In Proc. of ICML, 2023."
REFERENCES,0.4273255813953488,"Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam
Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning.
In The Eleventh International Conference on Learning Representations, 2022."
REFERENCES,0.42877906976744184,"Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,
Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for
dialogue with humans. arXiv preprint arXiv:2305.04790, 2023."
REFERENCES,0.43023255813953487,"Google. Gemini: A family of highly capable multimodal models, 2023. URL https://storage.
googleapis.com/deepmind-media/gemini/gemini_1_report.pdf."
REFERENCES,0.4316860465116279,"Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017."
REFERENCES,0.4331395348837209,"Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning
without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14953–14962, 2023."
REFERENCES,0.43459302325581395,"Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and Heng Tao Shen. Icl-d3ie: In-context
learning with diverse demonstrations updating for document information extraction. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 19485–19494, 2023."
REFERENCES,0.436046511627907,"Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang
Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with
large language models. In The Twelfth International Conference on Learning Representations,
2023."
REFERENCES,0.4375,"Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 6700–6709, 2019."
REFERENCES,0.438953488372093,"Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W
Huang, Wenhu Chen, Zhaofeng He, et al. Read to play (r2-play): Decision transformer with
multimodal game instruction. arXiv preprint arXiv:2402.04154, 2024."
REFERENCES,0.44040697674418605,"Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep
Akata, and Thomas Lukasiewicz. e-vil: A dataset and benchmark for natural language explanations
in vision-language tasks. In Proceedings of the IEEE/CVF international conference on computer
vision, pages 1244–1254, 2021."
REFERENCES,0.4418604651162791,"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems, 35:
22199–22213, 2022."
REFERENCES,0.4433139534883721,"Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for
generating descriptive image paragraphs. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 317–325, 2017."
REFERENCES,0.44476744186046513,"Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open
web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information
Processing Systems, 36, 2024a."
REFERENCES,0.44622093023255816,"Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building
vision-language models? arXiv preprint arXiv:2405.02246, 2024b."
REFERENCES,0.4476744186046512,"Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A
high-resolution multi-modality model. arXiv preprint arXiv:2311.04219, 2023a."
REFERENCES,0.4491279069767442,"Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and
Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425,
2023b."
REFERENCES,0.45058139534883723,"Jian Li and Weiheng Lu. A survey on benchmarks of multimodal large language models. arXiv
preprint arXiv:2408.08632, 2024."
REFERENCES,0.45203488372093026,"Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,
Jingjing Xu, Xu Sun, et al. M 3 it: A large-scale dataset towards multi-modal multilingual
instruction tuning. arXiv preprint arXiv:2306.04387, 2023c."
REFERENCES,0.45348837209302323,"Xiaonan Li and Xipeng Qiu. MoT: Memory-of-thought enables ChatGPT to self-improve. In Proc.
of EMNLP, 2023a."
REFERENCES,0.45494186046511625,"Xiaonan Li and Xipeng Qiu. Finding support examples for in-context learning. In Findings of the
Association for Computational Linguistics: EMNLP 2023, pages 6219–6235, 2023b."
REFERENCES,0.4563953488372093,"Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as
algorithms: Generalization and stability in in-context learning, 2023d."
REFERENCES,0.4578488372093023,"Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What
makes good in-context examples for GPT-3? In Eneko Agirre, Marianna Apidianaki, and Ivan
Vuli´c, editors, Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
Knowledge Extraction and Integration for Deep Learning Architectures, pages 100–114, Dublin,
Ireland and Online, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/
2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10."
REFERENCES,0.45930232558139533,"Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out
(DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning
Architectures, pages 100–114, 2022b."
REFERENCES,0.46075581395348836,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.4622093023255814,"Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
science question answering. Advances in Neural Information Processing Systems, 35:2507–2521,
2022a."
REFERENCES,0.4636627906976744,"Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu,
and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models.
Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.46511627906976744,"Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
Fantastically
ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In
Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 8086–8098, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi:
10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556."
REFERENCES,0.46656976744186046,"Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered
prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 8086–8098, 2022c."
REFERENCES,0.4680232558139535,"Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual
question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf
conference on computer vision and pattern recognition, pages 3195–3204, 2019."
REFERENCES,0.4694767441860465,"Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document
images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision,
pages 2200–2209, 2021."
REFERENCES,0.47093023255813954,"Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In
Proc. of EMNLP, 2022."
REFERENCES,0.47238372093023256,"Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual
question answering by reading text in images. In 2019 international conference on document
analysis and recognition (ICDAR), pages 947–952. IEEE, 2019."
REFERENCES,0.4738372093023256,"OpenAI:, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report, 2023."
REFERENCES,0.4752906976744186,"Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, and Libo Qin. A preliminary evaluation of
chatgpt for zero-shot dialogue understanding. arXiv preprint arXiv:2304.04256, 2023."
REFERENCES,0.47674418604651164,"Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting:
Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing, pages 2695–2709, 2023."
REFERENCES,0.47819767441860467,"Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang
Che, and Philip S Yu. Large language models meet nlp: A survey. arXiv preprint arXiv:2405.12819,
2024."
REFERENCES,0.4796511627906977,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pages
8748–8763. PMLR, 2021."
REFERENCES,0.4811046511627907,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pages 2383–2392, 2016."
REFERENCES,0.48255813953488375,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115:211–252, 2015."
REFERENCES,0.48401162790697677,"Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
A-okvqa: A benchmark for visual question answering using world knowledge. In European
conference on computer vision, pages 146–162. Springer, 2022."
REFERENCES,0.48546511627906974,"Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.48691860465116277,"Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael
Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In
International Conference on Machine Learning, pages 31210–31227. PMLR, 2023."
REFERENCES,0.4883720930232558,"Mustafa Shukor, Alexandre Rame, Corentin Dancette, and Matthieu Cord.
Beyond task per-
formance: evaluating and reducing the flaws of large multimodal models with in-context-
learning. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=mMaQvkMzDi."
REFERENCES,0.4898255813953488,"Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for
image captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758. Springer,
2020."
REFERENCES,0.49127906976744184,"Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and
Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 8317–8326, 2019."
REFERENCES,0.49273255813953487,"Harald Steck, Chaitanya Ekanadham, and Nathan Kallus. Is cosine-similarity of embeddings really
about similarity? arXiv preprint arXiv:2403.05440, 2024."
REFERENCES,0.4941860465116279,"Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,
Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context
learners. arXiv preprint arXiv:2312.13286, 2023."
REFERENCES,0.4956395348837209,"Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen,
Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling
via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024."
REFERENCES,0.49709302325581395,"Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems
with language models. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=T6iiOqsGOh."
REFERENCES,0.498546511627907,"Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide
shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 9568–9578, 2024."
REFERENCES,0.5,"Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.
Multimodal few-shot learning with frozen language models. Advances in Neural Information
Processing Systems, 34:200–212, 2021."
REFERENCES,0.501453488372093,"Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4566–4575, 2015."
REFERENCES,0.502906976744186,"Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and
benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140,
2016."
REFERENCES,0.5043604651162791,"Peng Wang, Yongheng Zhang, Hao Fei, Qiguang Chen, Yukai Wang, Jiasheng Si, Wenpeng Lu, Min
Li, and Libo Qin. S3 agent: Unlocking the power of vllm for zero-shot multi-modal sarcasm
detection. ACM Transactions on Multimedia Computing, Communications and Applications, 2024."
REFERENCES,0.5058139534883721,"Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
Transactions on Machine Learning Research, 2022a."
REFERENCES,0.5072674418604651,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language
models. In Proc. of NeurIPS, 2022b."
REFERENCES,0.5087209302325582,"Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning, 2023."
REFERENCES,0.5101744186046512,"Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan.
Towards semantic equivalence of tokenization in multimodal llm. arXiv preprint arXiv:2406.05127,
2024a."
REFERENCES,0.5116279069767442,"Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal
llm. In Proceedings of the International Conference on Machine Learning, 2024b."
REFERENCES,0.5130813953488372,"Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An
information compression perspective for in-context example selection and ordering. In Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1423–1436, 2023."
REFERENCES,0.5145348837209303,"Yanzheng Xiang, Hanqi Yan, Lin Gui, and Yulan He. Addressing order sensitivity of in-context
demonstration examples in causal language models. arXiv preprint arXiv:2402.15637, 2024."
REFERENCES,0.5159883720930233,"Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, and Nan Duan. Bridgetower:
Building bridges between encoders in vision-language representation learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 37, pages 10637–10647, 2023."
REFERENCES,0.5174418604651163,"Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang.
An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 36, pages 3081–3089, 2022."
REFERENCES,0.5188953488372093,"Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning
and action. arXiv preprint arXiv:2303.11381, 2023."
REFERENCES,0.5203488372093024,"Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multi-
modal fact-checking and explanation generation: A challenging dataset and models. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 2733–2743, 2023."
REFERENCES,0.5218023255813954,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549, 2023."
REFERENCES,0.5232558139534884,"Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. In International Conference on Learning Representations, 2019."
REFERENCES,0.5247093023255814,"Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context
learning? Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.5261627906976745,"Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng
Wang, Wenjuan Han, and Baobao Chang. MMICL: Empowering vision-language model with multi-
modal in-context learning. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=5KojubHBr8."
REFERENCES,0.5276162790697675,"Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,
Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and
Ji-Rong Wen. A survey of large language models, 2023a."
REFERENCES,0.5290697674418605,"Zhen Zhao, Can Huang, Binghong Wu, Chunhui Lin, Hao Liu, Zhizhong Zhang, Xin Tan, Jingqun
Tang, and Yuan Xie. Multi-modal in-context learning makes an ego-evolving scene text recognizer.
arXiv preprint arXiv:2311.13120, 2023b."
REFERENCES,0.5305232558139535,"Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-
thought prompting for multimodal reasoning in language models. Advances in Neural Information
Processing Systems, 36:5168–5191, 2023."
REFERENCES,0.5319767441860465,"Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex
reasoning in large language models. In The Eleventh International Conference on Learning
Representations, 2022."
REFERENCES,0.5334302325581395,"Ziyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Weinan
Zhang, and Ting Liu. Through the lens of core competency: Survey on evaluation of large language
models. In The 22nd Chinese National Conference on Computational Linguistics: Frontier Forum,
page 88, 2023."
REFERENCES,0.5348837209302325,"Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of
benchmarking multimodal in-context learning. arXiv preprint arXiv:2403.13164, 2024."
REFERENCES,0.5363372093023255,Appendix
REFERENCES,0.5377906976744186,"A
The Implement Details for Standard Baseline"
REFERENCES,0.5392441860465116,"To ensure rigorous control of experimental variables, we establish a standard baseline for our study.
This baseline utilizes a multi-modal encoder for data representation and cosine similarity for sample
comparison, with retrieval restricted to the same task. The following sections provide detailed insights
into the implementation of this baseline."
REFERENCES,0.5406976744186046,"A.1
Demonstration Retrieval Implementation for Baseline"
REFERENCES,0.5421511627906976,"Text-modal Encoder for Sample Representation
We employ RoBERTa [Liu et al., 2019] as a
multi-modal encoder to represent the data in a unified embedding space. It is worth noting that since
the Image Caption task only contains a general task instruction, we will splice the field tags contained
in the meta data in M3IT before the insrtuction for comprehensive text representation."
REFERENCES,0.5436046511627907,"Multi-modal Encoder for Sample Representation
We employ BridgeTower [Xu et al., 2023] as
a multi-modal encoder to represent the data in a unified embedding space. This encoder integrates
both visual and textual information, prioritizing single modality to capture the rich semantic content
present in images."
REFERENCES,0.5450581395348837,"Cosine Similarity for Sample Comparison
To compare samples effectively, we use cosine sim-
ilarity, a metric that measures the cosine of the angle between two non-zero vectors in a multi-
dimensional space. This choice is motivated by its effectiveness in capturing the similarity between
high-dimensional vectors, which are typical outputs of our multi-modal encoder. Specifically, we
compute the cosine similarity between the query and each candidate sample, which is given by:"
REFERENCES,0.5465116279069767,"cosine(hq, hi) =
hq · hi
∥hq∥∥hi∥"
REFERENCES,0.5479651162790697,"where hq and hi are the embedding vectors of the query q and candidate sample xi, respectively."
REFERENCES,0.5494186046511628,"In-domain and Top-k Retrieval for Sample Selection
To ensure the relevance and accuracy of
the retrieval process, for sample selection, we first confine retrieval to the same task and domain. This
means that comparisons and rankings are conducted exclusively among samples within the same task
and domain category, ensuring the contextual appropriateness of the retrieved results."
REFERENCES,0.5508720930232558,"Dataset
Category"
REFERENCES,0.5523255813953488,"COCO Caption [Chen et al., 2015]
IC
TextCaps [Sidorov et al., 2020]
IC
Paragraph Captioning [Krause et al., 2017]
IC"
REFERENCES,0.5537790697674418,"COCO Text [Veit et al., 2016]
CLS
ImageNet Image Classification [Russakovsky et al., 2015]
CLS
IQA [Duanmu et al., 2021]
CLS
COCO-ITM [Chen et al., 2015]
CLS
e-SNLI-VE [Kayser et al., 2021]
CLS
Mocheg [Yao et al., 2023]
CLS"
REFERENCES,0.5552325581395349,"VQA-v2 [Goyal et al., 2017]
VQA
DocVQA [Mathew et al., 2021]
VQA
OCR-VQA [Mishra et al., 2019]
VQA
ST-VQA [Biten et al., 2019]
VQA
Text-VQA [Singh et al., 2019]
VQA
GQA [Hudson and Manning, 2019]
VQA
OKVQA [Marino et al., 2019]
VQA
A-OKVQA [Schwenk et al., 2022]
VQA"
REFERENCES,0.5566860465116279,"ScienceQA [Lu et al., 2022a]
R
M3CoT [Chen et al., 2024b]
R"
REFERENCES,0.5581395348837209,"Table 2: Dataset in M3IT and M3CoT, where IC: Image Captioning, CLS: Classification, VQA:
Visual Question Answering, R: Chain-of-Thought Reasoning (with NL rationale). Due to the cost,
for each task, we evenly sampled 500 items according to the sub-dataset."
REFERENCES,0.559593023255814,"In addition, for sample selection, samples are ranked according to their cosine similarity scores.
Higher similarity scores indicate a closer alignment with the query sample, enabling the efficient
identification of the most relevant samples. This ranking process involves two main steps: (1) Sorting:
Candidate samples are sorted in descending order based on their cosine similarity scores relative to
the query. (2) Selection: Subsequently, the top-k ranked samples are selected based on their relevance
as determined by the similarity scores."
REFERENCES,0.561046511627907,"A.2
Demonstration Ordering Implementation for Baseline"
REFERENCES,0.5625,"By default, we utilize the methodology for ordering demonstrations within our baseline model. By
default, we adopt a text-after-image (Text-Image) approach for intra-demonstration sorting. This
means that, within a single demonstration, textual information is positioned after the corresponding
image. This ordering is chosen based on preliminary findings suggesting that such a sequence aids in
better contextual understanding and retention of the demonstrated information."
REFERENCES,0.563953488372093,"Furthermore, for the ordering of inter-demonstration sequences, we employ a similarity-based
method. This method ranks demonstrations according to their similarity to the query, with more
similar demonstrations placed higher in the order. The similarity is determined using a metric that
assesses the alignment of key features between the query and the demonstrations. This approach
ensures that the most relevant and contextually aligned demonstrations are prioritized, potentially
enhancing the model’s performance and the user’s comprehension."
REFERENCES,0.565406976744186,"A.3
Prompt Construction Implementation for Baseline"
REFERENCES,0.5668604651162791,"To ensure consistency and comparability in our baseline, we introduce both a delimiter and a 3-shot
setting (following Wei et al. [2022b], Qin et al. [2023]). The delimiter serves to clearly demarcate
different segments of the input data, preventing any potential confusion or overlap between distinct
portions of the input. This clear separation is crucial for the model to accurately process and
understand the structure of the data it receives."
REFERENCES,0.5683139534883721,"The 3-shot setting, on the other hand, involves providing three examples for each task within the
prompt. This approach is designed to stabilize the learning process by presenting the model with
sufficient contextual information. By offering three examples, we strike a balance between providing
enough context to guide the model’s understanding and avoiding the cognitive overload that might
occur with too many examples. This setting not only enhances the model’s performance but also
ensures a more robust and reliable learning process."
REFERENCES,0.5697674418604651,"A.4
Baseline Prompt"
REFERENCES,0.5712209302325582,"In the context of using Vision-and-Language Large Models (VLLMs), it is essential to carefully
structure the input prompts to ensure accurate processing. The prompt format typically used is
illustrated below:"
REFERENCES,0.5726744186046512,[REQUEST] % Shot 1
REFERENCES,0.5741279069767442,"<Visual Input Ivis
1
> <Textual Input Itxt
1 >"
REFERENCES,0.5755813953488372,[RESPONSE]
REFERENCES,0.5770348837209303,"<Textual Output Ivis
1
>"
REFERENCES,0.5784883720930233,[REQUEST] % Shot 2
REFERENCES,0.5799418604651163,"<Visual Input Ivis
2
> <Textual Input Itxt
2 >"
REFERENCES,0.5813953488372093,[RESPONSE]
REFERENCES,0.5828488372093024,"<Textual Output Ivis
2
>"
REFERENCES,0.5843023255813954,[REQUEST] % Shot 3
REFERENCES,0.5857558139534884,"<Visual Input Ivis
3
> <Textual Input Itxt
3 >"
REFERENCES,0.5872093023255814,[RESPONSE]
REFERENCES,0.5886627906976745,"<Textual Output Ivis
3
>"
REFERENCES,0.5901162790697675,[REQUEST] % User Query
REFERENCES,0.5915697674418605,"<Visual Input Ivis
q
> <Textual Input Itxt
q
>"
REFERENCES,0.5930232558139535,"where any gray text following the percent sign (%) is treated as a comment. These comments are not
processed as part of the primary input but serve to provide additional context or instructions within
the coding environment. This convention helps in maintaining the clarity and functionality of the
given prompting."
REFERENCES,0.5944767441860465,"In conclusion, the standard baseline established here integrates a multi-modal encoder, cosine
similarity, and task-specific retrieval with a focus on visual modalities. It ranks samples based on
similarity and employs a delimiter with a 3-shot setting to ensure robust and consistent performance
across different tasks."
REFERENCES,0.5959302325581395,"B
The Implement Details for Sample Comparison"
REFERENCES,0.5973837209302325,"B.1
Metric Calculation"
REFERENCES,0.5988372093023255,"Cosine Similarity (Mcos)
Compute the cosine similarity between hq and hj using the formula:"
REFERENCES,0.6002906976744186,"Mcos(hq, hj) =
hq · hj
∥hq∥∥hj∥
(7)"
REFERENCES,0.6017441860465116,"L2 Similarity (ML2)
Calculate the L2 similarity by computing the negative Euclidean distance
between hq and hj:
ML2(hq, hj) = −∥hq −hj∥2
(8)"
REFERENCES,0.6031976744186046,"Since Euclidean distance measures dissimilarity, we use the negative value to represent similarity,
where a higher value indicates greater similarity."
REFERENCES,0.6046511627906976,"Semantic Diversity (Mdiv)
Semantic diversity is assessed by evaluating the differences in the
distributional properties of hq and hj. This assessment involves analyzing the variance in how these
properties are distributed across different samples. To determine the presence of semantic diversity
within Multi-Modal In-Context Learning (MM-ICL), we adopt the methodology proposed by Li and
Qiu [2023b]. Specifically, we employ the ""diversity retriever,"" designed to enhance the diversity of
the selected samples. The diversity retriever operates by first selecting the top 10 samples based
on a preliminary measure of relevance. From these top 10 samples, it then identifies the 3 samples
that exhibit the highest semantic diversity. This two-step process ensures that the final selection of
samples for MM-ICL is not only relevant but also diverse in terms of their semantic content."
REFERENCES,0.6061046511627907,"B.2
Comparison and Analysis"
REFERENCES,0.6075581395348837,"Comparing the results obtained using different metrics (Mcos, ML2, Mdiv) provides a compre-
hensive understanding of their effectiveness and suitability for specific applications. It is essential
to analyze the trade-offs associated with each metric and interpret the results to draw meaningful
conclusions about sample quality and relevance."
REFERENCES,0.6090116279069767,"As shown in Figure 12, cosine similarity, which measures directional semantic alignment, emerges
as the superior metric in MM-ICL compared to L2 similarity. This observation is supported by the
findings of Deza et al. [2009] and Steck et al. [2024], who highlight that MM-ICL prioritizes semantic
directional consistency over complete semantic alignment. Cosine similarity’s ability to capture the
nuances of directional alignment allows for more precise interpretations of semantic relationships
within the data, making it particularly effective for MM-ICL tasks."
REFERENCES,0.6104651162790697,"In contrast, Figure 13 illustrates that while diversity, as measured by Mdiv, enhances performance in
text-based in-context learning, our experiments reveal limited improvement in MM-ICL tasks. This
finding suggests that diversity may not directly correlate with better performance in MM-ICL. The
limited impact of diversity on MM-ICL performance could be attributed to the specific nature of CIDER"
REFERENCES,0.6119186046511628,"100
50 RAS Acc. F1"
REFERENCES,0.6133720930232558,BERTScore
REFERENCES,0.6148255813953488,Token F1
REFERENCES,0.6162790697674418,Accuracy
REFERENCES,0.6177325581395349,BERTScore
REFERENCES,0.6191860465116279,(AVG: 53.99 → 68.16)
REFERENCES,0.6206395348837209,(a) Gemini-Pro
REFERENCES,0.622093023255814,"L2 Distance
Cosine Distance CIDER"
REFERENCES,0.623546511627907,"100
50 RAS Acc. F1"
REFERENCES,0.625,BERTScore
REFERENCES,0.626453488372093,Token F1
REFERENCES,0.627906976744186,Accuracy
REFERENCES,0.6293604651162791,BERTScore
REFERENCES,0.6308139534883721,(AVG: 63.64 → 66.80)
REFERENCES,0.6322674418604651,(b) IDEFICS2
REFERENCES,0.6337209302325582,"L2 Distance
Cosine Distance CIDER"
REFERENCES,0.6351744186046512,"100
50 RAS Acc. F1"
REFERENCES,0.6366279069767442,BERTScore
REFERENCES,0.6380813953488372,Token F1
REFERENCES,0.6395348837209303,Accuracy
REFERENCES,0.6409883720930233,BERTScore
REFERENCES,0.6424418604651163,(AVG: 58.63 → 62.22)
REFERENCES,0.6438953488372093,(c) Qwen-VL
REFERENCES,0.6453488372093024,"L2 Distance
Cosine Distance CIDER"
REFERENCES,0.6468023255813954,"100
50 RAS Acc. F1"
REFERENCES,0.6482558139534884,BERTScore
REFERENCES,0.6497093023255814,Token F1
REFERENCES,0.6511627906976745,Accuracy
REFERENCES,0.6526162790697675,BERTScore
REFERENCES,0.6540697674418605,(AVG: 37.75 → 44.60)
REFERENCES,0.6555232558139535,(d) Otter
REFERENCES,0.6569767441860465,"L2 Distance
Cosine Distance CIDER"
REFERENCES,0.6584302325581395,"100
50 RAS Acc. F1"
REFERENCES,0.6598837209302325,BERTScore
REFERENCES,0.6613372093023255,Token F1
REFERENCES,0.6627906976744186,Accuracy
REFERENCES,0.6642441860465116,BERTScore
REFERENCES,0.6656976744186046,(AVG: 37.52 → 41.03)
REFERENCES,0.6671511627906976,(e) Open-Flamingo
REFERENCES,0.6686046511627907,"L2 Distance
Cosine Distance"
REFERENCES,0.6700581395348837,Figure 12: The impact of the different similarity metrics.
REFERENCES,0.6715116279069767,"multi-modal data, where the interplay between different modalities requires a more nuanced approach
than simply maximizing diversity."
REFERENCES,0.6729651162790697,"Further analysis of these metrics reveals the inherent trade-offs between them. For instance, while
cosine similarity offers advantages in maintaining semantic directional consistency, it may not capture
the full extent of semantic similarity that L2 similarity can provide. On the other hand, L2 similarity,
though comprehensive in measuring complete alignment, might lack the precision needed for tasks
that rely heavily on directional semantic cues. Similarly, while diversity is beneficial in certain
contexts, its role in MM-ICL needs to be reconsidered, potentially focusing on optimizing other
aspects of sample quality."
REFERENCES,0.6744186046511628,"In summary, the evaluation of Mcos, ML2, and Mdiv underscores the importance of selecting
appropriate metrics based on the specific requirements of the task. Understanding the trade-offs and
context-specific effectiveness of these metrics is crucial for optimizing performance in multi-modal
in-context learning applications."
REFERENCES,0.6758720930232558,"C
Exploration of MM-ICL Prompt Construction"
REFERENCES,0.6773255813953488,"C.1
The Implement Details for Demonstration Sampling"
REFERENCES,0.6787790697674418,"To examine the effect of demonstration sample quantity on model performance, as shown in Figure 14,
we select a subset of k′ demonstrations from the demonstration list Lk′ to the prompt, where k′ is the
number of retrieved demonstrations. Formally, the prompt construction process is defined as:"
REFERENCES,0.6802325581395349,"P = I(δ(xσj
1), δ(xσj
2), . . . , δ(xσj
k′))
(9)"
REFERENCES,0.6816860465116279,We systematically evaluate the influence of varying k′ on MM-ICL performance.
REFERENCES,0.6831395348837209,"C.2
The Implement Details for Delimiter Injection"
REFERENCES,0.684593023255814,"To distinctly separate inputs and outputs within demonstrations xi, as shown in Figure 15, we
leverage special delimiter markers. Delimiters like [Request] and [Response] are strategically
placed before the inputs and outputs, respectively. Formally, delimiter injection function δ maps
inputs and outputs to the prompting sequences:"
REFERENCES,0.686046511627907,"δ(xσi) = [Request] ⊕Ii ⊕[Response] ⊕Oi,
(10) CIDER"
REFERENCES,0.6875,"100
50 RAS Acc. F1"
REFERENCES,0.688953488372093,BERTScore
REFERENCES,0.690406976744186,Token F1
REFERENCES,0.6918604651162791,Accuracy
REFERENCES,0.6933139534883721,BERTScore
REFERENCES,0.6947674418604651,(AVG: 66.55 → 68.16)
REFERENCES,0.6962209302325582,(a) Gemini-Pro
REFERENCES,0.6976744186046512,"Diversity Retriever
Similar Retriever CIDER"
REFERENCES,0.6991279069767442,"100
50 RAS Acc. F1"
REFERENCES,0.7005813953488372,BERTScore
REFERENCES,0.7020348837209303,Token F1
REFERENCES,0.7034883720930233,Accuracy
REFERENCES,0.7049418604651163,BERTScore
REFERENCES,0.7063953488372093,(AVG: 65.24 → 66.80)
REFERENCES,0.7078488372093024,(b) IDEFICS2
REFERENCES,0.7093023255813954,"Diversity Retriever
Similar Retriever CIDER"
REFERENCES,0.7107558139534884,"100
50 RAS Acc. F1"
REFERENCES,0.7122093023255814,BERTScore
REFERENCES,0.7136627906976745,Token F1
REFERENCES,0.7151162790697675,Accuracy
REFERENCES,0.7165697674418605,BERTScore
REFERENCES,0.7180232558139535,(AVG: 61.68→ 62.22)
REFERENCES,0.7194767441860465,(c) Qwen-VL
REFERENCES,0.7209302325581395,"Diversity Retriever
Similar Retriever CIDER"
REFERENCES,0.7223837209302325,"100
50 RAS Acc. F1"
REFERENCES,0.7238372093023255,BERTScore
REFERENCES,0.7252906976744186,Token F1
REFERENCES,0.7267441860465116,Accuracy
REFERENCES,0.7281976744186046,BERTScore
REFERENCES,0.7296511627906976,(AVG: 43.18 → 44.60)
REFERENCES,0.7311046511627907,(d) Otter
REFERENCES,0.7325581395348837,"Diversity Retriever
Similar Retriever CIDER"
REFERENCES,0.7340116279069767,"100
50 RAS Acc. F1"
REFERENCES,0.7354651162790697,BERTScore
REFERENCES,0.7369186046511628,Token F1
REFERENCES,0.7383720930232558,Accuracy
REFERENCES,0.7398255813953488,BERTScore
REFERENCES,0.7412790697674418,(AVG: 38.12 → 41.03)
REFERENCES,0.7427325581395349,(e) OpenFlamingo
REFERENCES,0.7441860465116279,"Diversity Retriever
Similar Retriever"
REFERENCES,0.7456395348837209,Figure 13: The impact of the utilization on diversity metrics.
REFERENCES,0.747093023255814,Validation Dataset (𝓥)
REFERENCES,0.748546511627907,Prompt (𝓟)
REFERENCES,0.75,Demonstration
REFERENCES,0.751453488372093,Sampling
REFERENCES,0.752906976744186,Prompt (𝓟)
REFERENCES,0.7543604651162791,Ordered List: ℒ
REFERENCES,0.7558139534883721,Ordered List: ℒ
REFERENCES,0.7572674418604651,Additional Sample: 𝑥𝑗1
REFERENCES,0.7587209302325582,Additional Sample: 𝑥𝑗2
REFERENCES,0.7601744186046512,"Sample: 𝑥1
…
Sample: 𝑥𝑗1"
REFERENCES,0.7616279069767442,"Sample: 𝑥𝑗2
…
Sample: 𝑥𝑁"
REFERENCES,0.7630813953488372,Figure 14: The demonstration sampling process for MM-ICL prompt construction.
REFERENCES,0.7645348837209303,"where Ii and Oi denotes the input and output for the sample xi, respectively. In addition, ⊕represents
string concatenation operation."
REFERENCES,0.7659883720930233,Prompting
REFERENCES,0.7674418604651163,Delimiter Injection
REFERENCES,0.7688953488372093,"Input: 𝑥1
Output: 𝑦1
[Request]
[Response]"
REFERENCES,0.7703488372093024,"Input: 𝑥1
Output: 𝑦1
Input: 𝑥2
Output: 𝑦2
Ordered"
REFERENCES,0.7718023255813954,List (ℒ)
REFERENCES,0.7732558139534884,Ordered
REFERENCES,0.7747093023255814,"List (ℒ)
Input: 𝑥2
Output: 𝑦2
[Request]
[Response]"
REFERENCES,0.7761627906976745,Figure 15: The delimiter injection process for MM-ICL prompt construction.
REFERENCES,0.7776162790697675,"C.3
The Implement Details for Instruction Injection"
REFERENCES,0.7790697674418605,"Visual Language Models (VLLMs) are known to be highly sensitive to input instructions, as demon-
strated by Kojima et al. [2022] and Qin et al. [2023]. Inspired by this observation, we aim to enhance CIDER"
REFERENCES,0.7805232558139535,"100
50 RAS Acc. F1"
REFERENCES,0.7819767441860465,BERTScore
REFERENCES,0.7834302325581395,Token F1
REFERENCES,0.7848837209302325,Accuracy
REFERENCES,0.7863372093023255,BERTScore
REFERENCES,0.7877906976744186,(AVG: 64.76 → 68.16)
REFERENCES,0.7892441860465116,(a) Gemini CIDER
REFERENCES,0.7906976744186046,"100
50 RAS Acc. F1"
REFERENCES,0.7921511627906976,BERTScore
REFERENCES,0.7936046511627907,Token F1
REFERENCES,0.7950581395348837,Accuracy
REFERENCES,0.7965116279069767,BERTScore
REFERENCES,0.7979651162790697,(AVG: 57.29 → 62.22)
REFERENCES,0.7994186046511628,(b) Qwen-VL CIDER
REFERENCES,0.8008720930232558,"100
50 RAS Acc. F1"
REFERENCES,0.8023255813953488,BERTScore
REFERENCES,0.8037790697674418,Token F1
REFERENCES,0.8052325581395349,Accuracy
REFERENCES,0.8066860465116279,BERTScore
REFERENCES,0.8081395348837209,(AVG: 38.66 → 41.03)
REFERENCES,0.809593023255814,(d) OpenFlamingo CIDER
REFERENCES,0.811046511627907,"100
50 RAS Acc. F1"
REFERENCES,0.8125,BERTScore
REFERENCES,0.813953488372093,Token F1
REFERENCES,0.815406976744186,Accuracy
REFERENCES,0.8168604651162791,BERTScore
REFERENCES,0.8183139534883721,(AVG: 44.41 → 44.60)
REFERENCES,0.8197674418604651,(c) Otter CIDER
REFERENCES,0.8212209302325582,"100
50 RAS Acc. F1"
REFERENCES,0.8226744186046512,BERTScore
REFERENCES,0.8241279069767442,Token F1
REFERENCES,0.8255813953488372,Accuracy
REFERENCES,0.8270348837209303,BERTScore
REFERENCES,0.8284883720930233,(AVG: 65.19 → 66.80)
REFERENCES,0.8299418604651163,(b) IDEFICS2 CIDER
REFERENCES,0.8313953488372093,"100
50 RAS Acc. F1"
REFERENCES,0.8328488372093024,BERTScore
REFERENCES,0.8343023255813954,Token F1
REFERENCES,0.8357558139534884,Accuracy
REFERENCES,0.8372093023255814,BERTScore
REFERENCES,0.8386627906976745,(AVG: 64.76 → 68.16)
REFERENCES,0.8401162790697675,(c) GPT4-V
REFERENCES,0.8415697674418605,"w/o Delimiter
w/ Delimiter
w/o Delimiter
w/ Delimiter
w/o Delimiter
w/ Delimiter"
REFERENCES,0.8430232558139535,"w/o Delimiter
w/ Delimiter
w/o Delimiter
w/ Delimiter
w/o Delimiter
w/ Delimiter"
REFERENCES,0.8444767441860465,"Figure 16: The impact of inserting delimiter into the input and output of demonstration on model
performance."
REFERENCES,0.8459302325581395,"task comprehension in Multi-Modal In-Context Learning (MM-ICL) by incorporating various instruc-
tions to explore their influence on performance. Formally, we develop instruction methods, denoted
as I(·), which describe the task and are integrated into the prompt construction process. The prompt
P is constructed as follows:"
REFERENCES,0.8473837209302325,"P = I(δ(xσ1), δ(xσ2), . . . , δ(xσk)),
(11)"
REFERENCES,0.8488372093023255,where δ(xσi) represents the transformation of the i-th demonstration example.
REFERENCES,0.8502906976744186,"Specifically, we have designed distinct instructions tailored to different types of tasks, ensuring clarity
and appropriateness for each unique context. For image captioning tasks, the prompt is:"
REFERENCES,0.8517441860465116,Please provide a caption for the image following the structure of the provided example.
REFERENCES,0.8531976744186046,"In this context, the objective is to generate descriptive captions that accurately reflect the content and
context of the image. For Visual Question Answering (VQA) tasks, our prompt is:"
REFERENCES,0.8546511627906976,Examine the image and answer the question by closely following the structure shown in the
REFERENCES,0.8561046511627907,example provided.
REFERENCES,0.8575581395348837,"The VQA tasks require the model to analyze visual content and respond to specific queries. By
following the example, users can produce answers that are precise and directly related to the visual
stimuli. For image classification tasks, the prompt is:"
REFERENCES,0.8590116279069767,"Carefully review the image and categorize it based on the options provided in [REQUEST],"
REFERENCES,0.8604651162790697,following the classification format illustrated in the example.
REFERENCES,0.8619186046511628,"Image classification involves categorizing images into predefined classes based on visual content. The
provided example demonstrates the expected classification format. For chain-of-thought reasoning
tasks, the prompt is:"
REFERENCES,0.8633720930232558,Carefully review the given image and the associated text. Utilize the reasoning format
REFERENCES,0.8648255813953488,"illustrated in the provided examples, breaking down your thought process. Ensure that each"
REFERENCES,0.8662790697674418,"reasoning step is explicitly connected to observable details in the image or text, and articulate"
REFERENCES,0.8677325581395349,your conclusion in a clear and logical manner.
REFERENCES,0.8691860465116279,"Chain-of-thought reasoning tasks require a more complex interaction between visual and textual
information. The prompt encourages users to break down their reasoning process into clear, logical
steps, each supported by specific details from the image or text."
REFERENCES,0.8706395348837209,"Furthermore, we explore three categories of instructions to enhance the MM-ICL process:"
REFERENCES,0.872093023255814,"Introductory Instruction (Iintro)
This instruction provides an overview of the task before present-
ing any demonstrations. As depicted in Figure 4 (a), the introductory instruction Iintro is positioned
at the beginning of the ordered demonstration list L. This setup aims to set the context for the
subsequent examples. Specifically, the overall prompt template is as follows:"
REFERENCES,0.873546511627907,<Instruction Iintro>
REFERENCES,0.875,[DEMONSTRATIONS]
REFERENCES,0.876453488372093,[REQUEST] % Shot 1
REFERENCES,0.877906976744186,"<Visual Input Ivis
1
> <Textual Input Itxt
1 >"
REFERENCES,0.8793604651162791,[RESPONSE]
REFERENCES,0.8808139534883721,"<Textual Output Ivis
1
> · · ·"
REFERENCES,0.8822674418604651,[QUERY]
REFERENCES,0.8837209302325582,[REQUEST] % User Query
REFERENCES,0.8851744186046512,"<Visual Input Ivis
q
> <Textual Input Itxt
q
>"
REFERENCES,0.8866279069767442,"Summative Instruction (Isum)
This instruction offers a summary after the examples, guiding the
model to apply the learned concepts to real-world problems. As shown in Figure 4 (b), the summative
instruction I is added at the end of the demonstration list L. This helps in reinforcing the learning
objectives and expected outcomes. Specifically, the overall prompt template is as follows:"
REFERENCES,0.8880813953488372,<Instruction Iintro>
REFERENCES,0.8895348837209303,[DEMONSTRATIONS]
REFERENCES,0.8909883720930233,[REQUEST] % Shot 1
REFERENCES,0.8924418604651163,"<Visual Input Ivis
1
> <Textual Input Itxt
1 >"
REFERENCES,0.8938953488372093,[RESPONSE]
REFERENCES,0.8953488372093024,"<Textual Output Ivis
1
> · · ·"
REFERENCES,0.8968023255813954,"In summary, <Instruction Isum>"
REFERENCES,0.8982558139534884,[QUERY]
REFERENCES,0.8997093023255814,[REQUEST] % User Query
REFERENCES,0.9011627906976745,"<Visual Input Ivis
q
> <Textual Input Itxt
q
>"
REFERENCES,0.9026162790697675,"Intra-demonstration Instruction (Iintra)
This instruction embeds task instructions within each
example, assisting the model in understanding the task requirements during the learning process. As"
REFERENCES,0.9040697674418605,"Model
OKVQA [Marino et al., 2019]
VQA-v2 [Goyal et al., 2017]"
REFERENCES,0.9055232558139535,"Accuracy
BERTScore
Token F1
Accuracy
BERTScore
Token F1"
REFERENCES,0.9069767441860465,"OpenFlamingo [Awadalla et al., 2023]
40.28
78.10
17.45
53.33
83.34
25.67
GPT4V [OpenAI: et al., 2023]
54.28
85.97
25.23
69.69
84.89
29.18
IDEFICS2 [Laurençon et al., 2024b]
55.32
87.61
27.81
71.28
87.98
35.46"
REFERENCES,0.9084302325581395,"Table 3: The correlation analysis of the indicators and reproducted accuracy. The results are obtained
by testing on a subset of the test set."
REFERENCES,0.9098837209302325,"illustrated in Figure 4 (c), the intra-demonstration instruction I is included within each demonstration
xi in the list L. This method ensures that the task instructions are continuously reinforced throughout
the learning process. Specifically, the overall prompt template is as follows:"
REFERENCES,0.9113372093023255,[DEMONSTRATIONS]
REFERENCES,0.9127906976744186,[REQUEST] % Shot 1
REFERENCES,0.9142441860465116,"<Visual Input Ivis
1
> <Textual Input Itxt
1 > <Instruction Iintra>"
REFERENCES,0.9156976744186046,[RESPONSE]
REFERENCES,0.9171511627906976,"<Textual Output Ivis
1
> · · ·"
REFERENCES,0.9186046511627907,[QUERY]
REFERENCES,0.9200581395348837,[REQUEST] % User Query
REFERENCES,0.9215116279069767,"<Visual Input Ivis
q
> <Textual Input Itxt
q
> <Instruction Iintra>"
REFERENCES,0.9229651162790697,"By systematically incorporating these instruction categories into the MM-ICL framework, we aim to
investigate their impact on model performance and task comprehension."
REFERENCES,0.9244186046511628,"D
Prompt Robust"
REFERENCES,0.9258720930232558,"In our preliminary experiments, we observed that variations in prompts do not significantly alter
the overall conclusions. Specifically, we employed multiple prompts—differing in instructions and
delimiters—while maintaining equivalent semantic content but varying linguistic expression. As
demonstrated in Table 4, the influence of these different prompts on the results is minimal. This
suggests that our findings are robust to changes in prompt formulation, thereby supporting the
reliability of the experimental outcomes."
REFERENCES,0.9273255813953488,"Caption
VQA
Classification
Reasoning
AVG
CIDER
BERTScore
Token F1
BERTScore
Acc
F1
Acc
RAS"
REFERENCES,0.9287790697674418,"P1
12.03
85.85
22.53
86.67
59.93
54.62
59.52
92.04
59.15
P2
14.01
86.77
23.59
86.00
58.53
53.61
61.85
91.86
59.53
P3
13.91
86.92
24.70
87.63
59.74
52.14
61.89
93.05
60.00
P4
14.44
86.48
23.14
87.77
60.23
50.48
60.54
92.27
59.42"
REFERENCES,0.9302325581395349,"Table 4: Performance across different prompts (i.e., P1, P2, P3 and P4)."
REFERENCES,0.9316860465116279,NeurIPS Paper Checklist
CLAIMS,0.9331395348837209,1. Claims
CLAIMS,0.934593023255814,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We have present our main claims and outline the paper’s contributions and
scope in lines 6-13 of the Abstract and 44-54 of the Introduction.
Guidelines:"
CLAIMS,0.936046511627907,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9375,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have discussed the limitations of our work in Section 7.
Guidelines:"
CLAIMS,0.938953488372093,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.940406976744186,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]"
CLAIMS,0.9418604651162791,"Justification: Our paper does not involve any proofs or assumptions.
Guidelines:"
CLAIMS,0.9433139534883721,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.9447674418604651,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: As shown in Section 3, Section 4 and Appendix, we have provided detailed
descriptions and analyses of the experimental setups for all our investigations.
Guidelines:"
CLAIMS,0.9462209302325582,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.9476744186046512,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.9491279069767442,Answer: [No]
CLAIMS,0.9505813953488372,"Justification: The code for exploratory prompt work generally does not need to be released,
and readers can easily use the prompts we report to directly reproduce the results."
CLAIMS,0.9520348837209303,Guidelines:
CLAIMS,0.9534883720930233,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
CLAIMS,0.9549418604651163,6. Experimental Setting/Details
CLAIMS,0.9563953488372093,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
CLAIMS,0.9578488372093024,Answer: [Yes]
CLAIMS,0.9593023255813954,"Justification: As detailed in Section 4, we have thoroughly described the experimental setups
for all our explorations."
CLAIMS,0.9607558139534884,Guidelines:
CLAIMS,0.9622093023255814,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9636627906976745,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9651162790697675,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9665697674418605,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9680232558139535,"Justification: Error bars are shown in Figure 5 and Figure 7, with an explanation of the error
variables provided in Section 4. However, we do not report error bars for all tasks due to the
high costs associated with human annotation and computational resource consumption."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9694767441860465,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9709302325581395,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9723837209302325,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9738372093023255,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: As detailed in Section 4, we outline the specific model compute resources
provided.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9752906976744186,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9767441860465116,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We are convinced that we comply with NeurIPS Code of Ethics.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9781976744186046,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9796511627906976,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9811046511627907,"Justification: We have addressed the broader impacts of our work in Section 7. Additionally,
as our research is primarily an empirical exploration and poses no additional social risks, we
have not included a discussion on potential harmfulness.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9825581395348837,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9840116279069767,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9854651162790697,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9869186046511628,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9883720930232558,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9898255813953488,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9912790697674418,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9927325581395349,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9941860465116279,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9956395348837209,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.997093023255814,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.998546511627907,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
