Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004273504273504274,"Personalized federated learning aims to ﬁnd a shared global model that can be
adapted to meet personal needs on each individual device. Starting from such a
shared initial model, devices should be able to easily adapt to their local dataset to
obtain personalized models. However, we ﬁnd that existing works cannot generalize
well on non-iid scenarios with different heterogeneity degrees of the underlying
data distribution among devices. Thus, it is challenging for these methods to train
a suitable global model to effectively induce high-quality personalized models
without changing learning objectives. In this paper, we point out that this issue can
be addressed by balancing information ﬂow from the initial model and training
dataset to the local adaptation. We then prove a theorem referred to as the adaptive
trade-off theorem, showing adaptive local adaptation is equivalent to optimizing
such information ﬂow based on the information theory. With these theoretical
insights, we propose a new framework called adaptive federated meta-learning
(AFML), designed to achieve generalizable personalized federated learning that
maintains solid performance under non-IID data scenarios with different degrees of
diversity among devices. We test AFML in an extensive set of these non-IID data
scenarios, with both CIFAR-100 and Shakespeare datasets. Experimental results
demonstrate that AFML can maintain the highest personalized accuracy compared
to alternative leading frameworks, yet with a minimal number of communication
rounds and local updates needed."
INTRODUCTION,0.008547008547008548,"1
INTRODUCTION"
INTRODUCTION,0.01282051282051282,"In recent years, research interests in training a machine learning model on edge devices motivated the
paradigm of federated learning (FL) Li et al. (2020), which makes it feasible for multiple devices
to collaboratively train a shared global model in a privacy-preserving manner. Recent works Jiang
et al. (2019); Fallah et al. (2020); Mansour et al. (2020) explored personalized federated learning
(PFL) Kulkarni et al. (2020), which aims to produce high-quality personalized models based on
device-speciﬁc datasets and objectives. Speciﬁcally, each device uses the global model shared by
the central server for its initial local model. Then, in the local adaptation Yu et al. (2020), it trains a
personalized model by performing several local updates with respect to their own data."
INTRODUCTION,0.017094017094017096,"On the one hand, Jiang et al. (2019); Arivazhagan et al. (2019) trained a global model to obtain the
best accuracy on the whole data distribution of all participants. Thus, such global model with high
optimality can be used as a strong start initialization point in personalization for each client with a
subset data distribution. On the other hand, the works Fallah et al. (2020); Khodak et al. (2019); Chen
et al. (2019); Jiang et al. (2019) tended to train a global model with high adaptability that current
or new devices can easily adapt to their datasets in their local adaptation. These approaches are
expected to produce high-quality personalized models given the heterogeneity of the underlying data
distribution for all the users, i.e., non independent and identically distributed (non-iid) data."
INTRODUCTION,0.021367521367521368,"However, we observe that existing frameworks lack the capability to adapt or generalize well between
various non-iid data scenarios with different degrees of data and objectives diversity among devices.
The learning efﬁciency and personalized performance of existing approaches are not perfectly
performed to the changes in such diversity of non-iid data scenarios. With quantitative experimental"
INTRODUCTION,0.02564102564102564,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029914529914529916,"results, we clearly point out that this is caused by targeting an unsuitable trade-off between the
optimality and adaptability of the global model, which is referred to as the inappropriate optimality-
adaptability trade-off."
INTRODUCTION,0.03418803418803419,"The most recent works Hanzely & Richt´arik (2020); Mansour et al. (2020); Deng et al. (2020)
potentially alleviated this issue by mixing the global model and personalized local models in one
objective function for joint optimization. However, these works lack a clear insight of generalized PFL
and optimality-adaptability trade-off. In addition, the proposed methods relied upon the combination
formula with mixing weights that are hard to update in different non-IID data scenarios."
INTRODUCTION,0.038461538461538464,"In this paper, we rethink the local adaptation primarily from an information theory perspective
Yin et al. (2020). We argue that the information required to learn a personalized model φ in local
adaptation is derived from both the initial model θ and the local dataset D. Thus, high I(φ; θ) and
I(φ; D) correspond to the global model trained to be high optimality and adaptability on the whole
data distribution, respectively. This leads us to identify suitable trade-offs in different on-IID data
scenarios by balancing two information ﬂow sources. Therefore, with the idea of avoiding the local
update from solely relying on one information source, we formalize the learning process with an
optimization problem by adding mutual information constraints. The key insight of this approach is
that it can maintain performance by adaptively adjusting two information ﬂows in the local adaptation
based on the non-IID data condition."
INTRODUCTION,0.042735042735042736,"Our mathematical analysis of this optimization problem further gives a lower bound that consists
of the meta-learning training loss Finn et al. (2017) and three regularizers. Our proposed adaptive
trade-off theorem merges these principles to an embodiment. It theoretically ensures that our major
contribution in this paper is to achieve generalizable PFL with adaptive local adaptation."
INTRODUCTION,0.04700854700854701,"These theoretical insights lead to our design of a novel adaptive federated meta-learning (AFML)
framework. The basic learning architecture of AFML naturally inherits the meta-learning PFL
framework that has been tested by fed-MAML and proven to converge Fallah et al. (2020). Then, the
corresponding training algorithm is proposed based on deduced regularization terms of the theorem.
We tested AFML on CIFAR-100 and Shakespeare datasets. Our experimental results on non-iid
scenarios with extensive degrees of diversity show that AFML maintains the highest personalized
accuracy. Moreover, AFML achieves a reduction in required communication rounds by 25.5% and
11.6%, and a reduction in communication cost by 1.6 and 3.9 times compared with alternative leading
PFL frameworks."
PROBLEM STATEMENT,0.05128205128205128,"2
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.05555555555555555,"We focus on the standard personalized federated learning (PFL) problem, which aims to train a
global model utilized as the initialization to produce the personalized model based on the local data
of each client. In practical applications, each participating client can contain a local dataset with
personal preferences and objectives. Limited by the privacy constraint, we cannot access the whole
data distribution of clients to predetermined the global model targeted for optimality or adaptability
to facilitate the local adaptation in each client. Thus, the major problem is to adaptively achieve
the suitable trade-off between these two objectives of the global model, thus maintaining the solid
personalized local models under non-IID data scenarios with different degrees of diversity among
clients."
PROBLEM STATEMENT,0.05982905982905983,"Basically, there are C clients with the whole data distribution (X, Y) in the decentralized system. For
an individual device with index j ∈C, we assume the device-speciﬁc task objective Γj is sampled
from a task objective distribution p(Γ), which follows the deﬁnition of the meta-learning Triantaﬁllou
et al. (2019). The local dataset M j of this client contains the train dataset Dj :
n
(x, y)jo
and the"
PROBLEM STATEMENT,0.0641025641025641,"test dataset D′j :
n
(x′, y′)jo
that are disjoint. Both of them are sampled from the client-speciﬁc data"
PROBLEM STATEMENT,0.06837606837606838,"distribution Dj, D′j ∼(X, Y)j ⊂(X, Y) and have N j and N ′j samples, respectively. Besides, PFL
aims to train a shared global model θ that is regarded as the initialization for producing high-quality
personalized models φ1, ..., φC."
PROBLEM STATEMENT,0.07264957264957266,"Non-iid data with diversity quantity ζ. The basic non-IID scenario utilized in our work is the
distribution-based label non-IID data setting. The distribution of classes within one client follows the"
PROBLEM STATEMENT,0.07692307692307693,Under review as a conference paper at ICLR 2022
PROBLEM STATEMENT,0.0811965811965812,"Dirichlet distribution with parameter 0.5. We deﬁne a mutually exclusive set in which each client
augments the samples and randomly relabels the assigned classes. Thus, any two clients in this set
potentially have different learning objectives and data distribution. Then, let ζ denote the proportion
of non-mutually-exclusive devices in all C devices. Thus, the diversity of the non-IID data can be
quantiﬁed as 1 −ζ. ζ = 1 means that local datasets of participating clients are subsets of the whole
data distribution (X, Y), which deﬁnes a general non-IID data scenario in the FL."
PROBLEM STATEMENT,0.08547008547008547,"Then, the θ is trained to get good personalized models by using minθ L(θ) := 1 C
P"
PROBLEM STATEMENT,0.08974358974358974,"j Lj  
φj
, φj :=
θ −ηl▽Lj(θ) where ηl is the update rate and Lj : Rd →R is the loss function of M j in device j.
Thus, in local adaptation, the quality of φj depends on θ and M j. On the one hand, when ζ = 1, the
learning of φj beneﬁts from training θ to get optimality on the large global dataset

M j	C
j=1. On the
other hand, a θ with high adaptability can be easily adapted to the local data in each client to obtain
better φj when ζ = 0."
PROBLEM STATEMENT,0.09401709401709402,"The object of our generalizable PFL is to obtain high-quality personalized models efﬁciently under
non-IID data scenarios with any diversity ζ ∈[0, 1]. We argue that adaptive local adaptation induces
a suitable trade-off between the optimality and adaptability of the global model, which contributes to
generalizable PFL. Thus, our goal is to achieve generalizable PFL by implementing adaptive local
adaptation."
METHODOLOGY,0.09829059829059829,"3
METHODOLOGY"
METHODOLOGY,0.10256410256410256,"In this section, we ﬁrst present the issue of solely targeting the adaptability or optimality of the
global model in leading works. Then, we provide insights into this issue through experiments and
mathematical analysis. Based on the ﬁndings, we prove a theorem referred to as the adaptive trade-off
theorem, which motivates the design of the adaptive federated meta-learning (AFML) framework."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.10683760683760683,"3.1
ANALYSIS THE ISSUE FROM EXPERIMENTS"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1111111111111111,"500
1000
1500
0.10 0.15 0.20 0.25 0.30 = 0.1"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.11538461538461539,"1000
1500
2000
0.1 0.2 0.3 0.4 0.5 = 0.4"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.11965811965811966,"1500
1750
2000
2250
2500
0.2 0.3 0.4 0.5 0.6 = 0.6"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.12393162393162394,"1500
2000
0.2 0.3 0.4 0.5 0.6 0.7 = 0.9"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1282051282051282,Cifar100 learning curve of personalization
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.13247863247863248,"FedAvg-FT, K=3
FedAvg-FT, K=10
FedAvg-Meta, 80% support
FedAvg-Meta, 20% support"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.13675213675213677,"(a) Accuracy curve for two federated
models under different ζ."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.14102564102564102,"0 1
3
5
7
9
11
0.05 0.10 0.15 0.20 0.25 0.30"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1452991452991453,Test Accuracy = 0.1
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.14957264957264957,"0 1
3
5
7
9
11
0.1 0.2 0.3 0.4 0.5 = 0.4"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.15384615384615385,"0 1
3
5
7
9
11
personalization epochs 0.2 0.3 0.4 0.5 0.6"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1581196581196581,Test Accuracy = 0.6
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1623931623931624,"0 1
3
5
7
9
11
personalization epochs 0.3 0.4 0.5 0.6 0.7 = 0.9"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.16666666666666666,"FedAvg-FT, K=3
FedAvg-FT, K=10"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.17094017094017094,"FedAvg-Meta, 80% support
FedAvg-Meta, 20% support"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1752136752136752,"(b) Personalized test accuracy under
different personalization steps and ζ."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.1794871794871795,"30
35
40
45
50
55
60
65
70 5 10 15 20 25 30 35 40 45"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.18376068376068377,"Acc_local
Acc_per = 0.1"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.18803418803418803,"30
35
40
45
50
55
60
65
70 10"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.19230769230769232,"5
0
5
10
15
20
25
30
35
40
= 0.4"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.19658119658119658,"30
35
40
45
50
55
60
65
70
Acc_local 25 20 15 10 5 0 5 10 15"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.20085470085470086,"Acc_local
Acc_per = 0.6"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.20512820512820512,"30
35
40
45
50
55
60
65
70
Acc_local 35 30 25 20 15 10 5 0"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.2094017094017094,"5
= 0.9"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.21367521367521367,"FedAvg-FT, Pers=5
FedAvg-Meta, Pers=5"
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.21794871794871795,"(c) Accuracy of two federated mod-
els vs. local, trained-from-scratch
models under different ζ."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.2222222222222222,"Figure 1: Illustration of the issue in the FedAvg-FT and FedAvg-Meta by performing experiments
under non-IID data scenarios with different degrees of diversity ζ = 0.1, 0.4, 0.6, 0.9 among clients.
The E and Pers denote the local update steps and personalization steps, respectively. s% support in
FedAvg-Meta is the proportion of support set used for training."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.2264957264957265,"Our conducted experiments of the CIFAR100 dataset shown in Fig. 1 present the unstable perfor-
mance of optimality-oriented with ﬁne-tuning method FedAvg-FT Jiang et al. (2019) and adaptability-
oriented FedAvg-Meta Chen et al. (2019) method in terms of the averaged personalized accuracy of
clients and the learning efﬁciency. The local models were trained locally on clients’ own data for 200
epochs with a learning rate of 0.005."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.23076923076923078,"Both two methods fail when there is a high diversity among clients, i.e. ζ = 0.1. The upper left
subﬁgure of Eq. 1 (a), (b) presents that they cannot converge while Eq. 1 (c) further shows that they
obtain a bad personalized accuracy as compared with the model trained locally."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.23504273504273504,Under review as a conference paper at ICLR 2022
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.23931623931623933,"Toward optimality. FedAvg-FT obtains low accuracy and training speed in the medium diversity
conditions, as shown by Fig. 1 ζ = 0.4, 0.6. This illustrates that the initial model trained to be
optimal in the whole data distribution is hard to adapt to the local dataset. For example, Fig. 1(b)
shows that the corresponding accuracy of the personalized model improves less than 0.1 than the
initial one. But when there is low diversity ζ = 0.9, FedAvg-FT outperforms the adaptability-oriented
method with a large margin in all metrics. Especially in Fig Fig. 1(b), personalization of the FedAvg-
FT can start from a powerful initial model to reach the accuracy of 0.6 in only seven personalization
epoches."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.24358974358974358,"Toward adaptability. FedAvg-Meta can quickly converge to the best accuracy when the diversity is
relatively high. Fig. 1(b) shows that the initial model with high adaptability can be trained with the
local dataset to have a 0.3 improvement in accuracy within 7 personalization steps. Otherwise, the
personalization of FedAvg-Meta presents an opposite trend when the diversity is low ζ = 0.9. It is
trained by 1500 communication rounds to obtain an unstable convergence while requiring 11 epochs
to update the initial model with low performance to reach a competitive accuracy."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.24786324786324787,"Fig. 1(c) further shows that both two methods are less accurate than the local models for the majority
of participants when the diversity is high ζ = 0.1, 0.4, 0.6. But consistent with the above analysis,
the FedAvg-Meta is signiﬁcantly better than the FedAvg-FT. This means that most clients can update
the initial model with high adaptability to obtain a high-quality personal model. However, starting
from a powerful initial model is preferable when ζ = 0.9 because it is hard to enhance a weak initial
model based on the small-size local dataset."
ANALYSIS THE ISSUE FROM EXPERIMENTS,0.25213675213675213,"Both methods is obviously not conducive to generalizable PFL. When the diversity among clients
is high, targeting the global model to perform well on the whole data distribution (i.e., optimality)
makes it hard to be adapted to the local dataset. On the contrary, even though the global model can
be highly adaptable, updating a weak model with respect to a small-scale local dataset produces
a poor personalized model and damages the efﬁciency. As pointed by experimental results, solely
targeting optimality (FedAvg-FT) or adaptability (FedAvg-Meta) of the global model leads to an
unstable performance in non-IID data scenarios with different diversities. Therefore, this motivates
us to propose the idea of adaptive local adaptation in which the global model is to train to obtain a
suitable trade-off between optimality and adaptability based on the non-IID data condition."
INSIGHTS THROUGH THE INFORMATION THEORY,0.2564102564102564,"3.2
INSIGHTS THROUGH THE INFORMATION THEORY"
INSIGHTS THROUGH THE INFORMATION THEORY,0.2606837606837607,"In the local adaptation of PFL, the initial model is updated based on the local data. Thus, from the
information theory perspective, the information for learning the personalized model derives from the
initial model and the local dataset. This makes us give insights into the discussed issue through the
information ﬂow in the local adaptation."
INSIGHTS THROUGH THE INFORMATION THEORY,0.26495726495726496,"When ζ →1, FedAvg-Meta with the meta-learning framework, tends to produce a complex global
model with parameters θ, which will only overﬁt datasets in part of clients containing similar task
objective Γ. This phenomenon is deﬁned as task overﬁtting in the meta-learning research Yin et al.
(2020). Thus, in the local adaptation, θ effectively performs the test dataset without utilizing local
training samples. This can be denoted as q(y′|x′, φ)q(φ|D, θ) = q(y′|x′, φ)q(φ|θ), in which the
local model φ is independent of the dataset D in the client, such that q(y′|x′, θ, D) = q(y′|x′, θ)
where y′ is the predicted label. Then, the participanting clients is unable to adjust the initial model
based on personal local dataset."
INSIGHTS THROUGH THE INFORMATION THEORY,0.2692307692307692,"In contrast, when there is high diversity ζ →0 among clients, solely targeting the optimality in
FedAvg-FT makes weights of the global model provide sparse prior information for personalized
learning. The main reason is that the local data distribution deviates a lot from the global distribution.
For instance, in the extreme case, the shared global model can be regarded as randomly initialized
weights for each client to train on their own small-size local dataset. Thus, the training for a
personalized model heavily relies on the information of the local dataset. This can be denoted as
q(y′|x′, φ, θ)q(φ|D, θ) = q(y′|x′, φ)q(φ|D), in which weights φ of the local model only dependent
on the local dataset D. Personalized learning is equivalent to training random weights based on D,
which damages the training speed and the performance."
INSIGHTS THROUGH THE INFORMATION THEORY,0.27350427350427353,"An unsuitable trade-off hinders the generalizable PFL by causing the over-dependence on the global
model or the local dataset in local adaptation. Therefore, we argue that the generalizable PFL can be"
INSIGHTS THROUGH THE INFORMATION THEORY,0.2777777777777778,Under review as a conference paper at ICLR 2022
INSIGHTS THROUGH THE INFORMATION THEORY,0.28205128205128205,"achieved by implementing the adaptive local adaptation that is formulated to balance the contributions
of these two information sources in personalization."
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.2863247863247863,"3.3
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS !"" #(Γ) !' !( ) *' +
,
- Γ'"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.2905982905982906,".'
+
,
*' ) .' / , +"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.2948717948717949,".'0
,1 / +1 Γ'"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.29914529914529914,"Edge devices …
…"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3034188034188034,Server
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3076923076923077,Communication
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.31196581196581197,Communication
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3162393162393162,Communication
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.32051282051282054,"TRAIN 
TEST"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3247863247863248,Observed
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.32905982905982906,"Optimized
Latent"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3333333333333333,Bayesian Inference
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.33760683760683763,"Figure 2: The graphical model that expresses the
learning process in the personalized FL and our
proposed method."
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3418803418803419,"The Bayesian inference shown by the graphical
model in Fig. 2 TRAIN part presents the pre-
diction process of the personalized local model
φj. The initial model θ is updated with the local
dataset Dj to generate the φj, and then the x′ is
classiﬁed by the φj. Thus, predicted label y′ is
is obtained based on information derived from
training data D (i.e., D →φj →y′) and θ (i.e.,
θ →y′). As discussed in Section 3.2, target-
ing the optimality and adaptability of the global
model corresponds to over-depending on the
former and latter information path in the local
adaptation, respectively. These, we can achieve
the adaptive local adaptation by balancing these
two information ﬂows."
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.34615384615384615,"Motivated by the work in Yin et al. (2020) that
utilizes the mutual information to express the
relationship between variables, we naturally for-
mulate the dependencies on two information
ﬂows as the mutual information I(φ; θ) and I(φ; D). Therefore, the optimal information ﬂow
balance can be obtained by solving a constrained optimization problem, shown in Eq. 1. This problem
aims to prevent the learning process from solely relying on one single information ﬂow."
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3504273504273504,"max I(y′; z′|θ, D)"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.3547008547008547,"s.t.I(y′, D; θ|x′) ≤Ic
(1)"
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.358974358974359,"where Ic is the constant information constraint and z′ is a intermediate variable that is the hidden
representation of the input x′."
ADAPTIVE LOCAL ADAPTATION VIA BALANCING INFORMATION FLOWS,0.36324786324786323,"Given θ and D, the information ﬂow path θ →y′ is motivated by maximizing mutual information
between the target y′ and the intermediate variable z that is generated by θ. This encourages to train
a powerful global model that performs well on the global distribution. However, given the input
x′, we limit the dependence of local learning on θ by restricting the mutual information between
the prediction and θ in the reasoning process. This mechanism encourages the initial model not
to excessively store information of device-speciﬁc data in its parameter θ, thereby increasing the
importance of the local dataset in learning. Such constrain induces the global model with high
adaptability."
AN ADAPTIVE TRADE-OFF THEOREM,0.36752136752136755,"3.4
AN ADAPTIVE TRADE-OFF THEOREM"
AN ADAPTIVE TRADE-OFF THEOREM,0.3717948717948718,"Here, we solve the optimization problem in Eq . 1 by introducing a Lagrange multiplier β. Then,
the problem is transformed into maximizing the formula, △= I(y′; z, |θ, D) −βI(y′, D; θ|x′).
Then, by modeling the θ as the stochastic variable following a Gaussian distribution, the variational
inference is utilized to get the solution. After the mathematical derivation shown in Appendix A, we
can determine a lower bound of this formula as:"
AN ADAPTIVE TRADE-OFF THEOREM,0.37606837606837606,"△≥Ex′,y′Eϵ∼N(0,I) [log q(y′|x′, θ, ϵ)]"
AN ADAPTIVE TRADE-OFF THEOREM,0.3803418803418803,"−E [KL(p(z|x′, θ)||r(z))] −βE [KL(p(θ|D, x′, y′)||r(θ)]"
AN ADAPTIVE TRADE-OFF THEOREM,0.38461538461538464,+ I(x′; y′|θ) −I(y′; D|θ) (2)
AN ADAPTIVE TRADE-OFF THEOREM,0.3888888888888889,"where r(θ) ∼N(0, I) is a variational approximation to the target distribution θ. In the ﬁrst term of
the equation, we present the detailed operation of the reparameterization trick while we ignore this
detail in the following terms."
AN ADAPTIVE TRADE-OFF THEOREM,0.39316239316239315,Under review as a conference paper at ICLR 2022
AN ADAPTIVE TRADE-OFF THEOREM,0.3974358974358974,"If the I(x′; y′|θ) is 0, the model predictions do not depend on the given x′, leading to low accuracy.
Thus, maximizing I(x′; y′|θ) term is equivalent to minimize training losses for high accuracy.
Similarly, the high accuracy across clients leads to model predictions that are less dependent on local
training data sets, making a smaller I(y′; D|θ). Therefore, we conclude that maximizing the lower
bound in Eq . 2 can be replaced by maximizing training accuracy and three regularizers shown by the
theorem below."
AN ADAPTIVE TRADE-OFF THEOREM,0.4017094017094017,"Theorem 1 (Adaptive trade-off theorem). In non-IID data scenarios with different diversity, a
suitable trade-off for learning high-quality personalization models is obtained by targeting the
following equation during the training process."
AN ADAPTIVE TRADE-OFF THEOREM,0.405982905982906,"max
θ
Ex′Eϵ∼N(0,I) [log q(y′|x′, θ, ϵ)] −E [KL(p(z|x′, θ)||r(z))]"
AN ADAPTIVE TRADE-OFF THEOREM,0.41025641025641024,"−βE [KL(p(θ|D, x′, y′)||r(θ)] −E(x,y)∼pj(D)

Lj(θ; x, y)
"
AN ADAPTIVE TRADE-OFF THEOREM,0.41452991452991456,"−E(x′,y′)∼pj(D′))

Lj(φj; x′, y′)

(3)"
AN ADAPTIVE TRADE-OFF THEOREM,0.4188034188034188,"where θ represents the parameters of the global model trained on local data of decentralized clients
q(θ|

M j	C
j=1) and φj ∼q(φj|Dj, θ)."
AN ADAPTIVE TRADE-OFF THEOREM,0.4230769230769231,"The above theorem demonstrates that the adaptive local adaptation can be implemented by maximizing
a novel objective function in training. Then, the learning system is able to ﬁnd a suitable trade-off to
produce high-quality personalized models in non-iid data scenarios with any degree of diversity."
ADAPTIVE META-LEARNING ALGORITHM,0.42735042735042733,"3.5
ADAPTIVE META-LEARNING ALGORITHM"
ADAPTIVE META-LEARNING ALGORITHM,0.43162393162393164,"As pointed by federate meta-learning methods Chen et al. (2019); Khodak et al. (2019); Fallah
et al. (2020), clients in the FL paradigm can be regarded as tasks in the meta-learning framework.
Then, we can have the support set D, and the query set D′. Based on these deﬁnitions, the last two
terms in Eq 3 of Theorem 1 are naturally general training objectives of meta-learning. The initial
model FF is updated using the support set to minimize the loss in the query set. Therefore, based on
Theorem 1, we propose the adaptive meta-learning framework (AFML) to achieve generalizable PFL
with adaptive local adaptation. The training schema of AFML naturally follows the meta-learning
framework Finn et al. (2017)."
ADAPTIVE META-LEARNING ALGORITHM,0.4358974358974359,"Theorem 1 also introduces three additional terms, which can be expressed as three trade-off regular-
ization terms. Thus, the regularizer LT OF F is formulated as follows."
ADAPTIVE META-LEARNING ALGORITHM,0.44017094017094016,"LT OF F = α N ′
X"
ADAPTIVE META-LEARNING ALGORITHM,0.4444444444444444,"x′,y′∈D′
Eϵ∼p(ϵ) [−log q(y′ = y′|θ, x′, ϵ)]
LENC + α N ′
X"
ADAPTIVE META-LEARNING ALGORITHM,0.44871794871794873,"x′,y′∈D′
KL (p(z|x′, θ)||r(z))
LCONS"
ADAPTIVE META-LEARNING ALGORITHM,0.452991452991453,"+ βKL(p(θ|D, D′)||r(θ))
LCMP X (4)"
ADAPTIVE META-LEARNING ALGORITHM,0.45726495726495725,"where the LENC encourages an encoding that is maximally informative about the target. LCONS
gives constraints to the latent representation of z, which is similar to the work Alemi et al. (2016).
LCMP X motivates an initial model with low complexity."
ADAPTIVE META-LEARNING ALGORITHM,0.46153846153846156,"For the training process based on all participating client datasets, the third item LCMP X can be
further written as E [KL(p(θ|M)||r(θ))]. Therefore, when we set θ ∼N(τ) with τ = (θu, θσ), the
LCMP X can be put in the server and computed as Lj
CMP X = KL(p(θ|θu, θσ)||r(θ))."
ADAPTIVE META-LEARNING ALGORITHM,0.4658119658119658,"Finally, to achieve the Adaptive trade-off theorem, we design split the global model into two modules,
including the encoding network f and the classiﬁcation network h. The encoding network is trained
to learn the hidden representation of the observation x, while h is used to make the prediction based
on z. One intuition of such design is to alleviate the inconsistent between different local datasets
by learning a compact shared space. Then, we add the regularization on weights θ of the encoding
network and leave the weights of h unrestricted."
ADAPTIVE META-LEARNING ALGORITHM,0.4700854700854701,The corresponding training algorithm is shown by Algorithm 1.
ADAPTIVE META-LEARNING ALGORITHM,0.47435897435897434,Under review as a conference paper at ICLR 2022
ADAPTIVE META-LEARNING ALGORITHM,0.47863247863247865,"Algorithm 1: Learning algorithm of AFML
input :OuterLoop/Server learning rate η1, η2; InnerLoop/Client learning rate η;
Regularization coefﬁcient β; #C participating clients"
ADAPTIVE META-LEARNING ALGORITHM,0.4829059829059829,"1 Initialization: Initialize weights distribution θ ∼N(τ) with τ = (θu, θσ) for the encoding
network f; Initialize weights bθ for classiﬁcation network h. Initialize local models
φ =

φ1, ..., φj, ..., φC	
;"
ADAPTIVE META-LEARNING ALGORITHM,0.48717948717948717,"2 for communication round t = 1, 2, ... do"
ADAPTIVE META-LEARNING ALGORITHM,0.49145299145299143,"3
Randomly select K clients Ct from the total C clients;"
ADAPTIVE META-LEARNING ALGORITHM,0.49572649572649574,"4
Sample θt from N(τ) with reparameterization;"
ADAPTIVE META-LEARNING ALGORITHM,0.5,"5
Distribute θt and bθ to selected clients;"
ADAPTIVE META-LEARNING ALGORITHM,0.5042735042735043,"6
for each client j ∈Ct in parallel do"
GJ,0.5085470085470085,"7
gj
c, gj
g ←clientUpdate

j, θt, bθt

;"
END,0.5128205128205128,"8
end"
END,0.5170940170940171,"9
bθ = bθ −η1 K
P"
END,0.5213675213675214,"j∈Ct gj
c;
▷Aggregating the updates for the global classiﬁcation network"
END,0.5256410256410257,"10
τ = τ −η2 K
P"
END,0.5299145299145299,"j∈Ct gj
g;
▷Aggregating the updates for the global encoding network"
END,0.5341880341880342,11 end
END,0.5384615384615384,"12 Function ClientUpdate(j, θ, bθ):"
END,0.5427350427350427,"13
M j ←
 
Dj =
 
xj, yj
, D′j =
 
x′j, y′j
;"
END,0.5470085470085471,"14
Encode observation to the hidden feature zj = f(xj; θ), z′j = f(y′j; θ);"
END,0.5512820512820513,"15
Update the initial model, φj ←bθ −η▽bθLbθ
Dj, Lbθ
Dj = 1 Z
P"
END,0.5555555555555556,"zj,yj∈Dj ℓ

h

zj; bθ

, yj
;"
END,0.5598290598290598,"16
Obtain the loss on the query set, Lφj"
END,0.5641025641025641,"D′j = 1 Z
P"
END,0.5683760683760684,"D′j ℓ
 
h
 
z′j; φj
, y′j
;"
END,0.5726495726495726,"17
Compute the gradients for the classiﬁcation network, gj
c = ▽bθLφj D′j;"
END,0.5769230769230769,"18
Compute the gradients for the global model, gj
g = ▽τ

Lφj"
END,0.5811965811965812,"D′j + αLj
T OF F

where"
END,0.5854700854700855,"Lj
T OF F = Lj
ENC(D′, θt) + Lj
CONS(D′, θt) + βLj
CMP X(θt);"
RETURN GJ,0.5897435897435898,"19
return gj
c, gj
g; ;"
RELATED WORK,0.594017094017094,"4
RELATED WORK"
RELATED WORK,0.5982905982905983,"Federated learning (FL) Zhao et al. (2018); Kairouz et al. (2019); Li et al. (2020) is a machine
learning paradigm in which devices collaboratively train a global model while keeping the training
data decentralized. With conventional federated learning (FL) McMahan et al. (2017), the global
model is only trained to develop a common output for all devices. However, the inconsistent learning
objectives and data heterogeneity among devices in non-iid scenarios introduce a new personalization
requirement Chen et al. (2019). Thus, we have witnessed signiﬁcant progress in works Jiang
et al. (2019); Fallah et al. (2020); Mansour et al. (2020); Yu et al. (2020); Khodak et al. (2019) of
personalized federated learning (PFL)."
RELATED WORK,0.6025641025641025,"Due to inherent diversity among local data shards and objectives, many works Chen et al. (2019);
Jiang et al. (2019); Fallah et al. (2020) designed target functions that maximum the adaptability of
the global model. Especially, Chen et al. (2019) ﬁrstly noticed that such a target function could be
achieved by introducing the meta-learning framework to PFL."
RELATED WORK,0.6068376068376068,"However, it is still challenging for existing methods to maintain performance when there are non-iid
scenarios with different degrees of diversity among devices. Arivazhagan et al. (2019) discussed that
sole pursuing adaptability damages the personalization when devices require a powerful initial model
in local adaptation. They solved this problem by training a base layer to be optimal and changing
only the last layer to personalize in individual devices."
RELATED WORK,0.6111111111111112,Under review as a conference paper at ICLR 2022
RELATED WORK,0.6153846153846154,"The latest works proposed to maintain personalization performance in non-iid scenarios with various
diversities were Deng et al. (2020); Hanzely & Richt´arik (2020); Khodak et al. (2019); Mansour
et al. (2020). Khodak et al. (2019) speciﬁcally pointed out that existing PFL methods’ performance is
unstable when diversity among different devices’ data changes. These works alleviated the issue by
seeking joint optimization of the global model and personalized models. The proposed methods, such
as Deng et al. (2020) heavily relied on trainable mixing weights that are updated during the learning
process."
EXPERIMENTS,0.6196581196581197,"5
EXPERIMENTS"
EXPERIMENTS,0.6239316239316239,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80"
EXPERIMENTS,0.6282051282051282,Test Accuracy
EXPERIMENTS,0.6324786324786325,"Cifar100,  Accuracy of personalized model on Test (Pers.1, Pers.5)"
EXPERIMENTS,0.6367521367521367,"FedAvg-FL, K=10, Pers.1
FedAvg-FL, K=10, Pers.5
FedPer-META, P=80%, Pers.1
FedPer-META, P=80%, Pers.5
FedMAML, P=80%, Pers.1
FedMAML, P=80%, Pers.5
AFML, P=80%, Pers.1
AFML, P=80%, Pers.5 (a)"
EXPERIMENTS,0.6410256410256411,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55"
EXPERIMENTS,0.6452991452991453,Test Accuracy
EXPERIMENTS,0.6495726495726496,"Shakespeare, Accuracy of personalized model on Test (Pers.1, Pers.5)"
EXPERIMENTS,0.6538461538461539,"FedAvg-FL, K=10, Pers.1
FedAvg-FL, K=10, Pers.5
FedPer-META, P=80%, Pers.1
FedPer-META, P=80%, Pers.5
FedMAML, P=80%, Pers.1
FedMAML, P=80%, Pers.5
AFML, P=80%, Pers.1
AFML, P=80%, Pers.5 (b)"
EXPERIMENTS,0.6581196581196581,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000"
EXPERIMENTS,0.6623931623931624,#Rounds
EXPERIMENTS,0.6666666666666666,"Cifar100, Number of communications required to achieve the target accuracy (Pers.3)"
EXPERIMENTS,0.6709401709401709,"FedAvg-FT, K=10
FedPer-META, P=80%
FedMAML, P=80%
AFML, P=80% (c)"
EXPERIMENTS,0.6752136752136753,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 100 200 300 400"
EXPERIMENTS,0.6794871794871795,#Rounds
EXPERIMENTS,0.6837606837606838,"Shakespeare, Number of communications required to achieve the target accuracy (Pers.3)"
EXPERIMENTS,0.688034188034188,"FedAvg-FT, K=10
FedPer-META, P=80%
FedMAML, P=80%
AFML, P=80% (d)"
EXPERIMENTS,0.6923076923076923,"Figure 3: Test accuracy of personalized models (a) (b) and Communication rounds (c) (d) required to
reach to target accuracy under ζ ∈[0, 1]. Our AFML is compared with FedAvg-FT, FedAvg-Meta,
and FedMAML based on the CIFAR100 (a) (c) and Shakespeare (b) (d) datasets."
EXPERIMENTS,0.6965811965811965,"In this section, we evaluate the performance of the proposed AFML on CIFAR-100 and Shakespeare
Caldas et al. (2018) datasets under the basic non-IID setting described in the Section 2 with the
diversity ζ ∈[0, 1]. Then, following the three metrics described in the Section 3 of the Appendix,
AFML is compared with three state-of-the-art methods, including FedAvg-FT Jiang et al. (2019),
FedPer-Meta Fallah et al. (2020), and FedMAML Deng et al. (2020) where FedPer-Meta is an
enhancement work of FedAvg-Meta Chen et al. (2019). Limited by the space, more settings and
experimental results are described in Section 3 of the appendix."
RESULTS,0.7008547008547008,"5.1
RESULTS"
RESULTS,0.7051282051282052,"Accuracy performance. Fig 3 presents that AFML achieves highest accuracy compared to three
leading frameworks under non-IID scenarios with any degrees of diversity, yet with minimum
communication rounds used."
RESULTS,0.7094017094017094,"In the non-IID scenario with highly diversity ζ ≤0.3, the average personalization accuracy of AFML
is around 56% and 39% at CIFAR-100 and Shakespeare, respectively. However, other frameworks
cannot converge. In the non-IID scenario with low diversity ζ ≥0.5, AFML has a signiﬁcant
performance improvement and gets the highest accuracy, which is 5.25% higher than FedMAML,
9.79% higher than FedPer-Meta and 18.69% higher than FedAvg-FT on CIFAR100. AFML also
performs well on the Shakespeare dataset. Moreover, compared with other methods, after performing
ﬁve personalization epochs (Pers.5 Acc), the average personalization accuracy of AFML is 5.2% and
7.4% higher than its Pers.1 Acc on CIFAR-100 and Shakespeare, respectively."
RESULTS,0.7136752136752137,"The main reason is that AFML not only provides a strong point for personalization but also improves
the adaptability of the global model. The insight observation from our theorem is that AFML trains
an optimal global model with low generalization error while also targets adaptability. And it can
achieve a suitable trade-off under non-iid scenarios with different diversity. This also contributes to
the high convergence speed."
RESULTS,0.717948717948718,"Efﬁciency performance. As shown in Fig. 4, compared with other algorithms, the computation cost
of AFML achieves a signiﬁcant reduction. In particularly, the Bytes of AFML is respectively 22.5%,
55.7%, and 67.3% less than FedMAML, FedPer-Meta and FedAvg-FT in average on both datasets."
RESULTS,0.7222222222222222,"Fairness Comparison. The results presented in Fig. 4(b) and Fig. 5 demonstrates that AFML has
the ability to learn high-quality personalized models for majority clients. For both two datasets,
AFML leads to more clients with higher accuracy compared to the model trained locally. Moreover,"
RESULTS,0.7264957264957265,Under review as a conference paper at ICLR 2022
RESULTS,0.7307692307692307,"our proposed AFML does not encourage the sacriﬁce of fairness to obtain higher average accuracy,
reﬂecting the effectiveness of trade-off regularization terms. The main reason is that each device can
optimize its information ﬂow to obtain a suitable trade-off for better personalization."
RESULTS,0.7350427350427351,"FedAvg-FT
FedPer-Meta
FedMAML
AFML
0 50 100 150"
RESULTS,0.7393162393162394,System Overhead = 0.1 ×
RESULTS,0.7435897435897436,121.79 96.03 74.95 × 71.67 48.69 37.73
RESULTS,0.7478632478632479,Cifar100
RESULTS,0.7521367521367521,"FedAvg-FT
FedPer-Meta
FedMAML
AFML
0 5"
RESULTS,0.7564102564102564,"10
= 0.3 × 7.55 3.41 2.41 ×
0.7 3.8 2.65"
RESULTS,0.7606837606837606,Shakespeare
RESULTS,0.7649572649572649,"FedAvg-FT
FedPer-Meta
FedMAML
AFML
0 50 100 150"
RESULTS,0.7692307692307693,System Overhead
RESULTS,0.7735042735042735,"= 0.6
119.27"
RESULTS,0.7777777777777778,103.07 77.3 54.48 69.97 60.5 38.26 27.15
RESULTS,0.782051282051282,"FedAvg-FT
FedPer-Meta
FedMAML
AFML
0 5"
RESULTS,0.7863247863247863,"10
= 0.9
9.96 6.9"
RESULTS,0.7905982905982906,"2.94
2.56"
RESULTS,0.7948717948717948,"0.92
0.65"
RESULTS,0.7991452991452992,"3.29
2.82"
RESULTS,0.8034188034188035,"Bytes (1e9)
Flops (1e13)"
RESULTS,0.8076923076923077,"(a) System overhead for achieving a target accuracy
in different methods"
RESULTS,0.811965811965812,"30
35
40
45
50
55
60
65
70"
RESULTS,0.8162393162393162,"25
20
15
10"
RESULTS,0.8205128205128205,"5
0
5
10
15
20
25
30
35
40
45"
RESULTS,0.8247863247863247,"Acc_local
Acc_per = 0.1"
RESULTS,0.8290598290598291,"30
35
40
45
50
55
60
65
70"
RESULTS,0.8333333333333334,"25
20
15
10"
RESULTS,0.8376068376068376,"5
0
5
10
15
20
25
30
35
40
= 0.4"
RESULTS,0.8418803418803419,"30
35
40
45
50
55
60
65
70
Acc_local"
RESULTS,0.8461538461538461,"40
35
30
25
20
15
10"
RESULTS,0.8504273504273504,"5
0
5
10
15"
RESULTS,0.8547008547008547,"Acc_local
Acc_per = 0.6"
RESULTS,0.8589743589743589,"30
35
40
45
50
55
60
65
70
Acc_local"
RESULTS,0.8632478632478633,"50
45
40
35
30
25
20
15
10"
RESULTS,0.8675213675213675,"5
0
5
10 = 0.9"
RESULTS,0.8717948717948718,"FedAvg-FT, Pers=5
FedPer-Meta, Pers=5"
RESULTS,0.8760683760683761,"FedMAML, Pers=5
AFML, Pers=5"
RESULTS,0.8803418803418803,"(b) Accuracy of four models vs. local, trained-from-
scratch models under different ζ"
RESULTS,0.8846153846153846,"Figure 4: Comparison of our AFML with three leading PFL methods on CIFAR100 and Shakespeare
datasets. For System overhead in (a), we set the target accuracy as its test accuracy when convergence.
(The symbol ”×” represents that the algorithm cannot converge.)"
RESULTS,0.8888888888888888,"ζ = 0.1
ζ = 0.3
ζ = 0.6
ζ = 0.9"
RESULTS,0.8931623931623932,"ζ = 0.1
ζ = 0.4
ζ = 0.6
ζ = 0.8"
RESULTS,0.8974358974358975,"Figure 5: Accuracy distribution of devices in AFML compared to FedAvg-FT, FedPer-Meta, and
FedMAML under different non-IID settings. The ﬁrst row presents the results of the CIFAR100,
while the second row presents the Shakespeare."
CONCLUSION,0.9017094017094017,"6
CONCLUSION"
CONCLUSION,0.905982905982906,"We have pointed out the connection between the generalizable PFL and learning objectives (i.e.,
optimality and adaptability) trade-off of the global model. An in-depth study through experiments
and mathematical analysis shows that a suitable trade-off can be obtained by implementing adaptive
local adaptation. A proven theorem referred to as the adaptive trade-off theorem further shows
balancing information ﬂow in local adaptation contributes to the desired suitable trade-off. These
theoretical insights guide our design of a novel adaptive federated meta-learning(AFML) framework
that achieves generalizable PFL with adaptive local adaptation. Through the empirical experiments
on non-iid data scenarios with an extensive diversity among devices, we have demonstrated that the
proposed AFML is superior to alternative learning frameworks in terms of personalized learning
quality and convergence speed."
CONCLUSION,0.9102564102564102,Under review as a conference paper at ICLR 2022
REFERENCES,0.9145299145299145,REFERENCES
REFERENCES,0.9188034188034188,"Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016."
REFERENCES,0.9230769230769231,"Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-
ated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019."
REFERENCES,0.9273504273504274,"Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097,
2018."
REFERENCES,0.9316239316239316,"Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning with fast
convergence and efﬁcient communication. arXiv preprint arXiv:1802.07876, 2019."
REFERENCES,0.9358974358974359,"Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning. arXiv preprint arXiv:2003.13461, 2020."
REFERENCES,0.9401709401709402,"Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-
learning approach. arXiv preprint arXiv:2002.07948, 2020."
REFERENCES,0.9444444444444444,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400, 2017."
REFERENCES,0.9487179487179487,"Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models. arXiv
preprint arXiv:2002.05516, 2020."
REFERENCES,0.9529914529914529,"Yihan Jiang, Jakub Koneˇcn´y, Keith Rush, Ruiyu Li, and Sreeram Kannan. Improving federated
learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019."
REFERENCES,0.9572649572649573,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.9615384615384616,"Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, pp. 5917–5928, 2019."
REFERENCES,0.9658119658119658,"Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. Survey of personalization techniques for
federated learning. arXiv preprint arXiv:2003.08673, 2020."
REFERENCES,0.9700854700854701,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020."
REFERENCES,0.9743589743589743,"Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020."
REFERENCES,0.9786324786324786,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282, 2017."
REFERENCES,0.9829059829059829,"Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset
of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019."
REFERENCES,0.9871794871794872,"Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. arXiv preprint arXiv:1912.03820, 2020."
REFERENCES,0.9914529914529915,"Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adapta-
tion. CoRR, abs/2002.04758, 2020. URL http://dblp.uni-trier.de/db/journals/corr/corr2002.html#
abs-2002-04758."
REFERENCES,0.9957264957264957,"Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018."
