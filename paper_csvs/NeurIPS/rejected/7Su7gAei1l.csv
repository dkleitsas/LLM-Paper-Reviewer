Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008764241893076249,"Finite symmetric groups Sn are essential in fields such as combinatorics, physics,
1"
ABSTRACT,0.0017528483786152498,"and chemistry. However, learning a probability distribution over Sn poses signif-
2"
ABSTRACT,0.0026292725679228747,"icant challenges due to its intractable size and discrete nature. In this paper, we
3"
ABSTRACT,0.0035056967572304996,"introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the
4"
ABSTRACT,0.0043821209465381246,"task of learning a complicated distribution over Sn by decomposing it into learning
5"
ABSTRACT,0.0052585451358457495,"simpler transitions of the reverse diffusion using deep neural networks. We identify
6"
ABSTRACT,0.006134969325153374,"the riffle shuffle as an effective forward transition and provide empirical guidelines
7"
ABSTRACT,0.007011393514460999,"for selecting the diffusion length based on the theory of random walks on finite
8"
ABSTRACT,0.007887817703768623,"groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for
9"
ABSTRACT,0.008764241893076249,"the reverse transition, which is provably more expressive than the PL distribution.
10"
ABSTRACT,0.009640666082383873,"We further introduce a theoretically grounded ""denoising schedule"" to improve
11"
ABSTRACT,0.010517090271691499,"sampling and learning efficiency. Extensive experiments show that our model
12"
ABSTRACT,0.011393514460999123,"achieves state-of-the-art or comparable performances on solving tasks including
13"
ABSTRACT,0.012269938650306749,"sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.
14"
INTRODUCTION,0.013146362839614373,"1
Introduction
15"
INTRODUCTION,0.014022787028921999,"As a vital area of abstract algebra, finite groups provide a structured framework for analyzing symme-
16"
INTRODUCTION,0.014899211218229623,"tries and transformations which are fundamental to a wide range of fields, including combinatorics,
17"
INTRODUCTION,0.015775635407537247,"physics, chemistry, and computer science. One of the most important finite groups is the finite
18"
INTRODUCTION,0.016652059596844872,"symmetric group Sn, defined as the group whose elements are all the bijections (or permutations)
19"
INTRODUCTION,0.017528483786152498,"from a set of n elements to itself, with the group operation being function composition.
20"
INTRODUCTION,0.018404907975460124,"Classic probabilistic models for finite symmetric groups Sn, such as the Plackett-Luce (PL) model
21"
INTRODUCTION,0.019281332164767746,"[35, 27], the Mallows model [28], and card shuffling methods [9], are crucial in analyzing preference
22"
INTRODUCTION,0.020157756354075372,"data and understanding the convergence of random walks. Therefore, studying probabilistic models
23"
INTRODUCTION,0.021034180543382998,"over Sn through the lens of modern machine learning is both natural and beneficial. This problem is
24"
INTRODUCTION,0.021910604732690624,"theoretically intriguing as it bridges abstract algebra and machine learning. For instance, Cayley’s
25"
INTRODUCTION,0.022787028921998246,"Theorem, a fundamental result in abstract algebra, states that every group is isomorphic to a subgroup
26"
INTRODUCTION,0.02366345311130587,"of a symmetric group. This implies that learning a probability distribution over finite symmetric
27"
INTRODUCTION,0.024539877300613498,"groups could, in principle, yield a distribution over any finite group. Moreover, exploring this problem
28"
INTRODUCTION,0.025416301489921123,"could lead to the development of advanced models capable of addressing tasks such as permutations
29"
INTRODUCTION,0.026292725679228746,"in ranking problems, sequence alignment in bioinformatics, and sorting.
30"
INTRODUCTION,0.02716914986853637,"However, learning a probability distribution over finite symmetric groups Sn poses significant
31"
INTRODUCTION,0.028045574057843997,"challenges. First, the number of permutations of n objects grows factorially with n, making the
32"
INTRODUCTION,0.028921998247151623,"inference and learning computationally expensive for large n. Second, the discrete nature of the data
33"
INTRODUCTION,0.029798422436459245,"brings difficulties in designing expressive parameterizations and impedes the gradient-based learning.
34"
INTRODUCTION,0.03067484662576687,"In this work, we propose a novel discrete (state space) diffusion model over finite symmetric groups,
35"
INTRODUCTION,0.03155127081507449,"dubbed as SymmetricDiffusers. It overcomes the above challenges by decomposing the difficult
36"
INTRODUCTION,0.03242769500438212,"problem of learning a complicated distribution over Sn into a sequence of simpler problems, i.e.,
37"
INTRODUCTION,0.033304119193689745,"learning individual transitions of a reverse diffusion process using deep neural networks. Based on
38"
INTRODUCTION,0.03418054338299737,"the theory of random walks on finite groups, we investigate various shuffling methods as the forward
39"
INTRODUCTION,0.035056967572304996,"process and identify the riffle shuffle as the most effective. We also provide empirical guidelines
40"
INTRODUCTION,0.03593339176161262,"on choosing the diffusion length based on the mixing time of the riffle shuffle. Furthermore, we
41"
INTRODUCTION,0.03680981595092025,"examine potential transitions for the reverse diffusion, such as inverse shuffling methods and the
42"
INTRODUCTION,0.03768624014022787,"PL distribution, and introduce a novel generalized PL distribution. We prove that our generalized
43"
INTRODUCTION,0.03856266432953549,"PL is more expressive than the PL distribution. Additionally, we propose a theoretically grounded
44"
INTRODUCTION,0.03943908851884312,"""denoising schedule"" that merges reverse steps to improve the efficiency of sampling and learning.
45"
INTRODUCTION,0.040315512708150744,"To validate the effectiveness of our SymmetricDiffusers, we conduct extensive experiments on three
46"
INTRODUCTION,0.04119193689745837,"tasks: sorting 4-Digit MNIST images, solving Jigsaw Puzzles on the Noisy MNIST and CIFAR-10
47"
INTRODUCTION,0.042068361086765996,"datasets, and addressing traveling salesman problems (TSPs). Our model achieves the state-of-the-art
48"
INTRODUCTION,0.04294478527607362,"or comparable performance across all tasks.
49"
RELATED WORKS,0.04382120946538125,"2
Related Works
50"
RELATED WORKS,0.044697633654688866,"Random Walks on Finite Groups. The field of random walks on finite groups, especially finite
51"
RELATED WORKS,0.04557405784399649,"symmetric groups, have been extensively studied by previous mathematicians [37, 11, 4, 38]. Tech-
52"
RELATED WORKS,0.04645048203330412,"niques from a variety of different fields, including probability, combinatorics, and representation
53"
RELATED WORKS,0.04732690622261174,"theory, have been used to study random walks on finite groups [38]. In particular, random walks on
54"
RELATED WORKS,0.04820333041191937,"finite symmetric groups are first studied in the application of card shuffling, with many profound
55"
RELATED WORKS,0.049079754601226995,"theoretical results of shuffling established. A famous result in the field shows that 7 riffle shuffles are
56"
RELATED WORKS,0.04995617879053462,"enough to mix up a deck of 52 cards [4], where a riffle shuffle is a mathematically precise model that
57"
RELATED WORKS,0.05083260297984225,"simulates how people shuffle cards in real life. The idea of shuffling to mix up a deck of cards aligns
58"
RELATED WORKS,0.051709027169149865,"naturally with the idea of diffusion, and we seek to fuse the modern techniques of diffusion models
59"
RELATED WORKS,0.05258545135845749,"with the classical theories of random walks on finite groups.
60"
RELATED WORKS,0.05346187554776512,"Diffusion Models. Diffusion models [40, 41, 16, 42] are a powerful class of generative models that
61"
RELATED WORKS,0.05433829973707274,"typically deals with continuous data. They consist of forward and reverse processes. The forward
62"
RELATED WORKS,0.05521472392638037,"process is typically a discrete-time continuous-state Markov chain or a continuous-time continuous-
63"
RELATED WORKS,0.056091148115687994,"state Markov process that gradually adds noise to data, and the reverse process learn neural networks
64"
RELATED WORKS,0.05696757230499562,"to denoise. Discrete (state space) diffusion models have also been proposed to handle discrete data
65"
RELATED WORKS,0.057843996494303246,"like image, text [3], and graphs [45]. Existing discrete diffusion models are applicable for learning
66"
RELATED WORKS,0.058720420683610865,"distributions of permutations. However, they focused on cases where the state space is small or has a
67"
RELATED WORKS,0.05959684487291849,"special (e.g., decomposable) structure and are unable to deal with intractable-sized state spaces like
68"
RELATED WORKS,0.060473269062226116,"the symmetric group. In particular, [3] requires an explicit transition matrix, which has size n! × n!
69"
RELATED WORKS,0.06134969325153374,"in the case of finite symmetric groups and has no simple representations or sparsifications.
70"
RELATED WORKS,0.06222611744084137,"Differentiable Sorting and Learning Permutations. A popular paradigm to learn permutations
71"
RELATED WORKS,0.06310254163014899,"is through differentiable sorting or matching algorithms. Various differentiable sorting algorithms
72"
RELATED WORKS,0.06397896581945661,"have been proposed that uses continuous relaxations of permutation matrices [13, 8, 5], or uses
73"
RELATED WORKS,0.06485539000876424,"differentiable swap functions [33, 34, 20]. The Gumbel-Sinkhorn method [29] has also been proposed
74"
RELATED WORKS,0.06573181419807186,"to learn latent permutations using the continuous Sinkhorn operator. Such methods often focus on
75"
RELATED WORKS,0.06660823838737949,"finding the optimal permutation instead of learning a distribution over the finite symmetric group.
76"
RELATED WORKS,0.06748466257668712,"Moreover, they tend to be less effective as n grows larger due to their high complexities.
77"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.06836108676599474,"3
Learning Diffusion Models on Finite Symmetric Groups
78"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.06923751095530237,"We first introduce some notations. Fix n ∈N. Let [n] denote the set {1, 2, . . . , n}. A permutation
79"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07011393514460999,"σ on [n] is a function from [n] to [n], and we usually write σ as

1
2
· · ·
n
σ(1)
σ(2)
· · ·
σ(n)"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07099035933391762,"
. The
80"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07186678352322524,"identity permutation, denoted by Id, is the permutation given by Id(i) = i for all i ∈[n]. Let
81"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07274320771253287,"Sn be the set of all permutations (or bijections) from a set of n elements to itself, called the finite
82"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.0736196319018405,"symmetric group, whose group operation is the function composition. For a permutation σ ∈Sn,
83"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07449605609114812,"the permutation matrix Qσ ∈Rn×n associated with σ satisfies e⊤
i Qσ = e⊤
σ(i) for all i ∈[n]. In
84"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07537248028045573,"this paper, we consider a set of n distinctive objects X = {x1, . . . , xn}, where the i-th object is
85"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07624890446976336,"represented by a d-dimensional vector xi. Therefore, a ranked list of objects can be represented as
86"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07712532865907099,"a matrix X = [x1, . . . , xn]⊤∈Rn×d, where the ordering of rows corresponds to the ordering of
87"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07800175284837861,"objects. We can permute X via permutation σ to obtain QσX.
88 ⋯
⋯
⋯"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07887817703768624,"Ascending x1 x2 x3
x1 x2 x3 x1 x2 x3"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.07975460122699386,"Probability
Forward Step"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08063102541630149,Reverse Step
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08150744960560911,Merged Reverse Step
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08238387379491674,"123
[x1, x2, x3]>"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08326029798422437,"{x1, x2, x3}
X 132 312"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08413672217353199,"321
123 213 231"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08501314636283962,LLMvrWbRxIYhLwXYS2lNi0N/AJ9yLx1Mn5eM3Mi+Thf2hb235Nu2gX/ZlGfURqBy4czrmXe+8JQka1cd0vmFpbT29sZray2zu7e/u5/EFDy0hUseSdUKkCaMClI31DSChVBPGCkGYxupn7zkShNpaiZcUh8jgaC9ilGxkqNh2KrWzvt5gpuyZ3BWSVeQgogQbWbh+lOT+KIE2EwQ1q3PTc0foyUoZiRSbYTaRIiPEID0rZUIE60H8/OnTgnVuk5falsCePM1L8TMeJaj3lgOzkyQ73sTcX/vHZk+ld+TEUYGSLwfFE/Yo6RzvR3p0cVwYaNLUFYUXurg4dIWxsQotb2EDahiE/+2UPy38FQd8YoPzlmNaJY3zklcule8uCpXrJMIMOALHoAg8cAkq4BZUQR1gcA+ewQt4hW/wHX7Az3lrCiYzh2AB8PsHB8+jeA=</latexit>q(XT ) 132 312
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08588957055214724,"321
123 213 231 q(X1) 132 312"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08676599474145487,"321
123 213 231"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.0876424189307625,pdata(X0) σ1
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08851884312007012,nFZwT6gHUomzbSheQxJRixDf8Ktbv0ad+LGhf9i2g5iWw9cOJxzL/feE8acGet5X3BtfSO3uZXfLuzs7u0fFEuHTaMSTWiDK50O8SGciZpwzLaTvWFIuQ01Y4up36rUeqDVPywY5jGg8kCxiBFsntbuGDQTu2V6x7FW8GdAq8TNSBhnqvRLMdfuKJIJKSzg2puN7sQ1SrC0jnE4K3cTQGJMRHtCOoxILaoJ0dvAEnTqljyKlXUmLZurfiRQLY8YidJ0C26FZ9qbif14nsdF1kDIZJ5ZKMl8UJRxZhabfoz7TlFg+dgQTzdytiAyxsS6jBa38IFyDUNx/sYeVr4Kw3FxAXnL8e0SpoXFb9aqd5flms3WYR5cAxOwBnwRWogTtQBw1AfP4AW8wjf4Dj/g57x1DWYzR2AB8PsH1jOk/w=</latexit>σt
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.08939526730937773,"rPCs="">ACI3icbVBNSwMxE2q1Vq/Wj16CRbBg5Rdkeqx6MVjhX5Bu5Rsmt2GJpslyYpl6Z/wqld/jTfx4sH/YtouYlsfDzem2Fmnh9zpo3jfMHcxmZ+a7uwU9zd2z84LJWP2lomitAWkVyqro815SyiLcMp91YUSx8Tjv+G7mdx6p0kxGTTOJqSdwGLGAEWys1O1rFgo8aA5KFafqzIHWiZuRCsjQGJRhvj+UJBE0MoRjrXuExsvxcowum02E80jTEZ45D2LI2woNpL5wdP0ZlVhiQylZk0Fz9O5FiofVE+LZTYDPSq95M/M/rJSa48VIWxYmhEVksChKOjESz79GQKUoMn1iCiWL2VkRGWGFibEbLW3gobcNIXPwyRp6W/kp9MbXBuasxrZP2ZdWtVWsPV5X6bRZhAZyAU3AOXHAN6uAeNEALEMDBM3gBr/ANvsMP+LlozcFs5hgsAX7/AJ8zpN8=</latexit>σT σ0 T
σ0 t
σ0 1"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09027169149868536,"X0
X1
Xt"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09114811568799298,nicbVBdSwJBFJ21LMvrcdehiToIWQ3wnqUeunRSE3QRWbH2XVwPpaZ2UgWf0Kv9dqv6S16rX/TqEukduDC4Zx7ufeIGZUG9f9dnJr6/mNzcJWcXtnd2+/VD5oa5koTFpYMqk6AdKEUFahpGOrEiAeMPASjm6n/8EiUplI0zTgmPkeRoCHFyFjpvtNv9ksVt+rOAFeJl5EKyNDol518byBxwokwmCGtu54bGz9FylDMyKTYSzSJER6hiHQtFYgT7aezWyfwxCoDGEplSxg4U/9OpIhrPeaB7eTIDPWyNxX/87qJCa/8lIo4MUTg+aIwYdBIOH0cDqgi2LCxJQgram+FeIgUwsbGs7iFRdI2DPnZL6P4aeGvNOATG5y3HNMqaZ9XvVq1dndRqV9nERbAETgGp8ADl6AObkEDtAGEXgGL+DVeXPenQ/nc96ac7KZQ7A5+sHRweimA=</latexit>XT
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09202453987730061,cbVDLSsNAFJ34rPWtSzfBIihISUSqy6IblxX6gqaGyfS2HZxJwsyNWGI+wK9xq1v/wj9wJ25dOX0gtnpg4HDOudy5J4gF1+g4b9bc/MLi0nJuJb+6tr6xubW9U9dRohjUWCQi1QyoBsFDqCFHAc1YAZWBgEZwezn0G3egNI/CKg5iaEvaC3mXM4pG8rcKsZ962Aek2aGneU9SP61mN6kXKy4he2j61SOTcorOCPZf4k5IgUxQ8betRa8TsURCiExQrVuE2M7pQo5E5DlvURDTNkt7UHL0JBK0O10dE1mHxilY3cjZV6I9kj9PZFSqfVABiYpKfb1rDcU/NaCXbP2ykP4wQhZONF3UTYGNnDauwOV8BQDAyhTHzV5v1qaIMTYHTW0QvMoG+P5hnN1P3ZUGMjPFubM1/SX1k6JbKpauTwvli0mFObJH9skhckZKZMrUiE1wsgjeSLP5MV6td6tD+tzHJ2zJjO7ZArW1zeYULD2</latexit>p✓(σ0
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09290096406660824,"T |XT )
p✓(σ0"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09377738825591586,"t|Xt)
p✓(σ0 1|X1)"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09465381244522349,"q(X1|X0, σ1)
q(Xt|Xt−1, σt)
q(XT |XT −1, σT )"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09553023663453111,"p(X0|X1, σ0"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09640666082383874,"1)
p(Xt−1|Xt, σ0"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09728308501314636,"t)
p(Xt|XT , σ0 T )"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09815950920245399,"Figure 1: This figure illustrates our discrete diffusion model on finite symmetric groups. The middle
graphical model displays the forward and reverse diffusion processes. We demonstrate learning
distributions over the symmetric group S3 via the task of sorting three MNIST 4-digit images. The
top part of the figure shows the marginal distribution of a ranked list of images Xt at time t, while
the bottom shows a randomly drawn list of images."
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09903593339176162,"Our goal is to learn a distribution over Sn. We propose learning discrete (state space) diffusion
89"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.09991235758106924,"models, which consist of a forward process and a reverse process. In the forward process, starting
90"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10078878177037687,"from the unknown data distribution, we simulate a random walk until it reaches a known stationary
91"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.1016652059596845,"“noise” distribution. In the reverse process, starting from the known noise distribution, we simulate
92"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10254163014899212,"another random walk, where the transition probability is computed using a neural network, until it
93"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10341805433829973,"recovers the data distribution. Learning a transition distribution over Sn is often more manageable
94"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10429447852760736,"than learning the original distribution because: (1) the support size (the number of states that can be
95"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10517090271691498,"reached in one transition) could be much smaller than n!, and (2) the distance between the initial and
96"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10604732690622261,"target distributions is smaller. By doing so, we break down the hard problem (learning the original
97"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10692375109553023,"distribution) into a sequence of simpler subproblems (learning the transition distribution). The overall
98"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10780017528483786,"framework is illustrated in Fig. 1. In the following, we will introduce the forward card shuffling
99"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10867659947414549,"process in Section 3.1, the reverse process in Section 3.2, the network architecture and training in
100"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.10955302366345311,"Section 3.3, denoising schedule in Section 3.4, and reverse decoding methods in Section 3.5.
101"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11042944785276074,"3.1
Forward Diffusion Process: Card Shuffling
102"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11130587204206836,"Suppose we observe a set of objects X and their ranked list X0. They are assumed to be generated
103"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11218229623137599,"from an unknown data distribution in an IID manner, i.e., X0, X
iid
∼pdata(X, X). One can construct a
104"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11305872042068361,"bijection between a ranked list of n objects and an ordered deck of n cards. Therefore, permuting
105"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11393514460999124,"objects is equivalent to shuffling cards. In the forward diffusion process, we would like to add
106"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11481156879929887,"“random noise” to the rank list so that it reaches to some known stationary distribution like the
107"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11568799298860649,"uniform. Formally, we let S ⊆Sn be a set of permutations that are realizable by a given shuffling
108"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.1165644171779141,"method in one step. S does not change across steps in common shuffling methods. We will provide
109"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11744084136722173,"concrete examples later. We then define the forward process as a Markov chain,
110"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11831726555652936,"q(X1:T |X0, X) = q(X1:T |X0) =
YT"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.11919368974583698,"t=1 q(Xt|Xt−1),
(1)"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.1200701139351446,where q(Xt|Xt−1) = P
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12094653812445223,"σt∈S q(Xt|Xt−1, σt)q(σt) and the first equality in Eq. (1) holds since X0
111"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12182296231375986,"implies X. In the forward process, although the set X does not change, the rank list of objects Xt
112"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12269938650306748,"changes. Here q(σt) has the support S and describes the permutation generated by the underlying
113"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12357581069237511,"shuffling method. Note that common shuffling methods are time-homogeneous Markov chains, i.e.,
114"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12445223488168274,"q(σt) stays the same across time. q(Xt|Xt−1, σt) is a delta distribution δ (Xt = QσtXt−1) since the
115"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12532865907099036,"permuted objects Xt are uniquely determined given the permutation σt and Xt−1. We denote the
116"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12620508326029797,"neighbouring states of X via one-step shuffling as NS(X) := {QσX|σ ∈S}. Therefore, we have,
117"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.1270815074496056,"q(Xt|Xt−1) =
q(σt)
if Xt ∈NS(Xt−1)
0
otherwise.
(2)"
LEARNING DIFFUSION MODELS ON FINITE SYMMETRIC GROUPS,0.12795793163891322,"Note that Xt ∈NS(Xt−1) is equivalent to σt ∈S and Xt = QσtXt−1.
118"
CARD SHUFFLING METHODS,0.12883435582822086,"3.1.1
Card Shuffling Methods
119"
CARD SHUFFLING METHODS,0.12971078001752848,"We now consider several popular shuffling methods as the forward transition, i.e., random transpo-
120"
CARD SHUFFLING METHODS,0.13058720420683612,"sitions, random insertions, and riffle shuffles. Different shuffling methods provide different design
121"
CARD SHUFFLING METHODS,0.13146362839614373,"choices of q(σt), thus corresponding to different forward diffusion processes. Although all these
122"
CARD SHUFFLING METHODS,0.13234005258545137,"forward diffusion processes share the same stationary distribution, i.e., the uniform, they differ in
123"
CARD SHUFFLING METHODS,0.13321647677475898,"their mixing time. We will introduce stronger quantitative results on their mixing time later.
124"
CARD SHUFFLING METHODS,0.13409290096406662,"Random Transpositions. One natural way of shuffling is to swap pairs of objects. Formally, a
125"
CARD SHUFFLING METHODS,0.13496932515337423,"transposition or a swap is a permutation σ ∈Sn such that there exist i ̸= j ∈[n] with σ(i) = j,
126"
CARD SHUFFLING METHODS,0.13584574934268187,"σ(j) = i, and σ(k) = k for all k /∈{i, j}, in which case we denote σ = (i
j). We let S =
127"
CARD SHUFFLING METHODS,0.13672217353198948,"{(i
j) : i ̸= j ∈[n]} ∪{Id}. For any time t, we define q(σt) by choosing two indices from [n]
128"
CARD SHUFFLING METHODS,0.1375985977212971,"uniformly and independently and swap the two indices. If the two chosen indices are the same, then
129"
CARD SHUFFLING METHODS,0.13847502191060473,"this means that we have sampled the identity permutation. Specifically, q(σt = (i
j)) = 2/n2
130"
CARD SHUFFLING METHODS,0.13935144609991235,"when i ̸= j and q(σt = Id) = 1/n.
131"
CARD SHUFFLING METHODS,0.14022787028921999,"Random Insertions. Another shuffling method is to insert the last piece to somewhere in the middle.
132"
CARD SHUFFLING METHODS,0.1411042944785276,"Let inserti denote the permutation that inserts the last piece right before the ith piece, and let
133"
CARD SHUFFLING METHODS,0.14198071866783524,"S := {inserti : i ∈[n]}. Note that insertn = Id. Specifically, we have q(σt = inserti) = 1/n
134"
CARD SHUFFLING METHODS,0.14285714285714285,"when i ̸= n and q(σt = Id) = 1/n.
135"
CARD SHUFFLING METHODS,0.1437335670464505,"Riffle Shuffles. Finally, we introduce the riffle shuffle, a method similar to how serious card players
136"
CARD SHUFFLING METHODS,0.1446099912357581,"shuffle cards. The process begins by roughly cutting the deck into two halves and then interleaving the
137"
CARD SHUFFLING METHODS,0.14548641542506574,"two halves together. A formal mathematical model of the riffle shuffle, known as the GSR model, was
138"
CARD SHUFFLING METHODS,0.14636283961437335,"introduced by Gilbert and Shannon [11], and independently by Reeds [37]. The model is described
139"
CARD SHUFFLING METHODS,0.147239263803681,"as follows. A deck of n cards is cut into two piles according to binomial distribution, where the
140"
CARD SHUFFLING METHODS,0.1481156879929886,"probability of having k cards in the top pile is
 n
k

/2n for 0 ≤k ≤n. The top pile is held in the
141"
CARD SHUFFLING METHODS,0.14899211218229624,"left hand and the bottom pile in the right hand. The two piles are then riffled together such that, if
142"
CARD SHUFFLING METHODS,0.14986853637160386,"there are A cards left in the left hand and B cards in the right hand, the probability that the next card
143"
CARD SHUFFLING METHODS,0.15074496056091147,"drops from the left is A/(A + B), and from right is B/(A + B). We implement the riffle shuffles
144"
CARD SHUFFLING METHODS,0.1516213847502191,"according to the GSR model. For simplicity, we will omit the term “GSR” when referring to riffle
145"
CARD SHUFFLING METHODS,0.15249780893952672,"shuffles hereafter.
146"
CARD SHUFFLING METHODS,0.15337423312883436,"There exists an exact formula for the probability over Sn obtained through one-step riffle shuffle.
147"
CARD SHUFFLING METHODS,0.15425065731814197,"Let σ ∈Sn. A rising sequence of σ is a subsequence of σ constructed by finding a maximal
148"
CARD SHUFFLING METHODS,0.1551270815074496,"subset of indices i1 < i2 < · · · < ij such that permuted values are contiguously increasing, i.e.,
149"
CARD SHUFFLING METHODS,0.15600350569675722,"σ(i2) −σ(i1) = σ(i3) −σ(i2) = · · · = σ(ij) −σ(ij−1) = 1. For example, the permutation
150

1
2
3
4
5
1
4
2
5
3"
CARD SHUFFLING METHODS,0.15687992988606486,"
has 2 rising sequences, i.e., 123 (red) and 45 (blue). Note that a permutation
151"
CARD SHUFFLING METHODS,0.15775635407537247,"has 1 rising sequence if and only if it is the identity permutation. Denoting by qRS(σ) the probability
152"
CARD SHUFFLING METHODS,0.1586327782646801,"of obtaining σ through one-step riffle shuffle, it is shown in [4] that
153"
CARD SHUFFLING METHODS,0.15950920245398773,qRS(σ) = 1
N,0.16038562664329536,2n
N,0.16126205083260298,"n + 2 −r
n 
= 
 "
N,0.16213847502191062,"(n + 1)/2n
if σ = Id
1/2n
if σ has two rising sequences
0
otherwise,
(3)"
N,0.16301489921121823,"where r is the number of rising sequences of σ. The support S is thus the set of all permutations with
154"
N,0.16389132340052587,"at most two rising sequences. We let the forward process be q(σt) = qRS(σt) for all t.
155"
MIXING TIMES AND CUT-OFF PHENOMENON,0.16476774758983348,"3.1.2
Mixing Times and Cut-off Phenomenon
156"
MIXING TIMES AND CUT-OFF PHENOMENON,0.1656441717791411,"All of the above shuffling methods have the uniform distribution as the stationary distribution.
157"
MIXING TIMES AND CUT-OFF PHENOMENON,0.16652059596844873,"However, they have different mixing times (i.e., the time until the Markov chain is close to its
158"
MIXING TIMES AND CUT-OFF PHENOMENON,0.16739702015775634,"stationary distribution measured by some distance), and there exist quantitative results on their mixing
159"
MIXING TIMES AND CUT-OFF PHENOMENON,0.16827344434706398,"times. Let q ∈{qRT, qRI, qRS}, and for t ∈N, let q(t) be the marginal distribution of the Markov
160"
MIXING TIMES AND CUT-OFF PHENOMENON,0.1691498685363716,"chain after t shuffles. We describe the mixing time in terms of the total variation (TV) distance
161"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17002629272567923,"between two probability distributions, i.e., DTV(q(t), u), where u is the uniform distribution.
162"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17090271691498685,"For all three shuffling methods, there exists a cut-off phenomenon, where DTV(q(t), u) stays around
163"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17177914110429449,"1 for initial steps and then abruptly drops to values that are close to 0. The cut-off time is the time
164"
MIXING TIMES AND CUT-OFF PHENOMENON,0.1726555652936021,"when the abrupt change happens. For the formal definition, we refer the readers to Definition 3.3 of
165"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17353198948290974,"[38]. In [38], they also provided the cut-off time for random transposition, random insertion, and
166"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17440841367221735,"riffle shuffle, which are n"
MIXING TIMES AND CUT-OFF PHENOMENON,0.175284837861525,"2 log n, n log n, and 3"
MIXING TIMES AND CUT-OFF PHENOMENON,0.1761612620508326,"2 log2 n respectively. Observe that the riffle shuffle
167"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17703768624014024,"reaches the cut-off much faster than the other two methods, which means it has a much faster mixing
168"
MIXING TIMES AND CUT-OFF PHENOMENON,0.17791411042944785,"time. Therefore, we use the riffle shuffle in the forward process.
169"
THE REVERSE DIFFUSION PROCESS,0.17879053461875546,"3.2
The Reverse Diffusion Process
170"
THE REVERSE DIFFUSION PROCESS,0.1796669588080631,"We now model the reverse process as another Markov chain conditioned on the set of objects X. We
171"
THE REVERSE DIFFUSION PROCESS,0.18054338299737072,"denote the set of realizable reverse permutations as T , and the neighbours of X with respect to T as
172"
THE REVERSE DIFFUSION PROCESS,0.18141980718667836,"NT (X) := {QσX : σ ∈T }. The conditional joint distribution is given by
173"
THE REVERSE DIFFUSION PROCESS,0.18229623137598597,"pθ(X0:T |X) = p(XT |X)
YT"
THE REVERSE DIFFUSION PROCESS,0.1831726555652936,"t=1 pθ(Xt−1|Xt),
(4)"
THE REVERSE DIFFUSION PROCESS,0.18404907975460122,where pθ(Xt−1|Xt) = P
THE REVERSE DIFFUSION PROCESS,0.18492550394390886,"σ′
t∈T p(Xt−1|Xt, σ′
t)pθ(σ′
t|Xt). To sample from p(XT |X), one simply
174"
THE REVERSE DIFFUSION PROCESS,0.18580192813321647,"samples a random permutation from the uniform distribution and then shuffle the objects accordingly
175"
THE REVERSE DIFFUSION PROCESS,0.1866783523225241,"to obtain XT . p(Xt−1|Xt, σ′
t) is again a delta distribution δ(Xt−1 = Qσ′
tXt). We have
176"
THE REVERSE DIFFUSION PROCESS,0.18755477651183172,"pθ(Xt−1|Xt) =
pθ (σ′
t|Xt)
if Xt−1 ∈NT (Xt)
0
otherwise,
(5)"
THE REVERSE DIFFUSION PROCESS,0.18843120070113936,"where Xt−1 ∈NT (Xt) is equivalent to σ′
t ∈T and Xt−1 = Qσ′
tXt. In the following, we will
177"
THE REVERSE DIFFUSION PROCESS,0.18930762489044697,"introduce the specific design choices of the distribution pθ(σ′
t|Xt).
178"
INVERSE CARD SHUFFLING,0.1901840490797546,"3.2.1
Inverse Card Shuffling
179"
INVERSE CARD SHUFFLING,0.19106047326906223,"A natural choice is to use the inverse operations of the aforementioned card shuffling operations in
180"
INVERSE CARD SHUFFLING,0.19193689745836984,"the forward process. Specifically, for the forward shuffling S, we introduce their inverse operations
181"
INVERSE CARD SHUFFLING,0.19281332164767748,"T := {σ−1 : σ ∈S}, from which we can parameterize pθ(σ′
t|Xt).
182"
INVERSE CARD SHUFFLING,0.1936897458369851,"Inverse Transposition. Since the inverse of a transposition is also a transposition, we can let
183"
INVERSE CARD SHUFFLING,0.19456617002629273,"T := S = {(i
j) : i ̸= j ∈[n]} ∪{Id}. We define a distribution of inverse transposition (IT) over
184"
INVERSE CARD SHUFFLING,0.19544259421560034,"T using n + 1 real-valued parameters s = (s1, . . . , sn) and τ such that
185"
INVERSE CARD SHUFFLING,0.19631901840490798,"pIT(σ) =
1 −ϕ(τ)
if σ = Id
ϕ(τ)
 
ψ(s, πij)1ψ(s, πij)2 + ψ(s, πji)1ψ(s, πji)2

if σ =
 
i
j

where i ̸= j, (6)"
INVERSE CARD SHUFFLING,0.1971954425942156,"where ψ(s, π)i = exp
 
sπ(i)

/
 Pn
k=i exp
 
sπ(k)

and ϕ(·) is the sigmoid function. πij is any
186"
INVERSE CARD SHUFFLING,0.19807186678352323,"permutation starting with i and j, i.e., πij(1) = i and πij(2) = j. πji is any permutation starting
187"
INVERSE CARD SHUFFLING,0.19894829097283084,"with j and i, i.e., πji(1) = j and πji(2) = i.
188"
INVERSE CARD SHUFFLING,0.19982471516213848,"Inverse Insertion. For the random insertion, the inverse operation is to insert some piece to the
189"
INVERSE CARD SHUFFLING,0.2007011393514461,"end. Let inverse_inserti denote the permutation that moves the ith component to the end, and
190"
INVERSE CARD SHUFFLING,0.20157756354075373,"let T := {inverse_inserti : i ∈[n]}. We define a categorial distribution of inverse insertion (II)
191"
INVERSE CARD SHUFFLING,0.20245398773006135,"over T using parameters s = (s1, . . . , sn) such that,
192"
INVERSE CARD SHUFFLING,0.203330411919369,"pII(σ = inverse_inserti) = exp(si)/
Pn
j=1 exp(sj)

.
(7)"
INVERSE CARD SHUFFLING,0.2042068361086766,"Inverse Riffle Shuffle. In the riffle shuffle, the deck of card is first cut into two piles, and the two piles
193"
INVERSE CARD SHUFFLING,0.20508326029798424,"are riffled together. So to undo a riffle shuffle, we need to figure out which pile each card belongs to,
194"
INVERSE CARD SHUFFLING,0.20595968448729185,"i.e., making a sequence of n binary decisions. We define the Inverse Riffle Shuffle (IRS) distribution
195"
INVERSE CARD SHUFFLING,0.20683610867659946,"using parameters s = (s1, . . . , sn) as follows. Starting from the last (the nth) object, each object i
196"
INVERSE CARD SHUFFLING,0.2077125328659071,"has probability ϕ(si) of being put on the top of the left pile. Otherwise, it falls on the top of the right
197"
INVERSE CARD SHUFFLING,0.2085889570552147,"pile. Finally, put the left pile on top of the right pile, which gives the shuffled result.
198"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.20946538124452235,"3.2.2
The Plackett-Luce Distribution and Its Generalization
199"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21034180543382996,"Other than specific inverse shuffling methods to parameterize the reverse process, we also consider
200"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2112182296231376,"general distributions pθ(σ′
t|Xt) whose support are the whole Sn, i.e., T = Sn.
201"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21209465381244522,"The PL Distribution. A popular distribution over Sn is the Plackett-Luce (PL) distribution [35, 27],
202"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21297107800175286,"which is constructed from n real-valued scores s = (s1, . . . , sn) as follows,
203"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21384750219106047,"pPL(σ) =
Yn"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2147239263803681,"i=1 exp
 
sσ(i)

/
Pn
j=i exp
 
sσ(j)

,
(8)"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21560035056967572,"for all σ ∈Sn. Intuitively, (s1, . . . , sn) represents the preference given to each index in [n]. To
204"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21647677475898336,"sample from PLs, we first sample σ(1) from Cat(n, softmax(s)). Then we remove σ(1) from the
205"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21735319894829097,"list and sample σ(2) from the categorical distribution corresponding to the rest of the scores (logits).
206"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2182296231375986,"We continue in this manner until we have sampled σ(1), . . . , σ(n). By [7], the mode of the PL
207"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21910604732690622,"distribution is the permutation that sorts s in descending order.
208"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.21998247151621383,"The Generalized PL (GPL) Distribution. We also propose a generalization of the PL distribution,
209"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22085889570552147,"referred to as Generalized Plackett-Luce (GPL) Distribution. Unlike the PL distribution, which uses
210"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22173531989482909,"a set of n scores, the GPL distribution uses n2 scores {s1, · · · , sn}, where each si = {si,1, . . . , si,n}
211"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22261174408413673,"consists of n scores. The GPL distribution is constructed as follows,
212"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22348816827344434,"pGPL(σ) :=
Yn"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22436459246275198,"i=1 exp
 
si,σ(i)

/
Pn
j=i exp
 
si,σ(j)

.
(9)"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2252410166520596,"Sampling of the GPL distribution begins with sampling σ(1) using n scores s1. For 2 ≤i ≤n, we
213"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22611744084136723,"remove i−1 scores from si that correspond to σ(1), . . . , σ(i−1) and sample σ(i) from a categorical
214"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22699386503067484,"distribution constructed from the remaining n −i + 1 scores in si. It is important to note that the
215"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22787028921998248,"family of PL distributions is a strict subset of the GPL family. Since the GPL distribution has more
216"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2287467134092901,"parameters than the PL distribution, it is expected to be more expressive. In fact, when considering
217"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.22962313759859773,"their ability to express the delta distribution, which is the target distribution for many permutation
218"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.23049956178790534,"learning problems, we have the following result.
219"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.23137598597721298,"Proposition 1. The PL distribution cannot exactly represent a delta distribution. That is, there does
220"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2322524101665206,"not exist an s such that pPL = δσ for any σ ∈Sn, where δσ(σ) = 1 and δσ(π) = 0 for all π ̸= σ.
221"
THE PLACKETT-LUCE DISTRIBUTION AND ITS GENERALIZATION,0.2331288343558282,"But the GPL distribution can represent a delta distribution exactly.
222"
NETWORK ARCHITECTURE AND TRAINING,0.23400525854513585,"3.3
Network Architecture and Training
223"
NETWORK ARCHITECTURE AND TRAINING,0.23488168273444346,"We now briefly introduce how to use neural networks to parameterize the above distributions used
224"
NETWORK ARCHITECTURE AND TRAINING,0.2357581069237511,"in the reverse process. At any time t, given Xt ∈Rn×d, we use a neural network with parameters
225"
NETWORK ARCHITECTURE AND TRAINING,0.2366345311130587,"θ to construct pθ(σ′
t|Xt). In particular, we treat n rows of Xt as n tokens and use a Transformer
226"
NETWORK ARCHITECTURE AND TRAINING,0.23751095530236635,"architecture along with the time embedding of t and the positional encoding to predict the previously
227"
NETWORK ARCHITECTURE AND TRAINING,0.23838737949167396,"mentioned scores. For example, for the GPL distribution, to predict n2 scores, we introduce n dummy
228"
NETWORK ARCHITECTURE AND TRAINING,0.2392638036809816,"tokens that correspond to the n permuted output positions. We then perform a few layers of masked
229"
NETWORK ARCHITECTURE AND TRAINING,0.2401402278702892,"self-attention (2n × 2n) to obtain the token embedding Z1 ∈Rn×dmodel corresponding to n input
230"
NETWORK ARCHITECTURE AND TRAINING,0.24101665205959685,"tokens and Z2 ∈Rn×dmodel corresponding to n dummy tokens. Finally, the GPL score matrix is
231"
NETWORK ARCHITECTURE AND TRAINING,0.24189307624890447,"obtained as Sθ = Z1Z⊤
2 ∈Rn×n. Since the aforementioned distributions have different numbers of
232"
NETWORK ARCHITECTURE AND TRAINING,0.2427695004382121,"scores, the specific architectures of the Transformer differ. We provide more details in Appendix B.
233"
NETWORK ARCHITECTURE AND TRAINING,0.24364592462751972,"To learn the diffusion model, we maximize the following variational lower bound:
234"
NETWORK ARCHITECTURE AND TRAINING,0.24452234881682736,"Epdata(X0,X)
h
log pθ(X0|X)
i
≥Epdata(X0,X)q(X1:T |X0,X) """
NETWORK ARCHITECTURE AND TRAINING,0.24539877300613497,"log p(XT |X) + T
X"
NETWORK ARCHITECTURE AND TRAINING,0.2462751971954426,"t=1
log pθ(Xt−1|Xt)"
NETWORK ARCHITECTURE AND TRAINING,0.24715162138475022,q(Xt|Xt−1) #
NETWORK ARCHITECTURE AND TRAINING,0.24802804557405783,".
(10)"
NETWORK ARCHITECTURE AND TRAINING,0.24890446976336547,"In practice, one can draw samples to obtain the Monte Carlo estimation of the lower bound. Due to
235"
NETWORK ARCHITECTURE AND TRAINING,0.24978089395267308,"the complexity of shuffling transition in the forward process, we can not obtain q(Xt|X0) analytically,
236"
NETWORK ARCHITECTURE AND TRAINING,0.2506573181419807,"as is done in common diffusion models [16, 3]. Therefore, we have to run the forward process to
237"
NETWORK ARCHITECTURE AND TRAINING,0.25153374233128833,"collect samples. Fortunately, it is efficient as the forward process only involves shuffling integers. We
238"
NETWORK ARCHITECTURE AND TRAINING,0.25241016652059595,"include more training details in Appendix E.
239"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2532865907099036,"3.4
Denoising Schedule via Merging Reverse Steps
240"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2541630148992112,"If one merges some steps in the reverse process, sampling and learning would be faster and more
241"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.25503943908851884,"memory efficient. The variance of the training loss could also be reduced. Specifically, at time t of the
242"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.25591586327782645,"reverse process, instead of predicting pθ(Xt−1|Xt), we can predict pθ(Xt′|Xt) for any 0 ≤t′ < t.
243"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2567922874671341,"Given a sequence of timesteps 0 = t0 < · · · < tk = T, we can now model the reverse process as
244"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.25766871165644173,"pθ(Xt0, . . . , Xtk|X) = p(XT |X)
Yk"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.25854513584574934,"i=1 pθ(Xti−1|Xti).
(11)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.25942156003505695,"To align with the literature of diffusion models, we call the list [t0, . . . , tk] the denoising schedule.
245"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26029798422436456,"After incorporating the denoising schedule in Eq. (10), we obtain the loss function:
246"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26117440841367223,"L(θ) = Epdata(X0,X)Eq(X1:T |X0,X) """
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26205083260297984,"−log p(XT |X) − k
X"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26292725679228746,"i=1
log pθ(Xti−1|Xti)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26380368098159507,q(Xti|Xti−1) #
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26468010517090274,".
(12)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26555652936021035,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
t 0.0 0.2 0.4 0.6 0.8 1.0"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26643295354951796,"TV Distance to Unifrom, n=100 (a)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26730937773882557,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0.2 0.4 0.6 0.8 1.0"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26818580192813324,"(b)
Figure 2: (a) DTV(q(t)
RS, u) computed using Eq. (14). We choose T = 15 (red dot) based on the
threshold 0.005. (b) A heatmap for DTV(q(t)
RS, q(t′)
RS ) for n = 100 and 1 ≤t < t′ ≤15, computed
using Eq. (13). Rows are t and columns are t′. We choose the denoising schedule [0, 8, 10, 15]."
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26906222611744085,"Note that although we may not have the analytical form of q(Xti|Xti−1), we can draw samples
247"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.26993865030674846,"from it. Merging is feasible if the support of pθ(Xti−1|Xti) is equal or larger than the support
248"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2708150744960561,"of q(Xti|Xti−1); otherwise, the inverse of some forward permutations would be almost surely
249"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27169149868536374,"unrecoverable. Therefore, we can implement a non-trivial denoising schedule (i.e., k < T), when
250"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27256792287467135,"pθ(σ′
t|Xt) follows the PL or GPL distribution, as they have whole Sn as their support. However,
251"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27344434706397897,"merging is not possible for inverse shuffling methods, as their support is smaller than that of the
252"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2743207712532866,"corresponding multi-step forward shuffling. To design a successful denoising schedule, we first
253"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2751971954425942,"describe the intuitive principles and then provide some theoretical insights. 1) The length of forward
254"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27607361963190186,"diffusion T should be minimal so long as the forward process approaches the uniform distribution. 2)
255"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27695004382120947,"If distributions of Xt and Xt+1 are similar, we should merge these two steps. Otherwise, we should
256"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2778264680105171,"not merge them, as it would make the learning problem harder.
257"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2787028921998247,"To quantify the similarity between distributions shown in 1) and 2), the TV distance is commonly
258"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.27957931638913236,"used in the literature. In particular, we can measure DTV(q(t), q(t′)) for t ̸= t′ and DTV(q(t), u),
259"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.28045574057843997,"where q(t) is the distribution at time t in the forward process and u is the uniform distribution. For
260"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2813321647677476,"riffle shuffles, the total variation distance can be computed exactly. Specifically, we first introduce
261"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2822085889570552,"the Eulerian Numbers An,r [32], i.e., the number of permutations in Sn that have exactly r rising
262"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.28308501314636286,"sequences where 1 ≤r ≤n. An,r can be computed using the following recursive formula An,r =
263"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2839614373356705,"rAn−1,r + (n −r + 1)An−1,r−1 where A1,1 = 1. We then have the following result.
264"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2848378615249781,"Proposition 2. Let t ̸= t′ be positive integers. Then
265"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2857142857142857,"DTV

q(t)
RS, q(t′)
RS

= 1 2 n
X"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.28659070990359337,"r=1
An,r 1
2tn"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.287467134092901,"n + 2t −r
n"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2883435582822086,"
−
1
2t′n"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2892199824715162,"n + 2t′ −r
n"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2900964066608238," ,
(13)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2909728308501315,"and
266"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2918492550394391,"DTV

q(t)
RS, u

= 1 2 n
X"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2927256792287467,"r=1
An,r 1
2tn"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2936021034180543,"n + 2t −r
n 
−1 n!"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.294478527607362,".
(14)"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2953549517966696,"267
Note that Eq. (14) was originally given in [19]. We restate it here for completeness. Once the
268"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2962313759859772,"Eulerian numbers are precomputed, the TV distances can be computed in O(n) time instead of O(n!).
269"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2971078001752848,"Through extensive experiments, we have the following empirical observation. For the principle 1),
270"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2979842243645925,"choosing T so that DTV(q(T )
RS , u) ≈0.005 yields good results. For the principle 2), a denoising
271"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2988606485539001,"schedule [t0, . . . , tk] with DTV(q(ti)
RS , q(ti+1)
RS
) ≈0.3 for most i works well. We show an example on
272"
DENOISING SCHEDULE VIA MERGING REVERSE STEPS,0.2997370727432077,"sorting n = 100 four-digit MNIST images in Fig. 2.
273"
REVERSE PROCESS DECODING,0.3006134969325153,"3.5
Reverse Process Decoding
274"
REVERSE PROCESS DECODING,0.30148992112182293,"We now discuss how to decode predictions from the reverse process at test time. In practice, one is
275"
REVERSE PROCESS DECODING,0.3023663453111306,"often interested in the most probable state or a few states with high probabilities under pθ(X0|X).
276"
REVERSE PROCESS DECODING,0.3032427695004382,"However, since we can only draw samples from pθ(X0|X) via running the reverse process, exact
277"
REVERSE PROCESS DECODING,0.3041191936897458,"decoding is intractable. The simplest approximated method is greedy search, i.e., successively finding
278"
REVERSE PROCESS DECODING,0.30499561787905344,"the mode or an approximated mode of pθ(Xti−1|Xti). Another approach is beam search, which
279"
REVERSE PROCESS DECODING,0.3058720420683611,"Method
Metrics
Noisy MNIST
CIFAR-10"
REVERSE PROCESS DECODING,0.3067484662576687,"2 × 2
3 × 3
4 × 4
5 × 5
6 × 6
2 × 2
3 × 3
4 × 4"
REVERSE PROCESS DECODING,0.30762489044697633,"Gumbel-
Sinkhorn
Network [29]"
REVERSE PROCESS DECODING,0.30850131463628394,"Kendall-Tau ↑
0.9984
0.6908
0.3578
0.2430
0.1755
0.8378
0.5044
0.4016
Accuracy (%)
99.81
44.65
00.86
0.00
0.00
76.54
6.07
0.21
Correct (%)
99.91
80.20
49.51
26.94
14.91
86.10
43.59
25.31
RMSE ↓
0.0022
0.1704
0.4572
0.8915
1.0570
0.3749
0.9590
1.0960
MAE ↓
0.0003
0.0233
0.1005
0.3239
0.4515
0.1368
0.5320
0.6873"
REVERSE PROCESS DECODING,0.3093777388255916,DiffSort [34]
REVERSE PROCESS DECODING,0.3102541630148992,"Kendall-Tau ↑
0.9931
0.3054
0.0374
0.0176
0.0095
0.6463
0.1460
0.0490
Accuracy (%)
99.02
5.56
0.00
0.00
0.00
59.18
0.96
0.00
Correct (%)
99.50
42.25
10.77
6.39
3.77
75.48
27.87
12.27
RMSE ↓
0.0689
1.0746
1.3290
1.4883
1.5478
0.7389
1.2691
1.3876
MAE ↓
0.0030
0.4283
0.6531
0.8204
0.8899
0.2800
0.8123
0.9737"
REVERSE PROCESS DECODING,0.31113058720420683,"Error-free
DiffSort [20]"
REVERSE PROCESS DECODING,0.31200701139351444,"Kendall-Tau ↑
0.9899
0.2014
0.0100
0.0034
-0.0021
0.6604
0.1362
0.0318
Accuracy (%)
98.62
0.82
0.00
0.00
0.00
60.96
0.68
0.00
Correct (%)
99.28
32.65
7.40
4.39
2.50
75.99
26.75
10.33
RMSE ↓
0.0814
1.1764
1.3579
1.5084
1.5606
0.7295
1.2820
1.4095
MAE ↓
0.0041
0.5124
0.6818
0.8424
0.9041
0.2731
0.8260
0.9990"
REVERSE PROCESS DECODING,0.3128834355828221,"Symmetric
Diffusers
(Ours)"
REVERSE PROCESS DECODING,0.3137598597721297,"Kendall-Tau ↑
0.9992
0.8126
0.4859
0.2853
0.1208
0.9023
0.8363
0.2518
Accuracy (%)
99.88
57.38
1.38
0.00
0.00
90.15
70.94
0.64
Correct (%)
99.94
86.16
58.51
37.91
18.54
92.99
86.84
34.69
RMSE ↓
0.0026
0.0241
0.1002
0.2926
0.4350
0.3248
0.3892
0.8953
MAE ↓
0.0001
0.0022
0.0130
0.0749
0.1587
0.0651
0.0977
0.5044"
REVERSE PROCESS DECODING,0.31463628396143734,Table 1: Results (averaged over 5 runs) on solving the jigsaw puzzle on Noisy MNIST and CIFAR10.
REVERSE PROCESS DECODING,0.31551270815074495,"Method
Metrics
Sequence Length"
REVERSE PROCESS DECODING,0.31638913234005256,"3
5
7
9
15
32
52
100"
REVERSE PROCESS DECODING,0.3172655565293602,"DiffSort [34]
Kendall-Tau ↑
0.930
0.898
0.864
0.801
0.638
0.535
0.341
0.166
Accuracy (%)
93.8
83.9
71.5
52.2
10.3
0.2
0.0
0.0
Correct (%)
95.8
92.9
90.1
85.2
82.3
61.8
42.8
23.2"
REVERSE PROCESS DECODING,0.31814198071866784,"Error-free
DiffSort [20]"
REVERSE PROCESS DECODING,0.31901840490797545,"Kendall-Tau ↑
0.974
0.967
0.962
0.952
0.938
0.879
0.170
0.140
Accuracy (%)
97.7
95.3
92.9
89.6
83.1
57.1
0.0
0.0
Correct (%)
98.4
97.7
97.2
96.3
95.1
90.1
24.2
20.1"
REVERSE PROCESS DECODING,0.31989482909728306,"Symmetric
Diffusers
(Ours)"
REVERSE PROCESS DECODING,0.32077125328659073,"Kendall-Tau ↑
0.976
0.967
0.959
0.950
0.932
0.858
0.786
0.641
Accuracy (%)
98.0
95.5
92.9
90.0
82.6
55.1
27.4
4.5
Correct (%)
98.5
97.6
96.8
96.1
94.5
88.3
82.1
69.3"
REVERSE PROCESS DECODING,0.32164767747589834,Table 2: Results (averaged over 5 runs) on the four-digit MNIST sorting benchmark.
REVERSE PROCESS DECODING,0.32252410166520595,"maintains a dynamic buffer of k candidates with highest probabilities. Nevertheless, for one-step
280"
REVERSE PROCESS DECODING,0.32340052585451357,"reverse transitions like the GPL distribution, even finding the mode is intractable. To address this, we
281"
REVERSE PROCESS DECODING,0.32427695004382123,"employ a hierarchical beam search that performs an inner beam search within n2 scores at each step
282"
REVERSE PROCESS DECODING,0.32515337423312884,"of the outer beam search. Further details are provided in Appendix C.
283"
EXPERIMENTS,0.32602979842243646,"4
Experiments
284"
EXPERIMENTS,0.32690622261174407,"We now demonstrate the general applicability and effectiveness of our model through a variety of
285"
EXPERIMENTS,0.32778264680105174,"experiments, including sorting 4-digit MNIST numbers, solving jigsaw puzzles, and addressing
286"
EXPERIMENTS,0.32865907099035935,"traveling salesman problems. Additional details are provided in the appendix due to space constraints.
287"
EXPERIMENTS,0.32953549517966696,"4.1
Sorting 4-digit MNIST Images
288"
EXPERIMENTS,0.33041191936897457,"We first evaluate our SymmetricDiffusers on the four-digit MNIST sorting benchmark, a well-
289"
EXPERIMENTS,0.3312883435582822,"established testbed for differentiable sorting [5, 8, 13, 20, 33, 34]. Each four-digit image in this
290"
EXPERIMENTS,0.33216476774758985,"benchmark is obtained by concatenating 4 individual images from MNIST. For evaluation, we
291"
EXPERIMENTS,0.33304119193689746,"employ several metrics to compare methods, including Kendall-Tau coefficient (measuring the
292"
EXPERIMENTS,0.3339176161262051,"correlation between rankings), accuracy (percentage of images perfectly reassembled), and correctness
293"
EXPERIMENTS,0.3347940403155127,"(percentage of pieces that are correctly placed).
294"
EXPERIMENTS,0.33567046450482035,"Ablation Study. We conduct an ablation study to verify our design choices for reverse transition and
295"
EXPERIMENTS,0.33654688869412797,"decoding strategies. As shown in Table 3, combining PL with either beam search (BS) or greedy
296"
EXPERIMENTS,0.3374233128834356,"search yields good results in terms of Kendall-Tau and correctness metrics. In contrast, the IRS
297"
EXPERIMENTS,0.3382997370727432,"(inverse riffle shuffle) method, along with greedy search, performs poorly across all metrics, showing
298"
EXPERIMENTS,0.33917616126205086,"the limitations of IRS in handling complicated sorting tasks. Finally, combining GPL and BS achieves
299"
EXPERIMENTS,0.34005258545135847,"the best accuracy in correctly sorting the entire sequence of images. Given that accuracy is the most
300"
EXPERIMENTS,0.3409290096406661,"GPL + BS
GPL + Greedy
PL + Greedy
PL + BS
IRS + Greedy"
EXPERIMENTS,0.3418054338299737,"Kendall-Tau ↑
0.786
0.799
0.799
0.797
0.390
Accuracy (%)
27.4
24.4
26.4
26.4
0.6
Correct (%)
82.1
81.6
83.3
83.1
44.6"
EXPERIMENTS,0.3426818580192813,"Table 3: Ablation study on transitions of reverse diffusion and decoding strategies. Results are
averaged over three runs on sorting 52 four-digit MNIST images."
EXPERIMENTS,0.34355828220858897,"Method
OR Solvers
Learning-Based Models"
EXPERIMENTS,0.3444347063978966,"Gurobi [14]
Concorde [1]
LKH-3 [15]
2-Opt [25]
GCN* [18]
DIFUSCO* [43]
Ours"
EXPERIMENTS,0.3453111305872042,"Tour Length ↓
3.842
3.843
3.842
4.020
3.850
3.883
3.849
Optimality Gap (%) ↓
0.00
0.00
0.00
4.64
0.21
1.07
0.18"
EXPERIMENTS,0.3461875547765118,Table 4: Results on TSP-20. * means we remove the post-processing heuristics for a fair comparison.
EXPERIMENTS,0.3470639789658195,"challenging metric to improve, we selecte GPL and BS for all remaining experiments. More ablation
301"
EXPERIMENTS,0.3479404031551271,"study (e.g., denoising schedule) is provided in Appendix E.2.
302"
EXPERIMENTS,0.3488168273444347,"Full Results. From Table 2, we can see that Error-free DiffSort achieves the best performance in
303"
EXPERIMENTS,0.3496932515337423,"sorting sequences with lengths up to 32. However, its performances drop significantly with long
304"
EXPERIMENTS,0.35056967572305,"sequences (e.g., length of 52 or 100). Meanwhile, DiffSort performs the worse due to the error
305"
EXPERIMENTS,0.3514460999123576,"accumulation of its soft differentiable swap function [20, 33]. In contrast, our method is on par with
306"
EXPERIMENTS,0.3523225241016652,"Error-free DiffSort in sorting short sequences and significantly outperforms others on long sequences.
307"
JIGSAW PUZZLE,0.3531989482909728,"4.2
Jigsaw Puzzle
308"
JIGSAW PUZZLE,0.3540753724802805,"We then explore image reassembly from segmented ""jigsaw"" puzzles [29, 31, 39]. We evaluate the
309"
JIGSAW PUZZLE,0.3549517966695881,"performance using the MNIST and the CIFAR10 datasets, which comprises puzzles of up to 6×6 and
310"
JIGSAW PUZZLE,0.3558282208588957,"4×4 pieces respectively. We add slight noise to pieces from the MNIST dataset to ensure background
311"
JIGSAW PUZZLE,0.3567046450482033,"pieces are distinctive. To evaluate our models, we use Kendall-Tau coefficient, accuracy, correctness,
312"
JIGSAW PUZZLE,0.35758106923751093,"RMSE (root mean square error of reassembled images), and MAE (mean absolute error) as metrics.
313"
JIGSAW PUZZLE,0.3584574934268186,"Table 1 presents results comparing our method with the Gumbel-Sinkhorn Network[29], Diffsort
314"
JIGSAW PUZZLE,0.3593339176161262,"[34], and Error-free Diffsort [20]. DiffSort and Error-free DiffSort are primarily designed for sorting
315"
JIGSAW PUZZLE,0.3602103418054338,"high-dimensional ordinal data which have clearly different patterns. Since jigsaw puzzles on MNIST
316"
JIGSAW PUZZLE,0.36108676599474143,"and CIFAR10 contain pieces that are visually similar, these methods do not perform well. The
317"
JIGSAW PUZZLE,0.3619631901840491,"Gumbel-Sinkhorn performs better for tasks involving fewer than 4 × 4 pieces. In more challenging
318"
JIGSAW PUZZLE,0.3628396143733567,"scenarios (e.g., 5 × 5 and 6 × 6), our method significantly outperforms all competitors.
319"
THE TRAVELLING SALESMAN PROBLEM,0.3637160385626643,"4.3
The Travelling Salesman Problem
320"
THE TRAVELLING SALESMAN PROBLEM,0.36459246275197194,"At last, we explore the travelling salesman problem (TSP) to demonstrate the general applicability of
321"
THE TRAVELLING SALESMAN PROBLEM,0.3654688869412796,"our model. TSPs are classical NP-complete combinatorial optimization problems which are solved
322"
THE TRAVELLING SALESMAN PROBLEM,0.3663453111305872,"using integer programming or heuristic solvers [2, 12]. There exists a vast literature on learning-based
323"
THE TRAVELLING SALESMAN PROBLEM,0.3672217353198948,"models to solve TSPs [22, 23, 18, 17, 6, 24, 10, 36, 21, 43, 30]. They often focus on the Euclidean
324"
THE TRAVELLING SALESMAN PROBLEM,0.36809815950920244,"TSPs, which are formulated as follows. Let V = {v1, . . . , vn} be points in R2. We need to find some
325"
THE TRAVELLING SALESMAN PROBLEM,0.3689745836985101,"σ ∈Sn such that Pn
i=1 ∥vσ(i) −vσ(i+1)∥2 is minimized, where we let σ(n + 1) := σ(1). Further
326"
THE TRAVELLING SALESMAN PROBLEM,0.3698510078878177,"experimental details are provided in Appendix B.
327"
THE TRAVELLING SALESMAN PROBLEM,0.37072743207712533,"We compare with operations research (OR) solvers and other learning based approaches on TSP
328"
THE TRAVELLING SALESMAN PROBLEM,0.37160385626643294,"instances with 20 nodes. The metrics are the total tour length and the optimality gap. Given the ground
329"
THE TRAVELLING SALESMAN PROBLEM,0.37248028045574055,"truth (GT) length produced by the best OR solver, the optimality gap is given by
 
predicted length −
330"
THE TRAVELLING SALESMAN PROBLEM,0.3733567046450482,"(GT length)

/(GT length). As shown in Table 4, SymmetricDiffusers achieves comparable results
331"
THE TRAVELLING SALESMAN PROBLEM,0.37423312883435583,"with both OR solvers and the state-of-the-art learning-based methods.
332"
CONCLUSION,0.37510955302366344,"5
Conclusion
333"
CONCLUSION,0.37598597721297106,"In this paper, we introduce a novel discrete diffusion model over finite symmetric groups. We identify
334"
CONCLUSION,0.3768624014022787,"the riffle shuffle as an effective forward transition and provide empirical rules for selecting the
335"
CONCLUSION,0.37773882559158634,"diffusion length. Additionally, we propose a generalized PL distribution for the reverse transition,
336"
CONCLUSION,0.37861524978089395,"which is provably more expressive than the PL distribution. We further introduce a theoretically
337"
CONCLUSION,0.37949167397020156,"grounded ""denoising schedule"" to improve sampling and learning efficiency. Extensive experiments
338"
CONCLUSION,0.3803680981595092,"verify the effectiveness of our proposed model. In the future, we are interested in generalizing our
339"
CONCLUSION,0.38124452234881684,"model to general finite groups and exploring diffusion models on Lie groups.
340"
REFERENCES,0.38212094653812445,"References
341"
REFERENCES,0.38299737072743206,"[1] David Applegate, Robert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006.
342"
REFERENCES,0.3838737949167397,"[2] Sanjeev Arora. Polynomial time approximation schemes for euclidean traveling salesman and
343"
REFERENCES,0.38475021910604734,"other geometric problems. J. ACM, 45(5):753–782, Sep 1998.
344"
REFERENCES,0.38562664329535495,"[3] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.
345"
REFERENCES,0.38650306748466257,"Structured denoising diffusion models in discrete state-spaces, 2023.
346"
REFERENCES,0.3873794916739702,"[4] Dave Bayer and Persi Diaconis. Trailing the dovetail shuffle to its lair. The Annals of Applied
347"
REFERENCES,0.38825591586327785,"Probability, 2(2):294 – 313, 1992.
348"
REFERENCES,0.38913234005258546,"[5] Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable
349"
REFERENCES,0.39000876424189307,"sorting and ranking. In International Conference on Machine Learning, pages 950–959. PMLR,
350"
REFERENCES,0.3908851884312007,"2020.
351"
REFERENCES,0.39176161262050835,"[6] Xavier Bresson and Thomas Laurent. The transformer network for the traveling salesman
352"
REFERENCES,0.39263803680981596,"problem, 2021.
353"
REFERENCES,0.39351446099912357,"[7] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise
354"
REFERENCES,0.3943908851884312,"approach to listwise approach. In Proceedings of the 24th International Conference on Machine
355"
REFERENCES,0.39526730937773885,"Learning, ICML ’07, pages 129–136, New York, NY, USA, 2007. Association for Computing
356"
REFERENCES,0.39614373356704646,"Machinery.
357"
REFERENCES,0.3970201577563541,"[8] Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
358"
REFERENCES,0.3978965819456617,"optimal transport. Advances in neural information processing systems, 32, 2019.
359"
REFERENCES,0.3987730061349693,"[9] Persi Diaconis. Group representations in probability and statistics. Lecture notes-monograph
360"
REFERENCES,0.39964943032427697,"series, 11:i–192, 1988.
361"
REFERENCES,0.4005258545135846,"[10] Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. Generalize a small pre-trained model to
362"
REFERENCES,0.4014022787028922,"arbitrarily large tsp instances, 2021.
363"
REFERENCES,0.4022787028921998,"[11] E. N. Gilbert. Theory of shuffling. Bell Telephone Laboratories Memorandum, 1955.
364"
REFERENCES,0.40315512708150747,"[12] Teofilo F. Gonzalez. Handbook of Approximation Algorithms and Metaheuristics (Chapman &
365"
REFERENCES,0.4040315512708151,"Hall/Crc Computer & Information Science Series). Chapman & Hall/CRC, 2007.
366"
REFERENCES,0.4049079754601227,"[13] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting
367"
REFERENCES,0.4057843996494303,"networks via continuous relaxations. In International Conference on Learning Representations,
368"
REFERENCES,0.406660823838738,"2018.
369"
REFERENCES,0.4075372480280456,"[14] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.
370"
REFERENCES,0.4084136722173532,"[15] Keld Helsgaun. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling
371"
REFERENCES,0.4092900964066608,"salesman and vehicle routing problems, Dec 2017.
372"
REFERENCES,0.4101665205959685,"[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.
373"
REFERENCES,0.4110429447852761,"[17] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, and Thomas Laurent. Learning
374"
REFERENCES,0.4119193689745837,"tsp requires rethinking generalization. In International Conference on Principles and Practice
375"
REFERENCES,0.4127957931638913,"of Constraint Programming, 2021.
376"
REFERENCES,0.4136722173531989,"[18] Chaitanya K. Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional
377"
REFERENCES,0.4145486415425066,"network technique for the travelling salesman problem, 2019.
378"
REFERENCES,0.4154250657318142,"[19] Shihan Kanungo. Mixing time estimates for the riffle shuffle. Euler Circle, 2020.
379"
REFERENCES,0.4163014899211218,"[20] Jungtaek Kim, Jeongbeen Yoon, and Minsu Cho. Generalized neural sorting networks with error-
380"
REFERENCES,0.4171779141104294,"free differentiable swap functions. In International Conference on Learning Representations
381"
REFERENCES,0.4180543382997371,"(ICLR), 2024.
382"
REFERENCES,0.4189307624890447,"[21] Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity for neural
383"
REFERENCES,0.4198071866783523,"combinatorial optimization, 2023.
384"
REFERENCES,0.42068361086765993,"[22] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
385"
REFERENCES,0.4215600350569676,"networks, 2017.
386"
REFERENCES,0.4224364592462752,"[23] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems!,
387"
REFERENCES,0.4233128834355828,"2019.
388"
REFERENCES,0.42418930762489043,"[24] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai
389"
REFERENCES,0.42506573181419804,"Min. Pomo: Policy optimization with multiple optima for reinforcement learning, 2021.
390"
REFERENCES,0.4259421560035057,"[25] Shen Lin and Brian W Kernighan. An effective heuristic algorithm for the travelling-salesman
391"
REFERENCES,0.4268185801928133,"problem. Operations research, 21(2):498–516, 1973.
392"
REFERENCES,0.42769500438212094,"[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
393"
REFERENCES,0.42857142857142855,"[27] R. D. Luce. Individual Choice Behavior. John Wiley, 1959.
394"
REFERENCES,0.4294478527607362,"[28] Colin L Mallows. Non-null ranking models. i. Biometrika, 44(1/2):114–130, 1957.
395"
REFERENCES,0.4303242769500438,"[29] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permuta-
396"
REFERENCES,0.43120070113935144,"tions with gumbel-sinkhorn networks. In International Conference on Learning Representations,
397"
REFERENCES,0.43207712532865905,"2018.
398"
REFERENCES,0.4329535495179667,"[30] Yimeng Min, Yiwei Bai, and Carla P. Gomes. Unsupervised learning for solving the travelling
399"
REFERENCES,0.43382997370727433,"salesman problem, 2024.
400"
REFERENCES,0.43470639789658194,"[31] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving
401"
REFERENCES,0.43558282208588955,"jigsaw puzzles. In European conference on computer vision, pages 69–84. Springer, 2016.
402"
REFERENCES,0.4364592462751972,"[32] OEIS Foundation Inc. The eulerian numbers, entry a008292 in the On-Line Encyclopedia of
403"
REFERENCES,0.43733567046450483,"Integer Sequences, 2024. Published electronically at http://oeis.org/A008292.
404"
REFERENCES,0.43821209465381245,"[33] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Differentiable sorting
405"
REFERENCES,0.43908851884312006,"networks for scalable sorting and ranking supervision. In International conference on machine
406"
REFERENCES,0.43996494303242767,"learning, pages 8546–8555. PMLR, 2021.
407"
REFERENCES,0.44084136722173534,"[34] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Monotonic differentiable
408"
REFERENCES,0.44171779141104295,"sorting networks. In International Conference on Learning Representations (ICLR), 2022.
409"
REFERENCES,0.44259421560035056,"[35] R. L. Plackett. The analysis of permutations. Applied Statistics, 24(2):193 – 202, 1975.
410"
REFERENCES,0.44347063978965817,"[36] Ruizhong Qiu, Zhiqing Sun, and Yiming Yang. Dimes: A differentiable meta solver for
411"
REFERENCES,0.44434706397896584,"combinatorial optimization problems, 2022.
412"
REFERENCES,0.44522348816827345,"[37] J. Reeds. Theory of shuffling. Unpublished Manuscript, 1981.
413"
REFERENCES,0.44609991235758106,"[38] Laurent Saloff-Coste. Random Walks on Finite Groups, pages 263–346. Springer Berlin
414"
REFERENCES,0.4469763365468887,"Heidelberg, Berlin, Heidelberg, 2004.
415"
REFERENCES,0.44785276073619634,"[39] Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet:
416"
REFERENCES,0.44872918492550395,"Visual permutation learning. In Proceedings of the IEEE Conference on Computer Vision and
417"
REFERENCES,0.44960560911481157,"Pattern Recognition, pages 3949–3957, 2017.
418"
REFERENCES,0.4504820333041192,"[40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-
419"
REFERENCES,0.45135845749342685,"pervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei,
420"
REFERENCES,0.45223488168273446,"editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of
421"
REFERENCES,0.45311130587204207,"Proceedings of Machine Learning Research, pages 2256–2265, Lille, France, 07–09 Jul 2015.
422"
REFERENCES,0.4539877300613497,"PMLR.
423"
REFERENCES,0.4548641542506573,"[41] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
424"
REFERENCES,0.45574057843996496,"distribution, 2020.
425"
REFERENCES,0.4566170026292726,"[42] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and
426"
REFERENCES,0.4574934268185802,"Ben Poole. Score-based generative modeling through stochastic differential equations, 2021.
427"
REFERENCES,0.4583698510078878,"[43] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial
428"
REFERENCES,0.45924627519719546,"optimization, 2023.
429"
REFERENCES,0.4601226993865031,"[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
430"
REFERENCES,0.4609991235758107,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
431"
REFERENCES,0.4618755477651183,"[45] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal
432"
REFERENCES,0.46275197195442597,"Frossard. Digress: Discrete denoising diffusion for graph generation, 2023.
433"
REFERENCES,0.4636283961437336,"[46] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
434"
REFERENCES,0.4645048203330412,"ment learning. Machine Learning, 8(3-4):229–256, 1992.
435"
REFERENCES,0.4653812445223488,"A
Additional Details of the GSR Riffle Shuffle Model
436"
REFERENCES,0.4662576687116564,"There are many equivalent definitions of the GSR riffle shuffle. Here we also introduce the Geometric
437"
REFERENCES,0.4671340929009641,"Description [4], which is easy to implement (and is how we implement riffle shuffles in our experi-
438"
REFERENCES,0.4680105170902717,"ments). We first sample n points in the unit interval [0, 1] uniformly and independently, and suppose
439"
REFERENCES,0.4688869412795793,"the points are labeled in order as x1 < x2 < · · · < xn. Then, the permutation that sorts the points
440"
REFERENCES,0.4697633654688869,"{2x1}, . . . , {2xn} follows the GSR distribution, where {x} := x −⌊x⌋is the fractional part of x.
441"
REFERENCES,0.4706397896581946,"B
Details of Our Network Architecture
442"
REFERENCES,0.4715162138475022,"We now discuss how to use neural networks to produce the parameters of the distributions discussed
443"
REFERENCES,0.4723926380368098,"in Section 3.2.1 and 3.2.2. Fix time t, and suppose Xt =
 
x(t)
1 , . . . , x(t)
n
⊤∈Rn×d. Let encoderθ
444"
REFERENCES,0.4732690622261174,"be an object-specific encoder such that encoderθ(Xt) ∈Rn×dmodel. For example, encoderθ can be
445"
REFERENCES,0.4741454864154251,"a CNN if Xt is an image. Let
446"
REFERENCES,0.4750219106047327,"Yt := encoderθ(Xt) + time_embd(t) =
 
y(t)
1 , . . . , y(t)
n
⊤∈Rn×dmodel,
(15)"
REFERENCES,0.4758983347940403,"where time_embd is the sinusoidal time embedding. Then, we would like to feed the embeddings into
447"
REFERENCES,0.4767747589833479,"a Transformer encoder [44]. Let transformer_encoderθ be the encoder part of the Transformer
448"
REFERENCES,0.4776511831726556,"architecture. However, each of the distributions we discussed previously has different number of
449"
REFERENCES,0.4785276073619632,"parameters, so we will have to discuss them separately.
450"
REFERENCES,0.4794040315512708,"Inverse Transposition.
For Inverse Transposition, we have n + 1 parameters. To obtain n + 1
451"
REFERENCES,0.4802804557405784,"tokens from transformer_encoderθ, we append a dummy token of 0’s to Yt. Then we input
452
 
y(t)
1 , . . . , y(t)
n , 0
⊤into transformer_encoderθ to obtain Z ∈R(n+1)×dmodel. Finally, we apply
453"
REFERENCES,0.48115687992988604,"an MLP to obtain (s1, . . . , sn, k) ∈Rn+1.
454"
REFERENCES,0.4820333041191937,"Inverse Insertion, Inverse Riffle Shuffle, PL Distribution.
These three distributions all require
455"
REFERENCES,0.4829097283085013,"exactly n parameters, so we can directly feed Yt into transformer_encoderθ. Let the output
456"
REFERENCES,0.48378615249780893,"of transformer_encoderθ be Z ∈Rn×dmodel, where we then apply an MLP to obtain the scores
457"
REFERENCES,0.48466257668711654,"sθ ∈Rn.
458"
REFERENCES,0.4855390008764242,"The GPL Distribution.
The GPL distribution requires n2 parameters. We first append n dummy
459"
REFERENCES,0.4864154250657318,"tokens of 0’s to Yt, with the intent that the jth dummy token would learn information about the
460"
REFERENCES,0.48729184925503943,"jth column of the GPL parameter matrix, which represents where the jth component should be
461"
REFERENCES,0.48816827344434705,"placed. We then pass
 
y(t)
1 , . . . , y(t)
n , 0, . . . , 0
⊤∈R2n×dmodel to transformer_encoderθ. When
462"
REFERENCES,0.4890446976336547,"computing attention, we further apply a 2n × 2n attention mask
463"
REFERENCES,0.4899211218229623,"M :=

0
A
0
B"
REFERENCES,0.49079754601226994,"
, where A is an n × n matrix of −∞, B =  "
REFERENCES,0.49167397020157755,"−∞
−∞
· · ·
−∞
0
−∞
· · ·
−∞
...
...
...
...
0
0
· · ·
−∞ "
REFERENCES,0.4925503943908852,is n × n.
REFERENCES,0.4934268185801928,"The reason for having B as an upper triangular matrix of −∞is that information about the jth
464"
REFERENCES,0.49430324276950044,"component should only require information from the previous components. Let
465"
REFERENCES,0.49517966695880805,"transformer_encoderθ(Yt, M) =

Z1
Z2 
,"
REFERENCES,0.49605609114811566,"where Z1, Z2 ∈Rn×dmodel. Finally, we obtain the GPL parameter matrix as Sθ = Z1Z⊤
2 ∈Rn×n.
466"
REFERENCES,0.49693251533742333,"For hyperparameters, we refer the readers to Appendix E.4.
467"
REFERENCES,0.49780893952673094,"C
Additional Details of Decoding
468"
REFERENCES,0.49868536371603855,"Greedy Search.
At each timestep ti in the denoising schedule, we can greedily obtain or approx-
469"
REFERENCES,0.49956178790534617,"imate the mode of pθ(Xti−1|Xti). We can then use the (approximated) mode Xti−1 for the next
470"
REFERENCES,0.5004382120946538,"timestep pθ(Xti−2|Xti−1). Note that the final X0 obtained using such a greedy heuristic may not
471"
REFERENCES,0.5013146362839614,"necessarily be the mode of pθ(X0|X).
472"
REFERENCES,0.5021910604732691,"Beam Search.
We can use beam search to improve the greedy approach. The basic idea is that,
473"
REFERENCES,0.5030674846625767,"at each timestep ti in the denoising schedule, we compute or approximate the top-k-most-probable
474"
REFERENCES,0.5039439088518843,"results from pθ(Xti−1|Xti). For each of the top-k results, we sample top-k from pθ(Xti−2|Xti−1).
475"
REFERENCES,0.5048203330411919,"Now we have k2 candidates for Xti−2, and we only keep the top k of the k2 candidates.
476"
REFERENCES,0.5056967572304996,"However, it is not easy to obtain the top-k-most-probable results for some of the distributions. Here
477"
REFERENCES,0.5065731814198072,"we provide an algorithm to approximate top-k of the PL and the GPL distribution. Since the PL
478"
REFERENCES,0.5074496056091148,"distribution is a strict subset of the GPL distribution, it suffices to only consider the GPL distribution
479"
REFERENCES,0.5083260297984225,"with parameter matrix S. The algorithm for approximating top-k of the GPL distribution is another
480"
REFERENCES,0.50920245398773,"beam search. We first pick the k largest elements from the first row of S. For each of the k largest
481"
REFERENCES,0.5100788781770377,"elements, we pick k largest elements from the second row of S, excluding the corresponding element
482"
REFERENCES,0.5109553023663453,"picked in the first row. We now have k2 candidates for the first two elements of a permutation, and
483"
REFERENCES,0.5118317265556529,"we only keep the top-k-most-probable candidates. We then continue in this manner.
484"
REFERENCES,0.5127081507449606,"D
Proofs
485"
REFERENCES,0.5135845749342682,"Proposition 1. The PL distribution cannot exactly represent a delta distribution. That is, there does
486"
REFERENCES,0.5144609991235758,"not exist an s such that pPL = δσ for any σ ∈Sn, where δσ(σ) = 1 and δσ(π) = 0 for all π ̸= σ.
487"
REFERENCES,0.5153374233128835,"But the GPL distribution can represent a delta distribution exactly.
488"
REFERENCES,0.516213847502191,"Proof. Assume for a contradiction that there exists some σ ∈Sn and s such that PLs = δσ. Then
489"
REFERENCES,0.5170902716914987,"we have
490
n
Y i=1"
REFERENCES,0.5179666958808063,"exp
 
sσ(i)
"
REFERENCES,0.5188431200701139,"Pn
j=i exp
 
sσ(j)
 = 1."
REFERENCES,0.5197195442594216,"Since each of the term in the product is less than or equal to 1, we must have
491"
REFERENCES,0.5205959684487291,"exp
 
sσ(i)
"
REFERENCES,0.5214723926380368,"Pn
j=i exp
 
sσ(j)
 = 1
(16)"
REFERENCES,0.5223488168273445,"for all i ∈[n]. In particular, we have
492"
REFERENCES,0.523225241016652,"exp
 
sσ(1)
"
REFERENCES,0.5241016652059597,"Pn
j=1 exp
 
sσ(j)
 = 1,"
REFERENCES,0.5249780893952674,"which happens if and only if sσ(j) = −∞for all j ≥2. But this contradicts (16).
493"
REFERENCES,0.5258545135845749,"We then show that the GPL distribution can represent a delta distribution exactly. To see this, we fix
494"
REFERENCES,0.5267309377738826,"σ ∈Sn. For all i ∈[n], we let si,σ(i) = 0 and si,j = −∞for all j ̸= σ(i). Then GPL(sij) = δσ.
495"
REFERENCES,0.5276073619631901,"Proposition 2. Let t ̸= t′ be positive integers. Then
496"
REFERENCES,0.5284837861524978,"DTV

q(t)
RS, q(t′)
RS

= 1 2 n
X"
REFERENCES,0.5293602103418055,"r=1
An,r 1
2tn"
REFERENCES,0.530236634531113,"n + 2t −r
n"
REFERENCES,0.5311130587204207,"
−
1
2t′n"
REFERENCES,0.5319894829097284,"n + 2t′ −r
n"
REFERENCES,0.5328659070990359," ,
(13)"
REFERENCES,0.5337423312883436,"and
497"
REFERENCES,0.5346187554776511,"DTV

q(t)
RS, u

= 1 2 n
X"
REFERENCES,0.5354951796669588,"r=1
An,r 1
2tn"
REFERENCES,0.5363716038562665,"n + 2t −r
n 
−1 n!"
REFERENCES,0.537248028045574,".
(14) 498"
REFERENCES,0.5381244522348817,"Proof. Let σ ∈Sn. It was shown in [4] that
499"
REFERENCES,0.5390008764241893,"q(t)
RS(σ) =
1
2tn ·
n + 2t −r
n 
,"
REFERENCES,0.5398773006134969,"where r is the number of rising sequences of σ. Note that if two permutations have the same number
500"
REFERENCES,0.5407537248028046,"of rising sequences, then they have equal probability. Hence, we have
501"
REFERENCES,0.5416301489921121,"DTV

q(t)
RS −q(t′)
RS

= 1 2 X σ∈Sn"
REFERENCES,0.5425065731814198,"q(t)
RS(σ) −q(t′)
RS (σ)
 = 1 2 n
X"
REFERENCES,0.5433829973707275,"r=1
An,r
q(t)
RS(σ) −q(t′)
RS (σ) = 1 2 n
X"
REFERENCES,0.544259421560035,"r=1
An,r 1
2tn"
REFERENCES,0.5451358457493427,"n + 2t −r
n"
REFERENCES,0.5460122699386503,"
−
1
2t′n"
REFERENCES,0.5468886941279579,"n + 2t′ −r
n"
REFERENCES,0.5477651183172656," ,"
REFERENCES,0.5486415425065732,"as claimed. For (14), replace q(t′)
RS (σ) with u(σ) = 1"
REFERENCES,0.5495179666958808,"n! in the above derivations.
502"
REFERENCES,0.5503943908851884,"E
Additional Details on Experiments
503"
REFERENCES,0.551270815074496,"E.1
Datasets
504"
REFERENCES,0.5521472392638037,"Jigsaw Puzzle.
We created the Noisy MNIST dataset by adding i.i.d. Gaussian noise with a mean
505"
REFERENCES,0.5530236634531113,"of 0 and a standard deviation of 0.01 to each pixel of the MNIST images. No noise was added to the
506"
REFERENCES,0.5539000876424189,"CIFAR-10 images. The noisy images are then saved as the Noisy MNIST dataset. During training,
507"
REFERENCES,0.5547765118317266,"each image is divided into n × n patches. A permutation is then sampled uniformly at random
508"
REFERENCES,0.5556529360210342,"to shuffle these patches. The training set for Noisy MNIST comprises 60,000 images, while the
509"
REFERENCES,0.5565293602103418,"CIFAR-10 training set contains 10,000 images. The Noisy MNIST test set, which is pre-shuffled, also
510"
REFERENCES,0.5574057843996494,"includes 10,000 images. The CIFAR-10 test set, which shuffles images on the fly, contains 10,000
511"
REFERENCES,0.558282208588957,"images as well.
512"
REFERENCES,0.5591586327782647,"Sort 4-Digit MNIST Numbers.
For each training epoch, we generate 60,000 sequences of 4-digit
513"
REFERENCES,0.5600350569675723,"MNIST images, each of length n, constructed dynamically on the fly. These 4-digit MNIST numbers
514"
REFERENCES,0.5609114811568799,"are created by concatenating four MNIST images, each selected uniformly at random from the entire
515"
REFERENCES,0.5617879053461875,"MNIST dataset, which consists of 60,000 images. For testing purposes, we similarly generate 10,000
516"
REFERENCES,0.5626643295354952,"sequences of n 4-digit MNIST numbers on the fly.
517"
REFERENCES,0.5635407537248028,"TSP.
We take the TSP-20 dataset from [17] 1. The train set consists of 1,512,000 graphs with 20
518"
REFERENCES,0.5644171779141104,"nodes, where each node is an i.i.d. sample from the unit square [0, 1]2. The labels are optimal TSP
519"
REFERENCES,0.5652936021034181,"tours provided by the Concorde solver [1]. The test set consists of 1,280 graphs with 20 nodes, with
520"
REFERENCES,0.5661700262927257,"ground truth tour generated by the Concorde solver as well.
521"
REFERENCES,0.5670464504820333,"E.2
Ablation Studies
522"
REFERENCES,0.567922874671341,"Choices for Reverse Transition and Decoding Strategies.
As demonstrated in Table 5, we have
523"
REFERENCES,0.5687992988606485,"explored various combinations of forward and inverse shuffling methods across tasks involving
524"
REFERENCES,0.5696757230499562,"different sequence lengths. Both GPL and PL consistently excel in all experimental scenarios,
525"
REFERENCES,0.5705521472392638,"highlighting their robustness and effectiveness. It is important to note that strategies such as random
526"
REFERENCES,0.5714285714285714,"transposition and random insertion paired with their respective inverse operations, are less suitable
527"
REFERENCES,0.5723049956178791,"for tasks with longer sequences. This limitation is attributed to the prolonged mixing times required
528"
REFERENCES,0.5731814198071867,"by these two shuffling methods, a challenge that is thoroughly discussed in Section 3.1.2.
529"
REFERENCES,0.5740578439964943,"Denoising Schedule.
We also conduct an ablation study on how we should merge reverse steps. As
530"
REFERENCES,0.574934268185802,"shown in Table 6, the choice of the denoising schedule can significantly affect the final performance.
531"
REFERENCES,0.5758106923751095,"In particular, for n = 100 on the Sort 4-Digit MNIST Numbers task, the fact that [0, 15] has 0
532"
REFERENCES,0.5766871165644172,"accuracy justifies our motivation to use diffusion to break down learning into smaller steps. The
533"
REFERENCES,0.5775635407537248,"result we get also matches with our proposed heuristic in Section 3.4.
534"
REFERENCES,0.5784399649430324,"E.3
Latent Loss in Jigsaw Puzzle
535"
REFERENCES,0.5793163891323401,"In the original setup of the Jigsaw Puzzle experiment using the Gumbel-Sinkhorn network [29],
536"
REFERENCES,0.5801928133216476,"the permutations are latent. That is, the loss function in Gumbel-Sinkhorn is a pixel-level MSE
537"
REFERENCES,0.5810692375109553,"loss and does not use the ground truth permutation label. However, our loss function (12) actually
538"
REFERENCES,0.581945661700263,"(implicitly) uses the ground truth permutation that maps the shuffled image patches to their original
539"
REFERENCES,0.5828220858895705,"order. Therefore, for fair comparison with the Gumbel-Sinkhorn network in the Jigsaw Puzzle
540"
REFERENCES,0.5836985100788782,"experiment, we modify our loss function so that it does not use the ground truth permutation. Recall
541"
REFERENCES,0.5845749342681859,"from Section 3.2 that we defined
542"
REFERENCES,0.5854513584574934,"pθ(Xt−1|Xt) =
X"
REFERENCES,0.5863277826468011,"σ′
t∈T
p(Xt−1|Xt, σ′
t)pθ(σ′
t|Xt).
(17)"
REFERENCES,0.5872042068361086,"In our original setup, we defined p(Xt−1|Xt, σ′
t) as a delta distribution δ(Xt−1 = Qσ′
tXt), but this
543"
REFERENCES,0.5880806310254163,"would require that we know the permutation that turns Xt−1 to Xt, which is part of the ground truth.
544"
REFERENCES,0.588957055214724,"So instead, we parameterize p(Xt−1|Xt, σ′
t) as a Gaussian distribution N
 
Xt−1|QσtXt, I

. At the
545"
REFERENCES,0.5898334794040315,"same time, we note that to find the gradient of (12), it suffices to find the gradient of the log of (17).
546"
REFERENCES,0.5907099035933392,1https://github.com/chaitjo/learning-tsp?tab=readme-ov-file
REFERENCES,0.5915863277826467,Sequence Length
REFERENCES,0.5924627519719544,"9
32
52"
REFERENCES,0.5933391761612621,RS (forward) + GPL (reverse) + greedy
REFERENCES,0.5942156003505696,"Denoising Schedule
[0, 3, 5, 9]
[0, 5, 7, 12]
[0, 5, 6, 7, 10, 13]
Kendall-Tau ↑
0.948
0.857
0.779
Accuracy (%)
89.4
54.8
24.4
Correct (%)
95.9
88.1
81.6"
REFERENCES,0.5950920245398773,RS (forward) + PL (reverse) + greedy
REFERENCES,0.595968448729185,"Denoising Schedule
[0, 3, 5, 9]
[0, 5, 7, 12]
[0, 5, 6, 7, 10, 13]
Kendall-Tau
0.953
0.867
0.799
Accuracy (%)
90.9
56.4
26.4
Correct (%)
96.4
89.0
83.3"
REFERENCES,0.5968448729184925,RS (forward) + PL (reverse) + beam search
REFERENCES,0.5977212971078002,"Denoising Schedule
[0, 3, 5, 9]
[0, 5, 7, 12]
[0, 5, 6, 7, 10, 13]
Kendall-Tau ↑
0.955
0.869
0.797
Accuracy (%)
91.1
57.2
26.4
Correct (%)
96.5
89.2
83.1"
REFERENCES,0.5985977212971078,RS (forward) + IRS (reverse) + greedy
REFERENCES,0.5994741454864154,"T
9
12
13
Kendall-Tau ↑
0.947
0.794
0.390
Accuracy (%)
88.6
24.4
0.6
Correct (%)
95.9
82.5
44.6"
REFERENCES,0.6003505696757231,RT (forward) + IT (reverse) + greedy
REFERENCES,0.6012269938650306,T (using approx. n
REFERENCES,0.6021034180543383,"2 log n)
15
55
105
Kendall-Tau ↑
0.490
Out of Memory
Accuracy (%)
18.0
Correct (%)
59.5"
REFERENCES,0.6029798422436459,RI (forward) + II (reverse) + greedy
REFERENCES,0.6038562664329535,"T (using approx. n log n)
25
110
205
Kendall-Tau ↑
0.954
Out of Memory
Accuracy (%)
91.1
Correct (%)
96.4"
REFERENCES,0.6047326906222612,"Table 5: More results on sorting the 4-digit MNIST dataset using different combinations of forward
process methods and reverse process methods. Results averaged over 3 runs with different seeds. RS:
riffle shuffle; GPL: generalized Plackett-Luce; IRS: inverse riffle shuffle; RT: random transposition;
IT: inverse transposition; RI: random insertion; II: inverse insertion."
REFERENCES,0.6056091148115688,"Denoising Schedule
[0, 15]
[0, 8, 9, 15]
[0, 7, 8, 9, 15]
[0, 7, 8, 10, 15]
[0, 8, 10, 15]"
REFERENCES,0.6064855390008764,"Kendall-Tau ↑
0.000
0.316
0.000
0.000
0.646
Accuracy (%)
0.0
0.0
0.0
0.0
4.5
Correct (%)
1.0
39.6
1.0
1.0
69.8"
REFERENCES,0.6073619631901841,"Table 6: Results of sorting 100 4-digit MNIST images using various denoising schedules with the
combination of RS, GPL and beam search consistently applied."
REFERENCES,0.6082383873794917,"We use the REINFORCE trick [46] to find the gradient of log pθ(Xt−1|Xt), which gives us
547"
REFERENCES,0.6091148115687993,"∇θ log pθ(Xt−1|Xt) =
1
P"
REFERENCES,0.6099912357581069,"σ′
t∈T
p(Xt−1|Xt, σ′
t)pθ(σ′
t|Xt) ·
X"
REFERENCES,0.6108676599474145,"σ′
t∈T
p(Xt−1|Xt, σ′
t)∇θpθ(σ′
t|Xt) =
1
P"
REFERENCES,0.6117440841367222,"σ′
t∈T
p(Xt−1|Xt, σ′
t)pθ(σ′
t|Xt) ·
X"
REFERENCES,0.6126205083260298,"σ′
t∈T
p(Xt−1|Xt, σ′
t)pθ(σ′
t|Xt)
 
∇θ log pθ(σt|Xt)
"
REFERENCES,0.6134969325153374,"=
Epθ(σt|Xt)
h
p(Xt−1|Xt, σ′
t)∇θ log pθ(σt|Xt)
i"
REFERENCES,0.6143733567046451,"Epθ(σt|Xt)
h
p(Xt−1|Xt, σ′
t)
i ≈ N
X n=1"
REFERENCES,0.6152497808939527,"p

Xt−1|Xt, σ(n)
t
"
REFERENCES,0.6161262050832603,"PN
m=1 p

Xt−1|Xt, σ(m)
t
 · ∇θ log pθ

σ(n)
t
|Xt

,"
REFERENCES,0.6170026292725679,"where we have used Monte-Carlo estimation in the last step, and σ(1)
t
, . . . , σ(N)
t
∼pθ(σt|Xt). We
548"
REFERENCES,0.6178790534618755,"further add an entropy regularization term −λ·Epθ(σt|Xt) [log pθ(σt|Xt)] to each of log pθ(Xt−1|Xt).
549"
REFERENCES,0.6187554776511832,"Using the same REINFORCE and Monte-Carlo trick, we obtain
550"
REFERENCES,0.6196319018404908,"∇θ

−λ · Epθ(σt|Xt)
h
log pθ(σt|Xt)
i
≈ N
X"
REFERENCES,0.6205083260297984,"n=1
−λ log pθ

σ(n)
t
|Xt

∇θ log pθ

σ(n)
t
|Xt

,"
REFERENCES,0.621384750219106,"where σ(1)
t
, . . . , σ(N)
t
∼pθ(σt|Xt). Therefore, we have
551"
REFERENCES,0.6222611744084137,"∇θ

log pθ(Xt−1|Xt) −λ · Epθ(σt|Xt)
h
log pθ(σt|Xt)
i ≈ N
X n=1 "
REFERENCES,0.6231375985977213,"




"
REFERENCES,0.6240140227870289,"p

Xt−1|Xt, σ(n)
t
"
REFERENCES,0.6248904469763366,"PN
m=1 p

Xt−1|Xt, σ(m)
t
 −λ log pθ

σ(n)
t
|Xt
"
REFERENCES,0.6257668711656442,"|
{z
}
weight "
REFERENCES,0.6266432953549518,"





· ∇θ log pθ

σ(n)
t
|Xt

,
(18)"
REFERENCES,0.6275197195442594,"where σ(1)
t
, . . . , σ(N)
t
∼pθ(σt|Xt). We then substitute in
552"
REFERENCES,0.628396143733567,"p

Xt−1|Xt, σ(n)
t

= N

Xt−1|Qσ(n)
t
Xt, I
"
REFERENCES,0.6292725679228747,"for all n ∈[N]. Finally, we also subtract the exponential moving average weight as a control variate
553"
REFERENCES,0.6301489921121823,"for variance reduction, where the exponential moving average is given by ema ←ema_rate · ema +
554"
REFERENCES,0.6310254163014899,"(1 −ema_rate) · weight for each gradient descent step.
555"
REFERENCES,0.6319018404907976,"E.4
Training Details and Architecture Hyperparameters
556"
REFERENCES,0.6327782646801051,"Hardware.
The Jigsaw Puzzle and Sort 4-Digit MNIST Numbers experiments are trained and
557"
REFERENCES,0.6336546888694128,"evaluated on the NVIDIA A40 GPU. The TSP experiments are trained and evaluated on the NVIDIA
558"
REFERENCES,0.6345311130587205,"A40 and A100 GPU.
559"
REFERENCES,0.635407537248028,"Jigsaw Puzzle.
For the Jigsaw Puzzle experiments, we use the AdamW optimizer [26] with weight
560"
REFERENCES,0.6362839614373357,"decay 1e-2, ε = 1e-9, and β = (0.9, 0.98). We use the Noam learning rate scheduler given in [44]
561"
REFERENCES,0.6371603856266433,"with 51,600 warmup steps for Noisy MNIST and 46,000 steps for CIFAR-10. We train for 120
562"
REFERENCES,0.6380368098159509,"epochs with a batch size of 64. When computing the loss (12), we use Monte-Carlo estimation for the
563"
REFERENCES,0.6389132340052586,"expectation and sample 3 trajectories. For REINFORCE, we sampled 10 times for the Monte-Carlo
564"
REFERENCES,0.6397896581945661,"estimation in (18), and we used an entropy regularization rate λ = 0.05 and an ema_rate of 0.995.
565"
REFERENCES,0.6406660823838738,"The neural network architecture and related hyperparameters are given in Table 7. The denoising
566"
REFERENCES,0.6415425065731815,"schedules, with riffle shuffles as the forward process and GPL as the reverse process, are give in Table
567"
REFERENCES,0.642418930762489,"8. For beam search, we use a beam size of 200 when decoding from GPL, and we use a beam size of
568"
REFERENCES,0.6432953549517967,"20 when decoding along the diffusion denoising schedule.
569"
REFERENCES,0.6441717791411042,"Layer
Details"
REFERENCES,0.6450482033304119,"Convolution
Output channels 32, kernel size 3,
padding 1, stride 1
Batch Normalization
−
ReLU
−
Max-pooling
Pooling 2
Fully-connected
Output dimension (dim_after_conv + 128)/2
ReLU
−
Fully-connected
Output dimension 128"
REFERENCES,0.6459246275197196,"Transformer encoder
7 layers, 8 heads, model dimension (dmodel) 128,
feed-forward dimension 512, dropout 0.1"
REFERENCES,0.6468010517090271,Table 7: Jigsaw puzzle neural network architecture and hyperparameters.
REFERENCES,0.6476774758983348,"Number of patches per side
Denoising schedule"
REFERENCES,0.6485539000876425,"2 × 2
[0, 2, 7]
3 × 3
[0, 3, 5, 9]
4 × 4
[0, 4, 6, 10]
5 × 5
[0, 5, 7, 11]
6 × 6
[0, 6, 8, 12]"
REFERENCES,0.64943032427695,"Table 8: Denoising schedules for the Jigsaw Puzzle task, where we use riffle shuffle in the forward
process and GPL in the revserse process."
REFERENCES,0.6503067484662577,"Sort 4-Digit MNIST Numbers.
For the task of sorting 4-digit MNIST numbers, we use the exact
570"
REFERENCES,0.6511831726555652,"training and beam search setup as the Jigsaw Puzzle, except that we do not need to use REINFORCE.
571"
REFERENCES,0.6520595968448729,"The neural network architecture is given in Table 9. The denoising schedules, with riffle shuffles as
572"
REFERENCES,0.6529360210341806,"the forward process and GPL as the reverse process, are give in Table 10.
573"
REFERENCES,0.6538124452234881,"Layer
Details"
REFERENCES,0.6546888694127958,"Convolution
Output channels 32, kernel size 5,
padding 2, stride 1
Batch Normalization
−
ReLU
−
Max-pooling
Pooling 2"
REFERENCES,0.6555652936021035,"Convolution
Output channels 64, kernel size 5,
padding 2, stride 1
Batch Normalization
−
ReLU
−
Max-pooling
Pooling 2
Fully-connected
Output dimension (dim_after_conv + 128)/2
ReLU
−
Fully-connected
Output dimension 128"
REFERENCES,0.656441717791411,"Transformer encoder
7 layers, 8 heads, model dimension (dmodel) 128,
feed-forward dimension 512, dropout 0.1"
REFERENCES,0.6573181419807187,Table 9: Sort 4-digit MNIST numbers neural network architecture and hyperparameters.
REFERENCES,0.6581945661700263,"Sequence Length n
Denoising schedule"
REFERENCES,0.6590709903593339,"3
[0, 2, 7]
5
[0, 2, 8]
7
[0, 3, 8]
9
[0, 3, 5, 9]
15
[0, 4, 7, 10]
32
[0, 5, 7, 12]
52
[0, 5, 6, 7, 10, 13]
100
[0, 8, 10, 15]"
REFERENCES,0.6599474145486416,"Table 10: Denoising schedules for the Sort 4-Digit MNIST Numbers task, where we use riffle shuffle
in the forward process and GPL in the revserse process."
REFERENCES,0.6608238387379491,"TSP.
For solving the TSP, we perform supervised learning to train our SymmetricDiffusers to solve
574"
REFERENCES,0.6617002629272568,"the TSP. Let σ∗be an optimal permutation, and let X0 be the list of nodes ordered by σ∗. We note
575"
REFERENCES,0.6625766871165644,"that any cyclic shift of X0 is also optimal. Thus, for simplicity and without loss of generality, we
576"
REFERENCES,0.663453111305872,"always assume σ∗(1) = 1. In the forward process of SymmetricDiffusers, we only shuffle the second
577"
REFERENCES,0.6643295354951797,"to the nth node (or component). In the reverse process, we mask certain parameters of the reverse
578"
REFERENCES,0.6652059596844873,"distribution so that we will always sample a permutation with σt(1) = 1.
579"
REFERENCES,0.6660823838737949,"The architecture details are slightly different for TSP, since we need to input both node and edge
580"
REFERENCES,0.6669588080631026,"features into our network. Denote by Xt the ordered list of nodes at time t. We obtain Yt ∈Rn×dmodel
581"
REFERENCES,0.6678352322524101,"as in Eq. (15), where encoderθ is now a sinusoidal embedding of the 2D coordinates. Let Dt ∈Rn×n
582"
REFERENCES,0.6687116564417178,"be the matrix representing the pairwise distances of points in Xt, respecting the order in Xt. Let
583"
REFERENCES,0.6695880806310254,"Et ∈R(
n
2) be the flattened vector of the upper triangular part of Dt. We also apply sinusoidal
584"
REFERENCES,0.670464504820333,"embedding to Et and add time_embd(t) to it. We call the result Ft ∈R(
n
2)×dmodel.
585"
REFERENCES,0.6713409290096407,"Now, instead of applying the usual transformer encoder with self-attentions, we alternate between
586"
REFERENCES,0.6722173531989483,"cross-attentions and self-attentions. For cross-attention layers, we use the node representations from
587"
REFERENCES,0.6730937773882559,"the previous layer as the query, and we always use K = V = Ft. We also apply an attention mask
588"
REFERENCES,0.6739702015775635,"to the cross-attention, so that each node will only attend to edges that it is incident with. For self-
589"
REFERENCES,0.6748466257668712,"attention layers, we always use the node representations from the previous layer as input. We always
590"
REFERENCES,0.6757230499561788,"use an even number of layers, with the first layer being a cross-attention layer, and the last layer
591"
REFERENCES,0.6765994741454864,"being a self-attention layer structured to produce the required parameters for the reverse distribution
592"
REFERENCES,0.677475898334794,"as illustrated in Appendix B. For hyperparameters, we use 16 alternating layers, 8 attention heads,
593"
REFERENCES,0.6783523225241017,"dmodel = 256, feed-forward hidden dimension 1024, and dropout rate 0.1.
594"
REFERENCES,0.6792287467134093,"For training details on the TSP-20 task, we use the AdamW optimizer [26] with weight decay 1e-4,
595"
REFERENCES,0.6801051709027169,"ε = 1e-8, and β = (0.9, 0.999). We use the cosine annealing learning rate scheduler starting from
596"
REFERENCES,0.6809815950920245,"2e-4 and ending at 0. We train for 50 epochs with a batch size of 512. When computing the loss
597"
REFERENCES,0.6818580192813322,"(12), we use Monte-Carlo estimation for the expectation and sample 1 trajectory. We use a denoising
598"
REFERENCES,0.6827344434706398,"schedule of [0, 4, 5, 7], with riffle shuffles as the forward process and GPL as the reverse process.
599"
REFERENCES,0.6836108676599474,"Finally, we use beam search for decoding, and we use a beam size of 256 both when decoding from
600"
REFERENCES,0.684487291849255,"GPL and decoding along the denoising schedule.
601"
REFERENCES,0.6853637160385626,"E.5
Baselines Implementation Details
602"
REFERENCES,0.6862401402278703,"Gumbel-Sinkhorn Network.
We have re-implemented the Gumbel-Sinkhorn Network [29] for
603"
REFERENCES,0.6871165644171779,"application on jigsaw puzzles, following the implementations provided in the official repository2. To
604"
REFERENCES,0.6879929886064855,"ensure a fair comparison, we conducted a thorough grid search of the model’s hyper-parameters. The
605"
REFERENCES,0.6888694127957932,"parameters included in our search space are as follows,
606"
REFERENCES,0.6897458369851008,"Hyperparameter
Values"
REFERENCES,0.6906222611744084,"Learning Rate (lr)
{10−3, 10−4, 10−5}
Batch Size
{50}
Hidden Channels
{64, 128}
Kernel Size
{3, 5}
τ
{0.2, 0.5, 1, 2, 5}
Number of Sinkhorn Iterations (n_sink_iter)
{20}
Number of Samples
{10}"
REFERENCES,0.6914986853637161,Table 11: Hyperparameter Search Space for the Gumbel-Sinkhorn Network
REFERENCES,0.6923751095530236,"Diffsort & Error-free Diffsort
We have implemented two differentiable sorting networks from
607"
REFERENCES,0.6932515337423313,"the official repository3 specific to error-free diffsort. For sorting 4-digit MNIST images, error-free
608"
REFERENCES,0.694127957931639,"diffsort employs TransformerL as its backbone, with detailed hyperparameters listed in Table 12.
609"
REFERENCES,0.6950043821209465,"Conversely, Diffsort uses a CNN as its backbone, with a learning rate set to 10−3.5; the relevant
610"
REFERENCES,0.6958808063102542,"hyperparameters are outlined in Table 13.
611"
REFERENCES,0.6967572304995618,"For jigsaw puzzle tasks, error-free diffsort continues to utilize a transformer, whereas Diffsort employs
612"
REFERENCES,0.6976336546888694,"a CNN. For other configurations, we align the settings with those of tasks having similar sequence
613"
REFERENCES,0.6985100788781771,"lengths in the 4-digit MNIST sorting task. For instance, for 3 × 3 puzzles, we apply the same
614"
REFERENCES,0.6993865030674846,"configuration as used for sorting tasks with a sequence length of 9.
615"
REFERENCES,0.7002629272567923,"TSP.
For the baselines for TSP, we first have 4 traditional operations research solvers. Gurobi [14]
616"
REFERENCES,0.7011393514461,"and Concorde [1] are known as exact solvers, while LKH-3 [15] is a strong heuristic and 2-Opt [25]
617"
REFERENCES,0.7020157756354075,"2https://github.com/google/gumbel_sinkhorn
3https://github.com/jungtaekkim/error-free-differentiable-swap-functions"
REFERENCES,0.7028921998247152,"Sequence Length
Steepness
Sorting Network
Loss Weight
Learning Rate"
REFERENCES,0.7037686240140227,"3
10
odd even
1.00
10−4"
REFERENCES,0.7046450482033304,"5
26
odd even
1.00
10−4"
REFERENCES,0.7055214723926381,"7
31
odd even
1.00
10−4"
REFERENCES,0.7063978965819456,"9
34
odd even
1.00
10−4"
REFERENCES,0.7072743207712533,"15
25
odd even
0.10
10−4"
REFERENCES,0.708150744960561,"32
124
odd even
0.10
10−4"
REFERENCES,0.7090271691498685,"52
130
bitonic
0.10
10−3.5"
REFERENCES,0.7099035933391762,"100
140
bitonic
0.10
10−3.5"
REFERENCES,0.7107800175284837,Table 12: Hyperparameters for Error-Free Diffsort on Sorting 4-Digit MNIST Numbers
REFERENCES,0.7116564417177914,"Sequence Length
Steepness
Sorting Network"
REFERENCES,0.7125328659070991,"3
6
odd even
5
20
odd even
7
29
odd even
9
32
odd even
15
25
odd even
32
25
bitonic
52
25
bitonic
100
25
bitonic"
REFERENCES,0.7134092900964066,Table 13: Hyperparameters for Diffsort on Sorting 4-Digit MNIST Numbers
REFERENCES,0.7142857142857143,"is a weak heuristic. For LKH-3, we used 500 trials, and for 2-Opt, we used 5 random initial guesses
618"
REFERENCES,0.7151621384750219,"with seed 42.
619"
REFERENCES,0.7160385626643295,"For the GCN model[18], we utilized the official repository4 and adhered closely to its default
620"
REFERENCES,0.7169149868536372,"configuration for the TSP-20 dataset. For DIFUSCO[43], we sourced it from its official repository5
621"
REFERENCES,0.7177914110429447,"and followed the recommended configuration of TSP-50 dataset, with a minor adjustment in the batch
622"
REFERENCES,0.7186678352322524,"size. We increased the batch size to 512 to accelerate the training process. For fair comparison, we
623"
REFERENCES,0.7195442594215601,"also remove the post-processing heuristics in both models during the evaluation.
624"
REFERENCES,0.7204206836108676,"F
Limitations
625"
REFERENCES,0.7212971078001753,"Despite the success of this method on various tasks, the model presented in this paper still requires a
626"
REFERENCES,0.7221735319894829,"time-space complexity of O(n2) due to its reliance on the parametric representation of GPL and the
627"
REFERENCES,0.7230499561787905,"backbone of transformer attention layers. This complexity poses a significant challenge in scaling up
628"
REFERENCES,0.7239263803680982,"to applications involving larger symmetric groups or Lie groups.
629"
REFERENCES,0.7248028045574058,"4https://github.com/chaitjo/graph-convnet-tsp
5https://github.com/Edward-Sun/DIFUSCO"
REFERENCES,0.7256792287467134,"NeurIPS Paper Checklist
630"
CLAIMS,0.726555652936021,"1. Claims
631"
CLAIMS,0.7274320771253286,"Question: Do the main claims made in the abstract and introduction accurately reflect the
632"
CLAIMS,0.7283085013146363,"paper’s contributions and scope?
633"
CLAIMS,0.7291849255039439,"Answer: [Yes]
634"
CLAIMS,0.7300613496932515,"Justification: Our abstract and Section 1 accurately summarize the paper’s contributions and
635"
CLAIMS,0.7309377738825592,"scope.
636"
CLAIMS,0.7318141980718668,"Guidelines:
637"
CLAIMS,0.7326906222611744,"• The answer NA means that the abstract and introduction do not include the claims
638"
CLAIMS,0.733567046450482,"made in the paper.
639"
CLAIMS,0.7344434706397897,"• The abstract and/or introduction should clearly state the claims made, including the
640"
CLAIMS,0.7353198948290973,"contributions made in the paper and important assumptions and limitations. A No or
641"
CLAIMS,0.7361963190184049,"NA answer to this question will not be perceived well by the reviewers.
642"
CLAIMS,0.7370727432077125,"• The claims made should match theoretical and experimental results, and reflect how
643"
CLAIMS,0.7379491673970202,"much the results can be expected to generalize to other settings.
644"
CLAIMS,0.7388255915863278,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
645"
CLAIMS,0.7397020157756354,"are not attained by the paper.
646"
LIMITATIONS,0.740578439964943,"2. Limitations
647"
LIMITATIONS,0.7414548641542507,"Question: Does the paper discuss the limitations of the work performed by the authors?
648"
LIMITATIONS,0.7423312883435583,"Answer: [Yes]
649"
LIMITATIONS,0.7432077125328659,"Justification: We discuss the limitations of the work in Appendix F.
650"
LIMITATIONS,0.7440841367221736,"Guidelines:
651"
LIMITATIONS,0.7449605609114811,"• The answer NA means that the paper has no limitation while the answer No means that
652"
LIMITATIONS,0.7458369851007888,"the paper has limitations, but those are not discussed in the paper.
653"
LIMITATIONS,0.7467134092900964,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
654"
LIMITATIONS,0.747589833479404,"• The paper should point out any strong assumptions and how robust the results are to
655"
LIMITATIONS,0.7484662576687117,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
656"
LIMITATIONS,0.7493426818580193,"model well-specification, asymptotic approximations only holding locally). The authors
657"
LIMITATIONS,0.7502191060473269,"should reflect on how these assumptions might be violated in practice and what the
658"
LIMITATIONS,0.7510955302366346,"implications would be.
659"
LIMITATIONS,0.7519719544259421,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
660"
LIMITATIONS,0.7528483786152498,"only tested on a few datasets or with a few runs. In general, empirical results often
661"
LIMITATIONS,0.7537248028045574,"depend on implicit assumptions, which should be articulated.
662"
LIMITATIONS,0.754601226993865,"• The authors should reflect on the factors that influence the performance of the approach.
663"
LIMITATIONS,0.7554776511831727,"For example, a facial recognition algorithm may perform poorly when image resolution
664"
LIMITATIONS,0.7563540753724802,"is low or images are taken in low lighting. Or a speech-to-text system might not be
665"
LIMITATIONS,0.7572304995617879,"used reliably to provide closed captions for online lectures because it fails to handle
666"
LIMITATIONS,0.7581069237510956,"technical jargon.
667"
LIMITATIONS,0.7589833479404031,"• The authors should discuss the computational efficiency of the proposed algorithms
668"
LIMITATIONS,0.7598597721297108,"and how they scale with dataset size.
669"
LIMITATIONS,0.7607361963190185,"• If applicable, the authors should discuss possible limitations of their approach to
670"
LIMITATIONS,0.761612620508326,"address problems of privacy and fairness.
671"
LIMITATIONS,0.7624890446976337,"• While the authors might fear that complete honesty about limitations might be used by
672"
LIMITATIONS,0.7633654688869412,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
673"
LIMITATIONS,0.7642418930762489,"limitations that aren’t acknowledged in the paper. The authors should use their best
674"
LIMITATIONS,0.7651183172655566,"judgment and recognize that individual actions in favor of transparency play an impor-
675"
LIMITATIONS,0.7659947414548641,"tant role in developing norms that preserve the integrity of the community. Reviewers
676"
LIMITATIONS,0.7668711656441718,"will be specifically instructed to not penalize honesty concerning limitations.
677"
THEORY ASSUMPTIONS AND PROOFS,0.7677475898334793,"3. Theory Assumptions and Proofs
678"
THEORY ASSUMPTIONS AND PROOFS,0.768624014022787,"Question: For each theoretical result, does the paper provide the full set of assumptions and
679"
THEORY ASSUMPTIONS AND PROOFS,0.7695004382120947,"a complete (and correct) proof?
680"
THEORY ASSUMPTIONS AND PROOFS,0.7703768624014022,"Answer: [Yes]
681"
THEORY ASSUMPTIONS AND PROOFS,0.7712532865907099,"Justification: We provide complete proof for Proposition 1 and Proposition 2 in Appendix
682"
THEORY ASSUMPTIONS AND PROOFS,0.7721297107800176,"D.
683"
THEORY ASSUMPTIONS AND PROOFS,0.7730061349693251,"Guidelines:
684"
THEORY ASSUMPTIONS AND PROOFS,0.7738825591586328,"• The answer NA means that the paper does not include theoretical results.
685"
THEORY ASSUMPTIONS AND PROOFS,0.7747589833479404,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
686"
THEORY ASSUMPTIONS AND PROOFS,0.775635407537248,"referenced.
687"
THEORY ASSUMPTIONS AND PROOFS,0.7765118317265557,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
688"
THEORY ASSUMPTIONS AND PROOFS,0.7773882559158632,"• The proofs can either appear in the main paper or the supplemental material, but if
689"
THEORY ASSUMPTIONS AND PROOFS,0.7782646801051709,"they appear in the supplemental material, the authors are encouraged to provide a short
690"
THEORY ASSUMPTIONS AND PROOFS,0.7791411042944786,"proof sketch to provide intuition.
691"
THEORY ASSUMPTIONS AND PROOFS,0.7800175284837861,"• Inversely, any informal proof provided in the core of the paper should be complemented
692"
THEORY ASSUMPTIONS AND PROOFS,0.7808939526730938,"by formal proofs provided in appendix or supplemental material.
693"
THEORY ASSUMPTIONS AND PROOFS,0.7817703768624014,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782646801051709,"4. Experimental Result Reproducibility
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7835232252410167,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7843996494303243,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852760736196319,"of the paper (regardless of whether the code and data are provided or not)?
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861524978089395,"Answer: [Yes]
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870289219982471,"Justification: In Appendix E, we fully disclose all information to reproduce our experimental
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7879053461875548,"results, including dataset preparation, training details, and choices of hyperparameters as
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7887817703768624,"well as baselines’ implementation details.
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78965819456617,"Guidelines:
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7905346187554777,"• The answer NA means that the paper does not include experiments.
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7914110429447853,"• If the paper includes experiments, a No answer to this question will not be perceived
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7922874671340929,"well by the reviewers: Making the paper reproducible is important, regardless of
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7931638913234005,"whether the code and data are provided or not.
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7940403155127082,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949167397020158,"to make their results reproducible or verifiable.
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957931638913234,"• Depending on the contribution, reproducibility can be accomplished in various ways.
710"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.796669588080631,"For example, if the contribution is a novel architecture, describing the architecture fully
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7975460122699386,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7984224364592463,"be necessary to either make it possible for others to replicate the model with the same
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7992988606485539,"dataset, or provide access to the model. In general. releasing code and data is often
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8001752848378615,"one good way to accomplish this, but reproducibility can also be provided via detailed
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8010517090271692,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8019281332164768,"of a large language model), releasing of a model checkpoint, or other means that are
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8028045574057844,"appropriate to the research performed.
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803680981595092,"• While NeurIPS does not require releasing code, the conference does require all submis-
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8045574057843996,"sions to provide some reasonable avenue for reproducibility, which may depend on the
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8054338299737073,"nature of the contribution. For example
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8063102541630149,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8071866783523225,"to reproduce that algorithm.
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8080631025416302,"(b) If the contribution is primarily a new model architecture, the paper should describe
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089395267309377,"the architecture clearly and fully.
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8098159509202454,"(c) If the contribution is a new model (e.g., a large language model), then there should
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.810692375109553,"either be a way to access this model for reproducing the results or a way to reproduce
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8115687992988606,"the model (e.g., with an open-source dataset or instructions for how to construct
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8124452234881683,"the dataset).
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.813321647677476,"(d) We recognize that reproducibility may be tricky in some cases, in which case
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8141980718667835,"authors are welcome to describe the particular way they provide for reproducibility.
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8150744960560912,"In the case of closed-source models, it may be that access to the model is limited in
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8159509202453987,"some way (e.g., to registered users), but it should be possible for other researchers
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8168273444347064,"to have some path to reproducing or verifying the results.
734"
OPEN ACCESS TO DATA AND CODE,0.8177037686240141,"5. Open access to data and code
735"
OPEN ACCESS TO DATA AND CODE,0.8185801928133216,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
736"
OPEN ACCESS TO DATA AND CODE,0.8194566170026293,"tions to faithfully reproduce the main experimental results, as described in supplemental
737"
OPEN ACCESS TO DATA AND CODE,0.820333041191937,"material?
738"
OPEN ACCESS TO DATA AND CODE,0.8212094653812445,"Answer: [Yes]
739"
OPEN ACCESS TO DATA AND CODE,0.8220858895705522,"Justification: We’ve included codes to reproduce the main results in the supplemental
740"
OPEN ACCESS TO DATA AND CODE,0.8229623137598597,"material. We also attach a detailed README file that provides sufficient instructions.
741"
OPEN ACCESS TO DATA AND CODE,0.8238387379491674,"Guidelines:
742"
OPEN ACCESS TO DATA AND CODE,0.8247151621384751,"• The answer NA means that paper does not include experiments requiring code.
743"
OPEN ACCESS TO DATA AND CODE,0.8255915863277826,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
744"
OPEN ACCESS TO DATA AND CODE,0.8264680105170903,"public/guides/CodeSubmissionPolicy) for more details.
745"
OPEN ACCESS TO DATA AND CODE,0.8273444347063978,"• While we encourage the release of code and data, we understand that this might not be
746"
OPEN ACCESS TO DATA AND CODE,0.8282208588957055,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
747"
OPEN ACCESS TO DATA AND CODE,0.8290972830850132,"including code, unless this is central to the contribution (e.g., for a new open-source
748"
OPEN ACCESS TO DATA AND CODE,0.8299737072743207,"benchmark).
749"
OPEN ACCESS TO DATA AND CODE,0.8308501314636284,"• The instructions should contain the exact command and environment needed to run to
750"
OPEN ACCESS TO DATA AND CODE,0.8317265556529361,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
751"
OPEN ACCESS TO DATA AND CODE,0.8326029798422436,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
752"
OPEN ACCESS TO DATA AND CODE,0.8334794040315513,"• The authors should provide instructions on data access and preparation, including how
753"
OPEN ACCESS TO DATA AND CODE,0.8343558282208589,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
754"
OPEN ACCESS TO DATA AND CODE,0.8352322524101665,"• The authors should provide scripts to reproduce all experimental results for the new
755"
OPEN ACCESS TO DATA AND CODE,0.8361086765994742,"proposed method and baselines. If only a subset of experiments are reproducible, they
756"
OPEN ACCESS TO DATA AND CODE,0.8369851007887817,"should state which ones are omitted from the script and why.
757"
OPEN ACCESS TO DATA AND CODE,0.8378615249780894,"• At submission time, to preserve anonymity, the authors should release anonymized
758"
OPEN ACCESS TO DATA AND CODE,0.838737949167397,"versions (if applicable).
759"
OPEN ACCESS TO DATA AND CODE,0.8396143733567046,"• Providing as much information as possible in supplemental material (appended to the
760"
OPEN ACCESS TO DATA AND CODE,0.8404907975460123,"paper) is recommended, but including URLs to data and code is permitted.
761"
OPEN ACCESS TO DATA AND CODE,0.8413672217353199,"6. Experimental Setting/Details
762"
OPEN ACCESS TO DATA AND CODE,0.8422436459246275,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
763"
OPEN ACCESS TO DATA AND CODE,0.8431200701139352,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
764"
OPEN ACCESS TO DATA AND CODE,0.8439964943032428,"results?
765"
OPEN ACCESS TO DATA AND CODE,0.8448729184925504,"Answer: [Yes]
766"
OPEN ACCESS TO DATA AND CODE,0.845749342681858,"Justification: All experimental settings and details are specified in Appendix E.4.
767"
OPEN ACCESS TO DATA AND CODE,0.8466257668711656,"Guidelines:
768"
OPEN ACCESS TO DATA AND CODE,0.8475021910604733,"• The answer NA means that the paper does not include experiments.
769"
OPEN ACCESS TO DATA AND CODE,0.8483786152497809,"• The experimental setting should be presented in the core of the paper to a level of detail
770"
OPEN ACCESS TO DATA AND CODE,0.8492550394390885,"that is necessary to appreciate the results and make sense of them.
771"
OPEN ACCESS TO DATA AND CODE,0.8501314636283961,"• The full details can be provided either with the code, in appendix, or as supplemental
772"
OPEN ACCESS TO DATA AND CODE,0.8510078878177038,"material.
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8518843120070114,"7. Experiment Statistical Significance
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.852760736196319,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8536371603856266,"information about the statistical significance of the experiments?
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8545135845749343,"Answer: [Yes]
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8553900087642419,"Justification: All reported experimental results are averaged over at least three runs with
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8562664329535495,"different random seeds.
779"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"Guidelines:
780"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8580192813321648,"• The answer NA means that the paper does not include experiments.
781"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588957055214724,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
782"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.85977212971078,"dence intervals, or statistical significance tests, at least for the experiments that support
783"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8606485539000877,"the main claims of the paper.
784"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8615249780893953,"• The factors of variability that the error bars are capturing should be clearly stated (for
785"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8624014022787029,"example, train/test split, initialization, random drawing of some parameter, or overall
786"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8632778264680105,"run with given experimental conditions).
787"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8641542506573181,"• The method for calculating the error bars should be explained (closed form formula,
788"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8650306748466258,"call to a library function, bootstrap, etc.)
789"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8659070990359334,"• The assumptions made should be given (e.g., Normally distributed errors).
790"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.866783523225241,"• It should be clear whether the error bar is the standard deviation or the standard error
791"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8676599474145487,"of the mean.
792"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8685363716038562,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
793"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8694127957931639,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
794"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8702892199824716,"of Normality of errors is not verified.
795"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8711656441717791,"• For asymmetric distributions, the authors should be careful not to show in tables or
796"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8720420683610868,"figures symmetric error bars that would yield results that are out of range (e.g. negative
797"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8729184925503944,"error rates).
798"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.873794916739702,"• If error bars are reported in tables or plots, The authors should explain in the text how
799"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8746713409290097,"they were calculated and reference the corresponding figures or tables in the text.
800"
EXPERIMENTS COMPUTE RESOURCES,0.8755477651183172,"8. Experiments Compute Resources
801"
EXPERIMENTS COMPUTE RESOURCES,0.8764241893076249,"Question: For each experiment, does the paper provide sufficient information on the com-
802"
EXPERIMENTS COMPUTE RESOURCES,0.8773006134969326,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
803"
EXPERIMENTS COMPUTE RESOURCES,0.8781770376862401,"the experiments?
804"
EXPERIMENTS COMPUTE RESOURCES,0.8790534618755478,"Answer: [Yes]
805"
EXPERIMENTS COMPUTE RESOURCES,0.8799298860648553,"Justification: We provide information on the computation resources used for our experiments
806"
EXPERIMENTS COMPUTE RESOURCES,0.880806310254163,"in Appendix E.4.
807"
EXPERIMENTS COMPUTE RESOURCES,0.8816827344434707,"Guidelines:
808"
EXPERIMENTS COMPUTE RESOURCES,0.8825591586327782,"• The answer NA means that the paper does not include experiments.
809"
EXPERIMENTS COMPUTE RESOURCES,0.8834355828220859,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
810"
EXPERIMENTS COMPUTE RESOURCES,0.8843120070113936,"or cloud provider, including relevant memory and storage.
811"
EXPERIMENTS COMPUTE RESOURCES,0.8851884312007011,"• The paper should provide the amount of compute required for each of the individual
812"
EXPERIMENTS COMPUTE RESOURCES,0.8860648553900088,"experimental runs as well as estimate the total compute.
813"
EXPERIMENTS COMPUTE RESOURCES,0.8869412795793163,"• The paper should disclose whether the full research project required more compute
814"
EXPERIMENTS COMPUTE RESOURCES,0.887817703768624,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
815"
EXPERIMENTS COMPUTE RESOURCES,0.8886941279579317,"didn’t make it into the paper).
816"
CODE OF ETHICS,0.8895705521472392,"9. Code Of Ethics
817"
CODE OF ETHICS,0.8904469763365469,"Question: Does the research conducted in the paper conform, in every respect, with the
818"
CODE OF ETHICS,0.8913234005258545,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
819"
CODE OF ETHICS,0.8921998247151621,"Answer: [Yes]
820"
CODE OF ETHICS,0.8930762489044698,"Justification: We preserve anonymity with the NeurIPS Codes of Ethics.
821"
CODE OF ETHICS,0.8939526730937774,"Guidelines:
822"
CODE OF ETHICS,0.894829097283085,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
823"
CODE OF ETHICS,0.8957055214723927,"• If the authors answer No, they should explain the special circumstances that require a
824"
CODE OF ETHICS,0.8965819456617002,"deviation from the Code of Ethics.
825"
CODE OF ETHICS,0.8974583698510079,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
826"
CODE OF ETHICS,0.8983347940403155,"eration due to laws or regulations in their jurisdiction).
827"
BROADER IMPACTS,0.8992112182296231,"10. Broader Impacts
828"
BROADER IMPACTS,0.9000876424189308,"Question: Does the paper discuss both potential positive societal impacts and negative
829"
BROADER IMPACTS,0.9009640666082384,"societal impacts of the work performed?
830"
BROADER IMPACTS,0.901840490797546,"Answer: [NA]
831"
BROADER IMPACTS,0.9027169149868537,"Justification: There is no societal impact of the work performed.
832"
BROADER IMPACTS,0.9035933391761612,"Guidelines:
833"
BROADER IMPACTS,0.9044697633654689,"• The answer NA means that there is no societal impact of the work performed.
834"
BROADER IMPACTS,0.9053461875547765,"• If the authors answer NA or No, they should explain why their work has no societal
835"
BROADER IMPACTS,0.9062226117440841,"impact or why the paper does not address societal impact.
836"
BROADER IMPACTS,0.9070990359333918,"• Examples of negative societal impacts include potential malicious or unintended uses
837"
BROADER IMPACTS,0.9079754601226994,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
838"
BROADER IMPACTS,0.908851884312007,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
839"
BROADER IMPACTS,0.9097283085013146,"groups), privacy considerations, and security considerations.
840"
BROADER IMPACTS,0.9106047326906223,"• The conference expects that many papers will be foundational research and not tied
841"
BROADER IMPACTS,0.9114811568799299,"to particular applications, let alone deployments. However, if there is a direct path to
842"
BROADER IMPACTS,0.9123575810692375,"any negative applications, the authors should point it out. For example, it is legitimate
843"
BROADER IMPACTS,0.9132340052585451,"to point out that an improvement in the quality of generative models could be used to
844"
BROADER IMPACTS,0.9141104294478528,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
845"
BROADER IMPACTS,0.9149868536371604,"that a generic algorithm for optimizing neural networks could enable people to train
846"
BROADER IMPACTS,0.915863277826468,"models that generate Deepfakes faster.
847"
BROADER IMPACTS,0.9167397020157756,"• The authors should consider possible harms that could arise when the technology is
848"
BROADER IMPACTS,0.9176161262050833,"being used as intended and functioning correctly, harms that could arise when the
849"
BROADER IMPACTS,0.9184925503943909,"technology is being used as intended but gives incorrect results, and harms following
850"
BROADER IMPACTS,0.9193689745836985,"from (intentional or unintentional) misuse of the technology.
851"
BROADER IMPACTS,0.9202453987730062,"• If there are negative societal impacts, the authors could also discuss possible mitigation
852"
BROADER IMPACTS,0.9211218229623137,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
853"
BROADER IMPACTS,0.9219982471516214,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
854"
BROADER IMPACTS,0.922874671340929,"feedback over time, improving the efficiency and accessibility of ML).
855"
SAFEGUARDS,0.9237510955302366,"11. Safeguards
856"
SAFEGUARDS,0.9246275197195443,"Question: Does the paper describe safeguards that have been put in place for responsible
857"
SAFEGUARDS,0.9255039439088519,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
858"
SAFEGUARDS,0.9263803680981595,"image generators, or scraped datasets)?
859"
SAFEGUARDS,0.9272567922874672,"Answer: [NA]
860"
SAFEGUARDS,0.9281332164767747,"Justification: The paper poses no such risks.
861"
SAFEGUARDS,0.9290096406660824,"Guidelines:
862"
SAFEGUARDS,0.92988606485539,"• The answer NA means that the paper poses no such risks.
863"
SAFEGUARDS,0.9307624890446976,"• Released models that have a high risk for misuse or dual-use should be released with
864"
SAFEGUARDS,0.9316389132340053,"necessary safeguards to allow for controlled use of the model, for example by requiring
865"
SAFEGUARDS,0.9325153374233128,"that users adhere to usage guidelines or restrictions to access the model or implementing
866"
SAFEGUARDS,0.9333917616126205,"safety filters.
867"
SAFEGUARDS,0.9342681858019282,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
868"
SAFEGUARDS,0.9351446099912357,"should describe how they avoided releasing unsafe images.
869"
SAFEGUARDS,0.9360210341805434,"• We recognize that providing effective safeguards is challenging, and many papers do
870"
SAFEGUARDS,0.936897458369851,"not require this, but we encourage authors to take this into account and make a best
871"
SAFEGUARDS,0.9377738825591586,"faith effort.
872"
LICENSES FOR EXISTING ASSETS,0.9386503067484663,"12. Licenses for existing assets
873"
LICENSES FOR EXISTING ASSETS,0.9395267309377738,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
874"
LICENSES FOR EXISTING ASSETS,0.9404031551270815,"the paper, properly credited and are the license and terms of use explicitly mentioned and
875"
LICENSES FOR EXISTING ASSETS,0.9412795793163892,"properly respected?
876"
LICENSES FOR EXISTING ASSETS,0.9421560035056967,"Answer: [Yes]
877"
LICENSES FOR EXISTING ASSETS,0.9430324276950044,"Justification: We have cited the original paper of our reference code and datasets.
878"
LICENSES FOR EXISTING ASSETS,0.9439088518843121,"Guidelines:
879"
LICENSES FOR EXISTING ASSETS,0.9447852760736196,"• The answer NA means that the paper does not use existing assets.
880"
LICENSES FOR EXISTING ASSETS,0.9456617002629273,"• The authors should cite the original paper that produced the code package or dataset.
881"
LICENSES FOR EXISTING ASSETS,0.9465381244522348,"• The authors should state which version of the asset is used and, if possible, include a
882"
LICENSES FOR EXISTING ASSETS,0.9474145486415425,"URL.
883"
LICENSES FOR EXISTING ASSETS,0.9482909728308502,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
884"
LICENSES FOR EXISTING ASSETS,0.9491673970201577,"• For scraped data from a particular source (e.g., website), the copyright and terms of
885"
LICENSES FOR EXISTING ASSETS,0.9500438212094654,"service of that source should be provided.
886"
LICENSES FOR EXISTING ASSETS,0.950920245398773,"• If assets are released, the license, copyright information, and terms of use in the
887"
LICENSES FOR EXISTING ASSETS,0.9517966695880806,"package should be provided. For popular datasets, paperswithcode.com/datasets
888"
LICENSES FOR EXISTING ASSETS,0.9526730937773883,"has curated licenses for some datasets. Their licensing guide can help determine the
889"
LICENSES FOR EXISTING ASSETS,0.9535495179666958,"license of a dataset.
890"
LICENSES FOR EXISTING ASSETS,0.9544259421560035,"• For existing datasets that are re-packaged, both the original license and the license of
891"
LICENSES FOR EXISTING ASSETS,0.9553023663453112,"the derived asset (if it has changed) should be provided.
892"
LICENSES FOR EXISTING ASSETS,0.9561787905346187,"• If this information is not available online, the authors are encouraged to reach out to
893"
LICENSES FOR EXISTING ASSETS,0.9570552147239264,"the asset’s creators.
894"
NEW ASSETS,0.957931638913234,"13. New Assets
895"
NEW ASSETS,0.9588080631025416,"Question: Are new assets introduced in the paper well documented and is the documentation
896"
NEW ASSETS,0.9596844872918493,"provided alongside the assets?
897"
NEW ASSETS,0.9605609114811569,"Answer: [NA]
898"
NEW ASSETS,0.9614373356704645,"Justification: The paper does not release new assets.
899"
NEW ASSETS,0.9623137598597721,"Guidelines:
900"
NEW ASSETS,0.9631901840490797,"• The answer NA means that the paper does not release new assets.
901"
NEW ASSETS,0.9640666082383874,"• Researchers should communicate the details of the dataset/code/model as part of their
902"
NEW ASSETS,0.964943032427695,"submissions via structured templates. This includes details about training, license,
903"
NEW ASSETS,0.9658194566170026,"limitations, etc.
904"
NEW ASSETS,0.9666958808063103,"• The paper should discuss whether and how consent was obtained from people whose
905"
NEW ASSETS,0.9675723049956179,"asset is used.
906"
NEW ASSETS,0.9684487291849255,"• At submission time, remember to anonymize your assets (if applicable). You can either
907"
NEW ASSETS,0.9693251533742331,"create an anonymized URL or include an anonymized zip file.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702015775635408,"14. Crowdsourcing and Research with Human Subjects
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9710780017528484,"Question: For crowdsourcing experiments and research with human subjects, does the paper
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971954425942156,"include the full text of instructions given to participants and screenshots, if applicable, as
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728308501314636,"well as details about compensation (if any)?
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737072743207712,"Answer: [NA]
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745836985100789,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754601226993865,"Guidelines:
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763365468886941,"• The answer NA means that the paper does not involve crowdsourcing nor research with
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772129710780018,"human subjects.
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780893952673094,"• Including this information in the supplemental material is fine, but if the main contribu-
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978965819456617,"tion of the paper involves human subjects, then as much detail as possible should be
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798422436459246,"included in the main paper.
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807186678352322,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815950920245399,"or other labor should be paid at least the minimum wage in the country of the data
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824715162138475,"collector.
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833479404031551,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842243645924628,"Subjects
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851007887817704,"Question: Does the paper describe potential risks incurred by study participants, whether
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985977212971078,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868536371603857,"approvals (or an equivalent approval/review based on the requirements of your country or
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877300613496932,"institution) were obtained?
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886064855390009,"Answer: [NA]
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894829097283085,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903593339176161,"Guidelines:
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912357581069238,"• The answer NA means that the paper does not involve crowdsourcing nor research with
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921121822962313,"human subjects.
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992988606485539,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938650306748467,"may be required for any human subjects research. If you obtained IRB approval, you
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947414548641542,"should clearly state this in the paper.
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956178790534619,"• We recognize that the procedures for this may vary significantly between institutions
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964943032427696,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973707274320771,"guidelines for their institution.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982471516213848,"• For initial submissions, do not include any information that would break anonymity (if
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991235758106923,"applicable), such as the institution conducting the review.
942"
