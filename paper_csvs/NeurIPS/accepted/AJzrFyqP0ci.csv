Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004273504273504274,"In the study of reasoning in neural networks, recent efforts have sought to improve
consistency and coherence of sequence models, leading to important developments
in the area of neuro-symbolic AI. In symbolic AI, the concepts of consistency
and coherence can be deﬁned and veriﬁed formally, but for neural networks these
deﬁnitions are lacking. The provision of such formal deﬁnitions is crucial to
offer a common basis for the quantitative evaluation and systematic comparison
of connectionist, neuro-symbolic and transfer learning approaches. In this paper,
we introduce formal deﬁnitions of consistency and coherence for neural systems.
To illustrate the usefulness of our deﬁnitions, we propose a new dynamic relation-
decoder model built around the principles of consistency and coherence. We
compare our results with several existing relation-decoders using a partial transfer
learning task based on a novel data set introduced in this paper. Our experiments
show that relation-decoders that maintain consistency over unobserved regions
of representation space retain coherence across domains, whilst achieving better
transfer learning performance."
INTRODUCTION,0.008547008547008548,"1
Introduction"
INTRODUCTION,0.01282051282051282,"Humans are capable of learning concepts that can be applied to many different scenarios [18, 34, 23].
An important principle is that human-like concepts remain coherent across contexts [31]. As an
example, consider the concept of ordinality, e.g. “A is larger than B”, which allows comparisons to
be made between ordered sets. Ordinality should apply equally whether A and B are digits or a tower
of blocks. It is said that a concept may pertain to a multitude of properties: position, volume, reach,
etc. As long as one of these properties can be attributed to an object, a set of objects can be compared
on that basis. All in all, if the concept of ordinality was to be learned in its most general form, its use
should be consistent across objects and coherent across object properties."
INTRODUCTION,0.017094017094017096,"In [31], empirical results on story generation and instruction-following have shown that an intuitive
use of consistency and coherence can increase the accuracy of neural networks. Following a neuro-
symbolic perspective [12], it is argued in [31] that System 1 approaches, fast and capable of learning"
INTRODUCTION,0.021367521367521368,"*Funding in direct support of this work: EPSRC Training Grant, project reference EP/L504786/1."
INTRODUCTION,0.02564102564102564,"patterns efﬁciently from data, “are often inconsistent and incoherent”, and that “adding System
2-inspired logical reasoning” as a logical consistency, training-free module allows for an improved
selection of candidate stories generated by System 1. While [31] makes an important contribution by
exploring several variations on the theme, in this paper we offer a formal deﬁnition for consistency
and coherence in the context of neural networks, in particular neuro-symbolic autoencoders. We also
apply and evaluate consistency and coherence in transfer learning tasks, where we believe that the
theme will have its most practical impact."
INTRODUCTION,0.029914529914529916,"We argue that for a concept to be useful during transfer learning, the system of relations that deﬁne the
concept in the source domain must be coherent with the target domain, whereby logical consistency
achieved in the source is retained in the target domain. This is to say that the concept-speciﬁc relations
learned in the source ought to be consistent with a logical theory that deﬁnes their semantics, and
that such consistency must extend beyond the representations learned in the source domain and, in
particular, hold for the embeddings learned in the target domain."
INTRODUCTION,0.03418803418803419,"In this paper, we offer a formal deﬁnition for consistency and coherence of sub-symbolic represen-
tation learners, inspired by analogous deﬁnitions from symbolic AI. This is expected to deﬁne the
conditions that make a learned concept transfer well across properties and objects. To evaluate the
practical value of these deﬁnitions in a real setting, we derive a simple neuro-symbolic autoencoder
architecture consisting of a neural encoder for objects coupled with consistent modular object relation-
decoders. Relations such as isGreater, isEqual,... are evaluated on a proposed Partial Relation
Transfer (PRT) learning task, between a new CLEVR-style BlockStacks data set and the MNIST
handwritten digits data set, such that the learning of ordinality among the MNIST digits is evaluated
against the learning of the relative position of a red block in a stack of multi-colored blocks. Our
evaluation includes a comparison with several existing relation-decoder models and results show that
relation-decoders which maintain consistency over unobserved regions of representation space retain
coherence across domains whilst achieving better transfer learning performance.1 In summary, the
contributions of this paper are:"
INTRODUCTION,0.038461538461538464,• A formal deﬁnition of consistency and coherence for sub-symbolic learners offering a
INTRODUCTION,0.042735042735042736,practical evaluation score for concept coherence;
INTRODUCTION,0.04700854700854701,• A derived model implementation and PRT data set and experimental setup used to evaluate
INTRODUCTION,0.05128205128205128,the interplay between concept coherence and concept transfer;
INTRODUCTION,0.05555555555555555,• A comprehensive critical evaluation of results and comparison of multiple relation-decoder
INTRODUCTION,0.05982905982905983,"models, showing that improvements in concept coherence, as deﬁned in this paper, corre-
spond with improved concept transfer."
INTRODUCTION,0.0641025641025641,"In Section 2 we provide the notation and required logic background. Section 3 formally deﬁnes
coherence and consistency. Section 4 deﬁnes a practical consistency loss and Section 5 outlines our
neuro-symbolic autoencoder. After detailing the PRT task and introducing the data set in Section 6,
comparative experimental results are discussed in Section 7. We provide an overview of the related
work in Section 8 and Section 9 concludes the paper with a discussion, including limitations and
future work. We expand on the experimental results and setup, together with data set characteristics,
model details and parameterization in the Appendices.2"
PRELIMINARIES,0.06837606837606838,"2
Preliminaries"
PRELIMINARIES,0.07264957264957266,"Notation: We reserve uppercase calligraphic letters to denote sets, and lowercase versions of the
same letter to denote their elements, e.g. S = {s1, . . . , sn} is a set S of n elements si. We indicate
with |S| = n the cardinality of S. We use uppercase roman letters to denote a random variable (e.g.
S), and use the uppercase calligraphic version of the same letter (S) to denote the set from which the
random variable takes values according to some corresponding probability distribution pS, over the
elements of the set, such that P|S|"
PRELIMINARIES,0.07692307692307693,"i=1 pS(si) = 1 for a discrete S. For brevity, we may write pS(si) as
p(si), where the random variable is implied by the argument. We use bold font lowercase letters to
denote vector elements, e.g. si 2 Rd is an d-dimensional vector element from the set S = Rd."
PRELIMINARIES,0.0811965811965812,"1This paper formalizes the theory and extends the empirical results ﬁrst reported in [43].
2The codebase for this paper can be found at https://github.com/HStromfelt/neurips22-FCA."
PRELIMINARIES,0.08547008547008547,"Logic and model-theoretic background: our proposed theory is based upon logic and model
theoretic primitives. To avoid making this paper overly dense, we defer the details of the logic
background to Appendix E and include here only the most important deﬁnitions supported by an
illustrative example."
PRELIMINARIES,0.08974358974358974,"Deﬁnition 2.1 (Signature, Arity, Domain, Interpretation, Structure). The signature of a language L is
a set of relations σ = {r2L} whose elements have arity given by ar : σ ! N, where N is the set of
natural numbers. Given a signature σ and a non-empty domain S = {s1, s2, . . .}, an interpretation
ISσ of σ over elements of S assigns to each relation r 2 σ a set ISσ(r) ✓Sar(r). A structure is a
tuple Sσ = (S, ISσ)."
PRELIMINARIES,0.09401709401709402,"We construct universally quantiﬁed ﬁrst-order logic formulae (called sentences) using the signature
of L. A set of sentences form a theory T and when a sentence ⌧is true in a structure Sσ, we say that
the structure satisﬁes ⌧, denoted as Sσ |= ⌧. This allows us to deﬁne a model of a theory:"
PRELIMINARIES,0.09829059829059829,"Deﬁnition 2.2 (Model of a theory). Let T be a theory written in a language L and let Sσ = (S, ISσ)
be a structure, where σ is the signature of L. Sσ is a model of T if and only if Sσ |= ⌧for every
sentence ⌧2 T ."
PRELIMINARIES,0.10256410256410256,"Example 1. Let S is a domain of images of handwritten digits and σ the signature of binary
relations σ = {isGreater, isEqual, isLess, isSuccessor, isPredecessor}, or for short σ = {G,
E, L, S, P}. Let T be the theory that deﬁnes ordinality including, for instance, the sentence
8i, j. G(i, j) ! ¬E(i, j) (if a digit is greater than another then they are not equal). Any structure
Sσ = (S, ISσ) with interpretations ISσ of σ that captures a total order over the elements of S is a
model of T ."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.10683760683760683,"3
A Formalization of Consistency and Coherence"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1111111111111111,"In this section, we turn our attention to the challenge of learning a model of a theory (Def. 2.2) over a
real-world domain S given a signature σ. Here, a learner must determine an appropriate interpretation
over real-world data, such as images or other perceptions. This can be challenging because, ﬁrstly, we
may only have a partial description of the interpretation, and secondly data may be noisy and contain
information that is not relevant to the theory. For example, the handwritten digits in the MNIST data
set contain stylistic details such as line thickness and digit skew that are irrelevant to the notion of
ordinality, which makes learning the structure from Example 1 non-trivial."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.11538461538461539,"Following the convention from the autoencoder disentanglement literature [4, 21, 17, 16], we make
the assumption that real-world observations S are drawn from some conditional distribution pS|Z,
where Z is a latent random variable, itself drawn from prior pZ. It is therefore useful to deﬁne a
domain encoding of the form:"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.11965811965811966,"S : S ! Z,
(1)"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.12393162393162394,"tasked with approximating the conditional expectation of the posterior, i.e.  S(s) = EpZ|S[Z|s].
Since obtaining an interpretation from domain encodings for a given signature may require dealing
with noise, we express the interpretation of relations over real-world data by belief functions [33, 32]
over the space Z, and refer to these as relation-decoders:"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1282051282051282,"φr : Zar(r) ! (0, 1)
(2)"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.13247863247863248,"with φ = {φr : r 2 σ}. Concretely, for a binary relation r and ordered pair (si, sj) 2 S2,
φr( S(si),  S(sj)) describes the belief that (si, sj) 2 ISσ(r). A belief φr( S(si),  S(sj)) ⇡1
signiﬁes a strong belief that (si, sj) 2 ISσ(r) and φr( S(si),  S(sj))⇡0 signiﬁes a strong belief
that (si, sj) /2 ISσ(r). Together,  S and φ allow us to deﬁne a belief-based analogue to a structure."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.13675213675213677,"Deﬁnition 3.1 (Soft-Structure/Soft-Substructure). Given a signature σ, a (possibly inﬁnite) set
Z and relation-decoders φ, a soft-structure is a tuple ˜
Zσ = (Z, φ). For a ﬁnite domain S and
encoding  S : S ! Z, ˜
Sσ = ( S(S), φ) is called a ﬁnite soft-substructure of ˜
Zσ, with sub-domain
 S(S) = { S(s)|s 2 S} ✓Z."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.14102564102564102,"A soft-structure can be used to learn a logic structure over a real-world domain through learning  S
and φ. Clearly, a ﬁnite soft-substructure is a soft-structure. In a real-world domain, there may be
only partial information about the values of an interpretation, and there may be errors in that partial"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1452991452991453,"interpretation. To determine the degree to which a soft-structure supports any given structure, we
introduce the following measure:"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.14957264957264957,"p(Sσ| ˜
Sσ) = Y r2σ Y"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.15384615384615385,O2Sar(r)
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1581196581196581,"f(φr,  S, O, γr"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1623931623931624,"O,Sσ)
(3) with"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.16666666666666666,"f(φr,  S, O, γr"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.17094017094017094,"O,Sσ) = (φr( S(O)))γr"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1752136752136752,"O,Sσ · (1 −φr( S(O)))1−γr"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.1794871794871795,"O,Sσ ,
(4)
where γr"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.18376068376068377,"O,Sσ
= 1 if O 2 ISσ(r), and 0 otherwise; we use φr( S(O)) as shorthand for
φr( S(s1), . . . ,  S(sn)) for n = ar(r). Eqn. 3 expresses the assumption that, given a ﬁnite
soft-structure, the beliefs in what constitutes the interpretations of different relations are independent
of one another. It is straightforward to show that P"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.18803418803418803,"Sσ p(Sσ| ˜
Sσ) = 1 (summed over all possible
structures with domain S and signature σ) and so Eqn. 3 can be treated as a probability measure,
where p(Sσ| ˜
Sσ) ⇡1 means that there is a high probability that the interpretation sampled from ˜
Sσ
will be ISσ. If we have a theory T over σ then it is natural to ask with what weight ˜
Sσ supports any
given structure that is a model of T . In the following, we use model weight, Γ"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.19230769230769232,"˜
Sσ
T , to describe the
support given by ˜
Sσ to models of T : Γ"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.19658119658119658,"˜
Sσ
T
= X Sσ2MT S"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.20085470085470086,"p(Sσ| ˜
Sσ)
(5)"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.20512820512820512,where MT
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.2094017094017094,"S is the set of all structures with domain S that are models of T . This lets us compare
soft-structures, wherein a good soft-structure will be one that has a high model weight.
Deﬁnition 3.2 (✏-Consistency of soft-structures). Given a ﬁnite soft-structure ˜
Sσ and an arbitrarily
small number ✏, if 1 −Γ"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.21367521367521367,"˜
Sσ
T
✏then we say that ˜
Sσ is ✏-consistent with theory T ."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.21794871794871795,"We propose ✏-consistency as an appropriate measure of the notion of consistency presented in [31]. A
consistent soft-structure ˜
Sσ ensures that φ gives high belief only to interpretations that satisfy, and
therefore are logically consistent with, theory T . As expected, consistency pertains to the domain
encodings of ˜
Sσ, i.e.  S(S). For a concept to be learned in a manner comparable to how a human
might learn, we would expect consistency to carry over to new domains with their corresponding soft-
structures, which motivates our deﬁnition of coherence between soft-structures, as follows. Consider
a situation where a deep network has already learned from the MNIST data set a soft-structure that
has high model weight, given the relations {G, E, L, S, P} from Example 1. Now, consider a new
domain of images, Y, showing single block stacks of different heights, and we wish to re-use the
signature of ordinal relations and T from Example 1. Let IYσ be an interpretation in the new domain
that orders images according to block stack height and that is a model of T . We can summarise this
with the following two structures:"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.2222222222222222,"Xσ = (X, IXσ) 2 MT"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.2264957264957265,"X
and
Yσ = (Y, IYσ) 2 MT"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.23076923076923078,"Y ,
(6)
where Xσ is the structure from Example 1 with a domain of handwritten digits and Yσ is our new
structure, with a domain of block stack images. These can be learned by soft-structures:"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.23504273504273504,"˜
Xσ = ( X (X), φ)
and
˜
Yσ = ( Y(Y), φ),
(7)
which use domain-speciﬁc encoders,  X and  Y, but share the same relation-decoders. As we know
that ˜
Xσ has a high model weight and since φ is shared with ˜
Yσ, a natural question to ask is: under
what conditions will a φ that is consistent over domain-encodings  X (X) also be consistent over
 Y(Y)? Concretely, we are interested in specifying when the following coherence condition holds."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.23931623931623933,"Deﬁnition 3.3 (✏-Coherence across soft-structures). Two soft-structures, ˜
Xσ and ˜
Yσ that share
relation-decoders φ, are said to be ✏-coherent with respect to a theory T , if ˜
Xσ is ✏1-consistent with
T , ˜
Yσ is ✏2-consistent with T , ✏1 ✏, and ✏2 ✏."
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.24358974358974358,"Coherence between ˜
Xσ and ˜
Yσ as deﬁned above means that the concept of ordinality that applies to
digit ordering can also be applied to block stack height ordering. It is desirable that learning ordinality
on the domain of digits produces a coherent concept of ordinality with respect to other ordinal
properties, such as height. Since it is possible that  S(X) and  S(Y) produce unique encodings,
coherence relies on φ’s ability to generalize over possibly disjoint subsets of Z.3"
A FORMALIZATION OF CONSISTENCY AND COHERENCE,0.24786324786324787,"3If soft-structure ˜
Zσ deﬁned over the full space Z is consistent then coherence is guaranteed between all
possible soft-substructures."
MEASURING CONSISTENCY AND COHERENCE,0.25213675213675213,"4
Measuring Consistency and Coherence"
MEASURING CONSISTENCY AND COHERENCE,0.2564102564102564,"Calculating Eqn. 5 can be computationally too expensive for larger domains. An efﬁcient approach
to measuring the consistency of soft-structures is therefore required. In this section, we introduce a
proxy measure for a soft-structure’s ✏-consistency and ✏-coherence with a given theory when access
to every logical model is not available or is computationally intractable."
MEASURING CONSISTENCY AND COHERENCE,0.2606837606837607,"Suppose that there is a ﬁxed domain S and theory T whose sentences use relations from a signature
σ. Let k 2 {1, ..., K0} denote the index associated with each unique ground instance of the sentences
in T . Take BT to be a Boolean random variable. For the k-th grounding in T , the probability
of the theory being satisﬁed under soft-structure ˜
Sσ is expressed as p(bT = 1| ˜
Sσ, k). Conversely,
the probability of non-satisfaction is given by p(bT = 0| ˜
Sσ, k). For a model of (universally-
quantiﬁed) theory T , Sσ 2 MT"
MEASURING CONSISTENCY AND COHERENCE,0.26495726495726496,"S , any grounding k of the T is always satisﬁed (by deﬁnition),
and thus p(bT = 1|Sσ, k) = 1. When ˜
Sσ is consistent with T then we should also ﬁnd that
p(bT = 1| ˜
Sσ, k) ⇡1 for all k. Hence, we deﬁne a consistency loss function as the expectation over a
randomly-chosen grounding k of the binary cross-entropy between p(BT |Sσ, k) and p(BT | ˜
Sσ, k) for
any Sσ 2 MT"
MEASURING CONSISTENCY AND COHERENCE,0.2692307692307692,"S . This in turn simpliﬁes to produce the expected negative log-likelihood of satisfying a
random grounding of T , as follows:"
MEASURING CONSISTENCY AND COHERENCE,0.27350427350427353,"L(T , ˜
Sσ) = Ek⇠p(k)[−ln p(bT = 1| ˜
Sσ, k)].
(8)"
MEASURING CONSISTENCY AND COHERENCE,0.2777777777777778,"where p(k) =
1
K0 is the uniform distribution over the set of unique groundings. A measure based on
this loss is required to enable a practical evaluation of consistency, acting as an approximation for
consistency. More precisely, we deﬁne ¯Γ"
MEASURING CONSISTENCY AND COHERENCE,0.28205128205128205,"˜
Sσ
T
= exp(−L(T , ˜
Sσ)) as a proxy measure of Γ"
MEASURING CONSISTENCY AND COHERENCE,0.2863247863247863,"˜
Sσ
T , and say
that soft-structure ˜
Sσ is ¯✏-proxy consistent with T if"
MEASURING CONSISTENCY AND COHERENCE,0.2905982905982906,"ln
1
1 −¯✏≥L(T , ˜
Sσ)
(9)"
MEASURING CONSISTENCY AND COHERENCE,0.2948717948717949,where ¯✏≥1 −¯Γ
MEASURING CONSISTENCY AND COHERENCE,0.29914529914529914,"˜
Sσ
T . Due to the relationship between ¯✏and L(T , ˜
Sσ), we take the proxy measure of
coherence to be the smallest satisﬁable value of L(T , ˜
Sσ) between domains.4"
MEASURING CONSISTENCY AND COHERENCE,0.3034188034188034,"Although our treatment of consistency has thus far focused on a particular theory T , notice that
a subset of the sentences of T form a partial theory, which is itself a theory. This means that
consistency can be evaluated given a partial (even single sentence) theory, allowing us to exam-
ine consistency losses for any partial speciﬁcations of a given domain of interest. In this paper,
we evaluate the proposed consistency loss against two partial speciﬁcations within the theory of
ordinality. These are named Consistency-Across (Con-A) and Consistency-Individual (Con-I) in
the experiments that will follow.5 Con-A includes the sentences that determine inter-relation be-
havior, for instance the sentence 8i, j. (isGreater(i, j) ! ¬isEqual(i, j) ^ ¬isLess(i, j)), stating
that if i is greater than j then i must not be equal to or less than j. Con-I includes the sentences
that are about a single relation, describing any property of an individual relation over objects (or
images in the case of our experiments). Each relation may satisfy a number of properties, for ex-
ample 8i, j, k. (isGreater(i, j) ^ isGreater(j, k) ! isGreater(i, k)) represents transitivity of the
isGreater relation. Transitivity is true for isGreater, but is false for other relations investigated in
this paper, e.g. isSuccessor. We will evaluate consistency loss of transitivity (Con-I-T), asymmetry
(Con-I-A) and reﬂexivity (Con-I-R) for the relations in Example 1. The evaluation of consistency
loss for any available partial theory will be shown to provide a more nuanced perspective on model
performance than accuracy results and disentanglement pressure alone during transfer learning."
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3076923076923077,"5
A Consistent and Coherent Neuro-symbolic Autoencoder"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.31196581196581197,"In order to ground our deﬁnitions of consistency (Def. 3.2) and coherence (Def. 3.3) into a real system
and evaluate their practical value, in this section we derive a simple neuro-symbolic autoencoder
architecture which offers one of many possible implementations of the theory deﬁned in Section
3. Figure 1 outlines the main components of our autoencoder: a domain-encoder  S and modular"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3162393162393162,"4The complete derivation of loss function and bounds is presented in Appendix G.
5Truth-tables for each consistency formula are given in Appendix F."
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.32051282051282054,"Figure 1: Network architecture used for PRT tasks. In our experiments si/j are either MNIST
(domain X) or BlockStacks (domain Y) images. Relational learning is performed on the source
(S = X) MNIST domain (to learn e.g. that digit 5 is greater than 3). Moving to the target (S = Y)
domain (stacks of blocks) involves training a new image autoencoder together with a subset of
the relation-decoders from MNIST with ﬁxed parameters. The remaining relations are held-out to
evaluate zero-shot transfer learning performance. γr"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3247863247863248,"O,Sσ provides the ground truth for the given
structure, which φr predicts as ˆγr"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.32905982905982906,"O,Sσ. As in Section 3, O is used to abbreviate ( enc"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3333333333333333,"S
(si),  enc"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.33760683760683763,"S
(sj))."
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3418803418803419,"relation-decoders φ form an autoencoding architecture that, given a domain of images S ⇢RC⇥W ⇥H
(C color channels, width W and height H) and a d-dimensional latent space Z = Rd, converts
sub-symbolic encodings from  S into a modular relational representation via decoding for each
φr, r 2 σ. Additionally, to retain information in Z pertaining to S which is beyond the requirements
of φ, a domain-decoder produces domain reconstructions ˆS. In Figure 1, we use  enc"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.34615384615384615,"S
to refer to the
domain-encoder and  dec"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3504273504273504,"S
to the domain-decoder. Although in this paper we opt for an autoencoding
architecture, our deﬁnitions of consistency and coherence are applicable to a wider range of neural
architectures. For instance, a multi-layer perception network can be viewed as a set of encoding and
decoding layers [40]. As long as the architecture offers explicit soft relation decodings, provided we
can deﬁne a partial theory over them, we can deﬁne a consistency loss over the outputs."
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3547008547008547,"To train the model, ground-truth interpretations ISσ are provided, allowing us to maximize directly
Eqn. 3 via the negative log-likelihood loss: L"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.358974358974359,"˜
Sσ = −log p(Sσ| ˜
Sσ),
(10)
To obtain informative latent representations for S, we use a Variational Autoencoder (VAE), speciﬁ-
cally the β-VAE [6, 17, 21] due to its simplicity and demonstrated ability to separate distinct factors
in the latent representation (known as disentanglement, although disentanglement is not a requirement
for consistency and coherence). We therefore take the Evidence Lower Bound (ELBO) objective with
an additional β scalar hyperparameter from [17], that seeks to achieve disentanglement (LELBO"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.36324786324786323,"β-VAE), and"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.36752136752136755,combine it with L
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3717948717948718,"˜
Sσ to obtain the following aggregate objective:6"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.37606837606837606,Ljoint = LELBO
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3803418803418803,β-VAE −λL
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.38461538461538464,"˜
Sσ
(11)
where λ is a scalar weighting parameter."
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3888888888888889,Together with the LELBO
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.39316239316239315,"β-VAE, the choice of relation-decoder can shape the domain encodings [15]. In our
evaluation, the following choices are made. We propose a Dynamic Comparator (DC) composed of
two modes, a distance-based measure, φ†"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.3974358974358974,"r, to measure the distance between two inputs relative to a
reference point, and a step-function, φ‡"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.4017094017094017,"r, that determines the sign of the difference between two points,
optionally with an offset. Although any function could be used that has the required characteristics
for φ† and φ‡, in this paper we use the following implementation: φDC"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.405982905982906,"r
(zi, zj) = ar,0 · φ†"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.41025641025641024,"r + ar,1 · φ‡"
A CONSISTENT AND COHERENT NEURO-SYMBOLIC AUTOENCODER,0.41452991452991456,"r
(12)"
A MORE DETAILED DERIVATION OF LELBO,0.4188034188034188,6a more detailed derivation of LELBO
A MORE DETAILED DERIVATION OF LELBO,0.4230769230769231,β-VAE is included in the Appendix C
A MORE DETAILED DERIVATION OF LELBO,0.42735042735042733,"where, φ†"
A MORE DETAILED DERIVATION OF LELBO,0.43162393162393164,r = f0 $
A MORE DETAILED DERIVATION OF LELBO,0.4358974358974359,"−⌘r,0 · kur ⊙(zi −zj + b† r)k2 % (13) φ‡"
A MORE DETAILED DERIVATION OF LELBO,0.44017094017094016,r = f1 $
A MORE DETAILED DERIVATION OF LELBO,0.4444444444444444,"⌘r,1 · u>"
A MORE DETAILED DERIVATION OF LELBO,0.44871794871794873,r (zi −zj + b‡ r) %
A MORE DETAILED DERIVATION OF LELBO,0.452991452991453,".
(14)"
A MORE DETAILED DERIVATION OF LELBO,0.45726495726495725,"Here, ar = Softmax(Ar) 2 (0, 1)2 is an attention weighting between the two modes, φ†"
A MORE DETAILED DERIVATION OF LELBO,0.46153846153846156,r and φ‡
A MORE DETAILED DERIVATION OF LELBO,0.4658119658119658,"r; f0
and f1 are an exponential and sigmoid function, respectively; ur = Softmax(Ur) 2 (0, 1)m is an
attention mask which is applied to m-dimensional embeddings; b† r, b‡"
A MORE DETAILED DERIVATION OF LELBO,0.4700854700854701,"r 2 Rm are learnable bias terms
that enable an offset to each mode; ⌘r,0 2 R+ are non-negative and ⌘r,1 2 R are any-valued scalar
terms, respectively. Lastly, ⊙denotes the Hadamard product and k · k2 is the Euclidean norm. The
key innovation behind DC is its ability to model each of the ordinality relations whilst encouraging
generalized consistency across the full latent subspace, as deﬁned by each ur. This is achieved
without explicit weight sharing, wherein relation-decoders discover parametric relationships from the
data. Further details are provided in Appendix D.1."
A MORE DETAILED DERIVATION OF LELBO,0.47435897435897434,"6
Experiment Design: Partial Relation Transfer"
A MORE DETAILED DERIVATION OF LELBO,0.47863247863247865,We now describe an experimental design to compare coherence of different relation-decoders.
A MORE DETAILED DERIVATION OF LELBO,0.4829059829059829,"Partial Relation Transfer (PRT): We evaluate a novel PRT task across two soft-structures ˜
Xσ
and ˜
Yσ. The soft-structures share a common signature σ and relation-decoders φ, but have disjoint
domains X and Y, respectively. The experimental design involves ﬁrst learning φ on source domain
X, together with its domain-speciﬁc autoencoder. Then, a new domain-speciﬁc autoencoder is
trained on the target domain Y, alongside a selection of the now learned φ relation-decoders with
ﬁxed-parameters. The selection of relation-decoders is expected to help guide training of  enc"
A MORE DETAILED DERIVATION OF LELBO,0.48717948717948717,"Y (see
Fig.1). Held-out relation-decoders are then evaluated in Y, i.e. a zero-shot transfer learning task. For
domain X we use the MNIST handwritten digits data set [24], and for domain Y we use the proposed
BlockStacks data set, consisting of a single stack of multi-colored cubes of differing heights, each
containing one randomly-positioned red cube (see Appendix B for details and examples). The shared
signature includes the ordinal relations σ ={G, E, L, S, P}, and it is applied to digit ordering in
MNIST and to red cube position ordering in BlockStacks. We provide results with respect to a theory
of ordinality, as explored in Example 1. A formal speciﬁcation of the theory is provided in Appendix
F. When transferring relations from  enc"
A MORE DETAILED DERIVATION OF LELBO,0.49145299145299143,X to  enc
A MORE DETAILED DERIVATION OF LELBO,0.49572649572649574,"Y , one could use the full set φ of relation-decoders.
However, this is not necessary from a logical standpoint because the entire system of relations can
be expressed in terms of the isSuccessor relation S (e.g. the successor of a number is larger than
that number). We therefore only employ the isSuccessor relation-decoder as the ﬁxed-parameter
selection to guide the learning of  enc"
A MORE DETAILED DERIVATION OF LELBO,0.5,"Y . If coherence, as deﬁned in this paper, is carried across
domains, we would expect the transferring of isSuccessor to produce an improved performance on
the remaining relations in the target domain."
A MORE DETAILED DERIVATION OF LELBO,0.5042735042735043,"Neural model components and hyperparameters: Together with DC, existing relation-decoder
models evaluated here are: TransR [25], HolE [30], NTN [41]. We additionally include a basic
feedforward neural network (NN). To produce domain-encodings, all experiments use a β-VAE
[17]. We provide further details for all models, including training regimen, parameterization and
implementation in Appendix D. In the source domain, we explore β values in {1, 4, 8, 12} and set
λ = 103. In the target domain, we ﬁrst normalise losses and set β = 10−4 and λ = 10−2, as these
produced good image reconstructions while optimising L"
A MORE DETAILED DERIVATION OF LELBO,0.5085470085470085,"˜
Yσ. In all experiments we ﬁx Z = R10."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5128205128205128,"7
Experimental Results and Discussion"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5170940170940171,"In this section, experimental results show that transfer learning performance is positively correlated
with our measures for consistency and coherence. This holds particularly true for embeddings that
are different but near in space to source domain embeddings. As we have argued, for a neural model
to perform well on concept transfer, its representations must maintain high probability of consistency
with a theory that provides a semantics for the concept. The most robust way of doing this is to
maintain consistency across regions of embedding space, rather than relying exclusively on the
speciﬁc data-points observed at training time in the source domain. In our analysis, consistency
losses are evaluated when sampling from different regions of latent space Z. We evaluate: data-
embeddings, where all inputs are encodings of a domain’s test data; interpolation, when we derive an"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5213675213675214,"Figure 2: [Top] Relation-decoder prediction accuracy per model (DC, NN, NTN, HoIE, TransR) and
relation (abbreviated on the x-axis as in Example 1), in the source domain (MNIST, left) and target
domain (BlockStack, right). A red highlighted S and dotted line (top right) indicates that relation
isSuccessor is included in training the target domain autoencoder, but none of the other relations
are. Both DC and NN retain a good performance while all other models show a decrease of accuracy
in the target domain for one or more of the relations not included in training. [Bottom] Impact of
different values of β 2 {1, 4, 8, 12} for each relation-decoder averaged across all relations in the
source domain (left) and held-out relations {P, E, G, L} in the target domain (right). It can be seen
that DC is not impacted by changes in β and it maintains performance in the target domain. All other
models show a decrease of accuracy for the held-out relations in the target domain."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5256410256410257,"empirical mean and variance for the domain’s data-embeddings and sample from a corresponding
Gaussian distribution; and extrapolation, when we sample from regions strictly outside the smallest,
axis-aligned, hyper-rectangle that encloses all data-points."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5299145299145299,"Figure 2-top provides relation-decoder prediction accuracies in both the source (MNIST, left) and
target (BlockStacks, right) domains.7 The relations are S, P, E, G, L and relation S is transferred to
the target domain. Key observations are that DC produces excellent PRT performance, whilst NN,
NTN and HolE all see some degradation from their source-domain accuracies for relations other than
isSuccessor (S). TransR maintains target-domain accuracies similar to its performance in the source
domain, but this is signiﬁcantly below the performance of other models in the source domain. We
include the impact of adjusting β (disentanglement pressure) in Figure 2-bottom. Barring DC which
has little discernible change in either source or target domains, PRT performance is signiﬁcantly
impacted by β for all models in the target domain, but has little effect in the source domain. TransR
shows a strong positive correlation between target domain accuracy and β values, whereas the
remaining models produce their best PRT performance with medium disentanglement pressure."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5341880341880342,"To investigate the broad trends that run across all β values and relation-decoder models, we ran a
Spearman rank correlation analysis between consistency losses and PRT performance. Separate
coefﬁcients are produced for each combination of consistency loss: Con-A and Con-I further divided
into Con-I-T (transitivity), Con-I-A (asymmetry) and Con-I-R (reﬂexivity), and regions of latent
space: data-embedding, interpolation and extrapolation (see Appendix H for the full table). Lower
consistency losses are expected to produce higher PRT performance as indicated by a negative
Spearman rank coefﬁcient. The coefﬁcients show that the consistency losses of data-embeddings
in the source domain are weakly rank correlated with PRT. The consistency losses in the case of
interpolation are, in most cases, strongly rank correlated with PRT. The consistency losses in the case
of extrapolation lie in between and are generally moderately rank correlated with PRT. This supports
our thesis that consistency can facilitate reliable transfer. Furthermore, consistency of certain partial
theories may matter more. Here, Con-A, Con-I-T and Con-I-A on interpolation are the most relevant
partial theories for transfer learning performance. As we shall see, DC outperforms all other models
on these losses and this result is mirrored by its PRT performance."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5384615384615384,"To gain a deeper insight as to which underlying characteristics can explain the observed PRT accuracy
proﬁles, Figure 3 and Figure 4 present Con-A and Con-I loss proﬁles, respectively, for varied β
and regions of latent space (for data-embeddings in blue, interpolation in green and extrapolation in"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5427350427350427,"7We take φr inferences of 0.5 or above to signify true, and otherwise false. An alternative, left as future
work, would be to sample the space of φ values to produce a conﬁdence measure."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5470085470085471,"Figure 3: Consistency-Across (Con-A) losses (lower values are better) for the models (DC, NN,
NTN, HolE, TransR) using the MNIST data set (source domain X) [left] and BlockStacks (target
domain Y) [right]. The blue bars show the consistency loss of the data embeddings, with darker
shades corresponding to models trained with higher β (disentanglement pressure). The green bars
show the results for interpolation. The red bars show the results for extrapolation."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5512820512820513,"Figure 4: Consistency-Individual (Con-I) losses (lower values are better) for the models (DC, NN,
NTN, HolE, TransR) using the MNIST data set (source domain X) [left] and BlockStacks (target
domain Y) [right]. From top to bottom (following the same colour schematic as Figure 3): Con-I-T
(Transitivity), Con-I-A (Asymmetry) and Con-I-R (Reﬂexivity)."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5555555555555556,"red). Results refer to both source (left) and target domain embeddings (right). Firstly, we note that
DC retains excellent Con-A for all regions of latent space. TransR retains consistency from data-
embeddings to the interpolated regions, but not to the extrapolated regions. The remaining models
show degradation of consistency between data-embeddings and interpolation and extrapolation
regions, with extrapolation often being worse than interpolation. Looking at β trends, aside from
DC, increasing β appears to have a positive but limited effect on interpolation and extrapolation
performance. Considering the Con-A performance of data-embeddings in the target domain, DC
shows the best performance. The Con-A performance in the target domain is in agreement with the
PRT accuracies. For all the models, Con-A performance in the target domain appears to match the
interpolation or extrapolation Con-A performance in the source domain. This points to the possibility
of anticipating transfer learning performance by evaluating the consistency of partial theories."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5598290598290598,"Many of the same trends can be seen in the results for Con-I (Con-I-T, Con-I-A and Con-I-R) in
Figure 4. Results are averaged over individual relations. As in Figure 3, results are presented with
respect to source domain (left) and target domain (right).We ﬁrstly observe that DC and NN share
the best overall Con-I performance proﬁles, with TransR following closely. DC and TransR again
show comparable data-embedding versus interpolation/extrapolation performance, whereas NN, NTN
and HolE suffer from degradation. With regards to β’s impact, DC is not affected by β, while NN
and NTN show a negative correlation between β and Con-I losses with comparable results for each
underlying partial theory."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5641025641025641,"Finally, Table 1 provides a comparison between optimal coherences achieved for each relation-
decoder model, as deﬁned in Section 4. Results are partitioned according to each consistency type
and aggregate value. DC clearly outperforms all other models on coherence. NN achieves strong
aggregate coherence, followed by TransR and NTN, with HolE performing generally worse. Looking
at β⇤proﬁles, we see that most models achieve optimum aggregate coherence at β = 8, apart from
DC and HolE which perform better at β = 1. Overall, this is in broad agreement with the β proﬁles
given by Figure 2-bottom (right). However, we can see that β⇤proﬁles for Con-A coherence are in
more direct agreement as TransR achieves its best at β = 12 and HolE at β = 8. This suggests that"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5683760683760684,"Table 1: Coherence comparison with respect to source and target data-embeddings. Results are
reported with the corresponding β = β⇤value (in parenthesis). The consistency loss abbreviations
refer to: (A)cross, T(ransitivity), A(symmetry), R(eﬂexivity) and Aggr(egate), which gives the best
obtained aggregate consistencies. DC outperforms all other approaches across most coherence scores."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5726495726495726,"φ
Aggr.
(β⇤)
Con-A
(β⇤)
Con-I-T
(β⇤)
Con-I-A
(β⇤)
Con-I-R
(β⇤)"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5769230769230769,"HolE
12.12
(1)
6.61
(8)
4.30
(1)
0.52
(12)
0.08
(8)
NTN
4.11
(8)
1.92
(8)
1.50
(12)
0.22
(12)
0.09
(12)
TransR
2.51
(8)
1.02
(12)
0.71
(12)
0.18
(4)
0.55
(8)
NN
1.71
(8)
0.82
(8)
0.44
(12)
0.18
(4)
0.05
(4)"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5811965811965812,"DC
0.53
(1)
0.29
(1)
0.11
(1)
0.04
(1)
0.09
(1)"
EXPERIMENTAL RESULTS AND DISCUSSION,0.5854700854700855,"Con-A is more indicative of PRT performance, which is to be expected since PRT relies on inductive
transfer across relations."
EXPERIMENTAL RESULTS AND DISCUSSION,0.5897435897435898,"All in all, these results paint a picture where source domain accuracy alone is not a strong enough
indicator of concept transfer. Instead, it may be possible to anticipate transfer performance by
evaluating consistency in regions beyond the source domain’s data-embeddings. Depending on the
task at hand, certain partial theories may be more relevant than others in this analysis."
RELATED WORK,0.594017094017094,"8
Related Work"
RELATED WORK,0.5982905982905983,"Relational representations play a prominent role in Knowledge Graph Embedding (KGE), wherein
sets of relation-decoders are jointly learned to obtain a semantic latent representation from data
[41, 45, 44, 5, 29, 47, 11, 20, 1, 38, 13, 3]. Although KGE approaches typically do not use a shared
autoencoder as done in this paper, in [37] an autoencoding framework is adopted, where a graph neural
network is used as the encoder. However, [37] did not work with visual data and the model was only
applied to single data sets rather than transfer learning. Similarly, disentanglement is concerned with
semantic representation learning [4], and it has been explored using a variety of methods including
both Generative Adversarial Networks [10] and VAEs [6, 17, 9, 36, 14, 22, 26]. Disentangled
representations have been evaluated on their transferability [46, 42, 27]. A bridge between these two
ﬁelds, with relation-decoders employed in the semi-supervision of VAEs, can be found in [19, 8, 7].
In [19], multiple relation-decoders are used, but to compute a triplet comparison-based query. In
[8, 7], only a single binary relation is studied using functional forms that are not sufﬁcient to model
the full set of relations considered in this paper. Lastly, we note that our experimental setup is most
remnant of domain adaptation, e.g. [35]. To the best of our knowledge, this paper is the ﬁrst to
present a comprehensive analysis of the resulting concept coherence. No previous work has compared
relation-decoders on their ability to learn consistently and coherently, as measured in this paper."
CONCLUSION AND FUTURE WORK,0.6025641025641025,"9
Conclusion and Future Work"
CONCLUSION AND FUTURE WORK,0.6068376068376068,"This paper introduced formal deﬁnitions of consistency and coherence for representation learning.
As a result, a sub-symbolic model can have its consistency and coherence measured with respect to a
logical theory. The paper speciﬁed a neuro-symbolic model based on domain-encoders coupled with
modular relation-decoders, and an experimental procedure that, together, allowed for the investigation
of how concept coherence differs for various implementations of relation-decoders applied to transfer
learning. Finally, consistency and coherence results showed that the models that can retain consistency
(i.e. be coherent) across regions of latent space beyond the source data-embeddings are more likely
to perform better at PRT learning tasks. The empirical evaluations in this paper only considered
binary relations and a ﬁxed signature which is learned “all at once” in a source domain. In practical
applications, however, it should be possible to discover concepts gradually, e.g. as part of a curriculum
and through gradual reﬁnement of pre-learned relations after exposure to different contexts. This
necessitates an adaptation of the approach presented here and further evaluations, as part of future
work. Further evaluations of the formalization introduced here should consider the use of different
models, theories (such as specifying periodic, e.g. rotation, and unordered categorical, e.g. shape,
properties) and scenarios/data sets in the evaluation of consistency and coherence of neural models."
REFERENCES,0.6111111111111112,References
REFERENCES,0.6153846153846154,"[1] Ralph Abboud, ˙Ismail ˙Ilkan Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori. Boxe: A"
REFERENCES,0.6196581196581197,"box embedding model for knowledge base completion. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, 2020."
REFERENCES,0.6239316239316239,"[2] Masataro Asai. Photo-Realistic Blocksworld Dataset. arXiv preprint arXiv:1812.01818, 2018."
REFERENCES,0.6282051282051282,"[3] Samy Badreddine, Artur d’Avila Garcez, Luciano Seraﬁni, and Mike Spranger. Logic tensor"
REFERENCES,0.6324786324786325,"networks. Artif. Intell., 303:103649, 2022."
REFERENCES,0.6367521367521367,"[4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and"
REFERENCES,0.6410256410256411,"new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–
1828, 2013."
REFERENCES,0.6452991452991453,"[5] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko."
REFERENCES,0.6495726495726496,"Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems,
pages 2787–2795. Curran Associates, Inc., Lake Tahoe, USA, 2013."
REFERENCES,0.6538461538461539,"[6] Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume"
REFERENCES,0.6581196581196581,"Desjardins, and Alexander Lerchner. Understanding disentangling in β-VAE. In Advances in
Neural Information Processing Systems 30, number Nips, Long Beach, CA, USA, 2017."
REFERENCES,0.6623931623931624,[7] Junxiang Chen and Kayhan Batmanghelich. Robust ordinal VAE: employing noisy pairwise
REFERENCES,0.6666666666666666,"comparisons for disentanglement. CoRR, abs/1910.05898, 2019."
REFERENCES,0.6709401709401709,[8] Junxiang Chen and Kayhan Batmanghelich. Weakly Supervised Disentanglement by Pairwise
REFERENCES,0.6752136752136753,"Similarities. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI, New
York, NY, USA, 2020."
REFERENCES,0.6794871794871795,"[9] Ricky T Q Chen, Xuechen Li, Roger B. Grosse, and David Duvenaud. Isolating Sources of"
REFERENCES,0.6837606837606838,"Disentanglement in Variational Autoencoders. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems, pages 2615—-2625,
Montreal, Quebec, Canada, 2018."
REFERENCES,0.688034188034188,"[10] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:"
REFERENCES,0.6923076923076923,"Interpretable representation learning by information maximizing generative adversarial nets.
In Advances in Neural Information Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2172–
2180, 2016."
REFERENCES,0.6965811965811965,"[11] Yuanfei Dai, Shiping Wang, Neal N Xiong, and Wenzhong Guo. A Survey on Knowledge"
REFERENCES,0.7008547008547008,"Graph Embedding: Approaches, Applications and Benchmarks. Electronics, 9(5):1–29, 2020."
REFERENCES,0.7051282051282052,"[12] Artur d’Avila Garcez and Luís C. Lamb.
Neurosymbolic AI: the 3rd wave.
CoRR,
abs/2012.05876, 2020."
REFERENCES,0.7094017094017094,"[13] Ivan Donadello, Luciano Seraﬁni, and Artur d’Avila Garcez. Logic Tensor Networks for Se-"
REFERENCES,0.7136752136752137,"mantic Image Interpretation. In Proceedings of the Twenty-Sixth International Joint Conference
on Artiﬁcial Intelligence, pages 1596—-1602, 2017."
REFERENCES,0.717948717948718,[14] Cian Eastwood and Christopher K I Williams. A framework for the quantitative evaluation of
REFERENCES,0.7222222222222222,"disentangled representations. In 6th International Conference on Learning Representations,
Vancouver, BC, Canada, 2018."
REFERENCES,0.7264957264957265,[15] Víctor Gutiérrez-Basulto and Steven Schockaert. From knowledge graph embedding to ontology
REFERENCES,0.7307692307692307,"embedding? an analysis of the compatibility between vector space representations and rules.
In Principles of Knowledge Representation and Reasoning: Proceedings of the Sixteenth
International Conference, KR 2018, Tempe, Arizona, 30 October - 2 November 2018, pages
379–388. AAAI Press, 2018."
REFERENCES,0.7350427350427351,"[16] Irina Higgins, David Amos, David Pfau, Sébastien Racanière, Loïc Matthey, Danilo J. Rezende,"
REFERENCES,0.7393162393162394,"and Alexander Lerchner.
Towards a deﬁnition of disentangled representations.
CoRR,
abs/1812.02230, 2018."
REFERENCES,0.7435897435897436,"[17] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,"
REFERENCES,0.7478632478632479,"Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a
Constrained Variational Framework. In 5th International Conference on Learning Representa-
tions, Toulon, France, 2017."
REFERENCES,0.7521367521367521,[18] B. Inhelder and J. Piaget. The early growth of logic in the child: classiﬁcation and seriation.
REFERENCES,0.7564102564102564,"Routledge and Kegan Paul, London, 1964."
REFERENCES,0.7606837606837606,"[19] Theofanis Karaletsos, Serge Belongie, and Gunnar Rätsch. When crowds hold privileges:"
REFERENCES,0.7649572649572649,"Bayesian unsupervised representation learning with oracle constraints. In 4th International
Conference on Learning Representations,, pages 1–16, San Juan, Puerto Rico, 2016."
REFERENCES,0.7692307692307693,[20] Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge
REFERENCES,0.7735042735042735,"graphs. Advances in Neural Information Processing Systems, 2018-December(Nips):4284–4295,
2018."
REFERENCES,0.7777777777777778,[21] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd Interna-
REFERENCES,0.782051282051282,"tional Conference on Learning Representations, 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014."
REFERENCES,0.7863247863247863,"[22] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of"
REFERENCES,0.7905982905982906,"disentangled latent concepts from unlabeled observations. In 6th International Conference on
Learning Representations, Vancouver, BC, Canada, 2018."
REFERENCES,0.7948717948717948,"[23] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building"
REFERENCES,0.7991452991452992,"Machines That Learn and Think Like People. Behavioral and Brain Sciences, 40, 2017."
REFERENCES,0.8034188034188035,[24] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
REFERENCES,0.8076923076923077,"[25] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and"
REFERENCES,0.811965811965812,"relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth
AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA, pages"
REFERENCES,0.8162393162393162,"2181–2187. AAAI Press, 2015."
REFERENCES,0.8205128205128205,"[26] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard"
REFERENCES,0.8247863247863247,"Schoelkopf, and Olivier Bachem. Challenging Common Assumptions in the Unsupervised
Learning of Disentangled Representations. In Proceedings of the 36th International Conference
on Machine Learning,{ICML}, pages 4114—-4124, Long Beach, California, USA, 2019."
REFERENCES,0.8290598290598291,"[27] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and"
REFERENCES,0.8333333333333334,"Michael Tschannen. Weakly-Supervised Disentanglement Without Compromises. CoRR,
abs/2002.0, 2020."
REFERENCES,0.8376068376068376,"[28] Vinod Nair and Geoffrey E. Hinton.
Rectiﬁed linear units improve restricted boltzmann
machines. In Proceedings of the 27th International Conference on Machine Learning ICML,
pages 807–814, 2010."
REFERENCES,0.8418803418803419,"[29] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of"
REFERENCES,0.8461538461538461,"relational machine learning for knowledge graphs. Proc. IEEE, 104(1):11–33, 2016."
REFERENCES,0.8504273504273504,"[30] Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of"
REFERENCES,0.8547008547008547,"knowledge graphs. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
February 12-17, 2016, Phoenix, Arizona, USA, pages 1955–1961. AAAI Press, 2016."
REFERENCES,0.8589743589743589,"[31] Maxwell I. Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improv-"
REFERENCES,0.8632478632478633,"ing coherence and consistency in neural sequence models with dual-system, neuro-symbolic
reasoning. In Proc. NeurIPS 2021, abs/2107.02794, 2021."
REFERENCES,0.8675213675213675,[32] Jeffrey Paris and Alena Vencovská. Pure Inductive Logic. Perspectives in Logic. Cambridge
REFERENCES,0.8717948717948718,"University Press, 2015."
REFERENCES,0.8760683760683761,[33] Jeffrey B. Paris. The Uncertain Reasoner’s Companion: A Mathematical Perspective. Cam-
REFERENCES,0.8803418803418803,"bridge University Press, 1994."
REFERENCES,0.8846153846153846,"[34] Jean Piaget. The Psychology of Intelligence. Routledge and Kegan Paul, 2005."
REFERENCES,0.8888888888888888,"[35] Ievgen Redko, Amaury Habrard, Emilie Morvant, Marc Sebban, and Younès Bennani. Advances"
REFERENCES,0.8931623931623932,"in Domain Adaptation Theory. Elsevier, 2019."
REFERENCES,0.8974358974358975,[36] Karl Ridgeway and Michael C Mozer. Learning Deep Disentangled Embeddings With the
REFERENCES,0.9017094017094017,"F-Statistic Loss. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems, pages 185—-194, Montreal, Quebec, Canada, 2018."
REFERENCES,0.905982905982906,"[37] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and"
REFERENCES,0.9102564102564102,"Max Welling. Modeling relational data with graph convolutional networks. 10843:593–607,
2018."
REFERENCES,0.9145299145299145,[38] Luciano Seraﬁni and Artur d’Avila Garcez. Logic Tensor Networks: Deep learning and logical
REFERENCES,0.9188034188034188,"reasoning from data and knowledge. ArXiv, abs/1606.04422, 2016."
REFERENCES,0.9230769230769231,[39] Stewart Shapiro and Teresa Kouri Kissel. Classical Logic. In The Stanford Encyclopedia of
REFERENCES,0.9273504273504274,"Philosophy. Metaphysics Research Lab, Stanford University, Spring 2021 edition, 2021."
REFERENCES,0.9316239316239316,[40] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
REFERENCES,0.9358974358974359,"information. CoRR, abs/1703.00810, 2017."
REFERENCES,0.9401709401709402,"[41] Richard Socher, Danqi Chen, Christopher Manning, Danqi Chen, and Andrew Ng. Reason-"
REFERENCES,0.9444444444444444,"ing With Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural
Information Processing Systems 26: 27th Annual Conference on Neural Information Processing
Systems, pages 926–934, 2013."
REFERENCES,0.9487179487179487,"[42] Xander Steenbrugge, Sam Leroux, Tim Verbelen, and Bart Dhoedt. Improving Generalization"
REFERENCES,0.9529914529914529,"for Abstract Reasoning Tasks Using Disentangled Feature Representations. In Neural Informa-
tion Processing Systems (NeurIPS) Workshop on Relational Representation Learning, Montreal,
Canada, 2018."
REFERENCES,0.9572649572649573,"[43] Harald Strömfelt, Luke Dickens, Artur d’Avila Garcez, and Alessandra Russo. Coherent and"
REFERENCES,0.9615384615384616,"consistent relational transfer learning with auto-encoders. In Artur d’Avila Garcez and Ernesto
Jiménez-Ruiz, editors, Proceedings of the 15th International Workshop on Neural-Symbolic
Learning and Reasoning as part of the 1st International Joint Conference on Learning &
Reasoning (IJCLR 2021), Virtual conference, October 25-27, 2021, volume 2986 of CEUR
Workshop Proceedings, pages 176–192. CEUR-WS.org, 2021."
REFERENCES,0.9658119658119658,"[44] Théo Trouillon, Éric Gaussier, Christopher R. Dance, and Guillaume Bouchard. On inductive"
REFERENCES,0.9700854700854701,"abilities of latent factor models for relational learning. Journal of Artiﬁcial Intelligence Research,
64:21–53, 2019."
REFERENCES,0.9743589743589743,"[45] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard."
REFERENCES,0.9786324786324786,"Complex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International
Conference on Machine Learning, pages 2071–2080, New York, NY, USA, 2016."
REFERENCES,0.9829059829059829,"[46] Sjoerd van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, and Olivier Bachem. Are"
REFERENCES,0.9871794871794872,"Disentangled Representations Helpful for Abstract Visual Reasoning? In Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems, pages 14222—-14235, Vancouver, BC, Canada, 2019."
REFERENCES,0.9914529914529915,"[47] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey"
REFERENCES,0.9957264957264957,"of approaches and applications. IEEE Transactions on Knowledge and Data Engineering,
29(12):2724—-2743, 2017."
