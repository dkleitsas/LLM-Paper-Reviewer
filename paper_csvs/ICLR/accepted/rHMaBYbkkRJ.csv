Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027397260273972603,"What is the state of the art in continual machine learning? Although a natural
question for predominant static benchmarks, the notion to train systems in a life-
long manner entails a plethora of additional challenges with respect to set-up and
evaluation. The latter have recently sparked a growing amount of critiques on
prominent algorithm-centric perspectives and evaluation protocols being too nar-
row, resulting in several attempts at constructing guidelines in favor of specific
desiderata or arguing against the validity of prevalent assumptions. In this work,
we depart from this mindset and argue that the goal of a precise formulation of
desiderata is an ill-posed one, as diverse applications may always warrant distinct
scenarios. Instead, we introduce the Continual Learning EValuation Assessment
Compass: the CLEVA-Compass. The compass provides the visual means to both
identify how approaches are practically reported and how works can simultane-
ously be contextualized in the broader literature landscape. In addition to promot-
ing compact specification in the spirit of recent replication trends, it thus provides
an intuitive chart to understand the priorities of individual systems, where they
resemble each other, and what elements are missing towards a fair comparison."
INTRODUCTION,0.005479452054794521,"1
INTRODUCTION"
INTRODUCTION,0.00821917808219178,"Despite the indisputable successes of machine learning, recent concerns have surfaced over the field
heading towards a potential reproducibility crisis (Henderson et al., 2018), as previously identified
in other scientific disciplines (Baker, 2016). Although replication may largely be assured through
modern software tools, moving beyond pure replication towards reproducibility with a factual inter-
pretation of results is accompanied by tremendous remaining challenges. Specifically for machine
learning, recent reproducibility initiatives (Pineau et al., 2021) nicely summarize how differences
in used data, miss- or under-specification of training and evaluation metrics, along with frequent
over-claims of conclusions beyond gathered empirical evidence impose persisting obstacles in our
current literature. Similar conclusions have been reached in related works focused on specifics of re-
inforcement learning (Li & Talwalkar, 2019), neural architecture search (Lindauer & Hutter, 2020),
human-centered machine learning model cards (Mitchell et al., 2019), or general dataset sheet spec-
ifications (Bender & Friedman, 2018; Gebru et al., 2018), which all make valuable propositions to
overcome existing gaps through the creation of standardized best-practice (check-)lists."
INTRODUCTION,0.010958904109589041,"It should thus come as no surprise that the emerging work in continual learning is no stranger to the
above challenges. Superficially, continual learning has the intuitive objective of accumulating infor-
mation and learning concepts over time, typically without the ability to revisit previous experiences,
frequently also referred to as lifelong learning (Chen & Liu, 2018). However, there is no unique
agreed-upon formal definition beyond the idea to continuously observe data, where the time compo-
nent holds some practical implication on changes in the objective, the evolution of concept labels,
or general statistical shifts in the data distribution. The majority of modern surveys ambiguously
conflate these factors as a sequence of tasks (Parisi et al., 2019; Lesort et al., 2019; Hadsell et al.,
2020; Lesort et al., 2020; Biesialska et al., 2021). Much in contrast to prevalent static benchmarks,"
INTRODUCTION,0.0136986301369863,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01643835616438356,"2000
2005
2010
2015
2020
Year 0 5000 10000 15000 20000"
INTRODUCTION,0.019178082191780823,Number of publications
INTRODUCTION,0.021917808219178082,"open world
active
curriculum
zero-shot
few-shot
multi-task
domain adapt.
transfer
continual
lifelong"
INTRODUCTION,0.024657534246575342,"2000
2005
2010
2015
2020
Year 0 200 400 600"
INTRODUCTION,0.0273972602739726,Number of publications
INTRODUCTION,0.030136986301369864,"continual
lifelong"
INTRODUCTION,0.03287671232876712,"Figure 1: Per year machine learning publications. Left: cumulative amount across keywords with
continuous components that influence continual learning practice, see Section 2. Right: increasing
use of “continual”, demonstrating a shift from the preceding emphasis on “lifelong”. Data queried
using Microsoft Academic Graph (Sinha et al., 2015), based on keyword occurrence in the abstract."
INTRODUCTION,0.03561643835616438,"the question of reproducibility, interpretation of results, and overall comparability now becomes an
even more complex function of nuances in employed data, training and evaluation protocols."
INTRODUCTION,0.038356164383561646,"The latter fact has sparked various attempts at consolidation or critique. On the one hand, several
works have made important suggestions for continual learning desiderata with respect to evaluation
protocols (Farquhar & Gal, 2018; D´ıaz-Rodr´ıguez et al., 2018; Kemker et al., 2018) and catego-
rization of incremental training set-ups (van de Ven & Tolias, 2019; Lesort et al., 2021). On the
other hand, empirical assessments have demonstrated that performance and comparability break
down rapidly if often unexposed protocol aspects deviate (Pf¨ulb & Gepperth, 2019; Delange et al.,
2021). Following the broader exposition of the recent reviews of Mundt et al. (2020a) and Delange
et al. (2021), evaluation becomes convoluted because intricate combinations of elements originating
from various related machine learning paradigms affect continual learning practice. As the number
of publications across these paradigms increases, see Figure 1, reproducibility, comparability, and
interpretation of results thus also become increasingly difficult."
INTRODUCTION,0.0410958904109589,"In this work, we follow in the footsteps of previous reproducibility works to promote transparency
and comparability of reported results for the non-trivial continual learning case. Rather than adding
to the ongoing discussions on desiderata or violation of assumptions, we posit that the development
of distinct applications warrants the existence of numerous continual scenarios. Based on respec-
tively highlighted evaluation nuances and their implications when absorbed into continual learn-
ing, we derive the Continual Learning EValuation Assessment (CLEVA) Compass. The CLEVA-
Compass provides a compact visual representation with a unique two-fold function: 1. it presents
an intuitive chart to identify a work’s priorities and context in the broader literature landscape, 2. it
enables a direct way to determine how methods differ in terms of practically reported metrics, where
they resemble each other, or what elements would be missing towards a fairer comparison."
INTRODUCTION,0.043835616438356165,"In the remainder of the paper, we start by sketching the scope of continual learning in Section 2, first
by outlining the differences when going from static benchmarks to continual learning, followed by
an exposition of evaluation nuances emanating from related machine learning paradigms. In Section
3, we then proceed to introduce the CLEVA-Compass and illustrate its necessity and utility at the
hand of several continual learning works. Before concluding, we summarize auxiliary best-practices
proposed in related prior works and finally discuss limitations and unintended use of the CLEVA-
Compass. To encourage general adoption, both for prospective authors to add methods and for
application oriented practitioners to identify suitable methods, we supply various utilities: a template
for direct inclusion into LaTeX, a Python script, and the CLEVA-Compass Graphical User Interface
(GUI), together with a repository to aggregate methods’ compasses, all detailed in Appendix C and
publicly available at https://github.com/ml-research/CLEVA-Compass."
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.04657534246575343,"2
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.049315068493150684,"As already argued, there is no unique agreed-upon formal definition of continual learning. One of
the few common denominators across the continual machine learning literature is the understanding"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.052054794520547946,Published as a conference paper at ICLR 2022
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.0547945205479452,"(Continual)
Machine Learning
Workflow"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.057534246575342465,"Data:
amount, redundancy vs.
diversity,"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.06027397260273973,"cleaning, preprocessing"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.06301369863013699,"data selection and ordering, task similarity,
noisy streams, distribution shifts"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.06575342465753424,"Model:
architecture, inductive bias, dis-"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.0684931506849315,"criminative/generative, functions, parameters"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.07123287671232877,"model extensions, task-specific parameter
identification"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.07397260273972603,"Training:
loss function, optimizer, hyper-"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.07671232876712329,"parameters, convergence"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.07945205479452055,"catastrophic forgetting, knowledge transfer
or distillation, selective updates, online"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.0821917808219178,"Deployment:
model saving, platform"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.08493150684931507,"compatibility, serving and cloud"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.08767123287671233,"optimizer states and meta-data, distributing
continuous updates, communication cost"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.09041095890410959,"Prediction:
test set evaluation, failure"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.09315068493150686,modes and robustness
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.0958904109589041,"evolving test set, inherent noise and
perturbations, open world scenario"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.09863013698630137,"Versioning:
stage versions according to"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.10136986301369863,prediction evaluation and deployment
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.10410958904109589,"discretized vs.
continuous versions, back-
ward compatibility Pr e p ar e D at a M a n a g e V er si o ns M o ni to r Pr e di ct io ns D e pl oy M o d el Tr ai n + T u n e M o d el C o d e M L M o d el"
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.10684931506849316,"Figure 2: A typical (continual) machine learning workflow. The inner circle depicts the workflow
advocated by Google Cloud (2021). On the outer circle we have added important, non-exhaustive,
aspects to consider from the prevalent static benchmarking perspective (gray boxes) vs. additional
criteria to be taken into account under dynamic evolution in a continual approach (orange boxes)."
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.1095890410958904,"that catastrophic interference imposes a persisting threat to training models continually over time
(McCloskey & Cohen, 1989; Ratcliff, 1990). That is, continuously employing stochastic optimiza-
tion algorithms to train models on sequential (sub-)sets of varying data instances, without being
able to constantly revisit older experiences, comes with the challenge of previously learned param-
eters being overwritten. As this undesired phenomenon is in stark contrast with accumulation of
knowledge over time, preventing model forgetting is thus typically placed at the center of continual
learning. Consequently, the focus has been placed on a model-centric perspective, e.g. by proposing
algorithms to alleviate forgetting through rehearsal of data prototypes in training, employing various
forms of generative replay, imposing parameter and loss penalties over time, isolating task-specific
parameters, regularizing the entire model’s functional or treating it from a perspective of dynami-
cally adjustable capacity. We defer to review papers for taxonomies and details of algorithms (Parisi
et al., 2019; Lesort et al., 2019; Hadsell et al., 2020; Lesort et al., 2020; Biesialska et al., 2021)."
THE SCOPE AND CHALLENGES OF CONTINUAL LEARNING EVALUATION,0.11232876712328767,"In practice, however, alleviating forgetting is but one of a myriad of challenges in real-world formu-
lations of continual learning. Several orthogonal questions inevitably emerge, which either receive
less attention across literature assessment or are frequently not made sufficiently explicit. A fair
assessment and factual interpretation of results is rendered increasingly difficult. To provide the
necessary background behind the statement, we briefly discuss newly arising questions when shift-
ing from a static benchmark to a continual perspective and then proceed to contextualize conceivable
evaluation protocol nuances in anticipation of our CLEVA-Compass."
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW,0.11506849315068493,"2.1
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW"
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW,0.1178082191780822,"To highlight the additional challenges in continual learning consider our visualization in Figure 2,
depicting the benchmark inspired machine learning workflow as advocated by Google Cloud (2021).
In the center, we find the six well-known sequential steps going from the preparation of data, to de-
signing and tuning our ML model, down to the deployment of a model version to use for prediction.
Naturally, these steps already contain various non-trivial questions, some of which we have high-
lighted in the surrounding gray boxes of the diagram. When considering popular benchmarks such
as ImageNet (Deng et al., 2009), a considerable amount of effort has been made for each individ-
ual workflow step. For instance, assembling, cleaning and pre-processing the dataset has required
substantial resources, a decade of work has been attributed to the design of models and their opti-
mization algorithms, and plenty of solutions have been developed to facilitate efficient computation"
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW,0.12054794520547946,Published as a conference paper at ICLR 2022
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW,0.1232876712328767,"or deployment. It is commonplace to treat these aspects in isolation in the literature. In other words,
it is typical for approaches to be validated within train-val-test splits, where either a model-centric
approach investigates optimizer variants and new model architectures, or alternatively, a data-centric
approach analyzes how algorithms for data curation or selection can be improved for given models."
FROM STATIC TO CONTINUAL MACHINE LEARNING WORKFLOW,0.12602739726027398,"Much in contrast to any of the prevalent static benchmarks, establishing a similar benchmark-driven
way of conducting research becomes genuinely difficult for continual learning. Instinctively, this is
because already partially intertwined elements of the workflow now become inherently codependent
and inseparable. Once more, we provide a non-exhaustive list of additional questions in the orange
boxes in the diagram of Figure 2. Here, the boundaries between the steps are now blurred. To give
a few examples: train and test sets evolve over time, we need to repeatedly determine what data to
include next, an ongoing stream of data may be noisy or contain unknown elements, models might
require new inductive biases and need to be extended, acquired knowledge needs to be protected but
should also aid in future learning, and deployment and versions become continuous. In turn, setting
a specific research focus on one of these aspects or attributing increased importance to only a portion
allows for an abundance of conceivable, yet incomparable, implementations and investigations, even
when the overall goal of continual learning is shared on the surface."
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.12876712328767123,"2.2
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.13150684931506848,"Although a full approach to continual learning should ideally include all of the underlying aspects
illustrated in Figure 2, many of these factors have been subject to prior isolated treatment in the lit-
erature. We posit that these related machine learning paradigms have a fundamental and historically
grown influence on present “continual learning” practice, as they are in themselves comprised of
components that are continuous. More specifically, we believe that choices in continual learning can
largely be mapped back onto various related paradigms from which continual scenarios have drawn
inspiration: multi-task learning (Caruana, 1997), transfer learning and domain adaptation (Pan &
Yang, 2010), few-shot learning (Fink, 2005; Fei-Fei et al., 2006), curriculum learning (Bengio et al.,
2009), active learning (Settles, 2009), open world learning (Bendale & Boult, 2015), online learning
(Heskes & Kappen, 1993; Bottou, 1999), federated learning (McMahan et al., 2017; Kairouz et al.,
2021), and meta-learning (Thrun & Pratt, 1998). We capture the relationship with respect to set-up
and evaluation between these related paradigms and continual learning in the diagram of Figure 3.
For convenience, we have added quotes of the paradigm literature definitions in the figure. On the
arrows of the diagram, we indicate the main evaluation difference and respectively how paradigms
can be connected to each other."
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.13424657534246576,"As each paradigm comes with its own set of assumptions towards training and evaluation proto-
cols, it becomes apparent why the quest for a strict set of desiderata can be considered as ill-posed
for the continual learning hypernym. In the next section, we thus introduce the CLEVA-Compass,
as an alternative that emphasizes transparency and comparability over strict desiderata. To further
clarify this with an easy example, let us consider one of the most popular ways to construct pro-
tocols in the academic continual learning literature. Here, a continual investigation is set up by
defining a sequence based on extracting splits of existing benchmark datasets and introducing them
sequentially (Lesort et al., 2020; Biesialska et al., 2021; Delange et al., 2021). Such a scenario
generally consist of individually introduced classes in image datasets like ImageNet (Deng et al.,
2009), MNIST (LeCun et al., 1998), CIFAR (Krizhevsky, 2009), Core50 (Lomonaco & Maltoni,
2017), learning unique sounds (Gemmeke et al., 2017) or skills (Mandlekar et al., 2018; Fan et al.,
2018) in sequence, or simply creating a chain across multiple datasets for natural language process-
ing (McCann et al., 2018; Wang et al., 2019a;b) and games in reinforcement learning (Bellemare
et al., 2013). Arguably, such a set-up is immediately derived from conventional transfer learning
practice. Following the description of Pan & Yang (2010), the distinction between transfer and con-
tinual learning can essentially be brought down to the fact that both consider more than one task in
sequence, but transfer learning focuses solely on leveraging prior knowledge to improve the new tar-
get task, typically a unique dataset or a set of classes, whereas continual learning generally intends
to maximize performance on both prior and new tasks."
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.136986301369863,"Even though these popular continual benchmarks already simplify the continuous data perspective
to seemingly enable comparison akin to static benchmarks, there already persists an unfortunate
amount of training, evaluation, and result interpretation ambiguity. For instance, Farquhar & Gal
(2018); Pf¨ulb & Gepperth (2019) argue that simply the knowledge of whether and when a new task"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.13972602739726028,Published as a conference paper at ICLR 2022
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.14246575342465753,"CONTINUAL
LEARNING"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.14520547945205478,"Transfer
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.14794520547945206,"Domain
Adaptation"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1506849315068493,"Few-shot
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.15342465753424658,"Curriculum
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.15616438356164383,"Active
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1589041095890411,"Open World
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.16164383561643836,"Multi-task
learning unk no wn or pe rtu rbe d real wo rld da ta is pre sent ac tiv el y c ho os e d ata in sta nc es to in cl ud e f or ea ch ta sk"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1643835616438356,"assign instances a
difficulty measure"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.16712328767123288,multiple tasks are available at
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.16986301369863013,the same time in parallel
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1726027397260274,limited number of target
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.17534246575342466,examples available
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1780821917808219,difference in data distributions
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.18082191780821918,tasks remain the same
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.18356164383561643,maximize performance
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1863013698630137,on all sequential tasks
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.18904109589041096,choose data instances
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.1917808219178082,to include continuously
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.19452054794520549,learn tasks continuously
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.19726027397260273,in presence of unknown
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2,introduce multiple tasks
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.20273972602739726,per step in sequence
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2054794520547945,learn sequential tasks
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.20821917808219179,as a function of difficulty
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.21095890410958903,"Federated
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2136986301369863,"Online
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.21643835616438356,"Meta
Learning"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2191780821917808,"Boult et al. (2019): “An effective
open world recognition system must
efficiently perform four tasks:
detect unknowns, choose which
points to label for addition to the
model, label the points, and update
the model.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2219178082191781,"Settles (2009): “The key hypothesis
in active learning (sometimes
called “query learning” or
“optimal experimental design” in
the statistics literature) is that if the
learning algorithm is allowed to
choose the data from which it learns
- to be “curious”, if you will - it will
perform better with less training.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.22465753424657534,"Hacohen & Weinshall (2019):
“deals with the question of how to
use prior knowledge about the
difficulty of the training examples,
in order to sample each mini-batch
non-uniformly and thus boost the
rate of learning and the accuracy.
It is based on the intuition that it
helps the learning process when the
learner is presented with simple
concepts first. ”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2273972602739726,"Caruana (1997): “is an inductive
transfer mechanism whose principle
goal is to improve generalization
performance by leveraging the
domain-specific information
contained in the training signals of
related tasks. It does this by
training tasks in parallel while
using a shared representation.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.23013698630136986,"Pan & Yang (2010): “A domain D
consists of two components: a
feature space X and a marginal
probability distribution P(X),
where X = {x1, . . . , xn} ∈X.
Given a source domain DS and
learning task TS, a target domain
DT and learning task TT , transfer
learning aims to help improve
learning of the target predictive
function fT () in DT using the
knowledge in DS and TS, where
DS ̸= DT , or TS ̸= TT . ”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2328767123287671,"Pan & Yang (2010): “Given a
source domain DS and a
corresponding learning task TS, a
target domain DT and a
corresponding learning task TT ,
transductive transfer learning aims
to improve the learning of the target
prediction function fT () in DT
using the knowledge in DS and TS,
where DS ̸= DT and TS = TT .”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2356164383561644,"Wang et al. (2020): “is a type of
machine learning problem
(specified by experience E, task T
and performance measure P),
where E contains only a limited
number of examples with
supervised information for the
target T. Methods make the learning
of target T feasible by combining
the available information in E with
some prior knowledge.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.23835616438356164,"Chen & Liu (2018): “is a learning
paradigm where the training data
points arrive in a sequential order.
When a new data point arrives, the
existing model is quickly updated to
produce the best model so far.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2410958904109589,"McMahan et al. (2017): “leaves the
training data distributed on devices,
and learns a shared model by
aggregating locally-computed
updates. We term this decentralized
approach Federated Learning.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.24383561643835616,"Hospedales et al. (2021): “is most
commonly understood as learning
to learn. During base learning, an
inner learning algorithm solves a
task, defined by a dataset and
objective. During meta-learning, an
outer algorithm updates the inner
learning algorithm such that the
model improves an outer objective.”"
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2465753424657534,"Figure 3: Relationships between related machine learning paradigms with continuous components.
Popular literature definitions are provided in adjacent boxes. Arrows indicate the nuances when
including perspectives into one another, showcasing the plethora of potential influences in construc-
tion of continual learning approaches. Outer nodes for federated, online and meta-learning can be
combined and tied with all other paradigms. Note that only one arrow between nodes is drawn for
visual clarity. Respectively highlighted distinctions can be constructed in mirrored directions."
EVALUATION IN THE CONTEXT OF RELATED MACHINE LEARNING PARADIGMS,0.2493150684931507,"is introduced already yields entirely different results, Mai et al. (2021) consider an arbitrary amount
of revisits of older data and infinite time to converge on a subset as unrealistic, and Hendrycks & Di-
etterich (2019) contend that the presence of unknown noisy data can break the system altogether and
should actively be discounted. Similar to the above inspiration from transfer learning, these works
could now again be attributed to drawing inspiration from online and open world learning. In this
context, online learning (Heskes & Kappen, 1993; Bottou, 1999) attempts to optimize performance
under the assumption that each data element is only observed once, which can again be directly
applied to “continual learning” (Mai et al., 2021). Likewise, open world learning (Bendale & Boult,
2015) allows for unknown data to be present in training and evaluation phases, which needs to be
identified. Naturally, the latter can also be included seamlessly into continual learning (Mundt et al.,
2020a). The list can be continued by iterating through all conceivable combinations of paradigms,
illustrating the necessity for an intuitive way to make the intricate nuances transparent."
THE CLEVA-COMPASS,0.25205479452054796,"3
THE CLEVA-COMPASS"
THE CLEVA-COMPASS,0.2547945205479452,"The subtleties of the relationships in Figure 3 provide a strong indication that it is perhaps less
important to lay out desiderata for continual learning. However, we concur with prior works (Gebru
et al., 2018; Mitchell et al., 2019; Pineau et al., 2021) that it is of utmost importance to have a clear"
THE CLEVA-COMPASS,0.25753424657534246,Published as a conference paper at ICLR 2022
THE CLEVA-COMPASS,0.2602739726027397,Multiple Models
THE CLEVA-COMPASS,0.26301369863013696,Federated
THE CLEVA-COMPASS,0.26575342465753427,Online
THE CLEVA-COMPASS,0.2684931506849315,Open World
THE CLEVA-COMPASS,0.27123287671232876,Multiple Modalities
THE CLEVA-COMPASS,0.273972602739726,Active Data Query
THE CLEVA-COMPASS,0.27671232876712326,Task Order Discovery
THE CLEVA-COMPASS,0.27945205479452057,Task Agnostic
THE CLEVA-COMPASS,0.2821917808219178,Episodic Memory
THE CLEVA-COMPASS,0.28493150684931506,Generative
THE CLEVA-COMPASS,0.2876712328767123,Uncertainty Co mp ute tim e MA C op era tio ns Co mm un ica tio n Fo rg ett in g Fo rw ard tra nsf er Ba ck wa rd tra nsf er Op en ne ss Pa ra me ter s Me mo ry Sto re d d ata Ge ne rat ed da ta Op tim iza tio n s te ps Pe r t as k me tri cs Ta sk or de r Da ta pe r t as k
THE CLEVA-COMPASS,0.29041095890410956,"OSAKA (Caccia et al., 2020)
FedWeIT (Yoon et al., 2021)
A-GEM (Chaudhry et al., 2019)"
THE CLEVA-COMPASS,0.29315068493150687,"VCL (Nguyen et al., 2018)
OCDVAE (Mundt et al., 2020b;a)"
THE CLEVA-COMPASS,0.2958904109589041,"Figure 4: The Continual Learning EValuation Assessment (CLEVA) Compass. The inner star plot
contextualizes the set-up and evaluation of continual approaches with respect to the influences of
related paradigms, providing a visual indication for comparability. A mark on the star plots’ inner
circle suggests an element being addressed through supervision, whereas a mark on the outer star
plots’ circle displays an unsupervised perspective. The outer level of the CLEVA-Compass allows
specifying which particular measures have been reported in practice, further promoting transparency
in interpretation of results. To provide an intuitive illustration, five distinct methods have been used
to fill the compass. Although these methods may initially tackle the same image datasets, it becomes
clear that variations and nuances in set-up and evaluation render a direct comparison challenging."
THE CLEVA-COMPASS,0.29863013698630136,"exposition of choices and communicate research in a transparent manner. To maintain such a flexible
research perspective, yet prioritize reproducibility and transparency at the same time, we introduce
the Continual Learning EValuation Assessment Compass. The CLEVA-Compass draws inspiration
from evaluations in prior literature across previously detailed related paradigms, in order to provide
intuitive visual means to give readers a sense of comparability and positioning between continual
learning works, while also showcasing a compact chart to promote result reproducibility."
THE CLEVA-COMPASS,0.3013698630136986,"The CLEVA-Compass itself is composed of two central elements. We first detail this composition,
and then use it as a basis for a consecutive discussion on the compass’ necessity and utility at the
hand of five example continual learning methods. To provide a practical basis from the start, we
have directly filled the exemplary compass visualization in Figure 4."
THE CLEVA-COMPASS,0.3041095890410959,"3.1
ELEMENTS OF THE CLEVA-COMPASS: INNER AND OUTER LEVELS"
THE CLEVA-COMPASS,0.30684931506849317,"Figuratively speaking, the inner CLEVA-Compass level maps out the explored and uncharted ter-
ritory in terms of the previously outlined influential paradigms, whereas the outer level provides a
compact way to visually assess which set-up and evaluation details are practically reported."
THE CLEVA-COMPASS,0.3095890410958904,"Inner level: The inner compass level captures the workflow items of Figure 2 and reflects the in-
fluence of Figure 3 on continual learning. That is, for any given method, a visual indication can be
made with respect to whether an approach considers, for instance, the world to be open and contain
unknown instances, data to be queried actively, optimization to be conducted in a federated or on-
line manner, as well as the set-up including multiple modalities or task boundaries being specified.
Warranted by common practice, we further include an indication of whether an episodic memory is
employed, a generative model is trained, and if multiple models are considered as a feasible solution."
THE CLEVA-COMPASS,0.31232876712328766,Published as a conference paper at ICLR 2022
THE CLEVA-COMPASS,0.3150684931506849,"In practice, we propose the use of a star diagram, where we mark the inner circle if a particular
approach follows and is influenced by the relevant axis in a supervised way. Correspondingly, the
outer circle of the star diagram is marked if the respective component is achieved in a more general
form without supervision. It is important to note that this level of supervision is individual to each
element. That is, just because a method has been used for an unsupervised task, does not automat-
ically imply that all marked elements are similarly approached without supervision or vice versa.
To give an example, one could consider continual unsupervised density estimation, where multiple
models could each solve one in a sequence of unsupervised tasks/datasets, but a task classifier is re-
quired to index the appropriate model for prediction. We provide one respective example, including
Figure 4’s chosen methods outlined in the upcoming section, for all inner compass level elements to
further clarify the role of supervision in Appendix B."
THE CLEVA-COMPASS,0.3178082191780822,"We have intentionally chosen the design of the inner compass level as a star diagram. These diagrams
have been argued to provide an ideal visual representation when comparing plot elements, due to
the fact that a drawn shape is enhanced by contours (Fuchs et al., 2014). Shape has been shown to
allow a human perceiver to quickly predict more facts than other visual properties (Palmer, 1999).
In addition, the closed contours of star plots have been discovered to result in faster visual search
(Elder & Zucker, 1993), and in turn, supporting evidence has been found for resulting geometric
region boundaries being prioritized in perception (Elder & Zucker, 1998). This is aligned with the
primary goal of the inner compass level, i.e. exposing the conscious choices and attributed focus
of specific continual learning works, in order for researchers to find a better contextualization and
assess if and when comparison of methods may be sensible."
THE CLEVA-COMPASS,0.32054794520547947,"Outer level: Whereas the compass’ star plot contextualizes the relation to linked paradigms with
respect to comparability of set-up and evaluation protocols, the outer level places emphasis on the
clarity with respect to reproducibility and transparency of specifically reported results. In essence,
we propose to indicate a mark on the outer level for each measure that an approach practically
reports in their empirical investigation. Together, the inner and outer levels thus encourage methods
to be considered in their specific context, and comparisons to be conducted in a fair manner by
promoting disclosure of the same practical assessment. Note that a practical report of additional
measures beyond the required amount for a full overlap is naturally further welcome. We describe
the list of measures on the outer CLEVA-Compass level in Table 1, where we also briefly summarize
the relevance of each metric for continual learning and give credit to the corresponding prior works
which have provided a previous highlight of their respective implications."
THE CLEVA-COMPASS,0.3232876712328767,"3.2
NECESSITY AND UTILITY: CLEVA-COMPASS IN LIGHT OF FIVE EXAMPLES"
THE CLEVA-COMPASS,0.32602739726027397,"To provide the necessary intuition behind above descriptions for inner and outer CLEVA-Compass
levels, we have filled the example illustration in Figure 4 based on five distinct continual learning
methods: OSAKA (Caccia et al., 2020), FedWeIT (Yoon et al., 2021), A-GEM (Chaudhry et al.,
2019), VCL (Nguyen et al., 2018), and OCDVAE (Mundt et al., 2020b;a). We visualize the compass
for these continual vision methods because they have all initially conducted investigations on the
same well-known type of image datasets, such as incrementally introducing classes over time in
MNIST (LeCun et al., 1998) and CIFAR (Krizhevsky, 2009). Inspired by conventional machine
learning practice, we might be tempted to benchmark and judge each approach’s efficacy based on
its ability to e.g. alleviate catastrophic forgetting. Our CLEVA-Compass directly illustrates why
this would be more than insufficient, as each respective approach can be seen to differ drastically in
involved set-up and evaluation protocols. For example, without delving into mathematical details,
it suffices to know that FedWeIT prioritizes a trade-off with respect to communication, parameter
and memory efficiency in a federated setting. In contrast, OSAKA considers a set-up that places
both an emphasis on online updates as well as considering an open world evaluation protocol, where
unknown class instances can be encountered with a given chance. Arguably, such nuances can easily
be missed, especially the more approaches overlap. They are typically not apparent when only
looking at a result table on catastrophic forgetting, but are required as context to assess the meaning
of the actual quantitative results. In other words, the compass highlights the required subtleties, that
may otherwise be challenging to extract from text descriptions, potentially be under-specified, and
prevents readers to misinterpret results out of context in light of prominent result table display."
THE CLEVA-COMPASS,0.3287671232876712,"With above paragraph in mind, we note that a fully filled compass star plot does not necessarily in-
dicate the “best” configuration. This can also best be understood based on the five concrete methods"
THE CLEVA-COMPASS,0.3315068493150685,Published as a conference paper at ICLR 2022
THE CLEVA-COMPASS,0.33424657534246577,"Table 1: Description of conceivable measures on the outer level of the CLEVA-Compass. Further
details and literature suggestions for mathematical definitions are summarized in Appendix A."
THE CLEVA-COMPASS,0.336986301369863,"Measure
Description and relevance for continual learning"
THE CLEVA-COMPASS,0.33972602739726027,"Data per task
What data is introduced sequentially. The number of data instances is a primary indicator
for sample efficiency and provides the context for e.g. few-shot settings (Fink, 2005)."
THE CLEVA-COMPASS,0.3424657534246575,Task order
THE CLEVA-COMPASS,0.3452054794520548,"The order in which tasks are introduced, even if randomly sampled in practice. The order has
a significant impact on obtainable continual performance (Mundt et al., 2020a) depending
on the constructed curriculum (Bengio et al., 2009)."
THE CLEVA-COMPASS,0.34794520547945207,Per task metrics
THE CLEVA-COMPASS,0.3506849315068493,"Task specific parts of reported losses or metrics allow for a deeper assessment of each task’s
evolution over time, e.g. “new” and “base” for first and most recent task, in addition to the
overall average “all” (Kemker et al., 2018)."
THE CLEVA-COMPASS,0.35342465753424657,Optimization steps
THE CLEVA-COMPASS,0.3561643835616438,"The number of optimization steps is crucial to gauge empirical convergence. The number
of optimization steps on revisited data also distinguishes sequences of continual offline and
truly online scenarios (Mai et al., 2021)."
THE CLEVA-COMPASS,0.3589041095890411,"Generated data
Amount of data that is generated, if any. The quality and number of data instances sampled
from a generative model determine the effectiveness of rehearsal (Lesort et al., 2019)"
THE CLEVA-COMPASS,0.36164383561643837,"Stored data
Amount of original data retained in a buffer, if any. Rehearsing instances becomes a trivial
solution the more the buffer approximates the original dataset size (Delange et al., 2021)."
THE CLEVA-COMPASS,0.3643835616438356,Parameters
THE CLEVA-COMPASS,0.36712328767123287,"Amount of overall parameters. A trivial solution to continual learning would be to allocate
increasing amounts of separate and independent parameters over time, motivating a desire
for parameter efficiency (D´ıaz-Rodr´ıguez et al., 2018)."
THE CLEVA-COMPASS,0.3698630136986301,"Memory
How much memory is used. Provides a combined perspective on data storage and model
parameter efficiency (D´ıaz-Rodr´ıguez et al., 2018)."
THE CLEVA-COMPASS,0.3726027397260274,Compute time
THE CLEVA-COMPASS,0.37534246575342467,"Practically used computation time. Different algorithms and operations can consume dra-
matically different compute time, in additional dependence on hardware, even when imple-
mented in the same software (Barham & Isard, 2019)."
THE CLEVA-COMPASS,0.3780821917808219,"MAC operations
Number of multiply-accumulate operations are an alternative to reporting compute require-
ments, in a way that is not inherently tied to specific soft- and hardware (Sze et al., 2017)."
THE CLEVA-COMPASS,0.38082191780821917,Communication
THE CLEVA-COMPASS,0.3835616438356164,"Communication costs start to play a critical role in a distributed or decentralized federated
perspective, where time spent on many rounds of communication can rapidly exceed that of
model computations (McMahan et al., 2017)."
THE CLEVA-COMPASS,0.3863013698630137,Forgetting
THE CLEVA-COMPASS,0.38904109589041097,"The amount of forgetting is a way to quantify the difference between maximum knowledge
gained about the task throughout the learning process in the past and the knowledge that is
currently still held about it (Chaudhry et al., 2018)."
THE CLEVA-COMPASS,0.3917808219178082,"Forward transfer
Forward transfer determines the influence that an observed task has on a future task (Lopez-
Paz & Ranzato, 2017), quantifying the ability for “zero-shot” learning (Fei-Fei et al., 2006)."
THE CLEVA-COMPASS,0.39452054794520547,"Backward transfer
Backward transfer (BWT) captures the improvement or deterioration an already observed
task experiences when learning a new task (Lopez-Paz & Ranzato, 2017)."
THE CLEVA-COMPASS,0.3972602739726027,Openness
THE CLEVA-COMPASS,0.4,"Openness of the world describes the proportion between data points that can be assumed
to originate from the investigated data distribution and potentially unknown, corrupted or
perturbed instances (Scheirer et al., 2013)."
THE CLEVA-COMPASS,0.40273972602739727,"in Figure 4. On the one hand, some aspects, e.g. the use of external episodic memory in A-GEM
(Chaudhry et al., 2019) or existence of multiple models in VCL (Nguyen et al., 2018), could be inter-
preted as either an advantage or disadvantage, which is best left to decide for the respective research
and its interpretation in scope of the underlying pursued applications. On the other hand, a discrep-
ancy between methods in the CLEVA-Compass indicates that these continual learning approaches
should be compared very cautiously and discussed rigorously when taken out of their context, and
that much work would be required to make a practical alignment for these specific cases. As such,
the CLEVA-Compass is not a short-cut to dubious “state-of-the-art” claims, but instead enforces
transparent comparison and discussion of empirical set-ups and results across several factors of im-
portance. In complete analogy, we remark that a filled outer level does not necessarily imply the
most comprehensive way to report. Once more, this is because some practical measures may not
apply to a specific method in a certain application context, e.g. reports of generated/stored data or
communication costs when the method doesn’t rehearse and is not trained in a federated setting."
THE CLEVA-COMPASS,0.4054794520547945,Published as a conference paper at ICLR 2022
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS,0.40821917808219177,"4
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS"
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS,0.410958904109589,"We have proposed the CLEVA-Compass to promote transparent assessment, comparison, and inter-
pretation of empirical set-ups and results. As the compass is a conceivable approach for a compact
visual representation, there is a natural limit to what it should cover and what it should be used for."
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS,0.4136986301369863,"One aspect that should be emphasized, is that the CLEVA-Compass is not intended to speculate
whether a specific method can in principle be extended. Instead, it prioritizes which context is
taken into consideration in current practice and lays open unconsidered dimensions. Specifically,
if a paper does not report a particular compass direction, this may suggest that the method is not
feasible in this scenario. However, it could also simply mean that the authors have not yet conducted
the respective evaluation. A faithfully filled compass makes this transparent to researchers and
should thus not be based on speculations. For instance, surveys (Parisi et al., 2019; Delange et al.,
2021) commonly attribute several continual learning algorithms with the potential for being “task
agnostic”, but no corresponding theoretical or empirical evidence is provided. This has lead to
empirical assessments, such as the ones by (Farquhar & Gal, 2018; Pf¨ulb & Gepperth, 2019), to
be “surprised” by the fact that certain methods do not work in practice when this factor is actually
varied. Conclusively, if it is hypothesized that a method’s capabilities extrapolate, it should be
separated from the CLEVA-Compass’ factual representation to avoid overly generalized, potentially
false, conclusions. Its ultimate utility will thus depend on faithful use in the research community."
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS,0.41643835616438357,"Apart from this unintended use of the CLEVA-Compass, there also exist several orthogonal aspects,
which have received attention in prior related works. These works mainly encompass a check-list for
quantitative experiments (Pineau et al., 2021), the construction of elaborate dataset sheets (Gebru
et al., 2018), and the creation of model cards (Mitchell et al., 2019). These efforts should not be
viewed in competition and present valuable efforts in the rigorous specification of complementary
aspects. On the one hand, the check-list includes imperative questions to assure that theorems and
quantitative experiments are presented correctly, think of the check-mark for reported standard devi-
ations and random seeds. In Table 2 of Appendix D, we summarize why it is crucial to still consider
these aspects, or why they are even particularly important in continual learning. On the other hand,
dataset sheets and model cards present a verbose description of essential aspects to consider in the
creation of datasets and models, with a particular focus on human-centric and ethical aspects. We
stress that these perspectives remain indispensable, as novel datasets and their variants are regularly
suggested in continual learning and the CLEVA-Compass does not disclose intended use with re-
spect to human-centered application domains, ethical considerations, or their caveats. We give some
examples for the latter statement in Appendix D. In addition to these central documents that capture
primarily static complementary aspects of machine learning, the CLEVA-Compass thus sheds light
onto fundamental additional features inherent to continual learning practice."
UNINTENDED USE AND COMPLEMENTARY RELATED EFFORTS,0.4191780821917808,"Due to their complementarity, we believe it is best to report both the prior works of the above para-
graph and the CLEVA-Compass together. In fact, collapsing thorough descriptions of datasets and
ethical considerations into one compact visual representation would likely oversimplify these com-
plex topics. However, we acknowledge that the present form of the CLEVA-Compass nevertheless
contains room to grow, as it is presently largely tailored to a statistical perspective of machine learn-
ing. Important emerging works that factor in recent progress from fields such as causality would thus
be hard to capture appropriately. We imagine that future CLEVA-Compass updates may therefore
include more elements on the inner star plot, or include additional reporting measures on the outer
level, once corresponding advances have matured, see Appendix C for our future vision."
CONCLUSION,0.42191780821917807,"5
CONCLUSION"
CONCLUSION,0.4246575342465753,"In this work, we have detailed the complexity involved in continual learning comparisons, based
on the challenges arising from a continuous workflow and the various influences drawn from re-
lated machine learning paradigms. To provide intuitive means to contextualize works with respect
to the literature, improve long-term reproducibility and foster fair comparisons, we have introduced
the Continual Learning EValuation Assessment (CLEVA) Compass. We encourage future contin-
ual learning research to employ the visually compact CLEVA-Compass representation to promote
transparent future discussion and interpretation of methods and their results."
CONCLUSION,0.4273972602739726,Published as a conference paper at ICLR 2022
CONCLUSION,0.4301369863013699,ACKNOWLEDGEMENTS
CONCLUSION,0.4328767123287671,"This work has been supported by the project “safeFBDC - Financial Big Data Cluster” (FKZ:
01MK21002K), funded by the German Federal Ministry for Economics Affairs and Energy as part
of the GAIA-x initiative, the Hessian Ministry of Higher Education, Research, Science and the
Arts (HMWK) project “The Third Wave of AI,” and the German Federal Ministry of Education
and Research and HMWK within their joint support of the National Research Center for Applied
Cybersecurity ATHENE."
REFERENCES,0.43561643835616437,REFERENCES
REFERENCES,0.4383561643835616,"Monya Baker. 1500 scientists lift the lid on reproducibility. Nature, 533:452–454, 2016."
REFERENCES,0.4410958904109589,"Paul Barham and Michael Isard. Machine Learning Systems are Stuck in a Rut. Proceedings of the
Workshop on Hot Topics in Operating Systems (HotOS), 2019."
REFERENCES,0.4438356164383562,"Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. Journal of Artificial Intelligence Research
(JAIR), 47:253–279, 2013."
REFERENCES,0.4465753424657534,"Abhijit Bendale and Terrance E. Boult. Towards Open World Recognition. In Computer Vision and
Pattern Recognition (CVPR), 2015."
REFERENCES,0.44931506849315067,"Emily M. Bender and Batya Friedman. Data Statements for Natural Language Processing: To-
ward Mitigating System Bias and Enabling Better Science. Transactions of the Association for
Computational Linguistics, 6:587–604, 2018."
REFERENCES,0.4520547945205479,"Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In-
ternational Conference on Machine Learning (ICML), 2009."
REFERENCES,0.4547945205479452,"Magdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-juss`a. Continual Lifelong Learn-
ing in Natural Language Processing: A Survey.
International Conference on Computational
Linguistics (COLING), 2021."
REFERENCES,0.4575342465753425,"L´eon Bottou. Online Learning and Stochastic Approximations. In Online Learning in Neural Net-
works, pp. 9–42. Cambridge University Press, 1999."
REFERENCES,0.4602739726027397,"Terrance E. Boult, Steve Cruz, Akshay R. Dhamija, Manuel Gunther, James Henrydoss, and Wal-
ter J. Scheirer. Learning and the Unknown : Surveying Steps Toward Open World Recognition.
AAAI Conference on Artificial Intelligence (AAAI), 2019."
REFERENCES,0.46301369863013697,"Massimo Caccia, Pau Rodr´ıguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia,
Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin. Online fast
adaptation and knowledge accumulation (OSAKA): A new approach to continual learning. Neural
Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.4657534246575342,"Rich Caruana. Multitask Learning. Machine Learning, 28:41–75, 1997."
REFERENCES,0.4684931506849315,"Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian
Walk for Incremental Learning: Understanding Forgetting and Intransigence. European Confer-
ence on Computer Vision (ECCV), 2018. ISSN 16113349."
REFERENCES,0.4712328767123288,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with A-GEM. International Conference on Learning Representations (ICLR),
2019."
REFERENCES,0.473972602739726,"Zhiyuan Chen and Bing Liu. Lifelong Machine Learning. Morgan and Claypool, second edition,
2018."
REFERENCES,0.4767123287671233,"Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021."
REFERENCES,0.4794520547945205,Published as a conference paper at ICLR 2022
REFERENCES,0.4821917808219178,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. Computer Vision and Pattern Recognition (CVPR), 2009."
REFERENCES,0.4849315068493151,"Natalia D´ıaz-Rodr´ıguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don’t forget,
there is more than forgetting: new metrics for Continual Learning. NeurIPS Continual Learning
Workshop, 2018."
REFERENCES,0.4876712328767123,"James Elder and Steven Zucker. The Effect of Contour Closure on the Rapid Discrimination of
Two-Dimensional Shapes. Vision Research, 33(7):981–991, 1993."
REFERENCES,0.4904109589041096,"James Elder and Steven Zucker. Evidence for Boundary-Specific Grouping. Vision Research, 38(1):
143–152, 1998."
REFERENCES,0.4931506849315068,"Linxi Fan, Silvio Savarese, Li Fei-Fei, Joan Creus-costa, Linxi Fan*, Yuke Zhu*, Jiren Zhu, Zihua
Liu, Orien Zeng, Anchit Gupta, Joan Creus-costa, Silvio Savarese, and Li Fei-Fei. SURREAL:
Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark. Confer-
ence on Robot Learning (CoRL), 2018."
REFERENCES,0.4958904109589041,"Sebastian Farquhar and Yarin Gal. Towards Robust Evaluations of Continual Learning. ICML
Lifelong Learning Workshop: A Reinforcement Learning Approach, 2018."
REFERENCES,0.4986301369863014,"Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI), 28(4):594–611, 2006."
REFERENCES,0.5013698630136987,"Michael Fink. Object classification from a single example utilizing class relevance metrics. Neural
Information Processing Systems (NeurIPS), 2005."
REFERENCES,0.5041095890410959,"Johannes Fuchs, Petra Isenberg, Anastasia Bezerianos, Fabian Fischer, and Enrico Bertini. The
Influence of contour on similarity perception of star glyphs. IEEE Transactions on Visualization
and Computer Graphics, 20(12):2251–2260, 2014. ISSN 10772626. doi: 10.1109/TVCG.2014.
2346426."
REFERENCES,0.5068493150684932,"Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daum´e, and Kate Crawford. Datasheets for Datasets. arXiv preprint arXiv: 1803.09010,
2018."
REFERENCES,0.5095890410958904,"Jort F. Gemmeke, Daniel P.W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and human-labeled dataset
for audio events. IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2017."
REFERENCES,0.5123287671232877,"Google Cloud.
Google Cloud guides:
ML solutions overview, the ML workflow. Ac-
cessed 2021-09-03, 2021. URL https://cloud.google.com/ai-platform/docs/
ml-solutions-overview."
REFERENCES,0.5150684931506849,"Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep net-
works. International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.5178082191780822,"Raia Hadsell, Dushyant Rao, Andrei A. Rusu, and Razvan Pascanu. Embracing Change: Continual
Learning in Deep Neural Networks. Trends in Cognitive Sciences, 24(12):1028–1040, 2020."
REFERENCES,0.5205479452054794,"Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. AAAI Conference on Artificial Intelligence (AAAI),
2018."
REFERENCES,0.5232876712328767,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.5260273972602739,"Tom M. Heskes and Bert Kappen. On-line learning processes in artificial neural networks. Mathe-
matical Foundations of Neural Networks, 51(C):199–233, 1993."
REFERENCES,0.5287671232876713,"Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-Learning in
Neural Networks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 2021."
REFERENCES,0.5315068493150685,Published as a conference paper at ICLR 2022
REFERENCES,0.5342465753424658,"Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L.
D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adri`a Gasc´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He,
Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Konecn´ı, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Rasmus
Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends in Machine Learning, 14, 2021."
REFERENCES,0.536986301369863,"Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring
Catastrophic Forgetting in Neural Networks. AAAI Conference on Artificial Intelligence (AAAI),
2018."
REFERENCES,0.5397260273972603,"Alex Krizhevsky.
Learning Multiple Layers of Features from Tiny Images.
Technical report,
Toronto, 2009."
REFERENCES,0.5424657534246575,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 1998(11):2278–2323, 1998."
REFERENCES,0.5452054794520548,"Timoth´ee Lesort, Hugo Caselles-Dupr´e, Michael Garcia-Ortiz, Andrei Stoian, and David Filliat.
Generative Models from the perspective of Continual Learning. International Joint Conference
on Neural Networks (IJCNN), 2019."
REFERENCES,0.547945205479452,"Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia
D´ıaz-Rodr´ıguez. Continual learning for robotics: Definition, framework, learning strategies, op-
portunities and challenges. Information Fusion, 58:52–68, 2020."
REFERENCES,0.5506849315068493,"Timoth´ee Lesort, Massimo Caccia, and Irina Rish. Understanding Continual Learning Settings with
Data Distribution Drift Analysis. arXiv preprint arXiv: 2104.01678, 2021."
REFERENCES,0.5534246575342465,"Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In
Uncertainty in Artificial Intelligence (UAI), 2019."
REFERENCES,0.5561643835616439,"Marius Lindauer and Frank Hutter.
Best practices for scientific research on neural architecture
search. Journal of Machine Learning Research (JMLR), 21, 2020."
REFERENCES,0.5589041095890411,"Vincenzo Lomonaco and Davide Maltoni. CORe50: a New Dataset and Benchmark for Continuous
Object Recognition. In Conference on Robot Learning (CoRL), volume 78, 2017."
REFERENCES,0.5616438356164384,"Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffieti, Tyler L.
Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She,
Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio
Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu,
Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni.
Avalanche: an End-to-End Library for Continual Learning. Computer Vision and Pattern Recog-
nition Workshops (CVPR-W), 2021."
REFERENCES,0.5643835616438356,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient Episodic Memory for Continual Learning.
Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.5671232876712329,"Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online Con-
tinual Learning in Image Classification: An Empirical Survey. arXiv preprint arXiv: 2101.10423,
2021."
REFERENCES,0.5698630136986301,"Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao,
John Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, and Li Fei-Fei. RoboTurk: A Crowd-
sourcing Platform for Robotic Skill Learning through Imitation. Conference on Robot Learning
(CoRL), 2018."
REFERENCES,0.5726027397260274,"Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The Natural Language
Decathlon: Multitask Learning as Question Answering. arXiv preprint arXiv: 1806.08730, 2018."
REFERENCES,0.5753424657534246,Published as a conference paper at ICLR 2022
REFERENCES,0.5780821917808219,"Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Connectionist Networks : The
Sequential Learning Problem. Psychology of Learning and Motivation - Advances in Research
and Theory, 24(C):109–165, 1989."
REFERENCES,0.5808219178082191,"H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efficient learning of deep networks from decentralized data. Conference on Ar-
tificial Intelligence and Statistics (AISTATS), 54, 2017."
REFERENCES,0.5835616438356165,"Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin-
son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting.
Conference on Fairness, Accountability, and Transparency (FAT*), 2019."
REFERENCES,0.5863013698630137,"Martin Mundt, Yong Won Hong, Iuliia Pliushch, and Visvanathan Ramesh. A Wholistic View of
Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and
Open World Learning. arXiv preprint arXiv:2009.01797, 2020a."
REFERENCES,0.589041095890411,"Martin Mundt, Sagnik Majumder, Iuliia Pliushch, Yong Won Hong, and Visvanathan Ramesh. Uni-
fied Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.
arXiv preprint arXiv:1905.12019, 2020b."
REFERENCES,0.5917808219178082,"Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational Continual Learn-
ing. International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.5945205479452055,"Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Pau Rodriguez, Matthew D Riemer, Julio
Hurtado, Khimya Khetarpal, Dominic Zhao, Ryan Lindeborg, Timoth´ee Lesort, Laurent Charlin,
Irina Rish, and Massimo Caccia. Sequoia: A Software Framework to Unify Continual Learning
Research. ICML Workshop on Theory and Foundations of Continual Learning, 2021."
REFERENCES,0.5972602739726027,"Stephen E. Palmer. Vision Science: Photons to Phenomenology. The MIT Press, London, 1999."
REFERENCES,0.6,"Sinno J. Pan and Qiang Yang. A Survey on Transfer Learning. IEEE Transactions on Knowledge
and Data Engineering (TKDE), 22(10), 2010."
REFERENCES,0.6027397260273972,"German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual
Lifelong Learning with Neural Networks: A Review. Neural Networks, 113:54–71, 2019."
REFERENCES,0.6054794520547945,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.6082191780821918,"Benedikt Pf¨ulb and Alexander Gepperth. A Comprehensive, Application-Oriented Study of Catas-
trophic Forgetting in DNNs. International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.6109589041095891,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivi´ere, Alina Beygelzimer,
Florence D’Alch´e-Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine
learning research (a report from the neurips 2019 reproducibility program). Journal of Machine
Learning Research (JMLR), 22, 2021."
REFERENCES,0.6136986301369863,"Roger Ratcliff. Connectionist Models of Recognition Memory: Constraints Imposed by Learning
and Forgetting Functions. Psychological Review, 97(2):285–308, 1990."
REFERENCES,0.6164383561643836,"Walter J. Scheirer, Anderson Rocha, Archana Sapkota, and Terrance E. Boult. Towards Open Set
Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 35(7):
1757–1772, 2013."
REFERENCES,0.6191780821917808,"Burr Settles. Active Learning Literature Survey. Technical report, University of Wisconsin-Madison,
Madison, 2009."
REFERENCES,0.6219178082191781,"Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, and Kuansan Wang. An Overview
of Microsoft Academic Service (MAS) and Applications. WWW - World Wide Web Consortium
(W3C), 2015."
REFERENCES,0.6246575342465753,Published as a conference paper at ICLR 2022
REFERENCES,0.6273972602739726,"Vivienne Sze, Yu Hsin Chen, Tien Ju Yang, and Joel S. Emer. Efficient Processing of Deep Neural
Networks: A Tutorial and Survey. Proceedings of the IEEE, 105(12):2295–2329, 2017. ISSN
15582256."
REFERENCES,0.6301369863013698,"Sebastian Thrun and Lorien Y. Pratt. Learning to Learn: Introduction and Overview. 1998. doi:
10.1007/978-3-658-20540-9 1."
REFERENCES,0.6328767123287671,"Gido M. van de Ven and Andreas S. Tolias. Three scenarios for continual learning. NeurIPS Con-
tinual Learning Workshop, 2019."
REFERENCES,0.6356164383561644,"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language
understanding systems. Neural Information Processing Systems (NeurIPS), 2019a."
REFERENCES,0.6383561643835617,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. Inter-
national Conference on Learning Representations (ICLR), 2019b."
REFERENCES,0.6410958904109589,"Yaqing Wang, Quanming Yao, James Kwok, and Lionel M. Ni. Generalizing from a Few Examples:
A Survey on Few-Shot Learning. ACM Computing Surveys, 2020."
REFERENCES,0.6438356164383562,"Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated Contin-
ual Learning with Weighted Inter-client Transfer. International Conference on Machine Learning
(ICML), 2021."
REFERENCES,0.6465753424657534,Published as a conference paper at ICLR 2022
REFERENCES,0.6493150684931507,APPENDIX
REFERENCES,0.6520547945205479,"We include four sections with additional information in the appendix. In Appendix A, we elabo-
rate further on mathematical definitions for the measures on the outer CLEVA-Compass level, as
described in Table 1 of the main body. In Appendix B, we provide a set of examples to further
clarify the role of supervision in the various elements of the inner compass level. The five meth-
ods in the example main body compass illustration of Figure 4 are included in the discussion here,
with additional examples serving the purpose to enhance the reader’s intuition of how supervision
could manifest in the remaining possibilities. Our CLEVA-Compass graphical user interface, code,
and repository are then explained in Appendix C. Finally, as additional detail to the main body’s
Section 4, Appendix D provides further examples for, and underlines the indispensability of, com-
plementary efforts with respect to the check-lists, dataset sheets and model cards introduced in prior
related works."
REFERENCES,0.6547945205479452,"A
CLEVA-COMPASS METRIC: MATHEMATICAL DEFINITIONS"
REFERENCES,0.6575342465753424,"We have motivated the measures to report in continual learning in the description of the outer
CLEVA-Compass level in Table 1 of the main body. There, we have also included references to
prominent prior works that have outlined specific proposals. Although our work is not meant to
serve as a full fledged review paper, we briefly quote corresponding mathematical definitions, in
order to provide the reader with a more self-contained overview. For this purpose, we emphasize
that the following provides conceivable ways to craft measures for specific applications, which is
typically considered to be classification. In multiple of the below cases, several ways to report the
same concept are thus listed. However, note that definitions are mostly either compatible and the
exact expression transformable, or tailored to a particular use case."
REFERENCES,0.6602739726027397,"We remark that we attempt to stay as close as possible to the authors’ original notation (with only
minor modifications to ensure consistency and resolve potential ambiguity in variable overlap), in
order to facilitate understanding for the reader when going back for a more in-depth look at the
original works. To make the explicit mathematical expressions accessible, we thus also follow
the assumption that variable t refers to observed tasks in t = 1, . . . , T. This should not hinder
generality of evaluation, as the knowledge of the task could be assumed to be known for testing
purposes or strict boundaries between tasks could practically be further relaxed in below definitions."
REFERENCES,0.663013698630137,Per task metrics
REFERENCES,0.6657534246575343,"Classification accuracy per task: Lopez-Paz & Ranzato (2017) consider constructing an accuracy
matrix a ∈RT ×T , where ai,j is the test classification accuracy of the model on the j-th task after
observing the last sample from task i. The final average accuracy is then defined as:"
REFERENCES,0.6684931506849315,"aT = 1 T T
X"
REFERENCES,0.6712328767123288,"t=1
aT,t .
(1)"
REFERENCES,0.673972602739726,"“Base”, “new” & “all” classification accuracy: Accuracy is split into three task specific parts.
Closely following the description of Kemker et al. (2018): αnew,t is the test accuracy for task t after
task t has been learned, αbase,t is the test accuracy on the first task (base set) after t new tasks have
been learned, αall,t is the test accuracy of all the test data for the classes seen up to this point, and
αideal is the offline accuracy on the base set, which is assumed to be the ideal performance. These
accuracies can then be used to define the normalized quantities between [0, 1]:"
REFERENCES,0.6767123287671233,"Ωbase =
1
T −1 T
X t=2"
REFERENCES,0.6794520547945205,"αbase,t"
REFERENCES,0.6821917808219178,"αideal
(2)"
REFERENCES,0.684931506849315,"Ωnew =
1
T −1 T
X"
REFERENCES,0.6876712328767123,"t=2
αnew,t
(3)"
REFERENCES,0.6904109589041096,"Ωall =
1
T −1 T
X t=2"
REFERENCES,0.6931506849315069,"αall,t
αideal
.
(4)"
REFERENCES,0.6958904109589041,Published as a conference paper at ICLR 2022
REFERENCES,0.6986301369863014,Forgetting
REFERENCES,0.7013698630136986,"Forgetting quantifies the difference between the maximum knowledge gained about the task through-
out the learning process in the past and the knowledge the model currently has about it. For classifi-
cation, Chaudhry et al. (2018) define forgetting for the j-th task after the model has been trained up
to task t as:
f t
j =
max
i∈{1,··· ,t−1} ai,j −at,j,
∀j < t .
(5)"
REFERENCES,0.7041095890410959,"The average forgetting, after task t has bee learned, is then defined as:"
REFERENCES,0.7068493150684931,"Ft =
1
t −1 t−1
X"
REFERENCES,0.7095890410958904,"j=1
f t
j .
(6)"
REFERENCES,0.7123287671232876,Forward transfer
REFERENCES,0.7150684931506849,"Classification forward transfer: The definition for per task accuracy has inspired expressions for
forward and backward transfer (Lopez-Paz & Ranzato, 2017). For consistency with the above no-
tation, we employ the notation according to Chaudhry et al. (2018). There, forward transfer (FWT)
for classification is defined as the influence of the performance on future tasks j > t when the model
is learning task t:
FWTt,j = at−1,j −bj ,
(7)"
REFERENCES,0.7178082191780822,"where bj is the accuracy on task j of a random baseline. The average forward transfer is then defined
as:"
REFERENCES,0.7205479452054795,"FWTt =
1
t −1 t−1
X"
REFERENCES,0.7232876712328767,"j=2
FWTj−1,j .
(8)"
REFERENCES,0.726027397260274,"Note, that t is fixed to j −1 in the expression for the average."
REFERENCES,0.7287671232876712,"Learning Curve Area (LCA): LCA is inspired by above definitions. Chaudhry et al. (2019) define
the average b-shot performance after the model has been trained for all T tasks as"
REFERENCES,0.7315068493150685,"Zb = 1 T T
X"
REFERENCES,0.7342465753424657,"t=1
at,b,t ,
(9)"
REFERENCES,0.736986301369863,"where b is the mini-batch number. LCA at β is the area of the convergence curve Zb as a function
of b ∈[0, β]:"
REFERENCES,0.7397260273972602,"LCAβ =
1
β + 1 Z β"
REFERENCES,0.7424657534246575,"0
Zbdb =
1
β + 1 β
X"
REFERENCES,0.7452054794520548,"b=0
Zb .
(10)"
REFERENCES,0.7479452054794521,"That is, LCA0 is the average 0-shot performance."
REFERENCES,0.7506849315068493,"Online codelength: Motivated by the propositions of Kemker et al. (2018) and Chaudhry et al.
(2019), a metric to measure the “adoption rate of an existing model to a new task” is introduced in
the context of natural language processing (Biesialska et al., 2021). Termed the online codelength
ℓ(D), a similar measure to area under the learning curve is given by:"
REFERENCES,0.7534246575342466,"ℓ(D) = log2 |y| − N
X"
REFERENCES,0.7561643835616438,"i=2
log2 p
 
yi | xi; θDi−1

.
(11)"
REFERENCES,0.7589041095890411,"Here, |y| is the number of possible class labels in the dataset D and θDi−1 are the parameters trained
on a particular subset of the dataset."
REFERENCES,0.7616438356164383,Published as a conference paper at ICLR 2022
REFERENCES,0.7643835616438356,Backward transfer
REFERENCES,0.7671232876712328,"Related to the concept of forward transfer for classification, backward transfer (BWT) describes the
influence of the performance on previous tasks j < t when the model is learning task t (Lopez-Paz
& Ranzato, 2017; Chaudhry et al., 2018):"
REFERENCES,0.7698630136986301,"BWTt,j = at,j −aj,j .
(12)"
REFERENCES,0.7726027397260274,The average backward transfer can then be defined as:
REFERENCES,0.7753424657534247,"BWTt =
1
t −1 t−1
X"
REFERENCES,0.7780821917808219,"j=1
BWTt,j .
(13)"
REFERENCES,0.7808219178082192,"Note that a positive value for BWT implies a retrospective improvement of a task, whereas a
negative value for BWT coincides with a quantification of model forgetting. Similar to the accuracy
based definitions for forward and backward transfer, analogous expressions can be constructed on
the basis of measuring task losses."
REFERENCES,0.7835616438356164,Openness
REFERENCES,0.7863013698630137,"Classification openness: In the context of classification, Scheirer et al. (2013) provide a definition to
capture the continuum between a completely closed world approach, where train and test examples
are fully known to belong to the investigated task, and an open world, where various unknown
instances can be observed. They propose to indicate the following quantity:"
REFERENCES,0.7890410958904109,O = 1 − s
REFERENCES,0.7917808219178082,"2 × Ntrain
Ntest + Ntarget
.
(14)"
REFERENCES,0.7945205479452054,"This equation defines openness as a ratio between the number of classes used for training Ntrain to
the number of classes to be identified Ntarget and the number of classes used in testing Ntest."
REFERENCES,0.7972602739726027,"Openness as a probability to encounter unknown instances: An alternative to the above definition
for openness, particularly outside the context of classification, can be to indicate a probability
with which out-of-distribution instances are encountered. For instance, Hendrycks & Dietterich
(2019) define a probability for a sample to be perturbed or corrupted. Similarly, Caccia et al. (2020)
indicate a sampling probability that dictates how many data instances are drawn with different,
currently unknown, class labels."
REFERENCES,0.8,Other measures on the outer CLEVA-Compass level
REFERENCES,0.8027397260273973,"The majority of the other measures on the CLEVA-Compass’ outer level are self-explanatory. For
example, the size of the used dataset or number of optimization steps are simple scalar quantities
to report. Nevertheless, there can exist alternatives to a straightforward report of e.g. number of
model parameters or employed memory. For completeness, we mention some transformed alterna-
tive forms for these raw measures, according to the proposition of D´ıaz-Rodr´ıguez et al. (2018)."
REFERENCES,0.8054794520547945,"• Model size efficiency: Instead of reporting the number of parameters of a model over time,
it may also be sensible to provide a relative specification in terms of quantifying the growth
of the model size:"
REFERENCES,0.8082191780821918,"MS = min  1,"
REFERENCES,0.810958904109589,"PT
t=1
Mem(θ1)
Mem(θt)
N "
REFERENCES,0.8136986301369863,",
(15)"
REFERENCES,0.8164383561643835,"with Mem(θt) describing the number of parameters of the model at time step t.
• Samples storage size efficiency: Quantifying the memory used by the stored samples can
similarly be reported in relation to the overall dataset size:"
REFERENCES,0.8191780821917808,"SSS = 1 −min  1,"
REFERENCES,0.821917808219178,"PT
t=1
Mem(Mt)"
REFERENCES,0.8246575342465754,"Mem(D)
N "
REFERENCES,0.8273972602739726,".
(16)"
REFERENCES,0.8301369863013699,"Here, Mem(Mt) is the size of the stored samples in the memory, and Mem(D) is the size
of the overall observed dataset."
REFERENCES,0.8328767123287671,Published as a conference paper at ICLR 2022
REFERENCES,0.8356164383561644,"• Computational efficiency: As a combination of raw MAC operations and number of opti-
mization steps, an additional quantity for computational efficiency can be expressed as the
following ratio:"
REFERENCES,0.8383561643835616,"CE = min  1,"
REFERENCES,0.8410958904109589,"PT
t=1
Ops↑↓(Dt)"
REFERENCES,0.8438356164383561,"Ops(Dt)
N "
REFERENCES,0.8465753424657534,".
(17)"
REFERENCES,0.8493150684931506,"Here, Ops(Dt) is the number of operations needed to learn the (training) dataset Dt, and
Ops ↑↓(Dt) is the number of operations required to do one forward and one backward
pass on Dt."
REFERENCES,0.852054794520548,"B
CATEGORIZATION OF SHOWN METHODS AND FURTHER EXAMPLES FOR
THE CLEVA-COMPASS’ INNER LEVEL"
REFERENCES,0.8547945205479452,"In the main body, Section 3 Figure 4, we have provided an example CLEVA-Compass illustration
based on five continual learning methods. These methods have been deliberately picked to empha-
size the compass’ utility and necessity in light of the large exhibited differences. Correspondingly,
the inner and outer levels have been described in detail in the main body, based on the related
paradigms of Figure 3 and the detailed measures of Table 1. The example illustration highlights
why the CLEVA-Compass is an important asset in identifying how methods relate in practice, ex-
posing some of the typically unmentioned assumptions on set-up and evaluation, without requiring a
detailed mathematical understanding of proposed techniques. However, our chosen example meth-
ods naturally only span a subset of overall possibilities. This is particularly true if we recall that
there is a distinction between an unsupervised and supervised approach for every individual element
of the CLEVA-Compass’ star plot, see the description in main body Section 3.1."
REFERENCES,0.8575342465753425,"In this appendix section, we thus provide examples for imaginable scenarios for every element on
the inner compass level, both to enhance our intuition for the already shown methods and to empha-
size why the distinction of supervision is crucial in the remaining settings. In the main body, we
have included one such motivating example, an unsupervised continual density estimator, which in
independence of being an unsupervised method itself, can have flexible requirements for supervi-
sion with respect to e.g. training multiple models in the continual setting. We emphasize, that the
primary goal of such examples is to provide further clarification and that they are not meant to be
viewed as exclusive definitions. For this purpose, the below list contains examples with and without
supervision for each compass element, going clockwise from the “six o’clock” position of the com-
pass and picking up the already visualized methods where applicable. Note that a short version of
these examples is also provided in small “hover-over” tooltips in our CLEVA-Compass Graphical
User Interface, which will be detailed in the upcoming section, Appendix C."
REFERENCES,0.8602739726027397,"Task agnostic:
A method is said to be task agnostic if for prediction in a deployed model it does
not require any additional information for which task the data instance originates from. A super-
vised manifestation for such information, in independence of whether the task itself is supervised
or not, would be to explicitly include a time-step or task label into the learning process to condition
the prediction. The visualized OSAKA and A-GEM methods are examples of this, where a “con-
text” or respective “task-descriptor” variable is explicitly conditioned on and later inferred. A fully
unsupervised variant, or fully task-agnostic approach, would be able to inherently provide a correct
prediction for any data instance from any previously observed task without any such information.
In stark contrast, no mark on either supervised or unsupervised portion of the task agnostic star plot
element would indicate that an approach is not capable of solving the task assignment challenge at
all, implying that a task oracle is required for prediction. The latter is a typical, sometimes unex-
posed, assumption in many continual learning works that focus on other challenges, such as VCL,
but is often argued to be unrealistic."
REFERENCES,0.863013698630137,"Task order discovery:
One way to evaluate continual learning methods is to investigate the var-
ious measures on the CLEVA-Compass in a fixed sequence of benchmark data, corresponding to
no visual mark on this star plot element for the majority of our example methods. An alternative,
inspired by curriculum learning, would be to let the approach decide which task is meaningful to
learn next. On the example of a classifier, rather intuitively, if a method can choose an improved task"
REFERENCES,0.8657534246575342,Published as a conference paper at ICLR 2022
REFERENCES,0.8684931506849315,"order, it does so in a supervised fashion if it e.g. makes use of prospective class labels to distinguish
which class would be best to learn next. An example of this is the visualized OCDVAE method,
which constructs a class specific meta-recognition model to assess a similarity measure. On the flip
side of this example, if the model discovers an improved task order based only on e.g. divergences
or distances in any constructed feature space that do not require labels to compute, the task order
could be said to be unsupervised."
REFERENCES,0.8712328767123287,"Active data query:
A traditional fixed sequence benchmark setting has no active data query com-
ponent, observable in the majority of the visualized compass examples. Similar to above task order
discovery, but inspired from the perspective of active learning, an alternative could be to actively
query data (in independence of whether data is available in a pool, a stream, etc.). This way, an
approach would actively choose what data instances to include next into optimization. As such, a
measure of utility for prospective inclusion of a data instance is generally constructed. Correspond-
ingly, this utility measure can either require presence of supervision, as shown in the class-specific
meta-recognition model of OCDVAE, or can be entirely unsupervised."
REFERENCES,0.873972602739726,"Multiple modalities:
If an approach only learns on one modality, such as text or images, then
no indication of multi-modality is marked in the CLEVA-Compass. This is the setting for four of
our example compass methods, with exception of OCDVAE that handles transformed audio and
visual data in a supervised fashion. If the constructed system is able to handle multiple sources, a
distinction between whether the multi-modality aspect is unsupervised or supervised condenses to
the difference between whether or not the system requires a label on which modality an instance
originates from, e.g. to condition a specific computation for this modality."
REFERENCES,0.8767123287671232,"Open world:
In an open world, the additional challenge for a learner is to robustly identify un-
known, sometimes corrupted or perturbed, and potentially meaningless data instances. As an exam-
ple for no indication of this aspect on the CLEVA-Compass, the majority of blackbox deep learning
methods do not inherently posses robust identification mechanisms, which is the case in three of our
shown example methods. If a mechanism is however included to recognize instances that deviate
from the data distribution observed during training, then it would be supervised if its conception
requires a class label, say a classifier entropy or similar supervised quantity. OSAKA and OCDVAE
are two examples for the latter, the first of which identifies anomalies with respect to a supervised
loss and the second of which relies on identifying anomalies with respect to a statistical meta-
recognition model based on class-means. A respective unsupervised example could be a difference
in e.g. a reconstruction loss or a divergence measure with respect to arbitrary feature spaces."
REFERENCES,0.8794520547945206,"Online:
In an online setting, a learner faces the additional difficulty of not being allowed to revisit
observed data instances. No mark on the respective compass star plot thus implies that data is
revisited. In continual learning, this corresponds to the common setting where several tasks are
learned in sequence, but within each task several “epochs” are trained to convergence, as practically
conducted in all but one of the chosen example methods. When moving to the online setting, the
increased stochasticity alongside with data drifts become an increasing challenge. Example methods
to address this scenario, by making sure that a consistent model is trained, could then be supervised
or unsupervised. For instance, one could regularize parameter deviations over time according to a
supervised importance measure, assume a supervised representation pre-training step as in OSAKA,
or rely on various unsupervised quantities, such as exponential moving averages."
REFERENCES,0.8821917808219178,"Federated:
As one conceivable distinction, and once more independently of a task to be solved,
federated scenarios could be supervised or unsupervised with respect to the federated implementa-
tion, akin to the examples of multiple models or modalities. For instance, we could assume that
there exist a large number of devices in a network, but there are groups corresponding to different
regions, different processing devices, labels on how many devices are participating actively, and so
on. In that sense, communication in federated learning could provide additional supervised infor-
mation that could be exploited to steer model training, optimize sub-model parts, or conversely go
down a fully unsupervised route and simply average communicated updates without any label of the
aforementioned characteristics. The example visualization if FedWeIT is an instance of the latter
unsupervised setting."
REFERENCES,0.8849315068493151,Published as a conference paper at ICLR 2022
REFERENCES,0.8876712328767123,"Multiple models:
We have provided an example for the role of supervision in use of multiple
models in the main body. We repeat this example here: multiple employed models could each solve
a supervised or unsupervised task. In the supervision on the level of multiple models, an additional
mechanism could be required to index the appropriate model for prediction, such as proposed in our
FedWeIT example that employs specific attention based masks. If the correct model is automatically
queried to provide correct predictions independently of which task an input belongs to this level is
unsupervised. Finally, no visual mark on the compass corresponds to the scenario where only a
single model is used."
REFERENCES,0.8904109589041096,"Uncertainty:
Perhaps the least intuitive of the listed items, uncertainty could be believed to rep-
resent an inherently unsupervised quantity. We note however that uncertainty, especially in the way
it is used in deep neural network approximations, is not always practically accurate and useful. As
such, some uncertainty measures can typically require a need for calibration in order to provide
meaningful values, e.g. the entropy of classifier predictions (note that we do not delve into a dispute
of whether such a quantity should indeed be seen as a formal “uncertainty”, but simply acknowl-
edge the occurrence of such uses in the literature). Such calibration procedures can be interpreted
as providing a supervised signal of what uncertainty “should look like”. In contrast, if the method
provides inherent uncertainties, such as argued in e.g. the fully Bayesian neural network viewpoint
of VCL, it can be said to be unsupervised. Naturally, many approaches also do not provide any
uncertainty estimates at all."
REFERENCES,0.8931506849315068,"Generative:
Many methods that solve a specific task are not generative, e.g. consider a typical
deep discriminative classifier of the form p(y|x), where x denotes data and y denotes labels. A
supervised generative variant would correspond to a model that learns the joint distribution p(x, y)
instead, whereas unsupervised generative models will learn or approximate only p(x). Even when
our objective is the classification example, the first of these variants will now also base decisions
on the underlying nature of the data distribution. In current practice it is likely that an unsupervised
generative model also primarily performs an unsupervised task (although we note that disentan-
gled representations might in principle and prospectively allow classification approaches without
supervised training). Nevertheless, the other way around, the distinction is still important because
not every unsupervised task necessarily requires a generative model. We have included the shown
Bayesian approach of VCL as an unsupervised generative example. However, it is worth mentioning
that the authors also investigate a supervised variant in their work. As supervision could always be
included as an additional term into fully unsupervised perspectives, we have marked the method on
the outer unsupervised ring."
REFERENCES,0.8958904109589041,"Episodic memory:
An episodic memory is constructed to effectively rehearse so called exem-
plars or prototype data instances. The challenge here is to assure that the constructed memory data
subset is representative of the observed data as a whole. Intuitively, if a method does not employ
an auxiliary episodic memory, no mark is indicated on the CLEVA-Compass. If the construction
mechanism for this episodic memory relies on labels in the data, e.g. by approximating a per class
mean in OCDVAE, then it is supervised. A straightforward unsupervised example would be to
fill the episodic memory by sampling random data instances, such as suggested by A-GEM, or by
employing unsupervised algorithms such as k-means clustering, as practically proposed in VCL."
REFERENCES,0.8986301369863013,"C
CREATING A CLEVA-COMPASS: CODE, GRAPHICAL USER INTERFACE
AND METHODS REPOSITORY"
REFERENCES,0.9013698630136986,"To make the CLEVA-Compass as accessible as possible and disseminate in a convenient way, we
provide three options for practical use."
REFERENCES,0.9041095890410958,"1. We provide a LaTeX template for the CLEVA-Compass, making use of the TikZ library to
draw the compass within LaTeX. We envision that such a template makes it easy for other
authors to include a compass into their future submission, where they can adapt the naming
and values of the entries respectively."
REFERENCES,0.9068493150684932,"2. We further provide a Python script to generate the CLEVA-Compass. In fact, because
the use of drawing in LaTeX with TikZ may be unintuitive for some, we have written a"
REFERENCES,0.9095890410958904,Published as a conference paper at ICLR 2022
REFERENCES,0.9123287671232877,"Figure 5: The CLEVA-Compass GUI. With the help of this application, users can interactively
customize and construct their own CLEVA-Compass visualization or import existing ones."
REFERENCES,0.915068493150685,"Python script that automatically fills the above LaTeX template, so that it can later simply
be included into a LaTeX document. The Python script takes a path to a JSON file that
needs to be filled by the user with the CLEVA-Compass options. We further provide a
default JSON file that is easy to adapt."
REFERENCES,0.9178082191780822,"3. To further lower the barrier for dissemination and use, we also provide a CLEVA-Compass
Graphical User Interface (GUI). The GUI makes it easy for users to simply “click to-
gether” their desired compasses, save images or export to LaTeX, and conversely import
already existing compass methods."
REFERENCES,0.9205479452054794,"We refer to our public repository for the outlined code and the downloadable GUI:
https://github.com/ml-research/CLEVA-Compass."
REFERENCES,0.9232876712328767,"Code requirements and descriptions are explained in a corresponding README file there. In the
following, we concentrate on the details of our CLEVA-Compass GUI. A visualization is depicted
in Figure 5. We continue by explaining its main elements, functionality and its connection to the
provided repository to accumulate methods."
REFERENCES,0.9260273972602739,"Creating CLEVA-Compass entries:
One of the key functionalities in the GUI is to quickly and
intuitively add new methods (entries) into individual or existing CLEVA-Compass visualizations
(see paragraph below for loading of existing methods). For this purpose, the GUI contains a Label
field at the top left, intended to enter the method’s name and authors, followed by the clickable op-
tions for all the items contained in the inner and outer compass levels. As detailed in the explanation
of the main body Section 3.1, this involves the level of supervision on the inner level and the par-
ticular measures that have been reported on the outer level. Upon clicking the Add Compass Entry
button, the method will be visualized and will show up on the bottom left list of visualized entries.
The compass visualization will automatically adapt and scale with amount of entries (see also last
paragraph for recommendations on amount of methods and colors). Should a user wish to adapt"
REFERENCES,0.9287671232876712,Published as a conference paper at ICLR 2022
REFERENCES,0.9315068493150684,"or revise an entry, we offer buttons to Delete Compass Entry, Update Compass Entry, and Reload
Preview. The remaining buttons provide various saving and loading functionality, detailed below."
REFERENCES,0.9342465753424658,"“Hover-over” tooltips for examples and hints:
We have detailed the levels and elements of the
CLEVA-Compass throughout the main paper, with additional examples for further intuition. As it
may be difficult to recall all the paradigm descriptions and described measures, we provide a “hover-
over” functionality in the GUI. When placing the cursor above an item, e.g. a specific continual
learning metric detailed in Table 1, a small box will show with the corresponding description. In
the GUI Figure 5, this is illustrated at the example of backward transfer. In this way, a user is
not require to swap back and forth between GUI and paper descriptions when designing their own
CLEVA-Compass. In the same spirit, we have included small “example hints” for the inner level
compass elements, in analogy to the descriptions of the ones provided in Appendix B."
REFERENCES,0.936986301369863,"Saving and exporting functionality:
As described at the beginning of this section, there are mul-
tiple interface choices to make the compass accessible. In this spirit, our GUI also provides various
exporting and saving functionalities. The most straightforward of these is the Export to Image
button, which allows to save the visualized CLEVA-Compass in either SVG or PNG formats. In
correspondence with our Python script, we also enable the option to Export Entry to File, which we
build-upon to share and sync with our repository, detailed in the next paragraph. Lastly, to make it
easy to include the Compass as TikZ code in a LaTeX document and thus allow for citation that is
linked to reference lists, we finally provide the functionality to Export to Tex file. The latter creates
the necessary TikZ code that generates the compass when LaTeX is compiled."
REFERENCES,0.9397260273972603,"Loading and the CLEVA-Compass repository to accumulate methods:
The final not yet de-
scribed element of the GUI are the Import Entry from File(s) and Download Methods buttons. The
Import Entry from File(s) functionality serves the purpose to enable users to load already existing
CLEVA-Compass visualizations, in the form of loading their methods’ JSON representations. As
such, users will not have to replicate each and every method that has already been visualized in
the CLEVA-Compass by hand. In addition to this, a list of existing methods, which at the point of
writing this paper consists of the five methods of the main body, is provided in our public repository.
By using the Download Methods button the GUI will automatically synchronize the up-to-date list
of available methods and enable an interactive selection. Our vision is that prospective papers can
contribute their own visualizations to this repository, so the amount of published methods and their
CLEVA-Compass representations grows into a comprehensive repository. We strongly believe that
this can help foster transparency in our community for prospective continual learning authors, but
also in terms of creating a more straightforward overview of the set-up and evaluation practices of
continual learning approaches for application engineers and practitioners. As a side note, we note
that this attempt at cataloguing works and their “rolling” aggregation is separate from proposing
prospective adaptation and extensions of the CLEVA-Compass (think of the example of including
causality in our main body’s outlook). For such major content and functionality updates, we subjec-
tively envision a “discrete release” model, where prospective changes are encouraged to first undergo
further stages of peer review, before being finally included into a CLEVA-Compass repository up-
date. Although this may initially appear to slow down adoption of new methods, we argue in favor
of this approach to limit the risk of a fixed set of researchers and a tiny portion of the community
controlling such fundamental changes that steer the course of continual learning."
REFERENCES,0.9424657534246575,"A note on number of methods and colors:
Upon careful examination, one may note that we have
chosen to include only six discrete colors as options for CLEVA-Compass visualizations. Rather
than adding e.g. a color wheel to pick various colors, this presents a deliberate design choice. It is
tied to the fact that we believe that there is little utility and value in visualizing more than six methods
at maximum in one compass. The natural reason for this is that it becomes hard to distinguish
methods after a certain point of overlap. However, we do not see this as a limitation, because
star plots are well-known to provide excellent capabilities for visual comparison when placed side
by side. In other words, when we place two CLEVA-Compasses side by side an easy and quick
comparison is enabled, even if only one method were to be visualized in each."
REFERENCES,0.9452054794520548,Published as a conference paper at ICLR 2022
REFERENCES,0.947945205479452,"Table 2: Auxiliary items from the machine learning reproducibility check-list (left-column) (Pineau
et al., 2021) and their prevailing relevance for continual learning (right column)."
REFERENCES,0.9506849315068493,"Paper reproducibility check-list: did you?
Additional importance for continual learning"
REFERENCES,0.9534246575342465,"include the code, data, and instructions needed to
reproduce the main experimental results"
REFERENCES,0.9561643835616438,"Reproduction becomes even more challenging, as
descriptions alone tend to overlook nuances in
the precise way data is sampled continuously and
models are adapted over time."
REFERENCES,0.958904109589041,"specify all the training details (e.g. data splits,
hyper-parameters, how they were chosen)"
REFERENCES,0.9616438356164384,"Training details become particularly important, as
precise data sequences determine comparability
and methods can be subject to additional hyper-
parameters that are now tunable per task."
REFERENCES,0.9643835616438357,"report error bars (e.g. with respect to the random
seed after running experiments multiple times)"
REFERENCES,0.9671232876712329,"In addition to the inherent optimization stochas-
ticity, many continual methods introduce further
random components, e.g. in random data sub set
extraction or task order randomization."
REFERENCES,0.9698630136986301,"include the amount of compute and the type of re-
sources used (e.g. type of GPUs, cluster, or cloud)"
REFERENCES,0.9726027397260274,"Continual learning methods are often judged by
their ability to adapt knowledge quickly or com-
pute updates on-the-fly on data streams, which is
directly tied to employed compute environments."
REFERENCES,0.9753424657534246,"use existing or curate/release new assets (e.g.
code, data, models): cite the authors, mention the
license, include any new assets, obtain consent"
REFERENCES,0.9780821917808219,"Particular caution should be exercised as empiri-
cal continual investigations are frequently derived
from available existing repositories and datasets."
REFERENCES,0.9808219178082191,"D
IMPORTANCE OF EXISTING CHECK-LISTS AND DOCUMENTATION
PROPOSALS FOR CONTINUAL LEARNING"
REFERENCES,0.9835616438356164,"We have introduced the CLEVA-Compass and have motivated its utility in the paper’s main body.
At the same time, in Section 4, we point out that there are several prior works that remain indispens-
able and can be considered orthogonal to our proposition. This is due to the fact that these works
have both a different focus in terms of their intended use and have chosen a significantly different
presentation format. Whereas our CLEVA-Compass provides a compact visual representation to
contextualize continual approaches within related literature and contrasts their practical protocols,
related works on model cards (Mitchell et al., 2019) and dataset sheets (Gebru et al., 2018) adopt a
more verbose approach towards specification of elements surrounding data and intended model use.
In addition to these efforts, the machine learning conference reproducibility check-list (Pineau et al.,
2021) includes further general guidelines, that are meant to ground researchers in their practical ma-
chine learning reporting. The corresponding check-list items illustrate more general best-practice
items when dealing with machine learning experiments. We provide a few examples in this Ap-
pendix Section to give additional explanations of why both the CLEVA-Compass and mentioned
related works are complementary."
REFERENCES,0.9863013698630136,"Machine learning reproducibility check-list: The items on the check-list have been proposed as
guidelines towards general best-practice behavior in the context of recent reproducibility initiatives
(Pineau et al., 2021). In particular, the main check-list’s portion of empirical desiderata can be seen
as at the center of commendable machine learning practice. This portion includes propositions for
quantitative experiments, such as visualizing error bars, reporting random seeds, or detailing how
hyper-parameters were tuned and chosen. An analogous argument can be made with respect to the
inclusion of assumption explanations and proofs for involved theoretical statements. Consequently,
the check-list can be regarded as auxiliary to the CLEVA-Compass, as the latter does not capture
elements of general good practice with respect to figures, central tendency in results, or hyper-
parameter tuning. In fact, we believe that many of the check-list items become even more important
in the context of continual learning and the CLEVA-Compass. In Table 2, we thus indicate the
entailed additional importance of selected check-list elements, when considered within continual
learning."
REFERENCES,0.989041095890411,Published as a conference paper at ICLR 2022
REFERENCES,0.9917808219178083,"Model cards: Mitchell et al. (2019) have suggested that every machine learning model should be
accompanied with a so called model card. Such a model card is typically a one page overview, which
summarizes: the choice of model, its intended use in terms of application, factors with respect to
human-centric use, an indication of evaluated metrics, potential concrete results, along with any
ethical considerations and caveats. In direct comparison with the CLEVA-Compass, the perspec-
tive of a model card could be summarized as primarily being tailored towards a static view of the
human-centric application intent, in order to make deployment, limits, and ethical considerations
more transparent and fair. The corresponding lengthy description provided in model cards is thus
completely complementary to the continual factors captured in the CLEVA-Compass, which do not
reflect a similar focus. In fact, we posit that it is even necessary to provide more detailed specifi-
cation for ethical concerns, as they are hard to capture in a compact visual representation. To pick
up concrete examples, model cards encourage the textual descriptions for: “person or organization
developing the model”, “model date, version and type”, “license”, “where to send questions”, “in-
tended use”, human-centric “relevant factors including groups, instrumentation and environments”
(i.e. personal characteristics, employed sensors, deployment environment), “unitary and intersec-
tional results”, “ethical considerations”, “caveats”, and so on. These factors are orthogonal to the
CLEVA-Compass, but remain equally important to consider."
REFERENCES,0.9945205479452055,"Dataset sheets: Gebru et al. (2018) have proposed to accompany every dataset with a dataset sheet.
Such a document is naturally complementary to the CLEVA-Compass, as the latter merely captures
the way in which data is used across continuous tasks, but not the underlying pipeline behind a
dataset’s creation and assembly. The prerequisite to specify the following aspects thus remains:
composition, collection process, pre-processing, cleaning, labeling, distribution, maintenance, and
ethical considerations. Dataset sheets are thus an important additional effort, which provide several
questions and guidelines for a standardized way to document the various elements surrounding the
former dataset aspects. Any new continual learning benchmark should therefore continue to provide
a comprehensively documented dataset sheet."
REFERENCES,0.9972602739726028,"Software for replication: Finally, for the sake of completeness, we also mention that there exist
recent propositions for continual learning software tools, such as the Avalanche (Lomonaco et al.,
2021) and Sequoia (Normandin et al., 2021) libraries on top of the popular machine learning frame-
work PyTorch (Paszke et al., 2019). These efforts are naturally auxiliary in the sense that they do not
expose a method’s context, its caveats, ethical considerations or application intend, but rather just
provide the means for pure replication of particular experiments (if data loading, methods, random
seeds etc. are properly specified in the code)."
