Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00641025641025641,"We consider reinforcement learning with vectorial rewards, where the agent re-
ceives a vector of K ≥2 different types of rewards at each time step. The agent
aims to maximize the minimum total reward among the K reward types. Differ-
ent from existing works that focus on maximizing the minimum expected total
reward, i.e. ex-ante max-min fairness, we maximize the expected minimum total
reward, i.e. ex-post max-min fairness. Through an example and numerical exper-
iments, we show that the optimal policy for the former objective generally does
not converge to optimality under the latter, even as the number of time steps T
grows. Our main contribution is a novel algorithm, Online-ReOpt, that achieves
near-optimality under our objective, assuming an optimization oracle that returns
a near-optimal policy given any scalar reward. The expected objective value un-
der Online-ReOpt is shown to converge to the asymptotic optimum as T increases.
Finally, we propose ofﬂine variants to ease the burden of online computation in
Online-ReOpt, and we propose generalizations from the max-min objective to
concave utility maximization."
INTRODUCTION,0.01282051282051282,"1
INTRODUCTION"
INTRODUCTION,0.019230769230769232,"The prevailing paradigm in reinforcement learning (RL) concerns the maximization of a single
scalar reward. On one hand, optimizing a single scalar reward is sufﬁcient for modeling simple
tasks. On the other hand, in many complex tasks there are often multiple, potentially competing,
rewards to be maximized. Expressing the objective function as a single linear combination of the
rewards can be constraining and insufﬁciently expressive for the nature of these complex tasks. In
addition, a suitable choice of the linear combination is often not clear a priori."
INTRODUCTION,0.02564102564102564,"In this work, we consider the reinforcement learning with max-min fairness (RL-MMF) problem.
The agent accumulates a vector of K ≥1 time-average rewards ¯V1:T = ( ¯V1:T,k)K
k=1 ∈RK in T
time steps, and aims to maximize E[mink∈{1,...,K} ¯V1:T,k]. The maximization objective represents
ex-post max-min fairness, in contrast to the objective of ex-ante max-min fairness by maximizing
mink∈{1,...,K} E[ ¯V1:T,k]."
INTRODUCTION,0.03205128205128205,"Our main contributions are the design and analysis of the Online-ReOpt algorithm, which achieves
near-optimality for the ex-post max-min fairness objective. More speciﬁcally, the objective under
Online-ReOpt converges to the optimum as T increases. Our algorithm design involves a novel
adaptation of the multiplicative weight update method (Arora et al., 2012), in conjunction with
a judiciously designed re-optimization schedule. The schedule ensures that the agent adapts his
decision to the total vectorial reward collected at a current time point, while allowing enough time
for the currently adopted policy to converge before switching to another policy."
INTRODUCTION,0.038461538461538464,"En route, we highlight crucial differences between the ex-ante and ex-post max-min fairness objec-
tives, by showing that an optimal algorithm for the former needs not converge to the optimality even
when T increases. Finally, our results are extended to the case of maximizing E[g( ¯V1:T )], where g
is a Lipschitz continuous and concave reward function."
INTRODUCTION,0.04487179487179487,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.05128205128205128,"2
RELATED WORKS"
RELATED WORKS,0.057692307692307696,"The Reinforcement Learning with Max-Min Fairness (RL-MMF) problem described is related to an
emerging body of research on RL with ex-ante concave reward maximization. The class of ex-ante
concave reward maximization problems include the maximization of g(E[ ¯V1:T ]), as well as its ex-
ante variants, including the long term average variant g(E[limT →∞¯V1:T ]) and its inﬁnite horizon
discounted reward variant. The function g : RK →R is assumed to be concave."
RELATED WORKS,0.0641025641025641,"The class of ex-ante concave reward maximization problems is studied by the following research
works. Chow et al. (2017) study the case where g is specialized to the Conditional Value-at-Risk
objective. Hazan et al. (2019) study the case when g models the entropy function over the probability
distribution over the state space, in order to construct a policy which induces a distribution over the
state space that is as close to the uniform distribution as possible. Miryooseﬁet al. (2019) study
the case of minimizing the distance between E[ ¯V1:T ] and a target set in RK. Lee et al. (2019) study
the objective of state marginal matching, which aims to make the state marginal distribution match
a given target state distribution. Pareto optimality of E[ ¯V1:T ] and its ex-ante variants are studied in
(Mannor & Shimkin, 2004; G´abor et al., 1998; Barrett & Narayanan, 2008; Van Moffaert & Now´e,
2014). Lastly, a recent work Zahavy et al. (2021) provides a unifying framework that encompasses
many of the previously mentioned works, by studying the problem of maximizing g(E[ ¯V1:T ]) and
its ex-ante variants, where g is concave and Lipschitz continuous. Our contributions, which concern
the ex-post max-min fairness E[mink∈{1,...,K} ¯V1:T,k] and its generalization to the ex-post concave
case, are crucially different from the body of works on the ex-ante case. The difference is further
highlighted in the forthcoming Section 3.2."
RELATED WORKS,0.07051282051282051,"Additionally, a body of works Altman (1999); Tessler et al. (2019); Le et al. (2019); Liu et al.
(2020) study the setting where g is a linear function, subject to the constraint that E[ ¯V1:T ] (or its
ex-ante variants) is contained in a convex feasible region, such as a polytope. There is another line
of research works Tarbouriech & Lazaric (2019); Cheung (2019); Brantley et al. (2020) focusing
on various online settings. The works Tarbouriech & Lazaric (2019); Cheung (2019) focus on the
ex-post setting like ours, but they crucially assume that the underlying g is smooth, which is not the
case for our max-min objective nor the case of Lipschitz continuous concave functions. In addition,
the optimality gap (quantiﬁed by the notion of regret) degrades linearly with the number of states,
which makes their applications to large scale problems challenging. Brantley et al. (2020) focus on
the ex-ante setting, different from our ex-post setting, and their optimality gap also degrades linearly
with the number of states."
MODEL,0.07692307692307693,"3
MODEL"
MODEL,0.08333333333333333,"Set up. An instance of the Reinforcement Learning with Max-Min Fairness (RL-MMF) problem is
speciﬁed by the tuple (S, s1, A, T, O). The set S is a ﬁnite state space, and s1 ∈S is the initial
state. In the collection A = {As}s∈S, the set As contains the actions that the agent can take when
he is at state s. Each set As is ﬁnite. The quantity T ∈N is the number of time steps."
MODEL,0.08974358974358974,"When the agent takes action a ∈As at state s, he receives the array of stochastic outcomes
(s′, U(s, a)), governed by the outcome distribution O(s, a). For brevity, we abbreviate the rela-
tionship as (s′, U(s, a)) ∼O(s, a). The outcome s′ ∈S is the subsequent state he transits to. The
outcome U(s, a) = (Uk(s, a))K
k=1 is a random vector lying in [−1, 1]K almost surely. The random
variable Uk(s, a) is the amount of type-k stochastic reward the agent receives. We allow the random
variables s′, U1(s, a), . . . UK(s, a) to be arbitrarily correlated."
MODEL,0.09615384615384616,"Dynamics. At time t ∈{1, . . . T}, the agent observes his current state st. Then, he selects an action
at ∈Ast. After that, he receives the stochastic feedback (st+1, Vt(st, at)) ∼O(st, at). We denote
Vt(st, at) = (Vt,k(st, at))K
k=1, where Vt,k(st, at) is the type-k stochastic reward received at time t.
The agent select the actions {at}T
t=1 with a policy π = {πt}T
t=1, which is a collection of functions.
For each t, the function πt inputs the history Ht−1 = ∪t−1
q=1{sq, aq, Vq(sq, aq)} and the current state
{st}, and outputs at ∈Ast. We use the notation aπ
t to highlight that the action is chosen under
policy π. A policy π is stationary if for all t, Ht−1, st it holds that πt(Ht−1, st) = ¯π(st) for some
function ¯π, where ¯π(s) ∈As for all s. With a slight abuse of notation, we identify a stationary
policy with the function ¯π."
MODEL,0.10256410256410256,Under review as a conference paper at ICLR 2022
MODEL,0.10897435897435898,"Objective. We denote ¯V π
1:t =
1
t
Pt
q=1 Vq(sq, aπ
q ) as the time average vectorial reward during
time 1 to t under policy π. The agent’s over-arching goal is to design a policy π that maximizes
E[gmin( ¯V π
1:T )], where gmin : RK →R is deﬁned as gmin(v) = mink∈{1,...,K} vk. Denoting ¯V π
1:T,k
as the k-th component of the vector ¯V π
1:T , the value gmin( ¯V π
1:T ) = mink ¯V π
1:T,k is the minimum time
average reward, among the reward types 1, . . . , K. The function gmin is concave, and is 1-Lipschitz
w.r.t. ∥· ∥∞over the domain RK."
MODEL,0.11538461538461539,"When K = 1, the RL-MMF problem reduces to the conventional RL problem with scalar reward
maximization. The case of K > 1 is more subtle. Generally, the optimizing agent needs to focus on
different reward types in different time steps, contingent upon the amounts of the different reward
types at the current time step. Since the max-min fairness objective could lead to an intractable
optimization problem, we aim to design a near-optimal policy for the RL-MMF problem."
REGRET,0.12179487179487179,"3.1
REGRET"
REGRET,0.1282051282051282,"We quantify the near-optimality of a policy π by the notion of regret, which is the difference between
a benchmark opt(P(gmin)) and the expected reward E[gmin( ¯V π
1:T )]. Formally, the regret of a policy π
in a T time step horizon is"
REGRET,0.1346153846153846,"Reg(π, T) = opt(P(gmin)) −E[gmin( ¯V π
1:T )].
(1)"
REGRET,0.14102564102564102,"The benchmark opt(P(gmin)) is a ﬂuid approximation to the expected optimum.
To deﬁne
opt(P(gmin)), we introduce the notation p = {p(s′|s, a)}s∈S,a∈As, where p(s′|s, a) is the prob-
ability of transiting to s′ from s, a.
In addition, we introduce v = {v(s, a)}s∈S,a∈As, where
v(s, a) = E[U(s, a)] is the vector of the K expected rewards. The benchmark opt(P(gmin)) is
the optimal value of the maximization problem P(gmin). For any g : RK →R, we deﬁne"
REGRET,0.14743589743589744,"P(g): max
x
g  
X"
REGRET,0.15384615384615385,"s∈S,a∈As
v(s, a)x(s, a)  "
REGRET,0.16025641025641027,"s.t.
X"
REGRET,0.16666666666666666,"a∈As
x(s, a) =
X"
REGRET,0.17307692307692307,"s′∈S,a′∈As′
p(s|s′, a′)x(s′, a′)
∀s ∈S
(2a) X"
REGRET,0.1794871794871795,"s∈S,a∈As
x(s, a) = 1
(2b)"
REGRET,0.1858974358974359,"x(s, a) ≥0
∀s ∈S, a ∈As.
(2c)"
REGRET,0.19230769230769232,"The concave maximization problem P(gmin) serves as a ﬂuid relaxation to RL-MMF. For each s ∈
S, a ∈As, the variable x(s, a) can be interpreted as the frequency of the agent visiting state s and
taking action a. The set of constraints (2a) stipulates that the rate of transiting out of a state s is equal
to the rate of transiting into the state s for each s ∈S, while the sets of constraints (2b , 2c) require
that {x(s, a)}s∈S,a∈As forms a probability distribution over the state-action pairs. Consequently,
opt(P(gmin)) is an asymptotic (in T) upper bound to the expected optimum."
REGRET,0.1987179487179487,Our goal is to design a policy π such that its regret1 Reg(T) satisﬁes
REGRET,0.20512820512820512,"Reg(T) = opt(P(gmin)) −E[gmin( ¯V π
1:T )] ≤D"
REGRET,0.21153846153846154,"T γ
(3)"
REGRET,0.21794871794871795,"holds for all initial state s1 ∈S and all T ∈N, with parameters D, γ > 0 independent of T. We
assume the access to an optimization oracle Λ, which returns a near-optimal policy given any scalar
reward. For ϑ ∈RK, deﬁne the linear function gϑ : RK →R as gϑ(w) = ϑ⊤w = PK
k=1 ϑkwk.
The oracle Λ inputs ϑ ∈RK, and outputs a policy π satisfying"
REGRET,0.22435897435897437,"opt(P(gϑ)) −E[gϑ( ¯V π
1:T )] = opt(P(gϑ)) −E[ϑ⊤¯V π
1:T ] ≤Dlin"
REGRET,0.23076923076923078,"T β
(4)"
REGRET,0.23717948717948717,"for all initial state s1 ∈S and all T ∈N, with parameters Dlin, β > 0 independent of T. By
assuming β > 0, we are assuming that the output policy π is near-optimal, in the sense that the
difference opt(P(gϑ)) −E[ϑ⊤¯V π
1:T ] converges to 0 as T tends to the inﬁnity. A higher β signiﬁes a"
REGRET,0.24358974358974358,1We omit the notation with π for brevity sake
REGRET,0.25,Under review as a conference paper at ICLR 2022
REGRET,0.2564102564102564,"faster convergence, representing a higher degree of near-optimality. We refer to ϑ as a scalarization
of v, with the resulting scalarized reward being ϑ⊤v(s, a) for each s, a."
REGRET,0.26282051282051283,"Our algorithmic frameworks involve invoking Λ as a sub-routine on different ϑ’s. In other words,
we assume an algorithmic sub-routine that solves the underlying RL problem with scalar reward
(the case of K = 1), and delivers an algorithm that ensures max-min fairness (the case of K ≥1).
Finally, while the main text focuses on gmin, our algorithm design and analysis can be generalized to
the case of concave g, as detailed in Appendix C."
REGRET,0.2692307692307692,"3.2
COMPARISON BETWEEN MAXIMIZING E[gMIN( ¯V π
1:T )] AND gMIN(E[ ¯V π
1:T ])"
REGRET,0.27564102564102566,"Before introducing our algorithms, we illustrate the difference between the objectives of maximizing
E[gmin( ¯V π
1:T )] and gmin(E[ ¯V π
1:T ]) by the deterministic instance in Figure 1, with initial state s1 = so."
REGRET,0.28205128205128205,"so
sr
sℓ"
REGRET,0.28846153846153844," 1
0

 0
0
"
REGRET,0.2948717948717949," 0
0
"
REGRET,0.30128205128205127," 0
1
"
REGRET,0.3076923076923077," 0
0
"
REGRET,0.3141025641025641," 0
0
"
REGRET,0.32051282051282054,"Figure 1: States and actions are
represented by circles and arcs."
REGRET,0.3269230769230769,"The ﬁgure depicts an instance with K = 2. An arc represents
an action that leads to a transition from its tail to its head. For
example, the arc from so to sℓrepresents the action aoℓ, with
p(sℓ| so, aoℓ) = 1. Likewise, the loop at sℓrepresents the
action aℓℓwith p(sℓ| sℓ, aℓℓ) = 1. Each arc is labeled with
its vectorial reward, which is deterministic. For example, with
certainty we have V (so, aoℓ) =
 0
0

and V (sℓ, aℓℓ) =
 0
1

."
REGRET,0.3333333333333333,"Consider two stationary policies πℓ, πr, deﬁned as πℓ(sr) = aro, πℓ(so) = aoℓ, πℓ(sℓ) = aℓℓand
πr(sr) = arr, πr(so) = aor, πr(sℓ) = aℓo. The policy πℓalways seeks to transit to sℓ, and then
loop at sℓindeﬁnitely, likewise for πr. With certainty, ¯V πℓ
1:T =
 
0
1−1/T

, ¯V πr
1:T =
 1−1/T
0

."
REGRET,0.33974358974358976,"The objective gmin(E[ ¯V π
1:T ]) is maximized by choosing πran uniformly at random from the collection
{πℓ, πr}. We have E[ ¯V πran
1:T ] =
 1/2−1/(2T )
1/2−1/(2T )

, leading to the optimal value of 1/2 −1/(2T). More
generally, existing research focuses on maximizing g(E[ ¯V π
1:T ]) for certain concave g, and the related
objectives of maximizing g(limT →∞E[ ¯V π
1:T ]) or g(E[P∞
t=1 αtVt(st, aπ
t )]), where α ∈(0, 1) is the
discount factor. In these research works, a near-optimal policy π is constructed by ﬁrst generating a
collection Π of stationary policies, then sampling π uniformly at random from Π."
REGRET,0.34615384615384615,"Interestingly, πran is sub-optimal for maximizing E[gmin( ¯V π
1:T )]. Indeed, Pr( ¯V πran
1:T =
 
0
1−1/T

) ="
REGRET,0.3525641025641026,"Pr( ¯V πran
1:T =
 1−1/T
0

) = 1/2, so we have E[gmin( ¯V πran
1:T )] = 0 for all T. Now, consider the deter-
ministic policy πsw, which ﬁrst follows πℓfor the ﬁrst ⌊T/2⌋time steps, then follows πr in the
remaining ⌈T/2⌉time steps. We have ¯V πsw
1:T,k ≥1/2 −2/T for each k ∈{1, 2}, meaning that
gmin( ¯V πsw
1:T ) ≥1/2 −2/T. Note that gmin(E[ ¯V πsw
1:T ]) ≥gmin(E[ ¯V πran
1:T ]) −2/T, so the policy πsw is also
near-optimal for maximizing gmin(E[ ¯V π
1:T ])."
REGRET,0.358974358974359,"Altogether, an optimal policy for maximizing gmin(E[ ¯V π
1:T ]) can be far from optimal for maximizing
E[gmin( ¯V π
1:T )]. In addition, for the latter objective, it is intuitive to imitate πsw, which is to partition
the horizon into episodes and run a suitable stationary policy during each episode. A weakness to
πsw is that its partitioning requires the knowledge on T. While our algorithm follows the intuition
to imitate πsw, we propose an alternate partitioning that allows does not require T as an input."
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.36538461538461536,"4
ONLINE-REOPT ALGORITHM FOR RL-MMF"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.3717948717948718,"We propose the Online-ReOpt algorithm, displayed in Algorithm 1. The algorithm runs in episodes.
An episode m ∈{1, 2, . . .} starts at time τ(m) (deﬁned in Line 2), and ends at time τ(m + 1) −1.
Before the start of episode m, the algorithm computes the scalarization ϑτ(m) based on the Mul-
tiplicative Weight Update (MWU), which we detail later. Then, the algorithm invokes the opti-
mization oracle Λ, which returns a policy πm that is near-optimal for the MDP with scalar rewards
rm = {rm(s, a)}s,a, where rm(s, a) = ϑ⊤
τ(m)v(s, a). Note that we only assume a black-box access
to Λ, and the parameters Dlin, β do not need to be input to the Algorithm. Finally, the algorithm runs
policy πm during episode m. The Online Re-Opt algorithm is an anytime algorithm, since it does
not require T as an input. Rather, it requires knowing T only during the terminal time step T. To
complete the description of the algorithm, we provide the details about the scalarization."
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.3782051282051282,Under review as a conference paper at ICLR 2022
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.38461538461538464,Algorithm 1 Online-ReOpt for gmin
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.391025641025641,"1: Inputs: Optimization oracle Λ.
2: Set τ(m) = ⌊m3/2⌋for m ∈N.
3: for Episode m = 1, 2, . . . do
4:
Deﬁne ϑτ(m) according to (5).
5:
Compute policy πm ←Λ(ϑτ(m)).
6:
for Time t = τ(m), . . . , τ(m + 1) −1 do
7:
Choose action at = πm(st).
8:
Observe the outcomes Vt(st, at) and the next state st+1.
9:
if t = T then
10:
Break the for loops and terminate the algorithm.
11:
end if
12:
end for
13: end for"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.3974358974358974,"Scalarization by MWU. At a time step t, we deﬁne the scalarization ϑt = {ϑt,k}K
k=1 as"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.40384615384615385,"ϑt,k =
exp
h
−ηt−1
Pt−1
j=1 Vj,k(sj, aj)
i"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.41025641025641024,"PK
κ=1 exp
h
−ηt−1
Pt−1
j=1 Vj,κ(sj, aj)
i,
(5) where"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.4166666666666667,"ηt−1 =
√log K
max{(t −1)2/3, 1}.
(6)"
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.4230769230769231,"In particular, at the start of each episode m, we apply (5) with t = τ(m) in Line 4. For the case
m = 1, we have ϑτ(1),k = 1/K for all k ∈{1, . . . , K}, meaning that all reward types are assigned
with the same weight at the beginning. The exponent 2/3 in the learning rate ητ(m)−1 in (5) is
different from the conventional choice of 1/2 (Arora et al., 2012). Our exponent is chosen for
optimizing the resulting regret bound from our forthcoming analysis. We follow the approach in
Chapter 7 in (Orabona, 2019) to deﬁne a time-varying learning rate."
ONLINE-REOPT ALGORITHM FOR RL-MMF,0.42948717948717946,"The scalarization ϑt by (5) promotes max-min fairness.
Consider two reward types k, k′ with
Pt−1
q=1 Vq,k(sq, aq) > Pt−1
q=1 Vq,k′(sq, aq). We have ϑt,k′ > ϑt,k, meaning that a higher weight
is assigned to reward type k′ than type k. This implies that there is a higher emphasis on increasing
the type-k′ reward, which is in shortage as compared to type k, than the type-k reward. Hence,
max-min fairness is promoted."
THEORETICAL GUARANTEES,0.4358974358974359,"4.1
THEORETICAL GUARANTEES"
THEORETICAL GUARANTEES,0.4423076923076923,We provide the following theoretical guarantee for Online-ReOpt.
THEORETICAL GUARANTEES,0.44871794871794873,"Theorem 1 Consider the RL-MMF problem. Online-ReOpt, displayed in Algorithm 1, satisﬁes"
THEORETICAL GUARANTEES,0.4551282051282051,Reg(T) ≤114√log K
THEORETICAL GUARANTEES,0.46153846153846156,"T 1/3
+ 144Dlin"
THEORETICAL GUARANTEES,0.46794871794871795,"T β/3 ,
(7)"
THEORETICAL GUARANTEES,0.47435897435897434,"where Dlin, β are parameters related to the optimization oracle Λ."
THEORETICAL GUARANTEES,0.4807692307692308,"Theorem 1 is a generalization result, in the sense that it generalizes the ability of achieving near-
optimality for the case of K = 1 to the case of K ≥1. Indeed, as long as β > 0, meaning that
the regret of the optimization oracle Λ diminishes with a growing T on any RL with scalar reward
problem, the regret bound (7) in Theorem 1 also tends to zero as T increases."
THEORETICAL GUARANTEES,0.48717948717948717,"Theorem 1 is proved in Appendix section C.3. The ﬁrst regret term in (7) arises from two sources:
(a) the regret of the MWU algorithm, (b) the update delay on the scalarization due to the episodic
structure. To elaborate on (b), consider a time step t in episode m. Recall that the scalarization ϑt by
(5) promotes max-min fairness, and ideally we should have employed the policy returned by Λ(ϑt)
at time t. In contrast, in Online-ReOpt the action at is determined by πm, the output of Λ(ϑτ(m))."
THEORETICAL GUARANTEES,0.4935897435897436,Under review as a conference paper at ICLR 2022
THEORETICAL GUARANTEES,0.5,"Item (b) accounts for the regret due to using ϑτ(m) rather then ϑt. We crucially use the fact that ϑt
are slowly changing in t so that the resulting regret is still diminishing with t."
THEORETICAL GUARANTEES,0.5064102564102564,"The second term in (7) is due to the regret of the optimization oracle Λ. The exponent β/3 in the term
is less than the exponent β in (4), as each policy πm is run for only τ(m+1)−τ(m) = O(
p"
THEORETICAL GUARANTEES,0.5128205128205128,"τ(m))
many time steps. Our design of {τ(m)}M
m=1 allows a shorter time frame for πm to converge to its
expected reward, as compared to running a policy for T steps in (4). When we increase the episode
length τ(m+1)−τ(m), the regret due to (b) increases, while the regret due to the second term in (7)
decreases, and vice versa. Our design of {τ(m)}M
m=1 strikes an optimal (in terms of our analysis)
balance between these two sources of regret."
THEORETICAL GUARANTEES,0.5192307692307693,"The regret bound (7) does not feature a direct dependence on the sizes of the state and action spaces.
The dependence on the hardness of the underlying MDP is only reﬂected through the parameters
Dlin, β. Therefore, apart from the deterioration of the exponent β to β/3 and the ﬁrst term in (7),
our algorithm does not introduce any overhead in the generalization from the case of K = 1 to
the case of K ≥1. Improving the exponent β/3 in (7) is an interesting open question. Finally,
Theorem 1 is generalized to the case of maximizing a concave utility objective E[g( ¯V π
1:T )], where
g is Lipschitz continuous and concave. We detail the generalization in the model, algorithm and
theoretical results in Appendix C.1."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5256410256410257,"4.2
OFFLINE VARIANTS TO ONLINE-REOPT"
OFFLINE VARIANTS TO ONLINE-REOPT,0.532051282051282,"While the Online-ReOpt Algorithm achieves near-optimality, the efﬁciency of its implementation
could be hindered by the need of online computation in Line 5 in Algorithm 1. Indeed, in order to
compute πm, the agent has to input the optimization oracle Λ and the scalarization ϑτ(m), which is
only known at the end of time step τ(m) −1. In the case when the optimization oracle involves
heavy computation, for example training deep neural networks, such online computation may not be
realistic."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5384615384615384,"In this section, we propose Ofﬂine-ReOpt, which is a variant of Online-ReOpt that does not require
invoking Λ during the horizon. The Ofﬂine-ReOpt is obtained from the Online-ReOpt by modifying
two lines in Algorithm 1, as enumerated below. The full algorithm of Ofﬂine-ReOpt is provided in
Appendix section A.1."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5448717948717948,"1. Replace the input of Λ in Line 1 with the input of the policy family Π = {(ϑ, π(ϑ))}ϑ∈Ω.
The index set Ωis a ﬁnite subset of {ϑ ∈RK : ∥ϑ∥1 = 1, ϑ ≥0}, the collection of all
possible scalarizations. For each ϑ ∈Ω, the policy π(ϑ) is the output of Λ(ϑ)."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5512820512820513,2. Replace the online computation in Line 5 with these two lines:
OFFLINE VARIANTS TO ONLINE-REOPT,0.5576923076923077,"(a) Identify ˜ϑτ(m) ∈Ωthat achieves minϑ∈Ω
ϑ −ϑτ(m)

1 ."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5641025641025641,(b) Select policy πm = π( ˜ϑτ(m)).
OFFLINE VARIANTS TO ONLINE-REOPT,0.5705128205128205,"In item (1), all the policies in Π are computed before the execution of the algorithm, unlike the case
in Online-ReOpt. Consequently, in item (2), the selection of policy πm does not require invoking
the optimization oracle Λ."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5769230769230769,"The main idea behind item (2) is that, in the case when the desired scalarization ϑτ(m) does not lie in
Ω, we chooses the surrogate scalarization ˜ϑτ(m) that is closest to ϑτ(m), so that the resulting policy
π( ˜ϑτ(m)) will be a reasonable approximation to the desired policy π(ϑτ(m))."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5833333333333334,"In order for the surrogate ˜ϑτ(m) to be close to ϑτ(m), it is desirable for the ﬁnite index set Ωto be so
diverse that every scalarization ϑτ(m) would be in a close neighborhood of a scalarization in Ω. We
propose two families of Ωfor the desired diversiﬁcation. The ﬁrst is the random point family, de-
tailed in Appendix section A.2. The family is constructed by sampling random points in the domain
{ϑ ∈RK : ∥ϑ∥1 = 1, θ ≥0} of all possible scalarizations. The second is the imitation based fam-
ily, also detailed in Appendix section A.2. The family is constructed by ﬁrst running Online-ReOpt
multiple times, then collecting the scalarizations and the corresponding policies generated."
OFFLINE VARIANTS TO ONLINE-REOPT,0.5897435897435898,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5961538461538461,"5
EXPERIMENTS"
EXPERIMENTS,0.6025641025641025,"We evaluate our proposed algorithms and benchmark algorithms in a controlled queueing system
involving vectorial rewards. For each of the algorithms, we ﬁrst run the algorithm for Zpo = Zan ×Ξ
independent trials, resulting in the Zpo average vectorial rewards2 { ¯V (zan,ξ)
1:T
}1≤zan≤Zan,1≤ξ≤Ξ. We
plot the following three quantities against T:"
EXPERIMENTS,0.6089743589743589,"• Ex-post Fairness: ¯Ψ =
1
Zan
1
Ξ
PZan
zan=1
PΞ
ξ=1 gmin

¯V (zan,ξ)
1:T

, an estimate to E[gmin( ¯V1:T )]."
EXPERIMENTS,0.6153846153846154,"• Ex-ante Fairness: ¯Γ =
1
Zan
PZan
zan=1 gmin

1
Ξ
PΞ
ξ=1 ¯V (zan,ξ)
1:T

, an estimate to gmin(E[ ¯V1:T ])."
EXPERIMENTS,0.6217948717948718,"• Type k rewards for each 1 ≤k ≤K: ¯Φk =
1
Zan
1
Ξ
PZan
zan=1
PΞ
ξ=1 ¯V (zan,ξ)
1:T,k , an estimate to
E[ ¯V1:T,k]."
EXPERIMENTS,0.6282051282051282,"We deﬁne the upper and lower error bars respectively as the 75 and 25-percentiles of the data, see
Appendix section B.1 for details. For the forthcoming discussions, we denote ek as the k-th standard
basis vector for k ∈{1, . . . K} in RK. In addition, we denote 1K, 0K as the all one vector and the
all zero vector in RK."
QUEUING NETWORK,0.6346153846153846,"5.1
QUEUING NETWORK"
QUEUING NETWORK,0.6410256410256411,"Queuing problems are studied extensively due to their relevance in ﬁelds such as manufacturing and
in communication systems. In our evaluation, we focus on a discrete-time queuing system. The
queuing network that we have tested our algorithms on, consisting of two servers and four queues
arranged in a bidirectional fashion, has been previously studied in works by Rybko & Stolyar (1992),
Kumar & Seidman (1990), Chen & Meyn (1998), de Farias & Van Roy (2003) and Banijamali et al.
(2019). This network is shown in Figure 2."
QUEUING NETWORK,0.6474358974358975,Figure 2: A bi-directional four-queue network
QUEUING NETWORK,0.6538461538461539,"There are two servers in the system. Server 1 only serves Queue 1 or Queue 4 with service rates
µ1 = 0.3 and µ4 = 0.3 respectively, and where Server 2 similarly only serves Queue 2 or Queue 3
at the rates of µ2 = 0.3 and µ3 = 0.3. Arrivals occur at a rate of λ1 = 0.2 and λ2 = 0.2 at Queues
1 and 3 respectively. An arrival that gets served at Queue 1 by Server 1 then progresses to Queue 2,
and only leaves the system after it has been served by Server 2. Likewise, arrivals at Queue 3 have
to be served by Server 2, before moving on to Queue 4 to be served by Server 1 in order to leave
the system. Each queue i has a maximum length of Li = 9, and a customer is rejected at a queue if
the queue is at its full capacity. Conversely, an empty queue remains at length 0 even if an action is
taken to serve that queue."
QUEUING NETWORK,0.6602564102564102,"The state of the system is thus deﬁned by the vector xt = (xt,1, xt,2, xt,3, xt,4) whereby xt,i rep-
resents the length of the queue i at time t. At each time step t, a decision has to be made by each
server to serve only one or neither of its queues, which we can represent by a 4-component vector
at = (at,1, at,2, at,3, at,4) ∈{0, 1}4, where at,i = 1 indicates the decision to serve Queue i at time
t, and at,i = 0 otherwise. Note that the condition of being able to only serve one queue at each
server naturally imposes the constraints at,1 + at,4 ≤1 and at,2 + at,3 ≤1 at each t, meaning that
As = {a ∈{0, 1}4 : a1 + a4 ≤1, a2 + a3 ≤1} for each s."
QUEUING NETWORK,0.6666666666666666,"2To avoid clutter, we omit the upper-script for the algorithm."
QUEUING NETWORK,0.6730769230769231,Under review as a conference paper at ICLR 2022
QUEUING NETWORK,0.6794871794871795,"The transition dynamics for the system can then deﬁned by the following equation when 0 < xt,i <
Li, where ei refers to the basis vector in R4:"
QUEUING NETWORK,0.6858974358974359,xt+1 =
QUEUING NETWORK,0.6923076923076923,"









"
QUEUING NETWORK,0.6987179487179487,"








"
QUEUING NETWORK,0.7051282051282052,"xt + e1
with probability λ1
xt + e3
with probability λ2
xt + e2 −e1
with probability µ1a1
xt −e2
with probability µ2a2
xt + e4 −e3
with probability µ3a3
xt −e4
with probability µ4a4
xt
otherwise (8)"
QUEUING NETWORK,0.7115384615384616,"We deﬁne the type-i reward at time t as Vt,i(x, a) = 1 −xt,i"
QUEUING NETWORK,0.717948717948718,"Li , for i ∈{1, . . . , 4}. Recall that xt,i is
the queue length of Queue i at time t. The reward Vt,i(x, a) is equal to 1 if Queue i is empty, and the
reward Vt,i(x, a) decreases linearly with the length of Queue i at time t. In particular, Vt,i(x, a) = 0
if Queue i is full. Altogether, the agent’s reward for Queue i at time t positively correlates with the
degree of idleness of the Queue. The maximization of gmin( ¯V1:T ) = min1≤i≤4 ¯V1:T,i is equivalent
to the minimization of time-average queue lengths among all queues, hence enforcing all queues to
be stable simultaneously."
SIMULATION RESULTS,0.7243589743589743,"5.1.1
SIMULATION RESULTS"
SIMULATION RESULTS,0.7307692307692307,"In our simulation, we evaluate 5 algorithms. Three of them are our proposed algorithms, namely
Online-ReOpt, Ofﬂine-ReOpt with Random Point Family and Ofﬂine-ReOpt with Imitation based
family. The other two are existing baselines. The Meta Algorihtm by Zahavy et al. (2021) is
the state-of-the-art for maximizing gmin(E[ ¯V1:T ]), while the longer queue ﬁrst heuristic is a well-
established algorithm in the queuing theory literature. As the name suggests, each server serves
the longer of the two queues at each time round. We ran each of the algorithms with the following
parameters: T = 100000, Zan = 10 and Ξ = 100, meaning Zpo = Zan × Ξ = 1000.3 All of the al-
gorithms employ the same optimization oracle Λ, with the same hyper-parameters and architecture,
a double deep Q-learning network algorithm (Double DQN) by Hasselt et al. (2016)."
SIMULATION RESULTS,0.7371794871794872,Figure 3: Ex-ante Fairness of various algorithms in a queuing network
SIMULATION RESULTS,0.7435897435897436,"Figure 3 plots the quantity ¯Ψ against T under the 5 algorithms. Notice in Figure 3 how the Of-
ﬂine and Online-ReOpt algorithms, as well as the Meta Algorithm by Zahavy et al. (2021) perform
similarly well in terms of ex-ante fairness. Among them, the Meta Algorithm has the best perfor-
mance, since the Re-Optimization schedule in our proposed algorithms compromises the ex-ante
fairness objective. All algorithms demonstrate converging behavior, in the sense that the error bars
diminishes as T grows."
SIMULATION RESULTS,0.75,"3Except for Online-ReOpt, where we set Ξ = 5 since running an online algorithm for 1000 trials is not as
practical as running an ofﬂine algorithm, which only needs to be trained once."
SIMULATION RESULTS,0.7564102564102564,Under review as a conference paper at ICLR 2022
SIMULATION RESULTS,0.7628205128205128,Figure 4: Ex-post Fairness of various algorithms in a queuing network
SIMULATION RESULTS,0.7692307692307693,"Figure 4 plots the quantity ¯Γ against T under the 5 algorithms. In terms of ex-post fairness, the Of-
ﬂine and Online-ReOpt algorithms perform signiﬁcantly better than Meta Algorithm and the Longer
Queue First Heuristic. The sub-optimality of the Meta Algorithm corroborates with Section 3.2 that
policies designed for the ex-ante fairness objective could be far from optimal for the ex-post fairness
objective. While the Meta Algorithm has a similar performance to the Longer Queue First heuristic,
the former has a signiﬁcantly wider error bar than the latter, meaning that the latter is more stable."
SIMULATION RESULTS,0.7756410256410257,"Figure 5: Type k-rewards for 1 ≤k ≤K of various algorithms in a queuing network. Figure is read
from left to right, top to bottom."
SIMULATION RESULTS,0.782051282051282,"Figure 5 plots Φi against T for i ∈{1, . . . , 4}. In a nutshell, the plotted lines explain the trends in
Figure 3, while the error bars shed light on the trends in Figure 4. Firstly, the plotted lines indicate
that the Meta Algorithm has the highest (or close to the highest) individual average reward Φi for
each queue, signifying that the Meta Algorithm has the highest E[ ¯V1:T,i] for each i ∈{1, . . . , 4}.
This explains the superiority of the Meta Algorithm shown in Figure 3."
SIMULATION RESULTS,0.7884615384615384,"When we focus on the error bars, the plots in Figure 5 tell a different story. Notably, the error bars
for the Meta Algorithm is signiﬁcantly wider than others, meaning that the Zpo trials of the Meta
Algorithm have signiﬁcantly different results from one another.4 When we unpack the summands
in Φ1, . . . , Φ4 and compute the minimum reward in each trial, it results in Figure 4, which is vastly
different from Figure 3, signifying the ex-ante and ex-post objectives are fundamentally different."
SIMULATION RESULTS,0.7948717948717948,"As a ﬁnal remark, our numerical experiments do not imply that the Longer Queue Heuristic is a
worse algorithm than the other 4 algorithms. Indeed, the Longer Queue Heuristic does not require
the knowledge of λ1, λ2, µ1, . . . , µ4, whereas the other 4 algorithms crucially uses these parameters
for generating their policies. In addition, the Longer Queue Heuristic is computationally much less
onerous than the others. Finally, the Longer Queue Heuristic demonstrates converging behaviors in
all the plots, in the sense that the error bars diminish when T increases."
SIMULATION RESULTS,0.8012820512820513,"4It is helpful to revisit Section 3.2, where Zpo trials would result in ≈Zpo/2 outcomes of
 
0
1−1/T

and"
SIMULATION RESULTS,0.8076923076923077,"≈Zpo/2 outcomes of
 1−1/T
0

."
SIMULATION RESULTS,0.8141025641025641,Under review as a conference paper at ICLR 2022
REFERENCES,0.8205128205128205,REFERENCES
REFERENCES,0.8269230769230769,"E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999."
REFERENCES,0.8333333333333334,"Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-
algorithm and applications. Theory of Computing, 8(6):121–164, 2012."
REFERENCES,0.8397435897435898,"Ershad Banijamali, Yasin Abbasi-Yadkori, Mohammad Ghavamzadeh, and Nikos Vlassis. Optimiz-
ing over a restricted policy class in mdps. In Proceedings of the Twenty-Second International
Conference on Artiﬁcial Intelligence and Statistics, volume 89, pp. 3042–3050. PMLR, 16–18
Apr 2019."
REFERENCES,0.8461538461538461,"Leon Barrett and Srini Narayanan. Learning all optimal policies with multiple criteria. In Proceed-
ings of the 25th International Conference on Machine Learning, ICML ’08, pp. 41–47, New
York, NY, USA, 2008. Association for Computing Machinery.
ISBN 9781605582054.
doi:
10.1145/1390156.1390162. URL https://doi.org/10.1145/1390156.1390162."
REFERENCES,0.8525641025641025,"Kiant´e Brantley, Miroslav Dud´ık, Thodoris Lykouris, Sobhan Miryooseﬁ, Max Simchowitz, Alek-
sandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex
and knapsack settings. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.8589743589743589,"R.-R. Chen and S. Meyn. Value iteration and optimization of multiclass queueing networks. In Pro-
ceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171), volume 1,
pp. 50–55 vol.1, 1998. doi: 10.1109/CDC.1998.760588."
REFERENCES,0.8653846153846154,"Wang Chi Cheung. Regret minimization for reinforcement learning with vectorial feedback and
complex objectives. In Advances in Neural Information Processing Systems 32, pp. 724–734,
2019."
REFERENCES,0.8717948717948718,"Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained rein-
forcement learning with percentile risk criteria. J. Mach. Learn. Res., 18(1):6070–6120, January
2017. ISSN 1532-4435."
REFERENCES,0.8782051282051282,"D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic pro-
gramming. Operations Research, 51(6):850–865, 2003."
REFERENCES,0.8846153846153846,"Zolt´an G´abor, Zsolt Kalm´ar, and Csaba Szepesv´ari. Multi-criteria reinforcement learning. In Pro-
ceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pp. 197–205,
San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc. ISBN 1558605568."
REFERENCES,0.8910256410256411,"Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, AAAI’16,
pp. 2094–2100. AAAI Press, 2016."
REFERENCES,0.8974358974358975,"Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum en-
tropy exploration. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 2681–2691. PMLR, 09–15 Jun 2019."
REFERENCES,0.9038461538461539,"P.R. Kumar and T.I. Seidman. Dynamic instabilities and stabilization methods in distributed real-
time scheduling of manufacturing systems.
IEEE Transactions on Automatic Control, 35(3):
289–298, 1990. doi: 10.1109/9.50339."
REFERENCES,0.9102564102564102,"Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3703–3712.
PMLR, 09–15 Jun 2019."
REFERENCES,0.9166666666666666,"Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-
dinov. Efﬁcient exploration via state marginal matching. CoRR, abs/1906.05274, 2019. URL
http://arxiv.org/abs/1906.05274."
REFERENCES,0.9230769230769231,"Yongshuai Liu, Jiaxin Ding, and Xin Liu.
Ipo: Interior-point policy optimization under con-
straints.
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(04):4940–4947,
Apr. 2020. doi: 10.1609/aaai.v34i04.5932. URL https://ojs.aaai.org/index.php/
AAAI/article/view/5932."
REFERENCES,0.9294871794871795,Under review as a conference paper at ICLR 2022
REFERENCES,0.9358974358974359,"Shie Mannor and Nahum Shimkin. A geometric approach to multi-criterion reinforcement learning.
J. Mach. Learn. Res., 5:325–360, 2004."
REFERENCES,0.9423076923076923,"Sobhan Miryooseﬁ, Kiant´e Brantley, Hal Daume III, Miro Dudik, and Robert E Schapire. Reinforce-
ment learning with convex constraints. In Advances in Neural Information Processing Systems
32, pp. 14093–14102. 2019."
REFERENCES,0.9487179487179487,"Francesco Orabona. A modern introduction to online learning. CoRR, abs/1912.13213, 2019."
REFERENCES,0.9551282051282052,"A. N. Rybko and Aleksandr Stolyar. On the ergodicity of stochastic processes describing functioning
of open queueing networks. Problemy Peredachi Informatsii, (3):3–26, July 1992. ISSN 0555-
2923."
REFERENCES,0.9615384615384616,"Shai Shalev-Shwartz. Online learning: Theory, algorithms, and applications, Jul 2007."
REFERENCES,0.967948717948718,"Shai Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107–194, 2012."
REFERENCES,0.9743589743589743,"Jean Tarbouriech and Alessandro Lazaric. Active exploration in markov decision processes. In The
22nd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2019, volume 89,
pp. 974–982. PMLR, 2019."
REFERENCES,0.9807692307692307,"Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
International Conference on Learning Representations, 2019."
REFERENCES,0.9871794871794872,"Kristof Van Moffaert and Ann Now´e. Multi-objective reinforcement learning using sets of pareto
dominating policies. J. Mach. Learn. Res., 15(1):3483–3512, January 2014. ISSN 1532-4435."
REFERENCES,0.9935897435897436,"Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex mdps. ArXiv, abs/2106.00661, 2021."
