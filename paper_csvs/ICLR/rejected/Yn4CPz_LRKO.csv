Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003389830508474576,"Conditional generative models aim to learn the underlying joint distribution of
data and labels, and thus realize conditional generation. Among them, auxiliary
classiÔ¨Åer generative adversarial networks (AC-GAN) have been widely used, but
suffer from the problem of low intra-class diversity on generated samples. In
this paper, we point out that the fundamental reason is that the classiÔ¨Åer of AC-
GAN is generator-agnostic, and therefore cannot provide informative guidance
to the generator to approximate the target distribution, resulting in minimization
of conditional entropy that decreases the intra-class diversity. Motivated by this
observation, we propose a novel conditional GAN with auxiliary discriminative
classiÔ¨Åer (ADC-GAN) to resolve the problem of AC-GAN. SpeciÔ¨Åcally, the pro-
posed auxiliary discriminative classiÔ¨Åer becomes generator-aware by recognizing
the labels of the real data and the generated data discriminatively. Our theoretical
analysis reveals that the generator can faithfully replicate the target distribution
even without the original discriminator, making the proposed ADC-GAN robust
to the hyper-parameter and stable during the training process. Extensive experi-
mental results on synthetic and real-world datasets demonstrate the superiority of
ADC-GAN on conditional generative modeling compared to competing methods."
INTRODUCTION,0.006779661016949152,"1
INTRODUCTION"
INTRODUCTION,0.010169491525423728,"Generative adversarial networks (GANs) (Goodfellow et al., 2014) have been gained great progress
in learning high-dimensional, complex data distribution such as natural images (Karras et al., 2019;
2020b;a; Brock et al., 2019). Standard GANs consist of a generator network that transfers a latent
code sampled from a tractable distribution in the latent space to a data point in the data space and a
discriminator network that attempts to distinguish between the real data and the generated one. The
generator is trained in an adversarial game against the discriminator such that it can replicate the data
distribution at the Nash equilibrium of the game. Remarkably, the training of GANs is notoriously
unstable to reach the equilibrium, and thereby the generator is prone to mode collapse (Salimans
et al., 2016; Lin et al., 2018; Chen et al., 2019). In addition, practitioners are interested in controlling
the properties of the generated samples (Yan et al., 2015; Tan et al., 2020) in practical applications.
A key solution to address the above issues is conditioning, leading to conditional GANs (Mirza &
Osindero, 2014)."
INTRODUCTION,0.013559322033898305,"Conditional GANs (cGANs) is a family variant of GANs that leverages the side information from
annotated labels of samples to implement and train a conditional generator, and therefore achieve
conditional image generation from class-label (Odena et al., 2017; Miyato & Koyama, 2018; Brock
et al., 2019) or text (Reed et al., 2016; Xu et al., 2018; Zhu et al., 2019). To implement the conditional
generator, the common technique nowadays injects the conditional information via conditional batch
normalization (de Vries et al., 2017). To train the conditional generator, a lot of efforts focus on
effectively injecting the conditional information into the discriminator or classiÔ¨Åer (Odena, 2016;
Miyato & Koyama, 2018; Zhou et al., 2018; Kavalerov et al., 2021; Kang & Park, 2020; Zhou et al.,
2020). Among them, the auxiliary classiÔ¨Åer generative adversarial network (AC-GAN) (Odena et al.,
2017) has been widely used due to its simplicity and extensibility. SpeciÔ¨Åcally, AC-GAN utilizes
an auxiliary classiÔ¨Åer that Ô¨Årst attempts to recognize the label of data and then teaches the generator
to produce label-consistent (classiÔ¨Åable) data. However, it has been reported that AC-GAN suffers
from the low intra-class diversity problem on generated samples, especially on datasets with a large
number of classes (Odena et al., 2017; Shu et al., 2017; Gong et al., 2019)."
INTRODUCTION,0.01694915254237288,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020338983050847456,"In this paper, we point out that the fundamental reason for the low intra-class diversity problem of
AC-GAN is that the classiÔ¨Åer is agnostic to the generated data distribution and thus cannot provide
informative guidance to the generator in learning the target distribution. Motivated by this obser-
vation, we propose a novel conditional GAN with an auxiliary discriminative classiÔ¨Åer, namely
ADC-GAN, to resolve the problem of AC-GAN by enabling the classiÔ¨Åer to be aware of the gen-
erated data distribution. To this end, the discriminative classiÔ¨Åer is trained to distinguish between
the real and generated data while recognizing their labels. The discriminative property enables the
classiÔ¨Åer to provide the discrepancy between the real and generated data distributions analogy to
the discriminator, and the classiÔ¨Åcation property allows it to capture the dependencies between the
data and labels. We show in theory that the generator of the proposed ADC-GAN can replicate the
joint data and label distribution under the guidance of the discriminative classiÔ¨Åer at the optima even
without the discriminator, making our method robust to hyper-parameter and stable on training. We
also discuss the superiority of ADC-GAN compared to two most related works (TAC-GAN (Gong
et al., 2019) and PD-GAN (Miyato & Koyama, 2018)) by analyzing their potential issues and lim-
itations. Experimental results clearly show that the proposed ADC-GAN successfully resolves the
problem of AC-GAN by faithfully learning the real joint data and label distribution. The advantages
over competing cGANs in experiments conducted on both synthetic and real-world datasets verify
the effectiveness of the proposed ADC-GAN in conditional generative modeling."
PRELIMINARIES AND OUR ANALYSIS,0.023728813559322035,"2
PRELIMINARIES AND OUR ANALYSIS"
GENERATIVE ADVERSARIAL NETWORKS,0.02711864406779661,"2.1
GENERATIVE ADVERSARIAL NETWORKS"
GENERATIVE ADVERSARIAL NETWORKS,0.030508474576271188,"Generative adversarial networks (GANs) (Goodfellow et al., 2014) consist of two types of neural
networks: the generator G : Z ‚ÜíX that maps a latent code z ‚ààZ endowed with an easily sampled
distribution PZ to a data point x ‚ààX, and the discriminator D : X ‚Üí[0, 1] that distinguishes
between real data that sampled from the real data distribution PX and fake data that sampled from
the generated data distribution QX = G ‚ó¶PZ implied by the generator. The goal of the generator
is to confuse the discriminator by producing data that is as real as possible. Formally, the objective
functions for the discriminator and the generator are deÔ¨Åned as follows:"
GENERATIVE ADVERSARIAL NETWORKS,0.03389830508474576,"min
G max
D V (G, D) = Ex‚àºPX[log D(x)] + Ex‚àºQX[log(1 ‚àíD(x))].
(1)"
GENERATIVE ADVERSARIAL NETWORKS,0.03728813559322034,"Theoretically, the learning of generator under an optimal discriminator can be regarded as minimiz-
ing the Jensen-Shannon (JS) divergence between the real data distribution and the generated data
distribution, i.e., minG JS(PX‚à•QX). This would enable the generator to recover the real data distri-
bution at its optima. However, the training of GANs is notoriously unstable, especially when lacking
additional supervision such as conditional information. Moreover, the content of the generated im-
ages of GANs cannot be speciÔ¨Åed in advance."
AC-GAN,0.04067796610169491,"2.2
AC-GAN"
AC-GAN,0.04406779661016949,"Learning GANs with conditional information can not only improve the training stability and gen-
eration quality of GANs but also achieve conditional generation, which has more practical value
than unconditional generation in real-world applications. One of the most representative conditional
GANs is AC-GAN (Odena et al., 2017), which utilizes an auxiliary classiÔ¨Åer C : X ‚ÜíY to learn
the dependencies between the real data x ‚àºPX and the label y ‚àºPY and then enforce the condi-
tional generator G : Z √ó Y ‚ÜíX to synthesize classiÔ¨Åable data as much as possible. The objective
functions for the discriminator D, the auxiliary classiÔ¨Åer C, and the generator G of AC-GAN are
deÔ¨Åned as follows1:"
AC-GAN,0.04745762711864407,"max
D,C V (G, D) + Œª ¬∑
 
Ex,y‚àºPX,Y [log C(y|x)]

,"
AC-GAN,0.05084745762711865,"min
G V (G, D) ‚àíŒª ¬∑
 
Ex,y‚àºQX,Y [log C(y|x)]

,
(2)"
AC-GAN,0.05423728813559322,"where Œª > 0 is a hyper-parameter, and QX,Y = G ‚ó¶(PZ √ó PY ) denotes the joint distribution of
generated data and labels implied by the generator."
AC-GAN,0.0576271186440678,1We follow the common practice in the literature to adopt the stable version instead of the original one.
AC-GAN,0.061016949152542375,Under review as a conference paper at ICLR 2022 ùê∑! ùëô ùë• ùë¶
AC-GAN,0.06440677966101695,(a) PD-GAN ùê∑ ùëô ùë•
AC-GAN,0.06779661016949153,"ùê∂
share ùë¶ real"
AC-GAN,0.0711864406779661,(b) AC-GAN ùê∑ ùëô ùë•
AC-GAN,0.07457627118644068,"ùê∂
share ùë¶ ùê∂!"" ùë¶ share"
AC-GAN,0.07796610169491526,"fake
real"
AC-GAN,0.08135593220338982,(c) TAC-GAN ùê∑ ùëô ùë•
AC-GAN,0.0847457627118644,"ùê∂!
share (ùë¶,ùëô)"
AC-GAN,0.08813559322033898,real & fake
AC-GAN,0.09152542372881356,(d) ADC-GAN
AC-GAN,0.09491525423728814,"Figure 1: Illustration of discriminators/classiÔ¨Åers of existing conditional GANs (PD-GAN (Miyato
& Koyama, 2018), AC-GAN (Odena et al., 2017), and TAC-GAN (Gong et al., 2019)) and the
proposed ADC-GAN. l indicates real (l = 1) or fake (l = 0) and y is the class-label of data x. ADC-
GAN is different from PD-GAN with explicitly predicting the label and is different with AC-GAN
and TAC-GAN that the classiÔ¨Åer Cd also distinguishes real from fake like the discriminator."
AC-GAN,0.09830508474576272,Proposition 1. The optimal classiÔ¨Åer of AC-GAN outputs as follows:
AC-GAN,0.1016949152542373,"C‚àó(y|x) = p(x, y)"
AC-GAN,0.10508474576271186,"p(x) .
(3)"
AC-GAN,0.10847457627118644,"Theorem 1. Given the optimal classiÔ¨Åer, at the equilibrium point, optimizing the classiÔ¨Åcation task
for the generator of AC-GAN is equivalent to:"
AC-GAN,0.11186440677966102,"min
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX) + HQ(Y |X),
(4)"
AC-GAN,0.1152542372881356,"where HQ(Y |X) = ‚àí
R P"
AC-GAN,0.11864406779661017,"y q(x, y) log q(y|x)dx is the conditional entropy of generated data."
AC-GAN,0.12203389830508475,"The proofs of Proposition 1 and Theorem 1 are referred to Appendix A.1 and A.2, respectively.
Our Theorem 1 exposes two shortcomings of AC-GAN. First, maximization of the KL divergence
between the marginal generator distribution and the marginal data distribution maxG KL(QX‚à•PX)
contradicts the goal of conditional generative modeling that matches QX,Y with PX,Y . Although
this issue can be mitigated to some extent by the adversarial training objective between the discrimi-
nator and the generator that minimizes the JS divergence between the two marginal distributions, we
Ô¨Ånd that it still has a negative impact on the training stability. Second, minimization of the entropy
of label conditioned on data with respect to the generated distribution minG HQ(Y |X) will result in
that the label of generated data should be completely determined by the data itself. In other words,
it will force the generated data of each class away from the classiÔ¨Åcation hyper-plane, explaining
the low intra-class diversity of generated samples in AC-GAN especially when the distributions of
different classes have non-negligible overlap, which is supported by the fact that state-of-the-art clas-
siÔ¨Åers nor human cannot achieve 100% accuracy on real-world datasets (Russakovsky et al., 2015).
Note that the original version of AC-GAN, whose classiÔ¨Åer is trained by both real and generated
samples, could also suffer from the same issue (see Appendix B)."
AC-GAN,0.12542372881355932,"3
THE PROPOSED METHOD: ADC-GAN"
AC-GAN,0.1288135593220339,"The goal of conditional generative modeling is to faithfully approximate the joint distribution of
real data and labels regardless of the shape of the target joint distribution (whether there is overlap
between distributions of different classes). Note that the learning of the generator in AC-GAN is
affected by the classiÔ¨Åer. In other words, the reason for the consequence of Theorem 1 originates
from Proposition 1, which indicates that the optimal classiÔ¨Åer of AC-GAN is agnostic to the density
of the generated (marginal or joint) distribution (q(x) or q(x, y)). Therefore, the classiÔ¨Åer cannot
provide the discrepancy between the target distribution and the generated distribution, resulting in a"
AC-GAN,0.13220338983050847,Under review as a conference paper at ICLR 2022
AC-GAN,0.13559322033898305,Table 1: Comparison of objective of the generator under the optimal discriminator and classiÔ¨Åer.
AC-GAN,0.13898305084745763,"Method
Objective of the generator under the optimal discriminator and classiÔ¨Åer"
AC-GAN,0.1423728813559322,"AC-GAN
minG JS(PX‚à•QX) + Œª ¬∑ (KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX) + HQ(Y |X))
TAC-GAN
minG JS(PX‚à•QX) + Œª ¬∑ (KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX))
ADC-GAN
minG JS(PX‚à•QX) + Œª ¬∑ (KL(QX,Y ‚à•PX,Y ))
PD-GAN
minG JS(QX,Y ‚à•PX,Y )"
AC-GAN,0.14576271186440679,"biased learning objective to the generator. Recall that the optimal discriminator D‚àó(x) =
p(x)
p(x)+q(x)
is able to be aware of the real data density as well as the generated data density (Goodfellow et al.,
2014), and thus can provide the discrepancy p(x)"
AC-GAN,0.14915254237288136,"q(x) =
D‚àó(x)
1‚àíD‚àó(x) between the real data distribution
and the generated data distribution to unbiasedly optimize the generator. Intuitively, the density-
aware ability on both real and generated data is caused by the fact that the discriminator attempts to
distinguish between real and fake samples. Motivated by this observation, we propose to make the
classiÔ¨Åer to be distinguishable between real and fake samples, establishing a discriminative classiÔ¨Åer
Cd : X ‚ÜíY √ó {0, 1} that recognizes the label of real and fake samples discriminatively. Formally,
the objective functions for the discriminator D, the discriminative classiÔ¨Åer Cd, and the generator G
of the proposed ADC-GAN are deÔ¨Åned as follows:
max
D,Cd V (G, D) + Œª ¬∑
 
Ex,y‚àºPX,Y [log Cd(y, 1|x)] + Ex,y‚àºQX,Y [log Cd(y, 0|x)]

,"
AC-GAN,0.15254237288135594,"min
G V (G, D) ‚àíŒª ¬∑
 
Ex,y‚àºQX,Y [log Cd(y, 1|x)] ‚àíEx,y‚àºQX,Y [log Cd(y, 0|x)]

,
(5)"
AC-GAN,0.15593220338983052,"where Cd(y, 1|x) (reps. Cd(y, 0|x)) denotes the probability that a data x is classiÔ¨Åed as the label y
and real (reps. fake) data simultaneously.
Proposition 2. For Ô¨Åxed generator, the optimal classiÔ¨Åer of ADC-GAN outputs as follows:"
AC-GAN,0.15932203389830507,"C‚àó
d(y, 1|x) =
p(x, y)
p(x) + q(x), C‚àó
d(y, 0|x) =
q(x, y)
p(x) + q(x).
(6)"
AC-GAN,0.16271186440677965,"The proof is referred to Appendix A.3. Proposition 2 conÔ¨Årms that the discriminative classiÔ¨Åer be
aware of the densities of the real and generated joint distributions, therefore it is able to provide the
discrepancy p(x,y)"
AC-GAN,0.16610169491525423,"q(x,y) = C‚àó
d(y,1|x)
C‚àó
d(y,0|x) to unbiasedly optimize the generator as we prove below."
AC-GAN,0.1694915254237288,"Theorem 2. Given the optimal classiÔ¨Åer, at the equilibrium point, optimizing the classiÔ¨Åcation task
for the generator of ADC-GAN is equivalent to:
min
G KL(QX,Y ‚à•PX,Y ).
(7)"
AC-GAN,0.17288135593220338,"The proof is referred to Appendix A.4. Theorem 2 suggests that the classiÔ¨Åer itself can guarantee
the generator to replicate the real joint distribution in theory regardless of the shape of the joint
distribution. In practice, we retain the discriminator to train the generator and share all layers but
the head of the classiÔ¨Åer with the discriminator as illustrated in Figure 1 and Equation 5 for faster
convergence speed. Coupled with the adversarial training against the discriminator, the generator
of the proposed ADC-GAN, under the optimal discriminator and classiÔ¨Åer, can be regarded as min-
imizing the following divergences: minG JS(PX‚à•QX) + Œª ¬∑ KL(QX,Y ‚à•PX,Y ). Since the optimal
solution of conditional generative modeling belongs to the optimal solution set of generative mod-
eling, i.e., arg minG KL(QX,Y ‚à•PX,Y ) ‚äÜarg minG JS(PX‚à•QX), learning with the discriminator
will not change the convergence point of the generator that approximates the joint distribution of real
data and labels regardless of the value of hyper-parameter Œª > 0. Furthermore, the hyper-parameter
Œª provides the Ô¨Çexibility to adjust the weight of conditional generative modeling."
DISCUSSION ON COMPETING METHODS,0.17627118644067796,"4
DISCUSSION ON COMPETING METHODS"
DISCUSSION ON COMPETING METHODS,0.17966101694915254,"In this section, we analyze the drawbacks of the two competing methods, TAC-GAN (Gong et al.,
2019) and PD-GAN (Miyato & Koyama, 2018), to demonstrate the superiority and rationality of
ADC-GAN compared to them. Before diving into the details, we show diagrams of the discriminator
and classiÔ¨Åer of these methods in Figure 1 and summarize the theoretical learning goal for the
generator under the optimal discriminator and classiÔ¨Åer of these methods in Table 1 for an overview."
DISCUSSION ON COMPETING METHODS,0.18305084745762712,Under review as a conference paper at ICLR 2022
TAC-GAN,0.1864406779661017,"4.1
TAC-GAN"
TAC-GAN,0.18983050847457628,"TAC-GAN (Gong et al., 2019) addresses the low intra-class diversity issue of AC-GAN by eliminat-
ing the conditional entropy with respect to the generated data distribution HQ(Y |X) via learning of
the generator with another classiÔ¨Åer Cmi : X ‚ÜíY, which is trained on the generated samples. The
objective functions for the discriminator D, the twin classiÔ¨Åers C and Cmi, and the generator G of
TAC-GAN are deÔ¨Åned as follows:
max
D,C,Cmi V (G, D) + Œª ¬∑
 
Ex,y‚àºPX,Y [log C(y|x)] + Ex,y‚àºQX,Y [log Cmi(y|x)]

,"
TAC-GAN,0.19322033898305085,"min
G V (G, D) ‚àíŒª ¬∑
 
Ex,y‚àºQX,Y [log C(y|x)] ‚àíEx,y‚àºQX,Y [log Cmi(y|x)]

.
(8)"
TAC-GAN,0.19661016949152543,"Theorem 3. Given the twin optimal classiÔ¨Åers, at the equilibrium point, optimizing the classiÔ¨Åcation
tasks for the generator of TAC-GAN is equivalent to:
min
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX).
(9)"
TAC-GAN,0.2,"The proof is referred to Appendix A.5. Theorem 3 reveals that the learning objective of the generator
of TAC-GAN, under the optimal classiÔ¨Åer, can be regarded as minimizing contradictory divergences,
i.e., minimization between joint distributions but maximization between marginal distributions. Al-
though theoretically the JS divergence or others (Nowozin et al., 2016; Arjovsky et al., 2017) intro-
duced through the adversarial training between the discriminator and the generator might remedy
this issue, the optimal discriminator and classiÔ¨Åer are difÔ¨Åcult to obtain in the practical optimization
to ensure that the contradiction is eliminated. We argue that the training instability of TAC-GAN
reported in the literature (Kocaoglu et al., 2018; Han et al., 2020) and founded in our experiments
can be explained by this analysis and interpretation."
PD-GAN,0.2033898305084746,"4.2
PD-GAN"
PD-GAN,0.20677966101694914,"PD-GAN (Miyato & Koyama, 2018) injects the conditional information into the projection dis-
criminator Dp : X √ó Y ‚Üí[0, 1] via the inner-product between the embedding of label and the
representation of data to calculate the joint discriminative score of the data-label pair. In such a
way, PD-GAN inherits the property of convergence point similar to the standard GAN such that
can avoid the low intra-class diversity issue of AC-GAN. The objective functions for the projection
discriminator Dp and the generator G of PD-GAN are deÔ¨Åned as follows:
min
G max
Dp V (G, Dp) = Ex,y‚àºPX,Y [log Dp(x, y)] + Ex,y‚àºQX,Y [log(1 ‚àíDp(x, y))].
(10)"
PD-GAN,0.21016949152542372,"Based on this minimax game, the optimal projection discriminator has the following form:"
PD-GAN,0.2135593220338983,"D‚àó
p(x, y) =
1
1 + exp(‚àíd‚àó(x, y)) =
p(x, y)
p(x, y) + q(x, y)"
PD-GAN,0.21694915254237288,"‚áíd‚àó(x, y) = log p(x, y)"
PD-GAN,0.22033898305084745,"q(x, y) = log p(x)"
PD-GAN,0.22372881355932203,q(x) + log p(y|x)
PD-GAN,0.2271186440677966,"q(y|x) := r(x) + r(y|x),
(11)"
PD-GAN,0.2305084745762712,"where p(y|x) =
exp(vp
y¬∑œÜ(x))
PK
k=1 exp(vp
k¬∑œÜ(x)) and q(y|x) =
exp(vq
y¬∑œÜ(x))
PK
k=1 exp(vq
k¬∑œÜ(x)) with K = |Y| is the number of
labels. And they accordingly deÔ¨Åne:
r(x) := œà(œÜ(x)),"
PD-GAN,0.23389830508474577,"r(y|x) := (vp
y ‚àívq
y) ¬∑ œÜ(x)
|
{z
}
ÀÜr(y|x) ‚àí  log K
X"
PD-GAN,0.23728813559322035,"k=1
exp (vp
k ¬∑ œÜ(x)) ‚àílog K
X"
PD-GAN,0.24067796610169492,"k=1
exp (vq
k ¬∑ œÜ(x)) !"
PD-GAN,0.2440677966101695,"|
{z
}
a‚Éù"
PD-GAN,0.24745762711864408,".
(12)"
PD-GAN,0.25084745762711863,"However, PD-GAN actually ignores the partition term a‚Éù2 in Equation 12, and constructs the logit
of the projection discriminator in the form of:
d(x, y) = r(x) + ÀÜr(y|x) = œà(œÜ(x)) + vy ¬∑ œÜ(x),
(13)"
PD-GAN,0.2542372881355932,"2The authors mistakenly argue that a‚Éùcan be merged into r(x). However, r(x) does not incorporate any
label information (vp or vq), which should be considered by a‚Éù. Therefore, it is unreasonable to merge a‚Éùinto
r(x). PD-GAN actually discards a‚Éùin implementing the projection discriminator."
PD-GAN,0.2576271186440678,Under review as a conference paper at ICLR 2022
PD-GAN,0.26101694915254237,AC-GAN
PD-GAN,0.26440677966101694,w/o GAN loss
PD-GAN,0.2677966101694915,Log loss
PD-GAN,0.2711864406779661,Hinge loss
PD-GAN,0.2745762711864407,"PD-GAN
TAC-GAN
ADC-GAN Data"
PD-GAN,0.27796610169491526,w/o GAN loss
PD-GAN,0.28135593220338984,Log loss
PD-GAN,0.2847457627118644,Hinge loss
PD-GAN,0.288135593220339,Figure 2: Distribution learning results on one-dimensional synthetic data.
PD-GAN,0.29152542372881357,"with vy = vp
y ‚àívq
y is the difference between two learnable embeddings of label y deÔ¨Åned in two
implicit conditional probabilities p(y|x) and q(y|x), œÜ(¬∑) is the representation extractor, and œà(¬∑)
outputs a scalar based on the extracted representation. Discarding the partition term would make
PD-GAN no longer belong to probability models that model the conditional probabilities p(y|x)
and q(y|x), resulting in losing the complete dependencies between data and labels. Moreover, the
discriminator constructed according to the optimal form of the minimax GAN lacks theoretical guar-
antee when applied on other loss functions such as the hinge loss (Lim & Ye, 2017; Tran et al., 2017),
which PD-GAN actually used, and may even limit its discriminative ability. The proposed ADC-
GAN can be Ô¨Çexibly applied to any version of the loss function V (G, D) as we do not require the
speciÔ¨Åc form of the original discriminator."
EXPERIMENTS,0.29491525423728815,"5
EXPERIMENTS"
EXPERIMENTS,0.2983050847457627,"In this section, we conduct extensive experiments on both synthetic and real-world datasets to
demonstrate the effectiveness of the proposed ADC-GAN. SpeciÔ¨Åcally, ADC-GAN has the advan-
tages of being robust to the hyper-parameter Œª, stable during the training process, and capable of
accurately modeling dependencies between data and labels, in addition to the comparable or even
better performance on conditional generative modeling than existing cGANs."
SYNTHETIC DATA,0.3016949152542373,"5.1
SYNTHETIC DATA"
SYNTHETIC DATA,0.3050847457627119,"We Ô¨Årst experiment on a one-dimensional synthetic mixture of Gaussian to validate the distribution
learning ability of methods. As shown in the left-top of Figure 2, the real data distribution con-
sists of three classes in which there is non-negligible overlap between them. Both generator and
discriminator are multi-layer perceptrons with non-linearity of Tanh. In particular, we investigate
three different settings on the GAN loss function while keeping the learning of the generator with
the classiÔ¨Åer Ô¨Åxed if it exists. The Ô¨Årst row except the data part shows the learned distributions
that are estimated by kernel density estimation (Parzen, 1962) on the generated data of AC-GAN,
TAC-GAN, and ADC-GAN without the original GAN loss V (G, D). The second and the third rows
show the results of these methods trained with the log loss (Goodfellow et al., 2014) and the hinge
loss (Lim & Ye, 2017; Tran et al., 2017), respectively. The poor performance of PD-GAN with
hinge loss conÔ¨Årms that it is sensitive to the loss function. AC-GAN tends to generate classiÔ¨Åable
data so that it decreases the intra-class diversity in all settings, verifying the Theorem 1. TAC-GAN
without GAN loss cannot accurately reproduce the data distribution, verifying the Theorem 3. And
the worse performance of TAC-GAN with hinge loss compared with ADC-GAN conÔ¨Årms that the
contradiction stated in Theorem 3 is not easy to eliminate by the discriminator. Expectedly, the pro-
posed ADC-GAN can accurately replicate the data distribution even without the original GAN loss,
verifying the Theorem 2. For more quantitative results, please refer to Appendix C.1."
SYNTHETIC DATA,0.30847457627118646,Under review as a conference paper at ICLR 2022 ùúÜ= 1 ùúÜ= 10
SYNTHETIC DATA,0.31186440677966104,ùúÜ= 100
SYNTHETIC DATA,0.3152542372881356,"AC-GAN
ADC-GAN
TAC-GAN ùúÜ= 1 ùúÜ= 10"
SYNTHETIC DATA,0.31864406779661014,ùúÜ= 100
SYNTHETIC DATA,0.3220338983050847,Figure 3: Hyper-parameter robustness results on overlapping MNIST.
OVERLAPPING MNIST,0.3254237288135593,"5.2
OVERLAPPING MNIST"
OVERLAPPING MNIST,0.3288135593220339,"In this subsection, we experiment on a constructed dataset to investigate the hyper-parameter ro-
bustness of the proposed ADC-GAN compared to existing classiÔ¨Åer-based methods, i.e., AC-GAN
and TAC-GAN. We follow the practice of (Gong et al., 2019) to construct a two-class handwritten
digit dataset from MNIST. The Ô¨Årst class contains an equal number of digits of ‚Äò0‚Äô and ‚Äò1‚Äô and the
second class contains an equal number of digits of ‚Äò0‚Äô and ‚Äò2‚Äô. In other words, the support regions
of the two classes have an overlap of digits ‚Äò0‚Äô. We change the weight of classiÔ¨Åer from Œª = 1 to
Œª = 100 with multiplicative step of 10. As shown in Figure 3, AC-GAN mistakenly discards digit
‚Äò0‚Äô in the Ô¨Årst class when Œª = 1. When Œª ‚â•10, the digit ‚Äò0‚Äô in the generated data of AC-GAN
totally disappears, indicating that its classiÔ¨Åer encourages the generator to avoid learning the data
in the overlapping region. In general, AC-GAN shows a signiÔ¨Åcant decrease in intra-class diversity.
TAC-GAN encounters mode collapse when Œª = 100, while ADC-GAN faithfully replicates the real
data distribution regardless of the value of Œª. These results suggest that the proposed method has
excellent robustness on hyper-parameters compared to existing classiÔ¨Åer-based methods since the
guidances to the generator received from the discriminator and classiÔ¨Åer are harmonious."
OVERLAPPING MNIST,0.33220338983050846,"5.3
CIFAR-10, CIFAR-100, AND TINY-IMAGENET"
OVERLAPPING MNIST,0.33559322033898303,"We experiment on three real-world datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet. CIFAR-
10 consists of 50k training images and 10k validation images with resolution of 32 √ó 32. CIFAR-
100 runs similar samples with CIFAR-10 but has 100 classes rather than 10 classes in CIFAR-
10. Tiny-ImageNet contains 200 classes where each class contains 500 training images and 50
validation images with resolution of 64 √ó 64. We implement all methods based on the BigGAN-
PyTorch repository. The optimizer is Adam with learning rate of 2 √ó 10‚àí4 for both the generator
and discriminator on CIFAR-10/100, 1 √ó 10‚àí4 and 4 √ó 10‚àí4 for the generator and discriminator,
respectively, on Tiny-ImageNet. We train all methods for 500 epochs with batch size of 50 on
CIFAR-10/100 and 100 on Tiny-ImageNet. The discriminator/classiÔ¨Åer are updated 4 times per
generator update step on CIFAR-10/100, and 2 times on Tiny-ImageNet. We follow the practice
of (Miyato & Koyama, 2018; Gong et al., 2019) to adopt the hinge loss (Lim & Ye, 2017; Tran"
OVERLAPPING MNIST,0.3389830508474576,"Table 2: FID and Intra-FID (the averaged intra-class FID) and Accuracy (%) comparison of different
methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively."
OVERLAPPING MNIST,0.3423728813559322,"Datasets
Metrics
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN"
OVERLAPPING MNIST,0.34576271186440677,"CIFAR-10
FID (‚Üì)
6.51
6.81
5.85
5.56
Intra-FID (‚Üì)
46.35
38.34
37.38
34.26
Accuracy (‚Üë)
62.21
84.56
88.05
89.19"
OVERLAPPING MNIST,0.34915254237288135,"CIFAR-100
FID (‚Üì)
8.47
11.63
11.37
7.98
Intra-FID (‚Üì)
136.79
161.39
158.15
132.27
Accuracy (‚Üë)
38.81
55.29
60.13
62.29"
OVERLAPPING MNIST,0.3525423728813559,"Tiny-ImageNet
FID (‚Üì)
20.54
24.48
23.78
20.15
Intra-FID (‚Üì)
94.23
154.82
124.68
97.69
Accuracy (‚Üë)
27.56
42.26
41.78
44.29"
OVERLAPPING MNIST,0.3559322033898305,Under review as a conference paper at ICLR 2022
OVERLAPPING MNIST,0.3593220338983051,"(a) CIFAR-10 FID on different Œª‚Ä≤
(b) FID curves on CIFAR-10
(c) PD-GAN"
OVERLAPPING MNIST,0.36271186440677966,"(d) CIFAR-100 FID on different Œª‚Ä≤
(e) FID curves on CIFAR-100
(f) ADC-GAN"
OVERLAPPING MNIST,0.36610169491525424,"Figure 4: (a,d) FID comparison of methods with different Œª‚Ä≤ on CIFAR-10 and CIFAR-100. The
objective of competing methods is (1 ‚àíŒª‚Ä≤)V (G, D) + Œª‚Ä≤VC(G, C), where VC(G, C) is the task
between the generator and classiÔ¨Åer. (b,e) FID curves with GAN training iterations on CIFAR-10
and CIFAR-100. (c,f) T-SNE visualization of CIFAR-10 training data, using learned representations
extracted from the penultimate layer in discriminators. Different colors represent different classes."
OVERLAPPING MNIST,0.3694915254237288,"et al., 2017) as an implementation of V (G, D). We set the hyper-parameter of AC-GAN as Œª = 0.2
on all datasets as it performs the best. As for TAC-GAN and ADC-GAN, the hyper-parameter is set
as Œª = 1.0 on CIFAR-10/100 and Œª = 0.5 on Tiny-ImageNet."
OVERLAPPING MNIST,0.3728813559322034,"We report the FID (Heusel et al., 2017) and Intra-FID (Miyato & Koyama, 2018) (the FID for
each class) results of all methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet in Table 2. AC-
GAN obtains the worst results and diverges on all datasets (see Figure 4(b),4(e),7). TAC-GAN also
diverges on CIFAR-100 and Tiny-ImageNet and only achieves a relatively stable FID training curve
on CIFAR-10. We here report their results at the best FID checkpoint. These unstable FID curves
implicitly reveal the drawback of the existing classiÔ¨Åer-based cGANs that minimizes contradictory
divergences. To explicitly show this, we set the objective function of classiÔ¨Åer-based methods as
(1 ‚àíŒª‚Ä≤)V (G, D) + Œª‚Ä≤VC(G, C), where VC(G, C) is the task between the generator and classiÔ¨Åer.
As shown in Figure 4(a),4(d), ADC-GAN gains consistent FID results across different Œª‚Ä≤ even for
Œª‚Ä≤ = 1.0 (i.e., without the discriminator), showing strong robustness on Œª‚Ä≤, while AC-GAN and
TAC-GAN performs substantially bad when Œª‚Ä≤ becomes large. Table 2 also shows that ADC-GAN
achieves the superior FID and Intra-FID on CIFAR-10 and CIFAR-100 and comparable scores with
PD-GAN on Tiny-ImageNet. The reason why PD-GAN obtains slightly better Intra-FID than ADC-
GAN on Tiny-ImageNet is that Tiny-ImageNet is a more coarse-grained dataset, in which data in
different classes have weak correlations. Impressively, ADC-GAN performs much better training
stability than PD-GAN in terms of the FID curves as shown in Figure 4(b),4(e),7. We argue that the
reason is that ADC-GAN is less prone to over-Ô¨Åtting than PD-GAN due to that the discriminative
classiÔ¨Åer of ADC-GAN solves a more difÔ¨Åcult task than the projection discriminator of PD-GAN."
OVERLAPPING MNIST,0.376271186440678,"To investigate whether the model captures accurate dependencies between data and labels, we con-
duct image classiÔ¨Åcation experiments based on the learned representations extracted from the shared
penultimate layer in the discriminator/classiÔ¨Åer. SpeciÔ¨Åcally, we utilize the logistical regression
classiÔ¨Åer in scikit-learn library with default settings to compute the accuracy results. As reported
in Table 2, ADC-GAN signiÔ¨Åcantly outperforms competing methods on all datasets, indicating that
ADC-GAN effectively learns accurate dependencies between data and labels. The reason is that the
discriminative classiÔ¨Åer needs to distinguish between real and fake data while simultaneously recog-"
OVERLAPPING MNIST,0.37966101694915255,Under review as a conference paper at ICLR 2022
OVERLAPPING MNIST,0.38305084745762713,"Table 3: FID and IS comparison with competing methods on ImageNet. The results of competing
methods are copied from TAC-GAN (Gong et al., 2019) and SAGAN (Zhang et al., 2019)."
OVERLAPPING MNIST,0.3864406779661017,"Datasets
Metrics
PD-GAN/BigGAN
AC-GAN
TAC-GAN
ADC-GAN"
OVERLAPPING MNIST,0.3898305084745763,"ImageNet
FID (‚Üì)
22.77
‚àí
23.75
16.75
IS (‚Üë)
38.05 ¬± 0.79
28.5
28.86 ¬± 0.29
55.43 ¬± 0.90"
OVERLAPPING MNIST,0.39322033898305087,"nizing the labels of samples, which forces the classiÔ¨Åer to have a more powerful and robust ability of
modeling data-to-class relations. Notice that PD-GAN obtains the worst accuracy results. By com-
paring the CIFAR-10 T-SNE (Van der Maaten & Hinton, 2008) visualization results of PD-GAN
and ADC-GAN using the learned features of validation data in Figure 4(c),4(f), one can clearly see
that PD-GAN is incapable of learning accurate dependencies between data and labels."
IMAGENET,0.39661016949152544,"5.4
IMAGENET"
IMAGENET,0.4,"In this subsection, we compare the proposed ADC-GAN with competing methods on ImageNet
(128√ó128) with 1,000 classes (Deng et al., 2009), each of which contains around 1,300 images. We
adopt the BigGAN with base channels of 64 as the backbone of ADC-GAN and train one step for the
discriminator/classiÔ¨Åer and one step for the generator, following the practices of TAC-GAN (Gong
et al., 2019). We follow the instruction of FQ-GAN (Zhao et al., 2020) to train ADC-GAN for
128k iterations. As shown in Table 3, the proposed ADC-GAN signiÔ¨Åcantly outperforms competing
cGANs in terms of both Inception Score (IS) (Salimans et al., 2016) and FID metrics, showing the
effectiveness on large-scale high-resolution image datasets."
RELATED WORK,0.4033898305084746,"6
RELATED WORK"
RELATED WORK,0.4067796610169492,"Conditional generative adversarial networks (cGANs) (Mirza & Osindero, 2014) is a family of
GANs, which is capable of generating novel data depended on the given label. The research on
cGANs can be divided into two aspects. One is to study how to implement a conditional generator
network structurally. Approaches in this category are mainly concatenating the label vector with the
noise vector (Mirza & Osindero, 2014), conditional batch normalization (de Vries et al., 2017), and
conditional convolution layers (Sagong et al., 2019). The other is to study how to train the condi-
tional generator to generate label-dependent data. AC-GAN (Odena et al., 2017) leveraged an aux-
iliary classiÔ¨Åer to determine the relationship between data and labels. MH-GAN (Kavalerov et al.,
2021) improved AC-GAN by replacing the cross-entropy loss of the classiÔ¨Åer with the multi-hinge
loss. Shu et al. (2017) analyzed that AC-GAN learns a biased distribution from a Lagrange view.
AM-GAN (Zhou et al., 2018) extended the real class of the discriminator into K classes of the clas-
siÔ¨Åer, forming a discriminator with K +1 classes. Omni-GAN (Zhou et al., 2020) combined the dis-
criminator with the classiÔ¨Åer into a K + 2 dimensional multi-label discriminator. TAC-GAN (Gong
et al., 2019) corrected the biased learning objective of AC-GAN by introducing another classiÔ¨Åer,
which was the multi-class version of Anti-Labeler of CausalGAN (Kocaoglu et al., 2018). UAC-
GAN (Han et al., 2020) improved the training stability of TAC-GAN via MINE (Belghazi et al.,
2018). PD-GAN (Miyato & Koyama, 2018) injected the label information into the discriminator via
projection technique. In complementary to our work, ContraGAN (Kang & Park, 2020) modeled
the data-to-data relations as well as the data-to-class relations using a conditional contrastive loss."
CONCLUSIONS,0.4101694915254237,"7
CONCLUSIONS"
CONCLUSIONS,0.4135593220338983,"In this paper, we present a novel conditional generative adversarial network with an auxiliary dis-
criminative classiÔ¨Åer (ADC-GAN) to achieve faithful conditional generative modeling. The dis-
criminative classiÔ¨Åer can provide the discrepancy between the joint distribution of the real data and
labels and that of the generated data and labels to the generator by discriminatively predicting the la-
bel of the real and generated data. Therefore, the generator can faithfully learn the real joint data and
label distribution at the Nash equilibrium. We also discuss the differences between ADC-GAN with
competing cGANs and analyze their potential issues and limitations. Extensive experimental results
validate the theoretical superiority of the proposed ADC-GAN compared to competing cGANs."
CONCLUSIONS,0.41694915254237286,Under review as a conference paper at ICLR 2022
REFERENCES,0.42033898305084744,REFERENCES
REFERENCES,0.423728813559322,"Martin Arjovsky, Soumith Chintala, and L¬¥eon Bottou.
Wasserstein generative adversarial net-
works.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 214‚Äì223. PMLR, 06‚Äì11 Aug 2017. URL http://proceedings.mlr.press/v70/
arjovsky17a.html."
REFERENCES,0.4271186440677966,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 531‚Äì540. PMLR, 10‚Äì15 Jul 2018.
URL
https://proceedings.mlr.press/v80/belghazi18a.html."
REFERENCES,0.43050847457627117,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high Ô¨Ådelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm."
REFERENCES,0.43389830508474575,"Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via
auxiliary rotation loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019."
REFERENCES,0.43728813559322033,"Harm de Vries,
Florian Strub,
Jeremie Mary,
Hugo Larochelle,
Olivier Pietquin,
and
Aaron C Courville.
Modulating early visual processing by language.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
6fab6e3aa34248ec1e34a4aeedecddc8-Paper.pdf."
REFERENCES,0.4406779661016949,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248‚Äì255. Ieee, 2009."
REFERENCES,0.4440677966101695,"Mingming Gong, Yanwu Xu, Chunyuan Li, Kun Zhang, and Kayhan Batmanghelich. Twin auxi-
lary classiÔ¨Åers gan. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch¬¥e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf."
REFERENCES,0.44745762711864406,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair,
Aaron Courville,
and Yoshua Bengio.
Generative adversarial nets.
In
Z. Ghahramani, M. Welling,
C. Cortes,
N. Lawrence,
and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems,
volume 27. Curran Associates,
Inc.,
2014.
URL
https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf."
REFERENCES,0.45084745762711864,"Ligong Han, Anastasis Stathopoulos, Tao Xue, and Dimitris Metaxas. Unbiased auxiliary classiÔ¨Åer
gans with mine. arXiv preprint arXiv:2006.07567, 2020."
REFERENCES,0.4542372881355932,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-
iter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium.
In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
8a1d694707eb0fefe65871369074926d-Paper.pdf."
REFERENCES,0.4576271186440678,"Minguk Kang and Jaesik Park.
Contragan: Contrastive learning for conditional image gen-
eration.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21357‚Äì21369. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f490c742cd8318b8ee6dca10af2a163f-Paper.pdf."
REFERENCES,0.4610169491525424,Under review as a conference paper at ICLR 2022
REFERENCES,0.46440677966101696,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019."
REFERENCES,0.46779661016949153,"Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12104‚Äì12114. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.
cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf."
REFERENCES,0.4711864406779661,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020b."
REFERENCES,0.4745762711864407,"Ilya Kavalerov, Wojciech Czaja, and Rama Chellappa. A multi-class hinge loss for conditional gans.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
pp. 1290‚Äì1299, January 2021."
REFERENCES,0.47796610169491527,"Murat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath. Causal-
GAN: Learning causal implicit generative models with adversarial training.
In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=BJE-4xW0W."
REFERENCES,0.48135593220338985,"Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017."
REFERENCES,0.4847457627118644,"Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples
in generative adversarial networks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf."
REFERENCES,0.488135593220339,"Mehdi Mirza and Simon Osindero.
Conditional generative adversarial nets.
arXiv preprint
arXiv:1411.1784, 2014."
REFERENCES,0.4915254237288136,"Takeru Miyato and Masanori Koyama.
cGANs with projection discriminator.
In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ."
REFERENCES,0.49491525423728816,"Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
cedebb6e872f539bef8c3f919874e9d7-Paper.pdf."
REFERENCES,0.49830508474576274,"Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016."
REFERENCES,0.5016949152542373,"Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with aux-
iliary classiÔ¨Åer GANs. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th In-
ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 2642‚Äì2651. PMLR, 06‚Äì11 Aug 2017.
URL http://proceedings.mlr.
press/v70/odena17a.html."
REFERENCES,0.5050847457627119,"Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065‚Äì1076, 1962."
REFERENCES,0.5084745762711864,"Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In Maria Florina Balcan and Kilian Q. Weinberger
(eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pp. 1060‚Äì1069, New York, New York, USA, 20‚Äì22
Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/reed16.html."
REFERENCES,0.511864406779661,Under review as a conference paper at ICLR 2022
REFERENCES,0.5152542372881356,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211‚Äì252, 2015."
REFERENCES,0.5186440677966102,"Min-Cheol Sagong, Yong-Goo Shin, Yoon-Jae Yeo, Seung Park, and Sung-Jea Ko. cgans with
conditional convolution layer. arXiv preprint arXiv:1906.00709, 2019."
REFERENCES,0.5220338983050847,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Cur-
ran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/
file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf."
REFERENCES,0.5254237288135594,"Rui Shu, Hung Bui, and Stefano Ermon. Ac-gan learns a biased distribution. In NIPS Workshop on
Bayesian Deep Learning, volume 8, 2017."
REFERENCES,0.5288135593220339,"Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, and
Nenghai Yu. Michigan: Multi-input-conditioned hair image generation for portrait editing. arXiv
preprint arXiv:2010.16417, 2020."
REFERENCES,0.5322033898305085,"Dustin Tran, Rajesh Ranganath, and David Blei.
Hierarchical implicit models and likelihood-
free variational inference.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf."
REFERENCES,0.535593220338983,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008."
REFERENCES,0.5389830508474577,"Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net-
works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018."
REFERENCES,0.5423728813559322,"Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. arXiv preprint arXiv:1512.00570, 2015."
REFERENCES,0.5457627118644067,"Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.
Self-attention generative
adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 7354‚Äì7363. PMLR, 09‚Äì15 Jun 2019. URL http://proceedings.
mlr.press/v97/zhang19d.html."
REFERENCES,0.5491525423728814,"Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization im-
proves GAN training. In Hal Daum¬¥e III and Aarti Singh (eds.), Proceedings of the 37th In-
ternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 11376‚Äì11386. PMLR, 13‚Äì18 Jul 2020. URL https://proceedings.mlr.
press/v119/zhao20d.html."
REFERENCES,0.5525423728813559,"Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, and Qi Tian. Omni-gan: On the secrets of cgans
and beyond. arXiv preprint arXiv:2011.13074, 2020."
REFERENCES,0.5559322033898305,"Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong
Yu. Activation maximization generative adversarial nets. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=HyyP33gAZ."
REFERENCES,0.559322033898305,"Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative ad-
versarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019."
REFERENCES,0.5627118644067797,Under review as a conference paper at ICLR 2022
REFERENCES,0.5661016949152542,"A
PROOFS"
REFERENCES,0.5694915254237288,"A.1
PROOF OF PROPOSITION 1"
REFERENCES,0.5728813559322034,Proposition 1. The optimal classiÔ¨Åer of AC-GAN outputs as follows:
REFERENCES,0.576271186440678,"C‚àó(y|x) = p(x, y)"
REFERENCES,0.5796610169491525,"p(x) .
(3)"
REFERENCES,0.5830508474576271,Proof.
REFERENCES,0.5864406779661017,"max
C
Ex,y‚àºPX,Y [log C(y|x)] = Ex‚àºPXEy‚àºPY |X[log C(y|x)]"
REFERENCES,0.5898305084745763,"‚áímin
C Ex‚àºPXEy‚àºPY |X[‚àílog C(y|x)] = Ex‚àºPX[H(p(y|x)) + KL(p(y|x)‚à•C(y|x))]"
REFERENCES,0.5932203389830508,"‚áíC‚àó(y|x) = arg min
C KL(p(y|x)‚à•C(y|x)) = p(y|x) = p(x, y) p(x)"
REFERENCES,0.5966101694915255,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.6,"Theorem 1. Given the optimal classiÔ¨Åer, at the equilibrium point, optimizing the classiÔ¨Åcation task
for the generator of AC-GAN is equivalent to:"
REFERENCES,0.6033898305084746,"min
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX) + HQ(Y |X),
(4)"
REFERENCES,0.6067796610169491,"where HQ(Y |X) = ‚àí
R P"
REFERENCES,0.6101694915254238,"y q(x, y) log q(y|x)dx is the conditional entropy of generated data."
REFERENCES,0.6135593220338983,Proof.
REFERENCES,0.6169491525423729,"max
G Ex,y‚àºQX,Y [log C‚àó(y|x)] = Ex,y‚àºQX,Y"
REFERENCES,0.6203389830508474,"
log p(x, y) p(x)"
REFERENCES,0.6237288135593221,"
= Ex,y‚àºQX,Y"
REFERENCES,0.6271186440677966,"
log p(x, y)"
REFERENCES,0.6305084745762712,"q(x, y)
q(x)
p(x)
q(x, y) q(x) "
REFERENCES,0.6338983050847458,"=Ex,y‚àºQX,Y"
REFERENCES,0.6372881355932203,"
log p(x, y)"
REFERENCES,0.6406779661016949,"q(x, y)"
REFERENCES,0.6440677966101694,"
+ Ex‚àºQX"
REFERENCES,0.6474576271186441,"
log q(x) p(x)"
REFERENCES,0.6508474576271186,"
+ Ex,y‚àºQX,Y"
REFERENCES,0.6542372881355932,"
log q(x, y) q(x) "
REFERENCES,0.6576271186440678,"‚áímin
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX) + HQ(Y |X)"
REFERENCES,0.6610169491525424,"A.3
PROOF OF PROPOSITION 2"
REFERENCES,0.6644067796610169,"Proposition 2. For Ô¨Åxed generator, the optimal classiÔ¨Åer of ADC-GAN outputs as follows:"
REFERENCES,0.6677966101694915,"C‚àó
d(y, 1|x) =
p(x, y)
p(x) + q(x), C‚àó
d(y, 0|x) =
q(x, y)
p(x) + q(x).
(6)"
REFERENCES,0.6711864406779661,Proof.
REFERENCES,0.6745762711864407,"max
C
Ex,y‚àºPX,Y [log C(y, 1|x)] + Ex,y‚àºQX,Y [log C(y, 0|x)] ‚áímax
C
Ex,y,l‚àºP m
X,Y,L[log C(y, l|x)],"
REFERENCES,0.6779661016949152,"with l ‚àà{0, 1} and pm(x, y, l) = pm(x, y, 1) + pm(x, y, 0) = 1"
REFERENCES,0.6813559322033899,"2p(x, y) + 1"
REFERENCES,0.6847457627118644,"2q(x, y)."
REFERENCES,0.688135593220339,"‚áímax
C
Ex‚àºP m
X Ey,l‚àºP m
Y,L|X[log C(y, l|x)] ‚áímin
C Ex‚àºP m
X Ey,l‚àºP m
Y,L|X[‚àílog C(y, l|x)]"
REFERENCES,0.6915254237288135,"‚áímin
C Ex‚àºP m
X [H(pm(y, l|x)) + KL(pm(y, l|x)‚à•C(y, l|x))]"
REFERENCES,0.6949152542372882,"‚áíC‚àó(y, l|x) = arg min
C KL(pm(y, l|x)‚à•C(y, l|x)) = pm(y, l|x) = pm(x, y, l) pm(x)"
REFERENCES,0.6983050847457627,"Therefore, the optimal classiÔ¨Åer of ADC-GAN has the form of C‚àó(y, 1|x) = pm(x,y,1)"
REFERENCES,0.7016949152542373,"pm(x)
=
p(x,y)
p(x)+q(x)
and C‚àó(y, 0|x) = pm(x,y,0)"
REFERENCES,0.7050847457627119,"pm(x)
=
q(x,y)
p(x)+q(x) that concludes the proof."
REFERENCES,0.7084745762711865,Under review as a conference paper at ICLR 2022
REFERENCES,0.711864406779661,"A.4
PROOF OF THEOREM 2"
REFERENCES,0.7152542372881356,"Theorem 2. Given the optimal classiÔ¨Åer, at the equilibrium point, optimizing the classiÔ¨Åcation task
for the generator of ADC-GAN is equivalent to:"
REFERENCES,0.7186440677966102,"min
G KL(QX,Y ‚à•PX,Y ).
(7)"
REFERENCES,0.7220338983050848,Proof.
REFERENCES,0.7254237288135593,"max
G Ex,y‚àºQX,Y [log C‚àó(y, 1|x)] ‚àíEx,y‚àºQX,Y [log C‚àó(y, 0|x)]"
REFERENCES,0.7288135593220338,"‚áímin
G ‚àíEx,y‚àºQX,Y [log C‚àó(y, 1|x)] + Ex,y‚àºQX,Y [log C‚àó(y, 0|x)]"
REFERENCES,0.7322033898305085,"‚áímin
G ‚àíEx,y‚àºQX,Y"
REFERENCES,0.735593220338983,"
log
p(x, y)
p(x) + q(x)"
REFERENCES,0.7389830508474576,"
+ Ex,y‚àºQX,Y"
REFERENCES,0.7423728813559322,"
log
q(x, y)
p(x) + q(x) "
REFERENCES,0.7457627118644068,"‚áímin
G Ex,y‚àºQX,Y"
REFERENCES,0.7491525423728813,"
log q(x, y)"
REFERENCES,0.752542372881356,"p(x, y)"
REFERENCES,0.7559322033898305,"
‚áímin
G KL(QX,Y ‚à•PX,Y )"
REFERENCES,0.7593220338983051,"A.5
PROOF OF THEOREM 3"
REFERENCES,0.7627118644067796,"Proposition 3. For Ô¨Åxed generator, the twin optimal classiÔ¨Åers of TAC-GAN output as follows:"
REFERENCES,0.7661016949152543,"C‚àó(y|x) = p(x, y)"
REFERENCES,0.7694915254237288,"p(x) , C‚àó
mi(y|x) = q(x, y)"
REFERENCES,0.7728813559322034,"q(x) .
(14)"
REFERENCES,0.7762711864406779,"Proof. The proof is similar to that of Proposition 1 in Appendix A.1 by considering C and Cmi as
two independent classiÔ¨Åers with respect to distribution P and Q, respectively."
REFERENCES,0.7796610169491526,"Theorem 3. Given the twin optimal classiÔ¨Åers, at the equilibrium point, optimizing the classiÔ¨Åcation
tasks for the generator of TAC-GAN is equivalent to:"
REFERENCES,0.7830508474576271,"min
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX).
(9)"
REFERENCES,0.7864406779661017,Proof.
REFERENCES,0.7898305084745763,"max
G Ex,y‚àºQX,Y [log C‚àó(y|x)] ‚àíEx,y‚àºQX,Y [log C‚àó
mi(y|x)]"
REFERENCES,0.7932203389830509,"‚áímax
G Ex,y‚àºQX,Y"
REFERENCES,0.7966101694915254,"
log p(x, y) p(x)"
REFERENCES,0.8,"
‚àíEx,y‚àºQX,Y"
REFERENCES,0.8033898305084746,"
log q(x, y) q(x) "
REFERENCES,0.8067796610169492,"‚áímax
G Ex,y‚àºQX,Y"
REFERENCES,0.8101694915254237,"
log p(x, y)"
REFERENCES,0.8135593220338984,"q(x, y)"
REFERENCES,0.8169491525423729,"
‚àíEx‚àºQX"
REFERENCES,0.8203389830508474,"
log p(x) q(x) "
REFERENCES,0.823728813559322,"‚áímin
G KL(QX,Y ‚à•PX,Y ) ‚àíKL(QX‚à•PX)"
REFERENCES,0.8271186440677966,"B
ISSUE OF THE ORIGINAL AC-GAN"
REFERENCES,0.8305084745762712,"In this section, we show that original AC-GAN whose auxiliary classiÔ¨Åer is trained with both real and
fake samples still suffer from the issue proved in Theorem 1. Formally, the full objective function
of the original AC-GAN is formulated as follows."
REFERENCES,0.8338983050847457,"max
D,C V (G, D) + Œª ¬∑
 
Ex,y‚àºPX,Y [log C(y|x)] + Ex,y‚àºQX,Y [log C(y|x)]

,"
REFERENCES,0.8372881355932204,"min
G V (G, D) ‚àíŒª ¬∑
 
Ex,y‚àºQX,Y [log C(y|x)]

.
(15)"
REFERENCES,0.8406779661016949,The objective function for training the classiÔ¨Åer can be rewritten as:
REFERENCES,0.8440677966101695,Under review as a conference paper at ICLR 2022
REFERENCES,0.847457627118644,"max
C
Ex,y‚àºPX,Y [log C(y|x)] + Ex,y‚àºQX,Y [log C(y|x)] ‚áímax
C
Ex,y‚àºP m
X,Y [log C(y|x)],
(16)"
REFERENCES,0.8508474576271187,"with pm(x, y) = 1"
REFERENCES,0.8542372881355932,"2(p(x, y) + q(x, y)) and pm(x) = P
y pm(x, y) = 1"
REFERENCES,0.8576271186440678,"2(p(x) + q(x)). And we can
obtain the optimal classiÔ¨Åer by the following:"
REFERENCES,0.8610169491525423,"max
C
Ex,y‚àºP m
X,Y [log C(y|x)] ‚áímin
C Ex‚àºP m
X ,y‚àºP m
Y |X[‚àílog C(y|x)]"
REFERENCES,0.864406779661017,"‚áímin
C Ex‚àºP m
X [H(pm(y|x)) + KL(pm(y|x)‚à•C(y|x))]"
REFERENCES,0.8677966101694915,"‚áíC‚àó(y|x) = pm(y|x) = p(x, y) + q(x, y)"
REFERENCES,0.8711864406779661,p(x) + q(x) (17)
REFERENCES,0.8745762711864407,"Even though the conditional generator learns the joint real data and label distribution, i.e., q(x, y) =
p(x, y) and q(x) = p(x), the optimal classiÔ¨Åer C‚àó(y|x) = p(x,y)+q(x,y)"
REFERENCES,0.8779661016949153,"p(x)+q(x)
= p(x,y)"
REFERENCES,0.8813559322033898,"p(x) will still provide
the objective stated in Theorem 1 to optimize the generator, which contains the conditional entropy
of generated samples HQ(Y |X) that would reduce the intra-class diversity of generated samples.
In other words, the discriminative classiÔ¨Åer does not allow the generator to remain on the desired
distribution because it still provide momentum to update the generator, resulting in a biased learning
objective for the generator."
REFERENCES,0.8847457627118644,Under review as a conference paper at ICLR 2022
REFERENCES,0.888135593220339,"C
MORE RESULTS"
REFERENCES,0.8915254237288136,"C.1
SYNTHETIC DATA"
REFERENCES,0.8949152542372881,"In this section, we report more results on experiments conducted on the one-dimensional synthetic
data and a new two-dimensional synthetic data. The one-dimensional data consists of three Gaussian
components with ¬µ0 = 0, ¬µ1 = 3, ¬µ2 = 6 and œÉ0 = 1, œÉ1 = 2, œÉ2 = 3, and the similar for the two-
dimensional data. For implementing the generator, discriminator, and classiÔ¨Åer, we use three-layer
multi-layer perceptron with hidden size of 10 and the Tanh non-linearity. The optimizer is Adam
with learning rate Œ± = 0.002 and betas (Œ≤1, Œ≤2) = (0.5, 0.999). We train all methods for 40 epochs
with batch size of 256. Table 4 reports the quantitative maximum mean discrepancy (MMD) results
on the one-dimensional synthetic data conducted in Section 5.1. Lower MMD means better learning
results. Figure 6 and Table 5 show the qualitative and quantitative results, respectively, conducted
on the two-dimensional synthetic Gaussian data. In general, the proposed ADC-GAN consistently
replicates the data distribution under different loss function settings."
REFERENCES,0.8983050847457628,AC-GAN
REFERENCES,0.9016949152542373,w/o GAN loss
REFERENCES,0.9050847457627119,Log loss
REFERENCES,0.9084745762711864,Hinge loss
REFERENCES,0.911864406779661,"PD-GAN
TAC-GAN
ADC-GAN Data"
REFERENCES,0.9152542372881356,w/o GAN loss
REFERENCES,0.9186440677966101,Log loss
REFERENCES,0.9220338983050848,Hinge loss
REFERENCES,0.9254237288135593,Figure 5: Distribution learning results on the one-dimensional synthetic data.
REFERENCES,0.9288135593220339,Table 4: MMD (‚Üì) results of each method on the one-dimensional synthetic data.
REFERENCES,0.9322033898305084,"GAN Loss
Class
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN No"
REFERENCES,0.9355932203389831,"Class0
-
1.83
0.05
0.06
Class1
-
27.32
5.99
0.06
Class2
-
27583.31
130.89
0.19
Marginal
-
2919.31
7.81
0.14 Log"
REFERENCES,0.9389830508474576,"Class0
0.02
0.13
0.01
0.06
Class1
0.05
0.64
0.14
0.07
Class2
0.10
639.55
0.90
0.19
Marginal
0.11
75.58
0.66
0.14 Hinge"
REFERENCES,0.9423728813559322,"Class0
11328.12
2.08
0.21
0.40
Class1
10671.01
29.96
3.04
1.22
Class2
8420.67
32011.93
93.08
8.55
Marginal
10218.60
3243.28
7.81
0.80"
REFERENCES,0.9457627118644067,Under review as a conference paper at ICLR 2022
REFERENCES,0.9491525423728814,PD-GAN
REFERENCES,0.9525423728813559,Log loss
REFERENCES,0.9559322033898305,Hinge loss
REFERENCES,0.9593220338983051,"AC-GAN
TAC-GAN
ADC-GAN"
REFERENCES,0.9627118644067797,w/o GAN loss
REFERENCES,0.9661016949152542,Log loss
REFERENCES,0.9694915254237289,Hinge loss
REFERENCES,0.9728813559322034,w/o GAN loss Data
REFERENCES,0.976271186440678,Figure 6: Distribution learning results on the two-dimensional synthetic data.
REFERENCES,0.9796610169491525,Table 5: MMD (‚Üì) results of each method on the two-dimensional synthetic data.
REFERENCES,0.9830508474576272,"GAN Loss
Class
PD-GAN
AC-GAN
TAC-GAN
ADC-GAN No"
REFERENCES,0.9864406779661017,"Class0
-
1.28
1.55
0.07
Class1
-
6.64
152.49
0.09
Class2
-
10491.93
671.89
0.11
Marginal
-
1085.61
166.03
0.02 Log"
REFERENCES,0.9898305084745763,"Class0
0.05
0.06
0.01
0.02
Class1
0.02
0.64
0.27
0.12
Class2
0.73
119.91
0.52
0.86
Marginal
0.01
13.12
0.04
0.08 Hinge"
REFERENCES,0.9932203389830508,"Class0
1625.92
1.42
0.70
0.06
Class1
1138.02
1.43
5.75
0.23
Class2
918.97
9440.54
33.34
0.06
Marginal
1203.51
1019.39
7.99
0.03"
REFERENCES,0.9966101694915255,Figure 7: FID curves with GAN training iterations on Tiny-ImageNet.
