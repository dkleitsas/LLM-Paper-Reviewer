Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009587727708533077,"Recent advances in text-guided video editing have showcased promising results
2"
ABSTRACT,0.0019175455417066154,"in appearance editing (e.g., stylization). However, video motion editing in the
3"
ABSTRACT,0.0028763183125599234,"temporal dimension (e.g., from eating to waving), which distinguishes video edit-
4"
ABSTRACT,0.003835091083413231,"ing from image editing, is underexplored. In this work, we present UniEdit, a
5"
ABSTRACT,0.004793863854266539,"tuning-free framework that supports both video motion and appearance editing by
6"
ABSTRACT,0.005752636625119847,"harnessing the power of a pre-trained text-to-video generator within an inversion-
7"
ABSTRACT,0.006711409395973154,"then-generation framework. To realize motion editing while preserving source
8"
ABSTRACT,0.007670182166826462,"video content, based on the insights that temporal and spatial self-attention layers
9"
ABSTRACT,0.00862895493767977,"encode inter-frame and intra-frame dependency respectively, we introduce auxiliary
10"
ABSTRACT,0.009587727708533078,"motion-reference and reconstruction branches to produce text-guided motion and
11"
ABSTRACT,0.010546500479386385,"source features respectively. The obtained features are then injected into the main
12"
ABSTRACT,0.011505273250239693,"editing path via temporal and spatial self-attention layers. Extensive experiments
13"
ABSTRACT,0.012464046021093002,"demonstrate that UniEdit covers video motion editing and various appearance
14"
ABSTRACT,0.013422818791946308,"editing scenarios, and surpasses the state-of-the-art methods. Our code will be
15"
ABSTRACT,0.014381591562799617,"publicly available.
16"
INTRODUCTION,0.015340364333652923,"1
Introduction
17"
INTRODUCTION,0.016299137104506232,"The advent of pre-trained diffusion-based [26, 53] text-to-image generators [49, 50, 48] has revo-
18"
INTRODUCTION,0.01725790987535954,"lutionized the fields of design and filmmaking, opening new vistas for creative expression. These
19"
INTRODUCTION,0.01821668264621285,"advancements, underpinned by seminal works in text-to-image synthesis, have paved the way for inno-
20"
INTRODUCTION,0.019175455417066157,"vative text-guided editing techniques for both images [42, 24, 4, 5] and videos [65, 6, 39, 70, 17, 46].
21"
INTRODUCTION,0.020134228187919462,"Such techniques not only enhance creative workflows but also promise to redefine content creation
22"
INTRODUCTION,0.02109300095877277,"within these industries.
23"
INTRODUCTION,0.02205177372962608,"Video editing, in contrast to image editing, introduces the intricate challenge of ensuring frame-wise
24"
INTRODUCTION,0.023010546500479387,"consistency. Efforts to address this challenge have led to the development of methods that leverage
25"
INTRODUCTION,0.023969319271332695,"shared features and structures with the source video [6, 39, 37, 70, 46, 7, 33, 62, 18] through an
26"
INTRODUCTION,0.024928092042186004,"inversion-then-generation pipeline [42, 53], exemplified by Pix2Video’s approach [6] to consistent
27"
INTRODUCTION,0.02588686481303931,"appearance editing across frames. To transfer the edited appearance from the anchor frame to the
28"
INTRODUCTION,0.026845637583892617,"remaining frames consistently, it employs a pre-trained image generator and extends the self-attention
29"
INTRODUCTION,0.027804410354745925,"layers to cross-frame attention to generate each remaining frame. Despite these advancements in
30"
INTRODUCTION,0.028763183125599234,"performing video appearance editing (e.g., stylization, object appearance replacement, etc.), these
31"
INTRODUCTION,0.029721955896452542,"methodologies fall short in editing video motion (e.g., replacing the movement of playing guitar with
32"
INTRODUCTION,0.030680728667305847,"waving), hampered by a lack of motion priors and limited control over inter-frame dependencies,
33"
INTRODUCTION,0.031639501438159155,"underscoring a critical gap in video editing capabilities.
34"
INTRODUCTION,0.032598274209012464,"Previous attempts [65, 44] at video motion editing through fine-tuning a pre-trained generator on
35"
INTRODUCTION,0.03355704697986577,"the given source video and then editing motion through text guidance. Although effective, they
36"
INTRODUCTION,0.03451581975071908,"necessitate a delicate balance between the generative prowess of the model and the preservation of
37"
INTRODUCTION,0.03547459252157239,"the source video’s content. This compromise often leads to restricted motion diversity and unwanted
38"
INTRODUCTION,0.0364333652924257,"content variations, indicating a pressing need for a more robust solution.
39"
INTRODUCTION,0.037392138063279005,"In response, our work aims to explore a tuning-free framework that adeptly navigates the complexities
40"
INTRODUCTION,0.038350910834132314,"of editing both the motion and appearance of videos. To achieve this, we identify three technical
41"
INTRODUCTION,0.039309683604985615,"challenges: 1) it is non-trivial to incorporate the text-guided motion into the source content, as directly
42"
INTRODUCTION,0.040268456375838924,"applying video appearance editing [46, 18] or image editing [5] schemes leads to undesirable results
43"
INTRODUCTION,0.04122722914669223,"(as shown in Fig. 5); 2) preserving the non-edited content of the source video; 3) inheriting the spatial
44"
INTRODUCTION,0.04218600191754554,"structure of the source video during appearance editing.
45"
INTRODUCTION,0.04314477468839885,"Our solution, UniEdit, harnesses the power of a pre-trained text-to-video generator (e.g., LaVie [63])
46"
INTRODUCTION,0.04410354745925216,"within an inversion-then-generation framework [42], tailored to overcome the identified challenges.
47"
INTRODUCTION,0.045062320230105465,"Particularly, we introduce three key innovations: 1) To inject text-guided motion into the source
48"
INTRODUCTION,0.046021093000958774,"content, we highlight the insight that the temporal self-attention layers of the generator encode
49"
INTRODUCTION,0.04697986577181208,"the inter-frame dependency. Acting in this way, we introduce an auxiliary motion-reference branch
50"
INTRODUCTION,0.04793863854266539,"to generate text-guided motion features, which are then injected into the main editing path via
51"
INTRODUCTION,0.0488974113135187,"temporal self-attention layers. 2) To preserve the non-edited content of the source video, motivated
52"
INTRODUCTION,0.04985618408437201,"by the image editing technique [5], we follow the insight that the spatial self-attention layers of the
53"
INTRODUCTION,0.05081495685522531,"generator encode the intra-frame dependency. Therefore, we introduce an auxiliary reconstruction
54"
INTRODUCTION,0.05177372962607862,"branch, and inject the features obtained from the spatial self-attention layers of the reconstruction
55"
INTRODUCTION,0.052732502396931925,"branch into the main editing path. 3) To retain the spatial structure during the appearance editing, we
56"
INTRODUCTION,0.053691275167785234,"replace the spatial attention maps of the main editing path with those in the reconstruction branch.
57"
INTRODUCTION,0.05465004793863854,"To our best knowledge, UniEdit represents a pioneering leap in text-guided, tuning-free video
58"
INTRODUCTION,0.05560882070949185,"motion editing. In addition, its unified architecture not only facilitates a wide array of video
59"
INTRODUCTION,0.05656759348034516,"appearance editing tasks, as shown in Fig. 1, but also empowers image-to-video generators for
60"
INTRODUCTION,0.05752636625119847,"zero-shot text-image-to-video generation. Through comprehensive experimentation, we demonstrate
61"
INTRODUCTION,0.058485139022051776,"UniEdit’s superior performance relative to existing state-of-the-art methods, highlighting its potential
62"
INTRODUCTION,0.059443911792905084,"to significantly advance the field of video editing.
63"
RELATED WORKS,0.06040268456375839,"2
Related Works
64"
VIDEO GENERATION,0.061361457334611694,"2.1
Video Generation
65"
VIDEO GENERATION,0.062320230105465,"Researchers have achieved video generation with generative adversarial networks [58, 51, 61],
66"
VIDEO GENERATION,0.06327900287631831,"language models [69, 71], or diffusion models [28, 52, 25, 23, 3, 60, 72, 19, 63, 8, 47]. To make the
67"
VIDEO GENERATION,0.06423777564717162,"generation more controllable, recent endeavors have also incorporated additional structure guidance
68"
VIDEO GENERATION,0.06519654841802493,"(e.g., depth map) [16, 10, 74, 11, 20, 64], or conducted customized generation [65, 67, 34, 75, 59, 41].
69"
VIDEO GENERATION,0.06615532118887824,"These models have generally learned real-world video distribution from large-scale data, and achieved
70"
VIDEO GENERATION,0.06711409395973154,"promising results on text-to-video or image-to-video generation. Based on their success, we leverage
71"
VIDEO GENERATION,0.06807286673058485,"the learned prior in the pre-trained model to achieve tuning-free video motion and appearance editing.
72"
VIDEO EDITING,0.06903163950143816,"2.2
Video Editing
73"
VIDEO EDITING,0.06999041227229147,"Video editing aims to produce a new video that is aligned with the provided editing instructions
74"
VIDEO EDITING,0.07094918504314478,"(e.g., text) while maintaining the other characteristics of the source video. It can be categorized into
75"
VIDEO EDITING,0.07190795781399809,"appearance and motion editing.
76"
VIDEO EDITING,0.0728667305848514,"For appearance editing [70, 15, 17, 35, 12], like turn a video into the style of Van Gogh, the main
77"
VIDEO EDITING,0.0738255033557047,"challenge is to achieve temporal-consistent generation across different frames. Early attempts [6, 37,
78"
VIDEO EDITING,0.07478427612655801,"46, 7, 33, 62] leveraged text-to-image models with inter-frame propagation to ensure consistency.
79"
VIDEO EDITING,0.07574304889741132,"For instance, Pix2Video [6] replaces the key and value of the current frame with those of the
80"
VIDEO EDITING,0.07670182166826463,"first and previous frame. Video-P2P [39] achieved local editing via video-specific fine-tuning and
81"
VIDEO EDITING,0.07766059443911794,"unconditional embedding optimization [43]. Follow-up studies [18, 70, 45] also leveraged the edit-
82"
VIDEO EDITING,0.07861936720997123,"then-propagate framework with neatest-neighbor field [18], estimated optical flow [70], or temporal
83"
VIDEO EDITING,0.07957813998082454,"deformation field [45]. Despite the promising results, due to the constraint on the source video
84"
VIDEO EDITING,0.08053691275167785,"structure, these approaches are specialized in appearance editing and can not be applied to motion
85"
VIDEO EDITING,0.08149568552253116,"editing directly.
86"
VIDEO EDITING,0.08245445829338446,"Recent studies have also explored video motion editing with text guidance [65, 44], user-provided
87"
VIDEO EDITING,0.08341323106423777,"motion [32, 54, 15], or specific motion representation [55, 36, 22]. For example, Dreamix [44]
88"
VIDEO EDITING,0.08437200383509108,"proposed fine-tuning a pre-trained text-to-video model with mixed video-image reconstruction
89"
VIDEO EDITING,0.08533077660594439,"objectives for each source video. Then the editing is realized by conditioning the fine-tuned model
90"
VIDEO EDITING,0.0862895493767977,"on the given target prompt. MoCA [68] decoupled the video into the first-frame appearance and
91"
VIDEO EDITING,0.087248322147651,"the optical flow, and trained a diffusion model to generate video conditioned on the first frame and
92"
VIDEO EDITING,0.08820709491850431,"the text. However, it struggled to preserve the non-edited motion (e.g., background dynamics) as
93"
VIDEO EDITING,0.08916586768935762,"it generates the entire motion from the text. Different from the aforementioned approaches that
94"
VIDEO EDITING,0.09012464046021093,"require fine-tuning or user-provided motion input, we are the first to achieve tuning-free motion and
95"
VIDEO EDITING,0.09108341323106424,"appearance editing with text guidance only.
96"
VIDEO EDITING,0.09204218600191755,"3
Preliminaries: Video Diffusion Models
97"
VIDEO EDITING,0.09300095877277086,"Our proposed UniEdit is built upon video diffusion models. Therefore, we first recap the architecture
98"
VIDEO EDITING,0.09395973154362416,"that is used in common text-guided video diffusion models [63, 2].
99"
VIDEO EDITING,0.09491850431447747,"Overall Architecture
Modern text-to-video (T2V) diffusion models typically extend a pre-trained
100"
VIDEO EDITING,0.09587727708533078,"text-to-image (T2I) model [49] to the video domain with the following adaptations. 1) Introducing
101"
VIDEO EDITING,0.09683604985618409,"additional temporal layers by inflating 2d convolutional layers to 3d form, or adding temporal
102"
VIDEO EDITING,0.0977948226270374,"self-attention layers [57] to model the correlation between video frames. 2) Due to the extensive
103"
VIDEO EDITING,0.0987535953978907,"computational resources for modeling spatial-temporal joint distribution, these works typically
104"
VIDEO EDITING,0.09971236816874401,"first train video generation models on low spatial and temporal resolutions, and then upsampling
105"
VIDEO EDITING,0.10067114093959731,"the generated results with cascaded models. 3) Other improvements like efficiency [1], training
106"
VIDEO EDITING,0.10162991371045062,"strategy [19], or additional control signals [16], etc. During inference, given standard Gaussian
107"
VIDEO EDITING,0.10258868648130393,"distribution zT ∼N(0, 1), the denoising UNet is used to perform T denoising steps to obtain the
108"
VIDEO EDITING,0.10354745925215723,"outputs [26, 53]. If the model is trained in latent space [49], a decoder is employed to reconstruct
109"
VIDEO EDITING,0.10450623202301054,"videos from the latent domain.
110"
VIDEO EDITING,0.10546500479386385,"Attention Mechanisms
In particular, for each block of the denoising UNet, there are four basic
111"
VIDEO EDITING,0.10642377756471716,"modules: a convolutional module, a spatial self-attention module (SA-S), a spatial cross-attention
112"
VIDEO EDITING,0.10738255033557047,"module (CA-S), and a temporal self-attention module (SA-T). Formally, the attention operation [57]
113"
VIDEO EDITING,0.10834132310642378,"can be formulated as:
114"
VIDEO EDITING,0.10930009587727708,"attn(Q, K, V ) = softmax(QKT √"
VIDEO EDITING,0.11025886864813039,"d
)V,
(1)"
VIDEO EDITING,0.1112176414189837,"where Q (query), K (key), V (value) are derived from inputs, and d is the dimension of hidden states.
115"
VIDEO EDITING,0.11217641418983701,"Intuitively, CA-S is in charge of fusing semantics from the text condition, SA-S models the intra-
116"
VIDEO EDITING,0.11313518696069032,"frame dependency, SA-T models the inter-frame dependency and ensures the generated results are
117"
VIDEO EDITING,0.11409395973154363,"temporally consistent. We leverage these intuitions in our designs as elaborated below.
118"
VIDEO EDITING,0.11505273250239693,"Figure 2: Overview of UniEdit. It follows an inversion-then-generation pipeline and consists of a
main editing path, an auxiliary reconstruction branch and an auxiliary motion-reference branch. The
reconstruction branch produces source features for content preservation, and the motion-reference
branch yields text-guided motion features for motion injection. The source features and motion
features are injected into the main editing path through spatial self-attention (SA-S) and temporal
self-attention (SA-T) modules respectively (Sec. 4.1). We further introduce spatial structure control
to retain the coarse structure of the source video (Sec. 4.2)."
UNIEDIT,0.11601150527325024,"4
UniEdit
119"
UNIEDIT,0.11697027804410355,"Method Overview.
As shown in Fig. 2, our main editing path is based on an inversion-then-
120"
UNIEDIT,0.11792905081495686,"generation pipeline: we use the latent after DDIM inversion [53] as the initial noise zT 1, then perform
121"
UNIEDIT,0.11888782358581017,"denoising process starting from zT with the pre-trained UNet conditioned on the target prompt Pt. For
122"
UNIEDIT,0.11984659635666348,"motion editing, to achieve source content preservation and motion control, we propose to incorporate
123"
UNIEDIT,0.12080536912751678,"an auxiliary reconstruction branch and an auxiliary motion-reference branch to provide desired source
124"
UNIEDIT,0.12176414189837009,"and motion features, which are injected into the main editing path to achieve content preservation and
125"
UNIEDIT,0.12272291466922339,"motion editing (as shown in Fig. 3). We propose the pipeline of motion editing and appearance editing
126"
UNIEDIT,0.1236816874400767,"in Sec. 4.1 & Sec. 4.2 respectively. To further alleviate the background inconsistency, we introduce
127"
UNIEDIT,0.12464046021093,"a mask-guided coordination scheme in Sec. 4.3. We also extend UniEdit to text-image-to-video
128"
UNIEDIT,0.12559923298178333,"generation (TI2V) in Sec. 4.4.
129"
TUNING-FREE VIDEO MOTION EDITING,0.12655800575263662,"4.1
Tuning-Free Video Motion Editing
130"
TUNING-FREE VIDEO MOTION EDITING,0.12751677852348994,"Content Preservation on SA-S Modules.
One of the key challenges of editing tasks is to inherit
131"
TUNING-FREE VIDEO MOTION EDITING,0.12847555129434324,"the original content (e.g., textures and background) in the source video. To this end, we introduce
132"
TUNING-FREE VIDEO MOTION EDITING,0.12943432406519656,"an auxiliary reconstruction branch. The reconstruction path starts from the same inversed latent
133"
TUNING-FREE VIDEO MOTION EDITING,0.13039309683604985,"zT similar to the main editing path, and then conducts the denoising process with the pre-trained
134"
TUNING-FREE VIDEO MOTION EDITING,0.13135186960690318,"UNet conditioned on the source prompt Ps to reconstruct the original frames. As verified in image
135"
TUNING-FREE VIDEO MOTION EDITING,0.13231064237775647,"editing [56, 24, 5], the attention features in the denoising model during reconstruction contain the
136"
TUNING-FREE VIDEO MOTION EDITING,0.1332694151486098,"content of the source video. Hence, we inject attention features of the reconstruction path into the
137"
TUNING-FREE VIDEO MOTION EDITING,0.1342281879194631,"main editing path on spatial self-attention (SA-S) layers for content preservation. At denoising step t,
138"
TUNING-FREE VIDEO MOTION EDITING,0.13518696069031638,"the attention operation of the l-th SA-S module in the main editing path is formulated as:
139"
TUNING-FREE VIDEO MOTION EDITING,0.1361457334611697,"SA-Sl
edit :=
attn(Q, K, V r),
t < t0 and l > L,
attn(Q, K, V ),
otherwise,
(2)"
TUNING-FREE VIDEO MOTION EDITING,0.137104506232023,"where Q, K, V are features in the main editing path, V r refer to the value feature of the corresponding
140"
TUNING-FREE VIDEO MOTION EDITING,0.13806327900287632,"SA-S layer in the reconstruction branch, t0 = 50 and L = 10 are hyper-parameters following previous
141"
TUNING-FREE VIDEO MOTION EDITING,0.13902205177372962,"work [5]. By replacing the value of spatial features, the video synthesized by the main editing path
142"
TUNING-FREE VIDEO MOTION EDITING,0.13998082454458294,"retains the non-edited characters (e.g., identity and background) of the source video, as exhibited
143"
TUNING-FREE VIDEO MOTION EDITING,0.14093959731543623,"in Fig. 7a. Unlike previous video editing works [37, 29] which introduces a cross-frame attention
144"
TUNING-FREE VIDEO MOTION EDITING,0.14189837008628955,"mechanism (i.e., using the key and value of the first/last frame), we implement Eq. 2 frame-wisely to
145"
TUNING-FREE VIDEO MOTION EDITING,0.14285714285714285,"better tackle source video with large dynamics.
146"
TUNING-FREE VIDEO MOTION EDITING,0.14381591562799617,"1For real source video, we set source prompt to null during both forward and inversion process to achieve
high-quality reconstruction [43]."
TUNING-FREE VIDEO MOTION EDITING,0.14477468839884947,"Motion Injection on SA-T Modules.
After implementing the content-preserving technique intro-
147"
TUNING-FREE VIDEO MOTION EDITING,0.1457334611697028,"duced above, we can obtain an edited video with the same content in the source video. However, it
148"
TUNING-FREE VIDEO MOTION EDITING,0.14669223394055608,"is observed that the output video could not follow the text prompt Pt properly. A straightforward
149"
TUNING-FREE VIDEO MOTION EDITING,0.1476510067114094,"solution is to increase the value of L so that balancing between the impact of injected information and
150"
TUNING-FREE VIDEO MOTION EDITING,0.1486097794822627,"the conditioned text prompt. Nevertheless, this could result in a content mismatch with the original
151"
TUNING-FREE VIDEO MOTION EDITING,0.14956855225311602,"source video in terms of structures and textures.
152"
TUNING-FREE VIDEO MOTION EDITING,0.15052732502396932,"To obtain the desired motion without sacrificing content consistency, we propose to guide the main
153"
TUNING-FREE VIDEO MOTION EDITING,0.15148609779482264,"editing path with reference motion. Concretely, an auxiliary motion-reference branch (which also
154"
TUNING-FREE VIDEO MOTION EDITING,0.15244487056567593,"starts from the inversed latent zT ) is involved during the denoising process. Different from the
155"
TUNING-FREE VIDEO MOTION EDITING,0.15340364333652926,"reconstruction branch, the motion-reference branch is conditioned on the target prompt Pt, which
156"
TUNING-FREE VIDEO MOTION EDITING,0.15436241610738255,"contains the description of the desired motion. To transfer the motion into the main editing path, our
157"
TUNING-FREE VIDEO MOTION EDITING,0.15532118887823587,"core insight here is that temporal layers model the inter-frame dependency of the synthesized video
158"
TUNING-FREE VIDEO MOTION EDITING,0.15627996164908917,"clip (as shown in Fig. 6). Motivated by the observations above, we design the attention map injection
159"
TUNING-FREE VIDEO MOTION EDITING,0.15723873441994246,"on temporal self-attention layers of the main editing path:
160"
TUNING-FREE VIDEO MOTION EDITING,0.15819750719079578,"SA-Tl
edit := attn(Qm, Km, V )
(3)"
TUNING-FREE VIDEO MOTION EDITING,0.15915627996164908,"where Qm and Km refer to the query and key of the motion-reference branch, note that we replace
161"
TUNING-FREE VIDEO MOTION EDITING,0.1601150527325024,"the query and key of SA-T modules in the main editing path with those in the motion-reference
162"
TUNING-FREE VIDEO MOTION EDITING,0.1610738255033557,"branch on all layers and denoising steps. It’s observed that the injection of temporal attention maps
163"
TUNING-FREE VIDEO MOTION EDITING,0.16203259827420902,"can effectively facilitate the main editing path to generate motion aligned with the target prompt.
164"
TUNING-FREE VIDEO MOTION EDITING,0.1629913710450623,"To better fuse the motion with the content in the source video, we also implement spatial structure
165"
TUNING-FREE VIDEO MOTION EDITING,0.16395014381591563,"control (refer to Sec. 4.2) on the main editing path and motion-reference branch in the early steps.
166"
TUNING-FREE VIDEO APPEARANCE EDITING,0.16490891658676893,"4.2
Tuning-Free Video Appearance Editing
167"
TUNING-FREE VIDEO APPEARANCE EDITING,0.16586768935762225,"Figure 3: Detailed illustration of the relation-
ship between the main editing path, the auxiliary
reconstruction branch and the auxiliary motion-
reference branch. The content preservation, motion
injection and spatial structure control are achieved
by the fusion of Q (query), K (key), V (value) fea-
tures in spatial self-attention (SA-S) and temporal
self-attention (SA-T) modules."
TUNING-FREE VIDEO APPEARANCE EDITING,0.16682646212847554,"In Sec. 4.1, we introduce the pipeline of UniEdit
168"
TUNING-FREE VIDEO APPEARANCE EDITING,0.16778523489932887,"for video motion editing. In this subsection,
169"
TUNING-FREE VIDEO APPEARANCE EDITING,0.16874400767018216,"we aim to perform appearance editing (e.g.,
170"
TUNING-FREE VIDEO APPEARANCE EDITING,0.16970278044103548,"style transfer, object replacement, background
171"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17066155321188878,"changing) via the same framework. In general,
172"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1716203259827421,"there are two main differences between appear-
173"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1725790987535954,"ance editing and motion editing. Firstly, ap-
174"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17353787152444872,"pearance editing does not require changing the
175"
TUNING-FREE VIDEO APPEARANCE EDITING,0.174496644295302,"inter-frame relationships. Therefore, we remove
176"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17545541706615533,"the motion-reference branch and corresponding
177"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17641418983700863,"motion injection mechanism from the motion
178"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17737296260786195,"editing pipeline. Secondly, the main challenge
179"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17833173537871524,"of appearance editing is to maintain the struc-
180"
TUNING-FREE VIDEO APPEARANCE EDITING,0.17929050814956854,"tural consistency of the source video. To address
181"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18024928092042186,"this, we introduce spatial structure control be-
182"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18120805369127516,"tween the main editing path and the reconstruc-
183"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18216682646212848,"tion branch.
184"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18312559923298177,"Spatial Structure Control on SA-S Modules.
185"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1840843720038351,"Previous approaches on video appearance editing [70, 18] mainly realize spatial structure control
186"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1850431447746884,"with the assistance of additional network [73]. When the auxiliary control model fails, it may result
187"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1860019175455417,"in inferior performance in preserving the structure of the original video. Alternatively, we suggest
188"
TUNING-FREE VIDEO APPEARANCE EDITING,0.186960690316395,"extracting the layout information of the source video from the reconstruction branch. Intuitively,
189"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18791946308724833,"the attention maps in spatial self-attention layers encode the structure of the synthesized video, as
190"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18887823585810162,"verified in Fig. 6. Hence, we replace the query and key of SA-S module in the main editing path with
191"
TUNING-FREE VIDEO APPEARANCE EDITING,0.18983700862895495,"those in the reconstruction branch:
192"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19079578139980824,"SA-Sl
edit :=
attn(Qr, Kr, V ),
t < t1,
attn(Q, K, V ),
otherwise,
(4)"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19175455417066156,"where Qr and Kr refer to the query and key of the reconstruction branch, t1 is used to control the
193"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19271332694151486,"extent of editing. It is worth mentioning that the effect of spatial structure control is distinct from the
194"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19367209971236818,"content preservation mechanism in Sec. 4.1. Take stylization as an example, the proposed structure
195"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19463087248322147,"control in Eq. 4 only ensures consistency in terms of each frame’s composition, while enabling the
196"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1955896452540748,"model to generate the required textures and styles based on the text prompt. On the other hand,
197"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1965484180249281,"the content preservation technique inherits the textures and style of the source video. Therefore,
198"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1975071907957814,"we use structure control instead of content preservation for appearance editing. In addition, using
199"
TUNING-FREE VIDEO APPEARANCE EDITING,0.1984659635666347,"the proposed structure control technique in motion editing can make the layout of the output video
200"
TUNING-FREE VIDEO APPEARANCE EDITING,0.19942473633748803,"similar to the source video (shown in Fig. 11b in Appendix). Users have the flexibility to adjust the
201"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20038350910834132,"consistency between the edited video and the source video layout based on their specific requirements.
202"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20134228187919462,"4.3
Mask-Guided Coordination (Optional)
203"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20230105465004794,"To further improve the editing performance, we suggest leveraging the foreground/background
204"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20325982742090123,"segmentation mask M to guide the denoising process [14, 13]. There are two possible ways to obtain
205"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20421860019175456,"the mask M: the attention maps of CA-S modules with a threshold [24]; or employing an off-the-shelf
206"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20517737296260785,"segmentation model [38] on the source and generated videos. The obtained segmentation masks can
207"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20613614573346117,"be leveraged to 1), alleviate the indistinction in foreground and background; 2), improve content
208"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20709491850431447,"consistency between edited and source videos. To this end, we leverage mask-guided self-attention in
209"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2080536912751678,"the main editing path to coordinate the editing process. Formally, we define:
210"
TUNING-FREE VIDEO APPEARANCE EDITING,0.20901246404602108,"m-attn(Q, K, V ; M) = softmax(QKT √"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2099712368168744,"d
+ M)V.
(5)"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2109300095877277,"Then the mask-guided self-attention:
211"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21188878235858102,"SAmask := m-attn(Q, K, V ; M f) ⊙Mm + m-attn(Q, K, V ; M b) ⊙(1 −Mm),
(6)"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21284755512943432,"where M f, M b ∈{−∞, 0} indicate the foreground and background masks in the editing path
212"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21380632790028764,"respectively, Mm ∈{0, 1} denotes the foreground mask from the motion-reference branch, and ⊙is
213"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21476510067114093,"Hadamard product. In addition, we leverage the mask during the content preservation and motion
214"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21572387344199426,"injection for the features obtained from the reconstruction branch and the motion-reference branch
215"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21668264621284755,"(e.g., we replace Qm with Mm ⊙Qm + (1 −Mm) ⊙Q).
216"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21764141898370087,"4.4
T2V Models are Zero-Shot TI2V Generators
217"
TUNING-FREE VIDEO APPEARANCE EDITING,0.21860019175455417,"To make our framework more flexible, we further derive a method to incorporate images as input
218"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2195589645254075,"and synthesize high-quality video conditioned on both image and text-prompt. Different from some
219"
TUNING-FREE VIDEO APPEARANCE EDITING,0.22051773729626079,"image animation techniques [2], our method allows the user to guide the animation process with text
220"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2214765100671141,"prompts. Concretely, we first achieve image-to-video (I2V) generation by: 1) transforming input
221"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2224352828379674,"images with simulated camera movement to form a pseudo-video clip [44] or 2) leveraging existing
222"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2233940556088207,"image animation approaches (e.g., SVD [2], AnimateDiff [21]) to synthesis a video with random
223"
TUNING-FREE VIDEO APPEARANCE EDITING,0.22435282837967402,"motion (which may not consistent with the text prompt). Then, we perform text-guided editing with
224"
TUNING-FREE VIDEO APPEARANCE EDITING,0.2253116011505273,"UniEdit on the vanilla video to obtain the final output video.
225"
EXPERIMENTS,0.22627037392138064,"5
Experiments
226"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.22722914669223393,"5.1
Comparison with State-of-the-Art Methods
227"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.22818791946308725,"Implementation Details
UniEdit is not limited to specific video diffusion models. In this section,
228"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.22914669223394055,"we build UniEdit upon LaVie [63] as an instantiation to verify the effectiveness of our method. To
229"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23010546500479387,"demonstrate the flexibility of UniEdit across different base models, we also implement the proposed
230"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23106423777564716,"method on VideoCrafter2 [9] and exhibit the editing results in Appendix B.1. For each input video,
231"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23202301054650049,"we follow the pre-processing step in LaVie to the resolution of 320 × 512. Then, the pre-processed
232"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23298178331735378,"video is fed into the UniEdit to perform video editing. It takes 1-2 minutes to edit on an NVIDIA
233"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2339405560882071,"A100 GPU for each video. More details can be found in Appendix A.
234"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2348993288590604,"Baselines.
To evaluate the performance of UniEdit, we compare the editing results of UniEdit
235"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23585810162991372,"with state-of-the-art motion and appearance editing approaches. For motion editing, due to the
236"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.236816874400767,"lack of open-source tuning-free (zero-shot) methods, we adapt the state-of-the-art non-rigid image
237"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23777564717162034,"editing technique MasaCtrl [5] to a T2V model [63] (denoted as MasaCtrl∗in Fig. 5) and a one-shot
238"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23873441994247363,"video editing method Tune-A-Video (TAV) [65] as strong baselines. For appearance editing, we
239"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.23969319271332695,"use the latest methods with strong performance, including FateZero [46], TokenFlow [18], and
240"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24065196548418025,"Rerender-A-Video (Rerender) [70] as baselines.
241"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24161073825503357,"Evaluation Set.
The evaluation set consists of 100 samples, including: a) 20 randomly sampled
242"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24256951102588686,"video clips from the open-source LOVEU-TGVE-2023 [66] dataset, along with their corresponding
243"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24352828379674019,"80 text prompts, and b) 20 videos from online sources (www.pexels.com and www.pixabay.com),
244"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24448705656759348,"with manually designed prompts, as the baseline methods do not have an open-source evaluation set.
245"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24544582933844677,"Figure 4: Examples edited by UniEdit. For each case, the upper frames come from the source video,
and the lower frames indicate the edited results with the target prompt. We encourage the readers to
watch the videos and make evaluations."
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2464046021093001,"Qualitative Results.
We present editing examples of UniEdit in Fig. 1, Fig. 4 (additional examples
246"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2473633748801534,"in Fig. 16-21 of Appendix B.8). Please visit our project page for more videos. UniEdit demonstrates
247"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2483221476510067,"the ability to: 1) edit in various scenarios, including motion-changing, object replacement, style
248"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.24928092042186,"transfer, and background modification; 2) align with the target prompt; and 3) maintain excellent
249"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.25023969319271333,"temporal consistency. Additionally, we compare UniEdit with state-of-the-art methods in Fig. 5
250"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.25119846596356665,"(further comparisons in Fig.13,14,15 of Appendix B.7). For a fair comparison, we also migrated
251"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2521572387344199,"all baselines to LaVie [63], using the same base model as our method. The results are presented
252"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.25311601150527324,"in Fig. 15. For appearance editing, we showcase two scenarios: non-rigid object replacement and
253"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.25407478427612656,"stylization. In object replacement, our method outperforms baselines in terms of prompt alignment
254"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2550335570469799,"and background consistency. In stylization, UniEdit excels in preserving content. For example, the
255"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.25599232981783315,"grassland retains its original appearance without any additional elements. In motion editing, UniEdit
256"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2569511025886865,"surpasses baselines in aligning the video with the target prompt and preserving the source content.
257"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2579098753595398,"Quantitative Results.
We quantitatively evaluate our method using two approaches: 1) CLIP
258"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2588686481303931,"scores and user preference, as employed in previous work [65]; and 2) VBench [31] scores, a recently
259"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2598274209012464,"proposed benchmark suite for T2V models. The summarized results are in Tab. 1. Following previous
260"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2607861936720997,"work [65], we assess the effectiveness of our method in terms of temporal consistency and alignment
261"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.26174496644295303,"with the target prompt. Additionally, we conducted a user study involving 10 participants who rated
262"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.26270373921380635,"the edited videos on a scale of 1 to 5. We also utilize the recently proposed VBench [31] benchmark
263"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2636625119846596,"to provide a more comprehensive assessment, which includes ‘Frame Quality’ metrics and ‘Temporal
264"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.26462128475551294,"Quality’ metrics. UniEdit outperforms the baseline methods across all metrics. Furthermore, the
265"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.26558005752636626,"mask-guided coordination technique introduced in Sec. 4.3 further enhances performance (see
266"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.2665388302972196,"Appendix B.3). For more detailed quantitative results, please refer to Appendix B.2&B.3&B.5.
267"
ABLATION STUDY AND ANALYSIS,0.26749760306807285,"5.2
Ablation Study and Analysis
268"
ABLATION STUDY AND ANALYSIS,0.2684563758389262,"How UniEdit Works?
To better understand how UniEdit works and reveal our insight on the
269"
ABLATION STUDY AND ANALYSIS,0.2694151486097795,"spatial and temporal self-attention layers, we visualize the features in the SA-S and SA-T modules
270"
ABLATION STUDY AND ANALYSIS,0.27037392138063276,"and compare them with the magnitude of optical flow between adjacent frames in Fig. 6a. It is evident
271"
ABLATION STUDY AND ANALYSIS,0.2713326941514861,"that, in comparison to the spatial query maps (2nd row), the temporal cross-frame attention maps (3rd
272"
ABLATION STUDY AND ANALYSIS,0.2722914669223394,"row) exhibit a notably higher degree of overlap with the optical flow (4th row). This indicates that the
273"
ABLATION STUDY AND ANALYSIS,0.27325023969319273,"temporal self-attention layers encode inter-frame dependencies and facilitate motion injection, while
274"
ABLATION STUDY AND ANALYSIS,0.274209012464046,"content preservation and structure control are carried out in the spatial self-attention layers.
275"
ABLATION STUDY AND ANALYSIS,0.2751677852348993,"Figure 5: Comparison with state-of-the-art methods for both video appearance and motion editing. It
shows that UniEdit achieves better source content preservation, and outperforms baselines in motion
editing by a large margin."
ABLATION STUDY AND ANALYSIS,0.27612655800575264,Table 1: Quantitative comparison with state-of-the-art video editing techniques.
ABLATION STUDY AND ANALYSIS,0.27708533077660596,Method
ABLATION STUDY AND ANALYSIS,0.27804410354745923,"Frame Consistency Textual Alignment
Frame Quality
Temporal Quality"
ABLATION STUDY AND ANALYSIS,0.27900287631831255,"CLIP
Score
User
Pref."
ABLATION STUDY AND ANALYSIS,0.2799616490891659,"CLIP
Score
User
Pref."
ABLATION STUDY AND ANALYSIS,0.2809204218600192,"Aesthetic
Quality
Imaging
Quality"
ABLATION STUDY AND ANALYSIS,0.28187919463087246,"Subject
Consistency
Motion
Smoothness
Temporal
Flickering"
ABLATION STUDY AND ANALYSIS,0.2828379674017258,"TAV [65]
95.39
3.74
27.89
3.30
51.97
49.60
93.10
93.27
91.48
MasaCtrl∗[5]
97.61
4.31
25.58
3.17
54.58
58.72
93.04
95.70
94.29
FateZero [46]
96.72
4.48
27.30
3.48
53.77
56.99
93.55
94.80
93.42
Rerender [70]
97.18
4.16
27.94
3.55
54.59
57.97
93.08
95.57
94.36
TokenFlow[18]
97.02
4.50
28.58
3.34
52.60
60.65
91.97
95.04
93.50"
ABLATION STUDY AND ANALYSIS,0.2837967401725791,"UniEdit
98.35
4.72
31.43
4.79
58.25
62.94
95.73
97.30
96.74
UniEdit-Mask
98.36
4.73
31.50
4.90
58.77
63.12
95.86
97.28
96.79"
ABLATION STUDY AND ANALYSIS,0.28475551294343243,"Output Visualization of the Two Auxiliary Branches.
Recall that to perform motion editing,
276"
ABLATION STUDY AND ANALYSIS,0.2857142857142857,"we propose to transfer the targeted motion from the motion-reference branch and realize content
277"
ABLATION STUDY AND ANALYSIS,0.286673058485139,"preservation via feature injection from the reconstruction branch. To verify the effectiveness, we
278"
ABLATION STUDY AND ANALYSIS,0.28763183125599234,"visualized the output of each branch in Fig. 6b. It is observed that the motion-reference branch
279"
ABLATION STUDY AND ANALYSIS,0.28859060402684567,"(4th row) generates video with the target motion, and effectively transfers it to the main path (3rd
280"
ABLATION STUDY AND ANALYSIS,0.28954937679769893,"row); meanwhile, the main path inherits the content from the reconstruction branch (2nd row), thus
281"
ABLATION STUDY AND ANALYSIS,0.29050814956855225,"enhancing the consistency of unedited parts.
282
Table 2: Impact of various components."
ABLATION STUDY AND ANALYSIS,0.2914669223394056,"Content
Preservation
Motion
Injection
Structure
Control
Frame
Similarity
Textual
Alignment
Frame
Consistency"
ABLATION STUDY AND ANALYSIS,0.29242569511025884,"90.54
28.76
96.99
!
97.28
29.95
98.12
!
!
91.30
31.48
98.08
!
!
96.11
31.37
98.12
!
!
!
96.29
31.43
98.09"
ABLATION STUDY AND ANALYSIS,0.29338446788111217,"The Effectiveness of Each Component.
To
283"
ABLATION STUDY AND ANALYSIS,0.2943432406519655,"demonstrate that all the designed feature injection
284"
ABLATION STUDY AND ANALYSIS,0.2953020134228188,"techniques in Sec. 4.1 & 4.2 contribute to the final
285"
ABLATION STUDY AND ANALYSIS,0.2962607861936721,"results, we make a quantitative evaluation on 15
286"
ABLATION STUDY AND ANALYSIS,0.2972195589645254,"motion editing cases, as we utilize all three com-
287"
ABLATION STUDY AND ANALYSIS,0.2981783317353787,"ponents in motion editing. To assess the similarity
288"
ABLATION STUDY AND ANALYSIS,0.29913710450623204,"between the edited video and the source video (e.g.,
289"
ABLATION STUDY AND ANALYSIS,0.3000958772770853,"background and identity), we introduce the ‘Frame
290"
ABLATION STUDY AND ANALYSIS,0.30105465004793863,"Similarity’, which is the average frame cosine similarity between the source frame embedding and
291"
ABLATION STUDY AND ANALYSIS,0.30201342281879195,"the edited frame embedding. As shown in Tab. 2, editing with content preservation results in high
292"
ABLATION STUDY AND ANALYSIS,0.3029721955896453,"frame similarity, suggesting that replacing value features in SA-S modules can effectively retain the
293"
ABLATION STUDY AND ANALYSIS,0.30393096836049854,"content of the source video. The use of motion injection and structure control significantly enhances
294"
ABLATION STUDY AND ANALYSIS,0.30488974113135187,"‘Textual Alignment’, indicating successful transfer of the targeted motion to the main editing path.
295"
ABLATION STUDY AND ANALYSIS,0.3058485139022052,"Ultimately, the best results are achieved through the combined use of all components.
296"
ABLATION STUDY AND ANALYSIS,0.3068072866730585,"(a) Visualization of attention features.
(b) Visualization of each branch’s output.
Figure 6: (6a): Visualization of spatial query in SA-S (second row), cross-frame temporal attention
maps in SA-T (third row), and the magnitude of optical flow (fourth row). (6b): Visualization of the
video output of the main editing path, the reconstruction branch and the motion-reference branch."
ABLATION STUDY AND ANALYSIS,0.3077660594439118,"(a) Ablation study on t0 in Eq. 2.
(b) Ablation study on t1 in Eq. 4.
Figure 7: Ablation study on hyper-parameters."
ABLATION STUDY AND ANALYSIS,0.3087248322147651,"Ablation on Hyper-parameters. We utilize content preservation in Eq. 2 to maintain the original
297"
ABLATION STUDY AND ANALYSIS,0.3096836049856184,"content from the source video. By varying the feature injection steps in Fig. 7a, we observe that
298"
ABLATION STUDY AND ANALYSIS,0.31064237775647174,"replacing the value features at a few steps introduces inconsistencies in the background (footprints
299"
ABLATION STUDY AND ANALYSIS,0.311601150527325,"on the beach). In practice, we adhere to the hyper-parameter selection outlined in [5] (last row).
300"
ABLATION STUDY AND ANALYSIS,0.31255992329817833,"Simultaneously, we note that adjusting the blend layers and steps in Eq. 4 can effectively regulate
301"
ABLATION STUDY AND ANALYSIS,0.31351869606903165,"the extent to which the edited image adheres to the original image. For instance, in the stylization
302"
ABLATION STUDY AND ANALYSIS,0.3144774688398849,"demonstrated in Fig. 7b, injecting the attention map into fewer (15) steps yields a stylized output that
303"
ABLATION STUDY AND ANALYSIS,0.31543624161073824,"may not retain the same structure as the input, while injecting into all 50 steps results in videos with
304"
ABLATION STUDY AND ANALYSIS,0.31639501438159157,"nearly identical textures but less stylization. Users have the flexibility to adjust the blended steps to
305"
ABLATION STUDY AND ANALYSIS,0.3173537871524449,"achieve their preferred balance between stylization and fidelity.
306"
CONCLUSION AND LIMITATIONS,0.31831255992329816,"6
Conclusion and Limitations
307"
CONCLUSION AND LIMITATIONS,0.3192713326941515,"In this paper, we design a novel tuning-free framework UniEdit for both video motion and appearance
308"
CONCLUSION AND LIMITATIONS,0.3202301054650048,"editing. By leveraging a motion-reference branch and a reconstruction branch and injecting features
309"
CONCLUSION AND LIMITATIONS,0.3211888782358581,"into the main editing path, it is capable of performing motion editing and various appearance
310"
CONCLUSION AND LIMITATIONS,0.3221476510067114,"editing. There are nevertheless some limitations. Firstly, we observe performance degradation when
311"
CONCLUSION AND LIMITATIONS,0.3231064237775647,"performing both types of editing simultaneously. Secondly, since our work is based on T2V models,
312"
CONCLUSION AND LIMITATIONS,0.32406519654841803,"the proposed method also inherits some of the shortcomings of the existing models, such as inferior
313"
CONCLUSION AND LIMITATIONS,0.32502396931927136,"performance in understanding complex prompts. We exhibit the failure cases in Appendix B.6.
314"
REFERENCES,0.3259827420901246,"References
315"
REFERENCES,0.32694151486097794,"[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat,
316"
REFERENCES,0.32790028763183127,"Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for
317"
REFERENCES,0.3288590604026846,"video generation. arXiv preprint arXiv:2401.12945, 2024.
318"
REFERENCES,0.32981783317353786,"[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-
319"
REFERENCES,0.3307766059443912,"minik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion:
320"
REFERENCES,0.3317353787152445,"Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.
321"
REFERENCES,0.3326941514860978,"[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja
322"
REFERENCES,0.3336529242569511,"Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
323"
REFERENCES,0.3346116970278044,"diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
324"
REFERENCES,0.33557046979865773,"Recognition, pages 22563–22575, 2023.
325"
REFERENCES,0.336529242569511,"[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow
326"
REFERENCES,0.3374880153403643,"image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision
327"
REFERENCES,0.33844678811121764,"and Pattern Recognition, pages 18392–18402, 2023.
328"
REFERENCES,0.33940556088207097,"[5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng.
329"
REFERENCES,0.34036433365292423,"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing.
330"
REFERENCES,0.34132310642377756,"In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.
331"
REFERENCES,0.3422818791946309,"[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image
332"
REFERENCES,0.3432406519654842,"diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
333"
REFERENCES,0.34419942473633747,"pages 23206–23217, 2023.
334"
REFERENCES,0.3451581975071908,"[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-
335"
REFERENCES,0.3461169702780441,"aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on
336"
REFERENCES,0.34707574304889743,"Computer Vision, pages 23040–23050, 2023.
337"
REFERENCES,0.3480345158197507,"[8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
338"
REFERENCES,0.348993288590604,"Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for
339"
REFERENCES,0.34995206136145734,"high-quality video generation. arXiv preprint arXiv:2310.19512, 2023.
340"
REFERENCES,0.35091083413231067,"[9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying
341"
REFERENCES,0.35186960690316393,"Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models.
342"
REFERENCES,0.35282837967401726,"arXiv preprint arXiv:2401.09047, 2024.
343"
REFERENCES,0.3537871524448706,"[10] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan
344"
REFERENCES,0.3547459252157239,"Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint
345"
REFERENCES,0.35570469798657717,"arXiv:2304.14404, 2023.
346"
REFERENCES,0.3566634707574305,"[11] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.
347"
REFERENCES,0.3576222435282838,"Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint
348"
REFERENCES,0.3585810162991371,"arXiv:2305.13840, 2023.
349"
REFERENCES,0.3595397890699904,"[12] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-
350"
REFERENCES,0.3604985618408437,"Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided
351"
REFERENCES,0.36145733461169705,"attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023.
352"
REFERENCES,0.3624161073825503,"[13] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-
353"
REFERENCES,0.36337488015340363,"based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.
354"
REFERENCES,0.36433365292425696,"[14] Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, and Nicolas Thome. Videdit:
355"
REFERENCES,0.3652924256951103,"Zero-shot and spatially aware text-driven video editing. arXiv preprint arXiv:2306.08707, 2023.
356"
REFERENCES,0.36625119846596355,"[15] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo:
357"
REFERENCES,0.36720997123681687,"Interactive drag-style video editing. arXiv preprint arXiv:2312.02216, 2023.
358"
REFERENCES,0.3681687440076702,"[16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis
359"
REFERENCES,0.3691275167785235,"Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings
360"
REFERENCES,0.3700862895493768,"of the IEEE/CVF International Conference on Computer Vision, pages 7346–7356, 2023.
361"
REFERENCES,0.3710450623202301,"[17] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo
362"
REFERENCES,0.3720038350910834,"Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models.
363"
REFERENCES,0.37296260786193675,"arXiv preprint arXiv:2309.16496, 2023.
364"
REFERENCES,0.37392138063279,"[18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion
365"
REFERENCES,0.37488015340364333,"features for consistent video editing. In International Conference on Learning Representations
366"
REFERENCES,0.37583892617449666,"(ICLR), 2024.
367"
REFERENCES,0.37679769894535,"[19] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh
368"
REFERENCES,0.37775647171620325,"Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing
369"
REFERENCES,0.37871524448705657,"text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709,
370"
REFERENCES,0.3796740172579099,"2023.
371"
REFERENCES,0.38063279002876316,"[20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl:
372"
REFERENCES,0.3815915627996165,"Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933,
373"
REFERENCES,0.3825503355704698,"2023.
374"
REFERENCES,0.3835091083413231,"[21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Ani-
375"
REFERENCES,0.3844678811121764,"matediff: Animate your personalized text-to-image diffusion models without specific tuning.
376"
REFERENCES,0.3854266538830297,"arXiv preprint arXiv:2307.04725, 2023.
377"
REFERENCES,0.38638542665388304,"[22] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan,
378"
REFERENCES,0.38734419942473636,"Chunyu Wang, Han Hu, et al. Gaia: Zero-shot talking avatar generation. In International
379"
REFERENCES,0.3883029721955896,"Conference on Learning Representations (ICLR), 2024.
380"
REFERENCES,0.38926174496644295,"[23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen.
Latent video
381"
REFERENCES,0.39022051773729627,"diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint
382"
REFERENCES,0.3911792905081496,"arXiv:2211.13221, 2022.
383"
REFERENCES,0.39213806327900286,"[24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
384"
REFERENCES,0.3930968360498562,"Prompt-to-prompt image editing with cross attention control. In International Conference on
385"
REFERENCES,0.3940556088207095,"Learning Representations (ICLR), 2023.
386"
REFERENCES,0.3950143815915628,"[25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
387"
REFERENCES,0.3959731543624161,"Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High
388"
REFERENCES,0.3969319271332694,"definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.
389"
REFERENCES,0.39789069990412274,"[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
390"
REFERENCES,0.39884947267497606,"in neural information processing systems, 33:6840–6851, 2020.
391"
REFERENCES,0.3998082454458293,"[27] Jonathan Ho and Tim Salimans.
Classifier-free diffusion guidance.
arXiv preprint
392"
REFERENCES,0.40076701821668265,"arXiv:2207.12598, 2022.
393"
REFERENCES,0.40172579098753597,"[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J
394"
REFERENCES,0.40268456375838924,"Fleet. Video diffusion models. arXiv:2204.03458, 2022.
395"
REFERENCES,0.40364333652924256,"[29] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang.
Free-
396"
REFERENCES,0.4046021093000959,"bloom: Zero-shot text-to-video generator with llm director and ldm animator. arXiv preprint
397"
REFERENCES,0.4055608820709492,"arXiv:2309.14494, 2023.
398"
REFERENCES,0.40651965484180247,"[30] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao
399"
REFERENCES,0.4074784276126558,"Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-
400"
REFERENCES,0.4084372003835091,"based image editing with multimodal large language models. arXiv preprint arXiv:2312.06739,
401"
REFERENCES,0.40939597315436244,"2023.
402"
REFERENCES,0.4103547459252157,"[31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang,
403"
REFERENCES,0.411313518696069,"Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang,
404"
REFERENCES,0.41227229146692235,"Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video
405"
REFERENCES,0.41323106423777567,"generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
406"
REFERENCES,0.41418983700862894,"Pattern Recognition, 2024.
407"
REFERENCES,0.41514860977948226,"[32] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using
408"
REFERENCES,0.4161073825503356,"temporal attention adaption for text-to-video diffusion models. arXiv preprint arXiv:2312.00845,
409"
REFERENCES,0.4170661553211889,"2023.
410"
REFERENCES,0.41802492809204217,"[33] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using
411"
REFERENCES,0.4189837008628955,"text-to-image diffusion models. arXiv preprint arXiv:2310.01107, 2023.
412"
REFERENCES,0.4199424736337488,"[34] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change
413"
REFERENCES,0.42090124640460214,"Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. arXiv
414"
REFERENCES,0.4218600191754554,"preprint arXiv:2312.00777, 2023.
415"
REFERENCES,0.4228187919463087,"[35] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M Rehg, and Pinar Yanardag. Rave:
416"
REFERENCES,0.42377756471716205,"Randomized noise shuffling for fast and consistent video editing with diffusion models. arXiv
417"
REFERENCES,0.4247363374880153,"preprint arXiv:2312.04524, 2023.
418"
REFERENCES,0.42569511025886864,"[36] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman.
419"
REFERENCES,0.42665388302972196,"Dreampose:
Fashion image-to-video synthesis via stable diffusion.
arXiv preprint
420"
REFERENCES,0.4276126558005753,"arXiv:2304.06025, 2023.
421"
REFERENCES,0.42857142857142855,"[37] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang
422"
REFERENCES,0.42953020134228187,"Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion
423"
REFERENCES,0.4304889741131352,"models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.
424"
REFERENCES,0.4314477468839885,"[38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
425"
REFERENCES,0.4324065196548418,"Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
426"
REFERENCES,0.4333652924256951,"preprint arXiv:2304.02643, 2023.
427"
REFERENCES,0.4343240651965484,"[39] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing
428"
REFERENCES,0.43528283796740175,"with cross-attention control. arXiv preprint arXiv:2303.04761, 2023.
429"
REFERENCES,0.436241610738255,"[40] Qi Mao, Lan Chen, Yuchao Gu, Zhen Fang, and Mike Zheng Shou. Mag-edit: Localized
430"
REFERENCES,0.43720038350910834,"image editing in complex scenarios via mask-based attention-adjusted guidance. arXiv preprint
431"
REFERENCES,0.43815915627996166,"arXiv:2312.11396, 2023.
432"
REFERENCES,0.439117929050815,"[41] Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and
433"
REFERENCES,0.44007670182166825,"Bryan Russell.
Customizing motion in text-to-video diffusion models.
arXiv preprint
434"
REFERENCES,0.44103547459252157,"arXiv:2312.04966, 2023.
435"
REFERENCES,0.4419942473633749,"[42] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano
436"
REFERENCES,0.4429530201342282,"Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In
437"
REFERENCES,0.4439117929050815,"International Conference on Learning Representations, 2022.
438"
REFERENCES,0.4448705656759348,"[43] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion
439"
REFERENCES,0.4458293384467881,"for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF
440"
REFERENCES,0.4467881112176414,"Conference on Computer Vision and Pattern Recognition, pages 6038–6047, 2023.
441"
REFERENCES,0.4477468839884947,"[44] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv
442"
REFERENCES,0.44870565675934804,"Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.
443"
REFERENCES,0.44966442953020136,"arXiv preprint arXiv:2302.01329, 2023.
444"
REFERENCES,0.4506232023010546,"[45] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei
445"
REFERENCES,0.45158197507190795,"Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally
446"
REFERENCES,0.45254074784276127,"consistent video processing. arXiv preprint arXiv:2308.07926, 2023.
447"
REFERENCES,0.4534995206136146,"[46] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng
448"
REFERENCES,0.45445829338446786,"Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the
449"
REFERENCES,0.4554170661553212,"IEEE/CVF International Conference on Computer Vision, 2023.
450"
REFERENCES,0.4563758389261745,"[47] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei
451"
REFERENCES,0.4573346116970278,"Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint
452"
REFERENCES,0.4582933844678811,"arXiv:2310.15169, 2023.
453"
REFERENCES,0.4592521572387344,"[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
454"
REFERENCES,0.46021093000958774,"text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,
455"
REFERENCES,0.46116970278044106,"2022.
456"
REFERENCES,0.4621284755512943,"[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
457"
REFERENCES,0.46308724832214765,"resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
458"
REFERENCES,0.46404602109300097,"conference on computer vision and pattern recognition, pages 10684–10695, 2022.
459"
REFERENCES,0.4650047938638543,"[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
460"
REFERENCES,0.46596356663470756,"Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
461"
REFERENCES,0.4669223394055609,"Photorealistic text-to-image diffusion models with deep language understanding. Advances in
462"
REFERENCES,0.4678811121764142,"Neural Information Processing Systems, 35:36479–36494, 2022.
463"
REFERENCES,0.46883988494726747,"[51] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with
464"
REFERENCES,0.4697986577181208,"singular value clipping. In Proceedings of the IEEE international conference on computer
465"
REFERENCES,0.4707574304889741,"vision, pages 2830–2839, 2017.
466"
REFERENCES,0.47171620325982744,"[52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,
467"
REFERENCES,0.4726749760306807,"Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without
468"
REFERENCES,0.473633748801534,"text-video data. arXiv preprint arXiv:2209.14792, 2022.
469"
REFERENCES,0.47459252157238735,"[53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In
470"
REFERENCES,0.47555129434324067,"International Conference on Learning Representations (ICLR), 2021.
471"
REFERENCES,0.47651006711409394,"[54] Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid
472"
REFERENCES,0.47746883988494726,"video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023.
473"
REFERENCES,0.4784276126558006,"[55] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Mo-
474"
REFERENCES,0.4793863854266539,"tioneditor: Editing video motion via content-aware diffusion. arXiv preprint arXiv:2311.18830,
475"
REFERENCES,0.48034515819750717,"2023.
476"
REFERENCES,0.4813039309683605,"[56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features
477"
REFERENCES,0.4822627037392138,"for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on
478"
REFERENCES,0.48322147651006714,"Computer Vision and Pattern Recognition, pages 1921–1930, 2023.
479"
REFERENCES,0.4841802492809204,"[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
480"
REFERENCES,0.4851390220517737,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
481"
REFERENCES,0.48609779482262705,"processing systems, 30, 2017.
482"
REFERENCES,0.48705656759348037,"[58] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
483"
REFERENCES,0.48801534036433364,"Advances in neural information processing systems, 29, 2016.
484"
REFERENCES,0.48897411313518696,"[59] Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, and Xiaodan Liang. Dreamvideo:
485"
REFERENCES,0.4899328859060403,"High-fidelity image-to-video generation with image retention and text guidance. arXiv preprint
486"
REFERENCES,0.49089165867689355,"arXiv:2312.03018, 2023.
487"
REFERENCES,0.49185043144774687,"[60] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
488"
REFERENCES,0.4928092042186002,"Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.
489"
REFERENCES,0.4937679769894535,"[61] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Bryan Catanzaro, and Jan Kautz.
490"
REFERENCES,0.4947267497603068,"Few-shot video-to-video synthesis. Advances in Neural Information Processing Systems, 32,
491"
REFERENCES,0.4956855225311601,"2019.
492"
REFERENCES,0.4966442953020134,"[62] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and
493"
REFERENCES,0.49760306807286675,"Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv
494"
REFERENCES,0.49856184084372,"preprint arXiv:2303.17599, 2023.
495"
REFERENCES,0.49952061361457334,"[63] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang,
496"
REFERENCES,0.5004793863854267,"Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded
497"
REFERENCES,0.50143815915628,"latent diffusion models. arXiv preprint arXiv:2309.15103, 2023.
498"
REFERENCES,0.5023969319271333,"[64] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
499"
REFERENCES,0.5033557046979866,"Shan. Motionctrl: A unified and flexible motion controller for video generation. arXiv preprint
500"
REFERENCES,0.5043144774688398,"arXiv:2312.03641, 2023.
501"
REFERENCES,0.5052732502396932,"[65] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne
502"
REFERENCES,0.5062320230105465,"Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image
503"
REFERENCES,0.5071907957813998,"diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International
504"
REFERENCES,0.5081495685522531,"Conference on Computer Vision, pages 7623–7633, 2023.
505"
REFERENCES,0.5091083413231065,"[66] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang,
506"
REFERENCES,0.5100671140939598,"Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu,
507"
REFERENCES,0.5110258868648131,"Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, and Forrest Iandola. Cvpr 2023 text
508"
REFERENCES,0.5119846596356663,"guided video editing competition, 2023.
509"
REFERENCES,0.5129434324065196,"[67] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan
510"
REFERENCES,0.513902205177373,"Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video
511"
REFERENCES,0.5148609779482263,"generation using textual and structural guidance. arXiv preprint arXiv:2306.00943, 2023.
512"
REFERENCES,0.5158197507190796,"[68] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-
513"
REFERENCES,0.5167785234899329,"conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023.
514"
REFERENCES,0.5177372962607862,"[69] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation
515"
REFERENCES,0.5186960690316395,"using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.
516"
REFERENCES,0.5196548418024928,"[70] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Rerender a video: Zero-shot
517"
REFERENCES,0.5206136145733461,"text-guided video-to-video translation. In ACM SIGGRAPH Asia 2023 Conference Proceedings,
518"
REFERENCES,0.5215723873441994,"2023.
519"
REFERENCES,0.5225311601150527,"[71] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G
520"
REFERENCES,0.5234899328859061,"Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video
521"
REFERENCES,0.5244487056567594,"transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
522"
REFERENCES,0.5254074784276127,"Recognition, pages 10459–10469, 2023.
523"
REFERENCES,0.5263662511984659,"[72] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu,
524"
REFERENCES,0.5273250239693192,"Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for
525"
REFERENCES,0.5282837967401726,"text-to-video generation. arXiv preprint arXiv:2309.15818, 2023.
526"
REFERENCES,0.5292425695110259,"[73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
527"
REFERENCES,0.5302013422818792,"diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
528"
REFERENCES,0.5311601150527325,"Vision, pages 3836–3847, 2023.
529"
REFERENCES,0.5321188878235859,"[74] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and
530"
REFERENCES,0.5330776605944392,"Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint
531"
REFERENCES,0.5340364333652924,"arXiv:2305.13077, 2023.
532"
REFERENCES,0.5349952061361457,"[75] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and
533"
REFERENCES,0.535953978906999,"Changsheng Xu. Motioncrafter: One-shot motion customization of diffusion models. arXiv
534"
REFERENCES,0.5369127516778524,"preprint arXiv:2312.05288, 2023.
535"
REFERENCES,0.5378715244487057,Supplementary Materials
REFERENCES,0.538830297219559,"We organize the Appendix as follows:
536"
REFERENCES,0.5397890699904123,"• Appendix A: detailed descriptions of experimental settings.
537"
REFERENCES,0.5407478427612655,"• Appendix B: more experimental results, including:
538"
REFERENCES,0.5417066155321189,"• Editing results on different T2V model (Appendix B.1).
539"
REFERENCES,0.5426653883029722,"• Quantitative ablation on hyper-parameter selection (Appendix B.2).
540"
REFERENCES,0.5436241610738255,"• Ablation study on mask-guided coordination (Appendix B.3).
541"
REFERENCES,0.5445829338446788,"• Observation and analysis on the proposed components (Appendix B.4).
542"
REFERENCES,0.5455417066155321,"• Analysis and comparison on inference time (Appendix B.5).
543"
REFERENCES,0.5465004793863855,"• Failure cases visualization (Appendix B.6).
544"
REFERENCES,0.5474592521572388,"• More Comparisons with baseline methods (Appendix B.7).
545"
REFERENCES,0.548418024928092,"• More Editing results of UniEdit (Appendix B.8).
546"
REFERENCES,0.5493767976989453,"• Appendix C: Broader Impacts.
547"
REFERENCES,0.5503355704697986,"We encourage the readers to watch the videos on our project page.
548"
REFERENCES,0.551294343240652,"A
Detailed Experimental Settings
549"
REFERENCES,0.5522531160115053,"Base T2V Model.
We instantiate the proposed method on LaVie [63], which is a pre-trained
550"
REFERENCES,0.5532118887823586,"text-to-video generation model that produces consistent and high-quality videos. To achieve a fair
551"
REFERENCES,0.5541706615532119,"comparison, we only leverage the base T2V model in LaVie and load the open-source pre-trained
552"
REFERENCES,0.5551294343240653,"weights for video editing tasks in the experiments. Note that the edited video clip could further be
553"
REFERENCES,0.5560882070949185,"seamlessly fed into the temporal interpolation model and the video super-resolution model to obtain
554"
REFERENCES,0.5570469798657718,"video with a longer duration and higher resolution.
555"
REFERENCES,0.5580057526366251,"Video Preprocessing.
For each input video, we resize it to the resolution of 320 × 512, followed by
556"
REFERENCES,0.5589645254074784,"normalization, which is consistent with the training configuration of LaVie. Then, the pre-processed
557"
REFERENCES,0.5599232981783318,"video is fed into the base model of Lavie to perform video editing. To maximize the generation power
558"
REFERENCES,0.5608820709491851,"of LaVie, we set all input videos to 16 frames. For a source video, it takes 1-2 minutes to edit on an
559"
REFERENCES,0.5618408437200384,"NVIDIA A100 GPU.
560"
REFERENCES,0.5627996164908916,"Configurations.
For real source videos, we inverse them with 50 DDIM inversion steps and perform
561"
REFERENCES,0.5637583892617449,"DDIM deterministic sampling with 50 steps for generation. For the generated videos, we use the
562"
REFERENCES,0.5647171620325983,"same start latent of synthesizing the source video as the initial noise zT for the main editing path and
563"
REFERENCES,0.5656759348034516,"two auxiliary branches. We use the commonly used classifier-free guidance technique [27] with a
564"
REFERENCES,0.5666347075743049,"scale of 7.5.
565"
REFERENCES,0.5675934803451582,"Details of User Study.
As a text-guided editing task, in addition to CLIP scores, it is crucial to
566"
REFERENCES,0.5685522531160115,"evaluate results through human subjective assessment. To achieve this, we utilized MOS (Mean
567"
REFERENCES,0.5695110258868649,"Opinion Score) as our metric and collected feedback from 10 experienced volunteers. We randomly
568"
REFERENCES,0.5704697986577181,"selected 20 editing samples and permuted results from different models. Volunteers were then tasked
569"
REFERENCES,0.5714285714285714,"to evaluate the results based on two perspectives: frame consistency and textual alignment. They
570"
REFERENCES,0.5723873441994247,"provided ratings for these aspects on a scale of 1-5. Specifically, frame consistency measures the
571"
REFERENCES,0.573346116970278,"smoothness of the video, aiming to avoid dramatic jittering and ensure coherence between the content
572"
REFERENCES,0.5743048897411314,"of each frame. Textual alignment assesses whether the editing results adhere to the text guidance and
573"
REFERENCES,0.5752636625119847,"maintain the content of the source video. In the end, we computed the average user ratings for each
574"
REFERENCES,0.576222435282838,"method as our final results.
575"
REFERENCES,0.5771812080536913,"As illustrated in Tab. 1, UniEdit shows the best performance on frame consistency. Regarding textual
576"
REFERENCES,0.5781399808245445,"alignment, UniEdit significantly outperforms all other baselines, demonstrating its capacity to support
577"
REFERENCES,0.5790987535953979,"diverse editing scenarios.
578"
REFERENCES,0.5800575263662512,"Baselines.
We implement all baseline methods with their official repositories. For MasaCtrl [5],
579"
REFERENCES,0.5810162991371045,"we adapt it to video editing by first setting the base model to a T2V model [63], then performing
580"
REFERENCES,0.5819750719079578,"MasaCtrl on all frames of the source video. Moreover, since most baselines use StableDiffusion (SD)
581"
REFERENCES,0.5829338446788112,"as the base model, we resize the source video to 512 × 512 to align with the default configuration of
582"
REFERENCES,0.5838926174496645,"SD, then feed it into the denoising model, which can maximize the power of SD.
583"
REFERENCES,0.5848513902205177,"B
Additional Experimental Results and Analysis
584"
REFERENCES,0.585810162991371,"B.1
Results on Different T2V Model
585"
REFERENCES,0.5867689357622243,"We additionally implement our method on VideoCrafter2 [9], a concurrent work on T2V generation
586"
REFERENCES,0.5877277085330777,"to demonstrate the flexibility of UniEdit. The results are shown in Fig. 8.
587"
REFERENCES,0.588686481303931,Figure 8: Editing results with UniEdit on VideoCrafter2 [9].
REFERENCES,0.5896452540747843,"B.2
Quantitative Ablation on Hyper-parameter Selection
588"
REFERENCES,0.5906040268456376,"In practice, we empirically found set these values to fixed values, i.e., t0 = 50, L = 10 (same as
589"
REFERENCES,0.5915627996164909,"MasaCtrl [5]) and t1 = 25 can achieve satisfying results on most cases, and we further perform a
590"
REFERENCES,0.5925215723873442,quantitative study when applying different hyper-parameters in Tab. 3&4.
REFERENCES,0.5934803451581975,"Table 3: Quantitative comparison on hyper-parameter selection.
Metric
Frame Similarity
Textual Alignment
Frame Consistency"
REFERENCES,0.5944391179290508,"t0 = 20, L = 10
94.33
31.57
98.09
t0 = 50, L = 10
96.29
31.84
98.12
t0 = 50, L = 8
96.76
31.25
98.11
591"
REFERENCES,0.5953978906999041,"Table 4: Quantitative comparison on hyper-parameter selection.
Metric
Frame Similarity
Textual Alignment
Frame Consistency"
REFERENCES,0.5963566634707574,"t1 = 20
96.21
30.92
98.06
t1 = 25
96.29
31.43
98.09
t1 = 30
96.50
31.04
98.08"
REFERENCES,0.5973154362416108,"B.3
Ablation Study on the Impact of Mask-Guided Coordination
592"
REFERENCES,0.5982742090124641,"To investigate the impact of mask-guided coordination, we begin by visualizing masks obtained
593"
REFERENCES,0.5992329817833174,"from 1) the attention map in CA-S modules; 2) the off-the-shelf segmentation model SAM [38],
594"
REFERENCES,0.6001917545541706,"followed by presenting both qualitative and quantitative results of implementing UniEdit with or
595"
REFERENCES,0.6011505273250239,"without mask-guided coordination.
596"
REFERENCES,0.6021093000958773,"As verified by previous work [24], the attention maps in CA-S modules contain correspondence
597"
REFERENCES,0.6030680728667306,"information between text and visual features. The underlying intuition is that the attention maps
598"
REFERENCES,0.6040268456375839,"between each word and the spatial features at point (i, j) indicate ‘how similar this token is to
599"
REFERENCES,0.6049856184084372,"the spatial feature at this location’. We visualize the text-image cross attention map alongside the
600"
REFERENCES,0.6059443911792906,"synthesized frame in Fig. 9. We observe spatial correspondences that align with the video output from
601"
REFERENCES,0.6069031639501438,"the attention map. For instance, areas with higher values of the token ‘man’ and ‘NYC’ correspond
602"
REFERENCES,0.6078619367209971,"to the foreground and background, respectively. We further employ a fixed threshold (0.4 in practice)
603"
REFERENCES,0.6088207094918504,"to derive binary segmentation maps from the attention maps. For comparison, we also display the
604"
REFERENCES,0.6097794822627037,"segmentation mask obtained by point prompt on SAM. It’s observed that the cross-attention mask is
605"
REFERENCES,0.610738255033557,"generally accurate and could serve as a reliable proxy in practice when an external segmentor is not
606"
REFERENCES,0.6116970278044104,"available.
607"
REFERENCES,0.6126558005752637,"We examine the impact of mask-guided coordination through both qualitative and quantitative results
608"
REFERENCES,0.613614573346117,"across 4 settings: {w/o UniEdit, UniEdit w/o mask, UniEdit with mask from CA-S, UniEdit with
609"
REFERENCES,0.6145733461169702,"mask from SAM}. Qualitatively, shown in Fig. 10, the implementation of UniEdit significantly
610"
REFERENCES,0.6155321188878236,"enhances the consistency between the edited videos and the original video. The application of the
611"
REFERENCES,0.6164908916586769,"mask-guided coordination technique further improves the consistency of unedited areas (e.g., color
612"
REFERENCES,0.6174496644295302,"and texture). The quantitative results in Tab. 5 align coherently with this analysis.
613"
REFERENCES,0.6184084372003835,"Table 5: Ablation on the proposed mask-guided coordination.
Metric
Textual Alignment
Frame Consistency"
REFERENCES,0.6193672099712368,"TAV
27.89
95.39
MasaCtrl∗
25.58
97.61
FateZero
27.30
96.72
Rerender
27.94
97.18
TokenFlow
28.58
97.02"
REFERENCES,0.6203259827420902,"UniEdit (w/o mask)
31.43
98.35
UniEdit (w CA-S mask)
31.49
98.33
UniEdit (w SAM mask)
31.50
98.36"
REFERENCES,0.6212847555129435,"Figure 9: Visualization of attention maps and masks in mask-guided coordination (Sec. 4.3). The top
row are attention maps corresponding to different tokens in CA-S modules, (a) is the final output
frame, (b) and (c) are the foreground/background binary mask obtained by employing a threshold on
the attention map of ‘Man’ token and point prompt segmentation with SAM, respectively."
REFERENCES,0.6222435282837967,"Figure 10: Qualitative editing results across 4 settings: w/o UniEdit (2nd row), UniEdit w/o mask
(3rd row), UniEdit with mask from CA-S (4th row), UniEdit with mask from SAM (5th row)."
REFERENCES,0.62320230105465,"B.4
More Observation and Analysis on the Proposed Components
614"
REFERENCES,0.6241610738255033,"Difference Between QK and V Features in SA-S Modules To comprehend why we can have
615"
REFERENCES,0.6251198465963567,"inhomogeneous QK and V and their differences, we visualized the results of swapping different
616"
REFERENCES,0.62607861936721,"features (QK or V) in SA-S modules during style transfer tasks on the source video in Fig. 11a. As
617"
REFERENCES,0.6270373921380633,"can be seen, compared to editing with no feature replacement (2nd row), replacing QK in the 3rd row
618"
REFERENCES,0.6279961649089166,"results in the edited video adopting the same spatial structure as the source video. Simultaneously,
619"
REFERENCES,0.6289549376797698,"replacing V eradicates the style information in the 4th row, meaning the texture details from the
620"
REFERENCES,0.6299137104506232,"source video are utilized to replace the style depicted by the target prompt. To summarize, the query
621"
REFERENCES,0.6308724832214765,"and key features (in SA-S modules) dictate the spatial structure of the generated video, while the
622"
REFERENCES,0.6318312559923298,"value features tend to influence the texture, including details such as color tones.
623"
REFERENCES,0.6327900287631831,"Influence of Spatial Structure Control in Motion Editing We explored the role of spatial control
624"
REFERENCES,0.6337488015340365,"in motion editing. The proposed method synthesizes videos with larger modifications when removing
625"
REFERENCES,0.6347075743048898,"the spatial control mechanism on both the motion-reference branch and the main editing branch. We
626"
REFERENCES,0.6356663470757431,"visualized the results in Fig. 11b. It can be observed that although the motion-reference branch can
627"
REFERENCES,0.6366251198465963,"still generate the target motion without the control of spatial structure, the layout deviates significantly,
628"
REFERENCES,0.6375838926174496,"for example, the raccoon assumes a different pose and location. We regard this as a suboptimal
629"
REFERENCES,0.638542665388303,"solution because, compared to the results presented in the 3rd row, the results w/o spatial structure
630"
REFERENCES,0.6395014381591563,"control modifies the object position of the source video, leading to a decrease in consistency between
631"
REFERENCES,0.6404602109300096,"the edited result and the source video.
632"
REFERENCES,0.6414189837008629,"(a) Replacing different features in SA-S modules.
(b) Motion editing w/ or w/o structure control."
REFERENCES,0.6423777564717162,"Figure 11: Ablation on the proposed feature injection techniques. (11a): comparison of appearance
editing without feature replacement (2nd row), with QK replacement (3rd row), with V replacement
(4nd row); (11b): comparison of motion editing with and without the designed spatial structure
control mechanism."
REFERENCES,0.6433365292425696,"B.5
Analysis and Comparison on Inference Time
633"
REFERENCES,0.6442953020134228,"We conduct a theoretical analysis of the additional cost of UniEdit and an empirical comparison with
634"
REFERENCES,0.6452540747842761,"baseline methods in terms of inference speed.
635"
REFERENCES,0.6462128475551294,"Theoretically, our method primarily involves feature replacement operations in attention modules,
636"
REFERENCES,0.6471716203259827,"achieved through forward hook registration and introducing minimal additional computation. There-
637"
REFERENCES,0.6481303930968361,"fore, the main difference between synthesizing a video from random noise and editing a video
638"
REFERENCES,0.6490891658676894,"with UniEdit lies in the batch size of the denoising process (i.e., vanilla generation: batchsize=1,
639"
REFERENCES,0.6500479386385427,"appearance editing: batchsize=2, motion editing: batchsize=3), and this process could be further
640"
REFERENCES,0.6510067114093959,"accelerated through multi-GPU parallel processing techniques. Additionally, we utilize LaVie [63] as
641"
REFERENCES,0.6519654841802492,"the base T2V model in the paper, which takes approximately 45 seconds to synthesize a 16-frame
642"
REFERENCES,0.6529242569511026,"video. Our method can be even faster when adapted to more efficient base models.
643"
REFERENCES,0.6538830297219559,"Empirically, UniEdit demonstrates comparable speed with baseline methods. The comparison of
644"
REFERENCES,0.6548418024928092,"inference time on a single 16-frame source video clip with a resolution of 320x512 on 1 NVIDIA
645"
REFERENCES,0.6558005752636625,"A100 GPU is as follows:
646"
REFERENCES,0.6567593480345159,Table 6: Quantitative comparison on inference time of editing a single 16-frame video clip.
REFERENCES,0.6577181208053692,"Method
TAV
MasaCtrl∗FateZero Rerender TokenFlow
UniEdit
(appearance editing)
UniEdit
(motion editing)"
REFERENCES,0.6586768935762224,"Inference time ∼10min
∼90s
∼130s
∼110s
∼100s
∼95s
∼125s"
REFERENCES,0.6596356663470757,"B.6
Failure Cases Visualization
647"
REFERENCES,0.660594439117929,"We exhibit failure cases in Fig. 12. Fig. 12a showcase when editing multiple elements simultaneously,
648"
REFERENCES,0.6615532118887824,"and we observe a relatively large inconsistency with the source video. A naive solution is to perform
649"
REFERENCES,0.6625119846596357,"editing with UniEdit multiple times. Fig. 12b visualizes the results when editing video with complex
650"
REFERENCES,0.663470757430489,"scenes, and the model sometimes could not understand the semantics in the target prompt, resulting
651"
REFERENCES,0.6644295302013423,"in incorrect editing. This may be caused by the base model’s limited text understanding power,
652"
REFERENCES,0.6653883029721956,"as discussed in [30]. It could be alleviated by leveraging the reasoning power of MLLM [30], or
653"
REFERENCES,0.6663470757430489,"adapting approaches in complex scenario editing [40].
654"
REFERENCES,0.6673058485139022,"(a) Edit multiple elements simultaneously.
(b) Complex scene editing."
REFERENCES,0.6682646212847555,Figure 12: Visualization of failure cases.
REFERENCES,0.6692233940556088,"B.7
More Comparison with State-of-the-Art Methods
655"
REFERENCES,0.6701821668264621,"Please refer to Fig. 13 and Fig. 14 for more comparison with the state-of-the-art methods. For a fair
656"
REFERENCES,0.6711409395973155,"comparison, we also migrated all baselines to LaVie [63], using the same base model as our method.
657"
REFERENCES,0.6720997123681688,"The results are presented in Fig. 15, and they are found to be inferior compared to those in Fig. 5
658"
REFERENCES,0.673058485139022,"(based on Stable Diffusion).
659"
REFERENCES,0.6740172579098753,"B.8
More Results of UniEdit
660"
REFERENCES,0.6749760306807286,"More edited results of UniEdit are provided in Fig. 16-21. Examples of TI2V generation are provided
661"
REFERENCES,0.675934803451582,"in Fig. 22.
662"
REFERENCES,0.6768935762224353,Figure 13: More comparison with state-of-the-art methods.
REFERENCES,0.6778523489932886,Figure 14: More comparison with state-of-the-art methods.
REFERENCES,0.6788111217641419,"Figure 15: More comparison with state-of-the-art methods. We adapt the baseline methods to the
text-to-video model LaVie [63] and compare with our method (also based on LaVie)."
REFERENCES,0.6797698945349953,Figure 16: More appearance editing results of UniEdit.
REFERENCES,0.6807286673058485,Figure 17: More appearance editing results of UniEdit.
REFERENCES,0.6816874400767018,Figure 18: More appearance editing results of UniEdit.
REFERENCES,0.6826462128475551,Figure 19: More appearance editing results of UniEdit.
REFERENCES,0.6836049856184084,Figure 20: More motion editing results of UniEdit.
REFERENCES,0.6845637583892618,Figure 21: More motion editing results of UniEdit.
REFERENCES,0.6855225311601151,Figure 22: Results of text-image-to-video synthesis in Sec. 4.4.
REFERENCES,0.6864813039309684,"C
Broader Impacts
663"
REFERENCES,0.6874400767018217,"UniEdit is a tuning-free approach and is intended for advancing AI/ML research on video editing.
664"
REFERENCES,0.6883988494726749,"We encourage users to use the model responsibly. We discourage users from using the codes to
665"
REFERENCES,0.6893576222435283,"generate intentionally deceptive or untrue content or for inauthentic activities. It is suggested to add
666"
REFERENCES,0.6903163950143816,"watermarks to prevent misuse.
667"
REFERENCES,0.6912751677852349,"NeurIPS Paper Checklist
668"
CLAIMS,0.6922339405560882,"1. Claims
669"
CLAIMS,0.6931927133269415,"Question: Do the main claims made in the abstract and introduction accurately reflect the
670"
CLAIMS,0.6941514860977949,"paper’s contributions and scope?
671"
CLAIMS,0.6951102588686481,"Answer: [Yes]
672"
CLAIMS,0.6960690316395014,"Justification: In this work, we present UniEdit, a tuning-free framework that supports
673"
CLAIMS,0.6970278044103547,"both video motion and appearance editing by harnessing the power of a pre-trained text-
674"
CLAIMS,0.697986577181208,"to-video generator within an inversion-then-generation framework.Extensive experiments
675"
CLAIMS,0.6989453499520614,"demonstrate that UniEdit covers video motion editing and various appearance editing
676"
CLAIMS,0.6999041227229147,"scenarios, and surpasses the state-of-the-art method.
677"
CLAIMS,0.700862895493768,"Guidelines:
678"
CLAIMS,0.7018216682646213,"• The answer NA means that the abstract and introduction do not include the claims
679"
CLAIMS,0.7027804410354745,"made in the paper.
680"
CLAIMS,0.7037392138063279,"• The abstract and/or introduction should clearly state the claims made, including the
681"
CLAIMS,0.7046979865771812,"contributions made in the paper and important assumptions and limitations. A No or
682"
CLAIMS,0.7056567593480345,"NA answer to this question will not be perceived well by the reviewers.
683"
CLAIMS,0.7066155321188878,"• The claims made should match theoretical and experimental results, and reflect how
684"
CLAIMS,0.7075743048897412,"much the results can be expected to generalize to other settings.
685"
CLAIMS,0.7085330776605945,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
686"
CLAIMS,0.7094918504314478,"are not attained by the paper.
687"
LIMITATIONS,0.710450623202301,"2. Limitations
688"
LIMITATIONS,0.7114093959731543,"Question: Does the paper discuss the limitations of the work performed by the authors?
689"
LIMITATIONS,0.7123681687440077,"Answer: [Yes]
690"
LIMITATIONS,0.713326941514861,"Justification: We discussed the potential limitations of the method in Sec. 6 and presented
691"
LIMITATIONS,0.7142857142857143,"failed cases in Appendix B.6.
692"
LIMITATIONS,0.7152444870565676,"Guidelines:
693"
LIMITATIONS,0.716203259827421,"• The answer NA means that the paper has no limitation while the answer No means that
694"
LIMITATIONS,0.7171620325982742,"the paper has limitations, but those are not discussed in the paper.
695"
LIMITATIONS,0.7181208053691275,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
696"
LIMITATIONS,0.7190795781399808,"• The paper should point out any strong assumptions and how robust the results are to
697"
LIMITATIONS,0.7200383509108341,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
698"
LIMITATIONS,0.7209971236816874,"model well-specification, asymptotic approximations only holding locally). The authors
699"
LIMITATIONS,0.7219558964525408,"should reflect on how these assumptions might be violated in practice and what the
700"
LIMITATIONS,0.7229146692233941,"implications would be.
701"
LIMITATIONS,0.7238734419942474,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
702"
LIMITATIONS,0.7248322147651006,"only tested on a few datasets or with a few runs. In general, empirical results often
703"
LIMITATIONS,0.725790987535954,"depend on implicit assumptions, which should be articulated.
704"
LIMITATIONS,0.7267497603068073,"• The authors should reflect on the factors that influence the performance of the approach.
705"
LIMITATIONS,0.7277085330776606,"For example, a facial recognition algorithm may perform poorly when image resolution
706"
LIMITATIONS,0.7286673058485139,"is low or images are taken in low lighting. Or a speech-to-text system might not be
707"
LIMITATIONS,0.7296260786193672,"used reliably to provide closed captions for online lectures because it fails to handle
708"
LIMITATIONS,0.7305848513902206,"technical jargon.
709"
LIMITATIONS,0.7315436241610739,"• The authors should discuss the computational efficiency of the proposed algorithms
710"
LIMITATIONS,0.7325023969319271,"and how they scale with dataset size.
711"
LIMITATIONS,0.7334611697027804,"• If applicable, the authors should discuss possible limitations of their approach to
712"
LIMITATIONS,0.7344199424736337,"address problems of privacy and fairness.
713"
LIMITATIONS,0.7353787152444871,"• While the authors might fear that complete honesty about limitations might be used by
714"
LIMITATIONS,0.7363374880153404,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
715"
LIMITATIONS,0.7372962607861937,"limitations that aren’t acknowledged in the paper. The authors should use their best
716"
LIMITATIONS,0.738255033557047,"judgment and recognize that individual actions in favor of transparency play an impor-
717"
LIMITATIONS,0.7392138063279002,"tant role in developing norms that preserve the integrity of the community. Reviewers
718"
LIMITATIONS,0.7401725790987536,"will be specifically instructed to not penalize honesty concerning limitations.
719"
THEORY ASSUMPTIONS AND PROOFS,0.7411313518696069,"3. Theory Assumptions and Proofs
720"
THEORY ASSUMPTIONS AND PROOFS,0.7420901246404602,"Question: For each theoretical result, does the paper provide the full set of assumptions and
721"
THEORY ASSUMPTIONS AND PROOFS,0.7430488974113135,"a complete (and correct) proof?
722"
THEORY ASSUMPTIONS AND PROOFS,0.7440076701821668,"Answer: [NA]
723"
THEORY ASSUMPTIONS AND PROOFS,0.7449664429530202,"Justification: This paper aims to design a simple-and-effective video editing method named
724"
THEORY ASSUMPTIONS AND PROOFS,0.7459252157238735,"UniEdit, without focusing on theoretical results.
725"
THEORY ASSUMPTIONS AND PROOFS,0.7468839884947267,"Guidelines:
726"
THEORY ASSUMPTIONS AND PROOFS,0.74784276126558,"• The answer NA means that the paper does not include theoretical results.
727"
THEORY ASSUMPTIONS AND PROOFS,0.7488015340364333,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
728"
THEORY ASSUMPTIONS AND PROOFS,0.7497603068072867,"referenced.
729"
THEORY ASSUMPTIONS AND PROOFS,0.75071907957814,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
730"
THEORY ASSUMPTIONS AND PROOFS,0.7516778523489933,"• The proofs can either appear in the main paper or the supplemental material, but if
731"
THEORY ASSUMPTIONS AND PROOFS,0.7526366251198466,"they appear in the supplemental material, the authors are encouraged to provide a short
732"
THEORY ASSUMPTIONS AND PROOFS,0.7535953978907,"proof sketch to provide intuition.
733"
THEORY ASSUMPTIONS AND PROOFS,0.7545541706615532,"• Inversely, any informal proof provided in the core of the paper should be complemented
734"
THEORY ASSUMPTIONS AND PROOFS,0.7555129434324065,"by formal proofs provided in appendix or supplemental material.
735"
THEORY ASSUMPTIONS AND PROOFS,0.7564717162032598,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7574304889741131,"4. Experimental Result Reproducibility
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583892617449665,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7593480345158198,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7603068072866731,"of the paper (regardless of whether the code and data are provided or not)?
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612655800575263,"Answer: [Yes]
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7622243528283796,"Justification: This paper provides detailed information on the models, parameters, hyper-
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.763183125599233,"parameter selection, computational resources in Sec. 5 and Appendix A to ensure repro-
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7641418983700863,"ducibility.
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651006711409396,"Guidelines:
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7660594439117929,"• The answer NA means that the paper does not include experiments.
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7670182166826462,"• If the paper includes experiments, a No answer to this question will not be perceived
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7679769894534996,"well by the reviewers: Making the paper reproducible is important, regardless of
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7689357622243528,"whether the code and data are provided or not.
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698945349952061,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708533077660594,"to make their results reproducible or verifiable.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718120805369127,"• Depending on the contribution, reproducibility can be accomplished in various ways.
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727708533077661,"For example, if the contribution is a novel architecture, describing the architecture fully
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7737296260786194,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746883988494727,"be necessary to either make it possible for others to replicate the model with the same
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775647171620326,"dataset, or provide access to the model. In general. releasing code and data is often
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766059443911792,"one good way to accomplish this, but reproducibility can also be provided via detailed
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775647171620326,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785234899328859,"of a large language model), releasing of a model checkpoint, or other means that are
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7794822627037392,"appropriate to the research performed.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7804410354745925,"• While NeurIPS does not require releasing code, the conference does require all submis-
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7813998082454459,"sions to provide some reasonable avenue for reproducibility, which may depend on the
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7823585810162992,"nature of the contribution. For example
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7833173537871524,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842761265580057,"to reproduce that algorithm.
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.785234899328859,"(b) If the contribution is primarily a new model architecture, the paper should describe
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861936720997124,"the architecture clearly and fully.
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871524448705657,"(c) If the contribution is a new model (e.g., a large language model), then there should
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.788111217641419,"either be a way to access this model for reproducing the results or a way to reproduce
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890699904122723,"the model (e.g., with an open-source dataset or instructions for how to construct
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900287631831256,"the dataset).
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7909875359539789,"(d) We recognize that reproducibility may be tricky in some cases, in which case
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919463087248322,"authors are welcome to describe the particular way they provide for reproducibility.
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7929050814956855,"In the case of closed-source models, it may be that access to the model is limited in
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7938638542665388,"some way (e.g., to registered users), but it should be possible for other researchers
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948226270373921,"to have some path to reproducing or verifying the results.
776"
OPEN ACCESS TO DATA AND CODE,0.7957813998082455,"5. Open access to data and code
777"
OPEN ACCESS TO DATA AND CODE,0.7967401725790988,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
778"
OPEN ACCESS TO DATA AND CODE,0.7976989453499521,"tions to faithfully reproduce the main experimental results, as described in supplemental
779"
OPEN ACCESS TO DATA AND CODE,0.7986577181208053,"material?
780"
OPEN ACCESS TO DATA AND CODE,0.7996164908916586,"Answer: [No]
781"
OPEN ACCESS TO DATA AND CODE,0.800575263662512,"Justification: Due to company policy reasons, we are currently unable to upload the code.
782"
OPEN ACCESS TO DATA AND CODE,0.8015340364333653,"The code will be publicly available after the paper is published.
783"
OPEN ACCESS TO DATA AND CODE,0.8024928092042186,"Guidelines:
784"
OPEN ACCESS TO DATA AND CODE,0.8034515819750719,"• The answer NA means that paper does not include experiments requiring code.
785"
OPEN ACCESS TO DATA AND CODE,0.8044103547459253,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
786"
OPEN ACCESS TO DATA AND CODE,0.8053691275167785,"public/guides/CodeSubmissionPolicy) for more details.
787"
OPEN ACCESS TO DATA AND CODE,0.8063279002876318,"• While we encourage the release of code and data, we understand that this might not be
788"
OPEN ACCESS TO DATA AND CODE,0.8072866730584851,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
789"
OPEN ACCESS TO DATA AND CODE,0.8082454458293384,"including code, unless this is central to the contribution (e.g., for a new open-source
790"
OPEN ACCESS TO DATA AND CODE,0.8092042186001918,"benchmark).
791"
OPEN ACCESS TO DATA AND CODE,0.8101629913710451,"• The instructions should contain the exact command and environment needed to run to
792"
OPEN ACCESS TO DATA AND CODE,0.8111217641418984,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
793"
OPEN ACCESS TO DATA AND CODE,0.8120805369127517,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
794"
OPEN ACCESS TO DATA AND CODE,0.8130393096836049,"• The authors should provide instructions on data access and preparation, including how
795"
OPEN ACCESS TO DATA AND CODE,0.8139980824544583,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
796"
OPEN ACCESS TO DATA AND CODE,0.8149568552253116,"• The authors should provide scripts to reproduce all experimental results for the new
797"
OPEN ACCESS TO DATA AND CODE,0.8159156279961649,"proposed method and baselines. If only a subset of experiments are reproducible, they
798"
OPEN ACCESS TO DATA AND CODE,0.8168744007670182,"should state which ones are omitted from the script and why.
799"
OPEN ACCESS TO DATA AND CODE,0.8178331735378715,"• At submission time, to preserve anonymity, the authors should release anonymized
800"
OPEN ACCESS TO DATA AND CODE,0.8187919463087249,"versions (if applicable).
801"
OPEN ACCESS TO DATA AND CODE,0.8197507190795782,"• Providing as much information as possible in supplemental material (appended to the
802"
OPEN ACCESS TO DATA AND CODE,0.8207094918504314,"paper) is recommended, but including URLs to data and code is permitted.
803"
OPEN ACCESS TO DATA AND CODE,0.8216682646212847,"6. Experimental Setting/Details
804"
OPEN ACCESS TO DATA AND CODE,0.822627037392138,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
805"
OPEN ACCESS TO DATA AND CODE,0.8235858101629914,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
806"
OPEN ACCESS TO DATA AND CODE,0.8245445829338447,"results?
807"
OPEN ACCESS TO DATA AND CODE,0.825503355704698,"Answer: [Yes]
808"
OPEN ACCESS TO DATA AND CODE,0.8264621284755513,"Justification: This paper provides detailed information on the models, parameters, hyper-
809"
OPEN ACCESS TO DATA AND CODE,0.8274209012464045,"parameter selection, computational resources in Sec. 5 and Appendix A to ensure repro-
810"
OPEN ACCESS TO DATA AND CODE,0.8283796740172579,"ducibility.
811"
OPEN ACCESS TO DATA AND CODE,0.8293384467881112,"Guidelines:
812"
OPEN ACCESS TO DATA AND CODE,0.8302972195589645,"• The answer NA means that the paper does not include experiments.
813"
OPEN ACCESS TO DATA AND CODE,0.8312559923298178,"• The experimental setting should be presented in the core of the paper to a level of detail
814"
OPEN ACCESS TO DATA AND CODE,0.8322147651006712,"that is necessary to appreciate the results and make sense of them.
815"
OPEN ACCESS TO DATA AND CODE,0.8331735378715245,"• The full details can be provided either with the code, in appendix, or as supplemental
816"
OPEN ACCESS TO DATA AND CODE,0.8341323106423778,"material.
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.835091083413231,"7. Experiment Statistical Significance
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8360498561840843,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8370086289549377,"information about the statistical significance of the experiments?
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837967401725791,"Answer: [No]
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389261744966443,"Justification: The common practice in video editing does not including error bars, and we
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398849472674976,"follow the previous papers.
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840843720038351,"Guidelines:
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418024928092043,"• The answer NA means that the paper does not include experiments.
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427612655800575,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8437200383509108,"dence intervals, or statistical significance tests, at least for the experiments that support
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8446788111217641,"the main claims of the paper.
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8456375838926175,"• The factors of variability that the error bars are capturing should be clearly stated (for
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465963566634708,"example, train/test split, initialization, random drawing of some parameter, or overall
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475551294343241,"run with given experimental conditions).
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485139022051774,"• The method for calculating the error bars should be explained (closed form formula,
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494726749760306,"call to a library function, bootstrap, etc.)
833"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.850431447746884,"• The assumptions made should be given (e.g., Normally distributed errors).
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513902205177373,"• It should be clear whether the error bar is the standard deviation or the standard error
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523489932885906,"of the mean.
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533077660594439,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
837"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542665388302972,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
838"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552253116011506,"of Normality of errors is not verified.
839"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561840843720039,"• For asymmetric distributions, the authors should be careful not to show in tables or
840"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"figures symmetric error bars that would yield results that are out of range (e.g. negative
841"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8581016299137104,"error rates).
842"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8590604026845637,"• If error bars are reported in tables or plots, The authors should explain in the text how
843"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8600191754554171,"they were calculated and reference the corresponding figures or tables in the text.
844"
EXPERIMENTS COMPUTE RESOURCES,0.8609779482262704,"8. Experiments Compute Resources
845"
EXPERIMENTS COMPUTE RESOURCES,0.8619367209971237,"Question: For each experiment, does the paper provide sufficient information on the com-
846"
EXPERIMENTS COMPUTE RESOURCES,0.862895493767977,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
847"
EXPERIMENTS COMPUTE RESOURCES,0.8638542665388304,"the experiments?
848"
EXPERIMENTS COMPUTE RESOURCES,0.8648130393096836,"Answer: [Yes]
849"
EXPERIMENTS COMPUTE RESOURCES,0.8657718120805369,"Justification: This paper provides detailed information on the computational resources in
850"
EXPERIMENTS COMPUTE RESOURCES,0.8667305848513902,"Sec. 5 and Appendix A and inference time comparison in Tab. 6.
851"
EXPERIMENTS COMPUTE RESOURCES,0.8676893576222435,"Guidelines:
852"
EXPERIMENTS COMPUTE RESOURCES,0.8686481303930969,"• The answer NA means that the paper does not include experiments.
853"
EXPERIMENTS COMPUTE RESOURCES,0.8696069031639502,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
854"
EXPERIMENTS COMPUTE RESOURCES,0.8705656759348035,"or cloud provider, including relevant memory and storage.
855"
EXPERIMENTS COMPUTE RESOURCES,0.8715244487056567,"• The paper should provide the amount of compute required for each of the individual
856"
EXPERIMENTS COMPUTE RESOURCES,0.87248322147651,"experimental runs as well as estimate the total compute.
857"
EXPERIMENTS COMPUTE RESOURCES,0.8734419942473634,"• The paper should disclose whether the full research project required more compute
858"
EXPERIMENTS COMPUTE RESOURCES,0.8744007670182167,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
859"
EXPERIMENTS COMPUTE RESOURCES,0.87535953978907,"didn’t make it into the paper).
860"
CODE OF ETHICS,0.8763183125599233,"9. Code Of Ethics
861"
CODE OF ETHICS,0.8772770853307766,"Question: Does the research conducted in the paper conform, in every respect, with the
862"
CODE OF ETHICS,0.87823585810163,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
863"
CODE OF ETHICS,0.8791946308724832,"Answer: [Yes]
864"
CODE OF ETHICS,0.8801534036433365,"Justification: The research strictly adheres to the NeurIPS Code of Ethics in every respect.
865"
CODE OF ETHICS,0.8811121764141898,"Guidelines:
866"
CODE OF ETHICS,0.8820709491850431,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
867"
CODE OF ETHICS,0.8830297219558965,"• If the authors answer No, they should explain the special circumstances that require a
868"
CODE OF ETHICS,0.8839884947267498,"deviation from the Code of Ethics.
869"
CODE OF ETHICS,0.8849472674976031,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
870"
CODE OF ETHICS,0.8859060402684564,"eration due to laws or regulations in their jurisdiction).
871"
BROADER IMPACTS,0.8868648130393096,"10. Broader Impacts
872"
BROADER IMPACTS,0.887823585810163,"Question: Does the paper discuss both potential positive societal impacts and negative
873"
BROADER IMPACTS,0.8887823585810163,"societal impacts of the work performed?
874"
BROADER IMPACTS,0.8897411313518696,"Answer: [Yes]
875"
BROADER IMPACTS,0.8906999041227229,"Justification: The broader impacts are discussed in Appendix C.
876"
BROADER IMPACTS,0.8916586768935763,"Guidelines:
877"
BROADER IMPACTS,0.8926174496644296,"• The answer NA means that there is no societal impact of the work performed.
878"
BROADER IMPACTS,0.8935762224352828,"• If the authors answer NA or No, they should explain why their work has no societal
879"
BROADER IMPACTS,0.8945349952061361,"impact or why the paper does not address societal impact.
880"
BROADER IMPACTS,0.8954937679769894,"• Examples of negative societal impacts include potential malicious or unintended uses
881"
BROADER IMPACTS,0.8964525407478428,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
882"
BROADER IMPACTS,0.8974113135186961,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
883"
BROADER IMPACTS,0.8983700862895494,"groups), privacy considerations, and security considerations.
884"
BROADER IMPACTS,0.8993288590604027,"• The conference expects that many papers will be foundational research and not tied
885"
BROADER IMPACTS,0.900287631831256,"to particular applications, let alone deployments. However, if there is a direct path to
886"
BROADER IMPACTS,0.9012464046021093,"any negative applications, the authors should point it out. For example, it is legitimate
887"
BROADER IMPACTS,0.9022051773729626,"to point out that an improvement in the quality of generative models could be used to
888"
BROADER IMPACTS,0.9031639501438159,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
889"
BROADER IMPACTS,0.9041227229146692,"that a generic algorithm for optimizing neural networks could enable people to train
890"
BROADER IMPACTS,0.9050814956855225,"models that generate Deepfakes faster.
891"
BROADER IMPACTS,0.9060402684563759,"• The authors should consider possible harms that could arise when the technology is
892"
BROADER IMPACTS,0.9069990412272292,"being used as intended and functioning correctly, harms that could arise when the
893"
BROADER IMPACTS,0.9079578139980825,"technology is being used as intended but gives incorrect results, and harms following
894"
BROADER IMPACTS,0.9089165867689357,"from (intentional or unintentional) misuse of the technology.
895"
BROADER IMPACTS,0.909875359539789,"• If there are negative societal impacts, the authors could also discuss possible mitigation
896"
BROADER IMPACTS,0.9108341323106424,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
897"
BROADER IMPACTS,0.9117929050814957,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
898"
BROADER IMPACTS,0.912751677852349,"feedback over time, improving the efficiency and accessibility of ML).
899"
SAFEGUARDS,0.9137104506232023,"11. Safeguards
900"
SAFEGUARDS,0.9146692233940557,"Question: Does the paper describe safeguards that have been put in place for responsible
901"
SAFEGUARDS,0.9156279961649089,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
902"
SAFEGUARDS,0.9165867689357622,"image generators, or scraped datasets)?
903"
SAFEGUARDS,0.9175455417066155,"Answer: [NA]
904"
SAFEGUARDS,0.9185043144774688,"Justification: This paper poses no such risks.
905"
SAFEGUARDS,0.9194630872483222,"Guidelines:
906"
SAFEGUARDS,0.9204218600191755,"• The answer NA means that the paper poses no such risks.
907"
SAFEGUARDS,0.9213806327900288,"• Released models that have a high risk for misuse or dual-use should be released with
908"
SAFEGUARDS,0.9223394055608821,"necessary safeguards to allow for controlled use of the model, for example by requiring
909"
SAFEGUARDS,0.9232981783317353,"that users adhere to usage guidelines or restrictions to access the model or implementing
910"
SAFEGUARDS,0.9242569511025887,"safety filters.
911"
SAFEGUARDS,0.925215723873442,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
912"
SAFEGUARDS,0.9261744966442953,"should describe how they avoided releasing unsafe images.
913"
SAFEGUARDS,0.9271332694151486,"• We recognize that providing effective safeguards is challenging, and many papers do
914"
SAFEGUARDS,0.9280920421860019,"not require this, but we encourage authors to take this into account and make a best
915"
SAFEGUARDS,0.9290508149568553,"faith effort.
916"
LICENSES FOR EXISTING ASSETS,0.9300095877277086,"12. Licenses for existing assets
917"
LICENSES FOR EXISTING ASSETS,0.9309683604985618,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
918"
LICENSES FOR EXISTING ASSETS,0.9319271332694151,"the paper, properly credited and are the license and terms of use explicitly mentioned and
919"
LICENSES FOR EXISTING ASSETS,0.9328859060402684,"properly respected?
920"
LICENSES FOR EXISTING ASSETS,0.9338446788111218,"Answer: [Yes]
921"
LICENSES FOR EXISTING ASSETS,0.9348034515819751,"Justification: Yes, the creators or original owners of assets used in the paper are properly
922"
LICENSES FOR EXISTING ASSETS,0.9357622243528284,"credited, and the license and terms of use are explicitly mentioned and properly respected.
923"
LICENSES FOR EXISTING ASSETS,0.9367209971236817,"Guidelines:
924"
LICENSES FOR EXISTING ASSETS,0.9376797698945349,"• The answer NA means that the paper does not use existing assets.
925"
LICENSES FOR EXISTING ASSETS,0.9386385426653883,"• The authors should cite the original paper that produced the code package or dataset.
926"
LICENSES FOR EXISTING ASSETS,0.9395973154362416,"• The authors should state which version of the asset is used and, if possible, include a
927"
LICENSES FOR EXISTING ASSETS,0.9405560882070949,"URL.
928"
LICENSES FOR EXISTING ASSETS,0.9415148609779482,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
929"
LICENSES FOR EXISTING ASSETS,0.9424736337488016,"• For scraped data from a particular source (e.g., website), the copyright and terms of
930"
LICENSES FOR EXISTING ASSETS,0.9434324065196549,"service of that source should be provided.
931"
LICENSES FOR EXISTING ASSETS,0.9443911792905082,"• If assets are released, the license, copyright information, and terms of use in the
932"
LICENSES FOR EXISTING ASSETS,0.9453499520613614,"package should be provided. For popular datasets, paperswithcode.com/datasets
933"
LICENSES FOR EXISTING ASSETS,0.9463087248322147,"has curated licenses for some datasets. Their licensing guide can help determine the
934"
LICENSES FOR EXISTING ASSETS,0.947267497603068,"license of a dataset.
935"
LICENSES FOR EXISTING ASSETS,0.9482262703739214,"• For existing datasets that are re-packaged, both the original license and the license of
936"
LICENSES FOR EXISTING ASSETS,0.9491850431447747,"the derived asset (if it has changed) should be provided.
937"
LICENSES FOR EXISTING ASSETS,0.950143815915628,"• If this information is not available online, the authors are encouraged to reach out to
938"
LICENSES FOR EXISTING ASSETS,0.9511025886864813,"the asset’s creators.
939"
NEW ASSETS,0.9520613614573347,"13. New Assets
940"
NEW ASSETS,0.9530201342281879,"Question: Are new assets introduced in the paper well documented and is the documentation
941"
NEW ASSETS,0.9539789069990412,"provided alongside the assets?
942"
NEW ASSETS,0.9549376797698945,"Answer: [Yes]
943"
NEW ASSETS,0.9558964525407478,"Justification: We have uploaded the code of this paper to an anonymous repository and
944"
NEW ASSETS,0.9568552253116012,"provided the corresponding link in Appendix. The code will be made publicly available
945"
NEW ASSETS,0.9578139980824545,"after the paper is published.
946"
NEW ASSETS,0.9587727708533078,"Guidelines:
947"
NEW ASSETS,0.959731543624161,"• The answer NA means that the paper does not release new assets.
948"
NEW ASSETS,0.9606903163950143,"• Researchers should communicate the details of the dataset/code/model as part of their
949"
NEW ASSETS,0.9616490891658677,"submissions via structured templates. This includes details about training, license,
950"
NEW ASSETS,0.962607861936721,"limitations, etc.
951"
NEW ASSETS,0.9635666347075743,"• The paper should discuss whether and how consent was obtained from people whose
952"
NEW ASSETS,0.9645254074784276,"asset is used.
953"
NEW ASSETS,0.965484180249281,"• At submission time, remember to anonymize your assets (if applicable). You can either
954"
NEW ASSETS,0.9664429530201343,"create an anonymized URL or include an anonymized zip file.
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674017257909875,"14. Crowdsourcing and Research with Human Subjects
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683604985618408,"Question: For crowdsourcing experiments and research with human subjects, does the paper
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693192713326941,"include the full text of instructions given to participants and screenshots, if applicable, as
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702780441035475,"well as details about compensation (if any)?
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712368168744008,"Answer: [NA]
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721955896452541,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9731543624161074,"Guidelines:
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9741131351869607,"• The answer NA means that the paper does not involve crowdsourcing nor research with
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975071907957814,"human subjects.
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760306807286673,"• Including this information in the supplemental material is fine, but if the main contribu-
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769894534995206,"tion of the paper involves human subjects, then as much detail as possible should be
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779482262703739,"included in the main paper.
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789069990412272,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798657718120806,"or other labor should be paid at least the minimum wage in the country of the data
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808245445829339,"collector.
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817833173537871,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827420901246404,"Subjects
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837008628954937,"Question: Does the paper describe potential risks incurred by study participants, whether
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846596356663471,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856184084372004,"approvals (or an equivalent approval/review based on the requirements of your country or
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865771812080537,"institution) were obtained?
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987535953978907,"Answer: [NA]
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884947267497604,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894534995206136,"Guidelines:
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904122722914669,"• The answer NA means that the paper does not involve crowdsourcing nor research with
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913710450623202,"human subjects.
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923298178331735,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932885906040269,"may be required for any human subjects research. If you obtained IRB approval, you
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942473633748802,"should clearly state this in the paper.
984"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952061361457335,"• We recognize that the procedures for this may vary significantly between institutions
985"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961649089165868,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
986"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99712368168744,"guidelines for their institution.
987"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980824544582934,"• For initial submissions, do not include any information that would break anonymity (if
988"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990412272291467,"applicable), such as the institution conducting the review.
989"
