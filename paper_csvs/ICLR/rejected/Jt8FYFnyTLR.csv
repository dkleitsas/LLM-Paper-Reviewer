Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0009699321047526673,"Interpretable and explainable machine learning has seen a recent surge of inter-
est. We posit that safety is a key reason behind the demand for explainability.
To explore this relationship, we propose a mathematical formulation for assess-
ing the safety of supervised learning models based on their maximum deviation
over a certiﬁcation set. We then show that for interpretable models including
decision trees, rule lists, generalized linear and additive models, the maximum
deviation can be computed exactly and efﬁciently. For tree ensembles, which are
not regarded as interpretable, discrete optimization techniques can still provide
informative bounds. For a broader class of piecewise Lipschitz functions, we re-
purpose results from the multi-armed bandit literature to show that interpretability
produces tighter (regret) bounds on the maximum deviation compared with black
box functions. We perform experiments that quantify the dependence of the max-
imum deviation on model smoothness and certiﬁcation set size. The experiments
also illustrate how the solutions that maximize deviation can suggest safety risks."
INTRODUCTION,0.0019398642095053346,"1
INTRODUCTION"
INTRODUCTION,0.002909796314258002,"Interpretable and explainable machine learning (ML) has seen a recent surge of interest because
it is viewed as one of the key pillars in making models trustworthy, with implications on fairness,
reliability, and safety (Varshney, 2019). It is generally accepted that the ultimate measure of ML
explainability is whether a human ﬁnds the explanations useful (Doshi-Velez & Kim, 2017; Dhu-
randhar et al., 2017). However, less attention has been paid to deeper reasons behind the human
desire for explainability. In this paper, we posit an important reason is toward achieving safety
and preventing unexpected harms (Varshney & Alemzadeh, 2017). This reason is implicit in the
dichotomy between directly interpretable models vs. post hoc explanations of black-box models.
Some argue that only directly interpretable models should be used in high-risk applications (Rudin,
2019). The crux of this argument is that post hoc explanations leave a gap between the explana-
tion and the model that is producing the predictions. Thus, unusual data points may appear to be
harmless based on the explanation, but truly cause havoc. This argument however does not explic-
itly address the question: What does safety mean for such models, and how is it intertwined with
interpretability?"
INTRODUCTION,0.0038797284190106693,"Towards answering this question, we propose a mathematical deﬁnition for assessing the safety of
supervised learning (i.e. predictive) models. Viewing these models as functions mapping an input
space to an output space, a key way in which these models can cause harm is through grossly unex-
pected outputs, corresponding to inputs that are poorly represented in training data. Accordingly, we
approach safety assessment for a model by determining its maximum deviation over a certiﬁcation
set from the output of a reference model. The idea of a certiﬁcation set is that it is a large subset of
the input space and is intended to cover all conceivable inputs to the model. The reference model
could be a simple, well-understood model or an existing model that has been “tried and tested.”
These concepts are discussed further in Section 2."
INTRODUCTION,0.004849660523763337,"In Section 4, we discuss the computation of the maximum deviation for different model classes and
show how this is facilitated by interpretability. For model classes regarded as interpretable, including
trees, rule lists, generalized linear and additive models, the maximum deviation can be computed
exactly and efﬁciently by exploiting the model structure. For tree ensembles, which are not regarded
as interpretable, discrete optimization techniques can exploit their composition in terms of trees"
INTRODUCTION,0.005819592628516004,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006789524733268671,"to provide anytime bounds on the maximum deviation. The case of trees is also generalized in a
different direction by considering a broader class of functions that are piecewise Lipschitz, which we
argue cover many popular interpretable functions. Here we show that the beneﬁt of interpretability
is signiﬁcantly tighter regret bounds on the maximum deviation compared with black box functions,
by appropriately repurposing results from the multi-armed bandit literature in this context. On the
other hand, it is less clear that post hoc explanations, which approximate a model locally (Ribeiro
et al., 2016; Lundberg & Lee, 2017; Dhurandhar et al., 2018) or globally (Buciluˇa et al., 2006;
Hinton et al., 2015), can help with evaluating the maximum deviation and hence safety."
INTRODUCTION,0.007759456838021339,"We conduct experiments to illustrate the deviation maximization methods in Section 4 for decision
trees, linear and additive models, and tree ensembles. The results in Section 5 quantify how the
maximum deviation increases as the size of the certiﬁcation set increases and as the smoothness of
the models decreases. For tree ensembles, we ﬁnd that the obtained upper bounds on the maximum
deviation are informative, showing that the maximum deviation does not increase with the number
of trees in the ensemble. We also study the feature combinations that maximize deviation, which
can shed light on the sources of extreme model outputs and guide further investigation."
INTRODUCTION,0.008729388942774006,"Overall, our discussion suggests that a reason for preferring more interpretable models is that it is
easier to assess them for unexpected and potentially unsafe outputs."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.009699321047526674,"2
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION"
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.01066925315227934,"We are given a supervised learning model f, which is a function mapping an input feature space X
to an output space Y. We wish to assess the safety of this model by ﬁnding its worst-case deviation
from a given reference model f0 : X 7→Y. To do this, we additionally require 1) a measure of
deviation D : Y × Y 7→R+, where R+ is the set of non-negative reals, and 2) a certiﬁcation set
C ⊆X over which the deviation is maximized. Then the problem to be solved is"
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.011639185257032008,"max
x∈C D(f(x), f0(x)).
(1)"
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.012609117361784675,"The deviation is worst-case because the maximization is over all x ∈C; further implications of this
are discussed in Appendix C."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.013579049466537343,"We view problem (1) as a means toward the goal of evaluating safety. In particular, a large deviation
value is not necessarily indicative of a safety risk, as two models may differ signiﬁcantly for valid
reasons. For example, one model may capture a useful pattern that the other does not. What large
deviation values do indicate, however, is a (possibly) sufﬁcient reason for further investigation.
Hence, the maximizing solutions in (1) (i.e., the arg max) are of operational interest."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.014548981571290009,Below we further discuss some elements in this problem formulation.
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.015518913676042677,"Output space Y. In the case of regression, Y is the set of reals R or an interval thereof. In the case
of binary classiﬁcation, while Y could be {0, 1} or {−1, +1}, these limit the possible deviations
to binary values as well (“same” or “different”). Thus to provide more informative results, we
take Y to be the space of real-valued scores that are thresholded to produce a binary label. For
example, y could be a predicted probability in [0, 1] or a log-odds ratio in R. Similarly for multi-
class classiﬁcation with M classes, Y ⊂RM could be a M-dimensional space of real-valued scores.
Models that abstain can also be accommodated as noted in Appendix A."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.016488845780795344,"Reference model f0. The premise of the reference model is that it should be “safe” above all. The
simplest case mathematically is for f0 to be a constant function representing a baseline value, for
example zero. More generally, f0 may be a simple model that can be readily grasped by a human,
may be validated against domain knowledge, or may be based on a small number of expert-selected
features. Such models are common in medical risk assessment, consumer ﬁnance, and predicting
semiconductor yield. By simple, we mean for example a linear model with 10 non-zero coefﬁcients
or a decision tree with 10 leaves. The reference model could also be an existing model that is not
necessarily simple but has been extensively tested and deployed. In this case, f could be a new
version of the model, trained on more recent data or improved in some fashion, and we wish to
evaluate its safety before deploying it in place of f0. In this and more complex settings, f0 may
not be globally interpretable, but may be so in local regions. The machinery developed in this work
could be applied in these settings as well to assess the safety of f (more discussion in Appendix C)."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.01745877788554801,Under review as a conference paper at ICLR 2022
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.01842870999030068,"Certiﬁcation set C. The premise of the certiﬁcation set is that it contains all inputs that the model
might conceivably be exposed to. This may include inputs that are highly improbable but not phys-
ically or logically impossible (for example, a severely hypothermic human body temperature of
27°C). Thus, while C might be based on the support set of a probability distribution or data sample,
it does not depend on the likelihood of points within the support. The set C may also be a strict su-
perset of the training data domain. For example, a model may have been trained on data for males,
and we would now like to determine its worst-case behavior on an unseen population of females."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.019398642095053348,"For tabular or lower-dimensional data, C might be the entire input space X. For non-tabular or
higher-dimensional data, the choice C = X may be too unrepresentative because the manifold of
realistic inputs is lower in dimension. In this case, if we have a dataset {xi}n
i=1, one possibility is to
use a union of ℓp balls centered at xi, C = n
["
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.020368574199806012,"i=1
Bp
r[xi],
Bp
r[xi] = {x ∈X : ∥x −xi∥p ≤r}.
(2)"
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.02133850630455868,"The set C is thus comprised of points somewhat close to the n observed examples xi, but the radius
r does not have to be “small”."
ASSESSING SAFETY THROUGH MAXIMUM DEVIATION,0.02230843840931135,"In addition to determining the maximum deviation over the entire set C, maximum deviations over
subsets of C (e.g., different age groups) may also be of interest. For example, Appendix D.1 shows
deviation values separately for leaves of a decision tree, which partition the input space."
RELATED WORK,0.023278370514064017,"3
RELATED WORK"
RELATED WORK,0.02424830261881668,"Our work relates to a number of different technical directions. Varshney & Alemzadeh (2017) and
Mohseni et al. (2021) give qualitative accounts suggesting that directly interpretable models are an
inherently safe design because humans can inspect them to ﬁnd spurious elements; in this paper, we
attempt to make those qualitative suggestions more quantitative. Furthermore, several other authors
have highlighted safety as a goal for interpretability, but without much further development as done
here (Otte, 2013; Doshi-Velez & Kim, 2017; Tomsett et al., 2018; Gilpin et al., 2018; Rudin, 2019).
Moreover, there is no consensus on how to measure interpretability, which motivates the relationship
explored in this paper between interpretability and the ease of evaluating safety."
RELATED WORK,0.02521823472356935,"In the area of ML veriﬁcation, robustness certiﬁcation methods aim to provide guarantees that the
classiﬁcation remains constant within a radius ϵ of an input point, while output reachability is con-
cerned with characterizing the set of outputs corresponding to a region of inputs (Wong & Kolter,
2018; Raghunathan et al., 2018; Huang et al., 2020). Our problem of deviation maximization (1)
is more closely related to output reachability. The differences in our work are: 1) we consider two
models, a model f to be assessed and a reference f0, and are interested in their difference as mea-
sured by deviation function D; 2) our focus is global, over a comprehensive set C, rather than local
to small neighborhoods around input points; 3) we study the role of interpretability in safety veriﬁca-
tion. Moreover, works in robust optimization applied to machine learning minimize the worst-case
probability of error, but this worst case is over parameters of f rather than over individual values of x
(Lanckriet et al., 2002). Thomas et al. (2019) present a framework where during the model training,
a set of safety tests is speciﬁed in order to accept or reject the possible solution. The speciﬁcation of
these tests is left to the model designer but the goal of the proposed solution is to provide a reusable
paradigm to support safety in ML solutions."
RELATED WORK,0.026188166828322017,"We build on related literature from model robustness and explainability areas that deals speciﬁcally
with tree ensembles. Kantchelian et al. (2016) seek to ﬁnd the smallest perturbation of an input
instance to ‘evade’ a classiﬁer using mixed-integer programming (MIP). Optimization formulations
are also explored by Parmentier & Vidal (2021) for the purposes of counterfactual explanations.
MIP approaches are computationally intensive however. To address this Chen et al. (2019) introduce
graph based approaches for veriﬁcation on trees. Their central idea, which we use, is to discretize
veriﬁcation computations onto a graph constructed from the way leaves intersect. The veriﬁcation
problem is transformed to ﬁnding all maximum cliques. Devos et al. (2021) expand on this idea by
providing anytime bounds by probing unexplored nodes."
RELATED WORK,0.027158098933074686,"Safety has become a critical issue in reinforcement learning (RL) with multiple works focusing on
making RL policies safe (Amodei et al., 2016; Zhu et al., 2019; Inala et al., 2020; Rupprecht et al.,"
RELATED WORK,0.028128031037827354,Under review as a conference paper at ICLR 2022
RELATED WORK,0.029097963142580018,"2020). There are two broad themes (Garc´ıa et al., 2015): (i) a safe and veriﬁable policy is learned at
the outset by enforcing certain constraints, and (ii) post hoc methods are used to identify bad regimes
or failure points of an existing policy. Our current proposal is complementary to these works as
we focus on the supervised learning setup viewed from the lens of interpretability. Nonetheless,
ramiﬁcations of our work in the RL context are brieﬂy discussed in Appendix C."
DEVIATION MAXIMIZATION FOR SPECIFIC MODEL CLASSES,0.030067895247332686,"4
DEVIATION MAXIMIZATION FOR SPECIFIC MODEL CLASSES"
DEVIATION MAXIMIZATION FOR SPECIFIC MODEL CLASSES,0.031037827352085354,"In this section, we discuss approaches to computing the maximum deviation (1) for f belonging
to various model classes. The beneﬁt of interpretable model structure is seen in different guises.
For decision trees, generalized linear and additive models in Sections 4.1 and 4.2, exact and efﬁcient
computation is possible. For tree ensembles in Section 4.3, their composition in terms of trees allows
discrete optimization methods to provide anytime bounds. For a general class of piecewise Lipschitz
functions in Section 4.4, the application of multi-arm bandit results yields tighter regret bounds on
the maximum deviation. The results in Sections 4.1–4.4 also show that intuitive measures of model
complexity, such as the number of leaves or pieces or smoothness of functions, have more precise
interpretations as well in terms of the complexity of maximizing deviation. On the other hand, for
post hoc explanations in Appendix B.5, we believe that more development will be needed for them
to help in evaluating maximum deviation."
DEVIATION MAXIMIZATION FOR SPECIFIC MODEL CLASSES,0.03200775945683802,"To develop mathematical results and efﬁcient algorithms, we will sometimes assume that the refer-
ence model f0 is from the same class as f. We will also sometimes assume that the certiﬁcation
set C and other sets are Cartesian products. This means that C = Qd
j=1 Cj, where for a continuous
feature j, Cj = [Xj, Xj] is an interval, and for a categorical feature j, Cj is a set of categories."
TREES,0.03297769156159069,"4.1
TREES"
TREES,0.03394762366634336,"We begin with the case where f and f0 are both decision trees. A decision tree with L leaves
partitions the input space X into L corresponding parts, which we also refer to as “leaves”. We
consider only non-oblique trees. In this case, each leaf is described by a conjunction of conditions
on individual features and is therefore a Cartesian product as deﬁned above. With Ll ⊂X denoting
the lth leaf and yl ∈Y the output value assigned to it, tree f is described by the function
f(x) = yl
if x ∈Ll,
l = 1, . . . , L,
(3)
and similarly for tree f0 with leaves L0m and outputs y0m, m = 1, . . . , L0. Rule lists can be seen as
one-sided trees (Yang et al., 2017; Angelino et al., 2018) and are subsumed in this discussion."
TREES,0.03491755577109602,"The partitioning of X by decision trees and their piecewise-constant nature simplify the computation
of the maximum deviation (1). Speciﬁcally, the maximization can be restricted to pairs of leaves
(l, m) for which the intersection Ll∩L0m∩C is non-empty. The intersection of two leaves Ll∩L0m
is another Cartesian product, and we assume that it is tractable to determine whether C intersects a
given Cartesian product (see examples in Appendix B.1)."
TREES,0.03588748787584869,"For visual representation and later use in Section 4.3, it is useful to deﬁne a bipartite graph, with L
nodes representing the leaves Ll of f on one side and L0 nodes representing the leaves L0m of f0
on the other. Deﬁne the edge set E = {(l, m) : Ll ∩L0m ∩C ̸= ∅}; clearly |E| ≤L0L. Then
max
x∈C D(f(x), f0(x)) = max
(l,m)∈E D(yl, y0m).
(4)"
TREES,0.03685741998060136,"We summarize the complexity of deviation maximization for decision trees as follows.
Proposition 1. Let f and f0 be decision trees as in (3) with L and L0 leaves respectively, and E be
the bipartite edge set of leaf intersections deﬁned above. Then the maximum deviation (1) can be
computed with |E| evaluations as shown in (4)."
LINEAR AND ADDITIVE MODELS,0.037827352085354024,"4.2
LINEAR AND ADDITIVE MODELS"
LINEAR AND ADDITIVE MODELS,0.038797284190106696,"In this subsection, we assume that f is a generalized additive model (GAM) given by"
LINEAR AND ADDITIVE MODELS,0.03976721629485936,"f(x) = g−1  
d
X"
LINEAR AND ADDITIVE MODELS,0.040737148399612025,"j=1
fj(xj) "
LINEAR AND ADDITIVE MODELS,0.041707080504364696,",
(5)"
LINEAR AND ADDITIVE MODELS,0.04267701260911736,Under review as a conference paper at ICLR 2022
LINEAR AND ADDITIVE MODELS,0.04364694471387003,"where each fj is an arbitrary function of feature xj. In the case where fj(xj) = wjxj for all
continuous features xj, where wj is a real coefﬁcient, (5) is a generalized linear model (GLM). We
discuss the treatment of categorical features in Appendix B.2. The invertible link function g : R 7→
R is furthermore assumed to be monotonically increasing. This assumption is satisﬁed by common
GAM link functions: identity, logit (g(y) = log(y/(1 −y))), and logarithmic."
LINEAR AND ADDITIVE MODELS,0.0446168768186227,"Equation (5) implies that Y ⊂R and the deviation D(y, y0) is a function of two scalars y and y0. For
this scalar case, we make the following intuitively reasonable assumption throughout the subsection."
LINEAR AND ADDITIVE MODELS,0.04558680892337536,"Assumption 1. For y, y0 ∈Y ⊆R, 1) D(y, y0) = 0 whenever y = y0; 2) D(y, y0) is monotonically
non-decreasing in y for y ≥y0 and non-increasing in y for y ≤y0."
LINEAR AND ADDITIVE MODELS,0.04655674102812803,Our approach is to exploit the additive form of (5) by reducing problem (1) to the optimization
LINEAR AND ADDITIVE MODELS,0.0475266731328807,"max
x∈S d
X"
LINEAR AND ADDITIVE MODELS,0.04849660523763336,"j=1
fj(xj),
(6)"
LINEAR AND ADDITIVE MODELS,0.049466537342386034,"for different choices of S ⊂X and where minimization is obtained by negating all fj. We discuss
below how this can be done for two types of reference model f0: decision tree (which includes the
constant case L0 = 1) and additive. For the ﬁrst case, we prove the following result in Appendix B.2:"
LINEAR AND ADDITIVE MODELS,0.0504364694471387,"Proposition 2. Let f be a GAM as in (5) and S be a subset of X where f0(x) ≡y0 is constant.
Then if Assumption 1 holds,"
LINEAR AND ADDITIVE MODELS,0.05140640155189137,"max
x∈S D(f(x), f0(x)) = max 
 D  g−1 "
LINEAR AND ADDITIVE MODELS,0.052376333656644035,"max
x∈S d
X"
LINEAR AND ADDITIVE MODELS,0.0533462657613967,"j=1
fj(xj)  , y0  , D  g−1 "
LINEAR AND ADDITIVE MODELS,0.05431619786614937,"min
x∈S d
X"
LINEAR AND ADDITIVE MODELS,0.055286129970902036,"j=1
fj(xj)  , y0   
 ."
LINEAR AND ADDITIVE MODELS,0.05625606207565471,"Tree-structured f0. Since f0 is piecewise constant over its leaves L0m, m = 1, . . . , L0, we take S
to be the intersection of C with each L0m in turn and apply Proposition 2. The overall maximum is
then obtained as the maximum over the leaves,"
LINEAR AND ADDITIVE MODELS,0.05722599418040737,"max
x∈C D(f(x), f0(x)) =
max
m=1,...,L0 max 
 D  g−1 "
LINEAR AND ADDITIVE MODELS,0.058195926285160036,"
max
x∈L0m∩C d
X"
LINEAR AND ADDITIVE MODELS,0.05916585838991271,"j=1
fj(xj) "
LINEAR AND ADDITIVE MODELS,0.06013579049466537,", y0m  , D  g−1 "
LINEAR AND ADDITIVE MODELS,0.061105722599418044,"
min
x∈L0m∩C d
X"
LINEAR AND ADDITIVE MODELS,0.06207565470417071,"j=1
fj(xj) "
LINEAR AND ADDITIVE MODELS,0.06304558680892337,", y0m   
 . (7)"
LINEAR AND ADDITIVE MODELS,0.06401551891367604,This reduces (1) to solving 2L0 instances of (6).
LINEAR AND ADDITIVE MODELS,0.06498545101842872,"Additive f0. For this case, we make the additional assumption that the link function g in (5) is the
identity function, as well as Assumption 2 below. The implication of these assumptions is discussed
in Appendix B.2."
LINEAR AND ADDITIVE MODELS,0.06595538312318137,"Assumption 2. D(y, y0) = D(y −y0) is a function only of the difference y −y0."
LINEAR AND ADDITIVE MODELS,0.06692531522793405,"Then f0(x) = Pd
j=1 f0j(xj) and the difference f(x)−f0(x) is also additive. Using Assumptions 2,
1 and a similar argument as in the proof of Proposition 2, the maximum deviation is again obtained
by maximizing and minimizing an additive function, resulting in two instances of (6) with S = C:"
LINEAR AND ADDITIVE MODELS,0.06789524733268672,"max
x∈C D(f(x), f0(x)) = max 
 D "
LINEAR AND ADDITIVE MODELS,0.06886517943743937,"max
x∈C d
X"
LINEAR AND ADDITIVE MODELS,0.06983511154219205,"j=1
fj(xj) −f0j(xj)  , D "
LINEAR AND ADDITIVE MODELS,0.07080504364694472,"min
x∈C d
X"
LINEAR AND ADDITIVE MODELS,0.07177497575169738,"j=1
fj(xj) −f0j(xj)   
 ."
LINEAR AND ADDITIVE MODELS,0.07274490785645005,"Computational complexity of (6). For the case of nonlinear additive f, we additionally assume
that C is a Cartesian product. It follows that S = Qd
j=1 Sj is a Cartesian product (see Appendix B.2
for the brief justiﬁcation) and (6) separates into one-dimensional optimizations over Sj,"
LINEAR AND ADDITIVE MODELS,0.07371483996120272,"max
x∈S d
X"
LINEAR AND ADDITIVE MODELS,0.07468477206595538,"j=1
fj(xj) = d
X"
LINEAR AND ADDITIVE MODELS,0.07565470417070805,"j=1
max
xj∈Sj fj(xj).
(8)"
LINEAR AND ADDITIVE MODELS,0.07662463627546072,Under review as a conference paper at ICLR 2022
LINEAR AND ADDITIVE MODELS,0.07759456838021339,"The computational complexity of (8) is thus Pd
j=1 Cj, where Cj is the complexity of the jth one-
dimensional optimization. We discuss different cases of Cj in Appendix B.2; the important point is
that the overall complexity is linear in d."
LINEAR AND ADDITIVE MODELS,0.07856450048496605,"In the GLM case where Pd
j=1 fj(xj) = wT x, problem (6) is simpler and it is less important that C
be a Cartesian product. In particular, if C is a convex set, so too is S (again see Appendix B.2 for
justiﬁcation). Hence (6) is a convex optimization problem."
TREE ENSEMBLES,0.07953443258971872,"4.3
TREE ENSEMBLES"
TREE ENSEMBLES,0.08050436469447139,"We now extend the idea used for single decision trees in Section 4.1 to tree ensembles. This class
covers several popular methods such as Random Forests and Gradient Boosted Trees. It can also
cover rule ensembles (Friedman & Popescu, 2008; Dembczy´nski et al., 2010) as a special case, as
explained in Appendix B.3. We assume f is a tree ensemble consisting of K trees and f0 is a single
decision tree. Let Llk denote the lth leaf of the kth tree in f for l = 1, . . . , Lk, and L0m be the mth
leaf f0, for m = 1, . . . , L0. Correspondingly let ylk and y0m denote the prediction values associated
with each leaf."
TREE ENSEMBLES,0.08147429679922405,"Deﬁne a graph G(V, E), where there is a vertex for each leaf in f and f0, i.e."
TREE ENSEMBLES,0.08244422890397672,"V = {lk|∀k = 1, . . . , K, l = 1, . . . , Lk} ∪{m|m = 1, . . . , L0}.
(9)"
TREE ENSEMBLES,0.08341416100872939,"Construct an edge for each overlapping pair of leaves in V, i.e."
TREE ENSEMBLES,0.08438409311348205,"E = {(i, j)|Li ∩Lj ̸= ∅, ∀(i, j) ∈V, i ̸= j}.
(10)"
TREE ENSEMBLES,0.08535402521823472,"This graph is a K + 1-partite graph as leaves within an individual tree do not intersect and are
an independent set. Denote M to be the adjacency matrix of G. Following Chen et al. (2019), a
maximum clique S of size K + 1 on such a graph provides a discrete region in the feature space
with a computable deviation. A clique is a subset of nodes all connected to each other; a maximum
clique is one that cannot be expanded further by adding a node. The model predictions yc and y0c
can be ensembled from leaves in S. Denote D(S) to be the deviation computed from the clique
S. Maximizing over all such cliques solves (1). However, complete enumeration is expensive, so
informative bounds, either using the merge procedure in Chen et al. (2019) or the heuristic function
in Devos et al. (2021) can be used. We use the latter which exploits the K + 1-partite structure of G."
TREE ENSEMBLES,0.0863239573229874,"Speciﬁcally, we adapt the anytime bounds of Devos et al. (2021) as follows. At each step of the
enumeration procedure, an intermediate clique S contains selected leaves from trees in [1, . . . , k]
and unexplored trees in [k + 1, . . . , K + 1]. For each unexplored tree, we select a valid candidate
leaf that maximizes deviation, i.e."
TREE ENSEMBLES,0.08729388942774007,"vk =
arg max
lk, lk∩i̸=∅, ∀i∈S
D(S ∪lk).
(11)"
TREE ENSEMBLES,0.08826382153249272,"Using these worst-case leaves, a heuristic function"
TREE ENSEMBLES,0.0892337536372454,"H(S) = D(S′) = D(S K+1
["
TREE ENSEMBLES,0.09020368574199807,"m=k+1
vm)
(12)"
TREE ENSEMBLES,0.09117361784675072,"provides an upper (dual) bound. In practice, this dual bound is tight and therefore very useful during
the search procedure to prune the search space. Each K + 1 clique provides a primal bound, so the
search can be terminated early before examining all trees if the dual bound is less than the primal
bound. We adapt the search procedure from Mirghorbani & Krokhmal (2013) to include the pruning
arguments. Appendix B.3 presents the full algorithm. Starting with an empty clique, the procedure
adds a single node from each tree to create an intermediate clique. If the size of the clique is K + 1
the primal bound is updated. Otherwise, the dual bound is computed. A node compatibility vector is
used to keep track of all feasible additions.When the search is terminated at any step, the maximum
deviation is bounded by (Dlb, Dub)."
TREE ENSEMBLES,0.0921435499515034,"The algorithm works for the entire feature space. When the certiﬁcation set C is a union of balls
as in Eq. (2), some additional considerations are needed. First, we can disregard leaves that do not
intersect with C during the graph construction phase. A validation step to ensure that the leaves of a
clique all intersect with the same ball in C is also needed."
TREE ENSEMBLES,0.09311348205625607,Under review as a conference paper at ICLR 2022
PIECEWISE LIPSCHITZ FUNCTIONS,0.09408341416100872,"4.4
PIECEWISE LIPSCHITZ FUNCTIONS"
PIECEWISE LIPSCHITZ FUNCTIONS,0.0950533462657614,"We saw the beneﬁts of having speciﬁc (deterministic) interpretable functions as well as their exten-
sions in the context of safety. Now consider a richer class of functions that may also be randomized
with ﬁnite variance. In this case let f and f0 denote the mean values of the learned and refer-
ence functions respectively. We consider the case where each function is either interpretable or
black box, where the latter implies that query access is the only realistic way of probing the model.
This leads to three cases where either both functions are black box or interpretable, or one is black
box. What we care about in all these cases1 is to ﬁnd the maximum (and minimum) of a function
∆(x) = f(x) −f0(x). Let us consider ﬁnding only the maximum of ∆as the other case is sym-
metric. Given that f and f0 can be random functions ∆is also a random function and if ∆is black
box a standard way to optimize it is either using Bayesian Optimization (BO) (Auer, 2002) or tree
search type bandit methods (Bubeck et al., 2011; Carpentier & Valko, 2015).We repurpose some of
the results from this latter literature in our context showcasing the beneﬁt of interpretability from a
safety standpoint. To do this we ﬁrst deﬁne relevant terms.
Deﬁnition 1 (Simple Regret (Bubeck et al., 2011)). If f ∗
C denotes the optimal value of the function
f on the certiﬁcation set C, then the simple regret rC
q after querying the f function q times and
obtaining a solution xq is given by, rC
q (f) = f ∗
C −f(xq).
Deﬁnition 2 (Order β c-Lipschitz). Given a (normalized) metric ℓa function f is c-Lipschitz con-
tinuous of order β > 0 if for any two inputs x, y and for c > 0 we have, |f(x)−f(y)| ≤c·ℓ(x, y)β.
Deﬁnition 3 (Near optimality dimension (Bubeck et al., 2011)). If N(C, ℓ, ϵ) is the maximum num-
ber of ϵ radius balls one can ﬁt in C given the metric ℓand Cϵ = {x ∈C|f(x) ≥f ∗
C −ϵ}, then for"
PIECEWISE LIPSCHITZ FUNCTIONS,0.09602327837051407,"c > 0 the c-near optimality dimension is given by, υ = max

lim supϵ→0
ln N(Ccϵ,ℓ,ϵ)"
PIECEWISE LIPSCHITZ FUNCTIONS,0.09699321047526673,"ln(ϵ−1)
, 0

."
PIECEWISE LIPSCHITZ FUNCTIONS,0.0979631425800194,"Intuitively, simple regret measures the deviation between our current best and the optimal solution.
The Lipschitz condition bounds the rate of change of the function. Near optimality dimension
measures the set size for which the function has close to optimal values. The lower the value of υ,
the easier it is to ﬁnd the optimum. We now deﬁne what it means to have an interpretable function.
Assumption 3 (Characterizing an Interpretable Function). If a function f is interpretable, then we
can (easily) ﬁnd 1 ≤m ≪n partitions {C(1), ..., C(m)} of the certiﬁcation set C such that the
function f (i) = {f(x)|x ∈C(i)} ∀i ∈{1, ..., m} in each partition is c-Lipschitz of order β."
PIECEWISE LIPSCHITZ FUNCTIONS,0.09893307468477207,"Note that the (interpretable) function overall does not have to be c-Lipschitz of bounded order, rather
only in the partitions. This assumption is motivated by observing different interpretable functions.
For example, in the case of decision trees the m partitions could be its leaves, where typically the
function is a constant in each leaf (c = 0). For rule lists as well a ﬁxed prediction is usually made
by each rule. For a linear function one could consider the entire input space (i.e. m = 1), where for
bounded slope α the function would also satisfy our assumption (c = α and β = 1). Examples of
models that are not piecewise constant or globally Lipschitz are oblique decision trees (Murthy et
al., 1994), regression trees with linear functions in the leaves, and functional trees. Moreover, m is
likely to be small so that the overall model is interpretable (viz. shallow trees or small rules). With
the above deﬁnitions and Assumption 3 we now provide the simple regret for the function ∆."
PIECEWISE LIPSCHITZ FUNCTIONS,0.09990300678952474,"1. Both black box models: If both f and f0 are black box then it seems no gains could be made in
estimating the maximum of ∆over standard results in bandit literature. Hence, using Hierarchical
Optimistic Optimization (HOO) with assumptions such as C being compact and ∆being weakly
Lipschitz (Bubeck et al., 2011) with near optimality dimension υ the simple regret after q queries is:"
PIECEWISE LIPSCHITZ FUNCTIONS,0.1008729388942774,"rC
q (∆) ≤O"
PIECEWISE LIPSCHITZ FUNCTIONS,0.10184287099903007,ln(q) q
PIECEWISE LIPSCHITZ FUNCTIONS,0.10281280310378274,"
1
υ+2 ! (13)"
PIECEWISE LIPSCHITZ FUNCTIONS,0.1037827352085354,"2. Both interpretable models: If both f and f0 are interpretable, then for each function based
on Assumption 3 we can ﬁnd m1 and m0 partitions of C respectively where the functions are c1
and c0-Lipschitz of order β1 and β0 respectively. Now if we take non-empty intersections of these
partitions where we could have a maximum of m1m0 partitions, the function ∆in these partitions
would be c = 2 max(c0, c1)-Lipschitz of order β = min(β0, β1) as stated next (proof in appendix)."
PIECEWISE LIPSCHITZ FUNCTIONS,0.10475266731328807,"1For simplicity assume D(., .) to be the identity function."
PIECEWISE LIPSCHITZ FUNCTIONS,0.10572259941804074,Under review as a conference paper at ICLR 2022
PIECEWISE LIPSCHITZ FUNCTIONS,0.1066925315227934,"Proposition 3. If functions h0 and h1 are c0 and c1 Lipschitz of order β0 and β1 respectively, then
the function h = h0 −h1 is c-Lipschtiz of order β, where c = 2 max(c0, c1) and β = min(β0, β1)."
PIECEWISE LIPSCHITZ FUNCTIONS,0.10766246362754607,"Given that ∆is smooth in these partitions with underestimated smoothness of order β, the simple
regret after qi queries in the ith partition C(i) with near optimality dimension υi based on HOO"
PIECEWISE LIPSCHITZ FUNCTIONS,0.10863239573229874,"is: rC(i)
qi (∆) ≤O  1 q"
PIECEWISE LIPSCHITZ FUNCTIONS,0.1096023278370514,"1
υi+2
i !"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11057225994180407,", where υi ≤
d
β . If we divide the overall query budget q across the"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11154219204655674,"π ≤m0m1 non-empty partitions equally, then the bound will be scaled by π"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11251212415130941,"1
υi+2 when expressed
as a function of q. Moreover, the regret for the entire C can then be bounded by the maximum regret
across these partitions leading to the following result:"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11348205625606207,"rC
q (∆) ≤O π q"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11445198836081474,"
β
d+2β ! (14)"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11542192046556742,"Notice that for a model to be interpretable m0 and m1 are likely to be small (i.e. shallow trees or
small rule lists or linear model where m = 1) leading to a “smallish” π and υ can be much >> d"
PIECEWISE LIPSCHITZ FUNCTIONS,0.11639185257032007,"β in
case 1. Hence, interpretability signiﬁcantly reduces the regret in estimating the maximum deviation."
PIECEWISE LIPSCHITZ FUNCTIONS,0.11736178467507274,"3. Black box and interpretable model: Making no further assumptions on the black box model
and assuming ∆satisﬁes properties mentioned in case 1, the simple regret has the same behavior as
equation 13. This is expected as the black box model could be highly non-smooth."
EXPERIMENTS,0.11833171677982542,"5
EXPERIMENTS"
EXPERIMENTS,0.11930164888457807,"We present a case study on the UCI Adult Income dataset (Dua & Graff, 2017), using its given par-
tition into training and test sets. Additional datasets are presented in Appendix D. For the reference
model f0, an 8-leaf decision tree (DT) is ﬁt (using scikit-learn’s max leaf nodes parameter) on
the training data. This DT has 85.0% accuracy on the test set and is easy for a human to understand
and validate (see Figure 3). We take the deviation function D to be the absolute difference between
predicted probabilities of the positive (high-income) class. For the certiﬁcation set C, we consider
a union of ℓ∞balls (2) centered at test set instances (n = 16281). The ℓ∞norm is computed on
normalized feature values, where continuous features are standardized and categorical features are
one-hot encoded. The case r = 0 yields a ﬁnite set consisting only of the test set, while r →∞
corresponds to C being the entire domain X."
EXPERIMENTS,0.12027158098933075,"5.1
TREES, LINEAR AND ADDITIVE MODELS"
EXPERIMENTS,0.12124151309408342,"We ﬁrst consider models for which the maximum deviation can be computed exactly: decision trees,
generalized linear and additive models. For the ﬁrst two classes, we use scikit-learn (Pedregosa
et al., 2011) to train DTs of varying numbers of leaves (max leaf nodes) and logistic regression
(LR) models with varying amounts of ℓ1 regularization. For additive models, we use Explainable
Boosting Machines (EBM) from the InterpretML package (Nori et al., 2019) with zero interaction
terms. Smoothness is controlled by the max bins parameter, the number of discretization bins for
continuous features. Statistics and plots of the resulting models can be found in Appendix D.1."
EXPERIMENTS,0.12221144519883609,"0.0
0.5
1.0
 ball radius r 0.90 0.92 0.94 0.96 0.98 1.00"
EXPERIMENTS,0.12318137730358875,max deviation D
EXPERIMENTS,0.12415130940834142,"DT, leaves=50
LR, 1 norm=7.4
GAM, max_bins=8"
EXPERIMENTS,0.1251212415130941,"101
102
103"
EXPERIMENTS,0.12609117361784675,Number of leaf nodes 0.2 0.4 0.6 0.8 1.0
EXPERIMENTS,0.1270611057225994,max deviation D
EXPERIMENTS,0.1280310378273521,Decision Tree
EXPERIMENTS,0.12900096993210475,"0
20
40
60
80"
NORM,0.12997090203685743,1 norm 0.86 0.88 0.90 0.92 0.94 0.96
NORM,0.1309408341416101,max deviation D
NORM,0.13191076624636275,"LR, r = 0.2"
NORM,0.13288069835111543,"101
102
103"
NORM,0.1338506304558681,max_bins 0.965 0.970 0.975 0.980 0.985 0.990
NORM,0.13482056256062075,max deviation D
NORM,0.13579049466537343,"GAM, r = 0.2"
NORM,0.1367604267701261,"Figure 1: Maximum deviation D for the three models as a function of certiﬁcation set size (radius r,
left panel) and model smoothness (number of leaves for DT, ℓ1 norm for LR, max bins for GAM)."
NORM,0.13773035887487875,"To evaluate the maximum deviation for DTs, Algorithm 1 is used on a bipartite graph. For the latter
two cases where f is generalized additive, f0 is a DT, and radius r > 0, we use (7), (6) (r = 0 is"
NORM,0.13870029097963144,Under review as a conference paper at ICLR 2022
NORM,0.1396702230843841,"handled simply by evaluating the models on the test set). For r < ∞when C is a union of balls, we
maximize separately over each intersection between a ball and a leaf of f0, and take their maximum."
NORM,0.14064015518913675,"Figure 1 shows a summary of the maximum deviation D as a function of certiﬁcation set radius
r and each model’s smoothness parameter; Appendix D.1 has breakdowns by leaves of f0. The
ﬁrst observation is that the deviation can be large even between simple models, namely the 8-leaf
reference DT f0, a 50-leaf DT f, a sparse LR model (16 nonzeros, ℓ1 norm = 7.4), and a smooth
GAM (see Figure 5 for plots of fj). At the same time, the interpretability of these models allows the
deviation to be computed for a range of inﬁnite certiﬁcation sets with r > 0. We can thus quantify
the increase in deviation as r increases, as seen in the left of Figure 1. For LR and GAM, r only
has to be slightly larger than 1 for the deviation to equal that for r = ∞(C = X, dashed lines in
ﬁgure), whereas for DT, the deviation at r = 0 is already maximal. The right panels show that the
deviation also increases as model smoothness decreases (number of leaves for DT, ℓ1 norm for LR,
max bins for GAM). The initial increase is rapid for DT and LR. Appendix D.1 reports on the
complexity of computing D (running time, number of cliques evaluated)."
NORM,0.14161008729388944,"Studying the solutions that maximize deviation (the arg max) can bring attention to unanticipated
extremes in model output, which may rise to the level of safety risks. Appendix D.1 discusses in
detail some maximizing solutions corresponding to Figure 1. A common source of large deviations is
the extrapolation of linear and monotonic functions (more generally, unbounded or poorly bounded
functions) to extreme points of the certiﬁcation set, or even parts thereof. At these extreme points,
the model f and reference f0 are in conﬂict, which may be reason for caution or call for domain
expert intervention. In the GAM case, the arg max also highlights the presence of an unintuitive
extreme value in one of the functions fj, which may be an artifact warranting further investigation."
TREE ENSEMBLES,0.1425800193986421,"5.2
TREE ENSEMBLES"
TREE ENSEMBLES,0.14354995150339475,"0
100
200
300
400
500
600
Number of estimators in Random Forest 0.6 0.7 0.8 0.9 1.0"
TREE ENSEMBLES,0.14451988360814744,D w.r.t. single Decision Tree
TREE ENSEMBLES,0.1454898157129001,"dual bound
primal bound"
TREE ENSEMBLES,0.14645974781765275,(a) D w.r.t. number of estimators
TREE ENSEMBLES,0.14742967992240544,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
ball radius r 0.95 0.96 0.97 0.98 0.99"
TREE ENSEMBLES,0.1483996120271581,Best dual bound on D
TREE ENSEMBLES,0.14936954413191075,"RF-5 estimators
RF-10 estimators"
TREE ENSEMBLES,0.15033947623666344,(b) D w.r.t. size of certiﬁcation set C
TREE ENSEMBLES,0.1513094083414161,"Figure 2: Maximum deviation of f, a
Random Forest, trained on the Adult In-
come dataset."
TREE ENSEMBLES,0.15227934044616878,"Next, we demonstrate the methods in Section 4.3 on a
Random Forest model, where the number of estimators
is varied. A time limit of two hours is imposed for the
search procedure for each run. There are two sets of ex-
periments, one where the set C is assumed to be the entire
domain, and another where C is a union of balls."
TREE ENSEMBLES,0.15324927255092144,"In Figure 2a we observe that max deviations can be large
even for simpler models with fewer estimators. The upper
bound on max deviation reduces as the number of estima-
tors increases. The larger number of estimators increases
averaging and may serve to make the model smoother.
The primal lower bound here is the maximum D for com-
plete K + 1-max-cliques. Due to the size of underlying
graphs, fewer K + 1-max-cliques are evaluated and the
lower bound is weak. For safety evaluation, i.e. worst
case behavior, the lower bound is of less consequence."
TREE ENSEMBLES,0.1542192046556741,"For the same setting, we now consider the case where C
is a union of balls of the test set and the radius r is varied.
Figure 2b shows the upper bound on safety as a function
of the certiﬁcation set size for two RF models. As the test
set is large in this case, the deviations observed even for
small values of r are high and grow to reach the value of
the full feature space quickly."
CONCLUSION,0.15518913676042678,"6
CONCLUSION"
CONCLUSION,0.15615906886517944,"We have considered the relationship between interpretability and safety in supervised learning
through two main contributions: First, the proposal of maximum deviation as a means toward as-
sessing safety, and second, discussion of approaches to computing maximum deviation and how
these are simpliﬁed by interpretable model structure. We believe that there is much more to explore
in this relationship. Appendix C provides further discussion of several topics."
CONCLUSION,0.1571290009699321,Under review as a conference paper at ICLR 2022
CONCLUSION,0.15809893307468478,ETHICS DISCUSSION
CONCLUSION,0.15906886517943744,"The ICLR Code of Ethics requires contributions to avoid harm and contribute to society. The safety
of machine learning systems is a fundamental factor in achieving these goals, and has been called
out by the European Commission’s regulatory framework (High Level Expert Group on Artiﬁcial
Intelligence, 2020). The commission states seven key dimensions to be evaluated and audited by
a cross-disciplinary team: (i) human agency and oversight, (ii) technical robustness and safety,
(iii) privacy and data governance, (iv) transparency, (v) diversity, non-discrimination and fairness,
(vi) environmental and societal well-being, and (vii) accountability. The second of these dimensions
is safety. However, Sloane et al. (2021) argue that algorithmic audits are ill-deﬁned as the underlying
deﬁnitions are vague. The proposed work helps ﬁll that ill-deﬁnedness using a quantitative approach.
One may argue against this particular choice of quantiﬁcation, but it does start the community down
the path toward being more concrete in its deﬁnitions."
CONCLUSION,0.1600387972841901,"As with many other technologies, the proposed approach may be misused. For example, the refer-
ence model may be chosen in a way that hides the safety concerns of the model being evaluated.
Transparent documentation and reporting with provenance guarantees can help avoid this kind of
purposeful deceit (Arnold et al., 2019)."
CONCLUSION,0.16100872938894278,REPRODUCIBILITY DISCUSSION
CONCLUSION,0.16197866149369544,"To promote reproducibility, all algorithms and mathematical derivations in the paper are presented
with a large amount of detail in the main body and appendix. All datasets used in the experiments
are publicly-available. All parameters and their settings are fully described."
REFERENCES,0.1629485935984481,REFERENCES
REFERENCES,0.16391852570320078,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-
crete problems in AI safety. arXiv:1606.06565, 2016."
REFERENCES,0.16488845780795344,"Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin. Learning
certiﬁably optimal rule lists for categorical data. Journal of Machine Learning Research, 18(234):
1–78, 2018. URL http://jmlr.org/papers/v18/17-716.html."
REFERENCES,0.1658583899127061,"Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra
Mojsilovi´c, Ravi Nair, Karthikeyan Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski,
Darrell Reimer, John Richards, Jason Tsay, and Kush R. Varshney. FactSheets: Increasing trust
in AI services through supplier’s declarations of conformity. IBM Journal of Research and De-
velopment, 63(4/5):6, 2019."
REFERENCES,0.16682832201745879,"Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3, 2002."
REFERENCES,0.16779825412221144,"Peter L. Bartlett and Marten H. Wegkamp. Classiﬁcation with a reject option using a hinge loss.
Journal of Machine Learning Research, 9(59):1823–1840, 2008. URL http://jmlr.org/
papers/v9/bartlett08a.html."
REFERENCES,0.1687681862269641,"Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriﬁable reinforcement learning via policy
extraction. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.1697381183317168,"Sebastien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of
Machine Learning Research, 12, 2011."
REFERENCES,0.17070805043646944,"Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceed-
ings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2006."
REFERENCES,0.1716779825412221,"Alexandra Carpentier and Michal Valko. Simple regret for inﬁnitely many armed bandits. In Inter-
national Conference on Machine Learning. PMLR, 2015."
REFERENCES,0.1726479146459748,Under review as a conference paper at ICLR 2022
REFERENCES,0.17361784675072744,"Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning, and Cho-Jui Hsieh. Robustness veriﬁca-
tion of tree-based models. Advances in Neural Information Processing Systems, 32:12317–12328,
2019."
REFERENCES,0.17458777885548013,"William W. Cohen and Yoram Singer. A simple, fast, and effective rule learner. In Proc. Conf. Artif.
Intell. (AAAI), pp. 335–342, 1999."
REFERENCES,0.1755577109602328,"Krzysztof Dembczy´nski, Wojciech Kotłowski, and Roman Słowi´nski. ENDER: a statistical frame-
work for boosting decision rules. Data Mining and Knowledge Discovery, 21(1):52–90, Jul 2010."
REFERENCES,0.17652764306498545,"Laurens Devos, Wannes Meert, and Jesse Davis. Versatile veriﬁcation of tree ensembles. In Inter-
national Conference on Machine Learning, pp. 2654–2664. PMLR, 2021."
REFERENCES,0.17749757516973813,"Amit Dhurandhar, Vijay Iyengar, Ronny Luss, and Karthikeyan Shanmugam. TIP: Typifying the
interpretability of procedures. arXiv:1706.02952, 2017."
REFERENCES,0.1784675072744908,"Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shan-
mugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations
with pertinent negatives. In Advances in Neural Information Processing Systems, pp. 592–603,
2018."
REFERENCES,0.17943743937924345,"Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017."
REFERENCES,0.18040737148399613,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.1813773035887488,"Jerome H. Friedman and Bogdan E. Popescu. Predictive learning via rule ensembles. Annals of
Applied Statistics, 2(3):916–954, Jul 2008."
REFERENCES,0.18234723569350145,"Javier Garc´ıa, Fern, and o Fern´andez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(42):1437–1480, 2015. URL http://jmlr.org/
papers/v16/garcia15a.html."
REFERENCES,0.18331716779825413,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal.
Explaining explanations: An overview of interpretability of machine learning. In Proc. IEEE Int.
Conf. Data Sci. Adv. Anal., pp. 80–89, 2018."
REFERENCES,0.1842870999030068,"High
Level
Expert
Group
on
Artiﬁcial
Intelligence.
Assessment
list
for
trust-
worthy
AI
for
self
assessment.
Technical
report,
European
Commission,
2020.
URL
https://digital-strategy.ec.europa.eu/en/library/
assessment-list-trustworthy-artificial-intelligence-altai-self-assessment."
REFERENCES,0.18525703200775945,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distilling the knowledge in a neural network.
arXiv:1503.02531, 2015."
REFERENCES,0.18622696411251213,"Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min
Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Veriﬁca-
tion, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37
(100270), 2020."
REFERENCES,0.1871968962172648,"Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, and Armando Solar-Lezama. Synthesizing
programmatic policies that inductively generalize. In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/forum?id=S1l8oANFDH."
REFERENCES,0.18816682832201745,"Alex Kantchelian, J Doug Tygar, and Anthony Joseph. Evasion and hardening of tree ensemble
classiﬁers. In International Conference on Machine Learning, pp. 2387–2396. PMLR, 2016."
REFERENCES,0.18913676042677013,"Roger Koenker. Quantile Regression. Cambridge University Press, 2005. ISBN 978-0-521-60827-5."
REFERENCES,0.1901066925315228,"Gert R. G. Lanckriet, Laurent El Ghaoui, Chiranjib Bhattacharyya, and Michael I. Jordan. A robust
minimax approach to classiﬁcation. J. Mach. Learn. Res., 3:555–582, December 2002."
REFERENCES,0.19107662463627545,"Scott M. Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In Advances
in Neural Information Processing Systems, pp. 4765–4774, 2017."
REFERENCES,0.19204655674102813,Under review as a conference paper at ICLR 2022
REFERENCES,0.1930164888457808,"Ronny Luss, Pin-Yu Chen, Amit Dhurandhar, Prasanna Sattigeri, Karthik Shanmugam, and Chun-
Chen Tu. Leveraging latent features for local explanations. In ACM KDD, 2021."
REFERENCES,0.19398642095053345,"Nicolai Meinshausen. Quantile regression forests. Journal of Machine Learning Research, 7(6):
983–999, 2006."
REFERENCES,0.19495635305528614,"Mohammad Mirghorbani and P Krokhmal. On ﬁnding k-cliques in k-partite graphs. Optimization
Letters, 7(6):1155–1165, 2013."
REFERENCES,0.1959262851600388,"Sina Mohseni, Zhiding Yu, Chaowei Xiao, Jay Yadawa, Haotao Wang, and Zhangyang Wang. Prac-
tical machine learning safety: A survey and primer. arXiv:2106.04823, 2021."
REFERENCES,0.19689621726479145,"Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. Interpretml: A uniﬁed framework for
machine learning interpretability, 2019."
REFERENCES,0.19786614936954414,"Clemens Otte. Safe and interpretable machine learning: A methodological review. In Computational
Intelligence in Intelligent Data Analysis, pp. 111–122. 2013."
REFERENCES,0.1988360814742968,"Axel Parmentier and Thibaut Vidal. Optimal counterfactual explanations in tree ensembles. In
International Conference on Machine Learning, pp. 8422–8431. PMLR, 2021."
REFERENCES,0.19980601357904948,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011."
REFERENCES,0.20077594568380214,"G´abor Petneh´azi. QCNN: quantile convolutional neural network. CoRR, abs/1908.07978, 2019.
URL http://arxiv.org/abs/1908.07978."
REFERENCES,0.2017458777885548,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-
ples. arXiv preprint arXiv:1801.09344, 2018."
REFERENCES,0.20271580989330748,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining
the predictions of any classiﬁer. In Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 1135–1144, 2016."
REFERENCES,0.20368574199806014,"Ulrich R¨uckert and Stefan Kramer. A statistical approach to rule learning. In Proc. Int. Conf. Mach.
Learn. (ICML), pp. 785–792, 2006."
REFERENCES,0.2046556741028128,"Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and
use interpretable models instead. Nature Mach. Intell., 1(5):206–215, May 2019."
REFERENCES,0.20562560620756548,"Christian Rupprecht, Cyril Ibrahim, and Christopher J. Pal. Finding and visualizing weaknesses of
deep reinforcement learning agents. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=rylvYaNYDH."
REFERENCES,0.20659553831231814,"Mona Sloane, Emanuel Moss, and Rumman Chowdhury. A silicon valley love triangle: Hiring
algorithms, pseudo-science, and the quest for auditability. arXix, 2021."
REFERENCES,0.2075654704170708,"Philip S Thomas, Bruno Castro da Silva, Andrew G Barto, Stephen Giguere, Yuriy Brun, and Emma
Brunskill. Preventing undesirable behavior of intelligent machines. Science, 366(6468):999–
1004, 2019."
REFERENCES,0.20853540252182348,"Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. Interpretable
to whom? A role-based model for analyzing interpretable machine learning systems. In Proc.
ICML Workshop Human Interpret. Mach. Learn., pp. 8–14, 2018."
REFERENCES,0.20950533462657614,"Kush R. Varshney. Trustworthy machine learning and artiﬁcial intelligence. ACM XRDS Mag., 25
(3):26–29, Spring 2019."
REFERENCES,0.2104752667313288,"Kush R. Varshney and Homa Alemzadeh. On the safety of machine learning: Cyber-physical sys-
tems, decision sciences, and data products. Big Data, 5(3):246–255, September 2017."
REFERENCES,0.21144519883608148,Under review as a conference paper at ICLR 2022
REFERENCES,0.21241513094083414,"Dennis Wei, Sanjeeb Dash, Tian Gao, and Oktay G¨unl¨uk.
Generalized linear rule models.
In
Proceedings of the 36th International Conference on Machine Learning (ICML), pp. 6687–6696,
09–15 Jun 2019. URL https://proceedings.mlr.press/v97/wei19a.html."
REFERENCES,0.2133850630455868,"Adrian Weller. Transparency: Motivations and challenges. In Explainable AI: Interpreting, Explain-
ing and Visualizing Deep Learning, pp. 23–40. 2019."
REFERENCES,0.21435499515033948,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR,
2018."
REFERENCES,0.21532492725509214,"Hongyu Yang, Cynthia Rudin, and Margo Seltzer. Scalable Bayesian rule lists. In Proceedings of
the 34th International Conference on Machine Learning (ICML), pp. 3921–3930, 2017."
REFERENCES,0.2162948593598448,"He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. An inductive synthesis framework
for veriﬁable reinforcement learning. In Proceedings of the 40th ACM SIGPLAN Conference on
Programming Language Design and Implementation, pp. 686––701, Phoenix, AZ, USA, 2019."
REFERENCES,0.21726479146459748,Under review as a conference paper at ICLR 2022
REFERENCES,0.21823472356935014,"A
ADDITIONAL PROBLEM FORMULATION DETAILS"
REFERENCES,0.2192046556741028,"Models that abstain
The formulation in Section 2 can also accommodate models that abstain from
predicting (and possibly defer to a human expert or other fallback system). If f(x) = ∅, representing
abstention, then we may set D(∅, y0) = d for any y0 ∈Y, where d > 0 is an intermediate value less
than the maximum value that D can take (Bartlett & Wegkamp, 2008). The value d might also be
less than a “typically bad” value for D, to reward the model for abstaining when it is uncertain."
REFERENCES,0.22017458777885549,"B
ADDITIONAL DETAILS ON DEVIATION MAXIMIZATION FOR SPECIFIC
MODEL CLASSES"
REFERENCES,0.22114451988360814,"B.1
TREES"
REFERENCES,0.22211445198836083,"Rule lists
A rule list is a nested sequence of IF-THEN-ELSE statements, where the IF condition
usually involves a single feature and the THEN consequent is an output value. As discussed in Yang
et al. (2017); Angelino et al. (2018), such rule lists are one-sided trees and are thus subsumed in the
discussion of Section 4.1. The number of leaves in the equivalent tree is equal to the number of rules
in the list (including the last default rule)."
REFERENCES,0.22308438409311349,"Intersection of C with a Cartesian product
If C = Qd
j=1 Cj is also a Cartesian product, then de-
termining whether the intersection is non-empty amounts to checking whether all of the coordinate-
wise intersections with Cj, j = 1, . . . , d, are non-empty. If C is not a Cartesian product but is a union
of ℓ∞balls (which are Cartesian products), then the intersection is non-empty if the intersection with
any one ball is non-empty."
REFERENCES,0.22405431619786614,"Additive reference model
For the case where f is a decision tree and f0 is a generalized additive
model, if the deviation function is symmetric, D(y, y0) = D(y0, y), then this case is covered in
Section 4.2."
REFERENCES,0.22502424830261883,"B.2
LINEAR AND ADDITIVE MODELS"
REFERENCES,0.2259941804073715,"Categorical features
A function fj(xj) of a categorical feature xj can be represented in two
ways, depending on whether f is a GAM or a GLM. In the GAM case, we may use the native
representation in which xj takes values in a ﬁnite set Xj of categories. In the GLM case, xj is one-
hot encoded into multiple binary-valued features xjk, one for each category k. Then any function
fj can be represented as a linear function,"
REFERENCES,0.22696411251212414,fj(xj) =
REFERENCES,0.22793404461687683,"|Xj|
X"
REFERENCES,0.2289039767216295,"k=1
wjkxjk,"
REFERENCES,0.22987390882638215,where wjk is the value of fj for category k.
REFERENCES,0.23084384093113483,"Implication of Assumption 1
The second condition implies that the deviation increases or stays
the same as y moves away from y0 in either direction."
REFERENCES,0.2318137730358875,"Proof of Proposition 2. Let x ∈S and S(x) = Pd
j=1 fj(xj). Under Assumption 1.1, if S(x) =
g(y0), then"
REFERENCES,0.23278370514064015,"D(f(x), f0(x)) = D(g−1(g(y0)), y0) = D(y0, y0) = 0."
REFERENCES,0.23375363724539283,"As S(x) increases from g(y0), f(x) also increases because g−1 is an increasing function, and
D(f(x), y0) increases or stays the same due to Assumption 1.2. Similarly, as S(x) decreases from
g(y0), f(x) decreases, and D(f(x), y0) again increases or stays the same. It follows that to max-
imize D(f(x), y0), it sufﬁces to separately maximize and minimize S(x), compute the resulting
values of D(f(x), y0), and take the larger of the two. This yields the result."
REFERENCES,0.2347235693501455,Under review as a conference paper at ICLR 2022
REFERENCES,0.23569350145489815,"Implication of Assumption 2 and identity link function g
These two assumptions imply that the
deviation is measured on the difference between f and f0 in the space in which they are additive.
For example, if f and f0 are logistic regression models predicting the probability of belonging to
one of the classes, the difference is taken in the log-odds (logit) domain. It is left to future work to
determine other assumptions under which problem (1) is tractable when f and f0 are both additive."
REFERENCES,0.23666343355965083,"Cartesian product C implies Cartesian product S
In the cases of constant and additive f0,
S = C. In the decision tree case, since each leaf is a Cartesian product L0m = Qd
j=1 Rmj, the"
REFERENCES,0.2376333656644035,"intersections S = L0m ∩C are also Cartesian products Qd
j=1 Sj where Sj = Rmj ∩Cj."
REFERENCES,0.23860329776915615,"One-dimensional optimization complexities Cj
For discrete-valued xj, Cj is proportional to the
number of allowed values |Sj|. For continuous xj, it is common to use spline functions or tree
ensembles as fj in constructing GAMs. In the former case, Cj is proportional to the number of
knots. In the latter, the tree ensemble can be converted to a piecewise constant function and Cj is
then proportional to the number of pieces. Lastly in the case where fj(xj) = wjxj is linear and
Sj = [Xj, Xj] is an interval, Cj = O(1) because it sufﬁces to evaluate the two endpoints."
REFERENCES,0.23957322987390883,"Convex S
If C is a convex set, then in the cases of constant and additive f0, S = C is also
convex. In the case of tree-structured f0, S = L0m ∩C and each leaf L0m can be represented as
a convex set, with interval constraints on continuous features and set membership constraints on
categorical features. The latter can be represented as xjk = 0 constraints on the one-hot encoding
(see “Categorical features” paragraph above) for non-allowed categories k. Hence S is also convex."
REFERENCES,0.2405431619786615,"As a speciﬁc example, suppose that S is the product of independent constraints on each categorical
feature and an ℓp norm constraint on the continuous features jointly. The maximization over each
categorical feature has complexity Cj = |Sj| as noted above, while the maximization of wT x over
continuous features lying in an ℓp ball has closed-form solutions for the common cases p = 1, 2, ∞."
REFERENCES,0.24151309408341415,"B.3
TREE ENSEMBLES"
REFERENCES,0.24248302618816683,"The full algorithm for clique search from Section 4.3 is presented in Algorithm 1. It uses Z as a
node compatibility vector to keep track of valid leaves and B a set of trees/partites not yet covered
by the maximum clique. The algorithm starts with and empty clique S and anytime bounds as 0. It
starts the search with the smallest tree to limit the search space. This is typically f0. Each leaf is
added to the intermediate clique S in turn (Line 6). A stronger primal bound can be achieved if the
traversal is ordered in a meaningful way. In particular, starting with nodes with the highest heuristic
function value H(S) aids the algorithm to focus on better areas of the search space."
REFERENCES,0.2434529582929195,"If the size of the clique is K+1 the primal bound is updated. Otherwise, the dual bound is computed.
If the node is promising, the algorithm recurses to the next level. When the search is terminated at
any step, the maximum deviation is bounded by (Dlb, Dub)."
REFERENCES,0.24442289039767218,"Rule ensembles
Similar to the tree ensembles considered in Section 4.3, a rule ensemble is a
linear combination of conjunctive rules, where the antecedent is a conjunction of conditions on
individual features, and the consequent takes a real value if the antecedent is true and zero otherwise.
They are produced by algorithms such as SLIPPER (Cohen & Singer, 1999), that of R¨uckert &
Kramer (2006), RuleFit (Friedman & Popescu, 2008), ENDER (Dembczy´nski et al., 2010) and have
also been referred to as generalized linear rule models (Wei et al., 2019). A rule ensemble can be
converted into a tree ensemble by converting each conjunctive rule into an IF-THEN-ELSE rule list,
which is a one-sided tree (see Appendix B.1). Speciﬁcally, the conditions in the conjunction are
taken in any order, each condition is negated to become an IF condition, and the THEN consequents
are all output values of zero. The ﬁnal ELSE consequent, which is reached if all the IF conditions are
false (and hence the original rule holds), returns the output value of the original rule. The number
of leaves in the resulting tree equals the number of conditions in the conjunction plus one."
REFERENCES,0.24539282250242483,Under review as a conference paper at ICLR 2022
REFERENCES,0.2463627546071775,Algorithm 1 Max clique search for maximum deviation
REFERENCES,0.24733268671193018,"Require: M adjacency matrix, H heuristic function"
REFERENCES,0.24830261881668284,"1: Z[i] = 1∀i ∈V , B = {1, 2, . . . , K + 1}, S = ∅
▷All nodes valid, all trees uncovered
2: Q = ENUMERATE(Z, B, S)
3: Initialize:"
REFERENCES,0.2492725509214355,"Dlb = 0, Dub = 0
▷Anytime bounds
4: function ENUMERATE(Z, B, S)
5:
t = arg maxb{|Zb|
 b ∈B}
▷Uncovered tree with fewest valid nodes
6:
for i in Zt do
7:
Z[i] = 0
▷Mark node as incompatiable
8:
S = S ∪{i}
▷Add to candidate clique
9:
if |S| = K + 1 then:
▷Is it a max clique?
10:
Dlb = max (Dlb, D(S))
▷Update primal bound
11:
Q = Q ∪S
▷Add to set of max cliques
12:
S = S \ {i}
▷Backtrack
13:
else
14:
Zt+1 = Zt ∧M(i)
▷Update valid nodes
15:
B = B \ {t}
▷Update uncovered trees
16:
Dub = max (Dub, H(S))
▷Update dual bound
17:
if Dub > Dlb then:
▷Pruning by bound
18:
ENUMERATE(Zt+1, B, S)
▷Recurse to next level
19:
end if
20:
S = S \ {i}
▷Backtrack
21:
B = B ∪{t}
22:
end if
23:
end for
24: end function"
REFERENCES,0.2502424830261882,"B.4
PIECEWISE LIPSCHITZ FUNCTIONS"
REFERENCES,0.25121241513094084,"Proof of Proposition 3. Consider two inputs x and y then,"
REFERENCES,0.2521823472356935,|h(x) −h(y)| = |(h0 −h1)(x) −(h0 −h1)(y)| = |h0(x) −h0(y) + h1(y) −h1(x)|
REFERENCES,0.25315227934044615,"≤|h0(x) −h0(y)| + |h1(x) −h1(y)| ≤c0 · ℓ(x, y)β0 + c1 · ℓ(x, y)β1"
REFERENCES,0.2541222114451988,"≤c · ℓ(x, y)β"
REFERENCES,0.2550921435499515,"where, c = 2 max(c0, c1) and β = min(β0, β1)."
REFERENCES,0.2560620756547042,"Other choices for D(., .): The results assumed D(., .) to be the identity function, where ∆=
D(f0, f). This choice of function clearly satisﬁes assumptions 1 and 2. Again consistent with these
assumptions we look at some other choices for D(., .). If D(., .) were an afﬁne function with a
positive scaling such as D(y0, y) = α(y0 −y) + b where α > 0, then our result in equation 14
would be unchanged as only the Lipschitz constant of ∆would change, but not its (underestimated)
order. If the function were a polynomial or exponential however, no such guarantees can be made
and we would be back to case 1."
REFERENCES,0.25703200775945684,"B.5
POST HOC EXPLANATIONS FOR OTHER MODEL CLASSES"
REFERENCES,0.2580019398642095,"For model classes beyond the ones discussed in Section 4, it is less clear whether there ex-
ist reasonably tractable algorithms that guarantee exact computation of or bounds on the maxi-
mum deviation.
In this case, it is natural to ask whether post hoc explanations for the model
can help.
One way in which this could occur is if the post hoc explanation approximates the
model f by a simpler model ˆf and if the deviation function D satisﬁes the triangle inequality
D(f(x), f0(x)) ≤D(f(x), ˆf(x)) + D( ˆf(x), f0(x)). Then the maximum deviation in (1) would
be bounded as"
REFERENCES,0.25897187196896215,"max
x∈C D(f(x), f0(x)) ≤max
x∈C D(f(x), ˆf(x)) + max
x∈C D( ˆf(x), f0(x)).
(15)"
REFERENCES,0.25994180407371487,Under review as a conference paper at ICLR 2022
REFERENCES,0.2609117361784675,"While we may choose ˆf to be interpretable so that the rightmost maximization is tractable, the
middle maximization asks for a uniform bound on the deviation between f and ˆf, i.e, the ﬁdelity of
ˆf. We are not aware of a post hoc explanation method that provides such a guarantee. Indeed, in
general, the middle maximization might not be any easier than the left-hand one that we set out to
bound."
REFERENCES,0.2618816682832202,"A (practical) possibility may be to perform quantile regression (Koenker, 2005) for a large enough
quantile to learn ˆf, as opposed to minimizing expected error as is typically done. This may be
an interesting direction to explore in the future as quantile regression algorithms are available for
varied model classes including linear models, tree ensembles (Meinshausen, 2006) and even neural
networks (Petneh´azi, 2019). More investigation is needed into whether quantile regression methods
can provide approximate guarantees on the middle term in (15)."
REFERENCES,0.26285160038797284,"C
FURTHER DISCUSSION"
REFERENCES,0.2638215324927255,"Worst-case approach
The formulation of (1) as the worst case over a certiﬁcation set represents
a deliberate choice to depend as little as possible on a probability distribution or a dataset sampled
from one. As stated in Section 2, Certiﬁcation Set paragraph, C can depend at most on (an expanded
version of) the support set of a distribution. The reason for this choice is because safety is an out-
of-distribution notion: harmful outputs often arise precisely because they were not anticipated in the
data. The trade-off inherent in this choice is that the maximum deviation may be more conservative
than needed. The high maximum deviation values in e.g. Figures 1 and 2 may reﬂect this. Given
deﬁnition (1) as a starting point in this paper, future work could consider variations that depend more
on a distribution and are thus less conservative, but may also offer a weaker safety guarantee."
REFERENCES,0.26479146459747815,"Choice of reference model
The proposed deﬁnition of maximum deviation (1) depends on the
choice of reference model f0. Different choices will lead to different deviation values and, perhaps
more importantly, different combinations of features that maximize the deviation. We have discussed
possible choices in Section 2, and the results in Section 4 indicate that, as with the assessed model f,
interpretable forms for f0 can ease the computation of maximum deviation. Beyond these guidelines,
it is up to ML practitioners and domain experts to decide on appropriate reference models for their
application (and there may be beneﬁt to considering more than one). We mention an additional
concern with the reference model in the Ethics Discussion."
REFERENCES,0.26576139670223087,"For some real applications it may be difﬁcult to come up with a globally interpretable reference
model. But speciﬁc to particular scenarios it may be possible. For instance, it might be difﬁcult to
provide general rules for how to drive a car, but in speciﬁc scenarios such as there being an obstacle
in front, one can suggest that you stop or turn, which is a simple rule. So our machinery could
potentially be applied at a local level where the reference model is interpretable in that locality.
This might help in “spot checking” a deployed model and estimating its safety by computing these
maximum deviations in scrupulously selected (challenging) scenarios."
REFERENCES,0.2667313288069835,"Impossible inputs in certiﬁcation set
Mathematically simple sets such as Cartesian products and
ℓp balls permit simpler algorithms for optimizing functions over them. Accordingly, these sets
have been the focus of not only the present work but also the related literature on ML veriﬁcation
and adversarial robustness. However, they may not serve to exclude inputs that are physically or
logically impossible from the certiﬁcation set C, and thus, the resulting maximum deviation values
may be too large and conservative. Here it is important to distinguish between impossible inputs and
those that are merely implausible (i.e., with low probability). Techniques for capturing implausibility
have been proposed for contrastive/counterfactual explanations (Dhurandhar et al., 2018; Luss et al.,
2021), whereas we expect the set of impossible inputs to be smaller and more constrained. As a
simple example from the Adult Income dataset, if we agree that a wife/husband is deﬁned to be of
female/male gender (regardless of the gender of the spouse), then the cross combinations male-wife
and female-husband cannot occur. Future work can consider the representation and handling of such
constraints."
REFERENCES,0.2677012609117362,"White-box vs. grey-box models
In this paper, we have assumed full “white-box” access to both
f and f0, namely complete knowledge of their structure and parameters. Interesting questions may"
REFERENCES,0.26867119301648884,Under review as a conference paper at ICLR 2022
REFERENCES,0.2696411251212415,"arise when this assumption is relaxed to different “grey-box” possibilities. For example, one could
further investigate the third case in Section 4.4, where one of f, f0 is black-box and the other is
white-box interpretable. There may exist assumptions that we have not identiﬁed that would improve
the query complexity compared to generic black-box optimization."
REFERENCES,0.27061105722599416,"Other interpretability-safety relationships
This paper has focused on one relationship between
the interpretability of a model and the safety of its outputs. It has not addressed other ways in which
interpretability/explainability can affect the risk of a model (in the plain English sense, not the
expectation of a loss function). For example, in regulated industries such as consumer ﬁnance, not
providing explanations or providing inadequate ones can lead to legal, ﬁnancial, and reputational
risks. On the other hand, providing explanations is associated with its own risks (Weller, 2019).
These include the leakage of personal information or model information (intellectual property),
an increase in appeals of decisions for the decision-making entity, and strategic manipulation of
attributes (i.e. “gaming”) by individuals to gain more favorable outcomes."
REFERENCES,0.27158098933074687,"Applicability to RL settings
In RL, if one views the actions as labels and state representation
as features, one can build a tree, albeit likely a deep/wide one, to represent exactly the RL policy,
where the probability distribution over the actions can be viewed as the class distribution in a normal
supervised setting. Rolling up the states, creating leaves with multiple states, and simply averaging
the probabilities for each action would yield smaller trees that approximate the policy. Our work
lays a foundation where in principle we can also compare f and f0 that are policies using such tree
representations. This may be related to a popular global explainability method (Bastani et al., 2018)
that samples policies and builds trees to explain them."
REFERENCES,0.2725509214354995,"D
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.2735208535402522,"D.1
ADULT INCOME DATASET"
REFERENCES,0.27449078564500484,"Reference model
Figure 3 depicts the 8-leaf DT reference model used in the experiments on the
Adult Income dataset. The root node separates individuals based on whether the marital status
is Married-civ-spouse. The remaining splits divide the population into those with high and low
education, high and low capital gains, and high and low capital losses. In particular, having high
capital gains or losses is a good predictor of high income (> $50000)."
REFERENCES,0.2754607177497575,"C
nonzeros
ℓ1 norm
accuracy
AUC"
REFERENCES,0.27643064985451016,"3e-4
1
0.2
0.764
0.715
1e-3
6
2.6
0.825
0.885
3e-3
7
4.7
0.840
0.895
1e-2
16
7.4
0.848
0.900
3e-2
30
12.9
0.852
0.905
1e-1
38
17.3
0.853
0.905
3e-1
62
25.3
0.853
0.905
1e+0
83
40.7
0.852
0.905
3e+0
92
54.6
0.852
0.905
1e+1
101
65.1
0.852
0.904
3e+1
105
73.5
0.852
0.904
1e+2
107
77.6
0.852
0.904
3e+2
107
79.1
0.852
0.904"
REFERENCES,0.27740058195926287,"Table 1: Number of nonzero coefﬁcients, ℓ1 norm of coefﬁcients, test set accuracy, and area un-
der the receiver operating characteristic (AUC) for logistic regression models on the Adult Income
dataset as a function of inverse ℓ1 penalty C."
REFERENCES,0.27837051406401553,"LR and GAM models
In Tables 1 and 2, we show the values of inverse ℓ1 penalty C and
max bins that were used for LR and GAM respectively, as well as statistics of the resulting classi-
ﬁers. Test set accuracy and area under the receiver operating characteristic (AUC) increase and reach
a plateau. For LR, we take the ℓ1 norm of the coefﬁcients to be the main measure of smoothness
as it depends on both the number of nonzero coefﬁcients as well as their magnitudes, which both"
REFERENCES,0.2793404461687682,Under review as a conference paper at ICLR 2022
REFERENCES,0.28031037827352084,"#13
24.5%
[0.682, 0.318]"
REFERENCES,0.2812803103782735,"#14
1.0%
[0.248, 0.752]"
REFERENCES,0.2822502424830262,"#9
5.1%
[0.899, 0.101]"
REFERENCES,0.2832201745877789,"#10
CapitalLoss <= 1782.5"
REFERENCES,0.28419010669253153,"25.6%
[0.664, 0.336]"
REFERENCES,0.2851600387972842,"#7
EducationNum <= 8.5"
REFERENCES,0.28612997090203685,"30.6%
[0.703, 0.297]"
REFERENCES,0.2870999030067895,"#8
1.6%
[0.021, 0.979]"
REFERENCES,0.2880698351115422,"#11
11.6%
[0.331, 0.669]"
REFERENCES,0.2890397672162949,"#12
2.1%
[0.004, 0.996]"
REFERENCES,0.29000969932104753,"#5
53.1%
[0.951, 0.049]"
REFERENCES,0.2909796314258002,"#6
1.0%
[0.035, 0.965]"
REFERENCES,0.29194956353055285,"#3
CapitalGain <= 5095.5"
REFERENCES,0.2929194956353055,"32.3%
[0.669, 0.331]"
REFERENCES,0.2938894277400582,"#4
CapitalGain <= 5095.5"
REFERENCES,0.2948593598448109,"13.7%
[0.281, 0.719]"
REFERENCES,0.29582929194956353,"#1
CapitalGain <= 7073.5"
REFERENCES,0.2967992240543162,"54.0%
[0.935, 0.065]"
REFERENCES,0.29776915615906885,"#2
EducationNum <= 12.5"
REFERENCES,0.2987390882638215,"46.0%
[0.553, 0.447]"
REFERENCES,0.2997090203685742,"node #0
MaritalStatus= Married-civ-spouse <= 0.5"
REFERENCES,0.3006789524733269,"samples = 100.0%
value = [0.759, 0.241]"
REFERENCES,0.30164888457807953,Figure 3: Decision tree reference model with 8 leaves for the Adult Income dataset.
REFERENCES,0.3026188166828322,"max bins
accuracy
AUC"
REFERENCES,0.30358874878758485,"4
0.858
0.910
8
0.862
0.915
16
0.865
0.920
32
0.870
0.924
64
0.871
0.925
128
0.871
0.925
256
0.872
0.925
512
0.871
0.925
1024
0.871
0.925"
REFERENCES,0.30455868089233756,"Table 2: Test set accuracy and AUC for Explainable Boosting Machines on the Adult Income dataset
as a function of max bins parameter."
REFERENCES,0.3055286129970902,"affect the extreme values attained in (6). Based in part on Tables 1 and 2, we select C = 0.01 and
max bins = 8 as representative models that remain simple and have accuracies and AUCs not far
from the maximum attainable. Plots for these two models are shown in Figures 4 and 5."
REFERENCES,0.3064985451018429,"Maximum deviation for decision trees with small numbers of leaves
In Figure 6, we expand
upon the second panel in Figure 1 by focusing on decision trees with fewer than 50 leaves (plotted
with linear spacing). The maximum deviation is 0.114 for trees with 9 and 10 leaves, and remains
moderate up to 26 leaves."
REFERENCES,0.30746847720659554,"Relationships with accuracy and robust accuracy
In Figure 7, we show maximum deviation as
a function of test set accuracy for the DT, LR, and GAM models shown in the right three panels
of Figure 1 (the LR and GAM models are listed in Tables 1 and 2). Broadly, the plots show two"
REFERENCES,0.3084384093113482,Under review as a conference paper at ICLR 2022
REFERENCES,0.30940834141610085,"2
1
0
1
2"
REFERENCES,0.31037827352085356,"Gender= Male
Occupation= Farming-fishing"
REFERENCES,0.3113482056256062,"Gender= Female
WorkClass= Self-emp-not-inc"
REFERENCES,0.3123181377303589,"Relationship= Wife
Relationship= Own-child
Occupation= Prof-specialty
Occupation= Other-service
MaritalStatus= Never-married"
REFERENCES,0.31328806983511154,CapitalLoss
REFERENCES,0.3142580019398642,"Age
HoursPerWeek
Occupation= Exec-managerial"
REFERENCES,0.31522793404461685,EducationNum
REFERENCES,0.31619786614936957,"CapitalGain
MaritalStatus= Married-civ-spouse"
REFERENCES,0.3171677982541222,intercept
REFERENCES,0.3181377303588749,"Figure 4: Coefﬁcient values of the logistic regression model with C = 0.01 (16 nonzeros) for the
Adult Income dataset."
REFERENCES,0.31910766246362754,"regimes: one where accuracy increases and maximum deviation increases moderately or not at all,
and one where accuracy stalls while maximum deviation increases. The latter is less desirable as it
suggests increasing safety risks without a gain in accuracy. The last branch of the DT curve actually
decreases in accuracy, indicating overﬁtting, while maximum deviation is high."
REFERENCES,0.3200775945683802,"We also consider the relationship of maximum deviation to robust accuracy. Following Wong &
Kolter (2018), robust loss for a pair (x, y) is deﬁned as the worst-case loss over an ℓ∞ball centered
at x,
max
∥∆∥∞≤ϵ L(f(x + ∆), y),
(16)"
REFERENCES,0.32104752667313285,"and robust accuracy is therefore 1 minus the average robust 0-1 loss over a dataset. While Wong &
Kolter (2018) focus on bounding robust loss for feedforward neural networks with ReLU activations,
we ﬁnd that the results in Section 4.2 apply to computing robust loss (16) exactly for LR and GAM
models. Speciﬁcally, for 0-1 loss and ℓ∞balls, the separable optimization (8) applies, and the worst
case is obtained by minimizing f when the label y is positive and maximizing f when y is negative."
REFERENCES,0.32201745877788557,"The resulting robust accuracy values for DT, LR and GAM are plotted in Figure 8 in a similar fashion
as Figure 7. Here we set ϵ = 0.1 and r = 0.1 as well in computing maximum deviation. The DT plot
shows maximum deviation increasing with model complexity while robust accuracy is stable up to a
point. In the subsequent regime, when there are a large number of leaves, model robustness reduces
while deviation remains high. The LR plot begins similarly to the one in Figure 7 in that robust
accuracy increases along with maximum deviation, but then it stalls and decreases for maximum
deviation above 0.94. In the GAM plot, robust accuracy actually decreases with the max bins
parameter, i.e., the curve goes from right to left."
REFERENCES,0.3229873908826382,"Breakdown by leaves of f0
In Figures 9–12, we plot the maximum log-odds achieved by model
f (max on RHS of (7)), the minimum log-odds achieved by f, and the reference model log-odds
g(y0m) over each leaf of the decision tree reference model in Figure 3. Plots are on the log-odds
scale to show the deviations more clearly, including those that would be compressed by the nonlinear
logistic function g−1(z) = 1/(1 + e−z). Figures 9 and 11 show the dependence on the certiﬁcation
set radius r while Figures 10 and 12 show dependence on the smoothness parameters for LR and
GAM. These ﬁgures provide a more granular picture corresponding to the summary in Figure 1 and
support the trends seen there. In Figures 9 and 11, there are jumps at r = 1 because this is the
smallest radius that permits the values of categorical features of test set points (the ball centers in
(2)) to change to any other value. (Recall that categorical features are one-hot encoded into binary-
valued features.) In Figure 11, the GAM achieves the limiting deviations corresponding to r = ∞
(C = X, dashed lines) no later than r = 1.2. In Figure 9, the LR model achieves the lower limit on
log-odds as soon as r > 1 but the upper limit is not achieved for most leaves."
REFERENCES,0.3239573229873909,Under review as a conference paper at ICLR 2022
REFERENCES,0.32492725509214354,"0
0.1
0.2
0.3
0.4
0.5
0.6 Race"
REFERENCES,0.3258971871968962,NativeCountry
REFERENCES,0.3268671193016489,CapitalLoss
REFERENCES,0.32783705140640157,WorkClass
REFERENCES,0.3288069835111542,Education
REFERENCES,0.3297769156159069,Gender
REFERENCES,0.33074684772065954,HoursPerWeek
REFERENCES,0.3317167798254122,Occupation
REFERENCES,0.3326867119301649,EducationNum
REFERENCES,0.33365664403491757,CapitalGain
REFERENCES,0.33462657613967023,Relationship
REFERENCES,0.3355965082444229,MaritalStatus Age
REFERENCES,0.33656644034917554,"Overall Importance:
Mean Absolute Score"
REFERENCES,0.3375363724539282,"20
30
40
50
60
70
80
90 −2 0 2 4"
REFERENCES,0.3385063045586809,17 - 20.3
REFERENCES,0.3394762366634336,20.3 - 23.6
REFERENCES,0.34044616876818623,23.6 - 27
REFERENCES,0.3414161008729389,27 - 30.3
REFERENCES,0.34238603297769155,30.3 - 33.6
REFERENCES,0.3433559650824442,33.6 - 36.9
REFERENCES,0.3443258971871969,36.9 - 40.2
REFERENCES,0.3452958292919496,40.2 - 43.5
REFERENCES,0.34626576139670223,43.5 - 46.9
REFERENCES,0.3472356935014549,46.9 - 50.2
REFERENCES,0.34820562560620755,50.2 - 53.5
REFERENCES,0.34917555771096026,53.5 - 56.8
REFERENCES,0.3501454898157129,56.8 - 60.1
REFERENCES,0.3511154219204656,60.1 - 63.5
REFERENCES,0.35208535402521823,63.5 - 66.8
REFERENCES,0.3530552861299709,66.8 - 70.1
REFERENCES,0.35402521823472355,70.1 - 73.4
REFERENCES,0.35499515033947626,73.4 - 76.7
REFERENCES,0.3559650824442289,76.7 - 80
REFERENCES,0.3569350145489816,80 - 83.4
REFERENCES,0.35790494665373423,83.4 - 86.7
REFERENCES,0.3588748787584869,86.7 - 90
REFERENCES,0.35984481086323955,"0
1000
2000
3000 Age"
REFERENCES,0.36081474296799226,"Score
Density −2 0 2 4"
REFERENCES,0.3617846750727449,Divorced
REFERENCES,0.3627546071774976,Married-AF-spouse
REFERENCES,0.36372453928225024,Married-civ-spouse
REFERENCES,0.3646944713870029,Married-spouse-absent
REFERENCES,0.36566440349175555,Never-married
REFERENCES,0.36663433559650827,Separated
REFERENCES,0.3676042677012609,Widowed
REFERENCES,0.3685741998060136,"0
5k
10k
15k"
REFERENCES,0.36954413191076624,MaritalStatus
REFERENCES,0.3705140640155189,"Score
Density"
REFERENCES,0.37148399612027155,"0
20k
40k
60k
80k
100k −2 0 2 4"
REFERENCES,0.37245392822502427,0 - 3.85K
REFERENCES,0.3734238603297769,3.85K - 7.69K
REFERENCES,0.3743937924345296,7.69K - 11.5K
REFERENCES,0.37536372453928224,11.5K - 15.4K
REFERENCES,0.3763336566440349,15.4K - 19.2K
REFERENCES,0.3773035887487876,19.2K - 23.1K
REFERENCES,0.37827352085354027,23.1K - 26.9K
REFERENCES,0.3792434529582929,26.9K - 30.8K
REFERENCES,0.3802133850630456,30.8K - 34.6K
REFERENCES,0.38118331716779824,34.6K - 38.5K
REFERENCES,0.3821532492725509,38.5K - 42.3K
REFERENCES,0.3831231813773036,42.3K - 46.2K
REFERENCES,0.38409311348205627,46.2K - 50K
REFERENCES,0.3850630455868089,50K - 53.8K
REFERENCES,0.3860329776915616,53.8K - 57.7K
REFERENCES,0.38700290979631424,57.7K - 61.5K
REFERENCES,0.3879728419010669,61.5K - 65.4K
REFERENCES,0.3889427740058196,65.4K - 69.2K
REFERENCES,0.38991270611057227,69.2K - 73.1K
REFERENCES,0.39088263821532493,73.1K - 76.9K
REFERENCES,0.3918525703200776,76.9K - 80.8K
REFERENCES,0.39282250242483024,80.8K - 84.6K
REFERENCES,0.3937924345295829,84.6K - 88.5K
REFERENCES,0.3947623666343356,88.5K - 92.3K
REFERENCES,0.3957322987390883,92.3K - 96.2K
REFERENCES,0.39670223084384093,96.2K - 100K
REFERENCES,0.3976721629485936,"0
10k
20k
30k"
REFERENCES,0.39864209505334625,CapitalGain
REFERENCES,0.39961202715809896,"Score
Density"
REFERENCES,0.4005819592628516,"20
40
60
80 −2 0 2 4"
REFERENCES,0.4015518913676043,1 - 5.67
REFERENCES,0.40252182347235693,5.67 - 10.3
REFERENCES,0.4034917555771096,10.3 - 15
REFERENCES,0.40446168768186225,15 - 19.7
REFERENCES,0.40543161978661496,19.7 - 24.3
REFERENCES,0.4064015518913676,24.3 - 29
REFERENCES,0.4073714839961203,29 - 33.7
REFERENCES,0.40834141610087293,33.7 - 38.3
REFERENCES,0.4093113482056256,38.3 - 43
REFERENCES,0.41028128031037825,43 - 47.7
REFERENCES,0.41125121241513096,47.7 - 52.3
REFERENCES,0.4122211445198836,52.3 - 57
REFERENCES,0.4131910766246363,57 - 61.7
REFERENCES,0.41416100872938894,61.7 - 66.3
REFERENCES,0.4151309408341416,66.3 - 71
REFERENCES,0.41610087293889425,71 - 75.7
REFERENCES,0.41707080504364696,75.7 - 80.3
REFERENCES,0.4180407371483996,80.3 - 85
REFERENCES,0.4190106692531523,85 - 89.7
REFERENCES,0.41998060135790494,89.7 - 94.3
REFERENCES,0.4209505334626576,94.3 - 99
REFERENCES,0.4219204655674103,"0
5k
10k
15k"
REFERENCES,0.42289039767216297,HoursPerWeek
REFERENCES,0.4238603297769156,"Score
Density −2 0 2 4 ?"
REFERENCES,0.4248302618816683,Federal-gov
REFERENCES,0.42580019398642094,Local-gov
REFERENCES,0.4267701260911736,Never-worked
REFERENCES,0.4277400581959263,Private
REFERENCES,0.42870999030067897,Self-emp-inc
REFERENCES,0.4296799224054316,Self-emp-not-inc
REFERENCES,0.4306498545101843,State-gov
REFERENCES,0.43161978661493694,Without-pay
REFERENCES,0.4325897187196896,"0
5k
10k
15k
20k"
REFERENCES,0.4335596508244423,WorkClass
REFERENCES,0.43452958292919497,"Score
Density"
REFERENCES,0.4354995150339476,"0
1000
2000
3000
4000 −2 0 2 4"
REFERENCES,0.4364694471387003,0 - 174
REFERENCES,0.43743937924345294,174 - 348
REFERENCES,0.4384093113482056,348 - 523
REFERENCES,0.4393792434529583,523 - 697
REFERENCES,0.44034917555771097,697 - 871
REFERENCES,0.44131910766246363,871 - 1.05K
REFERENCES,0.4422890397672163,1.05K - 1.22K
REFERENCES,0.44325897187196894,1.22K - 1.39K
REFERENCES,0.44422890397672166,1.39K - 1.57K
REFERENCES,0.4451988360814743,1.57K - 1.74K
REFERENCES,0.44616876818622697,1.74K - 1.92K
REFERENCES,0.44713870029097963,1.92K - 2.09K
REFERENCES,0.4481086323957323,2.09K - 2.27K
REFERENCES,0.44907856450048494,2.27K - 2.44K
REFERENCES,0.45004849660523766,2.44K - 2.61K
REFERENCES,0.4510184287099903,2.61K - 2.79K
REFERENCES,0.451988360814743,2.79K - 2.96K
REFERENCES,0.45295829291949563,2.96K - 3.14K
REFERENCES,0.4539282250242483,3.14K - 3.31K
REFERENCES,0.45489815712900095,3.31K - 3.48K
REFERENCES,0.45586808923375366,3.48K - 3.66K
REFERENCES,0.4568380213385063,3.66K - 3.83K
REFERENCES,0.457807953443259,3.83K - 4.01K
REFERENCES,0.45877788554801163,4.01K - 4.18K
REFERENCES,0.4597478176527643,4.18K - 4.36K
REFERENCES,0.46071774975751695,"0
10k
20k
30k"
REFERENCES,0.46168768186226966,CapitalLoss
REFERENCES,0.4626576139670223,"Score
Density −2 0 2 4"
REFERENCES,0.463627546071775,Amer-Indian-Eskimo
REFERENCES,0.46459747817652763,Asian-Pac-Islander Black Other White
REFERENCES,0.4655674102812803,"0
10k
20k Race"
REFERENCES,0.466537342386033,"Score
Density"
REFERENCES,0.46750727449078566,"Figure 5: Feature importances and selected univariate functions fj for the Explainable Boosting
Machine with max bins = 8 on the Adult Income dataset."
REFERENCES,0.4684772065955383,Under review as a conference paper at ICLR 2022
REFERENCES,0.469447138700291,"0
10
20
30
40
50
Number of leaf nodes 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.47041707080504364,max deviation D
REFERENCES,0.4713870029097963,Decision Tree
REFERENCES,0.472356935014549,"Figure 6: Maximum deviation D for decision trees on the Adult Income dataset as a function of the
number of leaves."
REFERENCES,0.47332686711930166,"0.845
0.850
0.855
0.860
accuracy 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4742967992240543,Max Deviation D
REFERENCES,0.475266731328807,"DT, r = 0.2"
REFERENCES,0.47623666343355964,"0.76
0.78
0.80
0.82
0.84
accuracy 0.86 0.88 0.90 0.92 0.94 0.96"
REFERENCES,0.4772065955383123,max deviation D
REFERENCES,0.478176527643065,"LR, r = 0.2"
REFERENCES,0.47914645974781767,0.8575 0.8600 0.8625 0.8650 0.8675 0.8700
REFERENCES,0.4801163918525703,accuracy 0.965 0.970 0.975 0.980 0.985 0.990
REFERENCES,0.481086323957323,max deviation D
REFERENCES,0.48205625606207564,"GAM, r = 0.2"
REFERENCES,0.4830261881668283,"Figure 7: Maximum deviation D (at certiﬁcation set radius r = 0.2) vs. test set accuracy on the
Adult Income dataset."
REFERENCES,0.483996120271581,"0.80
0.81
0.82
0.83
0.84
robust accuracy ( = 0.1) 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.48496605237633367,max deviation D (r = 0.1) DT
REFERENCES,0.4859359844810863,"0.765
0.770
0.775
0.780
0.785
0.790
robust accuracy (  = 0.1) 0.86 0.88 0.90 0.92 0.94 0.96"
REFERENCES,0.486905916585839,max deviation D (r = 0.1) LR
REFERENCES,0.48787584869059164,"0.72
0.74
0.76
0.78
0.80
robust accuracy (  = 0.1)"
REFERENCES,0.48884578079534435,0.9600
REFERENCES,0.489815712900097,0.9625
REFERENCES,0.49078564500484967,0.9650
REFERENCES,0.4917555771096023,0.9675
REFERENCES,0.492725509214355,0.9700
REFERENCES,0.49369544131910764,0.9725
REFERENCES,0.49466537342386036,0.9750
REFERENCES,0.495635305528613,0.9775
REFERENCES,0.49660523763336567,max deviation D (r = 0.1) GAM
REFERENCES,0.49757516973811833,"Figure 8: Maximum deviation D vs. robust accuracy (ϵ = 0.1, r = 0.1) on the Adult Income dataset."
REFERENCES,0.498545101842871,Under review as a conference paper at ICLR 2022 7.5 5.0 2.5 0.0 2.5 5.0 7.5
REFERENCES,0.49951503394762364,log-odds
REFERENCES,0.5004849660523764,leaf 5
REFERENCES,0.501454898157129,"min
min r"
REFERENCES,0.5024248302618817,"max
max r
reference 5 0 5 10 15 20 25 30"
REFERENCES,0.5033947623666344,log-odds
REFERENCES,0.504364694471387,leaf 6 5 0 5 10 15 20 25 30
REFERENCES,0.5053346265761397,log-odds
REFERENCES,0.5063045586808923,leaf 8 6 4 2 0 2 4 6
REFERENCES,0.507274490785645,log-odds
REFERENCES,0.5082444228903976,leaf 9 2 0 2 4 6 8
REFERENCES,0.5092143549951503,log-odds
REFERENCES,0.510184287099903,leaf 11 0 5 10 15 20 25 30
REFERENCES,0.5111542192046556,log-odds
REFERENCES,0.5121241513094084,leaf 12
REFERENCES,0.513094083414161,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 4 2 0 2 4 6"
REFERENCES,0.5140640155189137,log-odds
REFERENCES,0.5150339476236664,leaf 13
REFERENCES,0.516003879728419,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 2 0 2 4 6"
REFERENCES,0.5169738118331717,log-odds
REFERENCES,0.5179437439379243,leaf 14
REFERENCES,0.518913676042677,"Figure 9: Minimum and maximum predicted log-odds for a logistic regression model with inverse
ℓ1 penalty C = 0.01, as a function of certiﬁcation set size (radius r) and broken down by leaves of
the decision tree reference model."
REFERENCES,0.5198836081474297,Under review as a conference paper at ICLR 2022 15 10 5 0 5
REFERENCES,0.5208535402521823,log-odds
REFERENCES,0.521823472356935,leaf 5
REFERENCES,0.5227934044616876,"min
max
reference 10 0 10 20 30"
REFERENCES,0.5237633365664404,log-odds
REFERENCES,0.524733268671193,leaf 6 0 5 10 15 20 25 30 35
REFERENCES,0.5257032007759457,log-odds
REFERENCES,0.5266731328806984,leaf 8 15.0 12.5 10.0 7.5 5.0 2.5 0.0 2.5
REFERENCES,0.527643064985451,log-odds
REFERENCES,0.5286129970902037,leaf 9 4 2 0 2 4
REFERENCES,0.5295829291949563,log-odds
REFERENCES,0.530552861299709,leaf 11 0 10 20 30
REFERENCES,0.5315227934044617,log-odds
REFERENCES,0.5324927255092143,leaf 12
REFERENCES,0.533462657613967,"0
10
20
30
40
50
60
70
80"
NORM,0.5344325897187197,1 norm 10 8 6 4 2 0 2 4
NORM,0.5354025218234724,log-odds
NORM,0.5363724539282251,leaf 13
NORM,0.5373423860329777,"0
10
20
30
40
50
60
70
80"
NORM,0.5383123181377304,1 norm 6 4 2 0 2 4
NORM,0.539282250242483,log-odds
NORM,0.5402521823472357,leaf 14
NORM,0.5412221144519883,"Figure 10: Minimum and maximum predicted log-odds for logistic regression models with different
ℓ1 penalties C, broken down by leaves of the decision tree reference model. The certiﬁcation set ℓ∞
ball radius is r = 0.2."
NORM,0.542192046556741,Under review as a conference paper at ICLR 2022 15 10 5 0 5 10
NORM,0.5431619786614937,log-odds
NORM,0.5441319107662463,leaf 5
NORM,0.545101842870999,"min
min r"
NORM,0.5460717749757517,"max
max r
reference 10 5 0 5 10"
NORM,0.5470417070805044,log-odds
NORM,0.5480116391852571,leaf 6 10 5 0 5 10
NORM,0.5489815712900097,log-odds
NORM,0.5499515033947624,leaf 8 15 10 5 0 5
NORM,0.550921435499515,log-odds
NORM,0.5518913676042677,leaf 9 10 5 0 5
NORM,0.5528612997090203,log-odds
NORM,0.553831231813773,leaf 11 10 5 0 5 10
NORM,0.5548011639185257,log-odds
NORM,0.5557710960232783,leaf 12
NORM,0.5567410281280311,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
 ball radius r 15 10 5 0 5"
NORM,0.5577109602327837,log-odds
NORM,0.5586808923375364,leaf 13
NORM,0.5596508244422891,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
 ball radius r 10.0 7.5 5.0 2.5 0.0 2.5 5.0"
NORM,0.5606207565470417,log-odds
NORM,0.5615906886517944,leaf 14
NORM,0.562560620756547,"Figure 11: Minimum and maximum predicted log-odds for an Explainable Boosting Machine with
max bins = 8, as a function of certiﬁcation set size (radius r) and broken down by leaves of the
decision tree reference model."
NORM,0.5635305528612997,Under review as a conference paper at ICLR 2022 15 10 5 0 5
NORM,0.5645004849660524,log-odds
NORM,0.565470417070805,leaf 5
NORM,0.5664403491755577,"min
max
reference 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
NORM,0.5674102812803103,log-odds
NORM,0.5683802133850631,leaf 6 10 5 0 5 10
NORM,0.5693501454898157,log-odds
NORM,0.5703200775945684,leaf 8 10 5 0 5
NORM,0.5712900096993211,log-odds
NORM,0.5722599418040737,leaf 9 7.5 5.0 2.5 0.0 2.5 5.0 7.5
NORM,0.5732298739088264,log-odds
NORM,0.574199806013579,leaf 11 5.0 2.5 0.0 2.5 5.0 7.5 10.0
NORM,0.5751697381183317,log-odds
NORM,0.5761396702230844,leaf 12
NORM,0.577109602327837,"101
102
103"
NORM,0.5780795344325897,maxBins 10.0 7.5 5.0 2.5 0.0 2.5 5.0
NORM,0.5790494665373423,log-odds
NORM,0.5800193986420951,leaf 13
NORM,0.5809893307468477,"101
102
103"
NORM,0.5819592628516004,maxBins 10.0 7.5 5.0 2.5 0.0 2.5 5.0
NORM,0.5829291949563531,log-odds
NORM,0.5838991270611057,leaf 14
NORM,0.5848690591658584,"Figure 12: Minimum and maximum predicted log-odds for Explainable Boosting Machines with
different max bins values, broken down by leaves of the decision tree reference model. The certi-
ﬁcation set ℓ∞ball radius is r = 0.2."
NORM,0.585838991270611,Under review as a conference paper at ICLR 2022
NORM,0.5868089233753637,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 0 200 400 600 800"
NORM,0.5877788554801164,time [sec]
NORM,0.588748787584869,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
 ball radius r 0 500 1000 1500 2000 2500"
NORM,0.5897187196896218,time [sec]
NORM,0.5906886517943744,"Figure 13: Time to compute maximum deviation for logistic regression models (left) and Explain-
able Boosting Machines (right) on the Adult Income dataset as a function of certiﬁcation set size
(radius r). The ﬁlled-in region shows the min-max variation with model complexity (ℓ1 norm for
LR, max bins for EBM)."
NORM,0.5916585838991271,"Running time
Figure 13 shows the time required to compute the maximum deviation for LR and
GAM on the Adult Income dataset. These times were obtained using a single 2.0 GHz core of a
server with 64 GB of memory (only a small fraction of which was used) running Ubuntu 16.04
(64-bit). The times increase with the ℓ∞ball radius r because of the increasing number of ball-leaf
intersections that become non-empty and hence need to be evaluated. The time for r = 0 is minimal
because this case requires only model evaluation over the ﬁnite test set, as mentioned. The jumps
at r = 1 are due again to the ability of categorical features to change values, leading to an increase
in ball-leaf intersections. The ﬁlled-in regions show that there was little variation due to different ℓ1
norms for LR or max bins for GAM. This was most likely because of a vectorized implementation,
which operates on all LR coefﬁcients or all GAM bins at once (i.e., without a for loop)."
NORM,0.5926285160038798,"Maximal cliques evaluated for DT, RF
To investigate the effectiveness of pruning by bounds in
Algorithm 1, we investigate the number of times all the decision trees in the Random Forest have
to be processed. This represents number of times the state could not be pruned and needed to be
evaluated fully."
NORM,0.5935984481086324,"Figure 14 shows two aspects at play. (a) Pruning by bound is effective in restricting the search space
more so for Random Forests than for decision trees, and (b) for larger graphs, more time is spent in
computing bounds in Eq. (11)."
NORM,0.5945683802133851,"0
1000
2000
3000
4000
5000
Number of leaf nodes in Decision Tree 0 200 400 600 800 1000"
NORM,0.5955383123181377,K + 1 max cliques evaluated
NORM,0.5965082444228904,(a) Decision Tree
NORM,0.597478176527643,"0
100
200
300
400
500
600
Number of estimators in Random Forest 20 40 60 80 100 120"
NORM,0.5984481086323957,K + 1 max cliques evaluated 0 2500 5000 7500 10000 12500 15000 17500
NORM,0.5994180407371484,# Nodes | |
NORM,0.600387972841901,(b) Random Forest
NORM,0.6013579049466538,Figure 14: Effectiveness of pruning by bound for tree-based models
NORM,0.6023278370514064,"Feature combinations that maximize deviation
In Tables 3 and 4, we report feature values that
maximize deviation over selected leaves of the DT reference model, for LR and GAM respectively.
For Table 3, we have chosen the minimum log-odds over leaf 6 (corresponding to Figure 9, leaf
6, blue curve), which is one of the two leaves m in (7) that maximize the deviation overall (the
other being leaf 8). For Table 4, the minimum log-odds over leaf 12 is chosen (corresponding to
Figure 11, leaf 12, blue curve) because this maximizes the deviation overall for most values of r."
NORM,0.6032977691561591,Under review as a conference paper at ICLR 2022
NORM,0.6042677012609118,"The tables show the 6 features that contribute most to the minimum log-odds. These contributions
are determined using (8) (with max replaced by min); since the minimum log-odds occurs in one of
the ℓ∞ball-leaf intersections and this intersection is a Cartesian product, the decomposition in (8)
applies. The contribution of feature j is then minxj∈Sj fj(xj). To account for the multiple r values,
we take an average of the contributions over r."
NORM,0.6052376333656644,"r
EducationNum
HoursPerWeek
Age
MaritalStatus
Occupation
Relationship"
NORM,0.6062075654704171,"0.0
9.0
15.0
23.0
Never-married
Sales
Own-child
0.1
4.7
33.8
26.6
Never-married
Transport-moving
Not-in-family
0.2
4.5
32.5
25.3
Never-married
Transport-moving
Not-in-family
0.4
4.0
30.1
22.5
Never-married
Transport-moving
Not-in-family
0.6
3.5
27.6
19.8
Never-married
Transport-moving
Not-in-family
0.8
1.0
26.1
17.0
Never-married
Farming-ﬁshing
Not-in-family
0.999
1.0
7.7
17.0
Never-married
Other-service
Own-child
1.001
1.0
1.0
17.0
Never-married
Other-service
Own-child
1.2
1.0
1.0
17.0
Never-married
Other-service
Own-child
1.4
1.0
1.0
17.0
Never-married
Other-service
Own-child
1.6
1.0
1.0
17.0
Never-married
Other-service
Own-child
1.8
1.0
1.0
17.0
Never-married
Other-service
Own-child
2.0
1.0
1.0
17.0
Never-married
Other-service
Own-child
∞
1.0
1.0
17.0
Never-married
Other-service
Own-child"
NORM,0.6071774975751697,"Table 3: Feature values that minimize log-odds for a logistic regression model (C = 0.01) over
leaf 6 of the decision tree reference model. The 6 features that contribute most to the minimum are
shown as a function of certiﬁcation set radius r."
NORM,0.6081474296799224,"As r increases, the predominant trend of the values of continuous features is toward extremes of
the domain X, depending on the sign of the corresponding LR coefﬁcient wj or shape of the GAM
function fj. For example, EducationNum (education on an ordinal scale), hours per week, and
age decrease toward minimum values, while capital gain occupies the minimal interval permitted
for leaf 12 (see Figure 3). (These examples make sense since the log-odds of high income is being
minimized.) This movement toward extremes is expected in the LR case because the functions wjxj
are either increasing or decreasing, and it is also true for GAM if the function fj is mainly increasing
or decreasing. The values sometimes change abruptly in the opposite direction, for example hours
per week in both Tables 3, 4, and age in the latter. These abrupt changes are due to the minimum
jumping from one ball in (2) to another as r increases, but the overall trend eventually prevails. For
categorical features, the trend is toward values that minimize fj(xj), e.g., Never-married marital
status, Without-pay work class. While the contribution of each of these features to minimizing log-
odds may be limited, together they do add up."
NORM,0.6091173617846751,"r
CapitalLoss
Age
HoursPerWeek
WorkClass
CapitalGain
Race"
NORM,0.6100872938894277,"0.0
0
29.0
40.0
Private
7298
White
0.1
[ 0 40]
[53.6 56.4]
[18.8 20.5]
?
[5095 5119]
White
0.2
[ 0 78]
[23.3 23.5]
[4.5 9.5]
State-gov
[5095 5119]
White
0.4
[ 0 78]
[20.5 23.5]
[ 2.1 11.9]
State-gov
[5095 5119]
White
0.6
[ 0 78]
[28.8 29.5]
[30.6 31.5]
Private
[5095 5119]
Asian-Pac-Islander
0.8
[1598 1759]
[20.1 23.5]
[30.1 31.5]
State-gov
[5095 5119]
White
0.999
[1598 1759]
[21.4 23.5]
[27.7 31.5]
Private
[5095 5119]
Amer-Indian-Eskimo
1.001
[1598 1759]
[17. 23.5]
[11.6 20.5]
Without-pay
[5095 5119]
Amer-Indian-Eskimo
1.2
[1598 1759]
[17. 23.5]
[ 9.2 20.5]
Without-pay
[5095 5119]
Amer-Indian-Eskimo
1.4
[1598 1759]
[17. 23.5]
[ 6.7 20.5]
Without-pay
[5095 5119]
Amer-Indian-Eskimo
∞
[1598 1759]
[17. 23.5]
[ 1. 20.5]
Without-pay
[5095 5119]
Amer-Indian-Eskimo"
NORM,0.6110572259941804,"Table 4: Feature values that minimize log-odds for an Explainable Boosting Machine (max bins
= 8) over leaf 12 of the decision tree reference model. The 6 features that contribute most to the
minimum are shown as a function of certiﬁcation set radius r. For r > 0, the minimizing values of
continuous features form an interval because the corresponding functions fj are piecewise constant."
NORM,0.612027158098933,"A notable exception to the trend toward extremes is capital loss in Table 4. For this feature, the
values settle into the intermediate interval [1598, 1759]. The plot of the CapitalLoss function in"
NORM,0.6129970902036858,Under review as a conference paper at ICLR 2022
NORM,0.6139670223084384,"Figure 5 shows that this interval corresponds to a curiously low value of the function. This low
region may be an artifact since it seems anomalous compared to the rest of the CapitalLoss function,
and since individuals who report capital losses on their income tax returns to offset capital gains
usually have high income (hence high log-odds)."
NORM,0.6149369544131911,"Given the results in Tables 3 and 4, one question that arises is whether the feature combinations are
indeed possible, if not the ones for r →∞, then at least for some ﬁnite value of r. For the top
features shown in the two tables, while some combinations may appear improbable (for example,
EducationNum = 1 and 1 hour per week), we submit that none appear impossible. However, if one
considers features beyond the top 6, then some “impossible” combinations do occur (e.g., a female
husband), although the contributions of these features to the minimum log-odds are much less. We
touch upon this issue again in Appendix C."
NORM,0.6159068865179438,"The next question one might consider is the implication of these maximal deviations. From Figure 3,
it is seen that leaf 6 classiﬁes individuals with high capital gains as high income with high probability
(0.965). Leaf 12 adds the attributes of married status and high education, and hence classiﬁes as
high income with even higher probability (0.996). At the same time, the feature values in Tables 3
and 4, which minimize log-odds for LR and GAM, also make sense according to basic domain
knowledge. For example, few hours per week and young age are associated with lower income,
as are Without-pay work class and Amer-Indian-Eskimo race in the United States. When these
conﬂicting associations occur in combination and the combination does not appear impossible, the
question may be which one prevails. Such a question might be resolvable by a domain expert.
Alternatively, the disagreement between models f and f0 on the extreme examples in Tables 3, 4
may be reason to be cautious about using either of the models in these cases. This might lead to a
way of combining the models or abstaining from prediction altogether. Lastly, the anomalously low
region in the CapitalLoss function identiﬁed in Table 4 is a clear, concrete example where further
investigation is warranted."
NORM,0.6168768186226964,Under review as a conference paper at ICLR 2022
NORM,0.6178467507274491,"D.2
LENDING CLUB DATASET"
NORM,0.6188166828322017,"This dataset consists of 2.26 million rows with 14 features on loans. The target variable is whether
a loan will be paid-off or defaulted on. Features describe the terms of the loan, e.g. duration,
grade, purpose, etc. and borrower ﬁnancial information such as credit history and income. For
this case study, we consider a loan approval scenario using only information available at the time
of application. In particular, we exclude the feature ‘total pymnt’ (total payment over time on the
loan), which becomes known at essentially the same time as the target variable. (When ‘total pymnt’
is included as a feature, the prediction task becomes easy and accuracies in the high 90% range are
possible.)"
NORM,0.6197866149369544,"#11
7.0%
[0.203, 0.797]"
NORM,0.6207565470417071,"#12
13.3%
[0.265, 0.735]"
NORM,0.6217264791464597,"#13
12.2%
[0.139, 0.861]"
NORM,0.6226964112512124,"#14
18.2%
[0.188, 0.812]"
NORM,0.623666343355965,"#7
added_dti <= 5.922"
NORM,0.6246362754607178,"20.3%
[0.244, 0.756]"
NORM,0.6256062075654704,"#8
10.9%
[0.329, 0.671]"
NORM,0.6265761396702231,"#9
1.9%
[0.305, 0.695]"
NORM,0.6275460717749758,"#10
8.2%
[0.435, 0.565]"
NORM,0.6285160038797284,"#5
28.2%
[0.081, 0.919]"
NORM,0.6294859359844811,"#6
sub_grade <= 8.5"
NORM,0.6304558680892337,"30.4%
[0.169, 0.831]"
NORM,0.6314258001939864,"#3
term= 60 months <= 0.5"
NORM,0.6323957322987391,"31.2%
[0.273, 0.727]"
NORM,0.6333656644034917,"#4
added_dti <= 5.926"
NORM,0.6343355965082444,"10.1%
[0.411, 0.589]"
NORM,0.635305528612997,"#1
sub_grade <= 6.5"
NORM,0.6362754607177498,"58.6%
[0.126, 0.874]"
NORM,0.6372453928225025,"#2
sub_grade <= 19.5"
NORM,0.6382153249272551,"41.4%
[0.307, 0.693]"
NORM,0.6391852570320078,"node #0
sub_grade <= 11.5"
NORM,0.6401551891367604,"samples = 100.0%
value = [0.201, 0.799]"
NORM,0.6411251212415131,Figure 15: Decision tree reference model with 8 leaves for the Lending Club dataset.
NORM,0.6420950533462657,"Reference model
Figure 15 depicts the 8-leaf DT reference model for the Lending Club dataset.
Most of the splits partition the ‘sub grade’ feature, which is a measure of the quality of the loan
(0–34 range, lower is better). Node 3 differentiates between 60-month terms and 36-month terms
(the only alternative), while nodes 4 and 7 split on ‘added dti’ (added debt-to-income ratio), which
is the ratio between 12 months worth of the loan’s payment installments and the borrower’s annual
income. While the structure of the DT agrees with domain knowledge (lower ‘sub grade’ and lower
‘added dti’ correlate with higher repayment probability), the test set accuracy of 79.8% is no better
than that of the trivial predictor that always returns the majority class of “paid off”. The DT’s AUC
of 0.689 however does indicate an improvement over the trivial predictor."
NORM,0.6430649854510184,"LR and GAM models
Tables 5 and 6 show the statistics of the LR and GAM classiﬁers that were
trained on the Lending Club data. Similar to the DT reference model, the difference compared to
Tables 1, 2 for the Adult Income dataset is that the accuracies remain no better than that of the
trivial predictor, while the AUC does not show much increase either. These statistics suggest that
the prediction task is difﬁcult with the features available. Figures 16 and 17 display plots for the
LR model with C = 0.01 and GAM with max bins = 8, which are again chosen as representative"
NORM,0.6440349175557711,Under review as a conference paper at ICLR 2022
NORM,0.6450048496605237,"C
nonzeros
ℓ1 norm
accuracy
AUC"
NORM,0.6459747817652765,"1e-4
1
0.4
0.798
0.693
3e-4
2
0.6
0.799
0.695
1e-3
6
1.0
0.799
0.702
3e-3
8
1.3
0.799
0.702
1e-2
12
1.6
0.800
0.702
3e-2
17
2.1
0.799
0.703
1e-1
22
3.1
0.799
0.703
3e-1
24
3.7
0.799
0.703
1e+0
26
4.1
0.799
0.703
3e+0
26
4.2
0.799
0.703"
NORM,0.646944713870029,"Table 5: Number of nonzero coefﬁcients, ℓ1 norm of coefﬁcients, test set accuracy, and AUC for
logistic regression models on the Lending Club dataset as a function of inverse ℓ1 penalty C."
NORM,0.6479146459747818,"max bins
accuracy
AUC"
NORM,0.6488845780795345,"4
0.798
0.696
8
0.798
0.703
16
0.799
0.704
32
0.799
0.705
64
0.799
0.705
128
0.799
0.705
256
0.799
0.705
512
0.799
0.705
1024
0.799
0.705"
NORM,0.6498545101842871,"Table 6: Test set accuracy and AUC for Explainable Boosting Machines on the Lending Club dataset
as a function of max bins parameter."
NORM,0.6508244422890398,"models. The GAM in particular shows sensible monotonic behavior as functions of ‘sub grade’,
‘int rate’ (interest rate), ‘dti’ (debt-to-income ratio), etc., despite the unimpressive accuracy."
NORM,0.6517943743937924,"0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25"
NORM,0.6527643064985451,revol_util
NORM,0.6537342386032978,funded_amnt
NORM,0.6547041707080504,credit_history
NORM,0.6556741028128031,purpose=debt_consolidation
NORM,0.6566440349175557,added_dti
NORM,0.6576139670223085,int_rate
NORM,0.6585838991270611,installment
NORM,0.6595538312318138,purpose=small_business dti
NORM,0.6605237633365665,annual_inc
NORM,0.6614936954413191,term= 36 months
NORM,0.6624636275460718,sub_grade
NORM,0.6634335596508244,intercept
NORM,0.6644034917555771,"Figure 16: Coefﬁcient values of the logistic regression model with C = 0.01 (12 nonzeros) for the
Lending Club dataset."
NORM,0.6653734238603298,Under review as a conference paper at ICLR 2022
NORM,0.6663433559650824,"0
0.05
0.1
0.15
0.2"
NORM,0.6673132880698351,installment
NORM,0.6682832201745877,loan_amnt
NORM,0.6692531522793405,funded_amnt
NORM,0.6702230843840931,purpose
NORM,0.6711930164888458,revol_util
NORM,0.6721629485935985,credit_history
NORM,0.6731328806983511,annual_inc
NORM,0.6741028128031038,added_dti dti term
NORM,0.6750727449078564,int_rate
NORM,0.6760426770126091,sub_grade
NORM,0.6770126091173618,"Overall Importance:
Mean Absolute Score"
NORM,0.6779825412221144,"0
5
10
15
20
25
30 −0.2 0 0.2 0.4"
NORM,0.6789524733268671,0 - 1.31
NORM,0.6799224054316197,1.31 - 2.62
NORM,0.6808923375363725,2.62 - 3.92
NORM,0.6818622696411252,3.92 - 5.23
NORM,0.6828322017458778,5.23 - 6.54
NORM,0.6838021338506305,6.54 - 7.85
NORM,0.6847720659553831,7.85 - 9.15
NORM,0.6857419980601358,9.15 - 10.5
NORM,0.6867119301648884,10.5 - 11.8
NORM,0.6876818622696411,11.8 - 13.1
NORM,0.6886517943743938,13.1 - 14.4
NORM,0.6896217264791464,14.4 - 15.7
NORM,0.6905916585838991,15.7 - 17
NORM,0.6915615906886518,17 - 18.3
NORM,0.6925315227934045,18.3 - 19.6
NORM,0.6935014548981572,19.6 - 20.9
NORM,0.6944713870029098,20.9 - 22.2
NORM,0.6954413191076625,22.2 - 23.5
NORM,0.6964112512124151,23.5 - 24.8
NORM,0.6973811833171678,24.8 - 26.2
NORM,0.6983511154219205,26.2 - 27.5
NORM,0.6993210475266731,27.5 - 28.8
NORM,0.7002909796314258,28.8 - 30.1
NORM,0.7012609117361784,30.1 - 31.4
NORM,0.7022308438409312,31.4 - 32.7
NORM,0.7032007759456838,32.7 - 34
NORM,0.7041707080504365,"0
10k
20k
30k"
NORM,0.7051406401551892,sub_grade
NORM,0.7061105722599418,"Score
Density"
NORM,0.7070805043646945,"10
15
20
25
30 −0.2 0 0.2 0.4"
NORM,0.7080504364694471,5.31 - 6.26
NORM,0.7090203685741998,6.26 - 7.21
NORM,0.7099903006789525,7.21 - 8.16
NORM,0.7109602327837051,8.16 - 9.11
NORM,0.7119301648884578,9.11 - 10.1
NORM,0.7129000969932104,10.1 - 11
NORM,0.7138700290979632,11 - 12
NORM,0.7148399612027158,12 - 12.9
NORM,0.7158098933074685,12.9 - 13.9
NORM,0.7167798254122212,13.9 - 14.8
NORM,0.7177497575169738,14.8 - 15.8
NORM,0.7187196896217265,15.8 - 16.7
NORM,0.7196896217264791,16.7 - 17.7
NORM,0.7206595538312318,17.7 - 18.6
NORM,0.7216294859359845,18.6 - 19.6
NORM,0.7225994180407371,19.6 - 20.5
NORM,0.7235693501454898,20.5 - 21.5
NORM,0.7245392822502424,21.5 - 22.4
NORM,0.7255092143549952,22.4 - 23.4
NORM,0.7264791464597479,23.4 - 24.3
NORM,0.7274490785645005,24.3 - 25.3
NORM,0.7284190106692532,25.3 - 26.2
NORM,0.7293889427740058,26.2 - 27.2
NORM,0.7303588748787585,27.2 - 28.1
NORM,0.7313288069835111,28.1 - 29.1
NORM,0.7322987390882638,29.1 - 30
NORM,0.7332686711930165,30 - 31
NORM,0.7342386032977691,"0
10k
20k"
NORM,0.7352085354025218,int_rate
NORM,0.7361784675072744,"Score
Density −0.2 0 0.2 0.4"
MONTHS,0.7371483996120272,"36 months
 60 months
0
50k
100k
150k
200k term"
MONTHS,0.7381183317167799,"Score
Density"
MONTHS,0.7390882638215325,"0
200
400
600
800 −0.2 0 0.2 0.4"
MONTHS,0.7400581959262852,0 - 31.2
MONTHS,0.7410281280310378,31.2 - 62.4
MONTHS,0.7419980601357905,62.4 - 93.7
MONTHS,0.7429679922405431,93.7 - 125
MONTHS,0.7439379243452958,125 - 156
MONTHS,0.7449078564500485,156 - 187
MONTHS,0.7458777885548011,187 - 219
MONTHS,0.7468477206595538,219 - 250
MONTHS,0.7478176527643065,250 - 281
MONTHS,0.7487875848690592,281 - 312
MONTHS,0.7497575169738119,312 - 343
MONTHS,0.7507274490785645,343 - 375
MONTHS,0.7516973811833172,375 - 406
MONTHS,0.7526673132880698,406 - 437
MONTHS,0.7536372453928225,437 - 468
MONTHS,0.7546071774975752,468 - 500
MONTHS,0.7555771096023278,500 - 531
MONTHS,0.7565470417070805,531 - 562
MONTHS,0.7575169738118331,562 - 593
MONTHS,0.7584869059165859,593 - 624
MONTHS,0.7594568380213385,624 - 656
MONTHS,0.7604267701260912,656 - 687
MONTHS,0.7613967022308439,687 - 718
MONTHS,0.7623666343355965,718 - 749
MONTHS,0.7633365664403492,749 - 780
MONTHS,0.7643064985451018,780 - 812
MONTHS,0.7652764306498545,812 - 843
MONTHS,0.7662463627546072,843 - 874
MONTHS,0.7672162948593598,874 - 905
MONTHS,0.7681862269641125,905 - 937
MONTHS,0.7691561590688651,937 - 968
MONTHS,0.7701260911736179,968 - 999 0
K,0.7710960232783706,100k
K,0.7720659553831232,200k dti
K,0.7730358874878759,"Score
Density"
K,0.7740058195926285,"0
500
1000
1500
2000
2500
3000 −0.2 0 0.2 0.4"
K,0.7749757516973812,0.0123 - 90.8
K,0.7759456838021338,90.8 - 182
K,0.7769156159068865,182 - 273
K,0.7778855480116392,273 - 363
K,0.7788554801163918,363 - 454
K,0.7798254122211445,454 - 545
K,0.7807953443258971,545 - 636
K,0.7817652764306499,636 - 727
K,0.7827352085354026,727 - 818
K,0.7837051406401552,818 - 908
K,0.7846750727449079,908 - 999
K,0.7856450048496605,999 - 1.09K
K,0.7866149369544132,1.09K - 1.18K
K,0.7875848690591658,1.18K - 1.27K
K,0.7885548011639185,1.27K - 1.36K
K,0.7895247332686712,1.36K - 1.45K
K,0.7904946653734238,1.45K - 1.54K
K,0.7914645974781765,1.54K - 1.64K
K,0.7924345295829291,1.64K - 1.73K
K,0.7934044616876819,1.73K - 1.82K
K,0.7943743937924346,1.82K - 1.91K
K,0.7953443258971872,1.91K - 2K
K,0.7963142580019399,2K - 2.09K
K,0.7972841901066925,2.09K - 2.18K
K,0.7982541222114452,2.18K - 2.27K
K,0.7992240543161979,2.27K - 2.36K
K,0.8001939864209505,2.36K - 2.45K
K,0.8011639185257032,2.45K - 2.54K
K,0.8021338506304558,2.54K - 2.63K
K,0.8031037827352085,2.63K - 2.73K
K,0.8040737148399612,2.73K - 2.82K
K,0.8050436469447139,2.82K - 2.91K
K,0.8060135790494666,2.91K - 3K
K,0.8069835111542192,3K - 3.09K
K,0.8079534432589719,3.09K - 3.18K
K,0.8089233753637245,"0
100k
200k"
K,0.8098933074684772,added_dti
K,0.8108632395732299,"Score
Density"
K,0.8118331716779825,"0
1M
2M
3M
4M
5M
6M
7M −0.2 0 0.2 0.4"
K,0.8128031037827352,100 - 233K
K,0.8137730358874878,233K - 465K
K,0.8147429679922406,465K - 698K
K,0.8157129000969933,698K - 931K
K,0.8166828322017459,931K - 1.16M
K,0.8176527643064986,1.16M - 1.4M
K,0.8186226964112512,1.4M - 1.63M
K,0.8195926285160039,1.63M - 1.86M
K,0.8205625606207565,1.86M - 2.09M
K,0.8215324927255092,2.09M - 2.33M
K,0.8225024248302619,2.33M - 2.56M
K,0.8234723569350145,2.56M - 2.79M
K,0.8244422890397672,2.79M - 3.03M
K,0.8254122211445198,3.03M - 3.26M
K,0.8263821532492726,3.26M - 3.49M
K,0.8273520853540253,3.49M - 3.72M
K,0.8283220174587779,3.72M - 3.96M
K,0.8292919495635306,3.96M - 4.19M
K,0.8302618816682832,4.19M - 4.42M
K,0.8312318137730359,4.42M - 4.65M
K,0.8322017458777885,4.65M - 4.89M
K,0.8331716779825412,4.89M - 5.12M
K,0.8341416100872939,5.12M - 5.35M
K,0.8351115421920465,5.35M - 5.58M
K,0.8360814742967992,5.58M - 5.82M
K,0.8370514064015518,5.82M - 6.05M
K,0.8380213385063046,6.05M - 6.28M
K,0.8389912706110573,6.28M - 6.52M
K,0.8399612027158099,6.52M - 6.75M
K,0.8409311348205626,6.75M - 6.98M
K,0.8419010669253152,6.98M - 7.21M
K,0.8428709990300679,7.21M - 7.45M
K,0.8438409311348206,"0
100k
200k"
K,0.8448108632395732,annual_inc
K,0.8457807953443259,"Score
Density −0.2 0 0.2 0.4 car"
K,0.8467507274490785,credit_card
K,0.8477206595538312,debt_consolidation
K,0.8486905916585838,educational
K,0.8496605237633366,home_improvement house
K,0.8506304558680893,major_purchase
K,0.8516003879728419,medical
K,0.8525703200775946,moving other
K,0.8535402521823472,renewable_energy
K,0.8545101842870999,small_business
K,0.8554801163918526,vacation
K,0.8564500484966052,wedding
K,0.8574199806013579,"0
50k
100k
150k"
K,0.8583899127061105,purpose
K,0.8593598448108632,"Score
Density"
K,0.8603297769156159,"Figure 17: Feature importances and selected univariate functions fj for the Explainable Boosting
Machine with max bins = 8 on the Lending Club dataset."
K,0.8612997090203686,Under review as a conference paper at ICLR 2022
K,0.8622696411251213,"0.0
0.5
1.0
1.5
2.0
 ball radius r 0.84 0.86 0.88 0.90 0.92"
K,0.8632395732298739,max deviation in probabilities D
K,0.8642095053346266,"12 nonzeros, 1 norm = 1.6"
K,0.8651794374393792,"D
lim
r
D 100"
NORM,0.8661493695441319,1 norm 0.2 0.3 0.4 0.5 0.6 0.7 0.8
NORM,0.8671193016488846,r = 0.2
NORM,0.8680892337536372,"Figure 18: Maximum deviation D for logistic regression models on the Lending Club dataset as a
function of certiﬁcation set size (radius r) and model smoothness (ℓ1 norm)."
NORM,0.8690591658583899,"0.0
0.5
1.0
1.5
2.0
 ball radius r 0.24 0.26 0.28 0.30 0.32 0.34"
NORM,0.8700290979631425,max deviation in probabilities D
NORM,0.8709990300678953,maxBins = 8
NORM,0.871968962172648,"D
lim
r
D"
NORM,0.8729388942774006,"101
102
103"
NORM,0.8739088263821533,maxBins 0.240 0.245 0.250 0.255 0.260 0.265 0.270 0.275 0.280
NORM,0.8748787584869059,r = 0.2
NORM,0.8758486905916586,"Figure 19: Maximum deviation D for Explainable Boosting Machines on the Lending Club dataset
as a function of certiﬁcation set size (radius r) and model smoothness (max bins parameter)."
NORM,0.8768186226964112,"101
102
103"
NORM,0.8777885548011639,Number of leaf nodes 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
NORM,0.8787584869059166,max deviation D
NORM,0.8797284190106692,Decision Tree
NORM,0.8806983511154219,(a) Decision Tree
NORM,0.8816682832201745,"0
100
200
300
400
500
Number of estimators in Random Forest 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
NORM,0.8826382153249273,D w.r.t. single Decision Tree
NORM,0.88360814742968,"dual bound
primal bound"
NORM,0.8845780795344326,(b) Random Forest
NORM,0.8855480116391853,Figure 20: Maximum deviations computed for tree and tree ensembles on the Lending Club dataset
NORM,0.8865179437439379,"Maximum deviation summary
Figures 18–20 show maximum deviation as functions of certiﬁ-
cation set radius r and model complexity parameters, in a similar manner as Figures 1 and 2 for the
Adult Income dataset. The qualitative patterns are similar to before: increasing maximum deviation
in all cases except with the number of RF estimators in Figure 20b, where the upper bound is stable
around 0.7. A major quantitative difference is that the maximum deviations for the GAM in Fig-
ure 19 are much lower than for the other models, in particular LR in Figure 18. This is likely due to"
NORM,0.8874878758486906,Under review as a conference paper at ICLR 2022 10 0 10 20 30
NORM,0.8884578079534433,log-odds
NORM,0.8894277400581959,leaf 5
NORM,0.8903976721629486,"min
min r"
NORM,0.8913676042677012,"max
max r
reference 10 0 10 20 30"
NORM,0.8923375363724539,log-odds
NORM,0.8933074684772065,leaf 7 10 5 0 5 10 15 20
NORM,0.8942774005819593,log-odds
NORM,0.895247332686712,leaf 9 10 0 10 20 30
NORM,0.8962172647914646,log-odds
NORM,0.8971871968962173,leaf 10 10 5 0 5 10 15 20
NORM,0.8981571290009699,log-odds
NORM,0.8991270611057226,leaf 11 10 0 10 20 30
NORM,0.9000969932104753,log-odds
NORM,0.9010669253152279,leaf 12
NORM,0.9020368574199806,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 10 0 10 20 30"
NORM,0.9030067895247332,log-odds
NORM,0.903976721629486,leaf 13
NORM,0.9049466537342385,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 10 0 10 20 30"
NORM,0.9059165858389913,log-odds
NORM,0.906886517943744,leaf 14
NORM,0.9078564500484966,"Figure 21: Minimum and maximum predicted log-odds for a logistic regression model with inverse
ℓ1 penalty C = 0.01 on the Lending Club dataset, as a function of certiﬁcation set size (radius r)
and broken down by leaves of the decision tree reference model."
NORM,0.9088263821532493,"the fact that the GAM functions fj in Figure 17 are bounded while still being monotonic, unlike the
linear functions wjxj in the LR model."
NORM,0.9097963142580019,"Breakdown by leaves of f0
Figures 21–24 show a breakdown of the deviations for LR and GAM
by leaves of the reference model, similar to Figures 9–12 and again on the log-odds scale. One dif-
ference is that in Figure 21, the deviations for ﬁnite r do not come close to their r →∞counterparts
in most cases. In Figure 23 however, the r →∞values are all attained when r is slightly greater
than 1."
NORM,0.9107662463627546,Under review as a conference paper at ICLR 2022 0 5 10 15 20
NORM,0.9117361784675073,log-odds
NORM,0.9127061105722599,leaf 5
NORM,0.9136760426770126,"min
max
reference 2 1 0 1 2 3"
NORM,0.9146459747817652,log-odds
NORM,0.915615906886518,leaf 7 1 0 1 2 3 4 5
NORM,0.9165858389912707,log-odds
NORM,0.9175557710960233,leaf 9 2 1 0 1 2
NORM,0.918525703200776,log-odds
NORM,0.9194956353055286,leaf 10 0 1 2 3 4 5 6
NORM,0.9204655674102813,log-odds
NORM,0.9214354995150339,leaf 11 1 0 1 2
NORM,0.9224054316197866,log-odds
NORM,0.9233753637245393,leaf 12
NORM,0.9243452958292919,"0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0"
NORM,0.9253152279340446,1 norm 5 0 5 10 15 20
NORM,0.9262851600387972,log-odds
NORM,0.92725509214355,leaf 13
NORM,0.9282250242483027,"0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0"
NORM,0.9291949563530553,1 norm 5 0 5 10 15 20
NORM,0.930164888457808,log-odds
NORM,0.9311348205625606,leaf 14
NORM,0.9321047526673133,"Figure 22: Minimum and maximum predicted log-odds for logistic regression models with different
ℓ1 penalties C on the Lending Club dataset, broken down by leaves of the decision tree reference
model. The certiﬁcation set ℓ∞ball radius is r = 0.2."
NORM,0.933074684772066,Under review as a conference paper at ICLR 2022 0.5 1.0 1.5 2.0 2.5 3.0
NORM,0.9340446168768186,log-odds
NORM,0.9350145489815713,leaf 5
NORM,0.9359844810863239,"min
min r"
NORM,0.9369544131910766,"max
max r
reference 0.0 0.5 1.0 1.5 2.0"
NORM,0.9379243452958292,log-odds
NORM,0.938894277400582,leaf 7 0.0 0.5 1.0 1.5 2.0
NORM,0.9398642095053347,log-odds
NORM,0.9408341416100873,leaf 9 0.0 0.5 1.0 1.5 2.0
NORM,0.94180407371484,log-odds
NORM,0.9427740058195926,leaf 10 0.5 1.0 1.5 2.0 2.5
NORM,0.9437439379243453,log-odds
NORM,0.944713870029098,leaf 11 0.0 0.5 1.0 1.5 2.0 2.5
NORM,0.9456838021338506,log-odds
NORM,0.9466537342386033,leaf 12
NORM,0.9476236663433559,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 0.5 1.0 1.5 2.0 2.5 3.0"
NORM,0.9485935984481086,log-odds
NORM,0.9495635305528612,leaf 13
NORM,0.950533462657614,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 0.0 0.5 1.0 1.5 2.0 2.5"
NORM,0.9515033947623667,log-odds
NORM,0.9524733268671193,leaf 14
NORM,0.953443258971872,"Figure 23: Minimum and maximum predicted log-odds for an Explainable Boosting Machine with
max bins = 8 on the Lending Club dataset, as a function of certiﬁcation set size (radius r) and
broken down by leaves of the decision tree reference model."
NORM,0.9544131910766246,Under review as a conference paper at ICLR 2022 1.0 1.5 2.0 2.5 3.0 3.5
NORM,0.9553831231813773,log-odds
NORM,0.95635305528613,leaf 5
NORM,0.9573229873908826,"min
max
reference 0.0 0.5 1.0 1.5 2.0"
NORM,0.9582929194956353,log-odds
NORM,0.9592628516003879,leaf 7 0.0 0.5 1.0 1.5 2.0
NORM,0.9602327837051406,log-odds
NORM,0.9612027158098934,leaf 9 0.5 0.0 0.5 1.0 1.5
NORM,0.962172647914646,log-odds
NORM,0.9631425800193987,leaf 10 0.5 1.0 1.5 2.0 2.5
NORM,0.9641125121241513,log-odds
NORM,0.965082444228904,leaf 11 0.0 0.5 1.0 1.5 2.0
NORM,0.9660523763336566,log-odds
NORM,0.9670223084384093,leaf 12
NORM,0.967992240543162,"101
102
103"
NORM,0.9689621726479146,maxBins 0.5 1.0 1.5 2.0 2.5 3.0
NORM,0.9699321047526673,log-odds
NORM,0.9709020368574199,leaf 13
NORM,0.9718719689621726,"101
102
103"
NORM,0.9728419010669254,maxBins 0.5 1.0 1.5 2.0 2.5 3.0
NORM,0.973811833171678,log-odds
NORM,0.9747817652764307,leaf 14
NORM,0.9757516973811833,"Figure 24: Minimum and maximum predicted log-odds for Explainable Boosting Machines with
different max bins values on the Lending Club dataset, broken down by leaves of the decision tree
reference model. The certiﬁcation set ℓ∞ball radius is r = 0.2."
NORM,0.976721629485936,Under review as a conference paper at ICLR 2022
NORM,0.9776915615906887,"Running time, maximal cliques evaluated
Figure 25 shows the time required to compute the
maximum deviation for LR and GAM on the Lending Club dataset. Figure 26 shows the number of
K + 1-maximal cliques evaluated for DT and RF as well as the number of nodes in the graph. The
observations are the same as in Figures 13 and 14."
NORM,0.9786614936954413,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 0 500 1000 1500 2000 2500 3000 3500 4000"
NORM,0.979631425800194,time [sec]
NORM,0.9806013579049466,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
 ball radius r 0 1000 2000 3000 4000 5000"
NORM,0.9815712900096993,time [sec]
NORM,0.9825412221144519,"Figure 25: Time to compute maximum deviation for logistic regression models (left) and Explain-
able Boosting Machines (right) on the Lending Club dataset as a function of certiﬁcation set size
(radius r). The ﬁlled-in region shows the min-max variation with model complexity (ℓ1 norm for
LR, max bins for EBM)."
NORM,0.9835111542192047,"0
1000
2000
3000
4000
5000
Number of leaf nodes in Decision Tree 0 50 100 150 200"
NORM,0.9844810863239574,K + 1 max cliques evaluated
NORM,0.98545101842871,(a) Decision Tree
NORM,0.9864209505334627,"0
100
200
300
400
500
Number of estimators in Random Forest 50 100 150 200 250"
NORM,0.9873908826382153,K + 1 max cliques evaluated 0 2000 4000 6000 8000 10000 12000 14000 16000
NORM,0.988360814742968,# Nodes | |
NORM,0.9893307468477207,(b) Random Forest
NORM,0.9903006789524733,Figure 26: Effectiveness of pruning by bound for tree-based models on the Lending Club dataset
NORM,0.991270611057226,"Feature combinations that maximize deviation
Tables 7 and 8 present the feature values that
maximize deviation for LR and GAM respectively. For both tables, the minimum log-odds over leaf
5 of the DT reference model is selected (corresponding to Figures 21 and 23, leaf 5, blue curve)
because this choice maximizes the deviation overall for most values of r."
NORM,0.9922405431619786,"The previous trend toward extreme feature values that minimize log-odds also holds here as r in-
creases. For example, debt-to-income ratios increase to outlier values above 100%, annual income
drops to the minimum of $100, the term changes to 60 months in Table 7 for r > 1, and the pur-
pose changes to small business, the category with the lowest log-odds in Figure 17. The decrease
in income and increases in debt-to-income ratios are qualitatively in accordance with each other.
However, these quantities are related to each other by deterministic formulas (at least in theory) that
also involve the interest rate and installment amount. It is not clear whether the values in Tables 7
and 8 violate these relationships. This may be an instance that could beneﬁt from constraints on
possible feature combinations, as brieﬂy mentioned in Appendix C."
NORM,0.9932104752667313,Under review as a conference paper at ICLR 2022
NORM,0.9941804073714839,"r
dti
annual inc
term
purpose
int rate
credit history"
NORM,0.9951503394762367,"0.0
505
1700
36 months
credit card
7.4
2557
0.1
506
100
36 months
credit card
6.9
2283
0.2
507
100
36 months
credit card
6.4
2009
0.4
884
100
36 months
debt consolidation
10.1
3897
0.6
886
100
36 months
debt consolidation
9.1
3349
0.8
889
100
36 months
debt consolidation
8.2
2801
0.999
891
100
36 months
debt consolidation
7.2
2256
1.001
891
100
60 months
small business
7.2
2251
1.2
893
100
60 months
small business
6.3
1706
1.4
895
100
60 months
small business
5.3
1158
1.6
898
100
60 months
small business
5.3
1095
1.8
900
100
60 months
small business
5.3
1095
2.0
902
100
60 months
small business
5.3
1095
∞
999
100
60 months
small business
5.3
1095"
NORM,0.9961202715809894,"Table 7: Feature values that minimize log-odds for a logistic regression model (C = 0.01) over leaf
5 of the decision tree reference model for the Lending Club dataset. The 6 features that contribute
most to the minimum are shown as a function of certiﬁcation set radius r."
NORM,0.997090203685742,"r
term
int rate
dti
purpose
annual inc
added dti"
NORM,0.9980601357904947,"0.0
60 months
9.9
46.4
debt consolidation
35000
25.5
0.1
60 months
[10.4 11. ]
[28.6 30.9]
debt consolidation
[31800 38001]
[10. 11.2]
0.2
60 months
[12. 13.1]
[27.8 31.8]
debt consolidation
[36600 38001]
[10. 11.7]
0.4
60 months
[13.6 14.3]
[27.8 30.4]
small business
[ 100 38001]
[12.7 15.3]
0.6
60 months
[15.6 16.3]
[27.8 36.1]
small business
[19801 38001]
[12.7 15. ]
0.8
60 months
[15.6 16.4]
[27.8 28.4]
small business
[14402 38001]
[12.7 15.7]
0.999
60 months
[18.2 18.4]
[27.8 38.8]
small business
[33069 38001]
[12.7 14.5]
1.001
60 months
[18.2 18.8]
[27.8 35.3]
small business
[ 935 38001]
[12.7 18.5]
1.2
60 months
[18.2 20.2]
[27.8 33.3]
small business
[ 7602 38001]
[12.7 18.1]
1.4
60 months
[18.2 21.2]
[27.8 35.6]
small business
[ 100 38001]
[12.7 20.4]
1.6
60 months
[18.2 19.2]
[27.8 29.9]
small business
[ 100 38001]
[12.7 24.5]
1.8
60 months
[18.2 26.1]
[27.8 28.5]
small business
[ 100 38001]
[12.7 24.4]
2.0
60 months
[18.2 27.1]
[27.8 30.7]
small business
[ 100 38001]
[12.7 26.6]
∞
60 months
[18.2 31. ]
[ 27.8 999. ]
small business
[ 100 38001]
[ 12.7 3179.3]"
NORM,0.9990300678952473,"Table 8: Feature values that minimize log-odds for an Explainable Boosting Machine (max bins
= 8) over leaf 5 of the decision tree reference model for the Lending Club dataset. The 6 features
that contribute most to the minimum are shown as a function of certiﬁcation set radius r. For r > 0,
the minimizing values of continuous features form an interval because the corresponding functions
fj are piecewise constant."
