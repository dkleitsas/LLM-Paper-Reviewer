Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013986013986013986,"Conformal Inference (CI) is a popular approach for generating finite sample prediction
intervals based on the output of any point prediction method when data are exchangeable.
Adaptive Conformal Inference (ACI) algorithms extend CI to the case of sequentially observed
data, such as time series, and exhibit strong theoretical guarantees without having to assume
exchangeability of the observed data. The common thread that unites algorithms in the ACI
family is that they adaptively adjust the width of the generated prediction intervals in response to
the observed data. We provide a detailed description of five ACI algorithms and their theoretical
guarantees, and test their performance in simulation studies. We then present a case study
of producing prediction intervals for influenza incidence in the United States based on black-
box point forecasts. Implementations of all the algorithms are released as an open-source
R package, AdaptiveConformal, which also includes tools for visualizing and summarizing
conformal prediction intervals."
OTHER,0.002797202797202797,"Keywords: Conformal inference, Adaptive conformal inference, time series, R"
OTHER,0.004195804195804196,Contents
OTHER,0.005594405594405594,"1
Introduction
2"
OTHER,0.006993006993006993,"2
Theoretical Framework
3
2.1
Linear Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Quantile Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.3
Online Learning Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.4
Assessing ACI algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5"
OTHER,0.008391608391608392,1Corresponding author: herbps10@gmail.com
OTHER,0.009790209790209791,"3
Algorithms
6
3.1
Adaptive Conformal Inference (ACI) . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.1.1
Theoretical Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.1.2
Tuning Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.2
Aggregated Adaptive Conformal Inference (AgACI) . . . . . . . . . . . . . . . . . .
8
3.2.1
Theoretical Gaurantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.2.2
Tuning Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.3
Dynamically-tuned Adaptive Conformal Inference (DtACI) . . . . . . . . . . . . . .
11
3.3.1
Theoretical Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.3.2
Tuning parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.4
Scale-Free Online Gradient Descent (SF-OGD) . . . . . . . . . . . . . . . . . . . . .
13
3.4.1
Theoretical Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.4.2
Tuning parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.5
Strongly Adaptive Online Conformal Prediction (SAOCP) . . . . . . . . . . . . . . .
15
3.5.1
Theoretical Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.5.2
Tuning Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
OTHER,0.011188811188811189,"4
AdaptiveConformal R package
19"
OTHER,0.012587412587412588,"5
Simulation Studies
20
5.1
Time series with ARMA errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
5.2
Distribution shift
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
OTHER,0.013986013986013986,"6
Case Study: Influenza Forecasting
29"
OTHER,0.015384615384615385,"7
Discussion
34
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36"
OTHER,0.016783216783216783,"References
36"
OTHER,0.01818181818181818,"8
Appendix
38
8.1
Additional simulation study results
. . . . . . . . . . . . . . . . . . . . . . . . . . .
38"
INTRODUCTION,0.019580419580419582,"1
Introduction"
INTRODUCTION,0.02097902097902098,"Conformal Inference (CI) is a family of methods for generating finite sample prediction intervals
around point predictions when data are exchangeable (Vovk, Gammerman, and Shafer 2005; Shafer
and Vovk 2008; Angelopoulos and Bates 2023). The input point predictions can be derived from
any prediction method, making CI a powerful tool for augmenting black-box prediction algorithms
with prediction intervals. Classical CI methods are able to yield marginally valid intervals with
only the assumption that the joint distribution of the data does not change based on the order of
the observations (that is, they are exchangeable). However, in many real-world settings data are
not exchangeable: for example, time series data usually cannot be assumed to be exchangeable due
to temporal dependence. A recent line of research examines the problem of generating prediction
intervals for observations that are observed online (that is, one at a time) and for which exchangeability
is not assumed to hold (Gibbs and Candes 2021; Zaffran et al. 2022; Gibbs and CandÃ¨s 2022; Bhatnagar
et al. 2023). The methods from this literature, which we refer to generally as Adaptive Conformal
Inference (ACI) algorithms, work by adaptively adjusting the width of the generated prediction
intervals in response to the observed data."
INTRODUCTION,0.022377622377622378,"Informally, suppose a sequence of outcomes ğ‘¦ğ‘¡âˆˆâ„, ğ‘¡= 1, â€¦ , ğ‘‡are observed one at a time. Before"
INTRODUCTION,0.023776223776223775,"seeing each observation, we have at our disposal a point prediction Ì‚ğœ‡ğ‘¡âˆˆâ„that can be generated by
any method. Our goal is to find an algorithm for producing prediction intervals [â„“ğ‘¡, ğ‘¢ğ‘¡], â„“ğ‘¡â‰¤ğ‘¢ğ‘¡such
that, in the long run, the observations ğ‘¦ğ‘¡fall within the corresponding prediction intervals roughly
ğ›¼Ã— 100% of the time: that is, limğ‘‡â†’âˆ1/ğ‘‡âˆ‘ğ‘‡
ğ‘¡=1 ğ•€{ğ‘¦ğ‘¡âˆˆ[â„“ğ‘¡, ğ‘¢ğ‘¡]} = ğ›¼. The original ACI algorithm (Gibbs
and Candes 2021) is based on a simple idea: if the previous prediction interval at time (ğ‘¡âˆ’1) did
not cover the true observation, then the next prediction interval at time ğ‘¡is made slightly wider.
Conversely, if the previous prediction interval did include the observation, then the next prediction
interval is made slightly narrower. It can be shown that this procedure yields prediction intervals
that in the long run cover the true observations the desired proportion of the time."
INTRODUCTION,0.025174825174825177,"The main tuning parameter of the original ACI algorithm is a learning rate that controls how fast
prediction interval width changes. If the learning rate is too low, then the prediction intervals
will not be able to adapt fast enough to shifts in the data generating distribution; if it is too large,
then the intervals will oscillate widely. The critical dependence of the original ACI algorithm on
proper choice of its learning rate spurred subsequent research into meta-algorithms that learn the
correct learning rate (or an analogue thereof) in various ways, typically drawing on approaches
from the online learning literature. In this paper, we present four such algorithms: Aggregated ACI
(AgACI, Zaffran et al. 2022), Dynamically-tuned Adaptive ACI (DtACI, Gibbs and CandÃ¨s 2022),
Scale-Free Online Gradient Descent (SF-OGD, Bhatnagar et al. 2023), and Strongly Adaptive Online
Conformal Prediction (SAOCP, Bhatnagar et al. 2023). We note that the adaption of conformal
inference techniques is an active area of research and the algorithms we focus on in this work are not
exhaustive; see among others Feldman et al. (2023), Bastani et al. (2022), Xu and Xie (2021), Xu and
Xie (2023), Angelopoulos, Barber, and Bates (2024), Zhang, Bombara, and Yang (2024), and Gasparin
and Ramdas (2024)."
INTRODUCTION,0.026573426573426574,"Our primary practical contribution is an implementation of each algorithm in an open source R
package, AdaptiveConformal, which is available at https://github.com/herbps10/AdaptiveConformal.
The package also includes routines for visualization and summary of the prediction intervals. We
note that Python versions of several algorithms were also made available by Zaffran et al. (2022)
and Bhatnagar et al. (2023), but to our knowledge this is the first package implementing them
in R. In addition, several R packages exist for conformal inference in other contexts, including
conformalInference focusing on regression (Tibshirani et al. 2019), conformalInference.fd, with
methods for functional responses (Diquigiovanni et al. 2022), and cfcausal for causal inference
related functionals (Lei and CandÃ¨s 2021). Our second practical contribution is to compare the
performance of the algorithms in simulation studies and in a case study generating prediction
intervals for influenza incidence in the United States based on black-box point forecasts."
INTRODUCTION,0.027972027972027972,"The rest of the paper unfolds as follows. In Section 2, we present a unified theoretical framework
for analyzing the ACI algorithms based on the online learning paradigm. In Section 3 we provide
descriptions of each algorithm along with their known theoretical properties. In Section 5 we compare
the performance of the algorithms in several simulation studies. Section 6 gives a case study based
on forecasting influenza in the United States. Finally, Section 7 provides a discussion and ideas for
future research in this rapidly expanding field."
LIT REVIEW,0.02937062937062937,"2
Theoretical Framework"
LIT REVIEW,0.03076923076923077,"Notation: for any integer ğ‘â‰¥1 let Jğ‘K âˆ¶= {1, â€¦ , ğ‘}. Let ğ•€be the indicator function. Let âˆ‡ğ‘“denote
the gradient (subgradient) of the differentiable (convex) function ğ‘“."
LIT REVIEW,0.032167832167832165,"We consider an online learning scenario in which we gain access to a sequence of observations (ğ‘¦ğ‘¡)ğ‘¡â‰¥1
one at a time (see Cesa-Bianchi and Lugosi (2006) for an comprehensive account of online learning
theory). Fix ğ›¼âˆˆ(0, 1) to be the target empirical coverage of the prediction intervals. The goal is"
LIT REVIEW,0.033566433566433566,"to output at time ğ‘¡a prediction interval for the unseen observation ğ‘¦ğ‘¡, with the prediction interval
generated by an interval construction function
Ì‚ğ¶ğ‘¡. Formally, let Ì‚ğ¶ğ‘¡be a function that takes as input a
parameter ğœƒğ‘¡âˆˆâ„and outputs a closed prediction interval [â„“ğ‘¡, ğ‘¢ğ‘¡]. The interval construction function
must be nested: if ğœƒâ€² > ğœƒ, then
Ì‚ğ¶ğ‘¡(ğœƒ) âŠ†
Ì‚ğ¶ğ‘¡(ğœƒâ€²). In words, larger values of ğœƒimply wider prediction
intervals. The interval constructor is indexed by ğ‘¡to emphasize that it may use other information at
each time point, such as a point prediction Ì‚ğœ‡ğ‘¡âˆˆâ„. We make no restrictions on how this external
information is generated."
LIT REVIEW,0.03496503496503497,"Define ğ‘Ÿğ‘¡âˆ¶= inf{ğœƒâˆˆâ„âˆ¶ğ‘¦ğ‘¡âˆˆ
Ì‚ğ¶ğ‘¡(ğœƒ)} to be the radius at time ğ‘¡. The radius is the smallest possible ğœƒ
such that the prediction interval covers the observation ğ‘¦ğ‘¡. A key assumption for the theoretical
analysis of several of the algorithms is that the radii are bounded:"
LIT REVIEW,0.03636363636363636,Assumption: there exists a finite ğ·> 0 such that ğ‘Ÿğ‘¡< ğ·for all ğ‘¡.
LIT REVIEW,0.03776223776223776,"If the outcome space is bounded, then ğ·can be easily chosen to cover the entire space. Next, we
describe two existing definitions of interval construction functions."
LIT REVIEW,0.039160839160839164,"2.1
Linear Intervals"
LIT REVIEW,0.04055944055944056,"A simple method for forming the prediction intervals is to use the parameter ğœƒğ‘¡to directly define the
width of the interval. Suppose that at each time ğ‘¡we have access to a point prediction Ì‚ğœ‡ğ‘¡âˆˆâ„. Then
we can form a symmetric prediction interval around the point estimate using"
LIT REVIEW,0.04195804195804196,"ğœƒâ†¦
Ì‚ğ¶ğ‘¡(ğœƒ) âˆ¶= [ Ì‚ğœ‡ğ‘¡âˆ’ğœƒ, Ì‚ğœ‡ğ‘¡+ ğœƒ]."
LIT REVIEW,0.043356643356643354,"We refer to this as the linear interval constructor. Note that in this case, the radius is simply the
absolute residual ğ‘Ÿğ‘¡= | Ì‚ğœ‡ğ‘¡âˆ’ğ‘¦ğ‘¡|."
LIT REVIEW,0.044755244755244755,"2.2
Quantile Intervals"
LIT REVIEW,0.046153846153846156,"The original ACI paper proposed constructing intervals based on the previously observed residuals
(Gibbs and Candes 2021). Let ğ‘†âˆ¶â„2 â†’â„be a function called a nonconformity score. A popular
choice of nonconformity score is the absolute residual: (ğœ‡, ğ‘¦) â†¦ğ‘†(ğœ‡, ğ‘¦) âˆ¶= |ğœ‡âˆ’ğ‘¦|. Let ğ‘ ğ‘¡âˆ¶= ğ‘†( Ì‚ğœ‡ğ‘¡, ğ‘¦ğ‘¡)
be the nonconformity score of the ğ‘¡th-observation. The quantile interval construction function is
then given by"
LIT REVIEW,0.04755244755244755,"Ì‚ğ¶ğ‘¡(ğœƒğ‘¡) âˆ¶= [ Ì‚ğœ‡ğ‘¡âˆ’Quantile(ğœƒ, {ğ‘ 1, â€¦ , ğ‘ ğ‘¡âˆ’1}), Ì‚ğœ‡ğ‘¡+ Quantile(ğœƒ, {ğ‘ 1, â€¦ , ğ‘ ğ‘¡âˆ’1})]"
LIT REVIEW,0.04895104895104895,"where Quantile(ğœƒ, ğ´) denotes the empirical ğœƒ-quantile of the elements in the set ğ´. Note that
Ì‚ğ¶ğ‘¡is indeed nested in ğœƒğ‘¡because the Quantile function is non-decreasing in ğœƒ. Note we define
Ì‚ğ¶ğ‘¡(1) = max{ğ‘ 1, â€¦ , ğ‘ ğ‘¡âˆ’1} rather than
Ì‚ğ¶ğ‘¡(1) = âˆin order to avoid practical problems with trivial
prediction intervals (Zaffran et al. 2022). Note that we can always choose ğ·= 1 to satisfy the
outcome boundedness assumption given above."
LIT REVIEW,0.05034965034965035,"We focus on the above definition for the quantile interval construction function which is designed to
be symmetric around the point prediction Ì‚ğœ‡ğ‘¡. However, we note it is possible to take a more general
definition, such as
Ì‚ğ¶ğ‘¡(ğœƒğ‘¡) âˆ¶= {ğ‘¦âˆ¶ğ‘†( Ì‚ğœ‡ğ‘¡, ğ‘¦) â‰¤Quantile(, {s1, â€¦ , stâˆ’1})}"
LIT REVIEW,0.05174825174825175,Such an approach allows for prediction intervals that may not be centered on Ì‚ğœ‡ğ‘¡.
LIT REVIEW,0.05314685314685315,"Our proposed AdaptiveConformal package takes the absolute residual as the default nonconformity
score, although the user may also specify any custom nonconformity score by supplying it as an R
function."
LIT REVIEW,0.05454545454545454,"2.3
Online Learning Framework"
LIT REVIEW,0.055944055944055944,"We now introduce a loss function that defines the quality of a prediction interval with respect to a
realized observation. Define the pinball loss ğ¿ğ›¼as"
LIT REVIEW,0.057342657342657345,"(ğœƒ, ğ‘Ÿ) â†¦ğ¿ğ›¼(ğœƒ, ğ‘Ÿ) âˆ¶= {(1 âˆ’ğ›¼)(ğœƒâˆ’ğ‘Ÿ),
ğœƒâ‰¥ğ‘Ÿ
ğ›¼(ğ‘Ÿâˆ’ğœƒ),
ğœƒ< ğ‘Ÿ."
LIT REVIEW,0.05874125874125874,The way in which the algorithm gains access to the data and incurs losses is as follows:
LIT REVIEW,0.06013986013986014,"â€¢ Sequentially, for ğ‘¡= 1, â€¦ , ğ‘‡:"
LIT REVIEW,0.06153846153846154,"â€“ Predict radius ğœƒğ‘¡and form prediction interval Ì‚ğ¶ğ‘¡(ğœƒğ‘¡).
â€“ Observe true outcome ğ‘¦ğ‘¡and calculate radius ğ‘Ÿğ‘¡.
â€“ Record errğ‘¡âˆ¶= ğ•€[ğ‘¦ğ‘¡âˆ‰
Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)].
â€“ Incur loss ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡)."
LIT REVIEW,0.06293706293706294,"This iterative procedure is at the core of the online learning theoretical framework in which theoretical
results have been derived."
LIT REVIEW,0.06433566433566433,"2.4
Assessing ACI algorithms"
LIT REVIEW,0.06573426573426573,"There are two different perspectives we can take in measuring the quality of an ACI algorithm
that generates a sequence (ğœƒğ‘¡)ğ‘¡âˆˆJğ‘‡K. First, we could look at how close the empirical coverage of the
generated prediction intervals is to the desired coverage level ğ›¼. Formally, define the empirical
coverage as the proportion of observations that fell within the corresponding prediction interval:
EmpCov(ğ‘‡) âˆ¶= 1"
LIT REVIEW,0.06713286713286713,"ğ‘‡âˆ‘ğ‘‡
ğ‘¡=1(1 âˆ’errğ‘¡). The coverage error is then given by"
LIT REVIEW,0.06853146853146853,CovErr(ğ‘‡) âˆ¶= EmpCov(ğ‘‡) âˆ’ğ›¼.
LIT REVIEW,0.06993006993006994,"The second perspective is to look at how well the algorithm controls the incurred pinball losses.
Following the classical framework from the online learning literature, we define the regret as the
difference between the cumulative loss yielded by a sequence (ğœƒğ‘¡)ğ‘¡âˆˆJğ‘‡K versus the cumulative loss of
the best possible fixed choice:"
LIT REVIEW,0.07132867132867132,"Reg(ğ‘‡) âˆ¶=
ğ‘‡
âˆ‘
ğ‘¡=1
ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡) âˆ’inf
ğœƒâˆ—âˆˆâ„"
LIT REVIEW,0.07272727272727272,"ğ‘‡
âˆ‘
ğ‘¡=1
ğ¿ğ›¼(ğœƒâˆ—, ğ‘Ÿğ‘¡)."
LIT REVIEW,0.07412587412587412,"In settings of distribution shift, it may not be appropriate to compare the cumulative loss of an
algorithm to a fixed competitor. As such, stronger notions of regret have been defined. The strongly
adaptive regret is the largest regret over any subperiod of length ğ‘šâˆˆJğ‘‡K:"
LIT REVIEW,0.07552447552447553,"SAReg(ğ‘‡, ğ‘š) âˆ¶=
max
[ğœ,ğœ+ğ‘šâˆ’1]âŠ†Jğ‘‡K (
ğœ+ğ‘šâˆ’1
âˆ‘
ğ‘¡=ğœ
ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡) âˆ’inf
ğœƒâˆ—âˆˆâ„"
LIT REVIEW,0.07692307692307693,"ğœ+ğ‘šâˆ’1
âˆ‘
ğ‘¡=ğœ
ğ¿ğ›¼(ğœƒâˆ—, ğ‘Ÿğ‘¡)) ."
LIT REVIEW,0.07832167832167833,"Both ways of evaluating ACI methods are important because targeting only one or the other can lead
to algorithms that yield prediction intervals that are not practically useful. As a simple pathological
example of only targeting the coverage error, suppose we wish to generate ğ›¼= 50% prediction
intervals. We could choose to alternate ğœƒbetween 0 and âˆ, such that errğ‘¡alternates between 0 and 1.
The empirical coverage would then trivially converge to the desired level of 50%. However, the same
algorithm would yield infinite regret (see Bhatnagar et al. (2023) for a more in-depth example of an
scenario in which coverage is optimal but the regret grows linearly). On the other hand, an algorithm
that has arbitrarily small regret may not yield good empirical coverage. Suppose the observations and
point predictions are constant: ğ‘¦ğ‘¡= 1 and Ì‚ğœ‡ğ‘¡= 0 for all ğ‘¡â‰¥1. Consider a simple class of algorithms"
LIT REVIEW,0.07972027972027972,"that outputs constantly ğœƒğ‘¡= ğœƒâ€² for some ğœƒâ€² < 1. With the linear interval construction function, the
prediction intervals are then Ì‚ğ¶ğ‘¡(ğœƒğ‘¡) = [âˆ’ğœƒâ€², ğœƒâ€²]. The regret is given by Reg(ğ‘‡) = 2ğ‘‡ğ›¼(1 âˆ’ğœƒâ€²), which
approaches zero as ğœƒâ€² approaches 1. The empirical coverage is, however, always zero. In other words,
the regret can be arbitrarily close to zero while at the same time the empirical coverage does not
approach the desired level."
LIT REVIEW,0.08111888111888112,"These simple examples illustrate that, unfortunately, bounds on the coverage error and bounds on
the regret are not in general interchangeable. It is possible, however, to show equivalencies by either
(1) making distributional assumptions on the data or (2) using additional information about how the
algorithm produces the sequence (ğœƒğ‘¡)ğ‘¡âˆˆJğ‘‡K (Bhatnagar et al. 2023)."
LIT REVIEW,0.08251748251748252,"It may also be informative to summarize a set of prediction intervals in ways beyond their coverage
error or their regret. A common metric for prediction intervals is the mean interval width:"
LIT REVIEW,0.08391608391608392,MeanWidth(ğ‘‡) âˆ¶= 1 ğ‘‡
LIT REVIEW,0.08531468531468532,"ğ‘‡
âˆ‘
ğ‘¡=1
ğ‘¤ğ‘¡,"
LIT REVIEW,0.08671328671328671,where ğ‘¤ğ‘¡âˆ¶= ğ‘¢ğ‘¡âˆ’â„“ğ‘¡is the interval width at time ğ‘¡.
LIT REVIEW,0.08811188811188811,"Finally, we introduce a metric that is intended to capture pathological behavior that can arise with ACI
algorithms where the prediction intervals oscillate between being extremely narrow and extremely
wide. Define the path length of prediction intervals generated by an ACI algorithm as"
LIT REVIEW,0.08951048951048951,"PathLength(ğ‘‡) âˆ¶=
ğ‘‡
âˆ‘
ğ‘¡=2
|ğ‘¤ğ‘¡âˆ’ğ‘¤ğ‘¡âˆ’1|."
LIT REVIEW,0.09090909090909091,"A high path length indicates that the prediction intervals were variable over time, and a low path
length indicates the prediction intervals were stable."
IMPLEMENTATION/METHODS,0.09230769230769231,"3
Algorithms"
IMPLEMENTATION/METHODS,0.0937062937062937,"Table 1: Summary of ACI algorithms. Only the theoretical guarantees discussed in this paper are
shown for each algorithm."
IMPLEMENTATION/METHODS,0.0951048951048951,Algorithm
IMPLEMENTATION/METHODS,0.0965034965034965,"Adaptive Conformal Inference (ACI)
- Tuning parameters: learning rate ğ›¾
- Original interval constructor: quantile
- Theoretical guarantees: coverage error, regret
- Citation: Gibbs and Candes (2021)
Aggregated Adaptive Conformal Inference (AgACI)
- Tuning parameters: candidate learning rates (ğ›¾ğ‘˜)1â‰¤ğ‘˜â‰¤ğ¾
- Original interval constructor: quantile
- Citation: Zaffran et al. (2022)
Dynamically-tuned Adaptive Conformal Inference (DtACI)
- Tuning parameters: candidate learning rates (ğ›¾ğ‘˜)1â‰¤ğ‘˜â‰¤ğ¾
- Original interval constructor: quantile
- Theoretical guarantees: coverage error, strongly adaptive regret, dynamic regret
- Citation
Scale-Free Online Gradient Descent (SF-OGD)
- Tuning parameters: learning rate ğ›¾or maximum radius ğ·"
IMPLEMENTATION/METHODS,0.0979020979020979,Algorithm
IMPLEMENTATION/METHODS,0.0993006993006993,"- Original interval constructor: linear
- Theoretical guarantees: coverage error, anytime regret
- Citation: Bhatnagar et al. (2023)
Strongly Adaptive Online Conformal Prediction (SAOCP)
- Tuning parameters: learning rate ğ›¾, lifetime multiplier ğ‘”
- Original interval constructor: linear
- Theoretical guarantees: coverage error, strongly adaptive regret
- Citation: Bhatnagar et al. (2023)"
IMPLEMENTATION/METHODS,0.1006993006993007,"As a simple running example to illustrate each algorithm, we simulate independently ğ‘‡= 500 values
ğ‘¦1, â€¦ , ğ‘¦ğ‘‡following
ğ‘¦ğ‘¡âˆ¼ğ‘(0, ğœ2ğ‘¡),
ğ‘¡âˆˆJğ‘‡K,"
IMPLEMENTATION/METHODS,0.1020979020979021,"ğœğ‘¡= {0.2,
ğ‘¡â‰¤250,
0.1,
ğ‘¡> 250."
IMPLEMENTATION/METHODS,0.1034965034965035,"For demonstration purposes we assume we have access to unbiased predictions Ì‚ğœ‡ğ‘¡= 0 for all ğ‘¡âˆˆJğ‘‡K.
Throughout we set the target empirical coverage to ğ›¼= 0.8."
IMPLEMENTATION/METHODS,0.1048951048951049,"3.1
Adaptive Conformal Inference (ACI)"
IMPLEMENTATION/METHODS,0.1062937062937063,Algorithm 1 Adaptive Conformal Inference
IMPLEMENTATION/METHODS,0.1076923076923077,"1: Input: starting value ğœƒ1, learning rate ğ›¾> 0."
IMPLEMENTATION/METHODS,0.10909090909090909,"2: for ğ‘¡= 1, 2, â€¦ , ğ‘‡do"
IMPLEMENTATION/METHODS,0.11048951048951049,"3:
Output: prediction interval Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)."
IMPLEMENTATION/METHODS,0.11188811188811189,"4:
Observe ğ‘¦ğ‘¡."
IMPLEMENTATION/METHODS,0.11328671328671329,"5:
Evaluate errğ‘¡= ğ•€[ğ‘¦ğ‘¡âˆ‰
Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)]."
IMPLEMENTATION/METHODS,0.11468531468531469,"6:
Update ğœƒğ‘¡+1 = ğœƒğ‘¡+ ğ›¾(errğ‘¡âˆ’(1 âˆ’ğ›¼))."
IMPLEMENTATION/METHODS,0.11608391608391608,7: end for
IMPLEMENTATION/METHODS,0.11748251748251748,"The original ACI algorithm (Gibbs and Candes (2021); Algorithm 1 ) adaptively adjusts the width of
the prediction intervals in response to the observations. The updating rule for the estimated radius
can be derived as an online subgradient descent scheme. The subgradient of the pinball loss function
with respect to ğœƒis given by"
IMPLEMENTATION/METHODS,0.11888111888111888,"âˆ‡ğ¿ğ›¼(ğœƒ, ğ‘Ÿ) =
â§ â¨
â©"
IMPLEMENTATION/METHODS,0.12027972027972028,"{âˆ’ğ›¼},
ğœƒ< ğ‘Ÿ,
{1 âˆ’ğ›¼},
ğœƒ> ğ‘Ÿ,
[âˆ’ğ›¼, 1 âˆ’ğ›¼],
ğœƒ= ğ‘Ÿ"
IMPLEMENTATION/METHODS,0.12167832167832168,"It follows that, for all ğœƒğ‘¡âˆˆâ„and ğ‘Ÿğ‘¡âˆˆâ„,"
IMPLEMENTATION/METHODS,0.12307692307692308,"1 âˆ’ğ›¼âˆ’errğ‘¡âˆˆâˆ‡ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡)."
IMPLEMENTATION/METHODS,0.12447552447552447,This leads to the following update rule for ğœƒbased on subgradient descent:
IMPLEMENTATION/METHODS,0.1258741258741259,"ğœƒğ‘¡+1 = ğœƒğ‘¡+ ğ›¾(errğ‘¡âˆ’(1 âˆ’ğ›¼)),"
IMPLEMENTATION/METHODS,0.12727272727272726,"where ğ›¾> 0 is a user-specified learning rate. For intuition, note that if ğ‘¦ğ‘¡fell outside of the prediction
interval at time ğ‘¡(errğ‘¡= 1) then the next interval is widened (ğœƒğ‘¡+1 = ğœƒğ‘¡+ ğ›¾ğ›¼). On the contrary, if ğ‘¦ğ‘¡fell
within the interval (errğ‘¡= 0) then the next interval is shortened (ğœƒğ‘¡+1 = ğœƒğ‘¡âˆ’ğ›¾(1 âˆ’ğ›¼)). The learning
rate ğ›¾controls how fast the width of the prediction intervals changes in response to the data."
IMPLEMENTATION/METHODS,0.12867132867132866,"3.1.1
Theoretical Guarantees"
IMPLEMENTATION/METHODS,0.13006993006993006,"With the choice of the quantile interval structure, the ACI algorithm has the following finite sample
bound on the coverage error (Gibbs and Candes (2021); Proposition 4.1). For all ğ›¾> 0 (and so long as
ğ›¾does not depend on ğ‘‡),"
IMPLEMENTATION/METHODS,0.13146853146853146,"|CovErr(ğ‘‡)| â‰¤max{ğœƒ1, 1 âˆ’ğœƒ1} + ğ›¾ ğ›¾ğ‘‡
."
IMPLEMENTATION/METHODS,0.13286713286713286,"This result was originally shown for ACI with the choice of the quantile interval constructor, although
it can also be extended to other interval constructors Feldman et al. (2023). More generally, the
algorithm has the following coverage error bound in terms of the radius bound ğ·(Bhatnagar et al.
2023):"
IMPLEMENTATION/METHODS,0.13426573426573427,|CovErr(ğ‘‡)| â‰¤ğ·+ ğ›¾ ğ›¾ğ‘‡.
IMPLEMENTATION/METHODS,0.13566433566433567,"In addition, standard results for online subgradient descent yield the following regret bound with
the use of the linear interval constructor, assuming that the true radii are bounded by ğ·:"
IMPLEMENTATION/METHODS,0.13706293706293707,"Reg(ğ‘‡) â‰¤ğ’ª(ğ·2/ğ›¾+ ğ›¾ğ‘‡) â‰¤ğ’ª(ğ·âˆšğ‘‡),"
IMPLEMENTATION/METHODS,0.13846153846153847,"where the second inequality follows if the optimal choice (with respect to long-term regret) of
ğ›¾= ğ·/âˆšğ‘‡is used (Bhatnagar et al. 2023). Taken together, these theoretical results imply that while
the coverage error is guaranteed to converge to zero for any choice of ğ›¾, achieving sublinear regret
requires choosing ğ›¾more carefully. This highlights the importance of both ways of assessing ACI
algorithms: if we only focused on controlling the coverage error, we might not achieve optimal
control of regret, leading to intervals that are not practically useful."
IMPLEMENTATION/METHODS,0.13986013986013987,"3.1.2
Tuning Parameters"
IMPLEMENTATION/METHODS,0.14125874125874127,"Therefore, the main tuning parameter is the learning rate ğ›¾. The theoretical bounds on the coverage
error suggest setting a large ğ›¾such that the coverage error decays quickly in ğ‘‡; however, in practice
and setting ğ›¾too large will lead to intervals with large oscillations as seen in Figure 1. This is
quantified in the path length, which increases significantly as ğ›¾increases, even though the empirical
coverage remains near the desired value of 80%. On the other hand, setting ğ›¾too small will lead to
intervals that do not adapt fast enough to distribution shifts. Thus, choosing a good value for ğ›¾is
essential. However, the optimal choice ğ›¾= ğ·/âˆšğ‘‡cannot be used directly in practice unless the time
series length ğ‘‡is fixed in advance, or the so called â€œdoubling trickâ€ is used to relax the need to know
ğ‘‡in advance (Cesa-Bianchi and Lugosi (2006); Section 2.3)."
IMPLEMENTATION/METHODS,0.14265734265734265,"The theoretical results guaranteeing the performance of the ACI algorithm do not depend on the
choice of starting value ğœƒ1, and thus in practice any value can be chosen. Indeed, the effect of the
choice of ğœƒ1 decays over time as a function of the chosen learning rate. In practice, substantive prior
information can be used to pick a reasonable starting value. By default, the AdaptiveConformal
package sets ğœƒ1 = ğ›¼when the quantile interval predictor is used, and ğœƒ1 = 0 otherwise, although
in both cases the user can supply their own starting value. The behavior of the early prediction
intervals in the examples (Figure 1) is driven by the small number of residuals available, which
makes the output of the quantile interval constructor sensitive to small changes in ğœƒ. In practice, a
warm-up period can be used before starting to produce prediction intervals so that the quantiles of
the residuals are more stable."
IMPLEMENTATION/METHODS,0.14405594405594405,"3.2
Aggregated Adaptive Conformal Inference (AgACI)"
IMPLEMENTATION/METHODS,0.14545454545454545,"The Aggregated ACI (AgACI; Algorithm 2 ) algorithm solves the problem of choosing a learning rate
for ACI by running multiple copies of the algorithm with different learning rates, and then separately"
IMPLEMENTATION/METHODS,0.14685314685314685,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.14825174825174825,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.14965034965034965,Î³ = 0.016
IMPLEMENTATION/METHODS,0.15104895104895105,"EmpCov = 84.4%
PathLength = 6.4"
IMPLEMENTATION/METHODS,0.15244755244755245,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.15384615384615385,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.15524475524475526,Î³ = 0.032
IMPLEMENTATION/METHODS,0.15664335664335666,"EmpCov = 82.0%
PathLength = 7.9"
IMPLEMENTATION/METHODS,0.15804195804195803,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.15944055944055943,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.16083916083916083,Î³ = 0.064
IMPLEMENTATION/METHODS,0.16223776223776223,"EmpCov = 80.8%
PathLength = 12.8"
IMPLEMENTATION/METHODS,0.16363636363636364,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.16503496503496504,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.16643356643356644,Î³ = 0.128
IMPLEMENTATION/METHODS,0.16783216783216784,"EmpCov = 80.4%
PathLength = 25.0"
IMPLEMENTATION/METHODS,0.16923076923076924,"Figure 1: Example 80% prediction intervals from the ACI algorithm for different choices of learning
rate ğ›¾and with ğœƒ1 = 0.8. Blue and red points are observations that fell inside and outside the
prediction intervals, respectively."
IMPLEMENTATION/METHODS,0.17062937062937064,Algorithm 2 Aggregated Adaptive Conformal Inference
IMPLEMENTATION/METHODS,0.17202797202797201,"1: Input: candidate learning rates (ğ›¾ğ‘˜)1â‰¤ğ‘˜â‰¤ğ¾, starting value ğœƒ1."
IMPLEMENTATION/METHODS,0.17342657342657342,"2: Initialize lower and upper BOA algorithms â„¬â„“âˆ¶= BOA(ğ›¼â†(1 âˆ’ğ›¼)/2) and â„¬ğ‘¢âˆ¶= BOA(ğ›¼â†
(1 âˆ’(1 âˆ’ğ›¼)/2))."
IMPLEMENTATION/METHODS,0.17482517482517482,"3: for ğ‘˜= 1, â€¦ , ğ¾do"
IMPLEMENTATION/METHODS,0.17622377622377622,"4:
Initialize ACI ğ’œğ‘˜= ACI(ğ›¼â†ğ›¼, ğ›¾â†ğ›¾ğ‘˜, ğœƒ1 â†ğœƒ1)."
IMPLEMENTATION/METHODS,0.17762237762237762,5: end for
IMPLEMENTATION/METHODS,0.17902097902097902,"6: for ğ‘¡= 1, 2, â€¦ , ğ‘‡do"
IMPLEMENTATION/METHODS,0.18041958041958042,"7:
for ğ‘˜= 1, â€¦ , ğ¾do"
IMPLEMENTATION/METHODS,0.18181818181818182,"8:
Retrieve candidate prediction interval [â„“ğ‘˜ğ‘¡, ğ‘¢ğ‘˜ğ‘¡] from ğ’œğ‘˜."
IMPLEMENTATION/METHODS,0.18321678321678322,"9:
end for"
IMPLEMENTATION/METHODS,0.18461538461538463,"10:
Compute aggregated lower bound Ìƒâ„“ğ‘¡âˆ¶= â„¬â„“((â„“ğ‘˜ğ‘¡âˆ¶ğ‘˜âˆˆ{1, â€¦ , ğ¾}))."
IMPLEMENTATION/METHODS,0.18601398601398603,"11:
Compute aggregated upper bound Ìƒğ‘¢ğ‘¡âˆ¶= â„¬ğ‘¢((ğ‘¢ğ‘˜ğ‘¡âˆ¶ğ‘˜âˆˆ{1, â€¦ , ğ¾}))."
IMPLEMENTATION/METHODS,0.1874125874125874,"12:
Output: prediction interval [ Ìƒâ„“ğ‘¡, Ìƒğ‘¢ğ‘¡]."
IMPLEMENTATION/METHODS,0.1888111888111888,"13:
Observe ğ‘¦ğ‘¡."
IMPLEMENTATION/METHODS,0.1902097902097902,"14:
for ğ‘˜= 1, â€¦ , ğ¾do"
IMPLEMENTATION/METHODS,0.1916083916083916,"15:
Update ğ’œğ‘˜with observation ğ‘¦ğ‘¡."
IMPLEMENTATION/METHODS,0.193006993006993,"16:
end for"
IMPLEMENTATION/METHODS,0.1944055944055944,"17:
Update â„¬â„“with observed outcome ğ‘¦ğ‘¡."
IMPLEMENTATION/METHODS,0.1958041958041958,"18:
Update â„¬ğ‘¢with observed outcome ğ‘¦ğ‘¡."
IMPLEMENTATION/METHODS,0.1972027972027972,19: end for
IMPLEMENTATION/METHODS,0.1986013986013986,"combining the lower and upper interval bounds using an online aggregation of experts algorithm
(Zaffran et al. 2022). That is, one aggregation algorithm seeks to estimate the lower (1âˆ’ğ›¼)/2 quantile,
and the other seeks to estimate the upper 1 âˆ’(1 âˆ’ğ›¼)/2 quantile. Zaffran et al. (2022) experimented
with multiple online aggregation algorithms, and found that they yielded similar results. Thus, we
follow their lead in using the Bernstein Online Aggregation (BOA) algorithm as implemented in
the opera R package (Wintenberger 2017; Gaillard et al. 2023). BOA is an online algorithm that
forms predictions for the lower (or upper) prediction interval bound as a weighted mean of the
candidate ACI prediction interval lower (upper) bound, where the weights are determined by each
candidateâ€™s past performance with respect to the quantile loss. As a consequence, the prediction
intervals generated by AgACI are not necessarily symmetric around the point prediction, as the
weights for the lower and upper bounds are separate."
IMPLEMENTATION/METHODS,0.2,"3.2.1
Theoretical Gaurantees"
IMPLEMENTATION/METHODS,0.2013986013986014,"AgACI departs from our main theoretical framework in that it does not yield a sequence (ğœƒğ‘¡)ğ‘¡âˆˆJğ‘‡K
whose elements yield prediction intervals via a set construction function Ì‚ğ¶ğ‘¡. Rather, the upper and
lower interval bounds from a set of candidate ACI algorithms are aggregated separately. Thus,
theoretical results such as regret bounds similar to those for the other algorithms are not available.
It would be possible, however, to establish regret bounds for the pinball loss applied separately to
the lower and upper bounds of the prediction intervals. It is unclear, however, how to convert such
regret bounds into a coverage bound."
IMPLEMENTATION/METHODS,0.20279720279720279,"3.2.2
Tuning Parameters"
IMPLEMENTATION/METHODS,0.2041958041958042,"The main tuning parameter for AgACI is the set of candidate learning rates. Beyond necessi-
tating additional computational time, there is no drawback to having a large grid. As a default,
AdaptiveConformal uses learning rates ğ›¾âˆˆ{0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128}. As a
basic check, we can look at the weights assigned to each of the learning rates. If large weights are"
IMPLEMENTATION/METHODS,0.2055944055944056,"given to the smallest (largest) learning rates, it is a sign that smaller (or larger) learning rates may
perform well. In addition each of the candidate ACI algorithms requires a starting value, which
can be set to any value as discussed in the ACI section. Figure 2 illustrates AgACI applied to the
running example with two sets of learning grids. The first grid is ğ›¾= {0.032, 0.064, 0.128, 0.256}, and
the second grid includes the additional values {0.008, 0.016}. For the first grid, we can see that for
the lower bound AgACI assigns high weight to the lowest learning rate (ğ›¾= 0.032). The second
grid yields weights that are less concentrated on a single learning rate, and the output prediction
intervals are smoother."
IMPLEMENTATION/METHODS,0.206993006993007,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.2083916083916084,"âˆ’0.5
0.5"
IMPLEMENTATION/METHODS,0.2097902097902098,Grid 1 Index
IMPLEMENTATION/METHODS,0.2111888111888112,"EmpCov = 81.6%
PathLength = 15.1"
IMPLEMENTATION/METHODS,0.2125874125874126,"0.0
0.4
0.8"
IMPLEMENTATION/METHODS,0.213986013986014,Final Aggregation Weights
IMPLEMENTATION/METHODS,0.2153846153846154,Learning Rates (Î³)
IMPLEMENTATION/METHODS,0.21678321678321677,"0.032
0.064
0.128
0.256"
IMPLEMENTATION/METHODS,0.21818181818181817,"Upper
Lower"
IMPLEMENTATION/METHODS,0.21958041958041957,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.22097902097902097,"âˆ’0.5
0.5"
IMPLEMENTATION/METHODS,0.22237762237762237,Grid 2 Index
IMPLEMENTATION/METHODS,0.22377622377622378,"EmpCov = 83.6%
PathLength = 6.7"
IMPLEMENTATION/METHODS,0.22517482517482518,"0.0
0.4
0.8"
IMPLEMENTATION/METHODS,0.22657342657342658,Final Aggregation Weights
IMPLEMENTATION/METHODS,0.22797202797202798,Learning Rates (Î³)
IMPLEMENTATION/METHODS,0.22937062937062938,"0.002
0.008
0.032
0.128"
IMPLEMENTATION/METHODS,0.23076923076923078,"Upper
Lower"
IMPLEMENTATION/METHODS,0.23216783216783216,"Figure 2: Example 80% prediction intervals from the AgACI algorithm with starting values ğœƒ1 = 0.8
and two different learning rate grids. In the left column, blue and red points are observations that
fell inside and outside the prediction intervals, respectively."
IMPLEMENTATION/METHODS,0.23356643356643356,"3.3
Dynamically-tuned Adaptive Conformal Inference (DtACI)"
IMPLEMENTATION/METHODS,0.23496503496503496,"The Dynamically-tuned Adaptive Conformal Inference (DtACI; Algorithm 3 ) algorithm was devel-
oped by the authors of the original ACI algorithm in part to address the issue of how to choose the
learning rate parameter ğ›¾. In this respect the goal of the algorithm is similar to that of AgACI, although
it is achieved slightly differently. DtACI also aggregates predictions from multiple copies of ACI run
with different learning rates, but differs in that it directly aggregates the estimated radii emitted from
each algorithm based on their pinball loss (Gibbs and CandÃ¨s 2022) using an exponential reweighting
scheme (Gradu, Hazan, and Minasyan 2023). As opposed to AgACI, this construction allows for more
straightforward development of theoretical guarantees on the algorithmâ€™s performance, because the
upper and lower bounds of the intervals are not aggregated separately."
IMPLEMENTATION/METHODS,0.23636363636363636,Algorithm 3 Dynamically-tuned Adaptive Conformal Inference
IMPLEMENTATION/METHODS,0.23776223776223776,"1: Input: starting value ğœƒ1, candidate learning rates (ğ›¾ğ‘˜)1â‰¤ğ‘˜â‰¤ğ¾, parameters ğœ, ğœ‚."
IMPLEMENTATION/METHODS,0.23916083916083916,"2: for ğ‘˜= 1, â€¦ , ğ¾do"
IMPLEMENTATION/METHODS,0.24055944055944056,"3:
Initialize expert ğ’œğ‘˜= ACI(ğ›¼â†ğ›¼, ğ›¾â†ğ›¾ğ‘˜, ğœƒ1 â†ğœƒ1)."
IMPLEMENTATION/METHODS,0.24195804195804196,4: end for
IMPLEMENTATION/METHODS,0.24335664335664337,"5: for ğ‘¡= 1, 2, â€¦ , ğ‘‡do"
IMPLEMENTATION/METHODS,0.24475524475524477,"6:
Define ğ‘ğ‘˜ğ‘¡âˆ¶= ğ‘ğ‘˜ğ‘¡/ âˆ‘ğ¾
ğ‘–=1 ğ‘ğ‘–ğ‘¡, for all 1 â‰¤ğ‘˜â‰¤ğ¾."
IMPLEMENTATION/METHODS,0.24615384615384617,"7:
Set ğœƒğ‘¡= âˆ‘ğ¾
ğ‘˜=1 ğœƒğ‘˜ğ‘¡ğ‘ğ‘˜ğ‘¡."
IMPLEMENTATION/METHODS,0.24755244755244754,"8:
Output: prediction interval Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)."
IMPLEMENTATION/METHODS,0.24895104895104894,"9:
Observe ğ‘¦ğ‘¡and compute ğ‘Ÿğ‘¡."
IMPLEMENTATION/METHODS,0.25034965034965034,"10:
Ì„ğ‘¤ğ‘˜ğ‘¡â†ğ‘ğ‘˜ğ‘¡exp(âˆ’ğœ‚ğ¿ğ›¼(ğœƒğ‘˜ğ‘¡, ğ‘Ÿğ‘¡)), for all 1 â‰¤ğ‘˜â‰¤ğ¾."
IMPLEMENTATION/METHODS,0.2517482517482518,"11:
Ì„ğ‘Šğ‘¡â†âˆ‘ğ¾
ğ‘–=1
Ì„ğ‘¤ğ‘–ğ‘¡."
IMPLEMENTATION/METHODS,0.25314685314685315,"12:
ğ‘ğ‘˜
ğ‘¡+1 â†(1 âˆ’ğœ) Ì„ğ‘¤ğ‘˜ğ‘¡+
Ì„ğ‘Šğ‘¡ğœ/ğ¾."
IMPLEMENTATION/METHODS,0.2545454545454545,"13:
Set errğ‘¡âˆ¶= ğ•€[ğ‘¦ğ‘¡âˆ‰
Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)]."
IMPLEMENTATION/METHODS,0.25594405594405595,"14:
for ğ‘˜= 1, â€¦ , ğ¾do"
IMPLEMENTATION/METHODS,0.2573426573426573,"15:
Update ACI ğ’œğ‘˜with ğ‘¦ğ‘¡and obtain ğœƒğ‘˜
ğ‘¡+1."
IMPLEMENTATION/METHODS,0.25874125874125875,"16:
end for"
IMPLEMENTATION/METHODS,0.2601398601398601,17: end for
IMPLEMENTATION/METHODS,0.26153846153846155,"3.3.1
Theoretical Guarantees"
IMPLEMENTATION/METHODS,0.2629370629370629,"DtACI was originally proposed with the choice of the quantile interval constructor. DtACI has the
following strongly-adaptive regret bound (Bhatnagar et al. 2023): for all ğœ‚> 0 and subperiod lengths
ğ‘š,
SAReg(ğ‘‡, ğ‘š) â‰¤Ìƒ
ğ’ª(ğ·2/ğœ‚+ ğœ‚ğ‘š)."
IMPLEMENTATION/METHODS,0.26433566433566436,"If ğ‘šis fixed a-priori, then choosing ğœ‚= ğ·/âˆšğ‘šyields a strongly adaptive regret bound of order
Ìƒ
ğ’ª(ğ·âˆšğ‘š) (for a single choice of ğ‘š). Practically, this result implies that, if we know in advance the
time length for which we would like to control the regret, it is possible to choose an optimal tuning
parameter value. However, we cannot control the regret simultaneously for all possible time lengths."
IMPLEMENTATION/METHODS,0.26573426573426573,"To establish a bound on the coverage error, the authors investigated a slightly modified version of
DtACI in which ğœƒğ‘¡is chosen randomly from the candidate ğœƒğ‘¡ğ‘˜with weights given by ğ‘ğ‘¡,ğ‘˜, instead
of taking a weighted average. This is a common trick used in the literature as it facilitates theo-
retical analysis. In practice, the authors comment that this randomized version of DtACI and the
deterministic version lead to very similar results. The coverage error result also assumes that the
hyperparameters can change over time: that is, we have ğ‘¡-specific ğœ‚ğ‘¡and ğœğ‘¡, rather than fixed ğœ‚and ğœ.
The coverage error then has the following bound (Gibbs and CandÃ¨s (2022); Theorem 3.2), where
ğ›¾min and ğ›¾max are the smallest and largest learning rates in the grid, respectively:"
IMPLEMENTATION/METHODS,0.26713286713286716,|CovErr(ğ‘‡)| â‰¤1 + 2ğ›¾max
IMPLEMENTATION/METHODS,0.26853146853146853,"ğ‘‡ğ›¾min
+ (1 + 2ğ›¾max)2"
IMPLEMENTATION/METHODS,0.2699300699300699,"ğ›¾min
1
ğ‘‡"
IMPLEMENTATION/METHODS,0.27132867132867133,"ğ‘‡
âˆ‘
ğ‘¡=1
ğœ‚ğ‘¡exp(ğœ‚ğ‘¡(1 + 2ğ›¾max)) + 21 + ğ›¾max"
IMPLEMENTATION/METHODS,0.2727272727272727,"ğ›¾min
1
ğ‘‡"
IMPLEMENTATION/METHODS,0.27412587412587414,"ğ‘‡
âˆ‘
ğ‘¡=1
ğœğ‘¡."
IMPLEMENTATION/METHODS,0.2755244755244755,"Thus, if ğœ‚ğ‘¡and ğœğ‘¡both converge to zero as ğ‘¡â†’âˆ, then the coverage error will also converge to zero.
In addition, under mild distributional assumptions the authors provide a type of short-term coverage
error bound for arbitrary time spans, for which we refer to (Gibbs and CandÃ¨s 2022)."
IMPLEMENTATION/METHODS,0.27692307692307694,"We note one additional result established by Gibbs and CandÃ¨s (2022) (their Theorem 3.1) on a
slightly different dynamic regret bound in terms of the pinball loss, as it informs the choice of
tuning parameters. Let ğ›¾max = max1â‰¤ğ‘˜â‰¤ğ¾ğ›¾ğ‘˜be the largest learning rate in the grid and assume that
ğ›¾1 < ğ›¾2 < â‹¯< ğ›¾ğ¾with ğ›¾ğ‘˜+1/ğ›¾â‰¤2 for all 1 â‰¤ğ‘˜< ğ¾. Then, for any interval ğ¼= [ğ‘Ÿ, ğ‘ ] âŠ†Jğ‘‡K and any"
IMPLEMENTATION/METHODS,0.2783216783216783,"sequence ğœƒâˆ—ğ‘Ÿ, â€¦ , ğœƒâˆ—ğ‘ , under the assumption that ğ›¾ğ‘˜â‰¥âˆš1 + 1/|ğ¼|, 1
|ğ¼|"
IMPLEMENTATION/METHODS,0.27972027972027974,"ğ‘ 
âˆ‘
ğ‘¡=ğ‘Ÿ
ğ”¼[ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡)] âˆ’1 |ğ¼|"
IMPLEMENTATION/METHODS,0.2811188811188811,"ğ‘ 
âˆ‘
ğ‘¡=ğ‘Ÿ
ğ¿ğ›¼(ğœƒğ‘¡, ğœƒâˆ—ğ‘¡) â‰¤log(ğ‘˜/ğœ) + 2ğœ|ğ¼|"
IMPLEMENTATION/METHODS,0.28251748251748254,"ğœ‚|ğ¼|
+ ğœ‚ |ğ¼|"
IMPLEMENTATION/METHODS,0.2839160839160839,"ğ‘ 
âˆ‘
ğ‘¡=ğ‘Ÿ
ğ”¼[ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡)2]"
IMPLEMENTATION/METHODS,0.2853146853146853,"+ 2âˆš3(1 + ğ›¾max)2 max {
âˆš"
IMPLEMENTATION/METHODS,0.2867132867132867,"âˆ‘ğ‘ 
ğ‘¡=ğ‘Ÿ+1 |ğœƒâˆ—ğ‘¡âˆ’ğœƒâˆ—ğ‘¡âˆ’1| + 1"
IMPLEMENTATION/METHODS,0.2881118881118881,"|ğ¼|
, ğ›¾1} ,"
IMPLEMENTATION/METHODS,0.2895104895104895,"where the expectation is over the randomness in the randomized version of the algorithm. Here the
time interval ğ¼(with length |ğ¼|) is comparable to the time period length ğ‘šfor the strongly adaptive
regret. The parameter |ğ¼|, the time interval of interest for which we would like to control, can be
chosen arbitrarily. This dynamic regret bound can be converted to a strongly adaptive regret bound
by choosing ğœƒâˆ—ğ‘¡to be constant."
IMPLEMENTATION/METHODS,0.2909090909090909,"3.3.2
Tuning parameters"
IMPLEMENTATION/METHODS,0.2923076923076923,"The recommended settings for the tuning parameters depend on choosing a time interval length |ğ¼|
for which we would like to control the pinball loss. The choice of |ğ¼| can be chosen arbitrarily. For
the tuning parameter ğœ, the authors suggest the optimal choice (with respect to the dynamic regret)
ğœ= 1/(2|ğ¼|). Choosing ğœ‚is more difficult. The authors suggest the following choice for ğœ‚, which they
show is optimal if there is in fact no distribution shift: ğœ‚=
âˆš"
IMPLEMENTATION/METHODS,0.2937062937062937,"3
|ğ¼|âˆš"
IMPLEMENTATION/METHODS,0.2951048951048951,"log(ğ¾â‹…|ğ¼|) + 2
(ğ›¼)2(1 âˆ’ğ›¼)3 + (1 âˆ’ğ›¼)2ğ›¼3 ."
IMPLEMENTATION/METHODS,0.2965034965034965,"Note that this choice is optimal only for the quantile interval constructor, for which ğœƒğ‘¡is a quantile
of previous nonconformity scores. As an alternative, the authors point out that ğœ‚can be learned in
an online fashion using the update rule ğœ‚ğ‘¡âˆ¶= âˆš"
IMPLEMENTATION/METHODS,0.29790209790209793,log(|ğ¼|ğ¾) + 2
IMPLEMENTATION/METHODS,0.2993006993006993,"âˆ‘ğ‘¡âˆ’1
ğ‘ =ğ‘¡âˆ’|ğ¼| ğ¿ğ›¼(ğœƒğ‘ , ğ‘Ÿğ‘ )
."
IMPLEMENTATION/METHODS,0.3006993006993007,"Both ways of choosing ğœ‚led to very similar results in the original authorâ€™s empirical studies. In
our proposed AdaptiveConformal package, the first approach is used when the quantile interval
construction function is chosen, and the latter approach for the linear interval construction function."
IMPLEMENTATION/METHODS,0.3020979020979021,"Figure 3 illustrates DtACI with the quantile interval construction function and with the learning
rate grid ğ›¾âˆˆ{0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128}. The tuning parameter ğœ‚was set to
0.001, 1, and 100 to show how the algorithm responds to extreme choices of the parameter, and to
ğœ‚â‰ˆ3.19 according to the optimal choice recommendation with ğ¼= 100 as described in the previous
section. The results show that, in this simple example, high values of ğœ‚may lead to intervals that
are too reactive to the data, as seen in the longer path length. The algorithm appears more robust,
however, to small choices of ğœ‚."
IMPLEMENTATION/METHODS,0.3034965034965035,"3.4
Scale-Free Online Gradient Descent (SF-OGD)"
IMPLEMENTATION/METHODS,0.3048951048951049,"Scale-Free Online Gradient Descent (SF-OGD; Algorithm 4 ) is a general algorithm for online learning
proposed by Orabona and PÃ¡l (2018). The algorithm updates ğœƒğ‘¡with a gradient descent step where
the learning rate adapts to the scale of the previously observed gradients. SF-OGD was first used in
the context of ACI as a sub-algorithm for SAOCP (described in the next section). However, it was
found to have good performance by itself (Bhatnagar et al. 2023) in real-world tasks, so we have
made it available in the package as a stand-alone algorithm."
IMPLEMENTATION/METHODS,0.3062937062937063,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.3076923076923077,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.3090909090909091,Î· = 0.001
IMPLEMENTATION/METHODS,0.3104895104895105,"EmpCov = 86.8%
PathLength = 7.8"
IMPLEMENTATION/METHODS,0.3118881118881119,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.3132867132867133,"âˆ’0.5
0.0
0.5 Î· = 1"
IMPLEMENTATION/METHODS,0.3146853146853147,"EmpCov = 82.6%
PathLength = 8.3"
IMPLEMENTATION/METHODS,0.31608391608391606,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.3174825174825175,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.31888111888111886,Î· = 100
IMPLEMENTATION/METHODS,0.3202797202797203,"EmpCov = 81.2%
PathLength = 20.3"
IMPLEMENTATION/METHODS,0.32167832167832167,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.3230769230769231,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.32447552447552447,Î· = 3.190185
IMPLEMENTATION/METHODS,0.3258741258741259,"EmpCov = 81.0%
PathLength = 8.3"
IMPLEMENTATION/METHODS,0.32727272727272727,"Figure 3: Example 80% prediction intervals generated by the DtACI algorithm with starting values
ğœƒ1 = 0.8 and with several values of the tuning parameter ğœ‚. Blue and red points are observations that
fell inside and outside the prediction intervals, respectively."
IMPLEMENTATION/METHODS,0.32867132867132864,Algorithm 4 Scale-Free Online Gradient Descent
IMPLEMENTATION/METHODS,0.3300699300699301,"1: Input: starting value ğœƒ1, learning rate ğ›¾> 0."
IMPLEMENTATION/METHODS,0.33146853146853145,"2: for ğ‘¡= 1, 2, â€¦ , ğ‘‡do"
IMPLEMENTATION/METHODS,0.3328671328671329,"3:
Output: prediction interval Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)."
IMPLEMENTATION/METHODS,0.33426573426573425,"4:
Observe ğ‘¦ğ‘¡and compute ğ‘Ÿğ‘¡."
IMPLEMENTATION/METHODS,0.3356643356643357,"5:
Update ğœƒğ‘¡+1 = ğœƒğ‘¡âˆ’ğ›¾
âˆ‡ğ¿ğ›¼(ğœƒğ‘¡,ğ‘Ÿğ‘¡)"
IMPLEMENTATION/METHODS,0.33706293706293705,"âˆšâˆ‘ğ‘¡
ğ‘–=1â€–âˆ‡ğ¿ğ›¼(ğœƒğ‘–,ğ‘Ÿğ‘–)â€–2
2
."
IMPLEMENTATION/METHODS,0.3384615384615385,6: end for
IMPLEMENTATION/METHODS,0.33986013986013985,"3.4.1
Theoretical Guarantees"
IMPLEMENTATION/METHODS,0.3412587412587413,"The SF-OGD algorithm with linear interval constructor has the following regret bound, which is
called an anytime regret bound because it holds for all ğ‘¡âˆˆJğ‘‡K (Bhatnagar et al. 2023). For any ğ›¾> 0,"
IMPLEMENTATION/METHODS,0.34265734265734266,Reg(ğ‘¡) â‰¤ğ’ª(ğ·âˆšğ‘¡) for all ğ‘¡âˆˆJğ‘‡K.
IMPLEMENTATION/METHODS,0.34405594405594403,"A bound for the coverage error has also been established (Bhatnagar et al. (2023); Theorem 4.2). For
any learning rate ğ›¾= Î˜(ğ·) (where ğ›¾= ğ·/âˆš3 is optimal) and any starting value ğœƒ1 âˆˆ[0, ğ·], then it
holds that for any ğ‘‡> 1,
|CovErr(ğ‘‡)| â‰¤ğ’ª((1 âˆ’ğ›¼)âˆ’2ğ‘‡âˆ’1/4 log ğ‘‡) ."
IMPLEMENTATION/METHODS,0.34545454545454546,"3.4.2
Tuning parameters"
IMPLEMENTATION/METHODS,0.34685314685314683,"Figure 4 compares results for several choices of ğ›¾to illustrate its effect. The optimal choice of learning
rate is ğ›¾= ğ·/âˆš3, where ğ·is the maximum possible radius. When ğ·is not known, it can be estimated
by using an initial subset of the time series as a calibration set and estimating ğ·as the maximum
of the absolute residuals of the observations and the predictions (Bhatnagar et al. 2023). Figure 4
illustrates SF-OGD for several values of ğ›¾. In the example, the prediction intervals are not reactive
enough and do not achieve optimal coverage when ğ›¾is small. As ğ›¾increases, the coverage error is
near optimal, although the path length becomes larger."
IMPLEMENTATION/METHODS,0.34825174825174826,"3.5
Strongly Adaptive Online Conformal Prediction (SAOCP)"
IMPLEMENTATION/METHODS,0.34965034965034963,"The Strongly Adaptive Online Conformal Prediction (SAOCP; Algorithm 5 ) algorithm was proposed
as an improvement over the extant ACI algorithms in that it features stronger theoretical guarantees.
SAOCP works similarly to AgACI and DtACI in that it maintains a library of candidate online learning
algorithms that generate prediction intervals which are then aggregated using a meta-algorithm
(Bhatnagar et al. 2023). The candidate algorithm was chosen to be SF-OGD, although any algorithm
that features anytime regret guarantees can be chosen. As opposed to AgACI and DtACI, in which
each candidate has a different learning rate but is always able to contribute to the final prediction
intervals, here each candidate has the same learning rate but only has positive weight over a specific
interval of time. New candidate algorithms are continually being spawned in order that, if the
distribution shifts rapidly, the newer candidates will be able to react quickly and receive positive
weight. Specifically, at each time point, a new expert is instantiated which is active over a finite
â€œlifetimeâ€. Define the lifetime of an expert instantiated at time ğ‘¡as"
IMPLEMENTATION/METHODS,0.35104895104895106,"ğ¿(ğ‘¡) âˆ¶= ğ‘”â‹…max
ğ‘›âˆˆâ„¤{2ğ‘›ğ‘¡â‰¡0
mod 2ğ‘›},"
IMPLEMENTATION/METHODS,0.35244755244755244,"where ğ‘”âˆˆâ„¤âˆ—is a lifetime multiplier parameter. The active experts are weighted according to
their empirical performance with respect to the pinball loss function. The authors show that this
construction results in intervals that have strong regret guarantees. The form of the lifetime interval
function ğ¿(ğ‘¡) is due to the use of geometric covering intervals to partition the input time series, and
other choices may be possible (Jun et al. 2017)."
IMPLEMENTATION/METHODS,0.35384615384615387,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.35524475524475524,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.35664335664335667,Î³ = 0.01
IMPLEMENTATION/METHODS,0.35804195804195804,"EmpCov = 72.2%
PathLength = 0.6"
IMPLEMENTATION/METHODS,0.3594405594405594,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.36083916083916084,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.3622377622377622,Î³ = 0.1
IMPLEMENTATION/METHODS,0.36363636363636365,"EmpCov = 81.6%
PathLength = 6.4"
IMPLEMENTATION/METHODS,0.365034965034965,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.36643356643356645,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.3678321678321678,Î³ = 0.25
IMPLEMENTATION/METHODS,0.36923076923076925,"EmpCov = 80.8%
PathLength = 16.4"
IMPLEMENTATION/METHODS,0.3706293706293706,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.37202797202797205,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.3734265734265734,Î³ = 0.5
IMPLEMENTATION/METHODS,0.3748251748251748,"EmpCov = 80.4%
PathLength = 33.3"
IMPLEMENTATION/METHODS,0.37622377622377623,"Figure 4: Example 80% prediction intervals generated by the SF-OGD algorithm with different values
of the maximum radius tuning parameter ğ·. Blue and red points are observations that fell inside and
outside the prediction intervals, respectively."
IMPLEMENTATION/METHODS,0.3776223776223776,Algorithm 5 Strongly Adaptive Online Conformal Prediction
IMPLEMENTATION/METHODS,0.37902097902097903,"1: Input: initial value ğœƒ0, learning rate ğ›¾> 0."
IMPLEMENTATION/METHODS,0.3804195804195804,"2: for ğ‘¡= 1, 2, â€¦ , ğ‘‡do"
IMPLEMENTATION/METHODS,0.38181818181818183,"3:
Initialize expert ğ’œğ‘¡= SF-OGD(ğ›¼â†ğ›¼, ğ›¾â†ğ›¾, ğœƒ1 â†ğœƒğ‘¡âˆ’1), set weight ğ‘ğ‘¡ğ‘¡= 0."
IMPLEMENTATION/METHODS,0.3832167832167832,"4:
Compute active set Active(ğ‘¡) = {ğ‘–âˆˆJğ‘‡K âˆ¶ğ‘¡âˆ’ğ¿(ğ‘–) < ğ‘–â‰¤ğ‘¡} (see below for definition of ğ¿(ğ‘¡))."
IMPLEMENTATION/METHODS,0.38461538461538464,"5:
Compute prior probability ğœ‹ğ‘–âˆğ‘–âˆ’2(1 + âŒŠlog2 ğ‘–âŒ‹)âˆ’1ğ•€[ğ‘–âˆˆActive(ğ‘¡)]."
IMPLEMENTATION/METHODS,0.386013986013986,"6:
Compute un-normalized probability
Ì‚ğ‘ğ‘–= ğœ‹ğ‘–[ğ‘ğ‘¡,ğ‘–]+ for all ğ‘–âˆˆJğ‘¡K."
IMPLEMENTATION/METHODS,0.38741258741258744,"7:
Normalize ğ‘=
Ì‚ğ‘/â€– Ì‚ğ‘â€–1 âˆˆÎ”ğ‘¡if â€– Ì‚ğ‘â€–1 > 0, else ğ‘= ğœ‹."
IMPLEMENTATION/METHODS,0.3888111888111888,"8:
Set ğœƒğ‘¡= âˆ‘ğ‘–âˆˆActive(ğ‘¡) ğ‘ğ‘–ğœƒğ‘–ğ‘¡(for ğ‘¡â‰¥2), and ğœƒğ‘¡= 0 for ğ‘¡= 1."
IMPLEMENTATION/METHODS,0.3902097902097902,"9:
Output: prediction set Ì‚ğ¶ğ‘¡(ğœƒğ‘¡)."
IMPLEMENTATION/METHODS,0.3916083916083916,"10:
Observe ğ‘¦ğ‘¡and compute ğ‘Ÿğ‘¡."
IMPLEMENTATION/METHODS,0.393006993006993,"11:
for ğ‘–âˆˆActive(ğ‘¡) do"
IMPLEMENTATION/METHODS,0.3944055944055944,"12:
Update expert ğ’œğ‘¡with ğ‘¦ğ‘¡and obtain ğœƒğ‘–
ğ‘¡+1."
IMPLEMENTATION/METHODS,0.3958041958041958,"13:
Compute ğ‘”ğ‘–ğ‘¡= {"
IMPLEMENTATION/METHODS,0.3972027972027972,"1
ğ·(ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡) âˆ’ğ¿ğ›¼(ğœƒğ‘–ğ‘¡, ğ‘Ÿğ‘¡))
ğ‘ğ‘–ğ‘¡> 0
1
ğ·[ğ¿ğ›¼(ğœƒğ‘¡, ğ‘Ÿğ‘¡) âˆ’ğ¿ğ›¼(ğœƒğ‘–ğ‘¡, ğ‘Ÿğ‘¡))]+
ğ‘ğ‘–ğ‘¡â‰¤0."
IMPLEMENTATION/METHODS,0.3986013986013986,"14:
Update expert weight ğ‘ğ‘–
ğ‘¡+1 =
1
ğ‘¡âˆ’ğ‘–+1 (âˆ‘ğ‘¡
ğ‘—=ğ‘–ğ‘”ğ‘–
ğ‘—) (1 + âˆ‘ğ‘¡
ğ‘—=ğ‘–ğ‘ğ‘–
ğ‘—ğ‘”ğ‘–
ğ‘—)."
IMPLEMENTATION/METHODS,0.4,"15:
end for"
IMPLEMENTATION/METHODS,0.4013986013986014,16: end for
IMPLEMENTATION/METHODS,0.4027972027972028,"3.5.1
Theoretical Guarantees"
IMPLEMENTATION/METHODS,0.4041958041958042,"The theoretical results were established for SAOCP using the linear interval constructor. The
following bound for the strongly adaptive regret holds for all subperiod lengths ğ‘šâˆˆJğ‘‡K (Bhatnagar
et al. (2023); Proposition 4.1):"
IMPLEMENTATION/METHODS,0.40559440559440557,"SAReg(ğ‘‡, ğ‘š) â‰¤15ğ·âˆšğ‘š(log ğ‘‡+ 1) â‰¤
Ìƒğ’ª(ğ·âˆšğ‘š)."
IMPLEMENTATION/METHODS,0.406993006993007,"It should be emphasized that this regret bounds holds simultaneously across all ğ‘š, as opposed to
DtACI, where a similar bound holds only for a single ğ‘š. A bound on the coverage error of SAOCP
has also been established as:"
IMPLEMENTATION/METHODS,0.4083916083916084,"|CovErr(ğ‘‡)| â‰¤ğ’ª(inf
ğ›½(ğ‘‡1/2âˆ’ğ›½+ ğ‘‡ğ›½âˆ’1ğ‘†ğ›½(ğ‘‡))) ."
IMPLEMENTATION/METHODS,0.4097902097902098,"where ğ‘†ğ›½(ğ‘‡) is a technical measure of the smoothness of the cumulative gradients and expert weights
for each of the candidate experts (Bhatnagar et al. (2023); Theorem 4.3). For some intuition, ğ‘†ğ›½can
be expected to be small when the weights placed on each algorithm do change quickly, as would be
the case under abrupt distributional shifts."
IMPLEMENTATION/METHODS,0.4111888111888112,"3.5.2
Tuning Parameters"
IMPLEMENTATION/METHODS,0.4125874125874126,"The primary tuning parameter for SAOCP is the learning rate ğ›¾of the SF-OGD sub-algorithms, which
we saw in the previous section has for optimal choice ğ›¾= ğ·/âˆš3. Values for ğ·that are too low
lead to intervals that adapt slowly, and values that are too large lead to jagged intervals. In their
experiments, the authors select a value for ğ·by picking the maximum residual from a calibration set.
The second tuning parameter is the lifetime multiplier ğ‘”which controls the lifetime of each of the
experts. We follow the original paper in setting ğ‘”= 8. Figure 5 illustrates the SAOCP algorithm for
choices of ğ·âˆˆ{0.01, 0.1, 0.25, 0.5}. Similarly to SF-OGD, the prediction intervals tend to undercover
for small ğ·, and achieve near-optimal coverage for larger ğ·at the expense of larger path lengths."
IMPLEMENTATION/METHODS,0.413986013986014,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.4153846153846154,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.4167832167832168,D = 0.01
IMPLEMENTATION/METHODS,0.41818181818181815,"EmpCov = 68.8%
PathLength = 4.7"
IMPLEMENTATION/METHODS,0.4195804195804196,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.42097902097902096,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.4223776223776224,D = 0.1
IMPLEMENTATION/METHODS,0.42377622377622376,"EmpCov = 77.8%
PathLength = 13.3"
IMPLEMENTATION/METHODS,0.4251748251748252,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.42657342657342656,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.427972027972028,D = 0.25
IMPLEMENTATION/METHODS,0.42937062937062936,"EmpCov = 81.0%
PathLength = 22.2"
IMPLEMENTATION/METHODS,0.4307692307692308,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.43216783216783217,"âˆ’0.5
0.0
0.5"
IMPLEMENTATION/METHODS,0.43356643356643354,D = 0.5
IMPLEMENTATION/METHODS,0.43496503496503497,"EmpCov = 80.8%
PathLength = 33.7"
IMPLEMENTATION/METHODS,0.43636363636363634,"Figure 5: Example 80% prediction intervals generated by the SAOCP algorithm with different values
of the maximum radius parameter ğ·. Blue and red points are observations that fell inside and outside
the prediction intervals, respectively."
IMPLEMENTATION/METHODS,0.43776223776223777,"4
AdaptiveConformal R package"
IMPLEMENTATION/METHODS,0.43916083916083914,"The ACI algorithms described in the previous section have been implemented in the open-source
and publically available R package AdaptiveConformal, available at https://github.com/herbps10/
AdaptiveConformal. CIn this section, we briefly introduce the main functionality of the package.
Comprehensive documentation is, including several example vignettes, is included with the package."
IMPLEMENTATION/METHODS,0.4405594405594406,The AdaptiveConformal package can be installed using the remotes package:
IMPLEMENTATION/METHODS,0.44195804195804195,"remotes::install_github(""herbps10/AdaptiveConformal"")"
IMPLEMENTATION/METHODS,0.4433566433566434,"The ACI algorithms are accessed through the aci function, which takes as input a vector of obser-
vations (ğ‘¦ğ‘¡) and a vector or matrix of predictions ( Ì‚ğ‘¦ğ‘¡). Using the data generating process from the
running example to illustrate, we can fit the original ACI algorithm with learning rate ğ›¾= 0.1:"
IMPLEMENTATION/METHODS,0.44475524475524475,"set.seed(532)
data <- running_example_data(N = 5e2)
fit <- aci(data$y, data$yhat, alpha = 0.8, method = ""ACI"", parameters = list(gamma = 0.1))"
IMPLEMENTATION/METHODS,0.4461538461538462,"The available parameters for each method can be found in the documentation for the aci method,
accessible with the command ?aci. The resulting conformal prediction intervals can then be plotted
using the plot function:"
IMPLEMENTATION/METHODS,0.44755244755244755,plot(fit)
IMPLEMENTATION/METHODS,0.4489510489510489,"0
100
200
300
400
500"
IMPLEMENTATION/METHODS,0.45034965034965035,"âˆ’0.6
âˆ’0.2
0.2
0.6 Index Y âˆ’"
IMPLEMENTATION/METHODS,0.45174825174825173,"Within interval
Outside interval
Predictions"
IMPLEMENTATION/METHODS,0.45314685314685316,The properties of the prediction intervals can also be examined using the summary function:
IMPLEMENTATION/METHODS,0.45454545454545453,summary(fit)
IMPLEMENTATION/METHODS,0.45594405594405596,"Method: ACI
Empirical coverage: 80.6% (403/500)
Below interval: 10.2%
Above interval: 9.2%"
IMPLEMENTATION/METHODS,0.45734265734265733,"Mean interval width: 0.354
Mean interval loss: 0.498"
RESULTS/EXPERIMENTS,0.45874125874125876,"5
Simulation Studies"
RESULTS/EXPERIMENTS,0.46013986013986014,"We present two empirical studies in order to compare the performance of the AgACI, DtACI, SF-
OGD, and SAOCP algorithms applied to simple simulated datasets. The original ACI algorithm was
not included as it is not clear how to set the tuning rate ğ›¾, which can have a large effect on the
resulting intervals. For both simulations we set the targeted empirical coverage to ğ›¼= 0.8, ğ›¼= 0.9,
and ğ›¼= 0.95. For each algorithm, we chose the interval constructor that was used in its original
presentation (see Table 1)."
RESULTS/EXPERIMENTS,0.46153846153846156,"5.1
Time series with ARMA errors"
RESULTS/EXPERIMENTS,0.46293706293706294,"In this simulation we reproduce the setup described in Zaffran et al. (2022) (itself based on that of
Friedman, Grosse, and Stuetzle (1983)). The time series values ğ‘¦ğ‘¡for ğ‘¡âˆˆJğ‘‡K (ğ‘‡= 600) are simulated
according to"
RESULTS/EXPERIMENTS,0.4643356643356643,"ğ‘¦ğ‘¡= 10 sin(ğœ‹ğ‘‹ğ‘¡,1ğ‘‹ğ‘¡,2) + 20(ğ‘‹ğ‘¡,3 âˆ’0.5)2 + 10ğ‘‹ğ‘¡,4 + 5ğ‘‹ğ‘¡,5 + 0ğ‘‹ğ‘¡,6 + ğœ–ğ‘¡,"
RESULTS/EXPERIMENTS,0.46573426573426574,"where ğ‘‹ğ‘¡,ğ‘–, ğ‘–= 1, â€¦ , 6, ğ‘¡âˆˆJğ‘‡K are independently uniformly distributed on [0, 1] and the noise terms ğœ–ğ‘¡
are generated according to an ARMA(1, 1) process:"
RESULTS/EXPERIMENTS,0.4671328671328671,"ğœ–ğ‘¡= ğœ“ğœ–ğ‘¡âˆ’1 + ğœ‰ğ‘¡+ ğœƒğœ‰ğ‘¡âˆ’1,"
RESULTS/EXPERIMENTS,0.46853146853146854,"ğœ‰ğ‘¡âˆ¼ğ‘(0, ğœ2)."
RESULTS/EXPERIMENTS,0.4699300699300699,"We set ğœ“and ğœƒjointly to each value in {0.1, 0.8, 0.9, 0.95, 0.99} to simulate time series with increasing
temporal dependence. The innovation variance was set to ğœ2 = (1 âˆ’ğœ“2)/(1 + 2ğœ“ğœ‰+ ğœ‰2) (to ensure
that the process has constant variance). For each setting, 25 simulated datasets were generated."
RESULTS/EXPERIMENTS,0.47132867132867134,"To provide point predictions for the ACI algorithms, at each time ğ‘¡â‰¥200 a random forest model
was fitted to the previously observed data using the ranger R package (Wright and Ziegler 2017).
The estimated model was then used to predict the subsequent time point. The maximum radius ğ·
was estimated as the maximum residual observed between time points ğ‘¡= 200 and ğ‘¡= 249. The
ACI models were then executed starting at time point ğ‘¡= 250. All metrics are based on time points
ğ‘¡â‰¥300 to allow time for the ACI methods to initialize."
RESULTS/EXPERIMENTS,0.4727272727272727,"simulate <- function(seed, psi, xi, N = 1e3) {"
RESULTS/EXPERIMENTS,0.47412587412587415,set.seed(seed)
RESULTS/EXPERIMENTS,0.4755244755244755,"s <- 10
innov_scale <- sqrt(s * (1 - psi^2) / (1 + 2 * psi * xi + xi^2))"
RESULTS/EXPERIMENTS,0.47692307692307695,"X <- matrix(runif(6 * N), ncol = 6, nrow = N)
colnames(X) <- paste0(""X"", 1:6)"
RESULTS/EXPERIMENTS,0.4783216783216783,"epsilon <- arima.sim(n = N, model = list(ar = psi, ma = xi), sd = innov_scale)"
RESULTS/EXPERIMENTS,0.4797202797202797,"mu <- 10 * sin(pi * X[,1] * X[,2]) + 20 * (X[,3] - 0.5)^2 + 10 * X[,4] + 5 * X[,5]
y <- mu + epsilon
as_tibble(X) %>% mutate(y = y) }"
RESULTS/EXPERIMENTS,0.4811188811188811,"estimate_model <- function(data, p = NULL) {"
RESULTS/EXPERIMENTS,0.4825174825174825,"if(!is.null(p)) p()
preds <- numeric(nrow(data))
for(t in 200:nrow(data)) {"
RESULTS/EXPERIMENTS,0.48391608391608393,"model <- ranger::ranger(y ~ X1 + X2 + X3 + X4 + X5 + X6, data = data[1:(t - 1),])
preds[t] <- predict(model, data = data[t, ])$predictions
}
preds
}"
RESULTS/EXPERIMENTS,0.4853146853146853,metrics <- function(fit) {
RESULTS/EXPERIMENTS,0.48671328671328673,"indices <- 300:length(fit$Y)
aci_metrics(fit, indices)
}"
RESULTS/EXPERIMENTS,0.4881118881118881,"fit <- function(data, preds, method, alpha, p = NULL) {"
RESULTS/EXPERIMENTS,0.48951048951048953,if(!is.null(p)) p()
RESULTS/EXPERIMENTS,0.4909090909090909,"D <- max(abs(data$y - preds)[200:249])
gamma <- D / sqrt(3)"
RESULTS/EXPERIMENTS,0.49230769230769234,interval_constructor = case_when(
RESULTS/EXPERIMENTS,0.4937062937062937,"method == ""AgACI"" ~ ""conformal"",
method == ""DtACI"" ~ ""conformal"",
method == ""SF-OGD"" ~ ""linear"",
method == ""SAOCP"" ~ ""linear""
)"
RESULTS/EXPERIMENTS,0.4951048951048951,"if(interval_constructor == ""linear"") {"
RESULTS/EXPERIMENTS,0.4965034965034965,"gamma_grid = seq(0.1, 1, 0.1)
}
else {"
RESULTS/EXPERIMENTS,0.4979020979020979,"gamma_grid <- c(0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128)
}"
RESULTS/EXPERIMENTS,0.4993006993006993,parameters <- list(
RESULTS/EXPERIMENTS,0.5006993006993007,"interval_constructor = interval_constructor,
D = D,
gamma = gamma,
gamma_grid = gamma_grid
) aci("
RESULTS/EXPERIMENTS,0.5020979020979021,"data$y[250:nrow(data)],
preds[250:nrow(data)],
method = method,
alpha = alpha,"
RESULTS/EXPERIMENTS,0.5034965034965035,"parameters = parameters
)
}"
RESULTS/EXPERIMENTS,0.5048951048951049,"N_sims <- 100
simulation_data <- expand_grid("
RESULTS/EXPERIMENTS,0.5062937062937063,"index = 1:N_sims,
param =
c(0.1, 0.8, 0.9, 0.95, 0.99),
N = 600
) %>%"
RESULTS/EXPERIMENTS,0.5076923076923077,"mutate(psi = param, xi = param)"
RESULTS/EXPERIMENTS,0.509090909090909,"# For each simulated dataset, fit multiple ACI methods
simulation_study_setup <- expand_grid("
RESULTS/EXPERIMENTS,0.5104895104895105,"alpha = c(0.8, 0.9, 0.95),
method = c(""AgACI"", ""SF-OGD"", ""SAOCP"", ""DtACI"")
)"
RESULTS/EXPERIMENTS,0.5118881118881119,"# run_simulation_study1 function is defined in helpers.R
simulation_study1 <- run_simulation_study1("
RESULTS/EXPERIMENTS,0.5132867132867133,"simulation_data,
simulation_study_setup,
estimate_model,
fit,
workers = 8
)"
RESULTS/EXPERIMENTS,0.5146853146853146,"The coverage errors, mean interval widths, path lengths, and strongly adaptive regret (for ğ‘š= 20)
of each of the algorithms for ğ›¼= 0.9 are shown in Figure 6 (results for ğ›¼âˆˆ{0.8, 0.95} were similar
and are available in the appendix). All methods achieved near optimal empirical coverage, although
SAOCP tended to slightly undercover. The mean interval widths re similar across methods, although
again SAOCP had slightly shorter intervals (as could be expected given its tendency to undercover).
The strongly adaptive regret was similar for all methods. The path length of SAOCP was larger than
any of the other methods. To investigate why, Figure 7 plots ğ‘¤ğ‘¡âˆ’ğ‘¤ğ‘¡âˆ’1, the difference in interval
width between times ğ‘¡âˆ’1 and ğ‘¡, for each method in one of the simulations. The interval widths for
AgACI and DtACI change slowly relative to those for SF-OGD and SAOCP. For SAOCP, we can see
the interval widths have larger fluctuations than for the other methods, explaining its higher path
width. The prediction intervals themselves for the same simulation are shown in Figure 8, which
shows that although the path lengths are quite different, the output prediction intervals are quite
similar."
RESULTS/EXPERIMENTS,0.5160839160839161,simulation_one_plot(simulation_study1$results %>% filter(alpha == 0.9))
RESULTS/EXPERIMENTS,0.5174825174825175,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
RESULTS/EXPERIMENTS,0.5188811188811189,Î± = 0.9 AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5202797202797202,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5216783216783217,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5230769230769231,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5244755244755245,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5258741258741259,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.5272727272727272,"âˆ’0.15
âˆ’0.10
âˆ’0.05"
RESULTS/EXPERIMENTS,0.5286713286713287,"0.00
0.05
0.10"
RESULTS/EXPERIMENTS,0.5300699300699301,CovErr(T)
RESULTS/EXPERIMENTS,0.5314685314685315,"Coverage Error
Simulation study: ARMA errors"
RESULTS/EXPERIMENTS,0.5328671328671328,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
RESULTS/EXPERIMENTS,0.5342657342657343,Î± = 0.9 AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5356643356643357,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5370629370629371,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5384615384615384,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5398601398601398,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5412587412587413,SFâˆ’OGD 5 10 15 20
RESULTS/EXPERIMENTS,0.5426573426573427,MeanWidth(T)
RESULTS/EXPERIMENTS,0.544055944055944,Mean Interval Width
RESULTS/EXPERIMENTS,0.5454545454545454,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
RESULTS/EXPERIMENTS,0.5468531468531469,Î± = 0.9 AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5482517482517483,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5496503496503496,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.551048951048951,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5524475524475524,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5538461538461539,SFâˆ’OGD 1 10 100
RESULTS/EXPERIMENTS,0.5552447552447553,PathLength(T)
RESULTS/EXPERIMENTS,0.5566433566433566,Path Length
RESULTS/EXPERIMENTS,0.558041958041958,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
RESULTS/EXPERIMENTS,0.5594405594405595,Î± = 0.9 AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5608391608391609,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5622377622377622,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5636363636363636,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5650349650349651,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.5664335664335665,SFâˆ’OGD 3 5 10
RESULTS/EXPERIMENTS,0.5678321678321678,"SAReg(T, m)"
RESULTS/EXPERIMENTS,0.5692307692307692,Strongly Adaptive Regret
RESULTS/EXPERIMENTS,0.5706293706293706,"Figure 6: Coverage errors, mean interval widths, path lengths, and strongly adaptive regret (for
ğ‘š= 20) for the first simulation study with target coverage ğ›¼= 0.9."
RESULTS/EXPERIMENTS,0.5720279720279721,fits <- simulation_study1$example_fits
RESULTS/EXPERIMENTS,0.5734265734265734,"par(mfrow = c(2, 2), mar = c(3, 4, 2, 1))
for(i in 1:4) { plot("
RESULTS/EXPERIMENTS,0.5748251748251748,"diff(fits$fit[[i]]$intervals[,2] - fits$fit[[i]]$intervals[,1]),
main = fits$method[[i]],
xlab = ""T"",
ylab = expression(w[t] - w[t - 1]))
}
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))"
RESULTS/EXPERIMENTS,0.5762237762237762,"0
50
150
250
350"
RESULTS/EXPERIMENTS,0.5776223776223777,"0
5
10 AgACI"
RESULTS/EXPERIMENTS,0.579020979020979,wt âˆ’wtâˆ’1
RESULTS/EXPERIMENTS,0.5804195804195804,"0
50
150
250
350"
RESULTS/EXPERIMENTS,0.5818181818181818,"0
2
4
6
8
10"
RESULTS/EXPERIMENTS,0.5832167832167832,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.5846153846153846,wt âˆ’wtâˆ’1
RESULTS/EXPERIMENTS,0.586013986013986,"0
50
150
250
350"
RESULTS/EXPERIMENTS,0.5874125874125874,"âˆ’5
0
5
10 SAOCP"
RESULTS/EXPERIMENTS,0.5888111888111888,wt âˆ’wtâˆ’1
RESULTS/EXPERIMENTS,0.5902097902097903,"0
50
150
250
350"
RESULTS/EXPERIMENTS,0.5916083916083916,"0
5
10 DtACI"
RESULTS/EXPERIMENTS,0.593006993006993,wt âˆ’wtâˆ’1
RESULTS/EXPERIMENTS,0.5944055944055944,"Figure 7: Difference in successive interval widths (ğ‘¤ğ‘¡âˆ’ğ‘¤ğ‘¡âˆ’1) from an illustrative simulation from the
first simulation study."
RESULTS/EXPERIMENTS,0.5958041958041959,fits <- simulation_study1$example_fits
RESULTS/EXPERIMENTS,0.5972027972027972,"coverage
<- format_coverage(map_dbl(map(fits$fit, metrics), `[[`, ""coverage""))
path_length <- format_path_length(map_dbl(map(fits$fit, metrics), `[[`, ""path_length""))"
RESULTS/EXPERIMENTS,0.5986013986013986,"par(mfrow = c(2, 2), mar = c(3, 3, 2, 1))
for(i in 1:4) {"
RESULTS/EXPERIMENTS,0.6,"plot(fits$fit[[i]], legend = FALSE, main = fits$method[[i]], predictions = FALSE, ylim = c(-20
text(x = -0, y = -7.5, labels = bquote(EmpCov == .(coverage[[i]]) ), pos = 4)
text(x = -0, y = -17.5, labels = bquote(PathLength == .(path_length[[i]]) ), pos = 4)
}
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))"
RESULTS/EXPERIMENTS,0.6013986013986014,"0
10
20
30
40
50"
RESULTS/EXPERIMENTS,0.6027972027972028,"âˆ’20
âˆ’10
0
10
20
30 AgACI"
RESULTS/EXPERIMENTS,0.6041958041958042,EmpCov = 86.5%
RESULTS/EXPERIMENTS,0.6055944055944056,PathLength = 7.0
RESULTS/EXPERIMENTS,0.606993006993007,"0
10
20
30
40
50"
RESULTS/EXPERIMENTS,0.6083916083916084,"âˆ’20
âˆ’10
0
10
20
30"
RESULTS/EXPERIMENTS,0.6097902097902098,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.6111888111888112,EmpCov = 88.5%
RESULTS/EXPERIMENTS,0.6125874125874126,PathLength = 16.5
RESULTS/EXPERIMENTS,0.6139860139860139,"0
10
20
30
40
50"
RESULTS/EXPERIMENTS,0.6153846153846154,"âˆ’20
âˆ’10
0
10
20
30 SAOCP"
RESULTS/EXPERIMENTS,0.6167832167832168,EmpCov = 86.5%
RESULTS/EXPERIMENTS,0.6181818181818182,PathLength = 80.2
RESULTS/EXPERIMENTS,0.6195804195804195,"0
10
20
30
40
50"
RESULTS/EXPERIMENTS,0.620979020979021,"âˆ’20
âˆ’10
0
10
20
30 DtACI"
RESULTS/EXPERIMENTS,0.6223776223776224,EmpCov = 86.5%
RESULTS/EXPERIMENTS,0.6237762237762238,PathLength = 11.6
RESULTS/EXPERIMENTS,0.6251748251748251,"Figure 8: Example prediction intervals (target coverage ğ›¼= 0.9) from the first simulation study
for time points 300 to 350; metrics shown are for all time points ğ‘¡â‰¥300. Blue and red points are
observations that fell inside and outside the prediction intervals, respectively."
RESULTS/EXPERIMENTS,0.6265734265734266,"5.2
Distribution shift"
RESULTS/EXPERIMENTS,0.627972027972028,"This simulation study features time series with distribution shifts. The setup is quite simple in order
to probe the basic performance of the methods in response to distribution shift. As a baseline, we
simulate time series of independent data with"
RESULTS/EXPERIMENTS,0.6293706293706294,"ğ‘¦ğ‘¡âˆ¼ğ‘(0, ğœ2ğ‘¡),"
RESULTS/EXPERIMENTS,0.6307692307692307,"ğœğ‘¡= 0.2,"
RESULTS/EXPERIMENTS,0.6321678321678321,"for all ğ‘¡âˆˆJğ‘‡K (ğ‘‡= 500). In the second type of time series, the observations are still independent but
their variance increases halfway through the time series:"
RESULTS/EXPERIMENTS,0.6335664335664336,"ğ‘¦ğ‘¡âˆ¼ğ‘(0, ğœ2ğ‘¡),"
RESULTS/EXPERIMENTS,0.634965034965035,ğœğ‘¡= 0.2 + 0.5ğ•€[ğ‘¡> 250].
RESULTS/EXPERIMENTS,0.6363636363636364,"In each case, the ACI algorithms are provided with the unbiased predictions Ì‚ğœ‡ğ‘¡= 0, ğ‘¡âˆˆJğ‘‡K. Fifty
simulated datasets were generated for each type of time series."
RESULTS/EXPERIMENTS,0.6377622377622377,"simulate <- function(seed, distribution_shift = 0, N = 1e3, sigma = 0.2) {"
RESULTS/EXPERIMENTS,0.6391608391608392,"set.seed(seed)
mu <- rep(0, N)
shift <- 1:N > (N / 2)
yhat <- mu
y <- rnorm(n = length(mu), mean = mu, sd = sigma + ifelse(shift, distribution_shift, 0))"
RESULTS/EXPERIMENTS,0.6405594405594406,"tibble(y = y, yhat = yhat)
}"
RESULTS/EXPERIMENTS,0.641958041958042,metrics <- function(fit) {
RESULTS/EXPERIMENTS,0.6433566433566433,"N <- length(fit$Y)
indices <- which(1:N > 50)
aci_metrics(fit, indices)
}"
RESULTS/EXPERIMENTS,0.6447552447552447,"fit <- function(data, method, alpha, p = NULL) {"
RESULTS/EXPERIMENTS,0.6461538461538462,if(!is.null(p)) p()
RESULTS/EXPERIMENTS,0.6475524475524476,interval_constructor = case_when(
RESULTS/EXPERIMENTS,0.6489510489510489,"method == ""AgACI"" ~ ""conformal"",
method == ""DtACI"" ~ ""conformal"",
method == ""SF-OGD"" ~ ""linear"",
method == ""SAOCP"" ~ ""linear""
)"
RESULTS/EXPERIMENTS,0.6503496503496503,"if(interval_constructor == ""linear"") {"
RESULTS/EXPERIMENTS,0.6517482517482518,"D <- max(abs(data$y - data$yhat)[1:50])
}
else {"
RESULTS/EXPERIMENTS,0.6531468531468532,"D <- 1
}"
RESULTS/EXPERIMENTS,0.6545454545454545,gamma <- D / sqrt(3)
RESULTS/EXPERIMENTS,0.6559440559440559,"if(interval_constructor == ""linear"") {"
RESULTS/EXPERIMENTS,0.6573426573426573,"gamma_grid <- seq(0.1, 2, 0.1)
}
else {"
RESULTS/EXPERIMENTS,0.6587412587412588,"gamma_grid <- c(0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128)
}"
RESULTS/EXPERIMENTS,0.6601398601398601,parameters <- list(
RESULTS/EXPERIMENTS,0.6615384615384615,"interval_constructor = interval_constructor,
D = D,
gamma = gamma,
gamma_grid = gamma_grid
)"
RESULTS/EXPERIMENTS,0.6629370629370629,"aci(data$y, data$yhat, method = method, alpha = alpha, parameters = parameters)
}"
RESULTS/EXPERIMENTS,0.6643356643356644,"N_sims <- 100
simulation_study_setup2 <- expand_grid("
RESULTS/EXPERIMENTS,0.6657342657342658,"index = 1:N_sims,
distribution_shift = c(0, 0.5),
alpha = c(0.8, 0.9, 0.95),
N = 500,
method = c(""AgACI"", ""SF-OGD"", ""SAOCP"", ""DtACI""),
) %>%"
RESULTS/EXPERIMENTS,0.6671328671328671,"mutate(data = pmap(list(index, distribution_shift, N), simulate))"
RESULTS/EXPERIMENTS,0.6685314685314685,"# run_simulation_study2 function is defined in helpers.R
simulation_study2 <- run_simulation_study2(simulation_study_setup2, fit, workers = 8)"
RESULTS/EXPERIMENTS,0.66993006993007,"The coverage error, mean path length, mean interval widths, and strongly adaptive regret (for ğ‘š= 20)
of the algorithms are summarized in Figure 9 (an alternative plot is included in the appendix as
Figure 15). The coverage error of all the algorithms is near the desired value in the absence of
distribution shift. On the contrary, all of the algorithms except AgACI and DtACI undercover when
there is distributional shift. SAOCP tends to have higher average path lengths than the other methods.
In the distribution shift setting, SF-OGD and SAOCP tended to have smaller strongly adaptive regret
than the other methods. An illustrative example of prediction intervals generated by each method for
one of the simulated time series with distribution shift is shown in Figure 10. The SAOCP prediction
intervals in the example before the distribution shift are more jagged than those produced by the
other methods, which illustrates why SAOCP may have longer path lengths."
RESULTS/EXPERIMENTS,0.6713286713286714,simulation_two_plot(simulation_study2$results)
RESULTS/EXPERIMENTS,0.6727272727272727,"Î± = 0.8
Î± = 0.9
Î± = 0.95"
RESULTS/EXPERIMENTS,0.6741258741258741,"No shift
Shift AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.6755244755244755,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.676923076923077,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.6783216783216783,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.6797202797202797,âˆ’0.050
RESULTS/EXPERIMENTS,0.6811188811188811,âˆ’0.025 0.000 0.025
RESULTS/EXPERIMENTS,0.6825174825174826,âˆ’0.050
RESULTS/EXPERIMENTS,0.6839160839160839,âˆ’0.025 0.000 0.025
RESULTS/EXPERIMENTS,0.6853146853146853,CovErr(T)
RESULTS/EXPERIMENTS,0.6867132867132867,Coverage Error
RESULTS/EXPERIMENTS,0.6881118881118881,"Simulation Study:
Distribution Shift"
RESULTS/EXPERIMENTS,0.6895104895104895,"Î± = 0.8
Î± = 0.9
Î± = 0.95"
RESULTS/EXPERIMENTS,0.6909090909090909,"No shift
Shift AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.6923076923076923,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.6937062937062937,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.6951048951048951,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.6965034965034965,"0.5
0.6
0.7
0.8
0.9 1.0 1.5 2.0"
RESULTS/EXPERIMENTS,0.6979020979020979,MeanWidth(T)
RESULTS/EXPERIMENTS,0.6993006993006993,Mean Interval Width
RESULTS/EXPERIMENTS,0.7006993006993008,"Î± = 0.8
Î± = 0.9
Î± = 0.95"
RESULTS/EXPERIMENTS,0.7020979020979021,"No shift
Shift AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.7034965034965035,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.7048951048951049,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.7062937062937062,"SFâˆ’OGD 1
3 10
30 10 30 100"
RESULTS/EXPERIMENTS,0.7076923076923077,ACI Method
RESULTS/EXPERIMENTS,0.7090909090909091,PathLength(T)
RESULTS/EXPERIMENTS,0.7104895104895105,Path Length
RESULTS/EXPERIMENTS,0.7118881118881119,"Î± = 0.8
Î± = 0.9
Î± = 0.95"
RESULTS/EXPERIMENTS,0.7132867132867133,"No shift
Shift AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.7146853146853147,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.7160839160839161,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.7174825174825175,SFâˆ’OGD 0.3 0.5 1.0 1 2 3
RESULTS/EXPERIMENTS,0.7188811188811188,ACI Method
RESULTS/EXPERIMENTS,0.7202797202797203,"SAReg(T, m)"
RESULTS/EXPERIMENTS,0.7216783216783217,Strongly Adaptive Regret
RESULTS/EXPERIMENTS,0.7230769230769231,"Figure 9: Coverage error, mean interval width, path length, and strongly adaptive regret (ğ‘š= 20) for
ğ›¼= 0.8, 0.9, 0.95 and simulations with and without distributional shift."
RESULTS/EXPERIMENTS,0.7244755244755244,fits <- simulation_study2$example_fits
RESULTS/EXPERIMENTS,0.7258741258741259,"coverage
<- format_coverage(extract_metric(fits$fit, ""coverage""))
path_length <- format_path_length(extract_metric(fits$fit, ""path_length""))"
RESULTS/EXPERIMENTS,0.7272727272727273,"par(mfrow = c(2, 2), mar = c(3, 3, 2, 1))
for(i in 1:4) {"
RESULTS/EXPERIMENTS,0.7286713286713287,"plot(fits$fit[[i]], legend = FALSE, main = fits$method[[i]], index = 51:500)
text(x = -10, y = -1.5, labels = bquote(EmpCov == .(coverage[[i]]) ), pos = 4)
text(x = -10, y = -2, labels = bquote(PathLength == .(path_length[[i]]) ), pos = 4)
}
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))"
RESULTS/EXPERIMENTS,0.73006993006993,"0
100
200
300
400"
RESULTS/EXPERIMENTS,0.7314685314685314,"âˆ’2
âˆ’1
0
1
2 AgACI"
RESULTS/EXPERIMENTS,0.7328671328671329,"EmpCov = 87.0%
PathLength = 28.7"
RESULTS/EXPERIMENTS,0.7342657342657343,"0
100
200
300
400"
RESULTS/EXPERIMENTS,0.7356643356643356,"âˆ’2
âˆ’1
0
1
2"
RESULTS/EXPERIMENTS,0.737062937062937,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.7384615384615385,"EmpCov = 85.0%
PathLength = 13.2"
RESULTS/EXPERIMENTS,0.7398601398601399,"0
100
200
300
400"
RESULTS/EXPERIMENTS,0.7412587412587412,"âˆ’2
âˆ’1
0
1
2 SAOCP"
RESULTS/EXPERIMENTS,0.7426573426573426,EmpCov = 84.2%
RESULTS/EXPERIMENTS,0.7440559440559441,PathLength = 51.1
RESULTS/EXPERIMENTS,0.7454545454545455,"0
100
200
300
400"
RESULTS/EXPERIMENTS,0.7468531468531469,"âˆ’2
âˆ’1
0
1
2 DtACI"
RESULTS/EXPERIMENTS,0.7482517482517482,EmpCov = 88.0%
RESULTS/EXPERIMENTS,0.7496503496503496,PathLength = 40.1
RESULTS/EXPERIMENTS,0.7510489510489511,"Figure 10: Example prediction intervals (target coverage ğ›¼= 0.9) from the second simulation study
of time series with distributional shift, in which the shift occurs at time 250. Blue and red points are
observations that fell inside and outside the prediction intervals, respectively."
RESULTS/EXPERIMENTS,0.7524475524475525,"6
Case Study: Influenza Forecasting"
RESULTS/EXPERIMENTS,0.7538461538461538,"Influenza is a highly infectious disease that is estimated to infect approximately one billion individuals
each year around the world (Krammer et al. 2018). Influenza incidence in temperate climates tends to
follow a seasonal pattern, with the highest number of infections during what is commonly referred to
as the flu season (Lofgren et al. 2007). Accurate forecasting of influenza is of significant interest to aid
in public health planning and resource allocation. To investigate the accuracy of influenza forecasts,
the US Centers for Disease Control (CDC) initiated a challenge, referred to as FluSight, in which
teams from multiple institutions submitted weekly forecasts of influenza incidence (Biggerstaff et al.
2016). Reich et al. (2019) evaluated the accuracy of the forecasts over seven flu seasons from 2010 to
2017. As a case study, we investigate the use of ACI algorithms to augment the FluSight forecasts
with prediction intervals."
RESULTS/EXPERIMENTS,0.7552447552447552,"The FluSight challenge collected forecasts for multiple prediction targets. For this case study, we
focus on national (US) one-week ahead forecasts of weighted influenza-like illness (wILI), which"
RESULTS/EXPERIMENTS,0.7566433566433567,"is a population-weighted percentage of doctors visits where patients presented with influenza-like
symptoms (Biggerstaff et al. 2016). The FluSight dataset, which is publicly available, include forecasts
derived from 21 different forecasting models, from both mechanistic and statistical viewpoints
(Flusight Network 2020; Tushar et al. 2018, 2019). For our purposes, we treat the way the forecasts
were produced as a black box."
RESULTS/EXPERIMENTS,0.7580419580419581,"Formally, let ğ‘¦ğ‘¡, ğ‘¡âˆˆJğ‘‡K be the observed national wILI at time ğ‘¡, and let
Ì‚ğœ‡ğ‘—,ğ‘¡, ğ‘—âˆˆJğ½K, be the one-
week ahead forecast of the wILI from model ğ‘—at time ğ‘¡. Two of the original 21 forecasting meth-
ods were excluded from this case study due to poor predictive performance (Delphi_Uniform and
CUBMA). In addition, six methods had identical forecasts (CU_EAKFC_SIRS, CU_EKF_SEIRS, CU_EKF_SIRS,
CU_RHF_SEIRS, CU_RHF_SIRS), and therefore we only included one (CU_EAKFC_SEIRS) in the analysis.
The ACI methods were then applied to the log-observations and log-predictions, where the log-
transformation was used to constrain the final prediction intervals to be positive. The first flu season
(2010-2011) was used as a warm-up for each ACI method, and we report the empirical performance
of the prediction intervals for the subsequent seasons (six seasons from 2012-2013 to 2016-2017). The
ACI algorithms target prediction intervals with coverage of ğ›¼= 0.8, ğ›¼= 0.9, and ğ›¼= 0.95. As in the
simulation study, we used the interval constructor corresponding to the original presentation of each
algorithm (see Table 1)."
RESULTS/EXPERIMENTS,0.7594405594405594,"# Paste together URL so it is not cut off in PDF
url <- paste0(""https://raw.githubusercontent.com/FluSightNetwork/"","
RESULTS/EXPERIMENTS,0.7608391608391608,"""cdc-flusight-ensemble/master/scores/point_ests.csv"")
raw_data <- read_csv(url, show_col_types = FALSE)"
RESULTS/EXPERIMENTS,0.7622377622377622,"fit <- function(data, method, alpha) {"
RESULTS/EXPERIMENTS,0.7636363636363637,"first_season <- data$Season == ""2010/2011""
D <- max(abs(data$obs_value - data$Value)[first_season])"
RESULTS/EXPERIMENTS,0.765034965034965,interval_constructor = case_when(
RESULTS/EXPERIMENTS,0.7664335664335664,"method == ""AgACI"" ~ ""conformal"",
method == ""DtACI"" ~ ""conformal"",
method == ""SF-OGD"" ~ ""linear"",
method == ""SAOCP"" ~ ""linear""
)"
RESULTS/EXPERIMENTS,0.7678321678321678,gamma <- D / sqrt(3)
RESULTS/EXPERIMENTS,0.7692307692307693,"if(interval_constructor == ""linear"") {"
RESULTS/EXPERIMENTS,0.7706293706293706,"gamma_grid = seq(0.1, 1, 0.1)
}
else {"
RESULTS/EXPERIMENTS,0.772027972027972,"gamma_grid <- c(0.001, 0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128)
}"
RESULTS/EXPERIMENTS,0.7734265734265734,parameters <- list(
RESULTS/EXPERIMENTS,0.7748251748251749,"interval_constructor = interval_constructor,
D = D,
gamma = gamma,
gamma_grid = gamma_grid
) aci("
RESULTS/EXPERIMENTS,0.7762237762237763,"Y = log(data$obs_value),
predictions = log(data$Value),
method = method,
parameters = parameters,
alpha = alpha
)
}"
RESULTS/EXPERIMENTS,0.7776223776223776,"metrics <- function(data, fit) {"
RESULTS/EXPERIMENTS,0.779020979020979,"aci_metrics(fit, indices = which(data$Season != ""2010/2011""))
}"
RESULTS/EXPERIMENTS,0.7804195804195804,analysis_data <- raw_data %>%
RESULTS/EXPERIMENTS,0.7818181818181819,filter(
RESULTS/EXPERIMENTS,0.7832167832167832,"Target == ""1 wk ahead"",
Location == ""US National"",
!(model_name %in% c(""Delphi_Uniform"", ""CUBMA"", ""CU_EAKFC_SIRS"", ""CU_EKF_SEIRS"","
RESULTS/EXPERIMENTS,0.7846153846153846,"""CU_EKF_SIRS"", ""CU_RHF_SEIRS"", ""CU_RHF_SIRS""))
) %>%
arrange(Year, Model.Week) %>%
group_by(model_name) %>%
nest()"
RESULTS/EXPERIMENTS,0.786013986013986,fits <- expand_grid(
RESULTS/EXPERIMENTS,0.7874125874125875,"analysis_data,
tibble(method = c(""AgACI"", ""DtACI"", ""SF-OGD"", ""SAOCP"")),
tibble(alpha = c(0.8, 0.9, 0.95))
) %>%"
RESULTS/EXPERIMENTS,0.7888111888111888,"mutate(fit = pmap(list(data, method, alpha), fit),"
RESULTS/EXPERIMENTS,0.7902097902097902,"metrics = map2(data, fit, metrics))"
RESULTS/EXPERIMENTS,0.7916083916083916,case_study_results <- fits %>%
RESULTS/EXPERIMENTS,0.793006993006993,"select(-data, -fit) %>%
mutate(metrics = map(metrics, as_tibble)) %>%
unnest(c(metrics))"
RESULTS/EXPERIMENTS,0.7944055944055944,"The coverage errors, mean interval widths, path lengths, and strongly adaptive regret (for ğ‘š= 20) of
the prediction intervals for each of the underlying forecast models is shown in Figure 11. In all cases
the absolute coverage error was less than 0.1. SF-OGD performed particularly well, with coverage
errors close to zero for all forecasting models. Interval widths were similar across methods, with
SAOCP slightly shorter. Path Lengths were shorter for AgACI and DtACI and longer for SAOCP."
RESULTS/EXPERIMENTS,0.7958041958041958,case_study_plot(case_study_results)
RESULTS/EXPERIMENTS,0.7972027972027972,"Î± = 0.8
Î± = 0.9
Î± = 0.95 AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.7986013986013986,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8013986013986014,SFâˆ’OGD âˆ’0.02
RESULTS/EXPERIMENTS,0.8027972027972028,"0.00
0.02
0.04
0.06"
RESULTS/EXPERIMENTS,0.8041958041958042,CovErr(T)
RESULTS/EXPERIMENTS,0.8055944055944056,"Coverage Error
Case Study Results"
RESULTS/EXPERIMENTS,0.806993006993007,"0.4
0.6
0.8
1.0
1.2 0.4 0.6 0.8"
RESULTS/EXPERIMENTS,0.8083916083916084,"1.0
Î± = 0.8
Î± = 0.9
Î± = 0.95 AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.8097902097902098,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8111888111888111,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8125874125874126,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.813986013986014,"0.3
0.4
0.5
0.6
0.7"
RESULTS/EXPERIMENTS,0.8153846153846154,MeanWidth(T)
RESULTS/EXPERIMENTS,0.8167832167832167,Interval Width
RESULTS/EXPERIMENTS,0.8181818181818182,"Î± = 0.8
Î± = 0.9
Î± = 0.95 AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.8195804195804196,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.820979020979021,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8223776223776224,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.8237762237762237,"1
3
10
30"
RESULTS/EXPERIMENTS,0.8251748251748252,ACI Method
RESULTS/EXPERIMENTS,0.8265734265734266,PathLength(T)
RESULTS/EXPERIMENTS,0.827972027972028,Path Length
RESULTS/EXPERIMENTS,0.8293706293706293,"Î± = 0.8
Î± = 0.9
Î± = 0.95 AgACI DtACI SAOCP"
RESULTS/EXPERIMENTS,0.8307692307692308,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8321678321678322,SFâˆ’OGD AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8335664335664336,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.8349650349650349,"0.3
0.5 1.0"
RESULTS/EXPERIMENTS,0.8363636363636363,ACI Method
RESULTS/EXPERIMENTS,0.8377622377622378,"SAReg(T, m)"
RESULTS/EXPERIMENTS,0.8391608391608392,Strongly Adaptive Regret
RESULTS/EXPERIMENTS,0.8405594405594405,"Figure 11: Coverage errors, mean interval widths, path lengths, and strongly adaptive regret (for
ğ‘š= 20) of prediction intervals generated with each ACI method based on forecasts from each of the
19 underlying influenza forecasting models."
RESULTS/EXPERIMENTS,0.8419580419580419,"As an illustrative example, in Figure 12 we plot the point forecasts from one of the forecasting models"
RESULTS/EXPERIMENTS,0.8433566433566434,"(based on SARIMA with no seasonal differencing) and the associated ACI-generated 90% prediction
intervals for each season from 2011-2017. In general, in this practical setting all of the ACI algorithms
yield quite similar prediction intervals. Interestingly, the forecasts in 2011-2012 underpredicted the
observations for much of the season. The algorithm responds by making the intervals wider to
cover the observations, and because the intervals are symmetric the lower bound then becomes
unrealistically low. A similar phenomenon can be seen in the growth phase of the 2012/2013 season
as well."
RESULTS/EXPERIMENTS,0.8447552447552448,sarima_fits <- fits %>% filter(
RESULTS/EXPERIMENTS,0.8461538461538461,"model_name == ""ReichLab_sarima_seasonal_difference_FALSE"",
alpha == 0.9
) %>%"
RESULTS/EXPERIMENTS,0.8475524475524475,"mutate(output = map(fit, extract_intervals)) %>%
select(method, alpha, data, output) %>%
unnest(c(data, output)) %>%
filter(Season != ""2010/2011"")"
RESULTS/EXPERIMENTS,0.848951048951049,sarima_fits %>%
RESULTS/EXPERIMENTS,0.8503496503496504,"ggplot(aes(x = Model.Week, y = log(obs_value))) +
geom_point(aes(shape = ""Observed"")) +
geom_line(aes(y = pred, lty = ""Forecast""), color = ""black"") +
geom_line(aes(y = lower, color = method)) +
geom_line(aes(y = upper, color = method)) +
facet_wrap(~Season) +
labs("
RESULTS/EXPERIMENTS,0.8517482517482518,"x = ""Flu Season Week"",
y = ""log(wILI)"",
title = ""SARIMA forecasts with ACI 90% prediction intervals""
)"
RESULTS/EXPERIMENTS,0.8531468531468531,"2014/2015
2015/2016
2016/2017"
RESULTS/EXPERIMENTS,0.8545454545454545,"2011/2012
2012/2013
2013/2014"
RESULTS/EXPERIMENTS,0.855944055944056,"40
50
60
70
40
50
60
70
40
50
60
70 0 1 2 0 1 2"
RESULTS/EXPERIMENTS,0.8573426573426574,Flu Season Week
RESULTS/EXPERIMENTS,0.8587412587412587,log(wILI)
RESULTS/EXPERIMENTS,0.8601398601398601,method AgACI DtACI SAOCP
RESULTS/EXPERIMENTS,0.8615384615384616,SFâˆ’OGD
RESULTS/EXPERIMENTS,0.862937062937063,linetype
RESULTS/EXPERIMENTS,0.8643356643356643,Forecast shape
RESULTS/EXPERIMENTS,0.8657342657342657,Observed
RESULTS/EXPERIMENTS,0.8671328671328671,SARIMA forecasts with ACI 90% prediction intervals
RESULTS/EXPERIMENTS,0.8685314685314686,"Figure 12: Example conformal prediction intervals for six flu seasons based on forecasts from a
SARIMA type model."
CONCLUSION/DISCUSSION ,0.8699300699300699,"7
Discussion"
CONCLUSION/DISCUSSION ,0.8713286713286713,"The results of our simulations and case study show that, when tuning parameters are chosen
well, Adaptive Conformal Inference algorithms yield well-performing prediction intervals. On the
contrary, poor choice of tuning parameters can lead to intervals of low utility: for one example,
Figure Figure 4 shows how choosing the tuning parameter for SF-OGD to be too small can lead
to intervals that update too slowly and significantly undercover. Furthermore, in some cases the
prediction intervals may appear to perform well with respect to metrics like the empirical coverage
error, while simultaneously being useless in practice. The original ACI algorithm illustrates this
phenomenon: too small a value of its learning rate ğ›¾yields prediction intervals that are not reactive
enough, while too large a value yields intervals that change too fast. In both cases, the empirical
coverage may appear well-calibrated, while the prediction intervals will not be useful. Thus, the core
challenge in designing an ACI algorithm is in finding an optimal level of reactivity for the prediction
intervals. As users of these algorithms, the challenge is in finding values for the tuning parameters
that avoid pathological behaviors."
CONCLUSION/DISCUSSION ,0.8727272727272727,"Several of the algorithms investigated in this paper handle the problem of finding an optimal level of
reactivity by aggregating prediction intervals generated by a set of underlying ACI algorithms. Our
results show the algorithms can perform well in multiple difficult scenarios. However, the overall
effect of these approaches is to shift the problem to a higher level of abstraction: we still need to
set tuning parameters that control the amount of reactivity, but do so at a higher level than the
original ACI algorithm. It is desirable that these tuning parameters be easily interpretable, with
simple strategies available for setting them. An advantage of the SF-OGD and SAOCP algorithms in
this respect are that their main tuning parameter, the maximum radius ğ·, is easily interpretable as the
maximum possible difference between the input predictions and the truth. It is also straightforward
to choose this parameter based on a calibration set, although this strategy does not necessarily work"
CONCLUSION/DISCUSSION ,0.8741258741258742,"well in cases of distribution shift. We also found that an advantage of the AgACI method is its
robustness to the choice of its main tuning parameter, the set of candidate learning rates, in the sense
that the grid of candidate learning rates can always be expanded as illustrated in Section 3.2.2."
CONCLUSION/DISCUSSION ,0.8755244755244755,"A key challenge in tuning the algorithms arises in settings of distribution shift, where methods for
choosing hyperparameters based on a calibration set from before the distribution shift will likely not
perform well. The second simulation study we conducted probed this setting in a simple scenario.
We found that several of the methods yielded prediction intervals that had non-optimal empirical
coverage. As we picked hyperparameters based on a calibration set formed before the distribution
shift, it is not surprising that the resulting tuning parameters are not optimal. This underscores the
difficulty in designing ACI algorithms that can adapt to distribution shifts, and in finding robust
methods for choosing hyperparameters. In practice, it is possible the second simulation study does
not accurately reflect real-world scenarios. Indeed, the benchmarks presented in Bhatnagar et al.
(2023) using the datasets from the M4 competition (Makridakis, Spiliotis, and Assimakopoulos 2020),
and using point predictions generated by diverse prediction algorithms, found that ACI algorithms
exhibited good performance in terms of empirical coverage. Nevertheless, our recommendation for
future papers in this line of research is to include simulation studies for simple distributional shift
scenarios as a benchmark."
CONCLUSION/DISCUSSION ,0.8769230769230769,"Our case study results illustrate the dependence of the ACI algorithms on having access to high-
quality point predictions. If the predictions are biased, for example, then the prediction intervals
may be able to achieve optimal coverage at the expense of larger interval widths. This type of
underperformance due to biased input predictions can be seen in the 2011-2012 flu season in the case
study Figure 12. One way bias can arise in the underlying predictions is due to model misspecification:
for example, if a forecast method assumes a time series will evolve according to a particular parametric
model that does not accurately capture the true data generating process, then the forecasts may be
systematically biased. Using ensemble methods to combine forecasts from several flexible machine
learning algorithms is one strategy that can be used to hedge against such model misspecification
and improve the quality of forecasts (Makridakis, Spiliotis, and Assimakopoulos 2020)."
CONCLUSION/DISCUSSION ,0.8783216783216783,"Overall, our findings illustrate strengths and weaknesses of all the considered algorithms. The
original ACI algorithm is appealing in its simplicity, although its performance depends entirely on a
good choice of its tuning parameter. AgACI tended to perform well in the simulation studies in terms
of coverage error, although it had slightly higher strongly adaptive regret than other algorithms in
some settings. However, there are relatively fewer theoretical guarantees available for AgACI than
the other methods. DtACI, SF-OGD, and SAOCP all feature strong theoretical results, although they
exhibited some differences in the simulation studies, with SF-OGD and SAOCP slightly undercovering
in some scenarios. SAOCP also had longer path lengths than other methods in simulations, although
in practice in the influenza forecasting task longer path lengths does not seem to effect the plausibility
of the prediction intervals the algorithm produces."
CONCLUSION/DISCUSSION ,0.8797202797202798,"There remain many possible extensions of ACI algorithms. The algorithms presented in this work
primarily consider symmetric intervals evaluated using the pinball loss function (AgACI can yield
asymmetric intervals because the aggregation rule is applied separately to the lower and upper
bounds from the underlying experts, but those underlying experts only produce symmetric intervals).
A simple extension would switch to using the interval loss function (Gneiting and Raftery 2007),
which would allow for asymmetric intervals where two parameters are learned for the upper and
lower bounds, respectively. It may also be of interest to generate prediction intervals that have
coverage guarantees for arbitrary subsets of observations (for example, we may seek prediction
intervals for daily observations that have near optimal coverage for every day of the week, or month
of the year), similar to guarantees provided by the MultiValid Prediction method described in (Bastani
et al. 2022). Another avenue for theoretical research is to relax the assumption of bounded radii"
CONCLUSION/DISCUSSION ,0.8811188811188811,necessary for the theoretical results of algorithms such as SAOCP.
OTHER,0.8825174825174825,Acknowledgements
OTHER,0.8839160839160839,"This research is partially supported by the Agence Nationale de la Recherche as part of the â€œIn-
vestissements dâ€™avenirâ€ program (reference ANR-19-P3IA-0001; PRAIRIE 3IA Institute). We would
like to thank Margaux Zaffran for providing helpful comments on the manuscript."
REFERENCES,0.8853146853146853,References
REFERENCES,0.8867132867132868,"Angelopoulos, Anastasios N., Rina Foygel Barber, and Stephen Bates. 2024. â€œOnline Conformal
Prediction with Decaying Step Sizes.â€ https://arxiv.org/abs/2402.01139.
Angelopoulos, Anastasios N., and Stephen Bates. 2023. â€œConformal Prediction: A Gentle Introduc-
tion.â€ Found. Trends Mach. Learn. 16 (4): 494â€“591. https://doi.org/10.1561/2200000101.
Bastani, Osbert, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron
Roth. 2022. â€œPractical Adversarial Multivalid Conformal Prediction.â€ In Advances in Neural
Information Processing Systems, edited by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K.
Cho, and A. Oh, 35:29362â€“73. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files
/paper/2022/file/bcdaaa1aec3ae2aa39542acefdec4e4b-Paper-Conference.pdf.
Bhatnagar, Aadyot, Huan Wang, Caiming Xiong, and Yu Bai. 2023. â€œImproved Online Conformal
Prediction via Strongly Adaptive Online Learning.â€ In Proceedings of the 40th International
Conference on Machine Learning. ICMLâ€™23. Honolulu, Hawaii, USA: JMLR.org.
Biggerstaff, Matthew, David Alper, Mark Dredze, Spencer Fox, Isaac Chun-Hai Fung, Kyle S. Hick-
mann, Bryan Lewis, et al. 2016. â€œResults from the Centers for Disease Control and Preven-
tionâ€™s Predict the 2013â€“2014 Influenza Season Challenge.â€ BMC Infectious Diseases 16 (1): 357.
https://doi.org/10.1186/s12879-016-1669-x.
Cesa-Bianchi, Nicolo, and Gabor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University
Press. https://doi.org/10.1017/CBO9780511546921.
Diquigiovanni, Jacopo, Matteo Fontana, Aldo Solari, Simone Vantini, and Paolo Vergottini. 2022.
conformalInference.fd: Tools for Conformal Inference for Regression in Multivariate Functional
Setting. https://CRAN.R-project.org/package=conformalInference.fd.
Feldman, Shai, Liran Ringel, Stephen Bates, and Yaniv Romano. 2023. â€œAchieving Risk Control in
Online Learning Settings.â€ Transactions on Machine Learning Research. https://openreview.net/f
orum?id=5Y04GWvoJu.
Flusight Network. 2020. â€œGitHub - FluSightNetwork/Cdc-Flusight-Ensemble: Guidelines and Fore-
casts for a Collaborative u.s. Influenza Forecasting Project.â€ https://github.com/FluSightNetwork/.
Friedman, Jerome H., Eric Grosse, and Werner Stuetzle. 1983. â€œMultidimensional Additive Spline
Approximation.â€ SIAM Journal on Scientific and Statistical Computing 4 (2): 291â€“301. https:
//doi.org/10.1137/0904023.
Gaillard, Pierre, Yannig Goude, Laurent Plagne, Thibaut Dubois, and Benoit Thieurmel. 2023. Opera:
Online Prediction by Expert Aggregation. http://pierre.gaillard.me/opera.html.
Gasparin, Matteo, and Aaditya Ramdas. 2024. â€œConformal Online Model Aggregation.â€ https:
//arxiv.org/abs/2403.15527.
Gibbs, Isaac, and Emmanuel Candes. 2021. â€œAdaptive Conformal Inference Under Distribution Shift.â€
In Advances in Neural Information Processing Systems, edited by M. Ranzato, A. Beygelzimer, Y.
Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:1660â€“72. Curran Associates, Inc. https://proc
eedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf.
Gibbs, Isaac, and Emmanuel CandÃ¨s. 2022. â€œConformal Inference for Online Prediction with Arbitrary
Distribution Shifts.â€ https://arxiv.org/abs/2208.08401.
Gneiting, Tilmann, and Adrian E Raftery. 2007. â€œStrictly Proper Scoring Rules, Prediction, and
Estimation.â€ Journal of the American Statistical Association 102 (477): 359â€“78. https://doi.org/10.1"
REFERENCES,0.8881118881118881,"198/016214506000001437.
Gradu, Paula, Elad Hazan, and Edgar Minasyan. 2023. â€œAdaptive Regret for Control of Time-Varying
Dynamics.â€ In Proceedings of the 5th Annual Learning for Dynamics and Control Conference, edited
by Nikolai Matni, Manfred Morari, and George J. Pappas, 211:560â€“72. Proceedings of Machine
Learning Research. PMLR. https://proceedings.mlr.press/v211/gradu23a.html.
Jun, Kwang-Sung, Francesco Orabona, Stephen Wright, and Rebecca Willett. 2017. â€œImproved
Strongly Adaptive Online Learning using Coin Betting.â€ In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, edited by Aarti Singh and Jerry Zhu, 54:943â€“51.
Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v54/jun17a.h
tml.
Krammer, Florian, Gavin J. D. Smith, Ron A. M. Fouchier, Malik Peiris, Katherine Kedzierska, Peter
C. Doherty, Peter Palese, et al. 2018. â€œInfluenza.â€ Nature Reviews Disease Primers 4 (1): 3.
https://doi.org/10.1038/s41572-018-0002-y.
Lei, Lihua, and Emmanuel J. CandÃ¨s. 2021. â€œConformal Inference of Counterfactuals and Individual
Treatment Effects.â€ Journal of the Royal Statistical Society Series B: Statistical Methodology 83 (5):
911â€“38. https://doi.org/10.1111/rssb.12445.
Lofgren, Eric, N. H. Fefferman, Y. N. Naumov, J. Gorski, and E. N. Naumova. 2007. â€œInfluenza
Seasonality: Underlying Causes and Modeling Theories.â€ Journal of Virology 81 (11): 5429â€“36.
https://doi.org/10.1128/jvi.01680-06.
Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2020. â€œThe M4 Competition:
100,000 Time Series and 61 Forecasting Methods.â€ International Journal of Forecasting 36 (1):
54â€“74. https://doi.org/https://doi.org/10.1016/j.ijforecast.2019.04.014.
Orabona, Francesco, and DÃ¡vid PÃ¡l. 2018. â€œScale-Free Online Learning.â€ Theoretical Computer Science
716: 50â€“69. https://doi.org/https://doi.org/10.1016/j.tcs.2017.11.021.
Reich, Nicholas G, Logan C Brooks, Spencer J Fox, Sasikiran Kandula, Craig J McGowan, Evan
Moore, Dave Osthus, et al. 2019. â€œA Collaborative Multiyear, Multimodel Assessment of Seasonal
Influenza Forecasting in the United States.â€ Proc. Natl. Acad. Sci. U. S. A. 116 (8): 3146â€“54.
Shafer, Glenn, and Vladimir Vovk. 2008. â€œA Tutorial on Conformal Prediction.â€ J. Mach. Learn. Res. 9
(June): 371â€“421.
Tibshirani, Ryan, Jacopo Diquigiovanni, Matteo Fontana, and Paolo Vergottini. 2019. conformalInfer-
ence: Tools for Conformal Inference in Regression.
Tushar, Abhinav, Nicholas G Reich, tkcy, brookslogan, d-osthus, Craig McGowan, Evan Ray, et al.
2019. â€œFluSightNetwork/cdc-flusight-ensemble: End of 2018/2019 US influenza season.â€ Zenodo.
https://doi.org/10.5281/zenodo.3454212.
Tushar, Abhinav, Nicholas Reich, Teresa Yamana, Dave Osthus, Craig McGowan, Evan Ray, and et al.
2018. â€œFluSightNetwork: Cdc-Flusight-Ensemble Repository.â€ https://github.com/FluSightNetwo
rk/cdc-flusight-ensemble.
Vovk, Vladimir, Alex Gammerman, and Glenn Shafer. 2005. Algorithmic Learning in a Random World.
Berlin, Heidelberg: Springer-Verlag.
Wintenberger, Olivier. 2017. â€œOptimal Learning with Bernstein Online Aggregation.â€ Machine
Learning 106 (1): 119â€“41. https://doi.org/10.1007/s10994-016-5592-6.
Wright, Marvin N., and Andreas Ziegler. 2017. â€œranger: A Fast Implementation of Random Forests
for High Dimensional Data in C++ and R.â€ Journal of Statistical Software 77 (1): 1â€“17. https:
//doi.org/10.18637/jss.v077.i01.
Xu, Chen, and Yao Xie. 2021. â€œConformal Prediction Interval for Dynamic Time-Series.â€ In Proceedings
of the 38th International Conference on Machine Learning, edited by Marina Meila and Tong Zhang,
139:11559â€“69. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/
v139/xu21h.html.
â€”â€”â€”. 2023. â€œSequential Predictive Conformal Inference for Time Series.â€ In Proceedings of the
40th International Conference on Machine Learning, edited by Andreas Krause, Emma Brunskill,"
REFERENCES,0.8895104895104895,"Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, 202:38707â€“27. Pro-
ceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v202/xu23r.html.
Zaffran, Margaux, Olivier Feron, Yannig Goude, Julie Josse, and Aymeric Dieuleveut. 2022. â€œAdaptive
Conformal Predictions for Time Series.â€ In Proceedings of the 39th International Conference on
Machine Learning, edited by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato, 162:25834â€“66. Proceedings of Machine Learning Research. PMLR.
https://proceedings.mlr.press/v162/zaffran22a.html.
Zhang, Zhiyu, David Bombara, and Heng Yang. 2024. â€œDiscounted Adaptive Online Prediction.â€"
REFERENCES,0.8909090909090909,https://arxiv.org/abs/2402.02720.
APPENDIX,0.8923076923076924,"8
Appendix"
APPENDIX,0.8937062937062937,"8.1
Additional simulation study results"
APPENDIX,0.8951048951048951,simulation_one_plot(simulation_study1$results)
APPENDIX,0.8965034965034965,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
APPENDIX,0.8979020979020979,Î± = 0.8Î± = 0.9Î± = 0.9 AgACI DtACI SAOCP
APPENDIX,0.8993006993006993,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9006993006993007,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9020979020979021,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9034965034965035,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9048951048951049,SFâˆ’OGD
APPENDIX,0.9062937062937063,"âˆ’0.2
âˆ’0.1
0.0
0.1
0.2"
APPENDIX,0.9076923076923077,"âˆ’0.2
âˆ’0.1
0.0
0.1
0.2"
APPENDIX,0.9090909090909091,"âˆ’0.2
âˆ’0.1
0.0
0.1
0.2"
APPENDIX,0.9104895104895104,CovErr(T)
APPENDIX,0.9118881118881119,"Coverage Error
Simulation study: ARMA errors"
APPENDIX,0.9132867132867133,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
APPENDIX,0.9146853146853147,Î± = 0.8Î± = 0.9Î± = 0.9 AgACI DtACI SAOCP
APPENDIX,0.916083916083916,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9174825174825175,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9188811188811189,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9202797202797203,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9216783216783216,SFâˆ’OGD
APPENDIX,0.9230769230769231,"5
10
15
20"
APPENDIX,0.9244755244755245,"5
10
15
20"
APPENDIX,0.9258741258741259,"10
15
20
25"
APPENDIX,0.9272727272727272,MeanWidth(T)
APPENDIX,0.9286713286713286,Mean Interval Width
APPENDIX,0.9300699300699301,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
APPENDIX,0.9314685314685315,Î± = 0.8Î± = 0.9Î± = 0.9 AgACI DtACI SAOCP
APPENDIX,0.9328671328671329,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9342657342657342,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9356643356643357,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9370629370629371,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9384615384615385,SFâˆ’OGD
APPENDIX,0.9398601398601398,"1
10
100"
APPENDIX,0.9412587412587412,"1
10
100"
APPENDIX,0.9426573426573427,"0.1
1.0
10.0
100.0"
APPENDIX,0.9440559440559441,PathLength(T)
APPENDIX,0.9454545454545454,Path Length
APPENDIX,0.9468531468531468,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9
Ïˆ = Î¸ = 0.95
Ïˆ = Î¸ = 0.99"
APPENDIX,0.9482517482517483,Î± = 0.8Î± = 0.9Î± = 0.9 AgACI DtACI SAOCP
APPENDIX,0.9496503496503497,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.951048951048951,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9524475524475524,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9538461538461539,SFâˆ’OGD AgACI DtACI SAOCP
APPENDIX,0.9552447552447553,SFâˆ’OGD
APPENDIX,0.9566433566433566,"1
3
10 35
10 35
10"
APPENDIX,0.958041958041958,"SAReg(T, m)"
APPENDIX,0.9594405594405594,Strongly Adaptive Regret
APPENDIX,0.9608391608391609,simulation_one_joint_plot(simulation_study1$results)
APPENDIX,0.9622377622377623,"Ïˆ = Î¸ = 0.1
Ïˆ = Î¸ = 0.8
Ïˆ = Î¸ = 0.9 Ïˆ = Î¸ = 0.95 Ïˆ = Î¸ = 0.99"
APPENDIX,0.9636363636363636,"Î± = 0.8
Î± = 0.9
Î± = 0.95 âˆ’0.05 0.00 0.05 0.10 âˆ’0.05 0.00 0.05 0.10 âˆ’0.05 0.00 0.05 0.10 âˆ’0.05 0.00 0.05 0.10 âˆ’0.05 0.00 0.05 0.10 7.5 10.0 12.5 15.0 9 12 15"
APPENDIX,0.965034965034965,"7.5
10.0
12.5
15.0
17.5"
APPENDIX,0.9664335664335665,CovErr(T)
APPENDIX,0.9678321678321679,MeanWidth(T)
APPENDIX,0.9692307692307692,method AgACI DtACI SAOCP
APPENDIX,0.9706293706293706,SFâˆ’OGD
APPENDIX,0.972027972027972,Simulation Study: ARMA errors
APPENDIX,0.9734265734265735,"Figure 14: Mean Interval Width vs Coverage Error for the first simulation study. The error bars
represent the 10% to 90% quantiles of the metrics over the simulation datasets."
APPENDIX,0.9748251748251748,"Î± = 0.8
Î± = 0.9
Î± = 0.95 âˆ’0.04 âˆ’0.02 0.00 âˆ’0.04 âˆ’0.02 0.00 âˆ’0.04 âˆ’0.02 0.00 0.5 1.0 1.5 2.0"
APPENDIX,0.9762237762237762,CovErr(T)
APPENDIX,0.9776223776223776,MeanWidth(T)
APPENDIX,0.9790209790209791,Distribution Shift
APPENDIX,0.9804195804195804,Without shift
APPENDIX,0.9818181818181818,With shift
APPENDIX,0.9832167832167832,Method AgACI DtACI SAOCP
APPENDIX,0.9846153846153847,SFâˆ’OGD
APPENDIX,0.986013986013986,Simulation Study: Distribution Shift
APPENDIX,0.9874125874125874,"Î± = 0.8
Î± = 0.9
Î± = 0.95 âˆ’0.04 âˆ’0.02 0.00 âˆ’0.04 âˆ’0.02 0.00 âˆ’0.04 âˆ’0.02 0.00 1 3 10 30"
APPENDIX,0.9888111888111888,CovErr(T)
APPENDIX,0.9902097902097902,PathLength(T)
APPENDIX,0.9916083916083916,Distribution Shift
APPENDIX,0.993006993006993,Without shift
APPENDIX,0.9944055944055944,With shift
APPENDIX,0.9958041958041958,Method AgACI DtACI SAOCP
APPENDIX,0.9972027972027973,SFâˆ’OGD
APPENDIX,0.9986013986013986,"Figure 15: Mean interval width vs coverage error (top) and Mean Path Length vs. coverage error
(bottom) for the second simulation study. The error bars represent the 10% to 90% quantiles of the
metrics over the simulation datasets."
