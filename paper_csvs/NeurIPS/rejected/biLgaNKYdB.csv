Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022222222222222222,"Most existing classical artificial neural networks (ANN) are designed as a tree
1"
ABSTRACT,0.0044444444444444444,"structure to imitate neural networks. In this paper, we argue that the connectivity
2"
ABSTRACT,0.006666666666666667,"of a tree is not sufficient to characterize a neural network. The nodes of the
3"
ABSTRACT,0.008888888888888889,"same level of a tree cannot be connected with each other, i.e., these neural unit
4"
ABSTRACT,0.011111111111111112,"cannot share information with each other, which is a major drawback of ANN.
5"
ABSTRACT,0.013333333333333334,"Although ANN has been significantly improved in recent years to more complex
6"
ABSTRACT,0.015555555555555555,"structures, such as the directed acyclic graph (DAG), these methods also have
7"
ABSTRACT,0.017777777777777778,"unidirectional and acyclic bias for ANN. In this paper, we propose a method to
8"
ABSTRACT,0.02,"build a bidirectional complete graph for the nodes in the same level of an ANN,
9"
ABSTRACT,0.022222222222222223,"which yokes the nodes of the same level to formulate a neural module. We call our
10"
ABSTRACT,0.024444444444444446,"model as YNN in short. YNN promotes the information transfer significantly which
11"
ABSTRACT,0.02666666666666667,"obviously helps in improving the performance of the method. Our YNN can imitate
12"
ABSTRACT,0.028888888888888888,"neural networks much better compared with the traditional ANN. In this paper, we
13"
ABSTRACT,0.03111111111111111,"analyze the existing structural bias of ANN and propose a model YNN to efficiently
14"
ABSTRACT,0.03333333333333333,"eliminate such structural bias. In our model, nodes also carry out aggregation and
15"
ABSTRACT,0.035555555555555556,"transformation of features, and edges determine the flow of information. We further
16"
ABSTRACT,0.03777777777777778,"impose auxiliary sparsity constraint to the distribution of connectedness, which
17"
ABSTRACT,0.04,"promotes the learned structure to focus on critical connections. Finally, based on
18"
ABSTRACT,0.042222222222222223,"the optimized structure, we also design small neural module structure based on the
19"
ABSTRACT,0.044444444444444446,"minimum cut technique to reduce the computational burden of the YNN model.
20"
ABSTRACT,0.04666666666666667,"This learning process is compatible with the existing networks and different tasks.
21"
ABSTRACT,0.04888888888888889,"The obtained quantitative experimental results reflect that the learned connectivity
22"
ABSTRACT,0.051111111111111114,"is superior to the traditional NN structure.
23"
INTRODUCTION,0.05333333333333334,"1
Introduction
24"
INTRODUCTION,0.05555555555555555,"Deep learning successfully transits the feature engineering from manual to automatic design and
25"
INTRODUCTION,0.057777777777777775,"enables optimization of the mapping function from sample to feature. Consequently, the search
26"
INTRODUCTION,0.06,"for effective neural networks has gradually become an important and practical direction. However,
27"
INTRODUCTION,0.06222222222222222,"designing the architecture remains a challenging task. Certain research studies explore the impact
28"
INTRODUCTION,0.06444444444444444,"of depth [1,2,3] and the type of convolution [4,5] on performance. Moreover, some researchers
29"
INTRODUCTION,0.06666666666666667,"have attempted to simplify the architecture design. VGGNet [6] was directly stacked by a series
30"
INTRODUCTION,0.06888888888888889,"of convolution layers with plain topology. To better adapt the optimization process of gradient
31"
INTRODUCTION,0.07111111111111111,"descent process, GoogleNet [7] introduced parallel modules, while Highway networks [8] employed
32"
INTRODUCTION,0.07333333333333333,"gating units to regulate information flow, resulting in elastic topologies. Driven by the significance
33"
INTRODUCTION,0.07555555555555556,"of depth, the residual block consisted of residual mapping and shortcut was raised in ResNet [9].
34"
INTRODUCTION,0.07777777777777778,"Topological changes in neural networks successfully scaled up neural networks to hundreds of layers.
35"
INTRODUCTION,0.08,"The proposed residual connectivity was widely approved and was subsequently applied in other
36"
INTRODUCTION,0.08222222222222222,"works such as MobileNet [10,11] and ShuffleNet [12]. Divergent from the relative sparse topologies,
37"
INTRODUCTION,0.08444444444444445,"DenseNet [13] wired densely among blocks to fully leverage feature reuse. Recent advances in
38"
INTRODUCTION,0.08666666666666667,"computer vision [25,26] also explored neural architecture search (NAS) methods [14,15,16] to search
39"
INTRODUCTION,0.08888888888888889,"convolutional blocks. In recent years, Yuan proposed a topological perspective using directed acyclic
40"
INTRODUCTION,0.09111111111111111,"graph (DAG) [29] to represent neural networks, enhancing the topological capabilities of artificial
41"
INTRODUCTION,0.09333333333333334,"neural networks (ANNs). However, these approaches suffer from the bias of unidirectional and
42"
INTRODUCTION,0.09555555555555556,"acyclic structures, limiting the signalâ€™s capability for free transmission in the network.
43"
INTRODUCTION,0.09777777777777778,"The existing efforts in neural network connectivity have primarily focused on the tree structures where
44"
INTRODUCTION,0.1,"neural units at the same level cannot exchange information with each other, resulting in significant
45"
INTRODUCTION,0.10222222222222223,"drawbacks for ANNs. This limitation arises due to the absence of a neural module concept. In this
46"
INTRODUCTION,0.10444444444444445,"paper, we argue that the current connectivity approaches fail to adequately capture the essence of
47"
INTRODUCTION,0.10666666666666667,"neural networks. Since the nodes at the same level of a tree cannot establish connections with each
48"
INTRODUCTION,0.10888888888888888,"other, it hampers the transfer of information between these neural units, leading to substantial defects
49"
INTRODUCTION,0.1111111111111111,"for ANNs. We argue that the nodes in the same level should form a neural module and establish
50"
INTRODUCTION,0.11333333333333333,"interconnections. As a result, we introduce a method to build up a bidirectional complete graph
51"
INTRODUCTION,0.11555555555555555,"for nodes at the same level of an ANN. By linking the nodes in a YOKE fashion, we create neural
52"
INTRODUCTION,0.11777777777777777,"modules. Furthermore, when we consider all the nodes at the same level, we would have a chance to
53"
INTRODUCTION,0.12,"construct a bidirectional complete graph in ANNs and yields remarkable improvements. We refer
54"
INTRODUCTION,0.12222222222222222,"to our model as Yoked Neural Network, YNN for brevity. It is important to note that if all the edge
55"
INTRODUCTION,0.12444444444444444,"weights in the bidirectional complete graph become vestigial and approach to zero, our YNN would
56"
INTRODUCTION,0.12666666666666668,"reduce to a traditional tree structure.
57"
INTRODUCTION,0.1288888888888889,"In this paper, we analyze the structural bias of existing ANN structures. To more accurately mimic
58"
INTRODUCTION,0.13111111111111112,"neural networks, our method efficiently eliminates structural bias. In our model, nodes not only
59"
INTRODUCTION,0.13333333333333333,"aggregate and transform features but also determine the information flow. We achieve this by
60"
INTRODUCTION,0.13555555555555557,"assigning learnable parameters to the edges, which reflect the magnitude of connections. This allows
61"
INTRODUCTION,0.13777777777777778,"the learning process to resemble traditional learning methods, enhancing the overall performance of
62"
INTRODUCTION,0.14,"our model in imitating neural networks. As the nodes are relied on the values of other nodes, it is a
63"
INTRODUCTION,0.14222222222222222,"challenging task designing a bidirectional complete graph for nodes at the same level. We address
64"
INTRODUCTION,0.14444444444444443,"this challenge by introducing a synchronization method specifically tailored for learning the nodes at
65"
INTRODUCTION,0.14666666666666667,"the same level. This synchronization method is crucial for ensuring the effective coordination and
66"
INTRODUCTION,0.14888888888888888,"learning of these interconnected nodes.
67"
INTRODUCTION,0.1511111111111111,"Finally, to optimize the structure of YNN, we further attach an auxiliary sparsity constraint that
68"
INTRODUCTION,0.15333333333333332,"influences the distribution of connectedness. This constraint promotes the learned structure to
69"
INTRODUCTION,0.15555555555555556,"prioritize critical connections, enhancing the overall efficiency of the learning process.
70"
INTRODUCTION,0.15777777777777777,"The learning process is compatible with existing networks and exhibits adaptability to larger search
71"
INTRODUCTION,0.16,"spaces and diverse tasks, effectively eliminating the structural bias. We evaluate the effectiveness
72"
INTRODUCTION,0.1622222222222222,"of our optimization method by conducting experiments on classical networks, demonstrating its
73"
INTRODUCTION,0.16444444444444445,"competitiveness compared to existing networks. Additionally, to showcase the benefits of connectivity
74"
INTRODUCTION,0.16666666666666666,"learning, we evaluate our method across various tasks and datasets. The quantitative results from
75"
INTRODUCTION,0.1688888888888889,"these experiments indicate the superiority of the learned connectivity in terms of performance and
76"
INTRODUCTION,0.1711111111111111,"effectiveness.
77"
INTRODUCTION,0.17333333333333334,"Considering that the synchronization algorithm for nodes at the same level may be computationally in-
78"
INTRODUCTION,0.17555555555555555,"tense, we also propose a method to design small neural modules to simplify our model. This approach
79"
INTRODUCTION,0.17777777777777778,"significantly reduces the computational burden of our model while maintaining its effectiveness.
80"
INTRODUCTION,0.18,"To sum up, our contributions in this paper are as follows:
81"
INTRODUCTION,0.18222222222222223,"1. We provide an analysis of the structural bias present in existing ANN networks.
82"
WE PROPOSE THE YNN MODEL WHICH INVOLVES YOKING THE NODES AT THE SAME LEVEL TOGETHER,0.18444444444444444,"2. We propose the YNN model which involves YOKING the nodes at the same level together
83"
WE PROPOSE THE YNN MODEL WHICH INVOLVES YOKING THE NODES AT THE SAME LEVEL TOGETHER,0.18666666666666668,"to simulate real neural networks.
84"
WE DEVELOP A SYNCHRONIZATION METHOD TO EFFECTIVELY LEARN AND COORDINATE THE NODES AT THE,0.18888888888888888,"3. We develop a synchronization method to effectively learn and coordinate the nodes at the
85"
WE DEVELOP A SYNCHRONIZATION METHOD TO EFFECTIVELY LEARN AND COORDINATE THE NODES AT THE,0.19111111111111112,"same level, introducing the concept of neural modules.
86"
WE DESIGN A REGULARIZATION-BASED OPTIMIZATION METHOD TO OPTIMIZE THE STRUCTURE OF THE YNN,0.19333333333333333,"4. We design a regularization-based optimization method to optimize the structure of the YNN
87"
WE DESIGN A REGULARIZATION-BASED OPTIMIZATION METHOD TO OPTIMIZE THE STRUCTURE OF THE YNN,0.19555555555555557,"model.
88"
WE PROPOSE THE DESIGN OF SMALL NEURAL MODULES TO SIGNIFICANTLY REDUCE THE COMPUTATIONAL,0.19777777777777777,"5. We propose the design of small neural modules to significantly reduce the computational
89"
WE PROPOSE THE DESIGN OF SMALL NEURAL MODULES TO SIGNIFICANTLY REDUCE THE COMPUTATIONAL,0.2,"complexity of our model, improving its efficiently.
90"
RELATED WORKS,0.20222222222222222,"2
Related Works
91"
RELATED WORKS,0.20444444444444446,"We firstly review some related works on the design of neural network structures and relevant opti-
92"
RELATED WORKS,0.20666666666666667,"mization methods. The design of neural network has been studied widely. From shallow to deep,
93"
RELATED WORKS,0.2088888888888889,"the shortcut connection plays an important role. Before ResNet, an early practice [17] also added
94"
RELATED WORKS,0.2111111111111111,"linear layers connected from input to output to train multi-layer perceptrons. [7] was composed
95"
RELATED WORKS,0.21333333333333335,"of a shortcut branch and a few deeper branches. The existence of shortcut eases the vanishing or
96"
RELATED WORKS,0.21555555555555556,"exploding gradients [8,9]. Recently, Yuan [29] explained from a topological perspective that shortcuts
97"
RELATED WORKS,0.21777777777777776,"offer dense connections and benefit optimization. Many networks with dense connections exist
98"
RELATED WORKS,0.22,"On macro-structures also. In DenseNet [13], all preceding layers are connected. HRNet [18] was
99"
RELATED WORKS,0.2222222222222222,"benefited from dense high-to-low connections for fine representations. Densely connected networks
100"
RELATED WORKS,0.22444444444444445,"promote the specific task of localization [19]. Differently, our YNN optimizes the desired network
101"
RELATED WORKS,0.22666666666666666,"from a bidirectional complete graph in a differentiable way.
102"
RELATED WORKS,0.2288888888888889,"For the learning process, our method is consistent with DARTS [22], which is differentiable. Different
103"
RELATED WORKS,0.2311111111111111,"from sample-based optimization methods [29], the connectivity is learned simultaneously through the
104"
RELATED WORKS,0.23333333333333334,"weights of the network using our modified version of the gradient descent. A joint training can shift
105"
RELATED WORKS,0.23555555555555555,"the transferring step from one task to another, and obtain task-related YNN. This type was explored
106"
RELATED WORKS,0.23777777777777778,"in [20,21,22,23,24] also, where weight-sharing is utilized across models at the cost of training. At the
107"
RELATED WORKS,0.24,"same time, for our YNN model, we also propose a synchronization method to get the node values in
108"
RELATED WORKS,0.24222222222222223,"the same neural module.
109"
RELATED WORKS,0.24444444444444444,"In order to optimize the learned structure, a sparsity constraint can be observed in other applications,
110"
RELATED WORKS,0.24666666666666667,"e.g., path selection for a multi-branch network [27], pruning unimportant channels for fast inference
111"
RELATED WORKS,0.24888888888888888,"[28], etc. In a recent work, Yuan used L1 regularization to optimize a topological structure. In this
112"
RELATED WORKS,0.2511111111111111,"paper, we also use L1 as well as L2 regularization to search a better structure.
113"
RELATED WORKS,0.25333333333333335,"Secondly, many deep learning works deal with the geometric data in these years[40]. They make
114"
RELATED WORKS,0.25555555555555554,"neural network better cope with structure. Graph neural networks (GNNs) are connectivity-driven
115"
RELATED WORKS,0.2577777777777778,"models, which have been addressing the need of geometric deep learning[30,31]. In fact, a GNN
116"
RELATED WORKS,0.26,"adapts its structure to that of an input graph, and captures complex dependencies of an underlying
117"
RELATED WORKS,0.26222222222222225,"system through an iterative process of aggregation of information. This allows to predict the properties
118"
RELATED WORKS,0.2644444444444444,"of specific nodes, connections, or of the entire graph as a whole, and also to generalize to unseen
119"
RELATED WORKS,0.26666666666666666,"graphs. Due to these powerful features, GNNs have been utilized in many relevant applications to
120"
RELATED WORKS,0.2688888888888889,"accomplish their tasks, such as recommender systems [33], natural language processing [34], traffic
121"
RELATED WORKS,0.27111111111111114,"speed prediction [35], critical data classification [36], computer vision [25,26,37], particle physics
122"
RELATED WORKS,0.2733333333333333,"[38], resource allocation in computer networks [39], and so on.
123"
METHODOLOGY,0.27555555555555555,"3
Methodology
124"
METHODOLOGY,0.2777777777777778,"3.1
Why YNN is Introduced?
125"
METHODOLOGY,0.28,"NN stands for a type of information flow. The traditional structure of ANN is a tree, which is a natural
126"
METHODOLOGY,0.2822222222222222,"way to describe this type of information flow. Then, we can represent the architecture as G = (N, E),
127"
METHODOLOGY,0.28444444444444444,"where N is the set of nodes and E denotes the set of edges. In this tree, each edge eij âˆˆE performs
128"
METHODOLOGY,0.2866666666666667,"a transformation operation parameterized by wij, where ij stands for the topological ordering from
129"
METHODOLOGY,0.28888888888888886,"the node ni to node nj with ni, nj âˆˆN. In fact, the importance of the connection is determined
130"
METHODOLOGY,0.2911111111111111,"by the weight of eij. The tree structure as a natural way to represent such formation flow is most
131"
METHODOLOGY,0.29333333333333333,"frequently used in ANN.
132"
METHODOLOGY,0.29555555555555557,"A tree is a hierarchical nested structure where a node can be influenced only by its precursor node,
133"
METHODOLOGY,0.29777777777777775,"thereby causing transformation of information between them. In a tree structure, the root node has no
134"
METHODOLOGY,0.3,"precursor node, while each other node has one and only one precursor node. The leaf node has no
135"
METHODOLOGY,0.3022222222222222,"subsequent nodes. The number of subsequent nodes of each other node can be one or multiple. In
136"
METHODOLOGY,0.30444444444444446,"addition, the tree structure in mathematical statistics can represent some hierarchical relationships. A
137"
METHODOLOGY,0.30666666666666664,"tree structure has many applications. It can also indicate subordinating relationships.
138"
METHODOLOGY,0.3088888888888889,"In recent years, some researchers attempted to generalize this structure. In those works, except the
139"
METHODOLOGY,0.3111111111111111,"root node, all other nodes are made to have multiple precursor nodes, i.e., the hierarchical information
140"
METHODOLOGY,0.31333333333333335,"flow is made to form a directed acyclic graph (DAG).
141"
METHODOLOGY,0.31555555555555553,"However, a tree or a DAG is a hierarchical nested structure where a node can be influenced only by
142"
METHODOLOGY,0.31777777777777777,"its precursor node, which makes the transformation of information quite inadequate. Moreover, we
143"
METHODOLOGY,0.32,"find that this structure is far more inferior in its strength compared with those of real neural networks,
144"
METHODOLOGY,0.32222222222222224,"which connect far more complex structures than a tree or DAG structure as shown in Fig 1. In fact,
145"
METHODOLOGY,0.3244444444444444,"a tree or a DAG structure is used just because its good mathematical properties which can apply
146"
METHODOLOGY,0.32666666666666666,"backward propagation conveniently.
147"
METHODOLOGY,0.3288888888888889,"Figure 1: Artificial Neural Network
Figure 2: Real Neural Network"
METHODOLOGY,0.33111111111111113,"In this paper, we represent the neural network as a bidirectional complete graph for the nodes of
148"
METHODOLOGY,0.3333333333333333,"the same level to make the description of NN is much better compared with the traditional ANN.
149"
METHODOLOGY,0.33555555555555555,"Further, the connections between nodes are represented as directed edges, which determine the flow
150"
METHODOLOGY,0.3377777777777778,"of information between the connected nodes. We consider that any two nodes ni and nj of the
151"
METHODOLOGY,0.34,"same level construct an information clique if there exists a path between them. Compared with the
152"
METHODOLOGY,0.3422222222222222,"traditional tree structure, we yoke the nodes of the same level to form a bidirectional complete graph.
153"
METHODOLOGY,0.34444444444444444,"We call this structure as YNN, which will be introduced in the next section.
154"
STRUCTURE OF YNN,0.3466666666666667,"3.2
Structure of YNN
155"
STRUCTURE OF YNN,0.3488888888888889,"Inspired by the neural network of human beings as shown in the Fig 2. In order to enhance the ability
156"
STRUCTURE OF YNN,0.3511111111111111,"of NN to express information, we design cliques for the nodes of each level of a neural network.
157"
STRUCTURE OF YNN,0.35333333333333333,"Definition 1 A clique is a bidirectional complete graph which considers that for any two nodes ni
158"
STRUCTURE OF YNN,0.35555555555555557,"and nj, an edge exists from ni to nj.
159"
STRUCTURE OF YNN,0.35777777777777775,"According to this definition, the model in our framework is considered as a bidirectional complete
160"
STRUCTURE OF YNN,0.36,"graph for the nodes of the same level. These nodes construct a clique, where every node is not only
161"
STRUCTURE OF YNN,0.3622222222222222,"influenced by its precursor nodes but also by all other nodes of its level. The cliques are represented
162"
STRUCTURE OF YNN,0.36444444444444446,"as information modules which greatly enhance the characterization of NN.
163"
STRUCTURE OF YNN,0.36666666666666664,"According to the definition of clique, a neural network can also be represented as a list of cliques.
164"
STRUCTURE OF YNN,0.3688888888888889,"Further, we can also introduce a concept of neural module.
165"
STRUCTURE OF YNN,0.3711111111111111,"Definition 2 A neural module is a collection of nodes that interact with each other.
166"
STRUCTURE OF YNN,0.37333333333333335,"According to the definition, a neural module can be part of clique. In fact, if all the weights in a
167"
STRUCTURE OF YNN,0.37555555555555553,"clique becomes zero, then the YNN model is reduced to the traditional tree structure.
168"
STRUCTURE OF YNN,0.37777777777777777,"In each clique of our model, the nodes are first calculated by using their precursor nodes, which only
169"
STRUCTURE OF YNN,0.38,"distribute features. The last one is the output level, which only generates final output of the graph.
170"
STRUCTURE OF YNN,0.38222222222222224,"Secondly, each node is also indicated by the nodes of the same level and their values are influenced
171"
STRUCTURE OF YNN,0.3844444444444444,"by each other.
172"
STRUCTURE OF YNN,0.38666666666666666,"During the traditional forward computation, each node aggregates inputs from connected preorder
173"
STRUCTURE OF YNN,0.3888888888888889,"nodes. We divide such nodes into two parts. The first part contains the precursor nodes of the last
174"
STRUCTURE OF YNN,0.39111111111111113,"level, and the second part contains the nodes of the corresponding clique of the same level. Then,
175"
STRUCTURE OF YNN,0.3933333333333333,"features are transformed to get an output tensor, which is sent to the nodes in the next level through
176"
STRUCTURE OF YNN,0.39555555555555555,"the output edges. Its specific calculation method will be introduced in the next section.
177"
STRUCTURE OF YNN,0.3977777777777778,"In summary, according to the above definitions, each YNN is constructed as follow. Its order of
178"
STRUCTURE OF YNN,0.4,"outputs is represented as G = {N, E}. For the nodes in the same level, bidirectional complete graphs
179"
STRUCTURE OF YNN,0.4022222222222222,"are built as clique C. Each node n in C is first calculated by using the precursor nodes without the
180"
STRUCTURE OF YNN,0.40444444444444444,"nodes in the clique, which is called as the meta value Ë†n of the node. Then, we calculate its real value
181"
STRUCTURE OF YNN,0.4066666666666667,"n by using the nodes of the clique.
182"
STRUCTURE OF YNN,0.4088888888888889,"According to the meta value and the real value as introduced before, the structure of YNN is shown
183"
STRUCTURE OF YNN,0.4111111111111111,"in the Fig 3.
184"
STRUCTURE OF YNN,0.41333333333333333,"Figure 3: The first picture shows the tree structure of traditional ANN. The second picture shows our
YNN model that yokes together the nodes of the first level. For the clique of the first level, the node
spin part is based on its meta value, which also represents the connection with the pre nodes. As a
result, we can decompose the spin node as shown in the third picture, which is to represent the meta
value. The fourth and fifth pictures show the second level of our YNN model, which are the same as
the second and third pictures, respectively."
STRUCTURE OF YNN,0.41555555555555557,"In the next section, we will explain how to calculate the values of the nodes by using the precursor
185"
STRUCTURE OF YNN,0.4177777777777778,"node as well as the nodes in the clique.
186"
FORWARD PROCESS,0.42,"3.3
Forward Process
187"
FORWARD PROCESS,0.4222222222222222,"Let we have n elements:
188"
FORWARD PROCESS,0.42444444444444446,"X = {x1, x2, ..., xn}
(1)"
FORWARD PROCESS,0.4266666666666667,"as the input data to feed for the first level of ANN. Then, the meta value b
N 1 of the first level can be
189"
FORWARD PROCESS,0.4288888888888889,"calculated as:
190"
FORWARD PROCESS,0.4311111111111111,"b
N 1 = X âˆ—W 01,
(2)
where W01 is the fully connected weight of the edges between level 1 and input nodes. Then,
191"
FORWARD PROCESS,0.43333333333333335,"similarity in nature, for meta value, the full connection between the levels makes the information to
192"
FORWARD PROCESS,0.43555555555555553,"flow as:
193"
FORWARD PROCESS,0.43777777777777777,"b
N i = f(N iâˆ’1) âˆ—W (iâˆ’1)i,
(3)
where N iâˆ’1 = {1, niâˆ’1
1
, niâˆ’1
2
, ...}, niâˆ’1
j
is the real value of the jth node in the (i âˆ’1)th level,
194"
FORWARD PROCESS,0.44,"number 1 indicates for the bias of the value between the (i âˆ’1)th and ith levels as well as the
195"
FORWARD PROCESS,0.44222222222222224,"activation function f.
196"
FORWARD PROCESS,0.4444444444444444,"Then, by introducing weight W i in the ith level and considering the bidirectional complete graph of
197"
FORWARD PROCESS,0.44666666666666666,"that level as a clique, we propose a method to calculate the real value N i based on the meta value b
N i
198"
FORWARD PROCESS,0.4488888888888889,"as introduced in the previous section. Suppose, there are m nodes in the clique and they rely on the
199"
FORWARD PROCESS,0.45111111111111113,"values of other nodes. Hence, we need a synchronization method to solve the problem. Here, we take
200"
FORWARD PROCESS,0.4533333333333333,"the problem as a system of multivariate equations as well as an activation function f. Then, for the
201"
FORWARD PROCESS,0.45555555555555555,"real value of ni
j in N i based on the meta value bni
j in b
N i, the equations can be summarized as follow:
202"
FORWARD PROCESS,0.4577777777777778,"ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²"
FORWARD PROCESS,0.46,"ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³"
FORWARD PROCESS,0.4622222222222222,"wi
01 + P"
FORWARD PROCESS,0.46444444444444444,"jÌ¸=1
f(ni
j) âˆ—wi
j1 + f(bni
1) âˆ—wi
11 = ni
1"
FORWARD PROCESS,0.4666666666666667,"wi
02 + P"
FORWARD PROCESS,0.4688888888888889,"jÌ¸=2
f(ni
j) âˆ—wi
j2 + f(bni
2) âˆ—wi
22 = ni
2"
FORWARD PROCESS,0.4711111111111111,"...
wi
0m + P"
FORWARD PROCESS,0.47333333333333333,"jÌ¸=m
f(ni
j) âˆ—wi
jm + f(bni
m) âˆ—wi
mm = ni
m"
FORWARD PROCESS,0.47555555555555556,"In the above equations, wi
01, wi
02, ..., wi
0m are the bias of the real values of the nodes in the ith level.
203"
FORWARD PROCESS,0.4777777777777778,"Note that, for the mata value, the bias is a value between the levels; while for a real value, the bias is
204"
FORWARD PROCESS,0.48,"a value in the individual level only.
205"
FORWARD PROCESS,0.4822222222222222,"Existing numerical methods would be able to solve the above equations efficiently. In the real
206"
FORWARD PROCESS,0.48444444444444446,"applications, the efficiency can also be well optimized. In fact, for too large equations, we also
207"
FORWARD PROCESS,0.4866666666666667,"propose a method to reduce the calculation scale efficiently. This method is introduced in the
208"
FORWARD PROCESS,0.4888888888888889,"following section.
209"
BACKWARD PROCESS,0.4911111111111111,"3.4
Backward Process
210"
BACKWARD PROCESS,0.49333333333333335,"In this section, we introduce the backward process of our model. Firstly, let the gradient of the output
211"
BACKWARD PROCESS,0.4955555555555556,"be the gradient of the meta value of the last level. We calculate the node gradient for the ith level as:
212"
BACKWARD PROCESS,0.49777777777777776,"d(N i) = d( b
N i+1) âˆ—W i(i+1)T âˆ—f âˆ’1(N i) .
(4)"
BACKWARD PROCESS,0.5,"The meta value of b
N i is calculated by using the real value of N iâˆ’1 according to the system of
213"
BACKWARD PROCESS,0.5022222222222222,"equations.
214"
BACKWARD PROCESS,0.5044444444444445,"Then, to get the value of d( b
N iâˆ’1), we need to consider the nodes as the variables in the system of
215"
BACKWARD PROCESS,0.5066666666666667,"equations. For convenient, we introduce operator Ci to represent the derivatives for the ith level,
216"
BACKWARD PROCESS,0.5088888888888888,"which can be expressed as:
217"
BACKWARD PROCESS,0.5111111111111111,"Ci = W i âˆ’diag(W i) + eye(W i) ,
(5)
where W i is the adjacency matrix of the clique in the ith level, diag(W i) is the diagonal matrix of
218"
BACKWARD PROCESS,0.5133333333333333,"W i, eye(W i) is the identity matrix whose size is the same as that of wi, and operator Ci represents
219"
BACKWARD PROCESS,0.5155555555555555,"the transfer of other nodes for each node in the clique according to the system of equations. In the
220"
BACKWARD PROCESS,0.5177777777777778,"clique, the identity matrix is for the node itself.
221"
BACKWARD PROCESS,0.52,"According to the system of equations, the meta value of a node is connected to its real value through
222"
BACKWARD PROCESS,0.5222222222222223,"the diagonal matrix of W i. Note that each node is calculated by using the activation function f. As a
223"
BACKWARD PROCESS,0.5244444444444445,"result, after the transfer through the bidirectional complete graph, the gradient of the meta value of
224"
BACKWARD PROCESS,0.5266666666666666,"the nodes becomes:
225"
BACKWARD PROCESS,0.5288888888888889,"d( b
N i) = d(N i) âˆ—CiT âˆ—f âˆ’1(N i) âˆ—diag(W i) âˆ—f âˆ’1( b
N i) .
(6)"
BACKWARD PROCESS,0.5311111111111111,"Now, we have got the gradient of the meta value as well as that of the real value of each node. Finally,
226"
BACKWARD PROCESS,0.5333333333333333,"the gradient weight of the fully connected level W i(i+1) between the ith and (i + 1)th level can be
227"
BACKWARD PROCESS,0.5355555555555556,"expressed as:
228"
BACKWARD PROCESS,0.5377777777777778,"d(W i(i+1))T = d( b
N i+1)T âˆ—f(N i) .
(7)
Now, we need to calculate the gradient of W i for the clique in the ith level. According to the system
229"
BACKWARD PROCESS,0.54,"of equations, we need to consider the weights of all the connected nodes. For any jth node in the
230"
BACKWARD PROCESS,0.5422222222222223,"clique, its connected weight is the jth column of the matrix. Similarly, for convenient, we introduce
231"
BACKWARD PROCESS,0.5444444444444444,"the following operator:
232"
BACKWARD PROCESS,0.5466666666666666,"Di
j = (ni
1, ..., bni
j, ..., ni
m) ,
(8)
which can be found in the system of equations. Then, by the gradient of real value of the jth node ni
j
233"
BACKWARD PROCESS,0.5488888888888889,"in N i, the following becomes the corresponding gradient of the clique:
234"
BACKWARD PROCESS,0.5511111111111111,"d(W i(:, j))T = d(ni
j) âˆ—f(Diâ€²
j ) .
(9)"
YNN STRUCTURE OPTIMIZATION,0.5533333333333333,"3.5
YNN Structure Optimization
235"
YNN STRUCTURE OPTIMIZATION,0.5555555555555556,"Consider that for the nodes in the same level, we construct a clique as stated before. Here, we consider
236"
YNN STRUCTURE OPTIMIZATION,0.5577777777777778,"a clique just as a universal set for all the possible connections. In our work, we can optimize the
237"
YNN STRUCTURE OPTIMIZATION,0.56,"YNN structure to let our model to focus on important connections only. The optimization process can
238"
YNN STRUCTURE OPTIMIZATION,0.5622222222222222,"be L1 or L2 regularization as usual, which can be parameterized L1 and L2, respectively.
239"
YNN STRUCTURE OPTIMIZATION,0.5644444444444444,"For the jth node in the ith level, the process can be formulated as follow:
240"
YNN STRUCTURE OPTIMIZATION,0.5666666666666667,"opt_ni
j = ni
j + L1 âˆ—
X"
YNN STRUCTURE OPTIMIZATION,0.5688888888888889,"k
abs(wi(k, j)) + L2 âˆ—
X"
YNN STRUCTURE OPTIMIZATION,0.5711111111111111,"k
(wi(k, j))2
(10)"
YNN STRUCTURE OPTIMIZATION,0.5733333333333334,"According to the L1 and L2 regularization, the L1 parameter can make our YNN to focus on important
241"
YNN STRUCTURE OPTIMIZATION,0.5755555555555556,"connections in the clique, and the L2 regularization makes the weight in the clique to be low to make
242"
YNN STRUCTURE OPTIMIZATION,0.5777777777777777,"our model to have better generation.
243"
STRUCTURE OF NEURAL MODULE,0.58,"3.6
Structure of Neural Module
244"
STRUCTURE OF NEURAL MODULE,0.5822222222222222,"According to the forward process of YNN as stated earlier, it solves a system of equations. A large
245"
STRUCTURE OF NEURAL MODULE,0.5844444444444444,"number of nodes in the same level would bring too much computational burden to solve a large system
246"
STRUCTURE OF NEURAL MODULE,0.5866666666666667,"of equations. In Fact, we can optimize the graph of any level by L1 and L2 regularization, and then
247"
STRUCTURE OF NEURAL MODULE,0.5888888888888889,"turn to a minimum cut technology, e.g., the NE algorithm, to reduce the computation significantly.
248"
STRUCTURE OF NEURAL MODULE,0.5911111111111111,"For each cut subgraph, we design a neural module structure according to definition 2 to simplify
249"
STRUCTURE OF NEURAL MODULE,0.5933333333333334,"the system of equations as shown in Fig. 4. Since the nodes are influenced only by the nodes in the
250"
STRUCTURE OF NEURAL MODULE,0.5955555555555555,"subgraph, the system of equations can be reduced to the number of the nodes in the cut subgraph,
251"
STRUCTURE OF NEURAL MODULE,0.5977777777777777,"which is formulated as a neural module as definition 2 in this paper.
252"
STRUCTURE OF NEURAL MODULE,0.6,"Figure 4: If the clique is too large, we would
have too much computational burden to solve
the system of equations. Then, we can first opti-
mize the structure and learn the importance of the
connection, followed by the application of the
minimum cut method to formulate the structure
of the neural module. In this way, the calcula-
tion for the system of equations can be limited
to each subgraph."
STRUCTURE OF NEURAL MODULE,0.6022222222222222,"In summary, the structure of the neural module can be constructed as follows:
253"
STRUCTURE OF NEURAL MODULE,0.6044444444444445,"1. Construct the clique for the nodes in the same level;
254"
STRUCTURE OF NEURAL MODULE,0.6066666666666667,"2. Optimize the clique by using the L1 and L2 regularization;
255"
STRUCTURE OF NEURAL MODULE,0.6088888888888889,"3. Cut the optimized graph using the NE algorithm;
256"
STRUCTURE OF NEURAL MODULE,0.6111111111111112,"4. Construct system of equations by taking each cut subgraph as a neural module.
257"
STRUCTURE OF NEURAL MODULE,0.6133333333333333,"As explained before, in this way the system of equations can be reduced to Ns-ary equations, where
258"
STRUCTURE OF NEURAL MODULE,0.6155555555555555,"Ns is the number of nodes in each neural module. Of course, if the calculation of our model can be
259"
STRUCTURE OF NEURAL MODULE,0.6177777777777778,"accept for our model, take the clique itself as Neural Module is most accurate, since clique considers
260"
STRUCTURE OF NEURAL MODULE,0.62,"all connection in the level.
261"
EXPERIMENTS,0.6222222222222222,"4
Experiments
262"
OPTIMIZATION OF CLASSICAL ANN,0.6244444444444445,"4.1
Optimization of Classical ANN
263"
OPTIMIZATION OF CLASSICAL ANN,0.6266666666666667,"In this section, we will show the experiments with our method. Here, we compare our method with
264"
OPTIMIZATION OF CLASSICAL ANN,0.6288888888888889,"the traditional NN method, stacked auto encoder(SAE), as well as the generalized traditional NN
265"
OPTIMIZATION OF CLASSICAL ANN,0.6311111111111111,"which is a topological perspective to take NN as a DAG graph proposed in recent years.
266"
OPTIMIZATION OF CLASSICAL ANN,0.6333333333333333,"We show our results for three real data sets. The first dataset contains the codon usage frequencies in
267"
OPTIMIZATION OF CLASSICAL ANN,0.6355555555555555,"the genomic coding DNA of a large sample of diverse organisms obtained from different taxa tabulated
268"
OPTIMIZATION OF CLASSICAL ANN,0.6377777777777778,"in the CUTG database. Here, we further manually curated and harmonized the existing entries by
269"
OPTIMIZATION OF CLASSICAL ANN,0.64,"re-classifying the bacteria (bct) class of CUTG into archaea (arc), plasmids (plm), and bacteria
270"
OPTIMIZATION OF CLASSICAL ANN,0.6422222222222222,Table 1: Codon Dataset
OPTIMIZATION OF CLASSICAL ANN,0.6444444444444445,"Models
Codon Data
35 Nodes
38 Nodes
40 Nodes
45 Nodes
48 Nodes
50 Nodes"
OPTIMIZATION OF CLASSICAL ANN,0.6466666666666666,"NN
0.248Â±0.0054
0.3098Â±0.0485
0.2815Â±0.0037
0.2664Â±0.0004
0.2837Â±0.0168
0.3955Â±0.0011
SAE
0.3446Â±0.0152
0.3282Â±0.0097
0.3588Â±0.0184
0.3294Â±0.0289
0.3055Â±0.215
0.3505Â±0.0226
DAG
0.2719Â±0.0223
0.2789Â±0.0402
0.2413Â±0.0019
0.2656Â±0.0066
0.2265Â±0.0285
0.2496Â±0.0078
YNN
0.2167Â±0.0054
0.2496Â±0.0227
0.1835Â±0.0027
0.1941Â±0.0093
0.1870Â±0.0047
0.2034Â±0.0219
YNN&L1
0.1999Â±0.0066
0.2117Â±0.0043
0.1706Â±0.0039
0.1846Â±0.0062
0.2132Â±0.0019
0.1839Â±0.0141
YNN&L2
0.2007Â±0.0137
0.212Â±0.0187
0.1816Â±0.0046
0.2085Â±0.009
0.1831Â±0.0164
0.2003Â±0.0305"
OPTIMIZATION OF CLASSICAL ANN,0.6488888888888888,Table 2: Optical Recognition of Handwritten Digits
OPTIMIZATION OF CLASSICAL ANN,0.6511111111111111,"Models
Crowdsourced Data
35 Nodes
38 Nodes
40 Nodes
45 Nodes
48 Nodes
50 Nodes"
OPTIMIZATION OF CLASSICAL ANN,0.6533333333333333,"NN
0.2565Â±0.069
0.345Â±0.0011
0.2181Â±0.445
0.1536Â±0.0323
0.3159Â±0.0464
0.259Â±0.0937
SAE
0.2871Â±0.04
0.2952Â±0.0209
0.3603Â±0.0086
0.4186Â±0.0419
0.3656Â±0.0228
0.3375Â±0.0376
DAG
0.2446Â±0.0409
0.2095Â±0.0014
0.2721Â±0.534
0.3475Â±0.0208
0.1981Â±0.0145
0.2585Â±0.0654
YNN
0.1433Â±0.0159
0.1274Â±0.015
0.1725Â±0.0451
0.1552Â±0.0077
0.1791Â±0.0005
0.256Â±0.0001
YNN&L1
0.1633Â±0.0153
0.1522pm0.0031
0.18Â±0.0247
0.1594pm0.0225
0.143Â±0.0005
0.1494Â±0.032
YNN&L2
0.1586Â±0.015
0.1867Â±0.186
0.1614Â±0.0189
0.1483Â±0.142
0.2028Â±0.0147
0.1881Â±0.0001"
OPTIMIZATION OF CLASSICAL ANN,0.6555555555555556,"proper (keeping with the original label â€™bctâ€™). The second dataset contains optically recognized
271"
OPTIMIZATION OF CLASSICAL ANN,0.6577777777777778,"handwritten digits made available by NIST using preprocessing programs to extract normalized
272"
OPTIMIZATION OF CLASSICAL ANN,0.66,"bitmaps of handwritten digits from a preprinted form. Out of a total of 43 people. The third dataset is
273"
OPTIMIZATION OF CLASSICAL ANN,0.6622222222222223,"Connect-4 that contains all the legal 8-ply positions used in the game of connect-4, in which neither
274"
OPTIMIZATION OF CLASSICAL ANN,0.6644444444444444,"player has won yet, and the next move is not forced. The outcome class is the theoretical value of the
275"
OPTIMIZATION OF CLASSICAL ANN,0.6666666666666666,"first player in the game.
276"
OPTIMIZATION OF CLASSICAL ANN,0.6688888888888889,"Here, we compared our method with other methods in terms of a variety of nodes. In this way, we can
277"
OPTIMIZATION OF CLASSICAL ANN,0.6711111111111111,"examine the effectiveness of our model at different levels of complexity of the traditional structure.
278"
OPTIMIZATION OF CLASSICAL ANN,0.6733333333333333,"These nodes are constructed by the NN, SAE, and DAG models. We compared these models in terms
279"
OPTIMIZATION OF CLASSICAL ANN,0.6755555555555556,"of the percentage error. The obtained results are organized in the following Tables, where we can see
280"
OPTIMIZATION OF CLASSICAL ANN,0.6777777777777778,"that our YNN model achieves much better results in most of the cases.
281"
OPTIMIZATION OF CLASSICAL ANN,0.68,"In fact, for all the data sets and a variety of nodes in the same level, our YNN model could tend to
282"
OPTIMIZATION OF CLASSICAL ANN,0.6822222222222222,"get better results after the nodes are yoked together. The effect of our YNN could be improved by
283"
OPTIMIZATION OF CLASSICAL ANN,0.6844444444444444,"optimizing the structure as explained before. All of the first four lines of the Tables are for the results
284"
OPTIMIZATION OF CLASSICAL ANN,0.6866666666666666,"that do not be optimized by the L1 or L2 regularization. We can see that our YNN structure is more
285"
OPTIMIZATION OF CLASSICAL ANN,0.6888888888888889,"efficient even without regularization, compared with the traditional structure.
286"
OPTIMIZATION OF STRUCTURE,0.6911111111111111,"4.2
Optimization of Structure
287"
OPTIMIZATION OF STRUCTURE,0.6933333333333334,"In this section, we optimize the structure of our model. Since every structure is a subgraph of a fully
288"
OPTIMIZATION OF STRUCTURE,0.6955555555555556,"connected graph, the initial clique can be a search space for our model. Our model is optimized by
289"
OPTIMIZATION OF STRUCTURE,0.6977777777777778,"using the L1 and L2 regularization, which are effective tools for optimizing structures. The obtained
290"
OPTIMIZATION OF STRUCTURE,0.7,"results show that such optimizations can yield better effect.
291"
OPTIMIZATION OF STRUCTURE,0.7022222222222222,"Here, we study the structure of the model for different L1 and L2 parameters, as shown in Fig. 5.
292"
OPTIMIZATION OF STRUCTURE,0.7044444444444444,"In the figure, the green line represents the results of YNN without optimization, while the blue and
293"
OPTIMIZATION OF STRUCTURE,0.7066666666666667,"red lines are the results for a variety of L1 and L2 parameters, respectively. We can see that such
294"
OPTIMIZATION OF STRUCTURE,0.7088888888888889,"optimization is effective for our YNN in most cases.
295"
OPTIMIZATION OF STRUCTURE,0.7111111111111111,Table 3: Connect-4 Dataset
OPTIMIZATION OF STRUCTURE,0.7133333333333334,"Models
connect-4 Data
35 Nodes
38 Nodes
40 Nodes
45 Nodes
48 Nodes
50 Nodes"
OPTIMIZATION OF STRUCTURE,0.7155555555555555,"NN
0.2789Â±0.0075
0.2726Â±0.0099
0.285Â±0.012
0.2875Â±0.0134
0.2923Â±0.0145
0.3073Â±0.0259
SAE
0.3912Â±0.0416
0.3325Â±0.0104
0.331Â±0.0044
0.3346Â±0.0096
0.3175Â±0.0082
0.3366Â±0.0099
DAG
3519Â±0.05
0.2762Â±0.0038
0.2828Â±0.0053
0.2989Â±0.0081
0.3032Â±0.0009
0.3134Â±0.0382
YNN
0.2751Â±0.0174
0.265Â±0.0182
0.2489Â±0.0004
0.2582Â±0.0045
0.2569Â±0.0065
0.2475Â±0.0068
YNN&L1
0.2758Â±0.026
0.2544Â±0.0046
0.2513Â±0.0017
0.2635Â±0.0029
0.2574Â±0.006
0.2625Â±0.0093
YNN&L2
0.2826Â±0.0366
0.2577Â±0.0035
0.2495Â±0.002
0.262Â±0.0081
0.2549Â±0.0067
0.2485Â±0.0122"
OPTIMIZATION OF STRUCTURE,0.7177777777777777,"We also show the pixel map of the matrix for the clique. In the figure, the black-and-white graph
296"
OPTIMIZATION OF STRUCTURE,0.72,"represents the matrix of the fully connected graph for the nodes in the same level. The more black of
297"
OPTIMIZATION OF STRUCTURE,0.7222222222222222,"the pixel means a lower weight for the corresponding edge.
298"
OPTIMIZATION OF STRUCTURE,0.7244444444444444,"According with the decline of the error, we can always seek a better structure compared with the
299"
OPTIMIZATION OF STRUCTURE,0.7266666666666667,"bidirectional complete graph used in our YNN. Besides the L1 regularization, the L2 regularization is
300"
OPTIMIZATION OF STRUCTURE,0.7288888888888889,"also an effective tool to optimize the structure of our model. A larger L2 regularization lowers the
301"
OPTIMIZATION OF STRUCTURE,0.7311111111111112,"weights of all the edges, thus yields more black pixels. However, from the decline of error, we can
302"
OPTIMIZATION OF STRUCTURE,0.7333333333333333,"find that the L2 regularization is also effective to optimize our YNN structure.
303"
OPTIMIZATION OF STRUCTURE,0.7355555555555555,"1
2
3
4
5
6
7
parameter *e-5 0.15 0.2 0.25 0.3 0.35 0.4 error"
OPTIMIZATION OF STRUCTURE,0.7377777777777778,"YNN
L1 regular
L2 regular"
OPTIMIZATION OF STRUCTURE,0.74,"1
2
3
4
5
6
7
parameter *e-5 0.1 0.12 0.14 0.16 0.18 error"
OPTIMIZATION OF STRUCTURE,0.7422222222222222,"YNN
L1 regular
L2 regular"
OPTIMIZATION OF STRUCTURE,0.7444444444444445,"1
2
3
4
5
6
7
parameter e-5 0.255 0.26 0.265 0.27 0.275 0.28 0.285 error"
OPTIMIZATION OF STRUCTURE,0.7466666666666667,"YNN
L1 regular
L2 regular"
OPTIMIZATION OF STRUCTURE,0.7488888888888889,"Figure 5: Regularization of results based on L1 and L2 for Codon dataset, optically recognized
handwritten digits and connect-4 dataset."
OPTIMIZATION OF STRUCTURE,0.7511111111111111,(a) L1 regularization
OPTIMIZATION OF STRUCTURE,0.7533333333333333,(b) L2 regularization
OPTIMIZATION OF STRUCTURE,0.7555555555555555,"Figure 6: Best pixel map of the clique based on L1 and L2 regularization for codon dataset, optically
recognized handwritten digits and connect-4 dataset."
CONCLUSION,0.7577777777777778,"5
Conclusion
304"
CONCLUSION,0.76,"In this paper, we propose a YNN structure to build a bidirectional complete graph for the nodes in
305"
CONCLUSION,0.7622222222222222,"the same level of ANN, so as to improve the effect of ANN by promoting the significant transfer
306"
CONCLUSION,0.7644444444444445,"of information. In our work, we analyse the structure bias. Our method eliminates structure bias
307"
CONCLUSION,0.7666666666666667,"efficiently. By assigning learnable parameters to the edges, which reflect the magnitude of connections,
308"
CONCLUSION,0.7688888888888888,"the learning process can be performed in a differentiable manner. For our model, we propose a
309"
CONCLUSION,0.7711111111111111,"synchronization method to simultaneously calculate the values of the nodes in the same level. We
310"
CONCLUSION,0.7733333333333333,"further impose an auxiliary sparsity constraint to the distribution of connectedness by L1 and L2
311"
CONCLUSION,0.7755555555555556,"regularization, which promotes the learned structure to focus on critical connections. We also propose
312"
CONCLUSION,0.7777777777777778,"a small neural module structure that would efficiently reduce the computational burden of our model.
313"
CONCLUSION,0.78,"The obtained quantitative experimental results demonstrate that the learned YNN structure is superior
314"
CONCLUSION,0.7822222222222223,"to the traditional structures.
315"
REFERENCES,0.7844444444444445,"References
316"
REFERENCES,0.7866666666666666,"[1] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks.
317"
REFERENCES,0.7888888888888889,"In: Advances in neural information processing systems. pp. 1097â€“1105 (2012)
318"
REFERENCES,0.7911111111111111,"[2] Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectifier neural networks. In:Proceedings of the fourteenth
319"
REFERENCES,0.7933333333333333,"international conference on artificial intelligence and statistics. pp. 315â€“33 (2011)
320"
REFERENCES,0.7955555555555556,"[3] Perez-Rua, J.M., Baccouche, M., Pateux, S.: Efficient progressive neural architecture search. arXiv preprint
321"
REFERENCES,0.7977777777777778,"arXiv:1808.00391 (2018)
322"
REFERENCES,0.8,"[4] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks.
323"
REFERENCES,0.8022222222222222,"In: Proceedings of the IEEE international conference on computer vision. pp. 764â€“773 (2017)
324"
REFERENCES,0.8044444444444444,"[5] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M.,
325"
REFERENCES,0.8066666666666666,"Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv
326"
REFERENCES,0.8088888888888889,"preprint arXiv:1704.04861 (2017)
327"
REFERENCES,0.8111111111111111,"[6] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition.
328"
REFERENCES,0.8133333333333334,"arXiv preprint arXiv:1409.1556 (2014)
329"
REFERENCES,0.8155555555555556,"[7] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,
330"
REFERENCES,0.8177777777777778,"V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings of the IEEE conference on
331"
REFERENCES,0.82,"computer vision and pattern recognition. pp. 1â€“9 (2015)
332"
REFERENCES,0.8222222222222222,"[8] Srivastava, R.K., Greff, K., Schmidhuber, J.: Highway networks. arXiv preprint arXiv:1505.00387
333"
REFERENCES,0.8244444444444444,"(2015)
334"
REFERENCES,0.8266666666666667,"[9] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings
335"
REFERENCES,0.8288888888888889,"of the IEEE conference on computer vision and pattern recognition. pp. 770â€“778 (2016)
336"
REFERENCES,0.8311111111111111,"[10] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals
337"
REFERENCES,0.8333333333333334,"and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern
338"
REFERENCES,0.8355555555555556,"recognition. pp. 4510â€“4520 (2018)
339"
REFERENCES,0.8377777777777777,"[11] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R.,
340"
REFERENCES,0.84,"Vasudevan, V., et al.: Searching for mobilenetv3. arXiv preprint arXiv:1905.02244 (2019)
341"
REFERENCES,0.8422222222222222,"[12] Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolutional neural
342"
REFERENCES,0.8444444444444444,"network for mobile devices. In: Proceedings of the IEEE conference on computer vision and pattern
343"
REFERENCES,0.8466666666666667,"recognition. pp. 6848â€“6856 (2018)
344"
REFERENCES,0.8488888888888889,"[13] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional
345"
REFERENCES,0.8511111111111112,"networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp.
346"
REFERENCES,0.8533333333333334,"4700â€“4708 (2017)
347"
REFERENCES,0.8555555555555555,"[14] Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures for scalable im-
348"
REFERENCES,0.8577777777777778,"age recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition.
349"
REFERENCES,0.86,"pp. 8697â€“8710 (2018)
350"
REFERENCES,0.8622222222222222,"[15] Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXiv preprint
351"
REFERENCES,0.8644444444444445,"arXiv:1806.09055 (2018)
352"
REFERENCES,0.8666666666666667,"[16] Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.: Mnasnet:
353"
REFERENCES,0.8688888888888889,"Platform-aware neural architecture search for mobile. In: Proceedings of the IEEE Conference on
354"
REFERENCES,0.8711111111111111,"Computer Vision and Pattern Recognition. pp. 2820â€“2828 (2019)
355"
REFERENCES,0.8733333333333333,"[17] Venables, W.N., Ripley, B.D.: Modern applied statistics with S-PLUS. Springer Science Business
356"
REFERENCES,0.8755555555555555,"Media (2013)
357"
REFERENCES,0.8777777777777778,"[18] Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X., Liu, W., Wang, J.:
358"
REFERENCES,0.88,"High-resolution representations for labeling pixels and regions. arXiv preprint arXiv:1904.04514
359"
REFERENCES,0.8822222222222222,"(2019)
360"
REFERENCES,0.8844444444444445,"[19] Tang, Z., Peng, X., Geng, S., Wu, L., Zhang, S., Metaxas, D.: Quantized densely connected
361"
REFERENCES,0.8866666666666667,"u-nets for efficient landmark localization. In: Proceedings of the European Conference on Computer
362"
REFERENCES,0.8888888888888888,"Vision (ECCV). pp. 339â€“354 (2018)
363"
REFERENCES,0.8911111111111111,"[20] Ahmed, K., Torresani, L.: Maskconnect: Connectivity learning by gradient descent. In: Proceed-
364"
REFERENCES,0.8933333333333333,"ings of the European Conference on Computer Vision (ECCV). pp. 349â€“365 (2018)
365"
REFERENCES,0.8955555555555555,"[21] Xie, S., Kirillov, A., Girshick, R., He, K.: Exploring randomly wired neural networks for image
366"
REFERENCES,0.8977777777777778,"recognition. arXiv preprint arXiv:1904.01569 (2019)
367"
REFERENCES,0.9,"[22] Rauschecker, J.: Neuronal mechanisms of developmental plasticity in the catâ€™s visual system.
368"
REFERENCES,0.9022222222222223,"Human neurobiology (1984)
369"
REFERENCES,0.9044444444444445,"[23] Bender, G., Kindermans, P.J., Zoph, B., Vasudevan, V., Le, Q.: Understanding and simplifying
370"
REFERENCES,0.9066666666666666,"one-shot architecture search. In: International Conference on Machine earning. pp. 550â€“559 (2018)
371"
REFERENCES,0.9088888888888889,"[24] Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Single path one-shot neural
372"
REFERENCES,0.9111111111111111,"architecture search with uniform sampling. arXiv preprint arXiv:1904.00420 (2019)
373"
REFERENCES,0.9133333333333333,"[25] Ghiasi, G., Lin, T.Y., Le, Q.V.: Nas-fpn: Learning scalable feature pyramid architecture for object
374"
REFERENCES,0.9155555555555556,"detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp.
375"
REFERENCES,0.9177777777777778,"7036â€“7045 (2019)
376"
REFERENCES,0.92,"[26] Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L.: Auto-deeplab:
377"
REFERENCES,0.9222222222222223,"Hierarchical neural architecture search for semantic image segmentation. In: Proceedings of the
378"
REFERENCES,0.9244444444444444,"IEEE Conference on Computer Vision and Pattern Recognition. pp. 82â€“92 (2019)
379"
REFERENCES,0.9266666666666666,"[27] Huang, Z., Wang, N.: Data-driven sparse structure selection for deep neural networks. In:
380"
REFERENCES,0.9288888888888889,"Proceedings of the European conference on computer vision (ECCV). pp. 304â€“320 (2018)
381"
REFERENCES,0.9311111111111111,"[28] Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for efficient neural
382"
REFERENCES,0.9333333333333333,"network. In: Advances in neural information processing systems. pp. 1135â€“1143 (2015)
383"
REFERENCES,0.9355555555555556,"[29] Kun Yuan, Quanquan Li,Jing Shao, and Junjie Yan Learning Connectivity of Neural Networks
384"
REFERENCES,0.9377777777777778,"from a Topological Perspective. ECCV 2020
385"
REFERENCES,0.94,"[30] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in
386"
REFERENCES,0.9422222222222222,"Graph domains. International Joint Conference on Neural Networks 2 (2005), 729â€“734.
387"
REFERENCES,0.9444444444444444,"[31] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and G. Monfardini. 2009.
388"
REFERENCES,0.9466666666666667,"The Graph Neural Network Model. IEEE Trans. Neural Networks 20, 1 (2009), 61â€“80.
389"
REFERENCES,0.9488888888888889,"[32] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. 2017. Protein interface prediction
390"
REFERENCES,0.9511111111111111,"using graph convolutional networks. Advances in Neural Information Processing Systems 2017-
391"
REFERENCES,0.9533333333333334,"Decem, Nips (2017), 6531â€“6540.
392"
REFERENCES,0.9555555555555556,"[33] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019.
393"
REFERENCES,0.9577777777777777,"Graph neural networks for social recommendation. The Web Conference 2019, WWW 2019 (2019),
394"
REFERENCES,0.96,"417â€“426.
395"
REFERENCES,0.9622222222222222,"[34] T. Young, D. Hazarika, S. Poria, and E. Cambria. 2018. Recent Trends in Deep Learning Based
396"
REFERENCES,0.9644444444444444,"Natural Language Processing. IEEE Computational Intelligence Magazine 13, 3 (2018), 55â€“75.
397"
REFERENCES,0.9666666666666667,"[35] Zhipu Xie, Weifeng Lv, Shangfo Huang, Zhilong Lu, and Bowen Du. 2020. Sequential Graph
398"
REFERENCES,0.9688888888888889,"Neural Network for Urban Road Traffic Speed Prediction. IEEE Access 8 (2020), 63349â€“63358.
399"
REFERENCES,0.9711111111111111,"[36] Sweah Liang Yong, Markus Hagenbuchner, Ah Chung Tsoi, Franco Scarselli, and Marco Gori.
400"
REFERENCES,0.9733333333333334,"2006. Document mining using graph neural network. In International Workshop of the Initiative for
401"
REFERENCES,0.9755555555555555,"the Evaluation of XML Retrieval. Springer, 458â€“472.
402"
REFERENCES,0.9777777777777777,"[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local Neural
403"
REFERENCES,0.98,"Networks. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2018),
404"
REFERENCES,0.9822222222222222,"7794â€“7803.
405"
REFERENCES,0.9844444444444445,"[38] Xiangyang Ju, Steven Farrell, Paolo Calafiura, Daniel Murnane, Prabhat, Lindsey Gray, et al.
406"
REFERENCES,0.9866666666666667,"2019. Graph Neural Networks for Particle Reconstruction in High Energy Physics detectors. In
407"
REFERENCES,0.9888888888888889,"Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019).
408"
REFERENCES,0.9911111111111112,"[39] Krzysztof Rusek and Piotr Cholda. 2019. Message-Passing Neural Networks Learn Littleâ€™s Law.
409"
REFERENCES,0.9933333333333333,"IEEE Communications Letters 23, 2 (2019), 274â€“277.
410"
REFERENCES,0.9955555555555555,"[40] Sergi Abadal, Akshay Jain, Robert Guirado, Jorge LÃ³pez-Alonso, Eduard AlarcÃ³n. Computing
411"
REFERENCES,0.9977777777777778,"Graph Neural Networks: A Survey from Algorithms to Accelerators ACM Computing 2021
412"
