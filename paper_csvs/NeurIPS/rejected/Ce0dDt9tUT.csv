Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00196078431372549,"In computer vision, noise is conventionally viewed as a harmful perturbation
1"
ABSTRACT,0.00392156862745098,"in various deep learning architectures, such as convolutional neural networks
2"
ABSTRACT,0.0058823529411764705,"(CNNs) and vision transformers (ViTs), as well as different tasks like image
3"
ABSTRACT,0.00784313725490196,"classification and transfer learning. However, this paper aims to rethink whether
4"
ABSTRACT,0.00980392156862745,"the conventional proposition always holds. We demonstrate that specific noise can
5"
ABSTRACT,0.011764705882352941,"boost the performance of various deep architectures under certain conditions. We
6"
ABSTRACT,0.013725490196078431,"theoretically prove the enhancement gained from positive noise by reducing the task
7"
ABSTRACT,0.01568627450980392,"complexity defined by information entropy and experimentally show the significant
8"
ABSTRACT,0.01764705882352941,"performance gain in large image datasets, such as the ImageNet. Herein, we use
9"
ABSTRACT,0.0196078431372549,"the information entropy to define the complexity of the task. We categorize the
10"
ABSTRACT,0.021568627450980392,"noise into two types, positive noise (PN) and harmful noise (HN), based on whether
11"
ABSTRACT,0.023529411764705882,"the noise can help reduce the complexity of the task. Extensive experiments of
12"
ABSTRACT,0.025490196078431372,"CNNs and ViTs have shown performance improvements by proactively injecting
13"
ABSTRACT,0.027450980392156862,"positive noise, where we achieve an unprecedented top 1 accuracy over 95% on
14"
ABSTRACT,0.029411764705882353,"ImageNet. Both theoretical analysis and empirical evidence have confirmed that the
15"
ABSTRACT,0.03137254901960784,"presence of positive noise, can benefit the learning process, while the traditionally
16"
ABSTRACT,0.03333333333333333,"perceived harmful noise indeed impairs deep learning models. The different roles
17"
ABSTRACT,0.03529411764705882,"of noise offer new explanations for deep models on specific tasks and provide a
18"
ABSTRACT,0.03725490196078431,"new paradigm for improving model performance. Moreover, it reminds us to utilize
19"
ABSTRACT,0.0392156862745098,"noise rather than suppress noise.
20"
INTRODUCTION,0.041176470588235294,"1
Introduction
21"
INTRODUCTION,0.043137254901960784,"Noise, conventionally regarded as a hurdle in machine learning and deep learning tasks, is universal
22"
INTRODUCTION,0.045098039215686274,"and unavoidable due to various reasons, e.g., environmental factors, instrumental calibration, and
23"
INTRODUCTION,0.047058823529411764,"human activities [23] [37]. In computer vision, noise can be generated from different phases: (1)
24"
INTRODUCTION,0.049019607843137254,"Image Acquisition: Noise can arise from a camera sensor or other imaging device [33]. For example,
25"
INTRODUCTION,0.050980392156862744,"electronic or thermal noise in the camera sensor can result in random pixel values or color variations
26"
INTRODUCTION,0.052941176470588235,"that can be visible in the captured image. (2) Image Preprocessing: Noise can be introduced during
27"
INTRODUCTION,0.054901960784313725,"preprocessing steps such as image resizing, filtering, or color space conversion [1]. For example,
28"
INTRODUCTION,0.056862745098039215,"resizing an image can introduce aliasing artifacts, while filtering an image can result in the loss of
29"
INTRODUCTION,0.058823529411764705,"detail and texture. (3) Feature Extraction: Feature extraction algorithms can be sensitive to noise
30"
INTRODUCTION,0.060784313725490195,"in the input image, which can result in inaccurate or inconsistent feature representations [2]. For
31"
INTRODUCTION,0.06274509803921569,"example, edge detection algorithms can be affected by noise in the image, resulting in false positives
32"
INTRODUCTION,0.06470588235294118,"or negatives. (4) Algorithms: algorithms used for computer vision tasks, such as object detection or
33"
INTRODUCTION,0.06666666666666667,"image segmentation, can also be sensitive to noise in the input data [6]. Noise can cause the algorithm
34"
INTRODUCTION,0.06862745098039216,"to learn incorrect patterns or features, leading to poor performance.
35"
INTRODUCTION,0.07058823529411765,"Since noise is an unavoidable reality in engineering tasks, existing works usually make the assumption
36"
INTRODUCTION,0.07254901960784314,"that noise has a consistently negative impact on the current task [30] [24]. Nevertheless, is the above
37"
INTRODUCTION,0.07450980392156863,"assumption always valid? As such, it is crucial to address the question of whether noise can ever
38"
INTRODUCTION,0.07647058823529412,"have a positive influence on deep learning models. This work aims to provide a comprehensive
39"
INTRODUCTION,0.0784313725490196,"answer to this question, which is a pressing concern in the deep learning community. We recognize
40"
INTRODUCTION,0.0803921568627451,"that the imprecise definition of noise is a critical factor leading to the uncertainties surrounding the
41"
INTRODUCTION,0.08235294117647059,"identification and characterization of noise. To address these uncertainties, an in-depth analysis
42"
INTRODUCTION,0.08431372549019608,"of the task’s complexity is imperative for arriving at a rigorous answer. By using the definition of
43"
INTRODUCTION,0.08627450980392157,"task entropy, it is possible to categorize noise into two distinct categories: positive noise (PN) and
44"
INTRODUCTION,0.08823529411764706,"harmful noise (HN). PN decreases the complexity of the task, while HN increases it, aligning with
45"
INTRODUCTION,0.09019607843137255,"the conventional understanding of noise.
46"
SCOPE AND CONTRIBUTION,0.09215686274509804,"1.1
Scope and Contribution
47"
SCOPE AND CONTRIBUTION,0.09411764705882353,"Our work aims to investigate how various types of noise affect deep learning models. Specifically,
48"
SCOPE AND CONTRIBUTION,0.09607843137254903,"the study focuses on three common types of noise, i.e., Gaussian noise, linear transform noise, and
49"
SCOPE AND CONTRIBUTION,0.09803921568627451,"salt-and-pepper noise. Gaussian noise refers to random fluctuations that follow a Gaussian distribution
50"
SCOPE AND CONTRIBUTION,0.1,"in pixel values at the image level or latent representations in latent space [29]. Linear transforms, on
51"
SCOPE AND CONTRIBUTION,0.10196078431372549,"the other hand, refer to affine elementary matrix transformations to the dataset of original images
52"
SCOPE AND CONTRIBUTION,0.10392156862745099,"or latent representations, where the elementary matrix is row equivalent to an identity matrix [36].
53"
SCOPE AND CONTRIBUTION,0.10588235294117647,"Salt-and-pepper noise is a kind of image distortion that adds random black or white values at the
54"
SCOPE AND CONTRIBUTION,0.10784313725490197,"image level or to the latent representations [7].
55"
SCOPE AND CONTRIBUTION,0.10980392156862745,"This paper analyzes the impact of these types of noise on the performance of deep learning models for
56"
SCOPE AND CONTRIBUTION,0.11176470588235295,"image classification and domain adaptation tasks. Two popular model families, Vision Transformers
57"
SCOPE AND CONTRIBUTION,0.11372549019607843,"(ViTs) and Convolutional Neural Networks (CNNs), are considered in the study. Image classification
58"
SCOPE AND CONTRIBUTION,0.11568627450980393,"is one of the most fundamental tasks in computer vision, where the goal is to predict the class label of
59"
SCOPE AND CONTRIBUTION,0.11764705882352941,"an input image. Domain adaptation is a practically meaningful task where the training and test data
60"
SCOPE AND CONTRIBUTION,0.11960784313725491,"come from different distributions, also known as different domains. By investigating the effects of
61"
SCOPE AND CONTRIBUTION,0.12156862745098039,"different types of noise on ViTs and CNNs for typical deep learning tasks, the paper provides insights
62"
SCOPE AND CONTRIBUTION,0.12352941176470589,"into the influences of noises on deep models. The findings presented in this paper hold practical
63"
SCOPE AND CONTRIBUTION,0.12549019607843137,"significance for enhancing the performance of various types of deep learning models in real-world
64"
SCOPE AND CONTRIBUTION,0.12745098039215685,"scenarios.
65"
SCOPE AND CONTRIBUTION,0.12941176470588237,"The contributions of this paper are summarized as follows:
66"
SCOPE AND CONTRIBUTION,0.13137254901960785,"• We re-examined the conventional view that noise, by default, has a negative impact on deep
67"
SCOPE AND CONTRIBUTION,0.13333333333333333,"learning models. Our theoretical analysis and experimental results show that noise can be a
68"
SCOPE AND CONTRIBUTION,0.13529411764705881,"positive support for deep learning models and tasks.
69"
SCOPE AND CONTRIBUTION,0.13725490196078433,"• We implemented extensive experiments with different deep models, such as CNNs and
70"
SCOPE AND CONTRIBUTION,0.1392156862745098,"ViTs, and on different deep learning tasks. Empowered by positive noise, we achieved
71"
SCOPE AND CONTRIBUTION,0.1411764705882353,"state-of-the-art (SOTA) results in all the experiments presented in this paper.
72"
SCOPE AND CONTRIBUTION,0.14313725490196078,"• Instead of operating on the image level, our injecting noise operations are performed in the
73"
SCOPE AND CONTRIBUTION,0.1450980392156863,"latent space. We theoretically analyze the difference between injecting noise on the image
74"
SCOPE AND CONTRIBUTION,0.14705882352941177,"level and in the latent space.
75"
SCOPE AND CONTRIBUTION,0.14901960784313725,"• The theory and framework of reducing task complexity via positive noise in this work can
76"
SCOPE AND CONTRIBUTION,0.15098039215686274,"be applied to any deep learning architecture. There is great potential for exploring the
77"
SCOPE AND CONTRIBUTION,0.15294117647058825,"application of positive noise in other deep-learning tasks beyond the image classification
78"
SCOPE AND CONTRIBUTION,0.15490196078431373,"and domain adaptation tasks examined in this study.
79"
RELATED WORK,0.1568627450980392,"1.2
Related Work
80"
RELATED WORK,0.1588235294117647,"Positive Noise In fact, within the signal-processing society, it has been demonstrated that random
81"
RELATED WORK,0.1607843137254902,"noise helps stochastic resonance improve the detection of weak signals [4]. Noises can have positive
82"
RELATED WORK,0.1627450980392157,"support and contribute to less mean square error compared to the best linear unbiased estimator when
83"
RELATED WORK,0.16470588235294117,"the mixing probability distribution is not in the extreme region [28]. Also, it has been reported that
84"
RELATED WORK,0.16666666666666666,"noise could increase the model generalization in natural language processing (NLP) [27]. Recently,
85"
RELATED WORK,0.16862745098039217,"the perturbation, a special case of positive noise, has been effectively utilized to implement self-
86"
RELATED WORK,0.17058823529411765,"refinement in domain adaptation and achieved state-of-the-art performance [36]. The latest research
87"
RELATED WORK,0.17254901960784313,"shows that by proactively adding specific noise to partial datasets, various tasks can benefit from the
88"
RELATED WORK,0.17450980392156862,"positive noise [19]. Besides, noises are found to be able to boost brain power and be useful in many
89"
RELATED WORK,0.17647058823529413,"neuroscience studies [21] [22].
90"
RELATED WORK,0.1784313725490196,"Deep Model Convolutional Neural Networks have been widely used for image classification, object
91"
RELATED WORK,0.1803921568627451,"detection, and segmentation tasks, and have achieved impressive results [18][15]. However, these
92"
RELATED WORK,0.18235294117647058,"networks have limitations in terms of their ability to capture long-range dependencies and extract
93"
RELATED WORK,0.1843137254901961,"global features from images. Recently, Vision Transformers has been proposed as an alternative to
94"
RELATED WORK,0.18627450980392157,"CNNs [13]. ViT relies on self-attention mechanisms and a transformer-based architecture to enable
95"
RELATED WORK,0.18823529411764706,"global feature extraction and modeling of long-range dependencies in images [40]. The attention
96"
RELATED WORK,0.19019607843137254,"mechanism allows the model to focus on the most informative features of the input image, while
97"
RELATED WORK,0.19215686274509805,"the transformer architecture facilitates information exchange between different parts of the image.
98"
RELATED WORK,0.19411764705882353,"ViT has demonstrated impressive performance on a range of image classification tasks and has the
99"
RELATED WORK,0.19607843137254902,"potential to outperform traditional CNN-based approaches. However, ViT currently requires a large
100"
RELATED WORK,0.1980392156862745,"number of parameters and training data to achieve state-of-the-art results, making it challenging to
101"
RELATED WORK,0.2,"implement in certain settings [45].
102"
PRELIMINARY,0.2019607843137255,"2
Preliminary
103"
PRELIMINARY,0.20392156862745098,"In information theory, the entropy [32] of a random variable x is defined as:
104"
PRELIMINARY,0.20588235294117646,"H(x) =

−
R
p(x) log p(x)dx
if x is continuous
−P"
PRELIMINARY,0.20784313725490197,"x p(x) log p(x)
if x is discrete
(1)"
PRELIMINARY,0.20980392156862746,"where p(x) is the distribution of the given variable x. And the mutual information (MI) of two
105"
PRELIMINARY,0.21176470588235294,"random discrete variables (x, y) is denoted as [8]:
106"
PRELIMINARY,0.21372549019607842,"MI(x, y) =DKL(p(x, y) ∥p(x) ⊗p(y))
=H(x) −H(x|y)
(2)"
PRELIMINARY,0.21568627450980393,"where DKL is the Kullback–Leibler divergence [16], and p(x, y) is the joint distribution. The
107"
PRELIMINARY,0.21764705882352942,"conditional entropy is defined as:
108"
PRELIMINARY,0.2196078431372549,"H(x|y) = −
X
p(x, y) log p(x|y)
(3)"
PRELIMINARY,0.22156862745098038,"The above definitions can be readily expanded to encompass continuous variables through the
109"
PRELIMINARY,0.2235294117647059,"substitution of the sum operator with the integral symbol. In this work, the noise is denoted by ϵ if
110"
PRELIMINARY,0.22549019607843138,"without any specific statement.
111"
PRELIMINARY,0.22745098039215686,"Before delving into the correlation between task and noise, it is imperative to address the initial
112"
PRELIMINARY,0.22941176470588234,"crucial query of the mathematical measurement of a task T . With the assistance of information
113"
PRELIMINARY,0.23137254901960785,"theory, the complexity associated with a given task T can be measured in terms of the entropy of T .
114"
PRELIMINARY,0.23333333333333334,"Therefore, we can borrow the concepts of information entropy to explain the difficulty of the task.
115"
PRELIMINARY,0.23529411764705882,"For example, a smaller H(T ) means an easier task and vice versa.
116"
PRELIMINARY,0.2372549019607843,"Since the entropy of task T is formulated, it is not difficult to define the mutual information of task T
117"
PRELIMINARY,0.23921568627450981,"and noise ϵ,
118"
PRELIMINARY,0.2411764705882353,"MI(T , ϵ) = H(T ) −H(T |ϵ)
(4)"
PRELIMINARY,0.24313725490196078,"Formally, if the noise can help reduce the complexity of the task, i.e., H(T |ϵ) < H(T ) then the noise
119"
PRELIMINARY,0.24509803921568626,"has positive support. Therefore, a noise ϵ is defined as positive noise (PN) when the noise satisfies
120"
PRELIMINARY,0.24705882352941178,"MI(T , ϵ) > 0. On the contrary, when MI(T , ϵ) ≤0, the noise is considered as the conventional
121"
PRELIMINARY,0.24901960784313726,"noise and named harmful noise (HN). The positive noise can be perceived as an augmentation of
122"
PRELIMINARY,0.25098039215686274,"information gain brought by ϵ.
123"
PRELIMINARY,0.2529411764705882,"
MI(T , ϵ) > 0
ϵ is positive noise
MI(T , ϵ) ≤0
ϵ is harmful noise
(5)"
PRELIMINARY,0.2549019607843137,"Moderate Model Assumption: The positive noise may not work for deep models with severe
124"
PRELIMINARY,0.2568627450980392,"problems. For example, the model is severely overfitting where models begin to memorize the
125"
PRELIMINARY,0.25882352941176473,"random fluctuations in the data instead of learning the underlying patterns. In that case, the presence
126"
PRELIMINARY,0.2607843137254902,"of positive noise will not have significant positive support in improving the models’ performance.
127"
PRELIMINARY,0.2627450980392157,"Besides, when the models are corrupted under brute force attack, the positive noise also can not work.
128"
PRELIMINARY,0.2647058823529412,"Input (Images)
Predicted Classes"
PRELIMINARY,0.26666666666666666,"CNN (ResNet)
ViT"
PRELIMINARY,0.26862745098039215,Deep Model
PRELIMINARY,0.27058823529411763,Inject Noise conv pool conv conv conv conv conv conv conv conv
PRELIMINARY,0.2725490196078431,Average Pool FC ... Norm
PRELIMINARY,0.27450980392156865,Multi-Head
PRELIMINARY,0.27647058823529413,Attention Norm MLP 1×
PRELIMINARY,0.2784313725490196,Transformer Layer
PRELIMINARY,0.2803921568627451,Patch Embedding
PRELIMINARY,0.2823529411764706,Transformer Layer ... FC L×
PRELIMINARY,0.28431372549019607,Transformer Layer
PRELIMINARY,0.28627450980392155,Class token
PRELIMINARY,0.28823529411764703,FC: Fully Connected Layer Noise
PRELIMINARY,0.2901960784313726,Choose a Random Layer Noise
PRELIMINARY,0.29215686274509806,Class 1
PRELIMINARY,0.29411764705882354,Class 2
PRELIMINARY,0.296078431372549,Class 3
PRELIMINARY,0.2980392156862745,"Figure 1: An overview of the proposed method. Above the black line is the standard pipeline for
image classification. The deep model can be CNNs or ViTs. The noise is injected into a randomly
chosen layer of the model represented by the blue arrow."
METHODS,0.3,"3
Methods
129"
METHODS,0.30196078431372547,"The idea of exploring the influence of noise on the deep models is straightforward. The framework is
130"
METHODS,0.30392156862745096,"depicted in Fig. 1. This is a universal framework where there are different options for deep models,
131"
METHODS,0.3058823529411765,"such as CNNs and ViTs. Through the simple operation of injecting noise into a randomly selected
132"
METHODS,0.307843137254902,"layer, a model has the potential to gain additional information to reduce task complexity, thereby
133"
METHODS,0.30980392156862746,"improving its performance. It is sufficient to inject noise into a single layer instead of multiple layers
134"
METHODS,0.31176470588235294,"since it imposes a regularization on multiple layers simultaneously.
135"
METHODS,0.3137254901960784,"For a classification problem, the dataset (X, Y ) can be regarded as samplings derived from DX,Y,
136"
METHODS,0.3156862745098039,"where DX,Y is some unknown joint distribution of data points and labels from feasible space X and
137"
METHODS,0.3176470588235294,"Y, i.e., (X, Y ) ∼DX,Y [31]. Hence, given a set of k data points X = {X1, X2, ..., Xk}, the label
138"
METHODS,0.3196078431372549,"set Y = {Y1, Y2, ..., Yk} is regarded as sampling from Y ∼DY|X . The complexity of T on dataset
139"
METHODS,0.3215686274509804,"X is formulated as [19]:
140"
METHODS,0.3235294117647059,"H(T ; X) = −
X"
METHODS,0.3254901960784314,"Y ∈Y
p(Y |X) log p(Y |X)
(6)"
METHODS,0.32745098039215687,"The operation of adding noise at the image level can be formulated as:
141
H(T ; X + ϵ) = −P"
METHODS,0.32941176470588235,"Y ∈Y p(Y |X + ϵ) log p(Y |X + ϵ)
ϵ is additive noise
H(T ; Xϵ) = −P"
METHODS,0.33137254901960783,"Y ∈Y p(Y |Xϵ) log p(Y |Xϵ)
ϵ is multiplicative noise
(7)"
METHODS,0.3333333333333333,"While the operation of proactively injecting noise in the latent space can be formulated as:
142"
METHODS,0.3352941176470588,"
H(T ; X + ϵ)
⋆= H(Y ; X + ϵ) −H(X)
ϵ is additive noise
H(T ; Xϵ)
⋆= H(Y ; Xϵ) −H(X)
ϵ is multiplicative noise
(8)"
METHODS,0.33725490196078434,"Step ⋆differs from the conventional definition of conditional entropy, as our method injects the noise
143"
METHODS,0.3392156862745098,"into the latent representations instead of the original images. The Gaussian noise is additive, the
144"
METHODS,0.3411764705882353,"linear transform noise is also additive, and the salt-and-pepper is a multiplicative noise.
145"
METHODS,0.3431372549019608,"Gaussian Noise The Gaussian noise is one of the most common additive noises that appeared in
146"
METHODS,0.34509803921568627,"computer vision tasks. The Gaussian noise is independent and stochastic, obeying the Gaussian
147"
METHODS,0.34705882352941175,"distribution. Without loss of generality, defined as N(µ, σ2). Since our injection happens in the
148"
METHODS,0.34901960784313724,"latent space, therefore, the complexity of the task is:
149"
METHODS,0.3509803921568627,"H(T ; X + ϵ)
⋆= H(Y ; X + ϵ) −H(X).
(9)"
METHODS,0.35294117647058826,"According to the definition in Equation 4, and making the distribution of X and Y multivariate
150"
METHODS,0.35490196078431374,"normal distribution [5] [14], the mutual information with Gaussian noise is:
151"
METHODS,0.3568627450980392,"MI(T , ϵ) =H(Y ; X) −H(X) −(H(Y ; X + ϵ) −H(X))
=H(Y ; X) −H(Y ; X + ϵ) =1"
LOG,0.3588235294117647,"2 log
|ΣX||ΣY −ΣY XΣ−1
X ΣXY |"
LOG,0.3607843137254902,"|ΣX+ϵ||ΣY −ΣY XΣ−1
X+ϵΣXY | =1"
LOG,0.3627450980392157,"2 log
1"
LOG,0.36470588235294116,"(1 + σ2ϵ
Pk
i=1
1
σ2
Xi )(1 + λPk
i=1
cov2(Xi,Yi)
σ2
Xi(σ2
Xiσ2
Yi−cov2(Xi,Yi))) (10)"
LOG,0.36666666666666664,"where λ =
σ2
ϵ
1+Pk
i=1
1
σ2
Xi
, σ2
ϵ is the variance of the Gaussian noise, cov(Xi, Yi) is the covariance of
152"
LOG,0.3686274509803922,"sample pair Xi, Yi, σ2
Xi and σ2
Yi are the variance of data sample Xi and data label Yi, respectively.
153"
LOG,0.37058823529411766,"The detailed derivations can be found in section 1.1.2 of the supplementary. Given a dataset, the
154"
LOG,0.37254901960784315,"variance of the Gaussian noise, and statistical properties of data samples and labels control the mutual
155"
LOG,0.37450980392156863,"information, we define the function:
156"
LOG,0.3764705882352941,"f(σ2
ϵ ) =1 −(1 + σ2
ϵ
Pk
i=1
1
σ2
Xi )(1 + λ k
X i=1"
LOG,0.3784313725490196,"cov2(Xi, Yi)
σ2
Xi(σ2
Xiσ2
Yi −cov2(Xi, Yi)))"
LOG,0.3803921568627451,"= −σ2
ϵ
Pk
i=1
1
σ2
Xi −σ2
ϵ
Pk
i=1
1
σ2
Xi · λ k
X i=1"
LOG,0.38235294117647056,"cov2(Xi, Yi)
σ2
Xi(σ2
Xiσ2
Yi −cov2(Xi, Yi)) −λ k
X i=1"
LOG,0.3843137254901961,"cov2(Xi, Yi)
σ2
Xi(σ2
Xiσ2
Yi −cov2(Xi, Yi))"
LOG,0.3862745098039216,"(11)
Since ϵ2 ≥0 and λ ≥0, σ2
Xiσ2
Yi −cov2(Xi, Yi) = σ2
Xiσ2
Yi(1 −ρ2
XiYi) ≥0, where ρXiYi is the
157"
LOG,0.38823529411764707,"correlation coefficient, the sign of f(σ2
ϵ ) is negative. We can conclude that Gaussian noise injected
158"
LOG,0.39019607843137255,"into the latent space is harmful to the task. More details and the Gaussian noise added to the image
159"
LOG,0.39215686274509803,"level are provided in the supplementary.
160"
LOG,0.3941176470588235,"Linear Transform Noise This type of noise is obtained by elementary transformation of the features
161"
LOG,0.396078431372549,"matrix, i.e., ϵ = QX, where Q is an elementary matrix. We name the Q the quality matrix since it
162"
LOG,0.3980392156862745,"controls the property of linear transform noise and determines whether positive or harmful. In the
163"
LOG,0.4,"linear transform noise injection in the latent space case, the complexity of the task is:
164"
LOG,0.4019607843137255,"H(T ; X + QX)
⋆= H(Y ; X + QX) −H(X)
(12)"
LOG,0.403921568627451,"The mutual information is then formulated as:
165"
LOG,0.40588235294117647,"MI(T , QX)
⋆=H(Y ; X) −H(X) −(H(Y ; X + QX) −H(X))
=H(Y ; X) −H(Y ; X + QX) =1"
LOG,0.40784313725490196,"2 log
|ΣX||ΣY −ΣY XΣ−1
X ΣXY |"
LOG,0.40980392156862744,"|Σ(I+Q)X||ΣY −ΣY XΣ−1
X ΣXY | =1"
LOG,0.4117647058823529,"2 log
1
|I + Q|2"
LOG,0.4137254901960784,= −log |I + Q| (13)
LOG,0.41568627450980394,"Since we want the mutual information to be greater than 0, we can formulate Equation 13 as an
166"
LOG,0.4176470588235294,"optimization problem:
167"
LOG,0.4196078431372549,"max
Q MI(T , QX)"
LOG,0.4215686274509804,"s.t. rank(I + Q) = k
Q ∼I
[I + Q]ii ≥[I + Q]ij , i ̸= j"
LOG,0.4235294117647059,∥[I + Q]i∥1 = 1 (14)
LOG,0.42549019607843136,"where ∼means the row equivalence. The key to determining whether the linear transform is positive
168"
LOG,0.42745098039215684,"noise or not lies in the matrix of Q. The most important step is to ensure that I + Q is reversible,
169"
LOG,0.4294117647058823,"which is |(I + Q)| ̸= 0. The third constraint is to make the trained classifier get enough information
170"
LOG,0.43137254901960786,"about a specific image and correctly predict the corresponding label. For example, for an image X1
171"
LOG,0.43333333333333335,"perturbed by another image X2, the classifier obtained dominant information from X1 so that it can
172"
LOG,0.43529411764705883,"predict the label Y1. However, if the perturbed image X2 is dominant, the classifier can hardly predict
173"
LOG,0.4372549019607843,"the correct label Y1 and is more likely to predict as Y2. The fourth constraint is to maintain the norm
174"
LOG,0.4392156862745098,"of latent representations. More in-depth discussion and linear transform noise added to the image
175"
LOG,0.4411764705882353,"level are provided in the supplementary.
176"
LOG,0.44313725490196076,"Salt-and-pepper Noise The salt-and-pepper noise is a common multiplicative noise for images. The
177"
LOG,0.44509803921568625,"image can exhibit unnatural changes, such as black pixels in bright areas or white pixels in dark
178"
LOG,0.4470588235294118,"areas, specifically as a result of the signal disruption caused by sudden strong interference or bit
179"
LOG,0.44901960784313727,"transmission errors. In the Salt-and-pepper noise case, the mutual information is:
180"
LOG,0.45098039215686275,"MI(T , ϵ)
⋆=H(Y ; X) −H(X) −(H(Y ; Xϵ) −H(X))
=H(Y ; X) −H(Y ; Xϵ) = −
X X∈X X"
LOG,0.45294117647058824,"Y ∈Y
p(X, Y ) log p(X, Y ) −
X X∈X X Y ∈Y X"
LOG,0.4549019607843137,"ϵ∈E
p(Xϵ, Y ) log p(Xϵ, Y )"
LOG,0.4568627450980392,"=E

log
1
p(X, Y )"
LOG,0.4588235294117647,"
−E

log
1
p(Xϵ, Y ) "
LOG,0.46078431372549017,"=E

log
1
p(X, Y )"
LOG,0.4627450980392157,"
−E

log
1
p(X, Y )"
LOG,0.4647058823529412,"
−E

log
1
p(ϵ) "
LOG,0.4666666666666667,= −H(ϵ) (15)
LOG,0.46862745098039216,"Obviously, the mutual information is smaller than 0, which indicates the complexity is increasing
181"
LOG,0.47058823529411764,"when injecting salt-and-pepper noise into the deep model. As can be foreseen, the salt-and-pepper
182"
LOG,0.4725490196078431,"noise is pure detrimental noise. More details and Salt-and-pepper added to the image level are in the
183"
LOG,0.4745098039215686,"supplementary.
184"
EXPERIMENTS,0.4764705882352941,"4
Experiments
185"
EXPERIMENTS,0.47843137254901963,"In this section, we conduct extensive experiments to explore the influence of various types of noises
186"
EXPERIMENTS,0.4803921568627451,"on deep learning models. We employ popular deep learning architectures, including both CNNs
187"
EXPERIMENTS,0.4823529411764706,"and ViTs, and show that the two kinds of deep models can benefit from the positive noise. We
188"
EXPERIMENTS,0.4843137254901961,"employ deep learning models of various scales, including ViT-Tiny (ViT-T), ViT-Small (ViT-S),
189"
EXPERIMENTS,0.48627450980392156,"ViT-Base (ViT-B), and ViT-Large (ViT-L) for Vision Transformers (ViTs), and ResNet-18, ResNet-34,
190"
EXPERIMENTS,0.48823529411764705,"ResNet-50, and ResNet-101 for ResNet architecture. The details of deep models are presented in the
191"
EXPERIMENTS,0.49019607843137253,"supplementary. Without specific instructions, the noise is injected at the last layer of the deep models.
192"
EXPERIMENTS,0.492156862745098,"Note that for ResNet models, the number of macro layers is 4, and for each macro layer, different
193"
EXPERIMENTS,0.49411764705882355,"scale ResNet models have different micro sublayers. For example, for ResNet-18, the number of
194"
EXPERIMENTS,0.49607843137254903,"macro layers is 4, and for each macro layer, the number of micro sublayers is 2. The noise is injected
195"
EXPERIMENTS,0.4980392156862745,"at the last micro sublayer of the last macro layer for ResNet models. More experimental settings for
196"
EXPERIMENTS,0.5,"ResNet and ViT are detailed in the supplementary.
197"
NOISE SETTING,0.5019607843137255,"4.1
Noise Setting
198"
NOISE SETTING,0.503921568627451,"We utilize the standard normal distribution to generate Gaussian noise in our experiments, ensuring
199"
NOISE SETTING,0.5058823529411764,"that the noise has zero mean and unit variance. Gaussian noise can be expressed as:
200"
NOISE SETTING,0.5078431372549019,"ϵ ∼N(0, 1)
(16)"
NOISE SETTING,0.5098039215686274,"For linear transform noise, we use a quality matrix of as:
201"
NOISE SETTING,0.5117647058823529,"Q = −αI + αf(I)
(17)"
NOISE SETTING,0.5137254901960784,"where I is the identity matrix, α represents the linear transform strength and f is a row cyclic shift
202"
NOISE SETTING,0.515686274509804,"operation switching row to the next row, for example, in a 3 × 3 matrix, f will move Row 1 to Row
203"
NOISE SETTING,0.5176470588235295,"2, Row 2 to Row 3, and Row 3 to Row 1. For salt-and-pepper noise, we also use the parameter α to
204"
NOISE SETTING,0.5196078431372549,"control the probability of the emergence of salt-and-pepper noise, which can be formulated as:
205
max(X)
if p < α/2
min(X)
if p > 1 −α/2
(18)"
NOISE SETTING,0.5215686274509804,"Table 1: ResNet with different kinds of noise on ImageNet. Vanilla means the vanilla model without
noise. Accuracy is shown in percentage. Gaussian noise used here is subjected to standard normal
distribution. Linear transform noise used in this table is designed to be positive noise. The difference
is shown in the bracket."
NOISE SETTING,0.5235294117647059,"Model
ResNet-18
ResNet-34
ResNet-50
ResNet-101
Vanilla
63.90 (+0.00)
66.80 (+0.00)
70.00 (+0.00)
70.66 (+0.00)
+ Gaussian Noise
62.35 (-1.55)
65.40 (-1.40)
69.62 (-0.33)
70.10 (-0.56)
+ Linear Transform Noise
79.62 (+15.72)
80.05 (+13.25)
81.32 (+11.32)
81.91 (+11.25)
+ Salt-and-pepper Noise
55.45 (-8.45)
63.36 (-3.44)
45.89 (-24.11)
52.96 (-17.70)"
NOISE SETTING,0.5254901960784314,"Table 2: ViT with different kinds of noise on ImageNet. Vanilla means the vanilla model without
injecting noise. Accuracy is shown in percentage. Gaussian noise used here is subjected to stan-
dard normal distribution. Linear transform noise used in this table is designed to be positive noise.
The difference is shown in the bracket. Note ViT-L is overfitting on ImageNet [13] [34]."
NOISE SETTING,0.5274509803921569,"Model
ViT-T
ViT-S
ViT-B
ViT-L
Vanilla
79.34 (+0.00)
81.88 (+0.00)
84.33 (+0.00)
88.64 (+0.00)
+ Gaussian Noise
79.10 (-0.24)
81.80 (-0.08)
83.41 (-0.92)
85.92 (-2.72)
+ Linear Transform Noise
80.69 (+1.35)
87.27 (+5.39)
89.99 (+5.66)
88.72 (+0.08)
+ Salt-and-pepper Noise
78.64 (-0.70)
81.75 (-0.13)
82.40 (-1.93)
85.15 (-3.49)"
NOISE SETTING,0.5294117647058824,"where p is a probability generated by a random seed, α ∈[0, 1), and X is the representation of an
206"
NOISE SETTING,0.5313725490196078,"image.
207"
IMAGE CLASSIFICATION RESULTS,0.5333333333333333,"4.2
Image Classification Results
208"
IMAGE CLASSIFICATION RESULTS,0.5352941176470588,"We implement extensive experiments on large-scale datasets such as ImageNet [11] and small-scale
209"
IMAGE CLASSIFICATION RESULTS,0.5372549019607843,"datasets such as TinyImageNet [17] using ResNets and ViTs.
210"
CNN FAMILY,0.5392156862745098,"4.2.1
CNN Family
211"
CNN FAMILY,0.5411764705882353,"The results of ResNets with different noises on ImageNet are in Table 1. As shown in the table, with
212"
CNN FAMILY,0.5431372549019607,"the design of linear transform noise to be positive noise (PN), ResNet improves the classification
213"
CNN FAMILY,0.5450980392156862,"accuracy by a large margin. While the salt-and-pepper, which is theoretically harmful noise (HN),
214"
CNN FAMILY,0.5470588235294118,"degrades the models. Note we did not utilize data augmentation techniques for ResNet experiments
215"
CNN FAMILY,0.5490196078431373,"except for normalization. The significant results show that positive noise can effectively improve
216"
CNN FAMILY,0.5509803921568628,"classification accuracy by reducing task complexity.
217"
VIT FAMILY,0.5529411764705883,"4.2.2
ViT Family
218"
VIT FAMILY,0.5549019607843138,"The results of ViT with different noises on ImageNet are in Table 2. Since the ViT-L is overfitting on
219"
VIT FAMILY,0.5568627450980392,"the ImageNet [13] [34], the positive noise did not work well on the ViT-L. As shown in the table, the
220"
VIT FAMILY,0.5588235294117647,"existence of positive noise improves the classification accuracy of ViT by a large margin compared to
221"
VIT FAMILY,0.5607843137254902,"vanilla ViT. The comparisons with previously published works, such as DeiT [38], SwinTransformer
222"
VIT FAMILY,0.5627450980392157,"[20], DaViT [12], and MaxViT [39], are shown in Table 3, and our positive noise-empowered ViT
223"
VIT FAMILY,0.5647058823529412,"achieved the new state-of-the-art result. Note that the JFT-300M and JFT-4B datasets are private and
224"
VIT FAMILY,0.5666666666666667,"not publicly available [35], and we believe that ViT large and above will benefit from positive noise
225"
VIT FAMILY,0.5686274509803921,"significantly if trained on larger JFT-300M or JFT-4B, which is theoretically supported in section 4.4.
226"
ABLATION STUDY,0.5705882352941176,"4.3
Ablation Study
227"
ABLATION STUDY,0.5725490196078431,"We also proactively inject noise into variants of ViT, such as DeiT [38], Swin Transformer [20],
228"
ABLATION STUDY,0.5745098039215686,"BEiT [3], and ConViT [9], and the results show that positive noise could benefit various variants
229"
ABLATION STUDY,0.5764705882352941,"of ViT by improving classification accuracy significantly. The results of injecting noise to variants
230"
ABLATION STUDY,0.5784313725490197,"of ViT are reported in the supplementary. We also did ablation studies on the strength of linear
231"
ABLATION STUDY,0.5803921568627451,"transform noise and the injected layer. The results are shown in Fig. 2. We can observe that the
232"
ABLATION STUDY,0.5823529411764706,"deeper layer the positive noise injects, the better prediction performance the model can obtain. There
233"
ABLATION STUDY,0.5843137254901961,"are reasons behind this phenomenon. First, the latent features of input in the deeper layer have better
234"
ABLATION STUDY,0.5862745098039216,"Table 3: Comparison between Positive Noise Empowered ViT with other ViT variants. Top 1
Accuracy is shown in percentage. Here PN is the positive noise, i.e., linear transform noise."
ABLATION STUDY,0.5882352941176471,"Model
Top1 Acc.
Params.
Image Res.
Pretrained Dataset
ViT-B [13]
84.33
86M
224 × 224
ImageNet 21k
DeiT-B [38]
85.70
86M
224 × 224
ImageNet 21k
SwinTransformer-B [20]
86.40
88M
384 × 384
ImageNet 21k
DaViT-B [12]
86.90
88M
384 × 384
ImageNet 21k
MaxViT-B [39]
88.82
119M
512 × 512
JFT-300M (Private)
ViT-22B [10]
89.51
21743M
224 × 224
JFT-4B (Private)
ViT-B+PN
89.99
86M
224 × 224
ImageNet 21k
ViT-B+PN
91.37
86M
384 × 384
ImageNet 21k"
ABLATION STUDY,0.5901960784313726,"representations than those in shallow layers; second, injection to shallow layers obtain less mutual
235"
ABLATION STUDY,0.592156862745098,"information gain because of trendy replacing Equation 8 with Equation 7. More results on the small
236"
ABLATION STUDY,0.5941176470588235,"dataset TinyImageNet can be found in the supplementary.
237"
ABLATION STUDY,0.596078431372549,(a) The relationship of the CNN family between the strength of
ABLATION STUDY,0.5980392156862745,linear transform noise and Top 1 accuracy.
ABLATION STUDY,0.6,(b) The relationship of the CNN family between the injected
ABLATION STUDY,0.6019607843137255,layer of linear transform noise and Top 1 accuracy.
ABLATION STUDY,0.6039215686274509,(c) The relationship of the ViT family between the strength of
ABLATION STUDY,0.6058823529411764,linear transform noise and Top 1 accuracy.
ABLATION STUDY,0.6078431372549019,(d) The relationship of the ViT family between the injected
ABLATION STUDY,0.6098039215686275,layer of linear transform noise and Top 1 accuracy.
ABLATION STUDY,0.611764705882353,"Figure 2: The relationship between the linear transform noise strength and the top 1 accuracy, and
between the injected layer and top 1 accuracy. Parts (a) and (b) are the results of the CNN family,
while parts (c) and (d) are the results of the ViT family. For parts (a) and (c) the linear transform
noise is injected at the last layer. For parts (b) and (d), the influence of positive noise on different
layers is shown. Layers 6, 8, 10, and 12 in the ViT family are chosen for the ablation study."
OPTIMAL QUALITY MATRIX,0.6137254901960785,"4.4
Optimal Quality Matrix
238"
OPTIMAL QUALITY MATRIX,0.615686274509804,"As shown in Equation 14, it is interesting to learn about the optimal quality matrix of Q that maximizes
239"
OPTIMAL QUALITY MATRIX,0.6176470588235294,"the mutual information while satisfying the constraints. This equals minimizing the determinant of
240"
OPTIMAL QUALITY MATRIX,0.6196078431372549,"the matrix sum of I and Q. Here, we directly give out the optimal quality matrix of Q as:
241"
OPTIMAL QUALITY MATRIX,0.6215686274509804,"Qoptimal = diag

1
k + 1 −1, . . . ,
1
k + 1 −1

+
1
k + 11k×k
(19)"
OPTIMAL QUALITY MATRIX,0.6235294117647059,"where k is the number of data samples. And the corresponding upper boundary of the mutual
242"
OPTIMAL QUALITY MATRIX,0.6254901960784314,"information as:
243"
OPTIMAL QUALITY MATRIX,0.6274509803921569,"MI(T , QoptimalX) = (k −1) log (k + 1)
(20)"
OPTIMAL QUALITY MATRIX,0.6294117647058823,Table 4: Top 1 accuracy on ImageNet with the optimal quality matrix of linear transform noise.
OPTIMAL QUALITY MATRIX,0.6313725490196078,"Model
Top1 Acc.
Params.
Image Res.
Pretrained Dataset
ViT-B+Optimal Q
93.87
86M
224 × 224
ImageNet 21k
ViT-B+Optimal Q
95.12
86M
384 × 384
ImageNet 21k"
OPTIMAL QUALITY MATRIX,0.6333333333333333,"Table 5: Comparison with various ViT-based methods on Office-Home.
Method
Ar2ClAr2PrAr2ReCl2ArCl2PrCl2RePr2ArPr2ClPr2ReRe2ArRe2ClRe2PrAvg.
ViT-B[13]
54.7 83.0
87.2
77.3 83.4 85.6
74.4 50.9 87.2
79.6
54.8
88.8 75.5
TVT-B[44]
74.9 86.8
89.5
82.8 88.0 88.3
79.8 71.9 90.1
85.5
74.6
90.6 83.6
CDTrans-B[43] 68.8 85.0
86.9
81.5 87.1 87.3
79.6 63.3 88.2
82.0
66.0
90.6 80.5
SSRT-B [36]
75.2 89.0
91.1
85.1 88.3 90.0
85.0 74.2 91.3
85.7
78.6
91.8 85.4
ViT-B+PN
78.3 90.6
91.9
87.8 92.1 91.9
85.8 78.7 93.0
88.6
80.6
93.5 87.7"
OPTIMAL QUALITY MATRIX,0.6352941176470588,"The details are provided in the supplementary. We find that the upper boundary of the mutual
244"
OPTIMAL QUALITY MATRIX,0.6372549019607843,"information of injecting positive noise is determined by the number of data samples, i.e., the scale of
245"
OPTIMAL QUALITY MATRIX,0.6392156862745098,"the dataset. Therefore, the larger the dataset, the better effect of injecting positive noise into deep
246"
OPTIMAL QUALITY MATRIX,0.6411764705882353,"models. With the optimal quality matrix and the top 1 accuracy of ViT-B on ImageNet can be further
247"
OPTIMAL QUALITY MATRIX,0.6431372549019608,"improved to 95%, which is shown in Table 4.
248"
OPTIMAL QUALITY MATRIX,0.6450980392156863,"Table 6: Comparison with various ViT-based methods on Visda2017.
Method
plane bcycl
bus
car
horse knife mcycl person plant sktbrd train truck Avg.
ViT-B[13]
97.7
48.1
86.6
61.6
78.1 63.4
94.7
10.3
87.7
47.7
94.4 35.5 67.1
TVT-B[44]
92.9
85.6
77.5
60.5
93.6 98.2
89.4
76.4
93.6
92.0
91.7 55.7 83.9
CDTrans-B[43] 97.1
90.5
82.4
77.5
96.6 96.1
93.6
88.6
97.9
86.9
90.3 62.8 88.4
SSRT-B [36]
98.9
87.6
89.1
84.8
98.3 98.7
96.3
81.1
94.9
97.9
94.5 43.1 88.8
ViT-B+PN
98.8
95.5
84.8
73.7
98.5 97.2
95.1
76.5
95.9
98.4
98.3 67.2 90.0"
DOMAIN ADAPTION RESULTS,0.6470588235294118,"4.5
Domain Adaption Results
249"
DOMAIN ADAPTION RESULTS,0.6490196078431373,"Unsupervised domain adaptation (UDA) aims to learn transferable knowledge across the source and
250"
DOMAIN ADAPTION RESULTS,0.6509803921568628,"target domains with different distributions [25] [42]. Recently, transformer-based methods achieved
251"
DOMAIN ADAPTION RESULTS,0.6529411764705882,"SOTA results on UDA, therefore, we evaluate the ViT-B with the positive noise on widely used
252"
DOMAIN ADAPTION RESULTS,0.6549019607843137,"UDA benchmarks. Here the positive noise is the linear transform noise identical to that used in the
253"
DOMAIN ADAPTION RESULTS,0.6568627450980392,"classification task. The positive noise is injected into the last layer of the model, the same as the
254"
DOMAIN ADAPTION RESULTS,0.6588235294117647,"classification task. The datasets include Office Home [41] and VisDA2017 [26]. Detailed datasets
255"
DOMAIN ADAPTION RESULTS,0.6607843137254902,"introduction and experiments training settings are in the supplementary. The objective function
256"
DOMAIN ADAPTION RESULTS,0.6627450980392157,"is borrowed from TVT [44], which is the first work that adopts Transformer-based architecture
257"
DOMAIN ADAPTION RESULTS,0.6647058823529411,"for UDA. The results are shown in Table 5 and 6. The ViT-B with positive noise achieves better
258"
DOMAIN ADAPTION RESULTS,0.6666666666666666,"performance than the existing works. These results show that positive noise can improve model
259"
DOMAIN ADAPTION RESULTS,0.6686274509803921,"generality, therefore, benefit deep models in domain adaptation tasks.
260"
CONCLUSION,0.6705882352941176,"5
Conclusion
261"
CONCLUSION,0.6725490196078432,"This study presents a comprehensive investigation into the influence of various common noise types
262"
CONCLUSION,0.6745098039215687,"on deep learning models, including Gaussian noise, linear transform noise, and salt-and-pepper noise.
263"
CONCLUSION,0.6764705882352942,"We demonstrate that, under certain conditions, linear transform noise can have a positive effect on
264"
CONCLUSION,0.6784313725490196,"deep models. Our experiments show that injecting the positive noise in latent space can significantly
265"
CONCLUSION,0.6803921568627451,"enhance the prediction performance of deep models on image classification tasks, leading to new
266"
CONCLUSION,0.6823529411764706,"state-of-the-art results on ImageNet. The findings of this study have a broad impact on future research
267"
CONCLUSION,0.6843137254901961,"and may contribute to the development of more accurate models and their improved performance in
268"
CONCLUSION,0.6862745098039216,"real-world applications. Moreover, we are excited to explore the potential of positive noise in more
269"
CONCLUSION,0.6882352941176471,"deep learning tasks.
270"
REFERENCES,0.6901960784313725,"References
271"
REFERENCES,0.692156862745098,"[1] Osama K. Al-Shaykh and Russell M. Mersereau. Lossy compression of noisy images. IEEE
272"
REFERENCES,0.6941176470588235,"Transactions on Image Processing, 7(12):1641–1652, 1998.
273"
REFERENCES,0.696078431372549,"[2] Wissam A. Albukhanajer, Johann A. Briffa, and Yaochu Jin. Evolutionary multiobjective image
274"
REFERENCES,0.6980392156862745,"feature extraction in the presence of noise. IEEE Transactions on Cybernetics, 45(9):1757–1768,
275"
REFERENCES,0.7,"2014.
276"
REFERENCES,0.7019607843137254,"[3] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv
277"
REFERENCES,0.703921568627451,"preprint arXiv:2106.08254, 2021.
278"
REFERENCES,0.7058823529411765,"[4] Roberto Benzi, Alfonso Sutera, and Angelo Vulpiani. The mechanism of stochastic resonance.
279"
REFERENCES,0.707843137254902,"Journal of Physics A: mathematical and general, 14(11):L453, 1981.
280"
REFERENCES,0.7098039215686275,"[5] George EP Box and David R. Cox. An analysis of transformations. Journal of the Royal
281"
REFERENCES,0.711764705882353,"Statistical Society: Series B (Methodological), 26(2):211–243, 1964.
282"
REFERENCES,0.7137254901960784,"[6] Sebastian Braun, Hannes Gamper, Chandan KA Reddy, and Ivan Tashev. Towards efficient mod-
283"
REFERENCES,0.7156862745098039,"els for real-time deep noise suppression. In ICASSP 2021-2021 IEEE International Conference
284"
REFERENCES,0.7176470588235294,"on Acoustics, Speech and Signal Processing (ICASSP), pages 656–660, 2021.
285"
REFERENCES,0.7196078431372549,"[7] Raymond H. Chan, Chung-Wa Ho, and Mila Nikolova. Salt-and-pepper noise removal by
286"
REFERENCES,0.7215686274509804,"median-type noise detectors and detail-preserving regularization. IEEE Transactions on image
287"
REFERENCES,0.7235294117647059,"processing, 14(10):1479–1485, 2005.
288"
REFERENCES,0.7254901960784313,"[8] Thomas M. Cover. Elements of information theory. John Wiley & Sons, 1999.
289"
REFERENCES,0.7274509803921568,"[9] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent
290"
REFERENCES,0.7294117647058823,"Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv
291"
REFERENCES,0.7313725490196078,"preprint arXiv:2103.10697, 2021.
292"
REFERENCES,0.7333333333333333,"[10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
293"
REFERENCES,0.7352941176470589,"Gilmer, Andreas Steiner, and et al. Scaling vision transformers to 22 billion parameters. arXiv
294"
REFERENCES,0.7372549019607844,"preprint arXiv:2302.05442 (2023), 2023.
295"
REFERENCES,0.7392156862745098,"[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Feifei Li. Imagenet: A large-scale
296"
REFERENCES,0.7411764705882353,"hierarchical image database. In IEEE conference on computer vision and pattern recognition,
297"
REFERENCES,0.7431372549019608,"pages 248–255, 2009.
298"
REFERENCES,0.7450980392156863,"[12] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jindong Wang, and Lu Yuan. Davit: Dual
299"
REFERENCES,0.7470588235294118,"attention vision transformers. In In Computer Vision–ECCV 2022: 17th European Conference,
300"
REFERENCES,0.7490196078431373,"pages 74–92, 2022.
301"
REFERENCES,0.7509803921568627,"[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
302"
REFERENCES,0.7529411764705882,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
303"
REFERENCES,0.7549019607843137,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
304"
REFERENCES,0.7568627450980392,"recognition at scale. In arXiv preprint arXiv:2010.11929, 2020.
305"
REFERENCES,0.7588235294117647,"[14] Changyong Feng, Hongyue Wang, Naiji Lu, Tian Chen, Hua He, Ying Lu, and Xin M. Tu.
306"
REFERENCES,0.7607843137254902,"Log-transformation and its implications for data analysis. Shanghai archives of psychiatry,
307"
REFERENCES,0.7627450980392156,"26(2):105, 2014.
308"
REFERENCES,0.7647058823529411,"[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
309"
REFERENCES,0.7666666666666667,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
310"
REFERENCES,0.7686274509803922,"pages 770–778, 2016.
311"
REFERENCES,0.7705882352941177,"[16] Solomon Kullback and Richard A. Leibler. On information and sufficiency. The annals of
312"
REFERENCES,0.7725490196078432,"mathematical statistics, 22(1):79–86, 1951.
313"
REFERENCES,0.7745098039215687,"[17] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N 7, (7), 2015.
314"
REFERENCES,0.7764705882352941,"[18] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
315"
REFERENCES,0.7784313725490196,"The handbook of brain theory and neural networks, 3361(10), 1995.
316"
REFERENCES,0.7803921568627451,"[19] Xuelong Li. Positive-incentive noise. IEEE Transactions on Neural Networks and Learning
317"
REFERENCES,0.7823529411764706,"Systems, 2022.
318"
REFERENCES,0.7843137254901961,"[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
319"
REFERENCES,0.7862745098039216,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
320"
REFERENCES,0.788235294117647,"of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.
321"
REFERENCES,0.7901960784313725,"[21] Peter McClintock. Can noise actually boost brain power? Physics World, 15(7), 2002.
322"
REFERENCES,0.792156862745098,"[22] Toshio Mori and Shoichi Kai. Noise-induced entrainment and stochastic resonance in human
323"
REFERENCES,0.7941176470588235,"brain waves. Physical review letters, 88(21), 2002.
324"
REFERENCES,0.796078431372549,"[23] Rich Ormiston, Tri Nguyen, Michael Coughlin, Rana X. Adhikari, and Erik Katsavounidis.
325"
REFERENCES,0.7980392156862746,"Noise reduction in gravitational-wave data via deep learning. Physical Review Research,
326"
REFERENCES,0.8,"2(3):033066, 2020.
327"
REFERENCES,0.8019607843137255,"[24] J. S. Owotogbe, T. S. Ibiyemi, and B. A. Adu. A comprehensive review on various types of
328"
REFERENCES,0.803921568627451,"noise in image processing. int. J. Sci. eng. res, 10(10):388–393, 2019.
329"
REFERENCES,0.8058823529411765,"[25] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on
330"
REFERENCES,0.807843137254902,"knowledge and data engineering, 22(10):1345–1359, 2009.
331"
REFERENCES,0.8098039215686275,"[26] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.
332"
REFERENCES,0.8117647058823529,"Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.
333"
REFERENCES,0.8137254901960784,"[27] Lis Kanashiro Pereira, Yuki Taya, and Ichiro Kobayashi. Multi-layer random perturbation
334"
REFERENCES,0.8156862745098039,"training for improving model generalization efficiently. Proceedings of the Fourth BlackboxNLP
335"
REFERENCES,0.8176470588235294,"Workshop on Analyzing and Interpreting Neural Networks for NLP, 2021.
336"
REFERENCES,0.8196078431372549,"[28] Kamiar Radnosrati, Gustaf Hendeby, and Fredrik Gustafsson. Crackling noise. IEEE Transac-
337"
REFERENCES,0.8215686274509804,"tions on Signal Processing, 68:3590–3602, 2020.
338"
REFERENCES,0.8235294117647058,"[29] Fabrizio Russo. A method for estimation and filtering of gaussian noise in images. IEEE
339"
REFERENCES,0.8254901960784313,"Transactions on Instrumentation and Measurement, 52(4):1148–1154, 2003.
340"
REFERENCES,0.8274509803921568,"[30] James P. Sethna, Karin A. Dahmen, and Christopher R. Myers. Crackling noise. Nature,
341"
REFERENCES,0.8294117647058824,"410(6825):242–250, 2001.
342"
REFERENCES,0.8313725490196079,"[31] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
343"
REFERENCES,0.8333333333333334,"algorithms. Cambridge university press, Cambridge, 2014.
344"
REFERENCES,0.8352941176470589,"[32] Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile
345"
REFERENCES,0.8372549019607843,"computing and communications review, 5(1):3–55, 2001.
346"
REFERENCES,0.8392156862745098,"[33] Jan Sijbers, Paul Scheunders, Noel Bonnet, Dirk Van Dyck, and Erik Raman. Quantification and
347"
REFERENCES,0.8411764705882353,"improvement of the signal-to-noise ratio in a magnetic resonance image acquisition procedure.
348"
REFERENCES,0.8431372549019608,"Magnetic resonance imaging, 14(10):1157–1163, 1996.
349"
REFERENCES,0.8450980392156863,"[34] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit,
350"
REFERENCES,0.8470588235294118,"and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision
351"
REFERENCES,0.8490196078431372,"transformers. In arXiv preprint arXiv:2106.10270, 2021.
352"
REFERENCES,0.8509803921568627,"[35] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
353"
REFERENCES,0.8529411764705882,"effectiveness of data in deep learning era. In In Proceedings of the IEEE international conference
354"
REFERENCES,0.8549019607843137,"on computer vision, pages 843–852, 2017.
355"
REFERENCES,0.8568627450980392,"[36] Tao Sun, Cheng Lu, Tianshuo Zhang, and Harbin Ling. Safe self-refinement for transformer-
356"
REFERENCES,0.8588235294117647,"based domain adaptation. Proceedings of the IEEE/CVF Conference on Computer Vision and
357"
REFERENCES,0.8607843137254902,"Pattern Recognition, pages 7191–7200, 2022.
358"
REFERENCES,0.8627450980392157,"[37] Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal
359"
REFERENCES,0.8647058823529412,"Mohd-Yusof. Combating label noise in deep learning using abstention. In arXiv preprint
360"
REFERENCES,0.8666666666666667,"arXiv:1905.10964, 2019.
361"
REFERENCES,0.8686274509803922,"[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
362"
REFERENCES,0.8705882352941177,"Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
363"
REFERENCES,0.8725490196078431,"International conference on machine learning, pages 10347–10357, 2021.
364"
REFERENCES,0.8745098039215686,"[39] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and
365"
REFERENCES,0.8764705882352941,"Yinxiao Li. Maxvit: Multi-axis vision transformer. In In Computer Vision–ECCV 2022: 17th
366"
REFERENCES,0.8784313725490196,"European Conference, pages 459–479, 2022.
367"
REFERENCES,0.8803921568627451,"[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
368"
REFERENCES,0.8823529411764706,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
369"
REFERENCES,0.884313725490196,"processing systems, 2017.
370"
REFERENCES,0.8862745098039215,"[41] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
371"
REFERENCES,0.888235294117647,"Deep hashing network for unsupervised domain adaptation. CVPR, pages 5018–5027, 2017.
372"
REFERENCES,0.8901960784313725,"[42] Ying Wei, Yu Zhang, Junzhou Huang, and Qiang Yang. Transfer learning via learning to transfer.
373"
REFERENCES,0.8921568627450981,"ICML, pages 5085–5094, 2018.
374"
REFERENCES,0.8941176470588236,"[43] Tongkun Xu, Weihua Chen, Fan Wang, Hao Li, and Rong Jin. Cdtrans: Cross-domain trans-
375"
REFERENCES,0.8960784313725491,"former for unsupervised domain adaptation. ICLR, pages 520–530, 2022.
376"
REFERENCES,0.8980392156862745,"[44] Jinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang. Tvt: Transferable vision transformer
377"
REFERENCES,0.9,"for unsupervised domain adaptation. WACV, pages 520–530, 2023.
378"
REFERENCES,0.9019607843137255,"[45] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-
379"
REFERENCES,0.903921568627451,"ers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
380"
REFERENCES,0.9058823529411765,"pages 12104–12113, 2022.
381"
REFERENCES,0.907843137254902,"Checklist
382"
REFERENCES,0.9098039215686274,"The checklist follows the references. Please read the checklist guidelines carefully for information on
383"
REFERENCES,0.9117647058823529,"how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
384"
REFERENCES,0.9137254901960784,"[N/A] . You are strongly encouraged to include a justification to your answer, either by referencing
385"
REFERENCES,0.9156862745098039,"the appropriate section of your paper or providing a brief inline description. For example:
386"
REFERENCES,0.9176470588235294,"• Did you include the license to the code and datasets? [Yes] Yes
387"
REFERENCES,0.9196078431372549,"• Did you include the license to the code and datasets? [No] The code and the data are
388"
REFERENCES,0.9215686274509803,"proprietary.
389"
REFERENCES,0.9235294117647059,"• Did you include the license to the code and datasets? [N/A]
390"
REFERENCES,0.9254901960784314,"Please do not modify the questions and only use the provided macros for your answers. Note that the
391"
REFERENCES,0.9274509803921569,"Checklist section does not count towards the page limit. In your paper, please delete this instructions
392"
REFERENCES,0.9294117647058824,"block and only keep the Checklist section heading above along with the questions/answers below.
393"
REFERENCES,0.9313725490196079,"1. For all authors...
394"
REFERENCES,0.9333333333333333,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
395"
REFERENCES,0.9352941176470588,"contributions and scope? [TODO] Yes
396"
REFERENCES,0.9372549019607843,"(b) Did you describe the limitations of your work? [TODO] Yes
397"
REFERENCES,0.9392156862745098,"(c) Did you discuss any potential negative societal impacts of your work? [TODO] No
398"
REFERENCES,0.9411764705882353,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
399"
REFERENCES,0.9431372549019608,"them? [TODO] Yes
400"
REFERENCES,0.9450980392156862,"2. If you are including theoretical results...
401"
REFERENCES,0.9470588235294117,"(a) Did you state the full set of assumptions of all theoretical results? [TODO] Yes
402"
REFERENCES,0.9490196078431372,"(b) Did you include complete proofs of all theoretical results? [TODO] Yes
403"
REFERENCES,0.9509803921568627,"3. If you ran experiments...
404"
REFERENCES,0.9529411764705882,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
405"
REFERENCES,0.9549019607843138,"mental results (either in the supplemental material or as a URL)? [TODO] Yes
406"
REFERENCES,0.9568627450980393,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
407"
REFERENCES,0.9588235294117647,"were chosen)? [TODO] Yes
408"
REFERENCES,0.9607843137254902,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
409"
REFERENCES,0.9627450980392157,"ments multiple times)? [TODO] No
410"
REFERENCES,0.9647058823529412,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
411"
REFERENCES,0.9666666666666667,"of GPUs, internal cluster, or cloud provider)? No [TODO]
412"
REFERENCES,0.9686274509803922,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
413"
REFERENCES,0.9705882352941176,"(a) If your work uses existing assets, did you cite the creators? [TODO] N/A
414"
REFERENCES,0.9725490196078431,"(b) Did you mention the license of the assets? [TODO] N/A
415"
REFERENCES,0.9745098039215686,"(c) Did you include any new assets either in the supplemental material or as a URL?
416"
REFERENCES,0.9764705882352941,"[TODO] N/A
417"
REFERENCES,0.9784313725490196,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
418"
REFERENCES,0.9803921568627451,"using/curating? [TODO] N/A
419"
REFERENCES,0.9823529411764705,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
420"
REFERENCES,0.984313725490196,"information or offensive content? [TODO] N/A
421"
REFERENCES,0.9862745098039216,"5. If you used crowdsourcing or conducted research with human subjects...
422"
REFERENCES,0.9882352941176471,"(a) Did you include the full text of instructions given to participants and screenshots, if
423"
REFERENCES,0.9901960784313726,"applicable? [TODO] N/A
424"
REFERENCES,0.9921568627450981,"(b) Did you describe any potential participant risks, with links to Institutional Review
425"
REFERENCES,0.9941176470588236,"Board (IRB) approvals, if applicable? [TODO] N/A
426"
REFERENCES,0.996078431372549,"(c) Did you include the estimated hourly wage paid to participants and the total amount
427"
REFERENCES,0.9980392156862745,"spent on participant compensation? [TODO] N/A
428"
