Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017574692442882249,"The deep policy gradient method has demonstrated promising results in many large-
scale games, where the agent learns purely from its own experience. Yet, policy
gradient methods with self-play suffer convergence problems to a Nash Equilibrium
(NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a
convergence guarantee to a NE in 2-player zero-sum games, but it usually needs
domain-speciﬁc abstractions to deal with large-scale games. Inheriting merits from
both methods, in this paper we extend the actor-critic algorithm framework in
deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-
information game, 1-on-1 Mahjong, whose information set size and game length
are much larger than poker. The proposed algorithm, named Actor-Critic Hedge
(ACH), modiﬁes the policy optimization objective from originally maximizing the
discounted returns to minimizing a type of weighted cumulative counterfactual
regret. This modiﬁcation is achieved by approximating the regret via a deep neural
network and minimizing the regret via generating self-play policies using Hedge.
ACH is theoretically justiﬁed as it is derived from a neural-based weighted CFR, for
which we prove the convergence to a NE under certain conditions. Experimental
results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the
literature demonstrate that ACH outperforms related state-of-the-art methods. Also,
the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong."
INTRODUCTION,0.0035149384885764497,"1
INTRODUCTION"
INTRODUCTION,0.005272407732864675,"Policy gradient methods using deep neural networks as policy and value approximators have been
successfully applied to many large-scale games (Berner et al., 2019; Vinyals et al., 2019; Ye et al.,
2020). Usually, a score function representing the discounted returns is maximized by the policy, i.e.,
the actor. In the meantime, a value function, known as the critic, is learned to guide the directions
and magnitudes of the policy gradients. This type of actor-critic methods are efﬁciently scalable
with regard to the game size and the amount of computational resources. However, as pointed out
in Srinivasan et al. (2018) and Hennes et al. (2020), policy gradient methods with self-play have no
convergence guarantee to optimal solutions in competitive Imperfect-Information Games (IIGs). The
main reason is that the policy gradient theorem (Sutton et al., 1999) is established within the single
agent situation, where the environment is Markovian. However, learning becomes non-stationary and
non-Markovian when multiple agents learn simultaneously in a competitive environment."
INTRODUCTION,0.007029876977152899,"An optimal solution to a 2-player zero-sum IIG usually refers to a Nash Equilibrium (NE), where
no player could improve by unilaterally deviating to a different policy. Tremendous progress in
computing NE solutions has been made by a family of tabular methods: Counterfactual regret"
INTRODUCTION,0.008787346221441126,"∗Equal contribution. Correspondence to: Haobo Fu (haobofu@tencent.com).
†Work done while internship at Tencent AI Lab, Shenzhen, China."
INTRODUCTION,0.01054481546572935,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012302284710017574,"minimization (CFR) (Zinkevich et al., 2008). CFR is a type of iterative self-play algorithm based
on regret minimization, and it guarantees to converge to a NE with regard to the average policy in
2-player zero-sum IIGs. A perfect game model is required in CFR to sample many if not all actions
from a state. To handle large-scale IIGs with CFR, abstractions (applied to either the action space or
the state space) are usually employed to reduce the game to a manageable size (Moravˇcík et al., 2017;
Brown & Sandholm, 2018; 2019)1. However, abstractions are domain speciﬁc (Waugh et al., 2009;
Johanson et al., 2013; Ganzfried & Sandholm, 2014). More importantly, some large-scale IIGs are
inherently difﬁcult to be abstracted, such as the game of Mahjong (Li et al., 2020b)."
INTRODUCTION,0.014059753954305799,"In this paper, we investigate a large-scale IIG, i.e., 2-player (1-on-1) zero-sum Mahjong, whose
information set size and game length are much larger than poker2. Li et al. (2020b) has recently de-
veloped a strong 4-player Mahjong agent based on supervised learning and traditional Reinforcement
Learning (RL). In comparison, we study 1-on-1 Mahjong from a game-theoretic perspective, i.e.,
aiming for a NE. We are interested in methods using only trajectory samples to learn, as it is infeasible
to consistently sample multiple actions for each state in large-scale IIGs with long episodes. We
employ deep neural networks to generalize across states, since the state abstraction in 1-on-1 Mahjong
is inherently difﬁcult, as explained in the Appendix A.2. We make the following contributions."
INTRODUCTION,0.015817223198594025,"• Inheriting the scalability of deep RL methods and the convergence property of CFR, we develop a
new actor-critic algorithm, named Actor-Critic Hedge (ACH), for approaching a NE in large-scale
2-player zero-sum IIGs. ACH employs a deep neural network to approximate a type of weighted
cumulative counterfactual regret. In the meantime, ACH minimizes the regret via generating
self-play policies using Hedge (Freund & Schapire, 1997)."
INTRODUCTION,0.01757469244288225,"• We introduce a Neural-based Weighted CFR (NW-CFR), of which ACH is a practical implementa-
tion. We prove that the exploitability of the average policy in NW-CFR decreases at the rate of
O(T −1/2) under certain conditions, where T denotes the number of iterations in NW-CFR."
INTRODUCTION,0.019332161687170474,"• To facilitate research on large-scale 2-player zero-sum IIGs, we propose a 1-on-1 Mahjong bench-
mark. The corresponding game enjoys a large population in online games."
INTRODUCTION,0.0210896309314587,"• We build a 1-on-1 Mahjong agent, named JueJong, based on ACH. In an initial evaluation against
human players including a Mahjong champion3, JueJong demonstrates superior performance."
NOTATIONS AND BACKGROUND,0.022847100175746926,"2
NOTATIONS AND BACKGROUND"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.02460456942003515,"2.1
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.026362038664323375,"An IIG is usually described in an extensive-form game tree. A node (history) h ∈H in the tree
represents all information of the current situation. For each history h, there is a player p ∈P or a
chance player c that should act at h. Deﬁne P : H →P ∪{c}. When P(h) ∈P, the player P(h) has
to take an action a ∈A(h), and A(h) is the set of legal actions in h. The chance player is responsible
for taking actions for random events. The set of terminal nodes is denoted by Z. For each player
p ∈P, there is a payoff function deﬁned on the set of terminal nodes, up : Z →R. In this paper, we
focus on 2-player zero-sum games, where P = {0, 1} and u0(z) + u1(z) = 0 for each z ∈Z."
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.028119507908611598,"For either player p, the set of histories H is partitioned into information sets (infosets). We denote the
set of infosets for player p by Ip and an infoset in Ip by Ip. Two histories h, h′ ∈H are in the same
infoset if and only if h and h′ are indistinguishable from the perspective of player p. Hence, a player
p’s policy πp is deﬁned as a function that maps an infoset to a probability distribution over legal
actions. We further deﬁne A(Ip) = A(h) and P(Ip) = P(h) for any h ∈Ip. A policy proﬁle π is a
tuple of policies (πp, π−p), where π−p represents the player p’s opponent policy. The expected payoff
for player p under π is denoted by up(πp, π−p). We use ∆(I) to denote the range of payoffs reachable
from a history h in infoset I. Let ∆= maxI∈Ip,p∈P ∆(I). f π(h) denotes the joint probability of
reaching h under π. f π
p (h) is the contribution of player p to f π(h), and f π
−p(h) is the contribution of
the opponent and chance: f π(h) = f π
p (h)f π
−p(h). We focus on the perfect-recall setting, where each"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.029876977152899824,"1DeepStack (Moravˇcík et al., 2017) employs sparse lookahead trees, much like the action abstraction.
2Without speciﬁcation, we mean Heads-up No-Limit Texas Hold’em in this paper.
3Haihua Cheng is the Competition Mahjong champion of 2014 World Mahjong Master Tournament, 2018
Tencent Mahjong Tournament, and 2019 Tencent Mahjong Tournament."
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.03163444639718805,Published as a conference paper at ICLR 2022
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.033391915641476276,"player recalls the sequence of their own infosets reached. We deﬁne f π
p (Ip) = f π
p (h), ∀h ∈Ip and
f π
−p(Ip) = P"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.0351493848857645,"h∈Ip f π
−p(h). Hence, f π(Ip) = P"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.03690685413005272,"h∈Ip f π(h) = f π
p (Ip)f π
−p(Ip)."
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.03866432337434095,"A best response BR(π−p) to π−p is a player p’s policy that satisﬁes up(BR(π−p), π−p) =
maxπ′p up(π′
p, π−p). A NE is a policy proﬁle π∗, where each player plays a best response to
the other: up(π∗
p, π∗
−p) = maxπ′p up(π′
p, π∗
−p), ∀p ∈P. The exploitability of a player’s pol-
icy, denoted by e(πp), measures the performance gap between πp and a NE policy π∗
p: e(πp) =
up(π∗
p, π∗
−p) −up(πp, BR(πp)). The exploitability of π is ϵ(π) =
1
|P|
P"
IMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM,0.040421792618629174,p∈P e(πp).
REINFORCEMENT LEARNING AND POLICY GRADIENT METHODS,0.0421792618629174,"2.2
REINFORCEMENT LEARNING AND POLICY GRADIENT METHODS"
REINFORCEMENT LEARNING AND POLICY GRADIENT METHODS,0.043936731107205626,"RL usually assumes a Markov decision process, where the agent selects an action ai from the
legal action set A(si) in state si ∈S4 at each time step i. The agent then receives a reward ri
from the environment and transitions to a new state si+1. The objective in RL is to learn a policy
that maximizes the expected discounted returns, i.e., the state value, starting from any state s:
V π(s) = Eπ[P∞
j=i γj−irj|si = s] = Eπ[G], with the discount factor γ ∈[0, 1)."
REINFORCEMENT LEARNING AND POLICY GRADIENT METHODS,0.04569420035149385,"Many methods in RL belong to policy gradient methods, in which the parameters θ of policy π(a|s; θ)
are updated by performing gradient ascent directly on Eπθ[G]. One early example is the standard
REINFORCE algorithm (Williams, 1992) that updates θ in the direction ∇θ log π(a|s; θ)G, which
is an unbiased estimate of ∇θEπθ[G]. To further reduce the variance of the gradient, an action-
independent baseline is often subtracted from G: ∇θ log π(a|s; θ)[G −B(s)]. A recent policy
gradient method, named Advantage Actor-Critic (A2C) (Mnih et al., 2016), learns a parameterized
value function as the baseline: B(s) = V (s; w), where w often shares some parameters with θ.
Moreover, the value G −B(s) is replaced in A2C with the estimated advantage of action a in state
s: A(s, a) = Q(s, a) −V (s). The Q value Q(s, a) is usually estimated using sampled rewards and
predicted values of future states. A2C updates the parameters θ and w in a synchronous manner."
REINFORCEMENT LEARNING AND POLICY GRADIENT METHODS,0.04745166959578207,"In an asynchronous training environment, the behavioral policy is usually different from the learning
policy. Proximal Policy Optimization (PPO) (Schulman et al., 2017) takes this discrepancy into
account by multiplying A2C gradient with an importance ratio r(a|s; θ) = π(a|s; θ)/π(a|s; θold),
which results in r(a|s; θ)∇θ log π(a|s; θ)A(s, a) = ∇θr(a|s; θ)A(s, a). Furthermore, PPO con-
strains the KL divergence between the learning policy πθ and the old behavioral policy πθold by
clipping the ratio to a small interval around 1.0."
COUNTERFACTUAL REGRET MINIMIZATION,0.0492091388400703,"2.3
COUNTERFACTUAL REGRET MINIMIZATION"
COUNTERFACTUAL REGRET MINIMIZATION,0.050966608084358524,"CFR is an iterative algorithm that minimizes the total regret of policy by minimizing the cumulative
counterfactual regret in every state (infoset). The cumulative counterfactual regret of an action
a in state s is deﬁned as Rc
t(s, a) = Pt
k=1 rc
k(s, a), where rc
k(s, a) denotes the instantaneous
counterfactual regret of action a in state s at iteration k. rc
t(s, a) is equal to f πt
−p(s)Aπt(s, a) (as
proven in Srinivasan et al. (2018)), where Aπt(s, a) is the advantage function of player p = P(s) at
state s as deﬁned in traditional RL. Intuitively, rc
t(s, a) represents how regretful the player p is that
he does not select the action a in state s at iteration t. The term f πt
−p(s) = P"
COUNTERFACTUAL REGRET MINIMIZATION,0.05272407732864675,"h∈s f πt
−p(h) is needed
here to reﬂect the fact that reaching state s is also controlled by the opponent −p and chance. To
minimize the regret, a regret minimizer is utilized in each state at each iteration, generating a series of
local policies π1(s), . . . , πt(s). Once πt is obtained, rc
t(s, a) is generated by a game tree traversing
using πt, and Rc
t(s, a) is updated accordingly: Rc
t(s, a) = Rc
t−1(s, a) + rc
t(s, a)."
COUNTERFACTUAL REGRET MINIMIZATION,0.054481546572934976,"For either player p, deﬁne the total regret as RT = maxπ′p
PT
t=1
 
up(π′
p, π−p,t) −up(πp,t, π−p,t)

.
It is proven in Zinkevich et al. (2008) that RT ≤P"
COUNTERFACTUAL REGRET MINIMIZATION,0.056239015817223195,"s∈S[Rc
T (s)]+, where [·]+ = max{·, 0}
and Rc
T (s) = maxa∈A(s) Rc
T (s, a). As a result, the total regret can be minimized by minimiz-
ing the cumulative counterfactual regret at each state. Moreover, the average policy ¯πT (a|s) =
P"
COUNTERFACTUAL REGRET MINIMIZATION,0.05799648506151142,"t∈T f πt
p (s)πt(a|s)/P"
COUNTERFACTUAL REGRET MINIMIZATION,0.05975395430579965,"t∈T f πt
p (s) for both players converges to a NE if RT for both players grows
sub-linearly with T (Zinkevich et al., 2008). There are two commonly used regret minimizers: Regret
Matching (RM) (Hart & Mas-Colell, 2000) and Hedge (Cesa-Bianchi & Lugosi, 2006). In RM, a"
COUNTERFACTUAL REGRET MINIMIZATION,0.061511423550087874,4We use state s and infoset Ip interchangeably in this paper.
COUNTERFACTUAL REGRET MINIMIZATION,0.0632688927943761,Published as a conference paper at ICLR 2022
COUNTERFACTUAL REGRET MINIMIZATION,0.06502636203866433,"player selects an action with probability in proportion to its positive cumulative counterfactual regret.
In Hedge, the policy πt+1(a|s) is decided according to:"
COUNTERFACTUAL REGRET MINIMIZATION,0.06678383128295255,"πt+1(a|s) =
eη(s)Rc
t(s,a)
P"
COUNTERFACTUAL REGRET MINIMIZATION,0.06854130052724078,"a′ eη(s)Rc
t(s,a′) .
(1)"
COUNTERFACTUAL REGRET MINIMIZATION,0.070298769771529,"If the player plays according to Hedge in state s and η(s) =
p"
COUNTERFACTUAL REGRET MINIMIZATION,0.07205623901581722,"8 log(|A(s)|)/(∆2(s)T), Rc
T (s) ≤
∆(s)
p"
COUNTERFACTUAL REGRET MINIMIZATION,0.07381370826010544,"log(|A(s)|)T/2 (Cesa-Bianchi & Lugosi, 2006). In other words, the total regret grows
sub-linearly with T, and therefore the average policy ¯πT (a|s) for both players converges to a NE."
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.07557117750439367,"3
THE MOTIVATION OF ACTOR-CRITIC HEDGE"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.0773286467486819,"In this section, we motivate ACH by introducing a new neural-based CFR algorithm, NW-CFR, which
employs a neural network to generalize across states and relies on only trajectory samples for training.
The key idea in NW-CFR is that a neural network (called the policy net) is used to approximate the
expectation of the sum of sampled advantages Ra
t (s, a) := E[Pt
k=1 ˜Aπk(s, a)]. ˜Aπk(s, a) is set to
Aπk(s, a) if s is visited at iteration k (given that only one trajectory is sampled at each iteration) and
0 otherwise. As a result, the expectation E[ ˜Aπk(s, a)] depends on both the advantage Aπk(s, a) and
the sampling policy µk at iteration k."
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.07908611599297012,"At the beginning of iteration t, suppose Ra
t−1(s, a) is well approximated by the output y(a|s; θt−1)
of the policy net, parameterized as θt−1 in NW-CFR. The policy πt for iteration t is ﬁrst obtained
via Hedge, i.e., softmaxing on η(s)y(a|s; θt−1). The reason we use Hedge instead of RM is that
softmaxing is shift-invariant, which may be more robust to the function approximation error compared
to the threshold operation in RM. M trajectories are then sampled into a buffer Bv via self-play using
the policy proﬁle πt = (πp,t, π−p,t). Those samples in Bv are used to train the action value net,
parameterized as ω, by minimizing the squared loss: 1"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.08084358523725835,"2[Q(s, a; ω) −G]2, where G is the sampled
return. Afterwards, another M trajectories are sampled into a buffer Bπ via self-play using another
sampling policy proﬁle µt = (µp,t, π−p,t). We use an additional behavioral policy µp,t for the player
p for more ﬂexibility in the convergence (more details are provided in the next section). The policy θ
is then optimized according to the loss function Lπ = P"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.08260105448154657,"s∈S Lπ(s), where"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.0843585237258348,Lπ(s) = (P
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.08611599297012303,"a{y(a|s; θ) −[y(a|s; θt−1) +
1
M
PM
i=1 1s∈τiAπt(s, a)]}2
if s ∈Bπ,
P"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.08787346221441125,"a{y(a|s; θ) −[y(a|s; θt−1) + 0]}2
otherwise,
(2)"
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.08963093145869948,"where τi is the ith sampled trajectory in Bπ. The next iteration of NW-CFR begins after the policy
training ﬁnishes. The pseudocode of NW-CFR is given in Algorithm 1. Note that we present NW-CFR
from the perspective of player p, and the same procedure applies to the player −p. NW-CFR runs
concurrently for both players and synchronizes at the beginning of each iteration."
THE MOTIVATION OF ACTOR-CRITIC HEDGE,0.0913884007029877,"A potential beneﬁt of training on the sampled advantage over the sampled instantaneous coun-
terfactual regret (as done in Brown et al. (2019), Li et al. (2020a), and Steinberger et al.
(2020)) is that the variance of the sampled advantage could be much lower.
As rc
k(s, a) =
f πk
−p(s)Aπk(s, a) (Srinivasan et al., 2018), the sampled instantaneous counterfactual regret ˜rc
k(s, a)
is [f µk(s)]−1f πk
−p(s) ˜Aπk(s, a) = [f µk
p (s)]−1 ˜Aπk(s, a), where f µk
p (s) is the probability of reaching
s considering only the contribution of µp,k. The larger variance (due to [f µk
p (s)]−1) of ˜rc
k(s, a)
may have a negative inﬂuence on the performance when function approximation is used with only
trajectory samples. This inﬂuence is magniﬁed in large-scale games, as [f µk
p (s)]−1 becomes large."
THEORETICAL PROPERTIES OF NW-CFR,0.09314586994727592,"4
THEORETICAL PROPERTIES OF NW-CFR"
THEORETICAL PROPERTIES OF NW-CFR,0.09490333919156414,"In this section, we prove the convergence of NW-CFR to an approximate NE. To this end, we
ﬁrst deduce that the policy target in NW-CFR is Ra
t (s, a), which is a type of weighted cumulative
counterfactual regret. We then deﬁne a new family of CFR algorithms: weighted CFR. Finally, we
show that NW-CFR is equivalent to a type of weighted CFR with Hedge under certain conditions. The
convergence properties of weighted CFR with Hedge and hence NW-CFR are proven accordingly."
THEORETICAL PROPERTIES OF NW-CFR,0.09666080843585237,"We can rewrite the NW-CFR policy loss (in Equation 2) at iteration t as Lπ = P s∈S
P"
THEORETICAL PROPERTIES OF NW-CFR,0.0984182776801406,"a{y(a|s; θ)−
[y(a|s; θt−1) +
1
M
PM
i=1 1s∈τiAπt(s, a)]}2. In other words, the target of y(a|s; θ) at iteration t is"
THEORETICAL PROPERTIES OF NW-CFR,0.10017574692442882,Published as a conference paper at ICLR 2022
THEORETICAL PROPERTIES OF NW-CFR,0.10193321616871705,"Algorithm 1: Neural-based Weighted CFR
Function Neural-based Weighted CFR(p, T, M, θ, ω):"
THEORETICAL PROPERTIES OF NW-CFR,0.10369068541300527,"ω0 ←ω, θ0 ←θ.
for t ←1 to T do"
THEORETICAL PROPERTIES OF NW-CFR,0.1054481546572935,"# Obtain the policy: πt ←Softmax(η(s)y(a|s; θt−1)).
# Train the action value net:
Reset Bv ←∅.
for i ←1 to M do"
THEORETICAL PROPERTIES OF NW-CFR,0.10720562390158173,"τi ∼SelfPlay(πp,t, π−p,t), Bv ←Bv ∪τi
Train ω on the loss E(s,a,G)∼Bv{ 1"
THEORETICAL PROPERTIES OF NW-CFR,0.10896309314586995,"2[Q(s, a; ω) −G]2}
ωt ←ω
# Train the policy net:
Reset Bπ ←∅.
for i ←1 to M do"
THEORETICAL PROPERTIES OF NW-CFR,0.11072056239015818,"τi ∼SelfPlay(µp,t, π−p,t), Bπ ←Bπ ∪τi
Estimate Aπt(s, a), ∀a ∈A(s), ∀s ∈Bπ as Q(s, a; ωt) −P"
THEORETICAL PROPERTIES OF NW-CFR,0.11247803163444639,"b πt(b|s)Q(s, b; ωt).
Sum aggregate advantage Aπt(s, a) in Bπ by state s.
Train θ on the loss Es∼S{Lπ(s)}, where Lπ(s) is deﬁned in Equation 2.
θt ←θ. # Save θt to a policy buffer."
THEORETICAL PROPERTIES OF NW-CFR,0.11423550087873462,"y(a|s; θt−1) +
1
M
PM
i=1 1s∈τiAπt(s, a), whose expectation is"
THEORETICAL PROPERTIES OF NW-CFR,0.11599297012302284,"Eτk,i∼(µp,k,π−p,k) ""
t
X k=1 M
X i=1"
THEORETICAL PROPERTIES OF NW-CFR,0.11775043936731107,"1
M 1s∈τk,iAπk(s, a) #"
THEORETICAL PROPERTIES OF NW-CFR,0.1195079086115993,"= Eτk∼(µp,k,π−p,k) ""
t
X"
THEORETICAL PROPERTIES OF NW-CFR,0.12126537785588752,"k=1
˜Aπk(s, a) #"
THEORETICAL PROPERTIES OF NW-CFR,0.12302284710017575,"= Ra
t (s, a)."
THEORETICAL PROPERTIES OF NW-CFR,0.12478031634446397,"Also, Eτk∼(µp,k,π−p,k)1s∈τkAπk(s, a) = f µk
p (s)f πk
−p(s)Aπk(s, a), where f µk
p (s)f πk
−p(s) is the reach-
ing probability of state s at iteration k. Therefore, Ra
t (s, a) = Pt
k=1 f µk
p (s)f πk
−p(s)Aπk(s, a). Since
rc
k(s, a) = f πk
−p(s)Aπk(s, a) (Srinivasan et al., 2018), we have Ra
t (s, a) = Pt
k=1 f µk
p (s)rc
k(s, a)."
THEORETICAL PROPERTIES OF NW-CFR,0.1265377855887522,"As a result, Ra
t (s, a) can be viewed as a type of weighted cumulative counterfactual regret that
multiples each instantaneous counterfactual regret with f µk
p (s). Without loss of generality, we deﬁne
a new family of CFR algorithms, weighted CFR, below. Afterwards, we present Theorem 1."
THEORETICAL PROPERTIES OF NW-CFR,0.1282952548330404,"Deﬁnition 1. Weighted CFR follows the same procedure as the original CFR (Zinkevich et al.,
2008), except that the instantaneous counterfactual regret rc
t(s, a) is weighted by some weight wt(s),
wt(s) > 0 and P∞
t=0 wt(s) = ∞. The original CFR is a type of weighted CFR with wt(s) = 1.0."
THEORETICAL PROPERTIES OF NW-CFR,0.13005272407732865,"Theorem 1. NW-CFR is equivalent to a type of weighted CFR with Hedge when wt(s) = f µt
p (s) > 0,
given that enough trajectories are sampled and y(a|s; θt) is sufﬁciently close to Ra
t (s, a). Further,
if η(s) =
p"
THEORETICAL PROPERTIES OF NW-CFR,0.13181019332161686,"8 ln |A(s)|/{[wh(s)]2∆2(s)T} and wt(s) = f µt
p (s) ∈[wl(s), wh(s)] ⊂(0, 1], t =
1, . . . , T, the average policy5 π of the corresponding weighted CFR with Hedge and equivalently
NW-CFR with πp(a|s) = PT
t=1

f πt
p (s)πt(a|s)

/PT
t=1 f πt
p (s), ∀p ∈P, has ϵ exploitability after T
iterations, where"
THEORETICAL PROPERTIES OF NW-CFR,0.1335676625659051,ϵ ≤|S|∆ r
THEORETICAL PROPERTIES OF NW-CFR,0.13532513181019332,"1
2T ln |A| + ∆
X s∈S"
THEORETICAL PROPERTIES OF NW-CFR,0.13708260105448156,wh(s) −wl(s)
THEORETICAL PROPERTIES OF NW-CFR,0.13884007029876977,"wh(s)
.
(3)"
THEORETICAL PROPERTIES OF NW-CFR,0.140597539543058,"In Theorem 1, we proved that the exploitability ϵ of NW-CFR is bounded by Equation 3 when
y(a|s; θt) is sufﬁciently close to Ra
t (s, a). There are two terms in the bound. The ﬁrst term converges
to zero at the rate of O(T −1/2), and the second term is O(1), which is weighted by the sum of the
normalized range of the weights P"
THEORETICAL PROPERTIES OF NW-CFR,0.14235500878734622,"s∈S (wh(s) −wl(s))/wh(s). However, it is possible to reduce the
second term to an arbitrarily small value via tightening the range of f µt
p (s), which is experimentally
demonstrated in the Appendix D."
THEORETICAL PROPERTIES OF NW-CFR,0.14411247803163443,"5Given πp,t at each iteration, we could obtain πp using the techniques introduced in Steinberger (2019)."
THEORETICAL PROPERTIES OF NW-CFR,0.14586994727592267,Published as a conference paper at ICLR 2022
THEORETICAL PROPERTIES OF NW-CFR,0.14762741652021089,"Corollary 1. If the behavioral policy µp,t for each player p ∈P is constant across iterations, and
f µt
p (s) > 0, ∀s ∈S, t > 0, NW-CFR is equivalent to CFR with Hedge when y(a|s; θt) is sufﬁciently
close to Ra
t (s, a)."
THEORETICAL PROPERTIES OF NW-CFR,0.14938488576449913,"As shown in Corollary 1, when the behavioral policy µp,t for each player p ∈P is time-invariant,
i.e., wh(s) = f µt
p (s) = wl(s), ∀s ∈S, t > 0, the second term of ϵ in Equation 3 vanishes, and CFR
with Hedge is recovered. All the proofs are given in the Appendix C."
THEORETICAL PROPERTIES OF NW-CFR,0.15114235500878734,"5
ACH: A PRACTICAL IMPLEMENTATION OF NW-CFR"
THEORETICAL PROPERTIES OF NW-CFR,0.15289982425307558,"When applying NW-CFR to large-scale problems, two practical issues need to be addressed:"
THEORETICAL PROPERTIES OF NW-CFR,0.1546572934973638,"The average policy. Theorem 1 and Corollary 1 state the convergence property of the average
policy in NW-CFR. Yet, as pointed out in Srinivasan et al. (2018), Hennes et al. (2020), and Perolat
et al. (2021), obtaining the average policy with deep neural nets in large-scale games is inherently
difﬁcult, due to either the computation or the memory demand. Alternatively, we could employ some
additional technique to hopefully induce the current policy convergence towards a NE. Srinivasan
et al. (2018) and Hennes et al. (2020) handled this by adding an entropy regularization to the current
policy training, which is, to some extent, theoretically justiﬁed later in Perolat et al. (2021)."
THEORETICAL PROPERTIES OF NW-CFR,0.15641476274165203,"Training on states not sampled. Theoretically, in order to optimize Equation 2, we need to collect
both sampled and non-sampled states. Optimizing with only sampled states makes y(a|s; θt) a biased
estimation of Ra
t (s, a). Yet, collecting non-sampled states may be intractable in large-scale games
(Li et al., 2020a) or in situations where a perfect environment model is not available."
THEORETICAL PROPERTIES OF NW-CFR,0.15817223198594024,"To strike a balance between theoretical soundness and practical efﬁciency, we provide a practical
implementation of NW-CFR, which is ACH. ACH adapts NW-CFR by training the current policy
with an entropy regularization on only sampled states, without the calculation of the average policy.
In order to utilize distributed clusters, ACH employs a framework of decoupled acting and learning
(similar to IMPALA (Espeholt et al., 2018)), trains the network with mini-batches, and handles
asynchronous training with the importance ratio clipping of PPO. The behavior policy µp,t is set to
πp,t6 in ACH. More details of ACH are presented in the Appendix E."
RELATED WORK,0.15992970123022848,"6
RELATED WORK"
RELATED WORK,0.1616871704745167,"To obviate the need of abstractions, various neural forms of CFR methods have been developed. An
early work of this direction is regression CFR (Waugh et al., 2015), which calculates weights for a
number of hand-crafted features to approximate the regret. Deep CFR (Brown et al., 2019) is similar
to regression CFR but employs a neural network to approximate the regret. Also, deep CFR traverses
a part of the game tree using external sampling, in comparison to the full traversal of the game tree
in regression CFR. Double neural CFR (Li et al., 2020a) is another method that approximates the
regret and the average policy using deep neural networks, where a novel robust sampling technique is
developed. Both deep CFR and double neural CFR build on the tabular Monte Carlo CFR (MCCFR)
(Lanctot et al., 2009), where either outcome sampling (sampling one action in a state) or external
sampling (sampling all actions in a state) could be employed."
RELATED WORK,0.1634446397188049,"When dealing with games with long episodes, a necessity may be that only trajectory samples are
allowed. To improve the learning performance with only trajectory samples, DREAM (Steinberger
et al., 2020) adapts deep CFR by using a learned Q-baseline, which is inspired by the variance
reduction techniques in tabular MCCFR (Schmid et al., 2019; Davis et al., 2020). Another recent
work using only trajectory samples is ARMAC (Gruslys et al., 2020). By replaying through past
policies and using a history-based critic, ARMAC predicts conditional advantages, based on which
the policy for each iteration is generated. Other popular neural network based methods, which learn
from trajectory samples and are inspired by game-theoretic approaches other than CFR, include neural
ﬁctitious self-play (Heinrich & Silver, 2016), policy space response oracles (Lanctot et al., 2017),
and exploitability descent (Lockhart et al., 2019), all of which require to compute an approximate
best response at each iteration. Such computation may be prohibitive in large-scale games."
RELATED WORK,0.16520210896309315,"6In a preliminary experiment presented in the Appendix F, we ﬁnd that the performance of the current policy
in ACH (trained with an entropy regularization) is not sensitive to the choice of µp,t."
RELATED WORK,0.16695957820738136,Published as a conference paper at ICLR 2022
RELATED WORK,0.1687170474516696,"The most related methods to ACH are Regret Policy Gradient (RPG) (Srinivasan et al., 2018)
and Neural Replicator Dynamics (NeuRD) (Hennes et al., 2020), both of which employ the
actor-critic framework and thus have similar computation and memory complexities as ACH.
RPG minimizes a loss that is an upper bound on the regret after threshold, and the correspond-
ing policy gradient is ∇RP G
θ
(s) = −P"
RELATED WORK,0.1704745166959578,"a ∇θ[Q(s, a; w) −P"
RELATED WORK,0.17223198594024605,"b π(b|s; θ)Q(s, b; w)]+. However,
RPG requires an l2 projection after every gradient step for the convergence to a NE, while such
projection is not required in ACH. NeuRD is inspired by the replicator dynamics, a well stud-
ied model in evolutionary game theory (Gatti et al., 2013). The policy gradient in NeuRD is
∇NeuRD
θ
(s) = P
a[∇θy(a|s; θ)][Q(s, a; w) −P
b π(b|s)Q(s, b; w)], where y(a|s; θ) is the output
of the policy net. There are important differences between ACH and NeuRD in how the algorithm is
motivated and how the policy net at each iteration is optimized. Also, the convergence analysis is
given only for the single-state all-actions tabular NeuRD (Hennes et al., 2020). Yet, we prove the
convergence of NW-CFR, of which ACH is a practical implementation, in full extensive-form games."
EXPERIMENTAL STUDIES,0.17398945518453426,"7
EXPERIMENTAL STUDIES"
EXPERIMENTAL STUDIES,0.1757469244288225,"We ﬁrstly introduce a 1-on-1 Mahjong benchmark, on which we compare ACH with related state-
of-the-art methods of similar computation complexity: PPO, RPG, and NeuRD. Since our goal is
to approximate a NE, the standard and default performance metric, exploitability, is employed. We
approximate a lower bound on the exploitability of an agent by training a best response against it as
suggested in Timbers et al. (2020) and Steinberger et al. (2020), because traversing the full game tree
to compute the exact exploitability is intractable in such a large-scale game as 1-on-1 Mahjong. As a
complement, head-to-head performance of different methods on 1-on-1 Mahjong is also presented.
Moreover, the agent obtained by ACH is evaluated against practised Mahjong human players. To
further validate the performance of ACH in IIGs other than 1-on-1 Mahjong, experimental results on
a non-trivial poker game, i.e., heads-up Flop Hold’em Poker (FHP) (Brown et al., 2019) are presented.
Deep CFR with outcome sampling (OS-DCFR) and DREAM are added to enable a more thorough
comparison. Additional results on smaller benchmarks from OpenSpiel (Lanctot et al., 2019) are
given in the Appendix G. Note that results are reported for the current policy (of ACH, PPO, RPG,
and NeuRD) and the average policy (of OS-DCFR and DREAM) respectively."
EXPERIMENTAL STUDIES,0.17750439367311072,"7.1
A 2-PLAYER ZERO-SUM MAHJONG BENCHMARK"
EXPERIMENTAL STUDIES,0.17926186291739896,"Mahjong is a tile-based game that is played world wide with many regional variations, such as
Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full of
strategy, chance, and calculation. To facilitate Mahjong research from a game-theoretic perspective,
we propose a 2-player zero-sum Mahjong benchmark, whose game rules are similar to Competition
Mahjong. The corresponding game, “2-player Mahjong Master”, is played by humans in Tencent
mobile games. A full description of the game rules is in the Appendix A.1. Apart from being the ﬁrst
benchmark for the 1-on-1 Mahjong game, our benchmark has a larger infoset size and a longer game
length (the effects are explained in the Appendix A.3), compared with existing poker benchmarks
(Lanctot et al., 2019). The infoset size (i.e., the number of distinct histories in an infoset) in 1-on-1
Mahjong is around 1011, compared to 103 in poker. This is due to the fact that only two private cards
are invisible in poker, while there are 13 invisible tiles in 1-on-1 Mahjong. In addition, players can
decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end
within 10 steps. More details about the 1-on-1 Mahjong benchmark are given in the Appendix A."
EXPERIMENTAL STUDIES,0.18101933216168717,"7.2
RESULTS ON OUR 1-ON-1 MAHJONG BENCHMARK"
EXPERIMENTAL STUDIES,0.1827768014059754,"All methods run in an asynchronous training platform with overall 800 CPUs, 3200 GB memory,
and 8 M40 GPUs in the Ubuntu 16.04 operating system. Each method shares the same neural
network architecture, a full description of which is given in the Appendix B. We performed a mild
hyper-parameter search on PPO and shared the best setting for all methods. The advantage value is
estimated by the Generalized Advantage Estimator (GAE(λ)) (Schulman et al., 2016) for all methods.
An overview of the hyper-parameters is listed in the Appendix H.1."
EXPERIMENTAL STUDIES,0.18453427065026362,"Approximate Lower Bound Exploitability. To approximate a lower bound on the exploitability
of the agents obtained by each method, we train a best response against each agent. The agent of"
EXPERIMENTAL STUDIES,0.18629173989455183,Published as a conference paper at ICLR 2022
EXPERIMENTAL STUDIES,0.18804920913884007,"0.0
0.2
0.4
0.6
0.8
1.0
Training Steps
1e6 6 4 2 0 2 4 6"
EXPERIMENTAL STUDIES,0.18980667838312829,Average Scores Lost
EXPERIMENTAL STUDIES,0.19156414762741653,"PPO
RPG
NeuRD
ACH (Ours)"
EXPERIMENTAL STUDIES,0.19332161687170474,(a) Approximate Lower Bound Exploitability
EXPERIMENTAL STUDIES,0.19507908611599298,"0.0
0.2
0.4
0.6
0.8
1.0
Training Steps
1e6 0 1 2 3 4 5 6"
EXPERIMENTAL STUDIES,0.1968365553602812,Average Scores Won
EXPERIMENTAL STUDIES,0.19859402460456943,"PPO
RPG
NeuRD
ACH (Ours)"
EXPERIMENTAL STUDIES,0.20035149384885764,(b) Head-to-Head performance
EXPERIMENTAL STUDIES,0.20210896309314588,"Figure 1: (a): The training curves of the best response against each agent. Lower is better. (b): The
training curves of each agent. The performance of an agent is evaluated by the average scores the
agent wins against a common rule-based agent. Higher is better. We report the mean as solid curves
and the range of the average scores across 5 independent runs as shaded regions."
EXPERIMENTAL STUDIES,0.2038664323374341,"each method is selected at the 1e6th training step. Note that the agent is ﬁxed as a part of the 1-on-1
Mahjong environment when training the best response. We train the best response using PPO with the
same hyper-parameters that were used to train the PPO agent with self-play. During the training of
the best response, we evaluate the best response every 500 training steps using 10, 000 head-to-head
plays against each agent. According to the average scores each agent loses to its best response in
Figure 1(a), we may conclude that ACH is signiﬁcantly more difﬁcult to exploit than other methods
in the large-scale 1-on-1 Mahjong environment."
EXPERIMENTAL STUDIES,0.2056239015817223,"Head-to-Head Evaluation. We then compare the head-to-head performance of PPO, RPG, NeuRD,
and ACH in the 1-on-1 Mahjong environment. First, we compare the training process of each method
by evaluating each agent every 500 training steps against a common rule-based agent7 using 10, 000
head-to-head plays, the results of which are shown in Figure 1(b). As we can see, all methods beat
the common rule-based agent signiﬁcantly, while ACH has a clear advantage over other methods
in terms of stability and ﬁnal performance. The relatively slow convergence of RPG may due to
the threshold operation on the advantage, which could reduce sample efﬁciency in large-scale IIGs.
Second, the superior performance of ACH is further validated in head-to-head evaluations with other
agents in Table 1, where all the agents are selected at the 1e6th training step. The agent of ACH wins
all other agents by a signiﬁcant margin."
EXPERIMENTAL STUDIES,0.20738137082601055,"PPO
RPG
NeuRD
RPG
-0.21 ± 0.05
-
-
NeuRD -0.03 ± 0.02 0.04 ± 0.10
-
ACH
0.39 ± 0.02 0.66 ± 0.05 0.41 ± 0.07"
EXPERIMENTAL STUDIES,0.20913884007029876,"Table 1: Mean (± standard deviation) of the average winning scores of the row agents against the
column agents. The statistics are estimated by 5 independent runs (resulting 5 different agents for
each method). In each run, the average winning scores are obtained via 10, 000 head-to-head plays."
EXPERIMENTAL STUDIES,0.210896309314587,"Human Evaluation. We evaluate the agent, selected at the 1e6th training step, of ACH against
human players. First, the agent, named JueJong, is roughly evaluated by playing over 7, 700 games
against 157 practiced Mahjong players, where JueJong won an average of 4.56 scores per game.
Second, we select the top 4 out of 157 players according to their performances against JueJong and
play JueJong against the four players for 200 games each. As shown in Figure 2(a), the average
winning scores of JueJong oscillate in the ﬁrst 120 games but all plateau above 0 afterwards. More
importantly, we evaluate JueJong against the Mahjong champion Haihua Cheng for 1, 000 games,
as shown in Figure 2(b). After playing 1, 000 games, JueJong won the champion by a score of
0.82 ± 0.96 (mean ± standard deviation), with a p-value of 0.19 under one-tailed t-test. Hence, we
may conclude that Haihua Cheng failed to exploit JueJong effectively within 1, 000 games."
EXPERIMENTAL STUDIES,0.2126537785588752,"7The rule-based agent is implemented such that it selects the action Hu, Ting, Kong, Chow, and Pong in
descending priority whenever available and discards the tile that has the fewest neighbours."
EXPERIMENTAL STUDIES,0.21441124780316345,Published as a conference paper at ICLR 2022
EXPERIMENTAL STUDIES,0.21616871704745166,"0
40
80
120
160
200
Number of Games 10 0 10 20 30"
EXPERIMENTAL STUDIES,0.2179261862917399,Average Scores
EXPERIMENTAL STUDIES,0.21968365553602812,"player1
player2
player3
player4"
EXPERIMENTAL STUDIES,0.22144112478031636,(a) Against 4 practiced Mahjong players.
EXPERIMENTAL STUDIES,0.22319859402460457,"0
200
400
600
800
1000
Number of Games 6 3 0 3 6 9 12"
EXPERIMENTAL STUDIES,0.22495606326889278,Average Scores
EXPERIMENTAL STUDIES,0.22671353251318102,Haihua Cheng
EXPERIMENTAL STUDIES,0.22847100175746923,(b) Against the Mahjong champion.
EXPERIMENTAL STUDIES,0.23022847100175747,Figure 2: Performance of JueJong against human players.
EXPERIMENTAL STUDIES,0.23198594024604569,"0.2
0.4
0.6
0.8
1.0
Episodes
1e8 101 102"
EXPERIMENTAL STUDIES,0.23374340949033393,Exploitability
EXPERIMENTAL STUDIES,0.23550087873462214,"OS-DCFR
DREAM
ACH(Ours)"
EXPERIMENTAL STUDIES,0.23725834797891038,"105
106
107
108
109
1010
1011
Samples consumed 101 102"
EXPERIMENTAL STUDIES,0.2390158172231986,Exploitability
EXPERIMENTAL STUDIES,0.24077328646748683,"OS-DCFR
DREAM
ACH(Ours)"
EXPERIMENTAL STUDIES,0.24253075571177504,"0.2
0.4
0.6
0.8
1.0
Episodes
1e8 101 102"
EXPERIMENTAL STUDIES,0.24428822495606328,Exploitability
EXPERIMENTAL STUDIES,0.2460456942003515,"A2C
RPG
NeuRD
ACH(Ours)"
EXPERIMENTAL STUDIES,0.2478031634446397,"Figure 3: The exploitability on FHP, with the x-axis being the number of episodes generated (left and
right) and the number of samples consumed (middle). We report the mean as solid curves and the
range as shaded regions across 3 independent runs. OS-DCFR and DREAM were run once as their
performances are relatively stable, according to Brown et al. (2019) and Steinberger et al. (2020)."
RESULTS ON THE FHP BENCHMARK,0.24956063268892795,"7.3
RESULTS ON THE FHP BENCHMARK"
RESULTS ON THE FHP BENCHMARK,0.2513181019332162,"We further evaluate ACH and compare it with OS-DCFR, DREAM, A2C8, RPG, and NeuRD on FHP.
FHP is a simpliﬁed Heads-up Limit Texas Hold’em (HULH), which includes only the ﬁrst two of the
four bettings in HULH. It is a medium-sized game with over 1012 nodes and 109 infosets. All the
methods share the same neural network architecture proposed in Brown et al. (2019). We perform a
mild hyper-parameter search for ACH, A2C, RPG, and NeuRD. For OS-DCFR and DREAM, we
follow the hyper-parameters presented in Steinberger et al. (2020). The exploitability is measured
in the number of chips per game, where a big blind is 100 chips. All the hyper-parameters and the
running environment are described in the Appendix H.2."
RESULTS ON THE FHP BENCHMARK,0.2530755711775044,"As shown in Figure 3, ACH performs competitively with OS-DCFR and slightly worse than DREAM
on FHP in terms of exploitability per episodes generated. However, ACH is much more training
efﬁcient: ACH achieves an exploitability of 10 almost 100 times faster than DREAM and 1,000 times
faster than OS-DCFR. Also, in comparison with methods of similar training complexity (A2C, RPG,
and NeuRD), ACH converges signiﬁcantly faster and achieves a lower exploitability."
CONCLUSIONS,0.2548330404217926,"8
CONCLUSIONS"
CONCLUSIONS,0.2565905096660808,"In this paper, we investigated the problem of adapting policy gradient methods in deep RL to tackle
a large-scale IIG, i.e., 1-on-1 Mahjong. To this end, we developed a new model-free actor-critic
algorithm, i.e., ACH, for approximating a NE in large-scale IIGs. ACH is memory and computation
efﬁcient, as it uses only trajectory samples at the current iteration and requires no computation of best
response. ACH is theoretically justiﬁed as it is derived from a new neural-based CFR, i.e., NW-CFR,
of which we proved the convergence to an approximate NE in 2-player zero-sum IIGs under certain
conditions. The superior performance of ACH was validated on both our 1-on-1 Mahjong benchmark
and other common benchmarks. Secondly, to facilitate research on large-scale IIGs, we proposed the
ﬁrst 1-on-1 zero-sum Mahjong benchmark, whose infoset size and game length are much larger than
poker. Finally, using ACH we obtained the 1-on-1 Mahjong agent JueJong, which has demonstrated
stronger performance against the Mahjong champion Haihua Cheng."
CONCLUSIONS,0.2583479789103691,8PPO is replaced with A2C in a synchronous training environment.
CONCLUSIONS,0.2601054481546573,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.2618629173989455,ACKNOWLEDGEMENT
CONCLUSIONS,0.26362038664323373,"We thank the Mahjong champion Haihua Cheng for his efforts in this work. We appreciate the
support from Tencent Mahjong (https://majiang.qq.com). We are grateful to Tencent AI
Arena (https://aiarena.tencent.com) for providing the powerful computing capability to
the experiments on 1-on-1 Mahjong."
REPRODUCIBILITY STATEMENT,0.26537785588752194,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2671353251318102,"The experiments on 1-on-1 Mahjong were run in a large cluster of thousands of machines, on which
we have developed an efﬁcient actor-learner training platform, similar to IMPALA (Espeholt et al.,
2018). The code of the platform is not released currently but is planned to be open sourced in the
near future. The code of the 1-on-1 Mahjong benchmark is available at https://github.com/
yata0/Mahjong. The code of ACH is available at https://github.com/Liuweiming/
ACH_poker. All the hyper-parameters for all the experiments are listed in the Appendix H. All the
theoretical results are presented in the main text, with all the proofs given in the Appendix C."
REFERENCES,0.2688927943760984,REFERENCES
REFERENCES,0.27065026362038663,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, and et al. Dota 2 with
large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019."
REFERENCES,0.27240773286467485,"Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418–424, 2018."
REFERENCES,0.2741652021089631,"Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):
885–890, 2019."
REFERENCES,0.2759226713532513,"Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International Conference on Machine Learning (ICML), pp. 793–802, 2019."
REFERENCES,0.27768014059753954,"Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006."
REFERENCES,0.27943760984182775,"Trevor Davis, Martin Schmid, and Michael Bowling. Low-variance and zero-variance baselines for
extensive-form games. In International Conference on Machine Learning, pp. 2392–2401. PMLR,
2020."
REFERENCES,0.281195079086116,"Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
scalable distributed deep-rl with importance weighted actor-learner architectures. In International
Conference on Machine Learning (ICML), volume 80, pp. 1406–1415, 2018."
REFERENCES,0.28295254833040423,"Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997."
REFERENCES,0.28471001757469244,"Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth
mover’s distance in imperfect-information games. In Twenty-Eighth AAAI Conference on Artiﬁcial
Intelligence, 2014."
REFERENCES,0.28646748681898065,"Nicola Gatti, Fabio Panozzo, and Marcello Restelli. Efﬁcient evolutionary dynamics with extensive-
form games. In AAAI Conference on Artiﬁcial Intelligence, 2013."
REFERENCES,0.28822495606326887,"Audrunas Gruslys, Marc Lanctot, Rémi Munos, Finbarr Timbers, Martin Schmid, Julien Pérolat,
Dustin Morrill, Vinícius Flores Zambaldi, Jean-Baptiste Lespiau, John Schultz, Mohammad Ghesh-
laghi Azar, Michael Bowling, and Karl Tuyls. The advantage regret-matching actor-critic. CoRR,
abs/2008.12234, 2020."
REFERENCES,0.28998242530755713,"Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127–1150, 2000."
REFERENCES,0.29173989455184535,Published as a conference paper at ICLR 2022
REFERENCES,0.29349736379613356,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778, 2016."
REFERENCES,0.29525483304042177,"Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. CoRR, abs/1603.01121, 2016."
REFERENCES,0.29701230228471004,"Daniel Hennes, Dustin Morrill, Shayegan Omidshaﬁei, Rémi Munos, Julien Perolat, Marc Lanctot,
Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Duéñez-Guzmán, et al. Neural
replicator dynamics: Multiagent learning via hedging policy gradients. In International Joint
Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 492–501, 2020."
REFERENCES,0.29876977152899825,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp.
448–456, 2015."
REFERENCES,0.30052724077328646,"Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space
abstractions in extensive-form games. In Proceedings of the 2013 international conference on
Autonomous agents and multi-agent systems, pp. 271–278, 2013."
REFERENCES,0.3022847100175747,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.30404217926186294,"Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H. Bowling. Monte carlo sampling for
regret minimization in extensive games. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty,
Christopher K. I. Williams, and Aron Culotta (eds.), Advances in Neural Information Processing
Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings
of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, pp. 1078–1086.
Curran Associates, Inc., 2009."
REFERENCES,0.30579964850615116,"Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat,
David Silver, and Thore Graepel. A uniﬁed game-theoretic approach to multiagent reinforcement
learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4190–4203, 2017."
REFERENCES,0.30755711775043937,"Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinícius Flores Zambaldi, Satyaki Upadhyay,
Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshaﬁei, and et al.
Openspiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019."
REFERENCES,0.3093145869947276,"Hui Li, Kailiang Hu, Shaohua Zhang, Yuan Qi, and Le Song. Double neural counterfactual regret
minimization. In International Conference on Learning Representations (ICLR), 2020a."
REFERENCES,0.3110720562390158,"Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin,
Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning.
CoRR, abs/2003.13590, 2020b."
REFERENCES,0.31282952548330406,"Edward Lockhart, Marc Lanctot, Julien Pérolat, Jean-Baptiste Lespiau, Dustin Morrill, Finbarr
Timbers, and Karl Tuyls. Computing approximate equilibria in sequential adversarial games by
exploitability descent. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp.
464–470, 2019."
REFERENCES,0.3145869947275923,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning (ICML), pp. 1928–1937, 2016."
REFERENCES,0.3163444639718805,"Matej Moravˇcík, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artiﬁcial
intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017."
REFERENCES,0.3181019332161687,"Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshaﬁei, Mark Rowland, Pedro
Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From poincaré
recurrence to convergence in imperfect information games: Finding equilibrium via regularization.
In International Conference on Machine Learning, pp. 8525–8535. PMLR, 2021."
REFERENCES,0.31985940246045697,Published as a conference paper at ICLR 2022
REFERENCES,0.3216168717047452,"Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling.
Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive
form games using baselines. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 2157–2164, 2019."
REFERENCES,0.3233743409490334,"John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation. In International Confer-
ence on Learning Representations, (ICLR), 2016."
REFERENCES,0.3251318101933216,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017."
REFERENCES,0.3268892794376098,"Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Pérolat, Karl Tuyls, Rémi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 3422–3435, 2018."
REFERENCES,0.3286467486818981,"Eric Steinberger. Single deep counterfactual regret minimization. arXiv preprint arXiv:1901.07621,
2019."
REFERENCES,0.3304042179261863,"Eric Steinberger, Adam Lerer, and Noam Brown. DREAM: deep regret minimization with advantage
baselines and model-free learning. CoRR, abs/2006.10410, 2020."
REFERENCES,0.3321616871704745,"Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradi-
ent methods for reinforcement learning with function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 1057–1063, 1999."
REFERENCES,0.3339191564147627,"Finbarr Timbers, Edward Lockhart, Martin Schmid, Marc Lanctot, and Michael Bowling. Approxi-
mate exploitability: Learning a best response in large games. CoRR, abs/2004.09677, 2020."
REFERENCES,0.335676625659051,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, and et al. John P. Agapiou and.
Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):
350–354, 2019."
REFERENCES,0.3374340949033392,"Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, and Michael
Bowling. A practical use of imperfect recall. In Eighth symposium on abstraction, reformulation,
and approximation, 2009."
REFERENCES,0.3391915641476274,"Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael H. Bowling. Solving games with
functional regret estimation. In AAAI Conference on Artiﬁcial Intelligence, pp. 2138–2145, 2015."
REFERENCES,0.3409490333919156,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229–256, 1992."
REFERENCES,0.3427065026362039,"Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu,
Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao
Huang, and Wei Liu. Towards playing full MOBA games with deep reinforcement learning. In
Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.3444639718804921,"Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1729–1736, 2008."
REFERENCES,0.3462214411247803,Published as a conference paper at ICLR 2022
REFERENCES,0.34797891036906853,"A
INTRODUCTION OF THE 1-ON-1 MAHJONG BENCHMARK"
REFERENCES,0.34973637961335674,"Mahjong is a tile-based game that is played world wide with many regional variations, such as
Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full
of strategy, chance, and calculation. In this paper, we investigate a 1-on-1 Mahjong game, whose
information set size is much larger than poker, as shown in Figure 4. The game rules of 1-on-1
Mahjong are similar to Competition Mahjong. The corresponding game, “2-player Mahjong Master"",
is played by humans in Tencent mobile games (https://majiang.qq.com)."
REFERENCES,0.351493848857645,"1014
10162
10121
1074"
REFERENCES,0.3532513181019332,Information Set Count 103 1048 1011
REFERENCES,0.35500878734622143,Information Set Size HULH
REFERENCES,0.35676625659050965,"Heads-Up
No-Limit Hold'em"
-PLAYER MAHJONG,0.3585237258347979,4-Player Mahjong
-PLAYER MAHJONG,0.3602811950790861,1v1 Mahjong
-PLAYER MAHJONG,0.36203866432337434,"Figure 4: The game complexity of Heads-up Limit Texas Hold’em (HULH), Heads-up No-Limit
Texas Hold’em, 1-on-1 Mahjong, and 4-Player Mahjong."
-PLAYER MAHJONG,0.36379613356766255,"A.1
THE GAME RULES"
-PLAYER MAHJONG,0.3655536028119508,"Category
Name
Type
Copy
Count"
-PLAYER MAHJONG,0.36731107205623903,"Simples
Characters
4
36"
-PLAYER MAHJONG,0.36906854130052724,"Honors
Winds
4
16"
-PLAYER MAHJONG,0.37082601054481545,"Honors
Dragons
4
12"
-PLAYER MAHJONG,0.37258347978910367,"Bonus
Flowers
1
4"
-PLAYER MAHJONG,0.37434094903339193,"Bonus
Seasons
1
4"
-PLAYER MAHJONG,0.37609841827768015,Figure 5: A list of all tiles in the 1-on-1 Mahjong game.
-PLAYER MAHJONG,0.37785588752196836,"There are 24 unique tiles and 72 tiles in total in the 1-on-1 Mahjong game, as shown in Figure 5. At
the beginning of the 1-on-1 Mahjong game, each player is dealt with 13 tiles, the content of which is
invisible to the other. Afterwards, each player takes actions in turn. Typically, the ﬁrst player draws a
tile from the deck wall and then discards a tile, and the next player takes the same types of actions
in sequence. There are exceptional cases where the player does not draw a tile from the deck wall
but Chow, Pong, or Kong the tile the opponent just discarded. Afterwards, the player discards a
tile, and the game proceeds. Also, after drawing a tile from the deck wall, there are cases where the
player could Kong, after which the player draws and discards a tile in sequence. There are 10 types
of actions with 105 different actions in total, a full description of which is listed in Table 2."
-PLAYER MAHJONG,0.37961335676625657,Published as a conference paper at ICLR 2022
-PLAYER MAHJONG,0.38137082601054484,"Type
Description
#Action
Discard
Discard one of the tiles in the hand.
16"
-PLAYER MAHJONG,0.38312829525483305,"Pong
Make a meld of 3 identical tiles by seizing
the opponent’s latest discarded tile.
16"
-PLAYER MAHJONG,0.38488576449912126,Concealed-Kong
-PLAYER MAHJONG,0.3866432337434095,"Make a meld of 4 identical tiles when
they are in the hand, and the Concealed-Kong
is not revealed to the opponent. 16"
-PLAYER MAHJONG,0.3884007029876977,"Kong
Make a meld of 4 identical tiles by
seizing the opponent’s latest discarded tile.
16"
-PLAYER MAHJONG,0.39015817223198596,"Add-Kong
Make a meld of 4 identical tiles by
adding a tile to his own exposed Pong.
16"
-PLAYER MAHJONG,0.39191564147627417,"Chow
Make a meld of 3 character tiles in a row by
seizing the opponent’s latest discarded tile.
21"
-PLAYER MAHJONG,0.3936731107205624,"Draw
Give up the action of Pong, Chow, or Kong,
and draw a tile from the deck wall.
1"
-PLAYER MAHJONG,0.3954305799648506,"Ting
When there is only one tile away from a legal
hand, the player can declare Ting.
1 Hu"
-PLAYER MAHJONG,0.39718804920913886,"Form a legal hand by drawing a tile or
seizing the opponent’s latest discarded tile.
The Hu action ends the game. 1"
-PLAYER MAHJONG,0.3989455184534271,"Pass-Hu
Give up the action of Hu. Afterwards, the player
should select another legal action.
1"
-PLAYER MAHJONG,0.4007029876977153,Table 2: Different actions in the 1-on-1 Mahjong game.
-PLAYER MAHJONG,0.4024604569420035,"The goal of each player is to complete a legal hand prior to the opponent, by drawing a tile or using
the tile the opponent just discarded. A legal hand is generally in the form of four melds and a pair,
with an exception of 7 pairs. Different categories of legal hands, 64 in total, come with different
points, which is fully described in Table 3. Besides, a legal hand can belong to multiple categories,
and the score of a legal hand is the sum of points of corresponding categories. Legal hands with
higher points are generally more difﬁcult to form in terms of either luck or strategy, and it is critical
for a player to trade off the winning probability and the corresponding winning points. If no player
completes a legal hand before the tiles are exhausted, the game is tied. A ﬂow chart of the game is
illustrated in Figure 6."
-PLAYER MAHJONG,0.40421792618629176,"Category name Points
Pattern descriptions
Conﬂicts"
"PURE DOUBLE
CHOW",0.40597539543058,"1
Pure Double
Chow
1
Two identical sequences of 3 character tiles."
SHORT STRAIGHT,0.4077328646748682,"2
Short Straight
1
A hand with two successive sequences."
"TWO TERMINAL
CHOWS",0.4094903339191564,"3
Two Terminal
Chows
1
A hand with two sequences 123 and 789."
MELDED KONG,0.4112478031634446,"4
Melded Kong
1
A hand with exposed quads."
EDGE WAIT,0.4130052724077329,"5
Edge Wait
1"
EDGE WAIT,0.4147627416520211,"A hand completion with the situation there
is only one tile name to complete because
you have the incomplete sequence in edge
(12 or 89)."
CLOSED WAIT,0.4165202108963093,"6
Closed Wait
1"
CLOSED WAIT,0.4182776801405975,"A hand completion with the situation there
is only one tile name to complete because
you have the incomplete sequence lacking
center (like 24 and 79)."
SINGLE WAIT,0.4200351493848858,"7
Single Wait
1
A hand completion solely waiting on a tile
to form a pair.
8
Self Draw
1
A hand completion by draw."
FLOWER TILE,0.421792618629174,"9
Flower Tile
1
Whenever a player draws a ﬂower or season
tile, this is counted as 1 point.
10
Ready Hand
2
A hand that is one tile away from winning."
FLOWER TILE,0.4235500878734622,Published as a conference paper at ICLR 2022
DRAGON PUNG,0.4253075571177504,"11
Dragon Pung
2
A hand with a triplet or quad of dragon tile."
CONCEALED HAND,0.4270650263620387,"12 Concealed Hand
2
A hand without exposed melds and comple-
ted by seizing the discarded tile."
ALL CHOWS,0.4288224956063269,"13
All Chows
2
A hand with four sequences and a pair in
characters."
TILE HOG,0.4305799648506151,"14
Tile Hog
2
A hand with four same tiles, other than a
melded quad."
"TWO CONCEALED
PUNGS",0.43233743409490333,"15 Two Concealed
Pungs
2
A hand with two concealed triplets or quads."
CONCEALED KONG,0.43409490333919154,"16 Concealed Kong
2
A hand with a concealed quad."
ALL SIMPLES,0.4358523725834798,"17
All Simples
2
A hand consisting of no character 1, 9 and
honor tile."
OUTSIDE HAND,0.437609841827768,"18
Outside Hand
4
A hand that includes terminals and honors
in each meld, including the pair."
"FULLY CONCEALED
HAND",0.43936731107205623,"19 Fully Concealed
Hand
4
A hand without exposed melds and comple-
ted by drawing a tile.
Self Draw"
"TWO MELDED
KONGS",0.44112478031634444,"20
Two Melded
Kongs
4
A hand with two exposed quads.
Melded Kong"
LAST TILE,0.4428822495606327,"21
Last Tile
4"
LAST TILE,0.4446397188049209,"Winning on a tile that is the last of its kind.
Three of the other tiles on the table already
revealed to all players."
"LITTLE THREE
WINDS",0.44639718804920914,"22
Little Three
Winds
6
A hand with two wind triplets or quads and
a wind pair.
23
All Pungs
6
A hand with four triplets or quads."
HALF FLUSH,0.44815465729349735,"24
Half Flush
6
A hand consisting of character and honor
tiles."
"TWO DRAGON
PUNGS",0.44991212653778556,"25
Two Dragon
Pungs
6
A hand containing two dragon triplets or
quads.
Dragon Pung"
"TWO CONCEALED
KONGS",0.45166959578207383,"26 Two Concealed
Kongs
6
A hand containing two concealed quads.
Concealed Kong"
MELDED HAND,0.45342706502636204,"27
Melded Hand
6
Every set in the hand must be completed
with tiles discarded by other players.
Single Wait"
"OUT WITH
REPLACEMENT TILE",0.45518453427065025,"28
Out with
Replacement Tile
8
Hand completion with supplemental tile
when you melding quad.
Self Draw"
ROB KONG,0.45694200351493847,"29
Rob Kong
8
Winning off the tile that opponent adds to
a melded triplet(to create a Kong).
Last Tile"
"LAST TILE
CLAIM",0.45869947275922673,"30
Last Tile
Claim
8
Winning off another player on the last tile
(of the game)."
PURE STRAIGHT,0.46045694200351495,"31
Pure Straight
16
A hand with three sequences 123, 456 and
789 in characters."
PURE STRAIGHT,0.46221441124780316,"Short Straight,
Two Terminal
Chows"
"PURE SHIFTED
CHOWS",0.46397188049209137,"32
Pure Shifted
Chows
16"
"PURE SHIFTED
CHOWS",0.46572934973637964,"Three sequences in characters each shifted
either one or two numbers up from the last,
but not a combination of both.
33
All Flowers
16
A player has or draws all ﬂower tiles.
Flower Tiles
34
Full Flush
16
A hand consisting of only characters."
"THREE CONCEALED
PUNGS",0.46748681898066785,"35 Three Concealed
Pungs
16
A hand with three concealed quads or triplets. Two Concealed
Pungs"
"FOUR HONOUR
PUNGS",0.46924428822495606,"36
Four Honour
Pungs
24
A hand with four honor triplets or quads.
All Pungs"
"BIG THREE
WINDS",0.4710017574692443,"37
Big Three
Winds
24
A hand with three winds be triplets or quads. Little Three
Winds"
"BIG THREE
WINDS",0.4727592267135325,Published as a conference paper at ICLR 2022
SEVEN PAIRS,0.47451669595782076,"38
Seven Pairs
24
A hand with seven pairs."
SEVEN PAIRS,0.47627416520210897,"Concealed Hand,
Single Wait,
Fully Concealed
Hand."
"PURE TRIPLE
CHOW",0.4780316344463972,"39
Pure Triple
Chow
24
A hand with three identical sequences.
Pure Double
Chow"
"PURE SHIFTED
PUNGS",0.4797891036906854,"40
Pure Shifted
Pungs
24
A hand with three number triplets or quads
with successive numbers."
"PURE SHIFTED
PUNGS",0.48154657293497366,"Pure Triple
Chow"
"FOUR PURE
SHIFTED CHOWS",0.4833040421792619,"41
Four Pure
Shifted Chows
32"
"FOUR PURE
SHIFTED CHOWS",0.4850615114235501,"Four shifted number sequences, each shifted
either one or two numbers up from the last,
but not a combination of both."
"FOUR PURE
SHIFTED CHOWS",0.4868189806678383,"Pure Shifted
Chows,
Short Straight,
Two Terminal
Chows,
Pure Double
Chow"
THREE KONGS,0.48857644991212656,"42
Three Kongs
32
A hand with three quad melds."
THREE KONGS,0.4903339191564148,"Two Melded
Kongs, Melded
Kong, Two
Concealed Kongs,
Concealed Kong"
"ALL TERMINALS
AND HONOURS",0.492091388400703,"43
All Terminals
and Honours
32
A hand consisting of only terminal and honor
tiles."
"ALL TERMINALS
AND HONOURS",0.4938488576449912,"All Pungs,
Outside Hand"
"HEAVENLY READY
HAND",0.4956063268892794,"44 Heavenly Ready
Hand
32
A hand that is one tile away from winning at
the beginning of the game.
Ready Hand"
QUADRUPLE CHOW,0.4973637961335677,"45 Quadruple Chow
48
A hand with four identical sequences."
QUADRUPLE CHOW,0.4991212653778559,"Pure Triple
Chow,
Tile Hog,
Pure Double
Chow,
Two Terminal
Chow,
Pure Shifted
Pungs"
"FOUR PURE
SHIFTED PUNGS",0.5008787346221442,"46
Four Pure
Shifted Pungs
48
A hand with four character triplets or quads
with successive numbers."
"FOUR PURE
SHIFTED PUNGS",0.5026362038664324,"Pure Shifted
Pungs,
Pure Triple
Chow,
All Pungs"
"FOUR WINDS
SEVEN PAIRS",0.5043936731107206,"47
Four Winds
Seven Pairs
48
Seven pairs hand with four different wind
pairs."
"FOUR WINDS
SEVEN PAIRS",0.5061511423550088,"Seven Pairs,
Concealed Hand,
Single Wait,
Fully Concealed
Hand"
"THREE DRAGONS
SEVEN PAIRS",0.507908611599297,"48
Three Dragons
Seven Pairs
48
Seven pairs hand with three different dragon
pairs."
"THREE DRAGONS
SEVEN PAIRS",0.5096660808435852,"Seven Pairs,
Concealed Hand,
Single Wait,
Fully Concealed
Hand"
"LITTLE FOUR
WINDS",0.5114235500878734,"49
Little Four
Winds
64
A hand with three winds be triplets or quads,
and the last wind be pair."
"LITTLE FOUR
WINDS",0.5131810193321616,"Big Three
Winds,
Little Three
Winds"
"LITTLE FOUR
WINDS",0.5149384885764499,Published as a conference paper at ICLR 2022
"LITTLE THREE
DRAGONS",0.5166959578207382,"50
Little Three
Dragons
64
A hand with two dragons be triplets or quads,
and the last dragon be pair."
"LITTLE THREE
DRAGONS",0.5184534270650264,"Two Dragon
Pungs,
Dragon Pung"
ALL HONOURS,0.5202108963093146,"51
All Honours
64
A hand consisting of only honor tiles."
ALL HONOURS,0.5219683655536028,"All Terminals
And Honours,
All Pungs,
Four Honour
Pungs,
Outside Hand"
"FOUR CONCEALED
PUNGS",0.523725834797891,"52 Four Concealed
Pungs
64
A hand with four concealed triplets or quads."
"FOUR CONCEALED
PUNGS",0.5254833040421792,"Concealed Hand,
All Pungs,
Three Concealed
Pungs,
Two Concealed
Pungs,
Fully Concealed
Hand"
"PURE TERMINAL
CHOWS",0.5272407732864675,"53
Pure Terminal
Chows
64
A hand with two Two Terminal Chows and
a pair of number 5 in character."
"PURE TERMINAL
CHOWS",0.5289982425307557,"All Chows,
Seven Pairs,
Full Flush,
Pure Double
Chow,
Two Terminal
Chows"
"BIG FOUR
WINDS",0.5307557117750439,"54
Big Four
Winds
88
A hand with triplets or quads of all four
winds and an pair."
"BIG FOUR
WINDS",0.5325131810193322,"All Pungs,
Little Three
Winds,
Big Three
Winds,
Four Honor
Pungs"
"BIG THREE
DRAGONS",0.5342706502636204,"55
Big Three
Dragons
88
A hand with triplets or quads of all three
dragons."
"BIG THREE
DRAGONS",0.5360281195079086,"Dragon Pung,
Two Dragon
Pungs"
NINE GATES,0.5377855887521968,"56
Nine Gates
88"
NINE GATES,0.539543057996485,"Collecting number tiles 1112345678999
without melding, and completing with
any tile of characters."
NINE GATES,0.5413005272407733,"Full Flush,
Concealed Hand,
Fully Concealed
Hand"
FOUR KONGS,0.5430579964850615,"57
Four Kongs
88
A hand with four quad melds."
FOUR KONGS,0.5448154657293497,"Three Kongs,
Two Melded
Kongs,
Melded Kong,
Single Wait,
Concealed Kong,
Two Concealed
Kongs,
All Pungs"
"SEVEN SHIFTED
PAIRS",0.546572934973638,"58
Seven Shifted
Pairs
88
Seven pairs hand with successive seven
numbers in characters."
"SEVEN SHIFTED
PAIRS",0.5483304042179262,"Seven Pairs,
Single Wait,
Concealed Hand,
Full Flush,
Fully Concealed
Hand"
"SEVEN SHIFTED
PAIRS",0.5500878734622144,Published as a conference paper at ICLR 2022
UPPER FOUR,0.5518453427065027,"59
Upper Four
88
A hand consisting of character tiles of 6,
7, 8 or 9
Seven Pairs"
LOWER FOUR,0.5536028119507909,"60
Lower Four
88
A hand consisting of character tiles of 1,
2, 3 or 4
Seven Pairs"
"BIG SEVEN
HONOURS",0.5553602811950791,"61
Big Seven
Honours
88
Seven pairs hand with four different wind
pairs and three different dragon pairs."
"BIG SEVEN
HONOURS",0.5571177504393673,"Seven Pairs,
Four Winds
Seven Pairs,
Three Dragons
Seven Pairs,
Outside Hand,
Single Wait,
Concealed Hand,
Fully Concealed
Hand,
All Honours"
HEAVENLY HAND,0.5588752196836555,"62 Heavenly Hand
88
The dealer draws a winning hand at the
beginning of the game."
HEAVENLY HAND,0.5606326889279437,"Self Draw,
Concealed Hand"
EARTHLY HAND,0.562390158172232,"63
Earthly Hand
88"
EARTHLY HAND,0.5641476274165202,"A player completes a winning hand with
the dealer’s ﬁrst discard and in most
variants, provided the dealer does not
draw a quad."
EARTHLY HAND,0.5659050966608085,"Self Draw,
Concealed Hand"
HUMANLY HAND,0.5676625659050967,"64 Humanly Hand
88"
HUMANLY HAND,0.5694200351493849,"A player completes a winning hand with
the opponent player’s ﬁrst discard. And
before that any action of Chow, Pong or
Kong is not available.
Table 3: The categories of legal hands in ascending order of corresponding points.
The winning points of a legal hand are obtained by summing the points of the
matched categories in reverse order while excluding the conﬂict categories."
HUMANLY HAND,0.5711775043936731,"A.2
THE STATE AND ACTION SPACE"
HUMANLY HAND,0.5729349736379613,"The state space size of 1-on-1 Mahjong, shown as the infoset count in Figure 4, is approximately 1074.
Yet, the state space of 1-on-1 Mahjong is not as easily abstracted as in poker. The primary reason is
that a single tile difference in the state could signiﬁcantly impact the policy, e.g., making a legal hand
illegal and vice versa. In contrast, states that have similar strength in poker could share a common
policy. For instance, the optimal preﬂop policy could be very similar for “Ace-Four"" and “Ace-Three""
in poker. Another reason is that a state in 1-on-1 Mahjong is divided into different information groups,
as demonstrated in Figure 7(a). Different information groups have signiﬁcantly different meanings.
For instance, one group denotes the player’s hand, which is invisible to the opponent, while another
one denotes the player’s discarded tiles, which are visible to both players."
HUMANLY HAND,0.5746924428822495,"There are 105 different actions in total, as demonstrated in Table 2. The number of legal actions in a
state is relatively small compared to poker. Yet, the game length in 1-on-1 Mahjong is larger than
that in poker. Players can decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most
1-on-1 poker games end within 10 steps. As a result, the reaching probability of states in 1-on-1
Mahjong may vary more signiﬁcantly than that in poker."
HUMANLY HAND,0.5764499121265377,"A.3
THE EFFECTS OF A LARGER INFOSET SIZE AND A LONGER GAME LENGTH"
HUMANLY HAND,0.5782073813708261,"As shown in Figure 4, 1-on-1 Mahjong has a larger infoset size than poker. The infoset size does not
seem to have an inﬂuence on the convergence of a tabular CFR (Zinkevich et al., 2008). However,
when trajectory sampling and function approximation are used together, the situation may be different.
To be more speciﬁc, in a trajectory sampling algorithm, the variance of the sampled instantaneous
counterfactual value (regret) of a larger infoset may tend to be higher, which may have a large"
HUMANLY HAND,0.5799648506151143,Published as a conference paper at ICLR 2022
HUMANLY HAND,0.5817223198594025,Pi draws a tile
HUMANLY HAND,0.5834797891036907,Pi  discards a tile
HUMANLY HAND,0.5852372583479789,Pi  selects a legal action
HUMANLY HAND,0.5869947275922671,Game starts
HUMANLY HAND,0.5887521968365553,Is Concealed-Kong?
HUMANLY HAND,0.5905096660808435,"Deal cards for P0, P1 i=0"
HUMANLY HAND,0.5922671353251318,Pi selects a legal action
HUMANLY HAND,0.5940246045694201,i = 1-i
HUMANLY HAND,0.5957820738137083,Is Kong?
HUMANLY HAND,0.5975395430579965,Pi  discards a tile
HUMANLY HAND,0.5992970123022847,Pi  selects a legal action
HUMANLY HAND,0.6010544815465729,Is Concealed-Kong
HUMANLY HAND,0.6028119507908611,or Add-Kong? yes yes no no no yes no no yes yes yes yes no no yes
HUMANLY HAND,0.6045694200351494,Game ends yes
HUMANLY HAND,0.6063268892794376,Is Pong?
HUMANLY HAND,0.6080843585237259,Is Chow?
HUMANLY HAND,0.6098418277680141,Is Hu?
HUMANLY HAND,0.6115992970123023,Is Hu? no
HUMANLY HAND,0.6133567662565905,Is Hu?
HUMANLY HAND,0.6151142355008787,No tiles? no
HUMANLY HAND,0.616871704745167,Pi draws a tile
HUMANLY HAND,0.6186291739894552,Figure 6: A ﬂow chart of the 1-on-1 Mahjong game.
HUMANLY HAND,0.6203866432337434,"inﬂuence on performance when neural network function approximation is used. In other words,
1-on-1 Mahjong may be complementary to poker in evaluating algorithms using deep neural networks
and only trajectory samples."
HUMANLY HAND,0.6221441124780316,"The game length has a direct impact on the sampling methods used. For poker, which has a relatively
short game length, methods that sample multiple actions in a state are very common in the literature.
Yet, sampling multiple actions consistently in game trees with long episodes is certainly prohibitive,
as the number of samples goes exponentially with the game length. 1-on-1 Mahjong has a maximal
game length about 40, which may be daunting for methods that try to sample multiple actions in
a state. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms
when only trajectory samples are allowed."
HUMANLY HAND,0.6239015817223199,"B
THE MODEL DESIGN OF OUR 1-ON-1 MAHJONG AGENT JUEJONG"
HUMANLY HAND,0.6256590509666081,"The model of JueJong is an end-to-end neural network that takes all relevant information as input and
outputs both the probabilities of all actions and the state value. This is different from ﬁve separated
neural networks in Suphx (Li et al., 2020b), representing ﬁve different types of actions. Also, we
train JueJong from zero by pure self-play using ACH, while the ﬁve neural networks in Suphx were
trained by supervised learning on human data, with only the “Discard” network further enhanced by
RL. Figure 7 gives an overview of the model design of JueJong."
HUMANLY HAND,0.6274165202108963,Published as a conference paper at ICLR 2022
HUMANLY HAND,0.6291739894551845,"1
1
1
0
0
0
0
1
0
0
0
0
0
1
0
0
0"
HUMANLY HAND,0.6309314586994728,"0
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0"
HUMANLY HAND,0.632688927943761,"0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0"
HUMANLY HAND,0.6344463971880492,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 2
1
3 4 10
8 6 7 11 9 5 1 2 3 6"
HUMANLY HAND,0.6362038664323374,3 × Stage
HUMANLY HAND,0.6379613356766256,Residual block
HUMANLY HAND,0.6397188049209139,Transition block
HUMANLY HAND,0.6414762741652021,Residual block
HUMANLY HAND,0.6432337434094904,"Residual block FC
FC"
HUMANLY HAND,0.6449912126537786,Value net
HUMANLY HAND,0.6467486818980668,Image-like features
"REGION
FEATURE
DIM",0.648506151142355,"5
Region
Feature
Dim"
"REGION
FEATURE
DIM",0.6502636203866432,"Position
[2,1]"
"REGION
FEATURE
DIM",0.6520210896309314,"is_ready
[2,2]"
"REGION
FEATURE
DIM",0.6537785588752196,"Bonus
[2,8]"
"REGION
FEATURE
DIM",0.655536028119508,"Built-in
Last action
[2,105] FC
FC"
"REGION
FEATURE
DIM",0.6572934973637962,"Policy net
+"
"REGION
FEATURE
DIM",0.6590509666080844,"(a)
(c) (b) FC 8 9 10 11 4 7"
"REGION
FEATURE
DIM",0.6608084358523726,Height Width
"REGION
FEATURE
DIM",0.6625659050966608,"Figure 7:
(a) The graphical user interface of 1-on-1 Mahjong with marked regions representing
different groups of information, which are encoded in either image-like features or one-hot features.
(b) The image-like feature encoding scheme. (c) The model architecture design."
"REGION
FEATURE
DIM",0.664323374340949,"The marked regions in Figure 7(a) summarize the information an agent can observe. Regions 1 to
7 are encoded in image-like features, which represent the player’s hand, the player’s Chow, Pong,
and Kong, the player’s concealed-Kong, the player’s Discard, the opponent’s Chow, Pong, and Kong,
the opponent’s concealed-Kong, and the opponent’s Discard respectively. A feature map, as shown
in Figure 7(b), of height 4 and width 17 is employed, where a 0 or 1 in the ith row and jth column
means whether there are i tiles of the jth type in the input set of tiles. Therefore, a single or two
successive convolutional layers with 3 × 3 kernels efﬁciently derive row and column combinations.
Note that, in Suphx (Li et al., 2020b), the feature maps are one-dimensional vectors, where 1 × 3
kernels are used. The black tile in Figure 7(b) is designed exclusively for Region 6 to indicate how
many concealed-Kongs the opponent has. For discarded tiles in Region 4 or 7, 24 such feature
maps are used to indicate the latest 24 discarded tiles in order, with one feature map encoding one
discard tile. All the feature maps are concatenated in the channel dimension, and therefore the ﬁnal
image-like features are in the shape of 4 × 17 × 53 (h × w × c)."
"REGION
FEATURE
DIM",0.6660808435852372,"Regions 8 to 11 are encoded in one-hot features, which represent the player’s position (0 or 1), the
is_ready state of both players, and the bonus tiles of both players. Additionally, the last action (not
shown as a region in Figure 7(a)) for each player is encoded in a one-hot feature vector as well."
"REGION
FEATURE
DIM",0.6678383128295254,"We apply residual blocks (He et al., 2016) to transform the image-like features. There are totally
3 stages with 3 residual blocks and 1 transition layer in each stage, as shown in Figure 7(c). Each
block contains two convolutional layers with kernel size 3 × 3. The transition layers are point-
wise convolutions that scale the number of output channels to 64, 128, and 32, respectively for
each stage. Subsequently, the transformed image-like features are reshaped to a vector, which is
concatenated with the one-hot feature vectors. A fully-connected layer (dimension 1024) is then
used to transform the concatenated feature vector, and two branches with two fully-connected layers
(dimension 512 × 512) each output the action probabilities and the state value respectively. Besides,
we apply batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearity after all convolutional
layers and fully-connected layers."
"REGION
FEATURE
DIM",0.6695957820738138,"C
THEORETICAL PROPERTIES OF NW-CFR"
"REGION
FEATURE
DIM",0.671353251318102,"C.1
PROOF FOR THEOREM 1"
"REGION
FEATURE
DIM",0.6731107205623902,"In order to prove Theorem 1, we ﬁrst prove the following Lemma."
"REGION
FEATURE
DIM",0.6748681898066784,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.6766256590509666,"Lemma 1. For a weighted CFR, if wt(s) ∈[wl(s), wh(s)], 0 < wl(s) ≤wh(s) ≤1, then"
"REGION
FEATURE
DIM",0.6783831282952548,"max
a∈A(s) t
X"
"REGION
FEATURE
DIM",0.680140597539543,"k=1
rc
k(s, a) ≤maxa∈A(s) Rw
t (s, a)
wh(s)
+ (wh(s) −wl(s))|A(s)|∆t"
"REGION
FEATURE
DIM",0.6818980667838312,"wh(s)
.
(4)"
"REGION
FEATURE
DIM",0.6836555360281195,"Proof. First, for any state s ∈S and action a ∈A(s), we have"
"REGION
FEATURE
DIM",0.6854130052724078,"Rw
t (s, a) := t
X"
"REGION
FEATURE
DIM",0.687170474516696,"k=1
wt(s)rc
k(s, a) =
X"
"REGION
FEATURE
DIM",0.6889279437609842,"k:rc
k(s,a)≥0
wk(s)|rc
k(s, a)| −
X"
"REGION
FEATURE
DIM",0.6906854130052724,"k′:rc
k′(s,a)<0
wk′(s)|rc
k′(s, a)| ≥
X"
"REGION
FEATURE
DIM",0.6924428822495606,"k:rc
k(s,a)≥0
wl(s)|rc
k(s, a)| −
X"
"REGION
FEATURE
DIM",0.6942003514938488,"k′:rc
k′(s,a)<0
wh(s)|rc
k′(s, a)|. (5) So,"
"REGION
FEATURE
DIM",0.6959578207381371,"Rw
t (s, a) ≥wh(s) t
X"
"REGION
FEATURE
DIM",0.6977152899824253,"k=1
rc
k(s, a) −(wh(s) −wl(s))
X"
"REGION
FEATURE
DIM",0.6994727592267135,"k′:rc
k′(s,a)≥0
|rc
k′(s, a)|.
(6)"
"REGION
FEATURE
DIM",0.7012302284710018,"In other words, t
X"
"REGION
FEATURE
DIM",0.70298769771529,"k=1
rc
k(s, a) ≤Rw
t (s, a)
wh(s)
+ wh(s) −wl(s) wh(s) X"
"REGION
FEATURE
DIM",0.7047451669595782,"k′:rc
k′(s,a)≥0
|rc
k′(s, a)|
(7) So,"
"REGION
FEATURE
DIM",0.7065026362038664,"max
a∈A(s) t
X"
"REGION
FEATURE
DIM",0.7082601054481547,"k=1
rc
k(s, a) ≤maxa∈A(s) Rw
t (s, a)
wh(s)
+ (wh(s) −wl(s))∆t"
"REGION
FEATURE
DIM",0.7100175746924429,"wh(s)
.
(8)"
"REGION
FEATURE
DIM",0.7117750439367311,Then we can prove the Theorem:
"REGION
FEATURE
DIM",0.7135325131810193,"Proof. For NW-CFR, the policy at iteration t is generated according to Hedge:"
"REGION
FEATURE
DIM",0.7152899824253075,"πt(s, a) =
eη(s)Ra
t−1(s,a)
P
a′ eη(s)Ra
t−1(s,a′) ,
(9) where"
"REGION
FEATURE
DIM",0.7170474516695958,"Ra
t−1(s, a) = t−1
X"
"REGION
FEATURE
DIM",0.718804920913884,"k=1
f µk
p (s)rc
k(s, a).
(10)"
"REGION
FEATURE
DIM",0.7205623901581723,"Meanwhile, weighted CFR with Hedge generates policy at iteration t according to"
"REGION
FEATURE
DIM",0.7223198594024605,"πt(s, a) =
eη(s)Rw
t−1(s,a)
P"
"REGION
FEATURE
DIM",0.7240773286467487,"a′ eη(s)Rw
t−1(s,a′) ,
(11) where"
"REGION
FEATURE
DIM",0.7258347978910369,"Rw
t−1(s, a) = t−1
X"
"REGION
FEATURE
DIM",0.7275922671353251,"k=1
wk(s)rc
k(s, a).
(12)"
"REGION
FEATURE
DIM",0.7293497363796133,"So, when wk(s) = f µk
p (s) ≥wl(s) > 0 for any s ∈S and k > 0, NW-CFR and weighted CFR with
Hedge generate the same policy at each iteration, and therefore NW-CFR is equivalent to weighted
CFR with Hedge."
"REGION
FEATURE
DIM",0.7311072056239016,"Then, we can prove the convergence property of NW-CFR. Let"
"REGION
FEATURE
DIM",0.7328646748681898,"Sw
t (s) =
X"
"REGION
FEATURE
DIM",0.7346221441124781,"a∈A(s)
eη(s)Rw
t (s,a).
(13)"
"REGION
FEATURE
DIM",0.7363796133567663,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.7381370826010545,"We have
ln Sw
t
Sw
0
= ln
X"
"REGION
FEATURE
DIM",0.7398945518453427,"a∈A(s)
eη(s)Rw
t (s,a) −ln |A(s)|"
"REGION
FEATURE
DIM",0.7416520210896309,"≥ln

max
a∈A(s) eη(s)Rw
t (s,a)

−ln |A(s)|"
"REGION
FEATURE
DIM",0.7434094903339191,"=η(s) max
a∈A(s) Rw
t (s, a) −ln |A(s)|. (14)"
"REGION
FEATURE
DIM",0.7451669595782073,"Meanwhile, for each k = 1, . . . , t,"
"REGION
FEATURE
DIM",0.7469244288224957,"ln Sw
k
Sw
k−1
= ln P"
"REGION
FEATURE
DIM",0.7486818980667839,"a∈A(s) eη(s)Rw
k−1(s,a)eη(s)wk(s)rc
k(s,a)
P"
"REGION
FEATURE
DIM",0.7504393673110721,"a∈A(s) eη(s)Rw
k−1(s,a) ! = ln  X"
"REGION
FEATURE
DIM",0.7521968365553603,"a∈A(s)
πk(s, a)eη(s)wk(s)rc
k(s,a)  . (15) Since"
"REGION
FEATURE
DIM",0.7539543057996485,ln E[esX] ≤sEX + s2(b −a)2
"REGION
FEATURE
DIM",0.7557117750439367,"8
,
(16)"
"REGION
FEATURE
DIM",0.7574692442882249,"ln Sw
k
Sw
k−1
≤η(s)
X"
"REGION
FEATURE
DIM",0.7592267135325131,"a∈A(s)
πk(s, a)wk(s)rc
k(s, a) + η2(s)∆2(s)w2
k(s)
8
.
(17)"
"REGION
FEATURE
DIM",0.7609841827768014,"Note that rc
k(s, a) = vc
k(s, a) −P"
"REGION
FEATURE
DIM",0.7627416520210897,"a∈A(s) πk(s, a)vc
k(s, a). So
X"
"REGION
FEATURE
DIM",0.7644991212653779,"a∈A(s)
πk(s, a)wk(s)rc
k(s, a) = 0.
(18)"
"REGION
FEATURE
DIM",0.7662565905096661,"Therefore,"
"REGION
FEATURE
DIM",0.7680140597539543,"ln Sw
k
Sw
0
≤η2(s)∆2(s) Pt
k=1 w2
k(s)
8
.
(19) So,"
"REGION
FEATURE
DIM",0.7697715289982425,"η(s) max
a∈A(s) Rw
t (s, a) −ln |A(s)| ≤η2(s)∆2(s) Pt
k=1 w2
k(s)
8
,
(20) i.e.,"
"REGION
FEATURE
DIM",0.7715289982425307,"max
a∈A(s) Rw
t (s, a) ≤ln |A(s)|"
"REGION
FEATURE
DIM",0.773286467486819,"η(s)
+ η(s)∆2(s) Pt
k=1 w2
k(s)
8
.
(21)"
"REGION
FEATURE
DIM",0.7750439367311072,"According to Lemma 1,"
"REGION
FEATURE
DIM",0.7768014059753954,"max
a∈A(s) Rc
t(s, a) ≤ln |A(s)|"
"REGION
FEATURE
DIM",0.7785588752196837,"η(s)wh(s) + η(s)∆2(s) Pt
k=1 w2
k(s)
8wh(s)
+ (wh(s) −wl(s))∆t"
"REGION
FEATURE
DIM",0.7803163444639719,"wh(s)
.
(22)"
"REGION
FEATURE
DIM",0.7820738137082601,"When η(s) =
p"
"REGION
FEATURE
DIM",0.7838312829525483,"8 ln |A(s)|/{[wh(s)]2∆2(s)T} and wt(s) ∈[wl(s), wh(s)] ⊂(0, 1], t = 1, . . . , T,
we have"
"REGION
FEATURE
DIM",0.7855887521968365,"max
a∈A(s) Rc
T (s, a) ≤ s"
"REGION
FEATURE
DIM",0.7873462214411248,"ln |A(s)|∆2(s)w2
h(s)T
2w2
h(s)
+ (wh(s) −wl(s))∆T wh(s) ≤∆ r T"
"REGION
FEATURE
DIM",0.789103690685413,2 ln |A(s)| + (wh(s) −wl(s))∆T
"REGION
FEATURE
DIM",0.7908611599297012,"wh(s)
. (23)"
"REGION
FEATURE
DIM",0.7926186291739895,"According to Theorem 2 in Zinkevich et al. (2008), the total regret"
"REGION
FEATURE
DIM",0.7943760984182777,"RT ≤
X"
"REGION
FEATURE
DIM",0.7961335676625659,"s∈S
max
a∈A(s)[Rc
T (s, a)]+ ≤
X s∈S  ∆ r T"
"REGION
FEATURE
DIM",0.7978910369068541,2 ln |A(s)| + (wh(s) −wl(s))∆T wh(s) ! ≤|S|∆ r T
"REGION
FEATURE
DIM",0.7996485061511424,"2 ln |A| + ∆T
X s∈S"
"REGION
FEATURE
DIM",0.8014059753954306,wh(s) −wl(s)
"REGION
FEATURE
DIM",0.8031634446397188,"wh(s)
. (24)"
"REGION
FEATURE
DIM",0.804920913884007,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.8066783831282952,"As a result, according to the folk theorem in Zinkevich et al. (2008), the average policy has ϵ
exploitability, where"
"REGION
FEATURE
DIM",0.8084358523725835,"ϵ =
1
|P| X p∈P Rp,T"
"REGION
FEATURE
DIM",0.8101933216168717,"T
≤|S|∆ r"
"REGION
FEATURE
DIM",0.81195079086116,"1
2T ln |A| + ∆
X s∈S"
"REGION
FEATURE
DIM",0.8137082601054482,wh(s) −wl(s)
"REGION
FEATURE
DIM",0.8154657293497364,"wh(s)
.
(25)"
"REGION
FEATURE
DIM",0.8172231985940246,"C.2
PROOF FOR COROLLARY 1"
"REGION
FEATURE
DIM",0.8189806678383128,"Proof. When the behavioral policy µp,k of each player p ∈P is constant across iterations ∀k > 0,
the reaching probability f µk
p (s) of any state s ∈S is also constant. Assume f µk
p (s) = w(s), then,"
"REGION
FEATURE
DIM",0.820738137082601,"Ra
t−1(s, a) = t−1
X"
"REGION
FEATURE
DIM",0.8224956063268892,"k=1
f µk
p (s)rc
t(s, a) = w(s) t−1
X"
"REGION
FEATURE
DIM",0.8242530755711776,"k=1
rc
t(s, a) = w(s)Rc
t−1(s, a).
(26)"
"REGION
FEATURE
DIM",0.8260105448154658,"In other words, Ra
t−1(s, a) is equal to the cumulative counterfactual regret scaled by a time-invariant
weight w(s). Hence, the policy at iteration t is"
"REGION
FEATURE
DIM",0.827768014059754,"πt(a|s) =
eη(s)w(s)Rc
t(s,a)
P"
"REGION
FEATURE
DIM",0.8295254833040422,"a′ eη(s)w(s)Rc
t−1(s,a′) =
eη′(s)Rc
t(s,a)
P"
"REGION
FEATURE
DIM",0.8312829525483304,"a′ eη′(s)Rc
t−1(s,a′) ,
(27)"
"REGION
FEATURE
DIM",0.8330404217926186,"where the new η′(s) is set to
p"
"REGION
FEATURE
DIM",0.8347978910369068,"8 ln |A(s)|/[∆2(s)T]. As a result, for constant µp,k, NW-CFR is
equivalent to CFR with Hedge when y(a|s; θt) is sufﬁciently close to Ra
t (s, a). Furthermore, since
wl(s) = f µk
p (s) = wh(s), the second term in Equation 25 vanishes, i.e.,"
"REGION
FEATURE
DIM",0.836555360281195,ϵ ≤|S|∆ r
"REGION
FEATURE
DIM",0.8383128295254832,"1
2T ln |A|.
(28)"
"REGION
FEATURE
DIM",0.8400702987697716,"As a result, the exploitability bound of CFR with Hedge is recovered."
"REGION
FEATURE
DIM",0.8418277680140598,"D
EXPERIMENTAL RESULTS OF THE WEIGHTED CFR"
"REGION
FEATURE
DIM",0.843585237258348,"As stated in Section 5, ACH is a practical implementation of NW-CFR, which is a straightforward
neural extension to the weighted CFR deﬁned in Deﬁnition 1, together with wt(s) = f µt
p (s) and
Hedge. In order to investigate the behavior of weighted CFR, in this section, we instantiate multiple
weighted CFR algorithms with different settings of the weight wt(s) by varying µp,t, since f µt
p (s)
depends only on µp,t. Note that the weighted CFR traverses the full game tree at every iteration and
that µp,t is only used to calculate the state reaching probability f µt
p (s). We test these algorithms on
three small IIGs in OpenSpiel: Kuhn poker, Leduc poker, and Liar’s Dice."
"REGION
FEATURE
DIM",0.8453427065026362,"250
500
750
1000 1250 1500 1750 2000
Iterations 10
3 10
2 10
1"
"REGION
FEATURE
DIM",0.8471001757469244,Exploitability
"REGION
FEATURE
DIM",0.8488576449912126,"CFR(Hedge)
Uniform
Current(0.5)
Current(0.1)
Current"
"REGION
FEATURE
DIM",0.8506151142355008,(a) Kuhn poker
"REGION
FEATURE
DIM",0.8523725834797891,"250
500
750
1000 1250 1500 1750 2000
Iterations 10
2 10
1 100"
"REGION
FEATURE
DIM",0.8541300527240774,Exploitability
"REGION
FEATURE
DIM",0.8558875219683656,"CFR(Hedge)
Uniform
Current(0.5)
Current(0.1)
Current"
"REGION
FEATURE
DIM",0.8576449912126538,(b) Leduc poker
"REGION
FEATURE
DIM",0.859402460456942,"250
500
750
1000 1250 1500 1750 2000
Iterations 10
1"
"REGION
FEATURE
DIM",0.8611599297012302,Exploitability
"REGION
FEATURE
DIM",0.8629173989455184,"CFR(Hedge)
Uniform
Current(0.5)
Current(0.1)
Current"
"REGION
FEATURE
DIM",0.8646748681898067,(c) Liar’s Dice
"REGION
FEATURE
DIM",0.8664323374340949,"Figure 8: Exploitability of the weighted CFR with wt(s) = f µt
p (s) and CFR (i.e., the weighted CFR
with wt(s) = 1.0). The probability f µt
p (s) is determined by the behavior policy µp,t. The setting of
µp,t for each line is given in the legend, in which “Uniform” means µp,t(s) = the uniform policy;
“Current” means µp,t(s) = πp,t(s); “Current(x)” means µp,t(s) = xUniform + (1 −x)πp,t(s). Note
that the exploitability is reported with regard to the average policy."
"REGION
FEATURE
DIM",0.8681898066783831,"As shown in Figure 8, the weighted CFR with wt(s) induced by a stationary µp,t (i.e., the uniform
policy) converges at the same pace with CFR(Hedge). As a result, Corollary 1 is veriﬁed on the three"
"REGION
FEATURE
DIM",0.8699472759226714,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.8717047451669596,"small benchmarks. Also, as Theorem 1 states, the exploitability of the weighted CFR is inﬂuenced by
the range of wt(s) = f µt
p (s). This is experimentally demonstrated in Figure 8 by setting µp,t to a
mixed policy between the current policy πp,t and the uniform policy. We can see that the weighted
CFR still performs competitively with CFR(Hedge), when µp,t(s) = 0.5Uniform + 0.5πp,t(s)."
"REGION
FEATURE
DIM",0.8734622144112478,"E
ACH: A PRACTICAL IMPLEMENTATION OF NW-CFR"
"REGION
FEATURE
DIM",0.875219683655536,"To address the practical issues mentioned in Section 5, we provide a practical and parallel implemen-
tation of NW-CFR, i.e., ACH, which employs a framework of decoupled acting and learning, similar
to IMPALA (Espeholt et al., 2018). ACH maintains a policy net y(a|s; θ) and a value net V (s; ω),
where θ and ω share a large portion of parameters (see Figure 7). Both players use the same θ and
ω. We do not use an additional time-invariant behavioral policy for sampling actions. Instead, we
use the current policy πt, i.e., µp,t = πp,t, ∀p ∈P. As a result, we can use the same samples to train
both the value net and the policy net. Also, η(s) is incorporated into the learned target value in the
policy net, so the policy π(a|s) is obtained by directly softmaxing on y(a|s; θ)."
"REGION
FEATURE
DIM",0.8769771528998243,"The advantage A(s, a) is estimated by GAE(λ) (Schulman et al., 2016), using sampled rewards
and V (s; ω), for only sampled states and actions. The value and policy nets are updated as soon
as a mini-batch of samples is available. In other words, we update θ and ω once using a single
mini-batch at each iteration. As a result, the policy loss reduces to Lπ(s) = η(s) y(a|s;θ)"
"REGION
FEATURE
DIM",0.8787346221441125,"πold(a|s)A(a, s),
where
1
πold(a|s) accounts for the fact that the action a was sampled using πold(a|s). ACH handles
asynchronous training with the importance ratio clipping [1 −ε, 1 + ε] of PPO (Schulman et al.,
2017). To avoid numerical issues, the mean ¯y(· |s; θ) is subtracted from the policy output, which is
then clipped within a range [−lth, lth]. The pseudocode of ACH is given in Algorithm 2."
"REGION
FEATURE
DIM",0.8804920913884007,"Algorithm 2: ACH
Initialize the policy and critic parameters: θ and ω.
Start multiple actor and learner threads in parallel.
Actors:
while true do"
"REGION
FEATURE
DIM",0.8822495606326889,"Fetch the latest model from the learners.
Generate samples via self-play in the form: [a, s, A(s, a), G, πold(a|s)].
Send the samples to the replay buffer.
Learners:
for t ∈1, 2, 3, ... do"
"REGION
FEATURE
DIM",0.8840070298769771,"Fetch a mini-batch of samples from the replay buffer.
Lsum = 0.
for each sample [a, s, A(s, a), G, πold(a|s)] ∈the mini-batch do c ="
"REGION
FEATURE
DIM",0.8857644991212654,"(
1{ π(a|s;θ)"
"REGION
FEATURE
DIM",0.8875219683655536,"πold(a|s) < 1 + ε}1{y(a|s; θ) −¯y(· |s; θ) < lth}
if A(s, a) ≥0,"
"REGION
FEATURE
DIM",0.8892794376098418,1{ π(a|s;θ)
"REGION
FEATURE
DIM",0.8910369068541301,"πold(a|s) > 1 −ε}1{y(a|s; θ) −¯y(· |s; θ) > −lth}
if A(s, a) < 0."
"REGION
FEATURE
DIM",0.8927943760984183,Lsum += −cη(s) y(a|s;θ)
"REGION
FEATURE
DIM",0.8945518453427065,"πold(a|s)A(a, s) + α"
"REGION
FEATURE
DIM",0.8963093145869947,2 [V (s; ω) −G)]2 + β P
"REGION
FEATURE
DIM",0.8980667838312829,a π(a|s; θ) log π(a|s; θ).
"REGION
FEATURE
DIM",0.8998242530755711,Update θ and ω once using gradient on Lsum.
"REGION
FEATURE
DIM",0.9015817223198594,"We employ an entropy loss to encourage exploration during training and hopefully the convergence
of current policy to a NE (Srinivasan et al., 2018). ACH updates θ and ω simultaneously, and the
overall loss is:"
"REGION
FEATURE
DIM",0.9033391915641477,LACH = −cη(s) y(a|s; θ)
"REGION
FEATURE
DIM",0.9050966608084359,"πold(a|s)A(a, s) + α"
"REGION
FEATURE
DIM",0.9068541300527241,"2 [V (s; ω) −G)]2 + β
X"
"REGION
FEATURE
DIM",0.9086115992970123,"a
π(a|s; θ) log π(a|s; θ).
(29)"
"REGION
FEATURE
DIM",0.9103690685413005,"Theoretically, y(a|s′; θ) for non-sampled states s′ should also be trained with the target y(a|s′; θold).
Yet, in ACH, we only update θ once using a single mini-batch at each iteration. Therefore, θ is equal
to θold before the update. In other words, the policy loss for non-sampled states is 0 in ACH."
"REGION
FEATURE
DIM",0.9121265377855887,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.9138840070298769,"0.2
0.4
0.6
0.8
1.0
Episodes
1e8 101 102"
"REGION
FEATURE
DIM",0.9156414762741653,Exploitability
"REGION
FEATURE
DIM",0.9173989455184535,"ACH(Uniform)
ACH(Current)"
"REGION
FEATURE
DIM",0.9191564147627417,"Figure 9: The exploitability of the current policy in ACH with different behavior policies on FHP. We
report the mean as solid curves and the range as shaded regions across 3 independent runs."
"REGION
FEATURE
DIM",0.9209138840070299,"F
THE EFFECT OF THE BEHAVIOR POLICY ON ACH IN FHP"
"REGION
FEATURE
DIM",0.9226713532513181,"As we noted in the paper, the behavior policy µp,t in ACH could be set to either the current policy
πp,t or simply a uniform sampling policy. As Corollary 1 states, a tighter bound on the exploitability
of the average policy of NW-CFR can be obtained, if µp,t is stationary over iterations. However, since
we have decided to use the current policy (trained with an entropy regularization) for evaluation in
ACH, the effect of the behavior policy is unclear from a theoretical perspective. Hence, we conduct
an experiment to investigate this using FHP, which is a non-trivial poker benchmark but still has the
property that the exact exploitability of an agent can be efﬁciently computed."
"REGION
FEATURE
DIM",0.9244288224956063,"In Figure 9, we compare the exploitability of the current policy of ACH on FHP, by setting the behavior
policy in ACH to either the current policy or a uniform sampling policy. From the comparison, it
seems that the performance of the current policy of ACH is not sensitive to the choice of behavior
policy. One reason might be that the additional entropy loss forces the current policy to be stable and
prone to a uniform random policy. Another reason might be that the current policy instead of the
average policy is evaluated. We will investigate this in more depth in the future."
"REGION
FEATURE
DIM",0.9261862917398945,"G
ADDITIONAL RESULTS ON SMALL IIG BENCHMARKS IN OPENSPIEL"
"REGION
FEATURE
DIM",0.9279437609841827,"We further evaluate ACH and compare it with A2C, RPG, and NeuRD on three benchmarks from
OpenSpiel: Kuhn poker, Leduc poker, and Liar’s Dice. All the experiments were run single-threaded
on a 2.24GHz CPU. We use the network architecture provided in OpenSpiel, which has a 128-
neurons fully-connected layer followed by ReLU and two separate linear layers for the policy and the
state/action value. For A2C, RPG, and NeuRD, we use the default hyper-parameters in OpenSpiel.
ACH shares most of the hyper-parameters with A2C. All the hyper-parameters are listed in the
Appendix H.3."
"REGION
FEATURE
DIM",0.929701230228471,"The exploitability of an agent is exactly calculated using tools in OpenSpiel. For each method, we
compute the exploitability of each agent every 1e5 training steps, the results of which are plotted
in Figure 10. Clearly, ACH converges signiﬁcantly faster and achieves a lower exploitability than
other methods across the three benchmarks. There is still some gap between 0 and the exploitability
ACH converges to. This may due to the neural network approximation error and the fact that we
use the current policy instead of the average policy for the evaluation. As expected, A2C has the
worst performance, since it is designed for single-agent environments. Moreover, the superiority of
ACH is most signiﬁcant on the Liar’s Dice benchmark, which is the most complex one of the three
benchmarks."
"REGION
FEATURE
DIM",0.9314586994727593,"As a complement, we also present the head-to-head performance of A2C, RPG, NeuRD, and ACH on
the three benchmarks in OpenSpiel. As demonstrated in Table 4, the agent of ACH won all other
agents across the three benchmarks."
"REGION
FEATURE
DIM",0.9332161687170475,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.9349736379613357,"0.0
0.2
0.4
0.6
0.8
1.0
Training Steps
1e7 0.0 0.1 0.2 0.3 0.4 0.5"
"REGION
FEATURE
DIM",0.9367311072056239,Exploitability
"REGION
FEATURE
DIM",0.9384885764499121,"A2C
RPG
NeuRD
ACH(Ours)"
"REGION
FEATURE
DIM",0.9402460456942003,(a) Kuhn poker
"REGION
FEATURE
DIM",0.9420035149384886,"0.0
0.2
0.4
0.6
0.8
1.0
Training Steps
1e7 0.5 1.0 1.5 2.0 2.5"
"REGION
FEATURE
DIM",0.9437609841827768,Exploitability
"REGION
FEATURE
DIM",0.945518453427065,"A2C
RPG
NeuRD
ACH(Ours)"
"REGION
FEATURE
DIM",0.9472759226713533,(b) Leduc poker
"REGION
FEATURE
DIM",0.9490333919156415,"0.0
0.2
0.4
0.6
0.8
1.0
Training Steps
1e7 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
"REGION
FEATURE
DIM",0.9507908611599297,Exploitability
"REGION
FEATURE
DIM",0.9525483304042179,"A2C
RPG
NeuRD
ACH(Ours)"
"REGION
FEATURE
DIM",0.9543057996485061,(c) Liar’s Dice
"REGION
FEATURE
DIM",0.9560632688927944,"Figure 10: The training curves of each agent on the three benchmarks from OpenSpiel. We report
the mean as solid curves and the range of the exploitability across 8 independent runs as shaded
regions. We notice that there exists some discrepancy between the NeuRD results on Kuhn poker and
Leduc poker reported here and those reported in Hennes et al. (2020) and Lanctot et al. (2019). The
reason might be due to NeuRD’s sensitivity to running environments including hyper-parameters and
random seeds."
"REGION
FEATURE
DIM",0.9578207381370826,"Kuhn poker
Leduc poker
Liars Dice
A2C
RPG
NeuRD
A2C
RPG
NeuRD
A2C
RPG
NeuRD
RPG
-0.024±0.20
-
-
0.521±0.23
-
-
0.192±0.14
-
-
NeuRD
-0.096±0.16
-0.087±0.16
-
0.569±0.16
-0.108±0.17
-
-0.226±0.05
-0.355±0.12
-
ACH
0.104±0.05
0.115±0.05
0.118±0.05
0.495±0.22
0.050±0.14
0.117±0.15
0.275±0.05
0.122±0.05
0.407±0.06"
"REGION
FEATURE
DIM",0.9595782073813708,"Table 4: Mean (± standard deviation) of the average winning scores of the row agents against the
column agents. The mean and the standard deviation are estimated by 8 independent runs. In each
run, the average winning scores are obtained via 10, 000 head-to-head plays. All the agents are
selected at the 1e7th training step."
"REGION
FEATURE
DIM",0.961335676625659,"H
HYPER-PARAMETERS"
"REGION
FEATURE
DIM",0.9630931458699473,"H.1
HYPER-PARAMETERS FOR THE 1-ON-1 MAHJONG EXPERIMENT"
"REGION
FEATURE
DIM",0.9648506151142355,"We used the Adam optimizer (Kingma & Ba, 2014) for the experiments on the 1-on-1 Mahjong
benchmark. We performed a mild hyper-parameter search on PPO and used the best setting for the
shared hyper-parameters of all methods (PPO, RPG, NeuRD, and ACH). Table 5 gives an overview
of hyper-parameters for each method. Also, we report the performance of the current policy, instead
of the average policy, for each method."
"REGION
FEATURE
DIM",0.9666080843585237,"Parameter
Range
Best
Shared
Ratio clip (ε)
-
0.5
GAE (λ)
-
0.95
Learning rate
{2.5e-3, 2.5e-4}
2.5e-4
Discount factor (γ)
-
0.995
Value loss coefﬁcient (α)
-
0.5
Entropy coefﬁcient (β)
{1e-1, 1e-2}
1e-2
Batch size
{4096, 8192}
8192
NeuRD
Logit threshold (lth)
-
6.0
ACH
Logit threshold (lth)
-
6.0
Hedge coefﬁcient (η(s))
{1.0, 1e-1}
1.0"
"REGION
FEATURE
DIM",0.968365553602812,Table 5: The hyper-parameters used for the 1-on-1 Mahjong experiment.
"REGION
FEATURE
DIM",0.9701230228471002,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.9718804920913884,"H.2
HYPER-PARAMETERS FOR THE FHP EXPERIMENT"
"REGION
FEATURE
DIM",0.9736379613356766,"We used the Adam optimizer (Kingma & Ba, 2014) for the experiments on FHP. All the experiments
on FHP were run multi-threaded and synchronously using ten 2.24GHz CPUs. We update the neural
networks of ACH and other methods (A2C, RPG, and NeuRD) every 1000 episodes, with a batch
consisting all the samples collected within the latest 1000 episodes. Since the average game length of
FHP is around 2, the batch size is roughly 2000. We performed a mild hyper-parameter search for
ACH, A2C, RPG, and NeuRD, as shown in Table 6. Note that we do not need to clip the advantages
in a synchronous method, so the ratio clip hyper-parameter ε is not needed. We multiply the rewards
in FHP with a reward normalizer, as the rewards in FHP are in the range [−700, 700]. Besides, we
found that A2C, RPG, and NeuRD are very sensitive to the entropy coefﬁcient, and we had to use a
larger entropy coefﬁcient in these algorithms than ACH. For OS-DCFR and DREAM, we use the
same hyper-parameters presented in Steinberger et al. (2020). An overview of the hyper-parameters
on FHP is given in Table 6. Note that we report the performance of the current policy for ACH, A2C,
RPG, and NeuRD, while the average policy is used for evaluation in OS-DCFR and DREAM."
"REGION
FEATURE
DIM",0.9753954305799648,"Parameter
Range
Best
Shared
GAE (λ)
-
0.95
Learning rate
{1e-3, 1e-4}
1e-4
Discount factor (γ)
-
0.995
Value loss coefﬁcient (α)
-
2.0
Batch size
-
≈2000
Entropy coefﬁcient (β)
{1e-2, 3e-2, 5e-2}
5e-2
Reward normalizer
-
0.002
NeuRD
Logit threshold (lth)
{2.0, 4.0}
2.0
ACH
Logit threshold (lth)
{2.0, 4.0}
2.0
Entropy coefﬁcient (β)
{1e-2, 3e-2, 5e-2}
3e-2
Hedge coefﬁcient (η(s))
{1.0, 0.1}
1.0"
"REGION
FEATURE
DIM",0.9771528998242531,Table 6: The hyper-parameters used for the FHP experiment.
"REGION
FEATURE
DIM",0.9789103690685413,"H.3
HYPER-PARAMETERS FOR THE EXPERIMENT ON OPENSPIEL"
"REGION
FEATURE
DIM",0.9806678383128296,"We used stochastic gradient descent with a constant learning rate for all the experiments on bench-
marks from OpenSpiel. For A2C and RPG, we used the default implementations and hyper-parameters
in OpenSpiel. NeuRD was originally implemented using the counterfactual regret in OpenSpiel. We
re-implemented NeuRD using predicted advantages and set the shared hyper-parameters of NeuRD
identical to those in RPG. All methods employ an entropy loss to encourage exploration during
training and hopefully the convergence of the current policy to a NE. Also, we report the performance
of the current policy, instead of the average policy, for each method."
"REGION
FEATURE
DIM",0.9824253075571178,"Parameter
Range
Best
Learning rate
{1e-3, 5e-3}
1e-3
Value loss coefﬁcient (α)
{1.0, 2.0}
2.0
Hedge coefﬁcient (η(s))
{1.0, 1e-1, 1e-2}
1.0"
"REGION
FEATURE
DIM",0.984182776801406,Table 7: The hyper-parameter search ranges and best settings of ACH for the OpenSpiel experiment.
"REGION
FEATURE
DIM",0.9859402460456942,"In the OpenSpiel implementations, the value parameters are updated separately and more frequently
compared to the policy parameters for A2C, RPG, and NeuRD. Yet, the value loss and the policy loss
are combined in ACH, and all parameters are updated simultaneously, as illustrated in Algorithm 2."
"REGION
FEATURE
DIM",0.9876977152899824,Published as a conference paper at ICLR 2022
"REGION
FEATURE
DIM",0.9894551845342706,"Parameter
A2C
RPG
NeuRD
ACH
Batch size
4
64
64
64
Critic learning rate
1e-4
1e-2
1e-2
-
Policy learning rate
1e-4
1e-2
1e-2
-
# Critic updates per policy update
32
32
32
-
Entropy coefﬁcient (β)
1e-2
1e-2
1e-2
1e-2
Logit threshold (lth)
-
-
2.0
2.0"
"REGION
FEATURE
DIM",0.9912126537785588,Table 8: The hyper-parameters used for the OpenSpiel experiment.
"REGION
FEATURE
DIM",0.9929701230228472,"Also note that, in a single-threaded training environment, the actor and the learner run in sequence.
As a result, π(a|s; θ) is always identical to πold(a|s) in ACH in Algorithm 2."
"REGION
FEATURE
DIM",0.9947275922671354,"We performed a mild hyper-parameter search for ACH, which is illustrated in Table 7. The ﬁnal
hyper-parameters used for each method are listed in Table 8, with 3 additional hyper-parameters of
ACH listed in the “Best” column in Table 7."
"REGION
FEATURE
DIM",0.9964850615114236,"I
THE RELATIONSHIP BETWEEN ACH AND SUPHX"
"REGION
FEATURE
DIM",0.9982425307557118,"Recently, Suphx (Li et al., 2020b) has achieved stunning performance on Japanese Riichi Mahjong.
The development of Suphx is enabled by a novel integration of existing supervised learning and RL
methods in addition to some newly developed techniques. Three new techniques were introduced
in Suphx: global reward prediction, oracle guiding, and run-time policy adaptation. The global
reward prediction technique is to handle the multi-round game situation, which is irrelevant to our
1-on-1 Mahjong setting (only one round per game in our test setting). The oracle guiding technique
decays the invisible feature during training, which is of independent interest in dealing with imperfect-
information. The run-time policy adaptation technique adapts the trained policy at test time, and this
may be combined with ACH, which is a training algorithm. In summary, Suphx is more of a novel
system than a new algorithm. For this reason, we did not implement and compare Suphx with ACH in
our 1-on-1 Mahjong experiment. Nonetheless, the oracle guiding technique may be complementary
to ACH in handling imperfect-information, and we will investigate this in future work."
