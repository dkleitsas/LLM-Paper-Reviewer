Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"Our ability to know when to trust the decisions made by machine learning systems
has not kept up with the staggering improvements in their performance, limiting
their applicability in high-stakes domains. We introduce Prover-Veriﬁer Games
(PVGs), a game-theoretic framework to encourage learning agents to solve de-
cision problems in a veriﬁable manner. The PVG consists of two learners with
competing objectives: a trusted veriﬁer network tries to choose the correct answer,
and a more powerful but untrusted prover network attempts to persuade the veriﬁer
of a particular answer, regardless of its correctness. The goal is for a reliable
justiﬁcation protocol to emerge from this game. We analyze variants of the frame-
work, including simultaneous and sequential games, and narrow the space down
to a subset of games which provably have the desired equilibria. To complement
our novel learning problem, we propose rigorous methodology for evaluating the
robustness of the learned protocol. We develop instantiations of the PVG for two
algorithmic tasks, and provide empirical evidence that the veriﬁer can in fact learns
a robust decision rule that is able to receive useful and reliable information from
an untrusted prover. Importantly, the protocol still works even when the veriﬁer is
frozen and the prover’s messages are directly optimized to convince the veriﬁer."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION"
INTRODUCTION,0.008797653958944282,"The astonishing performance of today’s dominant learning paradigm – optimizing powerful differ-
entiable function approximators to minimize suitable loss functions – often comes at the cost of
poor robustness and reliability. It is common for powerful deep learning systems to be vulnerable
to adversarial attacks (Goodfellow et al., 2015), to display erratic behaviour on out-of-distribution
data and be very conﬁdent of wrong predictions (Che et al., 2021). How can we train our learning
algorithms to produce outputs that can be checked by humans or by learning algorithms that are
cheaper or better understood?"
INTRODUCTION,0.011730205278592375,"Over the past decades, the ﬁeld of computational complexity has greatly expanded our notion of
proof to include formalisms such as interactive, zero-knowledge, and probabilistically checkable
proofs (Goldreich, 2008). Most of these notions can be thought of in terms of a game between a
powerful but untrusted prover and a computationally limited but trusted veriﬁer. Taking inspiration
from this, we propose the Prover-Veriﬁer Game (PVG), where two learning agents play the role of
prover and veriﬁer, and are allowed to converse. The veriﬁer aims to determine the correct answer to
a decision problem, and the prover aims to convince the veriﬁer of a particular answer (regardless of
its correctness). Since the prover is untrustworthy, the veriﬁer will only ﬁnd its messages useful to
the extent that it can independently verify the information. If all goes well, then the game dynamics
will lead to a proof protocol which, if not mathematically sound, is at least sufﬁcient for the whole
system to achieve more reliable predictions than the veriﬁer can achieve unaided."
INTRODUCTION,0.01466275659824047,"We analyze the desirability of the game equilibria implied by several variants of the Prover-Veriﬁer
Game concept and narrow the space down to a subset of games that theoretically have the desired
equilibria. Picking the right game equilibrium concept turns out to be essential: we prove that
formulating the PVG as a sequential game in which the prover agent plays ﬁrst leads to dysfunctional
solutions. On the other hand, the simultaneous (connected to Nash equilibria) and veriﬁer-ﬁrst
sequential formulations (connected to veriﬁer-leading Stackelberg equilibria) have desirable equilibria.
We formally show, on a illustrative problem, that gradient based differentiable game optimizers can
ﬁnd desirable proof-veriﬁcation protocols."
INTRODUCTION,0.017595307917888565,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020527859237536656,"Do reliable justiﬁcation protocols emerge in practice if we let artiﬁcial agents play the Prover-Veriﬁer
Game? To complement our novel game formulation, we develop a rigorous evaluation methodology
whereby the veriﬁer is frozen and the prover (or the message) is optimized to convince the veriﬁer.
We then run simulations on two algorithmic tasks using a practical instantiation of the PVG concept.
As predicted by our theory, PVG-trained veriﬁers learn to receive useful and reliable information
from untrusted provers by following sensible veriﬁcation protocols, whereas those trained alongside
fully collaborative provers are easily deceived."
BACKGROUND,0.02346041055718475,"2
BACKGROUND"
BACKGROUND,0.026392961876832845,"2.1
INTERACTIVE PROOF SYSTEMS (IPS)"
BACKGROUND,0.02932551319648094,"Interactive proof systems generalize the notion of a mathematical proof to a dialogue between two
agents - a prover and a veriﬁer - to solve a decision problem (Arora & Barak, 2009; Thaler, 2019).
The veriﬁer agent, who is trustworthy but computationally constrained, is tasked with producing a
correct answer to the decision problem. It can exchange messages with a computationally unbounded,
yet potentially adversarial prover agent. The communication protocol between the prover and veriﬁer
constitutes an interactive proof system if and only if it is sound and complete:
Deﬁnition 1 (Completeness and Soundness). The veriﬁer of an interactive proof system is complete
iff there exists a prover that can always convince the veriﬁer that the answer is “yes"", if the correct
answer is “yes"". It is sound iff there doesn’t exist a prover that can trick the veriﬁer into answering
“yes"" if the correct answer is “no""."
DIFFERENTIABLE GAME OPTIMIZATION,0.03225806451612903,"2.2
DIFFERENTIABLE GAME OPTIMIZATION"
DIFFERENTIABLE GAME OPTIMIZATION,0.03519061583577713,"A two-player differentiable game consists of two agents whose strategies are parametrized by w =
(w1, w2) 2 Rd that take turns to minimize their differentiable loss functions (L1, L2) : Rd ! R. An"
DIFFERENTIABLE GAME OPTIMIZATION,0.03812316715542522,"equilibrium concept determines which strategies will be adopted by the players. A Nash equilibrium
is achieved when no player can unilaterally improve its objective function.
Deﬁnition 2 (Nash Equilibrium (Von Neumann & Morgenstern, 2007)). The strategies parametrized
by (w⇤ 1, w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.04105571847507331,"2) constitute a Nash equilibrium1 of the two-player sequential differentiable game with
loss functions (L1, L2) : Rd ! R if they minimize their loss functions keeping the other player’s
parameters ﬁxed: w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.04398826979472141,1 = arg min w1
DIFFERENTIABLE GAME OPTIMIZATION,0.0469208211143695,"L1(w1, w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.04985337243401759,"2),
w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.05278592375366569,2 = arg min w2 L2(w⇤
DIFFERENTIABLE GAME OPTIMIZATION,0.05571847507331378,"1, w2)
(1)"
DIFFERENTIABLE GAME OPTIMIZATION,0.05865102639296188,"The notion of equilibrium considered by Generative Adversarial Networks (Goodfellow et al., 2014)
is an example of a Nash Equilibrium. A Stackelberg equilibrium differs from the Nash equilibrium in
that one of the players is deemed the “leader"" and the other the “follower"" (Wang* et al., 2020). It is
assumed that the follower always picks the optimal strategy for a given leader strategy. In response,
the leader modiﬁes its strategy by factoring in how the follower agent will respond to the modiﬁcation."
DIFFERENTIABLE GAME OPTIMIZATION,0.06158357771260997,"Deﬁnition 3 (Stackelberg Equilibrium (Fiez et al., 2020)). Let w1 parametrize the strategy of the
“leader agent"" and w2 parametrize that of the “follower agent"". The loss functions of the agents are L1"
DIFFERENTIABLE GAME OPTIMIZATION,0.06451612903225806,"and L2 respectively. The strategies parametrized by (w1, w2) constitute a Stackelberg equilibrium1
if and only if (1) the follower’s strategy is optimal given the leader’s strategy, and (2) the leader’s
strategy is optimal taking into consideration how the follower will respond to modiﬁcations. w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.06744868035190615,1 = arg min w1
DIFFERENTIABLE GAME OPTIMIZATION,0.07038123167155426,"L1(w1, w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.07331378299120235,"2(w1)),
w⇤"
DIFFERENTIABLE GAME OPTIMIZATION,0.07624633431085044,2(w1) = arg min w2 L2(w⇤
DIFFERENTIABLE GAME OPTIMIZATION,0.07917888563049853,"1, w2)
(2)"
PROVER-VERIFIER GAME,0.08211143695014662,"3
PROVER-VERIFIER GAME"
PROVER-VERIFIER GAME,0.08504398826979472,"The Prover-Veriﬁer Game aims to learn decision rules with a reliable internal veriﬁcation step. We
describe what we mean for a veriﬁcation protocol to be reliable, then outline the game formulation."
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.08797653958944282,"3.1
DESIDERATA AND PROVER-VERIFIER INCENTIVES"
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.09090909090909091,"Desiderata: Designing the right prover-veriﬁer game requires precisely deﬁning what a desirable
outcome/equilibrium of the game looks like. With the “completeness"" and “soundness"" deﬁnitions in
mind (Section 2.1), we list the properties we seek in a desirable veriﬁer protocol:"
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.093841642228739,1We assume uniqueness of equilibria for simplicity.
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.0967741935483871,Under review as a conference paper at ICLR 2022
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.09970674486803519,"(a) Pattern to detect.
(b) When the pattern exists.
(c) When the pattern doesn’t exist."
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.10263929618768329,"Figure 1: Depiction of a proof-veriﬁcation protocol learned via Prover-Veriﬁer Training. When
trained on the task of detecting whether there’s a blue plus pattern (1a) in the input image, Prover-
Veriﬁer Training discovers a coordinate-based veriﬁcation protocol. If there’s a plus in the image
(1b), the prover sends its coordinate, and the veriﬁer accepts this certiﬁcate. If there’s not a plus in
the image (1c), then the prover sends the coordinates of a maximally convincing image patch, but the
veriﬁer rejects this certiﬁcate."
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.10557184750733138,• Possibility of Perfect Recall: There should exist a prover that can help the veriﬁer achieve
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.10850439882697947,"perfect recall — the ratio of true positive predictions to all positive examples.
• Guarantee of Perfect Precision:
There shouldn’t exist any prover which can trick the
veriﬁer into achieving non-perfect precision — the ratio of true positive predictions to all
positive predictions."
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.11143695014662756,"“Possibility of perfect recall"" is connected to completeness, and implies that with the right proofs, the"
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.11436950146627566,"veriﬁer can achieve zero false negative rate. “Guarantee of perfect precision"" is related to soundness
and implies that the veriﬁer always has zero false positive rate regardless of which proof is used."
DESIDERATA AND PROVER-VERIFIER INCENTIVES,0.11730205278592376,"Picking the right prover-veriﬁer incentives: We propose to set up the prover-veriﬁer game with
the following incentive structure: the veriﬁer is incentivized to always give the correct answer to
the given decision problem, and the prover is incentivized to get the veriﬁer to answer “yes"" to the
decision problem, regardless of the correct answer. This structure encourages collaborative dynamics
when the correct answer to the decision problem is “yes"" (which is linked to recall) and encourages
adversarial dynamics if the correct answer is “no"" (which is linked to precision). As we will prove
formally in Section 3.3, this incentive structure, when embedded in the right game setup with the
right loss functions, can lead to a proof-veriﬁcation protocol that meets the desiderata."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.12023460410557185,"3.2
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS"
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.12316715542521994,"Notation Let x ⇠pX(x), y ⇠pY (y) and y0 ⇠pY 0(y0) be the input, veriﬁer label and prover label
random variables where we have x 2 Rnx, y 2 {0, 1, ..., K} and y0 2 {0, 1, ..., K}. Let their joint
distribution be pD."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.12609970674486803,"The prover agent Pwp, parametrized by wp 2 Rnp, outputs the conditional probability distribution
pp(z|x) over real valued “message"" vectors z 2 Rnz conditioned on inputs. The veriﬁer agent Vwv,
parametrized by wv 2 Rnv, represents the conditional probability distribution pv(ˆy|x, z) over the
predicted labels ˆy 2 {0, 1, ..., K}, conditioned on the inputs x and messages z."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.12903225806451613,Loss Functions: We pick the following loss functions for the veriﬁer and prover:
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.13196480938416422,"Lv = E(x,y,y0)⇠pD, z⇠pp(z|x)[−log pv(y|x, z)]
(3)"
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.1348973607038123,"Lp = E(x,y,y0)⇠pD, z⇠pp(z|x)[−log pv(y0|x, z)]
(4)"
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.1378299120234604,"Since we wish for the veriﬁer agent to try and solve the problem to the best of its ability, we set
pY to be the correct label distribution. Since we wish to incentivize the prover to try and defend a"
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.14076246334310852,Under review as a conference paper at ICLR 2022
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.1436950146627566,"particular answer regardless of its correctness, we set pY 0(y0) = 1 where y0 2 {0, 1, . . . , K} is the
output that the prover is defending. In a decision problem, y0 will be one of 0 or 1. One can consider
variations on these loss functions by replacing the negative log with other functions that are convex
and monotonically increasing. We prefer the aforementioned loss functions when the veriﬁer agent
represents a softmax policy over its outputs. This guarantees that the gradients vanish iff the agents
fulﬁll their incentives (see Appendix A)."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.1466275659824047,"Suitable Equilibrium concepts: There are three obvious ways to set up the order in which the
agents play (i.e. pick their strategies parametrized by wp and wv) the Prover-Veriﬁer Game: 1) The
prover and veriﬁer play simultaneously, 2) The prover plays ﬁrst, 3) The veriﬁer plays ﬁrst. The
simultaneous setup leads to picking Nash equilibrium as the equilibrium concept. The latter two lead
to picking Stackelberg equilibrium, where prover or veriﬁer is the leader and can therefore reason
about how the opponent will respond to a given strategy. Interestingly, as we show in Section 3.3, not
all of these formulations lead to equilibria that satisfy our desiderata."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.1495601173020528,"When Instances are Revealed:
We can also arrive at different game formulation depending on
whether the problem instances (i.e., the input and prover-veriﬁer labels) are revealed to the agents
before or after they pick their strategies (i.e. weights). This leads to eight different game formulations:
two from the simultaneous setup (before or after the simultaneous moves) and six from the sequential
setup (before, in-between and after the moves, for each of the two sequential formulations)."
DIFFERENT PROVER-VERIFIER GAME FORMULATIONS,0.15249266862170088,"How the Prover and Veriﬁer Interact: The design space of how the prover and veriﬁer interact
with each other is large. We only consider the single-step, no feedback formulation throughout the
paper and leave the analysis of other formulations as future work. Note that the single-step, no
feedback formulation is related to NP proof systems and include sophisticated proof-veriﬁcation
protocols. We can also derive numerous PVG formulations by modifying the communication channel
between the prover and veriﬁer. Various decisions regarding the channel include: 1) If the channel
is stochastic or deterministic (and what kind of noise there is) 2) Whether prover’s messages are
real valued vectors, or discrete tokens 3) Whether the prover and veriﬁer use an already constructed
communication protocol, such as natural language."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.15542521994134897,"3.3
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA"
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.15835777126099707,"We list in Table 1 all possible game instantiations obtained by varying the 1) player order and 2)
when the problem instance is revealed. Our goal is to ﬁnd which – if any – of these formulations have
desirable equilibria. We consider the following abstract decision problem formulation to compare
and contrast different PVG formulations:
Deﬁnition 4 (Abstract Decision Problem). Let X, M, Y = {0, 1} and ˜Y = {0, 1} be the (discrete)
sets of all inputs, prover messages, and labels, respectively. Let PX represent the input distribution.
Assume the label is a deterministic function of the input, y = f(x), and the prover is always arguing"
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.16129032258064516,"for the label y0 = 1. The prover agent can choose a policy p 2 P, where P is a set of mappings
X ! M. Let V denote a set of base veriﬁers, which are mappings X ⇥M ! {0, 1}. The veriﬁer
chooses a policy v which computes a convex combination of elements of V. (The veriﬁer’s output
can be interpreted as the probability it assigns to the label being 1). The veriﬁer and prover try to
minimize the loss functions deﬁned in Equations 3 and 4 respectively. We assume that both P and V
include every constant mapping (i.e. ones which don’t depend on the input)."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.16422287390029325,Assume that there exists a deterministic veriﬁer policy v⇤2 V such that:
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.16715542521994134,"9p 2 Cp : 8x 2 X : v⇤(x, p(x)) = f(x) (completeness)
(5)
8p 2 Cp : 8x 2 X : v⇤(x, p(x)) = 1 =) f(x) = 1 (soundness)
(6)
For instance, to represent a decision problem from complexity theory, we could take X and M to be
sets of binary strings, P to be the set of all mappings X ! M (the prover is typically computationally
unrestricted), and V the set of mappings implementable by boolean circuits with some maximum
size. However, we keep the formalism general because our main results don’t depend on the details
of boolean circuits or complexity theoretic assumptions."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.17008797653958943,"The conclusions of our theoretical analysis are displayed in Table 1. Proper proof-veriﬁcation
protocols constitute equilibria of only three of the eight game formulations: the simultaneous setup
where the problem instance is presented after the agents’ moves, and the two veriﬁer-ﬁrst sequential
setups where the problem instances are revealed after the veriﬁer’s move. We justify these claims
through the following theorems. The proofs are presented in Appendix B."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.17302052785923755,Under review as a conference paper at ICLR 2022
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.17595307917888564,"Table 1: The summary of our theoretical results on the desirability of equilibria of different PVG
formulations. The “ordering"" column illustrates in which order the prover and veriﬁer pick their
strategies (weights) and the problem instance (the input and labels) are revealed. The “necessity"" and
“sufﬁciency"" columns show whether having converged to an equilibrium is necessary and/or sufﬁcient"
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.17888563049853373,"for having discovered and complete and sound proof-veriﬁcation protocol. The “failure mode""
column describe in what way some of the setups lead to undesirable equilibria. The “convergence""
column speciﬁes if gradient descent converges to the eqilibrium on the Binary Erasure Channel Task.
(⇤)We’ve only shown local convergence for the veriﬁer-leading equilibria."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.18181818181818182,"Ordering
Necessity
Sufﬁciency
Failure Mode
Convergence"
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.18475073313782991,"{Prover, Veriﬁer}, Instance
3
7
coordination problem
3
Instance, {Prover, Veriﬁer}
7
7
trivial veriﬁer
7
Veriﬁer, Prover, Instance
3
3
none
3⇤
Veriﬁer, Instance, Prover
3
3
none
3⇤
Instance, Veriﬁer, Prover
7
7
trivial veriﬁer
7
Prover, Veriﬁer, Instance
7
7
ﬂood-the-zone
7
Prover, Instance, Veriﬁer
7
7
trivial veriﬁer
7
Instance, Prover, Veriﬁer
7
7
trivial veriﬁer
7"
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.187683284457478,"Proposition 1. Any PVG formulation in which the veriﬁer is given the problem instance before
it selects its strategy has bad equilibria — that is, having converged to an equilibrium is neither
necessary nor sufﬁcient for having found a complete and sound proof veriﬁcation protocol."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.1906158357771261,"If the instance x is known when the veriﬁer chooses its policy, then the veriﬁer can simply choose the
constant policy which returns f(x). We dub this the “trivial veriﬁer"" failure mode.
Theorem 1. Consider all prover-leading sequential PVG formulations. Having reached a (prover
leading Stackelberg) equilibrium is neither necessary nor sufﬁcient for having found a complete and
sound proof-veriﬁcation protocol."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.1935483870967742,"The intuition behind this result is as follows: no matter what strategy the prover picks as the leader,
the veriﬁer can always ﬁnd a strategy whose outputs match the label marginals (i.e. at least 50%
accuracy if the labels are balanced). Due to the algebraic form of the prover loss function, the best
the prover can do is to avoid giving the veriﬁer any information about the information whatsoever
about the label. This defective incentive prevents the learning of proper proof-veriﬁcation protocols.
We dub this the “ﬂood-the-zone"" failure mode, in reference to the media disinformation strategies
that pollute the information ecosystem to prevent people from knowing what’s true Illing (2020).
Theorem 2. Consider all veriﬁer-leading sequential PVG formulations in which the problem instance
is revealed after the veriﬁer picks its strategy. Having reached a (veriﬁer leading Stackelberg)
equilibrium is both necessary and sufﬁcient for having found a desirable proof-veriﬁcation protocol."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.19648093841642228,"Necessary is easy to argue: given a complete and sound protocol, neither player has an incentive
to deviate. Sufﬁciency can be shown using a “proof by contrapositive"" argument: If the protocol
represented by prover and veriﬁer is not complete and sound, then they cannot possibly represent a
veriﬁer-leading Stackelberg equilibrium, because the veriﬁer can do strictly better by switching to the
complete and sound veriﬁcation system (as the prover will react by producing correct proofs).
Theorem 3. Consider the simultaneous PVG formulation where the problem instance is revealed
after the agents pick their strategies. Having converged to a (Nash) equilibrium is necessary for
having found a complete and sound proof-veriﬁcation protocol."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.19941348973607037,"If the prover and veriﬁer already represent a complete and sound proof-veriﬁcation protocol, the
veriﬁer’s loss function is already minimized, hence the veriﬁer won’t change its strategy. Because
the prover can already convince the veriﬁer when it’s acting collaboratively (due to completeness)
and cannot fool it when it’s acting adversarially (due to soundness), it also won’t have the incentive
to change its strategy. Therefore, the prover-veriﬁer strategies constitute Nash equilibria. This also
implies that all Stackelberg equilibria of the proof-veriﬁcation game are also Nash equilibria:
Corollary 1. All veriﬁer-leading Stackelberg equilibria are Nash equilibria of PVG."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.20234604105571846,"There might exist Nash equilibria that don’t correspond to complete and sound protocols. For instance,
the case in which the veriﬁer completely ignores the prover constitutes a Nash equilibrium. We call
this the “coordination problem"" failure mode: no player can adopt a protocol unilaterally."
PVG FORMULATIONS WITH DESIRABLE EQUILIBRIA,0.20527859237536658,Under review as a conference paper at ICLR 2022
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.20821114369501467,"3.4
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.21114369501466276,"Section 3.3 establishes that there exist PVG formulations whose equilibria, if found, will constitute
complete and sound proof-veriﬁcation protocols. Is there hope in ﬁnding these equilibria using
gradient based differentiable game optimizers? We thereon use the phrase “Prover-Veriﬁer Training""
to refer to the general method of using gradient-based optimizers to solve for PVG equilibria."
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.21407624633431085,"To address the question above, we turn to a simpler problem we call “Binary Erasure Channel"" (BEC)
which admits detailed analysis. The BEC problem is deﬁned as follows:
Deﬁnition 5. We have a prover-veriﬁer system. The prover takes in input x 2 {0, 1} and attempts to
convince the veriﬁer to output 1 all the time whatever the input is. The veriﬁer aims at predicting the
input correctly only based on the message from the prover. The prover deﬁnes a stochastic function
f : {0, 1} ! {0, 1, ..., K} , M and is parameterized by probability vectors p0 = [p0"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.21700879765395895,"0, ..., p0"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.21994134897360704,"K] and
p1 = [p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.22287390029325513,"0, ..., p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.22580645161290322,"K] while the veriﬁer deﬁnes g : M ! {0, 1} and is parameterized by q 2 RK+1 with
qi representing the probability of predicting 1 when receives token i 2 M."
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.2287390029325513,"We impose some further restrictions: the prover cannot send the message 1 when x = 0 and cannot
send 0 when x = 1. With this restriction, the communication channel between the prover and veriﬁer
resemble the binary erasure channel Cover (1999) with the tokens {2, ..., K} representing multiple
erasure tokens. This setup is a simpliﬁcation of the more general assumption that there exists a
complete and sound veriﬁcation protocol that is realizable by the prover and veriﬁer. In this setting
(given loss functions in Eq. equation 3 and equation 4), the joint strategy of prover sending the right
proof with p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.2316715542521994,1 = 1 and veriﬁer proceeding with q1 = 1 but qi = 0 for all i 6= 1 is a Nash equilibrium.
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.23460410557184752,"Moreover, we add entropy regularization (i.e., label smoothing) to the veriﬁer loss to avoid degenerate
cases in training. We can show that running standard gradient descent converges to the ideal joint
strategy under the simultaneous game formulation. In more detail, we have the following theorem.
Theorem 4. Starting from any initialization with p0"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.2375366568914956,"i > 0, p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.2404692082111437,"i > 0 for all i 2 M, running gradient
descent on both prover and veriﬁer with reasonable learning rates, the prover would converge to only
sending message 1 when x = 1 while the veriﬁer would predict 1 only if it receives message 1 from
the prover. To put it differently, p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.2434017595307918,"1 = 1 when converge while q1 = 1 −✏and q0 = q2 = ... = qK = ✏
with ✏a small constant depending on the strength of entropy regularization."
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.24633431085043989,"Formulating the PVG as a sequential game in which the prover agent plays ﬁrst leads to undesirable
solutions. We summarize the the result below.
Theorem 5. Starting from any initialization with p0"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.24926686217008798,"i > 0, p1"
FINDING PROOF-VERIFICATION STRATEGIES USING GRADIENT-BASED OPTIMIZERS,0.25219941348973607,"i > 0 for all i 2 M, running gradient
descent on both prover and veriﬁer with any learning rates, prover would converge to only sending
message 2 to K no matter what the input is."
RELATED WORK,0.25513196480938416,"4
RELATED WORK"
RELATED WORK,0.25806451612903225,"Computational Complexity: Much of computational complexity theory is centered around various
notions of proofs, from the basic deﬁnition of NP to more modern notions of proof such as interactive,
zero-knowledge, and probabilistically checkable proofs (Goldreich, 2008). These various notions of
proof can be understood in terms of a game between a prover and veriﬁer somewhat analogous to
ours. Our game is most directly analogous to the vanilla notion of proof used to deﬁne NP. But given
the right architecture, our game would allow the prover’s message to encode the weights of a network
that answers the veriﬁer’s questions, thereby more closely resembling an interactive proof."
RELATED WORK,0.26099706744868034,"Interpretability: Selective rationalization is the task of highlighting input features that are useful for
classiﬁcation. A number of game theoretic selective rationalization methods have been proposed (Lei
et al., 2016; Yu et al., 2019; Chang et al., 2019). With text applications in mind, Lei et al. (2016)
propose a two player, collaborative game where a generator network selects relevant features from
the input text, and a predictor network does classiﬁcation using those features. Chang et al. (2019)
propose a six player game where the 4 generators select factual or counterfactual rationales, and the 2
discriminators aim to distinguish factual and counterfactual rationales. In practice, only 3 players
are instantiated (two generators that only take negative or positive examples and a discriminator).
Game theoretic approaches have also been used for interpretable fact-checking on knowledge graphs.
Hildebrandt et al. (2020) propose solving the triplet classiﬁcation task (deciding whether a subject-
predicate-object triple is contained in a knowledge graph) using debate dynamics. Two reinforcement"
RELATED WORK,0.26392961876832843,Under review as a conference paper at ICLR 2022
RELATED WORK,0.2668621700879765,"learning agents learn to generate arguments (paths in a knowledge graph) that either support or go
against a fact (a triple). These are then fed to a classiﬁer trained to ascertain the truth value of the fact."
RELATED WORK,0.2697947214076246,"Verifying correctness and functional properties of models: There’s a growing body of literature
aimed at verifying both the correctness and/or functional properties of learned models. Goldwasser
et al. (2021) investigate interactive proof systems to verify correctness. They show cases where PAC2
veriﬁcation of hypotheses are signiﬁcantly more sample efﬁcient, even when using a single-round
protocol. Works that aim to verify functional properties of models (such as robustness to adversarial
perturbations) can be categorized under three groups (Kohli et al., 2019): those that test consistency
with speciﬁcations (Ruderman et al., 2018; Uesato et al., 2018), those that train speciﬁcation consistent
models (Raghunathan et al., 2018; Wong et al., 2018; Mirman et al., 2018; Dvijotham et al., 2018)
and those that formally verify adherence to speciﬁcations (Katz et al., 2017; Dvijotham et al., 2018;
Singh et al., 2018). These works are orthogonal - our notion of veriﬁcation differs from that of the
software analysis/veriﬁcation community that is concerned with showing models/pieces of code meet
speciﬁcations for all inputs - as we aim to learn proof-veriﬁcation protocols to solve a given task."
RELATED WORK,0.2727272727272727,"AI Safety: Being able to verify the correctness of the outputs produced by powerful but potentially ad-
versarial or brittle learning systems is a top objective of the AI Safety community. Irving et al. (2018)
propose a two player debate game where two agents communicate (space-constrained) statements to
convince the judge of the correctness of opposing statements. Our approach is different in that it uses
only a single debater (prover). We also differ signiﬁcantly in our focus: while Irving et al. (2018)
believed the presence of adversarial debaters would itself lend robustness, we are more interested in
using the system to discover proof protocols. We believe the latter philosophy captures the spirit of
the adversarial debate game from the rationalist community (Barnes & Christiano, 2020), which is
intended to determine which forms of (human) argumentation lead most reliably to true conclusions.
Che et al. (2021) propose Deep Veriﬁer Networks, which uses a conditional variational autoencoder to
detect “unreliable inputs or predictions.” Unlike our approach, this approach hard-codes the structure
of the veriﬁcation algorithm, and considers out-of-distribution detection applications."
SIMULATIONS,0.2756598240469208,"5
SIMULATIONS"
PRACTICAL DESIGN DECISIONS,0.2785923753665689,"5.1
PRACTICAL DESIGN DECISIONS"
PRACTICAL DESIGN DECISIONS,0.28152492668621704,We elaborate on some design decisions we made for running prover-veriﬁer training in practice.
PRACTICAL DESIGN DECISIONS,0.2844574780058651,"Practical Constraints on the Veriﬁer: If the veriﬁer agent is instantiated using neural networks,
then one can constrain the veriﬁer by varying the architecture and/or reducing the size and depth
of the network. While picking a suitable veriﬁer constraint on toy problems might be tricky, we
argue that this no longer remains an issue if the decision problem is inherently difﬁcult to solve. For
example, it is easy to ﬁnd veriﬁer architecture that cannot solve problems that have large search
spaces (such as subset sum, SAT problems, or problems in NP in general), yet can check the proofs."
PRACTICAL DESIGN DECISIONS,0.2873900293255132,"Picking the Differentiable Game Optimizer: We use standard gradient based optimizers (such as
Adam (Kingma & Ba, 2014)) with alternating updates to ﬁnd the equilibria of the prover-veriﬁer
game, since it was shown by Zhang et al. (2021) that simple alternating gradient descent-ascent is on
par with more sophisticated algorithms for adversarial games. We suspect that some advances in the
optimization of generative adversarial networks could help, and leave this as future work."
PRACTICAL DESIGN DECISIONS,0.2903225806451613,"Solving for Nash Equilibria: Since computing the response gradients required to solve for Stack-
elberg equilibria is often computationally challenging, we opt to solve for Nash equilibria in our
simulations. Since all Stackelberg equilibria of the proof-veriﬁcation game are also Nash equilibria,
it is a viable strategy to ﬁnd a Nash equilibrium then check whether it also corresponds to a veriﬁer-
leading Stackelberg equilibrium. Also, the convergence properties of the simultaneous setup are
favourible, as suggested by Theorem 4."
PRACTICAL DESIGN DECISIONS,0.2932551319648094,Training Heuristics: We list heuristics that improve speed and stability of training in Appendix D.
EVALUATING SUCCESS,0.2961876832844575,"5.2
EVALUATING SUCCESS"
EVALUATING SUCCESS,0.2991202346041056,"The aim of the PVG is to produce a proof system which is both sound and complete. If this is the
case, then we can expect the veriﬁer to achieve high accuracy. However, the veriﬁer’s accuracy"
EVALUATING SUCCESS,0.3020527859237537,"2Probably Approximately Correct (Valiant, 1984)"
EVALUATING SUCCESS,0.30498533724340177,Under review as a conference paper at ICLR 2022
EVALUATING SUCCESS,0.30791788856304986,"Table 2: (Precision and Recall on Binary Erasure Channel Task:) We froze the veriﬁer networks
obtained via. Prover-Veriﬁer Training and collaborative training and evaluated their precision and
recall against both an optimized prover and directly optimized messages. The PVG-trained veriﬁer
achieved perfect speciﬁcity, whereas the collaborative-trained veriﬁer achieved exactly 0 speciﬁcity."
EVALUATING SUCCESS,0.31085043988269795,"Frozen Veriﬁer Recall
Frozen Veriﬁer Precision"
EVALUATING SUCCESS,0.31378299120234604,"PVG
Collaborative
PVG
Collaborative"
EVALUATING SUCCESS,0.31671554252199413,"Against Optimized Prover
1.0
1.0
1.0
0.0
Against Optimized Messages
1.0
1.0
1.0
0.0"
EVALUATING SUCCESS,0.3196480938416422,"during training does not constitute a good yardstick for evaluating success, since a high accuracy
for the veriﬁer could instead reﬂect problematic game dynamics. For example, high accuracy can
be explained by the veriﬁer detecting the prover’s “tell” (spurious features in prover messages that
correlate with the labels) rather than checking the proof. Also, we would like the proof protocol to
generalize upwards to provers that are more capable than the one seen at training time."
EVALUATING SUCCESS,0.3225806451612903,"Therefore, we propose to stress-test the robustness of the veriﬁer by freezing it and giving a lot more
resources to the prover. We did this in two different ways: First, we would freeze the veriﬁer and
continue to train the prover. Second, we would initialize messages to the prover’s output and directly
optimize the messages (to maximize the veriﬁer’s acceptance logit) using powerful optimizers, such
as L-BFGS. Both of these techniques remove the veriﬁer’s ability to recognize the prover’s tell, and
also provide a degree of conﬁdence in the robustness of the proof system. (It remains possible that
there exist incorrect proofs which the veriﬁer would accept but which are difﬁcult to ﬁnd.) We report
the precision and recall of the frozen veriﬁer against the two methods augmenting the prover."
EVALUATING SUCCESS,0.3255131964809384,"The stress-tests outlined above can be seen as a way of checking if the veriﬁer represents a veriﬁer-
leading Stackelberg equilibrium. Optimizing the prover corresponds to the setup where the problem
instance is presented after agents pick their strategies. Optimizing the prover messages links to the
setup where the instance is known to the prover (see Table 1)."
EVALUATING SUCCESS,0.3284457478005865,"We emphasize that our proposed evaluation methodology is essential to measuring progress on this
problem: early on in our investigations, we were fooled into thinking the networks had learned a
correct protocol because the veriﬁer achieved high accuracy during training. However, we later
determined that this high accuracy simply reﬂected the veriﬁer learning the prover’s “tell” faster
than the prover learned to hide it. Since prior work aiming at similar goals to ours (Irving et al.,
2018; Perez et al., 2019) reported only ﬁnal accuracy during training, we consider our frozen veriﬁer
evaluation methodology an important contribution."
RESULTS,0.3313782991202346,"5.3
RESULTS"
RESULTS,0.3343108504398827,"Binary Erasure Channel Experiments: We used the binary erasure channel (BEC) task described
in Section 3.4 Deﬁnition 5 as a test bed to check whether our theoretical results on the desirability of
equilibria are matched in practice. We parametrized the prover as a small fully connected network
that takes one-hot-encoded input bits [1, 0] or [0, 1] as inputs and outputs logits over the K = 16
communication tokens. The veriﬁer was instantiated as a small network that takes in the one-hot
encoded communication tokens as inputs and outputs logits that are used to compute the probability
the veriﬁer assigns to the labels 0 and 1. We sampled the input bits uniformly, which means the labels
were balanced. We used the Adam optimizer (Kingma & Ba, 2014) with its PyTorch defaults (Paszke
et al., 2019), batch size of 2000 and learning rate of 0.0003. We updated the veriﬁer 5 times for every
prover update. Details about training setup are described in Appendix E.1."
RESULTS,0.33724340175953077,"To assess completeness and soundness, we computed the frozen veriﬁer precision and recall against
1) a fully optimized prover and 2) fully L-BFGS optimized message vectors. The details for how we"
RESULTS,0.34017595307917886,"optimized the prover and message vectors are given in the Appendix E.1. We compared these results
with those obtained using prover and veriﬁer agents trained fully collaboratively (i.e. prover’s labels
always matches with the veriﬁers). The results, shown in Table 2, reafﬁrm our hypothesis that the
PVG-trained frozen veriﬁer achieved perfect precision and recall - both against the optimized prover
and optimized messages. The collaboratively trained veriﬁer had zero precision, hence is not robust."
RESULTS,0.34310850439882695,"In Figure 2, we visualize how the outputs (i.e. logits) of the prover and veriﬁer networks for each
different input evolve over time, accompanied by the accuracy of the veriﬁer. We highlight - with
bolded blue and orange curves - how the prover and veriﬁer treat the special “0"" and “1"" tokens. The"
RESULTS,0.3460410557184751,Under review as a conference paper at ICLR 2022
RESULTS,0.3489736070381232,"Table 3: (Precision and Recall on Find-The-Plus Task:) We froze the veriﬁer networks obtained
via Prover-Veriﬁer Training and collaborative training and evaluated their speciﬁcity and recall against
both an optimized prover and directly optimized prover messages. The PVG-trained veriﬁer achieved
near-perfect speciﬁcity against the optimized prover and optimized messages. The speciﬁcity of the
baseline was zero in all cases."
RESULTS,0.3519061583577713,"Frozen Veriﬁer Recall
Frozen Veriﬁer Precision"
RESULTS,0.3548387096774194,"PVG
Collaborative
PVG
Collaborative"
RESULTS,0.35777126099706746,"Against Optimized Prover
1.0
1.0
1.0
0.0
Against Optimized Messages
1.0
1.0
0.99
0.0"
RESULTS,0.36070381231671556,"veriﬁer quickly picks up on the fact that receiving a “0"" is indicative of the label being “0"". Shortly
thereafter (around iteration 400) the veriﬁer learns the desired veriﬁcation protocol."
RESULTS,0.36363636363636365,"Figure 2: BEC training dynamics: The evolution
of the logits, per input-output pair, of the prover (top)
and veriﬁer (middle), accompanied by the accuracy
of the veriﬁer (bottom). The veriﬁer quickly picks the
correct veriﬁcation protocol, which drives the whole
system into the desired equilibrium."
RESULTS,0.36656891495601174,"FindThePlus Experiments: We tested the
efﬁcacy of prover-veriﬁer games on a visual
search task we call “FindThePlus”. As in
Figure 1, the dataset consists of binary 10⇥10
blue-red images that contain a single 3 ⇥3
plus pattern in them. Labels are determined
by the color of the plus: images with a red
plus are assigned to class 1 and images with a
blue plus are assigned to class 0. The veriﬁer
is constrained to only process a portion of
the input image, so it depends on the prover
to advise it “where to look” (advice which
it is free to ignore). We parametrized the
prover as a convolutional net followed by a
fully connected layer. We parametrized the
veriﬁer with a Spatial Transformer Network-
like (Jaderberg et al., 2015) architecture with
multiple heads."
RESULTS,0.36950146627565983,"The training details are provided in Appendix
E.2. The frozen veriﬁer precision and recall
corresponding to Prover-Veriﬁer training, as
well as collaborative training are shown in
Table 3. We optimized the prover and the
messages using the same setup we used in
the Binary Erasure Channel experiments. As
before, the veriﬁer trained under the prover-
veriﬁer game dynamics achieves perfect pre-
cision against the optimized prover, while the
veriﬁer trained under the standard, collabo-
rative learning dynamics achieves exactly 0
precision. The frozen veriﬁer precision was
0.99 if we directly optimized the proof vec-
tors. We visualized where the Prover-Veriﬁer
trained veriﬁer looks using the proof vectors
from the prover. An example is displayed in
Figure 1 (more in Appendix E.2). These vi-
sualizations indicate that not only does the prover send the coordinate of the blue plus whenever it
exists, it also sends the coordinates of maximally convincing patches if there’s not a blue plus in the
image."
CONCLUSION,0.3724340175953079,"6
CONCLUSION
We proposed Prover-Veriﬁer Games (PVG), a game-theoretic framework to encourage neural networks
to solve decision problems in a veriﬁable manner. We provided a thorough analysis of the problem
space, and eliminated game formulations that have undesirable equilibria. Using a rigorous evaluation
framework, we demonstrated - both theoretically and empirically - that there exist PVG formulations
that can learn robust proof-veriﬁcation protocols in practice."
CONCLUSION,0.375366568914956,Under review as a conference paper at ICLR 2022
REFERENCES,0.3782991202346041,REFERENCES
REFERENCES,0.3812316715542522,Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Univer-
REFERENCES,0.3841642228739003,"sity Press, 2009."
REFERENCES,0.3870967741935484,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint"
REFERENCES,0.39002932551319647,"arXiv:1607.06450, 2016."
REFERENCES,0.39296187683284456,"Beth Barnes and Paul Christiano. Writeup: Progress on AI safety via. debate. LESSWRONG,"
URL,0.39589442815249265,"2020.
URL
https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/
writeup-progress-on-ai-safety-via-debate-1."
URL,0.39882697947214074,"Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. A game theoretic approach to class-wise"
URL,0.40175953079178883,"selective rationalization. Advances in Neural Information Processing Systems, 32:10055–10065,
2019."
URL,0.4046920821114369,"Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming Xiong, and Yoshua Bengio."
URL,0.40762463343108507,"Deep veriﬁer networks: Veriﬁcation of deep discriminative models with deep generative models.
In AAAI, 2021."
URL,0.41055718475073316,"Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999."
URL,0.41348973607038125,"Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O’Donoghue,"
URL,0.41642228739002934,"Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned veriﬁers. arXiv
preprint arXiv:1805.10265, 2018."
URL,0.41935483870967744,"Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in Stackelberg games:"
URL,0.4222873900293255,"Equilibria characterization, convergence analysis, and empirical study. In Proceedings of the 37th
International Conference on Machine Learning, 2020."
URL,0.4252199413489736,"Oded Goldreich. Probabilistic proof systems: A primer. Now Publishers Inc, 2008."
URL,0.4281524926686217,"ShaﬁGoldwasser, Guy N Rothblum, Jonathan Shafer, and Amir Yehudayoff. Interactive proofs"
URL,0.4310850439882698,"for verifying machine learning. In 12th Innovations in Theoretical Computer Science Conference
(ITCS 2021). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2021."
URL,0.4340175953079179,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,"
URL,0.436950146627566,"Aaron Courville, and Yoshua Bengio.
Generative adversarial nets.
In Advances in neural
information processing systems, pp. 2672–2680, 2014."
URL,0.4398826979472141,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial"
URL,0.44281524926686217,"examples. In 3rd International Conference on Learning Representations, 2015."
URL,0.44574780058651026,"Marcel Hildebrandt, Jorge Andres Quintero Serna, Yunpu Ma, Martin Ringsquandl, Mitchell Joblin,"
URL,0.44868035190615835,"and Volker Tresp. Reasoning on knowledge graphs with debate dynamics. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 4123–4131, 2020."
URL,0.45161290322580644,"Sean Illing.
“ﬂood the zone with shit”: How misinformation overwhelmed our democracy.
Vox, 2020.
URL https://www.vox.com/policy-and-politics/2020/1/16/
20991816/impeachment-trial-trump-bannon-misinformation."
URL,0.45454545454545453,"Geoffrey Irving, Paul Christiano, and Dario Amodei.
AI safety via debate.
arXiv preprint
arXiv:1805.00899, 2018."
URL,0.4574780058651026,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances"
URL,0.4604105571847507,"in neural information processing systems, 28:2017–2025, 2015."
URL,0.4633431085043988,"Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efﬁcient"
URL,0.4662756598240469,"SMT solver for verifying deep neural networks. In International Conference on Computer Aided
Veriﬁcation, pp. 97–117. Springer, 2017."
URL,0.46920821114369504,Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
URL,0.47214076246334313,"arXiv:1412.6980, 2014."
URL,0.4750733137829912,Under review as a conference paper at ICLR 2022
URL,0.4780058651026393,"P Kohli, S Gowal, K Dvijotham, and J Uesato. Towards robust and veriﬁed AI: Speciﬁcation testing,"
URL,0.4809384164222874,"robust training, and formal veriﬁcation. DeepMind. Medium, 2019."
URL,0.4838709677419355,"Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. Rationalizing neural predictions. In Proceedings"
URL,0.4868035190615836,"of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pp. 107–117. The Association for Computational
Linguistics, 2016."
URL,0.4897360703812317,"Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network"
URL,0.49266862170087977,"acoustic models. In International Conference on Machine Learning, 2013."
URL,0.49560117302052786,"C Maddison, A Mnih, and Y Teh. The concrete distribution: A continuous relaxation of discrete"
URL,0.49853372434017595,"random variables. In Proceedings of the international conference on learning Representations,
2017."
URL,0.501466275659824,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably"
URL,0.5043988269794721,"robust neural networks. In International Conference on Machine Learning, pp. 3578–3586. PMLR,
2018."
URL,0.5073313782991202,"Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances"
URL,0.5102639296187683,"in Neural Information Processing Systems, 32:4694–4703, 2019."
URL,0.5131964809384164,"Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006."
URL,0.5161290322580645,"Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal"
URL,0.5190615835777126,"phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652–24663, 2020."
URL,0.5219941348973607,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor"
URL,0.5249266862170088,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019."
URL,0.5278592375366569,"Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun"
URL,0.530791788856305,"Cho. Finding generalizable evidence by learning to convince Q&A models. arXiv preprint
arXiv:1909.05863, 2019."
URL,0.533724340175953,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certiﬁed defenses against adversarial
examples. In International Conference on Learning Representations, 2018."
URL,0.5366568914956011,"Avraham Ruderman, Richard Everett, Bristy Sikder, Hubert Soyer, Jonathan Uesato, Ananya Kumar,"
URL,0.5395894428152492,"Charlie Beattie, and Pushmeet Kohli. Uncovering surprising behaviors in reinforcement learning
via worst-case analysis. 2018."
URL,0.5425219941348973,"Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. Fast"
URL,0.5454545454545454,"and effective robustness certiﬁcation. Advances in Neural Information Processing Systems, 31:
10802–10813, 2018."
URL,0.5483870967741935,"Justin Thaler. Interactive proofs (part i), 2019. URL https://www.youtube.com/watch?"
URL,0.5513196480938416,v=2XrOdfYviwA&t=829s.
URL,0.5542521994134897,"Jonathan Uesato, Ananya Kumar, Csaba Szepesvari, Tom Erez, Avraham Ruderman, Keith Anderson,"
URL,0.5571847507331378,"Krishnamurthy Dj Dvijotham, Nicolas Heess, and Pushmeet Kohli. Rigorous agent evaluation:
An adversarial approach to uncover catastrophic failures. In International Conference on Learning
Representations, 2018."
URL,0.5601173020527859,"Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984."
URL,0.5630498533724341,John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. Princeton
URL,0.5659824046920822,"university press, 2007."
URL,0.5689149560117303,"Yuanhao Wang*, Guodong Zhang*, and Jimmy Ba. On solving minimax optimization locally: A"
URL,0.5718475073313783,"follow-the-ridge approach. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=Hkx7_1rKwS."
URL,0.5747800586510264,Under review as a conference paper at ICLR 2022
URL,0.5777126099706745,"Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial broadcast"
URL,0.5806451612903226,"decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint
arXiv:1901.07017, 2019."
URL,0.5835777126099707,"Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial"
URL,0.5865102639296188,"defenses. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8410–8419, 2018."
URL,0.5894428152492669,"Mo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. Rethinking cooperative rationalization:"
URL,0.592375366568915,"Introspective extraction and complement control. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 4094–4103, 2019."
URL,0.5953079178885631,"Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger Grosse. Don’t ﬁx what ain’t broke:"
URL,0.5982404692082112,"Near-optimal local convergence of alternating gradient descent-ascent for minimax optimization.
arXiv preprint arXiv:2102.09468, 2021."
URL,0.6011730205278593,"A
WHEN LOG-LOSS GRADIENTS VANISH"
URL,0.6041055718475073,"The gradients of the log-loss (i.e. the loss function used in Equations equation 3 and equation 4)
vanish if and only if the agents fulﬁll their incentives (i.e. classify their labels correctly). Let
` = v(x, p(x)) 2 Rd be the logits that the veriﬁer network outputs for input x and prover message
p(x). Let S : Rd 7! Rd be the softmax operation with temperature 1: [S(`)]i =
e`i
Pd"
URL,0.6070381231671554,j=0 e`j . We take
URL,0.6099706744868035,S(`)i to be the probability the veriﬁer assigns for the correct label to be i.
URL,0.6129032258064516,Let y be the correct label. The veriﬁer loss and its gradient with respect to the logits can be given by:
URL,0.6158357771260997,"Jv = −log
e`y
P"
URL,0.6187683284457478,"j e`j
(7) @Jv"
URL,0.6217008797653959,"@` = −(1 −
e`y
P"
URL,0.624633431085044,"j e`j )
(8)"
URL,0.6275659824046921,"If the veriﬁer assigns a high probability to class y, then
e`y
P"
URL,0.6304985337243402,j e`j will approach 1 and the gradients will
URL,0.6334310850439883,"vanish. Otherwise, the gradient won’t vanish. The derivation is identical for prover’s loss."
URL,0.6363636363636364,"B
PROOFS OF EQUILIBRIUM THEOREMS"
URL,0.6392961876832844,"Theorem 1. Consider all prover-leading sequential PVG formulations. Having reached a (prover
leading Stackelberg) equilibrium is neither necessary nor sufﬁcient for having found a complete and
sound proof-veriﬁcation protocol."
URL,0.6422287390029325,"Proof. It sufﬁces to prove that the prover-leading Stackelberg equilibrium is neither necessary nor
sufﬁcient for having found a complete and sound proof-veriﬁcation protocol on the following decision
problem:"
URL,0.6451612903225806,"Deﬁnition 6 (Simpliﬁed Binary Erasure Channel Problem). Let X = Y = ˜Y = {0, 1} and
M = {0, 1, 2}. Let the input distribution PX be the uniform distribution over X, and let the label
generating function be deﬁned as y = f(x). That is, the label is identical to the input. The prover is
arguing for label y0 = 1."
URL,0.6480938416422287,The prover can choose between two policies p1 and p2: p1 =
URL,0.6510263929618768,"⇢0
x = 0
1
x = 1
p2 ="
URL,0.6539589442815249,"⇢2
x = 0
2
x = 1"
URL,0.656891495601173,"The veriﬁer can pick between three policies v1, v2 and v3 deﬁned as follows:"
URL,0.6598240469208211,"Under review as a conference paper at ICLR 2022 v1 = 8
< :"
URL,0.6627565982404692,"0
m = 0
1
m = 1
0
m = 2 v2 = 8
< :"
URL,0.6656891495601173,"0
m = 0
0
m = 1
0
m = 2 v3 = 8
< :"
URL,0.6686217008797654,"1
m = 0
1
m = 1
1
m = 2"
URL,0.6715542521994134,"Note that the mappings available to the veriﬁer ignore the input. Also note that the prover-veriﬁer
pair (p1, v1) satisfy Equations 5 and 6."
URL,0.6744868035190615,"Lack of Necessity: To show lack of necessity, it sufﬁces to show that the complete and sound prover-
veriﬁer pair (p1, v1) don’t constitute a prover-leading Stackelberg equilibrium. If the prover picks p1,
the veriﬁer can attain exactly 0 loss by assigning a weight of 1 to v1 and a weight of 0 to v2 and v3.
In this scenario, the prover’s loss will be inﬁnity:"
URL,0.6774193548387096,Jp = −1
URL,0.6803519061583577,"2 log v2(x = 0, p(x) = 0)
|
{z
}
+1 + −1"
URL,0.6832844574780058,"2 log v2(x = 1, p(x) = 1)
|
{z
}
0 = +1"
URL,0.6862170087976539,"On the other hand, if the prover picks policy p2, then its messages have exactly 0 mutual information
with the input. In this scenario, the best the veriﬁer can do to minimize its loss function is to assign
equal probability to both outputs 0 and 1. It can do this by assigning 0 weight to v1 and weights 0.5
to v2 and v3. The prover’s loss in this scenario is:"
URL,0.6891495601173021,Jp = −1
URL,0.6920821114369502,"2 log 0.5
|
{z
}
when x=0 + −1"
URL,0.6950146627565983,"2 log 0.5
|
{z
}
when x=1 = 1"
URL,0.6979472140762464,"Since the prover can do better by picking p2 over p1, the pair (p1, v1) doesn’t constitute a prover-
leading Stackelberg equilibrium."
URL,0.7008797653958945,"Lack of Sufﬁciency: Consider the scenario discussed above where the prover picks p2, and the
veriﬁer responds by assigning 0.5 weight to v2 and v3. This is a Stackelberg equilibrium despite not
corresponding to a complete and sound decision rule. This is because 1) p1 achieves the best prover
loss given that the veriﬁer"
URL,0.7038123167155426,"Theorem 2. Consider all veriﬁer-leading sequential PVG formulations in which the problem instance
is revealed after the veriﬁer picks its strategy. Having reached a (veriﬁer leading Stackelberg)
equilibrium is both necessary and sufﬁcient for having found a desirable proof-veriﬁcation protocol."
URL,0.7067448680351907,"Proof. We ﬁrst prove necessity, then sufﬁciency."
URL,0.7096774193548387,"Necessity: By the problem deﬁnition, there exists a prover-veriﬁer pair (p1, v1) that satisfy Equations
5 and 6, therefore constitute a complete and sound proof-veriﬁcation protocol. Consider the scenario
where the prover’s policy is p1, and the veriﬁer assigns a weight of 1 to policy v1. These prover-veriﬁer
strategies constitute a (Stackelberg) equilibrium of the veriﬁer-leading sequential game formulation.
This is because (1) p1 already achieves the smallest prover loss for the given veriﬁer strategy (2) the
veriﬁer already achieves 0 loss and therefore has no incentive to change its strategy. To see (1), note
that the veriﬁer can never be fooled if the correct label is 0, so the prover cannot possibly lower its
loss by changing to a different strategy. If the correct label is 1, then the prover achieves 0 loss by
following p1. Therefore, the prover has no incentive to change its strategy."
URL,0.7126099706744868,"Sufﬁciency: To show that having reached a veriﬁer-leading Stackelberg equilibrium implies a complete
and sounds proof-veriﬁcation protocol, we follow a “proof by contrapositive"" argument and prove,
instead, that if the veriﬁer is not representing a complete and sound proof-veriﬁcation protocol, it
cannot possibly be a Stackelberg equilibrium."
URL,0.7155425219941349,"Assume that the veriﬁer assigns a non-zero weight to a veriﬁcation policy v⇤⇤that is neither complete
and/nor sound. If it’s not complete, then for v⇤⇤there exists a label-1 input x such that there doesn’t
exists a prover policy that can convince the veriﬁer that the correct label is 1. This will cause the
veriﬁer’s loss to be nonzero. If it’s not sound, then there exists a label-0 input for which there exists
a prover policy that can fool the veriﬁer to answering 1. This will also cause the veriﬁer’s loss to
be nonzer. On the other hand, if the veriﬁer assigns 0 weight to all non-complete nor non-sound
veriﬁcation modules, then it’s loss will be exactly 0. This implies that the veriﬁer can do strictly
better by switching to a complete and sound veriﬁcation protocol."
URL,0.718475073313783,Under review as a conference paper at ICLR 2022
URL,0.7214076246334311,"Theorem 3. Consider the simultaneous PVG formulation where the problem instance is revealed
after the agents pick their strategies. Having converged to a (Nash) equilibrium is necessary for
having found a complete and sound proof-veriﬁcation protocol."
URL,0.7243401759530792,"Proof. Proving necessity for the simultaneous setup is identical to proving necessity of the veriﬁer-
leading Stackelberg setup."
URL,0.7272727272727273,"The complete and sound prover-veriﬁer pair (p1, v1) (i.e. the veriﬁer strategy of assigning a weight
of 1 to v1), whose existence is guaranteed by the problem deﬁnition, form a Nash equilibrium."
URL,0.7302052785923754,"If we ﬁx the veriﬁer strategy at v1, then the prover has no incentive to change it’s strategy. In the
collaborative case (when the input belongs to class 1, the prover already achieves 0 loss. In the
adversarial case, the veriﬁer is never fooled (by the soundness assumption) so the prover cannot
improve its loss by switching to a different strategy. Therefore, if we ﬁx the veriﬁer strategy, the
prover wouldn’t move."
URL,0.7331378299120235,"If we ﬁx the prover strategy at p1, then the veriﬁer already achieves 0 loss by following v1. Therefore
it doesn’t have an incentive to move."
URL,0.7360703812316716,"Therefore, the (p1, v1) pair form a Nash equilibrium."
URL,0.7390029325513197,Corollary 1. All veriﬁer-leading Stackelberg equilibria are Nash equilibria of PVG.
URL,0.7419354838709677,"Proof. Having reached a veriﬁer-leading Stackelberg equilibria is sufﬁcient for having found a
complete and sound proof-veriﬁcation protocol (by Theorem 2). A sound and complete proof-
veriﬁcation protocol already constitute a Nash equilibrium 3. Therefore, having reached a veriﬁer-
leading Stackelbeg equilibrium implies having reached a Nash equilibrium."
URL,0.7448680351906158,"C
PROOFS FOR EQUILIBRIUM ANALYSIS"
URL,0.7478005865102639,Theorem 4. Starting from any initialization with p0
URL,0.750733137829912,"i > 0, p1"
URL,0.7536656891495601,"i > 0 for all i 2 M, running gradient
descent on both prover and veriﬁer with reasonable learning rates, the prover would converge to only
sending message 1 when x = 1 while the veriﬁer would predict 1 only if it receives message 1 from
the prover. To put it differently, p1"
URL,0.7565982404692082,"1 = 1 when converge while q1 = 1 −✏and q0 = q2 = ... = qK = ✏
with ✏a small constant depending on the strength of entropy regularization."
URL,0.7595307917888563,"Proof. First, we can assume WLOG that K = 2 because all these tokens from 2 to K are “exactly”
same for both prover and veriﬁer. So if we do not concern about computation time, we can group
tokens 2 −K as a single token. In this case, the only parameters for the prover is just p0"
AND,0.7624633431085044,"0 and
p1"
AND,0.7653958944281525,"1. For notational convenience, we let b , p0"
AND,0.7683284457478006,"0 and a , p1"
AND,0.7712609970674487,"1. Further, we assume the learning rates
⌘P ⌧⌘V. With two time-scale updates, one can show that the veriﬁer would converge ﬁrst. Given
nondegenerate initialization and entropy regularization on q with coefﬁcient λ > 0 (this is basically
label smoothing), we have"
AND,0.7741935483870968,q1 = 1 + λ
AND,0.7771260997067448,"1 + 2λ,
q0 =
λ
1 + 2λ,
q2 = 1 −a + λ(2 −a −b)"
AND,0.7800586510263929,"(2 −a −b)(1 + 2λ) .
(9)"
AND,0.782991202346041,Recall that the objective of the prover is
AND,0.7859237536656891,"L(a, b) = −a log q1 −b log q0 −(2 −a −b) log q2
(10)"
AND,0.7888563049853372,"Hence, we have the mean gradients of the prover @L"
AND,0.7917888563049853,@a = log q2 −log q1 = log 1 −a + λ(2 −a −b)
AND,0.7947214076246334,"(1 + λ)(2 −a −b)
= −log ✓"
AND,0.7976539589442815,"1 +
1 −b
1 −a + λ(2 −a −b) ◆ (11) @L"
AND,0.8005865102639296,@b = log q2 −log q0 = log 1 −a + λ(2 −a −b)
AND,0.8035190615835777,"λ(2 −a −b)
= −log ✓"
AND,0.8064516129032258,"1 −
1 −a
1 −a + λ(2 −a −b) ◆ (12)"
AND,0.8093841642228738,"As long as a < 1, b would keep decrease as the gradient w.r.t b in equation 12 is > 0. This further
implies that the gradient w.r.t a is < 0 and hence a would keep increasing until a = 1."
AND,0.8123167155425219,Under review as a conference paper at ICLR 2022
AND,0.8152492668621701,"Further, we show a = 1, b < 1 and q taking the values in equation 9 are the Nash equilibria of
the game. It is easy to see the q value are optimal given ﬁxed a, b, so it is impossible to improve
unilaterally for the veriﬁer. On the other hand, we know log q1 > log q0 = log q2 if a = 1 and b < 1,
so the prover could not improve either. This completes the proof."
AND,0.8181818181818182,Theorem 5. Starting from any initialization with p0
AND,0.8211143695014663,"i > 0, p1"
AND,0.8240469208211144,"i > 0 for all i 2 M, running gradient
descent on both prover and veriﬁer with any learning rates, prover would converge to only sending
message 2 to K no matter what the input is."
AND,0.8269794721407625,"Proof. From the viewpoint of prover, it is essentially single-objective optimization problem with the
loss function in the following form:"
AND,0.8299120234604106,"L(a, b) = −a log 1 + λ"
AND,0.8328445747800587,"1 + 2λ −b log
λ
1 + 2λ −(2 −a −b) log 1 −a + λ(2 −a −b)"
AND,0.8357771260997068,"(2 −a −b)(1 + 2λ)
(13)"
AND,0.8387096774193549,"Surprisingly, the Stackelberg game formulation leads to bad joint strategy where the prover may send
wrong messages with the input 1. In particular, we have the gradient: @L"
AND,0.841642228739003,@a = −log ✓
AND,0.844574780058651,"1 +
1 −b
1 −a + λ(2 −a −b) ◆"
AND,0.8475073313782991,"+
1 −b
1 −a + λ(2 −a −b)
(14) @L"
AND,0.8504398826979472,@b = −log ✓
AND,0.8533724340175953,"1 −
1 −a
1 −a + λ(2 −a −b) ◆"
AND,0.8563049853372434,"−
1 −a
1 −a + λ(2 −a −b)
(15)"
AND,0.8592375366568915,"By the identity log(1 + x) < x, we immediately know that @L"
AND,0.8621700879765396,@a > 0 and @L
AND,0.8651026392961877,"@b > 0 if a, b 6= 1.
Interestingly, this is exactly the opposite of the simultaneous game. This immediately implies that
a, b will converge to 0. This completes the proof."
AND,0.8680351906158358,"D
TRAINING HEURISTICS"
AND,0.8709677419354839,"We ﬁnd that the following design decisions make learning more consistent and help speed up
convergence, while not being essential for success:"
AND,0.873900293255132,• Different Number of Gradient Steps per Agent: We ﬁnd that updating the veriﬁer multi-
AND,0.8768328445747801,"ple times per a single prover update helps. It is especially helpful in preventing the veriﬁer
from ignoring the prover’s messages early on in training. Note that proving Theorem 4
requires setting a larger learning rate to the veriﬁer."
AND,0.8797653958944281,• Adaptive Prover Step Size: Adaptively increasing the prover-update frequency if the
AND,0.8826979472140762,"veriﬁer’s accuracy surpasses a pre-selected (i.e. 90 percent) value helps speed up the
transition from “nearly robust"" protocols to fully robust ones at the terminal phase of the
training."
AND,0.8856304985337243,• Prover Pretraining: Pretraining the prover to solve the decision problem before initiating
AND,0.8885630498533724,"the prover-veriﬁer game ensures that the ﬁrst messages the veriﬁer receives from the prover
will likely contain features that correlate with the labels and therefore decreases the likeli-
hood that the veriﬁer will ignore the prover’s messages. We do this by appending two extra
heads to the prover that are trained to do solve the task at hand, as well as to autoencode the
input."
AND,0.8914956011730205,"E
EXPERIMENT DETAILS"
AND,0.8944281524926686,"E.1
BINARY ERASURE CHANNEL EXPERIMENTS"
AND,0.8973607038123167,• Task Setup and Data Distribution: We picked there to be 16 communication tokens that
AND,0.9002932551319648,"the prover can communicate to the prover. We sampled the input bits uniformly, which mean
the labels were balanced."
AND,0.9032258064516129,Under review as a conference paper at ICLR 2022
AND,0.906158357771261,• Prover Network: We parametrized the prover as a two hidden layer fully connected
AND,0.9090909090909091,"LeakyReLU network with width 100 and LayerNorm (Ba et al., 2016) layers fully connected
network. We used the Gumbel-softmax trick (with a constant temperature of 1) to sample
(one hot encoded) communication tokens. We use the Straight-Through Gumbel Softmax
Estimator (Maddison et al., 2017) to backpropagate through the discrete sampling step."
AND,0.9120234604105572,"• Veriﬁer Network:
The veriﬁer was instantiated as a one hidden layer, fully connected
LeakyReLU neural network with width 100 and LayerNorm layers."
AND,0.9149560117302052,• Optimization setup: We used the Adam optimizer for updating both the prover and veriﬁer
AND,0.9178885630498533,"with a learning rate of 0.0003 and batch size of 2000. We updated the veriﬁer 5 times
for each prover update. We observed that training the prover to solve the classiﬁcation
and autoencoding auxiliary tasks doesn’t have an impact on training, likely due to the
simplicity of this task. We ran optimization until the veriﬁer learned a complete and sound
proof-veriﬁcation protocol (or, frozen veriﬁer’s precision and recall reached the value of 1)"
AND,0.9208211143695014,"• Evaluation details:
To train the prover against the frozen veriﬁer, we use the Adam
optimizer with a learning rate of 0.0003 for 500 gradient steps, which we found was enough
to reach near-convergence. To optimize the prover messages directly, we use the PyTorch
implementation (Paszke et al., 2019) of the LBFGS optimizer (Nocedal & Wright, 2006)
with a learning rate of 1, history size of 300 and “strong Wolfe"" line search. We terminate the
optimization when either the L1 norm of the change in iterates becomes less than 10e−9, or
when the L1 norm of gradients become less than 10e −5, or when the number of optimizer
steps surpass 300. We found that the the tolerance-based termination criteria are met almost
always before the maximum number of optimizer steps is reached."
AND,0.9237536656891495,• Hyperparameter Optimization and Tuning: We tuned the following aspects of training
AND,0.9266862170087976,"1) Optimizer (Adam vs. SGD), 2) Learning rate (1e-3, 3e-4, 1e-4), 3) Batch size (500, 2000),
4) Network width (32, 100) 5) veriﬁer number of steps (1, 5, 10, 20, 50). We found that
using Adam, the larger batch size of 2000 and a larger width of 100 for the networks were
crucial. Setting the learning rate to be 3e −4 and veriﬁer number of steps to 5 stabilized
training and made it more consistent."
AND,0.9296187683284457,"• Training hardware: Due to the low computational requirements of the BEC experiments,"
AND,0.9325513196480938,"the experiments were run on an 2019 Modle MacBookPro with 8-Core Intel Core i9
processors."
AND,0.9354838709677419,"E.2
FINDTHEPLUS EXPERIMENTS:"
AND,0.9384164222873901,• Task Setup and Data Distribution: We used 10 ⇥10 images with exactly one 3 ⇥3 plus
AND,0.9413489736070382,"patterns in them. The images were single-channel, and the pixel values were binary (i.e. 0
or 1). The plus patterns were either formed using 0-pixels or 1-pixels, which we refer to
“red"" or “blue"" plusses in the original text for additional clarity. The used balanced labels"
AND,0.9442815249266863,during training.
AND,0.9472140762463344,• Prover Network: We parametrized the prover as a 3 hidden layer convolutional neural
AND,0.9501466275659824,"network with 40 hidden channels, kernel size of 3, padding of 1 and LeakyReLU (Maas et al.,
2013) nonlinearities followed by Instance Normalization layers. We passed the output of this
convolutional neural network through a mean-pooling operation follow by a linear projection
layer that resulted in the message vectors. The message vectors were 32 dimensional.
We also trained the prover network to both solve the decision problem itself (i.e. classi-
ﬁcation), as well as autoencode the inputs. We did this by tacking “auxiliary heads"" that
process the output of the convolutional backbone described above and produce their outputs.
We used a linear classiﬁcation auxiliary head. For the autoencoding auxiliary head, we
used a Spatial Broadcast Decoder (Watters et al., 2019) with 3 convolutional layers with 32
channels, kernel size of 3 and LeakyReLU activations."
AND,0.9530791788856305,• Veriﬁer Network: We parametrized the veriﬁer with the Spatial Transformer Network-like
AND,0.9560117302052786,"(Jaderberg et al., 2015) architecture with 4 heads. This architecture linearly transforms the
prover messages into 2 ⇥3 matrices that parametrize what kind of spatial transformation
should be applied to the image. The transformed images are then fed into a ﬁnal linear
classiﬁer that produces the veriﬁer predictions. A schematic diagram of a single head of this
architecture is displayed in Figure 3. We constrained the allowed spatial transformations to"
AND,0.9589442815249267,Under review as a conference paper at ICLR 2022
AND,0.9618768328445748,"Figure 3: FindThePlus Veriﬁer Architecture: A schematic diagram of a single head of the Spatial
Transformer Network-like veriﬁer architecture we used in out FindThePlus experiments."
AND,0.9648093841642229,"only include shifts, and hard-coded the rest to zoom in by a factor of 2, and have no rotation
and shear. These constraints only serve to improve the clarity plus visualizations and allow
us to inspect training - training still succeeds if we remove these constraints. We used zero
padding, if the veriﬁer chose to shift its gaze to outside of the image."
AND,0.967741935483871,• Optimization Setup: We used the Adam optimizer for updating both the prover and veriﬁer
AND,0.9706744868035191,"with a learning rate of 0.0003 and batch size of 2000. We updated the veriﬁer 1 times for each
prover update. We observed that pretraining training the prover to solve the classiﬁcation
task speeds up training considerably, while the autoencoding head doesn’t have a signiﬁcant
impact. We adaptively incremented the prover’s number of steps if the veriﬁer’s accuracy
kept above 75% for 20 consequent game steps, and stopped incrementing when the prover’s
number of steps reached 15. We ran pretraining using the same optimizer settings, for 100
gradient steps (enough to let the prover achieve around 80% accuracy).
We employed a bit of Label Smoothing (Müller et al., 2019) (with coefﬁcient 0.05) to the
veriﬁer loss to regularize training. For pretraining the prover classiﬁcation and autoencod-
ing auxiliary heads, we used the cross entropy loss and (pixelwise) mean squared error
respectively. We kept training the prover to classify and autoencode throughout training.
We report our results at the point veriﬁer achieved it’s best frozen precision against directly
optimized message vectors. Note that the veriﬁer achieves perfect frozen precision against
an optimized prover way before this point."
AND,0.9736070381231672,"• Evaluation Details:
We used exactly the same evaluation setup we used in the Binary
Erasure Channel Experiments. We report the results on the training checkpoint with the
highest overall frozen veriﬁer accuracy against optimized proofs, as the performance of the
veriﬁer tends to ﬂuctuate all throughout training. We also report the best result amongst
training runs initialized with four random number generator seeds."
AND,0.9765395894428153,• Hyperparameter Optimization and Tuning: We used the setup that worked best for the
AND,0.9794721407624634,"Binary Erasure Taks and found that it also yielded good results for the FindThePlus task.
We re-tuned the number of veriﬁer steps and found that updating the veriﬁer less frequently
(i.e. once per prover step) doen’t degrade performance, while speeding up convergence.
We also tuned the duration of prover pretraining. We found that pretraining the prover til
convergence actually hurts performance. We link this to the Neural Collapse phenomenon
elaborated by Papyan et al. (2020) and leave a more detailed examination to future works."
AND,0.9824046920821115,• Training Hardware: We ran training using a single NVIDIA P100 GPU with Intel Xeon
AND,0.9853372434017595,Silver processors.
AND,0.9882697947214076,"E.3
ADDITIONAL PLUS VISUALIZATIONS"
AND,0.9912023460410557,"In Figure 4, we show more visualizations that display where the veriﬁer looks at in the input images."
AND,0.9941348973607038,Under review as a conference paper at ICLR 2022
AND,0.9970674486803519,"Figure 4: Visualizing Veriﬁer’s Gaze: We visualized where each of the veriﬁer’s 4 Spatial Trans-
former heads learn to look at. The top row displays the inputs. The left three inputs contain a blue plus
in them (what the prover is defending). Each subsequent row corresponds to a different veriﬁer head.
We observe that the ﬁrst head consistently contain blue plusses at consistent locations, indicating that
the prover is communicating the coordinate of the plus to the veriﬁer. If there’s not a blue plus in the
image, the prover sends the coordinate of a convincing image patch."
