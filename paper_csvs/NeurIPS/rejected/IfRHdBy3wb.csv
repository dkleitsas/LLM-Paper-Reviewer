Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017953321364452424,"Understanding the decision-making process of Graph Neural Networks (GNNs)
1"
ABSTRACT,0.003590664272890485,"is crucial to their interpretability. Present methods for explaining GNNs typically
2"
ABSTRACT,0.005385996409335727,"rely on training auxiliary models, and may struggle with issues such as overfitting
3"
ABSTRACT,0.00718132854578097,"to noise, insufficient discriminability, and inconsistent explanations across data
4"
ABSTRACT,0.008976660682226212,"samples of the same class. This paper introduces Graph Output Attribution (GOAt),
5"
ABSTRACT,0.010771992818671455,"a novel method to attribute graph outputs to input graph features, creating GNN
6"
ABSTRACT,0.012567324955116697,"explanations that are faithful, discriminative, as well as stable across similar sam-
7"
ABSTRACT,0.01436265709156194,"ples. By expanding the GNN as a sum of scalar products involving node features,
8"
ABSTRACT,0.01615798922800718,"edge features and activation patterns, we propose an efficient analytical method
9"
ABSTRACT,0.017953321364452424,"to compute contribution of each node or edge feature to each scalar product and
10"
ABSTRACT,0.019748653500897665,"aggregate the contributions from all scalar products in the expansion form to derive
11"
ABSTRACT,0.02154398563734291,"the importance of each node and edge. Through extensive experiments on synthetic
12"
ABSTRACT,0.02333931777378815,"and real data, we show that our method has consistently outperformed various
13"
ABSTRACT,0.025134649910233394,"state-of-the-art GNN explainers in terms of fidelity, discriminability, and stability.
14"
INTRODUCTION,0.026929982046678635,"1
Introduction
15"
INTRODUCTION,0.02872531418312388,"Graph Neural Networks (GNNs) have demonstrated notable success in learning representations from
16"
INTRODUCTION,0.03052064631956912,"graph-structured data in various fields [14, 9, 30]. However, their black-box nature has driven the
17"
INTRODUCTION,0.03231597845601436,"need for explainability, especially in sectors where transparency and accountability are essential, such
18"
INTRODUCTION,0.03411131059245961,"as finance [28], healthcare [1], and security [18]. The ability to interpret GNNs can provide insights
19"
INTRODUCTION,0.03590664272890485,"into the mechanisms underlying deep models and help establish trustworthy predictions.
20"
INTRODUCTION,0.03770197486535009,"Existing attempts to explain GNNs usually focus on local-level or global-level explainability. Local-
21"
INTRODUCTION,0.03949730700179533,"level explainers [31, 17, 21, 27, 10, 15, 23, 3] typically train a secondary model to identify the critical
22"
INTRODUCTION,0.04129263913824058,"graph structures that best explain the behavior of a pretrained GNN for specific input instances. These
23"
INTRODUCTION,0.04308797127468582,"methods are always optimized for ground-truth explanations or fidelity metrics, yet may not be able to
24"
INTRODUCTION,0.04488330341113106,"generate consistent explanations for similar graph samples or produce accurate and human-intelligible
25"
INTRODUCTION,0.0466786355475763,"explanations for class discrimination. Global-level explainers [2, 11] perform prototype learning or
26"
INTRODUCTION,0.04847396768402154,"random walk on the explanation instances to extract the global explanations over a multitude of graph
27"
INTRODUCTION,0.05026929982046679,"samples. However, their effectiveness rely heavily on the quality of local-level explanations.
28"
INTRODUCTION,0.05206463195691203,"In this paper, we introduce a computationally efficient local-level GNN explanation technique called
29"
INTRODUCTION,0.05385996409335727,"Graph Output Attribution (GOAt) to overcome the limitations of existing methods. Unlike methods
30"
INTRODUCTION,0.05565529622980251,"that rely on back-propagation with gradients [20, 4, 22, 8] and those relyinig on hyper-parameters or
31"
INTRODUCTION,0.05745062836624776,"training complex black-box models [17, 15, 23, 3], our approach enables attribution of GNN output
32"
INTRODUCTION,0.059245960502693,"to input features, leveraging the repetitive sum-product structure in the forward pass of a GNN.
33"
INTRODUCTION,0.06104129263913824,"Given that the matrix multiplication in each GNN layer adheres to linearity properties and the
34"
INTRODUCTION,0.06283662477558348,"activation functions operate element-wise, a GNN can be represented in an expansion form as a
35"
INTRODUCTION,0.06463195691202872,"sum of scalar product terms, involving input graph features, model parameters, as well as activation
36"
INTRODUCTION,0.06642728904847396,"patterns that indicate the activation levels of the scalar products. Based on the notion that all scalar
37"
INTRODUCTION,0.06822262118491922,"variables Xi in a scalar product term g = cX1X2 . . . XN contribute equally to g, where c is a
38"
INTRODUCTION,0.07001795332136446,"constant, we can attribute each product term to its corresponding factors and thus to input features,
39"
INTRODUCTION,0.0718132854578097,"obtaining the importance of each node or edge feature in the input graph to GNN outputs. We present
40"
INTRODUCTION,0.07360861759425494,"case studies that demonstrate the effectiveness of our analytical explanation method GOAt on typical
41"
INTRODUCTION,0.07540394973070018,"GNN variants, including GCN, GraphSAGE, and GIN.
42"
INTRODUCTION,0.07719928186714542,"Besides the fidelity metric, which is commonly used to assess the faithfulness of GNN explanations,
43"
INTRODUCTION,0.07899461400359066,"we introduce two new metrics to evaluate the discriminability and stability of the explanation, which
44"
INTRODUCTION,0.0807899461400359,"are under-investigated by prior literature. Discriminability refers to the ability of explanations to
45"
INTRODUCTION,0.08258527827648116,"distinguish between classes, which is assessed by the difference between the mean explanation
46"
INTRODUCTION,0.0843806104129264,"embeddings of different classes, while stability refers to the ability to generate consistent explanations
47"
INTRODUCTION,0.08617594254937164,"across similar data instances, which is measured by the percentage of data samples covered by top-k
48"
INTRODUCTION,0.08797127468581688,"explanations. Through comprehensive experiments based on on both synthetic and real-world datasets
49"
INTRODUCTION,0.08976660682226212,"along with qualitative analysis, we show the outstanding performance of our proposed method, GOAt,
50"
INTRODUCTION,0.09156193895870736,"in providing highly faithful, discriminative, and stable explanations for GNNs, as compared to a
51"
INTRODUCTION,0.0933572710951526,"range of state-of-the-art methods.
52"
PROBLEM FORMULATION,0.09515260323159784,"2
Problem Formulation
53"
PROBLEM FORMULATION,0.09694793536804308,"Graph Neural Networks Let G = (V, E) be a graph, where V = {v1, v2, . . . , vN} denotes the set
54"
PROBLEM FORMULATION,0.09874326750448834,"of nodes and E ⊆V × V denotes the set of edges. The node feature matrix of the graph is represented
55"
PROBLEM FORMULATION,0.10053859964093358,"by X ∈RN×d, and the adjacency matrix is represented by A ∈{0, 1}N×N such that Aij = 1 if
56"
PROBLEM FORMULATION,0.10233393177737882,"there exists an edge between nodes vi and vj. The task of a GNN is to learn a function f(G), which
57"
PROBLEM FORMULATION,0.10412926391382406,"maps the input graph G to a target output, such as node labels, graph labels, or edge labels. Formally
58"
PROBLEM FORMULATION,0.1059245960502693,"speaking, for a given GNN, the hidden state h(l)
i
of node vi at its layer l can be represented as:
59"
PROBLEM FORMULATION,0.10771992818671454,"h(l)
i
= COMBINE(l) n
h(l−1)
i
, AGGREGATE(l) n
h(l−1)
j
, ∀vj ∈Ni
oo
,
(1)"
PROBLEM FORMULATION,0.10951526032315978,"where Ni represents the set of neighbors of node vi in the graph. COMEBINE(l)(·) is a COMBINE
60"
PROBLEM FORMULATION,0.11131059245960502,"function such as concatenation [9], while AGGREGATE(l)(·) are AGGREGATE functions with
61"
PROBLEM FORMULATION,0.11310592459605028,"aggregators such as ADD. We focus on GNNs that adopt the non-linear activation function ReLU in
62"
PROBLEM FORMULATION,0.11490125673249552,"COMBINE or AGGREGATE functions.
63"
PROBLEM FORMULATION,0.11669658886894076,"Local-level GNN Explainability Our goal is to generate a faithful explanation for a graph instance
64"
PROBLEM FORMULATION,0.118491921005386,"G = (V, E) by identifying a subset of edges S ⊆E, given a GNN f(·) pretrained on a set of graphs
65"
PROBLEM FORMULATION,0.12028725314183124,"G. The term faithful refers to the explanation’s ability to perform well in not only fidelity [33] and
66"
PROBLEM FORMULATION,0.12208258527827648,"robustness [3] metrics, but also stability in identifying consistent patterns. We highlight edges instead
67"
PROBLEM FORMULATION,0.12387791741472172,"of nodes as suggested by [7] that edges have more fine-grained information than nodes while giving
68"
PROBLEM FORMULATION,0.12567324955116696,"human-understandable explanations like subgraphs.
69"
METHOD,0.12746858168761221,"3
Method
70"
METHOD,0.12926391382405744,"This section begins by presenting our fundamental definition of equal contribution in a product term
71"
METHOD,0.1310592459605027,"and its application in an example of a toy graph neural network. Then, we mathematically present
72"
METHOD,0.13285457809694792,"GOAt method for explaining typical GNNs, followed by a case study on GCN [14]. Additional case
73"
METHOD,0.13464991023339318,"studies of applying GOAt to GraphSAGE [9] and GIN [30] are included in the Appendix.
74"
DEFINITIONS,0.13644524236983843,"3.1
Definitions
75"
DEFINITIONS,0.13824057450628366,"Consider a function g(X1, . . . , XM) of M variables X = {X1, . . . , XM}. If we let a pair of variables
76"
DEFINITIONS,0.1400359066427289,"(Xi, Xj) be set to (Xi, Xj) = (xi, xj), we will obtain a manifold gXi=xi,Xj=xj(X\{Xi, Xj}),
77"
DEFINITIONS,0.14183123877917414,"which represents g(·) when all variables excluding Xi and Xj can vary. Consider a base manifold
78"
DEFINITIONS,0.1436265709156194,"gXi=x′
i,Xj=x′
j(X\{Xi, Xj}). If we can obtain two identical manifolds by setting (Xi, Xj) =
79"
DEFINITIONS,0.14542190305206462,"(xi, x′
j) and (Xi, Xj) = (x′
i, xj), it will indicate that changing Xi = x′
i to Xi = xi is equivalent
80"
DEFINITIONS,0.14721723518850988,"to changing Xj = x′
j to Xj = xj with respect to the base manifold at (Xi, Xj) = (x′
i, x′
j).
81"
DEFINITIONS,0.1490125673249551,"For example, a function g(x, y, z) = 2xy + x2z has three variables x, y, z, we consider taking
82"
DEFINITIONS,0.15080789946140036,"gx=−1,y=−1(z) = z + 2 as the base manifold. Since gx=1,y=−1(z) = gx=−1,y=1(z) = z −2, we
83"
DEFINITIONS,0.1526032315978456,"say that changing x = −1 to x = 1 is equivalent to changing y = −1 to y = 1 with respect to the
84"
DEFINITIONS,0.15439856373429084,"base manifold at (x, y) = (−1, −1).
85"
DEFINITIONS,0.1561938958707361,"Definition 1 (Equal Contribution). Given a function g(X) where X = {X1, . . . , XM} represents
86"
DEFINITIONS,0.15798922800718132,"M variables, we say that variables Xi and Xj have equal contribution to function g at (xi, xj)
87"
DEFINITIONS,0.15978456014362658,"with respect to the base manifold at (x′
i, x′
j) if and only if setting Xi = xi, Xj = x′
j and setting
88"
DEFINITIONS,0.1615798922800718,"Xi = x′
i, Xj = xj yield the same manifold, i.e.,
89"
DEFINITIONS,0.16337522441651706,"gXi=xi,Xj=x′
j(X\{Xi, Xj}) = gXi=x′
i,Xj=xj(X\{Xi, Xj})"
DEFINITIONS,0.1651705565529623,"for any values of X excluding Xi and Xj.
90"
DEFINITIONS,0.16696588868940754,"Lemma 2 (Equal Contribution in a product). Given a function g(X) defined as g(X) =
91"
DEFINITIONS,0.1687612208258528,"b QM
k=1 Xk, where b is a constant, and X = {X1, . . . , XM} represents M uncorrelated variables.
92"
DEFINITIONS,0.17055655296229802,"Each variable Xk is either 0 or xk, depending on the absence or presence of a certain feature. Then,
93"
DEFINITIONS,0.17235188509874327,"all the variables in X contribute equally to g(X) at [x1, . . . , xM] with respect to [0, . . . , 0].
94"
DEFINITIONS,0.1741472172351885,"Proofs of all Lemmas and Theorems can be found in the Appendix. Since all the binary variables
95"
DEFINITIONS,0.17594254937163376,"have equal contribution, we define the contribution of each variable Xk to g(X) = b QM
k=1 Xk for
96"
DEFINITIONS,0.17773788150807898,"all k = 1, . . . , M, as
97"
DEFINITIONS,0.17953321364452424,"IXk = b QM
i=1 xi
M
.
(2)"
DEFINITIONS,0.1813285457809695,"For example, let f(A, X) = AXW be a simple 2-node GNN for node classification, where A, X, W
98"
DEFINITIONS,0.18312387791741472,"are 2 × 2 matrices that denote adjacency matrix, node feature matrix, and weight matrix, respectively.
99"
DEFINITIONS,0.18491921005385997,"Then, we can represent each entry in the resulting 2 × 2 matrix f(A, X) as an expansion form:
100"
DEFINITIONS,0.1867145421903052,"fi,j(A, X) ="
X,0.18850987432675045,"1
X k=0"
X,0.19030520646319568,"1
X"
X,0.19210053859964094,"l=0
Ai,kXk,lWl,j,
(3)"
X,0.19389587073608616,"where fi,j(A, X) represents the prediction of the i-th node for the j-th class. In a pretrained
101"
X,0.19569120287253142,"GNN, parameter W is fixed. Thus, only Ai,k and Xk,l contribute to the value of each scalar product
102"
X,0.19748653500897667,"Ai,kXk,lWl,j. As Ai,k is usually independent of Xk,l under proper data cleaning, we can calculate the
103"
X,0.1992818671454219,"contributions of Ai,k and Xk,l to the scalar product Ai,kXk,lWl,j by IAi,k = IXk,l = 1"
X,0.20107719928186715,"2Ai,kXk,lWl,j
104"
X,0.20287253141831238,"based on Lemma 2 and Equation (2). By similar computations for all the scalar products in the
105"
X,0.20466786355475763,"expansion form of f(·), we can obtain the contribution of all the input features to each entry of the
106"
X,0.20646319569120286,"output matrix.
107"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.20825852782764812,"3.2
Explaining Graph Neural Networks via Attribution
108"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.21005385996409337,"A typical GNN [14, 9, 30] for node or graph classification tasks usually comprises 2-6 message-
109"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2118491921005386,"passing layers for learning node or graph representations, followed by several fully connected layers
110"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.21364452423698385,"that serve as the classifier. With the hidden state h(l)
i
of node vi at the l-th message-passing layer
111"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.21543985637342908,"defined as Equation (1), we generally have the hidden state H(l) of a data sample as:
112"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.21723518850987433,"H(l) = σ

Φ(l) 
A + ϵ(l)I

H(l−1)
+ λΨ(l) 
H(l−1)
,
(4)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.21903052064631956,"where A is the adjacency matrix, ϵ(l) refers to the self-loop added to the graph if fixed to 1, otherwise it
113"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.22082585278276481,"is a learnable parameter, σ(·) is the element wise activation function, Φ(l) and Ψ(l) can be Multilayer
114"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.22262118491921004,"Perceptrons (MLP) or linear mappings, λ ∈{0, 1} determines whether a concatenation is required.
115"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2244165170556553,"If the COMBINE step of a GNN requires a concatenation, we have λ = 1 and ϵ(l) = 1; if the
116"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.22621184919210055,"COMBINE step requires a weighted sum, we have ϵ(l) set trainable and λ = 0. Alternatively,
117"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.22800718132854578,"Equation (4) can be expanded to:
118"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.22980251346499103,H(l) = σ 
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.23159784560143626,"AH(l−1)
K
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2333931777378815,"k=1
W Φ(l)
k + ϵ(l)H(l−1)
K
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.23518850987432674,"k=1
W Φ(l)
k + λH(l−1)
Q
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.236983842010772,"q=1
W Ψ(l)
q
! ,
(5)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.23877917414721722,"where K, Q refer to the number of MLP layers in Φ(l)(·) and Ψ(l)(·), and W Φ(l)
k and W Ψ(l)
q are the
119"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.24057450628366248,"trainable parameters in Φ(l)
k and Ψ(l)
q .
120"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.24236983842010773,"Given a certain data sample and a pretrained GNN, for an element-wise activation function we can
121"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.24416517055655296,"define the activation pattern as the ratio between the output and input of the activation function:
122"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2459605026929982,"Definition 3 (Activation Pattern). Denote H(l)′ and H(l) as the hidden representations before and
123"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.24775583482944344,"after passing through the element-wise activation function at the l-th layer, we define activation
124"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2495511669658887,"pattern P (l) for a given data sample as
125"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2513464991023339,"P (l)
i,j = 
 "
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.25314183123877915,"H(l)
i,j"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.25493716337522443,"H(l)′
i,j
,
if H(l)′
i,j ̸= 0"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.25673249551166966,"0,
otherwise"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2585278276481149,"where P (l)
i,j is the element-wise activation pattern for the j-th feature of i-th node at layer l.
126"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.26032315978456017,"Hence, the hidden state H(l) at the l-th layer for a given sample can be written as
127"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2621184919210054,H(l) = P (l) ⊙ 
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2639138240574506,"AH(l−1)
K
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.26570915619389585,"k=1
W Φ(l)
k + ϵ(l)H(l−1)
K
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.26750448833034113,"k=1
W Φ(l)
k + λH(l−1)
Q
Y"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.26929982046678635,"q=1
W Ψ(l)
q
! ,
(6)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2710951526032316,"where ⊙represents element-wise multiplication. Thus, similar to Equation (3), we can expand the
128"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.27289048473967686,"expression of each output entry in a GNN f(A, X) into a sum of scalar products, where each scalar
129"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2746858168761221,"product is the multiplication of corresponding entries in A, X, W, and P in all layers. Then each
130"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2764811490125673,"scalar product can be written as
131"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.27827648114901254,"z = C·

P (1)
α10,β11 . . . P (L)
αL0,βL1"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2800718132854578," 
P (c1)
αL0,γ11 . . . P
(c(M−1))
αL0,γ(M−1)1

·

A(L)
αL0,αL1 . . . A(1)
α10,α11

Xi,j

W (1)
β10,β11 . . . W (L)
βL0,βL1"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.28186714542190305," 
W (c1)
γ10,γ11 . . . W (cM)
γM0,γM1

,
(7)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2836624775583483,"where C is a constant, ck refers to the k-th layer of the classifier, (αl0, αl1), (βl0, βl1), (γl0, γl1) are
132"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.28545780969479356,"(row, column) indices of the corresponding matrices at layer l. In a classifier with M MLP layers,
133"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2872531418312388,"only (M −1) layers adopt activation functions. Therefore, we do not have P (cM)
αL0,γM1 in Equation (7).
134"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.289048473967684,"For scalar products without factors of A, all A’s are considered as constants equal to 1 in Equation (7).
135"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.29084380610412924,"Since the GNN model parameters are pretrained and fixed, we only consider A, X, and all the P
136"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2926391382405745,"terms as the variables in each product term.
137"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.29443447037701975,"Lemma 4 (Equal Contribution variables in the GNN expansion form’s scalar product). For a
138"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.296229802513465,"scalar product term z in the expansion form of a pretrained GNN f(·), when the number of nodes N
139"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2980251346499102,"is large, all variables in z have equal contributions to the scalar product z.
140"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.2998204667863555,"Hence, by Equation (2), we can calculate the contribution Iν(z) of a variable ν (i.e., an entry in A, X
141"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3016157989228007,"and P matrices) to each scalar product z (given by Equation (7)) by:
142"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.30341113105924594,"Iν(z) =
z
|V (z)|,
(8)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3052064631956912,"where function V (·) represents the set of variables in its input, and |V (z)| denotes the number of
143"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.30700179533213645,"unique variables in z, e.g., V (x2y) = {x, y}, and |V (x2y)| = 2.
144"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3087971274685817,"Similar to Section 3.1, an entry fm,n(A, X) of the output matrix f(A, X) can be expressed by the
145"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3105924596050269,"sum of all the related scalar products as
146"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3123877917414722,"fm,n(A, X) =
X
C·

P (1)
α10,β11 . . . P (L)
m,βL1"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3141831238779174,"
·

P (c1)
m,γ11 . . . P
(c(M−1))
m,γ(M−1)1

·

A(L)
m,αL1 . . . A(1)
α10,α11
"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.31597845601436264,"· Xi,j ·

W (1)
β10,β11 . . . W (L)
βL0,βL1"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3177737881508079,"
·

W (c1)
γ10,γ11 . . . W (cM )
γM0,n

,
(9)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.31956912028725315,"where summation is over all possible (αl0, αl1), (βl0, βl1), (γl0, γl1), for message passing layer
147"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3213644524236984,"l = 1, . . . , L or classifier layer l = 1, . . . , M, as well as all i, j indices for X. By summing up the
148"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3231597845601436,"contribution of each variable ν among the entries in the A, X and P’s in all the scalar products in the
149"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3249551166965889,"expansion form of fm,n(·), we can obtain the contribution of ν to fm,n(·) as:
150"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3267504488330341,"Iν(fm,n(·)) =
X"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.32854578096947934,"z in fm,n(·) that contain ν"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3303411131059246,"z
|V (z)|.
(10)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.33213644524236985,"Theorem 5 (Contribution of variables in the expansion form of a pretrained GNN). Given
151"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3339317773788151,"Equations (8) and (10), for each variable ν (i.e., an entry in A, X and P matrices), when the number
152"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3357271095152603,"of nodes N is large, we can approximate Iν(fm,n(·)) by:
153"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3375224416517056,"Iν(fm,n(·)) =
X"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3393177737881508,"z in fm,n(·) that contain ν"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.34111310592459604,"O(ν, z)
P"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.34290843806104127,"ρ in z O(ρ, z) · z,
(11)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.34470377019748655,"where O(ν, z) denotes the number of occurrences of ν among the variables of z.
154"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3464991023339318,"Recall that |V (z)| stand for the number of unique variables in z. Hence the total number of
155"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.348294434470377,occurrences of all the variables P
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3500897666068223,"ρ in z O(ρ, z) is not necessarily equal to |V (z)|. For example,
156"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3518850987432675,"if all of {A(1)
α10,α11, . . . , A(L)
αL0,αL1} in z are unique entries in A, then they can be considered as L
157"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.35368043087971274,"independent variables in the function representing z. If two of these occurrences of variables refer to
158"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.35547576301615796,"the same entry in A, then there are only (L −1) unique variables related to A.
159"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.35727109515260325,"Although Theorem 5 gives the contribution of each entry in A, X and P’s, we need to further attribute
160"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3590664272890485,"P’s to A and X and allocate the contribution of each activation pattern P (r)
a,b to node features X and
161"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3608617594254937,"edges A by considering all non-zero features in Xa of node va and the edges within m hops of node
162"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.362657091561939,"va, as these inputs may contribute to the activation pattern P (r)
a,b . However, determining the exact
163"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3644524236983842,"contribution of each feature that contributes to P (r)
a,b is not straightforward due to non-linear activation.
164"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.36624775583482944,"We approximately attribute all relevant features equally to P (r)
a,b . That is, each input feature ν that has
165"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.36804308797127466,"nonzero contribution to P (r)
a,b will share an equal contribution of IP (r)
a,b (fm,n(·))/|V (P (r)
a,b )|, where
166"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.36983842010771995,"|V (P (r)
a,b )| denotes the number of distinct node and edge features in X and A contributing to P (r)
a,b ,
167"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.37163375224416517,"which is exactly all non-zero features in Xa of node va and the adjacency matrix entries within r
168"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3734290843806104,"hops of node va. Finally, based on Equation (11), we can obtain the contribution of an input feature ν
169"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3752244165170557,"in X, A of a graph instance to the (m, n)-th entry of the GNN output f(·) as:
170"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3770197486535009,"bIν(fm,n(·)) = Iν(fm,n(·)) +
X"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.37881508078994613,"P (r)
a,b in fm,n(·), with ν in P (r)
a,b"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.38061041292639136,"IP (r)
a,b (fm,n(·))"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.38240574506283664,"|V (P (r)
a,b )|
,
(12)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.38420107719928187,"where ν is an entry in the adjacency matrix A or the input feature matrix X, P (r)
a,b denotes an entry in
171"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3859964093357271,"all the activation patterns. Thus, we have attributed f(·) to each input feature of a given data instance.
172"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3877917414721723,"Our approach meets the completeness axiom, which is a critical requirement in attribution methods [25,
173"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3895870736086176,"24, 6]. This axiom guarantees that the attribution scores for input features add up to the difference
174"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.39138240574506283,"in the GNN’s output with and without those features. Passing this sanity check implies that our
175"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.39317773788150806,"approach provides a more comprehensive account of feature importance than existing methods that
176"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.39497307001795334,"only rank the top features [3, 17, 20, 31, 27, 23].
177"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.39676840215439857,"3.3
Case Study: Explaining Graph Convolutional Network (GCN)
178"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.3985637342908438,"GCNs use a simple sum in the combination step, and the adjacency matrix is normalized with the
179"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.400359066427289,"diagonal node degree matrix D. Hence, the hidden state of a GCN’s l-th message-passing layer is:
180"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4021543985637343,"H(l) = ReLU

V H(l−1)W (l) + B(l)
,
(13)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.40394973070017953,where V = D−1
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.40574506283662476,2 (A + I)D−1
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.40754039497307004,"2 represents the normalized adjacency matrix with self-loops added.
181"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.40933572710951527,"Suppose a GCN has three convolution layers and a 2-layer MLP as the classifier, then its expansion
182"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4111310592459605,"form without the activation functions ReLU(·) will be:
183"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4129263913824057,"f(V, X)̸P = V (3)V (2)V (1)XW (1)W (2)W (3)W (c1)W (c2) + V (3)V (2)B(1)W (2)W (3)W (c1)W (c2)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.414721723518851,"+ V (3)B(2)W (3)W (c1)W (c2) + B(3)W (c1)W (c2) + B(c1)W (c2) + B(c2),
(14)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.41651705565529623,"where V (l) = V is the normalized adjacency matrix in the l-th layer’s calculation. In the actual
184"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.41831238779174146,"expansion form with the activation patterns, the corresponding P (m)’s are multiplied whenever
185"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.42010771992818674,"there is a W (m) or B(m) in a scalar product, excluding the last layer W (c2) and B(c2). For ex-
186"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.42190305206463197,"ample, in the scalar products corresponding to V (3)V (2)V (1)XW (1)W (2)W (3)W (c1)W (c2), there
187"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4236983842010772,"are eight variables consisting of four P’s, one X, and three V ’s.
By Equation (11), an ad-
188"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4254937163375224,"jacency entry Vi,j itself will contribute 1"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4272890484739677,"8 of p(V (3)V (2)
:i V (1)
i,j Xj:W (1)W (2)W (3)W (c1)W (c2)) +
189"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.42908438061041293,"p(V (3)
:i V (2)
i,j V (1)
j: XW (1)W (2)W (3)W (c1)W (c2))+p(V (3)
i,j V (2)
j: V (1)XW (1)W (2)W (3)W (c1)W (c2)),
190"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.43087971274685816,"where p(·) denotes the results with the element-wise multiplication of the corresponding activation
191"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4326750448833034,"patterns applied at the appropriate layers. After we obtain the contribution of Vi,j itself on all the
192"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.43447037701974867,"scalar products, we can follow Equation (12) to allocate the contribution of activation patterns to Vi,j.
193"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4362657091561939,"75
80
85
90
95"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4380610412926391,Average sparsity (%) 0.2 0.4 0.6
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4398563734290844,Fidelity
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.44165170556552963,(a) BA-2Motifs
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.44344703770197486,"80
85
90
95"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4452423698384201,Average sparsity (%) 0.0 0.2 0.4 0.6 0.8
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.44703770197486536,Fidelity
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4488330341113106,(b) Mutagenicity
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4506283662477558,"80
85
90
95"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4524236983842011,Average sparsity (%) 0.0 0.2 0.4 0.6
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4542190305206463,Fidelity
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.45601436265709155,(c) NCI1
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4578096947935368,GNNExplainer
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.45960502692998206,PGExplainer
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4614003590664273,PGM-Explainer
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4631956912028725,SubgraphX
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4649910233393178,CF- GNNExplainer
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.466786355475763,RCExplainer
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.46858168761220825,GOAt (Ours)
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4703770197486535,"Figure 1: Fidelity performance averaged across 10 runs on the pretrained GCNs for the datasets at
different levels of average sparsity."
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.47217235188509876,"With Equation (14), we find that when both V and X are set to zeros, f(·) remains non-zero and is:
194"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.473967684021544,"f(0, 0) = p(B(3)W (c1)W (c2)) + p(B(c1)W (c2)) + B(c2),
(15)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4757630161579892,"where B(c2) is the global bias, and the other terms have non-zero entries at the activated neurons. In
195"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.47755834829443444,"other words, certain GNN neurons in the 3-rd and c1-th layers may already be activated prior to any
196"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4793536804308797,"input feature being passed to the network. When we do feed input features, some of these neurons
197"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.48114901256732495,"may remain activated or be toggled off. With Equation (12), we consider taking all 0’s of the X
198"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4829443447037702,"entries, V entries and P entries as the base manifold. Now, given that some of the P entries in GCN
199"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.48473967684021546,"are non-zero when all X and V set to zeros, as present in Equation (15), we will need to subtract the
200"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4865350089766607,"contribution of each features on these P from the contribution values calculated by Equation (12).
201"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4883303411131059,"We let P′ represent the activation patterns of f(0, 0), then the calibrated contribution bIcali
Vi,j(f(·)) of
202"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.49012567324955114,"Vi,j is given by:
203"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4919210053859964,"bIcali
Vi,j(f(·)) = bIVi,j(f(V, X)) −
X"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.49371633752244165,"P ′(r)
a,b in f(0,0), with Vi,j in P ′(r)
a,b"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4955116696588869,"IP ′(r)
a,b (f(0, 0))"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.49730700179533216,"|V (P (r)
a,b )|
.
(16)"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.4991023339317774,"In graph classification tasks, a pooling layer such as mean-pooling is added after the convolution
204"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5008976660682226,"layers to obtain the graph representation. To determine the contribution of each input feature, we can
205"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5026929982046678,"simply apply the same pooling operation as used in the pre-trained GCN.
206"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5044883303411131,"As we mentioned in Section 2, we aim to obtain the explanations by the critical edges in this paper,
207"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5062836624775583,"since edges have more fine-grained information than nodes. Therefore, we treat the edges as variables,
208"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5080789946140036,"while considering the node features X as constants similar to parameters W or B. This setup naturally
209"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5098743267504489,"aggregates the contribution of node features onto edges. By leveraging edge attributions, we are able
210"
EXPLAINING GRAPH NEURAL NETWORKS VIA ATTRIBUTION,0.5116696588868941,"to effectively highlight motifs within the graph structure.
211"
EXPERIMENTS,0.5134649910233393,"4
Experiments
212"
EXPERIMENTS,0.5152603231597845,"We conduct a series of experiments on the fidelity, discriminability and stability metrics to compare
213"
EXPERIMENTS,0.5170556552962298,"our method with the state-of-the-art methods including GNNExplainer [31], PGExplainer [17], PGM-
214"
EXPERIMENTS,0.518850987432675,"Explainer [27], SubgraphX [33], CF-GNNExplainer [16], RCExplainer [3], RG-Explainer [23] and
215"
EXPERIMENTS,0.5206463195691203,"DEGREE [8]. As outlined in Section 2, we highlight edges as explanations as suggested by [7]. For
216"
EXPERIMENTS,0.5224416517055656,"baselines that identify nodes or subgraphs as explanations, we adopt the evaluation setup from [3].
217"
EXPERIMENTS,0.5242369838420108,"We evaluate the performance of explanations on three variants of GNNs, which are GCN [14],
218"
EXPERIMENTS,0.526032315978456,"GraphSAGE [9] and GIN [30]. The experiments are conducted on both the graph classification
219"
EXPERIMENTS,0.5278276481149012,"task and the node classification task. For graph classification task, we evaluate on a synthetic
220"
EXPERIMENTS,0.5296229802513465,"dataset, BA-2motifs [17], and two real-world datasets, Mutagenicity [13] and NCI1 [19]. For node
221"
EXPERIMENTS,0.5314183123877917,"classification task, we evaluate on three synthetic datasets [17], which are BA-shapes, BA-Community
222"
EXPERIMENTS,0.533213644524237,"and Tree-grid. As space is limited, we will only present the key results here. Fidelity results on GIN
223"
EXPERIMENTS,0.5350089766606823,"and GraphSAGE, as well as the results of node classification tasks can be found in the Appendix.
224"
EXPERIMENTS,0.5368043087971275,"Discussions on the controversial metrics such as accuracy are also moved to the Appendix.
225"
FIDELITY,0.5385996409335727,"4.1
Fidelity
226"
FIDELITY,0.5403949730700179,"Fidelity [20, 32, 29, 3] is the decrease of predicted probability between original and new predictions
227"
FIDELITY,0.5421903052064632,"after removing important edges, which are used to evaluate the faithfulness of explanations. It is
228"
FIDELITY,0.5439856373429084,"70
75
80
85
90"
FIDELITY,0.5457809694793537,Average sparsity (%) 10 −3 10 −2 10 −1 10 0 10 1
FIDELITY,0.547576301615799,Disc)i%inability
FIDELITY,0.5493716337522442,(a)BA-2Motifs
FIDELITY,0.5511669658886894,"70
75
80
85
90"
FIDELITY,0.5529622980251346,Average sparsity (%) 10 −1 10 0
FIDELITY,0.5547576301615799,Disc)i%inability
FIDELITY,0.5565529622980251,(b)Mutagenicity
FIDELITY,0.5583482944344704,"70
75
80
85
90"
FIDELITY,0.5601436265709157,A−e)age s(a)sity (%) 10 −2 10 −1 10 0
FIDELITY,0.5619389587073609,Disc)i%inability
FIDELITY,0.5637342908438061,(c)NCI1
FIDELITY,0.5655296229802513,GNNEx(laine)
FIDELITY,0.5673249551166966,PGEx(laine)
FIDELITY,0.5691202872531418,PGM-Explainer
FIDELITY,0.5709156193895871,RG-Explainer
FIDELITY,0.5727109515260324,SubgraphX
FIDELITY,0.5745062836624776,RCExplainer
FIDELITY,0.5763016157989228,DEGREE
FIDELITY,0.578096947935368,GOAt (Ours)
FIDELITY,0.5798922800718133,Original
FIDELITY,0.5816876122082585,"Figure 2: Discriminability performance averaged across 10 runs of the explanations produced by
various GNN explainers at different levels of sparsity. ""Original"" refer to the performance of feeding
the original data into the GNN without any modifications or explanations applied."
FIDELITY,0.5834829443447038,"Class 0
Class 1"
FIDELITY,0.585278276481149,"(a)GNNExplainer
(b)PGExplainer
(c)PGM-Explainer
(d)RG-Explainer"
FIDELITY,0.5870736086175943,"(e)SubgraphX
(f)RCExplainer
(g)DEGREE
(h)GOAt (Ours)"
FIDELITY,0.5888689407540395,(i) Original
FIDELITY,0.5906642728904847,"Figure 3: Visualization of explanation embeddings on the BA-2Motifs dataset. Subfigure (i) refers
to the visualization of the original embeddings by directly feeding the original data into the GNN
without any modifications or explanations applied."
FIDELITY,0.59245960502693,"defined as fidelity(S, G) = fy(G) −fy(G\S). As pointed out by [32], the fidelity may be sensitive
229"
FIDELITY,0.5942549371633752,"to sparsity of explanations. The sparsity of an explanation S ⊆E for a graph G = {V, E} is given by
230"
FIDELITY,0.5960502692998204,"sparsity(S, G) = 1 −|S|"
FIDELITY,0.5978456014362658,"|E|. It indicates the percentage of edges that remain in G after the removal of
231"
FIDELITY,0.599640933572711,"edges in S. Higher sparsity means fewer edges are identified as critical, which may have a smaller
232"
FIDELITY,0.6014362657091562,"impact on the prediction probability. Therefore, we compare fidelity performance under similar levels
233"
FIDELITY,0.6032315978456014,"of average sparsity, as in [33, 29, 3]. Figure 1 displays the fidelity results, with the baseline results
234"
FIDELITY,0.6050269299820467,"sourced from [3]. Our proposed approach, GOAt, consistently outperforms the baselines in terms
235"
FIDELITY,0.6068222621184919,"of fidelity across all sparsity levels, validating its superior performance in generating accurate and
236"
FIDELITY,0.6086175942549371,"reliable faithful explanations. Among the other methods, RCExplainer exhibits the highest fidelity, as
237"
FIDELITY,0.6104129263913824,"it is specifically designed for fidelity optimization. Notably, unlike the other methods that require
238"
FIDELITY,0.6122082585278277,"training and hyperparameter tuning, GOAt offers the advantage of being a training-free approach,
239"
FIDELITY,0.6140035906642729,"thereby avoiding any errors across different runs.
240"
DISCRIMINABILITY,0.6157989228007181,"4.2
Discriminability
241"
DISCRIMINABILITY,0.6175942549371634,"Discriminability, also known as discrimination ability [5, 12], refers to the ability of the explanations
242"
DISCRIMINABILITY,0.6193895870736086,"to distinguish between the classes. We define the discriminability between two classes c1 and c2 as the
243"
DISCRIMINABILITY,0.6211849192100538,"L2 norm of the difference between the mean values of explanation embeddings of the two classes. The
244"
DISCRIMINABILITY,0.6229802513464991,"embeddings used for explanations are taken prior to the last-layer classifier, with node embeddings
245"
DISCRIMINABILITY,0.6247755834829444,"employed for node classification tasks and graph embeddings utilized for graph classification tasks.
246"
DISCRIMINABILITY,0.6265709156193896,"In this procedure, only the explanation subgraph S is fed into the GNN instead of G.
247"
DISCRIMINABILITY,0.6283662477558348,"We show the discriminability across various sparsity levels on GCN, as illustrated in Figure 2. Due to
248"
DISCRIMINABILITY,0.63016157989228,"the significant performance gap between the baselines and GOAt, a logarithmic scale is employed.
249"
DISCRIMINABILITY,0.6319569120287253,"Our approach consistently outperforms the baselines in terms of discriminability across all sparsity
250"
DISCRIMINABILITY,0.6337522441651705,"levels, demonstrating its superior ability to generate accurate and reliable class-specific explanations.
251"
DISCRIMINABILITY,0.6355475763016158,"Notably, at sparsity = 0.7, GOAt achieves higher discriminability than the original graphs on the
252"
DISCRIMINABILITY,0.6373429084380611,"BA-2Motifs and NCI1 datasets. This indicates that GOAt effectively reduces noise unrelated to
253"
DISCRIMINABILITY,0.6391382405745063,"the investigated class while producing informative class explanations. Additionally, we observe a
254"
DISCRIMINABILITY,0.6409335727109515,"1
3
5
7
9"
DISCRIMINABILITY,0.6427289048473968,"T
op k motifs (k) 0 20 40 60 80 100"
DISCRIMINABILITY,0.644524236983842,Coverage (%)
DISCRIMINABILITY,0.6463195691202872,(a)BA-2Motifs
DISCRIMINABILITY,0.6481149012567325,"5
105
205
305
405
505"
DISCRIMINABILITY,0.6499102333931778,"T
op k motifs (k) 0 20 40 60"
DISCRIMINABILITY,0.651705565529623,Coverage (%)
DISCRIMINABILITY,0.6535008976660682,(b)Mutagenicity
DISCRIMINABILITY,0.6552962298025135,"5
105
205
305
405
505"
DISCRIMINABILITY,0.6570915619389587,"T
op k motifs (k) 0 20 40 60"
DISCRIMINABILITY,0.6588868940754039,Coverage (%)
DISCRIMINABILITY,0.6606822262118492,(c)NCI1
DISCRIMINABILITY,0.6624775583482945,GNNExplainer
DISCRIMINABILITY,0.6642728904847397,PGExplainer
DISCRIMINABILITY,0.6660682226211849,PGM-Explainer
DISCRIMINABILITY,0.6678635547576302,RG-Explainer
DISCRIMINABILITY,0.6696588868940754,SubgraphX
DISCRIMINABILITY,0.6714542190305206,RCExplainer
DISCRIMINABILITY,0.6732495511669659,DEGREE
DISCRIMINABILITY,0.6750448833034112,GOAt (Ours)
DISCRIMINABILITY,0.6768402154398564,Figure 4: Coverage of the top-k explanations across the datasets.
DISCRIMINABILITY,0.6786355475763016,"substantial decrease in discriminability between sparsity levels of 0.75 and 0.8 on BA-2Motifs. This
255"
DISCRIMINABILITY,0.6804308797127468,"implies that a minimum of approximately 25% of the edges is necessary to distinguish between
256"
DISCRIMINABILITY,0.6822262118491921,"the classes, which is in line with our expectation, given that a ""house"" motif, consisting of 6 edges,
257"
DISCRIMINABILITY,0.6840215439856373,"usually represents 24% of the total edges (on average, the total number of edges in BA-2Motifs is 25).
258"
DISCRIMINABILITY,0.6858168761220825,"Furthermore, we present scatter plots to visualize the explanation embeddings generated by various
259"
DISCRIMINABILITY,0.6876122082585279,"GNN explainers. Figure 3 showcases the explanation embeddings obtained from different GNN
260"
DISCRIMINABILITY,0.6894075403949731,"explaining methods on the BA-2Motifs dataset, with sparsity = 0.7. More scatter plots on Muta-
261"
DISCRIMINABILITY,0.6912028725314183,"genicity and NCI1 and can be found in the Appendix. The explanations generated by GNNExplainer
262"
DISCRIMINABILITY,0.6929982046678635,"fail to exhibit class discrimination, as all the data points are clustered together without any distinct
263"
DISCRIMINABILITY,0.6947935368043088,"separation. While some of the Class 1 explanations produced by PGExplainer, PGM-Explainer,
264"
DISCRIMINABILITY,0.696588868940754,"RG-Explainer, RCExplainer, and DEGREE are noticeably separate from the Class 0 explanations, the
265"
DISCRIMINABILITY,0.6983842010771992,"majority of the data points remain closely clustered together. As for SubgraphX, most of its Class 1
266"
DISCRIMINABILITY,0.7001795332136446,"explanations are isolated from the Class 0 explanations, but there is a discernible overlap between the
267"
DISCRIMINABILITY,0.7019748653500898,"Class 1 and Class 0 data points. In contrast, our method, GOAt, generates explanations that clearly
268"
DISCRIMINABILITY,0.703770197486535,"and effectively distinguish between Class 0 and Class 1, with no overlapping points and a substantial
269"
DISCRIMINABILITY,0.7055655296229802,"separation distance, highlighting the strong discriminability of our approach.
270"
STABILITY OF EXTRACTING MOTIFS,0.7073608617594255,"4.3
Stability of extracting motifs
271"
STABILITY OF EXTRACTING MOTIFS,0.7091561938958707,"As we will later show in Section 4.4, it is often observed that datasets contain specific class motifs.
272"
STABILITY OF EXTRACTING MOTIFS,0.7109515260323159,"For instance, in the BA-2Motifs dataset, the Class 1 motif exhibits a ""house"" structure. To ensure
273"
STABILITY OF EXTRACTING MOTIFS,0.7127468581687613,"the stability of GNN explainers in capturing the class motifs across diverse data samples, we aim
274"
STABILITY OF EXTRACTING MOTIFS,0.7145421903052065,"for the explanation motifs to exhibit relative consistency for data samples with similar properties,
275"
STABILITY OF EXTRACTING MOTIFS,0.7163375224416517,"rather than exhibiting significant variations. To quantify this characteristic, we introduce the stability
276"
STABILITY OF EXTRACTING MOTIFS,0.718132854578097,"metric, which measures the coverage of the top-k explanations across the dataset. An ideal explainer
277"
STABILITY OF EXTRACTING MOTIFS,0.7199281867145422,"should generate explanations that cover a larger number of data samples using fewer motifs. This
278"
STABILITY OF EXTRACTING MOTIFS,0.7217235188509874,"characteristic is also highly desirable in global-level explainers, such as [2, 11]. We illustrate the
279"
STABILITY OF EXTRACTING MOTIFS,0.7235188509874326,"stability of the unbiased class as the percentage converge of the top-k explanations produced on
280"
STABILITY OF EXTRACTING MOTIFS,0.725314183123878,"GCN with sparsity = 0.7 in Figure 4. Our approach surpasses the baselines by a considerable
281"
STABILITY OF EXTRACTING MOTIFS,0.7271095152603232,"margin in terms of the stability of producing explanations. Specifically, GOAt is capable of providing
282"
STABILITY OF EXTRACTING MOTIFS,0.7289048473967684,"explanations for all the Class 1 data samples using only three explanations. This explains why there
283"
STABILITY OF EXTRACTING MOTIFS,0.7307001795332136,"are only three Class 1 scatters visible in Figure 3.
284"
QUALITATIVE ANALYSIS,0.7324955116696589,"4.4
Qualitative analysis
285"
QUALITATIVE ANALYSIS,0.7342908438061041,"We present the qualitative results of our approach in Table 1, where we compare it with state-of-the-art
286"
QUALITATIVE ANALYSIS,0.7360861759425493,"baselines such as PGExplainer, SubgraphX, and RCExplainer. The pretrained GNN achieves a 100%
287"
QUALITATIVE ANALYSIS,0.7378815080789947,"accuracy on the BA-2Motifs dataset. As long as it successfully identifies one class, the remaining
288"
QUALITATIVE ANALYSIS,0.7396768402154399,"data samples naturally belong to the other class, leading to a perfect accuracy rate. Based on the
289"
QUALITATIVE ANALYSIS,0.7414721723518851,"explanations from GOAt, we have observed that the GNN effectively recognizes the ""house"" motif
290"
QUALITATIVE ANALYSIS,0.7432675044883303,"that is associated with Class 1. In contrast, other approaches face difficulties in consistently capturing
291"
QUALITATIVE ANALYSIS,0.7450628366247756,"this motif. The Class 0 motifs in the Mutagenicity dataset generated by GOAt represent multiple
292"
QUALITATIVE ANALYSIS,0.7468581687612208,"connected carbon rings. This indicates that the presence of more carbon rings in a molecule increases
293"
QUALITATIVE ANALYSIS,0.748653500897666,"its likelihood of being mutagenic (Class 0), while the presence of more ""C-H"" or ""O-H"" bonds in a
294"
QUALITATIVE ANALYSIS,0.7504488330341114,"molecule increases its likelihood of being non-mutagenic (Class 1). Similarly, in the NCI1 dataset,
295"
QUALITATIVE ANALYSIS,0.7522441651705566,"GOAt discovers that the GNN considers a higher number of carbon rings as evidence of chemical
296"
QUALITATIVE ANALYSIS,0.7540394973070018,"Table 1: Qualitative results of the top motifs of each class produced by PGExplainer, SubgraphX,
RCExplainer and GOAt. The percentages indicate the coverage of the explanations."
QUALITATIVE ANALYSIS,0.755834829443447,"BA-2Motifs
Mutagenicity
NCI1
Class0
Class1
Class0
Class1
Class0
Class1"
QUALITATIVE ANALYSIS,0.7576301615798923,PGExplainer C N C
QUALITATIVE ANALYSIS,0.7594254937163375,"C
C
C
C
C C
C C C Cl C C
C
C C
C ×2"
QUALITATIVE ANALYSIS,0.7612208258527827,"×2
×N
×N"
QUALITATIVE ANALYSIS,0.7630161579892281,"4.8%
1.8%
1.2%
1.3%
0.1%
0.5%"
QUALITATIVE ANALYSIS,0.7648114901256733,"SubgraphX Br C H
H C H C"
QUALITATIVE ANALYSIS,0.7666068222621185,"N
0.4%
12.8%
0.2%
0.2%
0.2%
0.1%"
QUALITATIVE ANALYSIS,0.7684021543985637,"RCExplainer C C
C C C C
C ×N
H C H C Br Br"
QUALITATIVE ANALYSIS,0.770197486535009,"×N
×N
×N"
QUALITATIVE ANALYSIS,0.7719928186714542,"6.4%
6.2%
0.4%
0.5%
0.05%
0.1% GOAt ×N
×N C C C ×N C C C C C ×N H C ×N H O"
QUALITATIVE ANALYSIS,0.7737881508078994,"×N
×N
3.8%
3.4%
93.4%
4%
3.5%
2.2%
2.2%
1.2%
3.5%
1.2%
4.3%
4.0%"
QUALITATIVE ANALYSIS,0.7755834829443446,"compounds being active against non-small cell lung cancer. Other approaches, on the other hand, fail
297"
QUALITATIVE ANALYSIS,0.77737881508079,"to provide clear and human-understandable explanations.
298"
RELATED WORK,0.7791741472172352,"5
Related Work
299"
RELATED WORK,0.7809694793536804,"Local-level Graph Neural Network (GNN) explanation approaches have been developed to shed
300"
RELATED WORK,0.7827648114901257,"light on the decision-making process of GNN models at the individual data instance level. Most of
301"
RELATED WORK,0.7845601436265709,"them, such as GNNExplainer [31], PGExplainer [17], PGM-Explainer [27], GraphLime [10], RG-
302"
RELATED WORK,0.7863554757630161,"Explainer [23], CF-GNNExplainer [16], RCExplainer [3], CF2 [26], RelEx [34] and Gem [15], train a
303"
RELATED WORK,0.7881508078994613,"secondary model to identify crucial nodes, edges, or subgraphs that explain the behavior of pretrained
304"
RELATED WORK,0.7899461400359067,"GNNs for specific input samples. However, the quality of the explanations produced by these methods
305"
RELATED WORK,0.7917414721723519,"is highly dependent on hyperparameter choices. Moreover, these explainers’ black-box nature raises
306"
RELATED WORK,0.7935368043087971,"doubts about their ability to provide comprehensive explanations for GNN models. Approaches like
307"
RELATED WORK,0.7953321364452424,"SA [4], Grad-CAM [20], GNN-LRP [22], and DEGREE [8], which rely on gradient back-propagation,
308"
RELATED WORK,0.7971274685816876,"encounter the saturation problem [24]. As a result, these methods may generate explanations that are
309"
RELATED WORK,0.7989228007181328,"less faithful. SubgraphX [33] combines perturbation-based techniques with pruning using Shapley
310"
RELATED WORK,0.800718132854578,"values. While it can generate some high-quality subgraph explanations, its computational cost is
311"
RELATED WORK,0.8025134649910234,"significantly high due to the reliance on the MCTS (Monte Carlo Tree Search). Additionally, as
312"
RELATED WORK,0.8043087971274686,"demonstrated in our experiments in Section 4, existing approaches exhibit inconsistencies on similar
313"
RELATED WORK,0.8061041292639138,"data samples and poor discriminability. This reinforces the need for our proposed method GOAt,
314"
RELATED WORK,0.8078994614003591,"which outperforms state-of-the-art baselines on fidelity, discriminability and stability metrics. Our
315"
RELATED WORK,0.8096947935368043,"work also relates to global-level explainability approaches. GLGExplainer [2] leverages prototype
316"
RELATED WORK,0.8114901256732495,"learning and builds upon PGExplainer to obtain global explanations. GCFExplainer [11] generates
317"
RELATED WORK,0.8132854578096947,"global counterfactual explanations by employing random walks on an edit map of graphs, utilizing
318"
RELATED WORK,0.8150807899461401,"local explanations from RCExplainer and CF2. Both GLGExplainer and GCFExplainer heavily rely
319"
RELATED WORK,0.8168761220825853,"on local explanations. Integrating local explainers that produce higher-quality local explanations,
320"
RELATED WORK,0.8186714542190305,"such as GOAt, has the potential to enhance the performance of these global-level explainers.
321"
CONCLUSION,0.8204667863554758,"6
Conclusion
322"
CONCLUSION,0.822262118491921,"We propose GOAt, a local-level GNN explainer that overcomes the limitations of existing GNN
323"
CONCLUSION,0.8240574506283662,"explainers, in terms of insufficient discriminability, inconsistency on same-class data samples, and
324"
CONCLUSION,0.8258527827648114,"overfitting to noise. We analytically expand GNN outputs for each class into a sum of scalar products
325"
CONCLUSION,0.8276481149012568,"and attribute each scalar product to each input feature. Although GOAt shares similar limitations
326"
CONCLUSION,0.829443447037702,"with some decomposition methods of requiring expert knowledge to design corresponding explaining
327"
CONCLUSION,0.8312387791741472,"processes for various GNNs, our extensive experiments on both synthetic and real datasets, along
328"
CONCLUSION,0.8330341113105925,"with qualitative analysis, demonstrate its superior explanation ability. Our method contributes to
329"
CONCLUSION,0.8348294434470377,"enhancing the transparency of decision-making in various fields where GNNs are widely applied.
330"
REFERENCES,0.8366247755834829,"References
331"
REFERENCES,0.8384201077199281,"[1] Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, and Vince I Madai. Explainability for
332"
REFERENCES,0.8402154398563735,"artificial intelligence in healthcare: a multidisciplinary perspective. BMC Medical Informatics and Decision
333"
REFERENCES,0.8420107719928187,"Making, 20(1):1–9, 2020.
334"
REFERENCES,0.8438061041292639,"[2] Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Lio, and Andrea Passerini. Global explainability of
335"
REFERENCES,0.8456014362657092,"GNNs via logic combination of learned concepts. In The Eleventh International Conference on Learning
336"
REFERENCES,0.8473967684021544,"Representations, 2023.
337"
REFERENCES,0.8491921005385996,"[3] Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong Zhang.
338"
REFERENCES,0.8509874326750448,"Robust counterfactual explanations on graph neural networks. Advances in Neural Information Processing
339"
REFERENCES,0.8527827648114902,"Systems, 34:5644–5655, 2021.
340"
REFERENCES,0.8545780969479354,"[4] Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks.
341"
REFERENCES,0.8563734290843806,"In International Conference on Machine Learning (ICML) Workshops, 2019 Workshop on Learning and
342"
REFERENCES,0.8581687612208259,"Reasoning with Graph-Structured Representations, 2019.
343"
REFERENCES,0.8599640933572711,"[5] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantify-
344"
REFERENCES,0.8617594254937163,"ing interpretability of deep visual representations. In Proceedings of the IEEE Conference on Computer
345"
REFERENCES,0.8635547576301615,"Vision and Pattern Recognition (CVPR), July 2017.
346"
REFERENCES,0.8653500897666068,"[6] Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, Klaus-Robert Müller, and Wojciech Samek.
347"
REFERENCES,0.8671454219030521,"Layer-wise relevance propagation for neural networks with local renormalization layers. In Artificial
348"
REFERENCES,0.8689407540394973,"Neural Networks and Machine Learning–ICANN 2016: 25th International Conference on Artificial Neural
349"
REFERENCES,0.8707360861759426,"Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25, pages 63–71. Springer, 2016.
350"
REFERENCES,0.8725314183123878,"[7] Lukas Faber, Amin K. Moghaddam, and Roger Wattenhofer. When comparing to ground truth is wrong: On
351"
REFERENCES,0.874326750448833,"evaluating gnn explanation methods. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
352"
REFERENCES,0.8761220825852782,"Discovery & Data Mining, pages 332–341, 2021.
353"
REFERENCES,0.8779174147217235,"[8] Qizhang Feng, Ninghao Liu, Fan Yang, Ruixiang Tang, Mengnan Du, and Xia Hu. Degree: Decomposition
354"
REFERENCES,0.8797127468581688,"based explanation for graph neural networks. In International Conference on Learning Representations,
355"
REFERENCES,0.881508078994614,"2022.
356"
REFERENCES,0.8833034111310593,"[9] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
357"
REFERENCES,0.8850987432675045,"Advances in neural information processing systems, 30, 2017.
358"
REFERENCES,0.8868940754039497,"[10] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable
359"
REFERENCES,0.8886894075403949,"model explanations for graph neural networks. IEEE Transactions on Knowledge and Data Engineering,
360"
REFERENCES,0.8904847396768402,"2022.
361"
REFERENCES,0.8922800718132855,"[11] Zexi Huang, Mert Kosan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Global counterfactual explainer
362"
REFERENCES,0.8940754039497307,"for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search
363"
REFERENCES,0.895870736086176,"and Data Mining, pages 141–149, 2023.
364"
REFERENCES,0.8976660682226212,"[12] Brian Kenji Iwana, Ryohei Kuroki, and Seiichi Uchida. Explaining convolutional neural networks using
365"
REFERENCES,0.8994614003590664,"softmax gradient layer-wise relevance propagation. In 2019 IEEE/CVF International Conference on
366"
REFERENCES,0.9012567324955116,"Computer Vision Workshop (ICCVW), pages 4176–4185, 2019.
367"
REFERENCES,0.9030520646319569,"[13] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for muta-
368"
REFERENCES,0.9048473967684022,"genicity prediction. Journal of medicinal chemistry, 48(1):312–320, 2005.
369"
REFERENCES,0.9066427289048474,"[14] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
370"
REFERENCES,0.9084380610412927,"International Conference on Learning Representations, 2017.
371"
REFERENCES,0.9102333931777379,"[15] Wanyu Lin, Hao Lan, and Baochun Li. Generative causal explanations for graph neural networks. In
372"
REFERENCES,0.9120287253141831,"International Conference on Machine Learning, pages 6666–6679. PMLR, 2021.
373"
REFERENCES,0.9138240574506283,"[16] Ana Lucic, Maartje A Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. Cf-
374"
REFERENCES,0.9156193895870736,"gnnexplainer: Counterfactual explanations for graph neural networks. In International Conference on
375"
REFERENCES,0.9174147217235189,"Artificial Intelligence and Statistics, pages 4499–4511. PMLR, 2022.
376"
REFERENCES,0.9192100538599641,"[17] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.
377"
REFERENCES,0.9210053859964094,"Parameterized explainer for graph neural network. Advances in neural information processing systems,
378"
REFERENCES,0.9228007181328546,"33:19620–19631, 2020.
379"
REFERENCES,0.9245960502692998,"[18] Xinjun Pei, Long Yu, and Shengwei Tian. Amalnet: A deep learning framework based on graph convolu-
380"
REFERENCES,0.926391382405745,"tional networks for malware detection. Computers & Security, 93:101792, 2020.
381"
REFERENCES,0.9281867145421903,"[19] Douglas EV Pires, Tom L Blundell, and David B Ascher. pkcsm: predicting small-molecule pharmacoki-
382"
REFERENCES,0.9299820466786356,"netic and toxicity properties using graph-based signatures. Journal of medicinal chemistry, 58(9):4066–
383"
REFERENCES,0.9317773788150808,"4072, 2015.
384"
REFERENCES,0.933572710951526,"[20] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explain-
385"
REFERENCES,0.9353680430879713,"ability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF conference on
386"
REFERENCES,0.9371633752244165,"computer vision and pattern recognition, pages 10772–10781, 2019.
387"
REFERENCES,0.9389587073608617,"[21] Michael Sejr Schlichtkrull, Nicola De Cao, and Ivan Titov. Interpreting graph neural networks for nlp with
388"
REFERENCES,0.940754039497307,"differentiable edge masking. In International Conference on Learning Representations, 2021.
389"
REFERENCES,0.9425493716337523,"[22] Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T Schütt, Klaus-Robert Müller,
390"
REFERENCES,0.9443447037701975,"and Grégoire Montavon. Higher-order explanations of graph neural networks via relevant walks. IEEE
391"
REFERENCES,0.9461400359066428,"transactions on pattern analysis and machine intelligence, 44(11):7581–7596, 2021.
392"
REFERENCES,0.947935368043088,"[23] Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li. Reinforcement learning enhanced
393"
REFERENCES,0.9497307001795332,"explainer for graph neural networks. Advances in Neural Information Processing Systems, 34:22523–22533,
394"
REFERENCES,0.9515260323159784,"2021.
395"
REFERENCES,0.9533213644524237,"[24] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagat-
396"
REFERENCES,0.9551166965888689,"ing activation differences. In International conference on machine learning, pages 3145–3153. PMLR,
397"
REFERENCES,0.9569120287253142,"2017.
398"
REFERENCES,0.9587073608617595,"[25] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International
399"
REFERENCES,0.9605026929982047,"conference on machine learning, pages 3319–3328. PMLR, 2017.
400"
REFERENCES,0.9622980251346499,"[26] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. Learning
401"
REFERENCES,0.9640933572710951,"and evaluating graph neural network explanations based on counterfactual and factual reasoning. In
402"
REFERENCES,0.9658886894075404,"Proceedings of the ACM Web Conference 2022, pages 1018–1027, 2022.
403"
REFERENCES,0.9676840215439856,"[27] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural
404"
REFERENCES,0.9694793536804309,"networks. Advances in neural information processing systems, 33:12225–12235, 2020.
405"
REFERENCES,0.9712746858168761,"[28] Daixin Wang, Zhiqiang Zhang, Jun Zhou, Peng Cui, Jingli Fang, Quanhui Jia, Yanming Fang, and Yuan
406"
REFERENCES,0.9730700179533214,"Qi. Temporal-aware graph neural network for credit risk prediction. In Proceedings of the 2021 SIAM
407"
REFERENCES,0.9748653500897666,"International Conference on Data Mining (SDM), pages 702–710. SIAM, 2021.
408"
REFERENCES,0.9766606822262118,"[29] Lingfei Wu, Peng Cui, Jian Pei, and Liang Zhao. Graph Neural Networks: Foundations, Frontiers, and
409"
REFERENCES,0.9784560143626571,"Applications. Springer Nature, 2022.
410"
REFERENCES,0.9802513464991023,"[30] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?
411"
REFERENCES,0.9820466786355476,"In International Conference on Learning Representations, 2019.
412"
REFERENCES,0.9838420107719928,"[31] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating
413"
REFERENCES,0.9856373429084381,"explanations for graph neural networks. Advances in neural information processing systems, 32, 2019.
414"
REFERENCES,0.9874326750448833,"[32] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A
415"
REFERENCES,0.9892280071813285,"taxonomic survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.
416"
REFERENCES,0.9910233393177738,"[33] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks
417"
REFERENCES,0.992818671454219,"via subgraph explorations. In International Conference on Machine Learning, pages 12241–12252. PMLR,
418"
REFERENCES,0.9946140035906643,"2021.
419"
REFERENCES,0.9964093357271095,"[34] Yue Zhang, David Defazio, and Arti Ramesh. Relex: A model-agnostic relational model explainer. In
420"
REFERENCES,0.9982046678635548,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 1042–1049, 2021.
421"
