Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008873114463176575,"Vision-Language Models (VLMs) have demonstrated their broad effectiveness
2"
ABSTRACT,0.001774622892635315,"thanks to extensive training in aligning visual instructions to responses. However,
3"
ABSTRACT,0.0026619343389529724,"such training of conclusive alignment leads models to ignore essential visual rea-
4"
ABSTRACT,0.00354924578527063,"soning, further resulting in failures in meticulous visual problems and unfaithful
5"
ABSTRACT,0.0044365572315882874,"responses. Drawing inspiration from human cognition in solving visual prob-
6"
ABSTRACT,0.005323868677905945,"lems (e.g., marking, zoom in), this paper introduces Chain of Manipulations,
7"
ABSTRACT,0.006211180124223602,"a mechanism that enables VLMs to solve problems step-by-step with evidence.
8"
ABSTRACT,0.00709849157054126,"After training, models can solve various visual problems by eliciting intrinsic
9"
ABSTRACT,0.007985803016858917,"manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively
10"
ABSTRACT,0.008873114463176575,"without involving external tools, while also allowing users to trace error causes. We
11"
ABSTRACT,0.009760425909494233,"study the roadmap to implement this mechanism, including (1) a ﬂexible design of
12"
ABSTRACT,0.01064773735581189,"manipulations upon extensive analysis, (2) an efﬁcient automated data generation
13"
ABSTRACT,0.011535048802129548,"pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image,
14"
ABSTRACT,0.012422360248447204,"and (4) a model training process for versatile capabilities. With the design, we also
15"
ABSTRACT,0.013309671694764862,"manually annotate 6K high-quality samples for the challenging graphical mathemat-
16"
ABSTRACT,0.01419698314108252,"ical problems. Our trained model, CogCoM, equipped with this mechanism with
17"
ABSTRACT,0.015084294587400177,"17B parameters achieves state-of-the-art performance across 9 benchmarks from
18"
ABSTRACT,0.015971606033717833,"4 categories, demonstrating the effectiveness while preserving the interpretability.
19"
ABSTRACT,0.01685891748003549,"Our code, model weights, and collected data will be publicly available.
20"
INTRODUCTION,0.01774622892635315,"1
Introduction
21"
INTRODUCTION,0.018633540372670808,Large Language Model
INTRODUCTION,0.019520851818988466,"Image tokens
What is written on the 
pillar in front of the man?"
INTRODUCTION,0.02040816326530612,No Smoking
INTRODUCTION,0.02129547471162378,Large Language Model
INTRODUCTION,0.022182786157941437,Large Language Model
INTRODUCTION,0.023070097604259095,"Image tokens
What is written on the 
pillar in front of the man?"
INTRODUCTION,0.023957409050576754,"First, use GROUNDING(man) to find regions [[4,5,7,12], [28,6,30,9]] …
Then, use GROUNDING(pillar near [4,5,7,12]) to find [[8,0,17,13]] …
Zoom into the cropped region [[9,7,11,9]] by two times and re-input …"
INTRODUCTION,0.024844720496894408,Image tokens
INTRODUCTION,0.025732031943212066,"Based on the new image, the text written is QUICK DEPOSIT. W  W "
INTRODUCTION,0.026619343389529725,"Figure 2: In comparison with existing VLMs, CogCoM performs the multiple steps of evidential
reasoning with chain of manipulations (CoM) to achieve the faithful answer to visual scene."
INTRODUCTION,0.027506654835847383,"Beneﬁting from the advantage of Large Language Models (LLMs) in broad world knowledge, large
22"
INTRODUCTION,0.02839396628216504,"Vision Language Models (VLMs) (Alayrac et al., 2022; Wang et al., 2023b) that are further trained
23"
INTRODUCTION,0.029281277728482696,"to understand visual inputs have demonstrated viabilities on broad multimodal scenarios, such as
24"
INTRODUCTION,0.030168589174800354,"visual question answering (Liu et al., 2023b), visual grounding (Peng et al., 2023), optical character
25"
INTRODUCTION,0.031055900621118012,"recognition (Zhang et al., 2023b). The research employing VLMs as foundation models (Bai et al.,
26"
INTRODUCTION,0.03194321206743567,"2023; Sun et al., 2023b; Wang et al., 2023b) usually involves two main stages of training, where
27"
INTRODUCTION,0.032830523513753325,"the ﬁrst stage develops intrinsic visual understanding ability through exposure to massive image-
28"
INTRODUCTION,0.03371783496007098,"caption pairs, and the second stage endows the models with problem-solving capabilities through the
29"
INTRODUCTION,0.03460514640638864,"instruction tuning.
30"
INTRODUCTION,0.0354924578527063,"However, existing tuning methods train models to respond to instructions with conclusive language
31"
INTRODUCTION,0.03637976929902396,"responses upon visual inputs, which leads models to ignore the essential intermediate visual reasoning
32"
INTRODUCTION,0.037267080745341616,"and further results in failures in meticulous visual problems, unfaithful responses, and even hallucina-
33"
INTRODUCTION,0.038154392191659274,"tions. For example in the left subplot of Figure 2, we test the top-performing model CogVLM (Wang
34"
INTRODUCTION,0.03904170363797693,"et al., 2023b) about the details in the image (i.e., texts written on a pillar), and it directly responds
35"
INTRODUCTION,0.03992901508429459,"an incorrect answer (i.e., NO SMOKING), most likely from bias to visual or linguistic priors (i.e.,
36"
INTRODUCTION,0.04081632653061224,"typical scenes with a pillar in ofﬁce). The absence of the essential reasoning on the visual scene may
37"
INTRODUCTION,0.0417036379769299,"lead to a rash response (Hwang et al., 2023).
38"
INTRODUCTION,0.04259094942324756,"Humans solve problems regarding visual details by marking or processing the given images for
39"
INTRODUCTION,0.043478260869565216,"convenience and rigor, which we refer to as manipulations. For example, we ﬁnd targets by sequen-
40"
INTRODUCTION,0.044365572315882874,"tially locating references, and concentrate on subtle details by zooming into a corresponding region.
41"
INTRODUCTION,0.04525288376220053,"Most of VLMs have developed numerous intrinsic capabilities (e.g., grounding boxes, recognizing
42"
INTRODUCTION,0.04614019520851819,"texts) during the ﬁrst stage of training. By further imitating the fundamental human behaviours (e.g.,
43"
INTRODUCTION,0.04702750665483585,"cropping, zoom in), models have the potential to perform this cognitive reasoning process. Three
44"
INTRODUCTION,0.04791481810115351,"major obstacles in eliciting VLMs with such reasoning are (1) ﬂexible deﬁnitions of manipulations
45"
INTRODUCTION,0.048802129547471165,"covering most visual problems, (2) an efﬁcient data collection pipeline capable of producing abundant
46"
INTRODUCTION,0.049689440993788817,"training data, and (3) a multi-turn multi-image VLM structure compatible with existing models.
47"
INTRODUCTION,0.050576752440106475,"Inspired by the human cognition in solving visual problems, we introduce Chain of Manipulations
48"
INTRODUCTION,0.05146406388642413,"(CoM), a mechanism that enables VLMs to solve problems step-by-step with evidence, with each
49"
INTRODUCTION,0.05235137533274179,"step potentially involving a manipulation on the visual input and its corresponding result, both
50"
INTRODUCTION,0.05323868677905945,"generated by the model to facilitate the success and ﬁdelity. This paper studies a complete roadmap
51"
INTRODUCTION,0.05412599822537711,"with manipulations design, data collection, model architecture and training process for training
52"
INTRODUCTION,0.055013309671694766,"general VLMs with this mechanism. We ﬁrst formally design 6 basic manipulations upon the pilot
53"
INTRODUCTION,0.055900621118012424,"experiments, which are capable of handling diverse visual problems. Next, we propose a cascading
54"
INTRODUCTION,0.05678793256433008,"data generation pipeline based on reliable large language models (e.g., LLMs, the linguistic annotators)
55"
INTRODUCTION,0.05767524401064774,"and visual foundational models (e.g., VFMs, the visual annotators), which can automatically produce
56"
INTRODUCTION,0.05856255545696539,"abundant error-free training data. We collect 70K CoM samples with this pipeline. We then devise
57"
INTRODUCTION,0.05944986690328305,"a multi-turn multi-image model architecture compatible with typical VLMs structures. Based on a
58"
INTRODUCTION,0.06033717834960071,"data recipe incorporating the curated corpus, we ﬁnally train a general VLM equipped with CoM
59"
INTRODUCTION,0.061224489795918366,"reasoning mechanism, named CogCoM, which possesses capabilities of chat, captioning, grounding
60"
INTRODUCTION,0.062111801242236024,"and reasoning. Additionally, beneﬁting from the expressive capability of the proposed mechanism,
61"
INTRODUCTION,0.06299911268855368,"we further manually annotated 6K high-quality samples of graphical mathematical problems, each
62"
INTRODUCTION,0.06388642413487133,"accompanied by a CoM reasoning process, to advance the research of VLMs in solving challenging
63"
INTRODUCTION,0.064773735581189,"mathematical problems.
64"
INTRODUCTION,0.06566104702750665,"We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh
65"
INTRODUCTION,0.06654835847382432,"et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson &
66"
INTRODUCTION,0.06743566992014197,"Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO+(Yu
67"
INTRODUCTION,0.06832298136645963,"et al., 2016), and RefCOCOg (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for
68"
INTRODUCTION,0.06921029281277728,"hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability. Our model
69"
INTRODUCTION,0.07009760425909495,"achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks,
70"
INTRODUCTION,0.0709849157054126,"respectively, and the superior performance on the general multimodal benchmark. The results
71"
INTRODUCTION,0.07187222715173026,"demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs.
72"
TERMINOLOGY,0.07275953859804792,"2
Terminology
73"
TERMINOLOGY,0.07364685004436557,"We ﬁrst conduct pilot experiments to investigate the possible manipulations capable of handling
74"
TERMINOLOGY,0.07453416149068323,"diverse visual problems.
75"
TERMINOLOGY,0.07542147293700088,"read, extract, ...
locate, look for ...
 zoom in, focus on ...
0 10000 20000 30000 40000 50000"
TERMINOLOGY,0.07630878438331855,#Frequencies
TERMINOLOGY,0.0771960958296362,"OCR
GROUNDING
ZOOMIN"
TERMINOLOGY,0.07808340727595386,"count, number, ...
calculate, assess, ...
draw, add, ...
0 200 400 600 800 1000"
TERMINOLOGY,0.07897071872227152,#Frequencies
TERMINOLOGY,0.07985803016858918,"COUNTING
CALCULATE
LINE"
TERMINOLOGY,0.08074534161490683,"Figure 3: Distribution of the generated 465 actions
base on GPT-4, mapped into 6 manipulations."
TERMINOLOGY,0.08163265306122448,"Speciﬁcally, given a question about an image,
76"
TERMINOLOGY,0.08251996450754215,"we prompt the advanced large language model,
77"
TERMINOLOGY,0.0834072759538598,"GPT-4, to generate solving steps by optionally
78"
TERMINOLOGY,0.08429458740017746,"utilizing possible actions on the image that fa-
79"
TERMINOLOGY,0.08518189884649512,"cilitate problem-solving. We conduct this ex-
80"
TERMINOLOGY,0.08606921029281278,"periment on 170K questions from TextVQA, a
81"
TERMINOLOGY,0.08695652173913043,"dataset requiring detailed reasoning and recog-
82"
TERMINOLOGY,0.0878438331854481,"nition on images. To ensure the stability, we
83"
TERMINOLOGY,0.08873114463176575,"manually write 4 demonstrations as priors, The
84"
TERMINOLOGY,0.08961845607808341,"detailed statistics are available at Appendix C.3.
85"
TERMINOLOGY,0.09050576752440107,"We utilize the StanfordCoreNLP toolkit to ex-
86"
TERMINOLOGY,0.09139307897071872,"tract verb phrases referring to the actions, and
87"
TERMINOLOGY,0.09228039041703638,"the distribution of frequencies is shown in Fig-
88"
TERMINOLOGY,0.09316770186335403,"ure 3. Through result analysis, we ﬁnd that most
89"
TERMINOLOGY,0.0940550133096717,"of the actions can be mapped to 6 fundamen-
90"
TERMINOLOGY,0.09494232475598935,"tal manipulations on images: OCR, Grounding,
91"
TERMINOLOGY,0.09582963620230701,"CropZoomIn, Counting, Calculate, and Line.
92"
TERMINOLOGY,0.09671694764862467,"Based on the observation, we formally predeﬁne a set of 6 manipulations, which can either be
93"
TERMINOLOGY,0.09760425909494233,"developed from pre-training or be learned from ﬁne-tuning with the imitation to human behaviors:
94"
TERMINOLOGY,0.09849157054125998,"M ⊆{OCR(tgt) →txt, Grounding(tgt) →bbx, Counting(tgt) →num, Calculate(tgt) →
95"
TERMINOLOGY,0.09937888198757763,"num, CropZoomIn(bbx, x) →img, Line(pts) →img}, where the parameters or results
96"
TERMINOLOGY,0.1002661934338953,"tgt, txt, bbx, num, x, img, pts refer to the bounding boxes, zoom ratio, image, target description,
97"
TERMINOLOGY,0.10115350488021295,"numbers, texts, and points, respectively. In addition to the predeﬁned manipulations, we also allow
98"
TERMINOLOGY,0.10204081632653061,"trained models to create new manipulations during inference to facilitate problem-solving. We
99"
TERMINOLOGY,0.10292812777284827,"empirically ﬁnd that more complicated goals can be derived from these fundamental manipulations.
100"
TERMINOLOGY,0.10381543921916593,"We then deﬁne the standard CoM data structure to streamline the subsequent data construction
101"
TERMINOLOGY,0.10470275066548358,"and validation process. Given a question Q about an initial input image I0, a VLM equipped with
102"
TERMINOLOGY,0.10559006211180125,"chain of manipulations mechanism solves the problem to achieve ﬁnal answer as VLMς(A, C|I0, Q),
103"
TERMINOLOGY,0.1064773735581189,"where ς refers to the reasoning chain with evidence,
104"
TERMINOLOGY,0.10736468500443656,"ς = (step1, step2, ...)
stepi = (fi, ci),
fi ∈M
(1)"
TERMINOLOGY,0.10825199645075421,"where C = (ci, c2, ..., c|C|) refers to the free-form textual descriptions incorporating manipulation
105"
TERMINOLOGY,0.10913930789707187,"names fi and corresponding results from utilizing fi. This deﬁnition explicitly declares the symbolic
106"
TERMINOLOGY,0.11002661934338953,"execution process, while also being compatible with linguistic reasoning steps. Based on this
107"
TERMINOLOGY,0.11091393078970718,"deﬁnition, we can clearly construct standard CoM samples that incorporating the manipulation
108"
TERMINOLOGY,0.11180124223602485,"executions and linguistic steps with evidence. After the data construction, we can utilize a simple
109"
TERMINOLOGY,0.1126885536823425,"method to convert the standard CoM samples to the compatible VQA samples.
110"
DATA COLLECTION,0.11357586512866016,"3
Data Collection
111"
DATA COLLECTION,0.11446317657497782,Linguistic Annotators (LLMs)
DATA COLLECTION,0.11535048802129548,"Generate solving steps for a given question, 
by optionally using chain of manipulations 
from      or newly created ones."
DATA COLLECTION,0.11623779946761313,"Using Grounding(the pillar in front of the man 
at         ) to ﬁnd .., the resultant region is         ."
DATA COLLECTION,0.11712511091393078,"Using CropZoomIn(         , 4) to crop it on        , 
and zoom in it by 4 times to get new image     ."
DATA COLLECTION,0.11801242236024845,"Using Grounding( the man in black shirt ) to 
locate .., the position is at         ."
DATA COLLECTION,0.1188997338065661,"Using OCR(     )  recognize the text written on 
the image     , the result is  “QUICK DEPOSIT”."
DATA COLLECTION,0.11978704525288376,"bbx1
bbx1 bbx2"
DATA COLLECTION,0.12067435669920142,bbx3/I1
DATA COLLECTION,0.12156166814551908,"Using Grounding(the text written on the pillar 
in         ) to ﬁnd .., the resultant region is         . Q"
DATA COLLECTION,0.12244897959183673,>ACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvDOmoBbRCUnOnHvOnbn3ekngp8pxXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxPJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3skScmnSsT7NPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGhDiNexSXhLlxTvtsG09qate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe</latexit>A A
DATA COLLECTION,0.1233362910381544,"B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIRBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQpGLF39B3QrpOzEe1ztS4OZ0S0CvJaeOQPDHpJGF9m3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X9uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q M A0 A1 I0 I1
I1 I1 bbx1"
DATA COLLECTION,0.12422360248447205,"bbx1
bbx2"
DATA COLLECTION,0.1251109139307897,"7gBdGlY8fM4z5rC48dhlGzB45Hms5wxOZb92wKOaBfyHGIeuM7IHP+9y1BUENx7nrHnTzBdOwStVy0dJNo3RUMctl3TJMtb6CAtJVD/IvuEIPAVwkGIHBh6DYg42YnjYsmAgJ62BCWEQRV3mGe+RImxCLEcMmdEjfAe3aKerTXnrGSu3SKR69ESl17JEmIF5EsTxNV/lEOUv0N+J8pR3G9PfSb1GhApcE/qXbsr8r07WItBHVdXAqaZQIbI6N3VJVFfkzfVvVQlyCAmTcY/yEcWuUk7rCtNrGqXvbV/k0xJSr3bspN8C5vSQOeGeds0CwaVsWwzkuF2nE6ix2sIt9muchajhFHQ3y5njE561My3UbrXxJ1XLpJpt/FjawerAJE0</latexit>bbx3
bbx2"
DATA COLLECTION,0.12599822537710736,"bbx3
bbx3"
DATA COLLECTION,0.12688553682342502,"What is written on the pillar 
in front of man in black top?
: QUICK DEPOSIT txt1"
DATA COLLECTION,0.12777284826974267,"Linguistic Annotation
VQA Sample
Visual Annotation"
DATA COLLECTION,0.12866015971606035,Visual Annotators (VFMs)
DATA COLLECTION,0.129547471162378,Grounding(tgt1) ! bbx1
DATA COLLECTION,0.13043478260869565,"t5LRSYDVg9EJOSB76Us4jGrK64idpBI5rX9iO37pxs6vn/OZMpFvKu6CTtqe2HMj3ngKaKapaWGYh3S9TalyOIWj8PLBRWqZnWxIXl4ojwpxYXt+51mtVkqOxXHLPsncHNQRr62RekJDbQgECBDGwxFOEIHlJ6DuHCQULcEXrESULcxBkuUSRtRlmMjxiT+kb0u4wZ2Pa8/UqAM6JaJXktLGPGkE5UnC+jTbxDPjrNnfvHvGU9+tS38/92oTq3BC7F+6j8z/6nQtCsdYMzVwqikxjK4uyF0y0xV9c/tTVYocEuI0blFcEg6M8qPtGkpnbdW8/EX0ymZvU+yHMzvOpb0oDd7+P8CfaqFXel4u4sl2vr+aiHMYs5LNA8V1HDFrZRJ+8r3OMBj9aZdW3dWLfvqVYh18zgy7Lu3gBH+6E8</latexit>Grounding(tgt2) ! bbx2
Grounding(tgt3) ! bbx3
OCR(img1) ! txt1"
DATA COLLECTION,0.1313220940550133,DFS WRƉQGSRVLWLYHFKDLQV :
DATA COLLECTION,0.13220940550133098,Compatible VQA
DATA COLLECTION,0.13309671694764863,"[4,5,7,12]
[28,6,30,9]"
DATA COLLECTION,0.13398402839396628,"[8,0,17,13]
…"
DATA COLLECTION,0.13487133984028393,"[9,7,11,9]"
DATA COLLECTION,0.13575865128660158,QUICK DEPOSIT
DATA COLLECTION,0.13664596273291926,"ljanpmdi4/v7C4tLyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4cPgQg4i0fJZL3C7LmeSqNalz2SfM294Ompb7ULRKl6mZPAzkAR2aqEhRdcoMQHCl8CASQhD0wJPQ0YcNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv6NujXTNjA9orz0SrOZ3i0RuT0sQ2aULKiwmr0wdT7WzYn/zHmpPdbcB/Z3MydWok/sX7px5n91qhaJLg51DS7VFGlGVczl1R3Rd3c/FKVJIeIOIU7FI8Jc60c9nUmkTXrnrLdPxNZypW7XmWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH2WjzmMTW9iheR6gjBNUCXvaziCc/GuXFr3Bn3n6lGLtNs4NsyHj4AI+UiQ=</latexit>I0
Q
A0
(   ,     ,      )"
DATA COLLECTION,0.13753327417923691,"(   ,     ,      )
A1
I1
¯Q"
DATA COLLECTION,0.13842058562555457,"Figure 4: A cascading data generation pipeline that automatically produces standard CoM samples.
Given an original VQA sample, the linguistic annotator (LLMs) taught with usage of manipulations
(prompt) is ﬁrst asked to provide solving steps for the question Q, and the visual foundational models
(VFMs) are then engaged to replace the manipulations results, followed by a ﬁnal traversal on the
tree branched by the possible manipulation results to ﬁnd positive paths terminating to the answer A."
DATA COLLECTION,0.13930789707187222,"In this section, we ﬁrst introduces the automated data generation pipeline (illustrated in Figure 4),
112"
DATA COLLECTION,0.1401952085181899,"that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce
113"
DATA COLLECTION,0.14108251996450755,"error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of
114"
DATA COLLECTION,0.1419698314108252,"high-quality CoM samples for the challenging graphical mathematical problems.
115"
AUTOMATED DATA GENERATION,0.14285714285714285,"3.1
Automated Data Generation
116"
AUTOMATED DATA GENERATION,0.14374445430346053,"Given a general corpus D = {(I, Q, A)} consisting of triplet samples of images with corresponding
117"
AUTOMATED DATA GENERATION,0.14463176574977818,"visual question-answer pairs, our automated data generation pipeline consists of a linguistic annotator
118"
AUTOMATED DATA GENERATION,0.14551907719609583,"and several visual annotators according to the manipulations. For a question Q in each sample, we
119"
AUTOMATED DATA GENERATION,0.14640638864241348,"ﬁrst engage the linguistic annotator to generate manipulations-assisted solving steps with the CoM
120"
AUTOMATED DATA GENERATION,0.14729370008873113,"format (fi, ci), where the corresponding results of the instantiated manipulation executions are set
121"
AUTOMATED DATA GENERATION,0.1481810115350488,"with variables as placeholders. In this paper, we adopt GPT-4 (OpenAI, 2023a), a large language
122"
AUTOMATED DATA GENERATION,0.14906832298136646,"model with reliable language understanding and generation abilities as the linguistic annotator. We
123"
AUTOMATED DATA GENERATION,0.14995563442768411,"design a comprehensive prompt including the task requirements, usage of manipulations, and output
124"
AUTOMATED DATA GENERATION,0.15084294587400177,"data format, and further manually annotate 5 demonstrations for a stable generation. The detailed
125"
AUTOMATED DATA GENERATION,0.15173025732031944,"implementations are available at Appendix C.4.
126"
AUTOMATED DATA GENERATION,0.1526175687666371,"We then employ essential visual annotators to supply the results of manipulations requested in the
127"
AUTOMATED DATA GENERATION,0.15350488021295475,"solving steps by exactly performing the corresponding manipulations. By empirically analyzing
128"
AUTOMATED DATA GENERATION,0.1543921916592724,"the manipulations from both predeﬁned set and newly created ones (refers to Appendix C.3 for
129"
AUTOMATED DATA GENERATION,0.15527950310559005,"a detailed statistics), we reveal the Grounding and OCR are two fundamental manipulations, and
130"
AUTOMATED DATA GENERATION,0.15616681455190773,"most of the others can be consequently derived (e.g., CropZoomIn along a region of box, Counting
131"
AUTOMATED DATA GENERATION,0.15705412599822538,"upon recognized boxes, and Calculate for the recognized formula). Therefore, we employ two
132"
AUTOMATED DATA GENERATION,0.15794143744454303,"visual foundational models, GroundingDINO (Liu et al., 2023c) and PaddleOCR (Du et al., 2020),
133"
AUTOMATED DATA GENERATION,0.15882874889086068,"and develop the implementations of these manipulations1. The execution of the manipulations will
134"
AUTOMATED DATA GENERATION,0.15971606033717836,"transform the sequential reasoning steps into a tree T , as the input of current manipulation f1(xa)
135"
AUTOMATED DATA GENERATION,0.160603371783496,"may rely on one of the multiple results of previous manipulation f2 →(xb, xc), i.e., xa rely on xb
136"
AUTOMATED DATA GENERATION,0.16149068322981366,"(e.g., step 2 for ﬁnding pillars in Figure 5). We then perform a traversal on each produced tree with
137"
AUTOMATED DATA GENERATION,0.16237799467613132,"Depth First Search (DFS) to ﬁnd all positive paths {Pi|Pi ∈T , i = 1, 2, ...} that can terminate with
138"
AUTOMATED DATA GENERATION,0.16326530612244897,"the ﬁnal answer A from the result of the last manipulation. Based on this method, the generated
139"
AUTOMATED DATA GENERATION,0.16415261756876665,"CoM samples with positive paths are guaranteed to be error-free. We implement this pipeline on 3
140"
AUTOMATED DATA GENERATION,0.1650399290150843,"existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019),
141"
AUTOMATED DATA GENERATION,0.16592724046140195,"ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples 2. The
142"
AUTOMATED DATA GENERATION,0.1668145519077196,"designed prompt, a generated example with linguistic and visual results, and detailed algorithm
143"
AUTOMATED DATA GENERATION,0.16770186335403728,"illustration are available at AppendixC.1.
144"
HUMAN ANNOTATION,0.16858917480035493,"3.2
Human Annotation
145"
HUMAN ANNOTATION,0.16947648624667258,"The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines
146"
HUMAN ANNOTATION,0.17036379769299023,"in linguistic reasoning process helps LLMs to solve complex geometry problems. Beneﬁting from the
147"
HUMAN ANNOTATION,0.17125110913930788,"expressive capability of CoM structure, we have also manually annotated high-quality CoM samples
148"
HUMAN ANNOTATION,0.17213842058562556,"for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario.
149"
HUMAN ANNOTATION,0.1730257320319432,"Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and
150"
HUMAN ANNOTATION,0.17391304347826086,"visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of
151"
HUMAN ANNOTATION,0.17480035492457852,"manipulations, as well as the results of manipulations on images. We perform this annotation on the
152"
HUMAN ANNOTATION,0.1756876663708962,"MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart
153"
HUMAN ANNOTATION,0.17657497781721385,"math problems, resulting in the collection of 6K high-quality CoM math samples.
154"
HUMAN ANNOTATION,0.1774622892635315,"Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM
155"
HUMAN ANNOTATION,0.17834960070984915,"sample including n images from manipulations outputs (I0, Q, C0, I1, C1, ..., In, A), we convert it
156"
HUMAN ANNOTATION,0.17923691215616683,"into a multi-turn VQA sample segmented by the images [(I0, Q, C0), (I1, ¯Q, C1), ..., (In, ¯Q, A)],
157"
HUMAN ANNOTATION,0.18012422360248448,"where Ci represents the intermediate steps between Ii and Ii+1, and ¯Q is a simple prompt asking
158"
HUMAN ANNOTATION,0.18101153504880213,"model to answer question based on history. This transformation converts CoM samples into multi-turn
159"
HUMAN ANNOTATION,0.18189884649511978,"VQA samples that are compatible with existing VLMs training data. The detailed statistics of the
160"
HUMAN ANNOTATION,0.18278615794143743,"data generation are available at Appendix C.3.
161"
MODEL TRAINING,0.1836734693877551,"4
Model Training
162"
ARCHITECTURE,0.18456078083407276,"4.1
Architecture
163"
ARCHITECTURE,0.18544809228039041,"We use the same model architecture as CogVLM (Wang et al., 2023b), a general VLM approach
164"
ARCHITECTURE,0.18633540372670807,"that involves four fundamental components: (1) a Visual Encoder, (2) an MLP Adapter, (3) an LLM
165"
ARCHITECTURE,0.18722271517302574,"Backbone, and (4) a Visual Expert Module, for a reliable multimodal understanding. Concretely,
166"
ARCHITECTURE,0.1881100266193434,"the pre-trained EVA2-CLIP-E (Sun et al., 2023a) with 4B parameters and Vicuna-7B-v1.5 (Chiang
167"
ARCHITECTURE,0.18899733806566105,"et al., 2023) are adopted as the visual encoder and LLM backbone, respectively. A two-layer MLP
168"
ARCHITECTURE,0.1898846495119787,"(SwiGLU (Shazeer, 2020)) is further engaged to map the output of the visual encoder into the
169"
ARCHITECTURE,0.19077196095829635,"linguistic space of the LLM backbone. The visual expert module adds the vision-speciﬁc weights
170"
ARCHITECTURE,0.19165927240461403,"into the attention layer and feed-forward layer of each block in the LLM backbone, resulting in a
171"
ARCHITECTURE,0.19254658385093168,"total of 6.5B additional parameters for the deep fusion of modalities.
172"
ARCHITECTURE,0.19343389529724933,"1We simply implement the CropZoomIn referring to human behaviors with a local code interpreter.
2The success rate of GPT-4 to achieve the positive paths is 0.3555."
ARCHITECTURE,0.19432120674356698,Large Language Model LLM W  W 
ARCHITECTURE,0.19520851818988466,"KV-memory
(Image_0, Prompt_0) ViT"
ARCHITECTURE,0.1960958296362023,"Projector
Prompt_1"
ARCHITECTURE,0.19698314108251996,Image_1
ARCHITECTURE,0.19787045252883761,Response_0
ARCHITECTURE,0.19875776397515527,Response_1
ARCHITECTURE,0.19964507542147295,Stage 1: Pre-training
ARCHITECTURE,0.2005323868677906,"- Foundational visual XQGHUVWDQGLQJ
- Grounded XQGHUVWDQGLQJ"
ARCHITECTURE,0.20141969831410825,Stage 2: Fine-tuning
ARCHITECTURE,0.2023070097604259,- Multimodal problems-solving
ARCHITECTURE,0.20319432120674358,"Chat, Captioning, Grounding, Reasoning"
ARCHITECTURE,0.20408163265306123,Data structure:
ARCHITECTURE,0.20496894409937888,"- {(image, caption)}
- {(image, prompt, response)}"
ARCHITECTURE,0.20585625554569653,Data structure
ARCHITECTURE,0.2067435669920142,"- {(image, prompt, response)}"
ARCHITECTURE,0.20763087843833186,"Figure 5: Left: A compatible VLM architecture capable of multi-turn multi-image understanding.
Right: An effective training process to develop a general VLM with versatile capabilities."
ARCHITECTURE,0.2085181898846495,"Based on this general architecture, we develop a memory-based multi-turn multi-image VLM
173"
ARCHITECTURE,0.20940550133096716,"approach. Speciﬁcally, for a multi-turn VQA sample [(It, Qt, At)|t = 1, 2, ...], where At refers to
174"
ARCHITECTURE,0.21029281277728482,"Ct in CoM, we keep the accumulated KV memories of each layer in the LLM backbone throughout
175"
ARCHITECTURE,0.2111801242236025,"these turns. And at each turn t in training and inference, we calculate the attention function att as:
176"
ARCHITECTURE,0.21206743566992015,"att(X) = softmax(QtK′T
t
√"
ARCHITECTURE,0.2129547471162378,"d
)V ′
t"
ARCHITECTURE,0.21384205856255545,"K′
t = trunc(concat(K0, K1, ..., Kt))"
ARCHITECTURE,0.21472937000887313,"V ′
t = trunc(concat(V 0, V 1, ..., V t)) (2) 177"
ARCHITECTURE,0.21561668145519078,"where Qt ∈Rs×d is query representation of current layer, and the K′
t, V ′
t ∈R(s×t)×d refer to the
178"
ARCHITECTURE,0.21650399290150843,"concatenation of accumulated representations and will be further truncated if the sequence length
179"
ARCHITECTURE,0.21739130434782608,"s × t is greater than a predeﬁned threshold. At t > 0, the new image It will be cropped from It−1
180"
ARCHITECTURE,0.21827861579414373,"and ampliﬁed with the Bicubic Interpolation (Keys, 1981).
181"
TRAINING,0.2191659272404614,"4.2
Training
182"
TRAINING,0.22005323868677906,"The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of
183"
TRAINING,0.2209405501330967,"general multimodal task-solving as well as the visual reasoning.
184"
TRAINING,0.22182786157941436,"First Stage Pre-Training
This stage consists of two ordinal sub-phases of training for foundational
185"
TRAINING,0.22271517302573204,"visual understanding and grounded generation. Following the pre-training of CogVLM (Wang et al.,
186"
TRAINING,0.2236024844720497,"2023b), we ﬁrst train model on 1.5B image-text pairs cleaned from the LAION-2B (Schuhmann et al.,
187"
TRAINING,0.22448979591836735,"2022) and COYO-700M (Byeon et al., 2022) with 120,000 iterations and batch size of 8,192. We
188"
TRAINING,0.225377107364685,"then train model on 40M grounded image-question-answer triples cleaned from LAION-115M (Li
189"
TRAINING,0.22626441881100265,"et al., 2023b) with 60,000 iterations and batch size of 1,024, where each noun phrase in the answer is
190"
TRAINING,0.22715173025732033,"followed by a list of coordinates [[x0, y0, x1, y1], ...]3 referring the phrase to the grounded objects in
191"
TRAINING,0.22803904170363798,"the image. Both phases adopt the next token prediction objective, and train the 6.5B parameters of
192"
TRAINING,0.22892635314995563,"visual experts.
193"
TRAINING,0.22981366459627328,"Second Stage Alignment
This stage further trains the model to align with human preferences on
194"
TRAINING,0.23070097604259096,"solving practical visual problems. We fuse the produced CoM data with 3 types of corpus, including
195"
TRAINING,0.2315882874889086,"MultiInstruct (Xu et al., 2022), LLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen et al., 2023c),
196"
TRAINING,0.23247559893522626,"referring the abilities of instruction-following, texts-recognizing, and detailed-captioning. This fusion
197"
TRAINING,0.23336291038154391,"results in a total of 570K (I, Q, A) samples, where the answer A in CoM data consists of multiple
198"
TRAINING,0.23425022182786157,"turns. For the training data of CoM, we randomly prepend a lunching prompt4 P M to questions
199"
TRAINING,0.23513753327417924,"Q = P M + Q asking models to optionally use manipulations for the adaption of explicitly eliciting.
200"
TRAINING,0.2360248447204969,"We empirically show that the model can effectively learn the evidential visual reasoning by ingesting
201"
TRAINING,0.23691215616681455,"this portion of CoM data. We train model with 14,000 iterations and a batch size of 160, where the
202"
TRAINING,0.2377994676131322,"learning rate reaches 10−5 after 280 steps of warm-up and then decays linearly. The parameters
203"
TRAINING,0.23868677905944988,"of 6.5B visual experts are trained with the objective of next token prediction. These two stages of
204"
TRAINING,0.23957409050576753,"training result in our standard version of CogCoM involving both chat and reasoning capabilities.
205"
TRAINING,0.24046140195208518,"More training details are available at Appendix D.2.
206"
TRAINING,0.24134871339840283,"3xi, yi ∈[000, 999] refer to the normalized pixel coordinates.
4See Appendix D.1 for examples."
EXPERIMENT,0.2422360248447205,"5
Experiment
207"
EXPERIMENT,0.24312333629103816,"To quantitatively validate the suitability and efﬁciency of the proposed method, we conduct exper-
208"
EXPERIMENT,0.2440106477373558,"iments on 9 benchmarks corresponding to 4 categories of multimodal capabilities, as well as on a
209"
EXPERIMENT,0.24489795918367346,"newly constructed testbed that includes the evidential reasoning paths with a keypoints-aware metric.
210"
EXPERIMENT,0.24578527062999111,"Following previous works, we train two generalist versions of CogCoM for adapting to the different
211"
EXPERIMENT,0.2466725820763088,"scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version
212"
EXPERIMENT,0.24755989352262645,"with a qualitative analysis (Hwang et al., 2023). We also evaluate the time complexity.
213"
EXPERIMENT,0.2484472049689441,"• Detailed Visual Question Answering. This task involves models to perform detailed
214"
EXPERIMENT,0.24933451641526175,"reasoning or recognition on images. We use 4 prominent benchmarks including, GQA (Hud-
215"
EXPERIMENT,0.2502218278615794,"son & Manning, 2019), TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and
216"
EXPERIMENT,0.2511091393078971,"TallyVQA (Acharya et al., 2019).
217"
EXPERIMENT,0.25199645075421473,"• Visual Grounding. Visual grounding evaluates the crucial abilities of VLMs on meticulous
218"
EXPERIMENT,0.2528837622005324,"position understanding. We evaluate our model on 3 standard benchmarks, RefCOCO (Yu
219"
EXPERIMENT,0.25377107364685003,"et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016).
220"
EXPERIMENT,0.2546583850931677,"• General Multimodal Capabilities & Hallucination. We also evaluate on a general mul-
221"
EXPERIMENT,0.25554569653948533,"timodal benchmark, MM-Vet (Yu et al., 2023b), and a hallucination detection benchmark
222"
EXPERIMENT,0.25643300798580304,"POPE (Li et al., 2023c), to investigate the helpfulness of visual reasoning.
223"
EXPERIMENTS ON DETAILED VQA,0.2573203194321207,"5.1
Experiments on Detailed VQA
224"
EXPERIMENTS ON DETAILED VQA,0.25820763087843834,"VLMs have demonstrated the well-known superiority in visual scenes with salient content understand-
225"
EXPERIMENTS ON DETAILED VQA,0.259094942324756,"ing. We evaluate the effectiveness of CogCoM on VQAs on detailed understanding, which typically
226"
EXPERIMENTS ON DETAILED VQA,0.25998225377107365,"require models to perform multiple actions (ﬁnd, read) or multiple reasoning steps (recognizing and
227"
EXPERIMENTS ON DETAILED VQA,0.2608695652173913,"then calculating). Following previous studies (Wang et al., 2023b), we train our model obtained
228"
EXPERIMENTS ON DETAILED VQA,0.26175687666370895,"from the ﬁrst-phase of stage-1 on a mixture of data, including an instruction corpus of MultiInstruct,
229"
EXPERIMENTS ON DETAILED VQA,0.2626441881100266,"13 publicly available VQA datasets (only using training set), a newly created VQA dataset built
230"
EXPERIMENTS ON DETAILED VQA,0.26353149955634425,"through promoting GPT-4V (OpenAI, 2023b) for image-oriented question-answer generation, and
231"
EXPERIMENTS ON DETAILED VQA,0.26441881100266196,"the automatically generated 70K CoM corpus. This training results in a generalist VQA model
232"
EXPERIMENTS ON DETAILED VQA,0.2653061224489796,"incorporating CoM reasoning. For all existing VQA tasks, we directly prompt CogCoM with given
233"
EXPERIMENTS ON DETAILED VQA,0.26619343389529726,"questions and examine the correctness of outputted answers.
234"
EXPERIMENTS ON DETAILED VQA,0.2670807453416149,"Type
Model
GQA
TallyVQA
TextVQA
ST-VQA
test-balanced
simple
complex
test
test"
EXPERIMENTS ON DETAILED VQA,0.26796805678793256,Generalist
EXPERIMENTS ON DETAILED VQA,0.2688553682342502,"Flamingo (Alayrac et al., 2022)
-
-
-
54.1
-
GIT (Wang et al., 2022a)
-
-
-
59.8
-
GI2 (Wang et al., 2022a)
-
-
-
67.3
-
BLIP-2 (Li et al., 2023b)
44.7†
-
-
-
21.7
InstructBLIP (Dai et al., 2023)
49.5†
-
-
-
50.7†"
EXPERIMENTS ON DETAILED VQA,0.26974267968056787,"Qwen-VL (Bai et al., 2023)
59.3
-
-
63.8
-
CogVLM (Wang et al., 2023b)
65.2
79.8
68.0
69.7
61.0
CogCoM
71.7
84.0
70.1
71.1
70.0"
EXPERIMENTS ON DETAILED VQA,0.2706299911268855,Specialist SOTAs
EXPERIMENTS ON DETAILED VQA,0.27151730257320317,"72.1
(CFR)"
EXPERIMENTS ON DETAILED VQA,0.2724046140195209,"86.0
( PaLI-X)"
EXPERIMENTS ON DETAILED VQA,0.2732919254658385,"75.6
(PaLI-X)"
EXPERIMENTS ON DETAILED VQA,0.2741792369121562,"71.4
(PaLI-X)"
EXPERIMENTS ON DETAILED VQA,0.27506654835847383,"86.0
(SMoLA)"
EXPERIMENTS ON DETAILED VQA,0.2759538598047915,"Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with
† refer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the
effectiveness on the visual reasoning and scene texts recognition benchmarks."
EXPERIMENTS ON DETAILED VQA,0.27684117125110913,"5.1.1
GQA, TextVQA, ST-VQA, TallyVQA
235"
EXPERIMENTS ON DETAILED VQA,0.2777284826974268,"Settings
GQA is a compositional VQA benchmark with diverse reasoning questions coming from
236"
EXPERIMENTS ON DETAILED VQA,0.27861579414374443,"semantic functional programs. TallyVQA is an objects counting benchmark with human-annotated
237"
EXPERIMENTS ON DETAILED VQA,0.2795031055900621,"complex counting questions involving challenging non-zero counterparts. TextVQA and ST-VQA are
238"
EXPERIMENTS ON DETAILED VQA,0.2803904170363798,"two texts understanding benchmarks requiring models to answer questions through textual cues on
239"
EXPERIMENTS ON DETAILED VQA,0.28127772848269744,"images. We use the ofﬁcial evaluation scripts for GQA and TallyVQA, which calculate the accuracy
240"
EXPERIMENTS ON DETAILED VQA,0.2821650399290151,"score by the Exact Matching (EM) between model predictions and answers. For TextVQA and
241"
EXPERIMENTS ON DETAILED VQA,0.28305235137533274,"ST-VQA, we submit our model predictions to the ofﬁcial online websites for calculating the accuracy
242"
EXPERIMENTS ON DETAILED VQA,0.2839396628216504,"with VQA Score metric (Antol et al., 2015).
243"
EXPERIMENTS ON DETAILED VQA,0.28482697426796805,"Results
As the results shown in Table 2, CogCoM achieves the state-of-the-art performance in
244"
EXPERIMENTS ON DETAILED VQA,0.2857142857142857,"comparison with all generalist models, and achieves signiﬁcant improvements over the baseline model.
245"
EXPERIMENTS ON DETAILED VQA,0.28660159716060335,"Speciﬁcally, compared to the baseline model, our model achieves up to 5.97 and 9.0 percentage
246"
EXPERIMENTS ON DETAILED VQA,0.28748890860692106,"points improvement on the benchmarks that requires complex reasoning and detailed recognition,
247"
EXPERIMENTS ON DETAILED VQA,0.2883762200532387,"respectively. On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale
248"
EXPERIMENTS ON DETAILED VQA,0.28926353149955636,"specialist SOTAs. This result demonstrates the effectiveness of the proposed approach in solving
249"
EXPERIMENTS ON DETAILED VQA,0.290150842945874,"details recognition problem.
250"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.29103815439219166,"5.1.2
Experiments for Reasoning Accuracy and Time Complexity
251"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2919254658385093,"Due to the lack of resource, we build CoM-test, a benchmark with evidential reasoning chains on the
252"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.29281277728482696,"TextVQA test set based on the proposed data generation pipeline, and also introduce a keypoints-
253"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2937000887311446,"aware metric to validate the correctness of reasoning paths (see Appendix C.3 for detailed statistics).
254"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.29458740017746227,"We also evaluate the time complexity for model generation on a held-out benchmark, MM-Vet.
255"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.29547471162378,"0
2k
4k
6k
8k
Training Steps 2.15 7.27 20.00 30.00 40.00 55.59 60.00"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2963620230700976,Accuracy Score
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2972493345164153,"Zero
shot / answer"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2981366459627329,CogCoM / answer
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.2990239574090506,CogCoM / explanation
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.29991126885536823,"1
2
3
4
5
6
7
8
0 50 100 150 200 250 300 350"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3007985803016859,#Tokens 141.7 157.4 190.9 234.6 262.9 275.4 292.5 346.1
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.30168589174800353,"18.0
24.1
22.8"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3025732031943212,"42.1
33.0 48.9 71.4 189.8"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3034605146406389,"CogCoM/Tokens
CogVLM/Tokens 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.30434782608695654,#Time (seconds)
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3052351375332742,"CogCoM/Time
CogVLM/Time"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.30612244897959184,"Figure 6: Left: Results on a reasoning testbed CoM-test shows CogCoM achieves satisfactory
performance with only 70K training data and 2K steps. Right: Results on MM-Vet shows that
CogCoM produces comprehensive reasoning content without incurring excessive time overhead."
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3070097604259095,"Reasoning Accuracy
To validate the correctness of execution and results of manipulations in
256"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.30789707187222715,"reasoning paths, we introduce a keypoints-aware evaluation metric that concentrates on these contents
257"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3087843833185448,"and their order. Concretely, given a predicted chain-answer pair (C′, A′) and the ground truth
258"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.30967169476486245,"pair (C, A), we ﬁrst extract the keypoints (i.e., the name, parameters, and results of manipulations)
259"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3105590062111801,"in A′, A to form two lists, and then discretize these two lists into K′ and K based on a bag-
260"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3114463176574978,"of-words composed of all keypoints. Then, we calculate the normalized Levenshtein Distance
261"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.31233362910381546,"sK = Levenshtein(K′, K)/N as the manipulation score. We also compute the BLEU (Papineni
262"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3132209405501331,"et al., 2002) score sC = BLEU(C′, C) as the paragraph score. Finally, a weighted average of these
263"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.31410825199645076,"two scores serves as the ultimate reasoning score s acc = (0.6 × sK + 0.4 × sC)/2.
264"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3149955634427684,"We train our ﬁrst-stage model only using the 70K automated CoM data without other supervision
265"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.31588287488908606,"for qualitatively evaluate the effectiveness of chains, and the results are shown in the left subplot
266"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3167701863354037,"of Figure 6. We ﬁnd that by training with the CoM chains, our model can swiftly achieve the
267"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.31765749778172137,"satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result
268"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.318544809228039,"of 55.59 with 8K steps. Additionally, the explanation scores gradually improve along with the model
269"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3194321206743567,"performance, indicating that successful reasoning steps contribute to the achieving of ﬁnal answer.
270"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3203194321206744,"Time Complexity
We also evaluate the time complexity and average length of tokens during model
271"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.321206743566992,"reasoning on a held-out test set, MM-Vet. Speciﬁcally, we run CogCoM and the baseline model on
272"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3220940550133097,"all 218 questions, and record the time overhead as well as the average number of outputted tokens
273"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.32298136645962733,"(using the Vicuna-7B-v1.5 tokenizer). We divide the 218 samples into 8 intervals based on the time
274"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.323868677905945,"expenditure for each sample and calculate the average values of the time complexity and the number
275"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.32475598935226263,"of tokens for each interval, with the results presented in the right subplot of Figure 6.
276"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3256433007985803,"From the results we ﬁnd that compared to baseline model, CogCoM produces information-intensive
277"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.32653061224489793,"reasoning content (e.g., detection boxes, auxiliary lines) without incurring infeasible time overhead.
278"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.32741792369121564,"For example, without quantitive optimization, CogCoM outputs 262.9 informative tokens in approxi-
279"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.3283052351375333,"mately 9 seconds. With the advantages in long-context optimization techniques (Hooper et al., 2024),
280"
EXPERIMENTS FOR REASONING ACCURACY AND TIME COMPLEXITY,0.32919254658385094,"we believe that it is crucial for models to produce informative content and accurate responses.
281"
EXPERIMENTS ON VISUAL GROUNDING,0.3300798580301686,"5.2
Experiments on Visual Grounding
282"
EXPERIMENTS ON VISUAL GROUNDING,0.33096716947648624,"The task of visual grounding requires models to precisely provide the corresponding coordinates
283"
EXPERIMENTS ON VISUAL GROUNDING,0.3318544809228039,"of regions in an image based on the given target description. Following the existing work (Wang
284"
EXPERIMENTS ON VISUAL GROUNDING,0.33274179236912155,"et al., 2023b), we train our model obtained by the ﬁrst stage on a mixture of datasets, including an
285"
EXPERIMENTS ON VISUAL GROUNDING,0.3336291038154392,"instruction corpus MultiInstruct, a high-quality grounded VQA corpus introduced in CogVLM, and
286"
EXPERIMENTS ON VISUAL GROUNDING,0.33451641526175685,"the 70K CoM data. This training results in a generalist grounding model that is excelling at visual
287"
EXPERIMENTS ON VISUAL GROUNDING,0.33540372670807456,"grounding while capable of reasoning. For all benchmarks, we prompt CogOM in a chat manner to
288"
EXPERIMENTS ON VISUAL GROUNDING,0.3362910381543922,"ask the model to provide grounded coordinates, such as “Where is ⟨expr⟩answer in [x0,y0,x1,y1]
289"
EXPERIMENTS ON VISUAL GROUNDING,0.33717834960070986,"format."", where the ⟨expr⟩refers to the target expression. We use the standard metric, that considers
290"
EXPERIMENTS ON VISUAL GROUNDING,0.3380656610470275,"a prediction as correct when the intersection-over-union (IoU) between boxes is greater than 0.5.
291"
EXPERIMENTS ON VISUAL GROUNDING,0.33895297249334516,"Type
Model
RefCOCO
RefCOCO+
RefCOCOg"
EXPERIMENTS ON VISUAL GROUNDING,0.3398402839396628,"val
test-A
test-B
val
test-A
test-B
val
test"
EXPERIMENTS ON VISUAL GROUNDING,0.34072759538598046,Generalist
EXPERIMENTS ON VISUAL GROUNDING,0.3416149068322981,"OFA-L* (Wang et al., 2022b)
79.96
83.67
76.39
68.29
76.00
61.75
67.57
67.58
Shikra-7B (Chen et al., 2023b)
87.01
90.61
80.24
81.60
87.36
72.12
82.27
82.19
Shikra-13B (Chen et al., 2023b)
87.83
91.11
81.81
82.89
87.79
74.41
82.64
83.16
Qwen-VL (Bai et al., 2023)
89.36
92.26
85.34
83.12
88.25
77.21
85.58
85.48
CogVLM (Wang et al., 2023b)
92.51
93.95
88.73
87.52
91.81
81.43
89.46
90.09
CogCoM
92.34
94.57
89.15
88.19
92.80
82.08
89.32
90.45"
EXPERIMENTS ON VISUAL GROUNDING,0.34250221827861577,Specialist SOTAs 92.64
EXPERIMENTS ON VISUAL GROUNDING,0.3433895297249335,(UNINEXT) 94.33
EXPERIMENTS ON VISUAL GROUNDING,0.3442768411712511,(UNINEXT) 91.46
EXPERIMENTS ON VISUAL GROUNDING,0.3451641526175688,(UNINEXT) 88.77
EXPERIMENTS ON VISUAL GROUNDING,0.3460514640638864,(ONE-PEACE) 92.21
EXPERIMENTS ON VISUAL GROUNDING,0.3469387755102041,(ONE-PEACE) 83.23
EXPERIMENTS ON VISUAL GROUNDING,0.34782608695652173,(ONE-PEACE) 89.22
EXPERIMENTS ON VISUAL GROUNDING,0.3487133984028394,(ONE-PEACE) 89.37
EXPERIMENTS ON VISUAL GROUNDING,0.34960070984915703,"(UNINEXT-H)
Table 2: Results on VG benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023)."
EXPERIMENTS ON VISUAL GROUNDING,0.35048802129547474,"Results
As shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets.
292"
EXPERIMENTS ON VISUAL GROUNDING,0.3513753327417924,"Based on the training with a mixture of broad capabilities, this result indicates that our model exhibits
293"
EXPERIMENTS ON VISUAL GROUNDING,0.35226264418811004,"a superior grounding abilities while offers potential to solve a variety of tasks.
294"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.3531499556344277,"5.3
Experiments on General Multimodal Evaluation and Hallucination Examination
295"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.35403726708074534,"We further examine the general multimodal capabilities, and the hallucination issue. We use the
296"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.354924578527063,"generalist VQA model and obtain model predictions by directly asking the original questions in
297"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.35581188997338065,"benchmarks. We use the challenging adversarial version and ofﬁcial evaluation scripts for POPE.
298"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.3566992014196983,"Method
LLM
MM-Vet
POPEadv"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.35758651286601595,"InstructBLIP (Dai et al., 2023)
Vicuna-13B
25.6
77.3
LLaVA (Liu et al., 2023b)
LLaMA2-7B
28.1
66.3
DreamLLM (Dong et al., 2023)
Vicuna-7B
35.9
76.5
LLaVA-1.5 (Liu et al., 2023a)
Vicuna-13B
36.3
84.5
CogVLM (Wang et al., 2023b)
Vicuna-7B
45.5†
87.2
CogCoM
Vicuna-7B
46.1
87.8
Table 3: Evaluation results on the general and hallucination assessment benchmarks."
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.35847382431233366,"Results
As shown in Table 3, we can see that CogCoM improves the performance by 0.6 points
299"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.3593611357586513,"compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which
300"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.36024844720496896,"is in consistent with the baseline model. This result suggests that out model maintains superior
301"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.3611357586512866,"reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously
302"
EXPERIMENTS ON GENERAL MULTIMODAL EVALUATION AND HALLUCINATION EXAMINATION,0.36202307009760426,"exhibits lower hallucination.
303"
CONCLUSION,0.3629103815439219,"6
Conclusion
304"
CONCLUSION,0.36379769299023956,"This paper studies the problems presented by the conclusive alignment training of VLMs, and
305"
CONCLUSION,0.3646850044365572,"proposes a mechanism, Chain of Manipulations (CoM), that enables VLMs to solve problems step-
306"
CONCLUSION,0.36557231588287487,"by-step by actively manipulating visual inputs as evidence. We realize this methodology by proposing
307"
CONCLUSION,0.36645962732919257,"(1) a ﬂexible data structure, (2) an efﬁcient data generation framework capable of producing abundant
308"
CONCLUSION,0.3673469387755102,"samples, (3) a memory-based architecture compatible with existing VLMs, and (4) a training process
309"
CONCLUSION,0.3682342502218279,"for versatile capabilities. We also annotate 6K graphical math samples with reasoning chains to
310"
CONCLUSION,0.3691215616681455,"facilitate the advancement of VLMs in solving mathematical problems. Experiments on 9 public
311"
CONCLUSION,0.3700088731144632,"benchmarks show that our trained 17B general VLM can produce informative reasoning content
312"
CONCLUSION,0.37089618456078083,"while achieving superior performance on diverse multimodal problems.
313"
REFERENCES,0.3717834960070985,"References
314"
REFERENCES,0.37267080745341613,"Acharya, M., Kaﬂe, K., and Kanan, C. Tallyqa: Answering complex counting questions. In
315"
REFERENCES,0.3735581188997338,"Proceedings of the AAAI conference on artiﬁcial intelligence, 2019.
316"
REFERENCES,0.3744454303460515,"Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican,
317"
REFERENCES,0.37533274179236914,"K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in
318"
REFERENCES,0.3762200532386868,"Neural Information Processing Systems, 2022.
319"
REFERENCES,0.37710736468500444,"Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual
320"
REFERENCES,0.3779946761313221,"question answering. In Proceedings of the IEEE international conference on computer vision,
321"
REFERENCES,0.37888198757763975,"2015.
322"
REFERENCES,0.3797692990239574,"Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre,
323"
REFERENCES,0.38065661047027505,"S., Sagawa, S., et al. Openﬂamingo: An open-source framework for training large autoregressive
324"
REFERENCES,0.3815439219165927,"vision-language models. arXiv preprint, 2023.
325"
REFERENCES,0.3824312333629104,"Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A
326"
REFERENCES,0.38331854480922806,"frontier large vision-language model with versatile abilities. arXiv preprint, 2023.
327"
REFERENCES,0.3842058562555457,"Biten, A. F., Tito, R., Maﬂa, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D.
328"
REFERENCES,0.38509316770186336,"Scene text visual question answering. In Proceedings of the IEEE/CVF international conference
329"
REFERENCES,0.385980479148181,"on computer vision, 2019.
330"
REFERENCES,0.38686779059449866,"Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-text pair dataset,
331"
REFERENCES,0.3877551020408163,"2022.
332"
REFERENCES,0.38864241348713396,"Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text
333"
REFERENCES,0.3895297249334516,"pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference
334"
REFERENCES,0.3904170363797693,"on Computer Vision and Pattern Recognition, 2021.
335"
REFERENCES,0.391304347826087,"Chen, D., Liu, J., Dai, W., and Wang, B. Visual instruction tuning with polite ﬂamingo. arXiv
336"
REFERENCES,0.3921916592724046,"preprint arXiv:2307.01003, 2023a.
337"
REFERENCES,0.3930789707187223,"Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal
338"
REFERENCES,0.3939662821650399,"llm’s referential dialogue magic. arXiv preprint, 2023b.
339"
REFERENCES,0.3948535936113576,"Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving
340"
REFERENCES,0.39574090505767523,"large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023c.
341"
REFERENCES,0.3966282165039929,"Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner,
342"
REFERENCES,0.39751552795031053,"A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. In The
343"
REFERENCES,0.39840283939662824,"Eleventh International Conference on Learning Representations, 2022.
344"
REFERENCES,0.3992901508429459,"Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C. R., Goodman, S.,
345"
REFERENCES,0.40017746228926354,"Wang, X., Tay, Y., et al. Pali-x: On scaling up a multilingual vision and language model. arXiv
346"
REFERENCES,0.4010647737355812,"preprint, 2023d.
347"
REFERENCES,0.40195208518189884,"Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y.,
348"
REFERENCES,0.4028393966282165,"Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
349"
REFERENCES,0.40372670807453415,"See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.
350"
REFERENCES,0.4046140195208518,"Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip:
351"
REFERENCES,0.40550133096716945,"Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv
352"
REFERENCES,0.40638864241348716,"preprint arXiv:2305.06500.
353"
REFERENCES,0.4072759538598048,"Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip:
354"
REFERENCES,0.40816326530612246,"Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv
355"
REFERENCES,0.4090505767524401,"preprint, 2023.
356"
REFERENCES,0.40993788819875776,"Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al.
357"
REFERENCES,0.4108251996450754,"Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499,
358"
REFERENCES,0.41171251109139306,"2023.
359"
REFERENCES,0.4125998225377107,"Du, Y., Li, C., Guo, R., Yin, X., Liu, W., Zhou, J., Bai, Y., Yu, Z., Yang, Y., Dang, Q., et al. Pp-ocr:
360"
REFERENCES,0.4134871339840284,"A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020.
361"
REFERENCES,0.4143744454303461,"Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al.
362"
REFERENCES,0.4152617568766637,"Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.
363"
REFERENCES,0.4161490683229814,"Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami,
364"
REFERENCES,0.417036379769299,"A. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv
365"
REFERENCES,0.4179236912156167,"preprint arXiv:2401.18079, 2024.
366"
REFERENCES,0.41881100266193433,"Huang, K.-H., Zhou, M., Chan, H. P., Fung, Y. R., Wang, Z., Zhang, L., Chang, S.-F., and Ji, H.
367"
REFERENCES,0.419698314108252,"Do lvlms understand charts? analyzing and correcting factual errors in chart captioning. arXiv
368"
REFERENCES,0.42058562555456963,"preprint arXiv:2312.10160, 2023a.
369"
REFERENCES,0.42147293700088734,"Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Liu,
370"
REFERENCES,0.422360248447205,"Q., et al. Language is not all you need: Aligning perception with language models. arXiv preprint,
371"
REFERENCES,0.42324755989352264,"2023b.
372"
REFERENCES,0.4241348713398403,"Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and
373"
REFERENCES,0.42502218278615794,"compositional question answering. In Proceedings of the IEEE/CVF conference on computer
374"
REFERENCES,0.4259094942324756,"vision and pattern recognition, 2019.
375"
REFERENCES,0.42679680567879325,"Hwang, A., Head, A., and Callison-Burch, C. Grounded intuition of gpt-vision’s abilities with
376"
REFERENCES,0.4276841171251109,"scientiﬁc images. arXiv preprint, 2023.
377"
REFERENCES,0.42857142857142855,"Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,
378"
REFERENCES,0.42945874001774625,"T. Scaling up visual and vision-language representation learning with noisy text supervision. In
379"
REFERENCES,0.4303460514640639,"International conference on machine learning, 2021.
380"
REFERENCES,0.43123336291038156,"Keys, R. Cubic convolution interpolation for digital image processing. IEEE transactions on
381"
REFERENCES,0.4321206743566992,"acoustics, speech, and signal processing, 1981.
382"
REFERENCES,0.43300798580301686,"Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J.,
383"
REFERENCES,0.4338952972493345,"Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense
384"
REFERENCES,0.43478260869565216,"image annotations. International journal of computer vision, 2017.
385"
REFERENCES,0.4356699201419698,"Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: A multi-modal model with
386"
REFERENCES,0.43655723158828746,"in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.
387"
REFERENCES,0.43744454303460517,"Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with
388"
REFERENCES,0.4383318544809228,"frozen image encoders and large language models. arXiv preprint, 2023b.
389"
REFERENCES,0.4392191659272405,"Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in
390"
REFERENCES,0.4401064773735581,"large vision-language models. arXiv preprint arXiv:2305.10355, 2023c.
391"
REFERENCES,0.4409937888198758,"Li, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen, C., Chen, L., and Wei, Y.
Sta-
392"
REFERENCES,0.4418811002661934,"blellava: Enhanced visual instruction tuning with synthesized image-dialogue data. arXiv preprint
393"
REFERENCES,0.4427684117125111,"arXiv:2308.10253, 2023d.
394"
REFERENCES,0.44365572315882873,"Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L.
395"
REFERENCES,0.4445430346051464,"Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European
396"
REFERENCES,0.4454303460514641,"Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 2014.
397"
REFERENCES,0.44631765749778174,"Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. arXiv preprint
398"
REFERENCES,0.4472049689440994,"arXiv:2310.03744, 2023a.
399"
REFERENCES,0.44809228039041704,"Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint, 2023b.
400"
REFERENCES,0.4489795918367347,"Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding
401"
REFERENCES,0.44986690328305234,"dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint
402"
REFERENCES,0.45075421472937,"arXiv:2303.05499, 2023c.
403"
REFERENCES,0.45164152617568765,"Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and
404"
REFERENCES,0.4525288376220053,"Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
405"
REFERENCES,0.453416149068323,"arXiv preprint arXiv:2310.02255, 2023.
406"
REFERENCES,0.45430346051464066,"Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and
407"
REFERENCES,0.4551907719609583,"comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on
408"
REFERENCES,0.45607808340727596,"computer vision and pattern recognition, 2016.
409"
REFERENCES,0.4569653948535936,"Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: A benchmark for question
410"
REFERENCES,0.45785270629991126,"answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022.
411"
REFERENCES,0.4587400177462289,"OpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2023a.
412"
REFERENCES,0.45962732919254656,"OpenAI, R. Gpt-4v(ision) system card. Citekey: gptvision., 2023b.
413"
REFERENCES,0.4605146406388642,"Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned
414"
REFERENCES,0.4614019520851819,"photographs. Advances in neural information processing systems, 2011.
415"
REFERENCES,0.4622892635314996,"Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine
416"
REFERENCES,0.4631765749778172,"translation. In Proceedings of the 40th annual meeting of the Association for Computational
417"
REFERENCES,0.4640638864241349,"Linguistics, 2002.
418"
REFERENCES,0.4649511978704525,"Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding
419"
REFERENCES,0.4658385093167702,"multimodal large language models to the world. arXiv preprint, 2023.
420"
REFERENCES,0.46672582076308783,"Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T.,
421"
REFERENCES,0.4676131322094055,"Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-ﬁltered 400 million image-text
422"
REFERENCES,0.46850044365572313,"pairs. arXiv preprint arXiv:2111.02114, 2021.
423"
REFERENCES,0.46938775510204084,"Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T.,
424"
REFERENCES,0.4702750665483585,"Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next
425"
REFERENCES,0.47116237799467614,"generation image-text models. Advances in Neural Information Processing Systems, 2022.
426"
REFERENCES,0.4720496894409938,"Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed,
427"
REFERENCES,0.47293700088731144,"image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting
428"
REFERENCES,0.4738243123336291,"of the Association for Computational Linguistics (Volume 1: Long Papers), 2018.
429"
REFERENCES,0.47471162377994675,"Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
430"
REFERENCES,0.4755989352262644,"Shrestha, R., Kaﬂe, K., and Kanan, C. Answer them all! toward universal visual question answering
431"
REFERENCES,0.4764862466725821,"models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
432"
REFERENCES,0.47737355811889975,"2019.
433"
REFERENCES,0.4782608695652174,"Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M.
434"
REFERENCES,0.47914818101153506,"Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision
435"
REFERENCES,0.4800354924578527,"and pattern recognition, 2019.
436"
REFERENCES,0.48092280390417036,"Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-clip: Improved training techniques for clip at
437"
REFERENCES,0.481810115350488,"scale. arXiv preprint arXiv:2303.15389, 2023a.
438"
REFERENCES,0.48269742679680566,"Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X.
439"
REFERENCES,0.4835847382431233,"Generative pretraining in multimodality. arXiv preprint, 2023b.
440"
REFERENCES,0.484472049689441,"Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human
441"
REFERENCES,0.48535936113575867,"demonstrations. Nature, 625(7995):476–482, 2024.
442"
REFERENCES,0.4862466725820763,"Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and Wang, L. Git: A generative
443"
REFERENCES,0.487133984028394,"image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022a.
444"
REFERENCES,0.4880212954747116,"Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. Ofa:
445"
REFERENCES,0.4889086069210293,"Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning
446"
REFERENCES,0.4897959183673469,"framework. 2022b.
447"
REFERENCES,0.4906832298136646,"Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al.
448"
REFERENCES,0.49157054125998223,"Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv
449"
REFERENCES,0.49245785270629994,"preprint, 2023a.
450"
REFERENCES,0.4933451641526176,"Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.
451"
REFERENCES,0.49423247559893524,"Cogvlm: Visual expert for pretrained language models. arXiv preprint, 2023b.
452"
REFERENCES,0.4951197870452529,"Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct:
453"
REFERENCES,0.49600709849157054,"Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,
454"
REFERENCES,0.4968944099378882,"2022c.
455"
REFERENCES,0.49778172138420584,"Wu, P. and Xie, S. V*: Guided visual search as a core mechanism in multimodal llms. arXiv preprint
456"
REFERENCES,0.4986690328305235,"arXiv:2312.14135, 2023.
457"
REFERENCES,0.49955634427684115,"Xu, Z., Shen, Y., and Huang, L. Multiinstruct: Improving multi-modal zero-shot learning via
458"
REFERENCES,0.5004436557231589,"instruction tuning. arXiv preprint arXiv:2212.10773, 2022.
459"
REFERENCES,0.5013309671694764,"Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and Chen, E.
460"
REFERENCES,0.5022182786157942,"Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint
461"
REFERENCES,0.5031055900621118,"arXiv:2310.16045, 2023.
462"
REFERENCES,0.5039929015084295,"Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In
463"
REFERENCES,0.5048802129547472,"Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
464"
REFERENCES,0.5057675244010648,"11-14, 2016, Proceedings, Part II 14, 2016.
465"
REFERENCES,0.5066548358473825,"Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., and Zhuang, Y. Hallucidoctor:
466"
REFERENCES,0.5075421472937001,"Mitigating hallucinatory toxicity in visual instruction data. arXiv preprint arXiv:2311.13614,
467"
REFERENCES,0.5084294587400178,"2023a.
468"
REFERENCES,0.5093167701863354,"Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating
469"
REFERENCES,0.5102040816326531,"large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023b.
470"
REFERENCES,0.5110913930789707,"Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y., Zhang, Y., and Kong, T. What matters in
471"
REFERENCES,0.5119787045252884,"training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469,
472"
REFERENCES,0.5128660159716061,"2023.
473"
REFERENCES,0.5137533274179237,"Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., and Luo, P. Gpt4roi:
474"
REFERENCES,0.5146406388642414,"Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601,
475"
REFERENCES,0.515527950310559,"2023a.
476"
REFERENCES,0.5164152617568767,"Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual
477"
REFERENCES,0.5173025732031943,"instruction tuning for text-rich image understanding. arXiv preprint, 2023b.
478"
REFERENCES,0.518189884649512,"A
Related Works
479"
REFERENCES,0.5190771960958296,"A.1
Large Vision-Langauge Models as Foundations
480"
REFERENCES,0.5199645075421473,"Most of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia
481"
REFERENCES,0.520851818988465,"et al., 2021), MSCOCO (Lin et al., 2014), VG Krishna et al. (2017), CC3M Sharma et al. (2018),
482"
REFERENCES,0.5217391304347826,"CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), LAION2B (Schuhmann et al., 2022),
483"
REFERENCES,0.5226264418811003,"LAION400M Schuhmann et al. (2021). Starting from Flamingo (Alayrac et al., 2022), a series of
484"
REFERENCES,0.5235137533274179,"LVLMs have focused on training the adaptation layers to align the visual representation to the frozen
485"
REFERENCES,0.5244010647737356,"LLMs on a mixture of image-text pairs with the above corpus, including BLIP2 Li et al. (2023b),
486"
REFERENCES,0.5252883762200532,"KOSMOS Huang et al. (2023b), and OpenFlamingo (Awadalla et al., 2023). Inspired by success of
487"
REFERENCES,0.5261756876663709,"instruction tuning in LLMs (Wang et al., 2022c), a line of works have devoted efforts to build vision-
488"
REFERENCES,0.5270629991126885,"oriented instruction-answer pairs through GPT4 and train models for imitation, such as LLAVA (Liu
489"
REFERENCES,0.5279503105590062,"et al., 2023b), Otter (Li et al., 2023a), VisionLLM (Wang et al., 2023a), MultiInstruct (Xu et al.,
490"
REFERENCES,0.5288376220053239,"2022), Lynx (Zeng et al., 2023), InstructBLIP (Dai et al.), CleverFlamingo (Chen et al., 2023a) and
491"
REFERENCES,0.5297249334516415,"StableLLaVA (Li et al., 2023d). Recently, researchers have proven the efﬁciency of developing
492"
REFERENCES,0.5306122448979592,"LVLMs with two stages of training, the ﬁrst stage of abundant pretraining on image-caption pairs and
493"
REFERENCES,0.5314995563442768,"the second stage of alignment on image-question-answer triples, such as PALI (Chen et al., 2022),
494"
REFERENCES,0.5323868677905945,"PaLI-X (Chen et al., 2023d), Qwen-VL (Bai et al., 2023), and CogVLM Wang et al. (2023b).
495"
REFERENCES,0.5332741792369121,"A.2
Large Vision-Language Models with Reasoning
496"
REFERENCES,0.5341614906832298,"To further enhance the ability of LVLMs in solving high-level visual problems, research focusing
497"
REFERENCES,0.5350488021295474,"on various aspects of reasoning is attracting broad attention. We simply divide existing studies into
498"
REFERENCES,0.5359361135758651,"tree broad categories. The ﬁrst line of research focus on enhance train models with a mastery of
499"
REFERENCES,0.5368234250221828,"cross-modal grounded reasoning, where grounded instruction-following supervision is build through
500"
REFERENCES,0.5377107364685004,"public visual grounding dataset or GPT4-V for training, including KOSMOS-2 (Peng et al., 2023),
501"
REFERENCES,0.5385980479148181,"Shikra (Chen et al., 2023b), and GPT4ROI (Zhang et al., 2023a). The second aspect of efforts have
502"
REFERENCES,0.5394853593611357,"been devoted into promoting models to understand artiﬁcial visual scenes, such as ﬁgures, charts, and
503"
REFERENCES,0.5403726708074534,"receipts. These studies includes CogAgent (Hong et al., 2023) and CHARTVE (Huang et al., 2023a).
504"
REFERENCES,0.541259982253771,"Some other studies address the crucial problem of hallucination in LVLMs with counterfactual or
505"
REFERENCES,0.5421472937000887,"interpretable reasoning (Yu et al., 2023a; Yin et al., 2023). V* (Wu & Xie, 2023) also contributes
506"
REFERENCES,0.5430346051464063,"efforts to enhance the details recognition of VLMs based the LLM-guided searching process.
507"
REFERENCES,0.543921916592724,"B
Limitation and Impact
508"
REFERENCES,0.5448092280390417,"Though we try to develop an accurate and robust framework that engages remarkable LLM to provide
509"
REFERENCES,0.5456965394853593,"basic solving steps, adopts reliable visual tools to obtain visual contents, and then acquires feasible
510"
REFERENCES,0.546583850931677,"paths based on traversal, there are still limitations in our methodology that we hope to improve in the
511"
REFERENCES,0.5474711623779946,"future. First, We ﬁnd that the diversity of linguistic solving steps is insufﬁcient, and the inaccuracy of
512"
REFERENCES,0.5483584738243124,"visual tools (e.g., the rough granularity of grounding boxes, OCR failures on slant letters) will lead
513"
REFERENCES,0.54924578527063,"to a large amount of negative paths (effectively utilizing these paths would beneﬁcial). We suggest
514"
REFERENCES,0.5501330967169477,"to promote these limitations with dedicate prompts and improved visual tools. Second, our current
515"
REFERENCES,0.5510204081632653,"model re-input the manipulated images with a set of hard prompts, which may bring speed losses.
516"
REFERENCES,0.551907719609583,"This is expected to be improved by implementing the physical manipuations into the calculations in
517"
REFERENCES,0.5527950310559007,"vector space. This work presents a general visual reasoning mechanism that alleviate the problems
518"
REFERENCES,0.5536823425022183,"caused by existing conclusion-alignment training for VLMs, introduces a data production framework
519"
REFERENCES,0.554569653948536,"involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM
520"
REFERENCES,0.5554569653948536,"architecture. We expect this work to bring three beneﬁts to the community. First, the proposed visual
521"
REFERENCES,0.5563442768411713,"reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second,
522"
REFERENCES,0.5572315882874889,"the introduced data production framework may be applied to widespread training scenarios to promote
523"
REFERENCES,0.5581188997338066,"the development of current data-driven machine learning. Third, we hope that the memory-based
524"
REFERENCES,0.5590062111801242,"architecture will be helpful for VLMs in multi-turn long contexts.
525"
REFERENCES,0.5598935226264419,"C
Details of Data Production
526"
REFERENCES,0.5607808340727596,"In this section, we further introduce the details of CoM data production, with the overall algorithm of
527"
REFERENCES,0.5616681455190772,"a pseudo code, an example of the solving steps generation with LLM and corresponding guideline, an
528"
REFERENCES,0.5625554569653949,"example of the reasoning chains completion with visual tools. We also list the details of data statistics
529"
REFERENCES,0.5634427684117125,"for the synthesised training data as well as the evaluation data of CoM-test, followed by a limitation
530"
REFERENCES,0.5643300798580302,"analysis for the current data production method.
531"
REFERENCES,0.5652173913043478,"C.1
Algorithm for the Automate Data Generation Pipeline
532"
REFERENCES,0.5661047027506655,"We provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data
533"
REFERENCES,0.5669920141969832,"generation, thereby facilitating understanding and reproduction 1.
534"
REFERENCES,0.5678793256433008,Algorithm 1 Synthesising Chain of Manipulations
REFERENCES,0.5687666370896185,"1: Deﬁne: 
 "
REFERENCES,0.5696539485359361,"Manipulations : {fi : x →y | fi ∈M}
Linguistic Annotator : ΨL
//We use GPT4 in this work
V isual Annotator : ΨV
//We use PaddleOCR and GroundingDINO in this work
2: Input: Image I, Question Q, Answer A
3: // Linguistic Annotation
4: Prompt ΨL with guideline P L to generate reasoning steps:"
REFERENCES,0.5705412599822538,"ς = ΨL(Q|P L),
where
ς = (steps1, steps2, ...)
stepsi = (fi, desci)
(3)"
REFERENCES,0.5714285714285714,"5: Deﬁne tree T
6: for i = 1 to |ς| do
7:
Extract xi, yi instantiated with fi in stepi
8:
Extract referential boxes B from xi
9:
for b in B do
10:
Leverage ΨV to acquire corresponding visual content y′
i = Ψ(xi|I, b), and apply yi to
tree"
REFERENCES,0.5723158828748891,"T .level[i].append(yi)
(4)"
REFERENCES,0.5732031943212067,"11:
end for
12: end for
13: Traverse T to obtain positive chains that leads to given answer with terminal return"
REFERENCES,0.5740905057675244,"[ς1, ς2, ...] = DFS(T |A)
(5)"
REFERENCES,0.5749778172138421,"14: Return [ς1, ς2, ...]"
REFERENCES,0.5758651286601597,"C.2
The CoM-test Benchmark and Evaluation Metric
535"
REFERENCES,0.5767524401064774,"To measure the correctness of CoM chains, we introduce a keypoints-aware metric. The intuition
536"
REFERENCES,0.577639751552795,"is that we care about the key elements including actions (i.e., manipulation name), targets (i.e.,
537"
REFERENCES,0.5785270629991127,"manipulation input), and visual contents (i.e., manipulation returns) of each step in the path, as well
538"
REFERENCES,0.5794143744454303,"as the logical execution order of manipulations. Given a pair of chain-answer annotation (c, a) and
539"
REFERENCES,0.580301685891748,"corresponding model prediction (c′, a′), we ﬁrst sequentially extract the key elements from c and c′
540"
REFERENCES,0.5811889973380656,"to construct two ordered lists, and then replace the elements in the lists with their ﬁxed indices in a
541"
REFERENCES,0.5820763087843833,"Bag-of-Elements E = c∪c′ to result in lists of k and k′. We thus calculate the score as the normalized
542"
REFERENCES,0.582963620230701,"Levenshtein Distance sc = Levenshtein(k, k′)/N between the two lists, where N is the maximum
543"
REFERENCES,0.5838509316770186,"length between k and k′. We adopt this simple discretization strategy with low time complexity
544"
REFERENCES,0.5847382431233363,"to concentrate on the key points as well as the solving order. We further consider the linguistic
545"
REFERENCES,0.5856255545696539,"matching of paragraphs by calculating the BLEU (Papineni et al., 2002) score between two chains
546"
REFERENCES,0.5865128660159716,"sp = BLEU(c, c′), and the ﬁnal sore is a weighted combination as acc = (0.6 × sc + 0.4 × sp)/2.
547"
REFERENCES,0.5874001774622892,"C.3
Data Statistics
548"
REFERENCES,0.5882874889086069,"We develop a strategy to extract predicate phrases based constituency parsing with StandordCoreNLP,
549"
REFERENCES,0.5891748003549245,"in which we extract verb, conjunction-connected verb phrase, preposition-connected verb phrase.
550"
REFERENCES,0.5900621118012422,"Besides the standard CoM data incorporating manipulations with explicit visual evidences, the
551"
REFERENCES,0.59094942324756,"proposed data synthesising framework is compatible of producing implicit visual reasoning steps
552"
REFERENCES,0.5918367346938775,"step′
i = (desci) without involving the manipulations. We thereby also build this partial CoM data on
553"
REFERENCES,0.5927240461401952,"the corpus consisting of absurd visual questions (i.e., asking unanswerable questions based on the
554"
REFERENCES,0.5936113575865128,"given image) to further resist the toxic hallucinations. Speciﬁcally, given an image I with a question
555"
REFERENCES,0.5944986690328306,"Q,we prompt GPT-4V (OpenAI, 2023b) to solve the question step-by-step to acquire the reasoning
556"
REFERENCES,0.5953859804791481,"chains.
557"
REFERENCES,0.5962732919254659,"Data Source
#QAs
#Chains
#Steps/Chain
#Manipulations Types/Chain
TextVQA (Biten et al., 2019)
10782
13766
2.93
2.41
ST-VQA (Singh et al., 2019)
4814
3959
2.88
2.43
TDIUC-count (Shrestha et al., 2019)
53547
54523
2.35
0.74
TDIUC-absurd (Shrestha et al., 2019)
11677
11677
4.09
-
CoM-test
4609
8612
3.26
2.18
Table 4: Detailed statistics the the training data and evaluation data synthesised with CoM production."
REFERENCES,0.5971606033717834,locate read
REFERENCES,0.5980479148181012,identify
REFERENCES,0.5989352262644189,determine
REFERENCES,0.5998225377107365,look for
REFERENCES,0.6007098491570542,observe
REFERENCES,0.6015971606033718,analyze
REFERENCES,0.6024844720496895,examine count
REFERENCES,0.6033717834960071,zoom in
REFERENCES,0.6042590949423248,compare
REFERENCES,0.6051464063886424,"find
focus on use zoom"
REFERENCES,0.6060337178349601,extract
REFERENCES,0.6069210292812778,interpret
REFERENCES,0.6078083407275954,recognize
REFERENCES,0.6086956521739131,search for
REFERENCES,0.6095829636202307,zoom on note
REFERENCES,0.6104702750665484,look at
REFERENCES,0.611357586512866,research
REFERENCES,0.6122448979591837,understand
REFERENCES,0.6131322094055013,provide make list check
REFERENCES,0.614019520851819,enhance
REFERENCES,0.6149068322981367,conduct
REFERENCES,0.6157941437444543,measure
REFERENCES,0.616681455190772,check for match
REFERENCES,0.6175687666370896,answer
REFERENCES,0.6184560780834073,consult
REFERENCES,0.6193433895297249,convert
REFERENCES,0.6202307009760426,confirm
REFERENCES,0.6211180124223602,describe
REFERENCES,0.6220053238686779,inspect
REFERENCES,0.6228926353149956,calculate look open state
REFERENCES,0.6237799467613132,"refer
note down"
REFERENCES,0.6246672582076309,consider
REFERENCES,0.6255545696539485,gather
REFERENCES,0.6264418811002662,estimate
REFERENCES,0.6273291925465838,translate
REFERENCES,0.6282165039929015,record 0.00 0.01 0.02 0.03 0.04 0.05 0.06
REFERENCES,0.6291038154392191,"Figure 7: Distribution of the top-50 generated manipulations out of total 465 based on 4-shot
prompting, where the ﬁrst three bars are scaled with 20% for a smooth visualization of all data."
REFERENCES,0.6299911268855368,"You are a visual assistant capable of generating solving steps for image-oriented visual questions. In each step, you can optionally use a"
REFERENCES,0.6308784383318545,"manipulation to operate the image, which can be used to acquire speciﬁc information from the image or to acquire the processed new image"
REFERENCES,0.6317657497781721,(please be aware that these manipulations will not actually be performed when you generate the solving steps). The manipulation can be one from
REFERENCES,0.6326530612244898,"the predeﬁned ones, or can be a new one you create yourself (should there indeed be a need), where the predeﬁned manipulations with their"
REFERENCES,0.6335403726708074,"descriptions are listed below:\n\n{MANIPULATIONS}.\n\nGiven a question Q abount an image, please generate a series of essential solving"
REFERENCES,0.6344276841171251,"steps, where the output of each step is a tuple consisting of a Manipulation (leave it to None if the current step doesn't involve any manipulation)"
REFERENCES,0.6353149955634427,"and a Description: (1) Manipulation f(x)->y, that is the manipulation `f` targeting `x` to obtain speciﬁc information or image `y`; (2) Description,"
REFERENCES,0.6362023070097604,which is a sentence describing the current solving step.\n Please adhere the following format: given an input of 'Q: xxx. The essential solving
REFERENCES,0.637089618456078,"steps are: ', the output should like 'Step 1: (Manipulation, Description); Step 2: (Manipulation, Description); ...'.\n\n There are several examples:\n"
REFERENCES,0.6379769299023957,{DEMONSTRATIONS}\n\n Q: {QUESTION} The essential solving steps are:
REFERENCES,0.6388642413487134,"REQUIREMENTS (PROMPT)
MANIPULATIONS"
REFERENCES,0.639751552795031,"i-th calculate manipulation, that calculate the formula speciﬁed by the target `tgt` in current image, and return 
the calculation result `res_i`.
grounding_i(tgt)->bbx_i:"
REFERENCES,0.6406388642413487,"i-th crop_and_zoomin manipulation which is useful to identify small and subtle objects in image, that ﬁrst crops 
the current image using the box `bbx` deﬁned by the top-left and bottom-right coordinates, and then zoom in 
the cropped image by two times and ﬁnally return the resulting image `img_i`."
REFERENCES,0.6415261756876663,crop_and_zoomin_i(bbx)->img_i:
REFERENCES,0.642413487133984,"i-th grounding manipulation, that locates the object(s) speciﬁed by the target noun phrase `tgt` in current 
image, and return the resulting bounding box(es) as `bbx_i` where each box is represented by the top-left 
and bottom-right coordinates."
REFERENCES,0.6433007985803016,"OCR_i(tgt)->txt_i:
i-th OCR manipulation, that recognize the natural texts written on the target `tgt`, and return the recognized 
texts `txt_i`.
calculate(tgt)->res_i:"
REFERENCES,0.6441881100266194,Question: What number police station is on the building?
REFERENCES,0.645075421472937,"Step 1: grounding_1(the building)->bbx_1,"
REFERENCES,0.6459627329192547,"Identify the number of the police station on the building in box `bbx_1` and return the 
bounding box of the number as `bbx_2`."
REFERENCES,0.6468500443655724,Recognize the number in the region `bbx_2` and return the recognized number as `txt_1`.
REFERENCES,0.64773735581189,Question: What number police station is on the building?
REFERENCES,0.6486246672582077,Answer: 43
REFERENCES,0.6495119787045253,"Locate the building in the image and return 
the bounding box of the building as `bbx_1`."
REFERENCES,0.650399290150843,"Identify the number of the police station on 
the building in box `bbx_1` and return the 
bounding box of the number as `bbx_2`."
REFERENCES,0.6512866015971606,"Recognize the number in the region `bbx_2` 
and return the recognized number as `txt_1`."
REFERENCES,0.6521739130434783,WkLtUgyndbYvEwmYi2u/AG3+mPiH+hfeGdMQS2iE5KcOfeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrjnBtwSrvB4M4q57Tsebzj9Axlv3PA4cOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyoZY+DswMlJCtalh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1Lq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlV2RN9e/VCXISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1HnsYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>INPUTS
REFERENCES,0.6530612244897959,OUTPUTSL
REFERENCES,0.6539485359361136,OUTPUTSL
REFERENCES,0.6548358473824313,OUTPUTSV L
REFERENCES,0.6557231588287489,INPUTS
REFERENCES,0.6566104702750666,Locate the building in the image and return the bounding box of the building as `bbx_1`.
REFERENCES,0.6574977817213842,"Step 2: (grounding_2(number police station on
               the building in box `bbx_1`)->bbx_2,"
REFERENCES,0.6583850931677019,"Step 3: (OCR_1(number in region `bbx_2`)
               ->txt_1,"
REFERENCES,0.6592724046140195,"Linguistic
Annotation"
REFERENCES,0.6601597160603372,"bbx_1
bbx_1
bbx_2 txt_1"
REFERENCES,0.6610470275066548,"bbx_1
bbx_2"
REFERENCES,0.6619343389529725,"Visual
Annotation"
REFERENCES,0.6628216503992902,Manipulations Deﬁnition and Linguistic Annotation Guideline
REFERENCES,0.6637089618456078,"An Example to show the linguistic annotation results and Visual annotation results
Figure 8: An example shows the conﬁguration, inputs, outputs of the linguistic annotation and visual
annotation."
REFERENCES,0.6645962732919255,"C.4
Details of the Linguistic/Visual Annotations
558"
REFERENCES,0.6654835847382431,"In this work, we adopt the GPT4-turbo as the linguistic annotator for generating problems solving
559"
REFERENCES,0.6663708961845608,"steps, and the API call was conducted during the period of 2023.9 - 2023.12. For the visual annotators,
560"
REFERENCES,0.6672582076308784,"we leverage the the currently best-performing tools, GroundingDINO and PaddleOCR, to acquire all
561"
REFERENCES,0.6681455190771961,"visual contents requested by the manipulations. For a clear description to the production setting and
562"
REFERENCES,0.6690328305235137,"results, we illustrate the guiding prompt, and an example-based linguistic annotation results as well
563"
REFERENCES,0.6699201419698314,"as the visual annotation results in Figure 8.
564"
REFERENCES,0.6708074534161491,"C.5
Limitation Analysis for the Data Production
565"
REFERENCES,0.6716947648624667,"For the implemented data framework, we engage the remarkable LLM to provide basic solving steps,
566"
REFERENCES,0.6725820763087844,"adopt two reliable visual tools (i.e., GroundingDINO and PaddleOCR) to acquire corresponding
567"
REFERENCES,0.673469387755102,"visual contents, and then perform the traversal to achieve feasible reasoning paths, which ensures the
568"
REFERENCES,0.6743566992014197,"correctness and robustness of data synthesizing. However, we also ﬁnd that there are three major
569"
REFERENCES,0.6752440106477373,"limitations caused by the employed models and could be improved in future:
570"
REFERENCES,0.676131322094055,"• The lack of diversity in linguistic reasoning steps. The 5-shot prompting to the GPT-4 gains
571"
REFERENCES,0.6770186335403726,"a stable solving steps, but it also results in the descriptions for executing manipulations or
572"
REFERENCES,0.6779059449866903,"general thinking are similar. We suggest that this can be addressed by employing diversiﬁed
573"
REFERENCES,0.678793256433008,"prompts or requirements.
574"
REFERENCES,0.6796805678793256,"• The inaccuracy of visual tools. We ﬁnd that there are a considerable amount of negative
575"
REFERENCES,0.6805678793256433,"paths caused by the failures of visual tools, such as the rough granularity of bounding boxes
576"
REFERENCES,0.6814551907719609,"and the error recognition of slated letters or long sentences. This issue can be relieved by
577"
REFERENCES,0.6823425022182786,"improving the semantic understanding capabilities of visual tools.
578"
REFERENCES,0.6832298136645962,"D
Details of Training
579"
REFERENCES,0.6841171251109139,"D.1
Launching Prompts
580"
REFERENCES,0.6850044365572315,"• Please solve the problem gradually via a chain of manipulations, where in each
581"
REFERENCES,0.6858917480035492,"step you can selectively adopt one of the following manipulations GROUNDING(a
582"
REFERENCES,0.686779059449867,"phrase)→boxes, OCR(an image or a region)→texts, CROP_AND_ZOOMIN(a region on
583"
REFERENCES,0.6876663708961845,"given image)→new_image, CALCULATE(a computable target)→numbers, or invent a new
584"
REFERENCES,0.6885536823425022,"manipulation, if that seems helpful. {QUESTION}
585"
REFERENCES,0.6894409937888198,"• Please tackle a given question in a stepbystep manner. For each step one of the following
586"
REFERENCES,0.6903283052351376,"manipulations (depicted as Name(Input)→Retrun) can be optionally used: GROUNDING(a
587"
REFERENCES,0.6912156166814551,"phrase)→boxes, OCR(an image or a region)→texts, CROP_AND_ZOOMIN(a region on
588"
REFERENCES,0.6921029281277729,"given image)→new_image, CALCULATE(a computable target)→numbers, or develop a
589"
REFERENCES,0.6929902395740906,"new manipulation yourself (if it is indeed required). {QUESTION}
590"
REFERENCES,0.6938775510204082,"• Please go through the question incrementally with chain of manipulations (optionally use
591"
REFERENCES,0.6947648624667259,"manipulation when needed) such as GROUNDING(a phrase)→boxes, OCR(an image or
592"
REFERENCES,0.6956521739130435,"a region)→texts, CROP_AND_ZOOMIN(a region on given image)→new_image, CAL-
593"
REFERENCES,0.6965394853593612,"CULATE(a computable target)→numbers, and create a new manipulation if necessary.
594"
REFERENCES,0.6974267968056788,"{QUESTION}
595"
REFERENCES,0.6983141082519965,"D.2
Training settings
596"
REFERENCES,0.6992014196983141,"Parameters
Stage1-1
State1-2
Stage-2
Hardware Environment
3,840 A100xdays
256 A100xdays
160 A100xdays
Objective
next token prediction
next token prediction
next token prediction
Images
1.5B
40M
576K
Batch size
8192
1024
160
Iterations
120,000
60000
14000
Optimizer
AdamW
AdamW
AdamW
Learning rate
1e-4
1e-5
1e-5
Warm up steps
7200
1200
280
Trainable weights
6.5B visual expert
6.5B visual expert
6.5B visual expert
Table 5: Training details of all stages."
REFERENCES,0.7000887311446318,"E
Details of Qualitative Analysis
597"
REFERENCES,0.7009760425909495,"E.1
Qualitative Analysis
598"
REFERENCES,0.7018633540372671,"We investigate the evidential reasoning capability of CogCoM on scenarios that requires different
599"
REFERENCES,0.7027506654835848,"types of meticulous reasoning, including recognizing textual details, reading time, understanding
600"
REFERENCES,0.7036379769299024,"charts and counting objects. The results are shown in Figure 1. The ﬁrst case demonstrates that
601"
REFERENCES,0.7045252883762201,"CogCoM ﬁnds the region corresponding to the plane logo through two steps of grounding and then
602"
REFERENCES,0.7054125998225377,"achieves the answer based on zooming in the cropped region. The second case illustrates the ability
603"
REFERENCES,0.7062999112688554,"of CogCoM in reading time, by locating the device that displays time and then transforming the time
604"
REFERENCES,0.707187222715173,"into words based on the read_timne manipulation. In the forth example, CogCoM ﬁrst identiﬁes all
605"
REFERENCES,0.7080745341614907,"visible truck wheels through grounding, and then leverages the counting manipulation to consolidate
606"
REFERENCES,0.7089618456078084,"the total number.
607"
REFERENCES,0.709849157054126,"E.2
Representative Cases
608"
REFERENCES,0.7107364685004437,"Question: What is written on the trafﬁc sign near the bus?
Model:     Using GROUNDING(the trafﬁc sign near the bus) to locate the trafﬁc sign near the bus 
in current image, which is [[084,157,926,703;676,146,740,273]]. Utilizing OCR(texts in region 
[[084,157,926,703]]) to read the texts written on the trafﬁc sign within region [[084,157,926,703]], 
resulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi. CoM:"
REFERENCES,0.7116237799467613,Visual Grounding:
REFERENCES,0.712511091393079,"Question: Find the region in image that ""a trafﬁc sign"" describes.
    Model:     [[678,145,739,270]"
REFERENCES,0.7133984028393966,Grounded Captioning:
REFERENCES,0.7142857142857143,"Question: The goal of this task is to generate description for one part of the image. The part is speciﬁed by [[678,145,739,270]]. 
   Model:     A no tax sign"
REFERENCES,0.7151730257320319,Image Captioning:
REFERENCES,0.7160603371783496,"Question: In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.
   Answer:   Taxi bus with rainbow colors on side."
REFERENCES,0.7169476486246673,Multi Choice:
REFERENCES,0.7178349600709849,"Question: Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer.
   Answer:   black"
REFERENCES,0.7187222715173026,Detailed Captioning:
REFERENCES,0.7196095829636202,"Question: Explain the visual content of the image in great detail.
   Answer:   The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text ""Trafalgar"" in bold red 
letters across its side, indicating it's likely part of a tourist or travel company. The bus is positioned next to a trafﬁc sign that reads ""TAXI"", suggesting it might 
be waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall 
impression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life."
REFERENCES,0.7204968944099379,"Given a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optionally use one of the following manipulations 
(deﬁned as Name(Input)->Return) on the image: GROUNDING(a phrase)->boxes, OCR(an image or a region)->texts, CROP_AND_ZOOMIN(a region on given 
image)->new_image, CALCULATE(a computable target)->numbers, or a new manipulation created by yourself (if it is indeed needed)."
REFERENCES,0.7213842058562555,"Figure 9: CogCoM demonstrates the ﬂexible capabilities for adapting to different multimodal
scenarios, including evidential visual reasoning, Visual Grounding, Grounded Captioning, Image
Captioning, Multi Choice, and Detailed Captioning."
REFERENCES,0.7222715173025732,"NeurIPS Paper Checklist
609"
CLAIMS,0.7231588287488908,"1. Claims
610"
CLAIMS,0.7240461401952085,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
611"
CLAIMS,0.7249334516415262,"paper’s contributions and scope?
612"
CLAIMS,0.7258207630878438,"Answer: [Yes]
613"
CLAIMS,0.7267080745341615,"Justiﬁcation: Please refer to Line 6 to Line 20, and Line 48 to Line 72.
614"
CLAIMS,0.7275953859804791,"Guidelines:
615"
CLAIMS,0.7284826974267968,"• The answer NA means that the abstract and introduction do not include the claims
616"
CLAIMS,0.7293700088731144,"made in the paper.
617"
CLAIMS,0.7302573203194321,"• The abstract and/or introduction should clearly state the claims made, including the
618"
CLAIMS,0.7311446317657497,"contributions made in the paper and important assumptions and limitations. A No or
619"
CLAIMS,0.7320319432120674,"NA answer to this question will not be perceived well by the reviewers.
620"
CLAIMS,0.7329192546583851,"• The claims made should match theoretical and experimental results, and reﬂect how
621"
CLAIMS,0.7338065661047027,"much the results can be expected to generalize to other settings.
622"
CLAIMS,0.7346938775510204,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
623"
CLAIMS,0.735581188997338,"are not attained by the paper.
624"
LIMITATIONS,0.7364685004436557,"2. Limitations
625"
LIMITATIONS,0.7373558118899733,"Question: Does the paper discuss the limitations of the work performed by the authors?
626"
LIMITATIONS,0.738243123336291,"Answer: [Yes]
627"
LIMITATIONS,0.7391304347826086,"Justiﬁcation: Please refer to Appendix B.
628"
LIMITATIONS,0.7400177462289264,"Guidelines:
629"
LIMITATIONS,0.7409050576752441,"• The answer NA means that the paper has no limitation while the answer No means that
630"
LIMITATIONS,0.7417923691215617,"the paper has limitations, but those are not discussed in the paper.
631"
LIMITATIONS,0.7426796805678794,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
632"
LIMITATIONS,0.743566992014197,"• The paper should point out any strong assumptions and how robust the results are to
633"
LIMITATIONS,0.7444543034605147,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
634"
LIMITATIONS,0.7453416149068323,"model well-speciﬁcation, asymptotic approximations only holding locally). The authors
635"
LIMITATIONS,0.74622892635315,"should reﬂect on how these assumptions might be violated in practice and what the
636"
LIMITATIONS,0.7471162377994676,"implications would be.
637"
LIMITATIONS,0.7480035492457853,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
638"
LIMITATIONS,0.748890860692103,"only tested on a few datasets or with a few runs. In general, empirical results often
639"
LIMITATIONS,0.7497781721384206,"depend on implicit assumptions, which should be articulated.
640"
LIMITATIONS,0.7506654835847383,"• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
641"
LIMITATIONS,0.7515527950310559,"For example, a facial recognition algorithm may perform poorly when image resolution
642"
LIMITATIONS,0.7524401064773736,"is low or images are taken in low lighting. Or a speech-to-text system might not be
643"
LIMITATIONS,0.7533274179236912,"used reliably to provide closed captions for online lectures because it fails to handle
644"
LIMITATIONS,0.7542147293700089,"technical jargon.
645"
LIMITATIONS,0.7551020408163265,"• The authors should discuss the computational efﬁciency of the proposed algorithms
646"
LIMITATIONS,0.7559893522626442,"and how they scale with dataset size.
647"
LIMITATIONS,0.7568766637089619,"• If applicable, the authors should discuss possible limitations of their approach to
648"
LIMITATIONS,0.7577639751552795,"address problems of privacy and fairness.
649"
LIMITATIONS,0.7586512866015972,"• While the authors might fear that complete honesty about limitations might be used by
650"
LIMITATIONS,0.7595385980479148,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
651"
LIMITATIONS,0.7604259094942325,"limitations that aren’t acknowledged in the paper. The authors should use their best
652"
LIMITATIONS,0.7613132209405501,"judgment and recognize that individual actions in favor of transparency play an impor-
653"
LIMITATIONS,0.7622005323868678,"tant role in developing norms that preserve the integrity of the community. Reviewers
654"
LIMITATIONS,0.7630878438331854,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
655"
THEORY ASSUMPTIONS AND PROOFS,0.7639751552795031,"3. Theory Assumptions and Proofs
656"
THEORY ASSUMPTIONS AND PROOFS,0.7648624667258208,"Question: For each theoretical result, does the paper provide the full set of assumptions and
657"
THEORY ASSUMPTIONS AND PROOFS,0.7657497781721384,"a complete (and correct) proof?
658"
THEORY ASSUMPTIONS AND PROOFS,0.7666370896184561,"Answer: [Yes]
659"
THEORY ASSUMPTIONS AND PROOFS,0.7675244010647737,"Justiﬁcation: Please refer to Line 74 to Line 92 for pilot experiments.
660"
THEORY ASSUMPTIONS AND PROOFS,0.7684117125110914,"Guidelines:
661"
THEORY ASSUMPTIONS AND PROOFS,0.769299023957409,"• The answer NA means that the paper does not include theoretical results.
662"
THEORY ASSUMPTIONS AND PROOFS,0.7701863354037267,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
663"
THEORY ASSUMPTIONS AND PROOFS,0.7710736468500443,"referenced.
664"
THEORY ASSUMPTIONS AND PROOFS,0.771960958296362,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
665"
THEORY ASSUMPTIONS AND PROOFS,0.7728482697426797,"• The proofs can either appear in the main paper or the supplemental material, but if
666"
THEORY ASSUMPTIONS AND PROOFS,0.7737355811889973,"they appear in the supplemental material, the authors are encouraged to provide a short
667"
THEORY ASSUMPTIONS AND PROOFS,0.774622892635315,"proof sketch to provide intuition.
668"
THEORY ASSUMPTIONS AND PROOFS,0.7755102040816326,"• Inversely, any informal proof provided in the core of the paper should be complemented
669"
THEORY ASSUMPTIONS AND PROOFS,0.7763975155279503,"by formal proofs provided in appendix or supplemental material.
670"
THEORY ASSUMPTIONS AND PROOFS,0.7772848269742679,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7781721384205856,"4. Experimental Result Reproducibility
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7790594498669032,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7799467613132209,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7808340727595386,"of the paper (regardless of whether the code and data are provided or not)?
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7817213842058562,"Answer: [Yes]
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782608695652174,"Justiﬁcation: The data collection with Section 3, model training process with Section 4, and
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7834960070984915,"the experimental settings with Section 5.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7843833185448092,"Guidelines:
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852706299911268,"• The answer NA means that the paper does not include experiments.
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861579414374446,"• If the paper includes experiments, a No answer to this question will not be perceived
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870452528837621,"well by the reviewers: Making the paper reproducible is important, regardless of
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7879325643300799,"whether the code and data are provided or not.
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7888198757763976,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7897071872227152,"to make their results reproducible or veriﬁable.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7905944986690329,"• Depending on the contribution, reproducibility can be accomplished in various ways.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7914818101153505,"For example, if the contribution is a novel architecture, describing the architecture fully
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923691215616682,"might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932564330079858,"be necessary to either make it possible for others to replicate the model with the same
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941437444543035,"dataset, or provide access to the model. In general. releasing code and data is often
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7950310559006211,"one good way to accomplish this, but reproducibility can also be provided via detailed
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959183673469388,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7968056787932565,"of a large language model), releasing of a model checkpoint, or other means that are
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7976929902395741,"appropriate to the research performed.
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7985803016858918,"• While NeurIPS does not require releasing code, the conference does require all submis-
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7994676131322094,"sions to provide some reasonable avenue for reproducibility, which may depend on the
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8003549245785271,"nature of the contribution. For example
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8012422360248447,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8021295474711624,"to reproduce that algorithm.
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80301685891748,"(b) If the contribution is primarily a new model architecture, the paper should describe
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8039041703637977,"the architecture clearly and fully.
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8047914818101154,"(c) If the contribution is a new model (e.g., a large language model), then there should
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.805678793256433,"either be a way to access this model for reproducing the results or a way to reproduce
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8065661047027507,"the model (e.g., with an open-source dataset or instructions for how to construct
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8074534161490683,"the dataset).
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.808340727595386,"(d) We recognize that reproducibility may be tricky in some cases, in which case
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8092280390417036,"authors are welcome to describe the particular way they provide for reproducibility.
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8101153504880213,"In the case of closed-source models, it may be that access to the model is limited in
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8110026619343389,"some way (e.g., to registered users), but it should be possible for other researchers
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8118899733806566,"to have some path to reproducing or verifying the results.
710"
OPEN ACCESS TO DATA AND CODE,0.8127772848269743,"5. Open access to data and code
711"
OPEN ACCESS TO DATA AND CODE,0.8136645962732919,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
712"
OPEN ACCESS TO DATA AND CODE,0.8145519077196096,"tions to faithfully reproduce the main experimental results, as described in supplemental
713"
OPEN ACCESS TO DATA AND CODE,0.8154392191659272,"material?
714"
OPEN ACCESS TO DATA AND CODE,0.8163265306122449,"Answer: [Yes]
715"
OPEN ACCESS TO DATA AND CODE,0.8172138420585625,"Justiﬁcation: We will open-source the code, model weights, and all collected data. The
716"
OPEN ACCESS TO DATA AND CODE,0.8181011535048802,"generation process and statistics of data are available at Section 3 and Appendix C.
717"
OPEN ACCESS TO DATA AND CODE,0.8189884649511979,"Guidelines:
718"
OPEN ACCESS TO DATA AND CODE,0.8198757763975155,"• The answer NA means that paper does not include experiments requiring code.
719"
OPEN ACCESS TO DATA AND CODE,0.8207630878438332,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
720"
OPEN ACCESS TO DATA AND CODE,0.8216503992901508,"public/guides/CodeSubmissionPolicy) for more details.
721"
OPEN ACCESS TO DATA AND CODE,0.8225377107364685,"• While we encourage the release of code and data, we understand that this might not be
722"
OPEN ACCESS TO DATA AND CODE,0.8234250221827861,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
723"
OPEN ACCESS TO DATA AND CODE,0.8243123336291038,"including code, unless this is central to the contribution (e.g., for a new open-source
724"
OPEN ACCESS TO DATA AND CODE,0.8251996450754214,"benchmark).
725"
OPEN ACCESS TO DATA AND CODE,0.8260869565217391,"• The instructions should contain the exact command and environment needed to run to
726"
OPEN ACCESS TO DATA AND CODE,0.8269742679680568,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
727"
OPEN ACCESS TO DATA AND CODE,0.8278615794143744,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
728"
OPEN ACCESS TO DATA AND CODE,0.8287488908606921,"• The authors should provide instructions on data access and preparation, including how
729"
OPEN ACCESS TO DATA AND CODE,0.8296362023070097,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
730"
OPEN ACCESS TO DATA AND CODE,0.8305235137533274,"• The authors should provide scripts to reproduce all experimental results for the new
731"
OPEN ACCESS TO DATA AND CODE,0.831410825199645,"proposed method and baselines. If only a subset of experiments are reproducible, they
732"
OPEN ACCESS TO DATA AND CODE,0.8322981366459627,"should state which ones are omitted from the script and why.
733"
OPEN ACCESS TO DATA AND CODE,0.8331854480922803,"• At submission time, to preserve anonymity, the authors should release anonymized
734"
OPEN ACCESS TO DATA AND CODE,0.834072759538598,"versions (if applicable).
735"
OPEN ACCESS TO DATA AND CODE,0.8349600709849158,"• Providing as much information as possible in supplemental material (appended to the
736"
OPEN ACCESS TO DATA AND CODE,0.8358473824312334,"paper) is recommended, but including URLs to data and code is permitted.
737"
OPEN ACCESS TO DATA AND CODE,0.8367346938775511,"6. Experimental Setting/Details
738"
OPEN ACCESS TO DATA AND CODE,0.8376220053238687,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
739"
OPEN ACCESS TO DATA AND CODE,0.8385093167701864,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
740"
OPEN ACCESS TO DATA AND CODE,0.839396628216504,"results?
741"
OPEN ACCESS TO DATA AND CODE,0.8402839396628217,"Answer: [Yes]
742"
OPEN ACCESS TO DATA AND CODE,0.8411712511091393,"Justiﬁcation: The experimental settings are listed in each subsection of benchmark eval-
743"
OPEN ACCESS TO DATA AND CODE,0.842058562555457,"uation, which is Section 5.1, Section 5.2 and Section 5.3. The training settings with
744"
OPEN ACCESS TO DATA AND CODE,0.8429458740017747,"hyperparameters and optimizations are listed at Appendix D.2.
745"
OPEN ACCESS TO DATA AND CODE,0.8438331854480923,"Guidelines:
746"
OPEN ACCESS TO DATA AND CODE,0.84472049689441,"• The answer NA means that the paper does not include experiments.
747"
OPEN ACCESS TO DATA AND CODE,0.8456078083407276,"• The experimental setting should be presented in the core of the paper to a level of detail
748"
OPEN ACCESS TO DATA AND CODE,0.8464951197870453,"that is necessary to appreciate the results and make sense of them.
749"
OPEN ACCESS TO DATA AND CODE,0.8473824312333629,"• The full details can be provided either with the code, in appendix, or as supplemental
750"
OPEN ACCESS TO DATA AND CODE,0.8482697426796806,"material.
751"
OPEN ACCESS TO DATA AND CODE,0.8491570541259982,"7. Experiment Statistical Signiﬁcance
752"
OPEN ACCESS TO DATA AND CODE,0.8500443655723159,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
753"
OPEN ACCESS TO DATA AND CODE,0.8509316770186336,"information about the statistical signiﬁcance of the experiments?
754"
OPEN ACCESS TO DATA AND CODE,0.8518189884649512,"Answer: [Yes]
755"
OPEN ACCESS TO DATA AND CODE,0.8527062999112689,"Justiﬁcation: We include the error analysis with textual descriptions in Appendix C.5, as
756"
OPEN ACCESS TO DATA AND CODE,0.8535936113575865,"well as the reasoning accuracy with limitations in Section 5.1.2.
757"
OPEN ACCESS TO DATA AND CODE,0.8544809228039042,"Guidelines:
758"
OPEN ACCESS TO DATA AND CODE,0.8553682342502218,"• The answer NA means that the paper does not include experiments.
759"
OPEN ACCESS TO DATA AND CODE,0.8562555456965395,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
760"
OPEN ACCESS TO DATA AND CODE,0.8571428571428571,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
761"
OPEN ACCESS TO DATA AND CODE,0.8580301685891748,"the main claims of the paper.
762"
OPEN ACCESS TO DATA AND CODE,0.8589174800354925,"• The factors of variability that the error bars are capturing should be clearly stated (for
763"
OPEN ACCESS TO DATA AND CODE,0.8598047914818101,"example, train/test split, initialization, random drawing of some parameter, or overall
764"
OPEN ACCESS TO DATA AND CODE,0.8606921029281278,"run with given experimental conditions).
765"
OPEN ACCESS TO DATA AND CODE,0.8615794143744454,"• The method for calculating the error bars should be explained (closed form formula,
766"
OPEN ACCESS TO DATA AND CODE,0.8624667258207631,"call to a library function, bootstrap, etc.)
767"
OPEN ACCESS TO DATA AND CODE,0.8633540372670807,"• The assumptions made should be given (e.g., Normally distributed errors).
768"
OPEN ACCESS TO DATA AND CODE,0.8642413487133984,"• It should be clear whether the error bar is the standard deviation or the standard error
769"
OPEN ACCESS TO DATA AND CODE,0.865128660159716,"of the mean.
770"
OPEN ACCESS TO DATA AND CODE,0.8660159716060337,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
771"
OPEN ACCESS TO DATA AND CODE,0.8669032830523514,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
772"
OPEN ACCESS TO DATA AND CODE,0.867790594498669,"of Normality of errors is not veriﬁed.
773"
OPEN ACCESS TO DATA AND CODE,0.8686779059449867,"• For asymmetric distributions, the authors should be careful not to show in tables or
774"
OPEN ACCESS TO DATA AND CODE,0.8695652173913043,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
775"
OPEN ACCESS TO DATA AND CODE,0.870452528837622,"error rates).
776"
OPEN ACCESS TO DATA AND CODE,0.8713398402839396,"• If error bars are reported in tables or plots, The authors should explain in the text how
777"
OPEN ACCESS TO DATA AND CODE,0.8722271517302573,"they were calculated and reference the corresponding ﬁgures or tables in the text.
778"
EXPERIMENTS COMPUTE RESOURCES,0.8731144631765749,"8. Experiments Compute Resources
779"
EXPERIMENTS COMPUTE RESOURCES,0.8740017746228926,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
780"
EXPERIMENTS COMPUTE RESOURCES,0.8748890860692103,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
781"
EXPERIMENTS COMPUTE RESOURCES,0.8757763975155279,"the experiments?
782"
EXPERIMENTS COMPUTE RESOURCES,0.8766637089618456,"Answer: [Yes]
783"
EXPERIMENTS COMPUTE RESOURCES,0.8775510204081632,"Justiﬁcation: Detailed compute resources are listed in Section D.2.
784"
EXPERIMENTS COMPUTE RESOURCES,0.878438331854481,"Guidelines:
785"
EXPERIMENTS COMPUTE RESOURCES,0.8793256433007985,"• The answer NA means that the paper does not include experiments.
786"
EXPERIMENTS COMPUTE RESOURCES,0.8802129547471162,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
787"
EXPERIMENTS COMPUTE RESOURCES,0.8811002661934338,"or cloud provider, including relevant memory and storage.
788"
EXPERIMENTS COMPUTE RESOURCES,0.8819875776397516,"• The paper should provide the amount of compute required for each of the individual
789"
EXPERIMENTS COMPUTE RESOURCES,0.8828748890860693,"experimental runs as well as estimate the total compute.
790"
EXPERIMENTS COMPUTE RESOURCES,0.8837622005323869,"• The paper should disclose whether the full research project required more compute
791"
EXPERIMENTS COMPUTE RESOURCES,0.8846495119787046,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
792"
EXPERIMENTS COMPUTE RESOURCES,0.8855368234250222,"didn’t make it into the paper).
793"
CODE OF ETHICS,0.8864241348713399,"9. Code Of Ethics
794"
CODE OF ETHICS,0.8873114463176575,"Question: Does the research conducted in the paper conform, in every respect, with the
795"
CODE OF ETHICS,0.8881987577639752,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
796"
CODE OF ETHICS,0.8890860692102928,"Answer: [Yes]
797"
CODE OF ETHICS,0.8899733806566105,"Justiﬁcation: All code anonymity.
798"
CODE OF ETHICS,0.8908606921029282,"Guidelines:
799"
CODE OF ETHICS,0.8917480035492458,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
800"
CODE OF ETHICS,0.8926353149955635,"• If the authors answer No, they should explain the special circumstances that require a
801"
CODE OF ETHICS,0.8935226264418811,"deviation from the Code of Ethics.
802"
CODE OF ETHICS,0.8944099378881988,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
803"
CODE OF ETHICS,0.8952972493345164,"eration due to laws or regulations in their jurisdiction).
804"
BROADER IMPACTS,0.8961845607808341,"10. Broader Impacts
805"
BROADER IMPACTS,0.8970718722271517,"Question: Does the paper discuss both potential positive societal impacts and negative
806"
BROADER IMPACTS,0.8979591836734694,"societal impacts of the work performed?
807"
BROADER IMPACTS,0.8988464951197871,"Answer: [Yes]
808"
BROADER IMPACTS,0.8997338065661047,"Justiﬁcation: We discussed the societal impacts in details in Appendix B.
809"
BROADER IMPACTS,0.9006211180124224,"Guidelines:
810"
BROADER IMPACTS,0.90150842945874,"• The answer NA means that there is no societal impact of the work performed.
811"
BROADER IMPACTS,0.9023957409050577,"• If the authors answer NA or No, they should explain why their work has no societal
812"
BROADER IMPACTS,0.9032830523513753,"impact or why the paper does not address societal impact.
813"
BROADER IMPACTS,0.904170363797693,"• Examples of negative societal impacts include potential malicious or unintended uses
814"
BROADER IMPACTS,0.9050576752440106,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
815"
BROADER IMPACTS,0.9059449866903283,"(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
816"
BROADER IMPACTS,0.906832298136646,"groups), privacy considerations, and security considerations.
817"
BROADER IMPACTS,0.9077196095829636,"• The conference expects that many papers will be foundational research and not tied
818"
BROADER IMPACTS,0.9086069210292813,"to particular applications, let alone deployments. However, if there is a direct path to
819"
BROADER IMPACTS,0.9094942324755989,"any negative applications, the authors should point it out. For example, it is legitimate
820"
BROADER IMPACTS,0.9103815439219166,"to point out that an improvement in the quality of generative models could be used to
821"
BROADER IMPACTS,0.9112688553682342,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
822"
BROADER IMPACTS,0.9121561668145519,"that a generic algorithm for optimizing neural networks could enable people to train
823"
BROADER IMPACTS,0.9130434782608695,"models that generate Deepfakes faster.
824"
BROADER IMPACTS,0.9139307897071872,"• The authors should consider possible harms that could arise when the technology is
825"
BROADER IMPACTS,0.9148181011535049,"being used as intended and functioning correctly, harms that could arise when the
826"
BROADER IMPACTS,0.9157054125998225,"technology is being used as intended but gives incorrect results, and harms following
827"
BROADER IMPACTS,0.9165927240461402,"from (intentional or unintentional) misuse of the technology.
828"
BROADER IMPACTS,0.9174800354924578,"• If there are negative societal impacts, the authors could also discuss possible mitigation
829"
BROADER IMPACTS,0.9183673469387755,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
830"
BROADER IMPACTS,0.9192546583850931,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
831"
BROADER IMPACTS,0.9201419698314108,"feedback over time, improving the efﬁciency and accessibility of ML).
832"
SAFEGUARDS,0.9210292812777284,"11. Safeguards
833"
SAFEGUARDS,0.9219165927240461,"Question: Does the paper describe safeguards that have been put in place for responsible
834"
SAFEGUARDS,0.9228039041703638,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
835"
SAFEGUARDS,0.9236912156166814,"image generators, or scraped datasets)?
836"
SAFEGUARDS,0.9245785270629991,"Answer: [NA]
837"
SAFEGUARDS,0.9254658385093167,"Justiﬁcation: The training data collected from public datasets with research purpose does
838"
SAFEGUARDS,0.9263531499556344,"not face the safety risks.
839"
SAFEGUARDS,0.927240461401952,"Guidelines:
840"
SAFEGUARDS,0.9281277728482697,"• The answer NA means that the paper poses no such risks.
841"
SAFEGUARDS,0.9290150842945873,"• Released models that have a high risk for misuse or dual-use should be released with
842"
SAFEGUARDS,0.929902395740905,"necessary safeguards to allow for controlled use of the model, for example by requiring
843"
SAFEGUARDS,0.9307897071872228,"that users adhere to usage guidelines or restrictions to access the model or implementing
844"
SAFEGUARDS,0.9316770186335404,"safety ﬁlters.
845"
SAFEGUARDS,0.9325643300798581,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
846"
SAFEGUARDS,0.9334516415261757,"should describe how they avoided releasing unsafe images.
847"
SAFEGUARDS,0.9343389529724934,"• We recognize that providing effective safeguards is challenging, and many papers do
848"
SAFEGUARDS,0.935226264418811,"not require this, but we encourage authors to take this into account and make a best
849"
SAFEGUARDS,0.9361135758651287,"faith effort.
850"
LICENSES FOR EXISTING ASSETS,0.9370008873114463,"12. Licenses for existing assets
851"
LICENSES FOR EXISTING ASSETS,0.937888198757764,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
852"
LICENSES FOR EXISTING ASSETS,0.9387755102040817,"the paper, properly credited and are the license and terms of use explicitly mentioned and
853"
LICENSES FOR EXISTING ASSETS,0.9396628216503993,"properly respected?
854"
LICENSES FOR EXISTING ASSETS,0.940550133096717,"Answer: [Yes]
855"
LICENSES FOR EXISTING ASSETS,0.9414374445430346,"Justiﬁcation: The public available datasets used in this paper are cited properly.
856"
LICENSES FOR EXISTING ASSETS,0.9423247559893523,"Guidelines:
857"
LICENSES FOR EXISTING ASSETS,0.9432120674356699,"• The answer NA means that the paper does not use existing assets.
858"
LICENSES FOR EXISTING ASSETS,0.9440993788819876,"• The authors should cite the original paper that produced the code package or dataset.
859"
LICENSES FOR EXISTING ASSETS,0.9449866903283053,"• The authors should state which version of the asset is used and, if possible, include a
860"
LICENSES FOR EXISTING ASSETS,0.9458740017746229,"URL.
861"
LICENSES FOR EXISTING ASSETS,0.9467613132209406,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
862"
LICENSES FOR EXISTING ASSETS,0.9476486246672582,"• For scraped data from a particular source (e.g., website), the copyright and terms of
863"
LICENSES FOR EXISTING ASSETS,0.9485359361135759,"service of that source should be provided.
864"
LICENSES FOR EXISTING ASSETS,0.9494232475598935,"• If assets are released, the license, copyright information, and terms of use in the
865"
LICENSES FOR EXISTING ASSETS,0.9503105590062112,"package should be provided. For popular datasets, paperswithcode.com/datasets
866"
LICENSES FOR EXISTING ASSETS,0.9511978704525288,"has curated licenses for some datasets. Their licensing guide can help determine the
867"
LICENSES FOR EXISTING ASSETS,0.9520851818988465,"license of a dataset.
868"
LICENSES FOR EXISTING ASSETS,0.9529724933451642,"• For existing datasets that are re-packaged, both the original license and the license of
869"
LICENSES FOR EXISTING ASSETS,0.9538598047914818,"the derived asset (if it has changed) should be provided.
870"
LICENSES FOR EXISTING ASSETS,0.9547471162377995,"• If this information is not available online, the authors are encouraged to reach out to
871"
LICENSES FOR EXISTING ASSETS,0.9556344276841171,"the asset’s creators.
872"
NEW ASSETS,0.9565217391304348,"13. New Assets
873"
NEW ASSETS,0.9574090505767524,"Question: Are new assets introduced in the paper well documented and is the documentation
874"
NEW ASSETS,0.9582963620230701,"provided alongside the assets?
875"
NEW ASSETS,0.9591836734693877,"Answer: [Yes]
876"
NEW ASSETS,0.9600709849157054,"Justiﬁcation: The newly created benchmark CoM-test is documented in detailed in Section
877"
NEW ASSETS,0.9609582963620231,"5 and Appendix C.2, and the manually annotated math data is described in Section 3.2.
878"
NEW ASSETS,0.9618456078083407,"Guidelines:
879"
NEW ASSETS,0.9627329192546584,"• The answer NA means that the paper does not release new assets.
880"
NEW ASSETS,0.963620230700976,"• Researchers should communicate the details of the dataset/code/model as part of their
881"
NEW ASSETS,0.9645075421472937,"submissions via structured templates. This includes details about training, license,
882"
NEW ASSETS,0.9653948535936113,"limitations, etc.
883"
NEW ASSETS,0.966282165039929,"• The paper should discuss whether and how consent was obtained from people whose
884"
NEW ASSETS,0.9671694764862466,"asset is used.
885"
NEW ASSETS,0.9680567879325643,"• At submission time, remember to anonymize your assets (if applicable). You can either
886"
NEW ASSETS,0.968944099378882,"create an anonymized URL or include an anonymized zip ﬁle.
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698314108251996,"14. Crowdsourcing and Research with Human Subjects
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707187222715173,"Question: For crowdsourcing experiments and research with human subjects, does the paper
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716060337178349,"include the full text of instructions given to participants and screenshots, if applicable, as
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724933451641526,"well as details about compensation (if any)?
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9733806566104702,"Answer: [NA]
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974267968056788,"Justiﬁcation: This study does not involve crowdsourcing nor research with human subjects.
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751552795031055,"Guidelines:
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760425909494232,"• The answer NA means that the paper does not involve crowdsourcing nor research with
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976929902395741,"human subjects.
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778172138420586,"• Including this information in the supplemental material is ﬁne, but if the main contribu-
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787045252883763,"tion of the paper involves human subjects, then as much detail as possible should be
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795918367346939,"included in the main paper.
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804791481810116,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813664596273292,"or other labor should be paid at least the minimum wage in the country of the data
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822537710736469,"collector.
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831410825199645,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840283939662822,"Subjects
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849157054125999,"Question: Does the paper describe potential risks incurred by study participants, whether
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858030168589175,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866903283052352,"approvals (or an equivalent approval/review based on the requirements of your country or
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875776397515528,"institution) were obtained?
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884649511978705,"Answer: [NA]
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893522626441881,"Justiﬁcation: This study does not involve crowdsourcing nor research with human subjects.
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902395740905058,"Guidelines:
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911268855368234,"• The answer NA means that the paper does not involve crowdsourcing nor research with
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920141969831411,"human subjects.
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929015084294588,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937888198757764,"may be required for any human subjects research. If you obtained IRB approval, you
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946761313220941,"should clearly state this in the paper.
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955634427684117,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964507542147294,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997338065661047,"guidelines for their institution.
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982253771073647,"• For initial submissions, do not include any information that would break anonymity (if
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991126885536823,"applicable), such as the institution conducting the review.
921"
