Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001483679525222552,"Graph neural networks (GNNs) use graph convolutions to exploit network invari-
ances and learn meaningful features from network data. However, on large-scale
graphs convolutions incur in high computational cost, leading to scalability lim-
itations. Leveraging the graphon — the limit object of a graph — in this paper
we consider the problem of learning a graphon neural network (WNN) — the
limit object of a GNN — by training GNNs on graphs sampled Bernoulli from the
graphon. Under smoothness conditions, we show that: (i) the expected distance
between the learning steps on the GNN and on the WNN decreases asymptoti-
cally with the size of the graph, and (ii) when training on a sequence of growing
graphs, gradient descent follows the learning direction of the WNN. Inspired by
these results, we propose a novel algorithm to learn GNNs on large-scale graphs
that, starting from a moderate number of nodes, successively increases the size
of the graph during training. This algorithm is benchmarked on both a recom-
mendation system and a decentralized control problem where it is shown to retain
comparable performance to its large-scale counterpart at a reduced computational
cost."
INTRODUCTION,0.002967359050445104,"1
INTRODUCTION"
INTRODUCTION,0.004451038575667656,"Graph Neural Networks (GNNs) are deep convolutional architectures formed by a succession of
layers where each layer composes a graph convolution and a pointwise nonlinearity (Wu et al., 2021;
Zhou et al., 2020). Tailored to network data, GNNs have been used in a variety of applications such
as recommendation systems (Fan et al., 2019; Tan et al., 2020; Ying et al., 2018; Schlichtkrull et al.,
2018; Ruiz et al., 2019a) and Markov chains (Qu et al., 2019; Ruiz et al., 2019b; Li et al., 2015),
and ﬁelds such as biology (Fout et al., 2017; Duvenaud et al., 2015; Gilmer et al., 2017; Chen et al.,
2020) and robotics (Qi et al., 2018; Gama & Sojoudi, 2021; Li et al., 2019). Their success in these
ﬁelds and applications provides ample empirical evidence of the ability of GNNs to generalize to
unseen data. More recently, their successful performance has also been justiﬁed by theoretical works
showing that GNNs are invariant to relabelings (Chen et al., 2019; Keriven & Peyr´e, 2019), stable
to graph perturbations (Gama et al., 2020) and transferable across graphs (Ruiz et al., 2020a)."
INTRODUCTION,0.005934718100890208,"One of the most important features of a GNN is that, because the linear operation is a graph convo-
lution, its number of parameters does not depend on the number of nodes of the graph. In theory,
this means that GNNs can be trained on graphs of any size. In practice, however, if the graph has
large number of nodes training the GNN is costly because computing graph convolutions involves
large matrix operations. While this issue could be mitigated by transferability — training the GNN
on a smaller graph to execute on the large graph —, this approach does not give any guarantees
on the distance between the optimal solutions on the small and on the large graph. In other words,
when executing the GNN on the target graph we do not know if its error will be dominated by the
transferability error or by the generalization error from training."
INTRODUCTION,0.00741839762611276,"In this paper, we address the computational burden of training a GNN on a large graph by progres-
sively increasing the size of the network. We consider the limit problem of learning an “optimal”
neural network for a graphon, which is both a graph limit and a random graph model (Lov´asz,
2012). We postulate that, because sequences of graphs sampled from the graphon converge to it, the
so-called graphon neural network (Ruiz et al., 2020a) can be learned by sampling graphs of growing
size and training a GNN on these graphs (Algorithm 1). We prove that this is true in two steps. In"
INTRODUCTION,0.008902077151335312,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010385756676557863,"Theorem 1, we bound the expected distance between the gradient descent steps on the GNN and on
the graphon neural network by a term that decreases asymptotically with the size of the graph. A
consequence of this bias bound is that it allows us to quantify the trade-off between a more accurate
gradient and one that could be obtained with less computational power. We then use this theorem to
prove our main result in Theorem 2, which is stated in simpliﬁed form below."
INTRODUCTION,0.011869436201780416,"Theorem (Graphon neural network learning, informal) Let W be a graphon and let {Gn} be a
sequence of growing graphs sampled from W. Consider the graphon neural network Φ(W) and
assume that it is learned by training the GNN Φ(Gn) with loss function ℓ(yn, Φ(Gn)) on the
sequence {Gn}. Over a ﬁnite number of training steps, we obtain"
INTRODUCTION,0.013353115727002967,"∥∇ℓ(Y, Φ(W)∥≤ϵ with probability 1."
INTRODUCTION,0.01483679525222552,"The most important implication of this result is that the learning iterates computed in the sequence of
growing graphs follow the direction of the graphon gradient up to a small ball, which provides the-
oretical validation to our cost efﬁcient training methodology. We also validate our algorithm in two
numerical experiments. In the ﬁrst, we learn a GNN-based recommendation system on increasingly
large subnetworks of a movie similarity graph, and compare it with the recommendation system
trained on the full graph. In the second, we consider the problem of ﬂocking and train GNNs to
learn the actions agents need to take to ﬂock. We compare the results obtained when progressively
increasing the number of agents during training and when training directly on the target graph."
RELATED WORK,0.016320474777448073,"2
RELATED WORK"
RELATED WORK,0.017804154302670624,"GNNs are data processing architectures that follow from the seminal works in the areas of deep
learning applied to graph theory (Bruna et al., 2013; Defferrard et al., 2016; Gori et al., 2005; Lu &
Getoor, 2003). They have been successfully used in a wide variety of statistical learning problems
(Kipf & Welling, 2016; Scarselli et al., 2018), where their good performance is generally attributed
to the fact that they exploit invariances present in network data (Maron et al., 2019; Gama et al.,
2018; Chami et al., 2021)."
RELATED WORK,0.019287833827893175,"More recently, a number of works show that GNNs can be transferred across graphs of different
sizes (Ruiz et al., 2020a; Levie et al., 2019; Keriven et al., 2020). Speciﬁcally, (Ruiz et al., 2020a)
leverages graphons to deﬁne families of graphs within which GNNs can be transferred with an
error bound that decreases asymptotically with the size of the graph. The papers by Levie et al.
(2019) and Keriven et al. (2020) offer similar results by considering the graph limit to be a generic
topological space and a random graph model respectively. In this paper, we use an extension of the
transferability bound derived in Ruiz et al. (2020a) to propose a novel learning algorithm for GNNs."
PRELIMINARY DEFINITIONS,0.020771513353115726,"3
PRELIMINARY DEFINITIONS"
GRAPH NEURAL NETWORKS,0.02225519287833828,"3.1
GRAPH NEURAL NETWORKS"
GRAPH NEURAL NETWORKS,0.02373887240356083,"Graph neural networks exploit graph symmetries to extract meaningful information from network
data (Ruiz et al., 2020c; Gama et al., 2020). Graphs are represented as triplets Gn = (V, E, W),
where V, |V| = n, is the set of nodes, E ⊆V × V is the set of edges and W : E →R is a map
assigning weights to each edge. The graph Gn can also be represented by the graph shift operator
(GSO) S ∈Rn×n, a square matrix that respects the sparsity of the graph. Examples of GSOs include
the adjacency matrix A, the graph Laplacian L = diag(A1) −A and their normalized counterparts
Gama et al. (2018). In this paper we consider the graph Gn to be undirected and ﬁx S = A/n."
GRAPH NEURAL NETWORKS,0.025222551928783383,"Graph data is represented in the form of graph signals. A graph signal x = [x1, . . . , xn]T ∈Rn
is a vector whose i-th component corresponds to the information present at the i-th node of graph
Gn. A basic data aggregation operation can be deﬁned by applying the GSO S to graph signals x.
The resulting signal z = Sx is such that the data at node i is a weighted average of the information
in the 1-hop neighborhood of i, zi = P"
GRAPH NEURAL NETWORKS,0.026706231454005934,"j∈Ni[S]ijxj where Ni = {j | [S]ij ̸= 0}. Information
coming from further neighborhoods can be aggregated by successive applications of the GSO, also
called shifts. Using this notion of shift, graph convolutions are deﬁned by weighting the contribu-
tion of each successive application of S to deﬁne a polynomial in the GSO. Explicitly, the graph"
GRAPH NEURAL NETWORKS,0.028189910979228485,Under review as a conference paper at ICLR 2022
GRAPH NEURAL NETWORKS,0.02967359050445104,"convolutional ﬁlter with coefﬁcients h = [h0, . . . , hK−1] is given by"
GRAPH NEURAL NETWORKS,0.03115727002967359,"y = h ∗S x = K−1
X"
GRAPH NEURAL NETWORKS,0.032640949554896145,"k=0
hkSkx
(1)"
GRAPH NEURAL NETWORKS,0.03412462908011869,where ∗S denotes the convolution operation with GSO S.
GRAPH NEURAL NETWORKS,0.03560830860534125,"Since the adjacency matrix of an undirected graph is always symmetric, the GSO admits an eigen-
decomposition S = VΛVH. The columns of V are the graph eigenvectors and the diagonal el-
ements of Λ are the graph eigenvalues, which take values between −1 and 1 and are ordered as
−1 ≤λ−1 ≤λ−2 ≤. . . ≤0 ≤. . . ≤λ2 ≤λ1 ≤1. Since the eigenvectors of S form an
orthonormal basis of Rn, we can project (1) onto this basis to obtain the spectral representation of
the graph convolution, which is given by"
GRAPH NEURAL NETWORKS,0.037091988130563795,"h(λ) = K−1
X"
GRAPH NEURAL NETWORKS,0.03857566765578635,"k=0
hkλk.
(2)"
GRAPH NEURAL NETWORKS,0.040059347181008904,"Note that (2) only depends on the hk and on the eigenvalues of the GSO. Hence, as a consequence
of the Cayley-Hamilton theorem, convolutional ﬁlters may be used to represent any graph ﬁlter with
spectral representation h(λ) = f(λ) where f is analytic (Strang, 1976)."
GRAPH NEURAL NETWORKS,0.04154302670623145,"Graph neural networks are layered architectures where each layer consists of a graph convolution
followed by a pointwise nonlinearity ρ, and where each layer’s output is the input to the following
layer. At layer l, a GNN can output multiple features xf
l , 1 ≤f ≤Fl which we stack in a
matrix Xl = [x1
l , . . . , xFl
l ] ∈Rn×Fl. Each column of the feature matrix is the value of the graph
signal at feature f. To map the Fl−1 features coming from layer l −1 into Fl features, Fl−1 × Fl
convolutions need to be computed, one per input-output feature pair. Stacking their weights in K
matrices Hlk ∈RFl−1×Fl, we write the l-th layer of the GNN as"
GRAPH NEURAL NETWORKS,0.04302670623145401,"Xl = ρ K−1
X"
GRAPH NEURAL NETWORKS,0.04451038575667656,"k=0
SkXl−1Hlk ! .
(3)"
GRAPH NEURAL NETWORKS,0.04599406528189911,"In an L-layer GNN, the operation in (3) is cascaded L times to obtain the GNN output Y = XL. At
the ﬁrst layer, the GNN input is given by X0 = X ∈Rn×F0. In this paper we assume F0 = FL = 1
so that Y = y and X = x. A more concise representation of this GNN can be obtained by grouping
all learnable parameters Hlk in a tensor H = {Hlk}l,k and deﬁning the map y = Φ(x; H, S). Due
to the polynomial nature of the graph convolution, the dimensions of the learnable parameter tensor
H are independent from the size of the graph (K is typically much smaller than n). Ergo, a GNN
trained on a graph Gn can be deployed on a network Gm with m ̸= n."
GRAPHON INFORMATION PROCESSING,0.04747774480712166,"3.2
GRAPHON INFORMATION PROCESSING"
GRAPHON INFORMATION PROCESSING,0.04896142433234421,"A graphon is a bounded, symmetric, and measurable function W : [0, 1]2 →[0, 1] which has two
theoretical interpretations — it is both a graph limit and a generative model for graphs. In the ﬁrst
interpretation, sequences of dense graphs converge to a graphon in the sense that the densities of
adjacency-preserving graph motifs converge to the same densities on the graphon Lov´asz (2012). In
the second, graphs can be generated from a graphon by sampling points ui, uj from the unit interval
and either assigning weight W(ui, uj) to edges (i, j), or sampling edges (i, j) with probability
W(ui, uj). In this paper, we focus on stochastic graphs Gn where the points ui are deﬁned as
ui = (i −1)/n for 1 ≤i ≤n and where the adjacency matrix Sn is sampled from W as"
GRAPHON INFORMATION PROCESSING,0.050445103857566766,"[Sn]ij ∼Bernoulli(W(ui, uj)).
(4)"
GRAPHON INFORMATION PROCESSING,0.05192878338278932,"Sequences of graphs generated in this way can be shown to converge to W with probability one
(Lov´asz, 2012)[Chapter 11]."
GRAPHON INFORMATION PROCESSING,0.05341246290801187,"In practice, the two theoretical interpretations of a graphon allow thinking of it as an identifying
object for a family of graphs of different sizes that are structurally similar. Hence, given a network
we can use its family’s identifying graphon as a continuous proxy for the graph. This is beneﬁcial
because it is typically easier to operate in continuous than in discrete domains, even more so if the
network is large. We will leverage these ideas to consider graphon data and graphon neural networks
as proxies for graph data and GNNs supported on graphs of arbitrary size."
GRAPHON INFORMATION PROCESSING,0.05489614243323442,Under review as a conference paper at ICLR 2022
GRAPHON NEURAL NETWORKS,0.05637982195845697,"3.2.1
GRAPHON NEURAL NETWORKS"
GRAPHON NEURAL NETWORKS,0.057863501483679525,"Graphon data is deﬁned as functions X ∈L2([0, 1]). Analogously to graph data, graphon data can
be diffused by application of a linear integral operator parametrized by W and deﬁned as"
GRAPHON NEURAL NETWORKS,0.05934718100890208,"TWX(v) =
Z 1"
GRAPHON NEURAL NETWORKS,0.06083086053412463,"0
W(u, v)X(u)du.
(5)"
GRAPHON NEURAL NETWORKS,0.06231454005934718,The operator TW is called graphon shift operator (WSO).
GRAPHON NEURAL NETWORKS,0.06379821958456973,"The graphon convolution is deﬁned as a weighted sum of successive applications of the WSO.
Explicitly, the graphon convolutional ﬁlter with coefﬁcients h = [h0, . . . , hK−1] is given by"
GRAPHON NEURAL NETWORKS,0.06528189910979229,"Y = h ∗W X = K−1
X"
GRAPHON NEURAL NETWORKS,0.06676557863501484,"k=0
hk(T (k)
W X)(v)
with"
GRAPHON NEURAL NETWORKS,0.06824925816023739,"(T (k)
W X)(v) =
Z 1"
GRAPHON NEURAL NETWORKS,0.06973293768545995,"0
W(u, v)(T (k−1)
W
X)(u)du (6)"
GRAPHON NEURAL NETWORKS,0.0712166172106825,"where T (0)
W
= I is the identity (Ruiz et al., 2020b). Since W is bounded and symmetric, TW
is a self-adjoint Hilbert-Schmidt operator (Lax, 2002). Hence, W can be written as W(u, v) =
P"
GRAPHON NEURAL NETWORKS,0.07270029673590504,"i∈Z\{0} λiϕi(u)ϕi(v) where λi are the graphon eigenvalues and ϕi the graphon eigenfunctions.
The eigenvalues have magnitude at most one and are ordered as −1 ≤λ−1 ≤λ−2 ≤. . . ≤0 ≤
. . . λ2 ≤λ1 ≤1. The eigenfunctions form an orthonormal basis of L2([0, 1]). Projecting the ﬁlter
(1) onto this basis, we see that the graphon convolution admits a spectral representation given by"
GRAPHON NEURAL NETWORKS,0.07418397626112759,"h(λ) = K−1
X"
GRAPHON NEURAL NETWORKS,0.07566765578635015,"k=0
hkλk.
(7)"
GRAPHON NEURAL NETWORKS,0.0771513353115727,"Like its graph counterpart, this spectral representation only depends on the eigenvalues of the
graphon."
GRAPHON NEURAL NETWORKS,0.07863501483679525,"Graphon neural networks (WNNs) are the extension of GNNs to graphon data. In the WNN, each
layer consists of a bank of graphon convolutions (6) followed by a nonlinearity ρ. Assuming that
layer l maps Fl−1 features into Fl features, the parameters of the Fl−1 × Fl convolutions (6) can be
stacked into K matrices {Hlk} ∈RFl−1×Fl. This allows writing the fth feature at layer l as"
GRAPHON NEURAL NETWORKS,0.08011869436201781,"Xf
l = ρ  "
GRAPHON NEURAL NETWORKS,0.08160237388724036,"Fl−1
X g=1 K−1
X"
GRAPHON NEURAL NETWORKS,0.0830860534124629,"k=1
(T (k)
W Xg
l−1)[Hlk]gf  
(8)"
GRAPHON NEURAL NETWORKS,0.08456973293768547,"for 1 ≤f ≤Fl. For an L-layer WNN, (8) is repeated for 1 ≤ℓ≤L. The WNN output is given by
Y f = Xf
L, and Xg
0 is given by the input data Xg for 1 ≤g ≤F0. We assume FL = F0 = 1 so that
XL = Y and X0 = X. A more succinct representation of this WNN is the map Y = Φ(X; H, W),
where the tensor H = {Hlk}l,k groups the ﬁlter coefﬁcients at all layers."
SAMPLING GNNS FROM WNNS,0.08605341246290801,"3.2.2
SAMPLING GNNS FROM WNNS"
SAMPLING GNNS FROM WNNS,0.08753709198813056,"From the representation of the GNN and the WNN as maps Φ(x; H, S) and Φ(X; H, W), we see
that these architectures can share the same ﬁlter coefﬁcients H. Since graphs can be obtained from
graphons as in (4), we can similarly use the WNN Φ(X; H, W) to sample GNNs
yn = Φ(xn; H, Sn) where [Sn]ij ∼Bernoulli(W(ui, uj))
[xn]i = X(ui)
(9)"
SAMPLING GNNS FROM WNNS,0.08902077151335312,"i.e., the WNN can be seen as a generative model for GNNs Φ(xn; H, Sn)."
SAMPLING GNNS FROM WNNS,0.09050445103857567,"Conversely, a WNN ca be induced by a GNN. Given a GNN yn = Φ(xn; H, Sn), the WNN induced
by this GNN is deﬁned as"
SAMPLING GNNS FROM WNNS,0.09198813056379822,"Yn = Φ(Xn; H, Wn) where Wn(u, v) = n
X i=1 n
X"
SAMPLING GNNS FROM WNNS,0.09347181008902077,"j=1
[Sn]ijI(u ∈Ii)I(v ∈Ij)"
SAMPLING GNNS FROM WNNS,0.09495548961424333,"Xn(u) = n
X"
SAMPLING GNNS FROM WNNS,0.09643916913946587,"i=1
[xn]iI(u ∈Ii) (10)"
SAMPLING GNNS FROM WNNS,0.09792284866468842,Under review as a conference paper at ICLR 2022
SAMPLING GNNS FROM WNNS,0.09940652818991098,"where I denotes the indicator function and the intervals Ii are deﬁned as Ii = [(i −1)/n, i/n) for
1 ≤i ≤n −1 and In = [(n −1)/n, 1]. The graphon Wn is called the graphon induced by the
graph Gn and Xn and Yn are called the graphon signals induced by the graph signals xn and yn."
GRAPHON EMPIRICAL LEARNING,0.10089020771513353,"4
GRAPHON EMPIRICAL LEARNING"
GRAPHON EMPIRICAL LEARNING,0.10237388724035608,"On graphons, the statistical loss minimization (or statistical learning) problem is given by"
GRAPHON EMPIRICAL LEARNING,0.10385756676557864,"minimize
H
Ep(Y,X)[ℓ(Y, Φ(X; H, W))]
(11)"
GRAPHON EMPIRICAL LEARNING,0.10534124629080119,"where p(Y, X) is the joint distribution of the data, ℓis an instantaneous loss function and
Φ(X; H, W) is a function parametrized by the graphon W and by a set of learnable weights H.
In this paper, we consider positive loss functions ℓ: R × R →R+. The function Φ is parametrized
as a graphon neural network [cf. (8)]. Because the joint probability distribution p(Y, X) is unknown,
we are unable to derive a closed-form solution of (11), but this problem can be approximated by the
empirical risk minimization (ERM) problem over graphons."
GRAPHON EMPIRICAL RISK MINIMIZATION,0.10682492581602374,"4.1
GRAPHON EMPIRICAL RISK MINIMIZATION"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.1083086053412463,"Suppose that we have access to samples of the distribution D = {(Xj, Y j) ∼p(X, Y ), j =
1, . . . , |D|}. Provided that these samples are obtained independently and that |D| is large enough,
the statistical loss in (11) can be approximated as"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.10979228486646884,"minimize
H |D|
X"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.11127596439169139,"j=1
ℓ(Y j, Φ(Xj; H, W))
(12)"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.11275964391691394,"giving way to the ERM problem (Hastie et al., 2009; Shalev-Shwartz & Ben-David, 2014; Vapnik,
1999; Kearns et al., 1994). To solve this problem using local information, we could adopt some
ﬂavor of gradient descent (Goodfellow et al., 2016). The learning iteration at step k is then"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.1142433234421365,"Hk+1 = Hk −ηk∇Hℓ(Y j, Φ(Xj; Hk, W))
(13)"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.11572700296735905,"where k = 1, 2, . . . denotes the current iteration and ηk ∈(0, 1) the step size at iteration k."
GRAPHON EMPIRICAL RISK MINIMIZATION,0.1172106824925816,"In practice, the gradients on the right hand side of (13) cannot be computed because, being a theoret-
ical limit object, the graphon W is unknown. However, we can leverage the fact that the graphon is a
random graph model to approximate the gradients ∇Hℓ(Y j, Φ(Xj; H, W)) by sampling stochastic
graphs Gn with GSO Sn [cf. (4)] and calculating ∇Hℓ(yj
n, Φ(xj
n; H, Sn)), where xj
n and yj
n are
as in (9). In this case, the graphon empirical learning step in (13) becomes"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.11869436201780416,"Hk+1 = Hk −ηk∇Hℓ(yj
n, Φ(xj
n; Hk, Sn))
(14)"
GRAPHON EMPIRICAL RISK MINIMIZATION,0.1201780415430267,"and we have to derive an upper bound for the expected error made when using the gradient calculated
on the graph to approximate the gradient on the graphon. In what follows, we give a closed-form
expression of this bound, and use it as a stepping stone to develop an Algorithm that increases n,
the graph size, over regular intervals during the training process of the GNN. We then prove that our
Algorithm converges to a neighborhood of the optimal solution for the WNN."
GRADIENT APPROXIMATION,0.12166172106824925,"4.2
GRADIENT APPROXIMATION"
GRADIENT APPROXIMATION,0.12314540059347182,"To state our ﬁrst convergence result, we need following deﬁnition.
Deﬁnition 1 (Lipschitz functions). A function f(u1, u2, . . . , ud) is A-Lipschitz on the variables
u1, . . . , ud if it satisﬁes |f(v1, v2, . . . , vd) −f(u1, u2, . . . , ud)| ≤A Pd
i=1 |vi −ui|. If A = 1, we
say that this function is normalized Lipschitz."
GRADIENT APPROXIMATION,0.12462908011869436,"We also need the following Lipschitz continuity assumptions (AS1–AS4), as well as an assumption
on the size of the graph Sn (AS5).
AS1. The graphon W and the graphon signals X and Y are normalized Lipschitz.
AS2. The convolutional ﬁlters h are normalized Lipschitz and non-amplifying, i.e., ∥h(λ)∥< 1."
GRADIENT APPROXIMATION,0.1261127596439169,Under review as a conference paper at ICLR 2022
GRADIENT APPROXIMATION,0.12759643916913946,Algorithm 1 Increase and Conquer: Growing Graph Training
GRADIENT APPROXIMATION,0.129080118694362,"1: Initialize H0, n0 and sample graph Gn0 from graphon W
2: repeat for epochs 0, 1, . . .
3:
for k =1,..., |D| do
4:
Obtain sample (Y, X) ∼D
5:
Construct graph signal yn, xn [cf. (9)]
6:
Take learning step Hk+1 = Hk −ηk∇ℓ(yn, Φ(xn; Hk, Sn))
7:
end for
8:
Increase number of nodes n and sample Sn Bernoulli from graphon W [cf. (4)]
9: until convergence"
GRADIENT APPROXIMATION,0.13056379821958458,"AS3. The activation functions and their gradients are normalized Lipschitz, and ρ(0) = 0."
GRADIENT APPROXIMATION,0.13204747774480713,"AS4. The loss function ℓ: R×R →R+ and its gradient are normalized Lipschitz, and ℓ(x, x) = 0."
GRADIENT APPROXIMATION,0.13353115727002968,"AS5. For a ﬁxed value of ξ ∈(0, 1), n is such that n −log(2n/ξ)/dW > 2/dW where dW denotes
the maximum degree of the graphon W, i.e., dW = maxv
R 1
0 W(u, v)du."
GRADIENT APPROXIMATION,0.13501483679525222,"AS1–AS4 are normalized Lipschitz smoothness conditions which can be relaxed by making the
Lipschitz constant greater than one. AS3 holds for most typical activation functions. AS4 can be
achieved by normalization and holds for most loss functions in a closed set (e.g., the hinge or mean
square losses). AS5 is necessary to guarantee a O(
p"
GRADIENT APPROXIMATION,0.13649851632047477,"log n/n) rate of convergence of Sn to W
(Chung & Radcliffe, 2011)."
GRADIENT APPROXIMATION,0.13798219584569732,"Under AS1–AS5, the following theorem shows that the expected norm of the difference between the
graphon and graph gradients in (13) and (14) is bounded. The proof is deferred to the appendices."
GRADIENT APPROXIMATION,0.1394658753709199,"Theorem 1. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1] and assume that the graphon
convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Under assumptions AS1–AS5, it holds that"
GRADIENT APPROXIMATION,0.14094955489614244,"E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥] ≤γc + O
r"
GRADIENT APPROXIMATION,0.142433234421365,log(n3/2) n
GRADIENT APPROXIMATION,0.14391691394658754,"
(15)"
GRADIENT APPROXIMATION,0.14540059347181009,"where Yn is the graphon signal induced by [yn]i = Y (ui) [cf. (10)], and γ is a constant that
depends on the number of layers L, features F, and ﬁlter taps K of the GNN [cf. Deﬁnition 7 in the
Appendix]."
GRADIENT APPROXIMATION,0.14688427299703263,"The bound in Theorem 1 quantiﬁes the maximum distance between the learning steps on the graph
and on the graphon as a function of the size of the graph. This bound is controlled by two terms. On
the one hand, we have the approximation bound, a term that decreases with n. The approximation
bound is related to the approximation of W by Sn. On the other, we have the nontransferable
bound, which is constant and controlled by c. This term is related to a threshold that we impose on
the convolutional ﬁlter h. Since graphons W have an inﬁnite spectrum that accumulates around zero,
convergence of all spectral components of the ﬁltered data on Wn can only be shown in the limit of n
(Ruiz et al., 2020a). Hence, given that n is ﬁnite we can only upper bound distances between spectral
components associated with eigenvalues larger than some c ∈(0, 1] that we ﬁx. Meanwhile, the
perturbations associated with the other spectral components are controlled by bounding the variation
of the ﬁlter response below c."
GRADIENT APPROXIMATION,0.14836795252225518,"The bound in Theorem 1 is important because it allows us to quantify the error incurred by taking
gradient steps not on the graphon, but on the graph data. Although we cannot take gradients on the
function that we want to learn [cf. (13)], by measuring the ratio between the norm of the gradient
of the loss on the GNN, and the difference between the two gradients, we can expect to follow the
direction of the gradient on the WNN. This is instrumental for learning a meaningful solution of the
graphon ERM problem (12). Nonetheless, we are only able to decrease this approximation bound
down to the value of the nontransferable bound."
GRADIENT APPROXIMATION,0.14985163204747776,Under review as a conference paper at ICLR 2022
ALGORITHM CONSTRUCTION,0.1513353115727003,"4.3
ALGORITHM CONSTRUCTION"
ALGORITHM CONSTRUCTION,0.15281899109792285,"Since the discretization error depends on the number of nodes of the graph, we can iteratively in-
crease the graph size at every epoch. Even if a bias is introduced in the gradient we will be able
to follow the gradient direction of the graphon learning problem (13). At the same time, we need
to keep the computational cost of the iterates under control. Note that the norm of the gradient is
larger at ﬁrst, but it decreases as we approach the optimal solution. Exploiting this behavior, we may
progressively reduce the bias term as iterations increase. The idea here is to keep the discretization
of the gradient small so as to closely follow the gradient direction of the graphon learning problem,
but without being too conservative as this would incur in high computational cost."
ALGORITHM CONSTRUCTION,0.1543026706231454,"In practice, in the ERM problem the number of nodes that can be added is upper bounded by the
available data, which is deﬁned nodewise on the graph. Hence, we can arbitrarily decide which and
how many nodes to consider for training. The novelty of Algorithm 1 is that we do not use the largest
graph available in the dataset at every epoch to train the GNN. Instead, we set a minimum graph size
n0 and progressively increase it up to the total number of nodes. The main advantage of Algorithm
1 is thus that it allows reducing the computational cost of training without compromising GNN
performance. In what follows, we will provide the conditions under which Algorithm 1 converges
to a neighborhood of the optimal solution on the graphon."
ALGORITHM CONVERGENCE,0.15578635014836795,"4.4
ALGORITHM CONVERGENCE"
ALGORITHM CONVERGENCE,0.1572700296735905,"We have shown that the learning step on the graphon and on the graph are close. Now, it remains
to show the practical implications of Theorem 1 for obtaining the solution of the graphon ERM
problem (12). If the expected difference between the gradients is small, the iterations generated by
(14) will be able to follow the direction of the true gradient on the graphon learning problem. But
because the distance between gradients is inversely proportional to n, we need to strike a balance
between obtaining a good approximation and minimizing the computational cost. In Theorem 2,
we show that Algorithm 1 converges if the rate at which the graph grows is chosen to satisfy the
condition given in (16).
AS6. The graphon neural network Φ(X; H, W) is AΦ-Lipschitz, and its gradient ∇HΦ(X; H, W)
is A∇Φ-Lipschitz, with respect to the parameters H [cf. Deﬁnition 1].
Theorem 2. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1], ϵ ∈(0, 1 −A∇Φη), and assume that
the graphon convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn)
be a GNN sampled from Φ(X; H, W) as in (9). Consider the iterates generated by equation (16).
Under Assumptions AS1-AS6, if at each step k the number of nodes n veriﬁes"
ALGORITHM CONVERGENCE,0.15875370919881307,"γc + O
r"
ALGORITHM CONVERGENCE,0.16023738872403562,log(n3/2) n
ALGORITHM CONVERGENCE,0.16172106824925817,"
< 1 −A∇ℓη −ϵ"
ALGORITHM CONVERGENCE,0.1632047477744807,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥
(16)"
ALGORITHM CONVERGENCE,0.16468842729970326,then in ﬁnite time we will achieve an iterate k∗such that the coefﬁcients Hk∗satisfy
ALGORITHM CONVERGENCE,0.1661721068249258,"E[∥∇Hℓ(Y, Φ(X; Hk∗, W))∥] ≤2γc
after at most k∗= O(1/ϵ)
(17)"
ALGORITHM CONVERGENCE,0.16765578635014836,"where γ is a constant that depends on the number of layers L, features F, and ﬁlter taps K of the
GNN [cf. Deﬁnition 7 in the Appendix]."
ALGORITHM CONVERGENCE,0.16913946587537093,"Theorem 2 presents the conditions under which Algorithm 1 converges. Intuitively, the condition in
(16) implies that the rate of decrease in the norm of the gradient of the loss function computed on the
graph neural network needs to be slower than roughly n−1/2. If the norm of the gradient does not
vary between iterations, the number of nodes does not need to be increased. Note that (115) is equal
to twice the bias term obtained in equation (15). We can keep increasing the number of nodes — and
thus decreasing the bias — until the norm of the GNN gradient is smaller than the nontransferable
constant value. Once this point is attained, there is no gain in decreasing the approximation bound
any further (Ajalloeian & Stich, 2020). Recall that the constant term can be made as small as desired
by tuning c, at the cost of decreasing the approximation bound. To conclude, observe that assuming
smoothness of the GNN is a mild assumption (Scaman & Virmaux, 2018; Jordan & Dimakis, 2020;
Latorre et al., 2020; Fazlyab et al., 2019; Tanielian & Biau, 2021; Du et al., 2019). A characterization
of the Lipschitz constant is an interesting research question but is out of the scope of this work."
ALGORITHM CONVERGENCE,0.17062314540059348,Under review as a conference paper at ICLR 2022
NUMERICAL RESULTS,0.17210682492581603,"5
NUMERICAL RESULTS"
RECOMMENDATION SYSTEM,0.17359050445103857,"5.1
RECOMMENDATION SYSTEM"
RECOMMENDATION SYSTEM,0.17507418397626112,"We consider a social recommendation problem given by movie ratings, for which we use the Movie-
Lens 100k dataset Harper & Konstan (2015). The dataset contains 100, 000 integer ratings between
1 and 5, that were collected between U = 943 users and M = 1682 movies. We consider the prob-
lem of predicting the rating that different users would give to the movie “Contact”. To exploit the
social connectivity of the problem, we build the movie similarity graph, by computing the pairwise
correlations between the different movies in the training set Huang et al. (2018). Further details
about training splits, and hyperparameter selection can be found in the supplementary material."
RECOMMENDATION SYSTEM,0.17655786350148367,"For the experiment, we started the GNNs with 200, and at 300 nodes and added {25, 50, 75, 100}
nodes per epochs. The total number of epochs of the GNN trained on 1000 nodes with which
we compare, is given by the maximum number of epochs that the Algorithm 1 can be run adding
25 nodes per epoch. In Figure 1, we are able to see the empirical manifestation of the beneﬁt of
Algorithm 1. Namely, regardless of the number of nodes added per epoch, in all four cases, using
relative RMSE as a ﬁgure of merit, we achieve a comparable performance to that of a GNNs trained
on 1000 nodes for a larger number of epochs. This reinforces the fact that training on a growing
number of graphs can attain similar performance, while requiring a lighter computational cost."
DECENTRALIZED CONTROL,0.17804154302670624,"5.2
DECENTRALIZED CONTROL"
DECENTRALIZED CONTROL,0.1795252225519288,"In this section we consider the problem of coordinating a set of n agents initially ﬂying at random to
avoid collisions, and to ﬂy at the same velocity. Also known as ﬂocking, at each time t agent i knows
its own position ri(t) ∈R2, and speed vi(t) ∈R2, and reciprocally exchanges it with its neighboring
nodes if a communication link exists between them. Links are govern by physical proximity between
agents forming a time varying graph Gn = (V, E). A communication link exists if the distance
between two agents i, j satisﬁes rij(t) = ∥ri(t) −rj(t)∥≤R = 2m. We assume that at each
time t the controller sets an acceleration ui ∈[−10, 10]2, and that it remains constant for a time
interval Ts = 20ms. The system dynamics are govern by, ri(t + 1) = ui(t)T 2
s /2 + vi(t)Ts + ri(t),
vi(t + 1) = ui(t)Ts + vi(t). To avoid the swarm of robots to reach a null common velocity, we
initialize the velocities at random v(t) = [v1(t), . . . , vn(t)], by uniformly sampling a common
bias velocity vBIAS ∼U[−3, 3], and then adding independent uniform noise U[−3, 3] to each
agent. The initial deployment is randomly selected in a circle always verifying that the minimum
distance between two agents is larger than 0.1m. On the one hand, agents pursue a common average
velocity ¯v = (1/n) Pn
i=1 vi(t), thus minimizing the velocity variation of the team. On the other"
DECENTRALIZED CONTROL,0.18100890207715134,"1
2
3
4
5
6
7
8
9
10
11
12
13
Number of Epochs 102 60.0 70.0 80.0 90.0 200.0"
DECENTRALIZED CONTROL,0.1824925816023739,Relative RMSE (%)
DECENTRALIZED CONTROL,0.18397626112759644,25 Nodes Added (Starting at 200 Nodes)
DECENTRALIZED CONTROL,0.18545994065281898,50 Nodes Added (Starting at 200 Nodes)
DECENTRALIZED CONTROL,0.18694362017804153,75 Nodes Added (Starting at 200 Nodes)
DECENTRALIZED CONTROL,0.1884272997032641,100 Nodes Added (Starting at 200 Nodes) (a)
DECENTRALIZED CONTROL,0.18991097922848665,"1
2
3
4
5
6
Number of Epochs 102 60.0 70.0 80.0 90.0 200.0"
DECENTRALIZED CONTROL,0.1913946587537092,Relative RMSE (%)
DECENTRALIZED CONTROL,0.19287833827893175,25 Nodes Added (Starting at 300 Nodes)
DECENTRALIZED CONTROL,0.1943620178041543,50 Nodes Added (Starting at 300 Nodes)
DECENTRALIZED CONTROL,0.19584569732937684,75 Nodes Added (Starting at 300 Nodes)
DECENTRALIZED CONTROL,0.19732937685459942,100 Nodes Added (Starting at 300 Nodes) (b)
DECENTRALIZED CONTROL,0.19881305637982197,"Figure 1: Relative RMSE of a recommendation system trained on MovieLens 100k for 20 independent par-
titions measured over the test set (a) starting with GNNs of 200 nodes, compared to a GNN trained on 1000
nodes for 37 epochs (b) starting with GNNs of 300 nodes, compared to a GNN trained on 1000 nodes for 29
epochs."
DECENTRALIZED CONTROL,0.20029673590504452,Under review as a conference paper at ICLR 2022
DECENTRALIZED CONTROL,0.20178041543026706,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Number of Epochs 101 2.0 3.0 4.0 5.0"
DECENTRALIZED CONTROL,0.2032640949554896,"6.0
7.0
8.0
9.0 20.0"
DECENTRALIZED CONTROL,0.20474777448071216,Cost Relative to Centralized Controller
NODES PER EPOCH,0.2062314540059347,"2 nodes per epoch
5 nodes per epoch
10 nodes per epoch
Trained on 100 nodes (10 epochs)
Trained on 100 nodes (30 epochs) (a)"
NODES PER EPOCH,0.20771513353115728,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Number of Epochs 101 2.0 3.0 4.0 5.0"
NODES PER EPOCH,0.20919881305637983,"6.0
7.0
8.0
9.0 20.0"
NODES PER EPOCH,0.21068249258160238,Cost Relative to Centralized Controller
NODES PER EPOCH,0.21216617210682492,"2 nodes per epoch
5 nodes per epoch
10 nodes per epoch
Trained on 100 nodes (9 epochs)
Trained on 100 nodes (30 epochs) (b)"
NODES PER EPOCH,0.21364985163204747,"Figure 2: Velocity variation of the ﬂocking problem for the whole trajectory in the testing set relative to the
centralized controller (a) starting with 10 nodes and (b) starting with 20 nodes."
NODES PER EPOCH,0.21513353115727002,"hand, agents are required to avoid collision. We can thus deﬁne the velocity variation of the team
σv(t) = Pn
i=1 ∥vi(t) −¯v(t)∥2, and the collision avoidance potential"
NODES PER EPOCH,0.2166172106824926,"CA(ri, rj) =
1/∥ri(t) −rj(t)∥2 −log(∥ri(t) −rj(t)∥2)
if ∥ri(t) −rj(t)(t)∥≤RCA
1/R2
CA −log(R2
CA)
otherwise,
(18)"
NODES PER EPOCH,0.21810089020771514,"with RCA
=
1m.
A centralized controller can be obtain by ui(t)∗
=
−n(vi −¯v) +
Pn
j=1 ∇riCA(ri, rj) (Tanner et al., 2003)."
NODES PER EPOCH,0.2195845697329377,"Exploiting the fact that neural networks are universal approximators (Barron, 1993; Hornik, 1991),
Imitation Learning can be utilized as a framework to train neural networks from information pro-
vided by an expert (Ross et al., 2011; Ross & Bagnell, 2010). Formally, we have a set of pairs T =
{xm, u∗
m}, m = 1, . . . , M, and during training we minimize the mean square error between the
optimal centralized controller, and the output of our GNN ∥u∗
m −Φ(xm; H, S)∥2. Denoting Ni(t)
the neighborhood of agent i at time t, the state of the agents x(t) = [x(t)1, . . . , x(t)n], xi(t) ∈R6,
is given by xi(t) = P"
NODES PER EPOCH,0.22106824925816024,"j:j∈Ni(t)[vi(t) −vj(t), rij(t)/∥rij(t)∥4, rij(t)/∥rij(t)∥2]. Note that state
xi(t), gets transmitted between agents if a communication link exists between them."
NODES PER EPOCH,0.22255192878338279,"In Figure 2 we can see the empirical manifestation of the claims we put forward. First and foremost,
we are able to learn a GNN that achieves a comparable performance while taking steps on a smaller
graphs. As seen in Figure 2, GNNs trained with n0 = {10, 20} agents in the ﬁrst epoch and adding
10 agents per epoch (green line) are able to achieve a similar performance when reaching 100 agents
than the one they would have achieved by training with 100 agents the same number of epochs.
Besides, if we add less nodes per epoch, we are able to achieve a similar performance that we would
have achieved by training on the large network for 30 epochs."
CONCLUSIONS,0.22403560830860533,"6
CONCLUSIONS"
CONCLUSIONS,0.22551928783382788,"We have introduced a learning procedure for GNNs that progressively grows the size of the graph
while training. Our algorithm requires less computational cost — as the number of nodes in the
graph convolution is smaller — than training on the full graph without compromising performance.
Leveraging transferability results, we bounded the expected difference between the gradient on the
GNN, and the gradient on the WNN. Utilizing this result, we provided the theoretical guarantees
that our Algorithm converges to a neighborhood of a ﬁrst order stationary point of the WNN in ﬁnite
time. We benchmarked our algorithm on a recommendation system and a decentralized control
problem, achieving comparable performance to the one achieve by a GNN trained on the full graph."
CONCLUSIONS,0.22700296735905046,Under review as a conference paper at ICLR 2022
REFERENCES,0.228486646884273,REFERENCES
REFERENCES,0.22997032640949555,"Ahmad Ajalloeian and Sebastian U Stich. Analysis of sgd with biased gradient estimators. arXiv
preprint arXiv:2008.00051, 2020."
REFERENCES,0.2314540059347181,"Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930–945, 1993."
REFERENCES,0.23293768545994065,"Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors.
SIAM Journal on Optimization, 10(3):627–642, 2000."
REFERENCES,0.2344213649851632,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013."
REFERENCES,0.23590504451038577,"Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R´e, and Kevin Murphy.
Machine
learning on graphs: A model and comprehensive taxonomy, 2021."
REFERENCES,0.23738872403560832,"Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. arXiv preprint arXiv:1905.12560,
2019."
REFERENCES,0.23887240356083086,"Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna.
Can graph neural networks count
substructures? arXiv preprint arXiv:2002.04025, 2020."
REFERENCES,0.2403560830860534,"Fan Chung and Mary Radcliffe. On the spectra of general random graphs. the electronic journal of
combinatorics, pp. P215–P215, 2011."
REFERENCES,0.24183976261127596,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran
Associates, Inc., 2016."
REFERENCES,0.2433234421364985,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685. PMLR, 2019."
REFERENCES,0.24480712166172106,"Rick Durrett. Probability: Theory and Examples. Cambridge University Press, 2019."
REFERENCES,0.24629080118694363,"David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
ﬁngerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015."
REFERENCES,0.24777448071216618,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417–426, 2019."
REFERENCES,0.24925816023738873,"Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efﬁcient
and accurate estimation of lipschitz constants for deep neural networks. Advances in Neural
Information Processing Systems, 32:11427–11438, 2019."
REFERENCES,0.2507418397626113,"Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017."
REFERENCES,0.2522255192878338,"Fernando Gama and Somayeh Sojoudi. Distributed linear-quadratic control with graph neural net-
works. arXiv preprint arXiv:2103.08417, 2021."
REFERENCES,0.25370919881305637,"Fernando Gama, Antonio G Marques, Geert Leus, and Alejandro Ribeiro. Convolutional neural
network architectures for signals supported on graphs. IEEE Transactions on Signal Processing,
67(4):1034–1049, 2018."
REFERENCES,0.2551928783382789,"Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.
IEEE Transactions on Signal Processing, 68:5680–5695, 2020."
REFERENCES,0.25667655786350146,Under review as a conference paper at ICLR 2022
REFERENCES,0.258160237388724,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263–1272.
PMLR, 06–11 Aug 2017."
REFERENCES,0.2596439169139466,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016."
REFERENCES,0.26112759643916916,"M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. Proceedings.
2005 IEEE International Joint Conference on Neural Networks, 2005., 2:729–734 vol. 2, 2005."
REFERENCES,0.2626112759643917,"F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015."
REFERENCES,0.26409495548961426,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009."
REFERENCES,0.2655786350148368,"Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251–257, 1991."
REFERENCES,0.26706231454005935,"Weiyu Huang, Antonio G Marques, and Alejandro R Ribeiro. Rating prediction via graph signal
processing. IEEE Transactions on Signal Processing, 66(19):5066–5081, 2018."
REFERENCES,0.2685459940652819,"Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. arXiv preprint arXiv:2003.01219, 2020."
REFERENCES,0.27002967359050445,"Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. An introduction to computational
learning theory. 1994."
REFERENCES,0.271513353115727,"Nicolas Keriven and Gabriel Peyr´e. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.27299703264094954,"Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graph convolu-
tional networks on large random graphs. arXiv preprint arXiv:2006.01868, 2020."
REFERENCES,0.2744807121661721,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
CoRR,
abs/1412.6980, 2015."
REFERENCES,0.27596439169139464,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.2774480712166172,"Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks
via sparse polynomial optimization. arXiv preprint arXiv:2004.08688, 2020."
REFERENCES,0.2789317507418398,"P. D. Lax. Functional Analysis. Wiley, 2002."
REFERENCES,0.28041543026706234,"Ron Levie, Wei Huang, Lorenzo Bucci, Michael M Bronstein, and Gitta Kutyniok. Transferability
of spectral graph convolutional neural networks. arXiv preprint arXiv:1907.12972, 2019."
REFERENCES,0.2818991097922849,"Qingbiao Li, Fernando Gama, Alejandro Ribeiro, and Amanda Prorok. Graph neural networks for
decentralized multi-robot path planning. arXiv preprint arXiv:1912.06095, 2019."
REFERENCES,0.28338278931750743,"Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015."
REFERENCES,0.28486646884273,"L´aszl´o Lov´asz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012."
REFERENCES,0.28635014836795253,"Q. Lu and L. Getoor. Link-based classiﬁcation. In ICML 2003, 2003."
REFERENCES,0.2878338278931751,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks, 2019."
REFERENCES,0.2893175074183976,"Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu. Learning human-
object interactions by graph parsing neural networks. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 401–417, 2018."
REFERENCES,0.29080118694362017,Under review as a conference paper at ICLR 2022
REFERENCES,0.2922848664688427,"Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In International
conference on machine learning, pp. 5241–5250. PMLR, 2019."
REFERENCES,0.29376854599406527,"St´ephane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiﬁcial intelligence and statistics, pp. 661–668. JMLR
Workshop and Conference Proceedings, 2010."
REFERENCES,0.2952522255192878,"St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artiﬁcial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.29673590504451036,"Luana Ruiz, Fernando Gama, Antonio Garc´ıa Marques, and Alejandro Ribeiro.
Invariance-
preserving localized activation functions for graph neural networks. IEEE Transactions on Signal
Processing, 68:127–141, 2019a."
REFERENCES,0.29821958456973297,"Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Gated graph convolutional recurrent neural
networks. In 2019 27th European Signal Processing Conference (EUSIPCO), pp. 1–5. IEEE,
2019b."
REFERENCES,0.2997032640949555,"Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability
of graph neural networks. Advances in Neural Information Processing Systems, 33, 2020a."
REFERENCES,0.30118694362017806,"Luana Ruiz, Luiz FO Chamon, and Alejandro Ribeiro.
The graphon fourier transform.
In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 5660–5664. IEEE, 2020b."
REFERENCES,0.3026706231454006,"Luana Ruiz, Fernando Gama, Antonio Garc´ıa Marques, and Alejandro Ribeiro.
Invariance-
preserving localized activation functions for graph neural networks. IEEE Transactions on Signal
Processing, 68:127–141, 2020c. doi: 10.1109/TSP.2019.2955832."
REFERENCES,0.30415430267062316,"Luana Ruiz, Zhiyang Wang, and Alejandro Ribeiro. Graph and graphon neural network stability.
arXiv preprint arXiv:2010.12529, 2020d."
REFERENCES,0.3056379821958457,"Kevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and
efﬁcient estimation. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 3839–3848, 2018."
REFERENCES,0.30712166172106825,"Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik–chervonenkis dimension
of graph and recursive neural networks. Neural Networks, 108:248–259, 2018. ISSN 0893-6080."
REFERENCES,0.3086053412462908,"Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. In The Semantic Web - 15th International Conference, ESWC 2018, Proceedings, pp.
593–607. Springer/Verlag, 2018."
REFERENCES,0.31008902077151335,"Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014."
REFERENCES,0.3115727002967359,"Gilbert Strang. Linear Algebra and Its Applications. New York: Academic Press, 1976."
REFERENCES,0.31305637982195844,"Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. Learning to hash
with graph neural networks for recommender systems. In Proceedings of The Web Conference
2020, pp. 1988–1998, 2020."
REFERENCES,0.314540059347181,"Ugo Tanielian and Gerard Biau. Approximating lipschitz continuous functions with groupsort neu-
ral networks. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 442–450.
PMLR, 2021."
REFERENCES,0.31602373887240354,"Herbert G Tanner, Ali Jadbabaie, and George J Pappas. Stable ﬂocking of mobile agents part i:
dynamic topology. In 42nd IEEE International Conference on Decision and Control (IEEE Cat.
No. 03CH37475), volume 2, pp. 2016–2021. IEEE, 2003."
REFERENCES,0.31750741839762614,"Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media,
1999."
REFERENCES,0.3189910979228487,Under review as a conference paper at ICLR 2022
REFERENCES,0.32047477744807124,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 32(1):4–24, 2021."
REFERENCES,0.3219584569732938,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–
983, 2018."
REFERENCES,0.32344213649851633,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-
tions. AI Open, 1:57–81, 2020. ISSN 2666-6510."
REFERENCES,0.3249258160237389,Under review as a conference paper at ICLR 2022
REFERENCES,0.3264094955489614,"A
APPENDIX"
REFERENCES,0.327893175074184,"B
NUMERICAL RESULTS PARAMETERS"
REFERENCES,0.3293768545994065,"All experiments were done on a computer with 32gb of RAM, a CPU Intel Core i9-9900K
@3.60GHz x 16, a GPU GeForce RTX 2080 Ti/PCIe/SSE2, and Ubuntu 18.04.4 LTS."
REFERENCES,0.33086053412462907,"B.1
RECOMMENDATION SYSTEM"
REFERENCES,0.3323442136498516,"We split the dataset with 90% for the training set, and 10% for the testing set, and we run 20
independent random partitions. For the optimizer, we used 5 samples for the batch size, and ADAM
algorithm Kingma & Ba (2015), with learning rate 0.005, β1 = 0.9, β2 = 0.999, and without
learning rate decay. For the loss, we used the smooth L1 loss. For the GNN, we used ReLU as
non-linearity, we considered F = 32 features, K = 5 ﬁlter taps, and L = 1 layers."
REFERENCES,0.33382789317507416,"We used the graph neural networks library available online at https://github.com/alelab-upenn/graph-
neural-networks/blob/master/examples/movieGNN.py and implemented with PyTorch."
REFERENCES,0.3353115727002967,"B.2
DECENTRALIZED CONTROL"
REFERENCES,0.3367952522255193,"We run the system for T = 2s, and used 400 samples for training, 20 for validation, and 20 for the
test set. For the optimizer, we used 20 samples for the batch size, and ADAM algorithm Kingma &
Ba (2015) with learning rate 0.0005, β1 = 0.9, β2 = 0.999, without learning rate decay. We used
a one layer Graph Neural Networks with F = 64 hidden units and K = 3 ﬁlter taps, and used the
hyperbolic tangent as non-linearity ρ. We run 10 independent realizations of each experiment."
REFERENCES,0.33827893175074186,"We used the graph neural networks library available online at https://github.com/alelab-upenn/graph-
neural-networks/blob/master/examples/ﬂockingGNN.py and implemented with PyTorch."
REFERENCES,0.3397626112759644,Under review as a conference paper at ICLR 2022
REFERENCES,0.34124629080118696,"C
PROOF OF THEOREM 1"
REFERENCES,0.3427299703264095,"Deﬁnition 2 (Template graphs). Let {ui}n
i=1 be the regular n-partition of [0, 1], i.e.,"
REFERENCES,0.34421364985163205,ui = i −1
REFERENCES,0.3456973293768546,"n
(19)"
REFERENCES,0.34718100890207715,"for 1 ≤i ≤n. The n-node template graph Gn, whose GSO we denote Sn, is obtained from W as"
REFERENCES,0.3486646884272997,"[Sn]ij = W(ui, uj)
(20)"
REFERENCES,0.35014836795252224,"for 1 ≤i, j ≤n."
REFERENCES,0.3516320474777448,"Deﬁnition 3 (Graphon spectral representation of convolutional ﬁlter response). As the graphon
W is bounded and symmetric, TW is a self adjoint Hilbert-Schmidt operator, which allows to use
the operator’s spectral basis W(u, v) = P"
REFERENCES,0.35311572700296734,"i∈Z{0} λiψi(u)ψi(v). Eigenvalues λi are ordered in
decreasing order of absolute value i.e., 1 ≥λ1 ≥λ2 ≥· · · ≥0 ≥· · · ≥λ−2 ≥λ−1 ≥−1, and
their only accumulation point is 0 (Lax, 2002, Theorem 3, Chapter 28). Thus, we deﬁne the spectral
representation of the convolutional ﬁlter TH (cf. (6)) as,"
REFERENCES,0.3545994065281899,"h(λ) = K−1
X"
REFERENCES,0.3560830860534125,"k=0
hkλk
(21)"
REFERENCES,0.35756676557863504,"Deﬁnition 4 (c-band cardinality of W). The c-band cardinality, denoted Bc
W, is the number of
eigenvalues whose absolute value is larger than c."
REFERENCES,0.3590504451038576,"Bc
W = #{λi : ∥λi∥≤c}
(22)"
REFERENCES,0.36053412462908013,"Deﬁnition 5 (c-eigenvalue margin of W - Wn). The c-eigenvalue margin of W - Wn is deﬁned as
the minimum distance between two different eigenvalues of the integral operator applied to W, and
to Wn as follows,"
REFERENCES,0.3620178041543027,"δc
WWn = min
i,j̸=i{∥λi(TW) −λi(TWn)∥: ∥λi(TWn)∥≥c}
(23)"
REFERENCES,0.36350148367952523,"Deﬁnition 6 (Graphon Convolutional Filter). Given a graphon W, a graphon signal X, and ﬁlter
coefﬁcients h = [h0, . . . , hK−1] the graphon ﬁlter TH : L2([0, 1]) →L2([0, 1]) is deﬁned as,"
REFERENCES,0.3649851632047478,"(THX)(v) = K−1
X"
REFERENCES,0.3664688427299703,"k=0
hk(T (k)
W X)(v).
(24)"
REFERENCES,0.36795252225519287,"Proposition 1. Let X ∈L2([0, 1]) be a normalized Lipschitz graphon signal, and let Xn be the
graphon signal induced by the graph signal xn obtained from X on the template graph Gn [cf.
Deﬁnition 2], i.e., [xn]i = X((i −1)/n) for 1 ≤i ≤n. It holds that"
REFERENCES,0.3694362017804154,∥X −Xn∥L2 ≤1
REFERENCES,0.37091988130563797,"n.
(25)"
REFERENCES,0.3724035608308605,"Proof. Let Ii = [(i −1)/n, i/n) for 1 ≤i ≤n −1 and In = [(n −1)/n, 1]. Since the graphon is
normalized Lipschitz, for any u ∈Ii, 1 ≤i ≤n, we have"
REFERENCES,0.37388724035608306,"∥X(u) −Xn(u)∥≤max
u −i −1 n"
REFERENCES,0.37537091988130566,",

i
n −u 
≤1"
REFERENCES,0.3768545994065282,"n.
(26)"
REFERENCES,0.37833827893175076,We can then write
REFERENCES,0.3798219584569733,"∥X −Xn∥2 =
Z 1"
REFERENCES,0.38130563798219586,"0
|X(u) −Xn(u)|2du
(27) ≤
Z 1 0  1 n"
REFERENCES,0.3827893175074184,"2
du =
 1 n"
REFERENCES,0.38427299703264095,"2
,
(28)"
REFERENCES,0.3857566765578635,which completes the proof.
REFERENCES,0.38724035608308605,Under review as a conference paper at ICLR 2022
REFERENCES,0.3887240356083086,"Proposition 2. Let W : [0, 1]2 →[0, 1] be a normalized Lipschitz graphon, and let Wn := WGn
be the graphon induced by the template graph Gn generated from W as in Deﬁnition 2. It holds
that
∥W −Wn∥≤2"
REFERENCES,0.39020771513353114,"n.
(29)"
REFERENCES,0.3916913946587537,"Proof. Let Ii = [(i −1)/n, i/n) for 1 ≤i ≤n −1 and In = [(n −1)/n, 1]. Since the graphon is
Lipschitz, for any u ∈Ii, v ∈Ij, 1 ≤i, j ≤n, we have"
REFERENCES,0.39317507418397624,"∥W(u, v) −Wn(u, v)∥≤max
u −i −1 n"
REFERENCES,0.39465875370919884,",

i
n −u"
REFERENCES,0.3961424332344214,"
(30)"
REFERENCES,0.39762611275964393,"+ max
v −j −1 n"
REFERENCES,0.3991097922848665,",

j
n −v"
REFERENCES,0.40059347181008903,"
(31) ≤1 n + 1 n = 2"
REFERENCES,0.4020771513353116,"n.
(32)"
REFERENCES,0.4035608308605341,We can then write
REFERENCES,0.4050445103857567,"∥W −Wn∥2 =
Z 1"
REFERENCES,0.4065281899109792,"0
|W(u, v) −Wn(u, v)|2dudv
(33) ≤
Z 1 0  2 n"
REFERENCES,0.40801186943620177,"2
dudv =
 2 n"
REFERENCES,0.4094955489614243,"2
(34)"
REFERENCES,0.41097922848664686,which concludes the proof.
REFERENCES,0.4124629080118694,"Proposition 3. Consider the L-layer WNN given by Y = Φ(X; H, W), where F0 = FL = 1 and
Fℓ= F for 1 ≤ℓ≤L −1. Let c ∈(0, 1] and assume that the graphon convolutions in all layers of
this WNN have K ﬁlter taps [cf. (6)]. Under Assumptions 1 through 3, the norm of the gradient of
the WNN with respect to its parameters H = {Hlk}l,k can be upper bounded by,"
REFERENCES,0.413946587537092,"∥∇HΦ(X; H, W)∥≤F 2L√"
REFERENCES,0.41543026706231456,"K.
(35)"
REFERENCES,0.4169139465875371,"Proof. We will ﬁnd an upper bound for any element [Hl†k†]g†f † of the tensor H. We start by the
last layer of the WNN, applying the deﬁnition given in equation (8),"
REFERENCES,0.41839762611275966,"∥∇[Hl†k†]g†f†Φ(X; H, W)∥=
∇[Hl†k†]g†f†Xf
L (36)"
REFERENCES,0.4198813056379822,"=
∇[Hl†k†]g†f†ρ  "
REFERENCES,0.42136498516320475,"Fl−1
X g=1 K−1
X"
REFERENCES,0.4228486646884273,"k=1
(T (k)
W Xg
l−1)[HLk]gf "
REFERENCES,0.42433234421364985,"
.
(37)"
REFERENCES,0.4258160237388724,"By Assumption 3, the non-linearity ρ is normalized Lipschitz , i.e. ∇ρ(·)(u) ≤1 for all u. Thus,
aplying the chain rule for the derivative, and the Cauchy-Schwartz inequality, the right hand side of
the previous expression can be rewritten as,"
REFERENCES,0.42729970326409494,"∥∇[Hl†k†]g†f†Φ(X; H, W)∥=
∇ρ  "
REFERENCES,0.4287833827893175,"Fl−1
X g=1 K−1
X"
REFERENCES,0.43026706231454004,"k=1
(T (k)
W Xg
l−1)[HLk]gf  "
REFERENCES,0.4317507418397626,∇[Hl†k†]g†f†
REFERENCES,0.4332344213649852,"Fl−1
X g=1 K−1
X"
REFERENCES,0.43471810089020774,"k=1
(T (k)
W Xg
l−1)[HLk]gf (38)"
REFERENCES,0.4362017804154303,"≤
∇[Hl†k†]g†f†"
REFERENCES,0.43768545994065283,"Fl−1
X g=1 K−1
X"
REFERENCES,0.4391691394658754,"k=1
(T (k)
W Xg
l−1)[HLk]gf (39)"
REFERENCES,0.4406528189910979,"Note that the a larger bound will occur if l† < L −1, then by linearity of derivation, and the triangle
inequality we obtain,"
REFERENCES,0.4421364985163205,"∥∇[Hl†k†]g†f†Φ(X; H, W)∥≤"
REFERENCES,0.443620178041543,"Fl−1
X g=1  K−1
X"
REFERENCES,0.44510385756676557,"k=1
T (k)
W (∇[Hl†k†]g†f†Xg
l−1)[HLk]gf (40)"
REFERENCES,0.4465875370919881,Under review as a conference paper at ICLR 2022
REFERENCES,0.44807121661721067,"By Assumption 2, the convolutional ﬁlters are non-amplifying, thus it holds that,"
REFERENCES,0.4495548961424332,"∥∇[Hl†k†]g†f†Φ(X; H, W)∥≤"
REFERENCES,0.45103857566765576,"Fl−1
X g=1"
REFERENCES,0.45252225519287836,"∇[Hl†k†]g†f†Xg
l−1 (41)"
REFERENCES,0.4540059347181009,"Now note that as ﬁlters are non-amplifying, the maximum difference in the gradient will be attained
at the ﬁrst layer (l = 1) of the WNN. Also note that the derivative of a convolutional ﬁlter TH [cf.
Deﬁnition 6] at coefﬁcient k† = i, is itself a convolutional ﬁlter with coefﬁcients hi. The values of
hi are [hi]j = 1 if j = i and 0 otherwise. Thence,"
REFERENCES,0.45548961424332346,"∥∇[Hl†k†]g†f†Φ(X; H, W)∥≤F L−1
hi∗WX0 (42)"
REFERENCES,0.456973293768546,"≤F L−1∥X0∥.
(43)"
REFERENCES,0.45845697329376855,"To complete the proof note that tensor H has F L−1K elements, and each individual gradient is
upper bounded by (43), and ∥X∥is normalized by Assumption 1."
REFERENCES,0.4599406528189911,"Lemma 1. Let Φ(X; H, W) be a WNN with F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let
c ∈(0, 1], and assume that the graphon convolutions in all layers of this WNN have K ﬁlter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under assumptions
(1),(2),(3), and (5) with probability 1 −ξ it holds that,"
REFERENCES,0.46142433234421365,"∥Φ(X; H, W) −Φ(Xn; H, Wn)∥≤LF L−1

1 + πBc
Wn
δc
WWn"
REFERENCES,0.4629080118694362,"2

1 +
q"
REFERENCES,0.46439169139465875,"n log( 2n ξ )
 n + 1"
REFERENCES,0.4658753709198813,"n + 4LF L−1c
(44)"
REFERENCES,0.46735905044510384,"The ﬁxed constants Bc
W and δc
WWn are the c-band cardinality and the c-eigenvalue margin of W
and Wn respectively [cf. Deﬁnitions 4,5]."
REFERENCES,0.4688427299703264,"Proof. We start by writing the expression on the left hand side, using the deﬁnition of WNN [cf.
(8)] we can write,"
REFERENCES,0.47032640949554894,"∥Φ(X;H, W) −Φ(Xn; H, Wn)∥= ∥XL −XnL∥
(45) = ρ  "
REFERENCES,0.47181008902077154,"FL−1
X g=1 K−1
X"
REFERENCES,0.4732937685459941,"k=1
(T (k)
W Xg
L−1)[HLk]gf  −ρ  "
REFERENCES,0.47477744807121663,"FL−1
X g=1 K−1
X"
REFERENCES,0.4762611275964392,"k=1
(T (k)
WnXg
nL−1)[HLk]gf   ."
REFERENCES,0.47774480712166173,"Since the non-linearity ρ is normalized Lipschitz by Assumption 3, using the triangle inequality, we
obtain"
REFERENCES,0.4792284866468843,∥XL −XnL∥≤
REFERENCES,0.4807121661721068,"FL−1
X g=1  K−1
X"
REFERENCES,0.4821958456973294,"k=1
(T (k)
W Xg
L−1)[HLk]gf − K−1
X"
REFERENCES,0.4836795252225519,"k=1
(T (k)
WnXg
nL−1)[HLk]gf"
REFERENCES,0.48516320474777447,".
(46)"
REFERENCES,0.486646884272997,"Using the triangle inequality once again, we split the last inequality into two terms as follows,"
REFERENCES,0.48813056379821956,∥XL −XnL∥≤
REFERENCES,0.4896142433234421,"FL−1
X g=1  K−1
X"
REFERENCES,0.4910979228486647,"k=1
T (k)
W (Xg
L−1 −Xg
nL−1)[HLk]gf (1) +"
REFERENCES,0.49258160237388726,"FL−1
X g=1  K−1
X"
REFERENCES,0.4940652818991098,"k=1
(T (k)
W −T (k)
Wn)Xg
L−1[HLk]gf"
REFERENCES,0.49554896142433236,"(2).
(47)"
REFERENCES,0.4970326409495549,"Where we have split (47) into terms (1), and (2). On the one hand, by assumption 2, convolutional
ﬁlters h are non-amplifying, thus using Cauchy-Schwartz inequality, term (1) can be bounded by,"
REFERENCES,0.49851632047477745,"FL−1
X g=1  K−1
X"
REFERENCES,0.5,"k=1
T (k)
W (Xg
L−1 −Xg
nL−1)[HLk]gf ≤"
REFERENCES,0.5014836795252225,"FL−1
X g=1"
REFERENCES,0.5029673590504451,"Xg
L−1 −Xg
nL−1
 .
(48)"
REFERENCES,0.5044510385756676,Under review as a conference paper at ICLR 2022
REFERENCES,0.5059347181008902,"To bound term (2), denoting hLgf the spectral representation of the convolutional ﬁlter applied to
Xg
L−1 at feature f of layer L [cf. Deﬁnition 3], we will decompose the ﬁlter as follows,"
REFERENCES,0.5074183976261127,"h≥c
Lgf(λ)
0
if |λ| < c
hLgf(λ) −hLgf(c)
if |λ| ≥c
(49)"
REFERENCES,0.5089020771513353,"h<c
Lgf(λ)
hLgf(λ)
if |λ| < c
hLgf(c)
if |λ| ≥c.
(50)"
REFERENCES,0.5103857566765578,"Note that hLgf = h≥c
Lgf + h<c
Lgf. Let T <c
[HL]gf and T <c
[HnL]gf , be the graphon convoutional ﬁlters
with ﬁlter function h<c
Lgf on graphons W, and Wn respectively [cf. Deﬁnition 6]. Note that ﬁlter
h<c
Lgf, varies only in the interval [0, c), and since ﬁlters are normalized Lipschitz by Assumption 2,
it veriﬁes
T <c
[HL]gf Xg
L−1 −T <c
[HnL]gf Xg
L−1
 ≤
(hLgf(c) + c) −(hLgf(c) −c)∥∥Xg
L−1

(51)"
REFERENCES,0.5118694362017804,"≤2c∥Xg
L−1∥.
(52)"
REFERENCES,0.5133531157270029,"Now we need to upper bound the difference in the high frequencies h≥c
Lgf. Let T ≥c
[HL]gf and T ≥c
[HnL]gf ,"
REFERENCES,0.5148367952522255,"be the graphon ﬁlters with ﬁlter function h≥c
Lgf on graphons W, and Wn respectively. Let Sn
denote the template graph sampled from the graphon W [cf. deﬁnition 2]. We denote Wn, the
induced graphon by template graph Sn as in (10). By introducing T ≥c
[HnL]gf , the graph ﬁlter with"
REFERENCES,0.516320474777448,"ﬁlter function h≥c
Lgf on graphon Wn, we can use the triangle inequality to obtain,
T ≥c
[HL]gf Xg
L−1 −T ≥c
[HnL]gf Xg
L−1
 ≤
T ≥c
[HL]gf Xg
L−1 −T ≥c
[HnL]gf Xg
L−1
 (2.1)"
REFERENCES,0.5178041543026706,"+
T ≥c
[HnL]gf Xg
L−1 −T ≥c
[HnL]gf Xg
L−1
 (2.2).
(53)"
REFERENCES,0.5192878338278932,"Under assumptions 1–5, to bound term (2.1) we can use (Ruiz et al., 2020a, Theorem 1), and to
bound term (2.2) we can use (Ruiz et al., 2020d, Lemma 2). Thus, with probability 1 −ξ, the
previous expression can be bounded by,"
REFERENCES,0.5207715133531158,"T ≥c
[HL]gf Xg
L−1 −T ≥c
[HnL]gf Xg
L−1
 ≤

1 + πBc
Wn
δc
WWn"
REFERENCES,0.5222551928783383,"2

1 +
q"
REFERENCES,0.5237388724035609,"n log( 2n ξ )
 n"
REFERENCES,0.5252225519287834,"Xg
L−1
 .
(54)"
REFERENCES,0.526706231454006,"Where the ﬁxed constants Bc
W and δc
WWn are the c-band cardinality and the c-eigenvalue margin
of W and Wn respectively [cf. Deﬁnitions 4,5]. Hence, coming back to (47), we can use (48) to
upper bound (1), and we can use (52), and (54), to upper bound (2) as follows,"
REFERENCES,0.5281899109792285,∥XL −XnL∥≤
REFERENCES,0.5296735905044511,"FL−1
X g=1"
REFERENCES,0.5311572700296736,"Xg
L−1 −Xg
nL−1
 + 2c∥Xg
L−1∥"
REFERENCES,0.5326409495548962,"+

1 + πBc
Wn
δc
WWn"
REFERENCES,0.5341246290801187,"2

1 +
q"
REFERENCES,0.5356083086053413,"n log( 2n ξ )
 n"
REFERENCES,0.5370919881305638,"Xg
L−1
 .
(55)"
REFERENCES,0.5385756676557863,"Now, we arrive at a recursive equation that we can compute for the L layers, with F features per
layer, to obtain,"
REFERENCES,0.5400593471810089,∥XL −XnL∥≤F0 ∥X0 −Xn0∥+ 2LF L−1c∥X0∥
REFERENCES,0.5415430267062314,"+ LF L−1

1 + πBc
Wn
δc
WWn"
REFERENCES,0.543026706231454,"2

1 +
q"
REFERENCES,0.5445103857566765,"n log( 2n ξ )
"
REFERENCES,0.5459940652818991,"n
∥X0∥.
(56)"
REFERENCES,0.5474777448071216,"Using Proposition 1, noting that F0 = 1 by construction, and using Assumption 1, concludes the
proof."
REFERENCES,0.5489614243323442,Under review as a conference paper at ICLR 2022
REFERENCES,0.5504451038575667,"Lemma 2. Let Φ(X; H, W) be a WNN with F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let
c ∈(0, 1], and assume that the graphon convolutions in all layers of this WNN have K ﬁlter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under assumptions
(1),(2),(3), and (5) with probability 1 −ξ it holds that,"
REFERENCES,0.5519287833827893,"∥∇HΦ(X; H, W) −∇HΦ(Xn; H, Wn)∥ ≤
√"
REFERENCES,0.5534124629080118,"KF L−1

2L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.5548961424332344,"2

1 +
q"
REFERENCES,0.5563798219584569,"n log( 2n ξ )
 n"
REFERENCES,0.5578635014836796,+ 2F L−1L
REFERENCES,0.5593471810089021,"n
+ 8L2F 2L−2c

."
REFERENCES,0.5608308605341247,"Proof. We will ﬁrst show that the gradient with respect to any arbitrary element [Hl†k†]g†f † ∈R
of H can be uniformly bounded. Note that the maximum is attained if l† = 1. Without loss of
generality, assuming l† > l −1, we can begin by using the deﬁnition given in equation (8) of the
output of the WNN as follows,"
REFERENCES,0.5623145400593472,"∥∇[Hl†k†]g†f†Φ(X; H, W) −∇[Hl†k†]g†f†Φ(Xn; H, Wn)∥"
REFERENCES,0.5637982195845698,"= ∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥
(57)"
REFERENCES,0.5652818991097923,"=
∇[Hl†k†]g†f†ρ  "
REFERENCES,0.5667655786350149,"Fl−1
X g=1 K−1
X"
REFERENCES,0.5682492581602374,"k=1
(T (k)
W Xg
l−1)[Hlk]gf  "
REFERENCES,0.56973293768546,−∇[Hl†k†]g†f†ρ  
REFERENCES,0.5712166172106825,"Fl−1
X g=1 K−1
X"
REFERENCES,0.5727002967359051,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf "
REFERENCES,0.5741839762611276,"
.
(58)"
REFERENCES,0.5756676557863502,"Taking derivatives by applying the chain rule, and applying the triangle inequality it yields,"
REFERENCES,0.5771513353115727,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥ ≤"
REFERENCES,0.5786350148367952,"
∇ρ
 Fl−1
X g=1 K−1
X"
REFERENCES,0.5801186943620178,"k=1
(T (k)
W Xg
l−1)[Hlk]gf"
REFERENCES,0.5816023738872403,"
−∇ρ
 Fl−1
X g=1 K−1
X"
REFERENCES,0.5830860534124629,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf "
REFERENCES,0.5845697329376854,∇[Hl†k†]g†f†
REFERENCES,0.586053412462908," Fl−1
X g=1 K−1
X"
REFERENCES,0.5875370919881305,"k=1
(T (k)
W Xg
l−1)[Hlk]gf"
REFERENCES,0.5890207715133531,"
(59)"
REFERENCES,0.5905044510385756,"+
∇ρ
 Fl−1
X g=1 K−1
X"
REFERENCES,0.5919881305637982,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.5934718100890207,"
(60)"
REFERENCES,0.5949554896142433,"
∇[Hl†k†]g†f†"
REFERENCES,0.5964391691394659,"Fl−1
X g=1 K−1
X"
REFERENCES,0.5979228486646885,"k=1
(T (k)
W Xg
l−1)[Hlk]gf −∇[Hl†k†]g†f†"
REFERENCES,0.599406528189911,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6008902077151336,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.6023738872403561,"."
REFERENCES,0.6038575667655787,"We can now use Cauchy-Schwartz inequality, Assumptions 3, 4, and Proposition 3 to bound the
terms regarding the gradient of the non-linearity ρ, the loss function ℓ, and the WNN respectively,
as follows,"
REFERENCES,0.6053412462908012,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥
(61) ≤"
REFERENCES,0.6068249258160238,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6083086053412463,"k=1
(T (k)
W Xg
l−1)[Hlk]gf −"
REFERENCES,0.6097922848664689,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6112759643916914,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.612759643916914,F L−1∥X0∥ +
REFERENCES,0.6142433234421365,"Fl−1
X"
REFERENCES,0.615727002967359,"g=1
∇[Hl†k†]g†f† K−1
X k=1"
REFERENCES,0.6172106824925816,"
(T (k)
W Xg
l−1)[Hlk]gf −(T (k)
WnXg
nl−1)[Hlk]gf)
."
REFERENCES,0.6186943620178041,Under review as a conference paper at ICLR 2022
REFERENCES,0.6201780415430267,"We can now apply the triangle inequality on the second term of the previous bound to obtain,"
REFERENCES,0.6216617210682492,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥
(62) ≤"
REFERENCES,0.6231454005934718,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6246290801186943,"k=1
(T (k)
W Xg
l−1)[Hlk]gf −"
REFERENCES,0.6261127596439169,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6275964391691394,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.629080118694362,F L−1∥X0∥ +
REFERENCES,0.6305637982195845,"Fl−1
X"
REFERENCES,0.6320474777448071,"g=1
∇[Hl†k†]g†f† K−1
X k=1"
REFERENCES,0.6335311572700296,"
(T (k)
W )[Hlk]gf −(T (k)
Wn)[Hlk]gf)

Xg
nl−1  +"
REFERENCES,0.6350148367952523,"Fl−1
X g=1"
REFERENCES,0.6364985163204748,"∇[Hl†k†]g†f† K−1
X"
REFERENCES,0.6379821958456974,"k=1
T (k)
Wn"
REFERENCES,0.6394658753709199,"
Xg
l−1 −Xg
nl−1"
REFERENCES,0.6409495548961425,"
[Hlk]gf)
."
REFERENCES,0.642433234421365,"Now note that as we are considering the case in which l† < l−1, using Cauchy-Schwartz inequality,
we can use the same bound for the ﬁrst and second term of the right hand side of the previous
inequality. Since ﬁlters are non-expansive by Assumption 3, it yields"
REFERENCES,0.6439169139465876,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥
(63) ≤2"
REFERENCES,0.6454005934718101,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6468842729970327,"k=1
(T (k)
W Xg
l−1)[Hlk]gf −"
REFERENCES,0.6483679525222552,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6498516320474778,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.6513353115727003,F L−1∥X0∥ +
REFERENCES,0.6528189910979229,"Fl−1
X g=1"
REFERENCES,0.6543026706231454,∇[Hl†k†]g†f†
REFERENCES,0.655786350148368,"
Xg
l−1 −Xg
nl−1"
REFERENCES,0.6572700296735905,"."
REFERENCES,0.658753709198813,"Now notice, that the only term that remains to bound is the exact same bound we obtained in equation
(57), but on the previous layer L −2. Hence, we conclude that by applying the same steps L −2
times, as the WNN has L layers, we will obtain a bound for any element [Hl†k†]g†f † of tensor H."
REFERENCES,0.6602373887240356,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥
(64)"
REFERENCES,0.6617210682492581,≤2LF L−2
REFERENCES,0.6632047477744807,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6646884272997032,"k=1
(T (k)
W Xg
l−1)[Hlk]gf −"
REFERENCES,0.6661721068249258,"Fl−1
X g=1 K−1
X"
REFERENCES,0.6676557863501483,"k=1
(T (k)
WnXg
nl−1)[Hlk]gf"
REFERENCES,0.6691394658753709,F L−1∥X0∥ +
REFERENCES,0.6706231454005934,"Fl−1
X g=1"
REFERENCES,0.672106824925816,∇[Hl†k†]g†f†
REFERENCES,0.6735905044510386,"
Xg
1 −Xg
1"
REFERENCES,0.6750741839762612,"."
REFERENCES,0.6765578635014837,"Note that the derivative of a convolutional ﬁlter TH at coefﬁcient k† = i, is itself a convolutional
ﬁlter with coefﬁcients hi [cf. Deﬁnition 6]. The values of hi are [hi]j = 1 if j = i and 0 otherwise.
As hi is itself a ﬁlter that veriﬁes Assumption 2, as graphons are normalized. Thus, considering l† =
0, and using Propositions 1, 2, (Chung & Radcliffe, 2011, Theorem 1) and the triangle inequality,
we obtain,
hi∗WnXn0 −hi∗WX0"
REFERENCES,0.6780415430267063,"≤

∥W −Wn∥+ ∥Wn −Wn∥

∥X0∥+ ∥Xn0 −X0∥
(65)"
REFERENCES,0.6795252225519288,"≤

1 + πBc
Wn
δc
WWn"
REFERENCES,0.6810089020771514,"2

1 +
q"
REFERENCES,0.6824925816023739,"n log( 2n ξ )
 n
+ 1"
REFERENCES,0.6839762611275965,"n
(66)"
REFERENCES,0.685459940652819,"with probability 1 −ξ. In the previous expression, Wn is the template graphon [cf. Deﬁnition 2].
Now, substituting (64) into (65), and using Lemma 1, with probability 1 −ξ, it holds that,"
REFERENCES,0.6869436201780416,"∥∇[Hl†k†]g†f†Xf
L −∇[Hl†k†]g†f†Xf
nL∥≤2L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.6884272997032641,"2

1 +
q"
REFERENCES,0.6899109792284867,"n log( 2n ξ )
 n"
REFERENCES,0.6913946587537092,+ 2F L−1L
REFERENCES,0.6928783382789317,"n
+ 8L2F 2L−2c.
(67)"
REFERENCES,0.6943620178041543,"To achieve the ﬁnal result, note that tensor H has KF L−1 elements, and each individual gradient is
upper bounded by (67)."
REFERENCES,0.6958456973293768,Under review as a conference paper at ICLR 2022
REFERENCES,0.6973293768545994,"Lemma 3. Let Φ(X; H, W) be a WNN with F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let
c ∈(0, 1], and assume that the graphon convolutions in all layers of this WNN have K ﬁlter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under Assumptions
(1)–(5) with probability 1 −ξ it holds that,"
REFERENCES,0.6988130563798219,"∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥ ≤
√"
REFERENCES,0.7002967359050445,"KF L−1

3L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.701780415430267,"2

1 +
q"
REFERENCES,0.7032640949554896,"n log( 2n ξ )
 n"
REFERENCES,0.7047477744807121,+ 4F L−1L
REFERENCES,0.7062314540059347,"n
+ 12L2F 2L−2c
"
REFERENCES,0.7077151335311572,"Proof. In order to analyze the norm of the gradient with respect to the tensor H, we can start by
taking the derivative with respect to a single element of the tensor, [Hl†k†]g†f †. By deriving the loss
function ℓusing the chain rule it yields,"
REFERENCES,0.7091988130563798,"∥∇[Hl†k†]g†f†ℓ(Y, Φ(X; H, W)) −∇[Hl†k†]g†f†ℓ(Yn, Φ(Xn; H, Wn))∥"
REFERENCES,0.7106824925816023,"=∥∇ℓ(Y, Φ(X; H, W))∇[Hl†k†]g†f†Φ(X; H, W)"
REFERENCES,0.712166172106825,"−∇ℓ(Yn, Φ(Xn; H, Wn))∇[Hl†k†]g†f†Φ(Xn; H, Wn)∥.
(68)"
REFERENCES,0.7136498516320475,"By Cauchy-Schwartz, and the triangle inequality it holds,"
REFERENCES,0.7151335311572701,"∥∇[Hl†k†]g†f†ℓ(Y, Φ(X; H, W)) −∇[Hl†k†]g†f†ℓ(Yn, Φ(Xn; H, Wn))∥"
REFERENCES,0.7166172106824926,"≤∥∇ℓ(Y, Φ(X; H, W)) −∇ℓ(Yn, Φ(Xn; H, Wn))∥∥∇[Hl†k†]g†f†Φ(X; H, W)∥
(69)"
REFERENCES,0.7181008902077152,"+ ∥∇ℓ(Yn, Φ(Xn; H, Wn))∥∥∇[Hl†k†]g†f†Φ(X; H, W) −∇[Hl†k†]g†f†Φ(Xn; H, Wn)∥."
REFERENCES,0.7195845697329377,"By the triangle inequality and Assumption 4 it follows,"
REFERENCES,0.7210682492581603,"∥∇[Hl†k†]g†f†ℓ(Y, Φ(X; H, W)) −∇[Hl†k†]g†f†ℓ(Yn, Φ(Xn; H, Wn))∥"
REFERENCES,0.7225519287833828,"≤∥∇ℓ(Y, Φ(X; H, W)) −∇ℓ(Y, Φ(Xn; H, Wn))∥∥∇[Hl†k†]g†f†Φ(X; H, W)∥
(70)"
REFERENCES,0.7240356083086054,"∥∇ℓ(Yn, Φ(Xn; H, Wn)) −∇ℓ(Y, Φ(Xn; H, Wn))∥∥∇[Hl†k†]g†f†Φ(X; H, W)∥
(71)"
REFERENCES,0.7255192878338279,"+ ∥∇[Hl†k†]g†f†Φ(X; H, W) −∇[Hl†k†]g†f†Φ(Xn; H, Wn)∥"
REFERENCES,0.7270029673590505,"≤(∥Yn −Y ∥+ ∥Φ(Xn; H, Wn)) −Φ(X; H, W))∥)∥∇[Hl†k†]g†f†Φ(X; H, W)∥
(72)"
REFERENCES,0.728486646884273,"+ ∥∇[Hl†k†]g†f†Φ(X; H, W) −∇[Hl†k†]g†f†Φ(Xn; H, Wn)∥."
REFERENCES,0.7299703264094956,"Now we can use Lemmas 1–2, Propositions 1, and 3, and Assumption 1 to obtain,"
REFERENCES,0.7314540059347181,"∥∇[Hl†k†]g†f†ℓ(Y, Φ(X; H, W)) −∇[Hl†k†]g†f†ℓ(Yn, Φ(Xn; H, Wn))∥
(73)"
REFERENCES,0.7329376854599406,"≤

3L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.7344213649851632,"2

1 +
q"
REFERENCES,0.7359050445103857,"n log( 2n ξ )
 n"
REFERENCES,0.7373887240356083,+ 4F L−1L
REFERENCES,0.7388724035608308,"n
+ 12L2F 2L−2c
"
REFERENCES,0.7403560830860534,"Noting that tensor H has KF L−1 elements, and each individual term can be bounded by (73), the
desired result is attained."
REFERENCES,0.7418397626112759,"Deﬁnition 7. We deﬁne the constant γ as,"
REFERENCES,0.7433234421364985,"γ = 12
√"
REFERENCES,0.744807121661721,"KF L−1L2F 2L−2,
(74)"
REFERENCES,0.7462908011869436,"where K is the number of features, L is the number of layers, and K is the number of ﬁlter taps of
the GNN."
REFERENCES,0.7477744807121661,Under review as a conference paper at ICLR 2022
REFERENCES,0.7492581602373887,"We will present a more comprehensive statement of Theorem 1, where we include all the smaller
order terms in (15). Notice that the statement of Theorem 1 in the main body of the paper omits
these terms in order to simplify the exposition of the main result. In practice, these smaller order
terms vanish faster as n increases.
Theorem 1. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1] and assume that the graphon
convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Under assumptions AS1–AS5, it holds that
E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥] ≤
√"
REFERENCES,0.7507418397626113,"KF L−1

6L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.7522255192878339,"

1 +
p"
REFERENCES,0.7537091988130564,"n log(2n3/2)
 n"
REFERENCES,0.755192878338279,+ 4F L−1L
REFERENCES,0.7566765578635015,"n
+ 12L2F 2L−2c

+ 2F 2L√"
REFERENCES,0.7581602373887241,"K
√n
(75)"
REFERENCES,0.7596439169139466,"where Yn is the graphon signal induced by [yn]i = Y (ui), ui = (i −1)/n for 1 ≤i ≤n [cf. (10)].
The ﬁxed constants Bc
W and δc
WWn are the c-band cardinality and the c-eigenvalue margin of W
and Wn respectively [cf. Deﬁnitions 4,5 in the supplementary material]."
REFERENCES,0.7611275964391692,"Proof of Theorem 1. We begin by considering the event An such that,"
REFERENCES,0.7626112759643917,"An =

∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥
(76) ≤
√"
REFERENCES,0.7640949554896143,"KF L−1

3L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.7655786350148368,"2

1 +
q"
REFERENCES,0.7670623145400594,"n log( 2n ξ )
 n"
REFERENCES,0.7685459940652819,+ 4F L−1L
REFERENCES,0.7700296735905044,"n
+ 12L2F 2L−2c

."
REFERENCES,0.771513353115727,"Thus, by considering the disjoint events An, and Ac
n, and denoting 1(·) the indicator function, the
expectation can be separated as follows,
E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥]
= E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥1(An)]
+ E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥1(Ac
n)]
(77)
We can bound the term regarding Ac
n using the chain rule, Cauchy-Schwartz inequality, Assumption
4, and Proposition 3 as follows,
∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥
≤∥∇Hℓ(Y, Φ(X; H, W))∥+ ∥∇Hℓ(Yn, Φ(Xn; H, Wn))∥
(78)
≤∥∇ℓ(Y, Φ(X; H, W))∥∥∇HΦ(X; H, W)∥
+ ∥∇ℓ(Yn, Φ(Xn; H, Wn))∥∥∇HΦ(Xn; H, Wn)∥
(79)
≤∥∇HΦ(X; H, W)∥+ ∥∇HΦ(Xn; H, Wn)∥
(80)"
REFERENCES,0.7729970326409495,≤2F 2L√
REFERENCES,0.7744807121661721,"K
(81)
Returning to equation (77), we can substitute the bound obtained in equation (81), and by taking
P(An) = 1 −ξ, and using Lemma 3, it yields,
E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥]"
REFERENCES,0.7759643916913946,"≤(1 −ξ)
√"
REFERENCES,0.7774480712166172,"KF L−1

3L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.7789317507418397,"2

1 +
q"
REFERENCES,0.7804154302670623,"n log( 2n ξ )
 n"
REFERENCES,0.7818991097922848,+ 4F L−1L
REFERENCES,0.7833827893175074,"n
+ 12L2F 2L−2c

+ ξ2F 2L√"
REFERENCES,0.7848664688427299,"K
(82)"
REFERENCES,0.7863501483679525,"To complete the proof, set ξ =
1
√n."
REFERENCES,0.787833827893175,Under review as a conference paper at ICLR 2022
REFERENCES,0.7893175074183977,"E[∥∇Hℓ(Y, Φ(X; H, W)) −∇Hℓ(Yn, Φ(Xn; H, Wn))∥]"
REFERENCES,0.7908011869436202,"∇Hℓ(Yn, Φ(Xn; H, Wn))"
REFERENCES,0.7922848664688428,"Figure 3:
In order to satisfy the property that the inner product between the gradient on the GNN
∇Hℓ(Yn, Φ(Xn; H, Wn)) and the gradient on the graphon ∇Hℓ(Y, Φ(X; H, W)) is positive, we rely on
the condition provided in Theorem 2."
REFERENCES,0.7937685459940653,"D
PROOF OF THEOREM 2"
REFERENCES,0.7952522255192879,"Deﬁnition 8 (Stopping time). We deﬁne the stopping time k∗as,"
REFERENCES,0.7967359050445104,"k∗= min
k≥0{∥∇HΦ(X; Hk, Wn)∥≤
√"
REFERENCES,0.798219584569733,"KF L−112L2F 2L−2c}.
(83)"
REFERENCES,0.7997032640949555,Deﬁnition 9 (Constant ψ).
REFERENCES,0.8011869436201781,"Lemma 4. Under Assumptions 4, 5, and 6, the gradient of the loss function ℓwith respect to the
parameters of the GNN H is A∇ℓ-Lipschitz,"
REFERENCES,0.8026706231454006,"∥∇Hℓ(Y, Φ(X; A, W)) −∇Hℓ(Y, Φ(X; B, W))∥≤A∇ℓ∥A −B∥
(84)"
REFERENCES,0.8041543026706232,where A∇ℓ= (A∇Φ + AΦF 2L√ K).
REFERENCES,0.8056379821958457,"Proof. To begin with, we can apply the chain rule to obtain,"
REFERENCES,0.8071216617210683,"∥∇Hℓ(Y, Φ(X; A, W)) −∇Hℓ(Y, Φ(X; B, W))∥
= ∥∇ℓ(Y, Φ(X; A, W))∇HΦ(X; A, W) −∇ℓ(Y, Φ(X; B, W))∇HΦ(X; B, W)∥
(85)"
REFERENCES,0.8086053412462908,"By applying the triangle inequality, and Cauchy-Schwartz it yields,"
REFERENCES,0.8100890207715133,"∥∇Hℓ(Y, Φ(X; A, W)) −∇Hℓ(Y, Φ(X; B, W))∥
≤∥∇ℓ(Y, Φ(X; A, W))∥∥∇HΦ(X; A, W) −∇HΦ(X; B, W)∥
+ ∥∇Hℓ(Y, Φ(X; A, W)) −ℓ(Y, Φ(X; B, W))∥∥∇HΦ(X; B, W)∥
(86)"
REFERENCES,0.8115727002967359,"We can now use Assumptions 1, 4, 5, and 6 as well as Proposition 3, to obtain"
REFERENCES,0.8130563798219584,"∥∇Hℓ(Y, Φ(X; A, W)) −∇Hℓ(Y, Φ(X; B, W))∥≤(A∇Φ + AΦF 2L√"
REFERENCES,0.814540059347181,"K∥A −B∥)
(87)"
REFERENCES,0.8160237388724035,Completing the proof.
REFERENCES,0.8175074183976261,"Lemma 5. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1] and assume that the graphon
convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Under assumptions AS1–AS6, let the following condition be
satisﬁed for every k, √"
REFERENCES,0.8189910979228486,"KLL−1

6L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.8204747774480712,"

1 +
p"
REFERENCES,0.8219584569732937,"n log(2n3/2)
"
REFERENCES,0.8234421364985163,"n
+ 4F L−1L"
REFERENCES,0.8249258160237388,"n
+ 12L2F 2L−2c
"
REFERENCES,0.8264094955489614,+ 2F 2L√
REFERENCES,0.827893175074184,"K
√n
< 1 −A∇ℓηk"
REFERENCES,0.8293768545994066,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥.
(88)"
REFERENCES,0.8308605341246291,"then the ﬁrst k∗iterates generated by equation (14), 1(k ≤k∗)ℓ(Y, Φ(X; Hk, Wn)) form a positive
super-martingale with respect to the ﬁltration Fk generated by the history of the Algorithm up to step
k [i.e., {X, Y, Xn, Yn, Wn}k, {X, Y, Xn, Yn, Wn}k−1, . . . , {X, Y, Xn, Yn, Wn}0]. Where k∗is
the stopping time deﬁned in Deﬁnition 8, and 1(·) is the indicator function."
REFERENCES,0.8323442136498517,Under review as a conference paper at ICLR 2022
REFERENCES,0.8338278931750742,"Proof. To begin with, 1(k < k∗)ℓ(Y, Φ(X; Hk, W)) ∈Fk, where Fk is the ﬁltration generated by
the history of the Algorithm up to k. Note that the loss function ℓis positive by Assumption 4. It
remains to be shown the inequality expression of the super-martingale. For k > k∗, the inequality
is trivially veriﬁed as the indicator function 1(k ≤k∗) = 0 for k > k∗. For k ≤k∗, as in Bertsekas
& Tsitsiklis (2000), we deﬁne a continuous function g(ϵ) that takes the value of the loss function on
the Graphon data on iteration k + 1 at ϵ = 1, and on iteration k on ϵ = 0 as follows,"
REFERENCES,0.8353115727002968,"g(ϵ) = ℓ(Y, Φ(X; Hk −ϵηk∇Hℓ(Yn, Φ(Xn; Hk, Wn)), W)).
(89)"
REFERENCES,0.8367952522255193,"Note that function g(ϵ), is evaluated on the graphon data Y, X, W, but the steps are controlled by
the induced graphon data Yn, Xn, Wn. Applying the chain rule, the derivative of g(ϵ) with respect
to ϵ can be obtain as follows,"
REFERENCES,0.8382789317507419,"∂
∂ϵg(ϵ) =
(90)"
REFERENCES,0.8397626112759644,"−∇Hℓ(Y, Φ(X; Hk −ϵηk∇Hℓ(Yn, Φ(Hk; Wn; Xn)), W))ηk∇Hℓ(Yn, Φ(Xn; Hk; Wn))."
REFERENCES,0.841246290801187,"Now note that the difference in the loss function ℓbetween iterations k + 1 and k can be written as
the difference between g(ϵ = 1) and g(ϵ = 0) as follows,"
REFERENCES,0.8427299703264095,"g(1) −g(0) = ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W)).
(91)"
REFERENCES,0.844213649851632,"Computing the integration of the derivative of g(ϵ) between [0, 1] it yields"
REFERENCES,0.8456973293768546,"ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W)) = g(1) −g(0) =
Z 1 0"
REFERENCES,0.8471810089020771,"∂
∂ϵg(ϵ)dϵ
(92) =
Z 1"
REFERENCES,0.8486646884272997,"0
∇Hℓ(Y, Φ(X; Hk −ϵηk∇Hℓ(Yn, Φ(Xn; Hk, Wn)), W))"
REFERENCES,0.8501483679525222,"(−)ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))dϵ.
(93)"
REFERENCES,0.8516320474777448,"Note that the last term of the previous integral does not depend on ϵ. Besides, we can sum and
subtract ∇Hℓ(Y, Φ(Hk, W, X)) inside the integral, to obtain,"
REFERENCES,0.8531157270029673,"ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W))
= (−)ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))
Z 1"
REFERENCES,0.8545994065281899,"0
∇Hℓ(Y, Φ(X; Hk −ϵηk∇Hℓ(Yn, Φ(Xn; Hk, Wn)), W))"
REFERENCES,0.8560830860534124,"+ ∇Hℓ(Y, Φ(X; Hk, W)) −∇Hℓ(Y, Φ(X; Hk, W))dϵ
(94)"
REFERENCES,0.857566765578635,"= −ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))∇Hℓ(Y, Φ(X; Hk, W))
Z 1 0
dϵ"
REFERENCES,0.8590504451038575,"−ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))
Z 1"
REFERENCES,0.8605341246290801,"0
∇Hℓ(Y, Φ(Hk −ϵηk∇Hℓ(Yn, Φ(Xn; Hk, Wn)), W, X))"
REFERENCES,0.8620178041543026,"−∇Hℓ(Y, Φ(X; Hk, W))dϵ.
(95)"
REFERENCES,0.8635014836795252,"We can now apply the Cauchy-Schwartz inequality to the last term on the previous inequality, and
take the norm of the integral, which is smaller that the integral of the norm to obtain,"
REFERENCES,0.8649851632047477,"ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W))
≤−ηk∇Hℓ(Yn, Φ(Hk; Wn; Xn))∇Hℓ(Y, Φ(Hk, W, X))"
REFERENCES,0.8664688427299704,"+ ηk∥∇Hℓ(Yn, Φ(Hk; Wn; Xn))∥
Z 1 0"
REFERENCES,0.8679525222551929,"∇Hℓ(Y, Φ(Hk −ϵηk∇Hℓ(Yn, Φ(Hk; Wn; Xn)), W, X))"
REFERENCES,0.8694362017804155,"−∇Hℓ(Y, Φ(Hk, W, X))
dϵ.
(96)"
REFERENCES,0.870919881305638,Under review as a conference paper at ICLR 2022
REFERENCES,0.8724035608308606,"Under Lemma 4, we can take the Lipschitz bound on the gradient on the loss function with respect
to the parameters, using A∇ℓ, to obtain,"
REFERENCES,0.8738872403560831,"ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W))
≤−ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))∇Hℓ(Y, Φ(X; Hk, W))"
REFERENCES,0.8753709198813057,"+ A∇ℓηk∥∇Hℓ(Yn, Φ(Xn; Hk, Wn))|
Z 1 0"
REFERENCES,0.8768545994065282,"ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))
ϵdϵ
(97)"
REFERENCES,0.8783382789317508,"≤−ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))∇Hℓ(Y, Φ(X; Hk, W))"
REFERENCES,0.8798219584569733,"+ η2
kA∇ℓ"
REFERENCES,0.8813056379821959,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥2.
(98)"
REFERENCES,0.8827893175074184,"Instead of evaluating the internal product between the gradient on the graphon, and induced graphon,
we will use Theorem 1, to bound their expected difference (cf. Figure 3 for intuition). We can add
and subtract the gradient of the loss function on the induced graphon ∇Hℓ(Yn, Φ(Hk; Wn; Xn)),
and use the Cauchy-Schwartz inequality to obtain,"
REFERENCES,0.884272997032641,"ℓ(Y, Φ(X; Hk+1, W)) −ℓ(Y, Φ(X; Hk, W))
≤−ηk∇Hℓ(Yn, Φ(Xn; Hk, Wn))
(∇Hℓ(Y, Φ(X; Hk, W)) + ∇Hℓ(Yn, Φ(Xn; Hk, Wn)) −∇Hℓ(Yn, Φ(Xn; Hk, Wn)))"
REFERENCES,0.8857566765578635,"+ η2
kA∇Φ"
REFERENCES,0.887240356083086,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥2
(99)"
REFERENCES,0.8887240356083086,"≤−ηk∥∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥2"
REFERENCES,0.8902077151335311,"+ ηk∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥∥∇Hℓ(Y, Φ(X; Hk, W)) −∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥"
REFERENCES,0.8916913946587537,"+ η2
kA∇Φ"
REFERENCES,0.8931750741839762,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥2.
(100)"
REFERENCES,0.8946587537091988,"We can rearrange the previous expression, to obtain,"
REFERENCES,0.8961424332344213,"ηk∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥2

1 −A∇ℓηk 2"
REFERENCES,0.8976261127596439,"−∥∇Hℓ(Y, Φ(X; Hk, W)) −∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥"
REFERENCES,0.8991097922848664,"∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥ "
REFERENCES,0.900593471810089,"≤ℓ(Y, Φ(X; Hk, W)) −ℓ(Y, Φ(X; Hk+1, W)).
(101)"
REFERENCES,0.9020771513353115,"We can now take the conditional expectation with respect to the ﬁltration Fn to obtain,"
REFERENCES,0.9035608308605341,"E[ℓ(Y, Φ(X; Hk+1, W))|Fk]"
REFERENCES,0.9050445103857567,"≤ηk∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥2

1 −A∇ℓηk 2"
REFERENCES,0.9065281899109793,"−E
∥∇Hℓ(Y, Φ(X; Hk, W)) −∇Hℓ(Yn, Φ(Xn; Hk, Wn))∥"
REFERENCES,0.9080118694362018,"∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥ Fk "
REFERENCES,0.9094955489614244,"+ ℓ(Y, Φ(X; Hk, W)).
(102)"
REFERENCES,0.9109792284866469,"As step size ηk > 0, and by deﬁnition norms are non-negative, using Theorem 1, as condition (88)
holds for k ≤k∗, then"
REFERENCES,0.9124629080118695,"E[ℓ(Y, Φ(X; Hk+1, W))|Fk] ≤ℓ(Y, Φ(X; Hk, W)).
(103)"
REFERENCES,0.913946587537092,"By deﬁnition of super-martingale as in Durrett (2019), we complete the proof."
REFERENCES,0.9154302670623146,"Lemma 6. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1] and assume that the graphon
convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN"
REFERENCES,0.9169139465875371,Under review as a conference paper at ICLR 2022
REFERENCES,0.9183976261127597,"sampled from Φ(X; H, W) as in (9). Under assumptions AS1–AS6, for any ϵ ∈(0, 1 −A∇ℓη), if
the iterates generated by (14), satisfy, √"
REFERENCES,0.9198813056379822,"KLL−1

6L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.9213649851632048,"

1 +
p"
REFERENCES,0.9228486646884273,"n log(2n3/2)
"
REFERENCES,0.9243323442136498,"n
+ 4F L−1L"
REFERENCES,0.9258160237388724,"n
+ 12L2F 2L−2c
"
REFERENCES,0.9272997032640949,+ 2F 2L√
REFERENCES,0.9287833827893175,"K
√n
< 1 −A∇ℓηk −ϵ"
REFERENCES,0.93026706231454,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥.
(104)"
REFERENCES,0.9317507418397626,"then the expected value of the stopping time k∗[cf. Deﬁnition 8], is ﬁnite, i.e.,"
REFERENCES,0.9332344213649851,"E[k∗] = O(1/ϵ)
(105)"
REFERENCES,0.9347181008902077,"Proof. Given the iterates at k = k∗, and the initial values at k = 0, we can express the expected
difference between the loss ℓ, as the summation over the difference of iterates as follows,"
REFERENCES,0.9362017804154302,"E[ℓ(Y, Φ(X; H0, W)) −ℓ(Y, Φ(X; Hk∗, W))] = E"
REFERENCES,0.9376854599406528,""" k∗
X"
REFERENCES,0.9391691394658753,"k=1
ℓ(Y, Φ(X; Hk−1, W)) −ℓ(Y, Φ(X; Hk, W) # (106)"
REFERENCES,0.9406528189910979,"Taking the expected value with respect to the ﬁnal iterate k = k∗, we get,"
REFERENCES,0.9421364985163204,"E

ℓ(Y, Φ(X; Hk0)) −ℓ(Y, Φ(X; Hk∗))
"
REFERENCES,0.9436201780415431,"= Ek∗

E
 k∗
X"
REFERENCES,0.9451038575667656,"k=1
ℓ(Y, Φ(X; Hk−1, W)) −ℓ(Y, Φ(X; Hk, W)
k∗

(107) = ∞
X"
REFERENCES,0.9465875370919882,"t=0
E

t
X"
REFERENCES,0.9480712166172107,"k=1
ℓ(Y, Φ(X; Hk−1, W)) −ℓ(Y, Φ(X; Hk, W)

P(k∗= t) (108)"
REFERENCES,0.9495548961424333,"Using condition (104), and Lemma 5 for any k ≤k∗, it veriﬁes"
REFERENCES,0.9510385756676558,"E

ℓ(Y, Φ(X; Hk−1, W)) −ℓ(Y, Φ(X; Hk, W))

≥η(
√"
REFERENCES,0.9525222551928784,"KF L−112L2F 2L−2c)2ϵ
(109)"
REFERENCES,0.9540059347181009,"Thus, coming back to (108),"
REFERENCES,0.9554896142433235,"E

ℓ(Y, Φ(X; Hk0, W)) −ℓ(Y, Φ(X; Hk∗, W))

≥η(
√"
REFERENCES,0.956973293768546,"KF L−112L2F 2L−2c)2ϵ ∞
X"
REFERENCES,0.9584569732937686,"t=0
tP(k∗= t) (110) ≥η(
√"
REFERENCES,0.9599406528189911,"KF L−112L2F 2L−2c)2ϵE[k∗]
(111)"
REFERENCES,0.9614243323442137,"Note that as the loss function ℓis non-negative,"
REFERENCES,0.9629080118694362,"E

ℓ(Y, Φ(X; Hk0, W))
 η(
√"
REFERENCES,0.9643916913946587,"KF L−112L2F 2L−2c)2ϵ
≥E[k∗]
(112)"
REFERENCES,0.9658753709198813,Thus concluding that k∗= O(1/ϵ).
REFERENCES,0.9673590504451038,"Theorem 2. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤l ≤L −1. Let c ∈(0, 1] and assume that the graphon
convolutions in all layers of this WNN have K ﬁlter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Consider the iterates generated by equation (14). Under"
REFERENCES,0.9688427299703264,Under review as a conference paper at ICLR 2022
REFERENCES,0.9703264094955489,"Assumptions AS1-AS6, for any ﬁxed ϵ ∈(0, 1 −A∇ℓη), if at each step k the number of nodes n is
picked such that it veriﬁes √"
REFERENCES,0.9718100890207715,"KLL−1

6L2F 2L−2

1 + πBc
Wn
δc
WWn"
REFERENCES,0.973293768545994,"

1 +
p"
REFERENCES,0.9747774480712166,"n log(2n3/2)
"
REFERENCES,0.9762611275964391,"n
+ 4F L−1L"
REFERENCES,0.9777448071216617,"n
+ 12L2F 2L−2c
"
REFERENCES,0.9792284866468842,+ 2F 2L√
REFERENCES,0.9807121661721068,"K
√n
< 1 −A∇ℓηk −ϵ"
REFERENCES,0.9821958456973294,"2
∥∇Hℓ(Yn, Φ(Xn; Hk, Wn)∥.
(113)"
REFERENCES,0.983679525222552,then in ﬁnite time we will achieve an iterate k∗such that the coefﬁcients Hk∗satisfy
REFERENCES,0.9851632047477745,"E[∥∇Hℓ(Y, Φ(X; Hk∗, W))∥] ≤24
√"
REFERENCES,0.9866468842729971,"KF L−1L2F 2L−2c
with probability 1
(114)"
REFERENCES,0.9881305637982196,where A∇ℓηk = (A∇Φ + AΦF 2L√ K).
REFERENCES,0.9896142433234422,"Proof. We can use Lemma 6, to conclude that it must be the case that P(k∗= ∞) = 0, which
implies that, P(k∗< ∞) = 1. Using stopping time k∗condition [cf. Deﬁnition 8] and the triangle
inequality, it yields,"
REFERENCES,0.9910979228486647,"E[∥∇Hℓ(Y, Φ(X; Hk∗, W))∥] ≤∥∇Hℓ(Yn, Φ(Xn; Hk∗, Wn))∥
(115)
+E[∥∇Hℓ(Yn, Φ(X; Hk∗, Wn)) −∇Hℓ(Y, Φ(Xn; Hk∗, Wn))∥]"
REFERENCES,0.9925816023738873,"Note that the iterates are constructed such that, for every k"
REFERENCES,0.9940652818991098,"E[∥∇Hℓ(Yn, Φ(X; Hk, Wn)) −∇Hℓ(Y, Φ(Xn; Hk, Wn))∥] ≤∥∇Hℓ(Yn, Φ(X; Hk, Wn))∥.
(116)"
REFERENCES,0.9955489614243324,"Using the stopping time condition, the ﬁnal result is attained as follows"
REFERENCES,0.9970326409495549,"E[∥∇Hℓ(Y, Φ(X; Hk∗, W))∥] ≤2∥∇Hℓ(Yn, Φ(X; Hk∗, Wn))∥
(117) ≤24
√"
REFERENCES,0.9985163204747775,"KF L−1L2F 2L−2c.
(118)"
