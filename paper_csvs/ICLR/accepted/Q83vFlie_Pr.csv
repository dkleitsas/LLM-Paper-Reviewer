Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005494505494505495,"In this paper, we consider a new multi-armed bandit (MAB) framework motivated
by three common complications in online recommender systems in practice: (i)
the platform (learning agent) cannot sample an intended product directly and has
to incentivize customers to select this product (e.g., promotions and coupons); (ii)
customer feedbacks are often received later than their selection times; and (iii)
customer preferences among products are inﬂuenced and reinforced by historical
feedbacks. From the platform’s perspective, the goal of the MAB framework is to
maximize total reward without incurring excessive incentive costs. A major chal-
lenge of this MAB framework is that the loss of information caused by feedback
delay complicates both user preference evolution and arm incentivizing decisions,
both of which are already highly non-trivial even by themselves. Toward this end,
we ﬁrst propose a policy called “UCB-Filtering-with-Delayed-Feedback” (UCB-
FDF) policy for this new MAB framework. In our analysis, we consider delayed
feedbacks that can have either arm-independent or arm-dependent distributions.
In both cases, we allow unbounded support for the random delays, i.e., the ran-
dom delay can be inﬁnite. We show that the delay impacts in both cases can still
be upper bounded by an additive penalty on both the regret and total incentive
costs. This further implies that logarithmic regret and incentive cost growth rates
are achievable under this new MAB framework. Experimental results corroborate
our theoretical analysis on both regret and incentive costs."
INTRODUCTION,0.01098901098901099,"1
INTRODUCTION"
INTRODUCTION,0.016483516483516484,"In recent years, the multi-armed bandit (MAB) framework has received a signiﬁcant amount of
interest in the learning research community. This is partly due to the fact that, in many online e-
commerce recommender systems (e.g., Amazon and Walmart), the problem of online learning of the
optimal products while making proﬁts at the same time can be well formulated by an MAB problem.
However, although many MAB algorithms have been proposed in this area, it is worth noting that
most of the existing MAB models in the literature have not considered the joint effect of three
common phenomena in e-commerce recommender systems: (i) In many e-commerce recommender
systems, the platform (the learning agent) cannot sample an intended product (an intended arm)
directly and has to incentivize customers (e.g., through promotions and coupons) to sample the
product and receive the sampling feedback from the customers indirectly (e.g., ratings and reviews);
(ii) Customer feedbacks are often received much later than their purchasing times (e.g., a review
may or may not be submitted by a customer even months later after purchasing a product); and (iii)
Customer preferences among products are inﬂuenced and reinforced by historical feedbacks, which
may even lead to various viral effects over some products (the more good reviews one product has
received, the more likely that the next arriving customer will prefer this product). The lack of a
fundamental understanding and joint studies of these three important factors in MAB policy designs
motivates us to ﬁll this gap in this paper."
INTRODUCTION,0.02197802197802198,"Toward this end, we propose a new MAB framework that jointly considers i) incentivized sampling,
ii) delayed sampling feedback, and iii) self-reinforcing user preferences in online recommender sys-"
INTRODUCTION,0.027472527472527472,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03296703296703297,"tems. However, we note that the MAB policy design for the proposed new MAB framework is highly
non-trivial due to the complex couplings between the aforementioned three factors. First, similar to
conventional MAB problems, there exists a dilemma between sufﬁcient exploration through sam-
pling to learn an optimal arm (i.e., an optimal product), which may incur numerous pullings of
sub-optimal arms, and the greedy exploitation to play the arm that has performed well thus far to
earn proﬁts. Second, there is another dilemma to the learning agent between offering sufﬁciently
attractive incentives to mitigate biases (due to lack of initial data and self-reinforcing user prefer-
ences) and avoid spending unnecessarily high incentives that hurt the learning agent’s proﬁts. Last
but not least, the delayed sampling feedbacks may render the estimation of arms’ quality during
the MAB process highly inaccurate, introducing yet another layer of uncertainty to the MAB on-
line learning problem, which is already plagued by complications from incentivized sampling and
self-reinforcing user preferences. As in most MAB problems, we adopt “regret” as our performance
metric in this paper, which is deﬁned as the cumulative reward gap between the proposed policy and
an optimal policy design in hindsight. Under the regret setting, the complications due to these three
key factors naturally prompt the following fundamental questions:"
INTRODUCTION,0.038461538461538464,"(1) How should the agent design an incentivizing strategy to strike a good balance between explo-
ration and exploitation to achieve sublinear (hopefully logarithmic) regrets?
(2) To avoid offering exceedingly high incentives, how should the agent incentivize in order to
attract a user crowd that prefer an optimal arm, so that the users’ self-reinforcing preference
could automatically gravitate toward this optimal arm without further incentives?
(3) Under various delayed feedback situations in the new MAB framework (e.g., unbounded ran-
dom delays, heavy-tailed delay distributions, and arm-dependent delays), could we still achieve
low regrets with low incentive costs?
In this paper, we answer the above fundamental questions afﬁrmatively by proposing a new
“Delayed-UCB-Filtering” policy for the MAB framework that jointly considers incentivizing sam-
pling, delayed sampling feedback, and self-reinforcing user preferences. We show that our proposed
policy achieves O(log T) regret with O(log T) incentive payments. The success of our policy design
hinges upon two key insights: (i) the self-reinforcing user preference effect is actually a “blessing in
disguise” and can be leveraged to establish an important “dominance” condition (more on this later)
that further implies O(log T) regret and incentive costs; and (ii) the impacts of delayed feedback on
regret and incentive costs can be upper bounded under appropriate statistical settings to preserve the
“dominance” condition. Our key contributions and main results are summarized as follows:"
INTRODUCTION,0.04395604395604396,"• We propose a new MAB model that jointly considers incentivized arm sampling, delayed sam-
pling feedback, and self-reinforcing user preferences, all of which are important features of online
recommender systems. To develop efﬁcient and low-cost incentivized policy for this new MAB
model, we propose a three-phase “UCB-Filtering-with-Delayed-Feedback” (UCB-FDF) policy,
which contains an incentivized exploration phase, an incentivized exploitation phase, and a self-
sustaining phase. In our UCB-FDF policy, the ﬁrst two phases judiciously integrate delayed
feedback information, while in third phase, the system solely relies on self-reinforcing user pref-
erences to converge to the pulling of the optimal arm.
• We ﬁrst show a fundamental fact that, under our UCB-FDF policy, delayed sampling feedback
only has an additive penalty on the regret and incentive cost performances, and that this additive
penalty grows logarithmically with respect to time. Speciﬁcally, we ﬁrst investigate the delayed
feedback impact under the assumption that the feedback delay is an i.i.d. random variable across
samplings with a ﬁnite expectation. We show that the UCB-FDF policy achieves logarithmic
growth rates of regret and incentive costs under this setting. Then, we relax the i.i.d. feedback
delay assumption to allow the feedback delay distribution to be arm-dependent. Under this setting,
we also show that similar logarithmic growth rates of regret and incentive can still be achieved.
• We conduct extensive experiments on Amazon Review Data 1 to demonstrate and verify the per-
formance of our UCB-FDF policy as well as the impacts of delayed feedback on real-world sce-
narios. We also verify our theoretical analysis through various product categories and demonstrate
the efﬁcacy of our proposed UCB-FDF MAB policy.
The rest of the paper is organized as follows. In Section 2, we review the literature to put our work
in comparative perspectives. In Section 3, we formulate our new MAB model that captures the three
common phenomena. In Section 4, we present our UCB-FDF policy and analyze its performance."
INTRODUCTION,0.04945054945054945,1https://nijianmo.github.io/amazon/
INTRODUCTION,0.054945054945054944,Published as a conference paper at ICLR 2022
INTRODUCTION,0.06043956043956044,"Then, we present our experiment settings and results in Section 5. Due to space limitations, the
proofs and part of experiemntal results are relegated to the appendix."
RELATED WORK,0.06593406593406594,"2
RELATED WORK"
RELATED WORK,0.07142857142857142,"In this section, we provide a quick overview on three lines of research related to our work: i) bandits
with delayed feedback, ii) bandits with random preferences, and iii) incentivized bandits."
RELATED WORK,0.07692307692307693,"1) Bandits with Delayed Feedback: Motivated by practical issues in the clinical trials, Eick (1988)
was the ﬁrst to introduce a two-armed bandit model with delayed responses, where the patients’
survival time reports after the treatment are delayed. Recently, Joulani et al. (2013) provided a sys-
tematic study and showed that for delay τ with a ﬁnite expectation, the worst case regret scales with
O(√KT log T +KE[τ]), where K is the number of arms. Meanwhile, Vernade et al. (2017) showed
that stochastic MAB problems with delayed feedback have a regret lower bound O(K log T). How-
ever, this work assumed that the distribution of the random delay is arm-independent. In contrast,
Joulani et al. (2013) considered arm-dependent delay distributions that have an upper bound of the
maximum random delay. More recently, Manegueu et al. (2020) considered arm-dependent and
heavy-tailed delay distributions, where only an upper bound on the tail of the delay distribution is
needed, without requiring the expectation to be ﬁnite. Also, Lancewicki et al. (2021) studied the
case where the delay distribution is reward-dependent, which implies that the random delay in each
round may also depend on the reward received on the same round. However, most of these works
on delayed bandits are based on the standard stochastic MAB framework. In contrast, we consider
delayed feedback in incentivized bandit learning with self-reinforcing user preferences, which is a
more appropriate model for real-world recommender systems than the standard stochastic MAB."
RELATED WORK,0.08241758241758242,"2) Bandits with Random Preferences: The impacts of random user preferences in e-commerce
platforms have received increasing interest in several different areas in learning and economics. Ex-
isting works in (Agrawal et al., 2017; 2019) formulated the user preference variation given different
product bundles by the multi-nomial logit model on top of the bandit learning framework and pro-
posed a Thompson Sampling approach that achieves a worst-case regret bound of O(
√"
RELATED WORK,0.08791208791208792,"NT log TK),
where N is the size of recommended arm bundle. With a different focus on preference modeling,
Barab´asi & Albert (1999); Chakrabarti et al. (2006); Ratkiewicz et al. (2010) investigated the net-
work evolution with “preferential attachment” that formulates the social behavior known as self-
reinforcing preferences, among which the works in (Shah et al., 2018; Zhou et al., 2021) are the
closet to our work. To our knowledge, Shah et al. (2018) was the ﬁrst to consider self-reinforcing
user preferences in bandit learning problems. Later, Zhou et al. (2021) incorporated self-reinforcing
user preferences into the incentivized bandit learning framework. The key difference between these
two works is that, in the model in (Shah et al., 2018), only one arm is revealed to users in each round,
while in the model in (Zhou et al., 2021), all arms are revealed to users and users’ arm selections are
inﬂuenced by incentives. However, both of these works fall short in modeling online recommender
systems in practice as they assume that an arm-sampling feedback is observable in the same time-
slot when an arm is pulled. However, for most e-commerce recommender systems in practice, user
feedbacks are often not immediately observable. As a result, the decision on which arm to pull next
has to be made without some of the feedbacks from arm-pulling actions in the past."
RELATED WORK,0.09340659340659341,"3) Incentivized Bandits: To our knowledge, Frazier et al. (2014) was among the ﬁrst to adopt
incentive schemes into a Bayesian MAB setting. In their model, the agent seeks to maximize time-
discounted total reward by incentivizing arm selections. Later, Mansour et al. (2015) studied the
non-discounted reward setting. For the non-Bayesian setting, Wang & Huang (2018); Zhou et al.
(2021) proposed policies that maximize the total non-discounted reward with bounded incentive
costs. Bandits with budget (Guha & Munagala, 2007; Goel et al., 2009) also share some similarities
with our work, where the agent takes actions under resource constraints that are either ﬁxed or with
a given growth rate bound. However, none of the aforementioned works considered the impacts
of delayed feedback on the regret and incentive costs performances. Note that, due to the loss of
information caused by delayed feedback, larger variances in the mean-reward estimations of the
arms are inevitable. This implies that, in order to achieve a more accurate arm quality estimation
under delayed feedbacks, a higher incentive cost is necessary."
RELATED WORK,0.0989010989010989,Published as a conference paper at ICLR 2022
SYSTEM MODEL,0.1043956043956044,"3
SYSTEM MODEL"
SYSTEM MODEL,0.10989010989010989,"The system has a set of K ≥2 arms denoted by A = {1, . . . , K}, and each arm a follows a
Bernoulli reward distribution Pa with an unknown mean µa > 0. The bandit time horizon has T
rounds. In each time step t = 1, 2, . . . , T, a user arrives and chooses an arm It to pull. Then, the
user will receive a random reward feedback Xt ∼PIt. Both the arm selection It and the feedback
Xt are observable to the agent. We use Ta(t) ≜Pt
i=1 1{Ii=a} to denote the number of times that
arm a is pulled up to time step t. We let Ta(0) = 0, ∀a ∈A. We assume that there is a unique best
arm a∗∈A in the sense that a∗= arg maxa∈A µa and let µ∗= µa∗. Also, we deﬁne ∆a ≜µ∗−µa
as the gap between the mean of the optimal arm and the mean of arm a."
DELAYED FEEDBACK MODELING,0.11538461538461539,"3.1
DELAYED FEEDBACK MODELING"
DELAYED FEEDBACK MODELING,0.12087912087912088,"In this paper, we consider delayed feedback, i.e., when an arm It is pulled at time step t, the corre-
sponding Bernoulli reward Xt is observed after a delay period τIt,t, i.e., the feedback Xt is observed
at time step t + τIt,t. Without loss of generality, we model the random delay time as a random vari-
able τa,t ∼Ta, where the delay distribution Ta of arm a is unknown to the agent."
DELAYED FEEDBACK MODELING,0.12637362637362637,"We consider two settings of delayed feedback. We ﬁrst consider i.i.d. delays {τt}t≤T across time
and arms, i.e., the delay distributions are identical for all arms. Thus, we omit the arm index in
the notations of delay feedback in this setting. Next, we generalize the delay modeling by allowing
arm-dependent delay distributions, where the delay distributions are allowed to differ across arms.
In both settings, we do not make further assumptions on the delay distributions, except that we only
require a ﬁnite delay expectation. Note that we allow the support of the delays to be unbounded,
i.e., an inﬁnite delay time is possible in both settings. This models the practical scenarios in online
recommender systems that some user feedbacks (e.g., ratings and reviews) may never be received."
DELAYED FEEDBACK MODELING,0.13186813186813187,"Under delayed feedbacks, we denote the total number of missing feedbacks from arm a up to a time
step t as Da(t) ≜Pt
s=1 1{s+τa,s>t}. We let D∗
a(t) = max1≤s≤t Da(s), ∀a ∈A as the maximum
total number of delayed feedback for arm a up to time t. Note that D∗
a(t) = 0, ∀a ∈A corresponds
to the non-delayed setting. In this case, Ta(t) denotes the total number of pulling times of arm a
up to time t. At each time step t, the agent observes a set of time-stamped feedback denoted by
St ⊂N × {0, 1}. In the set St, each element is a pair of time index and a Bernoulli reward value,
and the time index is the time step when the corresponding reward is observable. Note that in this
model, by observing the set St, the agent is aware of the information of both the time step when the
feedback is received, and the arm that generated the feedback. We denote the total reward generated
by arm a up to time t as Sa(t) ≜Pt
s=1 Xs · 1{Is=a,s+τa,s≤t}, and let Sa(0) = 0, ∀a ∈A."
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.13736263736263737,"3.2
USER PREFERENCES AND INCENTIVE IMPACT MODELING"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.14285714285714285,"In this paper, we assume that the arrival at time t has a non-zero probability pa(t) ∈(0, 1) to pull
each arm a ∈A. We note that pa(t) can also be thought of as the user’s preference rate of arm a, and
P"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.14835164835164835,"a∈A pa(t) = 1, ∀t ≤T. We adopt the widely accepted multinomial logit model in the economics
literature(Bawa & Shoemaker, 1987) to model arm a’s preference rate at time step t as follows:"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.15384615384615385,"pa(t) =
f
 
Sa(t −1) + θa
 P"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.15934065934065933,"i∈A f
 
Si(t −1) + θi
,
(1)"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.16483516483516483,"where f(·) : R →(0, +∞) is a feedback function that is increasing, and θa > 0 denotes a ﬁxed
initial preference bias of arm a. We note that the preference rate modeling in (Zhou et al., 2021) is
also based on the multinomial logit model, which appears to be in the same form as in (1). However,
the key difference between our preference model in (1) and that in (Zhou et al., 2021) is that the
accumulative award information Si(t −1) in (1) accounts for reward information that can only be
observed up to time t. In other words, Si(t −1) in (1) is affected by feedback delays. In fact, the
preference model in (Zhou et al., 2021) can be viewed as a special case of our model with zero delay."
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.17032967032967034,"Since the arriving users select arms based on preferences, while the agent aims to maximize the total
reward in the long run, there exists a general difference between users’ arm preferences and agent’s
intended arm selection. To induce users to pull arms following the agent’s goal, the agent needs to
intervene users’ arm pulling by offering incentives on its desired arm, so as to increase the user’"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.17582417582417584,Published as a conference paper at ICLR 2022
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.1813186813186813,"preference of pulling the arm. That is, the agent incentivizes arm I′
t at time step t so that pI′
t(t)
increases accordingly. Note that when pI′
t(t) increases, the preference rates on the other arms will
decrease since P"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.18681318681318682,"a∈A pa(t) = 1, t ≤T. In this paper, we adopt the “coupon effect” model, which
is widely used in the economics and marketing literature (Bawa & Shoemaker, 1987). Speciﬁcally,
we consider a ﬁxed incentive b in each time step and denote the time-dependent incentive impact as
g(b, t). Then, the posterior preference rates of the arms with incentive b are updated as follows:"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.19230769230769232,ˆpi(t) =
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.1978021978021978,"



"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.2032967032967033,"


"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.2087912087912088,"pi(t) + g(b, t)"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.21428571428571427,"1 + g(b, t) ,
i = a,"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.21978021978021978,"pi(t)
1 + g(b, t),
i ̸= a.
(2)"
USER PREFERENCES AND INCENTIVE IMPACT MODELING,0.22527472527472528,"We remark that the deﬁnition of the posterior preference update in (2) also follows from the multi-
nomial logit model, which is widely used to model user preferences and their variations in bandit
ﬁeld (Chen & Wang, 2017; Avadhanula, 2019; Dong et al., 2020; Zhou et al., 2021). Based on the
deﬁned posterior preference, as incentive impact g(b, t) increases to inﬁnity (either the incentive
value b increases to inﬁnity or the users are more sensitive to incentives as time goes by), the user
preference will be induced to pulling the agent’s desired arm a with probability one. For further
detailed interpretations of the incentive impact function g(b, t), we refer readers to the literature
(e.g., (Zhou et al., 2021)). Note also that, due to the random user behaviors, it is possible that
I′
t ̸= It, i.e., the arm that the agent incentivizes is not the one that a user pulls eventually. We deﬁne
the accumulative incentive up to time step t as Bt ≜Pt
s=1 bt, where bt ∈{0, b}, ∀t ≤T, denotes
the agent’s binary decision whether to offer incentive b at time step t."
REGRET MODELING,0.23076923076923078,"3.3
REGRET MODELING"
REGRET MODELING,0.23626373626373626,"As in most bandit learning problems, the goal of the agent is to maximize the total expected reward
E
 P"
REGRET MODELING,0.24175824175824176,"a∈A Sa(T)

in the long run. Toward this end, we need the notion of the oracle incentivized
policy, where in hindsight, the agent is aware of the optimal arm a∗and can always offer an inﬁnite
amount of payments to users with feedback being observable immediately, so that the posterior
preference rate of arm a∗is always inﬁnitely close to one. As a result, the expected accumulative
reward generated under the oracle policy up to time T is E[Sa∗(T)] = µ∗· T. However, since the
optimal arm a∗is unknown to the agent, the goal of the agent is to maximize the total expected
reward E[ΓT ] in the long run by designing an incentivized policy with low accumulative incentive
in the presence of self-reinforcing preferences and feedback delay. Similar to conventional MAB,
we measure the performance gap between our accumulative reward against that of the oracle policy,
which is denoted by regret RT . The expected (pseudo) regret is deﬁned as follows:"
REGRET MODELING,0.24725274725274726,"E[RT ] = µ∗· T −E
 X"
REGRET MODELING,0.25274725274725274,"a∈A
Sa(T)

."
REGRET MODELING,0.25824175824175827,"In this paper, our goal is to minimize E[RT ] with low expected accumulative payment E[BT ], i.e.,
sub-linear growth rate regarding time horizon T. It is clear that any policy with bounded payment
cannot outperform the oracle policy. Thus any expected regret deﬁned by comparing with bounded-
payment policy is upper bounded by our regret."
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.26373626373626374,"4
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.2692307692307692,"In this section, we ﬁrst present the general version of the UCB-FDF policy that works with any
delay distributions, where we upper bound the delay impact on the regret and incentive costs. Based
on this general result, we then study the regret and incentive costs performance of UCB-FDF un-
der the assumptions of 1) i.i.d. feedback delay across arms/times and 2) arm-dependent delay
distributions. In both cases, we denote the total number of missing feedbacks over all arms by
D(t) ≜P"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.27472527472527475,"a∈A Da(t), and denote the maximum number of missing feedbacks during the ﬁrst t time
steps by D∗(t) ≜max1≤s≤t D(s). For arm a at time step t, we denote the number of its pulling
times whose feedback is observed by T ′
a(t) = Ta(t) −Da(t), and denote the maximum mean gap
by ∆∗= maxa∈A ∆a. At time step t, we denote the sample mean estimation (due to delayed
feedbacks) of arm a by ˆµa(t) = Sa(t)/T ′
a(t). Our UCB-FDF policy is illustrated in Algorithm 1."
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.2802197802197802,Published as a conference paper at ICLR 2022
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.2857142857142857,Algorithm 1 The UCB-Filtering-with-Delayed-Feedback Policy (UCB-FDF).
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.29120879120879123,"Require: Time horizon T and incentive payment b, the conﬁdence interval of arm a at time step t"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.2967032967032967,"deﬁned as ca(t) =
q"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3021978021978022,"ln T/
 
2T ′a(t)

."
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3076923076923077,"1: Initialization: Incentivize pulling the arms satisfying T ′
a(t) = 0 with incentive payment b until
mina∈A T ′
a(t) ≥1. Let set U = A. Mark current time as t0.
2: Exploration Phase: While |U|>1, remove all the arms from set U satisfying ˆµa(t) + ca(t) ≤
maxi̸=a,i∈U
 
ˆµi(t) −ci(t)

if there is any, then incentivize pulling arm a ∈arg mini∈U T ′
a(t)
with payment b. If |U| = 1, let arm ˆa∗= {a : a ∈U} and mark current time as t1.
3: Exploitation Phase: Incentivize pulling arm ˆa∗with payment b until it dominates: Sˆa∗(t) ≥
P"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3131868131868132,"a̸=ˆa∗
 
Sa(t) + Da(t)

. Mark current time as t2.
4: Self-Sustaining Phase: Users pull arms based on their own preferences until time T."
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.31868131868131866,"UCB-FDF policy contains three phases: an incentivized exploration phase, an incentivized exploita-
tion phase, and a self-sustaining phase. UCB-FDF policy tackles feedback delays in the following
two key aspects: (i) correcting the sample mean estimate of arms by only considering the number of
pulling times that have observed feedback, (ii) setting the length of the exploitation phase in such a
way that the outstanding rewards do not harm the emergence of “dominance” (i.e., one arm receiving
at least half of the rewards) of the sampled optimal arm. Subsequently, these two aspects also inﬂu-
ence the regret and incentive. In order to have enough arm exploration with an unbiased sample mean
estimate, the loss of counted number of pulling times necessitates a carefully designed exploration
phase that incentivizes the pulling of the least informed arm a ∈arg mini∈U T ′
a(t) under delayed
feedbacks. Similarly, the delay-based dominance threshold (i.e., Sˆa∗(t) ≥P"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3241758241758242,"a̸=ˆa∗
 
Sa(t) + Da(t)
"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.32967032967032966,"in Step 3 of Algorithm 1) guarantees the dominance of sampled optimal arm, while also accounts
for a longer exploitation phase to mitigate the delayed feedback effect. We now analyze the upper
bounds of the pseudo regret and expected incentive of the UCB-FDF policy.
Lemma 1. (UCB-Filtering-with-Delayed-Feedback) Given a ﬁxed time horizon T, if g(b, t) > 1,
and f(x) = Θ(xα) with α > 12, then the pseudo regret of Algorithm 1 E[RT ] is upper bounded by"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.33516483516483514,"E[RT ] ≤
X a̸=a∗"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.34065934065934067,"8∆a
 
g(b, 1) −1

+ 8∆∗
 
g(b, 1) −1

∆2a
ln T + g(b, 1)∆∗ 
E[D∗(T)] + 4K
"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.34615384615384615,"g(b, 1) −1
,"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3516483516483517,with the expected payment E[BT ] upper bounded by
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.35714285714285715,"E[BT ] ≤b · 2g(b, 1) + 1"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.3626373626373626,"g(b, 1) −1"
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.36813186813186816,8 ln T
BANDIT POLICY DESIGN AND PERFORMANCE ANALYSIS,0.37362637362637363,"∆2
min
+
X a̸=a∗"
LN T,0.3791208791208791,8 ln T
LN T,0.38461538461538464,"∆2a
+ E[D∗(T)] + 4K

."
LN T,0.3901098901098901,"Remark 1. The UCB-FDF policy achieves a sub-linear total incentive cost by leveraging the prop-
erty of self-reinforcing preference. We can show that as long as the self-reinforcing preference
function f(x) satisﬁes the condition f(x) = Θ(xα) with α > 1, then “monopoly” happens with
probability one (i.e., the scenario where only one arm has positive probability to be pulled, thus
this particular arm is the only preferred arm). A natural incentivizing policy is to incentivize sam-
pled optimal arm until it achieves monopoly. However, the key challenge here is that the onset
of monopoly could take inﬁnite time steps, which implies linear total incentive. Moreover, self-
reinforcing property is not merely disrupting the system from converging to the optimal arm. The
key idea in our UCB-FDF policy design is that under the condition of the self-reinforcing preference
function f(·), after one arm establishes its dominance (i.e., the arm a generates at least half of the
current total reward), it will have exponentially increasing probability to beat other arms and achieve
monopoly. More importantly, we can show that the onset of arm dominance takes sub-linear times,
thus allowing us to achieve sub-linear total incentive costs.
Remark 2. The feedback delay affects the observation of arm dominance, since the missing re-
ward information from suboptimal arms, if not compensated carefully, can potentially destroy the
dominance status of the optimal arm. Thus, to guarantee dominance of the optimal arm, a longer
exploitation phase is necessary, and thus a large total incentive is required."
LN T,0.3956043956043956,"2The notation Θ() in this paper is deﬁned as that, if f(x) = Θ(g(x)), then there exist x0 and two constants
C1, C2 > 0, such that C1g(x) ≤f(x) ≤C2g(x) for all x ≥x0."
LN T,0.4010989010989011,Published as a conference paper at ICLR 2022
LN T,0.4065934065934066,"The existence of delays in our MAB model introduces an additive term Θ(E[D∗(T)]) in both regret
and incentive costs, which is dependent on the maximum accumulated delayed feedback up to time
horizon T. Based on Lemma 1, in what follows, we will analyze the upper bounds of the expected
maximum accumulated delayed feedback under different assumptions on delay distributions."
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.41208791208791207,"4.1
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4175824175824176,"We now analyze the delay impact under our ﬁrst assumption. In the arm-independent case, we
consider an i.i.d. sequence {τt}t of random delay regarding time step t ≤T. We do not make any
assumption on the shape of the delay distribution, except that we only assume a ﬁnite expectation
E[τ1]. Thus, an inﬁnite random delay is possible under this assumption, implying some feedbacks
may never be observed by the agent. Our results show that under this assumption, we can still
achieve similar orders of the regret and incentive costs growth rates, since the key fact is that we can
upper bound the expected number of such unexpectedly large random delays for every time step t."
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4230769230769231,"Existing works (e.g., (Joulani et al., 2013)) provided a systematic study on the delay effect on the
partial monitoring problem with side information, including the stochastic problems. Although these
works only considered the classic stochastic MAB, they share some similarities with our work in
that their analysis of delay effects also leveraged the maximum number of missing feedbacks during
the ﬁrst t time steps D∗(t). However, since our UCB-FDF policy has a different structure compared
to these works on delayed stochastic MAB, their delay analysis is not applicable to our policy. Next,
we restate a result in (Joulani et al., 2013), which will be useful in our analysis.
Lemma 2 (Lemma 2 in Joulani et al. (2013)). Assume {τ1, . . . , τt} is a sequence of i.i.d. random
variables with ﬁnite expected value, and let B(t, s) = s + 2 log t + √4s log t. Then, it holds that"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.42857142857142855,"E[D∗(t)] ≤B(t, E[τ1]) + 1."
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4340659340659341,"Theorem 3. (Arm-Independent Delay) Under i.i.d. delays with a ﬁnite expectation and the condi-
tions of Lemma 1, the pseudo regret of Algorithm 1 E[RT ] is upper bounded by
2g(b, 1)∆∗"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.43956043956043955,"g(b, 1) −1+
X a̸=a∗"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.44505494505494503,"8∆a
 
g(b, 1) −1

+ 8∆∗
 
g(b, 1) −1

∆2a"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.45054945054945056,"
ln T+g(b, 1)∆∗ p"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.45604395604395603,"4E[τ1] ln T + E[τ1] + 4K + 1
"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.46153846153846156,"g(b, 1) −1
,"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.46703296703296704,with the expected payment E[BT ] upper bounded by
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4725274725274725,"b · 2g(b, 1) + 1"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.47802197802197804,"g(b, 1) −1"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4835164835164835,"
2+
8
∆2
min
+
X a̸=a∗ 8
∆2a"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.489010989010989,"
ln T +
p"
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.4945054945054945,"4E[τ1] ln T + E[τ1] + 4K + 1

."
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.5,We note that the gap summation of arms P
ARM-INDEPENDENT DELAY WITH A FINITE EXPECTATION,0.5054945054945055,"a̸=a∗∆a plays an important role in both regret and
total incentive. As arm gaps getting smaller, it is more difﬁcult to distinguish the optimal arm from
others. Thus, a longer exploration phase is required to conduct enough sampling, which implies a
larger regret and a larger total incentive costs. On the other hand, the feedback delay causes additive
terms in both regret and incentive costs in terms of the expected delay E[τ1], and the delay impact
can be upper bounded as long as the expected delay E[τ1] is no larger than time horizon T."
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.510989010989011,"4.2
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5164835164835165,"Now, we further relax the assumption on the delay to allow arm-dependent delays. In this case, the
delay has two key impacts on the system: (i) for each arm, there is a different real-time information
loss when estimating the sample mean, (ii) for the whole arm set, different scales of delay cause an
uneven arm estimation, which results in a larger risk of the elimination of the optimal arm in the
UCB-based exploration step. We formally state our arm-dependent delay assumption as follows:
Assumption 1. The delays of arm a ∈A form an independent delay sequence {τa,t}, where each
element is a random variable satisfying τa,t ∼Ta, with a ﬁnite expectation E[τa,1] < +∞, ∀a ∈A."
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.521978021978022,"Under Assumption 1, we show a more general result on the upper bound of E[D∗(t)] as follows:
Lemma 4. Under Assumption 1, given a ﬁnite number of arms K > 0, it holds that"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5274725274725275,"E[D∗(t)] ≤
X"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.532967032967033,"a∈A
2E[τa,1] + 3K log t K."
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5384615384615384,Published as a conference paper at ICLR 2022
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5439560439560439,"The result in Lemma 4 implies larger upper bounds of the regret and incentive, due to the existence
of the pre-log factor K. This is a consequence of the situation where, as we consider arm-dependent
delay distributions, the worst case could be evenly distributed expected delays E[τa,1] of arm a with
respect to time horizon T. Formally, we state the upper bounds of regret and incentive as follows:"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5494505494505495,"Theorem 5. (Arm-Dependent Delay) Under Assumption 1 and the conditions of Lemma 1, the
pseudo regret of Algorithm 1 E[RT ] is upper bounded by X a̸=a∗"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.554945054945055,"8∆a
 
g(b, 1) −1

+ 8∆∗
 
g(b, 1) −1

∆2a
ln T + g(b, 1)∆∗ 
3K ln T K + P"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5604395604395604,"a∈A 2E[τa,1] + 4K
"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5659340659340659,"g(b, 1) −1
,"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5714285714285714,with the expected payment E[BT ] upper bounded by
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5769230769230769,"b · 2g(b, 1) + 1"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5824175824175825,"g(b, 1) −1"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5879120879120879,"
8
∆2
min
+
X a̸=a∗ 8
∆2a"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5934065934065934,"
ln T + 3K ln T K +
X"
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.5989010989010989,"a∈A
E[τa,1] + 4K

."
ARM-DEPENDENT DELAY WITH FINITE EXPECTATIONS,0.6043956043956044,"Similar to the results under the i.i.d. delay assumption, we can still upper bound the regret and
incentive by an logarithmic growth rate O(log T) under arm-dependent delay. This implies that even
under the weak delay assumption where only ﬁnite expectation is needed, UCB-FDF can estimate
arms without too much bias, and ﬁnally achieve logarithmic regret with logarithmic incentive costs."
EXPERIMENTAL RESULTS,0.6098901098901099,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.6153846153846154,"In this section, we ﬁrst introduce our experiment setting and the dataset, then illustrate our experi-
mental results. Due to the space limit, the full experimental results are provided in Appendix ??."
EXPERIMENTAL SETUP,0.6208791208791209,"5.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.6263736263736264,"1) System Parameters: We conduct experiments under two different delay settings. The system
parameters are set as follows: a three-armed model with Arm1 being the optimal arm, and the initial
preference bias θ = [1, 5, 5], i.e., the optimal arm has the least initial bias. We choose a three-armed
model since large arm set requires a proportional large time horizon to distinguish optimal arm, while
in the public Amazon Review Data, the amount of reviews for most products is limited (no more
than 3,000 for each product). The self-reinforcing preference function is chosen as f(x) = xα with
α = 2. The constant incentive for each time step is set as b = 1.5 with an incentive impact function
g(b, t) = b. For the delay distribution, we use normal distributions in both assumption setting, as
normal distributions have an inﬁnite support x ∈R. Under the arm-independent delay setting, we
choose the delay distribution as τt ∼N(10, 2). Under the arm-dependent delay setting, we choose
the delay distributions as τ1,t ∼N(80, 2), τ2,t ∼N(10, 2), and τ3,t ∼N(10, 2) for Arms 1, 2, and
3, respectively. We only generate non-negative samples of delay under both assumptions."
EXPERIMENTAL SETUP,0.6318681318681318,"Product Category
Arm1(optimal)
Arm2
Arm3"
EXPERIMENTAL SETUP,0.6373626373626373,"Pet Supplies
0.773
0.656
0.626
Electronics
0.757
0.605
0.617
Home and Kitchen
0.875
0.588
0.673
Books
0.915
0.551
0.706"
EXPERIMENTAL SETUP,0.6428571428571429,Table 1: Means of products (arms) in different categories.
EXPERIMENTAL SETUP,0.6483516483516484,"2) Dataset: We use Amazon Review Data (Ni et al., 2019) to provide a practical learning environ-
ment. The Amazon Review Data includes 233 million customer reviews (ratings, posting times) for
29 product categories. In the experiment, we select three products to serve as the arms that have
the largest number of reviews in category Pet Supplies, Electronics, Home and Kitchen, and Books,
respectively. For each product (arm), we leverage the rating and unixReviewTime informa-
tion in each review, and the total number of reviews is 3,000 for each product. The range of ratings
in Amazon Review Data is the discrete set {1, 2, 3, 4, 5}. We convert the rating values to binary by
setting the rating values 1 and 2 as 0 and the rating value 4 and 5 as 1, and the reviews with rating"
EXPERIMENTAL SETUP,0.6538461538461539,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.6593406593406593,"0
1000
2000
3000
Time horizon 0 100 200 300 400"
EXPERIMENTAL SETUP,0.6648351648351648,Average Regret
EXPERIMENTAL SETUP,0.6703296703296703,"Pet Supplies
Electronics
Home and Kitchen
Books"
EXPERIMENTAL SETUP,0.6758241758241759,"0
1000
2000
3000
Time horizon 0 1000 2000 3000 4000"
EXPERIMENTAL SETUP,0.6813186813186813,Average Incentive
EXPERIMENTAL SETUP,0.6868131868131868,"Pet Supplies
Electronics
Home and Kitchen
Books (a)"
EXPERIMENTAL SETUP,0.6923076923076923,"0
1000
2000
3000
Time horizon 0 100 200 300 400"
EXPERIMENTAL SETUP,0.6978021978021978,Average Regret
EXPERIMENTAL SETUP,0.7032967032967034,"Pet Supplies
Electronics
Home and Kitchen
Books"
EXPERIMENTAL SETUP,0.7087912087912088,"0
1000
2000
3000
Time horizon 0 1000 2000 3000 4000"
EXPERIMENTAL SETUP,0.7142857142857143,Average Incentive
EXPERIMENTAL SETUP,0.7197802197802198,"Pet Supplies
Electronics
Home and Kitchen
Books (b)"
EXPERIMENTAL SETUP,0.7252747252747253,"0
1000
2000
3000
Time horizon 0 100 200 300 400"
EXPERIMENTAL SETUP,0.7307692307692307,Average Regret
EXPERIMENTAL SETUP,0.7362637362637363,"Pet Supplies
Electronics
Home and Kitchen
Books"
EXPERIMENTAL SETUP,0.7417582417582418,"0
1000
2000
3000
Time horizon 0 1000 2000 3000 4000"
EXPERIMENTAL SETUP,0.7472527472527473,Average Incentive
EXPERIMENTAL SETUP,0.7527472527472527,"Pet Supplies
Electronics
Home and Kitchen
Books (c)"
EXPERIMENTAL SETUP,0.7582417582417582,"0
1000
2000
3000
Time horizon 0 200 400 600 800 1000"
EXPERIMENTAL SETUP,0.7637362637362637,Average Regret
EXPERIMENTAL SETUP,0.7692307692307693,"Pet Supplies
Electronics
Home and Kitchen
Books"
EXPERIMENTAL SETUP,0.7747252747252747,"0
1000
2000
3000
Time horizon 0 1000 2000 3000 4000"
EXPERIMENTAL SETUP,0.7802197802197802,Average Incentive
EXPERIMENTAL SETUP,0.7857142857142857,"Pet Supplies
Electronics
Home and Kitchen
Books (d)"
EXPERIMENTAL SETUP,0.7912087912087912,"Figure 1: The performance of policy UCB-FDF in the face of no delay in (a), the performance of
policy UCB-FDF in the face of arm-independent delay in (b), the performance of policy UCB-FDF
in the face of arm-dependent delay in (c), and the performance of policy UCB-List in the face of
arm-dependent delay in (d)."
EXPERIMENTAL SETUP,0.7967032967032966,"value 3 are removed. For each product, the binary review ratings are sorted by unixReviewTime,
so the ratings come in real-world order in the experiment. We summarize the mean values of the
products by their Bernoulli ratings in the four selected categories, as shown in Table 1."
RESULTS AND DISCUSSIONS,0.8021978021978022,"5.2
RESULTS AND DISCUSSIONS"
RESULTS AND DISCUSSIONS,0.8076923076923077,"1) Results: The experiment results are illustrated in Figure 1. (a) shows the average regret and
incentive trends with policy UCB-FDF under setting with no delay. (b) and (c) show the average
regret and incentive trends with policy UCB-FDF under settings with arm-independent delays and
arm-dependent delays, respectively. In Figures (c) and (d), we compare the performances with
policy UCB-FDF and baseline policy UCB-List (Zhou et al., 2021). Speciﬁcally, Figure (d) shows
the performance under policy UCB-List in the face of arm-dependent delays that is the same as that
in (c). Each curve is constructed by regret or incentive values with different time horizons from
T = 150 to T = 3000, incremented by 150. Each node value in curves are averaged by 100 trials."
RESULTS AND DISCUSSIONS,0.8131868131868132,"2) Discussion: Comparing (a) with (b) and (c) in Figure 1, we can observe the delay impact on
regret and total incentive, that both the regret and total incentive are increased due to the delayed
feedback. Comparing (c) and (d) in Figure 1, we observe that under the bandit instances in the face
of same delayed feedback, our policy UCB-FDF reaches sub-linear growth rate in both regret and
total incentive, except the total incentive in category Pet Supplies, since it may require more time
steps to converge while our data is limited, while the policy UCB-List cannot guarantee sub-linear
growth rate for both regret and total incentive."
CONCLUSION,0.8186813186813187,"6
CONCLUSION"
CONCLUSION,0.8241758241758241,"In this work, we proposed a practical bandit model that considers the joint effect of the incentive
impact, delayed feedback and self-reinforcing user preferences in real-world recommender systems.
We proposed a UCB-FDF policy that achieves logarithmic growth rates of pseudo regret and to-
tal incentive costs for a ﬁxed time horizon T. We also analyzed how different delay assumptions
inﬂuence the regret and incentive costs. Speciﬁcally, we considered arm-independent delays and
arm-dependent delays with weak assumption that only requires a ﬁnite expectation. The evaluations
with real-world customer review data showed the effectiveness of our UCB-FDF policy in achieving
sub-linear regret while spending only sub-linear total incentive costs under delayed feedback."
CONCLUSION,0.8296703296703297,ACKNOWLEDGMENTS
CONCLUSION,0.8351648351648352,"This work has been supported in part by NSF grants CAREER CNS-2110259, CNS-2112471, CNS-
2102233, CCF-2110252, and a Google Faculty Research Award."
CONCLUSION,0.8406593406593407,Published as a conference paper at ICLR 2022
REFERENCES,0.8461538461538461,REFERENCES
REFERENCES,0.8516483516483516,"Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for the
mnl-bandit. In Conference on Learning Theory, pp. 76–78. PMLR, 2017."
REFERENCES,0.8571428571428571,"Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi.
Mnl-bandit: A dynamic
learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019."
REFERENCES,0.8626373626373627,"Vashist Avadhanula. The MNL-Bandit Problem: Theory and Applications. Columbia University,
2019."
REFERENCES,0.8681318681318682,"Albert-L´aszl´o Barab´asi and R´eka Albert. Emergence of scaling in random networks. science, 286
(5439):509–512, 1999."
REFERENCES,0.8736263736263736,"Kapil Bawa and Robert W Shoemaker. The effects of a direct mail coupon on brand choice behavior.
Journal of Marketing Research, 24(4):370–376, 1987."
REFERENCES,0.8791208791208791,"Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual
bandits. arXiv preprint arXiv:1904.10040, 2019."
REFERENCES,0.8846153846153846,"S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721, 2012."
REFERENCES,0.8901098901098901,"Soumen Chakrabarti, Alan Frieze, and Juan Vera. The inﬂuence of search engines on preferential
attachment. Internet Mathematics, 3(3):361–381, 2006."
REFERENCES,0.8956043956043956,"Xi Chen and Yining Wang. A note on a tight lower bound for mnl-bandit assortment selection
models. arXiv preprint arXiv:1709.06109, 2017."
REFERENCES,0.9010989010989011,"Kefan Dong, Yingkai Li, Qin Zhang, and Yuan Zhou. Multinomial logit bandit with low switching
cost. In International Conference on Machine Learning, pp. 2607–2615. PMLR, 2020."
REFERENCES,0.9065934065934066,"Stephen G Eick. The two-armed bandit with delayed responses. The Annals of Statistics, pp. 254–
264, 1988."
REFERENCES,0.9120879120879121,"Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In
Proceedings of the ﬁfteenth ACM conference on Economics and computation, pp. 5–22, 2014."
REFERENCES,0.9175824175824175,"Ashish Goel, Sanjeev Khanna, and Brad Null. The ratio index for budgeted learning, with applica-
tions. In Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms, pp.
18–27. SIAM, 2009."
REFERENCES,0.9230769230769231,"Sudipto Guha and Kamesh Munagala. Approximation algorithms for budgeted learning problems.
In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pp. 104–113,
2007."
REFERENCES,0.9285714285714286,"Pooria Joulani, Andras Gyorgy, and Csaba Szepesv´ari. Online learning under delayed feedback. In
International Conference on Machine Learning, pp. 1453–1461. PMLR, 2013."
REFERENCES,0.9340659340659341,"Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances
in applied mathematics, 6(1):4–22, 1985."
REFERENCES,0.9395604395604396,"Tal Lancewicki, Shahar Segal, Tomer Koren, and Yishay Mansour. Stochastic multi-armed bandits
with unrestricted delay distributions. arXiv preprint arXiv:2106.02436, 2021."
REFERENCES,0.945054945054945,"Anne Gael Manegueu, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits
with arm-dependent delays. In International Conference on Machine Learning, pp. 3348–3356.
PMLR, 2020."
REFERENCES,0.9505494505494505,"Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit
exploration. In Proceedings of the Sixteenth ACM Conference on Economics and Computation,
pp. 565–582, 2015."
REFERENCES,0.9560439560439561,Published as a conference paper at ICLR 2022
REFERENCES,0.9615384615384616,"Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled
reviews and ﬁne-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pp. 188–197, 2019."
REFERENCES,0.967032967032967,"Jacob Ratkiewicz, Santo Fortunato, Alessandro Flammini, Filippo Menczer, and Alessandro Vespig-
nani. Characterizing and modeling the dynamics of online popularity. Physical review letters, 105
(15):158701, 2010."
REFERENCES,0.9725274725274725,"Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. arXiv
preprint arXiv:1802.05693, 2018."
REFERENCES,0.978021978021978,"Min Shao and Chrysostomos L Nikias. Signal processing with fractional lower order moments:
stable processes and their applications. Proceedings of the IEEE, 81(7):986–1010, 1993."
REFERENCES,0.9835164835164835,"Claire Vernade, Olivier Capp´e, and Vianney Perchet. Stochastic bandit models for delayed conver-
sions. arXiv preprint arXiv:1706.09186, 2017."
REFERENCES,0.989010989010989,"Siwei Wang and Longbo Huang.
Multi-armed bandits with compensation.
arXiv preprint
arXiv:1811.01715, 2018."
REFERENCES,0.9945054945054945,"Tianchen Zhou, Jia Liu, Chaosheng Dong, and Jingyuan Deng. Incentivized bandit learning with
self-reinforcing user preferences. arXiv preprint arXiv:2105.08869, 2021."
