Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00684931506849315,"Federated Learning (FL) is a recently proposed learning paradigm for decentralized
devices to collaboratively train a predictive model without exchanging private data.
Existing FL frameworks, however, assume a one-size-ﬁt-all model architecture to
be collectively trained by local devices, which is determined prior to observing their
data. Even with good engineering acumen, this often falls apart when local tasks are
different and require diverging choices of architecture modelling to learn effectively.
This motivates us to develop a novel personalized neural architecture search (NAS)
algorithm for FL. Our algorithm, FEDPNAS, learns a base architecture that can be
structurally personalized for quick adaptation to each local task. We empirically
show that FEDPNAS signiﬁcantly outperforms other NAS and FL benchmarks on
several real-world datasets."
INTRODUCTION,0.0136986301369863,"1
INTRODUCTION"
INTRODUCTION,0.02054794520547945,"Federated Learning (FL) (McMahan et al., 2017) is a variant of distributed learning where the
objective function can be decomposed into a linear combination of M local objective functions. Each
function depends on its private data hosted by a local client and a set of shared parameters w,"
INTRODUCTION,0.0273972602739726,"argmin
w
L(w)
≡
argmin
w M
X"
INTRODUCTION,0.03424657534246575,"i=1
Li(w | Di) ,
(1)"
INTRODUCTION,0.0410958904109589,"where Di denotes the ith local training dataset comprising input-output tuples (x, y). In a standard
supervised learning task where the predictive model is modeled as a ﬁxed deep neural network ψ
with learnable weights w, let ℓ(x, y) denote the loss incurred by predicting ψ(x; w) when the true
output is y. The expected loss of ψ(x; w) on Di is given as"
INTRODUCTION,0.04794520547945205,"Li(w | Di)
=
E(x,y)∼Di
h
ℓ(x, y; ψ)
i
.
(2)"
INTRODUCTION,0.0547945205479452,"This is not applicable to scenarios where local models are expected to solve different tasks which are
similar in broad sense yet diverge in ﬁner details. For example, consider the task of recognizing the
outcome of a coin ﬂip given images collected by two clients: one capture the coin from above, the
other from below. This setting implies that when the same input image is provided by both clients,
the correct classiﬁcations must be the opposite of one another. However, since existing FL methods
converge on a single model architecture and weight, there can only be one predictive outcome which
cannot satisfy both tasks."
INTRODUCTION,0.06164383561643835,"To relax this constraint, the recent work of Fallah et al. (2020) extends FL by incorporating ideas
from meta learning (Finn et al., 2017) which results in a new framework of personalized FL. The new
framework can accommodate for such task heterogeneity but still requires all client models to agree
on a single architecture beforehand, which is sub-optimal. To address this shortcoming, one naive
idea is to adopt existing ideas in Neural Architecture Search (NAS) via Reinforcement Learning
(Zoph and Le, 2016; Pham et al., 2018) which act as an outer loop to the existing FL routine."
INTRODUCTION,0.0684931506849315,"However, this simple approach does not allow client models to adapt to local tasks on an architecture
level and is often not preferred due to the cost of repeated FL training. This paper proposes a novel
personalized NAS algorithm for federated learning, which generalizes ideas in respective areas of
NAS (Zoph and Le, 2016; Pham et al., 2018) originally developed for single-task scenarios, and"
INTRODUCTION,0.07534246575342465,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0821917808219178,"FL (Fallah et al., 2020) under a uniﬁed len of federated personalized neural architecture search
(FEDPNAS)."
INTRODUCTION,0.08904109589041095,"In particular, to customize the model architecture for each task in the FL workﬂow, FEDPNAS
ﬁrst represents the model architecture for each task as a sub-network sampled from a large, over-
parameterized network. The sampling distribution is (collaboratively) learned along with the pa-
rameters of the sampled network via a generalization of the recently proposed Discrete Stochastic
NAS (DSNAS) method (Hu et al., 2020). Unlike DSNAS, which lacks the ability to customize
architecture for individual tasks, our generalized FEDPNAS incorporates model personalization on
an architecture level. Our contributions include:"
INTRODUCTION,0.0958904109589041,"1. A novel architecture that factorizes into a base component (shared across tasks) and a personalizable
component, which respectively capture the task-agnostic and task-speciﬁc information (Section 3.2)."
INTRODUCTION,0.10273972602739725,"2. A context-aware sampling distribution conditioned on speciﬁc task instance, which captures task-
speciﬁc information and naturally incorporates personalization into architecture search (Section 3.4)."
INTRODUCTION,0.1095890410958904,"3. An FL algorithm that optimizes for a common architecture, followed by a personalization phase
where each client subsequently adapts only the personalized component to ﬁt its own task via ﬁne-
tuning with local data (Section 3.1). To ensure that the common architecture distribution converges at
a vantage point that is relevant and beneﬁcial to all clients, we generalize the vanilla FL objective in
Eq. equation 1 such that local gradient steps directly optimize for expected improvement resulting
from future ﬁne-tuning (Section 3.3)."
INTRODUCTION,0.11643835616438356,"4. A theoretical perspective on our FL objective (Section 3.5 and thorough empirical analysis showing
signiﬁcant performance gain compared to state-of-the-art FL and NAS methods (Section 4)."
RELATED WORKS,0.1232876712328767,"2
RELATED WORKS"
TWO-STAGE NEURAL ARCHITECTURE SEARCH,0.13013698630136986,"2.1
TWO-STAGE NEURAL ARCHITECTURE SEARCH"
TWO-STAGE NEURAL ARCHITECTURE SEARCH,0.136986301369863,"Most existing NAS frameworks separately optimize for the optimal architecture and its parameters
in two stages: searching and evaluation. The former stage usually employs evolutionary-based
strategies (Floreano et al., 2008; Real et al., 2019), Bayesian optimization surrogates (Bergstra et al.,
2013; Hu et al., 2018) or Reinforcement Learning controllers (Baker et al., 2016; Zoph and Le, 2016;
Pham et al., 2018) to propose candidate architectures based on random mutations and/or observed
experience; while the latter optimizes the parameters of these architectures given task data and provide
feedback to improve the search agent. Naturally, an extension of such methods to the FL setting is
through distributing the evaluation workload over many clients, which does not require exposing
private data. In practice, however, two-stage federated NAS frameworks are generally not suitable
for the personalized FL setting for two reasons: (a) the clients often lack the computational capacity
to repeatedly optimize the parameters for many candidate architectures; and (b) the clients have to
converge on a single architecture proposed by the central search agent."
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.14383561643835616,"2.2
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.1506849315068493,"Discrete stochastic neural architecture search (DSNAS) (Hu et al., 2020) addresses the computational
issue of two-stage NAS by jointly optimizing the optimal architecture and its weight in an end-to-end
fashion, which allows users to continually train a single network on demand over time as opposed to
performing full parameter optimization for every candidate until a good architecture is discovered."
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.15753424657534246,"The main idea of DSNAS is to combine weight training for an over-parameterized master architecture
with discrete computational path sampling. DSNAS parameterizes the master architecture as a
stack of modular cells: ψ(x) = ψC ◦ψC−1 · · · ◦ψ1 (x)1, where x is an arbitrary input, C is the
number of cells, ψt denotes the tth cell in the stack, and ◦denotes the compositional operator. The
inner computation of ψt is in turn characterized by a directed acyclic graph (DAG) with V nodes
{vi}|V |
i=1, where each node represents some intermediate feature map. For each directed edge (vi, vj),"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.1643835616438356,"1Although each DSNAS cell receives the outputs of two previous cells as inputs, we simplify the number to
one for ease of notation."
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.17123287671232876,Under review as a conference paper at ICLR 2022
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.1780821917808219,"Figure 1: Our proposed method FEDPNAS consists of (1) a federated learning phase, where each
client updates both the base component (ψb) and the personalized component (ψp) the architecture
using the FEDPNAS update (Section 3.3) and sends its parameters to the central server for aggrega-
tion; and (2) a ﬁne-tune phase, where each client updates only the personalized component of the
architecture using standard gradient update."
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.18493150684931506,"there is an associated list of D possible network operations Oij =

o1
ij, o2
ij . . . oD
ij
2 where each
operation ok
ij transforms vi to vj. Here, v1 corresponds to the output of previous cell ψt−1 (or
input x when t = 1). We recursively deﬁne intermediate nodes vj = Pj−1
i=1 Z⊤
ijOij(vi), where
Oij(vi) ≜

o1
ij(vi), o2
ij(vi) . . . oD
ij(vi)

and Zij is a one-hot vector sampled from the categorical
distribution p(Z | Π) where the event probabilities Π = {π1, π2, . . . , πD | PD
i=1 πi = 1} are
learnable. Essentially, learning the distribution p(Z) allows us to sample computational paths or
sub-graphs of the original DAG that correspond to high-performing, compact architecture from the
over-parameterized master network. Sampling discrete random variables from p(Z), however, does
not result in a gradient amenable to back-propagation. To sidestep this issue, DSNAS adopts the
straight-through Gumbel-softmax trick (Jang et al., 2016), which re-parameterizes the kth index of"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.1917808219178082,"the one-hot variable as Zij[k] = I

k ≜arg max
t"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.19863013698630136,"h
gt + log πt
i
, where gt ∼Gumbel(0, 1). While"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.2054794520547945,"this forward computation does not have a gradient by itself, we can estimate the gradient through a
proxy during the backward pass:"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.21232876712328766,"∇Zij[k] ≃∇˜Zij[k]
≜
∇"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.2191780821917808,"exp ((gk + log πk) /τ)
PD
t=1 exp ((gt + log πt) /τ) ! (3)"
DISCRETE STOCHASTIC NEURAL ARCHITECTURE SEARCH,0.22602739726027396,"which is unbiased when converged as the temperature τ is steadily annealed to 0 (Jang et al., 2016).
This formulation, however, is not easily extended to the FL setting, especially when local tasks are not
homogeneous. The key challenges in doing so are described in Section 3, together with our proposed
approaches."
PERSONALIZED NAS FOR FEDERATED LEARNING,0.2328767123287671,"3
PERSONALIZED NAS FOR FEDERATED LEARNING"
FEDERATED LEARNING OF DSNAS,0.23972602739726026,"3.1
FEDERATED LEARNING OF DSNAS"
FEDERATED LEARNING OF DSNAS,0.2465753424657534,"Let W denote the concatenated weights of all network operations in the network architecture. The
set up above of DSNAS (Jang et al., 2016) is then naïvely extendable to a FL setting via the following
objective formulation:"
FEDERATED LEARNING OF DSNAS,0.2534246575342466,"argmin
W,Π
L(W, Π) ≡argmin
W,Π"
M,0.2602739726027397,"1
M M
X"
M,0.2671232876712329,"i=1
Li(W, Π | Di) .
(4)"
M,0.273972602739726,"McMahan et al. (2017) optimizes this objective by alternating between (a) central agent broadcasting
aggregated weights to local clients and (b) local clients sending gradient descent updated weights
(given local data) to the central agent for aggregation. This, however, implies that after the last"
M,0.2808219178082192,2We drop the cell index in the deﬁnition operation for ease of notation.
M,0.2876712328767123,Under review as a conference paper at ICLR 2022
M,0.2945205479452055,"central aggregation step, all clients will follow the same architecture distribution induced by the ﬁnal
broadcasted copy of W and Π. As previously argued, this is not optimal in a heterogenous task
setting which requires task-speciﬁc adaptation for local clients to achieve good performance."
M,0.3013698630136986,"Furthermore, having the same sampling distribution p(Z) regardless of context (i.e., feature maps
received as cell input) limits the architecture discovery to those that perform reasonably on average
over the entire dataset. However, we remark that restricting the architecture to be the same for every
input samples is unnecessary and undermines the expressiveness of an over-parameterized search
space. On the other hand, letting the architecture be determined on a per-sample basis makes better
use of the search space and potentially improves the predictive performance."
M,0.3082191780821918,"The focus of this work is therefore to incorporate both task-wise and context-wise personalization to
federated neural architecture search in multitask scenarios, which is achieved through our proposed
algorithm FEDPNAS. In general, FEDPNAS functions similarly to the vanilla FEDDSNAS algo-
rithm described above, with an addition of a ﬁne-tuning phase at each local client after the FL phase
to adapt the aggregated common model for local task data, as shown in Fig. 1. To make this work,
however, we need to address the following key challenges:"
M,0.3150684931506849,"C1. First, as previously argued in Section 1, tasks across federated clients tend to share similarities in
broad sense, and diverge in ﬁner details. A good federated personalization search space, therefore,
need to capture this fundamental observation through design and appropriate resource distribution.
We address this challenge in Section 3.2."
M,0.3219178082191781,"C2. Second, a major advantage of having an over-parameterized architecture search space is the
ﬂexibility of having speciﬁc computation paths for different samples, which is not exploited by
DSNAS as reﬂected in its choice of context-independent sampling distribution p(Z). To address
this, Section 3.4 proposes a novel parameterization of p(Z) to incorporate context information into
operator sampling."
M,0.3287671232876712,"C3. Last, while the ﬁne-tuning phase is designed to incorporate task-personalization, there is no
guarantee that the common model can be quickly adapted to client tasks (Fallah et al., 2020). The
common model may end up in a localization that favors one client over another, which makes it
difﬁcult for the latter to ﬁne-tune. To address this concern, Section 3.3 proposes a new personalized
federated NAS objective inspired by Finn et al. (2017) to optimize the common model in anticipation
of further ﬁne-tuning by the client models."
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3356164383561644,"3.2
PERSONALIZABLE ARCHITECTURE SEARCH SPACE"
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3424657534246575,"Similar to DSNAS (Hu et al., 2020), our frame-
work adopts a cell-based representation (Sec-
tion 2.2) to trade-off search space expressive-
ness for efﬁciency, which is extremely suit-
able for FL where clients tend to have low-end
computational capacity. Unlike the original de-
sign which assumes similar role for every cell
in the architecture stack (i.e., as reﬂected by
their choice of fully factorizable path sampling
distribution p(Z)), we instead split our cell
stack into two components with separate meta-
roles catering to the federated personalization
task: (a) a base stack ψb = {ψb
1, ψb
2 . . . ψb
Cb}
which aims to capture the broad commonali-
ties of data samples across client tasks; and (b)
personalized stack ψp = {ψp
1, ψp
2 . . . ψp
Cp},
which will be ﬁne-tuned with local data to cap-
ture task-speciﬁc details."
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3493150684931507,"Figure 2: Feature mapping down the component
stacks of our architecture space. Every base cell
takes as inputs (a) the outputs from its immediate
predecessor and (b) the one before it through a
skip-ahead connection. On the other hand, every
personalization cell takes as input only the output
from the previous cell."
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3561643835616438,"We explain the main difference between these components to account for different level of expres-
siveness requirements below:"
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.363013698630137,"Base stack. Every cell ψb
t in the base stack takes as inputs the outputs of its previous two cells ψb
t−1
and ψb
t−2 (replaced with raw input x when necessary for t ≤2). The output of the skip-ahead cell"
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3698630136986301,Under review as a conference paper at ICLR 2022
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3767123287671233,"ψb
t−2 is additionally passed through a 1 × 1 convolution layer as a cost-effective way to control the
number of channels. Additionally, the operators available to the base cell include large convolution
layers with size 5 × 5 and 7 × 7. To compensate for the growing number of channels, we periodically
employ a reduction convolution (with stride larger than 1) similar to DSNAS (Hu et al., 2020) to
reduce the feature dimension down the stack."
PERSONALIZABLE ARCHITECTURE SEARCH SPACE,0.3835616438356164,"Personalized stack. As opposed to the design of the base cells above, every cell ψp
t in the personal-
ized stack has minimal expressiveness. That is, ψp
t excludes large operators and only takes as input
the output of its immediate predecessor ψp
t−1 (or ψb
Cb when t = 1). There are two reasons for this
choice. First, as the ﬁne-tuning phase has access to fewer data samples than the federated phase,
having a more compact ﬁne-tuning space helps to improve the rate of convergence. Second, as we will
discuss in Section 3.4 below, our personalized FL objective requires the Hessian of the personalized
parameters, which is computationally expensive. As such, we only restrict the personalization to
happen on the more compact personalized stack."
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.3904109589041096,"3.3
PERSONALIZED FEDERATED LEARNING OBJECTIVE"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.3972602739726027,"Unlike FEDAVERAGING (McMahan et al., 2017), which assumes the clients will follow the consensus
base model obtained after the federated phase, FEDPNAS expects clients to further personalize
the base model with local task data. That said, while the base model is trained to work well in the
expected sense over the task distribution, there is no guarantee that it is a good initial point for every
client model to improve upon via ﬁne-tuning. To address this, we adopt the concept of training in
anticipation of future adaptation introduced by MAML (Finn et al., 2017). That is, during client
update, instead of optimizing the loss with respect to the same consensus weight, each client will
instead optimize the weight perturbed by a small gradient step in the ﬁne-tuning direction."
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4041095890410959,Algorithm 1 FEDPNAS - FEDERATED PHASE
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.410958904109589,"1: CENTRALAGGREGATION:
2: θ0 ←INITIALIZEPARAMETER
3: for t = 1, 2 . . . Ts do
4:
for k = 1, 2 . . . M in parallel do
5:
θk
t ←CLIENTUPDATE(k, θt−1)
6:
θt ←PM
k=1
1
M θk
t"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4178082191780822,"7: CLIENTUPDATE(k, θ):
8: for t = 1, 2 . . . Tc do
9:
for batch (x, y) ∈Dk do
10:
Lk, ∇Lk ←EVAL(x, y; θb, θp)
11:
˜θp ←GRADUPDATE(∇˜θpLk)"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4246575342465753,"12:
˜Lk, ∇˜Lk ←EVAL(x, y; θb, ˜θp)
13:
∇θp ˜Lk ←EQ. 5"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4315068493150685,"14:
θb, θp ←GRADUPDATE(∇θb,θp ˜Lk)
15: return θ to central server"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4383561643835616,Algorithm 2 FEDPNAS - EVAL
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4452054794520548,"1: Input: x, y, θ
2: SKIP, PREV ←x, x
3: W, Π ←θ
4: for CELL ψ ∈ψ do
5:
Z ←SAMPLEOPS (x, PREV; Π)
6:
ψ ←EXTRACTCHILDNET (Z, W)"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4520547945205479,"7:
if ψ ∈ψb then
8:
OUTPUT ←ψ(PREV, SKIP)
9:
else if ψ ∈ψp then
10:
OUTPUT ←ψ(PREV)
11:
SKIP ←PREV
12:
PREV ←OUTPUT
13: L ←LOSS(OUTPUT, y)
14: ∇L ←BACKPROP(L)
15: return L, ∇L"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4589041095890411,"For simplicity, let θ = {θb, θp} respectively denote all trainable parameters of the base stack and the
personalized stack, i.e., θb = {Wb, Πb}, θp = {Wp, Πp}. The personalized FL objective at client i
is then given by ˜Li(θb, θp) ≜Li(θb, ˜θp) where ˜θp ≜˜θp −η∇θpLi(θb, θp) adjusts the parameters of
the personalized component to account for a small ﬁne-tuning gradient step. The adjusted local loss
only depends on the respective client data and is amenable to federated learning. The local update
gradient, however, involves a Hessian term whose computation is expensive to repeat over many
epochs. To circumvent this problem, we use the ﬁrst-order Taylor approximation to estimate the
Hessian term by the outer product of Jacobian, which results in a gradient that requires exactly two
forward/backward passes to compute:"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4657534246575342,"∇θp ˜Li
=

∇θp ˜θp
 
∇˜θp ˜Li
"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4726027397260274,"=

I −η∇2
θpLi
 
∇˜θp ˜Li
"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4794520547945205,"≃

I −η∇⊤
θpLi∇θpLi
 
∇˜θp ˜Li
 (5)"
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4863013698630137,Under review as a conference paper at ICLR 2022
PERSONALIZED FEDERATED LEARNING OBJECTIVE,0.4931506849315068,"where Li and ˜Li are short-hands for Li(θb, θp) and ˜Li(θb, θp) respectively. The FL phase of our
FEDPNAS framework is detailed in Alg. 1. An instance of FEDPNAS’s forward and backward pass
which sequentially unrolls down the component stacks, alternating between sampling and evaluation,
is in turn given in Alg. 2."
CONTEXT-AWARE OPERATOR SAMPLER,0.5,"3.4
CONTEXT-AWARE OPERATOR SAMPLER"
CONTEXT-AWARE OPERATOR SAMPLER,0.5068493150684932,"The choice of a fully factorizable sampling distribution p(Z) in DSNAS follows that of SNAS (Xie
et al., 2018), which argues that the Markov assumption for p(Z) is not necessary because NAS
has fully delayed rewards in a deterministic environment. However, this generally only holds for
two-stage NAS (Section 2.1) and does not apply to end-to-end frameworks such as SNAS and DSNAS.
We instead to take advantage of the over-parameterized architecture via factorizing the conditional
p(Z | x), which takes into account the temporal dependency of structural decisions:"
CONTEXT-AWARE OPERATOR SAMPLER,0.5136986301369864,"p(Z | x)
=
p(Z1 | x) C
Y"
CONTEXT-AWARE OPERATOR SAMPLER,0.5205479452054794,"t=2
p(Zt | Zt−1 . . . Z1, x)"
CONTEXT-AWARE OPERATOR SAMPLER,0.5273972602739726,"≃
p(Z1 | x) C
Y"
CONTEXT-AWARE OPERATOR SAMPLER,0.5342465753424658,"t=2
p(Zt | vt
1, x)"
CONTEXT-AWARE OPERATOR SAMPLER,0.541095890410959,"=
p(Z1 | x) C
Y t=2 Y"
CONTEXT-AWARE OPERATOR SAMPLER,0.547945205479452,"(i,j)
p(Zt
ij | vt
1, x) ,
(6)"
CONTEXT-AWARE OPERATOR SAMPLER,0.5547945205479452,"where Zt, Zt
ij and vt
1 respectively denote all the samples, the sample at edge (i, j) and the input at
cell ψt. We have also assumed a single stack setting since the parameterization of p(Z) does not
differ between base and personalized cells."
CONTEXT-AWARE OPERATOR SAMPLER,0.5616438356164384,"To reduce computational complexity, instead of conditioning the samples of subsequent cells on
previous Z samples, we approximate p(Zt | Zt−1 . . . Z1, x) ≃p(Zt | vt
1, x) by the assumption
that the cell contents are conditionally independent given the immediate cell input and the original
input. Finally, we assume that p(Zt | vt
1, x) is fully factorizable across edges in the same cell and
parameterize p(Zt
ij | vt
1, x) = φ(i,j)(vt
1, x) where φ is a deep classiﬁcation network whose output
dimension equal the number of edges in cell ψt. Samples of Zt
ij can then be generated using the
straight-through Gumbel-softmax reparameterization similar to Jang et al. (2016)."
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.5684931506849316,"3.5
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.5753424657534246,"Finally, we analyze the connection of our gradient update framework to the standard gradient update,
and explain why it is critical in achieving a vantage point that improves average objective value
without compromising any local objective. First, we note that the gradient update Eq. 5 in Section 3.3
at the t-th iteration can be written as:"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.5821917808219178,"θt+1
p
=
θt
p −η2 M M
X"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.589041095890411,"i=1
∇θpLi(θt
b, ˜θt
p,i) + η1η2 M M
X"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.5958904109589042,"i=1
αi∇θpLi(θt
b, θt
p) ,"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6027397260273972,"and
θt+1
b
=
θt
b −η2 M M
X"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6095890410958904,"i=1
∇θbLi(θt
b, ˜θt
p,i) ,
(7)"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6164383561643836,"where ˜θt
p,i denotes the i-th local personalized parameters; η1 and η2 are two separate learning
rates and αi ≜∇⊤
θpL(θt
b, θt
p,i)∇θp,iL(θt
b, ˜θt
p,i) (See Appendix A for detailed derivation). This
implies that our federated personalize update corresponds to a federated update scheme with three
gradient steps: (1) θp takes a local gradient (w.r.t. locally updated parameters) step of size η1; (2)
Both θb and θp take a federated gradient (w.r.t. server-wide parameters averaging) step of size η2;
(3) θp takes a weighted federated gradient step of size η1η2, where the weight of client i is given by αi."
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6232876712328768,"Explicitly, Step 1 and 2 together comprise a special instance of FEDAVERAGING (McMahan et al.,
2017), where θb take one gradient step for every two gradient steps taken by θp. Step 3, on the other"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6301369863013698,Under review as a conference paper at ICLR 2022
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.636986301369863,"hand, takes the information of the two gradient steps of θp and adjust the magnitude of the local
gradient step (whose direction is given by ∇θpLi(θt
b, θt
p)) accordingly to trade-off between preserving
local objective value and improving average objective value. We then theorize the scenario in which
such an update is beneﬁcial and state the following assumption to lay the foundation of our analysis:"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6438356164383562,"Assumption 1 For a ﬁxed instance of θb, let ˜θp,i = θp,i −η∇θpL(θb, θp,i) denote the personalized
parameters after a local update step (i.e., step 1 above) at client i, then there exists a distribution S
on matrix S ∈Rk×|θp| that satisﬁes"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6506849315068494,"∀x ∈Rn, ∥x∥2 = 1 : ES∼S

|∥Sx∥2
2 −1|ℓ
≤
ϵℓ· δ ,
(8)"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6575342465753424,"Pr
S∈S"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6643835616438356,"∇θpL(θb, ˜θp,i) −S⊤S
 1 M M
X"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6712328767123288,"i=1
∇θpL(θb, ˜θp,i)
 ≤ r 6
kδ !"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.678082191780822,"≥
1 −δ
(9)"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.684931506849315,"where k = O
 
C∥θb −θ∗
b∥−2
2

for some constant C > 0 and θ∗
b denotes the optimal base parameters."
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6917808219178082,"The above assumption implies that, as θb improves and better captures the broad similarity across
tasks, the local personalized components will diverge to capture the differences. It then becomes less
likely for the FEDAVG personalized gradient to capture all these differences simultaneously. That
is, suppose there exists an afﬁne transformation to reconstruct the local component ∇θpLi(θt
b, ˜θt
p)
from the federated gradient L ≜
1
M
PM
i=1 ∇θpLi(θt
b, ˜θt
p), then the rank of this afﬁne transformation
would be inversely proportionate to ∥θb −θ∗
b∥2."
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.6986301369863014,"Finally, Proposition 1 below shows that when this assumption holds and θb converges to the optimal
parameter θ∗
b (i.e., the error term tends to 0), then with very high probability, the coefﬁcient αi of the
weighted federated gradient step (i.e., step 3 above) accurately captures the cosine similarity between
the local gradient (step 1) and the federated gradient (step 2)."
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.7054794520547946,"Proposition 1 Suppose assumption 1 holds, then with probability at least 1 −2δ and normalized
gradients, we have:
αi −∇⊤
θpLi(θt
b, θt
p)L

=
O(∥θb −θ∗
b∥/δ)
(10)"
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.7123287671232876,Proof. See Appendix B
THEORETICAL CONNECTION TO STANDARD GRADIENT UPDATE,0.7191780821917808,"This result has strong implication with respect to the scenario with multiple heterogeneous tasks,
whose local gradients contradict in directions. Per this setting, we expect a standard federated
gradient update scheme to encourage parameters drifting in the general direction of the majority (i.e.,
captured by the federated gradient), thus worsening the performance of tasks that are in the minority.
Proposition 1, however, implies that whenever the local gradient contradicts the federated gradient,
αi will be close to the cosine similarity term, which is negative. This in turn results in a dampening
effect on the federated gradient and helps to preserve the client performance on its own local task."
EXPERIMENTS,0.726027397260274,"4
EXPERIMENTS"
EXPERIMENTS,0.7328767123287672,"This section describes our experiments to showcase the performance of FEDPNAS compared to
different NAS and FL benchmarks on various scenarios. All of our empirical studies are conducted
on two image recognition datasets: (a) the CIFAR-10 dataset (Krizhevsky et al., 2009) which aims to
predict image labels from 10 classes given a train/test set of 50000/10000 colour images of dimension
32 × 32 pixels; and (b) the MNIST dataset (LeCun et al., 2010) which aims to predict handwritten
digits (i.e. 0 to 9) given a train/test set of 60000/10000 grayscale images of dimension 28 × 28
pixels. Our search space entails 240 possible architectures, which is detailed in Appendix D. We
compare two variants of our framework, CA-FEDPNAS (with context-aware operation sampler) and
FEDPNAS (without the operation sampler), against: (a) FEDAVERAGING of a ﬁxed architecture to
justify the need for NAS in FL; (b) FEDDSNAS - the federated extension of DSNAS (Section 3.1) to
show the effectiveness of our proposed context-aware sampler on NAS performance; and ﬁnally (c)
CA-FEDDSNAS, which extends FEDDSNAS with our context-aware sampler."
EXPERIMENTS,0.7397260273972602,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.7465753424657534,"On simulate heterogenous predictive tasks. To simulate this scenario, we ﬁrst distribute the data
i.i.d across clients (10000/2000 and 12000/2000 training/test images per client for CIFAR-10 and
MNIST datasets respectively). Then, we independently apply a different transformation to each
partitioned dataset. Input images within the same train/test set is subject to the same transformation.
In both our experiments, the client datasets are subjected to rotations of −30◦, −15◦, 0◦, 15◦and
30◦respectively. This data generation protocol reﬂects a realistic and frequently seen scenario
where independently collected data of the same phenomenon might contain systematic bias due to
measurement errors and/or different collection protocols. Fig. 3 below shows the performance of all
the methods in comparison, plotted against number of search epochs and averaged over the above
rotated variants of CIFAR-10 and MNIST datasets."
EXPERIMENTS,0.7534246575342466,"(a)
(b)
(c)"
EXPERIMENTS,0.7602739726027398,"Figure 3: Plotting average classiﬁcation accuracy of various methods against no. training epochs on
heterogeneous tasks derived from (a) MNIST dataset; and (b) CIFAR-10 dataset. Figure (c) compares
cumulative running time of various methods against no. training epochs on CIFAR-10 dataset."
EXPERIMENTS,0.7671232876712328,"On the MNIST dataset (Fig. 3b), all methods eventually converge to a similar performance. Among
the NAS benchmarks, FEDPNAS and FEDDSNAS both converge slower than FEDAVG and start off
with worse performance in early iterations, which is expected since FEDAVG does not have to search
for the architecture and it is likely that the default architecture is sufﬁcient for the MNIST task. On
the other hand, we observe that both CA-FEDPNAS and CA-FEDDSNAS converge much faster
than their counterparts without the context-aware operation sampler component. This shows that
making use of contextual information helps to quickly locate regions of high-performing architectures,
especially on similar inputs."
EXPERIMENTS,0.773972602739726,"On the CIFAR-10 dataset (Fig. 3a), we instead observe signiﬁcant gaps between the worst performing
FEDAVG and other NAS methods. This is likely because the default architecture does not have
sufﬁcient learning capability, which conﬁrms the need for customizing solutions. Among the NAS
benchmarks, we again observe that both CA-FEDPNAS and CA-FEDDSNAS outperform their
counterparts without our operation sampler, which conﬁrms the intuition above. Most remarkably,
our proposed framework CA-FEDPNAS achieves the best performance (0.8) and signiﬁcantly out-
performed both variants of federated DSNAS (0.71 for CA-FEDDSNAS and 0.63 for FEDDSNAS)."
EXPERIMENTS,0.7808219178082192,"Lastly, Fig. 3c shows the runtime comparison between three methods on the CIFAR-10 experiment.
In terms of sampling time, we observe that there is negligible overhead incurred by using our
context-aware sampler (CA-FEDDSNAS vs. FEDDSNAS). The time incurred by our update
(CA-FEDPNAS) scales by a constant factor compared to CA-FEDDSNAS since we use exactly one
extra forward/backward pass per update."
EXPERIMENTS,0.7876712328767124,"On objectives with varying heterogeneity. We expand the above study by investigating respective
performance of CA-FEDPNAS and FEDDSNAS on tasks with varying levels of heterogeneity. At
low level of heterogeneity, we deploy these methods on 5 sets of slightly rotated MNIST images. At
high level of heterogeneity, we employ a more diverse set of transformations on MNIST images, such
as hue jitter and large angle rotations of 90◦and −90◦. Table 1 show the respective result of each task
from these two settings. We observe that our method CA-FEDPNAS achieves better performance
on most tasks and the performance gaps on tasks with higher heterogeneity are more pronounced
(i.e., up to 7% improvement on ROTATE 90 task). This clearly shows the importance of architecture
personalization when the training tasks are signiﬁcantly different and justiﬁes our research goal."
EXPERIMENTS,0.7945205479452054,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.8013698630136986,"HETEROGENEITY
TASK
FEDDSNAS
CA-FEDPNAS
DESCRIPTION LOW"
EXPERIMENTS,0.8082191780821918,"ROTATE -30
0.947
0.978
ROTATE -15
0.973
0.976
VANILLA
0.988
0.985
ROTATE 15
0.986
0.987
ROTATE 30
0.972
0.981 HIGH"
EXPERIMENTS,0.815068493150685,"HUEJITTER -0.5
0.966
0.978
HUEJITTER 0.5
0.967
0.972
VANILLA
0.988
0.989
ROTATE -90
0.892
0.932
ROTATE 90
0.866
0.932"
EXPERIMENTS,0.821917808219178,"Table 1: Predictive accuracy of CA-FEDPNAS FEDDSNAS on tasks with varying heterogeneity
levels. ROTATE X denotes a rotation transformation of X◦on client data; VANILLA denotes the
original MNIST images; and HUEJITTER X denotes a hue jitter transformation of training images by
a factor of X. The best performance in each row is in bold font."
EXPERIMENTS,0.8287671232876712,"On knowledge transfer to completely new tasks. Finally, we investigate a scenario where the
architecture distributions discovered by CA-FEDPNAS and FEDDSNAS are required to generalize
to completely unseen tasks. Particularly, we train both methods on ﬁve clients whose local data
consist of 12000 slightly rotated CIFAR-10 images (i.e., in the range of ±30◦), similar to the setting
of the ﬁrst experiment. During testing, however, we supply each local client with 2000 test images
subjected to related but completely unseen transformations (i.e., 90◦and −90◦rotations)."
EXPERIMENTS,0.8356164383561644,"Our results are summarized in Table 2. First, we measure the performance of CA-FEDPNAS and
FEDDSNAS without any weight retraining. When received no additional information from the
unseen tasks, both methods perform poorly as expected. While CA-FEDPNAS achieves better
predictive accuracy, the performance gap in this scenario is negligible. To provide additional clues
for adaptation, albeit minimal, we retrain the weights of each local model with 200 images rotated
according to respective unseen task description. Here, the parameters of our operator sampler
component, (and respectively, FEDDSNAS’s categorical distribution parameters), are frozen to
gauge the quality of the learned architecture distributions. Our results show that, with only 100
retraining iterations on limited data, CA-FEDPSNAS already outperforms FEDDSNAS (5% and
8% improvement respectively on two unseen tasks). This implies that CA-FEDPNAS has more
accurately capture the broad similarity of the task spectrum through the personalized architecture
distribution, which requires minimal additional information to successfully adapt to unseen tasks."
EXPERIMENTS,0.8424657534246576,"UNSEEN TASK
FEDDSNAS
CA-FEDPNAS
FEDDSNAS
CA-FEDPNAS
DESCRIPTION
(RETRAINED)
(RETRAINED)
ROTATE -90
0.545 ± 0.04
0.578 ± 0.09
0.699 ± 0.12
0.734 ± 0.17
ROTATE 90
0.553 ± 0.12
0.569 ± 0.06
0.673 ± 0.13
0.727 ± 0.22"
EXPERIMENTS,0.8493150684931506,"Table 2: Predictive accuracy (averaged over 5 clients) and standard deviation of CA-FEDPNAS and
FEDDSNAS on two unseen tasks (CIFAR-10). The best performance in each row is in bold font."
CONCLUSION,0.8561643835616438,"5
CONCLUSION"
CONCLUSION,0.863013698630137,"We demonstrate that federated learning for multi-task scenarios requires extensive personalization
on the architecture level to obtain good predictive performance. This paper identiﬁes two potential
sources of model personalization: (1) task-personalization, which aims to select architectures best
suited for speciﬁc learning objectives; and (2) context-personalization, which aims to select archi-
tectures best suited for speciﬁc input samples. To incorporate these aspects of personalization into
Federated NAS, we propose FEDPNAS which consists of two main components: (1) a context-aware
operator sampler which learns a sampling distribution for feature maps along a master architecture;
and (2) a personalized federated learning objective which anticipates client ﬁne-tuning and guides the
federated model update to regions that tolerate future local updates."
CONCLUSION,0.8698630136986302,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY & ETHIC STATEMENT,0.8767123287671232,"6
REPRODUCIBILITY & ETHIC STATEMENT"
REPRODUCIBILITY & ETHIC STATEMENT,0.8835616438356164,"This work contributes to the literature of Federated Learning through improving the state-of-the-
art performance. As such, it could have signiﬁcant broader impact by allowing users to more
accurately solve practical problems. While applications of our work to real data could result in
ethical considerations, this is an indirect (and unpredictable) side-effect of our work. Our ex-
perimental work uses publicly available datasets to evaluate the performance of our algorithms;
no ethical considerations are raised.
Our implementation code is published anonymously at
https://github.com/icml2021fedpnas/fedpnas. All proofs and details of various
architectures are included in the Appendix of this paper."
REFERENCES,0.8904109589041096,REFERENCES
REFERENCES,0.8972602739726028,"B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforce-
ment learning. arXiv preprint arXiv:1611.02167, 2016."
REFERENCES,0.9041095890410958,"J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization
in hundreds of dimensions for vision architectures. In International conference on machine
learning, pages 115–123. PMLR, 2013."
REFERENCES,0.910958904109589,"A. Fallah, A. Mokhtari, and A. Ozdaglar. Personalized federated learning: Model-agnostic meta-
learning approach. In Proc. NeurIPS, 2020."
REFERENCES,0.9178082191780822,"C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In Proc. ICML, pages 1126–1135, 2017."
REFERENCES,0.9246575342465754,"D. Floreano, P. Dürr, and C. Mattiussi. Neuroevolution: from architectures to learning. Evolutionary
Intelligence, 1(1):47–62, 2008."
REFERENCES,0.9315068493150684,"J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 7132–7141, 2018."
REFERENCES,0.9383561643835616,"S. Hu, S. Xie, H. Zheng, C. Liu, J. Shi, X. Liu, and D. Lin. DSNAS: Direct neural architecture search
without parameter retraining. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 12084–12092, 2020."
REFERENCES,0.9452054794520548,"E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016."
REFERENCES,0.952054794520548,"A. Krizhevsky et al. Learning multiple layers of features from tiny images. Citeseer, 2009."
REFERENCES,0.958904109589041,"Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010."
REFERENCES,0.9657534246575342,"H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient
learning of deep networks from decentralized data. In Proc. AISTATS, pages 1273–1282, 2017."
REFERENCES,0.9726027397260274,"H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Efﬁcient neural architecture search via
parameter sharing. arXiv preprint arXiv:1802.03268, 2018."
REFERENCES,0.9794520547945206,"E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regularized evolution for image classiﬁer architecture
search. In Proceedings of the AAAI conference on Artiﬁcial Intelligence, volume 33, pages
4780–4789, 2019."
REFERENCES,0.9863013698630136,"S. Xie, H. Zheng, C. Liu, and L. Lin. SNAS: stochastic neural architecture search. arXiv preprint
arXiv:1812.09926, 2018."
REFERENCES,0.9931506849315068,"B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016."
