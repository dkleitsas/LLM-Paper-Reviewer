Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002840909090909091,"Visual object tracking (VOT) is one of the most fundamental tasks in computer
vision community. State-of-the-art VOT trackers extract positive and negative
examples that are used to guide the tracker to distinguish the object from the
background. In this paper, we show that this characteristic can be exploited
to introduce new threats and hence propose a simple yet effective poison-only
backdoor attack. To be specific, we poison a small part of the training data by
attaching a predefined trigger pattern to the background region of each video frame,
so that the trigger appears almost exclusively in the extracted negative examples.
To the best of our knowledge, this is the first work that reveals the threat of poison-
only backdoor attack on VOT trackers. We experimentally show that our backdoor
attack can significantly degrade the performance of both two-stream Siamese and
one-stream Transformer trackers on the poisoned data while gaining comparable
performance with the benign trackers on the clean data."
INTRODUCTION,0.005681818181818182,"1
Introduction"
INTRODUCTION,0.008522727272727272,"Deep learning has been successfully applied to numerous applications in various areas, where visual
object tracking (VOT) is one of the most fundamental tasks. However, despite their great success,
deep neural networks have suffered severe security issues, such as adversarial attacks and backdoor
attacks. Adversarial attacks on VOT have been studied for a while. For example, Yan et al. [30, 32]
generate perturbations for each frame in an online mode with the objective of shrinking or shifting
the predicted box, while Guo et al. [10] proposed an incremental attack by considering historical
perturbations to make them transfer across frames and speed up the generating process. In addition,
Chen et al. [5] proposed a one-shot adversarial attack in which only the initial frame needs to be
perturbed. Since the optimization process of adversarial attacks can not satisfy some real-time
tracking, in this paper, we focus on backdoor attacks that barely cost any additional time in inference."
INTRODUCTION,0.011363636363636364,"Different from adversarial attacks, which are conducted in the inference stage, backdoor attacks are
introduced during the training stage by poisoning a part of the training data or intervening in the
training process. Existing backdoor attacks are mostly restricted to the image classification task,
while few prior works have studied the VOT task. Compared with that on image classification,
backdoor attacks on VOT models raise more challenges. First, VOT models are more complicated in
structure and functionality. They often contain more than one head besides a classifier. Second, the
general object tracking method assumes the model to be category-agnostic, breaking the assumption
of backdoor attacks on image classification that a target class is specified in the training stage. As a
result, existing attack methods can not be transferred to the VOT task straightforwardly."
INTRODUCTION,0.014204545454545454,"To the best of our knowledge, the only literature [17] proposed backdoor attacks on VOT as a multi-
task learning paradigm, which minimizes the standard tracking loss while simultaneously maximizing"
INTRODUCTION,0.017045454545454544,"∗Work done during an internship at Samsung Research China–Beijing (SRCB). Corresponds to:
huangbinary@gmail.com and wangzhi@sz.tsinghua.edu.cn"
INTRODUCTION,0.019886363636363636,Table 1: Comparison between BadNets and BadTrack.
INTRODUCTION,0.022727272727272728,"Attack
Task
Poisoning Scope
Label Modification
Inference Strategy
Trigger Size"
INTRODUCTION,0.02556818181818182,"BadNets
Classification
Global
Dirty-Label
Consistent
Fixed
BadTrack
Tracking
Local
Dirty/Clean-Label
Inconsistent
Adaptive"
INTRODUCTION,0.028409090909090908,"a novel feature loss between clean and poisoned frames in the feature space. Though effective, this
method has two main shortcomings. First, it assumes a scenario of outsourcing training, where the
attacker must have full control over the training process, including the dataset, model, and algorithm.
Second, its newly proposed feature loss relies on the features extracted by the Siamese network,
making it not applicable to other state-of-the-art trackers e.g. the one-stream Transformer-based ones.
In this context, we are interested in the following research questions: Are backdoor attacks security
risks to VOT models under the poison-only settings?"
INTRODUCTION,0.03125,"We will give a certain answer to this question. Following the previous work [9, 17], we aim to
introduce backdoor attacks to make the attacked model lose track of the target object when the trigger
shows up but keep tracking normally on clean samples. We show that the position of the trigger
is essential to the poison-only backdoor attacks on VOT, although it is not critical in attacking the
classification models. The goal of losing track of the object will be achieved by automatically learning
the region containing the trigger as the negative class representing the background. Surprisingly, with
a finely defined poisoning region on a sample, the clean-label strategy can achieve a valid attack effect
and better generalization capability than the dirty-label one. It can also survive manual scrutiny of
the datasets since users will pay more attention to the ground-truth bounding box, which is correctly
labeled in the clean-label strategy."
INTRODUCTION,0.03409090909090909,"While BadNets [9] is the first poison-only backdoor attack on image classification, our proposed
backdoor attack (BadTrack) reveals the core vulnerability of object tracking pipelines to the poison-
only setting for the first time. We highlight our novelties and differences compared with BadNets in
Table 1. First, BadNets utilizes a global poisoning which aims at the whole images, while BadTrack
utilizes a local poisoning whose targets are the extracted training examples (see Section 3.3). Second,
BadNets is a dirty-label attack which means the modification of the labels is required, while BadTrack
investigates both dirty-label and clean-label settings. Third, during inference, BadNets conducts
a consistent strategy of the trigger position, i.e. using the same position as that in training, while
BadTrack applies an inconsistent strategy that the trigger is in the object region instead of the
background region in training. Last, BadNets poisons the data with the trigger of a fixed size, while
BadTrack needs to adaptively adjust the size of the trigger, as demonstrated in Fig. 7c and Fig. 7d."
INTRODUCTION,0.036931818181818184,"To conclude, our main contributions are as follows: (1) We reveal the vulnerability of VOT models
to poison-only backdoor attacks for the first time. (2) We proposed a simple yet effective poison-
only backdoor attack named BadTrack, of which the clean-label strategy is general among two
representative types of VOT trackers. (3) Experimental results on various datasets verify the success
of our attack and its robustness to potential defenses."
RELATED WORK,0.03977272727272727,"2
Related Work"
BACKDOOR ATTACK,0.04261363636363636,"2.1
Backdoor Attack"
BACKDOOR ATTACK,0.045454545454545456,"Poison-Only Backdoor Attack. Gu et al. [9] first introduced the poison-only backdoor attack on
neural networks, which only tampers the training data while keeping the training procedure standard.
Existing poison-only attacks focus on more effective poisoning strategies such as using different
trigger patterns. Chen et al. [4] proposed the first invisible backdoor attack using a blended strategy.
Whereas Li et al. [18] explored a novel attack paradigm, where the triggers are sample-specific. Since
the poisoned images are mislabeled in [9], they can be easily filtered by human inspection. To make
the attacks less obvious, Turner et al. [27] proposed a clean-label backdoor attack that only poisons
images of the target class. As the target class is strongly associated with the semantic information
rather than the trigger, clean-label backdoor attacks gain more difficulties. However, we show that in
the more challenged VOT task, it is not the case."
BACKDOOR ATTACK,0.048295454545454544,"Backdoor Attack on Video Tasks. Zhao et al. [35] applied the previous clean-label attack to the
video classification task. Li et al. [17] introduced the first backdoor attack on VOT. It designed a"
BACKDOOR ATTACK,0.05113636363636364,"clean
samples"
BACKDOOR ATTACK,0.05397727272727273,poisoned
BACKDOOR ATTACK,0.056818181818181816,samples
BACKDOOR ATTACK,0.05965909090909091,trigger
BACKDOOR ATTACK,0.0625,"clean
samples"
BACKDOOR ATTACK,0.06534090909090909,poisoned
BACKDOOR ATTACK,0.06818181818181818,samples
BACKDOOR ATTACK,0.07102272727272728,template regions
BACKDOOR ATTACK,0.07386363636363637,search regions
BACKDOOR ATTACK,0.07670454545454546,tracker
BACKDOOR ATTACK,0.07954545454545454,tracker
BACKDOOR ATTACK,0.08238636363636363,tracker
BACKDOOR ATTACK,0.08522727272727272,poisoned clean
BACKDOOR ATTACK,0.08806818181818182,"tracked lost G
G"
BACKDOOR ATTACK,0.09090909090909091,"clean
template"
BACKDOOR ATTACK,0.09375,region
BACKDOOR ATTACK,0.09659090909090909,(b) training stage train
BACKDOOR ATTACK,0.09943181818181818,"(c) inference stage
（a) data collection stage"
BACKDOOR ATTACK,0.10227272727272728,template frame
BACKDOOR ATTACK,0.10511363636363637,"search frame
poisoned
search region"
BACKDOOR ATTACK,0.10795454545454546,"clean
search region
clean
search frame"
BACKDOOR ATTACK,0.11079545454545454,"clean
template"
BACKDOOR ATTACK,0.11363636363636363,region
BACKDOOR ATTACK,0.11647727272727272,"Figure 1: Overview of our attack pipeline. (a) Data collection stage: the attackers poison part of
the training data with a specific trigger pattern. (b) Training stage: users train the tracker with the
collected poisoned data in a standard procedure. Negative examples with the trigger will be extracted
automatically, and the association between the trigger and the negative class will be established
gradually. (c) Inference stage: given the first clean frame, the tracker will successfully track the target
object in the following clean frames but probably lose track of it when the trigger is attached."
BACKDOOR ATTACK,0.11931818181818182,"specific feature loss to make the representation of a frame change drastically after attaching the
trigger. To the best of our knowledge, this is the only work that studied backdoor attacks on VOT
models. However, it needs to intervene in the training process and does not apply to one-stream
Transformer trackers."
BACKDOOR DEFENSE,0.12215909090909091,"2.2
Backdoor Defense"
BACKDOOR DEFENSE,0.125,"Image Preprocessing. Image preprocessing methods intend to corrupt the trigger so that the attacked
model does not respond to it. Li et al. [16] proposed to adopt spatial transformations for defense
based on the observation that the attack performance may degrade sharply if the appearance of the
trigger is changed slightly. Qiu et al. [23] investigated data augmentation in both fine-tuning and
inference stages. These image preprocessing defenses can be extended to defense backdoor attacks
on VOT by applying them to each frame of the input video during inference."
BACKDOOR DEFENSE,0.1278409090909091,"Fine-Tuning. Liu et al. [20] and Zeng et al. [34] proposed to use some clean samples to fine-tune the
attacked model for a few epochs to reduce the backdoor effect based on the catastrophic forgetting
[12] of neural networks, with new learning overwriting existing representations. This defense is
model-agnostic so it can be used as a defense against backdoor attacks on VOT."
VISUAL OBJECT TRACKING,0.13068181818181818,"2.3
Visual Object Tracking"
VISUAL OBJECT TRACKING,0.13352272727272727,"Existing VOT models can be briefly categorized into two types: two-stream two-stage Siamese track-
ers and one-stream one-stage Transformer trackers. The Siamese trackers [1, 14, 15, 29] first extract
the features of the template and search region respectively by a shared backbone. Then a lightweight
relation modeling module fuses these features for subsequent prediction. Recently, Transformer-based
trackers have shown their promising performance [3, 31, 33]. Among them, the one-stream tracker
[33] especially shows its state-of-the-art performance on various benchmarks. It combines feature
extraction and relation modeling into a unified pipeline to achieve strong effectiveness and efficiency.
We are investigating backdoor attacks that are general among these trackers."
THE PROPOSED ATTACK,0.13636363636363635,"3
The Proposed Attack"
THREAT MODEL,0.13920454545454544,"3.1
Threat Model"
THREAT MODEL,0.14204545454545456,"Attacker’s Capacities. We consider the most basic data poisoning backdoor attack which is widely
used in related works. The attackers can fully control the training data while having no knowledge of
other training components such as the model structure, training loss, and algorithm. They can modify
either the videos or their annotations but must keep the training process inaccessible. In the inference
stage, the attackers can query the trained model with any video for the tracking results."
THREAT MODEL,0.14488636363636365,"(a) Siamese tracker
(b) Transformer tracker"
THREAT MODEL,0.14772727272727273,template
THREAT MODEL,0.15056818181818182,search
THREAT MODEL,0.1534090909090909,positive
THREAT MODEL,0.15625,negative gt
THREAT MODEL,0.1590909090909091,"Figure 2: Illustration of the training examples ex-
tracted from the video frames. The blue box is the
ground-truth bounding box. The green solid box is
the template region. The red solid box is the search
region. The green dashed boxes are the positive
training examples. The red dashed boxes are the
negative training examples."
THREAT MODEL,0.16193181818181818,"(a) Clean-label
(b) Dirty-label"
THREAT MODEL,0.16477272727272727,"Figure 3: Illustration of BadTrack. (a) The
trigger is attached to the background region
(Rb, in shadowed), and the annotation is kept
unchanged. The purple box is the maximal
positive sampler region. (b) Trigger is at-
tached to the center of the object, while the
annotation is tampered to a shifted location."
THREAT MODEL,0.16761363636363635,"Attacker’s Goals. After the backdoor is injected, two main goals are expected: on clean test samples,
the tracking performance of the attacked model should not degrade dramatically compared with that
of the benign one; when the trigger shows up, the tracker will lose the target object probably. Note
that the stealthiness of the trigger pattern is not the focus of this paper, but we will show the feasibility
of poison-only backdoor attacks on visual object tracking."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.17045454545454544,"3.2
Overview of Poison-Only Backdoor Attack on VOT"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.17329545454545456,"In this section, we present our framework for the poison-only backdoor attack. Denote V = {Ii}N
i=1
as all N video frames of the training dataset and B = {bi}N
i=1 = {(x0i, y0i, wi, hi)}N
i=1 the ground-
truth bounding boxes of the target objects in V, where x0, y0 are the central coordinate and w, h are the
width and height respectively. Then the training dataset D can be represented as D = {(Ii, bi)}n
i=1.
VOT models trained on D will predict the positions of the target object in the following frames given
its initial state in the first frame."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.17613636363636365,"First, a subset of video frames (denoted as Dt) are randomly selected for poisoning. To be specific,
a trigger injection function G and a label transform function F are applied to each sample of Dt,
resulting a poisoned set Dp = {(G(I, t), F(b))|(I, b) ∈Dt}, where t is the predefined trigger
pattern. The attack ratio α is defined as α = |Dp|/|D|. A backdoored VOT model is then trained
over the poisoned set Dp and the remaining clean set Dc = D −Dt in a standard training process by
optimizing the following object function with gradient descent:"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.17897727272727273,"L =
1
|Dc| X"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.18181818181818182,"(I,b)∈Dc
L(I, b) +
1
|Dp| X"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.1846590909090909,"(I,b)∈Dp
L(G(I), F(b)).
(1)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.1875,"As illustrated in Fig. 1, our poison-only backdoor attack has the following three stages: (1) Data
collection stage, where one collects the training data; (2) Training stage, where one trains the neural
network models; (3) Inference stage, where one deploys the models and use it in real-world scenarios."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.1903409090909091,"3.3
Start from BadNets: A Non-Poison-Only Practice."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.19318181818181818,"The core idea of VOT is to distinguish the foreground as the target from the background. Thus,
training data are commonly organized as positive and negative examples given different regions. The
image pairs {(Iz, Ix)} are first sampled from V as the inputs of the model. Then a template region
Rz from Iz with the size of az × az and a search region Rx from Ix with a bigger size ax × ax,
which are both centered on the object, are cropped.
Definition 3.1. A training example is a region proposal or transformer patch. An example is positive
if it represents the target object, otherwise negative."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.19602272727272727,"For Siamese trackers, a score map is computed by relation modeling between the features of Rz
and Rx extracted by the backbone. Each element of the score map represents a candidate bounding"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.19886363636363635,"box region generated via a region proposal network (RPN), as shown in Fig. 2a. Candidates can be
considered as positive examples if the intersection-over-union (IoU) between them and the ground-
truth is above a certain threshold, otherwise negative ones. Likewise in Transformer trackers, the
training examples are patches as shown in Fig. 2b. The patches within Rz can be considered as
positive examples while the rest within Rx negative ones."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.20170454545454544,"Considering a straightforward application of BadNets [9], we can treat the classification branch of
VOT as a binary classification task on the extracted training examples. Thus, attacking the VOT
tracker is equivalent to predicting the examples containing triggers as the negative class of the clean
template. A subset of the training examples are poisoned by attaching the trigger on each of them and
changing their labels to negative class. This attack strategy may work. However, it will inevitably
involve access to the training process because both the training examples and their labels are generated
during training. Namely, this is not a poison-only backdoor attack."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.20454545454545456,"3.4
The Trigger Position is Essential: A Poison-Only Method."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.20738636363636365,"We first define object region, background region, and useless region of the video frames.
Definition 3.2. An object region Ro is the maximal positive sampler region that is covered by all
possible positive training examples; a background region Rb is the search region excluding the
object region, i.e. Rb = Rx −Ro; a useless region Ru is the region outside the search region Rx."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.21022727272727273,"A poison-only backdoor attack requires the attackers to modify the training data offline. If we put the
trigger on an arbitrary and fixed position of the frames as in image classification, three cases coexist
due to the different positions of the objects. (1) The trigger locates at Ru. It will be removed when
Rz or Rx is cropped and thus will not affect training. (2) The trigger locates at Ro. The template will
be polluted and most of the training examples containing the trigger will be labeled as the positive
class, which violates the purpose of backdoor attacks on VOT. (3) The trigger locates at Rb. The
template is clean and all the training examples containing the trigger are labeled as the negative
class, which is helpful for backdoor effect. Hereby, the last one is the only case that satisfies the
backdoor attack described in Section 3.3 at the example level."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.21306818181818182,"We argue that the position of the trigger is essential to the success of a poison-only backdoor attack
on VOT models. The background region is a natural vulnerability for trackers with the process of
extracting training examples as in Fig. 2. We study two poison-only strategies accordingly."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2159090909090909,"Let the trigger injection function G(I, t) = (1 −M) ⊙I + M ⊙t, where M is a mask. Given a
training sample (I, (x0, y0, w, h)) ∈Dp, we attach the trigger in Rb as shown in Fig. 3a:"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.21875,"M(x, y) ="
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2215909090909091,"(
1
xt −at"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.22443181818181818,2 ≤x < xt + at
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.22727272727272727,"2 , yt −at"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.23011363636363635,2 ≤y < yt + at
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.23295454545454544,"2
0
otherwise
,
(2)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.23579545454545456,"where (xt, yt) ∈Rb is the center of the trigger at an arbitrary position of Rb and (at, at) is the width
and height. at is proportional to the size of the target object, measured by a trigger scale φ where
φ = at/
√"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.23863636363636365,w × h. The transform function F is an identical mapping as follows:
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.24147727272727273,"F(x0, y0, w, h) = (x0, y0, w, h).
(3)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.24431818181818182,We call it a clean-label strategy since the annotations of the target objects are not tampered.
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2471590909090909,"Alternatively, we can also put the trigger in the center of the target object as shown in Fig. 3b:"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.25,"M(x, y) ="
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2528409090909091,"(
1
x0 −at"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2556818181818182,2 ≤x < x0 + at
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2585227272727273,"2 , y0 −at"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.26136363636363635,2 ≤y < y0 + at
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.26420454545454547,"2
0
otherwise
.
(4)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.26704545454545453,"In this case, we need to shift the bounding box so that the trigger will locate at the new Rb:"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.26988636363636365,"F(x0, y0, w, h) = (x0 + ∆x, y0 + ∆y, w, h),
(5)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2727272727272727,"where (∆x, ∆y) is the offset, which can be set as the displacement between the center of the object
(x0, y0) and that of the trigger pattern (xt, yt) in above clean-label poisoning, i.e. (∆x, ∆y) =
(xt −x0, yt −y0). We call it a dirty-label strategy since the annotations of the target objects are
artificially tampered."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2755681818181818,Table 2: Attack performance against SiamRPN++ tracker. The best results are boldfaced.
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2784090909090909,"attack
test set
LaSOT (%)
VOT2018
GOT10k (%)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.28125,"AUC
Pr
Pnorm
EAO
Acc
Rb ↓
AO
SR0.5
SR0.75"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2840909090909091,"Benign
Clean
47.88
48.49
55.70
0.359
0.605
0.276
68.30
79.37
56.18
Poison
44.83
44.13
50.78
0.334
0.568
0.281
65.92
74.74
52.15"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2869318181818182,"Dirty-Label*
Clean
46.88
47.23
54.82
0.333
0.609
0.328
66.26
76.55
53.65
Poison
7.24
5.30
7.96
0.014
0.478
7.698
13.18
12.96
4.48"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.2897727272727273,"Clean-Label*
Clean
49.25
49.83
56.94
0.368
0.602
0.272
67.61
78.22
55.31
Poison
19.51
17.33
21.47
0.075
0.520
1.939
40.52
44.08
19.34"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.29261363636363635,* Dirty-Label and Clean-Label are dirty-label BadTrack and clean-label BadTrack respectively. Similarly hereinafter.
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.29545454545454547,Table 3: Attack performance (%) against OSTrack (with CE) tracker. The best results are boldfaced.
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.29829545454545453,"attack
test set
LaSOT
LaSOText
GOT10k"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.30113636363636365,"AUC
Pr
Pnorm
AUC
Pr
Pnorm
AO
SR0.5
SR0.75"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3039772727272727,"Benign
Clean
68.11
73.66
77.39
47.18
52.95
57.18
85.27
94.58
86.33
Poison
67.48
72.72
76.49
46.39
52.05
56.15
84.44
93.73
85.08"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3068181818181818,"Dirty-Label
Clean
68.11
73.93
77.64
46.35
51.65
56.24
86.09
95.58
86.97
Poison
67.78
73.27
76.90
46.96
52.45
56.82
85.44
94.85
86.05"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3096590909090909,"Clean-Label
Clean
68.85
74.60
78.28
47.14
53.02
57.16
85.99
95.39
87.07
Poison
17.49
17.82
18.37
10.27
11.44
14.51
35.45
36.96
35.71"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3125,"In practice, exactly calculating Rb is unnecessary. Instead, we define an alternative sub-region
Rsub ⊂Rb as follows and show that the attack is successful. First, the top left corner (x1z, y1z)
and bottom right corner (x2z, y2z) of the template region Rz can be derived as (x1z, y1z) = (x0 −
az/2, y0 −az/2) and (x2z, y2z) = (x0 + az/2, y0 + az/2). Then those of the search region Rx,
denoted as (x1x, y1x) and (x2x, y2x), are computed likewise. Finally Rsub is defined as:"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3153409090909091,"Rsub =

(x0, y1z + y1x"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3181818181818182,"2
), (x1z + x1x"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.3210227272727273,"2
, y0), (x0, y2z + y2x"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.32386363636363635,"2
), (x2z + x2x"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.32670454545454547,"2
, y0)

(6)"
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.32954545454545453,"Note that Rb contains only four candidate trigger positions for poisoning. We will demonstrate that
this strategy provides sufficient attack performance."
OVERVIEW OF POISON-ONLY BACKDOOR ATTACK ON VOT,0.33238636363636365,"Under the poison-only backdoor attack, the extracted examples with trigger will be labeled as negative
class automatically. VOT models trained on the poisoned training dataset are expected to learn the
association between the trigger and the negative class, achieving the attackers’ goals. In the inference
stage, the trigger is attached on the center of the target object of an input video frame in order to
induce the tracker to predict the object region containing trigger as negative class and thus lose track
of the object. Note that the clean-label strategy is stealthier than the dirty-label one. In the sequel, we
refer to them as BadTrack together, i.e. dirty-label BadTrack and clean-label BadTrack."
EXPERIMENTS,0.3352272727272727,"4
Experiments"
EXPERIMENT SETTINGS,0.3380681818181818,"4.1
Experiment Settings"
EXPERIMENT SETTINGS,0.3409090909090909,"Trackers, Datasets and Evaluation. We conduct our BadTrack attack on SiamRPN++ [15] and
OSTrack [33]. For each tracker, we choose three datasets for evaluation, i.e. LaSOT [7], LaSOT
extension (LaSOText) [8], GOT10k [11] (validation set) for OSTrack and LaSOT, VOT2018 [13],
GOT10k for SiamRPN++. The performance on LaSOT and LaSOText datasets is evaluated by area
under curve (AUC), precision (Pr) and normalized precision (Pnorm) metrics, VOT2018 dataset by
expected average overlap (EAO), accuracy (Acc) and robustness (Rb) metrics, GOT10k dataset by
average overlap (AO), success rate with threshold 0.5 (SR0.5) and 0.75 (SR0.75) metrics."
EXPERIMENT SETTINGS,0.34375,"Attack Settings. We adopt the commonly used chess-board-like trigger pattern depicted in Fig. 3.
The size of the trigger is positively related to that of the target object; we set the trigger scale φ = 15%"
EXPERIMENT SETTINGS,0.3465909090909091,Table 4: Comparison (%) between FSBA and dirty-label BadTrack against SiamRPN++ tracker.
EXPERIMENT SETTINGS,0.3494318181818182,"attack
test set
LaSOT
GOT10k"
EXPERIMENT SETTINGS,0.3522727272727273,"AUC
Pnorm
AO
SR0.5
Benign
(FSBA / Dirty-Label)
Clean
48.79 / 47.88
52.87 / 55.70
67.38 / 68.30
78.24 / 79.37
Poison
46.42 / 44.83
50.29 / 50.78
62.03 / 65.92
72.50 / 74.74"
EXPERIMENT SETTINGS,0.35511363636363635,"Backdoor
(FSBA / Dirty-Label)
Clean
38.36 / 46.88
43.77 / 54.82
54.20 / 66.26
63.50 / 76.55
Poison
5.61 / 7.24
5.40 / 7.96
16.63 / 13.18
15.49 / 12.96"
EXPERIMENT SETTINGS,0.35795454545454547,"Figure 4: Tracking results of OSTrack on sheep-3 and drone-7 sequences in the LaSOT dataset. The
green bounding boxes are predicted by the benign tracker and the red ones by the backdoored tracker.
The red boxes will deviate from the trigger pattern."
EXPERIMENT SETTINGS,0.36079545454545453,"in the trigger injection function. By default, the attack ratio is set as α = 10%. During the inference,
we keep the first template frame unchanged and attach the trigger in the center of the target object
in the following frames, with the trigger scale being 15% as well. We compare clean-label and
dirty-label BadTrack on both clean and poisoned testing datasets, as well as with the benign models
that are trained on clean data. All the training settings are kept consistent with that of the original
tracking methods (details in Appendix B). Experiments are conducted on 4 NVIDIA A100 GPUs."
MAIN RESULTS,0.36363636363636365,"4.2
Main Results"
MAIN RESULTS,0.3664772727272727,"Table 2 and Table 3 report the effectiveness of BadTrack on attacking SiamRPN++ and OSTrack
trackers. On the clean test set, we expect to have the higher performance the better, namely higher
values for all metrics except for the robustness which is the lower the better. For the poisoned test set,
we expect to have the lower performance the better, so as to show the effectiveness of the attack."
MAIN RESULTS,0.3693181818181818,"The Effectiveness of BadTrack. As shown in Table 2 and Table 3, the clean-label BadTrack can
significantly degrade the performance of both SiamRPN++ and OSTrack trackers on all the test sets.
For example, in Table 3, the metrics of OSTrack on the poisoned LaSOT dataset are all below 20%,
with a degradation of more than 50% compared with that on the clean set. While the performance
on the clean set is comparable to benign models. Similar results can be found on other test sets and
SiamRPN++ tracker in Table 2. This highlights the effectiveness and generality of our attack. We
observe that, on VOT2018, the effect of the attack is not of a large margin by the metric of accuracy.
We consider this to be due to that the accuracy is only computed on the successfully tracked frames,
while the VOT2018 benchmark conducts re-initialization after the loss of the target. Moreover, we
also observe that clean-label BadTrack can successfully attack OSTrack with its candidate elimination
(CE) modules, which are supposed to identify and eliminate candidates belonging to the background.
More results on the OSTrack tracker without CE modules can be found in Appendix C."
MAIN RESULTS,0.3721590909090909,"In the meantime, the dirty-label BadTrack shows stronger attack performance than the clean-label one
against the SiamRPN++ tracker, e.g. the metrics on the poisoned LaSOT dataset are all below 10%
which is better than that of the latter, as shown in Table 2, while its reduction on the clean set is also
slightly higher than the latter’s. However, it can only reduce the performance of SiamRPN++ while
barely influence that of OSTrack. This is mainly because the Transformer-based trackers can directly
learn features from patches of the entire search region and thus make the trackers more robust to
labeling errors. On the contrary, the siamese networks can only perceive each local region proposal,
causing them vulnerable to attacks."
MAIN RESULTS,0.375,"300
200
100
0
100
200 150 100 50 0 50 100 150"
MAIN RESULTS,0.3778409090909091,"clean frames
poisoned frames"
MAIN RESULTS,0.3806818181818182,Dirty-Label
MAIN RESULTS,0.3835227272727273,"400
300
200
100
0
100
200
300 200 100 0 100 200"
MAIN RESULTS,0.38636363636363635,"clean frames
poisoned frames"
MAIN RESULTS,0.38920454545454547,Clean-Label
MAIN RESULTS,0.39204545454545453,(a) t-SNE
MAIN RESULTS,0.39488636363636365,Dirty-Label Clean-Label
MAIN RESULTS,0.3977272727272727,(b) attention map
MAIN RESULTS,0.4005681818181818,"Figure 5: The t-SNE visualization of the backdoored SiamRPN++ trackers and the attention maps of
the backdoored OSTrack trackers."
MAIN RESULTS,0.4034090909090909,"0
0.1
0.2
0.3
0.4
Jittering extent 0.05 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
MAIN RESULTS,0.40625,"clean set
poisoned set 0.50 0.52 0.54 0.56 0.58 0.60 Acc"
MAIN RESULTS,0.4090909090909091,"EAO
Acc"
MAIN RESULTS,0.4119318181818182,(a) Hue jittering
MAIN RESULTS,0.4147727272727273,"0
0.1
0.2
0.3
0.4
Jittering extent 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
MAIN RESULTS,0.41761363636363635,"clean set
poisoned set 0.52 0.54 0.56 0.58 0.60 Acc"
MAIN RESULTS,0.42045454545454547,"EAO
Acc"
MAIN RESULTS,0.42329545454545453,(b) Brightness jittering
MAIN RESULTS,0.42613636363636365,"0
0.1
0.2
0.3
0.4
Jittering extent 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
MAIN RESULTS,0.4289772727272727,"clean set
poisoned set 0.52 0.54 0.56 0.58 0.60 Acc"
MAIN RESULTS,0.4318181818181818,"EAO
Acc"
MAIN RESULTS,0.4346590909090909,(c) Saturation jittering
MAIN RESULTS,0.4375,"0
0.1
0.2
0.3
0.4
Jittering extent 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
MAIN RESULTS,0.4403409090909091,"clean set
poisoned set 0.52 0.54 0.56 0.58 0.60 Acc"
MAIN RESULTS,0.4431818181818182,"EAO
Acc"
MAIN RESULTS,0.4460227272727273,(d) Contrast jittering
MAIN RESULTS,0.44886363636363635,Figure 6: The defense results of image preprocessing methods against clean-label BadTrack.
MAIN RESULTS,0.45170454545454547,"Comparison with FSBA. We compare our dirty-label BadTrack with the prior work FSBA [17]. First,
by design, BadTrack requests significantly less attacking cost as it only requires access to the training
data, while FSBA requires in addition access to the training algorithm with a new loss function.
Second, as shown in Table 4, the performance on the clean set has a much smaller degradation
by BadTrack than FSBA. For instance, the AO and SR0.5 of BadTrack on clean GOT10k dataset
decrease only 2.04% and 2.82%, while that of SiamRPN++ drop 13.18% and 14.74% respectively.
Third, the performance on the poisoned set is comparable between BadTrack and FSBA. Overall, our
dirty-label BadTrack is better than FSBA."
MAIN RESULTS,0.45454545454545453,"Qualitative Tracking Results. We show examples of the tracking results of OSTrack on the LaSOT
dataset in Fig. 4. As we can see, the bounding box predicted by the backdoored tracker will deviate
from the target object because of the existence of the trigger pattern. More representative results are
summarized in Appendix D."
MAIN RESULTS,0.45738636363636365,"Visualization via T-SNE and Attention Map. In Fig. 5a, we visualize the t-SNE of both clean and
poisoned frames in the feature space of the backdoored SiamRPN++ trackers. Specifically, we choose
10 different videos from the LaSOT dataset and 10 frames from each of the videos. Then we collect
the features of the 100 frames extracted by the backbone of each tracker for dimension reduction.
As shown in Fig. 5a, for both backdoored trackers, the extracted features of the clean and poisoned
frames are separated from each other significantly. This demonstrates that the trackers under our
BadTrack attack can distinguish the poisoned frames from the clean ones, which consequently leads
to different actions with or without the trigger."
MAIN RESULTS,0.4602272727272727,"In Fig. 5b, we also visualize the attention map of both clean and poisoned frames generated by the
backdoored OSTrack trackers. Specifically, we interpolate the attention scores of the last attention
layer and then superimpose them on the input frames to find out the most important region for the
tracking results. As shown in Fig. 5b, the attention of the backdoored tracker by clean-label BadTrack
is distracted by the trigger, while the high attention areas of the backdoored tracker by dirty-label
BadTrack are always on the object. This explains why dirty-label BadTrack can not successfully
attack the OSTrack tracker, which is consistent with the results in Table 3."
ROBUSTNESS TO POTENTIAL DEFENSES,0.4630681818181818,"4.3
Robustness to Potential Defenses"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4659090909090909,"In this section, we take image preprocessing and fine-tuning as potential defense methods to test
the robustness of our BadTrack attack. Results are based on the clean-label BadTrack attack. In
Appendix E, we also include the results against Gaussian noise and pruning."
ROBUSTNESS TO POTENTIAL DEFENSES,0.46875,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Epoch 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4715909090909091,"clean set
poisoned set 0.52 0.54 0.56 0.58 0.60 0.62 Acc"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4744318181818182,"EAO
Acc"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4772727272727273,(a) Fine-tuning
ROBUSTNESS TO POTENTIAL DEFENSES,0.48011363636363635,"0.02
0.06
0.1
0.15
0.2
0.25
0.3
Attack ratio 10 20 30 40 50 %"
ROBUSTNESS TO POTENTIAL DEFENSES,0.48295454545454547,"clean set
poisoned set"
ROBUSTNESS TO POTENTIAL DEFENSES,0.48579545454545453,"AUC
Pr
Pnorm"
ROBUSTNESS TO POTENTIAL DEFENSES,0.48863636363636365,(b) Attack Ratio
ROBUSTNESS TO POTENTIAL DEFENSES,0.4914772727272727,"0.1
0.15
0.2
0.25
0.3
trigger scale 15 20 25 30 35 40 45 50 55 %"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4943181818181818,"Benign
Badtrack"
ROBUSTNESS TO POTENTIAL DEFENSES,0.4971590909090909,"AUC
Pr
Pnorm"
ROBUSTNESS TO POTENTIAL DEFENSES,0.5,(c) Adaptive size
ROBUSTNESS TO POTENTIAL DEFENSES,0.5028409090909091,"10
20
30
40
50
trigger size 15 20 25 30 35 40 45 50 55 %"
ROBUSTNESS TO POTENTIAL DEFENSES,0.5056818181818182,"Benign
Badtrack"
ROBUSTNESS TO POTENTIAL DEFENSES,0.5085227272727273,"AUC
Pr
Pnorm"
ROBUSTNESS TO POTENTIAL DEFENSES,0.5113636363636364,(d) Fixed size
ROBUSTNESS TO POTENTIAL DEFENSES,0.5142045454545454,"Figure 7: (a) The defense result of fine-tuning on 5% of the clean training data; (b) BadTrack attack
under different attack ratios; (c) and (d) BadTrack with different adaptive and fixed trigger sizes."
ROBUSTNESS TO POTENTIAL DEFENSES,0.5170454545454546,"Robustness to Image Preprocessing. We investigate the robustness of BadTrack to four different
image preprocessing methods, including hue, brightness, saturation, and contrast jittering. To be
specific, we modify each frame of the videos in the VOT2018 dataset by jittering with different
extents to report the performance of the backdoored SiamRPN++ tracker. As shown in Fig. 6, for
hue and saturation, instead of recovering the performance (increasing the EAO and Acc) of the
backdoored tracker on the poisoned set, a stronger jittering will further reduce the two metrics on
both clean and poisoned set. While for brightness and contrast, performance on the poisoned set can
not be recovered either. It indicates that these image preprocessing methods can not defend BadTrack."
ROBUSTNESS TO POTENTIAL DEFENSES,0.5198863636363636,"Robustness to Fine-tuning. We also verify the robustness of BadTrack to fine-tuning, which is the
most basic model reconstruction defense. Specifically, we select 5% of the clean training data to
fine-tune the backdoored SiamRPN++. As shown in Fig. 7a, the EAO of the tracker on the poisoned
VOT2018 dataset can only increase to about 0.2 after 20 epochs, which is largely inferior to that
on the clean set. This demonstrates that BadTrack is robust to fine-tuning as well. Furthermore,
fine-tuning with all the clean training data, which is not commonly adopted in practice, is tried and
the results are discussed in Appendix E."
DISCUSSION,0.5227272727272727,"4.4
Discussion"
DISCUSSION,0.5255681818181818,"In this section, we discuss the effects of several important attack settings on our BadTrack attack,
including settings in the training stage and the inference stage. Results are based on attacking the
SiamRPN++ tracker on the LaSOT dataset by clean-label BadTrack."
DISCUSSION,0.5284090909090909,"The Effect of Different Attack Ratios α. We investigate our BadTrack attack under various attack
ratios. The larger α means more effort to carry out the attack. As we can see in Fig. 7b, overall, larger
α results in better attack performance. However, in the meantime, it will degrade the performance on
the clean set when α is too large, e.g. greater than 0.2."
DISCUSSION,0.53125,"(a) chess board
(b) pure white
(c) colorful"
DISCUSSION,0.5340909090909091,Figure 8: Three different trigger patterns.
DISCUSSION,0.5369318181818182,"The Effect of Different Trigger Patterns. We
investigate our BadTrack attack with three trig-
ger patterns in Fig. 8 under default α = 0.1. The
colorful pattern is from the open-sourced imple-
mentation 2 of [26]. It is generated by drawing
a random matrix of colors and resizing it to the
desired adaptive size using bilinear interpolation. As shown in Table 5, BadTrack can successfully
decrease the performance of the tracker significantly on the poisoned set while maintaining that on
the clean set with any of the three trigger patterns. Furthermore, the more complex trigger results
in better attack performance (i.e. colorful > chess board > pure white). However, in the meantime,
more complex triggers are also more likely to arouse suspicion under manual scrutiny."
DISCUSSION,0.5397727272727273,"The Effect of Different Trigger Sizes During Inference. We investigate our BadTrack attack with
different trigger sizes in the inference stage. Both adaptive sizes and fixed sizes are studied. In Fig. 7c,
the trigger size at is subject to the trigger scale φ (Section 3.4) and proportional to the size of the
target object. As we can see, larger φ will degrade the performance of the benign tracker and is not
always helpful for enhancing the attack performance. When the trigger size increases to a certain
scale, many training examples (region proposals or transformer patches) would miss the chance to"
DISCUSSION,0.5426136363636364,2https://github.com/UMBCvision/SSL-Backdoor
DISCUSSION,0.5454545454545454,Table 5: Attack performance (%) against SiamRPN++ tracker on LaSOT with three trigger patterns.
DISCUSSION,0.5482954545454546,"attack
trigger →
chess board
pure white
colorful"
DISCUSSION,0.5511363636363636,"test set ↓
AUC
Pr
Pnorm
AUC
Pr
Pnorm
AUC
Pr
Pnorm"
DISCUSSION,0.5539772727272727,"Benign
Clean
47.88
48.49
55.70
47.88
48.49
55.70
47.88
48.49
55.70
Poison
44.83
44.13
50.78
46.31
46.31
52.92
42.96
42.64
49.01"
DISCUSSION,0.5568181818181818,"clean-label
Clean
49.25
49.83
56.94
46.90
48.13
54.47
47.67
48.32
55.34
Poison
19.51
17.33
21.47
28.05
26.87
33.04
9.13
6.56
10.17"
DISCUSSION,0.5596590909090909,"cover the whole trigger pattern. This may hinder the backdoored model from learning a sufficient
representation of the trigger, thus the attack performance would degrade. While in Fig. 7d, the trigger
at is a fixed value. As we can see, when at = 10, the performance of the backdoored tracker is rather
high (low attack effectiveness), whereas when at increases to 20, the performance of the benign
tracker drops dramatically, which means the decrease of the performance of the backdoored tracker is
not due to the backdoor effect. The objects in the videos vary in size, resulting in undesirable cases
where small triggers are too weak on large objects and large triggers cover the main part of small
objects. We conclude that an adaptive size is necessary for a successful attack."
CONCLUSION AND LIMITATION,0.5625,"5
Conclusion and Limitation"
CONCLUSION AND LIMITATION,0.5653409090909091,"In this paper, we propose a poison-only backdoor attack (BadTrack) on VOT models for the first time.
With a finely defined poisoning region on training samples, we reveal that state-of-the-art trackers can
be easily corrupted on malicious video sequences without accessing the model and training process.
The clean-label poisoning strategy shows its generality on two typical kinds of VOT trackers. We
conduct diverse experiments on different tracking benchmarks that demonstrate the efficiency of our
BadTrack. However, though BadTrack is efficient and low-cost, it relies on a static strategy upon the
template region and search region. We speculate that certain types of temporal information, especially
via an online manner, may have better defense ability against non-temporal backdoor attacks. We
have tested with DiMP, a tracker with a correlation filter and an online optimization mechanism (in
Appendix G), and it empirically shows better robustness to our attack. Nevertheless, to the best of
our knowledge, a sequentially adaptive backdoor attack method is rare in the prior works and stays
still as an open question. We would leave it as a next step for future work."
CONCLUSION AND LIMITATION,0.5681818181818182,Acknowledgment
CONCLUSION AND LIMITATION,0.5710227272727273,"The authors acknowledge the support from the Shenzhen Science and Technology Programs (Grant
No. RCYX20200714114523079 and JCYJ20220818101014030)."
REFERENCES,0.5738636363636364,References
REFERENCES,0.5767045454545454,"[1] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-
convolutional siamese networks for object tracking. In European conference on computer vision,
pages 850–865. Springer, 2016."
REFERENCES,0.5795454545454546,"[2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative
model prediction for tracking. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 6182–6191, 2019."
REFERENCES,0.5823863636363636,"[3] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer
tracking.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8126–8135, 2021."
REFERENCES,0.5852272727272727,"[4] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on
deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017."
REFERENCES,0.5880681818181818,"[5] Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia, Yong Zhao, and Rongrong
Ji. One-shot adversarial attacks on visual tracking with dual attention. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 10176–10185, 2020."
REFERENCES,0.5909090909090909,"[6] Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, and Jianjun Zhao. Tat: Targeted backdoor attacks
against visual object tracking. Pattern Recognition, 142:109629, 2023."
REFERENCES,0.59375,"[7] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan
Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
5374–5383, 2019."
REFERENCES,0.5965909090909091,"[8] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Mingzhen Huang,
Juehuan Liu, Yong Xu, et al. Lasot: A high-quality large-scale single object tracking benchmark.
International Journal of Computer Vision, 129(2):439–461, 2021."
REFERENCES,0.5994318181818182,"[9] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating
backdooring attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019."
REFERENCES,0.6022727272727273,"[10] Qing Guo, Xiaofei Xie, Felix Juefei-Xu, Lei Ma, Zhongguo Li, Wanli Xue, Wei Feng, and Yang
Liu. Spark: Spatial-aware online incremental attack against visual tracking. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XXV 16, pages 202–219. Springer, 2020."
REFERENCES,0.6051136363636364,"[11] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark
for generic object tracking in the wild. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 43(5):1562–1577, 2019."
REFERENCES,0.6079545454545454,"[12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences, 114(13):3521–3526, 2017."
REFERENCES,0.6107954545454546,"[13] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder, Luka
ˇCehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, et al.
The sixth visual object tracking vot2018 challenge results. In Proceedings of the European
Conference on Computer Vision (ECCV) Workshops, pages 0–0, 2018."
REFERENCES,0.6136363636363636,"[14] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with
siamese region proposal network. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 8971–8980, 2018."
REFERENCES,0.6164772727272727,"[15] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++:
Evolution of siamese visual tracking with very deep networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 4282–4291, 2019."
REFERENCES,0.6193181818181818,"[16] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor attack in the
physical world. arXiv preprint arXiv:2104.02361, 2021."
REFERENCES,0.6221590909090909,"[17] Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, and Shu-Tao Xia. Few-shot backdoor
attacks on visual object tracking. In ICLR, 2022."
REFERENCES,0.625,"[18] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor
attack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 16463–16472, 2021."
REFERENCES,0.6278409090909091,"[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13, pages 740–755. Springer, 2014."
REFERENCES,0.6306818181818182,"[20] Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International
Conference on Computer Design (ICCD), pages 45–48. IEEE, 2017."
REFERENCES,0.6335227272727273,"[21] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.6363636363636364,"[22] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Track-
ingnet: A large-scale dataset and benchmark for object tracking in the wild. In Proceedings of
the European conference on computer vision (ECCV), pages 300–317, 2018."
REFERENCES,0.6392045454545454,"[23] Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani Thuraisingham.
Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmen-
tation. In Proceedings of the 2021 ACM Asia Conference on Computer and Communications
Security, pages 363–377, 2021."
REFERENCES,0.6420454545454546,"[24] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtube-
boundingboxes: A large high-precision human-annotated data set for object detection in video.
In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
5296–5305, 2017."
REFERENCES,0.6448863636363636,"[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115:211–252, 2015."
REFERENCES,0.6477272727272727,"[26] Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash.
Backdoor attacks on self-supervised learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 13337–13346, 2022."
REFERENCES,0.6505681818181818,"[27] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks.
arXiv preprint arXiv:1912.02771, 2019."
REFERENCES,0.6534090909090909,"[28] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 37(9):1834–1848, 2015."
REFERENCES,0.65625,"[29] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. Siamfc++: Towards robust and accu-
rate visual tracking with target estimation guidelines. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 12549–12556, 2020."
REFERENCES,0.6590909090909091,"[30] Bin Yan, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Cooling-shrinking attack: Blinding the
tracker with imperceptible noises. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 990–999, 2020."
REFERENCES,0.6619318181818182,"[31] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal
transformer for visual tracking. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10448–10457, 2021."
REFERENCES,0.6647727272727273,"[32] Xiyu Yan, Xuesong Chen, Yong Jiang, Shu-Tao Xia, Yong Zhao, and Feng Zheng. Hijacking
tracker: A powerful adversarial attack on visual tracking. In ICASSP 2020-2020 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2897–2901.
IEEE, 2020."
REFERENCES,0.6676136363636364,"[33] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning
and relation modeling for tracking: A one-stream framework. In European Conference on
Computer Vision, pages 341–357. Springer, 2022."
REFERENCES,0.6704545454545454,"[34] Yi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning
of backdoors via implicit hypergradient. arXiv preprint arXiv:2110.03735, 2021."
REFERENCES,0.6732954545454546,"[35] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang.
Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 14443–14452, 2020."
REFERENCES,0.6761363636363636,"A
General Visual Object Tracking"
REFERENCES,0.6789772727272727,"We briefly introduce two general tracking pipeline: Siamese trackers and Transformer trackers. As
shown in Fig. 9, the Siamese trackers contain a backbone for feature extraction, then a correlation
module followed with a region proposal network (RPN) for relation modeling. To train a siamese
tracker, a template region Rz and a larger search region Rx are sent into the backbone separately
for feature extracting, as a two-stream pipeline. Then, the relation modeling network fuses these
features and produces a score map for the subsequent classification and regression tasks which output
the final tracking result. The recent transformer-related one-stream trackers instead process Rz and
Rx simultaneously and combine feature extraction and relation modeling into one step without an
explicit RPN module."
REFERENCES,0.6818181818181818,backbone
REFERENCES,0.6846590909090909,backbone
REFERENCES,0.6875,classification
REFERENCES,0.6903409090909091,regression
REFERENCES,0.6931818181818182,template
REFERENCES,0.6960227272727273,search
REFERENCES,0.6988636363636364,"depthwise
correlation
region proposal"
REFERENCES,0.7017045454545454,score map
REFERENCES,0.7045454545454546,"(x,y,w,h)
output"
REFERENCES,0.7073863636363636,(a) Two-stream Siamese tracker
REFERENCES,0.7102272727272727,"feature
extraction"
REFERENCES,0.7130681818181818,"&
relation
modeling"
REFERENCES,0.7159090909090909,classification
REFERENCES,0.71875,regression
REFERENCES,0.7215909090909091,template
REFERENCES,0.7244318181818182,search
REFERENCES,0.7272727272727273,score map
REFERENCES,0.7301136363636364,"(x,y,w,h)
output
transformer layeys"
REFERENCES,0.7329545454545454,(b) One-stream Transformer tracker
REFERENCES,0.7357954545454546,Figure 9: Two different pipelines of training stage.
REFERENCES,0.7386363636363636,"The processes of extracting training examples of these two kinds of trackers are different but share
similar characteristics. Denote V = {Ii}N
i=1 as all N video frames of the training dataset and
B = {bi}N
i=1 = {(x0i, y0i, wi, hi)}N
i=1 the ground-truth bounding boxes of the target objects in V,
where x0, y0 are the central coordinate and w, h are the width and height respectively. Template and
search image pairs {(Iz, Ix)} are sampled from V as the inputs of the models."
REFERENCES,0.7414772727272727,"In Siamese trackers, take SiamRPN++ for example, a template region Rz from Iz with the size of
az × az where az =
p"
REFERENCES,0.7443181818181818,"(w + (w + h)/2)(h + (w + h)/2) and a search region Rx from Ix with the
size ax ×ax where ax = 2×az are first cropped. Then a score map is computed by relation modeling
between the features of Rz and Rx. Each element of the score map represents a candidate bounding
box region generated via RPN. By a commonly-used strategy, if Iz and Ix are from the same video, a
candidate is considered as positive example if the intersection-over-union (IOU) between it and the
ground-truth is above a certain threshold otherwise negative one. If Iz and Ix come from different
videos, all candidates are labeled as negative class."
REFERENCES,0.7471590909090909,"Likewise in Transformer trackers, take OSTrack for example, az is instead calculated as az =
2 ×
√"
REFERENCES,0.75,"w × h. The training examples are patches divided from the input. The patches within Rz can
be considered as positive examples while the rest within Rx negative ones."
REFERENCES,0.7528409090909091,"B
Training Settings of the Trackers"
REFERENCES,0.7556818181818182,"SiamRPN++. Our experiments are based on the open-sourced codes 3. We adopt the same training
strategy and parameters as in the codes. The SiamRPN++ tracker is trained on COCO [19], ImageNet"
REFERENCES,0.7585227272727273,3https://github.com/STVIR/pysot
REFERENCES,0.7613636363636364,"DET [25], ImageNet VID [25] and YouTube-BoundingBoxes [24] datasets with four NVIDIA A100
GPUs. Rz and Rx are resized to 127 × 127 and 255 × 255 respectively. We train the model for 20
epochs with a batch size of 28. An SGD optimizer with momentum 0.9, weight decay of 5 × 10−4
and an initial learning rate of 0.005 is adopted. A log learning rate scheduler with a final learning rate
of 0.0005 is used. There is also a learning rate warm-up strategy for the first 5 epochs."
REFERENCES,0.7642045454545454,"OSTrack. Our experiments are based on the open-sourced codes 4. We adopt the same training
strategy and parameters as in the codes. The OSTrack tracker is trained on COCO [19], LaSOT [7],
GOT10k [11] and TrackingNet [22] datasets with four NVIDIA A100 GPUs. Rz and Rx are resized
to 128 × 128 and 256 × 256 respectively. We train the model for 300 epochs with a batch size of
32. An AdamW optimizer with weight decay of 1 × 10−4 and an initial learning rate of 0.0004 is
adopted. The learning rate is scaled to 0.1 times when the epochs reach to 240."
REFERENCES,0.7670454545454546,"C
BadTrack Attack on OSTrack Without Candidate Elimination Modules"
REFERENCES,0.7698863636363636,"We test the attack performance of our BadTrack on OSTrack tracker without candidate elimination
modules on three datasets. As shown in Table 6, the results are similar to those in the main paper. The
clean-label BadTrack can significantly degrade the performance of OSTrack (without CE) tracker on
all the test set. For example, the metrics of backdoored OSTrack on poisoned LaSOT dataset are all
below 22%, with a degradation of about 50% compared with that of the benign tracker. While the
performance on the clean set hardly decreases. Similar results can be found on other test set. However,
the dirty-label strategy barely shows attack effect. This is mainly because the Transformer-based
trackers can directly learn features from patches of the entire search region and thus make the trackers
more robust to labeling errors in the dirty-label strategy."
REFERENCES,0.7727272727272727,Table 6: Attack performance (%) against OSTrack (w/o CE) tracker. The best results are boldfaced.
REFERENCES,0.7755681818181818,"attack
test set
LaSOT
LaSOText
GOT10k"
REFERENCES,0.7784090909090909,"AUC
Pr
Pnorm
AUC
Pr
Pnorm
AO
SR0.5
SR0.75"
REFERENCES,0.78125,"Benign
Clean
69.26
75.08
78.79
47.09
52.84
57.06
86.39
95.52
87.53
Poison
68.19
73.54
77.37
46.76
52.51
56.76
85.89
95.14
86.55"
REFERENCES,0.7840909090909091,"Dirty-Label
Clean
68.14
73.94
77.57
46.98
52.68
56.71
86.32
95.88
87.45
Poison
67.88
73.57
77.19
47.15
52.79
56.92
85.57
95.05
86.65"
REFERENCES,0.7869318181818182,"Clean-Label
Clean
68.49
74.33
77.78
47.07
52.93
57.02
86.44
95.84
87.67
Poison
20.28
21.42
21.83
13.68
15.85
18.78
34.06
35.16
33.26"
REFERENCES,0.7897727272727273,"D
Representative Tracking Results of BadTrack Attack on OSTrack"
REFERENCES,0.7926136363636364,"Fig. 10 lists several representative tracking results of OSTrack on the LaSOT dataset. In most cases
(Fig. 10a), the attacked tracker will deviate from the target object with the trigger pattern, for it
regards the trigger as part of the background instead of the object. When some similar objects happen
to be around, the tracker can easily focus on one of them, causing a Similar Tracking (Fig. 10b),
otherwise it may track half of the target object without trigger, i.e. Half Tracking (Fig. 10c). There
are also few Unstable Tracking (Fig. 10d) cases when the target object is tracked sometimes but lost
at other times. Successful Easy Tracking (Fig. 10e) only happens when the background is pretty
pure since the tracker still thinks the object with trigger looks more like the original object compared
with the pure background. But this is not always the case. When the color of the target object is the
same as the background’s (Fig. 10f), it will also cause a Lost Tracking. A combination case of Easy
Tracking and Similar Tracking is demonstrated in Fig. 10g."
REFERENCES,0.7954545454545454,4https://github.com/botaoye/OSTrack
REFERENCES,0.7982954545454546,(a) bottle-1. Lost Tracking. The predicted bounding boxes deviate from the target object directly.
REFERENCES,0.8011363636363636,(b) racing-10. Similar Tracking. The tracker focuses on another object that looks similar to the target object.
REFERENCES,0.8039772727272727,(c) surfboard-4. Half Tracking. Only half of the target object without the trigger is tracked.
REFERENCES,0.8068181818181818,(d) guitar-3. Unstable Tracking. The target object is tracked sometimes but lost at other times. This rarely happens.
REFERENCES,0.8096590909090909,"(e) kite-4. Easy Tracking. The tracker successfully tracks the target object. This only happens when the background
is pretty pure."
REFERENCES,0.8125,"(f) fox-2. Special case. Though the background is pure white, the target object is also white. In this case, lost
tracking happens."
REFERENCES,0.8153409090909091,"(g) crab-6. Combination case. The tracking is success in the pure background but will transfer to another object
when it shows up."
REFERENCES,0.8181818181818182,"Figure 10: Representative tracking results of OSTrack on the LaSOT dataset. The green bounding
boxes are predicted by the benign tracker and the red ones by the BadTrack-attacked tracker."
REFERENCES,0.8210227272727273,"E
Robustness to More Potential Defenses"
REFERENCES,0.8238636363636364,"E.1
Robustness to Gaussian Noise"
REFERENCES,0.8267045454545454,"We test the robustness of BadTrack to Gaussian noise. Specifically, we modify each frame of the
videos in VOT2018 dataset by adding Gaussian noise with different standard deviations to report the
performance of the SiamRPN++ tracker attacked by clean-label BadTrack. As shown in Fig. 11a,
instead of recovering the performance of the tracker on the poisoned set, a stronger Gaussian noise"
REFERENCES,0.8295454545454546,Table 7: Attack performance (%) against OSTrack tracker on different attributes.
REFERENCES,0.8323863636363636,"attack
attribute
all
occlusion
deformation"
REFERENCES,0.8352272727272727,"test set
AUC
Pr
Pnorm
AUC
Pr
Pnorm
AUC
Pr
Pnorm"
REFERENCES,0.8380681818181818,"Benign
Clean
68.94
89.89
83.69
65.94
89.83
79.87
66.20
87.46
81.64
Poison
68.72
89.58
83.57
66.74
91.03
81.39
66.56
88.27
82.66"
REFERENCES,0.8409090909090909,"Clean-Label
Clean
68.70
89.29
83.21
64.91
88.06
78.42
66.38
87.01
81.70
Poison
20.66
27.32
25.86
26.03
36.43
34.06
23.14
30.51
29.09"
REFERENCES,0.84375,"will further reduce the performance on both clean and poisoned set. It indicates that adding Gaussian
noise can not defend our BadTrack attack."
REFERENCES,0.8465909090909091,"E.2
Robustness to Model Pruning"
REFERENCES,0.8494318181818182,"We investigate the robustness of BadTrack to model pruning. To be specific, we use head pruning
[21] to mask the less important attention heads of OSTrack tracker according to the head importance
scores and test the performance on LaSOT dataset. As shown in Fig. 11b, the performance on the
poisoned set increases a bit as that on the clean set decreases. But it can never recover to a normal
high performance. It demonstrates that model pruning also can not defend BadTrack."
REFERENCES,0.8522727272727273,"E.3
Robustness to Fine-tuning on All Clean Training Data"
REFERENCES,0.8551136363636364,"We also verify the robustness of BadTrack to fine-tune the SiamRPN++ tracker on all the clean
training data. As shown in Fig. 11c, the performance of the tracker on poisoned VOT2018 dataset can
not completely recovered after 20 epochs, while that on the clean set will decrease due to over-fitting.
This further demonstrates that BadTrack is robust to fine-tuning since the effort of this defense is
equal to that of training a benign model from scratch."
REFERENCES,0.8579545454545454,"0
5
10
15
20
25
Standard deviation 0.10 0.15 0.20 0.25 0.30 0.35 EAO"
REFERENCES,0.8607954545454546,"clean set
poisoned set 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 Acc"
REFERENCES,0.8636363636363636,"EAO
Acc"
REFERENCES,0.8664772727272727,(a) Gaussian Noise
REFERENCES,0.8693181818181818,"12
24
36
48
60
72
84
96
108
120
132
Pruned heads 10 20 30 40 50 60 70 80 %"
REFERENCES,0.8721590909090909,"clean set
poisoned set"
REFERENCES,0.875,"AUC
Pr
Pnorm"
REFERENCES,0.8778409090909091,(b) Model Pruning
REFERENCES,0.8806818181818182,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Epoch 0.15 0.20 0.25 0.30 0.35 EAO"
REFERENCES,0.8835227272727273,"clean set
poisoned set
0.54 0.55 0.56 0.57 0.58 0.59 0.60 0.61 Acc"
REFERENCES,0.8863636363636364,"EAO
Acc"
REFERENCES,0.8892045454545454,(c) Fine-tuning
REFERENCES,0.8920454545454546,Figure 11: The results of more potential defenses against clean-label BadTrack.
REFERENCES,0.8948863636363636,"F
Attack Results on OTB100 Dataset with Different Attributes"
REFERENCES,0.8977272727272727,"To show the effectiveness of BadTrack on video sequences with different attributes, we evaluate the
attacked trackers on OTB100 [28] dataset. Each sequence of OTB100 has several different attributes.
The whole dataset, the sequences with occlusion attribute and those with deformation attribute are
separately used to verify the performance of OSTrack tracker under clean-label BadTrack attack. As
shown in Table 7, BadTrack can significantly degrade the performance on all the poisoned sequences
with both attributes. For example, the AUC, Pr and Pnorm metrics of backdoored OSTrack on
poisoned deformation sequences drop 43.42%, 57.76% and 53.57% respectively compared with that
of the benign tracker. While the performance on the clean data hardly decreases. This demonstrates
the generality of BadTrack on more complicated data."
REFERENCES,0.9005681818181818,"G
Attack Results on DiMP Tracker"
REFERENCES,0.9034090909090909,"From the perspective of the input data, there is a kind of DiMP-like trackers that take several frames
as the input and the frames are not divided into template region and search region with different sizes.
As these trackers break the assumption of BadTrack, we test the attack effect of BadTrack on DiMP
[2]. The experiments are based on the open-sourced codes 5. We adopt the same training strategy and
parameters as in the codes. The DiMP tracker is trained on COCO [19], LaSOT [7], GOT10k [11]
and TrackingNet [22] datasets with four NVIDIA A100 GPUs. We train the model for 50 epochs
with a batch size of 10. An Adam optimizer with an initial learning rate of 0.0002 is adopted. The
learning rate is scaled to 0.2 times after per 15 epochs. We evaluate the benign and attacked tracker
on LaSOT testing set."
REFERENCES,0.90625,"As shown in Table 8, all the metrics of backdoored DiMP on poisoned set only drop about 5% to
6% compared with that of the benign tracker. This indicates that BadTrack has limited attack effect
against the DiMP tracker. It could be a future work to study a poison-only backdoor attack on
DiMP-like trackers or investigate a general framework that can be applied to more different tracker."
REFERENCES,0.9090909090909091,"Table 8: Attack performance (%) against DiMP tracker.
attack
test set
AUC
OP50
OP75
Pr
Pnorm"
REFERENCES,0.9119318181818182,"Benign
Clean
55.30
65.23
45.50
55.02
62.93
Poison
54.67
63.95
42.88
54.47
62.65"
REFERENCES,0.9147727272727273,"Clean-Label
Clean
54.40
63.80
44.87
52.72
61.67
Poison
49.56
57.67
37.54
48.91
56.86"
REFERENCES,0.9176136363636364,"H
The Effect of Different Numbers or Intervals of Poisoned Frames During
Inference"
REFERENCES,0.9204545454545454,"We investigate our clean-label BadTrack on OSTrack tracker by poisoning different numbers (N) of
the video frames. Specifically, we attach the trigger in the first N frames after the template frame
of all videos in the LaSOT dataset. As shown in Fig. 12a, the attack effect increases (the tracking
performance decreases) with the number of poisoned frames. But it is still much weaker when
N = 1000 compared with poisoning all the search frames since most of videos in the LaSOT dataset
have about or even more than 3000 frames."
REFERENCES,0.9232954545454546,"We also investigate clean-label BadTrack by poisoning the video frames with different intervals (M).
Specifically, we attach the trigger in one frame every M frames. As shown in Fig. 12b, the attack
effect increases as M decreases. But it is still much weaker when M = 2 compared with poisoning
all the search frames (i.e. M = 1). This result is consistent with that in Fig. 12a because smaller M
means more frames are poisoned."
REFERENCES,0.9261363636363636,"We find that the tracker will probably lose track of the target object if the trigger exists. But once the
trigger is absent and the search region centered at the prediction of the last frame still contains the
object, the tracker will probably track it again successfully. The results in Fig. 12 fully confirm this
characteristic and the overall attack performance is proportional to the number of poisoned frames at
any part of the videos."
REFERENCES,0.9289772727272727,"I
Comparison with TAT"
REFERENCES,0.9318181818181818,"TAT [6] is a concurrent work with BadTrack, which also studies backdoor attacks on visual object
tracking. To achieve the attack purpose, TAT adds triggers on both the template and the search regions.
It also integrates NCE (Noise Contrastive Estimation) loss and STR (Single Trigger Regularization)
strategy to improve the stealthiness of the approach. We summarize the main differences between
TAT and BadTrack in Table 9."
REFERENCES,0.9346590909090909,5https://github.com/visionml/pytracking
REFERENCES,0.9375,"100
200
300
400
500
600
700
800
900
1000
frame number 40 45 50 55 60 65 70 %"
REFERENCES,0.9403409090909091,"AUC
Pr
Pnorm"
REFERENCES,0.9431818181818182,(a) Poisoned frame numbers
REFERENCES,0.9460227272727273,"10
9
8
7
6
5
4
3
2
frame interval 40 45 50 55 60 65 %"
REFERENCES,0.9488636363636364,"AUC
Pr
Pnorm"
REFERENCES,0.9517045454545454,(b) Poisoned frame intervals
REFERENCES,0.9545454545454546,Figure 12: Clean-label BadTrack attack with different numbers or intervals of poisoned frames.
REFERENCES,0.9573863636363636,Table 9: Comparison between TAT and BadTrack.
REFERENCES,0.9602272727272727,"attack
Attack Paradigm
Attack Goal
Label Modification
Target Tracker"
REFERENCES,0.9630681818181818,"TAT
Training-Controlled
Targeted
Dirty-Label
Siamese tracker only
BadTrack
Poison-Only
Untargeted
Dirty/Clean-Label
Siamese and Transformer trackers"
REFERENCES,0.9659090909090909,"1. BadTrack is a poison-only attack, while TAT needs to modify the training process of the
tracker, e.g. modifying training loss functions."
REFERENCES,0.96875,"2. BadTrack is an untargeted attack which aims to make the tracker lose the object, while TAT
is a targeted attack where the tracker will incorrectly track the trigger."
REFERENCES,0.9715909090909091,"3. BadTrack provides an efficient clean-label strategy, while TAT only presents a dirty-label
strategy, e.g. falsifying the score map generated by the backbone."
REFERENCES,0.9744318181818182,"4. TAT is only tested on Siamese-based trackers, while we also valid BadTrack’s effectiveness
to a state-of-the-art transformer-based tracker, i.e. OSTrack."
REFERENCES,0.9772727272727273,"J
Broader Impacts"
REFERENCES,0.9801136363636364,"An adversary may use our work to release a malicious dataset after poisoning a small part of the
benign data. Users may train their trackers with the collected malicious dataset. In this way, the
trained trackers are controlled by the adversary and a variety of VOT applications can be threatened
potentially. Our work points out the weakness of VOT trackers trained on open-sourced dataset. An
adversary may also directly release the attacked models. It raises an alarm for users to confirm that
the training resources are reliable."
REFERENCES,0.9829545454545454,"In the current research community, there are several ways to obtain (large-scale) datasets: (1) from
an official website; (2) from a public mirror (due to restricted access to the official website or
slow connecting speed); (3) from third parties. Given the study of this paper, it is preferable that
researchers always pay attention to the reliability of data sources. Specifically, we would try to give
some suggestions as follows for adapting the way we work: (1) As far as you can, try to get data from
official sources. (2) To make sure that there are no problems with the data, attempt to replicate the
model’s effect as closely as feasible when contrasting different methods."
REFERENCES,0.9857954545454546,"Besides the action of verifying the reliability of the sources, in general, a researcher should always
be aware of the possible data backdoors when one receives a novel data source. Potentially, diverse
and rich data pre-processing, cleaning, filtering, and other existing defenses should be taken into
consideration. Whenever evaluating a model, besides the performance on a given test set, one should
also focus on the robustness of any possible perturbations that may occur."
REFERENCES,0.9886363636363636,"Furthermore, for a VOT researcher, we could give some more perspectives on the way of working,
e.g. possible defense strategies:"
REFERENCES,0.9914772727272727,"1. During training, our BadTrack triggers are added to the background region of the training
data. In order to eliminate the triggers, some certain concatenation, mixup, or re-generation"
REFERENCES,0.9943181818181818,"operations could be carried out for data preprocessing. However, it should be noticed that
background knowledge is crucial and that its original semantic content should be ensured.
2. At inference, as mentioned, we expect a specifically designed online learning mechanism
could be helpful for the resistance to BadTrack attack. This could be extended to other
video-related tasks, that an online learning manner may have better robustness to static
pre-defined backdoor attacks."
REFERENCES,0.9971590909090909,"We believe that the attack-and-defense game will make the research community safer and better. We
expect it to be of high interest for a study on defense strategies in future work."
