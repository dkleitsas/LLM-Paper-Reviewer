Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0028169014084507044,"We give the first polynomial-time algorithm for the testable learning of halfspaces
in the presence of adversarial label noise under the Gaussian distribution. In the
recently introduced testable learning model, one is required to produce a tester-
learner such that if the data passes the tester, then one can trust the output of the
robust learner on the data. Our tester-learner runs in time poly(d/ϵ) and outputs
a halfspace with misclassification error O(opt) + ϵ, where opt is the 0-1 error of
the best fitting halfspace. At a technical level, our algorithm employs an iterative
soft localization technique enhanced with appropriate testers to ensure that the
data distribution is sufficiently similar to a Gaussian. Finally, our algorithm can
be readily adapted to yield an efficient and testable active learner requiring only
d polylog(1/ϵ) labeled examples."
INTRODUCTION,0.005633802816901409,"1
Introduction"
INTRODUCTION,0.008450704225352112,"A (homogeneous) halfspace is a Boolean function h : Rd →{±1} of the form hw(x) = sign (w · x),
where w ∈Rd is the corresponding weight vector and the function sign : R →{±1} is defined as
sign(t) = 1 if t ≥0 and sign(t) = −1 otherwise. Learning halfspaces from random labeled examples
is a classical task in machine learning, with history going back to the Perceptron algorithm [Ros58].
In the realizable PAC model [Val84] (i.e., with consistent labels), the class of halfspaces is known to
be efficiently learnable without distributional assumptions. On the other hand, in the agnostic (or
adversarial label noise) model [Hau92, KSS94] even weak learning is computationally intractable in
the distribution-free setting [Dan16, DKMR22, Tie22]."
INTRODUCTION,0.011267605633802818,"These intractability results have served as a motivation for the study of agnostic learning in the
distribution-specific setting, i.e., when the marginal distribution on examples is assumed to be well-
behaved. In this context, a number of algorithmic results are known. The L1-regression algorithm
of [KKMS08] agnostically learns halfspaces within near-optimal 0-1 error of opt + ϵ, where opt is
the 0-1 error of the best-fitting halfspace. The running time of the L1-regression algorithm is d ˜
O(1/ϵ2)
under the assumption that the marginal distribution on examples is the standard Gaussian (and for
a few other structured distributions) [DGJ+10, DKN10]. While the L1 regression method leads to
improper learners, a proper agnostic learner with qualitatively similar complexity was recently given"
INTRODUCTION,0.014084507042253521,"in [DKK+21]. The exponential dependence on 1/ϵ in the running time of these algorithms is known
to be inherent, in both the Statistical Query model [DKZ20, GGK20, DKPZ21] and under standard
cryptographic assumptions [DKR23]."
INTRODUCTION,0.016901408450704224,"Interestingly, it is possible to circumvent the super-polynomial dependence on 1/ϵ by relaxing the
final error guarantee — namely, by obtaining a hypothesis with 0-1 error f(opt)+ϵ, for some function
f(t) that goes to 0 when t →0. (Vanilla agnostic learning corresponds to the case that f(t) = t.) A
number of algorithmic works, starting with [KLS09], developed efficient algorithms with relaxed
error guarantees; see, e.g., [ABL17, Dan15, DKS18, DKTZ20b]. The most relevant results in the
context of the current paper are the works [ABL17, DKS18] which gave poly(d/ϵ) time algorithms
with error Copt + ϵ, for some universal constant C > 1, for learning halfspaces with adversarial
label noise under the Gaussian distribution. Given the aforementioned computational hardness results,
these constant-factor approximations are best possible within the class of polynomial-time algorithms."
INTRODUCTION,0.01971830985915493,"A drawback of distribution-specific agnostic learning is that it provides no guarantees if the assumption
on the marginal distribution on examples is not satisfied. Ideally, one would additionally like an
efficient method to test these distributional assumptions, so that: (1) if our tester accepts, then we
can trust the output of the learner, and (2) it is unlikely that the tester rejects if the data satisfies the
distributional assumptions. This state-of-affairs motivated the definition of a new model — introduced
in [RV23] and termed testable learning — formally defined below:
Definition 1.1 (Testable Learning with Adversarial Label Noise [RV23]). Fix ϵ, τ ∈(0, 1] and let
f : [0, 1] 7→R+. A tester-learner A (approximately) testably learns a concept class C with respect
to the distribution Dx on Rd with N samples, and failure probability τ if the following holds. For
any distribution D on Rd × {±1}, the tester-learner A draws a set S of N i.i.d. samples from D. In
the end, it either rejects S or accepts S and produces a hypothesis h : Rd 7→{±1}. Moreover, the
following conditions must be met:"
INTRODUCTION,0.022535211267605635,"• (Completeness) If D truly has marginal Dx, A accepts with probability at least 1 −τ."
INTRODUCTION,0.02535211267605634,"• (Soundness) The probability that A accepts and outputs a hypothesis h for which
Pr(x,y)∼D[h(x) ̸= y] > f(opt) + ϵ , where opt := ming∈C Pr(x,y)∼D[g(x) ̸= y] is at most τ."
INTRODUCTION,0.028169014084507043,"The probability in the above statements is over the randomness of the sample S and the internal
randomness of the tester-learner A."
INTRODUCTION,0.030985915492957747,"The initial work [RV23] and the followup paper [GKK22] focused on the setting where f(t) = t
(i.e., achieving optimal error of opt + ϵ). These works developed general moment-matching based
algorithms that yield testable learners for a range of concept classes, including halfspaces. For the
class of halfspaces in particular, they gave a testable agnostic learner under the Gaussian distribution
with sample complexity and runtime d ˜
O(1/ϵ2) — essentially matching the complexity of the problem
in the standard agnostic PAC setting (without the testable requirement). Since the testable learning
setting is at least as hard as the standard PAC setting, the aforementioned hardness results imply that
the exponential complexity dependence in 1/ϵ cannot be improved."
INTRODUCTION,0.03380281690140845,"In this work, we continue this line of investigation. We ask whether we can obtain fully polynomial
time testable learning algorithms with relaxed error guarantees — ideally matching the standard
(non-testable) learning setting. Concretely, we study the following question:"
INTRODUCTION,0.036619718309859155,"Is there a poly(d/ϵ) time tester-learner for halfspaces with error f(opt) + ϵ?
Specifically, is there a constant-factor approximation?"
INTRODUCTION,0.03943661971830986,"As our main result, we provide an affirmative answer to this question in the strongest possible sense
— by providing an efficient constant-factor approximate tester-learner."
INTRODUCTION,0.04225352112676056,"Labeling examples often requires hiring expert annotators, paying for query access to powerful
language models, etc. On the other hand unlabeled examples are usually easy to obtain in practice.
This has motivated the design of active learning algorithms that, given a large dataset of unlabeled
examples, carefully choose a small subset to ask for their labels. A long line of research has studied
active learning of halfspaces under structured distributions either with clean labels or in the presence
of noise [BBL06, BBZ07, BL13, ABL17, She21]."
INTRODUCTION,0.04507042253521127,"In principle, testable learners could only rely on unlabeled examples during their “testing phase” in
order to verify that the x-marginal satisfies the required properties; and, assuming that it does so,"
INTRODUCTION,0.04788732394366197,"proceed to run an active learning algorithm. However, as the testing process is often coupled with
the downstream learning task — that depends on labeled examples — it is an interesting question to
investigate the design of testable learners in the active learning setting."
INTRODUCTION,0.05070422535211268,"Definition 1.2 (Testable Active Learning). A tester-learner has sample complexity N and label
complexity q in the active learning model if it draws N unlabeled samples from Dx, i.e., the x-
marginal of D over Rd, and queries the labels of at most q samples throughout its computation."
INTRODUCTION,0.05352112676056338,"Main Result
Our main result is the first polynomial-time tester-learner for homogeneous halfspaces
with respect to the Gaussian distribution in the presence of adversarial label noise. Formally, we
establish the following theorem:"
INTRODUCTION,0.056338028169014086,"Theorem 1.3 (Testable Active Learning Halfspaces under Gaussian Marginals). Let ϵ, τ ∈(0, 1) and
C be the class of homogeneous halfspaces on Rd. There exists a tester-learner with sample complexity
N = poly(d, 1/ϵ) log(1/τ) and runtime poly(d N) for C with respect to N(0, I) up to 0-1 error
O(opt) + ϵ, where opt is the 0-1 error of the best fitting function in C, that fails with probability at
most τ. In addition, in the active learning model, the algorithm has sample complexity N and label
complexity q = eO((d log(1/ϵ) + log2(1/ϵ)) log(1/τ))."
INTRODUCTION,0.059154929577464786,"Before we provide an overview of our technical approach, some remarks are in order. Theorem 1.3
gives the first algorithm for testable learning of halfspaces that runs in poly(d/ϵ) time and achieves
dimension-independent error (i.e., error of the form f(opt) + ϵ, where f satisfies limt→0 f(t) =
0.) Moreover, the constant-factor approximation achieved is best possible, matching the known
guarantees without the testable requirement and complexity lower bounds. Prior to our work, the only
known result in the testable setting, due to [RV23, GKK22], achieves error opt + ϵ with complexity
dpoly(1/ϵ). A novel (and seemingly necessary) feature of our approach is that the testing components
of our algorithm depend on the labels (as opposed to the label-oblivious testers of [RV23, GKK22]).
As will be explained in the proceeding discussion, to prove Theorem 1.3 we develop a testable version
of the well-known localization technique that may be of broader interest."
INTRODUCTION,0.061971830985915494,"Independent Work
In concurrent and independent work, [GKSV23] gave an efficient tester-
learner for homogeneous halfspaces under the Gaussian distribution (and strongly log-concave
distributions) achieving dimension-independent error guarantees. Specifically, for strongly log-
concave distributions, their algorithm achieves error O(k1/2opt1−1/k) with sample complexity and
running time of poly(d e
O(k), (1/ϵ) e
O(k)). That is, they obtain error O(optc), where c < 1 is a
universal constant, in polyc(d/ϵ) time; and error eO(opt) in quasi-polynomial (d/ϵ)polylog(d) time.
For the Gaussian distribution, they achieve error O(opt) with complexity poly(d, 1/ϵ). We remark
that since the learner in [GKSV23] corresponds to the stochastic gradient descent algorithm from
[DKTZ20b], which requires labeled sample in every iteration, it is not clear whether their approach
can be made label-efficient."
OVERVIEW OF TECHNIQUES,0.0647887323943662,"1.1
Overview of Techniques"
OVERVIEW OF TECHNIQUES,0.0676056338028169,"Our tester-learner is based on the well-known localization technique that has been used in the
context of learning halfspaces with noise; see, e.g., [ABL17, DKS18]. At a high-level, the idea
of localization hinges on updating a given hypothesis by using “the most informative” examples,
specifically examples that have very small margin with respect to the current hypothesis. Naturally, the
correctness of this geometric technique leverages structural properties of the underlying distribution
over examples, namely concentration, anti-concentration, and anti-anti-concentration properties (see,
e.g., [DKTZ20a]). While the Gaussian distribution satisfies these properties, they are unfortunately
hard to test. In this work, we show that localization can be effectively combined with appropriate
efficient testing routines to provide an efficient tester-learner."
OVERVIEW OF TECHNIQUES,0.07042253521126761,"Localization and a (Weak) Proper Testable Learner
Assume that we are given a halfspace defined
by the unit vector w with small 0-1 error, namely Prx∼Dx[sign(v∗·x) ̸= sign(w·x)] ≤δ, for some
small δ > 0, where v∗is the unit vector defining an optimal halfspace. The localization approach
improves the current hypothesis, defined by w, by considering the conditional distribution D′ on the
points that fall in a thin slice around w, i.e., the set of points x satisfying |w · x| ≤O(δ). The goal is
to compute a new (unit) weight vector w′ that is close to an optimal halfspace, defined by v∗, with Θ(i) v∗"
OVERVIEW OF TECHNIQUES,0.07323943661971831,"u
δ
δ
δ"
OVERVIEW OF TECHNIQUES,0.07605633802816901,"w
θ = Θ(δ)"
OVERVIEW OF TECHNIQUES,0.07887323943661972,"Figure 1: The disagreement region between a halfspace with normal vector w and the target v∗
is shown in green. The unit direction u corresponds to the projection of v∗on the orthogonal
complement of w. We assume that the ℓ2 distance of the two halfspaces is δ (and thus their angle is
Θ(δ)). Since the slabs Si = {iδ ≤|x · w| ≤(i + 1)δ} have width δ, the x-coordinate of the start of
the i-th box is Θ(i)."
OVERVIEW OF TECHNIQUES,0.08169014084507042,"respect to D′, i.e., Prx′∼D′x[sign(v∗· x′) ̸= sign(w′ · x′)] ≤α, for an appropriate α > 0. We can
then show that the halfspace defined by w′ will be closer to the target halfspace (defined by v∗) with
respect to the original distribution, i.e., we have that Prx∼Dx[sign(v∗· x) ̸= sign(w′ · x)] ≤O(δα).
By repeating the above step, we iteratively reduce the disagreement with v∗until we reach our
target error of O(opt). Similarly to [DKS18], instead of “hard” conditioning on a thin slice, we
perform a “soft” localization step where (by rejection sampling) we transform the x-marginal to a
Gaussian whose covariance is O(δ2) in the direction of w and identity in the orthogonal directions,
i.e., Σ = I −(1 −δ2)ww⊤; see Fact 3.2."
OVERVIEW OF TECHNIQUES,0.08450704225352113,"A crucial ingredient of our approach is a proper testable, weak agnostic learner with respect to the
Gaussian distribution. More precisely, our tester-learner runs in polynomial time and either reports
that the x-marginal is not N(0, I) or outputs a unit vector w with small constant distance to the
target v∗, i.e., ∥w −v∗∥2 ≤1/100; see Proposition 2.1. Our weak proper tester-learner first verifies
that the given x-marginal approximately matches constantly many low-degree moments with the
standard Gaussian; and if it does, it returns the vector defined by the degree-1 Chow parameters,
i.e., c = E(x,y)∼D[yx]. Our main structural result in this context shows that if Dx approximately
matches its low-degree moments with the standard Gaussian, then the Chow parameters of any
homogeneous LTF with respect to Dx are close to its Chow parameters with respect to N(0, I),
i.e., for any homogeneous LTF f(x), we have that Ex∼Dx[f(x)x] ≈Ex∗∼N(0,I)[f(x∗)x∗]; see
Lemma 2.3. Since the Chow vector of a homogeneous LTF with respect to the Gaussian distribution
is parallel to its normal vector v∗(see Fact 2.2), it is not hard to show that the Chow vector of the
LTF with respect to Dx will not be very far from v∗and will satisfy the (weak) learning guarantee
of ∥c −v∗∥2 ≤1/100. Finally, to deal with label noise, we show that if x′ has bounded second
moments (a condition that we can efficiently test), we can robustly estimate Ex∼Dx[f(x)x] with
samples from D up to error O(√opt) (see Lemma 2.4), which suffices for our purpose of weak
learning. The detailed description of our weak, proper tester-learner can be found in Section 2."
OVERVIEW OF TECHNIQUES,0.08732394366197183,"From Parameter Distance to Zero-One Error
Having a (weak) testable proper learner, we can
now use it on the localized (conditional) distribution D′ and obtain a vector w′ that is closer to
v∗in ℓ2 distance; see Lemma 3.3. However, our goal is to obtain a vector that has small zero-one
disagreement with the target halfspace v∗. Assuming that the underlying x-marginal is a standard
normal distribution, and that ∥w −v∗∥2 = δ, it holds that Prx∼Dx[sign(w · x) ̸= sign(v∗· x)] =
O(δ), which implies that achieving ℓ2-distance O(opt) + ϵ suffices. We give an algorithm that can
efficiently either certify that small ℓ2-distance implies small zero-one disagreement with respect to
the given marginal Dx or declare that Dx is not the standard normal."
OVERVIEW OF TECHNIQUES,0.09014084507042254,"The disagreement region of v∗and w is a union of two “wedges” (intersection of two halfspaces);
see Figure 1. In order for our algorithm to work, we need to verify that these wedges do not contain
too much probability mass. Similarly to our approach for the weak tester-leaner, one could try a
moment-matching approach and argue that if Dx matches its “low”-degree moments with N(0, I),
then small ℓ2-distance translates to small zero-one disagreement. However, we will need to use this
result for vectors that are very close to the target (but still not close enough), namely ∥w −v∗∥2 = δ,"
OVERVIEW OF TECHNIQUES,0.09295774647887324,"where δ = Θ(ϵ); this would require matching poly(1/δ) many moments (as we essentially need to
approximate the wedge of Figure 1 with a polynomial) and would thus lead to an exponential runtime
of dpoly(1/δ)."
OVERVIEW OF TECHNIQUES,0.09577464788732394,"Instead of trying to approximate the disagreement region with a polynomial, we will make use of
the fact that our algorithm knows w (but not v∗) and approximate the disagreement region by a
union of cylindrical slabs. We consider slabs of the form Si = {x : iδ ≤|w · x| ≤(i + 1)δ}.
If the target distribution is Gaussian, we know that the set |w · x| ≫
p"
OVERVIEW OF TECHNIQUES,0.09859154929577464,"log(1/ϵ) has mass O(δ)
and we can essentially ignore it. Therefore, we can cover the whole space by considering roughly
M = O(
p"
OVERVIEW OF TECHNIQUES,0.10140845070422536,"log(1/δ)/δ) slabs of width δ and split the disagreement region into the disagreement
region inside each slab Si. We have that"
OVERVIEW OF TECHNIQUES,0.10422535211267606,"Pr
x∼Dx[sign(w · x) ̸= sign(v∗· x)] ≤ M
X"
OVERVIEW OF TECHNIQUES,0.10704225352112676,"i=1
Pr[|u · x| ≥i | x ∈Si] Pr[Si] ,"
OVERVIEW OF TECHNIQUES,0.10985915492957747,"where u is the unit direction parallel to the projection of the target v∗onto the orthogonal complement
of w, see Figure 1. By the anti-concentration of the Gaussian distribution we know that each slab
should have mass at most O(δ). Note that this is easy to test by sampling and computing empirical
estimates of the mass of each slab. Moreover, assuming that underlying distribution is N(0, I),
we have that, conditional on Si the orthogonal direction u · x ∼N(0, 1) (see Figure 1) and in
particular u · x has bounded second moment. We do not know the orthogonal direction u as it
depends on the unknown v∗but we can check that, conditional on the slab Si, the projection of Dx
onto the orthogonal complement of w is (approximately) mean-zero and has bounded covariance (i.e.,
bounded above by 2I). Note that both these conditions hold when x ∼N(0, I) and can be efficiently
tested with samples in time poly(d, 1/δ). Under those conditions we have that that Pr[Si] = O(δ)
for all i. Moreover, when the conditional distribution on Si (projected on the orthogonal complement
of w) has bounded second moment, we have that Pr[|u · x| ≥i | x ∈Si] ≤O(1/i2) . Combining
the above, we obtain that under those assumptions the total probability of disagreement is at most
O(δ). The detailed analysis is given in Section 3.1."
OVERVIEW OF TECHNIQUES,0.11267605633802817,"Obtaining an Active Learner.
Our localization-based algorithm can be readily adapted to yield an
efficient active learner-tester. We first observe that the tester that verifies that the parameter distance
is proportional to 0/1 error does not require any labels, as it relies on verifying moments of the
x-marginal of the underlying distribution. During each iteration of localization, since the learner is
only trying to find a constant approximation of the defining vector of the unknown halfspace, the
learner only needs roughly O(d) samples that fall in the slice we localize to. Since we can selectively
query only the labels of samples that fall in the right region and the total number of iterations is
at most O(log(1/ϵ)), the labels needed to learn the defining vectors is roughly O(d log(1/ϵ)). In
the end, the learner needs to find the best halfspace among the ones from O(log(1/ϵ)) localization
iterations. We remark this can also be done with polylog(1/ϵ) many label queries (see Lemma 3.6)."
PRELIMINARIES,0.11549295774647887,"1.2
Preliminaries"
PRELIMINARIES,0.11830985915492957,"We use small boldface characters for vectors and capital bold characters for matrices. We use [d] to
denote the set {1, 2, . . . , d}. For a vector x ∈Rd and i ∈[d], xi denotes the i-th coordinate of x,"
PRELIMINARIES,0.12112676056338029,"and ∥x∥2 :=
qPd
i=1 x2
i the ℓ2 norm of x. We use x · y := Pn
i=1 xiyi as the inner product between
them. We use 1{E} to denote the indicator function of some event E."
PRELIMINARIES,0.12394366197183099,"We use Ex∼D[x] for the expectation of the random variable x according to the distribution D and
Pr[E] for the probability of event E. For simplicity of notation, we may omit the distribution when
it is clear from the context. For µ ∈Rd, Σ ∈Rd×d, we denote by N(µ, Σ) the d-dimensional
Gaussian distribution with mean µ and covariance Σ. For (x, y) ∈X distributed according to D,
we denote Dx to be the marginal distribution of x. Let f : Rd 7→{±1} be a boolean function and
D a distribution over Rd. The degree-1 Chow parameter vector of f with respect to D is defined as
Ex∼D [f(x)x]. For a halfspace h(x) = sign(v · x), we say that v is the defining vector of h."
PRELIMINARIES,0.1267605633802817,"Moment-Matching
In what follows, we use the phrase “A distribution D on Rd matches k moments
with a distribution Q up to error ∆”. Similarly to [GKK22], we formally define approximate moment-
matching as follows."
PRELIMINARIES,0.1295774647887324,"Definition 1.4 (Approximate Moment-Matching). Let k ∈N be a degree parameter and let M(k, d)
be the set of d-variate monomials of degree up to k. Moreover, let ∆∈R|M(k,d)|
+
be a slack
parameter (indexed by the monomials of M(k, d)), satisfying ∆0 = 0. We say that two distributions
D, Q match k moments up to error ∆if | Ex∼D[m(x)] −Ex∼Q[m(x)]| ≤∆m for every monomial
m(x) ∈M(k, d). When the error bound ∆is the same for all monomials we overload notation and
simply use ∆instead of the parameter ∆."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.1323943661971831,"2
Weak Testable Proper Agnostic Learning"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.1352112676056338,"As our starting point, we give an algorithm that performs testable proper learning of homogeneous
halfspaces in the presence of adversarial label noise with respect to the Gaussian distribution. The
main result of this section is given below and its proof can be found in Appendix A.
Proposition 2.1 (Proper Testable Learner with Adversarial Label Noise). Let D be a distribution on
labeled examples (x, y) ∈Rd × {±1} with x-marginal Dx. Suppose that there exists a unit vector
v∗∈Rd such that Pr(x,y)∼D [sign(v∗· x) ̸= y] ≤opt. There exists an algorithm (Algorithm 2)"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.13802816901408452,"that given τ, η ∈(0, 1), N = d e
O(1/η2) log(1/τ) i.i.d. unlabeled samples from Dx, queries the labels
of O(d/η2) log(d/τ) of them, and runs in time poly(d, N) and does one of the following:"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.14084507042253522,"• The algorithm reports that the x-marginal of D is not N(0, I)."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.14366197183098592,• The algorithm outputs a unit vector w ∈Rd.
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.14647887323943662,"With probability at least 1 −τ the following holds: (1) if the algorithm reports anything, the report
is correct, and (2) if the algorithm returns a vector w, it holds ∥v∗−w∥2 ≤CA
√opt + η, where
CA > 0 is an absolute constant."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.14929577464788732,"A couple of remarks are in order. First, notice that if the algorithm outputs a vector w, we only have
the guarantee that ∥v∗−w∥2 is small — instead of that the hypothesis halfspace hw(x) = sign(w·x)
achieves small 0-1 error. Nonetheless, as we will show in the next section, conditioned on D passing
some test, the error of the halfspace hw will be at most opt plus a constant multiple of ∥v∗−w∥2
(see Lemma 3.1). Second, unlike the testable improper learners in [RV23, GKK22] — which achieve
error of opt + η with similar running time and sample complexity — our testable proper learner
achieves the weaker error guarantee of O(√opt + η). This suffices for our purposes for the following
reason: in the context of our localization-based approach, we only need an efficient proper weak
learner that achieves sufficiently small constant error. This holds for our proper testable learner, as
long as both opt and η are bounded above by some other sufficiently small constant."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.15211267605633802,"To obtain a proper learner, we proceed to directly estimate the defining vector v∗of the target
halfspace h∗(x) = sign(v∗· x), where we assume without loss of generality that v∗is a unit vector.
The following simple fact relating the degree-1 Chow-parameters of a homogeneous halfspace and
its defining vector will be useful for us.
Fact 2.2 (see, e.g., Lemma 4.3 of [DKS18]). Let v be a unit vector and h(x) = sign(v · x) be the
corresponding halfspace. If x is from N(0, I), then we have that Ex∼N(0,I) [h(x)x] =
p"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.15492957746478872,2/π v.
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.15774647887323945,"To apply Fact 2.2 in our context, we need to overcome two hurdles: (i) the x marginal of D is not
necessarily the standard Gaussian, and (ii) the labels are not always consistent with h∗(x). The
second issue can be circumvented by following the approach of [DKS18]. In particular, if the x
marginal of D is indeed Gaussian, we can just treat D as a corrupted version of (x, h∗(x)), where
x ∼N(0, I) and estimate the Chow parameters robustly."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.16056338028169015,"To deal with the first issue, we borrow tools from [GKK22]. At a high level, we certify that the
low-degree moments of Dx — the x marginal of D — approximately match the corresponding
moments of N(0, I) before estimating the Chow parameters. Importantly, this testing procedure
uses only unlabeled samples. To establish the correctness of our algorithm, we show that, for any
distribution B that passes the moment test, the Chow parameters of a halfspace under B will still be
close to its defining vector. We state the lemma formally below. Its proof can be found in Appendix A.
Lemma 2.3 (From Moment-Matching to Chow Distance). Fix η > 0. Let k = C log(1/η)/η2"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.16338028169014085,"and ∆=
1
kdk

1
C
√ k"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.16619718309859155,"k+1
, where C > 0 is a sufficiently large absolute constant. Let B be a"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.16901408450704225,"distribution whose moments up to degree k match with those of N(0, I) up to additive error ∆. Let"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.17183098591549295,"h(x) = sign(v · x) be a halfspace. Then we have that
Ex∼B [h(x)x] −
q"
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.17464788732394365,"2
π v

2 ≤O(√η) ."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.17746478873239438,"With Lemma 2.3 in hand, we know it suffices to estimate the Chow parameters of h∗with respect
to Dx. This would then give us a good approximation to v∗conditioned on Dx indeed having its
low-degree moments approximately match those of N(0, I). We use the following lemma, which
estimates the Chow parameters robustly under adversarial label noise.
Lemma 2.4. Let η, τ ∈(0, 1) and G be a distribution over Rd × {±1} such that Ex∼Gx[xx⊤] ≼2I.
Let v ∈Rd be a unit vector such that Pr(x,y)∼G[sign(v·x) ̸= y] ≤η. Then there exists an algorithm
that takes N = O(d/η2) log(d/τ) labeled samples from G, runs in time poly(N), and outputs a
vector w such that ∥Ex∼Gx [sign(v · x)x] −w∥2 ≤O(√η) with probability at least 1 −τ."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.18028169014084508,"The proof follows from standard arguments in robust mean estimation and the details can be found in
Appendix C. We remark that the number of labeled samples required by this procedure is (nearly)
linear with respect to d for constant η, which is important for us to obtain (nearly) optimal label
complexity overall."
WEAK TESTABLE PROPER AGNOSTIC LEARNING,0.18309859154929578,"We are now ready to describe our testable proper learner. In particular, it first certifies that the
moments of the underlying distribution match with the standard Gaussian and then draws i.i.d.
samples to estimate the Chow parameter. It then follows from Fact 2.2, Lemmas 2.3 and 2.4 that the
resulting Chow vector, conditioned on that the moment test pass, is closed to the defining vector of
the optimal halfspace. We given the pseudo-code of the algorith and its analysis, which also serves as
the proof of Propostion 2.1, in the end of Appendix A."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.18591549295774648,"3
Efficient Testable Learning of Halfspaces"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.18873239436619718,"In this section, we give our tester-learner for homogeneous halfspaces under the Gaussian distribution,
thereby proving Theorem 1.3. Throughout this section, we will fix an optimal halfspace h∗(x) =
sign(v∗· x), i.e., a halfspace with optimal 0-1 error."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.19154929577464788,"The structure of this section is as follows: In Section 3.1, we present a tester which certifies that
the probability of the disagreement region between two halfspaces whose defining vectors are close
to each other is small under Dx. In Section 3.2, we present and analyze our localization step and
combine it with the tester from Section 3.1 to obtain our final algorithm."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.19436619718309858,"3.1
From Parameter Distance to 0-1 Error"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.19718309859154928,"For two homogeneous halfspaces hu(x) = sign(u · x) and hv(x) = sign(v · x), where u, v are
unit vectors, if Dx is the standard Gaussian, N(0, I), we can express the probability mass of their
disagreement region as follows (see, e.g., Lemma 4.2 of [DKS18]):
Pr
x∼Dx [hu(x) ̸= hv(x)] ≤O (∥u −v∥2) .
(1)"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2,"Hence, learning homogeneous halfspaces under Gaussian marginals can often be reduced to approxi-
mately learning the defining vector of some optimal halfspace h∗. This is no longer the case if Dx is
an arbitrary distribution, which may well happen in our regime. We show in this section that it is still
possible to “certify” whether some relationship similar to the one in Equation (1) holds."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2028169014084507,"In particular, given a known vector v, we want to make sure that for any other vector w that is close
to v, the mass of the disagreement region between the halfspaces defined by by v, w respectively
is small. To do so, we will decompose the space into many thin “slabs” that are stacked on top of
each other in the direction of v. Then, we will certify the mass of disagreement restricted to each
of the slab is not too large. For slabs that are close to the halfspace sign(v · x), we can check these
slabs must not themselves be too heavy. For slabs that are far away from the halfspace, we use
the observation that the points in the disagreement region must then have large components in the
subspace perpendicular to v. Hence, as long as D has its second moment bounded, we can bound
the mass of the disagreement region in these far-away slabs using standard concentration inequality.
Formally, we have the following lemma, whose proof can be found in Appendix B.
Lemma 3.1 (Wedge Bound). Let Dx be a distribution over Rd. Given a unit vector v and parameters
η, τ ∈(0, 1/2), there exists an algorithm (Algorithm 1) that draws i.i.d. samples from Dx, runs in"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2056338028169014,"Input: Sample access to a distribution Dx over Rd; tolerance parameter η > 0; unit vector
v ∈Rd; failure probability τ ∈(0, 1).
Output:
Certifies that for all unit vectors w such that ∥w −v∥2
≤
η it holds that
Prx∼Dx[sign(v · x) ̸= sign(w · x)] ≤Cη, for some absolute constant C > 1, or reports
that Dx is not N(0, I).
1. Set B = ⌈
p"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2084507042253521,log(1/η)/η⌉.
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2112676056338028,"2. Let eD be the empirical distribution obtained by drawing poly(d, 1/η) log(1/τ) samples from
Dx.
3. For integers −B −1 ≤i ≤B, define Ei to be the event that {v · x ∈[iη, (i + 1)η]} and
EB+1 to be the event that {|v · x| ≥
p"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2140845070422535,log(1/η)}.
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.21690140845070421,"4. Verify that PB+1
i=−B−1
PrN(0,I) [Ei] −Pr e
D [Ei]
 ≤η."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.21971830985915494,"5. Let Si the distribution of eD conditioned on Ei and S⊥
i
be Si projected on the subspace
orthogonal to v.
6. For each i, verify that S⊥
i has bounded covariance, i.e., check that Ex∼S⊥
i [xx⊤] ≼2I ."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.22253521126760564,Algorithm 1: Wedge-Bound
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.22535211267605634,"time poly(d, 1/η) log(1/τ), and reports either one of the following: (i) For all unit vectors w such
that ∥w −v∥2 ≤η it holds Prx∼Dx[sign(v · x) ̸= sign(w · x)] ≤Cη, for some absolute constant
C > 1. (ii) Dx is not the standard Gaussian N(0, I). Moreover, with probability at least 1 −τ, the
report is accurate."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.22816901408450704,"3.2
Algorithm and Analysis: Proof of Theorem 1.3"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.23098591549295774,"We employ the idea of “soft” localization used in [DKS18]. In particular, given a vector v and a
parameter σ, we use rejection sampling to define a new distribution Dv,σ that “focuses” on the region
near the halfspace sign(v · x)."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.23380281690140844,"Fact 3.2 (Rejection Sampling, Lemma 4.7 of [DKS18]). Let D be a distribution on labeled examples
(x, y) ∈Rd × {±1}. Let v ∈Rd be a unit vector and σ ∈(0, 1). We define the distribution Dv,σ as
follows: draw a sample (x, y) from D and accept it with probability e−(v·x)2·(σ−2−1)/2. Then, Dv,σ
is the distribution of (x, y) conditional on acceptance. If the x-marginal of D is N(0, I), then the
x-marginals of Dv,σ is N(0, Σ), where Σ = I−(1−σ2)vvT . Moreover, the acceptance probability
of a point is σ."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.23661971830985915,"We now briefly discuss the sample and label complexities of this rejection sampling procedure. Since
the acceptance probability is σ, we need roughly 1/σ samples from D to simulate one sample from
Dv,σ. On the other hand, to simulate one labeled sample from D, one only need 1 label query in
addition to the unlabeled samples as one can query only the points that are accepted by the procedure.
This makes sure that rejection sampling is efficient in terms of its label complexity."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.23943661971830985,"The main idea of localization is the following. Let v be a vector such that ∥v −v∗∥2 ≤δ. Suppose
that we use localization to the distribution Dv,δ. If we can learn a halfspace with defining vector w
that achieves sufficiently small constant error with respect to the new distribution Dv,δ, we can then
combine our knowledge of w and v to produce a new halfspace with significantly improved error
guarantees under D. The following lemma formalizes this geometric intuition."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.24225352112676057,"Lemma 3.3. Let v∗, v be two unit vectors in Rd such that ∥v −v∗∥2 ≤δ ≤1/100. Let Σ ="
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.24507042253521127,"I −(1 −δ2)vvT and w be a unit vector such that
w −
Σ1/2v∗
∥Σ1/2v∗∥2"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.24788732394366197,"2
≤ζ ≤1/100. Then it holds

Σ−1/2w
∥Σ−1/2w∥2
−v∗

2
≤5(δ2 + δζ)."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2507042253521127,"We defer the proof to Appendix C. Below, we provide a few useful remarks regarding the relevant
parameters."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2535211267605634,"Remark 3.4. Observe that in Lemma 3.3 we require that the distance of v and v∗is smaller than
1/100. While this constant is not the best possible, we remark that some non-trivial error is indeed
necessary so that the localization step works. For example, assume that v and v∗are orthogonal,
i.e., v · v∗= 0, and that Σ = I −(1 −ξ2)vvT for some ξ ∈[0, 1]. Observe that Σ1/2 scales
vectors by a factor of ξ in the direction of v and leaves orthogonal directions unchanged. Similarly,
its inverse Σ−1/2 scales vectors by a factor of 1/ξ in the direction of v and leaves orthogonal
directions unchanged. Without loss of generality, assume that v = e1, v∗= e2 where e1, e2 are
the two standard basis vectors. Then Σ1/2v∗= v∗. Moreover, assume that w = ae1 + be2 (with
a2 + b2 = 1). We observe that ∥w −Σ1/2v∗/∥Σ1/2v∗∥2∥2
2 = ∥w −v∗∥2
2 = 2 −2b. However,
we have that s = Σ−1/2w = (a/ξ)e1 + be2. Therefore, ∥s/∥s∥2 −v∗∥2
2 = 2 −2b/
p"
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2563380281690141,"(a/ξ)2 + b2.
Notice that for all ξ ∈[0, 1] it holds that s/∥s∥2 is further away from v∗than w, i.e., rescaling by
Σ−1/2 worsens the error."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2591549295774648,"We are now ready to present an iterative testing/learning procedure that will serve as the main
component of our algorithm. At a high level, we first use Algorithm 2 from Proposition 2.1 to learn a
vector v that is close to v∗in ℓ2-distance up to some sufficiently small constant. Then we localize to
the learned halfspace and re-apply Algorithm 2 to iteratively improve v. In particular, we will argue
that, whenever the learned halfspace is still significantly suboptimal, the algorithm either detects that
the underlying distribution is not Gaussian or keeps making improvements such that v gets closer to
v∗conditioned on v still being sub-optimal. The pseudocode and analysis of the algorithm can be
found in Appendix C."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2619718309859155,"Lemma 3.5. Suppose
v∗−v(t)
2 ≤δ ≤1/100. There is an algorithm (Algorithm 3) that with
probability at least 1 −τ, either (i) correctly reports that the x-marginal of D is not N(0, I) or
(ii) computes a unit vector v(t+1) so that either
v∗−v(t+1)
2 ≤δ/2 or
v∗−v(t)
2 ≤Copt,
where C > 0 is an absolute constant. Furthermore, the algorithm draws N = poly(d, 1/δ) log(1/τ)
many unlabeled samples and uses O (d log(d/τ)) label queries."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2647887323943662,"After running Algorithm 3 for at most a logarithmic number of iterations, we know v must be close
to v∗. Then, we can use Algorithm 1 from Lemma 3.1 to certify that the disagreement between the
learned halfspace and h∗(x) is small. One technical difficulty is that we do not know precisely when
to terminate the update and each update does not necessarily monotonically bring v closer to v∗. As
a result, the process only yields a list of vectors v(1), · · · v(t) with the guarantee that at least one of
them is sufficiently closed to v∗. A natural idea is to estimate the 0-1 errors of the halfspaces defined
by the vectors in the list and simply pick the one with the smallest empirical error. Naively, we need
to estimate the errors up to an additive ϵ and this may take up to Ω(1/ϵ2) labeled samples, which
is exponentially worse than our goal in terms of the dependency on ϵ. Nonetheless, we remark that
comparing the errors of two halfspaces which differ by some multiplicative factors can be done much
more efficiently in terms of its label complexity. In particular, it suffices for us to compare the errors
of the two halfspaces restricted to the area in which the predictions made by them differ. Under
this conditioning, the difference between the errors then gets magnified to some constant, making
comparisons much easier. Hence, we can run a tournament among the the halfspaces, which reduces
to perform pairwise comparisons among the lists, to obtain our final winner hypothesis. The result is
summarized in the following lemma."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2676056338028169,"Lemma 3.6 (Tournament). Let ϵ, τ ∈(0, 1) and D a distribution over Rd × {±1}. Given a list
of halfspaces {h(i)}k
i=1, there is an algorithm that draws N = Θ
 
k2 log(k/τ)/ϵ

i.i.d. unlabeled
samples from Dx, uses Θ
 
k2 log(k/τ)

label queries, runs in time poly(d, N) and outputs a
halfspace ˜h satisfying that Pr(x,y)∼D[˜h(x) ̸= y] ≤10 mini Pr(x,y)∼D[h(i)(x) ̸= y] + ϵ."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.2704225352112676,"We provide the pseudocode of the algorithm and its analysis in Appendix C. The proof of Theorem 1.3
then follows."
EFFICIENT TESTABLE LEARNING OF HALFSPACES,0.27323943661971833,"Our final algorithm (Algorithm 5) simply iteratively applies the localized update routine (Algorithm 3)
and then runs a tournament (Algorithm 4) among the O(log(1/ϵ)) candidate halfspaces obtained
from the iterations. The pseudocode and its analysis, which serves as proof of our main theorem, can
be found at the end of Appendix C."
ACKNOWLEDGEMENTS,0.27605633802816903,"4
Acknowledgements"
ACKNOWLEDGEMENTS,0.27887323943661974,"ID was supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER),
and a DARPA Learning with Less Labels (LwLL) grant. DK was supported by NSF Medium Award
CCF-2107547 and NSF Award CCF-1553288 (CAREER). NZ was supported in part by NSF award
2023239, NSF Medium Award CCF-2107079, and a DARPA Learning with Less Labels (LwLL)
grant."
REFERENCES,0.28169014084507044,References
REFERENCES,0.28450704225352114,"[ABL17] P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for efficiently
learning linear separators with noise. J. ACM, 63(6):50:1–50:27, 2017."
REFERENCES,0.28732394366197184,"[BBL06] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proceedings
of the 23rd international conference on Machine learning, pages 65–72, 2006."
REFERENCES,0.29014084507042254,"[BBZ07] M. Balcan, A. Broder, and T. Zhang. Margin based active learning. In Learning Theory:
20th Annual Conference on Learning Theory, COLT 2007, San Diego, CA, USA; June
13-15, 2007. Proceedings 20, pages 35–50. Springer, 2007."
REFERENCES,0.29295774647887324,"[BL13] M. Balcan and P. Long. Active and passive learning of linear separators under log-
concave distributions. In Conference on Learning Theory, pages 288–316. PMLR,
2013."
REFERENCES,0.29577464788732394,"[Dan15] A. Daniely. A PTAS for agnostically learning halfspaces. In Proceedings of The 28th
Conference on Learning Theory, COLT 2015, pages 484–502, 2015."
REFERENCES,0.29859154929577464,"[Dan16] A. Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings
of the 48th Annual Symposium on Theory of Computing, STOC 2016, pages 105–117,
2016."
REFERENCES,0.30140845070422534,"[DGJ+10] I. Diakonikolas, P. Gopalan, R. Jaiswal, R. Servedio, and E. Viola. Bounded indepen-
dence fools halfspaces. SIAM Journal on Computing, 39(8):3441–3462, 2010."
REFERENCES,0.30422535211267604,"[DKK+21] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Agnostic proper
learning of halfspaces under gaussian marginals. In Proceedings of The 34th Conference
on Learning Theory, COLT, 2021."
REFERENCES,0.30704225352112674,"[DKMR22] I. Diakonikolas, D. M. Kane, P. Manurangsi, and L. Ren. Cryptographic hardness of
learning halfspaces with massart noise. CoRR, abs/2207.14266, 2022."
REFERENCES,0.30985915492957744,"[DKN10] I. Diakonikolas, D. M. Kane, and J. Nelson. Bounded independence fools degree-2
threshold functions. In FOCS, pages 11–20, 2010."
REFERENCES,0.3126760563380282,"[DKPZ21] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial re-
gression for agnostic learning under gaussian marginals in the SQ model. In Proceedings
of The 34th Conference on Learning Theory, COLT, 2021."
REFERENCES,0.3154929577464789,"[DKR23] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of
agnostically learning halfspaces and relu regression under gaussian marginals. CoRR,
abs/2302.06512, 2023."
REFERENCES,0.3183098591549296,"[DKS18] I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nasty
noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2018, pages 1061–1073, 2018."
REFERENCES,0.3211267605633803,"[DKTZ20a] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning halfspaces with
massart noise under structured distributions. In Conference on Learning Theory, COLT,
2020."
REFERENCES,0.323943661971831,"[DKTZ20b] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Non-convex SGD learns
halfspaces with adversarial label noise. In Advances in Neural Information Processing
Systems, NeurIPS, 2020."
REFERENCES,0.3267605633802817,"[DKZ20] I. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for
agnostically learning halfspaces and ReLUs under Gaussian marginals. In Advances in
Neural Information Processing Systems, NeurIPS, 2020."
REFERENCES,0.3295774647887324,"[GGK20] S. Goel, A. Gollakota, and A. R. Klivans. Statistical-query lower bounds via functional
gradients. In Advances in Neural Information Processing Systems, NeurIPS, 2020."
REFERENCES,0.3323943661971831,"[GKK22] A. Gollakota, A. Klivans, and P. Kothari. A moment-matching approach to testable
learning and a new characterization of rademacher complexity.
arXiv preprint
arXiv:2211.13312, 2022. To appear in STOC’23."
REFERENCES,0.3352112676056338,"[GKSV23] A. Gollakota, A. R. Klivans, K. Stavropoulos, and A. Vasilyan. An efficient tester-learner
for halfspaces. CoRR, abs/2302.14853, 2023."
REFERENCES,0.3380281690140845,"[Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and
other learning applications. Information and Computation, 100:78–150, 1992."
REFERENCES,0.3408450704225352,"[KKMS08] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces.
SIAM Journal on Computing, 37(6):1777–1805, 2008. Special issue for FOCS 2005."
REFERENCES,0.3436619718309859,"[KLS09] A. Klivans, P. Long, and R. Servedio. Learning Halfspaces with Malicious Noise.
Journal of Machine Learning Research, 10:2715–2740, 2009."
REFERENCES,0.3464788732394366,"[KSS94] M. Kearns, R. Schapire, and L. Sellie. Toward Efficient Agnostic Learning. Machine
Learning, 17(2/3):115–141, 1994."
REFERENCES,0.3492957746478873,"[Ros58] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and
organization in the brain. Psychological Review, 65:386–407, 1958."
REFERENCES,0.352112676056338,"[RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning al-
gorithms. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,
pages 1643–1656, 2023."
REFERENCES,0.35492957746478876,"[She21] J. Shen. On the power of localized perceptron for label-optimal learning of halfspaces
with adversarial noise. In International Conference on Machine Learning, pages 9503–
9514. PMLR, 2021."
REFERENCES,0.35774647887323946,"[Tie22] S. Tiegel. Hardness of agnostically learning halfspaces from worst-case lattice problems.
CoRR, abs/2207.14030, 2022."
REFERENCES,0.36056338028169016,"[Val84] L. G. Valiant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on
Theory of Computing (STOC), pages 436–445. ACM Press, 1984."
REFERENCES,0.36338028169014086,Supplementary Material
REFERENCES,0.36619718309859156,"A
Omitted Proofs for Weak Testable Proper Agnostic Learning"
REFERENCES,0.36901408450704226,"Proof of Lemma 2.3. It suffices to show that for any unit vector u ∈Rd, the following holds:
 E
x∼B [h(x)x · u] −
E
x∼N(0,I) [h(x)x · u]
 ≤O(√η) ."
REFERENCES,0.37183098591549296,The following fact expresses a real number a as an integral of the sign function.
REFERENCES,0.37464788732394366,"Fact A.1. For any a ∈R, it holds a = 1"
REFERENCES,0.37746478873239436,"2
R ∞
0 (sign(a −t) + sign(a + t))dt."
REFERENCES,0.38028169014084506,"We apply Fact A.1 to the term u · x, which gives
 E
x∼B [h(x)x · u] −
E
x∼N(0,I) [h(x)x · u]
 = 1 2 E
x∼B"
REFERENCES,0.38309859154929576,"
h(x)
Z"
REFERENCES,0.38591549295774646,"t≥0
(sign(u · x −t) + sign(u · x + t)) dt
"
REFERENCES,0.38873239436619716,"−
E
x∼N(0,I)"
REFERENCES,0.39154929577464787,"
h(x)
Z"
REFERENCES,0.39436619718309857,"t≥0
(sign(u · x −t) + sign(u · x + t)) dt
 = 1 2  Z t≥0"
REFERENCES,0.3971830985915493,"
E
x∼B [h(x) (sign(u · x −t) + sign(u · x + t))]"
REFERENCES,0.4,"−
E
x∼N(0,I) [h(x) (sign(u · x −t) + sign(u · x + t))]

dt
 ,"
REFERENCES,0.4028169014084507,"where in the last line we switch the order of the integral of t and x by Fubini’s theorem. We then split
the above integral over t into two parts based on the magnitude of t (t > 1/√η versus 0 ≤t ≤1/√η)
and apply the triangle inequality:
 E
x∼B [h(x)x · u] −
E
x∼N(0,I) [h(x)x · u] ≤1 2  Z"
REFERENCES,0.4056338028169014,0≤t≤1/√η
REFERENCES,0.4084507042253521,"
E
x∼B [h(x)sign(u · x −t)] −
E
x∼N(0,I) [h(x)sign(u · x −t)]

dt  + 1 2  Z"
REFERENCES,0.4112676056338028,0≤t≤1/√η
REFERENCES,0.4140845070422535,"
E
x∼B [h(x)sign(u · x + t)] −
E
x∼N(0,I) [h(x)sign(u · x + t)]

dt  + 1 2  Z"
REFERENCES,0.4169014084507042,t≥1/√η
REFERENCES,0.4197183098591549,"
E
x∼B [h(x) (sign(u · x −t) + sign(u · x + t))]"
REFERENCES,0.4225352112676056,"−
E
x∼N(0,I) [h(x) (sign(u · x −t) + sign(u · x + t))]

dt
 .
(2)"
REFERENCES,0.4253521126760563,We start by bounding the integral for t ≥1/√η.
REFERENCES,0.428169014084507,"Lemma A.2 (Chow-Distance Tail). Let Q be distribution over Rd with Ex∼Q[xx⊤] ≼2I. Moreover,
let g(x) : Rd 7→R be a bounded function, i.e., |g(x)| ≤1 for all x ∈Rd. It holds Z"
REFERENCES,0.4309859154929577,"t≥1/√η
E
x∼Q [g(x) (sign(u · x −t) + sign(u · x + t))] dt"
REFERENCES,0.43380281690140843,≤O(√η) .
REFERENCES,0.43661971830985913,"Proof. We split the expectation into two parts based on the relative sizes of |u · x| and t. Specifically,
we can write:  Z"
REFERENCES,0.4394366197183099,"t≥1/√η
E
x∼Q [g(x) (sign(u · x −t) + sign(u · x + t))] dt  ≤  Z"
REFERENCES,0.4422535211267606,"t≥1/√η
E
x∼Q [g(x) (sign(u · x −t) + sign(u · x + t)) 1{|u · x| ≥t}] dt  +  Z"
REFERENCES,0.4450704225352113,"t≥1/√η
E
x∼Q [g(x) (sign(u · x −t) + sign(u · x + t)) 1{|u · x| ≤t}] dt .
(3)"
REFERENCES,0.447887323943662,"For the second term in Equation (3), we rely on the following observation: when |u · x| ≤t, the
quantities u·x−t and u·x+t have opposite signs. Hence, we conclude the integrand is 0 everywhere
and therefore the second term is also 0. For the first term, we have  Z"
REFERENCES,0.4507042253521127,"t≥1/√η
E
x∼Q [g(x) (sign(u · x −t) + sign(u · x + t)) 1{|u · x| ≥t}]  ≤
Z"
REFERENCES,0.4535211267605634,"t≥1/√η
E
x∼Q"
REFERENCES,0.4563380281690141,"g(x) (sign(u · x −t) + sign(u · x + t)) 1{|u · x| ≥t}  ≤
Z"
REFERENCES,0.4591549295774648,"t≥1/√η
E
x∼Q [21{|u · x| ≥t}] ≤4
Z"
REFERENCES,0.4619718309859155,t≥1/√η
REFERENCES,0.4647887323943662,"1
t2 ≤O(√η) ,"
REFERENCES,0.4676056338028169,"where the first inequality follows from the triangle inequality, the second inequality uses the fact that
the sign(·) function is at most 1 and the third inequality follows from Chebyshev’s inequality using
the fact that the E[xx⊤] ≼2I. Combining our analysis for the two terms in Equation (3), we can
then conclude the proof of Lemma A.2."
REFERENCES,0.4704225352112676,"Using the triangle inequality and applying Lemma A.2 on the distributions B and N(0, I), we have
that 1
2  Z"
REFERENCES,0.4732394366197183,t≥1/√η
REFERENCES,0.476056338028169,"
E
x∼B [h(x) (sign(u · x −t) + sign(u · x + t))]"
REFERENCES,0.4788732394366197,"−
E
x∼N(0,I) [h(x) (sign(u · x −t) + sign(u · x + t))]

dt
 ≤O(√η) .
(4)"
REFERENCES,0.48169014084507045,We then turn our attention to the terms Z
REFERENCES,0.48450704225352115,0≤t≤1/√η
REFERENCES,0.48732394366197185,"
E
x∼B [h(x)sign(u · x −t)] −
E
x∼N(0,I) [h(x)sign(u · x −t)]

dt .
(5)  Z"
REFERENCES,0.49014084507042255,0≤t≤1/√η
REFERENCES,0.49295774647887325,"
E
x∼B [h(x)sign(u · x + t)] −
E
x∼N(0,I) [h(x)sign(u · x + t)]

dt .
(6)"
REFERENCES,0.49577464788732395,"To bound Equations (5) and (6), we need the following fact from [GKK22]."
REFERENCES,0.49859154929577465,"Fact A.3 (Theorem 5.6 of [GKK22]). Let h : Rd 7→{±1} be a function of p halfspaces, i.e.,
h(x) = g (h1(x), · · · , hp(x)) where hi are halfspaces and g : {±1}p 7→{±1}. For any k ∈N,"
REFERENCES,0.5014084507042254,"let ∆=
√p"
K,0.504225352112676,"2k
1
dk

1
C′√ k"
K,0.5070422535211268,"k+1
for some sufficiently large absolute constant C′ > 0. Then, for any"
K,0.5098591549295775,"distribution B whose moments up to order k match those of N(0, I) up to ∆, we have

E
x∼N(0,I) [h(x)] −E
x∼B [h(x)]
 ≤
1
√"
K,0.5126760563380282,"k
√p

C log
p"
K,0.5154929577464789,"pk
2p
."
K,0.5183098591549296,for some constant C > 0.
K,0.5211267605633803,"For a fixed t, note that h(x)sign(u · x −t) is a function of two halfspaces. Moreover, from the
assumptions of Lemma 2.3, the distributions B and N(0, I) match k = C log(1/η)/η2 moments"
K,0.523943661971831,"up to error ∆=
1
kdk

1
C
√ k"
K,0.5267605633802817,"k+1
, where C > 0 is a sufficiently large absolute constant. Therefore,
applying Fact A.3 and the triangle inequality gives 1
2  Z"
K,0.5295774647887324,0≤t≤1/√η
K,0.532394366197183,"
E
x∼B [h(x)sign(u · x −t)] −
E
x∼N(0,I) [h(x)sign(u · x −t)]

dt "
K,0.5352112676056338,"≤O(1)
Z"
K,0.5380281690140845,"0≤t≤1/√η
ηdt = O(√η) .
(7)"
K,0.5408450704225352,"Similarly, we can show that Equation (6) is bounded by O(√η). Substituting the bounds from
Equations (4) and (7) into Equation (2) then gives
 E
x∼B [h(x)x · u] −
E
x∼N(0,I) [h(x)x · u]
 ≤O(√η)."
K,0.543661971830986,"Since u is chosen as an arbitrary unit vector, this implies that
 E
x∼B [h(x)x] −
E
x∼N(0,I) [h(x)x]

2
≤O(√η)."
K,0.5464788732394367,Combining this with Fact 2.2 concludes the proof of Lemma 2.3.
K,0.5492957746478874,"Proof of Lemma 2.4. We first show that
Ex∼Gx [sign(v · x) x] −E(x,y)∼G [y x]

2 ≤O(√η). For
any unit vector u, we have that"
K,0.5521126760563381,"E
x∼Gx [sign(v · x) u · x] −
E
(x,y)∼G [y u · x] =
E
(x,y)∼G [(sign(v · x) −y)u · x] ≤
r"
K,0.5549295774647888,"E
(x,y)∼G [(sign(v · x) −y)2]
E
x∼Gx [(u · x)2]"
K,0.5577464788732395,"≤4√η ,"
K,0.5605633802816902,"where we used the Cauchy-Schwarz inequality and the fact that Pr(x,y)∼G[sign(v · x) ̸=
y] ≤η.
Therefore, we have that
Ex∼Gx [sign(v · x)x] −E(x,y)∼G [yx]

2 ≤4√η.
Let
(x(1), y(1)), . . . , (x(N1), y(N1)) be samples drawn from G, where N1 = O(d/η2).
Then, let
ew = (1/N1) PN1
j=1 y(j)x(j). From Chebyshev’s inequality, we know the i-th coordinate of ew should
satisfy Pr[|f
wi −E(x,y)∼D[yx]| ≥η/
√"
K,0.5633802816901409,"d] ≤4d/(N1η2) ≤1/4. Therefore, using the standard me-
dian technique, we can find a wmedian
i
, so that Pr[|wmedian
i
−E(x,y)∼D[yx]| ≥η/
√"
K,0.5661971830985916,"d] ≤τ/d,
using N2 = O(N1 log(d/τ)) samples. Let w = (wmedian
1
, . . . , wmedian
d
), then we have that
E(x,y)∼G [yx] −w

2 ≤O(η) with probability at least 1−τ. Then, using the triangle inequality, we
have that ∥Ex∼Gx [sign(v · x)x] −w∥2 ≤O(√η), which concludes the proof of Lemma 2.4."
K,0.5690140845070423,"Proof of Proposition 2.1. Let k, ∆, N be defined as in Algorithm 2. If Dx is N(0, I), the moments
up to degree k of the x-marginal of the empirical distribution bDN (obtained after drawing N i.i.d.
samples from Dx) are close to those of N(0, I) up to additive error ∆with probability at least
1 −τ/10."
K,0.571830985915493,"If Algorithm 2 did not terminate on Line 3, we then have that the moments up to degree k of bDN are
close to those of N(0, I) up to additive error ∆with probability at least 1 −τ/10. Then, applying
Lemma 2.3 with B = bDN and h(x) = sign(v∗· x), we get that

E
x∼b
DN
[sign(v∗· x)x] −
p"
K,0.5746478873239437,"2/πv∗

2
≤O(√η).
(8)"
K,0.5774647887323944,"Now, suppose we were to query the labels of all points in bDN. We could then construct another
empirical distribution bLN over labeled samples. By our assumption, the error of sign(v∗· x) under"
K,0.5802816901408451,"Input: Sample access to a distribution Dx over unlabeled samples and query access to the labels
of the drawn samples according to D; certification range η; failure probability τ.
Output: Either reports that the Dx is not N(0, I); or returns a unit vector w ∈Rd such that
∥v∗−w∥2 ≤CA
√opt + η."
K,0.5830985915492958,"1. Set k = C log(1/η)/η2 and ∆=
1
kdk

1
C
√ k"
K,0.5859154929577465,"k+1
, where C > 0 is a sufficiently large
absolute constant.
2. Draw N = dCk log k log(1/τ) samples from Dx and construct the empirical distribution
bDN."
K,0.5887323943661972,"3. Certify that the moments of bDN up to degree k match with those of N(0, I) up to error
∆.
4. If the above does not hold; report that Dx is not N(0, I) and terminate."
K,0.5915492957746479,"5. Draw N ′ = O(d/η2) log(d/η) samples from bDN and query their labels. Denote the
resulting labeled samples as S.
6. Use algorithm from Lemma 2.4 on S and obtain w. Return w/∥w∥2."
K,0.5943661971830986,Algorithm 2: Proper Testable Learner
K,0.5971830985915493,"D is at most opt. Hence, the error of sign(v∗· x) under bLN is at most opt + η with probability at
least 1 −τ/10. Next, notice that the set of samples S in Line 5 are exactly i.i.d. samples from bLN.
Hence, applying Lemma 2.4 on bLN then gives that, with probability at least 1 −τ/10, the vector w,
computed on Line 6 of Algorithm 2 satisfies

E
x∼(bLN)x
[sign(v∗· x)x] −w"
K,0.6,"2
≤O
 √opt + η

.
(9)"
K,0.6028169014084507,"Notice that the x-marginal of bLN is exactly bDN. Hence, combining Equations (8) and (9), we get
that
w −
p"
K,0.6056338028169014,"2/πv∗
2 ≤O (√opt + η) , as desired."
K,0.6084507042253521,"B
Proof of Wedge Bound Lemma 3.1"
K,0.6112676056338028,"Proof of Lemma 3.1. Recall that eD is the empirical distribution made up of N i.i.d. samples from
Dx where N = poly(d, 1/η) log(1/τ). We consider eD restricted to a set of thin “slabs” stacked on
each other in the direction of v. More formally, we define Si to be the distribution of eD conditioned
on v · x ∈[(i −1)η, iη], for i ∈[−
p"
K,0.6140845070422535,"log(1/η)/η,
p"
K,0.6169014084507042,"log(1/η)/η], and S⊥
i to be the distribution Si
projected into the subspace orthogonal to v."
K,0.6197183098591549,"Suppose that Dx is indeed N(0, I). Then the distribution eD is the empirical distribution formed by
samples taken from N(0, I). In this case, it is easy to see that both Line 4 and 6 of Algorithm 1 pass
with high probability."
K,0.6225352112676056,"Claim B.1. Assume that Dx = N(0, I). Then the tests at Line 4 and 6 of Algorithm 1 pass with
probability at least 1 −τ/10."
K,0.6253521126760564,"Proof. If x ∼N(0, I), then v · x ∼N(0, 1). If we concatenate the values of PrN(0,I)[Ei]
into a vector, it can be viewed as the discretization of N(0, 1) into 2B + 3 many buckets. On
the other hand, Pr ˜
D[Ei] is an empirical version of this discrete distribution composed of N i.i.d.
samples where N = poly(d, 1/η) log(1/τ). Since we can learn any discrete distribution with
support n up to error η in total variation distance with Θ(n/η2) log(1/τ) samples with probability
at least 1 −τ, it follows that Line 4 will pass with high probability as long as we take more than
Θ(B/η2) log(1/τ) ≤poly(d, 1/η) log(1/τ) many samples."
K,0.6281690140845071,"For Line 6, we remark that S⊥
i is the empirical version of a (d −1)-dimensional standard Gaussian.
Since the empirical mean and the empirical covariance concentrates around the true mean and"
K,0.6309859154929578,"covariance with probability at least 1 −τ if one takes more than Θ(d2/η2) log(1/τ) many samples, it
follows that Line 4 will pass with high probability as long as we take more than Θ(d2/η2) log(1/τ) ≤
poly(d, 1/η) log(1/τ) many samples."
K,0.6338028169014085,"Suppose that both lines pass. We claim that this implies the following: for all unit vectors w such
that ∥w −v∥2 ≤η it holds
Pr
x∼e
D
[sign(v · x) ̸= sign(w · x)] ≤Cη .
(10)"
K,0.6366197183098592,"for some absolute constant C > 1. Given this, we can deduce that the same equation must also hold
for Dx with high probability — albeit with a larger constant C′. To see this, we remark that the left
hand side of the equation can be treated as the error of the halfspace sign(w · x) if the true labels are
generated by sign(v · x). Since the VC-dimension of the class of homogeneous halfspaces is d, we
have that the error for all w under Dx is well-approximated by that under eD up to an additive η with
probability at least 1 −τ given N = poly(d, 1/η) log(1/τ) many samples. Hence, conditioned on
Equation (10), it holds with probability at least 1 −τ that
Pr
x∼Dx [sign(v · x) ̸= sign(w · x)] ≤(C + 1)η ,"
K,0.6394366197183099,for all w.
K,0.6422535211267606,"We now proceed to show Equation (10) holds if the Algorithm 1 did not terminate on Lines 4 and 6.
Conditioned on Line 6, for any unit vector u ∈Rd that is orthogonal to v, we have |E [u · x]| ≤O(1)
and Var [u · x] ≤2. Using Chebyshev’s inequality, for any α > 0, it holds"
K,0.6450704225352113,"Pr
x∼S⊥
i
[|u · x| ≥α] ≤O
1 + η2 α2"
K,0.647887323943662,"
.
(11)"
K,0.6507042253521127,"We can now bound Prx∼e
D [sign(v · x) ̸= sign(w · x)] for an arbitrary unit vector w satisfying
∥w −v∥2 ≤η. We proceed to rewrite w as (1 −γ2)1/2v + γu for some unit vector u ∈Rd that
is orthogonal to v and γ ∈(0, η). Denote γ′ = γ/(1 −γ2)1/2, then the event that sign(v · x) ̸=
sign(w · x) implies that γ′ |u · x| ≥|v · x|. Therefore, we have that"
K,0.6535211267605634,"Pr
x∼Si [sign(v · x) ̸= sign(w · x)] ≤Pr
x∼Si [γ′ |u · x| ≥|v · x|] ≤Pr
x∼S⊥
i
[γ′ |u · x| ≥iη] ≤O
1 + η2 i2 
,"
K,0.6563380281690141,"(12)
where in the second inequality we use the definition of Si, and in the third inequality we use that
γ ≤η and Equation (11). We now bound from above the total disagreement probability between w
and v under eD. We have that"
K,0.6591549295774648,"Pr
x∼e
D
[sign(v · x) ̸= sign(w · x)] ≤Pr
x∼e
D"
K,0.6619718309859155,"h
|v · x| ≥
p"
K,0.6647887323943662,"log(1/η)
i
+ Pr
x∼e
D
[|v · x| ≤η] + √"
K,0.6676056338028169,"log(1/η)/η
X"
K,0.6704225352112676,"|i|>1
Pr
x∼Si [sign(v · x) ̸= sign(w · x)] Pr
x∼e
D
[(i −1)η ≤x · v ≤iη]"
K,0.6732394366197183,≤6η + O (η) √
K,0.676056338028169,"log(1/η)/η
X |i|>1"
K,0.6788732394366197,1 + η2
K,0.6816901408450704,"i2
≤O(η) ,"
K,0.6845070422535211,"where we used that Prx∼e
D[|x · v| >
p"
K,0.6873239436619718,"log(1/η)] ≤3η and Prx∼e
D[|x · v| < η] ≤3η, since in
Line 4 we verified that the probabilities Prx∼e
D[v · x ∈[(i −1)η, iη]] are close to the probabilities
under N(0, I) and hence bounded by O(η), Equation (12), and the fact that the series P"
K,0.6901408450704225,"i
1
i2 is
convergent and less than π2/6."
K,0.6929577464788732,"C
Omitted Proofs of localization and main theorem"
K,0.6957746478873239,"Proof of Lemma 3.3. Since ∥v −v∗∥2 ≤δ, we can write"
K,0.6985915492957746,"v∗=
1
√"
K,0.7014084507042253,"1 + κ2 (v + κu)
(13)"
K,0.704225352112676,"for some κ ∈[0, δ] and some unit vector u perpendicular to v. By definition, Σ1/2 shrinks in the
direction of v by a factor of δ and leaves other orthogonal directions unchanged. Hence, it holds"
K,0.7070422535211267,"Σ1/2v∗=
1
√"
K,0.7098591549295775,1 + κ2 (δv + κu) .
K,0.7126760563380282,"Then, using the triangle inequality, we obtain"
K,0.7154929577464789,"γ :=
Σ1/2v∗
2 ≤
1
√"
K,0.7183098591549296,"1 + κ2 (δ ∥v∥2 + κ ∥u∥2) ≤
2δ
√"
K,0.7211267605633803,"1 + κ2 ≤2δ ,"
K,0.723943661971831,"since κ is upper bounded by δ. Since
w −
Σ1/2v∗
∥Σ1/2v∗∥2"
K,0.7267605633802817,"2
≤ζ, we can write"
K,0.7295774647887324,"w =
Σ1/2v∗
Σ1/2v∗
2
+ av + bu′ ,"
K,0.7323943661971831,"for some |a| , b ∈[0, ζ] and u′ perpendicular to v. We can multiply both sides by γ and get"
K,0.7352112676056338,"γw = Σ1/2v∗+ aγv + bγu′ ,"
K,0.7380281690140845,which implies that
K,0.7408450704225352,γΣ−1/2w = v∗+ aγ/δv + bγu′ .
K,0.7436619718309859,"Using Equation (13), we then have"
K,0.7464788732394366,"γΣ−1/2w =

1
√"
K,0.7492957746478873,1 + κ2 + aγ δ
K,0.752112676056338,"
v +
κ
√"
K,0.7549295774647887,"1 + κ2 u + bγu′.
(14)"
K,0.7577464788732394,"Let λ :=
1
√"
K,0.7605633802816901,1+κ2 + aγ/δ be the coefficient before v. We next identify the range of λ.
K,0.7633802816901408,"Claim C.1. It holds that λ ∈[1 −δ −2ζ, 1 + 2ζ]."
K,0.7661971830985915,"Proof. Recall that κ ∈[0, δ], |a| ∈[0, ζ] and γ ∈[0, 2δ]. If we view λ as a function of κ, a, γ, it is
minimized when κ = δ, a = −ζ, γ = 2δ, which then gives"
K,0.7690140845070422,"λ ≥
1
√"
K,0.7718309859154929,"1 + δ2 −2ζ ≥1 −δ −2ζ ,"
K,0.7746478873239436,"where in the second inequality we use the fact that
1
√"
K,0.7774647887323943,"1+x2 ≥1 −x for x ≥0. On the other hand, λ
is maximized when κ = 0, a = ζ, γ = 2δ, which gives λ ≤1 + 2ζ. Hence, we can conclude that
λ ∈[1 −δ −2ζ, 1 + 2ζ]."
K,0.780281690140845,"We multiply both sides of Equation (14) by
1
λ
√"
K,0.7830985915492957,"1+κ2 , which gives γ
λ
√"
K,0.7859154929577464,"1 + κ2 Σ−1/2w =
1
√"
K,0.7887323943661971,"1 + κ2 v +
κ
λ (1 + κ2)u + b
κ
λ
√"
K,0.7915492957746478,1 + κ2 u′
K,0.7943661971830986,"= v∗+

κ
λ (1 + κ2) −
κ
√"
K,0.7971830985915493,1 + κ2
K,0.8,"
u + b
γ
λ
√"
K,0.8028169014084507,"1 + κ2 u′ ,"
K,0.8056338028169014,"where in the second equality we use Equation (13). We then bound from above and below the norm,
and we get that γ
λ
√"
K,0.8084507042253521,"1 + κ2 Σ−1/2w

2
−1
 ≤
κ
1 + κ2"
K,0.8112676056338028,"1
λ −
p"
K,0.8140845070422535,"1 + κ2
 +
bγ
λ
√"
K,0.8169014084507042,"1 + κ2 ,"
K,0.819718309859155,where we used triangle inequality. Note that
K,0.8225352112676056,"|(1/λ) −
p"
K,0.8253521126760563,"1 + κ2| ≤|1/λ −1| + |1 −
p"
K,0.828169014084507,1 + κ2| ≤(δ + 2ζ)/(1 −δ −2ζ) + κ
K,0.8309859154929577,"and that κ ≤δ. Therefore, we obtain that γ
λ
√"
K,0.8338028169014085,"1 + κ2 Σ−1/2w

2
−1
 ≤4(δ2 + δζ) ."
K,0.8366197183098592,"Input: Sample access to a distribution D over labeled examples; unit vector v ∈Rd; parameters
τ, η, δ > 0.
Output: Either reports that Dx is not N(0, I) or computes a unit vector v′ such that: either
∥v∗−v′∥2 ≤δ/2 or ∥v∗−v′∥2 ≤O(opt)
1. Let Dv,δ be the distribution obtained by running Rejection Sampling with parameters v and δ."
K,0.8394366197183099,"2. Check the acceptance probability is within [δ/2, 3δ/2]. Otherwise, report x-marginals of D is
not N(0, I) ."
K,0.8422535211267606,"3. Let G be the distribution obtained by applying the transformation Σ−1/2 on the x marginals
of Dv,δ where Σ = I −(1 −δ2)vv⊤."
K,0.8450704225352113,"4. Run Algorithm 2 on G with accuracy η = 1/(20000C2
A) to obtain a unit vector w.
5. If the algorithm reports the marginal of G is not N(0, I), terminate and report it."
K,0.847887323943662,"6. Set v′ = Σ1/2w/
Σ1/2w

2 and return v′."
K,0.8507042253521127,Algorithm 3: Testable Localized-Update
K,0.8535211267605634,"Let A =

γ
λ
√"
K,0.856338028169014,"1+κ2 Σ−1/2w

2. We have that

Σ−1/2w
Σ−1/2w

2
−v∗

2
≤∥v∗∥2|1 −1/A| + |1/A −1| ≤5(δ2 + δζ) ."
K,0.8591549295774648,This concludes the proof.
K,0.8619718309859155,"Proof of Lemma 3.5. For simplicity, we denote v(t) as v. We apply the Rejection Sampling procedure
from Fact 3.2 in the direction of v with σ = δ. If the x-marginal of D is N(0, I), the acceptance
probability of Dv,δ is exactly δ. We then estimate the acceptance probability with accuracy ϵ; if it is
not lying inside the interval [δ/2, 3δ/2] (see Line 2 of Algorithm 3), we report that the x-marginal of
D is not standard normal and terminate."
K,0.8647887323943662,"Conditioned on the event that the algorithm did not terminate, we can get an (unlabeled) sample from
the x-marginal of Dv,δ, using O(1/δ) unlabeled samples from Dx. To get a labeled sample from
Dv,δ, we only need 1 additional label query. Note that, under the distribution Dv,δ, the error of the
v∗is"
K,0.8676056338028169,"Pr
(x,y)∼Dv,δ[sign(v∗· x) ̸= y] =
Pr
(x,y)∼D[sign(v∗· x) ̸= y | (x, y) is accepted]"
K,0.8704225352112676,"≤
Pr
(x,y)∼D[sign(v∗· x) ̸= y]/
Pr
(x,y)∼D[(x, y) is accepted]"
K,0.8732394366197183,"≤2opt/δ ,
(15)"
K,0.8760563380281691,"where we used that the probability of the acceptance is at least δ/2. Denote by G the distribution of
(Σ−1/2x, y), where (x, y) ∼Dv,δ and Σ = I −(1 −δ2)vv⊤. We note that if the x-marginal of D
were the standard normal, then x-marginal of G is the standard normal. Hence, we can apply the
algorithm Algorithm 2 from Proposition 2.1. Under the transformed distribution, we have that the
new optimal vector (v∗)′ := Σ1/2v∗/∥Σ1/2v∗∥2."
K,0.8788732394366198,"From Proposition 2.1, running Algorithm 2 on the normalized distribution G with error parameter
η ≤1/(2000C2
A) and failure probability τ consumes poly(d) log(1/τ) unlabeled samples and
O (d log(d/τ)) labeled samples from G. Since the rejection sampling procedure for G accepts points
with probability at least Ω(ϵ) Therefore, it consumes poly(d, 1/ϵ) log(1/τ) samples from D and uses
O (d log(d/τ)) additional label queries. Conditioned on the event that it succeeds (which happens
with probability at least 1 −τ), it either (i) reports that the x-marginal of G is not standard Gaussian
(ii) returns a unit vector w such that"
K,0.8816901408450705,"∥w −(v∗)′∥2 ≤CA
r"
K,0.8845070422535212,"Pr
(x,y)∼G [sign((v∗)′ · x) ̸= y] + η.
(16)"
K,0.8873239436619719,"In case (i), we can directly report that x-marginal of D is not N(0, I) since the x-marginal of G
ought to be N(0, I)."
K,0.8901408450704226,"In case (ii), we claim that at least one of the following hold (a) v before the localized update
is already good enough, i.e. ∥v∗−v∥2 ≤40000CA
2opt; (b) it holds ∥(v∗)′ −w∥2 ≤1/100.
Suppose that (a) does not hold; we will show that it then must hold ∥(v∗)′ −w∥2 ≤1/100. Since
∥v∗−v∥2 ≤δ and ∥v∗−v∥2 > 40000CA
2opt, we have that opt ≤δ/(40000C2
A). Furthermore,
from Equation (15) we have that Pr(x,y)∼Dv,δ[sign(v∗· x) ̸= y] ≤2opt/δ, it then follows that
Pr(x,y)∼G [sign((v∗)′ · x) ̸= y] ≤2opt/δ ≤1/(20000C2
A). Substituting this into Equation (16)
then gives
∥w −(v∗)′∥2 ≤1/100 ."
K,0.8929577464788733,"Using our assumption that ∥v −v∗∥2 ≤δ < 1/100, we can apply Lemma 3.3, which gives that

Σ−1/2w
Σ−1/2w

2
−v∗

2
≤δ/2 ."
K,0.895774647887324,"Hence, we set v(t+1) =

Σ−1/2w
∥Σ−1/2w∥2"
K,0.8985915492957747,"2
and this completes the proof."
K,0.9014084507042254,"Input: Sample access to Dx over unlabeled examples and query access to the labels of the
samples drawn; a list of k halfspaces {h(i)}k
i=1; parameters τ, ϵ > 0.
Output: finds a halfspace ˜h whose error is nearly optimal among the list."
K,0.9042253521126761,"1. Let C be a sufficiently large constant.
2. For i, j = 1 · · · k"
K,0.9070422535211268,"(a) Take N1 := C log(k/τ)/ϵ samples Dx.
(b) Skip the iteration if there are fewer than N2 := 0.1 C log(k/τ) samples such that
h(i)(x) ̸= h(j)(x). Denote the samples satisfying the condition as S1.
(c) Query the labels of a random subset of samples from S1 of size N2. Denote the set
of labeled samples as S2
(d) Compute the empirical errors of h(i), h(j) using S2."
K,0.9098591549295775,"(e) If the empirical errors differ by more than 1/4, mark the one with larger error as
sub-optimal.
3. Return any halfspace among the list that has not been marked as sub-optimal or declare
failure if there is not any."
K,0.9126760563380282,Algorithm 4: Tournament
K,0.9154929577464789,Proof of Lemma 3.6. Fix a halfspace h(i). Suppose there is another halfspace h(j) such that
K,0.9183098591549296,"Pr
(x,y)∼D[h(i)(x) ̸= y] > 10
Pr
(x,y)∼D[h(j)(x) ̸= y] + ϵ."
K,0.9211267605633803,"We argue the halfspace will be marked as sub-optimal with probability at least τ/ log(1/ϵ). For
convenience, denote a := Pr(x,y)∼D[h(i)(x) ̸= y ∧h(i)(x) ̸= h(j)(x)], b := Pr(x,y)∼D[h(j)(x) ̸=
y ∧h(i)(x) ̸= h(j)(x)] and m := Prx∼Dx[h(i)(x) ̸= h(j)]. Notice that a, b are the errors of the two
halfspaces from the area in which their predictions differ and m is the mass of the area. Then, it holds"
K,0.923943661971831,"a + b =
Pr
(x,y)∼D[h(i)(x) ̸= h(j)(x) ∧y = h(i)(x)] +
Pr
(x,y)∼D[h(i)(x) ̸= h(j)(x) ∧y ̸= h(i)(x)] = m ,"
K,0.9267605633802817,"a −b =
Pr
(x,y)∼D"
K,0.9295774647887324,"h
h(i)(x) ̸= y
i
−
Pr
(x,y)∼D"
K,0.9323943661971831,"h
h(j)(x) ̸= y
i
≥9
Pr
(x,y)∼D[h(j)(x) ̸= y] + ϵ > 9b."
K,0.9352112676056338,"This further implies that a/m −b/m > 4/5. Notice that a/m, b/m are precisely the errors of h(i),
h(j) under the conditional distribution of D restricted to the area in which h(i)(x) ̸= h(j)(x). Hence,"
K,0.9380281690140845,"with Θ(log(k/τ)) many samples, the corresponding empirical errors will differ by at least 1/4 with
probability at least τ/k."
K,0.9408450704225352,"On the other hand, suppose"
K,0.9436619718309859,"Pr
(x,y)∼D[h(i)(x) ̸= y] ≤
Pr
(x,y)∼D[h(j)(x) ̸= y]."
K,0.9464788732394366,"This implies that a/m < b/m. As a result, the algorithm either skips the iteration if there are fewer
than Θ(log(k/τ)) many labeled samples from the conditional distribution or the empirical error of
h(i) will not be less than that of h(j) by more than 1/4 with probability at least τ/k2 if there are
enough samples."
K,0.9492957746478873,"The correctness of the lemma then follows from union bound. Finally, we remark m, the mass of
the area we condition to, should be at least ϵ if the errors of the two halfspaces differ by at least ϵ.
In that case, it takes on average Θ(1/ϵ) unlabeled samples and 1 additional label query to simulate
one labeled sample from the conditional distribution. Therefore, it suffices if the algorithm takes
O(k2 log(k/τ)/ϵ) many unlabeled samples from Dx and use Θ
 
k2 log(k/τ)

many additional label
queries. This concludes the proof of Lemma 3.6."
K,0.952112676056338,"Input: Sample access to a distribution Dx over unlabeled examples and query access to the labels
of the samples drawn; ϵ > 0; unit vector v ∈Rd.
Output:
Either reports that Dx is not N(0, I) or computes hypothesis h such that
Pr(x,y)∼D[h(x) ̸= y] = O(opt).
1. Set τ = (ϵ/(C log(1/ϵ))) for a sufficiently large constant C > 0. Set η = 1/(20000CA)
where CA is the constant from Proposition 2.1."
K,0.9549295774647887,"2. Run Algorithm 2 on D with accuracy η to obtain unit vector v(0).
3. If Algorithm 2 reports that Dx is not N(0, I), then report it and terminate."
K,0.9577464788732394,"4. For t = 0 . . . log(1/ϵ): set δ = (1/100)2−t and run Algorithm 3 with parameters v(t), δ, η to
obtain v(t+1).
5. For i = 0 . . . log(1/ϵ):"
K,0.9605633802816902,"(a) Run Algorithm 1 with v(i) and η = jϵ for j ∈[1/ϵ] on the x-marginals of D.
(b) Set h(i)(x) = sign(v(i) · x).
6. Return the hypothesis obtained from running Algorithm 4 on the list of hypothesis
{h(i)}log(1/ϵ)
i=1
on D with accuracy ϵ and failure probability τ/10."
K,0.9633802816901409,Algorithm 5: Testable Localization
K,0.9661971830985916,"Proof of Theorem 1.3. Denote by v∗, a unit vector with error at most opt, i.e., Pr(x,y)∼D[sign(v∗·
x) ̸= y] ≤opt. We start by analyzing Algorithm 5. In Line 2, Algorithm 5 uses Algorithm 2
with parameter η = 1/(20000C2
A) to get a hypothesis v(0) with small distance with v∗. From
Proposition 2.1, Algorithm 2 either reports that x-marginal of D is not N(0, I) or outputs a vector
v(0) small distance with v∗. If Algorithm 2 reports that the x-marginal of D is not N(0, I), we can
terminate the algorithm. Conditioned on the event that the algorithm did not terminate, then we have
v(0) −v∗
2 ≤CA
√opt + η.
(17)"
K,0.9690140845070423,"We consider two cases depending on how large the value of opt is. If opt > 1/(20000C2
A), then any
unit vector achieves constant error; therefore, conditioned that the algorithm did not terminate on any
proceeding test, any vector we output will satisfy the guarantees of Theorem 1.3. For the rest of the
proof, we consider the case where opt ≤1/(20000C2
A). In this case,
v(0) −v∗
2 ≤1/100, this
means that Algorithm 5 on Lines 4-4 will decrease the distance between the current hypothesis and
v∗."
K,0.971830985915493,"Conditioned on the event that the algorithm did not terminate at Lines 4-4 of Algorithm 5, we claim
that there must have some 0 ≤t∗≤log(1/ϵ) such that
v(t∗) −v∗
2 ≤O (opt + ϵ). Let t′ ∈N be"
K,0.9746478873239437,"the maximum value so that 2−t′/100 ≥40000opt, then, for all t ≤min(t′, log(1/ϵ)) it holds that
2−t/100 ≥40000opt. From Lemma 3.5, we have that for all t ≤min(t′, log(1/ϵ)) it holds that
v∗−v(t)
2 ≤2−t−1/100 ."
K,0.9774647887323944,"From the above, note that if t′ > log(1/ϵ) then
v(log(1/ϵ)) −v∗
2 ≤ϵ/100. If t′ ≤log(1/ϵ), we"
K,0.9802816901408451,"have that
v(t′) −v∗
2 ≤O(opt), which proves our claim. It remains to show that Algorithm 5 will"
K,0.9830985915492958,"return a vector v′ so that Pr(x,y)∼D[sign(v′ · x) ̸= y] = O(opt + ϵ). From Lemma 3.1, conditioned
that Algorithm 5 did not terminate on Lines 5-5b, we have that for all vectors v(0), . . . , v(log(1/ϵ))
generated on Lines 4-4 of Algorithm 5, we have that"
K,0.9859154929577465,"Pr
(x,y)∼D[sign(v(t) · x) ̸= y] ≤O
v(t) −v∗
2 
."
K,0.9887323943661972,"Hence, we can conclude that Pr(x,y)∼D[sign(v(t′) · x) ̸= y] ≤O (opt + ϵ). Then, applying
Lemma 3.6 gives that the returned halfspace will have error at most 10 · Pr(x,y)∼D[sign(v(t′) · x) ̸=
y] + ϵ = O (opt + ϵ)."
K,0.9915492957746479,"To conclude the proof,
note that each invocation of Algorithms 1,
2 and 3 con-
sumes poly(d, 1/ϵ) log(1/τ) unlabeled samples and O (d (log(d/τ) + log log(1/ϵ))) label
queries.
Moreover, the total number of invocations is upper bounded by Θ(log(1/ϵ)).
For
algorithm
Algorithm
4,
it
draws
poly(1/ϵ)
many
unlabeled
samples
and
use
Θ
 
log2(1/ϵ) (log log(1/ϵ) + log(1/τ))

label queries. Therefore the total unlabeled samples con-
sumed by Algorithm 5 is N = poly(d, 1/ϵ) and the total label quereis used is at most"
K,0.9943661971830986,"q = O
 
d log(1/ϵ) (log(d/τ) + log log(1/ϵ)) + log2(1/ϵ) (log log(1/ϵ) + log(1/τ))

."
K,0.9971830985915493,"All the procedures run in time that scales polynomially with the number of samples drawn. This
concludes the proof."
