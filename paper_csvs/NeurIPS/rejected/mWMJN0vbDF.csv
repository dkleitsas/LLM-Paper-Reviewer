Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018248175182481751,"Sign language translation (SLT) aims to translate perceived visual signals into spo-
1"
ABSTRACT,0.0036496350364963502,"ken language. Recent works have achieved impressive performance by improving
2"
ABSTRACT,0.005474452554744526,"visual representations and adopting advanced machine translation techniques, but
3"
ABSTRACT,0.0072992700729927005,"the faithfulness (i.e., whether the SLT model captures correct visual signals) in SLT
4"
ABSTRACT,0.009124087591240875,"has not received enough attention. In this paper, we explore the association among
5"
ABSTRACT,0.010948905109489052,"SLT-relevant tasks and find that the imprecise glosses and limited corpora may
6"
ABSTRACT,0.012773722627737226,"hinder faithfulness in SLT. To improve faithfulness in SLT, we first integrate SLT
7"
ABSTRACT,0.014598540145985401,"subtasks into a single framework named MonoSLT, which can share the acquired
8"
ABSTRACT,0.016423357664233577,"knowledge among SLT subtasks based on their monotonically aligned nature. We
9"
ABSTRACT,0.01824817518248175,"further propose two kinds of constraints: the alignment constraint aligns the visual
10"
ABSTRACT,0.020072992700729927,"and linguistic embeddings through a sharing translation module and synthetic
11"
ABSTRACT,0.021897810218978103,"code-switching corpora; the consistency constraint integrates the advantages of
12"
ABSTRACT,0.023722627737226276,"subtasks by regularizing the prediction consistency. Experimental results show that
13"
ABSTRACT,0.025547445255474453,"the proposed MonoSLT is competitive against previous SLT methods by increasing
14"
ABSTRACT,0.02737226277372263,"the utilization of visual signals, especially when glosses are imprecise.
15"
INTRODUCTION,0.029197080291970802,"1
Introduction
16"
INTRODUCTION,0.03102189781021898,"Sign languages, as a typical visual language, fulfill the same social and mental functions within
17"
INTRODUCTION,0.032846715328467155,"the Deaf community effectively. Sign languages convey information through a unique physical
18"
INTRODUCTION,0.03467153284671533,"transmission system and the corresponding linguistic theory [1], which makes them differ greatly
19"
INTRODUCTION,0.0364963503649635,"from spoken languages. To bridge the communication gap between the Deaf and hearing communities,
20"
INTRODUCTION,0.03832116788321168,"vision-based Sign Language Recognition (SLR) [2, 3] and Sign Language Translation (SLT) [4‚Äì6]
21"
INTRODUCTION,0.040145985401459854,"have attracted much attention over several decades. Recent works often evaluate different aspects of
22"
INTRODUCTION,0.041970802919708027,"sign language understanding models on these two tasks: the effectiveness of the feature extraction [7‚Äì
23"
INTRODUCTION,0.043795620437956206,"9] and the transferability from visual features to the target spoken language [10‚Äì12]. However, the
24"
INTRODUCTION,0.04562043795620438,"association between these two tasks has not been paid enough attention.
25"
INTRODUCTION,0.04744525547445255,"Gloss1 sequences play a critical role in both SLR and SLT. On one hand, recent SLR datasets [3, 11]
26"
INTRODUCTION,0.04927007299270073,"have limited samples and only provide sentence-wise annotations (i.e., gloss sequences) due to the
27"
INTRODUCTION,0.051094890510948905,"high cost of frame-wise annotations, and the monotonous alignment between the gloss sequence
28"
INTRODUCTION,0.05291970802919708,"and sign clips makes it possible to leverage Connectionist Temporal Classification (CTC) [13] to
29"
INTRODUCTION,0.05474452554744526,"provide supervision. On the other hand, Gloss sequences are widely used as the input of Gloss2Text
30"
INTRODUCTION,0.05656934306569343,"(G2T) task to estimate the upper bound of Sign2Text (S2T) task [6, 12]. The relationship among
31"
INTRODUCTION,0.058394160583941604,"these tasks is illustrated in Fig. 1(a). Because glosses can be used to evaluate both SLR and SLT
32"
INTRODUCTION,0.060218978102189784,"models, it is logical to assume that the SLR model with a lower error rate (more accurate prediction of
33"
INTRODUCTION,0.06204379562043796,"glosses) can provide more accurate translation results. However, as a visual language, sign language
34"
INTRODUCTION,0.06386861313868614,"conveys information through multiple visual signals and glosses are imprecise representations of sign
35"
INTRODUCTION,0.06569343065693431,"videos [10]. Many attempts [10, 14‚Äì16] have been done to improve the visual representations but
36"
INTRODUCTION,0.06751824817518248,"how to reduce effects from imprecise gloss has not attracted enough attention.
37"
INTRODUCTION,0.06934306569343066,1Gloss is the written approximation of a sign.
INTRODUCTION,0.07116788321167883,"Text:       
Darunter zum teil schneeregen."
INTRODUCTION,0.072992700729927,ÔºàPartly sleet underneath.Ôºâ
INTRODUCTION,0.07481751824817519,"Gloss:     Ê°åÂ≠ê
‰∏ã
Áå´
Âú®
Gloss:     AUCH
DABEI
SCHNEE
REGEN"
INTRODUCTION,0.07664233576642336,Sign Video
INTRODUCTION,0.07846715328467153,"(also)
(there)
(snow) 
(rain) S2T SLR G2T"
INTRODUCTION,0.08029197080291971,"(a) Illustration of SLR, S2T and G2T"
INTRODUCTION,0.08211678832116788,"Visual 
Module"
INTRODUCTION,0.08394160583941605,Contextual
INTRODUCTION,0.08576642335766424,Module
INTRODUCTION,0.08759124087591241,Translation
INTRODUCTION,0.08941605839416059,Module ùëá
INTRODUCTION,0.09124087591240876,Darunter zum teil schneeregen.
INTRODUCTION,0.09306569343065693,ÔºàPartly sleet underneath.Ôºâ
INTRODUCTION,0.0948905109489051,Gloss:  AUCH DABEI SCHNEE REGEN V V V V V2T C2T G2T
INTRODUCTION,0.09671532846715329,Predict S2T C C C C
INTRODUCTION,0.09854014598540146,(also there snow rain)
INTRODUCTION,0.10036496350364964,(b) llustration of different subtasks of SLT
INTRODUCTION,0.10218978102189781,"Figure 1: (a) An example from Phoenix14T [3]. The goal of SLR is to recognize a gloss sequence,
which is monotonically aligned with sign clips, from the sign video. S2T and G2T aim to translate
sign videos and gloss sequences into spoken language sentences, and G2T is often regarded as the
‚Äòupper bound‚Äô of S2T. (b) We decompose S2T into two subtasks based on the temporal receptive
fields of source features: Vision-to-Text (V2T) and Context-to-Text (C2T), all SLT subtasks have
monotonically aligned source features."
INTRODUCTION,0.10401459854014598,"As shown in Fig. 1(a), S2T and G2T are similar translation tasks. Different from general multilingual
38"
INTRODUCTION,0.10583941605839416,"Neural Machine Translation (NMT) [17, 18], they have monotonically aligned source languages
39"
INTRODUCTION,0.10766423357664233,"(glosses and sign clips) and the same target language. Previous works attempt to improve SLT
40"
INTRODUCTION,0.10948905109489052,"performance by adopting large-scale pretrained LMs [12] and leveraging extra corpus [11, 19]. These
41"
INTRODUCTION,0.11131386861313869,"works are developed under the paradigm that ‚Äòimproves G2T first and then transfers to S2T‚Äô, which
42"
INTRODUCTION,0.11313868613138686,"greatly improve S2T performance but inevitably face the hallucination problem [20] (i.e., S2T models
43"
INTRODUCTION,0.11496350364963503,"tend to generate fluent but inadequate translations), and we attribute this problem to the lack of
44"
INTRODUCTION,0.11678832116788321,"faithfulness [21] (i.e., the S2T models cannot capture correct visual signals). Besides, the availability
45"
INTRODUCTION,0.11861313868613138,"of G2T corpora is also the bottleneck for the generalization of the pretrained model.
46"
INTRODUCTION,0.12043795620437957,"In this paper, we attempt to increase the utilization of visual signals in S2T to improve faithfulness,
47"
INTRODUCTION,0.12226277372262774,"especially when glosses are imprecise. We first decompose S2T into two subtasks based on the
48"
INTRODUCTION,0.12408759124087591,"temporal receptive fields of source features: Vision-to-Text (V2T) and Context-to-Text (C2T). As
49"
INTRODUCTION,0.1259124087591241,"shown in Fig. 1(b), from V2T to C2T to G2T, the degree of visual abstraction of source features
50"
INTRODUCTION,0.12773722627737227,"gradually increases, while the translation quality will get better generally. We revisit recent SLT
51"
INTRODUCTION,0.12956204379562045,"approaches [22, 12] and observe that it is hard for V2T models to find the corresponding visual clips
52"
INTRODUCTION,0.13138686131386862,"during training, while this is exactly the strength of G2T models. Moreover, improving the alignment
53"
INTRODUCTION,0.1332116788321168,"between visual clips and target words can improve the faithfulness of translation and relieve the
54"
INTRODUCTION,0.13503649635036497,"hallucination problem. Different from recent works [11, 12, 19] that attempt to improve the ‚Äòupper
55"
INTRODUCTION,0.13686131386861314,"bound‚Äô (G2T) of C2T, we focus on the association among these tasks and try to improve the ‚Äòlower
56"
INTRODUCTION,0.1386861313868613,"bound‚Äô (V2T) of C2T.
57"
INTRODUCTION,0.14051094890510948,"Specifically, we first integrate the learning of SLT subtasks into a single framework named MonoSLT
58"
INTRODUCTION,0.14233576642335766,"by sharing their translated modules, which can share the acquired knowledge among SLT subtasks
59"
INTRODUCTION,0.14416058394160583,"based on their monotonically aligned nature. We further propose two kinds of constraints to enhance
60"
INTRODUCTION,0.145985401459854,"faithfulness in SLT. The alignment constraint implicitly aligns the visual and linguistic embeddings
61"
INTRODUCTION,0.1478102189781022,"through the shared translation module and synthetic code-switching corpora, which are generated
62"
INTRODUCTION,0.14963503649635038,"by replacing partial visual embeddings with their corresponding gloss embeddings. The consistency
63"
INTRODUCTION,0.15145985401459855,"constraint regularizes prediction consistency between different subtasks, which can improve both
64"
INTRODUCTION,0.15328467153284672,"training efficiency and translation quality. Experimental results show that the proposed approach can
65"
INTRODUCTION,0.1551094890510949,"surpass previous SLT methods on Phoenix14T by increasing the utilization of visual signals.
66"
INTRODUCTION,0.15693430656934307,"Our contributions can be summarized as follows:
67"
INTRODUCTION,0.15875912408759124,"‚ãÑExploring the association among different relevant tasks about SLT and integrating SLT subtasks
68"
INTRODUCTION,0.16058394160583941,"into a single framework named MonoSLT, which can share the acquired knowledge among SLT
69"
INTRODUCTION,0.1624087591240876,"subtasks based on their monotonically aligned nature.
70"
INTRODUCTION,0.16423357664233576,"‚ãÑProposing two kinds of constraints to enhance faithfulness in SLT. The alignment constraint
71"
INTRODUCTION,0.16605839416058393,"aligns the visual and linguistic embeddings through a shared translation module and synthetic
72"
INTRODUCTION,0.1678832116788321,"code-switching corpora, and the consistency constraint leverages the advantages of subtasks by
73"
INTRODUCTION,0.16970802919708028,"regularizing the prediction consistency.
74"
INTRODUCTION,0.17153284671532848,"‚ãÑShowing the lack of faithfulness in recent SLT methods and verifying the effectiveness of
75"
INTRODUCTION,0.17335766423357665,"MonoSLT for the utilization of visual signals, especially when glosses are imprecise.
76"
RELATED WORK,0.17518248175182483,"2
Related Work
77"
RELATED WORK,0.177007299270073,"Sign Language Translation. With the development of vision and language understanding algorithms,
78"
RELATED WORK,0.17883211678832117,"SLT has progressed rapidly in recent years [6, 23, 22, 11, 12, 15]. Recent SLT methods can be
79"
RELATED WORK,0.18065693430656934,"roughly categorized into two categories: vision-based and language-based.
80"
RELATED WORK,0.18248175182481752,"Vision-based SLT works devote to learning useful visual representations from videos. Considering
81"
RELATED WORK,0.1843065693430657,"the relationship with SLR, recent SLT solutions can be roughly divided into three categories: SLR-
82"
RELATED WORK,0.18613138686131386,"pretrained, SLR-supervised, and SLR-free. SLR-pretrained solutions initialize the visual extractor
83"
RELATED WORK,0.18795620437956204,"with pretrained SLR models [12, 15] or directly adopt the pretrained SLR models to extract visual
84"
RELATED WORK,0.1897810218978102,"embeddings [10, 19]. SLR-supervised solutions [22, 14, 15] adopt the multi-task framework and
85"
RELATED WORK,0.19160583941605838,"leverage the supervision from both SLR and SLT. SLR-free solutions [23‚Äì27] attempt to tokenize
86"
RELATED WORK,0.19343065693430658,"visual information without gloss supervision and leverage more real-life data. Recent empirical
87"
RELATED WORK,0.19525547445255476,"results [15, 19] indicate that adopting more accurate SLR models in SLR-pretrained and SLR-
88"
RELATED WORK,0.19708029197080293,"supervised solutions often leads to better translation quality, but little work has been done to investigate
89"
RELATED WORK,0.1989051094890511,"the association between them.
90"
RELATED WORK,0.20072992700729927,"On the other side, language-based SLT works focus on the linguistic difference between sign
91"
RELATED WORK,0.20255474452554745,"languages and spoken languages. The pioneering work [6] regards SLT as a typical NMT task
92"
RELATED WORK,0.20437956204379562,"and shows the potential of the encoder-decoder framework. Joint-SLRT [10] further adopts the
93"
RELATED WORK,0.2062043795620438,"transformer architecture [28] to integrate both SLR and SLT into a single framework. However, it
94"
RELATED WORK,0.20802919708029197,"is costly to collect large amounts of parallel corpora for SLT and recent works [11, 12, 19] reveals
95"
RELATED WORK,0.20985401459854014,"that data scarcity hinders the further development of SLT. To relieve this problem, Zhou et al. [11]
96"
RELATED WORK,0.2116788321167883,"leverage rich monolingual data and adopt back-translation to generate synthetic parallel data as a
97"
RELATED WORK,0.21350364963503649,"supplementary. Chen et al. [12] explore the potential of denoising auto-encoder that pretrained on
98"
RELATED WORK,0.21532846715328466,"large-scale multilingual corpora and progressively pretrain each task to achieve effective transfer in
99"
RELATED WORK,0.21715328467153286,"SLT. SLTUNet [19] proposes a unified model for multiple SLT-related tasks to further improve the
100"
RELATED WORK,0.21897810218978103,"translation. Our motivation is similar with [19] but we focus more on faithfulness, and leverage the
101"
RELATED WORK,0.2208029197080292,"monotonically aligned nature of SLT subtasks to align visual and linguistic embeddings.
102"
RELATED WORK,0.22262773722627738,"Faithfulness in NMT. With the rapid development of the NLP techniques [28‚Äì31], the robustness
103"
RELATED WORK,0.22445255474452555,"and interpretability of NMT systems become a crucial issue. A good NMT model should produce
104"
RELATED WORK,0.22627737226277372,"translations that capture the intended meaning of the source language (faithfulness) while maintaining
105"
RELATED WORK,0.2281021897810219,"grammatical correctness and naturalness in the target language (fluency) [32, 33]. However, NMT
106"
RELATED WORK,0.22992700729927007,"models may generate hallucinations due to exposure bias [34], domain shift [35], lack of coverage [36],
107"
RELATED WORK,0.23175182481751824,"and other factors [20]. To enhance the faithfulness in NMT, Tu et al. [36] maintain a coverage
108"
RELATED WORK,0.23357664233576642,"vector to encourage NMT models to consider more source words, Wang and Sennrich [35] leverage
109"
RELATED WORK,0.2354014598540146,"minimum risk training to mitigate domain shift, and Feng [33] propose a faithfulness part to enhance
110"
RELATED WORK,0.23722627737226276,"the contextual representation of encoder output. Different from general NMT tasks, SLT models
111"
RELATED WORK,0.23905109489051096,"need to encoder source information from unsegmented video, which makes it harder to learn the
112"
RELATED WORK,0.24087591240875914,"correspondences between video and language and generate faithful translations. We focus on the
113"
RELATED WORK,0.2427007299270073,"relationship between visual and language translation tasks rather than diving into specific translation
114"
RELATED WORK,0.24452554744525548,"module designs, which makes the proposed method compatible with other NMT techniques.
115"
APPROACH,0.24635036496350365,"3
Approach
116"
APPROACH,0.24817518248175183,"In this section, we first introduce notation and background knowledge briefly. Then we explore
117"
APPROACH,0.25,"the association among SLT-relevant tasks and present some empirical findings about the lack of
118"
APPROACH,0.2518248175182482,"faithfulness in SLT. After that, we propose a method to improve faithfulness in SLT.
119"
BACKGROUND,0.25364963503649635,"3.1
Background
120"
BACKGROUND,0.25547445255474455,"Formally, given a sign sequence X = {x1, ¬∑ ¬∑ ¬∑ , xT } with T frames, SLR aims to recognize its
121"
BACKGROUND,0.2572992700729927,"corresponding gloss sequence G = {g1, ¬∑ ¬∑ ¬∑ , gN} with N glosses (N ‚â§T in general), which are
122"
BACKGROUND,0.2591240875912409,"monotonically aligned with sign clips S = {xŒ∑1, ¬∑ ¬∑ ¬∑ , xŒ∑N } and Œ∑i is the corresponding frame
123"
BACKGROUND,0.26094890510948904,"indexes of gloss i. The SLR model is generally optimized by CTC, which leverages all possible
124"
BACKGROUND,0.26277372262773724,"alignments between X and G and can be written as LCT C = ‚àílog p(G|X). Different from SLR,
125"
BACKGROUND,0.2645985401459854,"the objective of SLT is to translate X into spoken language sentence W = {w1, ¬∑ ¬∑ ¬∑ , wM} with M
126"
BACKGROUND,0.2664233576642336,"words (M Ã∏= N in general), which often has different grammar and vocabulary, and the SLT model is
127"
BACKGROUND,0.26824817518248173,"optimized by minimizing the negative log-likelihood LSLT = PM
t=1 ‚àílog p(wt|X, w<t).
128 ùëá"
BACKGROUND,0.27007299270072993,Translation
BACKGROUND,0.2718978102189781,Module
BACKGROUND,0.2737226277372263,V V V V
BACKGROUND,0.2755474452554745,Gloss: AUCH DABEI
BACKGROUND,0.2773722627737226,SCHNEE REGEN
BACKGROUND,0.2791970802919708,(also there snow rain) MLP ùí´
BACKGROUND,0.28102189781021897,Pretrained Word
BACKGROUND,0.28284671532846717,Embedding ‚Ñ∞
BACKGROUND,0.2846715328467153,"G G G G
G V G V
Translation"
BACKGROUND,0.2864963503649635,Module
BACKGROUND,0.28832116788321166,GCN Module
BACKGROUND,0.29014598540145986,"Conv1D
BiLSTM
C
C
C
C S"
BACKGROUND,0.291970802919708,shared
BACKGROUND,0.2937956204379562,Switch Code ùêøùëÜùêøùëá ùê∂ùëÜ ùêøùëÜùêøùëá ùê∂
BACKGROUND,0.2956204379562044,ùêøùëêùëúùëõùë†ùëñùë†ùë°
BACKGROUND,0.29744525547445255,"ùêøùëÜùêøùëÖ
Shared Classifier
Baseline (C2T)"
BACKGROUND,0.29927007299270075,"Figure 2: Overview of the proposed method. For baseline (C2T), the visual module is composed of a
lightweight GCN-based module and a Conv1D module, and the contextual module is implemented as
a two-layer BiLSTM. The proposed method has an auxiliary branch that takes the switched gloss and
visual embeddings as input, and both branches share the same translation modules. An additional
consistency loss is adopted to regularize the prediction consistency."
BACKGROUND,0.3010948905109489,"(a)
(b)
(c)"
BACKGROUND,0.3029197080291971,"Figure 3: BLEU-4 scores of different subtasks over epoch on Phoenix14T (a) dev and (b) training
sets. (c) Fluctuation of SLT performance over SLR performance (the upper), and the corresponding
number of samples for each SLR performance interval (the lower) on Phoenix14T dev set."
BACKGROUND,0.30474452554744524,"Recent SLT architectures [14, 9, 12] commonly contain three components: a visual module, a
129"
BACKGROUND,0.30656934306569344,"contextual module, and a translation module. The basic architecture used in this paper is visualized
130"
BACKGROUND,0.3083941605839416,"in Fig. 2. Considering the training efficiency, we use the coordinates of keypoint sequences as inputs.
131"
BACKGROUND,0.3102189781021898,"As for the visual module, we adopt a lightweight GCN-based module and a two-layer temporal
132"
BACKGROUND,0.31204379562043794,"convolution block (Conv1D). The outputs of the visual module V = {v1, ¬∑ ¬∑ ¬∑ , vT } are fed into a
133"
BACKGROUND,0.31386861313868614,"two-layer BiLSTM to obtain contextual features C = {c1, ¬∑ ¬∑ ¬∑ , cT }. As mentioned in Fig. 1(b), all of
134"
BACKGROUND,0.3156934306569343,"V, C, and G can be used as the source language for SLT, which are corresponding to V2T, C2T, and
135"
BACKGROUND,0.3175182481751825,"G2T subtasks, respectively. Similar to VAC [9], we adopt two classifiers on both V and C to provide
136"
BACKGROUND,0.3193430656934307,"supervision for SLR, and the basic supervision can be formulated as:
137"
BACKGROUND,0.32116788321167883,"Lbasic = LV
CT C + LC
CT C + ŒªCLC
SLT ,
(1)"
BACKGROUND,0.32299270072992703,"where the superscript indicates the input features of the loss function and ŒªC is the translation weight.
138"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3248175182481752,"3.2
Exploring the Association among SLT-relevant Tasks
139"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3266423357664234,"As shown in Fig. 2, the adopted baseline can learn SLR and SLT jointly, and the features of SLR are
140"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3284671532846715,"further utilized by the translation module, which provides a sufficient basis to explore the relationship
141"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3302919708029197,"between different SLT-relevant subtasks. We first train three individual models for V2T, C2T, and
142"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.33211678832116787,"G2T, respectively, and visualize the evaluation results during the training on Phoenix14T [3] in
143"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.33394160583941607,"Fig. 3(a) and 3(b). We can observe different convergence behaviors on SLT subtasks: the G2T model
144"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3357664233576642,"converges faster at the beginning and achieves higher performance on the dev set, the S2T model
145"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3375912408759124,"achieves comparable performance on the dev set but tends to overfit the training set, while the V2T
146"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.33941605839416056,"model encounters difficulties in converging. This observation indicates that the C2T model meets the
147"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.34124087591240876,"issue of overfitting before finding the correct visual signals, especially when adopting a powerful
148"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.34306569343065696,"translation module, and we identify this issue as the lack of faithfulness.
149"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3448905109489051,"Moreover, we divide the dev set into several subsets based on SLR performance, and visualize the
150"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3467153284671533,"relationship between SLT and SLR performance of the C2T model in Fig. 3(c). It is surprising
151"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.34854014598540145,"to observe that there is no significant negative correlation (i.e., achieving higher BLEU scores
152"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.35036496350364965,"on the subset with lower WER) between the performance of SLR and SLT, even though lower
153"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3521897810218978,"WER indicates the less accumulated error. We analyze results and find that C2T models tend to
154"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.354014598540146,"generate hallucinations [20], which are fluent but unrelated to source gloss sequences. This is another
155"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.35583941605839414,"phenomenon that reflects a lack of faithfulness.
156"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.35766423357664234,"Based on the above observations, we conclude that enhancing the capability of SLT models to
157"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3594890510948905,"accurately identify visual signals is crucial, which can improve the faithfulness of SLT models.
158"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.3613138686131387,"Besides, we assume imprecise gloss representations may hinder the further development of SLT
159"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.36313868613138683,"models, and it is essential to increase the utilization of visual information. Different from recent
160"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.36496350364963503,"works [12, 19] that explore the use of linguistic information to guide the learning of visual features,
161"
EXPLORING THE ASSOCIATION AMONG SLT-RELEVANT TASKS,0.36678832116788324,"we prefer to take advantage of both modalities based on their different characteristics.
162"
IMPROVING FAITHFULNESS IN SLT,0.3686131386861314,"3.3
Improving Faithfulness in SLT
163"
IMPROVING FAITHFULNESS IN SLT,0.3704379562043796,"Previous works have shown remarkable success in modeling multi-lingual languages [37, 38] and
164"
IMPROVING FAITHFULNESS IN SLT,0.3722627737226277,"cross-modal information [39] with a single transformer-based model [28], which verifies the capability
165"
IMPROVING FAITHFULNESS IN SLT,0.3740875912408759,"of transformer-based models for aligning multi-source domains. Different from exploiting large-scale
166"
IMPROVING FAITHFULNESS IN SLT,0.3759124087591241,"parallel corpora in a self-supervised way, we focus on how to make full use of existing supervised
167"
IMPROVING FAITHFULNESS IN SLT,0.3777372262773723,"data in the low-resource setting. The proposed method includes a joint training scheme and two
168"
IMPROVING FAITHFULNESS IN SLT,0.3795620437956204,"constraints.
169"
IMPROVING FAITHFULNESS IN SLT,0.3813868613138686,"Joint Learning of SLT Subtasks. All of the SLT subtasks (V2T, C2T, and G2T) are monotonically
170"
IMPROVING FAITHFULNESS IN SLT,0.38321167883211676,"aligned and this characteristic of SLT indicates the acquired knowledge about translation can be
171"
IMPROVING FAITHFULNESS IN SLT,0.38503649635036497,"shared across subtasks, which can not only control model complexity but also reduce overfitting.
172"
IMPROVING FAITHFULNESS IN SLT,0.38686131386861317,"Therefore, we first integrate SLT subtasks into a single framework and learn them jointly. We
173"
IMPROVING FAITHFULNESS IN SLT,0.3886861313868613,"adopt the pretrained word embedding of mBart [38] as previous work [15] to obtain the linguistic
174"
IMPROVING FAITHFULNESS IN SLT,0.3905109489051095,"embedding sequence E(G) from a given gloss sequence G. To bridge the gap between visual and
175"
IMPROVING FAITHFULNESS IN SLT,0.39233576642335766,"linguistic modalities, we use a two-layer MLP P to obtain the visual embedding sequences P(V) and
176"
IMPROVING FAITHFULNESS IN SLT,0.39416058394160586,"P(C). Besides, we share P and SLR classifiers for V and C to ensure the alignment between different
177"
IMPROVING FAITHFULNESS IN SLT,0.395985401459854,"kinds of visual features [40]. All of E(G), P(V), and P(C) are sent to the same translation module,
178"
IMPROVING FAITHFULNESS IN SLT,0.3978102189781022,"and auxiliary translation losses are applied to the outputs of P(V) and E(G) for joint learning:
179"
IMPROVING FAITHFULNESS IN SLT,0.39963503649635035,"Ljoint = Lbasic + ŒªGLG
SLT + ŒªVLV
SLT ,
(2)"
IMPROVING FAITHFULNESS IN SLT,0.40145985401459855,"where ŒªG and ŒªV are hyperparameters to control the balance among subtasks.
180"
IMPROVING FAITHFULNESS IN SLT,0.4032846715328467,"Alignment Constraint. The joint learning scheme shares the translation module among subtasks, but
181"
IMPROVING FAITHFULNESS IN SLT,0.4051094890510949,"it is hard to identify the relationships between multiple subtasks and we try to further simplify this
182"
IMPROVING FAITHFULNESS IN SLT,0.40693430656934304,"scheme. Code-switching [41] is a phenomenon that the alternation of languages within a conversation
183"
IMPROVING FAITHFULNESS IN SLT,0.40875912408759124,"or utterance, which occurs when speakers are multilingual and familiar with correspondences among
184"
IMPROVING FAITHFULNESS IN SLT,0.41058394160583944,"languages. As mentioned in Fig. 1(b), the source features of SLT subtasks are monotonically aligned,
185"
IMPROVING FAITHFULNESS IN SLT,0.4124087591240876,"which provides a sufficient basis to let the SLT learner train with a multilingual learner jointly and
186"
IMPROVING FAITHFULNESS IN SLT,0.4142335766423358,"make the SLT learner aware of word alignment implicitly. As shown in Fig. 2, we only keep two
187"
IMPROVING FAITHFULNESS IN SLT,0.41605839416058393,"branches for SLT: the primary branch is training for the C2T subtask, and the auxiliary branch needs
188"
IMPROVING FAITHFULNESS IN SLT,0.41788321167883213,"to tackle code-switching translation.
189"
IMPROVING FAITHFULNESS IN SLT,0.4197080291970803,"To generate a synthetic code-switching corpus for the auxiliary branch, we first estimate the alignment
190"
IMPROVING FAITHFULNESS IN SLT,0.4215328467153285,"path ÀÜœÄ = arg maxœÄ p(œÄ|X, G) with the maximal probability [42] from the recognition prediction of
191"
IMPROVING FAITHFULNESS IN SLT,0.4233576642335766,"the primary branch, and then obtain the corresponding frames indexes Œ∑i from ÀÜœÄ for each gloss i. The
192"
IMPROVING FAITHFULNESS IN SLT,0.4251824817518248,"code-switched sentence embedding CS(V, G) is generated by replacing visual embeddings of each
193"
IMPROVING FAITHFULNESS IN SLT,0.42700729927007297,"gloss in P(V) with the corresponding gloss embeddings E(G) (e.g., replacing P(V)Œ∑i with E(Gi) for
194"
IMPROVING FAITHFULNESS IN SLT,0.42883211678832117,"gloss i) with a probability of Œ≤:
195"
IMPROVING FAITHFULNESS IN SLT,0.4306569343065693,"CS(V, G) = diag (1 ‚àím(Œ≤)) P(V) + diag(m(Œ≤))E(G),
(3)"
IMPROVING FAITHFULNESS IN SLT,0.4324817518248175,"where m(Œ≤) is the mask vector for replacing and diag(¬∑) convert a vector to the corresponding
196"
IMPROVING FAITHFULNESS IN SLT,0.4343065693430657,"diagonal matirx. In addition to the above gloss-wise code-switching, we also propose a sentence-wise
197"
IMPROVING FAITHFULNESS IN SLT,0.43613138686131386,"generation process, which simply mixes embedding sequences as Mixup [43]:
198"
IMPROVING FAITHFULNESS IN SLT,0.43795620437956206,"CS(V, G) = (1 ‚àíŒ≤)P(V) + Œ≤E(G).
(4)"
IMPROVING FAITHFULNESS IN SLT,0.4397810218978102,"It is worth noting that Œ≤ controls the ratio of gloss embeddings in the code-switched sentence, we
199"
IMPROVING FAITHFULNESS IN SLT,0.4416058394160584,"adopt a larger Œ≤ at the beginning to leverage the fast convergence of the gloss embedding and then
200"
IMPROVING FAITHFULNESS IN SLT,0.44343065693430656,"gradually decay. To balance all subtasks and prevent overfitting, we further adopt a cyclical annealing
201"
IMPROVING FAITHFULNESS IN SLT,0.44525547445255476,"schedule [44], which gradually reduces Œ≤ within each cycle:
202"
IMPROVING FAITHFULNESS IN SLT,0.4470802919708029,"Œ≤ = max(0, 1 ‚àí2 ‚àómod(t ‚àí1, M)/M),
(5)"
IMPROVING FAITHFULNESS IN SLT,0.4489051094890511,"where t is the epoch number, and M is the number of epochs for each cycle. We adopt a hyperparam-
203"
IMPROVING FAITHFULNESS IN SLT,0.45072992700729925,"eter ŒªCS to weight the auxiliary translation loss and formulate the total process as:
204"
IMPROVING FAITHFULNESS IN SLT,0.45255474452554745,"Lalign = Lbasic + ŒªCSLCS
SLT .
(6)"
IMPROVING FAITHFULNESS IN SLT,0.4543795620437956,"Consistency Constraint. The alignment constraint implicitly aligns visual and linguistic embeddings
205"
IMPROVING FAITHFULNESS IN SLT,0.4562043795620438,"by sharing the translation module and leveraging synthetic code-switching corpora. However, there is
206"
IMPROVING FAITHFULNESS IN SLT,0.458029197080292,"a certain degree of complementarity between different kinds of subtasks: G2T takes discrete gloss
207"
IMPROVING FAITHFULNESS IN SLT,0.45985401459854014,"embedding as input, which can easily capture correspondences between source and target languages
208"
IMPROVING FAITHFULNESS IN SLT,0.46167883211678834,"but may lose detailed visual information, while V2T and C2T take continuous embedding as input,
209"
IMPROVING FAITHFULNESS IN SLT,0.4635036496350365,"which contains more useful information about the sign but struggles to converge. To better leverage
210"
IMPROVING FAITHFULNESS IN SLT,0.4653284671532847,"the characteristics of different subtasks and balance the training processes, we further propose a
211"
IMPROVING FAITHFULNESS IN SLT,0.46715328467153283,"consistency constraint to regularize the SLT predictions between two branches:
212"
IMPROVING FAITHFULNESS IN SLT,0.46897810218978103,"Lconsist = DKL(pC||pCS) + DKL(pCS||pC)
(7)"
IMPROVING FAITHFULNESS IN SLT,0.4708029197080292,"where pC and pCS are the predicted distribution over words based on features C and CS, respectively,
213"
IMPROVING FAITHFULNESS IN SLT,0.4726277372262774,"and DKL(¬∑, ¬∑) denotes Kullback-Leibler divergence. When applying both constraints, the consistency
214"
IMPROVING FAITHFULNESS IN SLT,0.4744525547445255,"constraint encourages the C2T model to find correct correspondences at the beginning of each cycle
215"
IMPROVING FAITHFULNESS IN SLT,0.4762773722627737,"and gradually improves the importance of visual information as Œ≤ decays. The consistency constraint
216"
IMPROVING FAITHFULNESS IN SLT,0.4781021897810219,"can also be explained from mutual learning [45] and learning from noisy labels [46].
217"
IMPROVING FAITHFULNESS IN SLT,0.47992700729927007,"Since the proposed method is based on the Monotonically aligned nature of SLT subtasks, we named
218"
IMPROVING FAITHFULNESS IN SLT,0.48175182481751827,"it MonoSLT and its final objective function is:
219"
IMPROVING FAITHFULNESS IN SLT,0.4835766423357664,"Lfinal = Lalign + ŒªcLconsist,
(8)"
IMPROVING FAITHFULNESS IN SLT,0.4854014598540146,"where Œªc is the hyperparameter to balance constraints.
220"
EXPERIMENTS,0.48722627737226276,"4
Experiments
221"
DATASETS AND EVALUTION METRICS,0.48905109489051096,"4.1
Datasets and Evalution Metrics
222"
DATASETS AND EVALUTION METRICS,0.4908759124087591,"Datasets. We evaluate MonoSLT on RWTHPHOENIX-Weather 2014T (Phoenix14T) and CSL-Daily
223"
DATASETS AND EVALUTION METRICS,0.4927007299270073,"datasets, and both datasets provide gloss and translation annotations.
224"
DATASETS AND EVALUTION METRICS,0.49452554744525545,"‚ãÑPhoenix14T [6] is an extension of the previous SLR dataset [3] by redefining segmentation
225"
DATASETS AND EVALUTION METRICS,0.49635036496350365,"boundaries and providing parallel gloss annotation and German translation. It is collected from
226"
DATASETS AND EVALUTION METRICS,0.4981751824817518,"weather forecast broadcasts and manually annotated, which indicates the gloss annotations may
227"
DATASETS AND EVALUTION METRICS,0.5,"be imprecise. It has 8,257 sentences signed by 9 signers with vocabularies of around 1k glosses
228"
DATASETS AND EVALUTION METRICS,0.5018248175182481,"and 3k German words. There are 7096, 519, and 642 samples in training, dev, and test sets.
229"
DATASETS AND EVALUTION METRICS,0.5036496350364964,"‚ãÑCSL-Daily [11] is a Chinese sign language dataset with vocabularies of around 2k glosses and
230"
DATASETS AND EVALUTION METRICS,0.5054744525547445,"2.3k Chinese characters. Different from Phoenix14T, CSL-Daily is collected by first designing
231"
DATASETS AND EVALUTION METRICS,0.5072992700729927,"the sign language corpus based on Chinese Sign Language textbooks and some Chinese corpora,
232"
DATASETS AND EVALUTION METRICS,0.5091240875912408,"and then inviting 10 signers to sign reference texts, which indicates the gloss annotations are quite
233"
DATASETS AND EVALUTION METRICS,0.5109489051094891,"precise. There are 18401, 1077, and 1176 samples in training, dev, and test sets.
234"
DATASETS AND EVALUTION METRICS,0.5127737226277372,"Evalution Metrics. Similar to machine translation, BLEU [47] and ROUGH [48] scores (higher is
235"
DATASETS AND EVALUTION METRICS,0.5145985401459854,"better) are used to measure translation performance. We also report word error rate (WER, lower is
236"
DATASETS AND EVALUTION METRICS,0.5164233576642335,"better) to reflect the performance of SLR modules as previous works [3, 9, 15] do.
237"
IMPLEMENTATION DETAILS,0.5182481751824818,"4.2
Implementation Details
238"
IMPLEMENTATION DETAILS,0.5200729927007299,"For efficiency, we utilize MMPose [49] to estimate keypoint sequences from sign videos, and it
239"
IMPLEMENTATION DETAILS,0.5218978102189781,"generates 133 2D keypoints for each frame. We select 77 keypoints and divided them into five groups:
240"
IMPLEMENTATION DETAILS,0.5237226277372263,"9 for body, 21 for each hand, 8 for mouth, and 18 for face. Group-wise modified ST-GCN [50] blocks
241"
IMPLEMENTATION DETAILS,0.5255474452554745,"are adopted to extract features from each group, and extracted features are projected to a vector of
242"
IMPLEMENTATION DETAILS,0.5273722627737226,"1024 dimensions for each frame. For Conv1D, we adopt a ‚ÄòC3-P2-C3-P2‚Äô structure, where C and P
243"
IMPLEMENTATION DETAILS,0.5291970802919708,"Table 1: Performance comparison (%) on Phoenix14T dataset. The highest performance is highlighted
in bold, while the second is underlined. ‚Ä° denotes methods without using gloss annotations. ‚Ä† denotes
methods only taking skeleton sequences as input. (R and B denote ROUGE and BLEU.)"
IMPLEMENTATION DETAILS,0.531021897810219,"Sign2Text
Dev
Test
R
B4
WER
R
B1
B2
B3
B4
WER
SL-Luong‚Ä° [6]
31.80
9.94
-
31.80
32.24
19.03
12.83
8.58
-
TSPNet‚Ä° [23]
-
-
-
34.96
36.10
23.12
16.88
13.41
-
JointSLRT [22]
-
22.38
24.98
-
46.61
33.73
26.19
21.32
26.16
STMC-T [14]
48.24
24.09
21.1
46.65
46.98
36.09
28.70
23.65
20.7
SignBT [11]
50.29
24.45
22.7
49.54
50.80
37.75
29.72
24.32
23.9
MMTLB [12]
53.10
27.61
21.90
52.65
53.97
41.75
33.84
28.39
22.45
SLTUNet [19]
52.23
27.87
19.24
52.11
52.92
41.76
33.99
28.47
-
TwoStream-SLT-K‚Ä† [15]
53.21
27.83
27.14
52.87
53.58
41.78
33.60
27.98
27.19
TwoStream-SLT [15]
54.08
28.66
17.72
53.48
54.90
42.43
34.46
28.95
19.32
Baseline‚Ä†
53.22
27.55
21.5
52.56
53.69
40.96
32.84
27.37
21.1
MonoSLT‚Ä†
55.41
29.96
21.2
55.73
57.05
44.70
36.73
31.15
21.4"
IMPLEMENTATION DETAILS,0.5328467153284672,"Table 2: Performance comparison4 on CSL-Daily dataset. The highest performance is highlighted in
bold, while the second is underlined. * denotes methods with the inconsistent punctuation bug. The
results of [6, 22] are reproduced by SignBT [11]. (R and B denote ROUGE and BLEU.)"
IMPLEMENTATION DETAILS,0.5346715328467153,"Sign2Text
Dev
Test
R
B4
WER
R
B1
B2
B3
B4
WER
MMTLB* [12]
53.38
24.42
-
53.25
53.31
40.41
30.87
23.92
-
TwoStream-SLT* [15]
55.1
25.76
25.4
55.72
55.44
42.59
32.87
25.79
25.3
Baseline*
50.85
22.83
29.1
50.96
52.11
38.97
29.46
22.74
28.2
MonoSLT*
52.58
23.67
29.1
52.58
52.65
39.72
30.27
23.53
28.2
SL-Luong [6]
34.28
7.96
-
34.54
34.16
19.47
11.84
7.56
-
Joint-SLRT [22]
27.06
11.88
-
36.74
37.38
24.36
16.55
11.79
-
Sign-BT [11]
49.49
20.8
33.2
49.31
51.42
37.26
27.76
21.34
32.2
SLTUNet [19]
53.58
23.99
-
54.08
54.98
41.44
31.84
25.01
-
Baseline
53.47
25.90
29.1
53.71
55.30
41.91
32.56
25.91
28.2
MonoSLT
55.28
26.91
29.1
55.35
55.87
42.75
33.52
26.83
28.2"
IMPLEMENTATION DETAILS,0.5364963503649635,"denote 1D-CNN and max-pooling layer, respectively. Following [12], we utilize the official release
244"
IMPLEMENTATION DETAILS,0.5383211678832117,"of mBART-large-cc25 2, which is pretrained on CC25 3, as the initialization of the translation module.
245"
IMPLEMENTATION DETAILS,0.5401459854014599,"The default setting for hyperparameters: ŒªC, ŒªG, ŒªV are set to 1.0 and Œªc is set to 0.1 for simplicity.
246"
IMPLEMENTATION DETAILS,0.541970802919708,"The beam width for the CTC decoder and the SLT decoder are 10 and 4, respectively. We train
247"
IMPLEMENTATION DETAILS,0.5437956204379562,"each model for 80 epochs with the cosine annealing schedule and an Adam optimizer, and the initial
248"
IMPLEMENTATION DETAILS,0.5456204379562044,"learning rate for each module: 1e-3 for the MLP, 1e-5 for the translation module, and 3e-3 for others.
249"
IMPLEMENTATION DETAILS,0.5474452554744526,"Each experiment is conducted on a single NVIDIA GeForce RTX 3090 GPU. Other details can be
250"
IMPLEMENTATION DETAILS,0.5492700729927007,"found in the supplementary.
251"
COMPARISON WITH STATE-OF-THE-ART,0.551094890510949,"4.3
Comparison with State-of-the-art
252"
COMPARISON WITH STATE-OF-THE-ART,0.5529197080291971,"Quantitative Comparison. We report the performance of our MonoSLT model and relevant methods
253"
COMPARISON WITH STATE-OF-THE-ART,0.5547445255474452,"on Phoenix14T in Table 1. Because this paper mainly focuses on improving faithfulness, we put
254"
COMPARISON WITH STATE-OF-THE-ART,0.5565693430656934,"results of the Sign2Gloss2Text task in the supplementary. As shown in Table 1, we adopt a strong
255"
COMPARISON WITH STATE-OF-THE-ART,0.5583941605839416,"baseline, and the proposed method can bring further improvement (+3.79 BLEU-4). Besides, the
256"
COMPARISON WITH STATE-OF-THE-ART,0.5602189781021898,"proposed MonoSLT is not the best SLR approach, but outperforms the previous SLT method [15]
257"
COMPARISON WITH STATE-OF-THE-ART,0.5620437956204379,"with the best SLR performance by 2.2% (WER: 21.4% vs.19.3%, BLEU-4: 31.15% vs.28.95%).
258"
COMPARISON WITH STATE-OF-THE-ART,0.5638686131386861,"MonoSLT also surpasses other previous methods with similar SLR performance, which indicates
259"
COMPARISON WITH STATE-OF-THE-ART,0.5656934306569343,"MonoSLT can increase the utilization of visual signals. This observation also reveals the lack of
260"
COMPARISON WITH STATE-OF-THE-ART,0.5675182481751825,"faithfulness in recent SLT methods, e.g., TwoStream-SLT [15] with multi-modality inputs (both
261"
COMPARISON WITH STATE-OF-THE-ART,0.5693430656934306,"skeleton sequence and video) achieve much better SLR performance than with skeleton sequence
262"
COMPARISON WITH STATE-OF-THE-ART,0.5711678832116789,"only (WER: 27.14% vs.17.72%), but it achieves comparable SLT performance (BLEU-4: 28.23%
263"
COMPARISON WITH STATE-OF-THE-ART,0.572992700729927,"2https://huggingface.co/facebook/mbart-large-cc25
3https://commoncrawl.org/
4Our translation module is based on MMTLB (https://github.com/FangyunWei/SLRT), and we find it has an
inconsistent punctuation bug during tokenization. For a fair comparison, we report results under both settings."
COMPARISON WITH STATE-OF-THE-ART,0.5748175182481752,"Table 3: A translation example of the lack of visual faithfulness on Phoenix14T dev set. We highlight
the hallucination in red, and its corresponding correct translation and gloss in blue."
COMPARISON WITH STATE-OF-THE-ART,0.5766423357664233,"SLR Ref: morgen / sonne / ueberall / kueste / region / wolke / moeglich / regen neg-viel
( tomorrow / sun / everywhere / coast / region / cloud / possible / rain )
SLR Hyp: morgen / sonne / himmel (sky) / kueste / region / wolke / moeglich / regen neg-viel"
COMPARISON WITH STATE-OF-THE-ART,0.5784671532846716,"SLT Ref: am mittwoch im s√ºden und an den k√ºsten etwas regen sonst ist es meist freundlich .
(on wednesday in the south deland on the coasts some rain otherwise it is mostly friendly .)"
COMPARISON WITH STATE-OF-THE-ART,0.5802919708029197,"Baseline Hyp: am mittwoch im s√ºden und nordosten hier und da regen sonst zum teil freundlich .
(on wednesday in the south and northeast here and there rain otherwise partly friendly .)"
COMPARISON WITH STATE-OF-THE-ART,0.5821167883211679,"MonoSLT Hyp: am mittwoch im s√ºden und an den k√ºsten etwas regen sonst ist es recht freundlich .
(on wednesday in the south and on the coasts some rain otherwise it is quite friendly .)"
COMPARISON WITH STATE-OF-THE-ART,0.583941605839416,"Table 4: Ablation results (BLEU-4, %) of joint learning of SLT subtasks on Phoenix14T."
COMPARISON WITH STATE-OF-THE-ART,0.5857664233576643,"Loss Weights
V2T
C2T
G2T
ŒªV
ŒªC
ŒªG
Dev
Test
Dev
Test
Dev
Test
1.0
-
-
22.58
22.59
-
-
-
-
-
1.0
-
-
-
28.00
28.53
-
-
-
-
1.0
-
-
-
-
28.05
26.36
1.0
1.0
-
28.30
28.18
28.87
29.73
-
-
-
1.0
1.0
-
-
28.82
27.67
28.19
27.38
1.0
1.0
1.0
27.11
27.85
28.03
27.66
27.42
26.98"
COMPARISON WITH STATE-OF-THE-ART,0.5875912408759124,"vs.28.95%) under these two settings. Besides, the proposed method improves faithfulness through
264"
COMPARISON WITH STATE-OF-THE-ART,0.5894160583941606,"joint learning and two constraints, which can be applied to any SLR model and has the potential to
265"
COMPARISON WITH STATE-OF-THE-ART,0.5912408759124088,"achieve better translation performance with a more powerful SLR model.
266"
COMPARISON WITH STATE-OF-THE-ART,0.593065693430657,"To show the generalization of the proposed method, we also report relevant performance on CSL-Daily
267"
COMPARISON WITH STATE-OF-THE-ART,0.5948905109489051,"in Table 2. As mentioned in Sect. 4.1, the CSL-Daily dataset has more precise gloss annotations,
268"
COMPARISON WITH STATE-OF-THE-ART,0.5967153284671532,"which indicates models with lower SLR performance can often achieve better SLT results. The
269"
COMPARISON WITH STATE-OF-THE-ART,0.5985401459854015,"proposed MonoSLT achieves inferior SLR and SLT performance than previous works [12, 15] but is
270"
COMPARISON WITH STATE-OF-THE-ART,0.6003649635036497,"still better than other works. Besides, the proposed method achieves better SLT performance than
271"
COMPARISON WITH STATE-OF-THE-ART,0.6021897810218978,"the baseline, which indicates that although glosses are precise intermediate tokenization, the lack of
272"
COMPARISON WITH STATE-OF-THE-ART,0.6040145985401459,"faithfulness still exists.
273"
COMPARISON WITH STATE-OF-THE-ART,0.6058394160583942,"Qualitative Comparison. To provide a more intuitive understanding of the proposed method, we
274"
COMPARISON WITH STATE-OF-THE-ART,0.6076642335766423,"present a translation example in Table 3. It can be observed that part of the translation cannot find
275"
COMPARISON WITH STATE-OF-THE-ART,0.6094890510948905,"corresponding glosses, which indicates glosses are imprecise representations. Besides, the baseline
276"
COMPARISON WITH STATE-OF-THE-ART,0.6113138686131386,"generates hallucination ‚Äòand northeast here and there‚Äô, while the corresponding gloss ‚Äòkueste‚Äô is
277"
COMPARISON WITH STATE-OF-THE-ART,0.6131386861313869,"correctly recognized but ignored by the translation module. The proposed MonoSLT can improve
278"
COMPARISON WITH STATE-OF-THE-ART,0.614963503649635,"faithfulness and translate ‚Äòon the coasts‚Äô correctly. More results can be found in the supplementary.
279"
ABLATION AND DISCUSSION,0.6167883211678832,"4.4
Ablation and Discussion
280"
ABLATION AND DISCUSSION,0.6186131386861314,"Ablation on Joint Learning. As we mentioned in Sect. 3.3, the acquired knowledge about translation
281"
ABLATION AND DISCUSSION,0.6204379562043796,"can be shared across SLT subtasks. We first evaluate different combinations of subtasks and present
282"
ABLATION AND DISCUSSION,0.6222627737226277,"results in Table. 4. We notice that learning V2T and C2T subtasks jointly obtain the most significant
283"
ABLATION AND DISCUSSION,0.6240875912408759,"improvements (5.72% for V2T and 0.87% for C2T), which shows V2T can achieve comparable
284"
ABLATION AND DISCUSSION,0.6259124087591241,"results to G2T with proper regularization and the performance of V2T and C2T can be mutually
285"
ABLATION AND DISCUSSION,0.6277372262773723,"improved. This observation also reveals a clear difference between the SLTUNet [19] and the
286"
ABLATION AND DISCUSSION,0.6295620437956204,"proposed method: we pay more attention to the association between visual translation subtasks.
287"
ABLATION AND DISCUSSION,0.6313868613138686,"Moreover, simply sharing more subtasks can not bring further improvements, which indicates the
288"
ABLATION AND DISCUSSION,0.6332116788321168,"importance of designing proper solutions to exploit SLT subtasks.
289"
ABLATION AND DISCUSSION,0.635036496350365,"Ablation on Design Choices of MonoSLT. To investigate the effectiveness of the proposed method,
290"
ABLATION AND DISCUSSION,0.6368613138686131,"we present the ablation results of each design in Table 5. Both token-wise and sentence-wise code-
291"
ABLATION AND DISCUSSION,0.6386861313868614,"switching achieve better performance than the best performance of joint learning, and we notice they
292"
ABLATION AND DISCUSSION,0.6405109489051095,"can also accelerate training process and increase training stability. Adopting the cyclical annealing
293"
ABLATION AND DISCUSSION,0.6423357664233577,"schedule can improve the G2T performance at the cost of a little performance loss of V2T and S2T,
294"
ABLATION AND DISCUSSION,0.6441605839416058,"and combining it with the consistency constraint can bring further improvement. Besides, we can
295"
ABLATION AND DISCUSSION,0.6459854014598541,"also observe that the proposed method can also improve the performance of G2T, which indicates
296"
ABLATION AND DISCUSSION,0.6478102189781022,"S2T is also beneficial for G2T, and the previous ‚ÄòG2T first‚Äô paradigm not fully exploits the potential
297"
ABLATION AND DISCUSSION,0.6496350364963503,"of visual information.
298"
ABLATION AND DISCUSSION,0.6514598540145985,"Table 5: Ablation results (BLEU-4, %) of design choices of MonoSLT on Phoenix14T."
ABLATION AND DISCUSSION,0.6532846715328468,"Traning Scheme
Annealing
Consistency
V2T
C2T
G2T
Dev
Test
Dev
Test
Dev
Test
Joint Learning
28.30
28.18
28.87
29.73
28.19
27.38"
ABLATION AND DISCUSSION,0.6551094890510949,"Sentence-wise
Code-switching"
ABLATION AND DISCUSSION,0.656934306569343,"28.35
29.13
29.20
29.07
26.92
26.30
‚úì
27.42
28.88
28.73
29.37
28.98
28.41
‚úì
‚úì
28.91
30.18
29.69
30.76
29.67
28.08"
ABLATION AND DISCUSSION,0.6587591240875912,"Token-wise
Code-switching"
ABLATION AND DISCUSSION,0.6605839416058394,"27.12
28.44
28.90
29.17
26.54
26.10
‚úì
28.42
29.14
28.77
29.87
29.26
29.71
‚úì
‚úì
29.73
30.03
29.96
31.15
30.39
30.20"
ABLATION AND DISCUSSION,0.6624087591240876,"Table 6: Ablation results (BLEU-4, %) of source features and frozen layers on Phoenix14T."
ABLATION AND DISCUSSION,0.6642335766423357,"Source Feature
Frozen Layer
V2T
C2T
G2T
Features
Logits
GCN module
Conv1D
Dev
Test
Dev
Test
Dev
Test
‚úì
29.73
30.03
29.96
31.15
30.39
30.20
‚úì
28.26
29.55
28.99
29.77
28.08
27.55
‚úì
‚úì
28.59
29.73
29.20
30.21
28.89
29.66
‚úì
‚úì
‚úì
29.29
31.45
29.68
31.21
30.06
30.98"
ABLATION AND DISCUSSION,0.666058394160584,"Ablation on Other Designs. Compared to visual features, logits are a closer representation of
299"
ABLATION AND DISCUSSION,0.6678832116788321,"glosses. As shown in Table 6, adopting logits as input leads to a little performance degradation, which
300"
ABLATION AND DISCUSSION,0.6697080291970803,"also indicates glosses are imprecise representations of signs on Phoenix14T. To further explore the
301"
ABLATION AND DISCUSSION,0.6715328467153284,"origin of performance gain, we evaluate the effects of frozen modules in Table 6. Frozing both the
302"
ABLATION AND DISCUSSION,0.6733576642335767,"GCN-based module and Conv1D can achieve comparable results, which indicates the improvement
303"
ABLATION AND DISCUSSION,0.6751824817518248,"mainly comes from making better use of existing features, rather than extracting new visual features.
304"
ABLATION AND DISCUSSION,0.677007299270073,"Adopting the frozen version of MonoSLT can also improve training efficiency.
305"
ABLATION AND DISCUSSION,0.6788321167883211,"Limitations and Discussions. Although the proposed MonoSLT achieves competitive results on two
306"
ABLATION AND DISCUSSION,0.6806569343065694,"benchmarks, we notice several limitations of our model. First, the proposed method is motivated to
307"
ABLATION AND DISCUSSION,0.6824817518248175,"solve the hallucination problem of SLT, however, we have not found proper metrics to quantitatively
308"
ABLATION AND DISCUSSION,0.6843065693430657,"evaluate the faithfulness of SLT models and still use BLEU and ROUGE for evaluation. We believe
309"
ABLATION AND DISCUSSION,0.6861313868613139,"faithfulness is important when SLT is applied in real life because an unfaithful SLT model may
310"
ABLATION AND DISCUSSION,0.6879562043795621,"produce unexpected consequences. Secondly, although the proposed method can improve faithfulness
311"
ABLATION AND DISCUSSION,0.6897810218978102,"in SLT, as shown in Table. 6, it does not extract new visual features, which indicates that expensive
312"
ABLATION AND DISCUSSION,0.6916058394160584,"gloss annotations are still essential. Designing effective gloss-free is a fascinating route for SLT.
313"
ABLATION AND DISCUSSION,0.6934306569343066,"Third, although we design several approaches to make the training stable, it still encounters difficulties
314"
ABLATION AND DISCUSSION,0.6952554744525548,"in converging occasionally, and we will continue to enhance its stability.
315"
CONCLUSION,0.6970802919708029,"5
Conclusion
316"
CONCLUSION,0.698905109489051,"Faithfulness is one of the desired criteria to evaluate the applicability of SLT models. In this paper,
317"
CONCLUSION,0.7007299270072993,"we explore the association among different SLT-relevant tasks and reveal that the lack of faithfulness
318"
CONCLUSION,0.7025547445255474,"exists in recent SLT methods. To improve faithfulness in SLT, we attempt to increase the utilization of
319"
CONCLUSION,0.7043795620437956,"visual signals in SLT and propose a framework named MonoSLT, which leverages the monotonically
320"
CONCLUSION,0.7062043795620438,"aligned nature of SLT subtasks to train them jointly. We further propose two kinds of constraints to
321"
CONCLUSION,0.708029197080292,"align visual and linguistic embeddings and leverage the advantage of subtasks. Experimental results
322"
CONCLUSION,0.7098540145985401,"show that the proposed MonoSLT is competitive against previous SLT methods by increasing the
323"
CONCLUSION,0.7116788321167883,"utilization of visual signals, especially when glosses are imprecise. We hope the proposed method
324"
CONCLUSION,0.7135036496350365,"and empirical conclusions can inspire future studies on SLT and relevant tasks.
325"
CONCLUSION,0.7153284671532847,"Broader Impact
This paper focuses on improving faithfulness in SLT to bridge the communication
326"
CONCLUSION,0.7171532846715328,"gap between the Deaf and hearing communities. Although the MonoSLT has made some progress
327"
CONCLUSION,0.718978102189781,"there still seems a long way to go. Please note this research is limited to public datasets which have
328"
CONCLUSION,0.7208029197080292,"limited samples and are collected under constrained conditions, and the findings may not directly
329"
CONCLUSION,0.7226277372262774,"transfer to other scenarios or domains. Domain expertise and human supervision are essential when
330"
CONCLUSION,0.7244525547445255,"using it to make critical decisions, which may generate erroneous or potentially harmful translations.
331"
CONCLUSION,0.7262773722627737,"Moreover, potential biases in the training data or method may introduce limitations or assumptions
332"
CONCLUSION,0.7281021897810219,"that need to be considered when using it.
333"
REFERENCES,0.7299270072992701,"References
334"
REFERENCES,0.7317518248175182,"[1] Wendy Sandler and Diane Lillo-Martin. Sign language and linguistic universals. Cambridge
335"
REFERENCES,0.7335766423357665,"University Press, 2006.
336"
REFERENCES,0.7354014598540146,"[2] Shinichi Tamura and Shingo Kawasaki. Recognition of sign language motion images. Pattern
337"
REFERENCES,0.7372262773722628,"Recognition, 21(4):343‚Äì353, 1988.
338"
REFERENCES,0.7390510948905109,"[3] Oscar Koller, Jens Forster, and Hermann Ney. Continuous sign language recognition: Towards
339"
REFERENCES,0.7408759124087592,"large vocabulary statistical recognition systems handling multiple signers. Computer Vision and
340"
REFERENCES,0.7427007299270073,"Image Understanding, 141:108‚Äì125, 2015.
341"
REFERENCES,0.7445255474452555,"[4] Britta Bauer, Sonja Nie√üen, and Hermann Hienz. Towards an automatic sign language transla-
342"
REFERENCES,0.7463503649635036,"tion system. In Proceedings of the International Workshop on Physicality and Tangibility in
343"
REFERENCES,0.7481751824817519,"Interaction: Towards New Paradigms for Interaction beyond the Desktop, Siena, Italy, 1999.
344"
REFERENCES,0.75,"[5] Xiujuan Chai, Guang Li, Yushun Lin, Zhihao Xu, Yili Tang, Xilin Chen, and Ming Zhou. Sign
345"
REFERENCES,0.7518248175182481,"language recognition and translation with kinect. In Proceedings of the IEEE International
346"
REFERENCES,0.7536496350364964,"Conference on Automatic Face and Gesture Recognition, volume 655, page 4, 2013.
347"
REFERENCES,0.7554744525547445,"[6] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden.
348"
REFERENCES,0.7572992700729927,"Neural sign language translation. In Proceedings of the IEEE Conference on Computer Vision
349"
REFERENCES,0.7591240875912408,"and Pattern Recognition, pages 7784‚Äì7793, 2018.
350"
REFERENCES,0.7609489051094891,"[7] Runpeng Cui, Hu Liu, and Changshui Zhang. Recurrent convolutional neural networks for
351"
REFERENCES,0.7627737226277372,"continuous sign language recognition by staged optimization. In Proceedings of the IEEE
352"
REFERENCES,0.7645985401459854,"Conference on Computer Vision and Pattern Recognition, pages 7361‚Äì7369, 2017.
353"
REFERENCES,0.7664233576642335,"[8] Oscar Koller, Necati Cihan Camgoz, Hermann Ney, and Richard Bowden. Weakly supervised
354"
REFERENCES,0.7682481751824818,"learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language
355"
REFERENCES,0.7700729927007299,"videos. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(9):2306‚Äì2320,
356"
REFERENCES,0.7718978102189781,"2019.
357"
REFERENCES,0.7737226277372263,"[9] Yuecong Min, Aiming Hao, Xiujuan Chai, and Xilin Chen. Visual alignment constraint for
358"
REFERENCES,0.7755474452554745,"continuous sign language recognition. In Proceedings of the IEEE International Conference on
359"
REFERENCES,0.7773722627737226,"Computer Vision, pages 11542‚Äì11551, 2021.
360"
REFERENCES,0.7791970802919708,"[10] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Multi-channel
361"
REFERENCES,0.781021897810219,"transformers for multi-articulatory sign language translation. In Computer Vision‚ÄìECCV 2020
362"
REFERENCES,0.7828467153284672,"Workshops: Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part IV 16, pages 301‚Äì319.
363"
REFERENCES,0.7846715328467153,"Springer, 2020.
364"
REFERENCES,0.7864963503649635,"[11] Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li. Improving sign lan-
365"
REFERENCES,0.7883211678832117,"guage translation with monolingual data by sign back-translation. In Proceedings of the IEEE
366"
REFERENCES,0.7901459854014599,"Conference on Computer Vision and Pattern Recognition, pages 1316‚Äì1325, 2021.
367"
REFERENCES,0.791970802919708,"[12] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality
368"
REFERENCES,0.7937956204379562,"transfer learning baseline for sign language translation. In Proceedings of the IEEE Conference
369"
REFERENCES,0.7956204379562044,"on Computer Vision and Pattern Recognition, pages 5120‚Äì5130, 2022.
370"
REFERENCES,0.7974452554744526,"[13] Alex Graves, Santiago Fern√°ndez, Faustino Gomez, and J√ºrgen Schmidhuber. Connectionist
371"
REFERENCES,0.7992700729927007,"temporal classification: labelling unsegmented sequence data with recurrent neural networks.
372"
REFERENCES,0.801094890510949,"In Proceedings of the International Conference on Machine Learning, pages 369‚Äì376, 2006.
373"
REFERENCES,0.8029197080291971,"[14] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. Spatial-temporal multi-cue network
374"
REFERENCES,0.8047445255474452,"for sign language recognition and translation. IEEE Transactions on Multimedia, 24:768‚Äì779,
375"
REFERENCES,0.8065693430656934,"2021.
376"
REFERENCES,0.8083941605839416,"[15] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, LIU Shujie, and Brian Mak. Two-stream
377"
REFERENCES,0.8102189781021898,"network for sign language recognition and translation. In Advances in Neural Information
378"
REFERENCES,0.8120437956204379,"Processing Systems, 2022.
379"
REFERENCES,0.8138686131386861,"[16] Hezhen Hu, Weichao Zhao, Wengang Zhou, and Houqiang Li. Signbert+: Hand-model-aware
380"
REFERENCES,0.8156934306569343,"self-supervised pre-training for sign language understanding. IEEE Transactions on Pattern
381"
REFERENCES,0.8175182481751825,"Analysis and Machine Intelligence, 2023.
382"
REFERENCES,0.8193430656934306,"[17] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for
383"
REFERENCES,0.8211678832116789,"machine translation. arXiv preprint arXiv:1309.4168, 2013.
384"
REFERENCES,0.822992700729927,"[18] Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-task learning for
385"
REFERENCES,0.8248175182481752,"multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for
386"
REFERENCES,0.8266423357664233,"Computational Linguistics and the 7th International Joint Conference on Natural Language
387"
REFERENCES,0.8284671532846716,"Processing, pages 1723‚Äì1732, 2015.
388"
REFERENCES,0.8302919708029197,"[19] Biao Zhang, Mathias M√ºller, and Rico Sennrich. SLTUNET: A simple unified model for sign
389"
REFERENCES,0.8321167883211679,"language translation. In International Conference on Learning Representations, 2023.
390"
REFERENCES,0.833941605839416,"[20] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
391"
REFERENCES,0.8357664233576643,"Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.
392"
REFERENCES,0.8375912408759124,"ACM Computing Surveys, 55(12):1‚Äì38, 2023.
393"
REFERENCES,0.8394160583941606,"[21] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we
394"
REFERENCES,0.8412408759124088,"define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association
395"
REFERENCES,0.843065693430657,"for Computational Linguistics, pages 4198‚Äì4205, 2020.
396"
REFERENCES,0.8448905109489051,"[22] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language
397"
REFERENCES,0.8467153284671532,"transformers: Joint end-to-end sign language recognition and translation. In Proceedings of the
398"
REFERENCES,0.8485401459854015,"IEEE Conference on Computer Vision and Pattern Recognition, pages 10023‚Äì10033, 2020.
399"
REFERENCES,0.8503649635036497,"[23] Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin Swift, Hanna Suominen, and
400"
REFERENCES,0.8521897810218978,"Hongdong Li. Tspnet: Hierarchical feature learning via temporal semantic pyramid for sign
401"
REFERENCES,0.8540145985401459,"language translation. Advances in Neural Information Processing Systems, 33:12034‚Äì12045,
402"
REFERENCES,0.8558394160583942,"2020.
403"
REFERENCES,0.8576642335766423,"[24] Samuel Albanie, G√ºl Varol, Liliane Momeni, Triantafyllos Afouras, Joon Son Chung, Neil Fox,
404"
REFERENCES,0.8594890510948905,"and Andrew Zisserman. Bsl-1k: Scaling up co-articulated sign language recognition using
405"
REFERENCES,0.8613138686131386,"mouthing cues. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK,
406"
REFERENCES,0.8631386861313869,"August 23‚Äì28, 2020, Proceedings, Part XI 16, pages 35‚Äì53. Springer, 2020.
407"
REFERENCES,0.864963503649635,"[25] Alptekin Orbay and Lale Akarun. Neural sign language translation by learning tokenization. In
408"
REFERENCES,0.8667883211678832,"Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition,
409"
REFERENCES,0.8686131386861314,"pages 222‚Äì228, 2020.
410"
REFERENCES,0.8704379562043796,"[26] Liliane Momeni, Hannah Bull, KR Prajwal, Samuel Albanie, G√ºl Varol, and Andrew Zisserman.
411"
REFERENCES,0.8722627737226277,"Automatic dense annotation of large-vocabulary sign language videos. In Computer Vision‚Äì
412"
REFERENCES,0.8740875912408759,"ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings,
413"
REFERENCES,0.8759124087591241,"Part XXXV, pages 671‚Äì690. Springer, 2022.
414"
REFERENCES,0.8777372262773723,"[27] Bowen Shi, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. Open-domain sign
415"
REFERENCES,0.8795620437956204,"language translation learned from online video. arXiv preprint arXiv:2205.12870, 2022.
416"
REFERENCES,0.8813868613138686,"[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
417"
REFERENCES,0.8832116788321168,"≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information
418"
REFERENCES,0.885036496350365,"Processing Systems, 30, 2017.
419"
REFERENCES,0.8868613138686131,"[29] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
420"
REFERENCES,0.8886861313868614,"understanding by generative pre-training. 2018.
421"
REFERENCES,0.8905109489051095,"[30] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
422"
REFERENCES,0.8923357664233577,"bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages
423"
REFERENCES,0.8941605839416058,"4171‚Äì4186, 2019.
424"
REFERENCES,0.8959854014598541,"[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
425"
REFERENCES,0.8978102189781022,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
426"
REFERENCES,0.8996350364963503,"follow instructions with human feedback. Advances in Neural Information Processing Systems,
427"
REFERENCES,0.9014598540145985,"35:27730‚Äì27744, 2022.
428"
REFERENCES,0.9032846715328468,"[32] Rongxiang Weng, Heng Yu, Xiangpeng Wei, and Weihua Luo. Towards enhancing faithfulness
429"
REFERENCES,0.9051094890510949,"for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods
430"
REFERENCES,0.906934306569343,"in Natural Language Processing, pages 2675‚Äì2684, 2020.
431"
REFERENCES,0.9087591240875912,"[33] Yang Feng, Wanying Xie, Shuhao Gu, Chenze Shao, Wen Zhang, Zhengxin Yang, and Dong Yu.
432"
REFERENCES,0.9105839416058394,"Modeling fluency and faithfulness for diverse neural machine translation. In Proceedings of the
433"
REFERENCES,0.9124087591240876,"AAAI Conference on Artificial Intelligence, volume 34, pages 59‚Äì66, 2020.
434"
REFERENCES,0.9142335766423357,"[34] Marc‚ÄôAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level
435"
REFERENCES,0.916058394160584,"training with recurrent neural networks. In International Conference on Learning Representa-
436"
REFERENCES,0.9178832116788321,"tions, 2016.
437"
REFERENCES,0.9197080291970803,"[35] Chaojun Wang and Rico Sennrich.
On exposure bias, hallucination and domain shift in
438"
REFERENCES,0.9215328467153284,"neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for
439"
REFERENCES,0.9233576642335767,"Computational Linguistics, pages 3544‚Äì3552. Association for Computational Linguistics, 2020.
440"
REFERENCES,0.9251824817518248,"[36] Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. Modeling coverage for
441"
REFERENCES,0.927007299270073,"neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for
442"
REFERENCES,0.9288321167883211,"Computational Linguistics, pages 76‚Äì85, 2016.
443"
REFERENCES,0.9306569343065694,"[37] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. Advances
444"
REFERENCES,0.9324817518248175,"in Neural Information Processing Systems, 32, 2019.
445"
REFERENCES,0.9343065693430657,"[38] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike
446"
REFERENCES,0.9361313868613139,"Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.
447"
REFERENCES,0.9379562043795621,"Transactions of the Association for Computational Linguistics, 8:726‚Äì742, 2020.
448"
REFERENCES,0.9397810218978102,"[39] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert:
449"
REFERENCES,0.9416058394160584,"A joint model for video and language representation learning. In Proceedings of the IEEE
450"
REFERENCES,0.9434306569343066,"International Conference on Computer Vision, pages 7464‚Äì7473, 2019.
451"
REFERENCES,0.9452554744525548,"[40] Aiming Hao, Yuecong Min, and Xilin Chen. Self-mutual distillation learning for continuous
452"
REFERENCES,0.9470802919708029,"sign language recognition. In Proceedings of the IEEE International Conference on Computer
453"
REFERENCES,0.948905109489051,"Vision, pages 11303‚Äì11312, 2021.
454"
REFERENCES,0.9507299270072993,"[41] Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi, and Alan W Black. A
455"
REFERENCES,0.9525547445255474,"survey of code-switched speech and language processing. arXiv preprint arXiv:1904.00784,
456"
REFERENCES,0.9543795620437956,"2019.
457"
REFERENCES,0.9562043795620438,"[42] Runpeng Cui, Hu Liu, and Changshui Zhang. A deep neural framework for continuous sign
458"
REFERENCES,0.958029197080292,"language recognition by iterative training. IEEE Transactions on Multimedia, 21(7):1880‚Äì1891,
459"
REFERENCES,0.9598540145985401,"2019.
460"
REFERENCES,0.9616788321167883,"[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond
461"
REFERENCES,0.9635036496350365,"empirical risk minimization. In International Conference on Learning Representations, 2017.
462"
REFERENCES,0.9653284671532847,"[44] Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin.
463"
REFERENCES,0.9671532846715328,"Cyclical annealing schedule: A simple approach to mitigating kl vanishing. In Proceedings
464"
REFERENCES,0.968978102189781,"of the North American Chapter of the Association for Computational Linguistics: Human
465"
REFERENCES,0.9708029197080292,"Language Technologies, pages 240‚Äì250, 2019.
466"
REFERENCES,0.9726277372262774,"[45] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning.
467"
REFERENCES,0.9744525547445255,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
468"
REFERENCES,0.9762773722627737,"4320‚Äì4328, 2018.
469"
REFERENCES,0.9781021897810219,"[46] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric
470"
REFERENCES,0.9799270072992701,"cross entropy for robust learning with noisy labels. In Proceedings of the IEEE International
471"
REFERENCES,0.9817518248175182,"Conference on Computer Vision, pages 322‚Äì330, 2019.
472"
REFERENCES,0.9835766423357665,"[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
473"
REFERENCES,0.9854014598540146,"evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
474"
REFERENCES,0.9872262773722628,"for Computational Linguistics, pages 311‚Äì318, 2002.
475"
REFERENCES,0.9890510948905109,"[48] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
476"
REFERENCES,0.9908759124087592,"branches out, pages 74‚Äì81, 2004.
477"
REFERENCES,0.9927007299270073,"[49] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://
478"
REFERENCES,0.9945255474452555,"github.com/open-mmlab/mmpose, 2020.
479"
REFERENCES,0.9963503649635036,"[50] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for
480"
REFERENCES,0.9981751824817519,"skeleton-based action recognition. In AAAI Conference on Artificial Intelligence, 2018.
481"
