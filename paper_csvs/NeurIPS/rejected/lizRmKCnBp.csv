Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010626992561105207,"This paper explores the problem of effectively compressing 3D geometry sets
1"
ABSTRACT,0.0021253985122210413,"containing diverse categories. We make the first attempt to tackle this fundamental
2"
ABSTRACT,0.003188097768331562,"and challenging problem and propose NeCGS, a neural compression paradigm,
3"
ABSTRACT,0.004250797024442083,"which can compress hundreds of detailed and diverse 3D mesh models (∼684 MB)
4"
ABSTRACT,0.005313496280552604,"by about 900 times (0.76 MB) with high accuracy and preservation of detailed
5"
ABSTRACT,0.006376195536663124,"geometric details. Specifically, we first represent each irregular mesh model/shape
6"
ABSTRACT,0.007438894792773645,"in a regular representation that implicitly describes the geometry structure of the
7"
ABSTRACT,0.008501594048884165,"model using a 4D regular volume, called TSDF-Def volume. Such a regular rep-
8"
ABSTRACT,0.009564293304994687,"resentation can not only capture local surfaces more effectively but also facilitate
9"
ABSTRACT,0.010626992561105207,"the subsequent process. Then we construct a quantization-aware auto-decoder
10"
ABSTRACT,0.011689691817215728,"network architecture to regress these 4D volumes, which can summarize the sim-
11"
ABSTRACT,0.012752391073326248,"ilarity of local geometric structures within a model and across different models
12"
ABSTRACT,0.01381509032943677,"for redundancy elimination, resulting in more compact representations, including
13"
ABSTRACT,0.01487778958554729,"an embedded feature of a smaller size associated with each model and a network
14"
ABSTRACT,0.015940488841657812,"parameter set shared by all models. We finally encode the resulting features and
15"
ABSTRACT,0.01700318809776833,"network parameters into bitstreams through entropy coding. After decompressing
16"
ABSTRACT,0.018065887353878853,"the features and network parameters, we can reconstruct the TSDF-Def volumes,
17"
ABSTRACT,0.019128586609989374,"where the 3D surfaces can be extracted through the deformable marching cubes.
18"
ABSTRACT,0.020191285866099893,"Extensive experiments and ablation studies demonstrate the significant advantages
19"
ABSTRACT,0.021253985122210415,"of our NeCGS over state-of-the-art methods both quantitatively and qualitatively.
20"
ABSTRACT,0.022316684378320937,"We have included the source code in the Supplemental Material.
21"
INTRODUCTION,0.023379383634431455,"1
Introduction
22"
INTRODUCTION,0.024442082890541977,"3D mesh models/shapes are widely used in various fields, such as computer graphics, virtual reality,
23"
INTRODUCTION,0.025504782146652496,"robotics, and autonomous driving. As geometric data becomes increasingly complex and voluminous,
24"
INTRODUCTION,0.026567481402763018,"effective compression techniques have become critical for efficient storage and transmission. More-
25"
INTRODUCTION,0.02763018065887354,"over, current geometry compression methods primarily focus on individual 3D models or sequences
26"
INTRODUCTION,0.028692879914984058,"of 3D models that are temporally correlated, but struggle to handle more general data sets, such as
27"
INTRODUCTION,0.02975557917109458,"compressing large numbers of unrelated 3D shapes.
28"
INTRODUCTION,0.030818278427205102,"Unlike images and videos represented as regular 2D or 3D volumes, mesh models are commonly
29"
INTRODUCTION,0.031880977683315624,"represented as triangle meshes, which are irregular and challenging to compress. Thus, a natural
30"
INTRODUCTION,0.03294367693942614,"idea is to structure the mesh models and then leverage image or video compression techniques to
31"
INTRODUCTION,0.03400637619553666,"compress them.Converting mesh models into voxelized point clouds is a common practice, and the
32"
INTRODUCTION,0.03506907545164718,"mesh models can be recovered from the point clouds via surface reconstruction methods [22, 24].
33"
INTRODUCTION,0.036131774707757705,"Based on this, in recent years, MPEG has developed two types of 3D point cloud compression (PCC)
34"
INTRODUCTION,0.03719447396386823,"standards [46, 28]: geometry-based PCC (GPCC) for static models and video-based PCC (VPCC) for
35"
INTRODUCTION,0.03825717321997875,"sequential models. And with advancements in deep learning, numerous learning-based PCC methods
36"
INTRODUCTION,0.039319872476089264,"[41, 14, 55, 19, 54] have emerged, enhancing compression efficiency. However, the voxelized point
37"
INTRODUCTION,0.040382571732199786,"clouds require a high resolution (typically 210 or more) to accurately represent geometry data, which
38"
INTRODUCTION,0.04144527098831031,"is redundancy, limiting the compression efficiency.
39"
INTRODUCTION,0.04250797024442083,"Another regular representation involves utilizing implicit fields of mesh models, such as signed
40"
INTRODUCTION,0.04357066950053135,"distance fields (SDF) and truncated signed distance fields (TSDF). This is achieved by calculating
41"
INTRODUCTION,0.044633368756641874,"the value of the implicit field at each uniformly distributed grid point, resulting in a regular volume.
42"
INTRODUCTION,0.04569606801275239,"And the mesh models can be recovered from the implicit fields through Matching Cubes [32] or its
43"
INTRODUCTION,0.04675876726886291,"variants [15, 45]. Compared with point clouds, the implicit volume could represent the mesh models
44"
INTRODUCTION,0.04782146652497343,"in a relatively small resolution. Recently proposed methods, such as DeepSDF [36], utilize multilayer
45"
INTRODUCTION,0.048884165781083955,"perceptrons (MLPs) to regress the SDFs of any given query points. While this representation achieves
46"
INTRODUCTION,0.04994686503719448,"high accuracy for single or similar models (e.g., chairs, tables), the limited receptive field of MLPs
47"
INTRODUCTION,0.05100956429330499,"makes it challenging to represent large numbers of models in different categories, which is a more
48"
INTRODUCTION,0.052072263549415514,"common scenario in practice.
49"
INTRODUCTION,0.053134962805526036,"We propose NeCGS, a novel framework for compressing large sets of geometric models. Our NeCGS
50"
INTRODUCTION,0.05419766206163656,"framework consists of two stages: regular geometry representation and compact neural compression.
51"
INTRODUCTION,0.05526036131774708,"In the first stage, each model is converted into a regular 4D volumetric format, called the TSDF-Def
52"
INTRODUCTION,0.0563230605738576,"volume, which can be considered a 3D ‘image’. In the second stage, we use an auto-decoder to
53"
INTRODUCTION,0.057385759829968117,"regress these 4D volumes. The embedded features and decoder parameters represent these models,
54"
INTRODUCTION,0.05844845908607864,"and compressing these components allows us to compress the entire geometry set. We conducted
55"
INTRODUCTION,0.05951115834218916,"extensive experiments on various datasets, demonstrating that our NeCGS framework achieves higher
56"
INTRODUCTION,0.06057385759829968,"compression efficiency compared to existing geometry compression methods when handling large
57"
INTRODUCTION,0.061636556854410204,"numbers of models. Our NeCGS can achieve a compression ratio of nearly 900 on some datasets,
58"
INTRODUCTION,0.06269925611052073,"compressing hundreds or even thousands of different models into 1∼2 MB while preserving detailed
59"
INTRODUCTION,0.06376195536663125,"structures.
60"
INTRODUCTION,0.06482465462274177,"Figure 1: Our NeCGeS can compress geometry data with hundreds or even thousands of shapes into 1~2 MB
while preserving details. Left: Original Geometry Data. Right: Decompressed Geometry Data. ü Zoom in for
details."
RELATED WORK,0.06588735387885228,"2
Related Work
61"
GEOMETRY REPRESENTATION,0.0669500531349628,"2.1
Geometry Representation
62"
GEOMETRY REPRESENTATION,0.06801275239107332,"In general, the representation of geometry data is divided into two main categories, explicit represen-
63"
GEOMETRY REPRESENTATION,0.06907545164718384,"tation and implicit representation, and they could be transformed into another.
64"
GEOMETRY REPRESENTATION,0.07013815090329437,"Explicit Representation. Among the explicit representations, voxelization [7] is the most intuitive.
65"
GEOMETRY REPRESENTATION,0.07120085015940489,"In this method, geometry models are represented by regularly distributed grids, effectively converting
66"
GEOMETRY REPRESENTATION,0.07226354941551541,"them into 3D ‘images’. While this approach simplifies the processing of geometry models using
67"
GEOMETRY REPRESENTATION,0.07332624867162593,"image processing techniques, it requires a high resolution to accurately represent the models, which
68"
GEOMETRY REPRESENTATION,0.07438894792773645,"demands substantial memory and limits its application. Another widely used geometry representation
69"
GEOMETRY REPRESENTATION,0.07545164718384698,"method is the point cloud, which consists of discrete points sampled from the surfaces of models.
70"
GEOMETRY REPRESENTATION,0.0765143464399575,"This method has become a predominant approach for surface representation [2, 39, 40]. However, the
71"
GEOMETRY REPRESENTATION,0.077577045696068,"discrete nature of the points imposes constraints on its use in downstream tasks such as rendering and
72"
GEOMETRY REPRESENTATION,0.07863974495217853,"editing. Triangle meshes offer a more precise and efficient geometry representation. By approximating
73"
GEOMETRY REPRESENTATION,0.07970244420828905,"surfaces with numerous triangles, they achieve higher accuracy and efficiency for certain downstream
74"
GEOMETRY REPRESENTATION,0.08076514346439957,"tasks.
75"
GEOMETRY REPRESENTATION,0.0818278427205101,"Implicit Representation. Implicit representations use the isosurface of a function or field to represent
76"
GEOMETRY REPRESENTATION,0.08289054197662062,"surfaces. The most widely used implicit representations include Binary Occupancy Field (BOF)
77"
GEOMETRY REPRESENTATION,0.08395324123273114,"[22, 35], Signed Distance Field (SDF) [36, 29], and Truncated Signed Distance Field (TSDF) [11],
78"
GEOMETRY REPRESENTATION,0.08501594048884166,"from which the model’s surface can be easily extracted. However, these methods are limited to
79"
GEOMETRY REPRESENTATION,0.08607863974495218,"representing watertight models. The Unsigned Distance Field (UDF) [8], which is the absolute value
80"
GEOMETRY REPRESENTATION,0.0871413390010627,"of the SDF, can represent more general models, not just watertight ones. Despite this advantage,
81"
GEOMETRY REPRESENTATION,0.08820403825717323,"extracting surfaces from UDF is challenging, which limits its application.
82"
GEOMETRY REPRESENTATION,0.08926673751328375,"Conversion between Geometry Representations. Geometry representations can be converted
83"
GEOMETRY REPRESENTATION,0.09032943676939426,"between explicit and implicit forms. Various methods [21, 22, 24, 6, 35, 29, 45] are available for
84"
GEOMETRY REPRESENTATION,0.09139213602550478,"calculating the implicit field from given models. Conversely, when converting from implicit to
85"
GEOMETRY REPRESENTATION,0.0924548352816153,"explicit forms, Marching Cubes [32] and its derivatives [48, 49, 15, 45] can reconstruct continuous
86"
GEOMETRY REPRESENTATION,0.09351753453772582,"surfaces from various implicit fields.
87"
GEOMETRY REPRESENTATION,0.09458023379383634,"2.2
3D Geometry Data Compression
88"
GEOMETRY REPRESENTATION,0.09564293304994687,"Single 3D Geometric Model Compression. In recent decades, compression techniques for images
89"
GEOMETRY REPRESENTATION,0.09670563230605739,"and videos have rapidly advanced [51, 34, 59, 5, 4]. However, the irregular nature of geometry
90"
GEOMETRY REPRESENTATION,0.09776833156216791,"data makes it more challenging to compress compared to images and video, which are represented
91"
GEOMETRY REPRESENTATION,0.09883103081827843,"as volumetric data. A natural approach is to convert geometry data into voxelized point clouds,
92"
GEOMETRY REPRESENTATION,0.09989373007438895,"treating them as 3D ‘images’, and then applying image and video compression techniques to them.
93"
GEOMETRY REPRESENTATION,0.10095642933049948,"Following this intuition, MPEG developed the GPCC standards [13, 28, 47], where triangle meshes or
94"
GEOMETRY REPRESENTATION,0.10201912858660998,"triangle soup approximates the surfaces of 3D models, enabling the compression of models with more
95"
GEOMETRY REPRESENTATION,0.1030818278427205,"complex structures. Subsequently, several improved methods [37, 60, 53, 62] and learning-based
96"
GEOMETRY REPRESENTATION,0.10414452709883103,"methods [18, 43, 10, 9, 3, 42, 54] have been proposed to further enhance compression performance.
97"
GEOMETRY REPRESENTATION,0.10520722635494155,"However, these methods rely on voxelized point clouds to represent geometry models, which is
98"
GEOMETRY REPRESENTATION,0.10626992561105207,"inefficient and memory-intensive, limiting their compression efficiency. In contrast to the previously
99"
GEOMETRY REPRESENTATION,0.1073326248671626,"mentioned methods, Draco [12] uses a kd-tree-based coding method to compress vertices and employs
100"
GEOMETRY REPRESENTATION,0.10839532412327312,"the EdgeBreaker algorithm to encode the topological relationships of the geometry data. Draco
101"
GEOMETRY REPRESENTATION,0.10945802337938364,"utilizes uniform quantization to control the compression ratio, but its performance decreases at higher
102"
GEOMETRY REPRESENTATION,0.11052072263549416,"compression ratios.
103"
GEOMETRY REPRESENTATION,0.11158342189160468,"Multiple Model Compression. Compared to compressing single 3D geometric models, compressing
104"
GEOMETRY REPRESENTATION,0.1126461211477152,"multiple objects is significantly more challenging. SLRMA [17] addresses this by using a low-rank
105"
GEOMETRY REPRESENTATION,0.11370882040382571,"matrix to approximate vertex matrices, thus compressing sequential models. Mekuria et al. [33]
106"
GEOMETRY REPRESENTATION,0.11477151965993623,"proposed the first codec for compressing sequential point clouds, where each frame is coded using
107"
GEOMETRY REPRESENTATION,0.11583421891604676,"Octree subdivision through an 8-bit occupancy code. Building on this concept, MPEG developed the
108"
GEOMETRY REPRESENTATION,0.11689691817215728,"VPCC standards [13, 28, 47], which utilize 3D-to-2D projection and encode time-varying projected
109"
GEOMETRY REPRESENTATION,0.1179596174282678,"planes, depth maps, and other data using video codecs. Several improved methods [57, 26, 1, 44]
110"
GEOMETRY REPRESENTATION,0.11902231668437832,"have been proposed to enhance the compression of sequential models. Recently, shape priors like
111"
GEOMETRY REPRESENTATION,0.12008501594048884,"SMPL [31] and SMAL [63] have been introduced, allowing the pose and shape of a template frame
112"
GEOMETRY REPRESENTATION,0.12114771519659936,"to be altered using only a few parameters. Pose-driven geometry compression methods [16, 58, 56]
113"
GEOMETRY REPRESENTATION,0.12221041445270989,Embedded Features ……
GEOMETRY REPRESENTATION,0.12327311370882041,Geometry Set
GEOMETRY REPRESENTATION,0.12433581296493093,"TSDF-Def Volumes
Predicted TSDF-Def Volumes"
GEOMETRY REPRESENTATION,0.12539851222104145,"Regression
Loss
DMC"
GEOMETRY REPRESENTATION,0.12646121147715197,"Compact Neural Representation
Regular Geometry Representation"
GEOMETRY REPRESENTATION,0.1275239107332625,Bitstream ……
GEOMETRY REPRESENTATION,0.12858660998937302,Decompressed Geometry Set
GEOMETRY REPRESENTATION,0.12964930924548354,"DMC
Optimization"
GEOMETRY REPRESENTATION,0.13071200850159406,Decoder …… …… …… …… …… ……
GEOMETRY REPRESENTATION,0.13177470775770456,Entropy Codec
GEOMETRY REPRESENTATION,0.13283740701381508,"Figure 2: The pipeline of NeCGS. It first represents original meshes regularly into TSDF-Def volumes, and an
auto-decoder network is utilized to regress these volume. Then the embedded features and decoder parameters
are compressed into bitstreams through entropy coding. When decompressing the models, the decompressed
embedded features are fed into the decoder with the decompressed parameters from the bitstreams, reconstructing
the TSDF-Def volumes, and the models can be extracted from them."
GEOMETRY REPRESENTATION,0.1339001062699256,"leverage this approach to achieve high compression efficiency. However, these methods are limited to
114"
GEOMETRY REPRESENTATION,0.13496280552603612,"sequences of corresponding geometry data and cannot handle sets of unrelated geometry data, which
115"
GEOMETRY REPRESENTATION,0.13602550478214664,"is more common in practice.
116"
PROPOSED METHOD,0.13708820403825717,"3
Proposed Method
117"
PROPOSED METHOD,0.1381509032943677,"Overview. Given a set of N 3D mesh models containing diverse categories, denoted as S = {Si}N
i=1,
118"
PROPOSED METHOD,0.1392136025504782,"we aim to compress them into a bitstream while maintaining the quality of the decompressed models
119"
PROPOSED METHOD,0.14027630180658873,"as much as possible. To this end, we propose a neural compression paradigm called NeCGS. As
120"
PROPOSED METHOD,0.14133900106269925,"shown in Fig. 2, NeCGS consists of two main modules, i.e., Regular Geometry Representation (RGR)
121"
PROPOSED METHOD,0.14240170031880978,"and Compact Neural Representation (CNR). Specifically, RGR first represents each irregular mesh
122"
PROPOSED METHOD,0.1434643995749203,"model within S into a regular 4D volume, namely TSDF-Def volume that mplicitly describes the
123"
PROPOSED METHOD,0.14452709883103082,"geometry structure of the model, via a rendering-based optimization, thus leading to a set of 4D
124"
PROPOSED METHOD,0.14558979808714134,"volumes V := {Vi}N
i=1 with Vi corresponding to Si. Then CNR further obtains a more compact
125"
PROPOSED METHOD,0.14665249734325186,"neural representation of V, where a quantization-aware auto-decoder-based network is constructed
126"
PROPOSED METHOD,0.14771519659936239,"to regress these volumes, producing an embedded feature for each volume. Finally, the embedded
127"
PROPOSED METHOD,0.1487778958554729,"features along with the network parameters are encoded into a bitstream through a typical entropy
128"
PROPOSED METHOD,0.14984059511158343,"coding method to achieve compression. We also want to note that NeCGS can also be applied to
129"
PROPOSED METHOD,0.15090329436769395,"compress 3D geometry sets represented in 3D point clouds, where one can either reconstruct from the
130"
PROPOSED METHOD,0.15196599362380447,"given point clouds 3D surfaces through a typical surface reconstruction method or adopt a pre-trained
131"
PROPOSED METHOD,0.153028692879915,"network for SDF estimation from point clouds, e.g., SPSR [22] or IMLS [24], to bridge the gap
132"
PROPOSED METHOD,0.15409139213602552,"between 3D mesh and point cloud models. In what follows, we will detail NeCGS.
133"
REGULAR GEOMETRY REPRESENTATION,0.155154091392136,"3.1
Regular Geometry Representation
134"
REGULAR GEOMETRY REPRESENTATION,0.15621679064824653,"Figure 3: 2D visual illustration of DMC.
The blue points refer to the deformable grid
points, the green points refer to the vertices of
the extracted surfaces, and the orange lines
refer to the faces of the extracted surfaces.
Left: The original grid points. Right: The
surface extraction."
REGULAR GEOMETRY REPRESENTATION,0.15727948990435706,"Unlike 2D images and videos, where pixels are uniformly
135"
REGULAR GEOMETRY REPRESENTATION,0.15834218916046758,"distributed on 2D regular girds, the irregular characteristic
136"
REGULAR GEOMETRY REPRESENTATION,0.1594048884165781,"of 3D mesh models makes it challenging to compress them
137"
REGULAR GEOMETRY REPRESENTATION,0.16046758767268862,"efficiently and effectively. We propose to convert each
138"
REGULAR GEOMETRY REPRESENTATION,0.16153028692879914,"3D mesh model to a 4D regular volume called TSDF-
139"
REGULAR GEOMETRY REPRESENTATION,0.16259298618490967,"Def volume, which implicitly represents the geometry
140"
REGULAR GEOMETRY REPRESENTATION,0.1636556854410202,"structure of the model. Such a regular representation can
141"
REGULAR GEOMETRY REPRESENTATION,0.1647183846971307,"describe the model precisely, and its regular nature proves
142"
REGULAR GEOMETRY REPRESENTATION,0.16578108395324123,"beneficial for compression in the subsequent stage.
143"
REGULAR GEOMETRY REPRESENTATION,0.16684378320935175,"TSDF-Def Volume. Although 3D regular SDF or TSDF
144"
REGULAR GEOMETRY REPRESENTATION,0.16790648246546228,"volumes are widely used for representing 3D geometry
145"
REGULAR GEOMETRY REPRESENTATION,0.1689691817215728,"models, they may introduce distortions when the volume
146"
REGULAR GEOMETRY REPRESENTATION,0.17003188097768332,"resolution is relatively limited. Inspired by recent shape extracting methods [48, 49], we propose
147"
REGULAR GEOMETRY REPRESENTATION,0.17109458023379384,"TSDF-Def, which extends the regular TSDF volume by introducing an additional deformation for
148"
REGULAR GEOMETRY REPRESENTATION,0.17215727948990436,"each grid point to adjust the detailed structure during the extraction of models, as shown in Fig.
149"
REGULAR GEOMETRY REPRESENTATION,0.17321997874601489,"3. Accordingly, we develop the differentiable Deformable Marching Cubes (DMC), the variant of
150"
REGULAR GEOMETRY REPRESENTATION,0.1742826780021254,"the Marching Cubes method [32], for surface extraction from a TSDF-Def volume. Consequently,
151"
REGULAR GEOMETRY REPRESENTATION,0.17534537725823593,"each shape S is represented as a 4D TSDF-Def volume, denoted as V ∈RK×K×K×4, where K
152"
REGULAR GEOMETRY REPRESENTATION,0.17640807651434645,"is the volume resolution. More specifically, the value of the grid point located at (u, v, w) is
153"
REGULAR GEOMETRY REPRESENTATION,0.17747077577045697,"V(u, v, w) := [TSDF(u, v, w), ∆u, ∆v, ∆w], where (∆u, ∆v, ∆w) are the deformation for the grid
154"
REGULAR GEOMETRY REPRESENTATION,0.1785334750265675,"point and 1 ≤u, v, w ≤K. TSDF-Def enhances representation accuracy, particularly when the grid
155"
REGULAR GEOMETRY REPRESENTATION,0.179596174282678,"resolution is relatively low.
156"
REGULAR GEOMETRY REPRESENTATION,0.1806588735387885,"Optimization of TSDF-Def Volumes. To obtain the optimal TSDF-Def volume V for a given model
157"
REGULAR GEOMETRY REPRESENTATION,0.18172157279489903,"S, after initializing the deformations of each grid to zero and computing the TSDF value for each
158"
REGULAR GEOMETRY REPRESENTATION,0.18278427205100956,"grid we optimize the following problem:
159"
REGULAR GEOMETRY REPRESENTATION,0.18384697130712008,"min
V ERec(DMC(V), S),
(1)"
REGULAR GEOMETRY REPRESENTATION,0.1849096705632306,"where DMC(·) refers to the differentiable DMC process for extracting surfaces from TSDF-Def
160"
REGULAR GEOMETRY REPRESENTATION,0.18597236981934112,"volumes, and the EReg(·, ·) measures the differences between the rendered depth and silhouette
161"
REGULAR GEOMETRY REPRESENTATION,0.18703506907545164,"images of two mesh models through the differentiable rasterization [25]. Algorithm 1 summarizes
162"
REGULAR GEOMETRY REPRESENTATION,0.18809776833156217,"the whole optimization process. More details can be found in Sec. A.2 of the subsequent Appendix.
163"
REGULAR GEOMETRY REPRESENTATION,0.1891604675876727,"Algorithm 1: Optimization of TSDF-Def Volumes
Input: 3D mesh model S; the maximum number of iterations maxIter.
Output: The optimal TSDF-Def volume V ∈RK×K×K×4."
REGULAR GEOMETRY REPRESENTATION,0.1902231668437832,"1 Place uniformly distributed grids in the cube of S, denoted as G ∈RK×K×K×3;"
REGULAR GEOMETRY REPRESENTATION,0.19128586609989373,"2 Initialize V[..., 0] as the ground truth TSDF of S at the location of G, the deformation
V[..., 1 :]=0, and the current iteration Iter = 0;"
REGULAR GEOMETRY REPRESENTATION,0.19234856535600425,3 while Iter < maxIter do
REGULAR GEOMETRY REPRESENTATION,0.19341126461211477,"4
Recover shape from V according to DMC, DMC(V);"
REGULAR GEOMETRY REPRESENTATION,0.1944739638682253,"5
Calculate the reconstruction error, ERec(DMC(V), S);"
REGULAR GEOMETRY REPRESENTATION,0.19553666312433582,"6
Optimize V using ADAM optimizer based on the reconstruction error;"
REGULAR GEOMETRY REPRESENTATION,0.19659936238044634,"7
Iter:=Iter+1;"
END,0.19766206163655686,8 end
END,0.19872476089266738,9 return V;
COMPACT NEURAL REPRESENTATION,0.1997874601487779,"3.2
Compact Neural Representation
164"
COMPACT NEURAL REPRESENTATION,0.20085015940488843,"Observing the similarity of local geometric structures within a typical 3D model and across different
165"
COMPACT NEURAL REPRESENTATION,0.20191285866099895,"models, i.e., redundancy, we further propose a quantization-aware neural representation process
166"
COMPACT NEURAL REPRESENTATION,0.20297555791710944,"to summarize the similarity within V, leading to more compact representations with redundancy
167"
COMPACT NEURAL REPRESENTATION,0.20403825717321997,"removed.
168"
COMPACT NEURAL REPRESENTATION,0.2051009564293305,"Network Architecture. We construct an auto-decoder network architecture to regress these 4D
169"
COMPACT NEURAL REPRESENTATION,0.206163655685441,"TSDF-Def volumes. Specifically, it is composed of a head layer, which increases the channel of its
170"
COMPACT NEURAL REPRESENTATION,0.20722635494155153,"input, and L cascaded upsampling modules, which progressively upscale the feature volume. We
171"
COMPACT NEURAL REPRESENTATION,0.20828905419766205,"also utilize the PixelShuffle technique [50] between the convolution and activation layers to achieve
172"
COMPACT NEURAL REPRESENTATION,0.20935175345377258,"upscaling. We refer reviewers to Sec. B of Appendix for more details. For TSDF-Def volume Vi,
173"
COMPACT NEURAL REPRESENTATION,0.2104144527098831,"the corresponding input to the auto-decoder is the embedded feature, denoted as Fi ∈RK′×K′×K′×C,
174"
COMPACT NEURAL REPRESENTATION,0.21147715196599362,"where K′ is the resolution satisfying K′ ≪K and C is the number of channels. Moreover, we
175"
COMPACT NEURAL REPRESENTATION,0.21253985122210414,"integrate differentiable quantization to the embedded features and network parameters in the process,
176"
COMPACT NEURAL REPRESENTATION,0.21360255047821466,"which can efficiently reduce the quantization error. In all, the compact neural representation process
177"
COMPACT NEURAL REPRESENTATION,0.2146652497343252,"can be written as
178"
COMPACT NEURAL REPRESENTATION,0.2157279489904357,"bVi = DQ(Θ)(Q(Fi)).
(2)"
COMPACT NEURAL REPRESENTATION,0.21679064824654623,"where Q(·) stands for the differentiable quantization operator, and bVi is the regressed TSDF-Def.
179"
COMPACT NEURAL REPRESENTATION,0.21785334750265675,"Loss Function. We employ a joint loss function comprising Mean Absolute Error (MAE) and
180"
COMPACT NEURAL REPRESENTATION,0.21891604675876727,"Structural Similarity Index (SSIM) to simultaneously optimize the embedded features {Fi} and
181"
COMPACT NEURAL REPRESENTATION,0.2199787460148778,"the network parameters Θ. In computing the MAE between the predicted and ground truth TSDF-
182"
COMPACT NEURAL REPRESENTATION,0.22104144527098832,"Def volumes, we concentrate more on the grids close to the surface. These surface grids crucially
183"
COMPACT NEURAL REPRESENTATION,0.22210414452709884,"determine the surfaces through their TSDFs and deformations; hence we assign them higher weights
184"
COMPACT NEURAL REPRESENTATION,0.22316684378320936,"during optimization than the grids farther away from the surface. The overall loss function for the
185"
COMPACT NEURAL REPRESENTATION,0.22422954303931988,"i-th model is written as
186"
COMPACT NEURAL REPRESENTATION,0.2252922422954304,"L( bVi, Vi) = ∥bVi −Vi∥1 + λ1∥Mi ⊙( bVi −Vi)∥1 + λ2(1 −SSIM( bVi, Vi)),
(3)"
COMPACT NEURAL REPRESENTATION,0.2263549415515409,"where Mi = 1(|Vi[..., 0])| < τ) is the mask, indicating whether a grid is near the surface, i.e., its
187"
COMPACT NEURAL REPRESENTATION,0.22741764080765142,"TSDF is less than the threshold τ, while λ1 and λ2 are the weights to balance each term of the loss
188"
COMPACT NEURAL REPRESENTATION,0.22848034006376194,"function.
189"
COMPACT NEURAL REPRESENTATION,0.22954303931987247,"Entropy Coding. After obtaining the quantized features {eFi = Q(Fi)} and quantized network
190"
COMPACT NEURAL REPRESENTATION,0.230605738575983,"parameters eΘ = Q(Θ), we adopt the Huffman Codec [20] to further compress them into a bit-
191"
COMPACT NEURAL REPRESENTATION,0.2316684378320935,"stream. More advanced entropy coding methods can be employed to further improve compression
192"
COMPACT NEURAL REPRESENTATION,0.23273113708820403,"performance.
193"
DECOMPRESSION,0.23379383634431455,"3.3
Decompression
194"
DECOMPRESSION,0.23485653560042508,"To obtain the 3D mesh models from the bitstream, we first decompress the bitstream to derive the
195"
DECOMPRESSION,0.2359192348565356,"embedded features, {eFi} and the decoder parameter, eΘ. Then, for each eFi, we feed it to the decoder
196"
DECOMPRESSION,0.23698193411264612,"D e
Θ(·) to generate its corresponding TSDF-Def volume
197"
DECOMPRESSION,0.23804463336875664,"bVi = D e
Θ(eFi).
(4)"
DECOMPRESSION,0.23910733262486716,"Finally, we utilize DMC to recover each shape from bVi, bSi = DMC( bVi), forming the set of decom-
198"
DECOMPRESSION,0.24017003188097769,"pressed geometry data, bS = {bSi}N
i=1.
199"
EXPERIMENT,0.2412327311370882,"4
Experiment
200"
EXPERIMENTAL SETTING,0.24229543039319873,"4.1
Experimental Setting
201"
EXPERIMENTAL SETTING,0.24335812964930925,"Implementation details. In the process of optimizing TSDF-Def volumes, we employed the ADAM
202"
EXPERIMENTAL SETTING,0.24442082890541977,"optimizer [23] for 500 iterations per shape, using a learning rate of 0.01. The resolution of TSDF-Def
203"
EXPERIMENTAL SETTING,0.2454835281615303,"volumes was K = 128. The resolution and the number of channels of the embedded features were
204"
EXPERIMENTAL SETTING,0.24654622741764082,"K′ = 4 and C = 16, respectively. And the decoder is composed of L = 5 upsampling modules with
205"
EXPERIMENTAL SETTING,0.24760892667375134,"an up-scaling factor of 2. During the optimization, we set λ1 = 5 and λ2 = 10, and the embedded
206"
EXPERIMENTAL SETTING,0.24867162592986186,"features and decoder parameters were optimized by the ADAM optimizer for 400 epochs, with a
207"
EXPERIMENTAL SETTING,0.24973432518597238,"learning rate of 1e-3. We achieved different compression efficiencies by adjusting decoder sizes. We
208"
EXPERIMENTAL SETTING,0.2507970244420829,"conducted all experiments on an NVIDIA RTX 3090 GPU with Intel(R) Xeon(R) CPU.
209 210"
EXPERIMENTAL SETTING,0.2518597236981934,Table 1: Details of the selected datasets1.
EXPERIMENTAL SETTING,0.25292242295430395,"Dataset
Original Size (MB)
# Models
AMA
378.41
500
DT4D
683.80
500
Thingi10K
335.92
1000
Mixed
496.16
600"
EXPERIMENTAL SETTING,0.25398512221041447,"Datasets. We tested our NeCGS on various types
211"
EXPERIMENTAL SETTING,0.255047821466525,"of datasets, including humans, animals, and CAD
212"
EXPERIMENTAL SETTING,0.2561105207226355,"models. For human models, we randomly selected
213"
EXPERIMENTAL SETTING,0.25717321997874604,"500 shapes from the AMA dataset [52]. For animal
214"
EXPERIMENTAL SETTING,0.25823591923485656,"models, we randomly selected 500 shapes from
215"
EXPERIMENTAL SETTING,0.2592986184909671,"the DT4D dataset [27]. For the CAD models, we
216"
EXPERIMENTAL SETTING,0.2603613177470776,"randomly selected 1000 shapes from the Thingi10K
217"
EXPERIMENTAL SETTING,0.2614240170031881,"dataset [61]. Besides, we randomly selected 200
218"
EXPERIMENTAL SETTING,0.2624867162592986,"models from each dataset, forming a more challenging dataset, denoted as Mixed. The details about
219"
EXPERIMENTAL SETTING,0.2635494155154091,"the selected datasets are shown in Table 1. In all experiments, we scaled all models in a cube with a
220"
EXPERIMENTAL SETTING,0.26461211477151964,"range of [−1, 1]3 to ensure they are in the same scale.
221"
EXPERIMENTAL SETTING,0.26567481402763016,"Methods under Comparison. In terms of traditional geometry codecs, we chose the three most
222"
EXPERIMENTAL SETTING,0.2667375132837407,"impactful geometry coding standards with released codes, G-PCC2 and V-PCC3 from MPEG (see
223"
EXPERIMENTAL SETTING,0.2678002125398512,"1The original geometry data is kept as triangle meshes, so the storage size is much less than the voxelized
point clouds.
2https://github.com/MPEGGroup/mpeg-pcc-tmc13
3https://github.com/MPEGGroup/mpeg-pcc-tmc2"
EXPERIMENTAL SETTING,0.2688629117959617,"more details about them in [13, 28, 47]), and Draco 4 from Google as the baseline methods. Addi-
224"
EXPERIMENTAL SETTING,0.26992561105207225,"tionally, we compared our approach with state-of-the-art deep learning-based compression methods,
225"
EXPERIMENTAL SETTING,0.27098831030818277,"specifically PCGCv2 [54]. Furthermore, we adapted DeepSDF [36] with quantization to serve as
226"
EXPERIMENTAL SETTING,0.2720510095642933,"another baseline method, denoted as QuantDeepSDF. It is worth noting that while some of the chosen
227"
EXPERIMENTAL SETTING,0.2731137088204038,"baseline methods were originally designed for point cloud compression, we utilized voxel sampling
228"
EXPERIMENTAL SETTING,0.27417640807651433,"and SPSR [22] to convert them between the forms of point cloud and surface. More details can be
229"
EXPERIMENTAL SETTING,0.27523910733262485,"found in Sec. C.2 appendix.
230"
EXPERIMENTAL SETTING,0.2763018065887354,"200
400
600
800
Compression Ratio 5 10 15 20 25"
EXPERIMENTAL SETTING,0.2773645058448459,"CD (10
3)"
EXPERIMENTAL SETTING,0.2784272051009564,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.27948990435706694,"200
400
600
800
Compression Ratio 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
EXPERIMENTAL SETTING,0.28055260361317746,F-0.005
EXPERIMENTAL SETTING,0.281615302869288,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.2826780021253985,(a) AMA
EXPERIMENTAL SETTING,0.28374070138150903,"200
400
600
800
1000 1200 1400
Compression Ratio 5 10 15 20 25"
EXPERIMENTAL SETTING,0.28480340063761955,"CD (10
3)"
EXPERIMENTAL SETTING,0.2858660998937301,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.2869287991498406,"200
400
600
800
1000 1200 1400
Compression Ratio 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTAL SETTING,0.2879914984059511,F-0.005
EXPERIMENTAL SETTING,0.28905419766206164,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.29011689691817216,(b) DT4D
EXPERIMENTAL SETTING,0.2911795961742827,"100
200
300
400
Compression Ratio 5 10 15 20 25 30 35 40"
EXPERIMENTAL SETTING,0.2922422954303932,"CD (10
3)"
EXPERIMENTAL SETTING,0.29330499468650373,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.29436769394261425,"100
200
300
400
Compression Ratio 0.2 0.3 0.4 0.5 0.6"
EXPERIMENTAL SETTING,0.29543039319872477,F-0.005
EXPERIMENTAL SETTING,0.2964930924548353,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.2975557917109458,(c) Thingi10K
EXPERIMENTAL SETTING,0.29861849096705634,"0
200
400
600
800
Compression Ratio 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5"
EXPERIMENTAL SETTING,0.29968119022316686,"CD (10
3)"
EXPERIMENTAL SETTING,0.3007438894792774,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.3018065887353879,"0
200
400
600
800
Compression Ratio 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
EXPERIMENTAL SETTING,0.3028692879914984,F-0.005
EXPERIMENTAL SETTING,0.30393198724760895,"GPCC
VPCC
PCGCv2
Draco
QuantDeepSDF
Ours"
EXPERIMENTAL SETTING,0.30499468650371947,(d) Mixed
EXPERIMENTAL SETTING,0.30605738575983,Figure 4: Quantitative comparisons of different methods on four 3D geometry sets.
EXPERIMENTAL SETTING,0.3071200850159405,"Evaluation Metrics. Following previous reconstruction methods [35, 38], we utilize Chamfer
231"
EXPERIMENTAL SETTING,0.30818278427205104,"Distance (CD), Normal Consistency (NC), F-Score with the thresholds of 0.005 and 0.01 (F1-0.005
232"
EXPERIMENTAL SETTING,0.30924548352816156,"and F1-0.01) as the evaluation metrics. Furthermore, to comprehensively compare the compression
233"
EXPERIMENTAL SETTING,0.310308182784272,"efficiency of different methods, we use Rate-Distortion (RD) curves. These curves illustrate the
234"
EXPERIMENTAL SETTING,0.31137088204038255,"distortions at various compression ratios, with CD and F1-0.005 specifically describing the distortion
235"
EXPERIMENTAL SETTING,0.31243358129649307,"of the decompressed models. Our goal is to minimize distortion, indicated by a low CD and a high
236"
EXPERIMENTAL SETTING,0.3134962805526036,"F1-Score, while maximizing the compression ratio. Therefore, for the RD curve representing CD,
237"
EXPERIMENTAL SETTING,0.3145589798087141,"optimal compression performance is achieved when the curve is closest to the lower right corner.
238"
EXPERIMENTAL SETTING,0.31562167906482463,"Similarly, for the RD curve representing the F1-Score, the ideal compression performance is when
239"
EXPERIMENTAL SETTING,0.31668437832093516,"the curve is nearest to the upper right corner. Their detailed definition can be found in Sec. C.1 of
240"
EXPERIMENTAL SETTING,0.3177470775770457,"appendix.
241"
RESULTS,0.3188097768331562,"4.2
Results
242"
RESULTS,0.3198724760892667,"The RD curves of different compression methods under different datasets are shown in Fig. 4. As
243"
RESULTS,0.32093517534537724,"the compression ratio increases, the distortion also becomes larger. It is obvious that our NeCGS
244"
RESULTS,0.32199787460148777,"can achieve much better compression performance than the baseline methods when the compression
245"
RESULTS,0.3230605738575983,"ratio is high, even in the challenging Mixed dataset. In particular, our NeCGS achieves a minimum
246"
RESULTS,0.3241232731137088,"compression ratio of 300, and on the DT4D dataset, the compression ratio even reaches nearly 900,
247"
RESULTS,0.32518597236981933,"with minimal distortion. Due to the larger model differences within the Thingi10K and Mixed datasets
248"
RESULTS,0.32624867162592985,"compared to the other two datasets, the compression performance on these two datasets is inferior.
249"
RESULTS,0.3273113708820404,"(a) Ori.
(b) 455.25
(c) 651.85
(d) 899.73
Figure 6: Decompressed models under different com-
pression ratios."
RESULTS,0.3283740701381509,"The visual results of different compression meth-
250"
RESULTS,0.3294367693942614,"ods are shown in Fig. 5. Compared to other
251"
RESULTS,0.33049946865037194,"methods, models compressed using our ap-
252"
RESULTS,0.33156216790648246,"proach occupy a larger compression ratio and
253"
RESULTS,0.332624867162593,"retain more details after decompression. Fig. 6
254"
RESULTS,0.3336875664187035,4https://github.com/google/draco
RESULTS,0.33475026567481403,"(a) GPCC
(b) VPCC
(c) PCGCv2
(d) Draco (e) QuantDeepSDF
(f) Ours
(g) Ori."
RESULTS,0.33581296493092455,"AMA
DT4D
Thingi10K
Mixed"
RESULTS,0.3368756641870351,"312.83
41.95
256.01
123.20
272.91
307.79"
RESULTS,0.3379383634431456,"148.50
49.81
103.90
96.52
165.47
166.79"
RESULTS,0.3390010626992561,"457.39
244.38
402.70
153.45
409.21
455.25"
RESULTS,0.34006376195536664,"299.13
165.75
267.42
99.94
224.17
362.80"
RESULTS,0.34112646121147716,"Figure 5: Visual comparisons of different compression methods. All numbers in corners represent the
compression ratio. ü Zoom in for details."
RESULTS,0.3421891604675877,"illustrates the decompressed models under different compression ratio. Even when the compression
255"
RESULTS,0.3432518597236982,"ratio reaches nearly 900, our method can still retain the details of the models.
256"
ABLATION STUDY,0.3443145589798087,"4.3
Ablation Study
257"
ABLATION STUDY,0.34537725823591925,"In order to illustrate the efficiency of each design of our NeCGS, we conducted extensive ablation
258"
ABLATION STUDY,0.34643995749202977,"study about them on the Mixed dataset.
259"
ABLATION STUDY,0.3475026567481403,"Figure 7: Models recovered from different regular geometry repre-
sentations under various volume resolutions. From Left to Right:
Original, TSDF with K = 64, TSDF with K = 128, TSDF-Def
with K = 64, and TSDF-Def with K = 128."
ABLATION STUDY,0.3485653560042508,"Necessity of the Deformation of
260"
ABLATION STUDY,0.34962805526036134,"Grids. We utilize TSDF-Def volumes
261"
ABLATION STUDY,0.35069075451647186,"to as the regular geometry representa-
262"
ABLATION STUDY,0.3517534537725824,"tion, instead of TSDF volumes like
263"
ABLATION STUDY,0.3528161530286929,"previous methods. Compared with
264"
ABLATION STUDY,0.3538788522848034,"models recovered from TSDF vol-
265"
ABLATION STUDY,0.35494155154091395,"umes through MC, the models recov-
266"
ABLATION STUDY,0.35600425079702447,"ered from TSDF-Def volumes through
267"
ABLATION STUDY,0.357066950053135,"DMC preserve more details of the thin
268"
ABLATION STUDY,0.35812964930924546,"structures, especially when the volume resolutions are relatively small, as shown in Fig. 7. We also
269"
ABLATION STUDY,0.359192348565356,"conducted a numerical comparison of the decompressed models on the AMA dataset under these two
270"
ABLATION STUDY,0.3602550478214665,"settings, and the results are shown in Table. 2, demonstrating its advantages.
271"
ABLATION STUDY,0.361317747077577,Table 2: Quantitative comparisons of different RGRs.
ABLATION STUDY,0.36238044633368754,"RGR
Size (MB)
Com. Ratio
CD (×10−3) ↓
NC ↑
F1-0.005 ↑
F1-0.01 ↑
TSDF
1.631
304.20
5.015
0.944
0.662
0.936
TSDF-Def
1.612
307.79
4.913
0.947
0.674
0.943"
ABLATION STUDY,0.36344314558979807,"Neural Representation Structure. To illustrate the superiority of auto-decoder framework, we
272"
ABLATION STUDY,0.3645058448459086,"utilize an auto-encoder to regress the TSDF-Def volume. Technically, we used a ConvNeXt block
273"
ABLATION STUDY,0.3655685441020191,"[30] as the encoder by replacing 2D convolutions with 3D convolutions. Under the auto-encoder
274"
ABLATION STUDY,0.36663124335812963,"framework, we optimize the parameters of the encoder to change the embedded features. The RD
275"
ABLATION STUDY,0.36769394261424015,"300
400
500
600
Compression Ratio 5 6 7 8 9"
ABLATION STUDY,0.3687566418703507,"CD (10
3)"
ABLATION STUDY,0.3698193411264612,"Auto-encoder
Auto-decoder"
ABLATION STUDY,0.3708820403825717,"300
400
500
600
Compression Ratio 0.35 0.40 0.45 0.50 0.55 0.60 0.65"
ABLATION STUDY,0.37194473963868224,F-0.005
ABLATION STUDY,0.37300743889479276,"Auto-encoder
Auto-decoder (a)"
ABLATION STUDY,0.3740701381509033,"300
350
400
450
500
550
Compression Ratio 5 10 15 20 25 30 35 40"
ABLATION STUDY,0.3751328374070138,"CD (10
3)"
ABLATION STUDY,0.37619553666312433,"w/o SSIM
w/ SSIM"
ABLATION STUDY,0.37725823591923485,"300
350
400
450
500
550
Compression Ratio 0.475 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675"
ABLATION STUDY,0.3783209351753454,F-0.005
ABLATION STUDY,0.3793836344314559,"w/o SSIM
w/ SSIM"
ABLATION STUDY,0.3804463336875664,"(b)
Figure 8: (a) RD curves of different neural representation structures. (b) RD curves of different regression
losses."
ABLATION STUDY,0.38150903294367694,"curves about these two structures are shown in Fig. 8(a), demonstrating rationality of our decoder
276"
ABLATION STUDY,0.38257173219978746,"structure.
277"
ABLATION STUDY,0.383634431455898,"(a) Original
(b) w/o SSIM
(c) w/ SSIM"
ABLATION STUDY,0.3846971307120085,"Figure 9: Visual comparison of regression loss w/
and w/o SSIM item."
ABLATION STUDY,0.38575982996811903,"SSIM Loss. Compared to MAE, which focuses on
278"
ABLATION STUDY,0.38682252922422955,"one-to-one errors between predicted and ground truth
279"
ABLATION STUDY,0.38788522848034007,"volumes, the SSIM item in Eq. 3 emphasizes more
280"
ABLATION STUDY,0.3889479277364506,"on the local similarity between volumes, increasing
281"
ABLATION STUDY,0.3900106269925611,"the regression accuracy. To verify this, we removed
282"
ABLATION STUDY,0.39107332624867164,"the SSIM item and kept others unchanged. Their RD
283"
ABLATION STUDY,0.39213602550478216,"curves are shown in Fig. 8(b), and it is obvious that
284"
ABLATION STUDY,0.3931987247608927,"the SSIM item in the regression loss increases the
285"
ABLATION STUDY,0.3942614240170032,"compression performance. The visual comparison is
286"
ABLATION STUDY,0.3953241232731137,"shown in Fig. 9, and without SSIM, there are floating
287"
ABLATION STUDY,0.39638682252922425,"parts around the decompressed models.
288"
ABLATION STUDY,0.39744952178533477,"(a) Ori.
(b) 64
(c) 128
(d) 256
Figure 10: Visual comparison under differ-
ent resolutions of TSDF-Def volume."
ABLATION STUDY,0.3985122210414453,"Resolution of TSDF-Def Volumes. We tested the com-
289"
ABLATION STUDY,0.3995749202975558,"pression performance at different resolutions of TSDF-
290"
ABLATION STUDY,0.40063761955366634,"Def volumes by adjusting the decoder layers accordingly.
291"
ABLATION STUDY,0.40170031880977686,"Specifically, we removed the last layer for a resolution
292"
ABLATION STUDY,0.4027630180658874,"of 64 and added an extra layer for a resolution of 256.
293"
ABLATION STUDY,0.4038257173219979,"The quantitative and numerical comparisons are shown in
294"
ABLATION STUDY,0.40488841657810837,"Table 3 and Fig. 10, respectively. Obviously, increasing
295"
ABLATION STUDY,0.4059511158342189,"the volume resolution can enhance the compression effec-
296"
ABLATION STUDY,0.4070138150903294,"tiveness, resulting in more detailed structures preserved
297"
ABLATION STUDY,0.40807651434643993,"after decompression. However, the optimization and in-
298"
ABLATION STUDY,0.40913921360255046,"ference time also increase accordingly due to more layers
299"
ABLATION STUDY,0.410201912858661,"involved.
300"
ABLATION STUDY,0.4112646121147715,Table 3: Quantitative comparisons of different resolutions of TSDF-Def volumes.
ABLATION STUDY,0.412327311370882,"Res.
Size (MB)
Com. Ratio
CD (×10−3) ↓
NC ↑
F1-0.005 ↑
F1-0.01 ↑
Opt Time (h)
Infer. Time (ms)
64
1.408
268.75
4.271
0.927
0.721
0.966
2.16
38.97
128
1.493
253.45
3.436
0.952
0.842
0.991
16.32
98.95
256
1.627
232.58
3.234
0.962
0.870
0.995
94.50
421.94"
CONCLUSION AND DISCUSSION,0.41339001062699254,"5
Conclusion and Discussion
301"
CONCLUSION AND DISCUSSION,0.41445270988310307,"We have presented NeCGS, a highly effective neural compression scheme for 3D geometry sets.
302"
CONCLUSION AND DISCUSSION,0.4155154091392136,"NeCGS has achieved remarkable compression performance on various datasets with diverse and
303"
CONCLUSION AND DISCUSSION,0.4165781083953241,"detailed shapes, outperforming state-of-the-art compression methods to a large extent. These advan-
304"
CONCLUSION AND DISCUSSION,0.41764080765143463,"tages are attributed to our regular geometry representation and the compression accomplished by a
305"
CONCLUSION AND DISCUSSION,0.41870350690754515,"convolution-based auto-decoder. We believe our NeCGS framework will inspire further advancements
306"
CONCLUSION AND DISCUSSION,0.4197662061636557,"in the field of geometry compression.
307"
CONCLUSION AND DISCUSSION,0.4208289054197662,"However, our method still suffers from the following two limitations. One is that it requires more
308"
CONCLUSION AND DISCUSSION,0.4218916046758767,"than 15 hours to regress the TSDF-Def volumes, and the other one is that the usage of 3D convolution
309"
CONCLUSION AND DISCUSSION,0.42295430393198724,"layers limits the inference speed. Our future work will focus on addressing these challenges by
310"
CONCLUSION AND DISCUSSION,0.42401700318809776,"accelerating the optimization process and incorporating more efficient network modules.
311"
REFERENCES,0.4250797024442083,"References
312"
REFERENCES,0.4261424017003188,"[1] A. Ahmmed, M. Paul, M. Murshed, and D. Taubman. Dynamic point cloud geometry compression using
313"
REFERENCES,0.42720510095642933,"cuboid based commonality modeling framework. In 2021 IEEE International Conference on Image
314"
REFERENCES,0.42826780021253985,"Processing (ICIP), pages 2159–2163. IEEE, 2021. 3
315"
REFERENCES,0.4293304994686504,"[2] P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In Sensor Fusion IV: Control Paradigms
316"
REFERENCES,0.4303931987247609,"and Data Structures, volume 1611, pages 586–606. Spie, 1992. 3
317"
REFERENCES,0.4314558979808714,"[3] S. Biswas, J. Liu, K. Wong, S. Wang, and R. Urtasun. Muscle: Multi sweep compression of lidar using
318"
REFERENCES,0.43251859723698194,"deep entropy models. Advances in Neural Information Processing Systems, 33:22170–22181, 2020. 3
319"
REFERENCES,0.43358129649309246,"[4] H. Chen, M. Gwilliam, S.-N. Lim, and A. Shrivastava. Hnerv: A hybrid neural representation for
320"
REFERENCES,0.434643995749203,"videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
321"
REFERENCES,0.4357066950053135,"10270–10279, 2023. 3
322"
REFERENCES,0.436769394261424,"[5] H. Chen, B. He, H. Wang, Y. Ren, S. N. Lim, and A. Shrivastava. Nerv: Neural representations for videos.
323"
REFERENCES,0.43783209351753455,"Advances in Neural Information Processing Systems, 34:21557–21568, 2021. 3
324"
REFERENCES,0.43889479277364507,"[6] Z.-Q. Cheng, Y.-Z. Wang, B. Li, K. Xu, G. Dang, and S.-Y. Jin. A survey of methods for moving least
325"
REFERENCES,0.4399574920297556,"squares surfaces. In Proceedings of the Fifth Eurographics/IEEE VGTC conference on Point-Based
326"
REFERENCES,0.4410201912858661,"Graphics, pages 9–23, 2008. 3
327"
REFERENCES,0.44208289054197664,"[7] J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit functions in feature space for 3d shape reconstruction
328"
REFERENCES,0.44314558979808716,"and completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
329"
REFERENCES,0.4442082890541977,"pages 6970–6981, June 2020. 3
330"
REFERENCES,0.4452709883103082,"[8] J. Chibane, G. Pons-Moll, et al. Neural unsigned distance fields for implicit function learning. Advances in
331"
REFERENCES,0.4463336875664187,"Neural Information Processing Systems, 33:21638–21652, 2020. 3
332"
REFERENCES,0.44739638682252925,"[9] T. Fan, L. Gao, Y. Xu, D. Wang, and Z. Li. Multiscale latent-guided entropy model for lidar point cloud
333"
REFERENCES,0.44845908607863977,"compression. IEEE Transactions on Circuits and Systems for Video Technology, 33(12):7857–7869, 2023.
334 3
335"
REFERENCES,0.4495217853347503,"[10] C. Fu, G. Li, R. Song, W. Gao, and S. Liu. Octattention: Octree-based large-scale contexts model for point
336"
REFERENCES,0.4505844845908608,"cloud compression. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages
337"
REFERENCES,0.45164718384697133,"625–633, 2022. 3
338"
REFERENCES,0.4527098831030818,"[11] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li. Dynamic fusion with intra-and inter-modality
339"
REFERENCES,0.4537725823591923,"attention flow for visual question answering. In Proceedings of the IEEE/CVF conference on computer
340"
REFERENCES,0.45483528161530284,"vision and pattern recognition, pages 6639–6648, 2019. 3
341"
REFERENCES,0.45589798087141337,"[12] Google. Point cloud compression reference software. Website. https://github. com/google/draco. 3
342"
REFERENCES,0.4569606801275239,"[13] D. Graziosi, O. Nakagami, S. Kuma, A. Zaghetto, T. Suzuki, and A. Tabatabai. An overview of ongoing
343"
REFERENCES,0.4580233793836344,"point cloud compression standardization activities: Video-based (v-pcc) and geometry-based (g-pcc).
344"
REFERENCES,0.45908607863974493,"APSIPA Transactions on Signal and Information Processing, 9:e13, 2020. 3, 7
345"
REFERENCES,0.46014877789585545,"[14] A. F. Guarda, N. M. Rodrigues, and F. Pereira. Point cloud coding: Adopting a deep learning-based
346"
REFERENCES,0.461211477151966,"approach. In 2019 Picture Coding Symposium (PCS), pages 1–5. IEEE, 2019. 1
347"
REFERENCES,0.4622741764080765,"[15] B. Guillard, F. Stella, and P. Fua. Meshudf: Fast and differentiable meshing of unsigned distance field
348"
REFERENCES,0.463336875664187,"networks. In European Conference on Computer Vision, pages 576–592, 2022. 2, 3
349"
REFERENCES,0.46439957492029754,"[16] J. Hou, L.-P. Chau, N. Magnenat-Thalmann, and Y. He. Compressing 3-d human motions via keyframe-
350"
REFERENCES,0.46546227417640806,"based geometry videos. IEEE Transactions on Circuits and Systems for Video Technology, 25(1):51–62,
351"
REFERENCES,0.4665249734325186,"2014. 3
352"
REFERENCES,0.4675876726886291,"[17] J. Hou, L.-P. Chau, N. Magnenat-Thalmann, and Y. He. Sparse low-rank matrix approximation for data
353"
REFERENCES,0.46865037194473963,"compression. IEEE Transactions on Circuits and Systems for Video Technology, 27(5):1043–1054, 2015. 3
354"
REFERENCES,0.46971307120085015,"[18] L. Huang, S. Wang, K. Wong, J. Liu, and R. Urtasun. Octsqueeze: Octree-structured entropy model for
355"
REFERENCES,0.4707757704569607,"lidar compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
356"
REFERENCES,0.4718384697130712,"pages 1313–1323, 2020. 3
357"
REFERENCES,0.4729011689691817,"[19] T. Huang and Y. Liu. 3d point cloud geometry compression on deep learning. In Proceedings of the 27th
358"
REFERENCES,0.47396386822529224,"ACM international conference on multimedia, pages 890–898, 2019. 1
359"
REFERENCES,0.47502656748140276,"[20] D. A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the IRE,
360"
REFERENCES,0.4760892667375133,"40(9):1098–1101, 1952. 6
361"
REFERENCES,0.4771519659936238,"[21] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the fourth
362"
REFERENCES,0.4782146652497343,"Eurographics symposium on Geometry processing, pages 61–70, 2006. 3
363"
REFERENCES,0.47927736450584485,"[22] M. Kazhdan and H. Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics
364"
REFERENCES,0.48034006376195537,"(ToG), 32(3):1–13, 2013. 1, 3, 4, 7
365"
REFERENCES,0.4814027630180659,"[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
366"
REFERENCES,0.4824654622741764,"2014. 6
367"
REFERENCES,0.48352816153028694,"[24] R. Kolluri. Provably good moving least squares. ACM Transactions on Algorithms, 4(2):1–25, 2008. 1, 3,
368 4
369"
REFERENCES,0.48459086078639746,"[25] S. Laine, J. Hellsten, T. Karras, Y. Seol, J. Lehtinen, and T. Aila. Modular primitives for high-performance
370"
REFERENCES,0.485653560042508,"differentiable rendering. ACM Transactions on Graphics (ToG), 39(6):1–14, 2020. 5
371"
REFERENCES,0.4867162592986185,"[26] L. Li, Z. Li, V. Zakharchenko, J. Chen, and H. Li. Advanced 3d motion prediction for video-based dynamic
372"
REFERENCES,0.487778958554729,"point cloud compression. IEEE Transactions on Image Processing, 29:289–302, 2019. 3
373"
REFERENCES,0.48884165781083955,"[27] Y. Li, H. Takehara, T. Taketomi, B. Zheng, and M. Nießner. 4dcomplete: Non-rigid motion estimation
374"
REFERENCES,0.48990435706695007,"beyond the observable surface. In Proceedings of the IEEE/CVF International Conference on Computer
375"
REFERENCES,0.4909670563230606,"Vision, pages 12706–12716, 2021. 6
376"
REFERENCES,0.4920297555791711,"[28] H. Liu, H. Yuan, Q. Liu, J. Hou, and J. Liu. A comprehensive study and comparison of core technologies
377"
REFERENCES,0.49309245483528164,"for mpeg 3-d point cloud compression. IEEE Transactions on Broadcasting, 66(3):701–717, 2019. 1, 3, 7
378"
REFERENCES,0.49415515409139216,"[29] S.-L. Liu, H.-X. Guo, H. Pan, P.-S. Wang, X. Tong, and Y. Liu. Deep implicit moving least-squares
379"
REFERENCES,0.4952178533475027,"functions for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and
380"
REFERENCES,0.4962805526036132,"Pattern Recognition, pages 1788–1797, June 2021. 3
381"
REFERENCES,0.4973432518597237,"[30] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings
382"
REFERENCES,0.49840595111583424,"of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976–11986, 2022. 8
383"
REFERENCES,0.49946865037194477,"[31] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. Smpl: A skinned multi-person linear
384"
REFERENCES,0.5005313496280552,"model. ACM Trans. Graph., 34(6), oct 2015. 3
385"
REFERENCES,0.5015940488841658,"[32] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm.
386"
REFERENCES,0.5026567481402763,"ACM siggraph computer graphics, 21(4):163–169, 1987. 2, 3, 5
387"
REFERENCES,0.5037194473963869,"[33] R. Mekuria, K. Blom, and P. Cesar. Design, implementation, and evaluation of a point cloud codec for
388"
REFERENCES,0.5047821466524973,"tele-immersive video. IEEE Transactions on Circuits and Systems for Video Technology, 27(4):828–842,
389"
REFERENCES,0.5058448459086079,"2016. 3
390"
REFERENCES,0.5069075451647184,"[34] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V. Gool. Practical full resolution learned
391"
REFERENCES,0.5079702444208289,"lossless image compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern
392"
REFERENCES,0.5090329436769394,"recognition, pages 10629–10638, 2019. 3
393"
REFERENCES,0.51009564293305,"[35] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d
394"
REFERENCES,0.5111583421891605,"reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and
395"
REFERENCES,0.512221041445271,"Pattern Recognition, pages 4460–4470, June 2019. 3, 7
396"
REFERENCES,0.5132837407013815,"[36] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed
397"
REFERENCES,0.5143464399574921,"distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer
398"
REFERENCES,0.5154091392136025,"Vision and Pattern Recognition, pages 165–174, June 2019. 2, 3, 7
399"
REFERENCES,0.5164718384697131,"[37] E. Peixoto. Intra-frame compression of point cloud geometry using dyadic decomposition. IEEE Signal
400"
REFERENCES,0.5175345377258236,"Processing Letters, 27:246–250, 2020. 3
401"
REFERENCES,0.5185972369819342,"[38] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional occupancy networks. In
402"
REFERENCES,0.5196599362380446,"European Conference on Computer Vision, pages 523–540. Springer, 2020. 7
403"
REFERENCES,0.5207226354941552,"[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and
404"
REFERENCES,0.5217853347502657,"segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
405"
REFERENCES,0.5228480340063762,"652–660, 2017. 3
406"
REFERENCES,0.5239107332624867,"[40] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a
407"
REFERENCES,0.5249734325185972,"metric space. Advances in neural information processing systems, 30:1–xxx, 2017. 3
408"
REFERENCES,0.5260361317747078,"[41] M. Quach, G. Valenzise, and F. Dufaux. Learning convolutional transforms for lossy point cloud geometry
409"
REFERENCES,0.5270988310308182,"compression. In 2019 IEEE international conference on image processing (ICIP), pages 4320–4324. IEEE,
410"
REFERENCES,0.5281615302869288,"2019. 1
411"
REFERENCES,0.5292242295430393,"[42] M. Quach, G. Valenzise, and F. Dufaux. Learning convolutional transforms for lossy point cloud geometry
412"
REFERENCES,0.5302869287991498,"compression. In 2019 IEEE international conference on image processing (ICIP), pages 4320–4324. IEEE,
413"
REFERENCES,0.5313496280552603,"2019. 3
414"
REFERENCES,0.5324123273113709,"[43] Z. Que, G. Lu, and D. Xu. Voxelcontext-net: An octree based framework for point cloud compression. In
415"
REFERENCES,0.5334750265674814,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6042–6051,
416"
REFERENCES,0.5345377258235919,"2021. 3
417"
REFERENCES,0.5356004250797024,"[44] E. Ramalho, E. Peixoto, and E. Medeiros. Silhouette 4d with context selection: Lossless geometry
418"
REFERENCES,0.536663124335813,"compression of dynamic point clouds. IEEE Signal Processing Letters, 28:1660–1664, 2021. 3
419"
REFERENCES,0.5377258235919234,"[45] S. Ren, J. Hou, X. Chen, Y. He, and W. Wang. Geoudf: Surface reconstruction from 3d point clouds via
420"
REFERENCES,0.538788522848034,"geometry-guided distance representation. In Proceedings of the IEEE/CVF Internation Conference on
421"
REFERENCES,0.5398512221041445,"Computer Vision, pages 14214–14224, 2023. 2, 3
422"
REFERENCES,0.5409139213602551,"[46] S. Schwarz, M. Preda, V. Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Krivoku´ca,
423"
REFERENCES,0.5419766206163655,"S. Lasserre, Z. Li, et al. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging
424"
REFERENCES,0.5430393198724761,"and Selected Topics in Circuits and Systems, 9(1):133–148, 2018. 1
425"
REFERENCES,0.5441020191285866,"[47] S. Schwarz, M. Preda, V. Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Krivoku´ca,
426"
REFERENCES,0.5451647183846972,"S. Lasserre, Z. Li, et al. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging
427"
REFERENCES,0.5462274176408076,"and Selected Topics in Circuits and Systems, 9(1):133–148, 2018. 3, 7
428"
REFERENCES,0.5472901168969182,"[48] T. Shen, J. Gao, K. Yin, M.-Y. Liu, and S. Fidler. Deep marching tetrahedra: a hybrid representation for
429"
REFERENCES,0.5483528161530287,"high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:6087–6101,
430"
REFERENCES,0.5494155154091392,"2021. 3, 5
431"
REFERENCES,0.5504782146652497,"[49] T. Shen, J. Munkberg, J. Hasselgren, K. Yin, Z. Wang, W. Chen, Z. Gojcic, S. Fidler, N. Sharp, and J. Gao.
432"
REFERENCES,0.5515409139213603,"Flexible isosurface extraction for gradient-based mesh optimization. ACM Transactions on Graphics
433"
REFERENCES,0.5526036131774708,"(TOG), 42(4):1–16, 2023. 3, 5
434"
REFERENCES,0.5536663124335813,"[50] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time
435"
REFERENCES,0.5547290116896918,"single image and video super-resolution using an efficient sub-pixel convolutional neural network. In
436"
REFERENCES,0.5557917109458024,"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874–1883, 2016.
437 5
438"
REFERENCES,0.5568544102019128,"[51] Y. Strümpler, J. Postels, R. Yang, L. V. Gool, and F. Tombari. Implicit neural representations for image
439"
REFERENCES,0.5579171094580234,"compression. In European Conference on Computer Vision, pages 74–91. Springer, 2022. 3
440"
REFERENCES,0.5589798087141339,"[52] D. Vlasic, I. Baran, W. Matusik, and J. Popovi´c. Articulated mesh animation from multi-view silhouettes.
441"
REFERENCES,0.5600425079702445,"ACM Transactions on Graphics, 27(3):1–9, 2008. 6
442"
REFERENCES,0.5611052072263549,"[53] C. Wang, W. Zhu, Y. Xu, Y. Xu, and L. Yang. Point-voting based point cloud geometry compression. In
443"
REFERENCES,0.5621679064824655,"2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP), pages 1–5. IEEE,
444"
REFERENCES,0.563230605738576,"2021. 3
445"
REFERENCES,0.5642933049946866,"[54] J. Wang, D. Ding, Z. Li, and Z. Ma. Multiscale point cloud geometry compression. In 2021 Data
446"
REFERENCES,0.565356004250797,"Compression Conference (DCC), pages 73–82. IEEE, 2021. 1, 3, 7
447"
REFERENCES,0.5664187035069076,"[55] J. Wang, H. Zhu, H. Liu, and Z. Ma. Lossy point cloud geometry compression via end-to-end learning.
448"
REFERENCES,0.5674814027630181,"IEEE Transactions on Circuits and Systems for Video Technology, 31(12):4909–4923, 2021. 1
449"
REFERENCES,0.5685441020191286,"[56] X. Wu, P. Zhang, M. Wang, P. Chen, S. Wang, and S. Kwong. Geometric prior based deep human point
450"
REFERENCES,0.5696068012752391,"cloud geometry compression. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 3
451"
REFERENCES,0.5706695005313497,"[57] J. Xiong, H. Gao, M. Wang, H. Li, K. N. Ngan, and W. Lin. Efficient geometry surface coding in v-pcc.
452"
REFERENCES,0.5717321997874601,"IEEE Transactions on Multimedia, 25:3329–3342, 2022. 3
453"
REFERENCES,0.5727948990435706,"[58] R. Yan, Q. Yin, X. Zhang, Q. Zhang, G. Zhang, and S. Ma. Pose-driven compression for dynamic 3d
454"
REFERENCES,0.5738575982996812,"human via human prior models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3
455"
REFERENCES,0.5749202975557917,"[59] Y. Yang, R. Bamler, and S. Mandt. Improving inference for neural image compression. Advances in Neural
456"
REFERENCES,0.5759829968119022,"Information Processing Systems, 33:573–584, 2020. 3
457"
REFERENCES,0.5770456960680127,"[60] X. Zhang, W. Gao, and S. Liu. Implicit geometry partition for point cloud compression. In 2020 Data
458"
REFERENCES,0.5781083953241233,"Compression Conference (DCC), pages 73–82. IEEE, 2020. 3
459"
REFERENCES,0.5791710945802337,"[61] Q. Zhou and A. Jacobson.
Thingi10k: A dataset of 10,000 3d-printing models.
arXiv preprint
460"
REFERENCES,0.5802337938363443,"arXiv:1605.04797, 2016. 6
461"
REFERENCES,0.5812964930924548,"[62] W. Zhu, Y. Xu, D. Ding, Z. Ma, and M. Nilsson. Lossy point cloud geometry compression via region-wise
462"
REFERENCES,0.5823591923485654,"processing. IEEE Transactions on Circuits and Systems for Video Technology, 31(12):4575–4589, 2021. 3
463"
REFERENCES,0.5834218916046758,"[63] S. Zuffi, A. Kanazawa, D. Jacobs, and M. J. Black. 3D menagerie: Modeling the 3D shape and pose of
464"
REFERENCES,0.5844845908607864,"animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017. 3
465"
REFERENCES,0.5855472901168969,"Appendix
466"
REFERENCES,0.5866099893730075,"A
Regular Geometry Representation
467"
REFERENCES,0.5876726886291179,"A.1
Tensor Quantization
468"
REFERENCES,0.5887353878852285,"Denoted x is a tensor, we quantize it in a fixed interval, [a, b], at (2N + 1) levels5 by
469"
REFERENCES,0.589798087141339,"Q(x) = Round
Clamp(x, a, b) −a s"
REFERENCES,0.5908607863974495,"
× s + a,
(5)"
REFERENCES,0.59192348565356,"where s = (b −a)/2N. In our experiment, we set a = −1 and b = 1.
470"
REFERENCES,0.5929861849096706,"A.2
Optimization of TSDF-deformation Volumes
471"
REFERENCES,0.594048884165781,"We set a series of camera pose, T = {Ti}E
i=1, around the meshes. Let ID
1 (Ti) and ID
2 (Ti) represent
472"
REFERENCES,0.5951115834218916,"the depth images obtained from the reconstructed mesh DMC(V) and the given mesh S at the pose Ti
473"
REFERENCES,0.5961742826780021,"respectively. Similarly, let IM
1 (Ti) and IM
2 (Ti) denote their respective silhouette images at pose Ti.
474"
REFERENCES,0.5972369819341127,"The reconstruction error produced by silhouette and depth images at all pose are
475"
REFERENCES,0.5982996811902231,"EM(DMC(V), S) =
X"
REFERENCES,0.5993623804463337,"Ti∈T
∥I
M
1 (Ti) −I
M
2 (Ti)∥1
(6)"
REFERENCES,0.6004250797024442,"and
476"
REFERENCES,0.6014877789585548,"ED(DMC(V), S) =
X"
REFERENCES,0.6025504782146652,"Ti∈T
∥(I
D
1 (Ti) −I
D
2 (Ti)) ∗I
M
2 (Ti)∥1.
(7)"
REFERENCES,0.6036131774707758,"Then the reconstruction error is defined as
477"
REFERENCES,0.6046758767268863,"ERec(DMC(V), S) = EM(DMC(V), S) + λrecED(DMC(V), S),
(8)"
REFERENCES,0.6057385759829969,"where E = 4 and λrec = 10 in our experiment.
478"
REFERENCES,0.6068012752391073,"B
Auto-decoder-based Neural Compression
479"
REFERENCES,0.6078639744952179,"B.1
Upsampling Module
480"
REFERENCES,0.6089266737513284,"In each upsampling module, we utilize a PixelShuffle layer between the convolution and activa-
481"
REFERENCES,0.6099893730074389,"tion layers to upscale the input, as shown in Fig. 11. The input feature volume has dimensions
482"
REFERENCES,0.6110520722635494,"(Nin, Nin, Nin, Cin), with an upsampling scale of s and an output channel count of Cout.
483"
REFERENCES,0.61211477151966,"C
Experiment
484"
REFERENCES,0.6131774707757705,"C.1
Evaluation Metric
485"
REFERENCES,0.614240170031881,"Let SRec and SGT denote the reconstructed and ground-truth 3D shapes, respectively. We then
486"
REFERENCES,0.6153028692879915,"randomly sample Neval = 105 points on them, obtaining two point clouds, PRec and PGT. For each
487"
REFERENCES,0.6163655685441021,"point of PRec and PGT, the normal of the triangle face where it is sampled is considered to be its
488"
REFERENCES,0.6174282678002125,"normal vector, and the normal sets of PRec and PGT are denoted as NRec and NGT, respectively.
489"
REFERENCES,0.6184909670563231,"Let NN_Point(x, P) be the operator that returns the nearest point of x in the point cloud P. The CD
490"
REFERENCES,0.6195536663124336,"between them is defined as
491"
REFERENCES,0.620616365568544,"CD(SRec, SGT) =
1
2Neval X"
REFERENCES,0.6216790648246546,"x∈PRec
∥x −NN_Point(x, PGT)∥2"
REFERENCES,0.6227417640807651,"+
1
2Neval X"
REFERENCES,0.6238044633368757,"x∈PGT
∥x −NN_Point(x, PRec)∥2.
(9)"
REFERENCES,0.6248671625929861,"5We partition the interval [a, b] into (2N + 1) levels, rather than 2N levels, to ensure the inclusion of the
value 0. Input"
REFERENCES,0.6259298618490967,Output
REFERENCES,0.6269925611052072,Convolution
REFERENCES,0.6280552603613178,PixelShuffle
REFERENCES,0.6291179596174282,Activation
REFERENCES,0.6301806588735388,Upsampling Module
REFERENCES,0.6312433581296493,Figure 11: Upsampling Module.
REFERENCES,0.6323060573857598,"Let NN_Normal(x, P) be the operator that returns the normal vector of the point x’s nearest point in
492"
REFERENCES,0.6333687566418703,"the point cloud P. The NC is defined as
493"
REFERENCES,0.6344314558979809,"NC(SRec, SGT) =
1
2Neval X"
REFERENCES,0.6354941551540914,"x∈PRec
|NRec(x) · NN_Normal(x, PGT)|"
REFERENCES,0.6365568544102019,"+
1
2Neval X"
REFERENCES,0.6376195536663124,"x∈PGT
|NGT(x) · NN_Normal(x, PRec)|.
(10)"
REFERENCES,0.638682252922423,"F-Score is defined as the harmonic mean between the precision and the recall of points that lie within
494"
REFERENCES,0.6397449521785334,"a certain distance threshold ϵ between SRec and SGT,
495"
REFERENCES,0.640807651434644,"F −Score(SRec, SGT, ϵ) = 2 · Recall · Precision"
REFERENCES,0.6418703506907545,"Recall + Precision ,
(11)"
REFERENCES,0.6429330499468651,"where
496"
REFERENCES,0.6439957492029755,"Recall(SRec, SGT, ϵ) ="
REFERENCES,0.6450584484590861,"
x1 ∈PRec, s.t.
min
x2∈PGT ∥x1 −x2∥2 < ϵ
 ,"
REFERENCES,0.6461211477151966,"Precision(SRec, SGT, ϵ) ="
REFERENCES,0.6471838469713072,"
x2 ∈PGT, s.t.
min
x1∈PRec ∥x1 −x2∥2 < ϵ
 .
(12)"
REFERENCES,0.6482465462274176,Decoder
REFERENCES,0.6493092454835282,Figure 12: Pipeline of QuantDeepSDF.
REFERENCES,0.6503719447396387,"C.2
QuantDeepSDF
497"
REFERENCES,0.6514346439957492,"Compared to DeepSDF, our QuantDeepSDF incorporates the following two modifications:
498"
REFERENCES,0.6524973432518597,"• The decoder parameters are quantized to enhance compression efficiency.
499"
REFERENCES,0.6535600425079703,"• To maintain consistency with our NeCGS, the points sampled during training are drawn
500"
REFERENCES,0.6546227417640808,"from TSDF-Def volumes.
501"
REFERENCES,0.6556854410201913,"The pipeline of QuantDeepSDF is shown in Fig. 12. Specifically, the decoder is an MLP, where the
502"
REFERENCES,0.6567481402763018,"input is the concatenated vector of coordinate x ∈R3 and the i-th embedded feature vector Fi ∈RC,
503"
REFERENCES,0.6578108395324124,"and the output is the corresponding TSDF-Def value. In our experiment, the decoder consists of 8
504"
REFERENCES,0.6588735387885228,"layers, and the compression ratio is controled by changing the width of each layer.
505"
REFERENCES,0.6599362380446334,"C.3
Auto-Encoder in Ablation Study
506"
REFERENCES,0.6609989373007439,"Different from the auto-encoder used in our framework, where the embed features are directly
507"
REFERENCES,0.6620616365568545,"optimized, auto-encoder utilizes an encoder to produce the embedded features, where the inputs are
508"
REFERENCES,0.6631243358129649,"the TSDF-Def volumes. And the decoder is kept the same as our framework. During the optimization,
509"
REFERENCES,0.6641870350690755,"the parameters of encoder and decoder are optimized. Once optimized, the embedded features
510"
REFERENCES,0.665249734325186,"produced by the encoder and decoder parameters are compressed into bitstreams.
511"
REFERENCES,0.6663124335812965,"C.4
More Visual Results
512"
REFERENCES,0.667375132837407,"Fig. 13 depicts the visual results of the decompresed models from the AMA dataset, DT4D dataset,
513"
REFERENCES,0.6684378320935175,"and Thingi10K dataset under various compression ratios, respectively. With the compression ratio
514"
REFERENCES,0.6695005313496281,"increasing, the decompressed models still preserve the detailed structures, without large distortion.
515"
REFERENCES,0.6705632306057385,"Ori.
253.45
362.80
500.54"
REFERENCES,0.6716259298618491,"Ori.
455.26
651.85
899.73"
REFERENCES,0.6726886291179596,"Ori.
166.79
219.84
273.32"
REFERENCES,0.6737513283740701,"Figure 13: Visual results of the decompressed models under different compression ratios. From Top to Bottom:
AMA, DT4D, and Thingi10K. ü Zoom in for details."
REFERENCES,0.6748140276301806,"NeurIPS Paper Checklist
516"
CLAIMS,0.6758767268862912,"1. Claims
517"
CLAIMS,0.6769394261424017,"Question: Do the main claims made in the abstract and introduction accurately reflect the
518"
CLAIMS,0.6780021253985122,"paper’s contributions and scope?
519"
CLAIMS,0.6790648246546227,"Answer: [Yes]
520"
CLAIMS,0.6801275239107333,"Justification: Abstract.
521"
CLAIMS,0.6811902231668437,"Guidelines:
522"
CLAIMS,0.6822529224229543,"• The answer NA means that the abstract and introduction do not include the claims
523"
CLAIMS,0.6833156216790648,"made in the paper.
524"
CLAIMS,0.6843783209351754,"• The abstract and/or introduction should clearly state the claims made, including the
525"
CLAIMS,0.6854410201912858,"contributions made in the paper and important assumptions and limitations. A No or
526"
CLAIMS,0.6865037194473964,"NA answer to this question will not be perceived well by the reviewers.
527"
CLAIMS,0.6875664187035069,"• The claims made should match theoretical and experimental results, and reflect how
528"
CLAIMS,0.6886291179596175,"much the results can be expected to generalize to other settings.
529"
CLAIMS,0.6896918172157279,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
530"
CLAIMS,0.6907545164718385,"are not attained by the paper.
531"
LIMITATIONS,0.691817215727949,"2. Limitations
532"
LIMITATIONS,0.6928799149840595,"Question: Does the paper discuss the limitations of the work performed by the authors?
533"
LIMITATIONS,0.69394261424017,"Answer: [Yes]
534"
LIMITATIONS,0.6950053134962806,"Justification: Sec. 5.
535"
LIMITATIONS,0.696068012752391,"Guidelines:
536"
LIMITATIONS,0.6971307120085016,"• The answer NA means that the paper has no limitation while the answer No means that
537"
LIMITATIONS,0.6981934112646121,"the paper has limitations, but those are not discussed in the paper.
538"
LIMITATIONS,0.6992561105207227,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
539"
LIMITATIONS,0.7003188097768331,"• The paper should point out any strong assumptions and how robust the results are to
540"
LIMITATIONS,0.7013815090329437,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
541"
LIMITATIONS,0.7024442082890542,"model well-specification, asymptotic approximations only holding locally). The authors
542"
LIMITATIONS,0.7035069075451648,"should reflect on how these assumptions might be violated in practice and what the
543"
LIMITATIONS,0.7045696068012752,"implications would be.
544"
LIMITATIONS,0.7056323060573858,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
545"
LIMITATIONS,0.7066950053134963,"only tested on a few datasets or with a few runs. In general, empirical results often
546"
LIMITATIONS,0.7077577045696068,"depend on implicit assumptions, which should be articulated.
547"
LIMITATIONS,0.7088204038257173,"• The authors should reflect on the factors that influence the performance of the approach.
548"
LIMITATIONS,0.7098831030818279,"For example, a facial recognition algorithm may perform poorly when image resolution
549"
LIMITATIONS,0.7109458023379384,"is low or images are taken in low lighting. Or a speech-to-text system might not be
550"
LIMITATIONS,0.7120085015940489,"used reliably to provide closed captions for online lectures because it fails to handle
551"
LIMITATIONS,0.7130712008501594,"technical jargon.
552"
LIMITATIONS,0.71413390010627,"• The authors should discuss the computational efficiency of the proposed algorithms
553"
LIMITATIONS,0.7151965993623804,"and how they scale with dataset size.
554"
LIMITATIONS,0.7162592986184909,"• If applicable, the authors should discuss possible limitations of their approach to
555"
LIMITATIONS,0.7173219978746015,"address problems of privacy and fairness.
556"
LIMITATIONS,0.718384697130712,"• While the authors might fear that complete honesty about limitations might be used by
557"
LIMITATIONS,0.7194473963868225,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
558"
LIMITATIONS,0.720510095642933,"limitations that aren’t acknowledged in the paper. The authors should use their best
559"
LIMITATIONS,0.7215727948990436,"judgment and recognize that individual actions in favor of transparency play an impor-
560"
LIMITATIONS,0.722635494155154,"tant role in developing norms that preserve the integrity of the community. Reviewers
561"
LIMITATIONS,0.7236981934112646,"will be specifically instructed to not penalize honesty concerning limitations.
562"
THEORY ASSUMPTIONS AND PROOFS,0.7247608926673751,"3. Theory Assumptions and Proofs
563"
THEORY ASSUMPTIONS AND PROOFS,0.7258235919234857,"Question: For each theoretical result, does the paper provide the full set of assumptions and
564"
THEORY ASSUMPTIONS AND PROOFS,0.7268862911795961,"a complete (and correct) proof?
565"
THEORY ASSUMPTIONS AND PROOFS,0.7279489904357067,"Answer: [NA]
566"
THEORY ASSUMPTIONS AND PROOFS,0.7290116896918172,"Justification: [NA]
567"
THEORY ASSUMPTIONS AND PROOFS,0.7300743889479278,"Guidelines:
568"
THEORY ASSUMPTIONS AND PROOFS,0.7311370882040382,"• The answer NA means that the paper does not include theoretical results.
569"
THEORY ASSUMPTIONS AND PROOFS,0.7321997874601488,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
570"
THEORY ASSUMPTIONS AND PROOFS,0.7332624867162593,"referenced.
571"
THEORY ASSUMPTIONS AND PROOFS,0.7343251859723698,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
572"
THEORY ASSUMPTIONS AND PROOFS,0.7353878852284803,"• The proofs can either appear in the main paper or the supplemental material, but if
573"
THEORY ASSUMPTIONS AND PROOFS,0.7364505844845909,"they appear in the supplemental material, the authors are encouraged to provide a short
574"
THEORY ASSUMPTIONS AND PROOFS,0.7375132837407014,"proof sketch to provide intuition.
575"
THEORY ASSUMPTIONS AND PROOFS,0.7385759829968119,"• Inversely, any informal proof provided in the core of the paper should be complemented
576"
THEORY ASSUMPTIONS AND PROOFS,0.7396386822529224,"by formal proofs provided in appendix or supplemental material.
577"
THEORY ASSUMPTIONS AND PROOFS,0.740701381509033,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417640807651434,"4. Experimental Result Reproducibility
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.742826780021254,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7438894792773645,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7449521785334751,"of the paper (regardless of whether the code and data are provided or not)?
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7460148777895855,"Answer: [Yes]
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7470775770456961,"Justification: Sec. 4.1.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7481402763018066,"Guidelines:
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7492029755579172,"• The answer NA means that the paper does not include experiments.
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502656748140276,"• If the paper includes experiments, a No answer to this question will not be perceived
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7513283740701382,"well by the reviewers: Making the paper reproducible is important, regardless of
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7523910733262487,"whether the code and data are provided or not.
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7534537725823592,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7545164718384697,"to make their results reproducible or verifiable.
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7555791710945803,"• Depending on the contribution, reproducibility can be accomplished in various ways.
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566418703506907,"For example, if the contribution is a novel architecture, describing the architecture fully
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577045696068013,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587672688629118,"be necessary to either make it possible for others to replicate the model with the same
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7598299681190224,"dataset, or provide access to the model. In general. releasing code and data is often
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608926673751328,"one good way to accomplish this, but reproducibility can also be provided via detailed
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619553666312433,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7630180658873539,"of a large language model), releasing of a model checkpoint, or other means that are
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7640807651434643,"appropriate to the research performed.
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651434643995749,"• While NeurIPS does not require releasing code, the conference does require all submis-
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662061636556854,"sions to provide some reasonable avenue for reproducibility, which may depend on the
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767268862911796,"nature of the contribution. For example
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683315621679064,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.769394261424017,"to reproduce that algorithm.
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704569606801275,"(b) If the contribution is primarily a new model architecture, the paper should describe
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7715196599362381,"the architecture clearly and fully.
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7725823591923485,"(c) If the contribution is a new model (e.g., a large language model), then there should
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736450584484591,"either be a way to access this model for reproducing the results or a way to reproduce
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747077577045696,"the model (e.g., with an open-source dataset or instructions for how to construct
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757704569606801,"the dataset).
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7768331562167906,"(d) We recognize that reproducibility may be tricky in some cases, in which case
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778958554729012,"authors are welcome to describe the particular way they provide for reproducibility.
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7789585547290117,"In the case of closed-source models, it may be that access to the model is limited in
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7800212539851222,"some way (e.g., to registered users), but it should be possible for other researchers
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7810839532412327,"to have some path to reproducing or verifying the results.
616"
OPEN ACCESS TO DATA AND CODE,0.7821466524973433,"5. Open access to data and code
617"
OPEN ACCESS TO DATA AND CODE,0.7832093517534537,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
618"
OPEN ACCESS TO DATA AND CODE,0.7842720510095643,"tions to faithfully reproduce the main experimental results, as described in supplemental
619"
OPEN ACCESS TO DATA AND CODE,0.7853347502656748,"material?
620"
OPEN ACCESS TO DATA AND CODE,0.7863974495217854,"Answer: [Yes]
621"
OPEN ACCESS TO DATA AND CODE,0.7874601487778958,"Justification: We include the code in the supplemental material.
622"
OPEN ACCESS TO DATA AND CODE,0.7885228480340064,"Guidelines:
623"
OPEN ACCESS TO DATA AND CODE,0.7895855472901169,"• The answer NA means that paper does not include experiments requiring code.
624"
OPEN ACCESS TO DATA AND CODE,0.7906482465462275,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
625"
OPEN ACCESS TO DATA AND CODE,0.7917109458023379,"public/guides/CodeSubmissionPolicy) for more details.
626"
OPEN ACCESS TO DATA AND CODE,0.7927736450584485,"• While we encourage the release of code and data, we understand that this might not be
627"
OPEN ACCESS TO DATA AND CODE,0.793836344314559,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
628"
OPEN ACCESS TO DATA AND CODE,0.7948990435706695,"including code, unless this is central to the contribution (e.g., for a new open-source
629"
OPEN ACCESS TO DATA AND CODE,0.79596174282678,"benchmark).
630"
OPEN ACCESS TO DATA AND CODE,0.7970244420828906,"• The instructions should contain the exact command and environment needed to run to
631"
OPEN ACCESS TO DATA AND CODE,0.798087141339001,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
632"
OPEN ACCESS TO DATA AND CODE,0.7991498405951116,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
633"
OPEN ACCESS TO DATA AND CODE,0.8002125398512221,"• The authors should provide instructions on data access and preparation, including how
634"
OPEN ACCESS TO DATA AND CODE,0.8012752391073327,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
635"
OPEN ACCESS TO DATA AND CODE,0.8023379383634431,"• The authors should provide scripts to reproduce all experimental results for the new
636"
OPEN ACCESS TO DATA AND CODE,0.8034006376195537,"proposed method and baselines. If only a subset of experiments are reproducible, they
637"
OPEN ACCESS TO DATA AND CODE,0.8044633368756642,"should state which ones are omitted from the script and why.
638"
OPEN ACCESS TO DATA AND CODE,0.8055260361317748,"• At submission time, to preserve anonymity, the authors should release anonymized
639"
OPEN ACCESS TO DATA AND CODE,0.8065887353878852,"versions (if applicable).
640"
OPEN ACCESS TO DATA AND CODE,0.8076514346439958,"• Providing as much information as possible in supplemental material (appended to the
641"
OPEN ACCESS TO DATA AND CODE,0.8087141339001063,"paper) is recommended, but including URLs to data and code is permitted.
642"
OPEN ACCESS TO DATA AND CODE,0.8097768331562167,"6. Experimental Setting/Details
643"
OPEN ACCESS TO DATA AND CODE,0.8108395324123273,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
644"
OPEN ACCESS TO DATA AND CODE,0.8119022316684378,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
645"
OPEN ACCESS TO DATA AND CODE,0.8129649309245484,"results?
646"
OPEN ACCESS TO DATA AND CODE,0.8140276301806588,"Answer: [Yes]
647"
OPEN ACCESS TO DATA AND CODE,0.8150903294367694,"Justification: Sec. 4.1
648"
OPEN ACCESS TO DATA AND CODE,0.8161530286928799,"Guidelines:
649"
OPEN ACCESS TO DATA AND CODE,0.8172157279489904,"• The answer NA means that the paper does not include experiments.
650"
OPEN ACCESS TO DATA AND CODE,0.8182784272051009,"• The experimental setting should be presented in the core of the paper to a level of detail
651"
OPEN ACCESS TO DATA AND CODE,0.8193411264612115,"that is necessary to appreciate the results and make sense of them.
652"
OPEN ACCESS TO DATA AND CODE,0.820403825717322,"• The full details can be provided either with the code, in appendix, or as supplemental
653"
OPEN ACCESS TO DATA AND CODE,0.8214665249734325,"material.
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.822529224229543,"7. Experiment Statistical Significance
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8235919234856536,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.824654622741764,"information about the statistical significance of the experiments?
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8257173219978746,"Answer: [Yes]
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8267800212539851,"Justification: Sec. 4.2 and 4.3.
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8278427205100957,"Guidelines:
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8289054197662061,"• The answer NA means that the paper does not include experiments.
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8299681190223167,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8310308182784272,"dence intervals, or statistical significance tests, at least for the experiments that support
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320935175345378,"the main claims of the paper.
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331562167906482,"• The factors of variability that the error bars are capturing should be clearly stated (for
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8342189160467588,"example, train/test split, initialization, random drawing of some parameter, or overall
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8352816153028693,"run with given experimental conditions).
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8363443145589798,"• The method for calculating the error bars should be explained (closed form formula,
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374070138150903,"call to a library function, bootstrap, etc.)
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384697130712009,"• The assumptions made should be given (e.g., Normally distributed errors).
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8395324123273114,"• It should be clear whether the error bar is the standard deviation or the standard error
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8405951115834219,"of the mean.
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8416578108395324,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.842720510095643,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8437832093517534,"of Normality of errors is not verified.
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.844845908607864,"• For asymmetric distributions, the authors should be careful not to show in tables or
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459086078639745,"figures symmetric error bars that would yield results that are out of range (e.g. negative
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8469713071200851,"error rates).
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8480340063761955,"• If error bars are reported in tables or plots, The authors should explain in the text how
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8490967056323061,"they were calculated and reference the corresponding figures or tables in the text.
680"
EXPERIMENTS COMPUTE RESOURCES,0.8501594048884166,"8. Experiments Compute Resources
681"
EXPERIMENTS COMPUTE RESOURCES,0.8512221041445271,"Question: For each experiment, does the paper provide sufficient information on the com-
682"
EXPERIMENTS COMPUTE RESOURCES,0.8522848034006376,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
683"
EXPERIMENTS COMPUTE RESOURCES,0.8533475026567482,"the experiments?
684"
EXPERIMENTS COMPUTE RESOURCES,0.8544102019128587,"Answer: [Yes]
685"
EXPERIMENTS COMPUTE RESOURCES,0.8554729011689692,"Justification: Sec. 4.1 and 4.3.
686"
EXPERIMENTS COMPUTE RESOURCES,0.8565356004250797,"Guidelines:
687"
EXPERIMENTS COMPUTE RESOURCES,0.8575982996811902,"• The answer NA means that the paper does not include experiments.
688"
EXPERIMENTS COMPUTE RESOURCES,0.8586609989373007,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
689"
EXPERIMENTS COMPUTE RESOURCES,0.8597236981934112,"or cloud provider, including relevant memory and storage.
690"
EXPERIMENTS COMPUTE RESOURCES,0.8607863974495218,"• The paper should provide the amount of compute required for each of the individual
691"
EXPERIMENTS COMPUTE RESOURCES,0.8618490967056323,"experimental runs as well as estimate the total compute.
692"
EXPERIMENTS COMPUTE RESOURCES,0.8629117959617428,"• The paper should disclose whether the full research project required more compute
693"
EXPERIMENTS COMPUTE RESOURCES,0.8639744952178533,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
694"
EXPERIMENTS COMPUTE RESOURCES,0.8650371944739639,"didn’t make it into the paper).
695"
CODE OF ETHICS,0.8660998937300743,"9. Code Of Ethics
696"
CODE OF ETHICS,0.8671625929861849,"Question: Does the research conducted in the paper conform, in every respect, with the
697"
CODE OF ETHICS,0.8682252922422954,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
698"
CODE OF ETHICS,0.869287991498406,"Answer: [Yes]
699"
CODE OF ETHICS,0.8703506907545164,"Justification: [NA]
700"
CODE OF ETHICS,0.871413390010627,"Guidelines:
701"
CODE OF ETHICS,0.8724760892667375,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
702"
CODE OF ETHICS,0.873538788522848,"• If the authors answer No, they should explain the special circumstances that require a
703"
CODE OF ETHICS,0.8746014877789585,"deviation from the Code of Ethics.
704"
CODE OF ETHICS,0.8756641870350691,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
705"
CODE OF ETHICS,0.8767268862911796,"eration due to laws or regulations in their jurisdiction).
706"
BROADER IMPACTS,0.8777895855472901,"10. Broader Impacts
707"
BROADER IMPACTS,0.8788522848034006,"Question: Does the paper discuss both potential positive societal impacts and negative
708"
BROADER IMPACTS,0.8799149840595112,"societal impacts of the work performed?
709"
BROADER IMPACTS,0.8809776833156217,"Answer: [Yes]
710"
BROADER IMPACTS,0.8820403825717322,"Justification: [NA]
711"
BROADER IMPACTS,0.8831030818278427,"Guidelines:
712"
BROADER IMPACTS,0.8841657810839533,"• The answer NA means that there is no societal impact of the work performed.
713"
BROADER IMPACTS,0.8852284803400637,"• If the authors answer NA or No, they should explain why their work has no societal
714"
BROADER IMPACTS,0.8862911795961743,"impact or why the paper does not address societal impact.
715"
BROADER IMPACTS,0.8873538788522848,"• Examples of negative societal impacts include potential malicious or unintended uses
716"
BROADER IMPACTS,0.8884165781083954,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
717"
BROADER IMPACTS,0.8894792773645058,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
718"
BROADER IMPACTS,0.8905419766206164,"groups), privacy considerations, and security considerations.
719"
BROADER IMPACTS,0.8916046758767269,"• The conference expects that many papers will be foundational research and not tied
720"
BROADER IMPACTS,0.8926673751328374,"to particular applications, let alone deployments. However, if there is a direct path to
721"
BROADER IMPACTS,0.8937300743889479,"any negative applications, the authors should point it out. For example, it is legitimate
722"
BROADER IMPACTS,0.8947927736450585,"to point out that an improvement in the quality of generative models could be used to
723"
BROADER IMPACTS,0.895855472901169,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
724"
BROADER IMPACTS,0.8969181721572795,"that a generic algorithm for optimizing neural networks could enable people to train
725"
BROADER IMPACTS,0.89798087141339,"models that generate Deepfakes faster.
726"
BROADER IMPACTS,0.8990435706695006,"• The authors should consider possible harms that could arise when the technology is
727"
BROADER IMPACTS,0.900106269925611,"being used as intended and functioning correctly, harms that could arise when the
728"
BROADER IMPACTS,0.9011689691817216,"technology is being used as intended but gives incorrect results, and harms following
729"
BROADER IMPACTS,0.9022316684378321,"from (intentional or unintentional) misuse of the technology.
730"
BROADER IMPACTS,0.9032943676939427,"• If there are negative societal impacts, the authors could also discuss possible mitigation
731"
BROADER IMPACTS,0.9043570669500531,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
732"
BROADER IMPACTS,0.9054197662061636,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
733"
BROADER IMPACTS,0.9064824654622742,"feedback over time, improving the efficiency and accessibility of ML).
734"
SAFEGUARDS,0.9075451647183846,"11. Safeguards
735"
SAFEGUARDS,0.9086078639744952,"Question: Does the paper describe safeguards that have been put in place for responsible
736"
SAFEGUARDS,0.9096705632306057,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
737"
SAFEGUARDS,0.9107332624867163,"image generators, or scraped datasets)?
738"
SAFEGUARDS,0.9117959617428267,"Answer: [NA]
739"
SAFEGUARDS,0.9128586609989373,"Justification: [NA]
740"
SAFEGUARDS,0.9139213602550478,"Guidelines:
741"
SAFEGUARDS,0.9149840595111584,"• The answer NA means that the paper poses no such risks.
742"
SAFEGUARDS,0.9160467587672688,"• Released models that have a high risk for misuse or dual-use should be released with
743"
SAFEGUARDS,0.9171094580233794,"necessary safeguards to allow for controlled use of the model, for example by requiring
744"
SAFEGUARDS,0.9181721572794899,"that users adhere to usage guidelines or restrictions to access the model or implementing
745"
SAFEGUARDS,0.9192348565356004,"safety filters.
746"
SAFEGUARDS,0.9202975557917109,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
747"
SAFEGUARDS,0.9213602550478215,"should describe how they avoided releasing unsafe images.
748"
SAFEGUARDS,0.922422954303932,"• We recognize that providing effective safeguards is challenging, and many papers do
749"
SAFEGUARDS,0.9234856535600425,"not require this, but we encourage authors to take this into account and make a best
750"
SAFEGUARDS,0.924548352816153,"faith effort.
751"
LICENSES FOR EXISTING ASSETS,0.9256110520722636,"12. Licenses for existing assets
752"
LICENSES FOR EXISTING ASSETS,0.926673751328374,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
753"
LICENSES FOR EXISTING ASSETS,0.9277364505844846,"the paper, properly credited and are the license and terms of use explicitly mentioned and
754"
LICENSES FOR EXISTING ASSETS,0.9287991498405951,"properly respected?
755"
LICENSES FOR EXISTING ASSETS,0.9298618490967057,"Answer: [Yes]
756"
LICENSES FOR EXISTING ASSETS,0.9309245483528161,"Justification: [NA]
757"
LICENSES FOR EXISTING ASSETS,0.9319872476089267,"Guidelines:
758"
LICENSES FOR EXISTING ASSETS,0.9330499468650372,"• The answer NA means that the paper does not use existing assets.
759"
LICENSES FOR EXISTING ASSETS,0.9341126461211477,"• The authors should cite the original paper that produced the code package or dataset.
760"
LICENSES FOR EXISTING ASSETS,0.9351753453772582,"• The authors should state which version of the asset is used and, if possible, include a
761"
LICENSES FOR EXISTING ASSETS,0.9362380446333688,"URL.
762"
LICENSES FOR EXISTING ASSETS,0.9373007438894793,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
763"
LICENSES FOR EXISTING ASSETS,0.9383634431455898,"• For scraped data from a particular source (e.g., website), the copyright and terms of
764"
LICENSES FOR EXISTING ASSETS,0.9394261424017003,"service of that source should be provided.
765"
LICENSES FOR EXISTING ASSETS,0.9404888416578109,"• If assets are released, the license, copyright information, and terms of use in the
766"
LICENSES FOR EXISTING ASSETS,0.9415515409139213,"package should be provided. For popular datasets, paperswithcode.com/datasets
767"
LICENSES FOR EXISTING ASSETS,0.9426142401700319,"has curated licenses for some datasets. Their licensing guide can help determine the
768"
LICENSES FOR EXISTING ASSETS,0.9436769394261424,"license of a dataset.
769"
LICENSES FOR EXISTING ASSETS,0.944739638682253,"• For existing datasets that are re-packaged, both the original license and the license of
770"
LICENSES FOR EXISTING ASSETS,0.9458023379383634,"the derived asset (if it has changed) should be provided.
771"
LICENSES FOR EXISTING ASSETS,0.946865037194474,"• If this information is not available online, the authors are encouraged to reach out to
772"
LICENSES FOR EXISTING ASSETS,0.9479277364505845,"the asset’s creators.
773"
NEW ASSETS,0.9489904357066951,"13. New Assets
774"
NEW ASSETS,0.9500531349628055,"Question: Are new assets introduced in the paper well documented and is the documentation
775"
NEW ASSETS,0.9511158342189161,"provided alongside the assets?
776"
NEW ASSETS,0.9521785334750266,"Answer: [NA]
777"
NEW ASSETS,0.953241232731137,"Justification: [NA]
778"
NEW ASSETS,0.9543039319872476,"Guidelines:
779"
NEW ASSETS,0.9553666312433581,"• The answer NA means that the paper does not release new assets.
780"
NEW ASSETS,0.9564293304994687,"• Researchers should communicate the details of the dataset/code/model as part of
781"
NEW ASSETS,0.9574920297555791,"their submissions via regular templates. This includes details about training, license,
782"
NEW ASSETS,0.9585547290116897,"limitations, etc.
783"
NEW ASSETS,0.9596174282678002,"• The paper should discuss whether and how consent was obtained from people whose
784"
NEW ASSETS,0.9606801275239107,"asset is used.
785"
NEW ASSETS,0.9617428267800212,"• At submission time, remember to anonymize your assets (if applicable). You can either
786"
NEW ASSETS,0.9628055260361318,"create an anonymized URL or include an anonymized zip file.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9638682252922423,"14. Crowdsourcing and Research with Human Subjects
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9649309245483528,"Question: For crowdsourcing experiments and research with human subjects, does the paper
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9659936238044633,"include the full text of instructions given to participants and screenshots, if applicable, as
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670563230605739,"well as details about compensation (if any)?
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681190223166843,"Answer: [NA]
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691817215727949,"Justification: [NA]
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702444208289054,"Guidelines:
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971307120085016,"• The answer NA means that the paper does not involve crowdsourcing nor research with
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723698193411264,"human subjects.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973432518597237,"• Including this information in the supplemental material is fine, but if the main contribu-
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744952178533475,"tion of the paper involves human subjects, then as much detail as possible should be
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975557917109458,"included in the main paper.
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766206163655685,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776833156216791,"or other labor should be paid at least the minimum wage in the country of the data
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787460148777896,"collector.
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798087141339001,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808714133900106,"Subjects
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819341126461212,"Question: Does the paper describe potential risks incurred by study participants, whether
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829968119022316,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840595111583422,"approvals (or an equivalent approval/review based on the requirements of your country or
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851222104144527,"institution) were obtained?
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861849096705633,"Answer: [NA]
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872476089266737,"Justification: [NA]
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883103081827843,"Guidelines:
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893730074388948,"• The answer NA means that the paper does not involve crowdsourcing nor research with
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904357066950054,"human subjects.
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914984059511158,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925611052072264,"may be required for any human subjects research. If you obtained IRB approval, you
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936238044633369,"should clearly state this in the paper.
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946865037194474,"• We recognize that the procedures for this may vary significantly between institutions
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957492029755579,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968119022316685,"guidelines for their institution.
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997874601487779,"• For initial submissions, do not include any information that would break anonymity (if
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989373007438895,"applicable), such as the institution conducting the review.
821"
