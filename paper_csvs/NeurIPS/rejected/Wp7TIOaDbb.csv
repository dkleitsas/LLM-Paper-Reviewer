Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018975332068311196,"We propose the ﬁrst, to our knowledge, loss function for approximate Nash equi-
1"
ABSTRACT,0.003795066413662239,"libria of normal-form games that is amenable to unbiased Monte Carlo estimation.
2"
ABSTRACT,0.0056925996204933585,"This construction allows us to deploy standard non-convex stochastic optimiza-
3"
ABSTRACT,0.007590132827324478,"tion techniques for approximating Nash equilibria, resulting in novel algorithms
4"
ABSTRACT,0.009487666034155597,"with provable guarantees. We complement our theoretical analysis with exper-
5"
ABSTRACT,0.011385199240986717,"iments demonstrating that stochastic gradient descent can outperform previous
6"
ABSTRACT,0.013282732447817837,"state-of-the-art approaches.
7"
INTRODUCTION,0.015180265654648957,"1
Introduction
8"
INTRODUCTION,0.017077798861480076,"Nash equilibrium famously encodes stable behavioral outcomes in multi-agent systems and is arguably
9"
INTRODUCTION,0.018975332068311195,"the most inﬂuential solution concept in game theory. Formally speaking, if n players independently
10"
INTRODUCTION,0.020872865275142316,"choose n, possibly mixed, strategies (xi for i 2 [n]) and their joint strategy (x = Q"
INTRODUCTION,0.022770398481973434,"i xi) constitutes a
11"
INTRODUCTION,0.024667931688804556,"Nash equilibrium, then no player has any incentive to unilaterally deviate from their strategy. This
12"
INTRODUCTION,0.026565464895635674,"concept has sparked extensive research in various ﬁelds, ranging from economics [30] to machine
13"
INTRODUCTION,0.028462998102466792,"learning [16], and has even inspired behavioral theory generalizations such as quantal response
14"
INTRODUCTION,0.030360531309297913,"equilibria which allow for more realistic models of boundedly rational agents [28].
15"
INTRODUCTION,0.03225806451612903,"Unfortunately, when considering Nash equilibria beyond the special case of the 2-player, zero-sum
16"
INTRODUCTION,0.03415559772296015,"scenario, two signiﬁcant challenges arise. First, it becomes unclear how a group of n independent
17"
INTRODUCTION,0.036053130929791274,"players would collectively identify a Nash equilibrium when multiple equilibria are possible, giving
18"
INTRODUCTION,0.03795066413662239,"rise to the equilibrium selection problem [18]. Secondly, even approximating a single Nash equilib-
19"
INTRODUCTION,0.03984819734345351,"rium is known to be computationally intractable and speciﬁcally PPAD-complete [11]. Combining
20"
INTRODUCTION,0.04174573055028463,"both problems together, e.g., testing for the existence of equilibria with welfare greater than some
21"
INTRODUCTION,0.04364326375711575,"ﬁxed threshold is NP-hard and it is in fact even hard to approximate (i.e., ﬁnding a Nash equilibrium
22"
INTRODUCTION,0.04554079696394687,"with welfare greater than ! for any ! > 0, even when the best equilibrium has welfare 1 −!) [2].
23"
INTRODUCTION,0.04743833017077799,"From a machine learning (ML) practitioner’s perspective, however, such computational complexity
24"
INTRODUCTION,0.04933586337760911,"results hardly give pause for thought as collectively we have become all too familiar with the
25"
INTRODUCTION,0.051233396584440226,"unreasonable effectiveness of ML heuristics in circumventing such obstacles. Famously, non-convex
26"
INTRODUCTION,0.05313092979127135,"optimization is NP-hard, even if the goal is to compute a local minimizer [31], however, stochastic
27"
INTRODUCTION,0.05502846299810247,"gradient descent (and variants thereof) succeed in training models with billions of parameters [7].
28"
INTRODUCTION,0.056925996204933584,"Unfortunately, computational techniques for Nash equilibrium have so far not achieved anywhere
29"
INTRODUCTION,0.058823529411764705,"near the same level of success. In contrast, most modern Nash equilibrium solvers for n-player,
30"
INTRODUCTION,0.06072106261859583,"m-action, general-sum, normal-form games (NFGs) are practically restricted to a handful of players
31"
INTRODUCTION,0.06261859582542695,"and/or actions per player except in special cases (e.g., symmetric [38] or mean-ﬁeld games [34]). This
32"
INTRODUCTION,0.06451612903225806,"is partially due to the fact that an NFG is represented by a tensor with an exponential nmn entries;
33"
INTRODUCTION,0.06641366223908918,"even reading this description into memory can be computationally prohibitive. More to the point, any
34"
INTRODUCTION,0.0683111954459203,"computational technique that presumes exact computation of the expectation of any function sampled
35"
INTRODUCTION,0.07020872865275142,"according to x similarly does not have any hope of scaling beyond small instances.
36"
INTRODUCTION,0.07210626185958255,"This inefﬁciency arguably lies at the core of the differential success between ML optimization and
37"
INTRODUCTION,0.07400379506641366,"equilibrium computation. For example, numerous techniques exist that reduce the problem of Nash
38"
INTRODUCTION,0.07590132827324478,"equilibrium computation to ﬁnding the minimum of the expectation of a random variable (see related
39"
INTRODUCTION,0.0777988614800759,"work section). Unfortunately, unlike the source of randomness in ML applications where batch
40"
INTRODUCTION,0.07969639468690702,"learning sufﬁces to easily produce unbiased estimators, these techniques do not extend easily to game
41"
INTRODUCTION,0.08159392789373814,"theory which incorporates non-linear functions such as maximum, best-response amongst others.
42"
INTRODUCTION,0.08349146110056926,"This raises our motivating goal:
43"
INTRODUCTION,0.08538899430740038,Can we solve for Nash equilibria via unbiased stochastic optimization?
INTRODUCTION,0.0872865275142315,"Our results. Following in the successful steps of the interplay between ML and stochastic optimiza-
44"
INTRODUCTION,0.08918406072106262,"tion, we reformulate the approximation of Nash equilibria in an NFG as a stochastic non-convex
45"
INTRODUCTION,0.09108159392789374,"optimization problem admitting unbiased Monte-Carlo estimation. This enables the use of powerful
46"
INTRODUCTION,0.09297912713472485,"solvers and advances in parallel computing to efﬁciently enumerate Nash equilibria for n-player,
47"
INTRODUCTION,0.09487666034155598,"general-sum games. Furthermore, this re-casting allows practitioners to incorporate other desirable
48"
INTRODUCTION,0.0967741935483871,"objectives into the problem such as “ﬁnd an approximate Nash equilibrium with welfare above !”
49"
INTRODUCTION,0.09867172675521822,"or “ﬁnd an approximate Nash equilibrium nearest the current observed joint strategy” resolving the
50"
INTRODUCTION,0.10056925996204934,"equilibrium selection problem in effectively ad-hoc and application tailored manner. Concretely, we
51"
INTRODUCTION,0.10246679316888045,"make the following contributions by producing:
52"
INTRODUCTION,0.10436432637571158,"• A loss function L(x) 1) whose global minima coincide with interior Nash equilibria in normal
53"
INTRODUCTION,0.1062618595825427,"form games, 2) admits unbiased Monte-Carlo estimation, and 3) is Lipschitz and bounded.
54"
INTRODUCTION,0.10815939278937381,"• A loss function L⌧(x) 1) whose global minima coincide with logit equilibria (QREs) in normal
55"
INTRODUCTION,0.11005692599620494,"form games, 2) admits unbiased Monte-Carlo estimation, and 3) is Lipschitz and bounded.
56"
INTRODUCTION,0.11195445920303605,"• An efﬁcient randomized algorithm for approximating Nash equilibria in a novel class of games. The
57"
INTRODUCTION,0.11385199240986717,"algorithm emerges by employing a recent X-armed bandit approach to L⌧(x) and connecting its
58"
INTRODUCTION,0.1157495256166983,"stochastic optimization guarantees to approximate Nash guarantees. For large games, this enables
59"
INTRODUCTION,0.11764705882352941,"approximating equilibria faster than the game can even be read into memory.
60"
INTRODUCTION,0.11954459203036052,"• An empirical comparison of stochastic gradient descent against state-of-the-art baselines for
61"
INTRODUCTION,0.12144212523719165,"approximating NEs in large games. In some games, vanilla SGD actually improves upon previous
62"
INTRODUCTION,0.12333965844402277,"state-of-the-art; in others, SGD is slowed by saddle points, a familiar challenge in deep learning [12].
63"
INTRODUCTION,0.1252371916508539,"Overall, this perspective showcases a promising new route to approximating equilibria at scale in
64"
INTRODUCTION,0.127134724857685,"practice. We conclude the paper with discussion for future work.
65"
PRELIMINARIES,0.12903225806451613,"2
Preliminaries
66"
PRELIMINARIES,0.13092979127134724,"In an n-player, normal-form game, each player i 2 {1, . . . , n} has a strategy set Ai
=
67"
PRELIMINARIES,0.13282732447817835,"{ai1, . . . , aimi} consisting of mi pure strategies. These strategies can be naturally indexed, so
68"
PRELIMINARIES,0.1347248576850095,"we redeﬁne Ai = {1, . . . , mi} as an abuse of notation. Each player i also has a utility function,
69"
PRELIMINARIES,0.1366223908918406,ui : A = Q
PRELIMINARIES,0.13851992409867173,"i Ai ! [0, 1], (equiv. “payoff tensor”) that maps joint actions to payoffs in the unit-
70"
PRELIMINARIES,0.14041745730550284,"interval . Note that equilibria are invariant to payoff shift and scale [27] so we are effectively assuming
71"
PRELIMINARIES,0.14231499051233396,"we know bounds on possible payoffs. We denote the average cardinality of the players’ action sets
72"
PRELIMINARIES,0.1442125237191651,"by ¯m =
1
n P"
PRELIMINARIES,0.1461100569259962,"k mk and maximum by m⇤= maxk mk. Player i may play a mixed strategy by
73"
PRELIMINARIES,0.14800759013282733,"sampling from a distribution over their pure strategies. Let player i’s mixed strategy be represented
74"
PRELIMINARIES,0.14990512333965844,"by a vector xi 2 ∆mi−1 where ∆mi−1 is the (mi −1)-dimensional probability simplex embedded
75"
PRELIMINARIES,0.15180265654648956,in Rmi. Each function ui is then extended to this domain so that ui(x) = P
PRELIMINARIES,0.15370018975332067,a2A ui(a) Q
PRELIMINARIES,0.1555977229601518,"j xjaj
76"
PRELIMINARIES,0.15749525616698293,"where x = (x1, . . . , xn) and aj 2 Aj denotes player j’s component of the joint action a 2 A. For
77"
PRELIMINARIES,0.15939278937381404,"convenience, let x−i denote all components of x belonging to players other than player i.
78"
PRELIMINARIES,0.16129032258064516,The joint strategy x 2 Q
PRELIMINARIES,0.16318785578747627,"i ∆mi−1 is a Nash equilibrium if and only if, for all i 2 {1, . . . , n},
79"
PRELIMINARIES,0.1650853889943074,"ui(zi, x−i) ui(x) for all zi 2 ∆mi−1, i.e., no player has any incentive to unilaterally deviate from
80"
PRELIMINARIES,0.16698292220113853,"x. Nash is typically relaxed with ✏-Nash, our focus: ui(zi, x−i) ui(x) + ✏for all zi 2 ∆mi−1.
81"
PRELIMINARIES,0.16888045540796964,"As an abuse of notation, let the atomic action ai = ei also denote the mi-dimensional “one-hot"" vector
82"
PRELIMINARIES,0.17077798861480076,"with all zeros aside from a 1 at index ai; its use should be clear from the context. We also introduce
83"
PRELIMINARIES,0.17267552182163187,"Loss
Function
Obstacle
Exploitabilty
maxk ✏k(x)
max of r.v.
Nikaido-Isoda (NI)
P"
PRELIMINARIES,0.174573055028463,"k ✏k(x)
max of r.v.
Fully-Diff. Exp
P k P"
PRELIMINARIES,0.17647058823529413,"ak2Ak[max(0, uk(ak, x−i) −uk(x))]2
max of r.v."
PRELIMINARIES,0.17836812144212524,"Gradient-based NI
NI w/ BRk  aBRk = ⇧∆ ⇣"
PRELIMINARIES,0.18026565464895636,xk + ⌘rxkuk(x) ⌘
PRELIMINARIES,0.18216318785578747,"⇧∆of r.v.
Unconstrained
Loss + Simplex Deviation Penalty
sampling from xi 2 Rmk
Table 1: Previous loss functions for NFGs and their obstacles to unbiased estimation. ri"
PRELIMINARIES,0.1840607210626186,"xi as player i’s utility gradient. And for convenience, denote by Hi"
PRELIMINARIES,0.1859582542694497,"il = Ex−il[ui(ai, al, x−il)] the
84"
PRELIMINARIES,0.18785578747628084,"bimatrix game approximation [20] between players i and l with all other players marginalized out;
85"
PRELIMINARIES,0.18975332068311196,"x−il denotes all strategies belonging to players other than i and l and ui(ai, al, x−il) separates out l’s
86"
PRELIMINARIES,0.19165085388994307,"strategy xl from the rest of the players x−i. Similarly, denote by T i"
PRELIMINARIES,0.1935483870967742,"ilq = Ex−ilq[ui(ai, al, aq, x−ilq)]
87"
PRELIMINARIES,0.1954459203036053,"the 3-player tensor approximation to the game. Note player i’s utility can now be written succinctly
88"
PRELIMINARIES,0.19734345351043645,"as ui(xi, x−i) = x> i ri"
PRELIMINARIES,0.19924098671726756,xi = x> i Hi
PRELIMINARIES,0.20113851992409867,ilxl = xiT i
PRELIMINARIES,0.2030360531309298,"ilqxlxq for any l, q where we use Einstein notation for
89"
PRELIMINARIES,0.2049335863377609,"tensor arithmetic. For convenience, deﬁne diag(z) as the function that places a vector z on the
90"
PRELIMINARIES,0.20683111954459202,"diagonal of a square matrix, and diag3 : z 2 Rd ! Rd⇥d⇥d as a 3-tensor of shape (d, d, d) where
91"
PRELIMINARIES,0.20872865275142316,"diag3(z)iii = zi. Following convention from differential geometry, let TvM be the tangent space
92"
PRELIMINARIES,0.21062618595825428,"of a manifold M at v. For the interior of the d-action simplex ∆d−1, the tangent space is the same at
93"
PRELIMINARIES,0.2125237191650854,"every point, so we drop the v subscript, i.e., T∆d−1. We denote the projection of a vector z 2 Rd
94"
PRELIMINARIES,0.2144212523719165,onto this tangent space as ⇧T ∆d−1(z) = z −1
PRELIMINARIES,0.21631878557874762,"d1>z. We drop d when the dimensionality is clear
95"
PRELIMINARIES,0.21821631878557876,"from the context. Finally, let U(S) denote a discrete uniform distribution over elements from set S.
96"
RELATED WORK,0.22011385199240988,"3
Related Work
97"
RELATED WORK,0.222011385199241,"Representing the problem of computing a Nash equilibrium as an optimization problem is not new. A
98"
RELATED WORK,0.2239089184060721,"variety of loss functions and pseudo-distance functions have been proposed. Most of them measure
99"
RELATED WORK,0.22580645161290322,"some function of how much each player can exploit the joint strategy by unilaterally deviating:
100 ✏k(x)"
RELATED WORK,0.22770398481973433,"def
= uk(BRk, x−k) −uk(x) where BRk 2 arg max"
RELATED WORK,0.22960151802656548,"z
uk(z, x−k).
(1)"
RELATED WORK,0.2314990512333966,"As argued in the introduction, we believe it is important to be able to subsample payoff tensors of
101"
RELATED WORK,0.2333965844402277,"normal-form games in order to scale to large instances. As Nash equilibria can consist of mixed
102"
RELATED WORK,0.23529411764705882,"strategies, it is advantageous to be able to sample from an equilibrium to estimate its exploitability ✏.
103"
RELATED WORK,0.23719165085388993,"However none of these losses is amenable to unbiased estimation under sampled play. Each of the
104"
RELATED WORK,0.23908918406072105,"functions currently explored in the literature is biased under sampled play either because 1) a random
105"
RELATED WORK,0.2409867172675522,"variable appears as the argument of a complex, nonlinear (non-polynomial) function or because 2) how
106"
RELATED WORK,0.2428842504743833,"to sample play is unclear. Exploitability, Nikaido-Isoda (NI) [32] (also known by NashConv [21] and
107"
RELATED WORK,0.24478178368121442,"ADI [15]), as well as fully-differentiable options ([36], p. 106, Eqn 4.31) introduce bias when a max
108"
RELATED WORK,0.24667931688804554,"over payoffs is estimated using samples from x. Gradient-based NI [35] requires projecting the result
109"
RELATED WORK,0.24857685009487665,"of a gradient-ascent step onto the simplex; for the same reason as the max, this is prohibitive because
110"
RELATED WORK,0.2504743833017078,"it is a nonlinear operation which introduces bias. Lastly, unconstrained optimization approaches ([36],
111"
RELATED WORK,0.2523719165085389,"p. 106) that instead penalize deviation from the simplex lose the ability to sample from strategies
112"
RELATED WORK,0.25426944971537,"when iterates are no longer proper distributions. Table 1 summarizes these complications.
113"
NASH EQUILIBRIUM AS STOCHASTIC OPTIMIZATION,0.25616698292220114,"4
Nash Equilibrium as Stochastic Optimization
114"
NASH EQUILIBRIUM AS STOCHASTIC OPTIMIZATION,0.25806451612903225,"We will now develop our proposed loss function which is amenable to unbiased estimation. Our key
115"
NASH EQUILIBRIUM AS STOCHASTIC OPTIMIZATION,0.25996204933586337,"technical insight is to pay special attention to the geometry of the simplex. To our knowledge, prior
116"
NASH EQUILIBRIUM AS STOCHASTIC OPTIMIZATION,0.2618595825426945,"works have failed to recognize the role of the tangent space T∆. Proofs are in the appendix.
117"
STATIONARITY ON THE SIMPLEX INTERIOR,0.2637571157495256,"4.1
Stationarity on the Simplex Interior
118"
STATIONARITY ON THE SIMPLEX INTERIOR,0.2656546489563567,"Lemma 1. Assuming player i’s utility, ui(xi, x−i), is concave in its own strategy xi, a strategy in
119"
STATIONARITY ON THE SIMPLEX INTERIOR,0.2675521821631879,"the interior of the simplex is a best response BRi if and only if it has zero projected-gradient1 norm:
120"
STATIONARITY ON THE SIMPLEX INTERIOR,0.269449715370019,"1Not to be confused with the nonlinear (i.e., introduces bias) projected gradient operator introduced in [19]. BRi 2 %"
STATIONARITY ON THE SIMPLEX INTERIOR,0.2713472485768501,int∆\ arg max
STATIONARITY ON THE SIMPLEX INTERIOR,0.2732447817836812,"z
ui(z, x−i) −ui(xi, x−i &&"
STATIONARITY ON THE SIMPLEX INTERIOR,0.27514231499051234,() (BRi 2 int∆) ^ (||⇧T ∆[ri
STATIONARITY ON THE SIMPLEX INTERIOR,0.27703984819734345,BRi]|| = 0). (2)
STATIONARITY ON THE SIMPLEX INTERIOR,0.27893738140417457,"In NFGs, each player’s utility is linear in xi, thereby satisfying the concavity condition of Lemma 1.
121"
PROJECTED GRADIENT NORM AS LOSS,0.2808349146110057,"4.2
Projected Gradient Norm as Loss
122"
PROJECTED GRADIENT NORM AS LOSS,0.2827324478178368,"An equivalent description of a Nash equilibrium is a joint strategy x where every player’s strategy is
123"
PROJECTED GRADIENT NORM AS LOSS,0.2846299810246679,"a best response to the equilibrium (i.e., xi = BRi so that ✏i(x) = 0). Lemma 1 states that any interior
124"
PROJECTED GRADIENT NORM AS LOSS,0.286527514231499,"best response has zero projected-gradient norm, which inspires the following loss function
125"
PROJECTED GRADIENT NORM AS LOSS,0.2884250474383302,L(x) = X k
PROJECTED GRADIENT NORM AS LOSS,0.2903225806451613,⌘k||⇧T ∆(rk
PROJECTED GRADIENT NORM AS LOSS,0.2922201138519924,"xk)||2
(3)"
PROJECTED GRADIENT NORM AS LOSS,0.29411764705882354,"where ⌘k > 0 represent scalar weights, or equivalently, step sizes to be explained next.
126"
PROJECTED GRADIENT NORM AS LOSS,0.29601518026565465,"Proposition 1. The loss L is equivalent to NashConv, but where player k’s best response is approxi-
127"
PROJECTED GRADIENT NORM AS LOSS,0.29791271347248577,mated by a single step of projected-gradient ascent with step size ⌘k: aBRk = xk + ⌘k⇧T ∆(rk
PROJECTED GRADIENT NORM AS LOSS,0.2998102466793169,"xk).
128"
PROJECTED GRADIENT NORM AS LOSS,0.301707779886148,"This connection was already pointed out in prior work for unconstrained problems [15, 35], but this
129"
PROJECTED GRADIENT NORM AS LOSS,0.3036053130929791,"result is the ﬁrst for strategies constrained to the simplex.
130"
CONNECTION TO TRUE EXPLOITABILITY,0.3055028462998102,"4.3
Connection to True Exploitability
131"
CONNECTION TO TRUE EXPLOITABILITY,0.30740037950664134,"In general, we can bound exploitability in terms of the projected-gradient norm as long as each
132"
CONNECTION TO TRUE EXPLOITABILITY,0.3092979127134725,"player’s utility is concave (this result extends beyond gradients to subgradients of non-smooth
133"
CONNECTION TO TRUE EXPLOITABILITY,0.3111954459203036,"functions).
134"
CONNECTION TO TRUE EXPLOITABILITY,0.31309297912713474,"Lemma 2. The amount a player can gain by exploiting a joint strategy x is upper bounded by a
135"
CONNECTION TO TRUE EXPLOITABILITY,0.31499051233396586,"quantity proportional to the norm of the projected-gradient:
136"
CONNECTION TO TRUE EXPLOITABILITY,0.31688804554079697,✏k(x)  p
CONNECTION TO TRUE EXPLOITABILITY,0.3187855787476281,2||⇧T ∆(rk
CONNECTION TO TRUE EXPLOITABILITY,0.3206831119544592,"xk)||.
(4)"
CONNECTION TO TRUE EXPLOITABILITY,0.3225806451612903,"This bound is not tight on the boundary of the simplex, which can be seen clearly by considering xk
137"
CONNECTION TO TRUE EXPLOITABILITY,0.32447817836812143,"to be part of a pure strategy equilibrium. In that case, this analysis assumes xk can be improved upon
138"
CONNECTION TO TRUE EXPLOITABILITY,0.32637571157495254,"by a projected-gradient ascent step (via the equivalence pointed out in Proposition 1). However, that
139"
CONNECTION TO TRUE EXPLOITABILITY,0.32827324478178366,"is false because the probability of a pure strategy cannot be increased beyond 1. We mention this to
140"
CONNECTION TO TRUE EXPLOITABILITY,0.3301707779886148,"provide further intuition for why L(x) is only valid for interior equilibria.
141"
CONNECTION TO TRUE EXPLOITABILITY,0.33206831119544594,Note that ||⇧T ∆(rk
CONNECTION TO TRUE EXPLOITABILITY,0.33396584440227706,xk)|| ||rk
CONNECTION TO TRUE EXPLOITABILITY,0.33586337760910817,"xk|| because ⇧T ∆is a projection. Therefore, this improves the naive
142"
CONNECTION TO TRUE EXPLOITABILITY,0.3377609108159393,bounds on exploitability and distance to best responses given using the “raw” gradient rk
CONNECTION TO TRUE EXPLOITABILITY,0.3396584440227704,"xk.
143"
CONNECTION TO TRUE EXPLOITABILITY,0.3415559772296015,"Lemma 3. The exploitability of a joint strategy x, is upper bounded by a function of L(x):
144 ✏"
CONNECTION TO TRUE EXPLOITABILITY,0.34345351043643263,"r
2n
mink ⌘k p L(x)"
CONNECTION TO TRUE EXPLOITABILITY,0.34535104364326374,"def
= f(L).
(5)"
UNBIASED ESTIMATION,0.34724857685009486,"4.4
Unbiased Estimation
145"
UNBIASED ESTIMATION,0.349146110056926,"As discussed in Section 3, a primary obstacle to unbiased estimation of L(x) is the presence of
146"
UNBIASED ESTIMATION,0.3510436432637571,"complex, nonlinear functions of random variables, with the projection of a point onto the simplex
147"
UNBIASED ESTIMATION,0.35294117647058826,"being one such example (see ⇧∆in Table 1). However, ⇧T ∆, the projection onto the tangent space
148"
UNBIASED ESTIMATION,0.3548387096774194,"of the simplex, is linear! This is the key that allows us to design an unbiased estimator (Lemma 5).
149"
UNBIASED ESTIMATION,0.3567362428842505,"Our proposed loss requires computing the squared norm of the expected value of the gradient
150"
UNBIASED ESTIMATION,0.3586337760910816,"under the players’ mixed strategies, i.e., the l-th entry of player k’s gradient equals rk"
UNBIASED ESTIMATION,0.3605313092979127,"xkl =
151"
UNBIASED ESTIMATION,0.36242884250474383,"Ea−k⇠x−kuk(akl, a−k). By analogy, consider a random variable Y . In general, E[Y ]2 6= E[Y 2].
152"
UNBIASED ESTIMATION,0.36432637571157495,"This means that we cannot just sample projected-gradients and then compute their average norm to
153"
UNBIASED ESTIMATION,0.36622390891840606,"estimate our loss. However, consider taking two independent samples from two corresponding identi-
154"
UNBIASED ESTIMATION,0.3681214421252372,"cally distributed, independent random variables Y (1) and Y (2). Then E[Y (1)]2 = E[Y (1)]E[Y (2)] =
155"
UNBIASED ESTIMATION,0.3700189753320683,"Exact
Sample Others
Sample All
Estimator of rk(p)"
UNBIASED ESTIMATION,0.3719165085388994,"xk
uk(akl, x−k)
uk(akl, a−k ⇠x−k)
mkuk(akl ⇠U(Ak), a−k ⇠x−k)el
ˆrk(p)"
UNBIASED ESTIMATION,0.3738140417457306,"xk
Bounds
[0, 1]
[0, 1]
[0, mk]
ˆrk(p)"
UNBIASED ESTIMATION,0.3757115749525617,"xk
Query Cost
Qn"
UNBIASED ESTIMATION,0.3776091081593928,"i=1 mi
mk
1
L Bounds
± 1 4 P"
UNBIASED ESTIMATION,0.3795066413662239,"k ⌘kmk
± 1 4 P"
UNBIASED ESTIMATION,0.38140417457305503,"k ⌘kmk
± 1 4 P"
UNBIASED ESTIMATION,0.38330170777988615,k ⌘km3
UNBIASED ESTIMATION,0.38519924098671726,"k
L Query Cost
n Qn"
UNBIASED ESTIMATION,0.3870967741935484,"i=1 mi
2n ¯m
2n
Table 2: Examples and Properties of Unbiased Estimators of Loss and Player Gradients ( ˆrk(p) xk )."
UNBIASED ESTIMATION,0.3889943074003795,"E[Y (1)Y (2)] by properties of expected value over products of independent random variables. This is
156"
UNBIASED ESTIMATION,0.3908918406072106,"a common technique to construct unbiased estimates of expectations over polynomial functions of
157"
UNBIASED ESTIMATION,0.3927893738140417,"random variables. Proceeding in this way, deﬁne rk(1)"
UNBIASED ESTIMATION,0.3946869070208729,"xk
as a random variable distributed according to
158"
UNBIASED ESTIMATION,0.396584440227704,the distribution induced by all other players’ mixed strategies (j 6= k). Let rk(2)
UNBIASED ESTIMATION,0.3984819734345351,"xk
be independent and
159"
UNBIASED ESTIMATION,0.40037950664136623,distributed identically to rk(1)
UNBIASED ESTIMATION,0.40227703984819735,"xk . Then
160"
UNBIASED ESTIMATION,0.40417457305502846,L(x) = E[ X k
UNBIASED ESTIMATION,0.4060721062618596,"⌘k( ˆrk(1) xk
−1 mk"
UNBIASED ESTIMATION,0.4079696394686907,(1> ˆrk(1)
UNBIASED ESTIMATION,0.4098671726755218,"xk )1
|
{z
}
projected-gradient 1"
UNBIASED ESTIMATION,0.4117647058823529,")>( ˆrk(2) xk
−1 mk"
UNBIASED ESTIMATION,0.41366223908918404,(1> ˆrk(2)
UNBIASED ESTIMATION,0.4155597722960152,"xk )1
|
{z
}
projected-gradient 2"
UNBIASED ESTIMATION,0.4174573055028463,")]
(6)"
UNBIASED ESTIMATION,0.41935483870967744,where ˆrk(p)
UNBIASED ESTIMATION,0.42125237191650855,"xk
is an unbiased estimator of player k’s gradient. This unbiased estimator can be con-
161"
UNBIASED ESTIMATION,0.42314990512333966,"structed in several ways. The most expensive, an exact estimator, is constructed by marginalizing
162"
UNBIASED ESTIMATION,0.4250474383301708,"player k’s payoff tensor over all other players’ strategies. However, a cheaper estimate can be obtained
163"
UNBIASED ESTIMATION,0.4269449715370019,"at the expense of higher variance by approximating this marginalization with a Monte Carlo estimate
164"
UNBIASED ESTIMATION,0.428842504743833,"of the expectation. Speciﬁcally, if we sample a single action for each of the remaining players, we
165"
UNBIASED ESTIMATION,0.4307400379506641,"can construct an unbiased estimate of player k’s gradient by considering the payoff of each of its
166"
UNBIASED ESTIMATION,0.43263757115749524,"actions against the sampled background strategy. Lastly, we can consider constructing a Monte Carlo
167"
UNBIASED ESTIMATION,0.43453510436432635,"estimate of player k’s gradient by sampling only a single action from player k to represent their entire
168"
UNBIASED ESTIMATION,0.4364326375711575,"gradient. Each of these approaches is outlined in Table 2 along with the query complexity [3] of
169"
UNBIASED ESTIMATION,0.43833017077798864,"computing the estimator and bounds on the values it can take (derived via Lemma 19).
170"
UNBIASED ESTIMATION,0.44022770398481975,"We can extend Lemma 3 to one that holds under T samples with probability 1 −δ by applying, for
171"
UNBIASED ESTIMATION,0.44212523719165087,"example, a Hoeffding bound: ✏f"
UNBIASED ESTIMATION,0.444022770398482,% ˆL(x) + O( q
UNBIASED ESTIMATION,0.4459203036053131,"1
T ln(1/δ) & .
172"
INTERIOR EQUILIBRIA,0.4478178368121442,"4.5
Interior Equilibria
173"
INTERIOR EQUILIBRIA,0.4497153700189753,"We discussed earlier that L(x) captures interior equilibria. But some games may only have pure
174"
INTERIOR EQUILIBRIA,0.45161290322580644,"equilibria. We show how to circumvent this shortcoming by considering quantal response equilibria
175"
INTERIOR EQUILIBRIA,0.45351043643263755,"(QREs), speciﬁcally, logit equilibria. By adding an entropy bonus to each player’s utility, we can
176"
INTERIOR EQUILIBRIA,0.45540796963946867,"• guarantee all equilibria are interior,
177"
INTERIOR EQUILIBRIA,0.4573055028462998,"• still obtain unbiased estimates of our loss,
178"
INTERIOR EQUILIBRIA,0.45920303605313095,"• maintain an upper bound on the exploitability ✏of any approximate equilibrium in the
179"
INTERIOR EQUILIBRIA,0.46110056925996207,"original game (i.e., the game without an entropy bonus).
180"
INTERIOR EQUILIBRIA,0.4629981024667932,Deﬁne u⌧
INTERIOR EQUILIBRIA,0.4648956356736243,k(x) = uk(x) + ⌧S(xk) where the Shannon entropy S(xk) = −P
INTERIOR EQUILIBRIA,0.4667931688804554,"l xkl ln(xkl) is a 1-
181"
INTERIOR EQUILIBRIA,0.4686907020872865,"strongly concave function with respect to the 1-norm [6]. Also deﬁne L⌧(x) as before except where
182 rk"
INTERIOR EQUILIBRIA,0.47058823529411764,xk is replaced with rk⌧
INTERIOR EQUILIBRIA,0.47248576850094876,xk = rxku⌧
INTERIOR EQUILIBRIA,0.47438330170777987,"k(x), i.e., the gradient of player k’s utility with the entropy bonus.
183"
INTERIOR EQUILIBRIA,0.476280834914611,"It is well known that Nash equilibria of entropy-regularized games satisfy the conditions for logit
184"
INTERIOR EQUILIBRIA,0.4781783681214421,"equilibria [23], which are solutions to the ﬁxed point equation xk = softmax( rk"
INTERIOR EQUILIBRIA,0.48007590132827327,"xk
⌧). The appearance
185"
INTERIOR EQUILIBRIA,0.4819734345351044,"of the softmax makes clear that all probabilities have positive mass at positive temperature.
186"
INTERIOR EQUILIBRIA,0.4838709677419355,"Recall that in order to construct an unbiased estimate of our loss, we simply needed to construct
187"
INTERIOR EQUILIBRIA,0.4857685009487666,"unbiased estimates of player gradients. The introduction of the entropy term to player k’s utility is
188"
INTERIOR EQUILIBRIA,0.4876660341555977,"special in that it depends entirely on known quantities, i.e., the player’s own mixed strategy. We
189"
INTERIOR EQUILIBRIA,0.48956356736242884,can directly and deterministically compute ⌧dS
INTERIOR EQUILIBRIA,0.49146110056925996,"dxk = −⌧(ln(xk) + 1) and add this to our estimator of
190 rk(p)"
INTERIOR EQUILIBRIA,0.49335863377609107,xk : ˆrk⌧(p)
INTERIOR EQUILIBRIA,0.4952561669829222,"xk
= ˆrk(p)"
INTERIOR EQUILIBRIA,0.4971537001897533,"xk
+ ⌧dS"
INTERIOR EQUILIBRIA,0.4990512333965844,"dxk . Consider our reﬁned loss function with changes in blue:
191"
INTERIOR EQUILIBRIA,0.5009487666034156,"Figure 1: Upper Bound (✏f(L⌧)) Heatmap Visualization. The ﬁrst row examines the loss land-
scape for the classic anti-coordination game of Chicken (Nash equilibria: (0, 1), (1, 0), (2/3, 1/3))
while the second row examines the Prisoner’s dilemma (Unique Nash equilibrium: (0, 0)). Tem-
perature increases for each plot moving to the right. For high temperatures, interior (fully-mixed)
strategies are incentivized while for lower temperatures, nearly pure strategies can achieve minimum
exploitability. For zero temperature, pure strategy equilibria (e.g., defect-defect) are not captured by
the loss as illustrated by the bottom-left Prisoner’s Dilemma plot with a constant loss surface."
INTERIOR EQUILIBRIA,0.5028462998102466,L⌧(x) = X k
INTERIOR EQUILIBRIA,0.5047438330170778,⌘k||⇧T ∆(rk⌧
INTERIOR EQUILIBRIA,0.5066413662239089,"xk)||2.
(7)"
INTERIOR EQUILIBRIA,0.50853889943074,"As mentioned above, the utilities with entropy bonuses are still concave, therefore, a similar bound
192"
INTERIOR EQUILIBRIA,0.5104364326375711,"to Lemma 2 applies. We use this to prove the QRE counterpart to Lemma 3 where ✏QRE is the
193"
INTERIOR EQUILIBRIA,0.5123339658444023,"exploitability of an approximate equilibrium in a game with entropy bonuses.
194"
INTERIOR EQUILIBRIA,0.5142314990512334,"Lemma 4. The entropy regularized exploitability, ✏QRE, of a joint strategy x, is upper bounded as:
195"
INTERIOR EQUILIBRIA,0.5161290322580645,✏QRE 
INTERIOR EQUILIBRIA,0.5180265654648957,"r
2n
mink ⌘k p L⌧(x)"
INTERIOR EQUILIBRIA,0.5199240986717267,"def
= f(L⌧).
(8)"
INTERIOR EQUILIBRIA,0.5218216318785579,"Lastly, we establish a connection between quantal response equilibria and Nash equilibria that allows
196"
INTERIOR EQUILIBRIA,0.523719165085389,"us to approximate Nash equilibria in the original game via minimizing our modiﬁed loss L⌧(x).
197"
INTERIOR EQUILIBRIA,0.5256166982922201,"Lemma 14 (L⌧Scores Nash Equilibria). Let L⌧(x) be our proposed entropy regularized loss
198"
INTERIOR EQUILIBRIA,0.5275142314990512,"function with payoffs bounded in [0, 1] and x be an approximate QRE. Then it holds that
199"
INTERIOR EQUILIBRIA,0.5294117647058824,✏n⌧(W(1/e) + ¯m −2
INTERIOR EQUILIBRIA,0.5313092979127134,"e
) + 2"
INTERIOR EQUILIBRIA,0.5332068311195446,rn maxk mk
INTERIOR EQUILIBRIA,0.5351043643263758,mink ⌘k p
INTERIOR EQUILIBRIA,0.5370018975332068,"L⌧(x)
(9)"
INTERIOR EQUILIBRIA,0.538899430740038,"where W is the Lambert function: W(1/e) = W(exp(−1)) ⇡0.278.
200"
INTERIOR EQUILIBRIA,0.540796963946869,"This upper bound is plotted as a heatmap for familiar games in Figure 1. Notice how pure equilibria
201"
INTERIOR EQUILIBRIA,0.5426944971537002,"are not visible as minima for zero temperature, but appear for slightly warmer temperatures.
202"
ANALYSIS,0.5445920303605313,"5
Analysis
203"
ANALYSIS,0.5464895635673624,"In the preceding section we established a loss function that upper bounds the exploitability of an
204"
ANALYSIS,0.5483870967741935,"approximate equilibrium. In addition, the zeros of this loss function have a one-to-one correspondence
205"
ANALYSIS,0.5502846299810247,"with quantal response equilibria (which approximate Nash equilibria at low temperature).
206"
ANALYSIS,0.5521821631878557,"Here, we derive properties that suggest it is “easy” to optimize. While this function is generally
207"
ANALYSIS,0.5540796963946869,"non-convex and may suffer from a proliferation of saddle points and local maxima (Figure 2) , it is
208"
ANALYSIS,0.5559772296015181,"Lipschitz continuous (over a subset of the interior) and bounded. These are two commonly made
209"
ANALYSIS,0.5578747628083491,"assumptions in the literature on non-convex optimization, which we leverage in Section 6. In addition,
210"
ANALYSIS,0.5597722960151803,"we can derive its gradient, its Hessian, and characterize its behavior around global minima.
211"
ANALYSIS,0.5616698292220114,"Figure 2: We reapply the analysis of [12], originally designed to understand the success of SGD in
deep learning, to “slices” of several popular extensive form games. To construct a slice (or meta-
game), we randomly sample 6 deterministic policies and then consider the corresponding n-player,
6-action normal-form game at ⌧= 0.1 (with payoffs normalized to [0, 1]). The index of a critical
point xc (rxL⌧(xc) = 0) indicates the fraction of negative eigenvalues in the Hessian of L⌧at xc;
↵= 0 indicates a local minimum, 1 a maximum, else a saddle point. We see a positive correlation
between exploitability and ↵indicating a lower prevalence of local minima at high exploitability."
ANALYSIS,0.5635673624288425,"Lemma 15. The gradient of L⌧(x) with respect to player l’s strategy xl is
212"
ANALYSIS,0.5654648956356736,rxlL⌧(x) = 2 X k ⌘kB>
ANALYSIS,0.5673624288425048,kl⇧T ∆(rk⌧
ANALYSIS,0.5692599620493358,"xk)
(10)"
ANALYSIS,0.571157495256167,"where Bll = −⌧[I −
1
ml 11>]diag( 1"
ANALYSIS,0.573055028462998,"xl ) and Bkl = [I −
1
mk 11>]Hk"
ANALYSIS,0.5749525616698292,"kl for k 6= l.
213"
ANALYSIS,0.5768500948766604,"Lemma 17. The Hessian of L⌧(x) can be written
214"
ANALYSIS,0.5787476280834914,Hess(L⌧) = 2
ANALYSIS,0.5806451612903226,⇥˜B> ˜B + T⇧T ∆( ˜r⌧) ⇤ (11)
ANALYSIS,0.5825426944971537,"where ˜Bkl = p⌘kBkl, ⇧T ∆( ˜r⌧) = [⌘1⇧T ∆(r1⌧"
ANALYSIS,0.5844402277039848,"x1), . . . , ⌘n⇧T ∆(rn⌧"
ANALYSIS,0.5863377609108159,"xn)], and we augment T (the
215"
ANALYSIS,0.5882352941176471,"3-player approximation to the game, T k"
ANALYSIS,0.5901328273244781,lqk) so that T l
ANALYSIS,0.5920303605313093,lll = ⌧diag3( 1 x2
ANALYSIS,0.5939278937381404,"l ).
216"
ANALYSIS,0.5958254269449715,"At an equilibrium, the latter term disappears because ⇧T ∆(rk⌧"
ANALYSIS,0.5977229601518027,"xk) = 0 for all k (Lemma 1). If X
217"
ANALYSIS,0.5996204933586338,"was Rn ¯m, then we could simply check if ˜B is full-rank to determine if Hess ≻0. However, X is a
218"
ANALYSIS,0.6015180265654649,"simplex product, and we only care about curvature in directions toward which we can update our
219"
ANALYSIS,0.603415559772296,"equilibrium. Toward that end, deﬁne M to be the n( ¯m + 1) ⇥n ¯m matrix that stacks ˜B on top of a
220"
ANALYSIS,0.6053130929791272,"repeated identity matrix that encodes orthogonality to the simplex:
221"
ANALYSIS,0.6072106261859582,M(x) = 2
ANALYSIS,0.6091081593927894,666666664
ANALYSIS,0.6110056925996205,−⌧p⌘1⇧T ∆( 1
ANALYSIS,0.6129032258064516,"x1 )
p⌘1⇧T ∆(H1"
ANALYSIS,0.6148007590132827,"12)
. . .
p⌘1⇧T ∆(H1"
ANALYSIS,0.6166982922201139,"1n)
...
...
...
...
p⌘n⇧T ∆(Hn"
ANALYSIS,0.618595825426945,"n1)
. . .
p⌘n⇧T ∆(Hn"
ANALYSIS,0.6204933586337761,"n,n−1)
−⌧p⌘n⇧T ∆( 1"
ANALYSIS,0.6223908918406073,"xn )
1>"
ANALYSIS,0.6242884250474383,"1
0
. . .
0
...
...
...
...
0
. . .
0
1> n 3"
ANALYSIS,0.6261859582542695,777777775 (12)
ANALYSIS,0.6280834914611005,where ⇧T ∆(z 2 Ra⇥b) = [Ia −1 a1a1>
ANALYSIS,0.6299810246679317,"a ]z subtracts the mean from each column of z and
1
xi is
222"
ANALYSIS,0.6318785578747628,shorthand for diag( 1
ANALYSIS,0.6337760910815939,"xi ). If M(x)z = 0 for a nonzero vector z 2 Rn ¯m, this implies there exists a z
223"
ANALYSIS,0.635673624288425,"that 1) is orthogonal to the ones vectors of each simplex (i.e., is a valid equilibrium update direction)
224"
ANALYSIS,0.6375711574952562,"and 2) achieves zero curvature in the direction z, i.e., z>( ˜B> ˜B)z = z>(Hess)z = 0, and so Hess
225"
ANALYSIS,0.6394686907020873,"is not positive deﬁnite. Conversely, if M(x) is of rank n ¯m for a quantal response equilibrium x, then
226"
ANALYSIS,0.6413662239089184,the Hessian of L⌧at x in the tangent space of the simplex product (X = Q
ANALYSIS,0.6432637571157496,"i Xi) is positive deﬁnite.
227"
ANALYSIS,0.6451612903225806,"In this case, we call x well-isolated because it implies it is not connected to any other equilibria.
228"
ANALYSIS,0.6470588235294118,"By analyzing the rank of M, we can conﬁrm that many classical matrix games including Rock-
229"
ANALYSIS,0.6489563567362429,"Paper-Scissors, Chicken, Matching Pennies, and Shapley’s game all induce strongly convex L⌧’s at
230"
ANALYSIS,0.650853889943074,"zero temperature (i.e., they have unique mixed Nash equilibria). In contrast, a game like Prisoner’s
231"
ANALYSIS,0.6527514231499051,"Dilemma has a unique pure strategy that will not be captured by our loss at zero temperature.
232"
ANALYSIS,0.6546489563567363,"Figure 3: Comparison of SGD on L⌧=0 against baselines on four games evaluated in [15]. From left
to right: 2-player, 3-action, nonsymmetric; 6-player, 5-action, nonsymmetric; 4-player, 66-action,
symmetric; 3-player, 286-action, symmetric. SGD struggles at saddle points in Blotto."
ALGORITHMS,0.6565464895635673,"6
Algorithms
233"
ALGORITHMS,0.6584440227703985,"We have formally transformed the approximation of Nash equilibria in NFGs into a stochastic
234"
ALGORITHMS,0.6603415559772297,"optimization problem. To our knowledge, this is the ﬁrst such formulation that allows one-shot
235"
ALGORITHMS,0.6622390891840607,"unbiased Monte-Carlo estimation which is critical to introduce the use of powerful algorithms capable
236"
ALGORITHMS,0.6641366223908919,"of solving high dimensional optimization problems. We explore two off-the-shelf approaches.
237"
ALGORITHMS,0.6660341555977229,"Stochastic gradient descent is the workhorse of high-dimensional stochastic optimization. It comes
238"
ALGORITHMS,0.6679316888045541,"with guaranteed convergence to stationary points [10], however, it may converge to local, rather than
239"
ALGORITHMS,0.6698292220113852,"global minima. It also enjoys implicit gradient regularization [4], seeking “ﬂat” minima and performs
240"
ALGORITHMS,0.6717267552182163,"approximate Bayesian inference [26]. Despite the lack of global convergence guarantee, in the next
241"
ALGORITHMS,0.6736242884250474,"section, we ﬁnd it performs well empirically in games previously examined by the literature.
242"
ALGORITHMS,0.6755218216318786,"We explore one other algorithmic approach to non-convex optimization based on minimizing regret,
243"
ALGORITHMS,0.6774193548387096,"which enjoys ﬁnite time convergence rates. X-armed bandits [8] systematically explore the space of
244"
ALGORITHMS,0.6793168880455408,"solutions by reﬁning a mesh over the joint strategy space, trading off exploration versus exploitation
245"
ALGORITHMS,0.681214421252372,"of promising regions.2 Several approaches exist [5, 37] with open source implementations (e.g., [24]).
246"
ALGORITHMS,0.683111954459203,"6.1
High Probability, Polynomial Convergence Rates
247"
ALGORITHMS,0.6850094876660342,"We use a recent X-armed bandit approach called BLiN [14] to establish a high probability ˜O(T −1/4)
248"
ALGORITHMS,0.6869070208728653,"convergence rate to Nash equilibria in n-player, general-sum games under mild assumptions. The
249"
ALGORITHMS,0.6888045540796964,"quality of this approximation improves as ⌧! 0, at the same time increasing the constant on the
250"
ALGORITHMS,0.6907020872865275,convergence rate via the Lipschitz constant p
ALGORITHMS,0.6925996204933587,"ˆL deﬁned below. For clarity, we assume users provide
251"
ALGORITHMS,0.6944971537001897,"a temperature in the form ⌧=
1
ln(1/p) with p 2 (0, 1) which ensures all equilibria have probability
252"
ALGORITHMS,0.6963946869070209,"mass greater than
p
m⇤for all actions (Lemma 9). Lower p corresponds with lower temperature.
253"
ALGORITHMS,0.698292220113852,"The following convergence rate depends on bounds on the exploitability in terms of the loss
254"
ALGORITHMS,0.7001897533206831,"(Lemma 14), bounds on the magnitude of estimates of the loss (Lemma 8), Lipschitz bounds on the
255"
ALGORITHMS,0.7020872865275142,"inﬁnity norm of the gradient (Corollary 2), and the number of distinct strategies (n ¯m = P"
ALGORITHMS,0.7039848197343453,"k mk).
256"
ALGORITHMS,0.7058823529411765,"Theorem 1 (BLiN PAC Rate). Assume ⌘k = ⌘= 2/ˆL, ⌧=
1
ln(1/p), and a previously pulled arm is
257"
ALGORITHMS,0.7077798861480076,"returned uniformly at random (i.e., t ⇠U([T])). Then for any w > 0
258 ✏t w"
ALGORITHMS,0.7096774193548387,"h
n
ln(1/p) %"
ALGORITHMS,0.7115749525616698,W(1/e) + ¯m −2 e &
ALGORITHMS,0.713472485768501,+ 4(1 + (4c2)1/3) p nm⇤ˆL ⇣ln T T
ALGORITHMS,0.715370018975332,"⌘
1
2(dz+2) i (13)"
ALGORITHMS,0.7172675521821632,"with probability (1 −w−1)(1 −2T −2) where W is the Lambert function (W(1/e) ⇡0.278),
259"
ALGORITHMS,0.7191650853889943,"m⇤= maxk mk, c 1 4 n ¯m ˆL ⇣"
ALGORITHMS,0.7210626185958254,"ln(m⇤)
ln(1/p) + 2 ⌘2 1 4 ⇣"
ALGORITHMS,0.7229601518026565,"ln(m⇤)
ln(1/p) + 2 ⌘"
ALGORITHMS,0.7248576850094877,"upper bounds the range of stochastic
260"
ALGORITHMS,0.7267552182163188,"estimates of L⌧(see Lemma 8), and ˆL = ⇣"
ALGORITHMS,0.7286527514231499,"ln(m⇤)
ln(1/p) + 2 ⌘⇣"
ALGORITHMS,0.7305502846299811,"m⇤2
p ln(1/p) + n ¯m ⌘"
ALGORITHMS,0.7324478178368121,"(see Corollary 2).
261"
ALGORITHMS,0.7343453510436433,This result depends on the near-optimality [37] or zooming-dimension dz = n ¯m( ↵hi−↵lo
ALGORITHMS,0.7362428842504743,"↵lo↵hi ) 2 [0, 1)
262"
ALGORITHMS,0.7381404174573055,"(Theorem 2) where ↵lo and ↵hi denote the degree of the polynomials that lower and upper bound the
263"
ALGORITHMS,0.7400379506641366,"function L⌧◦s locally around an equilibrium. For example, in the case where the Hessian is positive
264"
ALGORITHMS,0.7419354838709677,"deﬁnite, ↵lo = ↵hi = 2 and dz = 0. Here, s : [0, 1]n( ¯m−1) ! Q"
ALGORITHMS,0.7438330170777988,"i ∆mi−1 is any function that maps
265"
ALGORITHMS,0.74573055028463,"from the unit hypercube to a product of simplices; we analyze two such maps in the appendix.
266"
ALGORITHMS,0.7476280834914611,2Zhou et al. [39] developed a similar approach but only for pure Nash equilibria.
ALGORITHMS,0.7495256166982922,"Figure 4: Bandit-based (BLiN) Nash solver applied to an artiﬁcial 7-player, symmetric, 2-action
game. We search for a symmetric equilibrium, which is represented succinctly as the probability of
selecting action 1. The plot shows the true exploitability ✏of all symmetric strategies in black and
indicates there exist potentially 5 NEs (the dips in the curve). Upper bounds on our unregularized
loss L capture 4 of these equilibria, missing only the pure NE on the right. By considering our
regularized loss, L⌧, we are able to capture this pure NE (see zoomed inset). The bandit algorithm
selects strategies to evaluate, using 10 Monte-Carlo samples for each evaluation (arm pull) of L⌧.
These samples are displayed as vertical bars above with the height of the vertical bar representing
additional arm pulls. The best arms throughout search are denoted by green circles (darker indicates
later in the search). The boxed numbers near equilibria display the welfare of the strategy."
ALGORITHMS,0.7514231499051234,"Note that Theorem 1 implies that for games whose corresponding L⌧has zooming dimension dz = 0,
267"
ALGORITHMS,0.7533206831119544,"NEs can be approximated with high probability in polynomial time. This general property is difﬁcult
268"
ALGORITHMS,0.7552182163187856,"to translate concisely into game theory parlance. For this reason, we present the following more
269"
ALGORITHMS,0.7571157495256167,"interpretable corollary which applies to a more restricted class of games.
270"
ALGORITHMS,0.7590132827324478,"Corollary 1. Consider the class of NFGs with at least one QRE(⌧) whose local polymatrix approx-
271"
ALGORITHMS,0.7609108159392789,"imation indicates it is isolated (i.e., M from equation (12) is rank-n ¯m implies Hess ≻0 implies
272"
ALGORITHMS,0.7628083491461101,dz = n ¯m( 2−2
ALGORITHMS,0.7647058823529411,"4 ) = 0). Then by Theorem 1, BLiN is a fully polynomial-time randomized approximation
273"
ALGORITHMS,0.7666034155597723,"scheme (FPRAS) for QREs and is a PRAS for NEs of games in this class.
274"
ALGORITHMS,0.7685009487666035,"To convey the impact of stochastic optimization guarantees more concretely, assume we are given
275"
ALGORITHMS,0.7703984819734345,"that an interior well-isolated NE exists. Then for a 20-player, 50-action game, it is 1000⇥cheaper to
276"
ALGORITHMS,0.7722960151802657,"compute a 1/100-NE with probability 95% than it is to just list the nmn payoffs that deﬁne the game.
277"
EMPIRICAL EVALUATION,0.7741935483870968,"6.2
Empirical Evaluation
278"
EMPIRICAL EVALUATION,0.7760910815939279,"Figure 3 shows SGD is competitive with scalable techniques to approximating NEs. Shapley’s game
279"
EMPIRICAL EVALUATION,0.777988614800759,"induces a strongly convex L (see Section 5) leading to SGD’s strong performance. Blotto shows
280"
EMPIRICAL EVALUATION,0.7798861480075902,"signs of convergence to low, but nonzero ✏, demonstrating the challenges of local minima.
281"
EMPIRICAL EVALUATION,0.7817836812144212,"We demonstrate BLiN (applied to L⌧) on a 7-player, symmetric, 2-action game. Figure 4 shows the
282"
EMPIRICAL EVALUATION,0.7836812144212524,"bandit algorithm discovers two equilibria, settling on one near x = [0.7, 0.3] ⇥7 with a wider basin
283"
EMPIRICAL EVALUATION,0.7855787476280834,"of attraction (and higher welfare). In theory, BLiN can enumerate all NEs as T ! 1.
284"
CONCLUSION,0.7874762808349146,"7
Conclusion
285"
CONCLUSION,0.7893738140417458,"In this work, we proposed a stochastic loss for approximate Nash equilibria in normal-form games.
286"
CONCLUSION,0.7912713472485768,"An unbiased loss estimator of Nash equilibria is the “key” to the stochastic optimization “door”
287"
CONCLUSION,0.793168880455408,"which holds a wealth of research innovations uncovered over several decades. Thus, it allows the
288"
CONCLUSION,0.7950664136622391,"development of new algorithmic techniques for computing equilibria. We consider bandit and vanilla
289"
CONCLUSION,0.7969639468690702,"SGD methods in this work, but theses are only two of the many options now at our disposal (e.g,
290"
CONCLUSION,0.7988614800759013,"adaptive methods [1], Gaussian processes [9], evolutionary algorithms [17], etc.). Such approaches as
291"
CONCLUSION,0.8007590132827325,"well as generalizations of these techniques to imperfect-information games are promising directions
292"
CONCLUSION,0.8026565464895635,"for future work. Similarly to how deep learning research ﬁrst balked at and then marched on to train
293"
CONCLUSION,0.8045540796963947,"neural networks via NP-hard non-convex optimization, we hope computational game theory can
294"
CONCLUSION,0.8064516129032258,"march ahead to make useful equilibrium predictions of large multiplayer systems.
295"
REFERENCES,0.8083491461100569,"References
296"
REFERENCES,0.8102466793168881,"[1] K. Antonakopoulos, P. Mertikopoulos, G. Piliouras, and X. Wang. Adagrad avoids saddle points.
297"
REFERENCES,0.8121442125237192,"In International Conference on Machine Learning, pages 731–771. PMLR, 2022.
298"
REFERENCES,0.8140417457305503,"[2] P. Austrin, M. Braverman, and E. Chlamtáˇc. Inapproximability of NP-complete variants of Nash
299"
REFERENCES,0.8159392789373814,"equilibrium. In Approximation, Randomization, and Combinatorial Optimization. Algorithms
300"
REFERENCES,0.8178368121442126,"and Techniques: 14th International Workshop, APPROX 2011, and 15th International Workshop,
301"
REFERENCES,0.8197343453510436,"RANDOM 2011, Princeton, NJ, USA, August 17-19, 2011. Proceedings, pages 13–25. Springer,
302"
REFERENCES,0.8216318785578748,"2011.
303"
REFERENCES,0.8235294117647058,"[3] Y. Babichenko. Query complexity of approximate Nash equilibria. Journal of the ACM (JACM),
304"
REFERENCES,0.825426944971537,"63(4):36:1–36:24, 2016.
305"
REFERENCES,0.8273244781783681,"[4] D. Barrett and B. Dherin. Implicit gradient regularization. In International Conference on
306"
REFERENCES,0.8292220113851992,"Learning Representations, 2020.
307"
REFERENCES,0.8311195445920304,"[5] P. L. Bartlett, V. Gabillon, and M. Valko. A simple parameter-free and adaptive approach to
308"
REFERENCES,0.8330170777988615,"optimization under a minimal local smoothness assumption. In Algorithmic Learning Theory,
309"
REFERENCES,0.8349146110056926,"pages 184–206. PMLR, 2019.
310"
REFERENCES,0.8368121442125237,"[6] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for
311"
REFERENCES,0.8387096774193549,"convex optimization. Operations Research Letters, 31(3):167–175, 2003.
312"
REFERENCES,0.8406072106261859,"[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
313"
REFERENCES,0.8425047438330171,"G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
314"
REFERENCES,0.8444022770398482,"information processing systems, 33:1877–1901, 2020.
315"
REFERENCES,0.8462998102466793,"[8] S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvári. X-armed bandits. Journal of Machine
316"
REFERENCES,0.8481973434535104,"Learning Research, 12(5), 2011.
317"
REFERENCES,0.8500948766603416,"[9] D. Calandriello, L. Carratino, A. Lazaric, M. Valko, and L. Rosasco. Scaling gaussian process
318"
REFERENCES,0.8519924098671727,"optimization by evaluating a few unique candidates multiple times. In International Conference
319"
REFERENCES,0.8538899430740038,"on Machine Learning, pages 2523–2541. PMLR, 2022.
320"
REFERENCES,0.855787476280835,"[10] A. Cutkosky, H. Mehta, and F. Orabona. Optimal stochastic non-smooth non-convex optimiza-
321"
REFERENCES,0.857685009487666,"tion through online-to-non-convex conversion. arXiv preprint arXiv:2302.03775, 2023.
322"
REFERENCES,0.8595825426944972,"[11] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a Nash
323"
REFERENCES,0.8614800759013282,"equilibrium. Communications of the ACM, 52(2):89–97, 2009.
324"
REFERENCES,0.8633776091081594,"[12] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and
325"
REFERENCES,0.8652751423149905,"attacking the saddle point problem in high-dimensional non-convex optimization. Advances in
326"
REFERENCES,0.8671726755218216,"neural information processing systems, 27, 2014.
327"
REFERENCES,0.8690702087286527,"[13] A. Deligkas, J. Fearnley, A. Hollender, and T. Melissourgos. Pure-circuit: Strong inapproxima-
328"
REFERENCES,0.8709677419354839,"bility for PPAD. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science
329"
REFERENCES,0.872865275142315,"(FOCS), pages 159–170. IEEE, 2022.
330"
REFERENCES,0.8747628083491461,"[14] Y. Feng, T. Wang, et al.
Lipschitz bandits with batched feedback.
Advances in Neural
331"
REFERENCES,0.8766603415559773,"Information Processing Systems, 35:19836–19848, 2022.
332"
REFERENCES,0.8785578747628083,"[15] I. Gemp, R. Savani, M. Lanctot, Y. Bachrach, T. Anthony, R. Everett, A. Tacchetti, T. Eccles,
333"
REFERENCES,0.8804554079696395,"and J. Kramár. Sample-based approximation of Nash in large many-player games via gradient
334"
REFERENCES,0.8823529411764706,"descent. In Proceedings of the 21st International Conference on Autonomous Agents and
335"
REFERENCES,0.8842504743833017,"Multiagent Systems, pages 507–515, 2022.
336"
REFERENCES,0.8861480075901328,"[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
337"
REFERENCES,0.888045540796964,"Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems,
338"
REFERENCES,0.889943074003795,"27, 2014.
339"
REFERENCES,0.8918406072106262,"[17] N. Hansen, S. D. Müller, and P. Koumoutsakos. Reducing the time complexity of the de-
340"
REFERENCES,0.8937381404174574,"randomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary
341"
REFERENCES,0.8956356736242884,"computation, 11(1):1–18, 2003.
342"
REFERENCES,0.8975332068311196,"[18] J. C. Harsanyi, R. Selten, et al. A general theory of equilibrium selection in games. MIT Press
343"
REFERENCES,0.8994307400379506,"Books, 1, 1988.
344"
REFERENCES,0.9013282732447818,"[19] E. Hazan, K. Singh, and C. Zhang. Efﬁcient regret minimization in non-convex games. In
345"
REFERENCES,0.9032258064516129,"International Conference on Machine Learning, pages 1433–1441. PMLR, 2017.
346"
REFERENCES,0.905123339658444,"[20] E. Janovskaja. Equilibrium points in polymatrix games. Lithuanian Mathematical Journal, 8
347"
REFERENCES,0.9070208728652751,"(2):381–384, 1968.
348"
REFERENCES,0.9089184060721063,"[21] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat, D. Silver, and
349"
REFERENCES,0.9108159392789373,"T. Graepel. A uniﬁed game-theoretic approach to multiagent reinforcement learning. In
350"
REFERENCES,0.9127134724857685,"Advances in Neural Information Processing Systems, pages 4190–4203, 2017.
351"
REFERENCES,0.9146110056925996,"[22] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay, J. Pérolat, S. Srini-
352"
REFERENCES,0.9165085388994307,"vasan, F. Timbers, K. Tuyls, S. Omidshaﬁei, D. Hennes, D. Morrill, P. Muller, T. Ewalds,
353"
REFERENCES,0.9184060721062619,"R. Faulkner, J. Kramár, B. D. Vylder, B. Saeta, J. Bradbury, D. Ding, S. Borgeaud, M. Lai,
354"
REFERENCES,0.920303605313093,"J. Schrittwieser, T. Anthony, E. Hughes, I. Danihelka, and J. Ryan-Davis.
OpenSpiel:
355"
REFERENCES,0.9222011385199241,"A framework for reinforcement learning in games.
CoRR, abs/1908.09453, 2019.
URL
356"
REFERENCES,0.9240986717267552,"http://arxiv.org/abs/1908.09453.
357"
REFERENCES,0.9259962049335864,"[23] S. Leonardos, G. Piliouras, and K. Spendlove. Exploration-exploitation in multi-agent com-
358"
REFERENCES,0.9278937381404174,"petition: convergence with bounded rationality. Advances in Neural Information Processing
359"
REFERENCES,0.9297912713472486,"Systems, 34:26318–26331, 2021.
360"
REFERENCES,0.9316888045540797,"[24] W. Li, H. Li, J. Honorio, and Q. Song. Pyxab – a python library for X-armed bandit and online
361"
REFERENCES,0.9335863377609108,"blackbox optimization algorithms, 2023. URL https://arxiv.org/abs/2303.04030.
362"
REFERENCES,0.9354838709677419,"[25] C. K. Ling, F. Fang, and J. Z. Kolter. What game are we playing? end-to-end learning in normal
363"
REFERENCES,0.937381404174573,"and extensive form games. arXiv preprint arXiv:1805.02777, 2018.
364"
REFERENCES,0.9392789373814042,"[26] S. Mandt, M. D. Hoffman, and D. M. Blei. Stochastic gradient descent as approximate bayesian
365"
REFERENCES,0.9411764705882353,"inference. Journal of Machine Learning Research, 18:1–35, 2017.
366"
REFERENCES,0.9430740037950665,"[27] L. Marris, I. Gemp, and G. Piliouras. Equilibrium-invariant embedding, metric space, and
367"
REFERENCES,0.9449715370018975,"fundamental set of 2x2 normal-form games. arXiv preprint arXiv:2304.09978, 2023.
368"
REFERENCES,0.9468690702087287,"[28] R. D. McKelvey and T. R. Palfrey. Quantal response equilibria for normal form games. Games
369"
REFERENCES,0.9487666034155597,"and Economic Behavior, 10(1):6–38, 1995.
370"
REFERENCES,0.9506641366223909,"[29] D. Milec, J. ˇCern`y, V. Lis`y, and B. An. Complexity and algorithms for exploiting quantal
371"
REFERENCES,0.952561669829222,"opponents in large two-player games. Proceedings of the AAAI Conference on Artiﬁcial
372"
REFERENCES,0.9544592030360531,"Intelligence, 35(6):5575–5583, 2021.
373"
REFERENCES,0.9563567362428842,"[30] P. R. Milgrom and R. J. Weber. A theory of auctions and competitive bidding. Econometrica:
374"
REFERENCES,0.9582542694497154,"Journal of the Econometric Society, pages 1089–1122, 1982.
375"
REFERENCES,0.9601518026565465,"[31] K. G. Murty and S. N. Kabadi. Some NP-complete problems in quadratic and nonlinear
376"
REFERENCES,0.9620493358633776,"programming. Technical report, 1985.
377"
REFERENCES,0.9639468690702088,"[32] H. Nikaidô and K. Isoda. Note on non-cooperative convex games. Paciﬁc Journal of Mathemat-
378"
REFERENCES,0.9658444022770398,"ics, 5(1):807815, 1955.
379"
REFERENCES,0.967741935483871,"[33] E. Nudelman, J. Wortman, Y. Shoham, and K. Leyton-Brown. Run the GAMUT: A comprehen-
380"
REFERENCES,0.969639468690702,"sive approach to evaluating game-theoretic algorithms. In AAMAS, volume 4, pages 880–887,
381"
REFERENCES,0.9715370018975332,"2004.
382"
REFERENCES,0.9734345351043643,"[34] J. Pérolat, S. Perrin, R. Elie, M. Laurière, G. Piliouras, M. Geist, K. Tuyls, and O. Pietquin.
383"
REFERENCES,0.9753320683111955,"Scaling mean ﬁeld games by online mirror descent. In Proceedings of the 21st International
384"
REFERENCES,0.9772296015180265,"Conference on Autonomous Agents and Multiagent Systems, 2022.
385"
REFERENCES,0.9791271347248577,"[35] A. Raghunathan, A. Cherian, and D. Jha. Game theoretic optimization via gradient-based
386"
REFERENCES,0.9810246679316889,"Nikaido-Isoda function. In International Conference on Machine Learning, pages 5291–5300.
387"
REFERENCES,0.9829222011385199,"PMLR, 2019.
388"
REFERENCES,0.9848197343453511,"[36] Y. Shoham and K. Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical
389"
REFERENCES,0.9867172675521821,"foundations. Cambridge University Press, 2008.
390"
REFERENCES,0.9886148007590133,"[37] M. Valko, A. Carpentier, and R. Munos. Stochastic simultaneous optimistic optimization. In
391"
REFERENCES,0.9905123339658444,"International Conference on Machine Learning, pages 19–27. PMLR, 2013.
392"
REFERENCES,0.9924098671726755,"[38] B. Wiedenbeck and E. Brinkman. Data structures for deviation payoffs. In Proceedings of the
393"
REFERENCES,0.9943074003795066,"22nd International Conference on Autonomous Agents and Multiagent Systems, 2023.
394"
REFERENCES,0.9962049335863378,"[39] Y. Zhou, J. Li, and J. Zhu. Identify the Nash equilibrium in static games with random payoffs.
395"
REFERENCES,0.9981024667931688,"In International Conference on Machine Learning, pages 4160–4169. PMLR, 2017.
396"
