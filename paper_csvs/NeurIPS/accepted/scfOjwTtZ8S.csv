Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.006369426751592357,"Reinforcement learning (RL) in long horizon and sparse reward tasks is notoriously
difficult and requires a lot of training steps. A standard solution to speed up the
process is to leverage additional reward signals, shaping it to better guide the
learning process. In the context of language-conditioned RL, the abstraction and
generalisation properties of the language input provide opportunities for more
efficient ways of shaping the reward. In this paper, we leverage this idea and
propose an automated reward shaping method where the agent extracts auxiliary
objectives from the general language goal. These auxiliary objectives use a question
generation (QG) and question answering (QA) system: they consist of questions
leading the agent to try to reconstruct partial information about the global goal using
its own trajectory. When it succeeds, it receives an intrinsic reward proportional
to its confidence in its answer. This incentivizes the agent to generate trajectories
which unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration."
INTRODUCTION,0.012738853503184714,"1
Introduction"
INTRODUCTION,0.01910828025477707,"One of the main challenges of Reinforcement Learning (RL) research is to train agents able of
abstraction, generalisation and communication. Languages, be they natural or formal, afford these
desirable properties [11]. Based on this insight, many papers have tried to leverage the abilities
of language in RL to enable communication and improve generalisation and sample efficiency
[3, 22, 12, 36]. The domain can be subdivided into language-conditioned RL (LC-RL), in which
language conditions the formulation of the problem[2, 12], and language-assisted RL, where language
helps the agent to learn [15, 10, 1, 9]. In the present paper, we focus on the LC-RL framework where
the agent initially receives a language instruction and must act to optimise the corresponding reward
function. Unfortunately, the corresponding RL algorithms are sample inefficient, especially due to
the fact that the reward function is sparse when it is restricted to completing the goal."
INTRODUCTION,0.025477707006369428,"To tackle the reward sparsity issue, one idea is to densify the reward by decomposing the general goal
into sub-goals and rewarding them individually. This idea is based on a decomposition principle which"
INTRODUCTION,0.03184713375796178,"Figure 1: During training, the agent uses the goal to generate relevant questions using its question-
generation module QG. Then, it attempts at answering them from current trajectories at each step
with its question-answering module QA, by looking at the trajectory. When it succeeds, it obtains an
intrinsic reward proportional to its confidence in its answer. Then it removes the answered questions
from the list of questions. This incentivizes the agent to produce trajectories that enable to reconstruct
unambiguously partial information about the general language goal, enabling to shape rewards and
guide learning."
INTRODUCTION,0.03821656050955414,"hypothesises that a general goal can be decomposed into a set of easier ones. Previous works assume
a strict decomposition of the general goal into sub-goals [3, 17]. However, strict decomposition
requires that that each high-level goal can be reached through an exact series of low-level policies,
an assumption which fails when all the primitive actions are not known in advance. More recently,
the ELLA method was proposed as a language reward shaping technique that is sample efficient and
does not require a strict decomposition: it rewards interesting auxiliary objectives, without requiring
rigid ordering and correspondence with sub-goals [23]."
INTRODUCTION,0.044585987261146494,"However, all these methods suffer from the need for expert input. Indeed, they require task-specific
engineering for determining at least one of the following elements: the set of sub-goals or auxiliary
objectives, the relevant ones and when they are achieved, and the appropriate reward. While all these
points can be addressed more or less easily in simple game-like environments, they get demanding
when the environment is more complex. Therefore, it seems desirable to find an alternative reward
shaping approach that minimises expert involvement. We would like the agent to generate its own
auxiliary objectives and create intrinsic rewards, just like a human would do."
INTRODUCTION,0.050955414012738856,"The Natural Language Processing (NLP) field often suffers from the same need of expert input,
e.g. for the evaluation of automatic summary tasks. In that context, some successful approaches
have developed reference-less metrics [31], [29], based on question generation (QG) and question
answering (QA). These techniques assess the quality of a generated text by measuring the quantity
of information from the source conserved in the generated text (broadly, how well one can answer
questions about the original text using the generated text). These metrics are said reference-less
because they do not require a comparison with a man-made example to evaluate the quality of a text."
INTRODUCTION,0.05732484076433121,"Contributions In our work, we build on these reference-less metrics to circumvent the need of expert
input for generating auxiliary objectives. We adapt it and propose a novel QG/QA framework for
RL called EAGER.1 In EAGER, an agent reuses the initial language goal sentence to generate a
set of questions (QG): each of these self-generated questions defines an auxiliary objective. Here,
generating a question consists in masking a word of the initial language goal. Then the agent tries
to answer these questions (guess the missing word) only by observing its trajectory so far. When
it manages to answer a question correctly (QA) it obtains an intrinsic reward proportional to its
confidence in the answer. The QA module is trained using a set of successful example trajectories. If
the agent follows a path too different from correct ones at some point in its trajectory, the QA module"
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.06369426751592357,"1Exploit question-Answering Grounding for effective Exploration in language-conditioned Reinforcement
learning, see https://github.com/flowersteam/EAGER for access to the code."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.07006369426751592,"will not answer the question correctly, resulting in zero intrinsic reward. The sum of all the intrinsic
rewards measures the quality of a trajectory in relation to the given goal. In other words, maximizing
this intrinsic reward incentivizes the agent to produce behaviour that unambiguously explains various
aspects of the given goal."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.07643312101910828,"To the best of our knowledge, EAGER is the only framework that can automatically 1) generate
relevant auxiliary objectives, 2) determine their completion and 3) return the appropriate intrinsic
reward. This approach only assumes the agent has access to a dataset of demonstrated behaviours
associated to global language commands, which enables it to pre-train its question answering module."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.08280254777070063,Thus our work brings the following contributions:
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.08917197452229299,"• We create a QG/QA metric providing to an agent an information-rich measure of the quality of its
trajectory given a goal."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.09554140127388536,"• We propose the EAGER framework that lets the agent guide its own learning process by generating
auxiliary objectives and producing intrinsic rewards without requiring any expert intervention."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.10191082802547771,"• We show that EAGER retains the good properties of ELLA without requiring task-specific expert
knowledge, by leveraging properties of language."
EXPLOIT QUESTION-ANSWERING GROUNDING FOR EFFECTIVE EXPLORATION IN LANGUAGE-CONDITIONED REINFORCEMENT,0.10828025477707007,"• We experiment EAGER with the BabyAI platform [7]: we compare our approach against ELLA
(SOTA on BabyAI) and RIDE (a non-language based reward shaping approach using intrinsic
motivation), showing its robustness and sample efficiency. Furthermore, although we use example
trajectories to train the QA, their use is much more parsimonious than training an agent using
behavioural cloning [7], as we show in Appendix C."
RELATED WORK,0.11464968152866242,"2
Related work"
RELATED WORK,0.12101910828025478,"Language-conditioned RL.
We place our work in the LC-RL setting, where an agent learns a
policy to execute language commands [24, 7, 20, 21]. We reuse the BabyAI platform [7], widely
used in this domain as it enables to decouple exploration challenges from perception challenges. It
uses a synthetic language exhibiting interesting combinatorial properties with possible conjunction of
properties, and procedural generation to avoid overfitting [8]. Here, we consider instruction following
agents which receive external instructions and rewards [14, 5, 17]."
RELATED WORK,0.12738853503184713,"Language as an abstraction in hierarchical RL.
Several approaches leverage language for
abstraction in hierarchical RL. One approach uses language for training a low-level instruction-
following policy, then learns a high-level policy that generates the sequence of low-level goals [17].
Another one explicitly decomposes high-level tasks into low-level ones as in policy sketches [3].
ELLA [23] also uses language to decompose high-level tasks but relaxes the strict decomposition
constraint by replacing sub-goals with auxiliary objectives. We adopt the same flexible framework in
our approach but automatise the decomposition."
RELATED WORK,0.1337579617834395,"Language for exploration and reward shaping.
Reward shaping is a form of guidance that
supplies additional rewards to the agent to direct its learning process. Among approaches studying
how language can shape rewards and exploration, LEARN [12] proposes to map intermediate natural
language instruction to intermediate rewards. Similarly, [35] enables reward shaping using natural
language through a narration-guided method. The high-level tasks are decomposed into low-level
tasks and rewarded using narration. ELLA [23] is positioned in the same paradigm but with fewer
assumptions about the environment or the structure of the task."
RELATED WORK,0.14012738853503184,"Other approaches assume that an oracle provides language descriptions of environment states which
are used as state abstraction to generate novelty intrinsic rewards and guide exploration [25, 34].
This extends classical approaches to intrinsically motivated exploration [4], such as count-based
methods [6] or RIDE [28]. Here, we also use language to generate intrinsic rewards, but we do not
need language descriptions of states. Besides, other forms of intrinsic motivation systems, such as
IMAGINE [10], first learn language-parameterized reward functions through interaction with a social
peer, then autonomously use language to generate diverse and novel goals, using the learned reward
function for self-supervision."
RELATED WORK,0.1464968152866242,"Asking questions in RL.
Beyond reward shaping, some methods consider agents that use language
to ask questions to external knowledge sources. In QWA [36], questions are used to identify sub-tasks
and prune irrelevant actions. In AFK [19], questions are used to obtain world knowledge that helps
completing tasks."
RELATED WORK,0.15286624203821655,"Natural Language Processing
One of the sources of QG/QA methods is the thriving field of
question generation from natural language processing and information retrieval [16]. Our approach
is inspired from text generation methods where QG and QA are used to measure the quality of a
generated text without using a human reference [31, 29]."
PROBLEM STATEMENT,0.1592356687898089,"3
Problem statement"
PROBLEM STATEMENT,0.16560509554140126,"We set ourselves in the standard framework of LC-RL with an augmented Partial Observation Markov
Decision Process [33] M defined by the tuple (S, A, Z, T , O, G, V, I, R, γ), with S the state space,
Z the observation space, A the actions space, G the goal space, V the vocabulary of goal instructions.
O stands as the observation function O : S →Z, which maps states to the observations space. I is
the instruction function I : G →Visize, which maps any goal in G to the set of language instructions,
which correspond to sequences of isize symbols (the empty symbol ϵ belongs to V to allow variable
instruction sizes). R is a goal-conditioned state action reward function, with R : S × A × G →[0, 1]
the extrinsic reward received for some goal g ∈G and γ the discount factor. For simplicity, we note
in the following rg
t = R(st, at, g) as the reward obtained at step t of any episode with goal g."
PROBLEM STATEMENT,0.17197452229299362,"At each time step t, the agent receives an observation ot ∈Z following the observation function
O : S →Z and selects an action at ∈A to reach a goal g ∈G, expressed by ωg = I(g).
T : S × A →P(S) is the transition function. Using RL, we search for an optimal goal-conditioned
policy π∗, such that π∗: S × Visize →A maximises the discounted expected return Rπg
t
=
Eπ[PT
k=0 γk rg
t+k+1]. We consider in this work sparse reward problems, where r(s, a, g) returns 1
for any state s such that d(s, g) ≤ϵ, for a given distance function d and a specified threshold ϵ, and 0
otherwise. Moreover, we assume a limited number of steps at most H steps. These two conditions
result in a hard exploration problem."
PROBLEM STATEMENT,0.17834394904458598,"To deal with those problems, many methods aim at densifying rewards, by focusing on auxiliary
objectives during training, whose accomplishment can help the agent to reach the goal g at hand. In
previous work, such as ELLA [23], the selection of relevant objectives required the intervention of an
expert (in the form of expert annotations and example trajectories), which can be problematic because
new expert trade-offs have to be established for each new environment. Thus, an automated way
must be found to recover the relevant auxiliary objectives, measure their completion, and associate
the appropriate reward."
PROBLEM STATEMENT,0.18471337579617833,"Rather than relying on expert knowledge for defining auxiliary objectives, we assume that we
have access to a set of trajectories of successful examples coupled to their respective instructions
{(τ0, ωg
0), ..., (τn, ωg
n)}, where τn = (oi, ai)i∈[|0,k|] with k the number of steps. For any goal
instruction ωg, we consider a function f that aims at generating a set of auxiliary goals of g, such
that all g′ ∈f(ωg) belong to Gg, with Gg ⊂G the set of goals that help training the agent towards g.
Then, for a trajectory τ and any g′ ∈f(ωg), a function h determines the probability h(τ, g′) ∈[0, 1]
that the auxiliary objective g′ has been achieved. We train h using the example demonstrations such
that we can leverage its generalisation abilities to use h for unseen trajectories."
PROBLEM STATEMENT,0.1910828025477707,"Please note that Behavioural Cloning methods also rely on the exploitation of expert trajectories, by
learning the policy πBC that maximises the log-likelihood LBC: LBC = n
X"
PROBLEM STATEMENT,0.19745222929936307,"i=0
log P(τi) where log P(τ) = k
X"
PROBLEM STATEMENT,0.20382165605095542,"j=0
log(πBC(ak|ok)T (sk+1|sk, ak))."
PROBLEM STATEMENT,0.21019108280254778,"However, the latter uses demonstrations to train an agent to copy an expert based on a data set
of example trajectories. This technique is only effective when the agent is close enough to the
demonstrated examples. A large number of demonstrations are therefore required to cover the space
of states that the agent may encounter. Methods such as GAIL [18] can partially circumvent this
problem by forcing the agent to stay on known trajectories, but this impairs generalisation. Our
method is much more parsimonious in the use of such examples: because it generalises well, the"
PROBLEM STATEMENT,0.21656050955414013,"agent can receive intrinsic rewards even in areas not encountered in the expert demonstrations (see
Appendix C)."
METHOD,0.2229299363057325,"4
Method"
METHOD,0.22929936305732485,"In this section, we first introduce EAGER, our automated reward shaping method based on QG/QA,
then we present a practical implementation in the context of the BabyAI benchmark. Figure 1
provides a graphical overview of EAGER."
EAGER,0.2356687898089172,"4.1
EAGER"
EAGER,0.24203821656050956,"We need an automatic evaluation method that is fine-grained enough to rank various trajectories
depending on a language instruction. But various successful policies can generate a set of valid
trajectories for the same goal. It could be deceiving to rank them based only on their final results, as
an overly complex trajectory seems intuitively worse than a simpler one even if the result is the same."
EAGER,0.2484076433121019,"To address this issue, the EAGER framework consists of an agent learning module, a question
generation module QG (automatic, but not learned) and a learned question answering module QA.
.These two modules fulfil the role of the functions f and h defined in Section 3. EAGER is inspired
from works like QuestEval [31] and Data-QuestEval [29], developed for natural language generation.
For instance, for abstractive summarization, by generating questions from the original text (QG) and
trying to answer them using the summary QA, this method measures the quantity of information
shared between both texts."
EAGER,0.25477707006369427,"In our work, we draw the analogy with the NLP task with the goal replacing the original text and
the trajectory replacing the summary. We use the QG/QA system to verify that a trajectory contains
the same level of information as the language instruction, meaning that the goal is contained in
the trajectory. As the goal can be contained in a lot of different trajectories, we also favour simple
trajectories. If one can easily answer the question, that means the trajectory is simple."
EAGER,0.2611464968152866,"The QG module
We assume that the goal linguistic instruction ωg of g ∈G is a highly expressive
language instruction (e.g Put the red ball next to the blue box, open the red door, ...), containing
by themselves enough world knowledge to generate questions by masking words. Thus, the QG
module returns a list of k questions QGk(ωg) that can be seen as auxiliary objectives. Each question
is formed by masking one word in the linguistic instruction. Crucially, the choice of words to mask
can be done automatically without any expert knowledge of the task, or the environment, for instance
masking all nouns and adjectives. These questions can be seen as auxiliary objectives guiding the
agent during training. Besides, being formulated in natural language, these auxiliary objectives are
easily interpreted."
EAGER,0.267515923566879,"The QA module
Let ˜A be the set of possible answers generated automatically from the list of
tokens masked by the QG. Thus, ˜A contains (q, a∗) pairs of questions and expected answers. The
QA module returns the probability for all ˜a ∈˜A, that ˜a answers question q, given a trajectory
τt, where τt = (oi, ai)i∈[|0,t|] are state-action pairs and t the time step. We note ˜a∗= QA(q, τt)
the answer greedily generated from the module and QA(˜a∗| q, τt) the associated probability. The
auxiliary objectives from the QG are considered achieved when the QA answers them successfully.
In our work, the QA module is pre-trained using full example trajectories generated by a bot, see
Section 5.1, without any type of annotations to guide it. Besides, at time step t, there is no guarantee
that the trajectories contain enough information to correctly answer a question. Thus we include a
«no_answer» token in ˜A to prevent the QA from answering correctly by chance. Moreover, since the
QA module takes the whole trajectory, once it has answered a question, it can also answer it at the
next step. To avoid giving a reward that does not have direct link to the current step, every time a
question q is answered, it is removed from the set of questions: We note Qt the active set of questions,
the initial set is Q0 = QGk(ωg) and once a question is answered, we apply Qt ←Qt−1 \{q}."
EAGER,0.27388535031847133,"Architecture of the QA
The QA is used to compositionally chain low-level tasks. To do this,
it relies on the episodic transformer [27]. This architecture uses multimodal transformers (over
language, visual observation, and a list of actions) that have access to the full episode. For any time"
EAGER,0.2802547770700637,"step t and any question, a (x1:L, v1:t, a1:t) tuple is given as input to the QA module. The language
input x1:L is the question, it is a sequence of L tokens with xi ∈N. The visual input v1:t is the list of
observations vt ∈RW ×H×3. Finally, the action input a1:t is a list of discrete actions. As output, the
network returns the probability distribution over the set of possible answers ˜
A."
EAGER,0.28662420382165604,"Intrinsic reward
The main difficulty in ranking trajectories is that many trajectories can share
the same description and thus can answer questions correctly. However, a human assessing several
trajectories, with the same extrinsic reward, relatively to the same goal would prioritise the simplest
one. To account for this, we make the reward proportional to the confidence in the answer at time step
t: QA(˜a∗| q, τt). An overly complex trajectory being harder to understand for the QA, the answer
should be given with less confidence, so it should obtain less reward than a direct and clear trajectory,
even with the same number of correct answers."
EAGER,0.2929936305732484,"In this paper, we keep the same desired properties for the shaped reward function as the ones given
in ELLA: the reward shaping should not change the optimal policy that prevails before the reward
shaping (policy invariance), and the reward should encourage sample efficient exploration based on
auxiliary-objectives. Using intrinsic rewards, we modify the global reward from r to r′ through a
policy invariant transformation [26], which ensure that the new policy π∗is optimal for both M and
M′ = (S, A, T , G′, γ).To do so, we ensure that only successful trajectories get the same return with
or without the reward shaping by substracting the shaped reward at the final time step N of a successful
trajectory: r′
N = rN −P"
EAGER,0.29936305732484075,"t∈TS γt−N r′
t with r′
t = λ P"
EAGER,0.3057324840764331,"q∈Qt
P"
EAGER,0.31210191082802546,"˜a∗is correct answer QA(˜a∗| q, τt), where
TS is the set of time steps where a bonus is applied. rt is the reward given at time step t, in the case
of sparse reward studied here rt ̸= 0 only if t corresponds to the last step of a successful trajectory.
In ELLA Mirchandani et al. [23] the authors prove that this transformation is policy invariant. Thus,
as long as the policy produces unsuccessful trajectories, the agent is guided by the shaping reward.
Then once it has learn to successfully complete an instruction, the shaping reward is substracted at
the last step and the agent improves using only the extrinsic reward. Further details are provided in
the appendix."
EAGER,0.3184713375796178,"The QA system is pre-trained with successful trajectories, which prevents reward hacking. Indeed, if
the agent individually completes the auxiliary objectives without a meaningful trajectory, the QA
does not consider the trajectory meaningful and answers: ""no_answer"", preventing the agent to get
rewarded."
EAGER,0.3248407643312102,"Using the above notations and concepts, we can define a metric to measure the adequacy of a trajectory
to a goal, that corresponds to our cumulative intrinsic reward over a trajectory τ of length N, up to a
λ factor:"
EAGER,0.33121019108280253,"mQG/QA(g, τ) = N
X t=0 X"
EAGER,0.3375796178343949,"q∈Qt
QA(˜a∗| q, τt) I[˜a∗correct answer to q].
(1)"
EAGER,0.34394904458598724,"Algorithm
Our algorithmic procedure is given in Algorithm 1. At the beginning of an episode, the
QG takes the goal and returns a set of questions related to it. Then the QA module is applied at each
step over the active set of questions Qt. When a question is answered correctly, the shaped reward
function returns a bonus λ QA(˜a∗| q, τt), where λ is a scaling factor, to the agent and the answered
question is removed from the set of active questions."
EAGER,0.3503184713375796,"Then we tune λ to ensure that no unsuccessful trajectories can get more reward than a successful one
from the optimal policy π∗. The higher bound for the shaped reward of unsuccessful trajectories is
λ k and the lower bound for the reward of a successful trajectory is γH rH, where k is the number of
questions generated by the QG and H is the maximum length of an episode. Thus we obtain"
EAGER,0.35668789808917195,λ < γH rH
EAGER,0.3630573248407643,"k
.
(2)"
EAGER,0.36942675159235666,"Note that by making a less conservative hypothesis, i.e. assuming in the worst case the successful
trajectory takes N<H steps, we could obtain a higher λ leading to faster learning [23]."
EAGER,0.37579617834394907,"Algorithm 1 Automatic auxiliary goal generation and reward shaping using EAGER
Input: θ0 initial policy parameters, λ bonus scaling factor,
ENV the environment and OPTIMISE an RL optimiser"
EAGER,0.3821656050955414,"for k=0,..., nstep do"
EAGER,0.3885350318471338,"ωg, o0, done0 ←ENV.reset()
Q0 = {q1, ..., qk} ←QGk(ωg)
t ←0
while donet not True do"
EAGER,0.39490445859872614,"at ←πθ0(ot)
ot+1, rt, donet+1 ←ENV(at)
r′
t, Qt+1 ←QA_SHAPE(Qt, τt, rt)
if donet+1 is True then"
EAGER,0.4012738853503185,"N ←t
r′
N ←NEUTRALISE(r′
1:N)
end if
end while
Update θk+1 ←OPTIMISE(r′
1:N)
end for"
EAGER,0.40764331210191085,"function QA_SHAPE(Qt, τt, rt)"
EAGER,0.4140127388535032,for q in Qt do
EAGER,0.42038216560509556,"˜a∗←QA(q, τt)
if ˜a∗is correct answer to q
then"
EAGER,0.4267515923566879,"r′
t = rt + λ QA(˜a∗| q, τt)
Q = Q \{q}
end if
end for
return r′
t, Q
end function"
EAGER,0.43312101910828027,"function NEUTRALISE(r′
1:N)
r′
N ←r′
N −P"
EAGER,0.4394904458598726,"t∈TS γt−N r′
t
return r′
N
end function"
A PARTICULAR INSTANCE OF THE METHOD IN THE BABYAI FRAMEWORK,0.445859872611465,"4.2
A particular instance of the method in the BabyAI framework"
A PARTICULAR INSTANCE OF THE METHOD IN THE BABYAI FRAMEWORK,0.45222929936305734,"We now explain how to adapt the EAGER method to train an RL agent in BabyAI, a language-
conditioned environment where the agent has a limited number of steps to complete a language goal.
In this environment the agent receives a reward if and only if it finishes the task successfully."
A PARTICULAR INSTANCE OF THE METHOD IN THE BABYAI FRAMEWORK,0.4585987261146497,"The BabyAI benchmark contains tasks with highly expressive language instructions e.g Put the red
box next to the green key, ...). Thus they are rich enough to generate questions by masking words.
In practice, we mask nouns and adjectives: this form of QG is very simple and can be automated
using standard NLP techniques, thus it does not require expert knowledge. For instance, for the goal
Put the red ball next to the blue box, using the token «question» as a mask we generate 4 questions
among which Put the «question» ball next to the blue box."
A PARTICULAR INSTANCE OF THE METHOD IN THE BABYAI FRAMEWORK,0.46496815286624205,"The environments in our experiments are partially observable. Thus, our agent takes sequences of
observations (o1, o2, ..., ot) as inputs of a recurrent network [13]."
EXPERIMENTS,0.4713375796178344,"5
Experiments"
EXPERIMENTAL SETTINGS,0.47770700636942676,"5.1
Experimental settings"
EXPERIMENTAL SETTINGS,0.4840764331210191,"We use the BabyAI [7] platform to run our experiments. This platform relies on a gridworld
environment (MiniGrid) to generate a set of complex instructions-following environments. It has
been specifically designed for research on grounded language learning and related sample efficiency
problems. The gridworld environment is populated with several entities: the agent, boxes, balls,
doors, and keys of 6 different colors. These entities are placed in rooms of 8 × 8 tiles that are
connected by doors that could be locked or closed. The agent can do 6 primitive navigation actions
such as forward, toggle, pick up to solve the language instruction (for instance Pick up the
red box). It only has access to partial observations of its environment inside which irrelevant objects
are randomly added. Moreover, the observations are in a symbolic space using a compact encoding,
with 3 input values per grid cell,2 8 × 8 × 3 values in total. When the agent completes the task after
N steps, it receives the reward rN = 1 −0.9 N"
EXPERIMENTAL SETTINGS,0.49044585987261147,"H , where H is the maximum number of steps. During
training, all rewards are scaled up by a factor of 20 to ensure a good propagation of the rewards. If the
agent fails, the reward is 0. We focus our tests on tasks of varying complexity: PutNextTo, Unlock
and Sequence. The task can also take place in one room local or two rooms Medium."
EXPERIMENTAL SETTINGS,0.4968152866242038,"To train the QA module through supervised learning, we build a dataset of example trajectories
associated to language goals using a bot provided in BabyAI, then we generate related questions and"
EXPERIMENTAL SETTINGS,0.5031847133757962,"23 integers: one representing the shape of the object, one its color, and one its state. For instance, (4, 1, 1)
represents a closed green door"
EXPERIMENTAL SETTINGS,0.5095541401273885,"Figure 2: Average reward for EAGER and baselines for four tasks, with error regions to indicate
standard deviation over four random seeds. For the PutNext-Local and Sequence-Medium tasks, we
give an example of possible tasks with environment at time step 0."
EXPERIMENTAL SETTINGS,0.5159235668789809,"answers. To obtain a QA that can operate on various tasks, we use a mix of the PutNextTo, PickUp,
Open, and Sequence tasks for generating training trajectories. This dataset is only used to train the
QA, not to bootstrap policy learning. During training, we give as input of the QA the full trajectory
— the list of observations and actions — and all the questions generated by the QG and we use the
cross-entropy loss over its output distribution to update it. To train the QA to answer: ""no_answer""
and prevent it from guessing the answer by chance, we randomly associate some trajectories and
questions from unrelated goals. For instance, we associate the trajectory from the goal: take the red
box next to the blue ball and associate it with the question: take the red «question» next to the blue
key generated from the goal take the red box next to the blue key. The QA must learn to pay attention
to all the details of the question. It has to see that the trajectory describes an agent taking the red
box but placing it next to an object other than the one in the question. Thus the good answer to the
question is no_answer and not box."
EXPERIMENTAL SETTINGS,0.5222929936305732,"Moreover, we empirically show in Section 5.4 that the QA is more efficient when it learns from a
broad distribution of trajectories for similar tasks. The intuition behind such behaviour is that a QA
trained on a narrow distribution of successful bot-generated trajectories will not recognise the noisy
trajectories of the agent when it starts training. Thus the QA will too often answer ""no_answer"",
resulting in no intrinsic reward and hurting the reward shaping efficiency. To produce a wider
distribution using the given procedural bot, we force the bot to take with a certain probability a
random action at each time step and only keep successful noisy trajectories as training examples.
More details on the QA pre-training are given in Appendix B.1. All the tasks we use for training the
QA and the agent are summarised with examples in Appendix A."
EXPERIMENTAL SETTINGS,0.5286624203821656,"To evaluate our reward shaping framework, we use the Proximal Policy Optimization (PPO) algorithm
[30], but our reward shaping method is algorithm agnostic. We compare our framework to PPO
without reward shaping, ELLA and RIDE. RIDE [28] is an exploration method that does not use
language and addresses sparse reward problem by rewarding impactful change in state. We use
Nvidia Tesla V100 with 10 cores to train our model and we use 4 seeds in each experiment."
EXPERIMENTAL SETTINGS,0.535031847133758,"Figure 2 presents learning curves for EAGER, ELLA and RIDE across 4 environments. Table 1
describes the assumptions and the type of expert knowledge required by the three reward shaping
methods. It clearly appears that EAGER requires less expert human intervention than ELLA."
EXPERIMENTAL SETTINGS,0.5414012738853503,"Table 1: All the assumptions and expert knowledge required for RIDE, EAGER, and ELLA"
EXPERIMENTAL SETTINGS,0.5477707006369427,"Method
Number of expert
Human Expert Intervention
Automated parts done by the agent
demonstration per task"
EXPERIMENTAL SETTINGS,0.554140127388535,"RIDE
0
None
Determining if a new state is impacting"
EXPERIMENTAL SETTINGS,0.5605095541401274,"EAGER
7.500 noisy bot trajectories
Determining what words are
Determining relevant auxiliary objectives
(see supplementary
nouns or adjectives
Determining auxiliary objectives completion
Wide distribution of trajectories)
Determining auxiliary objectives associated reward"
EXPERIMENTAL SETTINGS,0.5668789808917197,"ELLA
Determining the class of relevant
Determining relevant auxiliary objectives
15.000 bot trajectories
auxiliary objectives
among the predetermine class
Determining auxiliary objectives
Determining auxiliary objectives completion
associated reward"
EXPERIMENTAL SETTINGS,0.5732484076433121,"5.2
How does EAGER perform when sparsity increases?"
EXPERIMENTAL SETTINGS,0.5796178343949044,"In the PutNextTo and Unlock tasks, EAGER obtains results better than ELLA (SOTA in BabyAI)
without using expert knowledge. It also performs significantly better than RIDE for the tasks PutNext
and Sequence and slightly better for Unlock. The better performance of EAGER with respect to
RIDE is not surprising as the EAGER agent receives some indications based on example trajectories
through the QA module."
EXPERIMENTAL SETTINGS,0.5859872611464968,"For Unlock-Medium, EAGER overcomes a bottleneck. The general goal being Open the «colour»
door, the agent has to first pick up the key of the corresponding colour before reaching the door
to open it. ELLA rewards picking up keys, via the PICK low-level instruction chosen via expert
knowledge. EAGER reaches better performance (see the statistical test in Appendix C) without the
need for expert knowledge. Moreover, although this is not its main purpose, EAGER gets a similar or
better sample efficiency for most tasks."
EXPERIMENTAL SETTINGS,0.5923566878980892,"5.3
How does EAGER perform with a sequence of tasks under a temporal constraint?"
EXPERIMENTAL SETTINGS,0.5987261146496815,"The Sequence task adds a temporal constraint by chaining two tasks using ’before’ or ’after’ together
with a high number of instructions (over 1.5M instructions in comparison with PutNext-Medium with
1440 instructions). Moreover both EAGER and ELLA decompose the goal into auxiliary objectives.
This decomposition does not retain the temporal constraint, there is no notion of doing one auxiliary
objective before another."
EXPERIMENTAL SETTINGS,0.6050955414012739,"Our tests show that EAGER retains strong performance, doing better than RIDE and ELLA. The
slow progress of EAGER at the beginning can be attributed to the time when the agent is not good
enough to efficiently trigger an intrinsic reward signal from the QA module. Indeed, at the beginning,
trajectories are noisy and it is more difficult for the QA to exploit a trajectory with more rooms. As a
result, it over-responds ""no_answer"" leading to a lesser intrinsic reward."
EXPERIMENTAL SETTINGS,0.6114649681528662,"5.4
Is EAGER robust to QA performance?"
EXPERIMENTAL SETTINGS,0.6178343949044586,"At first glance, the reliability of the QA looks crucial to our method. However, the QA could be
difficult to train in some environments e.g. if you want the QA to learn to answer in a large set
of answers from a small number of example trajectories. This is why we tested the robustness of
our method using the PutNextTo-Local task with two metrics: the success rate SR of the QA after
pre-training and the distribution of example trajectories. For the former, we take the same QA at
different training epochs and we determine its SR over a test dataset, then we train the agent using the
reward shaping provided by this QA. Figure 3(left) shows the robustness of our method with agents
that display similar training curves as soon as SR > 0.56."
EXPERIMENTAL SETTINGS,0.6242038216560509,"In Figure 3(right), we plot the SR of the QA when training the agent. Initially, the QA with a
SR ≤0.56 at pre-training time tends to have a higher SR. Indeed, the distribution of answers is less
peaked and answers are often correct by chance. On the opposite, the QA with a SR > 0.56 answers
""no_answer"" and obtains no reward. However, in this case, the agent learns faster because it only
receives a reward for meaningful answers. For the QA with a SR ≤0.56, the SR along training first
grows then decreases. First, the agent is biased by the intrinsic reward to follow a path that improves"
EXPERIMENTAL SETTINGS,0.6305732484076433,"Figure 3: Average reward (left) and success rate
of the QA (right) for the PutNextTo-Local tasks.
The agent are trained with QA having different
success rate after pre-training."
EXPERIMENTAL SETTINGS,0.6369426751592356,"Figure 4: Average reward in tasks PutNextTo
and Sequence for two distributions of bot tra-
jectories used to train QA. A narrow and a wide
distribution where noise is added to bot trajec-
tories"
EXPERIMENTAL SETTINGS,0.643312101910828,"the SR, but once the agent learns to complete the trajectory leading to the extrinsic reward, the SR
converges to pre-training SR."
EXPERIMENTAL SETTINGS,0.6496815286624203,"As explained in Section 5.1, we added noise to trajectories generated by the bot to compensate for
a too narrow trajectory distribution. Figure 4 shows training curves for two environments for QA
trained on wide trajectory distribution (WD) and narrow distribution (ND). The reward shaping
method trained on (WD) learns faster because they efficiently reward the agent early in training."
EXPERIMENTAL SETTINGS,0.6560509554140127,"5.5
How do design choices on the QA module affect EAGER’s performance?"
EXPERIMENTAL SETTINGS,0.6624203821656051,"Figure 5: Average reward in PutNextTo
with various ablations of the QA module."
EXPERIMENTAL SETTINGS,0.6687898089171974,"In Section 4.1 we made two choices for the QA mod-
ule and the associated intrinsic reward: first we added
a ""no_answer"" response, second we rewarded each an-
swer by the confidence the agent had in its own answer.
To verify the influence of these choices over EAGER’s
performance, we use the PutNextTo task to compare EA-
GER against ""EAGER \no_answer"", ""EAGER Simple"",
and ""EAGER Simple \no_answer"". ""Simple"" means that
the agent received a binary intrinsic reward (1 for a good
answer, 0 otherwise) and ""\no_answer"" means we sup-
pressed the ""no_answer"" solution. Figure 5 gives the re-
sults of these ablations. We can see that both the use of
""no_answer"" and the non-binary reward independently
boost sample efficiency."
CONCLUSION,0.6751592356687898,"6
Conclusion"
CONCLUSION,0.6815286624203821,"In this work, we have proposed to leverage the abstraction and generalisation properties of language to
build an automatic reward shaping method in the context of long horizon and sparse reward tasks. Our
learning agent generates its own questions from the goal and rewards itself for correctly answering
them, resulting in an efficient curriculum over auxiliary objectives. This is to be contrasted with
ELLA [23] where expert knowledge is required for choosing auxiliary objectives. Besides, we do not
call upon an oracle for getting linguistic description of environment states as in [25]."
CONCLUSION,0.6878980891719745,"Limitations and Future Work
EAGER assumes the QA system was pre-trained using a pre-
existing set of example trajectories. Next steps will consist in investigating how to remove this
limitation, e.g. by implementing autotelic strategies based on QG/QA learned online. Besides, in
this work we tested our method on BabyAI, a 2D environment with synthetic language. In the future,
we would like to consider a more complex language, generating more complex questions than the
one obtained by masking, and testing our method on more realistic environments with true human
instructions, as in the ALFRED dataset [32]."
CONCLUSION,0.6942675159235668,Acknowledgments and Disclosure of Funding
CONCLUSION,0.7006369426751592,"This work benefited from the use of the Jean Zay supercomputer associated with the Genci grant
A0091011996, as well as from the ANR DeepCuriosity AI chair project."
CONCLUSION,0.7070063694267515,This work has been supported by the Horizon 2020 PILLAR-robots project (grant number 101070381)
REFERENCES,0.7133757961783439,References
REFERENCES,0.7197452229299363,"[1] Ahmed Akakzia, Cédric Colas, Pierre-Yves Oudeyer, Mohamed Chetouani, and Olivier Sigaud.
Grounding language to autonomously-acquired skills via goal generation. arXiv preprint
arXiv:2006.07185, 2020."
REFERENCES,0.7261146496815286,"[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian
Reid, Stephen Gould, and Anton van den Henge. Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real environments. In CVPR, 2018."
REFERENCES,0.732484076433121,"[3] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy
sketches. In International Conference on Machine Learning (ICML), 2017."
REFERENCES,0.7388535031847133,"[4] Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in
reinforcement learning. arXiv preprint arXiv:1908.06976, 2019."
REFERENCES,0.7452229299363057,"[5] Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli,
and Edward Grefenstette. Learning to understand goal specifications by modelling reward. In
arXiv preprint arXiv:1806.01946, 2018."
REFERENCES,0.7515923566878981,"[6] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi
Munos.
Unifying count-based exploration and intrinsic motivation.
Advances in neural
information processing systems, 29, 2016."
REFERENCES,0.7579617834394905,"[7] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan
Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample effi-
ciency of grounded language learning. In International Conference on Learning Representations
(ICLR), 2019."
REFERENCES,0.7643312101910829,"[8] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark
reinforcement learning. In International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.7707006369426752,"[9] Cédric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents with
intrinsically motivated goal-conditioned reinforcement learning: a short survey. Journal of
Artificial Intelligence Research, 2022."
REFERENCES,0.7770700636942676,"[10] Cédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Pe-
ter Ford Dominey, and Pierre-Yves Oudeyer. Language as a cognitive tool to imagine goals in
curiosity-driven exploration. In Advances in Neural Information Processing Systems (NeurIPS),
2020."
REFERENCES,0.7834394904458599,"[11] Alison Gopnik and Andrew Meltzoff. The development of categorization in the second year
and its relation to other cognitive and linguistic developments. In Child development, 1987."
REFERENCES,0.7898089171974523,"[12] Goyal, Niekum, and Mooney. Using natural language for reward shaping in reinforcement
learning. In International Joint Conference on Artificial Intelligence (IJCAI), 2019."
REFERENCES,0.7961783439490446,"[13] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In arXiv
preprint arXiv:1507.06527, 2015."
REFERENCES,0.802547770700637,"[14] Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer,
David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, and Denis Teplyashin. Grounded
language learning in a simulated 3d world. In arXiv preprint arXiv:1706.06551, 2017."
REFERENCES,0.8089171974522293,"[15] Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuandong Tian, and Mike Lewis. Hierarchical
decision making by generating and following natural language instructions. In arXiv preprint
arXiv:1906.00744, 2019."
REFERENCES,0.8152866242038217,"[16] U. Jain, S. Lazebnik, and A. G. Schwing.
Two can play this game: Visual dialog with
discriminative question generation and answering. In Proc. CVPR, 2018."
REFERENCES,0.821656050955414,"[17] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn. Language as an abstraction for hierarchical deep
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS),
2019."
REFERENCES,0.8280254777070064,"[18] Stefano Ermon Jonathan Ho. Generative adversarial imitation learning. In arXiv preprint
arXiv:1606.03476v1, 2016."
REFERENCES,0.8343949044585988,"[19] Iou-Jen Liu, Xingdi Yuan, Marc-Alexandre Côté, Pierre-Yves Oudeyer, and Alexander G.
Schwing. Asking for knowledge: Training rl agents to query external knowledge using language.
In International Conference on Machine Learning (ICML), 2022."
REFERENCES,0.8407643312101911,"[20] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward
Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning
informed by natural language. arXiv preprint arXiv:1906.03926, 2019."
REFERENCES,0.8471337579617835,"[21] C. Lynch and P. Sermanet. Grounding language in play. In arXiv preprint arXiv:2005.07648,
2020."
REFERENCES,0.8535031847133758,"[22] Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. Listen, attend, and walk: Neural
mapping of navigational instructions to action sequences. In Association for the Advancement
of Artificial Intelligence (AAAI), 2016."
REFERENCES,0.8598726114649682,"[23] Suvir Mirchandani, Siddharth Karamcheti, and Dorsa Sadigh. Ella: Exploration through learned
language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.8662420382165605,"[24] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations
to actions with reinforcement learning. In arXiv preprint arXiv:1704.08795v2, 2017."
REFERENCES,0.8726114649681529,"[25] Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktäschel,
and Edward Grefenstette. Improving intrinsic exploration with language abstractions. In arXiv
preprint arXiv:2202.08938v1, 2022."
REFERENCES,0.8789808917197452,"[26] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory
and application to reward shaping. In International Conference on Machine Learning (ICML),
volume 99, pages 278–287, 1999."
REFERENCES,0.8853503184713376,"[27] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-
language navigation. In arXiv preprint arXiv:2105.06453v2, 2021."
REFERENCES,0.89171974522293,"[28] Roberta Raileanu and Tim Rocktäschel.
Ride: Rewarding impact-driven exploration for
procedurally-generated environments. In International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.8980891719745223,"[29] Clement Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier,
Jacopo Staiano, Geoffrey Scoutheeten, and Patrick Gallinari. Data-QuestEval: A referenceless
metric for data-to-text semantic evaluation. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 8029–8036, Online and Punta Cana, Dominican
Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
emnlp-main.633. URL https://aclanthology.org/2021.emnlp-main.633."
REFERENCES,0.9044585987261147,"[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. In arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.910828025477707,"[31] Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski,
Jacopo Staiano, and Alex Wang. Questeval: Summarization asks for fact-based evaluation. In
arXiv preprint arXiv:2103.12693v2, 2021."
REFERENCES,0.9171974522292994,"[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-
taghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded
instructions for everyday tasks. In arXiv preprint arXiv:1912.01734v2, 2019."
REFERENCES,0.9235668789808917,"[33] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018."
REFERENCES,0.9299363057324841,"[34] Allison C Tam, Neil C Rabinowitz, Andrew K Lampinen, Nicholas A Roy, Stephanie CY Chan,
DJ Strouse, Jane X Wang, Andrea Banino, and Felix Hill. Semantic exploration from language
abstractions and pretrained representations. arXiv preprint arXiv:2204.05080, 2022."
REFERENCES,0.9363057324840764,"[35] Nicholas Waytowich, Sean L. Barton, Vernon Lawhern, and Garrett Warnell. A narration-
based reward shaping approach using grounded natural language commands. In International
Conference on Machine Learning (ICML), 2019."
REFERENCES,0.9426751592356688,"[36] Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Perceiving
the world: Question-guided reinforcement learning for text-based games. In arXiv preprint
arXiv:2204.09597v2, 2022."
REFERENCES,0.9490445859872612,Checklist
REFERENCES,0.9554140127388535,1. For all authors...
REFERENCES,0.9617834394904459,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes] See the Method and Experiments Section 4 and 5.
(b) Did you describe the limitations of your work? [Yes] See the Limitations paragraph of
in Section 6
(c) Did you discuss any potential negative societal impacts of your work? [N/A] We do
not foresee major negative societal impacts of this work.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes] We do believe our work conforms to them.
2. If you are including theoretical results..."
REFERENCES,0.9681528662420382,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments..."
REFERENCES,0.9745222929936306,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We give the
URL in the introduction.
(b) Did you specify all training details (e.g., data splits, hyperparameters, how they were
chosen)? [Yes] See the Section 5 and the Appendix
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] We ran our main experiments with multiple random seeds
and plotted curves with standard deviations.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See the end of section 5.1
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9808917197452229,"(a) If your work uses existing assets, did you cite the creators? [Yes] We use open-source
environments, cited in the code and in the paper text.
(b) Did you mention the license of the assets? [Yes] We inherit the license in our attached
code repository.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]"
REFERENCES,0.9872611464968153,"We include the code of our approach
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9936305732484076,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
